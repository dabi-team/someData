2
2
0
2

g
u
A
1
1

]
E
S
.
s
c
[

1
v
0
5
9
5
0
.
8
0
2
2
:
v
i
X
r
a

Interactive Code Generation via Test-Driven User-Intent
Formalization

Shuvendu K. Lahiri*1, Aaditya Naik*†2, Georgios Sakkas*†3, Piali Choudhury1,
Curtis von Veh1, Madanlal Musuvathi1, Jeevana Priya Inala1, Chenglong Wang1,
and Jianfeng Gao1

1Microsoft Research
{shuvendu, pialic, curtisvv, madanm, jinala,
chenwang, jfgao}@microsoft.com
2University of Pennsylvania
asnaik@seas.upenn.edu
3University of California, San Diego
gsakkas@eng.ucsd.edu

August 12, 2022

ABSTRACT

Pre-trained large language models (LLMs) such as OpenAI Codex have shown
immense potential in automating signiﬁcant aspects of coding by producing natu-
ral code from informal natural language (NL) intent. However, the code produced
does not have any correctness guarantees around satisfying users intent. In fact, it
is hard to deﬁne a notion of correctness since natural language can be ambiguous
and lacks a formal semantics.
In this paper, we take a ﬁrst step towards addressing the problem above by propos-
ing the workﬂow of test-driven user-intent formalization (TDUIF), which lever-
ages lightweight user feedback to jointly (a) formalize the user intent as tests (a
partial speciﬁcation), and (b) generates code that meets the formal user intent. To
perform a scalable and large-scale automated evaluation of the algorithms without
requiring a user in the loop, we describe how to simulate user interaction with
high-ﬁdelity using a reference solution. We also describe and implement alter-
nate implementations of several algorithmic components (including mutating and
ranking a set of tests) that can be composed for efﬁcient solutions to the TDUIF
problem.
We have developed a system TICODER that implements several solutions to
TDUIF, and compare their relative effectiveness on the MBPP academic code
generation benchmark. Our results are promising with using the OpenAI Codex
LLM on MBPP: our best algorithm improves the pass@1 code generation accu-
racy metric from 48.39% to 70.49% with a single user query, and up to 85.48%
with up to 5 user queries. Second, we can generate a non-trivial functional unit test
consistent with the user intent within an average of 1.69 user queries for 90.40%
of the examples in this dataset.

*Equal Contribution
†Work done while at Microsoft Research

1

 
 
 
 
 
 
1

INTRODUCTION

Pre-trained Large Language Models (LLMs) have shown tremendous potential in generating natural-
looking programs from informal intent expressed in natural language. There has been surge in
research on training large language models over programming language artifacts in just the last
year (Chen et al., 2021; Chowdhery et al., 2022; Nijkamp et al., 2022; Fried et al., 2022; Xu et al.,
2022). Most of these LLMs are based on recent advances in Transformer neural network archi-
tectures (Vaswani et al., 2017) and the availability of large corpus of source code in open source
(say, GitHub). Commercial offerings such as Copilot (GitHub, 2022) are now commercially avail-
able, and are already known to generate non-trivial fraction of code in real-world developer scenar-
ios (Ziegler et al., 2022).

However, the rise of code completion and code synthesis has also given rise to several interesting
challenges and opportunities for generating correct code. First, natural language is ambiguous,
unlike formal speciﬁcations, to express the user intent. As a simple example, the Python docstring
"""Sort a list of integers""" does not specify if the user wishes to sort the list in the
ascending or descending order of the values. The lack of a precise intent means that one cannot even
articulate the correctness of the code generated by a LLM. Second, users struggle to understand
code suggestions presented to them without the ability to run or debug the code (Vaithilingam et al.,
2022). This causes users to accept buggy code that they don’t trust, or reject correct code that are
too difﬁcult to understand and therefore trust. Finally, LLMs often present a large list of suggestions
(beyond the most likely one) that is hard to navigate for a user other than linearly scanning each
code suggestion and rejecting the incorrect ones.

In this paper, we advocate leveraging lightweight user-feedback to improve trust in LLM-generated
code. Speciﬁcally, we advocate the problem of test-driven user-intent formalization (or perhaps
test-driven user-intent discovery) to create an interactive framework to (a) reﬁne and formalize the
user intent through generated tests, and (b) generating code that is consistent with such tests.

Let us demonstrate a simple instantiation of the framework using the example of sorting a list of
integers in Python programming language. Consider the scenario when a user Alice prompts a LLM
to generate code that satisﬁes their query expressed in natural language with some Python syntax for
the method signature:

1 def sort(l):
2

"""Sort a list of integers""

Instead of displaying a list of suggestions (that usually assumes the user expects the list to be sorted
in an ascending order), our framework TICODER would query the user with a question:

Did you mean sort([3, 2, 4]) == [2, 3, 4]?

Let us assume that the user answers ’no’. The workﬂow would likely query the user again with the
following question:

Did you mean sort([3, 2, 4]) == [4, 3, 2]?

If the user says ’yes’, then the system would output the list of approved tests, as well as code
suggestions that are consistent with the tests.

"""Sort a list of integers"""

sorted(l, reverse=True)

1 def sort(l):
2
3
4
5 def test_sort():
6
7
8 test_sort()

assert sort([3, 2, 4] == [4, 3, 2]

The simple interactive framework demonstrates several aspects to leverage LLMs to build more
trusted and correct software (beyond code completion).

2

1. First, it can leverage user feedback from a test to prune a large fraction of likely suggestions
that do not satisfy user intent, including suggestions that contain syntax or semantic errors.
In its current form, a user has to linearly scan through the list of code suggestions and reject
each one of them.

2. Second, the framework automatically generates tests (whenever possible) that help reﬁne
the user intent and make it precise (albeit partial). These (unit) tests are also useful for
debugging individual functions early and detect errors (if the user manually changes the
suggestions as they often do Ziegler et al. (2022)) faster.

3. Finally, the overhead of the interaction (compared to uni-directional use of LLMs) is offset
whenever a test is generated that saves the effort of writing tests by a user. Although the
tests for the above examples are simple, real-world unit tests often comprise of carefully
selecting a sequence of statements to drive the system to an interesting state and writing a
test-oracle to check the intended behavior at such a state Dinella et al. (2022).

While the framework appears an intuitive extension of current usage of LLMs, the utility of the
interactive framework is contingent upon the cost-beneﬁt trade-off of the overhead of user interaction
versus the improvement in the quality of the generated code (and tests). In this paper, we contribute
by studying the problem of test-driven user-intent formalization through the following steps:

1. First, we deﬁne the problem of test-driven user-intent formalization as the problem of gen-

erating code and tests satisfying the natural language intent with a user in the loop.

2. We describe a way to simulate user response with high-ﬁdelity and establish a set of ofﬂine
metrics that can enable us to automatically evaluate different solutions on a given bench-
mark without requiring a user in the loop.

3. We describe an abstract workﬂow TestDrivenIntentDiscoveryWorkﬂow parameterized by
various well-speciﬁed components that can be instantiated to cover a large spectrum of
solutions.

4. We provide a programming language-agnostic approach by leveraging off-the-shelf LLMs
for generating code and test from NL, and provide (rule-based) implementation of the sev-
eral algorithmic components of the workﬂow, including alternate implementations.

5. We evaluate the various algorithms across various metrics on an academic benchmark of
simple Python programming benchmark MBPP, and demonstrate the effectiveness of some
of the heuristics.

The preliminary results from our experiments are encouraging. Our results are promising with using
the OpenAI Codex LLM on MBPP: our best algorithm improves the pass@1 code generation
accuracy metric from 48.39% to 70.49% with a single user query, and up to 85.48% with up to 5
user queries. Second, we can generate a functional unit test1 consistent with the user intent within
an average of 1.69 user queries for 90.40% of the examples in this dataset.

Additionally, we establish that there is signiﬁcant room to improve current algorithms given the best
performance an ideal algorithm can have. We also establish that the baseline for a purely neural
approach to test generation (Codex), and illustrate signiﬁcant improvements using new execution-
based test mutation and test and code ranking. We believe that the problem of TDUIF offers a rich
area of research to not only leverage existing models and user interaction to generate correct code
(i.e., code with some guarantees albeit weak), but also develop new neural models for correct code
generation.

2 WORKFLOW AND PROBLEM FORMULATION

In this section, we outline the workﬂow for leveraging test generation and user feedback to formalize
and reﬁne user intent. Next we deﬁne metrics to evaluate different approaches that implement the
workﬂow. Finally, we then discuss how to simulate the user feedback with high-ﬁdelity to perform
automated and scalable evaluation of approaches without a user in the loop.

1We deﬁne a functional unit test as a test of the form f (i) == o, to distinguish from a unit test that only

checks a weak assertion on the output state for a given input.

3

Figure 1: Workﬂow for test-driven user-intent formalization (TDUIF).

Metric
pass@k@m

pass@k@*

Meaning
Syntactic sugar for (possibly ranked) pass@k metric after m
user queries
Syntactic sugar for (possibly ranked) pass@k metric after
the ﬁrst test has been accepted

NumQueriesToAccept Number of queries for user to get the ﬁrst accepted test

Table 1: Code and test generation metrics to evaluate the quality of a TDUIF solution.

2.1 HIGH-LEVEL WORKFLOW AND METRICS

Figure 1 describes the high-level workﬂow of Test-Driven User-Intent Formalization (TDUIF).

1. The human user prompts the agent for completing a function body given the preﬁx in a ﬁle,
a natural language description and the function header/signature containing method name,
parameters and returns.

2. The agent repeatedly queries the user (until a stopping criterion is reached) asking if a set

of behaviors (or a test) is consistent with the user intent.

3. The user responds either YES, NO, or DONTKNOW to each of the queries from the agent.
4. Once the interaction terminates, the agent outputs (a) a set of tests that the user has ap-
proved, and (b) a ranked list of code suggestions that are consistent with the user responses.

We allow a 3-valued user response that includes DONTKNOW since there are cases when the user
may not be able to determine the evaluation of a test case. For example, if the test has parse error
(e.g., assert (foo(), the question of the test being consistent with the user intent is not well
deﬁned. Similarly, a ﬂaky test that depends on non-determinism within a function may not have a
unique answer (e.g., assert (CurrentDayOfWeek() == Sunday)) Luo et al. (2014).

Table 1 describes various metrics to evaluate the quality of an agent over a benchmark set.

• For evaluating the quality of the generated code suggestions, one can use the hidden2 unit
tests to determine the correctness of the code suggestions, and appeal to the popular metric
pass@k (Chen et al., 2021). A code suggestion is correct if it passes all the hidden tests
for the function, and pass@k determines the fraction of cases where at least one code
suggestion is correct given k tries. We deﬁne the syntactic sugar pass@k@m to denote
the pass@k for the code suggestions in G after m user queries. For m = 0, pass@k@0
coincides with pass@k.

2We say hidden to mean that the code or test generation algorithms do not have access to them.

4

• Alternately, one can also use a stopping criterion where the user is queried until she accepts
a test suggestion. We deﬁne the metric pass@k@* to denote pass@k value using this
stopping criterion.

• Finally, we also deﬁne a secondary metric for evaluating the efﬁcacy of test generation
using NumQueriesToAccept. This metric measures the number of user queries (or
proposed tests) needed for the user to accept a single test; this is identical to the number of
queries needed for the second stopping criterion.

Observe that the pass@k@m metric also indirectly serves to measure the quality of generated tests,
by favoring tests that better distinguish correct code suggestions from incorrect ones. The test gen-
eration metric NumQueriesToAccept metric is not strong enough in isolation; a test generator
that outputs trivial tests such as assert True can obtain optimal values.

2.2 ALGORITHM

We make the workﬂow more precise next in the setting of a class of simple programs containing a
single function with a hidden reference implementation and hidden unit tests.
Deﬁnition 2.1. A program p is a tuple (cid:104)prfx p, sp, hp, bp, Tp(cid:105), where prfx p is a preﬁx that may con-
tain deﬁnitions of other global variables and imports, sp is a natural language string description,hp
is the function header or signature, bp is the body of the function and Tp is a set of unit tests.

Figure 2: A simple Python program sample p and the provided test set Tp.

Figure 2 gives an example of a program p, where prfx p, sp, hp, bp, Tp are presented for a simple
problem where the programmer has to ”Sort a list of tuples using the 2nd value of each tuple”.

We further simplify the notion of a test t ∈ T to be an input-output pair (i, o). A function f satisﬁes
a test (i, o) if and only if the result of executing f on i terminates with a value o, i.e., f (i) = o.
Deﬁnition 2.2. An implementation f (cid:48) of a function fp is correct with respect to the program p (or
simply correct, when p is clear from the context) if it satisﬁes all the tests for p, i.e., for each test
(i, o) ∈ Tp, f (cid:48)(i) = o.

Algorithm 1 describes the workﬂow sketched previously in Figure 1. It takes as inputs the preﬁx
prfx in a ﬁle containing imports and other global variables, the natural language description of intent
s, the signature/header of a function h including the function name and parameters. The algorithm
also takes as input a stopping criteria predicate StoppingCriteria to terminate the interaction with
the user. Once terminated, the algorithm returns a set of tests approved by the user T + as well as a
ranked list G of candidate implementations of f that satisﬁes all the tests in T +. There are different

5

stopping criteria including (a) terminating after a constant number of MAX U queries to the user,
or (b) terminating only after a test has been accepted by the user.

The algorithm is parameterized by a number of components that are underlined. The algorithm
starts off by generating sets of code and test suggestions in to the variables G and U respectively.
The quality of these sets will depend on the choice of the large language model M as well as
the code and test prompts constructed from the problem description3. We allow the test genera-
tion prompt to take the set of generated codes in G, to possibly improve the prompt. The algo-
rithm maintains the invariant that the ﬁnal set of code suggestions returned to the user is always
a subset of the initial set of suggestions in G. On the other hand, we allow the set of tests in
U to be modiﬁed or augmented through both syntactic and dynamic mutation techniques using
SyntacticMutateTests and DynMutateTests components respectively. Notice the dynamic muta-
tion technique DynMutateTests takes the set of code suggestions in G as an input (in addition to
U ) as G.

Finally, it iterates in a loop (spanning lines 6 to 18) until the stopping critera is satisﬁed and U
is non-empty, ranking the tests in U using the method RankTests and presents the user with the
top-ranked test. If the user accepts the test, the set T + is updated, and any code suggestion in G
that disagrees with the test is pruned away. Conversely, if the user rejects the test, then any code
suggestion in G that agrees with the test is pruned away. No action is taken if the user responds with
DontKnow. Finally, the code suggestions in G are re-ranked with the remaining test suggestions
in U after the pruning.

Algorithm 1 TestDrivenIntentDiscoveryWorkﬂow
Input: Preﬁx prfx , description s, header h of a function f
Input: A predicate for stopping criteria for the interaction StoppingCriteria
Output: A ranked list of candidate implementations for f G,
Output: A set of user-approved tests T +
Output: f (cid:48)(i) == o for each f (cid:48) ∈ G and (i, o) in T +
1: G ← QueryLLM (M, CodeGenPrompt(prfx , s, h))
2: U ← QueryLLM (M, TestGenPrompt(prfx , s, h, G))
3: U ← SyntacticMutateTests(U )
4: U ← DynMutateTests(U, G)
5: T +, k ← {}, 0
6: while ¬StoppingCriteria(k, T +) and |U | > 0 do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end while
19: return G, T +

U ← RankTests(U, G)
(i, o) ← U.pop()
k ← k + 1
r ← SatisﬁesUserIntent((i, o), f )
if r == YES then

T + ← T + ∪ {(i, o)}
G ← G \ {c | c(i) (cid:54)= o}

end if
G ← RankCodes(G, U )

G ← G \ {c | c(i) == o}

else if r == NO then

(cid:46) Query LLM for codes
(cid:46) Query LLM for tests
(cid:46) Mutate tests statically
(cid:46) Mutate tests using dynamic execution

(cid:46) Rank the test suggestions
(cid:46) Remove the top ranked test
(cid:46) Number of user queries
(cid:46) Query user for intent

(cid:46) Prune codes that fail the accepted test

(cid:46) Prune codes that pass the rejected test

(cid:46) Rank the code suggestions

2.3 SIMULATING THE USER RESPONSE

The above formulation has one issue that makes it hard to study this problem with benchmark
datasets — namely that it requires a user to determine if a test is consistent with their intent in
the method SatisﬁesUserIntent (in Algorithm 1). This creates a challenge for an automatic evalua-
tion of different algorithms on large benchmarks without involving users. In this section, we provide

3Although there exists automated test generation tools such as Randoop Pacheco et al. (2007), our algorithm
uses large language models to generate the initial seed tests as (a) we do not have the body of the method under
test, and (b) these techniques are language-speciﬁc.

6

a problem formulation that allows high-ﬁdelity simulation of the action of any user. Our key insight
is to leverage the reference implementation (that contains bp as the body) to be a proxy for a user
and determine the user response by evaluating a test on the reference solution.
Deﬁnition 2.3. For a function f in a program p with a reference implementation fp (comprising of
hp as header and bp as body) of f , and a test (i, o),

• SatisﬁesUserIntent((i, o), f ) returns YES if fp(i) == o.

• SatisﬁesUserIntent((i, o), f ) returns NO if fp(i) terminates successfully (without throw-

ing any exceptions) with a value o(cid:48) (cid:54)= o.

• Finally SatisﬁesUserIntent((i, o), f ) returns DONTKNOW if outcome of fp(i) is unde-

ﬁned either due to syntax errors or runtime exceptions.

In general case, a test itself can have syntax errors. Even for our restricted case of tests being input-
output pairs, one can have syntax errors where the input (or output) are ill-formed (such as a string
literal without a closing quote ‘"’). As discussed earlier, a user would answer DONTKNOW to such
queries.

We leverage the above insight to design a problem that exploits the reference implementation and
unit tests to simulate the user in the interaction.
.
Deﬁnition 2.4. The problem of test-driven user-intent formalization for a given program p
=
(cid:104)prfx p, sp, hp, bp, Tp(cid:105) is to design an algorithm A(prfx p, sp, hp) that takes as input the preﬁx, header
and natural language description of a function and co-generates (a) a set of tests U that are consistent
with the hidden reference implementation (hp, bp), and (b) a set of candidates for the function body
that satisfy all the hidden unit tests in Tp.

2.4 THREATS TO VALIDITY TO SIMULATING USERS

Although the problem formulation above allows us to evaluate the quality of the solutions in a
completely automated manner, it is a proxy for using a real user study.

First, we inherit the well known false positive issue in program synthesis related to the incomplete-
ness of test suites as the correctness speciﬁcation for code. It admits degenerate solutions f (cid:48) that
only satisfy the exact input output pairs in the unit tests in Tp, but do not generalize to satisfy the
intent of the user for most other inputs.

Second, the use of non-exhaustive set of hidden tests also leads to unexpected situations where
pruning incorrect code using user feedback can reduce the pass@1. Consider the case where the
user wants to generate code satisfying the natural language description

1 def double(x):
2

"""double an integer"""

Let us also assume that the reference implementation consists of

1 def double(x):
2
3

"""double an integer"""
return x + x

The unit test suite Tp consists of a single test (2, 4).

Let us assume that a model generates a code suggestion:

1 def double(x):
2
3

"""double an integer"""
return 4

The pass@1 metric for this set of suggestions (without any interaction) is 1, since the code sugges-
tion satisﬁes the hidden tests. Consider the case when the user is presented with a generated test
(1, 2). Since this test is consistent with user intent (and satisﬁed by the reference solution), a user

7

would respond ’yes’. This results in pruning the solution returning 4, resulting in pass@1@1 to be
0, lower than pass@1.

Finally, the idea of using the reference solution to determine user response may be either optimistic
or be too conservative.

• Consider the case when a test t satisﬁes the reference solution fp, and we simulate the user
response as YES. However, consider the case when the test presents a query that is difﬁcult
for the user to determine — say, asking the result of the 335th Fibonacci number, if the test
asks Fib(335) == 1000043! A user may say DONTKNOW for such a query.

• Second, consider the case when asked about the Fibonacci on a negative number (e.g.,
Fib(−3) == 2). Again, a user may want to respond DONTKNOW as the test violates an
implicit precondition. However, it may be that evaluating this test case on the reference
solution does not throw a precondition violation exception, but simply fails the assertion
resulting is us reporting a NO for the user response.

Finally, real-world tests are often more than an input-output pair, and consists of a sequence of
statements terminating in an assertion. In rare cases, the overhead of inspecting multiple such tests
(although they seldom consists of conditional branches and loops) may outweigh the beneﬁt of
scanning through code suggestions directly when the code is relatively simple.

Although never a substitute for user-study, we still believe that in most cases where the unit tests are
fairly complete and the code suggestions are relatively complex, using the test suite and the reference
solution can serve as a good proxy for automatic and scalable evaluation of different solutions to the
interactive workﬂow.

3 TICODER COMPONENTS

In this section, we describe a tool TICODER that implements the various components that are un-
derlined in Algorithm 1. For each component (such as CodeGenPrompt, RankTests), we provide
several possible alternate implementations to deﬁne the space of solutions.

3.1 CODE AND TEST GENERATION PROMPTS

It is well-known that the choice of prompts that determine the actual string that is fed to a
large language model has a substantial impact on the quality of output (Reynolds & McDonell,
2021). In this section, we outline several choices for implementing the prompt generation routines
CodeGenPrompt and TestGenPrompt for generating code and test suggestions from the problem
description consisting of (prfx p, sp, hp).

Figure 3: Example code and test prompts for the running example in Figure 2 that instantiate the
algorithm A(prfx p, sp, hp) for the test-driven user-intent formalization problem.

Figure 3 presents a possible code prompt (in the blue boxes) that is generated by A(prfx p, sp, hp)
and can be passed to a LLM to produce code suggestions for the given problem in Figure 2. Query-
ing a LLM (say Codex) with the code generation prompt in Figure 3 will result in a set of code

8

suggestions as shown in Figure 4. Code suggestion c2 is a valid solution to the problem, while c1
is an incorrect code suggestion (since no output is returned by the function) and c3 is also incorrect
(since it returns only the second value of the tuples). Given the simple nature of our benchmarks
(say, MBPP) and the relatively small size of the prompt, we do not expect much innovation in code
generation prompts and therefore ﬁx this prompt for all our experiments.

On the other hand, there are interesting choices for the test generation prompts for TestGenPrompt
even for the simple setup. Given that we wish to generate a test for a function without an imple-
mentation, the problem of TestGenPrompt really boils down to completing the method body of f .
The green boxes in Figure 3 show the “Prompt Body” and the subsequent “Test Body” that together
constitute the test prompt. As an example completion of the method body, we use the statement
pass that corresponds to a placeholder implementation in Python. The generated test suggestions
(Figure 4) present the user with a set of tests. Some of these are consistent with the user intent (t3);
while others are either inconsistent with the user intent (t2), or contain syntax errors (t1).

Figure 4: Code and test suggestions for the running example in Figure 2 generated from a LLM.
Code suggestion c2 and test suggestion t3 are both correct, while code suggestions c1, c3 and test
suggestions t1, t2 are incorrect (appear shaded), i.e.
they don’t satisfy the problem prompts in
Figure 3.

There are several other interesting possibilities for designing TestGenPrompt. For instance, we can
sample a code completion b(cid:48)
p from the set of generated code suggestions G and use it as deﬁnition
of f . We therefore explore two options for bp in this work:

• pass: We can instantiate bp to simply be pass to keep the prompt for the test generation

syntactically correct, as illustrated in Figure 3.

• choose(G): Alternatively, we can sample a deﬁnition from the code suggestions and use it
to instantiate the body of f . The choice of the code suggestion may impact the quality of
the generated tests, and therefore can beneﬁt from good heuristics. In our experiment, we
choose the code suggestion that appears ﬁrst in the (unordered) set of code suggestions in
G.

3.2 STATIC MUTATION OF TESTS WITH SyntacticMutateTests

Given an initial set of candidate tests in U , one can perform various static mutations of a given test
t to yield new test cases and prune away unnecessary test cases. For each test t ∈ U , we consider
two options for statically mutating it:

• In case a test has a parsing error, we consider the longest preﬁx of t that parses.

9

• We consider the preﬁx of t up to the ﬁrst assertion; since each assertion is a point of failure,
considering only one assertion maximizes the chance of a test passing. We refer to this
technique as single-assert.

Finally, we can safely prune a test t if (a) t has a syntax error, or (b) if t does not contain any
assertions, in which case it will not help formalize the intent.

However, note that some of these decisions (such as single-assert) may also adversely impact
the performance as it weakens the tests.

3.3 DYNAMIC MUTATION OF TESTS WITH DynMutateTests

In addition to statically mutating tests, one can also exploit the ability to execute the tests to obtain
new tests.
Given a test (i, o) and a candidate implementation f (cid:48), we can generate an alternate test (i, f (cid:48)(i))
by modifying the output value observed by executing f (cid:48) (if any). The intuition behind this is that
if f (cid:48) happens to be a correct solution, then we generate at least one test that is consistent with f (cid:48).
On the other hand, this has the potential to create an explosion of tests; one can further devise
heuristics to pick a subset of tests, or prune the original tests generated by LLMs. We have imple-
mented assert-rewrite-all, where we augment U with all the tests obtained by rewriting
each (i, o) ∈ U with (i, f (i)) for each f ∈ G.

3.4 RANKING TEST SUGGESTIONS USING RankTests

Finally, we need a way to rank the set of tests in U to choose the order in which to present them to
the user. We can choose from several options to implement RankTests, including:

• Random (random): We randomly choose a test from U .
• Discrimination-based (entropy): We can sort the tests in U by how well they discrim-
inate the set of code suggestions in G. For a test t, we deﬁne sets G+
that pass
and fail an assertion, respectively, for the test t. Let min and max denote the minimum and
maximum, respectively, of the pair (G+

t ). We deﬁne the following metric:

t and G−
t

t , G−

score

.
=

(cid:26) 0

min/max

(cid:27)

if max is 0
otherwise

and sort U in descending order of this score. The intuition is that the tests with higher score
has a better chance of pruning a large number of suggestions in G, irrespective of the user
response of YES or NO

There are several other variations of these heuristics that one can employ. For example, we do not
account for the tests that throw an exception or syntax error as part of the equation above — our
intuition is that the user would most likely respond with DONTKNOW. However, one alternative
could be to consider any form of failure to deﬁne G−
t .

3.5 RANKING CODE SUGGESTIONS USING RankCodes

Finally, our goal is to present the user with a ranked list of code suggestions in G. We currently
deﬁne a single code ranking strategy (passing-tests) that uses the tests in U to determine an
ordering on G as follows:

• Each generated code c ∈ G is executed with every test t ∈ U (after pruning and/or mu-
tation) and gets assigned as a score the number of satisﬁed tests dc. The codes are then
ranked based on the decreasing order of dc.

Other possible alternatives (not currently implemented in TICODER) include creating an equivalence
class over the set of codes in G, where two suggestions c1 and c2 belong to the same class if they
satisfy the same set of candidate tests in U , and then ordering the equivalence classes by the number
of tests satisﬁed by codes in the class. Other variations of clustering and ranking code suggestions
using tests have also been previously explored in recent works (Chen et al., 2022; Li et al., 2022).

10

4 EVALUATION

4.1 RESEARCH QUESTIONS

We pose the following research questions in order to evaluate different approaches and techniques
for test-driven user-intent formalization problem:

1. RQ1: Does test-driven user-intent formalization workﬂow improve the accuracy of code

suggestions?

2. RQ2: How does the quality of generated code and tests vary with the number of user-

queries?

3. RQ3: How does each of the design decisions affect the metrics (ablation study)?

4.2 DATASET

We use the sanitized version of the MBPP dataset (Austin et al., 2021), an academic code generation
dataset, to answer RQ1 through RQ3. This version consists of 427 (cid:104)prfx p, sp, hp, bp, Tp(cid:105) tuples as
per Deﬁnition 2.1 where bp is the ground truth deﬁnition of the corresponding function. One example
of such a tuple has been discussed in Figure 2.

4.3 EXPERIMENTAL SETUP AND TOOLS

For all experiments we use Codex’s code-davinci-002 model for inference only.
In each
case, we query the Codex model for 100 code suggestions, with a temperature of 0.8 and a top p
of 0.95. The maximum code generation length is 300 tokens. Additionally, we query the Codex
model for 50 test suggestions using the same parameters as before. Results are reported using the
pass@k@m metric described in subsection 2.1. Further, in order to account for the non-determinism
of Codex, we only query Codex to generate the initial code and test suggestions into a cache of
Codex responses and refer to the same cache for all experiments.

We have implemented our approach in TICODER (Test-driven interactive Coder) and we compare
the performance of our approach with the Codex model for code generation that does not perform
user interaction. We also consider a Baseline version of TICODER, where the tests to be presented to
the user are generated by Codex with TestGenPrompt = pass with no further mutation, pruning
or ranking while also disabling code ranking.

We also consider a few settings that help us establish an upper bound on the performance of any
solution:

1. First, we deﬁne IdealTests where the tests presented to the user only consist of the hidden

tests from Tp.

2. Second, we deﬁne IdealRanking, where the set of tests generated in U are ordered such
that they result in the maximum number of incorrect code solutions to be pruned away.
Since this knowledge requires knowing the (hidden) reference solution, it is not a realizable
solution in practice. However, it helps determine the best ranking within a set of tests.

the default TICODER, we set TestGenPrompt = pass, StaticMutateTests =
For
single-assert, DynMutateTests = assert-rewrite-all, TestRanking = entropy,
and CodeRanking = passing-tests. We chose this conﬁguration as default empirically as it
performs the best on the pass@1@1 metric on the MBPP dataset.

Finally, to ensure complete automation for our evaluation and avoid the need to inspect the impact
of incompleteness of test suite Tp on pass@k@m, we ensure that the pass@k@m increases mono-
tonically despite the incompleteness of test cases as outlined in Section 2.4. We enforce that a code
suggestion is never removed from G if it satisﬁes all the hidden tests, even if it is non-equivalent to
the reference solution.

11

Figure 5: Results of the accuracy experiment

4.4 RQ1

To answer RQ1, we compare the accuracies of Baseline, IdealTests, IdealRanking and TICODER
(default option) restricted to the case of a single user query. We use the pass@k@m metric, where
k ∈ {1, 2, 5, 10} and m = 1. We also compare the tools with Codex without any user interaction
(m = 0). Additionally, to get the best possible result from Codex, we also show Codext=0, where
we query Codex for 1 suggestion with temperature 0. We do not show results for Codext=0 for
k ≥ 1, since we query for only one suggestion.

From Figure 5, we observe that TICODER has a pass@1@1 of 70.49%, outperforming Baseline
(48.61%), Codex (48.39%), and Codext=0 (61.36%). TICODER continues outperforming Baseline
and Codex for the remaining values of k. However, it consistently falls short of IdealTests, trailing
it by 11.52 percent points for pass@1@1 and 6.48 percent points for pass@10@1. This exhibits
the potential improvement that can be made by further exploring various components of TICODER.

Note that the performance of IdealTests and IdealRanking are comparable, but IdealRanking is al-
ways slightly lower (as expected). This shows that our test generation strategies that include tests
generated by Codex along with the static/dynamic mutations can for the most part generate tests that
can capture user intent; coupled with an ideal ranking policy, the performance of TICODER can be
close to optimal. However, such an ideal ranking policy is not realizable since it relies on the hidden
reference implementation and tests.

4.5 RQ2

To answer RQ2, we evaluate the four conﬁgurations over both the stopping criteria: (1) by limiting
the maximum number of user queries and (2) by stopping when the ﬁrst test is accepted by the
user. We show the results of stopping criteria (1) in Figure 6. In all cases, increasing the limit of
the maximum number of queries increases the performance. However, this increase is very slight
for the baseline, while it is substantial for TICODER and IdealTests. Note that IdealTests achieves
the highest possible performance matching the pass@100 value (as expected), while IdealRanking
closely follows it. This plot further reinforces the observations from RQ1; while TICODER achieves
comparable performance with IdealTests and IdealRanking for pass@1@5, it is 15.16 percent points
short of IdealRanking for pass@1@1, and IdealRanking with at most one user query does better
than TICODER with at most ﬁve.
Improvements to the ranking policy can result in substantial
improvements in performance for TICODER and can require fewer user interactions.

12

12510n020406080pass@n(%)pass@100=89.46%CodexCodex(temp=0)BaselineTiCoderIdealRankingIdealTestsFigure 6: Results of user interaction experiments

Tool
TICODER

pass@1@*
75.87

pass@2@*
79.85

pass@5@*
83.37

Table 2: Results of the user interaction studies for stopping criteria (2). Here, the maximum possible
number of user interactions is capped at 10.

Figure 7: Plot showing fraction of examples (on y-axis) with NumQueriesToAccept ≤ m for
different number of user queries m (on x-axis) for stopping criteria 2. The number of possible user
queries was capped at 10.

13

012345k5060708090pass@1@k48.39%49.48%49.85%50.14%50.52%51.36%70.49%76.11%78.92%83.14%85.48%86.5%89.25%89.46%85.65%87.69%87.91%87.97%87.97%pass@100=89.46%BaselineTiCoderIdealTestsIdealRanking246810Numberofuserinteractions657075808590Numberofexamples63.47%76.11%81.73%85.48%86.89%88.29%89.23%89.46%90.40%90.40%Tool
TICODER
- code prompt
- single assert
- dyn. mutation
- test ranking
- code ranking

pass@1@1
70.49
69.09
70.27
68.85
62.06
67.52

pass@2@1
74.47
75.17
75.33
72.13
65.10
75.00

pass@5@1
79.39
80.09
79.39
78.45
72.36
81.39

pass@1@2
76.11
77.28
76.01
75.35
64.87
74.44

pass@1@5
85.48
83.61
80.85
81.84
71.89
82.88

Table 3: Results of the ablation studies

The results of stopping criteria (2) are shown in Table 2. For this stopping criteria, the pass@1@*
value is 75.87%, more than pass@1@1 for stopping criteria 1, but less than pass@1@2. On aver-
age, the user performs an average of 1.69 interactions with TICODER, with 1 being the minimum,
and 9 being the maximum number of interactions. While a larger threshold number of user interac-
tions can result in better performance, it can also lead to overhead for a user.

Figure 7 shows the cumulative fraction of examples that produced an accepted test within m user
queries, i.e. NumQueriesToAccept ≤ m. We observe that TICODER is able to propose a test that is
consistent with the user intent for 90.40% of examples within 10 queries, whereas the ﬁrst query pro-
vides such a consistent test for 63.47% of examples. We notice that beyond 5 user interactions, the
improvements to performance become marginal as compared to the improvements seen for less than
5 interactions. The results demonstrate signiﬁcant room for improvement in test ranking strategies
that will affect the overall performance of TICODER.

4.6 RQ3

In order to examine the effects of various TICODER components, we conduct ablation studies
for each component individually. Speciﬁcally, we evaluate the performance of TICODER with
pass@k@1 for k ∈ {1, 2, 5}, as well as pass@1@m for m ∈ {2, 5}. We consider the follow-
ing ablations:

• TICODER: This is the default conﬁguration as described in subsection 4.3, chosen due to

its performance on the pass@1@1 metric.
• Code Prompt: TestGenPrompt = choose(G).
• Single Assert: StaticMutateTests = none.
• Dynamic Mutation: DynMutateTests = none.
• Test Ranking: TestRanking = random.
• Code Ranking: CodeRanking = none.

For each metric, the cell corresponding to the highest performing conﬁguration is marked in bold.
Each conﬁguration contributes differently to the evaluation, as we can see from the different metrics.
The default conﬁguration was chosen to be the conﬁguration that performed best on the pass@1@1
metric. We prioritize the pass@1@1 metric since it is the most practical metric for code generation
in an interactive setting, since it relies on at most one user interaction and produces only one code
suggestion as the ﬁnal output. Note that the default conﬁguration also performs best on pass@1@5.

Some components that have a signiﬁcant impact on the performance over all metrics include the test
and code ranking components. Presenting the user with randomly picked tests from the set of test
suggestions, rather than the top-ranked test, performs uniformly worse than the default conﬁguration.
This indicates the importance of the test-ranking policy as a component. Another component that is
uniformly helpful for the default conﬁguration is dynamic test mutation; removing it results in a dip
in performance (e.g., 3.5% for pass@1@5), though it is a smaller dip than removing test ranking.

Code ranking is clearly useful in the default conﬁguration for the pass@1@m metrics, but performs
worse for the pass@k@1 metrics for k ≥ 2. In other words, if we sample 1 suggestion from the
code suggestions, it is beneﬁcial to select the top-ranked code suggestion. However, if we sample
more, ranking the code suggestions can decrease performance, since the ranking policy may result
in cases where the correct suggestion is never sampled. We believe other heuristics for code ranking
(described in Section 3.5) may help improve these numbers uniformly.

14

The ablations also demonstrate that other components have non-trivial effect on the evaluation met-
rics. For example, disabling the static test mutation heuristic improves pass@2@1, while using
the code suggestions in the test generation prompt improves performance on pass@1@2. How-
ever, these conﬁgurations were not chosen to be the default since they all perform worse on the
pass@1@1 metric.

5 RELATED WORK

Our work aims to improve the trust in code generated through large language models. Recent years
have seen steady progress in the space of training larger and more powerful pre-trained language
models for code generation, with sizes up to about 12 billion (Chen et al., 2021). These language
models are variants of the decoder-only generational model such as GPT-3 (Brown et al., 2020).
OpenAI’s Codex (Chen et al., 2021) ﬁne-tunes GPT-3 on source code from GitHub. Google’s large
language model (Austin et al., 2021) trains a model and introduced the crowdsourced MBPP bench-
mark for evaluation. Saleforce’s CodeGen (Nijkamp et al., 2022) ﬁne-tunes a transformer decoder
model to optimize for scenarios where a user decomposes the larger coding task into smaller sub-
tasks. Facebook’s InCoder (Fried et al., 2022) can not only complete code, but also edit code.
Although these language models achieve impressive accuracy on benchmarks such as MBPP and
HumanEval, the generated code has no veriﬁable guarantees. TDUIF can augment any of these lan-
guage models in an interactive setting to obtain tests that can serve as explanation of code generated
as well as help formalize user intent, and also helps users to prune the space of incorrect suggestions.

Both AlphaCode (Li et al., 2022) and CodeT (Chen et al., 2022) exploit generated tests from models
to improve the quality of code generation. Both these approaches generate tests using LLMs (Alpha-
Code trains a new test-generation model) and then groups code suggestions by the set of tests they
satisfy. When suggesting code suggestions, only a single suggestion from each group is reported.
CodeT (Chen et al., 2022) reﬁnes the approach by scoring tests and code suggestions simultaneously
by prioritizing tests that satisfy many code suggestions, and prioritizing codes that satisfy many tests.
Unlike TDUIF, these approaches still target the same metric as any code generation models (namely,
pass@k) and do not account for user interaction or provide any guarantees on suggested code. On
the other hand, our test and code ranking components can beneﬁt from the algorithms in CodeT —
we leave it as future work.

Scalable test generation for software has a rich history, and a comprehensive coverage is outside
the scope of this work. The dominating approaches for real-world code are based on variants of
feedback-driven random testing (Pacheco et al., 2007) or on genetic programming (Fraser & Arcuri,
2011). These non-neural approaches presume the method under test is present and derive high-
coverage tests by executing the method under test. These approaches are optimized for maximizing
code coverage and ﬁnding runtime crashes. The non-neural approaches are not directly applicable
in TDUIF scenario for two primary reasons (a) we do not start with an implementation of method
under test but instead have a set of candidate implementations, and (b) it is critical to generate test
oracles (or expected output) without access to the method deﬁnition. Neural approaches have shown
promise recently in either addressing the test oracle (Tufano et al., 2022; Dinella et al., 2022) or
generating an entire test (Tufano et al., 2020), Of these, TOGA (Dinella et al., 2022) attempts to
generate the test oracle for a test preﬁx (or expected output for a given input) without relying on a
method implementation. We expect to harness both these approaches to generate the seed tests that
can be further mutated and ranked using suitable extensions to the algorithms presented in this work.

Finally, work on program synthesis (Gulwani et al., 2017; Solar-Lezama, 2009) generates code
that satisﬁes a formal speciﬁcation either expressed as a logical speciﬁcation or input-output exam-
ples (Gulwani, 2011). Unlike program synthesis, LLM generates code from informal speciﬁcations
(our setup) and evaluated through hidden tests or speciﬁcations. However, it would be interesting
future work to leverage user-provided tests to improve the quality of code generation, as explored
in recent works (Jain et al., 2022; Rahmani et al., 2021). Finally, the work on interactive program
synthesis (Le et al., 2017; Ji et al., 2020) has similarity with our work in querying users about
possible constraints on the code, in addition to initial speciﬁcation. However, work in this space
has been mostly been performed for restricted domains and does not directly apply to general code
generation.

15

6 CONCLUSIONS

In this paper, we have articulated the problem of test-driven user-intent formalization (or test-driven
user-intent discovery) to create an interactive framework to (a) reﬁne and formalize the user intent
through generated tests, and (b) generate code that is consistent with such tests. We proposed an
interactive workﬂow to address this problem and formalized an algorithm with well-deﬁned compo-
nents that can be instantiated in various manners to solve the given problem. Finally, we presented
our approach for tackling this problem, that we called TICODER. We performed a quantitative eval-
uation on TICODER and we showed that our best algorithm improves the pass@1 code generation
accuracy metric from 48.39% to 70.49% with a single user feedback, and up to 85.48% with up to
5 user feedback. TICODER also generates a functional test consistent with the user intent within an
average of 1.69 user queries for 90.40% of the examples in the sanitized MBPP dataset. Finally, we
conducted an ablation study to highlight the importance of our algorithm’s different components.

We leave a more thorough investigation of our approach on (a) the HumanEval dataset (Chen et al.,
2021) and (b) real-world benchmarks possibly collected from open-source projects on GitHub.
While Codex (Chen et al., 2021) has been shown to be one of the most effective LLM for code
generation, we would like to leverage and possibly ﬁne-tune neural models in the future for both
code and test generation, including models such as INCODER (Fried et al., 2022), CODEGEN (Ni-
jkamp et al., 2022), POLYCODER (Xu et al., 2022). We also leave as future work a user study of
TICODER in place of the simulated quantitative evaluation to further explore the usefulness of our
approach. Finally, we use tests as an instance of a partial speciﬁcation of intent for a function; we
believe one can easily extend the framework to richer forms of formal speciﬁcations (e.g., procedure
summaries) provided we have a scalable generator and checker of likely speciﬁcations.

Acknowledgements. We are grateful to Todd Mytkowicz for initial discussions on the idea of sim-
ulating user response and initial implementation of the dynamic mutation algorithm, and Johannes
Gehrke, Mark Encarnaci´on, Andres Codas, Mei Yang and Rahee Ghosh Peshawaria for fruitful
discussions and feedback on this work.

REFERENCES

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large
language models, 2021. URL https://arxiv.org/abs/2108.07732.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL
https://arxiv.org/abs/2005.14165.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu
Chen. Codet: Code generation with generated tests, 2022. URL https://arxiv.org/abs/
2207.10397.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-
Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large
language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.

16

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,
2022. URL https://arxiv.org/abs/2204.02311.

Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz,

A neural method for
URL
toga-a-neural-method-for-test-oracle-generation/.

Toga:
In ICSE 2022. ACM, May 2022.
https://www.microsoft.com/en-us/research/publication/

test oracle generation.

and Shuvendu Lahiri.

Gordon Fraser and Andrea Arcuri. Evolutionary generation of whole test suites. In International
Conference On Quality Software (QSIC), pp. 31–40, Los Alamitos, CA, USA, 2011. IEEE Com-
puter Society. doi: http://doi.ieeecomputersociety.org/10.1109/QSIC.2011.19.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis, 2022. URL https://arxiv.org/abs/2204.05999.

GitHub. Github copilot, 2022. Accessed August 5, 2022. https://github.com/features/

copilot/.

Sumit Gulwani.

amples.
URL
automating-string-processing-spreadsheets-using-input-output-examples/.

Automating string processing in spreadsheets using input-output ex-
January 2011.
https://www.microsoft.com/en-us/research/publication/

January 26-28, 2011, Austin, Texas, USA,

In PoPL’11,

Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Found. Trends Program.
Lang., 4(1-2):1–119, 2017. doi: 10.1561/2500000010. URL https://doi.org/10.1561/
2500000010.

Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy,
Large language models meet pro-
In International Conference on Software Engineering (ICSE), May
URL https://www.microsoft.com/en-us/research/publication/

Sriram Rajamani, and Rahul Sharma.
gram synthesis.
2022.
jigsaw-large-language-models-meet-program-synthesis/.

Jigsaw:

Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. Question selection for interac-
tive program synthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI 2020, pp. 1143–1158, New York, NY, USA, 2020.
Association for Computing Machinery. ISBN 9781450376136. doi: 10.1145/3385412.3386025.
URL https://doi.org/10.1145/3385412.3386025.

Vu Le, Daniel Perelman, Oleksandr Polozov, Mohammad Raza, Abhishek Udupa, and Sumit Gul-
wani. Interactive program synthesis, 2017. URL https://arxiv.org/abs/1703.03539.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven
Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Push-
meet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code
generation with alphacode, 2022. URL https://arxiv.org/abs/2203.07814.

17

Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. An empirical analysis of ﬂaky
tests. In Shing-Chi Cheung, Alessandro Orso, and Margaret-Anne D. Storey (eds.), Proceedings
of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,
(FSE-22), Hong Kong, China, November 16 - 22, 2014, pp. 643–653. ACM, 2014. doi: 10.1145/
2635868.2635920. URL https://doi.org/10.1145/2635868.2635920.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. A conversational paradigm for program synthesis, 2022. URL https:
//arxiv.org/abs/2203.13474.

Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. Feedback-directed ran-
dom test generation. In ICSE 2007, Proceedings of the 29th International Conference on Software
Engineering, pp. 75–84, Minneapolis, MN, USA, May 2007.

Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gus-
tavo Soares, and Ashish Tiwari. Multi-modal program inference: a marriage of pre-trained lan-
guage models and component-based synthesis. Proc. ACM Program. Lang., 5(OOPSLA):1–29,
2021. doi: 10.1145/3485535. URL https://doi.org/10.1145/3485535.

Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the

few-shot paradigm, 2021. URL https://arxiv.org/abs/2102.07350.

Armando Solar-Lezama. The sketching approach to program synthesis.

In Zhenjiang Hu (ed.),
Programming Languages and Systems, pp. 4–13, Berlin, Heidelberg, 2009. Springer Berlin Hei-
delberg. ISBN 978-3-642-10672-9.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit
test case generation with transformers and focal context, 2020. URL https://arxiv.org/
abs/2009.05617.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan. Generating accurate
assert statements for unit test cases using pretrained transformers. In IEEE/ACM International
Conference on Automation of Software Test, AST@ICSE 2022, Pittsburgh, PA, USA, May 21-22,
2022, pp. 54–64. ACM/IEEE, 2022. doi: 10.1145/3524481.3527220. URL https://doi.
org/10.1145/3524481.3527220.

Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation vs. experience: Evaluating
the usability of code generation tools powered by large language models. In Extended Abstracts
of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA ’22, New York,
NY, USA, 2022. Association for Computing Machinery. ISBN 9781450391566. doi: 10.1145/
3491101.3519665. URL https://doi.org/10.1145/3491101.3519665.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Inter-
national Conference on Neural Information Processing Systems, NIPS’17, pp. 6000–6010, Red
Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evalua-
tion of large language models of code. In Proceedings of the 6th ACM SIGPLAN International
Symposium on Machine Programming, MAPS 2022, pp. 1–10, New York, NY, USA, 2022. Asso-
ciation for Computing Machinery. ISBN 9781450392730. doi: 10.1145/3520312.3534862. URL
https://doi.org/10.1145/3520312.3534862.

Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister,
Ganesh Sittampalam, and Edward Aftandilian. Productivity assessment of neural code comple-
tion. In Swarat Chaudhuri and Charles Sutton (eds.), MAPS@PLDI 2022: 6th ACM SIGPLAN
International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022, pp.
21–29. ACM, 2022. doi: 10.1145/3520312.3534864. URL https://doi.org/10.1145/
3520312.3534864.

18

