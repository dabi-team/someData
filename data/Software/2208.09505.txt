2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

1
v
5
0
5
9
0
.
8
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

1

Metamorphic Testing for Web System Security

Nazanin Bayati Chaleshtari, Fabrizio Pastore, Member, IEEE, Arda Goknil,
and Lionel C. Briand, Fellow, IEEE

Abstract—Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this
often entails the veriﬁcation of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus
required to lower costs and increase the effectiveness of security testing.
Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle
problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response
to be received after a speciﬁc HTTP GET request).
In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates
test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers
to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the speciﬁcation of such
MRs, we provide a domain-speciﬁc language accompanied by an Eclipse editor. MST-wi automatically collects the input data and
transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect
vulnerabilities based on the relations and collected data.
We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security
testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of
vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE
CWE database. We also deﬁne guidelines that enable test engineers to improve the testability of the system under test with respect to
our approach.
We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically
detected 85% of their vulnerabilities and showed a high speciﬁcity (99.81% of the generated inputs do not lead to a false positive); our
ﬁndings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling
automated security testing overnight.

Index Terms—System Security Testing, Metamorphic Testing, Domain-speciﬁc Languages.

(cid:70)

1 INTRODUCTION
Web systems (i.e., software systems providing services
through Web pages consisting of HTML, Javascript, and
CSS) are one of the main means to deliver online services,
from e-commerce to online banking. These systems are
business-critical, manage critical assets (e.g., card transac-
tions), and often store sensitive information (e.g., customer
data). Therefore, in Web systems, discovering security vul-
nerabilities (i.e., faults preventing the system from fulﬁlling
its security requirements) is essential [1], [2], [3], [4], [5]. This
is the objective of security testing.

Unfortunately, it is hard to discover vulnerabilities in
Web systems during testing. Indeed, Web systems typically
consist of several input interfaces (i.e., Web pages provided
through URL requests). Each interface handles a large set
of inputs (e.g., Web forms, cookies, URL parameters) that
might be conﬁgured differently according to user roles.
Considering that vulnerabilities might result from speciﬁc
combinations of user roles, URL requests, and parameters,
it is necessary to exercise the system with a large set of

• N.B. Chaleshtari is afﬁliated with University of Ottawa, Canada. F.
Pastore is with the SnT Centre for Security, Reliability and Trust,
University of Luxembourg, Luxembourg. A. Goknil is afﬁliated with
SINTEF Digital, Norway. L.C. Briand is afﬁliated with both University
of Luxembourg and University of Ottawa.
E-mail:
arda.goknil@sintef.no lionel.briand@uni.lu lbriand@uottawa.ca

fabrizio.pastore@uni.lu

n.bayati@uottawa.ca

Manuscript received August XX, 2022; revised Month XX, 2022.

inputs, including inputs crafted to harm the system. There-
fore, strategies to automatically generate security test inputs are
necessary.

However, automatically generating inputs is insufﬁcient;
indeed, an automated test oracle (i.e., a mechanism for deter-
mining whether a test case has passed or failed) is needed.
Security testing is known to suffer from the oracle problem [6],
[7], [8]. It is indeed generally infeasible to automatically
determine the expected output for a given input or even
manually specify expected outputs for a large number of test
inputs. For instance, a security test case for a bypass autho-
rization schema vulnerability should verify, for every user
role, whether it is possible to access resources that should
be available only to a user having a different role [9]. We
can discover this vulnerability by verifying access to various
resources with different privileges and roles; however, to
determine the test outcome, we need a mapping between
roles and resources, which is not always available because of
the complexity of the operations provided by modern Web
systems. Recent incidents involving corporate Web sites,
such as Facebook’s, demonstrate that it is difﬁcult to verify,
at testing time, large sets of input sequences, including the
ones that trigger vulnerabilities [10], [11], [12], [13].

Although several security testing approaches exist in
the literature, they do not address the oracle problem and
assume the availability of an implicit test oracle [6]. Further-
more, most of them focus on a particular vulnerability type
(e.g., buffer overﬂows [14], [15]) and only uncover vulnera-

 
 
 
 
 
 
2

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

bilities that prevent a system from providing outputs (e.g.,
system crashes because of buffer overﬂows).

Metamorphic Testing (MT) is a testing technique that has
shown, in some contexts, to be very effective in alleviating
the oracle problem [16], [17]. MT is based on the idea that
it may be simpler to reason about relations between outputs
of multiple test executions, called metamorphic relations (MRs),
than to specify the system’s input-output behavior [18]. In MT,
system properties are captured as MRs that are used to
automatically transform an initial set of test inputs into
follow-up test inputs. If the system outputs for the initial
and follow-up test inputs violate the corresponding MR, it
is concluded that the system is faulty.

Considerable research has been devoted to developing
MT approaches for application domains such as computer
graphics (e.g., [19], [20], [21], [22]), Web services (e.g., [23],
[24], [25]), and embedded systems (e.g., [26], [27], [28],
[29]). Unfortunately, only a few approaches target security
aspects [30]; also, their applicability is limited to the func-
tional testing of security components (e.g., code obfusca-
tors [30]) or the veriﬁcation of speciﬁc security bugs (e.g.,
heartbleed [31]). They do not support the speciﬁcation of
general security properties by using MRs. Although MT
is automatable, few MT approaches provide proper tool
support [18].

Our goal in this paper is to use MT to both automatically
generate test inputs for security testing and address the test
oracle problem in security testing. We propose Metamorphic
Security Testing for Web-interactions (MST-wi), a technique
that supports engineers in specifying MRs to capture se-
curity properties of Web systems and automatically detect
vulnerabilities (i.e., violations of security properties) based
on those relations.

MST-wi automatically generates test inputs by altering
valid inputs as an attacker would do; the strategies adopted
to modify valid inputs are encoded in MRs. It automatically
collects valid inputs by relying on a Web crawler or by
reusing the scripts implemented by engineers for functional
testing. For example, an MR to spot bypass authorization
schema vulnerabilities should ensure that a Web system
returns different responses to two users when the ﬁrst user (User-
A) requests a URL that is provided to her by the GUI (e.g., in
HTML links) while the second user (User-B) requests the same
URL, which is not provided to her by the GUI. With MST-wi,
it is possible to deﬁne an executable MR that accesses, for
User-B, all the URLs that can be reached by the GUI for User-
A but not with the GUI for User-B. The MR veriﬁes that the
output returned to User-B is different from the one for User-
A; otherwise, a vulnerability is reported. If the output for
the two users is the same, User-B can access the same page
accessed by User-A instead of receiving an error message,
which is not the functionality exposed by the user interface.
MST-wi is built on top of the following novel solutions:

•

Security Metamorphic Relation Language (SMRL), a
Domain-Speciﬁc Language (DSL) for specifying MRs
for software security testing. SMRL is supported by
a custom editor, implemented as a plug-in for the
Eclipse IDE [32], which facilitates the speciﬁcation of
MRs.

• A catalog of system-agnostic MRs targeting 102 se-

curity vulnerabilities of Web systems.

• A data collection framework that automatically col-

lects the data required to perform MT.

• A testing framework that automatically performs
security testing based on the MRs and the collected
data.

Our DSL supports data representation functions and
boolean operators to specify security properties in MRs.
It also provides a set of utility functions to express data
properties that cannot be described with simple boolean or
arithmetic operators. MST-wi automatically transforms MRs
written in our DSL into executable Java code. It extends
the Crawljax Web crawler [33] to derive source inputs
automatically from the system under test. It provides an
MT algorithm integrated into the JUnit framework [34] as
part of the testing framework. The algorithm performs MT
based on the executable MRs in Java and the source inputs
collected by the data collection framework.

We evaluated our approach through the following anal-

yses:

• The capability of MST-wi to automate (including
oracles) the security testing activities suggested by
OWASP1. Our results show that MST-wi automates
39% of the security testing activities not automated
by other approaches relying on catalogs or implicit
oracles (e.g., crashes) and addressing speciﬁc vulner-
ability types.

• The applicability of MST-wi to discover common
and important security vulnerability types described
in the Common Weaknesses Enumeration Repos-
itory [36]. Our results show that MST-wi enables
testing for 101 (45%) vulnerability types due to errors
in applying security design principles.

• A study of testability guidelines that enable engi-
neers to improve the testability of Web systems with
MST-wi. We observe that controllability and an ade-
quate test support environment are key for applying
MST-wi.

• An analysis of the effectiveness of MST-wi when ap-
plied to discover vulnerabilities in Jenkins, a leading
open source automation server [37], and Joomla, a
content management system [38]. MST-wi has shown
to be largely effective; indeed, it automatically de-
tected 85% of the targeted vulnerabilities affecting
these two systems. Further, only a negligible fraction
of follow-up test inputs leads to false alarms (0.19%
max).

• An analysis of the scalability of MST-wi while test-
ing Jenkins and Joomla. Our results show that, for
most vulnerability types, MST-wi can be applied
overnight; further, technical improvements in our
framework implementation may enable overnight
execution for all the MRs in our catalog.

Our toolset and empirical data are publicly avail-

able [39], [40].

1. OWASP (Open Web Application Security Project) is one of the best

known organizations that focus on software security [35].

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

3

This paper largely extends our previous conference pa-
per [41] published at the 13th IEEE International Conference
on Software Testing, Veriﬁcation and Validation (ICST’20).
An earlier version of our tool was demonstrated [42] at
the 42nd International Conference on Software Engineering
(ICSE’20). This paper brings together, reﬁnes, and signiﬁ-
cantly extends the ideas from the above papers:

• We extend our DSL with additional data representa-

tion and utility functions.

• We extend our MR catalog with 54 new MRs.
• We present the results of an extensive study on
the types of security vulnerabilities that can be ad-
dressed by our approach. Precisely, we study the
security weaknesses organized in the CWE view
for common security architectural tactics [43], the
ones belonging to the CWE Top-25 most dangerous
software errors [44], and the ones in the OWASP Top-
10 Web security risks [45].

• We provide testability guidelines that assist engi-
neers in designing and implementing their software
to enable effective test automation with MST-wi.
• We provide substantial new empirical evidence to
demonstrate the effectiveness and scalability MST-wi
by applying all the MRs in our catalog to Jenkins and
Joomla (the latter being entirely new). Our results
include the discovery of a new security vulnerability
in Jenkins that received a CVE identiﬁer [46].

This paper is structured as follows. Section 2 provides
the background information regarding MT. In Section 3, we
present an overview of the approach. Sections 4 to 7 describe
the core technical solutions. Section 8 describes our catalog
of MRs. In Section 9, we investigate the security vulnerabil-
ity types that MST-wi can address and the testability guide-
lines for MST-wi to address these vulnerabilities. Section 10
reports on the results of the empirical validation conducted
with two open-source case studies. Section 11 discusses the
related work. We conclude the paper in Section 12.

2 BACKGROUND: METAMORPHIC TESTING

In this section, we present the basic concepts of MT. The
core of MT is a set of MRs, which are necessary properties
of the program under test in relation to multiple inputs
and their expected outputs [47]. MRs resemble the traditional
concept of program invariants, which are properties that hold at
certain points in programs. However, the key difference is that
an invariant has to hold for every possible program execution,
whereas a metamorphic relation captures a property of inputs and
outputs belonging to different executions [18].

In MT, a single test case run requires multiple executions
of the system under test with distinct inputs. The test
outcome (pass or fail) results from the veriﬁcation of the
outputs of different executions against the MR.

As an example, let us consider an algorithm f that
computes the shortest path for an undirected graph G. For
any two nodes a and b in the graph G, it may not be
practically feasible to generate all possible paths from a to b,
and then check whether the output path is really the shortest
path. However, a property of the shortest path algorithm is
that the length of the shortest path will remain unchanged

if the nodes a and b are swapped. Using this property, we
f (G, a, b)
can derive an MR, i.e.,
, in which
|
|
|
we need two executions of the function under test, one with
(G, a, b) and another one with (G, b, a). The results of the
two executions are veriﬁed against the relation. If there is a
violation of the relation, then f is faulty.

f (G, b, a)

=

|

We provide below basic deﬁnitions underpinning MT.
Deﬁnition 1 (Metamorphic Relation - MR). Let f be a
function under test. A function f typically processes a
set of arguments; we use the term input to refer to the
set of arguments processed by the function under test. In
our example, one possible input is (G, a, b). The function
f produces an output. An MR is a condition that holds
2, and their
x1, ..., xn
for any set of inputs
(cid:104)
(cid:105)
f (x1), ..., f (xn)
corresponding outputs
. MRs are typically
(cid:105)
(cid:104)
expressed as implications.

where n

≥

In our example, the property of the target algorithm f is
“the length of the shortest path will remain the same if the
start and end nodes are swapped”. The MR of this property
is (x1 = (G, a, b)) ∧ (x2 = (G, b, a)) → |f (x1)| = |f (x2)|.

Deﬁnition 2 (Source Input and Follow-up Input). An MR
implicitly deﬁnes how to generate a follow-up input from a
source input. A source input is an input in the domain of f .
A follow-up input is a different input that satisﬁes the prop-
erties expressed by the MR. In our example, (G, a, b) and
(G, b, a) are the source and follow-up inputs, respectively.

Follow-up inputs can be deﬁned by applying transforma-
tion functions to the source inputs. The use of transformation
functions in MRs simpliﬁes the identiﬁcation of follow-up in-
puts. In our example, a transformation function that swaps
the last two arguments of the source input can be used to
deﬁne the follow-up input:
x1 = (G, a, b) ∧ x2 = swapLastArguments(x1) → |f (x1)| = |f (x2)|
where swapLastArguments is the transformation function.

Deﬁnition 3 (Metamorphic Testing - MT). MT consists of

the following ﬁve steps:

1 Generate one source input (or more if required). In
our example, a (random) graph G is generated; two
vertices a and b in G are randomly selected for the
source input.

2 Derive follow-up inputs based on the MR. In our
example, the function swapLastArguments is applied
to (G, a, b).

3 Execute the function under test with the source and
follow-up inputs to obtain their respective outputs.
In our example, the shortest path function is exe-
cuted twice with (G, a, b) and (G, b, a).

4 Check whether the results violate the MR. If the MR
is violated, then the function under test is faulty.
5 Restart from (1), up to a predeﬁned number of itera-

tions.

3 OVERVIEW OF THE APPROACH
The process in Fig. 1 presents an overview of MST-wi. In
Step 1, the engineer selects, from a catalog of predeﬁned
MRs, the relations for the system under test. In general,
we expect the engineer to choose the whole set of MRs in
our catalog. In addition, the engineer can also specify new
relations by using our DSL. Step 1 is manual.

4

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

In Step 2, MST-wi automatically transforms the selected

MRs into executable Java code.

Fig. 1. Overview of the approach.

In Step 3, the engineer executes a Web crawler to auto-
matically collect information about the Web system under
test (hereafter, SUT). The crawler reveals the SUT structure
(e.g., URLs that can be visited by an anonymous user) and
the actions that trigger the generation of new content on
a page (e.g., clicking on an anchor or a textual label). The
collected data is used to derive source inputs for MT. The
engineer can process manually implemented test scripts, if
available, to obtain additional information. Step 3 does not
depend on other steps.

In Step 4, MST-wi automatically loads source inputs and
generates follow-up inputs as described by the relations.
After executing the source and follow-up inputs, the outputs
are checked according to the MRs.

Our DSL and the data collection framework can be
extended to support new language constructs and data
collection methods. The MT framework can also be im-
proved to deal with input interfaces not supported yet (e.g.,
Silverlight plug-ins [48]) and to load data collected by new
data collection methods.

Sections 4 to 7 explain the details of each step in Fig. 1,
with a focus on how we achieved our automation objectives.

catalog covers generic vulnerabilities that may affect any
Web system, we expect all the MRs to be chosen in most
cases.

In our implementation, the catalog of MRs is released
as an Eclipse project folder, including our MRs. The project
can be copied and reconﬁgured for every SUT. The selection
of MRs is performed within the Eclipse environment by
deﬁning a JUnit test suite containing the MRs to test (see
Section 7).

Since the selection of MRs is straightforward, we focus
on the SMRL DSL in the following paragraphs. We intro-
duce the SMRL grammar, the boolean operators, the data
representation functions, and the Web-utility functions.

4.1 SMRL Grammar

SMRL is an extension of Xbase [49], an expression language
provided by Xtext [50]. Xbase speciﬁcations can be trans-
lated to Java programs and compiled into executable Java
bytecode.

We rely on Xbase since DSLs extending Xbase inherit
the syntax of a Java-like expression language and language
infrastructure components, including a parser, a linker, a
compiler, and an interpreter [49]. Further, it provides fea-
tures common in modern programming languages, includ-
ing lambda expressions, type inference, and simple operator
overloading. These features will facilitate the adoption of
SMRL.

SMRL extends Xbase by introducing (i) data represen-
tation functions, (ii) boolean operators to specify security
properties, and (iii) Web-utility functions to express data
properties and transform data. We can extend these func-
tions by deﬁning new Java APIs invoked in MRs.

Fig. 2 presents an MR written in our SMRL editor. The
relation checks whether other users can access the URLs
dedicated to speciﬁc users through a direct request. We use
it as a running example in the rest of the paper.

The SMRL grammar extends the Xbase grammar, which
extends the Java grammar. Each SMRL speciﬁcation can rely
on external classes and APIs; in practice, it can have have an
arbitrary number of import declarations indicating the APIs
used in MRs (Line 1 in Fig. 2).

A package declaration resembles the Java package struc-
ture and can contain one or more MRs. Line 4 in Fig. 2
declares the package smrl.mr.owasp, which is the package
for our MRs automating the testing activities in the OWASP
testing guidelines [9]. Like in Java, MRs deﬁned in different
SMRL speciﬁcation ﬁles can belong to the same package.

An MR can contain an arbitrary number of XBlock-
Expressions, which are nonterminal symbols deﬁned in
the Xbase grammar. An XBlockExpression can have loops,
function calls, operators, and other XBlockExpressions.

4 STEP 1: SELECT AND SPECIFY MRS

Step 1 in Fig. 1 concerns selecting and specifying MRs.
To enable specifying new MRs, we provide a DSL called
Security Metamorphic Relation Language (SMRL).

The most common usage scenario for MST-wi is the
selection of MRs for the SUT from our MRs catalog (see
Section 8) without specifying additional ones. Since our

4.2 Data Representation Functions

SMRL provides 46 functions to represent different data
types used to refer to SUT inputs or outputs in MRs. At
a high level, we distinguish between four main categories
of data:

•

Interaction data characterize the interactions with the
SUT and is collected by the MST-wi Web crawler.

startSelect and Specify Metamorphic RelationsExecute the Data Collection FrameworkTransform Metamorphic Relations to Java ••••••••••••••••List of PredeﬁnedMetamorphic RelationsSystem to be TestedList ofMetamorphic Relations2Execute the Metamorphic Testing FrameworkExecutable Metamorphic RelationsSource Inputs4Test Results13S(x,y)BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

5

Description of the annotated MR statements: (1) For loop iterates over all actions of the Input. (2) Checks whether the user in User() is not a
supervisor of the user performing the current action. (3) Veriﬁes that the user cannot retrieve the URL of the action through the GUI (based on
the data collected by the crawler). (4) Deﬁnes a follow-up input that matches the source input except that the credentials of User() are used in
this case. (5) Veriﬁes that the follow-up input leads to an error page or the output generated by the action containing the URL indicated above
leads to two different outputs in the two cases.

Fig. 2. An MR for the Bypass Authorization Schema vulnerability.

Category Data function

Interaction

Input(int i)
Action(int i)
ActionAvailable-
WithoutLogin(int i)
User(int i)
ParameterValueUsedBy-
OtherUsers(Action
i)
RandomFilePath(int i)

i,

TABLE 1
Data functions in SMRL.

Description
Returns the ith input sequence.
Returns the ith input action.
Returns the ith input action that can be performed without logging into the system.

Returns the ith user of the system.
Returns, for the same action and parameter position, the ith parameter value used by a user different than the one
executing the action.

par

Returns the i-th ﬁle system path. We select paths of ﬁles in the Web system subfolder, ignoring images, and replacing
symbolic links (e.g., ‘plugins’ is mapped to ‘plugin’ in Jenkins).

SUT

MST-wi

Output

RandomAdminFilePath(int i) Returns the i-th path to conﬁguration ﬁle for the SUT.
Log(int i)
HttpMethod(int i)
SQLInjectionString(int i)
CodeInjectionString(int i)

Returns the i-th path to a log ﬁle generated by the SUT.
Returns the i-th name of an HTTP method (e.g., DELETE).
Returns the i-th attack string to be used to perform an SQL injection (e.g., ’or ’1’ = ’1).
Returns the i-th attack string to be used to perform an code injection, e.g., /%3C?php%20system(%22/bin/ls
%20-l%22);?%3E.
Returns the i-th attack string to be used to perform an XSS injection, e.g., <SCRIPT>alert(’XSS’);</SCRIPT>.
Returns the i-th attack string to be used to perform an static code injection.
Returns the i-th attack string to be used to perform an LDAP injection.
Returns the i-th attack string to be used to perform an XQuery injection.
Returns the i-th attack string to be used to perform a command injection.
Returns the i-th CRLF attack string, e.g., ‘);die(2’.
Returns the i-th weak password to be tested.
Returns the i-th special character to be tested.
Returns the path to the i-th ﬁle with invalid type to be tested.
Returns the path to the i-th XML ﬁle containing an XML injection.
Returns a random value of the given type.
Returns the sequence of outputs generated by Input i.
Returns the output generated by the nth action of Input i.

XSSInjectionString(int i)
StaticInjectionString(int i)
LDAPInjectionString(int i)
XQueryInjection(int i)
CommandInjection(int i)
CRLFAttackString(int i)
WeakPassword(int i)
SpecialCharacters(int i)
FileWithInvalidType(int i)
XMLInjectedFile(int i)
RandomValue(Type t)
Output(Input i)
Output(Input i, int n)

Note: for each data function in the table, except for the Output category, we provide a convenience method that does not include the parameter int i and simply
returns the ﬁrst data item for that type; this leads to 46 data functions.

•

SUT data is speciﬁc to the SUT and needs to be
speciﬁed by the end-user in MST-wi conﬁguration
ﬁles (e.g., the path of log ﬁles generated by the SUT).
• MST-wi data is provided with the MST framework
and does not need to be modiﬁed by the end-user
(e.g., attack vectors for SQL injection attacks).

• Output data is generated by the SUT in response to

an input (e.g., Web pages).

In SMRL, data is represented by a keyword followed
by an index number used to identify different data items.
To keep SMRL simple, we refer to data by using func-
tions (hereafter, data functions) with capitalized names (e.g.,

Input(1)). Table 1 presents the data functions in SMRL,
grouped by category. Each data function returns a data class
instance.

Fig. 3 presents the data model for Interaction and Output
data. We focus on these two categories because they include
the input and output types for the SUT. The other data
categories concern data that is represented using Strings and
used to assign values to Interaction data attributes. Input
refers to a sequence of interactions between a user and the
SUT; such interaction sequences are the only way to give
inputs to the SUT. It is consequently associated with Action,
which represents an activity performed by a user. SMRL

6

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

Fig. 3. Metamorphic data classes in SMRL.

provides two types of actions: WebAction and WaitAction.
WebAction captures a browser activity (e.g., requesting a
URL). It carries information about actions (e.g., a requested
URL and parameters in the URL query string). WaitAction
simulates time passing by changing the system time. User
represents a system user.

In SMRL, source and follow-up inputs are always in-
stances of the Input class, with the follow-up input derived
through a Web-utility function (see Section 4.4). A follow-up
input may differ from the source input because it includes
a set of actions that differ from those in the source input or
because it performs actions as a different user. In the rest of
the paper, to simplify the writing, we use the terms follow-
up action and follow-up user to indicate an Action or a User
derived by modifying an action or a user in the source input.
Instances of OutputData capture outputs generated
by the system during a system-user interaction; each in-
stance of OutputData is associated with an instance of
InteractionData.

4.3 MR Operators
SMRL provides seven operators, i.e., IMPLIES, AND, OR,
TRUE, FALSE, NOT, CREATE, and EQUAL. They enable the deﬁ-
nition of metamorphic expressions, which are boolean expres-
sions that should hold for an MR to be true. A metamorphic
expression is a speciﬁc kind of XBlockExpression in XText.
We use metamorphic expressions to decompose an MR into
simple properties. They are deﬁned in a declarative manner,
which is standard practice in MT.

The MR in Fig. 2 includes a metamorphic expression
using the operator IMPLIES. Since the expression is within a
loop body, the relation holds only if the expression evaluates
to true in all the iterations over the input actions.

The semantics of operators IMPLIES, AND, OR, TRUE,
FALSE, and NOT is straightforward. Operator CREATE de-

ﬁnes a follow-up input by creating a copy of the source
input passed as a second parameter. The follow-up input
is identiﬁed by the keyword provided as the ﬁrst parameter.
In Fig. 2, operator CREATE deﬁnes the follow-up input
Input(2) as a modiﬁed copy of Input(1). Operator CREATE
returns true if the follow-up input is successfully created.

Depending on the context, operator EQUAL either
two arguments or deﬁnes
evaluates the equality of
a follow-up input similarly to CREATE. The construct
EQUAL(Input(2), Input(1)) enables writing an MR in a
declarative manner without caring if Input(2) should be
generated as a copy of Input(1) or if Input(2) already exists
and it is equal to Input(1). Operator EQUAL is evaluated
to false when its ﬁrst parameter refers to an input that has
already been deﬁned and used previously, in addition to
not being equal to the second parameter. Operator CREATE
in Fig. 2 can be replaced with EQUAL to obtain an equivalent
MR (i.e., an MR that passes and fails with the same source
inputs). The main difference between CREATE and EQUAL is
that operator CREATE returns false if the identiﬁer provided
for the follow-up input already exists. Based on our experi-
ence, using operator CREATE simpliﬁes the understanding
of MRs for external readers.

4.4 Web-Utility Functions

MRs for security testing often capture complex properties
of Web systems that we cannot express with simple boolean
or arithmetic operators. Therefore, SMRL provides some
functions that capture standard Web system properties and
alter Web data. Table 2 describes a portion of the 55 Web-
speciﬁc functions in SMRL [40]. Each one is provided as a
method of the SMRL API. Engineers can specify additional
functions as Java methods. The new functions can be used
in SMRL thanks to the underlying Xtext framework.

InteractionData#ID: StringInputOutputData- timestamp: longAction- URL: String- parameters: MapUser- name: String- password: StringWebOutputSequenceWebOutput- URL: String- html: String11..*+input1..*1   / +userWebAction WaitActionBAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

7

TABLE 2
Excerpt of the Web-speciﬁc functions in SMRL.

Function
changeCredentials(Input i, User
u)

copyActionTo(Input i, int from,
int to)

cannotReachThroughGUI( User
u, String URL)

isLogin(Action a)
isSupervisorOf(User a,User b)

afterLogin(Action a)
isSignup(Action a)

isError(Output page)

userCanRetrieveContent(User u,
Object out)

isResetPassword(Action action)

EncodeUrl(String url)

setChannel(String string)

setSession(Session newSession)

Description
Creates a copy of the provided input se-
quence where the credentials of the spec-
iﬁed user are used (e.g., within login ac-
tions).
Creates a new input sequence where an
action is duplicated in the speciﬁed posi-
tion and the remaining actions are shifted
by one.
Returns true if a URL cannot be reached
by the given user by exploring the user
interface of the system (e.g., by traversing
anchors).
Returns true if the action performs a login.
Returns true if ‘a’ can access the URLs of
‘b’.
Returns true if the action follows a login.
Returns true if the action registers a new
user on the system.
Returns true if the page contains an error
message.
Returns true if the output data (i.e., the
argument ‘out’) has ever been received in
response to any of the input sequences
executed by the given user during data
collection.
Returns true if the action is resetting the
password.
Returns the UTF_8 encoded version of the
requested URL.
Modiﬁes the transfer protocol according
to the string value (e.g. Http).
Sets the session cookie value to match on
the passed one.

The MR in Fig. 2 uses the Web-speciﬁc functions
cannotReachThroughGUI, isSupervisorOf, isError, and
changeCredentials. The relation indicates that the same
sequence of actions should provide different outputs when
performed by two users under a condition: the two users
cannot access one of the requested URLs by browsing the
GUI of the system. In other words, if the system does not
provide a URL to a user through its GUI, then she should
not access the URL. Also, to avoid false alarms, the user
who cannot access the URL from the GUI (indicated as
User(2) in Fig. 2) should not be a supervisor with access
to all the resources of the other user (User(1)). Finally, we
avoid source inputs that return an error message to User(1)
because it is impossible with these inputs to characterize the
output that should be observed for User(2), which may be
the same error, a different error, or an empty page.

Function cannotReachThroughGUI in Fig. 2 checks if the
URL of the current action cannot be reached from the GUI
(Line 9). Function isSupervisorOf checks if User(2) is not
a supervisor of User(1) (Line 10). Function isError returns
true based on a conﬁgurable regular expression (Line 11)
which checks if an output page contains an error message.
Function changeCredentials creates a copy of a provided
input sequence using different credentials. It is invoked to
deﬁne the follow-up input (Line 12). Data function Output
executes the sequence of actions in an input sequence (e.g.,
requests a sequence of URLs) and returns the output of the
ith action.

5 STEP 2: TRANSFORM MRS TO JAVA
In Step 2, SMRL speciﬁcations are automatically trans-
formed into Java code. To this end, we extended the Xbase

compiler (hereafter, SMRL compiler). Each MR is trans-
formed into a Java class with the relation name and package.
The generated classes extend class MR and implement its
method mr.

Method mr executes the metamorphic expressions in
the MR. It returns true if the relation holds and false
otherwise. To do so, the SMRL compiler transforms each
boolean operator into a set of nested IF conditions. For
example, for operator IMPLIES, the generated code returns
false when the ﬁrst parameter is true and the second one
is false. For the case in which the MR holds, the SMRL
compiler generates a statement that returns true at the end
of method mr.

Fig. 4 shows the Java code generated from the MR
in Fig. 2. A loop control structure is derived from the
loop instruction in the relation (Line 10). The loop body
contains the Java code generated from the metamorphic
expression using operator IMPLIES (Lines 13-28). The ﬁrst
if condition checks whether the ﬁrst parameter of op-
erator IMPLIES holds (Lines 13-15). The nested IF block
examines whether the second parameter of IMPLIES holds
(Line 21). If the expression does not hold, mr returns false
(Line 24). The relation holds only if all the expressions in
the loop hold. Therefore, the SMRL compiler generates a
return true statement after the loop body (Line 25). Calls
to the methods ifThenBlock and expressionPass erase
the generated follow-up inputs at each iteration and keep
track of the last output observed. Function ifThenBlock
determines if we are within the ﬁrst ifThenBlock of the MR
(it happens when ifThenBlock is invoked before the ﬁrst
metamorphic expression of the MR) and, in the afﬁrmative
case, erases the follow-up inputs generated so far. Indeed,
the ﬁrst ifThenBlock function is executed at the beginning
of each iteration. Function expressionPass empties the list
of inputs processed by the last metamorphic expression.
Function Output tracks these inputs to provide engineers
with contextual information in the case of a failure.

6 STEP 3: DATA COLLECTION

In Step 3, we rely on an extended version of the Crawljax
Web crawler to automatically derive source inputs [51], [52].
Crawljax explores the user interface of a Web system (e.g.,
by requesting URLs in HTML anchors or by entering text in
HTML forms). It generates a graph whose nodes represent
the system states reached through the user interface and
whose edges capture the action performed to reach a given
state (e.g., clicking on a button). Crawljax detects the system
states based on the content of the displayed page. Our ex-
tension relies on the edit distance to distinguish the system
states [53]. We keep a cache of the HTML page associated
with each state detected by Crawljax.

When a new page is loaded, our extension computes the
edit distance between the loaded page and all the pages
associated with different system states. When the distance is
below a given threshold (5% of the page length), we assume
that two pages belong to the same state. If a page does not
belong to any state, Crawljax adds a new state to the graph.
Crawling stops when no more states are encountered, or
when a timeout is reached.

8

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

Fig. 4. Java code generated from the MR in Fig. 2.

Fig. 5. Data collection with a simpliﬁed example.

Our Crawljax extensions enable replicating and modi-
fying portions of a crawling session, which is necessary
to simulate attacks (e.g., by changing the URL accessed
within an input sequence). To this end, in addition to (i) the
Crawljax actions and (ii) the XPath of the elements targeted
by the actions (e.g., a button clicked on), our extension
records (iii) the URLs requested by the actions, (iv) the data
in the HTML forms, and (v) the background URL requests.
This additional data enables, for example, replicating mod-
iﬁed portions of crawling sessions that request URLs not
appearing on the last Web page returned by the system.
To crawl the SUT, we require only its URL and a list of
credentials.

Fig. 5 exempliﬁes the data collection steps. First, Crawl-
jax generates the graphs of the system under test. Second,
our toolset automatically derives source inputs from the
graphs. A source input is a path from the root to a leaf
of a Crawljax graph in a depth-ﬁrst traversal. Third, the
SMRL functions query the source inputs (see Section 7). For
example, Input(i) returns the ith input sequence; User(i)
returns the ith unique login credentials in the input se-
quences.

In addition to Crawljax, MST-wi processes manually im-
plemented test scripts to generate additional source inputs.
It processes scripts based on the Selenium framework [54]
and derives a source input from each script. We rely on
test scripts to exercise complex interaction sequences not
triggered by Crawljax (see Section 10). Crawljax performs an
almost exhaustive exploration of the Web interface, which is
typically not achieved by test scripts. Engineers can reuse
scripts developed for functional testing or deﬁne new ones.

7 STEP 4: EXECUTE THE MT FRAMEWORK

We automatically perform testing based on the exe-
cutable MRs in Java and the data collected by the
data collection framework (Step 4 in Fig. 1). Our test-
ing framework relies on the JUnit
to
testing environments. To
integrate MT into traditional
choose MRs to be executed, the engineer writes a JU-
nit
test case select-
ing multiple MRs (i.e., CWE 266.. OTG AUTHZ 002, and
through function test.
CWE 138.. OTG AUTHZ 001b)
MRs need to be copied into the workspace (see the ﬁles with
extension .smrl in the project explorer window in Fig. 6)

test case. Fig. 6 presents a JUnit

framework [34]

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

9

Fig. 6. An example JUnit test case to select and execute MRs.

and referred to in the JUnit test case (see JenkinsTest.java in
Fig. 6). MST-wi provides a Java class, MRBaseTest (Line
16 in Fig. 6), which extends the JUnit framework with
utility functions to facilitate the selection of MRs. Method
setUpBeforeClass is used to specify the conﬁguration
for WebOperationsProvider (Lines 21-26), i.e., the com-
ponent in charge of loading source inputs.

MRs are executed as standard JUnit test classes through
the Eclipse user interface or the console wrapper. Each MR
is treated as a distinct JUnit test case (Lines 40-48 in Fig. 6).
For each MR, class MRBaseTest automatically invokes our
MT algorithm that executes the MR.

Fig. 7 presents our MT algorithm. The algorithm takes as
input an MR and a data provider exposing the collected data
(source inputs). We ﬁrst process the bytecode of the MR to
identify the types of source inputs referenced by the relation
(e.g., Input and User). To do so, extractSourceInputTypes
(Line 2) identiﬁes the calls to the data representation functions
using the ASM static analysis framework [55]. Function
iterateOverInputTypes (Line 3) ensures that each source
input is used in at least one execution and that all possible
source input combinations are stressed during the execution
of the relation (e.g., all available URLs with all conﬁgured
users). It iterates on all available items for a given input type
(e.g., all available users). It is invoked recursively for each
input type in the MR.

Function iterateOverInputTypes is driven by the
methods exposed by the data provider (Lines 7 and 8).
The data provider works as a circular array that provides,
in each iteration of iterateOverInputTypes, a different
view of the collected data. For N input items of a given
type (e.g., User), function nextView (Line 8) generates N
different views with items shifted by one position.

The MR is executed (Line 12) after obtaining the views.
The algorithm generates follow-up inputs from the source
inputs at each invocation of operator EQUAL. For ex-
ample, operator EQUAL in Fig. 2 makes Input(2) refer
to a copy of the input sequence returned by function
changeCredentials.

Function addFailure stores the failure context infor-
mation (i.e., source inputs, follow-up inputs, and system
outputs) when the MR does not hold (Lines 13 and 14). MST-

Require: MR, the bytecode of the metamorphic relation to be executed
Require: dataProvider, an object that exposes the data collected by the crawlers
Ensure: Failures, a list of failing executions with contextual information

while dataProvider.hasMoreViews(dataTypes[i]) do

srcTypes ← extractSourceInputTypes(MR)
iterateOverInputTypes(MR, dataProvider, 0, dataTypes)
return Failures

1: function EXECUTEMETAMORPHICTESTING(MR, dataProvider)
2:
3:
4:
5: end function
6: function ITERATEOVERINPUTTYPES(MR, dataProvider, i, dataTypes)
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16:
end while
17: end function

result = MR.run() //execute the metamorphic relation
if ( result == false) //the MR does not hold

iterateOverInputTypes(MR,dataProvider, i+1,srcTypes)
else //we have set a view for every input type in the relation

dataProvider.nextView(dataTypes[i])
if (i < dataTypes.lenght) then //need to iterate over other types

addFailure(Failures,dataProvider) //trace the failure

Fig. 7. Metamorphic testing algorithm.

wi reports only failures that perform HTTP requests (e.g.,
accessing a URL) not generated by input sequences that
led to previously reported failures. Therefore, it reduces the
time spent to manually analyze failures triggered by distinct
follow-up inputs exercising the same vulnerability.

To guarantee that all input item combinations are used,
function nextView is iteratively invoked until all the items
of a given input type are processed (Line 7). The MST-
wi data representation function RandomValue (see Table 1)
returns a random data value. For scalability reasons, it
is conﬁgured to generate up to 100 different values, thus
leading to 100 views.

Fig. 8 illustrates the execution of the relation in Fig. 2.
The table on the left represents the sequence of functions
invoked by our algorithm. In this example, two views for
User are inspected for each view of Input. The ﬁrst two
invocations of MR.run return true (not shown in Fig. 8)
because the login and stats pages have been accessed by both
users devel and tester, and, thus, the implication holds. The
third invocation of MR.run returns false because the output
page for the startSlave URL is the same for the two input
sequences and, thus, the relation does not hold. We rely on
edit distance to determine if Web pages are equal.

10

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

Fig. 8. Example data processing for the relation in Fig. 2.

8 CATALOG OF METAMORPHIC RELATIONS

We derived a catalog of MRs from the activities described
in the OWASP book on security testing [9] and from a set of
security vulnerability types related to violations of security
design principles and reported in the Common Weakness
Enumeration (CWE) database [56].

The OWASP book on security testing presents detailed
descriptions of 90 testing activities (hereafter OWASP secu-
rity testing activities) for Web systems; each OWASP security
testing activity targets a speciﬁc vulnerability type. The ac-
tivities provide information for implementing metamorphic
relations. For example, for the bypass authorization schema
vulnerability, OWASP suggests collecting links in adminis-
trative interfaces and directly accessing the corresponding
URLs by using the credentials of other users. Using this
suggestion, we deﬁned the MR in Fig. 2.

The CWE vulnerability types considered for our catalog
concern violations of security design principles. As further
discussed in Section 9, they are the ones we selected to ad-
dress our research questions about the applicability of MST-
wi as they correspond to system-level violations resulting
from design mistakes.

We can perform security testing activities (e.g., simulate
attacks) in multiple ways and may have more than one
relation for each OWASP activity or CWE vulnerability type.
Also, not all testing activities beneﬁt from MT. We discuss
the capabilities of MT in Section 9.

Our catalog includes 76 MRs in total. 22 MRs automate
16 OWASP activities; 54 MRs detect 101 CWE vulnerability
types. 15 MRs both automate OWASP activities and de-
tect CWE vulnerability types, which is unsurprising since
OWASP activities aim to verify that secure design princi-
ples (e.g., correctly implement and conﬁgure authorization
mechanisms) are in place. Overall, our MR catalog discovers
102 security vulnerability types, including 101 belonging to

the CWE catalog (some of which are discovered by OWASP
testing activities) and one vulnerability type being targeted
by OWASP testing activities only.

We perform security testing using follow-up inputs that
cannot be generated by interacting with the GUI of the
system but conform with the input format of the system
and match its conﬁguration (e.g., the URLs requested by
the unauthorized user refer to existing system resources).
We inherit, from mutational fuzzing, the idea of generat-
ing follow-up inputs by altering valid source inputs [57].
However, we do not rely only on random values to obtain
valid inputs that match the system conﬁguration. Instead,
we modify source inputs using the data provided by the
SMRL Web-utility functions, which return domain-speciﬁc
information (e.g., protocol names), random values, and
crawled data. Finally, by capturing properties of the output
generated from the source and follow-up inputs, we identify
vulnerabilities that cannot be detected with implicit test
oracles [6] (e.g., crashes).

We present some of the MRs in our catalog in Figs. 10
to 20. The entire catalog is available for download [40].
All the MRs in our catalog follow the template in Fig. 9
in Section 8.1. Also, the sequences of invocations of Web-
utility functions in our MRs can be grouped into a set of
patterns given in Section 8.2. The identiﬁcation of patterns
facilitates the deﬁnition of solutions to a problem [58]; in our
case, the MR patterns facilitate the identiﬁcation of MRs to
address the problem of automatically performing security
testing based on the description of a vulnerability type
and corresponding attacks. In other words, MR patterns
help understand how to derive MRs that simulate certain
attacks; in turn, they enable engineers to deﬁne new MRs
for attacks not considered yet (e.g., by looking at similarities
with attacks already considered).

iterateOverInputTypes(..,1,..)nextView("Input")iterateOverInputTypes(..,2,..)nextView("User")MR.run()nextView("User")MR.run()nextView("Input")iterateOverInputTypes(..,2,..)nextView("User")MR.run()addFailure()nextView("User")...Call #Input Typei-thitem[1]Input<A1,A2><A1,A3><A1,A4>[2]User<"devel"><"tester">[3]User<"tester"><"devel">[4]Input<A1,A3><A1,A4><A1,A2>[5]User<"devel"><"tester">Input(1) → <A1,A2>User(2) → <"devel">cannotReachThroughGUI(<"devel">,"../login")→falsecannotReachThroughGUI(<"devel">,"../startSlave")→truechangeCredentials(..)→<{"../login";user="devel";pwd=... >Input(2) →<{"../login";user="devel";pwd="abc"},...>Output(Input(1),2) → <HTMLofStartSlave>Output(Input(2),2) → <HTMLofStartSlave>return falseSequence of functions invoked by the metamorphic testing algorithmLegend:→ val: f(..) :functionreturned value/object< .. > :complex data type with nested fieldsContent of the views generated by the different calls to method 'nextView'Method calls and data generated within 'MR.run()'[1][2][3][4][5]BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

11

1:
2:
3:

4:
5:
6:
7:
8:
9:
10:

Iterate over all the Actions of Input

(optional) Iterate over all the Actions of another Input
(optional) Iterates over all the parameters, form inputs, or session cookies

TABLE 3
MRs template element instances

of the selected Action

IMPLIES (cid:0)

[a precondition holds (cid:3)
[ and a follow-up input is successfully created(cid:3)
(cid:2) and the follow-up input is successfully modiﬁed (cid:3)

repeatable multiple times

repeatable multiple times

repeatable multiple times

Template element

Preconditions

,

(cid:1)

condition on the outputs generated for the source and follow-up inputs

Generation of follow-up inputs

Fig. 9. Template common to all the MRs in our catalog.

8.1 Metamorphic Relations Template

Output conditions (postconditions)

Element instance

User precondition
Action precondition
Same user
Different user
Same actions
Actions subset
Added action(s)
Modiﬁed Action(s)
Verify equality
Verify difference
Verify other predicate

This section presents the template followed by all the MRs
in our catalog. This template was not deﬁned up-front but is
the result of deriving MRs for more than 100 speciﬁcations
(16 OWASP activities and 101 CWE vulnerabilities) and
analyzing their commonalities.

All the MRs include a loop (Line 1 in Fig. 9) to de-
ﬁne multiple follow-up inputs by iteratively modifying
the actions of the source input. MRs may have additional
nested loops used either to combine multiple source in-
puts (Line 2) or to iterate over all the parameters, form
entries, or session cookies belonging to the Web page
on which the action is performed (Line 3). For example,
CWE_287a_425_OTG_AUTHN_001 (Fig. 10) works with all
the login actions observed in the source input. Function
isLogin() returns true only if the current action performs
a login; otherwise, the implication trivially holds, and no
follow-up input is generated.

We express all our MRs using an implication (operator
IMPLIES in Line 4 of Fig. 9). The left-hand side of the im-
plication often starts with verifying if a precondition holds
(Line 5). Then we check if the follow-up input is successfully
deﬁned (Line 6). Finally, we ensure that the follow-up input
can be further modiﬁed through multiple function calls
appearing on the left-hand side of the implication (Line 7).
For instance, the ﬁrst and second follow-up inputs in MR
CWE_302_471_472_784_807 (Fig. 11) are for performing
the actions of the source inputs with another user (to ensure
the follow-up user cannot perform them) and the attack,
respectively.
implication usually
The
captures
the
source and follow-up inputs. For instance, according to
CWE_287a_425_OTG_AUTHN_001 in Fig. 10, the output
for the follow-up input (which performs a login on the
unencrypted HTTP channel) should be different from the
output for the source input because it should not be possible
to login using the HTTP channel.

the
relation between the outputs of

right-hand side of

the

In the following, we describe the elements of our tem-
plate: preconditions, generation of follow-up inputs, and
output conditions (postconditions). Table 3 provides an
overview of possible template element instances.

8.1.1 Preconditions

Preconditions generally concern the actions in the source
input or the user performing these actions.

User preconditions are applied to identify the users
performing the follow-up inputs based on access level. For
instance, we use the function isSupervisorOf() to ensure that

the follow-up user doesn’t have access to a resource from
another user. Also, we rely on the function notTried(User,
actionURL) to avoid testing the same URL multiple times
with the same user; indeed, this function indicates if we
have already exercised a URL with a follow-up input for the
speciﬁed user.

Action preconditions avoid redundant follow-up in-
puts and false positives. For example, the function not-
Tried(actionURL) is used not to test the same URL twice
(regardless of the user performing the action) in MRs
that do not address authorization and authentication vul-
nerabilities. To reduce false positives, we may avoid ac-
tions whose output
leads to error or alert messages.
For instance, !Output(Input(1),pos).hasAlert for
CWE_79_a_XSSreflected (Fig. 12) checks if the source
input action does not lead to a popup message, i.e., the
condition used to determine if the XSS injection attack is
successful.

72 MRs (94%) and 33 MRs (43%) in our catalog include
an action precondition and a user precondition, respectively.

8.1.2 Generation of follow-up inputs

Follow-up inputs can be generated via input generation
strategies focusing on the sequence of actions in the source
input and the user performing actions.

We have two strategies for the user: (i) keeping the same
user of the original source input sequence and (ii) selecting
another user. The second one is adopted for MRs looking
for vulnerabilities related to conﬁdentiality, authorization,
or authentication (e.g., accessing a resource with different
users). The ﬁrst one is used in all the other MRs. In total,
eight MRs in our catalog create follow-up inputs whose
actions are performed with a user different than the one
in the source input.

Concerning actions, the strategies employed to derive
follow-up inputs include (i) relying on the same sequence
of actions of the source input sequence, (ii) retaining only a
subset of the actions, (iii) adding actions to the source input,
and (iv) modifying the actions in the source input.

We keep the same sequence of actions of the source in-
focus on authorization and authen-
put
for MRs that
tication vulnerabilities;
indeed, to test the authorization
mechanisms of a system, we can perform the same ac-
the source input but with a different access
tions of
level (e.g., CWE_266_.._OTG_AUTHZ_002 in Fig. 2). Eight
MRs in our catalog (10%) perform the same actions

12

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

(1) For loop iterates over all actions of Input(1). (2) Stores the parameters of the current action in a variable. (3) It checks that the action is a login.
(4) It veriﬁes that this login URL was not already tested. (5) Creates the follow-up input by copying the input(1). (6) Sets the HTTP channel for
the follow-up input. (7) A login operation should not succeed if performed on the HTTP channel. checks if the output generated by the login
operation is different in the two cases.

Fig. 10. CWE 287a 425 OTG AUTHN 001: Testing for credentials transported over an encrypted channel

of
the source input either with a different user (e.g.,
CWE_266_.._OTG_AUTHZ_002) or in an alternative chan-
nel (e.g., CWE_287a_425_OTG_AUTHN_001 in Fig. 10).

We take a subset of

the actions in the source in-
put to speed testing up or ensure that the system en-
forces the precedence relation between the actions (if
there are any). An example MR of
the ﬁrst case is
CWE_286_OTG_AUTHZ_002c (Fig. 13). This MR tests the
condition that if a user navigating the GUI cannot access
a URL, the same URL should not be available to the same
user when she directly requests it from the server. It relies
on a subset of the source input actions since, to test the
condition above, it is sufﬁcient to create a follow-up input
including only the action accessing the reserved URL rather
than performing all the actions. The system is vulnerable
if the follow-up input leads to a successful retrieval of the
resource pointed by the URL (i.e., the same output as the
source input action). An example for the second case (i.e.,
ensuring that precedence relations are enforced) is that of
a system that should validate a user session to ensure that
she has conﬁrmed her email address after signing up. The
system is vulnerable if the user can log in without conﬁrm-
ing her email address. The source input sequence includes
the registration to the service, the email conﬁrmation, and
a successful log-in. The follow-up input should cover only
the registration action and the log-in without conﬁrming
the email address. If the log-in is successful, the system is
vulnerable as it should ensure that the user is registered
with a valid email address. Note that MST-wi automatically
generates such follow-up inputs by iteratively generating
distinct subsets of actions from the source input sequence
(see MR CWE_841 in Fig. 14). Five MRs (7%) in our catalog
are obtained by selecting a subset of actions from follow-up
inputs.

We add one or more actions to a source input to test
scenarios when we expect such change in action sequence
to lead to different results. Usually, the actions added are
user actions belonging to a source input (not necessarily

the one used to deﬁne the follow-up input) or actions cap-
turing environmental factors (e.g., the passing of time). An
example user action copied into the follow-up input is given
in MR OTG_SESS_003 (Fig. 15); OTG_SESS_003 scans the
source inputs to identify a sign-up action to be copied into
the follow-up input, after the login. In OTG_SESS_003, the
presence of a sign-up action after the login enables us to
verify that the session ID is updated after every sign-up.
An example environment action added to a source input se-
quence is given in CWE_262_263_309_324 (Fig. 16), which
tests the system’s password aging mechanism through a
DelayAction. We consider a source input including login and
add a DelayAction that changes the system’s date to next
year, thus simulating the passing of time. When the user
performs the next action in the sequence, the system should
ask the user to change his credentials instead of completing
the requested action and returning the same results as the
source input. We obtain the follow-up inputs in nine MRs
by adding actions to the source input.

We modify actions by changing the assignments to pa-
rameters that are inputs of the SUT (i.e., URL parameters,
form entries, session cookies, certiﬁcates) or that control
the communication channel (i.e., encryption algorithms,
communication protocol, HTTP method). These changes
may be applied to follow-up inputs that match the source
inputs or include other differences (e.g., subsets). For ex-
ample, to test a system for code injection vulnerabilities,
we execute the same sequence of actions as in the source
inputs and modify form entries by relying on known at-
tack vectors (e.g., concatenating SQL injection commands
to the original
input values). An example MR that in-
volves the modiﬁcation of a communication protocol is
CWE_287a_425_OTG_AUTHN_001 in Fig. 10. In 57 MRs
of our catalog, the follow-up inputs perform the same
sequence of actions as the source input but with at least
one parameter modiﬁcation.

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

13

(1) For loop iterates over all actions of the Input(1). (2) Stores the parameters of the current action in a variable. (3) Saves the cookie value of the
Input(1) in a variable. (4) Creates a follow-up input with different credentials. (5) Saves the cookie value of the follow-up user in a variable. (6) To
speed up the process, veriﬁes that the element URL has not been tried before. (7) Stores the session key values in a variable. (8) For loop iterates
over the session values. (9) Assigns the type of the cookie to a variable. (10) Filters all cookie types other than Boolean (11) Makes sure the URL is
not accessible without login. (12) To avoid False positives, ﬁlters the actions with the same output for Input(1) and Input(2). (13) Creates the
follow-up input, named Input(3). (14) Sets the session2 as the session value of the Input(3). (15) Flip the value of the Boolean cookie. (16) The
system should give a different output or should show an error.

Fig. 11. CWE 302 471 472 784 807: Testing for assumed-immutable elements

8.1.3 Output conditions

Output conditions (postconditions) in our MRs check if the
outputs of the source and follow-up inputs (or a subset
of their actions) are equal, different, or satisfy a predicate
(e.g., the output is erroneous or includes information that
has already been observed by the user) implemented by an
SMRL function.

SMRL MRs capture properties that should hold if the
system is not vulnerable. Therefore, we expect the same
output for the source and follow-up inputs when the attack
captured by the follow-up input should not alter the system
behavior, that is, when the system is supposed to detect an
attack vector and ignore its effects. For example, the SUT
should sanitize the received inputs by removing the code
injection attack vector added to the original input and return
the same output as the source input. In 20 of our 76 MRs
(26%), the follow-up input is supposed to lead to the same
output as the source input.

In contrast, for 30 MRs in our catalog (39%), the out-
puts generated by the follow-up inputs are expected to
be different than those of the source inputs. For instance,
in MR CWE_266_.._OTG_AUTHZ_002 (Fig. 2), accessing a
resource dedicated to the source input’s user should lead to
a different output (e.g., the home page or an error page).

Last, we can also verify predicates on the generated
to verify
returned output
accessible by
in the MRs CWE_20_..OTG_AUTHZ_001a

outputs. Predicates are used,
that
the
the user

for example,

should be

CWE_15_639_OTG_AUTHZ_004.

MR
and
CWE_20_..OTG_AUTHZ_001a (Fig. 17)
replaces URL
parameters (e.g., an ID) with paths of ﬁles on the SUT.
Similarly, CWE_15_639_OTG_AUTHZ_004 (Fig. 18) replaces
URL parameter values with values observed only with
other users. Though we cannot predict the effect of altering
URL parameters — we cannot know in advance if the
value is legal for the user performing the action — the user
should be able to access the output when browsing the GUI.
Since the crawler browses the GUI with the different users,
function userCanRetrieveContent checks if the output
has already been observed with any source input collected
by the crawler. In our catalog, 61 MRs include at least one
predicate on the generated outputs.

8.2 Metamorphic Relation Patterns

This section presents how the instances of the template
elements are combined to form the MR patterns. A pattern is
captured by a set of element instances in one or more MRs.
To identify MR patterns, we ﬁrst ﬁlled in a mapping
table tracing MRs to the template elements in the previous
section. Then, we grouped the MRs covering the same com-
bination of elements. By deﬁnition, an MR can implement
only one pattern. The resulting MR patterns are reported in
Table 4.

In total, we identiﬁed 23 patterns. Six patterns are im-
plemented by at least ﬁve MRs, and twelve patterns are

14

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

(1) It avoids clicking on OK; because dialogs are normally ignored by our framework by clicking on OK. (2) For loop iterates over all actions of
the Input(1). (3) The second loop iterates over all parameters of the action. (4) Deﬁnes a variable to store the position of the Input(1)’s action. (5)
Checks if the parameter of the action URL has not seen before, to speed up the process. (6) Veriﬁes that the action does not originally contain any
alert. (7) Creates the follow-up input by copying the Input(1). (8) Sets the injected XSS string as the parameter value of the follow-up input. (9)
Checks either the attack was not performed or no effect shall be observed (the effect of the XSS is usually visualized when reaching the page
where we inject the XSS).

Fig. 12. CWE 79 a XSSreﬂected: Testing for reﬂected cross-site scripting

(1) The for loop iterates over all the actions of an input sequence. (2) Checks whether the user in User() is not a supervisor of the user performing
the y-th action. (3) Veriﬁes that the y-th action is performed after a login. (4) Veriﬁes that the follow-up user cannot retrieve the URL of the action
through the GUI (based on the data collected by the crawler). (5) Deﬁnes a follow-up input that performs the login as the follow-up user (6) The
system checks if the y-th action from the source input leads to an error page Or the output generated by the action containing the URL indicated
above, lead to two different outputs in the two cases.

Fig. 13. CWE 286 OTG AUTHZ 002c: Testing for incorrect user management

implemented by only one MR. In the following, we discuss
the three most frequent patterns.

Two patterns occur with the same frequency (i.e., P1 and
P2 in Table 4); we present them in the order they appear
in Table 4. The ﬁrst pattern (i.e., P1) includes the following
template elements: user precondition(s), action precondition(s),
same user, modiﬁed action(s), and verify other predicates. It is
implemented by 13 MRs and used to ensure that a user
cannot retrieve protected resources by providing crafted
data as input. Indeed, the same user performs the same
actions as in the source input sequence after modifying
some of them. User and action preconditions enable the
selection of cases where the MR should hold. All these 13

MRs include an action precondition verifying that the URL
is tested only once (to speed testing up). Further, 12 of these
13 MRs include a user precondition checking if the user
has already retrieved the content of the source input (to
avoid testing with non-deterministic sequences); one MR
veriﬁes that the user performing the follow-up actions is
not an administrator since she might have the permissions
to perform these actions. The predicates ensure that the
generated output is either an error page or content that
the user is supposed to be able to access (i.e., it has been
accessed during crawling or functional testing as reported
by functions userCanAccess or userCanRetrieveContent). After
providing a code injection string, CWE_94_96_B (Fig. 19),

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

15

(1) The ﬁrst loop iterates over all the actions to ﬁnd a signup action. (2) The second loop iterates over all the actions to ﬁnd a login action
performed after the signup. (3) Veriﬁes that the action y is a login. (4) Creates the follow-up input with the sequence of actions after the login
(action y). (5) Makes sure that we start with a signup action (6) Reset the actions for next iteration (7) Veriﬁes that the system gives two different
outputs or will give an error by executing the follow-up input.

Fig. 14. CWE 841: Testing for improper enforcement of behavioral workﬂow

(1) The ﬁrst loop iterates over the inputs to ﬁnd a sign up action. (2) The second loop iterates over the actions that follow the sign up. The second
loop is necessary to check that a sign up action repeated at any point of the action sequence leads to a new session ID. (3) Checks if the current
action has been performed after a login. (4) Deﬁnes a follow-up input with the sign up action being duplicated in a certain position. (5) Checks if
the session ID of the response page sent after the two successive login actions is different.

Fig. 15. OTG SESS 003: Testing for session ﬁxation

for instance, veriﬁes that a user can retrieve only an error
page or a resource that she can access.

The other most frequent pattern (i.e., P2) is similar to the
ﬁrst one, except it does not verify user preconditions and
veriﬁes output equality. It includes the following template
elements: action precondition, same user, modiﬁed action(s),
verify equality, and verify other predicates. It is used to simulate
attacks where the user provides a crafted input (e.g., invalid
character) that should be either sanitized (the same output is
returned) or lead to an error page (this last condition is ver-
iﬁed with a dedicated predicate on the output). An example
MR of this pattern is MR CWE 792 793 794 795 796 797 A
(Fig. 20).

The third pattern (i.e., P3) includes the following tem-
plate elements: action precondition(s), same user, added ac-
tion(s), and verify the difference. It is implemented by seven
MRs where a follow-up input with a different number of
steps should lead to different outputs (e.g., OTG SESS 003

in Fig. 15, which duplicates a signup action and veriﬁes that
the action leads to a session cookie different from the cookie
of the previous signup action).

Twelve patterns are implemented by only one MR. Six of
these patterns result from MRs creating multiple follow-up
inputs, which leads to combinations of template elements
normally not appearing together (e.g., equal and subset).
Another pattern (i.e., P12) concerns an MR that does not
verify any precondition (it generates follow-up inputs from
all the available source inputs) because discovering the vul-
nerability likely depends on the system state, the sequence
of actions previously executed, or the user performing the
action; it differs from all the other patterns because they
all include at least one precondition. The presence of state-
dependent vulnerabilities is however rare (e.g., the success-
ful exploitation of a code injection vulnerability is unlikely
to depend on the actions performed before), which is why
all our patterns, except one, have at least one precondition.

16

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

(1) For loop iterates over the actions of Input(1). (2) Checks if the current action x is doing ”log in”. (3) Checks that the action-x of the Input(1)
does not include any errors. (4) Creates the follow-up Input by copying the Input(1), named Input(2). (5) Add a wait action to Input(2) that
moves time forward for a year i.e. the expected time for resetting the password. (6) Based on the above description the results should be
different; it shall prompt a password update request.

Fig. 16. CWE 262 263 309 324: Testing for password aging

(1) Iterates over the actions of the Input(1). (2) The second for loop iterates over the parameters of the action. (3) Stores the parameters of the
current action in a variable. (4) To speed up the process, veriﬁes that the URL has not been tried before. (5) Creates the follow-up input, named
Input(2). (6) Sets the value of a parameter to a random ﬁle path. (7) Veriﬁes that the system shows an error page or the returned content is
something that the user has the right to access.

Fig. 17. CWE 20 73 99 219 220 528 530 642 732 OTG AUTHZ 001a: Testing for directory traversal

9 ANALYSIS OF MST-wi ’S APPLICABILITY AND
TESTABILITY GUIDELINES

In this section, we investigate (i) the types of security testing
activities presenting an oracle problem that can only be ad-
dressed by MST-wi, (ii) the types of security vulnerabilities
that can be identiﬁed by MST-wi, and (iii) the guidelines
(hereafter testability guidelines) that engineers should fol-
low to make MST-wi as effective as possible in software
projects. We investigate the following Research Questions
(RQs):

• RQ1. To what extent can MST-wi address the oracle
problem in the context of security testing? This
research question aims to identify the security testing
activities that, due to the oracle problem, can only be
automated using MST-wi.

• RQ2. What vulnerability types can MST-wi de-
tect? MST-wi has been designed and implemented
to perform security testing by reasoning on outputs
of multiple interactions with the system under test.

Not every type of vulnerability can be discovered
through relationships between outputs of multiple
user-system interactions (e.g., some may require pro-
gram analysis). This research question aims to de-
termine, in a systematic way, the types of security
vulnerabilities that can and cannot be discovered by
MST-wi.

• RQ3. Is it possible to deﬁne testability guidelines
to enable effective test automation with MST-wi?
Software testability is the degree to which a software
artifact (i.e., a software system, module, require-
ments, or design document) supports its testing [59].
A higher degree of testability results in decreased test
effort, increased quality of test activities, a higher
probability of ﬁnding software defects, and, as a
result, higher-quality software. This question investi-
gates if it is possible to identify testability guidelines
that assist engineers in designing, implementing, and
conﬁguring their software to enable effective test
automation with MST-wi.

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

17

(1) The ﬁrst loop iterates over all the actions of the input sequence. (2) The second iterates over all the parameters of the action. (3) The third loop
iterates over all used values by other users. (4) Stores the parameters of the current action in a variable. (5) Deﬁnes the follow-up input. (6) Sets a
parameter value to a value that is used by other users. (7) Checks if the content of the output is either an error message Or some content that can
be retrieved from the GUI.

Fig. 18. CWE 15 639 OTG AUTHZ 004: Testing for externally controlled elements

(1) It avoids clicking on OK; because dialogs are normally ignored by our framework by clicking on OK. (2) For loop iterate over all actions of
the Input(1). (3) Deﬁne a variable to store the position of the Input(1)’s action. (4) The second loop iterates over all parameters of the action if the
action contains a form input. (5) makes sure the user will submit the form. (6) Veriﬁes the current parameter was not tested before. (7) Filters out
the web pages with dynamic content. (8) Creates the follow-up input by copying the Input(1). (9) Injects some Static injection strings to the
follow-up input. (10) The system may show an error page to the user or the user is retrieving the content that has right to access it.

Fig. 19. CWE 94 96 B: Testing for static code injection

9.1 Targeted Vulnerabilities and Testing Activities

To address our ﬁrst research question, we study the security
testing activities recommended by OWASP [35]. These ac-
tivities have been described as part of the OWASP testing
guidelines Version 4.0 [9] to help engineers understand
the what, why, when, where, and how of testing security of
Web applications. The project provides these activities as a
complete testing framework, not merely a simple checklist
or prescription of issues that should be addressed. There are,
in total, 90 testing activities organized into 11 categories. For
instance, in testing activity Testing Session Timeout, engineers
check that the system under test automatically logs out a
user when that user has been idle for a certain amount of
time, ensuring that it is not possible to “reuse” the same

session and that no sensitive data remains stored in the
browser cache [60]. In our analysis, we include the security
testing activities recommended by OWASP because they
provide a comprehensive list of security testing methods
that we can analyze to identify whether they suffer from the
oracle problem.

To address our second and third research questions in a
systematic way, we study the list of weaknesses reported in
the Common Weakness Enumeration (CWE) database [56].
We provide the following deﬁnitions of vulnerability and
weakness since their deﬁnitions in the CWE framework [61]

18

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

(1) It avoids clicking on OK; because dialogues are normally ignored by our framework by clicking on OK. (2) For loop iterates over all actions of
the Input(1). (3) Iterates over all parameters of each action. (4) Stores the parameters of the action in the variable. (5) Reads the user’s input value
and keeps it in a variable. (6) Checks if the URL was not tried before, to speed up the process. (7) Filters out the boolean input values. (8) Creates
the follow-up input by copying the Input(1). (9) Sets the new input value which contains special character for the follow-up input. (10) Veriﬁes
that the system should show an error page or it would neutralize the input.

Fig. 20. CWE 792 793 794 795 796 797 A: Testing for incomplete ﬁltering of one or more instances of special elements

TABLE 4
MR Patterns

Pattern ID

Preconditions

Generation of follow-up inputs

Output condition

User
precondition

Action
precondition

Same
user

Different
user

Same
actions

Actions
subset

Added
action(s)

Modiﬁed
action(s)

Verify
equality

Verify
difference

P1
P2
P3
P4
P5
P6
P7
P8
P9
P10
P11
P12
P13
P14
P15
P16
P17
P18
P19
P20
P21
P22
P23

1
0
0
0
0
1
1
0
1
1
0
0
1
0
0
0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1
0
1
1
0
1
1
1
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0

0
0
0
0
0
0
0
0
0
1
0
0
1
1
0
0
0
0
1
1
1
0
1

0
0
0
0
0
0
0
0
0
1
1
0
0
1
0
0
1
1
0
0
0
0
1

0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
0
0
0
0
1
1
0

0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0

1
1
0
1
1
1
1
1
1
0
0
0
0
1
0
1
0
1
1
0
0
0
0

0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
1

0
0
1
0
1
1
0
1
0
1
0
0
1
1
1
1
1
0
1
0
0
1
0

Verify
other
Predicates
1
1
0
1
1
1
1
0
1
1
1
0
1
0
1
1
0
1
1
1
0
1
1

Number of MRs

13
13
7
6
5
5
4
4
3
2
2
1
1
1
1
1
1
1
1
1
1
1
1

lack clarity2. A vulnerability is a speciﬁc fault of the system
under test that causes the system to breach its security
requirements. A weakness represents a fault type (i.e., the
type of a vulnerability). It describes a human error made in
the analysis, design, or implementation of the system that

2. The deﬁnitions provided by CWE are unclear since they rely
on synonyms to distinguish vulnerability and weakness, as follows:
‘Weaknesses are ﬂaws, faults, bugs, and other errors in system design,
architecture, code, or implementation that if left unaddressed could
result in systems and networks, and hardware being vulnerable to
attack. Weaknesses can lead to vulnerabilities. A vulnerability is a
mistake in software or hardware that can be used by a malicious user to
gain access to a system or network [61]’.

may affect the degree to which the system meets its security
requirements.

The CWE database is organized into distinct views,
each view grouping weaknesses according to a different
set of categories, which are common security architectural
tactics [43], software development concepts [62], research con-
cepts [63], software fault patterns [64], most dangerous er-
rors [44], and hardware design [65]. Other views map the
weaknesses to some security-related catalogs (e.g., OWASP
Top 10 [45] and SERT CEI C Coding standards [66]).

The CWE view for common security architectural tactics
organizes weaknesses according to security design principles.

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

19

This view has twelve categories representing the individ-
ual security design principles that are part of a secure-
by-design approach to software development. It covers, in
total, 223 weaknesses. The security design principles assist
engineers in identifying potential mistakes that can be made
when designing software [67], [68]. A weakness is thus
the result of a design principle not being followed. For
instance, the weaknesses in design principle Authenticate
Actors are related to authentication-based components in
the system. These components deal with verifying that the
actor interacting with the system is who she claims to be.
The weaknesses in this category lead to a degradation of
the quality of authentication if they are not addressed when
designing and implementing the system under test [43]. The
views for software development concepts and hardware design
organize weaknesses based on the types of errors that affect
the software implementation (e.g., illegal pointer derefer-
ences) and hardware design (e.g., faults in semiconductor
logic), respectively. The views for software fault patterns and
research concepts group implementation errors into categories
capturing fault patterns [69] or high level descriptions of the
faulty software behaviour (e.g., incorrect comparison and
improper access control).

In our analysis, we focus on the weaknesses in the
CWE view for common security architectural tactics [43], the
weaknesses in the view for the CWE Top 25 most dangerous
software errors (CWE Top 25) [44], and the weaknesses in
the view for the OWASP Top 10 Web security risks (OWASP
Top 10) [70]. We select the common security architectural
tactics view because it enables us to determine the security
design principles that MST-wi can verify. We do not consider
the view for software development concepts because MST-wi is
a black-box testing approach, that does not aim to discover
speciﬁc implementation errors (e.g., type errors). We also
ignore the views for software fault patterns, research concepts,
and mappings to coding standards [66] since they mainly
focus on software implementation. We ignore the CWE view
for hardware design since MST-wi does not address hardware
vulnerabilities.

In our analysis, we include the CWE Top 25 and OWASP
Top 10 views to assess to what extent MST-wi can address
the most widespread and critical security vulnerabilities.
The CWE Top 25 view lists twenty-ﬁve most widespread
weaknesses which are often easy to ﬁnd and exploit. These
weaknesses are considered dangerous because they typ-
ically allow attackers to completely take over the control of
software, steal data, or prevent software from working [44]. The
OWASP Top 10 [45] is the list of the ten most common web
application security risks edited by the Open Web Appli-
cation Security Project [71], i.e., an online community pro-
ducing freely-available articles, methodologies, documenta-
tion, tools, and technologies in the ﬁeld of web application
security. It is updated every three to four years. The most
up-to-date version includes 43 weaknesses grouped into 10
categories [70].

9.2 RQ1: Oracle Problem

9.2.1 Analysis Procedure

To respond to RQ1, we study the security testing activities
recommended by OWASP [9]. We systematically analyzed

them to identify applicable, state-of-the-art oracle automa-
tion strategies. For each activity, we ﬁrst inspect its descrip-
tion, objective, methods, and tools, if available.

Based on the information collected from our inspection,
we identify oracle automation strategies, including meta-
morphic testing, that can be applied to address the oracle
problem if the activity is subject to it. For instance, testing
activity Testing for Bypass Authorization Schema focuses on
verifying how the authorization schema has been imple-
mented for each role or privilege to get access to reserved
functions and resources [72]. One of its testing method is
to try to access to resources assigned to a different role;
another one is to try to access to administrative functions.
In the ﬁrst method, it is not always feasible to verify the
access to resources with different privileges and roles when
the expected outputs need to be identiﬁed for a large set
of inputs (i.e., the oracle problem). To address the oracle
problem in the activity, we specify the MR in Fig. 2. It
compares the outputs of the executions of the system under
test for the same resource and different credentials. We
discuss the proportion of software testing activities that
cannot be automated with state-of-the-art oracle automation
strategies but can be automated with MST-wi.

9.2.2 Results

In our analysis, we identiﬁed four oracle automation
strategies for security testing: implicit oracle, catalog-based,
vulnerability-speciﬁc, and metamorphic testing. In the follow-
ing, we discuss the details of each strategy; in addition, we
discuss why in certain cases no oracle is needed or only a
manual oracle is feasible. To exemplify our descriptions, we
provide in Table 5, for each OWASP testing category, some
of its security testing activities along with oracle automation
strategies.

No oracle needed. Some activities collect data to reverse
engineer the system under test. They do not verify the secu-
rity properties of the system. They aim to retrieve informa-
tion which might be useful to identify potential weaknesses
(e.g., the use of vulnerable versions of a Web framework).
Therefore, these activities do not have an oracle problem.
For instance, in security testing activity Map Application
Architecture [73], engineers identify the components of a Web
system, e.g., reverse proxy, type of front-end Web server, and
version of the LDAP server. Commonly, hundreds of Web
applications are hosted on an interconnected Web server
infrastructure. A single vulnerability in one application may
risk the security of the entire infrastructure. Even small risks
may evolve into severe ones for other applications on the
same server. Therefore, it is important to perform an in-
depth review of known security issues for each application.
Before the review, engineers need to map the application
architecture through some tests to determine which applica-
tion components are used [73].

Manual oracle. Some activities require humans to de-
termine vulnerabilities based on system speciﬁcations. For
instance, testing activity Testing for the Circumvention of Work
Flows concerns vulnerabilities for the misuse of a system in a
way that allows malicious users to circumvent the intended
workﬂow [74]. Vulnerabilities related to the circumvention
of workﬂows are very system-speciﬁc. In short, the business

20

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 5
Subset of the security testing activities recommended by OWASP.

Security Testing Category

Security Testing Activity

Conﬁguration and Deployment Management Testing

Authentication Testing

Authorization Testing

Session Management Testing

Input Validation Testing

Business Logic Testing

Oracle Automation Strategy
Manual Oracle
No Oracle Needed

Test Application Platform Conﬁguration
Test HTTP Methods
Testing for Credentials Transported over an Encrypted Channel Metamorphic Testing
Testing for Default Credentials
Testing for Weak Password Policy
Testing for Weak Lock Out Mechanism
Testing Directory Traversal File Include
Testing for Bypassing Authorization Schema
Testing for Privilege Escalation
Testing for Insecure Direct Object References
Testing for Cookies Attributes
Testing for Session Fixation
Testing for Exposed Session Variables
Testing for Cross Site Request Forgery
Testing for SQL Injection
Testing for HTTP Verb Tampering
Testing for Buffer Overﬂow
Test for Process Timing
Test Number of Times a Function Can Be Used Limits
Testing for the Circumvention of Work Flows

Catalog-based
Catalog-based
Implicit Oracle
Metamorphic Testing
Metamorphic Testing
Metamorphic Testing
Metamorphic Testing
Manual Oracle
Metamorphic Testing
No Oracle Needed
Vulnerability Speciﬁc
Vulnerability Speciﬁc
Metamorphic Testing
Implicit Oracle
No Oracle Needed
Metamorphic Testing
Manual Oracle

process of the system under test must ensure that transac-
tions and actions proceed in the right order. For example,
when testing a pay-per-view system (i.e., a pay television
service by which a viewer can purchase events to view via
private telecast), it is necessary to determine transactions
that should not enable service access. Only a human can
decide, based on system speciﬁcations, whether pending
transactions should grant service access.

Implicit oracle. We can automate some of the security
testing activities by following randomized test input gen-
eration strategies relying on implicit oracles. An implicit
oracle refers to the detection of “obvious” faults such as
a program crash [6]. For instance, testing activity Testing for
Buffer Overﬂow in Table 5 is automated by looking for system
crashes in response to lengthy inputs [75]. One of the testing
methods in the activity is testing for the format string [76].
A format string is a null-terminated character sequence with
conversion speciﬁers interpreted or converted at run-time.
For systems concatenating user input with a format string,
we add additional conversion speciﬁers to cause a buffer
overﬂow and eventually a system crash.

Catalog-based. We can automate some activities based
on a predeﬁned catalog in which we specify test inputs
and oracles. For instance, testing activity Testing for Default
Credentials focuses on systems installed on servers with min-
imal conﬁguration or customization by the server adminis-
trator [77]. Often these systems are not properly conﬁgured,
and the default credentials for initial authentication and
conﬁguration are never changed. These default credentials
are well-known by malicious users, who use them to access
various types of systems. As part of the security testing
activity, we use a catalog of default credentials to test
whether easy-to-guess pairs of usernames and passwords
) can be used to log into the system
admin, admin
(e.g.,
(cid:104)
under test.

(cid:105)

Vulnerability-speciﬁc. Some activities can get automated
by state-of-the-art tools such as Burp Suite [78] and thus
may not necessarily beneﬁt from MT. These include OWASP
testing activities that detect cross-site scripting and code in-
jection vulnerabilities. Other activities are either not targeted

or partially automated. For example, Burp Suite does not
automate oracles for activity Testing for Bypassing Authoriza-
tion Schema [79]. Though it enables engineers to compare
the content of site maps [80] recorded in different user ses-
sions (e.g., with and without certain privileges), it requires
engineers to manually identify privileged resources and in-
spect the differences in the observed system outputs, which
is error-prone (e.g., overlooking resources) and expensive.
Even Burp Suite plug-ins using Crawljax to build site maps
do not address the oracle problem but generate JUnit tests
that simply retrieve the mapped resources [81]. With MST-
wi, engineers, instead, can focus on the speciﬁcations of
system-level properties without performing such manual
testing activities. Testing activities, including oracles, are
automated by the MST-wi framework.

MST-wi. In general, MST-wi can automate the testing of
activities that verify if a resource of the system under test
can be accessed under circumstances that should prevent
it (e.g., an unauthenticated user or unencrypted channel).
These activities beneﬁt from MST-wi since they entail the
veriﬁcation of numerous system resources and speciﬁc se-
curity properties (e.g., each Web page might be accessed
by a different set of users). For instance, testing activity
Testing Directory Traversal File Include focuses on reading
directories or ﬁles which normally cannot be read, accessing
data outside the web document root, and including scripts
and other kinds of ﬁles from external websites [82]. With
a large set of test inputs, it is not feasible to list all the
directories and ﬁles which a user normally cannot reach. To
address the oracle problem in this testing activity, we specify
an MR which veriﬁes that a ﬁle path should never enable a
user to access data that is not provided by the user interface
(see relation CWE_20_.._OTG_AUTHZ_001a in Fig. 17).

Table 6 presents a summary of the OWASP security
testing categories and the oracle automation strategies. The
ﬁrst column lists the security testing categories. The rest of
the columns present the numbers of testing activities in the
categories automated by each strategy.

In total, 19 out of 90 activities (21%) do not require a
test oracle while 71 activities (79%) do. Also, only 30 out of

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

21

TABLE 6
Oracle automation strategies for the OWASP security testing activities*.

Security Testing Category

Information Gathering
Conﬁguration and Deployment Management Testing
Identity Management Testing
Authentication Testing
Authorization Testing
Session Management Testing
Input Validation Testing
Testing for Error Handling
Testing for Weak Cryptography
Business Logic Testing
Client Side Testing
Total
% of testing activities

Implicit
Oracle
-
-
-
1
-
-
1
-
-
-
-
2
2%

Catalog-
based
-
1
2
3
-
-
-
-
-
-
-
6
8%

Oracle Automation Strategy
Manual
Oracle
-
3
3
3
-
2
2
-
3
6
3
25
28%

No Oracle
Needed
10
4
-
-
-
1
1
2
-
1
-
19
21%

Vulnerability-
speciﬁc
-
-
-
-
-
1
11
-
-
1
9
22
24%

MST-wi

-
1
-
3
4
4
2
-
1
1
-
16
18%

*Details are available online [40]. For readability, the symbol ’-’ stands for zero. The % of testing activities (last row) is computed with respect to
the 90 activities in the OWASP book [9].

these 71 activities (42%) can beneﬁt from existing oracle au-
tomation solutions (i.e., implicit oracle, catalog-based, and
vulnerability-speciﬁc), thus highlighting the severe impact
of the oracle problem on security testing automation.

session cookie(s) after successful user authentication [84].
OTG_SESS_003 veriﬁes whether a signup action leads to a
new session ID, even when the action is performed by a user
already logged in.

More than half of the activities not requiring a test oracle
(53%) are associated with testing category Information Gath-
ering because it covers all the activities which require reverse
engineering or manual information retrieval to collect data
about the system under test.

Among

traditional

automation

solutions,
oracle
vulnerability-speciﬁc approaches are the ones covering
the largest proportion of OWASP activities (i.e., 22 out of
90, 24%). Half of these activities are organized into category
Input Validation Testing and concern cross-site scripting and
injection vulnerabilities; however, as reported in Section 9.3,
MST-wi can still be used to test for these vulnerabilities thus
enabling engineers to rely on a single testing framework
(i.e., MST-wi) rather than several ones. Catalog-based and
Implicit oracles address a low percentage of activities
(8% and 2%, respectively). The limited applicability of
implicit oracles show that most of the solutions relying on
them (e.g., fuzz testing tools [83]), though useful, partially
address the needs of security testing engineers.

Among the activities that cannot beneﬁt from traditional
oracle automation solutions, 16 (39%) can be automated
thanks to MST-wi, which highlights the relevance of the con-
tribution of this paper. A majority of these activities (69%)
are in categories Authentication Testing, Authorization Testing,
and Session Management Testing. The testing activities in
these three categories require multiple interactions between
the system under test and one or more users (actors), two
characteristics well supported by MST-wi. Among them,
MST-wi can automate all the testing activities in category
Authorization Testing (i.e., activities Testing Directory Traver-
sal File Include, Testing for Bypassing Authorization Schema,
Testing for Privilege Escalation, and Testing for Insecure Direct
Object References in Table 5). Further, it can automate half
of the activities in category Session Management Testing. An
example is Testing for Session Fixation, which is automated by
OTG_SESS_003 in Fig. 15. A session ﬁxation vulnerability
occurs when the system under test does not renew its

Based on the above, we conclude that MST-wi can play a
key role in addressing the oracle problem in security testing. The
activities for which MST-wi can automate oracles are mostly
those that (i) verify that resources can be accessed only by
authorized users (Authorization Testing), (ii) test the initial
authentication and the transfer of the user’s authentication
data (Authentication Testing), (iii) discover vulnerabilities
associated with session management (Session Management
Testing), and (iv) test the system’s response to HTTP meth-
ods and parameters (Input Validation Testing).

9.3 RQ2: Vulnerability Types

9.3.1 Analysis Procedure

We aim to study which types of vulnerabilities can be
discovered by MST-wi. To enable a discussion structured
according to well-deﬁned categories of vulnerabilities, we
compute, for each category in the CWE views mentioned
in Section 9.1 (i.e., Common security architectural tactics, CWE
Top 25, and OWASP Top 10), the percentage of weaknesses
that can be automatically discovered by MST-wi.

We systematically analyzed all the weaknesses with the
objective of writing, for each one, one or more MRs using
SMRL. For each weakness, we ﬁrst inspect its description, its
demonstrative examples, the description of concrete vulner-
abilities (CVE) and common attack patterns (CAPEC) [85]
associated with the weakness. Based on the information col-
lected from our inspection, we implemented, using SMRL, a
new MR that address the weakness or reused, if possible, an
MR already available in the MST-wi catalog. Each time we
could not do so, we kept track of the reasons preventing the
writing of an MR. All the MRs resulting from this analysis
are part of the MRs catalog provided online [39].

We report the percentage of weaknesses for which it
has been possible to implement an MR; in other words,
the weaknesses that can be automatically tested with MST-
wi. Since some of the weaknesses in the CWE database

22

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 7
Subset of the security weaknesses in the CWE view for common security design principles.

Design principle

Weakness

Belongs to
Top 25

Belongs to
Top 10

Generic
Weakness

Addressed
by MST-wi

Testability Feature (TF) /
Reason MST-wi cannot be
applied (R)

Audit

Authenticate Actors

Authorize Actors

Encrypt Data

Identify Actors

Limit Access

Manage User Sessions

Validate Inputs

Omission of Security-relevant Information

Obscured Security-relevant Information by
Alternate Name
Improper Authentication

Weak Password Recovery Mechanism for For-
gotten Password
Improper Privilege Management

Process Control

Small Space of Random Values

Missing Encryption of Sensitive Data

Improper Validation of Certiﬁcate with Host
Mismatch
Improper Validation of Certiﬁcate Expiration

Improper Restriction of XML External Entity
Reference
External Control of File Name or Path

J2EE Bad Practices: Non-serializable Object
Stored in Session
Insufﬁcient Session Expiration

Cross-site Scripting

Deserialization of Untrusted Data

No

No

Yes

No

No

No

No

No
No

No

Yes

No

No

No

Yes

Yes

Yes

No

Yes

Yes

Yes

No

No

Yes
No

No

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes
Yes

Yes

Yes

Yes

No

No

Yes

No

No

No

Yes

No

Yes

No

No

No
No

Yes

Yes

Yes

No

Yes

Yes

No

R3

R3

TF3

R2

TF3

R1

R4

R3

R5
TF10

TF2

TF1

R1

TF2, TF8

TF2

R2

TABLE 8
Summary of the CWE architectural security design principles and
weaknesses addressed by MST-wi.

TABLE 9
Summary of the CWE Top 25 weaknesses addressed by MST-wi.

Security Design
Principle

Audit
Authenticate Actors
Authorize Actors
Cross Cutting
Encrypt Data
Identify Actors
Limit Access
Limit Exposure
Lock Computer
Manage User Sessions
Validate Inputs
Verify Message Integrity
Total

weaknesses
all
6
28
60
9
38
12
8
6
1
6
39
10
223

generic
6
27
55
8
37
12
5
6
1
4
33
10
204

addressed weaknesses

all
1 (16%) 10th
12 (43%) 4th
34 (57%) 3rd
3 (33%) 6th
8 (21%) 8th
3 (25%) 7th
3 (38%) 5th
0 (0%) 11th
0 (0%) 11th
4 (67%) 1st
31 (79%) 2nd
2 (20%) 9th
101 (45%)

generic
1 (16%) 10th
12 (44%) 4th
34 (62%) 3rd
3 (37%) 6th
8 (22%) 8th
3 (25%) 7th
2 (40%) 5th
0 (0%) 11th
0 (0%) 11th
4 (100%) 2nd
29 (88%) 1st
2 (20%) 9th
98 (48%)

are speciﬁc to certain types of systems (e.g., Java Enter-
prise [86]), we distinguish between the results achieved with
all the weaknesses (i.e., generic and speciﬁc), and the results
achieved with the generic weaknesses only.

To better characterize the weaknesses that cannot be
addressed by MST-wi, we analyze the distribution of the
reasons preventing its application, across the categories of
the views considered in our analysis. Finally, we discuss the
percentage of the weaknesses belonging to the CWE Top 25
and OWASP Top 10 lists.

To provide concrete examples of the weaknesses in our
analysis, we report, in Table 7, a subset of the weaknesses
in the CWE view for common security architectural tactics.
We refer to Table 7 in the rest of the section. Columns Design

Weaknesses
generic
all
18
25

Addressed weaknesses
generic
14 (78%)

all
15 (60%)

principle and Weakness report the security design principle
affected by a weakness and its name, respectively. Columns
Belongs to Top 25 and Belongs to Top 10 indicate whether a
weakness also belongs to the CWE Top 25 or the OWASP Top
10 view, respectively. We also indicate if the weakness can
be addressed by MST-wi. The remaining columns refer to
concepts introduced later in this section.

9.3.2 Results

Table 8 presents a summary of the CWE security design
principles and related security weaknesses addressed by
MST-wi. The ﬁrst column in Table 8 lists the security design
principles appearing in the common security architectural
tactics view. The second and third columns give, for each
design principle, the overall number of weaknesses and the
number of generic weaknesses, respectively. The fourth and
ﬁfth columns report the weaknesses that can be automat-
ically discovered by MST-wi among all and generic weak-
nesses (we report the number, percentage, and ranking).

In total, 101 out of all 223 weaknesses (45%) and 98 out
of 204 generic weaknesses (48%) in the view can be ad-
dressed by MST-wi. These numbers show that our approach
enables engineers to automatically discover a large subset
of the weaknesses. Readers can download the details of
our analysis for all 223 weaknesses from our replicability

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

23

TABLE 10
Summary of the security weaknesses for OWASP Top 10 security risks
addressed by MST-wi.

OWASP Security Risk

Broken Access Control

Cryptographic Failures

Injection

Insecure Design

Security Misconﬁguration

and Outdated

Vulnerable
Component
Identiﬁcation and Authentica-
tion Failures
Software and Data Integrity
Failures
Security Logging and Monitor-
ing Failures
Server-Side Request Forgery
(SSRF)

Weaknesses
all
20

generic
18

Addressed weaknesses
generic
15 (83%)

all
15 (75%)

24

21

22

5

0

20

9

4

1

24

18

20

4

0

3 (13%)

3(13%)

18 (86%)

16 (89%)

12(55%)

12 (60%)

3 (60%)

2 (50%)

0 (0%)

0 (0%)

20

11 (55%)

11 (55%)

8

4

1

1 (11%)

1 (13%)

1 (25%)

1 (25%)

0 (0%)

0 (0%)

Total

126

117

64 (51%)

61 (52%)

TABLE 11
Reasons preventing the application of MST-wi.

ID
R1

R2

R3

R4
R5

Reason
The weakness concerns a system that is not Web-based or
mobile-based.
The weakness can be discovered only by means of program
analysis.
It is not possible to distinguish valid and invalid behaviour
based on system output; a human needs to inspect it.
The weakness can be discovered only by means of data analysis.
The weakness can be discovered only by controlling a third-
party component.

package [39]. If we sort security design principles based
on the percentage of weaknesses addressed by MST-wi, we
observe that the rankings for generic and all weaknesses
match except for the ﬁrst two security design principles
in the rankings, which are swapped. These top ranked
security design principles are Validate Inputs (1st for generic
weaknesses, 2nd considering all the weaknesses) and Man-
age User Sessions (2nd for generic weaknesses, 1st consid-
ering all the weaknesses). They are about malicious actors
providing malformed input data (e.g., code injection) to
the system (Validate Inputs), and malicious actors accessing
resources because of session management faults (Manage
User Sessions). The main reason for the difference is that
speciﬁc weaknesses for session management faults include
two weakness (i.e., CWE-6 and CWE-579) related to the se-
rialization of J2EE objects that cannot be discovered through
MT but only through code inspection. Indeed, CWE-6 is
discovered by determining if the session includes a non-
serializable object; CWE-579 can be discovered by checking
if the session ID is stored in a ﬁeld that is shorter than the
one used in the J2EE session object. Since the rest of the
ranking match, in the following discussion, we report only
the percentages for generic weaknesses to simplify reading.
Other security design principles with a high percentage
of weaknesses (above 40%) being detected by MST-wi are
Authenticate Actors and Authorize Actors; they concern mali-
cious actors (external systems or users) accessing resources
they are not authorized to access. These weaknesses are

often discovered through interactions with the system and
therefore they can be tested with MST-wi; MST-wi cannot
cover cases in which program analysis is needed (e.g., Insuf-
ﬁcient Compartmentalization and Reliance on Security Through
Obscurity).

MST-wi addresses a low percentage (below 20%) of the
weaknesses related to the security design principles Audit,
Limit Exposure, and Lock Computer (i.e., 16%, 0%, and 0%
respectively). Such percentages are due to MST-wi relying,
for source inputs, on sequences of user-system interactions.
The weaknesses related to Audit, Limit Exposure, and Lock
Computer are, on the contrary, about quality of recorded
logs, information that the system exposes, and restrictions
of the lockout mechanism (e.g., lock an account after a
predeﬁned number of failed logins). They all require manual
data inspection.

Unsurprisingly, the design principles Authorize Actors
(34 weaknesses), Validate Inputs (31), and Authenticate Actors
(12) also have a high number of weaknesses addressed by
MST-wi. Indeed, they concern interactions between external
actors and the system, which is the main focus of MST-wi.

Table 9 gives a summary of the CWE Top 25 weaknesses
addressed by MST-wi. It can automatically discover 15 out
of the 25 CWE top weaknesses (60%) and 14 out of the 18
generic CWE top weaknesses (78%), which shows that MST-
wi is a key solution to identify widely spread weaknesses.

Among the CWE Top 25 weaknesses, MST-wi cannot
address the ones that require program analysis or interac-
tions with third parties (e.g., other system users or system
administrators) to be detected: Out-of-bounds Write, Out-
of-bounds Read, Improper Neutralization of Special Elements
used in an OS Command, Use After Free, Integer Overﬂow or
Wraparound, Deserialization of Untrusted Data, NULL Pointer
Dereference, Use of Hard-coded Credentials, Improper Restriction
of Operations within the Bounds of a Memory Buffer, Exposure of
Sensitive Information to an Unauthorized Actor, and Server-Side
Request Forgery (SSRF).

Table 10 presents a summary of the security weaknesses
related to the OWASP Top 10 security risks addressed
by MST-wi. It can address 64 out of the 126 weaknesses
(51%) and 61 out of the 117 generic weaknesses (52%)
in this view. It addresses a high percentage (above 55%)
of the weaknesses leading to security risks Broken Access
Control, Injection, Insecure Design, Security Misconﬁguration,
and Identiﬁcation and Authentication Failures (i.e., 75%, 86%,
55%, 60%, and 55%, respectively). These risks are about
unauthorized access to resources, injecting malicious client-
side scripts into a website, leveraging the lack of security
controls (e.g., an unprotected primary channel), gaining
system information thanks to system misconﬁguration, and
bypassing authentication. All involve malicious user-system
interactions.

MST-wi can address none of the weaknesses related
to Vulnerable and Outdated Components, which concern the
use of outdated libraries affected by known vulnerabilities.
Although it would be feasible to implement MRs that detect
failures of speciﬁc components, it is not possible for us
to provide a catalog of MRs that cover all the outdated
libraries in the market. Therefore we excluded them from
our analysis.

24

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 12
Distribution of reasons preventing the application of MST-wi to verify security design principles.

Security Design Principle

Audit
Authenticate Actor
Authorize Actor
Cross Cutting
Encrypt Data
Identify Actors
Limit Access
Limit Exposure
Lock Computer
Manage User Sessions
Validate Inputs
Verify Message Integrity
Total

Weaknesses
not addressed
5
16
26
6
30
9
5
6
1
2
8
8
122

R1

R2

R3

R4

R5

Sum

2
11
6
1
21
2
1
3

7
6
60

1
1

6

3
3
3
4
2
3
3
2
1
1

25

8

15
1
1
1
1
1

1
1

22

5
16
26
6
30
9
5
6
1
2
8
8
122

1
1

3

2
7

Table 11 presents the reasons preventing the applica-
tion of MST-wi to discover weaknesses. MST-wi has been
designed to select source inputs by relying either on a
Web crawler or on manually implemented test scripts that
automate the user-system interactions. Source inputs are au-
tomatically turned into follow-up inputs which are used to
discover vulnerabilities. MST-wi can discover vulnerabilities
that can be exercised through a sequence of interactions.
Therefore, MST-wi cannot be applied when the weakness
concerns a system that is not Web-based, or when it concerns
components of a Web system that exchange data through
dedicated protocols (see R1 in Table 11). Also, some weak-
nesses can be discovered only through program analysis,
which MST-wi does not support (see R2 in Table 11). In
that case, the analysis should be either performed without
actually executing programs (e.g., through code inspection)
or the output of program analysis should be reviewed
manually to eliminate false positives, which typically come
in large numbers [87]. Example cases are reported in Table 7
and concern log ﬁles omitting or inappropriately reporting
information about attempted attacks, relying on a weak
password recovery mechanism, improper validation of cer-
tiﬁcates, conﬁguring a small space for random variables,
lack of encryption, storing objects in session tokens, and
deserializing untrusted data.

In some cases, it is not possible to deﬁne an MR because
a human is needed to inspect the system output (R3).
For instance, to discover weakness Weak Password Recovery
Mechanism for Forgotten Password in Table 7, a human needs
to indicate that the system under test contains a mechanism
for the recovery of passwords that is weak (e.g., it is based
on a security question whose answer can be easily deter-
mined [88]).

Due to the MT foundations (i.e., based on MRs), MST-
wi cannot be applied for weaknesses that can be discovered
only through data analysis (see R4 in Table 11). More pre-
cisely, some weaknesses can be determined only by analyz-
ing large amounts of data (e.g., log ﬁles) based on statistics
or machine learning; this analysis cannot be performed with
an MR.

Finally, some weaknesses can be discovered only by con-
trolling a third-party system (see R5 in Table 11). This is the
case for Improper Validation of Certiﬁcate with Host Mismatch
(see Table 5), which requires setting up a malicious host with

the same IP of the SUT. Expressing such interactions in MRs,
in general, is infeasible.

Table 12 presents the distribution of reasons preventing
the application of MST-wi for the weaknesses regarding
common security architectural tactics. In 60 out of 122
weaknesses (49%) that cannot be discovered by MST-wi,
program analysis is required (see R2). This is expected since
program analysis complements software testing in software
veriﬁcation. R2 is particularly prevalent for the security de-
sign principle Encrypt Data, where 21 out of 30 weaknesses
(70%) are not addressed due to R2. These 21 weaknesses are
associated with the protection of credentials or password
(e.g., hard-coded cryptography key, password in conﬁgura-
tion ﬁle), or the use of encryption/hash algorithm (e.g., hash
without a salt). Therefore, in these cases, a static program
analysis approach should be used; for example, to ﬁnd hard-
coded cryptography keys.

The second most frequent category is R3, which, in most
of the cases, concerns the output generated in exceptional
situation (e.g., messages provided in log ﬁles). R3 uniformly
affects, with one to four cases, all the design principles
except the two principles concerning the handling of input
data (i.e., Validate Inputs and Verify Message Integrity). For
these two cases, static program analysis is more effective
than testing; this is the case for CWE-391 (Unchecked Error
Condition), where it is sufﬁcient to examine the source code
for missing operations following the veriﬁcation of function
results.

The third most frequent category is R1; however, it is
the prevalent reason for not applying MST-wi in the case
of authorization weaknesses (15 out of 26 weaknesses) and
has limited impact on all the other principles (i.e., one or
no cases). Authorization is a generic security property that
goes beyond Web-based systems; therefore, 15 out of these
26 weaknesses (58%) concern functionalities that are not
implemented by Web-systems. They concern the ﬁle system
(e.g., preserving or managing the permissions related to
ﬁles), the process control in an operating system, or the
process communication in a mobile operating system. These
weaknesses cannot be discovered by an automated test
framework dedicated to Web-based interactions.

R4 has a limited impact on almost all the design prin-
ciples except for Encrypt Data, where it prevents MST-wi
from detecting six weaknesses; indeed, in several cases,

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

25

TABLE 13
Distribution of reasons preventing the application of MST-wi to discover weaknesses associated with the OWASP Top 10 security risks.

OWASP Security Risk

Broken Access Control
Cryptographic Failures
Injection
Insecure Design
Security Misconﬁguration
Vulnerable and Outdated Component
Identiﬁcation and Authentication
Software and Data Integrity Failures
Security Logging and Monitoring
Server-Side Request Forgery (SSRF)
Total

Weaknesses
not addressed
5
21
3
10
2
0
9
8
3
1
62

R1

R2

R3

R4

R5

Sum

1

1
2

1
1

6

3
18
2
6
2

4
6
3

44

1
1

2

1
1

6

2

2

3

1
4

5
21
3
10
2
0
9
8
3
1
62

TABLE 14
Distribution of reasons preventing the application of MST-wi to discover CWE Top 25 weaknesses.

Weakness
Out-of-bounds Write
Out-of-bounds Read
Improper Neutralization of Special Elements used in an OS Command (’OS Command Injection’)
Use After Free
Integer Overﬂow or Wraparound
Deserialization of Untrusted Data
NULL Pointer Dereference
Use of Hard-coded Credentials
Improper Restriction of Operations within the Bounds of a Memory Buffer
Server-Side Request Forgery (SSRF)
Total

R1

1

1

R2
1
1

1

1
1

1

6

R3

R4

R5

1

1

2

1
1

0

TABLE 15
Testability features and factors for MST-wi.

ID
TF1
TF2
TF3
TF4
TF5
TF6
TF7
TF8
TF9
TF10

Testability feature
The feature under test is accessible via a URL/path
The testing framework supports modifying parameter values
It is possible to log-in with a predeﬁned list of credentials
System settings or conﬁguration elements can be controlled by the test engineer
The testing framework can control the Web-browser (e.g., click on back button)
The type of the parameters of the request (in URL or post-data) is known or can be easily determined
It is possible to access system artefacts (e.g., log ﬁles)
The system under test provides a feature to conﬁgure the system time
The testing framework supports handling multiple user sessions in parallel
The testing framework has a feature to select certiﬁcates

Testability factor
Controllability
Test support environment
Controllability
Controllability
Test support environment
Controllability
Observability
Controllability
Test support environment
Test support environment

the limitations of encryption algorithms (e.g., Insufﬁcient
entropy) can be discovered only through a statistical test.

R5 is the least frequent reason for not applying MST-wi
because most of the weaknesses are not due to interactions
among multiple components.

Tables 13 and 14 report the reasons preventing the appli-
cation of MST-wi to discover some highly critical vulnerabil-
ities. In these cases as well, the main reason is the necessity
to rely on static program analysis. This is expected since
testing and program analysis are complementary quality
assurance activities.

Summary. As a result of our analysis, we conclude that
MST-wi can address a large percentage (45%) of the weak-
nesses organized in the CWE view for common security
architectural tactics. Such percentage increases to 48% if we
consider generic weaknesses only. MST-wi can also address
most high-risk weaknesses (51% of the weaknesses related
to the OWASP Top 10 security risks and 60% of the CWE
Top 25 weaknesses). These results are promising as they

demonstrate that MST-wi is relevant for a large subset of
vulnerabilities occurring in practice. The weaknesses that
MST-wi cannot address are mostly those (i) that can be
discovered only using program analysis, (ii) that are not
based on user-system interactions, or (iii) that concern non-
Web-based systems.

9.4 RQ3: testability guidelines

9.4.1 Analysis Procedure

This research question investigates the possibility to deﬁne
testability guidelines that support engineers in automati-
cally testing software systems with MST-wi. More precisely,
we aim to identify a set of features (hereafter testability fea-
tures) that should be provided either by the software under
test or by the test framework and environment. Testability
guidelines should indicate which testability features are
required to detect speciﬁc categories of weaknesses, e.g.,
targeting a security design principle or entailing a high risk.

26

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

To identify testability features, we study the weaknesses
that can be discovered by MST-wi, in the CWE view for
common security architectural tactics. For each weakness,
we identify a set of features that are necessary to enable
automated testing with our approach.

The identiﬁcation of features or, more generally, factors
that affect or inﬂuence software testability is the subject
of active research on testability. A recent survey [89] lists
21 testability factors: observability, controllability, complexity,
cohesion, understandability, inheritance, reliability, availability,
ﬂexibility, test suite reusability, maintainability, unit size, state-
fulness, isolateability, software process capability, modularity, test
support environment, fault-proneness, manageability, quality of
the test suite, and self-documentation. To guide engineers to-
wards the inspection of the proposed testability features for
MST-wi, we match each testability feature to the testability
factors in the literature. This allows us to group related
testability features.

Finally, we analyze the distribution of the testability fea-
tures across the security design principles of the CWE view
for common security architectural tactics, the security risks
in the OWASP Top 10 CWE view, and the weaknesses in
the CWE Top 25 view. This analysis should assist engineers
in prioritizing the implementation of testability features for
the system under test, based on the targeted weaknesses and
security design principles.

9.4.2 Results
Table 15 presents the testability features for MST-wi and the
corresponding testability factors. In total, we have 10 testa-
bility features. Six features concern the system under test,
while the other four features concern the test environment
(see testability factor Test support environment).

In the case of MST-wi, three out of the 21 testability
factors in the literature are required; these are (i) Control-
lability (i.e., the degree to which it is possible to control
the state of the component under test [89]), (ii) Observability
(i.e., how easy it is to observe the behavior of a program
in terms of its outputs, effects on the environment, and
other hardware and software components [89]), and (iii) Test
Support Environment. In our context, testability factor Test
Support Environment refers to the capability of the testing
environment or framework to provide features to analyze
system outputs or to alter the inputs transmitted to the
system under test. The required testability factors are mostly
determined by the type of testing performed by MST-wi:
security vulnerability testing at the system level by mim-
icking the actions performed by a malicious user. Therefore,
to determine if the output of the system is correct, MST-
wi may require improved Observability. To test the system
under speciﬁc conﬁgurations, it needs high Controllability,
and to automate activities typically performed by malicious
users manually, the Test Support Environment requires a high
degree of automation.

Before discussing the distribution of testability features
across design principles, we explain some of the testability
features for the weaknesses in Table 7. For instance, weak-
ness Improper Authentication is a generic weakness associated
with design principle Authenticate Actors. It indicates that
the system under test does not properly verify the identity
claimed by an actor [90]. MST-wi can be applied to identify

this weakness when the feature under test is accessible
through a URL/path (see TF1 in Table 15). We match this
testability feature to testability factor Controllability. In weak-
ness Insufﬁcient Session Expiration in Table 7, a Web system
permits malicious users to reuse old session credentials or
session IDs for authorization [91]. MST-wi automatically
identiﬁes this weakness only when it is possible to modify
the values of the parameters passed in HTTP requests (TF2
concerning Test Support Environment), which is supported
by our MST-wi toolset, and when the system under test pro-
vides a feature to conﬁgure the system time (TF8 concerning
Controllability), which is usually feasible through secure shell
connection, a feature leveraged by our toolset). For instance,
an MR in SMRL can modify the HTTP-request (e.g., session
IDs and cookie values) to reuse old session credentials.

Table 16 presents the distribution of the testability fea-
tures across the security design principles in the CWE view
for common security architectural tactics. Please note that
there are more than one testability feature for some of the
weaknesses associated with the security design principles.
An example is weakness Insufﬁcient Session Expiration, which
is associated with testability features TF2 and TF8 (see Ta-
ble 7). TF2 supports the test engineer to reuse an old session
(e.g., credentials or ID) by modifying the corresponding
request parameters; TF8 helps to change the system time
in order to invalidate this session (i.e., make it expired).

In total, we identify 102 testability features for 101
weaknesses concerning the 12 security design principles in
Table 16. Security design principles Authorize Actors and
Validate Inputs are the ones that require the most testability
features; this mostly depends on the fact that they are
related to the largest subset of weaknesses (see Table 8). In
Table 16, rows Total and % of weaknesses show that the two
testability features with the largest number of associated
weaknesses are TF1 and TF2 (22% and 40%, respectively).
These two features are respectively related to testability
factors Controllability and test support environment.

In our analysis, we observe that controllability, test support
environment and observability are required to address 48%,
48% and 4% of the weaknesses, respectively. These numbers
are not fully in line with the literature on the topic, where
the two most popular factors are observability (mentioned
in 101 papers) and controllability (82 papers) [89]. We be-
lieve that this difference is due to the fact that MST-wi
automatically simulates actions performed by a user on a
Web system under speciﬁc conditions (e.g., after performing
a login). It thus requires a high degree of controllability
to exercise the features under test or control the state of
the system, instead of a high degree of observability. On
the other hand, the literature on testability mostly concerns
functional and robustness testing, which requires a high
degree of observability. The high relevance of the testability
factor Test Support Environment for MST-wi is due to the fact
that, to mimic a malicious user, it is necessary to automate
all the actions typically performed manually by malicious
users.

Tables 17 and 18 present the testability features that
enable testing for the weaknesses in the OWASP Top 10 and
CWE Top 25 views, respectively. In Table 17, numbers are in
line with the ones in Table 16. Indeed, the testability features
that are required for testing a higher subset of weaknesses

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

27

TABLE 16
Distribution of testability features for MST-wi.

Security Design Principle

Audit
Authenticate Actors
Authorize Actors
Cross Cutting
Encrypt Data
Identify Actors
Limit Access
Limit Exposure
Lock Computer
Manage User Sessions
Validate Inputs
Verify Message Integrity
Total
% of weaknesses *

TF1
-
2
14
-
-
1
1
-
-
-
4
1
23
22%

TF2
-
3
4
3
1
-
1
-
-
1
27
1
41
40%

TF3
-
2
9
-
1
-
-
-
-
-
-
-
12
12%

Testability Feature

TF4
-
3
2
-
3
-
-
-
-
-
-
-
8
8%

TF5
-
-
1
-
-
-
-
-
-
1
-
-
2
2%

TF6
-
-
2
-
-
-
-
-
-
-
-
-
2
2%

TF7
1
-
1
-
2
-
-
-
-
-
-
-
4
4%

TF8
-
2
-
-
1
-
-
-
-
1
-
-
4
4%

TF9
-
-
-
-
-
-
1
-
-
2
-
-
3
3%

TF10
-
-
1
-
-
2
-
-
-
-
-
-
3
3%

* Percentage of weaknesses that can be discovered thanks to a testability feature.

TABLE 17
Distribution of testability features of MST-wi for the weaknesses associated with the OWASP Top 10 security risks.

OWASP Security Risk

Broken Access Control
Cryptographic Failures
Injection
Insecure Design
Security Misconﬁguration
Vulnerable and Outdated Component
Identiﬁcation and Authentication Failures
Software and Data Integrity Failures
Security Logging and Monitoring Failures
Server-side Request Forgery (SSRF) & Monitoring
Total
% of weaknesses *

TF1
7
-
3
2
-
-
1
-
-
-
13
20%

TF2
3
-
14
3
1
-
3
1
-
-
25
38%

TF3
4
-
-
3
-
-
2
-
-
-
9
14%

Testability Feature

TF5
-
-
-
1
-
-
-
-
-
-
1
2%

TF6
-
-
-
1
-
-
-
-
-
-
1
2%

TF7
1
-
-
2
-
-
-
-
1
-
4
6%

TF4
-
2
-
-
2
-
4
-
-
-
8
12%

TF8
-
1
-
-
-
-
1
-
-
-
2
3%

TF9
-
-
1
-
-
-
1
-
-
-
2
3%

TF10
-
-
-
-
-
-
-
-
-
-
0
0%

* Percentage of weaknesses that can be discovered thanks to a testability feature.

are also TF1 and TF2 in Tables 17. In Table 18, TF2 and TF3
have a signiﬁcant role in testing the features.

Tables 16, 17, and 18 provide testability guidelines for
engineers. They enable engineers to determine, based on
the security requirements of the system under test, which
testability features need to be enabled. For example, if the
system provides authorization and authentication features,
engineers need to implement security design principles
Authenticate Actors and Authorize Actors. Consequently, it
might be useful to ensure that these two features under
test are accessible through a URL/path (TF1), that the test-
ing framework supports both modifying parameter values
(TF2), and that it is possible to log-in with a predeﬁned
list of credentials (TF3). Moreover, TF1, TF2, and TF3 en-
able test automation for highly critical weaknesses, which
concern code injection, authorization, and authentication. In
addition, by looking at the testability features addressing
more than 10% of the vulnerabilities in Tables 16, 17, and 18
(i.e., at least four weaknesses associated with the OWASP
Top 10 risks, two weakness in the CWE Top 25 list, and
ten weaknesses concerning the security design principles),
it is possible to identify a minimal set of testability features
(i.e., TF1, TF2, TF3, and TF4) that should be prioritized to
automatically verify both security design principles and top
security risks. Since TF2 is provided by our MST-wi imple-
mentation and available in manual Web testing frameworks,

and, further, TF1, TF3, and TF4 are common in Web systems,
we conclude that MT is likely applicable in most software
projects without the need for adapting existing design or
testing frameworks.

10 EMPIRICAL EVALUATION

In this section, we investigate, based on two open-source
case studies, the following RQs:

• RQ4. Is MST-wi effective? The goal of this research
question is to assess whether MST-wi enables, in a
reliable manner, the automated detection of security
vulnerabilities.

• RQ5. Is MST-wi scalable? The goal of this research
question is to analyze whether the execution time
entailed by MST-wi is acceptable in practice.

To perform our empirical evaluation we relied on our
implementation of MST-wi, a toolset that extends the Eclipse
IDE [32]. Additional information about the toolset, includ-
ing executable ﬁles, download instructions, and a screen-
cast, is available on the tool’s website at https://sntsvv.
github.io/SMRL/. A replicability package is provided as
well [39].

28

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 18
Distribution of testability features of MST-wi for the CWE Top 25 weaknesses.

CWE Top 25 Weakness

Improper Neutralization of Input During Web Page Generation (’Cross-site Script-
ing’)
Improper Input Validation
Improper Neutralization of Special Elements used in an SQL Command (’SQL
Injection’)
Improper Limitation of a Pathname to a Restricted Directory (’Path Traversal’)
Cross-Site Request Forgery (CSRF)
Unrestricted Upload of File with Dangerous Type
Missing Authentication for Critical Function
Improper Authentication
Missing Authorization
Incorrect Default Permissions
Exposure of Sensitive Information to an Unauthorized Actor
Insufﬁciently Protected Credentials
Incorrect Permission Assignment for Critical Resource
Improper Restriction of XML External Entity Reference
Improper Neutralization of Special Elements used in a Command (’Command
Injection’)
Total
% of weaknesses *

TF1
-

TF2
1

TF3
-

TF4
-

TF5
-

TF6
-

TF7
-

TF8
-

TF9
-

TF10
-

Testability Feature

-
-

1
-
-
-
-
-
-
1
-
1
-
-

1
1

-
1
-
-
-
-
-
-
-
-
1
1

-
-

-
-
-
1
1
1
1
-
1
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

-
-

-
-
1
-
-
-
-
-
-
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

-
-

-
-
-
-
-
-
-
-
-
-
-
-

3
20%

6
40%

5
33%

0
0%

0
0%

1
7%

0
0%

0
0%

0
0%

0
0%

* The percentage of weaknesses that can be discovered thanks to the testability feature.

10.1 Subjects of the Evaluation

We applied MST-wi to Jenkins [37], a leading open source
automation server, and Joomla [38], a popular open source
content management system. The two case study subjects
differ in programming languages and underlying frame-
works. Jenkins is a Java Web application that we can execute
within any servlet container [92]; Joomla is a PHP applica-
tion that relies on the MySQL RDBMS [93] and the Apache
HTTP server [94]. We chose them because they represent
modern Web systems having a plug-in architecture and Web
interfaces with advanced features such as Javascript-based
login and AJAX interfaces. Their differences in program-
ming languages and types of inputs may lead to differ-
ent vulnerabilities and contribute to the generalizability of
our empirical results. We used Jenkins version 2.121.1 and
Joomla version 3.8.7. We selected the Jenkins and Joomla
versions affected by all the vulnerabilities triggerable from
the Web interface, discovered in 2018 and reported in the
Common Vulnerabilities and Exposures (CVE) database [95]
after June 1st, 2018. Jenkins 2.121.1 and Joomla 3.8.7 are
affected by 20 and 16 vulnerabilities, respectively. Our cat-
alog of MRs addresses 40% (8 out of 20) and 31% (5 out
of 16) of the vulnerabilities affecting Jenkins and Joomla,
respectively. This outcome is consistent with our analysis in
RQ1. Among these vulnerabilities, we selected only the ones
(eight vulnerabilities for Jenkins and two for Joomla) whose
effects could be manually reproduced in our environment
(i.e., we could observe a security failure). For instance, we
could replicate only two vulnerabilities for Joomla due to
the lack of a detailed description of the attack scenarios.
(one is a software fault that received a CVE identiﬁer [46],
the other two are related to Jenkins’ default conﬁgura-
tion). In addition, we considered, for Jenkins, three new
vulnerabilities we discovered with MST-wi. We have, for
Joomla, one new additional conﬁguration-related vulnera-
bility discovered by MST-wi. To summarize, we considered
11 vulnerabilities for Jenkins and 3 for Joomla (see Table 19).

In the table, we provide two CWE IDs when the CVE report
refers to a generic vulnerability type (e.g., CWE 863 for
incorrect authorization in reference [96]), but a more speciﬁc
one ﬁts better (e.g., CWE 280 about improper handling of
privileges that may lead to incorrect authorization).

We conﬁgured, for each subject, our data collection
framework with multiple users having different roles. We
used four credentials for Jenkins and six credentials for
Joomla. We executed, for each role, the data collection
framework to crawl the system under test for a maximum
of 300 minutes. The data collection took 1000 minutes for
Jenkins and 2280 minutes for Joomla. Crawljax completed
the crawling in less than 300 minutes for the anonymous
role in Jenkins and Joomla because it visited all states. The
data collection time for Joomla was long because it has
two different user interfaces (i.e., user and administrative
interfaces). The crawling led to 156 and 147 input sequences
for Jenkins and Joomla, respectively. Also, we implemented
Selenium-based test scripts (two for Jenkins and one for
Joomla) to exercise use cases not covered by Crawljax. These
scripts entail a small overhead but address limitations in the
crawler (cost-beneﬁt trade-off).

10.2 RQ4: effectiveness

10.2.1 Experiment design

An automated testing approach is effective if it helps detect
a large proportion of the faults affecting the software under
test and generates a limited number of false alarms. Ideally,
MST-wi should identify all the vulnerabilities affecting our
case study subjects that can be reproduced in our environ-
ment. We executed MST-wi with the two case study subjects,
considering all the MRs in our catalog. For each MT failure
reported by MST-wi, we manually veriﬁed if the test input
actually triggered any vulnerability (true positive).

We measured speciﬁcity and sensitivity [109]. Speciﬁcity
(true negative rate) is the ratio of follow-up inputs not
triggering any vulnerability that (correctly) does not lead

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

29

TABLE 19
Vulnerabilities considered in our empirical evaluation.

TABLE 20
Summary of RQ4 results grouped by data collection method.

Subject

Ref.
[96]

Vuln. Type
CWE 863,
CWE 280

Jenkins

[97]

CWE 863,
CWE 285

[98]

CWE 200,
CWE 668

[99]

CWE 22

[100]

CWE 200

[101]

CWE 384

[102]

CWE 521

[103]

CWE 262

[104]

CWE 79

[105]

CWE 863

[106]

CWE 287

[107]

CWE 863

Joomla

[108]

CWE 200

[103]

CWE 262

Description
Jenkins does not perform a permis-
sion check for URLs handling can-
cellation of queued builds, allowing
users with Overall/Read permission
to cancel queued builds.
Jenkins does not perform a permis-
sion check for the URL that initiates
agent launches, allowing users with
Overall/Read permission to initiate
agent launches.
Files indicating when a plugin ﬁle
was last extracted into the Jenkins
plugins/ directory are accessible
via HTTP by users having Over-
all/Read permissions. This allows
unauthorized users to determine the
likely install date of a given plugin.
In the ﬁle name parameter of a Job
conﬁguration, users with Job/Con-
ﬁgure permissions can specify a rela-
tive path escaping the base directory.
Such path can be used to upload a
ﬁle on the Jenkins host, resulting in
an arbitrary ﬁle write vulnerability.
Users with Overall/Read permis-
sion ar able to access the URL serv-
ing agent logs on the UI due to a lack
of permission checks.
Jenkins does not invalidate the ex-
isting session when a user signs up
for a new user account. This allows
session ﬁxation.
Jenkins does not
require users
to have strong passwords, which
makes it easier for attackers to com-
promise user accounts.
Jenkins does not integrate any mech-
anism for managing password ag-
ing; consequently, users aren’t incen-
tivized to update passwords period-
ically.
set Content-
Jenkins does not
Security-Policy headers for ﬁles up-
loaded as ﬁle parameters to a build,
resulting in a stored XSS vulnerabil-
ity.
Users with Overall/Read permis-
sion can access the URL used to can-
cel scheduled restart jobs initiated
through the update center due to a
lack of permission checks.
Users with a valid cookie can re-
main logged in even if the Remem-
ber me feature has been disabled in
the Jenkins conﬁguration.
Inadequate checks on the tags search
ﬁelds can lead to an access level vio-
lation.
Inadequate checks allow users to see
the names of tags that were either
unpublished or published with re-
stricted view permission .
Joomla does not integrate any mech-
anism for managing password ag-
ing; consequently, users aren’t incen-
tivized to update passwords period-
ically.

Case study

Jenkins

Discovered
Vulnerabilities Speciﬁcity Sensitivity Speciﬁcity Sensitivity
81.81%

Crawljax & Manual

Crawljax

63.64%

99.94%

99.90%

[96],

[97],
[99],
[101],
[103],

[98],
[100],
[102],
[104]

Joomla

[103], [107],

99.79%

66.67%

99.71%

100.00%

Overall

[108]
12

99.87%

64.29%

99.81%

85.71%

Fig. 21. RQ4: Speciﬁcity distribution (values in Table 21)

10.2.2 Results

Table 20 summarizes the results obtained with the two data
collection methods supported by MST-wi, i.e., based on
Crawljax only or integrating Crawljax and manual scripts.

We observe that the approach has extremely high speci-
ﬁcity when relying on the crawler (99.87%) and when com-
bining the crawler and manual inputs (99.81%). The high
speciﬁcity indicates that only a negligible fraction of follow-
up inputs leads to false alarms (121 out of 93359, 0.13%, and
103 out of 55174, 0.19%). False alarms are due to limitations
in Crawljax, which, for Jenkins, did not traverse all the
URLs provided by the GUI for all users. Consequently, MRs
concerning authorization vulnerabilities fail. However, it is
easy to determine that the URLs causing the false alarms
should be accessible to all users.

Fig. 21 shows boxplots presenting the distribution of
speciﬁcity across MRs. Speciﬁcity is high for every MR, with
the median being 100%. The lowest whisker3 is 99.94% for
Joomla with Crawljax only, which indicates that, without

3. Computed as ﬁrst quartile − 1.5 ∗ Inter Quartile Range

TABLE 21
RQ4: Speciﬁcity distribution

to any MT failure. In other words, 1 - speciﬁcity is the false
positive rate and measures the proportion of unwarranted
MT failures. We reported the overall speciﬁcity of MST-wi
(across all the follow-up inputs generated in our experi-
ment) and the speciﬁcity distribution across MRs. Sensitivity
(true positive rate) is the ratio of vulnerabilities discovered.

First Quarltile
Second Quarltile
Third Quarltile
Lower whisker
Upper whisker

Jenkins

Joomla

Crawljax
100
100
100
100
100

C. & Manual
100
100
100
100
100

Crawljax
99.94
100
100
99.84
100

C. & Manual
100
100
100
100
100

30

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

outliers, the minimum speciﬁcity is above 99%. In practice,
for all our MRs, only a very small proportion of follow-up
inputs leads to false alarms.

is

(ﬁve

outlier

speciﬁcity

The worst

84.38% for
OTG_INPVAL_004 with Jenkins
false positives
out of 32 follow-up inputs tested). The false positives are
mainly due to asynchronous actions in the source inputs,
which remain to be completed when the follow-up input is
executed and thus lead to different outputs for the source
and follow-up inputs, making the MR fail. However, the
ﬁve false positives lead to a limited waste of developers’
time. Indeed, to discover these false positives, it is sufﬁcient
to manually test the URL of the original and the follow-up
action, which does not take more than ten minutes in total.

Sensitivity is high when data collection relies on both
Crawljax and manual test scripts: 81.81% for Jenkins and
100% for Joomla. Since sensitivity reﬂects the fault detection
rate (i.e., the proportion of vulnerabilities discovered), we
conclude that our approach is highly effective. Overall,
MST-wi detects 85.71% of the vulnerabilities targeted in our
evaluation. It misses two of the eight targeted vulnerabil-
ities in Jenkins. We can reveal one missing vulnerability
only if the server conﬁguration is modiﬁed during test
execution [106]. Unfortunately, our toolset does not support
the server conﬁguration during test execution. We cannot
reproduce the other missing vulnerability since it concerns
the termination of Jenkins’ reboot [105], which is not inter-
ruptible when Jenkins is not overloaded (our case).

When the data collection relies on Crawljax only, sensi-
tivity drops below 70% for both Jenkins and Joomla. The
low sensitivity occurs because of the incapability of our
crawler to exercise some particular interactions. For exam-
ple, Jenkins requires quick system interactions to exercise
some features (e.g., writing a valid Unix command in a
textbox to enqueue a batch job and then quickly pressing a
button to delete it from the queue). Joomla, instead, requires
interactions with a widget showing all the available tags
(in the presence of multiple widgets, Crawlajx may fail
to systematically exercise all the widgets). However, even
when the data collection is based on Crawljax only, with 9
out of 14 (64.29%) vulnerabilities detected, we nevertheless
consider the overall fault detection rate is satisfactory. In-
deed, automatically detecting 64% of the vulnerabilities not
targeted by state-of-the-art approaches, without the need for
any manual test script, is beneﬁcial.

The beneﬁts of MST-wi mostly stem from the MRs in our
catalog being reusable to test any Web system. Furthermore,
the required manual test scripts are few and inexpensive
to implement. For the Web systems above, we manually
wrote 5 test scripts which only contain 31 actions in total.
This manual effort is negligible compared to 93359 input
sequences (544806 actions) automatically generated by our
approach to test the two systems when Crawljax and man-
ual inputs are combined. A traditional way to verify the
same scenarios would require 93359 manually implemented
test scripts, each providing a distinct input sequence and
a dedicated oracle (e.g., an assertion statement). Therefore,
we conclude that MST-wi provides an advantageous cost-
effectiveness trade-off compared to current practice.

Fig. 22. RQ5. Box-plot reporting execution time in hours (values in
Table 22)

TABLE 22
RQ5: Distribution of execution time (hours)

Jenkins

Joomla

Crawljax
0.06
0.38

C. & Manual
0.11
0.34

Crawljax
0.05
0.28

C. & Manual
0.084
0.22

2.63
0.017
6.48
6

2.38
0.017
5.80
5

2.01
0.017
4.95
5

3.03
0.017
7.44
3

First Quarltile
Second
Quarltile
Third Quarltile
Lower whisker
Upper whisker
# of MRs tak-
ing more than
14 hours

10.3 RQ5: scalability

10.3.1 Experiment design

The execution time of MST-wi depends on the time required
to run the crawler and the time required to execute the MRs.
The execution time of crawling depends on the number
of user roles to be tested and the number of actions (i.e.,
inputs that can be provided through links and Web forms on
different Web pages) for the SUT. However, in our context,
the number of user roles has a limited impact because
crawling can be parallelized. Since the development of an
efﬁcient crawler is out of the scope of this paper, we did
not perform an empirical evaluation of the scalability of our
crawler (i.e., the extended Crawljax). The interested reader
is referred to the original Crawljax publication for further
discussions [33]. In this RQ, we discuss the time required to
execute the MRs in our catalog.

We aimed to determine if MST-wi is efﬁcient enough
to be used in practice and what are the main factors that
inﬂuence MRs’ execution time. To do so, we kept track of
the time required to execute each MR considered for RQ4
and discussed the execution time distribution across MRs.

By design, the time required to execute an MR is driven
by the number of source inputs and follow-up inputs exe-
cuted, the number of actions belonging to source and follow-
up inputs, and the complexity of the MR executed (e.g.,
the number of instructions in the MR and computational
complexity of the functions invoked by the MR). Finally,
the hardware components used to run the Web server can

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

31

have different response times and, consequently, affect the
execution time of the MR.

Based on the above, we studied the correlation between
execution time and the number of source inputs, follow-
up inputs, and actions in follow-up inputs by relying on
the non-parametric Spearman’s correlation coefﬁcient. As
for hardware, we did not study the effect of different
hardware conﬁgurations on MST-wi scalability but executed
our experiments in scenarios that we consider feasible for
testing software with MST-wi: a virtual machine installed on
professional desktop PCs (Dell G7 7500, RAM 16Gb, Intel(R)
Core(TM) i9-10885H CPU @ 2.40GHz) and terminal access
to a shared remote server with Intel(R) Xeon(R) Gold 6234
CPU (3.30GHz) and 8 CPU cores.

10.3.2 Results

Fig. 22 provides boxplots reporting the execution time dis-
tribution, in hours, for the MRs executed to address RQ4.
The highest third quartile is 3.03 hours and indicates that
75% of the MRs can be executed for both systems overnight
or in half a working day (i.e., less than four hours) if
we parallelize the executions of the MRs (e.g., by running
each on a dedicated virtual environment). Further, when
excluding outliers, the maximum execution time is about
seven hours and a half for Joomla tested with manual and
Crawljax-based inputs (see Upper whisker in Table 22), which
indicates that most MRs can be executed overnight. Consid-
ering that security testing is currently performed manually
and is error-prone, we believe that the cost of setting up
parallelized execution environments is justiﬁed.

From Table 22, we can see that, across conﬁgurations (i.e.,
executing MST-wi with inputs derived by relying on Crawl-
jax only or by combining Crawljax and manual inputs),
between four and six MRs could not be executed overnight
because they took more than 14 hours. These MRs look
for injection, authorization, and authentication problems
by relying on a catalog of crafted values passed to URL
parameters or form inputs. In practice, they test the same
URL multiple times with different users and inputs, leading
to a combinatorial explosion. However, in our framework,
it is possible to parallelize the execution of a single MR
across multiple nodes (e.g., virtual machines running on
a Cloud or grid infrastructure); each node iterates over
a subset of source inputs. With ten execution nodes for
each of these six MRs, it should be feasible to execute
them overnight. For example, we parallelized the execution
of the MR leading to the worst-case execution time (i.e.,
CWE_138..._OTG_AUTHZ_001b in Jenkins) and observed
a maximum execution time of 14 hours, thus conﬁrming our
assumption.

Table 23 reports, for our experiments, the Spearman’s
coefﬁcients capturing the correlation between execution
time and the number of (a) source inputs, (b) follow-up
inputs, and (c) actions in follow-up inputs. We observe
that, as expected, the execution time signiﬁcantly correlates
(i.e., coefﬁcient above 0.5 and p-value below 0.05) with the
number of actions in follow-up inputs; indeed, each action
leads to a request execution in the browser. We also note
a correlation with the number of follow-up inputs, which
is lower for the case of Jenkins. Such low correlation for
Jenkins is observed when testing with source inputs derived

from both Crawljax and manual test cases. Indeed, manual
test cases include a smaller set of actions than the ones
collected by the crawler, but they test URLs also covered
by the source inputs derived by the crawler. Consequently,
since several MRs do not test the same URL twice, MST-wi,
when data collection is based on Crawljax and manual test
cases, tends to generate follow-up inputs that are shorter
and, therefore, quicker to execute (the long follow-up inputs
derived from the Crawljax source inputs are not executed
because they include URLs already tested by relying on the
short manual source inputs). We do not observe the same
trend in Joomla, likely because the crawled source inputs
contain fewer actions (the average number of actions for
each source input is 5.9 for Joomla and 6.7 for Jenkins).
Finally, the correlation between the execution time and the
number of source inputs is more limited for Jenkins and
not signiﬁcant for Joomla. This limited correlation is mainly
due to source inputs not leading to follow-up inputs (e.g.,
because a precondition does not hold) and, consequently,
causing execution times that vary a lot across MRs even if
the number of source inputs processed by these MRs is the
same.

10.4 Threats to Validity

10.4.1 Internal validity
To minimize implementation errors, we have carefully in-
spected and tested the MST-wi toolset before running our
experiments. Also, we executed our MRs on DVWA [110],
a security benchmark, thus ensuring that our MRs can
discover some of the targeted vulnerabilities (injection vul-
nerabilities of different kinds, in this case).

10.4.2 Conclusion validity

The underlying distribution of the data (i.e., execution time
and the number of executed source inputs, follow-up inputs,
and actions in follow-up inputs) is not known in our context.
Therefore, we relied on Spearman’s rank correlation, which
is non-parametric, to avoid violating the assumptions of
parametric tests for correlation analysis.

In our context, the sources of random factors affecting
results might be (i) the workload of the machines used
to run the experiments (slowing down the performance
of both MST-wi and the case study subjects) and (ii) the
presence of other users interacting with the software under
test (affecting both execution time and system outputs). To
mitigate the effects of these sources, we run the experiments
in dedicated environments with the study subjects used
only by our framework.

10.4.3 Construct validity
We discuss construct validity in terms of face, content, con-
vergent, and predictive validity [111].

The constructs we considered in our work are effective-
ness and scalability. Effectiveness is measured through two
reﬂective indicators, which are sensitivity and speciﬁcity.
Scalability is measured in terms of execution time. Concern-
ing face validity, we believe our indicators are appropriate.
Indeed, sensitivity is the ratio of vulnerabilities discovered
and corresponds to the fault detection rate, commonly used
in software testing papers to evaluate testing effectiveness.

32

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 23
RQ5: Spearman correlation

Set of features\System under test

Execution time and number of source inputs
Execution time and number of follow-up inputs
Execution time and number of actions in follow-up inputs

Jenkins

Joomla

Crawljax

Spearman
0.39
0.52
0.51

P-value
0.00076
1.91e-06
4.19e-06

Crawljax & Manual
P-value
Spearman
0.00071
0.38
0.0017
0.36
0.00013
0.43

Crawljax

Spearman
0.16
0.56
0.52

P-value
0.20
4.15e-07
3.41e-06

Crawljax & Manual
P-value
Spearman
0.18
0.17
7.15e-07
0.55
5.72e-06
0.52

Speciﬁcity captures the proportion of follow-up inputs not
leading to failures (i.e., a large majority for stable systems
like Jenkins and Joomla) that, correctly, do not trigger any
failure report in MST-wi. Speciﬁcity captures developers’
time wasted on investigating false alarms. Indeed, develop-
ers’ time wasted is proportional to the number of follow-up
inputs checked in vain. Execution time is a direct measure
that enables us to assess if, for systems similar to our case
study subjects, MST-wi is efﬁcient enough to be integrated
within software development.

Content validity concerns the breadth of the construct.
High sensitivity (i.e., fault detection rate) is a condition
for a software testing technique to be useful. However,
it does not imply that engineers can understand the root
cause of a failure. Successful root cause analysis depends
on other factors such as the software engineer’s experience
and knowledge about the software under test and the avail-
ability of appropriate debugging and logging tools, which
is out of scope for this paper. The false positive rate, a
measure of the effect of false alarms, is equal to 1 - speciﬁcity
and is complementary to sensitivity; therefore, we believe
our choice of measurements to be complete. To discuss
scalability, we rely on the number of source inputs, follow-
up inputs, and actions in follow-up inputs; they capture the
complexity of the SUT since they reﬂect the actions that
might be performed by an attacker and are automatically
derived through a crawler exercising the system under test.
Other metrics for the size of the SUT (e.g., lines of code,
number of Web pages) may not capture SUT complexity
(e.g., knowing the number of Web pages is insufﬁcient to
understand how many form inputs might be submitted to
the SUT). Therefore, we believe that studying the correlation
between our selected metrics and execution time is adequate
to discuss the scalability of the approach.

≥

≤

Spearman’sρ

About convergence,

the number of source inputs is
weakly correlated to the number of follow-up inputs (i.e.,
0.14
0.23 for all the subjects). Several
source inputs ﬁltered by MRs’ preconditions may explain
this weak correlation. The number of follow-up inputs is
strongly correlated with the number of actions in follow-up
inputs (i.e., Spearman’s ρ
0.85 for all the subjects), which
is expected since we derive follow-up inputs by traversing
the crawler graph with DFS. Such a strong correlation
indicates that the number of actions that can be performed
from the root Web page to a leaf functionality (i.e., the last
page required to execute a functionality) tends to be similar.
However, a positive correlation is expected for reﬂective
indicators used to infer the same construct.

≥

As for predictive validity, we reported statistics for RQ5.

10.4.4 External validity

To strengthen the generalizability of our conclusions for
RQ4, we selected systems that are representative of mod-
ern Web systems but different from technical and process
aspects. For RQ5, we executed our experiments by relying
on hardware commonly available to Web and security engi-
neers (i.e., laptops, virtual machines, and web servers).

11 RELATED WORK

This section covers the related work across three categories:
(i) software security testing, (ii) model-based security testing,
and (iii) metamorphic testing. In the ﬁrst category, we present
existing security testing strategies and discuss how MST-
wi complements speciﬁc security testing techniques. Model-
based security testing is one of the standard security testing
methods [112], [113]. Although it is a relatively new research
ﬁeld, many model-based approaches have been published
lately and we therefore present and compare them with
MST-wi. Finally, in the last category, we discuss the MT
techniques since they represent the foundation on which
MST-wi has been deﬁned.

11.1 Software Security Testing

Security testing approaches can be categorized [2] as fol-
lows: (i) security functional testing validating whether the
speciﬁed security properties are implemented correctly, and
(ii) security vulnerability testing mimicking attacks that
target typical system vulnerabilities. MST-wi can be applied
to both security functional and vulnerability testing since
MRs can capture both security properties (e.g., a login
screen should always be shown after a session timeout) and
properties of the inputs and outputs involved in discovering
a vulnerability (e.g., admin pages are accessed without
authentication).

Many security vulnerability testing approaches rely on
an implicit test oracle, i.e., one that relies on tacit knowl-
edge to distinguish between correct and incorrect system
behavior [1]. It is the case for approaches targeting buffer
overﬂows, memory leaks, unhandled exceptions, and denial
of service [15], [114], [115], most of which rely on mutational
fuzzing [57], i.e., the generation of new inputs through
the random modiﬁcation of existing ones. For instance,
Bekrar et al. [114] present a technique combining mutational
fuzzing with data tainting and coverage analysis to iden-
tify security vulnerabilities in ﬁle processors and network
protocols. Implicit oracles deal with simple abnormal sys-
tem behavior, such as unexpected system termination, and
are system-agnostic. The literature, however, lacks implicit
oracles for vulnerabilities leading to invalid outputs (e.g.,
providing data to a user who is not supposed to access

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

33

it); indeed, such outputs are system-speciﬁc and difﬁcult to
capture with the mechanisms used for implicit oracles (e.g.,
runtime exceptions).

Vulnerability testing approaches for code injections also
suffer from the oracle problem [116], [117], [118], [119], [120],
[121], [122], [168]. For instance, test cases targeting SQL
injections are in the form of HTTP requests that trigger
responses from a Web application while a crawler receives
responses in which some predeﬁned keywords (e.g., “in-
valid”) are searched [168]. When no keywords are detected,
the crawler cannot determine whether the injection is ﬁl-
tered by the system under test. To resolve this problem,
Huang et al. [123] proposed an MT-like technique that sends
multiple HTTP requests, i.e., one request with an injection,
an intentionally invalid request, and a valid request. They
compare the responses to determine if the request with the
injection is ﬁltered. Unfortunately, MT-like approaches that
address a broader set of security vulnerabilities are missing.
In contrast, MST-wi targets a wide variety of security vulner-
abilities (including code injection vulnerabilities); a detailed
analysis of security vulnerabilities that can and cannot be
addressed by our approach was provided in Section 9.

11.2 Model-based Security Testing

Model-based approaches [112], [169] mostly target secu-
rity vulnerability testing (e.g., References [129], [131], [132],
[133], [134], [135], [136], [138], [139], [140], [170], [171], [172],
[173]), whereas some solutions address security functional
testing (e.g., References [124], [125], [126], [127], [128], [130],
[141]). Most of them only generate test sequences from
security models and do not address the oracle problem. For
instance, Marback et al. [134] propose a model-based secu-
rity testing approach that automatically generates security
test sequences from threat trees.

Model-based approaches that generate test cases with
oracles [139], [140], [141] rely on mappings between model-
level abstractions (i.e., tokens in markings of PrT networks)
and executable code implementing the oracle logic (e.g.,
searching for error messages in system output). For instance,
Xu et al. [139], [140] automatically generate executable
vulnerability test cases from formal threat models. Further,
model-level test oracles (tokens in markings of attack paths)
are directly mapped to implementation-level code. But such
mapping is not feasible when it is difﬁcult to specify precise
test oracles and automatically compare expected values to
the actual results. The same problem also affects another
approach that targets access control policies [141]. Overall,
these approaches do not free engineers from signiﬁcant
implementation effort since they require the manual imple-
mentation of the executable oracle code.

11.3 Metamorphic Testing

With MT, we aim to address the limitations of security
testing approaches described above. Indeed, MT supports
oracle automation thanks to MRs that can precisely capture
the relations between test inputs and outputs. Considerable
research has been devoted to developing MT approaches
for various domains such as computer graphics (e.g., [19],
[20], [21], [22]), simulation (e.g., [145], [146], [147]), Web
services (e.g., [23], [24], [25]), embedded systems (e.g., [26],

[27], [28], [29]), compilers (e.g., [149], [174]), variability and
decision support (e.g., [151], [152], [153], [175]), bioinformat-
ics (e.g., [154], [155], [176]), numerical programs (e.g., [156],
[157]), and machine learning (e.g., [158], [159]).

Although there is considerable research devoted to MT,
very little attention has been paid to its application in
security testing [18]. Preliminary applications of MT to
security testing focus on the functional testing of security
components (i.e., testing encryption programs in the absence
of oracles [163], verifying the output of code obfuscators and
the rendering of login interfaces [30]), and the veriﬁcation of
low-level properties broken by speciﬁc security bugs (e.g.,
the heartbleed bug [31] which affects the relation between
the size of the payload data ﬁeld of an SSL message and the
length declared in the same message). Although these works
demonstrate the feasibility of MT for security testing, they
focus on a narrow set of vulnerabilities and do not automate
the generation of executable metamorphic test cases, which
are manually implemented based on the identiﬁed MRs. In
contrast, MST-wi supports the speciﬁcation of MRs for many
vulnerabilities and automates the generation of executable
metamorphic test cases from the MRs.

Although MT is highly automatable, MT research has
mostly focused on the application of MT to speciﬁc testing
problems [18]. For instance, Kuo et al. [28] report on a
case study using metamorphic testing to detect faults in a
wireless metering system. Ding et al. [146] present a case
study for fault detection in a Monte Carlo modeling pro-
gram simulating photon propagation. Chen et al. [156] focus
on the application of MT to programs implementing partial
differential equations. These works do not provide any sys-
tematic method to specify MRs and proper tool support. In
general, very few approaches provide tool support enabling
engineers to write system-level MRs [18]. Those who do
require that MRs be deﬁned either as Java methods [177] or
pre-/post-conditions [178], which limits the adoption of MT
to verify system-level security properties. Since MRs often
employ a declarative notation, engineers need signiﬁcant,
additional effort to translate abstract, declarative MRs into
an imperative programming language. To avoid such over-
head, we propose a DSL as part of MST-wi (see Section 4).
Segura et al. [161], [162] provide a template-based approach
for the description of MRs. The proposed template aims
to ease communication among practitioners but does not
support security-related language constructs. Further, there
is no automated support to transform template-based MRs
into executable test cases.

The notion of metamorphic property (e.g., the permutative
property that speciﬁes that the order of inputs should not
affect the output) introduced by Murphy et al. [179] form the
basis for general metamorphic relations, which are analogous
to metamorphic relation patterns. Zhou et al. [180] deﬁne
the notion of metamorphic relation pattern (MRP) as an
abstraction that characterizes a set of (possibly inﬁnitely many)
metamorphic relations. Subclasses of MRPs are proposed in
the literature: metamorphic relation input pattern (MRIP) [180]
and metamorphic relation output pattern (MROP) [160]. An
MRIP is an abstraction characterizing the relations among
the source and follow-up inputs of a set of metamorphic
relations; an MROP describes an abstract relation among
the source and follow-up outputs.

34

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

TABLE 24
Summary and comparison of related work.

Support for
various vul-
nerabilities

No need for im-
plicit oracles

Model-based test
generation includ-
ing oracle

No need for model-
based mappings or
manual oracle im-
plementation

Application
of
MT to security
testing

DSL support
to
specify
MRs

y
t
i
r
u
c
e
S
e
r
a
w

t
f
o
S

g
n
i
t
s
e
T

d
e
s
a
b
-
l
e
d
o
M

g
n
i
t
s
e
T
y
t
i
r
u
c
e
S

c
i
h
p
r
o
m
a
t
e
M

g
n
i
t
s
e
T

+
−
+
+

MST-wi
Ognawala et al. [15]
Bekrar et al [114]
Takanen et al. [115]
Kals et al. [116]
−
Martin et al. [117]
−
Bau et al. [118]
−
Appelt et al. [119]
−
Salas et al. [120]
−
Tripp et al. [121]
−
Appelt et al. [122]
−
Huang et al. [123]
NA
Le Traon et al. [124]
Mouelhi et al. [125], [126] NA
NA
Martin et al. [127]
NA
Martin et al. [128]
Wimmel and J ¨urjens [129] NA
NA
Masood et al. [130]
+
Bertolino et al. [131]
+
Blome et al. [132]
+
He et al. [133]
+
Marback et al. [134]
+
J ¨urjens et al. [135]
+
Xu et al. [136]
+
Lebeau et al. [137]
+
Whittle et al. [138]
+
Xu et al. [139], [140]
NA
Xu et al. [141]
Mayer and Guderlei [19] NA
Just and Schweiggert [21] NA
NA
Kuo et al. [22]
NA
Jameel et al. [142]
NA
Chan et al. [143], [144]
NA
Chen et al. [145]
NA
Ding et al. [146]
NA
Murphy et al. [147]
NA
Sim et al. [148]
NA
Chan et al. [23]
NA
Sun et al. [24]
NA
Zhou et al. [25]
NA
Tse et al. [26]
NA
Chan et al. [27]
NA
Kuo et al. [28]
NA
Jiang et al. [29]
NA
Tao et al. [149]
NA
Yao et al. [150]
NA
Segura et al. [151], [152]
NA
Kuo et al. [153]
NA
Chen et al. [154]
NA
Pullum et al. [155]
NA
Chen et al. [156]
NA
Aruna and Prasad [157]
NA
Xie et al. [158]
NA
Murphy et al. [159]
NA
Segura et al. [160]
NA
Segura et al. [161], [162]
−
Chen et al. [30]
−
Sun et al. [163]
NA
Luu et al. [164]
NA
Lascu et al. [165]
NA
Ayerdi et al. [166]
NA
Xu et al. [167]

+
−
−
−

+
+
+
+
+
+
+
+
+
+
+
+
+
+
−
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+

NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
−
−
−
−
−
−
−
−
−
−
−
−
−
−
+
+
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA

+
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
−
−
−
−
−
−
−
+
−
−
−
−
−
−
−
−
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+

+
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
+
+
−
−
−
−

+
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
+
−
−
−
−
−
−

Publicly
Available
Tool
support
for MT
+
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
−
−
−
−
−
−
−
−
−
−
+
+
−
−
−
−
+
−
+
−
−
−
−
−
−
−
+
−
−
−
−
+
+
−

Segura et al. [160] propose six MROPs (equivalence,
equality, subset, disjoint, complete, and difference) for test-
ing RESTful web APIs (implementing create, read, update,
or delete operations over a resource). In our MR catalog, we
leverage some of these patterns to deﬁne output conditions;
in particular, our MRs verify equality, difference, and sub-
set (i.e., what we achieve with userCanRtrieveContent,
which checks if an output is a subset of what already ob-
served in previous executions). Segura et al. [181] also deﬁne
a catalog of MRPs for query-based systems, which focus on

the properties of inputs being conditions (e.g., the keywords
used when executing a search engine, which can be joined)
and outputs being data sets (e.g., the results returned by the
search engine, which can be disjoint, subsets, or shufﬂed).
Zhou et al. [180] propose a symmetry MRP and a change di-
rection MRIP to test the system from ”different viewpoints”,
e.g., checking if an object recognition system recognizes the
same object regardless of whether it is played forward and
backward (changing direction). The works described above
assume that source inputs are either single items or item

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

35

sets, which simpliﬁes the deﬁnition of patterns capturing
mathematical properties (e.g., symmetry or equality). In our
work, instead, we focus on source inputs and outputs that
are action sequences and corresponding output sequences;
action sequences are necessary to describe interactions with
complex systems. Consequently, our patterns capture the
different operations to be performed on these input/output
sequences. The patterns provided in the literature are part
of our MRs but they capture only output conditions, as
described above.

11.4 Summary

In Table 24, based on a set of features necessary for security
testing, we summarize the differences between MST-wi and
related work. For each approach, the symbol ’+’ indicates
that the approach provides the feature, ’-’ indicates it does
not provide the feature, and ’NA’ indicates that the feature
is not applicable because it is out of scope. For instance,
Ognawala et al. [15] employ symbolic execution to de-
tect memory out-of-bounds/buffer overﬂow vulnerabilities
caused by unhandled memory operations. Therefore, none
of the features related to MT, such as the support for
specifying MRs, are considered for Ognawala et al. [15]
as depicted in Table 24. Most automated security testing
approaches do not address the oracle problem [116], [117],
[118], [119], [120], [121], [122], [168] or rely on an implicit
test oracle [15], [114], [115]. The few approaches that do
address the oracle problem focus on a limited number of
security vulnerabilities [123]. Also, most model-based secu-
rity testing approaches do not address the oracle problem
since they only generate test sequences from security mod-
els [129], [134], [137], [138]. Some model-based approaches
derive test cases with oracles [139], [140], [141] but require
mappings between model-level abstractions and executable
code implementing the oracle logic. MT can overcome these
limitations. It can be applied to both security functional and
vulnerability testing since MRs can capture both security
properties (e.g., a login screen should always be shown after
a session timeout) and properties of the inputs and outputs
involved in discovering a vulnerability (e.g., access to admin
pages without authentication). But existing MT solutions
target a few speciﬁc security vulnerabilities and do not
support automated testing based on MRs capturing general
security properties. To overcome these limitations, we need
a dedicated DSL and algorithms that automate the execution
of MT. To the best of our knowledge, MST-wi is the only
approach that supports, with a DSL, the speciﬁcation of MRs
capturing a wide range of security properties, automates
the generation of executable metamorphic test cases from
the MRs, and automatically detects various vulnerabilities
based on those relations.

12 CONCLUSION

In this paper, we presented an approach, MST-wi, that
enables engineers to specify metamorphic relations (MRs)
capturing security properties of Web systems, and that
automatically detects security vulnerabilities based on those
relations. Our approach aims to alleviate the oracle problem
in security testing.

Our contributions include (1) a DSL and supporting
tools for specifying MRs for security testing; (2) a catalog
of MRs inspired by OWASP guidelines and vulnerability
descriptions in the CWE [36]; (3) a data collection frame-
work crawling the system under test to derive input data
automatically; and (4) a testing framework automatically
performing security testing based on the MRs and the input
data [40].

Our analysis of the OWASP guidelines shows that our
approach can automate 39% of the security testing activities
not currently targeted by state-of-the-art techniques, indicat-
ing that it signiﬁcantly contributes to addressing the oracle
problem in security testing. Further, our catalog of MRs can
detect 101 vulnerability types in the CWE view for security
design principles (45% of the total), thus highlighting the
wide applicability of MST-wi in the security testing context.
Our empirical results with two open source Web systems
show that the approach requires limited manual effort and
detects 85.71% of the targeted vulnerabilities, thus suggest-
ing it is highly effective. Moreover, the impact of false alarms
is minimal since at most 0.19% of the executed follow-up
inputs led to a false alarm. Finally, the execution of our MRs
can be parallelized, thus enabling metamorphic security
testing to be automatically performed overnight.

ACKNOWLEDGMENTS
This work has been carried out as part of the COSMOS
Project, which has received funding from the European
Union’s Horizon 2020 Research and Innovation Programme
under grant agreement No. 957254. This work was also
supported by NSERC of Canada under the Discovery and
CRC programs. The authors would like to thank Xuan Phu
Mai for his preliminary investigation of CWE vulnerabilities
and for setting up the Joomla case study.

REFERENCES

[1]

C. Haley, R. Laney, J. Moffett, and B. Nuseibeh, “Security re-
quirements engineering: A framework for representation and
analysis,” IEEE Transactions on Software Engineering, vol. 34, no. 1,
pp. 133–153, 2008.

[3]

[2] M. Felderer, M. Buchler, M. Johns, A. D. Brucker, R. Breu, and
A. Pretschner, “Security testing: A survey,” Advances in Comput-
ers, vol. 101, pp. 1–51, 2016.
P. X. Mai, A. Goknil, L. K. Shar, F. Pastore, L. C. Briand, and
S. Shaame, “Modeling security and privacy requirements: a use
case-driven approach,” Information and Software Technology, vol.
100, pp. 165–182, 2018.
P. X. Mai, F. Pastore, A. Goknil, and L. C. Briand, “A natural
language programming approach for requirements-based secu-
rity testing,” in Proceedings of 29th IEEE International Symposium
on Software Reliability Engineering (ISSRE’18), 2018, pp. 58–69,
note: the paper reports on E2 vulnerabilities targeted in Section
X. They concern OTG-AUTHN-001, OTG-AUTHN-004, OTG-
AUTHN-010, OTG-BUSLOGIC-005.

[4]

[5] ——, “Mcp: a security testing tool driven by requirements,” in

[6]

ICSE (Companion Volume)’19, 2019, pp. 55–58.
E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE Transactions
on Software Engineering, vol. 41, no. 5, pp. 507–525, 2015.

[7] M. Staats, M. W. Whalen, and M. P. Heimdahl, “Programs, tests,
and oracles: the foundations of testing revisited,” in ICSE’11,
2011, pp. 391–400.

[8] M. Pezze and C. Zhang, “Automated test oracles: A survey,”

Advances in Computers, vol. 95, pp. 1–48, 2014.

[9] M. Meucci and A. Muller, “OWASP Testing Guide v4,” https:

//www.owasp.org/images/1/19/OTGv4.pdf.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

36

[10] G. Rosen,

Vulnerability,”
security-update/.

3gtPo80.
[12] E. Woollacott,

“Facebook

Security Update

on ’View As’
https://newsroom.fb.com/news/2018/09/

[11] D. Deahl, “Another Facebook Vulnerability,” https://bit.ly/

“Facebook
bounty

account
for

takeover: Researcher
2022,

40k

bug

chained exploit,”

scoops
https://portswigger.net/daily-swig/facebook-account-
takeover-researcher-scoops-40k-bug-bounty-for-chained-exploit.
J. Walker, “Teen hacker scoops $4,500 bug bounty for Facebook
ﬂaw that allowed attackers to unmask page admins,” 2021,
https://portswigger.net/daily-swig/teen-hacker-scoops-4-
500-bug-bounty-for-facebook-ﬂaw-that-allowed-attackers-to-
unmask-page-admins.
I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos,
“Dowsing for overﬂows: A guided fuzzer to ﬁnd buffer bound-
ary violations,” in USENIX Security’13, 2013, pp. 49–64.
S. Ognawala, M. Ochoa, A. Pretschner, and T. Limmer, “MACKE:
Compositional analysis of low-level vulnerabilities with symbolic
execution,” in ASE’16, 2016, pp. 780–785.

[13]

[14]

[15]

[16] T. Y. Chen, S.-C. Cheung, and S.-M. Yiu, “Metamorphic testing:
a new approach for generating next test cases,” The Hong Kong
University of Science and Technology, Tech. Rep., 1998.

[18]

[17] H. Liu, F.-C. Kuo, D. Towey, and T. Y. Chen, “How effectively
does metamorphic testing alleviate the oracle problem?” IEEE
Transactions on Software Engineering, vol. 40, no. 1, pp. 4–22, 2014.
S. Segura, G. Fraser, A. B. Sanchez, and A. Ruiz-Cortes, “A
survey on metamorphic testing,” IEEE Transactions on Software
Engineering, vol. 42, no. 9, pp. 805–824, 2016.
J. Mayer and R. Guderlei, “On random testing of image process-
ing applications,” in QSIC’06, 2006, pp. 85–92.

[19]

[20] R. Guderlei and J. Mayer, “Towards automatic testing of imaging
software by means of random and metamorphic testing,” Inter-
national Journal of Software Engineering and Knowledge Engineering,
vol. 17, no. 6, pp. 757–781, 2007.

[21] R. Just and F. Schweiggert, “Evaluating testing strategies for
imaging software by means of mutation analysis,” in ICSTW’09,
2009, pp. 205–209.

[22] F.-C. Kuo, S. Liu, and T. Y. Chen, “Testing a binary space par-
titioning algorithm with metamorphic testing,” in SAC’11, 2011,
pp. 1482–1489.

[23] W. K. Chan, S. C. Cheung, and K. R. Leung, “A metamorphic
testing approach for online testing of service-oriented software
applications,” International Journal of Web Services Research, vol. 4,
no. 2, pp. 61–81, 2007.

[24] C.-a. Sun, G. Wang, B. Mu, H. Liu, Z. Wang, and T. Y. Chen,
“Metamorphic testing for web services: Framework and a case
study,” in ICWS’11, 2011, pp. 283–290.

[25] Z. Q. Zhou, S. Zhang, M. Hagenbuchner, T. Tse, F.-C. Kuo, and
T. Y. Chen, “Automated functional testing of online search ser-
vices,” Software: Testing, Veriﬁcation and Reliability, vol. 22, no. 4,
pp. 221–243, 2012.

[26] T. Tse and S. S. Yau, “Testing context-sensitive middleware-based
software applications,” in COMPSAC’04, 2004, pp. 458–466.
[27] W. K. Chan, T. Y. Chen, S. C. Cheung, T. Tse, and Z. Zhang,
“Towards the testing of power-aware software applications for
wireless sensor networks,” in ADA Europe’07, 2007, pp. 84–99.

[28] F.-C. Kuo, T. Y. Chen, and W. K. Tam, “Testing embedded soft-
ware by metamorphic testing: A wireless metering system case
study,” in LCN’11, 2011, pp. 291–294.

[29] M. Jiang, T. Y. Chen, F.-C. Kuo, and Z. Ding, “Testing central pro-
cessing unit scheduling algorithms using metamorphic testing,”
in ICSESS’13, 2013, pp. 530–536.

[31]

[30] T. Y. Chen, F. Kuo, W. Ma, W. Susilo, D. Towey, J. Voas, and Z. Q.
Zhou, “Metamorphic testing for cybersecurity,” Computer, vol. 49,
no. 6, pp. 48–55, June 2016.
Synopsys Inc., “Description of the openssl heartbleed vulnerabil-
ity.” http://heartbleed.com/.
“Eclipse IDE, https://www.eclipse.org/ide/.”

[32]
[33] A. Mesbah, A. van Deursen, and S. Lenselink, “Crawling Ajax-
based web applications through dynamic analysis of user inter-
face state changes,” ACM Transactions on the Web (TWEB), vol. 6,
no. 1, pp. 3:1–3:30, 2012.
“JUnit, https://junit.org/.”
“Open Web Application Security Project.” https://www.owasp.
org/.

[34]
[35]

[36]

“MITRE common weaknesses enumeration project, which pro-
vide a taxonomy of vulnerability types.” https://cwe.mitre.org.
[37] Eclipse Foundation, “Jenkins ci/cd server.” https://jenkins.io/.
[38]
[39] Authors of this paper, “Replicabiliy package,” 2022, will be

“Joomla, https://www.joomla.org/.”

moved to Zenodo if accepted.

[40] ——, “SMRL editor executable, catalog of MRs, MT framework,

experimental data.” https://sntsvv.github.io/SMRL/.

[41] P. X. Mai, F. Pastore, A. Goknil, and L. C. Briand, “Metamorphic
security testing for web systems,” in ICST’20, 2020, pp. 186–197.
[42] P. X. Mai, A. Goknil, F. Pastore, and L. C. Briand, “SMRL: a
metamorphic security testing tool for web systems,” in ICSE
(Companion Volume)’20, 2020, pp. 9–12.
“CWE VIEW: Architectural Concepts,” https://cwe.mitre.org/
data/deﬁnitions/1008.html.
“CWE Top 25 Most Dangerous Software Errors,” https://cwe.
mitre.org/top25/archive/2019/2019 cwe top25.html.
“OWASP Top 10 Web Application Security Risks,” https://
owasp.org/www-project-top-ten/.

[45]

[43]

[44]

[46] P. X. Mai, “CVE-2020-2162: Stored XSS vulnerability in
ﬁle parameters,” https://cve.mitre.org/cgi-bin/cvename.cgi?
name=CVE-2020-2162, 2020.

[47] T. Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey, T. H. Tse, and
Z. Q. Zhou, “Metamorphic testing: A review of challanges and
opportunities,” ACM Computing Surveys, vol. 51, no. 1, 2018.
[48] Microsoft Corp., “Silverlight plug-ins and development tools.”

[49]

https://www.microsoft.com/silverlight/.
S. Efftinge, M. Eysholdt, J. K ¨ohnlein, S. Zarnekow, R. von
Massow, W. Hasselbring, and M. Hanus, “Xbase: Implementing
domain-speciﬁc languages for java,” ACM SIGPLAN Notices -
GPCE ’12, vol. 48, no. 3, pp. 112–121, 2012.
“Xtext, https://www.eclipse.org/Xtext/.”

[50]
[51] A. Mesbah, A. Van Deursen, and S. Lenselink, “Crawling ajax-
based web applications through dynamic analysis of user inter-
face state changes,” ACM Transactions on the Web, vol. 6, no. 1,
p. 3, 2012.

[52] A. Mesbah, E. Bozdag, and A. Van Deursen, “Crawling ajax by
inferring user interface state changes,” in ICWE’08, 2008, pp. 122–
134.

[54]

[53] V. I. Levenshtein, “Binary Codes Capable of Correcting Deletions,
Insertions and Reversals,” Soviet Physics Doklady, vol. 10, Feb.
1966.
“Selenium Web Testing Framework, https://www.seleniumhq.
org/.”
“ASM bytecode manipulation framework.” https://asm.ow2.
io/.
“CWE - Common Weakness Enumeration,” https://cwe.mitre.
org.

[56]

[55]

[57] A. Zeller, R. Gopinath, M. B ¨ohme, G. Fraser, and C. Holler,
“The fuzzing book,” in The Fuzzing Book.
Saarland University,
2019, retrieved 2019-09-09 16:42:54+02:00. [Online]. Available:
https://www.fuzzingbook.org/

[58] C. Alexander, A Pattern Language: Towns, Buildings, Construction.

[59]

[60]

[61]
[62]

[63]

[64]

[65]

[66]

[67]

[68]

Oxford University Press, 1977.
J. M. Voas and K. W. Miller, “Software testability: The new
veriﬁcation,” IEEE software, vol. 12, no. 3, pp. 17–28, 1995.
“WSTG - v4.1: Testing Session Timeout,” https://owasp.
org/www-project-web-security-testing-guide/v41/4-Web
Application Security Testing/06-Session Management
Testing/07-Testing Session Timeout.html.
“CWE-FAQ,” https://cwe.mitre.org/about/faq.html#A.1.
“CWE VIEW: Software Development Concepts,” https://cwe.
mitre.org/data/deﬁnitions/699.html.
“CWE VIEW: Research Concepts,” https://cwe.mitre.org/data/
deﬁnitions/1000.html.
“CWE Category: software fault patterns,” https://cwe.mitre.
org/data/deﬁnitions/888.html.
“CWE VIEW: Hardware Design,” https://cwe.mitre.org/data/
deﬁnitions/1194.html.
“CWE Category: sert cei c coding standards,” https://cwe.mitre.
org/data/deﬁnitions/1154.html.
J. C. Santos, A. Peruma, M. Mirakhorli, M. Galstery, J. V. Vidal,
and A. Sejﬁa, “Understanding software vulnerabilities related
to architectural security tactics: An empirical investigation of
chromium, php and thunderbird,” in ICSA’17, 2017, pp. 69–78.
J. C. Santos, K. Tarrit, and M. Mirakhorli, “A catalog of security
architecture weaknesses,” in ICSAW’17, 2017, pp. 220–223.

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

37

[69] N. M. Ben A. Calloni, Djenana Campana, “Embedded Infor-
mation Systems Technology Support (EISTS). Task Order 0006:
Vulnerability Path Analysis and Demonstration (VPAD). Vol-
ume 2 - White Box Deﬁnitions of Software Fault Patterns,”
LOCKHEED MARTIN INC FORT WORTH TX, Tech. Rep., 2011,
https://apps.dtic.mil/docs/citations/ADB381215.
“CWE View: Weaknesses in owasp top ten (2017),” https://cwe.
mitre.org/data/deﬁnitions/1026.html.
“OWASP Top 10 Mobile Security Risks,” https://www.owasp.
org/index.php/Mobile Top 10 2016-Top 10.

[70]

[71]

[72] OWASP, “WSTG-ATHZ-02: Testing for Bypassing Authorization

Schema.” https://bit.ly/2Y6SloC.

[73] ——, “OTG-INFO-010: Mapping application architecture.”

https://www.owasp.org/index.php/Map Application
Architecture (OTG-INFO-010).

[74] ——, “OTG-BUSLOGIC-006: Testing for the circumvention of
workﬂows.” https://www.owasp.org/index.php/Testing for
the Circumven\protect\discretionary{\char\hyphenchar\font}
{}{}tion of Work Flows (OTG-BUSLOGIC-006).

[75] ——,

“OTG-INPVAL-014: Testing

for Buffer Overﬂow.”

https://www.owasp.org/index.php/Testing for Buffer
Overﬂow (OTG-INPVAL-014).
“Format string attack,” https://owasp.org/www-community/
attacks/Format string attack.

[76]

[77] OWASP,

“OTG-AUTHN-002: Testing for default

creden-
tials.” https://www.owasp.org/index.php/Testing for default
credentials (OTG-AUTHN-002).

[78] Portswigger, “Burp suite.” https://portswigger.net/burp.
[79] ——, “Using burp suite to test for bypass authorization schema
using site maps.” https://support.portswigger.net/customer/
portal/articles/1969842-using-burp-s-%22request-in-browser%
22-function-to-test-for-access-control-issues.

[80] ——, “Burp suite scanning (crawling)

feature.” https://

portswigger.net/burp/documentation/desktop/scanning.
[81] R. S. Liverani, “Integration of burp suite and crawljax.” https:

//github.com/portswigger/burp-csj.

[82] OWASP, “WSTG-ATHZ-01: Testing Directory Traversal File In-

clude.” https://bit.ly/3l1sRTl.

[83] V. J. Man`es, H. Han, C. Han, S. K. Cha, M. Egele, E. J. Schwartz,
and M. Woo, “The art, science, and engineering of fuzzing: A
survey,” IEEE Transactions on Software Engineering, vol. 47, no. 11,
pp. 2312–2331, 2019.

[84] OWASP, “WSTG-SESS-03: Testing for Session Fixation.” https:

[85]

[86]

//bit.ly/34myBkN.
“Common attack pattern enumeration and classiﬁcation
(CAPEC),” https://capec.mitre.org.
“Java Platform, Enterprise Edition,” https://www.oracle.com/
java/technologies/java-ee-glance.html.

[87] A. Austin, C. Holmgreen, and L. Williams, “A comparison
of the efﬁciency and effectiveness of vulnerability discovery
techniques,” Information and Software Technology, vol. 55, no. 7, pp.
1279–1288, 2013. [Online]. Available: https://www.sciencedirect.
com/science/article/pii/S0950584912002339
“CWE-640: Weak password recovery mechanism for forgotten
password,” https://cwe.mitre.org/data/deﬁnitions/640.html.

[88]

[89] V. Garousi, M. Felderer, and F. N. Kılıc¸aslan, “A survey on
software testability,” Information and Software Technology, vol. 108,
pp. 35–64, 2019.
“CWE-287:
data/deﬁnitions/287.html.
“CWE-613: Insufﬁcient session expiration,” https://cwe.mitre.
org/data/deﬁnitions/613.html.

Improper authentication,” https://cwe.mitre.org/

[90]

[91]

[92] Eclipse Foundation, “Jetty application server.” https://www.

eclipse.org/jetty/.

[93] Oracle

corp.,
https://www.mysql.com/.

“MYSQL

RDBMS

engine,”

2022,

[94] Apache software foundation, “Apache web server,” 2022,

https://httpd.apache.org/.

[95] MITRE Corporation, “Common vulnerabilities and exposures.”

https://cve.mitre.org/cve/.

[96] MITRE,

“CVE-2018-1999003,
OTG-AUTHZ-
https://cve.mitre.org/cgi-bin/cvename.cgi?name=

concerns

002,”
CVE-2018-1999003.

[99] ——, “CVE-2018-1000406, concerns OTG-AUTHN-001,” https://

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1000406.

[100] ——, “CVE-2018-1999046, concerns OTG-AUTHZ-002,” https://

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1999046.

[101] ——, “CVE-2018-1000409, concerns OTG-SESS-003,” https://cve.
mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1000409.
[102] ——, “CWE-521,” https://cwe.mitre.org/data/deﬁnitions/521.

html.

[103] ——, “CWE-262,” https://cwe.mitre.org/data/deﬁnitions/262.

html.

[104] ——, “CVE-2020-2162, concerns OTG-INPVAL-003,” https://
cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-2162.
[105] ——, “CVE-2018-1999047, concerns OTG-AUTHZ-002,” https://

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1999047.

[106] ——, “CVE-2018-1999045, concerns OTG-AUTHZ-002,” https://

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1999045.

[107] ——,

[108] ——,

“CVE-2018-17857,”
cvename.cgi?name=CVE-2018-17857.
“CVE-2018-11327,”
cvename.cgi?name=CVE-2018-11327.

https://cve.mitre.org/cgi-bin/

https://cve.mitre.org/cgi-bin/

[109] D. Lane, Online Statistics Education: A Multimedia Course of Study.

[Online]. Available: http://onlinestatbook.com/

[110] R. Wood, “Damn vulnerable web application (dvwa),” 2022,

https://dvwa.co.uk/.

[111] P. Ralph and E. Tempero, “Construct Validity in Software
Engineering Research and Software Metrics,” in Proceedings of
the 22nd International Conference on Evaluation and Assessment
in Software Engineering 2018, vol. Part F1377. New York,
NY, USA: ACM,
[Online]. Available:
jun 2018, pp. 13–23.
https://dl.acm.org/doi/10.1145/3210459.3210461

[112] M. Felderer, P. Zech, R. Breu, M. B ¨uchler, and A. Pretschner,
“Model-based security testing: A taxonomy and systematic clas-
siﬁcation,” Software: Testing, Veriﬁcation and Reliability, vol. 26,
no. 2, pp. 119–148, 2016.

[113] G. Tian-yang, S. Yin-Sheng, and F. You-yuan, “Research on soft-
ware security testing,” World Academy of Science, Engineering and
Technology, vol. 70, no. 69, pp. 647–651, 2010.

[114] S. Bekrar, C. Bekrar, R. Groz, and L. Mounier, “Finding software
vulnerabilities by smart fuzzing,” in ICST’11, 2011, pp. 427–430.
[115] A. Takanen, J. D. Demott, C. Miller, and A. Kettunen, Fuzzing for
Software Security Testing and Quality Assurance. Artech House,
2018.

[116] S. Kals, E. Kirda, C. Kruegel, and N. Jovanovic, “SecuBat: A web

vulnerability scanner,” in WWW’06, 2006, pp. 247–246.

[117] M. Martin and M. S. Lam, “Automatic generation of XSS and SQL
injection attacks with goal-directed model checking,” in USENIX
Security’08, 2008, pp. 31–43.

[118] J. Bau, E. Bursztein, D. Gupta, and J. Mitchell, “State of the art:
Automated black-box web application vulnerability testing,” in
SP’10, 2010, pp. 332–345.

[119] D. Appelt, C. D. Nguyen, L. C. Briand, and N. Alshahwan,
“Automated testing for sql injection vulnerabilities: An input
mutation approach,” in ISSTA’14, 2014, pp. 259–269.

[120] M. Salas and E. Martins, “Security testing methodology for
vulnerabilities detection of XSS in web services and WS-security,”
ENTCS, pp. 133–154, 2014.

[121] O. Tripp, O. Weisman, and L. Guy, “Finding your way in the
testing jungle: A learning approach to web security testing,” in
ISSTA’13, 2013, pp. 347–357.

[122] D. Appelt, N. Alshahwan, and L. Briand, “Assessing the impact
of ﬁrewalls and database proxies on sql injection testing,” in
FITTEST’13, 2013, pp. 32–47.

[123] Y.-W. Huang, S.-K. Huang, T.-P. Lin, and C.-H. Tsai, “Web ap-
plication security assessment by fault injection and behavior
monitoring,” in WWW’03, 2003, pp. 148–159.

[124] Y. Le Traon, T. Mouelhi, and B. Baudry, “Testing security policies:
Going beyond functional testing,” in ISSRE’07, 2007, pp. 93–102.
[125] T. Mouelhi, F. Fleurey, B. Baudry, and Y. Le Traon, “A model-
based framework for security policy speciﬁcation, deployment
and testing,” in MODELS’08, 2008, pp. 537–552.

[126] T. Mouelhi, Y. Le Traon, and B. Baudry, “Transforming and
selecting functional test cases for security policy testing,” in
ICST’09, 2009, pp. 171–180.

[97] ——, “CVE-2018-1999004, concerns OTG-AUTHZ-002,” https://

[127] E. Martin and T. Xie, “Automated test generation for access

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1999004.

control policies via change-impact analysis,” in SESS’07, 2007.

[98] ——, “CVE-2018-1999006, concerns OTG-AUTHZ-002,” https://

[128] ——, “A fault model and mutation testing of access control

cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1999006.

policies,” in WWW’07, 2007, pp. 667–676.

38

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 00, NO. 0, MONTH YEAR

[129] G. Wimmel and J. J ¨urjens, “Speciﬁcation-based test generation for
security-critical systems using mutations,” in ICFEM’02, 2002, pp.
471–482.

[152] ——, “Automated test data generation on the analyses of feature
models: A metamorphic testing approach,” in ICST’10, 2010, pp.
35–44.

[130] M. Masood, A. Ghafoor, and A. Mathur, “Conformance testing of
temporal role-based access control systems,” IEEE Transactions on
Dependable and Secure Computing, vol. 7, no. 2, pp. 144–158, 2010.
[131] A. Bertolino, S. Daoudagh, F. Lonetti, E. Marchetti, F. Martinelli,
and P. Mori, “Testing of PolPA authorization systems,” in AST’12,
2012, pp. 8–14.

[132] A. Blome, M. Ochoa, K. Li, M. Peroli, and M. T. Dashti, “Vera: A
ﬂexible model-based vulnerability testing tool,” in ICST’13, 2013,
pp. 471–478.

[133] K. He, Z. Feng, and X. Li, “An attack scenario based approach
for software security testing at design stage,” in ISCSCT’08, 2008,
pp. 782–787.

[134] A. Marback, H. Do, K. He, S. Kondamarri, and D. Xu, “A threat
model-based approach to security testing,” Software: Practice and
Experience, vol. 43, no. 2, pp. 241–258, 2013.

[135] J. J ¨urjens, “Model-based security testing using UMLsec: A case
study,” Electronic Notes in Theoretical Computer Science, vol. 220,
no. 1, pp. 93–104, 2008.

[136] D. Xu and K. E. Nygard, “Threat-driven modeling and veriﬁca-
tion of secure software using aspect-oriented petri nets,” IEEE
Transactions on Software Engineering, vol. 32, no. 4, pp. 265–278,
2006.

[137] F. Lebeau, B. Legeard, F. Peureux, and A. Vernotte, “Model-based
vulnerability testing for web applications,” in ICSTW’13, 2013,
pp. 445–452.

[138] J. Whittle, D. Wijesekera, and M. Hartong, “Executable misuse
cases for modeling security concerns,” in ICSE’08, 2008, pp. 121–
130.

[139] D. Xu, M. Tu, M. Sanford, L. Thomas, D. Woodraska, and W. Xu,
“Automated security test generation with formal threat models,”
IEEE Transactions on Dependable and Secure Computing, vol. 9, no. 4,
pp. 526–540, 2012.

[140] D. Xu, W. Xu, M. Kent, L. Thomas, and L. Wang, “An automated
test generation technique for software quality assurance,” IEEE
Transactions on Reliability, vol. 64, no. 1, pp. 247–268, 2015.
[141] D. Xu, L. Thomas, M. Kent, T. Mouelhi, and Y. Le Traon, “A
model-based approach to automated testing of access control
policies,” in SACMAT’12, 2012, pp. 209–218.

[142] T. Jameel, M. Lin, and L. Chao, “Test oracles based on metamor-
phic relations for image processing applications,” in SNPD’15,
2015, pp. 1–6.

[143] W. Chan, J. C. Ho, and T. Tse, “Piping classiﬁcation to metamor-
phic testing: An empirical study towards better effectiveness for
the identiﬁcation of failures in mesh simpliﬁcation programs,” in
COMPSAC’07, vol. 1, 2007, pp. 397–404.

[144] W. K. Chan, J. C. Ho, and T. Tse, “Finding failures from passed
test cases: Improving the pattern classiﬁcation approach to the
testing of mesh simpliﬁcation programs,” Software Testing, Veriﬁ-
cation and Reliability, vol. 20, no. 2, pp. 89–120, 2010.

[145] T. Y. Chen, F.-C. Kuo, H. Liu, and S. Wang, “Conformance testing
of network simulators based on metamorphic testing technique,”
in FORTE’09, 2009, pp. 243–248.

[146] J. Ding, T. Wu, D. Wu, J. Q. Lu, and X.-H. Hu, “Metamorphic
testing of a monte carlo modeling program,” in AST’11, 2011, pp.
1–7.

[147] C. Murphy, M. S. Raunak, A. King, S. Chen, C. Imbriano,
G. Kaiser, I. Lee, O. Sokolsky, L. Clarke, and L. Osterweil, “On
effective testing of health care simulation software,” in SEHC’11,
2011, pp. 40–47.

[148] K. Sim, W. Pao, and C. Lin, “Metamorphic testing using geomet-
ric interrogation technique and its application,” ECTI Transactions
on Computer and Information Technology (ECTI-CIT), vol. 1, no. 2,
pp. 91–95, 2005.

[149] Q. Tao, W. Wu, C. Zhao, and W. Shen, “An automatic testing
approach for compiler based on metamorphic testing technique,”
in APSEC’10, 2010, pp. 270–279.

[150] Y. Yao, S. Huang, and M. Ji, “Research on metamorphic testing
for oracle problem of integer bugs,” in AISC’12, 2012, pp. 95–100.
[151] S. Segura, R. M. Hierons, D. Benavides, and A. Ruiz-Cort´es,
“Automated metamorphic testing on the analyses of feature
models,” Information and Software Technology, vol. 53, no. 3, pp.
245–258, 2011.

[153] F.-C. Kuo, Z. Q. Zhou, J. Ma, and G. Zhang, “Metamorphic testing
of decision support systems: a case study,” IET software, vol. 4,
no. 4, pp. 294–301, 2010.

[154] T. Y. Chen, J. W. Ho, H. Liu, and X. Xie, “An innovative approach
for testing bioinformatics programs using metamorphic testing,”
BMC bioinformatics, vol. 10, no. 1, p. 24, 2009.

[155] L. L. Pullum and O. Ozmen, “Early results from metamorphic
testing of epidemiological models,” in BioMedCom’12, 2012, pp.
62–67.

[156] T. Y. Chen, J. Feng, and T. Tse, “Metamorphic testing of programs
on partial differential equations: a case study,” in COMPSAC’02,
2002, pp. 327–333.

[157] C. Aruna and R. S. R. Prasad, “Metamorphic relations to improve
the test accuracy of multi precision arithmetic software applica-
tions,” in ICACCI’14, 2014, pp. 2244–2248.

[158] X. Xie, J. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Y. Chen,
“Application of metamorphic testing to supervised classiﬁers,”
in QSIC’09, 2009, pp. 135–144.

[159] C. Murphy, G. E. Kaiser, and L. Hu, “Properties of machine
learning applications for use in metamorphic testing,” Columbia
University, Tech. Rep., 2008.

[160] S. Segura, J. A. Parejo, J. Troya, and A. Ruiz-Cort´es, “Metamor-
phic testing of RESTful web APIs,” IEEE Transactions on Software
Engineering, vol. 44, no. 11, pp. 1083–1099, 2017.

[161] S. Segura, A. Dur´an, J. Troya, and A. R. Cort´es, “A template-based
approach to describing metamorphic relations,” in MET’17, 2017,
pp. 3–9.

[162] S. Segura, A. Dur´an, J. Troya, and A. Ruiz-Cort´es, “Metamorphic
relation template v1. 0,” Applied Software Engineering Research
Group, Tech. Rep. ISA-17-TR-01, 2017.

[163] C.-a. Sun, Z. Wang, and G. Wang, “A property-based testing
framework for encryption programs,” Frontiers of Computer Sci-
ence, vol. 8, no. 3, pp. 478–489, 2014.

[164] Q.-H. Luu, M. F. Lau, S. P. Ng, and T. Y. Chen, “Testing multiple
linear regression systems with metamorphic testing,” Journal of
Systems and Software, vol. 182, p. 111062, 2021.

[165] A. Lascu, A. F. Donaldson, T. Grosser, and T. Hoeﬂer, “Metamor-

phic fuzzing of c++ libraries,” in ICST’22.

[166] J. Ayerdi, V. Terragni, A. Arrieta, P. Tonella, G. Sagardui, and
M. Arratibel, “Generating metamorphic relations for cyber-
physical systems with genetic programming: an industrial case
study,” in ESEC/FSE’21, 2021, pp. 1264–1274.

[167] L. Xu, D. Towey, A. P. French, S. Benford, Z. Q. Zhou, and
T. Y. Chen, “Using metamorphic relations to verify and enhance
artcode classiﬁcation,” Journal of Systems and Software, vol. 182, p.
111060, 2021.

[168] S. Raghavan and H. Garcia-Molina, “Crawling the hidden web,”

in VLDB’01, 2000, pp. 129–138.

[169] M. Felderer, B. Agreiter, P. Zech, and R. Breu, “A classiﬁcation for

model-based security testing,” in VALID’11, 2011, pp. 109–114.

[170] J. J ¨urjens, “UMLsec: Extending UML for secure systems develop-

ment,” in UML’02, 2002, pp. 412–425.

[171] ——, “Sound methods and effective tools for model-based secu-
rity engineering with UML,” in ICSE’05, 2005, pp. 322–331.
[172] ——, Secure Systems Development with UML. Springer Science &

Business Media, 2005.

[173] E. Martin, T. Xie, and T. Yu, “Deﬁning and measuring policy
coverage in testing access control policies,” in ICICS’06, 2006, pp.
139–158.

[174] V. Le, M. Afshari, and Z. Su, “Compiler validation via equiva-
lence modulo inputs,” ACM SIGPLAN Notices, vol. 49, no. 6, pp.
216–226, 2014.

[175] S. Segura, A. Dur´an, A. B. S´anchez, D. L. Berre, E. Lonca, and
A. Ruiz-Cort´es, “Automated metamorphic testing of variability
analysis tools,” Software Testing, Veriﬁcation and Reliability, vol. 25,
no. 2, pp. 138–163, 2015.

[176] A. Ramanathan, C. A. Steed, and L. L. Pullum, “Veriﬁcation
of compartmental epidemiological models using metamorphic
testing, model checking and visual analytics,” in BioMedCom’12,
2012, pp. 68–73.

[177] H. Zhu, “Jfuzz: A tool for automated java unit testing based on
data mutation and metamorphic testing methods,” in TSA’15,
2015, pp. 8–15.

BAYATI CHALESHTARI et al.: METAMORPHIC TESTING FOR WEB SYSTEM SECURITY

39

[178] C. Murphy, K. Shen, and G. Kaiser, “Using jml runtime asser-
tion checking to automate metamorphic testing in applications
without test oracles,” in ICST’09, 2009, pp. 436–445.

[179] C. Murphy, G. E. Kaiser, and L. Hu, “Properties of machine
learning applications for use in metamorphic testing,” 2008.
[180] Z. Q. Zhou, L. Sun, T. Y. Chen, and D. Towey, “Metamorphic
relations for enhancing system understanding and use,” IEEE
Transactions on Software Engineering, vol. 46, no. 10, pp. 1120–1154,
2018.

[181] S. Segura, A. Dur´an, J. Troya, and A. Ruiz-Cort´es, “Metamorphic
relation patterns for query-based systems,” in 2019 IEEE/ACM
4th International Workshop on Metamorphic Testing (MET), 2019, pp.
24–31.

