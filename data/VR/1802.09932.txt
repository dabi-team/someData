8
1
0
2

t
c
O
8
2

]

G
L
.
s
c
[

2
v
2
3
9
9
0
.
2
0
8
1
:
v
i
X
r
a

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

1

VR-SGD: A Simple Stochastic Variance
Reduction Method for Machine Learning

Fanhua Shang, Member, IEEE, Kaiwen Zhou, Hongying Liu, James Cheng, Ivor W. Tsang,
Lijun Zhang, Member, IEEE, Dacheng Tao, Fellow, IEEE, and Licheng Jiao, Fellow, IEEE

Abstract—In this paper, we propose a simple variant of the original SVRG, called variance reduced stochastic gradient descent
(VR-SGD). Unlike the choices of snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the two vectors of
VR-SGD are set to the average and last iterate of the previous epoch, respectively. The settings allow us to use much larger learning
rates, and also make our convergence analysis more challenging. We also design two different update rules for smooth and
non-smooth objective functions, respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly convex problems
directly without any reduction techniques. Moreover, we analyze the convergence properties of VR-SGD for strongly convex problems,
which show that VR-SGD attains linear convergence. Different from most algorithms that have no convergence guarantees for
non-strongly convex problems, we also provide the convergence guarantees of VR-SGD for this case, and empirically verify that
VR-SGD with varying learning rates achieves similar performance to its momentum accelerated variant that has the optimal
convergence rate O(1/T 2). Finally, we apply VR-SGD to solve various machine learning problems, such as convex and non-convex
empirical risk minimization, and leading eigenvalue computation. Experimental results show that VR-SGD converges signiﬁcantly faster
than SVRG and Prox-SVRG, and usually outperforms state-of-the-art accelerated methods, e.g., Katyusha.

Index Terms—Stochastic optimization, stochastic gradient descent (SGD), variance reduction, empirical risk minimization, strongly
convex and non-strongly convex, smooth and non-smooth

(cid:70)

1 INTRODUCTION

I N this paper, we focus on the following composite opti-

mization problem:

F (x) def=

min
x∈Rd

1
n

n
(cid:88)

i=1

fi(x) + g(x)

(1)

(cid:80)n

where f (x) = 1
i=1fi(x), fi(x) : Rd → R, i = 1, . . . , n are
n
the smooth functions, and g(x) is a relatively simple (but
possibly non-differentiable) convex function (referred to as
a regularizer). The formulation (1) arises in many places in
machine learning, signal processing, data science, statistics
and operations research, such as regularized empirical risk
minimization (ERM). For instance, one popular choice of the
component function fi(·) in binary classiﬁcation problems
is the logistic loss, i.e., fi(x) = log(1 + exp(−biaT
i x)), where

•

F. Shang, H. Liu (Corresponding author) and L. Jiao are with the Key
Laboratory of Intelligent Perception and Image Understanding of Ministry
of Education, School of Artiﬁcial Intelligence, Xidian University, China.
E-mails: {fhshang, hyliu}@xidian.edu.cn, lchjiao@mail.xidian.edu.cn.
• K. Zhou and J. Cheng are with the Department of Computer Science and
Engineering, The Chinese University of Hong Kong, Hong Kong. E-mails:
{kwzhou, jcheng}@cse.cuhk.edu.hk.
I.W. Tsang is with the Centre for Artiﬁcial
Intelligence, Univer-
sity of Technology Sydney, Ultimo, NSW 2007, Australia. E-mail:
Ivor.Tsang@uts.edu.au.
L. Zhang is with the National Key Laboratory for Novel Soft-
ware Technology, Nanjing University, Nanjing 210023, China. E-mail:
zhanglj@lamda.nju.edu.cn.

•

•

• D. Tao is with the UBTECH Sydney Artiﬁcial Intelligence Centre and
the School of Information Technologies, the Faculty of Engineering and
Information Technologies, the University of Sydney, 6 Cleveland St,
Darlington, NSW 2008, Australia. E-mail: dacheng.tao@sydney.edu.au.

Manuscript received March 22, 2018.

{(a1, b1), . . . , (an, bn)} is a collection of training examples,
and bi ∈ {±1}. Some popular choices for the regularizer
include the (cid:96)2-norm regularizer (i.e., g(x) = (λ/2)(cid:107)x(cid:107)2), the
(cid:96)1-norm regularizer (i.e., g(x) = λ(cid:107)x(cid:107)1), and the elastic-net
regularizer (i.e., g(x) = (λ1/2)(cid:107)x(cid:107)2+λ2(cid:107)x(cid:107)1). Some other ap-
plications include deep neural networks [1], [2], [3], [4], [5],
group Lasso [6], sparse learning and coding [7], [8], [9], [10],
non-negative matrix factorization [11], phase retrieval [12],
matrix completion [13], [14], conditional random ﬁelds [15],
generalized eigen-decomposition and canonical correlation
analysis [16], and eigenvector computation [17], [18] such
as principal component analysis (PCA) and singular value
decomposition (SVD).

1.1 Stochastic Gradient Descent

We are especially interested in developing efﬁcient algo-
rithms to solve Problem (1) involving the sum of a large
number of component functions. The standard and effective
method for solving (1) is the (proximal) gradient descent
including Nesterov’s accelerated gradient
(GD) method,
descent (AGD) [19], [20] and accelerated proximal gradient
(APG) [21], [22]. For the smooth problem (1), GD takes the
following update rule: starting with x0, and for any k ≥ 0

xk+1 = xk − ηk

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)

∇fi(xk) + ∇g(xk)

(2)

where ηk > 0 is commonly referred to as the learning rate
in machine learning or step-size in optimization. When g(·)

 
 
 
 
 
 
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

2

is non-smooth (e.g., the (cid:96)1-norm regularizer), we typically
introduce the following proximal operator to replace (2),

xk+1 = Proxg
ηk

(yk) := arg min

x∈Rd

(cid:26) 1
2ηk

(cid:107)x−yk(cid:107)2 +g(x)

(cid:27)

(3)

where yk = xk −(ηk/n) (cid:80)n
i=1∇fi(xk). GD has been proven
to achieve linear convergence for strongly convex problems,
and both AGD and APG attain the optimal convergence rate
O(1/T 2) for non-strongly convex problems, where T denotes
the number of iterations. However, the per-iteration cost of
all the batch (or deterministic) methods is O(nd), which is
expensive for very large n.

Instead of evaluating the full gradient of f (·) at each
iteration, an efﬁcient alternative is the stochastic (or in-
cremental) gradient descent (SGD) method [23]. SGD only
evaluates the gradient of a single component function at
each iteration, and has much lower per-iteration cost, O(d).
Thus, SGD has been successfully applied to many large-
scale learning problems [24], [25], [26], especially training
for deep learning models [2], [3], [27], and its update rule is

xk+1 = xk − ηk[∇fik(xk) + ∇g(xk)]

(4)

√

where ηk ∝ 1/
k, and the index ik can be chosen uniformly
at random from {1, 2, . . . , n}. Although the expectation
of the stochastic gradient estimator ∇fik(xk) is an unbi-
ased estimation for ∇f (xk), i.e., E[∇fik(xk)] = ∇f (xk), the
variance of ∇fik(xk) may be large due to the variance of
random sampling [1]. Thus, stochastic gradient estimators
are also called “noisy gradients”, and we need to gradually
reduce its step size, which leads to slow convergence. In
particular, even under the strongly convex (SC)condition,
standard SGD attains a slower sub-linear convergence rate
O(1/T ) [28].

1.2 Accelerated SGD

(SVRG)

Recently, many SGD methods with variance reduction have
been proposed, such as stochastic average gradient (SAG)
[29], stochastic variance reduced gradient
[1],
stochastic dual coordinate ascent (SDCA) [30], SAGA [31],
stochastic primal-dual coordinate (SPDC) [32], and their
proximal variants, such as Prox-SAG [33], Prox-SVRG [34]
and Prox-SDCA [35]. These accelerated SGD methods can
use a constant learning rate η instead of diminishing step
sizes for SGD, and fall into the following three categories:
primal methods such as SVRG and SAGA, dual methods
such as SDCA, and primal-dual methods such as SPDC. In
essence, many of the primal methods use the full gradient
at the snapshot (cid:101)x or the average gradient to progressively
reduce the variance of stochastic gradient estimators, as
well as the dual and primal-dual methods, which leads to
a revolution in the area of ﬁrst-order optimization [36]. Thus,
they are also known as the hybrid gradient descent method
[37] or semi-stochastic gradient descent method [38]. In
particular, under the strongly convex condition, most of
the accelerated SGD methods enjoy a linear convergence
rate (also known as a geometric or exponential rate) and
the oracle complexity of O((n+L/µ) log(1/(cid:15))) to obtain an
(cid:15)-suboptimal solution, where each fi(·) is L-smooth, and
F (·) is µ-strongly convex. The complexity bound shows
that they converge faster than accelerated deterministic

methods, whose oracle complexity is O(n(cid:112)L/µ log(1/(cid:15)))
[39], [40].

SVRG [1] and its proximal variant, Prox-SVRG [34], are
particularly attractive because of their low storage require-
ment compared with other methods such as SAG, SAGA
and SDCA, which require storage of all the gradients of
component functions or dual variables. At the beginning of
the s-th epoch in SVRG, the full gradient ∇f ((cid:101)xs−1) is com-
puted at the snapshot (cid:101)xs−1, which is updated periodically.
Deﬁnition 1. The stochastic variance reduced gradient estimator
is independently introduced in [1], [37] as follows:

(cid:101)∇fis

k

(xs

k) = ∇fis

k

(xs

k) − ∇fis

k

((cid:101)xs−1) + ∇f ((cid:101)xs−1),

(5)

where s is the epoch that iteration k belongs to.

k

(xs

(xs

k) (i.e., E(cid:107) (cid:101)∇fis

It is not hard to verify that the variance of the SVRG
k)(cid:107)2) can be
estimator (cid:101)∇fis
much smaller than that of the SGD estimator ∇fik(xs
k)
(i.e., E(cid:107)∇fik(xs
k)(cid:107)2). Theoretically, for non-strongly
convex (Non-SC) problems, the variance reduced methods
converge slower than the accelerated batch methods such as
FISTA [22], i.e., O(1/T ) vs. O(1/T 2).

k) − ∇f (xs

k)−∇f (xs

k

More recently, many acceleration techniques were pro-
posed to further speed up the stochastic variance reduced
methods mentioned above. These techniques mainly in-
clude the Nesterov’s acceleration techniques in [25], [39],
[40], [41], [42], reducing the number of gradient calcula-
tions in early iterations [36], [43], [44], the projection-free
property of the conditional gradient method (also known
as the Frank-Wolfe algorithm [45]) as in [46], the stochastic
sufﬁcient decrease technique [47], and the momentum accel-
eration tricks in [36], [48], [49]. [40] proposed an accelerating
Catalyst framework and achieved the oracle complexity
of O((n +(cid:112)nL/µ) log(L/µ) log(1/(cid:15))) for strongly convex
problems. [48] and [50] proved that the accelerated methods
can attain the oracle complexity of O(n log(1/(cid:15)) + (cid:112)nL/(cid:15))
for non-strongly convex problems. The overall complex-
ity matches the theoretical upper bound provided in [51].
Katyusha [48], point-SAGA [52] and MiG [50] achieve the
best-known oracle complexity of O((n+(cid:112)nL/µ) log(1/(cid:15)))
for strongly convex problems, which is identical to the
upper complexity bound in [51]. Hence, Katyusha and MiG
are the best-known stochastic optimization method for both
SC and Non-SC problems, as pointed out in [51]. However,
selecting the best values for the parameters in the accel-
erated methods (e.g., the momentum parameter) is still an
open problem. In particular, most of accelerated stochastic
variance reduction methods, including Katyusha, require at
least one auxiliary variable and one momentum parameter,
which lead to complicated algorithm design and high per-
iteration complexity, especially for very high-dimensional
and sparse data.

1.3 Our Contributions

From the above discussions, we can see that most of
the accelerated stochastic variance reduction methods such
as [36], [39], [44], [46], [47], [48], [53], [54] and applications
such as [7], [9], [10], [14], [17], [18], [55], [56], [57] are
based on the SVRG method [1]. Thus, any key improvement
on SVRG is very important for the research of stochastic

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

3

TABLE 1
Comparison of convergence rates of VR-SGD and its counterparts.

Algorithm 1 SVRG (Option I) and Prox-SVRG (Option II)
Input: The number of epochs S, the number of iterations m

SVRG [1] Prox-SVRG [34] VR-SGD
linear rate
unknown
linear rate
SC, smooth
linear rate
linear rate
unknown
SC, non-smooth
O(1/T )
unknown
Non-SC, smooth
unknown
O(1/T )
unknown
Non-SC, non-smooth unknown

optimization. In this paper, we propose a simple variant
of the original SVRG [1], called variance reduced stochastic
gradient descent (VR-SGD). The snapshot point and start-
ing point of each epoch in VR-SGD are set to the average
and last iterate of the previous epoch, respectively. This is
different from the settings of SVRG and Prox-SVRG [34],
where the two points of the former are set to be the last
iterate, and those of the latter are set to be the average of
the previous epoch. This difference makes the convergence
analysis of VR-SGD signiﬁcantly more challenging than that
of SVRG and Prox-SVRG. Our empirical results show that
the performance of VR-SGD is signiﬁcantly better than
its counterparts, SVRG and Prox-SVRG. Impressively, VR-
SGD with varying learning rates achieves better or at least
comparable performance with accelerated methods, such as
Catalyst [40] and Katyusha [48]. The main contributions of
this paper are summarized below.

m

m−1

(cid:80)m

k=1 xs

k=1 xs
k (Option II), and xs+1

• The snapshot and starting points of VR-SGD are set
to two different vectors, i.e., (cid:101)xs = 1
k (Option
(cid:80)m−1
I) or (cid:101)xs = 1
m. In
particular, we ﬁnd that the settings of VR-SGD allow
us to take much larger learning rates than SVRG,
e.g., 1/L vs. 1/(10L), and thus signiﬁcantly speed
up its convergence in practice. Moreover, VR-SGD
has an advantage over SVRG in terms of robustness
of learning rate selection.

0 = xs

• Unlike proximal stochastic gradient methods, e.g.,
Prox-SVRG and Katyusha, which have a uniﬁed
update rule for the two cases of smooth and non-
smooth objectives (see Section 2.2 for details), VR-
SGD employs two different update rules for the
two cases, respectively, as in (12) and (13) below.
Empirical results show that gradient update rules as
in (12) for smooth optimization problems are better
choices than proximal update formulas as in (10).
• We provide the convergence guarantees of VR-SGD
for solving smooth/non-smooth and non-strongly
convex (or general convex) functions. In comparison,
SVRG and Prox-SVRG do not have any convergence
guarantees, as shown in Table 3.

• Moreover, we also present a momentum accelerated
variant of VR-SGD, discuss their equivalent relation-
ship, and empirically verify that they achieve similar
performance to their variant that attains the optimal
convergence rate O(1/T 2).
Finally, we theoretically analyze the convergence
properties of VR-SGD with Option I or Option II for
smooth/non-smooth and strongly convex functions,
which show that VR-SGD attains linear convergence.

•

per epoch, and the learning rate η.

Initialize: (cid:101)x0.
1: for s = 1, 2, . . . , S do
2:
3:
4:

(cid:101)µs = 1
i=1∇fi((cid:101)xs−1), xs
for k = 0, 1, . . . , m − 1 do

(cid:80)n

n

0 = (cid:101)xs−1;

k

Pick is
k uniformly at random from [n];
(xs
(cid:101)∇fis
k) = ∇fis
Option I: xs
or xs

(xs
k) − ∇fis
(cid:104)
k+1 = xs
k − η
(cid:101)∇fis
(cid:16)
k+1 = Proxg
xs
k − η (cid:101)∇fis
η

((cid:101)xs−1) + (cid:101)µs;
(cid:105)
k) +∇g(xs
(xs
k)
(cid:17)
(xs
k)
;

k

k

k

k

,

5:

6:

7:

Option II:
xs
k+1= arg miny∈Rd
end for

8:
9: Option I: (cid:101)xs = xs
m;
(cid:80)m
10: Option II: (cid:101)xs = 1
11: end for
Output: (cid:101)xS

m

(cid:110)
g(y)+yT (cid:101)∇fis

(xs

k)+ 1

2η (cid:107)y−xs

k

k(cid:107)2(cid:111)
;

k=1xs
k;

// Last iterate for snapshot (cid:101)x
// Iterate averaging for (cid:101)x

2 PRELIMINARY AND RELATED WORK
Throughout this paper, we use (cid:107) · (cid:107) to denote the (cid:96)2-norm
(also known as the standard Euclidean norm), and (cid:107) · (cid:107)1 is
the (cid:96)1-norm, i.e., (cid:107)x(cid:107)1 = (cid:80)d
i=1|xi|. ∇f (·) denotes the full
gradient of f (·) if it is differentiable, or ∂f (·) the subgradient
if f (·) is only Lipschitz continuous. For each epoch s ∈ [S]
and inner iteration k ∈ {0, 1, . . . , m−1}, is
k ∈ [n] is the random
chosen index. We mostly focus on the case of Problem (1)
when each fi(·) is L-smooth1, and F (·) is µ-strongly convex.
The two common assumptions are deﬁned as follows.

2.1 Basic Assumptions
Assumption 1 (Smoothness). Each fi(·) is L-smooth, that is,
there exists a constant L > 0 such that for all x, y ∈ Rd,

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107).

(6)

Assumption 2 (Strong Convexity). F (x) is µ-strongly convex,
i.e., there exists a constant µ > 0 such that for all x, y ∈ Rd,

F (y) ≥ F (x) + (cid:104)∇F (x), y − x(cid:105) +

µ
2

(cid:107)x − y(cid:107)2.

(7)

Note that when g(·) is non-smooth, the inequality in (7)
needs to be revised by simply replacing the gradient ∇F (x)
with an arbitrary sub-gradient of F (·) at x. In contrast,
for a non-strongly convex or general convex function, the
inequality in (7) can always be satisﬁed with µ = 0.

2.2 Related Work

To speed up standard and proximal SGD, many stochastic
variance reduced methods [29], [30], [31], [37] have been
proposed for some special cases of Problem (1). In the case
when each fi(x) is L-smooth, f (x) is µ-strongly convex,
and g(x) ≡ 0, Roux et al. [29] proposed a stochastic average
gradient (SAG) method, which attains linear convergence.

1. In fact, we can extend the theoretical results for the case, when the
gradients of all component functions have the same Lipschitz constant
L, to the more general case, when some component functions fi(·) have
different degrees of smoothness.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

4

However, SAG, as well as other incremental aggregated gra-
dient methods such as SAGA [31], needs to store all gradi-
ents, so that O(nd) memory is required in general [43]. Sim-
ilarly, SDCA [30] requires storage of all dual variables [1],
which uses O(n) memory. In contrast, SVRG proposed by
Johnson and Zhang [1], as well as Prox-SVRG [34], has the
similar convergence rate to SAG and SDCA, but without the
memory requirements of all gradients and dual variables.
In particular, the SVRG estimator in (5) may be the most
popular choice for stochastic gradient estimators. The update
rule of SVRG for the case of Problem (1) when g(·) ≡ 0 is

k+1 = xs
xs

k − η (cid:101)∇fis

(xs

k).

k

(8)

k

(xs

k − η[ (cid:101)∇fis

k+1 = xs

k) + ∇g(xs

When the smooth regularizer g(·) (cid:54)= 0, the update rule in
(8) becomes: xs
k)]. Although
the original SVRG in [1] only has convergence guarantees
for the special case of Problem (1), when each fi(x) is L-
smooth, f (x) is µ-strongly convex, and g(x) ≡ 0, we can
extend SVRG to the proximal setting by introducing the
proximal operator in (3), as shown in Line 7 of Algorithm 1.
Based on the SVRG estimator in (5), some accelerated
algorithms [39], [40], [48] have been proposed. The proximal
update rules of Katyusha [48] are formulated as follows:

k+1 = w1ys
xs
ys
k+1 = arg min

k + w2 (cid:101)xs−1 + (1 − w1 − w2)zs
k,
(xs

k(cid:107)2 +yT (cid:101)∇fis

(cid:107)y−ys

k

k+1)+g(y)

(cid:27)

,

(cid:26) 1
2η
(cid:26)3L
2

y∈Rd

zs
k+1 = arg min

z∈Rd

(cid:107)z−xs

k+1(cid:107)2 +zT (cid:101)∇fis

(xs

k

(cid:27)
k+1)+g(z)

(9a)

(9b)

(9c)

where w1, w2 ∈ [0, 1] are two parameters. To eliminate the
need for parameter tuning, η is set to 1/(3w1L), and w2
is ﬁxed to 0.5 in [48]. In addition, [16], [17], [18] applied
efﬁcient stochastic solvers to compute leading eigenvectors
of a symmetric matrix or generalized eigenvectors of two
symmetric matrices. The ﬁrst such method is VR-PCA pro-
posed by Shamir [17], and the convergence properties of
VR-PCA for such a non-convex problem are also provided.
Garber et al. [18] analyzed the convergence rate of SVRG
when f (·) is a convex function that is a sum of non-
convex component functions. Moreover, [4], [5] and [58]
proved that SVRG and SAGA with minor modiﬁcations
can converge asymptotically to a stationary point of non-
convex functions. Some sparse approximation, parallel and
distributed variants [50], [59], [60], [61], [62] of accelerated
SGD methods have also been proposed.

An important class of stochastic methods is the prox-
imal stochastic gradient (Prox-SG) method, such as Prox-
SVRG [34], SAGA [31], and Katyusha [48]. Different from
standard variance reduction SGD methods such as SVRG,
the Prox-SG method has a uniﬁed update rule for both
smooth and non-smooth cases of g(·). For instance, the
update rule of Prox-SVRG [34] is formulated as follows:

(cid:26)

xs
k+1 = arg min

y∈Rd

g(y)+yT (cid:101)∇fis

(xs

k)+

k

(cid:107)y−xs

k(cid:107)2

(cid:27)

.

(10)

1
2η

For the sake of completeness, the details of Prox-SVRG [34]
are shown in Algorithm 1 with Option II. When g(·) is the

widely used (cid:96)2-norm regularizer, i.e., g(·) = (λ1/2)(cid:107) · (cid:107)2, the
proximal update formula in (10) becomes

xs
k+1 =

(cid:104)

1
1 + λ1η

xs
k − η (cid:101)∇fis

k

(xs
k)

(cid:105)

.

(11)

3 VARIANCE REDUCED SGD
In this section, we propose an efﬁcient variance reduced
stochastic gradient descent (VR-SGD) algorithm, as shown
in Algorithm 2. Different from the choices of the snapshot
and starting points in SVRG [1] and Prox-SVRG [34], the two
vectors of each epoch in VR-SGD are set to the average and
last iterate of the previous epoch, respectively. Moreover,
unlike existing proximal stochastic methods, we design two
different update rules for smooth and non-smooth objective
functions, respectively.

k

(xs

(cid:80)m

3.1 Snapshot and Starting Points
Like SVRG, VR-SGD is also divided into S epochs, and each
epoch consists of m stochastic gradient steps, where m is
usually chosen to be Θ(n), as suggested in [1], [34], [48].
Within each epoch, we need to compute the full gradient
∇f ((cid:101)xs) at the snapshot (cid:101)xs and use it to deﬁne the variance
reduced stochastic gradient estimator (cid:101)∇fis
k) in (5). Unlike
SVRG, whose snapshot is set to the last iterate of the previ-
ous epoch, the snapshot (cid:101)xs of VR-SGD is set to the average
of the previous epoch, e.g., (cid:101)xs = 1
k in Option I of
Algorithm 2, which leads to better robustness to gradient
noise2, as also suggested in [36], [47], [66]. In fact, the choice
of Option II in Algorithm 2, i.e., (cid:101)xs = 1
k, also
works well in practice, as shown in Fig. 2 in the Supple-
mentary Material. Therefore, we provide the convergence
guarantees for our algorithm with either Option I or Option
II in the next section. In particular, we ﬁnd that one of the
effects of the choice in Option I or Option II of Algorithm 2
is to allow taking much larger learning rates or step sizes
than SVRG in practice, e.g., 1/L for VR-SGD vs. 1/(10L) for
SVRG (see Fig. 11). Actually, a larger learning rate enjoyed
by VR-SGD means that the variance of its stochastic gradient
estimator goes asymptotically to zero faster.

k=1 xs

k=1 xs

(cid:80)m−1

m−1

m

Unlike Prox-SVRG [34] whose starting point is initialized
to the average of the previous epoch, the starting point of
VR-SGD is set to the last iterate of the previous epoch. That
is, in VR-SGD, the last iterate of the previous epoch becomes
the new starting point, while the two points of Prox-SVRG
are completely different, thereby leading to relatively slow
convergence in general. Both the starting and snapshot
points of SVRG [1] are set to the last iterate of the previous
epoch3, while the two points of Prox-SVRG [34] are set to

2. It should be emphasized that the noise introduced by random
sampling is inevitable, and generally slows down the convergence
speed in this sense. However, SGD and its variants are probably
the mostly used optimization algorithms for deep learning [63]. In
particular, [64] has shown that by adding gradient noise at each step,
noisy gradient descent can escape the saddle points efﬁciently and
converge to a local minimum of the non-convex minimization problem,
e.g., the application of deep neural networks in [65].

3. Note that the theoretical convergence of the original SVRG [1]
relies on its Option II, i.e., both (cid:101)xs and xs+1
k, where k
is randomly chosen from {1, 2, . . . , m}. However, the empirical results
in [1] suggest that Option I is a better choice than its Option II, and
the convergence guarantee of SVRG with Option I for strongly convex
objective functions is provided in [67].

are set to xs

0

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

5

Algorithm 2 VR-SGD for solving smooth problems
Input: The number of epochs S, and the number of itera-

tions m per epoch.

0 = (cid:101)x0, and {ηs}.
(cid:80)n

Initialize: x1
1: for s = 1, 2, . . . , S do
2:
3:
4:

(cid:101)µs = 1
for k = 0, 1, . . . , m − 1 do

i=1∇fi((cid:101)xs−1);

n

// Compute the full gradient

k

k

k

k

5:

((cid:101)xs−1) + (cid:101)µs;
k)];

(xs

k) − ∇fis
(xs
k) +∇g(xs

Pick is
k uniformly at random from [n];
(xs
(cid:101)∇fis
k) = ∇fis
xs
k+1= xs
k −ηs[ (cid:101)∇fis
6:
end for
7:
8: Option I: (cid:101)xs = 1
9: Option II: (cid:101)xs = 1
xs+1
0 = xs
m;
10:
11: end for
Output: (cid:98)xS = (cid:101)xS, if F ((cid:101)xS) ≤ F ( 1

k=1xs
k;
(cid:80)m−1
k=1 xs
// Initiate xs+1

(cid:80)m

(cid:80)S

m−1

m

S

0

// Iterate averaging for (cid:101)x
k; // Iterate averaging for (cid:101)x
for the next epoch

s=1 (cid:101)xs), and (cid:98)xS =

(cid:80)S

1
S

s=1 (cid:101)xs otherwise.

(a) Logistic regression: λ = 10−4 (left) and λ = 10−5 (right)

the average of the previous epoch (also suggested in [1]).
By setting the starting and snapshot points in VR-SGD to
the two different vectors mentioned above, the convergence
analysis of VR-SGD becomes signiﬁcantly more challenging
than that of SVRG and Prox-SVRG, as shown in Section 4.

Fig. 1. Comparison of SVRG [1] and VR-SGD with different learning
rates for solving (cid:96)2-norm regularized logistic regression and ridge re-
gression on Covtype. Note that the blue lines stand for the results of
SVRG, while the red lines correspond to the results of VR-SGD (best
viewed in colors).

(b) Ridge regression: λ = 10−4 (left) and λ = 10−5 (right)

3.2 The VR-SGD Algorithm

3.3 VR-SGD for Non-Strongly Convex Objectives

In this part, we propose an efﬁcient VR-SGD algorithm to
solve Problem (1), as outlined in Algorithm 2 for the case of
smooth objective functions. It is well known that the original
SVRG [1] only works for the case of smooth minimization
problems. However, in many machine learning applications,
e.g., elastic net regularized logistic regression, the strongly
convex objective function F (x) is non-smooth. To solve
this class of problems, the proximal variant of SVRG, Prox-
SVRG [34], was subsequently proposed. Unlike the original
SVRG, VR-SGD can not only solve smooth objective func-
tions, but also directly tackle non-smooth ones. That is, when
the regularizer g(x) is smooth (e.g., the (cid:96)2-norm regularizer),
the key update rule of VR-SGD is
k+1 = xs
xs

k) + ∇g(xs

k − ηs[ (cid:101)∇fis

k)].

(xs

(12)

k

When g(x) is non-smooth (e.g., the (cid:96)1-norm regularizer), the
key update rule of VR-SGD in Algorithm 2 becomes

k+1 = Prox g
xs
ηs

(cid:16)

xs
k − ηs (cid:101)∇fis

k

(cid:17)
(xs
k)

.

(13)

Unlike the proximal stochastic methods such as Prox-
SVRG [34], all of which have a uniﬁed update rule as in (10)
for both the smooth and non-smooth cases of g(·), VR-SGD
has two different update rules for the two cases, as in (12)
and (13). Fig. 11 demonstrates that VR-SGD has a signiﬁcant
advantage over SVRG in terms of robustness of learning rate
selection. That is, VR-SGD yields good performance within
the range of the learning rate from 0.2/L to 1.2/L, whereas
the performance of SVRG is very sensitive to the selection
of learning rates. Thus, VR-SGD is convenient to be applied
in various real-world problems of machine learning. In fact,
VR-SGD can use much larger learning rates than SVRG for
ridge regression problems in practice, e.g., 8/(5L) for VR-
SGD vs. 1/(5L) for SVRG, as shown in Fig. 1(b).

Although many stochastic variance reduced methods have
been proposed, most of them, including SVRG and Prox-
SVRG, only have convergence guarantees for the case of
Problem (1), when F (x) is strongly convex. However, F (x)
may be non-strongly convex in many machine learning
applications, such as Lasso and (cid:96)1-norm regularized logistic
regression. As suggested in [48], [68], this class of problems
can be transformed into strongly convex ones by adding
0(cid:107)2, which can be efﬁciently
a proximal term (τ /2)(cid:107)x − xs
solved by Algorithm 2. However, the reduction technique
may degrade the performance of the involved algorithms
both in theory and in practice [44]. Thus, we use VR-SGD to
directly solve non-strongly convex problems.

The learning rate ηs of Algorithm 2 can be ﬁxed to a
constant. Inspired by existing accelerated stochastic algo-
rithms [36], [48], the learning rate in Algorithm 2 can be
gradually increased in early iterations for both strongly
convex and non-strongly convex problems, which leads to
faster convergence (see Fig. 3 in the Supplementary Ma-
terial). Different from SGD and Katyusha [48], where the
learning rate of the former requires to be gradually decayed
and that of the latter needs to be gradually increased, the
update rule of ηs in Algorithm 2 is deﬁned as follows: η0 is
an initial learning rate, and for any s ≥ 1,

ηs = η0/ max{α, 2/(s + 1)}

(14)

where 0 < α ≤ 1 is a given constant, e.g., α = 0.2.

3.4 Extensions of VR-SGD

It has been shown in [38], [39] that mini-batching can effec-
tively decrease the variance of stochastic gradient estimates.
Therefore, we ﬁrst extend the proposed VR-SGD method
to the mini-batch setting, as well as its convergence results

05101520253010−1210−810−4Number of effective passesObjective minus bestη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=1.2/L0510152025303510−1210−810−4Number of effective passesObjective minus bestη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=1.2/L010203040506010−1210−810−4Number of effective passesObjective minus bestη=0.2/Lη=0.4/Lη=0.8/Lη=1.2/Lη=1.6/Lη=0.2/Lη=0.4/Lη=0.8/Lη=1.2/Lη=1.6/L010203040506010−1210−810−4Number of effective passesObjective minus bestη=0.2/Lη=0.4/Lη=0.8/Lη=1.2/Lη=1.6/Lη=0.2/Lη=0.4/Lη=0.8/Lη=1.2/Lη=1.6/LIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

6

below. Here, we denote by b the mini-batch size and I s
k the
selected random index set Ik ⊂ [n] for each outer-iteration
s ∈ [S] and inner-iteration k ∈ {0, 1, . . . , m−1}.

Deﬁnition 2. The stochastic variance reduced gradient estimator
in the mini-batch setting is deﬁned as

(cid:101)∇fI s

k

(xs

k) =

1
b

(cid:88)

i∈I s
k

(cid:2)∇fi(xs

k)−∇fi((cid:101)xs−1)(cid:3)+∇f ((cid:101)xs−1)

(15)

where I s

k ⊂ [n] is a mini-batch of size b.

If some component functions are non-smooth, we can
use the proximal operator oracle [68] or the Nesterov’s
smoothing [69] and homotopy smoothing [70] techniques to
smoothen them, and thereby obtain their smoothed approx-
imations. In addition, we can directly extend our VR-SGD
method to the non-smooth setting as in [36] (e.g., Algorithm
3 in [36]) without using any smoothing techniques.

Considering that each component function fi(x) may
have different degrees of smoothness, picking the random
index is
k from a non-uniform distribution is a much better
choice than commonly used uniform random sampling [71],
[72], as well as without-replacement sampling vs. with-
replacement sampling [73]. This can be done using the same
techniques in [34], [48], i.e., the sampling probabilities for
all fi(x) are proportional to their Lipschitz constants, i.e.,
pi = Li/(cid:80)n
j=1Lj. VR-SGD can also be combined with other
accelerated techniques used for SVRG. For instance, the
epoch length of VR-SGD can be automatically determined
by the techniques in [44], [74], instead of a ﬁxed epoch
length. We can reduce the number of gradient calculations
in early iterations as in [43], [44], which leads to faster
convergence in general (see Section 6 for details). Moreover,
we can introduce the Nesterov’s acceleration techniques in
[25], [39], [40], [41], [42] and momentum acceleration tricks
in [36], [48], [75] to further improve the performance of VR-
SGD.

4 ALGORITHM ANALYSIS

In this section, we provide the convergence guarantees of
VR-SGD for solving both smooth and non-smooth general
convex problems, and extend the results to the mini-batch
setting. We also study the convergence properties of VR-
SGD for solving both smooth and non-smooth strongly con-
vex objective functions. Moreover, we discuss the equivalent
relationship between VR-SGD and its momentum acceler-
ated variant, as well as some of its extensions.

4.1 Convergence Properties: Non-strongly Convex

In this part, we analyze the convergence properties of VR-
SGD for solving more general non-strongly convex prob-
lems. Considering that the proposed algorithm (i.e., Algo-
rithm 2) has two different update rules for smooth and non-
smooth cases, we give the convergence guarantees of VR-
SGD for the two cases as follows.

(cid:107) (cid:101)∇fis

simplify analysis, we denote F (x) by f (x), that is, fi(x) :=
fi(x) + g(x) for all i = 1, 2, . . . , n, and then g(x) ≡ 0.
Lemma 1 (Variance bound). Let x∗ be the optimal solution
of Problem (1). Suppose Assumption 1 holds. Then the following
inequality holds
(cid:104)
E

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)].
The proofs of this lemma, the lemmas and theorems
below are all
included in the Supplementary Material.
Lemma 3 provides the upper bound on the expected vari-
ance of the variance reduced gradient estimator in (5), i.e.,
the SVRG estimator. For Algorithm 2 with Option II and a
ﬁxed learning rate η, we have the following result.

k)−∇f (xs

≤ 4L[f (xs

k)(cid:107)2(cid:105)

(xs

k

Theorem 1 (Smooth objectives). Suppose Assumption 1 holds.
Then the following inequality holds

(cid:104)
E

(cid:105)
f ((cid:98)xS)

− f (x∗) ≤

2(m + 1)
[γ − 4m + 2]S

[f ((cid:101)x0) − f (x∗)]

+

β(β − 1)L
2[γ − 4m + 2]S

(cid:107)(cid:101)x0 − x∗(cid:107)2

where γ = (β −1)(m−1), and β = 1/(Lη).

From Theorem 1 and its proof, one can see that our
convergence analysis is very different from that of existing
stochastic methods, such as SVRG [1], Prox-SVRG [34], and
SVRG++ [44]. Similarly, the convergence of Algorithm 2
with Option I and a ﬁxed learning rate can be guaranteed,
as stated in Theorem 6 in the Supplementary Material. All
the results show that VR-SGD attains a convergence rate of
O(1/T ) for non-strongly convex functions.

4.1.2 Non-Smooth Objective Functions

We also provide the convergence guarantee of Algorithm 2
with Option I and (13) for solving Problem (1) when F (x) is
non-smooth and non-strongly convex, as shown below.

Theorem 2 (Non-smooth objectives). Suppose Assumption 1
holds. Then the following inequality holds

− F (x∗)

(cid:104)
E

(cid:105)
F ((cid:98)xS)
2(m+1)
(β −5)mS

≤

[F ((cid:101)x0)− F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Similarly, the convergence of Algorithm 2 with Option
II and a ﬁxed learning rate can be guaranteed, as stated in
Corollary 4 in the Supplementary Material.

4.1.3 Mini-Batch Settings
The upper bound on the variance of (cid:101)∇fis
the mini-batch setting as follows.

k

(xs

k) is extended to

Corollary 1 (Variance bound of mini-batch). If each fi(·) is
convex and L-smooth, then the following inequality holds

(cid:104)

E

(xs

(cid:107) (cid:101)∇fI s
≤ 4Lδ(b)[F (xs

k

k) − ∇f (xs
k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)]

k)(cid:107)2(cid:105)

4.1.1 Smooth Objective Functions

where δ(b) = (n−b)/[(n−1)b].

We ﬁrst provide the convergence guarantee of our algorithm
for solving Problem (1) when F (x) is smooth. In order to

This corollary is essentially identical

to Theorem 4
in [38], and hence its proof is omitted. It is not hard to verify

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

7

that 0 ≤ δ(b) ≤ 1. Based on the variance upper bound, we
further analyze the convergence properties of VR-SGD in
the mini-batch setting, as shown below.

Theorem 3 (Mini-batch). If each fi(·) is convex and L-smooth,
then the following inequality holds

− F (x∗)

(cid:105)

(cid:104)
E

F ((cid:98)xS)
2δ(b)(m+1)
ζmS

≤

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3)+

β(β −1)L
2ζmS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3)

where ζ = β −1−4δ(b).

From Theorem 3, one can see that when b = n (i.e., the
batch setting), δ(n) = 0, and the ﬁrst term on the right-hand
side of the above inequality diminishes. That is, VR-SGD
degenerates to a batch method. When b = 1, we have δ(1) =
1, and thus Theorem 3 degenerates to Theorem 2.

0 = v1

0 = (cid:101)x0, {ws}, α > 0, and η0.

Algorithm 3 The momentum accelerated algorithm
Input: S and m.
Initialize: x1
1: for s = 1, 2, . . . , S do
(cid:80)n
(cid:101)µs = 1
2:
3: Option I: vs
4:
5:

i=1∇fi((cid:101)xs−1), ηs = η0/ max{α, 2/(s+1)};
0 = xs
for k = 0, 1, . . . , m − 1 do

0, or Option II: xs

0 = wsvs

0+(1−ws)(cid:101)xs−1;

n

k

k

k

6:

((cid:101)xs−1) + (cid:101)µs;
k)];

Pick is
k uniformly at random from [n];
(xs
(xs
(cid:101)∇fis
k) = ∇fis
k) − ∇fis
(xs
k) + ∇g(xs
vs
k+1 = vs
k − ηs[ (cid:101)∇fis
7:
xs
k+1 − (cid:101)xs−1);
k+1 = (cid:101)xs−1 + ws(vs
8:
end for
9:
(cid:101)xs = 1
10:
11: Option I: xs+1
12: end for
Output: (cid:98)xS = (cid:101)xS, if F ((cid:101)xS) ≤ F ( 1

m, or Option II: vs+1

k=1xs
k;
0 = xs

(cid:80)m

(cid:80)S

m

S

k

0 = vs
m;

s=1 (cid:101)xs), and (cid:98)xS =

(cid:80)S

1
S

s=1 (cid:101)xs otherwise.

4.2 Convergence Properties: Strongly Convex

4.3 Equivalent to Its Momentum Accelerated Variant

We also analyze the convergence properties of VR-SGD for
solving strongly convex problems. We ﬁrst give the following
convergence result for Algorithm 2 with Option II.

Theorem 4 (Strongly convex). Suppose Assumptions 1, 2 and
3 in the Supplementary Material hold, and m is sufﬁciently large
so that

ρ :=

2Lη(m+c)
(m−1)(1−3Lη)

+

c(1−Lη)
µη(m−1)(1−3Lη)

< 1

where c is a constant. Then Algorithm 2 with Option II has the
following geometric convergence in expectation:

Inspired by the success of the momentum technique in
our previous work [6], [50], [75], we present a momentum
accelerated variant of Algorithm 2, as shown in Algo-
rithm 3. Unlike existing momentum techniques, e.g., [19],
[22], [25], [39], [40], [48], we use the convex combination
of the snapshot (cid:101)xs−1 and latest iterate vs
k for acceleration,
k + (1 − ws)(cid:101)xs−1. It is not
i.e., (cid:101)xs−1 + ws(vs
hard to verify that Algorithm 2 with Option I is equiva-
lent to its variant (i.e., Algorithm 3 with Option I), when
ws = max{α, 2/(s + 1)} and α is sufﬁciently small (see the
Supplementary Material for their equivalent analysis). We
emphasize that the only difference between Options I and II
in Algorithm 3 is the initialization of xs

k+1 − (cid:101)xs−1) = wsvs

0 and vs
0.

(cid:104)

E

F ((cid:98)xS) − F (x∗)

(cid:105)

≤ ρS(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) .

Theorem 5. Suppose Assumption 1 holds. Then the following
inequality holds:

We also provide the linear convergence guarantees for
Algorithm 2 with Option I for solving non-smooth and
strongly convex functions, as stated in Theorem 7 in the
Supplementary Material. Similarly, the linear convergence
of Algorithm 2 can be guaranteed for the smooth strongly-
convex case. All the theoretical results show that VR-SGD
attains a linear convergence rate and at most the oracle
complexity of O((n+L/µ) log(1/(cid:15))) for both smooth and
non-smooth strongly convex functions. In contrast, the con-
vergence of SVRG [1] is only guaranteed for smooth and
strongly convex problems.

Although the learning rate in Theorem 4 needs to be
less than 1/(3L), we can use much larger learning rates in
practice, e.g., η = 1/L. However, it can be easily veriﬁed
that the learning rate of SVRG should be less than 1/(4L)
in theory, and adopting a larger learning rate for SVRG
is not always helpful in practice, which means that VR-
SGD can use much larger learning rates than SVRG both in
theory and in practice. In other words, although they have
the same theoretical convergence rate, VR-SGD converges
signiﬁcantly faster than SVRG in practice, as shown by our
experiments. Note that similar to the convergence analysis
in [4], [5], [58], the convergence of VR-SGD for some non-
convex problems can also be guaranteed.

E[F ((cid:98)xS) − F (x∗)]
4(1−w1)
1(S +1)2 [F ((cid:101)x0) − F (x∗)] +
w2

≤

mη0(S +1)2 (cid:107)x∗ − (cid:101)x0(cid:107)2.
Choosing m = Θ(n), Algorithm 3 with Option II achieves an
(cid:15)-suboptimal solution (i.e., E[F ((cid:98)xS)]−F (x∗)≤ε) using at most
O(n(cid:112)[F ((cid:101)x0)−F (x∗)]/ε+(cid:112)nL/ε(cid:107)(cid:101)x0 −x∗(cid:107)) iterations.

2

This theorem shows that

the oracle complexity of
Algorithm 3 with Option II is consistent with that of
Katyusha [48], and is better than that of accelerated deter-
ministic methods (e.g., AGD [20]), (i.e., O(n(cid:112)L/ε)), which
are also veriﬁed by the experimental results in Fig. 2.
Our algorithm also achieves the optimal convergence rate
O(1/T 2) for non-strongly convex functions as in [48], [49].
Fig. 2 shows that Katyusha and Algorithm 3 with Option
II have similar performance as Algorithms 2 and 3 with
Option I (η0=3/(5L)) in terms of number of effective passes.
Clearly, Algorithm 3 and Katyusha have higher complexity
per iteration than Algorithm 2. Thus, we only report the
results of VR-SGD (i.e., Algorithm 2) in Section 5.

4.4 Complexity Analysis

From Algorithm 2, we can see that the per-iteration cost
k),
of VR-SGD is dominated by the computation of ∇fis

(xs

k

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

8

(a) Small dataset: MNIST

(b) Large dataset: Epsilon

(a) (cid:96)2-norm regularized logistic regression: λ = 10−5

Fig. 2. Comparison of AGD [20], Katyusha [48], Algorithm 3 with Option
I and II, and VR-SGD for solving logistic regression with λ = 0.

k

((cid:101)xs−1), and ∇g(xs

∇fis
k) or the proximal update in (13). Thus,
the complexity is O(d), which is as low as that of SVRG [1]
and Prox-SVRG [34]. In fact, for some ERM problems, we
can save the intermediate gradients ∇fi((cid:101)xs−1) in the com-
putation of (cid:101)µs, which generally requires O(n) additional
storage. As a result, each epoch only requires (n+m) compo-
nent gradient evaluations. In addition, for extremely sparse
data, we can introduce the lazy update tricks in [38], [76],
[77] to our algorithm, and perform the update steps in (12)
and (13) only for the non-zero dimensions of each sample,
rather than all dimensions. In other words, the per-iteration
complexity of VR-SGD can be improved from O(d) to O(d(cid:48)),
where d(cid:48) ≤ d is the sparsity of feature vectors. Moreover,
VR-SGD has a much lower per-iteration complexity than
existing accelerated stochastic variance reduction methods
such as Katyusha [48], which have more updating steps for
additional variables, as shown in (9a)-(9c).

5 EXPERIMENTAL RESULTS
In this section, we evaluate the performance of VR-SGD for
solving a number of convex and non-convex ERM problems
(such as logistic regression, Lasso and ridge regression),
and compare its performance with several state-of-the-art
stochastic variance reduced methods (including SVRG [1],
Prox-SVRG [34], SAGA [31]) and accelerated methods, such
as Catalyst [40] and Katyusha [48]. Moreover, we apply VR-
SGD to solve other machine learning problems, such as ERM
with non-convex loss and leading eigenvalue computation.

5.1 Experimental Setup

We used several publicly available data sets in the exper-
iments: Adult (also called a9a), Covtype, Epsilon, MNIST,
and RCV1, all of which can be downloaded from the LIB-
SVM Data website4. It should be noted that each sample of
these date sets was normalized so that they have unit length as
in [34], [36], which leads to the same upper bound on the Lipschitz
constants Li, i.e., L = Li for all i = 1, . . . , n. As suggested
in [1], [34], [48], the epoch length is set to m = 2n for
the stochastic variance reduced methods, SVRG [1], Prox-
SVRG [34], Catalyst [40], and Katyusha [48], as well as VR-
SGD. Then the only parameter we have to tune by hand
is the learning rate, η. More speciﬁcally, we select learning
rates from {10j, 2.5 × 10j, 5 × 10j, 7.5 × 10j, 10j+1}, where
j ∈ {−2, −1, 0}. Since Katyusha has a much higher per-
iteration complexity than SVRG and VR-SGD, we compare

4. https://www.csie.ntu.edu.tw/∼cjlin/libsvm/

(b) (cid:96)1-norm regularized logistic regression: λ = 10−5

Fig. 3. Comparison of deterministic and stochastic methods on Adult.

their performance in terms of both the number of effective
passes and running time (seconds), where computing a
single full gradient or evaluating n component gradients is
considered as one effective pass over the data. For fair com-
parison, we implemented SVRG, Prox-SVRG, SAGA, Cata-
lyst, Katyusha, and VR-SGD in C++ with a Matlab interface,
as well as their sparse versions with lazy update tricks, and
performed all the experiments on a PC with an Intel i5-4570
CPU and 16GB RAM. The source code of all the methods is
available at https://github.com/jnhujnhu/VR-SGD.

5.2 Deterministic Methods vs. Stochastic Methods

In this subsection, we compare the performance of stochastic
methods (including SGD, SVRG, Katyusha, and VR-SGD)
with that of deterministic methods such as AGD [19], [20]
and APG [22] for solving strongly and non-strongly convex
problems. Note that the important momentum parameter w
µ) as in [78], while that of
L+
of AGD is w = (
APG is deﬁned as follows: wk = (αk −1)/αk+1 for all k ≥ 1
1+4α2
[22], where αk+1 = (1+

k)/2, and α1 = 1.

µ)/(

L−

(cid:113)

√

√

√

√

Fig. 3 shows the the objective gap (i.e., F (xs)−F (x∗)) of
those deterministic and stochastic methods for solving (cid:96)2-
norm and (cid:96)1-norm regularized logistic regression problems
(see the Supplementary Material for more results). It can be
seen that the accelerated deterministic methods and SGD
have similar convergence speed, and APG usually performs
slightly better than SGD for non-strongly convex problems.
The variance reduction methods (e.g., SVRG, Katyusha
and VR-SGD) signiﬁcantly outperform the accelerated de-
terministic methods and SGD for both strongly and non-
strongly convex cases, suggesting the importance of vari-
ance reduction techniques. Although accelerated determin-
istic methods have a faster theoretical speed than SVRG for
general convex problems, as discussed in Section 1.2, APG
converges much slower in practice. VR-SGD consistently
outperforms the other methods (including Katyusha) in all
the settings, which veriﬁes the effectiveness of VR-SGD.

05010015010−310−210−1Number of effective passesObjective minus bestAGDAlgorithm 3 (Option I)Algorithm 3 (Option II)VR−SGDKatyusha05010015010−610−410−2Number of effective passesObjective minus bestAGDAlgorithm 3 (Option I)Algorithm 3 (Option II)VR−SGDKatyusha02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD00.511.5210−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD012345610−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

9

TABLE 2
The three choices of snapshot and starting points for stochastic variance reduction optimization.

Option I
m and xs+1

0 = xs
m

(cid:101)xs = xs

Option II
k and xs+1

0 = 1
m

(cid:80)m

k=1xs

(cid:80)m

k=1xs
k

(cid:101)xs = 1

m

(cid:80)m

k=1xs

k and xs+1

0 = xs
m

Option III

(cid:101)xs = 1

m

(a) Ridge regression: λ = 10−5 (left) and λ = 10−6 (right)

(b) Lasso: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 4. Comparison of the algorithms with Options I, II, and III for solving ridge regression and Lasso on Covtype. In each plot, the vertical axis
shows the objective value minus the minimum, and the horizontal axis denotes the number of effective passes.

(a) Adult: λ = 10−3 (left) and λ = 10−4 (right)

(b) Covtype: λ = 10−5 (left) and λ = 10−6 (right)

Fig. 5. Comparison of SVRG [1], Katyusha [48], VR-SGD and their proximal versions for solving ridge regression problems. In each plot, the vertical
axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes over data.

5.3 Different Choices for Snapshot and Starting Points

5.4 Common SG Updates vs. Prox-SG Updates

0

In the practical implementation of SVRG [1], both the snap-
shot (cid:101)xs and starting point xs+1
in each epoch are set to
the last iterate xs
m of the previous epoch (i.e., Option I in
Algorithm 1), while the two vectors in [34] are set to the
(cid:80)m
average point of the previous epoch, 1
k (i.e., Option
m
II in Algorithm 1). In contrast, (cid:101)xs and xs+1
in our algorithm
are set to 1
k=1xs
m (denoted by Option III, i.e.,
m
Option I5 in Algorithm 2), respectively.

k and xs

k=1xs

(cid:80)m

0

We compare the performance of the algorithms with the
three settings (i.e., the Options I, II and III listed in Table 2)
for solving ridge regression and Lasso problems, as shown
in Fig. 4 (see the Supplementary Material for more results).
Except for the three different settings for snapshot and start-
ing points, we use the update rules in (12) and (13) for ridge
regression and Lasso problems, respectively. We can see that
the algorithm with Option III (i.e., Algorithm 2 with Option
I) consistently converges much faster than the algorithms
with Options I and II for both strongly convex and non-
strongly convex cases. This indicates that the setting of
Option III suggested in this paper is a better choice than
Options I and II for stochastic optimization.

5. As Options I and II in Algorithm 2 achieve very similar perfor-

mance, we only report the results of our algorithm with Option I.

In this subsection, we compare the original Katyusha algo-
rithm in [48] with the slightly modiﬁed Katyusha algorithm
(denoted by Katyusha-I). In Katyusha-I, only the following
two update rules are used to replace the original proximal
stochastic gradient update rules in (9b) and (9c).

ys
k+1 = ys
k+1 = xs
zs

k − η[ (cid:101)∇fik(xs
k+1 − [ (cid:101)∇fik(xs

k+1) + ∇g(xs
k+1) + ∇g(xs

k+1)],
k+1)]/(3L).

(16)

Similarly, we also implement the proximal versions6 for
the original SVRG (called SVRG-I) and VR-SGD (denoted
by VR-SGD-I) methods, and denote their proximal variants
by SVRG-II and VR-SGD-II, respectively. In addition, the
original Katyusha is denoted by Katyusha-II.

Fig. 5 shows the performance of Katyusha-I and
Katyusha-II for solving ridge regression on the two popular
data sets: Adult and Covtype. We also report the results
of SVRG, VR-SGD, and their proximal variants. It is clear
that Katyusha-I usually performs better than Katyusha-II
(i.e., the original Katyusha [48]), and converges signiﬁcantly
faster in the case when the regularization parameter is 10−4
or 10−6. This seems to be the main reason why Katyusha

6. Here,

the proximal variant of SVRG is different from Prox-
SVRG [34], and their main difference is the choices of both the snapshot
point and starting point. That is, the two vectors of the former are set
to the last iterate xs
m, while those of Prox-SVRG are set to the average
point of the previous epoch, i.e., 1
k=1xs
k.
m

(cid:80)m

05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−IIIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

10

5.6.1 Convex Logistic Regression

In this part, we focus on the following generalized logistic
regression problem for binary classiﬁcation,

min
x∈Rd

1
n

n
(cid:88)

i=1

log(1 + exp(−biaT

i x)) +

λ1
2

(cid:107)x(cid:107)2 + λ2(cid:107)x(cid:107)1 (17)

where {(ai, bi)} is a set of training examples, and λ1, λ2 ≥ 0
are the regularization parameters. Note that when λ2 > 0,
fi(x) = log(1 + exp(−biaT
i x)) + (λ1/2)(cid:107)x(cid:107)2. The formula-
tion (17) includes the (cid:96)2-norm (i.e., λ2 = 0), (cid:96)1-norm (i.e.,
λ1 = 0), and elastic net (i.e., λ1 (cid:54)= 0 and λ2 (cid:54)= 0) regularized
logistic regression problems. Fig. 7 shows how the objective
gap decreases for the (cid:96)2-norm, (cid:96)1-norm, and elastic-net
regularized logistic regression problems, respectively (see
the Supplementary Material for more results). From all the
results, we make the following observations.

• When the problems are well-conditioned (e.g., λ1 =
10−4 or λ2 = 10−4), Prox-SVRG usually converges
faster than SVRG for both strongly convex (e.g.,
(cid:96)2-norm regularized logistic regression) and non-
strongly convex (e.g., (cid:96)1-norm regularized logistic
regression) cases. On the contrary, SVRG often out-
performs Prox-SVRG, when the problems are ill-
conditioned, e.g., λ1 = 10−6 or λ2 = 10−6 (see Figs.
11 and 12 in the Supplementary Material). The main
reason is that they have different initialization set-
tings, i.e., (cid:101)xs = xs
m for SVRG vs.
(cid:101)xs = 1
k=1xs
k=1xs
k for Prox-
SVRG.

m and xs+1
k and xs+1

0 = xs
(cid:80)m
0 = 1
m

(cid:80)m

m

• Katyusha converges much faster than SAGA, SVRG,
Prox-SVRG, and Catalyst in the cases when the prob-
lems are ill-conditioned, e.g., λ1 = 10−6, whereas it
often achieves similar or inferior performance when
the problems are well-conditioned, e.g., λ1 = 10−4
(see Figs. 11, 12 and 13 in the Supplementary Ma-
terial). Note that we implemented the original al-
gorithm with Option I in [48] for Katyusha. Obvi-
ously, the above observation matches the conver-
gence properties of Katyusha provided in [48], that
is, only if mµ/L ≤ 3/4, Katyusha attains the oracle
complexity of O((n+(cid:112)nL/µ) log(1/(cid:15))) for strongly
convex problems.

• VR-SGD converges signiﬁcantly faster than SAGA,
SVRG, Prox-SVRG and Catalyst, especially when
the problems are ill-conditioned, e.g., λ1 = 10−6 or
λ2 = 10−6 (see Figs. 11 and 12 in the Supplementary
Material). The main reason is that VR-SGD can use
much larger learning rates than them (e.g., 1/L for
VR-SGD vs. 1/(10L) for SVRG), which leads to faster
convergence. This further veriﬁes that the settings of
both snapshot and starting points in our algorithm
(i.e., Algorithm 2) are better choices than Options I
and II in Algorithm 1.
In particular, VR-SGD consistently outperforms the
best-known stochastic method, Katyusha, in terms of
the number of passes through the data, especially
when the problems are well-conditioned, e.g., 10−4
and 10−5 (see Figs. 11 and 12 in the Supplementary
Material). Since VR-SGD has a much lower per-
iteration complexity than Katyusha, VR-SGD has

•

(a) λ = 10−5

(b) λ = 10−6

Fig. 6. Comparison of SVRG++ [44], VR-SGD and VR-SGD++ for solv-
ing logistic regression problems on Epsilon.

has inferior performance when the regularization parameter
is relatively large, as shown in Section 6. In contrast, VR-
SGD and its proximal variant have similar performance,
and the former slightly outperforms the latter in most cases
(similar results are also observed for SVRG vs. its proximal
variant). This suggests that stochastic gradient update rules
as in (12) and (16) are better choices than proximal update
rules as in (10), (9b) and (9c) for smooth objective functions.
We also believe that our new insight can help us to design
accelerated stochastic optimization methods.

Both Katyusha-I and Katyusha-II usually outperform
SVRG and its proximal variant, especially when the reg-
ularization parameter is relatively small, e.g., λ = 10−6.
Moreover, it can be seen that both VR-SGD and its prox-
imal variant achieve much better performance than the
other methods in most cases, and are also comparable to
Katyusha-I and Katyusha-II in the remaining cases. This
further veriﬁes that VR-SGD is suitable for various large-
scale machine learning.

5.5 Growing Epoch Size Strategy in Early Iterations

In this subsection, we present a general growing epoch size
strategy in early iterations (i.e., If ms < 2n, ms+1 = (cid:98)ρms(cid:99)
with the factor ρ > 1. Otherwise, ms+1 = ms). Different
from the doubling-epoch technique used in SVRG++ [44]
(i.e., ms+1 = 2ms), we gradually increase the epoch size in
only the early iterations. Similar to the convergence analysis
in Section 4, VR-SGD with the growing epoch size strategy
(called VR-SGD++) can be guaranteed to converge. As sug-
gested in [44], we set m1 = (cid:98)n/4(cid:99) for both SVRG++ and VR-
SGD++, and ρ = 1.75 for VR-SGD++. Note that they use the
same initial learning rate. We compare their performance for
solving (cid:96)2-norm regularized logistic regression, as shown in
Fig. 6 (see the Supplementary Material for more results). All
the results show that VR-SGD++ converges faster than VR-
SGD, which means that reducing the number of gradient
calculations in early iterations can lead to faster convergence
as discussed in [43]. Moreover, both VR-SGD++ and VR-
SGD signiﬁcantly outperform SVRG++, especially when the
regularization parameter is relatively small, e.g., λ = 10−6.

5.6 Real-world Applications

In this subsection, we apply VR-SGD to solve a number
of machine learning problems, e.g., logistic regression, non-
convex ERM and eigenvalue computation.

0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

11

(a) Epsilon: λ1 = 10−6 and λ2 = 0

(b) RCV1: λ1 = 10−5 and λ2 = 0

(c) Covtype: λ1 = 0 and λ2 = 10−5

(d) Adult: λ1 = 10−6 and λ2 = 10−5

Fig. 7. Comparison of SAGA [31], SVRG [1], Prox-SVRG [34], Catalyst [40], Katyusha [48], and VR-SGD for solving (cid:96)2-norm (the ﬁrst row), (cid:96)1-norm
(c), and elastic net (d) regularized logistic regression problems. In each plot, the vertical axis shows the objective value minus the minimum, and
the horizontal axis is the number of effective passes (left) or running time (right).

(a) Adult

(b) MNIST

(c) Covtype

(d) RCV1

Fig. 8. Comparison of SAGA [58], SVRG [4], SVRG++ [44], and VR-SGD for solving non-convex ERM problems with sigmoid loss: λ = 10−5 (top)
and λ = 10−6 (bottom). Note that x∗ denotes the best solution obtained by running all those methods for a large number of iterations and multiple
random initializations.

more obvious advantage over Katyusha in terms
of running time, especially in the case of sparse
data (e.g., RCV1), as shown in Fig. 7(b). From the
algorithms of Katyusha proposed in [48], we can see
that the learning rate of Katyusha is at least set to
1/(3L). Similarly, the learning rate used in VR-SGD
is comparable to that of Katyusha, which may be
the main reason why the performance of VR-SGD is
much better than that of Katyusha. This also implies
that the algorithms (including VR-SGD) that enjoy
larger learning rates can converge faster in general.

5.6.2 ERM with Non-Convex Loss

In this part, we apply VR-SGD to solve the following regu-
larized ERM problem with non-convex sigmoid loss:

min
x∈Rd

1
n

n
(cid:88)

i=1

fi(x) +

λ
2

(cid:107)x(cid:107)2

(18)

where fi(x) = 1/[1 + exp(biaT
i x)]. Some work [4], [79] has
shown that the sigmoid function usually generalizes better
than some other loss functions (such as squared loss, logistic
loss and hinge loss) in terms of test accuracy especially when
there are outliers. Here, we consider binary classiﬁcation on
the four data sets: Adult, MNIST, Covtype and RCV1. Note
that we only consider classifying the ﬁrst class in MNIST.

We compare the performance (including training objec-
tive value and function suboptimality, i.e., F (xs) − F (x∗))

02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025030010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0246810−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01020304010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD010203040500.1550.160.1650.17Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.0120.0130.014Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.4250.4260.4270.4280.429Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0501001502000.030.040.050.06Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

12

Fig. 9. Testing accuracy comparison on Adult (left) and MNIST (right).

Fig. 10. Relative error comparison of leading eigenvalue computation on
RCV1 (left) and Epsilon (right).

of VR-SGD with that of SAGA [58], SVRG [4], and
SVRG++ [44], as shown in Fig. 8 (more results are provided
in the Supplementary Material), where x∗ denotes the best
solution obtained by running all those methods for a large
number of iterations and multiple random initializations.
Note that both SAGA and SVRG are two variants of the
original SAGA [31] and SVRG [1]. The results show that
our VR-SGD method has faster convergence than the other
methods, and its objective value is much lower. This implies
that VR-SGD can yield much better solutions than the
other methods including SVRG++. Furthermore, we can see
that VR-SGD has much greater advantage over the other
methods in the cases when the smaller λ is, which means
that the objective function becomes more “non-convex”.

Moreover, we report the classiﬁcation testing accuracies
of all those methods on the test sets of Adult and MNIST
in Fig. 9, as the number of effective passes over datasets
increases. Note that the regularization parameter is set to
λ = 10−4. It can be seen that our VR-SGD method obtains
higher test accuracies than the other methods with much
shorter running time, suggesting faster convergence.

5.6.3 Eigenvalue Computation

Finally, we apply VR-SGD to solve the following non-
convex leading eigenvalue computation problem:

min
x∈Rd:xTx=1

−xT

(cid:32)

1
n

n
(cid:88)

i=1

(cid:33)

aiaT
i

x.

(19)

We plot the performance of the classical Power iteration
method, VR-PCA [17], and VR-SGD on Epsilon and RCV1
in Fig. 10, where the relative error is deﬁned as in [17], i.e.,
log10(1 − (cid:107)AT x(cid:107)2/(maxu:uT u=1 (cid:107)AT u(cid:107)2)), and A ∈ Rd×n
is the data matrix. Note that the epoch length is set to
m = n for VR-PCA and VR-SGD, as suggested in [17], and
both of them use a constant learning rate. The results show
that the stochastic variance reduced methods, VR-PCA and
VR-SGD, signiﬁcantly outperform the traditional method,
Power. Moreover, our VR-SGD method often converges
much faster than VR-PCA.

for VR-SGD vs. 1/(10L) for SVRG, and also makes VR-
SGD much more robust to learning rate selection. Different
from existing proximal stochastic methods such as Prox-
SVRG and Katyusha [48], we designed two different update
rules for smooth and non-smooth problems, respectively,
which makes VR-SGD suitable for non-smooth and/or non-
strongly convex problems without using any reduction tech-
niques as in [68]. Our empirical results also showed that for
smooth problems stochastic gradient update rules as in (12)
are better choices than proximal update formulas as in (10).
On the practical side, the choices of the snapshot and
starting points make VR-SGD signiﬁcantly faster than its
counterparts, SVRG and Prox-SVRG. On the theoretical
side, the setting also makes our convergence analysis more
challenging. We analyzed the convergence properties of VR-
SGD for strongly convex objective functions, which show
that VR-SGD attains a linear convergence rate. Moreover,
we provided the convergence guarantees of VR-SGD for
non-strongly convex functions, and our experimental results
showed that VR-SGD achieves similar performance to its
momentum accelerated variant that has the optimal con-
vergence rate O(1/T 2). In contrast, SVRG and Prox-SVRG
cannot directly solve non-strongly convex functions [44].
Various experimental results show that VR-SGD signiﬁ-
cantly outperforms state-of-the-art variance reduction meth-
ods such as SAGA [58], SVRG [1] and Prox-SVRG [34],
and also achieves better or at least comparable perfor-
mance with recently-proposed acceleration methods, e.g.,
Catalyst [40] and Katyusha [48]. Since VR-SGD has a much
lower per-iteration complexity than accelerated methods
(e.g., Katyusha), it has more obvious advantage over them
in terms of running time, especially for high-dimensional
sparse data. This further veriﬁes that VR-SGD is suitable
for various large-scale machine learning. Furthermore, as
the update rules of VR-SGD are much simpler than existing
accelerated stochastic variance reduction methods such as
Katyusha, it is more friendly to asynchronous parallel and
distributed implementation similar to [59], [60], [62].

6 CONCLUSIONS

We proposed a simple variant of the original SVRG [1],
called variance reduced stochastic gradient descent (VR-
SGD). Unlike the choices of snapshot and starting points
in SVRG and Prox-SVRG [34], the two points of each
epoch in VR-SGD are set to the average and last iterate
of the previous epoch, respectively. This setting allows us
to use much larger learning rates than SVRG, e.g., 1/L

ACKNOWLEDGMENTS

Fanhua Shang, Hongying Liu and Licheng Jiao were sup-
ported in part by Project supported the Foundation for
Innovative Research Groups of the National Natural Science
Foundation of China (No. 61621005), the Major Research
Plan of the National Natural Science Foundation of China
(Nos. 91438201 and 91438103), the National Natural Science
Foundation of China (Nos. 61876220, 61836009, U1701267,
61871310, 61573267, 61502369, 61876221 and 61473215), the

51015200.8380.840.842Gradient evaluations / nTest accuracy (%)SAGASVRGSVRG++VR−SGD51015200.990.9910.9920.9930.994Gradient evaluations / nTest accuracy (%)SAGASVRGSVRG++VR−SGD510152010−1210−810−4Gradient evaluations / nErrorPowerVR−PCAVR−SGD510152010−1210−810−4Gradient evaluations / nErrorPowerVR−PCAVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

13

IRT 15R53),

Program for Cheung Kong Scholars and Innovative Re-
search Team in University (No.
the Fund
for Foreign Scholars in University Research and Teach-
ing Programs (the 111 Project) (No. B07048), and the Sci-
ence Foundation of Xidian University (No. 10251180018).
James Cheng was supported in part by Grants (CUHK
14206715 & 14222816) from the Hong Kong RGC. Ivor
Tsang was supported by ARC FT130100746, DP180100106
and LP150100671. Lijun Zhang was supported by JiangsuSF
(BK20160658). Dacheng Tao was supported by Australian
Research Council Projects FL-170100117, DP-180103424, and
IH180100002.

REFERENCES

[1] R. Johnson and T. Zhang, “Accelerating stochastic gradient de-
scent using predictive variance reduction,” in Proc. Adv. Neural
Inf. Process. Syst., 2013, pp. 315–323.

[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classi-
ﬁcation with deep convolutional neural networks,” in Proc. Adv.
Neural Inf. Process. Syst., 2012, pp. 1097–1105.
S. Zhang, A. Choromanska, and Y. LeCun, “Deep learning with
elastic averaging SGD,” in Proc. Adv. Neural Inf. Process. Syst., 2015,
pp. 685–693.

[3]

[4] Z. Allen-Zhu and E. Hazan, “Variance reduction for faster non-
convex optimization,” in Proc. Int. Conf. Mach. Learn., 2016, pp.
699–707.
S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, “Stochastic
variance reduction for nonconvex optimization,” in Proc. Int. Conf.
Mach. Learn., 2016, pp. 314–323.

[5]

[6] Y. Liu, F. Shang, and J. Cheng, “Accelerated variance reduced
stochastic ADMM,” in Proc. AAAI Conf. Artif. Intell., 2017, pp.
2287–2293.

[7] X. Li, T. Zhao, R. Arora, H. Liu, and J. Haupt, “Nonconvex sparse
learning via stochastic optimization with progressive variance
reduction,” in Proc. Int. Conf. Mach. Learn., 2016, pp. 917–925.
[8] W. Zhang, L. Zhang, Z. Jin, R. Jin, D. Cai, X. Li, R. Liang, and
X. He, “Sparse learning with stochastic composite optimization,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1223–1236,
Jun. 2017.

[9] C. Qu, Y. Li, and H. Xu, “Linear convergence of SVRG in statistical

estimation,” arXiv:1611.01957v3, 2017.

[10] C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui,
“Catalyst for gradient-based nonconvex optimization,” in Proc. Int.
Conf. Artif. Intell. Statist., 2018, pp. 613–622.

[11] H. Kasai, “Stochastic variance reduced multiplicative update for
nonnegative matrix factorization,” in Proc. IEEE Int. Conf. Acoust.
Speech Signal Process., 2018, pp. 6338–6342.

[12] J. Duchi and F. Ruan, “Stochastic methods for composite optimiza-

tion problems,” arXiv:1703.08570, 2017.

[13] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for
large-scale matrix completion,” Math. Prog. Comp., vol. 5, pp. 201–
226, 2013.

[14] L. Wang, X. Zhang, and Q. Gu, “A uniﬁed variance reduction-
based framework for nonconvex low-rank matrix recovery,” in
Proc. Int. Conf. Mach. Learn., 2017, pp. 3712–3721.

[15] M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton,
and A. Sarkar, “Non-uniform stochastic average gradient method
for training conditional random ﬁelds,” in Proc. Int. Conf. Artif.
Intell. Statist., 2015, pp. 819–828.

[16] Z. Allen-Zhu and Y. Li, “Doubly accelerated methods for faster
CCA and generalized eigendecomposition,” in Proc. Int. Conf.
Mach. Learn., 2017, pp. 98–106.

[17] O. Shamir, “A stochastic PCA and SVD algorithm with an expo-
nential convergence rate,” in Proc. Int. Conf. Mach. Learn., 2015, pp.
144–152.

[18] D. Garber, E. Hazan, C. Jin, S. M. Kakade, C. Musco, P. Netrapalli,
and A. Sidford, “Faster eigenvector computation via shift-and-
invert preconditioning,” in Proc. Int. Conf. Mach. Learn., 2016, pp.
2626–2634.

[19] Y. Nesterov, “A method of solving a convex programming problem
with convergence rate O(1/k2),” Soviet Math. Doklady, vol. 27, pp.
372–376, 1983.

[20] ——, Introductory Lectures on Convex Optimization: A Basic Course.

Boston: Kluwer Academic Publ., 2004.

[21] P. Tseng, “On accelerated proximal gradient methods for convex-
concave optimization,” Technical report, University of Washington,
2008.

[22] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding
algorithm for linear inverse problems,” SIAM J. Imaging Sci., vol. 2,
no. 1, pp. 183–202, 2009.

[23] H. Robbins and S. Monro, “A stochastic approximation method,”

Ann. Math. Statist., vol. 22, no. 3, pp. 400–407, 1951.

[24] T. Zhang, “Solving large scale linear prediction problems using
stochastic gradient descent algorithms,” in Proc. Int. Conf. Mach.
Learn., 2004, pp. 919–926.

[25] C. Hu, J. T. Kwok, and W. Pan, “Accelerated gradient methods for
stochastic optimization and online learning,” in Proc. Adv. Neural
Inf. Process. Syst., 2009, pp. 781–789.

[26] S. Bubeck, “Convex optimization: Algorithms and complexity,”

Found. Trends Mach. Learn., vol. 8, pp. 231–358, 2015.

[27] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lilli-
crap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Has-
sabis, “Mastering the game of go without human knowledge,”
Nature, vol. 550, pp. 354–359, 2017.

[28] A. Rakhlin, O. Shamir, and K. Sridharan, “Making gradient de-
scent optimal for strongly convex stochastic optimization,” in Proc.
Int. Conf. Mach. Learn., 2012, pp. 449–456.

[29] N. L. Roux, M. Schmidt, and F. Bach, “A stochastic gradient
method with an exponential convergence rate for ﬁnite training
sets,” in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 2672–2680.

[30] S. Shalev-Shwartz and T. Zhang, “Stochastic dual coordinate as-
cent methods for regularized loss minimization,” J. Mach. Learn.
Res., vol. 14, pp. 567–599, 2013.

[31] A. Defazio, F. Bach, and S. Lacoste-Julien, “SAGA: A fast incre-
mental gradient method with support for non-strongly convex
composite objectives,” in Proc. Adv. Neural Inf. Process. Syst., 2014,
pp. 1646–1654.

[32] Y. Zhang and L. Xiao, “Stochastic primal-dual coordinate method
for regularized empirical risk minimization,” in Proc. Int. Conf.
Mach. Learn., 2015, pp. 353–361.

[33] M. Schmidt, N. L. Roux, and F. Bach, “Minimizing ﬁnite sums with
the stochastic average gradient,” Math. Program., vol. 162, pp. 83–
112, 2017.

[34] L. Xiao and T. Zhang, “A proximal stochastic gradient method
with progressive variance reduction,” SIAM J. Optim., vol. 24,
no. 4, pp. 2057–2075, 2014.

[35] S. Shalev-Shwartz and T. Zhang, “Accelerated proximal stochastic
dual coordinate ascent for regularized loss minimization,” Math.
Program., vol. 155, pp. 105–145, 2016.

[36] F. Shang, Y. Liu, J. Cheng, and J. Zhuo, “Fast stochastic variance re-
duced gradient method with momentum acceleration for machine
learning,” arXiv:1703.07948, 2017.

[37] L. Zhang, M. Mahdavi, and R. Jin, “Linear convergence with
condition number independent access of full gradients,” in Proc.
Adv. Neural Inf. Process. Syst., 2013, pp. 980–988.

[38] J. Koneˇcn ´y, J. Liu, P. Richt´arik, , and M. Tak´aˇc, “Mini-batch semi-
stochastic gradient descent in the proximal setting,” IEEE J. Sel.
Top. Sign. Proces., vol. 10, no. 2, pp. 242–255, 2016.

[39] A. Nitanda, “Stochastic proximal gradient descent with accelera-
tion techniques,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp.
1574–1582.

[40] H. Lin, J. Mairal, and Z. Harchaoui, “A universal catalyst for ﬁrst-
order optimization,” in Proc. Adv. Neural Inf. Process. Syst., 2015,
pp. 3366–3374.

[41] R. Frostig, R. Ge, S. M. Kakade, and A. Sidford, “Un-regularizing:
approximate proximal point and faster stochastic algorithms for
empirical risk minimization,” in Proc. Int. Conf. Mach. Learn., 2015,
pp. 2540–2548.

[42] G. Lan and Y. Zhou, “An optimal randomized incremental gradi-

ent method,” Math. Program., 2017.

[43] R. Babanezhad, M. O. Ahmed, A. Virani, M. Schmidt, J. Konecny,
and S. Sallinen, “Stop wasting my gradients: Practical SVRG,” in
Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 2242–2250.

[44] Z. Allen-Zhu and Y. Yuan, “Improved SVRG for non-strongly-
convex or sum-of-non-convex objectives,” in Proc. Int. Conf. Mach.
Learn., 2016, pp. 1080–1089.

[45] M. Frank and P. Wolfe, “An algorithm for quadratic program-

ming,” Naval Res. Logist. Quart., vol. 3, pp. 95–110, 1956.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

14

[46] E. Hazan and H. Luo, “Variance-reduced and projection-free
stochastic optimization,” in Proc. Int. Conf. Mach. Learn., 2016, pp.
1263–1271.

[47] F. Shang, Y. Liu, J. Cheng, K. W. Ng, and Y. Yoshida, “Guaran-
teed sufﬁcient decrease for stochastic variance reduced gradient
optimization,” in Proc. 21st Int. Conf. Artif. Intell. Statist., 2018, pp.
1027–1036.

[48] Z. Allen-Zhu, “Katyusha: The ﬁrst direct acceleration of stochastic
gradient methods,” J. Mach. Learn. Res., vol. 18, no. 221, pp. 1–51,
2018.

[49] L. Hien, C. Lu, H. Xu, and J. Feng, “Accelerated stochastic mirror
descent algorithms for composite non-strongly convex optimiza-
tion,” arXiv:1605.06892v4, 2017.

[50] K. Zhou, F. Shang, and J. Cheng, “A simple stochastic variance
reduced algorithm with fast convergence rates,” in Proc. Int. Conf.
Mach. Learn., 2018, pp. 5975–5984.

[51] B. Woodworth and N. Srebro, “Tight complexity bounds for op-
timizing composite objectives,” in Proc. Adv. Neural Inf. Process.
Syst., 2016, pp. 3639–3647.

[52] A. Defazio, “A simple practical accelerated method for ﬁnite
sums,” in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 676–684.
[53] L. Liu, J. Liu, and D. Tao, “Variance reduced methods for non-
convex composition optimization,” arXiv:1711.04416v1, 2017.
[54] L. Lei, C. Ju, J. Chen, and M. I. Jordan, “Nonconvex ﬁnite-sum
optimization via SCSG methods,” in Proc. Adv. Neural Inf. Process.
Syst., 2017, pp. 2345–2355.

[55] L. Liu, M. Cheng, C.-J. Hsieh, and D. Tao, “Stochastic zeroth-order
optimization via variance reduction method,” arXiv:1805.11811,
2018.

[56] L. Liu, J. Liu, C.-J. Hsieh, and D. Tao, “Stochastically controlled
stochastic gradient for the convex and non-convex composition
problem,” arXiv:1809.02505, 2018.

[57] L. Liu, J. Liu, and D. Tao, “Duality-free methods for stochastic
composition optimization,” IEEE Trans. Neural Netw. Learn. Syst.,
2018.

[58] S. J. Reddi, S. Sra, B. P ´oczos, and A. Smola, “Fast incremental
method for smooth nonconvex optimization,” in Proc. Conf. Deci-
sion and Contorl, 2016, pp. 1971–1977.

[59] H. Mania, X. Pan, D. Papailiopoulos, B. Recht, K. Ramchandran,
and M. I. Jordan, “Perturbed iterate analysis for asynchronous
stochastic optimization,” SIAM J. Optim., vol. 27, no. 4, pp. 2202–
2229, 2017.

[60] S. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, “On variance
reduction in stochastic gradient descent and its asynchronous
variants,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 2629–
2637.

[61] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and
J. Liu, “Can decentralized algorithms outperform centralized algo-
rithms? A case study for decentralized parallel stochastic gradient
descent,” in Proc. Adv. Neural Inf. Process. Syst., 2017.

[62] J. D. Lee, Q. Lin, T. Ma, and T. Yang, “Distributed stochastic
variance reduced gradient methods by sampling extra data with
replacement,” J. Mach. Learn. Res., vol. 18, pp. 1–43, 2017.

[63] Y. Bengio, “Learning deep architectures for AI,” Found. Trends

Mach. Learn., vol. 2, no. 1, pp. 1–127, 2009.

[64] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points
– online stochastic gradient for tensor decomposition,” in Proc.
Conf. Learn. Theory, 2015, pp. 797–842.

[65] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Ku-
rach, and J. Martens, “Adding gradient noise improves learning
for very deep networks,” arXiv:1511.06807, 2015.

[66] N. Flammarion and F. Bach, “From averaging to acceleration, there
is only a step-size,” in Proc. Conf. Learn. Theory, 2015, pp. 658–695.
[67] C. Tan, S. Ma, Y. Dai, and Y. Qian, “Barzilai-Borwein step size for
stochastic gradient descent,” in Proc. Adv. Neural Inf. Process. Syst.,
2016, pp. 685–693.

[68] Z. Allen-Zhu and E. Hazan, “Optimal black-box reductions be-
tween optimization objectives,” in Proc. Adv. Neural Inf. Process.
Syst., 2016, pp. 1606–1614.

[69] Y. Nesterov, “Smooth minimization of non-smooth functions,”

Math. Program., vol. 103, pp. 127–152, 2005.

[70] Y. Xu, Y. Yan, Q. Lin, and T. Yang, “Homotopy smoothing for non-
smooth problems with lower complexity than O(1/(cid:15)),” in Proc.
Adv. Neural Inf. Process. Syst., 2016, pp. 1208–1216.

[71] P. Zhao and T. Zhang, “Stochastic optimization with importance
sampling for regularized loss minimization,” in Proc. Int. Conf.
Mach. Learn., 2015, pp. 1–9.

[72] D. Needell, N. Srebro, and R. Ward, “Stochastic gradient descent,
weighted sampling, and the randomized Kaczmarz algorithm,”
Math. Program., vol. 155, pp. 549–573, 2016.

[73] O. Shamir, “Without-replacement sampling for stochastic gradient

methods,” in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 46–54.

[74] J. Koneˇcn ´y and P. Richt´arik, “Semi-stochastic gradient descent
methods,” Optim. Method Softw., vol. 32, no. 5, pp. 993–1005, 2017.
[75] F. Shang, Y. Liu, L. Jiao, K. Zhou, J. Cheng, Y. Ren, and Y. Jin,
“ASVRG: Accelerated proximal SVRG,” in Proc. Mach. Learn. Res.,
2018, pp. 1–16.

[76] B. Carpenter, “Lazy sparse stochastic gradient descent for regular-

ized multinomial logistic regression,” Tech. Rep., 2008.

[77] J. Langford, L. Li, and T. Zhang, “Sparse online learning via
truncated gradient,” J. Mach. Learn. Res., vol. 10, pp. 777–801, 2009.
[78] W. Su, S. Boyd, and E. J. Candes, “A differential equation for
modeling Nesterov’s accelerated gradient method: Theory and
insights,” J. Mach. Learn. Res., vol. 17, pp. 1–43, 2016.

[79] S. Shalev-Shwartz, O. Shamir, and K. Sridharan, “Learning kernel-
based halfspaces with the 0-1 loss,” SIAM J. Comput., vol. 40, no. 6,
pp. 1623–1646, 2011.

[80] G. Lan, “An optimal method for stochastic composite optimiza-

tion,” Math. Program., vol. 133, pp. 365–397, 2012.

[81] Z. Allen-Zhu and Y. Yuan, “Improved SVRG for non-strongly-
convex or sum-of-non-convex objectives,” arXiv:1506.01972v3,
2016.

Fanhua Shang (M’14) received the Ph.D. de-
gree in Circuits and Systems from Xidian Uni-
versity, Xi’an, China, in 2012.

He is currently a professor with the School
of Artiﬁcial Intelligence, Xidian University, China.
Prior to joining Xidian University, he was a Re-
search Associate with the Department of Com-
puter Science and Engineering, The Chinese
University of Hong Kong. From 2013 to 2015,
he was a Post-Doctoral Research Fellow with
the Department of Computer Science and En-
gineering, The Chinese University of Hong Kong. From 2012 to 2013,
he was a Post-Doctoral Research Associate with the Department of
Electrical and Computer Engineering, Duke University, Durham, NC,
USA. His current research interests include machine learning, data
mining, pattern recognition, and computer vision.

Kaiwen Zhou received the BS degree in Com-
puter Science and Technology from Fudan Uni-
versity in 2017. He is currently working toward
his Master degree in the Department of Com-
puter Science and Engineering, The Chinese
University of Hong Kong, Hong Kong. His current
research interests include data mining, stochas-
tic optimization for machine learning, large scale
machine learning, etc.

Hongying Liu (M’10) received her B.E. and M.S.
degrees in Computer Science and Technology
from XiAn University of Technology, China, in
2006 and 2009, respectively, and Ph.D. in Engi-
neering from Waseda University, Japan in 2012.
Currently, she is a faculty member at the School
of Artiﬁcial Intelligence, and also with the Key
Laboratory of Intelligent Perception and Image
Understanding of Ministry of Education, Xidian
University, China. In addition, she is a member
of IEEE. Her major research interests include

image processing, intelligent signal processing, machine learning, etc.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

15

James Cheng is an Assistant Professor with the
Department of Computer Science and Engineer-
ing, The Chinese University of Hong Kong, Hong
Kong. His current research interests include dis-
tributed computing systems, large-scale network
analysis, temporal networks, and big data.

Dacheng Tao (F’15) is Professor of Computer
Science and ARC Laureate Fellow in the School
of Information Technologies and the Faculty of
Engineering and Information Technologies, and
the Inaugural Director of the UBTECH Sydney
Artiﬁcial Intelligence Centre, at the University of
Sydney. He mainly applies statistics and math-
ematics to Artiﬁcial Intelligence and Data Sci-
ence. His research results have expounded in
one monograph and 200+ publications at presti-
gious journals and prominent conferences, such
as IEEE T-PAMI, T-IP, T-NNLS, IJCV, JMLR, NIPS, ICML, CVPR, ICCV,
ECCV, ICDM, and ACM SIGKDD, with several best paper awards, such
as the best theory/algorithm paper runner up award in IEEE ICDM’07,
the best student paper award in IEEE ICDM’13, the distinguished paper
award in the 2018 IJCAI, the 2014 ICDM 10-year highest-impact paper
award, and the 2017 IEEE Signal Processing Society Best Paper Award.
He is a Fellow of the Australian Academy of Science, AAAS, IEEE, IAPR,
OSA and SPIE.

Ivor W. Tsang is an ARC future fellow and a pro-
fessor of artiﬁcial intelligence with the University
of Technology Sydney. He is also the research
director of the UTS Priority Research Centre
for Artiﬁcial Intelligence. His research focuses
on transfer learning, feature selection, big data
analytics for data with trillions of dimensions, and
their applications to computer vision and pattern
recognition. He has more than 140 research
papers published in top-tier journal and confer-
ence papers, including the Journal of Machine
Learning Research, the Madras Law Journal, the IEEE Transactions on
Pattern Analysis and Machine Intelligence, the IEEE Transactions on
Neural Networks and Learning Systems, NIPS, ICML, etc. In 2009, he
was conferred the 2008 Natural Science Award (Class II) by Ministry
of Education, China, which recognized his contributions to kernel meth-
ods. In 2013, he received his prestigious Australian Research Council
Future Fellowship for his research regarding Machine Learning on Big
Data. In addition, he had received the prestigious IEEE Transactions
on Neural Networks Outstanding 2004 Paper Award in 2007, the 2014
IEEE Transactions on Multimedia Prize Paper Award, and a number of
best paper awards and honors from reputable international conferences,
including the Best Student Paper Award at CVPR 2010, and the Best
Paper Award at ICTAI 2011. He was also awarded the ECCV 2012
Outstanding Reviewer Award.

Lijun Zhang received the BS and PhD degrees
in software engineering and computer science
from Zhejiang University, China, in 2007 and
2012, respectively. He is currently an associate
professor of the Department of Computer Sci-
ence and Technology, Nanjing University, China.
Prior to joining Nanjing University, he was a
postdoctoral researcher at the Department of
Computer Science and Engineering, Michigan
State University. His research interests include
machine learning, optimization, information re-

trieval, and data mining. He is a member of the IEEE.

Licheng Jiao (F’18) received the B.S. degree
from Shanghai Jiaotong University, Shanghai,
China, in 1982, and the M.S. and Ph.D. degrees
from Xian Jiaotong University, Xian, China, in
1984 and 1990, respectively.

He was a Post-Doctoral Fellow with the Na-
tional Key Laboratory for Radar Signal Process-
ing, Xidian University, Xian, from 1990 to 1991,
where he has been a Professor with the School
of Electronic Engineering, since 1992, and cur-
rently the Director of the Key Laboratory of In-
telligent Perception and Image Understanding, Ministry of Education
of China. He has charged of about 40 important scientiﬁc research
projects, and published over 20 monographs and a hundred papers in
international journals and conferences. His current research interests
include image processing, natural computation, machine learning, and
intelligent information processing.

Dr. Jiao is the Chairman of Awards and Recognition Committee, the
Vice Board Chairperson of the Chinese Association of Artiﬁcial Intelli-
gence, a Councilor of the Chinese Institute of Electronics, a Committee
Member of the Chinese Committee of Neural Networks, and an expert
of the Academic Degrees Committee of the State Council. He is a fellow
of the IEEE.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

16

Supplementary Materials for
“VR-SGD: A Simple Stochastic Variance Reduction Method for Machine Learning”

In this supplementary material, we give the detailed proofs of some lemmas and theorems, and provide convergence
analysis for Algorithm 2 with Option I or Option II, Algorithm 3, and Algorithm 4 (i.e., VR-SGD++). In addition, we also
report more experimental results for solving various machine learning problems on real-world data sets.

APPENDIX A: PROOFS OF LEMMA 1 AND THEOREM 1
We ﬁrst introduce the following lemma, which is useful in our analysis.

Lemma 2 (3-point property, [80]). Let ˆz be the optimal solution of the following minimization problem,

min
z∈Rd

τ
2

(cid:107)z − z0(cid:107)2 + r(z)

where τ ≥ 0, and r(z) is a convex function (but possibly non-differentiable). Then for any z ∈ Rd, the following inequality holds

r(ˆz) +

τ
2

(cid:107)ˆz − z0(cid:107)2 ≤ r(z) +

τ
2

(cid:107)z − z0(cid:107)2 −

τ
2

(cid:107)z − ˆz(cid:107)2.

The proof of Lemma 1 is similar to that of Theorem 1 in [1]. For completeness, we give the detailed proof of Lemma 1

below. Before proving Lemma 1, we ﬁrst give and prove the following lemma.

Lemma 3. Suppose each convex component function fi(·) is L-smooth, and let x∗ be the optimal solution of Problem (1) when
g(x) ≡ 07, then we have

E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis

k

(x) − ∇fis

k

(x∗)

≤ 2L [f (x) − f (x∗)] .

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Proof. Following Theorem 2.1.5 in [20] and Lemma 3.4 [34], we have

(cid:13)
(cid:13)
(cid:13)∇fis

k

(x) − ∇fis

k

(cid:13)
2
(x∗)
(cid:13)
(cid:13)

(cid:104)

≤ 2L

fis

k

(x) − fis

k

(x∗) −

(cid:68)

∇fis

k

(x∗), x − x∗(cid:69)(cid:105)

.

Summing the above inequality over i = 1, . . . , n, we obtain
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis

(cid:13)
(x∗)
(cid:13)
(cid:13)

(x) − ∇fis

2(cid:21)

=

E

k

k

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

By the optimality of x∗, i.e., x∗ = arg minx f (x), we have ∇f (x∗) = 0. Then

≤ 2L [f (x) − f (x∗) − (cid:104)∇f (x∗), x − x∗(cid:105)] .

E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis

k

(x) − ∇fis

k

2(cid:21)

(cid:13)
(x∗)
(cid:13)
(cid:13)

≤ 2L [f (x) − f (x∗) − (cid:104)∇f (x∗), x − x∗(cid:105)]

= 2L [f (x) − f (x∗)] .

This completes the proof.

Proof of Lemma 1:

Proof.

(cid:104)
E

(cid:107) (cid:101)∇fis

k

(xs

k) − ∇f (xs

k)(cid:107)2(cid:105)

(cid:13)
((cid:101)xs−1) + ∇f ((cid:101)xs−1) − ∇f (xs
(cid:13)
k)
(cid:13)

2(cid:21)

((cid:101)xs−1)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

− (cid:13)

(cid:13)∇f (xs

k) − ∇f ((cid:101)xs−1)(cid:13)
2
(cid:13)

k

k

= E

= E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
≤ 4L(cid:2)f (xs

≤ 2E

≤ E

k

k

(xs

k) − ∇fis

k

(xs

k) − ∇fis

k

(xs

k) − ∇fis

k

(xs

k) − ∇fis

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
2(cid:21)

((cid:101)xs−1)
(cid:13)
(x∗)
(cid:13)
(cid:13)

k

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis

k

((cid:101)xs−1) − ∇fis

k

2(cid:21)

(cid:13)
(x∗)
(cid:13)
(cid:13)

7. In order to simplify our analysis, we denote F (x) by f (x), that is, fi(x) := fi(x) + g(x) for all i = 1, 2, . . . , n, and then g(x) ≡ 0.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

17

where the second equality holds due to the fact that E[(cid:107)x−E[x](cid:107)2] = E[(cid:107)x(cid:107)2]−(cid:107)E[x](cid:107)2; the second inequality holds due to
the fact that (cid:107)a − b(cid:107)2 ≤ 2((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2); and the last inequality follows from Lemma 3.

Before proving Theorem 1, we ﬁrst give and prove the following lemma.

Lemma 4 (Smooth objectives and Option II). Let β = 1/(Lη). If each fi(·) is convex and L-smooth, then the following inequality

holds for all s = 1, 2, . . . , S,

β −3
β −1
2m
γ

≤

E[f ((cid:101)xs) − f (x∗)] +
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

1
m−1
2
γ

E[f (xs

m) − f (x∗)]

E[f (xs

0) − f (x∗)] +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3)

where γ = (β −1)(m−1).

Proof of Lemma 4:

((cid:101)xs−1) +
Proof. In order to simplify the notations, the stochastic gradient estimator is deﬁned as: (cid:101)∇is
∇f ((cid:101)xs−1). Since each function fi(x) is L-smooth, which implies that the gradient of the average function f (x) is Lipschitz
continuous with parameter L, i.e., for all x, y ∈ Rd, (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107), whose equivalent form is

k) − ∇fis

:= ∇fis

(xs

k

k

k

Using the above smoothness inequality, we have

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2.

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

(cid:11) +

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

= f (xs

k) + (cid:10)∇f (xs
(cid:68)

k), xs

k+1 − xs
k
(cid:69)

(cid:101)∇is

k

, xs

k+1 − xs
k

+

= f (xs

k) +
(cid:68)
∇f (xs

+

L
2
Lβ
2
(cid:107)xs

(cid:11) +
Lβ
2
(cid:69)

k(cid:107)2

k+1 − xs
L(β −1)
2

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

−

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

(20)

where β = 1/(Lη) > 3 is a constant. By Lemma 1, then we get

(cid:20)(cid:68)

E

∇f (xs

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

(cid:69)

−

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)f (xs
β −1

≤

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

L(β −1)
2
L(β −1)
2

(cid:107)∇f (xs

k) − (cid:101)∇is

k

(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

(21)

where the ﬁrst inequality holds due to the Young’s inequality (i.e., aT b ≤ (cid:107)a(cid:107)2/(2γ)+γ(cid:107)b(cid:107)2/2 for all γ > 0 and a, b ∈ Rd),

and the second inequality follows from Lemma 1.

Substituting the inequality in (21) into the inequality in (20), and taking the expectation with respect to the random

choice is

k, we have

E[f (xs

k+1)]

≤ f (xs

k) + E

(cid:20)(cid:68)

≤ f (xs

k) + E

(cid:20)(cid:68)

(cid:101)∇is

k

, xs

k+1 −xs
k

(cid:69)

+

Lβ
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

+

2
β −1

(cid:101)∇is

k

, x∗ −xs
k

(cid:69)

+

((cid:107)x∗ −xs

k(cid:107)2 − (cid:107)x∗ −xs

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)
(cid:2)f (xs
(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

+

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)

≤ f (xs

k) + (cid:104)∇f (xs

k), x∗ −xs

k(cid:105) + E

((cid:107)x∗ −xs

k(cid:107)2 − (cid:107)x∗ −xs

(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)

≤ f (x∗) +

Lβ
2

E(cid:2)(cid:0)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) .

2
β −1
2
β −1

+

Lβ
2
(cid:20) Lβ
2

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

18

Here, the ﬁrst inequality holds due to the inequality in (20) and the inequality in (21); the second inequality follows from
Lemma 2 with ˆz = xs
to the fact that E[ (cid:101)∇is
f (xs
k) + (cid:104)∇f (xs

k(cid:105); the third inequality holds due
k); and the last inequality follows from the convexity of the smooth function f (·), i.e.,

k(cid:105) ≤ f (x∗). The above inequality can be rewritten as follows:

k+1, z = x∗, z0 = xs
] = ∇f (xs

k, τ = Lβ = 1/η, and r(z) := (cid:104) (cid:101)∇is

k), x∗ − xs

, z − xs

k

k

E[f (xs

k+1)] − f (x∗) ≤

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

Summing the above inequality over k = 0, 1, . . . , m−1, then

m
(cid:88)

k=1

E[f (xs

k) − f (x∗)] ≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k−1(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k(cid:107)2(cid:3)

Due to the setting of (cid:101)xs = 1

m−1

(cid:80)m−1

k=1 xs

k in Option II, and the convexity of f (·), then we have

The left and right hand sides of the inequality in (22) can be rewritten as follows:

f ((cid:101)xs) ≤

1
m − 1

m−1
(cid:88)

k=1

f (xs

k).

m
(cid:88)

k=1

E[f (xs

k) − f (x∗)] =

m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)] ,

.

(22)

(23)

m
(cid:88)

(cid:26) 2

(cid:2)f (xs

k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k−1(cid:107)2 − (cid:107)x∗ − xs

k(cid:107)2(cid:3)

(cid:27)

β −1

k=1

=

2
β −1

m−1
(cid:88)

[f (xs

k)−f (x∗)] +

k=1

2
β −1

(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3).

Subtracting 2
β−1

(cid:80)m−1

k=1 [f (xs

k)−f (x∗)] from both sides of the inequality in (22), then we obtain

(cid:18)

1 −

2
β −1

(cid:19) m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)]

≤

2
β −1

E(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

Using the inequality in (23), we have

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

(cid:18)

(cid:18)

1 −

1 −

≤

2
β −1

2
β −1

(cid:19)

(m−1)E[f ((cid:101)xs) − f (x∗)] + E[f (xs

m) − f (x∗)]

(cid:19) m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)]

≤

2
β −1

E(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

Dividing both sides of the above inequality by (m−1), we arrive at

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

E[f ((cid:101)xs) − f (x∗)] +

1
m−1

E[f (xs

m) − f (x∗)]

(cid:18)

(cid:19)

1 −

2
β −1
2
(β −1)(m−1)

≤

E[f (xs

0)−f (x∗)]+

2m
(β −1)(m−1)

E(cid:2)f ((cid:101)xs−1)−f (x∗)(cid:3)+

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

This completes the proof.

Proof of Theorem 1:

Proof. Since 2/(β −1) < 1, it is easy to verify that

2
(β −1)(m−1)

{E[f (xs

m)] − f (x∗)} ≤

1
m−1

{E[f (xs

m)] − f (x∗)} .

(24)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

19

Using the above inequality and Lemma 4, we have

E[f ((cid:101)xs) − f (x∗)] +

E[f ((cid:101)xs) − f (x∗)] +

2
(β −1)(m−1)

E[f (xs

m) − f (x∗)]

1
m−1

E[f (xs

m) − f (x∗)]

(cid:19)

(cid:18)

(cid:18)

(cid:19)

1 −

2
β −1
2
β −1
2
(β −1)(m−1)

1 −

≤

≤

E[f (xs

0) − f (x∗)] +

2m
(β −1)(m−1)

E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Summing the above inequality over s = 1, 2, . . . , S, taking expectation with respect to the history of random variables is
k,
and using the setting of xs+1

m, we obtain

0 = xs
(cid:19)

2
β −1

E[f ((cid:101)xs) − f (x∗)]

S
(cid:88)

(cid:18)

1 −

s=1
S
(cid:88)

(cid:26)

s=1

≤

2
(β −1)(m−1)

E[f (xs

0) − f (x∗) − (f (xs

m) − f (x∗))] +

2m
(β −1)(m−1)

(cid:27)
E[f ((cid:101)xs−1) − f (x∗)]

+

Lβ
2(m−1)

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting

2m
(β−1)(m−1)

(cid:80)S−1

s=1 [f ((cid:101)xs)−f (x∗)] from both sides of the above inequality, we have

≤

2m
(β −1)(m−1)
2
(β −1)(m−1)
Lβ
(cid:104)
E
2(m − 1)

+

E[f ((cid:101)xS) − f (x∗)] +

S
(cid:88)

(cid:18)

1 −

s=1

4
β −1

−

2
(β −1)(m−1)

(cid:19)

E[f ((cid:101)xs) − f (x∗)]

(cid:104)
E

f (x1

0) − f (x∗) − (f (xS
m) − f (x∗))
m(cid:107)2(cid:105)

0(cid:107)2 − (cid:107)x∗ − xS

.

(cid:107)x∗ − x1

(cid:105)

+

2m
(β −1)(m−1)

E[f ((cid:101)x0) − f (x∗)]

Dividing both sides of the above inequality by S, and using the setting of (cid:101)x0 = x1
2
(β −1)(m−1)

E[f ((cid:101)xs) − f (x∗)]

4
β −1

(cid:19) S
(cid:88)

1 −

1
S

−

(cid:18)

s=1

0, we arrive at

≤

≤

=

2m
(β −1)(m−1)S

2
(β −1)(m−1)S
2(m + 1)
(β −1)(m−1)S

E[f ((cid:101)xS) − f (x∗)] +

(cid:18)

1
S

1 −

4
β −1

−

2
(β −1)(m−1)

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

[f (x1

0) − f (x∗)] +

2m
(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)x∗ − x1

0(cid:107)2

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2

where the ﬁrst inequality holds due to the fact that f ((cid:101)xS)−f (x∗) ≥ 0; the second inequality holds due to the facts that
f (xS

m(cid:107)2 ≥ 0; and the last equality follows from the setting of (cid:101)x0 = x1
0.

m)−f (x∗) ≥ 0 and (cid:107)x∗ −xS
Due to the deﬁnition of xS (i.e., xS = 1
S

(cid:80)S

s=1 (cid:101)xs) and the convexity of f (·), we have f (xS) ≤ 1

S

therefore the above inequality becomes:

(cid:18)

−

1 −

4
β −1
2(m + 1)
(β −1)(m−1)S

≤

2
(β −1)(m−1)

(cid:19)

(cid:104)
E

(cid:105)
f (xS) − f (x∗)

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Dividing both sides of the above inequality by c1 = 1− 4

β−1 −

2

(β−1)(m−1) > 0, we have

(cid:104)
E

(cid:105)
f (xS)

− f (x∗) ≤

2(m + 1)
c1(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

Lβ
2c1(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Due to the setting for the output of Algorithm 2, (cid:98)xS = (cid:101)xS if f ((cid:101)xS) ≤ f (xS). Then
(cid:105)

(cid:104)
E

(cid:105)
f ((cid:98)xS)

(cid:104)
− f (x∗) ≤ E

f (xS)

− f (x∗) ≤

2(m + 1)
c1(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

Lβ
2c1(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

(cid:80)S

s=1 f ((cid:101)xs), and

(25)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

20

Alternatively, when f ((cid:101)xS) ≥ f (xS), let (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

APPENDIX B: CONVERGENCE ANALYSIS OF ALGORITHM 2 WITH OPTION I

Similar to Algorithm 2 with Option II, we also analyze the convergence properties of Algorithm 2 with Option I for smooth
and non-strongly functions. We ﬁrst give and prove the following lemma.

Lemma 5 (Smooth objectives and Option I). If each fi(·) is convex and L-smooth, then the following inequality holds for all

s = 1, 2, . . . , S,

≤

β −3
β −1
2
(β −1)

E[f ((cid:101)xs)−f (x∗)] +

E[f (xs

m)−f (x∗)]

2
(β −1)m
2
(β −1)m

E(cid:2)f ((cid:101)xs−1)−f (x∗)(cid:3)+

E[f (xs

0)−f (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

This lemma is a slight generalization of Lemma 4, and we give its detailed proof for completeness.

Proof. For convenience, the stochastic gradient estimator is deﬁned as: (cid:101)∇is

((cid:101)xs−1)+∇f ((cid:101)xs−1). Since each
function fi(x) is L-smooth, which implies that the gradient of the average function f (x) is Lipschitz continuous with
parameter L, i.e., for all x, y ∈ Rd,

k)−∇fis

:= ∇fis

(xs

k

k

k

whose equivalent form is

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2.

Using the above smoothness inequality, we have

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

(cid:11) +

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

= f (xs

= f (xs

k) + (cid:10)∇f (xs
(cid:68)

k), xs

k+1 − xs
k
(cid:69)

(cid:101)∇is

k

, xs

k+1 − xs
k

+

k) +
(cid:68)
∇f (xs

L
2
Lβ
2
(cid:107)xs

(cid:11) +
Lβ
2
(cid:69)

k(cid:107)2

k+1 − xs
L(β −1)
2

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

−

(cid:107)xs

k+1 − xs

k(cid:107)2.

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

(26)

+

Using Lemma 1, then we get

(cid:20)(cid:68)

E

∇f (xs

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

(cid:69)

−

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)f (xs
β −1

≤

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

L(β −1)
2
L(β −1)
2

(cid:107)∇f (xs

k) − (cid:101)∇is

k

(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

(27)

where the ﬁrst inequality holds due to the Young’s inequality (i.e., yT z ≤ (cid:107)y(cid:107)2/(2θ)+θ(cid:107)z(cid:107)2/2 for all θ > 0 and y, z ∈ Rd),

and the second inequality follows from Lemma 1.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

21

Taking the expectation over the random choice of is

k and substituting the inequality in (27) into the inequality in (26),

we have

E[f (xs

k+1)]

≤ f (xs

k) + E

(cid:20)(cid:68)

≤ f (xs

k) + E

(cid:20)(cid:68)

(cid:101)∇is

k

, xs

k+1 − xs
k

(cid:69)

+

Lβ
2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

+

2
β −1

(cid:101)∇is

k

, x∗ − xs
k

(cid:69)

+

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)
(cid:2)f (xs
(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

+

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

Lβ
2
(cid:20) Lβ
2

k(cid:105) + E

2
β −1
2
β −1

+

(cid:21)
k+1(cid:107)2)

≤ f (xs

k), x∗ − xs

k) + (cid:104)∇f (xs
(cid:20) Lβ
2
E(cid:2)(cid:0)(cid:107)x∗ − xs

((cid:107)x∗ − xs

Lβ
2

≤ f (x∗) + E

= f (x∗) +

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs
(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

+

k(cid:107)2 − (cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

(cid:2)f (xs

2
β −1
2
β −1

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)
k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

where the ﬁrst inequality holds due to the inequality in (26) and the inequality in (27); the second inequality follows from
Lemma 2 with ˆz = xs
fact that E[ (cid:101)∇is

k, τ = Lβ = 1/η, and r(z) := (cid:104) (cid:101)∇is
k); and the last inequality follows from the convexity of f (·), i.e., f (xs

k(cid:105); the third inequality holds due to the
k(cid:105) ≤ f (x∗).

k+1, z = x∗, z0 = xs

k)+(cid:104)∇f (xs

k), x∗ −xs

] = ∇f (xs

, z − xs

k

k

The above inequality can be rewritten as follows:

E[f (xs

k+1)] − f (x∗) ≤

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

Summing the above inequality over k = 0, 1, . . . , m−1, we obtain

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

m−1
(cid:88)

k=0
m−1
(cid:88)

k=0

≤

(cid:8)E[f (xs

k+1)] − f (x∗)(cid:9)

(cid:26) 2

β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3)

Then

(cid:18)

1 −

2
β −1

(cid:19) m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)} +

2
β −1

m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)}

≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

(cid:27)
k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

(cid:27)

.

(28)

Due to the setting of (cid:101)xs = 1

m

(cid:80)m

k=1 xs

k in Option I, and the convexity of f (·), then

f ((cid:101)xs) ≤

1
m

m
(cid:88)

k=1

f (xs

k).

Using the above inequality, the inequality in (28) becomes

(cid:18)

m

1 −

(cid:19)

2
β −1

E[f ((cid:101)xs) − f (x∗)] +

2
β −1

m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)}

≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

(cid:27)
k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Dividing both sides of the above inequality by m and subtracting

2
(β−1)m

(cid:80)m−1

k=1 [f (xs

k)−f (x∗)] from both sides, we arrive

at

(cid:18)

1 −

(cid:19)

E[f ((cid:101)xs) − f (x∗)] +

2
β −1
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

2
(β −1)m
2
(β −1)m

≤

2
(β −1)

E[f (xs

m) − f (x∗)]

E[f (xs

0) − f (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

This completes the proof.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

22

Similar to Theorem 1, we also provide the convergence guarantees of Algorithm 2 with Option I for solving smooth

and non-strongly convex functions as follows.

Theorem 6 (Smooth objectives and Option I). If each fi(·) is convex and L-smooth, then the following inequality holds

(cid:104)
E

(cid:105)
f ((cid:98)xS)

− f (x∗) ≤

2(m+1)
mS(β −5)

[f ((cid:101)x0)−f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Proof. Using Lemma 5, we have

(1 −

2
β −1
2
(β −1)m

≤

)E[f ((cid:101)xs) − f (x∗)] +

E[f (xs

m) − f (x∗)]

E[f (xs

0) − f (x∗)] +

E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2m

E[(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2].

2
(β −1)m
2
β −1

Summing the above inequality over s = 1, 2, . . . , S, taking expectation with respect to the history of random variables is
k,
and using the setting of xs+1

m in Algorithm 2, we arrive at

0 = xs

S
(cid:88)

(1−

2
β −1

)E[f ((cid:101)xs) − f (x∗)]

s=1
S
(cid:88)

(cid:26)

s=1

≤

2
(β −1)m

E{f (xs

0) − f (x∗) − [f (xs

m) − f (x∗)]} +

2
β −1

(cid:27)
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting 2
β−1

(cid:80)S

s=1[f ((cid:101)xs) − f (x∗)] from both sides of the above inequality, we obtain

(cid:19)

E[f ((cid:101)xs) − f (x∗)]

4
β −1
(cid:110)

E

f (x1

S
(cid:88)

(cid:18)

1 −

s=1

2
(β −1)m
Lβ
(cid:104)
2m

+

E

≤

0) − f (x∗) − [f (xS
m(cid:107)2(cid:105)

0(cid:107)2 − (cid:107)x∗ − xS

.

(cid:107)x∗ − x1

(cid:111)
m) − f (x∗)]

+

2
β −1

E[f ((cid:101)x0) − f ((cid:101)xS)]

It is not hard to verify that E[f ((cid:101)x0)−f ((cid:101)xS)] ≤ f ((cid:101)x0)−f (x∗). Dividing both sides of the above inequality by S, and using

the choice of (cid:101)x0 = x1

0, we have

(cid:18)

1 −

1
S

4
β −1

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

≤

≤

=

2
(β −1)mS
2
(β −1)mS
2(m+1)
(β −1)mS

(cid:110)

E

f (x1

0) − f (x∗) − [f (xS

(cid:111)
m) − f (x∗)]

+

(cid:2)f ((cid:101)x0) − f (x∗)(cid:3) +

[f ((cid:101)x0) − f (x∗)] +

2
(β −1)S
Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2

2
(β −1)S
Lβ
2mS

[f ((cid:101)x0) − f (x∗)] +

(cid:107)x∗ − (cid:101)x0(cid:107)2

[f ((cid:101)x0) − f (x∗)] +

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2

(29)

where the ﬁrst inequality holds due to the facts that E[f ((cid:101)x0)−f ((cid:101)xS)] ≤ f ((cid:101)x0)−f (x∗) and E(cid:2)(cid:107)x∗ −xS
m)−f (x∗)(cid:3) ≥ 0 and (cid:101)x0 = x1
inequality uses the facts that E(cid:2)f (xS
0.
(cid:80)S
s=1 (cid:101)xs, and using the convexity of f (·), we have f (xS) ≤ 1

Since xS = 1
S

(cid:80)S

S

(29) becomes

(cid:18)

1 −

(cid:19)

4
β −1

(cid:104)
E

(cid:105)
f (xS) − f (x∗)

≤

(cid:18)

1 −

1
S

4
β −1

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

≤

2(m+1)
(β −1)mS

[f ((cid:101)x0) − f (x∗)] +

Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

m(cid:107)2(cid:3) ≥ 0, and the last

s=1f ((cid:101)xs), and therefore the inequality in

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

23

Dividing both sides of the above inequality by (1− 4

β−1 ) > 0 (i.e., η < 1/(5L)), we arrive at

(cid:104)

E

f (xS) − f (x∗)

(cid:105)

≤

2(m+1)
mS(β −5)

[f ((cid:101)x0) − f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2.

If f ((cid:101)xS) ≤ f (xS), then (cid:98)xS = (cid:101)xS, and
(cid:105)
f ((cid:98)xS) − f (x∗)

(cid:104)
E

(cid:104)
≤ E

(cid:105)
f (xS) − f (x∗)

≤

2(m+1)
mS(β −5)

[f ((cid:101)x0) − f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Alternatively, if f ((cid:101)xS) ≥ f (xS), then (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

APPENDIX C: PROOF OF THEOREM 2

We ﬁrst extend Lemma 1 to the non-smooth setting as follows [34], [47].

Corollary 2 (Variance bound of non-smooth objectives). If each fi(·) is L-smooth, then the following inequality holds

(cid:104)
E

(cid:107) (cid:101)∇fis

k

(xs

k) − ∇f (xs

k)(cid:107)2(cid:105)

≤ 4L[F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)].

Before proving Theorem 2, we ﬁrst give and prove the following lemma.

Lemma 6 (Non-smooth objectives). If each fi(·) is L-smooth, then the following inequality holds for all s = 1, 2, . . . , S,

E[F ((cid:101)xs)−F (x∗)] +

E[F (xs

m)−F (x∗)]

β −3
β −1
2
(β −1)m

≤

2
(β −1)m
2
β −1

E[F (xs

0)−F (x∗)]+

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2m

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

Proof of Lemma 6:

Proof. Since the average function f (x) is L-smooth, then for all x, y ∈ Rd,

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2,

which then implies

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

Using the above inequality, we have

F (xs

k+1) = f (xs

k+1) + g(xs

k+1)

(cid:11) +

L
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 .
(cid:13)

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

k+1) + (cid:10)∇f (xs
(cid:68)

k+1) +

(cid:101)∇is

k

, xs

≤ f (xs

k) + g(xs

= f (xs

k) + g(xs
(cid:68)
∇f (xs

+

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

Lβ
2
(cid:107)xs

k), xs

k+1 − xs
k
(cid:69)

(cid:11) +
Lβ
2
L(β −1)
2

+

−

k+1 − xs
k
(cid:69)

k+1 − xs

k(cid:107)2

(cid:107)xs

k+1 − xs

k(cid:107)2.

According to Corollary 2, then we obtain

(cid:20)(cid:68)

E

∇f (xs

k) − (cid:101)∇is

k

, xs

k+1 − xs
k

(cid:69)

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

−

L(β −1)
2
L(β −1)
2

(cid:107)∇f (xs

k) − (cid:101)∇is

k

(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) ,

(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)F (xs
β −1

≤

L(β −1)
2

(cid:21)

(cid:107)xs

k+1 −xs

k(cid:107)2

(30)

(31)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

24

where the ﬁrst inequality holds due to the Young’s inequality, and the second inequality follows from Corollary 2.
Substituting the inequality (31) into the inequality (30), and taking the expectation over the random choice is

k, we arrive at

E(cid:2)F (xs

k)] + E(cid:2)g(xs
k+1)(cid:3) ≤ E[f (xs
2
β −1

(cid:2)F (xs

+

k+1)(cid:3) + E

(cid:20)(cid:68)

(cid:101)∇is

k

, xs

k+1 − xs
k

(cid:69)

+

Lβ
2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)
Lβ
2

, x∗ − xs
k

(cid:101)∇is

(cid:20)(cid:68)

+

(cid:69)

k

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)

≤ E[f (xs

k)] + g(x∗) + E
2
β −1

(cid:2)F (xs

+

≤ f (x∗) + g(x∗) + E

(cid:20) Lβ
2

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:21)
k+1(cid:107)2)

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2)

(cid:21)

+

2
β −1

(cid:2)F (xs

= F (x∗) +

Lβ
2

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)
k+1(cid:107)2(cid:3) +
E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

2
β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) ,

where the ﬁrst inequality holds due to the inequality (30) and the inequality (31); the second inequality follows from
Lemma 2 with ˆz = xs
due to the fact that E[ (cid:101)∇is

k(cid:105) + g(z); and the third inequality holds
k(cid:105) ≤ f (x∗). Then the above

k, τ = Lβ = 1/η, and r(z) := (cid:104) (cid:101)∇is
k) and the convexity of f (·), i.e., f (xs

k+1, z = x∗, z0 = xs
] = ∇f (xs

, z − xs
k) + (cid:104)∇f (xs

k), x∗ − xs

k

k

inequality is rewritten as follows:

E[F (xs
2
β −1

k+1)] − F (x∗)
(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

≤

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

(32)

Summing the above inequality over k = 0, 1, . . . , (m−1) and taking expectation over whole history, we have

m−1
(cid:88)

k=0
m−1
(cid:88)

k=0

≤

(cid:8)E[F (xs

k+1)] − F (x∗)(cid:9)

(cid:26) 2

β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k+1(cid:107)2(cid:3)

.

Subtracting 2
β−1

(cid:80)m−2
k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3)

from both sides of the above inequality, we obtain

m−1
(cid:88)

E(cid:2)F (xs

k+1) − F (x∗)(cid:3) −

2
β −1

m−1
(cid:88)

k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3) +

2
β −1

E[F (xs

m)−F (x∗)]

k=0
m−1
(cid:88)

(cid:26) 2

β −1

k=0

≤

Then

(cid:2)F (xs

(cid:27)
k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

−

2
β −1

m−2
(cid:88)

k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3)+

Lβ
2

(cid:18)

1 −

2
β −1

(cid:19) m
(cid:88)

k=1

E[F (xs

k) − F (x∗)] +

2
β −1

E[F (xs

m)−F (x∗)]

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

≤

2
β −1

E[F (xs

0) − F (x∗)] +

2m
β −1

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

Due to the settings of (cid:101)xs = 1

m

1
m

(cid:80)m

k=1 F (xs

k), and

(cid:80)m

k=1 xs

k and xs+1

0 = xs

m, and the convexity of the objective function F (·), we have F ((cid:101)xs) ≤

(cid:18)

m

1 −

(cid:19)

2
β −1

E[F ((cid:101)xs) − F (x∗)] +

2
β −1

E[F (xs

m)−F (x∗)]

≤

2
β −1

E[F (xs

0) − F (x∗)] +

2m
β −1

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

Dividing both sides of the above inequality by m, we arrive at

(cid:18)

1 −

(cid:19)

2
β −1

≤

2
(β −1)m

E[F (xs

E[F ((cid:101)xs) − F (x∗)] +
2
β −1

0) − F (x∗)] +

m)−F (x∗)]

E[F (xs

2
(β −1)m
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2m

25

(33)

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

This completes the proof.

Proof of Theorem 2:

Proof. Summing the inequality in (33) over s = 1, 2, . . . , S, and taking expectation with respect to the history of is

k, we have

S
(cid:88)

(cid:18)

1−

(cid:19)

2
β −1

E[F ((cid:101)xs) − F (x∗)] +

S
(cid:88)

s=1

2
(β −1)m

E[F (xs

m) − F (x∗)]

2
(β −1)m

E[F (xs

0) − F (x∗)] +

2
β −1

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

(cid:27)

s=1
S
(cid:88)

(cid:26)

s=1

≤

+

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

E[F (xs

m)−F (x∗)] + 2
β−1

(cid:80)S

s=1[F ((cid:101)xs)−F (x∗)] from both sides of the above inequality, and using

Subtracting (cid:80)S
2
s=1
(β−1)m
the setting of xs+1
0 = xs
S
(cid:18)
(cid:88)

m, we arrive at

1 −

s=1

≤

2
(β −1)m

(cid:19)

4
β −1

E[F ((cid:101)xs) − F (x∗)]

(cid:104)
E

F (x1

(cid:105)
0)−F (xS
m)

+

(cid:104)
E

2
β −1

F ((cid:101)x0)−F ((cid:101)xS)

(cid:105)

+

(cid:104)
E

Lβ
2m

(cid:107)x∗ −x1

0(cid:107)2 − (cid:107)x∗ −xS

m(cid:107)2(cid:105)

.

It is easy to verify that E[F ((cid:101)x0)−F ((cid:101)xS)] ≤ F ((cid:101)x0)−F (x∗). Dividing both sides of the above inequality by S, and using the
choice (cid:101)x0 = x1

[F ((cid:101)x0) − F ((cid:101)xS)] +

(cid:107)x∗ − x1

0(cid:107)2

(34)

[F ((cid:101)x0) − F (x∗)] +

(cid:107)x∗ − (cid:101)x0(cid:107)2

Lβ
2mS
Lβ
2mS

0, we obtain
(cid:18)

1 −

4
β −1

(cid:19) 1
S

S
(cid:88)

E[F ((cid:101)xs) − F (x∗)]

s=1
(cid:105)
0)−F (xS
m)

2
(β −1)S

(cid:104)

E

≤

≤

+

F (x1

2
(β −1)mS
2
2
(β −1)mS
(β −1)S
2(m+1)
Lβ
(β −1)mS
2mS
where the ﬁrst inequality uses the fact that (cid:107)x∗ − xS
E(cid:2)F (x1

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

[F ((cid:101)x0) − F (x∗)] +

m)(cid:3) ≤ F (x1

0)−F (xS

=

(cid:107)(cid:101)x0 − x∗(cid:107)2

m(cid:107)2 ≥ 0; and the last inequality holds due to the facts that

0)−F (x∗), E[F ((cid:101)x0)−F ((cid:101)xS)] ≤ F ((cid:101)x0)−F (x∗), and (cid:101)x0 = x1
0.

Using the deﬁnition of xS = 1
S

(cid:80)S

s=1 (cid:101)xs and the convexity of the objective function F (·), we have F (xS) ≤ 1

S

(cid:80)S

s=1F ((cid:101)xs),

and therefore we can rewrite the above inequality in (34) as follows:

(cid:18)

1 −

(cid:19)

4
β −1

(cid:104)
E

(cid:105)
F (xS) − F (x∗)

≤

(cid:18)

1 −

4
β −1

(cid:19) 1
S

S
(cid:88)

s=1

E[F ((cid:101)xs) − F (x∗)]

≤

2(m+1)
(β −1)mS

[F ((cid:101)x0) − F (x∗)] +

Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Dividing both sides of the above inequality by (1− 4

β−1 ) > 0, we have

(cid:104)
E

(cid:105)
F (xS)

− F (x∗) ≤

2(m+1)
(β −5)mS

[F ((cid:101)x0) − F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

When F ((cid:101)xS) ≤ F (xS), then (cid:98)xS = (cid:101)xS, and
(cid:104)

E

(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

2(m+1)
(β −5)mS

[F ((cid:101)x0) − F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

26

Alternatively, if F ((cid:101)xS) ≥ F (xS), then (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

APPENDIX D: CONVERGENCE ANALYSIS OF ALGORITHM 2 WITH OPTION II

Corollary 3 (Option II and non-smooth objectives). If each fi(·) is convex and L-smooth, then the following inequality holds for

all s = 1, 2, . . . , S,

β −3
β −1
2m
γ

≤

E[F ((cid:101)xs) − F (x∗)] +
E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

1
m−1
2
γ

E[F (xs

m) − F (x∗)]

E[F (xs

0) − F (x∗)] +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Corollary 4 (Option II and non-smooth objectives). Suppose Assumption 1 holds. Then the following inequality holds

(cid:104)
E

(cid:105)

F ((cid:98)xS)

− F (x∗) ≤

2(m + 1)
[γ −4m+2]S

[F ((cid:101)x0) − F (x∗)] +

β(β − 1)L
2[γ −4m+2]S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Corollaries 3 and 4 can be viewed as the generalizations of Lemma 6 and Theorem 2, respectively, and hence their proofs
are omitted. From Theorem 2 and Corollary 4, one can see that Algorithm 2 also achieves a convergence rate O(1/T ) for
non-strongly convex and non-smooth functions.

APPENDIX E: PROOF OF THEOREM 3

Before proving Theorem 3, we ﬁrst give and prove the following lemma.

Lemma 7 (Mini-batch). Using the same notation as in Corollary 2, we have

2δ(b)
(β −1)m
2δ(b)
(β −1)

≤

{E[F (xs

m)]−F (x∗)}+(1−

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

2δ(b)
β −1

){E[F ((cid:101)xs)]−F (x∗)}
Lβ
2m

0) − F (x∗)] +

[F (xs

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Proof of Lemma 7:

Proof. In order to simplify notation, the stochastic gradient estimator of mini-batch is deﬁned as:

(cid:101)∇I s

k

:=

1
b

(cid:88)

i∈I s
k

(cid:2)∇fi(xs

k) − ∇fi((cid:101)xs−1)(cid:3) + ∇f ((cid:101)xs−1).

F (xs

k+1) ≤ g(xs

k+1) + f (xs

k) + (cid:10)∇f (xs
(cid:68)

k) +

(cid:101)∇I s

k

, xs

Lβ
2
(cid:107)xs

k), xs

k+1 − xs
k
(cid:69)

(cid:11) +
Lβ
2
L(β −1)
2

+

−

k+1 − xs
k
(cid:69)

k+1 − xs

k(cid:107)2

(cid:107)xs

k+1 − xs

k(cid:107)2.

k) − (cid:101)∇I s

k

, xs

k+1 − xs
k

= g(xs

k+1) + f (xs
(cid:68)

∇f (xs

+

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

Using Corollary 1, then we obtain

(cid:20)(cid:68)

∇f (xs

k) − (cid:101)∇I s

k

E

, xs

k+1 − xs
k

(cid:69)

−

(cid:107)∇f (xs

k) − (cid:101)∇I s

k

(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

L(β −1)
2
L(β −1)
2

(cid:20)
≤ E

≤

2δ(b)
β −1

1
2L(β −1)
(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)

(35)

(36)

(cid:21)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

27

where the ﬁrst inequality holds due to the Young’s inequality, and the second inequality follows from Corollary 1.
Substituting the inequality (36) into the inequality (35), and taking the expectation over the random mini-batch set I s
k,

we have

E[F (xs

k+1)]

≤ E[g(xs

k+1)] + f (xs

≤ g(x∗) + f (xs

k) + E

k) + E
(cid:20)(cid:68)

(cid:20)(cid:68)

(cid:101)∇I s

k

, xs

k+1 −xs
k

(cid:69)

+

Lβ
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

+

(cid:101)∇I s

k

, x∗ −xs
k

(cid:69)

+

Lβ
2

((cid:107)x∗ −xs

k(cid:107)2 −(cid:107)x∗ −xs

k+1(cid:107)2)

2δ(b)
β −1
(cid:21)

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

+

2δ(b)
β −1

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

≤ g(x∗) + f (x∗) + E

(cid:20) Lβ
2

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:21)
k+1(cid:107)2)

= F (x∗) +

Lβ
2

E(cid:2)(cid:0)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

2δ(b)
β − 1

+

2δ(b)
β −1
(cid:2)F (xs

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

where the second inequality holds from Lemma 2. Then the above inequality is rewritten as follows:

E(cid:2)F (xs

k+1)(cid:3) − F (x∗) ≤

2δ(b)
β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

Summing the above inequality over k = 0, 1, · · · , (m − 1), then

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

(37)

m−1
(cid:88)

k=0
m−1
(cid:88)

k=0

≤

(cid:8)E[F (xs

k+1)] − F (x∗)(cid:9)

(cid:26) 2δ(b)
β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k+1(cid:107)2(cid:3)

.

Since (cid:101)xs = 1

m

(cid:80)m

k=1 xs

k, we have F ((cid:101)xs) ≤ 1

m

{E[F (xs

m)] − F (x∗)} +

{E[F ((cid:101)xs)] − F (x∗)}

(cid:80)m

k=1 F (xs
(cid:18)
2δ(b)
β −1

1 −

k), and
(cid:19)

2δ(b)
(β −1)m
2δ(b)
(β −1)

≤

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

This completes the proof.

Proof of Theorem 3:

Proof. Using Lemma 7, we have

(cid:18)

1 −

≤

2δ(b)
(β −1)

(cid:19)

E[F ((cid:101)xs) − F (x∗)]

2δ(b)
β −1
(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Summing the above inequality over s = 1, 2, . . . , S, taking expectation over whole history of I s

k, and using xs+1

0 = xs

m, we

obtain

S
(cid:88)

(cid:18)

1 −

(cid:19)

2δ(b)
β −1

E [F ((cid:101)xs) − F (x∗)]

s=1
S
(cid:88)

s=1

≤

2δ(b)
(β −1)

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

S
(cid:88)

s=1

2δ(b)
(β −1)m

E[F (xs

0) − F (xs

m)] +

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting

2δ(b)
β−1

(cid:80)S−1
s=1

E[F ((cid:101)xs)−F (x∗)] from both sides of the above inequality, we have
(cid:19)

(cid:18)

(cid:104)
E

2δ(b)
β −1

(cid:105)
F ((cid:101)xS) − F (x∗)

+

S
(cid:88)

1 −

4δ(b)
β −1

E[F ((cid:101)xs) − F (x∗)]

≤

2δ(b)
(β −1)

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

(cid:104)
E

F (x1

(cid:105)
0) − F (xS
m)

+

(cid:104)

E

Lβ
2m

(cid:107)x∗ − x1

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

s=1

2δ(b)
(β −1)m

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

28

Dividing both sides of the above inequality by S and using E[F (x)] ≤ 1
S

(cid:80)S

s=1 F ((cid:101)xs), we arrive at

2δ(b)
(β −1)S
2δ(b)
(β −1)S

≤

E[F ((cid:101)xS) − F (x∗)] +

(cid:18)

1 −

(cid:19)

4δ(b)
β −1

E[F (x) − F (x∗)]

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

2δ(b)
(β −1)mS

(cid:104)
E

F (x1

0) − F (xS
m)

(cid:105)

+

(cid:104)

E

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

.

Subtracting

2δ(b)
(β−1)S
(cid:18)

1 −

(cid:19)

E[F ((cid:101)xS)−F (x∗)] from both sides of the above inequality, we have
4δ(b)
β −1
(cid:104)

E[F (x) − F (x∗)]

(cid:105)

(cid:105)

≤

2δ(b)
(β −1)S

E

F ((cid:101)x0) − F ((cid:101)xS)

+

2δ(b)
(β −1)mS

(cid:104)
E

F (x1

0) − F (xS
m)

+

(cid:104)
E

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

.

Dividing both sides of the above inequality by (1− 4δ(b)

β−1 ) > 0, we arrive at

E[F (x)] − F (x∗)
2δ(b)
(β −1−4δ(b))S
2δ(b)
(β −1−4δ(b))S
2δ(b)(m + 1)
(β −1−4δ(b))mS

≤

≤

=

(cid:104)
E

F ((cid:101)x0) − F ((cid:101)xS)

(cid:105)

+

E(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

E(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

(cid:104)

E

2δ(b)
(β −1−4δ(b))mS
2δ(b)
(β −1−4δ(b))mS
Lβ(β −1)
2(β −1−4δ(b))mS

(cid:105)
F ((cid:101)x0) − F (xS
m)

+

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3)

E(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3)

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3) .

When F ((cid:101)xS) ≤ F (xS), then (cid:98)xS = (cid:101)xS, and

E

(cid:104)

(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

2δ(b)(m + 1)
(β −1−4δ(b))mS

E(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3) .

Alternatively, if F ((cid:101)xS) ≥ F (xS), then (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

Theorem 3 shows that the mini-batch version of Algorithm 2 with Option I attains a convergence rate O(1/T ) for
non-strongly convex and non-smooth functions. In addition, we can also provide the convergence properties of Algorithm
2 for non-strongly convex and smooth functions.

APPENDIX F: PROOFS OF THEOREMS 4 AND 7

We also provide the convergence guarantees for VR-SGD under the strongly convex condition. We ﬁrst give the following
assumption.

Assumption 3. For all s = 1, 2, . . . , S, the following inequality holds

E[F (xs

0) − F (x∗)] ≤ c E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

where 0 < c (cid:28) m is a constant8.

Proof of Theorem 4:

Proof. Since each fi(·) is convex and L-smooth, then Corollary 3 holds, which then implies that

(cid:19)

(cid:18)

1 −

2
β −1
2m
(m−1)(β −1)

≤

E[F ((cid:101)xs) − F (x∗)] +

1
m−1

E(cid:2)F (xs+1

0

) − F (x∗)(cid:3)+

Lβ
2(m−1)

E(cid:2)(cid:107)xs+1

0 − x∗(cid:107)2(cid:3)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)+

2
(m−1)(β −1)

E[F (xs

0) − F (x∗)]+

Lβ
2(m−1)

E(cid:2)(cid:107)xs

0 − x∗(cid:107)2(cid:3) .

(38)

8. This assumption shows the relationship of the gaps between the function values at the starting and snapshot points of each epoch and the

optimal value of the objective function. In fact, as our algorithm progresses, (cid:101)xs−1 and xs−1
0) both converge toward the same optimal point
x∗, and thus c is far less than m, i.e., c (cid:28) m, as shown in Fig. 11. In fact, we can choose a simple restart technique as in [48] instead of this

m (i.e., xs

assumption and achieve the same convergence guarantee.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

29

(a) Adult: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 11. The value of c/m vs. the number of epochs for VR-SGD, where m = 2n.

(b) Covtype: λ = 10−4 (left) and λ = 10−5 (right)

Due to the strong convexity of F (·), we have (cid:107)xs

0 − x∗(cid:107)2 ≤ (2/µ)[F (xs

0) − F (x∗)]. Then the inequality in (38) can be

rewritten as follows:

(cid:18)

(cid:19)

1 −

2
β −1
2m
(m−1)(β −1)
2m
(m−1)(β −1)
(cid:18) 2(m+c)

≤

≤

=

(m−1)(β −1)

E[F ((cid:101)xs) − F (x∗)]

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)+

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)+
cLβ
µ(m−1)

+

(cid:19)

(cid:18)

(cid:18)

2
(m−1)(β −1)
2c
(m−1)(β −1)
E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

+

+

(cid:19)

(cid:19)

Lβ
µ(m−1)
cLβ
µ(m−1)

E[F (xs

0) − F (x∗)]

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

where the ﬁrst inequality holds due to the fact that (cid:107)xs

0) − F (x∗)], and the second inequality follows
from Assumption 3. Dividing both sides of the above inequality by [1−2/(β −1)] > 0 (that is, β is required to be larger than

0 − x∗(cid:107)2 ≤ (2/µ)[F (xs

3) and using the deﬁnition of β = 1/(Lη), we arrive at

E[F ((cid:98)xs) − F (x∗)] ≤ E[F ((cid:101)xs) − F (x∗)]

(cid:18) 2(m+c)

(m−1)(β −3)
(cid:18) 2(m+c)Lη

(m−1)(1−3Lη)

+

cLβ(β −1)
µ(m−1)(β −3)

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

+

c(1−Lη)
µη(m−1)(1−3Lη)

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) .

≤

=

This completes the proof.

2040608010010−610−510−410−3Number of epochsc/m2040608010010−610−510−410−3Number of epochsc/m2040608010010−810−610−4Number of epochsc/m2040608010010−610−510−410−3Number of epochsc/mIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

30

Similar to Theorem 4, we give the following convergence result for Algorithm 2 with Option I for non-smooth and

strongly-convex functions.

Theorem 7 (Option I). Suppose Assumptions 1, 2, and 3 hold, and m is sufﬁciently large so that

ρI :=

2Lη(m+c)
m(1−3Lη)

+

c(1−Lη)
mµη(1−3Lη)

< 1.

Then Algorithm 2 with Option I has the following geometric convergence in expectation:

E

(cid:104)

(cid:105)
F ((cid:98)xS) − F (x∗)

≤ ρS
I

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) .

Proof of Theorem 7:

Proof. Since each fi(·) is convex and L-smooth, then we have
(cid:19)

(cid:18)

E(cid:2)F (xs+1

0

) − F (x∗)(cid:3) +

E(cid:2)(cid:107)xs+1

0 − x∗(cid:107)2(cid:3)

1 −

2
β −1
E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

E[F ((cid:101)xs) − F (x∗)] +
2
m(β −1)
(cid:18)

2
m(β −1)

≤

≤

≤

2
β −1
2
β −1
(cid:18) 2(m+c)
m(β −1)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +
cLβ
mµ

+

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

E[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)xs

2
m(β −1)

+

(cid:19)

Lβ
mµ

E[F (xs

0) − F (x∗)]

Lβ
2m
0 − x∗(cid:107)2(cid:3)

where the ﬁrst inequality follows from Lemma 6; the second inequality holds due to the fact that (cid:107)xs
(2/µ)[F (xs

0) − F (x∗)]; and the last inequality follows from Assumption 3.
Due to the deﬁnition of β = 1/(Lη), the above inequality is rewritten as follows:

0 − x∗(cid:107)2 ≤

1−3Lη
1−Lη

E[F ((cid:101)xs) − F (x∗)] ≤

(cid:18) 2Lη(m+c)
m(1−Lη)

+

c
mµη

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) .

Dividing both sides of the above inequality by (1−3Lη)(1−Lη) > 0, we arrive at

E[F ((cid:98)xs) − F (x∗)] ≤ E[F ((cid:101)xs) − F (x∗)]

(cid:18) 2Lη(m+c)
m(1−3Lη)

+

c(1−Lη)
mµη(1−3Lη)

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) .

This completes the proof.

≤

It is clear that Theorem 7 shows that VR-SGD attains a linear convergence rate and at most the oracle complexity of

O((n+L/µ) log(1/(cid:15))) for non-smooth and strongly convex functions.

APPENDIX G: PROOF OF THEOREM 5 AND EQUIVALENT RELATIONSHIP
Before proving Theorem 5, we ﬁrst give and prove the following lemmas.

Lemma 8. Suppose each convex function fi(·) is L-smooth. Then the following inequality holds:

E

(cid:20)(cid:13)
(cid:13)
(cid:13) (cid:101)∇fis

k

(xs

(cid:13)
k) − ∇f (xs
(cid:13)
k)
(cid:13)

2(cid:21)

≤ 2L (cid:2)f ((cid:101)xs−1) − f (xs

k) − (cid:10)∇f (xs

k), (cid:101)xs−1 − xs

k

(cid:11)(cid:3) .

Lemma 8 is essentially identical to Lemma 3.4 in [48]. This lemma provides a tighter upper bound on the expected
k) than Lemma 1 in this paper and that of [34], [81], e.g., Lemma

(xs

variance of the variance-reduced gradient estimator (cid:101)∇fis
A.2 in [81].

k

Lemma 9. Suppose Assumption 1 holds. Let x∗ be an optimal solution of Problem (1), and {(vs

k, xs

k)} be the sequence generated by

Algorithm 3 with Option II. Then for all s = 1, . . . , S,

E[F ((cid:101)xs) − F (x∗)] ≤ (1 − ws)(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

+

w2
s
2mη0

E(cid:2)(cid:107)x∗ − vs

0(cid:107)2 − (cid:107)x∗ − vs

m(cid:107)2(cid:3) .

(39)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

31

Proof. Because Assumption 1 holds, the smoothness inequality in Assumption 1 has the following equivalent form,

fi(y) ≤ fi(x) + (cid:104)∇fi(x), y − x(cid:105) +

(cid:101)L
2

(cid:107)y − x(cid:107)2, ∀x, y ∈ Rd, i ∈ [n].

According to the deﬁnition of f (x) (i.e., f (x) := 1
n

(cid:80)n

i=1fi(x)), we have

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

g(y) ≤ g(x) + (cid:104)∇g(x), y − x(cid:105) +

(cid:101)L
2

L(cid:48)
2

(cid:107)y − x(cid:107)2, ∀x, y ∈ Rd,

(cid:107)y − x(cid:107)2, ∀x, y ∈ Rd

where the smooth convex regularizer g(x) is L(cid:48)-smooth. Then

F (y) ≤ F (x) + (cid:104)∇F (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2, ∀x, y ∈ Rd

where L = (cid:101)L + L(cid:48) ≥ (cid:101)L. In other words, F (x) is L-smooth. Let (cid:101)∇is
k) − ∇fis
be an appropriate constant, and α2 = α1 − 1 > 1. Using the above inequality, we have

:= ∇fis

(xs

k

k

k

((cid:101)xs−1) + ∇f ((cid:101)xs−1) + ∇g(xs

k), α1 > 2

F (xs

k+1) ≤ F (xs

k) + (cid:104)∇F (xs

k), xs

k+1 − xs

k(cid:105) +

(cid:107)xs

k+1 − xs

k(cid:107)2

= F (xs

k) + (cid:10)∇F (xs
(cid:68)

k), xs

k+1 − xs
k
(cid:69)

= F (xs

k) +

(cid:101)∇is

k

, xs

k+1 − xs
k

+

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

(cid:13)
2 −
(cid:13)

α2L
2
(cid:68)
∇F (xs

k+1 − xs

k(cid:107)2 +

k) − (cid:101)∇is

k

L
2
α1L
2
(cid:107)xs

(cid:11) +
α1L
2

(cid:69)(cid:105)

(cid:104)(cid:68)

∇F (xs

k) − (cid:101)∇is

k

(cid:107)∇F (xs

(cid:20) 1
2α2L
(cid:2)F ((cid:101)xs−1) − F (xs

, xs

k+1 − xs
k
α2L
2
k) − (cid:10)∇F (xs
k), (cid:101)xs−1 − xs

(cid:107)2 +

k+1 − xs

(cid:107)xs

k

k

k) − (cid:101)∇is

E

≤ E

≤

1
α2

(cid:21)

k(cid:107)2

(cid:11)(cid:3) +

α2L
2

, xs

k+1 − xs
k

(cid:69)

−

α2L
2

(cid:107)xs

k+1 − xs

k(cid:107)2.

E(cid:2)(cid:107)xs

k+1 − xs

k(cid:107)2(cid:3)

(40)

(41)

where the ﬁrst inequality follows from the Young’s inequality, i.e.,

xT y ≤

(cid:107)x(cid:107)2
2α

+

α(cid:107)y(cid:107)2
2

,

for all α > 0,

and the second inequality holds due to (cid:101)∇is
and Fi(x) is L-smooth. Substituting the inequality (41) into the inequality (40), and taking expectation over is

((cid:101)xs−1)+∇F ((cid:101)xs−1) and Lemma 8, where Fi(x) = fi(x)+g(x),

k)−∇Fis

= ∇Fis

(xs

k

k

k

k, we have

E(cid:2)F (xs
(cid:20)(cid:68)

k+1)(cid:3) − F (xs
k)
(cid:69)

(cid:101)∇is

k

, xs

k+1 − xs
k

≤ E

+

(cid:20)(cid:68)

= E

(cid:20)(cid:68)

≤ E

ws (cid:101)∇is

k

, vs

k+1 − vs
k

(cid:69)

ws (cid:101)∇is

k

, x∗ − vs
k

(cid:69)

+

= E

+

(cid:2)F ((cid:101)xs−1) − F (xs

1
α2
(cid:20) α1Lw2
s
2
(cid:104)(cid:68)
∇f ((cid:101)xs−1) − ∇fis

(cid:0)(cid:107)vs

k

+ E

= E

(cid:0)(cid:107)vs

(cid:20) α1Lw2
s
2
(cid:2)F ((cid:101)xs−1) − F (xs

1
α2

+

k)(cid:3)

+

(cid:107)xs

α1L
2
α1Lw2
s
2
α1Lw2
s
2

k+1 − xs

k(cid:107)2

(cid:21)

+

1
α2
(cid:21)

(cid:107)vs

k+1 − vs

k(cid:107)2

(cid:0)(cid:107)vs

k − x∗(cid:107)2 − (cid:107)vs

+

(cid:2)F ((cid:101)xs−1) − F (xs
(cid:21)

1
α2
k+1 − x∗(cid:107)2(cid:1)

(cid:2)F ((cid:101)xs−1) − F (xs

k) − (cid:10)∇F (xs

k), (cid:101)xs−1 − xs

k

(cid:11)(cid:3)

k) − (cid:10)∇F (xs

k), (cid:101)xs−1 − xs

k

(cid:11)(cid:3)

k) − (cid:10)∇F (xs

k), (cid:101)xs−1 − xs

k

(cid:11)(cid:3)

(42)

k − x∗(cid:107)2 − (cid:107)vs

k+1 − x∗(cid:107)2(cid:1)

(cid:28)

(cid:21)

+

∇F (xs

((cid:101)xs−1), wsx∗ + (1−ws)(cid:101)xs−1 − xs
(cid:21)

(cid:28)

k − x∗(cid:107)2 − (cid:107)vs

k+1 − x∗(cid:107)2(cid:1)

+

∇F (xs

(cid:69)(cid:105)

k −

k), wsx∗ + (1−ws)(cid:101)xs−1 − xs
1
α2
k), wsx∗ + (1−ws)(cid:101)xs−1 − xs

(cid:2)F ((cid:101)xs−1) − F (xs

k −

+

k

(cid:29)

((cid:101)xs−1 − xs
k)

(cid:29)

((cid:101)xs−1 − xs
k)

1
α2
k)(cid:3)
1
α2

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

32

k+1 −vs
k) (xs
k+1, z = x∗, z0 = vs

k+1 = (cid:101)xs−1 +ws(vs

k+1 − (cid:101)xs−1) = wsvs
k, τ = α1Lws, and r(z) := (cid:104) (cid:101)∇is

k+1 +(1−
, z −vs
k(cid:105).

k

where the ﬁrst equality holds due to the fact that xs
k = ws(vs
ws)(cid:101)xs−1); the second inequality holds due to Lemma 2 with z∗ = vs
The second equality follows from the facts that
(cid:69)
, ws(x∗ − vs
k)

, x∗ − vs
k

k+1 −xs

ws (cid:101)∇is

(cid:101)∇is

=

=

(cid:68)

(cid:69)

(cid:68)

(cid:68)

k

(xs

k

= ∇Fis

(cid:101)∇is
(cid:104)
holds due to the facts that E

k)−∇Fis

k

k

=

∇Fis

k

(xs

((cid:101)xs−1)+∇F ((cid:101)xs−1) = ∇Fis

k), wsx∗ +(1−ws)(cid:101)xs−1 −xs
(xs
k)−∇fis
(cid:105)
((cid:101)xs−1)

(cid:104)
= 0 and E

k

k

k

∇f ((cid:101)xs−1)−∇fis

(cid:101)∇is
(cid:68)

k

k

, wsx∗ +(1−ws)(cid:101)xs−1 − xs
(cid:68)

(cid:69)

k

(cid:69)

k

+

∇f ((cid:101)xs−1)−∇fis
((cid:101)xs−1)+∇f ((cid:101)xs−1), and E[∇Fis

k

((cid:101)xs−1), wsx∗ +(1−ws)(cid:101)xs−1 −xs

k

(cid:69)

,

(xs
k). The last equality
(cid:105)
((cid:101)xs−1)+∇f ((cid:101)xs−1), wsx∗ +(1−ws)(cid:101)xs−1−xs
k(cid:105)

k)] = ∇F (xs

= 0.

k

(cid:104)−∇fis

k

(cid:28)

∇F (xs

k), wsx∗ + (1−ws)(cid:101)xs−1 − xs

k +

∇F (xs

k), wsx∗ + (1−ws −

(cid:28)

=

(cid:18)

≤ F

wsx∗ + (1−ws −
(cid:18)

≤ wsF (x∗) +

1−ws −

1
α2
)(cid:101)xs−1 +
(cid:19)

)(cid:101)xs−1 +
(cid:19)
1
α2
F ((cid:101)xs−1) +

xs
k

1
α2

1
α2
1
α2

1
α2
1
α2

(cid:0)xs

k − (cid:101)xs−1(cid:1)
(cid:29)

k − xs
xs
k

(cid:29)

(43)

− F (xs
k)

F (xs

k) − F (xs
k)

where the ﬁrst inequality holds due to the fact that (cid:104)∇F (x1), x2 − x1(cid:105) ≤ F (x2)−F (x1), and the last inequality follows from

the convexity of F (·) and 1−ws −1/α2 ≥ 0. By combining the above two inequalities in (42) and (43), we have

E(cid:2)F (xs

k+1)(cid:3) ≤ wsF (x∗) + (1−ws)F ((cid:101)xs−1) +

α1Lw2
s
2

Subtracting F (x∗) from both sides of the above inequality, then

E(cid:2)(cid:107)vs

k − x∗(cid:107)2 − (cid:107)vs

k+1 − x∗(cid:107)2(cid:3) .

E(cid:2)F (xs

k+1) − F (x∗)(cid:3) ≤ (1−ws)[F ((cid:101)xs−1) − F (x∗)] +
(cid:80)m

Due to the convexity of F (·) and (cid:101)xs = 1

m

k=1xs

α1Lw2
s
2

E(cid:2)(cid:107)vs

k − x∗(cid:107)2 − (cid:107)vs

k+1 − x∗(cid:107)2(cid:3) .

k, then
(cid:32)

F ((cid:101)xs) = F

1
m

m
(cid:88)

(cid:33)

xs
k

≤

k=1

1
m

m
(cid:88)

k=1

F (xs

k).

Taking expectation over the history of is

1, . . . , is

m on the above inequality, and summing it up over k = 1, . . . , m at the s-th

epoch, we have

E[F ((cid:101)xs) − F (x∗)] ≤ (1−ws)(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

w2
s
2η0m

where η0 = 1/(α1L), e.g., η0 = 1/(3L). This completes the proof.

Proof of Theorem 5:

E(cid:2)(cid:107)vs

0 − x∗(cid:107)2 − (cid:107)vs

m − x∗(cid:107)2(cid:3)

Proof. Because ws = max{α, 2/(s+1)}, and α is sufﬁciently small, it is easy to verify that

(1 − ws+1)/w2

s+1 ≤ 1/w2
s.

(44)

Dividing both sides of the inequality in (39) by w2

s , we have

E[F ((cid:101)xs) − F (x∗)]/w2

s ≤

1−ws
w2
s

[F ((cid:101)xs−1) − F (x∗)] +

1
2mη0

for all s = 1, . . . , S. Using the above inequality, we obtain

E(cid:2)(cid:107)x∗ − vs

0(cid:107)2 − (cid:107)x∗ − vs

m(cid:107)2(cid:3),

E[F ((cid:101)xS) − F (x∗)]/w2

S ≤

[F ((cid:101)xS−1) − F (x∗)] +

(cid:104)
E

(cid:107)x∗ − vS

0 (cid:107)2 − (cid:107)x∗ − vS

m(cid:107)2(cid:105)

,

1−wS
w2
S
1−wS−1

w2

S−1

1
2mη0
1
2mη0

E[F ((cid:101)xS−1) − F (x∗)]/w2

S−1 ≤

[F ((cid:101)xS−2) − F (x∗)] +

(cid:104)
E

(cid:107)x∗ − vS−1

0

(cid:107)2 − (cid:107)x∗ − vS−1

m (cid:107)2(cid:105)

.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

33

Using the inequality in (44), e.g., (1 − wS)/w2

S ≤ 1/w2

S−1, and vs+1

0 = vs

m, e.g., vS

0 = vS−1

m , we have

(cid:104)

E

F ((cid:101)xS) − F (x∗)

(cid:105)

/w2

S ≤

1−wS−1

w2

S−1

[F ((cid:101)xS−2) − F (x∗)] +

(cid:104)
E

1
2mη0

(cid:107)x∗ − vS−1

0

(cid:107)2 − (cid:107)x∗ − vS

m(cid:107)2(cid:105)

,

(cid:104)
E

(cid:105)
F ((cid:101)xS) − F (x∗)

/w2

S ≤

1−w1
w2
1

[F ((cid:101)x0) − F (x∗)] +

(cid:104)
E

1
2mη0

(cid:107)x∗ − v1

0(cid:107)2 − (cid:107)x∗ − vS

m(cid:107)2(cid:105)

.

Due to the deﬁnition of w2

E[F ((cid:98)xS) − F (x∗)] ≤ E

0 = (cid:101)x0, then
S and v1
(cid:105)
(cid:104)
F ((cid:101)xS) − F (x∗)
4(1−w1)
1(S +1)2 [F ((cid:101)x0) − F (x∗)] +
w2
4(1−w1)
1(S +1)2 [F ((cid:101)x0) − F (x∗)] +
w2

≤

≤

2
mη0(S +1)2
2
mη0(S +1)2

(cid:104)

E

(cid:107)x∗ − v1

0(cid:107)2 − (cid:107)x∗ − vS

m(cid:107)2(cid:105)

(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3).

This completes the proof.

Theorem 5 shows that Algorithm 3 with Option II attains the optimal convergence rate O(1/T 2) for smooth and non-
strongly convex functions. Similarly, the convergence of Algorithm 3 with Option II can be guaranteed for non-smooth and
non-strongly convex functions. In addition, this theorem also requires that the learning rate η must be less than 1/(3L),
while that of SVRG should be less than 1/(4L). This means that our VR-SGD method can use much larger learning rates
than SVRG both in theory and in practice.

Equivalent Relationship

Here, we discuss the equivalent relationship between Algorithm 3 with Option I (i.e., vs
m) and
Algorithm 2 with Option I and a ﬁxed learning rate η0. Suppose ws = max{α, 2/(s+1)} and α is sufﬁciently small. Then
Algorithm 3 with Option I has the following update rule:

0 = xs

0 = xs

0 and xs+1

k+1 − xs
xs

k = ws
= ws

(cid:0)vs
(cid:0)vs
(cid:104)

k+1 − (cid:101)xs−1(cid:1) − ws
(cid:1) = −wsηs
k+1 − vs
k
(cid:105)
k) + ∇g(xs
(xs
k)

(cid:101)∇fis

k

.

(cid:0)vs
k − (cid:101)xs−1(cid:1)
(cid:104)
(xs
(cid:101)∇fis

k

= −η0

k) + ∇g(xs
k)

(cid:105)

In other words, xs
k − η0
same initial point (cid:101)x0 and xs+1
theoretically equivalent to Algorithm 2 with Option I.

k) + ∇g(xs
(xs
k)
m, together with vs

(cid:101)∇fis
0 = xs

k+1 = xs

k

(cid:104)

(cid:105)

. In addition, if Algorithms 2 and 3 have the same settings, e.g., the
0 for Algorithm 3. Therefore, Algorithm 3 with Option I is

0 = xs

APPENDIX H: CONVERGENCE ANALYSIS OF VR-SGD++

As shown in Section 5.5 of the main paper, VR-SGD++ is the variant of VR-SGD, and uses a general growing epoch size
strategy in early iterations (i.e., If ms < m, ms+1 = (cid:98)ρms(cid:99) with the factor ρ > 1. Otherwise, ms+1 = ms), as shown in
Algorithm 4. Note that the factor ρ in our VR-SGD++ method can be any constant satisfying ρ > 1, while the factor in
SVRG++ [44] only is equal to 2. In particular, unlike the doubling-epoch technique used in SVRG++ [44] (i.e., ms+1 = 2ms),
we gradually increase the epoch size in only the early iterations of the epochs when ms ≤ m. And all the early iterations can
viewed as the initialization steps of Algorithm 4. Similar to Algorithm 2, the convergence of Algorithm 4 (i.e., VR-SGD++)
can also be guaranteed.

APPENDIX I: MORE EXPERIMENTAL RESULTS

Experimental Setup

In this paper, we used ﬁve publicly available data sets in the experiments: Adult (also called a9a), Covtype, Epsilon, RCV1,
and MNIST, as listed in Table 3. For fair comparison, we implemented the state-of-the-art stochastic methods such as
SAGA [31], SVRG [1], Prox-SVRG [34], Catalyst [40], and Katyusha [48], and our VR-SGD method in C++ with a Matlab
interface, and conducted all the experiments on a PC with an Intel i5-4570 CPU and 16GB RAM. All the codes of VR-SGD
and related methods can be downloaded from https://github.com/jnhujnhu/VR-SGD. Furthermore, we also implemented
the accelerated deterministic methods such as AGD [20] and APG [22], and other stochastic methods such as SGD and
SVRG++ [44].

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

Algorithm 4 VR-SGD++
Input: The number of epochs S.
Initialize: x1

0 = (cid:101)x0, ρ > 1, m, m1 = (cid:98)n/4(cid:99), and {ηs}.

1: for s = 1, 2, . . . , S do

34

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

(cid:80)n

i=1∇fi((cid:101)xs−1);

(cid:101)µs = 1
for k = 0, 1, . . . , ms − 1 do

n

Pick is
k uniformly at random from [n];
(xs
(cid:101)∇fis
k) = ∇fis
xs
k+1 = xs

k) − ∇fis
(xs

((cid:101)xs−1) + (cid:101)µs;
k)] or xs

k) + ∇g(xs

k − ηs[ (cid:101)∇fis

(xs

k

k

k

k

k+1 = Proxg
η

(cid:16)

xs
k − ηs (cid:101)∇fis

k

(cid:17)
(xs
k)

;

end for
(cid:101)xs = 1
k=1xs
ms
if ms < m then

(cid:80)ms

k, xs+1

0 = xs

ms ;

ms+1 = (cid:98)ρms(cid:99);

else

ms+1 = ms;

end if

14: end for
Output: (cid:98)xS = (cid:101)xS, if F ((cid:101)xS) ≤ F ( 1

S

(cid:80)S

s=1 (cid:101)xs), and (cid:98)xS = 1

S

(cid:80)S

s=1 (cid:101)xs otherwise.

TABLE 3

Summary of data sets used for our experiments.

Data sets

Number of training samples, n

Number of dimensions, d

Adult

32,562

123

Covtype

Epsilon

581,012

400,000

54

Sparsity

Size

11.28%

22.12%

733K

50M

RCV1

20,242

47,236

0.16%

13M

MNIST

60,000

784

19.12%

12M

2,000

100%

11G

Comparison of VR-SGD with Options I and II

We ﬁrst compared the performance of VR-SGD with Option I and VR-SGD with Option II, as shown in Fig. 12. The results
show that the performance of VR-SGD with Option I is almost identical to that of VR-SGD with Option II.

Impact of Increasing Learning Rates
We compared the performance of Algorithm 2 with constant and varying learning rates for solving (cid:96)2-norm and (cid:96)1-norm
regularized logistic regression problems, as shown in Fig. 13. Note that the learning rate in Algorithm 2 is varied according
to the update formula in (13) (i.e., ηs = η0/ max{α, 2/(s + 1)}), and the initial learning rate η0 is set to the same value for
the two cases. We can observe that Algorithm 2 with varying learning rates converges much faster than Algorithm 2 with
ﬁxed learning rates in most cases, especially when the regularization parameter is relatively small, e.g., λ = 10−6 or 10−7.
This empirically veriﬁes the importance of increasing the learning rate.

More Results of Stochastic and Accelerated Deterministic Methods

In this part, we compared the well-known accelerated deterministic methods including AGD [20] and APG [22] with some
stochastic optimization methods, including SGD, one of stochastic variance reduced methods, SVRG [1], one of accelerated
variance reduction methods, Katyusha [48], and VR-SGD. Note that AGD and APG are used to solve smooth and non-
smooth objective functions, respectively. Fig. 14 shows the experimental results of SGD, AGD, SVRG, Katyusha, and VR-
SGD for solving (cid:96)2-norm regularized logistic regression problems with different regularization parameters. Furthermore,
we also reported their experimental results for solving (cid:96)1-norm regularized logistic regression problems in Fig. 15. All
the results show that the stochastic variance reduction methods, including SVRG, Katyusha, and VR-SGD, signiﬁcantly
outperform the accelerated deterministic methods, as well as SGD, for both strongly-convex and non-strongly convex cases,
which empirically veriﬁes the importance of variance reduction techniques.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

35

(a) Logistic regression

(b) Ridge regression

Fig. 12. Comparison of our VR-SGD method with Option I (called VR-SGD-I) and Option II (called VR-SGD-II) for solving (cid:96)2-norm (i.e., (λ/2)(cid:107)·(cid:107)2)
regularized logistic regression and ridge regression problems on the Covtype data set. In each plot, the vertical axis shows the objective value
minus the minimum, and the horizontal axis is the number of effective passes. Note that the blue lines stand for the results for λ = 10−4, while the
red lines correspond to the results for λ = 10−5 (best viewed in colors).

More Results for Different Choices of Snapshot and Starting Points

In this part, we presented more experimental results of the algorithms with the three choices (i.e., Options I, II and III in
Table I in the main paper) for snapshot and starting points for solving ridge regression and Lasso problems, as shown in
Figs. 16 and 17. All the results show that the setting Option III suggested in this paper (i.e., (cid:101)xs = 1
m)
is a better choice than Options I and II for stochastic variance reduction optimization.

k and xs+1

0 = xs

k=1xs

(cid:80)m

m

More Results for Common Stochastic Gradient and Prox-SG Updates

In this part, we reported more results of SVRG [1], Katyusha [48], VR-SGD and their proximal variants in Figs. 18 and 19.

More Results for SVRG++ and VR-SGD++

We also presented more experimental results of SVRG++ [44], VR-SGD++ (i.e., VR-SGD with reducing the number of
gradient calculations in early iterations, as shown in Algorithm 4), and VR-SGD, as shown in Fig. 20. The results show
that both VR-SGD++ and VR-SGD outperform SVRG++, which also means that if the epoch size is too large (due to the
doubling-epoch technique used in [44]), SVRG++ becomes slower and slower for later iterations.

More Results for Logistic Regression
Figs. 21, 22, and 23 show the experimental results for (cid:96)2-norm (i.e., λ2 = 0), (cid:96)1-norm (i.e., λ1 = 0), and elastic net (i.e., λ1 (cid:54)= 0
and λ2 (cid:54)= 0) regularized logistic regression with different regularization parameters.

More Results for ERM with Non-Convex Sigmoid Loss

Finally, we presented more experimental results (including training objective value and function suboptimality) in Figs. 24
and 25. Note that x∗ denotes the best solution obtained by running all those methods for a large number of iterations and
multiple random initializations.

024681010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )VR−SGD−IVR−SGD−IIVR−SGD−IVR−SGD−II024681010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )VR−SGD−IVR−SGD−IIVR−SGD−IVR−SGD−IIIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

36

(a) Results of (cid:96)2-norm regularized logistic regression with λ = 10−3, 10−4, 10−5, 10−6 (from left to right) on Adult.

(b) Results of (cid:96)2-norm regularized logistic regression with λ = 10−4, 10−5, 10−6, 10−7 (from left to right) on Covtype.

(c) Results of (cid:96)1-norm regularized logistic regression with λ = 10−3, 10−4, 10−5, 10−6 (from left to right) on Adult.

(d) Results of (cid:96)1-norm regularized logistic regression with λ = 10−3, 10−4, 10−5, 10−6 (from left to right) on Covtype.

Fig. 13. Comparison of our VR-SGD method (i.e., Algorithm 2) with ﬁxed and varying learning rates for solving (cid:96)2-norm (i.e., (λ/2)(cid:107) · (cid:107)2) and
(cid:96)1-norm (i.e., λ(cid:107) · (cid:107)1) regularized logistic regression problems.

05101510−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate02040608010010−610−410−2Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate024681010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate024681010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate024681010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rate02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Constant learning rateVarying learning rateIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

37

(a) Adult: λ = 10−5

(b) Covtype: λ = 10−5

(c) RCV1: λ = 10−4

(d) Epsilon: λ = 10−5

(e) Adult: λ = 10−6

(f) Covtype: λ = 10−6

(g) RCV1: λ = 10−5

(h) Epsilon: λ = 10−6

Fig. 14. Comparison of SGD, AGD [20], SVRG [1], Katyusha [48], and our VR-SGD method for solving (cid:96)2-norm regularized logistic regression

problems. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes

(top) or running time (bottom).

02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD00.511.5210−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD05101510−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD012345610−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD05010015020010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD0510152010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD024681010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGD05010015020010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAGDSVRGKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

38

(a) Adult: λ = 10−4

(b) Covtype: λ = 10−4

(c) RCV1: λ = 10−4

(d) Epsilon: λ = 10−4

(e) Adult: λ = 10−5

(f) Covtype: λ = 10−5

(g) RCV1: λ = 10−5

(h) Epsilon: λ = 10−5

Fig. 15. Comparison of SGD, APG [22], SVRG, Katyusha [48], and our VR-SGD method for solving (cid:96)1-norm regularized logistic regression problems.

In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes (top) or running

time (bottom).

02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD01020304050607010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05101520253010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD024681010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05010015020010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05010015020010−610−410−2Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD012345610−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD010203040506010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD024681010−610−410−2Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGD05010015020025030010−1210−810−4Running time (sec)F(xs ) − F(x* )SGDAPGSVRGKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

39

(a) Adult: λ = 10−4 (left) and λ = 10−5 (right)

(b) MNIST: λ = 10−5 (left) and λ = 10−6 (right)

(c) RCV1: λ = 10−4 (left) and λ = 10−5 (right)

(d) Epsilon: λ = 10−5 (left) and λ = 10−6 (right)

Fig. 16. Comparison of the stochastic algorithms with Options I, II, and III for solving ridge regression problems with the regularizer (λ/2)(cid:107) · (cid:107)2. In

each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes.

(a) Adult: λ = 10−4 (left) and λ = 10−5 (right)

(b) MNIST: λ = 10−4 (left) and λ = 10−5 (right)

(c) RCV1: λ = 10−4 (left) and λ = 10−5 (right)

(d) Epsilon: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 17. Comparison of the stochastic algorithms with Options I, II, and III for solving Lasso problems with the regularizer λ(cid:107) · (cid:107)1. In each plot, the

vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes.

01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III01020304050607010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III01020304050607010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015020010−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption IIIIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

40

(a) λ = 10−3

(b) λ = 10−4

(c) λ = 10−5

(d) λ = 10−6

(e) λ = 10−7

(f) λ = 0

Fig. 18. Comparison of SVRG [1], Katyusha [48], our VR-SGD method and their proximal versions for solving ridge regression problems with

different regularization parameters on Adult. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is

the number of effective passes (top) or running time (bottom).

0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.20.40.60.8110−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.20.40.60.8110−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.20.40.60.8110−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II05010015010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II05010015010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II00.511.510−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−IIIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

41

(a) λ = 10−3

(b) λ = 10−4

(c) λ = 10−5

(d) λ = 10−6

(e) λ = 10−7

(f) λ = 0

Fig. 19. Comparison of SVRG [1], Katyusha [48], our VR-SGD method and their proximal versions for solving ridge regression problems with

different regularization parameters on Covtype. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis

is the number of effective passes (top) or running time (bottom).

05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II01234510−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II024681010−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II024681010−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010020030040050010−210−1Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0102030405010−1210−810−4Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II02040608010−210−1Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−IIIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

42

(a) Adult: λ = 10−4 (left) and λ = 10−5 (right)

(b) MNIST: λ = 10−4 (left) and λ = 10−5 (right)

(c) Covtype: λ = 10−5 (left) and λ = 10−6 (right)

(d) RCV1: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 20. Comparison of SVRG++ [44], VR-SGD and VR-SGD++ for solving logistic regression problems with the regularizer (λ/2)(cid:107) · (cid:107)2. In each

plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes.

0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++024681010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SVRG++VR−SGDVR−SGD++IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

43

(a) Adult: λ1 = 10−4

(b) Covtype: λ1 = 10−4

(c) RCV1: λ1 = 10−4

(d) Epsilon: λ1 = 10−5

(e) Adult: λ1 = 10−5

(f) Covtype: λ1 = 10−5

(g) RCV1: λ1 = 10−5

(h) Epsilon: λ1 = 10−6

(i) Adult: λ1 = 10−6

(j) Covtype: λ1 = 10−6

(k) RCV1: λ1 = 10−6

(l) Epsilon: λ1 = 10−7

Fig. 21. Comparison of SAGA [31], SVRG [1], Prox-SVRG [34], Catalyst [40], Katyusha [48], and our VR-SGD method for solving (cid:96)2-norm

regularized logistic regression problems (i.e., λ2 = 0). In each plot, the vertical axis shows the objective value minus the minimum, and the

horizontal axis is the number of effective passes (top) or running time (bottom).

01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.20.40.60.8110−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0246810−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0246810−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025030010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01234510−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0510152010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05101520253010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040050010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

44

(a) λ2 = 10−4

(b) λ2 = 10−4

(c) λ2 = 10−5

(d) λ2 = 5∗10−5

Fig. 22. Comparison of SAGA [31], SVRG, Prox-SVRG [34], Catalyst [40], Katyusha [48], and our VR-SGD method for (cid:96)1-norm regularized logistic

regression problems (i.e., λ1 = 0) on the four data sets: Adult (the ﬁrst column), Covtype (the sconced column), Epsilon (the third column), and

RCV1 (the last column). In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of

(e) λ2 = 10−6

(f) λ2 = 10−5

effective passes (top) or running time (bottom).

010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.511.5210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0510152010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD024681010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025030010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0123456710−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD01020304010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02004006008001000120010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040050010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040050060010−310−2Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025030010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020025030010−710−510−3Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05101510−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020010−310−2Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05001000150010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05101510−710−510−3Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

45

(a) λ1 = 10−5 and λ2 = 10−4

(b) λ1 = 10−4 and λ2 = 10−5

(c) λ1 = 10−5 and λ2 = 10−5

(d) λ1 = 10−5 and λ2 = 10−4

(e) λ1 = 10−6 and λ2 = 10−5

(f) λ1 = 10−5 and λ2 = 10−5

Fig. 23. Comparison of SAGA [31], SVRG, Prox-SVRG [34], Catalyst [40], Katyusha [48], and our VR-SGD method for solving elastic net regularized

logistic regression problems on the four data sets: Adult (the ﬁrst column), Covtype (the sconced column), Epsilon (the third column), and RCV1

(the last column). In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective

passes (top) or running time (bottom).

0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.511.5210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0123456710−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040050010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0510152010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD012310−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0246810−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010020030040050060010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD010203040506010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD00.511.522.5310−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD0510152010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD020040060080010−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGD02468101210−1210−810−4Running time (sec)F(xs ) − F(x* )SAGASVRGProx−SVRGCatalystKatyushaVR−SGDIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

46

(a) Adult

(b) MNIST

(c) Covtype

(d) RCV1

Fig. 24. Comparison of SAGA [58], SVRG [4], SVRG++ [44], and our VR-SGD method for solving non-convex ERM problems with sigmoid loss on
the four data sets: λ = 10−4 (the ﬁrst row), λ = 10−5 (the sconced row), and λ = 10−6 (the last row). Note that x∗ denotes the best solution
obtained by running all those methods for a large number of iterations and multiple random initializations.

(a) Adult

(b) MNIST

(c) Covtype

(d) RCV1

Fig. 25. Comparison of SAGA [58], SVRG [4], SVRG++ [44], and our VR-SGD method for solving non-convex ERM problems with sigmoid loss on
the four data sets: λ = 10−5 (the ﬁrst row), λ = 10−6 (the sconced row), and λ = 10−7 (the last row).

05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD01020304010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD0102030405010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05101520253010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05010015020010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD02040608010010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD02040608010−1210−810−4Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD05010015020010−410−2Gradient evaluations / nF(xs ) − F(x* )SAGASVRGSVRG++VR−SGD010203040500.1690.16950.170.17050.171Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD010203040500.02110.02120.0213Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD010203040500.430.4310.4320.433Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.0960.0980.10.102Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD010203040500.1550.160.1650.17Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.0120.0130.014Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.4250.4260.4270.4280.429Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0501001502000.030.040.050.06Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0501001502000.150.1550.160.165Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0501001502000.0080.0090.010.0110.012Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0204060801000.4150.420.4250.43Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD0501001502000.010.020.030.040.05Gradient evaluations / nObjective valueSAGASVRGSVRG++VR−SGD