7
1
0
2

p
e
S
6
1

]

G
L
.
s
c
[

3
v
0
9
8
4
0
.
3
0
7
1
:
v
i
X
r
a

Riemannian stochastic quasi-Newton algorithm with
variance reduction and its convergence analysis

Hiroyuki Kasai∗

Hiroyuki Sato†

Bamdev Mishra‡

September 19, 2017

Abstract

Stochastic variance reduction algorithms have recently become popular for minimizing
the average of a large, but ﬁnite number of loss functions. The present paper proposes
a Riemannian stochastic quasi-Newton algorithm with variance reduction (R-SQN-VR).
The key challenges of averaging, adding, and subtracting multiple gradients are addressed
with notions of retraction and vector transport. We present convergence analyses of R-
SQN-VR on both non-convex and retraction-convex functions under retraction and vector
transport operators. The proposed algorithm is evaluated on the Karcher mean compu-
tation on the symmetric positive-deﬁnite manifold and the low-rank matrix completion
on the Grassmann manifold. In all cases, the proposed algorithm outperforms the state-
of-the-art Riemannian batch and stochastic gradient algorithms.

1 Introduction

Let f : M → R be a smooth real-valued function on a Riemannian manifold M [1]. The prob-
lem under consideration in the present paper is the minimization of the expected risk of f for
a given model variable w ∈ M taken with respect to the distribution of z, i.e., minw∈M f (w),
where f (w) = Ez[f (w; z)] = (cid:82) f (w; z)dP (z) and z is a random seed representing a single
sample or set of samples. When given a set of realizations {z[n]}N
n=1 of z, we deﬁne the loss
incurred by the parameter vector w with respect to the n-th sample as fn(w) := f (w; z[n]),
and then the empirical risk is deﬁned as the average of the sample losses:

(cid:40)

min
w∈M

f (w) :=

(cid:41)

fn(w)

,

1
N

N
(cid:88)

n=1

(1)

where N is the total number of the elements. This problem has many applications that
include, to name a few, principal component analysis (PCA) and the subspace tracking prob-
lem [2] on the Grassmann manifold. The low-rank matrix/tensor completion problem is a
promising example of the manifold of ﬁxed-rank matrices/tensors [3, 4]. The linear regression
problem is also deﬁned on the manifold of the ﬁxed-rank matrices [5].

∗Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan

(kasai@is.uec.ac.jp).

†Department of Information and Computer Technology, Tokyo University of Science, Tokyo, Japan

(hsato@rs.tus.ac.jp).

‡Core Machine Learning Team, Amazon.com, Bangalore, India (bamdevm@amazon.com.)

1

 
 
 
 
 
 
(cid:80)N

Riemannian gradient descent requires the Riemannian full gradient estimation, i.e., gradf (w) =
n=1 gradfn(w), for every iteration, where gradfn(w) is the Riemannian stochastic gradient
of fn(w) on the Riemannian manifold M for n-th sample. This estimation is computationally
heavy when N is extremely large. A popular alternative is Riemannian stochastic gradient
descent (R-SGD) that extends stochastic gradient descent (SGD) in the Euclidean space [6].
Because this uses only one gradfn(w), the complexity per iteration is independent of N . How-
ever, similarly to SGD [7], R-SGD suﬀers from a slow convergence due to a decaying step-size
sequence. Variance reduction (VR) methods have been proposed recently to accelerate the
convergence of SGD in the Euclidean space [8–12]. One distinguished feature is to calcu-
late a full gradient estimation periodically, and to re-use it to reduce the variance of noisy
stochastic gradient. However, because all previously described algorithms are ﬁrst-order algo-
rithms, their convergence speed can be slow because of their poor curvature approximations
in ill-conditioned problems as seen in Section 4. One promising approach is second-order
algorithms such as stochastic quasi-Newton (QN) methods using Hessian evaluations [13–16].
They achieve faster convergence by exploiting curvature information of the objective function
f . Furthermore, addressing these two acceleration techniques, [17] and [18] propose a hybrid
algorithm of the stochastic QN method accompanied with the VR method.

Examining the Riemannian manifolds again, many challenges on the QN method have
been addressed in deterministic settings [19–21]. The VR method in the Euclidean space has
also been extended to Riemannian manifolds, so-called R-SVRG [22, 23]. Nevertheless, the
second-order stochastic algorithm with the VR method has not been explored thoroughly for
the problem (1). To this end, we propose a Riemannian stochastic QN method based on
L-BFGS and the VR method.

Our contributions are four-fold; (i) we propose a novel (and to the best of our knowledge,
the ﬁrst) Riemannian limited-memory QN algorithm with a VR method. (ii) Our convergence
analysis deals with both non-convex and (strongly) retraction-convex functions. In this paper,
f is said to be strongly retraction-convex when f is (strongly) convex along a curve on M
deﬁned by a retraction R (Assumption 3) while the other functions are called as non-convex
functions. (iv) The proposed algorithm and its analyses are considered under computationally
eﬃcient retraction and vector transport operations instead of the more restrictive exponential
mapping and parallel translation operations. This is more challenging than R-SVRG [23], but
gives us a big advantage other than computational eﬃciency, i.e., wider kinds of applicable
manifolds. For example, while [23] cannot be applied to the Stiefel and ﬁxed-rank manifolds
because these manifolds do not have closed form expressions for parallel translation, our
analyses and algorithm can be directly applied to them.

The speciﬁc features of the algorithms are two-fold; (i) we update the curvature pair of
the QN method every outer loop by exploiting full gradient estimations in the VR method,
and thereby capture more precise and stabler curvature information. This avoids additional
sweeping of samples required in the Euclidean stochastic QN [16], additional gradient estima-
tions required in the Euclidean online BFGS (oBFGS) [13, 14, 24], or additional sub-sampling
of Hessian [16, 17]. (ii) Compared with a simple Riemannian extension of the QN method,
a noteworthy advantage of its combination with the VR method is that, as revealed below,
frequent transportations of curvature information between diﬀerent tangent spaces, which
are inextricable in such a simple Riemannian extension, can be drastically reduced. This is
a special beneﬁt of the Riemannian hybrid algorithm, which does not exist in the Euclidean
case [17,18]. More speciﬁcally, the calculations of curvature information and the second-order
modiﬁed Riemannian stochastic gradient are performed uniformly on the tangent space of the

2

outer loop.

The paper is organized as follows. Section 2 presents details of our proposed R-SQN-VR.
Section 3 presents the convergence analyses. In Section 4, numerical comparisons with R-SGD
and R-SVRG on two problems are provided with results suggesting the superior performances
of R-SQN-VR. The proposed R-SQN-VR is implemented in the Matlab toolbox Manopt [25].
The concrete proofs of theorems and additional experiments are provided as supplementary
material.

2 Riemannian stochastic quasi-Newton algorithm with vari-

ance reduction (R-SQN-VR)

We assume that the manifold M is endowed with a Riemannian metric structure, i.e., a
smooth inner product (cid:104)·, ·(cid:105)w of tangent vectors is associated with the tangent space TwM for
all w ∈ M [1]. The norm (cid:107)·(cid:107)w of a tangent vector is the norm associated with the Riemannian
metric. The metric structure allows a systematic framework for optimization over manifolds.
Conceptually, the constrained optimization problem (1) is translated into an unconstrained
problem over M.

2.1 R-SGD and R-SVRG

R-SGD: Given a starting point w0 ∈ M, R-SGD produces a sequence {wt} in M that
converges to a ﬁrst-order critical point of (1). Speciﬁcally, it updates w as

wt+1 = Rwt(−αtgradfn(wt, zt)),

where αt is the step-size, and where gradfn(wt, zt) is a Riemannian stochastic gradient, which
is a tangent vector at wt ∈ M. gradfn(wt, zt) represents an unbiased estimator of the Rie-
mannian full gradient gradf (wt), and the expectation of gradfn(wt, zt) over the choices of zt
is gradf (wt), i.e., Ezt[gradfn(wt, zt)] = gradf (wt). The update moves from wt in the direc-
tion −gradfn(wt, zt) with a step-size αt while remaining on M. This mapping, denoted as
Rw : TwM → M : ζw (cid:55)→ Rw(ζw), is called retraction at w, which maps the tangent bun-
dle TwM onto M with a local rigidity condition that preserves gradients at w. Exponential
mapping Exp is an instance of the retraction.
R-SVRG: R-SVRG has double loops where a k-th outer loop, called epoch, has mk inner
iterations. R-SVRG keeps ˜wk ∈ M after mk−1 inner iterations of (k − 1)-th epoch, and
computes the full Riemannian gradient gradf ( ˜wk) only for this stored ˜wk. It also computes
t -th sample
the Riemannian stochastic gradient gradfik
(wk
for each t-th inner iteration of k-th epoch at wk
t )
using both gradf ( ˜wk) and gradfik
( ˜wk). Because they belong to diﬀerent tangent spaces, a
simple addition of them is not well-deﬁned because Riemannian manifolds are not vector
spaces. Therefore, after gradfik
is
set as

( ˜wk) and gradf ( ˜wk) are transported to Twk

t -th sample. Then, picking ik

t , i.e., by modifying gradfik

t , we calculate ξk

M by T˜ηk

( ˜wk) for ik

, ξk
t

t

t

t

t

t

t

ξk
t = gradfik

t

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk) − gradf ( ˜wk)),

t ) = wk
where T represents vector transport from ˜wk to wk
t .
The vector transport T : T M ⊕ T M → T M, (ηw, ξw) (cid:55)→ Tηw ξw is associated with retraction

t ∈ T ˜wkM satisﬁes R ˜wk (˜ηk

t , and ˜ηk

3

Algorithm 1 Riemannian stochastic quasi-Newton with variance reduction (R-SQN-VR).
Require: Update frequency mk, step-size αk

t > 0, memory size L, number of epochs K, and

cautious update threshold (cid:15).

1: Initialize ˜w0, and calculate the Riemannian full gradient gradf ( ˜w0).
2: for k = 0, . . . , K − 1 do
3:

0 = ˜wk.

Store wk
for t = 0, 1, 2, . . . , mk − 1 do

Choose ik
Calculate the tangent vector ˜ηk
if k > 1 then

t ∈ {1, . . . , N } uniformly at random.
t from ˜wk to wk

t by ˜ηk

t = R−1

˜wk (wk
t ).

Transport the stochastic gradient gradfik
t as ˜ξk
Calculate ˜ξk
t = (T˜ηk
˜ξk
Calculate ˜Hk
t , transport ˜Hk
t
t
t+1 from wk
Update wk

)−1gradfik
(wk
˜ξk
t back to Twk
t Hk
t+1 = Rwk

t as wk

(−αk

t

t

t

t

t

M by T˜ηk
t ξk

t ).

t

(wk

t ) to T ˜wkM by (T˜ηk

)−1gradfik

(wk

t ).

t

t

t ) − (gradfik

t

( ˜wk) − gradf ( ˜wk)).

˜Hk
t

˜ξk
t , and obtain Hk

t ξk
t .

else

Calculate ξk
Update wk

t as ξk
t+1 from wk

t = gradfik
t as wk

t

(wk

t ) − T˜ηk

t+1 = Rwk

t

t

(gradfik
t ξk
t ).

(−αk

t

( ˜wk) − gradf ( ˜wk)).

1 , . . . , wk
mk

) (or ˜wk+1 = wk
t

for randomly chosen t ∈

end if
end for

16:
17: Option I: ˜wk+1 = gmk (wk

{1, . . . , mk}).
18: Option II: ˜wk+1 = wk
19:

.

mk

Calculate the Riemannian full gradient gradf ( ˜wk+1).
Calculate the tangent vector ηk from ˜wk to ˜wk+1 by ηk = R−1
Compute sk+1
k
(cid:107)ηk(cid:107) ˜wk /(cid:107)TRηk
, sk+1
if (cid:104)yk+1
k
k
Discard pair (sk

ηk(cid:107) ˜wk .
(cid:105) ˜wk+1 ≥ (cid:15)(cid:107)sk+1
k−L, yk

k−L) when k > L, and store pair (sk+1

= Tηk ηk, and yk+1

(cid:107)2
˜wk+1 then

= κ−1

k

k

k

, yk+1
k

).

˜wk ( ˜wk+1).
k gradf ( ˜wk+1) − Tηk gradf ( ˜wk) where κk =

j , yk

j )}k−1

j=k−τ +1 ∈ T ˜wkM to {(sk+1

j

, yk+1
j

)}k−1

j=k−τ +1 ∈ T ˜wk+1M by Tηk .

end if
Transport {(sk

25:
26: end for
27: Option III: output wsol = ˜wK
28: Option IV: output wsol = wk

t for randomly chosen t ∈ {1, . . . , mk} and k ∈ {1, . . . , K}.

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

20:

21:

22:

23:

24:

4

R and all ξw, ζw ∈ TwM. It holds that (i) Tηw ξw ∈ TR(ηw)M, (ii) T0w ξw = ξw, and (iii) Tηw
is a linear map. Parallel translation P is an instance of the vector transport. Consequently,
t ξk
the ﬁnal update is deﬁned as wk

t+1 = Rwk

(−αk

t ).

t

2.2 Proposed R-SQN-VR

We propose a Riemannian stochastic QN method accompanied with a VR method (R-SQN-
VR). A straightforward extension is to update the modiﬁed stochastic gradient ξk
t by premul-
tiplying a linear inverse Hessian approximation operator Hk

t at wk

t as

wk

t+1 = Rwk

t

(−αk

t Hk

t ξk

t ),

t

t

k

◦ ˜Hk ◦ (T˜ηk

t := T˜ηk

)−1 by denoting the inverse Hessian approximation at ˜wk simply
t should be positive
It is noteworthy
t throughout the

where Hk
as ˜Hk. Here, T is an isometric vector transport explained in Section 3. Hk
deﬁnite, i.e., Hk
t (cid:31) 0 and is close to the Hessian of f , i.e., Hessf (wk
t ).
that ˜Hk is calculated only every outer epoch, and remains to be used for Hk
corresponding k-th epoch.
): This paper particularly addresses the operator ˜Hk used in L-
Curvature pair (sk+1
, yk+1
k
BFGS intended for a large-scale data. Thus, let sk+1
be the variable variation and
the gradient variation at T ˜wk+1M, respectively, where the superscript expresses explicitly that
they belong to T ˜wk+1M. It should be noted that the curvature pair (sk+1
) is calculated
at the new T ˜wk+1M just after k-th epoch ﬁnished. Furthermore, after the epoch index k is
incremented, the curvature pair must be used only at T ˜wkM because the calculation of ˜Hk is
performed only at T ˜wkM.

The variable variation sk+1

is calculated from the diﬀerence between ˜wk+1 and ˜wk. This
is represented by the tangent vector ηk from ˜wk to ˜wk+1, which is calculated using the inverse
of the retraction R−1
˜wk ( ˜wk+1). Since ηk belongs to the T ˜wkM, transporting this onto T ˜wk+1M
yields

and yk+1

, yk+1
k

k

k

k

k

sk+1
k

= Tηk ηk (= Tηk R−1

˜wk ( ˜wk+1)).

(2)

The gradient variation yk+1
gradf ( ˜wk+1) ∈ T ˜wk+1M and the previous, but transported Tηk gradf ( ˜wk) ∈ T ˜wkM [21] as

is calculated from the diﬀerence between the new full gradient

k

yk+1
k

= κ−1

k gradf ( ˜wk+1) − Tηk gradf ( ˜wk),

(3)

,

ρk = 1/(cid:104)yk, sk(cid:105), ˇV k = id − ρkyks(cid:91)

where κk > 0 is explained in Section 3.
Inverse Hessian approximation operator ˜Hk: ˜Hk is calculated using the past curvature
k, where ˇHk = Tηk ◦
pairs. More speciﬁcally, ˜Hk is updated as ˜Hk+1 = ( ˇV k)(cid:91) ˇHk ˇV k + ρksks(cid:91)
˜Hk ◦ T −1
k with identity mapping id [21]. Therein, a(cid:91)
ηk
denotes the ﬂat of a ∈ TwM, i.e., a(cid:91) : TwM → R : v → (cid:104)a, v(cid:105)w. Thus, ˜Hk depends on
˜Hk−1 and (sk−1, yk−1), and similarly ˜Hk−1 depends on ˜Hk−2 and (sk−2, yk−2). Proceeding
recursively, ˜Hk is a function of the initial ˜H0 and all previous k curvature pairs {(sj, yj)}k−1
j=0 .
Meanwhile, L-BFGS restricts use to the most recent L pairs {(sj, yj)}k−1
j=k−L since (sj, yj) with
j < k −L are likely to have little curvature information. Based on this idea, L-BFGS performs
L updates by the initial ˜H0. We use the k pairs {(sj, yj)}k−1
Now, we consider the ﬁnal calculation of ˜Hk used for Hk
t

in the inner iterations of k-th
outer epoch using the L most recent curvature pairs. Here, since this calculation is executed

j=0 when k < L.

5

at T ˜wkM and a Riemannian manifold is in general not a vector space, all the L curvature
pairs must be located at T ˜wkM. To this end, just after the curvature pair is calculated
in (2) and (3), the past (L − 1) pairs of {(sk
j=k−L+1 ∈ T ˜wkM are transported into
T ˜wk+1M by the same vector transport Tηk used when calculating sk+1
. It should be
emphasized that this transport is necessary only for every outer epoch instead of every inner
loop, and results in drastic reduction of computational complexity in comparison with the
straightforward extension of the Euclidean stochastic L-BFGS [24] into the manifold setting.
Consequently, the update is deﬁned as

and yk+1

j )}k−1

j , yk

k

k

˜Hk = (( ˇV k

k−1)(cid:91) · · · ( ˇV k
k−1)(cid:91)sk

k−L)(cid:91)) ˇHk
k−2(sk

0( ˇV k
k−2)(cid:91)( ˇV k

+ ρk−2( ˇV k

k−L · · · ˇV k

k−1)
k−1) + ρk−1sk

k−1(sk

k−1)(cid:91),

j (sk

k−1, yk

0 = (cid:104)sk

j )(cid:91), and ˇHk

0 is the initial inverse Hessian approximation.

where ˇV k
j = id − ρjyk
id is the
0 is not necessarily ˇHk−L, and because it is any positive deﬁnite
identity mapping. Because ˇHk
self-adjoint operator, we use ˇHk
k−1(cid:105) ˜wk id similar to the Euclidean
case. The practical update of ˜Hk uses two-loop recursion algorithm [26] in Algorithm A.1 of
the supplementary material.
Cautious update: Euclidean L-BFGS fails on non-convex problems because the Hessian
approximation has eigenvalues that are away from zero and are not uniformly bounded above.
To circumvent this issue, cautious update has been proposed in the Euclidean space [27]. By
following this, we skip the update of the curvature pair when the following condition is not
satisﬁed;

k−1(cid:105) ˜wk /(cid:104)yk

k−1, yk

(cid:104)yk+1
k

, sk+1
k

(cid:105) ˜wk+1 ≥ (cid:15)(cid:107)sk+1

k

(cid:107)2
˜wk+1,

(4)

t

( ˜wk) at T ˜wkM into Twk

where (cid:15) > 0 is a predeﬁned constant parameter. According to this update, the positive
deﬁniteness of ˜Hk is guaranteed as far as ˜Hk−1 is positive deﬁnite.
Second-order modiﬁed stochastic gradient Hk
gradfik
strategy, we must also transport L pairs of {(sk
M
at every inner iteration. Addressing this problem and the fact that both the full gradient
(wk
and the curvature pairs belong to the same tangent space T ˜wkM, we transport gradfik
t )
M into T ˜wkM, and complete all the calculations on T ˜wkM. More speciﬁcally, after
from Twk
t ) from wk
t )), the
transporting gradfik
modiﬁed stochastic gradient ˜ξk

t ξk
M to add them to gradfik
j )}k−1

t : R-SVRG transports gradf ( ˜wk) and
M. If we follow the same

j=k−L ∈ T ˜wkM into the current Twk

t to ˜wk using ˜ηk

)−1gradfik

t ∈ T ˜wkM is computed as

t (= R−1

t ) as (T˜ηk

t ) at Twk

˜wk (wk

j , yk

(wk

(wk

(wk

t

t

t

t

t

t

t

t

t

˜ξk
t = (T˜ηk

t

)−1gradfik

t

(wk

t ) − (gradfik

t

( ˜wk) − gradf ( ˜wk)).

t

t+1 = Rwk

M by transporting ˜Hk
t
(−αk

After calculating ˜Hk
t
Twk
wk
t ξk
a descent direction, E
deﬁniteness of Hk
t gradf (wk
−Hk

t Hk

t ).

t

˜ξk
t to Twk

M by T˜ηk
t ). It should be noted that, although −ξk

˜ξk
t ∈ T ˜wkM using the two-loop recursion algorithm, we obtain Hk
t ξk
t ∈
t+1 from wk
t as
t is not generally guaranteed as
t ) is a descent direction. Furthermore, the positive
t ] =

is an average descent direction due to E

t ] = −gradf (wk
t ξk
t

˜ξk
t . Finally, we update wk

[−ξk
t yields that −Hk

[−Hk

t ξk

˜Hk
t

ik
t

t

t

ik
t

6

3 Convergence analysis

This section presents convergence analyses on both non-convex and retraction-convex func-
tions under retraction and vector transport operations.. The concrete proofs are in the sup-
plementary ﬁle.

Assumption 1. We assume below [21];

(1.1) The objective function f and its components f1, . . . , fN are twice continuously dif-

ferentiable.

(1.2) For a sequence {wk

t } generated by Algorithm 1, there exists a compact and connected
set K ⊂ M such that wk
t ∈ K for all k, t ≥ 0. Also, for each k ≥ 1, there exists a totally
retractive neighborhood Θk of ˜wk such that wk
t stays in Θk for any t ≥ 0, where the ρ-totally
retractive neighborhood Θ of w is a set such that for all z ∈ Θ, Θ ⊂ Rz(B(0z, ρ)), and
Rz(·) is a diﬀeomorphism on B(0z, ρ), which is the ball in TwM with center 0z and radius ρ,
where 0z is the zero vector in TzM. Furthermore, suppose that there exists I > 0 such that
inf k≥1{supz∈Θk

˜wk (z)(cid:107) ˜wk } ≥ I.

(1.3) The sequence {wk

t } continuously remains in ρ-totally retractive neighborhood Θ of
critical point w∗ and f is retraction-smooth with respect to retraction R in Θ. Here, f is said
to be retraction-smooth in Θ if f (Rw(tηw)) for all w ∈ M, i.e., there exists a constant 0 < Λ
such that d2f (Rw(tηw))
≤ Λ, for all w ∈ Θ, all (cid:107)ηw(cid:107)w = 1, and all t such that Rw(τ ηw) ∈ Θ
dt2
for all τ ∈ [0, t].

(cid:107)R−1

(1.4) The vector transport T is isometric on M.

It satisﬁes (cid:104)Tξw ηw, Tξw ζw(cid:105)Rw(ξw) =

(cid:104)ηw, ζw(cid:105)w for any w ∈ M and ξw, ηw, ζw ∈ TwM.

(1.5) There exists a constant c0 such that the vector transport T satisﬁes the following
conditions for all w, z ∈ U, which is some neighborhood of an arbitrary point ¯w ∈ M: (cid:107)Tηw −
TRηw (cid:107) ≤ c0(cid:107)ηw(cid:107)w, (cid:107)T −1
(cid:107) ≤ c0(cid:107)ηw(cid:107)w, where TR denotes the diﬀerentiated retraction,
i.e., TRζw

ξw = DRw(ζw)[ξw] with ξw ∈ TwM, and ηw = R−1

ηw − T −1
Rηw

(1.6) Riemannian stochastic gradient is bounded as E

w (z).
[(cid:107)gradfik

t

ik
t

(wk

t )(cid:107)2
wk
t

] < C2 as [14–16].

Essential inequalities. We brieﬂy summarize essential inequalities. They are detailed in
the supplementary material. For all w, z ∈ U, which is a neighborhood of ¯w, the diﬀerence
between the parallel translation and the vector transport is given with a constant θ as (Lemma
C.14)

(cid:107)Tηξ − Pηξ(cid:107)z ≤ θ(cid:107)ξ(cid:107)w(cid:107)η(cid:107)w,

(5)

where ξ, η ∈ TwM and Rw(η) = z. Similarly, as for the diﬀerence between the exponential
mapping and the retraction, there exist τ1 > 0, τ2 > 0 for all w ∈ U and all small length of
ξ ∈ TwM such that (Lemma C.15)

τ1dist(w, Rw(ξ)) ≤ (cid:107)ξ(cid:107)w ≤ τ2dist(w, Rw(ξ)).

Then, the variance of ξk
t
] ≤ 4(β2 + τ 2

[(cid:107)ξk

E

ik
t

t (cid:107)2
wk
t

is upper bounded by (Lemma D.9)

2 C2θ2)(7(dist(wk

t , w∗))2 + 4(dist( ˜wk, w∗))2),

(6)

(7)

where C is the constant of Assumption 1, β is a Lipschitz constant, and θ is the constant in
(5). Finally, there exist 0 < γ < Γ such that (Proposition C.7 for non-convex functions and
Proposition D.6 for retraction-convex functions)

γid (cid:22) Hk

t (cid:22) Γid,

7

(8)

where the A (cid:22) B with A, B ∈ Rn×n means that B − A is positive semideﬁnite.

Now, we ﬁrst present a global convergence analysis to a critical point starting from any
initialization point, which is common in a non-convex setting with additional but mild as-
sumptions;

Assumption 2. We assume that f is bounded below by a scalar finf , and a decaying step-size
sequence {αk
t )2 < ∞. Additionally, since Θ is compact, all
continuous functions on Θ can be bounded. Therefore, there exists S > 0 such that for all
w ∈ Θ and n ∈ N , we have (cid:107)gradf (w)(cid:107)w ≤ S and (cid:107)gradfn(w)(cid:107)w ≤ S.

t = ∞ and (cid:80)(αk

t } satisﬁes (cid:80) αk

Theorem 3.1 (Global convergence analysis on non-convex functions). Let M be a Rieman-
nian manifold and w∗ ∈ M be a non-degenerate local minimizer of f . Consider Algorithm 1
and suppose Assumptions 1 and 2, and that the mapping w (cid:55)→ (cid:107)gradf (w)(cid:107)2
w has the positive
real number that the largest eigenvalue of its Riemannian Hessian is bounded for all w ∈ M.
Then, we have limk→∞ E[(cid:107)gradf (wk

] = 0.

t )(cid:107)2
wk
t

We next present a global convergence rate analysis. This requires an strict selection of a
ﬁxed step size satisfying the condition below, but, instead, provides a convergence rate under
it.

Theorem 3.2 (Global convergence rate analysis on non-convex functions). Let M be a Rie-
mannian manifold and w∗ ∈ M be a non-degenerate local minimizer of f . Consider Algorithm
1 with Option II and IV, and suppose Assumption 1. Let the constants θ in (5), τ1 and τ2
in (6), and β, and C in (7). Λ is the constant Assumption 1.3, and γnc and Γnc are the
2 C2θ2Γncτ1
N a1/2

2 C2θ2N a1 Γncζa2
where 0 < a1 < 1, and 0 < a2 < 2. Given suﬃciently small µ0 ∈ (0, 1), suppose that (cid:37) > 0

constants γ and Γ in (8). Set ν =

ζ 1−a2 and αk

t = α =

β2+τ 2

β2+τ 2

µ0τ1

√

√

,

is chosen such that

√

2 C2θ2

β2+τ 2
ΛΓnc

γnc

(cid:16)

1 − (cid:37)Γnc
µ0γncτ1

(cid:17)

> 2µ0(e−1)
ζ2−a2 τ1

+ µ0τ1

N a1 ζa2 +

4µ2

0(e−1)
3a1
2 ζa2 (2τ1+1)

N

holds.

Set m = (cid:98)

N 3a1/2

5µ0ζ1−a2 τ1(2τ1+1) (cid:99) and T = mK. Then, we have

E[(cid:107)gradf (wsol)(cid:107)2] ≤

(cid:112)β2 + τ 2

2 C2θ2N a1ζ a2[f (w0) − f (w∗)]

T (cid:37)

.

(9)

The total number of gradient evaluations is O(N a1/(cid:15)) to obtain an (cid:15)-solution. The proof

is given by extending those of [12, 15, 23].

As a ﬁnal analysis, we present a local convergence rate in neighborhood of a local minimum
by introducing additionally a local assumption for retraction-convexity below. This is also
very common and standard in manifold optimization.

Assumption 3. We assume that the objective function f is strongly retraction-convex with
respect to R in Θ. Here, f is said to be strongly retraction-convex in Θ if f (Rw(tηw)) for
all w ∈ M and ηw ∈ TwM is strongly convex, i.e., there exists a constant 0 < λ such that
λ ≤ d2f (Rw(tηw))
, for all w ∈ Θ, all (cid:107)ηw(cid:107)w = 1, and all t such that Rw(τ ηw) ∈ Θ for all
τ ∈ [0, t]. Additionally, the vector transport T satisﬁes the locking condition, which is deﬁned
as

dt2

Tηw ξw = κTRηw ξw, where κ =

(cid:107)ξw(cid:107)w
(cid:107)TRηw ξw(cid:107)Rw(ηw)

,

(10)

for all ηw, ξw ∈ TwM and all w ∈ M.

8

It should be noted that, if we extend this local assumption to the entire manifold, as
R-SVRG [23], our rate below directly results in the global rate. However, such a global
assumption is fairly restrictive in terms of what cost functions and manifolds can be consid-
ered, and hence, the standard manifold literature mostly focuses on local rate analysis. For
example, R-SVRG [22] does not show a global rate on retraction-convex functions.

Theorem 3.3 (Local convergence rate analysis on retraction-convex functions). Let M be
a Riemannian manifold and w∗ ∈ M be a non-degenerate local minimizer of f . Suppose
Assumption 1 holds. Λ and λ are constants in Assumption 1 and 3, respectively. Let the
constants θ in (5), τ1 and τ2 in (6), and β, and C in (7). γc and Γc are the constants
in (8). Let α be a positive number satisfying λτ 2
2 C2θ2)) and
2 C2θ2). It then follows that for any sequence { ˜wk} generated by
γcλ2τ 2
t := α and mk := m converging to w∗,
Algorithm 1 with Option I under a ﬁxed step size αk
there exists 0 < Kth < K such that for all k > Kth,

1 > 2α(λ2τ 2

1 > 14αΛΓ2

1 − 14αΛΓ2

c(β2 + τ 2

c(β2 + τ 2

E[(dist( ˜wk+1, w∗))2] ≤

2(Λτ 2
mα(γcλ2τ 2

2 + 16mα2ΛΓ2
1 − 14αΛΓ2

c(β2 + τ 2
c(β2 + τ 2

2 C2θ2)
2 C2θ2))

E[(dist( ˜wk, w∗))2).

(11)

The proof structure is diﬀerent from that of [22,23] due to the way of bounding of E[(cid:107)ξk

t (cid:107)2]
and the existence of Hk
t . Additionally, comparing (11) with that of R-SVRG [22,23], we notice
the rate degradation. To the best of our knowledge, no theoretical rate result that is better
than or equals to that of SVRG [8] has been also given in the Euclidean SQN-VR [17]. Thus,
this issue is a common area of research in both the Euclidean and Riemannian settings to
further improve the theoretical rate. However, it should be emphasized that R-SQN-VR
shows much better performances than R-SVRG, especially on a ill-conditioned problem, as
shown later in Figure 1.

4 Numerical comparisons

This section compares R-SQN-VR with R-SGD with a decaying step-size sequence and R-
SVRG with a ﬁxed step size. The decaying step-size sequence is αk = α(1 + ας(cid:98)k/mk(cid:99))−1,
where (cid:98)·(cid:99) denotes the ﬂoor function. As references, we also compare them with two Rie-
mannian batch methods, i.e., R-SD, which is the steepest descent algorithm on Riemannian
manifolds with backtracking line search [1], and R-L-BFGS, which is the Riemannian L-BFGS
with strong wolfe condition [20, 28]. All experiments are executed in Matlab on a 4.0 GHz
Intel Core i7 PC with 16 GB RAM, and are stopped when the gradient norm gets below 10−8
or when they reach a predeﬁned maximum iteration. All results except R-SD and R-L-BFGS
are the best-tuned results from multiple choices of step sizes α and a ﬁxed ς = 10−3. This
paper addresses the Karcher mean computation problem of symmetric positive-deﬁnite (SPD)
manifold, and the low-rank matrix completion (MC) problem on the Grassmann manifold.
The details of the problems and manifolds are in the supplementary ﬁle.

Karcher mean problem on SPD manifold. The ﬁrst comparison is the Karcher
mean problem on SPD matrices [28]. All experiments use the batch size ﬁxed to 1 and L = 4,
and are initialized randomly and are stopped when the number of iterations reaches 10 for
R-SVRG and R-SQN-VR, and 60 for others. α are tuned from {10−5, . . . , 10−1}. mk and the
batch size are 3N and 1, respectively. Figures 1(a) and (b) show the results of the optimality
gap when N = 500 with d = 3 (Case KM-1) and the larger size case with N = 1500 (Case
KM-2), respectively. These results reveal that R-SQN-VR outperforms others.

9

MC problem on Grassmann manifold. We ﬁrst consider a synthetic dataset. The
proposed algorithm is also compared with Grouse [2], a state-of-the-art stochastic gradient
algorithm on the Grassmann manifold. Algorithms are initialized randomly as [29]. α are
tuned from {10−3, 5 × 10−3, . . . , 10−2, 5 × 10−2} for R-SGD, R-SVRG and R-SQN-VR, and
{1, 10, 100} for Grouse. We set explicitly the condition number, denoted as CN, of the matrix,
which represents the ratio of the maximal and the minimal singular values of the matrix. We
also set the over-sampling ratio (OS) for the number of known entries. The Gaussian noise is
also added with the noise level σ as suggested in [29]. mk and the batch size are set to 5N and
50, respectively. The maximum number of the outer iterations to stop is 100 for R-SVRG and
R-SQN-VR, and 100(mk + 1) for the others. This experiment evaluates the projection-based
vector transport and the QR-decomposition-based retraction, which do not satisfy the locking
condition, but is computationally eﬃcient. The baseline problem instance (Case MC-S1)
is the case of N = 5000, d = 200, rank r = 5, L = 10, OS = 8, σ = 10−10 and CN = 50.
Additionally, changing some parameters of those in Case MC-S1, we evaluate the lower-
sampling case with OS = 4 (Case MC-S2), the ill-conditioning case with CN = 100 (Case
MC-S3), the higher noise case with σ = 10−6 (Case MC-S4), and the higher rank case with
r = 10 (Case MC-S5). The results of the MSE on test set Φ, which is diﬀerent from the
training set Ω, are shown in Figures 1(c)-(h), respectively. This gives the prediction accuracy
of missing elements. From the ﬁgures, we conﬁrm the superior performance of R-SQN-VR.
Case MC-S6 for diﬀerent memory sizes L reveals that the larger size does not always show
better results, which is also noticed in [15]. Finally, we compare the algorithms on a real-world
dataset, the MovieLens-1M dataset1. It contains a million ratings for 3952 movies (N ) of 6040
users (d). We further randomly split this set into 80/10/10 percent data out of the entire
data as train/validation/test partitions. α is chosen from {10−5, 5 × 10−5, . . . , 10−2, 5 × 10−2},
the batch size is 50, r = 10, and L = 10. The algorithms are terminated when the MSE on
the validation set starts to increase or the number of the outer iteration reaches 100. Figure
1(i) shows the result except Grouse, which faces issues with convergence on this set (Case
MC-R). R-SQN-VR shows much faster convergences than others.

5 Conclusions

We have proposed a Riemannian stochastic quasi-Newton algorithm with variance reduction
(R-SQN-VR) on manifolds that is well suited for ﬁnite-sum minimization problems. We
presented a rigorous convergence analysis for taking the Hessian approximation into a variance
reduction stochastic setting on a manifold. Our proposed algorithm makes the explicit use of
retraction and vector transport operators on manifolds, which makes the proposed algorithm
appealing on a wider number of manifolds. The numerical comparisons show the beneﬁts of
our proposed algorithm on a number of applications.

1http://grouplens.org/datasets/movielens/

10

(a) Case KM-1: small size.

(b) Case KM-2: large size.

(c) Case MC-S1: baseline.

(d) Case MC-S2: low sampling.

(e) Case MC-S3: ill-conditioning.

(f) Case MC-S4: noisy data.

(g) Case MC-S5: higher rank.

(h) Case MC-S6: memory sizes.

(i) Case MC-R: MovieLens-1M.

Figure 1: Performance evaluations on Karcher mean (KM) problem and low-rank MC prob-
lem.

11

05101520253035#grad/N10-1010-5100Optimality gapR-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR05101520253035#grad/N10-1010-5100Optimality gapR-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR050100150200250300#grad/N10-1510-1010-5100Means square error on test set R-SQN-VR: L=5R-SQN-VR: L=10R-SQN-VR: L=20R-SQN-VR: L=400100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VRReferences

[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.

Princeton University Press, 2008.

[2] L. Balzano, R. Nowak, and B. Recht. Online identiﬁcation and tracking of subspaces

from highly incomplete information. In Allerton, pages 704–711, 2010.

[3] B. Mishra and R. Sepulchre. R3MC: A Riemannian three-factor algorithm for low-rank

matrix completion. In IEEE CDC, pages 1137–1142, 2014.

[4] H. Kasai and B. Mishra. Low-rank tensor completion: a Riemannian manifold precon-

ditioning approach. In ICML, 2016.

[5] G. Meyer, S. Bonnabel, and R. Sepulchre. Linear regression under ﬁxed-rank constraints:

A Riemannian approach. In ICML, 2011.

[6] S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Trans. on

Automatic Control, 58(9):2217–2229, 2013.

[7] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics,

pages 400–407, 1951.

[8] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive

variance reduction. In NIPS, pages 315–323, 2013.

[9] N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an expo-

nential convergence rate for ﬁnite training sets. In NIPS, pages 2663–2671, 2012.

[10] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regu-

larized loss minimization. JMLR, 14:567–599, 2013.

[11] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method

with support for non-strongly convex composite objectives. In NIPS, 2014.

[12] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction

for nonconvex optimization. In ICML, 2016.

[13] N. N. Schraudolph, J. Yu, and S. Gunter. A stochastic quasi-Newton method for online

convex optimization. In AISTATS, 2007.

[14] A. Mokhtari and A. Ribeiro. RES: Regularized stochastic BFGS algorithm. IEEE Trans.

on Signal Process., 62(23):6089–6104, 2014.

[15] X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for non-

convex stochastic optimization. arXiv preprint arXiv:1607.0123, 2016.

[16] R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer. A stochastic quasi-Newton method

for large-scale optimization. SIAM J. Optim., 26(2), 2016.

[17] P. Moritz, R. Nishihara, and M. I. Jordan. A linearly-convergent stochastic L-BFGS

algorithm. In AISTATS, pages 249–258, 2016.

12

[18] R. Kolte, M. Erdogdu, and A. Ozgur. Accelerating SVRG via second-order information,.

In OPT2015, 2015.

[19] D. Gabay. Minimizing a diﬀerentiable function over a diﬀerential manifold. Journal of

Optimization Theory and Applications, 37(2):177–219, 1982.

[20] W. Ring and B. Wirth. Optimization methods on Riemannian manifolds and their

application to shape space. SIAM J. Optim., 22(2):596–627, 2012.

[21] W. Huang, K. A. Gallivan, and P.-A. Absil. A Broyden class of quasi-Newton methods

for Riemannian optimization. SIAM J. Optim., 25(3):1660–1685, 2015.

[22] H. Sato, H. Kasai, and B. Mishra. Riemannian stochastic variance reduced gradient.

arXiv preprint: arXiv:1702.05594, 2017.

[23] H. Zhang, S. J. Reddi, and S. Sra. Fast stochastic optimization on Riemannian manifolds.

In NIPS, 2016.

[24] A. Mokhtari and A. Ribeiro. Global convergence of online limited memory BFGS. JMLR,

16:3151–3181, 2015.

[25] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt: a Matlab toolbox for

optimization on manifolds. JMLR, 15(1):1455–1459, 2014.

[26] J. Nocedal and Wright S.J. Numerical Optimization. Springer, New York, USA, 2006.

[27] D. Li and M. Fukushima. On the global convergence of BFGS method for nonconvex

unconstrained optimization. SIAM J. Optim., 11(4):1054–1064, 2011.

[28] X. Yuana, P.-A. Huang, W. Absil, and K. A. Gallivan. A Riemannian limited-memory

BFGS algorithm for computing the matrix geometric mean. In ICCS, 2016.

[29] D. Kressner, M. Steinlechner, and B. Vandereycken. Low-rank tensor completion by

Riemannian optimization. BIT Numer. Math., 54(2):447–468, 2014.

[30] B. Jeuris, R. Vandebril, and B. Vandereycken. A survey and comparison of contemporary

algorithms for computing the matrix geometric mean. ETNA, 2012.

[31] H Karcher. Riemannian center of mass and molliﬁer smoothing. Comm. Pure Appl.

Math., 30(5):509–541, 1977.

[32] S. Jayasumana, R. Hartley, M. Salzmann, H. Li, and M. Harandi. Kernel methods on
Riemannian manifolds with Gaussian RBF kernels. IEEE Trans. Pattern Anal. Mach.
Intell., 37(12), 2015.

[33] N. Boumal and P.-A. Absil. Low-rank matrix completion via preconditioned optimization
on the Grassmann manifold. Linear Algebra and its Applications, 475:200–239, 2015.

[34] L. Bottou, F. Curtis, and J. Nocedal. Optimization mehtods for large-scale machine

learning. arXiv preprint: arXiv:1606.04838, 2016.

[35] H. Zhang and S. Sra. First-order methods for geodesically convex optimization. In COLT,

2016.

13

[36] W. Huang, P.-A. Absil, and K. A. Gallivan. A Riemannian symmetric rank-one trust-

region method. Math. Program., Ser. A, 150:179–216, 2015.

[37] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

14

A Problems and manifolds in numerical comparison

This section gives a brief explanation of the problems and the manifolds that are evaluated
in the numerical comparisons in Section 4.

A.1 SPD manifold and Karcher mean problem

SPD manifold S d
with the Riemannian metric deﬁned by

++. Let S d

++ be the manifold of d × d SPD matrices. If we endow S d

++

(cid:104)ξX, ηX(cid:105)X = trace(ξXX−1ηXX−1)

at X ∈ S d
for the exponential mapping is given by

++, the SPD manifold S d

++ becomes a Riemannian manifold. The explicit formula

ExpX(ξX) = X1/2 exp(X−1/2ξXX−1/2)X1/2

for any ξX ∈ TXS d
proposed in [30] is a retraction, which is symmetric positive-deﬁnite for all ξX ∈ TXS d
X ∈ S d

++. On the other hand, RX(ξX) = X + ξX + 1

++. The parallel translation on S d

++ along ηX is given by

++ and X ∈ S d

2 ξXX−1ξX
++ and

PηX(ξX) = X1/2YX−1/2ξXX−1/2YX1/2,

where Y = exp(X−1/2ηXX−1/2/2). A more eﬃcient algorithm that constructs an isometric
vector transport is proposed based on a ﬁeld of orthonormal tangent bases [28] while satisfying
the locking condition (10). We use it in this experiment, and the details are in [21, 28]. The
logarithm map of Y at X is given by

LogX(Y) = X1/2 log(X−1/2YX−1/2)X1/2 = log(YX−1)X.

Karcher mean problem on S d

++. The Karcher mean is introduced as a notion of
mean on Riemannian manifolds by Karcher [31]. It generalizes the notion of an “average” on
a manifold. Given N points on S d
++ with matrix representations Q1, . . . , QN , the Karcher
mean is deﬁned as the solution to the problem

min
X∈Sd
++

1
N

N
(cid:88)

(dist(X, Qn))2,

n=1

(cid:80)N

1
N

++

n=1(dist(X, Qn))2, where dist(p, q) = (cid:107) log(p−1/2qp−1/2)(cid:107)F represents the dis-
minX∈Sd
tance along the corresponding geodesic between the elements on S d
++ with respect to the aﬃne-
n=1 −log(QnX−1)X.
invariant metric. The gradient of the loss function is computed as 2
N
The Karcher mean on S d
++ is frequently used for computer vision problems, such as visual
object categorization and pose categorization [32]. Since recursive calculations are needed
with each visual image, stochastic gradient algorithms become an appealing choice for large
datasets.

(cid:80)N

15

A.2 Grassmann manifold and MC problem

Grassmann manifold Gr(r, d). A point on the Grassmann manifold is an equivalence class
represented by a d × r orthogonal matrix U with orthonormal columns, i.e., UT U = I. Two
orthogonal matrices express the same element on the Grassmann manifold if they are related
by right multiplication of an r × r orthogonal matrix O ∈ O(r). Equivalently, an element of
Gr(r, d) is identiﬁed with a set of d × r orthogonal matrices [U] := {UO : O ∈ O(r)}. That
is, Gr(r, d) := St(r, d)/O(r), where St(r, d) is the Stiefel manifold that is the set of matrices
of size d × r with orthonormal columns. The Grassmann manifold has the structure of a
Riemannian quotient manifold [1, Section 3.4].

The exponential mapping for the Grassmann manifold from U(0) := U ∈ Gr(r, d) in the

direction of ξ ∈ TU(0)Gr(r, d) is given in a closed form as [1, Section 5.4]

U(t) = [U(0)V W]

(cid:21)

(cid:20) cos tΣ
sin tΣ

VT ,

where ξ = WΣVT is the singular value decomposition (SVD) of ξ with rank r. The sin(·)
and cos(·) operations are performed only on the diagonal entries. The parallel translation of
ζ ∈ TU(0)Gr(r, d) on the Grassmann manifold along γ(t) with ˙γ(0) = WΣVT is given in a
closed form by

(cid:18)

ζ(t) =

[U(0)V W]

(cid:21)

(cid:20) − sin tΣ
cos tΣ

WT + (I − WWT )

(cid:19)

ζ.

The logarithm map of U(t) at U(0) on the Grassmann manifold is given by

ξ = LogU(0)(U(t)) = W arctan(Σ)VT ,

where WΣVT is the SVD of (U(t) − U(0)U(0)T U(t))(U(0)T U(t))−1 with rank r. Further-
more, a popular retraction is

RU(0)(ξ) = qf(U(0) + tξ)

(= U(t))

which extracts the orthonormal factor based on QR decomposition, and a popular vector
transport uses an orthogonal projection of tξ to the horizontal space at U(t), i.e., (I −
U(t)U(t)T )tξ [1].

Matrix completion problem. The matrix completion problem is completing an incom-
plete matrix X, say of size d × N , from a small number of entries by assuming that the latent
structure of the matrix is low-rank. If Ω is the set of known indices in X, the rank-r matrix
completion problem amounts to solving

minU,A (cid:107)PΩ(UA) − PΩ(X)(cid:107)2
F ,
where U ∈ Rd×r, A ∈ Rr×N , and the operator PΩ acts as PΩ(Xij) = Xij if (i, j) ∈ Ω and
PΩ(Xij) = 0 otherwise. Partitioning X = [x1, . . . , xn], the previous problem is equivalent to

min
U∈Rd×r, an∈Rr

1
N

N
(cid:88)

n=1

(cid:107)PΩn(Uan) − PΩn(xn)(cid:107)2
2,

where xn ∈ Rd and the operator PΩn is the sampling operator for the n-th column. Given
U, an admits a closed form solution. Consequently, the problem only depends on the column
space of U and is on Gr(r, d) [33].

16

B Two-loop Hessian inverse updating algorithm

The section summarizes the Riemannian two-loop Hessian inverse updating algorithm in
Algorithm A.1. This is an straightforward extension of that in the Euclidean space explained
in [26, Section 7.2].

Algorithm A.1 Hessian inverse updating
Require: Pair-updating counter t, memory depth τ , correction pairs {sk

u, yk

u}k−1

u=k−τ , gradient

p.

5:

k = χkid = (cid:104)sk
(cid:104)yk

1: p0 = p.
2: H0
3: for u = 0, 1, 2, . . . , τ − 1 do
4:

t ,yk
t (cid:105)
t ,yk
t (cid:105)

k−u−1, yk

id.

k−u−1(cid:105).

ρk−u = 1/(cid:104)sk
αu = ρk−u−1(cid:104)sk
pu+1 = pu − αuyk

k−u−1, pu(cid:105).
k−u−1.

6:
7: end for
8: q0 = H0
9: for u = 0, 1, 2, . . . , τ − 1 do
k−τ +u, qu(cid:105).
10:

βu = ρk−τ +u(cid:104)yk
qu+1 = qu + (ατ −u−1 − βu)sk

kpτ .

11:
12: end for
13: q = qτ .

k−τ +u.

17

C Proofs of convergence analysis on non-convex functions

This section presents the proof of the global convergence analysis on non-convex functions.
Hereinafter, we use E[·] to express expectation with respect to the joint distribution of all
random variables. For example, wt is determined by the realizations of the independent
random variables {i1, i2, . . . , it−1}, the total expectation of f (wt) for any t ∈ N can be taken
Ei2 . . . Eit−1[f (wt)]. We also use Eit[·] to denote an expected value taken
as E[f (wt)] = Ei1
with respect to the distribution of the random variable it. In addition, we omit the subscript
˜wk for a Riemannian metric (cid:104)·, ·(cid:105) ˜wk when the tangent space to be considered is clear.

C.1 Preliminary lemmas

This subsection ﬁrst states some preliminary lemmas.

The literature [1] generalizes a Taylor’s theorem to Riemannian manifolds. However, it
addresses the exponential mapping instead of the retraction. Therefore, [21] applys Taylor’s
theorem on the retraction by newly introducing a function along a curve on the manifold.
)) for a twice continuously diﬀerentiable objective function.
Here, we denote f (Rwk
From Taylor’s theorem, we obtain below;

(tηk/(cid:107)ηk(cid:107)wk

t

t

Lemma C.1 (In Lemma 3.2 in [21]). Under Assumptions 1.1, 1.2, and 1.3, there exists Λ
such that

f (wk

t+1) − f (wk

t ) ≤ (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

t

+

1
2

Proof. From Taylor’s theorem, we have

Λ(αk

t (cid:107)ηk(cid:107)wk

t

)2.

(A.1)

f (wk
= f (Rwk
d
dτ

=

t

t+1) − f (wk
t )
t ηk)) − f (Rwk

(αk

(0))

t

f (Rwk

t

(τ ηk/(cid:107)ηk(cid:107)wk

t

= (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

+

≤ (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

+

t (cid:107)ηk(cid:107)wk

t

· αk

(cid:12)
(cid:12)
))
(cid:12)τ =0
d2
1
dτ 2 f (Rwk
2
1
Λ(αk
t (cid:107)ηk(cid:107)wk
2

t

t

)2,

t

t

+

1
2

d2
dτ 2 f (Rwk
(cid:12)
(cid:12)
))
(cid:12)τ =p

t

t

(τ ηk/(cid:107)ηk(cid:107)wk

t

(cid:12)
(cid:12)
))
(cid:12)τ =p

· (αk

t (cid:107)ηk(cid:107)wk

t

)2

· (αk

t (cid:107)ηk(cid:107)wk

t

)2

(τ ηk/(cid:107)ηk(cid:107)wk

where 0 ≤ p ≤ αk
proof.

t (cid:107)ηk(cid:107)wk

t

, and Λ is the constant in Assumption 1.3. This completes the

Lemma C.2. Suppose Assumption 1 holds. Then there exists a constant 0 < υ for all k such
that

υ ≤

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

.

(A.2)

Proof. The claim for the case sk = 0 is obvious. Assume that sk (cid:54)= 0 below. This is given by
applying Cauchy-Schwarz inequality to the condition (4) recursively. More speciﬁcally, (4)
yields that

(cid:15)(cid:107)sk(cid:107)2 ≤ (cid:104)yk, sk(cid:105) ≤ (cid:107)yk(cid:107)(cid:107)sk(cid:107),

18

and considering the most left and right terms, we obtain

Substituting this into the above equation yields

(cid:107)sk(cid:107) ≤

1
(cid:15)

(cid:107)yk(cid:107).

Consequently, we obtain

(cid:104)sk, yk(cid:105) ≤ (cid:107)sk(cid:107)(cid:107)yk(cid:107) ≤

1
(cid:15)

(cid:107)yk(cid:107)2.

(cid:107)yk(cid:107)2
(cid:104)sk, yk(cid:105)

≥ (cid:15)

(= υ).

This completes the claim by denoting (cid:15) as υ.

Lemma C.3. Suppose Assumption 1 holds. There exists a constant 0 < Υnc for all k such
that

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

≤ Υnc.

(A.3)

Proof. Most part of this proof is given in Lemma 3.9 of [21]. But, [21] uses the strongly
retraction-convexity assumption for the ﬁnal part. Therefore, we include that of [21] for
completeness, and describe the ﬁnal part with a slight modiﬁcations. This proof is also
included for the subsequent analysis.
k = gradf ( ˜wk+1) − P 1←0

gradf ( ˜wk), where γk(t) = R ˜wk (tηk), i.e., the retraction
curve connecting ˜wk and ˜wk+1, and Pγk is the parallel translation along γk(t). We have
k − ¯Hkηk(cid:107) ≤ b0(cid:107)ηk(cid:107)2 = b0(cid:107)sk(cid:107)2, where ¯Hk = (cid:82) 1
yP
(cid:107)P 1←0
dt and b0 > 0.
γk
It follows that

Hessf (γk(t))P t←0

Deﬁne yP

0 P 0←t
γk

γk

γk

(cid:107)yk(cid:107) ≤ (cid:107)yk − yP
= (cid:107)yk − yP
≤ (cid:107)yk − yP
≤ (cid:107)gradf ( ˜wk+1)/κk − Tηk gradf ( ˜wk) − gradf ( ˜wk+1) + P 0←1

yP
k (cid:107)
k − ¯Hkηk(cid:107) + (cid:107) ¯Hkηk(cid:107)
yP

k (cid:107) + (cid:107)yP
k (cid:107)
k (cid:107) + (cid:107)P 0←1
k (cid:107) + (cid:107)P 0←1

γk

γk

γk

gradf ( ˜wk)(cid:107)

+(cid:107) ¯Hkηk(cid:107) + b0(cid:107)sk(cid:107)2

≤ (cid:107)gradf ( ˜wk+1)/κk − gradf ( ˜wk+1)(cid:107) + (cid:107)P 0←1

γk

gradf ( ˜wk) − Tηk gradf ( ˜wk)(cid:107)

+(cid:107) ¯Hkηk(cid:107) + b0(cid:107)sk(cid:107)2

≤ b1(cid:107)sk(cid:107)(cid:107)gradf ( ˜wk+1)(cid:107) + b2(cid:107)sk(cid:107)(cid:107)gradf ( ˜wk)(cid:107) + b3(cid:107)sk(cid:107) + b0(cid:107)sk(cid:107)2
≤ b4(cid:107)sk(cid:107),

(A.4)

where b1, b2, b3, and b4 > 0. Here, we directly obtain the following fact from (4) in the cautious
update as

(cid:107)sk(cid:107)2
(cid:104)yk, sk(cid:105)

≤

1
(cid:15)

.

(A.5)

Therefore, we ﬁnally obtain the upper bound of (cid:104)yk,yk(cid:105)

(cid:104)sk,yk(cid:105) from (A.4) and (A.5) as

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

=

(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)

·

(cid:107)yk(cid:107)2
(cid:107)sk(cid:107)2 ≤

b2
4
(cid:15)

(= Υnc).

(A.6)

Denoting b2

4/(cid:15) as Υnc, this completes the proof.

19

Remark C.4. From the proof of Lemma C.3, if the parallel translation is used for vector
transport, i.e., T = P , the ﬁrst two terms in (A.4) are equal to zero, and the upper bound
Υnc in (A.3) can get smaller than that of the case in the vector transport.

C.2 Eigenvalue bounds of Hk

t on non-convex functions

:= T˜ηk

t at wk

t , i.e., Hk
t

This subsection presents Proposition C.7, which is an essential proposition that bounds the
eigenvalues of Hk
)−1. To this end, we particularly use
the Hessian approximation operator ˜Bk = ( ˜Hk)−1 as opposed to ˜Hk. Since mentioned in the
algorithm description, we consider the curvature information for ˜Hk at ˜wk, i.e., every outer
epoch, and reuse this ˜Hk in the calculation of the second-order modiﬁed stochastic gradient
Hk

t ξk
t at wk
1. We ﬁrst address the bounds of ˜Hk at ˜wk. The main task of the proof is to bound the

t . Thereby, the way of the proof consists of two steps as follows;

◦ ˜Hk ◦ (T˜ηk

t

t

Hessian operator ˜Bk = ( ˜Hk)−1.

2. Next, we bound Hk

t at wk

t based on the bounds of ˜Hk at ˜wk.

It should be noted that, in this subsection, the curvature pair {sk

j , yk

j }k−1

j=k−L ∈ T ˜wkM is

simply notated as {sj, yj}k−1

j=k−L.

First, we attempt to bound trace( ˆ˜B) in order to bound the eigenvalues of ˜Hk, where a hat
denotes the coordinate expression of the operator. The basic structure of the proof follows
stochastic L-BFGS methods in the Euclidean space, e.g., [15, 16, 24]. Nevertheless, some
special treatments considering the Riemannian setting and the lemmas earlier are required.
It should be noted that trace( ˆ˜B) does not depend on the chosen basis.
Lemma C.5 (Bounds of trace of ˜Bk). Consider the recursion of ˜Bk

u as

u+1 = ˇBk
˜Bk

u −

usk−τ +u( ˇBk
ˇBk
( ˇBk

usk−τ +u)(cid:91)
usk−τ +u)(cid:91)sk−τ +u

+

yk−τ +ty(cid:91)
k−τ +u
y(cid:91)
k−τ +usk−τ +u

,

(A.7)

u = Tηk

where ˇBk
˜Bk
u(Tηk )−1 for u = 0, . . . , τ − 1. The Hessian approximation at k-th outer
τ when u = τ − 1. Then, consider the Hessian approximation ˜Bk = ˜Bk
epoch is ˜Bk = ˜Bk
τ in
k id. If Assumption 1 holds, the trace( ˆ˜Bk) in a coordinate expression of
(A.7) with ˜Bk
˜Bk is uniformly upper bounded for all k ≥ 1 as

0 = γ−1

trace( ˆ˜Bk) ≤ (M + τ )Υnc,

(A.8)

where M is the dimension of M. Here, a hat expression represents the coordinate expression
of an operator.

Proof. The proof can be completed parallel to the Euclidean case [17]. We use a hat symbol
in order to represent the coordinate expression of the operator ˜Bk
u in update formula
(A.7). Because T is an isometric vector transport, Tηk is invertible for all k. Accordingly,
trace( ˆ˜Bk) and det( ˆ˜Bk) can be reformulated as

u+1 and ˇBk

trace( ˆˇBk) = trace( ˆTηk

ˆ˜Bk ˆT −1
ηk

) = trace( ˆ˜Bk),

(A.9)

(A.10)

20

We ﬁrst consider the trace lower bound of trace( ˆ˜Bk

τ ) from (A.9) and (A.7) as

trace( ˆ˜Bk

u+1) = trace( ˆˇBk

u) −

= trace( ˆ˜Bk

u) −

uˆsk−τ +u, ˆˇBk
(cid:104) ˆˇBk
uˆsk−τ +u(cid:105)
(cid:104) ˆˇBk
uˆsk−τ +u, ˆsk−τ +u(cid:105)
(cid:107) ˆˇBk
uˆsk−τ +u(cid:107)2
uˆsk−τ +u, ˆsk−τ +u(cid:105)

(cid:104) ˆˇBk

+

+

(cid:104)yk−τ +u, yk−τ +u(cid:105)
(cid:104)yk−τ +u, sk−τ +u(cid:105)

(cid:107)yk−τ +u(cid:107)2
(cid:104)yk−τ +u, sk−τ +u(cid:105)

,

where we use the same notation (cid:104)·, ·(cid:105) for the inner product with respect to local coordinates
corresponding to the Riemannian metric. Here, the positive deﬁniteness of ˆˇBk
u guarantees the
negativity of the second term. Therefore, the bound of the third term yields from Lemma
C.3 as

trace( ˆ˜Bk

u+1) ≤ trace( ˆ˜Bk

u) + Υnc.

By calculating recursively this for u = 0, · · · , τ − 1, we can conclude that

trace( ˆ˜Bk

u) ≤ trace( ˆ˜Bk

All that is left is to bound trace( ˆ˜Bk
where, as a common choice in L-BFGS in the Euclidean, χk = (cid:104)sk,yk(cid:105)

0 ) + uΥnc.
0 ). For this purpose, we consider the deﬁnition ˆ˜Bk
(cid:104)yk,yk(cid:105) , and we obtain

0 = id/χk,

trace( ˆ˜Bk

0 ) = trace

(cid:19)

(cid:18) I
χk

=

M
χk

= M

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

≤ M Υnc.

Consequently, we obtain

trace( ˆ˜Bk

u) ≤ (M + u)Υnc.

(A.11)

Plugging u = τ into (A.11) yields the claim (A.8).

Thus, this completes the proof.

Now we prove the main lemma for Proposition C.7.

Lemma C.6 (Bounds of ˜Hk). Suppose the constant 0 < γnc < Γnc < ∞. If Assumption 1
holds, the eigenvalues of ˜Hk is bounded by γnc and Γnc for all k ≥ 1 as

γncid (cid:22) ˜Hk (cid:22) Γncid,

where γnc and Γnc are some positive constants.

Proof. We ﬁrst state the lower bound part. The proof is obtained as parallel to the Euclidean
case [24]. The sum of its eigenvalues of ˆ˜Bk corresponds to the bounds on the trace. Here, we
denote πi as the i-th largest eigenvalue of the operator matrix ˆ˜Bk for 1 ≤ i ≤ M . From (A.8)
in Lemma C.5, the sum of the eigenvalues of ˆ˜Bk satisﬁes below

M
(cid:88)

i=1

πi = trace( ˆ˜Bk) ≤ (M + τ )Υnc.

(A.12)

21

Because all the eigenvalues are positive due to the positive deﬁniteness of ˆ˜Bk, it is obvious
that every eigenvalue is less than the upper bound of the sum of all of the eigenvalues.
Consequently, we obtain λi ≤ (M + τ )Υnc for all i, and ﬁnally obtain ˜Bk (cid:22) (M + τ )Υncid.
The bounds in (A.12) implies that its inverse is the bound for the eigenvalues of ˆ˜Hk = ( ˆ˜Bk)−1
as

(γncid =)

1
(M + τ )Υnc

id (cid:22) ˜Hk.

(A.13)

as γnc, we obtain the lower bound of the claim.

Denoting

1
(M +τ )Υnc

Next, we present the proof for the upper bound part by referring [15]. ˜Hk is deﬁned as

˜Hk

u = (id − ρkyks(cid:91)

k)(cid:91) ˇHk

u−1(id − ρkyks(cid:91)

k) + ρksks(cid:91)
k,

(A.14)

where ˇHk
sentation of ˜Hk

u−1 = Tηk ◦ ˜Hk
u is

u−1 ◦ T −1
ηk

, and ρk = 1/(cid:104)yk, sk(cid:105) [21]. Therefore, the coordinate repre-

ˆ˜Hk

u = (id − ρkyksT

k )T ˆˇHk

= ˆˇHk

u−1 − ρk( ˆˇHk

u−1yksT

u−1(id − ρkyksT
k + skyT ˆˇHk

k ) + ρksksT
k ,
k + ρ2
u−1) + ρksksT

kskyT
k

ˆˇHk

u−1yksT
k .

(A.15)

Here, noticing below from the fact T is isometric in Assumption 1.4,

(cid:107) ˇHk

u−1(cid:107) = (cid:107)Tηk ◦ ˜Hk

u−1 ◦ T −1
ηk

(cid:107) = (cid:107) ˜Hk

u−1(cid:107),

(A.16)

we obtain below from (A.5) and (A.6),

(cid:107) ˆ˜Hk

u(cid:107) ≤ (cid:107) ˆ˜Hk

u−1(cid:107) +

2(cid:107) ˆ˜Hk

u−1(cid:107)(cid:107)yk(cid:107)(cid:107)sk(cid:107)
(cid:104)sk, yk(cid:105)

= (cid:107) ˆ˜Hk

u−1(cid:107) + 2(cid:107) ˆ˜Hk
(cid:19)2

u−1(cid:107)

(cid:107) ˆ˜Hk

u−1(cid:107) +

(cid:20) (cid:107)yk(cid:107)2
(cid:104)sk, yk(cid:105)
1
(cid:15)

(cid:18)

=

1 +

b4
(cid:15)

+

(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)
(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)

·

+

(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)

(cid:107) ˆ˜Hk

u−1(cid:107)(cid:107)yk(cid:107)2
(cid:104)sk, yk(cid:105)

(cid:21)1/2

+

(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)

+

(cid:107)sk(cid:107)2
(cid:104)sk, yk(cid:105)

(cid:107) ˆ˜Hk

u−1(cid:107)(cid:107)yk(cid:107)2
(cid:104)sk, yk(cid:105)

= q(cid:107) ˆ˜Hk

u−1(cid:107) +

1
(cid:15)

,

(A.17)

where we denote (1+b4/(cid:15))2 as q for simplicity. Because we consider the deﬁnition ˆ˜Bk
(cid:104)yk,yk(cid:105) , we obtain (cid:107) ˆ˜Hk
where, as a common choice in L-BFGS in the Euclidean, χk = (cid:104)sk,yk(cid:105)

0 = id/χk,
0(cid:107) as

(cid:107) ˆ˜Hk

0(cid:107) = (cid:107)χk(cid:107) =

(cid:104)sk, yk(cid:105)
(cid:107)yk(cid:107)2 ≤

1
(cid:15)

,

where the last inequality uses (A.2) in Lemma C.2. Then, it follows that

(cid:107) ˆ˜Hk

1(cid:107) ≤ q(cid:107) ˆ˜Hk

0(cid:107) +

1
(cid:15)

≤

1
(cid:15)

(q + 1).

22

By recurrence relation, we calculate (cid:107) ˆ˜Hk

u(cid:107) from (cid:107) ˆ˜Hk

1(cid:107) as
(cid:19)

1
(cid:15)(1 − q)

qu−1 +

1
(cid:15)(1 − q)

(cid:107) ˆ˜Hk

u(cid:107) ≤

(cid:18) 1
(cid:15)

(q + 1) −

=

qu+1 − 1
(cid:15)(q − 1)

.

Consequently, plugging u = τ and considering ˆ˜Hk
below;

τ = ˆ˜Hk and λmax( ˆ˜Hk

τ ) = (cid:107) ˆ˜Hk

τ (cid:107), we obtain

˜Hk (cid:22)

(1 + b4/(cid:15))2(τ +1) − 1
(cid:15)((1 + b4/(cid:15))2 − 1)

id

(= Γncid).

(A.18)

Denoting the upper bound (1+b4/(cid:15))2(τ +1)−1
claim. This completes the proof.

(cid:15)((1+b4/(cid:15))2−1) as Γnc, we obtain the upper bound part of the

Finally we present Proposition C.7.

Proposition C.7 (Bounds of Hk
T˜ηk
and Γnc for all k ≥ 1, t ≥ 1, i.e.,

◦ ˜Hk ◦ (T˜ηk

t

t

)−1. If Assumption 1 holds, the range of eigenvalues of Hk

t on non-convex functions). Consider the operator Hk
t

:=
t is bounded by γnc

where γnc and Γnc are some positive constants.

γncid (cid:22) Hk

t (cid:22) Γncid,

(A.19)

:= T˜ηk

˜wk (wk
Proof. Considering Hk
t
transformation operator, we can conclude that the eigenvalues of Hk
Actually, let hat expressions be representation matrices with some bases of Twk
we have the relation below;

)−1, where ˜ηk

is a liner
t ), since T˜ηk
t and ˜Hk are identical.
M and T ˜wkM,

t = R−1

◦ ˜Hk ◦ (T˜ηk

t

t

t

t

det(λid − ˆHk

t ) = det(λid − ˆT˜ηk
= det( ˆTS

˜ηk
t

t

t

)−1)

ˆ˜Hk( ˆT˜ηk
)−1)
(λid − ˆ˜Hk)( ˆT˜ηk
)det(λid − ˆ˜Hk)det(( ˆT˜ηk
)det(λid − ˆ˜Hk)det( ˆT˜ηk

t

t

)−1)

)−1

t

= det( ˆTS

˜ηk
t

= det( ˆTS
= det(λid − ˆ˜Hk).

˜ηk
t

Therefore, Lemma C.6 directly yields the claim. This completes the proof.

C.3 Proof of global convergence analysis (Theorem 3.1)

This subsection shows the global convergence analysis of the proposed R-SQN-VR. This
analysis partially extends the expectation-based analysis of SGD in the Euclidean space [34]
into the proposed algorithm.

23

C.3.1 Essential lemmas

We ﬁrst obtain the following lemma from (A.1) in Lemma C.1. Subsequently, E
a meaningful quantity because wk

[f (wk
t through the update in Algorithm 1.

t+1 depends on ik

ik
t

t+1)] is

Lemma C.8. Under Lemma C.1, the iterates of Algorithm 1 satisfy the following inequality
for all k ∈ N:

E

ik
t

[f (wk

t+1)] − f (wk

t ) ≤ −αk

t (cid:104)gradf (wk

t ), E
ik
t

[Hk

t ξk

t ](cid:105)wk

t

+

1
2

(αk

t )2ΛE
ik
t

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

].

(A.20)

Proof. When wk
Algorithm 1 satisfy from (A.1) in Lemma C.1

t+1 = Rwk

(−αk

t Hk

t ξk

t ), substituting −Hk

t

t ξk
t

into ηk, the iterates generated by

f (wk

t+1) − f (wk

t ) ≤ (cid:104)gradf (wk

t ), −αk

t Hk

t ξk

t (cid:105)wk

t

= −αk

t (cid:104)gradf (wk

t ), Hk

t ξk

t (cid:105)wk

t

+

+

1
2
1
2

Λ(cid:107) − αk

t Hk

t ξk

t (cid:107)2
wk
t

(αk

t )2Λ(cid:107)Hk

t ξk

t (cid:107)2
wk
t

.

(A.21)

Taking expectations in the inequalities above with respect to the distribution of ik
that wk

t , we obtain the desired bound.

t , depends on ik

t+1, but not wk

t , and noting

This lemma shows that, regardless of how Algorithm 1 arrived at wk

t , the expected decrease
in the objective function yielded by the k-th step is bounded above by a quantity involving:
(i) the expected directional derivative of f at wk
t and (ii) the second moment of
Hk

t along −Hk

t ξk

t ξk
t .
Next, we derive the following lemma;

Lemma C.9. Under Assumptions 1 and 2, the sequence of average function f (wk

t ) satisﬁes

E[f (wk

t+1)] ≤ f (wk

t ) − αk

t γnc(cid:107)gradf (wk

t )(cid:107)2
wk
t

+

9Λ(αk

ncS2

t )2Γ2
2

.

(A.22)

Proof. Taking expectation (A.20) in Lemma C.8 with regard to wk
deterministic when wk

t is given, we write

t considering that Hk
t

is

E[f (wk

t+1)]

≤ f (wk

t ) − αk

t (cid:104)gradf (wk

t ), Hk
t

E

ik
t

[ξk

t ](cid:105)wk

t

+

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

]

≤ f (wk

t ) − αk

t (cid:104)gradf (wk

t ), Hk

t gradf (wk

≤ f (wk

t ) − αk

t (cid:104)gradf (wk

t ), Hk

t gradf (wk

≤ f (wk

t ) − αk

t γnc(cid:107)gradf (wk

t )(cid:107)2
wk
t

+

E

+

(αk
t )2Λ
ik
2
t
(αk
t )2Λ
2
(αk
t )2Λ
2
ncS2

+

,

t )(cid:105)wk

t

t )(cid:105)wk

t

9Λ(αk

t )2Γ2
2

E

ik
t

E

ik
t

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

]

[Γ2

nc(cid:107)ξk

t (cid:107)2
wk
t

]

(A.23)

where the second inequality is obtained from E
estimate of gradf (wk

ik
t
t ). The last inequality comes from Assumption 2 since

t ] = gradf (wk

t ) because ξk
t

[ξk

is an unbiased

(cid:107)ξk

t (cid:107)wk

t

(wk

= (cid:107)gradfik
≤ S + S + S = 3S,

t ) − T˜ηk

t

t

(cid:16)

gradfik

t

(cid:17)
( ˜wk)

+ T˜ηk

t

(cid:16)

(cid:17)
gradf ( ˜wk)

(cid:107)wk

t

(A.24)

where ˜ηk

t ∈ T ˜wkM satisﬁes R ˜wk (˜ηk

t ) = wk

t . This completes the proof.

24

Proposition C.10. Under Assumptions 1 and 2, suppose that Algorithm 1 is run with a
step-size sequence satisfying Assumption 2. Then, we have

(cid:34) K
(cid:88)

mk(cid:88)

E

k=1

t=1

(cid:35)

t (cid:107)gradf (wk
αk

t )(cid:107)2
wk
t

< ∞.

Proof. Taking the total expectation of (A.22) in Lemma C.9 yields

E[f (wk

t+1)] − E[f (wk

t )] ≤ −αk

t γncE[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] +

(A.25)

.

9Λ(αk

ncS2

t )2Γ2
2

Summing both sides of this inequality for {w1

1, . . . , w1

m1, . . . , wK−1

1

, . . . , wK−1
mK−1

, wK

1 , . . . , wK
mK

}

gives

finf − f (w1

t+1)] − f (w1
1) ≤ E[f (wk
1)
mk(cid:88)
K
(cid:88)

≤ −γnc

k=1

t=1

αk
t

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] +

9ΛΓ2
ncS2
2

K
(cid:88)

mk(cid:88)

k=1

t=1

(αk

t )2.

Dividing by γnc and rearranging the terms, we obtain

K
(cid:88)

mk(cid:88)

k=1

t=1

αk
t

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] ≤

(f (w1

1) − finf )
γnc

+

ncS2

9ΛΓ2
2γnc

K
(cid:88)

mk(cid:88)

k=1

t=1

(αk

t )2.

The second condition of the decaying step-size sequence in Assumption 2, i.e., (cid:80)(αk
t )2 <
∞, implies that the right-hand side of this inequality converges to a ﬁnite limit when K
increases. This completes the proof.

Then, we obtain the following proposition by taking (A.25) into account with the ﬁrst

condition of Assumption 2.

Proposition C.11. Under Assumptions 1 and 2, suppose that Algorithm 1 is run with a
step-size sequence satisfying Assumption 2. Then, we have

lim inf
k→∞

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] = 0.

(A.26)

Proof. The proof is by contradiction. Assume that (A.26) does not hold. Then, there exists
δ > 0 such that E[(cid:107)gradf (wk

] > δ for all k suﬃciently large, say, k > N . We have

t )(cid:107)2
wk
t

(cid:34) ∞
(cid:88)

mk(cid:88)

E

k=1

t=1

(cid:35)

t (cid:107)gradf (wk
αk

t )(cid:107)2
wk
t

≥

∞
(cid:88)

mk(cid:88)

k=N

t=1

αk
t

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] > δ

∞
(cid:88)

mk(cid:88)

k=N

t=1

αk

t = ∞.

This contradicts (A.25).

This implies that, for the R-SQN-VR with decaying step-sizes sequence, the expected

gradient norms cannot stay bounded away from zero.

25

C.3.2 Main proof of Theorem 3.1

Theorem. 3.1. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate
local minimizer of f . Consider Algorithm 1 and suppose Assumptions 1 and 2, and that the
mapping w (cid:55)→ (cid:107)gradf (w)(cid:107)2
w has the positive real number that the largest eigenvalue of its
Riemannian Hessian is bounded for all w ∈ M. Then, we have

lim
k→∞

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] = 0.

Proof. We deﬁne h(w) as h(w) := (cid:107)gradf (w)(cid:107)2
and let Λh be the absolute value of the
wk
t
eigenvalue with the largest magnitude of the Hessian of h. Then, from Taylor’s theorem, we
obtain

h(wk

t+1) − h(wk

t ) ≤ −2αk

t (cid:104)gradh(wk

t ), Hessf (wk

t )[Hk

t ξk

t ](cid:105)wk

t

+

1
2

(αk

t )2Λh(cid:107)Hk

t ξk

t (cid:107)2
wk
t

.

Taking the expectation with respect to the distribution of ik

t , we obtain below;

E

ik
t

[h(wk

t+1)] − h(wk
t )

≤ −2αk

t (cid:104)gradf (wk

t ), E
ik
t

[Hessf (wk

t )[Hk

t ξk

t ]](cid:105)wk

t

= −2αk

t (cid:104)gradf (wk

t ), Hessf (wk

t )[Hk
t

E

ik
t

[ξk

t ]](cid:105)wk

t

+

+

1
2
1
2

≤ 2αk

t (cid:107)gradf (wk

t )(cid:107)wk

t

(cid:107)Hessf (wk

t )[Hk

t gradf (wk

t )](cid:107)wk

t

≤ 2αk

t ΛΓnc(cid:107)gradf (wk

t )(cid:107)2
wk
t

+

9
2

(αk

t )2ΛhS2Γ2

nc,

(αk

t )2ΛhE
ik
t

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

]

]

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

(αk

t )2ΛhE
ik
t
1
2

(αk

+

t )2ΛhΓ2
nc

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

]

where the last inequality comes from (cid:107)Hessf (w)[Hk
ΛΓnc(cid:107)gradf (wk

.

t )(cid:107)wk

t

Taking the total expectation simply yields

t gradf (wk

t )](cid:107)wk

t

≤ Λ(cid:107)Hk

t gradf (wk

t )(cid:107)wk

t

≤

E[h(wk

t+1)] − E[h(wk

t )] ≤ 2αk

t ΛΓncE[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] +

9
2

(αk

t )2ΛhS2Γ2

nc.

(A.27)

(cid:80)mk

t=1(αk

Recall that Proposition C.10 establishes that the ﬁrst component of this bound is the
term of a convergent sum. The second component of this bound is also the term of a con-
vergent sum since (cid:80)∞
t )2 converges. This means that again the result of Propo-
k=1
sition C.10 can be applied. Therefore, the right-hand side of (A.27) is the term of a con-
(cid:80)mk
vergent sum. Let us now deﬁne S+
K =
(cid:80)K
t=1 max(0, E[h(wk

k=1
Since the bound (A.27) is positive and forms a convergent sum, the nondecreasing sequence
S+
K is upper bounded and therefore converges. Since, for any K ∈ N, one has E[h(wK)] =
h(w1) + S+
K is upper bounded and therefore also
converges. Therefore E[h(wK)] converges. Consequently, this implies that this limit must be
zero from Proposition C.11. This completes the proof.

K ≥ 0, the nondecreasing sequence S−

K = (cid:80)K
t+1)]).

t=1 max(0, E[h(wk

t+1)] − E[h(wk

t )]), and S−

t )] − E[h(wk

K − S−

(cid:80)mk

k=1

26

C.4 Proof of global convergence rate analysis (Theorem 3.2)

The global convergence rate analysis on non-convex functions in the Euclidean SVRG is pro-
posed in [12]. Its further extensions into the stochastic L-BFGS setting and the Riemannian
setting are proposed in [15] and [23], respectively. The proof in this subsection mainly follows
that in [12] by integrating its two extensions in [15, 23]. Besides that, the special and careful
treatments for the retraction and the vector transport operations are particularly taken in
the proof. The results for the exponential mapping and the parallel translation are given in
the corresponding corollaries as a special case.

C.4.1 Preliminary lemmas

We ﬁrst present some essential lemmas.

Lemma C.12 (Lemma 6 in [35]). If a, b, and c are the side lengths of a geodesic triangle in
an Alexandrov space with curvature lower-bounded by κ, and A is the angle between sides b
and c, then

a2 ≤

(cid:112)|κ|c
tanh((cid:112)|κ|c)

b2 + c2 − 2bc cos(A).

Lemma C.13 (In the proof of Lemma 3.9 in [21]). Under Assumptions 1.1 and 1.2, there
exists a constant β > 0 such that

(cid:107)P w←z
γ

(gradf (z)) − gradf (w)(cid:107)w ≤ βdist(z, w),

(A.28)

where w and z are in Θ in Assumption 1.2 and γ is a curve γ(t) := Rz(τ η) for η ∈ TzM
deﬁned by a retraction R on M. P w←z
(·) is a parallel translation operator along the curve γ
from z to w.

γ

Note that the curve γ in this lemma is not necessarily the geodesic. The relation (A.28)
is a generalization of the Lipschitz continuity condition. In addition, we speciﬁcally use β0
when the curve is geodesic.

Lemma C.14 (Lemma 3.5 in [21]). Let T ∈ C0 be a vector transport associated with the
same retraction R as that of the parallel translation P ∈ C∞. Under Assumption 1.5, for any
¯w ∈ M there exists a constant θ > 0 and a neighborhood U of ¯w such that for all w, z ∈ U,

(cid:107)Tηξ − Pηξ(cid:107)z ≤ θ(cid:107)ξ(cid:107)w(cid:107)η(cid:107)w,

(A.29)

where ξ, η ∈ TwM and Rw(η) = z.

Modifying slightly Lemma 3 in [36], we obtain the following lemma.

Lemma C.15 (Lemma 3 in [36]). Let M be a Riemannian manifold endowed with retraction
R and let ¯w ∈ M. Then there exist τ1 > 0, τ2 > 0 and δτ1,τ2 such that for all w in a
suﬃciently small neighborhood of ¯w and all ξ ∈ TwM with (cid:107)ξ(cid:107)w ≤ δτ1,τ2, the inequalities

τ1dist(w, Rw(ξ)) ≤ (cid:107)ξ(cid:107)w ≤ τ2dist(w, Rw(ξ))

(A.30)

hold.

27

C.4.2 Essential propositions

This subsection ﬁrst presents an essential lemma about the bound of E
], where the
vector transport is carefully handled to give the lemma. Next, an important proposition C.18
is presented by extending [12,15,23]. It should be noted that we carefully treat the diﬀerence
between the exponential case and the retraction case for Proposition C.18.

t (cid:107)2
wk
t

[(cid:107)ξk

ik
t

Lemma C.16. Suppose Assumptions 1.1, 1.2, 1.3, 1.4, 1.5, and 1.7, which guarantee Lem-
mas C.13, C.14, and C.15 for ¯w = w∗. Let β > 0 be a constant such that

(cid:107)P w←z
γ

(gradfn(z)) − gradfn(w)(cid:107)w ≤ βdist(z, w),

w, z ∈ Θ, n = 1, 2, . . . , N.

(A.31)

The existence of such β is guaranteed by Lemma C.13. Then, the upper bound of the variance
of E

] is given by

[(cid:107)ξk

ik
t

t (cid:107)2
wk
t

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] ≤ 4(β2 + τ 2

2 C2θ2)(dist(wk

t , ˜wk))2 + 2(cid:107)gradf (wk

t )(cid:107)2
wk
t

.

(A.32)

Proof. The proof is partially similar to that of Lemma 5.8 in [22]. We ﬁrst consider

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] = E

ik
t

[(cid:107)gradfik

t

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk)) + T˜ηk

t

(gradf ( ˜wk))(cid:107)2
wk
t

]. (A.33)

The ﬁrst and second terms in (A.33) is

E

ik
t

[gradfik

t

which is equivalent to

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk))] = gradf (wk

t ) − T˜ηk

t

(gradf ( ˜wk)),

T˜ηk

t

(gradf ( ˜wk)) = gradf (wk

t ) − E
ik
t

[gradfik

t

Plugging this into the third term of (A.33) yields

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk))].

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] = E

[(cid:107)gradfik

t

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk))

ik
t
−E

[gradfik

t

ik
t

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk))] + gradf (wk

t )(cid:107)2
wk
t

]

≤ 2E
ik
t
−E

[(cid:107)gradfik
[gradfik

t

(wk
(wk

t ) − T˜ηk
t ) − T˜ηk

t

(gradfik
(gradfik

t

ik
t

( ˜wk))
( ˜wk))](cid:107)2
wk
t

t

t

t

≤ 2E

ik
t

[(cid:107)gradfik

t

(wk

t ) − T˜ηk

t

(gradfik

t

( ˜wk))(cid:107)2
wk
t

] + 2(cid:107)gradf (wk

] + 2(cid:107)gradf (wk

t )(cid:107)2
wk
t
t )(cid:107)2
wk
t

,

(A.34)

where the second inequality comes from the fact that, for arbitrary random vector z on
arbitrary tangent space, E[(cid:107)z − E[z](cid:107)2 = E[(cid:107)z(cid:107)2] − (cid:107)E[z](cid:107)2 ≤ E[(cid:107)z(cid:107)2]. Now, the ﬁrst term in

28

the right-hand side is upper-bounded by the distance between ˜wk and wk

t as

E

ik
t

[(cid:107)gradfik
(cid:104)

t

(cid:107)gradfik

t

(wk

t

t ) − T˜ηk
t ) − P wk

(gradfik
t ← ˜wk

t

(wk

]

( ˜wk))(cid:107)2
wk
t
( ˜wk))

(gradfik

t

E

ik
t
+P wk
(cid:104)

2E

ik
t

+2E

ik
t

t ← ˜wk

(gradfik

(wk

(cid:107)gradfik
(cid:104)
(cid:107)P wk

t
t ← ˜wk

t

( ˜wk) − T˜ηk
t
t ← ˜wk
t ) − P wk

(gradfik

t

( ˜wk))(cid:107)2
wk
t

(gradfik

t

( ˜wk))(cid:107)2
wk
t

(cid:105)

(cid:105)

(gradfik

t

( ˜wk)) − T˜ηk

t

(gradfik

t

( ˜wk))(cid:107)2
wk
t

(cid:105)

2β2(dist(wk

2β2(dist(wk

2β2(dist(wk

t , ˜wk))2 + 2E
[θ2(cid:107)˜ηk
ik
t
t (cid:107)2
t , ˜wk))2 + 2θ2(cid:107)˜ηk
wk
t
t (cid:107)2
t , ˜wk))2 + 2C2θ2(cid:107)˜ηk
wk
t

t (cid:107)2
wk
t
E

ik
t

(cid:107)gradfik

t

[(cid:107)gradfik

t

2(β2 + τ 2

2 C2θ2)(dist(wk

t , ˜wk))2,

(wk

(wk

t )(cid:107)2
wk
t
t )(cid:107)2
wk
t

]

]

(A.35)

=

≤

(A.29)
≤

=

≤

(A.30)
≤

where the ﬁrst inequality uses (cid:107)a + b(cid:107) ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 for vector a and b, and the second
inequality uses (A.29) in Lemma C.14. The third inequality uses Assumption 1.7, and the
last inequality uses (A.30) in Lemma C.15. Substituting this into (A.34) yields the claimed
statement. This complete the proof.

We obtain the counterpart result of Lemma C.16 for the parallel translation and the

exponential mapping.

Corollary C.17. Suppose Assumptions 1.1, 1.2, 1.3, 1.4, and 1.5 which guarantee Lemmas
C.13 for ¯w = w∗. Consider T = P and R = Exp, i.e., the parallel translation and the
exponential mapping case. Let β > 0 be a constant such that

(cid:107)P w←z
γ

(gradfn(z)) − gradfn(w)(cid:107)w ≤ βdist(z, w),

w, z ∈ Θ, n = 1, 2, . . . , N.

The existence of such β0 is guaranteed by Lemma C.13. Then, the upper bound of the variance
of E

] is given by

[(cid:107)ξk

ik
t

t (cid:107)2
wk
t

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] ≤ 2β2

0(dist(wk

t , ˜wk))2 + 2(cid:107)gradf (wk

t )(cid:107)2
wk
t

.

Proposition C.18. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate
local minimizer of f (i.e., gradf (w∗) = 0 and the Hessian Hessf (w∗) of f at w∗ is positive
deﬁnite). Suppose Assumption 1 holds. Assume also that wk
t+1 are suﬃciently close to
each other such that (cid:104)R−1
≤ (cid:104)Exp−1
/φ for some
wk
wk
t
t
positive constant φ. Let the constants θ be in (5), τ2 in (6), and β and C in (7). Let Λ be
the constant in Assumption 1.3, and γnc and Γnc in (8). For ck

t and wk
(wk

t+1), Exp−1
wk
t

t+1), Exp−1
wk
t

t+1, νt > 0, we set

( ˜wk)(cid:105)wk

( ˜wk)(cid:105)wk

t , ck

(wk

t

t

t = ck
ck

t+1(1 + φαk

t νt + 4ζ(αk

t )2 Γ2
nc
τ 2
1

(β2 + τ 2

2 C2θ2)) + 2(αk

t )2ΛΓ2

nc(β2 + τ 2

2 C2θ2). (A.36)

We also deﬁne

(cid:32)

∆t

:= αk
t

γnc −

φck

t+1Γ2
nc
νt

− αk

t ΛΓ2

nc − 2ck

t+1ζαk
t

(cid:33)

.

Γ2
nc
τ 2
1

(A.37)

29

t , νt and ck

Let αk
generated by Algorithm 1 with option II and with a ﬁxed step-size αk
t
converging to w∗, the expected squared norm of the Riemannian gradient, gradf (wk
the following bound as

t+1 be deﬁned such that ∆t > 0. It then follows that for any sequence { ˜wk
t }
:= α and mk := m
t ), satisﬁes

E[(cid:107)gradf (wk

t )(cid:107)2] ≤

t − V k
V k
t+1
∆t

,

(A.38)

where V k
t

:= E[f (wk

t ) + ck

t (dist( ˜wk, wk

t ))2] for 0 ≤ k ≤ K − 1.

Proof. We ﬁrst obtain the following from (A.23) in Lemma C.9 as

E[f (wk

t+1)] ≤ f (wk

t ) − αk

t (cid:104)gradf (wk

t ), Hk

≤ f (wk

t ) − αk

t γnc(cid:107)gradf (wk

t gradf (wk
(αk

t )(cid:107)2
wk
t

+

t

+

t )(cid:105)wk
t )2ΛΓ2
nc
2

(αk
t )2Λ
2

E

ik
t

[Γ2

nc(cid:107)ξk

t (cid:107)2
wk
t

]

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

].

(A.39)

Next, we bound the expected squared distance between ˜wk and wk
from Lemma C.12 as

t+1, i.e., E[(dist( ˜wk, wk

t+1))2],

E[ζ(dist(wk

t+1))2]
E[(dist( ˜wk, wk
t+1))2 + (dist(wk
t , wk
E[ζ(dist(wk
t+1))2 + (dist(wk
t , wk
t Hk
t (cid:107)2
t ξk
wk
t
τ 2
1

(cid:107) − αk

E

(cid:35)

(cid:34)

ζ

+ (dist(wk

t , ˜wk))2 − 2(cid:104)Exp−1
wk
t
t , ˜wk))2 − 2φ(cid:104)R−1
wk
t

(wk

t+1), Exp−1
( ˜wk)(cid:105)wk
wk
t
t+1), Exp−1
( ˜wk)(cid:105)wk
]
wk
t

t

t

(wk

]

t , ˜wk))2 − 2φ(cid:104)−αk

t Hk

t ξk

t , Exp−1
wk
t

( ˜wk)(cid:105)wk

t

]

E[E

[[(cid:107)ξk

t (cid:107)2
wk
t

]] + E[(dist(wk

t , ˜wk))2

ik
t

nc(cid:107)gradf (wk
Γ2

t )(cid:107)2
wk
t

+ νt(cid:107)Exp−1
wk
t

( ˜wk)(cid:107)2
wk
t

(cid:21)

E[E

[[(cid:107)ξk

t (cid:107)2
wk
t

]] + E[(dist(wk

t , ˜wk))2]

ik
t

nc(cid:107)gradf (wk
Γ2

t )(cid:107)2
wk
t

(cid:21)

+ φαk
t

E

(cid:104)
νt(cid:107)Exp−1
wk
t

( ˜wk)(cid:107)2
wk
t

(cid:105)

E[E

ik
t

[[(cid:107)ξk

t (cid:107)2
wk
t

]] +

φαk
t Γ2
nc
νt

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

] + (1 + φαk

t νt)E[(dist(wk

t , ˜wk))2],

(A.40)

where the second inequality uses (A.30) in Lemma C.15 and E
ik
t
The third inequality uses the relation 2(cid:104)a, b(cid:105) ≤ 1
(cid:107)a(cid:107)2 + νt(cid:107)b(cid:107)2.
νt
Now, we introduce the following function deﬁned as

[−Hk

t ξk

t ] = −Hk

t gradf (wk

t ).

V k
t

:= E[f (wk

t ) + ck

t (dist( ˜wk, wk

t ))2].

(A.41)

This function measures how far the current parameter wk

t is from ˜wk and the objective function

30

≤

≤

(A.30)
≤

≤

=

=

ζ(αk

E

t )2 Γ2
nc
τ 2
1
(cid:20) 1
νt
t )2 Γ2
nc
τ 2
1
(cid:20) 1
νt
t )2 Γ2
nc
τ 2
1

E

+φαk
t

ζ(αk

+φαk
t

ζ(αk

value. Then, V k

t+1 is calculated from Lemma C.16 as
V k
t+1
E[f (wk

t+1(dist( ˜wk, wk

t+1) + ck

t+1))2]

=

(A.39),(A.40)
≤

t ) − αk
E[f (wk
(cid:20)
ζ(αk

+ck

t+1

+

]] +

(αk

t )2ΛΓ2
nc
2
φαk
t Γ2
nc
νt

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

]

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

]

ik
t

φck

E[E

[[(cid:107)ξk

γnc −

t (cid:107)2
wk
t

t )(cid:107)2
wk
t

t ) − αk
t

t γnc(cid:107)gradf (wk
t )2 Γ2
nc
τ 2
1
t νt)E[(dist(wk
+(1 + φαk
(cid:32)
t+1Γ2
nc
νt
t )2 Γ2
nc
τ 2
1
t+1Γ2
nc
νt
t )2 Γ2
nc
τ 2
1

t )2ΛΓ2
nc
2

t+1ζ(αk

t+1ζ(αk

t ) − αk
t

t )2ΛΓ2
nc
2
(cid:16)

γnc −

+ ck

+ ck

φck

(cid:32)

×

4(β2 + τ 2

(cid:105)
t , ˜wk))2]
(cid:33)

(cid:107)gradf (wk

t )(cid:107)2
wk
t

E[E

ik
t

[[(cid:107)ξk

t (cid:107)2
wk
t

]]

(cid:107)gradf (wk

t )(cid:107)2
wk
t

(cid:19)

(cid:33)

(cid:19)

(cid:34)

E

=

f (wk

(A.32)
≤

+

E

+

(cid:18) (αk

(cid:34)
f (wk

(cid:18) (αk

+ ck

t+1(1 + φαk

t νt)E[(dist(wk

t , ˜wk))2]

(cid:35)

(cid:35)

+ ck

t+1(1 + φαk

t νt)E[(dist(wk

t , ˜wk))2]

(cid:34)
f (wk

t ) − αk
t

E

(cid:32)

γnc −

φck

2 C2θ2)E[(dist(wk
t+1Γ2
nc
νt
t )2 Γ2
nc
τ 2
1

t , ˜wk))2] + 2E[(cid:107)gradf (wk
t )(cid:107)2
wk
t
(cid:33)

(cid:17)

].

(cid:35)

− αk

t ΛΓ2

nc − 2ck

t+1ζαk
t

(cid:107)gradf (wk

t )(cid:107)2
wk
t

Γ2
nc
τ 2
1

+(ck

t+1(1 + φαk

t νt + 4ζ(αk

(β2 + τ 2

2 C2θ2)) + 2(αk

t )2ΛΓ2

nc(β2 + τ 2

2 C2θ2))

(cid:32)

t − αk
V k
t

×E[(dist(wk

t , ˜wk))2]
t+1Γ2
φck
nc
ν
t )(cid:107)2
t − ∆tE[(cid:107)gradf (wk
V k
wk
t

γnc −

− αk

t ΛΓ2

nc − 2ck

t+1ζαk
t

(cid:33)

Γ2
nc
τ 2
1

E[(cid:107)gradf (wk

t )(cid:107)2
wk
t

]

].

(A.42)

=

=

=

Rearranging the above yields the claim. This completes the proof.

We obtain the counterpart result of Proposition C.18 for the parallel translation and the

exponential mapping.
Corollary C.19. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate local
minimizer of f . Consider Algorithm 1 with option II and with T = P and R = Exp, i.e., the
parallel translation and the exponential mapping case. Suppose Assumption 1 holds. Let the
constants θ be in (5), τ2 in (6), and β0, and C in (7). Λ is the constant in Assumption 1.3,
and γnc and Γnc are the constants γ and Γ in (8). For ck
ncβ2

t , ck
0) + 2(αk

t+1, νt > 0, we set

t+1(1 + νtαk

t = ck
ck

t + 2ζ(αk

t )2ΛΓ2

t )2Γ2

ncβ2
0.

(A.43)

We also deﬁne

(cid:32)

∆t

:= αk
t

γnc −

t+1Γ2
ck
nc
νt

− αk

t ΛΓ2

nc − 2ck

t+1ζαk

t Γ2
nc

(cid:33)

.

(A.44)

31

t , νt and ck

Let αk
generated by Algorithm 1 with a ﬁxed step-size αk
expected squared Riemannian gradient satisﬁes the following bound as

t+1 be deﬁned such that ∆t > 0. It then follows that, for any sequence { ˜wk
t }
t := α and mk := m converging to w∗, the

E[(cid:107)gradf (wk

t )(cid:107)2] ≤

t − V k
V k
t+1
∆t

,

(A.45)

where V k
t

:= E[f (wk

t ) + ck

t (dist( ˜wk, wk

t ))2] for 0 ≤ k ≤ K − 1.

The following proposition is very similar to Theorem 2 in [12].

Proposition C.20 (Theorem 2 in [12]). Let M be a Riemannian manifold and w∗ ∈ M
be a non-degenerate local minimizer of f . Consider Algorithm 1 with option II and IV, and
suppose Assumption 1 holds. Let the constants θ be in (5), τ1 and τ2 in (6), and β, and C
in (7). Λ is the constant in Assumption 1.3, and γnc and Γnc are the constants γ and Γ in
(8). Let cm = 0, αk
t is deﬁned as (A.36) such that ∆t deﬁned
in (A.37) satisﬁes ∆t > 0 for 0 ≤ t ≤ m − 1. Deﬁne δt := mint∆t. Let T be mK. It then
follows that, for the output wsol of Algorithm 1, we have

t = α > 0, νt = ν > 0, and ck

E[(cid:107)gradf (wsol)(cid:107)2] ≤

f (w0) − f (w∗)
T δt

.

(A.46)

Proof. Because the proof is identical to those in [12, 15, 23], we omit it. The complete proof
is therein. The sketch of the proof is as follows; we ﬁrst telescoping the sum of (A.45) from
t = 0 to t = m − 1 by introducing δt, then estimate its upper bound from the diﬀerence
between V s
0 deﬁned in (A.41). After showing that this diﬀerence is equivalent to the
expected diﬀerence between f ( ˜wk) and f ( ˜wk+1), summing up from k = 0 to k = K − 1, we
obtain the desired claim.

0 and V m

C.4.3 Main proof of Theorem 3.2

We ﬁnally present the proof of Theorem 3.2 based on the extensions of results in [12, 15, 23].
Theorem. 3.2. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate local
minimizer of f . Consider Algorithm 1 with option II and IV, and suppose Assumption 1 holds.
Let the constants θ in (5), τ1 and τ2 in (6), and β, and C in (7). Λ is the constant in As-
ζ 1−a2
, where 0 < a1 < 1, and 0 < a2 < 2. Given suﬃciently

sumption 1.3, and γnc and Γnc are the constants γ and Γ in (8). Set ν =
and αk

2 C2θ2Γnc

N a1/2τ1

β2+τ 2

µ0τ1

√

√

t = α =

2 C2θ2N a1 Γncζa2
small µ0 ∈ (0, 1), suppose that (cid:37) > 0 is chosen such that

β2+τ 2

(cid:112)β2 + τ 2
ΛΓnc

2 C2θ2

(cid:18)

γnc

1 −

(cid:19)

(cid:37)Γnc
µ0γnc

>

2φµ0(e − 1)τ1
ζ 2−a2

+

µ0τ1
N a1ζ a2

+

holds. Set m = (cid:98) N 3a1/2

5φ1µ0ζ1−2a2 (cid:99) and T = mK. Then, we have

4µ2

N

0(e − 1)
3a1
2 ζ a2τ1

(A.47)

E[(cid:107)gradf (wa)(cid:107)2] ≤

(cid:112)β2 + τ 2

2 C2θ2N a1ζ a2[f (w0) − f (w∗)]

T (cid:37)

.

(A.48)

32

Proof. From (A.37) in Proposition C.18, we need to consider the upper bound of ck
in (A.36). To this end, the upper bound of ck
ϕ = φαν + 4ζα2 Γ2
(β2 + τ 2
nc
τ 2
1

2 C2θ2) and ω = τ2Cθ, we ﬁrst consider the bound of ϕ as

t deﬁned
0 is ﬁrst derived. Denoting, for simplicity,

ϕ = φαν + 4ζα2 Γ2
nc
τ 2
1
4µ2

φµ0ζ 1−2a2
3a1
2

N

+

=

(β2 + ω2)

0ζ 1−2a2
N 2a1

=

φµ0ζ 1−2a2
3a1
2

N

(cid:32)

1 +

(cid:33)

.

4µ0

1
2a1

(cid:17)

.

φN
(cid:16) φµ0ζ1−2a2
N 3a1/2

, 5 φµ0ζ1−2a2
N 3a1/2
t+1(1 + ϕ) + 2α2Λ Γ2
nc
τ 2
1
(cid:19)

2α2ΛΓ2

(1 + ϕ)m +

(β2 + ω2) as

nc(β2 + ω2)
−ϕτ 2
1

Consequently, we obtain the bound of ϕ as ϕ ∈

Then, we consider the recurrence relation ck

t = c2

ck
0 =

(cid:18)

ck
m −

2α2ΛΓ2

nc(β2 + ω2)
−ϕτ 2
1
(1 + ϕ)m − 1
ϕ

= 2α2Λ

(β2 + ω2)

Γ2
nc
τ 2
1
µ2
0Λ
N 2a1ζ 2a2
µ0Λ
a1
2 ζ
N
µ0Λ
a1
2 ζ
N

= 2

≤ 2

≤ 2

(1 + ϕ)m − 1
ϕ

((1 + ϕ)m − 1)

(e − 1),

(A.49)

where the second equality uses ck
m = 0, and the ﬁrst inequality uses the lower bound of
ϕ derived above. Regarding the last inequality, because m = (cid:98) N 3a1/2
In
addition, noting that limr→∞(1 + 1/r)r = e for r > 0 where e is the Euler’s number, we used
the relation (1 + ϕ)m < e.
Now, we attempt to estimate the lower bound of δt, i.e., mint∆t.

5φµ0ζ1−2a2 (cid:99), ϕ ≤ 1/m.

δt

= min

t

∆t
(cid:32)

= min

t

α

γnc −

φck

t+1Γ2
nc
ν

− αΛΓ2

nc − 2ck

t+1ζα

(cid:33)

Γ2
nc
τ 2
1

(cid:18)

≥

α

γnc −

φck
0Γ2
nc
ν

− αΛΓ2

nc − 2ck

0ζα

(cid:19)

Γ2
nc
τ 2
1

(A.49)
≥

(cid:32)

α

γnc −

2φµ0(e − 1)
ζ 2−a2

Λ
(cid:112)β2 + ω2
(cid:33)

Γncτ1 −

µ0
N a1ζ a2

Λ
(cid:112)β2 + ω2

Γncτ1

−

4µ2

0(e − 1)
3a1
2 ζ a2

N

Λ
(cid:112)β2 + ω2

Γnc
τ1

(A.47)
≥

(cid:37)
(cid:112)β2 + ω2N a1ζ a2

,

33

(A.50)

where the second inequality uses (A.49) for the second and the fourth terms. Substituting
(A.50) into (A.46) in Proposition C.20 completes the proof.

Corollary C.21. Suppose the same assumptions and conditions as those of Theorem 3.2.
Then, the total number of gradient evaluations in Algorithm 1 is O(N a1/(cid:15)) to obtain an
(cid:15)-solution.

Proof. The total number of gradient evaluations is equals to (N + m)K. Comparing the left
term in (A.48) with (cid:15), we obtain K ≈ O(N a1/(m(cid:15))). Additionally, m = (cid:98) N 3a1/2
5µ0ζ1−2a2 (cid:99) ≈
O(N 3a1/2). Consequently,
it results in that the total number of gradient evaluations is
O(N a1/(cid:15)).

The obtained complexity is the same as that of R-SVRG [23] in terms of the total number

of samples, N .

We obtain the corresponding result of Theorem 3.2 when the parallel translation and the

exponential mapping are used.

Corollary C.22. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate local
minimizer of f . Consider Algorithm 1 with option II and IV and with T = P and R = Exp,
i.e., the parallel translation and the exponential mapping case. Let the constant β0 be β in
(7). Λ is the constant in Assumption 1.3, and γnc and Γnc are the constants γ and Γ in (8).
Set ν = β0Γncζ1−2a2
β0N a1 Γncζa2 , where 0 < a1 < 1, and 0 < a2 < 2. Given
suﬃciently small µ0 ∈ (0, 1), suppose that (cid:37) > 0 is chosen such that

t = α =

and αk

N a1/2

µ0

β0
ΛΓnc

(cid:18)

γnc

1 −

(cid:19)

(cid:37)Γnc
µ0γnc

>

µ0(e − 1)
ζ 2−a2

+

µ0
N a1ζ a2

+

2µ2
0(e − 1)
N 3a1/2ζ a2

holds. Set m = (cid:98) N 3a1/2

3µ0ζ1−2a2 (cid:99), and T = mK. Then, we have

E[(cid:107)gradf (wa)(cid:107)2] ≤

β0N a1ζ a2[f (w0) − f (w∗)]
T (cid:37)

.

Proof. The proof is similar to that of Theorem 3.2. Therefore, we omit it. However, it
should be noted that we follow Corollary C.17 to bound E
] instead of using Lemma
C.16.

t (cid:107)2
wk
t

[(cid:107)ξk

ik
t

34

D Proof of convergence analysis on retraction-convex func-

tions

This subsection presents a local convergence rate analysis in neighborhood of a local minimum
for retraction-convex functions. This local setting is very common and standard in manifold
optimization.

D.1 Preliminary lemmas

This subsection ﬁrst states some essential lemmas. Since f is strongly retraction-convex on

Θ by Assumption 3, there exist constants 0 < λ such that λ ≤
t ∈ [0, αk

]. From Taylor’s theorem, we obtain below;

t (cid:107)ηk(cid:107)wk

t

d2f (R

wk
t

(tηk/(cid:107)ηk(cid:107)
dt2

wk
t

))

for all

Lemma D.1 (In Lemma 3.2 in [21]). Under Assumptions 1.1, 1.2, and Assumption 3, there
exists λ such that

f (wk

t+1) − f (wk

t ) ≥ (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

t

+

1
2

Proof. From Taylor’s theorem, we have

λ(αk

t (cid:107)ηk(cid:107)wk

t

)2.

(A.51)

f (wk
= f (Rwk
d
dτ

=

t

t+1) − f (wk
t )
t ηk)) − f (Rwk

(αk

(0))

t

f (Rwk

t

(τ ηk/(cid:107)ηk(cid:107)wk

t

= (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

+

≥ (cid:104)gradf (wk

t ), αk

t ηk(cid:105)wk

+

t (cid:107)ηk(cid:107)wk

t

· αk

(cid:12)
(cid:12)
))
(cid:12)τ =0
d2
1
dτ 2 f (Rwk
2
1
λ(αk
t (cid:107)ηk(cid:107)wk
2

t

t

)2,

t

t

+

1
2

d2
dτ 2 f (Rwk
(cid:12)
(cid:12)
))
(cid:12)τ =p

t

t

(τ ηk/(cid:107)ηk(cid:107)wk

t

(cid:12)
(cid:12)
))
(cid:12)τ =p

· (αk

t (cid:107)ηk(cid:107)wk

t

)2

· (αk

t (cid:107)ηk(cid:107)wk

t

)2

(τ ηk/(cid:107)ηk(cid:107)wk

where 0 ≤ p ≤ αk
completes the proof.

t (cid:107)ηk(cid:107)wk

t

, and the inequality uses Assumption 3. This yields (A.51). This

Lemma D.2 (Lemma 3.3 in [21]). Under Assumptions 1.1, 1.2, 1.3, 1.4, 1.6, and Assumption
3, there exist two constants 0 < λ < Λ such that

λ ≤

(cid:104)sk, yk(cid:105)
(cid:104)sk, sk(cid:105)

≤ Λ

(A.52)

for all k.

Proof. From Lemma D.1, the proof of this lemma is given, but we omit it. The reader can
see the complete proof in Lemma 3.3 in [21].

Lemma D.3 (Lemma 3.9 in [21]). Suppose Assumption 1 and Assumption 3 hold, there exists
a constant 0 < Υc for all k such that

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

≤ Υc.

35

(A.53)

Proof. (Lemma 3.9 in [21]) The complete proof is given in [21]. It is also stated in Lemma C.3
for completeness. (A.4) yields (cid:107)yk(cid:107) ≤ b4(cid:107)sk(cid:107) derived in Lemma C.3, where b4 > 0. Therefore,
by Lemma D.2, we have

(cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

≤

(cid:104)yk, yk(cid:105)
λ(cid:104)sk, sk(cid:105)

≤

b2
4
λ

(= Υc).

(A.54)

Denoting b2

4/λ as Υc, this completes the proof.

t on retraction-convex functions

D.2 Eigenvalue bounds of Hk
Now, we attempt to bound trace( ˆ˜B) and det( ˆ˜B) in order to bound the eigenvalues of ˜Hk,
where a hat denotes the coordinate expression of the operator. The bound of trace( ˆ˜B) is
identical to that of the non-convex case in Lemma C.5. Therefore, we concentrate on the
bound of det( ˆ˜B). As the same as Lemma C.5, the proof follows stochastic L-BFGS methods
in the Euclidean space, e.g., [16,24]. Similarly to Section C.2, it should be noted that trace( ˆ˜B)
and det( ˆ˜B) do not depend on the chosen basis.

Lemma D.4 (Bounds of trace and determinant of ˜Bk). Consider the recursion of ˜Bk
u deﬁned
in (A.7). If Assumption 1 and Assumption 3 hold, the trace( ˆ˜Bk) in a coordinate expression
of ˜Bk is uniformly upper bounded for all k ≥ 1,

trace( ˆ˜Bk) ≤ (M + τ )Υc.

(A.55)

where M is the dimension ofM. Similarly, if Assumption 1 and Assumption 3 hold, the
det( ˆ˜Bk) in a coordinate expression of ˜Bk is uniformly lower bounded for all k,

det( ˆ˜Bk) ≥ υM

(cid:20)

λ
(M + τ )Υc

(cid:21)τ

.

(A.56)

Here, a hat expression represents the coordinate expression of an operator.

Proof. The proof can be completed parallel to the Euclidean case [17]. As mentioned, the
proof for the bound of trace( ˆ˜B) is given in Lemma C.5, we address only det( ˆ˜Bk).

Because T is an isometry vector transport, Tηk is invertible for all k. Accordingly, det( ˆ˜Bk)

can be reformulated as

det( ˆˇBk) = det( ˆTηk

ˆ˜Bk ˆT −1
ηk

) = det( ˆ˜Bk).

(A.57)

36

We consider the determinant lower bound of det( ˆ˜Bk,τ ) from (A.7) as


det( ˆ˜Bk

u+1) = det( ˆˇBk

u)det

I −

( ˆˇBk

+

u)−1yk−τ +uyT
(cid:104)yk−τ +u, sk−τ +u(cid:105)

k−τ +u

usk−τ +u)T
usk−τ +u, sk−τ +u(cid:105)

sk−τ +u( ˆˇBk
(cid:104) ˆˇBk
( ˆˇBk
usk−τ +u)T
usk−τ +u, sk−τ +u(cid:105)

(cid:104) ˆˇBk

(cid:33)

( ˆˇBk

u)−1yk−τ +u

(cid:32)

= det( ˆˇBk

u)det

= det( ˆˇBk
u)

= det( ˆˇBk
u)

≥ det( ˆˇBk
u)

≥ det( ˆˇBk
u)

≥ det( ˆˇBk
u)

(cid:107)sk−τ +u(cid:107)2
usk−τ +u, sk−τ +u(cid:105)

(cid:104) ˆˇBk

usk−τ +u, sk−τ +u(cid:105)

(cid:104)sk−τ +u, yk−τ +u(cid:105)
(cid:104) ˆˇBk
(cid:104)sk−τ +u, yk−τ +u)
(cid:107)sk−τ +u(cid:107)2
λ
λmax( ˆBk
u)
λ
trace( ˆBk
u)
λ
(M + τ )Υc

.





(A.58)

1 v1)(uT

2 v2) by setting u1 = −sk−τ +u, v1 = ˆBk

Regarding the second equality, we obtain it from the formula det(I + u1vT
usk−τ +u/(cid:104) ˆBk

1 v1)(1+uT
2 v2)−(uT
uT
−1
u2 = ( ˆBk
yk−τ +u, and v2 = yk−τ +u/(cid:104)sk−τ +u, yk−τ +u(cid:105). The ﬁrst inequality follows from
u)
usk−τ +u, sk−τ +u(cid:105) ≤ λmax( ˆBk
(A.52) in Lemma D.2 and the fact (cid:104) ˆBk
u)(cid:107)sk−τ +u(cid:107)2. Actually, we
use the fact the trace of a positive deﬁnite matrix bounds its maximal eigenvalue for the
second inequality. The last inequality follows (A.11). Then, applying (A.57), (A.58) turns to
be

1 + u2vT
usk−τ +u, sk−τ +u(cid:105),

2 ) = (1 +

det( ˆ˜Bk

u+1) ≥ det( ˆ˜Bk
u)

λ
(M + τ )Υc

.

(A.59)

Applying (A.59) recursively from u = 0 to u = τ − 1, we obtain that

det( ˆ˜Bk

τ ) ≥

(cid:20)

λ
(M + τ )Υc

(cid:21)τ

det( ˆ˜Bk
0 ).

To bound the determinant of ˆ˜Bk

0 , considering ˆ˜Bk

0 = id/χk as above and Lemma C.2, we

can rewrite for k ≥ 1 as

det( ˆ˜Bk

0 ) = det

(cid:19)

(cid:18) I
χk

=

1
χM
k

=

(cid:19)M

(cid:18) (cid:104)yk, yk(cid:105)
(cid:104)sk, yk(cid:105)

≥ υM ,

where υ is deﬁned in Lemma C.2. Consequently, we obtain as

det( ˆ˜Bk

τ ) ≥ υM

(cid:20)

λ
(M + τ )Υc

(cid:21)τ

.

Thus, this yields (A.56), and these complete the proof.

37

Now we prove the main lemma for Proposition D.6.

Lemma D.5. If Assumption 1 and Assumption 3 hold, the eigenvalues of ˜Hk is bounded by
γc and Γc with 0 < γc < Γc < ∞ for all k ≥ 1 as

γcid (cid:22) ˜Hk (cid:22) Γcid.

Proof. The proof is obtained as parallel to the Euclidean case [24]. The lower part is identical
to the proof that is given in Lemma C.5. Regarding the upper bound, because the determinant
of a matrix is the product of its eigenvalues, the lower bound in (A.56) bounds the product
of the eigenvalues of ˆ˜Bk from below. This means that (cid:81)M
[(M +τ )Υc]τ . Thus, we have
below for any given eigenvalue of ˆ˜Bk, say λj,

i=1 λi ≥ λτ υM

λj ≥

1
k=1,k(cid:54)=j λk

(cid:81)M

·

λτ υM
[(M + τ )Υc]τ .

(A.60)

Considering that (M + τ )Υc is an upper bound for the eigenvalues of ˆ˜Bk, [(M + τ )Υc]M −1
gives the upper bound of the product of the (M − 1) eigenvalues (cid:81)M

k=1,k(cid:54)=j λk.

As a result, we obtain that any eigenvalues of ˆBk is lower bounded as

λj ≥

1
[(M + τ )Υc]M −1 ·

λτ υM
[(M + τ )Υc]τ =

λτ υM
[(M + τ )Υc]M +τ −1 .

(A.61)

[(M +τ )Υc]M +τ −1 id (cid:22) ˜Bk.
Consequently, we ﬁnally obtain
Now, we obtain the claim. The bounds in (A.55) and (A.61) imply that their inverses are

λτ υM

bounds for the eigenvalues of ˆ˜Hk = ( ˆ˜Bk)−1 as

(γcid =)

1
(M + τ )Υc

id (cid:22) ˜Hk (cid:22)

[(M + τ )Υc]M +τ −1
λτ υM

id (= Γcid).

(A.62)

Denoting
This completes the proof.

1
(M +τ )Υc

as γc as in Lemma C.6, and [(M +τ )Υc]M +τ −1

λτ υM

as Γc, we obtain the claim.

Finally we give Proposition D.6 for retraction-convex functions.

Proposition D.6 (Bounds of Hk
t
ˇHk := T˜ηk
◦ ˜Hk ◦ (T˜ηk
range of eigenvalues of Hk

t

t

for retraction-convex functions). Consider the operator
)−1. Deﬁne the constant 0 < γc < Γc < ∞. If Assumption 1 holds, the

t is bounded by γc and Γc for all k ≥ 1, t ≥ 1, i.e.,

γcid (cid:22) Hk

t (cid:22) Γcid.

(A.63)

Proof. The proof is identical to that of Proposition C.7.

Remark D.7. We discuss the obtained bounds of Hk
t by comparing the retraction-convex
case in Proposition D.6 with the non-convex case in Proposition C.7. The lower bound of Hk
t
in the convex case is γc = 1/((M + τ )Υc) = λ/((M + τ )b2
4) from (A.53) and (A.62). The
non-convex case is γnc = 1/((M + τ )Υnc) = (cid:15)/((M + τ )b2
4) from (A.6) and (A.13). In terms
of (cid:15) and λ, γc is O(λ) and γnc is O((cid:15)). Assuming λ of the strongly retraction-convex functions

38

much larger than (cid:15) because it is generally set to very small value [27], we conclude γc > γnc.
4]M +τ −1
Meanwhile, the upper bound of Hk
from (A.53) and (A.62). The non-convex case is Γnc = (1+b4/(cid:15))2(τ +1)−1
from (A.18). With
(cid:15)((1+b4/(cid:15))2−1)
respect to (cid:15) and λ, Γc is O(1/(λM +2τ −1)(cid:15)M ) and Γnc is O(1/(cid:15)2τ ). Similarly to the lower
bound mentioned above, we conclude Γc < Γnc. Consequently, the range of the bounds of Hk
t
on strongly retraction-convex functions is smaller than that on non-convex functions.

t in the convex case is Γc = [(M +τ )Υc]M +τ −1

= [(M +τ )b2

λM +2τ −1(cid:15)M

λτ υM

D.3 Proof of local convergence rate analysis (Theorem 3.3)

This subsection ﬁrst introduces some essential lemmas. Then, the main proof of Theorem 3.3
is given. This section also derives at the end a corollary about the analysis when the using
exponential mapping and the parallel translation that are special cases of the retraction and
the vector transport.

D.3.1 Essential lemmas

We ﬁrst introduce a property of the Karcher mean on a general Riemannian manifold.

Lemma D.8 (Lemma C.2 in [22]). Let w1, . . . , wm be points on a Riemannian manifold M
and let w be the Karcher mean of the m points. For an arbitrary point p on M, we have

(dist(p, w))2 ≤

4
m

m
(cid:88)

i=1

(dist(p, wi))2.

We now bound the variance of ξk

t as follows.

Lemma D.9 (Lemma 5.8 in [22]). Suppose Assumptions 1.1, 1.2, 1.4, 1.5, and 1.7, which
guarantee Lemmas C.13, C.14, and C.15 for ¯w = w∗. Let β > 0 be a constant such that

(cid:107)P w←z
γ

(gradfn(z)) − gradfn(w)(cid:107)w ≤ βdist(z, w),

w, z ∈ Θ, n = 1, 2, . . . , N.

The existence of such β is guaranteed by Lemma C.13. The upper bound of the variance of
ξk
t is given by

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] ≤ 4(β2 + τ 2

2 C2θ2)(7(dist(wk

t , w∗))2 + 4(dist( ˜wk, w∗))2),

(A.64)

where the constant θ corresponds to that in Lemma C.14, C is the constant of Assumption 1,
and τ2 > 0 appears in (A.30).

We also have the following corollary of the previous lemma with the case R = Exp and

T = P .

Corollary D.10 (Corollary 5.1 in [22]). Consider Algorithm 1 with T = P and R = Exp, i.e.,
the parallel translation and the exponential mapping case. When each gradfn is β0-Lipschitz
continuously diﬀerentiable, the upper bound of the variance of ξk

t is given by
t , w∗))2 + 8dist( ˜wk, w∗))2).

(A.65)

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

] ≤ β2

0(14(dist(wk

Next, we show the lemma that ﬁnds a lower bound for (cid:107)gradf (wk

with respect to the
t ) − f (w∗), which is a standard derivation in the Euclidean space. See, e.g., [37].

t )(cid:107)wk

t

error f (wk
We extend this into manifolds.

39

Lemma D.11. Let w ∈ M and z be in a totally retractive neighborhood of w. It holds that

2λ(f (w) − f (z)) ≤ (cid:107)gradf (w)(cid:107)2
w.

(A.66)

Proof. Let ζ = R−1
convexity of gradf , we obtain

w (z). Using (A.51) in Lemma D.1, which is equivalent to the strong

f (z) ≥ f (w) + (cid:104)gradf (w), ζ(cid:105)w +

(cid:18)

λ
2

(cid:107)ζ(cid:107)2
w

≥ f (w) + min

(cid:104)gradf (w), ξ(cid:105)w +

≥ f (w) −

(cid:107)gradf (w)(cid:107)2
w.

ξ∈TwM
1
2λ

(cid:19)

λ
2

(cid:107)ξ(cid:107)2
w

Rearranging this inequality completes the proof.

D.3.2 Main proof of Theorem 3.3

Theorem 3.3. Let M be a Riemannian manifold and w∗ ∈ M be a non-degenerate local
minimizer of f (i.e., gradf (w∗) = 0 and the Hessian Hessf (w∗) of f at w∗ is positive deﬁnite).
Suppose Assumptions 1 and 3 hold. Let the constants β, θ, and C be in Lemma D.9, and τ1
and τ2 be in Lemma C.15. Λ and λ are the constants in Lemmas C.1 and D.1, respectively.
γc and Γc are the constants in Proposition D.6. Let α be a positive number satisfying λτ 2
1 >
2 C2θ2). It then follows that for
2α(λ2τ 2
any sequence { ˜wk} generated by Algorithm 1 with Option II under a ﬁxed step-size αk
t := α
and mk := m converging to w∗, there exists 0 < Kth < K such that for all k > Kth,

2 C2θ2)) and γcλ2τ 2

1 > 14αΛΓ2

1 −14αΛΓ2

c(β2 +τ 2

c(β2 +τ 2

E[(dist( ˜wk+1, w∗))2] ≤

2(Λτ 2
mα(γcλ2τ 2

2 + 16mα2ΛΓ2
1 − 14αΛΓ2

c(β2 + τ 2
c(β2 + τ 2

2 C2θ2)
2 C2θ2))

E[(dist( ˜wk−1, w∗))2].

Proof. Using (A.1) in Lemma C.1, which is equivalent to the Lipschitz continuity of gradf
from Assumptions 1, we obtain

f (wk

t+1) − f (wk

t ) ≤ (cid:104)gradf (wk

t ), −αHk

t ξk

t (cid:105)wk

t

+

1
2

Taking expectation with regard to ik

t , this becomes

Λ(−α(cid:107)Hk

t ξk

t (cid:107)wk

t

)2.

E

ik
t

[f (wk

t+1)] − f (wk

t ) ≤ E
ik
t

[(cid:104)gradf (wk

t ), −αHk

t ξk

t (cid:105)wk

t

≤ −α(cid:104)gradf (wk

t ), E
ik
t

[Hk

t ξk

t ](cid:105)wk

t

+

+

1
2
1
2

α2Λ(cid:107)Hk

t ξk

t (cid:107)2
wk
t

]

α2ΛE

ik
t

[(cid:107)Hk

t ξk

t (cid:107)2
wk
t

]

≤ −α(cid:104)gradf (wk

t ), Hk

t )(cid:105)wk

t

+

α2ΛE

ik
t

[(cid:107)Hk

t ξk

t (cid:107)2]

≤ −αγc(cid:107)gradf (wk

α2ΛΓ2
c

E

ik
t

t (cid:107)2
wk
t

].

(A.67)

t gradf (wk
1
2

+

t )(cid:107)2
wk
t

1
2
[(cid:107)ξk

where the third inequality used the fact that E
used the bound of Hk

t in Proposition D.6.

ik
t

40

[Hk

t ξk

t ] = Hk

t gradf (wk

t ). The last inequality

From Lemma D.11, (A.67) yields

E

ik
t

[f (wk

t+1)] − f (wk

t ) ≤ −2αγcλ(f (wk

t ) − f (w∗)) +

1
2

α2ΛΓ2
c

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

].

(A.68)

Using (A.51) in Lemma D.1 with gradf (w∗) = 0, and using Lemma C.15, we obtain

f (wk

t ) − f (w∗) ≥

λ
2

(cid:107)R−1

w∗ (wk

t )(cid:107)2

w∗ ≥

λτ 2
1
2

(dist(wk

t , w∗))2.

(A.69)

Plugging (A.69) and the bound of E

ik
t

[(cid:107)ξk

k (cid:107)2 in (A.64) in Lemma D.9 into (A.68) yields

E

ik
t

[f (wk

t+1)] − f (wk

1
2

α2ΛΓ2
c

E

ik
t

[(cid:107)ξk

t (cid:107)2
wk
t

]

t , w∗))2 +
t , w∗))2

t ) ≤ −αγcλ2τ 2
≤ −αγcλ2τ 2
1
α2ΛΓ2
2
≤ (−αγcλ2τ 2
+8α2ΛΓ2

1 (dist(wk
1 (dist(wk
c{4(β2 + τ 2
1 + 14α2ΛΓ2
c(β2 + τ 2

+

2 C2θ2)(7(dist(wk

t , w∗))2 + 4(dist( ˜wk, w∗))2)}
t , w∗))2

2 C2θ2))(dist(wk

c(β2 + τ 2

2 C2θ2)(dist( ˜wk, w∗))2.

Taking expectations over all random variables, we obtain below by further summing over

t = 0, . . . , m − 1 of the inner loop on k-th epoch

E[f (wk

m) − f (wk

0 )] ≤ −(αγcλ2τ 2

1 − 14α2ΛΓ2

c(β2 + τ 2

2 C2θ2))

m−1
(cid:88)

t=0

E[(dist(wk

t , w∗))2]

+8mα2ΛΓ2

c(β2 + τ 2

2 C2θ2)E[(dist( ˜wk, w∗))2].

(A.70)

Here, considering the diﬀerence with the solution w∗ in terms of the cost function value,

we obtain

E[f (wk

m) − f (wk

0 )] = E[f (wk
E[λτ 2

≥

m) − f (w∗) − (f (wk
1 (dist(wk

m, w∗))2 − Λτ 2

0 ) − f (w∗))]
2 (dist(wk

0 , w∗))2].

1
2

Plugging the above into (A.70) yields

E[λτ 2

1 (dist(wk

m, w∗))2 − Λτ 2

2 (dist(wk

0 , w∗))2]
m−1
(cid:88)

≤ −2α(γcλ2τ 2

1 − 14αΛΓ2

c(β2 + τ 2

2 C2θ2))

E[(dist(wk

t , w∗))2]

+16mα2ΛΓ2

c(β2 + τ 2

2 C2θ2)E[(dist( ˜wk, w∗))2].

t=0

Rearranging this gives

2α(γcλ2τ 2

1 − 14αΛΓ2

c(β2 + τ 2

2 C2θ2))

m−1
(cid:88)

E[(dist(wk

t , w∗))2]

≤ E[Λτ 2

2 (dist(wk

+16mα2ΛΓ2

0 , w∗))2] − E[λτ 2
c(β2 + τ 2

t=0
m, w∗))2]
1 (dist(wk
2 C2θ2)E[(dist( ˜wk, w∗))2].

(A.71)

41

Now, addressing option I in Algorithm 1, which uses ˜wk+1 = gmk (wk

1 , . . . , wk

m), we derive

below from Lemma D.8 as

≤

(A.71)
≤

≤

2α(γcλ2τ 2

m
4
2α(γcλ2τ 2

1 − 14αΛΓ2
(cid:20)m−1
(cid:88)

t=0

1 − 14αΛΓ2

c(β2 + τ 2

2 C2θ2))E[(dist( ˜wk+1, w∗))2]

c(β2 + τ 2

2 C2θ2))

×E

(dist(wk

t , w∗))2 + (dist(wk

m, w∗))2 − (dist(wk

0 , w∗))2

(cid:21)

2 (dist(wk

E[Λτ 2
+16mα2ΛΓ2
+2α(γcλ2τ 2
(Λτ 2
+16mα2ΛΓ2
−(λτ 2

m, w∗))2]
1 (dist(wk
2 C2θ2)E[(dist( ˜wk, w∗))2]

0 , w∗))2] − E[λτ 2
c(β2 + τ 2
1 − 14αΛΓ2

c(β2 + τ 2

2 C2θ2))E[(dist(wk

2 − 2α(γcλ2τ 2

c(β2 + τ 2

1 − 14αΛΓ2

c(β2 + τ 2
2 C2θ2)E[(dist( ˜wk, w∗))2)
c(β2 + τ 2

1 − 14αΛΓ2

1 − 2α(γcλ2τ 2

2 C2θ2)))E[(dist(wk

m, w∗))2].

2 C2θ2)))E[(dist(wk

m, w∗))2 − (dist(wk
0 , w∗))2]

0 , w∗))2]

Combining the relation Λτ 2

2 > λτ 2

1 and the assumption λτ 2

1 > 2α(γcλ2τ 2

1 − 14α)ΛΓ2

c(β2 +

2 C2θ2)), since wk
τ 2

0 = ˜wk, we obtain

m
4
≤ (Λτ 2

2α(γcλ2τ 2
2 + 16mα2ΛΓ2

1 − 14αΛΓ2

c(β2 + τ 2

2 C2θ2))E[(dist( ˜wk+1, w∗))2]

c(β2 + τ 2

2 C2θ2))E[(dist( ˜wk, w∗))2].

Finally, we obtain

E[(dist( ˜wk+1, w∗))2] ≤

This completes the proof.

2(Λτ 2
mα(γcλ2τ 2

2 + 16mα2ΛΓ2
1 − 14αΛΓ2

c(β2 + τ 2
c(β2 + τ 2

2 C2θ2)
2 C2θ2))

E[(dist( ˜wk, w∗))2).

(A.72)

Remark D.12. From the proof of Lemma D.3, if we adopt the parallel translation as the
vector transport, i.e., T = P , the ﬁrst two terms in (A.4) are equal to zero, and Υc in (A.53)
gets smaller than that of the case of vector transport. This leads to a smaller Γc and a larger
γc in Proposition D.6. Then, the smaller Γc and the larger γc leads to a smaller coeﬃcient
in (A.72) of Theorem 3.3. Consequently, the parallel translation can result in a faster local
convergence rate.

We obtain the following corollary of the previous theorem with the case R = Exp and

T = P .

Corollary D.13. Consider Algorithm 1 with T = P and R = Exp, i.e., the parallel trans-
lation and the exponential mapping case. Let M be a Riemannian manifold and w∗ ∈ M be
a non-degenerate local minimizer of f (i.e., gradf (w∗) = 0 and the Hessian Hessf (w∗) of f
at w∗ is positive deﬁnite). Suppose Assumptions 1 and 3 hold. Let the constants θ, and C
in Lemma D.9. β0 is the constant in Corollary D.10. Λ and λ are the constants in Lemmas
C.1 and D.1, respectively. γc and Γc are the constants in Proposition D.6. Let α be a positive
number satisfying λ > 2α(γcλ2 − 7αΛΓ2
2 C2θ2). It then
follows that for any sequence { ˜wk} generated by Algorithm 1 with Option II under a ﬁxed

0) and γcλ2τ 2

1 > 14αΛΓ2

c(β2 + τ 2

cβ2

42

step-size αk
k > Kth,

t := α and mk := m converging to w∗, there exists 0 < Kth < K such that for all

E[(dist( ˜wk+1, w∗))2] ≤

cβ2
2(Λ + 8mα2ΛΓ2
0)
cβ2
mα(γcλ2 − 7αΛΓ2
0)

E[(dist( ˜wk, w∗))2)

(A.73)

Proof. The proof is given similarly to Theorem 3.3. We use Corollary D.10, and also set as
θ = 0 in Lemma C.14, and as τ1 = τ2 = 1 in Lemma C.15.

43

E Additional numerical experiments

In this section, we show additional numerical experiments which do not appear in the main
text.

E.1 Matrix completion problem on synthetic datasets

E.1.1 Additional results

This section shows the results of six problem instances. Due to the page limitations, we only
show the loss on a test set Φ, which is diﬀerent from the training set Ω. The loss on the test
set demonstrates the convergence speed to a good prediction accuracy of missing entries.
Case MC-S1: We ﬁrst show the results of the comparison when the number of samples
N = 5000, the dimension d = 200, the memory size L = 10, the oversampling ratio (OS) is 8,
and the condition number (CN) is 50. We also add Gaussian noise σ = 10−10. Figures A.1
show the results of 4 runs except the result shown in the main text, which corresponds to
”run 1.” They show superior performances than other algorithms.
Case MC-S2: inﬂuence on low sampling. We look into problem instances from scarcely
sampled data, e.g. OS is 4. Other conditions are the same as Case MC-S1. From Figures
A.2, we can ﬁnd that the proposed algorithm gives much better and stabler performances
against other algorithms.
Case MC-S3:
inﬂuence on ill-conditioning. We consider the problem instances with
higher condition number (CN) 100. Other conditions are the same as Case MC-S1. Figures
A.3 show the superior performances of the proposed algorithm against other algorithms.
inﬂuence on higher noise. We consider noisy problem instances, where
Case MC-S4:
σ = 10−6. Other conditions are the same as Case MC-S1. Figures A.4 show that the
convergent MSE values are much higher than the other cases. Then, we can see the superior
performance of the proposed R-SQN-VR against other algorithms.
inﬂuence on higher rank. We consider problem instances with higher
Case MC-S5:
rank, where r = 10. Other conditions are the same as Case MC-S1. From Figures A.5, the
proposed R-SQN-VR still shows the superior performances against other algorithms. Grouse
indicates the faster decrease of the MSE at the begging of the iterations. However, the
convergent MSE values are much higher than those of others.

44

(a) run 2

(b) run 3

(c) run 4

(d) run 5

Figure A.1: Performance evaluations on low-rank MC problem (Case MC-S1: baseline.).

45

0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VRrun 2

(b) run 3

(c) run 4

(d) run 5

Figure A.2: Performance evaluations on low-rank MC problem (Case MC-S2: low sam-
pling.).

46

0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR(a) run 2

(b) run 3

(c) run 4

(d) run 5

Figure A.3: Performance evaluations on low-rank MC problem (Case MC-S3:
conditioning.).

ill-

47

0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR(a) run 2

(b) run 3

(c) run 4

(d) run 5

Figure A.4: Performance evaluations on low-rank MC problem (Case MC-S4: noisy data.).

48

0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR(a) run 2

(b) run 3

(c) run 4

(d) run 5

Figure A.5: Performance evaluations on low-rank MC problem (Case MC-S5: higher
rank.).

49

0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VR0100200300400500600#grad/N10-1510-1010-5100Means square error on test set R-SDR-L-BFGSGrouseR-SGDR-SVRGR-SQN-VRE.1.2 Processing time experiments

The results in terms of the processing time is presented.

Case MC-S7: Comparison in terms of processing time. Because one major con-
cern of second-order algorithms is, in general, higher computational processing load than
ﬁrst-order algorithms, we additionally show the results in terms of the processing time. This
evaluation addresses only R-SGD, R-SVRG and R-SQN-VR because the code structures of
them are similar whereas the batch-based algorithms, i.e., R-SD and R-L-BFGS, have com-
pletely diﬀerent implementations. Figures A.6 (a)-(e) show the results of the relationship
between test MSE and the processing time [sec]. From the ﬁgures, as expected, R-SGD gains
much faster speed in comparison with the results in terms of iteration than other algorithms.
However, it should be noted that R-SGD suﬀers from the problem that it heavily decreases
the convergence speed around the solution as reported in the literature. Comparing R-SQN-
VR with R-SVRG, R-SQN-VR still gives better performance although R-SQN-VR requires
one more additional vector transport of a gradient in each inner iteration and L vector trans-
ports of the curvature pairs at every outer epoch than R-SVRG does. Overall, R-SQN-VR
outperforms R-SGD and R-SVRG in terms of the processing time. Consequently, we also
have conﬁrmed the eﬀectiveness of the proposed R-SQN-VR from the viewpoint of processing
time.

(a) Case MC-S1:
baseline.

(b) Case MC-S2:
low sampling.

(c) Case MC-S3:
ill-conditioning.

(d) Case MC-S4:
noisy data.

(e) Case MC-S5:
higher rank.

Figure A.6: Performance evaluations on low-rank MC problem (Case MC-S7).

50

050100150Time [sec]10-1510-1010-5100Means square error on test set R-SGDR-SVRGR-SQN-VR050100150Time [sec]10-1510-1010-5100Means square error on test set R-SGDR-SVRGR-SQN-VR050100150Time [sec]10-1510-1010-5100Means square error on test set R-SGDR-SVRGR-SQN-VR050100150Time [sec]10-1510-1010-5100Means square error on test set R-SGDR-SVRGR-SQN-VR050100150200250Time [sec]10-1510-1010-5100Means square error on test set R-SGDR-SVRGR-SQN-VRFinally, Figure A.7 shows the results when the memory size of L is changed in R-SQN-
VR. Comparing the results with Figure 1 (h), the lower size cases improved their results very
slightly, but we do not observe a big advantage of lower memory sizes in terms of processing
load. From these results of both the convergence speed and the processing load, we cannot
conclude which size of L is the best. This should be left to a future research topic.

Figure A.7: Performance evaluations on low-rank MC problem (processing time) (Case MC-
S6: diﬀerent memory sizes).

E.2 Matrix completion problem on MovieLens 1M dataset

lower rank) and
Figures A.8 and A.9 show the results of the cases of r = 10 (MC-R1:
r = 20 (MC-R2: higher rank). They show the convergence plots of the training error on Ω
and the test error on Φ for all the ﬁve runs when rank r = 10 and r = 20, respectively. They
show that the proposed R-SQN-VR give good performances on other algorithms in all runs.

51

020406080Time [sec]10-1510-1010-5100Means square error on test set R-SQN-VR: L=5R-SQN-VR: L=10R-SQN-VR: L=20R-SQN-VR: L=40(a-1) run 1

(a-2) run2

(a-3) run 3

(a-4) run 4

(a-5) run 5

(a) MSE on train set Ω

(b-1) run 1

(b-2) run2

(b-3) run 3

(b-4) run 4

(b-5) run 5

(b) MSE on test set Φ

Figure A.8: Performance evaluations on low-rank MC problem (MC-R1: lower rank).

52

0100200300400#grad/N0.911.11.21.31.41.5Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.911.11.21.31.41.5Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.911.11.21.31.41.5Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.911.11.21.31.41.5Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.911.11.21.31.41.5Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.10.1050.110.1150.120.1250.130.1350.14Means square error on test set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR(a-1) run 1

(a-2) run2

(a-3) run 3

(a-4) run 4

(a-5) run 5

(a) MSE on train set Ω

(b-1) run 1

(b-2) run2

(b-3) run 3

(b-4) run 4

(b-5) run 5

(b) MSE on test set Φ

Figure A.9: Performance evaluations on low-rank MC problem (MC-R2: higher rank).

53

0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR0100200300400#grad/N0.40.50.60.70.8Means square error on train set R-SDR-L-BFGSR-SGDR-SVRGR-SQN-VR