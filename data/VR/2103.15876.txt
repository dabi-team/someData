High-ﬁdelity Face Tracking for AR/VR via Deep Lighting Adaptation

Lele Chen1,2 Chen Cao1

Fernando De la Torre1
1 Facebook Reality Labs

Jason Saragih1 Chenliang Xu2 Yaser Sheikh1

2 Univeristy of Rochester

1
2
0
2

r
a

M
9
2

]

V
C
.
s
c
[

1
v
6
7
8
5
1
.
3
0
1
2
:
v
i
X
r
a

Figure 1. High-ﬁdelity face tracking results using our method. From left to right: Input image captured by iPhone, normal map, fully-lit
avatar (avatar before relighting), relit avatar (avatar after relighting), and relit avatar under different viewpoints. Please notice the specular
highlight changes on the avatars under different viewpoints.

Abstract

visit https://www.youtube.com/watch?v=dtz1LgZR8cc for
more visual results.

3D video avatars can empower virtual communications
by providing compression, privacy, entertainment, and a
sense of presence in AR/VR. Best 3D photo-realistic AR/VR
avatars driven by video, that can minimize uncanny effects,
rely on person-speciﬁc models. However, existing person-
speciﬁc photo-realistic 3D models are not robust to light-
ing, hence their results typically miss subtle facial behav-
iors and cause artifacts in the avatar. This is a major
drawback for the scalability of these models in commu-
nication systems (e.g., Messenger, Skype, FaceTime) and
AR/VR. This paper addresses previous limitations by learn-
ing a deep learning lighting model, that in combination
with a high-quality 3D face tracking algorithm, provides
a method for subtle and robust facial motion transfer from
a regular video to a 3D photo-realistic avatar. Extensive
experimental validation and comparisons to other state-of-
the-art methods demonstrate the effectiveness of the pro-
posed framework in real-world scenarios with variability in
pose, expression, and illumination. Our project page can
be found at https://www.cs.rochester.edu/u/lchen63. Please

1. Introduction

Currently, video conferencing (e.g., Zoom, Skype, Mes-
senger) is the best 2D available technology for internet com-
munication. To allow for more advance levels of communi-
cation and sense of presence, Augmented Reality (AR) and
Virtual Reality (VR) technologies aim to build 3D person-
alized avatars, and superimpose virtual objects in the real
space. If successful, this new form of face-to-face interac-
tion will allow extended remote work experiences that can
improve productivity, reducing cost and stress of commut-
ing, have a huge impact on the environment, and overall
improving the work/life balance.

Today most real-time systems for avatars in AR are
cartoon-like (e.g., Apple Animoji, Tiktok FaceAnimation,
Hyprsense, Loom AI); on the other hand, digital creators in
movies have developed uncanny digital humans using ad-
vanced computer graphics technology and person-speciﬁc
(PS) models (e.g., Siren). While some of these avatars can

1

Captured imageRelit avatar under different viewpointsRelit avatarNormal map Fully-lit avatar  
 
 
 
 
 
3D face

photo-realistic

be driven in real-time from egocentric cameras (e.g., Doug
character made by digital domain), building the PS model is
an extremely time-consuming and hand-tuned process that
prevents democratizing this technology. This paper con-
tributes toward this direction, and it proposes new algo-
rithms to robustly and accurately drive 3D video-realistic
avatars from monocular cameras to be consumed by AR/VR
displays (see Fig. 1).
Model-based

reconstruc-
tion/animation from a video has been a core area of
research in computer vision and graphics in the last
thirty years [3, 4, 6, 8, 14, 16, 23, 28, 33, 49, 44, 59, 2].
While different versions of morphable models or active
appearance models have provided good facial animation
results, the existing 3D models do not provide the quality
that is needed it for a good immersive viewing experience
in AR/VR. In particular, the complex lighting, motion, and
other in-the-wild conditions do result in artifacts in the
avatar due to poor decouple of rigid and non-rigid motion,
as well as, no accurate texture reconstruction. To tackle
this problem, we build on recent work on Deep Appearance
Model (DAM) [27] that learns a person-speciﬁc model from
a multi-view capture setup. [27] can render photo-realistic
avatars in a VR headset by inferring 3D geometry and
view-dependent texture from egocentric cameras .

This paper extends DAM [27] with a new deep light-
ing adaptation method to recover subtle facial expressions
from monocular videos in-the-wild and transfer them to a
3D video-realistic avatar. The method is able to decouple
rigid and non-rigid facial motions, as well as, shape, ap-
pearance and lighting from videos in-the-wild. Our method
combines a prior lighting model learned in a lab-controlled
scenario and adapts it to the in-the-wild video, recovering
accurate texture and geometric details in the avatar from
images with complex illuminations and challenging poses
(e.g. proﬁle). There are two main contributions of our work.
First, we provide a framework for ﬁtting a non-linear ap-
pearance model (based on a variational auto-encoder) to in-
the-wild videos. Second, we propose a new lighting trans-
fer module to learn a global illumination model. Experi-
mental validation shows that our proposed algorithm with
deep lighting adaptation outperforms state-of-the-art meth-
ods and provides robust solutions in realistic scenarios.

2. Related Work

3D morphable

3D Face Tracking.
face models
(3DMM) [3, 4, 6, 14, 16, 23, 33] and Active Appear-
ance Models (AAMs) [8, 28, 49] have been extensively
utilized for learning facial animations from 3D scans
and face tracking. These methods produce texture and
geometry through the idea of analysis-by-synthesis, where
a parametric face model is iteratively adapted until the
synthesized face matches the target image. For example,

2

by leveraging the photometric error in both shape and
texture, AAMs [8] have shown strong efﬁciency and
expressibility to register faces in images. More recently,
Deep Appearance Model (DAM) [27] extends the AAMs
with deep neural networks in-place of linear generative
functions. DAM learns the latent presentation of geometry
and texture using a conditional variational autoencoder [19]
and is able to reconstruct a high-ﬁdelity view-dependent
avatar with the aid of the multi-view camera system.

In-the-Wild Face Reconstruction. Face reconstruction
under an in-the-wild scenario is known as a challenging
problem since the rigid, lighting, and expression are un-
known. For example, the surface information presented by
a single image [15, 24, 34, 38, 42, 43, 57, 48, 54, 58, 56,
45, 17, 12] or even an image collection [35, 36, 25, 37, 50,
41, 46] is limited. Thus, achieving high-ﬁdelity face recon-
struction from in-the-wild imagery usually relies on prior
knowledge like 3DMMs [3], FLAME [23], or DAM [27].
For example, instead of directly regressing the geometry,
MoFA [43] uses a CNN-based image encoder to extract
the semantically meaningful parameters (e.g., facial expres-
sion, shape, and skin reﬂectance) from a single 2D image
and then uses the parametric model-based decoder to ren-
der the output image. Tran et al. [48] propose a weakly
supervised model that jointly learns a nonlinear 3DMM and
its ﬁtting algorithm from 2D in-the-wild image collection.
Gecer et al. [13] train a facial texture generator in the UV
space with self-supervision as their statistical parametric
representation of the facial texture. Lin et al. [24] propose
a GCN-based network to reﬁne the texture generated by a
3DMM-based method with facial details from the input im-
age. Yoon et al. [54] propose a network (I2ZNet) to learn a
latent vector z and head pose for DAM from a single image
to reconstruct the texture and geometry.

Lighting Estimation for In-the-Wild Imagery. Most ex-
isting face reconstruction works [24, 26, 39, 42, 43, 47,
48, 29] estimate the illumination using spherical harmonics
(SH) basis function. For instance, Tewari et al. [42] regress
the illumination parameters from the input image, and the
rendering loss is computed after combining the estimated
illumination and skin reﬂectance in a self-supervised fash-
ion. In this way, it is hard to analyze the quality of the es-
timated illumination and skin albedo. Moreover, low-order
spherical harmonics cannot produce hard shadows cast from
point light sources, which will decrease the face reconstruc-
tion quality in many real-world scenarios. I2ZNet [54] pro-
poses a MOTC module to convert the color of the predicted
texture to the in-the-wild texture of the input image. The
MOTC can be viewed as a color correction matrix that cor-
rects the white-balance between the two textures. However,
the MOTC can only model the low-frequent lighting infor-
mation, which will decrease the face registration perfor-
mance if the lighting environment is complicated. Mean-

while, the surface information presented by a single im-
age is limited, which leads to some artifacts such as over-
smoothing and incorrect expression. To explicitly model the
lighting, we propose a physics-based lighting model to learn
the high-frequent lighting pattern (e.g., shading, and bright-
ness) from data captured in a lab-controlled environment.
Besides, a domain adaptation schema is proposed to bridge
the domain mismatch between lab and wild environments.

3. Adaptive Lighting Model

This section describes existing work on DAMs [27]
(Sec. 3.1), construction of the lighting model with light-
stage data (Sec. 3.2), and the adaptation of the model for
in-the-wild settings in Sec. 3.3.

3.1. Deep Appearance Models

Our work is based on the face representation described
in DAM [27]. It uses a variational auto-encoder (VAE) [20]
to jointly represent the 3D facial geometry and appearance
that are captured from a multi-view capture light-stage. The
decoder, D, can generate instances of a person’s face by tak-
ing, as input, a latent code z, which encodes the expression,
and a vector vv that represents viewing direction as a nor-
malized vector pointing from the center of the head to the
camera v:

ˆM, ˆTv ← D(z, vv) .

(1)

Here, ˆM denotes the 3D face mesh (geometry) and ˆTv, the
view-dependent texture.

In this work, we assume the availability of a pre-trained
DAM decoder of a subject, and propose a system to ﬁt the
model to images by estimating the rigid and non-rigid mo-
tion (i.e., facial expression). A major challenge in imple-
menting such a system is how to account for illumination
differences between the high controlled studio lighting sys-
tem where the avatar was captured, and the in-the-wild cap-
tures where the avatar is deployed. In the following, we will
describe an adaptive lighting model that extends the origi-
nal DAM formulation to enable high precision tracking in
uncontrolled and complex environments.

3.2. Lighting Model

In order to incorporate a generative model of lighting
into the DAM formulation, we extend the capture system
in [27] to include 460 controllable lights that are synchro-
nized with the multi-view camera system. The captured
sequence was extended to include a portion where non-
overlapping groups of approximately 10 lights were turned
on, interleaved with fully lit frames that were used for track-
ing. This data was used to build a relightable face model
using the scheme illustrated in Figure 2.

Our formulation is inspired by the light-varying residual
proposed by Nestmeyer et al. [31], where illumination vari-

3

Figure 2. Training the lighting model on the light-stage data. We
update the lighting model G and per-frame expression code z
while ﬁxing the other parameters.

ations are represented using gain and bias maps, g and b,
each matching DAM’s texture dimensions (H × W × 3):

Tv = ˆTv (cid:12) gv + bv,

(2)

where (cid:12) denotes the element-wise product, Tv is the relit
texture, and ˆTv is the DAM avatar’s original texture (fully-
lit). The gain and bias maps depend on the lighting, head
pose1, viewpoint, and expression. These inputs, represented
by l, hv, and ˆTv, are processed by MLPs and are spatially
repeated for concatenation with the DAM’s texture followed
by additional convolution operations to produce the ﬁnal re-
lit texture. This lighting model is, thus, deﬁned as follows:

gv, bv ← G(l, hv, ˆTv; φ) ,

(3)

where φ denotes the weights of the network. Further details
about G are in Sec. 4.1 and in the supplementary material.
Since the goal of our lighting model is to enable accu-
rate registration in uncontrolled scenarios, we do not require
a lighting representation that is geometrically interpretable,
only one that can span the space of facial illuminations. As
such, we represent lighting conditions using a one vector
that speciﬁes which of the lights, in each color channel, are
active for a given training frame. We use a binary vector
of 150 dimensions, which comprises the 50 lighting groups
with three color channels each. The fully-lit frames are en-
coded as the all-one vector. In combination with the contin-
uously parameterized head pose, this representation allows
for continuous and smoothly varying illumination synthe-
sis on the face that can model complex effects such as how
shadows move as the subject’s head rotates in the scene.

1Here, the rigid head pose consists of two parts: rigid rotation r ∈
R3 and camera viewpoint vector vv ∈ R3. Similar to [27], we assume
that the viewpoint vector is relative to the rigid head orientation that is
estimated from the tracking algorithm.

(cid:71)(cid:253)(cid:333)(cid:368)(cid:2)(cid:366)(cid:253)(cid:414)(cid:2)(cid:104)(cid:333)(cid:321)(cid:328)(cid:438)(cid:333)(cid:368)(cid:321)(cid:366)(cid:378)(cid:289)(cid:296)(cid:357)(cid:31)(cid:333)(cid:253)(cid:425)(cid:2)(cid:366)(cid:253)(cid:414)(cid:2)(cid:39)(cid:3)(cid:114)(cid:188)(cid:296)(cid:480)(cid:438)(cid:446)(cid:417)(cid:296)(cid:2)(cid:114)(cid:296)(cid:425)(cid:328)(cid:2)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:281)(cid:481)(cid:2)(cid:47)(cid:416)(cid:1083)(cid:1096)(cid:1014)(cid:1097)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:438)(cid:296)(cid:480)(cid:438)(cid:446)(cid:417)(cid:296)(cid:39)(cid:333)(cid:497)(cid:320)(cid:296)(cid:417)(cid:296)(cid:368)(cid:438)(cid:333)(cid:253)(cid:281)(cid:357)(cid:296)(cid:2)(cid:166)(cid:296)(cid:368)(cid:289)(cid:296)(cid:417)(cid:333)(cid:368)(cid:321)(cid:166)(cid:296)(cid:368)(cid:289)(cid:296)(cid:417)(cid:296)(cid:289)(cid:2)(cid:333)(cid:366)(cid:253)(cid:321)(cid:296)(cid:2)(cid:32)(cid:253)(cid:414)(cid:438)(cid:446)(cid:417)(cid:296)(cid:289)(cid:2)(cid:333)(cid:366)(cid:253)(cid:321)(cid:296)Algorithm 1 Lighting Model Adaptation

Input: lighting model G with weights φ, K key frames
with initial face parameters {(I, ˜p)k}, camera viewpoint
vector vv
Output: adapted lighting model G and lighting code l
Initialization: set l to zeros, φ to pre-trained weights by
Sec. 3.2, face parameters {pk} to {˜pk}
for number of iterations do

# Fitting l
for number of iterations do

Unfreeze l, freeze φ and {pk}
Calculate Lpix using Eq. 7
l ← Adam{Lpix}
# Fitting φ and {pk}
for number of iterations do

Freeze l, unfreeze φ and {pk}
Calculate Lpix using Eq. 7
φ, {pk} ← Adam{Lpix}

To train G, we take a pre-trained DAM decoder and ﬁx
its weights while minimizing the reconstruction error over
all camera views in the subject’s sequence, that is:
(cid:12) mv(cid:13)
(cid:13)
(cid:13)1

Lrender(φ, Z) =

(cid:17)
t , ˆMt)

t − R(Tv
I v

, (4)

(cid:88)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

t,v

where the minimization is performed over the lighting
model’s weights, φ, as well as the expression codes for
each frame Z = {zt}. In Equation 4, mv is a foreground
mask from view v and R is a differentiable rasterization
function [32]. To ensure stable convergence, we employ
L2-shrinkage on the expression codes, Z, and use the tech-
nique in [52] to obtain a tracked mesh, Mt, from the fully-
lit frames, that are used to geometrically constrain the opti-
mization, resulting in the following total objective:

L = Lrender + λgeo

(cid:88)

t

(cid:107)Mt − ˆMt(cid:107)2 + λreg(cid:107)Z(cid:107)2.

(5)

Regularization weights of λgeo = 1.0 and λreg = 0.1 were
chosen for all experiments in Sec. 4 by cross validation.

3.3. Registration In-the-Wild

With the lighting model described in Sec. 3.2, we have a
personalized face model that can synthesize realistic vari-
ations in expression as well as lighting suitable for an
analysis-by-synthesis approach to register the model to in-
the-wild video with uncontrolled illumination. However,
to achieve robust and precise results, special care needs to
be applied in how registration is performed. Our algorithm
comprises three steps (Fig. 3) outlined below, each designed
to address initialization, accuracy, and computational efﬁ-
ciency, respectively.

4

Step 1: Initialization. To avoid registration terminating
in poor local minima, we initialize the pose and expression
parameters by matching against facial keypoints in the im-
age found via an off-the-shelf detector (e.g.,[22]). Speciﬁ-
cally, face landmarks, {Li}, describing facial features such
as eye corners, lip contours, and the face silhouette corre-
spond to ﬁxed vertices in the face model’s geometry, {(cid:96)i}.
The initial face parameters, ˜p = [˜r, ˜t, ˜z], are then found
by minimizing the reprojection error over these landmark
points in all camera views, v, for every frame:

Lland(˜p) =

(cid:13)
(cid:13)
(cid:13)Πv

(cid:88)

v,i

(cid:17)
(cid:16)
˜r ˜M((cid:96)i) + ˜t

− Lv
i

(cid:13)
2
(cid:13)
(cid:13)

,

(6)

where Πv is the projection operator based on camera param-
eters that are assumed to be available. The face mesh, ˜M,
is calculated using Eq. 1 with the expression code ˜z. Due
to the landmarks’ sparsity and detection noise, minimizing
Lland results in only a rough alignment of the face in each
frame (e.g., Fig. 4 (b)). Nonetheless, it places the model
within the vicinity of the solution, allowing the more elabo-
rated optimization procedure described next to converge.
Step 2: Lighting Model Adaptation. Although the light-
ing model described in Sec. 3.2 equips us with the abil-
ity to synthesize variations in facial illumination, using the
light-stage to simulate the total span of lighting variations
encounter in the wild remains challenging. Effects such
as nearﬁeld lighting, cast shadows, and reﬂections from
nearby objects are commonly observed in uncontrolled set-
tings. To account for variations not spanned by our lighting
model, G, in addition to solving for the lighting parame-
ters, l, we simultaneously ﬁne-tune the model’s weights, φ,
to obtain a better ﬁt to in-the-wild images. Speciﬁcally, we
minimize the following loss over a collection of K frames 2
from the target environment:

Lpix(l, φ, {pk}) =

(cid:88)

(cid:107)rk (cid:12) wk(cid:107)1 + λ(cid:52) (cid:107)(cid:52)rk (cid:12) wk(cid:107)1 ,

k

(7)
where rk = Ik − ˆI(pk) is the reconstruction residual, wk
is the foreground mask, and pk = [rk, tk, zk]. Here, ∆
denotes the image Laplacian operator that makes the loss
more robust to residual differences due to illumination and
generally improves results.
Step 3: Face Tracking. The procedure described previ-
ously can generate accurate estimates of facial expression,
but requires batch processing to adapt the lighting model si-
multaneously over several frames. However, once the light-
ing model has been adapted, the parameters p = [r, t, z]
for any new frames can be estimated independently of the
lighting model G. Thus, in practice, we adapt the lighting
model using only a small subset of K frames and estimate

2We can select K frames out of the testing sequence for adaptation if

the lighting of the K frames is the same as the testing sequence.

Figure 3. The pipeline of in-the-wild registration. We estimate the initial tracking parameters in step 1, adapt the lighting model and
tracking parameters l, φ, {pk} with K reference frames in Step 2, and further optimize the tracking parameters in step 3.

4.1. Experimental Settings

Dataset Collection. We recorded our light-stage data in a
calibrated multi-view light-stage consisting of 40 machine
vision cameras capable of synchronously capturing HDR
images at 1334×2048 / 90 fps and a total of 460 white LED
lights. We ﬂash a group of LEDs (at most 10) per frame
and instruct our subjects to make diverse expressions with
head movements. There are 50 different lighting patterns
and one fully-lit pattern. We record a total of 13 minutes
video sequence of one subject (see supplementary videos).
The in-the-wild video test were gathered using the
frontal camera of an iPhone. We captured videos for 10
subjects. We collected around 5 video clips for each sub-
ject, performing different facial expressions and head move-
ments, under various lighting conditions and environments.
Implementation Details.
The light-stage training step
(Sec. 3.2) and the lighting adaptation step cost 36 hours and
4mins, respectively, on an NVIDIA DGX machine. In all
our experiments, we used the Adam optimizer [18] to op-
In order to cover more lighting space
timize the losses.
during the training of the lighting transfer module, we aug-
mented the RGB channels of the light-stage lighting color
with randomly selected scales, which are also used to scale
the lighting code l in the training data. The architecture of
our lighting decoder G is as follows: we ﬁrst encode the in-
put head pose hv and l with two MLPs to 256 dimensions.
After concatenating the two latent features, we pass it to a
fully-connected layer and a convolution layer followed by
four transposed convolutions with each layer. The fully-lit
texture ˆTv is encoded by four convolution layers with each
layer followed by a down-sampling layer to texture feature.
Then we concatenate the texture feature and lighting feature
and pass it to two separate branches consists of two trans-
pose convolution. The two branches output gain map gv
t and
bias map bv
t with the resolution of 256 × 256, and we up-
sample them 4 times using bilinear interpolation to the same
resolution as texture ˆTv. Please refer to supplementary ma-
terials for details. While training the lighting model on the
light-stage data in Sec. 3.2, the rendering loss is optimized
with an initial learning rate of 1e−3, which is decreased by
a quarter after every 10 epochs.

During the registration of the in-the-wild videos in
Sec. 3.3, we ﬁt the DAM code zk in step 1 with 1000 it-

Figure 4. Result of each step in Sec. 3.3. From left to right: (a)
captured image, (b) avatar output by step 1, (c) relit avatar after
step 2, (d) fully-lit avatar after step 3, and (e) relit avatar after
step 3. The last column shows the zoom-in of corresponding color
rectangle, please notice the lip shapes and gaze directions.

p with the updated model G. To further improve accuracy,
in addition to the optimizing the loss in Eq. 7, similarly to
[5], we use dense optical ﬂow [21] between the rendered
model and the image to further constrain the optimization.
It is computed over all projected mesh vertices in all camera
views as follows:

Lﬂow(p) =

(cid:88)

v,i

(cid:16)

(cid:13)
(cid:13)
(cid:13)

rM(i) + t

(cid:17)

− Πv

(cid:17)
(cid:16)
˜r ˜M(i) + ˜t

− dv
i

(cid:13)
2
(cid:13)
(cid:13)

,

(8)
where mesh M and ˜M are calculated using Eq. 1 with the
latent face code z and ˜z respectively, and (˜r, ˜t, ˜z) are ini-
tial parameters from Step 1. We optimize the per-frame
face parameters p = {r, t, z} by minimizing the total loss
L = Lpix + λﬂowLﬂow, where λﬂow = 3.0 was chosen via
cross validation. Fig. 4 (d-e) shows some examples of re-
sults obtained through this process, demonstrating accurate
alignment and reconstruction of lip shape and gaze direction
that were absent in Step 1.

4. Experiments

In this section, we conduct quantitative and qualitative
experiments to show the performance of the proposed face
tracking framework on in-the-wild videos. Sec. 4.1 explains
dataset and implementation details. Sec. 4.2 compares our
method to state-of-the-art methods, and Sec. 4.3 describes
the ablation studies.

5

(cid:1096)(cid:253)(cid:1097)(cid:2)(cid:32)(cid:253)(cid:414)(cid:438)(cid:446)(cid:417)(cid:296)(cid:289)(cid:2)(cid:333)(cid:366)(cid:253)(cid:321)(cid:296)(cid:1096)(cid:281)(cid:1097)(cid:2)(cid:174)(cid:438)(cid:296)(cid:414)(cid:1013)(cid:2)(cid:1096)(cid:70)(cid:446)(cid:357)(cid:357)(cid:481)(cid:1102)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1097)(cid:1096)(cid:282)(cid:1097)(cid:2)(cid:174)(cid:438)(cid:296)(cid:414)(cid:1014)(cid:2)(cid:1096)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1097)(cid:2)(cid:1096)(cid:289)(cid:1097)(cid:2)(cid:174)(cid:438)(cid:296)(cid:414)(cid:1015)(cid:2)(cid:1096)(cid:70)(cid:446)(cid:357)(cid:357)(cid:481)(cid:1102)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1097)(cid:1096)(cid:296)(cid:1097)(cid:2)(cid:174)(cid:438)(cid:296)(cid:414)(cid:1015)(cid:2)(cid:1096)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1097)Figure 5. Qualitative comparison. We suggest to view it using a monitor for better visual quality. We selected 8 subjects from the test set
with different lighting conditions, facial expression, and head motion. From left to right: (a) Captured image, (b) Abrevaya et al. [1], (c)
3DDFAv2 [15], (d) PRNet [11], (e) RingNet [37], (f) FaceScape [53], (g) Deng et al. [10], (h) MGCNet [40], and (i) our method.

iterations with a learning rate of 1e−3. We alternatively up-
date l, φ, and {pk} for 4 times. In step 3, we update the face
parameters {pk} with the same hyper-parameters as step 1.
Evaluation Metrics. We used a variety of perceptual
measures to quantitatively compare the registered image
against the ground-truth in-the-wild image. Besides the
pixel-level L2 distance, we adopted PSNR and structural
similarity (SSIM) [51] for human perceptual response. To
evaluate the realism of the output avatar, we computed the
cosine similarity (CSIM) between embedding vectors of
the state-of-the-art face recognition network [9] suggested
by [55, 7] for measuring identity mismatch between the in-
put image and reconstructed avatar.

4.2. Comparison with state-of-the-art methods

To demonstrate the effectiveness of our lighting model
and the face tracking quality, we compared our algorithm
against the following set of related methods using the pre-
trained models provided by the authors: Abrevaya et al. [1],
3DDFAv2 [15], PRNet [11], RingNet [37], FaceScape [53],

Figure 6. Visual comparison between our method and I2ZNet [54]
on testing video frames. From top to bottom: captured image,
I2ZNet [54], and our method.

erations. The initial learning rate is 0.1, and we decrease it
by half after every 500 iterations. In step 2, the K frames
are uniformly sampled according to the value of ˜r to cover
diverse head movements. We ﬁt the lighting code l with
250 iterations with a learning rate of 1e−2, then update G’s
network parameter φ and face parameters {pk} with 500

6

(a)  Input(h) MGCNet  (c) 3DFFA-V2(d) PRNet (e) RingNet(i) Ours   (b) Abrevaya et al.(g) Deng et al.(f) FaceScape(cid:41)(cid:76)(cid:74)(cid:17)(cid:3)(cid:25)Figure 7. Visual results under different lighting conditions. Besides our ﬁnal relit avtar (g), we also show the tracked avatar (h) using
spherical harmonics (SH) as illumination model. Our method can handle different lighting conditions well.

Deng et al. [10], MGCNet [40], and I2ZNet [54]. As the
baseline method, we used the same face registration method
proposed in Sec. 3, but use the standard spherical harmonics
(SH) [30] illumination model, which is denoted as ours-SH.
We adopt the same parameter setting as [43] for SH, and
train a regression network to regress the input images to the
27 dimensional SH parameters.
Qualitative Evaluation.
Fig. 5 shows the tracked geom-
etry results for different methods 3. We can observe that
our proposed method is robust to diverse facial expressions,
poses, and lighting conditions. For example, in the 3rd and
4th row, all other methods failed to describe the expression
(e.g., lips) except our method and Abrevaya et al. [1]. In the
7th row, all other methods can not output the correct head
pose, while our method can still reconstruct high-quality ge-
ometry and texture under dark lighting conditions. We also
show the reconstructed avatars by (g) [10], (h) [40], and
(i) our method (fully-lit and relit avatar). Comparing with
other methods, our method not only generates a more realis-
tic avatar, but also considers the lighting details (e.g., shad-
ows and specular highlights, see the relit avatar in 1th, 3rd,

3Note that our method relies on person-speciﬁc DAM.

and 6th row). Furthermore, we compare our relit avatar with
I2ZNet [54] in Fig. 6. Although I2ZNet is a person-speciﬁc
model, our method produces better visual results.

Fig. 7 shows the visual results under different real-world
lighting environments. The proposed method is robust to
different unseen lighting conditions, and our face tracking
system can output a high-quality avatar with the aid of our
lighting model. Fig. 7(h) shows the tracking results using
the SH illumination model. The reconstruction error be-
tween the captured frame and the avatar relit by SH model
is large, and it decreases the face tracking performance.

We also show our avatar rendered from different view-
points in Fig. 1 and Fig. 8. We can ﬁnd that our method
can output the high-ﬁdelity avatar from any viewpoint. Our
lighting model is conditioned on the camera viewpoints, so
the gain map and bias map will be adjusted to match the
lighting in the speciﬁc view. Please refer to the supplemen-
tary video for more visual results.
Quantitative Evaluation.
Tab. 1 shows the quantitative
results of MGCNet [40], Deng et al. [10], I2ZNet [54], and
our method. Our method outperforms other methods not
only at the human perception level, but also at the iden-
tity preserving ability (CSIM score). The high CSIM score

7

(a) Captured image(c) Fully-lit avatar(d) Gain map(e) Bias map(g) Relit avatar(b) Fully-lit texture(f) Relit texture(h) Relit avatar-SHFig. 7Methods

Metrics

w/o Pre-train.
w/o Adapt.
K =1
K =12
K = 48
K = All

L2 ↓
14.88
18.03
18.58
12.30
11.35
11.33

SSIM↑
0.941
0.938
0.900
0.950
0.950
0.952

PSNR↑ CSIM↑
0.783
37.08
0.593
35.59
0.550
35.48
0.842
37.32
0.809
37.66
0.863
37.78

Table 2. Quantitative results of the ablation study.

Figure 8. The visual results for different viewpoints. From left
to right: captured image, normal map, fully-lit texture, fully-lit
avatar, gain map, relit texture, and relit avatar. From top to bottom:
different viewpoints. Please notice the changes of the gain map
due to different viewpoints.

Methods

Metrics

MGCNet [40]
Deng et al. [10]
I2ZNet [54]
Ours-SH
Ours

L2 ↓
24.56
27.33
20.49
41.01
12.59

SSIM↑
0.86
0.86
0.927
0.79
0.93

PSNR↑ CSIM↑
0.305
34.47
0.434
33.81
0.592
35.03
0.693
32.00
0.871
37.93

Table 1. The quantitative evaluation on the test set. ↓/↑ denote the
lower/higher, the better. The top-1 scores are highlighted.

indicates that our reconstructed avatar produces high iden-
tity similarity. We can see that the L2 and other percep-
tual scores of the avatar optimized with the SH illumination
model result in higher error, although it preserves the iden-
tity information (CSIM).

Figure 9. Visual comparison of the ablation study.

slightly better than K = 1, with more reference frames for
adaptation, the adapted lighting model performs much bet-
ter than without adaptation. Fig. 9 shows the visual compar-
ison. We can see that the pre-training on light-stage data and
lighting adaptation both contribute to the ﬁnal tracking re-
sults, where the pre-training on light-stage data enables the
lighting interpretation ability, and the adaptation step en-
ables the lighting model to generate accurate gain and bias
map for target video frames.
Inﬂuence of the value of K.
To evaluate the effect on
the number of reference frames used in the lighting model
adaptation step, we sample reference frames with different
K and keep other settings the same, and show the results in
Tab. 2 and Fig. 9. We can ﬁnd that with only 48 reference
frames, the lighting model can be perfectly adapted to the
target video and achieve comparable results in visual met-
rics (L2, SSIM, and PSNR). If the amount of the selected
reference frames is too small (e.g., K = 1), the lighting
model will be over-ﬁtted to the selected frames and loss the
lighting interpolation ability.

4.3. Ablation Study

5. Conclusion

We have already shown the superiority of our face track-
ing algorithm along with the lighting model over other
methods. To further demonstrate the effectiveness of dif-
ferent steps in Sec. 3, we make a comprehensive ablation
study on a subset of the testing videos.
The lighting model. To evaluate the role of our lighting
model, we test our face tracking system with the lighting
model under two different settings: without pre-training on
light-stage data (w/o Pre-train.) and without lighting adap-
tation on wild video frames (w/o Adapt.). We set K = 48
in the without pre-training experiment. Tab. 2 shows the
quantitative evaluation results, and we can see that all the
scores are improved with light-stage pre-training (1st and
5th row). Although without adaptation (2nd row) performs

We present a new in-the-wild face tracking algorithm
with an adaptive lighting model that can infer a high-ﬁdelity
3D avatar. The proposed lighting model inherits the prior
knowledge from the light-stage data, and it adapts to in-the-
wild samples to produce high-quality re-lighting results for
our face tracking. Results conﬁrm that relatively few adap-
tion samples (48) are enough to produce hyper-realistic re-
sults in the avatar. While the proposed method can generate
photo-realistic avatars from videos in the wild, our lighting
model assumes that the lighting source in the testing video
is ﬁxed, and it uses one single lighting code to represent the
lighting in the whole video sequence. This is a limitation of
the current model, and the model can produce undesirable
results if the lighting is changing in the video. In the future,

8

(cid:1096)(cid:253)(cid:1097)(cid:2)(cid:32)(cid:253)(cid:414)(cid:438)(cid:446)(cid:417)(cid:296)(cid:289)(cid:2)(cid:333)(cid:366)(cid:253)(cid:321)(cid:296)(cid:1096)(cid:281)(cid:1097)(cid:2)(cid:116)(cid:378)(cid:417)(cid:366)(cid:253)(cid:357)(cid:2)(cid:366)(cid:253)(cid:414)(cid:1096)(cid:282)(cid:1097)(cid:2)(cid:70)(cid:446)(cid:357)(cid:357)(cid:481)(cid:1102)(cid:357)(cid:333)(cid:438)(cid:2)(cid:438)(cid:296)(cid:480)(cid:438)(cid:446)(cid:417)(cid:296)(cid:1096)(cid:289)(cid:1097)(cid:2)(cid:70)(cid:446)(cid:357)(cid:357)(cid:481)(cid:1102)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1096)(cid:296)(cid:1097)(cid:2)(cid:71)(cid:253)(cid:333)(cid:368)(cid:2)(cid:366)(cid:253)(cid:414)(cid:1096)(cid:321)(cid:1097)(cid:2)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:253)(cid:474)(cid:253)(cid:438)(cid:253)(cid:417)(cid:1096)(cid:320)(cid:1097)(cid:2)(cid:166)(cid:296)(cid:357)(cid:333)(cid:438)(cid:2)(cid:438)(cid:296)(cid:480)(cid:438)(cid:446)(cid:417)(cid:296)(cid:41)(cid:76)(cid:74)(cid:17)(cid:3)(cid:27)(cid:2)(cid:1096)(cid:253)(cid:1097)(cid:2)(cid:32)(cid:253)(cid:414)(cid:438)(cid:446)(cid:417)(cid:296)(cid:289)(cid:2)(cid:333)(cid:366)(cid:253)(cid:321)(cid:296)(cid:2)(cid:1096)(cid:281)(cid:1097)(cid:2)(cid:475)(cid:1089)(cid:378)(cid:2)(cid:414)(cid:417)(cid:296)(cid:1102)(cid:438)(cid:417)(cid:253)(cid:333)(cid:368)(cid:333)(cid:368)(cid:321)(cid:2)(cid:1096)(cid:282)(cid:1097)(cid:2)(cid:475)(cid:1089)(cid:378)(cid:2)(cid:253)(cid:289)(cid:253)(cid:414)(cid:438)(cid:253)(cid:438)(cid:333)(cid:378)(cid:368)(cid:2)(cid:1096)(cid:289)(cid:1097)(cid:2)(cid:102)(cid:2)(cid:1154)(cid:2)(cid:1013)(cid:2)(cid:1096)(cid:296)(cid:1097)(cid:2)(cid:102)(cid:2)(cid:1154)(cid:2)(cid:1013)(cid:1014)(cid:2)(cid:1096)(cid:320)(cid:1097)(cid:2)(cid:102)(cid:2)(cid:1154)(cid:2)(cid:1016)(cid:1020)(cid:2)(cid:1096)(cid:321)(cid:1097)(cid:2)(cid:102)(cid:2)(cid:1154)(cid:2)(cid:3)(cid:357)(cid:357)we will explore on-line adaptation methods to address this
limitation of the current work.
Acknowledgement.
Chen was an intern at Facebook Reality Labs.

This work was done when Lele

References

[1] V. F. Abrevaya, A. Boukhayma, P. H. Torr, and E. Boyer.
Cross-modal deep face normals with deactivable skip con-
In Proceedings of the IEEE/CVF Conference
nections.
on Computer Vision and Pattern Recognition, pages 4979–
4989, 2020. 6, 7

[2] T. Beeler, F. Hahn, D. Bradley, B. Bickel, P. Beardsley,
C. Gotsman, R. W. Sumner, and M. Gross. High-quality pas-
sive facial performance capture using anchor frames. ACM
Trans. Graph., 30:75:1–75:10, August 2011. 2

[3] V. Blanz and T. Vetter. A morphable model for the synthesis
of 3d faces. In Proceedings of the 26th annual conference on
Computer graphics and interactive techniques, pages 187–
194, 1999. 2

[4] J. Booth, A. Roussos, A. Ponniah, D. Dunaway, and
Interna-
S. Zafeiriou. Large scale 3d morphable models.
tional Journal of Computer Vision, 126(2-4):233–254, 2018.
2

[5] C. Cao, M. Chai, O. Woodford, and L. Luo. Stabilized real-
time face tracking via a learned dynamic rigidity prior. ACM
Transactions on Graphics (TOG), 37(6):1–11, 2018. 5
[6] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. Faceware-
house: A 3d facial expression database for visual computing.
IEEE Transactions on Visualization and Computer Graphics,
20(3):413–425, 2013. 2

[7] L. Chen, G. Cui, C. Liu, Z. Li, Z. Kou, Y. Xu, and C. Xu.
Talking-head generation with rhythmic head motion.
In
European Conference on Computer Vision, pages 35–51.
Springer, 2020. 6

[8] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appear-
IEEE Transactions on pattern analysis and

ance models.
machine intelligence, 23(6):681–685, 2001. 2

[9] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive
angular margin loss for deep face recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4690–4699, 2019. 6

[10] Y. Deng, J. Yang, S. Xu, D. Chen, Y. Jia, and X. Tong. Ac-
curate 3d face reconstruction with weakly-supervised learn-
ing: From single image to image set. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pages 0–0, 2019. 6, 7, 8

[11] Y. Feng, F. Wu, X. Shao, Y. Wang, and X. Zhou. Joint 3d
face reconstruction and dense alignment with position map
In Proceedings of the European Con-
regression network.
ference on Computer Vision (ECCV), pages 534–551, 2018.
6

[12] P. Garrido, M. Zollh¨ofer, D. Casas, L. Valgaerts, K. Varanasi,
P. P´erez, and C. Theobalt. Reconstruction of personalized
3d face rigs from monocular video. ACM Transactions on
Graphics (TOG), 35(3):1–15, 2016. 2

[13] B. Gecer, S. Ploumpis, I. Kotsia, and S. Zafeiriou. Gan-
ﬁt: Generative adversarial network ﬁtting for high ﬁdelity 3d
face reconstruction. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1155–
1164, 2019. 2

[14] T. Gerig, A. Morel-Forster, C. Blumer, B. Egger, M. Luthi,
S. Sch¨onborn, and T. Vetter. Morphable face models-an open
framework. In 2018 13th IEEE International Conference on
Automatic Face & Gesture Recognition (FG 2018), pages
75–82. IEEE, 2018. 2

[15] J. Guo, X. Zhu, Y. Yang, F. Yang, Z. Lei, and S. Z. Li.
Towards fast, accurate and stable 3d dense face alignment.
arXiv preprint arXiv:2009.09960, 2020. 2, 6

[16] P. Huber, G. Hu, R. Tena, P. Mortazavian, P. Koppen, W. J.
Christmas, M. Ratsch, and J. Kittler. A multiresolution 3d
In Proceed-
morphable face model and ﬁtting framework.
ings of the 11th International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Appli-
cations, 2016. 2

[17] H. Kim, M. Zoll¨ofer, A. Tewari, J. Thies, C. Richardt, and
T. Christian. InverseFaceNet: Deep Single-Shot Inverse Face
Rendering From A Single Image. In Proceedings of Com-
puter Vision and Pattern Recognition (CVPR 2018), 2018.
2

[18] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014. 5
[19] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013. 2

[20] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In 2nd International Conference on Learning Repre-
sentations, 2014. 3

[21] T. Kroeger, R. Timofte, D. Dai, and L. Van Gool. Fast op-
tical ﬂow using dense inverse search. In B. Leibe, J. Matas,
N. Sebe, and M. Welling, editors, Computer Vision – ECCV
2016, pages 471–488, Cham, 2016. Springer International
Publishing. 5

[22] D. Lee, H. Park, and C. D. Yoo. Face alignment using cas-
cade gaussian process regression trees. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4204–4212, 2015. 4

[23] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero. Learning
a model of facial shape and expression from 4d scans. ACM
Trans. Graph., 36(6):194–1, 2017. 2

[24] J. Lin, Y. Yuan, T. Shao, and K. Zhou. Towards high-ﬁdelity
3d face reconstruction from in-the-wild images using graph
In Proceedings of the IEEE/CVF
convolutional networks.
Conference on Computer Vision and Pattern Recognition,
pages 5891–5900, 2020. 2

[25] C. Liu and X. Li. Superimposition-guided facial reconstruc-

tion from skull. Arxiv, arXiv:1810.00107, 2018. 2

[26] C. Liu, Z. Li, S. Quan, and Y. Xu. Lighting estimation via
In 2020 IEEE Con-
differentiable screen-space rendering.
ference on Virtual Reality and 3D User Interfaces Abstracts
and Workshops (VRW), pages 575–576, 2020. 2

[27] S. Lombardi, J. Saragih, T. Simon, and Y. Sheikh. Deep
appearance models for face rendering. ACM Transactions
on Graphics (TOG), 37(4):1–13, 2018. 2, 3

9

[28] I. Matthews and S. Baker. Active appearance models revis-
International journal of computer vision, 60(2):135–

ited.
164, 2004. 2

[29] A. Meka, R. Pandey, C. H¨ane, S. Orts-Escolano, P. Barnum,
P. David-Son, D. Erickson, Y. Zhang, J. Taylor, S. Bouaziz,
et al. Deep relightable textures: volumetric performance cap-
ture with neural rendering. ACM Transactions on Graphics
(TOG), 39(6):1–21, 2020. 2

[30] C. M¨uller. Spherical harmonics, volume 17. Springer, 2006.

7

[31] T. Nestmeyer, J.-F. Lalonde, I. Matthews, and A. Lehrmann.
Learning physics-guided face relighting under directional
light. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 5124–5133,
2020. 3

[32] S. G. Parker, J. Bigler, A. Dietrich, H. Friedrich, J. Hoberock,
D. Luebke, D. McAllister, M. McGuire, K. Morley, A. Robi-
son, et al. Optix: a general purpose ray tracing engine. Acm
transactions on graphics (tog), 29(4):1–13, 2010. 4

[33] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vet-
ter. A 3d face model for pose and illumination invariant face
recognition. In 2009 Sixth IEEE International Conference on
Advanced Video and Signal Based Surveillance, pages 296–
301. Ieee, 2009. 2

[34] E. Richardson, M. Sela, and R. Kimmel. 3d face reconstruc-
tion by learning from synthetic data. In 2016 fourth interna-
tional conference on 3D vision (3DV), pages 460–469. IEEE,
2016. 2

[35] J. Roth, Y. Tong, and X. Liu. Unconstrained 3d face re-
In Proceedings of the IEEE conference on
construction.
computer vision and pattern recognition, pages 2606–2615,
2015. 2

[36] J. Roth, Y. Tong, and X. Liu. Adaptive 3d face reconstruction
IEEE Transactions
from unconstrained photo collections.
on Pattern Analysis and Machine Intelligence, 39(11):2127–
2141, December 2016. 2

[37] S. Sanyal, T. Bolkart, H. Feng, and M. J. Black. Learning
to regress 3d face shape and expression from an image with-
out 3d supervision. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 7763–
7772, 2019. 2, 6

[38] M. Sela, E. Richardson, and R. Kimmel. Unrestricted fa-
cial geometry reconstruction using image-to-image transla-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1576–1585, 2017. 2

[39] S. Sengupta, A. Kanazawa, C. D. Castillo, and D. W. Ja-
cobs. Sfsnet: Learning shape, reﬂectance and illuminance
of facesin the wild’. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 6296–
6305, 2018. 2

[40] J. Shang, T. Shen, S. Li, L. Zhou, M. Zhen, T. Fang, and
L. Quan. Self-supervised monocular 3d face reconstruction
by occlusion-aware multi-view geometry consistency. arXiv
preprint arXiv:2007.12494, 2020. 6, 7, 8

[41] A. Tewari, F. Bernard, P. Garrido, G. Bharaj, M. Elgharib,
H.-P. Seidel, P. P´erez, M. Zollh¨ofer, and C. Theobalt. FML:
Face Model Learning from Videos. In Proceedings of Com-

puter Vision and Pattern Recognition (CVPR 2019), 2019.
2

[42] A. Tewari, M. Zollh¨ofer, P. Garrido, F. Bernard, H. Kim,
P. P´erez, and C. Theobalt. Self-supervised multi-level face
model learning for monocular reconstruction at over 250 hz.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2549–2559, 2018. 2

[43] A. Tewari, M. Zollhofer, H. Kim, P. Garrido, F. Bernard,
P. Perez, and C. Theobalt. Mofa: Model-based deep con-
volutional face autoencoder for unsupervised monocular re-
construction. In Proceedings of the IEEE International Con-
ference on Computer Vision Workshops, pages 1274–1283,
2017. 2, 7

[44] J. Thies, M. Zollh¨ofer, M. Nießner, L. Valgaerts, M. Stam-
minger, and C. Theobalt. Real-time expression transfer for
facial reenactment. ACM Transactions on Graphics (TOG),
34(6), 2015. 2

[45] J. Thies, M. Zollh¨ofer, M. Stamminger, C. Theobalt, and
M. Niessner. Face2face: Real-time face capture and reen-
actment of rgb videos. Commun. ACM, 62(1):96–104, Dec.
2018. 2

[46] L. Thies, M. Zollhoefer, C. Richardt, C. Theobalt, and
G. Greiner. Real-time halfway domain reconstruction of mo-
tion and geometry. In Proceedings of the International Con-
ference on 3D Vision (3DV), 2016. 2

[47] L. Tran, F. Liu, and X. Liu. Towards high-ﬁdelity nonlinear
3d face morphable model. In In Proceeding of IEEE Com-
puter Vision and Pattern Recognition, Long Beach, CA, June
2019. 2

[48] L. Tran and X. Liu. On learning 3d face morphable model
from in-the-wild images. IEEE transactions on pattern anal-
ysis and machine intelligence, 2019. 2

[49] G. Tzimiropoulos, J. Alabort-i Medina, S. Zafeiriou, and
M. Pantic. Generic active appearance models revisited.
In Asian Conference on Computer Vision, pages 650–663.
Springer, 2012. 2

[50] X. Wang, Y. Guo, B. Deng, and J. Zhang. Lightweight pho-
tometric stereo for facial details recovery. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 740–749, 2020. 2

[51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
from error visibility to
IEEE transactions on image process-

Image quality assessment:

celli.
structural similarity.
ing, 13(4):600–612, 2004. 6

[52] C. Wu, T. Shiratori, and Y. Sheikh. Deep incremental learn-
ing for efﬁcient high-ﬁdelity face tracking. ACM Trans.
Graph., 37(6), Dec. 2018. 4

[53] H. Yang, H. Zhu, Y. Wang, M. Huang, Q. Shen, R. Yang, and
X. Cao. Facescape: A large-scale high quality 3d face dataset
In IEEE/CVF
and detailed riggable 3d face prediction.
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020. 6

[54] J. S. Yoon, T. Shiratori, S.-I. Yu, and H. S. Park. Self-
supervised adaptation of high-ﬁdelity face models for
In Proceedings of the
monocular performance tracking.
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 4601–4609, 2019. 2, 6, 7, 8

10

[55] E. Zakharov, A. Shysheya, E. Burkov, and V. Lempitsky.
Few-shot adversarial learning of realistic neural talking head
models. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 9459–9468, 2019. 6
[56] H. Zhou, J. Liu, Z. Liu, Y. Liu, and X. Wang. Rotate-
and-render: Unsupervised photorealistic face rotation from
the IEEE/CVF
single-view images.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2020. 2

In Proceedings of

[57] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, and Z. Liu.
Pose-controllable talking face generation by implicitly mod-
ularized audio-visual representation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2021. 2

[58] X. Zhu, X. Liu, Z. Lei, and S. Z. Li. Face alignment in full
pose range: A 3d total solution. IEEE transactions on pat-
tern analysis and machine intelligence, 41(1):78–92, 2017.
2

[59] M. Zollh¨ofer.

Dynamic Scenes.
Nuremberg, 2015. 2

Real-Time Reconstruction of Static and
PhD thesis, University of Erlangen-

11

Figure 10. The detailed network structure of our lighting transfer
network (G).

Supplemental Materials

A. Network Structure

We present the detailed network structure in Fig. 10.

B. Inputs and Outputs

The lighting code l is a pre-deﬁned vector when we
train G on light-stage data, and is a learnable vector when
we reﬁne G on in-the-wild video frames. During train-
ing on light-stage data, the lighting direction is encoded
by the position of the non-zero element in l, and the light-
ing color is encoded by the value of the non-zero ele-
ment in l. The view-dependent head pose hv ∈ R6 =
{rx, ry, rz, vv
z }, where r = {rx, ry, rz} and vv =
{vv
z } are rigid head rotation and viewpoint vector,
respectively. The fully-lit texture ˆTv is obtained from DAM
decoder, and we down-sample it to the size of 3×256×256.
The outputs are the gain and bias map gv, bv, and we
upsample the output gv, bv back to the size of 3 × 1024 ×
1024 by bilinear interpolation.

x, vv

x, vv

y, vv

y, vv

12

25625612812838440962564ConvolutionFullyconnectedlayer+LeakyReLULeakyReLUConvolution2564LeakyReLUTransposeConvolution256812816LeakyReLUConvolution32256325664256641286412864641286412832128321281625616256161283212832646464643212832128256256：：：：：Concatenation：Down-sample：Reshape