Human Performance Capture from Monocular Video in the Wild

Chen Guo1 Xu Chen1,2

Jie Song1 Otmar Hilliges1

1ETH Z¨urich

2Max Planck Institute for Intelligent Systems, T¨ubingen

1
2
0
2

v
o
N
0
3

]

V
C
.
s
c
[

2
v
2
7
6
4
1
.
1
1
1
2
:
v
i
X
r
a

Abstract

Capturing the dynamically deforming 3D shape of
clothed human is essential for numerous applications, in-
cluding VR/AR, autonomous driving, and human-computer
interaction. Existing methods either require a highly spe-
cialized capturing setup, such as expensive multi-view
imaging systems, or they lack robustness to challenging
body poses.
In this work, we propose a method capable
of capturing the dynamic 3D human shape from a monocu-
lar video featuring challenging body poses, without any ad-
ditional input. We ﬁrst build a 3D template human model
of the subject based on a learned regression model. We
then track this template model’s deformation under chal-
lenging body articulations based on 2D image observa-
tions. Our method outperforms state-of-the-art methods on
an in-the-wild human video dataset 3DPW. Moreover, we
demonstrate its efﬁcacy in robustness and generalizability
on videos from iPER datasets.

1. Introduction

In this paper, we study the problem of human perfor-
mance capture from monocular in-the-wild video.
It is a
task of reconstructing dynamically deforming 3D shapes of
human in clothing from a video featuring human motion,
which is key to many applications in ﬁlm/sport industry,
VR/AR, and also human-computer interaction. However,
reconstructing the detailed 3D geometry of human is chal-
lenging due to depth ambiguities from monocular input, the
inherently complex human motions, and the high degrees of
freedom in clothing deformations.

Recently, there has been remarkable progress in this
setting which can be categorized into two paradigms:
learning-based approaches [41, 42], and tracking-based ap-
proaches [54, 16, 17]. Methods following the learning-
based paradigm [41, 42] learn the mapping from 2D pix-
els to 3D shapes using a large amount of 3D human scans.
Although these methods can provide highly detailed recon-
structions of human in clothing, they typically struggle in
the out-of-distribution setting. On the other hand, tracking-
based methods [54, 16, 17] use a pre-rigged and subject-

Figure 1. Human performance capture from monocular video
in the wild. We present a method to reconstruct the dynamically
deforming 3D surface of human from a monocular video. Our
method does not require a pre-scanned subject-speciﬁc template
model and generalizes well to challenging poses, thus it is appli-
cable in in-the-wild settings.
speciﬁc 3D template and track the template across time
from a monocular video. These methods generalize better
in in-the-wild setting and are more robust under challenging
poses. However, acquiring the subject-speciﬁc template re-
quires a massive multi-view capturing setup, and extensive
manual efforts for post-processing, preventing such meth-
ods from being deployed to real-life applications.

In this paper, we explore to combine the best of both
lines of work to build an automatic and effective system,
which can model detailed clothing deformations and is ro-
bust to in-the-wild settings without assuming a subject-
speciﬁc template. Given a monocular video as input, we
ﬁrst leverage a learning-based single-view reconstruction
method to build a 3D template from the initial frame, with-
out additional capturing process or manual interference. We
then track this template human model along time by de-
forming it based on 2D image observations. More specif-

 
 
 
 
 
 
ically, we utilize a pre-trained single view human recon-
struction model [42] to ﬁrst infer the rigid mesh model from
the initial frame. We then register a generic human model
SMPL [34] onto the rigid mesh by optimizing SMPL pose
and shape parameters as well as the per-vertex displace-
ments. In this way, we obtain a parametric template model
without any additional input or human effort.

Next, we track the detailed 3D geometry at each frame
by estimating the deformations of the template. The defor-
mations are predicted by ﬁtting the current template to 2D
observations, including joints and silhouettes via a gradient-
based optimization scheme. The ﬁtting process is decom-
posed into two stages, similar to [54]. We ﬁrst optimize the
pose parameter of the model, yielding a coarse alignment
to the image, and then optimize the detailed surface defor-
mations to further reﬁne the alignment.
In this way, our
method faithfully estimates both the body motion and the
local surface deformations, hence produces the detailed 3D
shapes at each frame.

We evaluate our proposed method on an in-the-wild hu-
man video dataset [50] and demonstrate that our method
outperforms the state-of-the-art learning-based method es-
pecially when poses are challenging. We further compare
our method with other tracking-based methods. Our method
achieves on-par results but eliminates the need for multi-
view capturing setup and manual efforts.

2. Related Work

Human Reconstruction from Multi-view/Depth: In the
multi-view setting, current approaches [12, 22, 33, 45, 49,
39, 20, 44, 19] estimate detailed 3D human shape based
on geometric and photometric cues such as silhouette [45],
multi-view correspondences [33], and shading [52]. Such
methods typically require a large amount of cameras to
achieve compelling results. Recent works [2, 4, 5] attempt
to reconstruct shape from fewer cameras or pseudo multi-
view setting where the subject rotates in front of a monoc-
ular camera with ﬁxed body pose. Depth-based approaches
[37, 9, 10] reconstructs the human shape by fusing depth
measurements across time, in order to ﬁlter sensor noise
and complete occluded regions. Body prior has been intro-
duced to handle large deformations [56, 57, 30, 51, 58, 29].
While the aforementioned methods achieve compelling re-
sults, they require a specialized capturing setup and are
hence not applicable to in-the-wild settings. In contrast, our
method is capable of recovering the dynamic human shape
in the wild from a monocular RGB video as the sole input.

eral clothing from a monocular video by ﬁtting the tem-
plate to estimated 2D and 3D human joints and 2D silhou-
ettes. LiveCap [17] further incorporates body and cloth-
ing segmentation cues to model different non-rigid defor-
mation behaviors of skin and apparel. DeepCap [17] re-
places iterative optimization with deep neural networks for
estimating both poses and surface deformations. However,
obtaining the subject-speciﬁc template requires a massive
multi-view capturing setup and extensive manual efforts for
post-processing. Our method achieves comparable results
but does not require a pre-built template. Therefore, our
method can be applied in in-the-wild settings.

Learning-based Approaches with Monocular RGB:
Learning-based methods learn to regress 3D human shape
from images by leveraging large-scale datasets. [23, 27, 38,
25, 15, 43, 26, 47, 31] learn to infer body pose and shape
from a single image, but only consider minimally clothed
human. Various methods [48, 60, 6, 42, 41, 18, 21, 59,
28, 36, 13] have recently been proposed to reconstruct hu-
man in clothing. BodyNet [48] and DeepHuman [60] out-
put human shape in the form of occupancy voxel grids.
Such representation has difﬁculties to capture ﬁne details
due to the high memory footprint. Neural implicit functions
have been introduced to replace an explicit voxel grid and
have enabled high-ﬁdelity reconstructions from single im-
ages [42, 41, 18, 21, 59, 28]. A major limitation of these
methods is the lack of generalization to unseen poses in the
wild. Our method leverages such methods to reconstruct a
template of human in clothing, and generalizes well to poses
beyond the training distribution by tracking the template’s
deformations based on image observations.

3. Methodology

Given a monocular video, our goal is to estimate the dy-
namically changing 3D surface of the subject at each frame.
As shown in Fig. 2, we ﬁrst build a template from the initial
frame of the given video, and then track how this template
deforms in the successive frames based on 2D observations.

3.1. Template Construction from Image

At the ﬁrst stage, we construct a parametric 3D template
of human with clothing for the subject. The construction
process only uses one frame from the input video, without
requiring multi-view setup or manual efforts.

3.1.1 Single-view Human Reconstruction

Tracking-based Approaches with Monocular RGB:
Tracking-based methods assume a pre-built,
subject-
speciﬁc 3D template model and track this model across time
based on monocular video sequences [16, 17, 53, 54, 55].
MonoPerfCap [54] captures the dynamic human with gen-

We ﬁrst leverage a state-of-the-art single-view human re-
construction method [42] to reconstruct the detailed shape
of human from a single frame. We run the pre-trained model
to obtain a rigid 3D mesh S. The mesh surface is extracted
from the implicit representation via marching cubes.

Figure 2. Method overview. Given a monocular video as input, a parametric template model is automatically constructed from the initial
frame of the video. We reconstruct the rigid 3D shape using the state-of-the-art single view reconstruction method [42]. We then register a
generic human model SMPL [34] onto the rigid shape to build the parametric template with an embedded deformation graph. To reconstruct
the dynamically deforming 3D surface at each frame, we optimize the pose, shape, and surface deformation parameters of the template to
image observations. We ﬁrst extract 2D joints and silhouettes from the RGB image. From 2D joints, we estimate 3D body poses using
[43] to initialize the pose parameter. After initialization, we optimize the pose parameters by aligning the template with the 2D joints and
silhouette. Afterward, we further optimize the detailed surface deformation by silhouette alignment.

3.1.2 Parametric 3D Template

The resulting 3D mesh from the previous step does not yet
support tracking, as vertices from independent body parts,
e.g., hand and torso, might be connected. Such incorrect
connectivity prevents these parts from being separated in
later frames. To ensure correct connectivity of the mesh, we
register a parametric human model SMPL onto the recon-
structed rigid mesh S, obtaining a parametric template. Be-
side guaranteeing correct connectivity, this parametric tem-
plate also disentangles global skeletal deformations from lo-
cal surface deformations, which enables tracking the defor-
mations in a coarse-to-ﬁne manner at the later stage. We
ﬁrst introduce the parameterization of the template, and
then describe how we obtain such a template from the rigid
scan.

R72, shape β ∈ R10 and global translation t ∈ R3 to the
positions of N = 6890 vertices of a human mesh. To model
details such as clothing, we further introduce the vertex dis-
placements D ∈ RN ×3 as additional parameters, similar to
[3]. The vertex positions M are determined as

M (θ, β, D) = W (T (θ, β, D), β, θ, W),
T (θ, β, D) = Tµ + Bs(β) + Bp(θ) + D,

(1)

(2)

where W (·) is the linear blend skinning algorithm which
deforms a canonical mesh T (θ, β, D) to desired body poses
based on the pre-deﬁned skinning weights W and bone
transformations derived from pose θ and shape β. The
canonical mesh T (θ, β, D) is obtained by linearly combin-
ing shape-dependent deformations Bs(β), pose-dependent
deformations Bp(θ) and the vertex displacements D.

Parameterization: SMPL [34] is a linear statistical model
of minimally clothed human bodies, mapping pose θ ∈

Registration: To build the subject-speciﬁc template, we
register the parametric template to the estimated rigid shape

PoseRefineSurfaceRefineInput Frame 0Input Frame f3D RigidShape3D ParametricTemplateDeformationGraphEstimated 2D Joints3D Pose InitializationPose RefinedOutput 3D ShapeEstimated 2D SilhouetteRegistrationPre-trained NetworksOptimizationS. We ﬁrst use IP-Net [7] to obtain a minimally clothed reg-
istration, i.e. the shape β and pose θ parameter of SMPL, as
initialization. Then we optimize the vertex displacements
D to minimize the following energy function:

min
D

Ereg = Echamfer + λlapElap + λoffsetEoffset,

(3)

where Echamfer depicts the bi-directional Chamfer difference
between the template M and the reconstructed rigid mesh
S, and Elap and Eoffset are regularization terms weighted by
λlap and λoffset respectively. Elap is the Laplacian regularizer
and Eoffset is the L2 norm of the vertex displacements D,
which penalizes deviation from the minimally clothed body.

Embedded Deformation Graph: Directly optimizing ver-
tex displacements D based on 2D images is subject to er-
rors and artifacts due to the high degrees of freedom. Thus,
following [46], we build an embedded deformation graph
D with K = 689 nodes, parameterized with axis angles
A ∈ RK×3 and translations T ∈ RK×3. The vertex dis-
placements are then derived from the associated deforma-
tion nodes, hence the number of parameters to be optimized
is greatly reduced.

3.2. Video-based Template Tracking

From the previous stage, we obtain a template parame-
terized by the body pose θ and shape β, which are inher-
ited from SMPL, and the surface deformations controlled
by the deformation graph D. To infer the human surface at
successive frames, we optimize these parameters by ﬁtting
the model to image observations. This section introduces
energy terms used during the optimization procedure, the
initialization scheme for the parameters, and ﬁnally the op-
timization routine.

3.2.1 Energy Functions

2D Joint Alignment Ejoint: This term measures the dis-
tance between estimated 2D joints J2D,est from OpenPose
[11] and the 2D projection of 3D SMPL joints:

Ejoint(θ, β, t) =

Njoint
(cid:88)

i=1

wiρ (Π (J(θ, β, t)) − J2D,est,i) (4)

where J(θ, β, t) is the 3D SMPL joints given the SMPL
parameters. We sum up the distances for each joint i over-
all counted joints Njoint. Π denotes the 3D to 2D projec-
tion of joints with intrinsic camera parameters. To account
for detection noise, the error terms are weighted by the
corresponding detection conﬁdence wi. A robust Geman-
McClure error function ρ [14] is applied to down-weight
outlier 2D detections.

2D Silhouette Alignment Esil: This term measures the
overlap between the projected silhouette of the model and

the estimated silhouette in the image. It serves as an impor-
tant cue for inferring the surface deformations. We extract
human silhouette from images using MODNet [24]. We
calculate the overlap by comparing the difference for each
pixel p in the image and take an average among all pixels P
as the ﬁnal result:

Esil(θ, β, t, D) =

1
|P|

(cid:88)

p∈P

(cid:107)Sproj,p(θ, β, t, D) − Sest,p(cid:107)2
2

(5)

where Sproj is the silhouette rendered by a differentiable
mesh renderer [40].

Pose Plausibility Eprior: This term, proposed in [8], reﬂects
how plausible a pose is, given a pose prior learned from a
large scale realistic pose corpus [1, 35]. The pose prior is
modelled as a mixture of Ngauss = 8 Gaussian distributions
with learned weights αj, mean µj, and variance Σj. The
pose plausibility is given as:

Eprior(θ) = − log

Ngauss
(cid:88)

j=1

αjN (cid:0)θ; µj, Σj

(cid:1) .

(6)

Temporal Pose Stability Estab: This energy term is deﬁned
as the mean squared error of the current frame and the last
frame 3D SMPL joints, which penalizes temporal pose jit-
tering:

Estab (θ, β, t) =

Njoint
(cid:88)

i=1

(cid:13)
(cid:13)J(θ, β, t)f
(cid:13)

i − J f −1

i

(cid:13)
2
(cid:13)
(cid:13)
2

.

(7)

As-rigid-as-possible Earap: This term reﬂects the deviation
of estimated local surface deformations from rigid transfor-
mations. Here, g ∈ RK×3 are the original positions of
the nodes in the embedded graph D and Φ(k) is the 1-ring
neighbourhood of deformation node k.

Earap(D) =

(cid:88)

(cid:88)

k∈K

l∈Φ(k)

(cid:107)dk,l(A, T)(cid:107)2
2 ,

(8)

dk,l(A, T) = R (Ak) (gl − gk) + Tk + gk − (gl + Tl) ,
(9)

where R (·) is the Rodrigues’ rotation formula that com-
putes a rotation matrix from an axis–angle representation.

3.2.2

Initialization

At the ﬁrst frame, we estimate the global translation t based
on the human bounding box in the 2D image. We initialize
the rotations A and translations T of the deformation graph
with zero values.

For each frame, the global translation t and deformation
graph D are initialized with the results from the previous

frame. In terms of pose parameter θ, we obtain initial val-
ues from the state-of-the-art human mesh recovery method
LGD [43]. This is crucial to recover from lost track and to
prevent error accumulation during tracking. The shape pa-
rameter β is only optimized at the ﬁrst frame and is kept
ﬁxed for the remaining.

3.2.3 Optimization Routine

We decompose the optimization routine into two stages.
The ﬁrst stage is responsible for capturing the accurate hu-
man pose, and the second stage is designed to reﬁne the
outer surface.

Pose Reﬁnement.: We reﬁne the pose θ, shape β, and the
global translation t of SMPL model to minimize the 2D
joint and silhouette alignment energy terms, regularized by
the pose prior and stability terms:

min
θ,β,t

Epose = EJ2D + λsilEsil
(cid:125)

(cid:124)

(cid:123)(cid:122)
data ﬁtting

+ λstabEstab + λpriorEprior
(cid:123)(cid:122)
(cid:125)
regularization

(cid:124)

(10)

Surface Reﬁnement.: We further reﬁne the surface defor-
mations, represented by the deformation graph D, to better
align the parametric template to the extracted image silhou-
ettes. This step captures the non-rigid surface deformations
of apparel and skin. We optimize D with the silhouette
alignment term and the as-rigid-as-possible regularizer, and
keep other parameters ﬁxed:

min
D

Esurf = Esil
(cid:124)(cid:123)(cid:122)(cid:125)
data ﬁtting

+ λarapEarap
(cid:124)
(cid:125)
(cid:123)(cid:122)
regularization

.

(11)

where λ(·) are the weights for the corresponding energy
terms.
the per-frame estimates are temporally
smoothed based on a centered sliding window of 5 frames.
Please refer to the supplementary material for more details.

Finally,

4. Experiments

We compare our method with the state-of-the-art
learning-based single view reconstruction method on an in-
the-wild video dataset. In addition, we also conduct a qual-
itative evaluation with a tracking-based method that relies
on a pre-scanned template mesh. Finally, we visualize the
effect of individual components on the ﬁnal results.

4.1. Datasets

We use the following datasets for evaluation and note

that none of our modules is trained on any dataset below:

3DPW Dataset [50]: This dataset records challenging in-
the-wild video sequences with accurate 3D human poses re-
covered by using IMUs and a moving camera. Moreover, it

Pose Occ.

H

E

E

H

H

H

E

E

Sequence
downtown car
downtown downstairs
downtown runForBus
downtown sitOnStairs
downtown upstairs
downtown walkUphill
downtown weeklyMarket
outdoors fencing
Avg.
downtown enterShop
downtown stairs
downtown walkBridge
downtown walking
downtown windowShopping
Avg.
downtown bar
downtown cafe
downtown warmWelcome
ﬂat actions
ofﬁce phoneCall
Avg.
downtown arguing
downtown bus
downtown crossStreets
downtown rampAndStairs
Avg.
Overall Avg.

PIFuHD SMPL Tracking Ours
3.26
2.78
3.36
2.98
2.58
2.70
2.72
3.39
3.08
2.58
3.48
2.96
3.15
2.71
2.98
3.74
3.03
3.84
3.13
2.58
3.26
3.39
3.20
3.00
3.11
3.19
3.12

3.57
2.93
3.72
3.15
2.74
2.83
3.09
3.87
3.35
2.77
3.79
3.21
3.28
3.17
3.23
4.02
3.09
4.05
3.14
2.56
3.38
3.61
3.40
3.33
3.16
3.38
3.34

3.57
3.03
4.33
4.41
2.83
3.44
3.29
6.29
4.09
2.65
4.24
3.01
3.38
2.85
3.21
4.26
4.00
3.92
6.21
3.34
4.43
2.84
3.20
3.02
3.01
3.03
3.76

Table 1. Quantitative evaluation on 3DPW dataset. Chamfer
distance (cm) between reconstructed and ground-truth meshes are
reported. The test dataset is divided into 4 parts based on the com-
plexity (Easy and Hard) of pose and occlusion. Our method out-
performs PIFuHD in most scenarios, especially under challenging
conditions, and consistently outperforms SMPL tracking baseline.

includes 3D scans and registered 3D people models with 18
clothing variations. By feeding the human model with the
ground-truth poses and shapes, we can obtain quasi-scans
to evaluate our method in terms of surface reconstruction
accuracy. We evaluate our method on the test split1, and
consider every 10-th frame for evaluation. Following the
standard 3DPW evaluation protocol, we discard frames in
which less than 6 joints are detected. In total, the evalua-
tion set contains 24 video sequences with 3569 frames. We
compute Chamfer distance (in cm) between our prediction
and the ground-truth averaged over all frames for the cor-
responding sequence as the surface reconstruction metric.

MonoPerfCap Dataset [54]: This dataset contains videos
of people in different garment types and actions. Subject-
speciﬁc templates are also provided, which are required
for tracking-based methods. In contrast, our method only
uses the video not the provided template. As no per-frame
ground-truth surface is provided, we resort to qualitative
comparison with this baseline.

iPER Dataset [32]: This dataset contains videos of subjects
in various shapes and garments performing various actions.

1in case of heavy occlusions in the initial frame, we reconstruct the

template from a later frame.

Figure 3. Qualitative comparison on 3DPW and iPER dataset. Results of learning-based method PIFuHD, SMPL-based tracking and
our method are shown. For 3DPW, we show the results in different levels of difﬁculties as speciﬁed in respective sub-captions. Our method
produces plausible results under challenging scenarios and achieves accurate surface-image alignment.
In contrast, PIFuHD’s results
degenerate under challenging body poses and heavy occlusions. Using SMPL model as a generic template instead of the automatically
constructed template, the method fails to capture the clothing shape and deformation.

4.2. Comparison with Learning-based Method

We consider PIFuHD [42] as our learning-based base-
line. This method is state-of-the-art in single view human
reconstruction. It uses pixel-aligned features extracted from
high-resolution images to guide the coarse-to-ﬁne recon-
struction. We quantitatively evaluate our method and PI-
FuHD on the 3DPW dataset. Tab. 1 summarizes surface
reconstruction accuracy. As can be seen, our method on
average achieves approximately 17% less error under these
challenging scenarios, demonstrating the robustness of our
method. This improvement is even more visible qualita-
tively as shown in Fig. 3. Our method produces plausible
results even for highly dynamic poses and heavy occlusions,
which are challenging for PIFuHD.

To further understand our performance, we divide the
test dataset into 4 different parts with different levels of
complexity in terms of pose and occlusion. Please refer
In the case
to Fig. 3 and Tab. 1 for results in each split.
of hard poses (H) but little occlusions (E), our approach
consistently outperforms PIFuHD by a large margin. As for
simple poses (E) with strong occlusions (H), our method
also shows its advantage of being able to reconstruct unseen
regions. In the case where both pose (H) and occlusion (H)
are challenging, our method is still able to produce mean-

ingful results while PIFuHD struggles to reconstruct plau-
sible shapes. Finally, in ideal conditions with simple poses
(E) and few occlusions (E), our method is less accurate
than PIFuHD due to the limited resolution of the template
mesh compared to PIFuHD’s output.

4.3. Comparison with Tracking-based Method

We compare our method with MonoPerfCap [54], a rep-
resentative tracking-based method. This method captures
the human performance from a monocular video, but re-
quires a pre-built subject-speciﬁc template model. We thus
conduct the evaluation on their own dataset, which provides
such templates. Note that our method does not use these
templates but only takes the video as input. As no ground-
truth surface is provided in MonoPerfCap’s dataset, we are
only able to conduct a qualitative comparison. As shown in
Fig. 4, without requiring the pre-built subject-speciﬁc tem-
plate, our method achieves comparable results in terms of
the body pose accuracy and the ﬁdelity of local details.

4.4. Effect of Template Reconstruction from Image

To verify the necessity of building the parametric 3D
template for the subject, we provide an additional base-
line in which we replace the reconstructed template with
SMPL model as a generic template. As shown in Tab. 1,

Figure 4. Qualitative comparison with MonoPerfCap. MonoP-
erfCap requires a subject-speciﬁc template in addition to the
monocular video, which requires multi-view capturing setup and
manual efforts. In contrast, our method does not require such a
template as input and achieves comparable perceptual results.

our method consistently outperforms this baseline (SMPL
tracking). The reason is that this baseline fails to align the
model to the image observations due to the shape mismatch,
as displayed in Fig. 3. In addition, this baseline also fails to
capture clothing and body details.

4.5. User Study

We conduct a user study to quantify the visual effects of
our method. We randomly pick 8 video clips from 3DPW
and iPER dataset and ask 30 users to choose which method
is preferred in terms of accuracy and perceptual ﬁdelity. The
survey in Tab. 2 indicates that our method is favored more
often than baselines. PIFuHD’s low performance relates to
the occasional ﬂickering when the method fails entirely.

Vote rate

PIFuHD SMPL Tracking Ours
8.33%
5.83%

85.83%

Table 2. User study. Vote rate in average.

4.6. Effect of Optimization Stages

We now illustrate the effect of main steps during track-
ing, namely, 3D pose initialization, pose and surface reﬁne-
ment. First, we use the pose from the previous frame to
replace the learned 3D initialization. As shown in Fig. 5,
while the estimated surface still aligns with the 2D joints
and silhouette, the 3D pose is implausible, demonstrating
that the learned 3D pose initialization is important to tackle

Figure 5. Qualitative evaluation of optimization stages. With-
out a learned 3D pose initialization, the method tends to produce
invalid poses due to accumulated error. Without pose reﬁnement,
the method suffers from noticeable misalignment to the image ob-
servation. Without surface reﬁnement, the method fails to capture
the non-linear deformations of the body and clothing.

the inherent depth ambiguity from a single view. Secondly,
we keep all components but skip the pose reﬁnement stage.
This leads to notable misalignment to the image observa-
tion, e.g., the hands in Fig. 5. Finally, removing the surface
reﬁnement step from the full pipeline leads to even more
notable misalignment, e.g., at the boundary of the pants.
The complete pipeline achieves accurate surface-to-image
alignment without suffering from degenerated poses.

4.7. Qualitative Results

We show qualitative results on different datasets in Fig. 6
with overlaid images and the ones from 3D free-view
points. Our approach can generalize to online videos with
different garments, contexts and gestures. Please refer to
the supplementary materials and video for more samples.

5. Conclusion

We propose a method to estimate 3D human shape
in clothing from a sole monocular video. Compared to
tracking-based methods, our method does not require a pre-
scanned template thus can be applied more broadly, such
as internet videos. Compared to learning-based ones, our
method generalizes better to in-the-wild videos with natural
and dynamic poses. Our attempt demonstrates the potential
of integrating tracking and learning-based methods to tackle
the problem of 3D human reconstruction.
Acknowledgements: Xu Chen was supported by the Max Planck
ETH Center for Learning Systems.

Figure 6. Additional qualitative results on iPER, MonoPerfCap and online videos. Every two rows form a group. The top row shows
the input images and the bottom row shows the estimated surfaces. On the right side, we visualize the surface from a new viewpoint.

References

[1] http://mocap.cs.cmu.edu. 4
[2] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and
G. Pons-Moll. Learning to reconstruct people in clothing
from a single RGB camera. In Proc. IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2019. 2
[3] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and
G. Pons-Moll. Learning to reconstruct people in clothing
from a single RGB camera. In Proc. IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2019. 3
[4] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-
Moll. Detailed human avatars from monocular video.
In
International Conference on 3D Vision (3DV), 2018. 2
[5] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-
Moll. Video based reconstruction of 3d people models. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 2

[6] T. Alldieck, G. Pons-Moll, C. Theobalt, and M. Magnor.
Tex2shape: Detailed full human body geometry from a sin-
gle image. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), 2019. 2

[7] B. L. Bhatnagar, C. Sminchisescu, C. Theobalt, and G. Pons-
Moll. Combining implicit function learning and parametric
models for 3d human reconstruction. In Proc. of the Euro-
pean Conf. on Computer Vision (ECCV), 2020. 4

[8] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero,
and M. J. Black. Keep it SMPL: Automatic estimation of 3D
human pose and shape from a single image. In Proc. of the
European Conf. on Computer Vision (ECCV), 2016. 4
[9] A. Boˇziˇc, P. Palafox, M. Zollh¨ofer, J. Thies, A. Dai,
and M. Nießner. Neural deformation graphs for globally-
consistent non-rigid reconstruction. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2021. 2
[10] A. Boˇziˇc, M. Zollh¨ofer, C. Theobalt, and M. Nießner. Deep-
deform: Learning non-rigid rgb-d reconstruction with semi-
In Proc. IEEE Conf. on Computer Vision
supervised data.
and Pattern Recognition (CVPR), 2020. 2

[11] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.
Sheikh. Openpose: Realtime multi-person 2d pose estima-
tion using part afﬁnity ﬁelds. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2019. 4

[12] E. de Aguiar, C. Stoll, C. Theobalt, N. Ahmed, H.-P. Seidel,
and S. Thrun. Performance capture from sparse multi-view
video. ACM Trans. on Graphics, 27(3):1–10, 2008. 2
[13] V. Gabeur, J.-S. Franco, X. Martin, C. Schmid, and G. Ro-
gez. Moulding humans: Non-parametric 3d human shape
estimation from single images. In Proc. of the IEEE Interna-
tional Conf. on Computer Vision (ICCV), 2019. 2

[14] S. Geman and D. E. McClure. Statistical methods for tomo-

graphic image reconstruction. 1987. 4

[15] R. A. Guler and I. Kokkinos. Holopose: Holistic 3d human
reconstruction in-the-wild. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2019. 2

[16] M. Habermann, W. Xu, , M. Zollhoefer, G. Pons-Moll, and
C. Theobalt. Livecap: Real-time human performance capture
from monocular video. ACM Trans. on Graphics, 2019. 1, 2

[17] M. Habermann, W. Xu, M. Zollhoefer, G. Pons-Moll, and
C. Theobalt. Deepcap: Monocular human performance cap-
ture using weak supervision. In Proc. IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2020. 1, 2
[18] T. He, J. Collomosse, H. Jin, and S. Soatto. Geo-pifu: Ge-
ometry and pixel aligned implicit functions for single-view
human reconstruction. arXiv.org, 2006.08072, 2020. 2
[19] A. Hilton and J. Starck. Multiple view reconstruction of
In Proceedings. 2nd International Symposium on
people.
3D Data Processing, Visualization and Transmission, 2004.
3DPVT 2004., pages 357–364, 2004. 2

[20] Y. Huang, F. Bogo, C. Lassner, A. Kanazawa, P. V. Gehler,
J. Romero, I. Akhter, and M. J. Black. Towards accurate
marker-less human shape and pose estimation over time. In
International Conference on 3D Vision (3DV), pages 421–
430, 2017. 2

[21] Z. Huang, Y. Xu, C. Lassner, H. Li, and T. Tung. ARCH: An-
imatable Reconstruction of Clothed Humans. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
Seattle, Washington, 2020. IEEE. 2

[22] T. Kanade, P. Rander, and P. Narayanan. Virtualized reality:
constructing virtual worlds from real scenes. IEEE MultiMe-
dia, 4(1):34–47, 1997. 2

[23] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
In Proc. IEEE
to-end recovery of human shape and pose.
Conf. on Computer Vision and Pattern Recognition (CVPR),
2018. 2

[24] Z. Ke, K. Li, Y. Zhou, Q. Wu, X. Mao, Q. Yan, and R. W.
Lau. Is a green screen really necessary for real-time portrait
matting? arXiv.org, 2011.11961, 2020. 4

[25] M. Kocabas, N. Athanasiou, and M. J. Black. Vibe: Video
In
inference for human body pose and shape estimation.
Proc. IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2020. 2

[26] M. Kocabas, C.-H. P. Huang, O. Hilliges, and M. J. Black.
Pare: Part attention regressor for 3d human body estimation.
arXiv.org, 2104.08527, 2021. 2

[27] N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis.
Learning to reconstruct 3d human pose and shape via model-
ﬁtting in the loop. In Proc. of the IEEE International Conf.
on Computer Vision (ICCV), 2019. 2

[28] R. Li, Y. Xiu, S. Saito, Z. Huang, K. Olszewski, and H. Li.
Monocular real-time volumetric performance capture.
In
Proc. of the European Conf. on Computer Vision (ECCV),
pages 49–67. Springer, 2020. 2

[29] Z. Li, T. Yu, C. Pan, Z. Zheng, and Y. Liu. Robust 3d self-
portraits in seconds. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2020. 2

[30] Z. Li, T. Yu, Z. Zheng, K. Guo, and Y. Liu. Posefusion:
Pose-guided selective fusion for single-view human volu-
In Proc. IEEE Conf. on Computer Vision
metric capture.
and Pattern Recognition (CVPR), 2021. 2

[31] K. Lin, L. Wang, and Z. Liu. End-to-end human pose and
mesh reconstruction with transformers. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2021.
2

[32] W. Liu, Z. Piao, J. Min, W. Luo, L. Ma, and S. Gao.
Liquid warping gan: A uniﬁed framework for human mo-
tion imitation, appearance transfer and novel view synthesis.
arXiv.org, 1909.12224, 2019. 5

[33] Y. Liu, Q. Dai, and W. Xu. A point-cloud-based multiview
stereo algorithm for free-viewpoint video. IEEE Trans. Vis.
Comput. Graph., 16(3):407–418, 2010. 2

[34] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J.
Black. SMPL: A skinned multi-person linear model. ACM
Trans. on Graphics, 34(6):248:1–248:16, Oct. 2015. 2, 3
[35] M. M. Loper, N. Mahmood, and M. J. Black. MoSh: Motion
and shape capture from sparse markers. ACM Transactions
on Graphics, (Proc. SIGGRAPH Asia), 33(6):220:1–220:13,
Nov. 2014. 4

[36] R. Natsume, S. Saito, Z. Huang, W. Chen, C. Ma, H. Li, and
S. Morishima. Siclope: Silhouette-based clothed people. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2019. 2

[37] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion:
Reconstruction and tracking of non-rigid scenes in real-time.
In Proc. IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), pages 343–352, 2015. 2

[38] M. Omran, C. Lassner, G. Pons-Moll, P. V. Gehler, and
B. Schiele. Neural body ﬁtting: Unifying deep learning and
model-based human pose and shape estimation. In Interna-
tional Conference on 3D Vision (3DV), 2018. 2

[39] G. Pons-Moll, S. Pujades, S. Hu, and M. J. Black. ClothCap:
Seamless 4D Clothing Capture and Retargeting. ACM Trans.
on Graphics, 36(4):1–15, 2017. 2

[40] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo,
J. Johnson, and G. Gkioxari. Accelerating 3d deep learning
with pytorch3d. arXiv.org, 2007.08501, 2020. 4

[41] S. Saito, Z. Huang, R. Natsume, S. Morishima,
A. Kanazawa, and H. Li.
Pifu: Pixel-aligned implicit
function for high-resolution clothed human digitization. In
Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2019. 1, 2

[42] S. Saito, T. Simon, J. Saragih, and H. Joo. Pifuhd: Multi-
level pixel-aligned implicit function for high-resolution 3d
human digitization. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2020. 1, 2, 3, 6

[43] J. Song, X. Chen, and O. Hilliges. Human body model ﬁtting
by learned gradient descent. In Proc. of the European Conf.
on Computer Vision (ECCV), 2020. 2, 3, 5

[44] J. Starck and A. Hilton. Model-based multiple view recon-
struction of people. In Proc. of the IEEE International Conf.
on Computer Vision (ICCV), USA, 2003. IEEE Computer
Society. 2

[45] J. Starck and A. Hilton. Surface capture for performance-
IEEE Computer Graphics and Applica-

based animation.
tions, 27(3):21–31, 2007. 2

[46] R. Sumner, J. Schmid, and M. Pauly. Embedded deformation
for shape manipulation. ACM Trans. on Graphics, 26, 07
2007. 4

[47] H.-Y. Tung, H.-W. Tung, E. Yumer, and K. Fragkiadaki.
In I. Guyon,

Self-supervised learning of motion capture.

U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems (NeurIPS), pages 5236–5246.
Curran Associates, Inc., 2017. 2

[48] G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev,
and C. Schmid. BodyNet: Volumetric inference of 3D hu-
man body shapes. In Proc. of the European Conf. on Com-
puter Vision (ECCV), 2018. 2

[49] D. Vlasic, I. Baran, W. Matusik, and J. Popovi´c. Articulated
mesh animation from multi-view silhouettes. ACM Trans. on
Graphics, 27(3):1–9, 2008. 2

[50] T. von Marcard, R. Henschel, M. Black, B. Rosenhahn, and
G. Pons-Moll. Recovering accurate 3d human pose in the
wild using imus and a moving camera. In Proc. of the Euro-
pean Conf. on Computer Vision (ECCV), 2018. 2, 5

[51] L. Wang, X. Zhao, T. Yu, S. Wang, and Y. Liu. Normalgan:
Learning detailed 3d human from a single rgb-d image. In
Proc. of the European Conf. on Computer Vision (ECCV),
2020. 2

[52] C. Wu, K. Varanasi, Y. Liu, H.-P. Seidel, and C. Theobalt.
Shading-based dynamic shape reﬁnement from multi-view
video under general illumination. In Proc. of the IEEE In-
ternational Conf. on Computer Vision (ICCV), pages 1108–
1115, 2011. 2

[53] D. Xiang, F. Prada, C. Wu, and J. K. Hodgins. Monocloth-
cap: Towards temporally coherent clothing capture from
monocular RGB video. arXiv.org, 2009.10711, 2020. 2
[54] W. Xu, A. Chatterjee, M. Zollh¨ofer, H. Rhodin, D. Mehta,
H.-P. Seidel, and C. Theobalt. Monoperfcap: Human per-
formance capture from monocular video. ACM Trans. on
Graphics, 37(2):27:1–27:15, May 2018. 1, 2, 5, 6

[55] J. Yang, J.-S. Franco, F. H´etroy-Wheeler, and S. Wuhrer. Es-
timation of Human Body Shape in Motion with Wide Cloth-
In ECCV 2016 - European Conference on Computer
ing.
Vision 2016, 2016. 2

[56] T. Yu, K. Guo, F. Xu, Y. Dong, Z. Su, J. Zhao, J. Li, Q. Dai,
and Y. Liu. Bodyfusion: Real-time capture of human motion
and surface geometry using a single depth camera. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2017. 2

[57] T. Yu, Z. Zheng, K. Guo, J. Zhao, Q. Dai, H. Li, G. Pons-
Moll, and Y. Liu. Doublefusion: Real-time capture of human
performances with inner body shapes from a single depth
sensor. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2018. 2

[58] T. Yu, Z. Zheng, Y. Zhong, J. Zhao, Q. Dai, G. Pons-Moll,
and Y. Liu. Simulcap : Single-view human performance cap-
ture with cloth simulation. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2019. 2

[59] Z. Zheng, T. Yu, Y. Liu, and Q. Dai. Pamir: Parametric
model-conditioned implicit representation for image-based
human reconstruction. arXiv.org, 2007.03858, 2020. 2
[60] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu. Deephuman: 3d
human reconstruction from a single image. In Proc. of the
IEEE International Conf. on Computer Vision (ICCV), 2019.
2

