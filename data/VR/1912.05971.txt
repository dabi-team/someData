ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

1

Toward Better Understanding of Saliency Prediction
in Augmented 360 Degree Videos

Yucheng Zhu, Xiongkuo Min, DanDan Zhu, Ke Gu, Jiantao Zhou, Guangtao Zhai,
Xiaokang Yang, and Wenjun Zhang

0
2
0
2

l
u
J

0
2

]

V

I
.
s
s
e
e
[

2
v
1
7
9
5
0
.
2
1
9
1
:
v
i
X
r
a

Abstract—Augmented reality (AR) overlays digital content
onto the reality. In AR system, correct and precise estimations of
users visual ﬁxations and head movements can enhance the qual-
ity of experience by allocating more computation resources for
analysing, rendering and 3D registration on the areas of interest.
However, there is inadequate research about understanding the
visual exploration of users when using an AR system or modeling
AR visual attention. To bridge the gap between the saliency
prediction on real-world scene and on scene augmented by virtual
information, we construct the ARVR saliency dataset with 12
diverse videos viewed by 20 people. The virtual reality (VR)
technique is employed to simulate the real-world. Annotations
of object recognition and tracking as augmented contents are
blended into the omnidirectional videos. The saliency annotations
of head and eye movements for both original and augmented
videos are collected and together constitute the ARVR dataset.
We also design a model which is capable of solving the saliency
prediction problem in AR. Local block images are extracted to
simulate the viewport and offset the projection distortion. Con-
spicuous visual cues in local viewports are extracted to constitute
the spatial features. The optical ﬂow information is estimated as
the important temporal feature. We also consider the interplay
between virtual information and reality. The composition of the
augmentation information is distinguished, and the joint effects of
adversarial augmentation and complementary augmentation are
estimated. We generate a graph by taking each block image as one
node. Both the visual saliency mechanism and the characteristics
of viewing behaviors are considered in the computation of edge
weights on the graph which are interpreted as Markov chains.
The fraction of the visual attention that is diverted to each
block image is estimated through equilibrium distribution of this
chain. Extensive experiments are conducted to demonstrate the
effectiveness of the proposed method.

Index Terms—Augmented reality, visual attention, virtual re-

ality, saliency prediction

I. INTRODUCTION

A Ugmented reality (AR) blends the computer-generated

perceptual information into the real world and enhances
users’ visual perception of reality. To provide meaningful
information, AR must understand the context of physical
world [1]. Based on the content analysis and understanding,
AR technology requires accurate 3D registration of virtual

Yucheng Zhu, Guangtao Zhai, Xiongkuo Min, Dandan Zhu, Xiaokang
Yang and Wenjun Zhang are with the Institute of Image Communication and
Network Engineering, and Artiﬁcial Intelligence Institute, Shanghai Jiao Tong
University, Shanghai, 200240, China (e-mail:zyc420@sjtu.edu.cn).

Ke Gu is with the Beijing Key Laboratory of Computational Intelligence
and Intelligent System, Faculty of Information Technology, Beijing University
of Technology, Beijing 100124, China.

Jiantao Zhou is with the State Key Laboratory of Internet of Things for
Smart City, and Department of Computer and Information Science, University
of Macau, Macau, 999078, China.

Fig. 1: The augmentation can be complementary or adversar-
ial. In many cases, the augmentation is a combination of the
complementary and adversarial information. The two sample
images are selected from [3].

and real objects [2]. With the help of fast-growing computer
vision technologies and advanced display technologies, AR is
entering more and more application ﬁelds [3], [4], e.g., by
combining AR with real-time object recognition and tracking
technology, users are able to have a fast understanding of
surroundings. One intuitive example is that when people are
trying to ﬁnd one target in a crowded room, a bit more time
might be wasted if they are not familiar with the surroundings.
But if an AR system can help to analyse the scene and provide
the auxiliary information, the user will quickly ﬁgure out the
situation.

The work in [5] has researched the perceptual issues in
AR. Some panoramic images have the resolution of 8K,
and such a large image will bring about difﬁculties for the
storage and transmission. Besides,
there are two separate
views for left and right eyes, which can be used to bring
stereoscopic perception, but will increase the data size at the
same time. In the past several years, some saliency prediction
models have been proposed [6]. Visual attention has its wide
applications in video compression and quality assessment [7]–
[10]. Therefore, it is meaningful to develop saliency prediction
techniques to allocate processing resources more reasonably in
AR system. In bottom-up saliency prediction algorithms, low-
level features are derived from the sensory information using
the feed-forward model structure. In top-down models, high-
level features make more explorations of how the attention
is inﬂuenced by contextual cues. And in the framework for
video saliency prediction, both spatial features as well as
temporal features are important. However,
the problem of
saliency prediction on immersive medias is still at its infancy.
Some models employ the handcrafted features and follow the
strategy of heuristics [11]–[16]. Others seek to the advent of
deep learning technique [17], [18]. However, most existing
methods for immersive images/videos suffer the problem that
the prediction accuracy is unsatisfactory, and inadequate in-

ComplementaryAdversarial 
 
 
 
 
 
ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

2

vestigations are made about the interconnection between the
instantaneous viewing behavior and long-term saliency results.
Besides, for AR videos, the virtual information is overlaid on
the reality. It is straightforward that the mixture of virtual
contents will alter the distribution of visual attention. The
relationship between the augmentations and the real-world
perception should be considered in the prediction of visual
saliency. However, existing saliency prediction models do not
distinguish the relationship.

In AR experience, we can ﬁnd that the focus of users
information can be
for the real-world and for the virtual
complementary or adversarial as shown in Fig. 1. The comple-
mentary augmentation usually has a strong correlation with the
surroundings. In Fig. 1 (left), the augmentation provides much
the characteristics of the surroundings.
information about
However, when the augmentation is the adversarial type, the
augmentation is usually more independent from the surround-
ings. In this situation, the augmentation is usually conspicuous
as shown in Fig. 1 (right). In many cases, the augmentation is
a combination of the complementary and adversarial informa-
tion. Complementary relationship is required in some speciﬁc
scenes (e.g., driving), because in such circumstances our focus
for the real environment is important. For some other applica-
tions like virtual advertisements, the augmented contents might
be designed to dominate the visual attention. However, existing
saliency prediction models do not make a distinction between
the two different situations, which will cause the inaccurate
prediction results. In the implementation of AR, users can view
the real world by the means of video see-through or optical
see-through [3]. For the video see-through strategy, the virtual
information is overlaid on the video of the real world. And
for the optical see-through strategy, the optical elements is
employed through which the real world is viewed. Because VR
can provide an immersive media experience in which viewers
can get a sense of presence, we turn to employ virtual reality
(VR) to simulate video see-through AR. We can simulate the
see-through videos by overlaying computer-generated digital
contents onto immersive videos which are regarded as real-
world scenes.

For the problem of saliency prediction for the augmented
omnidirectional videos, there are some concerns should be
addressed. In many cases, the augmentation is a combination
of the complementary and adversarial information. Adversarial
augmentation and complementary augmentation have different
inﬂuences on the visual saliency. We should investigate the
joint effects of the complementary and adversarial information
on the visual saliency. Another problem is that during the
capturing, the all 360 degrees of the scene will be recorded.
The spherical ﬁeld of view is usually mapped to a ﬂat image
through different projection methods, e.g., equirectangular pro-
jection and cube mapping. If the 360 degree image is viewed
in 2D, there will be geometric distortions that are caused by
the projection. Therefore, we should mitigate the projection
distortions to ensure the accuracy of 2D signal processing.
Besides, when watching immersive videos, users can have very
different experience to perceive the immersive content in the
ﬁrst person perspective in the head mounted display (HMD).
Due to the tremendous extent of the observable content and the

limited extent of ﬁeld of view (FoV), the viewing behaviors
including head movements (HM) and eye movements (EM)
are commonplaces in immersive experience. Therefore, it is
plausible to investigate the impact of instantaneous viewing
behaviors on the saliency results.

The amount of conveyed information of the augmenta-
tion can be used to measure the level of inﬂuence on the
visual saliency. For the complementary augmentation,
the
augmentation can enhance the associated surrounding areas.
On the contrary, the adversarial augmentation attenuates the
surroundings. So the conveyed information of the two types
of augmentations play different roles according to the augmen-
tation type. In other words, the level of inﬂuence is determined
by the amount of conveyed information, and the type of
augmentations control the choice of attenuation or enhance-
ment. In many cases, the augmentation is a combination of
the complementary and adversarial
information. Therefore,
a soft-decision strategy is a better choice to distinguish the
composition of the augmentation. The motion difference be-
tween the augmented area and the surrounding area can reﬂect
the degree of coherence, which can be used to estimate the
probabilities of augmentation types. Based on the estimated
probabilities, the expectation of inﬂuence can be calculated
as the joint effects of the complementary and adversarial
information on the visual saliency. In the feature extraction, we
can divide the panorama into blocks to simulate the viewport
and offset the projection distortion. Considering the impact
of instantaneous viewing behaviors on the saliency results,
the graph can be generated with each block as one node.
Then we can formulate the accompanying adjacency matrix,
of which each element indicates the relationship of adjacency
from one node to the other. Weights for different edges can
be determined by incorporating the visual saliency mechanism
and characteristics of instantaneous viewing behaviors. The
framework of utilizing graph to tackle the problem of saliency
prediction is reasonable [19], because both the features which
can be low- or high-level reﬂecting the mechanism of human
visual system (HVS) and the characteristics of human viewing
behaviors can be incorporated in the transition matrix.

Hence, we propose a detailed methodology, named VBAS
(Visual Behavior Adaptive Saliency) Model,
to tackle the
problem of saliency prediction on the augmented omnidi-
rectional videos. Our method can adapt to different visual
behaviors when viewers are watching augmented immersive
videos. Speciﬁcally, local block images are extracted to sim-
ulate viewport and reduce the impact of distortions that are
caused by projections. Both spatial and temporal visual cues
are extracted. In the extraction of spatial features, we take
the advantage from the adequate saliency annotations for
traditional 2D images. A convolutional neural network (CNN)
based model is designed and trained on the Large-scale scene
understanding (LSUN) database [22] to extract conspicuous
cues on local viewports. The optical ﬂow is estimated as the
temporal feature. The FlowNet [23] is employed to extract
motion features. The interplay between virtual information
and reality is also considered. The joint effects of the comple-
mentary and adversarial information on the visual saliency are
estimated, based on which the spatial weighting is conducted.

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

3

Fig. 2: Framework of the proposed VBAS model. In this framework, input and output are marked with red, modules that
need calculations are marked with green, results that contain the information for all block images that are extracted from the
equirectangular image are marked with black, results for augmented blocks are marked with blue, and results for non-augmented
blocks are marked with gray. Bottom left script “B” indicates that the results are in block domain. Local block images are
extracted to simulate viewport and reduce the impact of distortions that are caused by projections. The center of viewports are
designed to evenly distribute on the sphere [20], [21]. We distinguish the augmented block images and non-augmented block
images. On the augmented block images, the interplay between virtual information and reality is considered. The joint effects
of the complementary and adversarial information on the visual saliency are estimated, based on which the spatial weighting
is conducted. In the feature extractions, a CNN based model is designed and trained on the Large-scale scene understanding
(LSUN) database [22] to extract conspicuous cues on local viewports. The optical ﬂow is calculated as the temporal feature.
The FlowNet [23] is employed to extract motion features. To ensure our model concentrate on signiﬁcant block images, we
generate the graph by taking each block image as one node. We combine the extracted visual cues with the characteristics
of viewing behaviors to deﬁne edge weights on graphs which are interpreted as Markov chains. And different strategies are
employed to fuse the spatial and temporal features.

To ensure our model concentrate on signiﬁcant block images,
we generate the graph by taking each block image as one node.
We combine the extracted visual cues with the characteristics
of viewing behaviors to deﬁne edge weights on graphs which
are interpreted as Markov chains. And different strategies
are employed to fuse the spatial and temporal saliency. Fig.
2 shows the overall framework of our method. To validate
the effectiveness of the proposed method, we construct an
ARVR saliency dataset. Considering the sense of presence in
VR, we employ VR to simulate real-world environments. We
implement the blending of virtual contents by using the named
bounding box to label and follow the salient objects. We
display the augmented videos in a VR HMD with an embedded
eye tracker to record the head and eye movements when users
are experiencing the simulated AR. Also, the original videos
are also evaluated in the VR HMD. We generate saliency maps
from head and eye movements and compare our model with
some state-of-the-art models on the ARVR dataset. Our model
provides the most promising results compared with others.

The rest of this paper is organized as follows. In Section II,
we summarize the main contributions of this work. In Section
III, we describe the implementation details of our model to
predict AR saliency. Section IV describes the construction
of our ARVR dataset and the experimental evaluation results

and comparisons. Finally, concluding remarks are described in
Section V.

II. OUR WORKS

In this paper, we make several contributions. First, we
address the differences between adversarial augmentations
and complementary augmentations. For the complementary
augmentation, the augmentation can enhance the associated
surrounding areas. On the contrary, the adversarial augmenta-
tion addresses itself and attenuates the surroundings. Second,
we design the model which is capable of solving the saliency
prediction problem in augmented 360 degree videos. Local
block images are extracted to simulate the viewport and offset
the projection distortion. Conspicuous visual cues in local
viewports are extracted to constitute the spatial features. And
the optical ﬂow information is estimated as the important
temporal feature. We consider the interplay between virtual
information and reality. The composition of the augmentation
information is distinguished, and the joint effects of adversarial
augmentation and complementary augmentation are estimated.
Third, we generate the graph by taking each block image
as one node. Both the visual saliency mechanism and the
characteristics of viewing behaviors are considered in the
computation of edge weights on graphs which are interpreted

AugmentedBlock Set AExtractedBlock Set E Non-AugmentedBlock Set (E-A)Augmented Area VDistinguishing and DivisionSegmentationAugmentated Area BNon- Augmentated AreaBSpatial&TemporalFeatures FusionFeature ExtractionSpatial WeightingEach Block in Set ASpatial&Temporal FeaturesSpatial&Temporal FeaturesBBEach Block in Set AEach Block in Set (E-A)Feature Maps for Each Block inSet EBGraph Based Weighting and Spatial RearrangementEquirectangular Domain to Block DomainBlock Domain to Equirectangular DomainOutput Saliency Map for Frame iInput Frame iZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

4

as Markov chains. The fraction of the visual attention that is
diverted to each block image is estimated through equilibrium
distribution of this chain.

Besides, we construct the ARVR dataset. The dataset is
comprised of 12 original panoramic videos and 12 simulated
video see-through AR sequences. We simulate video see-
through AR sequences by adding the labelled bounding box
on the moving objects. We use the “Adobe Premiere Pro”
software and spherically aware immersive video effects that
can compensate for distortion in equirectangular content. The
insertion of labelled bounding box is conducted manually on
the sphere. During the collection of head and eye movements
data, HMD equipped with eye tracker is used. Each of the
video is saliency-annotated by 20 subjects. On the constructed
dataset, we conduct extensive experiments and demonstrate the
effectiveness of the proposed method.

III. VISUAL BEHAVIOR ADAPTIVE MODEL
ARCHITECTURE

To predict visual saliency in AR, we developed the Visual
Behavior Adaptive Saliency (VBAS) prediction model based
on the following considerations. Fig. 2 shows the overall
framework of the proposed VBAS model.

1) Spatial Local Saliency. Image blocks are extracted to
offset the projection distortions. CNN-based model is
designed to predict the spatial saliency.

2) Temporal Motion. Considering that motion information
is a crucial cue for visual saliency, we extract motion
features as temporal saliency.

3) Impact of Augmentation. The impact of augmenta-
tion information is considered. The joint effects of the
complementary and adversarial information on the visual
saliency are estimated, based on which the spatial weight-
ing is conducted.

4) Viewport Saliency Fusion. To highlight signiﬁcant block
maps, we design the weighting strategy by forming
graphs based on block images and computing the equi-
librium distribution. We consider the visual saliency
mechanism and the characteristics of viewing behaviors
in the calculation of edge weights on graphs which are
interpreted as Markov chains.

A. Spatial Features Extraction

The local saliency prediction aims to offset the pixel dis-
tortion in equirectangular projection and highlight the area
of interests locally during the watching of panoramic videos.
We use the perspective projection method to map the sphere
to multiple block images. Speciﬁcally, the block image is
generated by projecting the scene within a 60 degree view
frustum from a given perspective centre and target position
to the block image. The 60 degree view frustum is able to
cover the center vision and near peripheral vision that are most
acute vision areas in eyes [24]. The sphere centre is set as the
centre of perspective. To ensure an even distribution of target
position on the sphere, we should generate the points on the
sphere and make the number of points in an area proportional

TABLE I: THE NETWORK ARCHITECTURE

Layer Name

Output Size

Kernel, Depth, Stride

Conv1

112 × 112

7×7, 64, stride 2















Conv2 x

56 × 56

Conv3 x

28 × 28

Conv4 x

14 × 14

Conv5 x

7 × 7

DConv6

DConv7

DConv8

DConv9

Conv10

14 × 14

28 × 28

56 × 56

112 × 112

112 × 112

3 × 3 max pool, stride 2


1 × 1,
3 × 3,
1 × 1,

64
64
256


 × 3

1 × 1,
3 × 3,
1 × 1,

1 × 1,
3 × 3,
1 × 1,

1 × 1,
3 × 3,
1 × 1,


 × 4


 × 6


 × 3

128
128
512

256
256
1024

512
512
2048

5 × 5, 512, stride 2

5 × 5, 128, stride 2

5 × 5, 32, stride 2

5 × 5, 8, stride 2

1 × 1, 1

to the area. Some study [20] showed the way to achieve the
even distribution on sphere, which is also employed in the
toolbox to measure the performance of saliency prediction on
360 degree images [21]

γ = 2π(1 − 1/GR),
θi = iγ,
ϕi = arccos(1 − 2i/N ),
Q = {(ϕi, θi), i = 1...N },

(1)

where N is the total number of target points, GR = 1+
, ϕi
2
and θi are latitude and longitude respectively, Q is the set of
evenly distributed target points. We can extract block images
on the panorama from different target positions

5

√

E = {E|E = Extract(Ipano, ϕ, θ), (ϕ, θ) ∈ Q},

(2)

where Ipano is the panorama. E is the set containing extracted
block images.

We design the CNN-based model to predict the spatial local
saliency. The saliency prediction model takes the block images
(224×224×3) as input and outputs the predicted saliency
maps. The backbone of the saliency prediction model is the
Resnet50 [25] network, pre-trained on ImageNet [26]. We
remove the average-pooling layer and fully-connected layer
in Resnet. To upscale the feature map to the dimension of
112×112, four transpose convolution layers are employed to
conduct the deconvolution. And the ﬁnal convolution layer is
employed to produce the saliency map. Table I shows more
details.

B. Temporal Features Extraction

Motion information is a crucial cue for visual saliency,
so motion features are extracted as temporal saliency. In
the literature, the FlowNet [23] has the outstanding motion

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

5

describing ability. Considering that visual attention is often
attracted by motion, we take the extracted motion as temporal
saliency. FlowNet is employed to detect motion on each block
image in E between frames. To reduce the impact of noise, in
our processing, we ﬁlter the magnitude of FlowNet’s results
with Gaussian.

To integrate the spatial and temporal saliency maps, we can

employ the following feature fusion methods [27].

• Product after normalization:
(cid:32)

f1(Ss, St) = N

(cid:89)

(cid:33)

N (Si)

,

• Max after normalization:

i

f2(Ss, St) = N

(cid:16)

max
i

(cid:17)

N (Si)

,

• Summation after normalization:

f3(Ss, St) = N

(cid:33)

N (Si)

,

(cid:32)

(cid:88)

i

(3)

(4)

(5)

where in Eqs. (3), (4), (5), i ∈ {s, t} which represents spatial
and temporal domain respectively, and N is a normalization
operator which normalizes the saliency map S linearly to the
dynamic range [0, 1]. A pixel-wise manner is adopted in the
summation, product and max operations in Eqs. (3), (4), (5).

C. Spatial Weighting on Augmented Blocks

The complementary augmentation bears a strong correlation
with the surroundings. So, complementary augmentation can
highlight its associated target. However, when the augmenta-
tion is adversarial, the virtual object is usually independent
from the surroundings. So, the adversarial augmentation is
usually conspicuous. In many cases, the augmentation is a
combination of the complementary and adversarial informa-
tion. Therefore, it is plausible to calculate the amount of
complementary and adversarial information in the augmenta-
tions and approximate their joint effects on the visual saliency
distribution. The amount of conveyed information of the
augmentation can be used to measure the level of inﬂuence
on the visual saliency. For the complementary augmentation,
the augmentation can enhance the associated surrounding
areas. On the contrary, the adversarial augmentation attenuates
the surroundings. So the conveyed information of the two
types of augmentations play different roles according to the
augmentation type. In other words, the level of inﬂuence is
determined by the amount of conveyed information, and the
type of augmentations control the choice of attenuation or
enhancement. Because in many cases, the augmentation is a
combination of the complementary and adversarial informa-
tion, the soft-decision strategy can perform better. So we can
estimate the probabilities for the two types of augmentations.
The motion difference between the augmented area and the
surrounding area can reﬂect the degree of coherence, which
can be used to estimate the probabilities of augmentation
types.

area and satisfy that the overlap ratio is greater than 0.3. The
suitable block images constitute the set A

A = {A|A ∈ E, A∩V

A > 0.3},

(6)

where V represents the occlusion area where laid on the
augmentation information, and A consists of the augmented
block images that are extracted on the panorama from different
target positions in set E, and satisfy that the overlap ratio is
greater than 0.3. Each augmented block image can be further
segmented on the pixel level

Aa = A ∩ V,
Ae = A − V,

(7)

where Aa contains the pixels that belong to augmentations, Ae
contains the pixels that belong to the reality environment. We
calculate the entropy on the Aa to estimate the information
that is conveyed by the augmented area

H(Aa) =

(cid:88)

m

−pm log2(pm)

(8)

where M is the number of gray levels and pm is the probability
associated with gray scale m. We enhance the Aa when it
is adversarial augmentation. Otherwise, we enhance the Ae
when it is complementary type. There are two augmentation
types that are complementary (c1) and adversarial (c2). So,
the augmentation type (r) has the sample space {c1, c2}.
In the estimation of the joint effects of the two types of
augmentations, the soft-decision strategy is designed by

(cid:40)

wa = Er∼p(r)(2 · Sigmoid(K(r) · H(Aa))),
we = 2 − wa,

(9)

where wa is the weight for Aa, we is the weight for Ae,
r ∈ {c1, c2}, Er∼p(r)
is the expectation of the sigmoid
function based on the probability p(r). In Eq. (9), the K(r)
controls the choice of attenuation or enhancement according
to the augmentation type

K(r) =

(cid:26) −1,
1,

if
if

r = c1
r = c2

(10)

K(r) is designed that when the augmentation is complemen-
tary, the augmented information will divert visual attention
to the surrounding associated areas, otherwise, the adversarial
augmentation itself will attract visual attention. With the cal-
culated spatial weights, we can conduct the spatial weighting
by

f4(S(x, y)) =

(cid:26) wa · S(x, y),
we · S(x, y),

if (x, y) ∈ Aa
if (x, y) ∈ Ae

(11)

where S(x, y) is the feature value at pixel (x, y).

To determine the probability p(r) in Eq. (9), we assume
that the motion difference between the augmented area and
the surrounding area can reﬂect the degree of coherence. We
calculate the mean of pixel ﬂows on the given area

fi

t =

1
Mi

(cid:88)

fx,y,t

(x,y)∈Ai

(12)

Speciﬁcally, among these block images, we search the
“augmented block images” that overlap with the augmented

where i ∈ {a, e} that is deﬁned in Eq. (7), and fx,y,t =
(u(x, y, t), v(x, y, t)) is the ﬂow of a pixel (x, y) at time t, Mi

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

6

is number of pixels in Ai. Then we calculate the difference of
the mean ﬂow on the Aa and Ae

(cid:52)f t = (cid:107)fa

t − fe

t(cid:107)2.

(13)

The beliefs about the augmentation type before the prediction
is considered as the prior π. The probability of the augmen-
tation type conditioned on the prior knowledge is calculated
as





p(c1|π = c1) = G(tanh((cid:52)f ); µc1, σc1),
p(c2|π = c1) = 1 − p(c1),
p(c2|π = c2) = G(tanh((cid:52)f ); µc2, σc2),
p(c1|π = c2) = 1 − p(c2),

(14)

where G is the Gaussian, π ∈ {c1, c2}. tanh(∗) maps the (cid:52)f
to the range of [0, 1). The parameters are set as µc1 = 0,
σc1 = 0.85, µc2 = 1, σc2 = 0.85. Then we can use Eq. (14)
to calculate probability p(r) and use Eq. (9) to conduct the
spatial weighting.

D. Graph based Weighting and Spatial Rearrangement

After feature extractions in spatial and temporal domain, and
spatial weighting for augmented block images, we get feature
is
maps for each block image. The spatial rearrangement
required to transform the block images to the equirectangular
image. If uniform weights are assigned to blocks, the resulting
combined saliency map will be too uniformly distributed and
uninformative. To solve this problem, considering the visual
saliency mechanism and characteristics of visual behavior, we
design the graph based weighting for the spatial rearrange-
ment. For traditional 2D images, eye movements are nearly
sufﬁcient to view the image. Our eyes have a limited acute
vision angle of about 15 degrees in the foveal area. To see
areas accurately, we can move eyes to direct the foveal vision
to the target. Through eye movements, we can perceive most
targets in front of us. However, to explore the environment of
360 degrees, we need to change the orientation of our head.
So we should consider the characteristics of instantaneous
viewing behavior when viewers are watching 360 degree
videos.

We form the fully-connected directed graph G by connect-
ing every node in the set Q that is deﬁned in Eq. (1) with other
N − 1 nodes. The weights of the outbound edges of each node
can be normalized to 1. Then we can deﬁne a Markov chain
on G by taking the nodes as the state, and edge weights as
transition probabilities. The fraction of the visual attention that
is diverted to each node can be estimated through equilibrium
distribution of this chain. The visual saliency mechanism as
well as the instantaneous visual behavior are considered in the
determination of the transition probability

µ(Sj )

µ(Sk) · F (g(qi, qj))

w(qi, qj) =

sin(ϕj) ·

(cid:80)

sin(ϕj) ·

(cid:80)






qk ∈Qqi
if qj ∈ Qqi,

µ(Si)

qk ∈Qqi
if qj /∈ Qqi

µ(Sk) · F (g(qi, qj))

(15)

where the weight of the edge from start node pi to destination
node pj is proportional to the ﬁrst term sin(ϕj) that is the

equator bias of the node pj. The equator bias is an assumption
that viewers are more likely to hold their heads erectly when
they are using HMDs. Because the upright head position is
the natural position that makes people feel comfortable. Many
elements in life take this critical consideration for ergonomic
design. So, the probability of the situation when our heads are
tilted forwards or backwards is much lower than that when
our heads are in upright position.

In the second term in Eq. (15), we deﬁne the set Qqi that
consists of the nodes in the ﬁeld of view (FoV) centered at
the node qi

Qqi = {q|q ∈ Q, |ϕ − ϕi| < ϕ(cid:52), |θ − θi| < θ∆},

(16)

where q = (ϕ, θ), qi = (ϕi, θi). The ϕ(cid:52) and θ∆ are the
half width and half height of the viewport, and qi locates at
the center of the viewport. In this paper, the data collection
is conducted with the video see-through equipment with FoV
of 110 degrees. Therefore, the ϕ(cid:52) and θ∆ are set as 0.3π
accordingly. If the AR is implemented with optical see-through
equipment, the parameters can be set according to the portion
of space in which objects are visible at the same moment
during steady ﬁxation of gaze in one direction.

In the second term in Eq. (15), there are two cases to
consider. In the ﬁrst case, the destination node is within the
FoV that is centered at the start node qi. Although our eyes
can only see details in the center of gaze clearly and sharply,
the areas outside the central vision can also receive the light
[28]. Therefore, eyes can perceive the content in the area that
is located at qj when the ﬁxation of gaze is in qi, and more
details of the area in qj will be distinguished when the gaze
is shifted from qi to qj. To estimate the ability of qj to attract
attention from the whole visible FoV, we calculate the ratio
of the local saliency to the accumulated saliency in the FoV.
We set the numerator as µ(S) to calculate the pixels’ mean
value on the block feature map S, and set the denominator
to calculate the accumulated feature value in the FoV. In the
second case, the destination node is outside the FoV that is
centered at the start node qi, which means that there is no
perception of the area in qj when the ﬁxation of gaze is in
qi. In this case, the visual system rejects the candidates in the
FoV and selects qj that is outside the FoV. Since the content
in qj is invisible, we estimate the probability that reject the
µ(Sk) , where Si is the block feature
candidates by
map that is centered at qi.

qk ∈Qqi

µ(Si)

(cid:80)

In the third term in Eq. (15), we estimate the inﬂuence
of the gaze shift distance. According to statistics, when the
size of destination area remains unchanged, the probability
that the ﬁxation locates within the destination area is inverse
proportional to the active area. Therefore, the probability that
the gaze is directed within the destination area is inverse
proportional to the distance of the gaze shift. So, F (·) is
chosen to impose a penalty on the distance. Since image pixels
are located on sphere, the distance between two points is
measured along the surface of the sphere. So the geodesic dis-
tance g(qi, qj) is calculated. As suggested by [19], a Gaussian
function is utilized to generate the weight for distance. We set
F (x) = exp( −x2
2σ2 ), and set the parameter σ to approximately

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

7

0.25π (45◦) which is derived from half the maximum head
turning angle (60◦) plus the best eye rotation angle (15◦) [24].
The transition matrix W can be formed by determining
all the weights of directed edges according to Eq. (15). We
can ﬁnd that all components in the transition matrix are
strictly positive. The principal eigenvector of the matrix can be
computed as the equilibrium distribution. According to Perron-
Frobenius theorem [29], a real square matrix with positive
entries has a unique largest real eigenvalue, and there is the
associated principal eigenvector, all of whose elements are
positive. Therefore, we can calculate the principal eigenvector
α of the transition matrix W. Each element of α is the positive
weight for the corresponding block map. Then the spatial
rearrangement can be conducted to transform the block feature
maps to euqirectangular format

f5(α, S) =

(cid:88)

Ei∈E

R(αi · SEi),

(17)

Fig. 3: Some example frames that are extracted from the
augmented omnidirectional videos (videos annotated with
bounding box) in our dataset.

where R consists of two steps that are the inverse operation
of the ‘Extract’ in Eq. (2) followed by a Gaussian ﬁltering to
offset the discontinuity in the equirectangular domain. E is the
set containing extracted block images. SEi is the block feature
map for the ith block image.

IV. EXPERIMENTS AND RESULTS

A. Subjective Experiment

1) Video Stimuli: Considering the sense of presence in
VR, we employ VR to simulate real-world environments. In
total, 12 different videos were used in the experiment. The
used videos were chosen among omnidirectional videos from
YouTube. Table II provides the YouTube ID to the original
omnidirectional videos hosted in YouTube. Besides, the dura-
tion, frame rate and resolution of each video are also provided.
The selection of omnidirectional videos was carried out with
the objective of covering various characteristics. Speciﬁcally,
amount of foreground objects, shooting environments, amount
of motion information were considered. The video selection
resulted in a minimum of 5 videos within each one of the
following groups: Indoor, Outdoor, People, Rich motion, Rare
motion. Example frame images of the videos in our dataset are
shown in Fig. 3. And all of the videos are in equirectangular
format.

We implement the blending of virtual contents by using
the named bounding box to label and follow the conspicuous
foreground objects. The bounding box is designed to follow
the objects to simulate the 3D registration in AR. To ensure
the quality, we use software called the “Adobe Premiere Pro”
and conduct the annotations manually on the sphere frame by
frame. The appearance of the bounding box is shown in Fig.
3.

2) Subjective Experiment Execution: The ‘HTC VIVE PRO
EYE’ was employed in our experiments [31]. This type of
HMD can track eye movements precisely and has a wide ﬁeld
of view of 110 degrees, a refresh rate of 90Hz and a high
resolution of 1440 × 1600 per eye. Besides, the headphones of
the HMD can be used to play the audio. The data collection
software was developed based on the platform named “Unity”,

TABLE II: THE DERIVATION OF THE VIDEOS AND THE
INFORMATION OF EACH VIDEO CLIP.

Stimulus name
Bar counter
Auto show
Underwater
Street dance
Lawn
Airport
Car race
Spacesuit Training
Grassland
Bicycle train
Ofﬁce
Time square

YouTube ID
HSSveMXedU4
kbkSLdB5MWI
TkmYAjh7oQ0
xZA9fw1ja1s
j3N4WI4wgyc
HyoI VCalN8
Q2BtoxRtpjw
crt1XNocRKc
WYnQP8mZN9k
JT5GRWQMpy4
WHSFeVbNzuc
6Yc-SXEFveo

Duration(s)
38
30
18
30
24
30
30
30
30
20
30
30

Frame rate
24
24
24
24
30
30
30
30
30
30
30
30

Resolution
4096×2048
4096×2048
4096×2048
4096×2048
4096×2048
4096×2048
4096×2048
4096×2048
3840×1920
4096×2048
3840×1920
3840×1920

running on the high-performance computer equipped with the
“1080Ti” graphics card.

Twenty subjects participated in the subjective experiment.
There were 2 females and 18 males, whose ages ranged
between 20 and 30. All of them reported normal or corrected-
to-normal vision. And the majority of the participants were
familiar with the VR equipment and had the VR experience
more than 5 times. During the experiment, for the convenience
of exploration, viewers were seated on a swivel chair. The
swivel chair was ﬁxed in the ground and all subjects were
only allowed to move eyes and head and rotate the swivel
chair. Carefully calibration of the eye tracking module was
conducted for each subject before the recording of head and
eye movements. The training and explanation of the procedure
were also provided prior the test. The omnidirectional videos
were displayed in a sequence with ﬁxed order. 5 seconds of
black screen was inserted between each two videos as a rest
to make viewers be ready for the next video. All subjects
were asked to look around in this step to get natural-viewing
visual attention data. The duration of the experiment for each
participant lasted less than 20 minutes, avoiding the fatigue
of observers. We collect the HM and EM data on videos
with annotations as well as videos without annotations. The
data collection on the two video sets are undertook separately.
We separate the data recording for AR set and VR set with
an interval of 20 days to eliminate the inﬂuence of twice
experience.

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

8

Fig. 4: Comparisons between the saliency maps. In each row, from top to down are the original and the augmented example
frames to describe the video, and the packed saliency map for the video; the local image patch from the augmented frames,
the corresponding ground-truth saliency map, and the local saliency map generated by SalGAN [30]; the local image patch
from original frames, the corresponding ground-truth saliency map, and the local saliency map generated by SalGAN [30].

Fig. 5: The velocity with different directions and magnitudes are calculated and represented by the heat map. Different colors
indicate the number of end points of the velocity vector from origin in different areas. The magnitude is limited within the
square with width 20, of which the unit is rad/s. The origin is set at the center of the map, and the direction is set to match
the movement of human eyes.

3) Gaze Data Processing: The raw data from the head
gyroscope sensor includes the 3 Euclidean rotation angles:
pitch, yaw and roll. The data of the gaze point is captured
and represented in the 3D Cartesian coordinate system. For
the convenience of calculating on the sphere as well as on
the 2D plane, we convert the captured data to the geographic
coordinate system. All data is projected in latitude/longitude.
Saccade is the very fast eye movement between two ﬁx-
ations. The sampling rate of eye tracker is 120Hz, so many
recordings during the saccade is involved. During the saccade,
brain barely process the visual input. Therefore, saccade does
not reﬂect the attention of viewers and it is better to ﬁlter
out the eye movement data during the saccade [32]. However,
there also exists the smooth eye pursuit in the video watching,
which is made when the eye is following a moving object and

is much similar with saccade but is more informative [33]. So
we should preserve the data of the smooth eye pursuit.

To offset the stretching distortion near the poles, for each
viewer, we calculate the geodesic distance between adjacent
two recorded ﬁxations. The ﬁltering of the data between the
saccade is conducted as

X’ = {x|µ(X) − 3σ(X) ≤ x ≤ µ(X) + 3σ(X), x ∈ X}, (18)

where X is the set of geodesic distances of one subject during
the interval of two frames. We calculate the mean and standard
deviation of the geodesic distances in X. We remove the eye
movements that are out of the range and form X’ as the gaze
data of each viewer on each frame. The same operations are
conducted to ﬁlter the head movements data.

(a) Car race (original)(b) Car race (augmented)(c) Bicycle (original)Log scale(d) Bicycle (augmented)ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

9

After the determination of ﬁxations, we can generate the
ﬁxation map through aggregating all subjects’ ﬁxation points
at a given time stamp. By aggregating all subjects’ data of
eye movements, the ﬁxation map for each frame is generated.
It is crucial to determine continuous region of interest. Ap-
plying a Gaussian ﬁlter on the ﬁxation map can tackle this
problem [34]. The saliency level of an area is determined
based on the density of ﬁxations. Besides, Gaussian ﬁltering
can reduce the impact of noise, calibration error, and fall-off
in visual acuity outside the foveal region [35]. Because the
employed equirectangular projection has stretching distortions
near poles, the Gaussian ﬁltering is conducted on the sphere
to offset the projection distortion of equirectangular map.

4) Dataset Analytics: Our dataset consists of the head and
eye movements data, based on which we generate saliency
maps for both head and eye movements on each frame image
of the stimulus. It is worth noting that for the adversarial
augmentations, the augmentations are designed to dominate
users’ visual attention. On the contrary, the complementary
augmentations are designed for assistance. With the estab-
lished dataset, we can investigate how the augmentations inﬂu-
ence the viewing behavior and analyse the differences between
visual attention distributions when viewers are experiencing
the AR or VR techniques.

To analyse the difference in the distribution of visual atten-
tion between AR and VR, we choose two videos, wherein the
“car race” is outdoor scene and contains concentrated motion
and salient objects, the “bicycle training” contains multiple
moving salient objects. In Fig. 4, the “packed” saliency map
is calculated as Eq. (19) to estimate the overall distribution of
visual attention

Spacked = N

(cid:32)

(cid:88)

(cid:33)

Si

,

i

(19)

where we sum and normalize saliency maps for all frames
in one video. In Eq. (19), N is a normalization operator
which normalizes the saliency map linearly to the dynamic
range [0, 1]. Besides, we extract several frames from the two
videos and extract the local image patch in the frame for
better visualization. The corresponding saliency maps for the
local images patch are also extracted. From the results in
local image patch, we can see that ﬁxations are located on
moving objects. Careful calibrations ensure that the head-eye
movements are well captured by the eye tracker and motion
sensors in the HMD. To analyse how the virtual information
change the distribution of visual attention, Fig. 4 also presents
the comparisons of saliency map between the original video
and the augmented video. From the packed saliency map,
we can observe the similarity between the distribution of
visual attention for the original video and the video with
augmentation information. From the local image patch and
the corresponding saliency map, we can observe that
the
augmentation can attract the visual attention to the object
itself.

We also calculate the velocity of head movements. Fig. 5
shows the distribution of velocity of different directions and
magnitudes, from which we can see that the eye movements
for augmented videos are more concentrated and regular, and

TABLE III: PERFORMANCE COMPARISON WITH STATE-OF-
THE-ART SALIENCY MODELS ON SUBSET I.

Category & Method

Non-deep
Static

Deep
Static

Non-deep
Dynamic
Deep
Dynamic

Panoramic
Static

Proposed

BMS [36]
GBVS [19]
Itti [37]
Mlnet [38]
Salgan [30]
Salicon [39]
PQFT [40]
SeR [41]
SalEMA [42]
TASED [43]
SJTU H [16]
SJTU HE [16]
SalNet360 [44]
IIP360 [45]
VBASf1 ,f4,f5
VBASf2 ,f4,f5
VBASf3 ,f4,f5
VBASf3,f4
VBASf3,f5

Performance

AUC-Judd ↑
0.697
0.667
0.680
0.737
0.736
0.738
0.684
0.676
0.728
0.743
0.759
0.759
0.733
0.762
0.801
0.773
0.834
0.739
0.793

NSS ↑
1.608
1.240
1.358
2.046
1.718
1.775
1.756
1.784
1.812
2.063
1.559
1.578
1.310
1.984
1.925
1.729
2.387
1.924
2.119

KL ↓
8.437
9.361
9.024
7.460
7.695
8.171
8.403
8.363
6.222
5.475
8.324
8.304
9.566
8.171
5.082
5.690
4.870
5.330
5.113

CC ↑
0.393
0.321
0.348
0.468
0.421
0.386
0.303
0.265
0.429
0.481
0.420
0.423
0.341
0.437
0.553
0.452
0.549
0.439
0.521

are more coherent with the movements of objects. So, the
augmented virtual information will change the movements
of eyes, and thus change the distribution of visual saliency
distribution. However, the relationship between the augmen-
tations and the real-world perception is not distinguished in
existing saliency prediction models. To address the problem,
we perform a large-scale model comparison in the experiments
and present some results in advance here to show that the
precarious prediction will occur if the augmented information
is not distinguished. From the results in Fig. 4 we can observe
that the prediction results from SalGAN [30] for the “bicycle
training” is consistent with the ground truth. We think the
complex background has the luminance and contrast masking
effects. However, the prediction results for the “car race” is
more sensitive to the augmented contents. More predicted
visual attentions are concentrated on the label, but the ground
truth shows that the bounding box and label have limited
inﬂuence on the visual attention distribution.

B. Experimental Setup

Test Datasets. There are some panoramic video eye-
tracking datasets in the literature, but they only include the
recording of head and eye movements for panoramic videos.
To investigate the visual behavior of viewers when they are
experiencing AR, we construct
the ARVR dataset. In the
ARVR dataset, 12 panoramic videos and 12 annotated AR
videos are included. Head movements and eye movements are
recorded for both panoramic videos and AR videos. We refer
to the videos with virtual augmentations as Subset I, and the
original videos as Subset II. We mainly use Subset I as the
testbed. Subset II is used as a complementary to measure the
performance of our model in terms of saliency prediction for
panoramic videos.

Competing Models. Some visual saliency models are com-
pared with the proposed VBAS model. Although some of
these saliency models are designed for traditional 2D images
and videos,
the state-of-the-art of
ﬁxation prediction. Speciﬁcally, a total of 14 visual saliency
models are selected as competitors: BMS [36], GBVS [19], Itti
[37], Mlnet [38], Salgan [30], Salicon [39], PQFT [40], SeR

they can still represent

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

10

Fig. 6: Examples of saliency maps. Six example frames are from: ‘Bar counter’, ‘Car race’, ‘Auto show’, ‘Street dance’,
‘Grassland’, and ‘Ofﬁce’. The ground-truth saliency map, and the saliency predictions from our model, TASED, GBVS,
Salgan, IIP360, Salicon, SalEMA, Mlnet, and BMS are shown.

[41], SalEMA [42], TASED [43], SJTU H [16], SJTU HE
[16], SalNet360 [44] and IIP360 [45]. BMS, GBVS and Itti
are traditional non-deep learning based saliency models for
static scenes. Mlnet, Salgan and Salicon are deep learning
based saliency models for static scenes. PQFT and SeR are
traditional non-deep learning based video saliency models
for dynamic scenes. SalEMA and TASED are deep learning
based video saliency models for dynamic scenes. SJTU H,
SJTU HE, SalNet360 and IIP360 are saliency models for
panoramas.

Evaluation Criteria. Several criteria are available for
comparing the saliency models. We employ the toolbox to
measure the performance of saliency prediction on 360 degree
contents [21]. We use 4 saliency model evaluation metrics,
wherein AUC-Judd [46] calculates the area under the receiver
operating characteristic (ROC) curve. To create the curve, the
threshold value is swept in the range of saliency values at ﬁxa-
tion locations. A value of 1 indicates a perfect classiﬁcation for
AUC-Judd. Normalized Scanpath Saliency (NNS) [47] metric
evaluates the prediction accuracy of models by calculating
the mean value of the predicted saliency at the ground-truth
ﬁxations. A higher NSS value indicates a better prediction
result. Kullback-Leibler (KL) [48] divergence measures the
distance between the distribution of predicted saliency map
and the distribution of ground-truth map. If the prediction
exactly matches with the ground truth, the KL value equals
zero. The KL value varies from zero to inﬁnity. Correlation

coefﬁcient (CC) [49] evaluates the linear correlation between
the prediction and the ground truth. And a value of 1 indicates
a perfect correlation.

Implementation Details. The proposed method involves the
spatial and temporal saliency estimation. The designed CNN-
based network is employed to predict spatial saliency, which
is trained on the LSUN dataset [22]. The FlowNet [23] is
employed to predict the motion information as the temporal
saliency. All videos are analyzed at a reduced frame rate. We
sample every ﬁve frames and measure the consistency between
predictions on the frames and the ground truth. The mean
performance of the frames are reported.

C. Performance Evaluations and Comparisons

We ﬁrst conduct the ablation study and test all the saliency
models on Subset I. The results are summarized in Table III.
The performance is ﬁrst averaged over frames of the same
video and then averaged over all videos. We give the intuitive
illustration of the prediction results on different videos in
Fig. 6. Performance of models on each stimulus is presented
in Table IV. For each stimulus, the ﬁrst column reports the
averaged result over all frames of the video. The second
column reports the standard deviation over all frames. For
each stimulus, we sum and normalize the saliency over all
frames, and form the packed saliency map that is deﬁned in Eq.
(19). The computation results on packed saliency is reported

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

11

TABLE IV: PERFORMANCE COMPARISON WITH STATE-OF-THE-ART SALIENCY MODELS ON SUBSET I. AUC VALUES ON
12 VIDEOS ARE SHOWN. THE TOP 2 MODELS ARE IN BOLD, AND THE TOP 1 NON-PANORAMIC MODEL IS IN RED.

Category & Method

Non-deep
Static

Deep
Static

Non-deep
Dynamic
Deep
Dynamic

Panoramic
Static

Proposed

BMS [36]
GBVS [19]
Itti [37]
Mlnet [38]
Salgan [30]
Salicon [39]
PQFT [40]
SeR [41]
SalEMA [42]
TASED [43]
SJTU H [16]
SJTU HE [16]
SalNet360 [44]
IIP360 [45]
VBAS

Category & Method

Non-deep
Static

Deep
Static

Non-deep
Dynamic
Deep
Dynamic

Panoramic
Static

Proposed

BMS [36]
GBVS [19]
Itti [37]
Mlnet [38]
Salgan [30]
Salicon [39]
PQFT [40]
SeR [41]
SalEMA [42]
TASED [43]
SJTU H [16]
SJTU HE [16]
SalNet360 [44]
IIP360 [45]
VBAS

Category & Method

Non-deep
Static

Deep
Static

Non-deep
Dynamic
Deep
Dynamic

Panoramic
Static

Proposed

BMS [36]
GBVS [19]
Itti [37]
Mlnet [38]
Salgan [30]
Salicon [39]
PQFT [40]
SeR [41]
SalEMA [42]
TASED [43]
SJTU H [16]
SJTU HE [16]
SalNet360 [44]
IIP360 [45]
VBAS

Bar counter
std
0.031
0.041
0.026
0.041
0.037
0.033
0.032
0.069
0.048
0.039
0.033
0.033
0.031
0.027
0.038

Lawn
std
0.032
0.042
0.034
0.057
0.064
0.041
0.032
0.055
0.034
0.028
0.050
0.050
0.034
0.034
0.050

Grassland
std
0.028
0.029
0.027
0.042
0.043
0.030
0.037
0.061
0.027
0.034
0.045
0.044
0.045
0.027
0.030

All
0.704
0.691
0.684
0.739
0.738
0.736
0.691
0.701
0.717
0.747
0.741
0.742
0.739
0.748
0.805

All
0.697
0.677
0.692
0.716
0.724
0.750
0.698
0.698
0.695
0.725
0.765
0.765
0.752
0.764
0.821

All
0.716
0.672
0.690
0.753
0.760
0.761
0.716
0.715
0.771
0.754
0.762
0.762
0.754
0.768
0.848

Mean
0.718
0.703
0.698
0.756
0.754
0.748
0.698
0.666
0.732
0.752
0.757
0.758
0.756
0.764
0.837

Mean
0.712
0.695
0.706
0.740
0.741
0.769
0.700
0.697
0.714
0.742
0.785
0.785
0.769
0.782
0.846

Mean
0.735
0.692
0.710
0.774
0.782
0.782
0.718
0.706
0.792
0.782
0.788
0.788
0.769
0.789
0.886

Auto show
std
0.024
0.030
0.027
0.034
0.034
0.030
0.029
0.031
0.035
0.033
0.037
0.037
0.025
0.029
0.031

Airport
std
0.033
0.038
0.030
0.036
0.032
0.027
0.046
0.053
0.041
0.042
0.048
0.047
0.047
0.030
0.036

Bicycle train
std
0.024
0.032
0.030
0.033
0.031
0.028
0.031
0.032
0.029
0.029
0.032
0.031
0.026
0.018
0.024

All
0.676
0.647
0.673
0.744
0.721
0.743
0.682
0.675
0.728
0.732
0.749
0.749
0.735
0.750
0.815

All
0.685
0.668
0.677
0.722
0.707
0.734
0.683
0.688
0.697
0.728
0.722
0.724
0.686
0.740
0.807

All
0.738
0.696
0.700
0.771
0.767
0.785
0.728
0.719
0.763
0.775
0.778
0.776
0.773
0.803
0.863

Mean
0.686
0.656
0.672
0.759
0.737
0.750
0.683
0.682
0.740
0.737
0.765
0.764
0.746
0.767
0.838

Mean
0.700
0.684
0.690
0.736
0.723
0.747
0.681
0.687
0.710
0.739
0.743
0.744
0.660
0.763
0.841

Mean
0.742
0.710
0.708
0.787
0.778
0.792
0.720
0.728
0.777
0.788
0.791
0.789
0.783
0.813
0.898

Underwater
std
0.037
0.033
0.034
0.057
0.050
0.048
0.047
0.074
0.037
0.041
0.030
0.031
0.034
0.050
0.059

Car race
std
0.030
0.042
0.030
0.043
0.031
0.028
0.054
0.030
0.048
0.044
0.039
0.040
0.029
0.027
0.025

Ofﬁce
std
0.020
0.025
0.020
0.024
0.028
0.026
0.031
0.033
0.030
0.028
0.032
0.032
0.024
0.021
0.030

All
0.671
0.633
0.655
0.661
0.675
0.683
0.629
0.571
0.676
0.668
0.719
0.718
0.682
0.697
0.737

All
0.710
0.699
0.704
0.746
0.767
0.778
0.727
0.710
0.735
0.763
0.801
0.801
0.766
0.782
0.858

All
0.702
0.666
0.702
0.763
0.752
0.735
0.685
0.699
0.738
0.747
0.703
0.703
0.729
0.766
0.805

Mean
0.682
0.638
0.659
0.654
0.687
0.684
0.624
0.555
0.694
0.686
0.736
0.735
0.684
0.704
0.726

Mean
0.733
0.717
0.723
0.769
0.788
0.798
0.724
0.736
0.759
0.784
0.817
0.818
0.785
0.799
0.897

Mean
0.709
0.677
0.708
0.773
0.766
0.739
0.685
0.697
0.748
0.751
0.713
0.713
0.743
0.778
0.832

Street dance
std
0.031
0.048
0.037
0.035
0.039
0.031
0.029
0.052
0.040
0.025
0.044
0.043
0.034
0.028
0.035

All
0.700
0.639
0.665
0.745
0.729
0.741
0.697
0.693
0.694
0.754
0.738
0.738
0.700
0.766
0.814

Mean
0.712
0.650
0.677
0.763
0.746
0.757
0.699
0.689
0.701
0.766
0.752
0.753
0.709
0.783
0.851

Spacesuit Training
std
0.040
0.043
0.039
0.062
0.061
0.057
0.052
0.047
0.053
0.059
0.052
0.052
0.056
0.045
0.053

All
0.625
0.603
0.616
0.676
0.673
0.685
0.640
0.649
0.660
0.690
0.719
0.719
0.682
0.711
0.775

Mean
0.624
0.604
0.619
0.688
0.685
0.686
0.621
0.645
0.670
0.707
0.732
0.731
0.669
0.723
0.789

Time square
std
0.037
0.037
0.032
0.052
0.043
0.055
0.030
0.033
0.046
0.038
0.033
0.033
0.031
0.040
0.034

All
0.614
0.583
0.598
0.645
0.641
0.606
0.643
0.601
0.688
0.674
0.715
0.715
0.723
0.679
0.751

Mean
0.615
0.582
0.594
0.649
0.642
0.604
0.651
0.629
0.696
0.680
0.727
0.727
0.729
0.683
0.776

in the third column. Besides, we test all the saliency models
on Subset II. The results are summarized in Table V.

Ablation Study. In our model, we employ different feature
fusion strategies (f1, f2, f3). Besides, we consider the impact
of augmentations according to augmentation types (f4). And
we combine the extracted visual cues with the characteristics
of viewing behaviors to deﬁne edge weights on graphs which
are interpreted as Markov chains (f5). We conduct the ablation
study to single out the contribution of each component. The
experimental results are summarized in Table III. From the
results we can make the observation that the proposed model
can be greatly improved by f5, which can ensure our model
concentrate on signiﬁcant block maps rather than uniform
is made by f4, which
weights. Besides, an improvement
consider the interplay between virtual information and reality
to make a distinction between augmentation types. We also test
the performance of the proposed method using different fusion
strategies. In our experiments, summation after normalization
(f3) performs better than product after normalization (f1) and

max after normalization (f2).

Qualitative Analysis. The performance of the proposed
model has been explored in the qualitative perspective. Videos
can be characterized by content complexity. The spatial and
temporal complexity contribute to the content diversity. Some
videos contain obvious foreground objects, while others have
no obvious contrast between background and foreground.
Besides, some videos contain rich texture information, while
others contain the plain areas of less texture information. And
some videos have rich and distributed motion information,
while others have concentrated motion. We choose 6 videos
and extract the example frames. The “Bar counter” is indoor
scene and contains concentrated motion and salient objects,
and rich texture information in the background. The “Car race”
is outdoor scene and contains concentrated motion and salient
objects. The “Auto show” has no obvious contrast between
background and foreground and have multiple salient objects.
The “Street dance” has no background motion and several
salient objects. The “Grassland” has some salient objects that

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

12

TABLE V: PERFORMANCE COMPARISON WITH STATE-OF-
THE-ART SALIENCY MODELS ON SUBSET II.

Category & Method

Non-deep
Static

Deep
Static

Non-deep
Dynamic
Deep
Dynamic

Panoramic
Static

Proposed

BMS [36]
GBVS [19]
Itti [37]
Mlnet [38]
Salgan [30]
Salicon [39]
PQFT [40]
SeR [41]
SalEMA [42]
TASED [43]
SJTU H [16]
SJTU HE [16]
SalNet360 [44]
IIP360 [45]
VBAS

AUC-Judd ↑
0.668
0.679
0.707
0.754
0.713
0.726
0.685
0.730
0.764
0.794
0.748
0.749
0.712
0.743
0.807

Performance

NSS ↑
1.587
1.334
1.340
2.068
1.811
1.775
1.800
1.764
1.906
2.123
1.627
1.634
1.387
2.010
2.155

KL ↓
8.328
8.638
9.002
7.749
7.737
8.131
7.597
8.042
6.074
5.445
7.503
7.464
9.956
7.526
5.188

CC ↑
0.390
0.378
0.322
0.484
0.399
0.365
0.340
0.320
0.474
0.522
0.417
0.420
0.339
0.481
0.521

are away from the equator. The “Ofﬁce” has complex low-level
features.

From the results we can observe that our computation in
the block image can result in the much ﬁne-grained motion
predictions and saliency predictions. The prediction results
from TASED and SalEMA show that they can highlight the
motion area, but they do not pay much attention on some static
yet salient areas. Besides, some complex textures do not attract
attentions, but will interfere the predictions of some models
like GBVS and BMS, and make the prediction too uniformly
distributed and uninformative. From the “Bar counter” we can
see that our model’s prediction can omit the complex texture
in the background and meanwhile capture the salient areas.
From the “Car race”, we can see that our model can select the
prominent object near the equator rather than highlight the
whole area that is near the equator. And on the “Grassland”,
we can see that our model can also highlight the salient object
that is away from the equator. The complex background in
“Auto show” and “Ofﬁce” is proven to have limited impact
on our model. It can also be observed in “Street dance” that
our model can focus more on the object itself rather than
the virtual
the
weighting strategy according to the information amount and
the type of the augmentations.

information. Because our model can adapt

Comparison with Current Models. We test all the saliency
models on Subset I, and average all results over frames of
the video. From Table III, it is observed that the proposed
VBAS model can be comparable to the state-of-the-art saliency
models. For non-deep methods, the dynamic methods do not
show superior performance compared with static methods.
For deep-learning-based methods, the dynamic methods show
signiﬁcant superior performance in terms of KL compared
with static methods, but the dynamic methods do not show
superior performance in AUC-Judd, NSS and CC. The results
are consistent with some recent study [50]. This phenomenon
may be caused by insufﬁcient study of the visual behavior and
visual attention mechanism for 360-degree dynamic scenes.

AUC-Judd of models on each stimulus is presented in Table
IV. Compared with other saliency models, the proposed model
achieves good performance and generalizability on the videos.
Besides,
the IIP360, Salicon and TASED also have good
performance. From the second column in Table IV, which
reports the standard deviation over all frames, we can observe

that for the proposed model, there is small dispersion of the
prediction accuracy over frames. For each stimulus, we form
the packed saliency map that is deﬁned in Eq. (19). From the
results in the third column in Table IV, it can be observed that
the AUC-Judd result on packed saliency is consistent with the
result that is averaged on each frame.

the model

Although the proposed model is designed for augmented
panoramic contents, we also test
in general
panoramic scenes to test its generalizability. The same com-
peting models are evaluated on Subset II. The performance
results are summarized in Table V. It can be observed that the
proposed model delivers the most promising result. Besides,
TASED also perform well. Although the superiority of the
proposed model on Subset II is not so signiﬁcant like on Subset
I, it can also achieve the best performance in AUC-Judd, NSS
and KL. The results on Subset II show that our model that
involves computation on the local blocks and the estimation
of fraction of the visual attention for each block image can be a
promising way to predict the saliency for 360-degree contents.
Admitting that AR of video-through and see-through types
can offer users interactions with the real world, and we only
investigate the impact of augmentations, how to properly deal
with interactions with the real world will be part of our future
work. In real practice, there are many kinds of augmentations,
e.g., static augmentation, dynamic augmentation, marker based
augmentation and computer-generated imagery. Enriching the
dataset by simulating different kinds of augmentations and
investigating visual saliency distribution on these stimuli will
be another possible future work.

V. CONCLUSION

We construct an ARVR saliency dataset. The saliency
annotations of head and eye movements for both original and
augmented videos are collected, which together constitute the
ARVR dataset. We also design a model which is capable of
solving the saliency prediction problem in AR. We extract
local block images to simulate the viewport and offset the
projection distortion. Conspicuous visual cues in local view-
ports are extracted in the spatial domain and temporal domain.
We estimate the joint effects of adversarial augmentation and
complementary augmentation and apply the spatial weighting
strategy accordingly. We build graphs on block images which
are interpreted as Markov chains. In the computation of edge
weights, we emphasize the characteristics of viewing behaviors
and visual saliency mechanism. The fraction of the visual
attention that is diverted to each block image is estimated
through equilibrium distribution of this chain. The experimen-
tal results demonstrate the effectiveness of our method.

REFERENCES

[1] R. Azuma, “Tracking requirements for augmented reality,” Communica-

tions of the ACM, vol. 36, no. 7, pp. 50–52, 1993.

[2] D. Van Krevelen and R. Poelman, “A survey of augmented reality
journal of

technologies, applications and limitations,” International
virtual reality, vol. 9, no. 2, pp. 1–20, 2010.

[3] B. Furht, Handbook of augmented reality. Springer Science & Business

Media, 2011.

[4] M. Billinghurst and H. Kato, “Collaborative augmented reality,” Com-

munications of the ACM, vol. 45, no. 7, pp. 64–70, 2002.

ZHU et al.: TOWARD BETTER UNDERSTANDING OF SALIENCY PREDICTION IN AUGMENTED 360 DEGREE VIDEOS

13

[5] E. Kruijff, J. E. Swan, and S. Feiner, “Perceptual issues in augmented
reality revisited,” in 2010 IEEE International Symposium on Mixed and
Augmented Reality, Oct 2010, pp. 3–12.

[6] L. Itti and A. Borji, “Computational models: Bottom-up and top-down

[30] J. Pan, C. C. Ferrer, K. McGuinness, N. E. O’Connor, J. Torres,
E. Sayrol, and X. Giro-i Nieto, “SalGAN: Visual saliency prediction
with generative adversarial networks,” arXiv preprint, 2017.

[31] Vive Pro, “VIVE PRO EYE: HMD with Precise Eye Tracking,” https:

//enterprise.vive.com/us/product/vive-pro-eye/, 2019.

[32] A. Nguyen and Z. Yan, “A saliency dataset for 360-degree videos,” in
Proceedings of the 10th ACM Multimedia Systems Conference. ACM,
2019, pp. 279–284.

[33] K. Rayner, “Eye movements and cognitive processes in reading, visual
North-

search, and scene perception,” in Eye Movement Research.
Holland, 1995, vol. 6, pp. 3 – 22.

[34] Z. Bylinskii, T. Judd, A. Borji, L. Itti, F. Durand, A. Oliva, and

A. Torralba, “Mit saliency benchmark.”

[35] B. John, P. Raiturkar, O. Le Meur, and E. Jain, “A benchmark of four
methods for generating 360 saliency maps from eye tracking data,”
International Journal of Semantic Computing, vol. 13, no. 03, pp. 329–
341, 2019.

[36] J. Zhang and S. Sclaroff, “Saliency detection: A boolean map approach,”
in Proceedings of the IEEE international conference on computer vision,
2013, pp. 153–160.

[37] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual at-
tention for rapid scene analysis,” IEEE Transactions on pattern analysis
and machine intelligence, vol. 20, no. 11, pp. 1254–1259, 1998.
[38] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level
network for saliency prediction,” in International Conference on Pattern
Recognition.

IEEE, 2016, pp. 3488–3493.

[39] M. Jiang, S. Huang, J. Duan, and Q. Zhao, “Salicon: Saliency in
context,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2015, pp. 1072–1080.

[40] C. Guo, Q. Ma, and L. Zhang, “Spatio-temporal saliency detection using
phase spectrum of quaternion fourier transform,” in IEEE Conference on
Computer Vision and Pattern Recognition.

IEEE, 2008, pp. 1–8.

[41] H. J. Seo and P. Milanfar, “Static and space-time visual saliency
detection by self-resemblance,” Journal of vision, vol. 9, no. 12, pp.
15–15, 2009.

[42] P. Linardos, E. Mohedano, J. J. Nieto, N. E. O’Connor, X. Giro-i Nieto,
and K. McGuinness, “Simple vs complex temporal recurrences for video
saliency prediction,” arXiv preprint arXiv:1907.01869, 2019.

[43] K. Min and J. J. Corso, “Tased-net: Temporally-aggregating spatial
encoder-decoder network for video saliency detection,” in Proceedings
of the IEEE International Conference on Computer Vision, 2019, pp.
2394–2403.

[44] R. Monroy, S. Lutz, T. Chalasani, and A. Smolic, “Salnet360: Saliency
maps for omni-directional images with cnn,” Signal Processing: Image
Communication, vol. 69, pp. 26–34, 2018.

[45] K. Zhang and Z. Chen, “Video saliency prediction based on spatial-
two-stream network,” IEEE Transactions on Circuits and

temporal
Systems for Video Technology, vol. 29, no. 12, pp. 3544–3557, 2018.

[46] T. Judd, F. Durand, and A. Torralba, “A benchmark of computational

models of saliency to predict human ﬁxations,” 2012.

[47] R. J. Peters, A. Iyer, L. Itti, and C. Koch, “Components of bottom-up
gaze allocation in natural images.” Vision Research, vol. 45, no. 18, pp.
2397–2416, 2005.

[48] B. W. Tatler, R. J. Baddeley, and I. D. Gilchrist, “Visual correlates of
ﬁxation selection: Effects of scale and time,” Vision research, vol. 45,
no. 5, pp. 643–659, 2005.

[49] T. Jost, N. Ouerhani, R. Von Wartburg, R. M¨uri, and H. H¨ugli, “Assess-
ing the contribution of color in visual attention,” Computer Vision and
Image Understanding, vol. 100, no. 1-2, pp. 107–123, 2005.

[50] W. Wang, J. Shen, F. Guo, M.-M. Cheng, and A. Borji, “Revisiting video
saliency: A large-scale benchmark and a new model,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 4894–4903.

aspects,” ArXiv e-prints, Oct. 2015.

[7] R. Yang, M. Xu, Z. Wang, Y. Duan, and X. Tao, “Saliency-guided com-
plexity control for hevc decoding,” IEEE Transactions on Broadcasting,
vol. 64, no. 4, pp. 865–882, 2018.

[8] M. Paul, “Efﬁcient multiview video coding using 3-d coding and
saliency-based bit allocation,” IEEE Transactions on Broadcasting,
vol. 64, no. 2, pp. 235–246, 2018.

[9] B. Yang, X. Zhang, L. Chen, and Z. Gao, “Principal component analysis-
based visual saliency detection,” IEEE Transactions on Broadcasting,
vol. 62, no. 4, pp. 842–854, 2016.

[10] Q. Huynh-Thu, M. Barkowsky, and P. Le Callet, “The importance of
visual attention in improving the 3d-tv viewing experience: Overview
and new perspectives,” IEEE Transactions on Broadcasting, vol. 57,
no. 2, pp. 421–431, 2011.

[11] Y. Rai, P. L. Callet, and P. Guillotel, “Which saliency weighting for omni
directional image quality assessment?” in International Conference on
Quality of Multimedia Experience.

IEEE, 2017, pp. 1–6.

[12] M. Startsev and M. Dorr, “360-aware saliency estimation with conven-
tional image saliency predictors,” Signal Processing: Image Communi-
cation, vol. 69, pp. 43 – 52, 2018.

[13] F. Battisti, S. Baldoni, M. Brizzi, and M. Carli, “A feature-based
approach for saliency estimation of omni-directional images,” Signal
Processing: Image Communication, vol. 69, pp. 53–59, 2018.

[14] J. Ling, K. Zhang, Y. Zhang, D. Yang, and Z. Chen, “A saliency predic-
tion model on 360 degree images using color dictionary based sparse
representation,” Signal Processing: Image Communication, vol. 69, pp.
60 – 68, 2018.

[15] P. Lebreton and A. Raake, “GBVS360, BMS360, ProSal: Extending
existing saliency prediction models from 2D to omnidirectional images,”
Signal Processing: Image Communication, vol. 69, pp. 69 – 78, 2018.
[16] Y. Zhu, G. Zhai, and X. Min, “The prediction of head and eye movement
for 360 degree images,” Signal Processing: Image Communication,
vol. 69, pp. 15 – 25, 2018.

[17] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, and M. Sun,
“Cube padding for weakly-supervised saliency prediction in 360 videos,”
in IEEE Conference on Computer Vision and Pattern Recognition.
IEEE, 2018, pp. 1420–1429.

[18] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and S. Gao, “Gaze
prediction in dynamic 360◦ immersive videos,” in IEEE Conference on
Computer Vision and Pattern Recognition.
IEEE, 2018, pp. 5333–5342.
[19] J. Harel, C. Koch, and P. Perona, “Graph-based visual saliency,” in
Advances in neural information processing systems, 2007, pp. 545–552.
[20] C. Carlson, “How I made wine glasses from sunﬂowers,” http://blog.

wolfram.com/2011/07/28/how-i-made-wine-glasses-from-sunﬂowers/.

[21] J. Guti´errez, E. David, Y. Rai, and P. Le Callet, “Toolbox and dataset
for the development of saliency and scanpath models for omnidi-
rectional/360 still images,” Signal Processing: Image Communication,
vol. 69, pp. 35–42, 2018.

[22] Dataset, “Large-scale scene understanding (lsun) database,” http://

salicon.net/challenge-2017/, 2017.

[23] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning
optical ﬂow with convolutional networks,” in Proceedings of the IEEE
international conference on computer vision, 2015, pp. 2758–2766.
[24] S. J. Guastello, Human factors engineering and ergonomics: A systems

approach. CRC Press, 2013.

[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in IEEE Conference on Computer Vision and Pattern
Recognition.

IEEE, 2016, pp. 770–778.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in Neural Infor-
mation Processing Systems, 2012, pp. 1097–1105.

[27] X. Min, G. Zhai, K. Gu, and X. Yang, “Fixation prediction through
multimodal analysis,” ACM Transactions on Multimedia Computing,
Communications, and Applications, vol. 13, no. 1, pp. 1–23, 2016.
[28] T. Brandt, J. Dichgans, and E. Koenig, “Differential effects of central
versus peripheral vision on egocentric and exocentric motion percep-
tion,” Experimental brain research, vol. 16, no. 5, pp. 476–491, 1973.
[29] K.-C. Chang, K. Pearson, and T. Zhang, “Perron-frobenius theorem for
nonnegative tensors,” Communications in Mathematical Sciences, vol. 6,
no. 2, pp. 507–520, 2008.

