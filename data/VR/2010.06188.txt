When Wireless Communications Meet Computer Vision
in Beyond 5G

Takayuki Nishio, Yusuke Koda†, Jihong Park‡, Mehdi Bennis††, and Klaus Doppler‡‡

1

(cid:70)

0
2
0
2

t
c
O
3
1

]

V
C
.
s
c
[

1
v
8
8
1
6
0
.
0
1
0
2
:
v
i
X
r
a

Abstract—This article articulates the emerging paradigm, sitting at
the conﬂuence of computer vision and wireless communication, to en-
able beyond-5G/6G mission-critical applications (autonomous/remote-
controlled vehicles, visuo-haptic VR, and other cyber-physical applica-
tions). First, drawing on recent advances in machine learning and the
availability of non-RF data, vision-aided wireless networks are shown
to signiﬁcantly enhance the reliability of wireless communication without
sacriﬁcing spectral efﬁciency. In particular, we demonstrate how com-
puter vision enables look-ahead prediction in a millimeter-wave channel
blockage scenario, before the blockage actually happens. From a com-
puter vision perspective, we highlight how radio frequency (RF) based
sensing and imaging are instrumental in robustifying computer vision
applications against occlusion and failure. This is corroborated via an
RF-based image reconstruction use case, showcasing a receiver-side
image failure correction resulting in reduced retransmission and latency.
Taken together, this article sheds light on the much-needed convergence
of RF and non-RF modalities to enable ultra-reliable communication and
truly intelligent 6G networks.

INTRODUCTION

The overarching goal of ultra-reliable and low-latency com-
munication (URLLC) lies in satisfying the stringent reliabil-
ity and latency requirements of mission and safety-critical
applications. In order to achieve these stringent require-
ments, current 5G URLLC solutions come at the cost of low
spectral efﬁciency due to channel probing and estimation. In
addition, 5G URLLC presumes a static channel model that
fails to capture non-stationary channel dynamics and exoge-
nous uncertainties (e.g., out-of-distribution or other under-
modeled rare events), which are germane to uncontrolled
environments [1]. To overcome these fundamental limita-
tions, driven by the recent advances in machine learning
(ML) and computer vision, one key enabler for beyond-5G
URLLC is leveraging visual data (e.g., RGB depth (RGB-D)
camera imagery, LiDAR point cloud, etc.) generated from
a variety of vision sensors that are prevalent in intelligent
machines such as robots, drones, and autonomous vehicles.
From a wireless standpoint, these visual data enable a more

T. Nishio is with the School of Engineering, Tokyo Institute of Technology,
152-8550, Tokyo, Japan. (email: nishio@ict.e.titech.ac.jp)
†Y. Koda is with the Graduate School of Informatics, Kyoto University, 606-
8501 Kyoto, Japan (email: koda@imc.cce.i.kyoto-u.ac.jp).
‡J. Park is with the School of Information Technology, Deakin University,
Geelong, VIC 3220, Australia (e-mail: jihong.park@deakin.edu.au).
†† M. Bennis is with the Centre for Wireless Communications, University of
Oulu, 90014 Oulu, Finland (email: mehdi.bennis@oulu.ﬁ).
‡‡ K. Doppler is with Nokia Bell Labs, Sunnyvale, CA 94086, USA.

Fig. 1. An illustration of vision aided wireless communication, i.e., view to
communicate (V2C), for millimeter-wave (mmWave) channel prediction
and predictive handover, and RF signal assisted imaging, i.e., commu-
nicate to view (C2V) for image inpainting.

accurate prediction of wireless channel dynamics such as
future received power and channel blockages, as well as
constructing high-deﬁnition 3D environmental maps for
improved indoor positioning and navigation [2]. This line
of works is referred to as view to communicate (V2C).

On the other hand, in some scenarios computer vision is
vulnerable to occlusions of visible light by walls, human
body, and other environmental artifacts such as lighting.
This can be addressed by leveraging radio frequency (RF)
sensing such as using Wi-Fi signals to diffract and detour
blockages as opposed to visible light, thereby precisely
tracking user locations even behind walls [3]. More re-
cently, exploiting millimeter-wave (mmWave) and terahertz
(THz) signals can provide even higher-resolution sensing
capabilities that can penetrate body tissues for non-invasive
medical imaging [4]. This research direction is referred to as
communicate to view (C2V).

Motivated by the aforementioned conﬂuence of com-
puter vision and RF-based wireless transmission, this article
sheds light on the synergies and complementarities of the
integration of both visual and RF modalities for enabling
URLLC in 5G and beyond. To this end, we discuss the chal-
lenges and research opportunities in V2C and C2V. Then,
their feasibility is demonstrated using selected use cases,

HumanblockageVision-based RF predictionPredictCamera imagesRSSTimeHetero-modal image reconstruction Inpainting missing part fromRF signalsOcclusion in camera vieworinterference on wireless linkBS 2RL-basedDecision makingHandoverBS1→BS2Predictive decision making (handover)BSMobile userReceived powerBS 1Imagesfrom BS 1Imagesfrom BS 2Receive image w/ failureTransmit imageRSSTimeReceived power of RF signalBrokenimageInpaintedimage 
 
 
 
 
 
ranging from vision-aided mmWave channel prediction and
proactive handover decisions, to image reconstruction of
lost visual parts caused by packet loss. Finally, we conclude
this article by laying down future research directions.

V2C: VISION-AIDED WIRELESS SYSTEMS
A new paradigm in beyond-5G wireless systems is to lever-
age non-RF data, among which visual images complement
traditional RF-based systems [1]. For instance, one can pre-
dict future mmWave channel conditions using a sequence
of camera images containing mobile blockage patterns [5],
thereby enabling proactive decision-making (e.g., handover,
beamforming, multi-path transmission, etc.). In what fol-
lows, the rationale related works, and future research op-
portunities of V2C are elaborated.

Vision-Based RF Channel Prediction

Motivation.
In beyond-5G systems, mmWave and THz
signals are envisaged to play an important role thanks
to their abundant bandwidth. However, these signals are
highly directional and vulnerable to blockages, such as mov-
ing pedestrians, vehicles, and so forth. Hence, predicting
the occurrence of blocked and non-blocked channels, that
is line-of-sight (LOS) and non-LOS (NLOS), is crucial in
ensuring reliable connectivity, notably for mission-critical
applications. Predicting such events using past RF signals is
extremely challenging while consuming spectral resources.
To obviate this problem, visual data such as RGB-D images
and 3D point cloud that capture a variety of hidden features
in wireless environments (e.g., object locations, shapes, ma-
terials, and mobility patterns) can be exploited. In so doing,
one can accurately predict future mmWave and THz channel
conditions without consuming RF resources to probe and
estimate the channels.

Related Works. RGB-D images are useful for accurately
predicting the future received power in mmWave (i.e., above
6 GHz) and sub-6 GHz carrier frequencies. In [5], the future
mmWave received power is predicted by feeding past RGB-
D images into a deep neural network (DNN), in which two
randomly moving people block the communication link in
an indoor experiment. Similarly, in [6], future 2.4 GHz chan-
nel states in an indoor experiment are accurately predicted
using RGB-D images fed into a DNN. As demonstrated by
these prior experiments, vision-based solutions can achieve
accurate RF channel prediction without consuming any RF
resources. This is in stark contrast to traditional channel
prediction methods that frequently exchange RF pilot sig-
nals for high prediction reliability, which is not feasible in
URLLC due to the stringent latency requirements.

Opportunities. Beyond the
aforementioned received
power prediction, V2C has far more potential in predicting
packet error rates, the number of reﬂectively propagating
paths, optimal beam directions, to mention a few. Fur-
thermore, in addition to indoor environments, it is worth
studying the effectiveness of V2C in urban outdoor en-
vironments wherein the channel prediction becomes more
challenging due to highly dynamic mobile blockage pat-
terns, higher number of blockers and reﬂectively propa-
gating paths. Last but not least, it is important to develop

2

sample-efﬁcient prediction techniques since conventional
DNN training frameworks often require a large number
of data samples. Alternatively, by exploiting meta learning
and transfer learning, one can pre-train a DNN using easily
accessible data (e.g., data collected from public repository,
ray-tracing simulations, etc.), and then ﬁne-tune the DNN
with only a few on-site data.

Hetero-Modal Vision-Based RF Channel Prediction

Motivation. Fusing visual data with other modalities can
enrich the useful features of wireless environments while
complementing the missing features in the visual modality.
Because vision is vulnerable to object occlusion while hav-
ing restricted ﬁeld-of-views (FoVs), audio data can partly
complement such limitations; for example, by hearing the
Doppler effect, one can predict a vehicle’s moving direction
and speed. Another example is inertial measurement unit
(IMU) data tracking the user movements during blockages
which can also be used to track relative velocities to block-
ages.

Related Works. DNNs are capable of fusing heteroge-
neous data modalities. In [7], 2D images and 3D face
renderings are vectorized, and concatenated at the input
layer of a convolutional neural network (CNN) for learn-
ing facial representations. In [8], to predict future channel
conditions, received RF signal power and RGB-D images
are fused using a multi-modal split learning architecture,
while RGB-D images captured from different FoVs are in-
tegrated via an average pooling layer. Such fusion can be
immediately achieved without incurring any extra latency,
as opposed to traditional data fusion algorithms consuming
non-negligible computing time.

Opportunities. Understanding the pros and cons of each
data modality is crucial. As an example, for user localiza-
tion, Wi-Fi signals (sub 6 GHz) are useful to cope with
blockages [9], yet can hardly achieve high precision since
the blockages result in NLOS communications.By contrast,
mmWave signals (above 6 GHz) are vulnerable to blockages
and require denser deployment. Therefore, it is mostly LOS
communications and can achieve high-precision localiza-
tion. This highlights the importance of selecting and match-
ing useful data types. Furthermore, the accuracy and cost of
channel prediction hinge on how to fuse the hetero-modal
data. For instance, compared to average pooling, concatena-
tion consumes more energy due to the increase in the model
size, in return for achieving higher prediction accuracy.
Hence, it is important to optimize the fusion framework,
consisting of DNN architectures, training algorithms, and
data pre/post-processing, subject to energy requirements.
To this end, split learning is a promising framework, in
which a DNN is split into multiple subnetworks that are
individually stored by each device [8]. By adjusting the
cut layer, one can reliably satisfy each device’s energy
constraint.

Vision-Based Proactive Decision-Making

Motivation. Based on predicted future channel informa-
tion, one promising way is to proactively carry out decision-
making in wireless systems. For example, by predicting

future blockage occurrences at each base station (BS), one
can seamlessly handover users in order not to experience
NLOS channels. In a similar vein, content can be proactively
cached at the user prior to a blockage.

Related Works. The effectiveness of predictive beamform-
ing has been demonstrated in [10]. Therein, blockage pat-
terns are learned by a statistical learning method, and a
proactive beamforming algorithm is applied to reduce the
link outage probability. Moreover, vision-based handover
methods have been investigated in [11], in which a reinforce-
ment learning (RL) framework learns an optimal mapping
from visual data onto handover strategies. More details of
this work will be elaborated in a later section as a selected
use case.

Opportunities. Towards supporting URLLC, a single
proactive decision based on a wrong prediction may cause
catastrophic consequences. This calls for designing robust-
ness against prediction failures and for increased prediction
accuracy. For example, prediction errors due to video packet
loss, dead camera pixels or stand vision sensors can be
reduced by using RF data to reconstruct the distorted visual
data, emphasizing the importance of the research direction
of C2V, to be discussed in the following section.

C2V: WIRELESS-AIDED COMPUTER VISION

Traditional computer vision is based on the imagery cap-
tured using visible light, so is limited within line-of-sight
(LOS). Compared to visible light, RF signals are more
diffractive, thereby enabling non-LOS imaging (e.g., see-
through-walls [4], [9]) that is necessary for non-intrusive in-
spection in mission-critical and time-sensitive applications.
Furthermore, fusing the visible imagery and RF signals, one
can improve the imagery resolution while reconstructing
distorted or occluded objects. From the perspective of such
a C2V research direction, the rationale, related works, and
future opportunities are elaborated next.

RF-Based Imaging

Motivation. Ultra-high frequencies such as mmWave and
THz bands are expected to be a key enabler for high-
resolution NLOS imaging. Building walls and ﬂoors typ-
ically behave to a ﬁrst order as mirrors and reﬂect the
high-frequency signals, especially THz signals, which en-
ables seeing behind walls and around corners assuming
sufﬁcient reﬂection or scattering paths [12]. In addition to
NLOS imaging, mmWave and THz based imaging are less
impacted by weather and ambient light compared to optical
cameras. Another advantage is short exposure time. The
typical exposure time of optical camera is several to few
tens milliseconds, while that of RF-imaging is microseconds,
which enables high speed RF cameras to track fast move-
ment.

Related Works. The feasibility of THz-based NLOS imag-
ing was demonstrated through imaging examples in the
220–330 GHz band using common building materials [4].
A mmWave-based gait recognition method was studied for
recognizing persons from their walking postures, which is

3

expected to be still effective under non-line-of-sight scenar-
ios [13].

Opportunities. Severe signal attenuation of mmWave and
THz signals induced by pathloss and blockage limits the
coverage of RF-based imaging. In 5G/6G networks based
on mmWave and THz bands, highly directional antenna and
densely deployment are exploited to compensate the signal
attenuation, and hence these solutions could be utilized
in mmWave/THz-based imaging. However,
interference
among both RF-based imaging and mmWave/THz commu-
nication systems remains a critical issue. For enabling co-
existence of RF-based imaging and communication systems,
one can exploit wireless resource scheduling and multiple
access mechanisms such as time division multiple access
(TDMA), carrier sense multiple access/collision avoidance
(CSMA/CA), and non-orthogonal multiple access (NOMA).
Another way to mitigate the interference is interference can-
cellation which processes the known transmitted imaging
or communication signal to generate a negative that, when
added to the composite signal, reverts the effect of the
interference.

Multi-Band RF-Based Imaging

Motivation.
Improving resolution and accuracy is an im-
portant challenge in RF-based imaging. To this end, joint use
of multiple signals on different frequency band is a promis-
ing way. Recent wireless networks can leverage multiple
frequency bands. For example, Wi-Fi devices will be able to
utilize sub-GHz, 6 GHz, and mmWave (60 GHz) in addition
to 2.4 and 5 GHz. Such different frequencies have different
propagation characteristics, resulting in different resolution
and FoV on imaging. Thus, cooperatively using multiple
signals could improve image resolution and sensing accu-
racy.

Related Works. A super-resolution of multi-band radar
data on 3–12 GHz bands and decimeter-level localization
leveraging multi-band signals on 900 MHz, 2.4 GHz, and
5 GHz have been studied in [14] and [2], respectively. These
works demonstrated that cooperative use of multiple signals
on different frequency bands can improve imaging resolu-
tion or localization accuracy.

Opportunities. With increase in range of frequency bands
(e.g., joint use of sub-6 GHz and mmWave signals), we need
to consider resolution-coverage trade-off. MmWave and
THz signals enable high-resolution imaging, but the high
attenuation limits the coverage of RF-based imaging. On
the other hand, lower frequency (sub-GHz) generates lower-
resolution images than mmWave/THz imaging, but their
coverage is wider than mmWave/THz imaging. Therefore,
adaptive use of multiple frequency bands is expected to
achieve better trade-off between the resolution and cover-
age. Moreover, utilizing multiple channels and wider band-
width could cause severe interference with multiple com-
munication systems and make the interference management
more difﬁcult. Thus, the co-existence mechanism of commu-
nication and imaging systems becomes more important.

Hetero-modal RF-Based Imaging

Motivation. Leveraging heterogeneous modalities could
be another solution to improve resolution and reliability
of imaging. Smart devices such as smart phones, vehicles,
and drones have multiple imaging sensors (e.g., camera and
LiDAR) and RF modules (e.g., Wi-Fi, Bluetooth, 4G/5G, and
WiGig). We can exploit these modalities cooperatively for
imaging and sensing. However, there is an open issue of
how to integrate the heterogeneous modalities.

Related Works.
In computer vision, deep learning based
multi-modal image fusion is studied for improving image
quality [15]. Multi-modal images (e.g., Visual, IR, CT, and
MRI images) are fused based on their pixel values via some
fusion rule, which is called as pixel level fusion. There are
other fusion approaches; feature level fusion and decision
level fusion. In the feature level fusion, prominent features
(e.g., edges, corner points, and shapes) are extracted from
different images and combined into a feature map. In the
decision level fusion, the different images are pre-processed
and leveraged for decision making separately. Then, the
individual decisions are integrated to provide more accurate
decision.

Opportunities. Although the pixel level fusion generally
requires a heavier computation than other level fusion
techniques, it is still widely used in many ﬁelds such as
remote sensing because of higher accuracy. A major issue of
the multi-modal imaging is spatial and temporal misregis-
tration induced by different image scales, resolutions, and
deployed angles and locations of the sensors. Moreover, in
RF-based imaging, there could be a new fusion level, that is
the signal level fusion. In the fusion, RF signals are directly
fused, and new features are generated for more accurate
imaging. The next section details a case of the signal level
fusion for predicting a missing part of an image.

SELECTED USE CASES

Hetero-Modal mmWave Received Power Prediction

As discussed in the earlier section, past image sequences are
informative to forecast sudden LOS and NLOS transitions,
which is hardly observable from RF received power se-
quences. On the contrary, past RF received power sequences
are informative to predict future received powers highly
correlated with the past ones under LOS conditions. To
beneﬁt from these two modalities and thereby achieve better
accuracy, the prediction method fusing these two modalities
are studied as follows.

Scenario. Consider a depth camera with 30 Hz frame rate
monitoring a mmWave link that is intermittently blocked by
two moving pedestrians. Our objective is to predict future
received powers with a look ahead horizon of 120 ms based
on a past depth image sequence and a received power
sequence. To this end, a split NN architecture is designed
to integrate the depth image and received power sequences,
thereby performing mmWave received power prediction
with the two types of modalities. Speciﬁcally, the split NN
comprises convolutional layers that extract image features
and a recurrent layer that concatenates the sequence of

4

Fig. 2. Hetero-modal mmWave received power prediction. The error
between actual mmWave RSS and its predicted value by fusing past
RSS data and RGB-D images.

image features and RF received powers and performs time-
series prediction of mmWave received power [11].

Results.
In Fig. 2, showing the prediction accuracy in root-
mean-square error (RMSE) in different channel conditions,
we demonstrate that the prediction using both images and
RF received powers (Img+RF) achieves higher prediction
accuracy than the prediction using either one (Img and RF).
Img+RF does not only predict LOS/NLOS transitions as
well as Img, but also predicts received powers correlated
with the input received power sequence for a given LOS
and NLOS conditions better than Img. This result exactly
demonstrates the feasibility of the beneﬁt from integrating
image and RF modalities.

Multi-Vision Based Predictive mmWave BS Handover

This section introduces the use case of handover manage-
ment in mmWave communications to illustrate the impor-
tance of vision-based proactive decision-making.
Scenario. Handover can result in disruptions of the com-
munication link, and hence, in determining handover tim-
ings, one should be aware of not only current RF con-
ditions, but also how well each BS performs in a long
run to prevent myopic decisions. Traditionally, handover
strategy is formed based on current RF conditions (e.g.,
channel state or received power); however, RF conditions
are not necessarily informative to forecast sudden transi-
tions between LOS and NLOS conditions. This is where
image modality comes as a rescue, wherein one can form
a handover strategy being aware of mobility of obstacles
and thereby predicting future LOS and NLOS transitions
[8]. Moreover, recent advancement of RL, namely deep RL,
helps us achieve the aforementioned objective by feasibly
handling a higher dimensionality of image modalities.

LoSNLoSLoS/NLostransitionRMSE (dB)012345671 31.032.033.0(cid:239)(cid:23)(cid:19)(cid:239)(cid:22)(cid:19)(cid:239)(cid:21)(cid:19)Time (s)RSS (dBm)Ground truth321123Humanblockage0246810121410-310-210-1100CCDFError |RSSdBest-RSSdBest|7.657.77.757.87.857.9Time (s)-40-30-20RFImgRF+ImgGround-truthRSSTimeImgTimemobile blockageRSS (dBm)RGB-D and RSS sequences1Future RSS prediction2ImgRSSRSS0246810121410-310-210-1100CCDFError |RSSdBest-RSSdBest|7.657.77.757.87.857.9Time (s)-40-30-20RFImgRF+ImgGround-truthRSSTimeImgTimemobile blockageRSS (dBm)RGB-D and RSS sequences1Future RSS prediction2ImgRSS2. Future RSS prediction1. Scenario3. Results5

based imaging. The goal of this work is image inpainting
with hetero-modal information, in which a missing part of
an image is reconstructed from the defective image and a
sequence of mmWave received power values.
Scenario.
Consider a depth camera monitoring a
mmWave link that is intermittently blocked by two moving
pedestrians, but a part of the image is missing due to occlu-
sion or failure on the camera. The objective is to reconstruct
the missing part of the image based on signal attenuation
on the mmWave link. As shown in the previous use cases,
the mmWave signals are strongly attenuated when an ob-
stacle blocks LOS path, and the timing and intensity of the
attenuation suggest where the obstacle moved.

Deep auto-encoder is leveraged for the hetero-modal
inpainting. The encoder has two input layers for an im-
age with occlusion and sequence of received power of RF
signals. The input layer for image is followed by convolu-
tion layers, and the input layer for RF signal is followed
by fully connected layers. The outputs of these layers are
concatenated at the end of the encoder part and inputted
to the decoder part consisting of convolution layers, which
depicts an image including missing parts.
Results. Fig. 4 depicts samples of depth-camera image
without missing parts (ground truth), defective image (in-
put image), and images reconstructed from both the input
image and RF signal (Img+RF), or only from RF signal (RF
only). Even though the reconstructed imaging uses limited
features of RF signal, that is 32 points of mmWave received
power sampled at 66 ms intervals, Img+RF depicts the miss-
ing part on the input images as similar to the ground-
truth images for both cases where the pedestrian was in
the missing part or not. Such inpainting of imagery with
a large missing part is difﬁcult for the conventional image
inpainting which leverages only the imagery, because there
is no information about the missing part. In contrast, the
Img+RF reconstruction can obtain the information about the
missing part from RF signals and reconstruct it.

Moreover, the Img+RF reconstruction depicts images
more accurately than RF only. Thereby we can ﬁnd even the
direction of the pedestrian on the image reconstructed by
Img+RF. These results demonstrate the feasibility of image
inpainting with RF signals and the beneﬁt from integrating
image and RF modalities.

CONCLUSIONS
This article outlined the vision of fusing computer vision
and wireless communication to spearhead the next gen-
eration of URLLC toward beyond-5G/6G mission-critical
applications. This convergence opens up untapped research
directions that go beyond the scope of this paper. An inter-
esting direction is to investigate how much visual informa-
tion is contained in RF signals of wireless communications.
As demonstrated in the selected use cases, mmWave com-
munication signals contain visual information of obstacles
and help image inpainting. However, the current inpainted
images are mimicry of training data, and it is still unclear
what information (e.g., shape and location of obstacles)
can and cannot be retrieved from RF signals, calling for a
novel visual information capacity analysis for a given task.
Another interesting direction is the creation and update

Fig. 3. RL-based predictive handover. Action value for selecting each BS
learned in Img-RL using depth images and in RF-RL using RF received
powers.

For example, in a simple scenario with two static BSs
and single static STA. The BS serving higher received power
in LoS conditions (termed BS 1) initially associates with the
STA. The other BS termed BS 2 is a candidate to which a
handover is performed and thereby compensates for the
degraded data rates provided by BS 1 due to LOS blockages.
The BSs are served by depth cameras with depth images.
The associated BS runs deep RL to learn the optimal action
value associated with each depth image that quantiﬁes how
well each BS performs in a long time horizon. Thereby, the
optimal timing to perform a handover is determined.

Results. Fig. 3 shows the learned action value using im-
ages (Img-RL) and RF received powers (RF-RL). The action
value for selecting BS 1 learned in Img-RL decreases as a
pedestrian approaches a LoS path while that in RF-RL does
not. This result exactly indicates that Img-RL feasibly forms
a handover policy being aware of future blockage events.
Therein, Img-RL triggers a handover earlier than RF-RL, and
thereby, avoids the blockage event. Thus, Img-RL exhibits a
higher throughput (118 Mbit/s) than RF-RL (113 Mbit/s).

Hetero-Modal Image Reconstruction

This section presents the setting of mmWave signal-aided
image reconstruction as a use case of hetero-modal RF-

PedestrianBS 2BS 1UserBS IDfor handover1. Scenario2. Predictive decision making3. ResultsImages for BS1Images for BS2Time6

[12] T. Rappaport et al., “Wireless communications and applications
above 100 GHz: Opportunities and challenges for 6G and be-
yond,” IEEE Access, vol. 7, pp. 78 729–78 757, 2019.

[13] Z. Meng et al., “Gait recognition for co-existing multiple people
using millimeter wave sensing,” in Proc. AAAI Conf. Artif. Intell.,
vol. 34, no. 1, Feb. 2020, pp. 849–856.

[14] H. H. Zhang and R. S. Chen, “Coherent processing and su-
perresolution technique of multi-band radar data based on fast
sparse bayesian learning algorithm,” IEEE Trans. Antennas Propag.,
vol. 62, no. 12, pp. 6217–6227, 2014.

[15] Y. Liu et al., “Deep learning for pixel-level image fusion: Recent
advances and future prospects,” Information Fusion, vol. 42, pp.
158 – 173, 2018.

Takayuki Nishio is an associate professor at the School of Engineering,
Tokyo Institute of Technology, Japan. He received the B.E. degree in
electrical and electronic engineering and the master’s and Ph.D. de-
grees in informatics from Kyoto University in 2010, 2012, and 2013,
respectively. He was an assistant professor in communications and
computer engineering with the Graduate School of Informatics, Kyoto
University from 2013 to 2020. From 2016 to 2017, he was a visiting
researcher in Wireless Information Network Laboratory (WINLAB), Rut-
gers University, United States. His current research interests include
machine learning-based network control, machine learning in wireless
networks, and heterogeneous resource management.

Yusuke Koda received the B.E. degree in electrical and electronic
engineering from Kyoto University in 2016, and the M.E. degree from
the Graduate School of Informatics, Kyoto University in 2018, where he
is currently pursuing the Ph.D. degree. In 2019, he visited the Centre
for Wireless Communications, University of Oulu, Finland, to conduct
collaborative research. He received the VTS Japan Young Researcher’s
Encouragement Award in 2017. He was a Recipient of the Nokia Foun-
dation Centennial Scholarship in 2019.

Jihong Park is a Lecturer at the School of IT, Deakin University, Aus-
tralia. He received the B.S. and Ph.D. degrees from Yonsei University,
Korea. His research interests include ultra-dense/ultra-reliable/mmWave
system designs, as well as distributed learning/control/ledger technolo-
gies and their applications for beyond-5G/6G communication systems.
He served as a Conference/Workshop Program Committee Member for
IEEE GLOBECOM, ICC, and WCNC, as well as for NeurIPS, ICML,
and IJCAI. He is an Associate Editor of Frontiers in Data Science for
Communications, and a Review Editor of Frontiers in Aerial and Space
Networks.

Mehdi Bennis is an associate professor at the Centre for Wireless
Communications, University of Oulu, Finland. His main research in-
terests are in radio resource management, and machine learning in
5G/6G networks. He has coauthored 1 book and published more than
200 research articles in international conferences, journals, and book
chapters. He was a recipient of the prestigious 2015 Fred W. Ellersick
Prize from the IEEE Communications Society, the 2016 Best Tutorial
Prize from the IEEE Communications Society, the 2017 EURASIP Best
Paper Award for the Journal of Wireless Communications and Networks,
and the all-University of Oulu Research Award.

Fig. 4. Hetero-modal image reconstruction. A missing part in the image
is reconstructed from the received power of RF signals and an image
with failure by deep auto-encoder.

of a real-time digital replica of the physical space around
wireless access points using hetero-modal sensing that com-
bines RGB-D, LiDAR, and RADAR. Such a replica can keep
track of and predict the movement of people and objects
through space. The resulting 3D model can also be utilized
to perform ray tracing simulations to predict RF link quality.
Moreover, RGB-D cameras can capture faces and behaviors
of mobile users, through which their quality of experiences
(QoEs) can be accurately predicted.

REFERENCES

[1]

J. Park et al., “Extreme urllc: Vision, challenges, and key enablers,”
arXiv preprint arXiv:2001.09683, 2020.

[2] R. Nandakumar, V. Iyer, and S. Gollakota, “3D localization for sub-
centimeter sized devices,” in Proc. ACM Conf. Embedded Networked
Sensor Syst., 2018, p. 108–119.

[3] A. Alahi, A. Haque, and L. Fei-Fei, “RGB-W: When vision meets
wireless,” in Proc. IEEE Int. Conf. Comput. Vision (ICCV), Dec. 2015.
S. k. Doddalla and G. C. Trichopoulos, “Non-line of sight tera-
hertz imaging from a single viewpoint,” in Proc. IEEE/MTT-S Int.
Microw. Symp. (IMS), June 2018, pp. 1527–1529.

[4]

[5] T. Nishio et al., “Proactive received power prediction using ma-
chine learning and depth images for mmWave networks,” IEEE J.
Sel. Areas Commun., vol. 37, no. 11, pp. 2413–2427, Nov. 2019.
S. Ayvas¸undeﬁnedk, H. M. G ¨ursu, and W. Kellerer, “Veni vidi
dixi: Reliable wireless communication with depth images,” in Proc.
ACM Int. Conf. Emerg. Netw. Expe. Tech. (CoNEXT)., Dec. 2019, p.
172–185.

[6]

[7] C. Ding and D. Tao, “Robust face recognition via multimodal deep
face representation,” IEEE Trans. Multimedia, vol. 17, no. 11, pp.
2049–2058, 2015.

[8] Y. Koda et al., “Handover management for mmwave networks
with proactive performance prediction using camera images and
deep reinforcement learning,” IEEE Trans. Cogn. Commun. Netw.,
vol. 6, no. 2, pp. 802–816, 2020.

[9] F. Adib and D. Katabi, “See through walls with wiﬁ!” SIGCOMM

Comput. Commun. Rev., vol. 43, no. 4, p. 75–86, Aug. 2013.

[10] H. Iimori et al., “Stochastic learning robust beamforming for
millimeter-wave systems with path blockage,” IEEE Wireless Com-
mun. Lett., pp. 1–1, 2020.

[11] Y. Koda et al., “Communication-efﬁcient multimodal split learning
for mmwave received power prediction,” IEEE Commun. Lett.,
vol. 24, no. 6, pp. 1284–1288, 2020.

InputTarget output:NLOS imageRF signalImg. w/ failureEncoderDecoderDeep auto-encoder1. Scenario2. Image reconstructionCameraRxTxPedestriansGround truthReconstructed fromRF signalsReconstructed fromimage and RFsignals3. ResultsInput imageImage w/ failure(i) Pedestrian facing left is in missing part(ii) Pedestrian facing right is in missing part(iii) Pedestrian at the back was in missing part(iv) There is no pedestrian in missing part0246810121410-310-210-1100CCDFError |RSSdBest-RSSdBest|7.657.77.757.87.857.9Time (s)-40-30-20RFImgRF+ImgGround-truthRSSTimeImgTimemobile blockageRSS (dBm)RGB-D and RSS sequences1Future RSS prediction2ImgRSSKlaus Doppler is the technical lead of the Mirror X project in Nokia Bell
Labs. He has been heading the Indoor Networks Research focusing
on ubiquitous Gigabit connectivity and platforms for smart buildings,
enterprises and factories. In the past, he has been responsible for
wireless research and standardization in Nokia Technologies, incubated

a new business line and pioneered research on D2D Communications
underlaying LTE networks. He received inventor awards in Nokia for
100+ granted patent applications. He has published 40+ scientiﬁc publi-
cations, received his PhD. from Aalto University, Finland in 2010 and his
MSc. from Graz University of Technology, Austria in 2003.

7

