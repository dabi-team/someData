9
1
0
2

n
u
J

1
1

]

V
C
.
s
c
[

2
v
0
6
2
2
0
.
6
0
9
1
:
v
i
X
r
a

Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN
Models for Facial Tracking

TianXing Li ∗†
ModiFace Inc.
tianxingli24@gmail.com

Zhi Yu ∗
ModiFace Inc.
vicky@modiface.com

Edmund Phung
ModiFace Inc.
edmund@modiface.com

Brendan Duke
ModiFace Inc.
brendan@modiface.com

Irina Kezele
ModiFace Inc.
irina@modiface.com

Parham Aarabi
ModiFace Inc.
parham@modiface.com

Abstract

Recent works on convolutional neural networks (CNNs)
for facial alignment have demonstrated unprecedented ac-
curacy on a variety of large, publicly available datasets.
However, the developed models are often both cumbersome
and computationally expensive, and are not adapted to ap-
plications on resource restricted devices. In this work, we
look into developing and training compact facial alignment
models that feature fast inference speed and small deploy-
ment size, making them suitable for applications on the
aforementioned category of devices. Our main contribu-
tion lies in designing such small models while maintain-
ing high accuracy of facial alignment. The models we
propose make use of light CNN architectures adapted to
the facial alignment problem for accurate two-stage pre-
diction of facial landmark coordinates from low-resolution
output heatmaps. We further combine the developed fa-
cial tracker with a rendering method, and build a real-
time makeup try-on demo that runs client-side in smart-
phone Web browsers. We prepared a demo link to our Web
demo that can be tested in Chrome and Firefox on Android,
or in Safari on iOS: https://s3.amazonaws.com/
makeup-paper-demo/index.html12

1. Introduction

Facial alignment is a key component of virtual makeup
try-on methods. These methods require very high accuracy
in facial alignment, as any error in makeup placement
would result in a poor user experience. The alignment
also needs to be robust to variations in lighting, pose, face

shape, and skin tone. It is particularly important that the
landmarks are precisely aligned for frontal and close to
frontal face poses, as those are common in virtual try-on
applications.

In the context of real-time applications on resource
constrained platforms such as mobile and Web, we need
to address the requirements for both low computational
demands and small model size. The latter is of particular
importance for client-side Web applications where long
loading times are not ideal.

The

alignment

state-of-the-art

architectures
facial
[10][19][22] have not been developed for the purpose
real-time inference on resource restricted devices.
of
However, our applications primarily target such devices. To
strike a better balance for real-time applications on those
platforms, the ideal architecture should minimize load and
execution time while preserving alignment accuracy.

Our proposed architecture does the following to meet
its ﬁrst stage makes coarse initial
these requirements:
predictions,
from which crops of shared convolutional
features are taken; these Regions of Interest (RoIs) are then
processed by the second stage to produce more spatially
reﬁned predictions. Besides the coarse-to-ﬁne alignment
approach, we also balance the number of layers and the
resolution of feature maps, and achieve ﬁne-level alignment
with high computational efﬁciency, while maintaining a
small model size. The resulting architecture is suitable for
real-time Web applications in mobile browsers.

2. Related Work
2.1. Facial Landmark Alignment

∗These two authors contributed equally.
†The work performed during internship at ModiFace Inc.
1The link works in listed browsers on corresponding devices, and

should start with “https://”.

2The project website is: http://research.modiface.com/

makeup-try-on-cvprw2019/

The facial landmark alignment problem has a long his-
tory with classical computer vision solutions. For instance,
the fast ensemble tree based [8] algorithm achieves reason-
able accuracy and is widely used for real-time face tracking
[9]. However, the model size required to achieve such ac-

1

 
 
 
 
 
 
curacy is prohibitively large.

Current state-of-the-art accuracy for facial

landmark
alignment are achieved by convolutional neural network
based methods. To maximize accuracy on challenging
datasets [1][11][17], they use large neural networks that are
not real-time, and have model sizes of tens to hundreds of
MB [22][13] that would entail unreasonable load times for
Web applications.
2.2. Efﬁcient CNN Architectures

To bring the performance of convolutional neural net-
works to mobile vision applications, numerous architec-
tures with efﬁcient building blocks such as MobileNetV2
[18], SqueezeNet [7] and ShufﬂeNet [24] have recently
been released. These networks aim to maximize perfor-
mance (e.g., classiﬁcation accuracy) for a given compu-
tational budget, which consists of the number of required
learnable parameters (the model size) and multiply-adds.

We will focus our discussion on the state-of-the-art
MobileNetV2, whose inverted residual blocks are used in
our own design. Their choice of depthwise convolutions
over regular convolutions drastically reduces the number of
multiply-adds and learnable parameters, at a slight cost in
performance [18]. Furthermore, the inverted design, which
is based upon the principle that network expressiveness can
be separated from capacity, allows for a large reduction in
the number of cross-channel computations within the net-
work [18].

Finally, the residual design similar to ResNet [6] eases

issues with gradient propagation in deeper networks.
2.3. Heatmap

Fully convolutional neural network architectures based
on heatmap regression [2][20][3][14] have been widely
used on human pose estimation tasks. The use of heatmaps
provides a high degree of accuracy, along with an intu-
itive means of seeing the network’s understanding and con-
ﬁdence of landmark regression. This technique has also
been used in recent facial alignment algorithms such as the
Stacked Hourglass architecture [22]. However, the Stacked
Hourglass approach [22] uses high resolution heatmaps,
which require a large amount of computation in the de-
coding layers. There is room for optimization here, as the
heatmaps only have non-negligible values in a very concen-
trated and small portion of the overall image. This observa-
tion motivates us to use regional processing, which allows
for the network to focus its processing in relevant areas.
(i.e., the approximate Rregion of Interest).
2.4. Mask RCNN

There are a series of frameworks which are ﬂexible and
robust to object detection and semantic segmentation like
Fast R-CNN [4], Faster R-CNN [15] and Fully Convo-
lutional Network [12]. Faster R-CNN proposes a multi-
branch design to perform bounding box regression and clas-
siﬁcation in parallel. Mask-RCNN [5] is an extension of

Faster-RCNN, and adds a new branch for predicting seg-
mentation masks based on each Region of Interest. Of par-
ticular interest is Mask-RCNN’s use of RoI align [5], which
allows for signiﬁcant savings in computation time by taking
crops from shared convolutional features. By doing this, it
avoids recomputing features for overlapping Regions of In-
terest.
3. Proposed Method

There are two main contributions of our model:
1. We use RoI align [5] for each individual landmark to
save potentially overlapping computation, allow the
network to avoid non-relevant regions, and force the
network to learn to produce useful shared features.
2. Our two-stage localization architecture along with
auxiliary coordinate regression loss allow us to work
with extremely small and computationally cheap
heatmaps at both stages.

3.1. Model Structure

The model has two-stages and is trained end-to-end, as

illustrated in Figure 1.

Figure 1. Our two-stages Network Architecture.

The ﬁrst stage is formed by a list of Inverted Resid-
ual Blocks [18], which predict H × H heatmaps, one for
each facial landmark.
Interpreting the normalized activa-
tions over the heatmaps as a probability distribution, we
compute the expected values of these heatmaps to obtain
the x, y coordinates. This will be described in more detail
in the following subsection.

The second stage has several shared convolutional lay-
ers, which branch off from part of the ﬁrst stage. Using the
coarse predictions from the previous stage, we apply RoI
align [5] to the ﬁnal shared convolutional features. Each
of the cropped features are input to one ﬁnal convolutional
layer, which has separate weights for each individual land-
mark. Our predict block implements this efﬁciently with
the use of group convolutions [24]. The ﬁnal output is a
heatmap for each landmark. The coordinates obtained from
these heatmaps indicate the required offset from the initial
coarse prediction, i.e., if the heatmap at this stage is per-
fectly centered, then there is effectively no reﬁnement ap-
plied to the ﬁrst stage prediction.

3.2. Coordinate Regression from Heatmaps

For our ground truth heatmaps, we use a Gaussian dis-
tribution with a mode corresponding to the ground truth co-
ordinates’ positions. Letting x, y denote the coordinates of
any pixel in the feature map, we can compute its value us-
ing a 2D Gaussian distribution with the corresponding land-
mark coordinate as center.

The regressed xpred, ypred is then the expected value
of the pixel locations according to the distribution com-
puted from the heatmap. Let i, j denote the coordinates in
the heatmap, and ρij denote the corresponding probability
value:

(cid:2)xpred, ypred

(cid:3) =

ρij

(cid:2)xi, yj

(cid:3)

(1)

W
(cid:88)

H
(cid:88)

i=1

j=1

3.3. Loss Function

Lh =

1
N

We apply a pixelwise sigmoid cross entropy [23] to learn
the heatmaps, which we denote as Lh. Additionally, in or-
der to alleviate issues with the heatmaps being cut off for
landmarks near boundaries, we add on an L2 distance loss
with a loss weight λ: Ltotal = Lh + λ · L2.
L
(cid:88)

N
(cid:88)

W
(cid:88)

H
(cid:88)

n=1

l=1

i=1

j=1

(2)

[ρl

ijlog ˆρl

ij + (1 − ρl

ij)log(1 − ˆρl

ij)] · wl

ij

(3)

wl

ij =

l )2) · 2

((in − ˆin

l )2 + (jn − ˆjn
W 2 + H 2
ij is the prediction value of the heatmap in the lth
ij is
ij is the weight at that lo-
l ) is the

Where ρl
channel at pixel location (i, j) of n’s sample, while ˆρl
the corresponding ground truth. wl
cation, which is calculated from Equation 3. ( ˆin
ground truth coordinate of the n’s sample’s lth landmark.
3.4. Data

l , ˆjn

Our dataset consists of a union of Helen [11], LFPW
[1] and iBUG [17] datasets and additional images that we
collected in-house, with 3681 images in total. The image
annotation follows Helen annotation format [11], but with
corrected original annotations and with an addition of a cou-
ple of new eyebrow landmarks. The contour annotation is
sparse. This dataset has a total of 62 inner points and 3 con-
tour points as shown in Figure 2. The pose distribution is
approximately -30◦to 30◦in yaw angle, which agrees well
with our application scenario. For the purpose of compari-
son with state-of-the-art methods, we further train and test
our model design on the 300W dataset [16].

4. Results and Ablation Study

The reported error is the commonly used mean squared
error normalized by the inter-pupil distance. Table 1 shows
the full model error (of the two-stage model trained with the
heatmap loss, combined with L2 loss) in the last row, and
the results when only L2 loss (ﬁrst row), or only heatmap
loss (second row), or only one CNN stage (third row) are
used. This ablation study clearly shows that all three tested

components: heatmap loss, L2 loss, and extending the ar-
chitecture with a second processing stage, bear individual
importance and improves the model accuracy.

The tests on 300W dataset and comparisons with Look
at Boundary (LAB) [21], a state-of-the-art method for ac-
curate facial alignment, resulted in the following (Table
2): our full-size model of 6.6MB achieves similar accu-
racy as LAB method on 300W Common Subset, while ex-
hibiting some drop in accuracy on the 300W Full dataset,
which can be explained by our architectural design being
adapted more towards landmark detection on unoccluded
and mostly frontal faces. In addition, “half α” (α = 0.5),
which halves the number of channels per layer, preserves
the accuracy when tested on the Common Subset. Finally,
we show that a further reduction in model size and input im-
age resolution does not signiﬁcantly degrade the accuracy,
as shown in Table 3, while attaining much faster computa-
tional speeds and overall a smaller model size.

Method
Without heatmap loss
Without L2 loss
Without second stage
Our full model

Inner Error Contour Error

3.53
4.45
3.50
3.21

9.01
12.3
9.32
9.00

Table 1. Results on our dataset with 65 points.

Method

LAB (4-stack)[21]
LAB (8-stack)[21]
Our model (α = 1)
Our model (α = 0.5)

Common
Subset
4.20
3.42
3.42
3.43

Challenging
Subset
7.41
6.98
8.51
14.7

Fullset

4.92
4.12
4.42
5.64

Table 2. Results on 300W dataset.

Total params
Total MAdd
Total Flops
Model Size
Inference Time
(iPhone XR)

158,091
173.69M
90.75M
607KB

20ms

Figure 2. The
of 65 landmarks.
5. Rendering

annotation

Table 3. Our demo model, results
in Table 1 last row.

Following landmark prediction by our CNN model, we
deﬁne facial parts (e.g., lips) and render makeup on them.
Figure 3 depicts the full pipeline, included rendering mod-
ules.

Figure 3. The Rendering Pipeline.

multi person pose estimation.
Computer Vision and Pattern Recognition, 2016. 2

In the IEEE Conference on

[15] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, 2015. 2

[16] Christos Sagonas, Georgios Tzimiropoulos, Stefanos
Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge:
The ﬁrst facial landmark localization challenge. In the IEEE
International Conference on Computer Vision Workshops,
2013. 3

[17] Christos Sagonas, Georgios Tzimiropoulos, Stefanos
Zafeiriou, and Maja Pantic. A semi-automatic methodology
for facial landmark annotation. In the IEEE conference on
computer vision and pattern recognition workshops, 2013.
2, 3

[18] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In the IEEE Conference on
Computer Vision and Pattern Recognition, 2018. 2

[19] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convo-
In the
lutional network cascade for facial point detection.
IEEE conference on computer vision and pattern recogni-
tion, 2013. 1

[20] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser
Sheikh. Convolutional pose machines. In the IEEE confer-
ence on computer vision and pattern recognition workshops,
2016. 2

[21] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai,
and Qiang Zhou. Look at boundary: A boundary-aware face
alignment algorithm. In the IEEE Conference on Computer
Vision and Pattern Recognition, 2018. 3

[22] Kevan Yuen and Mohan M Trivedi. An occluded stacked
hourglass approach to facial landmark localization and oc-
clusion estimation. IEEE Transactions on Intelligent Vehi-
cles, 2017. 1, 2

[23] Ning Zhang, Evan Shelhamer, Yang Gao, and Trevor Dar-
rell. Fine-grained pose prediction, normalization, and recog-
nition. arXiv preprint arXiv:1511.07063, 2015. 3

[24] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2018. 2

6. Acknowledge

We thank Brittany Wanka, Rena Lu and Sarah Koehler
for assistance with cleaning and annotating data. We
would also like thank Irene Jiang for comments and dis-
cussions.

References

[1] Peter N. Belhumeur, David W. Jacobs, David J. Kriegman,
and Neeraj Kumar. Localizing parts of faces using a consen-
sus of exemplars. In The 24th IEEE Conference on Computer
Vision and Pattern Recognition, 2011. 2, 3

[2] Adrian Bulat and Georgios Tzimiropoulos. Human pose es-
timation via convolutional part heatmap regression. In Euro-
pean Conference on Computer Vision(ECCV), 2016. 2
[3] Yu Chen, Chunhua Shen, Hao Chen, Xiu-Shen Wei,
Lingqiao Liu, and Jian Yang. Adversarial
learning of
structure-aware fully convolutional networks for landmark
localization. IEEE transactions on pattern analysis and ma-
chine intelligence, 2019. 2

[4] Ross Girshick. Fast r-cnn. In the IEEE international confer-

ence on computer vision, 2015. 2

[5] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In the IEEE international conference on
computer vision, 2017. 2

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In the
IEEE conference on computer vision and pattern recognition
workshops, 2016. 2

[7] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally,
and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parame-
ters and¡ 0.5 mb model size. 2016. 2

[8] Vahid Kazemi and Josephine Sullivan. One millisecond
face alignment with an ensemble of regression trees. In the
IEEE conference on computer vision and pattern recognition
workshops, 2014. 1

[9] Davis E King. Dlib-ml: A machine learning toolkit. Journal

of Machine Learning Research, 2009. 1

[10] Marek Kowalski, Jacek Naruniec, and Tomasz Trzcinski.
Deep alignment network: A convolutional neural network
for robust face alignment. In the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, 2017. 1

[11] Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Bourdev, and
Thomas S Huang. Interactive facial feature localization. In
European conference on computer vision, 2012. 2, 3
[12] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In the
convolutional networks for semantic segmentation.
IEEE conference on computer vision and pattern recogni-
tion, 2015. 2

[13] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In European Con-
ference on Computer Vision, 2016. 2

[14] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern
Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt
Schiele. Deepcut: Joint subset partition and labeling for

