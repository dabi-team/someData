0
2
0
2

t
c
O
2

]

R

I
.
s
c
[

1
v
4
8
9
0
0
.
0
1
0
2
:
v
i
X
r
a

An Empirical Study of DNNs Robustification Inefficacy in
Protecting Visual Recommenders

Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio Merraâˆ—
{vitowalter.anelli,tommaso.dinoia,daniele.malitesta,felice.merra}@poliba.it
Polytechnic University of Bari
Bari, Italy

ABSTRACT
Visual-based recommender systems (VRSs) enhance recommen-
dation performance by integrating usersâ€™ feedback with the vi-
sual features of product images extracted from a deep neural net-
work (DNN). Recently, human-imperceptible images perturbations,
defined adversarial attacks, have been demonstrated to alter the
VRSs recommendation performance, e.g., pushing/nuking category
of products. However, since adversarial training techniques have
proven to successfully robustify DNNs in preserving classification
accuracy, to the best of our knowledge, two important questions
have not been investigated yet: 1) How well can these defensive
mechanisms protect the VRSs performance? 2) What are the reasons
behind ineffective/effective defenses? To answer these questions,
we define a set of defense and attack settings, as well as recom-
mender models, to empirically investigate the efficacy of defensive
mechanisms. The results indicate alarming risks in protecting a
VRS through the DNN robustification. Our experiments shed light
on the importance of visual features in very effective attack sce-
narios. Given the financial impact of VRSs on many companies,
we believe this work might rise the need to investigate how to
successfully protect visual-based recommenders. Source code and
data are available at https://anonymous.4open.science/r/868f87ca-
c8a4-41ba-9af9-20c41de33029/.

1 INTRODUCTION
Recommender Systems (RSs) have terrifically taken over online
shopping by providing personalized recommendations to users in
the flood of products of e-commerce platforms. Catching a large
number of historical interactions, RSs learn what each user might
like, and show short ranked lists of the presumably desired prod-
ucts. In domains such as fashion, food, and point-of-interest rec-
ommendations, images are associated with items to get customersâ€™
attention. Visual-based Recommender Systems (VRSs) are the cor-
nerstone of recommender models to learn usersâ€™ preferences by mix-
ing past interactions with high-level visual features extracted from
those item photos. The intuition behind this class of recommenders
is that usersâ€™ preference is influenced by the observable style of
product images. Thanks to the power of Deep Neural Networks
(DNNs) in capturing high-level visual aspects, the state-of-the-art
of VRSs incorporates deep visual features extracted from Convo-
lutional Neural Networks (CNNs). For instance, He et al. [17, 18]
proposed one of the first models, named VBPR, to integrate vi-
sual features getting outperforming recommendation performance
compared to the basic version of the recommender (BPR-MF [32]).

âˆ—The authors are in alphabetical order. Corresponding author: Felice Antonio Merra
(felice.merra@poliba.it), Daniele Malitesta (daniele.malitesta@poliba.it).

While different variants of VRSs have been proposed in the last
years â€”taking the quality of DNNs-extracted features for granted
â€” the work by Szegedy et al. [35] raised security concerns as they
showed that a malicious person, an adversary, may lead the net-
work to misclassify an image corrupted by a human-imperceptible
adversarial perturbation. Starting from Szegedyâ€™s publication, dif-
ferent adversarial strategies (e.g., FGSM [13], PGD [25], and Carlini
& Wagner [7]) have demonstrated stronger and stronger attack
power. In parallel, a complementary branch of research has been
devoted to building robust DNNs (e.g., Adversarial Training [13],
Free Adversarial Training [34]). Consequently, the term Adversar-
ial Machine Learning (AML) currently denotes the study of such
attacks and defenses.

Motivated by the attacks abilities and the partial protection of the
state-of-the-art defense strategies, we identify VRSs as the category
of RSs most at risk. Indeed, while AML techniques have been widely
investigated in various domains (e.g., object detection [30], mal-
ware detection [40], speech recognition [20], graph [11]), studies
in recommendation scenarios have been conducted only recently.
For instance, He et al. [19] demonstrated the weakness of matrix
factorization recommenders with respect to adversarial perturba-
tions on model embeddings and proposed an adversarial training
procedure to make the system robust. Similarly, Tang et al. [36]
verified the efficacy of adversarial training in protecting VBPR (i.e.,
a VRS) from perturbations on image features. Moreover, Di Noia
et al. [10] proved that targeted adversarial attacks, i.e., FGSM and
PGD, applied directly to input images (and not their features) can
disturb the recommendation performance.

Differently from the previous works, in this work we propose
an empirical framework, Visual Adversarial Recommendation (VAR),
to investigate whether state-of-the-art defense strategies applied
to robustify the Image Feature Extractor (IFE) component of a VRS
are capable to mitigate the effect of up-to-date adversarial attack
strategies, i.e., FGSM, PGD, and even Carlini & Wagner [1, 6]. The
motivational scenario involves a competitor willing to increase the
recommendability of a category of products on an e-commerce
platform (e.g., sandals) by simply uploading adversarially perturbed
product images that are misclassified by the IFE as a much more
popular class (e.g., running shoes).

The main contributions of this work are summarized as follows:

â€¢ Study the efficacy of IFE defense approaches in protecting
the recommender through the analysis of 54 combinations
of defenses, attacks, and recommendation approached on
three real-world datasets.

â€¢ Joint evaluation of the alteration of visual recommendation
and features extraction performance, with a particular focus
on the variation of feature loss on perturbed images.

 
 
 
 
 
 
Conferenceâ€™20, October 2020, Virtual-

Anelli, Di Noia, Malitesta, and Merra

â€¢ Propose a novel rank-based evaluation metric, named Cat-
egory normalized Discounted Cumulative Gain, to deeply
explore the efficacy of defenses (or the effects of attacks).
â€¢ Analyze the variation of global and beyond-accuracy recom-
mendation performance with (and without) defenses applied
under the most powerful attack scenarios.

The rest of the paper is organized as follows. First, we review related
work in Section 2. Then, we present the experimental framework
in Section 3. In Sections 4 and 5 we introduce the experimental
setups and present and discuss the empirical results. Finally, we
draw conclusion and raise open directions in Section 6.

2 RELATED WORK
Adversarial Machine Learning. ML models have demonstrated
vulnerabilities to adversarial attacks [3, 35], i.e., specifically created
data samples able to mislead the model despite being highly similar
to their clean version. Particularly, great research effort has been
put into finding the minimum visual perturbation to attack images
to fool CNN classifiers. Szegedy et al. [35] formalized the adver-
sarial generation problem by solving a box-constrained L-BFGS.
Goodfellow et al. proposed Fast Gradient Sign Method (FGSM) [13],
a simple one-shot attack method that uses the sign of the gradient
of the loss function. Basic Iterative Method (BIM) [12] and Projected
Gradient Descent (PGD) [25] re-adapted FGSM to create stronger
attacks by iteratively updating the adversarial perturbation. Carlini
and Wagner [7] improved the problem definition presented in [35]
and built attacks powerful in deceiving several detection strate-
gies [6]. Along with the proposed attacks, many solutions have
also been provided regarding defense. Adversarial Training [13]
creates new adversarial samples at training time, making the model
more robust to such perturbed inputs. Defensive Distillation [29]
transfers knowledge between two networks to reduce the resilience
to adversarial samples, but was proven not to be as secure as ex-
pected against C & W attacks [5]. Free Adversarial Training [34]
truly eases the computational complexity of standard adversarial
training without giving up its effectiveness.

Security of Visual-based Recommender Systems. In this
work, the recommendation component is a visual-based recom-
mender model. Different works have demonstrated that the inte-
gration of image features in userâ€™s preference predictor leads to
outperforming both recommendation [17, 18, 26] and search [22, 39]
tasks. The intuition is that the visual appearance of product im-
ages influences customerâ€™s decisions (e.g., a customer who loves
red colors will likely buy red clothes) [14]. For instance, He et
al. [18] extended BPR-MF [32] by integrating high-level features
extracted from a pre-trained CNN, and Wang et al. [21] used im-
age features to predict complementary fashion articles. Chu et al.,
and Wang et al. [8, 38] demonstrated significantly improvements
in POI-recommendations when considering food images features.
Recently, Zhang and Caverlee [41] proposed a novel VRS showing
that dynamic visual features based on fashion blogger posts bring
improvements in fashion recommendations.

However, recommender models have been demonstrated to be
steadily under security risks. The security of RSs relates to the
study of different hand-engineered strategies to generate shilling

profiles which lead to the alteration of collaborative recommenda-
tions [24], and their defense mechanisms (e.g., detection [2] and
robustness [27]). On the other hand, the application of AML in
RSs [9] differs from previous works in the use of optimized pertur-
bations, and their respective defenses, that lead to drastic perfor-
mance reduction [19, 36, 37]. For example, He et al. [19] proposed
an adversarial training procedure to make the model robust to such
perturbations. Furthermore, Tang et al. [36] applied this defense to
make the proposed VRS (i.e., AMR) more robust to adversarial per-
turbations on image features. However, Di Noia et al. [10] noticed
the partial protection of VBPR and AMR against targeted adver-
sarial attacks on product images. Differently from these works, we
empirically verified DNN robustification strategies are not always
able to protect the recommender models against strong adversarial
attacks (e.g., C & W on all) when they are used to robustify the IFE.

3 THE PROPOSED FRAMEWORK
In this section, we describe the proposed Visual Adversarial Rec-
ommendation (VAR) experimental framework. First, we define some
preliminary concepts. Then, we provide an overview on all VAR com-
ponents. Finally, we present the evaluation measures to quantify
the effectiveness of the adversarial defenses under attacks.

3.1 Preliminaries
We introduce some notions and notation to formalize VAR.

Recommendation Task. We define the set of users, items and
0/1-valued preference feedback as U, I, and S, where |U|, |I|,
and |S| are the set sizes respectively. We reserve the use of ğ‘¢, ğ‘–,
and ğ‘ ğ‘¢ğ‘– , to indicate a user in U, an item in I and the feedback (e.g.,
a review) given by ğ‘¢ to ğ‘– saved in S. Furthermore, we define the
recommendation task as the action to suggest items that maximize,
for each user, a utility function. We indicate with Ë†ğ‘ ğ‘¢ğ‘– the predicted
score learned from the RS upon historical preferences, represented
as a user-item preference-feedback matrix (UPM).

Deep Neural Network. Given a set of data samples (ğ‘¥ğ‘–, ğ‘¦ğ‘– ),
where ğ‘¥ğ‘– is the ğ‘–-th image and ğ‘¦ğ‘– is the one-hot encoded representa-
tion of ğ‘¥ğ‘– â€™s image category, we define ğ¹ as a DNN classifier function
trained on all (ğ‘¥ğ‘–, ğ‘¦ğ‘– ). Then, we set ğ¹ (ğ‘¥ğ‘– ) = Ë†ğ‘¦ğ‘– as the predicted prob-
ability vector of ğ‘¥ğ‘– belonging to each of all the admissible classes,
and we calculate its predicted class as the index of Ë†ğ‘¦ğ‘– with maximum
probability value, and represent it as ğ¹ğ‘ (ğ‘¥ğ‘– ). Moreover, assuming
an ğ¿-layers DNN classifier, we indicate with ğ¹ (ğ‘™) (ğ‘¥ğ‘– ) the output of
the ğ‘™-th layer of ğ¹ given the input ğ‘¥ğ‘– .

Adversarial Attack and Defense. We define an Adversarial
attack as the problem of finding the best value for a perturbation
ğ›¿ğ‘– such that:

(1)

minimize d (ğ‘¥ğ‘–, ğ‘¥ğ‘– + ğ›¿ğ‘– )
ğ¹ğ‘ (ğ‘¥ğ‘– + ğ›¿ğ‘– ) â‰  ğ¹ğ‘ (ğ‘¥ğ‘– )
ğ‘¥ğ‘– + ğ›¿ğ‘– âˆˆ [0, 1]ğ‘›
where d (Â·) is a distance metric function (e.g., ğ¿0, ğ¿2 and ğ¿âˆ norms).
The above definition states that (i) the attacked image ğ‘¥ âˆ—
ğ‘– = ğ‘¥ğ‘– + ğ›¿ğ‘–
must be visually similar to ğ‘¥ğ‘– , (ii) the predicted class for ğ‘¥ âˆ—
ğ‘– must
be different from the original one and (iii) ğ‘¥ âˆ—
ğ‘– must stay within its
original value range (i.e., [0, 1] for 8-bit RGB images re-scaled by
a factor 255). When ğ¹ğ‘ (ğ‘¥ âˆ—
ğ‘– ) is required to be generically different
from ğ¹ğ‘ (ğ‘¥ğ‘– ), we say the attack is untargeted. On the contrary, when

An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders

Conferenceâ€™20, October 2020, Virtual-

Figure 1: Overview of the VAR framework for the evaluation of adversarial attacks and defenses effects on a VRS. The adver-
sary can perturb a product image, the Image Feature Extractor (IFE) extract the image visual features, and the Visual-based
Recommender System (VRS) gets in input the user-item preference matrix (UPM) and the features to compute the top-ğ¾ lists.

ğ¹ğ‘ (ğ‘¥ âˆ—
ğ‘– ) is specifically required to be equal to a target class ğ‘¡, we say
the attack is targeted. Finally, we define a Defense as the problem
to find ways of limiting the impact of adversarial attacks against a
DNN. For instance, a common solution consists of training a more
robust version of the original model function â€”we will refer to it
as (cid:101)ğ¹ â€” which attempts to classify attacked images correctly.

3.2 Empirical Framework
After the definition of preliminaries, we discuss the VAR compo-
nents. Figure 1 shows an overview on the three main elements: the
adversary (i.e., a malicious user), the image feature extractor (IFE),
and the visual-based recommender system (VRS). First, we describe
the main characteristics of each mentioned component. Then, we
introduce a novel set of metrics to evaluate the recommendation
performance at varying of the adversary, the IFE, and the VRS.

Adversary. To align with the AML literature, we follow the
attack â€”and defenseâ€” adversary threat model outlined in Carlini
et al. [4]. The adversaryâ€™s goal is to attack the IFE so that images
of low-ranked categories of products are incorrectly classified as
the category of high-ranked ones. That is, the former will likely
be recommended more (on average) than before. To this malicious
purpose, the adversary is aware of all recommendation lists (used
to choose the source and target categories), and she has perfect
knowledge of the IFE, i.e., its architecture, its trainable weights,
and its output (white-box attack). Additionally, we suppose the
adversary can perform ğ¿âˆ (i.e., FGSM and PGD), and ğ¿2 (i.e., Carlini
& Wagner) attacks (see section 4.2 for further details). Finally, in
our motivating scenario, the adversary can replace the original
images on the physical servers of the e-commerce platform with
the attacked one, which will be used by the VRS to produce the
recommendation lists.

Image Feature Extractor (IFE). The input sample ğ‘¥ğ‘– represents
the photo associated with the ğ‘–-th product in the set of items I,
which may appear in the top-ğ¾ recommendation list shown to an e-
commerce platform customer. Hence, the IFE is a DNN pre-trained
classifier to extract high-level visual features from ğ‘¥ğ‘– . The actual
extraction takes place at one of the last layers of the network, i.e.,
ğ¹ (ğ‘’) (ğ‘¥ğ‘– ), where ğ‘’ refers to the extraction layer. We define this layer
output ğ¹ (ğ‘’) (ğ‘¥ğ‘– ) = ğœ‘ğ‘– as a ğ›¾-dimensional vector that will be the
input of the VRS. In the case of defense, the IFE model function is
replaced by (cid:101)ğ¹ since the defense strategy is applied to the pre-trained
traditional model previously indicated as ğ¹ . Note that the IFE is a
key component in VAR since it represents the connection between
the adversary â€”the author of the attackâ€” and the VRS.

Visual-based Recommender System (VRS). In the VAR frame-
work, the VRS is the component aimed at addressing the recom-
mendation task. The model accepts two inputs: (i) the historical
UPM, and (ii) the set of features of item images extracted from the
IFE component. Hence, it produces recommendation lists sorted by
the preference prediction score evaluated for each user-item pair
without previous interactions. Indeed, the VRS preference predictor
takes advantage of the pure collaborative filtering source of data
(i.e., the UPM) especially when integrated with the high-level mul-
timedia features since they unveil the visual aspects that arouse
customerâ€™s preferences [18]. In the VAR motivating example, the
VRS is the final victim of the malicious user. For this reason, we
focus our analysis on its variation performance and propose novel
measures to investigate how much it is influenced by different
settings of both the adversary and the IFE.

3.3 Evaluation
To answer the research questions proposed in Section 1, we need
to perform three levels of investigation on (i) the effectiveness of

TRADITIONALDEFENDEDResNet50ResNet50IMAGE FEATUREEXTRACTORAttacker Knowledge about IFEVISUAL RECOMMENDER SYSTEMADVERSARYUser-item Preference Matrix UPMAdversarial PerturbationItemFeaturesPREFERENCE PREDICTORTOP-5 RECOMMENDATIONS OF+1st2nd3rd4th5thConferenceâ€™20, October 2020, Virtual-

Anelli, Di Noia, Malitesta, and Merra

adversarial attacks in misusing the classification performance of the
DNN used to implement the IFE, (ii) the variation of the accuracyâ€”
and beyond-accuracyâ€” recommendation performance, and (iii) the
evaluation of the consequences of attack and defense mechanisms
on the recommendability of the attacked category of products.

In AML, several publications focused on quantifying adversarial
attacks success in corrupting the classification performance of a
target classifier (i.e., the attack Success Rate (ğ‘†ğ‘…) [7]). Similarly,
there is a vast literature about accuracy and beyond accuracy of
RSs [33] recommendation metrics. On the other hand, we have
observed a lack of literature in evaluating adversarial attacks on
RSs content data. As a matter of facts, Tang et al. [36] evaluate the
effects of untargeted attacks on classical system accuracy metrics,
i.e., Hit Ratio (ğ»ğ‘…) and normalized Discounted Cumulative Gain
(ğ‘›ğ·ğ¶ğº), while Di Noia et al. [10] propose a modified version of
ğ»ğ‘… to evaluate the fraction of adversarially perturbed items in the
top-ğ¾ recommendations. To fill this evaluation metrics gap, we
redefined the Category Hit Ratio (ğ¶ğ»ğ‘…@ğ¾) [10] and formalized the
normalized Category Discounted Cumulative Gain (ğ‘›ğ¶ğ·ğ¶ğº@ğ¾).
Definition 1 (Category Hit Ratio). Let C be the set of the
classes extracted from the IFE, let Iğ‘ = {ğ‘– âˆˆ I, ğ‘ âˆˆ C|ğ¹ğ‘ (ğ‘¥ğ‘– ) = ğ‘} be
the set of items whose images are classified by the IFE in the ğ‘-class
(i.e., the category of low recommended items), we define categorical
hit (chit) as:

ğ‘â„ğ‘–ğ‘¡ (ğ‘¢, ğ‘˜) =

(cid:40)1,
0,

if k-th item âˆˆ Iğ‘
if k-th item âˆ‰ Iğ‘

(2)

where categorical hit (ğ‘â„ğ‘–ğ‘¡ (ğ‘¢, ğ‘˜)) is a 0/1-valued function that is 1
when the item in the ğ‘˜-th position of the top-ğ¾ recommendation
list of the user ğ‘¢ is in the set of attacked items not-interacted by ğ‘¢.
Consequently, we define the ğ¶ğ»ğ‘…@ğ¾ as follows:

ğ¶ğ»ğ‘…ğ‘¢ @ğ¾ =

1
ğ¾

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ‘â„ğ‘–ğ‘¡ (ğ‘¢, ğ‘˜)

(3)

Since Category Hit Ratio does not pay attention to the ranking
of recommended items, we propose a novel rank-wise positional
metric, named Category normalized Discounted Cumulative
Gain, that assigns a gain to each considered ranking position. By
considering a relevance threshold ğœ, we assume that each item
ğ‘– âˆˆ Iğ‘ has an ideal relevance value of:

ğ‘–ğ‘‘ğ‘’ğ‘ğ‘™ğ‘Ÿğ‘’ğ‘™ (ğ‘–) = 2(ğ‘ ğ‘šğ‘ğ‘¥ âˆ’ğœ+1) âˆ’ 1
where ğ‘ ğ‘šğ‘ğ‘¥ is the maximum possible score for items. By considering
a recommendation list provided to user ğ‘¢, we define the relevance
(ğ‘Ÿğ‘’ğ‘™ (Â·)) of a suggested item ğ‘– as:

(4)

ğ‘Ÿğ‘’ğ‘™ (ğ‘˜) =

(cid:40)2(ğ‘ ğ‘¢ğ‘– âˆ’ğœ+1) âˆ’ 1,
0,

if k-th item âˆˆ Iğ‘
if k-th item âˆ‰ Iğ‘

(5)

where ğ‘˜ is the position of the item ğ‘– in the recommendation list.
In Information Retrieval, the Discounted Cumulative Gain (ğ·ğ¶ğº)
is a metric of ranking quality that measures the usefulness of a
document based on its relevance and its position in the result list.
Analogously, we define Category Discounted Cumulative Gain
(ğ¶ğ·ğ¶ğº) as:

ğ¶ğ·ğ¶ğºğ‘¢ @ğ¾ =

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ‘Ÿğ‘’ğ‘™ (ğ‘˜)
log2 (1 + ğ‘˜)

(6)

Since recommendation results may vary in length depending on
the user, it is not possible to compare performance among different
users, so the cumulative gain at each position should be normalized
across users. In this respect, we define the Ideal Category Discounted
Cumulative Gain (ğ¼ğ¶ğ·ğ¶ğº@ğ¾) as follows:

ğ¼ğ¶ğ·ğ¶ğº@ğ¾ =

ğ‘šğ‘–ğ‘› (ğ¾, |Iğ‘ |)
âˆ‘ï¸

ğ‘Ÿğ‘’ğ‘™ (ğ‘˜)
log2 (1 + ğ‘˜)

(7)

ğ‘˜=1
In practical terms, ğ¼ğ¶ğ·ğ¶ğº@ğ‘ indicates the score obtained by an
ideal recommendation list that contains only relevant items.

Definition 2 (normalized Category Discounted Cumula-
tive Gain). Let C be the set of the classes extracted from the IFE,
let Iğ‘ = {ğ‘– âˆˆ I, ğ‘ âˆˆ C|ğ¹ğ‘ (ğ‘¥ğ‘– ) = ğ‘} be the set of items whose images
are classified by the IFE in the ğ‘-class (i.e., the category of low recom-
mended items). Let ğ‘Ÿğ‘’ğ‘™ (ğ‘˜) be a function computing the relevance of
the ğ‘˜-th item of the top-ğ¾ recommendation list, and ğ¼ğ¶ğ·ğ¶ğº@ğ¾ be
the ğ¶ğ·ğ¶ğº for an ideal recommendation list only composed of relevant
items. We define the normalized Category Discounted Cumulative
Gain (ğ‘›ğ¶ğ·ğ¶ğº), as:

ğ‘›ğ¶ğ·ğ¶ğºğ‘¢ @ğ¾ =

1
ğ¼ğ¶ğ·ğ¶ğº@ğ¾

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ‘Ÿğ‘’ğ‘™ (ğ‘˜)
log2 (1 + ğ‘˜)

(8)

The ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ is ranged in a [0, 1] interval, where values close
to 1 mean that the attacked items are recommended in higher
positions (e.g., the attack is effective). In Information Retrieval, a
logarithm with base 2 is commonly adopted to ensure that all the
recommendation list positions are discounted.

4 EXPERIMENTAL SETUP
In this section, we first introduce the three real-world datasets,
the adversarial attack strategies, the defense methods to make the
IFE more robust, and the VRSs. Conclusively, we present the com-
plete set of evaluation measures and a detailed presentation of the
experimental choices to make the results reproducible.

4.1 Datasets
Amazon Men & Amazon Women [17] are two datasets about
menâ€™s and womenâ€™s products belonging to Amazon category "Cloth-
ing, Shoes and Jewelry". They come with both usersâ€™ ratings and
item images. Since we consider an implicit feedback setting, we
transformed each userâ€™s rating into an implicit 0/1-feedback.

Tradesy [18] dataset contains implicit feedback (i.e., purchase
histories, and desired products) extracted from the second-hand
selling social platform of the same name. We followed the same
pre-processing procedure seen for Amazon Men and Amazon Women.
Moreover, to reduce the degrading effects of cold-users and items
on recommendation performance, we applied different k-core set-
tings [17]. In particular, we chose different k values to explore
various density dataset characteristic settings. Table 1 shows the
dataset statistics as a result of the pre-processing steps described
above. The datasets are available on the code repository web page.

4.2 Adversarial Attacks and Defenses
In this section we present all the adversarial attack and defense
techniques adopted in the experimental phase.

An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders

Conferenceâ€™20, October 2020, Virtual-

Table 1: Dataset statistics.

Dataset
Amazon Men
Amazon Women
Tradesy

k-cores
5
10
10

|U|
24, 379
16, 668
6, 253

|I|
7, 371
2, 981
1, 670

|S|
89, 020
54, 473
21, 533

density
0.000495
0.001096
0.002062

4.2.1 Attacks. We explored three state-of-the-art adversarial at-
tacks against images.

Fast Gradient Sign Method (FGSM) [13] is an ğ¿âˆ-norm opti-
mized attack that produces an adversarial version of a given image
in just one evaluation step. A perturbation budget ğœ– is set to modify
the strength â€”and consequently, the visual perceptibilityâ€” of the
attack, i.e., higher ğœ– values mean stronger attacks but also more
evident visual artifacts.

Projected Gradient Descent (PGD) [25] is an ğ¿âˆ-norm op-
timized attack that takes a uniform random noise as the initial
perturbation, and iteratively applies an FGSM attack with a contin-
uously updated small perturbation ğ›¼ â€”clipped within the ğœ–-ballâ€”
until either it effectively reaches the network misclassification (i.e.,
ğ¹ğ‘ (ğ‘¥ğ‘– + ğ›¼ğ‘– ) = ğ‘¡) or it completes the number of possible iterations
(i.e., 10 iterations in our evaluation setting).

Carlini and Wagner attacks (C & W) [7] are three attack
strategies based on ğ¿0, ğ¿2 and ğ¿âˆ norms that re-formulate the
traditional adversarial attack problem (see 1) by replacing the dis-
tance metric with a well-chosen objective function. This integrates
a parameter ğœ…, i.e., the confidence of the attacked image being clas-
sified as ğ‘¡, and an additional parameter ğ‘, i.e., the trade-off between
optimizing the objective function and the classifier loss function.

4.2.2 Defenses. We explored two defense strategies.

Adversarial Training [13] consists of injecting adversarial sam-
ples into the training set to make the trained model robust to them.
The major limitations of this idea are that it increases the com-
putational time to complete the training phase, and it is deeply
dependent on the type of attack strategy used to craft adversar-
ial samples. For instance, Madry et al. [25] generates adversarial
images with the PGD-method to make the trained model robust
against both one-step and multi-steps attack strategies.

Free Adversarial Training [34] proposes a training procedure
3 âˆ’ 30 times faster than the classical Adversarial Training [13, 25].
Differently from the previous one, this method updates both the
model parameters and the adversarial perturbations doing a unique
backward pass in which gradients are computed on the network
loss. Moreover, to simulate a multi-step attack â€”which would make
the trained network more robustâ€” it keeps retraining on the same
minibatch for ğ‘š times in a row.

4.3 Visual-based Recommender Models
To evaluate VAR approach, we have considered three VRSs.

Factorization Machine (FM) [31] is a recommender model
proposed by Rendle [31] to estimate the user-item preference score
with a factorization technique. For a fair comparison with VBPR and
AMR, we used BPR [32] loss function to optimize the personalized
ranking. In this respect, we adopted LightFM [23] implementation
integrating the UPM with the extracted continuous features. It is
worth noticing that, differently from the recommenders we are

going to present later, this model is not specifically designed for
visual-recommendation tasks.

ğ‘¢ ğ‘ğ‘– + ğœƒğ‘‡

Visual Bayesian Personalized Ranking (VBPR) [18] is a typ-
ical matrix factorization CF model to learn user-item latent rep-
resentation by optimizing a BPR rank-wise loss function. Given a
user ğ‘¢ and a not-interacted item ğ‘–, the preference prediction score
is Ë†ğ‘ ğ‘¢ğ‘– = ğ‘ğ‘‡
ğ‘¢ (Eğœ‘ğ‘– ) + ğ›½ğ‘¢ğ‘– , where ğ‘ğ‘¢ âˆˆ P |U |Ã—â„ and ğ‘ğ‘– âˆˆ Q| I |Ã—â„
are latent vectors of user ğ‘¢ and item ğ‘– respectively, â„ is the latent
space dimension (â„ << |U|, |I|), and ğœƒğ‘¢ is a ğœ-dimensional vector
to capture the visual interaction between ğ‘¢ and the projection of
ğœ‘ğ‘– into a low-dimensional space through a (ğœ Ã— ğ›¾)-kernel matrix
E. Furthermore, ğ›½ğ‘¢ğ‘– includes the sum of the overall offset, and the
user, item and global visual bias.

Adversarial Multimedia Recommendation (AMR) [36] is
an extension of VBPR that integrates the adversarial training pro-
cedure proposed by He et al. [19] named adversarial regularization
to build a model that is increasingly robust to FGSM-based pertur-
bations against image features. The score prediction function is the
same as VBPR since the differences are included in the training
procedure.

4.4 Evaluation Metrics
In addition to ğ¶ğ»ğ‘…@ğ¾ and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾â€”proposed in Section 3â€” ,
we studied both the variation of overall recommendation perfor-
mance and the consequences of adversarial images on th IFE.

Adversarial attacks, and defenses, performance are evalu-
ated through the attack Success Rate (ğ‘†ğ‘…), and the Feature Loss (ğ¹ ğ¿),
i.e., the mean squared error between the extracted image features
before and after the attack.

Recommendation performance is evaluated with ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾,
that considers the fraction of recommended products in the top-ğ¾
recommendation that hit test items, and ğ‘›ğ·ğ¶ğº@ğ¾, that increas-
ingly discounts the hits by the log2 of the item positions in the list.
Moreover, we investigate three beyond-accuracy measures: the item
coverage (ğ¼ğ¶ğ‘œğ‘£@ğ¾), the Gini index (ğºğ‘–ğ‘›ğ‘–@ğ¾), and the expected
free discovery (ğ¸ğ¹ ğ·@ğ¾). Please note that ğ¶ğ»ğ‘…@ğ¾, ğ‘›ğ¶ğ·ğ¶ğº@ğ¾,
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾, ğ‘›ğ·ğ¶ğº@ğ¾, and ğ¸ğ¹ ğ·@ğ¾ are computed on a per-user
basis, and then averaged across all users.

4.5 Reproducibility
To let other researchers to continue our study, and further integrate
the framework, in this section, we provide reproducibility details.
Adversarial attacks were implemented with the Python library
CleverHans [28]. For both FGSM and PGD, we adopted ğœ– = {4, 8}
re-scaled by 255. Then, for PGDâ€™s ğ›¼ parameter, we set the multi-
step size as ğœ–/6 and the number of iterations to 10. As for the C
& W attack, we ran a 5-steps binary search to calculate ğ‘, starting
from an initial value of 10âˆ’2 and set ğœ… to 0. Furthermore, we set the
maximum number of iteration to 1000 and adopted Adam optimizer
with a learning rate of 5 Ã— 10âˆ’3 as suggested in C & W [7]. Note that,
to reproduce a real attack scenario, we saved the adversarial images
in tiff format (i.e., a lossless compression), as lossy compression
(e.g., JPEG) may affect the effectiveness of attacks [15].

Feature extraction and Defenses. We used ResNet50 [16] to
extract high-level image features. From the PyTorch implementa-
tion, we set AdaptiveAvgPool2d as extraction layer, whose output

Conferenceâ€™20, October 2020, Virtual-

Anelli, Di Noia, Malitesta, and Merra

Table 2: Origin-target category classes selected for the VAR ex-
perimental evaluation. ğ¶ğ»ğ‘…@50 is averaged across the 9 com-
binations of recommenders and defenses without attacks.

Dataset
Amazon Men
Amazon Women
Tradesy

Origin
Sandal
Jersey, T-shirt
Suit

ğ¶ğ»ğ‘…@50 Target

1.0310
1.2573
0.8951

Running Shoe
Brassiere, Bandeau
Trench Coat

ğ¶ğ»ğ‘…@50
4.7852
4.2672
3.6955

Table 3: Average values of Success Rate (ğ‘†ğ‘…) and Feature Loss
(ğ¹ ğ¿) for each combination. ğ‘†ğ‘… values are multiplied by 10âˆ’3.

Image Feature Extractor

Dataset

Adversarial
Attack

Amazon
Men

Amazon
Women

Tradesy

FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W

Traditional
ğ‘†ğ‘…
ğ¹ ğ¿
65%
14.0948
36.3190
87%
97%
36.8843
100% 134.9854
20.5172
89%
18%
9.6677
22.0499
28%
85%
27.6645
100% 130.3309
21.2380
89%
83%
21.4011
46.2579
93%
53.4589
100%
100% 175.7102
25.9374
100%

ğ¹ ğ¿

Adv. Train.
ğ‘†ğ‘…
18% 0.0330
0.2658
24%
0.0334
18%
0.2801
24%
2.8022
48%
0%
0.0113
0.0851
3%
0.0119
0%
0.0974
4%
0.1770
6%
43% 0.0308
0.2376
47%
0.0311
43%
0.2478
47%
2.1185
80%

ğ¹ ğ¿

Free Adv. Train.
ğ‘†ğ‘…
15%
20%
15%
21%
42%
0%
0%
0%
0%
6%
30%
47%
30%
47%
63%

0.0278
0.2320
0.0283
0.2371
1.9080
0.0094
0.0671
0.0102
0.0735
0.3376
0.0274
0.2130
0.0273
0.2078
1.9739

is a 2048-dimensional vector. In the non-defended scenario, we
adopted ResNet50 pre-trained on ImageNet with traditional train-
ing. Conversely, when applying defense techniques, we adopted
ResNet50 pre-trained on ImageNet with Adversarial Training and
Free Adversarial Training. For the former, we used a model trained
with ğœ– = 4. For the latter, we used a model trained with ğœ– = 4 and
ğ‘š = 4. Both models are available in the published repository.

Recommenders. We realized the FM model using the LightFM
library [23]. We trained the model for 100 epochs and left all the
parameters with the default values in the library. Both VBPR and
AMR were implemented in TensorFlow. We trained the models
following the training settings adopted in Tang et al. [36]. We did
not apply the hyper-parameters search, and used the parameters
suggested in the referenced works, since the goal of our evaluation
is to investigate the protection abilities of defense mechanisms
against attacks by fixing a VRS.

Experimental Scenario. We trained each recommender on clean
images, we selected the origin-target categories such that target ones
was about four times more recommended of the origin one, and trained
a novel model using the perturbed images. Table 2 shows the selected
categories. For each dataset, we used the leave-one-out training-test
protocol putting in the test set the last time-aware userâ€™s interaction.

5 DISCUSSION OF THE RESULTS
In this section, we present and discuss the VAR experimental results.
As for the recommendation results, we evaluate the top-20, and
top-50 recommendation lists since they correspond to 26 and 48
fashion items shown on smartphones and desktop navigation on

Amazon.com, respectively. In the remainder of this section, we may
adopt the notation <dataset, VRS, attack, defense> to indicate a
specific VAR configuration, where each field in the quadruple may
vary depending on the datasets and methods described in Section 4.
Analysis of the effectiveness of defense mechanisms in pro-
tecting the model from adversarial attacks against IFEs. We
start from analyzing the experimental results shown in Table 3 (i.e.,
on attackâ€™s ğ‘†ğ‘…, ğ¹ ğ¿) and Table 4 (i.e., ğ¶ğ»ğ‘…@ğ¾, ğ‘›ğ¶ğ·ğ¶ğº@ğ¾).

Analysis of Attack Success Rate. We start the VAR analysis by
exploring the success rate of experimented attacks in fooling the IFE
with or without the adversarial robustification techniques. Results
showed in Table 3 confirm PGD and C & W as the strongest attacks
when applied to lower the classification accuracy of a defense-free
CNN classifier. For instance, PGD (ğœ– = 8) reaches the 100% of ğ‘†ğ‘…
for all the studied datasets, while C & Wâ€™s ğ‘†ğ‘… is always more than
89%. As expected, this behavior is different when VAR is tested
with defense strategies. Under this setting, C & W emerges as
the best offensive solution against defense strategies, as already
demonstrated in [7]. As an example, we observe an average ğ‘†ğ‘…
reduction in the ğ‘†ğ‘… results of 75% for FGSM-methods and 79% for
PGD, while it decreases by 43% for C & W.

Hence, we compare the ğ‘†ğ‘… results to the variation of visual-
aware recommendations of the products belonging to the perturbed
category of images. Table 4 presents the results of the proposed
VAR rank-based evaluation with respect to the origin-target attack
scenarios defined in Table 2. Quite surprisingly, Table 4 shows PGD
attacks as extremely more incisive than C & W in the defense-free
settings, i.e., the average value of ğ¶ğ»ğ‘…@20 for PGD (ğœ– = 8) is
1.2612, while it is 0.5690 for C & W. Conversely, this difference is
less observable under defense-activated VAR settings, where all the
attacks share almost comparable results. These outcomes are in
contrast with the ğ‘†ğ‘…, thus we raise the first contribution: attack
success rate is not directly related to the effects on the recommendation
performance. In other words, be powerful enough to lead a classifier
in mislabelling an origin product image towards a target class does
not explain the effects on the recommendation lists.

Analysis of Features Loss. Motivated by the previous obser-
vations, we investigate the Feature Loss (ğ¹ ğ¿) between original and
attacked samples whose values are displayed in Table 3. Comparing
the results in Table 3 and Table 4 we discover a correlation between
the variation of ğ¹ ğ¿ and the attack efficacy on VRSs. For instance,
PGD and C & W results about ğ¶ğ»ğ‘…@ğ¾, and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾, are coher-
ent with the differences observed on the ğ¹ ğ¿, i.e., the average value
of ğ¹ ğ¿ for PGD (ğœ– = 8) is 0.1470 and it is 0.0225 for C & W. This
association is further confirmed by the less oscillating values of
ğ¶ğ»ğ‘…@ğ¾ and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ under defense-activated VAR settings. For
instance, <Amazon Men, Traditional, VBPR> and <Amazon Men, (Ad-
versarial Training, Free Adversarial Training), VBPR> experiments
get a ğ¶ğ»ğ‘…@20 standard deviation of 0.3950 and 0.0260 respectively,
i.e., a difference of more than one order of magnitude. This trend
holds true also for ğ¹ ğ¿ values, i.e., 0.0441 and 0.0011, respectively.
Then, we derive the following contribution: the modification of VRS
is closely linked to the magnitude difference between original and per-
turbed image features. In short, perturbations leading to larger feature
modifications may cause a strong influence on the recommendability
of the altered product categories.

An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders

Conferenceâ€™20, October 2020, Virtual-

Table 4: Results of the VAR framework. Bold values are the highest values for each <dataset, VRS, defense> combination.

Dataset

VRS

FM

Amazon
Men

VBPR

AMR

FM

Amazon
Women

VBPR

AMR

FM

Tradesy

VBPR

AMR

Adversarial
Attack

Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W
Original
FGSM (ğœ– = 4)
FGSM (ğœ– = 8)
PGD (ğœ– = 4)
PGD (ğœ– = 8)
C & W

Traditional

Adversarial Training

Free Adversarial Training

Image Feature Extractor

ğ¶ğ»ğ‘…@20 ğ¶ğ»ğ‘…@50
1.1959
0.4857
1.2670
0.5193
1.2693
0.5092
0.5147
1.2692
1.2672
0.5024
1.2573
0.5155
1.4931
0.6352
1.2607
0.5665
1.3498
0.6052
2.6175
1.0936
3.7285
1.5736
1.4003
0.5972
0.8587
0.3876
0.8282
0.3295
0.8668
0.3053
1.9749
0.8064
5.2984
2.1264
0.8171
0.3610
1.6409
0.6771
1.6805
0.6816
1.6876
0.6880
1.6920
0.6900
1.6561
0.6727
1.6461
0.6746
1.2812
0.4377
1.0793
0.3860
1.2445
0.4057
2.3673
0.9142
3.4759
1.4462
1.2121
0.4147
2.0206
0.9449
2.4648
1.3173
2.2121
1.2814
2.0161
1.1958
2.6192
1.2377
2.2742
1.3012
0.8935
0.3371
0.9098
0.3617
0.9232
0.3696
0.9095
0.3603
0.9811
0.4028
0.9242
0.3750
1.0721
0.4108
1.2273
0.5202
1.4667
0.7251
2.2920
1.2552
2.9039
1.6982
1.0561
0.4523
1.0573
0.3653
1.2386
0.4759
1.3448
0.5896
1.9715
1.0393
2.6768
1.6016
1.1161
0.4302

ğ‘›ğ¶ğ·ğ¶ğº@20
0.0246
0.0266
0.0263
0.0266
0.0256
0.0263
0.0288
0.0299
0.0342
0.0538
0.0781
0.0290
0.0196
0.0150
0.0135
0.0418
0.1179
0.0170
0.0335
0.0354
0.0355
0.0356
0.0348
0.0329
0.0199
0.0174
0.0179
0.0459
0.0748
0.0173
0.0462
0.0862
0.0876
0.0713
0.0679
0.0746
0.0145
0.0161
0.0164
0.0158
0.0181
0.0167
0.0186
0.0260
0.0408
0.0649
0.0913
0.0221
0.0154
0.0215
0.0302
0.0523
0.0781
0.0189

ğ‘›ğ¶ğ·ğ¶ğº@50 ğ¶ğ»ğ‘…@20 ğ¶ğ»ğ‘…@50

ğ‘›ğ¶ğ·ğ¶ğº@20

ğ‘›ğ¶ğ·ğ¶ğº@50 ğ¶ğ»ğ‘…@20 ğ¶ğ»ğ‘…@50

ğ‘›ğ¶ğ·ğ¶ğº@20

ğ‘›ğ¶ğ·ğ¶ğº@50

0.0245
0.0262
0.0261
0.0263
0.0256
0.0261
0.0291
0.0269
0.0300
0.0539
0.0780
0.0285
0.0178
0.0159
0.0160
0.0413
0.1141
0.0163
0.0333
0.0351
0.0351
0.0352
0.0344
0.0329
0.0237
0.0198
0.0228
0.0483
0.0741
0.0214
0.0422
0.0649
0.0620
0.0517
0.0593
0.0558
0.0217
0.0228
0.0229
0.0226
0.0250
0.0234
0.0271
0.0333
0.0458
0.0699
0.0971
0.0286
0.0252
0.0325
0.0376
0.0593
0.0852
0.0278

0.4003
0.3811
0.3715
0.3729
0.3765
0.3765
0.3028
0.6029
0.5879
0.6211
0.6247
0.6652
0.4924
0.4332
0.4318
0.4435
0.4323
0.4293
0.4622
0.4708
0.4730
0.4737
0.4919
0.4655
0.5108
0.6032
0.6186
0.6309
0.6413
0.6280
0.8342
0.7135
0.7137
0.6473
0.6770
0.7159
0.3579
0.3744
0.3822
0.3598
0.3741
0.3913
0.2973
0.5055
0.4807
0.4431
0.4726
0.4766
0.1626
0.3587
0.3512
0.3438
0.3494
0.3577

0.9793
0.9315
0.9299
0.9187
0.9215
0.9338
0.8130
1.4496
1.4333
1.4763
1.4565
1.4487
1.1802
1.1736
1.1827
1.1756
1.1447
1.1227
1.1589
1.1550
1.1593
1.1503
1.1811
1.1409
1.2390
1.3813
1.4160
1.4456
1.4674
1.4277
1.4602
1.7675
1.7595
1.7284
1.7451
1.7976
0.8852
0.9151
0.9141
0.9024
0.9018
0.9356
0.7526
0.9778
0.9567
0.8885
0.9346
0.9920
0.5346
0.8059
0.7887
0.7686
0.7726
0.8218

0.0204
0.0198
0.0193
0.0191
0.0194
0.0194
0.0141
0.0316
0.0316
0.0324
0.0335
0.0336
0.0228
0.0235
0.0238
0.0242
0.0237
0.0230
0.0236
0.0243
0.0242
0.0244
0.0254
0.0240
0.0251
0.0310
0.0319
0.0315
0.0336
0.0326
0.0433
0.0334
0.0341
0.0307
0.0322
0.0336
0.0160
0.0168
0.0166
0.0158
0.0165
0.0179
0.0122
0.0242
0.0224
0.0199
0.0211
0.0223
0.0059
0.0162
0.0165
0.0160
0.0160
0.0175

0.0201
0.0192
0.0191
0.0188
0.0190
0.0191
0.0155
0.0306
0.0302
0.0309
0.0312
0.0305
0.0230
0.0242
0.0246
0.0247
0.0241
0.0233
0.0237
0.0239
0.0239
0.0238
0.0246
0.0237
0.0251
0.0292
0.0301
0.0296
0.0314
0.0303
0.0332
0.0351
0.0356
0.0342
0.0346
0.0357
0.0219
0.0227
0.0218
0.0216
0.0221
0.0233
0.0173
0.0267
0.0252
0.0226
0.0243
0.0257
0.0115
0.0202
0.0200
0.0193
0.0197
0.0217

0.3984
0.3750
0.3837
0.3735
0.3825
0.3798
0.3702
0.5688
0.5596
0.5778
0.6141
0.6444
0.1070
0.4103
0.4007
0.4173
0.3942
0.4378
0.3186
0.2985
0.2985
0.3057
0.2988
0.2844
0.3417
0.6088
0.6313
0.6263
0.6139
0.5729
0.5063
0.4565
0.4429
0.4900
0.4445
0.4977
0.4649
0.5118
0.5119
0.5081
0.5092
0.5116
0.3179
0.5644
0.4868
0.5217
0.5317
0.5474
0.2189
0.4041
0.3745
0.3635
0.3864
0.3621

0.9791
0.9324
0.9281
0.9112
0.9367
0.9428
1.1547
1.4924
1.4433
1.5003
1.5768
1.5641
0.6255
1.1595
1.1250
1.1657
1.1386
1.1623
0.7741
0.7369
0.7319
0.7451
0.7374
0.7062
0.9570
1.1151
1.1662
1.1165
1.1194
1.1019
0.7841
1.0392
1.0408
1.0750
1.0364
1.0714
1.1398
1.2292
1.2271
1.2300
1.2295
1.2260
0.9602
1.1905
1.0908
1.1586
1.1580
1.2309
0.7609
0.9674
0.9132
0.8986
0.9416
0.9178

0.0202
0.0194
0.0195
0.0193
0.0194
0.0194
0.0159
0.0283
0.0277
0.0286
0.0310
0.0348
0.0038
0.0187
0.0188
0.0193
0.0181
0.0202
0.0158
0.0145
0.0143
0.0155
0.0151
0.0144
0.0161
0.0303
0.0332
0.0330
0.0322
0.0302
0.0303
0.0230
0.0221
0.0240
0.0221
0.0243
0.0212
0.0236
0.0233
0.0236
0.0234
0.0240
0.0129
0.0261
0.0210
0.0235
0.0232
0.0242
0.0079
0.0180
0.0159
0.0160
0.0171
0.0147

0.0201
0.0192
0.0189
0.0188
0.0191
0.0192
0.0207
0.0296
0.0290
0.0301
0.0320
0.0334
0.0100
0.0217
0.0214
0.0220
0.0213
0.0224
0.0155
0.0144
0.0142
0.0152
0.0150
0.0144
0.0184
0.0246
0.0263
0.0257
0.0254
0.0247
0.0207
0.0217
0.0213
0.0219
0.0213
0.0219
0.0246
0.0261
0.0257
0.0256
0.0259
0.0269
0.0180
0.0267
0.0230
0.0243
0.0245
0.0252
0.0135
0.0202
0.0184
0.0184
0.0193
0.0177

Analysis of Category-based Performance. After having jus-
tified the results in Table 4, we discuss the category-based measures
across models and datasets. Studying the ğ¶ğ»ğ‘…@ğ¾ and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾
from recommenders point of view, FM appears as the least affected
model. For instance <Amazon Men, (Adversarial Training, Free Ad-
versarial Training), FM> and <Amazon Women, Free Adversarial
Training, FM> register reductions even in terms of category mea-
sures after all the attacks. In general, the average ğ¶ğ»ğ‘…@20 variation
between each FM attack-free setting and the most effective attack
is only 8.66%, i.e., a rather small value compared to other models
(127.02% for VBPR experiments). We explain the highly negligible

impact of attacks against FM by recalling that the implemented ver-
sion [23] is not designed to integrate continuous visual features with
a large dimensionality (i.e., 2048) unlike VBPR and AMR.

As regards VBPR, the PGD strategy reaches the larger variation
of ğ¶ğ»ğ‘…@ğ¾ and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ in the defense-free setting, confirming
the previous considerations made about these metrics with the
feature loss. For instance, PGD (ğœ– = 8) leads to a 4 times increase of
the ğ¶ğ»ğ‘…@20 of the attacked category (i.e., suit), and about a 5 times
increase of the ğ‘›ğ¶ğ·ğ¶ğº@20 on the Tradesy dataset. Additionally,
C & W is the attack with the best average variation on all the rank-
based metrics in the defense scenario. For instance, C & W attacks

Conferenceâ€™20, October 2020, Virtual-

Anelli, Di Noia, Malitesta, and Merra

make ğ¶ğ»ğ‘…@20 and ğ‘›ğ¶ğ·ğ¶ğº@20 grow by an average of 69.48%,
and 91.01%, respectively. These results suggest that the adversarial
robustification strategies have not protected VBPR from the injection
of perturbed product images despite they got high performance in
protecting the classification task.

The third tested VRS is AMR. We chose this model since it is
the first VRS to integrate adversarial protection by design, so we
expected to get a limited variation in traditional performance under
attack settings. Surprisingly, results show that AMR is prone to
the effects of attacks as much as VBPR. For example, PGD (ğœ– = 8)
method represents the biggest security threat on the VRS in defense-
free settings, while C & W is the best attack when the IFE is de-
fended. Moreover, <Free Adversarial Training, AMR> and <Adver-
sarial Training, AMR> do not provide any protection improvements,
notwithstanding the two defense techniques applied on both the
IFE and the VRS respectively. For instance, the mild ğ¶ğ»ğ‘…@20 im-
provement seen in <C & W, Free Adversarial Training, AMR> is
higher than the one obtained in <C & W, Free Adversarial Training,
VBPR> (i.e., 77.64% and 69.48% respectively), but the latter did not
involve any defense method on the VRS. We conclude that the com-
bination of the state-of-the-art defense techniques against adversarial
perturbations applied on both DNNs and VRSs does not preserve the
quality of the recommendation.

Conclusively, we compare ğ¶ğ»ğ‘…@ğ¾ and ğ‘›ğ¶ğ·ğ¶ğº@ğ¾. The calcu-
lated values for ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ confirm the trends noticed in ğ¶ğ»ğ‘…@ğ¾.
Furthermore, we observe ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ shows a relatively higher im-
provement than the one captured by the ğ¶ğ»ğ‘…@ğ¾ in any <dataset,
attack, defense, recommender> setting. Here, we draw two final
considerations: (i) ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ and ğ¶ğ»ğ‘…@ğ¾ are two metrics suit-
able to evaluate the motivating scenario (i.e., an adversary who wants
to push a category of products), and (ii) even though attacks have
pushed the products into the top-ğ¾ lists (see ğ¶ğ»ğ‘…@ğ¾ results), an
argument could be made that these products get very high positions
since ğ‘›ğ¶ğ·ğ¶ğº@ğ¾ have increased even more that ğ¶ğ»ğ‘…@ğ¾.

Evaluation of the effects of adversarial defenses on the IFE
in the variation of overall recommendation measures. Table 5
shows the accuracy and beyond accuracy results on the Amazon
Men dataset for the two most powerful attacks previously discussed.
The intuition behind this evaluation is to understand whether the
application of adversarial defenses â€”adopted to alleviate attacksâ€™
influenceâ€” may generate a drastic variation of the overall recom-
mendation performance. In Table 5, we bolded the best results
for each <defense, VRS> experiment. Surprisingly, we see that the
application of powerful attacks has not tragically worsened the ac-
curacy and beyond accuracy performance. On the contrary, some
measures have significantly improved as a consequence of the attack.
For instance, AMR has its best performance under all combinations
attacks combinations independently from the application of a de-
fense mechanism. On the other side, it is worth noticing the item
coverage generally gets better under attack settings. We explain this
effect by the fact that, since adversarial images can be considered
a source of randomness, they might help the recommendation of
products from the long tail (e.g., products similar to the attacked
ones may get benefit from the attack process).

Furthermore, Table 5 shows that VBPR and FM have their best
accuracy performance in defense-activated settings. For instance,
<Amazon Men, Original, Defense-free, VBPR> has Recall and nDCG

Table 5: Overall recommendation performance evaluated on
the top-20 recommendation lists.

Dataset VRS

Adversarial
Attack

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

Image Feature Extractor
ğ¸ğ¹ ğ·
ğ‘”ğ‘–ğ‘›ğ‘–

ğ‘›ğ·ğ¶ğº

FM

VBPR

AMR

FM

Amazon
Men

VBPR

AMR

FM

VBPR

AMR

Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W

Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W

Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W
Original
PGD (ğœ– = 8)
C & W

0.0027
0.0024
0.0023
0.0127
0.0126
0.0132
0.0330
0.0316
0.0333

0.0034
0.0023
0.0023
0.0193
0.0129
0.0124
0.0312
0.0332
0.0327

0.0024
0.0025
0.0021
0.0236
0.0129
0.0123
0.0317
0.0331
0.0325

Traditional

0.6025
0.5995
0.6026
0.0519
0.0531
0.0519
0.0272
0.0314
0.0256

0.0018
0.0016
0.0016
0.0073
0.0070
0.0072
0.0171
0.0166
0.0174

0.0010
0.0009
0.0009
0.0046
0.0044
0.0045
0.0123
0.0119
0.0125
Adversarial Training
0.6406
0.0012
0.5940
0.0009
0.5828
0.0009
0.0494
0.0076
0.0045
0.0550
0.0543
0.0045
0.0039
0.0118
0.0279
0.0125
0.0277
0.0124

0.0021
0.0016
0.0016
0.0112
0.0072
0.0071
0.0151
0.0174
0.0172
Free Adversarial Training
0.0017
0.6329
0.6016
0.0018
0.5991
0.0015
0.0325
0.0125
0.0073
0.0528
0.0070
0.0555
0.0154
0.0067
0.0171
0.0271
0.0265
0.0171

0.0009
0.0010
0.0008
0.0089
0.0046
0.0044
0.0118
0.0123
0.0123

ğ‘–ğ¶ğ‘œğ‘£

7367
7369
7370
1954
1951
1925
1586
1716
1488

7370
7368
7368
1826
1990
1956
224
1496
1429

7371
7369
7366
1184
1900
1992
402
1420
1425

equal to 0.0127 and 0.0046, while in the defense-activated settings
both metrics are quite doubled, i.e., 0.0193 and 0.0076 in Adversarial
Training and 0.0236 and 0.0089 in Free Adversarial Training. The
confirmation of this trend in attack scenarios raises the following
conclusion: the application of defense mechanisms on the IFE is a
system design possibility that preserves, or even improves, the overall
performance of a VRS, while does not guarantee the protection from
altering the recommendability of the category of products.

6 CONCLUSION AND FUTURE WORK
We have presented an evaluation framework, i.e., Visual Adversarial
Recommendation (VAR), to explore the efficacy of adversarial robus-
tification mechanisms against several state-of-the-art adversarial
attacks and to investigate the impact of perturbed product images
on visually-aware recommendations. Experimental results have
shown that defense mechanisms do not guarantee the protections
of recommenders against attacks also in the case of low-success
rate attacks. Interestingly, we have found that the effectiveness
of attacks in altering the recommenders is more related to high
feature losses than high success rates. This finding raises inter-
esting opportunities to develop novel recommender models along
with defense strategies. Finally, we have verified that overall recom-
mendation performance has not worsened under the experimented
threat model, and (surprisingly) defended IFEs may even improve in
non-attack settings. This further opens future directions in finding
the reasons behind this behavior, getting the most benefits from it.

An Empirical Study of DNNs Robustification Inefficacy in Protecting Visual Recommenders

Conferenceâ€™20, October 2020, Virtual-

Neural Networks. In SP 2016.

[30] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal Networks. In NeurIPS
2015.

[31] Steffen Rendle. 2010. Factorization Machines. In ICDM 2010.
[32] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
209. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI 2009.
[33] Francesco Ricci, Lior Rokach, and Bracha Shapira (Eds.). 2015. Recommender

Systems Handbook. Springer.

[34] Ali Shafahi, Mahyar Najibi, AmGhiasi, Zheng Xu, John P. Dickerson, Christoph
Studer, Larry S. Davis, GavTaylor, and Tom Goldstein. 2019. Adversarial training
for free!. In NeurIPS 2019.

[35] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In ICLR 2014.

[36] J. Tang, X. Du, X. He, F. Yuan, Q. Tian, and T. Chua. 2019. Adversarial Training

Towards Robust Multimedia Recommender System. TKDE 2019 (2019).

[37] Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra, and Suhang
Wang. 2020. Transferring Robustness for Graph Neural Network Against Poison-
ing Attacks. In WSDM 2020.

[38] Suhang Wang, YilWang, Jiliang Tang, Kai Shu, Suhas Ranganath, and Huan Liu.
2017. What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest
Recommendation. In WWW 2017.

[39] Zhijing Wu, Yiqun Liu, Qianfan Zhang, Kailu Wu, Min Zhang, and Shaoping Ma.
2019. The Influence of Image Search Intents on User Behavior and Satisfaction.
In WSDM 2019.

[40] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. 2014. Droid-Sec:

deep learning android malware detection. In SIGCOMM 2014.

[41] YZhang and James Caverlee. 2019. Instagrammers, Fashionistas, and Me: Recur-
rent Fashion Recommendation with Implicit Visual Influence. In CIKM 2019.

REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gra-
dients Give a False Sense of Security: Circumventing Defenses to Adversarial
Examples. In ICML 2018.

[2] Runa Bhaumik, Chad Williams, Bamshad Mobasher, and Robin Burke. 2006. Secur-
ing collaborative filtering against malicious attacks through anomaly detection.
In ITWP 2006.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks against
Machine Learning at Test Time. In ECML-PKDD 2013.

[4] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian J. Goodfellow, Aleksander Madry, and Alexey Ku-
rakin. 2019. On Evaluating Adversarial Robustness. CoRR 2019 (2019).

[5] Nicholas Carlini and David A. Wagner. 2016. Defensive Distillation is Not Robust

to Adversarial Examples. CoRR 2016 (2016).

[6] Nicholas Carlini and David A. Wagner. 2017. Adversarial Examples Are Not
Easily Detected: Bypassing Ten Detection Methods. In AISec@CCS 2017.
[7] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness

of Neural Networks. In SP 2017.

[8] Wei-Ta Chu and Ya-Lun Tsai. 2017. A hybrid recommendation system considering
visual information for predicting favorite restaurants. WWW 2017 (2017).
[9] Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. 2020. Adversarial
Machine Learning Recommender Systems (AML-RecSys). In WSDM 2020.
[10] Tommaso Di Noia, Daniele Malitesta, and Felice Antonio Merra. 2020. TAaMR:
Targeted Adversarial Attack against Multimedia Recommender Systems. In
DSNâ€“DSML 2020.

[11] Negin Entezari, Saba A. Al-Sayouri, Amirali Darvishzadeh, and Evangelos E.
Papalexakis. 2020. All You Need Is Low (Rank): Defending Against Adversarial
Attacks on Graphs. In WSDM 2020.

[12] Alexey Kurakand Ian J. Goodfellow and Samy Bengio. 2017. Adversarial examples

the physical world. In ICLR 2017.

[13] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and

Harnessing Adversarial Examples. In ICLR 2015.

[14] Kristen Grauman. 2020. Computer Vision for Fashion: From Individual Recom-

mendations to World-wide Trends. In WSDM 2020.

[15] Chuan Guo, Mayank Rana, Moustapha CissÃ©, and Laurens van der Maaten. 2018.
Countering Adversarial Images using Input Transformations. In ICLR 2018.
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual

Learning for Image Recognition. In CVPR 2016.

[17] Ruining He and Julian J. McAuley. 2016. Ups and Downs: Modeling the Visual
Evolution of Fashion Trends with One-Class Collaborative Filtering. In WWW
2016.

[18] Ruining He and Julian J. McAuley. 2016. VBPR: Visual Bayesian Personalized

Ranking from Implicit Feedback. In AAAI 2016.

[19] Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. 2018. Adversarial

Personalized Ranking for Recommendation. In SIGIR 2018.

[20] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V.
Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. 2012. Deep Neural
Networks for Acoustic Modeling Speech Recognition: The Shared Views of Four
Research Groups. IEEE Signal Processing Magazine (2012).

[21] Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian J. McAuley. [n.d.].
Visually-Aware Fashion Recommendation and Design with Generative Image
Models. In ICDM 2017.

[22] Saeid Balaneshin Kordan and Alexander Kotov. 2018. Deep Neural Architecture
for Multi-Modal Retrieval based on Joint Embedding Space for Text and Images.
In WSDM 2018.

[23] Maciej Kula. 2015. Metadata Embeddings for User and Item Cold-start Recom-

mendations. In CBRecSys@RecSys 2015.

[24] Shyong K. Lam and John Riedl. 2004. Shilling recommender systems for fun and

profit. In WWW 2004.

[25] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In ICLR 2018.

[26] Wei Niu, James Caverlee, and Haokai Lu. 2018. Neural Personalized Ranking for

Image Recommendation. In WSDM 2018.

[27] Michael P. Oâ€™Mahony, Neil J. Hurley, Nicholas Kushmerick, and Guenole C. M.
Silvestre. 2004. Collaborative recommendation: A robustness analysis. ACM
Trans. Internet Techn. (2004).

[28] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Fein-
man, Alexey Kurakand Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexan-
der Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-
LJuang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke,
Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long.
2018. Technical Report on the CleverHans v2.1.0 Adversarial Examples Library.
Corr 2018 (2018).

[29] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram
Swami. 2016. Distillation as a Defense to Adversarial Perturbations Against Deep

