Over-the-Air Decentralized Federated Learning

Yandong Shi∗†‡, Yong Zhou∗ , and Yuanming Shi∗
∗School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China
†Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, China
‡University of Chinese Academy of Sciences, Beijing 100049, China
Email: {shiyd, zhouyong, shiym}@shanghaitech.edu.cn

1
2
0
2

n
u
J

5
1

]
T
I
.
s
c
[

1
v
1
1
0
8
0
.
6
0
1
2
:
v
i
X
r
a

Abstract—In this paper, we consider decentralized federated
learning (FL) over wireless networks, where over-the-air compu-
tation (AirComp) is adopted to facilitate the local model consen-
sus in a device-to-device (D2D) communication manner. However,
the AirComp-based consensus phase brings the additive noise in
each algorithm iterate and the consensus needs to be robust to
wireless network topology changes, which introduce a coupled
and novel challenge of establishing the convergence for wireless
decentralized FL algorithm. To facilitate consensus phase, we
propose an AirComp-based DSGD with gradient tracking and
variance reduction (DSGT-VR) algorithm, where both precoding
and decoding strategies are developed for D2D communication.
Furthermore, we prove that the proposed algorithm converges
linearly and establish the optimality gap for strongly convex
and smooth loss functions, taking into account the channel
fading and noise. The theoretical result shows that the additional
error bound in the optimality gap depends on the number of
devices. Extensive simulations verify the theoretical results and
show that the proposed algorithm outperforms other benchmark
decentralized FL algorithms over wireless networks.

I. INTRODUCTION

Federated learning (FL), as a novel

type of distributed
machine learning [1], has received a growing interest re-
cently, both from academia and industry. With FL, many
mobile devices collaboratively train a global model under
the orchestration of a parameter server (PS), while keeping
the training data unmoved. The PS only receives the local
model rather than raw data from mobile devices,
thereby
signiﬁcantly reducing the overhead of data collection and
preserving additional privacy [2], [3]. FL has enabled a wide
range of applications in industrial Internet of Things (IIoT)
[4]–[6], e.g., autonomous driving and collaborative robotics,
in which high levels of security, privacy and reliability are de-
manded. In future wireless networks, the driving applications
of FL including resource management, channel estimation and
signal detection, edge caching, and computation ofﬂoading [7],
[8], which also poses unique challenges including statistical
heterogeneity, communication cost and resource allocation [9].
It thus becomes critical to implement FL for the future 6G
wireless networks [10], [11].

Due to limited radio resources, communication is a funda-
mental bottleneck in FL over wireless networks [12], [13]. The
digital [14]–[16] and analog [17]–[19] transmission schemes
have been proposed to support FL over wireless networks. In

This work was supported in part by the National Natural Science Founda-

tion of China (NSFC) under grant 62001294.

particular, the authors in [16] partitioned a large-scale learning
task over multiple resource-constrained edge devices, where
joint parameter-and-bandwidth allocation was proposed to
reduce the total learning-and-communication latency. In [14],
[15], a joint learning, wireless resource allocation, and user
selection problem was formulated to minimize FL training loss
and FL convergence time. However, the orthogonal channels
are required in digital FL systems which suffer from hugely
demanding bandwidth especially when number of edge devices
is large [18]. As for analog transmission scheme, over-the-air
computation (AirComp) was proposed to support fast model
aggregation for FL by exploiting the superposition property
of multi-access channel (MAC), where multiple edge devices
share the same frequency channel [17]–[19]. Speciﬁcally, a
joint device selection and beamforming design was proposed
to improve the statistical learning performance for FL [17].
The authors in [18], [19] focused on designing a stochas-
tic gradient descent (SGD) based algorithm over MAC and
investigate the effect of channel fading and noise on the
convergence analysis of FL.

The aforementioned studies require a central PS to or-
chestrate the training process. However, in some application
scenarios, e.g., cooperative driving and robotics [6], a central
PS may not always be available and reliable when the number
of edge devices is large [20]. The centralized FL also faces
straggler’s dilemma [21] due to the heterogeneity of edge
devices, i.e., the FL training speed is limited by the devices
with slowest computation and worst channel conditions. To
overcome these challenges, device-to-device (D2D) commu-
nications based decentralized FL [22]–[24] was proposed,
where each device only communicates with its neighbors. In
particular, [24] considered digital transmission schemes in a
joint learning and network simulation framework, to quantify
the effects of model pruning, quantization and physical layer
constraints for decentralized FL. Due to limited wireless
bandwidth resources, the authors in [22], [23] proposed a
decentralized stochastic gradient descent (DSGD) algorithm
to improve the convergence performance in decentralized FL,
where AirComp based D2D communication was developed to
facilitate the consensus phase. However, the AirComp-based
consensus phase brings the channel fading and additive noise
in each algorithm iterate. On the other hand, the neighbor-
hood weighted average involving the information of network
topology performs the consensus phase. The decentralized

 
 
 
 
 
 
FL algorithm thus needs to be robust against changes in
network topology, otherwise it may not converge [20]. These
introduce a coupled and unique challenge of establishing
the convergence of decentralized FL algorithm over wireless
networks.

In this paper, we shall consider a decentralized FL model
over wireless networks, where no central PS exists to orches-
trate the training process. To investigate and facilitate consen-
sus phase over wireless network, we propose an AirComp-
based DSGT-VR algorithm in decentralized FL, where both
precoding and decoding strategies at devices are developed
to guarantee algorithm convergence. In addition, the gradient
tracking and variance reduction techniques are involved in
the proposed algorithm, which can further improve algorithm
convergence performance. We analyze the performance of
AirComp-based DSGT-VR algorithm theoretically by intro-
ducing the auxiliary variable in consensus phase,
the
mean of all local parameters. For strongly convex and smooth
local loss functions, we prove that the proposed algorithm
converges linearly and also establish the optimality gap, taking
into account the channel fading and noise. In addition, the
convergence result shows that the additive error bound in the
optimality gap depends on the number of devices. Numerical
experiments are conducted to validate the theoretical analysis
and demonstrate the superior performance of the proposed
algorithm over wireless networks.

i.e.,

Notations: Throughout this paper, we denote the cardinality
of set A by |A|, (cid:96)2-norm of vector x by (cid:107)x(cid:107)2 and the second
largest singular value of matrix W by

W

.

(cid:57)

(cid:57)

II. SYSTEM MODEL AND PROBLEM FORMULATION

A. System Model

As shown in Fig. 1, we consider a federated learning
system supported by a decentralized wireless communication
network in decentralized setting where no central PS exists to
coordinate training process of all edge devices [25]. We denote
N = {1, ..., N } as the set of devices and G = (N , E) as an
undirected graph with vertex set N and edge set E that rep-
resents the set of communication links. The set of connected
neighbors of device i is denoted as Ni = {j|(i, j) ∈ E}. Each
device n ∈ N has its own data set Dn and all devices aim
to collaboratively learn a common machine learning model
by communicating with each other through device-to-device
communication link over graph G.

The goal of this paper is to learn a global model by tackling

the distributed stochastic optimization problem

minimize
θ∈Rd

F (θ) (cid:44) 1
N

N
(cid:88)

i=1

fi (θ) ,

(1)

(cid:80)

ξ∈Di

where F (θ) is the global

loss function and fi(θ) =
1
fi,ξ(θ) is the local loss function at device i. In
|Di|
particular, fi,ξ(θ) is the loss for the parameter θ ∈ Rd on a
data sample ξ at device i. In the distributed learning process,
each device i ∈ N has a local parameter vector θt
that
i
approximates the solution of problem (1) at the t-th iteration.

Fig. 1: Illustration of the decentralized FL model over wireless
networks consisting of 7 devices.

B. DSGD with Gradient Tracking and Variance Reduction

When the devices communicate over error-free orthogonal
links, the commonly adopted method for solving problem (1)
is DSGD [26]. We denote W as the mixing matrix encoding
the network structure at iteration t.

Deﬁnition 1 (Mixing matrix). Mixing matrix W is a sym-
metric doubly stochastic matrix whose ij-th entry wij > 0
indicates that device i and j are connected,

W ∈ W, W = W T , W 1 = 1, 1T W = 1T ,

(2)

where W = {W ∈ [0, 1]N ×N |wij = 0 if(i, j) /∈ E and i (cid:54)= j}.

With the DSGD algorithm, the information exchange can
only occur between connected devices. The algorithm for each
device i ∈ N at iteration t consists of two phases:

1) Computation phase: each device computes a stochastic
i and then

gradient based on a random data sample ξt
performs stochastic gradient updates,

θt+ 1

2

i

= θt

i − αt∇fi,ξt

i

(θt

i ).

(3)

2) Consensus phase: devices average their local model

based on mixing matrix,

θt+1
i =

(cid:88)

j∈Ni

wijθt+ 1

2

j

.

(4)

However, DSGD decays linearly with a ﬁxed step size
αt ≡ α, but performs inexact convergence (the steady error
of DSGD has an additional bias). Although properly reducing
αt enables exact convergence (each θt
n converges to the same
exact solution), DSGD convergences slowly both in practice
and theory [27]. With the help of dynamic average-consensus,
the DSGD with gradient tracking (DSGT) removes the bias
which characterizes the difference between local loss function
fi in DSGD, thereby achieving a much lower steady-state error
when the data sample across devices are largely heterogeneous
[28]. And by adding variance reduction to DSGT, [27] shows
that the DSGT with variance reduction (DSGT-VR) leads to
an exact linear convergence with a constant step-size.

In DSGT-VR, each device i maintains a gradient

table
{∇fi,j( ˆθi,j)}|Di|
j=1, where ˆθi,j denotes the most recent parame-
ter when fi,j was computed, to store all local gradients. Then

at iteration t, each device i randomly chooses a data sample
i and computes the unbiased gradient gt
ξt

gt
i = ∇fi,ξt

i

(θt

i ) − ∇fi,ξt

i

( ˆθi,ξt

i

) +

1
|Di|

i as follows
|Di|
(cid:88)

j=1

∇fi,j( ˆθi,j). (5)

Next, we replace the recent gradient ∇fi,ξt
∇fi,ξt
dt+1
i

i ) with
( ˆθi,ξt
) in gradient table. Last, the gradient estimator
i
can be computed by using gradient tracking technology,

i

i

(θt

We assume the computation phase at each device to be
instantaneous after transmission. Based on the received signal
yt
i , device i can average its neighbor’s local model and
get close to the solution of problem (1). However, due to
the inﬂuence of channel fading and transmission noise, the
received signal is distorted, which will negatively inﬂuence
the convergence of DSGT-VR algorithms. Hence, we aim to
develop a reliable transmission strategy based on DSGT-VR
to support FL in decentralized setting.

wijdt

i + gt+1

i − gt
i .

(6)

D. AirComp-based DSGT-VR algortihm

dt+1
i =

(cid:88)

j∈Ni
C. Communication Model

In this section, we focus on the design of communication
model for AirComp-based DSGT-VR algorithm in decentral-
ized setting. The designed communication model consists of
the following two parts:

• Scheduling: To cope with wireless interference, different
devices should be scheduled to communicate in different
transmission blocks.

• Transmission: The transmission strategies are designed

to support consensus phase over wireless networks.

1) Scheduling: To improve the communication perfor-
mance, we consider the D2D communication [29] in this
paper. Since the precoding strategy (8) is designed for target
device to guarantee the convergence of proposed algorithm, it
brings interference to other devices. So we focus on design
interference-free scheduling by scheduling enrolled receiving
devices as active “PS” in different transmission block. In this
interference-free schedule scheme, there are no two enrolled
receiving devices connected to the same device. A naive
scheduling policy is to schedule to the enrolled receiving
devices one by one, which require N transmission blocks
during one consensus phase. The graph coloring algorithm can
be adopted to ﬁnd a suitable scheduling policy that decreases
the number of transmission blocks [30]. The authors in [22]
has investigated the effect of scheduling policy. In this paper
we focus on the convergence versus the consensus iteration t
over wireless networks.

2) Transmission: To enhance the spectral efﬁciency, Air-
Comp has been envisioned to have a wide range of applications
in the areas of consensus [31]. In AirComp, the receiver device
receives a superposition of the signals that simultaneously
transmitted by its neighbor [32], [33]. Block ﬂat-fading chan-
nel are considered in this paper and one transmission block
contains d time slots. Hence, at the transmission block t, the
received signal at enrolled receiving device i can be written
as

yt
i =

(cid:88)

j∈Ni

ijxt
ht

j + zt
i ,

(7)

ij ∈ C is the channel coefﬁcient between devices i and
where ht
j at transmission block t, the transmit signal xt
j encodes the
i ∈ Cd represents the
information of the local model θt
additive noise vector. In addition, channel inputs are subject
to a peak power constraint, i.e., E[(cid:107)xt

j and zt

2] ≤ P, ∀i ∈ N .

i(cid:107)2

First, we develop precoding and decoding strategies at de-
vices to guarantee the convergence of AirComp-based DSGT-
VR. Each consensus iteration k includes R ≤ N transmission
blocks and the devices are scheduled as active “PS” in one
transmission block.

In the precoding phase before AirComp, we assume that
perfect CSI are available at all devices. Since the topology of
the communication network is known, each device contains
the mixing matrix weights among its connected neighbors, i.e.,
wij, ∀j ∈ Ni. In transmission block t, the signal xt
j, j ∈ Ni
transmitted to device i is precoded as
ij)H
(ht
ij|2 θt
j,
|ht

ptwij

j =

(8)

xt

(cid:112)

√

|ht

ij |
(cid:107)θt

P
i (cid:107)2

where (cid:112)pt = mini
, ∀j is a uniform scaling factor to
satisfy the peak power constraint. However, in decentralized
setting, it is observed that the weak channels lead to the small
transmit power. In this case, the inﬂuence of channel noise
will signiﬁcantly increase. Therefore, we form the connectivity
graph G by connecting the devices i and j if the channel gain
|hij| is above a certain threshold γ [34]–[36]. Then, for device
i, we get all its connected neighbors Ni, which is given by

Ni = {j|(i, j) ∈ E, if |hij| > γ}.

(9)

Second, in the decoding phase and based on (8), the received

signal at device i after AirComp can be written as
(cid:88)

(cid:112)

yt
i =

ptwijθt

j + zt
i .

(10)

j∈Ni\{i}

Then the device i can decode the local model signal yt

i , by

θt+1
i =

1
(cid:112)pt

i + wiiθt
yt

i =

(cid:88)

j∈Ni

wijθt

j + (cid:101)zt
i ,

(11)

i√

i = zt

pt ∼ CN (0, σ2

where (cid:101)zt
pt ) is the channel noise. Since
equation (11) is a superposition of the local model parameter,
we apply AirComp to perform the consensus phases in DSGT-
VR algorithm in wireless communication.

The AirComp techniques operate over star graphs in a pair
of consecutive phases of time slots [18], [19]. However, in
decentralized setting, since there is no central PS, each device
can play a role of central PS through AirComp over star
graphs [25]. Specially, in the ﬁrst phase of time slot, the
device i at the center of a star sub-graph receive the signals

Algorithm 1: DSGT-VR with Over-the-Air Consensus
i ), i ∈ N ,
Input

i = ∇fi(θ0

i = θ0, d0

: Initial: θ0

i = g0

step sizes α, max iterations T , W and
gradient table {∇fi,j( ˆθi,j)}|Di|

j=1, ˆθi,j = θ0

i , ∀j

1 for t = 1, 2, . . . , T do
2

for r = 1, 2, . . . , M transmission block do

2

= θt

( ˆθi,ξt

) by ∇fi,ξt

randomly, compute

for each i ∈ N do in parallel
i − αdt
i.
n

Updating θt+ 1
i
Get a data sample ξt+1
unbiased gradient (5).
Replace ∇fi,ξt
i
gradient table.
Updating dt+ 1
i + gt
2
if device i is scheduled as active “PS” then
The device i received signal via (10)
and decoding via (11) to get θt+1
.
Broadcast dt+ 1
neighbors.

back to all its

i = dt

i − gt−1
i

i ) in

(θt

.

i

i

2

i

i

else

Precoding θt+ 1
precoded signal to all its neighbors Ni.

via (8) and send the

i

2

end

end

end
Each device locally updates gradient eatimator
i = (cid:80)
dt+1

wijdt+ 1

.

i

2

j∈Ni

17 end

3

4

5

6

7

8

9

10

11

12

13

14

15

16

the devices broadcast

simultaneously transmitted by all its connected neighbors in
Nn with a superposition form (10). In the second phase of
time slot, this central device i broadcasts its update gradient
estimator (6) to all its connected neighbors in Nn. In this
paper,
the gradient estimator rather
than model parameter [22], [23] in downlink communication.
The complete implementation of AirComp-based DSGT-VR
is summarized in Algorithm 1. Note that gradient estimator
of each device is received in different transmission block via
error-free link due to scheduling, then we can update gradient
estimator based on (6) after M transmission blocks. And we
leave the variance reduction techniques over noisy links for
future work.

III. CONVERGENCE ANALYSIS

In this section, we provide the convergence analysis of
the AirComp-based DSGT-VR. The convergence results are
established under the following assumptions.

Assumption 1. The local cost function fi is both µ-strongly
convex and L-smooth for any i ∈ N .

Assumption 2. The network graph G is undirected and
connected, i.e., there exists a path between any two devices.

Assumption 3. The local parameter is bounded by a universal
i (cid:107)2 ≤ B2, ∀i, t.
constant B ≥ 0, i.e., (cid:107)θt

Assumption 2 implies that β =

< 1 [37].
Based on above assumptions, we denote M = maxi |Di|, m =
mini |Di|, ∀i ∈ N and κ = L
µ be the condition number of
the global loss function F . The main convergence result of
AirComp-based DSGT-VR is established as follows,

N 1N 1(cid:62)

W − 1

(cid:57)

(cid:57)

N

Theorem 1. Let θ(cid:92) denote the solution of the distributed
optimization problem (1). If Assumptions 1 and 2 hold and
the step-size in AirComp-based DSGT-VR algorithm is such
that

α = min{O(

1
µM

), O(

m(1 − β)2
M Lκ2

)},

then ∀t ≥ 0, ∀i ∈ N and for some 0 < ρ < 1, it holds that

E (cid:2)F (θt

i )(cid:3) − F (θ(cid:92)) ≤

cL
2

ρt +

LN
2(N − 1)

dσ2B2
γ2P

t
(cid:88)

τ =1

ρt−τ ,

(12)

where c = N

N −1 (cid:107)θ0

i − θ

0

(cid:107)2 + N (cid:107)θ

0

− θ(cid:92)(cid:107)2.

Proof. See Appendix A.

Theorem 1 establishes the upper bound of estimation er-
ror for strongly convex and smooth local loss function in
decentralized federated learning system. The error bound is
divided into the initial distance due to the error in original
algorithm and the additive noise caused by the channel noise.
In addition, the initial distance shows that the AirComp-based
DSGT-VR achieves the same convergence rate as that of the
DSGT-VR algorithm, i.e., linear convergence rate. Theorem 1
demonstrates that impact of the additive noise decreases as the
number of devices increases.

IV. NUMERICAL EXPERIMENTS

In this section, we numerically demonstrate the convergence
behavior of the proposed AirComp-based DSGT-VR, and
compare with the-state-of-art decentralized FL algorithms.
We consider a decentralized federated learning setting where
N = 20 devices with 1000 data samples locally at each device
cooperatively train a regularized logistic regression model for
binary classiﬁcation,

F (θ) =

1
N

N
(cid:88)

i=1

1
mi

mi(cid:88)

j=1

log[1 + e−(aT

ij θ)bij ] +

λ
2

(cid:107)x(cid:107)2
2,

(13)

where device i holds mi training samples {aij, bij}mi
j=1, aij
denotes the feature of j-th training sample at i-th device,
bij ∈ {+1, −1} is the corresponding binary label and λ is
a regularization parameter.

For data sample, we use classes “3” and “5” in the MNIST
dataset for binary classiﬁcation. We use 1000 samples for
training and 1968 samples for testing, and each device is
assigned by 40 training samples. We use the channel ht
ij ∼
CN (0, 1), noise σ2 to be 0 dBm and the threshold γ = 0.5. In
this paper, we adopted Laplacian matrix [38] as mixing matrix
for fast and provable convergence analysis. To stable the
training process, all features are normalized to be unit vectors,

Fig. 2: Optimality gap versus number
of consensus iteration t.

Fig. 3: Test accuracy versus number
of consensus iteration t.

Fig. 4: Optimality gap versus number
of consensus iteration t with different N .

(cid:80)N

1
i=1 mi

i.e., (cid:107)aij(cid:107)2 = 1, ∀i, j, and set the regularization parameter
λ =
. And we characterize the performance in terms
of the optimality gap, i.e., E [F (θt
i )]−F (θ(cid:92)). To further verify
the effectiveness of the proposed, we compare the proposed
algorithm with the AirComp-based DSGD algorithm [22],
[23], which is the state-of-the-art algorithm of the wireless
decentralized federated edge learning system.

Fig. 2 shows that both AirComp-based DSGD and the
proposed algorithm achieve a linear convergence rate, but the
proposed algorithm can converge to a more accurate solution.
Specially,
the error gap of AirComp-based DSGD almost
reaches 10−4, while the proposed algorithm only has the
optimality gap of 10−10. However, due to the effect by fading
channels and additive noise, there is still a gap between the
solution obtained by proposed algorithm and optimal solution,
which corresponds to our theoretical results. And the test
accuracy is evaluated versus the number of consensus iteration
is illustrated in Fig. 3. Obviously, the proposed algorithm out-
performs the AirComp-based DSGD algorithm while reaches
the nearly the same accuracy of DSGT-VR algorithm.

In Fig 4, we illustrate the impact of the different number
of devices on optimality gap of the proposed algorithm in the
same network topology. Clearly, the proposed algorithm with
20 devices reaches a more accurate solution than that with 5
devices, which veriﬁes our theoretical results. However, the
AirComp-based DSGD has the opposite result that the more
devices the less accurate solution.

V. CONCLUSIONS

In this paper, we proposed a novel AirComp-based DSGT-
VR algorithm to facilitate the consensus phase in decentralized
FL over wireless networks, where both precoding and decod-
ing strategies at devices are developed for D2D communica-
tions. Furthermore, we proved the linear convergence rate of
the proposed algorithm and provided the error bound to reveal
the impact of fading channel and its noise. Simulation results
were conducted to verify the theoretical result and the superior
performance of the proposed algorithm.

APPENDIX A
PROOF OF THEOREM 1

Under assumption 1, the global cost F is also L-smooth and
we denote θ(cid:92) is the optimal solution of F , then by smoothness

E (cid:2)F (θt

i )(cid:3) − F (θ(cid:92)) ≤

L
2

E (cid:2)(cid:107)θt

i − θ(cid:92)(cid:107)2(cid:3) .

(14)

First, we introduce the auxiliary variable {θ
mean of θt
E (cid:2)(cid:107)θt

i − θ(cid:92)(cid:107)2(cid:3) , ∀i ∈ N turn out to be

N in error-free case,

1, · · · , θt

t

} which is
then the error

t

t

− θ(cid:92)(cid:107)2(cid:105)

+ θ

E (cid:2)(cid:107)θt

(a)
≤

i − θ(cid:92)(cid:107)2(cid:3) = E
(cid:104)
N
N − 1
(cid:124)

(cid:107)θt

E

i − θ

(cid:104)
(cid:107)θt
i − θ
(cid:107)2(cid:105)
(cid:125)

t

+ N E
(cid:124)

(cid:123)(cid:122)
T1

(cid:104)
(cid:107)θ

t

− θ(cid:92)(cid:107)2(cid:105)
(cid:123)(cid:122)
(cid:125)
T2

,

(15)

where (a) comes from Young’s inequality that (cid:107)a + b(cid:107)2 ≤
η )(cid:107)b(cid:107)2, ∀a, b ∈ Rd, ∀η ≥ 0 and we
(1 + η)(cid:107)a(cid:107)2 + (1 + 1
i − θ(cid:92)(cid:107)2(cid:3) is divided
set η = N − 1. Then the error E (cid:2)(cid:107)θt
into two parts, i.e., T1 and T2. Next, we focus on establish
the upper bounds for T1 and T2. Recall (11), we have θt
i =
(cid:107)2(cid:105)
t
(cid:80)

, then we can rewrite E

(cid:107)θt

(cid:104)

i − θ

j∈Ni

wijθt−1
in T1 as follows

j + (cid:101)zt−1

i

E

(cid:104)
(cid:107)θt

i − θ

t

(cid:107)2(cid:105) (a)

= E



(cid:107)

(cid:88)

j∈Ni

wijθt−1

j − θ

t

(cid:107)2


 + E (cid:2)(cid:107)(cid:101)zt−1

i

(cid:107)2(cid:3) ,

(b)
≤ ρE

(cid:104)

(cid:107)θt−1

i − θ

t−1

(cid:107)2(cid:105)

+

dσ2B2
γ2P

,

i

where (a) comes from that noise (cid:101)zt−1
is zero-mean and
independent to model parameters and (b) comes from [39,
Theorem 1, Lemma 2] with 0 < ρ < 1 and the uniform scaling
factor (cid:112)pt, i.e., E (cid:2)(cid:107)(cid:101)zt−1
(cid:107)2(cid:3) = dσ2
γ2P . According to
i
µM ), O( m(1−β)2
[37, Propositon 1], then if α = min{O( 1
M Lκ2 )},
we have T2 ≤ ρE

pt ≤ dσ2B2

. Hence, we get

− θ(cid:92)(cid:107)2(cid:105)

N (cid:107)θ

t−1

(cid:104)

E (cid:2)(cid:107)θt

i − θ(cid:92)(cid:107)2(cid:3) ≤ cρt +

N
N − 1

dσ2B2
γ2P

t
(cid:88)

τ =1

ρt−τ ,

02500500075001000012500150001750020000 , W H U D W L R Q10−1510−1310−1110−910−710−510−310−1101 2 S W L P D O L W \  J D S ' 6 * 7  9 5 $ L U & R P S  E D V H G  ' 6 * ' $ L U & R P S  E D V H G  ' 6 * 7  9 50100200300400500 , W H U D W L R Q0.600.650.700.750.800.850.900.951.00 7 H V W  D F F X U D F \ ' 6 * 7  9 5 $ L U & R P S  E D V H G  ' 6 * ' $ L U & R P S  E D V H G  ' 6 * 7  9 502500500075001000012500150001750020000 , W H U D W L R Q10−1110−910−710−510−310−1101 2 S W L P D O L W \  J D S $ L U & R P S  E D V H G  ' 6 * '  1     $ L U & R P S  E D V H G  ' 6 * '  1    $ L U & R P S  E D V H G  ' 6 * 7  9 5  1     $ L U & R P S  E D V H G  ' 6 * 7  9 5  1   where c = N
N −1 (cid:107)θ0
Based on (14), we can obtain

i − θ

(cid:107)2 + N (cid:107)θ

0

0

− θ(cid:92)(cid:107)2 and 0 < ρ < 1.

E (cid:2)F (θt

i )(cid:3) − F (θ(cid:92)) ≤

cL
2

ρt +

LN
2(N − 1)

dσ2B2
γ2P

t
(cid:88)

τ =1

ρt−τ ,

(16)

which completes the proof.

REFERENCES

[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Proc. Int. Conf. Artif. Intell. and Statist. (AISTATS). PMLR,
2017, pp. 1273–1282.

[2] S. A. Rahman, H. Tout, H. Ould-Slimane, A. Mourad, C. Talhi, and
M. Guizani,
“A survey on federated learning: The journey from
centralized to distributed on-site learning and beyond,” IEEE Internet
of Things J., Apr. 2021.

[3] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-
efﬁcient edge ai: Algorithms and systems,” IEEE Commun. Surveys
Tuts., vol. 22, no. 4, pp. 2167–2191, Oct. 2020.

[4] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and
federated learning for privacy-preserved data sharing in industrial IoT,”
IEEE Trans. Industr. Inform., vol. 16, no. 6, pp. 4177–4186, Jun. 2020.
[5] Q. V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu, T. Huynh-
“Fusion of federated learning and industrial internet of

The, et al.,
things: A survey,” arXiv preprint arXiv:2101.00798, 2021.

[6] S. Savazzi, M. Nicoli, M. Bennis, S. Kianoush, and L. Barbieri, “Oppor-
tunities of federated learning in connected, cooperative, and automated
industrial systems,” IEEE Commun. Mag., Mar. 2021.

[7] Z. Yang, M. Chen, K. Wong, H. Poor, and S. Cui, “Federated learning
for 6G: Applications, challenges, and opportunities,” arXiv preprint
arXiv:2101.01338, 2021.

[8] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y. C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks:
A comprehensive survey,” IEEE Commun. Surveys Tuts., vol. 22, no. 3,
pp. 2031–2063, Apr. 2020.

[9] S. Niknam, H. S. Dhillon, and J. H. Reed, “Federated learning for
wireless communications: Motivation, opportunities, and challenges,”
IEEE Commun. Mag., Jul. 2020.

[10] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y. A. Zhang, “The roadmap
to 6G: AI empowered wireless networks,” IEEE Commun. Mag., vol.
57, no. 8, pp. 84–90, Aug. 2019.

[11] K. Yang, Y. Shi, Y. Zhou, Z. Yang, L. Fu, and W. Chen, “Federated ma-
chine learning for intelligent IoT via reconﬁgurable intelligent surface,”
IEEE Netw., vol. 34, no. 5, pp. 16–22, Oct. 2020.

[12] L. Li, L. Yang, X. Guo, Y. Shi, H. Wang, W. Chen, and K. B.
Letaief,
“Delay analysis of wireless federated learning based on
saddle point approximation and large deviation theory,” arXiv preprint
arXiv:2103.16994, 2021.

[13] Z. Wang, J. Qiu, Y. Zhou, Y. Shi, L. Fu, W. Chen, and K. B. Lataief,
“Federated learning via intelligent reﬂecting surface,” arXiv preprint
arXiv:2011.05051, 2020.

[14] M. Chen, H. Vincent Poor, W. Saad, and S. Cui, “Convergence time
optimization for federated learning over wireless networks,” IEEE Trans.
Wireless Commun., vol. 20, no. 4, pp. 2457–2471, Apr. 2021.

[15] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint
learning and communications framework for federated learning over
wireless networks,” IEEE Trans. Wireless Commun., vol. 20, no. 1,
pp. 269–283, Jan. 2021.

[16] D. Wen, M. Bennis, and K. Huang, “Joint parameter-and-bandwidth
allocation for improving the efﬁciency of partitioned edge learning,”
IEEE Trans. Wireless Commun., vol. 19, no. 12, pp. 8272–8286, Dec.
2020.

[17] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-
the-air computation,” IEEE Trans. Wireless Commun., vol. 19, no. 3,
pp. 2022–2035, Mar. 2020.

[18] T. Sery and K. Cohen,

multiple access fading channels,”
68, pp. 2897–2911, Apr. 2020.

“On analog gradient descent learning over
IEEE Trans. Signal Process., vol.

[19] M. Mohammadi Amiri and D. G¨und¨uz, “Machine learning at the wire-
less edge: Distributed stochastic gradient descent over-the-air,” IEEE
Trans. Signal Process., vol. 68, pp. 2155–2169, Mar. 2020.

[20] X. Lian, C. Zhang, H. Zhang, C. Hsieh, W. Zhang, and J. Liu, “Can
decentralized algorithms outperform centralized algorithms? a case study
for decentralized parallel stochastic gradient descent,” in Proc. Neural
Inf. Process. Syst. (NeurIPS), 2017, pp. 5330–5340.

[21] X. Cai, X. Mo, J. Chen, and J. Xu, “D2D-enabled data sharing for
distributed machine learning at wireless network edge,” IEEE Wireless
Commun. Lett., vol. 9, no. 9, pp. 1457–1461, Sept. 2020.

[22] E. Ozfatura, Stefano Rini, and D. G¨und¨uz, “Decentralized SGD with
in Proc. IEEE Global Commun. Conf.

over-the-air computation,”
(Globecom), 2020, pp. 1–6.

[23] H. Xing, O. Simeone, and S. Bi, “Decentralized federated learning via
SGD over wireless D2D networks,” in Proc. IEEE Int. Workshop Signal
Process. Adv. Wireless Commun. (SPAWC), 2020, pp. 1–5.

[24] S. Savazzi, S. Kianoush, V. Rampa, and M. Bennis, “A joint decentral-
ized federated learning and communications framework for industrial
networks,” in Proc. IEEE Int. Workshop Comput.-Aided Model., Anal.,
and Dess. Commun. Links and Netw. (CAMAD), 2020, pp. 1–7.
[25] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al.,
“Advances and open problems in federated learning,” arXiv preprint
arXiv:1912.04977, 2019.

[26] A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. U. Stich, “A
uniﬁed theory of decentralized sgd with changing topology and local
updates,” arXiv preprint arXiv:2003.10422, 2020.

[27] R. Xin, S. Kar, and U. A. Khan, “Decentralized stochastic optimization
and machine learning: A uniﬁed variance-reduction framework for robust
performance and fast convergence,” IEEE Signal Process. Mag., vol. 37,
no. 3, pp. 102–113, May 2020.

[28] S. Pu and A. Nedi´c, “Distributed stochastic gradient tracking methods,”

Math. Program., pp. 1–49, 2020.

[29] M. N. Tehrani, M. Uysal, and H. Yanikomeroglu, “Device-to-device
communication in 5G cellular networks: challenges, solutions, and future
directions,” IEEE Commun. Mag., vol. 52, no. 5, pp. 86–92, May 2014.
[30] M. S. Molloy, M. Molloy, and B. Reed, Graph colouring and the
probabilistic method, vol. 23, Springer Science & Business Media, 2002.
[31] G. Zhu, J. Xu, and K. Huang, “Over-the-air computing for 6G–turning

air into a computer,” arXiv preprint arXiv:2009.02181, 2020.

[32] Z. Wang, Y. Shi, Y. Zhou, H. Zhou, and N. Zhang,

“Wireless-
powered over-the-air computation in intelligent reﬂecting surface-aided
IoT networks,” IEEE Internet of Things J., vol. 8, no. 3, pp. 1585–1598,
Feb. 2021.

[33] J. Dong, Y. Shi, and Z. Ding, “Blind over-the-air computation and data
fusion via provable wirtinger ﬂow,” IEEE Trans. Signal Process., vol.
68, pp. 1136–1151, Jan. 2020.

[34] L. Ruan and V. K. N. Lau, “Dynamic interference mitigation for gen-
eralized partially connected quasi-static MIMO interference channel,”
IEEE Trans. Signal Process., vol. 59, no. 8, pp. 3788–3798, Aug. 2011.
[35] L. Ruan, V. K. N. Lau, and X. Rao, “Interference alignment for partially
connected MIMO cellular networks,” IEEE Trans. Signal Process., vol.
60, no. 7, pp. 3692–3701, Jul. 2012.

[36] S. A. Jafar,

“Topological

through index
coding,” IEEE Trans. Inf. Theory, vol. 60, no. 1, pp. 529–568, Oct.
2014.

interference management

[37] R. Xin, U. A. Khan, and S. Kar,

stochastic optimization with accelerated convergence,”
Signal Process., vol. 68, pp. 6255–6271, Oct. 2020.

“Variance-reduced decentralized
IEEE Trans.

[38] L. Xiao and S. Boyd, “Fast linear iterations for distributed averaging,”

Systems & Control Letters, vol. 53, no. 1, pp. 65–78, 2004.

[39] G. Qu and N. Li,

“Harnessing smoothness to accelerate distributed
optimization,” IEEE Trans. Control. Netw. Syst., vol. 5, no. 3, pp. 1245–
1260, Sept. 2018.

