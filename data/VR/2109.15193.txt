AIive: Interactive Visualization and Soniﬁcation of Neural Networks
in Virtual Reality

Zhuoyue Lyu
Department of Computer Science
University of Toronto
Toronto, Canada
zhuoyue@dgp.toronto.edu

Jiannan Li
Department of Computer Science
University of Toronto
Toronto, Canada
jiannanli@dgp.toronto.edu

Bryan Wang
Department of Computer Science
University of Toronto
Toronto, Canada
bryanw@dgp.toronto.edu

1
2
0
2

t
c
O
2
2

]

C
H
.
s
c
[

2
v
3
9
1
5
1
.
9
0
1
2
:
v
i
X
r
a

sounds as training progresses, and even responds to the
users’ interaction with it.

To instantiate this concept, we present AIive, an interactive
visualization that brings AI “alive”, it leverages basic human
sensory and motor activities: seeing, listening, and grabbing
and moving objects for an intuitive, artistic, and enjoyable
experience. Given the beneﬁts of immersive analytics [9],
[10], data visualization and soniﬁcation in 3D [11]–[13],
we extend the 2D implementation of Immersions [4], and
visualize NNs as 3D force-directed graphs in VR that
allow users to experience the training of the model and
manipulate its architecture using virtual hands. We also
provide the soniﬁcation of the accuracy, loss, learning rate,
and momentum for real-time hyperparameter tuning.

II. RELATED WORK

A. Explainable AI and Democratizing AI

The hope of improving AI systems’ transparency and
accessibility triggered the research of explainable AI [7],
[8] and democratization of AI [14]. For example, the Au-
toAI/AutoML by IBM [15] and H2O.ai [16] automate the
end-to-end AI lifecycle to save data scientists from the low-
level coding tasks; The TensorFlow Playground [17] and
GAN Lab [18] by Google allows direct-manipulation on the
in-browser visualizations to help non-experts learn NNs and
GANs. Those tools either decrease the experts’ workload
or help non-experts learn AI, thus requiring numeracy and
graph literacy. AIive however, focus on making AI training
experienceable, thus only relying on basic sensory activities:
sight, hearing, and touch in VR, which would potentially
reach a broader audience.

B. Immersive Analytics and Exploration

Immersion provides beneﬁts such as increased spatial
understanding, decreased information clutter [9], and the
“sense of being there”, which is closely associated with
satisfaction and appealing experience [10]. VR headsets
have been used to provide immersion: DataHop enables
users to layout data analysis steps in VR [19]; AeroVR
provides an immersive environment to aid the aerodynamic

Figure 1. The AIive system, with the neural network at the center visualized
as an interactive force-directed graph. zhuoyuelyu.com/AIive

Abstract—Artiﬁcial Intelligence (AI), especially Neural Net-
works (NNs), has become increasingly popular. However, people
usually treat AI as a tool, focusing on improving outcome,
accuracy, and performance while paying less attention to the
representation of AI itself. We present AIive, an interactive vi-
sualization of AI in Virtual Reality (VR) that brings AI “alive”.
AIive enables users to manipulate the parameters of NNs with
virtual hands and provides auditory feedback for the real-time
values of loss, accuracy, and hyperparameters. Thus, AIive
contributes an artistic and intuitive way to represent AI by
integrating visualization, soniﬁcation, and direct manipulation
in VR, potentially targeting a wide range of audiences.

Keywords-artiﬁcial

intelligence; virtual

reality; human-

computer interaction; soniﬁcation; visualization;

I. INTRODUCTION

AI has led to breakthroughs in areas such as image
classiﬁcation, object detection, and machine translation [1].
However, most research treats AI as a tool to accomplish
tasks, focusing on boosting outcome and performance, with
little attention paid to exploring the representation of NNs
itself [2]–[5]. Meanwhile, the lack of transparency and inter-
pretability of AI is a problem that needs to be addressed [6]–
[8]. In this paper, we explore the concept of representing AI
as a living being to uncover the inner workings of NNs in
an experienceable way: it moves, changes colors, creates

 
 
 
 
 
 
design [20], and VanHorn et al. developed a deep learning
development environment in VR [21]. VR has also been
used to teach programming [22]–[24], and students found
it more user-friendly, engaging, and better for visualization
concepts compared to the traditional web-based system [23].
Thus, we built AIive in VR to leverage those beneﬁts.

C. Data Visualization and Soniﬁcation in 3D

3D data visualization provides the expanded domain of
sensibility [11], enabling a more comprehensive under-
standing of presented information: “data objects”, “data
sculptures” were built to make data experienceable; Game-
like infographics from datasets were made for playable and
engaging experiences [25]. Previous studies have shown
that people found 3D visualization to be more satisﬁed
and have lower workloads than the 2D counterpart [26].
Data soniﬁcation has been used in various ﬁelds such as
social sciences [27], [28], arts [29], and health [30], it can
enhance visual representations without creating information
overload [12], thus suitable for conveying dynamical infor-
mation [13]. There have been attempts to combine soniﬁca-
tion with 3D visualization for representing the connectome
of the human brain [13], communicating sensor data in
workspaces [31], and assisting music composition [32].
AIive builds upon the node-link visualization [2]–[5] and
focuses on sonifying parameters and performances of NNs.

III. AIIVE INTERFACE

A. Visualization

As shown in Figure 2, the NN is visualized using the
node-link approach: (a) Nodes, shown as glowing spheres,
represent neurons in the network. Blue nodes represent the
ﬁrst hidden layer; yellow nodes represent the second hidden
layer. (b) Links, shown as thin white lines between nodes,
represent the weights (Wi) between two neurons, with their
transparency reﬂecting the magnitude of the weight: the
smaller the weight, the more transparent the link. Limited by
the computing power of the VR headset, also for simplicity
purposes, we render the input (48 × 48) as a single node in
red, the output (seven categories) as a single node in green.
We use a force-directed graph to represent the network.
Speciﬁcally, the graph ﬂoats in a zero-gravity environment,
with no energy loss. There are two types of forces between
every two nodes (i, j), the attractive (FAij) and repulsive
(FRij) force. The forces are deﬁned as follows, where ka
and kr being adjustable coefﬁcients and Wij being the real-
time weight between i and j:

−→
FAij = ka ×

−→
W ij

−→
FRij =

kr
distance2(i, j)

For node pairs (i, j) in the same layer, there is no link and
weight between them; thus, we use uniform weights with
|Wij| = 1 to calculate their attraction. We normalized the
value of weights (Wij) to mitigate the impact when weights
vary dramatically among neurons.

Figure 2. The visualization of the neural network with three neurons in
the ﬁrst hidden layer (blue) and four neurons in the second hidden layer
(yellow). The red node and the green node are input and output, respectively.

B. Soniﬁcation

Finding suitable mappings between the space of data and
the space of sounds is conceptual [32]. For simplicity, we
map the values of validation Accuracy and Loss in each
epoch directly to the frequency of sine wave oscillators,
using a Unity plug-in Chunity [33]. Our system plays the
soniﬁcation of Accuracy on both channels by default, but
the user can choose to listen to both Accuracy and Loss at
the same time (with Accuracy on the right channel, Loss on
the left), or only the sound of Loss on both channels.

C. Interaction

AIive supports three types of interactions to update the
model’s number of neurons, learning rate & momentum, and
weights of a single neuron (Figure 3).

1) Number of Neurons: To update the number of neurons,
the user can move the left hand close to the graph’s center
of mass (< 5-unit distance in Unity), which would trigger
the appearance of a small sphere, indicating the graph is
paused. A “Paused” message will be sent to the backend
to stop the training. After that, the user can use the right
hand to approach one of the hidden layers. Once the right
hand enters the threshold of 3-unit distance, a small sphere
in the respective layer’s color would appear at its center,
highlighting that the layer will be manipulated. If the user
moves the right hand further closer to the layer’s center of
mass (< 1-unit distance), a new node will be generated, and
the user can drag it to the desired position. To delete a node,
the user can drag it back to its layer’s center. Once they have
done updating, the user can put the left hand down, and the
training would restart with the updated number of neurons.
2) Learning Rate and Momentum: Once the training
begins, the user can adjust the learning rate by placing the
right hand in the mid-air and the left hand close to the
left ear. A white sphere would then appear at the network’s
center, indicating the training is paused. The user can then
adjust the magnitude of learning rates by lifting (increasing)
or dropping (decreasing) the right hand. The new learning
rate’s real-time value will be soniﬁed in the same way as
Accuracy and Loss described in Section III-B. Likewise, the
user can adjust the momentum by placing the right hand
close to the right ear and lifting/dropping the left hand. Once

Figure 3. Tuning momentum and learning rate (A, E) through hand movements; Exploring different weights by dragging nodes around (B); Updating the
number of neurons by pulling new nodes out of the center (C) or dumping old nodes into the center (D).

the user ﬁnishes updating and puts both hands down, the
training would resume with updated hyperparameters.

3) Weights of a Single Neuron: We also investigate the
idea of updating weights of neurons manually, as opposed
to using gradient descent. Similar to the way of updating the
number of neurons in Section III-C1, the user can pause the
training with the left hand, then drag any existing node in the
graph with the right hand. Since the operation would change
the distances between the dragged node and the nodes that
it connects with, the weights associated with those nodes
will be updated and sent to the backend for evaluation. To
guide the user on selecting the desired position, the real-time
soniﬁcations of Accuracy and Loss are provided, the user
can move the node around and ﬁnd the sweet spot where
the pitch of Accuracy sounds high (or low, in the case of
Loss) and release the node. After the user puts down the left
hand, the training would resume with the updated weights.

IV. IMPLEMENTATION

A. Apparatus

The NN model (back-end) runs in the python terminal of
a laptop, the 3D visualization and soniﬁcation (front-end)
was built as a Unity [34] project into the Oculus Quest
VR headset [35]. The communication between them was
accomplished through TCP connections [36] wirelessly.

B. Model

We deployed our system on a simple fully-connected NN
in python, based on an intro to machine learning course’s
materials [37]. The dataset provided to the model
is a
subset of the Toronto Faces Dataset (TFD) [38], with 3374,
419, and 385 grayscale images from TFD as the training,
validation, and testing set, respectively. The network has two
hidden layers, each with an adjustable number of neurons.
The structure of the network, as well as cross-entropy loss
(L), are shown as follows, where t, y, h1, h2, x represents
the targets, outputs, the ﬁrst hidden layer, the second hidden
layer, and inputs, respectively:

z1 = W1x + b1
z2 = W2h1 + b2
z3 = W3h2 + b3

h1 = ReLU(z1)
h2 = ReLU(z2)
y = Softmax(z3)

L = −t(cid:124)(log y)

In each training step (i), weights (Wi) are updated according
to the Stochastic Gradient Descent (SGD) [39], [40] on the
cross-entropy loss (L) with adjustable batch size, momentum
(µ) and learning rate (ε), where ∂L(cid:48)
represent the gradient
∂W (cid:48)
i
from the previous step, ∂L
represents the current gradient:
∂Wi

Wi ← Wi + µ

∂L(cid:48)
∂W (cid:48)
i

− ε

∂L
∂Wi

After each epoch (input size/batch size steps), the Accu-
racy and Loss of the model’s performance on validation sets
are calculated and sent back to the VR headset, together with
the new weights (Wi) among all neurons.

V. DISCUSSION AND FUTURE WORK

Our preliminary implementation and exploration with
AIive point out several promising future directions: (1) AIive
may potentially help users learn and understand neural
networks’ training concepts. Therefore, we plan to improve
the system and conduct comprehensive studies to evaluate
its educational beneﬁts; (2) Our system only supports simple
neural networks due to the limited computing power of VR
systems. Modern deep learning models typically consist of
tens or hundreds of thousands of neurons [41], it would be
interesting to investigate suitable designs for users to interact
with a larger number of neurons in VR environments; (3)
Since prior work has found different sounds could evoke
different emotional states of the listener [42], it is interesting
to explore how different soniﬁcation in terms of timbres,
pitches, and complexity would affect the user experience.

VI. CONCLUSION

it

This paper presents AIive, an interactive representation
that brings AI “alive”,
leverages visual and auditory
feedback to provide an artistic and intuitive way to interact
with AI. Since the system does not rely on numerical
values or scientiﬁc graphs, it could potentially reach a broad
audience. While AIive is still an early implementation, we
hope to share the core idea of representing AI through the
combination of 3D visualizations, soniﬁcation, and direct
manipulation in VR to the broader community. We look for-
ward to stimulating interesting conversations and to eliciting
useful feedback for our future development on AIive.

ACKNOWLEDGMENT

The authors would like to thank the anonymous reviewers
for their reviews and suggestions, Prof. Chris Chafe and
Prof. Tovi Grossman for their early feedbacks, and Prof.
Fernanda Vi´egas, Dr. Sławomir Tadeja, Fengyuan Zhu, Jiahe
Lyu, Kaiqu Liang, Chenxinran Shen for their comments.

REFERENCES

[1] D. Zhang, S. Mishra, E. Brynjolfsson, J. Etchemendy,
D. Ganguli, B. Grosz, T. Lyons, J. Manyika, J. C. Niebles,
M. Sellitto, Y. Shoham, J. Clark, and R. Perrault, “The
AI Index 2021 Annual Report,” 2021. [Online]. Available:
https://arxiv.org/abs/2103.06312

[2] A. W. Harley, “An Interactive Node-Link Visualization of
Convolutional Neural Networks,” in Advances in Visual
Computing, G. Bebis, R. Boyle, B. Parvin, D. Koracin,
I. Pavlidis, R. Feris, T. McGraw, M. Elendt, R. Kopper,
E. Ragan, Z. Ye, and G. Weber, Eds. Cham: Springer
International Publishing, 2015, pp. 867–877.
[Online].
Available: https://doi.org/10.1007/978-3-319-27857-5 77

[3] M. Bellgardt, C. Scheiderer, and T. W. Kuhlen, “An
Immersive Node-Link Visualization of Artiﬁcial Neural
Networks for Machine Learning Experts,” in 2020 IEEE
International Conference on Artiﬁcial
Intelligence and
Virtual Reality (AIVR), 2020, pp. 33–36. [Online]. Available:
https://doi.org/10.1109/AIVR50618.2020.00015

[4] V. Herrmann, “Visualizing and sonifying how an artiﬁcial
ear hears music,” in Proceedings of
the NeurIPS 2019
Competition and Demonstration Track, ser. Proceedings of
Machine Learning Research, H. J. Escalante and R. Hadsell,
Eds., vol. 123.
PMLR, 08–14 Dec 2020, pp. 192–
202. [Online]. Available: https://proceedings.mlr.press/v123/
herrmann20a.html

[5] A. Zell, N. Mache, R. H¨ubner, G. Mamier, M. Vogt,
M. Schmalzl, and K.-U. Herrmann, SNNS (Stuttgart Neural
Network Simulator). Boston, MA: Springer US, 1994,
pp. 165–186.
[Online]. Available: https://doi.org/10.1007/
978-1-4615-2736-7 9

[6] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau, “Visual
Analytics in Deep Learning: An Interrogative Survey for
the Next Frontiers,” IEEE Transactions on Visualization
and Computer Graphics, vol. 25, no. 8, pp. 2674–2693,
[Online]. Available: https://doi.org/10.1109/TVCG.
2019.
2018.2843369

[7] F. K. Doˇsilovi´c, M. Brˇci´c, and N. Hlupi´c, “Explainable
artiﬁcial intelligence: A survey,” in 2018 41st International
Convention on Information and Communication Technology,
Electronics and Microelectronics (MIPRO), 2018, pp. 0210–
0215. [Online]. Available: https://doi.org/10.23919/MIPRO.
2018.8400040

[8] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti,
and D. Pedreschi, “A Survey of Methods for Explaining
Black Box Models,” ACM Comput. Surv., vol. 51, no. 5, Aug.
2018. [Online]. Available: https://doi.org/10.1145/3236009

[9] D. A. Bowman and R. P. McMahan, “Virtual Reality:
How Much Immersion Is Enough?” Computer, vol. 40,
no. 7, pp. 36–43, July 2007. [Online]. Available: https:
//doi.org/10.1109/MC.2007.257

[10] S. Sylaiou, K. Mania, A. Karoulis, and M. White,
“Exploring the relationship between presence and enjoyment
in a virtual museum,” International Journal of Human-
Computer Studies, vol. 68, no. 5, pp. 243–253, 2010.
[Online]. Available: https://www.sciencedirect.com/science/
article/pii/S1071581909001761

[11] M. B. Fazi, “Feed-Forward: On the Future of Twenty-First-
[Online]. Available: https://press.

Century Media,” 2016.
uchicago.edu/ucp/books/book/chicago/F/bo19211873.html

[12] G. Kramer, B. Walker, T. Bonebright, P. Cook, J. H. Flowers,
N. Miner, and J. Neuhoff, “Soniﬁcation report: Status of
the ﬁeld and research agenda,” 2010. [Online]. Available:
https://digitalcommons.unl.edu/psychfacpub/444/

[13] P. Papachristodoulou, A. Betella,

and P. Verschure,
“Soniﬁcation of Large Datasets
in a 3D Immersive
Environment: A Neuroscience Case Study,” in ACHI
2014, 2014. [Online]. Available: http://www.thinkmind.org/
download.php?articleid=achi 2014 2 20 20146

[14] A. Sudmann, Ed., The Democratization of Artiﬁcial
of Learning
transcript Verlag, 2020. [Online]. Available:

Intelligence: Net Politics
Algorithms.
https://doi.org/10.14361/9783839447192

the Era

in

[15] D. Wang, P. Ram, D. K. I. Weidele, S. Liu, M. Muller, J. D.
Weisz, A. Valente, A. Chaudhary, D. Torres, H. Samulowitz,
and L. Amini, “AutoAI: Automating the End-to-End AI
Lifecycle with Humans-in-the-Loop,” in Proceedings of the
25th International Conference on Intelligent User Interfaces
Companion, ser. IUI ’20. New York, NY, USA: Association
for Computing Machinery, 2020, p. 77–78.
[Online].
Available: https://doi.org/10.1145/3379336.3381474

[16] A. Candel, V. Parmar, E. LeDell,

and A. Arora,
“Deep learning with H2O,” H2O. ai
Inc, pp. 1–
21, 2016. [Online]. Available: https://web.archive.org/web/
20211018215342/https://www.h2o.ai/

[17] D. Smilkov, S. Carter, D. Sculley, F. B. Vi´egas, and
M. Wattenberg, “Direct-Manipulation Visualization of Deep
Networks,” 2017. [Online]. Available: https://arxiv.org/abs/
1708.03788

[18] M. Kahng, N. Thorat, D. H. Chau, F. B. Vi´egas, and
M. Wattenberg, “GAN Lab: Understanding Complex Deep
Generative Models using Interactive Visual Experimentation,”
IEEE Transactions on Visualization and Computer Graphics,
vol. 25, no. 1, pp. 310–320, Jan 2019. [Online]. Available:
https://doi.org/10.1109/TVCG.2018.2864500

[19] D. Hayatpur, H. Xia, and D. Wigdor, “DataHop: Spatial
Data Exploration in Virtual Reality,” in Proceedings of the
33rd Annual ACM Symposium on User Interface Software
and Technology, ser. UIST ’20. New York, NY, USA:
Association for Computing Machinery, 2020, p. 818–828.
[Online]. Available: https://doi.org/10.1145/3379337.3415878

[20] S. Tadeja, P. Seshadri, and P. Kristensson, “AeroVR: An
immersive visualisation system for aerospace design and
digital twinning in virtual reality,” The Aeronautical Journal,
vol. 124, no. 1280, p. 1615–1635, 2020. [Online]. Available:
https://doi.org/10.1017/aer.2020.49

[30] W. R Michael, A. Kalra, and B. N. Walker, “Hearing
artiﬁcial
intelligence: Soniﬁcation guidelines & results
from a case-study in melanoma diagnosis.” Georgia
Institute of Technology, 2019. [Online]. Available: https:
//doi.org/10.21785/icad2019.021

[21] K. C. VanHorn, M. Zinn, and M. C. Cobanoglu, “Deep
in Virtual Reality,”

Learning Development Environment
2019. [Online]. Available: https://arxiv.org/abs/1906.05925

[22] M. Chandramouli, M. Zahraee, and C. Winer, “A fun-learning
approach to programming: An adaptive Virtual Reality (VR)
platform to teach programming to engineering students,”
in IEEE International Conference on Electro/Information
Technology, June 2014, pp. 581–586. [Online]. Available:
https://doi.org/10.1109/EIT.2014.6871829

[23] J.

Pirker,

J. Kopf, A. Kainz, A. Dengel,

and
B. Buchbauer, “The Potential of Virtual Reality for
Computer Science Education -Engaging Students through
Immersive Visualizations,” in 2021 IEEE Conference on
Virtual Reality and 3D User Interfaces Abstracts and
Workshops (VRW), March 2021, pp. 297–302.
[Online].
Available: https://doi.org/10.1109/VRW52623.2021.00060

[24] S. Paul and S. Hamad, “The Role of Virtual Reality
in Story telling and Data Visualization for motivating
in 2020 Seventh
students
International Conference on Information Technology Trends
(ITT), Nov 2020, pp. 169–173. [Online]. Available: https:
//doi.org/10.1109/ITT51279.2020.9320876

in learning programming,”

[25] N. Diakopoulos, F. Kivran-Swaine,

and M. Naaman,
“Playable Data: Characterizing the Design Space of
the SIGCHI
Game-y Infographics,” in Proceedings of
Conference on Human Factors
in Computing Systems,
’11. New York, NY, USA: Association for
ser. CHI
Computing Machinery, 2011, p. 1717–1726.
[Online].
Available: https://doi.org/10.1145/1978942.1979193

[26] P. Millais, S. L. Jones, and R. Kelly, “Exploring Data in
Virtual Reality: Comparisons with 2D Data Visualizations,”
the 2018 CHI Conference on
in Extended Abstracts of
Human Factors
in Computing Systems,
ser. CHI EA
’18. New York, NY, USA: Association for Computing
Machinery, 2018, p. 1–6.
[Online]. Available: https:
//doi.org/10.1145/3170427.3188537

[27] A. de Campo, Soniﬁcation of social data. Ann Arbor, MI:
Michigan Publishing, University of Michigan Library, 1999.
[Online]. Available: http://hdl.handle.net/2027/spo.bbp2372.
1999.397

[28] S. Nath, “Hear Her Fear: Data Soniﬁcation for Sensitizing
Society on Crime Against Women in India,” in IndiaHCI
’20: Proceedings of the 11th Indian Conference on Human-
Computer Interaction, ser. IndiaHCI 2020. New York, NY,
USA: Association for Computing Machinery, 2020, p. 86–91.
[Online]. Available: https://doi.org/10.1145/3429290.3429307

[29] R. Valenti, A. Jaimes, and N. Sebe, “Sonify Your Face:
Facial Expressions for Sound Generation,” in Proceedings
of the 18th ACM International Conference on Multimedia,
ser. MM ’10. New York, NY, USA: Association for
Computing Machinery, 2010, p. 1363–1372.
[Online].
Available: https://doi.org/10.1145/1873951.1874219

[31] G. Dublon, L. S. Pardue, B. Mayton, N. Swartz, N. Joliat,
P. Hurst, and J. A. Paradiso, “DoppelLab: Tools
for
exploring and harnessing multimodal sensor network data,”
in SENSORS, 2011 IEEE, Oct 2011, pp. 1612–1615. [Online].
Available: https://doi.org/10.1109/ICSENS.2011.6126903

[32] H. Kaper, E. Wiebel, and S. Tipei, “Data soniﬁcation and
sound visualization,” Computing in Science and Engineering,
vol. 1, no. 4, pp. 48–58, July 1999. [Online]. Available:
https://doi.org/10.1109/5992.774840

[33] J. Atherton and G. Wang, “Chunity: Integrated Audiovisual
Programming in Unity,” in NIME, 2018, pp. 102–107.
[Online]. Available: https://www.nime.org/proceedings/2018/
nime2018 paper0024.pdf

[34] Unity, “Unity Real-Time Development Platform — 3D,
2D VR and AR Engine.” [Online]. Available: https:
//web.archive.org/web/20211020091658/https://unity.com/

[35] Oculus, “Oculus Quest VR headset.” [Online]. Avail-
https://web.archive.org/web/20211018111646/https://

able:
www.oculus.com/quest/features/

[36] Wikipedia, “Transmission Control Protocol — Wikipedia,
https://en.wikipedia.org/wiki/
accessed
[Online;

the
Transmission Control Protocol, 2021,
20-October-2021].

encyclopedia,”

free

[37] R.

Grosse,

Introduc-
“CSC
to Machine
Avail-
https://web.archive.org/web/20211020015616/https://

tion
able:
www.cs.toronto.edu/∼rgrosse/courses/csc311 f20/

311
Fall
Learning.”

2020:
[Online].

[38] J. M. Susskind, A. K. Anderson, and G. E. Hinton,
“The toronto face database,” Department of Computer
Science, University of Toronto, Toronto, ON, Canada,
Tech. Rep, vol. 3, 2010.
[Online]. Available: https:
//zhuoyuelyu.github.io/AIive/toronto face.npz

[39] G.

Hinton,

N.
networks

Srivastava,
for machine

Swersky,
6a
“Neural
overview of mini-batch gradient descent,” Cited on,
vol.
[Online]. Available:
https://web.archive.org/web/20211018165553/https://www.cs.
toronto.edu/∼tijmen/csc321/slides/lecture slides lec6.pdf

and
learning

lecture

2012.

14,

no.

K.

8,

2,

p.

[40] S. Ruder, “An overview of gradient descent optimization
preprint
algorithms,”
2016.
[Online]. Available: https://arxiv.org/pdf/1609.04747.pdf

arXiv:1609.04747,

arXiv

[41] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,”
[Online]. Available:

vol. 521, no. 7553, pp. 436–444.
https://doi.org/10.1038/nature14539

[42] N. Sawe, C. Chafe,

and

J. Trevi˜no,

data
soniﬁcation
overcome
to
numeracy,
communication,”
and visualization barriers
Frontiers in Communication, vol. 5, p. 46, 2020. [Online].
Available: https://www.frontiersin.org/article/10.3389/fcomm.
2020.00046

science
in science

literacy,

“Using

