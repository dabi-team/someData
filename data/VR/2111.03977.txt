A Virtual Reality Simulation Pipeline for Online Mental Workload Modeling

Robert L. Wilson1, Daniel Browne1, Jon Wagstaﬀ1, Steve McGuire1∗

1
2
0
2

v
o
N
4
2

]

C
H
.
s
c
[

2
v
7
7
9
3
0
.
1
1
1
2
:
v
i
X
r
a

Abstract

Seamless human robot interaction (HRI) and cooperative
human-robot (HR) teaming critically rely upon accurate and
timely human mental workload (MW) models. Cognitive
Load Theory (CLT) suggests representative physical envi-
ronments produce representative mental processes; physical
environment ﬁdelity corresponds with improved modeling
accuracy. Virtual Reality (VR) systems provide immersive
environments capable of replicating complicated scenarios,
particularly those associated with high-risk, high-stress sce-
narios. Passive biosignal modeling shows promise as a non-
invasive method of MW modeling. However, VR systems
rarely include multimodal psychophysiological feedback or
capitalize on biosignal data for online MW modeling. Here,
we develop a novel VR simulation pipeline, inspired by the
NASA Multi-Attribute Task Battery II (MATB-II) task ar-
chitecture, capable of synchronous collection of objective
performance, subjective performance, and passive human
biosignals in a simulated hazardous exploration environment.
Our system design extracts and publishes biofeatures through
the Robot Operating System (ROS), facilitating real time
psychophysiology-based MW model integration into complete
end-to-end systems. A VR simulation pipeline capable of
evaluating MWs online could be foundational for advancing
HR systems and VR experiences by enabling these systems
to adaptively alter their behaviors in response to operator
MW.

Index Terms: Human Robot Interaction—Human Robot
Teaming—Virtual Reality—Cognitive Load Theory Men-
tal Workload—Psychophysiology—Biosignals—Multimodal
Modeling

1

Introduction

Frameworks for Mental Workload (MW) modeling, particu-
larly for implementation in human robot interaction (HRI)
and teaming, typically operate on limited biosignals and are
incapable of operating online, leaving room for vast improve-
ments in MW model development and evaluation. Human-
Robot (HR) teams, in both the physical and virtual domains,
are transitioning to peer-like paradigms where each team
member can dynamically respond to the current state of the
other [20, 22, 31]. In particular, improvements in MW for
high-risk environments (e.g., driving, surgery, exploration)
stand to improve safety and mission outcomes [19]. However,
data collection in these environments, a known strategy for
improving model outcomes [11], can be risky and quite costly
[10]. Virtual Reality (VR) is a key tool for improving MW
models through data collection closer to the operational en-
vironment, without the costs and risks associated with real
operations.

Model mismatches between HR teammates, often due to poor

∗1Department of Electrical and Computer Engineering, Uni-
versity of California Santa Cruz. Santa Cruz, CA. 95060 email:
robert.wilson@ucsc.edu

MW models, can risk detrimental outcomes. Human oper-
ators function optimally within individual workload bound-
aries [16, 45, 47]. Cognitive overload can lead to stress and
operational mistakes, while cognitive underload can result in
a lack of motivation or attention drift and decrease team per-
formance. Operator interruption at inopportune times can
increase cognitive demands and negatively impact mission
eﬃcacy [16].

Cognitive Load Theory (CLT), the preeminent model of hu-
man cognitive architecture, provides a theoretical framework
for characterizing and formalizing MW [34, 35, 42]. CLT
suggests MW is a composite of intrinsic, extrinsic, and ger-
mane loads. Intrinsic load relates to the eﬀort required to
learn a particular concept. The section of MW related to the
information presentation format deﬁnes the extrinsic load.
Germane load encompasses the processing involved in under-
standing information relevant to the task at hand. Diﬀerent
component combinations indicate various levels of MW. The
NASA Multi-Attribute Task Battery II (MATB-II) provides
a well-studied and useful structure for manipulating MW
[13, 40]. MATB-II consists of four main sections: Resource
Management (RM), tracking, System Monitoring (SM), and
communications. The diﬃculty of each MATB-II section
can be independently altered, allowing for ﬁne-grained ex-
perimental control. However, the use of MATB-II has been
mostly relegated to traditional on-screen testing [3, 18].

MW modeling predictions could improve via a robust mul-
timodal model based on passive biosignals integrated into
a VR system. Regardless of the chosen MW model (e.g.,
supervised learning, reinforcement learning), psychophysio-
logical analysis aims to map a hidden cognitive state (e.g.,
MW) to observable external manifestations by teasing out
concomitants (physical traits that map to many cognitive
functions) [5]. Multimodal biosensor models are on the rise,
but are rarely robust in design (i.e., include more than two
biosignals), allowing for limited model performance [11, 45].
CLT indicates data collection and downstream model perfor-
mance are likely to improve with the use of task relevant data
[12]. However, most MW work does not employ a relevant
task for data collection and training, introducing a major
disconnect between theory and practice. A VR environment
not only enables studies with higher task relevancy, which is
particularly useful for MW models aimed at high-risk scenar-
ios, but also facilitates online model use in addition to more
traditional oﬄine evaluation.

Here, we describe an online VR environment, speciﬁcally de-
signed to modulate MW, and the real-time data the pipeline
produces. A robust sweep of biosensors allows for multimodal
feature extraction. The VR framework, complete with realis-
tic motion cues, enables deployment and possible evaluation
of both closed (online) and open (oﬄine) MW models, in
task relevant settings. The VR pipeline facilitates passive
biosignal generation potentially closer to those that could be
elicited during the actual task, thus likely improving MW
model performance, particularly in high-risk scenarios.

2 Relevant Background

We constructed our VR model by combining multiple aspects
of research in both HRI and psychology into a multifaceted

 
 
 
 
 
 
Lunar Rover Simulation Environment

Online Data Collection

MATB-II Style Controls
Tracking
System Monitoring
Resource Management
Communications

Mental Workload
Intrinsic Load
Extrinsic Load
Germane Load

Cardiovascular

VR Exploration Task

Dermal Activity

Objective
Performance
Measures

Subjective
TLX
Survey

Derived
Biosignal
Features

Mental Workload Models
Multivariate Linear
Supervised Learning
Reinforcement Learning

Closed
Loop

Open
Loop

Biosensor Suite

Respiration

Empatica
E4

Zephyr
BH

Vive
Pro Eye

Pupillometry

Figure 1: Our lunar rover VR simulation produces online time synced human performance and in-simulation objective
measures for investigation of online mental workload estimation. The exploratory simulation, founded on MATB-II, allows for
manipulation of each component separately. All four control components play a role in a MW model (i.e., Intrinsic, Extrinsic,
Germane Load). A biosensor suite records a robust sweep of raw physiological data and extracts relevant biofeatures data
during play (see subsection 4.3). In-game objective data and extracted biofeatures are time synced through ROS. Between-run
TLX survey results paired to time synced biofeatures and objective measures enable MW modeling online and post simulation.

approach. Performance measures, both objective and sub-
jective, enable external measurement of human achievement.
Biosignals allow inference of the Autonomic Nervous Sys-
tem (ANS) and Central Nervous System (CNS), potentially
providing deeper insights. VR is a novel test bed for MW
studies promoting task relevant simulations.

2.1 Performance Measures

Performance measures display an intricate relationship to
MW [14]. Optimum MWs consistently yield maximized ob-
jective performance [18, 45]. In VR, objective performance
measures have demonstrated improved task performance and
been linked to MW [6, 44]. A multitude of methods have
been subjective surveys have been explored in relation to
MW [39]. The NASA Task Load Index (NASA-TLX) sur-
vey has demonstrated a relationship to MW in a variety of
modalities [17], including proving the usefulness of VR in
modulating MW [10, 37].

2.2 Biosignals

MW estimates [46]. For example, calculating Stress-Induced
Vascular Response Index (sVRI), a PPG signal derivative,
requires a lower time period of analysis to estimate MW than
HRV, making it a good candidate for real-time estimation
[48]. Notably, physical exertion confounds cardiovascular
measures, making them poorly suited for tasks with signiﬁ-
cant physical loads [15].

Dermal measures relate to MW as a response to general
ANS stimulation. EDA, also known as Galvanic Skin Re-
sponse (GSR), quantiﬁes the electrical resistance of skin.
EDA waveforms consist of tonic, slow acting, and phasic,
event speciﬁc, responses [9]. Tonic components have indi-
cated moderate utility for MW estimation (e.g., tonic com-
ponents loosely correlate with MW), while shorter, more
frequent, phasic peaks suggest a stronger MW relationship
[9]. ST can be utilized in MW estimations, but with temporal
limitations of slow rise times and delayed event responses.
Generally, ST decreases in response to MW [28]. However,
ST responses to MW may be location dependent [1].

Physiological signals give quantitative insight into an indi-
vidual MW [11]. For our work, we selected six raw biosignals
(and multiple derived features) that have demonstrated MW
correlations: Electrocardiogram (ECG), Electrodermal Ac-
tivity (EDA), Photoplethysmography (PPG), Respiration,
Skin Temperature (ST), and Pupillometry.

The Respiration Waveform gives insight into MW through
breathing rate and variations [15]. In a relaxed state, a hu-
man breathes slowly and consistently. However, as MW rises,
overall breathing rate increases along with an increased preva-
lence of irregular rhythms, quick variations, and cessations
[26].

ECG, derived from cardiac electrical activity, and PPG,
blood ﬂow interpolation via light refraction, provide indi-
rect insight into MW via cardiovascular activity measure-
ment [11]. Temporally, ECG measures (HR and Heart Rate
Variability (HRV)) have demonstrated relationships to MW
[11, 38, 45]. In the frequency domain, power bands increase or
decrease as a function of task load [11]. PPG, either individ-
ually or in conjunction with ECG, may provide more useful

Pupillometry measures ﬂuctuations in pupil size and reactiv-
ity, which have a direct relationship to MW through the CNS
[32]. For example, variations in the rate and magnitude of
microsaccades, involuntary eye movements that occur during
ﬁxation, can separate diﬀerent MW levels [27]. One study
speciﬁcally records pupillometry in VR and found a positive
correlation between pupil diameter and subjective task load
scores [41].

2.3 VR in MW modeling

navigates to the objective point.

VR provides an immersive environment capable of simulating
complex situations in a controlled manner [10]. While VR has
been employed in a variety of use cases [24, 30], the utility of
VR starts to shine in high-risk scenarios. Driving simulations
commonly vary route options or obstacle diﬃculty in order to
induce diﬀerent MWs [2, 21, 47]. Users of surgical simulators
have subjectively reported the utility of VR in operations
preparation [7] and objectively demonstrated decreased MWs
after system use [49]. The use of VR for ﬂight simulation
assessment has been able to diﬀerentiate between low and
high MWs [4, 23].

3 System Design

Motivated by the cognitive state insights provided by perfor-
mance and biosignal measures as well as the role VR can play
(see section 2), we designed a VR exploration environment
with the aim of modulating MW through simulation diﬃculty.
While playing the simulation, the user dons three biosensors
(i.e., Zephyr Bioharness, Empatica E4, HTC Vive Pro Eye).
The system collects objective performance and physiological
measures online, with NASA-TLX survey responses collected
between runs (see Figure 1). The Zephyr Bioharness commu-
nicates directly with Robot Operating System (ROS) while
the Empatica E4 and Unity connect to ROS through Win-
dows via the vendor provided streaming server and rosbridge
respectively. ROS time synchronizes all collected data, which
can be used in a variety of MW models, enabling real-time
(closed loop) use while still providing traditional oﬄine (open
loop) analysis.

The simulation begins with the user randomly placed on a
1 km2 lunar texture, onboard the Apollo rover. A prompt
informs the user of the mission scenario and objective. While
on an exploration mission to an area of scientiﬁc interest (i.e.,
objective point), the rover’s autonomous systems have failed.
The rover’s responsibilities now fall on the human as the user

3.1 Simulation Design

The simulation tasks the user with navigating to a randomly
generated objective point (represented as a green column
(Figure 2A)) and drop a marker while aboard the Apollo
lunar rover, in the shortest amount of time. A 6-axis mo-
tion platform provides motion cues (Figure 1), generated
by the lunar texture the user drives across, complete with
corresponding lunar physics (e.g., gravity of 0.166G). A side
mounted Hands On Throttle And Stick (HOTAS), in the
same layout as the in simulation rover, provides in game
movement and interaction (Figure 1). While navigating, the
user must track or respond to various rover alerts, all chosen
in relationship to the MATB-II due to its ability to regulate
the various components of MW. Upon objective completion,
a NASA-TLX prompt appears.

3.1.1 Tracking

A radar indicator (Figure 2B) details the signal strength
and optimal direction of the on-board radar dish. The user
must maintain optimal connection to the stationary relay
station as they navigate through a button on the HOTAS
stick. The user’s ﬁeld of view includes the actual on-board
radar for added visual feedback. As signal strength worsens,
the radar indicator drops bars and changes colors (i.e., green
to yellow to red), while the arrow indicates the necessary
directional change. If the user does not correct a drop in
signal strength, the indicator will start ﬂashing and increase
ﬂashing frequency over time.

3.1.2 Communication

While navigating, contact must be maintained with either
the base station or the lunar habitat. Separate audio (i.e.,
male for base station and female for lunar habitat) and visual
(Figure 2C) prompts tell the user which frequency to broad-
cast to. Upon receiving the prompt, the user must switch

B.) Tracking

C.) Communications

D.) Resource
Management

A.) Objective

E.) System
Management

Figure 2: The VR simulation is founded on MATB-II enabling easy manipulation of each characteristic. (A) The user must
navigate the lunar terrain to an objective point. (B) Updating the radar position with respect to a ﬁxed location requires
the user concentrate on tracking. (C) Switching radio communications channels, prompted by separate voices, aims to satisfy
the communications requirement. (D) The diﬀerent rates of O2 usage and CO2 buildup force the user to consider resource
management. (E) Motor temperature ﬂuctuations and limitations require system management. Together, all pieces of the
simulation act to modulate MW based on in-simulation diﬃculty.

Trial I

Trial II

Trial III

Trial IV

6

)

m
m

(

r
e
t
e
m
a
i
D

3
140

p
m
e
T

)
C
◦
(

50
1

e
d
u
t
i
l

p
m
0A
0

Time (min)

60
Figure 3: A full trial consists of four separate runs, at alter-
nating intensities, separated by three minutes of free play.
Time synchronized biometrics (e.g., pupil diameter), objec-
tive performance (e.g., motor temperature), and subjective
(NASA-TLX survey administration) performance measures
enable online information use. At run initialization, the user
must navigate to the objective point, while all metrics are
recorded in real time. Periods of free play separate each run
allowing the user biosignals to return to baseline.

channels via a button located on the HOTAS stick. Delays
in response cause the prompt to reoccur at an increased
frequency. The system may also separately ask the user to
report in, prompting a separate trigger response.

3.1.3 Resource Management

Distance traveled as well as motor speed and temperature
dictate the rover’s limited battery, part of the RM triad
(Figure 2D). Overdrive capabilities, shown in the on-board
display (Figure 2E), can increase speed for a short burst
but at the cost of signiﬁcant battery life. Depletion of rover
battery results in a failed run. CO2 management, which
requires the user to expel its contents after reaching a certain
threshold, and O2 consumption, the depletion of which results
in a failed run, act as the other two constituents. Breathing
rate, supplied by the Zephyr Bioharness, drives both CO2
buildup and O2 consumption, but at diﬀerent scales (i.e.,
CO2 buildup happens multiple times a run while O2 depletion
is a one time event).

3.1.4 System Monitoring

Motor temperature and rover speed fulﬁll the role of SM
(Figure 2E). The on-board display reﬂects rover temperature
variations, which are controlled by the HOTAS throttle on
the motion platform. Throttle speed and terrain dictate the
actual rover speed, with the user feeling all motion in real
time through the motion platform. Vehicle torque drives
motor temperature. If motor temperature reaches a critical
temperature, the entire rover will stall while the system cools
down, drastically aﬀecting completion time. An overdrive
button increases the torque over the recommended limit for
a short time at the cost of an increased motor temperature.

3.2 Trial Design

We structured each run in a repeated measures fashion, de-
signed to last for one-hour (Figure 3). Each trial begins by
equipping the user with biosensors and initializing the ROS
data collection. Upon situating themselves in the motion

platform and being briefed on the mission objective, the user
explores the lunar terrain in free play mode, with the Heads
Up Display (HUD) and on-board display turned oﬀ, for ﬁve
minutes. The ﬁve-minute period acts as an establishment of
physiological baseline [18]. Afterward, the system sets the
run’s in-game diﬃculty, an objective point appears, and the
information systems (i.e., the HUD and on-board display)
turn on. The user then completes the task as previously
described (see subsection 3.1). After a run completion, the
user enters three minutes of free play to reestablish baseline
physiology. Subsequently, the system varies run diﬃculty
(i.e., low to high or high to low) and begins the cycle again.
The user completes four separate runs (two high and two low
diﬃculty) during testing.

4 System Output

ROS time syncs all collected data across multiple sensors
and operating systems (i.e., Linux and Windows). After an
individual study completion, a single rosbag carries all data
for a single run. The ROS architecture enables isolation of
each data stream to separate topics, with each topic han-
dling diﬀerent measurement types (e.g., ECG topic, rover
topic, NASA-TLX survey topic). The system collects in-game
objective performance measures, between run NASA-TLX
surveys, and multiple biosignal (e.g., EDA, Pupillometry)
features.

4.1 Objective Measures

In-simulation measures indicate user performance. For the
rover, the system reports rover pose (position, rotation), twist
(velocity, angular velocity), fuel, and motor temperature at
10 Hz. Boolean ﬂags mark radio requests and responses.
Twelve total states represent antenna accuracy. We also
collect current CO2 and O2 (at 10 Hz), the current trial leg
and diﬃculty (see subsection 3.2), and elapsed time of play.

4.2 Subjective Measures

A NASA-TLX survey appears after the user has reached
the objective and dropped their marker. The survey asks
questions aimed at subjectively evaluating user performance
(e.g., How mentally demanding was the task?), in six cate-
gories: mental demand, physical demand, temporal demand,
performance, eﬀort, and frustration.

4.3 Biofeatures

4.3.1 Raw Biosignals

Raw biosignals are collected at resolutions speciﬁed by the
manufacturer. Using the vendor supplied Windows streaming
server, the Empatica E4 collects and relays raw PPG at 64
Hz as well as EDA and ST at 4 Hz. The Zephyr Bioharness
directly relays a raw respiration waveform at 1.008 Hz and
ECG waveform at 252 Hz. Pupillometry, provided through
the HTC Vive Pro Eye linked to Unity, reports 2-D eye
position and pupil diameter at 120Hz.

4.3.2 Derived Biosignal Features

Using third-party packages [8, 36], our system derives biosig-
nal features with a 30s windowing interval [18, 43]. Biofea-
tures are derived in both time and frequency domains, rang-
ing from traditional features (e.g., heart rate) to newer ap-
proaches (e.g., smooth pursuits). A full list of derived biosig-
nal features can be found in Table 1.

(A)
1.5

e
d
u
t
i
l
p
m
A

-0.5
(B)
10

e
d
u
t
i
l
p
m
A

-10
(C)
0.58

)
S
µ
(
A
D
E

0.55
(D)
100

P
V
B

-100
(E)
31.5

)
C
◦

(
.
p
m
e
T

30.5

0
(F)
45

)

◦

(

n
o
i
t
i
s
o
P
Y

Electrocardiogram

Table 1 List of biosignals and the corresponding available
derived features produced in the VR pipeline.

Biosignal

Features

Respiration

ECG

Electrodermal Activity

Photoplethysmography

Respiration

EDA

ST

PPG

Skin Temperature

Pupillometry

Time Domain
HRV (RMSSD)
SDSD
R2R (RRMean)
RRStD
Min, Max
HRV Tri Index
Time Domain
Respiration Rate

Non-Linear Frequency Domain

PNN10
PNN25
PNN50
SD1, SD2
SD1/2
SDell

VLF Power
LF Power
HF Power
Total Power

Frequency Domain
LF Power, HF Power
Power Ratio

Tonic
Mean
StD
Range
AUC

Phasic
Mean
StD
Range
AUC

Peak (Phasic)
Max, Min
Mean, Quantity
Mean Duration
Mean Slope
Time Domain
Mean, Median, StD, Max, Min
Time Domain
Digital PA, Reﬂection index
R2R, AUC, sVRI, IPA, PRV
Time Domain
Pupil Diameter, Smooth Pursuit Frequency
Saccade Frequency, Mean Saccade Amplitude
Fixation Frequency, Fixation Duration
Post Saccadic Oscillations (PSO) Frequency

Time(s)
Pupil Position

30

ration. Raw biosignal data (Figure 4) empowers our pipeline
to extract a plethora of relevant biofeatures (Table 1), close
to real time, providing a relevant test bed for open and closed
loop MW model exploration.

The immersive capability of VR, coupled with synchronized
biofeatures, enable a deep exploration of MW modeling. Im-
mersive environments ensure realistic data collection by caus-
ing more realistic mental and physical responses [12]. A
multitude of biofeatures relate to MW (see subsection 2.2),
yet few studies directly compare information gain across
modalities [45]. Direct comparisons could inform systems
designed for resource constrained environments. machine
learning (ML) model approaches are end goal dependent
(e.g., training evaluation or HRI). Online methods, such as
deep learning (DL) or reinforcement learning (RL) models,
may improve HR teaming eﬃcacy and safety [25, 29]. Oﬄine
modeling, such as traditional supervised learning models,
may be more appropriate for testing individual skill sets
[33]. The ﬂexibility of our pipeline design with ROS integra-
tion allows this approach to be ported to a plethora of VR
environments, or even into physical robotic systems.

We learned many lessons and discovered implicit design lim-
itations while executing our system design. Many techni-
cal issues stemmed from communicating and synchronizing
across two diﬀerent operating systems (i.e., Windows and
Linux). The latest iteration of ROS, ROS2, is available for
Windows. However, keeping our system in Linux enables
downstream use in real-world robotic systems, the major-
ity of which operate on Linux. The Empatica streaming
server instability and hardware limitations (e.g., dropping
or not discovering devices and speciﬁc Bluetooth module
requirements) was an unforeseen complication, which could
have been remedied with an open-source option; we are cur-
rently developing such an option for community beneﬁt. Poor
error messaging made debugging the ROS to Unity connec-
tion diﬃcult (e.g., mistyped Unity components break the
connection but do not indicate where the error occurred).

-45

60

-60

X Position (◦ )
Figure 4: Raw biosignals, collected across three seperate
devices, enable downstream feature extraction. (A-B) ECG
and Respiration waveforms are collected from the Zephyr
Bioharness. (C-E) EDA, PPG, and Skin Temerature are
relayed through the Emaptica E4 streaming server.
(F)
Pupil position is reported through the HTC Vive Pro Eye. A
rolling 30s window is applied to each waveform or positional
data to extract and report biofeatures (Table 1).

5 Discussion

MW models are critical for eﬀective HR teaming and immer-
sive VR experiences, but available processes rarely include
online capabilities or relevant settings. This work developed
a VR pipeline capable of time synchronizing numerous objec-
tive and subjective performance data with passive biosignals,
which can be utilized in MW modeling on or oﬄine (Figure 1).
VR task design compels users to focus on the four pillars of
MATB-II (i.e., tracking, RM, SM, and communication) while
navigating to an objective (Figure 2). VR enables situational
data collection, by emulating high-risk extraplanetary explo-

Incompatibilities between Unity VR and Steam VR hindered
full headset integration, prompting a switch to Unity XR.
We tested many iterations in order to balance the MATB-II
style features to ensure playability and goal completion while
being able to modulate diﬃculty. Our Object Oriented Pro-
gramming (OOP) approach for feature extraction allows easy
third-party package changes, ensuring easy future modiﬁca-
tions. Other biosensors could be added to the suite (e.g.,
Electroencephalography (EEG), Electromyography (EMG)),
but may be impractical for VR or result in large rosbag or
rosmsg ﬁles that can slow down system processing. The 30s
windowing time temporally limits MW predictions. Short-
ening the window time without signiﬁcant information loss
may be possible [43].

Simulation design leaves the door open for a variety of fu-
ture work. An independent samples t-test would validate
simulation diﬃculty against MW (i.e., over or under-loaded).
We have begun human subject data collection to this end
(α=0.05, β=0.8 | n≈35). With a subject database, we can
explore a variety of MW ML models. We intend to evaluate
traditional multivariate linear and simple supervised learning
approaches, followed by DL and RL methods. To utilize the
MW models online, we are developing an artiﬁcially intelli-
gent (AI) agent equipped with the MW model and capable
of online simulation control variation.

6 Conclusion

MWs could be useful in a wide variety of systems, from HRI
and teaming to operator training and testing. Such systems
would utilize ML-based MW models that ideally function on
passive biosignals produced in realistic environments. VRs
immersive capabilities facilitate reproducible simulations of
complex scenarios, fulﬁlling this critical need. The developed
simulation, based on CLT and MATB-II, aims to achieve
this goal in conjunction with a robust sweep of biosignals
and features. The online time synced design not only allows
for oﬄine and open loop analysis, but enables real-time MW
assessment, which stands to improve a variety of MW models
and form the basis for closed-loop control systems reactive
to an operator’s MW.

References

[1] Y. Abdelrahman, E. Velloso, T. Dingler, A. Schmidt, and
F. Vetere. Cognitive heat: exploring the usage of thermal
imaging to unobtrusively estimate cognitive load. Proceedings
of the ACM on Interactive, Mobile, Wearable and Ubiquitous
Technologies, 1(3):1–20, 2017.

[2] U. A. Abdurrahman, S.-C. Yeh, Y. Wong, and L. Wei. Ef-
fects of neuro-cognitive load on learning transfer using a
virtual reality-based driving system. Big Data and Cognitive
Computing, 5(4):54, 2021.

[3] I. Albuquerque, A. Tiwari, M. Parent, R. Cassani, J.-F.
Gagnon, D. Lafond, S. Tremblay, and T. H. Falk. Wauc:
a multi-modal database for mental workload assessment un-
der physical activity. Frontiers in Neuroscience, 14, 2020.
[4] P. Andrievskaia, K. Van Benthem, and C. M. Herdman. Neu-
ral correlates of mental workload in virtual ﬂight simulation.
In International Conference on Human-Computer Interac-
tion, pp. 521–528. Springer, 2020.

[5] R. W. Backs. Application of psychophysiological models
to mental workload. In Proceedings of the Human Factors
and Ergonomics Society Annual Meeting, vol. 44, pp. 3–464.
SAGE Publications Sage CA: Los Angeles, CA, 2000.

[6] J. Bailenson, K. Patel, A. Nielsen, R. Bajscy, S.-H. Jung, and
G. Kurillo. The eﬀect of interactivity on learning physical
actions in virtual reality. Media Psychology, 11(3):354–376,
2008.

[7] J. Barr´e, D. Michelet, J. Truchot, E. Jolivet, T. Recanzone,
S. Stiti, A. Tesni`ere, and G. Pourcher. Virtual reality single-
port sleeve gastrectomy training decreases physical and men-
tal workload in novice surgeons: an exploratory study. Obesity
surgery, 29(4):1309–1316, 2019.

[8] A. Bizzego, A. Battisti, G. Gabrieli, G. Esposito, and
C. Furlanello. pyphysio: A physiological signal processing
library for data science approaches in physiology. SoftwareX,
10:100287, 2019.

[9] J. J. Braithwaite, D. G. Watson, R. Jones, and M. Rowe.
A guide for analysing electrodermal activity (EDA) & skin
conductance responses (SCRs) for psychological experiments.
Psychophysiology, 49(1):1017–1034, 2013.

[10] C.-J. Chao, S.-Y. Wu, Y.-J. Yau, W.-Y. Feng, and F.-Y.
Tseng. Eﬀects of three-dimensional virtual reality and tra-
ditional training methods on mental workload and training
performance. Human Factors and Ergonomics in Manufac-
turing & Service Industries, 27(4):187–196, 2017.

[11] R. L. Charles and J. Nixon. Measuring mental workload
using physiological measures: A systematic review. Applied
ergonomics, 74:221–232, 2019.

[12] H.-H. Choi, J. J. Van Merri¨enboer, and F. Paas. Eﬀects of the
physical environment on cognitive load and learning: Towards
a new model of cognitive load. Educational Psychology Review,
26(2):225–244, 2014.

[13] Y. Daviaux, C. Bey, L. Arsac, O. Morellec, and S. Lini.
Feedback on the use of MATB-II task for modeling of cognitive
control levels through psycho-physiological biosignals. In 20th
International Symposium on Aviation Psychology, p. 205,
2019.

[14] E. Galy, M. Cariou, and C. M´elan. What is the relation-
ship between mental workload factors and cognitive load
types? International journal of psychophysiology, 83(3):269–
275, 2012.

[15] M. Grassmann, E. Vlemincx, A. Von Leupoldt, J. M. Mit-
telst¨adt, and O. Van den Bergh. Respiratory changes in
response to cognitive load: a systematic review. Neural
plasticity, 2016.

[16] E. Haapalainen, S. Kim, J. F. Forlizzi, and A. K. Dey. Psycho-
physiological measures for assessing cognitive load. In Proceed-
ings of the 12th ACM international conference on Ubiquitous
computing, pp. 301–310, 2010.

[17] S. G. Hart. NASA-task load index (NASA-TLX); 20 years
later. In Proceedings of the human factors and ergonomics
society annual meeting, vol. 50, pp. 904–908. Sage publications
Sage CA: Los Angeles, CA, 2006.

[18] J. Heard and J. A. Adams. Multi-dimensional human work-
load assessment for supervisory human–machine teams. Jour-
nal of Cognitive Engineering and Decision Making, 13(3):146–
170, 2019.

[19] J. Heard, C. E. Harriott, and J. A. Adams. A human workload
assessment algorithm for collaborative human-machine teams.
In 2017 26th IEEE International Symposium on Robot and
Human Interactive Communication (RO-MAN), pp. 366–371.
IEEE, 2017.

[20] K. Holden, N. Ezer, and G. Vos. Evidence report: risk of

inadequate human-computer interaction. 2013.

[21] S. Imhoﬀ, M. Lavalli`ere, N. Teasdale, and P. Fait. Driving
assessment and rehabilitation using a driving simulator in
individuals with traumatic brain injury: A scoping review.
NeuroRehabilitation, 39(2):239–251, 2016.

[22] M. Johnson, J. M. Bradshaw, P. J. Feltovich, C. M. Jonker,
M. B. Van Riemsdijk, and M. Sierhuis. Coactive design: De-
signing support for interdependence in joint activity. Journal
of Human-Robot Interaction, 3 (1), 2014, 2014.

[23] I. Kakkos, G. N. Dimitrakopoulos, L. Gao, Y. Zhang, P. Qi,
G. K. Matsopoulos, N. Thakor, A. Bezerianos, and Y. Sun.
Mental workload drives diﬀerent reorganizations of functional
cortical connectivity between 2D and 3D simulated ﬂight
experiments. IEEE Transactions on Neural Systems and

Rehabilitation Engineering, 27(9):1704–1713, 2019.

1986:1–13, 2008.

[42] J. Sweller, J. J. Van Merrienboer, and F. G. Paas. Cognitive
architecture and instructional design. Educational psychology
review, 10(3):251–296, 1998.

[43] J. Tervonen, K. Pettersson, and J. M¨antyj¨arvi. Ultra-short
window length and feature importance analysis for cognitive
load detection from wearable sensors. Electronics, 10(5):613,
2021.

[44] J. Tichon. Training cognitive skills in virtual reality: Measur-
ing performance. CyberPsychology & Behavior, 10(2):286–289,
2006.

[45] P. Vanneste, A. Raes, J. Morton, K. Bombeke, B. B.
Van Acker, C. Larmuseau, F. Depaepe, and W. Van den
Noortgate. Towards measuring cognitive load through mul-
timodal physiological data. Cognition, Technology & Work,
pp. 1–19, 2020.

[46] C. Wang and J. Guo. A data-driven framework for learners’
cognitive load detection using ECG-PPG physiological feature
fusion and XGBoost classiﬁcation. Procedia computer science,
147:338–348, 2019.

[47] S. Zepf, J. Hernandez, A. Schmitt, W. Minker, and R. W.
Picard. Driver emotion recognition for intelligent vehicles: A
survey. ACM Computing Surveys (CSUR), 53(3):1–30, 2020.
[48] X. Zhang, Y. Lyu, X. Hu, Z. Hu, Y. Shi, and H. Yin. Eval-
uating photoplethysmogram as a real-time cognitive load
assessment during game playing. International Journal of
Human–Computer Interaction, 34(8):695–706, 2018.

[49] B. Zheng, X. Jiang, G. Tien, A. Meneghetti, O. N. M. Panton,
and M. S. Atkins. Workload assessment of surgeons: corre-
lation between NASA-TLX and blinks. Surgical endoscopy,
26(10):2746–2750, 2012.

[24] M. Kaufeld and P. Nickel. Level of robot autonomy and infor-
mation aids in human-robot interaction aﬀect human mental
workload–an investigation in virtual reality. In International
Conference on Human-Computer Interaction, pp. 278–291.
Springer, 2019.

[25] S. K. Kim, E. A. Kirchner, A. Stefes, and F. Kirchner. In-
trinsic interactive reinforcement learning–using error-related
potentials for real world human-robot interaction. Scientiﬁc
reports, 7(1):1–16, 2017.

[26] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani,
T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras. DEAP:
A database for emotion analysis using physiological signals.
IEEE transactions on aﬀective computing, 3(1):18–31, 2011.
[27] K. Krejtz, A. T. Duchowski, A. Niedzielska, C. Biele, and
I. Krejtz. Eye tracking cognitive load using pupil diameter
and microsaccades with ﬁxed gaze. PloS one, 13(9):e0203629,
2018.

[28] C. Larmuseau, J. Cornelis, L. Lancieri, P. Desmet, and F. De-
paepe. Multimodal learning analytics to investigate cognitive
load during online problem solving. British Journal of Edu-
cational Technology, 51(5):1548–1562, 2020.

[29] T. D. Le, D. T. Huynh, and H. V. Pham. Eﬃcient human-
robot interaction using deep learning with mask R-CNN: de-
tection, recognition, tracking, and segmentation. In 2018 15th
International Conference on Control, Automation, Robotics
and Vision (ICARCV), pp. 162–167. IEEE, 2018.

[30] T. Luong, A. Lecuyer, N. Martin, and F. Argelaguet. A
survey on aﬀective and cognitive VR. IEEE Transactions on
Visualization and Computer Graphics, 2021.

[31] L. M. Ma, T. Fong, M. J. Micire, Y. K. Kim, and K. Feigh.
Human-robot teaming: Concepts and components for design.
In Field and service robotics, pp. 649–663. Springer, 2018.

[32] G. Marquart, C. Cabrall, and J. de Winter. Review of eye-
related measures of drivers’ mental workload. Procedia Man-
ufacturing, 3:2854–2861, 2015.

[33] K. Moustafa, S. Luz, and L. Longo. Assessment of mental
workload: a comparison of machine learning methods and
subjective assessment techniques. In International symposium
on human mental workload: Models and applications, pp. 30–
50. Springer, 2017.

[34] F. Paas, J. E. Tuovinen, H. Tabbers, and P. W. Van Gerven.
Cognitive load measurement as a means to advance cognitive
load theory. Educational psychologist, 38(1):63–71, 2003.
[35] F. G. Paas and J. J. Van Merri¨enboer. Instructional control
of cognitive load in the training of complex cognitive tasks.
Educational psychology review, 6(4):351–371, 1994.

[36] J. Pekkanen and O. Lappi. A new and general approach
to signal denoising and eye movement classiﬁcation based
on segmented linear regression. Scientiﬁc reports, 7(1):1–13,
2017.

[37] L. Pouliquen-Lardy, I. Milleville-Pennel, F. Guillaume, and
F. Mars. Remote collaboration in virtual reality: asymmet-
rical eﬀects of task distribution on spatial processing and
mental workload. Virtual Reality, 20(4):213–220, 2016.
[38] A. Rieger, R. Stoll, S. Kreuzfeld, K. Behrens, and M. Weip-
pert. Heart rate and heart rate variability as indirect markers
of surgeons’ intraoperative stress. International archives of
occupational and environmental health, 87(2):165–174, 2014.
[39] S. Rubio, E. D´ıaz, J. Mart´ın, and J. M. Puente. Evaluation of
subjective mental workload: A comparison of SWAT, NASA-
TLX, and workload proﬁle methods. Applied psychology,
53(1):61–86, 2004.

[40] Y. Santiago-Espada, R. R. Myer, K. A. Latorella, and J. R.
Comstock Jr. The Multi-Attribute Task Battery II (MATB-
II) software for human performance and workload research:
A user’s guide. 2011.

[41] M. Schwalm, A. Keinath, and H. D. Zimmer. Pupillometry as
a method for measuring mental workload within a simulated
driving task. Human Factors for assistance and automation,

