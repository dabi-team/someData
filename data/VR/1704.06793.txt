7
1
0
2

r
p
A
2
2

]

C
O
.
h
t
a
m

[

1
v
3
9
7
6
0
.
4
0
7
1
:
v
i
X
r
a

Faster and Non-ergodic O(1/K)
Stochastic Alternating Direction Method of Multipliers

Cong Fang
fangcong@pku.edu.cn
Peking University

Feng Cheng
fengcheng@pku.edu.cn
Peking University

Zhouchen Lin
zlin@pku.edu.cn
Peking University

Original circulated date: 22th April, 2017.

Abstract

We study stochastic convex optimization subjected to linear equality constraints. Traditional
Stochastic Alternating Direction Method of Multipliers Ouyang et al. (2013) and its Nesterov’s
acceleration scheme AzadiSra & Sra (2014) can only achieve ergodic O(1/√K) convergence rates,
where K is the number of iteration. By introducing Variance Reduction (VR) techniques, the
convergence rates improve to ergodic O(1/K) Zhong & Kwok (2014); Zheng & Kwok (2016).
In this paper, we propose a new stochastic ADMM which elaborately integrates Nesterov’s
extrapolation and VR techniques. We prove that our algorithm can achieve a non-ergodic
O(1/K) convergence rate which is optimal for separable linearly constrained non-smooth convex
problems, while the convergence rates of VR based ADMM methods are actually tight O(1/√K)
in non-ergodic sense. To the best of our knowledge, this is the ﬁrst work that achieves a truly
accelerated, stochastic convergence rate for constrained convex problems. The experimental
results demonstrate that our algorithm is faster than the existing state-of-the-art stochastic
ADMM methods.

1 Introduction

We consider the following general convex ﬁnite-sum problems with linear constraints:

min
x1,x2

s.t.

h1(x1) + f1(x1) + h2(x2) +

A1x1 + A2x2 = b,

1
n

n
(cid:88)

i=1

f2,i(x2),

(1)

where f1(x1) and f2,i(x2) with i ∈ {1, 2, · · · , n} are convex and have Lipschitz continuous gradients,
h1(x1) and h2(x2) are also convex. We denote that L1 is the Lipschitz constant of f1(x1), L2
is the Lipschitz constant of f2,i(x2) with i ∈ {1, 2, · · · , n}, and f2(x) = 1
i=1 f2,i(x). We deﬁne
n
Fi(xi) = hi(xi)+fi(xi) for i = 1, 2, x = (x1, x2), F (x1, x2) = F1(x1)+F2(x2), and Ax = (cid:80)2
i=1 Aixi.
Problem (1) is of great importance in machine learning. The ﬁnite-sum function f2(x2) is typi-
cally a loss over training samples, and the remaining functions control the structure or regularize the
model to aid generalization AzadiSra & Sra (2014). The idea of using linear constraints to decouple
the loss and regularization terms enables researchers to consider some more sophisticated regular-
ization terms which might be very complicated to solve through proximity operators for Gradient
Descent Beck & Teboulle (2009) methods. For example, for multitask learning problems Argyriou
et al. (2007); Shen et al. (2015), the regularization term is set as µ1(cid:107)x(cid:107)∗ + µ2(cid:107)x(cid:107)1, for most graph-
guided fused Lasso and overlapping group Lasso problem Kim et al. (2009); Zheng & Kwok (2016),

(cid:80)n

1

 
 
 
 
 
 
Table 1: Convergence rates of ADMM type methods solving Problem (1) (“non-” indicates “non-
ergodic”, while “er-” indicates “ergodic”. “Sto.” is short for “Stochstic”, and “Bat.” indicates batch
or deterministic algorithms).

Type

Bat.

Sto.

Algorithm
ADMM (Davis & Yin, 2016)
LADM-NE (Li & Lin, 2016)
STOC-ADMM (Ouyang et al., 2013)
OPG-ADMM (Suzuki, 2013)
OPT-ADMM (AzadiSra & Sra, 2014)
SDCA-ADMM (Suzuki, 2014)
SAG-ADMM (Zhong & Kwok, 2014)
SVRG-ADMM (Zheng & Kwok, 2016)
ACC-SADMM (ours)

Convergence Rate
Tight non-O( 1√
)
K
Optimal non-O( 1
K )
er-O( 1√
)
K
er-O( 1√
)
K
er–O( 1√
)
K
unknown
Tight non–O( 1√
)
K
Tight non–O( 1√
)
K
Optimal non–O( 1
K )

the regularization term can be written as µ(cid:107)Ax(cid:107)1, and for many multi-view learning tasks Wang
et al. (2016), the regularization terms always involve µ1(cid:107)x(cid:107)2,1 + µ2(cid:107)x(cid:107)∗.

Alternating Direction Method of Multipliers (ADMM) is a very popular optimization method to
solve Problem (1), with its advantages in speed, easy implementation, good scalability, shown in lots
of literatures (see survey Boyd et al. (2011)). However, though ADMM is eﬀective in practice, the
provable convergence rate is not fast. A popular criterion to judge convergence is in ergodic sense.
And it is proved in (He & Yuan, 2012; Lin et al., 2015b) that ADMM converges with an O(1/K)
ergodic rate. Since the non-ergodic results (xK), rather than the ergodic one (convex combination
of x1, x2, · · · , xK) is much faster in practice, researchers gradually turn to analyse the convergence
rate in non-ergodic sense. Davis & Yin (2016) prove that the Douglas-Rachford (DR) splitting
converges in non-ergodic O(1/
K). They also construct a family of functions showing that non-
ergodic O(1/
K) for Linearized ADMM. Then Li
& Lin (2016) accelerate ADMM through Nesterov’s extrapolation and obtain a non-ergodic O(1/K)
convergence rate. They also prove that the lower complexity bound of ADMM type methods for the
separable linearly constrained nonsmooth convex problems is exactly O(1/K), which demonstrates
that their algorithm is optimal. The convergence rates for diﬀerent ADMM based algorithms are
shown in Table 1.

K) is tight. Chen et al. (2015) establish O(1/

√

√

√

√

On the other hand, to meet the demands of solving large-scale machine learning problems,
stochastic algorithms Bottou (2004) have drawn a lot of interest in recent years. For stochas-
tic ADMM (SADMM), the prior works are from STOC-ADMM Ouyang et al. (2013) and OPG-
ADMM Suzuki (2013). Due to the noise of gradient, both of the two algorithms can only achieve
an ergodic O(1/
K) convergence rate. There are two lines of research to accelerate SADMM. The
ﬁrst is to introduce the Variance Reduction (VR) Johnson & Zhang (2013); Defazio et al. (2014);
Schmidt et al. (2013) techniques into SADMM. VR methods are widely accepted tricks for ﬁnite
sum problems which ensure the descent direction to have a bounded variance and so can achieve
faster convergence rates. The existing VR based SADMM algorithms include SDCA-ADMM Suzuki
(2014), SAG-ADMM Zhong & Kwok (2014) and SVRG-ADMM Zheng & Kwok (2016). SAG-ADMM
and SVRG-ADMM can provably achieve ergodic O(1/K) rates for Porblem (1). However, in non-
K) (please see detailed discussions in Section 5.2),
ergodic sense, their convergence rates are O(1/
while the fastest rate for batch ADMM method is O(1/K) Li & Lin (2016). So there still exists a
gap between stochastic and batch (deterministic) ADMM. The second line to accelerate SADMM
is through the Nesterov’s acceleration Nesterov (1983). This work is from AzadiSra & Sra (2014),
in which the authors propose an ergodic O( R2
K2 + Dy+ρ
) stochastic algorithm (OPT-ADMM).
The dependence on the smoothness constant of the convergence rate is O(1/K 2) and so each term

K + σ√

√

K

2

in the convergence rate seems to have been improved to optimal. This method is imperfect due
to the following two reasons: 1) The worst convergence rate is still the ergodic O(1/
K). There
is no theoretical improvement in the order K. 2) OPT-SADMM is not very eﬀective in practice.
The method does not adopt any special technique to tackle the noise of gradient except adding a
proximal term (cid:107)xk+1−xk(cid:107)2
to ensure convergence. As the gradients have noise, directly applying the
original Nesterov’s extrapolation on the variables often causes the objective function to oscillate and
decrease slowly during iteration.

σk3/2

√

In this paper, we propose Accelerated Stochastic ADMM (ACC-SADMM) for large scale general
convex ﬁnite-sum problems with linear constraints. By elaborately integrating Nesterov’s extrapo-
lation and VR techniques, ACC-SADMM provably achieves a non-ergodic O(1/K) convergence rate
which is optimal for non-smooth problems. So ACC-SADMM ﬁlls the theoretical gap between the
stochastic and batch (deterministic) ADMM. The original idea to design our ACC-SADMM is by
explicitly considering the snapshot vector ˜x into the extrapolation terms. This is, to some degree, in-
spired by Allen-Zhu (2017) who proposes an O(1/K 2) stochastic gradient algorithm named Katyusha
for convex problems. However, there are many distinctions between the two algorithms (please
see detailed discussions in Section 5.1). Our method is also very eﬃcient in practice since we
have suﬃciently considered the noise of gradient into our acceleration scheme. For example, we
adopt extrapolation as yk
) in the inner loop, where θ2 is a con-
stant and θ1,s decreases after each whole inner loop, instead of directly adopting extrapolation as
yk = xk + θk
(xk − xk−1) in the original Nesterov’s scheme as AzadiSra & Sra (2014) does.
So our extrapolation is more “conservative” to tackle the noise of gradients. There are also variants
on updating of multiplier and the snapshot vector. We list the contributions of our work as follows:

s + (1 − θ1,s − θ2)(xk

1 (1−θk−1
1
θk−1
1

s − xk−1
s

s = xk

)

• We propose ACC-SADMM for large scale convex ﬁnite-sum problems with linear constraints
which integrates Nesterov’s extrapolation and VR techniques. We prove that our algorithm
converges in non-ergodic O(1/K) which is optimal for separable linearly constrained non-
smooth convex problems. To our best knowledge, this is the ﬁrst work that achieves a truly
accelerated, stochastic convergence rate for constrained convex problems.

• Our algorithm is fast in practice. We have suﬃciently considered the noise of gradient into the
extrapolation scheme. The memory cost of our method is also low. The experiments on four
bench-mark datasets demonstrate the superiority of our algorithm. We also do experiments
on the Multitask Learning Argyriou et al. (2007) problem to demonstrate that our algorithm
can be used on very large datasets.

√

√

Notation. We denote (cid:107)x(cid:107) as the Euclidean norm, (cid:104)x, y(cid:105) = xT y, (cid:107)x(cid:107)2 =
xT Gx,
and (cid:104)x, y(cid:105)G = xT Gy, where G (cid:23) 0. For a matrix X, (cid:107)X(cid:107) is its spectral norm. We use I to denote
the identity matrix. Besides, a function f has Lipschitz continuous gradients if (cid:107)∇f (x) − ∇f (y)(cid:107) ≤
L(cid:107)x − y(cid:107), which implies Nesterov (2013):

xT x, (cid:107)x(cid:107)G =

f (y) ≤ f (x) + (cid:104)∇f (x), x − y(cid:105) +

L
2

(cid:107)x − y(cid:107)2,

(2)

where ∇f (x) denotes the gradient of f .

2 Related Works and Preliminary

2.1 Accelerated Stochastic Gradient Algorithms

There are several works in which the authors propose accelerated, stochastic algorithms for un-
constrained convex problems. Nitanda (2014) accelerates SVRG Johnson & Zhang (2013) through
Nesterov’s extrapolation for strongly convex problems. However, their method cannot be extended

3

to general convex problems. Catalyst Frostig et al. (2015) or APPA Lin et al. (2015a) reduction also
take strategies to obtain faster convergence rate for stochastic convex problems. When the objective
function is smooth, these methods can achieve optimal O(1/K 2) convergence rate. Recently, Allen-
Zhu (2017) and Hien et al. (2016) propose optimal O(1/K 2) algorithms for general convex problems,
named Katyusha and ASMD, respectively. For σ-strongly convex problems, Katyusha also meets the
(cid:15) )) rate. However, none of the above algorithms considers the problems
optimal O((n +
with constraints.

nL/σ) log 1

(cid:112)

2.2 Accelerated Batch ADMM Methods

K2 + Dy

There are two lines of works which attempt to accelerate Batch ADMM through Nesterov’s accel-
eration schemes. The ﬁrst line adopts acceleration only on the smooth term (fi(x)). The works
are from Ouyang et al. (2015); Lu et al. (2016). The convergence rate that they obtain is ergodic
O( R2
K ). The dependence on the smoothness constant is accelerated to O(1/K 2). So these
methods are faster than ADMM but still remain O(1/K) in the ergodic sense. The second line
is to adopt acceleration on both fi(x) and constraints. The resultant algorithm is from Li & Lin
(2016) which is proven to have a non-ergodic O(1/K) rate. Since the original ADMM have a tight
O(1/
K) convergence rate Davis & Yin (2016) in the non-ergodic sense, their method is faster
theoretically.

√

2.3 SADMM and Its Variance Reduction Variants

We introduce some preliminaries of SADMM. Most SADMM methods alternately minimize the
following variant surrogate of the augmented Lagrangian:

L(cid:48)(x1, x2, λ, β) = h1(x1) + (cid:104)∇f1(x1), x1(cid:105) +

(cid:107)x1 − xk

1(cid:107)2
G1

L1
2
L2
+h2(x2) + (cid:104) ˜∇f2(x2), x2(cid:105) +
2

(cid:107)x2 − xk

2(cid:107)2

G2 +

β
2

(cid:107)A1x1 + A2x2 − b +

(3)

λ
β

(cid:107)2,

where ˜∇f2(x2) is an estimator of ∇f2(x2) from one or a mini-batch of training samples. So the
computation cost for each iteration reduces from O(n) to O(b) instead, where b is the mini-batch
size. When fi(x) = 0 and Gi = 0, with i = 1, 2, Problem (1) is solved as exact ADMM. When
there is no hi(xi), Gi is set as the identity matrix I, with i = 1, 2, the subproblem in xi can be
solved through matrix inversion. This scheme is advocated in many SADMM methods Ouyang
et al. (2013); Zhong & Kwok (2014). Another common approach is linearization (also called the
inexact Uzawa method) Lin et al. (2011); Zhang et al. (2011), where Gi is set as ηiI − β
i Ai with
Li
ηi ≥ 1 + β
Li

i Ai(cid:107).

(cid:107)AT

AT

For STOC-ADMM Ouyang et al. (2013), ˜∇f2(x2) is simply set as:

˜∇f2(x2) =

1
b

(cid:88)

ik∈Ik

∇f2,ik (x2),

(4)

where Ik is the mini-batch of size b from {1, 2, · · · , n}.

VR methods Suzuki (2014); Zhong & Kwok (2014); Zheng & Kwok (2016) choose more sophis-
ticated gradient estimator to achieve faster convergence rates. As our method bounds the variance
through the technique of SVRG Johnson & Zhang (2013), we introduce SVRG-ADMM Zheng &
Kwok (2016), which uses the gradient estimator as:

˜∇f2(x2) =

1
b

(cid:88)

ik∈Ik

(∇f2,ik (x2) − ∇f2,ik (˜x2)) + ∇f2(˜x2),

(5)

4

where ˜x2 is a snapshot vector. An advantage of SVRG Johnson & Zhang (2013) based methods is
its low storage requirement, independent of the number of training samples. This makes them more
practical on very large datasets. In our multitask learning experiments, SAG-ADMM Zhong & Kwok
(2014) needs 38.2TB for storing the weights, and SDCA-ADMM needs 9.6GB Suzuki (2014) for the
dual variables, while the memory cost for our method and SVRG-ADMM is no more than 250MB.
Zheng & Kwok (2016) prove that SVRG-ADMM converges in ergodic O(1/K). Like batch-ADMM,
in non-ergodic sense, the convergence rate is tight O(1/

K) (see the discussions in Section 5.2).

√

3 Our Algorithm

3.1 ACC-SADMM

In this section, we introduce our Algorithm: ACC-SADMM, which is shown in Algorithm 1. For
simplicity, we directly linearize both the smooth term fi(xi) and the augmented term β
2 (cid:107)A1x1 +
A2x2 − b + λ
β (cid:107)2 . It is not hard to extend our method to other schemes mentioned in Section 2.3.
ACC-SADMM includes two loops. In the inner loop, it updates the primal and dual variables xk
s,1,
s . Then in the outer loop, it preserves snapshot vectors ˜xs,1, ˜xs,2 and ˜bs, and then resets
xk
s,2 and λk
the initial value of the extrapolation term y0
s+1. Speciﬁcally, in the inner iteration, x1 is updated
as:

xk+1

s,1 = argmin

h1(x1) + (cid:104)∇f1(yk

s,1), x1(cid:105)

x1
β
θs
1

+(cid:104)

(cid:0)A1yk

s,1 + A2yk

s,2 − b(cid:1) + λk

s , AT

1 x1(cid:105) +

(cid:18) L1
2

+

β(cid:107)AT
1 A1(cid:107)
2θs
1

(cid:19)

(cid:107)x1 − yk

s,1(cid:107)2.

And x2 is updated using the latest information of x1, which can be written as:

xk+1

s,2 = argmin

x2

h2(x2) + (cid:104) ˜∇f2(yk

s,1), x2(cid:105)

(6)

(7)

+(cid:104)

β
θs
1

(cid:0)A1xk+1

s,1 + A2yk

s,2 − b(cid:1) + λk

s , AT

2 x2(cid:105) +

(cid:32)

)L2

(1 + 1
bθ2
2

+

β(cid:107)AT
2 A2(cid:107)
2θs
1

(cid:33)

(cid:107)x2 − yk

s,2(cid:107)2,

where ˜∇f2(yk

s,2) is obtained by the technique of SVRG Johnson & Zhang (2013) with the form:

˜∇f2(yk

s,2) =

1
b

(cid:88)

ik,s∈I(k,s)

(cid:0)∇f2,ik,s (yk

s,2 − ∇f2,ik,s (˜xs,2) + ∇f2(˜xs,2)(cid:1) .

And yk+1

s

is generated as

s = xk+1
yk+1

s + (1 − θ1,s − θ2)(xk+1

s − xk

s ),

(8)

when k ≥ 0. One can ﬁnd that 1 − θ1,s − θ2 ≤ 1 − θ1,s. We do extrapolation in a more “conservative”
way to tackle the noise of gradient. Then the multiplier is updated through Eq. (9) and Eq. (10). We
(A1x1 + A2x2 − ˜bs) to ensure
can ﬁnd that λk
A1x1 + A2x2 not to go far from A1˜x1 + A2˜x2 in the course of iteration.

s additionally accumulates a compensation term βθ2
θ1,s

In the outer loop, we set the snapshot vector ˜xs+1 as:

˜xs+1 =

(cid:32)(cid:20)

1 −

1
m

(τ − 1)θ1,s+1
θ2

(cid:21)

(cid:20)

1 +

xm

s +

(τ − 1)θ1,s+1
(m − 1)θ2

(cid:21) m−1
(cid:88)

(cid:33)

xk
s

.

k=1

(12)

5

Algorithm 1 ACC-SADMM
Input: epoch length m > 1, β, τ = 2, c = 2, x0

θ1,s = 1
for s = 0 to S − 1 do

c+τ s , θ2 = m−τ

τ (m−1) .

0 = 0, ˜λ0

0 = 0, ˜x0 = x0

0, y0

0 = x0
0,

for k = 0 to m − 1 do

s = ˜λk
λk

s +

(cid:16)

βθ2
θ1,s

Update xk+1
s,1
Update xk+1
s,2

through Eq. (6).
through Eq. (7).

A1xk

s,1 + A2xk

s,2 − ˜bs

(cid:17)

.

˜λk+1
s = λk

s + β (cid:0)A1xk+1

s,1 + A2xk+1

s,2 − b(cid:1) .

Update yk+1

s

through Eq. (8).

end for k.
s+1 = xm
x0
s .
Update ˜xs+1 through Eq. (12).

˜λ0
s+1 = λm−1

s

˜bs+1 = A1˜xs+1,1 + A2˜xs+1,2.
Update y0

s+1 through Eq. (13).

+ β(1 − τ ) (cid:0)A1xm

s,1 + A2xm

s,2 − b(cid:1) .

(9)

(10)

(11)

end for s.

Output:

ˆxS =

1
(m − 1)(θ1,S + θ2) + 1

xm

S +

θ1,S + θ2
(m −1)(θ1,S + θ2) + 1

m−1
(cid:88)

k=1

xk
S.

˜xs+1 is not the average of {xk
s }, diﬀerent from most SVRG-based methods Johnson & Zhang (2013);
Zheng & Kwok (2016). The way of generating ˜x guarantees a faster convergence rate for the
constraints. Then at the last step, we reset y0
s+1 as:

s+1 = (1 − θ2)xm
y0

s + θ2˜xs+1 +

θ1,s+1
θ1,s

The whole algorithm is shown Algorithm 1.

3.2 Intuition

(cid:2)(1 − θ1,s)xm

s − (1 − θ1,s − θ2)xm−1

s

− θ2˜xs

(cid:3) .

Though Algorithm 1 is a little complex at the ﬁrst sight, our intuition to design the algorithm is
straightforward. To bound the variance, we use the technique of SVRG Johnson & Zhang (2013).
Like Johnson & Zhang (2013); Allen-Zhu (2017), the variance of gradient is bounded through

(cid:16)

Eik

(cid:107)∇f2(yk

2 ) − ˜∇f2(yk

2 )(cid:107)2(cid:17)

≤

2L2
b

(cid:2)f2(˜x2) − f2(yk

2 ) + (cid:104)∇f2(yk

2 ), yk

2 − ˜x2(cid:105)(cid:3) ,

(13)

2 , ˜x2 and xk

where Eik indicates that the expectation is taken over the random choice of ik,s, under the condition
that yk
2 (the randomness in the ﬁrst sm + k iterations are ﬁxed) are known. We ﬁrst
consider the case that there is no linear constraint. Then by the convexity of F1(x1) Beck & Teboulle
(2009), we have

(cid:107)2 − L1(cid:104)xk+1

1 − yk

1 , xk+1

1 − u1(cid:105),

(14)

F1(xk+1
1

) ≤ F1(u1) +

L1
2

(cid:107)xk+1

1 − yk+1

1

6

Setting u1 be xk
θ2, and θ1, respectively, and adding them, we have

1, ˜x1 and x∗

1, respectively, then multiplying the three inequalities by (1 − θ1 − θ2),

F1(xk+1
1

) ≤ (1 − θ1 − θ2)F1(xk

−L1(cid:104)xk+1

1 −yk

1 , xk+1

1) + θ2F1(˜x1) + θ1F1(x∗
1)
1 − θ2˜x1 −θ1x∗

1 −(1 − θ1 − θ2)xk

1(cid:105) +

L1
2

(cid:107)xk+1

1 − yk

1 (cid:107)2.

(15)

2 ), yk

2 + θ3(yk

where θ1 and θ2 are undetermined coeﬃcients. Comparing with Eq. (13), we can ﬁnd that there is
one more term (cid:104)∇f2(yk
2 −˜x2(cid:105) that we need to eliminate. To solve this issue, we analyse the points
2 − ˜x2) and zk+1 = xk+1
at wk = yk
2 − ˜x2), where θ3 is an undetermined coeﬃcient.
When θ3 > 0, wk and zk+1 is closer to ˜x2 compared with yk
. Then by the convexity of
F2(x2), we can generate a negative (cid:104)∇f2(yk
2 − ˜x2(cid:105), which can help to eliminate the variance
term. Next we consider the multiplier term. To construct a recursive term of L(xk+1
s,2 , λ∗) −
(1−θ1,s −θ2)L(xk
s,2, λ∗)−θ2L(˜xs,1, ˜xs,2, λ∗), where L(x1, x2, λ) satisﬁes Eq. (17), the multiplier
s,1, xk
should satisfy the following equations:

2 and xk+1

2 + θ3(yk

s,1 , xk+1

2 ), yk

2

s − ˆλk
ˆλk+1

s =

β
θ1,s

and

A (cid:0)xk+1

s −(1 − θ1 − θ2)xk

s − θ1x∗ − θ2˜xs

(cid:1) ,

ˆλk+1
s = λk

s +

β
θ1,s

(Axk+1

s − b),

where ˆλk

s is undetermined and

L(x1, x2, λ) = F (x1, x2) + (cid:104)λ, A1x1 + A2x2 − b(cid:105),

is the Lagrangian function. By introducing a new variable ˜λk

s , then setting

s = ˜λk
ˆλk

s +

β(1 − θ1,s)
θ1,s

(Axk

s − b),

(16)

(17)

(18)

and with Eq. (9) and Eq. (10), Eq. (16) and Eq. (16) are satisﬁed. Then Eq. (11) is obtained as we
need ˆλ0

s−1 when s ≥ 1.

s = ˆλm

4 Convergence Analysis

In this section, we give the convergence results of ACC-SADMM. The proof can be found in Sup-
plementary Material. We ﬁrst analyse each inner iteration. The result is shown in Lemma 1, which
connects xk

.

s to xk+1

s

Lemma 1 Assume that f1(x1) and f2,i(x2) with i ∈ {1, 2, · · · , n} are convex and have Lipschitz
continuous gradients. L1 is the Lipschitz constant of f1(x1). L2 is the Lipschitz constant of f2,i(x2)
with i ∈ {1, 2, · · · , n} . h1(x1) and h2(x2) is also convex. For Algorithm 1, in any epoch, we have

1

(cid:2)L(xk+1
(cid:16)

, xk+1
2
(cid:107)ˆλk − λ∗(cid:107)2 − Eik

(cid:104)

(cid:107)ˆλk+1 − λ∗(cid:107)2(cid:105)(cid:17)

, λ∗)(cid:3) − θ2L(˜x1, ˜x2, λ∗) − (1 − θ2 − θ1)L(xk

1, xk

2, λ∗)

(cid:107)yk

1 − (1 − θ1 − θ2)xk

1 − θ2˜x1 − θ1x∗

1(cid:107)2
G3

(cid:107)yk

2 − (1 − θ1 − θ2)xk

2 − θ2˜x2 − θ1x∗

2(cid:107)2
G4

−

−

1
2
1
2

Eik

Eik

(cid:0)(cid:107)xk+1

1 − (1 − θ1 − θ2)xk

1 − θ2˜x1 − θ1x∗

1(cid:107)2
G3

(cid:0)(cid:107)xk+1

2 − (1 − θ1 − θ2)xk
2 − θ2˜x2 − θ1x∗
(cid:17)

(cid:16)

L1 + β(cid:107)AT

1 A1(cid:107)
θ1

I − βAT
1 A1
θ1

, and

2(cid:107)2
G4

(cid:1)

(cid:1) ,

where L(x1, x2, λ) satisﬁes Eq.(17) and ˆλ satisﬁes Eq.(18), G3 =
(cid:17)

(cid:16)

G4 =

(1 + 1
bθ2

)L2 + β(cid:107)AT

2 A2(cid:107)
θ1

I. We have ignored subscript s as s is ﬁxed in each epoch.

7

≤

Eik
θ1
2β
1
2
1
2

+

+

Then Theorem 1 analyses ACC-SADMM in the whole iteration, which is the key convergence result
of the paper.

Theorem 1 If the conditions in Lemma 1 hold, then we have

(AˆxS −b) −

(cid:0)Ax0

0 − b(cid:1) + ˜λ0

0 − λ∗(cid:107)2 +

(F (ˆxS) − F (x∗) + (cid:104)λ∗, AˆxS − b(cid:105))

(cid:19)

m
θ1,S

E

βm
θ1,S

(cid:107)

(cid:18) 1
2β
(cid:0)F (x0

β(m−1)θ2
θ1,0
0 − b(cid:105)(cid:1) +

≤ C3

0) − F (x∗) + (cid:104)λ∗, Ax0

+

1
2

(cid:107)x0

0,1 − x∗

1(cid:107)2

(θ1,0L1+(cid:107)AT

1 A1(cid:107))I−AT

1 A1

+

1
2β
1
2

(cid:107)˜λ0

0 +

β(1 − θ1,0)
θ1,0

(Ax0

0 − b) − λ∗(cid:107)2

(cid:107)x0

0,2 − x∗

2(cid:107)2
(cid:16)

(1+ 1
bθ2

)θ1,0L2+(cid:107)AT

2 A2(cid:107)

,

(cid:17)

I

(19)

where C3 = 1−θ1,0+(m−1)θ2

θ1,0

.

Corollary 1 directly demonstrates that ACC-SADMM have a non-ergodic O(1/K) convergence rate.

Corollary 1 If the conditions in Lemma 1 holds, we have

E(cid:107)F (ˆxS) − F (x∗)| ≤ O(

E(cid:107)AˆxS − b(cid:107) ≤ O(

1
S
1
S

),

).

(20)

We can ﬁnd that ˆxS depends on the latest m information of xk
S. So our convergence results is in non-
ergodic sense, while the analysis for SVRG-ADMM Zheng & Kwok (2016) and SAG-ADMM Zhong
& Kwok (2014) is in ergodic sense, since they consider the point ˆxS = 1
s , which is
mS
the convex combination of xk

s over all the iterations.

k=1 xk

(cid:80)m

(cid:80)S

s=1

Now we directly use the theoretical results of Li & Lin (2016) to demonstrate that our algorithm

is optimal when there exists non-smooth term in the objective function.

Theorem 2 For the following problem:

min
x1,x2

F1(x1) + F2(x2),

s.t. x1 − x2 = 0,

(21)

let the ADMM type algorithm to solve it be:

• Generate λk

2 and yk
2 in any way,
(cid:16)
2 − λk
yk

(cid:17)

2
βk

,

• xk+1

1 = ProxF1/βk

• Generate λk+1

1

and yk+1

1

in any way,

• xk+1

2 = ProxF2/βk

(cid:16)

1 − λk+1
yk+1

1
βk

(cid:17)

.

Then there exist convex functions F1 and F2 deﬁned on X = {x ∈ R6k+5 : (cid:107)x(cid:107) ≤ B} for the above
general ADMM method, satsifying

L(cid:107)ˆxk

2 − ˆxk

1(cid:107) + |F1(ˆxk

1) − F1(x∗

1) + F1(ˆxk

2) − F2(x∗

2)| ≥

LB
8(k + 1)

,

(22)

where ˆxk

1 = (cid:80)k

i=1 αi

1xi

1 and ˆxk

2 = (cid:80)k

i=1 αi

2xi

2 for any αi

1 and αi

2 with i from 1 to k.

Theorem 2 is Theorem 11 in Li & Lin (2016). More details can be found in it. Problem (21) is a
special case of Problem (1) as we can set each F2,i(x2) = F (x2) with i = 1, · · · , n or set n = 1. So
there is no better ADMM type algorithm which converges faster than O(1/K) for Problem (1).

8

5 Discussions

We discuss some properties of ACC-SADMM and make further comparisons with some related
methods.

5.1 Comparison with Katyusha

As we have mentioned in Introduction, some intuition of our algorithm is inspired by Katyusha Allen-
Zhu (2017), which obtains an O(1/K 2) algorithm for convex ﬁnite-sum problems. However, Katyusha
cannot solve the problem with linear constraints. Besides, Katyusha uses the Nesterov’s second
scheme to accelerate the algorithm while our method conducts acceleration through Nesterov’s ex-
trapolation (Nesterov’s ﬁrst scheme). And our proof uses the technique of Tseng (2008), which is
diﬀerent from Allen-Zhu (2017). Our algorithm can be easily extended to unconstrained convex
ﬁnite-sum and can also obtain a O(1/K 2) rate but belongs to the Nesterov’s ﬁrst scheme 1.

5.2 The Importance of Non-ergodic O(1/K)

√

√

SAG-ADMM Zhong & Kwok (2014) and SVRG-ADMM Zheng & Kwok (2016) accelerate SADMM
In Theorem 9 of Li & Lin (2016), the authors generate a class of functions
to ergodic O(1/K).
K) convergence rate. When n = 1,
showing that the original ADMM has a tight non-ergodic O(1/
SAG-ADMM and SVRG-ADMM are the same as batch ADMM, so their convergence rates are
no better than O(1/
K). This shows that our algorithm has a faster convergence rate than VR
based SADMM methods in non-ergodic sense. One may deem that judging convergence in ergodic
or non-ergodic is unimportant in practice. However, in experiments, our algorithm is much faster
than VR based SADMM methods. Actually, though VR based SADMM methods have provably
faster rates than STOC-ADMM, the improvement in practice is evident only iterates are close to
the convergence point, rather than at the early stage. Both Zhong & Kwok (2014) and Zheng &
Kwok (2016) show that SAG-ADMM and SVRG-ADMM are sensitive to the initial points. We
also ﬁnd that if the step sizes are set based on the their theoretical guidances, sometimes they are
even slower than STOC-ADMM (see Fig. 1) as the early stage lasts longer when the step size is
small. Our algorithm is faster than the two algorithms whenever the step sizes are set based on the
theoretical guidances or are tuned to achieve the fastest speed (see Fig. 2). This demonstrates that
Nesterov’s extrapolation has truly accelerated the speed and the integration of extrapolation and
VR techniques is harmonious and complementary.

5.3 The Growth of Penalty Factor β
θs
1

One can ﬁnd that the penalty factor β
increases linearly with the iteration. This might make our
θs
1
algorithm impractical because after dozens of epoches, the large value of penalty factor might slow
down the decrement of function value. However, in experiments, we have not found any bad inﬂuence.
There may be two reasons 1. In our algorithm, θ1,s decreases after each epoch (m iterations), which
is much slower than LADM-NE Li & Lin (2016). For most stochastic problems, algorithms converge
in less than 100 epoches, thus θ1,s is only 200 times of θ1,0. The growth of penalty factor works
as a continuation technique Zuo & Lin (2011), which may help to decrease the function value. 2.
From Theorem 1, our algorithm converges in O(1/S) whenever θ1,s is large. So from the theoretical
viewpoint, a large θ1,s cannot slow down our algorithm. We ﬁnd that OPT-ADMM AzadiSra & Sra
(2014) also needs to decrease the step size with the iteration. However, its step size decreasing rate
is O(k

3
2 ) and is faster than ours.

1We follow Tseng (2008) to name the extrapolation scheme as Nesterov’s ﬁrst scheme and the three-step

scheme Nesterov (1988) as the Nesterov’s second scheme.

9

(a) a9a-training

(b) covertype-training

(c) mnist-training

(d) dna-training

(e) a9a-testing

(f) covertype-testing

(g) mnist-testing

(h) dna-testing

Figure 1: Experimental results of solving the original Lasso problem (Eq. (23)) (Top: objective gap;
Bottom: testing loss). The step sizes are set based on the theoretical guidances. The computation
time has included the cost of calculating full gradients for SVRG based methods. SVRG-ADMM
and SAG-ADMM are initialized by running STOC-ADMM for 3n
b iterations. “-ERG” represents the
ergodic results for the corresponding algorithms.

6 Experiments

We conduct experiments to show the eﬀectiveness of our method2. We compare our method with
the following the-state-of-the-art SADMM algorithms: (1) STOC-ADMM Ouyang et al. (2013),
(2) SVRG-ADMM Zheng & Kwok (2016), (3) OPT-SADMM AzadiSra & Sra (2014), (4) SAG-
ADMM Zhong & Kwok (2014). We ignore the comparison with SDCA-ADMM Suzuki (2014)
since there is no analysis for it on general convex problems and it is also not faster than SVRG-
ADMM Zheng & Kwok (2016). Experiments are performed on Intel(R) CPU i7-4770 @ 3.40GHz
machine with 16 GB memory.

6.1 Lasso Problems

We perform experiments to solve two typical Lasso problems. The ﬁrst is the original Lasso problem:

min
x

µ(cid:107)x(cid:107)1 +

1
n

n
(cid:88)

i=1

(cid:107)hi − xT ai(cid:107)2,

(23)

where hi and ai are the tag and the data vector, respectively. The second is Graph-Guided Fused
Lasso model:

min
x

µ(cid:107)Ax(cid:107)1 +

1
n

n
(cid:88)

i=1

li(x),

(24)

2We will put our code online once our paper is accepted.

10

510152025303540number of effective passes 10-510-410-310-2objective gap510152025303540number of effective passes 10-310-2objective gap510152025303540number of effective passes 10-310-210-1objective gap510152025303540number of effective passes 10-410-3objective gap510152025303540number of effective passes 0.1120.1140.1160.1180.12test loss510152025303540number of effective passes 0.3690.370.3710.3720.3730.3740.3750.376test loss510152025303540number of effective passes 0.190.1950.20.2050.21test loss510152025303540number of effective passes 2.833.23.43.63.8test loss#10-3880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989FasterandNon-ergodicO(1/K)StochasticAlternatingDirectionMethodofMultipliers510152025303540number of effective passes 10-510-410-310-2objective gapSTOC-ADMMSTOC-ADMM-ERGOPT-ADMMSVRG-ADMMSVRG-ADMM-ERGSAG-ADMMSAG-ADMM-ERGACC-SADMM510152025303540number of effective passes 0.1120.1140.1160.1180.12test loss510152025303540number of effective passes 10-310-2objective gap510152025303540number of effective passes 0.370.3720.3740.376test loss510152025303540number of effective passes 10-310-210-1objective gap510152025303540number of effective passes 0.190.1950.20.2050.21test loss510152025303540number of effective passes 10-410-3objective gap(a) a9a(b) covertype(b) mnist(d) dna510152025303540number of effective passes 2.833.23.43.63.8test loss×10-3Figure3.Illustrationoftheproposedapproach.TheevolutionaryprocessofourPDE(solidarrow)withrespecttothetime(t=0,T/N,···,T,)extractsthefeaturefromtheimageandthegradientdescentprocess(hollowarrow)learnsatransformtorepresentthefeature.Frostig,Roy,Ge,Rong,Kakade,Sham,andSidford,Aaron.Un-regularizing:approximateproximalpointandfasterstochasticalgorithmsforempiricalriskmin-imization.InProc.Int’l.Conf.onMachineLearning,2015.He,BingshengandYuan,Xiaoming.OntheO(1/n)con-vergencerateofthedouglas–rachfordalternatingdirec-tionmethod.SIAMJournalonNumericalAnalysis,50(2):700–709,2012.Hien,LeThiKhanh,Lu,Canyi,Xu,Huan,andFeng,Ji-ashi.Acceleratedstochasticmirrordescentalgorithmsforcompositenon-stronglyconvexoptimization.arXivpreprintarXiv:1605.06892,2016.Johnson,RieandZhang,Tong.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2013.Kim,Seyoung,Sohn,Kyung-Ah,andXing,EricP.Amul-tivariateregressionapproachtoassociationanalysisofaquantitativetraitnetwork.Bioinformatics,25(12):i204–i212,2009.Li,HuanandLin,Zhouchen.OptimalnonergodicO(1/k)convergencerate:WhenlinearizedADMmeetsnes-terov’sextrapolation.arXivpreprintarXiv:1608.06366,2016.Lin,Hongzhou,Mairal,Julien,andHarchaoui,Zaid.Auniversalcatalystforﬁrst-orderoptimization.InProc.Conf.AdvancesinNeuralInformationProcessingSys-tems,2015a.Lin,Zhouchen,Liu,Risheng,andSu,Zhixun.Linearizedalternatingdirectionmethodwithadaptivepenaltyforlow-rankrepresentation.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2011.Lin,Zhouchen,Liu,Risheng,andLi,Huan.Linearizedalternatingdirectionmethodwithparallelsplittingandadaptivepenaltyforseparableconvexprogramsinma-chinelearning.MachineLearning,99(2):287–325,2015b.Lu,Canyi,Li,Huan,Lin,Zhouchen,andYan,Shuicheng.Fastproximallinearizedalternatingdirectionmethodofmultiplierwithparallelsplitting.arXivpreprintarX-iv:1511.05133,2015.Nesterov,Yurii.Amethodforunconstrainedconvexmini-mizationproblemwiththerateofconvergenceO(1/k2).InDokladyanSSSR,volume269,pp.543–547,1983.Nesterov,Yurii.Onanapproachtotheconstructionofop-timalmethodsofminimizationofsmoothconvexfunc-tions.EkonomikaiMateaticheskieMetody,24(3):509–517,1988.Nesterov,Yurii.Introductorylecturesonconvexoptimiza-tion:Abasiccourse,volume87.2013.Nitanda,Atsushi.Stochasticproximalgradientdescentwithaccelerationtechniques.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2014.Ouyang,Hua,He,Niao,Tran,Long,andGray,Alexan-derG.Stochasticalternatingdirectionmethodofmulti-pliers.Proc.Int’l.Conf.onMachineLearning,2013.(a) a9a-training

(b) covertype-training

(c) mnist-training

(d) dna-training

(e) a9a-testing

(f) covertype-testing

(g) mnist-testing

(h) dna-testing

Figure 2: Experimental results of solving the Graph-Guided Fused Lasso problem (Eq. (24)) (Top:
objective gap; Bottom: testing loss). The step size is tuned to be the best for each algorithm.
The computation time has included the cost of calculating full gradients for SVRG based methods.
SVRG-ADMM and SAG-ADMM are initialized by running STOC-ADMM for 3n
b iterations. “-ERG”
represents the ergodic results for the corresponding algorithms.

where li(x) is the logistic loss on sample i, and A = [G; I] is a matrix encoding the feature sparsity
pattern. G is the sparsity pattern of the graph obtained by sparse inverse covariance estima-
tion Friedman et al. (2008). The experiments are performed on four benchmark data sets: a9a,
covertype, mnist and dna3. The details of the dataset and the mini-batch size that we use in all
SADMM are shown in Table 2. And like Zhong & Kwok (2014) and Zheng & Kwok (2016), we ﬁx
µ to be 10−5 and report the performance based on (xt, Axt) to satisfy the constraints of ADMM.
Results are averaged over ﬁve repetitions. And we set m = 2n
for all the algorithms. To solve the
b
problems by ADMM methods, we introduce an variable y = x or y = Ax. The update for x can
be written as: xk+1 = xk − γ( ˜∇f2(x2) + βAT (Ax − y) + AT λ), where γ is the step size, which
depends on the penalty factor β and the Lipschitz constant L2. For the original Lasso (Eq (23)), L2
has a closed-form solution, namely, we set L2 = maxi (cid:107)ai(cid:107)2 = 14. So in this task, the step sizes are
set through theoretical guidances for each algorithm. For Graph-Guided Fused Lasso (Eq (24)), we
regard L2 as a tunable parameter and tune L2 to obtain the best step size for each algorithm, which
is similar to Zheng & Kwok (2016) and Zhong & Kwok (2014). Except ACC-SADMM, we use the
continuation technique Zuo & Lin (2011) to accelerate algorithms. We set βs = min(10, ρsβ0). Since
SAG-ADMM and SVRG-ADMM are sensitive to initial points, like Zheng & Kwok (2016), they are
initialized by running STOC-ADMM for 3n
b iterations. SAG-ADMM is performed on the ﬁrst three
datasets due to its large memory requirement. More details about parameter setting can be found
in Supplementary Materials.

The experimental results of original Lasso and Graph-Guided Fused Lasso are shown in Fig. 1
and Fig. 2, respectively. From the results, we can ﬁnd that SVRG-ADMM performs much better
than STOC-ADMM when the step size is large while the improvement is not large when the step

3a9a, covertype and dna are from: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/, and mnist is

from: http://yann.lecun.com/exdb/mnist/.

4We normalize the Frobenius norm of each feature to 1.

11

102030405060number of effective passes 10-310-210-1objective gap102030405060number of effective passes 10-510-410-310-210-1100objective gap102030405060number of effective passes 10-310-210-1100objective gap510152025303540number of effective passes 10-810-610-410-2100objective gap102030405060number of effective passes 0.3250.330.3350.340.3450.35test loss102030405060number of effective passes 0.530.5310.5320.5330.5340.535test loss102030405060number of effective passes 0.250.30.350.4test loss510152025303540number of effective passes 0.020.030.040.050.060.070.08test loss880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989FasterandNon-ergodicO(1/K)StochasticAlternatingDirectionMethodofMultipliers510152025303540number of effective passes 10-510-410-310-2objective gapSTOC-ADMMSTOC-ADMM-ERGOPT-ADMMSVRG-ADMMSVRG-ADMM-ERGSAG-ADMMSAG-ADMM-ERGACC-SADMM510152025303540number of effective passes 0.1120.1140.1160.1180.12test loss510152025303540number of effective passes 10-310-2objective gap510152025303540number of effective passes 0.370.3720.3740.376test loss510152025303540number of effective passes 10-310-210-1objective gap510152025303540number of effective passes 0.190.1950.20.2050.21test loss510152025303540number of effective passes 10-410-3objective gap(a) a9a(b) covertype(b) mnist(d) dna510152025303540number of effective passes 2.833.23.43.63.8test loss×10-3Figure3.Illustrationoftheproposedapproach.TheevolutionaryprocessofourPDE(solidarrow)withrespecttothetime(t=0,T/N,···,T,)extractsthefeaturefromtheimageandthegradientdescentprocess(hollowarrow)learnsatransformtorepresentthefeature.Frostig,Roy,Ge,Rong,Kakade,Sham,andSidford,Aaron.Un-regularizing:approximateproximalpointandfasterstochasticalgorithmsforempiricalriskmin-imization.InProc.Int’l.Conf.onMachineLearning,2015.He,BingshengandYuan,Xiaoming.OntheO(1/n)con-vergencerateofthedouglas–rachfordalternatingdirec-tionmethod.SIAMJournalonNumericalAnalysis,50(2):700–709,2012.Hien,LeThiKhanh,Lu,Canyi,Xu,Huan,andFeng,Ji-ashi.Acceleratedstochasticmirrordescentalgorithmsforcompositenon-stronglyconvexoptimization.arXivpreprintarXiv:1605.06892,2016.Johnson,RieandZhang,Tong.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2013.Kim,Seyoung,Sohn,Kyung-Ah,andXing,EricP.Amul-tivariateregressionapproachtoassociationanalysisofaquantitativetraitnetwork.Bioinformatics,25(12):i204–i212,2009.Li,HuanandLin,Zhouchen.OptimalnonergodicO(1/k)convergencerate:WhenlinearizedADMmeetsnes-terov’sextrapolation.arXivpreprintarXiv:1608.06366,2016.Lin,Hongzhou,Mairal,Julien,andHarchaoui,Zaid.Auniversalcatalystforﬁrst-orderoptimization.InProc.Conf.AdvancesinNeuralInformationProcessingSys-tems,2015a.Lin,Zhouchen,Liu,Risheng,andSu,Zhixun.Linearizedalternatingdirectionmethodwithadaptivepenaltyforlow-rankrepresentation.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2011.Lin,Zhouchen,Liu,Risheng,andLi,Huan.Linearizedalternatingdirectionmethodwithparallelsplittingandadaptivepenaltyforseparableconvexprogramsinma-chinelearning.MachineLearning,99(2):287–325,2015b.Lu,Canyi,Li,Huan,Lin,Zhouchen,andYan,Shuicheng.Fastproximallinearizedalternatingdirectionmethodofmultiplierwithparallelsplitting.arXivpreprintarX-iv:1511.05133,2015.Nesterov,Yurii.Amethodforunconstrainedconvexmini-mizationproblemwiththerateofconvergenceO(1/k2).InDokladyanSSSR,volume269,pp.543–547,1983.Nesterov,Yurii.Onanapproachtotheconstructionofop-timalmethodsofminimizationofsmoothconvexfunc-tions.EkonomikaiMateaticheskieMetody,24(3):509–517,1988.Nesterov,Yurii.Introductorylecturesonconvexoptimiza-tion:Abasiccourse,volume87.2013.Nitanda,Atsushi.Stochasticproximalgradientdescentwithaccelerationtechniques.InProc.Conf.AdvancesinNeuralInformationProcessingSystems,2014.Ouyang,Hua,He,Niao,Tran,Long,andGray,Alexan-derG.Stochasticalternatingdirectionmethodofmulti-pliers.Proc.Int’l.Conf.onMachineLearning,2013.(a) objective gap vs. iteration

(b) test error vs. iteration

Figure 3: The experimental result of Multitask Learning.

Table 2: Details of datasets. (Dim., Cla, and mini., are short for dimensionality, class, and mini-
batch, respectively. Las. is short for the Lasso problem. Mul. is short for Multitask Learning.).

Pro.

Las.

Mul.

Dataset
a9a
covertype
mnist
dna
ImageNet

# training # testing

72, 876
290, 506
60, 000
2, 400, 000
1, 281, 167

72, 875
290, 506
10, 000
600, 000
50, 000

Dim.
74
54
784
800

×
×
×
×
×

Cla. # mini.
2
2
10
2
1, 000

500
2, 000

100

4, 096

×

size is small as it has the cost of calculating the full gradient. SAG-ADMM encounters a similar
situation as x is not updated on the latest information. OPT-ADMM performs well on the small step
size. However, when the step size is large, the noise of the gradients limits the eﬀectiveness of the
extrapolation. Our algorithm is faster than other SADMM on both problems. More experimental
results where we set a larger ﬁxed step size and the memory costs of all algorithms are shown in
Supplementary Materials.

6.2 Multitask Learning

We perform experiments on multitask learning Argyriou et al. (2007). A similar experiment is
also conducted by Zheng & Kwok (2016). The experiment is performed on a 1000-class ImageNet
dataset Russakovsky et al. (2015) (see Table 2). The features are generated from the last fully
connected layer of the convolutional VGG-16 net Simonyan & Zisserman (2014). More detailed
descriptions on the problem are shown in Supplementary Materials.

Fig. 3 shows the objective gap and test error against iteration. Our method is also faster than

other SADMM.

7 Conclusion

We propose ACC-SADMM for the general convex ﬁnite-sum problems. ACC-SADMM integrates
Nesterov’s extrapolation and VR techniques and achieves a non-ergodic O(1/K) convergence rate.
We do experiments to demonstrate that our algorithm is faster than other SADMM.

12

510152025303540number of effective passes 10-1100101objective gapSTOC-ADMMOPT-ADMMSVRG-ADMMACC-SADMM510152025303540number of effective passes 0.320.340.360.380.4test error (%)STOC-ADMMOPT-ADMMSVRG-ADMMACC-ADMMReferences

Allen-Zhu, Zeyuan. Katyusha: The ﬁrst truly accelerated stochastic gradient method. In Annual

Symposium on the Theory of Computing, 2017.

Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano. Multi-task feature learning.

Proc. Conf. Advances in Neural Information Processing Systems, 2007.

AzadiSra, Samaneh and Sra, Suvrit. Towards an optimal stochastic alternating direction method of

multipliers. In Proc. Int’l. Conf. on Machine Learning, 2014.

Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

Bottou, L´eon. Stochastic learning. In Advanced lectures on machine learning, pp. 146–168. 2004.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.

Chen, Caihua, Chan, Raymond H, Ma, Shiqian, and Yang, Junfeng. Inertial proximal ADMM for
linearly constrained separable convex optimization. SIAM Journal on Imaging Sciences, 8(4):
2239–2267, 2015.

Davis, Damek and Yin, Wotao. Convergence rate analysis of several splitting schemes. In Splitting

Methods in Communication, Imaging, Science, and Engineering, pp. 115–163. 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Proc. Conf. Advances in
Neural Information Processing Systems, 2014.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. Sparse inverse covariance estimation

with the graphical lasso. Biostatistics, 9(3):432–441, 2008.

Frostig, Roy, Ge, Rong, Kakade, Sham, and Sidford, Aaron. Un-regularizing: approximate proximal
point and faster stochastic algorithms for empirical risk minimization. In Proc. Int’l. Conf. on
Machine Learning, 2015.

He, Bingsheng and Yuan, Xiaoming. On the O(1/n) convergence rate of the Douglas–Rachford

alternating direction method. SIAM Journal on Numerical Analysis, 50(2):700–709, 2012.

Hien, Le Thi Khanh, Lu, Canyi, Xu, Huan, and Feng, Jiashi. Accelerated stochastic mirror descent
algorithms for composite non-strongly convex optimization. arXiv preprint arXiv:1605.06892,
2016.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Proc. Conf. Advances in Neural Information Processing Systems, 2013.

Kim, Seyoung, Sohn, Kyung-Ah, and Xing, Eric P. A multivariate regression approach to association

analysis of a quantitative trait network. Bioinformatics, 25(12):i204–i212, 2009.

Li, Huan and Lin, Zhouchen. Optimal nonergodic O(1/k) convergence rate: When linearized ADM

meets nesterov’s extrapolation. arXiv preprint arXiv:1608.06366, 2016.

Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid. A universal catalyst for ﬁrst-order optimization.

In Proc. Conf. Advances in Neural Information Processing Systems, 2015a.

Lin, Zhouchen, Liu, Risheng, and Su, Zhixun. Linearized alternating direction method with adaptive
penalty for low-rank representation. In Proc. Conf. Advances in Neural Information Processing
Systems, 2011.

Lin, Zhouchen, Liu, Risheng, and Li, Huan. Linearized alternating direction method with paral-
lel splitting and adaptive penalty for separable convex programs in machine learning. Machine
Learning, 99(2):287–325, 2015b.

13

Lu, Canyi, Li, Huan, Lin, Zhouchen, and Yan, Shuicheng. Fast proximal linearized alternating direc-
tion method of multiplier with parallel splitting. In Proc. AAAI Conf. on Artiﬁcial Intelligence,
2016.

Nesterov, Yurii. A method for unconstrained convex minimization problem with the rate of conver-

gence O(1/k2). In Doklady an SSSR, volume 269, pp. 543–547, 1983.

Nesterov, Yurii. On an approach to the construction of optimal methods of minimization of smooth

convex functions. Ekonomika i Mateaticheskie Metody, 24(3):509–517, 1988.

Nesterov, Yurii. Introductory lectures on convex optimization: A basic course, volume 87. 2013.
Nitanda, Atsushi. Stochastic proximal gradient descent with acceleration techniques. In Proc. Conf.

Advances in Neural Information Processing Systems, 2014.

Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander G. Stochastic alternating direction

method of multipliers. Proc. Int’l. Conf. on Machine Learning, 2013.

Ouyang, Yuyuan, Chen, Yunmei, Lan, Guanghui, and Pasiliao Jr, Eduardo. An accelerated linearized
alternating direction method of multipliers. SIAM Journal on Imaging Sciences, 8(1):644–681,
2015.

Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang,
Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, et al. Imagenet large scale visual
recognition challenge. Int’l. Journal of Computer Vision, 115(3):211–252, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, pp. 1–30, 2013.

Shen, Li, Sun, Gang, Lin, Zhouchen, Huang, Qingming, and Wu, Enhua. Adaptive sharing for image

classiﬁcation. In Proc. Int’l. Joint Conf. on Artiﬁcial Intelligence, 2015.

Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

Suzuki, Taiji. Dual averaging and proximal gradient descent for online alternating direction multiplier

method. In Proc. Int’l. Conf. on Machine Learning, 2013.

Suzuki, Taiji. Stochastic dual coordinate ascent with alternating direction method of multipliers. In

Proc. Int’l. Conf. on Machine Learning, 2014.

Tseng, Paul. On accelerated proximal gradient methods for convex-concave optimization. In Tech-

nical report, 2008.

Wang, Kaiye, He, Ran, Wang, Liang, Wang, Wei, and Tan, Tieniu. Joint feature selection and
subspace learning for cross-modal retrieval. IEEE Trans. on Pattern Analysis and Machine Intel-
ligence, 38(10):1–1, 2016.

Zhang, Xiaoqun, Burger, Martin, and Osher, Stanley. A uniﬁed primal-dual algorithm framework

based on bregman iteration. Journal of Scientiﬁc Computing, 46:20–46, 2011.

Zheng, Shuai and Kwok, James T. Fast-and-light stochastic admm. In Proc. Int’l. Joint Conf. on

Artiﬁcial Intelligence, 2016.

Zhong, Wenliang and Kwok, James Tin-Yau. Fast stochastic alternating direction method of mul-

tipliers. In Proc. Int’l. Conf. on Machine Learning, 2014.

Zuo, Wangmeng and Lin, Zhouchen. A generalized accelerated proximal gradient approach for total

variation-based image restoration. IEEE Trans. on Image Processing, 20(10):2748, 2011.

14

