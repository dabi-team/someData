0
2
0
2

l
u
J

9
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
9
8
6
4
1
.
7
0
0
2
:
v
i
X
r
a

Generating a Machine-learned Equation of State

for Fluid Properties

Kezheng Zhu and Erich A. M¨uller∗

Department of Chemical Engineering, Imperial College London, U.K.

E-mail: e.muller@imperial.ac.uk

Abstract

Equations of State (EoS) for ﬂuids have been a staple of engineering design and

practice for over a century. Available EoS are based on the ﬁtting of a closed-form

analytical expression to suitable experimental data. The underlying mathematical

structure and the underlying physical model signiﬁcantly restrain the applicability and

accuracy of the resulting EoS. This contribution explores the issues surrounding the

substitution of analytical EoS for machine-learned models, in particular, we describe,

as a proof of concept, the eﬀectiveness of a machine-learned model to replicate statisti-

cal associating ﬂuid theory (SAFT-VR-Mie) EoS for pure ﬂuids. By utilizing Artiﬁcial

Neural Network and Gaussian Process Regression, predictions of thermodynamic prop-

erties such as critical pressure and temperature, vapor pressures and densities of pure

model ﬂuids are performed based on molecular descriptors. To quantify the eﬀective-

ness of the Machine Learning techniques, a large data set is constructed using the

comparisons between the Machine-Learned EoS and the surrogate data set suggest

that the proposed approach shows promise as a viable technique for the correlation,

extrapolation and prediction of thermophysical properties of ﬂuids.

1

 
 
 
 
 
 
Introduction

The virtual issue where this contribution is included and its predecessor 1 are devoted to

showcasing examples where machine learning (ML) has been employed to contribute to the

physical sciences. The reader is referred to the indexes of these issues for examples and

reviews of the most recent advances in this rapidly evolving topical ﬁeld. Inspection of this,

and other literature, suggests that while the pharma and biomedical research groups have, in

growing numbers, been successfully employing ML to, for example, explore candidate drug

molecules, 2,3 or the design of new bioactive ones, 4 the application of ML in the physical and

chemical sciences has been much less prevalent. 5,6 In particular, a much slower progress has

been evidenced in the ﬁeld of engineering, with salient counterexamples in process system

modelling 7 and the prediction of properties of inorganic materials. 8–10 Given this general

scenario, this manuscript deals with the premise that the currently available hardware and

software can allow the wide-spread implementation of ML to the prediction and correlation

of thermophysical properties of ﬂuids.

Physical properties of ﬂuids can, and have, been measured experimentally since the dawn

of modern science. Densities, vapor pressures, critical point data, viscosities, solubilities, sur-

face tensions, thermal conductivities, etc. can all be routinely measured and are collated

in existing open-access and closed-source databanks. 11–14 Pure compound data is presently

available for several thousand to tens of thousands chemical compounds of interest (de-

pending on the databank), which pales against the 84 million chemical structures positively

identiﬁed to date 11 and the 166.4 billion possible molecules of up to 17 atoms of C, N, O, S

and halogens collected in GBD-17. 15 When one considers mixtures, a much smaller (relative)

number of systems have been explored. The slow pace (and cost) of experimental acquisi-

tion of data is at odds with the massive phase space of pure and mixture systems of interest.

Databases themselves grow at a slow pace, to the order of 1 M data points per year. Entire

scientiﬁc journals are devoted to this pursuit and the progress is very incremental, at the

most.

2

The current engineering folklore for dealing with this lack of data is to use empirical

correlations and/or ﬁtted simpliﬁed theories to essentially interpolate and/or extend the

results. Historians of Thermodynamics have given detailed account of the quest of scientists

to rationalize the interrelation between the thermophysical properties of ﬂuids. 16,17 On the

other hand, the prediction of properties by quantum mechanical calculations are restricted

to small, relatively simple molecules and the use of molecular simulation using empirical

force-ﬁelds also has limits in terms of the speed of calculation and the estimation of the

expected accuracy. 18,19 Given this scenario, there is still a pressing need for the correlation,

and possible generalization, of available experimental data.

The correlation of volumetric properties of ﬂuids has it most renowned exemplary in the

1910 physics Nobel prize awarded to J.D. van der Waals, for the recognition that analytical

(mathematical) models were capable of modelling vapor-liquid equilibria. Surprisingly, a

century later, these simple expressions, better known today as cubic Equations of State

(EoS) are a staple of engineering design. Expectedly, the ﬁeld has advanced and extremely

reﬁned EoS, are now available. 20 Current propositions are based on our interpretation of the

molecular nature of matter. i.e. theories start with an expected simpliﬁed molecular model

and develop corresponding approximations in order to arrive to a closed form expression.

The resulting model is further commonly employed as correlation tool, with parameters ﬁt

to experimental properties. There are several restricting limitations to this approach as

current EoS will commonly have limits with respect to the range of chemical interactions

(e.g. length and shape of molecules, polarity, presence of ions) and the availability of pure

ﬂuid and mixture parameters.

The quest for employing ML in Thermodynamics as a correlation tool is not new, but im-

provements in the available hardware and software have produced a resurgence in the ﬁeld. 21

Most of the eﬀort has been focused on employing artiﬁcial neural networks (ANN), mostly

due to the ease in which they can be deployed (see for example ref. 22 for classroom examples

of creating boiling point correlations with ANN). A review of some of the ﬁrst applications

3

of ANN to the chemical sciences and engineering was provided by Himmelblau. 23 Further

to that, ANNs have been used to selectively correlate a limited number of thermophysical

properties of restricted families of compounds, for example alkanes, 24,25 ionic liquids, 26 refrig-

erants, 27,28 components of biofuels, 29 gases. 30,31 Critical properties, 32 Interfacial properties 33

and partition coeﬃcients 34,35 have all been individually explored. The reader is referred to

an excellent recent review by Forte et al 36 and references therein for a modern account.

In most of the aforementioned examples, the onus has been on the correlation of a selected

and well-chosen sub-set of properties of a well-deﬁned family of chemical compounds. As

such, the versatility and universality of common engineering methods (e.g. EoS) has not been

matched. This contribution aims at taking a step in this direction, exploring the question of

whether current ML approaches can be employed to substitute analytical EoS. In a seminal

contribution, Arce et al. 37 compared the quality of ﬁt of common EoS for binary systems

involving solvents and biodiesel components at supercritical conditions to that of an ANN,

ﬁnding a good agreement between both the EoS, the ANN and experimental data. The

work of Arce et al. suggests that an ANN could be built to substitute an analytical EoS.

But further questions arise, is the error associated with the correlation acceptable? More

importantly, are ANN the optimal choice of a ML algorithm for this purpose? A recent

contribution 38 pointed towards ANNs as being the most robust for this purpose, but lauded

the beneﬁts of kernel-based methods, leaving the matter unsolved.

To assess the above-posed questions, in a most general (and ideal scenario), one would

start with experimental data and use part of the database as a training set to ﬁt a ML model.

However, experimental data is both noisy and very poorly distributed; a larger abundance

of data points are at experimentally easy conditions, which may or may not be the most

appropriate to describe the whole phase space. To circumvent this, we chose to generate

pseudo-experimental data from a current state-of-the-art analytical EoS, the Statistical As-

sociating Fluid Theory variable range Mie equation 39 (SAFT-VR-Mie). This procedure,

developed in this contribution, allows us to a) test the capacity of the ML algorithms to

4

correlate the data, b) To generate a large amount of pseudo-data covering all regions of the

relevant phase space, and most importantly c) allows for a well-deﬁned characterization of

the error associated with the ML ﬁtting.

SAFT Equation of State

The SAFT-VR-Mie framework is a coarse-grained model wherein molecules are depicted

as collections of tangent spherical segments, colloquially termed as beads. The interactions

acting between two identical segments follows the Mie potential (a generalized Lennard-Jones

potential), which is speciﬁed by four force ﬁeld parameters (σ, ε, λr, λa). The distinguishing

aspect of the SAFT force ﬁeld model is that there is a direct correspondence between the

underlying force ﬁeld and the analytical EoS, i.e. the molecular simulations and the EoS

produce the same outputs for a given set of molecular parameters. In terms of what concerns

us here, the advantage of the SAFT formulation is that molecules are described through a set

of descriptors that relate directly to molecular properties. A wide range of real substances

has been shown to be modelled eﬀectively as chains of coarse-grained beads, 40,41 from which

ﬂuid behaviour and thermodynamic properties can be determined. We stress from the onset

that the focus of this contribution is not to correlate experimental data and/or to produce

the ultimate EoS, but to assess the extent to which a ML model can replace an analytical

EoS in a well deﬁned context. The underlying force ﬁeld is based on the Mie potential: 42,43

UMie(rij) = CMieε

(cid:34)(cid:18) σ
rij

(cid:19)λr

(cid:18) σ
rij

−

(cid:19)λa(cid:35)

(1)

where r is the inter-segment distance, ε is the potential depth, σ is the segment diameter

(or position at which the potential is zero), and λr and λa are the repulsive and attractive

exponents which characterize the pair-wise energy. The constant CMie is deﬁned as:

(cid:19) λa
λr

λa

−

CMie =

(cid:18) λr
λr

−

(cid:19) (cid:18) λr
λa

λa

5

(2)

Tying together the Mie potential and thermodynamic properties is the SAFT-VR-Mie equa-

tion of state, 39 a closed analytical form of the Helmholtz free energy for chains of spherical

segments interacting through the Mie potential. The SAFT-VR-Mie is a special case of

the SAFT-γ model 44 where molecules are represented by a collection of homonuclear beads

with no distinction between functional groups in a molecule. The use of the SAFT-γ model

implies a higher level of complexity which is irrelevant to the objectives of this study. The

reader is referred to Laﬁtte et al. 39 for the explicit expressions in the SAFT-VR-Mie equation

of state.

In a typical use of Mie force ﬁeld parameters (σ, ε, λr, λa) to represent a ﬂuid, we look to

the SAFT-VR-Mie EoS to estimate the parameters that provide the best representation of

available macroscopic experimental data. This procedure has been performed for thousands

of ﬂuids, and the parameters are available in the web tool named Bottled SAFT.. 45 As

the EoS explicitly calculates Helmholtz free energy, macroscopic thermodynamic properties

such as the vapor pressure Pv and saturated liquid density ρL and equilibria vapor-liquid

conditions can be derived directly by standard thermodynamic relationships.

It should

be noted that other second-derivative properties such as Joule-Thomson coeﬃcient, heat

capacity and speed of sound can also be extracted from the EoS.

By using the EoS to produce ﬂuid phase equilibria data for a wide range of descriptors

corresponding to hypothetical “Mie ﬂuids”, a vast amount of data can be generated for the

use of training a ML model. The following sections will very brieﬂy describe two algorithms

which were employed in this work: artiﬁcial neural network (ANN) and Gaussian process

regression (GPR).

Artiﬁcial Neural Network (ANN)

ANN is a computational model partly inspired by biological neural systems, where inter-

connected neurons create a network of mathematical operations to model multi-dimensional

problems. 46,47 The most common type of ANN is a feed-forward neural network, where neu-

6

rons are ordered in interconnected layers: an input layer, hidden layer(s) and an output

layer. Each neuron represents a single unit of computation, where it receives a set of inputs

for which a weight is assigned based on relative importance. Taking the weighted sum of its

inputs, the neuron then generates an output by adding a bias. Mathematically, output y of

a single layer with m neurons given an input vector x of size n can be expressed as:

y = Wx + b =









w1,1 w1,2
...
...

. . . w1,n
...
. . .

wm,1 wm,2

. . . wm,n

























x1
...

xn

+









b1
...

bm









(3)

where w is the weight matrix (m

×

vector corresponding to the layer.

n) for each input element and each neuron, b is the bias

The linear output of each neuron will also be passed through an activation function to

introduce non-linearity into an ANN model, with common functions such as the sigmoid

function, hyperbolic tangent function and exponential linear unit (ELU). The continuity

of derivatives for the ANN model is an important criterion when considering the choice of

activation functions, for which the Tanh and ELU functions are used:

The Tanh or hyperbolic tangent activation function transforms values of x to a range between

-1 and 1,

f (x) = tanh(x) =

1
e−
−
1 + e−

2x

2x

(4)

while the ELU or Exponential linear unit changes the value of x to an exponential decay.

f (x) =





x

α (ex

1)

−

x > 0

x < 0

7

(5)

Mathematically, a N -layered neural network can be written as:

y = fN (WN . . . (W2f1(W1x + b1) + b2)

+ bN )

· · ·

(6)

To train the ANN, supervised learning is employed. By providing the input data (or features)

and output data (or labels), the ANN is trained by evaluating the inputs, comparing the

outputs and adjusting the parameters. The neural network is initialized with randomly

assigned weights and biases, while employing an optimizer such as stochastic gradient descent

(SGD) or the Adam optimizer 48 to minimize the loss function deﬁned by the mean squared

error (MSE) between the predicted outputs and the training data.

Aside from a common feed-forward neural network, there are multiple types of ANN

developed for diﬀerent purposes. Convolutional layers in neural network (CNN) corresponds

to inputs arranged in a 2 dimensional matrix, employs ﬁlter (or kernel) in hidden layers to

recognize edges for image recognition. 49 Recurrent neural network (RNN) works memory

storage in its neurons such that past iterations has an eﬀect on current iteration, primarily

used for time series or problems involving unknown number of features. 50 Diﬀerent network

structures exist outside of feed-forward neural networks, with architectures such as bridged

multi-layer (BMLP) and fully connected cascade (FCC) 51 where non-adjacent layers are

connected such that there are more adjustable parameters for lesser neurons. With the wide

variety of neuron cells, activation functions and possible network architectures, ANN has a

key advantage of ﬂexibility in implementation amongst machine learning algorithms.

In this work, we used a multilayer perceptron ANN, referring to a feed-forward neural

network with multiple hidden layers, along with the SGD optimizer. The SGD optimizer is

common staple of optimization in deep learning, where gradient descent towards the objective

function (minimizing loss function) is done with an estimate of the gradient, considering a

random subset of data to reduce computational burden. Although the Adam optimizer is

widely recommended as the default optimizer for deep learning, 52 the adaptive nature of the

8

optimization algorithm suits problems with sparse gradients and/or noisy problems, which

is not required here.

Gaussian Process Regression (GPR)

A Gaussian process (GP) is deﬁned as a collection of Gaussian distributed random variables

f , for which a function can be represented as an inﬁnitely long vector of such random variables

[f1, f2, . . . ]. 53 Each distribution or point exhibits a corresponding mean µ and a covariance

vector Σ, while the function itself is speciﬁed by a mean function m(xi) (the average of the

distribution for all the random variables) and a covariance function k(xi, xj) (kernel). 53

A GP model “learns” by “observing” data points and deriving the posterior distribution

from the prior by determining the conditional probability through the covariance matrix. In

simple terms, by treating the functions as random variables fi, any two variable is correlated

through the covariance function (or kernel). By observing a single point fa, or a set of

values, corresponding to a point in the phase space, any other point in the phase space

can be associated with a new observed value using the prior distribution and a Gaussian

likelihood function. The result is a posterior distribution, where the mean, variance and

covariance matrix can be derived from the prior. Repeating the process for the whole data

set, the GP model “learns” as each data point is “observed”.

The covariance function, or kernel, in GP regression (GPR) determines the shape and

smoothness of the mean function. In this work, the use of the Radial-basis function (RBF)

and the Mat´ern kernel is explored.

The radial basis function (RBF) is also known as the squared exponential covariance

function, can be expressed as:

K (xi, xj) = σ2 exp

(cid:18)

−

xi

||

xj

−
2l2

(cid:19)

2
||

(7)

where

xi

||

xj

2 is the Euclidean distance between the two data points xi and xj, l is a
||

−

9

trainable length scale parameter and σ2 is the variance. Taking d as the Euclidean distance,

the Mat´ern kernel 54 is expressed as:

K (xi, xj) = σ2 21

ν

−
Γ(ν)

(cid:18)

√2ν

(cid:19)ν

d
l

(cid:18)

√2ν

Kν

(cid:19)

d
l

(8)

where Γ is the gamma function, Kν is the modiﬁed Bessel function, and ν is an additional

parameter for which larger ν indicates a smoother covariance function. The Mat´ern function

(Cν) converges to RBF as ν approaches inﬁnity. Most commonly, ν = 1/2, 3/2 and 5/2 are

used, corresponding to the Matern12, Matern32 and Matern52 kernel:

Cν=1/2 = σ2 exp

(cid:18)

(cid:32)

Cν=3/2 = σ2

1 +

(cid:19)

d
l

−
√3d
l

(cid:33)

(cid:32)

exp

(cid:33)

√3d
l

−

(cid:32)

Cν=5/2 = σ2

1 +

√5d
l

+

5d2
3l2

(cid:33)

(cid:32)

exp

(cid:33)

√5d
l

−

(9)

(10)

(11)

It is important to note that the Mat´ern kernel is diﬀerentiable to

ν

(cid:100)

(cid:101) −

1: the 1/2 kernel

is not diﬀerentiable, 3/2 is once diﬀerentiable and 5/2 is twice diﬀerentiable. Naturally, the

RBF is inﬁnitely diﬀerentiable. The reader is referred to Williams and Rasmussen 53 for

detailed explanation of GPR and the kernel functions.

The key advantage is using GPR is its ability to generate accurate models with less but

noise-free data, or data with known error margin. This is advantageous for models where the

user knows the inherent smoothness and/or function shape of the model. With the ability to

calculate variance of the output function, GPR also has a built-in error indicator, which can

be advantageous for researchers to determine the next data point to conduct experiment or

simulation to improve the model. The drawback with GPR is the computation complexity:

due to the inversion of the covariance matrix, the computation time scales in O(n3) with

respect to data size of n. 55

10

Methodology

In this study, we examine the ability of both non-linear ANN and GPR to correlate thermo-

dynamic properties in the way a traditional EoS would. Since the ML algorithm relies on the

processing of large amounts of data, the direct use of experimental data might turn out to

produce inconclusive results, as the typical data points will be biased in quantity and quality

towards the experimentally “easier” state points (e.g. room temperature and/or pressure).

By generating a larger database of pseudo-experimental data employing the SAFT EoS, we

remove these limitations and biases and gauge both the ability of ML models to recognize

and correlate the data, and are able, by direct comparison to the EoS, to quantify the associ-

ated error. Within the SAFT-VR-Mie framework, the parameters which describe a molecule

are ms, σ, ε, λr and λa. In this particular scenario, the conformality of the Mie potential 56

helps reduce the dimensionality of the problem by setting λa to 6 and scaling properties with

respect to the length (σ) and energy (ε) scale. The result is that a given molecule can be

uniquely characterized by the number of spheres ms and the repulsive exponent λr. 45,57 In

particular, we look at the dimensionless form of temperature T , pressure P , and segment

density ρ.

T ∗ =

P ∗ =

kT
ε
P σ3
ε

ρ∗ = ρσ3

(12)

(13)

(14)

Although we ﬁx the attractive parameter to λa = 6, the combination of repulsive and

attractive exponents can be replaced with a unique parameter α, where diﬀerent exponent

pairs (λr,λa) giving the same value of α parameters provide the same dimensionless vapor-

11

liquid equilibrium (VLE) behaviour. 56 The α parameter is given by:

α = CMie

(cid:20)(cid:18) 1
λa

−

(cid:19)

3

−

(cid:18) 1
λr

−

3

(cid:19)(cid:21)

(15)

hence α and λr may be used indistinctively to characterize the shape of the potential.

To compare the eﬀectiveness of ANN and GPR for diﬀerent levels of complexity, we

attempt to replicate the equation of state for critical properties, VLE properties and super-

critical density. Each set of properties corresponds to a problem with diﬀerent complexity,

being a 2-dimensional problem (2 input features) for critical properties, 3-dimensional prob-

lem for VLE properties and 4-dimensional problem for supercritical (or one phase) density.

For ANN, the choice of the number of nodes and layers is linked to the complexity

of the problem. The approach used here is to gradually increase the number of layers

and nodes just until the accuracy of the predicted model plateaus to prevent over-ﬁtting.

For more than 1 hidden layer, subsequent layers have lesser nodes in order to compress

information approaching the output. 58 As we are employing ANN for a regression problem,

the activation functions used should be a non-linear continuous function. The tanh function

is chosen as it is inﬁnitely diﬀerentiable and has continuous derivatives (the second derivative

of an ELU function is discontinuous), a property that is important in the applications of

thermodynamics. The overall data sets are also split into a training and validation set, with

a ratio of 0.8 training data vs 0.2 validation data which the model will not see and will be

validated against. The implementation of the ANN model is done using PyTorch. 59

For GPR, a linear combination of the Radial Basis Function and a linear kernel for each

dimension is used to ensure that the produced model is inﬁnitely diﬀerentiable. Due to the

signiﬁcant increase in computational time for each additional data point and the low marginal

beneﬁt of increasing dataset beyond a certain limit, we ﬁnd that keeping the dataset between

500 to 1000 points results in suﬃciently accurate models for analysis.

12

Figure 1: Plot of selected generated data for reduced temperature, T ∗, (with respect to
energy scale) against saturated liquid and vapor density, ρ∗, for molecules with the same
repulsive exponent λr = 16 but diﬀerent number of segments ms (left). Regularization is
accomplished by scaling the temperature with respect to the critical point, T ∗c , and expressing
the molecular density in terms of the segment density, ρ∗s. In this way, direct and known
relationships (such as that between density and ms) are removed, allowing for the model to
regress solely the subtle changes in response to diﬀerent molecular descriptors (right).

Figure 2: Plot of selected generated data for reduced temperature, T ∗, (with respect to
energy scale) against saturated liquid and vapor density, ρ∗, for molecules with the same
number of segments ms = 1 but diﬀerent repulsive exponent λr (left). Similar to Figure 1,
scaling temperature with respect to critical temperature results in a more concise data set
for modelling (right).

Data Transformations

Although, in principle, ML algorithms seem capable of deducing the relationships amongst

explicit and/or hidden properties, a more robust prediction can be obtained if one can

incorporate some known information in the model. Furthermore, regularization (understood

13

00.20.40.60.810.511.522.5ms=1ms=2ms=3ms=5ms=8ms=12ρ∗T∗00.20.40.60.810.40.60.811.2ρ∗sT∗rT∗r=T∗T∗cρ∗s=msρ∗00.20.40.60.810.511.52ρ∗T∗λr=8λr=10λr=13λr=18λr=3400.20.40.60.810.40.60.811.2ρ∗T∗rT∗r=T∗T∗chere as an appropriate scaling of the data) is a key trick of the trade in ML, as it reduces

bias in the correlation of data. Prior knowledge of simple thermodynamic relationships

helps in this respect. For example, the boiling point T ∗ and vapor pressure P ∗v for the VLE

envelope converges at the critical point (T ∗c , P ∗c ), hence scaling the temperature inputs and

vapor pressure outputs with the critical temperature and pressure of a molecule respectively

reduces the impact of diﬀering critical point for molecules while scaling the absolute values to

the range [0,1]. Similarly, the saturated liquid and vapor density expressed in the molecular

density ρ∗ diﬀers greatly with respect to ms, while the segment density ρ∗s = msρ∗ for all

molecules varies in a tighter range.

Figure 1 illustrates the variation of the VLE phase envelope for the same repulsive ex-

ponent λr but diﬀerent number of segments ms, and the eﬀect of data transformation based

on existing knowledge. Similarly, Figure 2 exempliﬁes the eﬀect of scaling with respect to

critical temperature when considering diﬀerent repulsive exponents for the same ms. By

introducing prior scientiﬁc knowledge of the system behaviour into transforming the data,

the data-driven model can be trained more eﬀectively to the desired correlations we wish to

observe.

Critical Properties

The critical point on the phase diagram of any ﬂuid corresponds to a temperature, T ∗c , and

pressure, P ∗c , where the vapor-liquid boundary terminates. The critical point has zero degrees

of freedom as for any pure substance, the critical point can only exist at one temperature

and one pressure. In order to predict critical properties from the molecular descriptors, the

problem could be deﬁned as:

F1 : (ms, α)

→

(T ∗c , P ∗c )

(16)

where

F

i refers to the speciﬁc relation we seek to discover.

Using the SAFT-VR Mie equation of state, the critical properties can be obtained by

14

using a solving algorithm for the stated condition above. By sampling diﬀerent repulsive

exponents λr (directly related to α, as λa = 6) at a typical range of 8 - 34 and number of

segments ms at 1 to 20, 540 data points are generated in order to train a machine learning

model.

VLE Properties

For a pure ﬂuid in vapor-liquid equilibrium, the two coexisting phases must satisfy the me-

chanical and diﬀusive equilibrium conditions. Provided that the pressure of both phases is

equal, PL = PV , and the Gibbs free energy (or chemical potential) is also equal, GL = GV ,

one is left with a single degree of freedom. This allows us to formulate a three-dimensional

problem with respect to temperature T to predict VLE properties vapor pressure Pv, satu-

rated liquid density ρL and saturated vapor density ρV .

F2 : (ms, α, T ∗r )

→

(P ∗v,r, ρ∗s,L, ρ∗s,V )

(17)

The subscript s corresponds to segment density, which is used to transform the calculated

density as the saturated densities scales signiﬁcantly number of segments ms (cf. Figure 1).

Temperature is scaled with respect to the critical temperature (T ∗r = T ∗/T ∗c ) (see Figure

1 and 2) and vapor pressure is scaled with respect to the critical pressure (P ∗v,r = P ∗v /P ∗c ).

For the case of vapor pressure, it is also useful to explore the feasibility of linearizing the

temperature-pressure relationship by transforming the reduced temperature and pressure to

1/T ∗r and ln P ∗v,r respectively, in agreement with the ClausiusClapeyron equation: 60

ln

P2
P1

=

∆Hv
R

−

(cid:18) 1

T2 −

(cid:19)

1
T1

(18)

where ∆Hv is the heat of vaporization and R is the ideal gas constant. Note that the values

of ∆Hv and R are not used in the linearization discussed above.

15,000 VLE data points are generated for 540 diﬀerent Mie ﬂuids, sampling temperatures

15

between 0.5Tc to 0.98Tc at regular intervals of 0.02.

Supercritical Density

In a single supercritical phase, there are two degrees of freedom for state properties, for

which temperature T and pressure P are typically set as independent variables. Modelling

density of a single phase ﬂuid corresponds to a four-dimensional problem:

F3 : (ms, α, T ∗r , P ∗r )

→

(ρ∗s)

(19)

For this case, 27,000 supercritical density data points were generated from 540 diﬀerent Mie

ﬂuids, sampling temperatures between Tc to 2Tc and pressures between Pc to 2Pc, with a

uniformly random sampling method.

Model Evaluation

For each model, the performance and accuracy of the trained machine-learning model is

determined by collecting at statistical indicators such as R2 values, mean squared error

(MSE) and absolute average deviation (AAD). Although the use of statistical indicators has

the advantage of evaluating a model more robustly, it is important to also visually evaluate

the individual VLE envelope and isotherms shapes to detect and isolate systematic errors

and deviations.

Results

Critical Properties

Using an ANN with a structure of 3 hidden layers (20,10,5), a ML model was ﬁtted to predict

critical pressure and critical temperature between ms = 1 to 20 and λr = 8 to 34. With a

training data ratio of 0.8, the model performed with high statistical accuracy, achieving R2

16

Table 1: Input speciﬁcations and model performance for ANN models of critical pressure
and critical temperature

ANN Speciﬁcations
Input
Output
Hidden Layers
Activation Functions
Training Data Points
Validation Data Points
Model Performance
Coeﬃcient of Determination R2
Mean Squared Error (MSE)
Average Absolute Deviation (AAD)

ms, α
P ∗c , T ∗c
(15, 10, 5)
tanh
432
108
P ∗c
0.9999
3.4
×
1.0%

8
10−

T ∗c
0.9999
5.9
×
0.2%

5
10−

values of 0.9999 and absolute average deviation of less than 1% for both dimensionless critical

pressure and critical temperature. Further transformation of the outputs (such as taking the

natural log of critical pressure) was not necessary as it did not improve on the performance.

Figure 3 shows the scatter plot of all training and validation data points between model

predicted values and original data values.

With a combination of RBF and linear kernels, GPR can predict critical points with

similar performance (R2 = 0.9999) with only 300 data points. In addition to predicting the

critical properties, GPR has an advantage of producing the variances at each predicted point

which could be used as an error indicator. In general, this particular feature of GPR is useful

as it has a direct application in extensions of this work, as new data points can be included

in the EoS tuning by selecting the point with highest variance in the GPR model.

In Figure 4, a colour plot of the variance of critical temperature for the corresponding

ranges of ms and λr is illustrated. From the GPR model of critical properties, the variance

plot shows signiﬁcant edge eﬀects which causes larger uncertainty for the minimum and

maximum values in the range of each inputs. In particular, the corners of the 2-dimensional

problem has the largest variance. This, on its own, is not surprising and is an artefact of the

way the GPR model ﬁts the data. As a general suggestion, increasing the range beyond the

desired working phase space should be considered as a strategy for more accurate predictions.

17

Figure 3: Plot of neural network predicted dimensionless critical pressure P ∗c (left) and
critical temperature T ∗c (right) against original SAFT-VR calculated values. Blue histograms
describe the distribution of data points.

Figure 4: Variance σ2 of critical temperature evaluated with Gaussian process regression
(GPR) model, as spread across the phase space of molecular descriptors: number of spheres
(ms) and repulsive exponent (λr).

Vapor Pressure

For vapor pressure, the viability of taking the data transformation based on the Clasius-

Clapeyron equation is explored. Using an ANN with a structure of 3 hidden layers (48,24,12),

18

00.10.20.10.2R2=0.9999MSE=3.4×10−8AAD=1.0%EoSP∗cPredictedP∗c135135R2=0.9999MSE=5.9×10−5AAD=0.2%EoST∗cPredictedT∗c15101520814202632msλr00.20.40.60.8Table 2: Input speciﬁcations and model performance of ANN models for vapor pressure,
saturated liquid density and satuarated vapor density.

ANN Speciﬁcations
Input
Output
Hidden Layers
Activation Functions
Training Data Points
Validation Data Points
Model Performance
Coeﬃcient of Determination R2
Mean Squared Error (MSE)
Average Absolute Deviation (AAD)

ms, α, 1/T ∗
ln P ∗v /P ∗c , ρ∗L, ρ∗V
(48, 24, 12)
tanh
12861
3216
P ∗v
0.9985
2.16
4.7%

10−

×

6

ρ∗L
0.9995
1.37
0.5%

×

5

10−

ρ∗V
0.9987
5.50
2.7%

×

6
10−

a ML model was ﬁtted to the same range of ms and λr, with a reduced temperature T /Tc

range of 0.5 to 0.95. The predicted output is the vapor pressure reduced with respect to

critical pressure (Pv/Pc), such that the maximum value is 1.

For the normal (unscaled) model, the R2 value of 0.9993 suggests a good model ﬁt

for vapor pressure. However, it is important to note that vapor pressure naturally scales

logarithmically which results in huge deviations for the very low pressures, resulting in an

AAD of over 1000%. A magniﬁed view of the predicted vs original plot in Figure 5 shows

that there are even negative predicted values by the model, which can be interpreted as a

small deviation by a statistical analysis but does not make sense from a physical point of

view.

Taking the Clausius-Clapeyron equation transformation of the P-T data (eq. 18) re-

sults in a much better performance for smaller order of magnitude pressures. For the ln P ∗v

and 1/T ∗ model, although deviation at higher values nearer to the critical points are more

signiﬁcant, the overall performance improves with an AAD value of 4.7%. The magniﬁed

view in Figure 5 shows much stronger agreement between the predicted values and the data

set at lower orders of magnitude, and inspecting the vapor pressure curves for individual

components (Figure 6) shows that the model managed to capture the shape of the curve

successfully. The downside of taking a natural log is the ampliﬁcation of error and uncer-

19

Figure 5: Plot of ANN predicted reduced vapor pressure against benchmark SAFT-VR
calculated values, using a non-transformed model with T ∗ as input and P ∗v /P ∗c as output
(left) and a log-transformed model with 1/T ∗ as input and ln P ∗v /P ∗c as output (right). Blue
histograms describe the distribution of data points.

Figure 6: Sample vapor pressure curves predicted by the ANN model (solid line) with data
points from the training and validation data set (marks), for ms = 1 (left) and ms = 10
(right).

20

00.510.51R2=0.9993MSE=5.4×10−5AAD>1000%EoSP∗v/P∗cPredictedP∗v/P∗c00.510.51R2=0.9993MSE=5.4×10−5AAD>1000%EoSP∗v/P∗cPredictedP∗v/P∗c00.01·10−200.01·10−200.510.51R2=0.9985MSE=2.162×10−6AAD=4.7%EoSP∗v/P∗cPredictedP∗v/P∗c00.510.51R2=0.9985MSE=2.162×10−6AAD=4.7%EoSP∗v/P∗cPredictedP∗v/P∗c00.01·10−200.01·10−20.51.01.52.000.050.10.150.2λr=32λr=16λr=10λr=8T∗P∗v1.02.03.04.05.000.010.020.03λr=32λr=16λr=10λr=8T∗P∗vTable 3: Input speciﬁcations and model performance of a GPR model for vapor pressure.

GPR Speciﬁcations
Input
Output
Kernel(s)
Training Data Points
Validation Data Points
Model Performance
Coeﬃcient of Determination R2
Mean Squared Error (MSE)
Average Absolute Deviation (AAD)

ms, α, 1/T ∗
ln P ∗v /P ∗c
RBF, Linear
320
15757
P ∗v
0.9994
4.87
2.4%

5
10−

×

Figure 7: Model performance of GPR model for vapor pressure, with a plot of predicted
vapor pressure against original SAFT-VR values (left) and a sample vapor pressure curve
with 95% conﬁdence interval shaded (right) for ms = 18 and λr = 14.

tainty towards larger order of magnitude. This can be seen with a larger spread in Figure 5

(right) where the the inaccuracies increases as the predicted vapor pressure approaches the

critical pressure. Using a GPR model taking ms, α and 1/T ∗ as inputs and ln(P ∗v /P ∗c ) as

output, for which a good ﬁt is produced with only 320 data points (see Figure 7 and Table

3). The shaded area in Figure 7 (right) shows the 95% conﬁdence interval for a compound

(i.e. ms and λr values) unseen by the GPR model, showing the increasing variance (and

hence uncertainty) for larger values of P ∗v /P ∗c .

21

00.510.51R2=0.9994MSE=4.87×10−5AAD=2.4%EoSP∗v/P∗cPredictedP∗v/P∗c0.50.60.70.80.9100.250.50.751T/TcPvPcFigure 8: Samples of temperature T ∗r - saturated densities ρ∗s VLE envelopes for 12 diﬀerent
molecules, corresponding to ms = 1, 5, 20 and λr = 8, 16, 24, 32. Red indicates saturated
vapor densities, while blue indicates saturated liquid densities, with solid line representing
ANN predicted values, and symbols indicate data points from training and validation data.

22

0.60.81ms=1λr=8T∗rms=5λr=8ms=20λr=80.60.81ms=1λr=16T∗rms=5λr=16ms=20λr=160.60.81ms=1λr=24T∗rms=5λr=24ms=20λr=2400.20.40.60.810.60.81ms=1λr=32ρ∗sT∗r00.20.40.60.8ms=5λr=32ρ∗s00.20.40.60.8ms=20λr=32ρ∗sSaturated Densities

The ANN model employed for vapor pressures, as speciﬁed in Table 2, can be used in

parallel to predict saturated liquid and vapor densities. We experience that a good model

performance as described by common statistical indicators does not necessarily guarantee

the correct VLE envelope shape. A visually correct VLE envelope shape was achieved by

adding critical temperature and densities into the data set for both saturated liquid and

vapor densities. Figure 8 shows the predicted densities next to the benchmark data points

for an unbiased selection of 12 diﬀerent individual molecules. While most molecules shows

accurate VLE envelopes, it is clear that the corner of the phase space (e.g. ms = 1, λr = 8)

has visibly less accurate predictions.

A similar GPR model is developed for saturated densities, which albeit converging and

providing acceptable statistical indicators fails to capture the VLE envelope shape even with

the inclusion of critical points and employing over 2000 data points in the ﬁtting process.

Supercritical Density

Table 4: Input speciﬁcations and model performance for ANN models of supercritical den-
sities.

ANN Speciﬁcations
Input
Output
Hidden Layers
Activation Functions
Training Data Points
Validation Data Points
Model Performance
Coeﬃcient of Determination R2
Mean Squared Error (MSE)
Average Absolute Deviation (AAD)

ms, α, T ∗, P ∗
ρ∗
(48, 24, 12, 6)
tanh
20250
6750
P ∗c
0.9974
2.55
2.4%

3
10−

×

For supercritical densities, the data set was generated using a random sampling method

as opposed to at regular interval for VLE properties. The immediate observation is that

23

Figure 9: Plot of ANN model predicted supercritical densities against benchmark SAFT-VR
calculated values (left), and plot of a sample chain ﬂuid (ms = 14 and λr = 21) with 3
isotherms corresponding to T /Tc = 1.2, 1.5, 1.9 (right), with solid line representing ANN
model predictions and symbols representing SAFT-VR calculated values for the speciﬁc
volume (v∗ = 1/ρ∗) not included in the training set.

much more training iterations are required to achieve similar accuracy with ANN. Using an

ANN with a structure of 4 hidden layers (48,24,12,6), a model was ﬁtted for the same range

of ms and λr, between reduced temperature T ∗/T ∗c and reduced pressure P ∗/P ∗c of 1.0 to

2.0. The statistical indicators performed very well even for a 4-input model, with R2 score

of 0.9974 and AAD of 2.4%. Remarkably, randomly selected P-V isotherms within the range

of the supercritical temperatures for a random component not included in the training set

(Figure 9) shows that the ANN model managed to capture the general shape of the isotherm.

The predicted ﬂuid is a 14-mer chain which, if used to correlate experimental data, would

correspond roughly to a C42 alkane (dotetracontane).

Conclusion

Through the analysis of ML models for diﬀerent thermodynamic problems, we conclude that

both ANN and GPR can be used eﬀectively as surrogates for an analytical EoS. Comparing

24

00.20.40.60.20.40.6R2=0.9974MSE=2.55×10−3AAD=2.4%EoSρ∗Predictedρ∗2003004005006001.21.41.61.82v∗=1/ρ∗P/PcT/Tc=1.2T/Tc=1.5T/Tc=1.9the two models, GPR requires much less data to achieve a working accuracy and this par-

ticular aspect is important if only a reduced experimental data set is available. However,

the ability to capture certain shapes of the curves is signiﬁcantly impeded with the reduced

amount of information passed through to GPR, and the computational cost increases sig-

niﬁcantly with an increase in data set. Fitting through ANNs proves to be a more ﬂexible

and robust technique which allows the prediction individual properties with quantitative

accuracy and importantly captures the general shapes of diﬀerent plots of interest. On the

downside, ANNs require a much larger data set to train.

The wide range of application of the ML EoS has to be taken into perspective. Although

no attempt is made here to match experimental data of real compounds, a value of ms = 20

would roughly correspond to a 60 carbon linear alkane chain (n-hexacontane) and repulsive

exponents of λr > 20 are useful to describe highly ﬂuorinated compounds, 61 while a soft

potential λr = 8 is essential for modelling water. 62 The impressive point here is the relative

ease with which ML models can both “develop” and “learn” an EoS, which in our experience

typically requires years of dedicated eﬀort.

There are still many open challenges in the application of ML to predict and correlate

thermodynamic data, 63,64 and the sparsity of real experimental data is a consistent problem.

What would happen if computer calculations took over this problem? Modern force ﬁelds

and algorithms allow the prediction of properties of industrial ﬂuids in silico with a level of

accuracy comparable to that of experiments with the advantage of being predictive in nature

and extensible to regions where experiments would not be practical (e.g. high pressures and

temperatures, toxic compounds, etc.). In these cases, gaps in the data can be ﬁlled employing

classical molecular simulations or in some cases, even quantum mechanical calculations.

Existing data can be used to estimate the error (and thus improve on the prediction of

the molecular models). More importantly, the vast amount of data (real and pseudo-data)

could be correlated with ML algorithms. The use of molecular simulation to generate pseudo-

experimental data to provide for a suﬃciently large data set which can be optimally employed

25

by ML models is an avenue which our group (and others 65,66) are currently exploring. The

product of this enterprise would allow an exponential advance in the eﬃciency in almost all

aspects of bioengineering, energy industries, food and personal care industries, to name but

a few.

Acknowledgement

E.A.M. acknowledges the support from EPSRC through research grants to the Molecular

Systems Engineering group (grant nos. EP/E016340, EP/J014958 and EP/R013152). Com-

putations were performed employing the resources of the Imperial College Research Com-

puting Service (DOI: 10.14469/hpc/2232) and the UK Materials and Molecular Modelling

Hub, which is partially funded by EPSRC (grant no. EP/P020194).

References

(1) Schneider, W. F.; Guo, H. Machine Learning. J. Phys. Chem. A 2018, 122, 879–879.

(2) Pereira, J. C.; Caﬀarena, E. R.; dos Santos, C. N. Boosting docking-based virtual

screening with deep learning. J. Chem. Inf. Model. 2016, 56, 2495–2506.

(3) Subramanian, G.; Ramsundar, B.; Pande, V.; Denny, R. A. Computational modeling of

β-secretase 1 (BACE-1) inhibitors using ligand based approaches. J. Chem. Inf. Model.

2016, 56, 1936–1949.

(4) Ash, J.; Fourches, D. Characterizing the chemical space of ERK2 kinase inhibitors using

descriptors computed from molecular dynamics trajectories. J. Chem. Inf. Model. 2017,

57, 1286–1299.

(5) Sumpter, B. G.; Getino, C.; Noid, D. W. Theory and applications of neural computing

in chemical science. Annu. Rev. Phys. Chem. 1994, 45, 439–481.

26

(6) Schneider, W. F.; Guo, H. Machine Learning. J. Phys. Chem. B 2018, 122, 1347–1347.

(7) Venkatasubramanian, V.; Chan, K. A neural network methodology for process fault

diagnosis. AIChE J. 1989, 35, 1993–2002.

(8) Sumpter, B. G.; Vasudevan, R. K.; Potok, T.; Kalinin, S. V. A bridge for accelerating

materials by design. NPJ Computational Materials 2015, 1, 1–11.

(9) Ward, L.; Agrawal, A.; Choudhary, A.; Wolverton, C. A general-purpose machine learn-

ing framework for predicting properties of inorganic materials. NPJ Computational

Materials 2016, 2, 16028.

(10) Spellings, M.; Glotzer, S. C. Machine learning for crystal identiﬁcation and discovery.

AIChE J. 2018, 64, 2198–2206.

(11) Royal Chemical Society, ChemSpider. http://www.chemspider.com, Accessed on 20

June 2020.

(12) AIChE, The Design Institute for Physical Properties (DIPPR). https://www.aiche.

org/dippr, Accessed on 20 June 2020.

(13) U.S. Department of Commerce, National Institute of Standards and technology (NIST).

https://www.nist.gov/, Accessed on 20 June 2020.

(14) DECHEMA, German Gesellschaft f¨ur Chemische Technik und Biotechnologie. https:

//dechema.de/en/, Accessed on 20 June 2020.

(15) Ruddigkeit, L.; Van Deursen, R.; Blum, L. C.; Reymond, J.-L. Enumeration of 166

billion organic small molecules in the chemical universe database GDB-17. Journal of

chemical information and modeling 2012, 52, 2864–2875.

(16) M¨uller, I. A history of thermodynamics: the doctrine of energy and entropy; Springer

Science & Business Media, 2007.

27

(17) Rowlinson, J. S. Cohesion: a scientiﬁc history of intermolecular forces; Cambridge

University Press, 2005.

(18) Ungerer, P.; Nieto-Draghi, C.; Rousseau, B.; Ahunbay, G.; Lachet, V. Molecular simula-

tion of the thermophysical properties of ﬂuids: From understanding toward quantitative

predictions. J. Mol. Liq. 2007, 134, 71–89.

(19) Nieto-Draghi, C.; Fayet, G.; Creton, B.; Rozanska, X.; Rotureau, P.; de Hemptinne, J.-

C.; Ungerer, P.; Rousseau, B.; Adamo, C. A general guidebook for the theoretical

prediction of physicochemical properties of chemicals for regulatory purposes. Chemical

reviews 2015, 115, 13093–13164.

(20) Kontogeorgis, G. M.; Folas, G. K. Thermodynamic models for industrial applications:

from classical and advanced mixing rules to association theories; John Wiley & Sons,

2009.

(21) Venkatasubramanian, V. The promise of artiﬁcial intelligence in chemical engineering:

Is it here, ﬁnally. AIChE J. 2019, 65, 466–78.

(22) Joss, L.; M¨uller, E. A. Machine Learning for Fluid Property Correlations: Classroom

Examples with MATLAB. Journal of Chemical Education 2019, 96, 697–703.

(23) Himmelblau, D. M. Accounts of experiences in the application of artiﬁcial neural net-

works in chemical engineering. Industrial & Engineering Chemistry Research 2008, 47,

5782–5796.

(24) Pirdashti, M.; Movagharnejad, K.; Akbarpour, P.; Dragoi, E. N.; Khoiroh, I. Ther-

mophysical Properties and Experimental and Modeling Density of Alkanol+ Alkane

Mixtures Using Neural Networks Developed with Diﬀerential Evolution Algorithm. In-

ternational Journal of Thermophysics 2020, 41, 35.

28

(25) Santak, P.; Conduit, G. Predicting physical properties of alkanes with neural networks.

Fluid Phase Equilibria 2019, 501, 112259.

(26) Golzar, K.; Amjad-Iranagh, S.; Modarress, H. Prediction of thermophysical properties

for binary mixtures of common ionic liquids with water or alcohol at several temper-

atures and atmospheric pressure by means of artiﬁcial neural network. Industrial &

Engineering Chemistry Research 2014, 53, 7247–7262.

(27) S¸encan, A.; K¨ose, ˙I. ˙I.; Selba¸s, R. Prediction of thermophysical properties of mixed

refrigerants using artiﬁcial neural network. Energy conversion and management 2011,

52, 958–974.

(28) Azari, A.; Atashrouz, S.; Mirshekar, H. Prediction the vapor-liquid equilibria of co 2-

containing binary refrigerant mixtures using artiﬁcial neural networks. ISRN Chemical

Engineering 2013, 2013 .

(29) Saldana, D. A.; Starck, L.; Mougin, P.; Rousseau, B.; Ferrando, N.; Creton, B. Pre-

diction of density and viscosity of biofuel compounds using machine learning methods.

Energy & fuels 2012, 26, 2416–2426.

(30) Coccia, G.; Di Nicola, G.; Tomassetti, S.; Pierantozzi, M.; Passerini, G. Determination

of the Boyle temperature of pure gases using artiﬁcial neural networks. Fluid Phase

Equilibria 2019, 493, 36–42.

(31) Bouzidi, A.; Hanini, S.; Souahi, F.; Mohammedi, B.; Touiza, M. Viscosity calculation at

moderate pressure for nonpolar gases via neural network. Journal of Applied Sciences

2007, 7, 2450–2455.

(32) Hall, L. H.; Story, C. Boiling point and critical temperature of a heterogeneous data set:

QSAR with atom type electrotopological state indices using artiﬁcial neural networks.

Journal of chemical information and computer sciences 1996, 36, 1004–1014.

29

(33) Vasseghian, Y.; Bahadori, A.; Khataee, A.; Dragoi, E.-N.; Moradi, M. Modeling the

Interfacial Tension of Water-Based Binary and Ternary Systems at High Pressures

Using a Neuro-Evolutive Technique. ACS Omega 2019,

(34) Huuskonen, J. J.; Livingstone, D. J.; Tetko, I. V. Neural network modeling for estima-

tion of partition coeﬃcient based on atom-type electrotopological state indices. Journal

of chemical information and computer sciences 2000, 40, 947–955.

(35) Lowe, E. W.; Butkiewicz, M.; Spellings, M.; Omlor, A.; Meiler, J. Comparative analysis

of machine learning techniques for the prediction of logP. 2011 IEEE Symposium on

Computational Intelligence in Bioinformatics and Computational Biology (CIBCB).

2011; pp 1–6.

(36) Forte, E.; Jirasek, F.; Bortz, M.; Burger, J.; Vrabec, J.; Hasse, H. Digitalization in

thermodynamics. Chemie Ingenieur Technik 2019, 91, 201–214.

(37) Arce, P. F.; Vieira, N. F.; Igarashi, E. M. Thermodynamic Modeling and Simulation

of Biodiesel Systems at Supercritical Conditions. Industrial & Engineering Chemistry

Research 2018, 57, 751–767.

(38) Cendagorta, J. R.; Tolpin, J.; Schneider, E.; Topper, R. Q.; Tuckerman, M. E. Compar-

ison of the Performance of Machine Learning Models in Representing High-Dimensional

Free Energy Surfaces and Generating Observables. The Journal of Physical Chemistry

B 2020, 124, 3647–3660.

(39) Laﬁtte, T.; Apostolakou, A.; Avenda˜no, C.; Galindo, A.; Adjiman, C. S.; Mller, E. A.;

Jackson, G. Accurate statistical associating ﬂuid theory for chain molecules formed

from Mie segments. J. Chem. Phys. 2013, 139, 154504.

(40) Avenda˜no, C.; Laﬁtte, T.; Galindo, A.; Adjiman, C. S.; Jackson, G.; M¨uller, E. A.

SAFT-γ Force Field for the Simulation of Molecular Fluids. 1. A Single-Site Coarse

Grained Model of Carbon Dioxide. J. Phys. Chem. B 2011, 115, 11154–11169.

30

(41) Papaioannou, V.; Laﬁtte, T.; Avenda˜no, C.; Adjiman, C. S.; Jackson, G.; M¨uller, E. A.;

Galindo, A. Group contribution methodology based on the statistical associating ﬂuid

theory for heteronuclear molecules formed from Mie segments. J. Chem. Phys. 2014,

140, 054107.

(42) Mie, G. Zur kinetischen Theorie der einatomigen Krper. Annalen der Physik 1903,

316, 657–697.

(43) Gr¨uneisen, E. Theorie des festen Zustandes einatomiger Elemente. Annalen der Physik

1912, 344, 257–306.

(44) Avenda˜no, C.; Laﬁtte, T.; Adjiman, C. S.; Galindo, A.; M¨uller, E. A.; Jackson, G.

SAFT-γ Force Field for the Simulation of Molecular Fluids: 2. Coarse-Grained Models

of Greenhouse Gases, Refrigerants, and Long Alkanes. The Journal of Physical Chem-

istry B 2013, 117, 2717–2733.

(45) Ervik, ˚A.; Meja, A.; M¨uller, E. A. Bottled SAFT: A Web App Providing SAFT-γ Mie

Force Field Parameters for Thousands of Molecular Fluids. J. Chem. Inf. Model. 2016,

56, 1609–1614.

(46) Sha, W.; Edwards, K. The use of artiﬁcial neural networks in materials science based

research. Materials & design 2007, 28, 1747–1752.

(47) Goodfellow, I.; Bengio, Y.; Courville, A. Deep Learning; Adaptive Computation and

Machine Learning series; MIT Press, 2016.

(48) Kingma, D. P.; Ba, J. Adam: A Method for Stochastic Optimization. arXiv e-prints

2014, arXiv:1412.6980.

(49) others,, et al. Best practices for convolutional neural networks applied to visual docu-

ment analysis. Icdar. 2003.

31

(50) Chung, J.; G¨ul¸cehre, C¸ .; Cho, K.; Bengio, Y. Empirical Evaluation of Gated Recurrent

Neural Networks on Sequence Modeling. CoRR 2014, abs/1412.3555 .

(51) Wilamowski, B. M. Neural network architectures and learning algorithms. IEEE In-

dustrial Electronics Magazine 2009, 3, 56–63.

(52) Ruder, S. An overview of gradient descent optimization algorithms. arXiv preprint

arXiv:1609.04747 2016,

(53) Williams, C. K.; Rasmussen, C. E. Gaussian processes for machine learning; MIT press

Cambridge, MA, 2006; Vol. 2.

(54) Handcock, M. S.; Stein, M. L. A Bayesian analysis of kriging. Technometrics 1993, 35,

403–410.

(55) Krishnamoorthy, A.; Menon, D. Matrix inversion using Cholesky decomposition. 2013

signal processing: Algorithms, architectures, arrangements, and applications (SPA).

2013; pp 70–72.

(56) Ramrattan, N.; Avenda˜no, C.; M¨uller, E.; Galindo, A. A corresponding-states frame-

work for the description of the Mie family of intermolecular potentials. Molecular

Physics 2015, 113, 932–947.

(57) Mej´ıa, A.; Herdes, C.; M¨uller, E. A. Force ﬁelds for coarse-grained molecular simulations

from a corresponding states correlation. Ind. Eng. Chem. Res. 2014, 53, 4131–4141.

(58) others,, et al. Universal approximation using incremental constructive feedforward net-

works with random hidden nodes. IEEE Trans. Neural Networks 2006, 17, 879–892.

(59) Paszke, A. et al. In Advances in Neural Information Processing Systems 32 ; Wallach, H.,

Larochelle, H., Beygelzimer, A., d’ Alch´e-Buc, F., Fox, E., Garnett, R., Eds.; Curran

Associates, Inc., 2019; pp 8024–8035.

32

(60) Clausius, R. Ueber die bewegende Kraft der Wrme und die Gesetze, welche sich daraus

fr die Wrmelehre selbst ableiten lassen. Annalen der Physik 1850, 155, 368–397.

(61) Rahman, S.; Lobanova, O.; Jim´enez-Serratos, G.; Braga, C.; Raptis, V.; M¨uller, E. A.;

Jackson, G.; Avenda˜no, C.; Galindo, A. SAFT-γ Force Field for the simulation of

molecular ﬂuids. 5. Hetero-group coarse-grained models of linear alkanes and the im-

portance of intramolecular interactions. The Journal of Physical Chemistry B 2018,

122, 9161–9177.

(62) Lobanova, O.; Avenda˜no, C.; Laﬁtte, T.; M¨uller, E. A.; Jackson, G. SAFT-γ force ﬁeld

for the simulation of molecular ﬂuids: 4. A single-site coarse-grained model of water

applicable over a wide temperature range. Molecular Physics 2015, 113, 1228–1249.

(63) Haghighatlari, M.; Hachmann, J. Advances of machine learning in molecular modeling

and simulation. Current Opinion in Chemical Engineering 2019, 23, 51–57.

(64) Fa´undez, C. A.; Campusano, R. A.; Valderrama, J. O. Misleading results on the use of

artiﬁcial neural networks for correlating and predicting properties of ﬂuids. A case on

the solubility of refrigerant R-32 in ionic liquids. J. Mol. Liq. 2020, 298, 112009.

(65) Gong, Z.; Wu, Y.; Wu, L.; Sun, H. Predicting thermodynamic properties of alkanes

by high-throughput force ﬁeld simulation and machine learning. J. Chem. Inf. Model.

2018, 58, 2502–2516.

(66) Kirch, A.; Celaschi, Y. M.; de Almeida, J. M.; Miranda, C. R. Brine–Oil Interfacial Ten-

sion Modeling: Assessment of Machine Learning Techniques Combined with Molecular

Dynamics. ACS Applied Materials & Interfaces 2020, 12, 15837–15843.

33

Graphical TOC Entry

34

T∗....................................P∗ρ∗Lρ∗Vρ∗sT∗