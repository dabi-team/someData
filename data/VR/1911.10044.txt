Virtual Lenses as Embodied Tools for Immersive Analytics

Sven Kluge1,*

Stefan Gladisch2

Uwe Freiherr von Lukas2

Oliver Staadt1

Christian Tominski1

1Institute for Visual & Analytic Computing, University of Rostock
2 Fraunhofer Institute for Computer Graphics Research IGD, Rostock

9
1
0
2

v
o
N
2
2

]

R
G
.
s
c
[

1
v
4
4
0
0
1
.
1
1
9
1
:
v
i
X
r
a

Figure 1: Combining the effect of two lenses to reveal a ship wreck hidden in a 3D sonar dataset.

ABSTRACT

Interactive lenses are useful tools for supporting the analysis of data
in different ways. Most existing lenses are designed for 2D visualiza-
tion and are operated using standard mouse and keyboard interaction.
On the other hand, research on virtual lenses for novel 3D immersive
visualization environments is scarce. Our work aims to narrow this
gap in the literature. We focus particularly on the interaction with
lenses. Inspired by natural interaction with magnifying glasses in
the real world, our lenses are designed as graspable tools that can
be created and removed as needed, manipulated and parameterized
depending on the task, and even combined to ﬂexibly create new
views on the data. We implemented our ideas in a system for the
visual analysis of 3D sonar data. Informal user feedback from more
than a hundred people suggests that the designed lens interaction is
easy to use for the task of ﬁnding a hidden wreck in sonar data.

Index Terms: Human-centered computing—Visualization—Visu-
alization techniques—Treemaps; Human-centered computing—
Visualization—Visualization design and evaluation methods

1 INTRODUCTION

Visualization scientists have recognized the need for research into
technologies that go beyond standard desktop environments with
mouse and keyboard control [14]. In this context, the new emerging
topic of immersive analytics aims at exploring how data analysis can
beneﬁt from advances in virtual reality and augmented reality [16].
The idea is to immerse users in their data to strengthen the interplay
between human and machine in data analysis settings.

Typically, established visualization methods and interaction tech-
niques cannot be applied in a straightforward fashion to new display

*e-mail: sven.kluge@uni-rostock.de

environments and interaction modalities [11]. Instead, it is necessary
to rethink and adapt existing approaches to make them ﬁt for the new
technologies. In this work, we are particularly interested in the use of
interactive lenses in the context of immersive analytics. Lenses are
widely applied in visualization [25]. Yet, most lens techniques are
designed for classic 2D visual representations and standard mouse
interaction. How interactive lenses can be utilized for immersive
analytics remains an under-researched question. In particular the
natural interaction with lenses in an immersive setting deserves more
attention.

In this paper, we present a concrete interaction design for virtual
3D lenses for immersive analytics. Immersive 3D environments pro-
vide us the opportunity to enhance lenses by representing them as
virtual and graspable tools. To make immersive lenses intuitive to op-
erate in a direct and natural way, we employ the extended interaction
capabilities of virtual reality (VR). Our interaction design focuses
on utilizing the hands for creating, manipulating, parameterizing,
and combining lenses. For lenses that are not within the user’s reach,
we create proxies that allow users to control lenses remotely. All
interactions are aligned with the idea of ﬂuid interaction to promote
ﬂow and user immersion [9].

Our techniques have been continuously tested in informal hands-
on sessions by more than a hundred people over a period of two
years. The acquired user feedback helped us to improve our design
over several iterations. Overall, the feedback suggests that users can
easily utilize our virtual lenses in immersive analytics environments.

2 RELATED WORK

The work we present here is related to immersive analytics and inter-
active lenses. Both topics will be reviewed brieﬂy in the following.

 
 
 
 
 
 
2.1 Immersive Analytics

Recent availability of affordable high-quality virtual reality (VR)
and augmented reality (AR) devices, such as the HTC Vive or Mi-
crosoft Hololens, respectively, have opened up new opportunities
for applying these technologies in the context of data analysis.

Research on immersive analytics aims at immersing users deeper
in their data-driven analysis work [16]. Deeper immersion can poten-
tially lead to higher user engagement, more conscious experiences,
and smoother workﬂows in comparison to traditional desktop envi-
ronments [1, 4, 10, 21]. Especially when the primary dimensions of
the data are spatial, immersive approaches can make the analysis
more effective [7]. Immersion can also lead to a higher degree of
presence [6, 23]. An important factor for this is the embodiment of
the user, which can be reached by supporting natural propriocep-
tion [17]. In our case, this is supported by the head and hand tracking
of the VR system. The deﬁnition of embodiment can also include
every object in the virtual environment [8]. The ImAxes system
presented by Cordeil et. al. [5] explored the usage of embodied
tools for multivariate data exploration in an immersive environment.
These embodied tools behave like real physical objects and can be
directly manipulated by the user. We pick up on this idea and create
our virtual lenses as embodied tools.

2.2 Interactive Lenses

Interactive lenses are lightweight tools that provide a transient alter-
native representation of the data in a locally conﬁned region of the
display [25]. Much of the value of lenses lies in the fact that they
serve two purposes simultaneously: (1) they are the tool to select the
part of the data or the virtual environment to be affected and (2) they
represent the space where the alternative representation is shown.

Interactive lenses can be applied to different types of data and vari-
ous analytical tasks. Tominski et al. provide a comprehensive survey
of more than 50 lens techniques for visualization purposes [25]. Still
more lens techniques have been published since then, both for 2D
and 3D visualizations. For example, Shao et al. allow users explore
local regressions by moving lenses across a 2D scatter plot [22].
Rocha et al. investigated how the lens concept can be adapted to
support the exploration of data on 3D surfaces [20]. Their idea is to
attach the lens to the 3D surface very much like a decal.

Previous work has also investigated more natural interaction with
lenses. For example, Spindler et al. study tangible interaction for
using lenses on and above a table-top surface display [24]. Others
have applied lenses to AR [3, 15] and VR [18, 26]. Most recently,
Mota et al. explored the utilization of lenses for immersive ana-
lytics [19]. They present a concept based on the combination of a
3D spherical lens and a 3D surface decal lens. With their approach,
the selection of the part of the virtual environment to be altered
corresponds to a ﬁnite 3D volume of interest. In contrast to that,
our lenses are designed as see-through tools with a (theoretically)
unbounded selection volume. That said, the lens design presented
in the following is a consequent next step towards exploring the
beneﬁts of interactive lenses in immersive data analysis settings.

3 NATURAL LENS TOOLS FOR IMMERSIVE ANALYTICS

Following the classic loupe metaphor, we decided to represent our
virtual lenses as 3D discs that can be positioned and oriented freely
in the environment. A conceptual view is depicted in Fig. 2. The
orange disc represents the lens through which the user can observe
objects in the virtual environment. The lens interior will show an
alternative view of the virtual world depending on the active lens
effect. A lens ring serves as a clear delineation between the regular
and the alternative view. As can be seen in a practical example
in Fig. 3, our virtual lenses look not that different from traditional
interactive lenses. Yet, the way one can interact with them will be
different.

Figure 2: Concept of virtual lenses.

Figure 3: Using a virtual lens for looking into a 3D sonar dataset.

3.1 Direct Lens Interaction
As mentioned before, our work concentrates on the question of
how lenses can be interacted with utilizing the modalities of VR
environments. There are three fundamental lens operations that need
to be supported:

• Create and remove: When a lens is needed, the user must be
able to create and place one in the virtual environment. The
inverse operation is to dismiss a lens when it is no longer
needed.

• Manipulate and parameterize: Interactively exploring data
with a lens primarily means manipulating the lens position,
orientation, and size. This way, users can select the part of the
display to be visualized in a different fashion. Parameters of
the lens can further be adjusted to ﬁne-tune the lens effect.

• Combine: Lenses naturally lend themselves to combining their
individual effects by stacking them. Both, manual positioning
of lenses and semi-automatic support for combining lenses are
practical solutions.

We deﬁne naturalness and directness is the primary design goals,
taking into account interaction costs [13], especially physical motion
costs and the time needed to execute an action. With this design
rationale, we aim to reach a high degree of user immersion.

Lens creation and removal

In the ﬁrst place, we need a means
to create lenses when they are needed for a particular task and to
dismiss them once the task has been accomplished. To keep the
focus of the user on the data and to avoid clutter in the virtual world,
interaction tools such as lenses are stored in a semantically structured
toolbox that allows the user to add or remove certain tools to or from
the virtual world on demand.

The toolbox is designed as a multi-layered radial VR menu in
which lenses of different type are available. The menu can be
spawned by pressing a button at the controller in the user’s non-
dominant hand. A smooth animation brings up the menu from the

Figure 4: VR menu as a toolbox for lens creation and removal.

Figure 5: Positioning and orienting a lens with a virtual hand.

virtual controller to a position above the user’s virtual hand, as shown
in Fig. 4. When the user moves the non-dominant hand, the menu
moves along with it, which makes it easy to position the menu in a
location that reduces occlusion of the visualization.

For each available lens, there is a corresponding section in the
radial menu. Small icons in the menu symbolize the different lens
effects. In order to create a lens, the controller in the dominant hand
can be used to push down a section in the radial menu. The created
lens then appears smoothly animated in the center of the menu. From
there it can be grabbed directly with the user’s dominant virtual hand
(e.g., by using the hand trigger of an Oculus Touch Controller or a
grab gesture with a Leap Motion device) and placed in the virtual
environment. A lens will maintain its position and orientation unless
the user deliberately manipulates it. When a lens is no longer needed,
the user can simply grab the lens with the dominant hand, spawn the
menu with the non-dominant hand, and put the lens back into the
toolbox.

Lens manipulation and parameterization Easy manipulation
of a lens is crucial, because it selects the part of the world for
which an alternative view will be shown. For classic 2D lenses,
the selection is dependent on the lens position and the lens size,
which are typically controlled with the mouse. For 3D lenses in a
virtual environment, the lens orientation and the user’s head position
are additional factors inﬂuencing the selection. Consequently, our
virtual lenses are manipulated with the hands, while the head position
represents an additional degree of freedom to adjust the perspective
onto a lens, very much like in the real-world.

Positioning and orienting a lens are very natural in a virtual
environment. As shown in Fig. 5, the user simply grabs the lens disc
with either hand and manipulates it in six degrees of freedom as if
dealing with a real-world object.

Figure 6: Resizing a lens with an adapted two-handed pinch gesture.

Real-world magnifying glasses are usually ﬁxed in size, so there
is no real-world inspiration for virtual resizing. Yet, there is a
quasi-standard gesture for resizing: the pinch gesture. We adapted
the two-ﬁnger pinch gesture to a two-handed resize gesture. As
illustrated in Fig. 6, the lens is ﬁrst grabbed with both hands. Then,
while holding the lens ring, the hands can be moved in order adjust
the lens size.

Manipulating a lens’ position, orientation, and size allows users
to select which part of the world should be shown differently. Pa-
rameterizing a lens allows users to adjust how the alternative rep-
resentation should look like. We follow previous work and add
user interface controls directly to the lens ring [12]. The controls
enable the user to switch between a set of predeﬁned lens effects
and to adjust their parameters as needed. To minimize occlusion,
the interface is kept compact and displayed only on demand. For a
more direct selection of the lens effect, it is also possible to show
different effects on the front face and the back face of the lens [24].
Switching between the two effects is then as simple as ﬂipping the
lens in the virtual world.

At any time, users can change their view through a lens by moving
the head. On the one hand, this is very natural and intuitive. Yet,
on the other hand, the head position is usually hard to ﬁx in one
place. Small movements of the head can substantially change the
perspective towards a lens, and thus, the selection that is associated
with it. This can make a detailed inspection of the visualized data
difﬁcult, because the view is constantly in ﬂux. Solving this problem
remains a task for future work, not only speciﬁcally for immersive
lenses, but for immersive analytics in general.

Lens combination When multiple instances of lenses can co-
exist in a virtual world, it is only natural to attempt to combine
their individual effects. Based on the idea of Viega et al. [27],
we enable users to combine the effects of lenses by grabbing and
intersecting them much like combining camera ﬁlters in reality. Of
course, this should be restricted to lenses whose combination leads
to a meaningful and clear result. Fig. 1 illustrates an example where
a lens showing the data’s ﬁrst derivative is combined with a lens
showing a maximum intensity projection. Overlapping lenses like
this creates a composite lens effect that is temporary. It exists only
as long as the overlap exists.

To permanently combine lens effects, the user has to create an
almost maximal overlap of two lenses. Once the system detects
the almost maximal overlap, the two lenses snap together and build
a new single lens with the combined effect. The user interface
elements of the individual lenses are rearranged at the new lens and
an additional button is inserted for undoing the combination. This
semi-automatic lens combination reduces interaction costs (one hand
becomes free) and is particularly useful for combine several effects
into one lens.

Figure 7: Far away lenses can be interacted with by selecting them with a raycast (left). This triggers the appearance of a semi-transparent
proxy object (middle). Adjustments of the proxy will be applied to the remote lens (right).

3.2 Lens Interaction From Afar

All interactions introduced so far assume that the user has the lens
within arm’s reach to control it directly with the hands. However,
in a virtual world, this may not always be the case. The user may
have moved away from the data in order to acquire an overview. The
question now is how users can interact remotely with a lens that is
not within their reach? Of course, one could walk up to the lens and
grab it. However, this would be time-consuming and the user would
lose the deliberately chosen point of view.

Addressing these issues, we integrated a technique for lens inter-
action from a distance which maintains the user’s point of view and
is still based on direct interaction. The idea is to interact with lens
proxies, rather than with the remote lens. As illustrated in Fig. 7,
ﬁrst, the user selects an out-of-reach lens via a raycast selection [2]
(or through the toolbox). This triggers the animated appearance of a
semi-transparent proxy of the lens in front of the user. The user can
then interact with the proxy and all adjustments are automatically
applied to the remote lens.

4 APPLICATION EXAMPLE AND USER FEEDBACK

The described interaction techniques have been implemented in a
system to support the analysis of 3D sonar data. The system uses
Oculus Rift devices and the Unity platform together with a modern
graphics computer. The data contain measurements of the reﬂections
of different materials in the water column and the seabed in the Baltic
Sea. The original 80m × 40m × 4m volume with about 144M data
points has been reduced to a 512 × 256 × 256 resolution to achieve
the high frame-rates required for VR.

For the collectors of the sonar data, it is of interest to examine
local areas with high value changes (e.g., for analyzing structural
anomalies) in the context of the global distribution of reﬂection
values (e.g., for detecting material layers in which an anomaly is
located). To support these tasks, several alternative visual represen-
tations of the data are necessary, which provides a nice opportunity
to apply our lens techniques.

The base visualization is a direct volume rendering of the data.
Users can walk around in the virtual environment and look at the
data from different perspectives. Lenses can be utilized to look at
different representations in a focus+context fashion. Fig. 1 shows
two virtual lenses visualizing the maximum intensity projection
and the ﬁrst derivative. In their overlap, both effects are combined.
Only this combined lens effect reveals a buried wooden wreck in a
material layer 0.5m below the seabed (bright structure inside lens
overlap).

Using this application and the task of revealing the wooden wreck,
we continuously tested our design in informal hands-on sessions.
Over two years of development, we have acquired feedback from
more than a hundred people from various backgrounds and domains

(e.g., laypersons with no experience in VR or computer science,
experienced researchers in the ﬁeld of VR, and experts from the
maritime domain). The sessions were typically unstructured public
and internal lab demonstrations.

During early sessions, we could observe many users trying actions
and expecting reactions resembling their real-world experiences.
These observations inspired much of the design of our techniques.
An example for this is the combination of lenses by overlapping. If
our system did not support this operation, user expectations would
not be met and the immersion would break. From the user feedback
we also learned that multi-sensory feedback is important for a deep
immersion. Therefore, we added haptic and acoustic feedback in
addition to visual feedback.

The people with no experience in VR needed a short time to
familiarize themselves with the environment and the interaction
modalities. Once acquainted, they were rather enthusiastic about
working with the lenses to track down the hidden wreck. Some of
the people with VR experience had a particular problem with how
the grabbing was implemented. They were used to commercial VR
games where grabbing is typically done with the middle ﬁnger, and
not with the index ﬁnger as in our system. This pointed us to add an
option for switching the grabbing gesture between the two modes.
More abstract interactions such as using the radial menu for
creating new lenses seem to be more prone to errors. For example,
several users opened the radial menu, but then forgot about it and
just by moving their hands accidentally pressed buttons of the menu.
Despite these minor issues, the overall feedback for the developed
lenses was very positive. This suggests to us that our interaction
techniques for virtual lenses can be useful in the context of immer-
sive analytics. Yet, controlled studies are needed to actually conﬁrm
the beneﬁts of naturally-inspired lens interaction and to compare
different lens designs.

5 CONCLUSION AND FUTURE WORK

We presented novel interactive lenses as virtual graspable tools
for immersive analytics. We devised interaction techniques for
working with lenses in a natural way, both directly and remotely.
Our techniques have been implemented for the Oculus Rift and
applied to support the analysis of a sonar volume dataset. Informal
user feedback suggests that working with virtual lenses in VR can
be intuitive and easy.

A particular concern to be addressed in future work is to decouple
the lens manipulation from the (involuntary) head movement of the
user. However, this is a delicate issue because the measures taken
(e.g., Kalman ﬁltering) might contradict with naturalness and hence
might harm immersion. For further future work, we can imagine to
extend our concepts to other data (e.g., lenses for graph visualization
in VR) and to collaborative data analysis settings.

Environments. In Proc. ISVC, 2011. doi: 10.1007/978-3-642-24028-7
49

[22] L. Shao, A. Mahajan, T. Schreck, and D. J. Lehmann.

Interactive
Regression Lens for Exploring Scatter Plots. Comput. Graph. For.,
36(3), 2017. doi: 10.1111/cgf.13176

[23] M. Slater and S. Wilbur. A Framework for Immersive Virtual En-
vironments (FIVE): Speculations on the Role of Presence in Virtual
Environments. Presence: Teleoperators and Virtual Environments,
6(6), 1997. doi: 10.1162/pres.1997.6.6.603

[24] M. Spindler, C. Tominski, H. Schumann, and R. Dachselt. Tangible
Views for Information Visualization. In Proc. ITS. ACM, 2010. doi: 10
.1145/1936652.1936684

[25] C. Tominski, S. Gladisch, U. Kister, R. Dachselt, and H. Schumann.
Interactive Lenses for Visualization: An Extended Survey. Comput.
Graph. Forum, 36(6), 2017. doi: 10.1111/cgf.12871

[26] X. Tong, C. Li, and H.-W. Shen. GlyphLens: View-Dependent Occlu-
sion Management in the Interactive Glyph Visualization. IEEE Trans.
Visual. Comput. Graph., 23(1), 2017. doi: 10.1109/tvcg.2016.2599049
[27] J. Viega, M. J. Conway, G. Williams, and R. Pausch. 3D Magic Lenses.

In Proc. UIST, 1996. doi: 10.1145/237091.237098

REFERENCES

[1] B. Bach, R. Dachselt, S. Carpendale, T. Dwyer, C. Collins, and B. Lee.
In Proc. ISS, 2016. doi: 10.1145/2992154.

Immersive Analytics.
2996365

[2] D. A. Bowman and L. F. Hodges. An Evaluation of Techniques for
Grabbing and Manipulating Remote Objects in Immersive Virtual
Environments. In Proc. SI3D, 1997. doi: 10.1145/253284.253301
[3] L. Brown and H. Hua. Magic Lenses for Augmented Virtual Environ-
ments. IEEE Comput. Graph. Appl., 26(4), 2006. doi: 10.1109/mcg.
2006.84

[4] T. Chandler, M. Cordeil, T. Czauderna, T. Dwyer, J. Glowacki,
C. Goncu, M. Klapperstueck, K. Klein, K. Marriott, F. Schreiber, and
E. Wilson. Immersive Analytics. In Proc. Big Data Visual Analytics,
2015. doi: 10.1109/BDVA.2015.7314296

[5] M. Cordeil, A. Cunningham, T. Dwyer, B. H. Thomas, and K. Marriott.
ImAxes: Immersive Axes as Embodied Affordances for Interactive
Multivariate Data Visualisation. In Proc. UIST, 2017. doi: 10.1145/
3126594.3126613

[6] J. J. Cummings and J. N. Bailenson. How Immersive Is Enough? A
Meta-Analysis of the Effect of Immersive Technology on User Pres-
ence. Media Psychology, 19(2), 2015. doi: 10.1080/15213269.2015.
1015740

[7] C. Donalek, S. G. Djorgovski, A. Cioc, A. Wang, J. Zhang, E. Lawler,
S. Yeh, A. Mahabal, M. Graham, A. Drake, S. Davidoff, J. S. Norris,
and G. Longo. Immersive and Collaborative Data Visualization Using
Virtual Reality Platforms. In Proc. Big Data, 2014. doi: 10.1109/
BigData.2014.7004282

[8] P. Dourish. Where the Action is: The Foundations of Embodied Inter-

action. MIT Press, 2004.

[9] N. Elmqvist, A. V. Moere, H.-C. Jetter, D. Cernea, H. Reiterer, and
T. Jankun-Kelly. Fluid interaction for information visualization. Inform.
Visual., 10(4), 2011. doi: 10.1177/1473871611413180

[10] R. Hackathorn and T. Margolis. Immersive Analytics: Building Virtual
In Workshop on
Data Worlds for Collaborative Decision Support.
Immersive Analytics, 2016. doi: 10.1109/IMMERSIVE.2016.7932382
[11] D. F. Keefe and T. Isenberg. Reimagining the Scientiﬁc Visualization
Interaction Paradigm. IEEE Computer, 46(5), 2013. doi: 10.1109/MC.
2013.178

[12] U. Kister, P. Reipschl¨ager, and R. Dachselt. MultiLens: Fluent In-
teraction with Multi-Functional Multi-Touch Lenses for Information
Visualization. In Proc. ISS, 2016. doi: 10.1145/2992154.2992168
[13] H. Lam. A Framework of Interaction Costs in Information Visual-
ization. IEEE Trans. Visual. Comput. Graph., 14(6), 2008. doi: 10.
1109/TVCG.2008.109

[14] B. Lee, P. Isenberg, N. H. Riche, and S. Carpendale. Beyond Mouse
and Keyboard: Expanding Design Considerations for Information
Visualization Interactions. IEEE Trans. Visual. Comput. Graph., 18(12),
2012. doi: 10.1109/tvcg.2012.204

[15] J. Looser, R. Grasset, and M. Billinghurst. A 3D Flexible and Tangible
Magic Lens in Augmented Reality. In Proc. ISMAR, 2007. doi: 10.
1109/ISMAR.2007.4538825

[16] K. Marriott, F. Schreiber, T. Dwyer, K. Klein, N. H. Riche, T. Itoh,
W. Stuerzlinger, and B. H. Thomas, eds. Immersive Analytics, vol.
11190 of LNCS. Springer, 2018. doi: 10.1007/978-3-030-01388-2
[17] M. R. Mine, F. P. Brooks, and C. H. Sequin. Moving Objects in Space.

In Proc. SIGGRAPH, 1997. doi: 10.1145/258734.258747

[18] R. C. R. Mota, S. Cartwright, E. Sharlin, H. Hamdi, M. C. Sousa,
and Z. Chen. Exploring Immersive Interfaces for Well Placement
Optimization in Reservoir Models. In Proc. SUI, 2016. doi: 10.1145/
2983310.2985762

[19] R. C. R. Mota, A. Rocha, J. D. Silva, U. Alim, and E. Sharlin. 3De
Interactive Lenses for Visualization in Virtual Environments. In Proc.
SciVis Short Papers, 2018. doi: 10.1109/SciVis.2018.8823618
[20] A. Rocha, J. D. Silva, U. R. Alim, S. Carpendale, and M. C. Sousa.
Decal-lenses: Interactive lenses on surfaces for multivariate visual-
ization. IEEE Trans. Visual. Comput. Graph., 25(8), 2019. doi: 10.
1109/tvcg.2018.2850781

[21] R. Rosenbaum, J. Bottleson, Z. Liu, and B. Hamann.

Involve Me
and I Will Understand! – Abstract Data Visualization in Immersive

