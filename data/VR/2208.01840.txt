2
2
0
2

g
u
A
2
1

]

V
C
.
s
c
[

2
v
0
4
8
1
0
.
8
0
2
2
:
v
i
X
r
a

‘Labelling the Gaps’: A Weakly Supervised
Automatic Eye Gaze Estimation

Shreya Ghosh1, Abhinav Dhall1,2, Jarrod Knibbe3 and Munawar Hayat1

1 Monash University, 2 IIT Ropar, 3 University of Melbourne
{shreya.ghosh, munawar.hayat}@monash.edu, abhinav@iitrpr.ac.in,
jarrod.knibbe@unimelb.edu.au

Abstract. Over the past few years, there has been an increasing interest
to interpret gaze direction in an unconstrained environment with limited
supervision. Owing to data curation and annotation issues, replicating
gaze estimation method to other platforms, such as unconstrained out-
door or AR/VR, might lead to signiﬁcant drop in performance due to
insuﬃcient availability of accurately annotated data for model training.
In this paper, we explore an interesting yet challenging problem of gaze
estimation method with a limited amount of labelled data. The proposed
method distills knowledge from the labelled subset with visual features;
including identity-speciﬁc appearance, gaze trajectory consistency and
motion features. Given a gaze trajectory, the method utilizes label in-
formation of only the start and the end frames of a gaze sequence. An
extension of the proposed method further reduces the requirement of
labelled frames to only the start frame with a minor drop in the gener-
ated label’s quality. We evaluate the proposed method on four benchmark
datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled
YouTube videos. Our proposed method reduces the annotation eﬀort to
as low as 2.67%, with minimal impact on performance; indicating the
potential of our model enabling gaze estimation ‘in-the-wild’ setup1.

1

Introduction

The ‘language of the eyes’ provides an insight into a complex mental state such
as visual attention [1] and human cognition (emotions, beliefs and desires) [2].
Accurate gaze estimation has wide applications in computer vision-related assis-
tive technologies [3,4] where the gaze is measured as a line of sight of the pupil
in 3D/2D space or 2D screen location [5].

Recent advances in computer vision and deep learning have signiﬁcantly en-
hanced the accuracy of gaze estimation [6]. Most promising eye gaze estimation
techniques either require specialized hardware (for example Tobii [7]) or use
supervised image processing solutions [8,9]. Device and sensor based gaze esti-
mation methods are highly dependent on user assistance, illumination speciﬁcity,
the high device failure rate in an uncontrolled environment and constraints on
the device’s working distance. On the other hand, supervised methods require

1 https://github.com/i-am-shreya/Labelling-the-Gaps

 
 
 
 
 
 
2

Ghosh et al.

a large amount of labelled data for training. Manual labelling of human gaze
information is a complex, noisy, resource-expensive and time-consuming task.
To overcome these limitations, weakly-supervised learning provides a promising
paradigm since it enables learning from a large amount of readily available non-
annotated data. Few works [10,11,12,13] explore in this direction to eliminate the
data curation and annotation issue. However, these methods mostly investigate
from a spatial analysis perspective. Human eye movement is a spatio-temporal,
dynamic process which is either task-driven or involuntary action. Thus, it would
be interesting to simplify the ballistic eye movement and curate large-scale train-
ing data for gaze representation learning.

To this end, we propose a weakly supervised eye gaze estimation framework.
Our proposed technique reduces the requirement of a large number of annotated
training samples. We show that the technique can also be used to facilitate
the annotation process and reduce the bias in the data annotations. The pro-
posed method requires the ground truth labels of start and end frames in a
pre-deﬁned gaze trajectory. We further reﬁne this strategy where only the start
frame’s gaze annotation is required. Our proposed method signiﬁcantly reduces
the annotation eﬀort which could be beneﬁcial for annotating large-scale gaze
datasets quickly. Moreover, it can be used in several applications such as im-
mersive, augmented and virtual reality [14,15] (especially in Foveated Rending
(FR)), animation industry [16,17] and social robotics [18,19], where unsuper-
vised or weakly supervised calibration is highly desirable. In Foveated Rending
(FR), gaze based interaction demands low latency gaze estimation to reduce en-
ergy consumption. To achieve this, the virtual environment displays high-quality
images only from the user’s point of view and blurs the peripheral region. Due
to the subsequent delays in the frame-wise gaze estimation pipeline, the usage
of FR is quite limited and mostly headpose direction is used to approximate the
ﬁeld of view [15]. Our proposed method has the potential to bridge the gap and
reduce energy consumption by interpolating the gaze trajectory of the user. An-
other potential application includes animation industry [16,17]. Given the start
and end Point of Gaze (PoG) of a virtual avatar, our method can easily generate
realistic labels for intermediate frames to display realistic facial gestures in the
interaction environment [16,17]. Similarly, in social robotics, multi-modal gaze
control strategies have been explored for guiding the robot’s gaze. For example,
an array of microphones has been utilized [18] to guide the gaze direction of a
robot named Maggie. The other well-established methods include the usage of
infrared laser and multimodal stimuli (e.g., visual, auditory and tactile) for mod-
elling any known gaze trajectories [19]. Our proposed methods could eliminate
the aforementioned requirement of specialised hardware or pre-deﬁned heuris-
tics to navigate the environment. The main contributions of the paper are as
follows:

1. We propose two weakly supervised neural networks (2-labels, 1-label) for
gaze estimation. ‘2-labels’ and ‘1-label’ require the labels of two and one
frames in a gaze sequence, respectively.

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

3

2. We use task-speciﬁc information to bridge the gap between labelled and un-
labelled samples. Our proposed method leverages facial appearance, relative
motion, trajectory ordering and embedding consistency. This task-speciﬁc
knowledge bridge the gap between labelled and unlabeled samples via learn-
ing.

3. We evaluate the performance of the proposed networks in two settings: 1)
On benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) and 2) On
unlabelled ‘in the wild’ YouTube data where ground truth annotation is not
available. Additionally, we perform cross-dataset experiments to validate the
generalizability of the framework.

4. We also demonstrate the eﬀectiveness of our proposed techniques by re-
learning state-of-the-art eye gaze estimation methods with the labels gener-
ated by our method with very few prior annotations. The results indicate
comparable performance for state-of-the-art frameworks (for example, 3.8
degrees by pictorial gaze [20] and 4 degrees with our 2-label technique).
5. We also validate our learning based interpolation method on unlabelled
YouTube data where ground truth annotation is not available. Our exper-
imental results suggest that this annotation method can be useful for ex-
tracting substantial training data for learning gaze estimation models.

The rest of the paper structure is as follows: Section 2 describes prior works
in this area. Section 3 describes preliminaries regarding eye movements, gaze
trajectory and notations. Section 4 is about the proposed methods. Experimental
details and results are discussed in Section 5 and 6. Conclusion, limitations and
future research directions are discussed in Section 7.

2 Related Work

Gaze Estimation. Recent advances in computer vision and deep learning tech-
niques have signiﬁcantly enhanced the gaze estimation performance [20,5]. A
thorough analysis of gaze estimation literature is mentioned in a recent sur-
vey [6]. Appearance based gaze estimation methods [21,22,23] learn image to
gaze mapping either via support vector regression [9] or deep learning meth-
ods [24,22,25,26,27,28]. Among the deep learning based methods, supervised
learning methods [22,27,26,20] mostly encode appearance based gaze which re-
quire a large amount of annotated data. To overcome the limitation, few works
explore gaze estimation with limited supervision such as ‘learning-by-synthesis’
[29], hierarchical generative models [30], conditional random ﬁeld [31], unsu-
pervised gaze target discovery [8], unsupervised representation learning [13,12],
weakly supervised learning [10], pseudo labelling [11] and few-shot learning [32,33].
Among these studies, the few shot learning approach required very few (≤ 9)
calibration samples for gaze inference. Our method requires even less data anno-
tation for gaze estimation (CAVE: 6.56%, TabletGaze: < 1%, MPII: 4.67% and
Gaze360: 2.38%).
Gaze Motion. Eye movements are divided into the following categories: 1) Sac-
cade. Saccades are voluntary eye movements to adjust the PoG in a visual ﬁeld

4

Ghosh et al.

−−→
Fig. 1: Weakly supervised labelling approach illustration. The zig-zag path
AD in the
left is an example of a human gaze movement. This path can further be broken into
−→
−−→
several ‘gaze trajectories’ (
AB with gaze
CD). Given a gaze trajectory
annotation for A and B, the objective of this work is to annotate the unlabelled frames
i.e. P1, P2 and P3. The right side is an example of a gaze trajectory [22].

−−→
BC and

−→
AB,

and it usually lasts for 10 to 100 ms. 2) Smooth Pursuit. It is an involuntary
eye movement that occurs while tracking a moving visual target. 3) Fixations.
Fixations consist of three involuntary eye movements termed as tremor, drift
and microsaccades [34]. The main objective is to stabilize the PoG on an ob-
ject. Prior works along this line mainly used velocity based thresholding [35],
BLSTM [36], Bayesian framework [37], and hierarchical HMM [38] for classiﬁ-
cation. Arabadzhiyska et al. [39] model saccade dynamics for gaze-contingent
rendering. Our proposed method uses trajectory constrained gaze interpolation
using temporal coherency with limited ground truth labels.
Gaze Datasets. In the past decade, several datasets [9,42,40,26,25,28,5,43,44]
have been proposed to estimate gaze accurately. The dataset collection tech-
nique has evolved from constrained lab environments [9,41] to unconstrained in-
door [40,26,25,28,41] and outdoor settings [5]. To consider both the constrained
and unconstrained settings, we evaluate our weakly supervised framework in
CAVE [9], TabletGaze [40], MPII [26] and Gaze360 [5] datasets. The data collec-
tion process requires either manual or sensor based annotations (Refer Table 1).
As this being time consuming process, the research community moves towards
the data generation process for benchmarking with a large variation in data at-
tributes. Prior works in this domain generate both synthetic and real image. In
order to capture the possible rotational variation in image, gaze redirection tech-
niques [45,46,47,33] are quite popular. The other approaches are mainly based on
random forest [48] and style transfer [49]. However, due to several image quality
based limitations, these generated datasets are not used for benchmarking.
Weakly Supervised Neural Networks. Over the past few years, several
promising weakly supervised methods have been proposed which mainly infer on
the basis of prior knowledge [50,51], task-speciﬁc domain knowledge [52,53,54],

Table 1: Comparison of benchmark datasets for cost analysis.

Dataset

Cost Analysis

CAVE [9]

MPII [22]

Canon EOS Rebel T3i camera and a Canon EF-S
18–135 mm IS f/3.5–5.6 zoom lens
Laptop
Collection duration: 3 months

TabletGaze [40] Samsung Galaxy Tab S
Gaze360 [5]

Ladybug5 360°panoramic camera, AprilTag
18 Canon 250D SLR camera, ESPER trigger box,
Raspberry Pi and with controlled illumination.

ETH-XGaze [41]

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

5

representation learning [55,56], loss-imposed learning paradigms [53,57] and com-
binations of the above [53]. Williams et al. [58] propose a semi-supervised Gaus-
sian process model to predict the gaze. This method simpliﬁes the data collection
process as well. Bilen et al. [50] use pre-trained deep CNN for the object detec-
tion task. Arandjelovic et al. [53] propose a weakly supervised ranking loss for
the place recognition task. Haeusser et al. [59] introduce ‘associative learning’
paradigm, which allows semi-supervised end-to-end training of any arbitrary net-
work architecture. Unlike these studies, we explore loss-imposed domain knowl-
edge for our framework.

3 Preliminaries

−−→
BC and

−−→
AB). The red points (P1, P2 and P3) in

Gaze Trajectory. Human eye movement follows an arbitrary continuous path
in three-dimensional space termed as the ‘gaze trajectory’ [60]. Gaze trajectories
generally depend on the person, context and external factors [61]. Eye movements
can be divided into three types: ﬁxations, saccades and smooth pursuit [60].
Fixation occurs when the gaze may pause in a speciﬁc position voluntarily or
involuntarily. Conversely, gaze moves from one to another position for a saccade.
The human gaze consists of a series of ﬁxations and saccades in random order.
In this work, we consider a small duration of eye movement from one position
to another. Let us assume, the zig-zag path (left of Fig. 1) is an example of the
human gaze movement path. This path can be divided into three small sub-paths
−−→
−−→
(
CD). We conduct our experiments on each such small sub-path.
AB,
We use the term ‘gaze trajectory’ to refer to this simpliﬁed version of the gaze
−−→
AB are the frames in the A to
path (i.e.
B sequence. Considering these as discrete, they can be split into 3-point subsets
with a constraint: each subset should contain the start and endpoints and the
points should maintain the order of trajectory sequence. We term the 3 points
set as 3-frame set. For example, {A, P1, B}, {A, P2, B} and {A, P3, B} are three
3-frame sets.
Problem Statement. In the context of a video, the points (A, P1, P2, P3 and
B) are frames in a speciﬁc trajectory order. Throughout this paper, we term A
and B as start and end frames. Given the annotated start and end frames, the
gaze trajectory sequence can be divided into small segments. There might be a
high and insigniﬁcant temporal coherence if the segment duration is too small.
On the other hand, a longer duration could aﬀect the learning of meaningful
representation due to diversity. We observed that the average diﬀerence in large
time segment consisting of approx. ∼ 80 frames is around ∼ 35°. On the other
hand, the smallest segment has angular diﬀerence of < 1°. On this front, from a
long sequence, we mine 3-frame subsets of gaze trajectory for learning meaningful
representation. We work on two experimental settings: (a) 2-labels: When the
start and end frame annotations are available. (b) 1-label: When only the start
−−→
frame annotation is available. Given a gaze trajectory similar to
AB with labels
for A and B, the objective of this work is to annotate the unlabelled frames

6

Ghosh et al.

i.e. P1, P2, P3, . . . Pn where, n is the number of intermediate frames in a ‘gaze-
trajectory’.
Notations. Suppose that we have a set of N ‘3-frame set’ samples in a dataset
D = {Xn, Ysn , Yen}N
n=1, where Xn is a nth 3-frame set consisting of start, mid-
dle/unlabelled and end frames (fs, ful, fe)n, Ysn and Yen are the nth start and
end frame labels, respectively. Lets assume our model G with learnable pa-
rameter θ maps input Xn ∈ R3×100×50×3 to the relevant label spaces i.e.,
Ysn ∈ R3, Yuln ∈ R3 and Yen ∈ R3. The mapping function is denoted as
Gθ : Xn → {Ysn , Yuln , Yen }.

4 Gaze Labelling Framework

4.1 Architectural Overview of ‘2-labels’

s , Y p

s , Y p

ul and Y p

ul and Y p

The overview of the proposed framework is shown in Fig. 2. Given a 3-frame set
Xt : {fs, ful, fe}, we deﬁne an encoder E which maps the input Xt to latent space
Zt : {Zs, Zul, Ze}, where Zs ∈ R2048, Zul ∈ R2048, Ze ∈ R2048 (Refer Fig. 2 Left).
After E : Xt → Zt mapping, two motion features Ms ul and Mul e are extracted
between start-middle frames and middle-end frames. These motion features are
concatenated with the latent embeddings. On top of it, Fully Connected (FC)
layers having 512 and 1024 nodes are appended before prediction. Finally, the
e , where, Y p
network predicts Y p
e are gaze information
corresponding to the input frames {fs, ful, fe}. The backbone network is not
architecture-speciﬁc, although we use VGG-16 [62] and Resnet-50 [63] for our
experiments. The rationale behind the framework design is explained as follows:
Identity Adaptation The obvious usefulness of user adaptation of gaze cali-
bration for AR and VR devices motivates us to design an identity speciﬁc gaze
labelling framework [64]. Thus, we purposefully select identity speciﬁc 3-frame
sets. At the same time, it is important for the framework to be able to learn
the variations across a large number of subjects with a diﬀerent head pose, gaze
direction, appearance, illumination, image resolution and many other conﬁgu-
rations. Additionally, the framework should encode rich features relevant to eye
region appearance, which is the most important factor for weakly supervised
gaze labelling. Similar to recent studies [25,32], we use eye region images as in-
put for gaze inference. The 3-frame set is selected over a small duration temporal
window in a video. As there is a subtle change in the appearance of eyes from
one frame to another, the latent representation of the image also has minimal
change. At a conceptual level, we are motivated by the smoothness constraint
in optical ﬂow algorithms. We choose to calculate cosine distance between start
and middle, middle and end pair while calculating consistency loss. It helps in
preserving the identity speciﬁc features across the 3-frame set.
Motion Feature. At ﬁrst glance, the task of predicting gaze from sparsely la-
belled data may seem overly challenging. However, given a ‘3-frame set’ sequence
of a subject in very small time duration, there will be a high correspondence be-
tween frame A, Pi and B, where i = {1, 2, 3}. Given this constraint, the objective

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

7

Fig. 2: Overview of ‘2-labels’ and ‘1-label’ network pipelines. The frameworks take 3-
frame set as input and learn to interpolate intermediate labels via weak supervision.
Refer Sec. 4 for more details.

is reduced to modelling the head and eye motion information to bridge the gap.
This motivates us to use motion features for sequence modelling. Similar to [65]
in pose estimation domain, we encode a weak inter-frame motion by computing
the (cid:96)1 distance between two consecutive frames in the latent space. We deﬁne
motion feature by Ms ul ⊕ Mul e where Ms ul = Zs − Zul and Mul e = Zul − Ze.
Ms ul and Mul e represent motion feature between start-middle and middle-end
frames, respectively. Further, we use this feature to estimate the gaze-direction
in a given trajectory. To train the above-mentioned network, we use the following
loss functions.
Regression Loss. Corresponding to each 3-frame set Xt, the start and end
frames are annotated in the 2-labels setting. These annotations provide strong
supervisory information to predict the gaze information of the middle unlabelled
frame. It provides information regarding an arbitrary gaze trajectory. The unla-
belled middle frame lies in between start and end frames in that speciﬁc trajec-
tory. Thus, it belongs to the same distribution of the start and end frames. The
regression loss is deﬁned as: lreg = M SE(Ys, Y p
e ) Here, MSE is
Mean Squared Error, Ys, Y p
e are start label, predicted start label,
end label and predicted end label, respectively.
Consistency Loss. The main aim of the consistency loss is to maintain con-
sistency between latent and label space. Here, latent space is denoted as Z and
label space is the output space of the ‘2-labels’ framework. In the latent space, let
the distance between Zs and Zul be dz
s ul and the distance between Zul and Ze
be dz
s ul
and the distance between Y p
ul e. According to our hypothesis, the
distance from start to unlabelled frame and unlabelled to end frame remains con-
sistent. The loss is deﬁned as follows: lconsistency = {|dy
ul e|}
In the equation, the cosine distance is considered. The rationale behind the
choice is as follows: the cosine distance is applied pairwise to utilize the partial
annotations and bridge the gap between labelled and unlabelled frames. The
distance has following properties: it leverages the identity speciﬁc information
across the 3-frame set and it captures the motion-similarity information which
indirectly encodes relative ordering of the frames. The angular distance (da b)
between the frames a and b is deﬁned as follows: da b = a
where, a
||a||2

ul e. Similarly in the label space, let the distance between Y p
s and Ye be dy

s ) + M SE(Ye, Y p

s , Ye and Y p

s ul|+|dy

s and Y p

s be dy

ul e −dz

s ul −dz

b
||b||2

.

8

Ghosh et al.

and b are latent or label space embeddings of the start, unlabelled and end
frames. ‘(.)’ denotes the dot product. According to [66], this term is calculated
), which
in two stages. First, the latent embeddings are L2-normalized (i.e.
maps the d-dimensional latent embedding to a unit hyper-sphere, where the
cosine similarity/distance and dot product are equivalent. As the human gaze
follows a spatio-temporal trajectory [60], the 3-frame sets satisfy their relative
trajectory ordering as well. The condition is satisﬁed by the consistency loss as
it incorporates their relative position w.r.t. the middle unlabelled frame from
label space to the latent space.
Overall Loss Function. The ﬁnal loss function is deﬁned below:

a
||a||2

Loss = λ1lreg + λ2lconsistency

(1)

It includes regression and consistency losses. Here, λ1,2 are the regularization
parameters. Further, we reduce the label requirement (by removing the require-
ment of the end frame’s label) in the next framework with minimal impact in
the performance.

4.2 Architectural Overview of ‘1-label’

The network architecture of 1-label network is shown in Fig. 2 Right. Similar
to the 2-labels network, the motion feature and identity speciﬁc appearance are
leveraged for gaze estimation. The main motivation for moving from 2-labels
to 1-label architecture is to leverage rich features with even lesser number of
total annotations in a dataset. In ‘1-label’ architecture, we have additional de-
coder module deﬁned as D(Z (cid:48)
e; φD) consisting of an FC layer. The decoder D,
e ∈ R1024) to
parameterized by φD, maps the penultimate layer features (i.e. Z (cid:48)
e ∈ R2048. The rationale behind this remapping is adding
latent-embeddings Z r
constraints which can enhance the performance of the network by encoding
meaningful representation from fe. l2 loss is computed between Ze and Z r
e . The
backbone network is not architecture-speciﬁc similar to the 2-labels architecture,
although we use VGG-16 [62] and Resnet-50 [63] as backbone networks for our
experiments. To train this network, we use the following loss functions:
Regression Loss. Similar to 2-labels architecture, we compute the regression
loss corresponding to the start label as follows: lreg = M SE(Ys, Y p
s ) Here, MSE
is mean squared error, Ys and Y p
s are start and predicted start label, respectively.
Consistency Loss. Similar to 2-labels, in the 1-label technique, we add the
consistency loss lconsistency. The loss is deﬁned as follows: lconsistency = {|dy
s ul −
s ul|} Here, dy
dz
s ul are distance between start and unlabelled frame in
label and latent space, respectively.
Similar Distribution. We leverage on the constraint that the gaze information
belongs to similar distribution. Given a 3-frame set Xt, the output gaze should
belong to a speciﬁc distribution and for that we compute Kullback-Leibler di-
vergence (KL). To computes KL divergence loss between Ytrue and Ypred, the
following equation is followed: loss = Ytrue(log Ytrue
). The objective function
Ypred

s ul and dz

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

9

ldivergence is the loss term to minimize the divergence between the gaze informa-
tion of start and unlabeled frames, deﬁned as follows: ldivergence = (KL(Ys, Yul))
Embedding Loss. It is the (cid:96)2 loss computed between Ze and Z r
e . This pattern
is predicting future embeddings from prior knowledge.
Overall Loss Function. The ﬁnal loss function for ‘1-label’ is an ensemble of re-
gression, consistency, similar distribution and embedding losses, where, λ1 · · · λ4
are the regularization parameters.

Loss = λ1lreg + λ2lconsistency + λ3ldivergence + λ4lembedding

(2)

5 Experiments

We perform experiments on 4 benchmark datasets (CAVE, TabletGaze, MPII
and Gaze360) as well as ‘in-the-wild’ YouTube data, where the ground truth
labels are not available. To validate the performance of the YouTube data, we
generate pseudo labels (via SLERP and other methods) for comparison. The
details are as follows:

5.1 Automatic 3-frame set mining

The ﬁrst step to implement our proposed method is 3-frame set mining. For gen-
erating 3-frame sets, we perform dataset-speciﬁc pre-processing as the datasets
are collected in diﬀerent setups. Please note that we require ground truth labels
to deﬁne the gaze trajectories, especially for CAVE and MPII datasets as tem-
poral information is not present. Moreover, the annotated subset required for
weak supervision is a special subset which contains start and end frame of the
trajectories. Additionally, there is no temporal overlap between 3-frame sets for
any of the datasets. Table 2 shows the comparison among dataset statistics to
show the amount of images in the original data and derived data (3-frames set
mined used for weak-supervision).

Automatic 3-frame set mining on benchmark datasets. We validate
the proposed methods on 4 benchmark datasets: CAVE, Tabletgaze, MPII and
Gaze360. These datasets are collected in fully constraint (CAVE), less con-
strained (Tabletgaze and MPII) and unconstrained (Gaze360) environments.
CAVE [9] contains 5,880 images of 56 subjects with diﬀerent gaze directions
and head poses. There are 21 diﬀerent gaze directions for each person and the
data was collected in a constrained lab environment. CAVE dataset is collected
with 7 horizontal and 3 vertical gaze locations as shown in the left part of
Fig. 3. Considering these positions as 7 × 3 grids, we deﬁned three types of
gaze trajectories: horizontal, vertical and diagonal. As temporal information is
missing, we can consider bi-directional gaze trajectories. We reverse the order
for bidirectional set mining (Refer Fig. 3). The bidirectional gaze trajectories
are applied for CAVE dataset only due to the absence of temporal information.
Note that this does not impact the requirement of ground truth annotation for

10

Ghosh et al.

Fig. 3: 3-frame set mining process for CAVE [9] and TabletGaze dataset [40].
Here, red, blue and black arrows represent horizontal, vertical and diagonal gaze
trajectories.

weak supervision. In this way, we collect 3,024 3-frame sets for training. For this
dataset, we require 6.56% and 3.28% of prior data annotation for our ‘2-labels’
and ‘1-label’ paradigms.
TabletGaze [40] is a large unconstrained gaze dataset of 51 subjects with 4
diﬀerent postures and 35 gaze locations collected using a tablet in an indoor en-
vironment. TabletGaze dataset is also collected in a 7 × 5 grid format (as shown
in the middle image of Fig. 3). Similar to CAVE dataset, we deﬁne horizon-
tal, vertical and diagonal gaze trajectories and collect 108,524 3-frame sets. For
TabletGaze dataset, as temporal information is also present, we consider unidi-
rectional frames only. For 3-frame set mining on TabletGaze data, we require
less than 1% prior data annotation for both the frameworks.
MPII [26] gaze dataset contains 213,659 images collected from 15 subjects dur-
ing natural everyday events in front of a laptop over a three-month duration.
MPII gaze dataset is collected by showing random points on the laptop screen
to the participants. To make the gaze trajectory smooth, we sort the given co-
ordinates of the points in ascending order and consider it as a gaze trajectory.
Further, 3-frame sets are collected in a day-wise for each participant. Following
this procedure, we collect 32,751 3-frame sets. We extracted these 3-frame sets
with 4.67% prior data annotation.
Gaze360 [5] is a large-scale gaze estimation dataset collected from 238 subjects
in unconstrained indoor and outdoor settings with a wide range of head pose.
We compute person-speciﬁc 3-frame sets. As each participant ﬁxates gaze at a
moving target, we consider the target’s trajectory as the gaze trajectory. This
results in 197,588 3-frame sets and we use 2.38% of annotated data.

3-frame set mining on unlabelled ‘in the wild’ YouTube data. We
evaluate our method on an ‘in the wild’ data i.e. when the expert/ground truth
labels are not available. We leverage two eye symmetry property i.e. the change
in relative position of the iris is symmetrical while scanning 3D space [13]. We
collect approximately 400,000 frames from YouTube videos using this strategy
mentioned below.
Gaze Trajectory Selection As the relative positions of pupil-centers provide
the most important information regarding gaze direction, we utilize this property
to detect gaze trajectories. We utilize two eye symmetry property i.e. the change
in relative position of the iris is symmetrical while scanning 3D space [13]. Based
on this hypothesis, we compare the vertical angles formed with the following

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

11

Table 2: Comparison of benchmark dataset statistics to show the amount of images in
the original data and derived data (3-frames set mined).

Dataset

Original Dataset Derived Dataset

CAVE [9]
MPII [22]
TabletGaze [40]
Gaze360 [5]

5,880
213,659
1,428 min
172,000

3,024
32,751
108,524
197,588

points i.e. pupil-center, nose in both eyes. In Fig. 5, I1 and I2 are the pupil
centers; V is the vertical direction w.r.t. the nose tip point and θ1, θ2 are the
above mentioned angles. The change in θ1 and θ2 depicts the path of the gaze
trajectory sequence. For example, if a person shifts his/her gaze from left to
right, the values of the angles will be as follows: initially, θ1 will be greater than
θ2; then gradually θ1 will decrease and θ2 will increase; ﬁnally, θ2 will be greater
than θ1. Thus, by monitoring these angles we can approximate gaze trajectories.
The heuristic also considers the trajectory segment if it starts from the middle
until there is a change in any of the angles θ1 and θ2. Although the proposed
method is robust to head movements within the range of −10° to 10°. After
identifying the gaze trajectories, we annotate the start and end frames with
OpenFace [67] and collect possible 3-frame sets. In this 3-frame set collection
and data annotation procedure, we require 5.34% and 2.67% annotation of the
overall data for ‘2-labels’ and ‘1-label’ settings. According to literature [68],
human gaze trajectories are considered to be spherical. Thus for ground truth
annotation, we label remaining frames by SLERP interpolation method [68].
Further, we apply our ‘2-label’ and ‘1-label’ model on this data.

5.2 Experimental Settings.

We deﬁne the following terminologies for easy navigation in the upcoming sec-
tions. 1) Original Data (OD): It refers to benchmark dataset’s unaltered data;
2) Derived Data (DD): 3-frame set mined data derived from OD; 3) Original
Labels (OL): Original ground-truth labels provided with OD; and 4) Predicted
Labels (PL): Labels predicted from ‘2-labels’ and ‘1-label’ methods (i.e. Y p
ul).
We perform experiments with the following settings:
1) Validation w.r.t. Ground Truth Labels. First, we applied ‘3-frame set
mining’ to obtain the DD (Refer Table 2) from OD. Further, we split the DD
into 80%-20% train-test splits without any identity overlap. We evaluate our
proposed method with OL.
2) Label Quality Assessment via State-of-the-art Methods’ Perfor-
mance. We train the state-of-the-art methods [20,40,5] on PL and validate on
OL for label quality assessment.
3) Experiments with Diﬀerent Data Partitions. We train state-of-the-art
models [20,40,5] with diﬀerent input data settings as follows: a) start and end
frames of 3-frame sets, b) 50% of the whole data and c) newly labelled frames.

12

Ghosh et al.

4) Ablation Studies. We have conducted extensive ablation studies to show
the importance of loss function, motion feature, sequential modelling and regu-
larization parameters.
5) Gaze Labelling ‘in the wild’. We collect ‘in the wild’ gaze data from
YouTube videos having creative common licence and compare the label quality
with various model based techniques [69,70,71].
Further, we did perform additional experiments to evaluate the generalizibity of
the proposed method.

5.3 Evaluation Metrics

i=1 |yp−y|
n

For quantitative evaluation, we use Mean Absolute Error (MAE), Correlation
Coeﬃcient (CC) and Angular Error. Mean Absolute Error is calculated as:
(cid:80)n

Correlation coeﬃcient is calculated as:

i −yp)2
Here, yp is the predicted label and y is the ground truth label in normalized
space, (.) indicates mean across the samples. Similar to the previous meth-
ods [41,72,32], angular error is the average error across test data measured in
terms of cosine angle between ground truth and predicted gaze direction. It is
Here, g and g(cid:48) respectively denote the ground
measured as follows:
truth and predicted gaze in terms of 3D gaze direction vector.

g(cid:48)
||g(cid:48)||2

g
||g||2

.

√

(cid:80)n
i=1(yi−y)(yp
i=1(yi−y)2 (cid:80)n

i −yp)
i=1(yp

(cid:80)n

5.4 Training Details.

After the 3-frame set mining, we apply Dlib face detector [73] for eye detection.
If face detection (dlib) fails especially for Gaze360 dataset, we use cropped head-
pose provided with the dataset2. Otherwise, we use the resized input image. For
the backbone network, we choose VGG-16 [62] and ResNet-50 [63] architectures.
For training, we use SGD optimizer with 0.001 learning rate with 1 × e−6 decay
per epoch. The values of λ1 · · · λ4 are 1. In each case, the models are trained for
1,000 epochs with batch size 32 and early stopping.

6 Results

6.1 Gaze Labelling and Estimation Performance Comparison.

To show the eﬀectiveness of the proposed method, we evaluate on four benchmark
datasets and YouTube data. First, we applied 3-frame set mining to get the
derived data (Refer Table 2). Further, we split the derived data randomly into
train and test sets (train set: 80% and test set: 20%). Please note that the
test set does not have identity overlap with training partition. The results are
mentioned in Table 3 in terms of MAE, CC and angular error. We use VGG-
16 and Resnet-50 as backbone networks to show the impact of diﬀerent network

2 https://github.com/erkil1452/gaze360/tree/master/dataset

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

13

Table 3: Comparison of ‘2-labels’ and ‘1-label’ techniques using MAE, CC and Angular
Error. Both frameworks with both VGG-16 and ResNet-50 backbone are trained on
80% of the Derived Data (DD, i.e. 3-frame set mined) and validated on 20% of the
DD. This 80%-20% partition does not have any identity overlap. Here, MAE: Mean
Absolute Error, CC: Correlation Coeﬃcient, TG: TabletGaze.

Dataset

2-labels

1-label

2-labels

VGG-16

MAE CC MAE CC MAE CC MAE CC

ResNet-50
1-label 2-labels 1-label
Angular Error
(in °, TG: in cm)

CAVE
MPII
Gaze360
TabletGaze
YouTube
‘in the wild’

0.29 0.85 0.53 0.21 0.25 0.90 0.43 0.25
0.43 0.62 0.57 0.25 0.42 0.63 0.57 0.25
0.38 0.69 0.45 0.42 0.34 0.71 0.40 0.70
0.49 0.54 0.50 0.58 0.47 0.55 0.49 0.55

3.05
5.00
15.00
2.27

3.30
5.40
15.80
2.61

0.27 0.90 0.36 0.81 0.24 0.92 0.34 0.86

9.41

12.07

Table 4: Results of the re-trained state-of-the-art methods on MPII and CAVE dataset
in terms of angular error in °. OP=Original Predictions is the result as mentioned in
the original papers [20].

Dataset Method Alexnet VGG 16

Pictorial Gaze
[20]

E
V
A
C

I
I
P
M

Train:YouTube

OP
2-labels
1-label
OP
2-labels
1-label
CAVE
MPII

4.20
4.10
4.55
5.70
5.90
6.30
–
–

3.90
4.46
4.84
5.40
5.79
6.20
–
–

3.80
4.00
4.36
4.50
4.70
4.90
3.90
4.57

architectures. Quantitatively, ResNet-50 performs slightly better than the VGG-
16. From Table 3, it is also observed that ‘2-labels’ technique is closer to the
original label distribution as compared to the ‘1-label’ technique due to the
absence of supervisory signal (i.e. absence of end frame label). Due to high-
resolution images in CAVE dataset, generated labels’ similarity as compared
with original labels is high for ‘2-labels’ setting. Instead of sequential modelling,
our loss imposed gaze estimation method improves model performance.

6.2 Cross Dataset Evaluation

We perform a cross dataset evaluation for predicting the generalization ability
of the proposed method. This evaluation is conducted in two settings: The ﬁrst
conﬁguration is a classical cross dataset evaluation protocol. In the second con-
ﬁguration, the training is performed on the train partition of OD using the PL
i.e. output of ‘2-labels’ technique Y p
ul and it is evaluated on the test partition
of other datasets. The second conﬁguration is conducted for cross dataset label
quality assessment of the proposed method.

The ﬁrst conﬁguration is a classical cross dataset evaluation. Given any two
datasets D1 and D2, the training is performed on the train partition of D1

14

Ghosh et al.

Table 5: Results of the re-trained methods on TabletGaze and Gaze360 dataset in
terms of angular error in °and cm. OP=Original Predictions is the result as mentioned
in the [5,40].

TabletGaze

mHOG+SVR
[40]

Gaze360

Pinball LSTM
[5]

2.50
2.70
3.10
2.30

OP
2-labels
1-label
Train:YouTube
Table 6: Performance comparison for
diﬀerent input data settings with the
state-of-the-art methods
on MPII,
CAVE, TabletGaze (TG) and Gaze360
dataset. NL: Newly Labelled, SF: Start
Frame, EF: End Frame.

OP
2-labels
1-label
Train:YouTube

13.50
14.40
17.20
12.80

Dataset CAVE TG Gaze360 MPII
[20]
[40]
Method
6.10
8.60
(SF+EF)
5.40
4.50
50% data
4.70
2.70
2-labels
NL frames
4.30
3.00
4.90
3.10
1-label

[5]
22.10
18.70
14.40
14.90
17.20

[20]
7.14
5.50
4.00
4.30
4.36

2

1

1

) using the PL i.e. output of ‘2-labels’ technique Y p

(i.e. Dtrain
) using the OL and ResNet-50 as backbone network. Further, it is
evaluated on test partition of D2 (i.e. Dtest
). The results are shown in Table
8. In the second conﬁguration, the training is performed on the train partition
of D1 (i.e. Dtrain
ul, while it
is evaluated on the test partition of D2 (i.e. Dtest
). The results are depicted
on Table 9. Please note that CAVE, TabletGaze and MPII do not have proper
train-validation-test partitions [9,26,40]. We train the model on D1 and evaluate
on D2. From both the Tables 8 and 9, it is observed that with the predicted label,
the cross dataset performance is increased signiﬁcantly which in turn, shows the
generalization capability of our models. Apart from generalizability, it is also
observed that the ‘1-label’ framework performs comparatively well even after it
is trained with less supervision.

2

For ‘in-the-wild’ data, the cross dataset performance is depicted in Table 9
for both 2-labels and 1-label frameworks. Here (Table 9), we have not ﬁne-tuned
the model.
Comparison with State-of-the-art Methods. We also evaluate the label
generation quality of our proposed methods. For this purpose, we conduct ex-
periments by training existing state-of-the-art methods [20,5,28,40] with the la-
bels predicted from our method. The state-of-the-art network’s performance is
measured by comparing with the original ground truth labels. The performance
comparison is mentioned in the Table 4 and 5. For [20,5], we use author’s GitHub
implementations [74]. It is observed that ‘2-labels’ performs better than ‘1-label’
for all the datasets. For CAVE dataset, ‘gazemap’ based method [20] for ‘2-
labels’ (i.e. 4.08°) performs better than other settings. It is to be noted that the
results of re-trained methods are comparable to when they were trained with
the original labels. By using less than 5% labelled data (For MPII 4.67% and
Gaze360 2.38%), the labels generated by our weakly-supervised method perform
favourably when evaluated on state-of-the-art methods [20,5]. This shows the
usefulness of our weakly-supervised approach of label generation.

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

15

Loss

lreg
lreg + lconsistency
lreg + lconsistency + ldivergence
(for 1-label)
lreg + lconsistency + ldivergence
+lembedding (for 1-label)

MAE
(2-labels)
0.98
0.42

MAE
(1-label)
0.99
0.76

Table 7: Ablation study: eﬀect
of loss functions for ‘2-labels’
(Eq. 1) & 1-label (Eq. 2). NA:
Not Applicable

NA

NA

0.58

0.57

Table 8: Cross dataset performance evaluation among the benchmark datasets in terms
of MAE in both ‘2-labels’ and ‘1-label’ settings. Given any two datasets D1 and D2, the
training is performed on the train partition of D1 (i.e. Dtrain
) using the Original Label
(OL) and ResNet-50 as backbone network. Further, it is evaluated on test partition of
D2 (i.e. Dtest

).

1

2

l
e
b
a
L

l
a
n
i
g
i
r
O

2-labels
Test→
Train
↓
CAVE
MPII
Gaze360
TabletGaze

l
e
b
a
L

l
a
n
i
g
i
r
O

1-label
Test→
Train
↓
CAVE
MPII
Gaze360
TabletGaze

CAVE MPII Gaze360 TabletGaze

–
0.35
0.21
0.36

0.50
–
0.40
0.42

0.55
0.53
–
0.50

0.49
0.51
0.44
–

CAVE MPII Gaze360 TabletGaze

–
0.57
0.24
0.27

0.54
–
0.39
0.49

0.60
0.61
–
0.59

0.50
0.90
0.50
–

16

Ghosh et al.

Re-train State-of-the-art Methods with Subset of Data.To further val-
idate the generated labels, we perform following experiments: we re-train from
scratch [20], [40] and [5] using the following labelled sets independently: a) start
and end frames of 3-frame sets only, b) 50% of originally labelled training data,
c) frames with labels generated with 2-labels method, d) frames with labels gen-
erated with 2-labels method apart from start and end frames, and e) frames
with labels generated with 1-labels method. The results are shown in Table 6.
When we train the networks with start and end frames, the error is high as the
start and end frames consist of < 10% of the whole dataset. When we use 50%
of the whole labelled data, the results signiﬁcantly improve. Similarly, for the
newly labelled frames, the error is less as compared to the above two settings.
Please note that in the newly labelled case the training is performed on 90-95%
of the training data. These results validate the quality of labels generated by our
methods.

Table 9: Cross dataset performance evaluation among the benchmark datasets and
YouTube data in terms of MAE in both ‘2-labels’ and ‘1-label’ settings. Given any
two datasets D1 and D2, the training is performed on the train partition of D1 (i.e.
Dtrain
ul. Further,
1
it is evaluated on the test partition of D2 (i.e. Dtest

) using the Predicted Label (PL) i.e. output of ‘2-labels’ technique Y p

).

2

l
e
b
a
L

d
e
t
c
i
d
e
r
P

2-labels
Test→
Train
↓
CAVE
MPII
Gaze360
TabletGaze
YouTube

l
e
b
a
L

d
e
t
c
i
d
e
r
P

1-label
Test→
Train
↓
CAVE
MPII
Gaze360
TabletGaze
YouTube

CAVE MPII Gaze360 TabletGaze

–
0.27
0.20
0.32
0.25

0.54
–
0.34
0.44
0.45

0.51
0.50
–
0.50
0.46

0.47
0.48
0.42
–
0.43

CAVE MPII Gaze360 TabletGaze

–
0.30
0.23
0.25
0.30

0.55
–
0.37
0.46
0.52

0.54
0.52
–
0.53
0.49

0.50
0.50
0.49
–
0.45

6.3 Ablation Studies.

Impact of Loss Function. We progressively integrate diﬀerent parts of our
method. We assess the impact of each loss term mentioned in Eq. 1 and Eq. 2
by considering them one at a time during learning on the MPII dataset. The

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

17

Table 10: Comparison of model based
methods on YouTube data. MAE: Mean
Absolute Error, AE: Angular Error.

results are shown in Table 7. For ‘2-labels’ and ‘1-label’, if only regression loss
is considered, the error is high for the two techniques (0.98 and 0.99). We argue
that this could be due to lack of domain knowledge. Further, consistency loss is
added to the network, which reduces the error (0.42 and 0.76) signiﬁcantly for
both settings. On the other hand, KL divergence based loss is added to the ‘1-
label’ framework, which again reduces the error signiﬁcantly from 0.76 to 0.58.
Additionally, embedding loss is introduced to consider future frame consistency,
though we note that for MPII, the change in MAE is very less (i.e. from 0.58 to
0.57). These experiments clearly establish the individual importance of each of
the proposed loss terms in our framework.
Impact of Motion Feature. To
judge the impact of the motion fea-
ture, we evaluate the performance of
‘2-labels’ on the CAVE dataset with-
out using the motion feature i.e. the
latent space features are directly con-
catenated (Refer Fig. 2). The results
suggests that without the motion fea-
ture the MAE is reduced from 0.34 to
0.25. Thus, it is important to include
this information in the framework.
Sequential Modelling Vs
‘2-
labels’. We incorporate an LSTM
module having 3 steps instead of the whole pipeline. The input to the LSTM
module is 3-frame set and it estimate the gaze of the unlabelled frame. The
MAE is quite high (i.e. 0.95) as compared to our ResNet-50 based ‘2-labels’
framework (i.e. 0.25). The possible reason for this performance enhancement
owes to task-relevant losses posed within very small temporal information.
‘Resnet-50+FC’ Vs ‘2-labels’. We also evaluate our method against a simple
‘ResNet+FC’ trained on training partition and tested on the test partition. The
MAE is 0.39 as compared to our ResNet-50 based ‘2-labels’ framework (i.e. 0.25).
This experiment also indicates the advantage of using triplet module.
Regularization Parameters. We have also experimented with diﬀerent values
of the regularization parameters. The trade-oﬀ is shown in Fig. 4. In this ﬁgure,
the overall loss is plotted against diﬀerent values of λ. It indicates that the
optimal setting is achieved when all values are 1.

Method MAE AE
0.24 9.41
2-labels
0.74 15.30
[69]
0.52 13.90
[70]
0.57 14.01
[71]

6.4 Failure Cases

We also investigate the failure cases of our methods. The generated labels are
noisy when the illumination is dark and the eyes are not open. Fig. 5 shows a
few cases, where the correlation is low as compared to the ground truth labels.
Gaze Labelling ‘in the wild’. The best results on the collected data in terms
of MAE and CC are mentioned in the last row of Table 3. The angular errors
w.r.t. the SLERP [75] for ‘2-labels’ and ‘1-label’ are 9.41°and 12.07°, although
SLERP is a weak baseline to compare eye gaze in an unconstrained environment.

18

Ghosh et al.

Fig. 4: Impact of regularization parameters in ‘2-labels’ and ‘1-label’ settings.

When we use the generated labels on YouTube dataset to complement the other
dataset, the performance improves for TabletGaze and Gaze360 (see Table 4
and 5). For adapting the SOTA methods (Table 4 and 5), we ﬁne-tune the models
following standard protocol [13,33]. The results suggest that the network learns a
meaningful representation. Moreover, we compare our method with model based
approaches [69,70,71]. The results are depicted in Table 10. From the table, it is
observed that our proposed method outperforms model based methods.

6.5 Qualitative Analysis.

Fig. 6 illustrates few examples of gaze trajectories along with the predictions of
the proposed method. The trajectories consist of start, unlabelled and end frames
and the trajectory length is not limited to 3. During training, the proposed
method takes start, end and one of the unlabelled frame as input. Please note
that the eye patch cropped from the facial images are used as input. In the ‘2-
labels’ case, the ground truth labels of both start and end frames are provided for
weak supervision. In Fig. 6, the green and red arrow indicates ground truth and
predicted gaze direction. For better understanding, we plot the gaze direction
corresponding to each eye originated from detected pupil center. This qualitative
analysis indicates that our weakly supervised method learns to interpolate gaze
labels eﬃciently from terminal frames of a trajectory.

Fig. 5: (Left) Two sample images for which our methods generate noisy labels due
to illumination conditions and eye openness. (Right) Heuristic for gaze trajectory
selection.

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

19

Fig. 6: Few examples of gaze trajectories along with qualitative prediction results. Here,
red and green arrow represent predicted and ground truth gaze direction, respectively.

7 Limitations, Conclusion and Future Work

The main limitation of our study is the ﬁxed gaze trajectory requirement. More-
over, we did not remove eye blink frames which leads to most of the failure cases.
Another limitation of our framework is that it only considers mostly frontal faces
(in terms of yaw and pitch axis of headpose). Our study introduces a weakly-
supervised approach for generating labels for intermediate frames in a deﬁned
gaze trajectory. The proposed frameworks leverage task-speciﬁc domain knowl-
edge i.e. trajectory ordering, motion and appearance features etc. With extensive
experiments, we show that the labels generated by our methods are compara-
ble to the ground truth labels. Further, we also show that the state-of-the-art
existing techniques re-trained using the labels generated by our method give
comparable performance. This applies that with just 1%-5.6% labelled data (de-
pendent on the dataset) training can be performed with performance comparable
to when 100% training data is available. Further, we also propose a technique
to collect and label eye gaze ‘in the wild’. The proposed method can be used for
other computer vision based applications (e.g. gaze tracking devices for AR and
VR) without the prior need of having to use the whole labelled dataset during
training. In the future, we will investigate subject speciﬁc gaze estimation in chal-
lenging situations such as low-resolution images, uneven illumination conditions
and extreme head-poses. Moreover, it will be interesting to explore extrapola-
tion from the given gaze labels. Although our methods consider gaze annotation

20

Ghosh et al.

in few of the aforementioned diverse conditions, it would be interesting to have
more in-depth study in this domain.

References

1. Liu, H., Heynderickx, I.: Visual attention in objective image quality assessment:
Based on eye-tracking data. IEEE Transactions on Circuits and Systems for Video
Technology 21 (2011) 971–982

2. Wang, W., Shen, J.: Deep visual attention prediction. IEEE Transactions on Image

Processing 27 (2017) 2368–2378

3. Mustafa, A., Kaur, A., Mehta, L., Dhall, A.: Prediction and localization of student

engagement in the wild. arXiv preprint arXiv:1804.00858 (2018)

4. Ghosh, S., Dhall, A., Sharma, G., Gupta, S., Sebe, N.: Speak2label: Using domain
knowledge for creating a large scale driver gaze zone estimation dataset. arXiv
preprint arXiv:2004.05973 (2020)

5. Kellnhofer, P., Recasens, A., Stent, S., Matusik, W., Torralba, A.: Gaze360: Physi-
cally unconstrained gaze estimation in the wild. In: IEEE International Conference
on Computer Vision. (2019)

6. Ghosh, S., Dhall, A., Hayat, M., Knibbe, J., Ji, Q.: Automatic gaze analysis: A
survey of deep learning based approaches. arXiv preprint arXiv:2108.05479 (2021)
7. Niehorster, D.C., Hessels, R.S., Benjamins, J.S.: Glassesviewer: Open-source soft-
ware for viewing and analyzing data from the tobii pro glasses 2 eye tracker. Be-
havior Research Methods (2020) 1–10

8. Zhang, X., Sugano, Y., Bulling, A.: Everyday eye contact detection using unsu-
pervised gaze target discovery. In: ACM User Interface Software and Technology.
(2017) 193–203

9. Smith, B., Yin, Q., Feiner, S., Nayar, S.: Gaze locking: passive eye contact detection
In: ACM User Interface Software & Technology.

for human-object interaction.
(2013)

10. Kothari, R., De Mello, S., Iqbal, U., Byeon, W., Park, S., Kautz, J.: Weakly-
supervised physically unconstrained gaze estimation.
In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2021) 9980–
9989

11. Ghosh, S., Hayat, M., Dhall, A., Knibbe, J.: Mtgls: Multi-task gaze estimation
with limited supervision. In: Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. (2022) 3223–3234

12. Yu, Y., Odobez, J.: Unsupervised representation learning for gaze estimation.
IEEE Conference on Computer Vision and Pattern Recognition (2020) 1–13
13. Dubey, N., Ghosh, S., Dhall, A.: Unsupervised learning of eye gaze representa-
tion from the web. In: 2019 International Joint Conference on Neural Networks
(IJCNN), IEEE (2019) 1–7

14. Swaminathan, A., Ramachandran, M.: Enabling augmented reality using eye gaze

tracking (2018) US Patent 9,996,150.

15. Blattgerste, J., Renner, P., Pfeiﬀer, T.: Advantages of eye-gaze over head-gaze-
based selection in virtual and augmented reality under varying ﬁeld of views. In:
Proceedings of the Workshop on Communication by Gaze Interaction. (2018) 1–9
16. Gumilar, I., Barde, A., Hayati, A.F., Billinghurst, M., Lee, G., Momin, A., Averill,
C., Dey, A.: Connecting the brains via virtual eyes: Eye-gaze directions and inter-
brain synchrony in vr.
In: Extended Abstracts of the 2021 CHI Conference on
Human Factors in Computing Systems. (2021) 1–7

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

21

17. Park, W., Heo, J., Lee, J.: Talking through the eyes: User experience design for
In: International Conference on

eye gaze redirection in live video conferencing.
Human-Computer Interaction, Springer (2021) 75–88

18. Alonso-Mart´ın, F., Gorostiza, J.F., Malfaz, M., Salichs, M.A.: User localization

during human-robot interaction. Sensors 12 (2012) 9913–9935

19. Zabala, U., Rodriguez, I., Mart´ınez-Otzeta, J.M., Lazkano, E.: Modeling and eval-
uating beat gestures for social robots. Multimedia Tools and Applications (2021)
1–18

20. Park, S., Spurr, A., Hilliges, O.: Deep pictorial gaze estimation.

In: European

Conference on Computer Vision. (2018) 721–738

21. Lu, F., Chen, X., Sato, Y.: Appearance-based gaze estimation via uncalibrated gaze
pattern recovery. IEEE Transactions on Image Processing 26 (2017) 1543–1553
22. Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Appearance-based gaze estimation
in the wild. In: IEEE Computer Vision and Pattern Recognition. (2015) 4511–4520
23. Lu, F., Sugano, Y., Okabe, T., Sato, Y.: Gaze estimation from eye appearance:
A head pose-free method via eye image synthesis. IEEE Transactions on Image
Processing 24 (2015) 3680–3693

24. Krafka, K., Khosla, A., Kellnhofer, P., Kannan, H., Bhandarkar, S., Matusik, W.,
Torralba, A.: Eye tracking for everyone. In: IEEE Computer Vision and Pattern
Recognition. (2016) 2176–2184

25. Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: It’s written all over your face: Full-
face appearance-based gaze estimation. In: IEEE Computer Vision and Pattern
Recognition Workshop. (2017)

26. Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Mpiigaze: Real-world dataset and
deep appearance-based gaze estimation. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2017)

27. Jyoti, S., Dhall, A.: Automatic eye gaze estimation using geometric & texture-
based networks. In: International Conference on Pattern Recognition, IEEE (2018)
2474–2479

28. Fischer, T., Chang, H.J., Demiris, Y.: RT-GENE: Real-Time Eye Gaze Estimation
in Natural Environments. In: European Conference on Computer Vision. (2018)
339–357

29. Sugano, Y., Matsushita, Y., Sato, Y.: Learning-by-synthesis for appearance-based
3d gaze estimation. In: IEEE Computer Vision and Pattern Recognition. (2014)
1821–1828

30. Wang, K., Zhao, R., Ji, Q.: A hierarchical generative model for eye image synthesis
and eye gaze estimation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. (2018) 440–448

31. Benfold, B., Reid, I.: Unsupervised learning of a scene-speciﬁc coarse gaze estima-
tor. In: IEEE International Conference on Computer Vision. (2011) 2344–2351
32. Park, S., Mello, S.D., Molchanov, P., Iqbal, U., Hilliges, O., Kautz, J.: Few-shot
adaptive gaze estimation. In: IEEE International Conference on Computer Vision.
(2019) 9368–9377

33. Yu, Y., Liu, G., Odobez, J.: Improving few-shot user-speciﬁc gaze adaptation via
gaze redirection synthesis. In: IEEE Conference on Computer Vision and Pattern
Recognition. (2019) 11937–11946

34. Duchowski, A.T., Duchowski, A.T.: Eye tracking methodology: Theory and prac-

tice. Springer (2017)

35. Komogortsev, O.V., Karpov, A.: Automated classiﬁcation and scoring of smooth
pursuit eye movements in the presence of ﬁxations and saccades. Behavior research
methods 45 (2013) 203–215

22

Ghosh et al.

36. Startsev, M., Agtzidis, I., Dorr, M.: 1d cnn with blstm for automated classiﬁcation
of ﬁxations, saccades, and smooth pursuits. Behavior Research Methods 51 (2019)
556–572

37. Santini, T., Fuhl, W., K¨ubler, T., Kasneci, E.: Bayesian identiﬁcation of ﬁxa-
tions, saccades, and smooth pursuits. In: Proceedings of the Ninth Biennial ACM
Symposium on Eye Tracking Research & Applications. (2016) 163–170

38. Zhu, Y., Yan, Y., Komogortsev, O.: Hierarchical hmm for eye movement classiﬁ-
cation. In: European Conference on Computer Vision, Springer (2020) 544–554
39. Arabadzhiyska, E., Tursun, O.T., Myszkowski, K., Seidel, H.P., Didyk, P.: Saccade
landing position prediction for gaze-contingent rendering. ACM Transactions on
Graphics (TOG) 36 (2017) 1–12

40. Huang, Q., Veeraraghavan, A., Sabharwal, A.: Tabletgaze: dataset and analysis
for unconstrained appearance-based gaze estimation in mobile tablets. Machine
Vision and Applications 28 (2017) 445–461

41. Zhang, X., Park, S., Beeler, T., Bradley, D., Tang, S., Hilliges, O.: Eth-xgaze: A
large scale dataset for gaze estimation under extreme head pose and gaze variation.
In: European Conference on Computer Vision, Springer (2020) 365–381

42. Funes Mora, K.A., Monay, F., Odobez, J.M.: Eyediap: A database for the devel-
opment and evaluation of gaze estimation algorithms from rgb and rgb-d cameras.
In: ACM Symposium on Eye Tracking Research and Applications. (2014)

43. Garbin, S.J., Shen, Y., Schuetz, I., Cavin, R., Hughes, G., Talathi, S.S.: Openeds:

Open eye dataset. arXiv preprint arXiv:1905.03702 (2019)

44. Palmero, C., Sharma, A., Behrendt, K., Krishnakumar, K., Komogortsev, O.V.,
Talathi, S.S.: Openeds2020: Open eyes dataset. arXiv preprint arXiv:2005.03876
(2020)

45. Ganin, Y., Kononenko, D., Sungatullina, D., Lempitsky, V.: Deepwarp: Photoreal-
istic image resynthesis for gaze manipulation. In: European conference on computer
vision, Springer (2016) 311–326

46. He, Z., Spurr, A., Zhang, X., Hilliges, O.: Photo-realistic monocular gaze redirec-
tion using generative adversarial networks. In: Proceedings of the IEEE Interna-
tional Conference on Computer Vision. (2019) 6932–6941

47. Wood, E., Baltruˇsaitis, T., Morency, L.P., Robinson, P., Bulling, A.: Gazedirector:
In: Computer Graphics Forum.

Fully articulated eye gaze redirection in video.
Volume 37., Wiley Online Library (2018) 217–225

48. Kononenko, D., Lempitsky, V.: Learning to look up: Realtime monocular gaze
In: Proceedings of the IEEE Conference on

correction using machine learning.
Computer Vision and Pattern Recognition. (2015) 4667–4675

49. Sela, M., Xu, P., He, J., Navalpakkam, V., Lagun, D.: Gazegan-unpaired adversarial
image generation for gaze estimation. arXiv preprint arXiv:1711.09767 (2017)

50. Bilen, H., Vedaldi, A.: Weakly supervised deep detection networks.

In: IEEE

Computer Vision and Pattern Recognition. (2016) 2846–2854

51. Lee, D.H.: Pseudo-label: The simple and eﬃcient semi-supervised learning method
for deep neural networks. In: International Conference on Machine Learning Work-
shop. Volume 3. (2013) 2

52. Zhang, Y., Dong, W., Hu, B.G., Ji, Q.: Weakly-supervised deep convolutional neu-
ral network learning for facial action unit intensity estimation. In: IEEE Computer
Vision and Pattern Recognition. (2018) 2314–2323

53. Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn archi-
tecture for weakly supervised place recognition. In: IEEE Computer Vision and
Pattern Recognition. (2016) 5297–5307

‘Labelling the Gaps’: A Weakly Supervised Automatic Eye Gaze Estimation

23

54. Zhang, Y., Zhao, R., Dong, W., Hu, B.G., Ji, Q.: Bilateral ordinal relevance multi-
instance regression for facial action unit intensity estimation. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. (2018) 7034–7043
55. Weston, J., Ratle, F., Mobahi, H., Collobert, R.: Deep learning via semi-supervised
embedding. In: Neural networks: Tricks of the trade. Springer (2012) 639–655
56. Zhao, J., Mathieu, M., Goroshin, R., Lecun, Y.: Stacked what-where auto-encoders.

International Conference on Learning Representations Workshop (2015)

57. Sajjadi, M., Javanmardi, M., Tasdizen, T.: Mutual exclusivity loss for semi-
supervised deep learning. In: IEEE International Conference on Image Processing.
(2016) 1908–1912

58. Williams, O., Blake, A., Cipolla, R.: Sparse and semi-supervised visual mapping
with the s3gp. In: IEEE Computer Vision and Pattern Recognition. (2006)

59. Haeusser, P., Mordvintsev, A., Cremers, D.: Learning by association–a versatile
semi-supervised training method for neural networks. In: IEEE Computer Vision
and Pattern Recognition. (2017) 89–98

60. Purves, D., Morgenstern, Y., Wojtach, W.T.: Perception and reality: why a wholly
empirical paradigm is needed to understand vision. Frontiers in systems neuro-
science 9 (2015) 156

61. Majaranta, P.: Gaze Interaction and Applications of Eye Tracking: Advances in
Assistive Technologies: Advances in Assistive Technologies. IGI Global (2011)
62. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. International Conference on Learning Representations (2014)

63. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: IEEE Computer Vision and Pattern Recognition. (2016) 770–778

64. Schmidt, S., Bruder, G., Steinicke, F.: Depth perception and manipulation in
projection-based spatial augmented reality. PRESENCE: Virtual and Augmented
Reality 27 (2020) 242–256

65. Bertasius, G., Feichtenhofer, C., Tran, D., Shi, J., Torresani, L.: Learning temporal
pose estimation from sparsely-labeled videos. arXiv preprint arXiv:1906.04016
(2019)

66. Barz, B., Denzler, J.: Deep learning on small datasets without pre-training using
In: IEEE Winter Conference on Applications of Computer Vision.

cosine loss.
(2020) 1371–1380

67. Baltruˇsaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial be-
havior analysis toolkit. In: IEEE Winter Conference on Applications of Computer
Vision. (2016) 1–10

68. Eberly, D.: A fast and accurate algorithm for computing slerp. Journal of Graphics,

GPU, and Game Tools 15 (2011) 161–176

69. Valenti, R., Sebe, N., Gevers, T.: Combining head pose and eye location infor-
mation for gaze estimation. IEEE Transactions on Image Processing 21 (2011)
802–815

70. Yamazoe, H., Utsumi, A., Yonezawa, T., Abe, S.: Remote gaze estimation with a
single camera based on facial-feature tracking without special calibration actions.
In: Proceedings of the 2008 symposium on Eye tracking research & applications.
(2008) 245–250

71. Ishikawa, T.: Passive driver gaze tracking with active appearance models. (2004)
72. Park, S., Zhang, X., Bulling, A., Hilliges, O.: Learning to ﬁnd eye region landmarks
for remote gaze estimation in unconstrained settings. In: Proceedings of the 2018
ACM Symposium on Eye Tracking Research & Applications. (2018) 1–10

24

Ghosh et al.

73. Sharma, S., Shanmugasundaram, K., Ramasamy, S.K.: Farec—cnn based eﬃcient
face recognition technique using dlib. In: International Conference on Advanced
Communication Control and Computing Technologies. (2016) 192–195

74. :

gaze
Erkil1452/gaze360 (-)

code:https://github.com/swook/GazeML,https://github.com/

75. Peters, C., Qureshi, A.: A head movement propensity model for animating gaze
shifts and blinks of virtual characters. Computers & Graphics 34 (2010) 677–687

