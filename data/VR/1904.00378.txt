Date of publication xxxx 00, 0000, date of current version March 16, 2020.

Digital Object Identiﬁer xx.xxxx/ACCESS.xxxx.DOI

MAT-Fly: an educational platform for
simulating Unmanned Aerial Vehicles
aimed to detect and track moving
objects

GIUSEPPE SILANO1,2, (Student Member, IEEE), and LUIGI IANNELLI2, (Senior Member,
IEEE)
1Faculty of Electrical Engineering, Czech Technical University in Prague; 16636 Prague 6 (Czech Republic) (e-mail: giuseppe.silano@fel.cvut.cz)
2Department of Engineering, University of Sannio in Benevento, Piazza Roma 21; 82100 Benevento (Italy) (e-mail: {giuseppe.silano,
luigi.iannelli}@unisannio.it)

Corresponding author: Giuseppe Silano (e-mail: giuseppe.silano@fel.cvut.cz).

This work was partially funded by the ECSEL Joint Undertaking research and innovation programme COMP4DRONES under grant
agreement no. 826610 and by the European Union’s Horizon 2020 research and innovation programme AERIAL-CORE under grant
agreement no. 871479.

1
2
0
2

r
a

M
0
1

]

O
R
.
s
c
[

4
v
8
7
3
0
0
.
4
0
9
1
:
v
i
X
r
a

ABSTRACT
The main motivation of this work is to propose a simulation approach for a speciﬁc task within the Un-
manned Aerial Vehicle (UAV) ﬁeld, i.e., the visual detection and tracking of arbitrary moving objects. In
particular, it is described MAT-Fly, a numerical simulation platform for multi-rotor aircraft characterized by
the ease of use and control development. The platform is based on Matlab® and the MathWorks™ Virtual
Reality (VR) and Computer Vision System (CVS) toolboxes that work together to simulate the behavior
of a quad-rotor while tracking a car that moves along a nontrivial path. The VR toolbox has been chosen
due to the familiarity that students have with Matlab and because it does not require a notable effort by the
user for the learning and development phase thanks to its simple structure. The overall architecture is quite
modular so that each block can be easily replaced with others simplifying the code reuse and the platform
customization.
Some simple testbeds are presented to show the validity of the approach and how the platform works. The
simulator is released as open-source, making it possible to go through any part of the system, and available
for educational purposes.

INDEX TERMS
educational, Matlab/Simulink, image-based visual servoing, trajectory control, vision detection and track-
ing, software-in-the-loop, unmanned aerial vehicles, multi-rotor

I. INTRODUCTION

U NMANNED Aerial Vehicles (UAVs), although origi-

nally designed and developed for defense and military
purposes (e.g., aerial attacks or military air covering), in
the recent years gained an increasing interest and attention
related to civilian use. Nowadays, UAVs are employed for
several tasks and services like surveying and mapping [1], for
spatial information acquisition and buildings inspection [2],
data collection from inaccessible areas [3], agricultural crops
and monitoring [4], manipulation and transportation or navi-
gation purposes [5].

Many existing algorithms for the autonomous control [6]

and navigation [7] are provided in the literature, but it is
particularly difﬁcult to make the UAVs able to work au-
tonomously in constrained and cluttered environments or
also indoors. Thus, it follows the need for tools that allow
to understand what it happens when some new applications
are going to be developed in unknown or critical situations.
Simulation is one of such helpful tools, widely used in
robotics [8]–[12], whose main beneﬁts are costs and time
savings, enabling not only to create various scenarios, but
also to carry out and to study complex missions that might
be time consuming and risky in real world applications.

VOLUME 4, 2016

1

 
 
 
 
 
 
Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

Moreover, bugs and mistakes cost virtually nothing: it is
possible to crash a vehicle several times and thereby getting a
better understanding of implemented methods under various
conditions. Thus, simulation environments are very impor-
tant for fast prototyping and educational purposes, although
they may have some drawbacks and limitations, such as the
lack of noisy real data or the fact that simulated models are
usually incomplete or inaccurate. Despite the limitations, the
advantages that the simulation provides are more, as like as
to manage the complexity and heterogeneity of the hardware,
to promote the integration of new technologies, to simplify
the software design, to hide the complexity of low-level
communication [13].

Different solutions, typically based on external robotic
simulators such as Gazebo [14], V-REP [15], AirSim [16],
MORSE [17], are available. They employ recent advances
in computation and computer graphics (e.g., AirSim is a
photorealistic environment [7]) in order to simulate physical
phenomena (e.g., gravity, magnetism, atmospheric condi-
tions) and perception (e.g., providing sensor models) in such
a way that the environment realistically reﬂects the actual
world. In some cases, those solutions do not have enough
features that could allow to create large scale complex en-
vironments close to reality. On the other hand, when the
tools provide such possibilities, they are difﬁcult to use or
they require high computational capabilities [16]. Deﬁnitely,
it comes out that simulating the real world is a nontrivial
task, not only due to multiple phenomena that need to be
modeled, but also because their complex interactions ask the
user a notable effort for the learning and development phase.
For all such reasons, having a complete software platform
that makes possible to test different algorithms and control
strategies for UAVs moving in a simulated 3D environment
is increasingly important both for the whole design process
and for educational purposes.

In this paper, it is presented a software platform in which
detection, tracking and control algorithms can be evaluated
and tested all together in a 3D graphical tool. Due to the sim-
ple implementation and the limited possibilities of interfacing
it with dedicated middlewares (e.g., ROS [18], YARP [19],
GenoM [20]), the proposed platform should be meant with an
educational purpose. However, that does not imply a loss of
generality nor makes the platform less important. Indeed, as
highlighted in [21], the use of interactive learning approaches
allows students to improve their technical knowledge and
communication skills, giving them the experience of what
they will encounter in a real world environment. Therefore,
the platform can be appreciated for its potentialities thanks
to the advantages coming from the use of a Software-in-
the-loop (SIL) approach [6], [22], [23]. In other words, the
functionalities provided by the simulator can be easily ex-
panded by students, researchers, and developers modifying or
integrating new vehicles dynamics (e.g., hexarotor [24], fully
actuated platform [25]), control algorithms (e.g., geometric
control laws [26], ﬂatness-based control methods [27]) or
detection and tracking techniques (e.g., YOLO [28], [29],

FIGURE 1. Three consecutive frames produces as output by the CAMShift
algorithm while tracking the target. The image and the bounding box centroids
as well as the distance vector among centroids are depicted (see Sec. IV).

Fast R-CNN [29], [30]) for their purposes.

Compared to the commercial and open-source platforms
available in the literature [31]–[35], the proposed framework
runs on a built-in environment (i.e., Matlab and its tool-
boxes) and has no constraints in terms of hardware (e.g.,
memory, unit processor, etc.). Moreover, the simulator is self-
contained (i.e., everything is in one place) and can also be
used by people without programming skills (i.e., algorithms
are typically written in the most common programming lan-
guages). Matlab and the Computer Vision System (CVS)1
and Virtual Reality (VR)2 toolboxes are the only tools the
user needs to work with.

The speciﬁc domain of interest regards the behavior of
multi-rotor aircraft acting in accordance with the Image-
Based Visual Servoing (IBVS) approach [36], [37]. The eye-
in-hand camera conﬁguration [38] along with the pinhole
camera model is considered for the aerial vehicle. Compared
to other approaches [39], the camera is rigidly attached to
the UAV frame and moves according to the aircraft motion.

The application that is considered is an extension of the
authors’ previous work [40], that has been revised for making
the aircraft able to detect and track a speciﬁc object (a car)
moving along a nontrivial path. This simple scenario is used
as a testbed to show: (i) how the platform works, (ii) the
elements that make up the software architecture, and (iii)
the adaptability of the platform to the different needs of the
user. Compared to the previous work, a tracking algorithm
has been added into the loop: the classiﬁer is used to detect
the target only at the ﬁrst step or in case of partial occlusions.
Apart from such scenarios, a Continuously Adaptive Mean-
Shift (CAMShift) tracking algorithm [41] is employed to
follow the car along the path, thus reducing the computational
burden and the possibility to lose the target during the track-
ing. Moreover, in this paper it is proposed a novel procedure
based on ad hoc Matlab scripts that automatically select the
bounding box area of the target (see, Fig. 1) avoiding to use
speciﬁc Matlab tools, such as Training Image Labeler. These
scripts also allow comparing various classiﬁer conﬁgurations
to help select the most suitable for the case study among
different features types (e.g., Haar, HOG, LBP) [42] and

1https://www.mathworks.com/products/computer-vision.html
2https://www.mathworks.com/products/3d-animation.html

2

VOLUME 4, 2016

12GiuseppeSilano,LuigiIannelli(a)(b)(c)Fig.11ThreeconsecutiveframesproducesasoutputbytheCAMShifttrackingalgorithm.Theimageandtheboundingboxcentroidsaswellasthedistancevectoramongcentroidsarereported.FrameOimgvuhimgwimghbbwbb(uimg,vimg)(ubb,vbb)Fig.12Theschemeillustratestheinformationextractedbytheframes.Inbluethedistancevectorandinyellowtheboundingboxarereported,respectively.Theimage(uimg,vimg)andboundingbox(ubb,vbb)centroidsarealsorepresented.anymulti-rotorsaircraftandconﬁgurationmakingthesoftwareplatformparticularlyusefulforeducationalpurposes.Thedesignofahighperformanceattitudeandpositioncontrollerrequiresoftenanaccuratemodelofthesystem.Wehererecallthecommonlyuseddynamicalmodelofaquadrotor[30]and,byfollowingusualapproaches,weintroducetwoorthonormalframes:theﬁxed-frameOFI(whereFIstandsforFixedInertial),alsocalledinertial(orreference)frame,andthebody-frameOABC(whereABCstandsforAircraftBodyCenter)thatisﬁxedintheaircraftcenterofmassandisorientedaccordingtotheaircraftorientation(attitude),seeFig.13.Thetranslationaldynamicequationsoftheaircraftcanbeexpressedintheinertialframeasfollows:m¨ξ=−mgEz+uTR(ϕ,θ,ψ)Ez,(2)wheregdenotesthegravityacceleration,mthemass,uTthetotalthrustproducedbytherotors,ξ=(cid:0)xyz(cid:1)>thedronepositionexpressedintheinertialframe,Ez=(cid:0)001(cid:1)>istheunitvectoralongtheZ-axis,whileR(ϕ,θ,ψ)istherotationmatrixfromthebodySilano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

number of training stages. Finally, the software platform is
published as open-source3 with the aim to share results with
other researchers, students, and developers that might use the
platform for testing their algorithms and understanding how
different approaches can improve the performance and affect
the system stability.

The paper is organized as follows. Section II explains
the simulation scenario and its functionalities. The classiﬁer
training phase and the vision-based target detection and
tracking algorithms are presented in Sec. III and IV, respec-
tively. Section V brieﬂy describes the quad-rotor model while
numerical results and the control algorithm are reported in
Sec. VI. Finally, Section VII concludes the paper.

II. SYSTEM DESCRIPTION
This section aims to describe MAT-Fly and how it works
together with the Matlab VR and CVS toolboxes. An illus-
trative application, i.e., the object tracking example, where
a drone tracks a car moving along a nontrivial path is con-
sidered. An overview of the main elements that make up the
system is depicted in Fig. 2.

The software platform is mainly divided into four parts:
the classiﬁer training phase (see, Sec. III), the vision-based
target detection and tracking (see, Sec. IV), the ﬂight control
system (see, Sec. VI), and the Matlab VR toolbox. To facili-
tate the development of various control and computer vision
strategies and the reuse of existing software components, the
system was set up using a modular approach splitting each
functionality into interchangeable modules. In other words,
each part of the system (e.g., the vision-based target detection
and tracking, the ﬂight control system) was developed by
isolating every feature (e.g., the detection algorithm, the
reference generator) in such a way they can be easily replaced
with others by facilitating the test and evaluation process.

The Matlab VR toolbox allows to simulate a scenario
as much similar as to the real world accounting for the
interaction between complex dynamic systems with the sur-
rounding scenario. Moreover, thanks to animation recording
functionalities, frames and videos from the scene can be
acquired and used to implement an IBVS problem. Also, the
tool makes it easy to add external viewpoints to monitor any
moving object in the 3D environment from different positions
and orientations.
the

(speciﬁcally the
vr_octavia_2cars example) that describes a quite detailed
dynamical model of a car moving along a nontrivial path was
used as a starting point (see, Fig. 3). The example represents
a standard double-lane-change maneuver [43] conducted in
two-vehicles conﬁguration, where one engages the Electronic
Stability Program (ESP) control while the other switches
off such control unit when changing the lane. From this
perspective, a simpler scenario was considered by removing

examples4

available

One of

3https://github.com/gsilano/MAT-Fly
4The list of the ready-to-use scenarios is accessible at https://goo.gl/

rtEx3S.

VOLUME 4, 2016

FIGURE 2. The proposed software platform architecture. Arrows represent
the data exchanged among blocks and how they interact with each other.
Colors point out the four parts making up the system: the classiﬁer training
phase (in blue), the vision-based target detection and tracking (in green), the
ﬂight control system (in yellow), and the Matlab VR toolbox (in red).

FIGURE 3. Initial frame extracted from the object tracking example. The
steering angle visualizer allows monitoring the car movements along the path.

one of the two vehicle conﬁgurations, i.e., the car without
the ESP controller.

Then, an external viewpoint was added to the scheme
for simulating the behavior of a quad-rotor that ﬂies by
observing the car moving along the path. In Matlab VR a
viewpoint has six Degrees of Freedoms (DoFs): the spatial
coordinates x, y, and z, and the angles yaw (ψ), pitch (ϑ),
and roll (ϕ). The whole process is the following: images
are updated according to the position and the orientation of
the quad-rotor w.r.t. the car; such images are acquired and
elaborated for getting the necessary information to detect and
track the target, and to run the control strategy designed for
the tracking problem. The outputs of the control algorithm
consists of the commands uϕ, uϑ, uψ, and uT that should be
given to the drone in order to update its position (xd, yd, and
zd) and orientation (ϕd, ϑd, and ψd ), see Fig. 4.

It is worth noticing that ground truth data are used by the
tracking controller (see Sec. VI). Therefore, any analysis can
be conducted on the correctness of the data and how this
affects the navigation. However, this does not constitute a
limitation for the proposed framework thanks to the modular

3

6GiuseppeSilano,LuigiIannelliOFIXFIZFIYFI−YFIOFVRXFVRYFVRZFVR−ZFVRFig.4ThepictureillustratestheclassicﬁxedframeOFI(left)andthecorrespondingvirtualﬁxedOFVR(right)referencesystem.FramesAcquisitionImagesCroppingClassiﬁerSynthesisPerformanceEvaluationSIMULATIONOFF-LINECONFIGURATIONVisionTargetSelectorDetectionAlgorithmTrackingAlgorithmDistanceVectorComputingﬁrststepnextstepsReferenceGeneratorTrajectoryControlDroneModelMATLABVIRTUALWORLDFig.5Proposedsoftwareplatformarchitecture.Arrowsrepresentthedataexchangedamongblocksandhowtheyinteracteachother.toolbox(inred).Adashedlineisusedtoseparatetheclassiﬁerlearningphasefromtherestofthescheme,sinceitdoesnottakepartdirectlyinthesimulationalthoughitsoutputsareemployedbythedetectionalgorithmandreferencegeneratorfortheobjecttracking(seeSecs.3and6).Tosimplifythereuseofsoftwarecomponents,theentireplatformwasdesignedbyap-plyingamodularapproach:eachpart(theclassiﬁerlearningphase,thevisionbasedtargetdetection,etc.)hasbeendividedintosmallerones(e.g.,thedetectionalgorithm,therefer-encegenerator,theclassiﬁersynthesis,etc.),puttingsomeeffortinreducingtheirdependen-ciesandthusmakingthemreadytobeusedorreplacedwithotherscomponents.Insuchaway,differentcomputervisionandcontrolalgorithmscanbecombinedandtestedevaluat-Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

y = r cos β
x = r sin β sin α
z = r sin β cos α

,




(1)

FIGURE 4. The control scheme. Subscript d indicates the drone variables,
while r represents the references to the controller. Each block is mapped with
the section that describes its content to help matching the blocks with the
corresponding description within the paper.

interface exhibited by the platform [44].

In Figure 5 the Simulink scheme employed for simulating
the drone and the car dynamics is reported. The esp_on and
the coordinates_transformation blocks compute the steer-
ing angle, the linear velocity and the position of the car,
and all forces needed to follow a given path. Instead, the
observer_position and rotation_matrix blocks represent the
aircraft position and orientation (it is expressed by using
the direction cosine matrix [45] and the Rodrigues’s for-
mula [46]), respectively. The processed data are sent to the
VR Visualization block that takes care of the drone and car
movements in the simulated scenario.

Note that Matlab VR adopts a reference system
(OFVR) [47] slightly different from the classic ﬁxed refer-
ence frame OFI (see Sec. V), thus simple references transfor-
mations have been taken into account in all elaborations.

Finally, the Simulink scheme saves the current car position
(xcar, ycar, and zcar), used for comparing the drone and the
car trajectories (see, Sec. VI-A), and frames of the virtual
scenario observed from the drone point of view. Those frames
are used, as described in next sections, for pattern recogni-
tion.

III. CLASSIFIER TRAINING PHASE
The classiﬁer training phase is the most important part of
the system: the object detection and tracking depend on it.
Matlab scripts have been developed to automate the entire
procedure, from the frames acquisition to the classiﬁer syn-
thesis and performance evaluation. To this aim, the training
process has been divided into four parts, as depicted in
Fig. 2: the frames acquisition, the bounding box selection,
the classiﬁer synthesis and the performance evaluation.

A. FRAMES ACQUISITION
When going to train a classiﬁer, a high number of images
is needed. The images are divided into two groups: positive
(that contain the target) and negative images. Following what
described in [46], 2626 positive and 10504 negative images
were used achieving a 1 : 4 ratio in accordance to the Pareto’s
principle (aka the 80/20 rule).

For the frames acquisition, a simulation was performed
with the quad-rotor moving along a spiral trajectory around
the car parked in its initial state (see, Fig. 6). The aircraft
attitude and position have been computed for each frame so
as described by the sphere surface equations,



where r, the sphere radius, is the distance between the car
and the drone (assumed to be ﬁxed and equal to 15 meters),
[0, π/2] angles,
and together with α
identiﬁes the drone position in the 3D space, as depicted
in Fig. 6. A video showing the quad-rotor camera point of
view while observing the car parked in its initial state while
following the spiral trajectory is available in [48].

[0, 2π] and β

∈

∈

B. BOUNDING BOX SELECTION
To train the classiﬁer, the Region of Interest (ROI) of the
target needs to be computed. Due to the high number of
images, manual labeling tools, such as the MathWorks Train-
ing Image Labeler, cannot be used. Thus a Matlab script
was developed to automatically select the bounding box area
surrounding the target. The image segmentation process was
used to simplify and to change the image representation: from
RGB to grayscale (Figs. 7(a) and 7(b), respectively). The
result is a set of contours that make the image meaningful and
easier to analyze: each group of pixels in a region is similar
w.r.t. some characteristics or computed properties, intensity,
or texture, while adjacent regions are signiﬁcantly different
w.r.t. the same properties.

To automatically select each group of pixels, the Balanced
Histogram Thresholding (BTH) method [49] was used. Such
a method allows to separate the background from the fore-
ground image by dividing the data into two main classes (see,
Fig. 8) and by searching for the optimum threshold level.

Starting from the foreground grayscale image (see,
Fig. 7(b)), the script deals with labeling the individual blobs
by using the connected-component labeling algorithm [46]
with a ﬁxed heuristic (8-connected, in the considered case).
In Figure 7(c) the obtained blobs are depicted with different
colors and numbers for visual convenience. Then, the script
selects the blob that meets the criteria in terms of size and
intensity (chosen to match the target properties) in order
to obtain a unique bounding box surrounding the target
(see, Figs. 7(d) and 7(e)). Finally, the script provides as
output a MAT-ﬁle containing, for each positive image, the
suitable ROI components, i.e., the bounding box centroid, its
width and height. This ﬁle is used for the classiﬁer synthesis
in the target detection design process (see, Fig. 2).

The proposed approach allows to automatically label the
target (the car) from the positive images, thus decreasing the
time spent for the training phase. In Figure 7, for a single
sample frame, all elaboration steps are reported.

On the considered data set, the script was able to auto-
matically detect the ROIs with an error of 8.18 %: the target
was not recognized only in 215 frames out of 2626 positive
images, and the ﬁrst ROI loss appeared at the 1791th frame.

C. CLASSIFIER SYNTHESIS
The Viola & Jones algorithm [50] was chosen as object
detection framework to recognize the car along the path.

4

VOLUME 4, 2016

MAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects5TargetDetectorSectionIVReferenceGeneratorSectionVI-ATrajectoryControlSectionVI-BDroneDynamicsSectionVMatlabVirtualWorldSectionIIeu,evareabbzr,ψrxr,yruψ,uTuϕ,uϑϕd,ϑd,ψdxd,yd,zdIMGCarFig.2Thecontrolscheme.Subscriptdindicatesthedronevariables,whilerindicatesthereferencestothecontroller.Forthereader,eachblockhasbeenmappedwiththesectionthatdescribesitscontent.Car_2.Car_PositionCar_2.Car_RotationCar_2.WheelFL_RotationCar_2.WheelFL_TranslationCar_2.WheelFL_Spin_RotationCar_2.WheelFR_RotationCar_2.WheelFR_TranslationCar_2.WheelFR_Spin_RotationCar_2.WheelRL_RotationCar_2.WheelRL_TranslationCar_2.WheelRL_Spin_RotationCar_2.WheelRR_RotationCar_2.WheelRR_TranslationCar_2.WheelRR_Spin_RotationCar_2.SteeringWheel1_RotationCar_2.Backlights_SwitchCar_2.Forces_SwitchCar_2.Force1_ScaleCar_2.Force2_ScaleCar_2.Force3_ScaleCar_2.Force4_ScaleCar_2.Car_DiffuseColorCar_2.Body_ScaleFlyingCamera.orientationFlyingCamera.positionVR Visualization[255  0  0]Car Colouresp_onESP OnIn1Vehicle_posVehicle_rotWheel1_rotWheel1_posWheel1_spinWheel2_rotWheel2_posWheel2_spinWheel3_rotWheel3_posWheel3_spinWheel4_rotWheel4_posWheel4_spinSteering_angleBrake_pedalForces_OnOffForces1Forces2Forces3Forces4Coordinate Transformations [1.01 1.01 1.01]observer_positionObserver Positionrotation_matrixRotation MatrixVirtualWorldImageImage To WorkspaceVRRotation Matrixto VRML Rotationcar_positionCar PositionCar ScaleFig.3Simulinkschemeemployedforsimulatingthedroneandcardynamicsinthevirtualscenario.angle,thelinearvelocityandthepositionofthecar,andallforcesneededtofollowagivenpath.While,theobserverpositionandrotationmatrixblocksrepresenttheaircraftpositionandorientation(itisexpressedbyusingthedirectioncosinematrix[30]andtheRodrigues’sformula[31]),respectively.TheprocesseddataaresenttotheVRVisualizationblockthattakescareofthedroneandcarmovementsinthesimulatedscenario.NotethatMatlabVRadoptsareferencesystemslightlydifferentfromtheclassicﬁxedreferenceframeOFI.Figure4illustratessuchdifference.Inparticular,axesaredifferentlyorientedand,furthermore,thevirtualscenarioreferencesystemiscenteredinthecarcenterofgravity,althoughtheaxesorientationisﬁxed.Thesedifferencesaretakenintoaccountinallelaborations.Finally,theSimulinkschemesavesthecurrentcarposition(xcar,ycarandzcar),usedforcomparingthedroneandthecartrajectories(see,Sec.6.1),andframesofthevirtualscenarioobservedfromthedronepointofview.Thoseframeswillbeused,asdescribedinthenextsections,forpatternrecognition.InFigure5theschemeoftheoverallsoftwareplatformarchitectureisdepicted.Colorshighlightthedifferentpartsofthesystem:theclassiﬁerlearningphase(inblue),thevisionbasedtargetdetection(ingreen),theﬂightcontrolsystem(inyellow)andtheMatlabVRSilano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

FIGURE 5. Simulink scheme employed for simulating the drone and car dynamics in the 3D simulation environment.

FIGURE 6. Drone trajectory around the car parked in its initial state during the
frames acquisition phase.

The algorithm was originally designed and developed for
face detection problem, but it can be easily trained to detect
any object [51] by using different features types (e.g., Haar,
HOG, LBP) [42] and training stages5. Although even more
complicated and performing object detection frameworks
(e.g., YOLO [28], [29], Fast R-CNN [29], [30]) are available
in the literature, historical reasons motivated this choice:
the Viola & Jones classiﬁer was the ﬁrst object detection
framework in real-time. Thus, it is of interest, at least for
educational purposes, to have a simulation platform that
performs object detection with such a solution.

5This is common in cascade classiﬁers where each stage is an ensemble

of weak learners, i.e., simple classiﬁers called decision stumps.

FIGURE 7. Frames obtained by the images segmentation process. From the
RGB ﬁle format (a) to the cropped image (e) the steps are shown, sequentially.

For the considered testbed, the Haar features were used
to design the classiﬁer. These features along with LBP are
often used to detect faces due to their ﬁne-scale textures while
HOG features are often employed to detect objects. However,
the obtained results suggested to choose Haar features which
appeared more useful for capturing the overall shape of the
target (see, Fig. 9) even if longer time was needed during the
training phase.

VOLUME 4, 2016

5

Car_2.Car_PositionCar_2.Car_RotationCar_2.WheelFL_RotationCar_2.WheelFL_TranslationCar_2.WheelFL_Spin_RotationCar_2.WheelFR_RotationCar_2.WheelFR_TranslationCar_2.WheelFR_Spin_RotationCar_2.WheelRL_RotationCar_2.WheelRL_TranslationCar_2.WheelRL_Spin_RotationCar_2.WheelRR_RotationCar_2.WheelRR_TranslationCar_2.WheelRR_Spin_RotationCar_2.SteeringWheel1_RotationCar_2.Backlights_SwitchCar_2.Forces_SwitchCar_2.Force1_ScaleCar_2.Force2_ScaleCar_2.Force3_ScaleCar_2.Force4_ScaleCar_2.Car_DiffuseColorCar_2.Body_ScaleFlyingCamera.orientationFlyingCamera.positionVR Visualization[255  0  0]Car Colouresp_onESP OnIn1Vehicle_posVehicle_rotWheel1_rotWheel1_posWheel1_spinWheel2_rotWheel2_posWheel2_spinWheel3_rotWheel3_posWheel3_spinWheel4_rotWheel4_posWheel4_spinSteering_angleBrake_pedalForces_OnOffForces1Forces2Forces3Forces4Coordinate Transformations [1.01 1.01 1.01]observer_positionObserver Positionrotation_matrixRotation MatrixVirtualWorldImageImage To WorkspaceVRRotation Matrixto VRML Rotationcar_positionCar PositionCar ScaleMAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects7YFVRXFVRZFVR−ZFVRβαrCameraCarPxz−xPzPyPFig.6Dronetrajectoryaroundthecarparkedinitsinitialstateduringtheframesacquisition.3ClassiﬁerLearningPhaseTheclassiﬁerlearningphaseisthemostimportantpartofthesystem:theobjectdetectionandtrackingdependonit.Matlabscriptshavebeendevelopedtoautomatetheentirepro-cedure,fromtheframesacquisitiontotheclassiﬁersynthesisandperformanceevaluation.Tothisaim,thelearningprocesshasbeensplitintofourparts,asitisdepictedinFig.5:theframesacquisition,theimagecropping,theclassiﬁersynthesisandtheperformanceevalua-tion.3.1FramesacquisitionWhengoingtotrainaclassiﬁer,ahighnumberofimagesareneeded.Theimagesaredividedintotwogroups:positive(thatcontainthetarget)andnegativeimages.Byfollowingassuggestedin[31],inparticularweused2626positiveimagesand5252negativeimagesachievinga1:2ratio.Forcollectingtheimages,wesimulatedthedronemovingalongaspiraltrajectoryaroundthecarparkedinitsinitialstate(see,Fig.6).Theaircraftattitudeandpositionhavebeencomputedforeachframesoasdescribedbythespheresurfaceequations,y=rcosβx=rsinβsinαz=rsinβcosα,(1)wherer,thesphereradius,isthedistancebetweenthecarandthedrone(assumedﬁxedandequalto15meters),usedasreferenceforthetrajectorygeneration(see,Sec.6.1).Whereas,theanglesα∈[0,2π]andβ∈[0,π/2]allowtodiscriminatethedronepositionandorienta-tionalongthesurfaceofthesphere,asdepictedinFig.6.Avideoshowingtheresultshasbeenmadeavailableatthelinkhttps://youtu.be/A70zed84zv0.8GiuseppeSilano,LuigiIannelli(a)RGB.(b)Grayscale.(c)Detectedblobswithpseudocolors.(d)Boundingboxarea.(e)Croppedimage.Fig.7Framesobtainedbytheimagescomputingprocess.FromtheRGBﬁleformat(a)tothecroppedimage(f)thestepsarereported,sequentially.Eachﬁgureislabeledaccordingtothephaseduringwhichithasbeenobtained.3.2ImagecroppingFollowingtheapproachproposedin[32],aMatlabscriptwasdevelopedtoautomaticallyse-lecttheboundingboxareaofthecar.Theimagesegmentationprocesswasusedtosimplifyandtochangetherepresentation:fromRGBtograyscale(Figs.7(a)and7(b),respectively).Theresultisasetofcontoursthatmaketheimagemoremeaningfulandeasiertoanalyze:eachgroupofpixelsinaregionissimilarw.r.t.somecharacteristicsorcomputedproperties,e.g.,color(theredofthecar,inourcase),intensity,ortexture,whileadjacentregionsaresigniﬁcantlydifferentw.r.t.thesameproperties,thusallowingtoeasilydetectthetarget.Toautomaticallyselecteachgroupofpixels,theBalancedHistogramThresholding(BTH)method[33]waschosen.Suchmethodallowstoseparatethebackgroundfromtheforegroundbydividingtheimagedataintotwomainclasses(see,Fig.8)andbysearchingfortheoptimumthresholdlevel.Startingfromtheforegroundgrayscaleimage(see,Fig.7(b)),thescriptusestheconnected-componentlabelingalgorithm[31]withaﬁxedheuristic(8-connected,inourcase)forse-lectingindividualblobsfromtheimage.InFigure7(c)suchblobsaredepictedwithdifferentcolorsandnumbers.Then,afterhavingextractedallimagesproperties,thescriptchoosestheblob(boundedandoverlappedinFig.7(d))thatmeetscriteriaintermsofsizeandintensity(ﬁxedtomatchthetargetproperties)inordertoobtainauniqueboundingboxsurroundingthetarget.InFigure7(e)itssizehasbeenusedforcroppingthetargetwithintheframe.Finally,thescriptmakesasoutputaMAT-ﬁlecontaining,foreachpositiveimage,thesuit-Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

FIGURE 8. Histogram of the image data. The red line, i.e., the grayscale
threshold, divides the graph into two parts: the background and the
foreground. The gray gradient bars indicate the associated color to each
x-value, from 0 to 255.

FIGURE 9. Detection results obtained by using Haar, HOG and LBP feature
types. The false alarm and true positive rates are 0.001 and 0.995,
respectively, while the number of training stages is 4. Two different target
models are considered: the “corrected” and “uncorrected” version.

D. PERFORMANCE COMPARISON
When designing a cascade object detector, the number of
training stages, the false alarm and true positive rates, need
to be tuned in accordance to the required performance (e.g.,
accuracy) and constraints (e.g., time response). To facilitate
the analysis as well as to ﬁnd the most suitable set of pa-
rameters that ﬁt the problem, a Matlab script was developed
to evaluate the performance of the classiﬁer. This script is
part of the proposed software platform (see, Fig. 2) and
allows to compare in a few steps various conﬁgurations and
models getting a general overview of how the object detector
behaves.

In Fig. 9 the results obtained for a single sample frame
are reported. Two different models were considered to prove
the validity of the proposed approach: the uncorrected and
corrected models. The ﬁrst one uses the ROIs automatically
extracted from the algorithm presented in Sec. III-B, while
the second one employs those obtained using the Matlab tool
Training Image Labeler. In all revelations, the car is only par-
tially detected despite the large number of images employed

FIGURE 10. Bounding box selection algorithm. The detection results are
obtained by using the Haar cascade features type. The maximum (left) and
average (right) bounding boxes are computed by using the result obtained
from detection (center).

to train the classiﬁer. Except for some cases, there are no
revelation errors: different bounding boxes are detected in the
image. This is probably due to the absence of photorealism
in the collected frames. As described in [52], the reality gap
(i.e., realistic geometry, textures, lighting conditions, camera
noise, and distortion) affects the performance of computer
vision algorithms. On the other hand, they introduce enough
“useful noise” to help the detection (the presence of several
image view points).

Many tests have been conducted in order to assess the true
performance of the classiﬁer. As shown in Figs. 9(b) and 9(c),
the detection results are very similar for both models. Thus, it
is a good approximation to consider the “uncorrected” ROIs
instead of the “corrected” version in the classiﬁer design
process. Such approximation allows to save time during the
training phase thus avoiding to use speciﬁc tools, such as the
Matlab tool Training Image Labeler, when the ROI detection
fails. Moreover, it proves the validity and the effectiveness
of the automatic tool procedure for bounding box selection.
Of course, further tests may be carried out considering more
valuable evaluation criteria, such as confusion matrix, accu-
racy, precision, recall, speciﬁcity [29].

IV. VISION-BASED TARGET DETECTION
The vision-based target detection phase sets up the IBVS
problem using the classiﬁer and tracking algorithms as feed-
back from the environment (see, Fig. 2). There are four com-
ponents that constitute this part: the vision target selector,
the detection and tracking algorithm, and the distance vector
computing.

The vision target selector takes care of switching between
the detection and the tracking algorithms based on the recog-
nition results: the detector is used only at the ﬁrst step or
in case of partial occlusion, otherwise a CAMShift tracking
algorithm [46] is employed to follow the car along the path.
This algorithm performs target tracking by searching for its
probability distribution pattern in a local adaptive size win-
dow. Although it does not guarantee the best performances,
the algorithm supplies reliable and robust results [53].

Due to multiple target revelations (see, Sec. III-D), a
Matlab script was used to obtain a unique bounding box

6

VOLUME 4, 2016

MAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects902040608010012014016018020022024000.511.522.5·105Thresholdedat100graylevelsBackgroundForegroundFig.8Histogramoftheimagedata.Theredline,thegrayscalethreshold,dividesthegraphintotwoparts:thebackgroundandtheforeground.Thegraygradientbarsindicatetheassociatedcolortoeachx-value,from0to255.Finally,thescriptmakesasoutputaMAT-ﬁlecontaining,foreachpositiveimage,thesuit-ableROIcomponents,i.e.,theboundingboxcentroids,itswidthandheight.Suchﬁleisemployedduringtheclassiﬁersynthesis(see,Fig.5)todesignthetargetdetector.Theproposedapproachallowstocompletelyandautomaticallydetectthetarget(thecar),decreasingthelearningphasetimeandavoidingtopassthroughthespeciﬁcMatlabtoolTrainingImageLabeler.InFigure7,forasinglesampleframe,allimageprocessingsteps,fromtheacquiredframetothecroppedimage,aredepicted.Suchstepsallowtobetterunderstandhowthealgorithmworks.Forthespeciﬁcconsideredimageset,thescriptwasabletoautomaticallydetecttheROIswithanerrorof8.18%:thetargetwasnotrecognizedonlyfor215framesoutof2626positiveimages,andtheﬁrstROIlossappearedafter1791frames.3.3ClassiﬁersynthesisTheViola&Jonesalgorithm[34]hasbeenusedtorecognizethecaralongthepath.Al-thoughthealgorithmwasoriginallydesignedanddevelopedforfacedetectionproblem,itcanbetrainedtodetectanyobject[35]byusingdifferentfeaturestypes(Haar,HOG,LBP)[27]andtrainingstages.Fortheconsideredtestbed,theHaarfeatureshavebeenusedfordesigningtheclassiﬁer.AlthoughHaarandLBPfeaturesareoftenusedtodetectfacesduetotheirﬁne-scaletextureswhiletheHOGfeaturesareoftenusedtodetectobjects(e.g.,peoplesandcars),theyresultedmoreusefulforcapturingtheoverallshapeofthetarget(see,Fig.9)evenifmuchlongertimewasneededduringthetrainingphase.10GiuseppeSilano,LuigiIannelli(a)Testimage.(b)Haar.(c)Haarcorrected.(d)HOG.(e)HOGcorrected.(f)LBP.(g)LBPcorrected.Fig.9DetectionresultsobtainedbyusingHaar,HOGandLBPfeaturestypes.Thefalsealarmandtruepositiverateshasbeenﬁxedequalto0.001and0.995,respectively,whilethenumberoftrainingstageswaschosenequalto4.Twodifferenttargetmodelswereconsidered:the“corrected”and“uncorrected”version.oftrainingstages,andfeaturestypes.Suchscriptispartoftheproposedsoftwareplatform(see,Fig.5),andallowsquiteeasilytocompareresultscomingfromdifferentclassiﬁers.Twodifferentmodelshavebeenconsideredindesigningthedetector.TheﬁrstoneusestheROIsautomaticallyextractedfromthevirtualscenario,asdescribedinSec.3.2,whilethesecondone“corrects”thoseROIsthroughtheMatlabtoolTrainingImageLabeler.InFigure9thedetectionresultsobtainedfortheconsideredsampleframebyusingtheHarr,HOGandLBPfeaturestypearedepicted.Inallrevelations,thecarisonlypartiallydetectedinspiteofthehighnumberofimagesemployedinthelearningprocess.Exceptinsamecases,therearenorevelationerrors:differentboundingboxesaredetectedintheim-age.Thisisprobablyduetotheseveralimageviewpointsusedduringthelearningprocessandtheabsenceofphotorealisminthecollectedframes.Asdescribedin[36],therealitygap(i.e.,realisticgeometry,textures,lightingconditions,cameranoiseanddistortion)directin-ﬂuencestheperformancesofcomputervisionalgorithms.Ontheotherhand,theyintroduceenough“usefulnoise”tohelpthedetection.Manytestshavebeenconductedinordertoassessthetrueperformanceoftheclassiﬁer.AsshowninFigs.9(b)and9(c),thedetectionresultsareverysimilarforbothmodels.Thus,itisagoodapproximationtoconsiderthe“uncorrected”ROIsinsteadofthe“cor-rected”versionintheclassiﬁerdesign.SuchapproximationallowstosavetimeduringthetrainingphasethusavoidingtopassthroughtheMatlabtoolTrainingImageLaebeler.4VisionBasedTargetDetectionThevision-basedtargetdetectionphase(see,Fig.5)isdividedintofourparts:thevisiontar-getselector,thedetectionandtrackingalgorithmsandthedistancevectorcomputing.TheMAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects11(a)Maximumboundingbox.(b)Allboundingboxes.(c)Averageboundingbox.Fig.10Boundingboxselectionalgorithm.ThedetectionresultsareobtainedbyusingtheHaarcascadefeaturestype.Themaximum(left)andaverage(right)boundingboxesarecomputedbyusingtheresultobtainedfromdetection(center).visiontargetselectormanagestheswitchingfromdetectiontotrackingwhenrecognizingthetarget:thedetector(seeSec.3.3)isusedonlyattheﬁrststeporincaseofpartialocclu-sion,otherwiseaCAMShifttrackingalgorithm[31]isemployedtofollowthecaralongthepath.Then,recognizedthetarget,thedistancevectorcomputingblockdealswithgeneratingthereferencesforthedronetrajectorycontrolmeasuringthedistancebetweentheimageandboundingboxcentroids.Duetomultipletargetrevelations(asdescribedinSec.3.4),aMatlabscriptwasusedforobtainingauniqueboundingboxsurroundingthetarget(thecar).Thescriptcomputesthemaximum(Fig.10(a))andtheaverage(Fig.10(c))boundingboxes,asshowninFig.10.Themaximumapproachputmoretrustinthedetectionresults,whiletheaverageapproachtriestoﬁlterouttherevelationerrors.The“good”choicedependsontheparticularusedclassiﬁerandontheamountframesemployedduringthetrainingphase.Inourcasestudy,themaximumboundingboxhasbeenchosentoﬁgureouttheimage-basedvisualproblem.Whereas,theCAMShiftalgorithmwasusedtofollowthecaralongthepath.Thisalgo-rithmperformstargettrackingbysearchingforitsprobabilitydistributionpatterninalocaladaptivesizewindowwhoseinitialsizewindowistheoutputoftheboundingboxselectionscript.Althoughitdoesnotguaranteethebestperformances,thealgorithmsuppliesreliableandrobustresults[37].InFigure11threeconsecutiveframesproducedasoutputbythetrackerarereported.Theframesshowhowthealgorithmworksexploitinglowsensitivenessw.r.t.anychangeintheobjectappearance(e.g.,shapedeformation,scale,andilluminationchangesorcameramotion),comparedwithdetection.Then,thedistancevectorbetweentheimage(uimg,vimg)andtheboundingcentroids(ubb,vbb)iscomputed,asdepictedinFigs.11and12.Thevectoraimstoprovidetheref-erencesignals(euandev)tothedronetrajectorycontrol[21](seeSec.6.1),sotomovethedroneinsuchawaythatthecarboundingboxcenteroverlapswiththeimagecentroid.5DroneDynamicalModelInourcasestudy,weconsideredadronewithfourrotorsinaplusconﬁguration[38].How-ever,themodularapproachusedtodevelopthesimulationarchitectureallowstosimulateSilano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

FIGURE 11. The diagram shows how frames are processed once the target is
detected. The distance vector and the bounding box are represented in blue
and yellow, respectively. The image (uimg, vimg) and bounding box (ubb, vbb)
centroids are also reported.

surrounding the target (the car). The script computes the
maximum (Fig. 10(a)) and the average (Fig. 10(c)) bounding
boxes, as shown in Fig. 10. The maximum approach puts
more trust in the detection results, while the average approach
tries to ﬁlter out the revelation errors. The “good” choice
depends on the particular employed classiﬁer and on the
amount of frames used during the training phase. For the
considered testbed, the maximum bounding box was chosen
to ﬁgure out the IBVS problem.

Once the target has been recognized, the distance vec-
tor computing block generates the references for the drone
trajectory control (see, Sec. VI and Fig. 4) measuring the
distance between the image (uimg, vimg) and bounding box
(ubb, vbb) centroids, as depicted in Fig. 11. The vector aims to
provide the reference signals (eu and ev) to move the drone
so that the center of the bounding box surrounding the car
overlaps the centroid of the image.

V. DRONE DYNAMICAL MODEL
For the speciﬁc case study, a quad-rotor in a plus conﬁgura-
tion has been considered. The design of a high performance
attitude and position controller requires often an accurate
model of the system. It is here recalled the commonly used
dynamical model of a quad-rotor [54] and, by following
usual approaches, two orthonormal frames are introduced:
the ﬁxed-frame OFI (where FI stands for Fixed Inertial),
also called inertial (or reference) frame, and the body-frame
OABC (where ABC stands for Aircraft Body Center) that is
ﬁxed in the aircraft center of mass and is oriented according
to the aircraft orientation, see Fig. 12.

The translational dynamic equations of the aircraft can be

expressed in the inertial frame as follows:

m¨ξ¨ξ¨ξ =

mgEz + uT R(ϕ, ϑ, ψ)Ez,

−

(2)

where g denotes the gravity acceleration, m the mass, uT
the total thrust produced by the rotors, ξξξ = (x, y, z)(cid:62)
∈
R3 the drone position expressed in the inertial frame,
Ez = (0, 0, 1)(cid:62) is the unit vector along the Z-axis, while
R3×3 is the rotation matrix from the body
R(ϕ, ϑ, ψ)
to the inertial frame and it depends on the attitude ηηη =
R3 (i.e., Euler angles roll, pitch, and yaw, re-
(ϕ, ϑ, ψ)(cid:62)

∈

∈

VOLUME 4, 2016

FIGURE 12. Drone in the body-frame (OABC) and the ﬁxed-frame (OFI)
reference systems. Forces produced by each rotor, spin directions and
propeller velocities, Ωi, are also reported.

spectively) that describes the body-frame orientation accord-
ing to the ZYX convention [55]. Furthermore, the rotational
dynamics can be expressed as

I ˙ω˙ω˙ωB =

ωωωB

−

×

IωωωB + τττ ,

(3)

×

’ denotes the vector product, ωωωB = (ωx, ωy, ωz)(cid:62)

where ‘
∈
R3 is the angular velocity vector expressed in the body-
R3×3 is the inertia matrix of
frame, I = diag(Ix, Iy, Iz)
∈
the vehicle w.r.t. its principal axes, and τττ = (uϕ, uϑ, uψ)(cid:62)
∈
R3 is the control torque vector obtained by actuating the
rotors speeds according to the rotors conﬁguration and the
vehicle shape.

At low speeds and around the hovering state, the simpliﬁed
dynamic model consists of six second order differential equa-
tions obtained from balancing forces and momenta acting
on the drone, where c• and s• denote the cos(
)
functions, respectively:

) and sin(
•
•

Ix ¨ϕ = ˙ϑ ˙ψ (Iy
Iy ¨ϑ = ˙ϕ ˙ψ (Iz
Iz ¨ψ = ˙ϑ ˙ϕ (Ix

Iz) + uϕ,

Ix) + uϑ,

Iy) + uψ,

−

−

−

m¨x =uT (cϕsϑcψ + sϕsψ) ,
m¨y =uT (cϕsϑsψ
sϕcψ) ,
m¨z =uT cϑcϕ

−
mg,

−

with

and

uT = bf

(cid:0)

Ω2

1 + Ω2

2 + Ω2

3 + Ω2
4

,

(cid:1)

uϕ
uϑ
uψ









=

bf
bm 

−



Ω2
bml
4 −
Ω2
bml
3 −
(cid:0)
1 + Ω2
2 −
(cid:0)

Ω2
2
Ω2
1
(cid:1)
3 + Ω2
Ω2
4
(cid:1)

Ω2

(4a)

(4b)

(4c)

(5a)

(5b)

(5c)

(6)

(7)

7

,





12GiuseppeSilano,LuigiIannelli(a)(b)(c)Fig.11ThreeconsecutiveframesproducesasoutputbytheCAMShifttrackingalgorithm.Theimageandtheboundingboxcentroidsaswellasthedistancevectoramongcentroidsarereported.FrameOimgvuhimgwimghbbwbb(uimg,vimg)(ubb,vbb)Fig.12Theschemeillustratestheinformationextractedbytheframes.Inbluethedistancevectorandinyellowtheboundingboxarereported,respectively.Theimage(uimg,vimg)andboundingbox(ubb,vbb)centroidsarealsorepresented.erencesignals(euandev)tothedronetrajectorycontrol[21](seeSec.6.1),sotomovethedroneinsuchawaythatthecarboundingboxcenteroverlapswiththeimagecentroid.5DroneDynamicalModelInourcasestudy,weconsideredadronewithfourrotorsinaplusconﬁguration[38].How-ever,themodularapproachusedtodevelopthesimulationarchitectureallowstosimulateanymulti-rotorsaircraftandconﬁgurationmakingthesoftwareplatformparticularlyusefulforeducationalpurposes.Thedesignofahighperformanceattitudeandpositioncontrollerrequiresoftenanaccuratemodelofthesystem.Wehererecallthecommonlyuseddynamicalmodelofaquadrotor[30]and,byfollowingusualapproaches,weintroducetwoorthonormalframes:theﬁxed-frameOFI(whereFIstandsforFixedInertial),alsocalledinertial(orreference)frame,andthebody-frameOABC(whereABCstandsforAircraftBodyCenter)thatisﬁxedintheaircraftcenterofmassandisorientedaccordingtotheaircraftorientation(attitude),seeFig.13.MAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects13ψϕϑezexeyOABCΩ1Ω2Ω3Ω4ZXYOFIFig.13Droneinthebody-frame(OABC)andtheﬁxed-frame(OFI)referencesystems.Forcesexertedfromeachrotor,spindirectionsandpropellervelocities,Ωi,arealsoreported.totheinertialframeanditdependsontheattitudeη=(cid:0)ϕϑψ(cid:1)>(i.e.,Euleranglesroll,pitchandyaw,respectively)thatdescribesthebody-frameorientationaccordingtotheZYXconvention[38].Conversely,therotationaldynamicscanbeexpressedasI˙ωB=−ωB×IωB+τ,(3)where‘×’denotesthevectorproduct,ωB=(cid:0)ωxωyωz(cid:1)>isthevectoroftheangularveloc-ityexpressedinthebody-frame,I=diag(Ix,Iy,Iz)istheinertiamatrixofthevehiclew.r.t.itsprincipalaxis,andτ=(cid:0)uϕuϑuψ(cid:1)>isthecontroltorquevectorobtainedbyactuatingtherotorsspeedsaccordingtotherotorsconﬁgurationandthevehicleshape.Atlowspeedsandaroundthehoveringstatethesimpliﬁeddynamicmodelconsistsofsixsecondorderdifferentialequationsobtainedfrombalancingforcesandmomentaactingonthedrone,wherec•ands•denotecos(•)andsin(•)functions,respectively:Ix¨ϕ=˙ϑ˙ψ(Iy−Iz)+uϕ(4a)Iy¨ϑ=˙ϕ˙ψ(Iz−Ix)+uϑ(4b)Iz¨ψ=˙ϑ˙ϕ(Ix−Iy)+uψ,(4c)m¨x=uT(cid:0)cϕsϑcψ+sϕsψ(cid:1)(5a)m¨y=uT(cid:0)cϕsϑsψ−sϕcψ(cid:1)(5b)m¨z=uTcϑcϕ−mg.(5c)Equations(4)–(5)representthenominalmodelusedfordesigningthecontrollawin[30]andheredescribedinSect.6.2.Howeveramoredetailedmodelshouldbeconsideredwhensimulationhastobeemployedaspartofthecontroldesignprocess.ThusweintroducedfurtherdetailsforcatchingmorerealisticbehaviorswritingthemodelinputsasuT=bf(cid:0)Ω21+Ω22+Ω23+Ω24(cid:1),(6)Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

Mass
Distance to center of gravity
Thrust factor
Drag factor
Inertia component along ex-axis
Inertia component along ey-axis
Inertia component along ez-axis

Sym.
m
l
bf
bm
Ix
Iy
Iz

Value
0.65
0.23
7.5 · 10−7
3.13 · 10−5
7.5 · 10−3
7.5 · 10−3
1.3 · 10−3

Unit
kg
m
kg
kg m
kg m2
kg m2
kg m2

TABLE 1. Drone parameter values for the considered case study.

FIGURE 14. The car (in blue) and the output of the reference generator (in
red) while tracking the target.

A. REFERENCE GENERATOR

The reference generator is decomposed into two parts: the
attitude and the position controller, both illustrated in Fig. 13.
The attitude controller tunes the yaw (ψr) and the pitch
(ϑr) angles trying to overlap the image (uimg, vimg) and
the bounding box (ubb, vbb) centroids (see, Fig. 11), while
the roll (ϕr) angle is computed by the IB controller6. These
values are later used by the position controller to vary the
drone reference position zr and yr, while xr is computed
comparing the detected area areabb with those obtained
when training the classiﬁer arearef

7

The proposed control architecture is based on control loops
that are nothing but Proportional-Integral-Derivative (PID)
controllers. These are a standard solution in the literature for
quad-rotor control design [58]. For the considered case study,
the vehicle starts ﬂying 4 meters over the ground (Z-axis)
with a distance of 15 meters from the car along the X-axis in
the OFVR reference system.

Figure 14 reports the trajectories followed by the car and
the drone when running the simulation, while a further video
has been made available at [59].

B. INTEGRAL BACKSTEPPING CONTROLLER

The integral backstepping of [45], [54] has been used as tra-
jectory controller for the path tracking. It performs robustness
against external disturbances (offered by backstepping) and
sturdiness w.r.t. model uncertainties (given by the integral
action). Starting from the outputs of the reference generator,
the IB controller computes the orientation (ϕref IB and ϑref IB)
that the drone should assume to follow the reference path (xr
and zr). The ϕref IB and ϑref IB reference angles are computed

6All elaborations are expressed in the OFVR reference system.
7This values is obtained as sample of mean of the collected ROIs while

training the classiﬁer (see Sec. III-A).

FIGURE 13. The reference generator scheme referred to virtual reference
= 1 · 10−5,
system (OFVR). The obtained heuristic PID gains are: KPψr
KIψr
KIxr
KIzr

= 1 · 10−3, KPϑr
= 6 · 10−6, KPyr
= 57.5 and KDzr

= 1 · 10−5, KIϑr
= 1 · 10−2, KIyr
= 3.75.

= 1 · 10−3, KPxr
= 1 · 10−2, KPzr

= 1 · 10−6,
= 15,

∈ {

1, 2, 3, 4

where Ωi, i
, are the actual rotors angular
}
velocities expressed in rad s−1, l is the distance from the pro-
pellers to the center of mass, while bf and bm are the thrust
and drag factors, respectively. Further details can be found
in [45], [54], [55]. Table 1 reports the parameters values of
the drone for the considered case study (see Sec. VI-C).

VI. FLIGHT CONTROL SYSTEM
Various state-of-the-art solutions investigate the trajectory
tracking problem with quad-rotors. However, not all of them
are suitable for the speciﬁc case of application [56]. There-
fore, with the aim of illustrating a control design methodol-
ogy exploiting the IBVS approach, it has been considered the
ﬂight control system described in [45] and [57] that uses a
reference generator and an integral backstepping controller
to ﬁgure out the drone trajectory tracking problem. The ref-
erence generator extracts the information from the images to
generate the path to follow, while the Integral Backstepping
(IB) controller uses those references to compute the needed
drone command signals. Figures 13 and 15 describe the
overall control scheme.

8

VOLUME 4, 2016

MAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects15AttitudeReferenceControlPositionReferenceControlPIψrPIDzruimgubbψrefrzr+−+−euψrPIϑrPIyrvimgvbbϑrefryr+−+−evϑrPIxrarearefareabbxr+−ebbFig.14Thereferencegeneratorschemereferredtovirtualreferencesystem(OFVR).TheobtainedheuristicPIDgainsare:KPψr=1·10−5,KIψr=1·10−3,KPϑr=1·10−5,KIϑr=1·10−3,KPxr=1·10−6,KIxr=6·10−6,KPyr=1·10−2,KIyr=1·10−2,KPzr=15,KIzr=57.5andKDzr=3.75.Whereas,thevaluesψrefrandϑrefr4(see,Fig.14)representthereferencesattitudethatthevehicleshouldassumeduringthetargettracking.Finally,theerrorsignalebb,obtainedasthedifferencebetweentheboundingbox(wbb·hbb,akaareabbinFig.14)andthereference5areas(arearef),isusedtotunethedistancexr.Figure15reportsthetrajectoriesfollowedbythecarandthedroneduringthesimulation,whileafurthervideohasbeenmadeavailableathttps://youtu.be/qAtndBIwdasforshowingtheresultsoftheproposedapproach.Insuchvideo,thequadrotordynamicshasbeenneglectedtohighlighthowthereferencegeneratorworks.6.2IntegralBacksteppingcontrollerTheintegralbacksteppingof[30]hasbeenusedasthetrajectorycontrollerforthepathtracking.Wedecidedtoemploythiscontrollerinparticularforeducationalpurposes:itwasamongthepapersthatﬁrstdescribedhowtodesignamulti-rotorsaircraftcontroller,espe-ciallyforaquadrotor.Moreover,thechosenintegralbacksteppingcontrollerperformsro-bustnessagainstexternaldisturbances(offeredbybackstepping)andsturdinessw.r.t.modeluncertainties(givenbytheintegralaction).Startingfromthereferencegenerator’soutputs,theIBcontrollercomputestheorienta-tion(ϕrefIBandϑrefIB)thatthedroneshouldassumetofollowthereferencepath(xrandzr).4Inourcasestudy,wesupposedthatthedroneshouldmaintainaﬂatorientationasmuchaspossibleduringthetracking.Thereforetheanglesψrefrandϑrefrhavebeensetequaltozero.5ThereferencevalueisgivenbythesamplemeanofROIscollectedduringthelearningprocess.16GiuseppeSilano,LuigiIannelli−50510152001002003004005000246−xFVR[m]zFVR[m]yFVR[m]Fig.15Thecar(inblue)andthereferencepath(inred)describedduringthesimulationwhenneglectingthedronedynamics.bustnessagainstexternaldisturbances(offeredbybackstepping)andsturdinessw.r.t.modeluncertainties(givenbytheintegralaction).Startingfromthereferencegenerator’soutputs,theIBcontrollercomputestheorienta-tion(ϕrefIBandθrefIB)thatthedroneshouldassumetofollowthereferencepath(xrandzr).TheϕrefIBandθrefIBreferenceanglesarecomputedas:θrefIB=muT"(cid:0)1−c21+λ1(cid:1)ex+(c1+c2)exIB−c1λ1Zt0ex(τ)dτ#(8a)ϕrefIB=−muT"(cid:0)1−c23+λ2(cid:1)ez+(c3+c4)ezIB−c3λ2Zt0ez(τ)dτ#,(8b)withexIB(t)=λ1Zt0ex(τ)dτ+c1ex(t)+˙ex(t)(9a)ezIB(t)=λ2Zt0ez(τ)dτ+c3ez(t)+˙ez(t),(9b)andex=xr−xd(10a)ez=zr−zd,(10b)where(c1,c2,c3,c4,λ1andλ2)arepositiveconstants.Fortheconsideredmotivatingex-amples,thefollowingvalueshavebeenchosen:λ1=0.025,λ2=0.025,c1=2,c2=0.5,c3=2andc4=0.5.Figure16showstheschemedescribingasthecontrolsystemworks.6.3NumericalresultsTheoverallsystemhasbeensimulatedinMatlabandtheresultsillustrateinadirectwayhowthesystemperforms(thevideoisavailableathttps://youtu.be/b8mTHRkRDmA).InSilano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

C. NUMERICAL RESULTS
To prove the validity and effectiveness of the proposed frame-
work, numerical simulations have been carried out by using
the 2015b release of Matlab equipped with CVS and VR
toolboxes8. The video available at [60] illustrates in a direct
way how the system works, i.e., the ability of the quad-rotor
to follow the car that moves along the nontrivial path. In
addition, the video shows the behavior of the detection and
tracking algorithms that never lose the target while tracking
the target. Moreover, the video shows the capabilities of the
control system in reacting to changes in the car’s dynamics:
during the double lane change maneuver the vehicle suddenly
increases its speed and the aircraft tilts around the X-axis
(the car seems to climb a hill9) to capture the shift in the
dynamics.

A further scenario (the video is available at [61]) was
considered to show how the simulation can be easily cus-
tomized without the need to redesign the entire system. The
numerical example aims to show how the performance of
the detection and tracking algorithms can be easily evaluated
while running alongside the drone tracking controller. Two
cars are considered for the case of interest. One (red car)
engages the ESP control while the other (yellow) switches
off such control unit when changing the lane. As it can be
seen from the video, the search window adapts its sizes in
response to partial occlusions of the target.

Finally, the video at [62] shows the advantages of using a
modular architecture for the platform. The video shows how
the whole system architecture can be tested under various
conditions simply changing the scenario10. As shown in the
video, halfway through the simulation (26 s) increasing the
speed of the car causes the drone to tilt excessively moving
to instability.

The proposed scenarios demonstrate as the software plat-
form allows to test the complex system while interacting
with the surrounding environment and computer vision and
control algorithms are in the loop.

VII. CONCLUSIONS
In this paper, a numerical simulation platform for multi-rotor
aircraft based on Matlab and the MathWorks Virtual Reality
and Computer Vision System toolboxes has been described.
The platform makes easy to implement and to simulate
complex scenarios where computer vision algorithms can be
run and tested together with drone tracking controllers. The
simulator provides a ready-to-use environment allowing stu-
dents, researchers, and developers to easily test and evaluate
their own algorithms. The platform also constitutes the ﬁrst
step towards the development of a more structured software
tool where exploiting the advantages of software-in-the-loop
simulations. The software has been released as open-source3
making it possible to go through any part of the system.

8The simulator is fully compatible with each further release of Matlab.
9The drone ﬂies in an eye-in-hand conﬁguration, i.e., tilts around the axis

direct affects the camera orientation.

10The vr_octavia scenario was considered.

FIGURE 15. The drone trajectory controller. All variables are expressed in the
virtual reference system (OFVR). Here we recall the heuristic control gains
employed into the simulation scenario: KPyatt
= 200,
KPϕatt
and KDψatt

= 8, KDϕatt
= 4.

= 1000, KDyatt

= 12, KDϑatt

= 4, KPψatt

= 4, KPϑatt

= 10,

as:

ϑref IB =

m
uT (cid:34)

c2
1 + λ1

1

−

ex + (c1 + c2) exIB+ (8a)

(cid:0)
c1λ1

t

(cid:1)
ex(τ )dτ

,

(cid:35)

0
(cid:90)

−

c2
3 + λ2

ez + (c3 + c4) ezIB + (9a)

1

m
uT (cid:34)
(cid:0)

−

t

ϕref IB =

−

−

c3λ2

ez(τ )dτ

(cid:1)
,

(cid:35)

0
(cid:90)

t

0
(cid:90)

t

0
(cid:90)

with

and

exIB(t) = λ1

ezIB(t) = λ2

ex(τ )dτ + c1ex(t) + ˙ex(t),

(10a)

ez(τ )dτ + c3ez(t) + ˙ez(t),

(10b)

ex = xr
ez = zr

xd,
zd.

−

−

(11a)

(11b)

For the considered motivating examples, the following values
have been chosen: λ1 = 0.025, λ2 = 0.025, c1 = 2, c2 =
0.5, c3 = 2 and c4 = 0.5. Figure 15 shows the overall control
system architecture.

VOLUME 4, 2016

9

MAT-Fly:aneducationalplatformforsimulatingUAVsaimedtodetectandtrackmovingobjects17PDϑattϑdeϑϑrefIB+−PDϕattϕdeϕϕrefIB+−PDψattψdeψψr+−PDyattydeyyr+−DRONEDYNAMYCSuϑuϕuψuTϑdϕdψdydFig.16Thedronetrajectorycontroller.Allvariablesareexpressedinthevirtualreferencesystem(OFVR).Herewerecalltheheuristiccontrolgainsemployedintothesimulationscenario:KPyatt=1000,KDyatt=200,KPϕatt=8,KDϕatt=4,KPϑatt=12,KDϑatt=4,KPψatt=10andKDψatt=4.dynamicchange.Also,afterafewsecondsofsimulation,thequadrotortiltsaroundthex-axisduetotheincreasingoftherollangle(dronesﬂiesinaneye-in-handconﬁguration)andthecarseemstofaceaclimb.Afurtherscenario(thevideoisavailableathttps://youtu.be/RjXBtPqZZBc)hasbeenconsideredtoprovetheeffectivenessandtherobustnessoftheproposedapproachaswellastheeasinesswithwhichthesoftwareplatformcanbecustomizedaddingmorethanonevehicleintothevirtualenvironment.Aswecansee,thedetectionandtrackingalgorithmsareabletodetectandtrackthecaralongthepathevenifanothervehiclewithadifferentcolor(yellow,fortheconsideredexample)isinvolvedinthesimulation.Inpartic-ular,thetrackerisabletoresizethesearchwindowduringtheexperimentavoidingtolosethetargetuntilthesimulationstops.Finally,thevideoathttps://youtu.be/m43Zadq-6XMshowshowthemodularapproachusedindevelopingthesoftwareplatformmakeseasytochangetheworldsce-narioinafewstepswithouttheneedtoredesigntheoverallarchitecture.Fortheconsideredexample,thevroctavia2carsexamplehasbeenreplacedwithvroctaviaonekeepingev-erythingelseunchanged.Inthemiddleofthesimulation(at26s)thecarspeedbecomesmuchhigherthanthedronespeedcausinganexcessiveUAVrolling.Duetotheeye-in-handconﬁguration,thetargetcomesoutofthecameraviewanditislost.Thoseresultsdemonstratedasthesystemworksandthelimitoftheeye-in-handconﬁgu-ration,aswell.Anyhow,thesoftwareplatformallowedtotestthecomplexsystemcomposedbycomputervisionandcontrolalgorithmsinteractingamongthemandwiththemovingob-jectsdynamics.Theexperimentsaswellasthesoftwareplatformweredevelopedwiththe2015breleaseofMatlab,equippedwithComputerVisionSystemandVirtualRealitytoolboxes,butitiscompatiblewithanyMatlabsuccessiverelease.Thecodeisspeciﬁcfortheusecasestudy,Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

Future work includes the integration of the platform with
more advanced robotics middleware and the creation of the
interface with the hardware moving toward hardware-in-the-
loop tests.

REFERENCES

[1] D. Scaramuzza, M. C. Achtelik, L. Doitsidis, F. Friedrich, E. Kosmatopou-
los, A. Martinelli, M. W. Achtelik, M. Chli, S. Chatzichristoﬁs, L. Kneip,
D. Gurdan, L. Heng, G. H. Lee, S. Lynen, M. Pollefeys, A. Renzaglia,
R. Siegwart, J. C. Stumpf, P. Tanskanen, C. Troiani, S. Weiss, and
L. Meier, “Vision-Controlled Micro Flying Robots: From System Design
to Autonomous Navigation and Mapping in GPS-Denied Environments,”
IEEE Robotics Automation Magazine, vol. 21, no. 3, pp. 26–40, 2014.
[2] S. Choi and E. Kim, “Image acquisition system for construction inspection
based on small unmanned aerial vehicle,” Lecture Notes in Electrical
Engineering, vol. 352, pp. 273–280, 2015.

[3] F. Fraundorfer, L. Heng, D. Honegger, G. H. Lee, L. Meier, P. Tanskanen,
and M. Pollefeys, “Vision-based autonomous mapping and exploration
using a quadrotor,” in IEEE International Conference on Intelligent Robots
and Systems, 2012, pp. 4557–4564.

[4] D. Anthony, S. Elbaum, A. Lorenz, and C. Detweiler, “On crop height
estimation with UAVs,” in IEEE International Conference on Intelligent
Robots and Systems, 2014, pp. 4805–4812.

[5] M. Blosch, S. Weiss, D. Scaramuzza, and R. Siegwart, “Vision based
MAV navigation in unknown and unstructured environments,” in IEEE
International Conference on Robotics and Automation, 2010, pp. 21–28.

[6] D. Ferreira de Castro and D. A. dos Santos, “A Software-in-the-Loop Sim-
ulation Scheme for Position Formation Flight of Multicopters,” Journal of
Aerospace Technology and Management, vol. 8, no. 4, pp. 431–440, 2016.
[7] M. Mancini, G. Costante, P. Valigi, T. A. Ciarfuglia, J. Delmerico,
and D. Scaramuzza, “Toward Domain Independence for Learning-Based
Monocular Depth Estimation,” IEEE Robotics and Automation Letters,
vol. 2, no. 3, pp. 1778–1785, 2017.

[8] A. Tallavajhula and A. Kelly, “Construction and validation of a high delity
simulator for a planar range sensor,” in IEEE Conference on Robotics and
Automation, 2015, pp. 1050–4729.

[9] R. Dianko and J. Kuffner, “Openrave: A planning architecture for au-
tonomous robotics,” Robotics Institute, Pittsburgh, PA, Tech. Rep. 79,
2008.

[10] G. Silano and L. Iannelli, “CrazyS: A Software-in-the-Loop Simulation
Platform for the Crazyﬂie 2.0 Nano-Quadcopter,” in Robot Operating
System (ROS): The Complete Reference (Volume 4), Koubaa, Anis, Ed.
Springer International Publishing, 2020, pp. 81–115.

[11] S. Sinha, N. K. Goyal, and R. Mall, “Reliability and availability prediction
of embedded systems based on environment modeling and simulation,”
Simulation Modelling Practice and Theory, vol. 108, pp. 1–26, 2021.
[12] G. Silano, P. Oppido, and L. Iannelli, “Software-in-the-loop simulation
for improving ﬂight control system design: a quadrotor case study,” in
IEEE International Conference on Systems, Man and Cybernetics, 2019,
pp. 466–471.

[13] A. Elkady and T. Sobh, “Robotics Middleware: A Comprehensive Litera-
ture Survey and Attribute-Based Bibliography,” Journal of Robotics, 2012.
[14] N. Koenig and A. Howard, “Design and use paradigms for Gazebo, an
open-source multi-robot simulator,” in Proceedings of the IEEE Interna-
tional Conference on Intelligent Robots and Systems, vol. 3, 2004, pp.
2149–2154.

[15] E. Rohmer, S. P. N. Singh, and M. Freese, “V-REP: a Versatile and Scal-
able Robot Simulation Framework,” in Proceedings of The International
Conference on Intelligent Robots and Systems, 2013, pp. 1321–1326.
[16] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “AirSim: High-Fidelity
Visual and Physical Simulation for Autonomous Vehicles,” in Field and
Service Robotics, M. Hutter and R. Siegwart, Eds. Springer International
Publishing, 2018, pp. 621–635.

[17] G. Echeverria, N. Lassabe, A. Degroote, and S. Lemaignan, “Modular
open robots simulation engine: MORSE,” in IEEE International Confer-
ence on Robotics and Automation, 2011, pp. 46–51.

[18] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler,
and A. Y. Ng, “ROS: an open-source Robot Operating System,” in Pro-
ceedings ICRA Workshop Open Source Software, 2009, pp. 1–6.

[19] G. Metta, P. Fitzpatrick, and L. Natale, “YARP: Yet another robot plat-
form,” International Journal on Advanced Robotics Systems, vol. 3, no. 1,
pp. 43–48, 2006.

[20] A. Mallet, S. Fleury, and H. Bruyninckx, “A speciﬁcation of generic
robotics software components: future evolutions of GenoM in the Orocos
context,” in IEEE International Conference on Intelligent Robots and
Systems, 2002, pp. 2292–2297.

[21] S. Khan, M. H. Jaffery, A. Hanif, and M. R. Asif, “Teaching Tool for a
Control Systems Laboratory Using a Quadrotor as a Plant in MATLAB,”
IEEE Transactions on Education, vol. 60, no. 99, pp. 1–8, 2017.

[22] M. A. Day, M. R. Clement, J. D. Russo, D. Davis, and T. H. Chung,
“Multi-uav software systems and simulation architecture,” in International
Conference on Unmanned Aircraft Systems, 2015, pp. 426–435.

[23] H. Shokry and M. Hinchey, “Model-Based Veriﬁcation of Embedded

Software,” Computer, vol. 42, no. 4, pp. 53–59, 2009.

[24] J. Unicomb, L. Dantanarayana, J. Arukgoda, R. Ranasinghe, G. Dis-
sanayake, and T. Furukawa, “Distance function based 6DOF localization
for unmanned aerial vehicles in GPS denied environments,” in IEEE
International Conference on Intelligent Robots and Systems, 2017, pp.
5292–5297.

[25] M. Ryll, G. Muscio, F. Pierri, E. Cataldi, G. Antonelli, F. Caccavale, and
A. Franchi, “6D physical interaction with a fully actuated aerial robot,”
in IEEE International Conference on Robotics and Automation, 2017, pp.
5190–5195.

[26] T. Lee, M. Leok, and N. H. McClamroch, “Geometric tracking control of
a quadrotor UAV on SE(3),” in 49th IEEE Conference on Decision and
Control, 2010, pp. 5420–5425.

[27] M. Faessler, A. Franchi, and D. Scaramuzza, “Differential Flatness of
Quadrotor Dynamics Subject to Rotor Drag for Accurate Tracking of
High-Speed Trajectories,” IEEE Robotics and Automation Letters, vol. 3,
no. 2, pp. 620–626, 2018.

[28] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look
Once: Uniﬁed, Real-Time Object Detection,” in IEEE Conference on
Computer Vision and Pattern Recognition, 2016, pp. 779–788.

[29] L. Jiao, F. Zhang, F. Liu, S. Yang, L. Li, Z. Feng, and R. Qu, “A Survey
of Deep Learning-Based Object Detection,” IEEE Access, vol. 7, pp.
128 837–128 868, 2019.

[30] R. Girshick, “Fast R-CNN,” in IEEE International Conference on Com-

puter Vision, 2015, pp. 1440–1448.

[31] T. Baca, M. Petrlik, M. Vrba, V. Spurny, R. Penicka, D. Hert,
and M. Saska, “The MRS UAV System: Pushing the Frontiers of
Reproducible Research, Real-world Deployment, and Education with
Autonomous Unmanned Aerial Vehicles,” 2020. [Online]. Available:
https://arxiv.org/pdf/2008.08050

[32] E. Pignaton de Freitas, L. A. L. F. da Costa, C. Felipe Emygdio de
Melo, M. Basso, M. Rodrigues Vizzotto, M. Schein Cavalheiro Corrêa,
and T. Dapper e Silva, “Design, Implementation and Validation of a
Multipurpose Localization Service for Cooperative Multi-UAV Systems,”
in 2020 International Conference on Unmanned Aircraft Systems, 2020,
pp. 295–302.

[33] J. L. Sanchez-Lopez, M. Molina, H. Bavle, C. Sampedro, R. A. Suarez
Fernandez, and P. Campoy, “A Multi-Layered Component-Based Ap-
proach for the Development of Aerial Robotic Systems: The Aerostack
Framework,” Journal of Intelligent & Robotic Systems, vol. 88, no. 2, pp.
683–709, 2017.

[34] H. Lim, J. Park, D. Lee, and H. J. Kim, “Build Your Own Quadrotor:
Open-Source Projects on Unmanned Aerial Vehicles,” IEEE Robotics
Automation Magazine, vol. 19, no. 3, pp. 33–45, 2012.

[35] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, “RotorS – A Modular
Gazebo MAV Simulator Framework,” in Robot Operating System (ROS):
The Complete Reference (Volume 1), A. Koubaa, Ed. Springer Interna-
tional Publishing, 2016, pp. 595–625.

[36] F. Chaumette and S. Hutchinson, “Visual servo control. I. Basic ap-
proaches,” IEEE Robotics & Automation Magazine, vol. 13, no. 4, pp.
82–90, 2006.

[37] ——, “Visual servo control. II. Advanced approaches [Tutorial],” IEEE
Robotics & Automation Magazine, vol. 14, no. 1, pp. 109–118, 2007.
[38] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo, Robotics: Modelling,

Planning and Control. Springer-Verlag, 2009, ISBN: 978-1846286414.

[39] V. Lippiello, B. Siciliano, and L. Villani, “Eye-in-Hand/Eye-to-Hand
Multi-Camera Visual Servoing,” in Proceedings of the 44th IEEE Con-
ference on Decision and Control, 2005, pp. 5354–5359.

[40] G. Silano and L. Iannelli, “An educational simulation platform for GPS-
denied Unmanned Aerial Vehicles aimed to the detection and tracking of
moving objects,” in IEEE Conference on Control Applications, 2016, pp.
1018–1023.

10

VOLUME 4, 2016

Silano et al.: MAT-Fly: an educational platform for simulating Unmanned Aerial Vehicles aimed to detect and track moving objects

GIUSEPPE SILANO (S’16) received the bach-
elor and master degrees in computer (2012) and
electronic engineering (2016), respectively, and
the PhD degree in information engineering (2020)
from the University of Sannio, Italy. From June
2020, he is with the Czech Technical University
in Prague where he holds a post-doctoral research
fellowship position. From March to November
2019, he was a visiting student at LAAS–CNRS,
in Toulouse, France. His research interests are in
simulation and control, temporal logic, model predictive control, software-
in-the-loop, and planning for micro aerial vehicles. He was among the ﬁnal-
ists of the “Aerial robotics control and perception challenge”, the Industrial
Challenge of the 26th Mediterranean Conference on Control and Automation
(MED’18), and among the participants of the LAAS Team selected as a
ﬁnalist of the Mohammed Bin Zayed Robotics Competition (MBZIRC)
2020. He is a member of the IEEE Control System Society (CSS) and IEEE
Robotics and Automation Society (RAS).

LUIGI IANNELLI (S’00-M’02-SM’12) received
the master’s degree (Laurea) in computer engi-
neering from the University of Sannio, Italy, in
1999 and the PhD degree in information engi-
neering from the University of Napoli Federico
II, Italy, in 2003. After a postdoc position with
the University of Napoli Federico II, he joined
the University of Sannio as an assistant professor,
and since 2016 he has been an associate professor
of automatic control. He held visiting researcher
positions in the Royal Institute of Technology, Sweden, and at the Johann
Bernoulli Institute of Mathematics and Computer Science, University of
Groningen, The Netherlands. His current research interests include analy-
sis and control of switched and nonsmooth systems, stability analysis of
piecewise-linear systems, smart grid control, and applications of control
theory to power electronics and UAVs. He was co-editor of the book
Dynamics and Control of Switched Electronic Systems (Springer, 2012). He
is a senior member of the IEEE.

[41] G. R. Bradski, “Computer vision face tracking for use in a perceptual user

interface,” Intel Technology Journal, 2nd Quarter 1998.

[42] E. Rosten and T. Drummond, “Machine Learning for High-Speed Corner
Detection,” in 9th European Conference on Computer Vision, L. Aleš,
B. Horst, and A. Pinz, Eds. Springer Berlin Heidelberg, 2006, pp. 430–
443.

[43] S. Arefnezhad, A. Ghaffari, A. Khodayari, and S. Nosoudi, “Modeling
of Double Lane Change Maneuver of Vehicles,” International Journal of
Automotive Technology, vol. 19, pp. 271–279, 2018.
“IMU

Simulink,”
Sensor
2020. [Online]. Available: https://www.mathworks.com/help/fusion/ug/
imu-sensor-fusion-with-simulink.html

[44] MathWorks

Fusion

with

Inc.,

[45] S. Bouabdallah, P. Murrieri, and R. Siegwart, “Towards Autonomous
Indoor Micro VTOL,” Autonomous Robots, vol. 18, no. 2, pp. 171–183,
2005.

[46] P. Corke, Robotics, Vision and Control: Fundamental Algorithms in MAT-

LAB. Springer, 2011, ISBN 978-3-319-54413-7.

[47] The MathWorks Inc., “Virtual Reality Toolbox: For use with MATLAB
and Simulink,” MathWorks ofﬁcial website. [Online]. Available: http:
//cda.psych.uiuc.edu/matlab_pdf/vr.pdf

[48] G. Silano, “Frames acquisition video,” YouTube. [Online]. Available:

https://youtu.be/A70zed84zv0

[49] A. dos Anjos and H. R. Shahbazkia, “BI-LEVEL IMAGE THRESHOLD-
ING - A Fast Method,” in Proceedings of the First International Confer-
ence on Bio-inspired Systems and Signal Processing, vol. 2. SciTePress,
2008, pp. 70–76.

[50] P. Viola and M. Jones, “Rapid object detection using a boosted cascade
of simple features,” in Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition, vol. 1, 2001, pp.
511–518.

[51] Y. Xu, G. Yu, X. Wu, Y. Wang, and Y. Ma, “An Enhanced Viola-Jones
Vehicle Detection Method From Unmanned Aerial Vehicles Imagery,”
IEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 7,
pp. 1845–1856, 2017.

[52] P. Martinez-Gonzalez, S. Oprea, A. Garcia-Garcia, A. Jover-Alvarez,
S. Orts-Escolano, and J. Garcia-Rodriguez, “UnrealROX: an extremely
photorealistic virtual reality environment for robotics simulations and
synthetic data generation,” Virtual Reality, 2019.

[53] N. M. Artner and W. Burger, “A Comparison of Mean Shift Tracking
Methods,” in 2017 International Conference on Unmanned Aircraft Sys-
tems, 2008, pp. 197–204.

[54] S. Bouabdallah and R. Siegwart, “Backstepping and Sliding-mode Tech-
niques Applied to an Indoor Micro Quadrotor,” in Proceedings of the 2005
IEEE International Conference on Robotics and Automation, 2005, pp.
2247–2252.

[55] B. L. Stevens, F. L. Lewis, and E. N. Johnson, Aircraft control and
simulation: dynamics, controls design, and autonomous systems.
John
Wiley & Sons, 2015, ISBN: 978-1-118-87098-3.

[56] T. P. Nascimento and M. Saska, “Position and attitude control of multi-
rotor aerial vehicles: A survey,” Annual Reviews in Control, vol. 48, pp.
129–146, 2019.

[57] J. Pestana, J. L. Sanchez-Lopez, S. Saripalli, and P. Campoy, “Com-
puter vision based general object following for GPS-denied multirotor
unmanned vehicles,” in IEEE American Control Conference, 2014, pp.
1886–1891.

[58] T. N. Dief and S. Yoshida, “Review: Modeling and Classical Controller Of
Quad-rotor,” International Journal of Computer Science and Information
Technology & Security, vol. 5, no. 4, pp. 314–319, 2015.

[59] G. Silano, “Vision-based target tracking scenario. The drone dynamics
and the control system are neglected to evaluate the performance
of
[Online]. Available: https:
//youtu.be/qAtndBIwdas
[60] ——, “Vision-based target

the reference generator,” YouTube.

tracking scenario,” YouTube.

[Online].

Available: https://youtu.be/b8mTHRkRDmA

[61] ——, “Vision-based target tracking scenario when the target is partially

covered,” YouTube. [Online]. Available: https://youtu.be/RjXBtPqZZBc

[62] ——, “Quad-rotor

that moves along a nontrivial
path in the vr_octavia scenario,” YouTube. [Online]. Available: https:
//youtu.be/m43Zadq-6XM

following a car

VOLUME 4, 2016

11

