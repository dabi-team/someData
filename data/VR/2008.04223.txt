Integrating Variable Reduction Strategy with
Evolutionary Algorithm for Solving Nonlinear
Equations Systems

Aijuan Song, Guohua Wu, and Witold Pedrycz, Fellow, IEEE

1

0
2
0
2

l
u
J

3
1

]
E
N
.
s
c
[

1
v
3
2
2
4
0
.
8
0
0
2
:
v
i
X
r
a

Abstract—Nonlinear equations systems (NESs) are widely used
in real-world problems while they are also difﬁcult to solve
due to their characteristics of nonlinearity and multiple roots.
Evolutionary algorithm (EA) is one of the methods for solving
NESs, given their global search capability and an ability to locate
multiple roots of a NES simultaneously within one run. Currently,
the majority of research on using EAs to solve NESs focuses on
transformation techniques and improving the performance of the
used EAs. By contrast, the problem domain knowledge of NESs
is particularly investigated in this study, using which we propose
to incorporate the variable reduction strategy (VRS) into EAs to
solve NESs. VRS makes full use of the systems of expressing a
NES and uses some variables (i.e., core variable) to represent
other variables (i.e., reduced variables) through the variable
relationships existing in the equation systems. It enables to reduce
partial variables and equations and shrink the decision space,
thereby reducing the complexity of the problem and improving
the search efﬁciency of the EAs. To test the effectiveness of
VRS in dealing with NESs, this paper integrates VRS into
two existing state-of-the-art EA methods (i.e., MONES and DR-
JADE), respectively. Experimental results show that, with the
assistance of VRS, the EA methods can signiﬁcantly produce
better results than the original methods and other compared
methods.

Index Terms—Nonlinear equations systems, Evolutionary algo-
rithm, Transformation techniques, Problem domain knowledge,
Variable reduction strategy.

I. INTRODUCTION

N ONLINEAR equations systems (NESs) have emerged

in ﬁelds of science, engineering, economics, etc [1].
Numerous real-world problems can be modeled as NESs such
as chemistry [2], robotics [3], electronic [4], signal processing
[5], and physics [6]. Unlike linear equations systems, NESs
ln x, xn and
have plenty of nonlinear operators like ax,
trigonometric function. Furthermore, the overwhelming major-
ity of NESs have more than a single equally important root.
The above two characteristics make the problem challenging.
There is a class of methods for ﬁnding the numerical
solutions of NESs. This kind of method mainly includes New-
ton methods [7], quasi-Newton methods [7], interval-based

Aijuan Song and Guohua Wu are with the School of Trafﬁc and Transporta-
tion Engineering, Central South University, Changsha 410075, China. E-mail:
aijuansong@csu.edu.cn; guohuawu@csu.edu.cn.

Witold Pedrycz is with the Department of Electrical and Computer Engi-
neering, University of Alberta, Edmonton, AB T6G 2V4, Canada, with the
Department of Electrical and Computer Engineering, Faculty of Engineering,
King Abdulaziz University, Jeddah 21589, Saudi Arabia, and also with the
Systems Research Institute, Polish Academy of Sciences, Warsaw 01447,
Poland. E-mail: wpedrycz@ualberta.ca.

methods, e.g.,
interval-Newton [8], homotopy continuation
(embedding) methods [8], [9], trust-region method [9], secant
method [10], Halley method [11], branch and bound method
[11], etc.. Nevertheless, a host of these methods have the
following weaknesses: a) Aiming at locating just a single
optimal solution rather than multiple optimal solutions in a
single run; b) High requirements for prior knowledge, such
as derivative, good starting value; c) The quality of solutions
is strongly problem-dependent and depends on the initial
guess. Therefore, such methods are no longer completely
sufﬁcient. Over the past decade, people have also developed
metaheuristics to solve NESs while continuously improving
these methods to study the numerical solution of NESs.

The metaheuristics possess advantages such as simple
implementation, strong versatility and strong global search
capability [12]. In comparison with the methods to study
the numerical solution of NESs, it enjoys unique beneﬁts
especially for solving complex problems such as less prior
knowledge required, no derivative requirement, and less de-
pendence on problem characteristics and the initial solution.
However, metaheuristics also face quite a few challenges when
solving NESs, for instance, when dealing with large-scale and
high-dimensional NESs and ﬁnding multiple roots in a single
run.

Evolutionary algorithm (EA) is essentially a global stochas-
tic search algorithm [12] and demonstrates the capacity for
locating multiple solutions over a single run. EAs are prevalent
and effective methods for solving NESs. A NES needs to
be transformed into an optimization problem prior to the
solving process by EAs. The transformed problem is generally
a multi-modal or multi-objective optimization problem. At
present, EAs that have been applied to solve NESs comprise
evolutionary strategy (ES), particle swarm optimization algo-
rithm (PSO), differential evolution algorithm (DE), genetic
algorithm (GA), etc.. For example, Tong et al. proposed a
ranking method in ES for solving NESs [13]. Ouyang et al.
developed a hybrid PSO [14], which solves NESs by combin-
ing PSO with the Nelder-Mead simplex method. Turgut et al.
designed a chaotic behavior PSO to solve NESs [15], which
improves the robustness and effectiveness of the algorithm
through different chaotic maps. Gong et al. argued that locating
multiple roots by repulsion techniques is a promising method
and they proposed a repulsion-based adaptive DE (RADE) for
solving NESs [16]. Ren et al. developed an efﬁcient GA with
symmetric and harmonious individuals for solving NESs [17]
and Joshi et al. used an improved GA to solve NESs [18].

 
 
 
 
 
 
Thereby, obtaining multiple optimization solutions for the
optimization problem by EAs corresponds to getting multiple
roots of the NES.

To solve NESs by EAs effectively, we can pay attention to
the transformation technique, the algorithm and the problem
itself, i.e., designing more reliable transformation techniques,
designing more efﬁcient EAs and reducing the complexity of
the problem. Some research shows that effectively integrating
an algorithm with problem domain knowledge can generally
improve the performance of the algorithm [19]. Unfortunately,
at present, for solving NES the main research dedicates to
transformation techniques and the performance of EAs while
lacking relevant research on the complexity reduction of
problems.

VRS can make full use of the problem domain knowledge
to reduce problems and trigger the complexity reduction
of problems. Currently, VRS has been applied to equality
constrained optimization problems [20] and derivative un-
constrained optimization problems [21], which have signiﬁ-
cantly improved the optimization efﬁciency. Therefore, it is
of considerable signiﬁcance to study how to apply VRS to
solve NESs effectively. Based on the above considerations,
we propose to integrate VRS with EAs for solving NESs.
VRS represents some variables with other variables through
the relationships among variables in the equations, resulting
in reducing the complexity of the problems and improving the
search efﬁciency of the algorithm. The main contributions of
this paper can be summarized as follows:

• We propose to utilize VRS to reduce the complexity of
NESs. We elaborately analyze and explain how to apply
VRS to simplify a NES. With the assistance of VRS,
a NES can entail the smaller decision space and lower
complexity.

• A general framework is proposed for integrating VRS
with an arbitrary EA for solving NESs. By this frame-
work, the optimization efﬁciency of the used EAs can be
signiﬁcantly improved when solving NESs. To evaluate
the proposed methods, we speciﬁcally integrate VRS with
two state-of-the-art algorithms, which refer to DR-JADE
(dynamic repulsion-based adaptive DE with optional
external archive) [22] and MONES (the method that
transforms NES to a bi-objective optimization problem)
[23], respectively. Moreover, extensive experiments on
two test suites, which respectively include 7 and 46 NESs,
are conducted. Experimental results show that with the
assistance of VRS, the methods can obtain better perfor-
mance than the original methods, thus demonstrating the
effectiveness of VRS for solving NESs.

future research directions.

II. PROBLEM DESCRIPTION AND RELATED WORK

A. Nonlinear equations systems

A NES can be formulated as:

f (~x) =

f1 (~x)
f2 (~x)
...
fm (~x)








,








2

(1)

where f1 (~x) ,
, fm (~x) indicates that the NES has m func-
tions, and at least one function is non-linear. ~x is a decision
vector containing n decision variables:

· · ·

~x = (x1, x2,

, xn)

S

Rn,

(2)

· · ·

∈
Rn is the decision space deﬁned by the parametric
where S
constraints of the decision variables and it is a compact set that
denotes the feasible region of the search space. The decision
space can be described as:

⊆

⊆

n

S =

xj, xj

,

Yj=1 (cid:2)

(cid:3)

(3)

where j = 1, . . . , n, xj and xj are the lower bound and upper
bound of xj.

Solving the NES shown in (1) is to ﬁnd a series of optimiza-
tion solutions in the decision space, where each optimization
solution ~x∗

S satisﬁes the following relationships:

∈

f1(~x∗) = 0
f2(~x∗) = 0
...
fm(~x∗) = 0

(4)





Most of NESs have more than a single root. For instance,
Fig. 1 depicts a NES problem with two nonlinear equations
and two decision variables, and the expression is:

(cid:26)

where xi

4x3
4x3

1 + 4x1x2 + 2x2
2 + 2x2
1 + 4x1x2

2 −
−

42x1
26x2

−
−

14 = 0
22 = 0

(a)
(b)

,

(5)

[

5, 5], i = 1, 2.

∈

−

As can be seen from Fig. 1, the NES shown in Eq. (5)
has nine roots. Each root could be equally crucial, so one of
the major tasks for solving NESs is locating multiple roots.
Besides, improving the quality of the found solutions is also
crucial. The quality of the solutions refers to how close the
solutions obtained by an algorithm is to the real solutions of
a NES.

The paper is organized as follows. Section II describes
NESs and brieﬂy reviews two transformation techniques of
NESs. In Section III, the core idea and the reduction process
of VRS are described. Then, we respectively integrate VRS
with MONES and DR-JADE after presenting the framework
of integrating VRS and EAs. Section IV selects two test suites
to reduce and the relevant experiments are designed to study
the superiority of the methods with VRS compared with other
methods. Section V concludes this paper and brieﬂy explores

B. Transformation techniques

NESs are generally transformed into optimization problems
to efﬁciently develop the roots, which possesses several ad-
vantages, such as low dependence on problem characteris-
tics, locating multiple solutions in a single run. Currently,
popular transformation techniques can be roughly classi-
ﬁed into three categories:
i) single-objective optimization-
based transformation techniques [22], [24], [25], ii) multi-
objective optimization-based transformation techniques [23],

2

x

5

4

3

2

1

0

-1

-2

-3

-4

-5

(x)

f

1

f

(x)

2
Root

area. The γ of the t-th generation in this work is set to:

where

γt = γmin + λt(γmax

γmin),

−

γmin = 0.01

n
min
i=1

(xi −

×

xi),

3

(10)

(11)

(12)

(13)

γmax = 0.5

×

λt = (1

xi),

n
min
i=1

(xi −
t
tmax

)2,

−
where t is the current iteration counter. tmax is the maximal
number of iterations. DREA can be classiﬁed as a multiplica-
tive repulsion technique.

MONES transforms a NES into a bi-objective optimization
problem to locate multiple roots of NESs. The transformed
bi-objective optimization problem consists of two parts: the
location function and the system function. The location func-
tion can be formulated as follows:

minmize α1(~x) = x1
minmize α2(~x) = 1

(cid:26)

−

x1

(14)

where x1 is the ﬁrst decision variable of the decision vector
~x. Eq. (14) determines the Pareto front of the optimization
problem. The system function of MONES is:

(cid:26)

minmize β2(~x) = m

minmize β1(~x) =

m
i=1 |
f1(~x)
|

fi(~x)
|
,
,

· · ·

max(
P
|

×

fm(~x)
|

|

)

(15)
Eq. (15) relates the two possible transformed optimization
problem versions with the NES. By combining the loca-
tion function with the two optimization problem versions in
Eq. (15), we can derive a bi-objective optimization problem
representing the original NES:

minmize g1(~x) = α1(~x) + β1(~x)
minmize g2(~x) = α2(~x) + β2(~x)

(cid:26)

(16)

Eq. (16) makes the Pareto optimal solutions of the trans-
formed bi-objective optimization problem correspond to the
optimal solutions of the NES. Since the system functions of
the NES optimization solutions are equal to 0, their images in
the objective space are located on the line segment deﬁned by
y = 1

x.

−

III. VARIABLE REDUCTION STRATEGY

A. Variable reduction strategy in nonlinear equation systems

The main idea of VRS is to ﬁrstly explore the relationships
among variables by utilizing the equality optimality condition
of an optimization problem. The equality optimality condition
refers to the equality condition that the optimization problem
must satisfy when obtaining an optimal solution, which is
expressed in the form of equations. It is a necessary condition
but not necessarily a sufﬁcient condition. For a NES, the
equality optimality condition is the equations in the NES.
Secondarily, according to the types and relationships of the
variables, we always use a part of variables to represent
and calculate the other parts of variables during the iterative
search process of an EA. In this way, the variables represented

-5

-4

-3

-2

-1

1

2

3

4

5

0
x

1

Fig. 1. An example of a NES problem with multiple roots

[24], [26], iii) constrained optimization-based transformation
techniques [27], [28]. Herein, we will brieﬂy introduce two
representative transformation techniques about multiobjec-
tive optimization-based transformation techniques and single-
objective optimization-based transformation techniques that
are more commonly used transformation techniques than con-
strained optimization-based transformation techniques, namely
Dynamic Repulsion-based EA (DREA) [22] and MONES [23],
to prepare for the following research.

DREA transforms a NES to a single-objective optimization
problem and locates multiple roots by repulsion techniques.
The repulsive radius presented in the repulsion techniques is
vital to the performance of algorithms. However, the opti-
mal setting of the repulsive radius is arduous and problem-
dependent, and it should be debugged by trial-and-error.
DREA makes the repulsion radius dynamically change during
the evolutionary process. The repulsion function of DREA is
as follows:

minimize R(~x) = g(~x)

sA

×

Yj=1

ζγ(ρ, dj),

(6)

where

g(~x) =

m

i=1

f 2
i (~x),

X

dj =

~x

||

−

~x∗
j ||

,

ζ(ρ, dj ) =

|

(cid:26)

erf(ρ

−1,

dj)
|

×
1,

if dj
γ
otherwise

≤

,

(7)

(8)

(9)

where g(~x) and R(~x) are the objective and repulsion values
of ~x, respectively. sA is the number of roots found. ~x∗
j is
the j-th root in the archive A (The archive A saves the roots
located during the run). dj is the Euclidean distance of ~x and
~x∗
dη.
j . ”erf” is the error function, erf(x) = 2/√π
The parameter ρ > 0 scales the penalty, ρ=0.1 is selected in
the experimental section. γ adjusts the radius of the repulsion

x
0 e−η
R

2

 
and computed by other variables can be reduced and are
not directly optimized (i.e., as search dimensions) during the
problem-solving process. As a result, some variables and
spatial dimensionality can be reduced, such that
it could
reduce the complexity of the problem and improve the search
efﬁciency of the EA.

Take the NES shown in (1)-(4) as an example. Assume
that A denotes the set of decision variables included in the
, Aj is the collection of the
NES, A=
, n
· · ·
A.
decision variables involved in the j-th equation, Aj
For the equation fj(~x)=0, 1
m, if we can obtain a
relationship as:

k = 1, 2,

xk

≤

≤

∈

}

{

j

|

xk = Rk,j (
{

xl

l

|

∈

Aj, l

= k

)

}

(17)

|

l

}

∈

= k

Aj , l

Then, in the process of locating the NES solutions, xk
can be calculated by the relationship Rk,j and the values of
. Thus, the decision variable xk can be
xl
{
reduced via the j-th equation. Meanwhile, since the variable
relationship (17) is deduced from fj(~x)=0,
the equation
fj(~x)=0 is always satisﬁed when computing the xk value.
Therefore, the equation fj(~x)=0 can be reduced as well. In
addition, a constraint condition associated with the variable xk
is added:

xk ≤

xk = Rk,j (
{

xl

l

|

∈

Aj, l

= k

)

}

≤

xk

(18)

For the sake of clear description, some key concepts are

given as below:

1) Core variable(s): The variable(s) used to represent other

variables in the equations;

2) Reduced variable(s): The variable(s) expressed and com-

puted by core variables;

3) Eliminated equation(s): The equation(s) eliminated
along with the reduction of variables due to be totally
satisﬁed by all solutions.

|

l

{

}

xl

= k

For example, in Eq. (17), xk is a reduced variable,

∈
is the collection of core variables, fj(~x)=0 is
Aj, l
the eliminated equation. Through the above variable reduction
strategy, all variables in the NES can be divided into two
categories: core variables and reduced variables. The collection
of q core variables is denoted as:
X C=
xci ≤

xc1, xc2,
xci,

, xcq
i = 1, 2,

n
≤
, q

{
xci

(19)

· · ·

· · ·

≤

}

q

,

The collection composed of l reduced variables is:

X R=
xri ≤
≤
Hence, we have X C

xr1, xr2,
{
xri
xri,

· · ·

, xrl
,
}
i = 1, 2,

l=n
, l

−

· · ·

q

(20)

X R = A and X C

.
∅
The reduced decision vector can be represented by the core
variables. The reduced decision space formed by the reduced
decision vector is recorded as S∗. The collection of l elimi-
nated equations can be denoted as:

X R =

∩

∪

g =

{

fs1(~x)=0,

· · ·

, fsl(~x)=0

}

(21)

Accordingly, we can obtain the reduced NES:

4

f1(~x) = 0
f2(~x) = 0
...
fp(~x) = 0

,

(22)





s.t. xri ≤

xri = Rri,sj(~xC )

xri, i = 1,

≤

, l, j

· · ·

∈

[1,

· · ·
(23)

, l],

S∗ =

q

Yj=1

[xcj, ¯xcj],

(24)

where p is the number of the equations in the reduced NES,
l. Rri,sj(~xC ) denotes the reduction relationship, in
p = m
which the reduced variable xri can be reduced through the
sj-th eliminated equation.

−

In order to show how VRS works for a NES, consider the

following quintessential illustrative example [22]:

3x2

1 + sin(x1x2)
1 + x2
2x3
2 −
sin(2x1) + cos(x2x3) + x2

−

x3 + 3.0 = 0

x2
3 + 2.0 = 0

1.0 = 0




(a)
(b)
(c)

,

(25)

−
1, 3], x3

[

∈

5, 5], x2


5, 5]. A NES
where x1
[
−
may have more than one reduction scheme. For (25), we can
choose x3 in the equation (b) as the reduced variable, and the
following reduction scheme can be obtained:

−

−

∈

∈

[

x3=2x3

1 + x2

2 + 3.0

(26)

Consequently, in this case, x3 is the reduced variable. x1
1+

and x2 are the core variables. The eliminated equation is 2x3
x2
2 −

x3 + 3.0 = 0. The obtained reduced NES is:

(cid:26)

3x2

1 + sin(x1x2)

x2
3 + 2.0 = 0

sin(2x1) + cos(x2x3) + x2
−
1 + x2
2 + 3.0
≤
3
x2
1

−
x3=2x3
5,
x1

1.0 = 0
5

s.t.

5
−
5
−

≤
≤

(a)
(b)

(27)

≤

≤

≤

−
During the reduction process, constraint condition(s) asso-
ciated with the reduced variable(s) should be considered. For
example, reducing Eq. (25) brings in the constraint conditions
if we choose x3 in
presented in Eq. (27). Furthermore,
equation (a) of Eq. (25) as the reduced variable and we get:

x3 =

3x2

1 + sin(x1x2) + 2.0

(28)

±q

Like the reduction scheme in Eq. (28), we could choose
the variable with an absolute or quadratic term as a reduced
variable, such that a reduced variable may have more than one
value computed from the variable relationship. Therefore, the
values of reduced variables that exceed the upper and lower
bounds of the constraint conditions could be treated by the
following handling technique.

On the one hand, just a reduced variable value corresponds
to one value like Eq. (26). On this occasion, when the reduced
variable value is higher than its upper bound value, we make
the reduced variable value equal to the upper bound value. On
the contrary, we make the reduced variable value equal to the
lower bound value if the reduced variable value is less than
its lower bound value.

6
6
6
6
On the other hand, a reduced variable has more than
one candidate value like Eq. (28), we preferentially choose
the value that does not violate the constraint condition. For
example, when the constraint condition is 0
1
and the core variable x2= 1, x1 calculated by x2 are either 0
or 2. x1= 2 violates the constraint condition and is infeasible,
so we make x1 values equal to 0. If all the reduced variable
values violate the constraint condition, we make them equal
to its upper bound value or lower bound value.

x1= 1

x2

±

≤

≤

5

p
i=1 |

p

i=1 f 2

function value refers to

for VR-MONES or
i (~x) for VR-DR-JADE in this paper. Taking the origi-
nal NES Eq. (1)-(4) and the reduced NES Eq. (22)-(24) as an
P
example, Fig. 3 intuitively displays the calculation process of
the objective function value for an individual in a population.

fi(~x)
|

P

B. Integration of VRS and EA

1) A framework for integrating VRS with EA to solve NESs:
The framework for integrating VRS with EA is exhibited in
Fig. 2. In this framework, a NES is ﬁrst processed by VRS.
The variable reduction process can be seen as a pre-processing
of the NES. Second, a transformation technique is used to
transform the reduced NES into an optimization problem.
Then an EA is used to solve the transformed optimization
problem. Thereby, a series of optimization solutions for the
transformed optimization problem can be obtained, which
corresponds to obtaining the roots for the reduced NES. At
last, the relationships between the reduced variables and the
core variables are used to compute the values of the reduced
variables. By combining the values of the reduced variables
and core variables, a series of roots for the original NES are
ﬁnally obtained.

In the framework, VRS can be theoretically combined with
any transformation technique and EA. In this work, we mainly
study the integration of the VRS with two state-of-the-art
methods, i.e., MONES and DR-JADE.

Fig. 2. A framework for the integration of VRS and EA

2) The integration of VRS and DR-JADE or MONES:
MONES [23] was proposed by Wang Yong et al. in 2014.
MONES transforms the NES described by (1)-(4) into the form
(16), and then solves the transformed bi-objective optimization
problems by NSGA-II (a fast and elitist multi-objective genetic
algorithm) [29]. The method that integrates VRS into MONES
is abbreviated as VR-MONES.

DREA [22] was presented by Liao Z et al. in 2020. DREA
transforms a NES into the form (6). DREA uses JADE (adap-
tive differential evolution with an optional external archive)
as the optimization engine [30]. The combined method is
abbreviated as DR-JADE. We integrate VRS into DR-JADE
and the resultant method is named as VR-DR-JADE for short.
As mentioned in Section II-A, a reduced variable may have
more than one possible value, which may cause different
objective function values for an individual. The objective

Fig. 3. Illustration of computing the objective function value for the individual
~xC
i

· · ·

In Fig. 3,

i,r with the individual ~xC
i

the bottom layer is the i-th individual in a
population during the evolution, which consists of q core
variable values, i.e., ~xC
, xi,cq). First, we
i = (xi,c1, xi,c2,
compute the reduced variable values via the core variable
values in ~xC
and the variable relationships expressed in
i
Eq. (23). The obtained reduced variable values are put in the
set Xi,r, where Xi,rj denotes the set of the value(s) of the
j-th reduced variable (a reduced variable may have more than
one candidate values). Second, we handle the reduced variable
values violating the constraint by the technique introduced in
Section II-A and thus we obtain the feasible reduced variables
set X ′
i,r. Then, we combine the values of the reduced variables
in the set X ′
(i.e., the set of core
variable values) to form the new individual(s) denoted by
Xi. After that, values in Xi is substituted into the reduced
equations (22) to compute the objective function value(s).
or
Finally, we select the minimum value of
k (~x) as the objective function value of the individual

k=1 f 2
~xC
i .
P
Next, for VR-MONES, we should compute the transformed
bi-objective optimization function by Eq. (14)-(16), in which
x1 is the ﬁrst decision variable of the decision vector ~xC after
the objective function value
reduction. For VR-DR-JADE,
obtained by Fig. 3 is the g(~x) value of the individual ~xC
i .
We can compute the value of the corresponding R(~x) by
Eq. (6). Then, we can use VR-MONES or VR-DR-JADE to
iterate and get an evolved population pop. Finally, combining
the values of core variable in pop and the values of reduced
variable computed by reduction relationships forms the ﬁnal
population.

fk(~x)
|

p
k=1 |

P

p

IV. EXPERIMENTAL STUDY

To demonstrate that VRS can improve the performance
of the original algorithms, this section mainly focuses on

the comparison between VR-MONES and VR-DR-JADE with
their corresponding original algorithms, i.e., MONES and DR-
JADE, respectively. In Section IV-A, we use a benchmark
suite with 7 NESs (in which two test problems are real-
world problems) to test the efﬁciency of VRS by comparing
the experimental results between VR-MONES and MONES.
Moreover, in Section IV-B, a large scale test suite of 46
NESs (in which ﬁve test problems are real-world problems)
is used to testify the effectiveness of VRS by comparing VR-
DR-JADE with other popular and state-of-the-art methods. In
Section IV-C, we brieﬂy conclude the experimental results
obtained by Section IV-A and Section IV-B.

A. Experimental study on VR-MONES

We perform VRS for the 7 NESs in reference [23], and com-
pare the performance of VR-MONES and MONES in terms
of two performance indicators, i.e., the inverted generational
distance and the number of the optimal solutions found.

1) Test problems:

In this section, seven test problems
(denoted as F1-F7) are used to investigate the effectiveness
of VR-MONES. Among them, the optimal solutions of F1-F4
are known. F5-F7 have inﬁnitely optimal solutions, which are
not completely known up to now. F6 and F7 are real-world
problems and are derived from neurophysiology application
model and economics system model. The brief information on
the seven test problems is summarized in Table I, including
the number of the decision variables (D), the decision space
(S), the number of the linear equations (LE), the number of
the nonlinear equations (N E), and the number of the roots
(N oR).

TABLE I
CHARACTERIZATIONS OF THE TEST PROBLEMS F1-F7.

Test instance D
2
20
2
2
3
6
20

F1
F2
F3
F4
F5
F6
F7

S
[−1, 1]2
[−1, 1]20
[−1, 1]2
[−1, 1]2
[−1, 1]3
[−1, 1]6
[−1, 1]20

LE N E

1
0
1
0
1
0
1

1
2
1
2
1
6
19

N oR
2
2
11
15
inﬁnite
inﬁnite
inﬁnite

2) Performance metrics: In this section, two performance
indicators are introduced to evaluate the capability of VR-
MONES and MONES to locate the roots of a NES.

1) The inverted generational distance (IGD) [31]: The IGD

indicator is computed as:

IGD(IP, IP ∗) =

P

|

|IP
i=1 d(~νi, IP )
IP ∗

|

∗

|

,

(29)

where IP is a set of the images of the individuals of a
population in the objective space and IP ∗ is a set of the
images of all optimal solutions of a NES in the objective
space: IP ∗=
. d(~νi, IP ) is the minimum
Euclidean distance between ~νi and the points in IP . If
IP ∗
a NES (such as F5, F6 or F7) has inﬁnite roots,
|
is a set of uniformly distributed points in the objective

~ν1,
{

, ~ν2

· · ·

}

|

6

|

|

IP ∗
space along Pareto front.
is the number of optimal
solutions in IP ∗, we set IP ∗ = 100 for F5, F6 and F7.
In this section, the objective space is deﬁned by x = xr
and y = 1
xr for MONES or VR-MONES. xr is the
ﬁrst decision variable of a NES for MONES or the ﬁrst
decision variable of a reduced NES for VR-MONES.
IGD can measure both the diversity and convergence of
IP .

−

2) Number of the optimal solutions found (NOF) [23]: The

NOF indicator is computed as:

NOF(IP, IP ∗) =

|IP

∗

|

i=1

X

f lag(~νi),

(30)

where

f lag(~νi) = 1,
f lag(~νi) = 0,

(cid:26)

if d(~νi, IP )
otherwise

≤

ε, ~νi

∈

IP ∗

(31)

Here, ε is a user-deﬁned threshold value. In this section,
we set ε=0.01 for F5 and 0.02 for the other six problems
according to the number of decision variables [23]. The
larger the NOF-indicator value is, the more roots are
found.

3) Variable reduction results: According to the reduction
method given in Section II-A, a NES may have more than
one reduction scheme. In this section, we only show one
reduction scheme thought to be promising for each NES. The
expressions of the 7 NESs and the related reduction schemes
are shown in Table II. The successive experiments are based
on the reduction schemes shown in Table II.

As can be seen from Table II, each problem in F1-F4
contains two decision variables and two equations, in which
one variable and one equation can be reduced. Two variables
and all the two equations can be reduced for NES F5. Three
variables and three equations can be reduced for NES F6, and
the reduced F6 contains three variables and equations. In this
1 for F7. One variable
Section, we set ck = 0, 1
and equation can be reduced for NES F7.

D

≤

≤

−

k

4) Experimental results and discussions on F1-F7: For a
fair comparison, the parameter settings of VR-MONES are the
same as those of the original MONES in [23]. To make the
experimental results reliable, 30 independent runs are executed
on each NES, and the maximum number of generations is
set to 500 (i.e., the maximum function evaluation number is
50,000) for each run. Table III presents the best, mean, worst
and standard deviation of the IGD-indictor and NOF-indicator
values generated by VR-MONES and MONES.

From the results in Table III, in regard to the IGD indicator,
the best, mean, worst and standard deviation of the IGD-
indicator values obtained by VR-MONES have signiﬁcantly
improved for all the test problems. For example, for NES
F3, the best, mean, worst and standard deviation of the IGD-
indicator values obtained by VR-MONES have been respec-
tively improved by 92.61%, 95.34%, 98.84% and 99.21%
compared with those obtained by MONES. The phenomenon
indicates that the solutions obtained by VR-MONE are closer
to the actual known solutions for all
the test problems
than those obtained by MONES. We also implemented the
Wilcoxon test on the mean IGD-indicator for all the test

TABLE II
EXPRESSIONS AND VARIABLE REDUCTION OF TEST SUITE F1-F7.

7

Test instance

F1

F2

F3

F4

F5

F6

F7

The expression of NES
1 + x2
x2
2 − 1 = 0 (1)
x1 − x2 = 0 (2)
D
i=1 x2
P
|x1 − x2| +

i − 1 = 0 (1)
D
i=3 x2

P

x1 − sin(5πx) = 0 (1)
x1 − x2 = 0 (2)
x1 − cos(4πx2) = 0 (1)

1 + x2
x2

2 = 1 (2)

i = 0 (2)

x1 + x2 + x3 − 1 = 0 (1)

2 = 0 (2)
3 = 1 (1)
4 = 1 (2)

x1 − x3
x2
1 + x2
x2
2 + x2
x5x3
3 + x6x3
1 + x6x3
x5x3
3 + x6x2
x5x1x2
1 + x6x2
x5x3x2

4 = 0 (3)
2 = 0 (4)
4x2 = 0 (5)
2x4 = 0 (6)
xixi+k)xD − ck = 0,

D−k−1
i=1
1 ≤ k ≤ D − 1 (1)
D−1
l=1 xl + 1 = 0 (2)

P

(xk +

P

Reduced variable(s) and eliminated equation(s)
Equation (2)
x2 = x1
Equation (1)

x2 = ±q1 − (x2

D
i=3 x2
i )

P

1 +
Equation (2)
x2 = x1
Equation (1)
x1= cos(4πx2)
Equation (1) and (2)
x1=x3
2
x3 = 1 − x1 − x2

Equation (1), (2) and (3)
x1 = ±q1 − x2
x2 = ±q1 − x2
4
3/x3
x6 = −x5x3
4

3

Equation (2)
18
j=1 xj

x19 = 1 −

P

TABLE III
STATUS OF IGD-INDICATOR AND NOF-INDICATOR IN VR-MONES AND
MONES. THE BETTER OR EQUAL IGD-INDICATOR AND NOF-INDICATOR
FOR EACH NES ARE HIGHLIGHTED IN BOLDFACE.

Test
instance

Status

F1

F2

F3

F4

F5

F6

F7

Best
Mean
Worst
Std
Best
Mean
Worst
Std
Best
Mean
Worst
Std
Best
Mean
Worst
Std
Best
Mean
Worst
Std
Best
Mean
Worst
Std
Best
Mean
Worst
Std

IGD

NOF

MONES

1.51E-04
2.05E-04
4.00E-04
6.92E-05
1.95E-04
6.09E-04
1.46E-03
2.85E-04
1.34E-03
3.82E-03
2.63E-02
6.02E-03
2.75E-03
1.23E-02
9.00E-02
1.73E-02
1.56E-02
4.08E-02
1.78E-01
3.82E-02
1.31E-02
2.21E-02
4.94E-02
8.93E-03
4.09E-02
1.84E-01
8.92E-01
2.24E-01

VR-
MONES
1.47E-04
1.57E-04
2.14E-04
1.34E-05
1.48E-04
1.71E-04
2.68E-04
2.74E-05
9.90E-05
1.78E-04
3.06E-04
4.76E-05
2.05E-03
2.20E-03
2.63E-03
1.08E-04
5.36E-03
5.96E-03
6.63E-03
3.77E-04
8.83E-03
1.10E-02
1.18E-02
7.59E-04
8.13E-03
9.44E-03
1.06E-02
6.16E-04

MONES

2.00E+00
2.00E+00
2.00E+00
0.00E+00
2.00E+00
2.00E+00
2.00E+00
0.00E+00
1.10E+01
1.09E+01
1.00E+01
2.54E-01
1.50E+01
1.42E+01
1.10E+01
1.15E+00
5.00E+01
3.74E+01
1.90E+01
7.25E+00
8.50E+01
7.33E+01
6.00E+01
5.33E+00
3.60E+01
1.43E+01
0.00E+00
1.07E+01

VR-
MONES
2.00E+00
2.00E+00
2.00E+00
0.00E+00
2.00E+00
2.00E+00
2.00E+00
0.00E+00
1.10E+01
1.10E+01
1.10E+01
0.00E+00
1.50E+01
1.50E+01
1.50E+01
0.00E+00
9.00E+01
8.35E+01
7.60E+01
3.10E+00
9.50E+01
8.86E+01
8.40E+01
2.86E+00
9.80E+01
9.14E+01
8.60E+01
2.59E+00

problem over 30 runs1. Compared VR-MONES with MONES,
we can get R+ = 28.0, R− = 0.0 and p = 1.56E
02 by
the Wilcoxon test. Since VR-MONES can provide higher R+

−

value than R− value and the p value is less than 0.05, VR-
MONES is signiﬁcantly better than MONES on the seven test
problems.

With respect to the NOF-indicator, it is clear that the best,
mean, worst and standard deviation of NOF-indicator values
obtained by VR-MONES are better or at least equal to those
obtained by MONES for NESs F1-F7. The results reveal that
VR-MONES can ﬁnd more roots than MONES. For each
NES in F1-F4 with known optimal solutions, VR-MONES
can successfully locate all the roots over 30 runs. For each
NES in F5-F7 with inﬁnitely many roots (the default number
of the optimal solutions is 100 in this section), VR-MONES
has the capability to maintain much more roots than MONES,
especially for F5 and F7.

To further show the performance of MONES and VR-
MONES, Fig. 4 provides the convergence process of the mean
IGD-indicator values provided by MONES and VR-MONES
for all the test problems over 30 independent runs.

As depicted in Fig. 4, at the beginning of evolution, the
convergence curve of VR-MONES or MONES starts with a
relatively large mean IGD-indicator value. As the optimization
proceeding, the mean IGD-indicator values in VR-MONES
and MONES both converge continuously toward a positive
number close to 0. Particularly, VR-MONES can converge to
a smaller IGD-indicator value for each NES in F1-F7. For
example, during the evolution, the mean IGD-indicator values
of MONES and VR-MONES start at 0.2699 and 0.064 and
eventually converge to 0.0094 and 0.0025 respectively for NES
F4. This reveals that VR-MONES can robustly obtain better
solutions while maintaining the diversity of the population and
that the integration of VRS can noticeably improve the search
efﬁciency of MONES.

B. Experimental study on VR-DR-JADE

1The statistical tests reported in this paper are caculated by the KEEL3.0

software [32].

To further evaluate the effectiveness of VRS, this section
applies VRS to another test suite with 46 NESs. In this case,

8

VR-MONES
MONES

400

450

500

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

0

0

50

100

F1

F2

F3

F4

VR-MONES
MONES

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

9

8

7

6

5

4

3

2

1

0

400

450

500

0

50

100

VR-MONES
MONES

0.35

0.3

0.25

0.2

0.15

0.1

0.05

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

400

450

500

0

0

50

100

VR-MONES
MONES

0.3

0.25

0.2

0.15

0.1

0.05

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

400

450

500

0

0

50

100

200

350
250
150
Number of Generations

300

200

350
250
150
Number of Generations

300

200

350
250
150
Number of Generations

300

200

350
250
150
Number of Generations

300

(a) F1

(b) F2

(c) F3

(d) F4

F5

F6

F7

0.35

0.3

0.25

0.2

0.15

0.1

0.05

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

0

0

50

100

VR-MONES
MONES

1.2

1

0.8

0.6

0.4

0.2

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

400

450

500

0

0

50

100

VR-MONES
MONES

2.5

2

1.5

1

0.5

e
u
l
a
V
r
o
t
a
c
i
d
n
i
-

D
G
I

f
o
n
a
e
M

400

450

500

0

0

50

100

200

350
250
150
Number of Generations

300

VR-MONES
MONES

400

450

500

200

350
250
150
Number of Generations

300

200

350
250
150
Number of Generations

300

Fig. 4. Mean of IGD-indicator values for VR-MONES and MONES during the evolution

(e) F5

(f) F6

(g) F7

the representative algorithm DR-JADE with and without VRS
are used to solve the test problems, respectively. The perfor-
mance of VR-DR-JADE and other state-of-the-art methods is
evaluated by the values of root ratio, success rate and other
indicators obtained from the experiments.

1) Test problems: In this section, we choose 46 NESs with
diverse features to extensively evaluate the performance of an
algorithm. The brief information and the maximal number of
function evaluations (N F Esmax) of the 46 NESs are shown
in which the meaning of ”D”, ”LE”, ”NE”,
in Table IV,
and ”NoR” are the same as Table I. The optimal solutions
of NESs E1-E42 are known, while the optimal solutions of
NESs F43-F46 are unknown. NESs E13 and E43-E46 come
from real-world applications. N F Esmax values are different
for different problems owing to their different difﬁculties. In
this section, to fairly compare the performance of VR-DR-
JADE and DR-JADE, we use the same N F Esmax as literature
[22].

2) Performance metrics: In this section, two performance
metrics inspired by the multi-model and multi-objective prob-
lems are adopted to comprehensively assess the solutions
found by a method. Besides, one performance metric is
employed to evaluate the quality of the roots found by a
method.

1) Root ratio (RR) [33]: RR computes the average ratio of

roots found over multiple runs:

RR =

Nr
i=1 Nf,i
Nr

,

(32)

·

P
N oR
where Nr is the number of runs. Nf,i is the number
of roots found in the i-th run and N oR is the number
of actual known roots of a NES. In this section, for a
5, it can
solution ~x, if its repulsion value R(~x) < 1e
be regarded as a root [22]. To make the experimental
results general and reliable, each algorithm is executed
over Nr= 30 independent runs for each NES.

−

2) Success rate (SR) [34]: SR measures the ratio of success-
ful runs. A successful run refers to a run where all the

actual known roots of a NES are found, the expression
of SR is:

SR =

,

(33)

Nr,s
Nr

where Nr,s is the number of successful runs.
The optimal solutions of NESs E43-E46 are unknown,
so the RR and SR criteria cannot be used for the perfor-
mance evaluation. We will discuss the performance of
NESs E43-E46 in the next section.

3) Evaluating the quality of roots found (QR): The mean
j (~x) for the i-th

of the objective function values
run:

j=1 f 2

m

QR =

Nf,i

l=1

P
f 2
j (~xl)/Nf,i,

m

j=1

(34)

X

X
Where m is the number of equations for a NES, ~xl is the
l-th of roots found in the i-th run. QR indicator adapted
from IGD indicator, which can measure the quality of
the roots obtained by an algorithm.

3) Variable reduction results: Due to space limitation, the
expressions of the 46 NESs and the selected reduction schemes
are shown in the supplementary ﬁle.

Among the reduction schemes of the 46 NESs, except
E5, E12, and E21, all the NESs can be reduced by VRS.
A NES may have more than one reduction scheme. We
show one promising reduction scheme for each NES in the
supplementary ﬁle. For the NESs that cannot be reduced, they
can be divided into two categories:

1) No variable in a NES can be explicitly expressed by

other variables, such as E5 and E21.

2) There is a periodic function in a NES, and no variable
can be completely represented and calculated by other
variables in the NES, such as E12.

4) Experimental results and discussion on E1-E46: Except
NESs E5, E12, E21 that cannot be reduced, the experimental
results of DR-JADE and VR-DR-JADE on the other 39 NESs
concerning RR-indicator, SR-indicator and QR-indicator over
30 independent runs are reported in Table V.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABLE IV
BRIEF DESCRIPTION AND THE N F Esmax OF TEST PROBLEMS E1-E46.

Test instance
E1
E2
E3
E4
E5
E6
E7
E8
E9
E10
E11
E12
E13
E14
E15
E16
E17
E18
E19
E20
E21
E22
E23
E24
E25
E26
E27
E28
E29
E30
E31
E32
E33
E34
E35
E36
E37
E38
E39
E40
E41
E42
E43
E44
E45
E46

D
2
2
3
2
2
3
2
2
20
2
2
2
10
2
4
2
5
6
2
2
2
8
2
20
3
2
2
3
3
3
2
2
2
2
3
2
3
2
2
3
2
2
5
6
10
5

LE
0
0
0
1
0
1
0
1
0
1
0
0
0
1
0
0
4
0
0
0
0
0
0
19
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
4
1

NE
2
2
3
1
2
2
2
1
2
1
2
2
10
1
4
2
1
6
2
2
2
8
2
1
3
2
2
3
3
3
2
2
2
2
3
2
3
2
2
3
2
2
5
6
6
4

NoR
2
3
2
2
3
3
3
2
2
11
15
13
1
8
1
7
3
1
10
9
13
16
6
2
7
4
6
8
2
12
2
4
4
2
1
2
1
3
2
5
4
2
inﬁnite
inﬁnite
inﬁnite
inﬁnite

N F Esmax
10 000
50 000
100 000
20 000
100 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
100 000
50 000
50 000
50 000
50 000
100 000
50 000
200 000
50 000
50 000
50 000
100 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
50 000
200 000
200 000
200 000
200 000

The major purpose of DR-JADE is to obtain multiple roots
of a NES, while the quality of the roots is slightly neglected.
From the results in Table V, compared VR-DR-JADE with
DR-JADE, except a slight increase for E25, the mean and
standard deviation values of QR-indicator values for each test
problem in E1-E42 (except E5, E12 and E21) have decreased.
The above phenomenon indicates that the most roots located
by VR-DR-JADE are closer to the actual known roots than the
roots located by DR-JADE. Additionally, to further compare
the quality of the overall solutions of VR-DR-JADE and DR-
JADE, the Wilcoxon test on the mean QR-indicator in Table V
is performed. Compared VR-DR-JADE with DR-JADE, we
can get R+ = 687.0, R− = 16.0 and p = 2.46E
09. Since
VR-DR-JADE can provide higher R+ value than R− value
and the p value is less than 0.05, VR-DR-JADE signiﬁcantly
outperforms DR-JADE in terms of the overall quality of
solutions.

−

9

VR-DR-JADE can get 10 better and 27 equal values in
both RR and SR compared with DR-JADE for NESs E1-E42
(except E5, E12 and E21). It is worth noting that VR-DR-
JADE can locate all the roots of NES E24 over each run.
In contrast, DR-JADE cannot locate any roots for NES E24
over 30 runs. For NES E9, VR-DR-JADE and DR-JADE both
cannot ﬁnd any roots. But when the number of the decision
variables of NES E9 is set to 10, VR-DR-JADE can ﬁnd all
the roots of NES E9 while DR-JADE still cannot ﬁnd any
roots.

For NES E16, VR-DR-JADE always has one root not
found over each run, which may be related to the search
ability of DR-JADE itself. The RR and SR values of VR-
DR-JADE is slightly lower than DR-JADEs for NES E30.
The reason may be that the adoption of VRS makes the
great majority of reduced variable values violate the reduced
variable’s boundary constraint during the evolution, which
greatly reduces the number of feasible solutions.

To further study the roots obtained by VR-DR-JADE, we
compare VR-DR-JADE with ten peer methods,
i.e., DR-
JADE [22], A-WeB[33] [35], VR-MONES, MONES [23], I-
HS [36], NCDE [37], NSDE [37], GA-SQP [38], PSO-NM
[39], and NCSA [40]2. Note that if we map the individuals
in a population to the objective space deﬁned by MONES
and VR-MONES for NESs E1-E42, the roots that have the
same values in the ﬁrst decision variable will be considered
to be discovered even if only a few of them are located. To
address this issue, we propose another way to judge whether an
individual in the ﬁnal population is a root or not, i.e., when the
minimum Euclidean distance between an individual in the ﬁnal
population and the individuals in the set of optimal solutions
for a NES is less than 0.01, the individual can be considered as
a root of the NES. Table VI shows the Friedman test of RR and
SR for the 11 methods. Table VII displays the Wilcoxon test
obtained by the comparison of VR-DR-JADE and the other
ten methods for both RR and SR.

From Table VI, VR-DR-JADE has achieved the highest
Friedman test rankings for both SR and RR. What’s more,
VR-MONES can also obtain higher Friedman test rankings
for both SR and RR than MONES. The above results reveal
that the integration of VRS can make the algorithms locate
more roots than the original algorithms overall. Meanwhile,
Table VII shows that VR-DR-JADE signiﬁcantly outperforms
the other ten methods for RR and SR by the Wilcoxon test,
since it can provide higher R+ values than R− values in all
cases and all p values are less than 0.05 except the p values
obtained by VR-DR-JADE and DR-JADE. Therefore, we can
conclude that the integration of VRS can be an effective way
to improve the performance of the existing method.

To compare the convergence process of VR-DR-JADE and
DR-JADE, we portray the roots located by VR-DR-JADE and
DR-JADE when the number of iterations are 1, 5, 15, and 30
over a typical run. Fig. 5-6 and Fig. 7-8 respectively show the
results for E11 and E19.

2Except for VR-DR-JADE, DR-JADE, VR-MONES, and MONES, the data
of other seven compared methods are from the literature [22] and the detailed
results of the eleven methods are reported in the supplementary ﬁle.

TABLE V
STATUS OF QR-INDICATOR, RR-INDICATOR AND SR-INDICATOR OF DR-JADE AND VR-DR-JADE, WHERE NAN MEANS NOT AVAILABLE.

10

Test problem Status

E1

E2

E3

E4

E6

E7

E8

E9

E10

E11

E13

E14

E15

E16

E17

E18

E19

E20

E22

E23

E24

E25

E26

E27

E28

E29

E30

E31

E32

E33

E34

E35

E36

E37

E38

E39

E40

E41

E42

Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std

QR

RR

SR

DR-JADE
2.35E-06
1.52E-06
5.24E-16
2.15E-15
3.12E-15
1.11E-14
1.05E-07
5.74E-07
8.72E-24
4.02E-23
7.79E-20
4.24E-19
2.39E-20
1.13E-19
NaN
NaN
3.76E-08
1.17E-07
7.02E-09
3.24E-08
1.93E-06
3.24E-08
4.05E-09
1.61E-08
8.28E-08
4.53E-07
8.02E-08
2.60E-07
3.28E-13
1.19E-12
9.78E-32
3.16E-31
4.61E-09
1.56E-08
3.43E-09
1.83E-08
7.67E-11
2.90E-10
3.51E-13
1.92E-12
NaN
NaN
3.16E-16
1.27E-15
3.46E-12
1.89E-11
3.15E-11
1.72E-10
2.39E-07
1.90E-07
4.93E-15
2.69E-14
6.35E-09
1.30E-08
1.61E-21
8.82E-21
1.54E-14
8.41E-14
1.11E-15
4.66E-15
4.98E-07
7.16E-07
2.16E-17
6.56E-17
1.89E-19
1.03E-18
9.56E-09
4.91E-08
3.05E-19
1.08E-18
8.40E-09
4.60E-08
8.82E-09
4.81E-08
3.73E-20
1.81E-19
1.23E-21
6.73E-21

VR-DR-JADE
4.12E-07
8.41E-07
9.56E-22
5.06E-21
1.36E-16
5.18E-16
2.10E-31
1.15E-30
0.00E+00
0.00E+00
6.70E-31
3.36E-30
1.74E-24
9.52E-24
NaN
NaN
7.68E-11
4.08E-10
1.28E-09
6.97E-09
8.30E-12
6.97E-09
9.32E-16
4.07E-15
1.03E-28
8.72E-29
4.69E-09
1.62E-08
8.05E-31
3.56E-46
0.00E+00
0.00E+00
1.70E-20
9.31E-20
3.30E-13
1.15E-12
6.09E-27
2.69E-26
7.11E-29
3.83E-28
0.00E+00
0.00E+00
2.04E-14
1.12E-13
4.74E-27
2.60E-26
9.58E-18
5.23E-17
1.51E-32
1.54E-33
1.55E-18
8.45E-18
3.92E-09
9.38E-09
2.99E-48
1.64E-47
4.45E-20
2.43E-19
7.39E-19
4.05E-18
8.73E-26
4.78E-25
3.02E-18
1.13E-17
4.72E-29
2.58E-28
2.31E-09
1.26E-08
9.57E-24
4.81E-23
2.33E-25
0.00E+00
3.05E-09
1.67E-08
3.94E-31
0.00E+00
1.83E-28
7.00E-28

DR-JADE

VR-DR-JADE

DR-JADE

VR-DR-JADE

1.0000

1.0000

0.8667

1.0000

1.0000

1.0000

1.0000

0.0000

0.9970

0.9378

1.0000

0.9667

1.0000

1.0000

1.0000

1.0000

0.8600

1.0000

0.8375

1.0000

0.0000

1.0000

1.0000

1.0000

0.9208

1.0000

0.9306

1.0000

1.0000

1.0000

0.5000

1.0000

1.0000

1.0000

1.0000

1.0000

0.9467

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.8571

1.0000

1.0000

1.0000

1.0000

0.8729

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.8833

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.9867

1.0000

1.0000

1.0000

1.0000

0.7333

1.0000

1.0000

1.0000

1.0000

0.0000

0.9667

0.2667

1.0000

0.7333

1.0000

1.0000

1.0000

1.0000

0.0333

1.0000

0.0333

1.0000

0.0000

1.0000

1.0000

1.0000

0.5000

1.0000

0.3667

1.0000

1.0000

1.0000

0.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.7333

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.0000

1.0000

1.0000

1.0000

1.0000

0.0667

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.3333

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

1.0000

0.9333

1.0000

1.0000

TABLE VI
AVERAGE RANKINGS OF VR-DR-JADE AND THE OTHER TEN
STATE-OF-THE-ART ALGORITHMS OBTAINED BY THE FRIEDMAN TEST FOR
BOTH RR AND SR

Algorithm
VR-DR-JADE
DR-JADE
VR-MONES
MONES
A-WeB
NCDE
NSDE
I-HS
GA-SQP
PSO-NM
NCSA

Ranking (RR)
3.2308
3.8718
4.6538
7.1282
5.1282
5.7821
5.2564
6.1667
9.8974
6.8974
7.9872

Ranking (SR)
3.2564
3.9103
4.8077
7.1795
5.0641
5.6410
5.3333
6.2949
9.4359
7.0256
8.0513

TABLE VII
RESULTS OF VR-DR-JADE COMPARED WITH THE OTHER TEN
STATE-OF-THE-ART ALGORITHMS OBTAINED BY THE WILCOXON TEST
FOR BOTH RR AND SR

VR-DR-JADE
V S
DR-JADE
VR-MONE
MONES
A-WeB
NCDE
NSDE
I-HS
GA-SQP
PSO-NM
NCSA

R+
498.5
528.5
665.0
674.0
597.0
625.5
670.0
778.5
701.5
735.0

RR

R−
242.5
212.5
76.0
106.0
144.0
154.5
71.0
1.5
39.5
45.0

p-value
6.40E-02
2.11E-02
3.76E-06
2.57E-05
6.83E-04
6.87E-04
2.30E-06
9.09E-12
5.92E-08
6.12E-08

R+
528.5
529.0
696.0
660.0
593.0
623.5
710.5
739.5
695.0
717.5

SR
R−
212.5
212.0
84.0
120.0
148.0
156.5
69.5
1.5
46.0
23.5

p-value
2.11E-02
2.07E-02
4.02E-06
7.35E-05
8.70E-04
7.72E-04
9.98E-07
1.82E-11
1.39E-07
5.10E-09

Compared Fig. 5 with Fig. 6, at t = 1 VR-DR-JADE and
DR-JADE can both ﬁnd one of the roots for E11. At t = 5,
VR-DR-JADE can locate ﬁve roots while DR-JADE only ﬁnds
three roots. At t = 15, VR-DR-JADE can locate 11 roots, but
DR-JADE only locates nine roots. At t = 30, VR-DR-JADE
has found all 15 roots of E11, while DR-JADE only located
13 roots.

Compared Fig. 7 with Fig. 8, at t = 1 owing to the reduced
variable with a quadratic term for E19, we can locate two
roots for VR-DR-JADE. At t = 5, VR-DR-JADE has located
all the ten roots while DR-JADE only has found three roots.
At t = 15 and t = 30, DR-JADE has located 5 and 8 roots
respectively and there have been still two roots not found at the
end of evolution. The above comparison results demonstrate
that the application of VRS improves the search efﬁciency of
DR-JADE and makes VR-DR-JADE can get more roots than
DR-JADE under the same number of iterations.

In the previous sections, the performance of VR-DR-JADE
is veriﬁed through the 42 NESs with known roots. For E43-
E46, we evaluate the number of obtained roots for DR-JADE
and VR-DR-JADE by the best, the worst, the mean, and the
standard deviation of the number of obtained roots over 30
runs. What’s more, we evaluate the quality of obtained roots
for DR-JADE and VR-DR-JADE by the best, the mean, and
the standard deviation of objective function values of obtained

11

roots over a typical run. The results are reported in Table VIII3.
As shown in Table VIII, for each NES in E43-E46, the
number of roots obtained by VR-DR-JADE is better than or
equivalent to DR-JADE in terms of the best, the worst, the
mean, and the standard deviation values (except the standard
deviation values of E46). Especially for E43 and E46, the
mean values of the number of roots obtained by VR-DR-JADE
are 30 and 28.97 over 30 runs respectively, which is much
more than those obtained by DR-JADE. For E46, although
the standard deviation value of the number of roots obtained
by VR-DR-JADE is larger than that obtained by DR-JADE,
the best, the worst, and the mean values of the number of
roots obtained by VR-DR-JADE have a great improvement.
Especially for E44 and E46, the roots found by VR-DR-
JADE have signiﬁcantly better quality than those found by
DR-JADE. The above phenomenon reveals that VR-DR-JADE
is capable of locating more better roots than DR-JADE when
a NES has an inﬁnite number of solutions.

C. Experimental conclusions

According to the experimental results and discussions

above, we can safely draw some conclusions:

1) VR-MONES can achieve better IGD-indicator and NOF-

indicator values than MONES.

2) For solving NESs E1-E42 with known optimal solutions,
VR-DR-JADE almost always can get higher or equal
RR-indicator and SR-indicator values than DR-JADE
and the other nine methods, and VR-DR-JADE can
obtain smaller QR-indicator than DR-JADE.

3) For solving NESs E43-E46 with unknown optimal so-
lutions, VR-DR-JADE can locate more roots and the
obtained roots are closer to the actual roots in a single
run compared to DR-JADE.

In conclusion, the integration of VRS enables MONES and
DR-JADE not only to locate more roots but also signiﬁcantly
improve the quality of the located roots.

V. CONCLUSIONS AND FUTURE WORK

This paper proposes to incorporate VRS into EAs to solve
NESs. VRS reduces the number of variables and equations
of a NES, accordingly shrinks the decision space and re-
duces the complexity of the NES, and results in improving
the optimization efﬁciency of the original EA for solving
the NES.VRS is speciﬁcally integrated with two state-of-
the-art methods (MONES and DR-JADE), respectively. The
experimental results on two test suites with 7 NESs and 46
NESs respectively verify the effectiveness of VRS in solving
NESs. According to the framework of the combination of VRS
and EAs, VRS theoretically can also be integrated with any
EA. The research in this paper reveals that combining with
problem domain knowledge could improve the performance
of algorithms.

It is noted that there are still shortcomings in this work. On
the one hand, VRS cannot be applied to all NESs. On the other
hand, for two NESs, EAs may even obtain worse results after

3The date of DR-JADE comes from the literature [22].

E11    t=1

E11    t=5

E11    t=15

E11    t=30

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

12

(x)

f

1

f

(x)

2
Root

(a) t=1

(b) t=5

(c) t=15

(d) t=30

Fig. 5. Evolution of VR-DR-JADE over a typical run on E11

E11    t=1

E11    t=5

E11    t=15

E11    t=30

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

1

0.8

0.6

0.4

0.2

2

x

0

-0.2

-0.4

-0.6

-0.8

-1

-1

(x)

f

1

f

(x)

2
Root

-0.8

-0.6

-0.4

-0.2

0
x

1

0.2

0.4

0.6

0.8

1

(a) t=1

(b) t=5

(c) t=15

(d) t=30

Fig. 6. Evolution of DR-JADE over a typical run on E11

E19    t=1

E19    t=5

E19    t=15

E19     t=30

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

(a) t=1

(b) t=5

(c) t=15

(d) t=30

Fig. 7. Evolution of VR-DR-JADE over a typical run on E19

E19    t=1

E19    t=5

E19    t=15

E19    t=30

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

2

1.5

1

0.5

2

x

0

-0.5

-1

-1.5

-2

-2

(x)

f

1

f

(x)

2
Root

-1.5

-1

-0.5

0
x

1

0.5

1

1.5

2

(a) t=1

(b) t=5

(c) t=15

(d) t=30

Fig. 8. Evolution of DR-JADE over a typical run on E19

TABLE VIII
STATUS OF DR-JADE AND VR-DR-JADE FOR THE NUMBER OF OBTAINED ROOTS AND THE OBJECTIVE FUNCTION VALUES

Test problem

Algorithm

E43

E44

E45

E46

DR-JADE
VR-DR-JADE
DR-JADE
VR-DR-JADE
DR-JADE
VR-DR-JADE
DR-JADE
VR-DR-JADE

Objective function values of obtained roots

Number of obtained roots

Best
1.66E-25
1.05E-33
3.85E-28
0.00E+00
8.45E-17
9.28E-26
2.49E-29
1.70E-166

Mean
3.45E-06
1.88E-07
2.06E-12
6.16E-34
4.07E-13
8.95E-16
4.07E-13
1.51E-114

Std
3.27E-06
6.37E-07
7.70E-12
3.38E-33
1.36E-12
1.28E-15
1.36E-12
8.13E-114

Best Worst Mean
8.93
7.00
12.00
30.00
30.00
30.00
24.10
22.00
28.00
29.70
29.00
30.00
30.00
30.00
30.00
30.00
30.00
30.00
3.13
3.00
4.00
28.97
28.00
30.00

Std
1.86
0.00
1.54
0.47
0.00
0.00
0.12
0.72

integrating with VRS. For the NESs that cannot be explicitly
reduced, an approximative variable reduction strategy may be
useful to resolve this problem. Moreover, we can develop more
efﬁcient transformation techniques and EAs to combine with
VRS to solve NESs, e.g., the ensemble algorithms [41] and
objective space partition strategy [42]. In summary, extending
VRS and designing more efﬁcient EAs and transformation
techniques to integrate with VRS deserve further investigation
in the future.

REFERENCES

[1] D. Mehta and C. Grosan, “A collection of challenging optimization prob-
lems in science, engineering and economics,” in 2015 IEEE Congress
on Evolutionary Computation (CEC).

IEEE, 2015, pp. 2697–2704.

[2] A. Holstad, “Numerical solution of nonlinear equations in chemical
speciation calculations,” Computational geosciences, vol. 3, no. 3-4, pp.
229–257, 1999.

[3] C. L. Collins, “Forward kinematics of planar parallel manipulators in
the clifford algebra of p2,” Mechanism and Machine Theory, vol. 37,
no. 8, pp. 799–813, 2002.

[4] F. Facchinei and C. Kanzow, “Generalized nash equilibrium problems,”

4or, vol. 5, no. 3, pp. 173–210, 2007.

[5] N. I. Chaudhary, M. S. Aslam, and M. A. Z. Raja, “Modiﬁed volterra lms
algorithm to fractional order for identiﬁcation of hammerstein non-linear
system,” IET Signal Processing, vol. 11, no. 8, pp. 975–985, 2017.
[6] G. Yuan and X. Lu, “A new backtracking inexact bfgs method for
symmetric nonlinear equations,” Computers & Mathematics with Ap-
plications, vol. 55, no. 1, pp. 116–129, 2008.

[7] C. L. Karr, B. Weck, and L. M. Freeman, “Solutions to systems of
nonlinear equations via a genetic algorithm,” Engineering Applications
of Artiﬁcial Intelligence, vol. 11, no. 3, pp. 369–375, 1998.

[8] C. Grosan and A. Abraham, “A new approach for solving nonlin-
ear equations systems,” IEEE Transactions on Systems, Man, and
Cybernetics-Part A: Systems and Humans, vol. 38, no. 3, pp. 698–714,
2008.

[9] D. J. Bates, A. J. Sommese, J. D. Hauenstein, and C. W. Wampler,
Numerically solving polynomial systems with Bertini. SIAM, 2013.

[10] J. Denis and H. Wolkowicz, “Least change secant methods, sizing, and
shifting,” SIAM Journal of Numerical Analisys, vol. 30, pp. 1291–1314,
1993.

[11] J. M. Ortega and W. C. Rheinboldt, Iterative solution of nonlinear

equations in several variables. Siam, 1970, vol. 30.

[12] L. Wang, “Intelligent optimization algorithms with applications,” Ts-

inghua University & Springer Press, Beijing, 2001.

[13] H.-T. Geng, Y.-J. Sun, Q.-X. Song, and T.-T. Wu, “Research of ranking
method in evolution strategy for solving nonlinear system of equations,”
in 2009 First International Conference on Information Science and
Engineering.

IEEE, 2009, pp. 348–351.

[14] A. Ouyang, Y. Zhou, and Q. Luo, “Hybrid particle swarm optimization
algorithm for solving systems of nonlinear equations,” in 2009 IEEE
International Conference on Granular Computing.
IEEE, 2009, pp.
460–465.

[15] O. E. Turgut, M. S. Turgut, and M. T. Coban, “Chaotic quantum behaved
particle swarm optimization algorithm for solving nonlinear system of
equations,” Computers & Mathematics with Applications, vol. 68, no. 4,
pp. 508–530, 2014.

[16] W. Gong, Y. Wang, Z. Cai, and L. Wang, “Finding multiple roots of
nonlinear equation systems via a repulsion-based adaptive differential
evolution,” IEEE Transactions on Systems, Man, and Cybernetics:
Systems, vol. 50, no. 4, pp. 1499–1513, 2020.

[17] H. Ren, L. Wu, W. Bi, and I. K. Argyros, “Solving nonlinear equations
system via an efﬁcient genetic algorithm with symmetric and harmonious
individuals,” Applied Mathematics and Computation, vol. 219, no. 23,
pp. 10 967–10 973, 2013.

[18] G. Joshi and M. B. Krishna, “Solving system of non-linear equations
using genetic algorithm,” in 2014 International Conference on Advances
in Computing, Communications and Informatics (ICACCI). IEEE, 2014,
pp. 1302–1308.

[19] H.-G. Beyer and H.-P. Schwefel, “Evolution strategies–a comprehensive
introduction,” Natural computing, vol. 1, no. 1, pp. 3–52, 2002.
[20] G. Wu, W. Pedrycz, P. N. Suganthan, and R. Mallipeddi, “A variable
reduction strategy for evolutionary algorithms handling equality con-
straints,” Applied Soft Computing, vol. 37, pp. 774–786, 2015.

13

[21] G. Wu, W. Pedrycz, P. N. Suganthan, and H. Li, “Using variable
reduction strategy to accelerate evolutionary optimization,” Applied Soft
Computing, vol. 61, pp. 283–293, 2017.

[22] Z. Liao, W. Gong, X. Yan, L. Wang, and C. Hu, “Solving nonlinear equa-
tions system with dynamic repulsion-based evolutionary algorithms,”
IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 50,
no. 4, pp. 1590–1601, 2020.

[23] W. Song, Y. Wang, H.-X. Li, and Z. Cai, “Locating multiple optimal
solutions of nonlinear equation systems based on multiobjective opti-
mization,” IEEE Transactions on Evolutionary Computation, vol. 19,
no. 3, pp. 414–431, 2014.

[24] S. Qin, S. Zeng, W. Dong, and X. Li, “Nonlinear equation systems solved
by many-objective hype,” in 2015 IEEE Congress on Evolutionary
Computation (CEC).

IEEE, 2015, pp. 2691–2696.

[25] M. J. Hirsch, P. M. Pardalos, and M. G. Resende, “Solving systems of
nonlinear equations with continuous grasp,” Nonlinear Analysis: Real
World Applications, vol. 10, no. 4, pp. 2000–2006, 2009.

[26] Y. Wang, H.-X. Li, G. G. Yen, and W. Song, “Mommop: Multiobjective
optimization for locating multiple optimal solutions of multimodal
optimization problems,” IEEE transactions on cybernetics, vol. 45, no. 4,
pp. 830–843, 2014.

[27] A. Mousa and I. El-Desoky, “Genls: Co-evolutionary algorithm for
nonlinear system of equations,” Applied Mathematics and Computation,
vol. 197, no. 2, pp. 633–642, 2008.

[28] A. F. Kuri-Morales, R. H. No, and D. M´exico, “Solution of simultaneous
non-linear equations using genetic algorithms,” WSEAS Transactions on
Systems, vol. 2, no. 1, pp. 44–51, 2003.

[29] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist
multiobjective genetic algorithm: Nsga-ii,” IEEE transactions on evolu-
tionary computation, vol. 6, no. 2, pp. 182–197, 2002.

[30] J. Zhang and A. C. Sanderson, “Jade: adaptive differential evolution
with optional external archive,” IEEE Transactions on evolutionary
computation, vol. 13, no. 5, pp. 945–958, 2009.

[31] Y. Wang, J. Xiang, and Z. Cai, “A regularity model-based multiobjective
estimation of distribution algorithm with reducing redundant cluster
operator,” Applied Soft Computing, vol. 12, no. 11, pp. 3526–3538, 2012.
[32] I. Triguero, S. Gonz´alez, J. M. Moyano et al., “Keel 3.0: an open source
software for multi-stage analysis in data mining,” International Journal
of Computational Intelligence Systems, vol. 10, pp. 1238–1249, 2017.
[33] R. Thomsen, “Multimodal optimization using crowding-based differen-
tial evolution,” in Proceedings of the 2004 Congress on Evolutionary
Computation (IEEE Cat. No. 04TH8753), vol. 2.
IEEE, 2004, pp.
1382–1389.

[34] X. Li, A. Engelbrecht, and M. G. Epitropakis, “Benchmark functions
for cec2013 special session and competition on niching methods for
multimodal function optimization,” RMIT University, Evolutionary Com-
putation and Machine Learning Group, Australia, Tech. Rep, 2013.
[35] W. Gong, Y. Wang, Z. Cai, and S. Yang, “A weighted biobjective trans-
formation technique for locating multiple optimal solutions of nonlinear
equation systems,” IEEE Transactions on Evolutionary Computation,
vol. 21, no. 5, pp. 697–713, 2017.

[36] G. C. Ramadas, E. M. Fernandes, and A. M. A. Rocha, “Multiple roots
of systems of equations by repulsion merit functions,” in International
Conference on Computational Science and Its Applications. Springer,
2014, pp. 126–139.

[37] B.-Y. Qu, P. N. Suganthan, and J.-J. Liang, “Differential evolution with
neighborhood mutation for multimodal optimization,” IEEE transactions
on evolutionary computation, vol. 16, no. 5, pp. 601–614, 2012.
[38] M. A. Z. Raja, A. K. Kiani, A. Shehzad, and A. Zameer, “Memetic
computing through bio-inspired heuristics integration with sequential
quadratic programming for nonlinear systems arising in different phys-
ical models,” SpringerPlus, vol. 5, no. 1, p. 2063, 2016.

[39] M. A. Z. Raja, A. Zameer, A. K. Kiani, A. Shehzad, and M. A. R. Khan,
“Nature-inspired computational
intelligence integration with nelder–
mead method to solve nonlinear benchmark models,” Neural Computing
and Applications, vol. 29, no. 4, pp. 1169–1193, 2018.

[40] X. Zhang, Q. Wan, and Y. Fan, “Applying modiﬁed cuckoo search al-
gorithm for solving systems of nonlinear equations,” Neural Computing
and Applications, vol. 31, no. 2, pp. 553–576, 2019.

[41] G. Wu, R. Mallipeddi, and P. N. Suganthan, “Ensemble strategies
for population-based optimization algorithms–a survey,” Swarm and
evolutionary computation, vol. 44, pp. 695–711, 2019.

[42] H. Chen, G. Wu, W. Pedrycz, P. N. Suganthan, L. Xing, and X. Zhu, “An
adaptive resource allocation strategy for objective space partition-based
multiobjective optimization,” IEEE Transactions on Systems, Man, and
Cybernetics: Systems, pp. 1–16, 2019.

