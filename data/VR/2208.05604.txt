Going Incognito in the Metaverse

Vivek Nair*
UC Berkeley
vcn@berkeley.edu

Gonzalo Munilla Garrido*
Technical University of Munich
gonzalo.munilla-garrido@tum.de

Dawn Song
UC Berkeley
dawnsong@berkeley.edu

2
2
0
2

g
u
A
9
1

]

R
C
.
s
c
[

2
v
4
0
6
5
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Virtual reality (VR) telepresence applications and
the so-called “metaverse” promise to be the next major medium
of interaction with the internet. However, with numerous recent
studies showing the ease at which VR users can be proﬁled,
deanonymized, and data harvested, metaverse platforms carry
all the privacy risks of the current internet and more while
at present having none of the defensive privacy tools we are
accustomed to using on the web. To remedy this, we present
the ﬁrst known method of implementing an “incognito mode”
for VR. Our technique leverages local ε-differential privacy
to quantiﬁably obscure sensitive user data attributes, with a
focus on intelligently adding noise when and where it is needed
most to maximize privacy while minimizing usability impact.
Moreover, our system is capable of ﬂexibly adapting to the
unique needs of each metaverse application to further optimize
this trade-off. We implement our solution as a universal Unity
(C#) plugin that we then evaluate using several popular VR
applications. Upon faithfully replicating the most well known
VR privacy attack studies, we show a signiﬁcant degradation
of attacker capabilities when using our proposed solution.

Index Terms—Virtual reality, Usable security, Incognito mode,
Data harvesting, Private browsing, Differential privacy

1. Introduction

Recent years have seen an explosive growth in invest-
ment into “metaverse” platforms [70], immersive augmented
and virtual reality (AR/VR) applications that claim to realize
the next major iteration of the internet as a 3D virtual
world. Such platforms, by their very nature, transform every
movement of a VR user into a stream of data to be rendered
as a virtual character model for other users around the world.
As one might expect, the same data stream can also be
used for less benign purposes. Recent studies have shown
that seemingly-anonymous VR users can easily and accu-
rately be tracked [18], proﬁled [62], data mined [82], and
deanonymized [54] from just a few minutes of tracking data.
They further show that while “the potential scale and scope
of this data collection far exceed what is feasible within
traditional mobile and web applications” [62], users are less
broadly aware of security and privacy risks in VR than they
are of similar risks in more traditional environments [4].

*Equal contribution.

Of course, data privacy challenges are not unique to VR.
Nearly every major communications technology advance-
ment of the past century has been accompanied by corre-
sponding privacy risks, from the wiretapping of landlines
beginning in the 1890s through to new privacy concerns
about smart home and wearable devices today [26].

On the web,

third-party tracking cookies are a
quintessential example of this phenomenon. While cookies
serve an important, legitimate purpose in web browsers,
adversaries can leverage them to attach identiﬁers and track
users across websites [12]. Nevertheless, the maturation of
web technologies has brought an enhanced understanding
of (and countermeasures to) such attacks. Technologies like
Tor [20], VPNs, proxies, and most of all, private browsing
(or “incognito”) mode in browsers [6], have provided users
with vital defensive tools for reclaiming their privacy in
the face of such attacks. Equivalent comprehensive privacy
defenses have yet to be developed for the metaverse.

We ﬁnd ourselves now in the dangerous situation of
facing unprecedented privacy threats in VR while lacking
the defensive resources we have become accustomed to
on the web. In this paper, we aim to begin addressing this
challenge by designing and implementing an “incognito
mode” for VR. Our method uses local ε-differential privacy
to provide provable resilience against known VR privacy
attacks according to a user-adjustable privacy parameter ε.
In doing so, it allows for inherent privacy and usability
trade-offs to be dynamically rebalanced according to the
risks and requirements of each VR application, with a
focus on the targeted addition of noise to those parameters
which are most vulnerable. Using an implementation of
our solution as a Unity plugin, we replicate existing VR
privacy attack studies and show a signiﬁcant degradation
of attacker capabilities when using our extension.

Contributions

1) We provide an ε-differential privacy framework for pro-
tecting a range of sensitive data attributes in VR (§3).
2) We design and describe a concrete implementation of a
modular “VR Incognito Mode” plugin for Unity (§4).
3) We experimentally demonstrate the efﬁcacy of our ap-
proach at defeating known VR privacy attacks (§5.3).
4) We show that our protections are sufﬁcient to degrade
the proﬁling and deanonymization of VR users (§5.4).

 
 
 
 
 
 
2. Background & Motivation

2.2. Metaverse Threat Model

In this section, we aim to motivate the need for an
“incognito mode” in VR. We begin by analyzing known pri-
vacy threats in VR, highlighting the ones we aim to address
in this paper. Next, we present a comprehensive threat model
to illustrate which threats are feasibly mitigated by software-
based client-side defenses. We then brieﬂy discuss private
web browsing to draw an analogue to the goals of this paper.
Finally, we introduce differential privacy and randomized
response, the theoretical building blocks for our solution.

2.1. VR Privacy Attacks

The primary motivator for this research is the breadth
of prior work demonstrating compelling privacy risks
within the metaverse. Amongst the extant research at the
intersection of privacy and VR are studies ranging from
high-level literature reviews on VR privacy [65], [11], [44],
[28], [61], [78], [49], [19], [17] to targeted risk assessment
frameworks [31], [82] and privacy guidelines [47]. Among
the technical works, we ﬁnd the following relevant studies:

Eye and Body Tracking. Several works focus speciﬁcally
on the security and privacy of eye tracking [39], [42]. We
place a limited emphasis on eye tracking in this paper,
as such features remain relatively rare on commercial
VR devices, and there are already known effective
countermeasures [76], [48], [16], [36]. We set aside the
privacy full-body motion capture systems [59], [67], [71],
[45] for similar reasons. Instead, we focus on the simple
conﬁguration of a headset plus two handheld controllers,
as is found on most commercially available devices today.

Comprehensive Attacks. The attacks most relevant to this
paper are those of the 2020 Miller et al. “TTI” study [54]
(as well as the 2022 follow-up [55]) and the 2022 Nair et
al. “MetaData” study [62]. The TTI study demonstrated
that 95% of all seemingly-anonymous VR users could be
deanonymized by an adversary from just 5 minutes of
tracking data. The MetaData study expanded on this result,
showing that said attackers can also ascertain more than
25 private data points from VR users, including various
environmental, demographic, and anthropometric attributes.
In aggregate, the below attributes are those that the extant
literature suggests can be harvested from VR users and that
we believe our techniques can protect:

• Anthropometrics: height, wingspan, arm lengths, ﬁtness,
interpupillary distance, handedness, reaction time [62].

• Environment: room size, geolocation [62].

• Technical: tracking/refresh rate, device model [62], [79].

• Demographics: gender, age, ethnicity, income [62].

• Identity [54], [55].

We now present a threat model to contextualize our
contributions within the broader ecosystem of VR privacy.
Our model is adapted from the VR threat model of the
MetaData study [62]. We consider a target user who
interacts with the metaverse over multiple usage sessions.
The parties which could plausibly observe a session are:

• A (I) Hardware Attacker, which controls the hardware
and ﬁrmware of the target user’s VR device, and thus has
access to raw sensor data from the VR hardware.

• A (II) Client Attacker, which controls the client-side VR
application running on the target user’s device, and thus
has access to data provided by the device APIs.

• A (III) Server Attacker, which controls the external
server used to facilitate multi-player functionality, and
thus receives a stream of telemetric data from the client.
• A (IV) User Attacker, which represents another end-user
of the same VR application, and thus naturally receives
from the server a stream of data about the target user.

In our model, the goals of an attacker are to correctly
observe attributes of the target user, or to identify them
across multiple sessions. Fig. 1 shows that the four attackers
lie on a continuum; the later attackers have less privilege and
attack accuracy, but can more easily conceal their attacks.

Figure 1: Continuum of VR privacy attackers.

In this paper, we present algorithmic statistical defenses
for the vulnerable attributes of §2.1 that can be implemented
at either the device ﬁrmware or client software level. Tab. 1
shows the attackers covered by each implementation possi-
bility. In practice, lacking any special access to VR device
ﬁrmware, our evaluated implementations were all at the
software level.

Attackers
II

III

IV

I

(cid:33) (cid:33)

(cid:33) (cid:33) (cid:33)

Software
Incognito
Firmware
Incognito

TABLE 1: Coverage of proposed defenses.

Overall, the “VR incognito mode” defenses proposed in
this paper are unable to address the threat of hardware and
ﬁrmware level attackers. We argue that this is a necessary
concession of a software-based defense, and that unlike the
client, server, and user attackers we cover, hardware and
ﬁrmware attacks can be discovered via reverse engineering.

(I)(II)(III)(IV)Decreasing Capability & FidelityIncreasing Ease & Concealment2.3. Private Web Browsing

We now detour brieﬂy to the more mature ﬁeld of private
web browsing to seek inspiration from the web privacy
solutions which have stood the test of time.

The research community has surveyed the ﬁeld of web
privacy [58], [80], and identiﬁed observable attributes rang-
ing from tracking cookies [12] and HTTP headers [43] to
browsing histories [46] and motion sensor data [88]. As in
VR, these attributes can be combined to achieve proﬁling
[27], [30], ﬁngerprinting [43] and deanonymization [5].

Furthermore,

the attack model used by web privacy
researchers resembles the metaverse threat model presented
in §2.2, with most client-side defenses focusing on web
servers and other users, many on client-side applications,
and relatively few on the underlying hardware.

In response to these threats, proposed solutions have
included proxies, VPNs [37], Tor [20], [38], and, of course,
private browsing or “incognito” mode in browsers, as well
as dedicated private browsers and search engines, e.g.,
Brave [6] and DuckDuckGo [22]. Of these solutions, incog-
nito mode stands out due to its ease of use; a range of defen-
sive modiﬁcations to protocols, APIs, cookies, and browsing
history can all be deployed with a single click [8]. Due
perhaps to this outward simplicity, surveys of web privacy
protections used in practice have found private browsing
mode to be by far the most popular at 73% adoption [32].
In summary, web privacy is surprisingly analogous to
metaverse privacy; although the data attributes being pro-
tected are vastly different, the threat of combining attributes
to proﬁle and deanonymize users is a constant, as is the
threat model used to characterize both ﬁelds. On the other
hand, the size and scope of data collection in VR potentially
exceeds that of the web [62], while users are simultaneously
less aware of the threat in VR [50], and the equivalent
privacy tools are not generally available. We thus are moti-
vated by the popularity of incognito mode as a web privacy
solutions to seek an equivalent for VR, with the same
fundamental goal as in browsers: allowing users, at the ﬂick
of a switch, to become harder to trace across sessions.

2.4. Differential Privacy

Having established our motivation for pursuing a meta-
verse equivalent
to “incognito mode,” we now lay out
the tools necessary to enable its realization. Chief among
these is differential privacy [24], which provides a context-
agnostic mathematical deﬁnition of privacy that statistically
bounds the information gained by a hypothetical adversary
from the output of a given function M(·):

Deﬁnition 1. (ε-Differential Privacy [25]). A randomized
function M(·) is ε-differentially private if
input
datasets D and D(cid:48) differing on at most one element, and
for all possible outputs S ⊆ Range(M):

for all

Pr[M(D) ∈ S] ≤ eε × Pr[M(D(cid:48)) ∈ S].

A function M(·) fulﬁlls differential privacy if its out-
puts with and without the presence of an individual input
element are indistinguishable with respect to the privacy
parameter ε ≥ 0. In practice, a randomized function M(·)
typically ensures differential privacy by adding calibrated
random noise to the output of a deterministic function,
M(x) = f (x)+Noise. Lower ε values correspond to higher
noise, making it harder to distinguish outputs and strength-
ening the privacy protection. In addition to ε, the required
noise is affected by the sensitivity (∆) of the deterministic
function, which quantiﬁes the maximum difference between
a function’ outputs between D and D(cid:48).

Another aspect worth highlighting is sequential compo-
sition [25]: if M(·) is computed n times over D with εi,
the total privacy budget consumed is (cid:80) εi. Thus, users’
attributes become less protected with every query execu-
tion. Differentially private outputs are also immune to post-
processing [25]; an adversary can compute any function on
the output (e.g., rounding) without reducing privacy.

In practice, differential privacy can be used centrally,
whereby a server adds noise to an aggregation function
computed over multiple data points from clients, or locally,
whereby clients add noise to data points before sharing
them with a server. While local differential privacy is noisier
than the central variant, it also requires less trust of the
server. Since servers are considered potential adversaries
(§2.2), we use local differential
in our
privacy to protect VR users in this paper. Speciﬁcally, we
implement
local differential privacy using the Bounded
Laplace Mechanism [33], [25] for continuous attributes and
randomized response [85] for Boolean attributes.

threat model

Bounded Laplace Mechanism. The Laplace mecha-
nism [25], known as
the “workhorse of differential
privacy,” [33] is a popular method of implementing local
differential privacy for continuous attributes; however, its
unbounded noise can yield edge cases with semantically
erroneous results (e.g., a negative value for the height
attribute). Thus, in this paper, we use the Bounded Laplace
mechanism [33], which transforms the noise distribution
according to the privacy parameters and deterministic
value,
then samples outputs until a value falls within
pre-determined bounds without compromising differential
privacy. Inputs that fall outside the bounds are automatically
clamped to the nearest bound. Additionally, we employ
the modiﬁed sampling technique of Holohan et. al [34] to
avoid a known ﬁnite ﬂoating-point vulnerability of other
differential privacy implementations [57].

Randomized Response. To achieve local differential pri-
vacy for Boolean attributes, we can apply the randomized
response method from Warner [85], which is (ε = ln 3)-
differentially private [25]: (i) the client ﬂips a coin, (ii) if
heads, the client sends a truthful response, (iii) else, the
client ﬂips a coin again and sends “true” if heads and “false”
if tails. One can vary ε by changing the bias of the coin ﬂip.

3. VR Privacy Defenses

3.1. Preliminaries

In this section, we provide a differentially-private frame-
work for user data attribute protection in VR. We de-
ﬁne each attribute defense in terms of abstract coordinate
transformations, without regard to any speciﬁc method of
implementation. Later, in §4, we describe a concrete system
for implementing these defenses within VR applications.

Our “incognito mode” defenses aim to prevent adver-
saries from tracking VR users across sessions in the meta-
verse. In practice, this means limiting the number of data
attributes adversaries can reliably harvest from users and
use to infer their identity. Local differential privacy (LDP)
is the primary tool that allows us to achieve this with a
mathematically quantiﬁable degree of privacy. LDP has the
effect of signiﬁcantly widening the range of attribute values
observed by an adversary given a particular ground truth
attribute value of a user. In doing so, it ensures that the
observable attribute proﬁle of a user always signiﬁcantly
overlaps with that of at least several other users, thus making
a precise determination of identity infeasible. The noise
added by LDP may have some negative impacts on user
experience, as is the case with incognito mode in browsers.
However, users can tune the privacy parameter (ε) to reduce
the impact of noise on user experience as required.

Upon initiating a new metaverse session (i.e., connecting
to a VR server), the defenses generate a random set of
“offset” values, which are then used throughout the session
to obfuscate attributes within the VR telemetry data stream
through a set of deterministic coordinate transformations.
The re-randomization of offset values at the start of each
session ensures that all usage sessions of a user are statis-
tically unlinkable.1 On the other hand, these offsets remain
consistent within a session to ensure adversaries never re-
ceive more than one view of sensitive attribute values.

What follows are the speciﬁc differentially-private co-
ordinate transformations that protect user data attributes
(and thus allow them to “go incognito”) in VR. While for
simplicity this section considers the protections for each
attribute in isolation, in practice, our implementation uses a
relative transformation hierarchy to allow any set of enabled
defenses to seamlessly combine with each other (see §4.4).
The coordinates used throughout this paper refer to the left-
handed, Y-up Unity coordinate system, pictured in Fig. 2.

In our setting, LDP protects against adversaries with
knowledge of observed attributes across all user sessions
except for the current session of a target user (D(cid:48)). Sequen-
tial composition allows us to provide an upper bound for a
user’s privacy budget as the sum of each ε used per attribute.
We identiﬁed the Bounded Laplace mechanism [33] as
our tool of choice for protecting continuous attributes like
height, wingspan, and room size in VR because it produces
random noise centered around the sensitive value (e.g.,
height) while preserving the semantic consistency of the
attribute (e.g., height > 0). The Laplacian noise distribution
is preferable over, e.g., simply imbuing uniformly distributed
random noise, because it has the property of minimizing the
mean-squared error of any attribute at a given privacy level
(ε) [41], thereby minimizing its impact the user experience.
Where Boolean attributes are concerned, we use ran-
domized response [85] with a weighted coin to provide
ε-differential privacy for chosen values of ε. The use of
randomized response over simpler mechanisms (e.g., a sin-
gle coin ﬂip) aligns Boolean attributes with the same ε-
differential privacy framework as continuous attributes, and
thus allows the ε values of multiple attributes to be com-
bined into a single “privacy budget” if desired.

Throughout
variable notation in our algorithm statements:

this paper, we use the following standard

• v: sensitive deterministic value (“ground truth”)
• (lv, uv): population bounds of v
• ε ≥ 0: differential privacy parameter
• p: randomized response coin bias
• (xh, yh, zh): headset coordinates
• (xr, yr, zr): right controller coordinates
• (xl, yl, zl): left controller coordinates

For a given attribute a (e.g., height), we use a(cid:48) (e.g.,
height (cid:48)) to denote the LDP-protected value an adversary
observes. Our use of local differential privacy requires ∆ to
cover the entire range of the bounded interval [l, u] (∆ =
|u−l|). Alg. 1 contains helper functions for the mechanisms
discussed here that will be used throughout §3.

Algorithm 1: Preliminaries for privacy defenses.
1 Function LDPNoisyOffset(v, ε, lv, uv):
2

return BoundedLaplacianNoise(v, |uv − lv|, ε, lv, uv)

3 Function RandomizedResponse(v, p):
4

if Random(0, 1) ≤ p then

5

6

7

return v

else

return Random(0, 1) ≤ 0.5

Figure 2: Left-handed, Y-up Unity 3D coordinate system.

1. Methods for tracking users that are not unique to VR (such as via
their IP addresses) are not considered to be within the scope of this paper.

9

10

11

12

13

8 Function PolarTransform(xr, zr, xl, zl):

(cid:126)dr = (cid:104)xr, zr(cid:105) − (cid:104)

xr + xl
2
xr + xl
2

zr + zl
,
2
zr + zl
2

(cid:105)

,

(cid:126)dl = (cid:104)xl, zl(cid:105) − (cid:104)
dr, dl = | (cid:126)dr|, | (cid:126)dl|
αr, αl = ArcTan( (cid:126)drx , (cid:126)drz ), ArcTan( (cid:126)dlx , (cid:126)dlz )
return dr, dl, αr, αl

(cid:105)

ZXY3.2. Continuous Attributes

Multiplicative Offset

Using the preliminaries established above, and in partic-
ular the Bounded Laplace mechanism, we now describe co-
ordinate transformations for protecting continuous attributes
in VR. Each defense begins by calculating an oﬀset using
the LDPNoisyOffset helper function before diverging
into two distinct categories: additive offset defenses, which
protect attributes such as interpupillary distance (IPD) that
are not expected to change over the course of a session, and
multiplicative offset defenses, which protect attributes like
observed height that might be updated each frame.

Additive Offset

There are two continuous attributes that we can protect
by simply adding a ﬁxed oﬀset value to the ground truth
as a one-time transformation: interpupillary distance (IPD),
and voice pitch. The use of an additive offset is sufﬁcient
to protect these attributes without impacting usability due
to the relatively static nature of such attributes throughout
a session, with the resulting defenses being shown in Alg. 2.

IPD. We start with IPD as it is amongst the easiest attributes
to defend due to the fact that it should not reasonably
be expected to change during a session. Our suggested
countermeasure to attacks on IPD defends the player by
scaling their avatar such that when an adversary measures
the gap between their left and right eyes, the distance will
correspond to a differentially private value.

Voice Pitch. An attacker can also ﬁngerprint a VR user by
observing the median frequency of their speech as measured
by a microphone on their VR device, which they can use in
particular to infer a user’s gender in addition to simply being
a unique identiﬁer. Thus, we suggest pitch-correcting the
voice stream according to the differentially-private oﬀset.
As with IPD, the attacker can now only observe a differen-
tially private pitch +oﬀset value. Incidentally, we found that
this defense is also sufﬁcient to confuse machine learning
models which attempt to infer the user’s ethnicity based on
their accent (see §5), though that effect may be less resilient.
Studies which focus entirely on speech privacy [89] have
presented more sophisticated techniques for obfuscating
voice than the ones discussed here, but we include this
differentially-private defense for completeness given the
inclusion of speech attributes in VR attack papers [62].

We now turn our attention to the bulk of attributes
for which a multiplicative offset is required. Consider, for
example, the case of wingspan, where the perceived distance
between a user’s hands should appear to be 0 when their
hands are touching, but should reﬂect wingspan + oﬀset
when their hands are fully extended. Simply adding oﬀset to
the distance in all cases, as per the additive offset approach,
is insufﬁcient to achieve this property. Instead, we scale
the entire range of values by v(cid:48)/v as shown in Fig. 3. As
a result, observable attributes attain a differentially-private
value at their extremes, while their zero-point is maintained.
We present in this section multiplicative offset defenses for
a variety of attributes, as summarized in Alg. 3.

Figure 3: Additive vs. multiplicative offset transformations.

Height. A typical method for inferring the height of a VR
user is to record the y-coordinate of the VR headset (yh)
over the course of a session, and then use the highest
observed coordinate (or, e.g., the 99th percentile) as a direct
linear correlate of height. This attack is effective because
yh = height when a user is standing upright, which they
generally are for a large portion of their session.

While one may be tempted to simply adjust yh by oﬀset
at all times, doing so could cause the relative error of a ﬁxed
offset can grow to become disproportionate in applications
where users are required to get close to the ground. In fact,
in an extreme scenario where a user decides to lie ﬂat on the
ground, an adversary may observe y(cid:48)
h = 0 + oﬀset, which
could defeat the privacy of this method by revealing oﬀset.
Therefore, our suggested countermeasure is to use a
h = yh ∗ (height(cid:48)/height).
multiplicative offset, whereby y(cid:48)
When yh = height,
the adversary now observes the
differentially-private value y(cid:48)
h = height + oﬀset, while
y(cid:48)
h = 0 when yh = 0 as shown in Fig. 4. We also suggest
adjusting yr and yl such that the relative distance between
the user’s head and hands appears to remain unchanged.

Algorithm 2: Local differential privacy for con-
tinuous numerical attributes with additive offsets.
1 Function IPD(IPD, ε, li, ui):
2

offset = LDPNoisyOffset(IPD, ε, li, ui)
IPD (cid:48) = IPD+ offset
return IPD (cid:48)

3

4

5 Function Pitch(pitch, ε, lp, up):
6

offset = LDPNoisyOffset(pitch, ε, lp, up)
pitch(cid:48) = pitch+ offset
return pitch(cid:48)

7

8

Figure 4: Use of additive vs. multiplicative offset for height.

000offsetvvv + offsetv + offsetAdditive offsetMultiplicative offset00vv + offsetMultiplicative Offset0offsetvv + offsetAdditive OffsetSquat Depth. Prior works have shown that an adversary
can assess a proxy of a user’s physical ﬁtness by covertly
prompting the users to squat and measuring their squat
depth, i.e., depth = height − yh, where yh is the lowest
headset coordinate recorded during the squat. The aim of
this defense is to ensure that an adversary can only observe
a differentially private depth value. While this could be
achieved by setting a strict lower bound on yh, doing so has
the potential to be disorienting and could potentially have
a negative impact on the VR user experience perspective.
Instead, our suggested defense offsets yh using the following
transformation (independent of any defenses to height):

h = height − (height − yh) ∗ (depth(cid:48)/depth)
y(cid:48)

Consequently, that y(cid:48)

to height − depth + noise as yh goes from height
height − depth, obscuring the user’s actual squat depth.

h smoothly transitions from height
to

Wingspan. The wingspan attribute is harvested in a similar
way to height, with an adversary monitoring the distance d
between the left and right controllers over the course of a
usage session and using the maximum observed value of d as
a strong correlate of the user’s wingspan. A VR application
could require a user to fully extend their arms for seemingly
legitimate gaming purposes, thus revealing their wingspan
to potential attackers. The defense must therefore modify
the observed distance d when the user’s arms are extended.
However, as discussed at the start of this section, simply
adding a ﬁxed offset to d does not allow d = 0 when the
user’s hands are touching, which is desirable for UX.

In function Wingspan of Alg. 3, we formally introduce
our recommended defense, where arm R and arm L are the
arm length measurements in VR. As with our protection of
squat depth, we ensure that the noise scales smoothly to
preserve the user experience. As a result, when the user’s
hands are at the same coordinates, the observed distance
is 0; thus, when the user touches their physical hands, the
virtual hands also touch. On the other hand, when the arms
are extended completely, the real-time distances between
the controllers and their midpoint become dr = arm R and
dl = arm L, where dr + dl = span. In such a position, the
observed wingspan becomes differentially private:
span (cid:48)
span (cid:48)
2
2

dl
armL
2 − dl = span(cid:48) − (dr + dl) = oﬀset
to each arm.
Consequently, the adversary will only observe a differen-
tially private wingspan value when using the controllers’
coordinates ((xr, zr) and (xl, zl)) to calculate the distance:

dr
∗
armR
2 − dr + span(cid:48)

The defense adds half the total offset

∴ span(cid:48)

oﬀset =

− dr +

− dl

∗

|(cid:104)xr, zr(cid:105) − (cid:104)xl, zl(cid:105)| = span(cid:48)

2 + span(cid:48)

2 = span(cid:48)

As with the other multiplicative offset defenses, post-
processing immunity protects the sensitive values when
multiplied by w
v ∈ [0, 1], and the adversary can only learn
span(cid:48) from the observed distances in the range [0, span(cid:48)].

Arm Length Ratio. If an adversary manages to measure the
wingspan of a user, determining the arm length ratio is
possible by using the headset as an approximate midpoint.
As function Arms of Algorithm 3 shows, the corresponding
defense is almost equivalent to that of the user’s wingspan,
but while the wingspan protection adds noise symmetrically
to both arms, in this case, we add noise asymmetrically to
obfuscate the ratio of arm lengths.

Room Size. Lastly, previous works have demonstrated that
an adversary can determine the dimensions of a user’s play
area by observing the range of their movement. Once again,
an additive offset would fail to defend against this attack by
simply shifting the user’s position rather than affecting their
movement range. We therefore employ a similar technique
as with the other multiplicative offset transformations in that
the dynamic noise at the center of the room is 0, which
increases as the user approaches the edges of their play area.
When the user is at the center of the room, (xh, zh) =
the offsets are 0. When the user is at a cor-
(0, 0),
ner of the room, e.g., at (xh, zh) = ( width
), the
offsets become half the noise added to each room di-
mension ( Noisex
). Consequently, the adversary can
only collect the noisy room dimensions, e.g., for width:
h = xh + oﬀsetx = width/2
x(cid:48)
. Thus, the
adversary would only learn a differentially private room
dimension from observing x(cid:48)
], with
the same being true of length. Note that offsets added to xh
and zh are intentionally chosen independently so that the
adversary cannot even learn the proportions of the room.

width ∗ width(cid:48) = width(cid:48)

h in the range [0, width(cid:48)

, length
2

, Noisez
2

2

2

2

2

Security Arguments. We conclude by arguing why the mul-
tiplicative offset approach maintains differential privacy,
emphasizing that applying a ﬁxed oﬀset multiplicatively is
very different from re-sampling the random oﬀset value.

Proposition 1. Given an single individual’s ground truth
value v ∈ [l, u] collected locally once, where l and u
are the lower and upper bounds of possible values of v,
and an offset N sampled once from a differentially private
distribution, broadcasting any v(cid:48) = w
v (v + N) to a server
protects v with differential privacy, where w ∈ [0, v] is a
real-time value continuously generated locally.

Proof: Firstly, an adversary cannot learn the sensitive
value from the ratio w
v ∈ [0, 1] without knowing w. Thus,
an adversary can only learn v + N from the possible stream
of broadcasted values v(cid:48) = {0, ..., v + N} sent
to the
server. Given that N is sampled from a differentially private
distribution s.t. v +N is centered around v, v +N is immune
to post-processing and is thus differentially private [25].

To provide a concrete example, consider again the attribute
of height: v = height, v(cid:48) = height + oﬀset, w = yh. Given
that height(cid:48) is differentially private, an adversary who does
not know the user’s current yh value (between 0 and height)
will only be able to observe the current y(cid:48)
h value (between
0 and height(cid:48)), which cannot be used to ﬁnd height.

Algorithm 3: Local differential privacy for con-
tinuous attributes with multiplicative offsets.
1 Function Height(yh, yr, yl, height, ε, lh, uh):
2

height(cid:48) = height + LDPNoisyOffset(height, ε, lh, uh)
offset = yh ∗ (height(cid:48)/height) − yh
return y(cid:48)

l = yh+ offset , yr+ offset , yl+ offset

h, y(cid:48)

r, y(cid:48)

3

4

5 Function Depth(yh, yr, yl, height, depth, ε, ld, ud):
6

depth(cid:48) = depth+ LDPNoisyOffset(depth, ε, ld, ud)
oﬀset = (height − ((height − yh)/depth) ∗ depth(cid:48)) − yh
return y(cid:48)

l = yh+ offset, yr+ offset, yl+ offset

h, y(cid:48)

r, y(cid:48)

7

8

9 Function Wingspan(xr, zr, xl, zl, armR, armL, ε, lw, uw):
10

span = armR + armL
span (cid:48) = span+ LDPNoisyOffset(span, ε, lw, uw)
dr, dl, αr, αl = PolarTransform(xr, zr, xl, zl)
oﬀsetr = (dr/armR) ∗ (span (cid:48)/2) − dr
oﬀsetl = (dl/armL) ∗ (span (cid:48)/2) − dl
oﬀsetrx , oﬀsetrz = oﬀsetr ∗ cos(αr), oﬀsetr ∗ sin(αr)
oﬀsetlx , oﬀsetlz = oﬀsetl ∗ cos(αl), oﬀsetl ∗ sin(αl)
x(cid:48)
r, z(cid:48)
x(cid:48)
l, z(cid:48)
return x(cid:48)

r = xr + oﬀsetrx , zr + oﬀsetrz
l = xl + oﬀsetlx , zl + oﬀsetlz
l, z(cid:48)
l

r, x(cid:48)

r, z(cid:48)

11

12

13

14

15

16

17

18

19

29

22

23

24

25

26

27

28

20 Function Arms(xr, zr, xl, zl, armR, armL, ε, lrat, urat):
21

span = armR + armL
ratio = armR/span
ratio(cid:48) = ratio+ LDPNoisyOffset(ratio, ε, lrat, urat)
dr, dl, αr, αl = PolarTransform(xr, zr, xl, zl)
oﬀsetr = (dr/armR) ∗ span ∗ ratio(cid:48) − dr
oﬀsetl = (dl/armL) ∗ span ∗ (1/ratio(cid:48)) − dl
oﬀsetrx , oﬀsetrz = oﬀsetr ∗ cos(αr), oﬀsetr ∗ sin(αr)
oﬀsetlx , oﬀsetlz = oﬀsetl ∗ cos(αl), oﬀsetl ∗ sin(αl)
x(cid:48)
r, z(cid:48)
x(cid:48)
l, z(cid:48)
return x(cid:48)

r = xr + oﬀsetrx , zr + oﬀsetrz
l = xl + oﬀsetlx , zl + oﬀsetlz
l, z(cid:48)
l
32 Function Room(xh, zh, xr, zr, xl, zl, L, W, ε, l, u):
L(cid:48) = L+ LDPNoisyOffset(L, ε, l, u)
33
W (cid:48) = W +LDPNoisyOffset(W, ε, l, u)
oﬀsetx, oﬀsetz = (xh/W ) ∗ W (cid:48) − xh, (zh/L) ∗ L(cid:48) − zh
r, x(cid:48)
h, x(cid:48)
x(cid:48)
z(cid:48)
r, z(cid:48)
h, z(cid:48)
return x(cid:48)

l = xh + oﬀsetx, xr + oﬀsetx, xl + oﬀsetx
l = xh + oﬀsetz, zr + oﬀsetz, zl + oﬀsetz
h, x(cid:48)

h, z(cid:48)

r, z(cid:48)
l

r, x(cid:48)

r, x(cid:48)

r, z(cid:48)

l, z(cid:48)

38

37

35

36

34

30

31

3.3. Binary Attributes

We now switch our focus to attributes like handedness
which can be represented as Boolean variables. For such at-
tributes, we deploy the RandomizedResponse function
of Alg. 1. If randomized response suggests an untruthful
response, the user’s virtual avatar is mirrored for other users,
as is their view of the virtual world. While the user can still
interact with the world and other avatars normally, we found
that this approach comes at the cost of all text appearing to
be backwards absent any special corrective measures.

Handedness. An adversary may observe a user’s behavior,
e.g., which hand they use to interact with virtual objects, to
determine their handedness over time. Mirroring the user’s
avatar randomly on each VR session obfuscates handedness.

Arm Length Asymmetry. Using a mirrored avatar also pro-
vides plausible protection against adversaries observing
which arm is longer; however, there is a large degree of
overlap between this defense and that of arm length ratio.

3.4. Network Communication Attributes

Finally, we turn our attention to network-layer attributes,
namely latency, which can reveal geolocation via multilat-
eration, and throughput, which can reveal the VR device
model. Such attributes are extremely difﬁcult
to protect
with differential privacy due to their one-way boundedness;
for example, while we can add artiﬁcial delay to increase
perceived latency, there is no way to decrease the latency of
a system below its intrinsic value, which would be necessary
to provide differential privacy based on the ground truth.
Instead, we resort to clamping, which has the effect of
grouping observed attribute values into distinct clusters that
effectively anonymize users within their cluster.
Geolocation. A server attacker can observe the round-trip
delay of a signal traveling between a VR client device and
multiple servers to determine a user’s location via multilat-
eration (hyperbolic positioning). Furthermore, prior works
suggest a user attacker can also use the round-trip delay of
the target’s audio signal as a proxy for latency. In response,
our defense clamps the latency of all broadcasted signals to
a ﬁxed round-trip delay by artiﬁcially delaying each packet.
Due to the sensitivity of hyperbolic positioning, even a 1 ms
offset can skew the adversaries’ prediction by ≈ 300 km.
Reaction Time. Likewise, adversaries can measure a user’s
reaction time by timing the delay between a stimulus (e.g.,
a visual or audio cue) and the user’s response. In addition
to being a further identifying metric, reaction time is also
highly correlated with age [87]. While the technical defense
for reaction time is largely equivalent to that of geolocation,
the speciﬁc clamping values are different because the sensi-
tivity of the underlying attributes vary greatly. If the defenses
for geolocation and reaction are simultaneously enabled, the
higher latency clamp should be applied so as to protect both.
Refresh/Tracking Rate. Finally, a server attacker can use the
telemetry throughput to ascertain the VR headset’s refresh
and tracking rate and thus potentially identify the make and
model of a user’s VR device. Moreover, user attackers can
leverage a VR environment with moving objects that users
perceive differently depending on their refresh rates to deter-
mine the refresh rate of the VR display. Thus, our defenses
clamp the rate at which the VR device broadcasts its tracked
coordinates to obfuscate the true device speciﬁcations.
Summary. While our aim in this section was to be as thor-
ough as possible with regard to covering known VR privacy
attacks, we by no means claim to have comprehensively
addressed every possible VR privacy threat vector. Instead,
we hope to have accomplished two simple goals. Firstly, we
believe the combined defenses of this section are sufﬁcient
to signiﬁcantly hinder attempts to deanonymize users in the
metaverse. Within a large enough group of users, adver-
saries may have to combine dozens of unique attributes to
reliably identify individuals; the absence of the low-hanging
attributes discussed herein should obstruct their ability to
do so. Secondly, we hope that the attributes covered in this
section were diverse enough, and the corresponding defenses
ﬂexible enough, to be extended to future VR privacy threats.

Figure 5: Mixed reality photo of a player using “MetaGuard,” our implementation of incognito mode for VR.

(C) Privacy Slider. Lastly, we present users with a “privacy
level” slider that adjusts the privacy parameter (ε) for each
defense, allowing users to dynamically adjust the inherent
trade-off between privacy and accuracy when using the
defenses of §3. Users can choose from the following options:
• High Privacy, intended for virtual telepresence applica-

tions such as VRChat [35] and others [53], [51].

• Balanced, intended for casual gaming applications, such

as virtual board games requiring some dexterity [3].

• High Accuracy, intended for noise-sensitive competitive

gaming applications [73] such as Beat Saber [10].

We generally refer to these options simply as the “low,”
“medium,” and “high” privacy settings.

4. VR Incognito Mode

In this section, we introduce “MetaGuard,”2 our practical
implementation of the defenses presented in §3 and the
ﬁrst known “incognito mode” for the metaverse. We built
MetaGuard as an open-source Unity (C#) plugin that can
easily be patched into virtually any VR application using
MelonLoader [2].3 We begin by describing the options and
interface made available to MetaGuard users. We then dis-
cuss our choice of DP parameters (ε, attribute bounds, etc.)
and outline how MetaGuard calibrates noise to each user.
Finally, we describe the concrete game object transforma-
tions applied to the virtual world to implement the defenses
of §3. Fig. 5 shows a mixed reality photo of a player using
the MetaGuard VR plugin within a virtual world.

4.1. Settings & User Interface

The main objective of MetaGuard is to protect VR
user privacy while minimizing usability impact. The ﬂexible
interface of MetaGuard (shown in Fig. 6) reﬂects this goal,
allowing users to tune the defense proﬁle according to their
preferences and to the needs of the particular VR application
in use. Speciﬁcally, we expose the following options:

(A) Master Toggle. The prominent master switch allows
users to “go incognito” at the press of a button, with safe
defaults that invite (but don’t require) further customization.

(B) Feature Toggles. The feature switches allow users to
toggle individual defenses according to their needs; e.g., in
a game like Beat Saber [10], users may wish to disable
defenses that interfere with gameplay (i.e., wingspan and
arm lengths), while keeping other defenses enabled.

2. Short for “Metaverse Guard.”
3. Unlike mobile apps, desktop VR apps can be modiﬁed by end users.

Figure 6: VR user interface of MetaGuard plugin.

ABC4.2. Selecting ε-Values & Attribute Bounds

As discussed in §2.4, the level of privacy provided by
the defenses of §3 depends on the appropriate selection of
DP parameters, namely ε, ∆, and attribute bounds. Although
our approach in MetaGuard is to allow users to adjust the
privacy parameter (ε) according to their preferences, we
must nevertheless translate the semantic settings of “low,“
“medium,“ and “high“ privacy into concrete ε-values, noting
that a given privacy level may translate to a different ε-
value for each attribute depending on its sensitivity to noise.
Furthermore, the speciﬁc lower bound (l) and upper bound
(u) of each attribute (and thus ∆ = |u − l|) must be
determined in order to use the Bounded Laplace mechanism.
This section outlines our method of selecting these values,
with the results shown in Tab. 2.

Selecting ε-Values & Clamps

Continuous Anthropometrics. We conducted a small em-
pirical analysis to select appropriate ε-values for each of the
continuous anthropometric attributes at each privacy level.
We began by selecting three VR applications (VRChat [35],
Tabletop Simulator [3], and Beat Saber [10]) that represent
the most popular examples of the intended use cases for the
high, medium, and low privacy modes respectively. We then
tested a wide range of ε-values for each attribute in each
application while monitoring their effect on usability. For
example, in Beat Saber, we had both a novice and expert-
level player complete the same challenges at different ε-
values to evaluate the impact of noise on in-game perfor-
mance. By contrast, in VRChat, we were simply interested
in the impact of noise on the ability to hold a conversation
(e.g., to maintain virtual “eye contact”).

attribute, the vast majority of privacy beneﬁt is already
realized at ε = 1. We combined these results with the
ﬁndings of our usability analysis to produce the ﬁnal ε-
values shown in Tab. 2 according to the appropriate balance
of privacy and usability for the intended use of each level.

Binary Anthropometrics. For attributes where the defenses
of §3 suggest the use of randomized response, we selected ε-
values such that the corresponding prediction accuracy was
degraded by 15%, 50% and 85% at the low, medium, and
high privacy levels respectively.

Voice. Although technically a continuous anthropometric,
vocal frequency cannot be calibrated via playthroughs due
to the lack of a tangible impact on gameplay performance.
Instead, we selected ε-values which degraded inference of
gender by roughly 25%, 50% and 75% at the low, medium,
and high privacy levels respectively.

Clamps. Finally, for attributes where the corresponding de-
fense of §3 suggests clamping, we chose clamp values which
have the effect of anonymizing users within progressively
larger groups. For example, for refresh/tracking rate, we
selected clamps which hide users within the set of high
(90Hz [29]), medium (72Hz [52]), and low (60Hz [74])
ﬁdelity VR devices. For the latency-related attributes, we
selected values below the perceptible 100ms threshold [13],
[56], [60] that signiﬁcantly decreased prediction accuracy.

Selecting Attribute Bounds

Finally, beyond ε, the Bounded Laplace mechanism also
requires attribute bounds to constrain the outputs to seman-
tically consistent values. We used public datasets to obtain
the 95th percentile bounds for anthropometric measurements
[14], [21], [69], [72]; our use of local DP causes ∆ to reﬂect
the full range of possible values. For room size, we extracted
the bounds from ofﬁcial VR setup speciﬁcations [84]. We
list the bounds and corresponding references in Tab. 2.

Data Point

Height [14]
IPD [21]
Voice Pitch [69]
Squat Depth [62]
Wingspan [72]
Arm Ratio [62]
Room Size [84]
Handedness
Latency (Geolocation)
Reaction Time
Refresh/Tracking Rate

Bounds

Privacy Levels

Upper
1.826m

Low
Lower
(cid:15)=5
1.496m
55.696mm 71.024mm (cid:15)=5
(cid:15)=6
85 Hz
(cid:15)=5
0m
(cid:15)=3
1.556m
(cid:15)=3
0.95
(cid:15)=0.1
0m
(cid:15)=1.28
0
25ms
Clamped
10ms
Clamped
90 Hz
Clamped

255 Hz
0.913m
1.899m
1.05
5m
1

Medium High
(cid:15)=3
(cid:15)=3
(cid:15)=1
(cid:15)=3
(cid:15)=1
(cid:15)=1
(cid:15)=1
(cid:15)=0.88
30ms
20ms
72 Hz

(cid:15)=1
(cid:15)=1
(cid:15)=0.1
(cid:15)=1
(cid:15)=0.5
(cid:15)=0.5
(cid:15)=3
(cid:15)=0.73
50ms
100ms
60 Hz

Figure 7: Coefﬁcients of determination of height from pre-
dictions on actual vs. noisy data as ε increases.

Next, we analyzed the concrete privacy impact of can-
didate ε choices by simulating attackers at a variety of ε-
values. For example, Fig. 7 illustrates that for the height

TABLE 2: Selected ε, clamps, and attribute bound values.

We emphasize that the sole purpose of our informal
experimentation in this section is to set a reasonable range
of ε-values that cover a variety of VR use cases. Given
the lack of consensus on a formal method for selecting DP
parameters [23], our choices simply serve to establish a plau-
sible spectrum of ε-values corresponding to our perceived
boundaries of the privacy-usability trade-off. The power to
select exactly which point on this spectrum is best suited
for a particular application remains with the end user.

1021011001010.00.20.40.60.81.0Population mean of R²Prediction on noisy dataPrediction on actual data4.3. Ground Truth Calibration & Noise Centering

One ﬁnal parameter is required to successfully imple-
ment the continuous attribute defenses of §3: the ground
truth attribute values of the end user. Centering the Laplacian
noise distribution around the ground truth attribute values of
the current user has the effect of minimizing noise for as
many users as possible, particularly those who are outliers.
To achieve this, the MetaGuard extension calculates in-
stantaneous ground truth estimates upon instantiation using
the OpenVR
the method shown in Fig. 8. Speciﬁcally,
API [81] provides MetaGuard with one-time snapshot lo-
cations of the user’s head, left and right eyes, left and right
hands, and a plane representing the play area. Estimates for
the ground truth values of height, wingspan, IPD, room size,
and left and right arm lengths can then be derived from these
measurements. We note that the privacy of MetaGuard is not
dependent on the accuracy of the ground truth estimates,
which exist only to ensure that the added noise is not more
than is necessary to protect a given user.

Figure 9: Game object hierarchy with existing (dark grey)
and inserted (light grey) game objects, and coordinate trans-
formations used to implement VR Incognito Mode defenses.

Setup Phase. When a defense is ﬁrst enabled, the Meta-
Guard system uses the calibration procedures of §4.3 to
estimate the ground truth attribute values of the user. These
ground truth values are then used in combination with the
ε-values and bounds of §4.2 to calculate noisy offsets cor-
responding to each privacy level using the methods outlined
in §3, and are then immediately discarded from program
memory (with only offsets retained) so as to minimize the
chance of unintentional data leakage. By default, the Unity
game engine uses telemetry data from OpenVR [83] to
position game objects within a virtual environment, which
are then manipulated by a VR application. During the setup
phase, the system modiﬁes the game object hierarchy by
inserting intermediate “offset” objects as shown in Fig. 9.
Update Phase. During the update phase, the system ﬁrst
checks which defenses the user has enabled in the interface
(see §4.1). For all disabled attributes, the corresponding
offset
transformations in the game object hierarchy (as
shown in Fig. 9) are set to the identity matrix. For each
enabled feature, the system implements the corresponding
defense of §3 by fetching the noisy attribute value calculated
during the setup phase for the currently-selected privacy
level and enabling the relevant coordinate transformation
on one or more of the inserted offset objects such that
the observable attribute value matches the noisy attribute
value. Speciﬁcally, Fig. 9 illustrates how the position of each
game object is deﬁned with respect to another object in the
hierarchy, and how the defenses modify the relative position
or scale of each object with respect to its parent.

Figure 8: Instantaneous calibration of ground truth for height
(H), left arm (LA), right arm (RA), wingspan (W), IPD (I),
room width (RW), and room length (RL), using head (H),
ﬂoor (F), left/right controllers (L/R); ﬁgure not to scale.

4.4. Defense Implementation

We now ﬁnally provide a complete description of our
“VR Incognito Mode” system for implementing the defenses
of §3 in light of the interface, ε-values, bounds, and cal-
ibration procedures described above. Our implementation
follows two phases: a setup phase, which executes exactly
once on the frame when a defense is enabled, and an update
phase, which executes every frame thereafter.

LRHFLARAHWLEREIRWRLHead OffsetOriginHeadLeft OffsetRight OffsetLeftHandRight HandBody OffsetxP,zP: 1WorldxS: 2yP: 3yP: 4xP,zP: 5xP,zP: 6xP,zP: 5xP,zP: 6xS: 7xP,yP,zP: AxP,yP,zP: BxP,yP,zP: CTransformations (Defenses)1.Room Size Transform2.Binary Transform3.Height Transform4.Fitness Transform5.Wingspan Transform6.Arm Transform7.IPD TransformKeycP: Coordinate positioncS: Coordinate scaleTracking (Telemetry)A.Head TrackingB.Left Hand TrackingC.Right Hand TrackingAttribute

Height

Physical Fitness

IPD (Vive Pro 2)

IPD (All Devices)

Wingspan

Room Size

Longer Arm

Handedness

Geolocation

Table 3A: Primary and Secondary Attributes

Metric
Within 5cm
Within 7cm
R²
Categorical
Within 0.5mm
R²
Within 0.5mm
R²
Within 7cm
Within 12cm
R²
Within 2m²
Within 3m²
R²
≥ 1cm Difference
≥ 3cm Difference
Categorical
Within 400km
Within 500km
Categorical
Within 3 Hz

No Privacy
70%
100%
0.79
90%
96%
0.991
87%
0.857
87%
100%
0.669
78%
97%
0.974
63%
100%
97%
50%
90%
87.50%
100%
100%
100%

Low Privacy
53.07% ±2.41%
68.6% ±2.18%
0.37 ±0.040
86.11% ±2.65%
18.53% ±1.76%
0.399 ±0.041
19.47% ±1.81%
0.318 ±0.038
53.93% ±3.61%
78.80% ±2.76%
0.134 ±0.042
22.11% ±2.85%
33.52% ±3.80%
0.406 ±0.153
58.63% ±5.79%
77.78% ±13.46%
92.5%
0%
6.66%
79.20%
0%
0%
10%

Reaction Time
HMD Refresh Rate
Tracking Refresh Rate Within 2.5 Hz
VR Device

Categorical

Attribute

Voice

Gender
Age
Ethnicity
Income
Identity

Metric
Gender
Ethnicity
Categorical
Within 1yr
Categorical
Within $10k
Identity

Table 3B: Inferred Attributes

No Privacy
97%
63%
100%
100%
100%
100%
100%

Low Privacy
72.5% ±15%
52.5% ±7.5%
76.5% ±1.29%
41.75% ±1.65%
51.25% ±2.70%
26.15% ±1.41%
5.44% ±0.68%

Medium Privacy High Privacy
32.63% ±2.3%
45.00% ±2.35%
44.47% ±2.43%
58.17% ±2.09%
0.06 ±0.020
0.22 ±0.035
61.56% ±4.15%
79.11% ±2.60%
11.10% ±1.24%
13.40% ±1.33%
0.068 ±0.019
0.165 ±0.031
12.17% ±1.26%
14.17% ±1.35%
0.068 ±0.017
0.134 ±0.027
40.80% ±2.80%
42.13% ±3.32%
65.46% ±3.14%
66.00% ±3.31%
0.036 ±0.021
0.047 ±0.019
12.66% ±2.98%
16.33% ±2.74%
19.53% ±2.92%
23.44% ±3.08%
0.360 ±0.136
0.495 ±0.171
54.90% ±5.12%
52.35% ±6.83%
53.33% ±15.64%
62.22% ±15.09%
57.5%
75%
0%
0%
0%
0%
54.20%
62.50%
0%
0%
0%
0%
0%
0%

Medium Privacy High Privacy
65% ±15%
40% ±5%
70.47% ±1.85%
36.09% ±1.87%
40.75% ±2.36%
28.00% ±1.87%
4.59% ±0.76%

61.25% ±13.75%
32.5% ±0.5%
57.19% ±2.20%
24.28% ±1.87%
31.37% ±2.40%
26.06% ±2.11%
4.0% ±0.67%

TABLE 3: Main Results (accuracy and R² values with 99% conﬁdence intervals)

5. Evaluation & Results

In this section, we demonstrate the effectiveness of the
defenses introduced in §3 by evaluating their impact on the
accuracy of a theoretical attacker. To do so, we faithfully
replicated the attacks of the MetaData [62] and TTI [54]
studies to measure their accuracy both with no defenses and
with the MetaGuard extension at the low, medium, and high
privacy levels. The results of this evaluation are summarized
in Tab. 3. The presented accuracy values represent what a
server attacker could achieve, and also provide an upper
bound for the capabilities of user attackers.

5.1. Evaluation Method

We obtained anonymized frame-by-frame telemetry data
recordings of the 30 user sessions from the MetaData study.
Using this data, we could virtually “replay” the original
sessions exactly as they occurred, and were able to repro-
duce the anthropometric, environmental, and device-related
attacks described in the MetaData study with nearly identical

results. Next, we repeated this process thrice more for each
session with MetaGuard enabled at the low, medium, and
high privacy levels. The resulting decrease in attack accuracy
for each attribute at each privacy level is shown in Tab. 3A.
To emulate a realistic metaverse threat environment, we
streamed telemetry data from the client to a remote game
server via a WebSocket. The MetaGuard extension was
allowed to clamp the bandwidth and latency of this data
stream as discussed in §3. The network-related attacks were
then run on the server side to emulate a true server adversary.
Beyond the attacks which deterministically harvest sen-
sitive data attributes, both the MetaData and TTI studies
propose attacks using machine learning to identify users
or proﬁle their demographics. We used Azure Machine
Learning [1] to replicate the published methods as closely as
possible, using the same model types and metaparameters as
in the original papers. Once again, we replicated the original
results with nearly identical accuracy, with the decrease in
accuracy corresponding to the use of the low, medium, and
high privacy levels of MetaGuard being shown in Tab. 3B.

5.2. Ethical Considerations

6. Discussion

Other than the ε-calibration effort described in §4.2,
which was performed by the authors, this paper does not
involve any original research with human subjects. Instead,
our results rely on the replication of prior studies using
anonymous data obtained either from public online repos-
itories or directly from the authors of those studies. We
veriﬁed that all original studies from which we obtained
data were non-deceptive and were each subject to individual
ethics review processes by OHRP-registered institutional re-
view boards. Furthermore, the informed consent documents
of those studies explicitly included permission to re-use
collected data for follow-up studies, and we strictly fol-
lowed the data handling requirements of the original consent
documentation, such as the requirements to only publish
statistical aggregates rather than individual data points.

5.3. Primary & Secondary Attributes
Continuous Anthropometrics. Tab. 3A shows that our
defenses effectively reduce the coefﬁcients of determination
to values below 0.5 for the targeted continuous attributes.
We found that physical ﬁtness (squat depth) is the most
challenging attribute to protect while preserving user expe-
rience, as it shows the smallest drops in prediction accuracy.
The remaining attributes show signiﬁcant decreases in attack
accuracy even at the low privacy level: IPD (−67.53%),
room size (−55.89% within 2m²), wingspan (−33.07%
within 7 cm) and height (−16.93% within 5 cm).
Binary Anthropometrics. An advantage of the randomized
response technique is the precise control a practitioner can
have over accuracy levels by choosing the values of ε.
Unsurprisingly, the prediction accuracy of handedness was
92.5%, 75%, and 57.5% for the low, medium, and high
privacy levels, as we had chosen ε-values accordingly.
Network Attributes. Tab. 3A shows that the prediction ac-
curacy of the attributes dependent on latency and throughput
dramatically dropped thanks to clamping (except for reac-
tion time, which showed a modest accuracy drop of 8.3% at
a low privacy level). Altogether, the low accuracy of these
predictions signiﬁcantly impedes the ability of adversaries
to determine which VR device an individual is using.

5.4. Inferred Attributes

The machine learning models of the MetaData study
primarily use the attributes discussed above as model inputs
to infer demographics. Clearly, the reduction in accuracy of
these primary attributes will have a negative impact on the
accuracy of inferences based on them; nonetheless, we ran
the models on the noisy attributes to quantify this impact.
The results show signiﬁcant accuracy drops in predicting
gender (−23.5%), age (−58.25%), ethnicity (−48.75%),
and income (−73.85%), even at the lowest privacy setting.
Most importantly, the TTI identiﬁcation model simulating an
attacker identifying a user amongst the group of 30 had a
signiﬁcant drop in accuracy (−94.56%) at the lowest privacy
setting. We thus deem MetaGuard successful at its primary
goal of preventing users from being deanonymized.

to design,

In this study, we set out

implement, and
evaluate a comprehensive suite of VR privacy defenses to
protect VR users against a wide range of known attacks.
In the absence of any defenses, these attacks demonstrated
the ability to not only infer speciﬁc sensitive attributes
like height, IPD, and wingspan, but also to combine these
attributes to infer demographics like age, gender, ethnicity,
and income, and even to deanonymize users entirely, with
a striking degree of accuracy.

Through our evaluation of MetaGuard, our practical
implementation of a “VR incognito mode” plugin, we have
demonstrated that ε-differential privacy can pose an effective
countermeasure to such attacks. Our results show a consider-
able accuracy reduction in both the primary data attributes
presented in Tab. 3A and the identiﬁcation and proﬁling
of users as shown in Tab. 3B. While the original Meta-
Data study claimed that users could be uniquely identiﬁed
amongst a group of 30 based on height and wingspan alone,
the results our evaluation show that even the combination
of 10 attributes is insufﬁcient to reliably deanonymize users
when MetaGuard is enabled. These results are more sig-
niﬁcant still when considered in light of the “worst-case
scenario” presented by the ultra-precise high-ﬁdelity VR
systems used in the original MetaData study.

internet,

The importance of privacy-enhancing software like
MetaGuard will become more pronounced as current market
trends make virtual reality increasingly ubiquitous and shape
the next generation of the social
the so-called
“metaverse” [61], [70], [75]. The metaverse raises a series of
concerns among researchers and users alike, with countless
works already warning about the implications and ethical
issues of converging social networks with VR [66], [7], [64],
[40], [86], [47]. As it stands, some VR device manufacturers
have been observed selling VR hardware at losses of up
to $10 billion per year [68], presumably with the goal
of recouping this investment through software-based after-
sale revenue, such as via surveillance advertisement [15],
[7]. But despite using the terms “attacker,” and “adversary”
throughout our writing, it’s likely that such actions would
in practice be entirely above board, with users agreeing
(knowingly or otherwise) to have their data collected. It
is thus more important than ever to give users the ability
to protect their data through purely technological means,
independent of any warranted data privacy regulations, and
to do so in a way that is as easy to use as the privacy tools
they have become accustomed to using on the web.

Limitations. Our decision to base our evaluation on data
from prior studies means that we inherit the biases of the
original studies. In particular, the test subjects of the Meta-
Data study from which the majority of our data is derived
were not perfectly representative of the general population
of VR users; for example, there was only one left-handed
individual, most participants were students, and there were
more men than women. Furthermore, while our evaluation
method does precisely replicate the telemetry stream that

would have been generated by the original participants were
they using the MetaGuard extension, it does so under the
assumption that their use of MetaGuard would not have
changed their behavior. The accuracy of MetaGuard could
be somewhat diminished in practice if it turns out that users
modify their behavior to compensate for the added noise.

Future Work. Lacking access to VR device ﬁrmware, we
implemented the MetaGuard extension described in this
paper at the client software layer, providing an effective
defense against server and user attackers. In future work,
we believe the same defenses could be easily applied at
the ﬁrmware level, allowing data to also be protected from
client attackers. However, protecting data from hardware
likely require entirely
or ﬁrmware-level adversaries will
different methods to the ones presented in this paper.

While our aim in this paper was to be as comprehensive
as possible when addressing VR privacy attacks, there were
a few niche VR hardware features that we speciﬁcally
excluded. Future systems could extend the techniques of this
paper to less common VR accessories, such as pupil tracking
and full-body tracking systems, that we did not address in
this work. Moreover, we think it is necessary to enlarge the
body of known VR privacy attack vectors, and hope the
framework of the MetaGuard extension is modular enough
to support implementation of their corresponding defenses.
An important aspect of the MetaGuard system is the
ability for users to toggle individual VR defenses according
to the requirements of the application being used. While
this process is entirely manual in our implementation, in the
future, the “incognito mode” system could be conﬁgured to
automatically proﬁle VR applications and determine which
defenses are appropriate for a given scenario. Furthermore,
the application could incorporate the differential privacy
concept of a “privacy budget,” adding more noise to enabled
attributes to compensate for the privacy loss of disabled
attributes and maintain the same level of overall anonymity.
Finally, our method of selecting ε-values was somewhat
informal, in part due to the lack of a quantitative metric of
usability impact for noise in VR. We therefore encourage
HCI researchers to produce works which rigorously quantify
the impact of adding noise to various attributes on the VR
user experience, so as to better understand the cost vs.
beneﬁt of noisy mechanisms like differential privacy in VR.

7. Related Work

We analyzed a large number of VR security and privacy
literature reviews [7], [49], [28], [11], [44], [61], [78],
[19], [66], [77], [17] to asses the current state of the art
with respect to metaverse privacy attacks and defenses. The
variety of attacks mentioned in these works were a major
motivation for producing this paper, as discussed in §2.1.

With respect to defenses, there are a limited number
of studies proposing the use of differential privacy in VR.
Related works have primarily focused on using differential
privacy to protect eye-tracking data [76], [48], [16], [36]
without regard to other types of VR telemetry. For example,

Steil et al. [76] and Ao et al. [48] use differential privacy
to protect visual attention heatmaps, while Johnn et al. [36]
proposes the use of “snow” pixels to obscure the iris signal
and prevent spooﬁng while preserving gaze.

A few of the defenses proposed in this paper have also
previously been discussed outside the context of VR. For
example, Avery et al. [9] discuss defenses against attacks
inferring handedness in the context of mobile devices, and
Sun et. al [89] has proposed countermeasures to inferring
attributes from speech in mobile and desktop applications.
During our review, we also identiﬁed several works
which further motivate the need for a comprehensive “incog-
nito mode” system such as the one presented in this paper.
In particular, the survey of Adams et. al [7] highlighted
the consensus view of VR application developers that the
privacy risks in VR are higher than in other platforms.
Maloney et.al. [49] showed that users believe that giving
up biometric information is inevitable in VR applications, a
view that our study hopes to dispel.

In summary, MetaGuard ﬁlls an important gap in the
VR privacy landscape, not only by being the ﬁrst to de-
fend various anthropometric, environmental, demographic,
and device attributes, but also in general by presenting a
comprehensive usable metaverse privacy solution rather than
focusing on any one particular data point.

8. Conclusion

In this paper, we have presented the ﬁrst comprehensive
“incognito mode for VR.” Speciﬁcally, we designed a suite
of defenses that quantiﬁably obfuscate a variety of sensitive
user data attributes with ε-differential privacy. We then
implemented these defenses as a universal Unity VR plugin
that we call “MetaGuard.” Our implementation, which is
compatible with a wide range of popular VR applications,
gives users the power to “go incognito” in the metaverse
with a single click, with the ﬂexibility of adjusting the
defenses and privacy level as they see ﬁt.

Upon replicating well-known VR privacy attacks us-
ing real user data from prior studies, we demonstrated a
signiﬁcant decrease in attacker capabilities across a wide
range of metrics. In particular, the ability of an attacker to
deanonymize a VR user was degraded by 94.6–96.0% while
using the MetaGuard extension.

Over the course of decades of research in web privacy,
private browsing mode has remained amongst the most ubiq-
uitous privacy tools in popular use today. We were inspired
by the success of “incognito mode” on the web to produce
a metaverse equivalent that is just as user-friendly, while
serving the same fundamental purpose of helping users
remain untraceable across multiple sessions. We hope our
open-source MetaGuard plugin and promising results serve
as a foundation for other privacy practitioners to continue
exploring usable privacy solutions in this important ﬁeld.

Acknowledgments

We thank Christopher Harth-Kitzerow, Sriram Sridhar,
Xiaoyuan Liu, Lun Wang, and Syomantak Chaudhuri for
their feedback. This work was supported in part by the Na-
tional Science Foundation, by the National Physical Science
Consortium, and by the Fannie and John Hertz Foundation.
Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the authors and
do not necessarily reﬂect the views of the supporting entities.
We sincerely thank the ITT and MetaData study participants
for making this work possible.

Availability

Our public GitHub organization hosts open-source
repositories for MetaGuard [63], such as Unity (C#) scripts
implementing defenses, “incognito mode” plugins for VR
applications (including Beat Saber, Tabletop Simulator, and
VR Chat), and local and remote evaluation scripts:

https://github.com/MetaGuard

References

[1] Azure Automated Machine Learning - AutoML | Microsoft Azure.

[2] Melonloader. https://melonwiki.xyz. Online.

[3] Tabletop Simulator). https://www.tabletopsimulator.com. Online.

[4] National Tracking Poll 2203015. page 137, 2022.

[5] Targeted deanonymization via the cache side channel: Attacks and
In 31st USENIX Security Symposium (USENIX Security

defenses.
22), Boston, MA, August 2022. USENIX Association.

[6] Brave Software, Inc. Brave. https://brave.com/. Online; accessed 21

July 2022.

[7] Devon Adams, Alseny Bah, Catherine Barwulor, Nureli Musaby,
Kadeem Pitkin, and Elissa M. Redmiles. Ethics emerging: the story
of privacy and security perceptions in virtual reality. In Fourteenth
Symposium on Usable Privacy and Security (SOUPS 2018), pages
427–442, Baltimore, MD, August 2018. USENIX Association.

[8] Gaurav Aggarwal, Elie Bursztein, Collin Jackson, and Dan Boneh.
In
An analysis of private browsing modes in modern browsers.
Proceedings of the 19th USENIX Conference on Security, USENIX
Security’10, page 6, USA, 2010. USENIX Association.

[9]

Jeff Avery, Daniel Vogel, Edward Lank, Damien Masson, and Hanae
Rateau. Holding patterns: detecting handedness with a moving
In Proceedings of the 31st Conference on
smartphone at pickup.
l’Interaction Homme-Machine - IHM ’19, pages 1–7, Grenoble,
France, 2019. ACM Press.

[10] Beat Games. Beat Saber. https://beatsaber.com/. Online; accessed 13

July 2022.

[11] Kent Bye, Diane Hosfelt, Sam Chase, Matt Miesnieks, and Taylor
Beck. The ethical and privacy implications of mixed reality. In ACM
SIGGRAPH 2019 Panels, SIGGRAPH ’19, New York, NY, USA,
2019. Association for Computing Machinery.

[12] Aaron Cahn, Scott Alfeld, Paul Barford, and S. Muthukrishnan. An
empirical study of web cookies. In Proceedings of the 25th Interna-
tional Conference on World Wide Web, WWW ’16, page 891–901,
Republic and Canton of Geneva, CHE, 2016. International World
Wide Web Conferences Steering Committee.

[13] Stuart K. Card, George G. Robertson, and Jock D. Mackinlay. The
information visualizer, an information workspace. In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems,
CHI ’91, page 181–186, New York, NY, USA, 1991. Association for
Computing Machinery.

[14] Center of Disease Control and Prevention. Percentile Data Files with
LMS Values. https://www.cdc.gov/growthcharts/percentile data ﬁles.
htm. Online; accessed 17 July 2022.

[15] Matthew Crain. Proﬁt Over Privacy. Minneapolis: University of

Minnesota Press, 2021.

[16] Brendan David-John, Diane Hosfelt, Kevin Butler, and Eakta Jain.
A privacy-preserving approach to streaming eye-tracking data. IEEE
Transactions on Visualization and Computer Graphics, 27(5):2555–
2565, 2021.

[17] Jaybie A. De Guzman, Kanchana Thilakarathna, and Aruna Senevi-
ratne. Security and privacy approaches in mixed reality: A literature
survey. 52(6):1–37.

[18] Jaybie A. De Guzman, Kanchana Thilakarathna, and Aruna Senevi-
ratne. Security and privacy approaches in mixed reality: A literature
survey. ACM Comput. Surv., 52(6), oct 2019.

[19] Ellysse Dick. Balancing user privacy and innovation in augmented

and virtual reality. page 28.

[20] Roger Dingledine, Nick Mathewson, and Paul F. Syverson. Tor: The
In USENIX Security Symposium,

second-generation onion router.
2004.

[21] Neil A Dodgson. Variation and extrema of human interpupillary
In Stereoscopic displays and virtual reality systems XI,

distance.
volume 5291, pages 36–46. SPIE, 2004.

[22] Duck Duck Go, Inc. Duck Duck Go.

https://duckduckgo.com/.

Online; accessed 21 July 2022.

[23] Cynthia Dwork, Nitin Kohli, and Deirdre Mulligan. Differential
Privacy in Practice: Expose your Epsilons! Journal of Privacy and
Conﬁdentiality, 9(2), 2019.

[24] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
Calibrating noise to sensitivity in private data analysis.
In Shai
Halevi and Tal Rabin, editors, Theory of Cryptography, pages 265–
284, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. Online;
accessed 30 December 2021.

[25] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of
Differential Privacy. Foundations and Trends in Theoretical Computer
Science, 9(3-4):211–407, 2013.

[26] Jide S. Edu, Jose M. Such, and Guillermo Suarez-Tangil. Smart Home
Personal Assistants: A Security and Privacy Review. ACM Computing
Surveys, 53(6):1–36, November 2021. arXiv:1903.05593 [cs].

[27] Steven Englehardt, Dillon Reisman, Christian Eubank, Peter Zim-
merman, Jonathan Mayer, Arvind Narayanan, and Edward W. Felten.
Cookies that give you away: The surveillance implications of web
In Proceedings of the 24th International Conference on
tracking.
World Wide Web, pages 289–299. International World Wide Web
Conferences Steering Committee.

[28] Ben Falchuk, Shoshana Loeb, and Ralph Neff. The social metaverse:
Battle for privacy. IEEE Technology and Society Magazine, 37(2):52–
61, 2018.

[29] Gamespot. Valve and HTC Reveal Vive VR Headset.

https:

//www.gamespot.com/articles/valve-and-htc-reveal-vive-vr-headset/
1100-6425606/. Online; accessed 17 July 2022.

[30] Roberto Gonzalez, Claudio Soriente, Juan Miguel Carrascosa, Alberto
Garcia-Duran, Costas Iordanou, and Mathias Niepert. User proﬁling
In Proceedings of the 17th International
by network observers.
Conference on Emerging Networking EXperiments and Technologies,
CoNEXT ’21, page 212–222, New York, NY, USA, 2021. Association
for Computing Machinery.

[31] Aniket Gulhane, Akhil Vyas, Reshmi Mitra, Roland Oruche, Gabriela
Hoefer, Samaikya Valluripally, Prasad Calyam, and Khaza Anuarul
Hoque.
Security, privacy and safety risk assessment for virtual
reality learning environment applications. In 2019 16th IEEE Annual
Consumer Communications Networking Conference (CCNC), pages
1–9, 2019.

[32] Hana Habib, Jessica Colnago, Vidya Gopalakrishnan, Sarah Pear-
man, Jeremy Thomas, Alessandro Acquisti, Nicolas Christin, and
Lorrie Faith Cranor. Away from prying eyes: Analyzing usage
In Fourteenth Symposium
and understanding of private browsing.
on Usable Privacy and Security (SOUPS 2018), pages 159–175,
Baltimore, MD, August 2018. USENIX Association.

[33] Naoise Holohan, Spiros Antonatos, Stefano Braghin, and P´ol
Mac Aonghusa. The Bounded Laplace Mechanism in Differential
Privacy. Journal of Privacy and Conﬁdentiality, 10(1), December
2019.

[34] Naoise Holohan and Stefano Braghin. Secure Random Sampling in
Differential Privacy. In Elisa Bertino, Haya Shulman, and Michael
Waidner, editors, Computer Security – ESORICS 2021, volume
12973, pages 523–542. Springer International Publishing, Cham,
2021. Series Title: Lecture Notes in Computer Science.

[35] VRChat Inc. Vrchat. https://hello.vrchat.com/. Online; accessed 17

May 2022.

[36] Brendan John, Ao Liu, Lirong Xia, Sanjeev Koppal, and Eakta Jain.
Let it snow: Adding pixel noise to protect the user’s identity.
In
ACM Symposium on Eye Tracking Research and Applications, ETRA
’20 Adjunct, New York, NY, USA, 2020. Association for Computing
Machinery.

[37] Nesrine Kaaniche, Maryline Laurent, and Sana Belguith. Privacy
enhancing technologies for solving the privacy-personalization para-
Journal of Network and Computer
dox: Taxonomy and survey.
Applications, 2020.

[38] Ishan Karunanayake, Nadeem Ahmed, Robert Malaney, Raﬁqul Is-
lam, and Sanjay K. Jha. De-anonymisation attacks on tor: A survey.
IEEE Communications Surveys & Tutorials, 23(4):2324–2350, 2021.

[39] Christina Katsini, Yasmeen Abdrabou, George E. Raptis, Mohamed
Khamis, and Florian Alt. The role of eye gaze in security and
privacy applications: Survey and future hci research directions.
In
Proceedings of
the 2020 CHI Conference on Human Factors in
Computing Systems, CHI ’20, page 1–21, New York, NY, USA, 2020.
Association for Computing Machinery.

[40] Orin S Kerr. Criminal law in virtual worlds. page 17, 2008.

[41] Fragkiskos Koufogiannis, Shuo Han, and George J. Pappas. Optimal-

ity of the laplace mechanism in differential privacy, 2015.

[42] Jacob Leon Kr¨oger, Otto Hans-Martin Lutz, and Florian M¨uller. What
Does Your Gaze Reveal About You? On the Privacy Implications of
In Michael Friedewald, Melek ¨Onen, Eva Lievens,
Eye Tracking.
Stephan Krenn, and Samuel Fricker, editors, Privacy and Identity
Management. Data for Better Living: AI and Privacy: 14th IFIP WG
9.2, 9.6/11.7, 11.6/SIG 9.2.2 International Summer School, Windisch,
Switzerland, August 19–23, 2019, Revised Selected Papers, pages
226–241. Springer International Publishing, Cham, 2020.

[43] Pierre Laperdrix, Nataliia Bielova, Benoit Baudry, and Gildas Avoine.
Browser ﬁngerprinting: A survey. ACM Trans. Web, 14(2), apr 2020.

[44] Ronald Leenes. Privacy in the metaverse. In Simone Fischer-H¨ubner,
Penny Duquenoy, Albin Zuccato, and Leonardo Martucci, editors, The
Future of Identity in the Information Society, pages 95–112, Boston,
MA, 2008. Springer US.

[45] Sugang Li, Ashwin Ashok, Yanyong Zhang, Chenren Xu, Janne
Lindqvist, and Macro Gruteser. Whose move is it anyway? authenti-
cating smart wearable devices using unique head movement patterns.
In 2016 IEEE International Conference on Pervasive Computing and
Communications (PerCom), pages 1–9, 2016.

[46] Bin Liang, Wei You, Liangkun Liu, Wenchang Shi, and Mario Hei-
derich. Scriptless timing attacks on web browser privacy. In 2014 44th
Annual IEEE/IFIP International Conference on Dependable Systems
and Networks, pages 112–123, 2014.

[47] Junsu Lim, Hyeonggeun Yun, Auejin Ham, and Sunjun Kim. Mine
yourself!: A role-playing privacy tutorial in virtual reality environ-
ment. In CHI Conference on Human Factors in Computing Systems
Extended Abstracts, CHI EA ’22, New York, NY, USA, 2022. Asso-
ciation for Computing Machinery.

[48] Ao Liu, Lirong Xia, Andrew Duchowski, Reynold Bailey, Kenneth
Holmqvist, and Eakta Jain. Differential privacy for eye-tracking data.
ETRA ’19, New York, NY, USA, 2019. Association for Computing
Machinery.

[49] Divine Maloney, Samaneh Zamanifard,

and Guo Freeman.
Anonymity vs. familiarity: Self-disclosure and privacy in social
virtual reality. In 26th ACM Symposium on Virtual Reality Software
and Technology, VRST ’20, New York, NY, USA, 2020. Association
for Computing Machinery.

[50] Divine Maloney, Samaneh Zamanifard,

and Guo Freeman.
Anonymity vs. familiarity: Self-disclosure and privacy in social
virtual reality. In 26th ACM Symposium on Virtual Reality Software
and Technology, VRST ’20, New York, NY, USA, 2020. Association
for Computing Machinery.

[51] Meta. Horizon worlds.

https://www.oculus.com/horizon-worlds/.

Online; accessed 17 May 2022.

[52] Meta. Oculus Go Features.

https://www.oculus.com/go/features/.

Online; accessed 17 July 2022.

[53] Microsoft. Altspacevr. https://altvr.com. Online; accessed 17 May

2022.

[54] Mark Roman Miller, Fernanda Herrera, Hanseul Jun, James A. Lan-
day, and Jeremy N. Bailenson. Personal identiﬁability of user tracking
data during observation of 360-degree VR video. 10(1):17404.

[55] Robert Miller, Natasha Kholgade Banerjee, and Sean Banerjee. Com-
bining real-world constraints on user behavior with deep neural
networks for virtual reality (vr) biometrics. In 2022 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), pages 409–418, 2022.

[56] Robert B. Miller. Response time in man-computer conversational
transactions. In Proceedings of the December 9-11, 1968, Fall Joint
Computer Conference, Part I, AFIPS ’68 (Fall, part I), page 267–277,
New York, NY, USA, 1968. Association for Computing Machinery.

[57] Ilya Mironov. On signiﬁcance of the least signiﬁcant bits for differen-
tial privacy. In Proceedings of the 2012 ACM conference on Computer
and communications security - CCS ’12, page 650, Raleigh, North
Carolina, USA, 2012. ACM Press.

[58] Kelley Misata, Raymond A. Hansen, and Baijian Yang. A taxonomy
of privacy-protecting tools to browse the world wide web. In Pro-
ceedings of the 3rd Annual Conference on Research in Information
Technology, RIIT ’14, page 63–68, New York, NY, USA, 2014.
Association for Computing Machinery.

[59] Tahrima Mustafa, Richard Matovu, Abdul Serwadda, and Nicholas
Muirhead. Unsure how to authenticate on your vr headset? come on,
use your head! In Proceedings of the Fourth ACM International Work-
shop on Security and Privacy Analytics, IWSPA ’18, page 23–30,
New York, NY, USA, 2018. Association for Computing Machinery.

[60] Brad A. Myers. The importance of percent-done progress indica-
tors for computer-human interfaces. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’85,
page 11–17, New York, NY, USA, 1985. Association for Computing
Machinery.

[61] Stylianos Mystakidis. Metaverse. 2(1):486–497.

[62] Vivek Nair, Gonzalo Munilla Garrido, and Dawn Song. Exploring

the unprecedented privacy risks of the metaverse, 2022.

[63] Nair, Vivek and Munilla Garrido, Gonzalo. MetaGuard repository).
https://github.com/MetaGuard. Online; accessed 6 August 2022.

[82] Samaikya Valluripally, Aniket Gulhane, Reshmi Mitra, Khaza An-
uarul Hoque, and Prasad Calyam. Attack trees for security and privacy
In 2020 IEEE 17th
in social virtual reality learning environments.
Annual Consumer Communications Networking Conference (CCNC),
pages 1–9, 2020.

[83] Valve. OpenVR). https://github.com/ValveSoftware/openvr. Online.

[84] Vive. SteamVR Base Station 2.0. https://www.vive.com/us/accessory/

base-station2/. Online; accessed 17 July 2022.

[85] Stanley L Warner. Randomized response: A survey technique for
eliminating evasive answer bias. Journal of the American Statistical
Association, 60(309):63–69, 1965.

[86] Ian Warren and Darren Palmer. Crime risks of three-dimensional

virtual environments. page 7, 2010.

[87] David L. Woods, John M. Wyma, E. William Yund, Timothy J.
Herron, and Bruce Reed. Age-related slowing of response selection
and production in a visual choice reaction time task. 9.

[88] Chuan Yue.

Sensor-based mobile web ﬁngerprinting and cross-
In 2016 IEEE Security and Privacy

site input inference attacks.
Workshops (SPW), pages 241–244, 2016.

[89] Hao Zhu, Yanyong Zhang, Xing Guo, and Xiang-Yang Li. Anti
leakage: Protecting privacy hidden in our speech. In 2021 7th Inter-
national Conference on Big Data Computing and Communications
(BigCom), pages 114–120, 2021.

[64] John William Nelson. A virtual property solution: How privacy law

can protect the citizens of virtual worlds. page 24, 2010.

[65] Fiachra O’Brolch´ain, Tim Jacquemard, David Monaghan, Noel
O’Connor, Peter Novitzky, and Bert Gordijn. The convergence of
virtual reality and social networks: Threats to privacy and autonomy.
22(1):1–29.

[66] Fiachra O’Brolch´ain, Tim Jacquemard, David Monaghan, Noel
O’Connor, Peter Novitzky, and Bert Gordijn. The convergence of
virtual reality and social networks: Threats to privacy and autonomy.
22(1):1–29.

[67] Ken Pfeuffer, Matthias J. Geiger, Sarah Prange, Lukas Mecke, Daniel
Buschek, and Florian Alt. Behavioural biometrics in vr: Identifying
people from body motion and relations in virtual reality. In Proceed-
ings of the 2019 CHI Conference on Human Factors in Computing
Systems, CHI ’19, page 1–12, New York, NY, USA, 2019. Association
for Computing Machinery.

[68] Michael L. Hicks published. Despite Quest 2 sales success, Meta lost

$10.2 billion on VR/AR last year, February 2022.

[69] Daniel E. Re, Jillian J. M. O’Connor, Patrick J. Bennett, and David R.
Feinberg. Preferences for very low and very high voice pitch in
humans. 7(3):e32719.

[70] Black Rock.

now.
metaverse-investing-in-the-future.
2022.

The metaverse:

future
https://www.blackrock.com/us/individual/insights/
accessed 17 May
Online;

Investing

the

in

[71] Cynthia E. Rogers, Alexander W. Witt, Alexander D. Solomon, and
Krishna K. Venkatasubramanian. An approach for user identiﬁcation
for head-mounted displays. In Proceedings of the 2015 ACM Interna-
tional Symposium on Wearable Computers, ISWC ’15, page 143–146,
New York, NY, USA, 2015. Association for Computing Machinery.

[72] Amitav Sarma, Bhupen Barman, GautamC Das, Hiranya Saikia, and
AmbathD Momin. Correlation between the arm-span and the standing
height among males and females of the khasi tribal population of
meghalaya state of north-eastern india. 9(12):6125.

[73] KW Studios Sector3 Studios. Raceroom. https://www.raceroom.com/

en/. Online; accessed 17 May 2022.

[74] Sizescreens. Samsung Gear VR 2017 detailed speciﬁcations. https://
www.sizescreens.com/samsung-gear-vr-2017-speciﬁcations/. Online;
accessed 17 July 2022.

[75] Morgan Stanley. Metaverse: more evolutionary than revolutionary.

Online; accessed 17 May 2022.

[76] Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas
Bulling. Privacy-aware eye tracking using differential privacy.
In
Proceedings of the 11th ACM Symposium on Eye Tracking Research
& amp; Applications, ETRA ’19, New York, NY, USA, 2019. Asso-
ciation for Computing Machinery.

[77] Sophie Stephenson, Bijeeta Pal, Stephen Fan, Earlence Fernandes,
Yuhang Zhao, and Rahul Chatterjee. SoK: Authentication in aug-
mented and virtual reality. page 18, 2022.

[78] Philipp Sykownik, Divine Maloney, Guo Freeman, and Maic Masuch.
Something personal from the metaverse: Goals, topics, and contextual
factors of self-disclosure in commercial social vr. In CHI Conference
on Human Factors in Computing Systems, CHI ’22, New York, NY,
USA, 2022. Association for Computing Machinery.

[79] Rahmadi Trimananda, Hieu Le, Hao Cui, Janice Tran Ho, Anastasia
Shuba, and Athina Markopoulou. OVRseen: Auditing network trafﬁc
and privacy policies in oculus VR. In 31st USENIX Security Sympo-
sium (USENIX Security 22), pages 3789–3806, Boston, MA, August
2022. USENIX Association.

[80] Nikolaos Tsalis, Alexios Mylonas, Antonia Nisioti, Dimitris Gritzalis,
and Vasilios Katos. Exploring the protection of private browsing in
desktop browsers. 67:181–197.

[81] Unity.

Unity documentation.

https://docs.unity3d.com/Manual/

VROverview.html. Online; accessed 17 July 2022.

