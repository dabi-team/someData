9
1
0
2

t
c
O
2
2

]

C
H
.
s
c
[

1
v
0
0
8
9
0
.
0
1
9
1
:
v
i
X
r
a

AeroVR: Immersive Visualization System for Aerospace Design

Sławomir Konrad Tadeja∗ (cid:63), Pranay Seshadri(cid:63), Per Ola Kristensson(cid:63)

(cid:63)Department of Engineering, University of Cambridge, Cambridge, U. K.

Abstract

One of today’s most propitious immersive technologies is virtual reality (VR). This term is colloquially associated
with headsets that transport users to a bespoke, built-for-purpose immersive 3D virtual environment. It has given rise
to the ﬁeld of immersive analytics—a new ﬁeld of research that aims to use immersive technologies for enhancing and
empowering data analytics. However, in developing such a new set of tools, one has to ask whether the move from
standard hardware setup to a fully immersive 3D environment is justiﬁed—both in terms of eﬃciency and development
costs. To this end, in this paper, we present the AeroVR–an immersive aerospace design environment with the objective
of aiding the component aerodynamic design process by interactively visualizing performance and geometry. We
decompose the design of such an environment into function structures, identify the primary and secondary tasks,
present an implementation of the system, and verify the interface in terms of usability and expressiveness. We deploy
AeroVR on a prototypical design study of a compressor blade for an engine.

1 INTRODUCTION

Virtual reality (VR) is rapidly being hailed as the new paradigm for interactive visualization of
data. Its ability to fuse visual, audio, and haptic sensory feedback in a computer-generated simula-
tion environment is deemed to have tremendous potential. While the phrase virtual reality has been
used for decades, in the context of computer aided visualization, today it is synonymous with head-
mounted displays [41] (HMDs) or headsets [24, 23]. Although still in a nascent stage, HMDs have
demonstrated their usefulness in the computer gaming, education, fashion and real-estate industries,
with countless more application areas currently being pursued, including information visualization
in aerospace [7]. One potentially promising application is aerospace design—a complex, multi-
disciplinary, multi-objective and multi-dimensional problem—where technologies that oﬀer faster
design cycle times, with potentially greater eﬃciency gains, can be real game-changers. However,
as the aerospace community usually works on state-of-the-art computational tools and sophisti-
cated computer-aided design packages, there are tremendous hurdles in getting the community to
embrace VR. Furthermore, at this stage, it is not precisely clear what the beneﬁts are in migrating to
a VR-based design framework. Thus, what is required by the aerospace design community is an ini-
tial sketch of an immersive VR aerospace design environment—the AeroVR, a computer-generated
environment that leverages the full visual, audio, and haptic sensory frameworks aﬀorded by VR
technology.

Our focus in this paper is to explore how aerospace design workﬂows can beneﬁt from VR. To
aid our eﬀort, we will be using ideas from parameter-space dimension reduction [4, 6, 34]. This
topic has recently received considerable attention from both the applied mathematics and computa-
tional engineering communities, where the aim has been to reduce the cost of expensive computer

∗Address all correspondence to skt40@cam.ac.uk

1

 
 
 
 
 
 
parameter studies—that is, optimization, uncertainty quantiﬁcation, and more generally design of
experiments. In 2, we present some of the key theoretical ideas that underpin dimension reduction.
This is followed in section 3 with a presentation of the VR aerospace design environment includ-
ing its function and system structures, tasks analysis, and interaction features. The next section
4 describes a veriﬁcation of the interface with respect to usability and expressiveness. Section 5
summarizes the contributions and outlines future work.

2 PARAMETER-SPACE DIMENSION REDUCTION

Consider a function f (x) where f : Rd → R. Here f represents our chosen quantity of interest
(qoi); the desired output of a computational model. This qoi can be the lift coeﬃcient of a wing
or indeed the eﬃciency of a turbomachinery blade. Let x ∈ Rd be a vector of design parameters.
Now when d ≤ 2, visualizing the design space of f is trivial, one needs to simply run a design
of experiment and view the results as a scatter plot. However, when d ≥ 3 visualizing the design
space becomes diﬃcult. One way forward is to approximate f , with

f (x) ≈ g(U T x),

(1)

where g : Rm → R and U ∈ Rd×m, with m ≤ d. We call the subspace associated with the span
of U its ridge subspace and g(U T x) its ridge approximation. Further, we assume that the columns
of U are orthonormal, i.e., U T U = I. The above deﬁnitions imply that the gradient of f is nearly
zero along directions that are orthogonal to the subspace of U . In other words, if we replace x
with x + h where U T h = 0, then f (x + h) = g(U T (x + h)) = f (x). Visualizing fi along the
i U for all designs i, can provide extremely powerful inference; such scatter plots
coordinates of xT
are called suﬃcient summary plots.

2.1 Techniques for dimension reduction

Techniques for estimating U build on ideas from suﬃcient dimension reduction [6] and more
recent works such active subspaces [4] and polynomial [12, 5] and Gaussian [35] ridge approxima-
tions. While our work in this paper is invariant to the speciﬁc parameter-space dimension reduction
technique utilized, we brieﬂy detail a few ideas within ridge approximation. Our high-level objec-
tive is to solve the optimization problem

minimize
U ∈Rd×m, α∈Rp

(cid:13)
(cid:13)f (x) − gα

(cid:0)U T x(cid:1)(cid:13)
2
2 ,
(cid:13)

(2)

over the the space of matrix manifolds U and the coeﬃcients (or hyperparameters) α associated
with the parametric function g. This is a challenging optimization problem and it is not convex.
In Seshadri et al. [35] the authors assume that g is the posterior mean of a Gaussian process (GP)
and iteratively solve for the hyperparameters associated with the GP, whilst optimizing U using a
conjugate gradient optimizer on the Stiefel manifold (see Absil et al. [1]). In Constantine et al. [5]
the authors set g to be a polynomial and iteratively solve for its coeﬃcients—using standard least
squares regression—whilst optimizing over the Grassman manifold to estimate the subspace U . It
should be noted that these techniques are motivated by the need to break the curse of dimensionality.
In other words, one would like to estimate both g and U for a d dimensional, scalar-valued, function
f without requiring a large number of computational simulations.

2

The dimension reduction strategy we pursue in this paper is based on active subspaces [4] com-
putational heuristic tailored for identifying subspaces that can be used for the approximation in (2).
Broadly speaking, active subspaces requires the approximation of a covariance matrix C ∈ Rd×d

C =

(cid:90)

X

∇xf (x) ∇xf (x)T ρ (x) dx,

(3)

where ∇f (x) represents the gradient of the function f and ρ is the probability density function that
characterizes the input parameter space X ∈ Rd. The matrix C is symmetric positive semi-deﬁnite
and as a result it admits the eigenvalue decomposition

C = W ΛW T = (cid:0) W1 W2

(cid:18) Λ1

(cid:1)

(cid:19)

Λ2

(cid:0) W1 W2

(cid:1)T ,

(4)

where the ﬁrst m eigenvectors W1 ∈ Rd×m, where m << d—selected based on the decay of
the eigenvalues Λ—are on average directions along which the function varies more, compared
to the directions given by the remaining (d − m) eigenvectors W2. Readers will note that the
notion of computing eigenvalues and eigenvectors of an assembled covariance matrix is analogous
to principal components analysis (PCA). However, in (3) our covariance matrix is based on the
average outer product of the gradient, while in PCA it is simply the average outer product of samples,
i.e., xxT . Now, once the subspace W1 has be identiﬁed, one can approximate f via

f (x) = f (cid:0)W W T x(cid:1) = f (cid:0)W1W T
1 x(cid:1) ,

≈ g (cid:0)W T

1 x + W2W T

2 x(cid:1)

(5)

in other words we project individual samples xi onto the subspace W1. Moreover, as the func-
tion (on average) is relatively ﬂat along directions W2, we can approximate f using the directions
encoded in W1.

But how do we compute (3), as for a given f we may not necessarily have access to its gradients?
In [33], the authors construct a global quadratic model to a 3D Reynolds Averaged Navier Stokes
(RANS) simulation of a turbomachinery blade and then analytically estimate its gradients. We de-
tail their strategy below as we adopt the same technique for facilitating parameter-space dimension
reduction.

Assume we have N input-output pairs {xi, fi}N
obtained by running a suitable design of exper-
i=1
iment (see [26]) within our parameter space. We assume that the samples xi ∈ Rd are independent
and identically distributed and that they admit a joint distribution given by ρ (x). Here we will
assume that ρ (x) is uniform over the hypercube X ∈ [−1, 1]d. We ﬁt a global quadratic model to
the data,

f (x) ≈

1
2

xT Ax + cT x + d,

(6)

using least squares. This yields us values for the coeﬃcients A, c and the constant d. Then, we
estimate the covariance matrix in (3) using

ˆC =

(cid:90)

X

(Ax + c) (Ax + c)T ρ (x) dx.

(7)

Following the computation of the eigenvectors of ˆC, one can then generate suﬃcient summary plots
that are useful for subsequent inference and approximation.

3

Figure 1: Hardware interfaces in VR: (a) Xbox controller; (b) Oculus Rift’s motion sensor; (c) Oculus Rift VR headset.

We apply this quadratic recipe and show the suﬃcient summary plots for a 3D turbomachinery
blade in section 3, both in a standard desktop environment and in virtual reality. This comparison—
the central objective of this paper—is motivated by the need to explore the gains VR technologies
can aﬀord in aerospace design. That said, prior to delving into our chosen case study, an overview
of existing immersive visual technologies and their associated frameworks is in order.

3 SUPPORTING AEROSPACE DESIGN IN VR

Visual analytics (VA), a phrase ﬁrst coined by J.J. Thomas et al. [44] has two ingredients: 1)
an interactive visual interface [44]; and 2) analytical reasoning [44]. Recent advances and break-
throughs in the development of VR (virtual reality) and AR (augmented reality) have spun oﬀ
another branch of research known as Immersive Analytics [2] (IA). IA seeks to understand how the
latest wave of immersive technologies can be leveraged to create more compelling, more intuitive
and more eﬀective visual analytics frameworks. There are still numerous hurdles to overcome for
VR-based data analytics tools to be widespread. Issues associated with any type of a 3D interface,
for example, potential occlusion eﬀects [38], high computing power demands and specialized, (of-
ten costly) hardware, have to be resolved, or at least minimized. Moreover, interaction techniques
have to facilitate a user’s understanding of the visualization and avoid becoming a distraction. Fi-
nally, certain guidelines have to be incorporated to mitigate the risk of the simulation sickness
[15, 29] symptoms that can manifest during or immediately after the use of a VR headset.

Many interaction techniques and devices have been conceived that can be used separately or in
combination for interaction and control of a VR environment. In this paper we use the standard oﬀ-
the-shelf Xbox [18] controller shown in Fig. 1(a) that comes prepacked with the Oculus Rift[24]
bundle (see Fig. 1(b) and Fig. 1(c)). This controller-style has achieved very high adoption in gaming
industry; its design is ergonomic and easy to learn. As one example of wider adoption, the US
Navy recently adopted the use of a Xbox controller to operate the periscope on nuclear-powered
submarine1.

Here, we present a VR aerospace design environment with a focus on dimension reduction. In-
formation on one of the earliest examples of research into using VR for applications in the scope

1https://www.digitaltrends.com/cool-tech/navy-xbox-controllers-attack-submarines/, Last accessed: August 2019

4

of aerospace design can be found in Hale [11]. García-Hernández et al. [7] pointed out that the
VR technology is starting to gain ground in aerospace design and listed a range of aerospace re-
search topics in which VR already is, or can be, successfully applied. This includes, among others,
spacecraft design optimization (e.g. Mizell [19] discusses use of VR and AR in aircraft design
and manufacturing whereas Stump et al. [40] used IA to aid a satellite design process) and aerody-
namic design, in which 3D scatter plots are already in-use (see Jeong et al. [14]). Other applications
include use of the haptic feedback (e.g. Savall et al. [32] describes REVIMA system for maintain-
ability simulation and Sagardia et al.[31] presents the VR-based system for on-orbit servicing simu-
lation), collaborative environments (e.g. Roberts et al. [28] introduces an environment for the Space
operation and science whereas Clergeaud et al. [3] discusses implementation of the IA tools used
in context of the aerospace with Airbus Group), aerospace simulation (e.g. Stone et.al [39] discuss
the evolution of aerospace simulation that uses immersive technologies), telemetry and sensor data
visualization (e.g. see Wright et al. [47], Lecakes et al. [17] or Russell et al. [30]), or planetary
exploration (e.g. see Wright et al. [47]). García-Hernández et al. [7] suggests that three elements
are especially promising for a VR-based approach: 1) integration of multiple 2D graphs for 3D
data [7]; 2) 3D parallel coordinates [7] (see Tadeja et al. [42, 43]); and 3) visualization of complex
graphs [7]. In this paper, we loosely follow (1), but with a key diﬀerence: we use subspace-based
dimension reduction to generate the 3D graphs for high-dimensional data.

3.1 Applications in design

Our dimension reduction results and case study is based on the work undertaken in Seshadri
et al. [33]. Here the authors study the 25D design space of a fan blade using the quadratic active
subspaces recipe detailed in 2. Towards this end, we used the design of an experiment with N = 548
3D RANS computations with diﬀerent designs; the design space used in this study included ﬁve
degrees of freedom speciﬁed at ﬁve spanwise locations. These degrees of freedom comprised of an
axial displacement, a tangential displacement, a rotation about the blade’s centroidal axis, leading
edge recambering and trailing edge recambering, speciﬁced at 0, 25, 50, 75 and 100% span. Thus,
we obtained values of the eﬃciency and pressure ratio for each design vector xi. These are two
important output quantities of interest in the design of a blade. By studying the eigenvalues and
eigenvectors of the covariance matrix for these two objectives the authors were able to discover a 1D
ridge approximation for the pressure ratio of a fan and a 2D ridge approximation for the eﬃciency.
These suﬃcient summary plots are shown in Fig. 2. There are a few important remarks to make
regarding these plots.

For the pressure ratio suﬃcient summary plot, shown in Fig. 2(a), the horizontal axis is the
ﬁrst eigenvector of the covariance matrix associated with the pressure ratio, u1. For the eﬃciency
suﬃcient summary plot, shown in Fig. 2(b), the two horizontal axes are the ﬁrst two eigenvectors
of the covariance matrix associated with the eﬃciency [u1, u2]. It is important to note that the
subspaces associated with eﬃciency and pressure ratio are distinct.

The suﬃcient summary plots above permit us to identify and visualize low-dimensional structure
in the high-dimensional data. More speciﬁcally, these plots can be used in the design process as
they permit engineers to make the following inquiries:

• What linear combination of design variables is the most important for increasing / decreasing

the pressure ratio?

• How do we increase the eﬃciency?

5

(a)

(b)

Figure 2: Suﬃcient summary plots of (a) Pressure ratios; (b) Eﬃciency, for a range of diﬀerent computational designs
for turbomachinery blade, obtained from a design of experiment study. Based on work in Seshadri et al. [33].

Figure 3: The user’s ﬁeld-of-view: the right-hand plot can, for example, show the lift coeﬃcients whereas the left-hand
side plot can contain the drag coeﬃcient values. The nominal blade geometry is visualized in the middle between the
two plots. The orange circle is a cross-hair singalising where user is looking at the moment.

• What are the characteristics of designs that satisfy a certain pressure ratio?

• What are the characteristics of designs that satisfy the same eﬃciency?

We use these suﬃcient summary plots in a bespoke VR environment. Our high-level objective
is to ascertain whether it is possible to leverage tools in VR in conjunction with parameter-space
dimension reduction to facilitate better design decision-making and inference. To achieve this goal,
we seamlessly integrate the aforementioned suﬃcient summary plots with the 3D geometric design
of the blade, i.e.,

pressure ratio (cid:10) geometry visualization (cid:10) eﬃciency.

6

-2-1.5-1-0.500.511.5-0.015-0.01-0.00500.0050.010.0150.02-0.8-0.6-0.4-0.2200.20.40210-1-2-2In other words, as the user selects a diﬀerent design—by selecting a suitable level of perfor-
mance from the suﬃcient summary plots—they visualize the geometry of the blade that yields that
performance. Moreover, they should be able to compare this geometry with that of the nominal
design. We clarify and make precise these notions in the forthcoming subsections.

3.2 Function structures

We model the function structures of the system using Function Analysis Systems Technique [37]
(FAST). Fig. 5 shows the function structures of the VR visualization environment for an aerospace
design workﬂow with dimension reduction. The FAST-diagram in Fig. 5 models the level of ab-
straction on the horizontal axis and function sequence on the vertical axis.

3.3 System structure

We model the system internal structure by observing the internal ﬂow of signals between the
individual system elements. The visualization consists of four distinguishable parts: (A) the user
who is responsible for all the actions of the system once the data had been loaded and visualized,
(B) eﬃciency and pressure-ratio 3D scatter plots, (C) blade model, and (D) engine geometry model.
The signals are usually bi-directional and can introduce a chaining eﬀect. For instance, when user
is gazing over an interactive object, which is internally facilitated by the ray-tracing, the object
highlights itself, that is, the user receives a return feedback signal in the form of a visual clue.
Moreover, selection of a data point on the scatter plot through an implementation of the linking &
brushing interaction technique, leads to a selection of the mapped data point on the other scatter plot
and visualization of a new geometry overlapping with the nominal shape. The signal ﬂow analysis
is presented on Fig. 4. The main signal ﬂows are decomposed into:

1. The user: The user interacts with the system using a combination of gaze-tracking and ray-
tracing. This works as follows. Gaze-tracking is achieved with the help of a cross-hair in
the middle of user’s ﬁeld of view, placed a certain, ﬁxed distance along the camera’s forward
direction. Rays extending from the cross-hair are constantly checked for intersection with
other interactive objects i.e. data points on the scatter plots. If such an interaction occurs, the
object automatically highlights, providing a signal to the user that it can be interacted with.
The way in which the user directly receives signals from other parts of the visualization is
unidirectional, that is, a user’s action results in a visual response. The way the user interacts
with other objects is through a combination of gaze-tracking and ray-tracing (i.e., an orange
cross-hair, see Fig. 3 and Fig. 7) as well as actions invoked with a tap of a button (see Fig. 1).

2. 3D scatter plots: The scatter plots receives signals from the user through a mixture of gaze-
tracking and ray-tracing inputs combined with the tap of a button on the controller. This is
reﬂected back to the user by, for example, highlighting scatter plots elements, such as data
points or movement selectors, that are being gazed over or changed their color after selection.
In turn, this action invokes unidirectional changes in the visualized blade geometry and the
turbofan engine.

3. Blade geometry model: The blade geometry visualization receives signals from both scatter
plots by the user performing a selection of a data point on any of the plots, which automatically
visualizes the new blade geometry. Moreover, even though the user cannot directly inﬂuence
the geometry, by using the movement and maneuvering techniques in the system, the user can

7

inspect the geometry by zooming in on its internal and external surfaces. Hence the relation
between the scatter plots and the blade is unidirectional, whereas the relation between the user
and the blade model is bidirectional. Furthermore, once the new blade has been visualized,
the visualization of the hub with blades in the engine model simultaneously changes as well.
This can be thought of as another unidirectional relation as it cannot happen the other way
around.

4. Engine geometry model: Once the new geometry shape is selected by the user, the blade
row with the series of blades embedded in the engine model is automatically replaced. This
change is immediately visible to the user providing visual feedback.

Figure 4: The diagram shows how the signals are ﬂowing within the system between its four main components: (A) the
user grouped together with a controller used for user input; (B) a set of performance parameters visualized as the 3D
scatter plots, in this case, eﬃciency and pressure-ratio 3D scatter plots; (C) blade geometry model; and (D) complete
engine geometry model.

3.4 Tasks analysis

From the limitations imposed by the current state and understanding of the VR environment and
from our own analysis of the system achieved by the FAST analysis (see Fig. 5) we identiﬁed two
primary, high-level tasks:

T1—Gaining design overview: The system should permit the user to easily gain an overview of
the entire design space i.e. possible blades geometries together with their associated performance
parameters.

T2—Compare geometries: The system should permit the user to easily compare the nominal

blade geometry with the one associated with a particular set of performance.
These two main tasks (i.e. T1 and T2) were supported and augmented by a number of low-level
tasks:

T3—Movement and interaction: Due to the nature of spatial, 3D immersive workspace pro-
vided by the VR environment, this task has a dominant and a supporting role with respect to all

8

Engine ModelBlade ModelController(C)(A)(D)(B)UserScatter Plot AScatter Plot BScatter Plot CScatter Plot DScatter Plot …Figure 5: A functional model of the interactive system.

the other tasks. Movement, maneuvering and interaction are achieved through the gaze-tracking
and with the help of a gamepad controller. All movement facilitated by either the joystick [J] or
triggers [T ] (see Fig. 6) takes place with respect to the users gaze (see orange cross-hair on Fig. 3).
Moreover, the user can interact with an object through gazing over an object and tap of a button (see
Fig. 6). Zooming in or out on a part of the visualization is also achievable by the user’s movement
in the virtual space.

T4—Visualization of performance parameters: The performance parameters, such as eﬃ-
ciency and pressure ratio that were used in our case, are visualized as an interactive 3D scatter
plots ﬂoating in the 3D space. These can be freely moved, rotated about each of the main axes
and implements the linking & brushing interaction technique i.e. changes in one scatter plot are
simultaneously reﬂected on the other scatter plot and blade and engine visualizations as well. This
task mainly supports T1.

T5—Visualization of blades and engine models: The nominal blade geometry and associated
engine visualization are immediately visible at the start of the visualization. Once the new geometry
is selected, the nominal blade renders semi-transparent and the shape of the new blade is superim-
posed over it. Moreover, the hub with a row of blades are substituted with the new geometries in

9

Identify appropriate designWhy?How?ComparegeometriesGaindesign overviewVisualize efficiencyOverlay geometriesSelect performanceparameterCheck correspondingparametersInspect modelsVisualize performance parametersVisualize modelsVisualize pressure ratio Move in 3D SpaceShow valueZoom-in on geometryZoom-out of geometryInspect modelsVisualize new geometryVisualize nominal geometryVisualize engineVisualize new engine geometrythe engine model.

T6—Models inspection: The inspection of the changes in the engine visualization and the blade

geometry itself can be made through the T3 task.

3.5 Visualization framework

The visualization framework is built using Unity3D—one of the most widely used game engines
with built-in VR development support. Both of the two mainstream VR headsets provide supporting
packages developed natively for Unity3D, which substantially speeds up the development process.
This software is built on top of the Unity VR Samples pack[45] and uses the Oculus Utilities for
Unity[25] package as well as parts of the Unity asset[10]. In addition, we use the asset store available
for the Unity3D game engine, which contains many VR-ready tools and supporting packages.

A survey by Wagner et al. [46] highlights that game engines “do not support any data explo-
ration”[46] techniques. In other words, these features have to be designed and implemented from
scratch. To allow user interaction with data we use an Xbox Controller connected with the laptop
via USB cable (see Fig. 1(a)) in combination with gaze-tracking through a cross-hair which moves
with the user’s head and is placed straight from the camera (visualized as an orange cross-hair, see
Fig. 3, Fig. 7 and Fig. 10).

3.6

Interaction and movement

Figure 6: The Xbox controller: (a) shows the top view with the left-hand joystick [J] used to control the 2D movement
on the X-Z plane whereas (b) shows the front view with the two triggers [T ] responsible for vertical movement along
the Y axis. The other action buttons indicated in (a) have the following meanings: [R] for reload of the visualization;
[L] for loading next dataset; [A] for selection of an interactive item; and [X] for moving or rotating the scatter plots.

The interaction is designed around gaze-tracking in combination with the standard buttons on

the Xbox[18] controller (see Fig. 6). Supported interactions are mapped as follows:

• Left-hand joystick and [T ] buttons (see Fig. 6): Triggers movement along the X-Z plane and
movement along the vertical axis respectively, right-hand [T ] is assigned to “up” and left-hand
[T ] is “down”. The movement in the X-Z plane is always with respect to the user’s gaze. This
manoeuvring combination permits the user to move in any direction and in any position in 3D

10

T(b)J(a)TAXLRFigure 7: By selecting any data point on any plot, the user can immediately observe the blade’s geometry associated
with this particular design. Moreover, the user can observe and compare the diﬀerences between the nominal and
perturbed geometries as the former is kept rendered as a semi-transparent shape overlaying the latter. As users can
freely maneuver in 3D space they can visually inspect the entire blade from any direction and zoom in on any of its
parts, as shown in (a–f).

space. The user moves with constant velocity and with ﬂuid movement to ensure the user is
receiving continuous closed-loop visual feedback on their changing position in relation to the
surroundings.

• Action button [A]: Selects an interactive element, such as a scatter plot rotation and movement
selector, or a data point (see Fig. 10). Objects highlight themselves when the user’s gaze, as
indicated by a cross-hair, is on them. Double-tapping on the [A] button selects the highlighted
object.

• Button [X]: When tapped after the selection of a scatter plot point, it will re-position the point
to a certain distance towards the user’s present gaze direction. Furthermore, if the rotation
selector is active, selecting this button will initiate the scatter plot’s rotation over 90◦ based on
the current direction of the user’s gaze.

• Button [R]: Resets the visualization and all its associated elements to their original state.

• Button [L]: This button loads the next dataset: a new set of performance parameters and

associated blade geometries.

3.7 Blade and engine visualizations

As alluded to previously, the central artifact in our VR environment is the geometry of the de-
signs. Our virtual environment contains as many geometries as there are data points, resulting in
a total of 548 stereo lithography (STL) ﬁles. Hence, whenever a data point is selected on one of
the plots, the accompanying shape is instantly visualized. To provide the user with a quick and an
eﬀective way of comparing the new perturbed design, the nominal geometry is still kept visible and
rendered as a translucent object, as can be seen in Fig. 7. This solution, combined with unlimited
movement dexterity, allows the user to visually inspect and observe any diﬀerences between the

11

two geometries. Furthermore, by simply changing their position, or by tilting their head (thereby
changing the rotational angle), the user can zoom-in and zoom-out on any of the blades parts for a
close inspection as presented in Fig. 7(f).

Figure 8: The entire visualization as it is seen by the user with the complete engine model in the back and the two 3D
scatter plots and the nominal blade geometry (in blue) in front. The hub with a series of blades is also shown (in blue).

The visualized engine model [36] (see Fig. 8) consists of six independent parts including the hub
with connected blades. When a new blade geometry is being investigated by the user, the blades
visible in the engine are automatically replaced as well. Due to limitations imposed by the used
CAD model itself, it was not possible to substitute the blades individually, thus the entire hub with
the series of attached blades could be replaced altogether which is signaled to the user by changing
color of this entire part (see Fig. 9).

3.8 Suﬃcient summary plots

The key element in the framework is the suﬃcient summary plots; they are visualized as three
ﬁxed-size, axis-aligned translucent orthogonal rectangles. The data points are scaled so the values

12

Figure 9: The same view as in Fig. 8 with a single data point selected on one of the scatter plots (the one on the bottom-
right). The nominal blade geometry was rendered as semi-transparent shape with a new geometry superimposed on
top of it. The engine hub with a new series of blades is also shown (in tan).

of their respective coordinates are within the range of the translucent surfaces. When any of the
spheres denoting a data point is selected, the marker lights up and switches to a selection color
(light green). Moreover, as we have a 1:1 mapping between the plots, the corresponding design on
the other plot is selected. Furthermore, a number of semitransparent cones2 was embedded into
these suﬃcient summary plots to denote their axes: one for the X-axis, two for the Y -axis and
three for the Z-axis. Selection of a shape with its pointing tip has an additional advantage—the
orientation informs the user of the positive side of a given axis. Each selection can be reverted by
double-tapping the [A] button while gazing over it.

The 3D spheres in the plots were used to denote both the data points and various selectors’ mark-
ers. Using shape perception has a long-standing application history for VR-based visualizations.
For instance, Ribarsky et al. [27] used simple 3D shapes such as cones, spheres and cuboids in
their system. They also highlight that glyphs with their intrinsic characteristics, such as “position,

2W. Kresse, used under CC BY-SA 3.0; http://wiki.unity3d.com/

13

Figure 10: The scatter plot as it is seen by the user. Figure (a) shows a user’s gaze (orange cross-hair) hovering over a
data point which instantly displays the associated values (e.g. its coordinates) in. Visible, formerly selected points (in
green) are reﬂected on the other scatter plot and the associated geometries are also shown. Figure (b) shows the same
plot from a distance. The highlighted spheres (in orange) are the movement selectors: if the user gazes at any point
in space and taps the [X] button on the controller the plots will be translated towards that point in space. Figure (c)
shows a rotation by 90◦ towards the user along the X-axis with the axis rotation selector highlighted (in orange). Only
a single rotation selector can be active at once across all the scatter plots.

shape, color, orientation and so forth” [27] are very useful when visualizing complex datasets.

3.8.1 Initial placement and re-positioning

The two scatter plots—one for pressure ratio and another for eﬃciency—are automatically po-
sitioned on both sides of the blade, which in turn is positioned in front of the initial user’s ﬁeld
of view; see Fig. 3. The plots are placed at the same, pre-conﬁgured distance from the user, at a
roughly 45◦ angle from the X-axis.

To ensure that the user does not feel constrained in a nearby region and to make better use of
virtually inﬁnite 3D space provided by the VR-environment, users are provided with the possi-
bility of moving the scatter plots. The interaction occurs via gaze-tracking and the select & move
metaphor. Every scatter plot has a color-coded interactive sphere, that is, a selector (see Fig. 10(b)),
attached to it in the right-hand top corner. When the user’s gaze hovers over it, the selector auto-
matically highlights it and, if selected by double-tapping the [B] button, changes its color to orange
(see Fig. 10(b)). If the user presses the [X] button while a selection is active, the scatter plot is
re-positioned at a certain distance towards the point determined by the user’s current gaze. If the
button is held the plot will follow the cross-hair’s movement.

It is also possible to move both plots at once if more selectors are simultaneously active. In such
a scenario, to keep the current relative position of these scatter plots, a barycentre B = (xB, yB, zB)
of all these objects is calculated using the formula:

B =

1
N

(xB =

N
(cid:88)

i=0

xi, yB =

N
(cid:88)

i=0

yi, zB =

N
(cid:88)

i=0

zi).

(8)

This point is moved along the forward vector from the camera in the same manner as in the case of
a single scatter plot. Selected objects are then grouped together and displaced with respect to the

14

new position of the barycentre whilst simultaneously keeping their internal (current position with
respect to the local axes) and external (axis-alignment of surfaces) rotations.

3.8.2 Scatter plot rotation

The scatter plot can be rotated in 90◦ steps about one of the three main axes. This is achieved by
double-tapping the [A] button while gazing over one of the three rotation selectors (see Fig. 10 (b-
c)). A further press of the [X] button will result in a rotation of all the plot’s components, including
data points, rotation and movement selectors and axis cone markers. If users are situated in such
a way that their gaze is located exactly in front of the active axis selector, the rotation will occur
towards the direction provided by the camera’s forward vector. Similar to the movement, the plot’s
orientation in the global coordinate system will not be aﬀected.

3.8.3 Relationships between the visualization elements

The data points on one of the plots are correlated in a 1:1:1 (one-to-one-to-one) manner onto
the other plot and vice-versa. Moreover, each of the data points is mapped onto one-and-only-one
unique blade design. Therefore, whenever a marker is selected on one of the plots, the system will
automatically highlight the corresponding data point on the other plot and switch the visualized
blade onto the new, corresponding shape. Furthermore, the nominal shape will be kept as a translu-
cent point of reference (see Fig. 7) that overlays the new design to show the user how, where, and
to what degree, the new shape diﬀers from the nominal one.

3.8.4 Labeling

Both the data points and the axes are automatically labeled. In case of the latter, the strings
embedded into the edges of the semitransparent rectangles denoting the axes are read directly from
the input text ﬁle (see Fig. 10 (a)). The labeling of the data points is only visible once the user is
hovering with his or her gaze over the marker (for example, a sphere) and disappears once the user
looks at another point, or other parts of the visualization (see Fig. 10 (a)). Furthermore, the small
box with the values associated with the point (in this case its coordinates) is always rotated towards
the user and follows their gaze. It is rendered on top of any other visualization elements as seen in
Fig. 10 (a).

4 INTERFACE VERIFICATION

The VR aerospace design environment in this paper is still at an early stage and the objective
of this paper is not to present a complete solution but to demonstrate potential beneﬁts of VR for
aerospace design.

Here, we verify the fundamental usability of the system using two formative evaluation methods.
First, we assess the usability of the system using Nielsen’s [22, 21] guidelines. Second, we use the
cognitive dimensions of notations framework [8, 9] to reason about the expressiveness of the system.

4.1 Usability

• Visibility of system status: The system provides immediate feedback to users in response to
their actions. For instance, whenever a user’s gaze hovers over an interactive object (such as,

15

for example, movement and rotation selectors or a data point) it is instantly highlighted. In
addition, once an object is selected, the object also changes its color in response. In addition,
following the selection of a data point, its corresponding data points on the other plot are
also simultaneously selected and the accompanying geometry is loaded automatically. This
ensures the user remains synchronized with the system’s current state. Finally, whenever the
user’s gaze is hovering over an object, a gaze-locked text is also displayed to the user, which
reveals values associated with this particular data point.

• Match between the system and the real-world: First, the system uses the cross-hair concept
which is well known in the real-world to focus and help guiding the users’ gaze on objects
placed directly behind or near it. Moreover, the 3D scatter plots were designed to immediately
resemble their two- or three-dimensional desktop-based counterparts. In addition, initially a
user observes all the main elements of the visualization in the ﬁeld-of-view placed at roughly
the same height and direction as how they would be perceived in the real-world if they were
visualized on standard computer displays.

• User control and freedom: The user can either load the new set of data or reload the entire
visualization with a click of a button. No direct “undo” and “redo” actions were directly im-
plemented, however, users can always deselect any object or redo the last executed operation.
Moreover, using a combination of the gaze-tracking and controller-based interaction, users
can locate themselves at any position in 3D space.

• Consistency and standards and Flexibility and eﬃciency of use: As the VR in its current,
almost fully immersive form, is a fairly recent development, the technology itself, not to men-
tion its main applicability areas or interaction design principles, is not yet fully understood.
However, the system design is as consistent as possible, for instance, all interactive objects
can be (de)selected using exactly the same method.

• Error prevention: Measures to prevent the user from errors, such as missing or broken input
data (for example, geometries and data points), are directly built into the system. As there is a
1:1:1 mapping between the system elements (data points) on the two plots and the geometries,
missing any of the elements would lead to omitting this particular entry from the visualization
and detailed information of such event being written into the log ﬁle. Hence, the main error-
prone conditions are eliminated. In addition, the system incorporates certain constraints, such
as a user cannot have more than a single axis-rotation selected at the time—the new selection
will automatically deselect any previously activated selector. This prevents an error caused by
the system being unable to recognize about which axis the scatter plot should be rotated.

• Help users recognize, diagnose, and recover from errors: There are not many errors that user
can commit, assuming the dataset is correct. The countermeasures against plotting incomplete
data are built-in into the system. However, due to the nature of the visualization, erroneous
blade geometries or any anomalies in their shapes will be detected by the user in a close-up
inspection possible through the mixture of movement and maneuvering in the 3D space. The
same can be said about the 3D scatter plots where user is able to rotate them and see them from
every direction and can zoom in and zoom out from any data point using the same techniques.

• Recognition rather than recall and Aesthetic and minimalist design: The blade’s geometry is
visualized as a replication of its physical appearance in the real-world. Moreover, both plots
use volumetric glyphs to denote the points which reassembles the 2D scatter plots versions

16

(see Fig. 10). Previously selected data points are also highlighted. Furthermore, the system
was designed to be minimalistic—only the eﬃciency and pressure ratio plots together with the
geometry visualization are included to avoid overloading the user with information. Hence, for
instance, the values of the data points are not initially visible, however, the scatter plots oﬀer
a possibility of gaining a high-level understanding of the data at a ﬁrst glance. More detail of
each individual point is available on an on-demand basis when the gaze cursor hovers over a
data marker.

• Help and documentation: A succinct single page documentation sheet describing the interac-

tion techniques is provided to the users.

4.2 Expressiveness

The expressiveness of the system is here analyzed using the cognitive dimensions of notation
framework [8]. This framework provides a vocabulary for analyzing the possibilities and limitations
of an interactive notational system. Below we articulate how the keywords in this vocabulary maps
onto the expressiveness of the system.

• Closeness of mapping: The visualized geometry is a detail mapping of how would the blade

would look like in the real-world.

• Consistency: All interactions are consistently designed and all interactive objects have con-

sistent interaction qualities, such selectors.

• Diﬀuseness / terseness: The number of used symbols is minimized by only using sphere-like
markers with their characteristics (such as color, size and relative placement) to denote all the
interactive elements of the visualization.

• Error-proneness: Errors are prevented using prevention mechanisms against error states, such

as selection of multiple rotation-selectors.

• Hard mental operations: Cognitive load and mental demand is kept to a minimum with

straight-forward interaction methods and use of comparative visualizations.

• Hidden dependencies: As there is a 1:1:1 mapping between the visualization elements all the
interdependencies are easily observed by the user since selection of a data point on one of the
plots leads to a simultaneous selection of a corresponding data point on the other plot as well
and the visualization of the associated geometry.

• Juxtaposability and Visibility: All three main parts of the visualization, that is, the eﬃciency
and pressure ratio plots together with the blade’s geometry, are initially placed next to each
other. Furthermore, the plots can be freely rearranged in the space as a group or individually.
Moreover, the selection of any data point in a plot is automatically mapped on to the other
plot as well and the corresponding geometry is immediately visible. In addition, selected data
points are clearly visible through change in color and luminosity.

• Premature commitment: The user’s workﬂow with the system is ﬂexible and a user is free to
initially inspect the nominal geometry, any or both plots, or to immediately select a data point.

• Progressive evaluation: Since all the user’s actions result in immediate visual feedback (closed-
loop interaction) the progress of the visual analytics task can be evaluated by the user at any
time.

17

• Role-expressiveness: The individual roles of the three components, that is, the eﬃciency and
pressure-ratio plots and the blade’s geometry, are clear from the beginning, especially if the
system is used by a domain expert.

5 CONCLUSIONS AND FUTURE WORK

The goal in this paper has been to introduce the AeroVR system—a novel VR aerospace de-
sign environment with a particular emphasis on dimension reduction. We have identiﬁed the main
structures of the design environment and implemented a fully working system for commodity VR
headsets. We have also veriﬁed the interface from two perspectives: usability and expressiveness.
The two main identiﬁed tasks were (i) gaining the overview over the design and (ii) comparing the
nominal geometry with the one associated with a speciﬁc performance parameters. The former was
achieved through a mixture or visualization (e.g., blade and engine geometry, and scatter plots) and
lower-level tasks (e.g., movement and interaction). The latter was achieved through visualization
of the overlaying geometries: semi-transparent nominal blade superimposing over solid shape of a
new design.

Moving forward, our goal is to undertake the complete 3D design of a turbomachinery compo-
nent in VR. In addition to the suﬃcient summary plots, our goal is to incorporate characteristics
of blade performance at multiple operating points and have reduced order models to estimate the
performance characteristics of new designs.

In forthcoming years, the cost of the headset, and the required computing resources, will be fur-
ther minimized with the introduction of next-generation of controllers, wireless headsets (e.g., Ocu-
lus Quest[24]), and gestures (e.g., Leap Motion[16]), or speech-based interfaces. Simultaneously,
these rapid advancements in hardware and software open up completely new possibilities in terms
of interaction techniques, rapid information analysis and the amount of data that can be processed
and visualized at once. The two most promising venues of further development of the AeroVR are
investigation of which interaction techniques may bring the most beneﬁt to the user and integration
of our system with a system operating on knowledge from domain expert, i.e. a knowledge-based
system[13, 20]. The former would include adding either controller-based laser-pointing or hand-
tracking capabilities or a combination of thereof depending of the particular user’s needs. The latter
would require to develop and integrate a knowledge-(data)base[13] with the interface provided by
our system to build a knowledge-based system[13, 20].

ACKNOWLEDGEMENTS

This work was supported by studentships from the Engineering and Physical Sciences Research
Council (EPSRC-1788814), and the Cambridge European & Trinity Hall Scholarship. The authors
are grateful for all the generous support. The authors would also like to thank Timoleon Kipouros
for his valuable suggestions.

References

[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press, 2009.

18

[2] T. Chandler, M. Cordeil, T. Czauderna, T. Dwyer, J. Glowacki, C. Goncu, M. Klapperstueck,
K. Klein, K. Marriott, F. Schreiber, and E. Wilson.
In U. Engelke,
J. Heinrich, T. Bednarz, K. Klein, and Q. Nguyen, editors, 2015 Big Data Visual Analytics
(BDVA), United States, 2015. IEEE, Institute of Electrical and Electronics Engineers.

Immersive analytics.

[3] D. Clergeaud, F. Guillaume, and P. Guitton. 3D collaborative interaction for aerospace in-
dustry. In 2016 IEEE 3rd VR International Workshop on Collaborative Virtual Environments
(3DCVE), pages 13–15, 2016.

[4] P. G. Constantine. Active subspaces: Emerging ideas for dimension reduction in parameter

studies, volume 2. SIAM, 2015.

[5] P. G. Constantine, A. Eftekhari, J. Hokanson, and R. A. Ward. A near-stationary subspace for
ridge approximation. Computer Methods in Applied Mechanics and Engineering, 326:402–
421, 2017.

[6] R. D. Cook. Regression graphics: Ideas for studying regressions through graphics, volume

482. John Wiley & Sons, 2009.

[7] R. J. García-Hernández, C. Anthes, M. Wiedemann, and D. Kranzlmüller. Perspectives for
using virtual reality to extend visual data mining in information visualization. In 2016 IEEE
Aerospace Conference, pages 1–11, Mar. 2016.

[8] T. R. G. Green. Cognitive dimensions of notations. In People and Computers V, pages 443–

460. University Press, 1989.

[9] T. R. G. Green. The Cognitive Dimension of Viscosity: A Sticky Problem for HCI. In Pro-
ceedings of the IFIP TC13 Third Interational Conference on Human-Computer Interaction,
INTERACT ’90, pages 79–86, Amsterdam, The Netherlands, The Netherlands, 1990. North-
Holland Publishing Co.

[10] Greyman studios S.L. Oﬀ Screen Indicator. https://assetstore.unity.com/packages/

tools/gui/off-screen-indicator-57062, Last accessed: July 2017.

[11] J. P. Hale. Applied virtual reality in aerospace design. In Proceedings of WESCON ’94, pages

378–383, Sept. 1994.

[12] J. M. Hokanson and P. G. Constantine. Data-driven polynomial ridge approximation using
variable projection. SIAM Journal on Scientiﬁc Computing, 40(3):A1566–A1589, 2018.

[13] M. Jarke, B. Neumann, Y. Vassiliou, and W. Wahlster. KBMS Requirements of Knowledge-
Based Systems. In M. L. Brodie, J. Mylopoulos, J. W. Schmidt, J. W. Schmidt, and C. Thanos,
editors, Foundations of Knowledge Base Management, pages 381–394. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 1989.

[14] S. Jeong, K. Chiba, and S. Obayashi. Data Mining for Aerodynamic Design Space. In 23rd
AIAA Applied Aerodynamics Conference. American Institute of Aeronautics and Astronautics.

[15] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G.Lilienthal. Simulator sickness question-
naire: An enhanced method for quantifying simulator sickness. The International Journal of
Aviation Psychology, 3(3):203–220, DOI:10.1207/s15327108ijap0303_3, 1993.

[16] Leap Motion. Leap motion. https://www.leapmotion.com/, Last accessed: Nov 2018.

19

[17] G. D. Lecakes, M. Russell, S. Mandayam, J. A. Morris, and J. L. Schmalzel. Visualization of
multiple sensor measurements in a VR environment for integrated systems health management
in rocket engine tests. In 2009 IEEE Sensors Applications Symposium, pages 132–136, Feb.
2009.

[18] Microsoft. Xbox gaming platform. https://www.microsoft.com, Last accessed: Nov

2018.

[19] D. W. Mizell. Virtual reality and augmented reality in aircraft design and manufacturing. In

Proceedings of WESCON ’94, pages 91–, Sept. 1994.

[20] G. J. Nalepa. Modeling with Rules Using Semantic Knowledge Engineering. Intelligent Sys-

tems Reference Library. Springer International Publishing, 2018.

[21] J. Nielsen. Enhancing the Explanatory Power of Usability Heuristics. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, CHI ’94, pages 152–158, New
York, NY, USA, 1994. ACM.

[22] J. Nielsen. Usability Inspection Methods. In Conference Companion on Human Factors in

Computing Systems, CHI ’94, pages 413–414, New York, NY, USA, 1994. ACM.

[23] T. Nokuo and T. Sumii. Head mounted display, Dec. 23 2014. US Patent App. 29/502,182.

[24] Oculus VR. Oculus Rift, Oculus Quest, Oculus Go. https://www.oculus.com, Last ac-

cessed: Nov 2018.

[25] Oculus VR. Oculus Utilities for Unity 5. https://developer.oculus.com, Last accessed:

Nov 2018.

[26] F. Pukelsheim. Optimal design of experiments, volume 50. siam, 1993.
[27] W. Ribarsky, J. Bolter, A. Op Den Bosch, and R. Van Teylingen. Visualization and Analysis
Using Virtual Reality. Computer Graphics and Applications, IEEE, 14:10–12, Feb. 1994.
[28] D. J. Roberts, A. S. Garcia, J. Dodiya, R. Wolﬀ, A. J. Fairchild, and T. Fernando. Collaborative
telepresence workspaces for space operation and science. In 2015 IEEE Virtual Reality (VR),
pages 275–276, Mar. 2015.

[29] R. A. Ruddle.

The eﬀect of environment characteristics and user interaction on lev-
IEEE Virtual Reality 2004, pages 141–285,

els of virtual environment sickness.
DOI:10.1109/VR.2004.1310067, 2004.

[30] M. Russell, G. D. Lecakes, S. Mandayam, J. A. Morris, M. Turowski, and J. L. Schmalzel.
Acquisition, interfacing and analysis of sensor measurements in a VR environment for inte-
grated systems health management in rocket engine tests. In 2009 IEEE Sensors Applications
Symposium, pages 128–131, Feb. 2009.

[31] M. Sagardia, K. Hertkorn, T. Hulin, R. Wolﬀ, J. Hummell, J. Dodiya, and A. Gerndt. An
interactive virtual reality system for on-orbit servicing. In 2013 IEEE Virtual Reality (VR),
pages 1–1, Mar. 2013.

[32] J. Savall, D. Borro, J. J. Gil, and L. Matey. Description of a haptic system for virtual main-
tainability in aeronautics. In IEEE/RSJ International Conference on Intelligent Robots and
Systems, volume 3, pages 2887–2892 vol.3, Sept. 2002.

20

[33] P. Seshadri, S. Shahpar, P. Constantine, G. Parks, and M. Adams. Turbomachinery active
subspace performance maps. Journal of Turbomachinery, 140(4):DOI: 10.1115/GT2017–
64528, 2018.

[34] P. Seshadri, S. Yuchi, G. Parks, and S. Shahpar. Supporting multi-point fan design with di-

mension reduction. arXiv, 2019.

[35] P. Seshadri, S. Yuchi, and G. T. Parks. Dimension reduction via gaussian ridge functions.

SIAM/ASA J. of Uncertainty Quantiﬁcation (accepted), 2018.

[36] Shakal, C., https://grabcad.com/chris.shakal. Trent 900 turbofan model, https:
//grabcad.com/library/trent-900-turbofan-1. GrabCAD https://grabcad.com,
Last accessed: September 2019.

[37] S. Shefelbine, J. Clarkson, R. Farmer, and S. Eason. Good Design Practice for Medical De-
vices and Equipment - Requirements Capture. University of Cambridge Engineering Design
Centre and University of Cambridge Institute for Manufacturing, 2002.

[38] B. Shneiderman. Why not make interfaces better than 3D reality? IEEE Comput. Graph.

Appl., 23(6):12–15, DOI:10.1109/MCG.2003.1242376, Nov. 2003.

[39] R. J. Stone, P. B. Panﬁlov, and V. E. Shukshunov. Evolution of aerospace simulation: From
immersive Virtual Reality to serious games. In Proceedings of 5th International Conference
on Recent Advances in Space Technologies - RAST2011, pages 655–662, June 2011.

[40] G. M. Stump, M. Yukish, T. W. Simpson, and J. J. O’Hara. Trade space exploration of satellite
datasets using a design by shopping paradigm. In 2004 IEEE Aerospace Conference Proceed-
ings (IEEE Cat. No.04TH8720), volume 6, pages 3885–3895 Vol.6, Mar. 2004.

[41] I. E. Sutherland. A Head-mounted Three Dimensional Display. AFIPS ’68 (Fall, part I)
Proceedings of the December 9-11, 1968, fall joint computer conference, part I, pages 757–
764, DOI:10.1145/1476589.1476686, 1968.

[42] S. K. Tadeja, T. Kipouros, and P. O. Kristensson. Exploring Parallel Coordinates in Virtual
Reality. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing
Systems (CHI’19), Glasgow, Scotland UK, May 2019.

[43] S. K. Tadeja, T. Kipouros, and P. O. Kristensson. IPCP: Immersive Parallel Coordinates Plots
for Engineering Design Processes. In Proceedings of AIAA SciTech Forum and Exposition,
Orlando, Florida, Jan. 2020, forthcoming.

[44] J. J. Thomas and K. A. Cook. A visual analytics agenda.

26(1):10–13, Jan. 2006.

IEEE Comput. Graph. Appl.,

[45] Unity3D Game Engine. Unity VR Samples pack. https://assetstore.unity.com/
packages/essentials/tutorial-projects/vr-samples-51519, Last accessed: Nov
2018.

[46] M. Wagner, K. Blumenstein, A. Rind, M. Seidl, G. Schmiedl, T. Lammarsch, and
W. Aigner. Native Cross-Platform Visualization: A Proof of Concept Based on the Unity3d
Game Engine.
20th International Conference Information Visualisation., pages 39–44,
DOI:10.1109/IV.2016.35, 2016.

21

[47] J. Wright, F. Hartman, and B. Cooper.

Immersive environment technologies for planetary

exploration. In Proceedings IEEE Virtual Reality 2001, pages 183–190, Mar. 2001.

22

