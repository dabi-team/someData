2021 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 29 Nov. – 2 Dec. 2021, Lloret de Mar, Spain

Pluto: Motion Detection for Navigation in a VR Headset

Dmitri Kovalenko1
dmitri.a.kovalenko@gmail.com

Artem Migukin1
migukin.a@gmail.com

Svetlana Ryabkova
s.ryabkova@samsung.com

Vitaly Chernov1
vitaliy.chernov@gmail.com

Samsung R&D Institute Russia

1
2
0
2

g
u
A
1
1

]

O
R
.
s
c
[

2
v
0
3
0
2
1
.
7
0
1
2
:
v
i
X
r
a

Abstract—Untethered, inside-out tracking is considered a new
goalpost for virtual reality, which became attainable with ad-
vent of machine learning in SLAM. Yet computer vision-based
navigation is always at risk of a tracking failure due to poor
illumination or saliency of the environment. An extension for a
navigation system is proposed, which recognizes agents motion
and stillness states with 87% accuracy from accelerometer data.
40% reduction in navigation drift is demonstrated in a repeated
tracking failure scenario on a challenging dataset.

Index Terms—inertial navigation, motion detection, deep learn-

ing, time series analysis

I. INTRODUCTION

The virtual reality has been a subject of extensive research
for decades. The advancements in algorithms and hardware
brought commodity setups to the mass-market. Novel systems
provide untethered experience, where a user is not constrained
by the cable connecting a headset to a computer. Spatial re-
strictions due to the infra-red beacon operation range, inherent
in outside-in navigation [1], were removed with the inside-
out navigation approach, supported by SLAM (Simultaneous
Localization and Mapping) running onboard, processing stereo
or depth camera images, enhanced by sensor fusion with
accelerometer and gyroscope data [2].

Inertial sensors providing an input to a navigation system
enable high-frequency position updates, but are impractical
as a sole data source, as the double integration error accu-
mulates [3]. Yet the situation may arise when the camera-
based navigation system enters a failure mode due to motion
blur, textureless environments, low illumination, occlusions,
and rigid world assumption violations. Current headsets halt
position updates until the tracking is reestablished. The user
experience would not have been disrupted if a navigation
system was equipped with a fallback technique for position
estimates during a tracking failure. A proprioceptive nature of
IMU (Inertial Measurement Unit) makes it a valid candidate
for sensor fusion in SLAM, as wealth of information may
be inferred from its data with help of machine learning. The
proposed method recognizes stillness and motion states of
an agent and sends linear acceleration and velocity pseudo-
thus reducing positioning
updates to a navigation system,
drift. We named it Pluto. Main contributions are:1 1. Novel
motion detector neural network, based on Temporal Causal

1D. Kovalenko, A. Migukin and V. Chernov made contributions during their
time in Samsung R&D Institute Russia, but currently are working for Yandex,
Huawei and Align Technology respectively.

978-1-6654-0402-0/21/$31.00 © 2021 IEEE

2. Nagivation system, robust to tracking
Convolutions [4];
failures, embodied in a prototype of virtual reality headset
(Fig. 1.c);
3. Evaluation on the real-world dataset of highly
discontinuous movements (Fig. 1.d).

II. RELATED WORK

Even though Pluto is a visual-inertial navigation system, the
main focus of this inquiry is a tracking failure handling, when
the system degenerates into a purely inertial one, which has
been addressed by a great number of works. The excellent
survey [5] on Pedestrian Dead Reckoning recognizes two
different approaches: 1. Inertial Navigation Systems; 2. Step
and Heading Systems. The proposed method would fall into
the former category under such a classiﬁcation.

A. Inertial Navigation Systems

These systems utilize "zero velocity" updates (ZUPT) as a
measure to contain the double integration error. Technically,
updates are velocity resets, triggered by a step event detector,
analyzing accelerometer data for peaks, as shown in [6]. The
detector accuracy is higher when the inertial sensor registers
acceleration, related solely to a gait, not other body motions.
Hence the sensor placement on a foot is widely preferred;
Skog et al. [7] take it to a limit, creating a low-drift system
with a sensor box of IMU and pressure sensors, placed inside
the heel of a boot. The velocity trend, caused by integration
is removed in [8]. Gusenbauer et
of accelerometer bias,
al. [9] consider extra correction sources: localization in a radio
footprint map, plausibility check with a building plan. Most
works report 0.14 − 2.3% relative positioning error, the lowest
achieved on level ground with a constant gait cycle, ensured
by a metronome [7], while higher drift was registered on rough
and sandy terrains by Sun [10]. Above works mostly consider
2D navigation, because the gravity inﬂuence on acceleration
measurements is stronger along the vertical axis. An extension
to 2.5D was achieved by the ﬂoor change recognition in [11].
Notable contributions made recently are: [12], [13], where
a neural network was trained to infer 3D velocity of an
agent, providing a useful constraint to EKF (Extended Kalman
Filter). In [14], a convolutional LSTM (Long Short-Term
Memory) network was applied to public datasets in an end-
to-end fashion, to replace EKF as a source of 6D inertial
odometry. Liu et al. [15] propose a neural network estimating
3D displacements along with their uncertainties from IMU
data, which enables to use displacements as a source within a
probabilistic framework of EKF.

 
 
 
 
 
 
The evolution in ZUPT publications is three-dimensional:
2. Cheaper IMU sensors,
1. Stronger neural networks [16];
3. More natural placements [17],
setups with fewer IMUs;
head-mounted and hand-held, instead of belt strap-down or
shoe mount. The present study is aligned with all
three
dimensions, utilizing a single low-cost head-mounted IMU for
navigation.

B. Step and Heading Systems

The alternative approach is not to integrate acceleration at
all, but to count steps and infer their direction. The foot-
mounted sensor boxes are not required in that case, as demon-
strated with a belt strap-down in [18]. Goyal et al. derive
heading from 3D attitude, implying only a forward movement
of an agent, which might be true for many commuter scenarios,
but does not hold for a virtual reality interaction. Jiang
et al. estimate heading as a direction of spectrally ﬁltered
accelerometer data [19]. A neural network provided an online
step length calibration in [20] for a mixed indoor/outdoor
case, where GPS was exploited. Beauregard et al. were among
a few, who also considered a head-mounted setup. Some
pedestrian dead reckoning systems rely on an assumption
that an agent moves only forward, having its orientation and
heading always aligned. Windau et al. removes this constraint
in their work [21].

The inherent discretisation of the step and heading approach
makes it less enticing for virtual reality, hence we forgo that
in the Pluto navigation system.

C. Action Classiﬁcation

A coalescent area of research, not being directly involved
with navigation, is highly relevant: human action classiﬁcation.
Attempts to recognize pedestrian action were conducted with
various setups: from a minimalistic wristband [22] to a set
of 19 IMUs mounted on every part of a human body [23],
with 95% F1 score reported by latter with Deep Convolutional
LSTM for 17 actions classiﬁcation. Anguita et al. [24] propose
a waistband dataset and SVM action classiﬁer performing with
89% accuracy. This inquiry was taken further with Recurrent
Neural Networks, achieving 94% on the same dataset in [25].
3-class time series classiﬁer was implemented [26], discern-
ing a steady walk, rest and irregular motions. Contrary to the
above works, Sun et al. [26] have also demonstrated a way
the classiﬁer inﬂuences the navigation system, halting step
counting during two latter states. A similar classiﬁer was put
forward more recently in [27].

The present study went beyond accuracy evaluation, inves-
tigating detection delays and intervals between false positives,
which provides insights into the system behavior over time.
We proceed with detailing the design of navigation system
and motion detector network.

III. PLUTO NAVIGATION SYSTEM

The headset prototype (Fig. 1.c), implementing virtual real-
ity capabilities and hosting the online visual-inertial navigation
system (Pluto) was devised. The orientation estimation system

(b)

(c)

(a)

(d)

a) An experiment participant

Fig. 1.
taking a backward step b) The
virtual scene explored by study participants c) The virtual reality headset
prototype; the IMU sensor placement is marked (approximately) by a cross
d) A participant movement trajectory in a bird-eye view during the study; no
instructions on a gait pattern were given, except to explore a virtual reality
scene naturally

is purely inertial and closely follows [28], using complemen-
tary ﬁlters with accelerometer and magnetometer data for the
tilt and yaw drift correction. The magnetic ﬁeld is modeled as
in [29], with an additional capability to detect magnetic ﬁeld
anomalies and recalibrate when necessary. Strong correlations
were registered between IMU temperature and magnitudes of
accelerometer and gyroscope noise. Instead of random walk
bias modeling, the online calibration accumulates samples and
solves for biases for every temperature value (IMU provides
temperature measurements at 0.5◦ C increments). Optimal
sample sizes are determined accordingly with [30].

The orientation estimation system has shown a competitive

yaw drift of 5◦ per hour, given a low-cost sensor.

The visual-inertial position estimation is a Kalman Filter,
with a state consisting of translation and linear velocity as in
Eq. 1.

µt|t−1 = (cid:2)tx, vx, ty, vy, tz, vz

(cid:3)T

(1)

The prediction step in Eq. 2 (µ, Σ)t−1|t−1 → (µ, Σ)t|t−1
runs at 500 Hz on accelerometer measurements at, which
have: 1. Been brought into the world coordinate frame by
2. The gravity vector substracted;
an orientation estimate;
3. The noise ﬁltered by a high-pass ﬁlter.

The ﬁlter operates in 3D, but without a loss of generality,
Eqs. 2, 3 are formulated in 1D for compactness. F is a state
transition matrix, matrix G maps an acceleration measurement
to a state, matrix H maps a state to a velocity measurement.

µt|t−1 = F µt−1|t−1 + Gat
Σt|t−1 = F Σt−1|t−1F T + GQtGT

F =

(cid:21)

(cid:20)1 ∆t
1
0

G =

(cid:21)

(cid:20) ∆t2
2
∆t

H = (cid:2)0

1(cid:3)

(2)

−1.5−1.0−0.50.00.51.01.52.02.5x, m−1.0−0.50.00.51.01.5z, mFull motion trajectory of a participantGTQt and Rt are accelerometer and velocity measurement
covariances respectively, maintained as empirical distributions
in a sliding window fashion.

The correction step in Eq. 3 (µ, Σ)t|t−1 → (µ, Σ)t|t uses
velocity vt, supplied by a visual tracker or a motion capture
system (the latter is the case in experiments, Sec. V-C).

µt|t = µt|t−1 + Kt(vt − Hµt|t−1)
Σt|t = (I − KtH)Σt|t−1
Kt = Σt|t−1H T (HΣt|t−1H T + Rt)−1

(3)

Timestamps in state subscripts imply that the visual tracker
and the IMU run by the aligned clock, which is not true in
practice and moreover, the latency of visual updates varies.
The interpolation by agent kinematics would beneﬁt the sys-
tem accuracy and may be implemented by e.g. a sliding
window Kalman Filter, but its practical realization is out of
scope for the present study.

The main proposition is the way acceleration and velocity
is provided to the Kalman Filter: at, vt are replaced with zero
pseudo-updates 03×1 when the system is in a stilless mode
and processed normally otherwise. The mode transitioning is
further explained.

Fig. 2. Pluto motion detector architecture consists of the TCN classiﬁer (blue)
and LSTM (purple) for smoothing over the logits. The numbers after block
names are hidden dimension sizes. The TCN blocks amount and kernel size
were selected to cover all timestamps in a sliding-window. The input is a
window of 3D acceleration data stream, outputs are likelihoods of possible
agent states: {motion, stillness}.

V. EXPERIMENTS

A thorough evaluation of Pluto navigation system comprises
two parts: 1. Pluto motion detector classiﬁcation performance
is studied in comparison with several baselines; 2. Pluto nav-
igation system positioning drift is evaluated under conditions
of repeated tracking failures. We proceed with the speciﬁcation
of collected data, which were made available for the beneﬁt
of the research community2.

IV. MOTION DETECTOR

A. Dataset

Pluto navigation system transitions to the pseudo-update
mode by virtue of the motion detector, which is a deep neural
network-based two-class classiﬁer, taking a 3D acceleration
data stream as an input and outputting a label {motion,
stillness}.

Windows of 3D acceleration data, registered during partic-
ipants walking forward, backward, or sideways, regadless of
a head rotating, tilting, or being kept still to the body are
considered motion. Other windows of 3D acceleration data,
when a participant stays still on feet, even if their head rotates
or tilts, are considered stillness.

The input and output tensors are [B, 100, 3] and [B, 1, 2]
respectively with dimensions encoding a batch size,
time
domain, and data dimensionality. During inference the batch
size is 1 and data are supplied by the sliding-window cache,
which shifts by 1 timestamp at consecutive inference calls.

TCN (Temporal Convolutional Network), based on dilated
causal convolutions, has demonstrated superior performance
on a wide range of sequence modeling tasks [4]. Experiments
(Tab. I) show that TCN overperforms baselines accuracy by a
notable margin in the accelerometer data classiﬁcation.

Despite the higher accuracy, TCN is at a disadvantage from
standpoint of false-positive rate, which is required to be as low
as possible for a virtual reality application. We attribute that to
the TCN output being independent of the network inferences
on previous timestamps. The proposed solution (Fig. 2) stacks
TCN with a stateful LSTM, which is fed with TCN logit
outputs, hence explicitly taking the temporal context
into
account. Networks are trained separately.

The dataset was taken with a headset prototype, spanning
30 min in time, where 4 subjects, varying in age and gender
were asked to explore a virtual reality scene, while moving
freely and naturally (Fig. 1.a). During the procudure following
were collected: 1. IMU data (accelerometer, gyroscope, mag-
2. 6D positioning ground truth. High-precision
metometer);
6D positioning ground truth data were collected with a motion
capture system OptiTrack® at 125 Hz. The environment for
the study was a well
lighted lab room, free of dynamic
agents other than a subject. Recorded trajectories are rich with
sporadic movements, side- and backward steps, participants
lean and change direction and orientation restlessly, rotate
and tilt their heads, while observing a virtual reality scene.
Data, registered from one of subjects, are held out for testing,
remaining data are used in a network training. Such a parti-
tioning enables to verify the networks ability to generalize to
inputs produced by a previously unseen person.

the

integrated

navigation

Experiments with

system
(Sec. V-C) were carried out on a testing partition, which
was split onto 16 sequences of variable length (5 . . . 12 s) at
random. Each of these tracks is having a simulated tracking
lasting a random time (2 . . . 8 s) at a
failure introduced,
random offset from the start. The four tracks shown in Fig. 5
are segments of a full sequence from Fig. 1.d.

B. Motion Detection Performance

The agent state classiﬁcation evaluation starts with a qualita-
tive visualization of motion starts and stops, as were registered

2https://github.com/wf34/pluto

4/27/2020arch.drawio1/1LSTM State TupleLogitsLogitsLSTM Cell 100 LSTM Cell 100 LSTM Cell 100TCN Block 200GT labelsTCN Block 200TCN Block 200TCN Block 200TABLE II
MOTION DETECTION METRICS

Method Accuracy Precision Recall F1

Otsu
Pluto

0.746
0.874

0.758
0.825

0.745 0.752
0.960 0.887

Pluto motion detector shows a superior classiﬁcation performance to the baseline on a
dataset testing partition (Sec. V-B).

is a registered acceleration in the global coordinate frame.
TCN and CNN networks were trained with Adam optimizer,
LSTM with RMSProp and the cross-entropy loss was used
for all. The hyperparameter choice (the sliding window size
of 100 timestamps = 0.2 s) was done by a grid search (with
boundaries 0.05 . . . 3 s) on a training partition with the baseline
CNN network. All networks were implemented in Tensorﬂow,
underwent several
iterations of coarse-to-ﬁne tuning, have
approximately the same size and were trained for the same
time. The times per epoch in Tab. I were registered for CPU
training at 20 cores on Intel Xeon E5. Pluto motion detector
inference is tangible for realtime running on the headset
prototype at a decreased frequency (10 Hz).

The achieved accuracy of 87% (Tab. II) could have been
improved by the dataset increase, due to movements of the
test subject (and their IMU accelerations) laying outside of a
network domain, because only a small set of anthropometri-
cally different study subjects comprise the training partition.
Nonetheless, network overﬁtting was avoided, which evi-
dences from accuracy being approximately equal on training
and testing dataset partitions.

Attempted network training approaches, which have shown
no classiﬁcation improvement are: data augmentation by afﬁne
transformations, input dimensionality extension by gyroscope
and magnetic data, pretraining on synthetic3 accelerations.

User experience tests in virtual reality have shown a low
correlation with the accuracy metric because the temporal
context is not captured: the metric value is the same, whether
a detection is off just by a one timestamp or 50. To adjust
for that, an additional performance evaluation was conducted
with [33]: 1. Mean detection delay;
2. Mean interval between
false-positive detections. These metrics, contrary to accuracy,
precision, recall, F1 are in a temporal domain. The detection
delay is a time in seconds since a labeled ground truth event
until its true positive detection, issued by the detector. The
interval between false positive detections is a time in seconds
between consecutive detections, issued by the detector and not
having labeled ground truth events corresponding to them.

Due to Tab. III, Pluto motion detector issues false positive
detections 4 times less often than Otsu on average, while has
approximately same true positive detection delays. To capture
results beyond a mean and variance, the histogram of motion
start detection delays is shown in Fig. 4.

A sizable variance in metric magnitudes is found in Tab. III.
It is due to the fact that neural network sensitivity is hard to

Fig. 3. Pluto motion detector output labels in comparison with motion capture
ground truth labels (GT). A low false positive rate with low detection delays
are demonstrated.

TABLE I
NEURAL NETWORKS TRAINING AND INFERENCE

Model #weights, M Depth

(in blocks)

Train time
(per epoch, h)

Train

Validation

Test

accuracy, % loss

accuracy, % loss

accuracy, % loss

CNN
LSTM
TCN
Pluto

2.7
1.9
1.4
1.4 + 0.04

5
4
4
4+3

0.28
1.43
0.62
1.55

0.899
0.853
0.948
0.901

0.324
0.473
0.139
0.244

0.751
0.829
0.889
0.811

0.702
0.596
0.304
0.437

0.723
0.820
0.871
0.869

1.013
0.687
0.371
0.376

TCN (a core building block of Pluto) is the best architecture for signal processing of
accelerometer. Pluto motion detector, which connects TCN to LSTM, trades off a
moderate accuracy loss for a radical false positive rate reduction (Tab. III).

by motion capture and classiﬁed by Pluto motion detector on
a testing sequence in Fig. 3. Inferred labels do not lag and
demonstrate a low false-positive rate. One nuisance is that the
system was unable to detect short stops between sequences
of steps in 53 . . . 55, 60 . . . 65 s., due to the participants head
still slightly moving, while the stepping paused. These micro
stops are included in ground truth class labels, which were
automatically converted from 6D ground truth trajectories with
the following thresholding on velocity: (cid:107)v(cid:107)2 < 0.2 m
s .

Networks converge despite a few mislabelings, but label-
ing imperfections also produce outliers in evaluation. Fig. 4
shows detection delay histograms for two most competitive
algorithms. Classiﬁcation time series from Fig. 3 allow to
claim that longer delays (> 5 s) are absent in reality and
appear on a ﬁgure due to ground truth event mislabelings.

Pluto motion detector is compared to LSTM, CNN, TCN,
and a variant [31] of signal processing classic, Otsu thresh-
olding [32]. Results in Tab. I suggest
that TCN, a core
building block of Pluto motion detector, is superior in terms
of accuracy, training time and weights amount. The variant
of Adaptive Otsu thresholding, which was employed in an
evaluation, operates on a 1D signal (a norm of registered
acceleration ||a||2), maintains its histogram and updates a
threshold, which splits the histogram into two parts with a
maximal inter-class variance. A relation between a current
sample and the threshold value is used for classiﬁcation.

All evaluated networks were trained with a 3D signal, which

3https://github.com/Aceinna/gnss-ins-sim

4045505560657075Time, sStillnessMotionMotion states over timeGTPlutoEvents, #

TABLE IV
RELATIVE POSITIONING DRIFT, %

if no STF KF = Pluto

Pluto with STF

µ

Avg over 16 seqs

2.52

σ

0.93

µ

σ

KF with STF
σ
µ

7.58

7.28

12.95

15.07

The baseline (KF) shows 5 times drift increase under conditions of simulated tracking
failures (STF) relative to a failure-free tracking, while drift of the proposed Pluto
system grows moderately.

Time, s
Fig. 4. Histogram of delays in detecting a motion start, [sec]. Pluto motion
detector shows more low-delay events when compared to adaptive Otsu
thresholding. Outliers in a right part of the domain were addressed in Sec. V-B

TABLE III
MOTION DETECTION PERFORMANCE, [SEC]

Method

Detection Delayl

Starts Detection
Interval
between
false positivesh

Stops Detection

Detection Delayl

µ

σ

µ

σ

µ

σ

2.409
Otsu
CNN
2.634
LSTM 0.292
1.893
TCN
2.389
Pluto

2.324
17.182
0.346
1.935
2.128

8.856 12.013 1.435
0.890 0.316
0.487
0.431 0.252
0.188
2.969 1.071
2.125
14.889 18.425 1.771

1.205
0.568
0.246
1.195
1.427

Interval
between
false positivesh

µ

σ

11.060 10.829
0.881
0.492
0.188
0.429
3.242
2.160
40.093 48.488

Pluto motion detector shows low detection delays and high intervals between false
MFPI for start and stop events both. l means
alarms, outperforming competitors in MDD

lower is better; h means higher is better.

Fig. 5. Navagation system trajectories, produced in Sec. V-C experiments
and selected at random, provide comparison of Pluto (blue) with the ground
truth (red) and the baseline (Kalman ﬁlter, black). Pluto navagation system is
less prone to drift during tracking failures.

adjust: LSTM has converged to be very sensitive to signal
changes while producing more false positives, while TCN is
less prone to false positive detections, but results in longer
delays. The proposed motion detector Pluto combines advan-
tages of both, enabling the higher responsiveness with fewer
false positives.

C. Effects of Motion Detection on Navigation

The motion detector capable of discerning an agents still-
ness and motion may beneﬁt a navigation system, as proposed
in Sec. III. The claim is evaluated in simulated experiments,
conducted on 16 sequences, taken from testing data, as de-
scribed in Sec. V-A. The navigation system is a Kalman
ﬁlter, with prediction steps by IMU acceleration and correction
steps by linear velocity. Velocity and orientation estimates are
provided by a motion capture system. Velocity updates are not
provided during a simulated tracking failure.

The navigation precision is estimated by Relative Posi-
tioning Drift, which is a fraction of error accumulated by
a navigation system since the speciﬁed point in time. In a
present case, the reference timestamp was chosen such that a
relative path between the reference timestamp and the current
timestamp is 1 m, measured by a motion capture system. The
mean average relative drift over 16 sequences and its variance
are presented in Tab. IV. Also, four sequences were picked at
random and visualized in Fig. 5.

Obtained results show that motion detector capabilities may
substantially improve the resilience of a navigation system in
a face of tracking failures.

VI. CONCLUSIONS

A neural network-based motion detector was proposed,
which classiﬁes motion and stillness states of an agent with
87% accuracy while generalizing to the motion pattern of a
person, who was not present in training data. The detector is
capable of online operation, with the mean interval between
false-positive detections more than 14 seconds, which is longer
than a typical relocalization time of a computer vision-based
navigation system.

The impact of the motion classiﬁcation on navigation was
demonstrated, decreasing positioning drift by 40% in a re-
peated tracking failure scenario. A practical drift reduction
could be even higher because virtual reality users tend to
move less during a tracking failure occurrence. In that case,
the motion detector would register stillness and update a
system kinematic state accordingly. A working prototype was
developed and tested; a high-quality dataset with motion
capture ground truth was obtained and made available.

We are grateful to Ilya Nedelko, Sergiy Pometun, Oleg
Muratov, Tarek Dakhran, Andriy Marchenko and Mikhail
Rychagov for their contributions and support. We extend
gratitude to anonymous reviewers for the thoughtful input.

02468024681012Motion Start Delay DistributionPlutoOtsu−1.0−0.50.00.51.0x, m−1.00−0.75−0.50−0.250.000.250.500.751.00z, mSeq #1GTKF with STFPluto with STF−0.6−0.4−0.20.00.20.40.60.8x, m−0.20.00.20.40.60.8z, mSeq #5GTKF with STFPluto with STF−0.250.000.250.500.751.001.25x, m−0.4−0.20.00.20.40.60.8z, mSeq #6GTKF with STFPluto with STF−1.0−0.50.00.51.01.52.0x, m−2.0−1.5−1.0−0.50.0z, mSeq #13GTKF with STFPluto with STFREFERENCES

[1] D. R. Colomer, G. Lopes, D. Kim, C. Honnet, D.
Moratal, and A. Kampff. “HIVE Tracker: a tiny, low-
cost, and scalable device for sub-millimetric 3D posi-
tioning”. In: Augmented Human 9 (2018), pp. 1–8.
[2] E. S. Jones and S. Soatto. “Visual-inertial navigation,
mapping and localization: A scalable real-time causal
approach”. In: The International Journal of Robotics
Research 30.4 (2011), pp. 407–430.

[3] C. Verplaetse. “Inertial proproceptive devices: Self-
motion-sensing toys and tools”. In: IBM Systems Jour-
nal 35.3.4 (1996), pp. 639–650.

[4] S Bai, J. Kolter, and V Koltun. “An empirical evaluation
of generic convolutional and recurrent networks for
sequence modeling. arXiv 2018”. In: arXiv preprint
arXiv:1803.01271 ().

[5] R. Harle. “A survey of indoor inertial positioning
systems for pedestrians.” In: IEEE Communications
Surveys and Tutorials 15.3 (2013), pp. 1281–1293.
[6] L. Xiaofang, M. Yuliang, X. Ling, C. Jiabin, and S.
Chunlei. “Applications of zero-velocity detector and
Kalman ﬁlter in zero velocity update for inertial navi-
gation system”. In: Guidance, Navigation and Control
Conference (CGNCC). IEEE. 2014, pp. 1760–1763.
I. Skog, J.-O. Nilsson, and P. Händel. “Evaluation of
zero-velocity detectors for foot-mounted inertial nav-
igation systems”. In: Indoor Positioning and Indoor
Navigation (IPIN). IEEE. 2010, pp. 1–6.

[7]

[8] R. Feliz, E. Zalama, and J. Gómez-García-Bermejo.
“Pedestrian Tracking Using Inertial Sensors”. In: Jour-
nal of Physical Agents 3 (Jan. 2009). DOI: 10.14198/
JoPha.2009.3.1.05.

[9] D. Gusenbauer, C. Isert, and J. Krösche. “Self-contained
indoor positioning on off-the-shelf mobile devices”. In:
Indoor positioning and indoor navigation (IPIN). IEEE.
2010, pp. 1–9.

[10] X. Sun, K. Wu, Y. Li, and K. Di. “A Zupt-Based Method
for Astronaut Navigation on Planetary Surface and
Performance Evaluation under Different Locomotion
Patterns”. In: ISPRS - International Archives of the Pho-
togrammetry, Remote Sensing and Spatial Information
Sciences XL-4 (Mar. 2014), p. 239. DOI: 10 . 5194 /
isprsarchives-XL-4-239-2014.

[11] M. G. Puyol, D. Bobkov, P. Robertson, and T. Jost.
“Pedestrian simultaneous localization and mapping in
multistory buildings using inertial sensors”. In: IEEE
Transactions on Intelligent Transportation Systems 15.4
(2014), pp. 1714–1727.

[12] S. Cortés, A. Solin, and J. Kannala. “Deep Learning
Based Speed Estimation for Constraining Strapdown
Inertial Navigation on Smartphones”. In: IEEE 28th
International Workshop on Machine Learning for Signal
Processing (MLSP). IEEE. 2018, pp. 1–6.

[13] T. Feigl, S. Kram, P. Woller, R. H. Siddiqui, M.
Philippsen, and C. Mutschler. “RNN-aided human ve-

[14]

locity estimation from a single IMU”. In: vol. 20.
13. Multidisciplinary Digital Publishing Institute, 2020,
p. 3656.
J. P. Silva do Monte Lima, H. Uchiyama, and R.-i.
Taniguchi. “End-to-End Learning Framework for IMU-
Based 6-DOF Odometry”. In: Sensors 19.17 (2019),
p. 3777.

[15] W. Liu, D. Caruso, E. Ilg, J. Dong, A. I. Mourikis,
K. Daniilidis, V. Kumar, and J. Engel. “TLIO: Tight
Learned Inertial Odometry”. In: IEEE Robotics and
Automation Letters 5.4 (2020), pp. 5653–5660.
[16] B. Wagstaff and J. Kelly. “LSTM-based zero-velocity
detection for robust inertial navigation”. In: 2018 Inter-
national Conference on Indoor Positioning and Indoor
Navigation (IPIN). IEEE. 2018, pp. 1–8.

[17] H. Yan, Q. Shan, and Y. Furukawa. “RIDI: Robust
IMU double integration”. In: Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV). 2018,
pp. 621–636.

[18] P. Goyal, V. J. Ribeiro, H. Saran, and A. Kumar. “Strap-
down pedestrian dead-reckoning system”. In: Indoor
Positioning and Indoor Navigation (IPIN). IEEE. 2011,
pp. 1–7.

[19] W. Jiang and Z. Yin. “Human tracking using wearable
sensors in the pocket”. In: IEEE Global Conference on
Signal and Information Processing (GlobalSIP). IEEE.
2015, pp. 958–962.

[20] S. Beauregard. “A helmet-mounted pedestrian dead
reckoning system”. In: 3rd International Forum on
Applied Wearable Computing (IFAWC). VDE. 2006,
pp. 1–11.
J. Windau and L. Itti. “Walking compass with head-
mounted IMU sensor”. In: 2016 IEEE International
Conference on Robotics and Automation (ICRA). IEEE.
2016, pp. 5542–5547.

[21]

[22] M. Gjoreski, H. Gjoreski, M. Luštrek, and M. Gams.
“Recognizing atomic activities with wrist-worn ac-
celerometer using machine learning”. In: Proceedings
of the 18th International Multiconference Information
Society (IS), Ljubljana, Slovenia. 2015, pp. 10–11.
[23] F. J. Ordóñez and D. Roggen. “Deep convolutional and
lstm recurrent neural networks for multimodal wearable
activity recognition”. In: Sensors 16.1 (2016), p. 115.
[24] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. Reyes-
Ortiz. “Human Activity Recognition on Smartphones
Using a Multiclass Hardware-Friendly Support Vector
Machine”. In: vol. 7657. Dec. 2012, pp. 216–223. ISBN:
978-3-642-35394-9. DOI: 10.1007/978- 3- 642- 35395-
6_30.

[25] LSTMs for Human Activity Recognition. 2018. URL:
https://github.com/guillaume-chevalier/LSTM-Human-
Activity-Recognition (visited on 05/31/2018).

[26] Z. Sun, X. Mao, W. Tian, and X. Zhang. “Activ-
ity Classiﬁcation and Dead Reckoning for Pedestrian
Navigation with Wearable Sensors”. In: Measurement

[27]

Science and Technology 20 (Nov. 2008), p. 015203.
DOI: 10.1088/0957-0233/20/1/015203.
J. Rantanen, M. Makela, L. Ruotsalainen, and M.
Kirkko-Jaakkola. “Motion Context Adaptive Fusion of
Inertial and Visual Pedestrian Navigation”. In: Sept.
2018, pp. 206–212. DOI: 10.1109/IPIN.2018.8533872.
[28] S. M. LaValle, A. Yershova, M. Katsev, and M.
Antonov. “Head tracking for the Oculus Rift”. In:
International Conference on Robotics and Automation
(ICRA). IEEE. 2014, pp. 187–194.

[29] T. Ozyagcilar. “Calibrating an ecompass in the presence
of hard and soft-iron interference”. In: Freescale Semi-
conductor Ltd (2012), pp. 1–17.

[30] A. Foi, V. Katkovnik, and K. Egiazarian. “Point-
wise shape-adaptive DCT for high-quality denoising
and deblocking of grayscale and color images”. In:
IEEE Transactions on Image Processing 16.5 (2007),
pp. 1395–1411.

[31] A Migukin, D Kovalenko, S Ryabkova, and V Chernov.
Method and device for strap down inertial navigation.
RU Patent 2685767C1, 2018.08.13.

[32] N. Otsu. “A threshold selection method from gray-level
histograms”. In: IEEE transactions on systems, man,
and cybernetics 9.1 (1979), pp. 62–66.

[33] Detection of abrupt changes: theory and application.

Prentice Hall Englewood Cliffs.

