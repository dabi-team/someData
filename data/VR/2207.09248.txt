Don’t Stop Learning: Towards Continual Learning
for the CLIP Model

Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang and Haoxuan Ding

1

2
2
0
2

l
u
J

0
2

]

V
C
.
s
c
[

2
v
8
4
2
9
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—The Contrastive Language-Image Pre-training
(CLIP) Model is a recently proposed large-scale pre-train model
which attracts increasing attention in the computer vision com-
munity. Beneﬁting from its gigantic image-text training set,
the CLIP model has learned outstanding capabilities in zero-
shot learning and image-text matching. To boost the recognition
performance of CLIP on some target visual concepts, it is often
desirable to further update the CLIP model by ﬁne-tuning some
classes-of-interest on extra training data. This operation, however,
raises an important concern: will the update hurt the zero-
shot learning or image-text matching capability of the CLIP,
i.e., the catastrophic forgetting issue? If yes, could existing
continual learning algorithms be adapted to alleviate the risk
of catastrophic forgetting? To answer these questions, this work
conducts a systemic study on the continual learning issue of
the CLIP model. We construct evaluation protocols to measure
the impact of ﬁne-tuning updates and explore different ways
to upgrade existing continual learning methods to mitigate the
forgetting issue of the CLIP model. Our study reveals the
particular challenges of CLIP continual learning problem and
lays a foundation for further researches. Moreover, we propose
a new algorithm, dubbed Learning without Forgetting via Re-
played Vocabulary (VR-LwF), which shows exact effectiveness
for alleviating the forgetting issue of the CLIP model.

Index Terms—Continual learning, vision-and-language, CLIP

model, knowledge distillation.

I. INTRODUCTION

Motivated by the success of pre-trained language models
like Bidirectional Encoder Representation from Transformers
(BERT) [1] and Generate Pre-Training Model (GPT) [2] in
natural language processing, the vision-language community
has started to embrace the idea of pre-training large neural net-
works on huge datasets. Various methods, such as ViLBERT
[3] and LXMERT [4], have been developed in recent years.
Contrastive Language-Image Pre-training (CLIP) [5] is one of
the latest advances in this direction. CLIP is optimized by
the contrastive loss over a training set of 400 million noisy
image-text pairs crawled from the Internet. The ﬁnal model
demonstrates unprecedented zero-shot classiﬁcation capacity.

(Corresponding author: Chunna Tian, Lingqiao Liu.)
Yuxuan Ding and Chunna Tian are with the School of Electronic
(e-mail: yxd-

Engineering, Xidian University, Xi’an 710071, China
ing@stu.xidian.edu.cn; chnatian@xidian.edu.cn).

Lingqiao Liu is with the Australian Institute for Machine Learn-
(e-mail:

the University of Adelaide, Adelaide 5005, Australia

ing,
lingqiao.liu@adelaide.edu.au).

Jingyuan Yang is with the College of Computer Science and Software

Engineering, Shenzhen University, China (e-mail: jyyang@szu.edu.cn).

Haoxuan Ding is with the Unmanned System Research Institute, North-
western Polytechnical University, Xi’an 710072, China (e-mial: haox-
uan.ding@mail.nwpu.edu.cn)

The training and inference pipelines are illustrated in Fig. 1.
The zero-shot inference is applied by text prompt technology,
the name of a class will be transformed into a sentence,
such as “this is a photo of [Class Name].”, and then contrast
with images (details are explained in Sec. II-A). Under the
circumstances, the CLIP tackles any classes which can be
described by language and outperforms state-of-the-art fully-
supervised baselines on 16 datasets without using any labeled
data. The model quickly sparks attention since its release,
which has been successfully applied in zero-shot detection [6],
open set detection [7] and video retrieval [8]–[10].

However, even with its gigantic volume of training data
and wide coverage of visual concepts, the CLIP model still
cannot attain satisfying performance in some visual categories.
A practical solution is to ﬁne-tune CLIP on extra training
examples collected for categories where CLIP falls short of
expectation. However, this na¨ıve solution raises a concern on
whether ﬁne-tuning CLIP would hurt its zero-shot learning
and/or image-text matching capabilities. This concern is
related to the catastrophic forgetting problem that has been
intensively studied in continual learning with the context of
image classiﬁcation. Thus a natural follow-up question is
whether existing continual learning approaches could help
alleviate the potential forgetting issue.

Those two questions motivate this paper. We conduct a
systematic study on the continual learning problem of the
CLIP model, focusing on the impact of additional ﬁne-tuning
on the zero-shot and text-image matching performance of the
CLIP model.1 Our study shows that the CLIP model also
suffers from the catastrophic forgetting problem: the zero-
shot learning and image-text matching capabilities would be
signiﬁcantly damaged after ﬁne-tuning. We further investigate
whether existing continual learning approaches can be adapted
as a solution. We explore this possibility by modifying four
representative continual learning approaches, i.e., LwF [11],
GeoDL [12], IMM [13], and RKR [14], from three commonly
used families of continual learning. Moreover, we propose a
new solution to address the forgetting issue of the CLIP model.
Our key idea is to enforce the prediction logits for a set of ran-
domly synthesized classes before and after the model update.
Despite its simplicity, this approach demonstrates excellent
performance in our experimental study and outperforms the
direct extensions of existing continual learning approaches. It
can mitigate the forgetting issue while making the CLIP still
be beneﬁted from ﬁne-tuning.

1We use the ”ViT-B/32” CLIP model in this paper.

 
 
 
 
 
 
2

Fig. 1. The workﬂow of the CLIP model. (a) The training of CLIP is conducted by a huge dataset of noisy image-text pairs. (b) At zero-shot inference, the
input of the text encoder is retrieval captions or text prompts. Image and text embeddings are generated by the encoders and compute the similarities. The
most similar match can be seen as the prediction.

To sum up, the main contributions of this paper are three-

calculated as

fold:

• We present a systematic study of continual

learning
problem for the CLIP model. An evaluation protocol is
designed to assess the learning and forgetting after ﬁne-
tuning the original CLIP.

• We explore the extension of popular existing continual

learning algorithms on the CLIP model.

• We propose a simple-but-effective solution named Learn-
ing without Forgetting via Replayed Vocabulary (VR-
LwF), which largely alleviates the catastrophic forgetting
after CLIP ﬁne-tuning.

p (y = c|x) =

exp (τ · (cid:104)f , tc(cid:105))
j=1 exp (τ · (cid:104)f , tj(cid:105))

(cid:80)K

,

f = CLIPi (x; θi) ,
tc = CLIPt (Prompt (yc) ; θt) ,

(1)

(2)

(3)

where x and yc denote the input image and class name of the
c-th class, respectively. CLIPi (·) and CLIPt (·) are image en-
coder and text encoder, f and tc are derived visual and textual
embeddings. Prompt (yc) denotes the prompt extension of
class name yc. For simplicity, we omit Prompt (·) and use yc
in the following sections. τ is a logit scaling factor and (cid:104)·, ·(cid:105) is
the inner product between two normalized embedding vectors.
The framework of original CLIP is illustrated in Fig. 1.

II. BACKGROUND AND RELATED WORK

B. Continual Learning

A. The CLIP Model

Motivated by the success of pre-trained language models
in NLP, e.g., BERT [1], several vision-language-based pre-
training models have been proposed recently. Early attempts
[3], [4], [15]–[18] usually learn with a strategy that is similar
to the mask language modeling in BERT. More recently, CLIP
[5], and several similar works [19], [20] adopt the contrastive
learning strategy and a dual-tower structure to build the model.
They utilize an image encoder and a text encoder to process the
input images and the input sentence separately. After obtaining
their embeddings, a contrastive-learning-alike loss is utilized
to encourage correct matching across modalities.

Although being trained for image-text matching, the CLIP
model shows an outstanding capability in zero-shot classiﬁ-
cation. To perform zero-shot classiﬁcation, CLIP employs a
prompt to convert the classiﬁcation problem into text match-
ing. For example, the candidate class names are ﬁlled into a
textual prompt like “this is a photo of [Class Name].”, then
the prompt text is encoded to match the visual feature. By
comparing similarities between image and text embeddings,
the probability of assigning an image to a candidate class is

Continual learning (CL) aims at solving the catastrophic
forgetting problem [21], which refers to the performance drop
of preceding tasks when learning on a new task, i.e., forgetting
previously acquired knowledge. Continual learning has been
considerably interested in many areas, such as classiﬁcation
[23], reinforcement learning [24], and online learning [25].
Main solutions can be grouped into four categories: adaptive-
plasticity-based methods, distillation-based methods, replay-
based methods, and architecture-based methods.

Adaptive-plasticity-based methods learn different levels of
plasticity for the model parameters, expecting important pa-
rameters of former tasks could be retained as much as possible.
EWC [22] is a typical study, authors use Fisher information
matrix to measure the importance of parameters, the higher
Fisher information brings higher penalty on plasticity. But
EWC preserves the penalties on all previous tasks, which will
cost heavy when task number is large. Online-EWC [26] is a
further research, where looses the penalty only on the last old
task. MAS [27] and SI [28] estimate the neuron importance
by observing the inﬂuence of small parameter perturbation
respectively on network output and loss value. NPC [29]
assigns the stable neurons lower learning rates rather than
restriction to retain knowledge. IMM [13] uses the framework

Text EncoderImage Encoder••A boy in a field playing with a frisbee. A small bird with orange flank and a long thin black bill.•…(a)Contrastive Trainingwith Image-Text pairs This is a photo ofairplanebird…•A boy in a fieldplaying with a frisbee.•A small bird withorange flank and along thin black bill.•…(b)Text EncoderImage Encoder……ClassificationTaskRetrievalTask……Similarity Scoresof Bayesian network and proposes two methods, IMM-Mean
and IMM-Mode. IMM-Mean simply average the parameters
of new and old models, IMM-Mode combines two networks
with Laplacian approximation [30].

Distillation-based methods usually equip with knowledge
distilling technique [31]. The current model is trained with su-
pervision from both the new task and a teacher model, which is
usually the old model. LwF [11], LwM [32], PODNet [33] and
MCIL [34] representative works in this family. LwF distills the
output logits from the old model. LwM keeps both the Grad-
CAM [35] attention maps and logits. PODNet distills feature
maps with a mixture of different pooling types. PODNet and
MCIL both distill knowledge from a exemplar set. But MCIL
proposes to learn at exemplar-level, the exemplars are adjusted
at each task.

Replay-based methods have to store a small number of
samples from the previous tasks [36]–[38] or learn to generate
synthetic data [39], [40]. SER [36] uses reservoir sampling
strategy to store the experiences. iCARL [37] builds a ﬁxed
K memory budget to store former tasks’ exemplars. Samples
which is best approximate to the class prototypes are inputted
to the budget ﬁrst. When new classes are added, exemplars
at the end of each class will be removed. Approaches us-
ing synthetic data also called pseudo-replay methods. DGR
[39] employs generative adversarial network (GAN) [42].
The GAN is updated by current data and replayed synthetic
data at each task so as to generate replayed sample in the
future task session. CCL-GM [40] is designed with variational
autoencoder (VAE) [43] architecture, the previous model can
be directly used as a generator.

Architecture-based methods modify the network architec-
ture, such as dynamically expanding the model parameters
[44], [45] or masking different parameters for different tasks
[47]. It is notable that due to the costing pre-training of CLIP,
many continual learning methods are limited to ﬁne-tune the
CLIP. Methods with exemplars and regularization methods
need the previous training information are not optimal for
CLIP continual learning.

III. CONTINUAL LEARNING FOR THE CLIP MODEL

A. Challenges

The zero-shot learning capability of CLIP comes from the
massive amount of image-text pairs in its training set. How-
ever, due to the impossibility of covering all visual concepts
in a single training stage, it is not surprising that CLIP fails
to produce a satisfactory classiﬁcation performance on some
categories. A practical solution is to ﬁne-tune the CLIP with
extra training data for the target classes. However, this may
lead to the catastrophic forgetting issue. That is, the zero-shot
capability of CLIP may be damaged after ﬁne-tuning. Thus,
we may need continual learning methods to overcome this
issue.

Compared with the standard-setting in the existing literature
of continual learning, the continual learning problem on CLIP
(denoted as the CL-CLIP problem hereafter) has the two major
differences:

3

• For CL-CLIP problems, we expect to maintain the zero-
shot learning capability of the model while making the
model still beneﬁt from ﬁne-tuning with extra training
data. In contrast, the traditional CL problem tries to main-
tain the classiﬁcation capability of previously learned
image categories.

• The CLIP training is conducted on image-text pairs.
There is no explicit “class” concept, and the visual
patterns covered by CLIP are several orders larger than
existing CL settings.

The above differences make the CL-CLIP problem particularly
challenging. For example, since in zero-shot classiﬁcation, the
classes are not directly trained at the training stage, the logits
difference between a true class and the other classes may be
small. Thus a slight change in the model may lead to a wrong
prediction, and therefore the model might forget the zero-shot
learning capability more easily. The second difference prevents
us from directly using replay methods to solve the CL-CLIP
problem because we cannot directly store exemplar images for
the learned “classes”. Besides, some adaptive-plasticity-based
methods are impracticable, such as EWC [22], SI [28], and
MAS [27]. These methods need previous training procedure
to provide information, like Fisher information in EWC and
importance estimation in SI and MAS, for continual learning.
However, the pre-training of CLIP is costly (400 million data).

B. Evaluation of the CL-CLIP

Since the CL-CLIP is a new problem and there is no existing
evaluation benchmark2. We conduct a systematic study on
the CL-CLIP problem. The goal of a CL-CLIP algorithm is
twofold: (1) When ﬁne-tuning on new training data of to-
be-enhanced classes, called updated task or updated classes,
the classiﬁcation performance is supposed to be improved. (2)
Zero-shot learning performance of the ﬁne-tuned model should
be largely maintained or even improved compared to the
original CLIP. Hence, in our study, we use additional training
data to ﬁrst update the CLIP model. Then we evaluate the
updated model by its performance on updated classes, zero-
shot learning, and image-text matching tasks. In other words,
there are one training set and three test datasets (demonstrated
in Fig. 2). In the following part, we elaborate on our evaluation
protocol.

a) Data for Updated Task: We use MSCOCO [49] to
construct the training data of the updated classes. 80-way
objects are cropped by their bounding-boxes to build the
classiﬁcation task. To be speciﬁc, training images are from
the standard MSCOCO training set, while the test set is from
the images of the matching task described below. There are
596,974 cropped training images and 35,360 testing images
in total. We also preserve a validation set from the 5000
validation images in Karpathy’s split [50] of MSCOCO.

b) Data for Zero-shot Learning: We use tiered-ImageNet
[51] dataset, a subset of ImageNet [52], as the evaluation
set for zero-shot learning performance. 351 classes with 100

2A recent work [48] also addresses the ﬁne-tuning problem of CLIP, but
focuses on improving the out-of-distribution generalization capability of CLIP
in the context of zero-shot learning rather than the continual learning problem.

4

TABLE I
THE SESSION SPLITS FOR MSCOCO MULTI-SESSION TRAINING. THE
PROMPTED NAMES ARE LOCATED IN THE SECOND COLUMN. THE
AMOUNTS OF TRAINING AND TESTING SAMPLES ARE LISTED IN THE
THIRD COLUMN.

Session

1

2

3

4

5

6

7

8

train,

truck, boat,

Class Names
person, bicycle, car, motorcycle, air-
trafﬁc
plane, bus,
light.
ﬁre hydrant, stop sign, parking meter,
bench, bird, cat, dog, horse, sheep,
cow.
elephant, bear, zebra, giraffe, back-
pack, umbrella, handbag, tie, suitcase,
frisbee.
skis, snowboard, sports ball, kite, base-
ball bat, baseball glove, skateboard,
surfboard, tennis racket, bottle.
wine glass, cup, fork, knife, spoon,
bowl, banana, apple, sandwich, orange.
broccoli, carrot, hot dog, pizza, donut,
cake, chair, couch, potted plant, bed.
dining table, toilet, tv, laptop, mouse,
remote, keyboard, cell phone, mi-
crowave, oven.
toaster, sink, refrigerator, book, clock,
vase, scissors, teddy bear, hair drier,
toothbrush.

Number of Samples

258,719 / 14,886

41,024 / 2,678

45,049 / 2,643

49,939 / 3,048

62,360 / 3,801

65,932 / 3,947

36,776 / 2,127

37,175 / 2,230

only the image encoder, or only the text encoder, or both
of them, named as Whole Model (WM), Image Only (IO),
and Text Only (TO). In the following parts, we ﬁrst discuss
the extension of existing continual learning approaches for
CL-CLIP and then describe a simple-but-effective strategy
proposed in this work.

B. Extensions for CL-CLIP

In this section, we consider the extension of four existing
methods for the CL-CLIP problem,
including LwF [11],
GeoDL [12], IMM [13] and RKR [14], where LwF and GeoDL
(we only use the distillation in GeoDL, described below)
represent the distillation-based approach, IMM is an adaptive-
plasticity-based approach, and RKR is an architecture-based
approach.

a) LwF [11]: LwF is a classic distillation-based con-
tinual learning method. LwF adds an additional distillation
loss to encourage the posterior probabilities predicted from
the updated model
to be similar to the previous model.
Speciﬁcally, it employs the following loss term:

L = LCE + β · LLwF ,

LLwF (pnew, pold) = −

Kp
(cid:88)

c=1

new · log (pc
pc

old),

(4)

(5)

where pnew and pold are posterior probabilities estimated from
the updated model and the original model. pc
∗ is the probability
of class c and Kp denotes the number of previous classes. β
is the trade-off weight between the two losses.

CL-CLIP extension: During training, the images and class
prompts are encoded by previous and updated CLIP respec-
tively, estimating pnew and pold via Eq. (1). It is notable
that there is no explicit “class” for original CLIP, we have
to perform distillation on updated classes. For one-session
training, the total 80 classes are trained with classiﬁcation

Fig. 2. Overview of our proposed evaluation protocols. The CLIP model is
ﬁne-tuned with a constructed object classiﬁcation dataset. After ﬁne-tuning,
the updated task, zero-shot classiﬁcation and image-text retrieval task are
evaluated.

samples in each class are utilized for zero-shot evaluation.
Since our target is to examine the CL-CLIP methods, not
pursuing the careful prompt engineering, we use a basic
prompt text: “this is a photo of [Class Name].” The rest 160
classes of tiered-ImageNet are reserved for validation.

c) Data for Image-text Matching: Besides classiﬁcation,
MSCOCO [49] is also used as an image-text retrieval dataset.
Three are 5,000 testing images and 25,000 captions (Karpa-
thy’s test split), which are usually tested by averaging over
ﬁve folds of 1,000 images. Note that the updated classes are
included in the retrieval texts. We deliberately maintain this
overlap and expect a perfect CL-CLIP algorithm can lead
to improved performance on image-text matching since the
modeling of the overlapped visual concepts is improved via
ﬁne-tuning.

d) One-Session and Multi-Session Training: Apart from
the dataset, two ﬁne-tuning settings are designed for evaluating
CL-CLIP methods, One-Session Training (OST) and Multi-
Session Training (MST). One-session training only updates
the model with one more task, where all training data is acces-
sible at the same time. Multi-session training provides training
classes sequentially, which is similar to the typical setting of
continual learning. Speciﬁcally, 80 classes are divided into 8
sessions (the splits are listed in Tab. I). Data from each session
are used to sequentially update the model. This is regarded as
a complex situation of CL-CLIP.

IV. CONTINUAL LEARNING METHOD EXTENSIONS

A. Building Options

Unlike standard image classiﬁcation networks, the CLIP
model comprises an image encoder and a text encoder. The
dual encoder structure gives additional ﬂexibility in designing
continual learning algorithms. One could choose to update

Continual Fine-tuningUpdated ClassificationEvaluations on Three TasksUpdated ClassificationZero-shot ClassificationImage-Text RetrievalHair DrierOrangeStop SignClock…Indigo BirdBowtieFluteToyshop•A selection of woodenkitchen tools on a counter.•A man with a red helmet ona small moped on a dirt road.•…………Hair DrierOrangeStop SignClock5

Fig. 3. The framework of the proposed VR-LwF method. The pseudo vocabulary sequences are fed into both previous and current text encoders. The logits
of the replayed classes are enforced. The model is trained with the combination of cross-entropy classiﬁcation loss and replaying distillation loss. The grey
part in the ﬁgure indicates the previous model and its outputs.

and distillation simultaneously. For multi-session training, the
classiﬁcation number is 10 because training data is limited on
a session. But the distilled classes are both current classes and
previous classes.

b) GeoDL [12]: GeoDL is recently proposed knowl-
edge distillation-based CL method. It ﬁrst constructs low-
dimensional manifolds for the features extracted from the
previous and current models. Then GeoDL minimizes the
representation dissimilarity by the geodesic similarity rather
than the Euclidean similarity. We denote feature z ∈ {t, f } as
CLIP visual or textual feature, zold and znew are from the old
and new model, respectively. The feature matrices (Zold and
Znew), which is usually stacked by visual or textual features in
a batch, are decomposed to obtain subspaces Pold and Pnew.
After the estimation of geodesic ﬂow Π from the subspaces,
the inner product of zold and znew are computed along Π:

Π (ν) = (cid:2) Pold R (cid:3)

(cid:20) U1Γ (ν)
−U2Σ (ν)

(cid:21)

,

Q =

(cid:90) 1

0

Π (ν) Π(ν)(cid:62)dν,

gΠ (zold, znew) = znew

(cid:62)Qzold,

(6)

(7)

(8)

where R is orthogonal complement of Pold, Γ (ν) and Σ (ν)
are two diagonal matrices. gΠ (·, ·) is the inner product, Q
is the matrix which deﬁnes the manifold structure between
features of two phases. Details and proofs can be found in its
original paper [12].

CL-CLIP extension: The original GeoDL method is con-
ducted with replaying samples, which is not applicable for
CL-CLIP. Thus, we only treat it as a distillation method and
perform it for both the text and image embeddings. During
training, the images and prompts are inputted to old model

and new model, extracting feature matrices, Fold, Fnew and
Told, Tnew, for vision and text respectively. Notice that for
MST, the text feature matrix only contains current and previ-
ous classes, which is the same as LwF. Then the subspaces
and manifolds Qi, Qt are estimated, where Qi is manifold
between old and new image features and Qt is for text. The
GeoDL distillation aims to minimize the inner product of
image and text features between old and new model:

Li

GeoDL = −

Lt

GeoDL = −

fnew
(cid:13)
(cid:13)Qi1/2fnew
(cid:13)
tc
new
(cid:13)
(cid:13)Qt1/2tc
(cid:13)

(cid:62)Qifold
(cid:13)
(cid:13)
(cid:13)Qi1/2fold
(cid:13)
(cid:13)
(cid:13)
(cid:62)Qttc
(cid:13)
(cid:13)
(cid:13)Qt1/2tc
(cid:13)
(cid:13)
(cid:13)

old

new

old

,

(cid:13)
(cid:13)
(cid:13)

,

(cid:13)
(cid:13)
(cid:13)

(9)

(10)

GeoDL.

where f is visual embedding and tc is textual embedding of
class c. The distillation losses are combined with classiﬁcation
loss by L = LCE + β · Li

GeoDL + β · Lt
c) IMM [13]: IMM is a adaptive-plasticity-based con-
tinual learning method. There are two variants in IMM (i.e.,
IMM-Mean and IMM-Mode). IMM-Mode combines two net-
works with the Laplacian approximation [30], but requires the
training information of previous phases, which is expensive
for the CL-CLIP model. IMM-Mean simply averages the
parameters of current and preceding models, which suits CL-
CLIP well. The IMM-mean approach is also employed in a
recent work called WiSE-FT [48] to beneﬁt out-of-distribution
generalization of CLIP. In WiSE-FT [48], the IMM-mean is
slightly modiﬁed to leverage the weighted-combination rather
than a direct average. Please note that WiSE-FT [48] and
our method are targeting different problems, i.e., they try to
improve CLIP’s domain generalization capability while we are
targeting the CL-CLIP problem.

Replayed Vocabularies•[households] [father] [ppl][somewhere] [ans] [rider][themwomen] [completely][enhanced] .•…Current Text EncoderCurrent Image EncoderCLIP Vocabulary SetPrevious Image EncoderImagesClassesThis is a photo of[Class Name].PromptSamplingPrevious Text EncoderReplayed VocabulariesUpdated ClassesReplayed VocabulariesUpdated ImagesUpdated ImagesClassification Cross-entropy LossVR-LwF Distillation LossVR Similarity ScoresPreviousVR Similarity ScoresCL-CLIP extension: IMM can be applied to the text-encoder
and/or image encoder. In all cases, we generate the ﬁnal model
by taking the weighted average of ﬁne-tuned CLIP and the
original CLIP:

f = CLIPi (x; (1 − α) · θi,old + α · θi,new) ,
tc = CLIPt (yc; (1 − α) · θt,old + α · θt,new) ,

(11)

(12)

where θ∗,old and θ∗,new are model parameters before and
after training. Depending on the updating options, we could
have three variants of CL-CLIP IMM. During training, the
old model is original CLIP for OST but the previous session
model for MST.

d) RKR [14]: RKR keeps the previously trained parame-
ters ﬁxed and adds very few extra learnable parameters. There
are two modules in RKR, including the rectiﬁcation generator
(RG) and the scaling factor generator (SFG). RKR transforms
the ﬁxed model as follows:

θw = θw + rw,
ow = ow · sw,

(13)

(14)

where θw is the frozen parameter for layer w, rw is generated
weight rectiﬁcation, ow is the output of layer and it is scaled
by learned sw.

CL-CLIP extension: Both the text encoder and image en-
coder of the CLIP are transformer [53] architectures. We only
add RG to linear layers in the transformer, but according to
our experiments, the RG causes the severe forgetting of zero-
shot ability. So we only apply SFG to each block’s output and
sw are initialized by all one vectors. During both OST and
MST training, the only ﬁne-tuned part is the added parameters.
Notice that, in MST, the RKR parameters are installed at the
ﬁrst session and continually trained in all tasks.

V. METHOD

One of the most successful families of continual learning
algorithms is the replaying-based approach. Speciﬁcally, it
stores a few samples from each previously learned class in
a replaying memory. The replaying method tries to maintain
their corresponding outputs from the old model when training
on a new task. Unfortunately, for the CL-CLIP problem, the
replaying method is not suitable for two reasons: (1) In the
CLIP, there is no explicit “class” concept. (2) CLIP covers a
huge number of visual concepts, which is memory-consuming
to store the exemplars.

that

However, we recognize that

the replying scheme could
be applied to the text encoder, which creates a solution
for CL-CLIP. The idea is motivated by the fact
the
classiﬁer of classes is derived from the text encoder by
prompted sentences, the text encoder is a mapping function
from class names to classiﬁer weights (see Eq. (3)). The
input of this mapping function is essentially the combination
of tokens from a vocabulary set, which is available without
the need for images. Thus, we can generate sentences from
the vocabularies and perform distillation – this allows us to
simulate the zero-shot and previous classes during training,
the
alleviating the catastrophic forgetting. To be speciﬁc,
replayed vocabularies can be regarded as pseudo-classes, we

Algorithm 1 Replayed Vocabularies Building.
Input: CLIP vocabulary set VCLIP , sampling length M , and

6

replaying class number Ks.

Output: A set Ys of Ks elements, which contains replayed

classes ys.

1: Initialize Ys with an empty set.
2: for s = 1 to Ks do
3:
4:
5:

Set ys as an empty sequence.
for i = 1 to M do

w ← Random select a vocabulary in VCLIP ;
ys ← Add the word w into sequence;

6:
7:
8: Ys ← Add the replayed vocabularies ys into set.
9: end for

end for

TABLE II
RETRIEVAL RECALLS (%) OF ORIGINAL CLIP MODEL WHEN RANDOM
SHUFFLE THE INPUT TEXT.

Method
VSE++ [54]
Non-shufﬂe
Shufﬂe

TR@1
58.30
69.34
54.06

TR@5
-
91.08
81.38

TR@10
93.30
95.70
90.26

IR@1
43.60
49.67
35.64

IR@5
-
79.17
66.00

IR@10
87.80
88.77
78.72

implement distillation on their logits. This is similar to the
LwF method, but LwF is executed on true classes while our
method utilizes replayed classes. Hence, the method is entitled
Learning without Forgetting via Replayed Vocabularies (VR-
LwF).

A problem for this idea is how can we generate sentences
from the existing vocabularies, e.g., should we follow certain
grammar structure? In our work, we take a crude approxi-
mation by completely ignoring the grammar. This scheme is
motivated by the following observation: we observe that by
randomly shufﬂing the word order of a sentence, the text-to-
image retrieval capability of CLIP can be largely preserved.
As seen from Tab. II, by shufﬂing the word order, the TR@n
of the retrieval performance can still be quite reasonable and
can even be competitive to an existing retrieval approach
VSE++ [54]. This inspires us to generate (pseudo-)sentences
by directly stacking randomly chosen tokens from the vocabu-
lary. Formally, we arbitrarily sample M words from the CLIP
vocabulary to constitute a text input ys, as shown in Alg. 1.
The pseudo-sentence is fed into both the current text encoder
and the original text encoder to obtain the embeddings ts
new
and ts
old. Meanwhile, the image embeddings of current model
and previous model are fnew and fold. Then a distillation loss
is applied:

ps
old =

(cid:80)Ks

(cid:16)

exp (τold · (cid:104)fold, ts

old(cid:105))
(cid:68)
fold, tj
τold ·
exp (τnew · (cid:104)fnew, ts

j=1 exp

(cid:69)(cid:17) ,

(15)

(16)

(cid:69)(cid:17) ,

old
new(cid:105))
fnew, tj

new

(cid:68)

ps
new =

(cid:80)Ks

j=1 exp

(cid:16)

τnew ·
Ks(cid:88)

LV R

LwF (ps

new, ps

old) = −

new · log (ps
ps

old),

(17)

where ps

new and ps

old are probability distribution among the

s=1

pseudo classes. ps
∗ is the probability of class s and Ks is the
number of replayed classes. Combining with the cross-entropy
loss for training the updated classes, the ﬁnal loss function is
given by L = LCE + β · LV R
LwF . The illustration is shown in
Fig. 3. The replayed vocabularies act as unseen and previous
classes, but for MST, it is a more effective way to directly
append former classes to the replayed vocabularies. The loss
function would be L = LCE + β · (cid:0)LV R
(cid:1), where
LP re
LwF is a LwF loss on old session classes. Note that this is
different with LwF in Eq. (5), the LwF extension distils on all
seen classes (including previous classes and current classes),
while ours only contains previous classes. It is not a good
choice for LwF extension to use only previous classes, since
it will degenerate to direct ﬁne-tuning at the ﬁrst session, but
VR-LwF can still use other replayed classes.

LwF + LP re
LwF

VI. EXPERIMENTS

This section presents our experimental studies on the CL-
CLIP problem. We started by introducing the implementation
details and evaluation metrics. Then, we conduct a series of
experiments to answer the following questions: (1) Whether
the CLIP model suffers from the catastrophic forgetting issue?
(2) Can current continual learning methods, at least the four
selected representative methods, be extended to alleviate the
forgetting issue? (3) What experiences are explored under the
two training settings? (4) Can the proposed VR-LwF method
lead to improved performance? If yes, what is the impact of
the hyper-parameters in the proposed method?

A. Implementation Details and Evaluation Metrics

a) Training Details: As mentioned above, we evaluate
CL-CLIP under two settings: one-session training (OST) and
multi-session training (MST). We ﬁne-tune 15 epochs for OST,
the initial learning rate is 1e-6 and it will decay by 0.1 after
10 epochs. The number of epochs for each MST session is 10,
where are 80 epochs in total. Though this is much more than
OST, the training step is only equivalent to 10-epoch OST.
The learning rate of MST is 1e-6 for each session, without
decline. All compared methods are optimized with Adam [55]
optimizer, and we set the batch size as 100. For our VR-
LwF, the length of pseudo-sentence is 10, and the number of
pseudo-sentences is 100 per batch.

b) Evaluation Metrics: We use the following metric to
evaluate the performance of the updated model with different
CL-CLIP methods:

• UT-Acc: the model accuracy of the updated task test
set. We expect a high UT-Acc when the model has
successfully ﬁne-tuned from the update-task training data.
Please note that in OST, the accuracy is overall accuracy
(calculated by the percentage of the correct prediction
versus the total number of samples). For the MST setting,
UT-Acc is calculated by averaging over all sessions. Due
to the number of samples being different for each session
in MST, the UT-Acc of MST is not directly comparable
with that of OST even for the same set of predictions
(25.36% and 39.80% in Tab. III and Tab. IV).

7

• ZS-Acc: the zero-shot learning test accuracy. Generally
speaking, there is a trade-off between UT-Acc and ZS-
Acc, a model that achieves high UT-Acc may come with
sacriﬁcing its zero-shot learning capability and result in
a low ZS-Acc.

• R@k: Recall at 1 and 5 (TR@1, TR@5 for text retrieval
and IR@1, IR@5 for image retrieval) are used to evaluate
the image-text and text-image matching capability.

(cid:80)S−1

• Bwt: backward transfer is only conducted for MST. It
is the average drop of accuracy on previous sessions
after ﬁne-tuning with the current task, which is deﬁned
in [56]: Bwt = 1
i=1 AccS,i − Acci,i, where S is
S−1
the current session number, AccS,i is the accuracy of
session i classes after current ﬁne-tuning and Acci,i is
the accuracy of task i after the i-th session training. As
the forgetting often appears in continual learning, Bwt is
usually small than 0, and the smaller the Bwt, the worse
the forgetting.

• A-Acc: the average of UT-Acc and ZS-Acc. Since there
is a trade-off between UT-Acc and ZS-Acc, it will be
less convenient to judge the performance of an algorithm
by examining two accuracies. We thus use their average
accuracy as a single indicator for the goodness of CL-
CLIP algorithms.
c) Hyper-parameter Choices: Hyper-parameters of each
method are searched from the validation set. Speciﬁcally, we
choose the hyper-parameter that brings the highest A-Acc on
the validation set. The hyper-parameters to be optimized are
the combination weight α in IMM, and the trade-off weights
β for the distillation terms in LwF, GeoDL, VR-LwF.

d) Compared Methods: As discussed in Sec. IV, we can
extend an existing CL method with three options, that is,
updating the whole model (WM), only the image encoder
(IO), or only the text encoder (TO). We apply those three
options to both the ﬁne-tuning baseline (FT) and the four
CL methods, i.e., LwF, GeoDL, IMM, and RKR. We use
the notation “method-WM/IO/TO” to denote these variants,
including our VR-LwF.

B. Experimental Results

In this section, we compare different CL-CLIP approaches.
Tab. III presents an overview of one-session training results.
Tab. IV presents the results of MST, which reports after the
training of the last session. We also introduce the accuracy
curves in Fig. 4, note that the UT-Acc in the ﬁgure is overall
accuracy rather than average accuracy.3 We summarize our
ﬁndings as follows:

a) CLIP suffers from catastrophic forgetting: We ﬁrstly
i.e., directly ﬁne-tuning the model without
focus on FT,
applying any continue learning algorithms. From the results
in Tab. III. We can ﬁnd that ﬁne-tuning leads to signiﬁcantly
improved performance on the updated task as expected. How-
ever, it causes damage to the zero-shot learning and image-
text matching capabilities. We observe a sharp performance

3This is same as continual learning literature, the average accuracy and
Bwt are usually reported in the table, like Tab. IV, and overall accuracy is
for curves.

TABLE III
RESULTS (%) OF ONE-SESSION TRAINING. THE ORIGINAL CLIP MODEL DOES NOT PARTICIPATE IN THE COMPARISON. THE BEST SCORES ARE MARKED
IN BOLD. NOTE THAT THE BEST RESULTS IN EACH GROUP ARE UNDERLINED.

8

Method
Original CLIP
FT-WM
LwF-WM
GeoDL-WM
IMM-WM
RKR-WM
VR-LwF-WM
FT-IO
LwF-IO
GeoDL-IO
IMM-IO
RKR-IO
VR-LwF-IO
FT-TO
LwF-TO
GeoDL-TO
IMM-TO
RKR-TO
VR-LwF-TO

UT-Acc
25.36
74.21
74.41
74.30
71.25
56.60
72.82
74.11
73.91
74.23
70.25
50.11
74.69
64.86
64.35
64.52
63.50
54.18
64.85

ZS-Acc
62.62
44.17
58.75
60.50
59.94
63.82
62.03
38.84
57.19
57.03
57.21
62.26
58.58
56.44
59.52
60.50
61.31
64.74
61.79

TR@1
69.34
32.54
62.34
65.18
60.66
69.36
68.34
34.68
62.70
62.48
61.74
69.96
64.76
61.26
66.16
66.00
68.50
69.48
68.52

TR@5
91.08
59.50
86.34
87.54
85.08
90.76
89.96
61.30
86.72
86.74
86.10
90.66
88.42
84.64
88.54
88.52
89.50
90.66
90.48

IR@1
49.67
25.49
45.86
46.37
43.03
51.27
47.82
23.77
48.27
47.15
43.29
51.04
46.97
41.16
45.04
45.30
47.43
50.11
49.41

IR@5
79.17
53.55
76.1
76.14
73.60
80.60
77.41
50.83
78.30
76.85
73.02
80.37
76.77
70.65
74.85
75.79
77.04
79.58
78.90

A-Acc
43.99
59.19
66.58
67.40
65.60
60.21
67.43
56.48
65.55
65.63
63.73
56.19
66.63
60.65
61.93
62.51
62.41
59.46
63.32

drop of ZS-Acc, TR@k, and IR@k for either WM, IO, or
TO variants. This result suggests that the CLIP model suffers
from catastrophic forgetting. Likewise, this can be seen in
Tab. IV, the MST setting. The small Bwt also demonstrates
that the previously learned classes are forgotten after new
session training.

b) Different updating options perform dissimilarly: As
observed from Tab. III and Tab. IV, the choice of the to-be-
updated module would signiﬁcantly impact the performance.
Generally, the TO variant is slightly safer than the other
two, but the accuracy of updated task is limited. Taking FT-
TO as an example, its ZS-Acc only drops from 62.62% to
56.44%/55.45% (OST/MST), which is the best among FT
variants. But the UT-Acc is 64.86% (OST, since the UT-
Acc of MST is also inﬂuenced by forgetting, its UT-Acc
is higher than WM and IO),
lower than FT-IO and FT-
WM (74.21%/74.11%). On the contrary, IO variant suffers
from the worst forgetting among all variants, the ZS-Acc
of almost all methods is lower than WM and TO. A possible
explanation for this might be that the gradient of TO ﬁne-
tuning loss solely propagates to the discrete tokens (the prompt
and class names) in the vocabulary set, so it does not impact
other vocabularies. However, the performance still declines
because model parameter change brings the mismatch with
these vocabularies. But texts are more discrete than images,
which means there would be more space for the shift of text
embeddings, hence the performance loss of TO variant is
lower than IO. What is interesting about the retrieval metrics
is that the TR@k of IO and IR@k of TO usually decline
more. Finally for the WM variant, only ﬁne-tuning is not the
best choice (A-Acc of FT-WM is worse than FT-TO’s). But
the WM variant performs best by cooperating with CL
methods. This means that when ﬁne-tuning text encoder and

image encoder simultaneously, the change of two parts can be
slighter than forcing only one part to update, with the help of
CL methods. This also indicates that it is able to get better
performance by ﬁne-tuning more parameters.

c) There are CL methods to alleviate the forgetting issue,
but only for OST: Closer inspection of the Tab. III shows
GeoDL is the best CL method, whose performance is better
than others except for our VR-LwF. GeoDL-WM even gets
the superior A-Acc, which is close to ours (67.40% vs.
67.43%). But under MST setting, it is incapable for forgetting
challenge, the gap with VR-LwF is blown-up and it is worse
than IMM. This can be inferred from the ZS-Acc drop in
OST. ZS-Acc of GeoDL-WM decreases around 2% (60.50%
vs. 62.62%), comparing with Original CLIP. This forgetting
will be further distinguished when training tasks sequentially.
Previous classes will become zero-shot classes in current
session, which means they prone to be forgotten, resulting
in the inferior UT-Acc. It can be observed that the ZS-Acc
of GeoDL-WM is near between OST and MST, so the gap is
mainly caused by UT-Acc. It also happens on the second-best
CL method (in OST) LwF. This illustrates that existing CL
methods may alleviate the forgetting in one session training
but they are weak for multi-session CL-CLIP.

d) CL-CLIP is different from traditional continual learn-
ing: In Fig. 4 we plot the performance curves of compared
methods. For updated task, We introduce Original CLIP and
Joint-FT as the lower and upper-performance bounds. The
Joint-FT is built, as same as CL literature, by ﬁne-tuning with
all seen data. Its average accuracy of session 8 is the same as it
in Tab. IV. In the bottom charts, the grey line presents Original
CLIP ZS-Acc (62.62%). There are some interesting ﬁndings.
First of all, some UT-Acc curves even increase, such as LwF,
IMM, and RKR, which is strange in continual learning. The

TABLE IV
RESULTS (%) OF MULTI-SESSION TRAINING. THE NOTATION IS THE SAME AS THE ONE-SESSION TRAINING. JOINT-FT MODEL REVEALS THE UPPER
BOUND OF UT-ACC, BECAUSE OF ITS SERIOUS DAMAGE ON ZERO-SHOT PERFORMANCE, OTHER METRICS ARE NOT REPORTED.

9

Method
Original CLIP
Joint-FT
FT-WM
LwF-WM
GeoDL-WM
IMM-WM
RKR-WM
VR-LwF-WM
FT-IO
LwF-IO
GeoDL-IO
IMM-IO
RKR-IO
VR-LwF-WM
FT-TO
LwF-TO
GeoDL-TO
IMM-TO
RKR-TO
VR-LwF-WM

UT-Acc
39.80
67.33
51.51
52.65
53.27
54.75
45.37
58.03
49.05
48.75
54.90
51.33
43.68
56.87
52.27
51.27
52.64
51.03
44.24
53.80

Bwt
-
-
-18.45
-1.60
-11.14
-2.69
0.68
-1.36
-22.23
1.50
-4.47
-2.94
0.25
-2.76
-3.51
-1.12
-6.09
-0.30
1.64
-0.24

ZS-Acc
62.62
-
50.01
58.43
60.09
61.91
64.00
63.91
47.53
59.58
59.48
60.23
61.99
59.09
55.45
59.77
55.84
61.61
64.89
62.57

TR@1
69.34
-
46.40
63.72
63.60
65.74
69.78
68.12
49.26
65.12
65.46
65.90
69.82
64.32
62.26
68.68
63.26
67.74
69.24
68.94

TR@5
91.08
-
73.82
87.44
87.14
88.90
90.96
90.38
75.86
88.42
88.88
88.60
90.78
88.18
87.20
90.10
87.28
90.28
90.82
90.52

IR@1
49.67
-
35.94
44.22
42.81
48.34
51.37
48.61
36.17
48.89
48.80
48.03
51.26
48.18
41.54
42.62
37.54
48.31
49.88
48.87

IR@5
79.17
-
66.56
73.81
73.66
77.80
80.80
77.96
66.56
78.39
78.19
77.80
80.60
78.02
71.86
73.27
68.67
77.98
79.47
78.61

A-Acc
51.21
-
50.76
55.54
56.68
58.33
54.68
60.97
48.29
54.16
57.19
55.78
52.83
57.98
53.86
55.52
54.24
56.32
54.57
58.18

main reason lies in the paradigm of CLIP, that the classiﬁer
weights are not independent. As introduced above, the text
encoder (classiﬁer) of CLIP is a mapping function, so the
change of this mapping would bring changes to all classes, this
is different from the fully-connected classiﬁer whose weights
are independent. The rising curve means the subsequent
training improves the former classiﬁcation. However, this
property also leads to a problem that, to keep the zero-shot
capability, CL methods limit the performance of updated
classes. The CL updating is effective partly, as we can see that
all methods beat the Original CLIP for UT-Acc. But compared
with FT, their gain of ZS-Acc is much higher than UT-Acc.
And some CL methods are limiting on updated task. This
is apparent on IMM, it is the third-best method on ZS-Acc,
while its UT-Acc is low-rank among CL methods. This ﬁnding
is consistent with the above discussion that the loss of CL
methods on MST is mainly caused by the drop of UT-Acc.
The proposed VR-LwF is superior to CL extensions. The ZS-
Acc is equal to the Original CLIP and UT-Acc is the best.
Unfortunately, VR-LwF is still around 15% lower than the
Joint-FT. A further study with more focus on increasing the
performance of updated classes is therefore suggested.

e) RKR keeps the original performance best, but learns
least: Surprisingly, the RKR method was found to get the best
zero-shot performance. The ZS-Acc and retrieval recalls are
all the best in each variant group and each training setting
(underlined in Tab. III and Tab. IV means the best in each
group). Some of them are even higher than the original CLIP
model, such as ZS-Acc and IR. These results are related to
RKR which keeps the CLIP model frozen and trains addi-
tional parameters. Furthermore, it should be noted that RKR’s

TABLE V
RESULTS (%) OF ONE-SESSION VR-LWF-WM WITH DIFFERENT
REPLAYING METHODS. WE EXAMINE THE INFLUENCE OF DIFFERENT
RANDOM-SAMPLING LENGTHS AND THE CAPTION REPLAYING.

Method
Original CLIP
M =10
M =20
M =30
M =50
M =70
Caption Corpus

UT-Acc
25.36
72.82
74.11
74.41
74.45
74.26
71.57

ZS-Acc
62.62
62.03
61.06
60.87
60.75
60.69
62.52

TR@1
69.34
68.34
66.56
65.80
64.90
66.22
69.04

IR@1
49.67
47.82
47.14
46.47
46.09
46.83
47.36

A-Acc
43.99
67.43
67.59
67.64
67.60
67.47
67.05

TABLE VI
RESULTS (%) OF ONE-SESSION VR-LWF-WM WITH DIFFERENT
REPLAYING NUMBERS.

Method
Original CLIP
Ks=50
Ks=100
Ks=200
Ks=300
Ks=400

UT-Acc
25.36
73.19
72.82
73.09
72.94
72.99

ZS-Acc
62.62
61.67
62.03
61.86
61.89
61.79

TR@1
69.34
67.64
68.34
67.76
67.88
67.92

IR@1
49.67
47.28
47.82
47.98
47.86
48.13

A-Acc
43.99
67.43
67.43
67.47
67.42
67.39

updated task performance is the worst among all methods,
usually 10% lower than others. The major factor is also the
adding parameters, as mentioned before, the RKR extension
only contains the scaling factor part. This is lacking compared
with other ﬁne-tuning methods, but adding the rectiﬁcation
part makes the CLIP forget all zero-shot abilities.

f) The proposed VR-LwF achieves the overall best per-
formance: Tab. III and Tab. IV are quite revealing in the
superior performance of our proposed VR-LwF method. The

10

Fig. 4. Accuracy (%) of methods in each MST session. The ﬁrst row of charts is updated accuracy, the second row is for ZS-Acc. From left to right, each
column is for WM, IO, and TO variants in sequence. The Original CLIP and Joint-FT are lower and upper bounds of UT-Acc, respectively.

TABLE VII
RESULTS (%) OF MULTI-SESSION VR-LWF-WM WITH DIFFERENT
REPLAYING METHODS. WE EXAMINE THE INFLUENCE OF DIFFERENT
RANDOM-SAMPLING LENGTHS AND THE CAPTION REPLAYING.

Method
Original CLIP
M =10
M =20
M =30
M =50
M =70
Caption Corpus

UT-Acc
39.80
58.03
58.58
58.60
58.53
58.48
57.30

Bwt
-
-1.36
-1.13
-1.73
-2.02
-1.45
-0.75

ZS-Acc
62.62
63.91
63.61
63.66
63.76
63.88
61.95

TR@1
69.34
68.12
68.00
68.32
68.42
69.06
68.80

IR@1 A-Acc
51.21
49.67
60.97
48.61
48.62
61.10
61.13
48.31
61.14
47.90
61.18
48.45
59.62
47.52

TABLE VIII
RESULTS (%) OF MULTI-SESSION VR-LWF-WM WITH DIFFERENT
REPLAYING NUMBERS.

Method
Original CLIP
Ks=50
Ks=100
Ks=200
Ks=300
Ks=400

UT-Acc
39.80
58.05
58.03
58.12
58.43
58.33

Bwt
-
-1.17
-1.36
-0.65
-0.86
-0.79

ZS-Acc
62.62
63.98
63.91
63.92
63.84
63.86

TR@1
69.34
68.46
68.12
68.26
68.76
68.38

IR@1 A-Acc
51.21
49.67
61.02
48.70
60.97
48.61
61.02
48.98
61.14
48.99
49.02
61.09

best A-Acc of OST and MST both belong to VR-LwF-WM
(67.43% and 60.97%). Also, for each VR-LwF variant, they all
get the best A-Acc in their group. But the performance trend
is consistent with former discussion, where TO is safer, IO is
substandard, and WM is capable. For zero-shot abilities, their
ZS-Acc and recalls are usually the second best, which is only
below the RKR discussed before. The only difference between
VR-LwF and LwF extension is to distill on which classes.
For example in OST, LwF distills on updated classes and VR-
LwF distills on replayed classes, comparing their outcomes,
replaying is more advantageous. Though LwF is higher on
UT-Acc (74.41% vs. 72.82%), VR-LwF’s ZS-Acc is better
(62.03% vs. 58.75%), it can be seen that the recalls of LwF are
worse too. Moreover, as mentioned above, the gap is distinct
in MST.

C. Ablations

In the VR-LwF methods, the pseudo-sentence is generated
from a set of randomly sampled words. A crucial question is
whether the random replaying is enough? To answer this, we
build a caption replaying method, which is randomly sampling
image captions from MSCOCO dataset. This variant ensures
that the sampled sentence is a meaningful natural sentence.
Once the sentences are sampled, the rest part is identical
to the VR-LwF. We report the experimental comparisons in
Tab. V and Tab. VII, for OST and MST respectively. Caption-
replaying for OST is slightly better on ZS-Acc and retrieval
recalls but not very signiﬁcant. This somehow suggests that
grammar does not play a predominant role. As for MST, the
caption-replaying is even worse than random sampling. We
also examine the effect of sampling length M , in Tab. V and
Tab. VII, it is seen that M is not inﬂuential for the results
either. The longer length seems better for MST, but the zero-
shot performance of OST drops a little. The drop may be
explained by the long random-sampling causes more noise.

123456781525354555657585UT-AccWM123456781525354555657585IO123456781525354555657585TO12345678MST Session4550556065ZS-Acc12345678MST Session455055606512345678MST Session4550556065Original CLIPJoint-FTFTLwFGeoDLIMMRKRVR-LwF11

[4] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder
representations from transformers,” arXiv preprint arXiv:1908.07490,
2019.

[5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,
“Learning transferable visual models from natural language supervi-
sion,” in Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol. 139, 2021,
pp. 8748–8763.

[6] X. Gu, T. Lin, W. Kuo, and Y. Cui, “Zero-shot detection via vision and
language knowledge distillation,” CoRR, vol. abs/2104.13921, 2021.
[7] S. Esmaeilpour, B. Liu, E. Robertson, and L. Shu, “Zero-shot open set
detection by extending CLIP,” CoRR, vol. abs/2109.02748, 2021.
[8] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen, “Improving video-
text retrieval by multi-stream corpus alignment and dual softmax loss,”
CoRR, vol. abs/2109.04290, 2021.

[9] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, “Clip4clip:
An empirical study of CLIP for end to end video clip retrieval,” CoRR,
vol. abs/2104.08860, 2021.

[10] H. Fang, P. Xiong, L. Xu, and Y. Chen, “Clip2video: Mastering video-
text retrieval via image CLIP,” CoRR, vol. abs/2106.11097, 2021.
[11] Z. Li and D. Hoiem, “Learning without forgetting,” IEEE transactions
on pattern analysis and machine intelligence, vol. 40, no. 12, pp. 2935–
2947, 2017.

[12] C. Simon, P. Koniusz, and M. Harandi, “On learning the geodesic path
for incremental learning,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021, pp. 1591–1600.

[13] S. Lee, J. Kim, J. Jun, J. Ha, and B. Zhang, “Overcoming catastrophic
forgetting by incremental moment matching,” in Advances in Neural
Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA, 2017, pp. 4652–4662.

[14] P. Singh, P. Mazumder, P. Rai, and V. P. Namboodiri, “Rectiﬁcation-
based knowledge retention for continual learning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 15 282–15 291.

[15] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, “Videobert:
A joint model for video and language representation learning,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2019, pp. 7464–7473.

[16] Y. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, and
J. Liu, “UNITER: learning universal image-text representations,” CoRR,
vol. abs/1909.11740, 2019.

[17] L. H. Li, M. Yatskar, D. Yin, C. Hsieh, and K. Chang, “Visualbert: A
simple and performant baseline for vision and language,” CoRR, vol.
abs/1908.03557, 2019.

[18] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, “12-in-1: Multi-
task vision and language representation learning,” in 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020, 2020, pp. 10 434–10 443.

[19] C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. V. Le,
Y. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
representation learning with noisy text supervision,” in Proceedings of
the 38th International Conference on Machine Learning, ICML 2021,
18-24 July 2021, Virtual Event, vol. 139, 2021, pp. 4904–4916.
[20] Y. Huo, M. Zhang, G. Liu, H. Lu, Y. Gao, G. Yang, J. Wen, H. Zhang,
B. Xu, W. Zheng, Z. Xi, Y. Yang, A. Hu, J. Zhao, R. Li, Y. Zhao,
L. Zhang, Y. Song, X. Hong, W. Cui, D. Y. Hou, Y. Li, J. Li, P. Liu,
Z. Gong, C. Jin, Y. Sun, S. Chen, Z. Lu, Z. Dou, Q. Jin, Y. Lan, W. X.
Zhao, R. Song, and J. Wen, “Wenlan: Bridging vision and language by
large-scale multi-modal pre-training,” CoRR, vol. abs/2103.06561, 2021.
[21] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec-
tionist networks: The sequential learning problem,” in Psychology of
learning and motivation. Elsevier, 1989, vol. 24, pp. 109–165.
[22] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al., “Overcoming catastrophic forgetting in neural networks,” Pro-
ceedings of the national academy of sciences, vol. 114, no. 13, pp.
3521–3526, 2017.

[23] D.-W. Zhou, Y. Yang, and D.-C. Zhan, “Learning to classify with
incremental new class,” IEEE Transactions on Neural Networks and
Learning Systems, 2021.

[24] Z. Wang, H.-X. Li, and C. Chen, “Incremental reinforcement learning
in continuous spaces via policy relaxation and importance weighting,”
IEEE transactions on neural networks and learning systems, vol. 31,
no. 6, pp. 1870–1883, 2019.

Fig. 5. Comparison of Flickr [57] and COCO [49] retrieval performance,
horizontal axis presents TR@1(%) and vertical axis presents IR@1(%). The
top/bottom chart is for OST/MST results. The ﬁgure shows that distributions
of Flickr and COCO results are the same, which means the evaluation is
transferable.

As for replaying number Ks, Tab. VI and Tab. VIII illustrate
that the difference is not distinct. However, the large number
leads to a heavy cost of training time.

To evaluate the transferability of the experimental results,
we also report the retrieval performance of Flickr30k dataset
[57]. In Fig. 5, the Flickr results present the same distribution
as COCO retrieval, which suggests the generalization of our
method and conducted protocol.

VII. CONCLUSION

This paper presents the ﬁrst study on the continual learning
problem for large pre-trained CLIP models. We propose an
evaluation benchmark with detailed protocols for our study
and future research. We explore the extensions of existing
continual learning algorithms to address the forgetting issues
of CLIP ﬁne-tuning. Our study shows that
the CL-CLIP
is different from traditional continual learning problem and
the model update scheme, e.g., ﬁxing some parts of model
parameters, impacts the performance largely. The extension
of existing CL methods alleviates the forgetting issue but
sacriﬁces the ﬁne-tuning performance. Moreover, we propose
a new method called Learning without Forgetting via Replayed
Vocabularies (VR-LwF), which can largely maintain the zero-
shot learning capability while making the CLIP model beneﬁt
from ﬁne-tuning. To our knowledge, our study complies with
general ethical conduct and does not have any ethical issues.

REFERENCES

[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training
of deep bidirectional transformers for language understanding,” CoRR,
vol. abs/1810.04805, 2018.

[2] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving

language understanding by generative pre-training.”

[3] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-
agnostic visiolinguistic representations for vision-and-language tasks,”
arXiv preprint arXiv:1908.02265, 2019.

304050607080OST-IR@12030405060OST-TR@1FlickrCOCOFTLwFGeoDLIMMRKRVR-LwF455055606570758085MST-IR@130405060MST-TR@112

[47] J. Serra, D. Suris, M. Miron, and A. Karatzoglou, “Overcoming catas-
trophic forgetting with hard attention to the task,” in International
Conference on Machine Learning, 2018, pp. 4548–4557.

[48] M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi,
H. Namkoong, and L. Schmidt, “Robust ﬁne-tuning of zero-shot mod-
els,” CoRR, vol. abs/2109.01903, 2021.

[49] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision, 2014, pp. 740–
755.

[50] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for
generating image descriptions,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 3128–3137.
[51] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,
H. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised
few-shot classiﬁcation,” in 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings, 2018.

[52] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International journal of computer
vision, vol. 115, no. 3, pp. 211–252, 2015.

[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, 2017, pp. 5998–6008.

[54] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, “Vse++: Improv-
ing visual-semantic embeddings with hard negatives,” arXiv preprint
arXiv:1707.05612, 2017.

[55] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015.

[56] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for continual
learning,” Advances in neural information processing systems, vol. 30,
pp. 6467–6476, 2017.

[57] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hocken-
maier, and S. Lazebnik, “Flickr30k entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models,” International
Journal of Computer Vision, vol. 123, no. 1, pp. 74–93, 2017.

[25] J.-Y. Park and J.-H. Kim, “Online incremental classiﬁcation resonance
network and its application to human–robot interaction,” IEEE trans-
actions on neural networks and learning systems, vol. 31, no. 5, pp.
1426–1436, 2019.

[26] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W.
Teh, R. Pascanu, and R. Hadsell, “Progress & compress: A scalable
learning,” in International Conference on
framework for continual
Machine Learning, 2018, pp. 4528–4537.

[27] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars,
“Memory aware synapses: Learning what (not) to forget,” in Proceedings
of the European Conference on Computer Vision (ECCV), 2018, pp.
139–154.

[28] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through synaptic
intelligence,” in International Conference on Machine Learning, 2017,
pp. 3987–3995.

[29] I. Paik, S. Oh, T. Kwak, and I. Kim, “Overcoming catastrophic for-
getting by neuron-level plasticity control,” in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 5339–
5346.

[30] D. J. MacKay, “A practical bayesian framework for backpropagation
networks,” Neural computation, vol. 4, no. 3, pp. 448–472, 1992.
[31] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” CoRR, vol. abs/1503.02531, 2015.

[32] P. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa, “Learning
without memorizing,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2019, pp. 5138–5146.
[33] A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle, “Podnet:
Pooled outputs distillation for small-tasks incremental
learning,” in
Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XX 16, 2020, pp. 86–102.
[34] Y. Liu, Y. Su, A.-A. Liu, B. Schiele, and Q. Sun, “Mnemonics training:
Multi-class incremental learning without forgetting,” in Proceedings of
the IEEE/CVF conference on Computer Vision and Pattern Recognition,
2020, pp. 12 245–12 254.

[35] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618–626.

[36] D. Isele and A. Cosgun, “Selective experience replay for lifelong learn-
ing,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, no. 1, 2018.

[37] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl:
Incremental classiﬁer and representation learning,” in Proceedings of the
IEEE conference on Computer Vision and Pattern Recognition, 2017, pp.
2001–2010.

[38] A. Prabhu, P. H. Torr, and P. K. Dokania, “Gdumb: A simple approach
that questions our progress in continual learning,” in European confer-
ence on computer vision, 2020, pp. 524–540.

[39] H. Shin, J. K. Lee, J. Kim, and J. Kim, “Continual learning with
deep generative replay,” in Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp.
2990–2999.

[40] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis, “Con-
tinual classiﬁcation learning using generative models,” CoRR, vol.
abs/1810.10612, 2018.

[41] L. Wang, B. Lei, Q. Li, H. Su, J. Zhu, and Y. Zhong, “Triple-
memory networks: A brain-inspired method for continual learning,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 33,
no. 5, pp. 1925–1934, 2021.

[42] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Advances in neural information processing systems, vol. 27, 2014.
[43] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013.

[44] S. Yan, J. Xie, and X. He, “Der: Dynamically expandable representation
for class incremental learning,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 2021, pp. 3014–3023.
[45] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with
dynamically expandable networks,” in 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings, 2018.

[46] G.-M. Park, S.-M. Yoo, and J.-H. Kim, “Convolutional neural network
with developmental memory for continual learning,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 32, no. 6, pp. 2691–
2705, 2020.

