JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

FOVQA: Blind Foveated Video Quality Assessment

Yize Jin, Anjul Patney, Richard Webb, Alan C. Bovik, Fellow, IEEE

1
2
0
2

n
u
J

4
2

]

V

I
.
s
s
e
e
[

1
v
8
2
3
3
1
.
6
0
1
2
:
v
i
X
r
a

Abstract—Previous blind or No Reference (NR) video quality
assessment (VQA) models largely rely on features drawn from
natural scene statistics (NSS), but under the assumption that the
image statistics are stationary in the spatial domain. Several of
these models are quite successful on standard pictures. However,
in Virtual Reality (VR) applications, foveated video compression
is regaining attention, and the concept of space-variant quality
assessment is of interest, given the availability of increasingly
high spatial and temporal resolution contents and practical
ways of measuring gaze direction. Distortions from foveated
video compression increase with increased eccentricity, implying
that
the natural scene statistics are space-variant. Towards
advancing the development of foveated compression / streaming
algorithms, we have devised a no-reference (NR) foveated video
quality assessment model, called FOVQA, which is based on new
models of space-variant natural scene statistics (NSS) and natural
video statistics (NVS). Speciﬁcally, we deploy a space-variant
generalized Gaussian distribution (SV-GGD) model and a space-
variant asynchronous generalized Gaussian distribution (SV-
AGGD) model of mean subtracted contrast normalized (MSCN)
coefﬁcients and products of neighboring MSCN coefﬁcients,
respectively. We devise a foveated video quality predictor that
extracts radial basis features, and other features that capture per-
ceptually annoying rapid quality fall-offs. We ﬁnd that FOVQA
achieves state-of-the-art (SOTA) performance on the new 2D
LIVE-FBT-FCVR database, as compared with other leading
FIQA / VQA models. we have made our implementation of
FOVQA available at: http://live.ece.utexas.edu/research/Quality/
FOVQA.zip.

Index Terms—foveated video quality assessment, no reference
video quality assessment, space-variant natural scene statistics,
Virtual Reality.

I. INTRODUCTION

R ECENT advancements in Virtual Reality (VR) have

drawn increasing attention to the development of im-
mersive video contents, including high-resolution (4K+) 360◦
videos. Until recently, head-mouted displays (HMDs) for
VR have supported resolutions of about 1Kx1K to 2Kx2K.
However, more recent HMDs deliver wide ﬁelds of view
(FOV) approaching 200◦, frame rates exceeding 75Hz, and
the spatial resolutions that are approaching 8K. These spatial
resolutions equate to angular resolutions of 10 ∼ 20 pixels per
degree (ppd), while the maximum resolution of the human
eye approaches 120 ppd. Future immersive and 360◦ video
displays systems can beneﬁt by increased resolutions which
will drive even greater demand on the already signiﬁcant
bandwidth consumption.

One way to reduce bandwidth consumption is by using
foveated protocols for video compression, which is a topic

Y. Jin and A. C. Bovik are with the Department of Electrical and Computer
Engineering, The University of Texas at Austin, Austin, TX, 78712 USA e-
mail: yizejin@utexas.edu; bovik@ece.utexas.edu

A. Patney was with Facebook Reality Labs. He is now with NVIDIA. e-

mail: anjul.patney@gmail.com

R. Webb is with Facebook Reality Labs. e-mail: rwebb@fb.com

of increasing research interest, because of the availability
of inexpensive and accurate consumer eyetrackers. Foveated
compression techniques exploit the spatially decreasing acuity
of the human vision system (HVS) away from the foveal center
to achieve signiﬁcant bandwidth savings, as for example by
assigning larger quantization parameters (QP) to contents lying
in the visual periphery. While several foveated compression
algorithms [1]–[4] have been designed on top of modern video
codec standards like H.264 / AVC and H.265 / HEVC, it is
important to understand how the HVS perceives the outcomes
of these compression protocols. For example, the authors of
[1], [4] conducted user studies to measure the quality of
foveated compression / streaming algorithms they proposed.
Towards creating a more generally applicable tools capable
of predicting the perceptual quality of foveated / compressed
contents, we recently designed 2D and 3D VR foveated video
quality databases (LIVE-FBT-FCVR) [5] containing a wide
spectrum of foveated and compressed distortions, on which
we conducted extensive subjective studies of perceived quality.
This new resource is intended to help escalate the development
of accurate and efﬁcient objective foveated video quality
assessment (FVQA) models, which in turn can be used to
help advance the development of improved foveated video
compression techniques.

The ﬁeld of objective FIQA / FVQA is sparse, especially in
the area of no reference (NR) models. Although existing non-
foveated (traditional) NR algorithms can be directly applied to
foveated videos, they are unable to adequately capture the the
perceptual effects of space-varying distortions. For example,
even perceptually acceptable foveated videos may contain
very low-quality contents in the visual periphery, as long as
high-quality (low QP) contents ﬁll the inner FOV, i.e., the
foveal and parafoveal projections. Traditional algorithms way
accurately respond to foveal and parafoveal distortions, but
will inaccurately respond to (often intentional) degradations
of peripheral contents, due to their underlying assumption of
distortions that are uniformly distributed in the spatial domain.
Here we seek to advance progress on automatically assess-
ing foveation distortions in the form of a new FVQA model,
that we call FOVQA. FOVQA is driven by space-variant
natural scene statistics (NSS) and natural video statistics
(NVS) models [6], [7], wherein the assumptions of spatially
stationarity are removed. Speciﬁcally, we deploy space-variant
generalized Gaussian distribution (SV-GGD) and space-variant
asynchronous generalized Gaussian distribution (SV-AGGD)
models of the distributions of Mean Subtracted Contrast Nor-
malized (MSCN) coefﬁcients of VR videos having known or
measured ﬁxation coordinates.

We also propose a number of other foveation-speciﬁc fea-
tures which are able to capture important and unique factors
the perceptual quality of foveated videos. We
that affect

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

advance progress towards this goal by designing a set of spatial
weighting patterns to extract and pool features derived under
SV-GGD and SV-AGGD over a discrete range of granularities.
We also model an important perceptual phenomenon whereby
foveated video quality is not only affected by the distortion
levels, but also by the rapidity of quality fall-off from fovea to
far periphery. We have found that this new source of signiﬁcant
quality degradation can be captured by analyzing the gradients
of local video statistics.

We thoroughly tested the efﬁcacy of FOVQA by a detailed
ablation study, and comparing its performance against existing
foveated and non-foveated VQA models. The rest of the paper
is organized as follows: Section II studies previous work
on video quality assessment, including both traditional VQA
models and foveated VQA models. Section III describes the
proposed FOVQA algorithm. Experiments and results are pre-
sented and discussed in Section IV, and Section V concludes
the paper and discusses possible future improvements.

II. RELATED WORK

Objective video quality assessment models have signiﬁ-
cantly evolved over the past two decades. Diverse application
scenarios have guided researchers to develop models that rely
on varying amounts of information from a pristine reference
video, ranging from full reference (FR), reduced reference
(RR), to no reference (NR) models.

FR models have been extensively studied and used, and are
well exempliﬁed by SSIM [8] and MS-SSIM [9], whereby
perceptually relevant luminance, contrast, and structure com-
parison measurements are integrated. The use of natural scene
statistics models for picture quality prediction were ﬁrst used
in [10], [11], which model bandpass images as obeying a
Gaussian scale mixture (GSM) [12] model. The visual infor-
mation ﬁdelity (VIF) [11] deploys a neural noise model of
uncertainty in the perceptual process. Another popular algo-
rithm called the feature similarity (FSIM) index [13] measures
image phase congruency (PC) and gradient magnitude (GM)
in a SSIM-like setting.

The aforementioned models, while often used to conduct
VQA, do not make any temporal measurements. Among those
that do, an early model called the Video Quality Metric (VQM)
[14] uses local spatial-temporal (S-T) features to predict video
quality. The MOVIE index [15] models motion sensitive neural
responses in extra-cortical area MT [16] to extract temporal
artifacts. The Video Multimethod Assessment Fusion (VMAF)
[17] combines features from VIF [11], Detail Loss Metric
(DLM) [18], and frame differences, using them to train a
Support Vector Regressor (SVR) to predict video quality.

RR VQA models are applicable in video quality monitoring
scenarios, where only a small amount of information is drawn
from the reference videos. Models like [19]–[21] exploit nat-
ural scene statistics (NSS) and natural video statistics (NVS)
to measure distortion-induced statistical deviations of distorted
videos from pristine videos.

Many existing NR VQA models rely on NSS and / or
NVS. Frame-based algorithms like BRISQUE [22] and NIQE
[23] extract simple spatial NSS parameters from bandpass and

locally divisively normalized luminance frames, mapping them
to quality predictions via an SVR [24] or a statistical distance
[23]. The Integrated Local NIQE (IL-NIQE) [25] extends
NIQE by incorporating gradient and chromatic statistics into
the NIQE framework. V-BLIINDS [26] injects temporal infor-
mation into the video quality prediction process by employing
natural video statistics (NVS) [27] models of statistics of
frame differences, and a motion masking model. VIIDEO [28]
extended NIQE by incorporating NVS into prediction without
training. The Two Level Video Quality Model (TLVQM) [29]
takes a different approach by using a set of highly handcrafted
features, obtaining SOTA performance on several datasets
[30]–[32].

Limited progress has been made on FIQA / FVQA models.
The Foveated Wavelet Quality Index (FWQI) [33] combines
an eccentricity-dependent contrast sensitivity function (CSF)
[34] with a visually detectable noise threshold model [35],
to quantify the inﬂuence of peripheral distortions on the
overall perceptual quality. The Foveated PSNR (FPSNR) and
foveated weighted SNR (FWSNR) models [36] account for
foveated distortions by integrating curvilinear coordinate sys-
tems into the traditional PSNR / SNR metrics. The Foveation-
based Content Adaptive SSIM (FA-SSIM) [37] model com-
bines SSIM with a foveation-based sensitivity function [38],
whereby the effects of object velocity in the visual periphery
are considered. A recent extension of BRISQUE to include
foveation called Space-Variant BRISQUE (SVBRISQUE) [39]
deploys NSS and NVS models over foveation-graded concen-
tric regions.

Towards further advancing progress on the foveated video
quality prediction problem, we have developed a new predic-
tion model called FOVQA that includes the following features:
• We devised space-variant GGD and AGGD (SV-GGD
and SV-AGGD) models, and use them to capture space-
variant distortions that are characteristic of foveated com-
pression.

• We deploy a unique model of foveated quality fall-off,
which we use to capture perceptual sensitivity to rapid
changes in quality with increased eccentricity relative to
visual ﬁxations.

• Our feature extraction methods are all linear operations,
hence simple online averaging can be used to stabilize the
features or statistics computed from them across frames
and viewing directions. This is much more memory
efﬁcient then accumulating video frames or coefﬁcients
computed from them.

III. A SPACE-VARIANT NSS MODEL

A. Statistics of Normalized Bandpass Coefﬁcients

In [40], Ruderman pointed out that divisively normalizing
bandpass-ﬁltered natural images with the deviations of neigh-
boring bandpass samples tends to yield decorrelated, Gaussian
distributed coefﬁcients. A simple version of the bandpass and
normalization process is:

ˆI =

I(i, j) − µ(i, j)
σ(i, j) + C

,

(1)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

where (i, j) are spatial indices, and C is a stabilizing or
saturation constant. The local mean µ and standard deviation
σ are:

µ(i, j) =

wk,lIk,l(i, j)

(2)

K
(cid:88)

L
(cid:88)

k=−K

l=−L

to model MSCN coefﬁcients as instead following space-variant
generalized Gaussian distributions (SV-GGD):

ˆI(r) ∼ f (x; α(r), σ(r)2)

=

α(r)
2β(r)Γ( 1

α(r) )

(cid:32)

exp

−

(cid:19)α(r)(cid:33)
,

(cid:18) |x|
β(r)

(9)

and

σ(i, j) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

L
(cid:88)

k=−K

l=−L

wk,l(Ik,l(i, j) − µ(i, j))2,

(3)

where r = (i, j) are spatial indices, and α(r) and β(r) vary
spatially with r. In a similar way, AGGD models can be made
space-variant (SV-AGGD):

where w is a 2D Gaussian weighting window of size (2K +
1, 2L + 1) sampled out to three standard deviations. We will
refer to (1) as mean subtracted contrast normalized (MSCN)
coefﬁcents. Widely-used NR VQA models seek to quantify
perceptual distortions as a mapping between measurable dis-
tortions from these statistical regularities to perceptual quality
[22], [41], [42]. The empirical distributions (histograms) of
the MSCN coefﬁcients of both natural (α = 2) and distorted
images can be modeled as following a generalized Gaussian
distribution (GGD):

f (x; α, σ2) =

α
2βΓ(1/α)

(cid:18)

exp

−

(cid:18) |x|
β

(cid:19)α(cid:19)

,

where α and σ2 are shape and scale parameters,

β = σ

(cid:115)

Γ(1/α)
Γ(3/α)

,

and Γ(·) is the gamma function:

Γ(a) =

(cid:90) ∞

0

ta−1e−tdt a > 0.

(4)

(5)

(6)

Likewise, the products of pairs of adjacent MSCN coefﬁ-
cients have been effectively modeled as following zero mode
asymmetric GGD (AGGD) models. The estimated parameters
of both (4) and the AGGD models have been successfully
used as quality-aware features. The paired products of MSCN
coefﬁcients are deﬁned as:

P (i, j) = ˆI(i, j) ˆI(i + d1, j + d2),

(7)

where (d1, d2) ∈ {(0, 1), (1, 0), (1, 1), (1, −1)}, while the
AGGD model is:

f (x; ν, σ2

l , σ2

r ) =






ν
(βl+βr)Γ( 1
ν
(βl+βr)Γ( 1

ν ) exp
ν ) exp

(cid:16)

(cid:16)

−

−

(cid:16) −x
βl
(cid:16) −x
βr

(cid:17)ν(cid:17)

(cid:17)ν(cid:17)

x < 0

x ≥ 0,

(8)
where βl and βr are the scale parameters of the left half and
the right half of (8).

B. Space-Variant GGD and AGGD Models

Popular NSS / NVS based models like BRISQUE [22] and
V-BLIINDS [26] generally assume that video distortions are
uniformly distributed over space, hence MSCN coefﬁcients
are tacitly assumed to be spatially stationary. However, the
distortions of foveated videos are intrinsically space-variant.
To capture this property of foveated distortions, it is natural

f (x; ν(r), σ2



l (r), σ2
ν(r)

(βl(r)+βr(r))Γ( 1

r (r)) =
(cid:18)

ν(r) ) exp



ν(r)

(βl(r)+βr(r))Γ( 1

ν(r) ) exp

(cid:17)ν(r)(cid:19)

(cid:17)ν(r)(cid:19)

(cid:16) −x
βl(r)
(cid:16) −x
βr(r)

−

−

x < 0

x ≥ 0,

(10)

(cid:18)

where ν(r), βl(r), and βr(r) also vary with (r).

treatments. Since the maximum likelihood is,

Estimation of the parameters (α, β, ν, βl, βr) requires spe-
cial
in this
case, a functional of these space-variant parameters, maximum
likelihood estimation (MLE) would require a difﬁcult and ex-
pensive variational formulating. We do assume that the MSCN
coefﬁcients and their paired products are locally stationary, i.e.
within a P ×P window, the coefﬁcients / products within share
the same distribution, allowing the parameters to be estimated,
e.g. by the popular moment-matching approach in [43]. In
practice, we have found it sufﬁcient and computationally
efﬁcient to partition each input frame into non-overlapping
P ×P patches, then estimate a set of parameters on each patch,
without a loss of performance relative to a denser sampling.
We also introduce a neural noise on the input images:

˜I(i, j) = I(i, j) + Ws,

(11)

where Ws ∼ N (0, σ2
ws
MSCN coefﬁcients are calculated as:

), I(i, j) ∈ [0, 255] and the modiﬁed

ˆI =

˜I(i, j) − ˜µ(i, j)
˜σ(i, j) + Cs

.

(12)

The neural noise model is motivated in two ways. First,
similar to [11], it is a way for accounting for uncertainty
of visual perception, including noise affecting neurons along
the visual pathway. Unlike [11], the noise is not hypothetical,
since we explicitly introduce small amounts of simulated noise
on the video frames before processing via (12). This has the
second important beneﬁt of introducing a small amount of
variation on constant or near-constant frame regions, sewing to
regulate the behaviour of (12) where numeric zeros may occur,
as on saturated over- or under-exposed portions of frames.
These kinds of imperfections often occur on videos captured
by current 360◦ videos because of their (typically) limited
dynamic ranges.

As a result, we obtain an (cid:98)M/P (cid:99) × (cid:98)N/P (cid:99) matrix of each
parameter, where (M, N ) indicate the resolution of the input
image, and each element of the matrix is a quality-aware
feature corresponding to a local image patch.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

(a) Foveation Pattern

(b) Shape Matrix

(c) Variance Matrix

(d) Shape Matrix x0.5

(e) Variance Matrix x0.5

(a) Illustration of the 5-level foveation compression protocol. The compressions applied to the concentric regions (from center and moving outward)
Fig. 1.
are no compression, −crf = 51, −crf = 56, −crf = 60, and −crf = 63, respectively, viz., increasing from cneter outward. (b) The spatial shape map of
the local SV-GGD model of the foveated frame. (c) The variance map of the local SV-GGD model of the foveated frame. (d) The shape map of the SV-GGD
model of the downscaled (by 2) foveated frame. (e) The variance map of the SV-GGD model of the downscaled foveated frame. It can be seen that (b), (c),
(d) followed the foveation pattern in (a), while (d) is much more noisy.

C. Radial Basis Feature Extraction

While NSS features have proven to be highly predictive of
perceptual video quality, current models apply them under the
stationarity assumption. If applied to nonstationary, foveated
videos, they are also nonstationary, which must be accounted
for within any VQA model utilizing them.

As shown Fig. 1, we begin by synthesizing a 5-level
foveation distortion by ﬁrst dividing the ﬁeld of view (FOV)
of a given video frame into 5 concentric regions. Our method
of foveation involves compressing the content in each of the
annular regions using different quantization parameters (QPs),
as shown in Fig. 1a. An alternative would be to blur, then
compress each region, but we have found that this added
complexity does not improve results. To test this idea, we
sampled 100 video frames from among the contents in [5]
with this foveated compression protocal applied with various
parameters, each of resolution 1024x1024 and a ﬁeld of
view (FOV) of 90◦. On each of these foveation distorted
video viewports, we computed the MSCN coefﬁcients using
equations 11 and 12 with σws = 10−2, and Cs = 0.1. We
then estimated the parameter maps of the best ﬁtting SV-
GGD and SV-AGGD models by setting P = 32 on each
foveated frame and averaged the per-frame parameter maps.
We plotted the averaged spatial shape and variance maps
of the best-ﬁtting SV-GGD model of the MSCN coefﬁcents
of the foveated viewports in Figs. 1b and 1c, respectively,
then downscaled the foveated frame by 2, and again plotted
the averaged shape and variance maps in Figs. 1d and 1e,
respectively. It can be seen that the shape and variance maps
of the frame followed the foveation pattern in Fig. 1a. How-
ever, on the downscaled viewports, while the variance map
followed the foveation pattern, the shape map is quite noisy.
Hence, we did not use the shape map from downscaled video
viewports. We also discarded the shape map of the SV-AGGD
model of the downscaled viewports, for the same reason. We
observed similar noisy patterns of the local shape parameters
of downscaled frames from the LIVE IQA and VQA databases
[41], [44], as well, showing that this phenomenon generally
unlikely to be a database bias.

Following these observations and building upon our prelim-
inary ideas [39], we employed a set of normalized weighting

(a)

(b)

Fig. 2. Examples of toroidal functions of differing radii rk and widths
σGRBF . Different choices of rk and σGRBF makes it possible to extract
quality-aware NSS information at different granularities and eccentricities.

patterns to capture foveation-speciﬁc features from the pa-
rameter maps. First deﬁne a set of toroidal functions indexed
k ∈ 1, 2, ..., K by convolving isotropic (Dirac) impulse rings
of radii rk with a Gaussian function:

wk(r) =

(cid:18)

1

(cid:19)

| (cid:80)

r wk(r)|

· [δ(||r|| − rk) ∗ G(||r||; σGRBF )] ,

(13)

where δ(||r||) and ∗ indicate Dirac function and the con-
volution operation, respectively, and where r = (i, j) in-
indices, with i ∈ {1, 2, ..., (cid:98)M/P (cid:99)}, j ∈
dicates spatial
{1, 2, ..., (cid:98)N/P (cid:99)}. The parameter σGRBF dictates the spread
of G(||r||; σGRBF ), and hence the widths of the toroidal
functions, making it possible to extract quality-aware NSS at
various granularities and eccentricities, as illustrated in Fig. 2.
The norm ||r|| is deﬁned by:

||r||2 = (i −

(cid:98)M/P (cid:99) + 1
2

)2 + (j −

(cid:98)N/P (cid:99) + 1
2

)2,

(14)

where we assume the gaze point is the center of the image.
We extract / pool K features from each parameter map by
weighted summation:

fk,m =

(cid:88)

x

wk(r) · pm(r),

(15)

where pm indicates the mth parameter matrix.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

i
i
d
a
r

f
o

s
n
o
i
t
a
n
i
b
m
o
C

[0.24,0.32]

14.4

13.8

10.6

13

10.3

10.2

11.1

[0.16,0.32]

13.3

12.9

[0.16,0.24]

13.3

12

[0.08,0.32]

13.8

12.3

[0.08,0.24]

11.8

10.7

[0.08,0.16]

11.6

9.3

10

8.1

9.3

8.1

5.3

10.9

10.8

9.7

9.7

10.9

9.8

7.7

7.5

6.5

5.3

8.6

7.5

5.8

5.3

3.6

10.6

9.5

9.7

8.2

7.4

8.3

7.4

7.4

6.1

5.4

4.4

7.9

6.8

5.3

5.4

4.4

5.3

6.4

6.3

5.8

4.8

3.6

2.3

[0,51,56]

[0,51,60]

[0,51,63]

[0,56,60]

[0,56,63]

[0,60,63]

[51,56,60]

[51,56,63]

[51,60,63]

[56,60,63]

Combinations of compression

12

9

6

3

Fig. 3. Mean Ranked Opinion Scores (MROS) of each foveation distortion in the 2D LIVE-FBT-FCVR database, represented in combinations of compression
and radii. Higher MROS indicates that the combination / distortion is considered to have higher perceptual quality. By comparing [0, 51, 63] (the 3rd column)
against [0, 56, 60] (the 4th column), and [0, 56, 63] (the 5th column) against [51, 56, 60] (the 7th column), where the changes in foveation distortion occur
smoothly rather than with similar compression but sharper changes of distortion, smoother quality fall-offs from the center of foveation generally lead to
better perceptual quality.

ref

−crf 51

−crf 56

−crf 60

−crf 63

Best foveated quality.

Worst foveated quality.

Fig. 4. Ilustration of methods of creating foveated / compressed videos. The
inner radii deﬁne concentric regions as shown in the second row. The distorted
videos are deﬁned by selecting inner radii that separate the multiple adjacent
foveation regions. The solid arrows indicate the best foveated quality that is
allowed, while the dashed arrows indicate the worst foveated quality possible.

In our implementation, ﬁrst estimate the shape (α(r)) and
variance maps (β(r)) from each input frame under the SV-
GGD model (9), then estimate one shape map (ν(r)), one
mean map [22], [45], and two variance maps (βl(r), βr(r)) for
each of the four products of adjacent pairs in (7) modeled as
SV-AGGD (10), yielding 18 feature maps, thus 18K features
are extracted using (15). Then, from the downscaled input,
only estimate a variance map from the SV-GGD model and
two variance maps for each of the four paired products
modeled as SV-AGGD, obtaining 9 additional feature maps,
hence 9K extracted features. Overall, we obtain 27K features
from the feature extraction process. In our experiments and
in the ﬁnal FOVQA model, K = 10, hence 270 features are
learned from overall.

D. Modelling Rapid Quality Fall-off

creasing eccentricity appear to cause greater degradations of
perceptual quality, even if the intrinsic distortion is less. On
the LIVE-FBT-FCVR databases, foveation distortions were
created by i) ﬁrst sampling the space of compressed videos
using 4 QP values (−crf = 51, 56, 60, 63 in VP9), yielding 5
levels of perceptually discriminable levels of uniform distor-
tions (including the references), ii) then dividing the FOV into
one central, three annular, and one peripheral region, (hence 4
radii 0.08, 0.16, 0.24, 0.32 in radians), iii) and ﬁnally choosing
combinations of 3 of 5 compression levels, and 2 of 4 radii,
as shown in Fig. 4.

We illustrate the inﬂuence of rapid quality fall-offs in
Fig. 3, wherein Mean Ranked Opinion Scores (MROS) were
calculated on each foveated / distorted video created by
combinations of compression and radii. Higher MROS in-
dicates that the perceptual quality of the combination was
more preferred by subjects. It may be observed that generally,
MROS took higher values near the upper-left corner, where
less compression distortion was introduced and larger radii
were used (larger areas of high quality contents), as might be
expected. However, this does not explain the relatively high
MROS given to the compression combinations [0, 56, 60] (the
4th column) and [51, 56, 60] (the 7th column), which represent
smoother quality fall-offs than the neighboring compression
combinations / columns. Moreover, by comparing different
combinations of radii, it may be observed that the differences
of MROS between smooth combinations and their neighbors
become more evident when the radii are smaller, suggesting
that any quality fall-off effects have a reciprocal relationship
with eccentricity, viz., nearer the fovea.

Based on these observations, we devised a model of the
effects of quality fall-off. Sudden spatial increases in com-
pression can be quite noticeable, especially on moving content.
Fall-off can be captured by computing the gradient information
on the ground-truth QP map of the foveated frame. However,
in the blind video quality prediction setup, information re-
garding the QP values is usually not available. Hence, we ﬁrst
designed a database on which a QP predictor can be learned
to map NSS features to QP values, then extracted the gradient
information in the predicted QP maps.

An interesting phenomenon that we observed on the LIVE-
FBT-FCVR databases is that rapid quality fall-offs with in-

We ﬁrst collected 15 8K pristine immersive images which
from the contents in the LIVE-FBT-FCVR

are different

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Fig. 5. The QP prediction database. For all 15 pristine images, each was compressed into 6 distorted versions via VP9 using QP values 21, 42, 51, 56, 60,
and 63, then 18 viewport images were generated for each of the 7 versions (including the reference). Then 3 NSS features were computed on each viewport
image, and averaged across all the viewports. Finally an SVR was trained to map the features to QP values.

the predicted QP map is averaged across all the viewports of a
foveated video. It may be seen that the predicted map follows
the ground truth map, but noisier as could be expected.

Then, to compute the gradient information, we designed
a set of ﬁlters for the FVQA problem by deﬁning Gaussian
smoothed derivatives in the radial direction in polar coordi-
nates. There are two reasons for choosing Gaussian smoothed
derivatives: i) The Gaussian smoothing reduces the effects of
noise in the predicted QP maps, and ii) the variance of the
Gaussian function allows us to capture gradient information at
various scales of QP maps. The Gaussian smoothed derivatives
in the radial direction are deﬁned as:
∂
∂r
∂x
∂r

∂
∂y
= Gx(r; σG) cos θ + Gy(r; σG) sin θ,

Gr(r; σG) =

)G(r; σG)

G(r; σG)

∂
∂x

∂y
∂r

(16)

= (

+

where r = (x, y) = (r cos θ, r sin θ), r = ||r|| is the radius
with respect to the foveation point, θ is the polar angle, and
G(r; σG) is a Gaussian function with variance σG:

G(r; σG) =

1
√

(cid:18)

exp

−

(cid:19)

,

||r||2
2σ2
G

(17)

2π
Gx and Gy are orthogonal Gaussian derivatives in Cartesian
coordinates:

σG

√

Gx(x, y) = −

· exp− x2+y2

x
2π · σ3
y
2π · σ3
Then, the Gaussian derivative ﬁlters are applied to the pre-
dicted QP maps via convolution to obtain the gradient maps:

· exp− x2+y2

Gy(x, y) = −

(18)

√

2σ2

2σ2

.

Dr(r; σG) = QP (r) ∗ Gr(r; σG)

(19)

where QP (r) is the QP map.

However, given the gradient of the QP map, it is difﬁcult
to quantify the overall amount of perceptual quality fall-
off: the relation between the QPs and perceptual quality is

(a)

(b)

Fig. 6. (a) A ground truth QP map. (b) The corresponding predicted QP map
averaged across viewports and frames. It may be seen that the predicted QP
map follows the ground truth map, but is noisier as could be expected.

databases, all of them in ERP format, as shown in Fig. 5.
Then, we compressed these images using VP9 into 6 distorted
versions, using −crf = 21, 42, 51, 56, 60, and 63. To better
simulate VR viewing condition, we sampled 18 3D viewing
directions on a unit sphere, and generated a viewport frame
for each direction, as described in detail in Section V-A. For
each viewport frame, a set of NSS features was computed
exactly as by BRISQUE, then these features were averaged
across all 18 viewports. We found that the shape and variance
parameters of the GGD model from the two scales of the input
frames are generally useful features for QP prediction across
different databases, but did not use the shape parameter from
the downscaled frames to match the features used in FOVQA.
An SVR was then trained to map the selected 3 NSS features
to the ground truth QP.

When predicting the QP maps for the quality fall-off model,
for each viewport frame, we applied the previously learned
SVR to the spatial shape and variance maps of SV-GGD, and
the variance map of SV-GGD from the downscaled input, in a
pixel-by-pixel manner in the feature map space, yielding a QP
map for each viewport frame. A comparison of a ground truth
QP map and a predicted QP map is shown in Fig. 6, where

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

complicated, so the quality fall-off distribution is a complex
combination of fall-offs at different locations. Hence, we used
a set of Kf o weighting patterns wk as in (13) to capture
gradient information at different eccentricities, using these
features to train an SVR. The feature extraction process is
formulated as:

f f o
k,l =

(cid:88)

x

wk(r) · Dr(r; σl

G),

(20)

where σl
G, l ∈ {1, 2, ..., L} is a set of variances controlling
the scale of the gradient. In total, we extracted Kf oL fall-off
features.

One alternative method is to ﬁrst ﬁlter the QP maps us-
ing orthogonal Gaussian derivatives, then compute the root-
squared-sum of the responses (gradient magnitude), as in many
edge-detection related tasks [46], [47]:

Dx(r; σG) = QP (r) ∗ Gx(r; σG)
Dy(r; σG) = QP (r) ∗ Gy(r; σG)
(cid:113)

Dmag(r; σG) =

D2

x(r; σG) + D2

y(r; σG).

However, we use the ﬁlters (Gr(r; σG)) due to the following
reasons: i) variations of foveation distortions generally occur
along the radial direction, hence it is desirable to take Gaussian
smoothed derivatives along the radial direction to capture these
changes. ii) When applied to radially symmetric maps, such
as the ground truth QP maps (where QP (r) can be written
as QP (r)), the gradient map Dr(r; σG) in (19) is the same
as Dmag(r; σG) in (21). iii) Most importantly, Dr(r; σG) is
linearly related to QP (r), so that noise in the predicted QP
maps can be reduced via simple online updating, such as
feature averaging. When applied to VQA problems, the spatial
quality-aware features are generally applied on a frame-by-
frame basis, then averaged across all frames. In FVQA, the
per-frame space-variant statistics or predicted QP maps are
generally noisy due to there being a limited number of samples
used in local estimation, which will affect the accuracies of
the extracted quality-aware features. However, the noise can
be reduced by averaging across multiple frames since (19) and
(20) deﬁne a linear mapping between fall-off features and QP
maps, and the noise-reducing property is preserved through
the linearity.

IV. THE SPACE-VARIANT NVS MODEL

As ﬁrst shown in [48], there are no general regularities of the
statistics of motion vectors [20]. However, as shown in [20],
[21], high-quality video frame difference signals, following
bandpass ﬁltering and normalization by local contrast, obey
reliable space-time statistical regularities. These regularities
are impaired by distortions, and capturing measurements of
these altered statistics yields important quality-aware infor-
mation. An example of the distribution of MSCN coefﬁcients
after spatially displaced frame differencing is shown in Fig. 7
The spatial-temporal statistics of videos, also generally
referred as Natural Video Statistics (NVS), are widely used
in NR VQA models [6], [26], [28]. In our proposed model,
instead of using frame differences, we deploy the statistics of

-crf 0
-crf 51
-crf 56
-crf 60
-crf 63

·10−2

2

1.5

y
t
i
l
i
b
a
b
o
r
P

1

0.5

0

−3 −2 −1

0

1

2

3

MSCN coefﬁcients

Fig. 7. Distribution of MSCN coefﬁcients after displaced differencing frames.

(21)

displaced differences of foveated frames [7] using SV-GGDs
(9) and SV-AGGDs (10).

We also applied the previously described neural noise model

on the displaced frame differences for the same reasons:

I nvs
k,d1,d2

(i, j) = Ik+1(i, j) − Ik(i − d1, j − d2) + Wnvs, (22)

where (d1, d2) ∈ {(0, 1), (0, −1), (1, 0), (−1, 0)}, Ik indicates
the kth frame, and Wnvs ∼ N (0, wnvs). We then compute the
MSCN coefﬁcients as:

ˆI nvs
k,d1,d2

(i, j) =

I nvs
k,d1,d2
σnvs

k,d1,d2

(i, j) − µnvs

k,d1,d2

(i, j)

(i, j) + Cnvs

.

(23)

The MSCN coefﬁcients (I nvs

) and paired products of the
k
MSCN coefﬁcients (7) are modeled as obeying SV-GGDs
and SV-AGGDs, respectively, where we have used col-located
blocks of MSCN coefﬁcients (or paired products) across all
displacements (d1, d2) to yield a single set of parameter maps
to stabilize the statistics. Thus, we extracted 27K features
from the NVS model by the weighting patterns wk(r) to the
extracted NVS feature maps, and trained another SVR to map
these features to quality scores. Then the scores obtained from
the spatial and temporal SVR outputs are combined using
exponential weights:

ST Score = SScoreγT Score1−γ,

(24)

where SScore and T Score are the predictions from the two
trained SVRs, and γ weights their relative importances. We
found that the combined scores accurately predict the ground
truth quality scores when γ lies within a range of [0.65,0.95].
We summarize all of the extracted features in TABLE I.

V. PERFORMANCE AND ANALYSIS

We use the newly built 2D LIVE-FBT-FCVR database to
compare our proposed FOVQA model against a large variety
of existing FIQA / FVQA algorithms. We ﬁrst describe the
evaluation framework under which all of the algorithms were

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

TABLE I
SUMMARY OF SELECTED PARAMETER MAPS.

Feature ID
f1–f2K
f2K+1–f18K
f18K+1–f19K
f19K+1–f27K
f27K+1–f27K+Kf oL

f nvs
–f nvs
1
2K
f nvs
2K+1–f nvs
18K
f nvs
18K+1–f19Knvs
19K+1–f nvs
f nvs
27K

Feature generation
apply wk to the shape and variance maps
apply wk to the shape, mean, left variance, and right variance maps
apply wk to the variance map
apply wk to the left variance and right variance maps
apply wk to the gradients of the predicted QP map
apply wk to the shape and variance maps
apply wk to the shape, mean, left variance, and right variance maps
apply wk to the variance map
apply wk to the left variance and right variance maps

Computation Procedure
Fit SV-GGD to MSCN coefﬁcients
Fit SV-AGGD to pairwise products
Downscale the input, then ﬁt SV-GGD to MSCN coefﬁcients
Downscale then ﬁt SV-AGGD to pairwise products
Predict the QP map using NSS features and compute the gradient map
Fit SV-GGD to MSCN coefﬁcients from DFD†
Fit SV-AGGD to pairwise products from DFD
Downscale the input, then ﬁt SV-GGD to MSCN coefﬁcients from DFD
Downscale then ﬁt SV-AGGD to pairwise products from DFD

† DFD: displaced frame differences.

Longitude
Latitude

0
π/4

π/6
π/4

π/3
π/4

π/2
π/4

2π/3
π/4

5π/6
π/4

0
π/2

π/6
π/2

π/3
π/2

π/2
π/2

2π/3
π/2

5π/6
π/2

0
3π/4

π/6
3π/4

π/3
3π/4

π/2
3π/4

2π/3
3π/4

5π/6
3π/4

TABLE II
DIRECTIONS USED IN THE EVALUATION FRAMEWORK.

compared. Four criteria were used: Pearson’s linear correla-
tion coefﬁcient (PLCC), root mean squared error (RMSE),
Spearman’s rank order correlation coefﬁcient (SROCC), and
Kendall’s rank order correlation coefﬁcient (KROCC). Follow-
ing usual practice [39], logistic non-linearity was applied to
the predicted scores before computing PLCC and RMSE:

Q(x) = β2 +

β1 − β2
1 + exp(− x−β3
|β4| )

.

(25)

Finally, we analyze the performance of the proposed FOVQA
algorithm.

A. Evaluation Framework

To recover the foveated experience and to better enable the
more realistic comparison of algorithms, we simulate real-
time foveation in the 2D LIVE-FBT-FCVR database [5] in
the same way as [39], i.e. a set of 3D viewing directions
were selected on the unit sphere, and uniformly distributed in
terms of longitude and latitude, as shown in TABLE II. Along
each viewing direction, a viewport video for each foveation
distortion was generated at a resolution of 1024x1024 and a
90◦ FOV. The compared VQA models are evaluated on these
viewport videos.

B. Choice of Hyperparameters in FOVQA

We set the block size P in both the SV-GGD and SV-
AGGD models to 32. For the radial basis feature extraction
in Section III-C, we set ws = 0.01 and Cs = 0.1, and chose
K = 10, achieving a ﬁne granularity of features as compared
to SVBRISQUE, where the values of rk are linearly spaced
between 0 and 20, thus obtaining 270 features. The spread
parameter σGRBF was simply set to the spacing between two
adjacent rk values (rk − rk−1).

For the perceptual quality fall-off model, we used L = 7 by
setting σl
G ∈ {0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0} and set the
values of rk linearly spaced between 0 and 13, since we have
found that, outside this range, there is no QP fall-off on the
videos in the 2D LIVE-FBT-FCVR database. We set Kf o = 6,

TABLE III
COMPARISON OF VQA MODELS ON THE 2D LIVE-FBT-FCVR
DATABASE. THE HIGHEST PERFORMANCES ARE BOLDFACED.

Methods
PSNR
SSIM
MSSSIM
VIF
SRRED
STRRED
SPEEDIQA
SPEEDVQA
FSIM
FWQI
FASSIM
VMAF
BRISQUE
NIQE
V-BLIINDS
TLVQM
SVBRISQUE
FOVQA

SROCC↑
0.6954
0.7191
0.7243
0.8068
0.7885
0.7010
0.7866
0.6238
0.7808
0.7848
0.7418
0.8103
0.797±0.22
0.605±0.32
0.440±0.25
0.509±0.36
0.900±0.11
0.939±0.10

KROCC↑
0.5044
0.5250
0.5273
0.6182
0.5873
0.5182
0.5872
0.4539
0.5850
0.5909
0.5531
0.6176
0.639±0.18
0.457±0.24
0.327±0.20
0.381±0.26
0.736±0.12
0.804±0.12

PLCC↑
0.6941
0.7260
0.7288
0.8102
0.7896
0.6922
0.7760
0.6584
0.7752
0.7906
0.7573
0.8047
0.708±0.18
0.675±0.31
0.431±0.25
0.470±0.36
0.884±0.10
0.921±0.10

RMSE↓
7.09
6.77
6.75
5.77
6.04
7.10
6.21
7.41
6.22
6.03
6.43
5.84
9.60±3.29
6.47±2.27
11.11±2.07
10.38±3.09
6.91±2.53
5.09±2.33

yielding 42 features. An SVR was trained on these overall 312
spatial features.

For the NVS model, we similarly obtained 270 features, and
another SVR was trained on these temporal features. When
combining the spatial and temporal models, we set γ = 0.85,
and we show that varying γ ∈ [0.65, 0.95] yields robust
performance in Section V-E.

C. Comparison of Algorithms

Among FR IQA algorithms, we included PSNR, SSIM [8],
MS-SSIM [9], VIF [11], S-RRED [19], Speed-IQA [21], FSIM
[13], FWQI [33], and FA-SSIM [37]. These IQA algorithms
were applied on every frame of the 18 viewports of every
foveation compression distorted video,
then were average
pooled into a single ﬁnal score. Among FR / RR VQA
algorithms, we included ST-RRED [20], Speed-VQA [21], and
VMAF [17]. These VQA algorithms were applied on each of
the 18 viewport videos, and averaged into a single ﬁnal score.
Among the compared NR VQA algorithms, we included
BRISQUE [22], SVBRISQUE [39], NIQE [23], V-BLIINDS

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

1

0.75

0.5

0.25

0

C
C
O
R
S

−0.25

−0.5

−0.75

E
U

RISQ

B

E
NIQ

DS
LIIN

B
V

M
Q
LV
T

E
U

RISQ

B
SV

A-1
Q
V
FO

A-2
Q
V
FO

A-t
Q
V
FO

A
Q
V
FO

Fig. 8. SROCC performance of NR IQA / VQA algorithms. FOVQA-1 shows the results when only radial basis features are used, FOVQA-2 shows the results
when quality fall-off features are included, FOVQA-t shows the results when only temporal features are used, and ﬁnally FOVQA shows the performance
when combining FOVQA-2 and temporal FOVQA-t using γ = 0.85.

[26], TLVQM [29], and the proposed FOVQA algorithm.
To implement BRISQUE, SVBRISQUE, V-BLIINDS, and
TLVQM, we extracted and averaged features across every
frame of each viewport video, obtaining one set of features
for each foveated / distorted video, and trained an SVR to
map features into quality scores. For NIQE, we followed the
same procedure as the FR VQA models, by averaging quality
scores obtained from each viewport frame and over all 18
viewport videos.

For these NR algorithms, we ran the following procedure
1000 times and reported the median performance: ﬁrst split
the database into train (80%) and test datasets (20%) without
content overlaps, then use a 4-fold cross validation to ﬁnd the
best hyperparameters for the SVR, ﬁnally train the SVR using
the training set and record the performance on the test set.

The comparison of algorithms on the 2D LIVE-FBT-FCVR
database is shown in TABLE III, where the predictions from
FR algorithms are compared against Difference Mean Opinion
Scores (DMOS), while the NR algorithms were trained on
and compared against Mean Opinion Scores (MOS), following
common practice. It may be readily observed that FOVQA
algorithm was able to achieve a signiﬁcant leap in SOTA per-
formance, reaching a rank-order correlation of 0.939 against
the human judgments.

TABLE IV
ABLATION STUDY OF THE PROPOSED FOVQA ALGORITHM. FOVQA-1
SHOWS THE RESULTS WHEN ONLY THE RADIAL BASIS FEATURES ARE
USED, FOVQA-2 SHOWS THE RESULTS WHEN QUALITY FALL-OFF
FEATURES ARE ALSO APPLIED, FOVQA-T SHOWS THE RESULTS WHEN
ONLY TEMPORAL FEATURES ARE USED, AND FOVQA SHOWS THE
COMBINED PERFORMANCE WHEN γ = 0.85. BEST PERFORMANCES ARE
BOLDFACED.

Methods
FOVQA-1
FOVQA-2
FOVQA-t
FOVQA

SROCC↑
0.922±0.10
0.934±0.10
0.843±0.16
0.939±0.10

KROCC↑
0.778±0.11
0.795±0.12
0.673±0.15
0.804±0.12

PLCC↑
0.904±0.10
0.916±0.10
0.846±0.14
0.921±0.10

RMSE↓
5.78±1.89
5.29±2.21
7.20±2.20
5.09±2.33

TABLE V
ROBUSTNESS OF PERFORMANCE AGAINST DIFFERENT VALUES OF γ. BEST
PERFORMANCES ARE BOLDFACED.

γ
0.65
0.7
0.75
0.8
0.85
0.9
0.95

SROCC↑
0.934±0.11
0.936±0.10
0.938±0.10
0.938±0.10
0.939±0.10
0.939±0.10
0.937±0.10

KROCC↑
0.792±0.12
0.795±0.12
0.801±0.12
0.801±0.12
0.804±0.12
0.801±0.12
0.801±0.12

PLCC↑
0.924±0.11
0.922±0.05
0.922±0.10
0.921±0.10
0.921±0.10
0.920±0.10
0.919±0.10

RMSE↓
5.12±2.42
5.08±2.41
5.07±2.39
5.06±2.37
5.09±2.33
5.17±2.29
5.22±2.25

D. Ablation Study

E. Rubustness of γ

We conducted an ablation study by performing a step-by-
step analysis of the performance of FOVQA, as shown in Fig.
8 and TABLE IV. It may be observed that both the spatial and
temporal FOVQA models outperformed other prior existing
models by a large margin. In addition, it may be seen that
the perceptual quality fall-off features provide complementary
information to the other NSS features.

We studied the performance obtained by combining the
spatial and temporal outputs of the corresponding SVRs using
different values of γ, as shown in TABLE V. For better
comparison, we used a ﬁxed set of 1000 train-test splits. It
may be seen that the performance of FOVQA is robust against
the choice of γ, but skewed towards spatial predictions.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

VI. CONCLUSION AND FUTURE WORK

We extended traditional natural scene statistics (NSS) and
natural video statistics (NVS) models into space-variant NSS
/ NVS models suitable for foveated compression scenarios,
and proposed a new foveated video quality assessment al-
gorithm, called FOVQA, that is based on space-variant gen-
eralized Gaussian distributions (SV-GGD) and space-variant
asynchronous generalized Gaussian distributions (SV-AGGD).
The new FOVQA algorithm has several distinguishing ele-
ments, including the use of parametric space-variant statistical
models, new feature extraction / pooling schemes, and unique
features that model the rapidity of quality fall-off of foveation
distortions, which can signiﬁcantly degrade perceptual qual-
ity. We conducted ablation studies to show that the various
’quality-aware’ features we utilize in FOVQA indeed provide
beneﬁcial, but not redundant information towards predicting
perceptual quality.

One possible future direction is to reduce the time com-
plexity of parameter estimation of the space-variant models.
While the computational complexity of FOVQA is already
signiﬁcantly reduced by using non-overlapping local patches,
computing the parameter maps still requires signiﬁcant compu-
tation. A light-weight FVQA algorithm would be desirable and
beneﬁcial for real-time foveated VR video quality assessment
applications.

ACKNOWLEDGMENT

The authors thank Facebook Technologies for the fruitful
discussions and for supporting this research. The authors
would also like to thank Li-Heng Chen for sharing compu-
tational resources.

REFERENCES

[1] J. Ryoo, K. Yun, D. Samaras, S. R. Das, and G. Zelinsky, “Design
and evaluation of a foveated video streaming service for commodity
the 7th International Conference
client devices,” in Proceedings of
on Multimedia Systems, ser. MMSys ’16.
New York, NY, USA:
Association for Computing Machinery, 2016.

[2] M. F. Romero-Rond´on, L. Sassatelli, F. Precioso, and R. Aparicio-
Pardo, “Foveated streaming of virtual reality videos,” in Proceedings
of
the 9th ACM Multimedia Systems Conference, ser. MMSys ’18.
New York, NY, USA: Association for Computing Machinery, 2018, p.
494–497. [Online]. Available: https://doi.org/10.1145/3204949.3208114
[3] H. Kim, J. Yang, M. Choi, J. Lee, S. Yoon, Y. Kim, and W. Park, “Eye
tracking based foveated rendering for 360 vr tiled video,” in Proceedings
of
the 9th ACM Multimedia Systems Conference, ser. MMSys ’18.
New York, NY, USA: Association for Computing Machinery, 2018, p.
484–486. [Online]. Available: https://doi.org/10.1145/3204949.3208111
[4] G. K. Illahi, T. V. Gemert, M. Siekkinen, E. Masala, A. Oulasvirta, and
A. Yl¨a-J¨a¨aski, “Cloud gaming with foveated video encoding,” ACM
Trans. Multimedia Comput. Commun. Appl., vol. 16, no. 1, Feb. 2020.
[Online]. Available: https://doi.org/10.1145/3369110

[5] Y. Jin, M. Chen, T. Goodall, A. Patney, and A. C. Bovik, “Subjective and
objective quality assessment of 2d and 3d foveated video compression
in virtual reality,” IEEE Transactions on Image Processing, submitted
for publication.

[6] X. Yu, N. Birkbeck, Y. Wang, C. G. Bampis, B. Adsumilli, and A. C.
Bovik, “Predicting the quality of compressed videos with pre-existing
distortions,” IEEE Transactions on Image Processing, submitted for
publication.

[7] D. Lee, H. Ko, J. Kim, and A. C. Bovik, “On the space-time statistics
of motion pictures,” in Journal of Vision, submitted for publication.
[8] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

[9] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural
similarity for image quality assessment,” in The Thrity-Seventh Asilomar
Conference on Signals, Systems Computers, 2003, vol. 2, 2003, pp.
1398–1402 Vol.2.

[10] H. R. Sheikh, A. C. Bovik, and G. de Veciana, “An information ﬁdelity
criterion for image quality assessment using natural scene statistics,”
IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2117–
2128, 2005.

[11] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,”
IEEE Transactions on Image Processing, vol. 15, no. 2, pp. 430–444,
2006.

[12] M. J. Wainwright and E. P. Simoncelli, “Scale mixtures of gaussians
and the statistics of natural images,” ser. NIPS’99. Cambridge, MA,
USA: MIT Press, 1999, p. 855–861.

[13] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “Fsim: A feature similarity
index for image quality assessment,” IEEE Transactions on Image
Processing, vol. 20, no. 8, pp. 2378–2386, 2011.

[14] M. H. Pinson and S. Wolf, “A new standardized method for objectively
measuring video quality,” IEEE Transactions on Broadcasting, vol. 50,
no. 3, pp. 312–322, 2004.

[15] K. Seshadrinathan and A. C. Bovik, “A model of neuronal responses in

visual area mt,” Vision Research, vol. 19, no. 2, pp. 335–350, 1998.

[16] E. P. Simoncelli and D. J. Heeger, “A Model of Neural Responses in

Area MT,” Vision research, vol. 38, no. 5, pp. 743–761, 1998.

[17] Z. Li, A. Aaron, A. Moorthy, and M. Manohara. Toward a
practical perceptual video quality metric. [Online]. Available: http:
//techblog.netﬂix.com/2016/06/toward-practical-perceptual-video.html

[18] S. Li, F. Zhang, L. Ma, and K. N. Ngan, “Image quality assessment
by separately evaluating detail losses and additive impairments,” IEEE
Transactions on Multimedia, vol. 13, no. 5, pp. 935–949, 2011.
[19] R. Soundararajan and A. C. Bovik, “Rred indices: Reduced reference
entropic differencing for image quality assessment,” IEEE Transactions
on Image Processing, vol. 21, no. 2, pp. 517–526, 2012.

[20] ——, “Video quality assessment by reduced reference spatio-temporal
entropic differencing,” IEEE Transactions on Circuits and Systems for
Video Technology, vol. 23, no. 4, pp. 684–694, 2013.

[21] C. G. Bampis, P. Gupta, R. Soundararajan, and A. C. Bovik, “Speed-
qa: Spatial efﬁcient entropic differencing for image and video quality,”
IEEE Signal Processing Letters, vol. 24, no. 9, pp. 1333–1337, 2017.

[22] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image
quality assessment in the spatial domain,” IEEE Transactions on Image
Processing, vol. 21, no. 12, pp. 4695–4708, 2012.

[23] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely
blind” image quality analyzer,” IEEE Signal Processing Letters, vol. 20,
no. 3, pp. 209–212, 2013.
[24] C. C. Chang and C.

support
vector machines. [Online]. Available: http://www.csie.ntu.edu.tw/∼cjlin/
libsvm/

J. Lin. Libsvm: A library for

[25] L. Zhang, L. Zhang, and A. C. Bovik, “A feature-enriched completely
blind image quality evaluator,” IEEE Transactions on Image Processing,
vol. 24, no. 8, pp. 2579–2591, 2015.

[26] M. A. Saad, A. C. Bovik, and C. Charrier, “Blind prediction of natural
video quality,” IEEE Transactions on Image Processing, vol. 23, no. 3,
pp. 1352–1365, 2014.

[27] D. W. Dong and J. J. Atick, “Statistics of natural time-varying images,”
Network: Computation in Neural Systems, vol. 6, no. 3, pp. 345–358,
1995. [Online]. Available: https://doi.org/10.1088/0954-898X 6 3 003
[28] A. Mittal, M. A. Saad, and A. C. Bovik, “A completely blind video
integrity oracle,” IEEE Transactions on Image Processing, vol. 25, no. 1,
pp. 289–300, 2016.

[29] J. Korhonen, “Two-level approach for no-reference consumer video
quality assessment,” IEEE Transactions on Image Processing, vol. 28,
no. 12, pp. 5923–5938, 2019.

[30] M. Nuutinen, T. Virtanen, M. Vaahteranoksa, T. Vuori, P. Oittinen, and
J. H¨akkinen, “Cvd2014—a database for evaluating no-reference video
quality assessment algorithms,” IEEE Transactions on Image Processing,
vol. 25, no. 7, pp. 3073–3086, 2016.

[31] V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir´anyi, S. Li, and
D. Saupe, “The konstanz natural video database (konvid-1k),” in 2017
Ninth International Conference on Quality of Multimedia Experience
(QoMEX), 2017, pp. 1–6.

[32] D. Ghadiyaram, J. Pan, A. C. Bovik, A. K. Moorthy, P. Panda, and
K. Yang, “In-capture mobile video distortions: A study of subjective
behavior and objective algorithms,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. 28, no. 9, pp. 2061–2077, 2018.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

[33] Z. Wang, A. C. Bovik, L. Lu, and J. L. Kouloheris, “Foveated wavelet
image quality index,” in Applications of Digital Image Processing
XXIV, A. G. Tescher, Ed., vol. 4472, International Society for Optics
and Photonics.
[Online]. Available:
https://doi.org/10.1117/12.449797

SPIE, 2001, pp. 42 – 52.

[34] W. S. Geisler and J. S. Perry, “Real-time foveated multiresolution
system for low-bandwidth video communication,” in Human Vision and
Electronic Imaging III, B. E. Rogowitz and T. N. Pappas, Eds., vol.
3299, International Society for Optics and Photonics. SPIE, 1998, pp.
294 – 305. [Online]. Available: https://doi.org/10.1117/12.320120
[35] A. B. Watson, G. Y. Yang, J. A. Solomon, and J. Villasenor, “Visibility
of wavelet quantization noise,” IEEE Transactions on Image Processing,
vol. 6, no. 8, pp. 1164–1175, 1997.

[36] Sanghoon Lee, M. S. Pattichis, and A. C. Bovik, “Foveated video quality
assessment,” IEEE Transactions on Multimedia, vol. 4, no. 1, pp. 129–
132, 2002.

[37] S. Rimac-Drlje, G. Martinovi´c, and B. Zovko-Cihlar, “Foveation-based
content adaptive structural similarity index,” in 2011 18th International
Conference on Systems, Signals and Image Processing, 2011, pp. 1–4.
“Foveated mean
squared error—a novel video quality metric,” Multimedia Tools and
Applications, vol. 49, no. 3, pp. 425–445, Sep 2010.
[Online].
Available: https://doi.org/10.1007/s11042-009-0442-1

[38] S. Rimac-Drlje, M. Vranjeˇs,

and D.

ˇZagar,

[39] Y. Jin, T. Goodall, A. Patney, and A. C. Bovik, “A natural scene statistics
based foveated video quality assessment model,” IEEE International
Conference on Image Processing, submitted for publication.

[40] D. L. Ruderman, “The statistics of natural images,” Network: Compu-

tation in Neural Systems, vol. 5, no. 4, pp. 517–548, 1994.

[41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, “A statistical evaluation
of recent full reference image quality assessment algorithms,” IEEE
Transactions on Image Processing, vol. 15, no. 11, pp. 3440–3451, 2006.
[42] A. K. Moorthy and A. C. Bovik, “Statistics of natural image distortions,”
in 2010 IEEE International Conference on Acoustics, Speech and Signal
Processing, 2010, pp. 962–965.

[43] K. Shariﬁ and A. Leon-Garcia, “Estimation of shape parameter for
generalized gaussian distributions in subband decompositions of video,”
IEEE Transactions on Circuits and Systems for Video Technology, vol. 5,
no. 1, pp. 52–56, 1995.

[44] K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack,
“Study of subjective and objective quality assessment of video,” IEEE
Transactions on Image Processing, vol. 19, no. 6, pp. 1427–1441, 2010.
[45] P. Gupta, J. Glover, N. Paulter, and A. Bovik, “Studying the statistics
of natural x-ray pictures,” Journal of Testing and Evaluation, vol. 46,
pp. 1478–1488, 2018.

[46] D. Marr and E. Hildbeth, “Theory of edge detection,” Proc. R. Soc.

Lond. B., vol. 207, pp. 187–217, 1980.

[47] A. C. Bovik, The Essential Guide to Image Processing.

6277 Sea
Harbor Drive Orlando, FL, United States: Academic Press, Inc., 2009.
[48] S. Roth and M. J. Black, “On the spatial statistics of optical ﬂow,” in
Tenth IEEE International Conference on Computer Vision (ICCV’05)
Volume 1, vol. 1, 2005, pp. 42–49 Vol. 1.

