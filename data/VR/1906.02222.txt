Nail Polish Try-On: Realtime Semantic Segmentation of Small Objects for
Native and Browser Smartphone AR Applications

Brendan Duke Abdalla Ahmed

Edmund Phung
ModiFace Inc.
{brendan,abdalla,edmund,irina,parham}@modiface.com

Irina Kezele

Parham Aarabi

9
1
0
2

n
u
J

0
1

]

V
C
.
s
c
[

2
v
2
2
2
2
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We provide a system for semantic segmentation of small
objects that enables nail polish try-on AR applications to
run client-side in realtime in native and web mobile ap-
plications. By adjusting input resolution and neural net-
work depth, our model design enables a smooth trade-
off of performance and runtime, with the highest perfor-
mance setting achieving 94.5 mIoU at 29.8ms runtime
in native applications on an iPad Pro. We also pro-
vide a postprocessing and rendering algorithm for nail
polish try-on, which integrates with our semantic seg-
mentation and ﬁngernail base-tip direction predictions.
Project page: https://research.modiface.com/
nail-polish-try-on-cvprw2019

1. Introduction

We present our end-to-end solution for simultaneous re-
altime tracking of ﬁngernails and rendering of nail pol-
ish. Our system locates and identiﬁes ﬁngernails from a
videostream in realtime with pixel accuracy, and provides
enough landmark information, e.g., directionality, to sup-
port rendering techniques.

We collected an entirely new dataset with semantic seg-
mentation and landmark labels, developed a high-resolution
neural network model for mobile devices, and combined
these data and this model with postprocessing and rendering
algorithms for nail polish try-on.

We deployed our trained models on two hardware plat-
forms:
iOS via CoreML, and web browsers via Tensor-
Flow.js [8]. Our model and algorithm design is ﬂexible
enough to support both the higher computation native iOS
platform, as well as the more resource constrained web plat-
form, by making only minor tweaks to our model archi-
tecture, and without any major negative impact on perfor-
mance.

1.1. Contributions

Below we enumerate our work’s novel contributions.

•

We created a dataset of 1438 images sourced from pho-
tos and/or videos of the hands of over 300 subjects,
and annotated with foreground-background, per-ﬁnger
class, and base-tip direction ﬁeld labels.

•

•

We developed a novel neural network architecture for
semantic segmentation designed for both running on
mobile devices and precisely segmenting small ob-
jects.

We developed a postprocessing algorithm that uses
multiple outputs from our ﬁngernail tracking model to
both segment ﬁngernails and localize individual ﬁnger-
nails, as well as to ﬁnd their 2D orientation.

2. Related Work

Our work builds on MobileNetV2 [7] by using it as an
encoder-backbone in our cascaded semantic segmentation
model architecture. However, our system is agnostic to the
speciﬁc encoder model used, so any existing efﬁcient model
from the literature [5, 9, 10, 12] could be used as a drop-in
replacement for our encoder.

Our Loss Max-Pooling (LMP) loss is based on [1],
where we ﬁx their p-norm parameter to p = 1 for sim-
plicity. Our experiments further support the effectiveness
of LMP in the intrinsically class imbalanced ﬁngernail seg-
mentation task.

Our cascaded architecture is related to ICNet [11] in
that our neural network model combines shallow/high-
resolution and deep/low-resolution branches. Unlike IC-
Net, our model is designed to run on mobile devices, and
therefore we completely redesigned the encoder and de-
coder based on this requirement.

3. Methods

3.1. Dataset

Due to a lack of prior work speciﬁcally on ﬁngernail
tracking, we created an entirely new dataset for this task.
We collected egocentric data from participants, who we
asked to take either photos or videos of their hands as if
they were showing off their ﬁngernails for a post on social
media.

Our annotations included a combination of three label
types: foreground/background polygons, individual ﬁnger-
nail class polygons, and base/tip landmarks to deﬁne per-
polygon orientation. We used the ﬁngernail base/tip land-
marks to generate a dense direction ﬁeld, where each pixel

1

 
 
 
 
 
 
of mobile devices, and to produce our multi-task outputs. A
top-level view of our model architecture is in Figure 1.

×

×

×

We initialized the encoder of our model with Mo-
bileNetV2 [7] model weights pretrained on ImageNet [2].
We used a cascade of two α = 1.0 MobileNetV2 encoder
backbones, both pretrained on 224
224 ImageNet images.
The encoder cascade consists of one shallow network with
high-resolution inputs (stage_high1..4), and one deep
network with low-resolution inputs (stage_low1..8),
both of which are preﬁxes of the full MobileNetV2. To
maintain a higher spatial resolution of feature maps out-
put by the low-resolution encoder we changed stage 6 from
stride 2 to stride 1, and used dilated 2
convolutions in
stages 7 and 8 to compensate, reducing its overall stride
from 32

to 16

×

.

×

16 ×

W , our decoder fuses the H

Our model’s decoder is shown in the middle and bot-
tom right of Figure 1, and a detailed view of the de-
coder fusion block is shown in Figure 2. For an origi-
W
nal input of size H
16
features from stage_low4 with the upsampled features
from stage_low8, then upsamples and fuses the result-
ing features with the H
W
8 features from stage_high4.
8 ×
The convolution on F1 is dilated to match F1’s
2 upsam-
pling rate, and the elementwise sum (to form F (cid:48)2) allows the
projected F2 to directly reﬁne the coarser F1 feature maps.
1 convolutional classiﬁer is
×
applied to the upsampled F1 features, which are used to pre-
dict downsampled labels. As in [3], this “Laplacian pyra-
mid” of outputs optimizes the higher-resolution, smaller
receptive ﬁeld feature maps to focus on reﬁning the pre-
dictions from low-resolution, larger receptive ﬁeld feature
maps.

As shown in Figure 2, a 1

×

Also shown in Figure 2, our system uses multiple out-
put decoder branches to provide directionality information
(i.e., vectors from base to tip) needed to render over ﬁnger-
nail tips, and ﬁngernail class predictions needed to ﬁnd ﬁn-
gernail instances using connected components. We trained
these additional decoders to produce dense predictions pe-
nalized only in the annotated ﬁngernail area of the image.

3.3. Criticism

To deal with the class imbalance between background
(overrepresented class) and ﬁngernail (underrepresented
class), in our objective function we used LMP [1] over all
pixels in a minibatch by sorting by the loss magnitude of
each pixel, and taking the mean over the top 10% of pix-
els as the minibatch loss. Compared with a baseline that
more than the
weighted the ﬁngernail class’s loss by 20
background, LMP yielded a gain of
2% mIoU, reﬂected
in sharper nail edges where the baseline consistently over-
segmented.

×

≈

We used three loss functions corresponding to the three
outputs of our model shown in Figure 2. The ﬁngernail class

Figure 1: A top level overview of our nail segmentation
model. In this example, the input resolution is H
W . See
×
Figure 2 for fusion and output block architectures.

Figure 2: The fusion module (left) and output branch (right)
of our model. We fuse upsampled low-resolution, deep fea-
tures F1 with high-resolution, shallow features F2 to pro-
duce high-resolution fused features F (cid:48)2 in the decoder. The
output branch is used in each fusion block, and at the full-
resolution output layer of the model.

contains a base-to-tip unit vector for the ﬁngernail that that
pixel belongs to.

Our annotated dataset consists of 1438 annotated images
in total, which we split into train, validation and test sets
based on the participant who contributed the images. The
split dataset contains 941, 254 and 243 images each in train,
val and test, respectively.

3.2. Model

The core of our nail tracking system is an encoder-
decoder convolutional neural network (CNN) architecture
trained to output foreground/background and ﬁngernail
class segmentations, as well as base-tip direction ﬁelds.
Our model architecture draws inspiration from ICNet [11],
which we improved to meet the runtime and size constraints

Downx232W4stagelow1stagelow216W424W8stagelow332W16stagelow464W32stagelow596W32stagelow6160W32stagelow7320W32stagelow832W2stagehigh1stagehigh216W224W4stagehigh332W8stagehigh4Upx2Fusion0Upx2Fusion1Upx8Output1xF1F2320WDilatedOutput320WProjection+F02320Isharedfea-tures101×1conv10Isoftmax101×1conv10Isoftmax2I1×1convL2lossNLLlossLMPNLLlossand foreground/background predictions both minimize the
negative log-likelihood of a multinomial distribution given
in Equation 1, where c is the ground truth class and xij
c is
the pre-softmax prediction of the cth class by the model.
Each of the following equations refers to indices i and j
corresponding to pixels at (x, y) = (i, j).

(cid:32)

ij
class,f gbg =

L

log

−

exp xij
c
c(cid:48) exp xij

c(cid:48)

(cid:80)

(cid:33)

(1)

, while
In the case of class predictions, c
}
∈ {
for foreground/background predictions c
. We used
}
LMP for foreground/background predictions only; since
ﬁngernail class predictions are only valid in the ﬁngernail
region, those classes are balanced and do not require LMP.

1, 2, . . . , 10
1, 2

∈ {

(cid:96)f gbg =

1
Nthresh

(cid:88)

ij

(cid:104)
L

(cid:105)

ij
f gbg > τ

ij
f gbg

· L

(2)

In Equation 2, Nthresh = (cid:80)
ij
0.1

old τ is the loss value of the
pixel. The [

(cid:98)
] operator is the indicator function.
·

×

×

(cid:99)

(cid:104)
ij
f gbg > τ
L
W
H

(cid:105)

, and thresh-

th highest loss

For the direction ﬁeld output, we apply an L2 loss (other
loss functions such as Huber and L1 could be used in its
place) on the normalized base to tip direction of the nail for
each pixel inside the ground truth nail.

(cid:96)f ield =

1

×

H

(cid:88)

W

ij

ˆuij
f ield −
(cid:107)

2
uij
f ield(cid:107)
2

(3)

The

ﬁeld

labels

normalized

uij
f ield(cid:107)2
(cid:107)

so
are
direction
that
= 1. Since the direction ﬁeld and the
ﬁngernail class losses have no class imbalance problem, we
simply set these two losses to the means of their respective
ij
individual losses (i.e., (cid:96)class =
class). The
overall loss is (cid:96) = (cid:96)f gbg + (cid:96)class + (cid:96)f ield.

ij L

(cid:80)

W

H

×

1

3.4. Postprocessing and Rendering

Our postprocessing and rendering algorithm uses the
output of our tracking predictions to draw realistic nail pol-
ish on a user’s ﬁngernails. Our algorithm uses the individual
ﬁngernail location and direction information predicted by
our ﬁngernail tracking module in order to render gradients,
and to hide the light-coloured distal edge of natural nails.

We ﬁrst use connected components [4] on the class pre-
dictions to extract a connected blob for each nail. We then
estimate each nail’s angle by using the foreground (nail)
softmax scores to take a weighted average of the direction
predictions in each nail region. We use the ﬁngernails’ con-
nected components and orientations to implement rendering
effects, including rendering gradients to approximate spec-
ular reﬂections, and stretching the nail masks towards the
ﬁngernail tips in order to hide the ﬁngernails’ light-coloured
distal edges.

Model

mIoU

Baseline
+ LMP
+ Cascade

91.3
93.3
94.5

Figure 3: Runtime/mIoU comparison for model settings on
an iPad Pro device (right) and ablation study of LMP and
cascade model architecture features (left).

3.5. Performance and Runtime Results

We trained our neural network models using Py-
Torch [6]. We deployed the trained models to iOS using
CoreML, and to web browsers using TensorFlow.js [8].

software

Given current

implementations,

namely
CoreML and TensorFlow.js, and current mobile device
hardware, our system could run in realtime (i.e., at
10
480 (native mobile)
FPS) at all resolutions up to 640
×
for which we trained
and 480
360 (web mobile),
on randomly-cropped input resolutions of 448
448
and 336

336, respectively.

×

×

≥

In Figure 3 we evaluate performance in binary mIoU
on our validation set against runtime on an iPad Pro de-
vice on CoreML (native iOS) and TensorFlow.js (Safari
web browser) software platforms using two variants of our
model (55-layer and 43-layer encoders) at three resolutions
(288px, 352px, and 480px), and compare the effect of intro-
ducing LMP and the low/high-resolution cascade into our
model design.

×

4. Workshop Demo

For our submission we propose both a web browser-
based and iOS demo of our virtual nail polish try-on tech-
nology. We will provide an iOS tablet device for users to try
the virtual nail try-on application in person, as well as a web
link at https://ola.modiface.com/nailsweb/
cvpr2019demo (best viewed on a mobile device using
Safari on iOS, Chrome or Firefox on Android) that users
can access with their own devices and use the technology
by enabling camera access in the browser (“https” preﬁx re-
quired).

5. Conclusion

We presented a ﬁngernail tracking system that runs in
realtime on both iOS and web platforms, and enables pixel-
accurate ﬁngernail predictions at up to 640
480 resolution.
We also proposed a postprocessing and rendering algorithm
that integrates with our model to produce realistic nail pol-
ish rendering effects.

×

References

[1] Samuel Rota Bul`o, Gerhard Neuhold,

and Peter
Kontschieder.
Loss max-pooling for semantic image
segmentation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 1, 2

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A Large-Scale Hierarchical Image Database.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2009. 2

[3] Golnaz Ghiasi and Charless C. Fowlkes. Laplacian re-
construction and reﬁnement for semantic segmentation. In
ECCV, 2016. 2

[4] C. Grana, D. Borghesani, and R. Cucchiara. Optimized
block-based connected components labeling with decision
trees. IEEE Transactions on Image Processing, 2010. 3
[5] Forrest N. Iandola, Song Han, Matthew W. Moskewicz,
Khalid Ashraf, William J. Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer parame-
ters and <0.5mb model size. arXiv:1602.07360, 2016. 1
[6] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017. 3

[7] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
In The IEEE Conference
residuals and linear bottlenecks.
on Computer Vision and Pattern Recognition (CVPR), June
2018. 1, 2

[8] Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann
Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai,
Eric Nielsen, David Soergel, Stan Bileschi, Michael Terry,
Charles Nicholson, Sandeep N. Gupta, Sarah Sirajuddin, D.
Sculley, Rajat Monga, Greg Corrado, Fernanda B. Vi´egas,
and Martin Wattenberg. Tensorﬂow.js: Machine learning
for the web and beyond. arXiv preprint arXiv:1901.05350,
2019. 1, 3

[9] Robert J Wang, Xiang Li, and Charles X Ling. Pelee: A real-
time object detection system on mobile devices. In Advances
in Neural Information Processing Systems 31, 2018. 1
[10] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 1
[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping
Shi, and Jiaya Jia. Icnet for real-time semantic segmentation
on high-resolution images. In ECCV, 2018. 1, 2

[12] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V.
Le. Learning transferable architectures for scalable image
In The IEEE Conference on Computer Vision
recognition.
and Pattern Recognition (CVPR), 2018. 1

