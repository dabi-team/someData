A Study of Preference and Comfort
for Users Immersed in a Telepresence Robot

Adhi Widagdo, Markku Suomalainen, Basak Sakcak, Katherine J. Mimnaugh,
Juho Kalliokoski, Alexis P. Chambers, Timo Ojala, and Steven M. LaValle

2
2
0
2

r
a

M
5

]

C
H
.
s
c
[

1
v
9
9
6
2
0
.
3
0
2
2
:
v
i
X
r
a

Abstract— In this paper, we show that unwinding the rota-
tions of a user immersed in a telepresence robot is preferred
and may increase the feeling of presence or “being there”.
By immersive telepresence, we mean a scenario where a user
wearing a head-mounted display embodies a mobile robot
equipped with a 360° camera in another location, such that
the user can move the robot and communicate with people
around it. By unwinding the rotations, the user never perceives
rotational motion through the head-mounted display while
staying stationary, avoiding sensory mismatch which causes
a major part of VR sickness. We performed a user study
(N=32) on a Dolly mobile robot platform, mimicking an earlier
similar study done in simulation. Unlike the simulated study,
in this study there is no signiﬁcant difference in the VR
sickness suffered by the participants, or the condition they ﬁnd
more comfortable (unwinding or automatic rotations). However,
participants still prefer the unwinding condition, and they judge
it to render a stronger feeling of presence, a major piece in
natural communication. We show that participants aboard a
real telepresence robot perceive distances similarly suitable as
in simulation, presenting further evidence on the applicability
of VR as a research platform for robotics and human-robot
interaction.

I. INTRODUCTION

Telepresence robots are a rising trend in robotics, with
commercial companies such as GoBe and Double marketing
their robots as tools for hybrid meetings. However, these
commercial robots do not make users feel like they really
were at the robot’s location, due to, for example, the inability
to look around: the robot should do a rotation to just explore
the view. Additionally, this lack of immersiveness leads to a
lack of presence, the feeling of ”being there” [1], which is
essential for a user to communicate naturally with the people
around the robot.

Virtual reality has emerged in the robotics ﬁeld to com-
plement the trend of hybrid meetings, which have recently
increased signiﬁcantly. The advantage offered by integrated
robotics-VR, such as shown in Fig. 1,
is the increased
immersion of the Head-Mounted Display (HMD), which
can make the user really feel present. With the use of a
360° camera capable of capturing omnidirectional videos,
instead of a standard camera used in the current commercial
telepresence robots, the experience will be more immersive.

*This work was supported by a European Research Council Advanced
Grant (ERC AdG, ILLUSIVE: Foundations of Perception Engineering,
101020977), Academy of Finland (projects PERCEPT 322637, CHiMP
342556), and Business Finland (project HUMOR 3656/31/2019).

Authors are with Center of Ubiquitous Computing, Faculty of Informa-
tion Technology and Electrical Engineering, University of Oulu, Finland.
{name.surname}@oulu.fi

Fig. 1: Robotic telepresence: a user wearing an HMD can
embody a mobile robot equipped with a 360° camera. Left:
Dolly mobile robot platform and a 360° camera on a boom,
used in this study. Right: the physical setup of our user study.

However, using an HMD can create side effects, namely
VR sickness, with symptoms such as dizziness, headache,
eye strain, blurred vision, vomiting, and nausea [2]. This
is caused by a sensory mismatch between vision and the
vestibular organ; when the user’s eye informs the brain about
acceleration of the body, but the acceleration sensor in the
form of the utricle and saccule in the otolith organ does
not, and the result is a conﬂict with the information received
by the brain [3]. In addition, feeling the immersiveness can
also cause discomfort if objects are too close, or if rotations
of the robot are surprising, issues that are not present in a
conventional telepresence scenario [4], [5].

In this paper, we present a user study on immersive telep-
resence where participants embodied a real mobile robot that
was equipped with a 360° camera and traversed a predeﬁned
path at a university campus. We explore unwinding the
rotations of the user’s viewpoint, which has been shown to
reduce VR sickness in a study based on a simulation [6]. This
method negates the rotation of the user’s view relative to the
robot, so that the direction of the view is determined solely
by the user. We test the same three hypotheses that were
conﬁrmed about unwinding rotations in the simulation study.
However, this time only the user preference of unwinding the
rotations is conﬁrmed; there is no statistically signiﬁcant dif-
ference in user comfort or Simulator Sickness Questionnaire
(SSQ) measures, a commonly used questionnaire for VR
sickness. We hypothesize, based on answers to open-ended
questions, that these different results are more likely due to
simply a different sample than other issues such as video
quality or the vibrations of the real mobile robot missing
from the simulation study, even though the latter cannot
be ruled out. These ﬁndings call for further studies with
more participants with both simulated and real mobile robots.
Additionally, we observe participants feeling present enough

 
 
 
 
 
 
that 11 out of 32 respond (with either words or gestures)
to people in the video waving or talking to the robot, even
though the participants are told it is a recording; there is
also a perceived difference in how present participants feel
between the unwinding and coupled rotations conditions.
Finally, we conﬁrm that users perceive the distances to
passing objects and the speed of the real robot similarly as
in the simulation study, giving further justiﬁcation to study
telepresence and HRI with simulations in the future.

results received in VR be generalized to the real world?
There are a growing number of studies on the topic, such
as the comparison of proxemics between robots and humans
in VR and in the real world [26], robot navigation among
people in VR [27], and designing robots for human-robot
interaction in VR [28]. In this paper, we continue this trend
by exploring whether participants aboard a real telepresence
robot perceive objects or people similarly or differently (too
close or too far) to a previous simulation study.

II. RELATED WORK

III. UNWINDING ROTATIONS

Most of the previous research investigating teleoperation
and telepresence, the idea of embodying a robot equipped
with a camera, used standard screen-based technologies,
before consumer grade VR headsets and 5G networks [7].
There is an unprecedented opportunity to leverage these tech-
nologies to bring telepresence to a new level [8] [9]. The key
improvement that an HMD can bring to telepresence is in-
creased immersion, which in turn can lead to greater feelings
of presence [10]. Standard telepresence robots equipped with
2D screens provide similar improvement over conventional
videoconferencing by enhancing interaction and navigation
tasks for remote users [11]; however, the feeling is still closer
to video conferencing than really being there. Use cases
for telepresence robots are, for example, distance education
[12], telemedical consultation [13] and all kinds of remote
collaborative work [14], [15].

While a lack of presence can reduce task performance
[16], wider ﬁeld of view improves awareness and immersion
for the user of telepresence robots as well as tasks efﬁciency
[17]. Using an HMD in telepresence for increased immersion
has been reported to yield better results in many application
domains including education [12], collaboration through con-
versation [18], scenarios for earthquakes [19], and advanced
navigation for robots [20]. However, for immersive HMD-
based telepresence to reach a wider audience, the comfort of
the HMD user and reducing VR sickness needs to be studied
further.

VR sickness typically occurs due to a mismatch between
the real-world and perceived accelerations in an HMD [21].
Also, issues with hardware and content such as repeated mo-
tions often lead to nausea [22]. Several approaches have been
introduced for reducing motion sickness, such as minimizing
the optical ﬂow to slow down motions [2], adding a rest
frame to the user’s view [23], or using a narrow ﬁeld of view
[24]. However, an important aspect is to consider criteria
based on human perception [25]. In multimedia, quality of
experience (QoE) measures have been developed as system
is worthwhile to
performance criteria [22]. Therefore,
explore the design of robot motions that would reduce
accelerations mismatches, and how such motions could be
realized for robots used in immersive telepresence.

it

Besides telepresence, VR is becoming a popular technol-
ogy in robotics research in general, especially in human-
robot interaction, as VR enables cost-effective simulations
of complex real world use cases. However, the applicability
of VR simulations is an open question; how much can the

In [6], we introduced an approach for decoupling the user’s
viewpoint from the rotations of the robot, which we called
unwinding rotations. We present our approach applicable for
mobile robots here for completeness; details on applying the
method to robots with more complex motions than on a plane
can be found from [6].

We consider a mobile robot moving in a two-dimensional
plane. The discrete-time kinematic model of the robot is
given as

xk+1 = xk + vk cos(θk)dt
yk+1 = yk + vk sin(θk)dt
θk+1 = θk + wkdt,

(1)

in which (xk, yk) is the position and θk is the orientation of
the robot at time instance kdt with respect to an absolute ref-
erence frame and vk, wk are the control inputs corresponding
to the linear and angular velocity with respect to a robot-
ﬁxed reference frame. We assume that the control input is
kept constant for time window t ∈ [kdt, (k + 1)dt].

Suppose that the robot orientation θk can be estimated and
denote its estimate by ˆθk. Let R( ˆθk) ∈ SO(2) be the rotation
matrix corresponding to the robot orientation. Then, we can
deﬁne unwinding rotations as rotating the camera frame such
that any point pc ∈ R3 represented in the camera frame is
related to the point p(cid:48)
c, whose coordinates are expressed in
the rotated camera frame, through

p(cid:48)
c =

(cid:20)R( ˆθk)T
01×2

(cid:21)

02×1
1

pc.

(2)

This operation negates the rotation of the viewpoint caused
by the robot rotation. Thus, the users need to rotate them-
selves in order to face the direction that the robot is facing
or face the same direction in the virtual environment for the
whole motion.

IV. CONDITIONS AND HYPOTHESES

In the unwound rotations (UR) condition,

the user’s
viewpoint does not change when the robot changes direc-
tion. In the coupled rotations (CR) condition, the user’s
viewpoint rotates when the robot rotates.

We pre-registed the following three hypotheses in the
Open Science Foundation (OSF, https://osf.io/
tw7ef). They are identical to the conﬁrmed hypotheses
of a previous study with a simulated robot
in a virtual
environment [6] (https://osf.io/eks6t), as the goal
of this study is to test the hypotheses with a real robot.

maximum rotating speed of 0.7 rad/s and a maximum
rotational acceleration of 2.5 rad/s2. The length of the path
traversed by the robot is 61 meters, the route is shown in
Fig. 2; screenshots from the video are presented in Fig. 3.
The minimum distance to the obstacles is 0.95 m and the
minimum distance to the people is 1.2 m.

B. 360° Video

We used a commercial Kandao QooCam 8K camera to
record 360° video as the robot moves in the environment. The
camera was mounted on a vertical boom so that the center
of the camera lenses was 155 cm above ﬂoor level. The
camera captured a video of 7680x3840 pixels in resolution
at 30fps with H.265 encoding. The duration of the video
was 2 minutes 6 seconds, during which the robot rotated
a total of 648.7°. This camera has a built-in IMU sensor,
making it easy to estimate ˆθk for (2). However, for earlier
tests we also implemented unwinding rotations on a camera
which did not have a built-in IMU; for such cameras the key
point is the synchronization of the IMU and camera data.
We also note that in these studies the unwinding rotations
was performed as post-processing; adapting the method for
online processing is part of future work once we establish
that unwinding rotations is favored by the users.

The original video had equirectangular projection for the
mapping between 2D textures and 3D direction vectors for
tilling the surface of a sphere [29]. However, such a sphere
suffers from seam-like artifacts commonly occurring around
the poles [30]. We argue that seeing such seam-like artifacts
would decrease immersiveness. To remove the seams, we
constructed a uniform spherical grid that is originally trans-
formed from a cube (cube-sphere) [31]. First, video textures
from the equirectangular mapping are reconstructed into the
cube mapping by shaders at the GPU level. Then, the video’s
UV coordinates are uniformized with the cube-sphere’s UV
mapping, using coordinates generated by the cube-sphere
mesh generator, so that the image will be fully projected on
the geometry. We render the video under the Unity engine
on a desktop PC furnished with an Intel Xeon 3.80 GHz
CPU, 32 GB RAM and NVidia Quadro RTX6000 GPU, and
transmit the video to an Oculus Quest 2 HMD via a link
cable.

C. Procedure

We counterbalanced both the order of the videos (which
video is played ﬁrst) and gender by presenting the two videos
of the UR and CR conditions equally for both men and
women as the ﬁrst video. Each participant was asked to read
the privacy notice and study information, and to sign the
consent form. Before starting the experiment, the researcher
did pre-screening such as asking whether the participant has
a headache or feels nauseous (rescheduling was an option
for a participant if they felt unwell). After that, they were
asked to sit and adjust
the chair’s height on the swivel
chair so that they could rotate around easily (see Fig. 1,
right). Next, the researcher gave the instructions and told the
participant about the brief experiment information, including

Fig. 2: A top-down view of the environment tested trajecto-
ries.

H1: Less VR sickness in UR condition as indicated by lower

total weighted SSQ score.

H2: The UR condition is more comfortable as indicated by
asking directly which was more comfortable (forced
choice).

H3: The UR condition is preferred as indicated by asking
directly which the user preferred (forced choice).

According to VR sickness theory, that a mismatch between
vestibular system and vision induces sickness, we predicted
in H1 that there will be less sensory conﬂict if the user
performs their own physical rotations. H2 and H3 were under
the general assumption that people avoid sickness, hence UR
is most likely to be preferred and comfortable.

V. METHODOLOGY

A. Mobile Robot

We used a wheeled mobile robot Probot Dolly (Fig. 1,
left) equipped with a 360° camera mounted on a vertical
boom. The dimensions of the robot are 60 (L) x 40 (W)
x 20 (H) cm. The robot uses 2 active wheels and 4 omni
wheels so that the robot’s movement is based on a differential
drive mobile robot (DDMR). The robot is equipped with
a laser scanner and an odometer to move according to the
direction of the navigation, which is controlled by a motion
planning algorithm. The robot uses a client-server software
architecture where a laptop hosts Robot Operating System
(ROS2) galactic as the main operating system and as the
server. The main control board of the robot is a Raspberry
Pi 4 and it acts as a client. The client and the server
communicate over a WiFi connection using the TCP/IP
protocol. A laser connected to Ethernet port of the main
control board is used for mapping (SLAM) and obstacle
avoidance. The actuator integrated with the HAL sensor is
driven by the VESC controller which is connected to the
CAN-bus so that it can function as an odometer.

In ROS2, we used waypoint based navigation and dynamic
window (DWB) approach as controllers to follow the path.
The robot moved inside the building of the University
of Oulu campus with a maximum speed of 0.7 m/s, a

(a)

(b)

(c)

(d)

Fig. 3: Screenshots from the environment from the user’s view: (a) a person passing the robot; (b) A VR user watching their
own body through a mirror’s reﬂection; (c) People standing in the midway; (d) a person greeting and asking a VR user.

how to put on the HMD. Afterwards, the researcher started
the video, calibrated the participant position, and asked them
to rotate around while wearing the HMD to make sure they
understand they can easily rotate with it; they were however
not prompted to rotate themselves during the video. After
the participant conﬁrmed that they are ready to start, then the
video of the robot moving started playing. During the video,
the participant’s head orientation was recorded. After the
video ﬁnished, participants were asked to take off the headset
and ﬁll out
the questionnaire regarding their experience.
The same procedure was applied for the second video. The
participants were compensated with a 20 C Amazon gift card.
We used recommended precautions for COVID-19 during the
experiment such as wearing a mask, keeping a safe distance
from the subject and sanitizing the devices (including the
environment).

D. Measures

In terms of questionnaires, we measured sickness with the
SSQ [32] which presented 16 possible sickness symptoms,
rated on a scale of none (0) to severe (3). The SSQ total
score and subscales are calculated by weighting the answers
for a maximum score of 236. We used the SUS questionnaire
[33], [34] to attempt measuring their feeling of presence. The
experience questionnaires also included Likert-scale (ranging
1-7) questions and open-ended questions for reasoning in
there were more
some choices. After the second video,
questions in the shape of forced-choice questions to compare
the two videos in regards to choosing the more comfortable,
preferred, and more intuitive condition for the participant.
Furthermore, we also asked questions (in Likert-scale) about
the feeling towards distance to walls, distance to humans,
and linear speed of the robot. In the end,
the VR and
gaming experience, and demographics were also queried in
the questionnaire.

E. Participants

twice a year, 15.6% once or twice a month, and 3.1% once
or twice a week. Meanwhile, they also responded of how
often they play computer games: 15.6% never, 25% once or
just a couple of times ever, 25% once or twice a year, 9.4%
once or twice a month, 12.5% once or twice a week, 9.4%
several times a week, and 3.1% everyday.

VI. RESULTS

We ran data analysis in SPSS with the Wilcoxon signed-
rank test to ﬁnd whether there was a difference in the total
weighted SSQ score under two different video conditions
(UR and CR). An exact binomial test with exact Clopper-
Pearson 95% CI was used to determine if a greater proportion
of participants were more comfortable and preferred one of
the videos when shown UR compared to the CR conditions.
The SUS score was calculated as the number of SUS
questionnaire items given a 6 or 7 rating by a participant,
resulting in a score that could range from 0 to 6 for each
participant [33], [34]. We performed a Wilcoxon signed-rank
test (one-sided) to test for greater SUS scores

A. Conﬁrmatory Results

A Wilcoxon signed-ranks test showed that UR did not
elicit a statistically signiﬁcant median decrease in the total
weighted SSQ score (Z = −0.144, p = .886) with median
14.96 and 16.83 for the CR.

While 21 out of 32 participants felt more comfortable
in UR (65, 6%) than the CR, 23 participants preferred UR
(71, 9%) when they responded to the forced-choice questions:
“Which of the two videos is more comfortable?” and “Which
of the two videos do you prefer?” (see Fig.4). A binomial test
with exact Clopper-Pearson found that the condition UR was
not signiﬁcantly more comfortable, had 95% CI of 46, 8% to
81, 4%, p = .112 and the preference in UR was statistically
signiﬁcant, had 95% CI of 53.3% to 86.3%, p = .022.

We collected the data from 32 participants similarly to the
prior study [6] consisting of equal numbers of both males and
females. Their age ranged from 20 to 33 years old and they
reported having normal or corrected-to-normal vision. There
were no reports of color blindness. The responses of how
often they use VR systems were as such: 18.8% said never,
53.1% once or just a couple of times ever, 9.4% once or

B. Exploratory Results

1) Quantitative Data: A Wilcoxon signed-rank test indi-
cated no statistically signiﬁcant difference in the Likert-scale
comfort ratings Z = −1.082, p = .279 with Mean = 5.03 for
UR and (Mean = 4.78) for CR.

In the SUS scores, UR (Mean = 4.96) and CR (Mean =
4.84) did not have a statistically signiﬁcant difference in the

Fig. 4: The distributions of responses to the questions re-
garding comfort, preference, intuitiveness, and presence.

Value
Distance to walls
min. 0.95 m
Distance to people min. 1.2 m
Speed

max. 0.7 m/s

Median
4
4
4

Variance
1.29
1.19
0.547

TABLE I: The values of certain attributes of the path, and the
median and variance of 7-point Likert scale subjective opinions (1
meaning too small/too close).

sense of being in the remote environment, Z = −.510, p =
.610.

In addition to the pre-registered hypotheses, we asked
two further forced-choice questions: “Which of the two
experiences feels more intuitive for you?” and “Thinking
back to both of the experiences, which one gave a better
sense of being in the robot’s location?”. A binomial test with
exact Clopper-Pearson showed that the UR was signiﬁcantly
more intuitive, 95% CI of 53, 3% to 86.3%, p = .022 and
elicited signiﬁcantly better presence, 95% CI of 56, 6% to
88.5%, p = .008 (see Fig. 4).

We asked Likert-scale questions about how the participants
perceived the distance to walls, distance to humans and the
speed during moving with the robot. The results shown in
Table I indicated that the speed and distance to people and
objects were considered appropriate on a 7-point Likert scale.
2) Qualitative Data: We analyzed the open-ended data
using the inductive approach on thematic analysis [35]. The
accumulated responses for the open-ended question about
why participants found either video more comfortable can be
seen in Fig. 5 in which the most frequent keywords towards
open-ended questions of UR as chosen comfortable video
were as follows: smooth (3) (for example, “The movement
was smoother, both the straight-going parts, and the turning
parts”), less sickening (3)(“I did not feel nauseous at all
after the ﬁrst video, after the second one I felt a little
nauseous”), and natural (2) (“Moving with the robot felt
more natural”).

On the contrary, the participants who chose the CR com-
mented: less sickening (3) (“more static, so less motion
sickness. Second video changing position/ changing gaze
direction caused discomfort even though it was more engag-
ing”); less self-rotation (2) (“The comfortable is quite the
same, but I choose the ﬁrst one since I dont need to turn the
chair. Also, the second one can provide a reversed walk that
is not natural for the speed”); and view follows (2) (“I did

Fig. 5: The frequently found codes from the question “Please
explain why that video is more comfortable” which was
asked after the participant watched both videos.

Fig. 6: Frequently found codes for question “Please explain
why you prefer that video”.

not require to turn to see where the robot was going”).

The open-ended question on the video preference was
commented, shown in Fig. 6: In UR there was smooth (4),
“It was much smoother, especially the turning. I did not fully
appreciate the ﬁrst video, until after I watched the second
one”; control over viewpoint (5), “maybe more engagement
with what I was seeing because I had to move to change
feel uncomfortable at
view”; comfortable (3), “It didn’t
all and I could relax”, natural (3), ”it is quite real and
comfortable”; less blurry (3), “The video seemed clearer,
and the view was less blurry even while robot was making
turns”; less sickening (2), “It didn’t feel uncomfortable at
all and I could relax”; and sense of presence (2), “I think
the ﬁrst video gave me a better sense of actually being in
that place, but I don’t know if that’s speciﬁcally because of
a difference between the videos themselves or it is due to
that being my ﬁrst contact with the video”.

In contrast,

the CR had comments such as sense of
presence (1), “It feel [sic] like the two videos were honestly
the same, but I chose the ﬁrst one here, because it made me
believe I was actually there more (as in I didn’t think how
I was sitting in the chair and so on)” and view follows(2),
“The ﬁrst video was less disorienting and rather easier to
maneuver in the space. The second video felt as if I was
being pushed and not in line with my natural progression
(of movement)”.

During the experiments, we observed the participants’
reactions. We created the events (interaction to the robot)
in the video, shown in Fig. 2. The ﬁrst event was when a

lady walked past the robot, secondly a man waved his hand to
the robot, thirdly the talking people standing in the middle of
the path and a man crossed in front of the robot, and lastly
a man greeted, waved hand and asked towards the robot.
Surprisingly, there were some participants who responded
back, especially towards the second and fourth events; in
total 11 of the 32 participants responded to either of both
people in the video communicating with them, either with
gesture or even by speaking. There were also participants
who answered the greeting at the last events.

VII. DISCUSSION

From the pre-registered hypotheses conﬁrmed in a simu-
lation study [6], only H3, namely that users would prefer
the unwinding, held: there was no statistically signiﬁcant
difference in either the SSQ scores or the forced-choice
question of comfort. This is surprising, since there is not
supposed to be a lot of difference between real 360° video
and a simulated one when it comes to VR sickness. It is
possible that the use of the real robot brings in other causes
of VR sickness, such as vibrations; however, this was not
found in the open-ended answers, since the word ”smooth”
seemed most often to refer to the turns. Moreover, the mean
SSQ value (16.83) for CR was exactly the same as in the
simulation study, but the UR SSQ value was higher than
in the simulation study (14.96 and 9.35). As VR sickness
is typically caused by multiple sources [2], it is likely that
increased vibrations would also have increased the SSQ
scores of the CR conditions. We also note that there was a
difference in the total rotation made by the robot: whereas in
the simulation the robot rotated a total of 438° , in this study
the total amount of rotation was 648.7°. This was mainly due
to the small corrections a robot needs to constantly make
while moving; however, it is unclear how this could have
made the observed difference in SSQ scores between the
simulated study and this study.

The answers to open-ended questions reveal interesting
differences between the participants of this study and the
simulated study, which could partly explain the differences
in SSQ scores. Whereas in the simulated study only one
person commented annoyance towards having to rotate the
chair, here 4 people gave that comment (the participants
saying ”view follows” (“I did not need to turn when the
robot turned. It was easy for me.”) and “less self-rotations”
were different participants). Moreover, it is surprising to
observe three people consider CR less sickening, but these
answers did not reveal any details of why. Thus, for comfort,
it seems that simply the participants had different preferences
when it comes to self-rotation than in the simulated study.
This does not explain the non-signiﬁcant difference in SSQ
scores; however, as there is partial evidence from these
studies together pointing towards unwinding reducing VR
sickness, and the conﬁrmed preference from both studies, we
can conclude that unwinding does have clear advantages over
coupled rotations also in a real-world telepresence scenario.
Although in a real-world deployment, it may be beneﬁcial to

allow users to choose which condition to use, since it seems
personal preferences play a role in this choice.

The large difference in forced-choice answers to presence
(”thinking back to both of the experiences, which one gave
a better sense of being in the robot’s location?” 24 for UR,
8 for CR, p=.008) was a surprise: the simulated study [6]
showed promise towards this direction, which prompted us
to explicitly measure it. Again, this was hinted directly in
a few open-ended answers, such as “felt like I was more
in charge of the situation/ what I was seeing and not just a
passive observer”. This result is in agreement with the theory
of presence, since sensorimotor contingencies, meaning that
the user’s physical actions are matched visually in the user’s
HMD view, are shown to increase the feeling of presence
[1]. Whereas the nonsigniﬁcant difference in the SUS scores
limits overemphasizing this result, remembering that
the
increase in presence was the main reason for using an HMD
in the ﬁrst place, even a small increase with other positive
effects is welcome.

the fact

When not comparing the conditions,

that 11
out of 32 participants responded to greetings in the video
is a positive sign of presence; the responses were despite
the fact that we told the participants they are watching a
recording, which also prompted a few participants to mention
afterwards that they felt silly having responded. As this kind
of behavior is exactly the reason to use an HMD for this task,
we can say that the results are still very promising. The next
step is making live telepresence comparison between HMD-
based telepresence and conventional telepresence.

Finally, after this study, we can conﬁrm that the sugges-
tions made for the robot’s speed, and distances to people
and objects in the simulation study, can be conﬁrmed. The
minimal distance to inanimate objects in this study was
almost equal when compared against the simulation study
(0.95m and 0.9m), and the result was the same, median
score 4 in a 7-point Likert scale. The distance to people
was slightly more in this study (1.2m and 1m), and this
was reﬂected in the results; whereas in the simulation study
the person was considered to be slightly too close at 1m
(Median=3), in this study with 1.2m was considered suitable.
Whereas this is not an exact comparison, we can conclude
that doing telepresence research in VR is a good substitute,
at least considering how the person aboard the robot sees the
world and its distances.

VIII. CONCLUSIONS

In this study, we evaluated whether the results shown
for a simulated telepresence robot
in [6] also hold true
for a telepresence robot with a 360° camera; namely, that
unwinding the rotations of the user, such that the user never
sees rotational motion but must rotate themselves along with
the robot, are preferred, more comfortable, and induce less
VR sickness. However, this study found the same results
only for preference as there was no statistically signiﬁcant
difference between the conditions in sickness or comfort.
We believe the most likely reason for this difference was
simply a different participant population, since there were

several such hints in the open-ended questions. A slightly
less likely explanation is that vibrations in the video feed
caused by the robot motion increased the sickness scores,
but this should have affected sickness in both conditions. We
do believe, however, that this evidence shows the promise of
unwinding, and also promise of virtual environments as a
research platform for immersive telepresence. Future work
will include a live immersive telepresence scenario, as well
as testing unwinding on different, semi-autonomous steering
methods. In the long-term, future research to enable two-
way communication is merited as HMD-based telepresence
robots should be able to present the remote user’s face to
the people around the robot, similarly as current conven-
tional telepresence robots [36] do. There is already research
towards showing a person’s face who is wearing an HMD
[37]; thus, one should bring this technology to HMD-based
telepresence robots to fully unlock the power of immersive
telepresence.

REFERENCES

[1] M. Slater, “Place illusion and plausibility can lead to realistic be-
haviour in immersive virtual environments,” Philosophical Transac-
tions of the Royal Society B: Biological Sciences, vol. 364, pp. 3549–
3557, Dec. 2009.

[2] J. J. LaViola Jr, “A discussion of cybersickness in virtual environ-

ments,” ACM Sigchi Bulletin, vol. 32, no. 1, pp. 47–56, 2000.
[3] S. M. LaValle, Virtual reality. Cambridge University Press, 2021.
[4] M. Suomalainen, K. J. Mimnaugh, I. Becerra, E. Lozano, R. Murrieta-
Cid, and S. M. LaValle, “Comfort and sickness while virtually aboard
an autonomous telepresence robot,” in International Conference on
Virtual Reality and Mixed Reality, pp. 3–24, Springer, 2021.

[5] K. J. Mimnaugh, M. Suomalainen, I. Becerra, E. Lozano, R. Murrieta-
Cid, and S. M. LaValle, “Analysis of user preferences for robot
motions in immersive telepresence,” in 2021 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 4252–4259,
IEEE.

[6] M. Suomalainen, B. Sakcak, A. Widagdo, J. Kalliokoski, K. J.
Mimnaugh, A. P. Chambers, T. Ojala, and S. M. LaValle, “Unwinding
rotations improves user comfort with immersive telepresence robots,”
in Accepted for publication in the 2022 ACM/IEEE International
Conference on Human-Robot Interaction (HRI), 2022.

[7] A. Kheddar, R. Chellali, and P. Coiffet, “Virtual environment–assisted
teleoperation,” in Handbook of Virtual Environments, pp. 1102–1136,
CRC Press, 2014.

[8] E. Bastug, M. Bennis, M. M´edard, and M. Debbah, “Toward intercon-
nected virtual reality: Opportunities, challenges, and enablers,” IEEE
Communications Magazine, vol. 55, no. 6, pp. 110–117, 2017.
[9] T. Aykut, C. Zou, J. Xu, D. Van Opdenbosch, and E. Steinbach, “A
delay compensation approach for pan-tilt-unit-based stereoscopic 360
degree telepresence systems using head motion prediction,” in 2018
IEEE International Conference on Robotics and Automation (ICRA),
pp. 3323–3330, IEEE, 2018.

[10] R. Skarbez, F. P. Brooks, Jr, and M. C. Whitton, “A survey of presence
and related concepts,” ACM Computing Surveys (CSUR), vol. 50, no. 6,
pp. 1–39, 2017.

[11] I. Rae, B. Mutlu, and L. Takayama, “Bodies in motion: mobility,
presence, and task awareness in telepresence,” in Proceedings of the
ACM conference on Human factors in computing systems (CHI),
pp. 2153–2162, 2014.

[12] J. Botev and F. J. Rodr´ıguez Lera, “Immersive robotic telepresence for
remote educational scenarios,” Sustainability, vol. 13, no. 9, p. 4717,
2021.

[13] K. A. R. Carranza, N. J. B. Day, L. M. S. Lin, A. R. Ponce,
W. R. O. Reyes, A. C. Abad, and R. G. Baldovino, “Akibot: A
telepresence robot for medical teleconsultation,” in 2018 IEEE 10th
International Conference on Humanoid, Nanotechnology, Information
Technology, Communication and Control, Environment and Manage-
ment (HNICEM), pp. 1–4, IEEE, 2018.

[14] M. K. Lee and L. Takayama, “Now, i have a body: Uses and social
norms for mobile remote presence in the workplace,” in Proceedings
of the SIGCHI conference on human factors in computing systems,
pp. 33–42, ACM, 2011.

[15] G. Venolia, J. Tang, R. Cervantes, S. Bly, G. Robertson, B. Lee,
and K. Inkpen, “Embodied social proxy: Mediating interpersonal
connection in hub-and-satellite teams,” in Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’10,
p. 1049–1058, Association for Computing Machinery, 2010.

[16] B. Stoll, S. Reig, L. He, I. Kaplan, M. F. Jung, and S. R. Fussell,
“Wait, can you move the robot? examining telepresence robot use in
collaborative teams,” in Proceedings of the 2018 ACM/IEEE Interna-
tional Conference on Human-Robot Interaction, pp. 14–22, 2018.
[17] S. Johnson, I. Rae, B. Mutlu, and L. Takayama, “Can you see me
now? how ﬁeld of view affects collaboration in robotic telepresence,”
in Proceedings of the 33rd annual acm conference on human factors
in computing systems, pp. 2397–2406, 2015.

[18] J. Du, H. M. Do, and W. Sheng, “Human–robot collaborative control
in a virtual-reality-based telepresence system,” International Journal
of Social Robotics, pp. 1–12, 2020.

[19] F. Negrello, A. Settimi, D. Caporale, G. Lentini, M. Poggiani,
D. Kanoulas, L. Muratore, E. Luberto, G. Santaera, L. Ciarleglio,
et al., “Humanoids at work: The walk-man robot in a postearthquake
scenario,” IEEE Robotics & Automation Magazine, vol. 25, no. 3,
pp. 8–22, 2018.

[20] G. Baker, T. Bridgwater, P. Bremner, and M. Giuliani, “Towards an
immersive user interface for waypoint navigation of a mobile robot,”
in International Workshop on Virtual, Augmented and Mixed Reality
for Human-Robot Interaction (VAM-HRI), 2020.

[21] L. Rebenitsch and C. Owen, “Review on cybersickness in applications
and visual displays,” Virtual Reality, vol. 20, no. 2, pp. 101–125, 2016.
[22] B. Keshavarz, H. Hecht, and B. Lawson, “Visually induced motion
sickness: characteristics, causes, and countermeasures,” Handbook
of virtual environments: Design, implementation, and applications,
pp. 648–697, 2014.

[23] Z. Cao, J. Jerald, and R. Kopper, “Visually-induced motion sickness
reduction via static and dynamic rest frames,” in 2018 IEEE Confer-
ence on Virtual Reality and 3D User Interfaces (VR), pp. 105–112,
IEEE, 2018.

[24] J. Teixeira and S. Palmisano, “Effects of dynamic ﬁeld-of-view re-
striction on cybersickness and presence in hmd-based virtual reality,”
Virtual Reality, pp. 1–13, 2020.

[25] M. Urvoy, M. Barkowsky, and P. Le Callet, “How visual fatigue and
discomfort impact 3d-tv quality of experience: a comprehensive review
of technological, psychophysical, and psychological factors,” annals of
telecommunications-annales des t´el´ecommunications, vol. 68, no. 11,
pp. 641–655, 2013.

[26] R. Li, M. van Almkerk, S. van Waveren, E. Carter, and I. Leite,
“Comparing human-robot proxemics between virtual reality and the
real world,” in 2019 14th ACM/IEEE International Conference on
Human-Robot Interaction (HRI), pp. 431–439, IEEE, 2019.

[27] F. Grzeskowiak, M. Babel, J. Bruneau, and J. Pettr´e, “Toward virtual
reality-based evaluation of robot navigation among people,” in 2020
IEEE Conference on Virtual Reality and 3D User Interfaces (VR),
pp. 766–774, IEEE, 2020.

[28] J. Zamﬁrescu-Pereira, D. Sirkin, D. Goedicke, R. LC, N. Friedman,
I. Mandel, N. Martelaro, and W. Ju, “Fake it to make it: Exploratory
prototyping in hri,” in Companion of the 2021 ACM/IEEE Interna-
tional Conference on Human-Robot Interaction, pp. 19–28, 2021.
[29] M. Zucker and Y. Higashi, “Cube-to-sphere projections for procedural
texturing and beyond,” Journal of Computer Graphics Techniques Vol,
vol. 7, no. 2, 2018.

[30] R. G. d. A. Azevedo, N. Birkbeck, F. De Simone, I. Janatra,
B. Adsumilli, and P. Frossard, “Visual distortions in 360-degree
videos,” arXiv preprint arXiv:1901.01848, 2019.

[31] D. Ros¸ca and G. Plonka, “Uniform spherical grids via equal area
projection from the cube to the sphere,” Journal of Computational
and Applied Mathematics, vol. 236, no. 6, pp. 1033–1041, 2011.
[32] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal, “Sim-
ulator sickness questionnaire: An enhanced method for quantifying
simulator sickness,” The International Journal of Aviation Psychology,
vol. 3, no. 3, pp. 203–220, 1993.

[33] M. Slater, M. Usoh, and A. Steed, “Depth of presence in virtual en-
vironments,” Presence: Teleoperators & Virtual Environments, vol. 3,
no. 2, pp. 130–144, 1994.

[34] M. Usoh, E. Catena, S. Arman, and M. Slater, “Using Presence
Questionnaires in Reality,” Presence: Teleoperators and Virtual En-
vironments, vol. 9, pp. 497–503, 10 2000.

[35] M. Q. Patton, “Qualitative research,” Encyclopedia of Statistics in

Behavioral Science, 2005.

[36] A. Kristoffersson, S. Coradeschi, and A. Loutﬁ, “A review of mo-

bile robotic telepresence,” Advances in Human-Computer Interaction,
vol. 2013, 2013.

[37] N. Matsuda, B. Wheelwright, J. Hegland, and D. Lanman, “Reverse
pass-through vr,” in Special Interest Group on Computer Graphics and
Interactive Techniques Conference Emerging Technologies, pp. 1–4,
2021.

