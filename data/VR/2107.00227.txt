Computers & Graphics (2021)

Contents lists available at ScienceDirect

Computers & Graphics

journal homepage: www.elsevier.com/locate/cag

UrbanVR: An immersive analytics system for context-aware urban design

Chi Zhanga, Wei Zengb, Ligang Liua

aUniversity of Science and Technology of China, Hefei, 230026, China
bShenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China

1
2
0
2

g
u
A
3
2

2
v
7
2
2
0
0
.
7
0
1
2
:
v
i
X
r
a

A R T I C L E I N F O

Article history:
Received August 24, 2021

]

C
H
.
s
c
[

Keywords: Immersive analytics, urban
design, virtual PCP, viewport optimiza-
tion, gesture interaction

A B S T R A C T

Urban design is a highly visual discipline that requires visualization for informed de-
cision making. However, traditional urban design tools are mostly limited to represen-
tations on 2D displays that lack intuitive awareness. The popularity of head-mounted
displays (HMDs) promotes a promising alternative with consumer-grade 3D displays.
We introduce UrbanVR, an immersive analytics system with eﬀective visualization and
interaction techniques, to enable architects to assess designs in a virtual reality (VR)
environment. Speciﬁcally, UrbanVR incorporates 1) a customized parallel coordinates
plot (PCP) design to facilitate quantitative assessment of high-dimensional design met-
rics, 2) a series of egocentric interactions, including gesture interactions and handle-bar
metaphors, to facilitate user interactions, and 3) a viewpoint optimization algorithm to
help users explore both the PCP for quantitative analysis, and objects of interest for
context awareness. Eﬀectiveness and feasibility of the system are validated through
quantitative user studies and qualitative expert feedbacks.

© 2021 Elsevier B.V. All rights reserved.

1. Introduction

Visualization plays a key role in supporting informed discus-
sions among stakeholders in an urban design process: For de-
signers, the design process comprises visual representations in
practically all stages and for most aspects − from ideation and
speciﬁcation to analysis and communication [1]; For decision
makers, visualizations help to understand implications of the
design [2]. Speciﬁcally, the process of designing a city’s devel-
opment site is rather tedious: designers need to come up with
several design options, then analyze them against key perfor-
mance indicators, and ﬁnally select one. The process requires
eﬀective analysis and visualization tools.

Urban environments are composed of buildings and various
surroundings, which can be naturally represented as 3D mod-
els. As such, many 3D visual analytics systems have been de-
veloped for a variety of urban design applications, including
cityscape and visibility analysis [3, 4], ﬂood management [5, 6],

e-mail: wei.zeng@siat.ac.cn (Wei Zeng)

vitality improvement [7], and recycling [8]. Yet, most of them
are developed for desktop displays, which lack spatial presence
that is the sense of “being there” in the world depicted by the
virtual environment [9]. The 3D nature of urban environments
calls for immersive analytics tools that can provide vivid pres-
ence of the urban environment beyond the desktop.

The advance of aﬀordable, consumer head-mounted displays
(HMDs) such as the HTC Vive, has revived the ﬁelds of VR and
immersive analytics. Recently, there is a growing interesting in
utilizing VR technology for urban planning, which can improve
design eﬃciency and facilitate public engagement [10, 11].
Nevertheless, developing such an immersive analytics system is
challenging: First, the system needs to present the urban envi-
ronment (3D spatial information) together with mostly quantita-
tive analysis data (abstract information) that are typically high-
dimensional. It remains challenging to display both spatial and
abstract information in the virtual environment [12]. Second,
conventional interactions for desktop displays, such as mouses
and keyboard, are infeasible for VR. New ways to interact with
the immersive analytics systems are required [13].

 
 
 
 
 
 
2

Preprint submitted for review / Computers & Graphics (2021)

We present UrbanVR, an immersive analytics system that
integrates advanced analytics and visualization techniques to
support the decision-making process in site development in a
VR environment. UrbanVR caters to various analytical tasks
that are feasible for analysis and visualization in VR, identi-
ﬁed from semi-structured interviews with a collaborating ar-
chitect (Sect. 3). We then focus on visualization and interac-
tion design for supporting site development in a virtual envi-
ronment. Speciﬁcally, we design a parallel coordinates plot
(PCP) that can be spatially situated with 3D physical objects,
to facilitate quantitative assessment of high-dimensional analy-
sis metrics (Sect. 4.2). Next, we integrate a series of egocen-
tric interactions, including gesture interactions, and handle bar
metaphors, to facilitate user interactions in VR (Sect. 4.3). We
further develop a viewpoint optimization algorithm to mitigate
occlusions and help users explore spatial and abstract informa-
tion (Sect. 4.4). A quantitative user study demonstrates eﬀec-
tiveness of these interactions, and qualitative feedbacks from
domain experts conﬁrm the applicability of UrbanVR in sup-
porting shading and visibility analysis (Sect. 5).

The primary contributions of our work include:

• UrbanVR is an immersive analytics system with a 3D vi-
sualization of an urban site for context-aware exploration,
and a customized parallel coordinate plot (PCP) for quan-
titative analysis in VR.

• UrbanVR integrates viewport optimization and various
egocentric interactions such as gesture interactions and
handle-bar metaphors, to facilitate user exploration in VR.

• A quantitative user study together with qualitative expert
interviews demonstrate the eﬀectiveness and applicability
of UrbanVR in supporting urban design in virtual reality.

2. Related Work

This section summarizes previous studies closely related to

our work in the following categories.

Visual Analytics for Urban Data. Nowadays, big urban data
are being ubiquitously available, which has promoted the de-
velopment of evidence-based urban design. Meanwhile, visual-
ization has been recognized as an eﬀective means for commu-
nication and analysis. As such, many visual analytics systems
have been developed to support various urban design applica-
tions, like transportation planning [14] and environment assess-
ment [15]. Most of these systems utilize 2D maps for visual-
izing big urban data [16], which omits 3D buildings and sur-
roundings in an urban site. Recent years have witnessed some
visual analytics systems for urban data in 3D. For instance, Vi-
talVizor [7] arranges 3D visualization of physical entities and
2D representation of quantitative metrics side-by-side for urban
vitality analysis. Closely related to this work, Urbane [3], Vis-
A-Ware [4], and Shadow Proﬁler [17] provide 3D visualizations
for comparing eﬀects of new buildings on landmark visibility,
sky exposure, and shading. Our work also caters to landmark
visibility and shadow analysis. But instead of presenting 3D

spatial information on 2D display, we render 3D visualizations
in a VR environment, to enable more vivid experience.

Immersive Analytics. VR HMDs provide an alternative to 2D
displays for data visualization in an immersive environment.
Immersive visualization is naturally appropriate for spatial data,
e.g., scientiﬁc data [18]. Advances of technical features of VR
HMDs including higher resolution and lower latency tracking,
promote a wide adoption of consumer-grade HMDs for data vi-
sualization and analytics in VR, i.e., immersive analytics [19].
Nevertheless, there are many challenges when developing
immersive analytics systems. The challenges can be grouped
into four topics: spatially situated data visualization, collabo-
rative analytics, interacting with immersive analytics systems,
and user scenarios [13]. Recently, many immersive visualiza-
tion techniques for displaying abstract information have been
developed and evaluated, such as DXR toolkit [20], glyph
visualization [21], ﬂow maps [22], heatmaps [23, 24], and
tilt map [25]. These techniques provide expressive visualiza-
tions for building immersive analytics. For example, DXR
toolkit [20] provides a library of pre-deﬁned visualizations such
as scatterplots, bar charts, and ﬂow visualizations. Yet, these vi-
sualizations are infeasible for displaying the high-dimensional
shadow and visibility analysis metrics. As such, we design a
customized PCP that can be overlaid on top of a building, to
address the challenge of spatially situated data visualization.

Interaction Design. Interactivity is one of the key elements of a
vivid experience in VR [26]. Advanced interaction techniques
are required for simultaneous exploration of 3D urban context
and abstract analysis metrics. Egocentric interaction that can
embed users in VR is the most common approach for immer-
sive visualizations [27]. We design interactions as follows to
facilitate interaction with immersive analytics systems.

• Direction Manipulation. VR HMDs block users’ vision
of the real environment, thus traditional interactions (e.g.,
mouse and keyboard) for 2D displays are not applicable
anymore. On the other hand, mid-air gestural interaction
can mimic the physical actions we make in the real world,
which has been studied as a promising approach to 3D
manipulation [28]. For example, Huang et al. [29] devel-
oped a gesture system for abstract graph visualization in
VR. Moreover, virtual hand metaphors have been studied
for enhancing 3D manipulation, for both multi-touch dis-
plays [30] and mid-air [31]. In line with these studies, this
work presents a systematical categorization and develop-
ment of gesture interactions required for immersive urban
development, which will allow users to manipulate 3D ur-
ban context and interact with abstract analysis metrics.

• Viewpoint Motion Control. Occlusion is considered as a
In the development
weakness of 3D visualization [32].
stage, we observed that users often spend much time on
changing their viewpoint when manipulating 3D physical
objects, for mitigating the occlusion problem. To facilitate
the navigation process, Ragan et al. [33] designed a head
rotation ampliﬁcation technique that maps the user’s phys-
ical head rotation to a scaled virtual rotation. Alternatively,

Preprint submitted for review / Computers & Graphics (2021)

3

viewpoint optimization techniques have been proven ef-
fective by extensive studies in visualization [34] and com-
puter graphics [35]. A principle metric for these methods
is view complexity [36], and the metrics are expanded with
visual saliency, view stability, and view goodness [37]. As
such, we develop an automatic approach for viewpoint op-
timization to reduce navigation time.

3. Overview

In this section, we discuss domain problems derived from a
collaborating architect (Sect. 3.1), followed by a summary of
distilled analytical tasks (Sect. 3.2).

3.1. Domain Requirements

In a six-month collaboration period, we worked closely with
an architect having over 10 years of experience in urban design.
The architect is often asked to propose building development
schemes for a development site, which mainly involves consid-
erations of the following entities.

• Static environment: Physical entities surrounding the de-

velopment site. The entities cannot be modiﬁed.

• Building candidates: The buildings that are proposed by
architects to be placed on the development site. Their sizes
and orientations can be modiﬁed.

• Landmark: A physical entity in the development site that

is easily seen and recognized.

Quantitative evaluation of the building candidates need to be
carried out, in order to meet the evidence-based design prin-
ciple. Many evaluation criteria have been proposed, such as
visibility, accessibility, openness, etc. After evaluation, the
building candidates and corresponding evaluation criteria will
be demonstrated to stakeholders.

• Visualization Requirements. The expert showed us exist-
ing tools for site development, and pointed out the lack
of intuitive visualizations. He expressed a strong desire
of utilizing emerging VR HMDs, as stakeholders are usu-
ally impressed with new technologies. The visualization
should include both 3D geometries of the development
site, and abstract information of analysis results.

• Analytics Requirements. Together with the expert, we
identiﬁed two key criteria that should be carefully evalu-
ated in site development: visibility and shadow. Visibility
analysis is applicable to a landmark, while shadow analy-
sis for the static environment. Both criteria are preferably
visualized in an immersive environment.

• Interaction Requirements. Architects working with city
planning and development often manually manipulate
physical models with hands. As such, the architects prefer
to interact with virtual objects using gestural interactions,
instead of HMD coupled controllers that they are not fa-
miliar with. The collaborating architect also suggested to
support system exploration without body movement to re-
duce the risk of falling, stumbling over cables, or inducing
motion sickness.

Fig. 1. Illustration of visibility analysis along a path of viewpoints using
image analysis.

3.2. Task Abstraction

The overall goal of this work is to develop an immersive an-
alytics system for accessing site development. Based on the
domain requirements, we follow the nested model for visual-
ization design and validation [38], and compile a list of ana-
lytical tasks in terms of data/operation abstraction design, and
encoding/interaction technique design:

• T.1: Quantitative Analysis. The system should provide
quantitative analysis of the identiﬁed criteria, i.e., visibil-
ity (T.1.1) and shadow (T.1.2). Eﬃcient computation algo-
rithms should be developed such that the analyses can be
performed interactively.

• T.2: Multi-Perspective Visualization. The system should
present multi-perspective information in an immersive en-
vironment. First, a 3D view of the urban area should be
provided to enable context-aware exploration (T.2.1). Sec-
ond, an analytics view should be presented to visualize
quantitative results (T.2.2).

• T.3: Eﬀective Interaction. The system should be coupled
with a robust gesture interaction system (T.3.1). Actions
on virtual objects should be directly visible to the users
(T.3.2). Lastly, the system should provide optimal view-
points to reduce body movement (T.3.3).

4. UrbanVR System

The UrbanVR system consists of three components: data
analysis (Sect. 4.1), visualization design (Sect. 4.2), and inter-
action design (Sect. 4.3).

4.1. Data Analysis

The analysis component supports the quantitative analysis
task (T.1). Here, both shadow and visibility measurements are
calculated using GPU-accelerated image processing methods.
The analyses include the following steps:

• Viewpoints Generation. Visibility and shadow are mea-
sured for a landmark and static environment, respectively.
Both measurements are performed along a path of view-
points in accordance to real-world scenarios, e.g., a city
tourist tour and sun movement over one day. Speciﬁcally,
viewpoints for visibility analysis are manually speciﬁed
by the collaborating architect, which simulates a popular
route in the urban area. Viewpoints for shadow analysis are
automatically generated using a solar position algorithm1.

1https://midcdmz.nrel.gov/spa/

4

Preprint submitted for review / Computers & Graphics (2021)

mainly consists of two closely linked views:

Physical View. The view presents 3D visualization of an ur-
ban area in VR to enable urban context awareness (T.2.1). We
employ Unity3D’s built-in VR rendering framework to gener-
ate the view. All rendering features are enabled and adjusted
to optimal settings, and our system is able to run at interac-
tive frame rates. Selection, manipulation, and navigation op-
erations are enabled in the view through a gesture interaction
system described below. Besides, the view also supports ani-
mation, which is integrated to facilitate users’ understanding of
shadow and visibility. For shadow analysis in animation mode,
the user’s position will be moved to the optimal viewpoint for
the building candidate, and the system’s lighting direction will
change over time according to the sun movement. For visibility
analysis, the user’s position moves along the architect’s deﬁned
path while keeping the target building in view.

Analytics View. The view supports visualization of quantitative
analysis results (T.2.2), which is mainly made up of a parallel
coordinate plot (PCP) as shown in Fig. 2. The PCP consists
of seven axes, where the middle three correspond to buildings,
orientations, and scales of building candidates, the left two cor-
respond to time and shading values for shadow analysis, and the
right two correspond to path and visibility results for visibility
analysis, respectively. We bend the time axis into an arc to hint
sunrise and sundown over time, and arrange the path axis in
according to street topology of the path [39]. Since architects
would like to ﬁrst compare building candidates, we integrate
bar charts on the left- and right-most axes to indicate shading
and visibility distributions for all orientations and scales of a
speciﬁc candidate. If the candidate’s orientation and scale are
further speciﬁed, a red line is added in the PCP to show shading
and visibility values at certain time and path position.

Besides PCP, various buttons are integrated in the Analytics
View. For the middle three axes, plus and minus buttons are
placed at the top and bottom, respectively. Users can click the
buttons to select a diﬀerent design, i.e., to change building can-
didate, orientation, or scale. For time and path axes, start and
stop buttons are placed at the top and bottom to control anima-
tion. The view is always placed on top of the development site,
and if a design is selected, the view will be moved up to the top
of the candidate building.

4.3. Interaction Design

The speciﬁc interaction requirements raised by the architect
bring in research challenges for interaction design. To support
eﬃcient exploration (T.3), we develop a series of egocentric in-
teractions, including robust gesture interaction system, and in-
tuitive handle bar metaphor.

4.3.1. Gesture Interaction System

To cater to the requirement of eﬃcient and natural interac-
tions, we decided to implement an interaction system based on
handle gesture. We start with a careful study of required opera-
tions and raw input gestures.

Fig. 2. Analytics view consists of a customized PCP presenting shadow and
visibility analysis results, and various buttons for user interactions.

• Image Rendering. For each viewpoint, two high-resolution
images are rendered. First, we create a frame buﬀer ob-
ject (FBO) denoted as FBO-1, and render only the target
in blue color to FBO-1 (Fig. 1(left)). Then, we create
FBO-2 and render the entire urban scene to it, with the
target rendered in blue color and other models in red color
(Fig. 1(right)). For both images, the camera is directed
towards the center of the target.

• Pixel Counting. We can then extract the amount of blue
1 and PCb
pixels, denoted as PCb
2 in FBO-1 and FBO-2, re-
spectively. Then we compute visibility as PCb
1. For
shadow analysis, a building candidate produces shadows
on the static environment. Thus, the shadow brought in
by a building candidate is computed as 1 − PCb
1. To
accelerate the computation, we develop a parallel comput-
ing method that divides each image into 10 × 10 grids and
counts blue pixels in each grid using GPU.

2/PCb

2/PCb

An alternative approach to the above described pixel
counting is to directly clip other models when rendering
the target, which could be even faster. Yet the image pro-
cessing method can be generalized to other urban design
scenarios, such as isovist analysis that measures the vol-
ume of space visible from a given point in space.

At each viewpoint, we precompute these measurements for
each building in π/3 increments and at three scales. In this way,
we generate a total of 18 (6 orientations × 3 scales) design vari-
ations for each building candidate. The shadows from 8:00 to
18:00 for each design are measured every 30 minutes. Our sys-
tem loads the precomputed results for visualization. At runtime,
our system also allows users to manipulate a building candidate,
such as to scale the building up, or change its orientation. After
the interaction, the system will recompute the measurements in
the background. With the GPU-accelerated image processing
method and this background computing approach, our system
achieves interactive frame rates at runtime. Note that visibil-
ity analysis is an approximation of stereographic projection of
a landmark onto VR glasses. This approximation is acceptable
since human eyes distance is much smaller than the viewpoint
sampling step, which is about 20 meters in our work.

4.2. Visualization Design

The visualization component

the
multi-perspective visualization task (T.2). The component

is designed to support

Preprint submitted for review / Computers & Graphics (2021)

5

Fig. 3. Gesture interaction design for interacting with a virtual object: (a) Select, (b) Translate, (c) Rotate, and (d) Scale.

Required Operations. We summarize required operations into
the following categories.

• Selection. Users can select an object in the virtual envi-
ronment, e.g., to select a development site, or to select a
building candidate.

• Manipulation. After selecting a building candidate, users
can manipulate it, including translate its position, rotate its
orientation, and scale its size.

• Navigation. As requested by the architect, our system
should allow users to explore the urban area in the vir-
tual space while keeping their body stationary in the real
world. We opt to seated user postures with gestural inter-
actions for users to navigate the urban scene, which allow
users to sit comfortably and suit for most people [40, 41].
We deﬁne two types of navigation operations in our sys-
tem: pan in x-, y- and z-dimensions, and rotate around
y-axis (Unity3D uses y-up coordinate system).

Raw Input Gestures. Our system detects hand gestures using
a Leap Motion device, which is attached to the front of the
HMD. When a hand is detected, the device captures various
motion tracking data about hands and ﬁngers, including palm
position and orientation, ﬁngertip positions and directions, etc.
Before deciding what gestures to be used, we ﬁrst tested diﬀer-
ent kinds of gestures, including index ﬁnger and thumb up, ﬁve
ﬁngers open, and ﬁst, etc. These gestures are tested with both
right and left hands, from diﬀerent distances, and in diﬀerent
orientations. We identify three most recognizable and stable
gestures, i.e., index ﬁnger up, ﬁve ﬁngers open, and ﬁst. We
denote them as Point, Open, and Close, respectively.

Gesture Interaction Design. When one of the three gestures
is recognized at time t, our system will model the gesture
Ghand(t) :=<Status, P, O>, where hand ∈ {l, r} denoting left
and right hand respectively, and Status ∈ {Point, Open, Close}.
P refers to gesture position in the virtual space, which is set to
the index ﬁnger’s tip position for Point and the palm position for
Open and Close. O refers to orientation of the gesture, which
is taken as the palm orientation. Orientation is a necessary at-
tribute in our system, as we use orientation stability to classify
consecutive gestures belonging to the same operation. Both P
and O are 3D vector type data.

We map the raw gestures to interactions as follows.
• Select. Humans naturally select an object by putting up
index ﬁnger and pointing at the object. We modify this ap-
proach to a two-step operation in our system. First, when

a Point gesture is detected, our system measures distances
from all interactive objects (including building candidates,
development sites, and buttons) to the gesture’s position.
The closest object with distance less than a threshold will
be highlighted. Second, users conﬁrm the selection by
opening up the palm, as shown in Fig. 3(a).

• Manipulate. After an object is selected, users can manip-
ulate it with translate, rotate, and scale operations. These
operations also work in two steps. First, left- and right-
hand Close gestures need to be positioned besides the ob-
ject at time t0, and our system records the gestures as Gl(t0)
and Gr(t0), respectively. Second, the gestures will move
to other positions at time t1, recorded as Gl(t1) and Gr(t1).
Corresponding P of these gestures are used to generate two
vectors in 3D space:

Vl = Pl(t1) − Pr(t0)
Vr = Pr(t1) − Pr(t0)

(1)

representing movements of left and right hands.
Translate: Next, we measure the angle θ1 between Vl
and Vr.
If θ1 is less than π/6, we consider the interac-
tion as Translate. A translation corresponding to Vtrans =
(Vl + Vr)/2 will be applied to position of the object. If the
condition is not met, we measure two more vectors:

V(t0) = Pl(t0) − Pr(t0)
V(t1) = Pl(t1) − Pr(t1)

(2)

Rotate: If θ2 is above π/12, we consider the interaction as
Rotate. Buildings can only be rotated around the y-axis.
Hence we measure the angle of θ2 mapped on XZ plane
and rotate the building accordingly.
Scale: If θ2 is less than π/12, we consider the interaction
as Scale. The operation is applicable to an object in x-
, y-, and z-dimensions. Scaling factor is proportional to
(cid:107)V(t1)(cid:107)/(cid:107)V(t0)(cid:107), and divided in x-, y-, and z-dimensions.
Notice that scaling is only applicable to a selected virtual
object, but not to the whole scene. This is because we
would like to maintain the relative proportion between the
virtual body and the environment.

• Navigate. UrbanVR system supports map navigation by
camera panning and tilting. The operations are imple-
mented similarly to Translate and Rotate manipulation op-
erations. The diﬀerences include: ﬁrst, Open gesture is
used instead of Close gesture; and second, no object needs
to be selected ﬁrst.

212(a) Select: Highlight with Point gesture, then Confirm with Open gesture.t(b) Translate: two Close gestures move in the same direction.(c) Rotate: two Close gestures rotate in reverse directions.(d) Scale: two Close gestures move towards/outwards each other.VleftVrightVt0Vt1θ2Vt1Vt06

Preprint submitted for review / Computers & Graphics (2021)

Fig. 4. Handle bar metaphors representing building rotation (left) and map
navigation (right).

4.3.2. Handle Bar Metaphor

Continuous visual representations of user operations are nec-
essary for designing eﬀective interactions. Besides choosing
the three most accurate gestures, we further improve user inter-
actions through a virtual handle bar.

Handle bar metaphor was proven eﬀective for manipulating
virtual objects with mid-air gesture interactions [31]. Our sys-
tem adapts this approach: when users are manipulating building
candidates or navigating the map, a green and blue handle bar
are presented for the initial and moved gesture positions, re-
spectively. First, our system detects if two-hand Open or Close
gestures can be detected for 100 consecutive frames (about one
second). Once the ﬁrst step succeeds, a green handle bar with
two balls at both ends and a linking dashed line will be drawn to
represent the initial gesture positions. Next, the system detects
if follow-up gestures can make up a manipulation or naviga-
tion operation, as described in the above section. If an opera-
tion is matched, a blue handle bar with a solid connecting line
will be presented at the moved gesture positions. Speciﬁcally,
for Translate operations, a gray dashed line connecting middle
points of the two handle bars is drawn as well. Fig. 4 presents
examples of handle bar metaphors representing building rota-
tion (left), and map navigation (right).

4.4. Viewpoint Optimization

The complex urban scene can easily cause occlusion that hin-
ders users’ exploration. Making occluders semi-transparent is
not a good solution, since the buildings are coupled with high-
resolution textures and translucency of the textures will defect
users’ perception on the PCP. Alternatively, users can use ges-
ture interactions described above to navigate the map. However,
map navigation is relatively slow and usually takes a long time
to ﬁnd a good viewpoint. This motivates us to develop a method
which can help users automatically ﬁnd an optimal viewpoint.
Speciﬁcally, this work optimizes the camera position that
generates maximum view goodness for a target. The maximum
view goodness is deﬁned on the following requirements:

• R1: Distance and viewing angle to the target shall be not
too close that fails to view the whole target, meanwhile not
too far away that loses details of the target.

• R2: Occlusion of the target shall be minimized. Speciﬁ-
cally, the camera shall not fall inside any building (R2.1),
and there shall be minimum occluders in-between the cam-
era and the target (R2.2).

Fig. 5. Illustration of building abstraction projected to XY plane, and the
diﬀerent energy terms that are used for viewpoint optimization.

Next, we formulate these requirements as an energy function
that can be feasibly solved with optimization algorithms.

Problem Deﬁnition. In our setup, viewpoint optimization is ap-
plied to a target, e.g., the development site or building candi-
date. We adapt a LoD simpliﬁcation approach that ﬁrst simpli-
ﬁes LoD3 buildings in an urban scene into LoD1 cuboids [42],
and then maximizes the visibility for the target. The target can
be represented as {CTi}n
i=1, where CTi represents a cuboid be-
longing to the target. Target can be occluded by other buildings,
and we represent these occluding buildings {COi}m
i=1. Since ﬁeld
of view (FOV) is ﬁxed by the HMD, we can only change the
viewpoint position and view direction. We further simplify the
problem by always pointing the view ray to the center of the
target. Thus, we deﬁne the viewport optimization problem as:

given cuboids of a target {CTi}n
i=1, and cuboids of oc-
cluding buildings {COi}m
i=1, to ﬁnd an optimal posi-
tion P for observation that generates maximum view
goodness of the target.

Energy Function. To solve the problem, we introduce a con-
straint to optimize P to the target distance (R1), a constraint to
avoid P inside occluding buildings (R2.1), and a constraint to
minimize view occlusion of the target (R2.2). We formulate
these constraints into diﬀerent energy terms and assemble the
energy terms into an energy function.

Fig. 5 presents an illustration of these energy constraints of
one building projected onto the XY plane. We further repeat
projections and measurements on XZ and YZ planes, and com-
bine results from all three planes together. Detailed procedures
are described as follows.

• Camera Target Distance (E1). After projection, we can get
a list of vertices in the XY plane from {CTi}n
i=1. We extract
a closed polygon, i.e., α-shape from these vertices, which
is the boundary of the target projected onto the XY plane.
Maximum diagonal distance of the α-shape is calculated
and denoted as dα. Then we use a point to polygon dis-
tance function which measures distance d from projected

(b) Camera Obstruction d <0d >0(a) Camera Target Distance θdxyBuilding Abstraction (c) View Occlusion  θ2θ1α-shapedαCOCTCOPreprint submitted for review / Computers & Graphics (2021)

7

viewpoint Pxy to the α-shape. The distance is negative if
the point falls in the closed polygon. To allow arbitrary
viewing angles, we introduce θ which measures the ele-
vation angle of the viewpoint over the horizon from the
center of the α-shape to the viewpoint. Thus, we create
the sub-energy term on the XY plane:

E1(Pxy) = λ0e(d−dmin)×(d−dmax) + λ1e(θ−θ0)2+(θ−θ1)2

(3)

where λ0 and λ1 are the weights for each term. dmin and
dmax are preferred distance range and set to 0.5 × dα and
1.5 × dα, respectively. θ0 and θ1 are preferred view di-
rections meeting the condition θ0 + θ1 = π, and they are
empirically set to π/4 and 3π/4. E1(P) is the sum of all
sub-energy terms on each plane.

• Camera Obstruction (E2). After projecting {COi}m

i=1 onto
i=1. For each
xy, we measure its distance to Pxy, denoted as di. The

the XY plane, we can get m polygons {COi
COi
sub-energy term can be constructed as:

xy}m

E2(Pxy, COi

xy) = edi

(4)

To make sure E2 becomes large when P falls in any COi,
we model E2 as:

E2(P) =

m(cid:88)

i=1

1
(cid:80) E2(PΠ, COi
Π)

, Π ∈ {xy, xz, yz}

(5)

{CTi}n

• View Occlusion (E3).

i=1 are projected on the XY
xy}n
plane, and n corresponding polygons {CT i
i=1 are gener-
xy, we measure how much another C j
ated. For each CT i
xy
can occlude it. Here, C j
i=1 − {CT i
i=1 ∪ {CT i
xy}.
The measurement is calculated as follows:

xy ∈ {COi

xy}m

xy}n

1. Generate a vector Q j

noted as Q j
a second vector Q j

xy) to center of CT i

2. Measure angle θ1 between Q j
θ1 means more likely C j
CT i

xy.

xyQi

xy (de-
xy), and

xy from center of C j
xy (denoted as Qi
xy to Pxy.
xy and Q j
xyPxy. Lager
xy may lay in-between Pxy and

xyPxy from Q j
xyQi

3. Generate vectors from Pxy to all vertices in CT i

xy, and
extract two vectors with maximum and minimum an-
gles, denoted as PxyQi0
xy. These vectors
represent up- and low-bound view angles for CT i
xy,
In the same way, extract PxyQ j0
respectively.
xy and
xy for C j
PxyQ j1
xy.

xy and PxyQi1

4. Lastly, we measure intersection angle θ2 between
xy]. Larger θ2

xy] and [PxyQ j0

xy, PxyQ j1

[PxyQi0
xy, PxyQi1
means more occlusions.

We construct the energy term as:

E3(Pxy) =

n(cid:88)

m+n(cid:88)

i=1

j=1

1
eθ1 + 1

× eθ2 , where j (cid:44) i

(6)

E3(P) is the sum of all sub-energy terms on each plane.

Fig. 6. Eﬀects of diﬀerent energy terms on viewport optimization. Left:
camera target distance (E1) and camera obstruction (E2) only cause oc-
clusion of the target building. Middle: camera target distance (E1) and
view occlusion (E3) only block the camera. Right: E1 + E2 + E3 produces
optimal viewport for the target building.

With these terms deﬁned, we can model the problem as min-

imization of the following energy:

E(P) =

3(cid:88)

i=1

ωiEi(P)

(7)

where ωi represents the weight for each term, which are empir-
ically set as ω1 = 1, ω2 = 100, ω3 = 10, respectively. ω2 is
the largest to prevent the camera from moving inside a physi-
cal object, and ω3 is larger than ω1 to minimize view occlusion
meanwhile prevent the camera from moving too far away.

Fig. 6 illustrates the eﬀects of diﬀerent energy terms on view-
port optimization. On the left, we adopt the energy function
based on defaults weights for camera target distance (E1) and
camera obstruction (E2), whilst the weight for view occlusion
(E3) is set to zero. The target building (red outline) is obscured
by a surrounding building. In the middle, we adopt the energy
function based on default weights for E1 and E3, whilst the
weight for E2 is set to zero. The camera is now positioned in-
side a building that blocks the target building. The full energy
function based on E1 + E2 + E3 addresses these issues and
leads to optimal viewport as shown on the right. Note that we
neglect the result of energy function based on E2 + E3, since
the camera will be located in a far distance.

4.5. System Implementation

UrbanVR is implemented in Unity3D. The input models
contain 3D geometry information, including geo-positions in
WGS-84 coordinate system [43], and a third dimension for
height. All building models have high-resolution textures, mak-
ing it suitable for immersive visualization. This, however, also
increases computation costs when users interact with our sys-
tem. In order to accelerate the computation process, we pre-
process all building models in LOD3 by abstracting each model
into up to ﬁve cuboids in LOD1 [42]. The cuboids act as bound-
ing boxes in 3D space for a model. When the system starts, each
model is loaded with corresponding cuboids, which are used in
viewpoint optimization and gesture interactions.

The viewpoint optimization energy (Equation 7) is a contin-
uous derivative function, which works well for many optimiza-
tion algorithms. This work employs a Quasi-Newton method.
In each computation iteration, the method will ﬁnd the gradi-
ent and length to next position. The process will stop if either
a local minimum is found, or the number of iterations is ex-
ceeded (1000 in our case). To ﬁnd multiple local minima, we

8

Preprint submitted for review / Computers & Graphics (2021)

start 10 parallel computation processes from 10 diﬀerent ini-
tial positions, making full use of the computing resource. The
initial positions are uniformly sampled on the circle that is at
a distance of dα (deﬁned in energy term E1) from the center of
the target building, and forms a 60 degree angle with the ground
above the map. We also accelerate the optimization process by
considering only the buildings near the target building. Speciﬁ-
cally, we exclude buildings whose distances are more than 7×dα
from the target building. After all processes are ﬁnished, we
combine all candidates and choose the best one.

5. Evaluation

UrbanVR is evaluated from two perspectives: First, a quanti-
tative user study is conducted to assess the performance of ego-
centric interactions and viewport optimization, in accomplish-
ing the task of manipulating a selected building to match the
target building. Second, qualitative expert feedbacks associated
with a case study in Singapore, are collected for applicability
evaluation.

5.1. User Study
Experiment Design. To evaluate the egocentric interactions, we
design a within-subjects experiment with 12 conditions: 4 inter-
action mode × 3 scene complexity. Each participant participated
in each condition. For each condition, a target building colored
in translucent cyan is positioned in a development site. Partici-
pants are asked to select one from multiple building candidates.
The selected candidate has diﬀerent position, orientation, and
scale with the target building. Completion time is recorded and
used as the evaluation criteria.

• Interaction Mode. The gesture system provides fundamen-
tal interactions for UrbanVR, while viewpoint optimiza-
tion and handle bar are complementary for improving user
experience. We would like to ﬁrst test if gestures alone
work, and then verify if the other two can facilitate user
exploration. Hence, we design four modes of interaction:

– C1: Gesture only (Ge).
– C2: Gesture and viewpoint optimization (Ge+VO).
– C3: Gesture and handle bar (Ge+HB).
– C4: Gesture, viewpoint optimization, and handle bar

together (Ge+VO+HB).

• Scene Complexity. In reality, surroundings of a develop-
ment site can be sparse or dense. High buildings may
occlude users’ view of the site, so more interactions are
needed to avoid occlusion. This causes more exploration
diﬃculties. As illustrated in Fig. 7, we design three scenes
with diﬀerent complexities:

– S1: Simple. The surrounding consists of only two

buildings; Fig. 7 (left).

– S2: Moderate. The site is surrounded by six sparsely
located buildings. Two of them are relatively low
height on one side, from where the space can be
viewed without occlusion; Fig. 7 (middle).

– S3: Complex. The surrounding consists of eight
densely located buildings. All buildings are higher
than the target building, thus the space is occluded
from almost all viewpoints; Fig. 7 (right).

Participants. We recruited 15 participants, 10 males and ﬁve
females. 12 of them are graduate students, and the others are
research staﬀ. The age of the participants range between 20 and
30 years. Two participants played VR games using the HTC
Vive controllers. No participant has experience with gesture
interactions in VR before the study. Three participants have a
background in architecture, while the others have no knowledge
about urban design.

Apparatus and Implementation. UrbanVR system was imple-
mented in Unity3D. The experiments were conducted on a
desktop PC with 12 × Intel(R) Core(TM) i7-6800K CPU @
3.4GHz, 32GB memory, and a GeForce GTX1080 Ti graphics
card. The VR environment was running in a HTC Vive VR
HMD, and the hand gestures were captured using a Leap Mo-
tion attached to the front of HMD.

Procedure. The studies are performed in the order of introduc-
tion, training, experiment, and questionnaire. First, we present
a 5-min introduction about the interactions, followed by a 10-
min training session to make sure all participants are familiar
with the interactions. Then, the experiment starts, and comple-
tion time for each experiment condition is recorded. In the end,
feedbacks are collected. The questions include if they had expe-
rience with gesture interactions in VR, if they feel dizzy, if they
think the gestures are natural, and advices for improvement.

In each experiment condition, the starting viewpoint is po-
sitioned on top of the urban scene. Participants are asked to
complete the task requiring the following operations:

• Navigate to the site. This can be either done through
gesture-based map navigation, or viewpoint optimization
by selecting the site.

• Select a building. Participants can open up a menu through
left-hand Open gesture, and eight building candidates will
be presented at the left hand position. Then participants
can select a building that matches the target through right
hand Selection. The selected building will be placed be-
sides the menu.

• Manipulate the selected building to match the target. Po-
sition, orientation, and size of the selected building diﬀer
from the target. Participants need to manipulate the se-
lected building to match the target.

All operations are completed while the participants are sitting
in a chair. This process is repeated in total 12 times (4 interac-
tion combinations × 3 scene complexities) for each participant.
The participants are asked to take a break (3 minutes) in every
10 minutes, and take a break (5 minutes) after ﬁnishing a task.
Sequence of the interaction combination is pseudo-randomly
assigned to each participant in order to suppress learning ef-
fects gained from previous assignments. Each participant was
compensated with SGD $40.00 (∼USD $30) after the study.

Preprint submitted for review / Computers & Graphics (2021)

9

Fig. 7. Scenes set up in the user study: simple, moderate, and complex from left to right.

Hypotheses. We postulate the following hypotheses:

• H1. Gesture only (C1) is the slowest interaction technique.

• H2. Viewpoint optimization (C2) can facilitate gesture

only (C1) interaction, i.e., time(C1) > time(C2).

• H3. Handle bar metaphor (C3) can facilitate gesture only

(C1) interaction, i.e., time(C1) > time(C3).

• H4. All techniques together (C4) is the fastest technique,

i.e., time(C2) > time(C4) and time(C3) > time(C4).

Experiment Results. 15 experiment results are generated for
each experiment condition. We ﬁrst test the results in each
condition against normal distribution using Shapiro-Wilk test.
The results show that conditions of C2 do not follow normal
distribution, while the results in other conditions are following
normal distributions under certain probability.

Prerequisites for computing ANOVA are fulﬁlled for H1,
H3 and H4. We perform a two-way ANOVA (3 interac-
tion combinations × 3 scene complexities) on them. Signif-
icant eﬀects of interactions on completion time (F(2,126) =
64.97, p < 0.001, η2 = 1.718) are observed. Scene com-
plexity also has a signiﬁcant main eﬀect on completion time
(F(2,126) = 59.63, p < 0.001, η2 = 1.646). We use Kruskal-
Wallis test [44] to evaluate H2. The result shows the view-
point optimization has signiﬁcant eﬀect on completion time
(H(2) = 25.53, p < 0.001, η2 = 0.560).

We also perform post hoc comparisons of completion times
among the interaction combinations. The results are shown in
Fig. 8. C1 is on average more than 33s slower than C3 (t(44) =
33.42, p < 0.001, dCohen = 0.882), while C3 is on average 23s
slower than C4 (t(44) = 22.84, p < 0.001, dCohen = 0.955); C1
is on average more than 32s slower than C2 (t(44) = 32.38, p <
0.001, dCohen = 0.772), while C2 is on average 24s slower
than C4 (t(44) = 23.89, p = 0.001, dCohen = 0.799). The re-
sults conﬁrmed H1, H2, H3, and H4. Through more detailed
probes, we ﬁgure out: C1 is slower than C3 (t(14) = 21.07, p =
0.018, dCohen = 0.921), (t(14) = 23.93, p = 0.011, dCohen =
1.032), (t(14) = 55.37, p < 0.001, dCohen = 1.698), while
C3 is slower than C4 (t(14) = 16.73, p = 0.003, dCohen =
1.225), (t(14) = 23.20, p = 0.002, dCohen = 1.336), (t(14) =
28.60, p = 0.002, dCohen = 1.399); C1 is slower than C2
(t(14) = 19.20, p = 0.119, dCohen = 0.656), (t(14) = 32.067, p =
0.035, dCohen = 1.284), (t(14) = 45.87, p = 0.156, dCohen =

Fig. 8. Task completion time (second) for each condition.

1.242), while C2 is slower than C4 (t(14) = 18.60, p =
0.148, dCohen = 0.816), (t(14) = 15.07, p = 0.170, dCohen =
0.765), (t(14) = 38.00, p < 0.031, dCohen = 1.413) for S1,
S2, and S3, respectively. We use False Discovery Rate [45]
(α = 0.05) for the correction of above data. The results sug-
gest that for more complicated scenes, handle bar metaphor and
viewpoint optimization techniques make interactive VR explo-
ration more eﬃcient.

User Feedback. All participants ﬁnished the experiment in 1.5
hours. No one felt dizzy with our system in 10-min tests, but
one felt a bit eye dry. All participants agreed that the gestures
are natural and easy to use, and viewpoint optimization and han-
dle bar are quite helpful. Three participants especially liked the
viewpoint optimization, as they got easily bored and tired when
navigating the map using gestures. “Viewpoint optimization is
really cool. I think too much navigation just makes users feel
bored.” There are also some negative feedbacks. Two partic-
ipants felt the HMD resolution is not high enough, which re-
duces immersive feeling. One participant was not able to ma-
nipulate the building precisely, even with the handle bar. It took
her a long time to complete the task.

5.2. Expert Feedbacks

To evaluate the applicability of UrbanVR in urban design,
we further conducted expert interviews with two independent
architects (denoted as EA & EB) other than our collaborating
architect. EA is a registered architect in Germany with more

10

Preprint submitted for review / Computers & Graphics (2021)

Fig. 9. Two building candidates are evaluated against shading and visibility using UrbanVR.

than 15 years of experience, while EB has about ﬁve years ex-
perience. In the interviews, we started by explaining the tasks,
visual interfaces, and interaction designs, and demonstrated a
case study of how UrbanVR can be utilized in a real-world sce-
nario in Singapore. The following data are received from the
collaborating architect.

• 3D models of a central business district in Singapore. The
district is about 1 km long and approximately 0.5 km wide.
The models include about 50 buildings, and a number of
other objects, such as roads and street furniture.

• Eight building models that are used as building candidates.

In the study, we ﬁrst specify a development site and a land-
mark building at back for visibility analysis. Visibility and
shadow analysis results are precomputed for the building can-
didates. Next, we select candidates and evaluate their visibility
and shading impacts. Two candidates are compared in Fig. 9.

• The left ﬁgure presents a low and wide candidate. From
the bar chart on the shading axis, we can see that the shad-
ing values are concentrated in between [0, 0.5], indicating
the candidate generates little shadow on the surrounding.
From the bar chart on the visibility axis, the values are ap-
proximately evenly distributed in between [0, 1], except
for a high concentration at 1.
If we further specify the
view position at a street corner (the turning point on the
path axis), we can see the visibility is low, which indicates
that the candidate occlude the landmark a lot at the corner.

• A tall and thin candidate is shown in the right ﬁgure. In
comparison with Fig. 9 (left), the bar distribution on the
shading axis shows relatively more values over 0.5, and
visibility values are more concentrated at 1. This indicates
that the candidate generates more shading while aﬀect less
visibility than the left one. In addition, the red line also
indicates that the candidate does not occlude the landmark
at the corner.

After the demonstration, the experts were asked to explore
the system by themselves for about 20 minutes. Both of them
repeated the case study scenario successfully. Feedbacks about
the system were collected and summarized below.

Feasibility. The experts agreed that VR is a new technology
that is certainly worth exploration for urban design. Both ex-
perts appreciated our work in applying immersive analytics for
urban design. They know some planning teams are explor-
ing VR and AR technologies. However, these works are more
“focusing on demonstrations and lack analysis”, according to
EA. In comparison, our system well integrates visualization and
analysis. EB also commented that UrbanVR provides “immer-
sive feelings of an urban design, and more importantly, being
able to modify the plan and provide quantitative results”.

Visual Design. Both experts had no motion sickness with the
visualization. They are familiar with the urban scene studied
in the work, yet they did not expect to get much more immer-
sive feelings in VR than desktop 3D visualizations. “The an-
imation is very vivid, especially when navigating on the path”
in the visibility analysis, commented by EB. The experts had
some diﬃculty in understanding the PCP in the beginning. EA
thought some simple line charts would be more eﬀective. He
was persuaded after we explained that there could be too much
line charts to represent all visibility and shadow analysis. They
liked the idea of bending time-axis into a curve, and arranging
path-axis in the same topology with the streets. “These small
adjustments give me strong visual cues of reality”, commented
by EB. The study clearly reveals that for the development site,
shading is determined by building heights, while visibility is
mainly aﬀected by building width. The collaborating architect
appreciated this ﬁnding and praised the bar chart design on the
shading and visibility axes.

Interaction Design. Both experts got used to the gesture inter-
actions quickly. They tried both building manipulation and map
navigation using gestures, and felt the gestures are easy to use in
VR. EA especially liked the handle bar metaphor, which “helps
a lot when manipulating the buildings”. In the beginning, the
experts did not realize that when visualizing the case study, the
viewpoint was optimized. After the demonstration, they liked
the idea and felt it is necessary, as EA felt “the HMD is too
heavy - not suitable for long-time wearing”.

Limitations. The experts pointed out two major limitations in
our system. First, the experts felt the “gestures are not com-
parable with mouse regarding accuracy”, even though we have

DevelopmentSiteLandmarkDevelopmentSiteLandmarkPreprint submitted for review / Computers & Graphics (2021)

11

provided handle bars for visual feedbacks. Nevertheless, they
liked gestural interactions because hand guestures are natural,
and they encouraged us to further improve the accuracy. Sec-
ond, the experts would like to see more analysis features in-
tegrated into our system, such as building functionality, street
accessibility, transportation mobility, etc. The current analyses
are not covering all necessary evaluation criteria they need.

6. Conclusion, Discussion, and Future Work

We have presented UrbanVR − an immersive analytics sys-
tem that can facilitate site development. UrbanVR integrates
a GPU-based image processing method to support quantitative
analysis, a customized PCP design in VR to present analysis
results, and immersive visualization of an urban site. In com-
parison to similar tools on the desktop (e.g., [3, 4, 17]), the main
advantage of UrbanVR is that the immersive environment gives
users the feeling of spatial presence of “being there”. This is
especially appreciated for context-aware urban design, as com-
mented by the experts. Nevertheless, the immersive environ-
ment also brings about diﬃculty for user interaction. We de-
velop several egocentric interactions, including gesture inter-
actions, viewpoint optimization, and handle bar metaphors, to
facilitate user interactions. The results of the user study show
that viewpoint optimization and handle bar metaphor can im-
prove gesture interactions, especially for more complex urban
scenes. There is an emerging trend for immersive data analysis
in various application domains. In addition to general immer-
sive visualizations, customized designs that can fulﬁll speciﬁc
analytical tasks, are also in high demand.

Discussion. There is a strong desire for immersive analyt-
ics systems that can facilitate urban design using personalized
HMDs. Examples can be found in both virtual (e.g. [10, 11])
and augmented reality (e.g. [46, 47]) environments. However,
challenges exist in every stage of system development, from
task characterization to data analysis to visualization and inter-
action designs. Through the development of UrbanVR, we gain
insightful experience. First, a close collaboration with domain
experts is necessary from the beginning, and iterative feedbacks
from domain experts can help reduce unnecessary eﬀorts. Sec-
ond, integration of advanced techniques from cross domains of
visualization, graphics, and HCI ﬁelds can greatly improve us-
ability of the system.

Currently, limited user interactions are supported by Ur-
banVR. In the gesture interaction design (Sect. 4.3.1), we do
not implement scene scaling, but only provide camera panning.
Even though both interactions can make the scene look bigger
or smaller, the experience they bring to the user is completely
diﬀerent in the VR environment, and this aﬀects user behav-
ior [48, 49]. Besides, more advanced interactions for exploring
analysis results, such as to ﬁlter and reconﬁgure the PCP [50],
are missing. This limits analysis functionality of our system.
For instance, the collaborating architect would like our system
to ﬁlter and sort designs according to a speciﬁc criteria. Ur-
banVR will be more useful if such features are integrated. A
main obstacle is that the raw gesture detection provided by Leap

Motion is not accurate enough. We look forward to more ad-
vanced interaction paradigms, such as hybrid interactions that
combine gestural and controller-based interactions [28], and
more accurate interaction algorithms, such as deep learning
techniques [51], in the near future. With more accurate and ro-
bust gesture interactions, we can further improve the interface
design, following the guidelines for interface design for virtual
reality [52].

For the moment, we integrate viewpoint optimization and
handle bar metaphor to improve gesture interactions. The user
study shows that viewport optimization (C2), handle bar (C3),
and handle bar + viewpoint optimization (C4) facilitate gesture
interaction in terms of completion time. The tasks require users
to match a building’s position, size, and orientation with a tar-
get building. Hence, the matching can also be considered as
an accuracy test. In terms of this, the handle bar also improves
the accuracy of gesture interactions. Results for viewpoint op-
timization as a supplement to gesture interactions do not dis-
tribute normally. This may be because the participants are not
familiar with viewpoint optimization. More try-outs may sup-
press the participant’s cognitive bias on viewpoint optimization.
Besides viewpoint optimization, another popular occlusion
minimization and view enhancement method is f ocus+context,
which has also been well-studied in the visualization and graph-
ics communities. Recently, some study has applied this method
to urban scenes [53]. The method allows buildings to be shifted
and scaled, thus should generate views with less occlusion than
our approach. However, the distortion of urban scene may in-
crease the burden of mental mapping of reality in the virtual
world. Considering the beneﬁts and drawbacks, it is worth an-
other study to compare the eﬀectiveness of these methods for
immersive analytics.

Future Work. There are several directions for future work.
First, we would like to implement the f ocus + context tech-
nique in VR, and compare it with the viewpoint optimization
method to check which one is more eﬀective. In addition, we
will continue collaborating with domain experts, to ﬁnd more
problems and data that are suitable for immersive analytics. To
facilitate applicability of UrbanVR, we plan to add support for
common data formats of urban planning, and integrate an ex-
port function that allows users to export their designs, in the
near future. Nevertheless, the system is expected to encounter
scalability issues when the tasks become complex and data be-
come diverse. This will bring in new research challenges and
opportunities for immersive analytics.

Acknowledgments

This work is supported partially by National Natural Sci-
ence Foundation of China (62025207) and Guangdong Basic
and Applied Basic Research Foundation (2021A1515011700).

References

[1] Burkhard, RA, Andrienko, G, Andrienko, N, Dykes, J, Koutamanis, A,
Kienreich, W, et al. Visualization summit 2007: ten research goals for
2010. Information Visualization 2007;6(3):169–188.

12

Preprint submitted for review / Computers & Graphics (2021)

[2] Delaney, B. Visualization in urban planning: they didn’t build LA in a
day. IEEE Computer Graphics and Applications 2000;20(3):10–16.
[3] Ferreira, N, Lage, M, Doraiswamy, H, Vo, H, Wilson, L, Werner, H,
et al. Urbane: A 3D framework to support data driven decision making
in urban development.
In: Proceedings of IEEE Conference on Visual
Analytics Science and Technology (VAST). 2015, p. 97–104.

[4] Ortner, T, Sorger, J, Steinlechner, H, Hesina, G, Piringer, H, Gr¨oller,
Integrating spatial and non-spatial visualization for
E. Vis-A-Ware:
visibility-aware urban planning. IEEE Transactions on Visualization and
Computer Graphics 2017;23(2):1139–1151.

[5] Waser, J, Konev, A, Sadransky, B, Horv´ath, Z, Ribicic, H, Carnecky,
R, et al. Many plans: Multidimensional ensembles for visual decision sup-
port in ﬂood management. Computer Graphics Forum 2014;33(3):281–
290.

[6] Cornel, D, Konev, A, Sadransky, B, Horvath, Z, Gr¨oller, E, Waser,
J. Visualization of object-centered vulnerability to possible ﬂood hazards.
Computer Graphics Forum 2015;34(3).

[7] Zeng, W, Ye, Y. VitalVizor: A visual analytics system for studying urban
vitality. IEEE Computer Graphics and Applications 2018;38(5):38–53.
[8] von Richthofen, A, Zeng, W, Asada, S, Burkhard, R, Heisel, F,
Mueller Arisona, S, et al. Urban mining: Visualizing the availability
of construction materials for re-use in future cities. In: Proceedings of In-
ternational Conference on Information Visualisation. 2017, p. 306 – 311.
[9] Slater, M. Measuring presence: A response to the witmer and
Presence: Teleoper Virtual Environ

singer presence questionnaire.
1999;8(5):560–565.

[10] Liu, X. Three-dimensional visualized urban landscape planning and de-
sign based on virtual reality technology. IEEE Access 2020;8:149510–
149521.

[11] Schrom-Feiertag, H, Stubenschrott, M, Regal, G, Matyus, T, Seer, S.
An interactive and responsive virtual reality environment for participatory
urban planning.
In: Proceedings of the Symposium on Simulation for
Architecture and Urban Design (SimAUD). 2020, p. 119–125.

[12] Chen, Z, Wang, Y, Sun, T, Gao, X, Chen, W, Pan, Z, et al. Explor-
ing the design space of immersive urban analytics. Visual Informatics
2017;1(2):132–142.

[13] Ens, B, Bach, B, Cordeil, M, Engelke, U, Serrano, M, Willett, W,
et al. Grand challenges in immersive analytics. In: Proceedings of the
CHI Conference on Human Factors in Computing Systems. Association
for Computing Machinery; 2021, p. 1–17.

[14] Zeng, W, Fu, CW, Mueller Arisona, S, Erath, A, Qu, H. Visualizing
mobility of public transportation system. IEEE Transactions on Visual-
ization and Computer Graphics 2014;20(12):1833–1842.

[15] Shen, Q, Zeng, W, Ye, Y, Mueller Arisona, S, Schubiger, S, Burkhard,
R, et al. StreetVizor: Visual exploration of human-scale urban forms
based on street views. IEEE Transactions on Visualization and Computer
Graphics 2018;24(1):1004 – 1013.

[16] Zheng, Y, Wu, W, Chen, Y, Qu, H, Ni, LM. Visual analytics in urban
computing: An overview. IEEE Transactions on Big Data 2016;2(3):276–
296.

[17] Miranda, F, Doraiswamy, H, Lage, M, Wilson, L, Hsieh, M, Silva,
CT. Shadow accrual maps: Eﬃcient accumulation of city-scale shadows
over time. IEEE Transactions on Visualization and Computer Graphics
2019;25(3):1559–1574.

[18] Bryson, S. Virtual reality in scientiﬁc visualization. Communications of

the ACM 1996;39(5):62–71.

[19] Dwyer, T, Marriott, K, Isenberg, T, Klein, K, Riche, N, Schreiber,
F, et al. Immersive Analytics: An Introduction. Springer International
Publishing; 2018, p. 1–23.

[20] Sicat, R, Li, J, Choi, J, Cordeil, M, Jeong, WK, Bach, B, et al. DXR:
A toolkit for building immersive data visualizations. IEEE Transactions
on Visualization and Computer Graphics 2019;25(1):715–725.

[21] Chen, Z, Su, Y, Wang, Y, Wang, Q, Qu, H, Wu, Y. MARVisT:
Authoring glyph-based visualization in mobile augmented reality. IEEE
Transactions on Visualization and Computer Graphics 2019;26(8):2645 –
2658.

[22] Yang, Y, Dwyer, T, Jenny, B, Marriott, K, Cordeil, M, Chen, H. Origin-
destination ﬂow maps in immersive environments. IEEE Transactions on
Visualization and Computer Graphics 2019;25(1):693–703.

[23] Perhac, J, Zeng, W, Asada, S, Burkhard, R, Mueller Arisona, S, Schu-
biger, S, et al. Urban fusion: Visualizing urban data fused with social
feeds via a game engine. In: Proceedings of International Conference on

Information Visualisation. 2017, p. 312–317.

[24] Kraus, M, Angerbauer, K, Buchm¨uller, J, Schweitzer, D, Keim, DA,
Sedlmair, M, et al. Assessing 2d and 3d heatmaps for comparative anal-
ysis: An empirical study.
In: Proceedings of the CHI Conference on
Human Factors in Computing Systems. Association for Computing Ma-
chinery; 2020, p. 1–14.

[25] Yang, Y, Dwyer, T, Marriott, K, Jenny, B, Goodwin, S. Tilt map: In-
teractive transitions between choropleth map, prism map and bar chart in
immersive environments. IEEE Transactions on Visualization and Com-
puter Graphics 2020;:1–1.

[26] Sherman, WR, Craig, AB. Interacting with the virtual world. In: Under-

standing Virtual Reality. Morgan Kaufmann; 2003, p. 283–378.

[27] Poupyrev,

I, Ichikawa, T, Weghorst, S, Billinghurst, M. Egocentric
object manipulation in virtual environments: Empirical evaluation of in-
teraction techniques. Computer Graphics Forum 1998;17(3):41–52.
[28] Besanc¸on, L, Ynnerman, A, Keefe, DF, Yu, L, Isenberg, T. The state
of the art of spatial interfaces for 3d visualization. Computer Graphics
Forum 2021;40(1):293–326.

[29] Huang, YJ, Fujiwara, T, Lin, YX, Lin, WC, Ma, KL. A gesture system
for graph visualization in virtual reality environments. In: Proceedings of
IEEE Paciﬁc Visualization Symposium (PaciﬁcVis). 2017, p. 41–45.
[30] Sun, Q, Lin, J, Fu, CW, Kaijima, S, He, Y. A multi-touch interface
for fast architectural sketching and massing. In: Proceedings of SIGCHI
Conference on Human Factors in Computing Systems. 2013, p. 247–256.
[31] Song, P, Goh, WB, Hutama, W, Fu, CW, Liu, X. A handle bar metaphor
for virtual object manipulation with mid-air interaction. In: Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems.
2012, p. 1297–1306.

[32] Teyseyre, AR, Campo, MR. An overview of 3D software visual-
IEEE Transactions on Visualization and Computer Graphics

ization.
2009;15(1):87–105.

[33] Ragan, ED, Scerbo, S, Bacim, F, Bowman, DA. Ampliﬁed head rotation
in virtual reality and the eﬀects on 3D search, training transfer, and spatial
orientation. IEEE Transactions on Visualization and Computer Graphics
2017;23(8):1880–1895.

[34] Bordoloi, UD, Shen, H. View selection for volume rendering. In: Pro-

ceedings of IEEE Visualization. 2005, p. 487–494.

[35] And´ujar, C, V´azquez, P, Fair´en, M. Way-Finder: guided tours
Computer Graphics Forum

through complex walkthrough models.
2004;23(3):499–508.

[36] Plemenos, D, Sbert, M, Feixas, M. On viewpoint complexity of 3D
In: International Conference GraphiCon. GraphiCon; 2004, p.

scenes.
24–31.

[37] Christie, M, Olivier, P, Normand,

JM. Camera control in computer

graphics. Computer Graphics Forum 2008;27(8):2197–2218.

[38] Munzner,

T. A nested model for visualization design and vali-
IEEE Transactions on Visualization and Computer Graphics

dation.
2009;15(6):921–928.

[39] Shen, Q, Zeng, W, Ye, Y, Arisona, SM, Schubiger, S, Burkhard,
R, et al. StreetVizor: Visual exploration of human-scale urban forms
based on street views. IEEE Transactions on Visualization and Computer
Graphics 2018;24(1):1004–1013.

[40] Zielasko, D, Horn, S, Freitag, S, Weyers, B, Kuhlen, TW. Evaluation of
hands-free hmd-based navigation techniques for immersive data analysis.
In: IEEE Symposium on 3D User Interfaces. 2016, p. 113–119.

[41] Zielasko, D, Riecke, BE. Sitting vs. standing in VR: Towards a sys-
tematic classiﬁcation of challenges and (dis)advantages. In: Proceedings
of IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts
and Workshops (VRW). 2020, p. 297–298.

[42] Verdie, Y, Lafarge, F, Alliez, P. LOD generation for urban scenes. ACM

Transactions on Graphics 2015;34(3):30:1–14.

[43] Defense Mapping Agency, . Department of defense world geodetic sys-
its deﬁnition and relationships with local geodetic systems.

tem 1984:
Tech. Rep.; 1987.

[44] Kruskal-wallis test.

In: The Concise Encyclopedia of Statistics. New

York, NY: Springer New York; 2008, p. 288–290.

[45] Benjamini, Y, Hochberg, Y. Controlling the false discovery rate: A
practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society: Series B (Methodological) 1995;57(1):289–300.
[46] Noyman, A, Sakai, Y, Larson, K. CityScopeAR: urban design and

crowdsourced engagement platform. arXiv:190708586 2019;.

[47] Noyman, A, Larson, K. DeepScope: HCI platform for generative

Preprint submitted for review / Computers & Graphics (2021)

13

cityscape visualization. In: Extended Abstracts of the CHI Conference
on Human Factors in Computing Systems. 2020, p. 1–9.

[48] Zhang, X. Multiscale traveling: crossing the boundary between space

and scale. Virtual Reality 2009;13(2):101.

[49] Langbehn, E, Bruder, G, Steinicke, F. Scale matters! analysis of dom-
inant scale estimation in the presence of conﬂicting cues in multi-scale
collaborative virtual environments. In: Proceedings of IEEE Symposium
on 3D User Interfaces (3DUI). 2016, p. 211–220.

[50] Yi, JS, a. Kang, Y, Stasko, J, Jacko, JA. Toward a deeper understanding
of the role of interaction in information visualization. IEEE Transactions
on Visualization and Computer Graphics 2007;13(6):1224–1231.
[51] Chen, Z, Zeng, W, Yang, Z, Yu, L, Fu, CW, Qu, H. LassoNet: Deep
lasso-selection of 3D point clouds. IEEE Transactions on Visualization
and Computer Graphics 2020;26(1):195–204.

[52] Bowman, DA, Hodges, LF. Formalizing the design, evaluation, and
application of interaction techniques for immersive virtual environments.
Journal of Visual Languages & Computing 1999;10(1):37–53.

[53] Deng, H, Zhang, L, Mao, X, Qu, H. Interactive urban context-aware
visualization via multiple disocclusion operators. IEEE Transactions on
Visualization and Computer Graphics 2016;22(7):1862–1874.

