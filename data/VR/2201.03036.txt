JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

In-Device Feedback in Immersive Head-Mounted
Displays for Distance Perception During
Teleoperation of Unmanned Ground Vehicles

Yiming Luo1, Jialin Wang1, Rongkai Shi1, Hai-Ning Liang1,∗ and Shan Luo2

2
2
0
2

n
a
J

9

]

C
H
.
s
c
[

1
v
6
3
0
3
0
.
1
0
2
2
:
v
i
X
r
a

Abstract—In recent years, Virtual Reality (VR) Head-Mounted
Displays (HMD) have been used to provide an immersive, ﬁrst-
person view in real-time for the remote-control of Unmanned
Ground Vehicles (UGV). One critical issue is that it is challenging
to perceive the distance of obstacles surrounding the vehicle from
2D views in the HMD, which deteriorates the control of UGV.
Conventional distance indicators used in HMD take up screen
space which leads clutter on the display and can further reduce
situation awareness of the physical environment. To address the
issue, in this paper we propose off-screen in-device feedback using
vibro-tactile and/or light-visual cues to provide real-time distance
information for the remote control of UGV. Results from a study
show a signiﬁcantly better performance with either feedback
type, reduced workload and improved usability in a driving task
that requires continuous perception of the distance between the
UGV and its environmental objects or obstacles. Our ﬁndings
show a solid case for in-device vibro-tactile and/or light-visual
feedback to support remote operation of UGVs that highly relies
on distance perception of objects.

Index Terms—Virtual Reality, Teleoperation, Tactile Feedback,

Unmanned Ground Vehicles, Distance Perception.

I. INTRODUCTION

Interest in improving real-time remote human-robot inter-
action is growing rapidly. First-person view (FPV) using VR
for unmanned ground vehicle (UGV) with remote or (semi-)
automatic control is increasingly used for search and rescue
operations, in disaster recovery, and for terrain and object
surveillance, especially in unsafe environments [1], [2]. Im-
mersive VR displays for UGV teleoperation can improve the
user’s concentration and performance in obstacles avoidance
tasks when compared to a normal display such as desktop
monitor [3]. UGV can explore the surrounding environment
and send information captured via sensors or cameras to
remotely located users in real-time. However, most cameras
attached to these robots have limitations, such as low degrees
of freedom, narrow ﬁeld-of-view, and poor photo-sensitivity,
especially in dark and complex environments with interference
from the objects or obstacles found in such environments.

Distance perception has been studied to improve user expe-
rience and task efﬁciency while using an HMD. One common
way for perceiving distance information when wearing an
HMD is to give visual distance indicators or warning signs on

1Y. Luo, J. Wang, R. Shi, H.-N. Liang are with the Department of

Computing, Xi’an Jiaotong-Liverpool University, Suzhou, China, 215028.

2S. Luo is with the Department of Computer Science, The University of
Liverpool, Liverpool, UK, and with the Department of Engineering, King’s
College London, London, UK, WC2R 2LS.

∗Corresponding author (haining.liang@xjtlu.edu.cn).

Fig. 1.
Interface from DJI VR HMD. Its interface for teleoperation uses
visual elements on the display to provide distance information, which occupy
part of the screen.

the screen (e.g., DJI Goggles1, as shown in Fig. 1). However,
these extra visual elements take up screen space, which is often
limited in HMD. The second way is to use sound feedback, but
this approach is not suitable for noisy environments [4] and is
not as accessible and accurate as visual elements. These issues
could be addressed with a real-time feedback system for FPV
UGV using tactile and off-screen visual feedback.

Current technology for vibro-tactile devices provides low-
energy, wearable, and wireless components, which offer a
good level of reliability [5]. Prior research has indicated that
vibro-tactile feedback could be employed in distance judgment
and alerting people of incoming dangers (e.g., [6], [7], [8]).
It can also improve the user’s experience and sense of the
environment. Users can feel the vibration feedback through
direct contact with the actuators. When the user is focusing
on the screen, vibro-tactile feedback can provide a second
channel to assist the user in teleoperating a UGV. In addition,
off-screen visual feedback from LED lights installed on the
periphery of the screen can also be used as a source of distance
perception information without taking up screen space in the
HMD. It is possible to have different types of sensors and
cameras on the UGV to allow them to scan the environment
and provide more comprehensive and precise information to
their remote users. This information can also be used to
provide tactile and off-screen visual feedback and offer real-
time and precise cues to users.

In this research, we explore the following research question:
when the user operates a UGV remotely using a VR HMD,
how can the user’s distance perception of objects be improved
without adding visual elements to the VR screen? To ﬁnd
answers to this, we investigated whether adding vibro-tactile
and/or light-visual feedback to an HMD could improve the
distance perception of users and their performance when

1DJI Goggles: https://www.dji.com/dji-goggles

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

driving a UGV remotely. We developed a VR HMD proto-
type to allow operating a UGV remotely. We ran a set of
experiments to assess users’ driving performance, distance
perception, workload demands, and preferences when driving
a UGV in FPV using a VR HMD. Our results show that
adding vibro-tactile feedback signiﬁcantly improved operators’
distance perception, reduced work demands, and increased
user preferences.

The main contribution of this paper is an examination
of two off-screen feedback types that can provide distance
information to users when controlling a UGV remotely. Our
VR HMD prototype using vibro-tactile actuators and LED
lights placed inside the HMD is a simple, efﬁcient, and low-
cost approach. To the best of our knowledge, this research
is the ﬁrst to apply these two types of feedback inside a VR
HMD to assist teleoperators in gauging the distance of objects
that surrounds a UGV.

II. RELATED WORK

A. Distance Perception during Teleoperation

One important issue during teleoperation, when compared
to onsite operation, is the loss of perceptual and environmental
information (e.g., obstacle distance) due to the limited ﬁeld-
of-view, low display resolution, and reduced depth perception
details of objects in the environment [9], [10]. To enhance the
view of the remote environment, researchers have attempted
to use 3D stereoscopic views instead of 2D views by using
a binocular camera to improve the depth perception and
immersion level when controlling an unmanned aerial vehicle
remotely (e.g., [11], [12], [13]). Their approach provides
additional depth information to the operator to make it easier
for the operator to estimate obstacles’ distance towards the
robot. Similarly, Brown et al. [14] explored the performance of
a spherical camera in combination with a VR HMD to supply
more intuitive multiple views and more information from
the environment to improve the teleoperation process. These
methods have focused on providing more visual information
on the display and perception issues remain for efﬁcient
remote controllability. While they can help estimate distance
information, the visual elements take up screen space and
make the interface cluttered.

Instead of cameras, approaches that use of distance sensors
as actuators include sonar [15], [16], LiDAR [17], LASER
[18], [19] can improve distance perception in teleoperation.
These sensors can provide more accurate distance information
than through video cameras. Typically, during teleoperation,
objects of interest are those close to the UGV. Sonar sensors
have stable and accurate performance when detecting these
near objects. In our research, sonar sensors are attached to the
UGV to help detect the distance between the vehicle and its
surrounding objects.

B. Vibro-tactile Feedback for Immersive Displays

Vibro-tactile feedback provided via actuators has been ap-
plied to a variety of devices, such as mobile phones or game
controllers [20], [21], to communicate information about in-
coming text messages and give warning notices of a dangerous

situation for visually impaired people, like an obstacle on
their way [22], [23]. One important feature of this type of
feedback is that it is eyes-free, allowing people to focus on
other activities. In addition, it can be sometimes hands-free,
which allows people to do other activities with their hands.

Other research has explored how to use haptic feedback
when perceived via people’s heads [23], [24], [25]. Spa-
tial (orientation) recognition, comfort, accuracy, and tolerable
vibration frequency range are important factors for vibro-
tactile devices placed inside a VR HMD [6], [24], [26]. The
installation position and placement of the actuator are quite
important because different parts of the head have varied levels
of tactile perception. For example, results in [24] show that
the hairless skin on people’s forehead is the most accurate area
for spatial discrimination. On the other hand, the occipital and
temple regions of the head are most sensitive to vibration [27].
When it comes to using vibratory feedback on the head, the
vibration frequency is also important. Research has shown that
users feel discomfort when the vibration frequency is above
150 Hz while the appropriate frequency is 32 Hz [24]. The
next factor that has an impact is the number of vibration
motors. The actuator density inﬂuences the performance when
it comes to obstacle detection or navigation tasks using vibro-
tactile feedback [26]. It has been shown that there is a negative
correlation between increased reaction time and decreased
spatial resolution when there are higher numbers of motors
[28]. In short, based on this prior research, to be effective for
VR HMD, the motor(s) should be placed inside the headset
and closer to users’ skin to maximize contact.

C. Light-visual Feedback for Immersive Displays

Using light for feedback inside VR HMD has also been
explored (e.g., in [29], [30]). Unlike visual feedback shown
on the display directly, using LED light strips placed inside
the HMD and around the two displays could also be used
to give users an off-screen non-visual cluttered way to receive
feedback. Xiao and Benko [29] presented SparseLight, a proto-
type with a matrix of LED placed in an HMD to create higher
immersive experiences. Results of their experiment show that
SparseLight is helpful in conveying peripheral information,
improving situational awareness, and reducing motion sickness
[31]. Gruenefeld et al. [32] imitated SparseLight and made
two ring-shaped LED matrices placed on the two circular
screens of the HMD to indicate direction information and
help with object selection tasks. Given their usefulness in
conveying information in an eyes-free manner, we would
also like to explore how effective light is to communicate
distance information of objects surrounding a UGV. As such,
in addition to vibro-tactile actuators, our HMD prototype
also incorporates strips of LED lights. Researchers in [33]
employed both light and vibro-tactile for feedback in an alarm
system integrated into a multi-modal HMD. The prototype
they made inspired us to design our prototype in the placement
of actuators and stimulation patterns.

III. USER STUDY
In this study, we aimed to explore whether in-device feed-
back, as an off-screen and non-auditory secondary channel,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 2. The components of the VR HMD prototype. (a) Three distance sensors were placed on the UGV; the detection range was set to be 5 cm; (b) upper
part: the communication and processing blocks on UGV and HMD; they were connected through a Bluetooth module; lower part: the placement of actuators
(coin motors and LED strips); and (c) the placement of the block on the HMD worn by a user; the block was relatively light and its placement on the HMD
was designed to achieve balance with the weight of the front part of the device; (d) A diagram of the experimental site with the six local areas (tasks) – NSP,
RAT, STT, FP, NBC and SP; (e) a photograph of how the actual site looks like. The two whiteboards represent the start and end areas.

TABLE I
THE METRICS OF THE TWO FEEDBACK TYPES (VIBRATION AND LIGHT)

Distance (d)/ cm Vibration/ PWM level
[0, 1, 2, 3, 4, 5]
>5

[155, 124, 93, 62, 31, 0]
0

Light/ lit LEDs
[5, 4, 3, 2, 1, 0]
0

is helpful to provide efﬁcient, simple, and practical distance
information to users when controlling a UGV. The prototype
used in this study offers two feedback types based on LED
lights and vibro-tactile actuators. Thus, we identiﬁed two
variables (with-without light-visual feedback × with-without
vibro-tactile feedback), which led to four conditions in total:
(1) No feedback, Baseline (N); (2) Light-visual only (L); (3)
Vibro-tactile only (V); (4) Light-visual + Vibro-tactile (LV).

This study involved 16 participants (12 males and 4 females,
aged between 19–28, M = 20.5) and followed a within-
subjects approach, where all participants experienced all four
conditions counterbalanced using a Latin square design to
reduce the learning effects. Data from the pre-experiment
questionnaire showed that it was the ﬁrst time for all 16
participants to operate a remotely-controlled UGV using an
HMD in FPV. Interviews with participants revealed that none
of them had signiﬁcant physical discomfort, health problems,
simulator sickness, or vision issues. All of them were able to
complete the pre-training session successfully.

A. Apparatus

A DJI RoboMaster S12 was used as the mobile UGV. The
FPV images from the camera were transmitted to a VR HMD,
which in our case was an HTC Vive Pro Eye3. The HMD
was connected to a desktop with 16GB RAM, an i7-9700k
CPU, a GeForce GTX 2080Ti GPU. Participants used an Xbox
wireless controller as the input device to control the UGV.

Fig. 2a shows the UGV prototype used in our experiment.
We attached three ultrasonic range sensors that can detect
distances from 2cm to 500cm with low latency. We placed the
sensors on the car where they were stable while the car was
moving and were able to sense its surroundings with precision.

2DJI RoboMaster S1: https://www.dji.com/robomaster-s1
3HTC Vive Pro:https://www.vive.com/uk/product/vive-pro-eye/specs/

There were two communication and processing blocks (see
the upper part of Fig. 2b). Each of them consists of one
Arduino Uno micro-controller and one Bluetooth module.
The two blocks were placed on the UGV and the HMD,
respectively. The camera’s images captured from the UGV
were transmitted in real-time to the HMD via an image
transmission unit.

According to [23], when the voltage on the vibration motor
is changed, we can manipulate the speed of its rotor to control
its vibration intensity. It is important to note that in coin motors
the vibration amplitude and vibration frequency are closely
linked. Each motor is controlled by pulse-width modulation
(PWM) signal from the processing block. Therefore, the haptic
stimulation changes with driving voltage. In order to make the
haptic stimulation within the acceptable range, we limited the
diving voltage to a certain range ([0, 3V] with PWM range:
[0, 155]) to meet the requirements of the vibration frequency
mentioned in [24]. As for visual stimulation, we chose the
number of lit LEDs to present the light intensity. To compare
the two feedback types, we show the metrics in Table I. The
lower part of Fig. 2b shows the ﬁnal design, with the placement
of the vibration motors and LED strips inside the HMD. Their
placement was determined by a pre-experiment with 8 pilot
users. Our main requirement was for the items to be in the
hairless area of the front and occipital regions [14]. The pre-
experiment sessions also helped determine the placement of
the LED strips, which was inspired and adapted from [29]. In
addition, the maximum comfort intensity for the vibration and
LED light strength was also determined in this pre-experiment.
Our design of these two off-screen feedback mechanisms was
efﬁcient and practical, and allowed for ease of integration into
the HMD without increasing discomfort.

In our design, the intensity of stimulation increases as the
UGV approaches the obstacles. According to [6], the transfer
functions (Step, Linear, Gamma, Linear-step, Gamma-step)
for mapping the distance between obstacles and sensor to a
vibration level of an actuator did not show signiﬁcant effects
on the number of collisions and completion time. As such,
we did not evaluate the effect of different transfer functions.
Since the main purpose of the vibration feedback is to provide
distance information, the stimulation should be strong at a very
close distance, which results in a linearly decreasing function.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

B. Tasks and Procedure

To investigate the performance of the two different types of
feedback, we asked participants to drive the robot through a
customized test site (see Fig. 2e). The site had walls made
of cardboard along the navigation path because we were
interested in observing the effect of the sensors in providing
dynamic information to users as the distance between the car
and the walls changed in real-time.

Participants had to wear the HMD prototype and drive the
UGV remotely. The driving scenario was inspired by elements
of actual driving tests. The main purpose of this test is to
complete a series of tasks such as reversing or turning, while
the UGV is not allowed to be outside of the designated areas.
The requirement of our experiment is to avoid collisions with
the walls and complete the prescribed tasks in the shortest
time possible. The number of collisions and completion time
was video-recorded with two high-deﬁnition video cameras for
later analysis.

A simple pre-training driving session was given to the
participants before they started the formal trials. The purpose
of this training was to get participants familiar with the
controllers,
the HMD and each feedback mechanism, and
controlling the robot. After this training, the participants were
asked to run the formal trials. Each participant had four trials,
one for each condition, whose order was pre-determined and
counterbalanced to minimize any learning effects.

There were six tasks involved in the driving test (see Fig. 2d)
that were mapped to the site. (1) Narrow straight passage
(NSP): The UGV needed to pass through a narrow and straight
passage with different width; (2) Right-angle turn (RAT): The
UGV had to make a right-angle turn; (3) S-type turn (STT):
The UGV needed to make an S-type turn; (4) Forward parking
(FP): The UGV had to go inside an enclosure and make a full
stop to park in a forward direction; (5) Narrow bridge crossing
(NBC): The UGV had to cross a narrow bridge; and (6) Side
parking (SP): The UGV needed to be parked sideways in the
enclosure. After each condition, the participants were given a
break and completed the NASA-TLX workload questionnaire
[34] and User Experience Questionnaire (UEQ) [35] for the
condition they just experienced.

C. Hypotheses

Based on our review of the literature and experiment design,

we formulated the following three two-part hypotheses.

• H1: with light-visual feedback would lead to less number
of collisions than no feedback; with vibro-tactile feedback
would lead to less number of collisions than no feedback;
• H2: with light-visual feedback would lead to longer com-
pletion time than no feedback; with vibro-tactile feedback
would lead to longer completion time than no feedback;
• H3: with light-visual feedback would lead to lower
simulator sickness than no feedback; with vibro-tactile
feedback would lead to less workload demands than no
feedback.

IV. RESULTS & DISCUSSION
All participants understood the nature of the tasks, and all
recorded data was valid. We discuss the results in terms of

Fig. 3.
(a) Mean number of collisions, and (b) mean completion time (s)
of each task. The error bars represent 95% conﬁdence intervals. ‘***’, ‘**’,
and ‘*’ represent a ‘.001’, ‘.01’, and ‘.05’ signiﬁcance levels, respectively. N:
No feedback, Baseline; L: Light-visual only; V: Vibro-tactile only; and LV:
Light-visual + Vibro-tactile.

participants’ performance in six tasks in each condition. A
Shapiro-Wilk test was ﬁrst performed on objective measures
separately for each condition. All the objective data followed
normal distributions (p > .05). Thus, we conducted two-way
repeated measure ANOVAs (RM-ANOVAs) for the compar-
ison of objective measurements. We applied Aligned Rank
Transform (ART) [36] on NASA-TLX and UEQ data before
performing RM-ANOVAs on them. To improve the readability,
we mainly report signiﬁcant results in this section. Post-hoc
tests were run with Bonferroni corrections if a signiﬁcant
difference was found.

A. Objective Results

The number of collisions for each trial was collected by an
experimenter frame by frame from the video and validated by
another experimenter. Based on our observations, the collisions
in our experiment could be either discrete or continuous. A
discrete collision happened when the participant immediately
controlled the UGV to leave the wall, which commonly took
around 0.5 seconds. We considered each discrete collision as
one collision. On the other hand, while less frequent, if the
participant did not adjust UGV’s direction immediately after
colliding with any part of a wall and continuously letting the
UGV hit the wall, we considered this series of collisions as
continuous collision, in which every 0.5 seconds of it was
counted as one collision.

RM-ANOVA tests revealed the mean number of collisions
differed signiﬁcantly for with-without light-visual feedback in
NSP (p < .05), RAT (p < .05), STT (p < .001) and SP
(p < .001); and for with-without vibro-tactile feedback in NSP
(p < .001), RAT (p < .001), STT (p < .001) and SP (p <
.001). In addition, there was also a signiﬁcant interaction effect
in NSP (p < .001), RAT (p < .001), NBC (p < .01) and SP
(p < .001).

Fig. 3a summarizes the mean number of collisions for each
task in each condition and the signiﬁcant differences derived

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

from post-hoc tests. It showed that having light-visual feed-
back only, having vibro-tactile feedback only, and having both
signiﬁcantly reduced the number of collisions compared to no
feedback (baseline) in NSP, RAT, STT and SP. The results
supported H1. These tasks require participants to estimate the
distance for both sides of UGV to obstacles. If there were
only images from the screen, it would be difﬁcult for users to
judge distance because the information would be out of their
ﬁeld-of-view. With our prototype, the extra feedback could
supply additional sensorial details for users and improve their
distance perception. However, the number of collisions did not
signiﬁcantly change between having light-visual and vibro-
tactile feedback together and having one of them only. The
interaction effect only occurred when one of the two feedback
types was provided. As such, we can only conﬁrm that the two
types of feedback could improve the user’s distance perception
by supplying additional information to users efﬁciently and
dynamically when they work individually. Both of the two
feedback mechanisms can provide precise and rapid distance
information.

Results of RM-ANOVA test showed that the mean comple-
tion time differed signiﬁcantly for with-without light-visual
feedback in NSP (p < .01), RAT (p < .05), STT (p < .001),
NBC (p < .05) and FP (p < .001); and for with-without
vibro-tactile feedback in NSP (p < .05) and SP (p < .05); In
addition, we also found an interaction effect in NSP (p < .01),
STT (p < .001) and NBC (p < .001). The results from post-
hoc tests were summarized in Fig. 3b.

As shown in Fig. 3b, having light-visual feedback only,
having vibro-tactile feedback only, and having both of them
signiﬁcantly increase the completion time in three tasks (NSP,
STT and, NBC) compared to no feedback (baseline) in dif-
ferent ways – H2 is largely supported. We had expected
participants to take a longer time for two reasons. One is to be
accustomed and be comfortable with the two types of feedback
and internalize their use, and the second is the additional
time to apply the changes to the UGV when receiving the
feedback information in real-time. It is important to note that
our participants had limited training prior to the experiment
and were still able to use the feedback types effectively. Based
on data collected from the interview, they said that with more
practice, they would be more efﬁcient and faster.

B. Subjective Results

RM-ANOVA tests with ART data revealed that having light-
visual feedback (p < .05) signiﬁcantly reduced ﬁve elements
of NASA-TLX workload demands (Mental, p < .001; Phys-
ical, p < .001; Temporal, p < .001; Performance, p < .001;
Frustration, p < .05). Having vibro-tactile feedback reduced
three demands (Performance, p < .001; Effort, p < .01; and
Frustration, p < .05). H3 is supported to a large extent. It
can be seen from the above results that light-visual feedback
can reduce more aspects of workload demands than vibro-
tactile feedback does. Although vibro-tactile feedback can
improve users’ conﬁdence in performance, it did not lead to
a signiﬁcant reduction in mental and physical stress.

Another RM-ANOVA test

to transformed UEQ results
showed that having light-visual feedback only (Perspicuity,

p < .05; and Dependability, p < .01) or having vibro-
tactile feedback only (Dependability, p < .001; Stimulation,
p < .001; and Novelty, p < .01) was signiﬁcantly more
popular with users. However, we did not ﬁnd interaction
effects.

According to the above results, we found that having vibro-
tactile only is more creative, exciting and motivating to users
compared to having light-visual feedback only. On the other
hand, while light-visual feedback is less novel, it is easier for
users to become familiar and comfortable with it.

V. LIMITATIONS AND FUTURE WORK

As some participants commented, both vibro-tactile and
light-visual feedback presented some usability issues, though
they were minor. There is a need to address the problems with
vibration producing small amounts of sound. Also, we need to
eliminate the problem of the LED lights making small parts
of the screen brighter, if possible.

Other types of feedback, such as non-contact haptics (ultra-
sound or wind) and smell, can be explored. In terms of distance
perception, the smell does not provide direct and practical
help. However, non-contact haptics can offer distance percep-
tion similar to vibro-tactile information while avoiding direct
contact. Direct contact can cause vibration to be transmitted
into the skull, which could further increase discomfort if the
vibration actuator is attached too tight to the head. We plan
to investigate the suitability and effectiveness of non-contact
haptics on distance perception or other tasks in the future.

We conﬁrmed that our feedback for a head is feasible to pro-
vide simple and efﬁcient distance perception when remotely
controlling the UGV, but we still need to consider some issues.
In a complex environment that a UGV is navigating in general,
the requirements of the accuracy of the distance information
are higher, which increases the difﬁculty of our simple dis-
tance perception feedback to provide enough information with
limited actuators. Due to the resolution of the perception of
human head is limited, the number of the actuators cannot
be large [28]. Therefore, we recommend using our technology
under the premise of fewer actuators and rapid response. When
more information is needed, it is still recommended to obtain
the information on the screen.

When there are primary and secondary tasks at the same
time, the cognitive load could increase. In the future, we also
plan to investigate this by evaluating the cognitive load level
for each feedback and the combination of them.

VI. CONCLUSION

This paper presented an in-device feedback system for
distance perception for unmanned ground vehicles (UGV)
controlled remotely via a virtual reality head-mounted display
(VR HMD). It uses on-vehicle sensors and vibro-actuators
and LED lights placed inside the HMD to provide distance
information of obstacles around the UGV. Results from a
driving task experiment show that the additional feedback
allowed for effective navigation, with users being able to
detect the distance with relatively high precision between the
UGV and surrounding obstacles in real-time. This additional

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

sensory information allowed users to move the UGV in a
more precise manner and led to a more positive experience.
While both vibro-tactile and light-visual feedback are helpful,
each of them has its advantages. Light-visual is easier to get
accustomed to using, while vibro-tactile feedback seems to
provide higher accuracy, is more exciting and novel to use,
and is more resistant to interference under complex driving
conditions. In short, as our results show, in-device feedback
using vibro-tactile actuators and LED lights is a simple,
efﬁcient, and low-cost approach to provide precise distance
information for UGVs controlled via a VR HMD in real-time.

ACKNOWLEDGMENT

The authors would like to thank the participants who joined
the study and the reviewers for their insightful comments
that helped to improve our paper. This work was supported
in part by Xi’an Jiaotong-Liverpool University–Key Special
Fund (#KSF-A-03), the Future Network Scientiﬁc Research
Fund (#FNSRFP-2021-YB-41), and Engineering and Physical
Sciences Research Council (#EP/T033517/1).

REFERENCES

[1] S. Kim, Y. Kim, J. Ha, and S. Jo, “Mapping system with virtual reality
for mobile robot teleoperation,” in 2018 18th International Conference
on Control, Automation and Systems (ICCAS).
IEEE, 2018, pp. 1541–
1541.

[2] R. R. Murphy, “Human-robot

interaction in rescue robotics,” IEEE
Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), vol. 34, no. 2, pp. 138–153, 2004.

[3] Y. Luo, J. Wang, H.-N. Liang, S. Luo, and E. G. Lim, “Monoscopic vs.
stereoscopic views and display types in the teleoperation of unmanned
ground vehicles for object avoidance,” in 2021 30th IEEE International
Conference on Robot & Human Interactive Communication (RO-MAN).
IEEE, 2021, pp. 418–425.

[4] H.-G. Hirsch and D. Pearce, “The aurora experimental framework for
the performance evaluation of speech recognition systems under noisy
conditions,” in ASR2000-Automatic speech recognition: challenges for
the new Millenium ISCA tutorial and research workshop (ITRW), 2000,
pp. 181–188.

[5] M. Lange, F. Steinicke, G. Bruder et al., “Vibrotactile assistance for
user guidance towards selection targets in vr and the cognitive resources
involved,” in 2017 IEEE Symposium on 3D User Interfaces (3DUI).
IEEE, 2017, pp. 95–98.

[6] D. Valkov and L. Linsen, “Vibro-tactile feedback for real-world aware-
ness in immersive virtual environments,” in 2019 IEEE Conference on
Virtual Reality and 3D User Interfaces (VR).
IEEE, 2019, pp. 340–349.
[7] F. Ahmed, J. D. Cohen, K. S. Binder, and C. L. Fennema, “Inﬂuence
of tactile feedback and presence on egocentric distance perception in
virtual environments,” in 2010 IEEE Virtual Reality Conference (VR).
IEEE, 2010, pp. 195–202.

[8] J. M. Van De Lagemaat, I. A. Kuling, and Y. Visell, “Tactile distances are
greatly underestimated in perception and motor reproduction,” in 2018
IEEE Haptics Symposium (HAPTICS).

IEEE, 2018, pp. 301–306.

[9] T. M. Lam, V. D’Amelio, M. Mulder, and M. Van Paassen, “UAV
tele-operation using haptics with a degraded visual interface,” in 2006
IEEE International Conference on Systems, Man and Cybernetics, vol. 3.
IEEE, 2006, pp. 2440–2445.

[10] G. L. Calhoun, M. H. Draper, J. T. Nelson, and H. A. Ruff, “Advanced
display concepts for UAV sensor operations: Landmark cues and picture-
in-picture,” in Proceedings of
the Human Factors and Ergonomics
Society Annual Meeting, vol. 50, no. 1. SAGE Publications Sage CA:
Los Angeles, CA, 2006, pp. 121–125.

[11] N. Smolyanskiy and M. Gonzalez-Franco, “Stereoscopic ﬁrst person
view system for drone navigation,” Frontiers in Robotics and AI, vol. 4,
p. 11, 2017.

[12] D.-H. Kim, Y.-G. Go, and S.-M. Choi, “An aerial mixed-reality envi-
ronment for ﬁrst-person-view drone ﬂying,” Applied Sciences, vol. 10,
no. 16, p. 5436, 2020.

[13] H. Shin, Y. Kim, J. Cho, and Y.-J. Cho, “A tele-operated gesture
recognition mobile robot using a stereo vision,” IFAC Proceedings
Volumes, vol. 41, no. 2, pp. 12 661–12 666, 2008.

[14] K. Kr¨uckel, F. Nolden, A. Ferrein, and I. Scholl, “Intuitive visual
teleoperation for UGVs using free-look augmented reality displays,”
in 2015 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2015, pp. 4412–4417.

[15] S. Karakaya, G. K¨uc¸ ¨ukyildiz, and H. Ocak, “Detection of obstacle-free
gaps for mobile robot applications using 2-D lidar data.” International
Journal of Natural & Engineering Sciences, vol. 10, no. 3, pp. 23–27,
2016.

[16] G. A. Kumar, A. K. Patil, R. Patil, S. S. Park, and Y. H. Chai, “A
LiDAR and IMU integrated indoor navigation system for UAVs and its
application in real-time pipeline classiﬁcation,” Sensors, vol. 17, no. 6,
p. 1268, 2017.

[17] N. Shalal, T. Low, C. McCarthy, and N. Hancock, “Orchard mapping and
mobile robot localisation using on-board camera and laser scanner data
fusion–part b: Mapping and localisation,” Computers and Electronics in
Agriculture, vol. 119, pp. 267–278, 2015.

[18] T. Lim, C. Yeong, E. Su, S. Chik, P. Chin, and P. Tan, “Performance
evaluation of various 2-D laser scanners for mobile robot map building
and localization,” Journal of Telecommunication, Electronic and Com-
puter Engineering (JTEC), vol. 8, no. 11, pp. 105–109, 2016.

[19] S. Livatino, G. Muscato, S. Sessa, and V. Neri, “Depth-enhanced mobile
robot teleguide based on laser images,” Mechatronics, vol. 20, no. 7, pp.
739–750, 2010.

[20] L. M. Brown, S. A. Brewster, and H. C. Purchase, “Multidimensional
tactons for non-visual information presentation in mobile devices,” in
Proceedings of the 8th conference on Human-computer interaction with
mobile devices and services, 2006, pp. 231–238.

[21] K. Gilliland and R. E. Schlegel, “Tactile stimulation of the human head
for information display,” Human Factors, vol. 36, no. 4, pp. 700–717,
1994.

[22] F. Alary, M. Duquette, R. Goldstein, C. E. Chapman, P. Voss, V. La Buis-
sonni`ere-Ariza, and F. Lepore, “Tactile acuity in the blind: a closer look
reveals superiority over the sighted in some but not all cutaneous tasks,”
Neuropsychologia, vol. 47, no. 10, pp. 2037–2043, 2009.

[23] L. Brayda, C. Campus, R. Chellali, G. Rodriguez, and C. Martinoli,
“An investigation of search behaviour in a tactile exploration task
for sighted and non-sighted adults.” in CHI’11 Extended Abstracts on
Human Factors in Computing Systems, 2011, pp. 2317–2322.

[24] K. Myles and J. T. Kalb, “Guidelines for head tactile communication
(report arl-tr-5116),” Aberdeen Proving Ground, MD: Army Research
Laboratory, 2010.

[25] O. B. Kaul and M. Rohs, “Haptichead: A spherical vibrotactile grid
around the head for 3d guidance in virtual and augmented reality,”
in Proceedings of the 2017 CHI Conference on Human Factors in
Computing Systems, ser. CHI ’17. New York, NY, USA: Association
for Computing Machinery, 2017, p. 3729–3740.

[26] V. A. de Jesus Oliveira, L. Nedel, A. Maciel, and L. Brayda, “Localized
magniﬁcation in vibrotactile HMDs for accurate spatial awareness,” in
International Conference on Human Haptic Sensing and Touch Enabled
Computer Applications. Springer, 2016, pp. 55–64.

[27] V. A. de Jesus Oliveira, L. Nedel, A. Maciel, and L. Brayda, “Spatial
discrimination of vibrotactile stimuli around the head,” in 2016 IEEE
Haptics Symposium (HAPTICS).

IEEE, 2016, pp. 1–6.

[28] C. E. Rash, M. B. Russo, T. R. Letowski, and E. T. Schmeisser, “Helmet-
mounted displays: Sensation, perception and cognition issues,” ARMY
AEROMEDICAL RESEARCH LAB FORT RUCKER AL, 2009.

[29] R. Xiao and H. Benko, “Augmenting the ﬁeld-of-view of head-mounted
displays with sparse peripheral displays,” in Proceedings of the 2016
CHI Conference on Human Factors in Computing Systems, 2016, pp.
1221–1232.

[30] E. Koskinen, I. Rakkolainen, and R. Raisamo, “Direct retinal signals for
virtual environments,” ser. VRST ’17. New York, NY, USA: Association
for Computing Machinery, 2017, pp. 1–2.

[31] R. Shi, H.-N. Liang, Y. Wu, D. Yu, and W. Xu, “Virtual reality sickness
mitigation methods: A comparative study in a racing game,” Proceedings
of the ACM on Computer Graphics and Interactive Techniques, vol. 4,
no. 1, pp. 1–16, 2021.

[32] U. Gruenefeld, T. C. Stratmann, A. E. Ali, S. Boll, and W. Heuten, “Ra-
diallight: Exploring radial peripheral leds for directional cues in head-
mounted displays,” in Proceedings of the 20th International Conference
on Human-Computer Interaction with Mobile Devices and Services,
2018, pp. 1–6.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Hai-Ning Liang is Professor of Computing at Xi’an
Jiaotong-Liverpool University, China, where is also
Head of the Department of Computing. He received
his PhD in computer science from Western Uni-
versity, Canada. His main research interests fall in
the area of human-computer interaction, focusing on
virtual/augmented reality, visualization, and gaming
technologies. He has published widely in highly
rated journals and conferences in these areas, such
as ACM ToG, ACM ToCHI, ACM IMWUT, IEEE
TVCG, ACM CHI, IEEE VR, IEEE ISMAR.

[33] V. Cobus, W. Heuten, and S. Boll, “Multimodal head-mounted display
for multimodal alarms in intensive care units,” in Proceedings of the 6th
ACM International Symposium on Pervasive Displays, 2017, pp. 1–2.

[34] S. G. Hart, “Nasa-task load index (NASA-TLX); 20 years later,”
Proceedings of the Human Factors and Ergonomics Society Annual
Meeting, vol. 50, no. 9, pp. 904–908, 2006.

[35] B. Laugwitz, T. Held, and M. Schrepp, “Construction and evaluation of a
user experience questionnaire,” in HCI and Usability for Education and
Work, A. Holzinger, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg,
2008, pp. 63–76.

[36] J. O. Wobbrock, L. Findlater, D. Gergle, and J. J. Higgins, “The aligned
rank transform for nonparametric factorial analyses using only anova
procedures,” in Proceedings of the SIGCHI conference on human factors
in computing systems, 2011, pp. 143–146.

Yiming Luo is currently a PhD student in the De-
partment of Computing at Xi’an Jiaotong-Liverpool
University, China. His research interests focus on
robotics, human-robot interaction, and virtual reality.

Jialin Wang is currently a ﬁrst year PhD student
in the Department of Computing at Xi’an Jiaotong-
Liverpool University, China. His research interests
include virtual reality, robotics, and computer graph-
ics.

Rongkai Shi is currently a PhD student in the De-
partment of Computing at Xi’an Jiaotong-Liverpool
University, China. His research interests focus on
virtual reality, augmented reality, and interaction
design.

Shan Luo is Senior Lecturer (Associate Professor)
at the Department of Engineering, King’s College
London. Previously, he was Lecturer at the Univer-
sity of Liverpool, and Research Fellow at Harvard
University and University of Leeds. He was also
a Visiting Scientist at the Computer Science and
Artiﬁcial Intelligence Laboratory (CSAIL), MIT. He
received the BEng degree in Automatic Control from
China University of Petroleum, Qingdao, China, in
2012. He was awarded the PhD degree in Robotics
in 2016. His
from King’s College London, UK,
research interests include tactile sensing, robot learning and robot visual-tactile
perception.

