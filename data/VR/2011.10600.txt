0
2
0
2

v
o
N
0
2

]

V
C
.
s
c
[

1
v
0
0
6
0
1
.
1
1
0
2
:
v
i
X
r
a

ATSal: An Attention Based Architecture for
Saliency Prediction in 360◦ Videos

Yasser Dahou1,(cid:63), Marouane Tliba2, Kevin McGuinness1, and Noel O’Connor1

1 Insight Centre for Data Analytics, Dublin City University, Dublin 9, Ireland
2 Institut National des T´el´ecommunications et des TIC, Oran, Algeria
yasser.dahoudjilali2@mail.dcu.ie

Abstract. The spherical domain representation of 360◦ video/image
presents many challenges related to the storage, processing, transmission
and rendering of omnidirectional videos (ODV). Models of human visual
attention can be used so that only a single viewport is rendered at a
time, which is important when developing systems that allow users to ex-
plore ODV with head mounted displays (HMD). Accordingly, researchers
have proposed various saliency models for 360◦ video/images. This paper
proposes ATSal, a novel attention based (head-eye) saliency model for
360◦ videos. The attention mechanism explicitly encodes global static
visual attention allowing expert models to focus on learning the saliency
on local patches throughout consecutive frames. We compare the pro-
posed approach to other state-of-the-art saliency models on two datasets:
Salient360! and VR-EyeTracking. Experimental results on over 80 ODV
videos (75K+ frames) show that the proposed method outperforms the
existing state-of-the-art.

Keywords: Omnidirectional video (ODV), head and eye saliency pre-
diction, deep learning.

1

Introduction

360◦ video, referred to as panoramic, spherical or omnidirectional video, is a
recently introduced type of multimedia that provides the user with an immersive
experience. The content of ODV is rendered to cover the entire 360 × 180 viewing
space. Humans, however, naturally focus on the most attractive and interesting
ﬁeld-of-views (FoV) while ignoring others in their visual ﬁeld, using a set of visual
operations know as visual attention. Such selective mechanisms allow humans
to interpret and analyze complex scenes in real time and devote their limited
perceptual and cognitive resources to the most pertinent subsets of sensory data.
Inspired by this visual perception phenomenon, saliency prediction or modeling is
the process that aims to model the gaze ﬁxation distribution patterns of humans
on static and dynamic scenes. Modeling visual attention in omnidirectional video
is an important component of being able to deliver optimal immersive experiences.

(cid:63) Corresponding author

 
 
 
 
 
 
2

Y. Dahou et al.

The main objective is to predict the most probable viewports in a frame reﬂecting
the average person’s head movements (HM) and eye movements (EM) reﬂecting
the region-of-interest (RoI) inside the predicted viewport. Thus, when predicting
the most salient pixels, it is necessary to predict both HM and EM for 360◦
video/image visual attention modeling.

Despite the remarkable advances in the ﬁeld of visual attention modeling on a
ﬁxed viewport (see [3–5] for a comprehensive review), saliency prediction studies
on 360◦ video/image are still limited. This is in part due to the comparatively
small amount of research that has investigated the visual attention features that
aﬀect human perception in panoramic scenes. This is further compounded by the
lack of commonly used large-scale head and eye-gaze datasets for 360◦ content
as well as the associated diﬃculties of using this data compared with publicly
available 2D stimuli datasets.

A recent survey conducted by Xu et al. [1] reviewed many works for predicting
the HM/EM saliency maps of 360◦ images and video that model the probability
distribution of viewports/RoIs of multiple subjects. Rai et al. [17] derived the
volcano-like distribution of EM with the viewport. Results show that eye-gaze
ﬁxations are quasi-isotropically distributed in orientation, typically far away from
the center of the viewport. Furthermore, ODV presents some statistical biases, as
investigated in [18]. Human attention on 360◦ video is biased toward the equator
and the front region, known as the equator bias, which could be leveraged as
priors in modelling. Along with the statistical bias, a subject’s attention is driven
by the most salient objects in the scene. It has been shown that a smaller number
of closer objects capture more human attention [19].

Motivated by this, our novel approach combines global visual features over
the entire view with features derived from local patches in consecutive frames. A
good set of features are those that share the minimal information necessary to
perform well at the saliency task. The goal of our representation is to capture
as much information as possible about the stimulus. This technique results in
state-of-the-art accuracy on standard benchmarks.
The contributions of this paper are as follows:

– We demonstrate the importance of global visual attention features in achieving

better saliency performance.

– A new approach, ATSal, is presented that combines an attention mechanism
with expert instances for each patch location to learn eﬀective features for
saliency modelling.

– We pre-process the VR-EyeTracking [6] dataset by extracting the well an-
notated ﬁxation/saliency maps from the provided raw data and make this
available to others to use at this link.

– We compare our approach against a representative selection of state-of-the-art
360◦ approaches on both the VR-EyeTracking and Salient360! datasets.

The rest of the paper is organized as follows: Section 2 provides an overview
of related 360◦ video saliency works. Section 3 gives a detailed description of the
proposed framework. Section 4 compares the experimental results to state-of-

ATSal: a saliency prediction model for 360◦ videos

3

the-art methods. Finally, we conclude this work in Section 5. The results can be
reproduced with the source code and trained models available on GitHub: link.

2 Related work

In this section, we present the important works related to attention modelling
for 2D dynamic stimuli and 360◦ video/image. They mainly refer to predicting
the HM/EM saliency maps of 360◦ video/images and can be further grouped
into heuristic approaches and data-driven approaches.

2.1 2D dynamic saliency models

Video saliency prediction has advanced signiﬁcantly since the advent of deep
learning. Recently, many works have investigated deep neural networks (DNNs)
for video saliency prediction (e.g. [7–13]). SalEMA [10] added a conceptually
simple exponential moving average of an internal convolutional state to the
SalGAN network [14]. 3DSAL [11] performs 3D convolutions on the generated
VGG-16 features, while using a weighting cuboid function to smooth the spatial
features; the novel contribution is learning saliency by fusing spatio-temporal
features. Unisal [7] proposed four novel domain adaptation techniques to enable
strong shared features: domain-adaptive priors, domain-adaptive fusion, domain-
adaptive smoothing, and Bypass-RNN. Unisal achieved the state-of-the-art results
on the DHF1K benchmark [12] (e.g. AUC-J: 0.901, NSS: 2.776).

2.2 360◦ Heuristic approaches

The heuristic approaches encode saliency on the 360◦ sphere using handcrafted
features. The pioneer works by Iva et al. [15, 16] generate the spherical static
saliency map by combining together chromatic, intensity, and three cue conspicuity
maps after normalization, through multiscale analysis on the sphere. They build
the motion pyramid on the sphere by applying block matching and varying the
block size. Finally, the two maps are fused to produce the dynamic saliency map
on the sphere. Unfortunately, no quantitative results were provided in [15, 16]
since the HMD was not yet available at the time. RM3 [26] for 360◦ images,
combines low-level features hue, saturation and GBVS features, with the high-
level features present in each viewport. Unlike [15,16,26], Fang et al. [27] proposed
the extraction of low-level features of color, texture luminance, and boundary
connectivity from the super-pixels at multiple levels segmented from the input
equirectangular projection (ERP) image.

Other works have adapted existing 2D saliency models into 360◦ video/images.
This approach, however, suﬀers from geometric distortion and border artifacts.
Abreu et al. [20] introduced a fused saliency map (FSM) approach to HM saliency
prediction on 360◦ images; they adapted SALICON [21] (a 2D image saliency
prediction model) to 360◦ images using ERP. Lebreton et al. [22] extended the 2D
Boolean Map Saliency (BMS [23]) and Graph-Based Visual Saliency (GBVS [24])

4

Y. Dahou et al.

models to integrate the properties for equirectangular images, naming their
approaches BMS360 and GBVS360 respectively. Maugey et al. [25] applied a 2D
saliency model on each face generated under the cubemap projection (CMP).
Recently, it has become easier to collect HM and EM data and thus there have
emerged many end-to-end saliency prediction approaches for 360◦ video/images.

2.3 360◦ Data-driven approaches

Trained on recently published datasets [6, 18, 28, 29], a number of deep neu-
ral netrowk (DNN) saliency prediction approaches for 360◦ video have been
proposed [18, 29–35]. Nguyen et al. [33] ﬁne-tuned the PanoSalNet 2D static
model on 360◦ video datasets to predict HM saliency map of each frame without
considering the temporal dimension. The predicted saliency is enhanced by a
prior ﬁlter based on an a-priori statistical bias. Cheng et al. [30] proposed a
DNN-based spatial-temporal network, consisting of a static model and a Con-
vLSTM module to adjust the outputs of the static model based on temporal
features. They also aggregated a “Cube Padding” technique in the convolution,
pooling, and convolutional LSTM layers to keep connectivity between the cube
faces by propagating the shared information across the views. Lebreton et al. [32]
extended BMS360 [22] to V-BMS360 by adding a temporal bias and optical
ﬂow-based motion features. Hu at al. [35] proposed a deep learning-based agent
for automatic piloting through 360◦ sports videos. At each frame, the agent
observes a panoramic image and has the knowledge of previously selected viewing
angles. The developed online policy allows shifting the current viewing angle to
the next preferred one through a recurrent neural network. Fang et al. [36] ﬁne
tuned SalGAN [4] on the Salient360! image dataset with a new loss function
combining three saliency metrics. Qiao et al. [37] proposed a Multi-Task Deep
Neural Network (MT-DNN) model for head movement prediction; the center of
each viewport is spatio-temporally aligned with 8 shared convolution layers to
predict saliency features.

Unlike all previous approaches, Zhang et al. [29] proposed a spherical convo-
lutional neural network, spherical U-NET, trained following the teacher forcing
technique, for applying a planar saliency CNN to a 3D geometry space, where
the kernel is deﬁned on a spherical crown, and the convolution involves the
rotation of the kernel along the sphere. The model input includes one frame and
the predicted saliency maps of several previous frames to allow for improved
modelling of dynamic saliency. The authors also deﬁned a spherical MSE (S-MSE)
loss function for training the spherical U-Net to reduce the non-uniform sampling
of the ERP. Furthermore, instead of using supervised learning to learn saliency
from data, Mai at al. [18] applied deep reinforcement learning (DRL) to predict
HM positions by maximizing the reward of imitating human HM scanpaths
through the agent’s actions. The reward, which measures the similarity between
the predicted and ground-truth HM scanpaths, is estimated to evaluate the action
made by the DRL model. The reward is then used to make a decision on the
action through the DRL model: i.e., the HM scanpath in the current frame.

ATSal: a saliency prediction model for 360◦ videos

5

Fig. 1. Network Architecture of ATSal. The attention stream is for extracting global
saliency. The experts model operates on local patches. The ﬁnal saliency is obtained by
pixel-wise multiplication between the two maps generated by each stream.

It is clear from the above review that the number of models targeting HM and
EM 360◦ video visual attention modeling is still considerably limited compared
with 2D video saliency prediction. There is still much work to be done to meet
the speciﬁc requirements of ODV. The contributions of this paper as outlined
above attempt to address this.

3 Proposed model

This section describes our framework (see Figure. 1). The design consists of an
attention model, expert models, and the fusion method.

Our approach operates in two parallel streams. One stream is dedicated to
extracting global attention statistics via the attention mechanism applied on
the ERP frame. This explicitly captures static saliency information allowing the
second steam to learn more eﬀective temporal features. Moreover, the expert
models embed two instances of SalEMA on the cube faces, SalEMA Poles
for the Zenith and Nadir and SalEMA Equator specialized for the equator
viewports. This is motivated by ﬁndings that investigated the properties of
panoramic scenes that argued that the ﬁxations distribution is highly correlated
with the locations of the viewports [37]. Thus, we adapted SalEMA weights to
meet these requirements.

6

Y. Dahou et al.

3.1 Attention mechanism

Despite the strong research interest and investigations of saliency prediction over
360◦ stimuli, to the best of our knowledge, no previous work has exploited the
global attention eﬀect related to omnidirectional scenes in general and for head
and eye saliency prediction in particular. We address this by implementing an
attention mechanism in a parallel stream which is able to encode the global
statistics of the 360◦ video/image. A dense mask is learned at the middle of the
bottleneck with an enlarged receptive ﬁeld, and this attention mask is combined
by performing pixel-wise multiplication with the feature map followed by pixel-
wise summation among the channels to obtain the ﬁnal optimal latent variable
representation, thereby embedding as much information as possible about the
input frame.

Why an attention mechanism? Recent studies [38–41] have shown the
eﬀectiveness of attention mechanisms for several computer vision downstream
tasks. In such approaches, attention is learned in an adaptive, joint, and task-
oriented manner, allowing the network to focus on the most relevant parts of the
input space. Similar to [12], our latent attention module extracts strong spatial
saliency information and was trained on both Salient360! [42] and Sitzman [43]
360 image saliency datasets in a supervised fashion. Such a design leads to
generalization of the prediction performance across diﬀerent 360 saliency datasets.
As shown in Fig.2, the input is an ERP image X ∈ R640×320×3. This shape implies
the preservation of the initial spherical characteristics of the data distribution,
which is of ratio 2:1. However, a larger receptive ﬁeld is needed to propagate each
pixel-wise response to a ﬁeld of 640×320 which is the suﬃcient condition of having
the global attention. The approach is motivated by the following considerations:

Table 1. Attention model results on Salient360! validation images.

AUC-J ↑ NSS ↑

Salient360!
CC ↑

SIM ↑

KLD ↓

Attention model

0.837

1.680

0.631

0.629

0.801

– As investigated in [44], human attention is driven by complementary pathways,
which compete on global versus local features. Global features such as:
semantics, pose, and spatial conﬁguration, called scene layout or “gist”, guides
the early visual processing. In a panoramic scenario, the CMP prediction
forces the model to lose the global contextual information while considering
each face separately. Through the attention mechanism, the auto-encoder is
forced to learn a more explicit global static representation. The global and
local features are disentangled for robust predictions.

– The receptive ﬁeld of the encoder needs to cover the entire input. To this
end, we modiﬁed the VGG-16 encoder, by deleting the last two max pooling

ATSal: a saliency prediction model for 360◦ videos

7

layers to better preserve spatial frequencies, and set the max poling layer-3
stride factor and kernel size from (2 → 4). This replaces the initial VGG-16
receptive ﬁeld (212 × 212) with an enlarged one of (244 × 244) but this is still
not suﬃcient to cover the whole input X ∈ R640×320×3. To tackle this issue,
we extended the conv5 layer with an attention mechanism containing several
convolution layers interspersed with pooling and up-sampling operations.
Given the latent variable:

z1 = fθ(X) ∈ R20×40×512,

(1)

where fθ is the encoder, the attention mechanism yields a dense attention
map M ∈ (cid:2)0, 1]20×40 with an enlarged receptive ﬁeld of 676 × 676.

The attention module is activated with a sigmoid function, which relaxes the
sum to one constraint often used in softmax based neural attention. The feature
map z1 is further enhanced by:

zc = (1 + M ) ◦ zc
1,

(2)

where c ∈ {1, ..., 512} is the channel index and M operates as a feature wise
selector on z1, with the residual connection to keep all the useful information [45].
The optimal latent variable z ∈ R20×40×512 is fed to the decoder to learn the
predicted saliency map Y1 ∈ R640×320. Table. 2 summarizes the architecture of
the attention mechanism.

Layer (type)

MaxPool2D 1
Conv2D 1
ReLU
Conv2D 2
ReLU
MaxPool2D 2
Conv2D 3
ReLU
Conv2D 4
ReLU
Conv2D 5
Upsample
Sigmoid

Table 2. Architecture of the attention mechanism

Output shape

# Parameters

1 × 512 × 10 × 20
1 × 64 × 10 × 20
1 × 64 × 10 × 20
1 × 128 × 10 × 20
1 × 128 × 10 × 20
1 × 128 × 5 × 10
1 × 64 × 5 × 10
1 × 64 × 5 × 10
1 × 128 × 5 × 10
1 × 128 × 5 × 10
1 × 1 × 5 × 10
1 × 1 × 20 × 40
1 × 1 × 20 × 40

Total Parameters: 516,609

0
294,976
0
73,856
0
0
73,792
0
73,856
0
129
0
0

3.2 Expert models

All previous works learn the same network parameters for the six faces of the
cube. However, the saliency density is mostly represented in the equator, forcing

8

Y. Dahou et al.

the network to over estimate ﬁxations in the poles, which is one of the main
reasons the prediction performance drops. In short, the expert stream of our
framework instantiates two expert versions from SalEMA [10] and combines their
results through the inverse projection to predict the ﬁnal saliency. The main
point of the expert stream is to predict 360◦ dynamic saliency viewports based on
both spatiotemporal content and viewport location. Accordingly, each viewport
is predicted independently with shared weights. The input frame in ERP format
X is ﬁrst projected to the CMP format resulting in xi, i ∈ {1, ..6}. Where x0,1,2,3
represents the front, left, right and back views, and x4,5 are the Zenith and
Nadir views. The original SalEMA was trained on DHF1K dataset using the
binary cross entropy (BCE) loss and a SalGAN encoder-decoder with an added
exponential moving average (EMA) recurrence. The exponential weighted average
takes the state St output by a convolutional layer at time t. The output Et is
then propagated the next layers using a convex combination of the state St and
previous states:

Et = αSt + (1 − α)Et−1.

(3)

with initial α = 0.1. We further changed the training protocol of SalEMA, by
adjusting each corresponding cube face into a batch of 80 for the poles, but we
kept the initial 10 for the equators; also, we kept BCE as the objective function
for ﬁne-tuning SalEMA. This is motivated by the low motion present in each
viewport (see [46]). SalEMA Poles was ﬁne-tuned on batches of the faces x4,5
to capture the underlying features on these viewports, while SalEMA Poles
was adapted to faces x0,1,2,3. The generated local saliency per cube face is then
inversely projected into the ERP format using CM P −1; we denote Y2 ∈ R640×320
the generated saliency map after the inverse projection. The ﬁnal saliency map
Yt is obtained after pixel wise multiplication between Y1 and Y2:

Y = Y1 ◦ Y2 ∈ (cid:2)0, 1]640×320

(4)

3.3 Loss function for the attention stream

According to [47], the saliency metrics cover diﬀerent aspects of the saliency map.
Thus, we deﬁne the loss function as a combination between the Kullback-Leibler
Divergence (KL) and the Normalized Scanpath Saliency (NSS). We denote the
predicted saliency Y1 ∈ (cid:2)0, 1]640×320, the ﬁxation map as F ∈ {0, 1}640×320, the
dense mask at the middle of the bottleneck M ∈ (cid:2)0, 1]20×40, and the continuous
saliency map obtained after blurring F with a Gaussian ﬁlter (σ = 9.35◦) as
Q1 ∈ (cid:2)0, 1]640×320. Q2 ∈ (cid:2)0, 1]40×20 is the down-sampled version. The attention
stream loss function is deﬁned as follows:

LT (Y1, M, F, Q1, Q2) = α1LKL(Y1, Q1)+
α2LNSS(Y1, F ) + βLKL(M, Q2)

where α2 = β = 0.2, and α2 = 0.8. LKL is chosen as the primary loss:

LKL(Y1, Q1) =

(cid:18)

Q1i log

(cid:15) +

(cid:19)

.

Q1i
(cid:15) + Y1i

(cid:88)

i

(5)

(6)

ATSal: a saliency prediction model for 360◦ videos

9

Fig. 2. Qualitative results of our ATSal model and ﬁve other competitors on sample
frames from VR-EyeTracking and Salient360! datasets. It can be observed that the
proposed ATSal is able to handle various challenging scenes well and produces consistent
omnidirectional video saliency results.

LNSS is derived from the NSS metric, which is a similarity metric. We therefore
optimize its negative:

LN SS(Y1, F ) = −

1
N

(cid:88)

i

¯Y1i × Fi,

(7)

where N = (cid:80) Fi and ¯Y1i = (Y1i − µ(Y1i))/σ(Y1i).

4 Experiments

4.1 Experimental setup

Training. The attention model was trained in two stages. First, to encode static
saliency, we trained the attention model on 360◦ images from the Salient360! and
Sitzman datasets. Due to the small amount of labelled static data (103 omni-
directional images), we applied some common data augmentation techniques:
mirroring, rotations, and horizontal ﬂipping. The resulting dataset contains 2,368

10

Y. Dahou et al.

Fig. 3. Overlaid saliency of the attention model, experts model, and the ﬁnal ATSal. It
can be seen that the attention model captures more global saliency, while the experts
focus on local patches. ATSal gathers both features for the ﬁnal prediction.

360◦ images. Results on the images Salient360! validation set are shown in Ta-
ble. 1. For dynamic stimuli, we consider 2 settings: training both streams on: (i)
Video-Salient360!, (ii) VR-EyeTracking. For Salient360!, we used 15 ODV for
training and 04 for validation. For VR-EyeTracking, the training and testing sets
include 140 and 75 videos respectively. We evaluate our model on the the test
set of VR-EyeTracking and the validation sets (images and videos) of Salient360!
(due to the unavailability of the reserved set after we contacted the authors), in
total 79 ODVs with over 75,000 frames.

Competitors. ATSal is compared with seven models corresponding to three
state-of-the-art 2D video saliency models: Unisal [7], 3DSAL [11], SalEMA [10],
and four 360◦ specialized models: U-NET [29], CP360 [30], SalGAN360 [36],
and Two-Stream [34]. This choice is motivated by the publicly available code.
However, U-NET inference protocol was not discussed in the paper since the
authors use the teacher forcing technique to train the model, so we feed the
predicted saliency maps at time t − 1 into the model at time t. All models
were evaluated according to ﬁve diﬀerent saliency metrics: Normalized Scanpath
Saliency (NSS), Kullback-Leibler Divergence(KLD), Similarity (SIM), Linear
Correlation Coeﬃcient (CC), and AUC-Judd (AUC-J). Please refer to [47] for
an extensive review of these metrics.

Technical details. The attention model is implemented in Pytorch. The
Adam optimizer was used for the training with a learning rate of 10−5. The
attention model was trained end-to-end using a NVIDIA GTX 1080 and a 3.90
GHz I7 7820 HK Intel processor. SalEMA experts were ﬁne-tuned with a modiﬁed
input size of 160 × 160.

Dataset processing. The VR-EyeTracking dataset recorded by [47], consists
of 208 diverse content 4k ODVs, where the head/eye saccades were obtained
from 45 participants. However, neither ﬁxation nor saliency maps were published
publicly. We processed the gaze recording to obtain the 2048 × 1024 ﬁxation
maps. Saliency maps were generated by convolving each ﬁxation point (for all
observers of one video) with a Gaussian ﬁlter, with σ = 9.35◦ for head and eye
data. The processed dataset is now available at this link.

ATSal: a saliency prediction model for 360◦ videos

11

Table 3. Comparative performance study on: Salient360! [28], VR-EyeTracking [6]
datasets.

Dataset

Models

Unisal [7]
SalEMA [10]
3DSal [11]

U-NET [29]
Cheng et al. [30]
Salgan360 [36]
Two-Stream [34]

2D models

360◦ models

Training setting (i)

Training setting (ii)

Attention
Experts
Ours

Attention
Experts
Ours

4.2 Results

Salient360!

VR-EyeTracking

AUC-J ↑ NSS ↑ CC ↑ SIM ↑ KLD ↓ AUC-J ↑ NSS ↑ CC ↑ SIM ↑ KLD ↓

0.746 1.258 0.206 0.190
–

8.524
–
0.656 0.898 0.192 0.139 10.129

–

–

–

0.725 0.864 0.129 0.122
0.819 1.094 0.175 0.213
–
0.787 1.608 0.265 0.179

–

–

–

9.810
9.313
–
8.252

0.764 1.798 0.314 0.281
0.772 1.790 0.320 0.284
0.679 1.229 0.228 0.232

0.818 1.430 0331 0.247
0.735 0.914 0.171 0.179
0.704 1.267 0.236 0.238
0.827 1.906 0.346 0.254

8.225
0.827 1.397 0.222 0.168
0.870 2.438 0.366 0.227
6.990
0.881 2.580 0.363 0.252 6.327

0.795 1.338 0.255 0.229
0.754 1.292 0.221 0.224
0.801 1.590 0.265 0.255

6.767
6.532
8.317

7.070
8.879
7.625
7.127

7.405
7.840
6.571

0.796 1.299 0.201 0.168
0.807 1.564 0.242 0.223
0.837 1.764 0.285 0.255

7.973
7.679
7.382

5.664
0.836 1.791 0.342 0.302
7.407
0.778 1.388 0.259 0.211
0.862 2.185 0.363 0.312 5.561

Table. 3 shows the comparative study with the aforementioned models according
to the diﬀerent saliency metrics on Salient360! and VR-EyeTracking datasets
(4/75) test ODVs. Our model is very competitive in the two datasets: ATSal
exhibits the best score for all metrics. Surprisingly, 2D approaches achieve mostly
the same results as 360◦ specialized models; this points to the potential to consider
the direct transfer of the well veriﬁed visual attention features from 2D to 360◦,
but also perhaps reﬂects the lack of large scale well-annotated 360◦ datasets. We
invite the reader to watch the qualitative results of the proposed method in the
form of a video available here.

4.3 Performance study

VR-EyeTracking. The 75 diverse test ODVs of this dataset makes the prediction
task very challenging. ATSal outperforms all competitors according to all ﬁve
metrics due to the combination of global features with the attention stream, but
also due to the local patches encoding the temporal domain predicted by the
experts model.

Salient360!. On both distributions of the Salient360! dataset (images and
videos), the proposed model gains a substantial quantitative advantage in accuracy
compared with other models. This demonstrates the capacity of our model to
handle a larger set of scenarios.

Encouraged by the positive results of the attention model trained on static
images, and later ﬁne-tuned for the Salient360! and VR-EyeTracking ODVs. We
evaluated the model components separately on both datasets. Table. 3 indicates
the competitiveness of the attention model with respect to other competitors,
even without the integration of the expert model. A possible explanation could

12

Y. Dahou et al.

relate to smaller motion eﬀects in ODVs compared to 2D videos, where viewport
locations are the most prominent features.

Figure. 2 illustrates the prediction task on a sample of 360◦ frames from the
two datasets: Salient360! and VR-EyeTracking. It can be seen that the generated
saliency maps with ATSal are more comprehensive and look remarkably similar
to the Ground truth maps in terms of saliency distribution. Other competitors
shown in the same ﬁgure overestimate saliency in general, or overlay bias the
equator/center. Furthermore, the eﬀectiveness of ATSal in capturing the main
objects in the scene is clear.

For more qualitative results, Figure. 3 shows the overlaid saliency maps on
sample frames from Salient360! for the attention model, expert models, and
ATSal. Two key points can be seen from these ﬁgures:

– Figure. 2: ATSal predicts consistent saliency at the very beginning of each
video due to the attention mechanism modeling the spatial features, while
other competitors center the saliency around the equator. As the scene
progresses, ATSal ignores some static regions and only focuses on other
moving viewports. In video (b), all models only detect faces near the equator,
while ATSal also attends to other parts of the scene.

– Figure. 3: the generated saliency maps using only the attention model are
sparse and cover all the possible outcomes in the scene. This is due to the
enlarged receptive ﬁeld of the latent space: the model tends to give a high
probability to a given pixel, which makes it salient. The expert models act
as a saliency selector through the pixel wise multiplication. This approach
encourages not overestimating saliency, but also forces the model to generate
more focused and consistent saliency regions.

Table 4. GPU inference time comparison of video saliency prediction methods (NVIDIA
GTX 1080). All methods are reported based on the VR-EyeTracking benchmark [6].
Best computational performance among dedicated 360◦ models is shown in bold.

Model

SalGAN360 [36]
Two-stream [34]
U-Net [29]
(*) 3DSAL [11]
(*) SalEMA [10]
(*) Unisal

[7]

Attention (ours)
Experts (ours)
ATSal (ours)

(*) 2D models.

Runtime (s)

17.330
2.061
1.802
0.100
0.020
0.010

0.050
0.170
0.230

ATSal: a saliency prediction model for 360◦ videos

13

Computational load. Model eﬃciency is a key factor for real-time 360◦
videos application like streaming. Table. 4 shows a GPU runtime comparison
(processing time per 360◦ frame) of the diﬀerent competitors on the 4K VR-
EyeTracking ODVs. Compared with other 360◦ specialized models, ATSal is over
9× faster than Two-stream, which is the current state-of-the-art model according
to the Salient360! leaderboard.

5 Conclusion

We proposed a novel deep learning based 360◦ video saliency model that embeds
two parallel streams encoding both the global and local features. The attention
model explicitly captures the global visual static saliency information through an
attention mechanism at the middle of the bottleneck with an enlarged receptive
ﬁeld propagating the contextual pixel-wise information to the whole frame space.
This design allows the expert models to learn more eﬀective local region based
saliency. The temporal domain is augmented using the simple exponential moving
average (EMA).

We performed extensive evaluations on the Salient360! and VR-EyeTracking
datasets, and compared the results of our model with the previous 2D and
360◦ static and dynamic models. The qualitative and quantitative results have
shown that the proposed method is consistent, eﬃcient, and outperforms other
state-of-the-art.

Acknowledgement

This publication has emanated from research supported by Science Foundation
Ireland (SFI) under Grant Number SFI/12/RC/2289 P2, co-funded by the Euro-
pean Regional Development Fund, through the SFI Centre for Research Training
in Machine Learning (18/CRT/6183).

References

1. Xu, M., Li, C., Zhang, S., & Le Callet, P. (2020). State-of-the-art in 360 video/image
processing: Perception, assessment and compression. IEEE Journal of Selected
Topics in Signal Processing, 14(1), 5-26.

2. De Abreu, A., Ozcinar, C., & Smolic, A. (2017, May). Look around you: saliency
maps for omnidirectional images in VR applications. In 2017 Ninth International
Conference on Quality of Multimedia Experience (QoMEX) (pp. 1-6). IEEE.
3. Itti, L., Koch, C. (2000). A saliency-based search mechanism for overt and covert

shifts of visual attention. Vision research, 40(10-12), 1489-1506.

4. Pan, J., Ferrer, C. C., McGuinness, K., O’Connor, N. E., Torres, J., Sayrol, E., Giro-
i-Nieto, X. (2017). SalGAN: Visual saliency prediction with generative adversarial
networks. arXiv preprint arXiv:1701.01081.

5. Borji, A. (2018). Saliency prediction in the deep learning era: An empirical investi-

gation. arXiv preprint arXiv:1810.03716, 10.

14

Y. Dahou et al.

6. Xu, Y., Dong, Y., Wu, J., Sun, Z., Shi, Z., Yu, J., Gao, S. (2018). Gaze prediction in
dynamic 360 immersive videos. In proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (pp. 5333-5342).

7. Droste, R., Jiao, J., Noble, J. A. (2020). Uniﬁed Image and Video Saliency Modeling.

arXiv preprint arXiv:2003.05477.

8. Min, K., Corso, J. J. (2019). TASED-net: Temporally-aggregating spatial encoder-
decoder network for video saliency detection. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (pp. 2394-2403). ISO 690

9. Lai, Q., Wang, W., Sun, H., Shen, J. (2019). Video saliency prediction using
spatiotemporal residual attentive networks. IEEE Transactions on Image Processing,
29, 1113-1126.

10. Linardos, P., Mohedano, E., Nieto, J. J., O’Connor, N. E., Giro-i-Nieto, X., McGuin-
ness, K. (2019). Simple vs complex temporal recurrences for video saliency prediction.
British Machine Vision Conference (BMVC), 2019

11. Djilali, Y. A. D., Sayah, M., McGuinness, K., O’Connor, N. E. (2020). 3DSAL: an

eﬃcient 3D-CNN architecture for video saliency prediction.

12. Wang, W., Shen, J., Guo, F., Cheng, M. M., Borji, A. (2018). Revisiting video
saliency: A large-scale benchmark and a new model. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (pp. 4894-4903).

13. Bak, C., Kocak, A., Erdem, E., Erdem, A. (2017). Spatio-temporal saliency networks
for dynamic saliency prediction. IEEE Transactions on Multimedia, 20(7), 1688-
1698.

14. Pan, J., Sayrol, E., Nieto, X. G. I., Ferrer, C. C., Torres, J., McGuinness, K.,
OConnor, N. E. (2017, July). Salgan: Visual saliency prediction with adversarial
networks. In CVPR Scene Understanding Workshop (SUNw).

15. Bogdanova, I., Bur, A., H¨ugli, H., Farine, P. A. (2010). Dynamic visual attention
on the sphere. Computer Vision and Image Understanding, 114(1), 100-110.
16. Bogdanova, I., Bur, A., Hugli, H. (2008). Visual attention on the sphere. IEEE

Transactions on Image Processing, 17(11), 2000-2014.

17. Rai, Y., Le Callet, P., & Guillotel, P. (2017, May). Which saliency weighting for
omni directional image quality assessment?. In 2017 Ninth International Conference
on Quality of Multimedia Experience (QoMEX) (pp. 1-6). IEEE.

18. Xu, M., Song, Y., Wang, J., Qiao, M., Huo, L., Wang, Z. (2018). Predicting head
movement in panoramic video: A deep reinforcement learning approach. IEEE
transactions on pattern analysis and machine intelligence, 41(11), 2693-2708.
19. Sitzmann, V., Serrano, A., Pavel, A., Agrawala, M., Gutierrez, D., Masia, B.,
Wetzstein, G. (2018). Saliency in VR: How do people explore virtual environments?.
IEEE transactions on visualization and computer graphics, 24(4), 1633-1642.
20. De Abreu, A., Ozcinar, C., & Smolic, A. (2017, May). Look around you: saliency
maps for omnidirectional images in VR applications. In 2017 Ninth International
Conference on Quality of Multimedia Experience (QoMEX) (pp. 1-6). IEEE.
21. Huang, X., Shen, C., Boix, X., & Zhao, Q. (2015). SALICON: Reducing the semantic
gap in saliency prediction by adapting deep neural networks. In Proceedings of the
IEEE International Conference on Computer Vision (pp. 262-270).

22. Lebreton, P., & Raake, A. (2018). GBVS360, BMS360, ProSal: Extending existing
saliency prediction models from 2D to omnidirectional images. Signal Processing:
Image Communication, 69, 69-78.

23. Zhang, J., & Sclaroﬀ, S. (2013). Saliency detection: A boolean map approach. In
Proceedings of the IEEE international conference on computer vision (pp. 153-160).
24. Harel, J., Koch, C., Perona, P. (2007). Graph-based visual saliency. In Advances in

neural information processing systems (pp. 545-552).

ATSal: a saliency prediction model for 360◦ videos

15

25. T. Maugey, O. Le Meur and Z. Liu, ”Saliency-based navigation in omnidirectional
image,” 2017 IEEE 19th International Workshop on Multimedia Signal Processing
(MMSP), Luton, 2017, pp. 1-6.

26. Battisti, F., Baldoni, S., Brizzi, M., Carli, M. (2018). A feature-based approach for
saliency estimation of omni-directional images. Signal Processing: Image Communi-
cation, 69, 53-59.

27. Fang, Y., Zhang, X., Imamoglu, N. (2018). A novel superpixel-based saliency
detection model for 360-degree images. Signal Processing: Image Communication,
69, 1-7.

28. David, E. J., Guti´errez, J., Coutrot, A., Da Silva, M. P., & Callet, P. L. (2018,
June). A dataset of head and eye movements for 360 videos. In Proceedings of the
9th ACM Multimedia Systems Conference (pp. 432-437). ISO 690

29. Zhang, Z., Xu, Y., Yu, J., Gao, S. (2018). Saliency detection in 360 videos. In
Proceedings of the European Conference on Computer Vision (ECCV) (pp. 488-503).
30. Cheng, H. T., Chao, C. H., Dong, J. D., Wen, H. K., Liu, T. L., Sun, M. (2018). Cube
padding for weakly-supervised saliency prediction in 360 videos. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1420-1429).
31. Suzuki, T., Yamanaka, T. (2018, October). Saliency map estimation for omni-
directional image considering prior distributions. In 2018 IEEE International Con-
ference on Systems, Man, and Cybernetics (SMC) (pp. 2079-2084). IEEE.

32. Lebreton, P., Fremerey, S., Raake, A. (2018, July). V-BMS360: A video extention
to the BMS360 image saliency model. In 2018 IEEE International Conference on
Multimedia &Expo Workshops (ICMEW) (pp. 1-4). IEEE.

33. Nguyen, A., Yan, Z., Nahrstedt, K. (2018, October). Your attention is unique:
Detecting 360-degree video saliency in head-mounted display for head movement
prediction. In Proceedings of the 26th ACM international conference on Multimedia
(pp. 1190-1198).

34. Zhang, K., Chen, Z. (2018). Video saliency prediction based on spatial-temporal two-
stream network. IEEE Transactions on Circuits and Systems for Video Technology,
29(12), 3544-3557.

35. Hu, H. N., Lin, Y. C., Liu, M. Y., Cheng, H. T., Chang, Y. J., & Sun, M. (2017,
July). Deep 360 pilot: Learning a deep agent for piloting through 360 sports videos.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(pp. 1396-1405). IEEE.

36. Chao, F. Y., Zhang, L., Hamidouche, W., & Deforges, O. (2018, July). SalGAN360:
Visual saliency prediction on 360 degree images with generative adversarial networks.
In 2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)
(pp. 01-04). IEEE.

37. Qiao, M., Xu, M., Wang, Z., & Borji, A. (2020). Viewport-dependent Saliency

Prediction in 360° Video. IEEE Transactions on Multimedia.

38. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... & Tang, X. (2017).
Residual attention network for image classiﬁcation. In Proceedings of the IEEE
conference on computer vision and pattern recognition (pp. 3156-3164).

39. Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2016). Stacked attention networks
for image question answering. In Proceedings of the IEEE conference on computer
vision and pattern recognition (pp. 21-29).

40. Tao, A., Sapra, K., & Catanzaro, B. (2020). Hierarchical Multi-Scale Attention for

Semantic Segmentation. arXiv preprint arXiv:2005.10821.

41. Chen, L. C., Yang, Y., Wang, J., Xu, W., & Yuille, A. L. (2016). Attention to scale:
Scale-aware semantic image segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition (pp. 3640-3649).

16

Y. Dahou et al.

42. Rai, Y., Guti´errez, J., & Le Callet, P. (2017, June). A dataset of head and eye
movements for 360 degree images. In Proceedings of the 8th ACM on Multimedia
Systems Conference (pp. 205-210).

43. Sitzmann, V., Serrano, A., Pavel, A., Agrawala, M., Gutierrez, D., Masia, B., &
Wetzstein, G. (2016). How do people explore virtual environments?. arXiv preprint
arXiv:1612.04335.

44. Oliva, A., & Torralba, A. (2007). The role of context in object recognition. Trends

in cognitive sciences, 11(12), 520-527.

45. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition (pp. 770-778).

46. Bao, Y., Zhang, T., Pande, A., Wu, H., & Liu, X. (2017, June). Motion-prediction-
based multicast for 360-degree video transmissions. In 2017 14th Annual IEEE
International Conference on Sensing, Communication, and Networking (SECON)
(pp. 1-9). IEEE.

47. Bylinskii, Z., Judd, T., Oliva, A., Torralba, A., & Durand, F. (2018). What do
diﬀerent evaluation metrics tell us about saliency models?. IEEE transactions on
pattern analysis and machine intelligence, 41(3), 740-757.

