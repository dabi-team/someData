8
1
0
2

n
u
J

3
1

]

R
G
.
s
c
[

1
v
4
2
9
4
0
.
6
0
8
1
:
v
i
X
r
a

MovieEditingandCognitiveEventSegmentationinVirtualRealityVideoANASERRANO,UniversidaddeZaragoza,I3AVINCENTSITZMANN,StanfordUniversityJAIMERUIZ-BORAU,UniversidaddeZaragoza,I3AGORDONWETZSTEIN,StanfordUniversityDIEGOGUTIERREZ,UniversidaddeZaragoza,I3ABELENMASIA,UniversidaddeZaragoza,I3AFig.1.Differentfromtraditionalcinematography,watchingaVRmovieoffersviewerscontroloverthecamera.Thisposesmanyquestionsastowhateditingtechniquescanbeappliedinthisnewscenario.WeinvestigatetheperceptionofcontinuitywhilewatchingeditedVRcontent,gatheringeyetrackingdatafrommanyobservers.Werelyonrecentcognitivestudies,aswellaswell-establishedcinematographictechniques,toprovideanin-depthanalysisofsuchdata,andtounderstandhowdifferentconditionsaffectviewers’gazebehavior.Traditionalcinematographyhasreliedforoveracenturyonawell-establishedsetofeditingrules,calledcontinuityediting,tocreateasenseofsituationalcontinuity.Despitemassivechangesinvisualcontentacrosscuts,viewersingeneralexperiencenotroubleperceivingthediscontinuousflowofinfor-mationasacoherentsetofevents.However,VirtualReality(VR)moviesareintrinsicallydifferentfromtraditionalmoviesinthattheviewercon-trolsthecameraorientationatalltimes.Asaconsequence,commoneditingtechniquesthatrelyoncameraorientations,zooms,etc.,cannotbeused.InthispaperweinvestigatekeyrelevantquestionstounderstandhowwelltraditionalmovieeditingcarriesovertoVR,suchas:Doestheperceptionofcontinuityholdacrosseditboundaries?Underwhichconditions?Doesviewers’observationalbehaviorchangeafterthecuts?Todoso,werelyonrecentcognitionstudiesandtheeventsegmentationtheory,whichstatesthatourbrainssegmentcontinuousactionsintoaseriesofdiscrete,mea-ningfulevents.WefirstreplicateoneofthesestudiestoassesswhetherthepredictionsofsuchtheorycanbeappliedtoVR.WenextgathergazedatafromviewerswatchingVRvideoscontainingdifferenteditswithvaryingparameters,andprovidethefirstsystematicanalysisofviewers’behaviorandtheperceptionofcontinuityinVR.Fromthisanalysiswemakeaseriesofrelevantfindings;forinstance,ourdatasuggeststhatpredictionsfromthecognitiveeventsegmentationtheoryareusefulguidesforVRediting;thatdifferenttypesofeditsareequallywellunderstoodintermsofcontinuity;andthatspatialmisalignmentsbetweenregionsofinterestattheeditboun-dariesfavoramoreexploratorybehaviorevenafterviewershavefixatedonanewregionofinterest.Inaddition,weproposeanumberofmetricsto©2017AssociationforComputingMachinery.Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.ThedefinitiveVersionofRecordwaspublishedinACMTransactionsonGraphics,https://doi.org/10.1145/3072959.3073668.describeviewers’attentionalbehaviorinVR.WebelievetheinsightsderivedfromourworkcanbeusefulasguidelinesforVRcontentcreation.CCSConcepts:•Human-centeredcomputing→Virtualreality;AdditionalKeyWordsandPhrases:Immersiveenvironments,cinemato-graphyACMReferenceformat:AnaSerrano,VincentSitzmann,JaimeRuiz-Borau,GordonWetzstein,DiegoGutierrez,andBelenMasia.2017.MovieEditingandCognitiveEventSeg-mentationinVirtualRealityVideo.ACMTrans.Graph.36,4,Article47(July2017),12pages.https://doi.org/10.1145/3072959.30736681INTRODUCTIONMoviesaremadeupofmanydifferentcamerashots,usuallytakenatverydifferenttimesandlocations,separatedbycuts.Giventhattheresultingflowofinformationisusuallydiscontinuousinspace,time,andaction,whiletherealworldisnot,itissomewhatsurprisingthattheresultisperceivedasacoherentsequenceofevents.Thekeytomaintainingthisillusionliesinhowtheseshotsareeditedtogether,forwhichfilmmakersrelyonasystemcalledcontinuityediting[Bordwelletal.1997;O’Steen2009].Althoughothertechni-quesexisttolinkshotstogether,suchasthefade-out,fade-in,ordissolve,approximately95%ofeditingboundariesarecuts[Cutting2004],whichdirectlysplicetwocameraframes.Thegoalofcontinuityeditinginmoviesisthentocreateasenseofsituationalcontinuity(asequenceofshotsperceivedasasingleACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017. 
 
 
 
 
 
47:2•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.Masiaevent),ordiscontinuity(transitionsfromoneeventorepisodetoanother).Professionaleditorshavedevelopedbothastrongsenseofrhythmandasolidintuitionforeditingtogethercamerashots,makingthecuts“invisible”.Forinstance,spatialcontinuityismain-tainedlargelybythe180◦rule,statingthatthecamerashouldnotcrosstheaxisofactionconnectingtwocharactersinashot,whilecontinuityofactionisachievedbystartingtheactioninoneshotandimmediatelycontinuingitintheshotafterthecut.Someaut-horshaveproposedpartialtheoriestoexplainwhytheseeditedshotsareperceivedasacontinuousevent.Forexample,the180◦rulecreatesavirtualstagewheretheactionunfolds[Bordwelletal.1997],whilemechanismstoprocessandtrackbiologicalmotionmaymaskanactioncut[SmithandHenderson2008].However,thehigherlevelcognitiveprocessesthatmakecon-tinuityeditingworkarenotyetcompletelyunderstood.Whatisunderstood,though,isthatacorecomponentofspatialperceptionisourabilitytosegmentawholeintoparts[Biederman1987].Recentcognitiveandneuroscienceresearchindicatesthatasimilarsegmen-tationalsooccursinthetemporaldomain,breakingupacontinuousactivityintoaseriesofmeaningfulevents.Thishasleadtothede-velopmentoftheeventsegmentationtheory[KurbyandZacks2008;Reynoldsetal.2007;ZacksandSwallow2007],whichpostulatesthatourbrainsusethisdiscreterepresentationtopredicttheim-mediatecourseofevents,andtocreateaninternal,interconnectedrepresentationinmemory.Neweventsareregisteredwheneverachangeinaction,space,ortime,occurs.Basedonthistheory,recentworkshaveexploredhowcontinuityisperceivedinmovies,acrossdifferenttypesofcuts[Cutting2014;MaglianoandZacks2011;Zacksetal.2010].Interestingly,itseemsthatthepredictiveprocesssuggestedbyeventsegmentationtheoryisconsistentwithcommonpracticebyprofessionalmovieeditors.Inthiswork,weinvestigatecontinuityeditingforvirtualrealityvideos1.Virtualreality(VR)contentisintrinsicallydifferentfromtraditionalmoviesinthatviewersnowhavepartialcontrolofthecamera;whilethepositionoftheviewerwithinthesceneisdeci-dedduringacquisition,theorientationisnot.Thisnewly-gainedfreedomofusers,however,rendersmanyusualtechniques,suchascameraanglesandzooms,ineffectivewheneditingthemovie.Ne-vertheless,newdegreesoffreedomforcontentcreatorsareenabled,andfundamentalquestionsastowhataspectsofthewell-establishedcinematographiclanguageapplytoVRshouldberevisited(Fig.1).Inparticular,weseektoinvestigateanswerstothefollowingkeyquestions:•DoescontinuityeditingworkinVR,i.e.,istheperceptionofeventsinaneditedVRmoviesimilartotraditionalcine-matography?•Acommon,safebeliefwheneditingaVRmovieisthattheregionsofinterestshouldbealignedbeforeandafterthecut.Isthistheonlypossibleoption?Whataretheconsequencesofintroducingamisalignmentacrossthecutboundaries?•Arecertaintypesofdiscontinuities(cuts)inVRfavoredoverothers?Dotheyaffectviewerbehaviordifferently?1Inthisworkwedealwith360◦movies;throughoutthetextwewillusethetermsVRand360◦moviesinterchangeably.Weuseaheadmounteddisplay(HMD),equippedwithaneyetracker,andgatherbehavioraldataofusersviewingdifferentVRvideoscontainingdifferenttypesofedits,andwithvaryingparame-ters.Wefirstperformastudytoanalyzewhethertheconnectionsbetweentraditionalcinematographyandcognitiveeventsegmen-tationapplytoimmersiveVR(Sec.4).Tothisend,wereplicatearecentcognitiveexperiment,previouslycarriedoutusingtraditionalcinematographicfootage[MaglianoandZacks2011],usinginsteadaVRmovie.Ourresultsshowsimilartrendsinthedata,suggestingthatthesamekeycognitivemechanismscomeintoplay,withanoverallperceptionofcontinuityacrosseditboundaries.WefurtheranalyzecontinuityeditingforVR,exploringalargeparameterspacethatincludesthetypeofeditfromthecognitivepointofviewofeventsegmentation,thenumberandpositionofre-gionsofinterestbeforeandafterthecut,ortheirrelativealignmentacrossthecutboundary(Sec.5).Weproposeandleverageaseriesofnovelmetricsthatallowtodescribeviewers’attentionalbehaviorinVR(Sec.6),includingfixationsonaregionofinterest,alterationofgazeafteracut,exploratorynatureoftheviewingexperience,andastatesequenceanalysisofthetemporaldomainaccordingtowhethertheviewerisfixatingonaregionorperformingsaccadicmovements.OuranalysesrevealsomefindingsthatcanberelevantforVRcontentcreatorsandeditors:forinstance,thatpredictionsfromthecognitiveeventsegmentationtheoryseemtobeusefulguidesforVRediting;thatdifferenttypesofeditsareequallywellunderstoodintermsofcontinuity;howthedependenceofthetimetoconvergencetoaregionofinterestafteracutisnotlinearwiththemisalignmentbetweenregionsofinterestatthecut,butratherappearstofollowanexponentialtrend;orhowspatialmisalignmentsbetweenregionsofinterestattheeditboundarieselicitamoreexploratorybehaviorevenafterviewershavefixatedonanewregionofinterest.Webelieveourworkisthefirsttoempiricallytesttheconnectionsbetweencontinuityediting,cognition,andnarrativeVR,aswellastolookintotheproblemofeditinginVR,inasystematicmanner.Inaddition,weprovidealloureyetrackingdata,videos,andcodetohelpotherresearchersbuilduponourwork.22RELATEDWORKToolsforediting.Creatingasequenceofshotsfromrawfootagewhilemaintainingvisualcontinuityishard,especiallyfornoviceusers[Davisetal.2013].Automaticcinematographyfor3Denvi-ronmentswasproposedbyHeatal.[1996],encodingfilmidiomsashierarchicalfinitestatemachines,whileChristiansonetal.[1996]proposedadeclarativecameracontrolsystem.Manyotherdifferenttoolshavebeendevisedtohelpintheeditingprocess,usuallyle-veragingtheparticularcharacteristicsofspecificdomainssuchas3Danimations[Galvaneetal.2015],interviewvideos[Berthouzozetal.2012],narratedvideos[Truongetal.2016],classroomlectu-res[Hecketal.2007],groupmeetings[Ranjanetal.2008],egocentricfootage[LuandGrauman2013],ormultiplesocialcameras[Arevetal.2014].Jainetal.[2014]proposedagaze-driven,re-editingsystemforretargetingvideotodifferentdisplays.Morerecently,WuandChristie[2015]createdalanguagetodefinecameraframing2http://webdiis.unizar.es/~aserrano/projects/VR-cinematographyACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.MovieEditingandCognitiveEventSegmentationinVRVideo•47:3andshotsequencing.OthermethodsfocusingoncameraplacementandplanningcanbefoundintheworkofChristieetal.[Christieetal.2005].Allthesetoolshavebeendesignedfortraditional,two-dimensionalviewingexperiences,wherethespectatorsitspassivelyinfrontofascreen.Incontrast,ourgoalistoanalyzecontinuityeditingforvirtualrealityvideos.Continuityandcognition.Severalworkshaveanalyzedtheeffectsofeditsorcutsfromacomputervisionperspective(e.g.,[CarrollandBever1976;HochbergandBrooks2006;SmithandHenderson2008]).Closertoourapproach,afewworkshaveanalyzedtheperceptionofcontinuityfromacognitivesciencepointofview.Forinstance,Cohn’sanalysesofcomicstrips[2013]suggestthatviewerscanbuildlinksbetweenframeswhilemaintainingaglobalsenseofthenarrative;however,rearrangingelementscanquicklyleadtoconfusion.Someresearchersarguethatourperceptionofrealityisaveryflexibleprocess,andthisflexibilityallowsustoadaptandperceiveeditedfilmasacontinuousstory[Anderson1996;Cutting2004].Smith[2012]performedanempiricalstudytounderstandhowcontinuityeditingalignswithourperceptualabilities,identifyingtheroleofvisualattentionintheperceptionofcontinuitybetweenedits.Inourwork,weexploretherecenttheoryofeventsegmentation[KurbyandZacks2008;Reynoldsetal.2007;Zacks2010;ZacksandSwallow2007],andanalyzeitsconnectionswithcontinuityeditingforVR.3BACKGROUNDONEVENTSEGMENTATIONWepresenthereabriefsummaryoftheeventsegmentationtheory,andreferthereadertotheoriginalpublicationsforamorethoroughexplanation[KurbyandZacks2008;Reynoldsetal.2007;Zacks2010;ZacksandSwallow2007].Recentresearchsuggeststhateventsegmentationisanautomatickeycomponentofourperceptualprocessing,reducingacontinuousflowofactivityintoahierarchical,discretesetofevents.Theadvantagesofthisstrategyaretwofold:First,itisveryefficientintermsofinternalrepresentationandmemory.Second,itprovidesamucheasierwaytothinkabouteventsinrelationtooneanother.Itcanbeseenasthetimeequivalenttothewell-knownspatialsegmentationinvision,wherewesegmentanobject(e.g.,acar)intomanycomponentssuchaswheels,chassis,engine,etc.Thisdiscretementalrepresentationisusedasabasisforpre-dictingtheimmediatecourseofevents:apersonwalkingdownthestreetwillcontinuetodoso,orsomebodywillansweraquestionwhenasked.Whenthesepredictionsareviolated,itisanindicationofanew(discrete)event;inotherwords,itseemsthatunexpectedchangesleadtotheperceptionofaneventboundary.Morepreci-sely,theeventsegmentationtheoryassumesthatneweventsareregisteredwhenchangesinaction,space,ortime,occur;whenthishappens,themechanismsofeventsegmentationupdatethemen-talrepresentationoftheevent,storingtheoldoneinlong-termmemory.Thiseventsegmentationtheoryhasrecentlybeentestedinthecontextoffilmunderstanding.Someexperimentshaveevenre-cordedbrainactivitywithfunctionalmagneticresonanceimaging(fMRI)whilewatchingamovie,andshowedthatmanyregionsinthecortexunderwentsubstantialchangesinresponsetothesituatio-naldiscontinuities(unexpectedchanges)introducedbysomemoviecuts[MaglianoandZacks2011;Zacksetal.2010].Aninterestingobservationfollows:thepredictiveprocesssuggestedbyeventseg-mentationtheoryisconsistentwithcommonpracticebyprofessionalmovieeditors,whoplacecutstosupportorbreaktheexpectationsofeventcontinuitybytheviewers[Bordwelletal.1997].Whenacutintroducesamajorchange,thebraindoesnottrytoexplaintheper-ceiveddiscontinuity;instead,itadaptstothechange,createsanewmentalrepresentation,andbeginspopulatingitwithdetails[Mag-lianoandZacks2011].Thisautomaticmechanismmightbeakeyprocesstoexplainwhycontinuityeditingworks.ThenextsectionexploreswhetherthisconnectionbetweeneventsegmentationandcontinuityeditingstudiedintraditionalcinemacarriesovertoVRmovies,akeyquestionbeforewecandiveintoamoredetailedin-vestigation.Notethat,inthefollowing,weusethetermedittorefertoadiscontinuitybetweentwoshots,whilecutreferstotheactualcinematographicimplementation(match-on-action,jumpcut,etc.)oftheedit.4DOESCONTINUITYEDITINGWORKINVR?AswehaveseeninSec.3,thereisconsiderableevidencethatcon-tinuityeditingperformedintraditionalmoviesmayberelatedtohowourbrainsprocesseventsandsituationalchanges,andthatthismaybethecausewhycontinuityeditinghasbeensosuccessfulinconveyingthenarrative.Therefore,beforeweanalyzespecificaspectsrelatedtoeditinginVRmovies,wefirstwanttoassertthatcontinuityeditingappliestoVRscenarios.Forthispurpose,wecheckwhethertheconnectionsbetweeneventsegmentationandedits,whichhavebeenidentifiedandanalyzedintraditionalmo-vies[MaglianoandZacks2011]alsoholdinVRmovies,wheretheviewingconditionsandtheperceptionofimmersionchangesignifi-cantly.Thisisthegoaloftheexperimentdescribedinthissection.Weaimtoreplicatethemethodologyofrecentcognitionstudies,sharingasimilargoalinthecontextsofeventsegmentation[ZacksandSwallow1976],andfilmunderstanding[MaglianoandZacks2011;Zacksetal.2010].Weintroducesuchworksandourownexperimentinthefollowingparagraphs.Typesofedits.Followingcommonpracticeinfilmediting,Magli-anoandZacks[2011]defineacontinuitydomainalongthedimen-sionsofspace,time,andaction.Theythenclassifyeditsintothreedifferentclasses,whichwecallhereE1,E2,andE3:•E1:editsthatarediscontinuousinspaceortime,anddis-continuousinaction(actiondiscontinuities);•E2:editsthatarediscontinuousinspaceortime,butconti-nuousinaction(spatial/temporaldiscontinuities);•E3:editsthatarecontinuousinspace,time,andaction(continuityedits).Weadoptthesametaxonomyforeditsinthisexperiment,andintherestofthepaper.Cognitionstudieswithtraditionalmoviecontent.Inthesestu-dies[MaglianoandZacks2011;Zacksetal.2010],participantswatchedTheRedBalloon(a33-minute,1956moviebyA.Lamorisse),andwereaskedtosegmentthemovieintomeaningfuleventsbyACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.47:4•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.Masiapressingabutton.Theywereaskedtodothistwice,oncedefiningthe“largestunitsofnaturalandmeaningfulactivity”(coarsesegmen-tation),andoncedefiningthesmallestunits(finesegmentation);theorderofthisdivisionwasrandomizedbetweenparticipants,whofirstpracticedthetaskonadifferent,2.5-minutemovie.TheRedBalloonwaspresentedin7-to-10-minutesections,toavoidfati-gue.Previoustothistask,theauthorsadditionallyidentifiedallthelocationswhereeditsoccurredinthemovie,andcodedeachoneaccordingtotheabovecategorization:E1,E2,orE3.BasedontheprinciplesoffilmeditingdiscussedinSec.3,actiondiscontinuitiesE1shouldhavethelargestinfluenceonperceiveddiscontinuities,whereascontinuityeditsE3shouldmostlymaintaintheperceivedcontinuity.Theanalysisofthedatadiscretizedinfive-secondbins,alongwithfMRIinformation,confirmedthispredictedtrend.ReplicationofthestudywithVRcontent.WefollowedthesamemethodologyinourVRstudy.Specifically,weaskedsevenpartici-pants(agesbetween21and31,threefemale)towatchfourpubliclyavailableVRmovies(seeAppendixfordetails).Theirinitialtaskwastomarkperceivedeventboundariesbothatcoarseandfinescales,similartotheoriginalexperiments.Previoustotheexperi-ment,theparticipantswatchedatrainingmovie3.WedidnotfindstrongcorrelationsbetweenthefineeventsegmentationandtheeditsinthefirsttestedVRmovie.Thisisexpectedsince,differentfromtraditionalmovies,thetimescaleofsuchfineperceivedeventsisaboutoneorderofmagnitudesmallerthantheaverageVRshot(secondsvs.tensofseconds).Thereforeintheotherthreemoviesweonlyaskedthemtomarkeventsatcoarsescale;weanalyzethisdataonlyintherestofthepaper.Participantswatchedthemovieswhileseated,wearinganOculusRift.Wechosetheseparticularmoviesamongseveralcandidatessince:i)likeTheRedBalloon,theyarenarrativemovieswitharichenoughstructure;ii)theylastlessthaneightminutes,whichfallswithintherangefixedbypreviousstudiestoavoidfatigue;iii)theiraverageshotlastsabout20seconds,closetotheaverageweobtainedfromanalyzingseveral360◦movies;andiv)theycontaineditsofallthreekinds.Fig.2(top)showsrepresentativeframesofallthreetypesofeditsforoneofthesemovies:StarWars-HuntingoftheFallen.Insights.Toinvestigatetherelationbetweentheeditsandtheperceivedeventsegmentation,weidentifiedthelocationandtypeofeachedit,andbinnedalltheeventboundariesmarkedbytheparticipantswithina±3secondwindow,centeredattheedit.Asexpected,someperceivedboundarieswerenotlinkedtoedits,buttoneweventsoractionswithinashot(e.g.,anewactorenteringascene,orthestartofaconversation).Fig.2(bottom)showstheresultsforallfourmovies,groupedbyeditcategory.Ourfindingsshowsimilaritieswiththepreviousstudiesontraditionalcinemato-graphy[MaglianoandZacks2011;Zacksetal.2010].First,actiondiscontinuitiesdominateeventsegmentation,andarethereforethestrongestpredictorsofeventboundaries.Second,continuityeditssucceedinmaintainingasenseofcontinuityofaction,evenacrosstheeditboundaries.Itthusappearsthatthekeycognitiveaspectsoftraditionalmovieeditingthatmakeitworksowellcarryoverto3https://youtu.be/fz88kpRNTqMFig.2.Top:Representativeframesofthe360◦movieStarWars-HuntingoftheFallen,before(left)andafter(right)aneditforeachofthethreetypesofedits(originalvideopropertyofCubeFX(http://cubefx.cz);videoandimagesusedwithpermission).Toprow:actiondiscontinuity(E1).Theframebeforetheeditisnotrelatedwiththeframeaftertheedit;thereisacompletechangeofaction,space,andtime.Middlerow:spatialdiscontinuity(E2).Theeditfollowstheactionbythespaceship(match-on-action),butchangesthelocation;theactionisthereforetheelementconnectingtheframesbeforeandaftertheedit.Bottomrow:continuityedit(E3).Thesamesceneisdepictedbeforeandaftertheeditwithacontinuityinspace,time,andaction;onlythecameraanglechanges.Bottom:Resultsofourcoarsesegmentationtest,showingthepercentageofeditsofeachtypemarkedasaneventboundarybysubjects,andnormalizedbythenumberofoccurrencesofeachtypeofedit.E1actiondiscontinuitiesdominateeventsegmentation,whileE3continuityeditsmaintaintheperceivedcontinuityoftheevent.Thereisalsoasmallpercentageofeventboundariesthatweremarkednotatedits.Thesefindingsmatchresultsofsimilarstudiesintraditionalcinematography,andsuggestthatmovieeditingrulesandcommonpracticecanbeingeneralappliedtonarrativeVRaswell.360◦immersivenarrativemovies.Toourknowledge,thisisthefirsttimethatthishasbeenempiricallytested.5MEASURINGCONTINUITYINVRAfterconfirmingintheprevioussectionthattheperceptionofcontinuityismaintainedacrosseditboundariesinVRnarrativecontent,wenowperformasecond,in-depthstudytoassesshowthedifferentparametersthatdefineaneditinVRaffecttheviewers’behavioraftertheedittakesplace.Giventhehighdimensionalityofthisspace,wefocusonfourmainparameters(orvariablesofinfluence),whichare:thetypeofedit,forwhichwefollowthecognitivetaxonomydescribedinprevioussections;thedegreeofmisalignmentoftheregionsofinterest(ROIs)beforeandaftertheACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.MovieEditingandCognitiveEventSegmentationinVRVideo•47:5Fig.3.Representativeframesofthreeofthescenesdepictedinourclips:Kitchen,Stairs,andStudy(refertotheprojectpageforfullvideos).Fromlefttoright,examplescorrespondingtothefollowingregionofinterest(ROI)configurations:1ROI,2ROIsinthesameFOV,and2ROIsindifferentFOV.Forclarity,ROIsaremarkedbyabluebox.edit;andthenumberandlocationofsuchROIs,bothbeforeandaftertheeditboundaries.Inthefollowingwedescribeourstimuli,variablesofinfluence,andprocedure.AdditionaldetailscanbefoundintheAppendix,whileacompletetablewithallthepossiblecombinationsofconditionstestedcanbefoundinthesupplementalmaterial.5.1StimuliOurstimuliarecreatedfrom360◦monocularvideos,professionallycapturedandstitchedbyalocalcompany.Wechoosetousemo-nocular(andnotstereo)footagesinceitismorecommonamongexistingcapturedevicesandpublicrepositories(e.g.,YouTube360).Thevideosdepictfourdifferentscenarios(Stairs,Kitchen,LivingRoom,Study),withfourdifferentactionsineachone,totallingsix-teenvideosrangingfrom13secondsto2minutesinlength.Fig.3showssomerepresentativeframesinequirectangularprojection.Theywerecapturedusingtwodifferentrigs:aGoProOmni(a360-videorigconsistingofsixGoProHero4cameras),andaFreedom3603×rig(withthreeGoProHero4cameraswithmodifiedEntaniya220lenses).SoundwasrecordedusingaZoomF8recorderwithwirelessmicrophones.Fromthesevideos,wecreatedatotalof216clips,samplingourparameterspaceasexplainedinthefollowingsubsection.Eachclipismadeupoftwoshots,separatedbyanedit.Shotsaretakenfromshortsequencesbothwithinandacrossthefourdifferentscenarios,tomaximizevariety.Eachshotlastssixseconds,toprovideenoughtimetotheviewerstounderstandtheactionsbeingshown.5.2VariablesofinfluenceandparameterspaceTypeofedit.Werelyontheeventsegmentationtheory,andiniti-allyconsiderthethreedifferenttypesofedits{E1,E2,E3},definedalongthedimensionsofspace,time,andaction,asintroducedinSec.3.However,afteranalyzingseventeenVRmovieswehaveob-servedthatE3(whichessentiallyreferstoachangeofviewpointwithinthesamescene)israrelyusedinnarrativeVR.Inthesemo-vies,73%oftheeditscorrespondedtoE1,25%toE2,andonly2%toE3.Thisdiffersfromtraditionalmovies,wheremostoftheeditsarecontinuityedits(E3)[MaglianoandZacks2011],andreflectsaninterestingcontrastbetweentheestablishedstorytellingtechniquesforthetwomedia.DuetotherareappearanceofE3editsinVRmovies,weremoveitfromourconditions,andfocusonthetwomostprominenttypesofedits:E={E1,E2}.Fortheactualimplementationoftheseedits,werevisetraditionalcinematographytechniquesandanalyzeexistingVRvideos,andselectthemostcommoncutsforeachtypeofedit:FortypeE1(dis-continuousinaction,andintimeorspace)weusejumpcuts,whileforE2(continuousinaction,discontinuousintimeorspace)weusecompressedtimecuts,andmatch-on-actioncuts(see,e.g.,[Chand-ler2004;Dmytryk1984];pleaserefertotheAppendixforabriefexplanationofeachone).Tokeepabalancednumberofclipsforeachtypeofedit,weincludetwiceasmanyjumpcuts(typeE1)asmatch-on-actionandcompressedtimecuts(typeE2).AlignmentofROIs.Wedefinetheregionsofinterest(ROIs)astheareasinthe360◦frameinwhichtheactiontakesplace4.SincethepointofviewofthecameracannotbecontrolledbythefilmmakerinVR,acommonpracticeamongcontentcreatorsistosimplyalignROIsbeforeandafteranedit,tomakesurethattheviewerdoesnotmissimportantinformation.However,theexplorationofcontrolled(mis)alignmentsisinterestingforthefollowingreasons:First,thedirectormaywanttointroducesomemisalignmentbetweenROIsforartisticornarrativepurposes(e.g.,tocreatetension).Second,theviewermaynotbelookingatthepredictedROIbeforethecut,thusrenderingthealignmentaftertheedituseless.Third,theremightbemultipleROIswithinascene.Wethereforetestthreedifferentalignmentconditions:(i)perfectalignmentbeforeandaftertheedit(i.e.,0◦betweenROIs);(ii)amisalignmentthatisjustwithinthefieldofview(FOV)oftheHMD5;wechose40◦sinceitisclosetotheaveragemisalignmentin360◦videosfoundinpublicrepositories;and(iii)amisalignmentthatisoutsidetheFOV;wechose80◦,sincewefoundthatlargervaluesareveryrare.WenametheseconditionsA={A0,A40,A80}.ROIconfiguration.ThecontrolofthevieweroverthecameraalsomakesthedispositionandnumberofROIsinthesceneplayakeyroleingazebehavior.ToanalyzetheROIconfigurationbeforeandaf-tertheedit,weintroducetwovariables,RbandRarespectively.Thespaceofpossibleconfigurationsisinfinite,sotokeepthetasktrac-tablewetestthreepossibilitiesforeachone:asingleROI(R{b|a},0),twoROIsbothfallingwithinasingleFOV(R{b|a},1),andtwoROIsnotwithinthesameFOV,i.e.,morethan95◦apart(R{b|a},2).Ex-amplesofthethreeconfigurationsareshowninFig.3.ThepossiblecombinationsofRbandRayieldatotalofnineconditions.4WemanuallylabelROIsascontinuousregionsatseveralkeyframes,creatingtherestthroughinterpolation.WedefinethecenterofeachROIasthecentroidofitspixels.5OurOculusDK2HMDhasahorizontalFOVof95◦,soa40◦misalignmentfallsjustintheperipheryoftheFOV.ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.47:6•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.MasiaSummary.Thissamplingleadsto2(typesofedit)×3(alignments)×9(ROIconfigurations)=54differentconditions.Foreachone,weincludefourdifferentclips,tominimizetheeffectoftheparticularsceneshown,yieldingourfinalnumberof216stimuli.5.3HardwareandprocedureWeusedanOculusDK2HMDequippedwithabinoculareyetrackerfrompupil-labs6,whichrecordsdataat120Hzwithaspatialaccu-racyof1degree.Wealsousedapairofheadphonestoreproducestereosound.Subjectsstoodupwhileviewingthevideo.Atotalof49subjects(34male,15female,µaдe=25.4years,σaдe=7.7years)participatedintheexperiment.Allofthemreportednormalorcorrected-to-normalvision.Eachsubjectfirstcarriedouttheeyetrackercalibrationprocedure.Then,theywereshown36stimulifromthetotalof216,inrandomorder.Thisrandomizationwassuchthatnosubjectviewedtwoalignmentconditionsofthesameclip,whileguaranteeingthateachclipwasviewedbyatleastfivepeople.Italsoavoidspotentiallearningandfatigueeffectsaffectingtheresults.FollowingSitzmannetal.[2016],inordertoensurethatthestartingconditionwasthesameforallsubjects,agrayenvironmentwithasmallredboxwasdisplayedbetweenclips;usershadtofinditandaligntheirheaddirectionwithit,whichwouldlaunchanewclipafter500ms.TheUnitygameenginewasusedtoshowthevideos,andtorecordheadorientationonthesamecomputer,whileeyetrackingdatawasrecordedonasecondcomputer.Afterviewingtheclipstheexperimenterdidadebriefingsessionwiththesubject.Thetotaltimeperexperimentwasaround15minutes.Fromtherawgathereddata,weperformedoutlierrejectionandthencompu-tedscanpaths,definedasatemporalsequencecontainingonegazesampleperframe.MoredetailsontheseaspectscanbefoundintheAppendix(gazedataprocessingandoutlierrejection),andinthesupplementalmaterial(datacollectionanddebriefing).Fromthisdatawedefine,compute,andanalyzeaseriesofmetrics,asdescribedinthenextsection.6HOWDOEDITSINVRAFFECTGAZEBEHAVIOR?Toobtainmeaningfuldataaboutviewers’gazebehavioracrosseventboundariesinVR,wefirstgatheradditionalbaselinedatatocompareagainst.Wemaketheassumptionthatthehigherthegazesimilaritybetweentheeditedclipsandthecorresponding(unedited)baselinevideos,thehighertheperceptionofcontinuity;thisassumptionissimilartopreviousworksanalyzinggazetoassesstheimpactofretargetingandeditingoperationsinimagesandvideo[Castilloetal.2011;Jainetal.2014].Inthefollowing,wefirstdescribehowthisbaselinedataisobtained,thenintroduceourcontinuitymetrics,anddescribetheresultsofouranalysis.6.1BaselinedataInordertocapturebaselineeyetrackingdatafromtheuneditedvideos,wegatheredtennewsubjects(ninemale,onefemale,µaдe=28.1years,σaдe=5.2years)andcollectedheadorientationandgazedatafollowingtheproceduredescribedinSec.5.3.Videoswerewatchedinrandomorder.Wecomputethebaselinescanpathsforeachvideofromtheobtainedgazedataasthemeanscanpathacross6https://pupil-labs.com/Fig.4.Subjectsmeanscanpathforoneexamplevideo.Weplotonescanlineperframeofthevideo,thex-axisshowinglongitudinalposition(0◦-360◦),andthey-axistime.Superimposed(orangeline)weplotthescanpath,showingthetemporalevolutionofthelongitudinalpositionofthegaze.Wealsoplotthefullframe(equirectangularprojection)atthreekeyinstantsthatcorrespondtothethreemarkedtemporalinstants.Viewers’gazeisclearlydirectedbythemovementoftheROIalongtime.users.WeshowinFig.4themeanscanpathcorrespondingtooneofourvideos:Wedisplaythetemporalevolutionofthelongitudinalgazeposition(0◦-360◦),anditshowshowviewers’attentionisdriventowardstheROImovingacrossthescene.Toensurethatthisdatacanbeusedasbaselineforoursubsequentanalyses,weneedtoascertainthecongruencybetweensubjects.Todoso,werelyonareceiveroperatingcharacteristiccurve(ROC)metric,whichprovidesameasureoftheInterObserverCongruency(IOC)[LeMeuretal.2011]overtime.First,weaggregatealltheusers’fixations(pleaserefertotheAppendixforadescriptionofhowfixationsarecomputedfromgazedata)intwo-secondwindows,andconvolvethemwitha2DGaussianofσ=1degreeofvisualangle[LeMeurandBaccino2013],yieldingasaliencymapforeachtimewindow.ThecorrespondingROCcurveisthenobtainedusingaone-against-allapproachbyleavingouttheithsubject:wecompute,foreachsaliencymap,thek%mostsalientregions,andthencalculatethepercentageoffixationsoftheithsubjectthatfallwithinthoseregions.Thisprocessisperformedforasetofthresholdsk=0%..100%,andtheresultingpointsdefineeachcurve.Additionally,wecomputetheAreaUndertheCurve(AUC)foreachwindow,whichprovidesaneasierinterpretationoftheevolutionoftheIOCalongtime(Fig.5,right).TheAUCtakesvaluesbetween0(incongruitybetweenusers)and100(completecongruency).AsdisplayedinFig.5,thecongruencybetweensubjectsremainsveryhighalongtime.Ontheleftofthefigure,theIOCrapidlyreachesavalueof1withk=2%mostsalientregions,andremainsconstantforincreasingvaluesofk.Ontheright,thesameinterpretationfromanAUCperspective:alltheviewer’sfixationsfallonaveragewithinthe2%regionsconsideredmostsalientbytherestoftheviewers,yieldingaveryhighAUC.Thisindicatesthatalltheviewersconsistentlyconsideredthesameregionssalient.Please,refertothesupplementalmaterialfortheresultsforallourvideos.ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.MovieEditingandCognitiveEventSegmentationinVRVideo•47:7Fig.5.Left:InterObserverCongruency(IOC)foroneofourvideos.WecomputeaROCcurveforeachsecondofthevideo.Right:Temporalevo-lutionoftheAreaUndertheCurve(AUC)calculatedforeachoftheROCcurves.ThehighvaluesoftheIOCandAUCindicesindicatethatalltheviewersconsistentlyconsideredthesameregionssalient(refertothemaintextfordetails).6.2MetricsMeasuringtheperceivedcontinuityacrosseditboundariesinanobjectivemannerisnotasimpletask,sincenopredefinedmetricsexist.Wedescribehereourfourdifferentmetricsusedtoanalyzegazebehaviorafteranedit.Inaddition,tofurtherlookforunderlyingpatternsintheusers’behaviorthatourmetricsmaynotcapture,wealsointroduceastatesequenceanalysis.FramestoreachaROI(framesToROI).Thisisthesimplestofourmetrics,simplyindicatingthenumberofframesaftertheoccurrenceoftheeditbeforetheobserverfixatedonaROI.Itisindicativeofthetimetakentoconvergeagaintothemainaction(s)aftertheedit.PercentageoftotalfixationsinsidetheROI(percFixInside).ThispercentageiscomputedafterfixatingonaROIaftertheedit.ItisthusindependentofframesToROI.DifferentconfigurationsoftheROIsmayimply,bynature,differentnumberoffixationsinsidetheROI.Tocompensateforthis,wecomputepercFixInsiderelativetotheaveragepercentageoffixationsinsideaROI,foreachROIconfiguration,beforetheedit.ThismetricisindicativeoftheinterestoftheviewerintheROI(s).Scanpatherror(scanpathError).WecomputetheRMSEofeachscanpathwithrespecttothecorrespondingbaselinescanpath(seeSec.6.1).Thismetricindicateshowgazebehaviorisalteredbytheedit;again,wecomputethismetricafterfixatingonaROIaftertheedit,tomakeitindependentofframesToROI.Numberoffixations(nFix).Wecomputetheratiobetweenthenumberoffixations,andthetotalnumberofgazesamplesaftertheeditafterfixatingonaROI;thisway,weeliminatethepossibleincreaseinsaccadeswhilesearchingfortheROIaftertheedit.Thismetricisthereforeindicativeofhowmanyfixationsandsaccadesthesubjectperforms.Alowvaluecorrespondstoahigherquantityofsaccades,whichinturnsuggestsamoreexploratorybehavior,fixatinglessonanyparticularregionoraction.Statesequences.Weclassifyusers’fixationsalongtimeinfourdifferentstates,correspondingtotheROIs(eachcliphavingone,ortwo),thebackground,andaso-calledidlestatewheresaccadiceyemovementstakeplaceandnofixationsarerecorded.Withthisclassificationweareabletodescribeusers’behaviorasastatese-quence,observingthesuccessionofstateswithtime,aswellasthetimespentineachofthem.Inparticular,weuseastatedistribu-tionanalysistorepresentthegeneralpatternofstatesequencesforeachcondition,whichprovidesanaggregatedviewofthefre-quencyofeachstateforeachtimeinterval.WeusetheRlibraryTraMineR[Gabadinhoetal.2011]forthisanalysis.6.3AnalysisSincewecannotassumethatourobservationsareindependent,weemploymultilevelmodeling[BrowneandRasbash2004;Rau-densbushandBryk2002]inouranalysis,whichiswell-suitedforgroupedorrelateddatalikeours.Multilevelmodelingallowsthespecificationofrandomeffectsamongthepredictors,i.e.,itcon-templatesthepossibilitythatthemodelmightdifferfordifferentvaluesoftheserandomeffects.Inourcase,therandomeffectistheparticularsubjectviewingthestimuli,forwhichweconsideredarandomintercept.Weincludeintheregressionallfourfactors(A,E,RbandRa),aswellasthefirst-orderinteractionsbetweenthem.Sincewehaveca-tegoricalvariablesamongourpredictors,werecodethemtodummybinaryvariablesfortheregression.Fortwoofourmetrics(percFixIn-sideandnFix),theeffectofthesubjectwassignificant(p=0.002andp=0.005,respectively,inWald’stest),indicatingthatwecannottreatthesamplesasindependent;wethereforereportsignificancevaluesgivenbymultilevelmodeling.Fortheothertwometrics(fra-mesToROIandscanpathError),theeffectofthesubjectwasfoundtobenon-significant(p=0.201andp=0.046,respectively).Therefore,samplescanbeconsideredindependent,andweperformfactorialANOVA,togetherwithBonferroniposthocanalysestofurtherlookforsignificanteffectsinourdata.Throughouttheanalysisweuseasignificancelevelof0.01.InfluenceofpreviousVRexperience.Inadditiontoanalyzingtheinfluenceofthedifferentfactors,detailedbelow,wealsoanalyzewhetherthesubjects’previousexperienceusingVRhadaneffectontheresults.Werecordthisinformationinthepre-testquestionnaire.NoneofoursubjectsusedVRfrequently,but69%ofthemhadusedVRbeforeatsomepoint.WhenlookingattheeffectofthispreviousVRexperienceonthemetricsemployed,wefoundthatithadnoeffectonanyofthemetricstested(p=0.600forpercFixInside,p=0.832fornFix,p=0.197forframesToROI,andp=0.480forscanpathError).Thisistobeexpected,sinceanoccasionalorrareuseofVRisunlikelytocauseanychangeintheresults.InfluenceofalignmentA.Thefirstthingweobserveisthatthereisacleareffectofthealignmentfactoronthefourdependentvariables(metrics)understudy.InthecaseoftheframesToROI(F(2,787)=198.059,p<0.001),theBonferroniposthocfurthershowsasignificantdifference(p<0.001)betweenallthreelevels(A0,A40andA80).Asexpected,thefurtherawaytheROIis,thelongerittakesviewerstofindit.Interestingly,themetricsugge-stsanexponentialtrendwiththedegreesofmisalignment.ThisisshowninFig.6(left),whichincludesthegoodnessoffit,andthe95%confidenceinterval.Fig.7alsoillustratesthis,withstrongpeaksandlargertailsofbackgroundfixationsaftertheedit(t=6secs.)forA80(bottomrow)thanA0(toprow).ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.47:8•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.MasiaFig.6.Left:AverageframesToROIforeachalignment.Thegreenandbluecurvesshowaveragedataforthetwotypesofedit(E1andE2,respectively).Wealsoshowafittoanexponentialfunction,withtheassociated95%confidenceinterval.Right:MeanRMSEwithrespecttothebaselinebeforetheedit,andaftertheeditafterseeingtheROI(scanpathError)forthedifferentalignmentconditionstested.Inbothplots,errorbarsshowa95%confidenceintervalforthemean.OurscanpathErrormetric(F(2,787)=14.511,p<0.001)allowsustodigdeeperintothisfinding,showingintheposthocanalysesthatthereisnosignificantdifferencebetweenA0andA40(p=0.277),whileA80issignificantlydifferenttobothofthem(p≤0.001inbothcases).ThisisshowninFig.6(right),comparingdirectlywiththeequivalentvaluesbeforetheedit(where,asexpected,nosignificantdifferencewasfound).AsimilartrendcanbeseeninpercFixInside:A80issignificantlydifferenttoA0(p<0.001),butA40isnot(p=0.138).ThiseffectseemstoindicatethatthelargemisalignmentaltersviewerbehaviornotonlyinthetimeittakestofixateontheROI,butalsoafteritisfound.AcloserlookrevealsthatthesamesignificantdifferenceholdsfornFix:thenumberoffixationsissignificantlylower(p=0.003)forA80comparedtoA0,butthisisnotthecaseforA40(p=0.954).This,alsoshowninFig.8(top)asaradarplot,isaveryinterestingfinding,suggestingthatviewerscouldbemoreinclinedtoexplorethescenewhenthereisahighmisalignmentacrosstheeditboundary.InfluenceoftypeofeditE.Interestingly,thetypeofedit(E)hasnoeffectonthefixationalbehavioraftertheeditafterfixatingontheROI(p=0.674andp=0.430forpercFixInsideandnFix,respectively).ThetypeofeditdidnothaveasignificanteffectonscanpathError(F(1,787)=0.038,p=0.846)either,buttheinteractionsofthetypeofeditwithbothROIconfigurationsdid(p=0.002inbothcases).Surprisingly,thetypeofedithadnosignificanteffectontheframesToROIeither(F(1,787)=1.373,p=0.242),ashintedinFig.6.InfluenceofROIconfigurationsRbandRa.Weobservenosigni-ficantinfluenceofthesefactorsonnFix,indicatingthatROIconfi-gurationdoesnotinfluencetheexploratorybehavior(howmuchviewersfixate,ingeneral)oftheviewersaftertheeditoncetheyseeoneoftheROIs.Interestingly,however,RbhasaneffectonpercFixInside,i.e.,onhowmuchviewersfixateontheROI(s)aftertheeditafterfixating,comparedtothetotalnumberoffixationsinthattimeperiod.Notethat,whiledifferentROIconfigurationsmayimplybynaturedifferentnumberoffixationsinside,wearecompen-satingforthiseffectinthecomputationofpercFixInside(Sec.6.2).Specifically,RbrevealsadifferencebetweentwoROIsinthesameFOV,andoneROI(Rb,1vs.Rb,0,p=0.015),butnotincaseoftwoROIsindifferentFOVs(Rb,2vs.Rb,0,p=0.792).ThiscanbeseeninFig.8(middle):twoROIsinthesameFOVbeforetheeditleadtolessfixationsontheROI(s)aftertheedit.WehypothesizethatthisisbecausemultipleROIsbeforetheeditelicitamoreexploratorybehavioraftertheedit,insearchformoreROI(s)evenafterhavingfixatedonone.WealsofoundasignificantinfluenceoftheROIconfigurationaftertheeditRaonthedeviationofthescanpathwrt.thebaseline,scanpathError(F(2,787)=168.569,p<0.001forRa);meanwhile,Rbhadnosignificantinfluence(F(2,787)=1.660,p=0.191forRb).BonferroniposthocsshowthatRa,2issignificantlydifferenttotheothertwo(p<0.001),whileRa,0andRa,1arenotsignificantlydifferentbetweenthem(p=0.804).Fig.8(bottom)showsthiseffect:thescanpathErrorissignificantlyhigherforRa,2(twoROIsindifferentFOVs),indicatingthatthereismorevariabilityinthescanpathssincethetwoROIscannotbelookedatsimultaneously.Finally,bothRbandRahadalsoasignificanteffectonframesToROI(F(2,787)=6.478,p=0.002forRb,andF(2,787)=10.300,p<0.001forRa).Othereffects.Additionally,wecanobservesomeneweffectsinthestatedistributionsequences(Figs.7and9).Inparticular,wefindanexplorationpeakrightatthebeginningofeachclip,bothwhenthevideostartsandrightaftertheedit;thispeakusuallylastsaround1-2seconds.Itisfollowedbyanattentionpeak,againlastingaround1-2seconds.ThiseffectappearsregardlessoftheROIconfigurationsandthealignment,andcanbeobservedinFig.7.Thissuggeststhatusersrequiresometimetounderstandtheirenvironmentandstabilizetheirgazepatternswhenachangeofscenariooccurs;afterthattransitorystate,however,theirgazeisstronglyattractedtotheactionsbeingperformed(theROIs).Last,weanalyzemoreindepththeeffectofthetwotypesofedits(E1andE2)intheparticularcaseof(Rb,0,Ra,0)(editsfromoneROItooneROI).Thisisoneofthesimplestcases,butalsooneofthemostrelevant,sincemanycurrentVRfilm-makingstrategiesarecommonlybasedonasingleROIacrossscenes.InFig.9weshowthestatedistributionforthisparticularcase(Rb,0,Ra,0)foralignmentsA0andA80,andforthetwotypesofedits.Eventhoughwefoundnosignificanteffectofthetypeofeditinourmetrics,thegraphssuggestadifferencethatourmetricsarenotcapturing.Inparticular,itseemsthatE2attractsmoreattentiontotheROIaftertheeditthanE1,asseeninthedeeperbluevalleyaftertheeditintherightcolumn),andthiseffectisconsistentacrossallalignments.ApotentialexplanationisthatthecontinuityinactionbeforeandaftertheE2editactsasananchor.7DISCUSSIONANDCONCLUSIONSToourknowledge,ourworkisthefirsttoattemptasystematicanalysisofviewerbehaviorandperceivedcontinuityinnarrativeVRcontent.Asystematicexplorationofthistopicischallengingfortwomainreasons:(i)theextremehighdimensionalityofitsparameterspace;and(ii)thatitinvolvesmanydiscrete,categorical(asopposedtointervalorordinal)variablesofinfluence.Moreover,otherbasicissuesneedtobeaddressed,suchas:Howdoesonemeasurecontinuity,orviewerbehavior?Whicharethebestmetricstouse?Areourobservationsindependentofthesubjects?WehaveACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.MovieEditingandCognitiveEventSegmentationinVRVideo•47:9Fig.7.StatedistributionforallthedifferentcombinationsofRbandRa,andforalignmentsA0(firstrow)andA80(secondrow).ThedifferenttypesofeditsEareaggregatedineachoftheaforementionedconditions.Theabscissaeshowtimeinseconds(theedittakesplaceatt=6),whiletheordinatesshowaggregatedpercentageofusers.Eachplotshowsthepercentageofusersineachstateateachtimeinstant.TheshortIdleperiodsatthebeginningandtheendareduetoblackframesbeforeandafterthemovie,andthusarenotsignificanttoouranalysis.Thetworedlinesonthetop-leftimageillustratetheexplorationandattentionpeaks,respectively,reportedinthepaper.reliedontheeventsegmentationtheory,whichhasprovideduswithsomesolidgroundtocarryoutourresearch,andhaveanalyzedpreviousrelatedstudiesontraditionalcinematography.OurresultsmayhavedirectimplicationsinVR,informingcontentcreatorsaboutthepotentialresponsesthatcertaineditconfigurati-onsmayelicitintheaudience.Forinstance,forafast-pacedactionmovieourresultssuggestthatROIsshouldbealignedacrossedits,whiletoevokeamoreexploratorybehavior,misalignmentsarere-commended.Additionally,fromallthenarrative360◦movieswehaveexplored,wehavefoundaninterestingtrendinthenumberandclassificationofedits:whileinVRmoviesthegreatmajorityofeditsaretypeE1(actiondiscontinuity),theyarebyfartheleastfrequentintraditionalcinematography,whereE3continuityeditsarethemostprominent.Forexample,TheRedBalloonhas85con-tinuityedits,67spatial/temporaldiscontinuities,andonly18actiondiscontinuities.WebelievethisisduetotheimmersivenatureofnarrativeVR,whereanexcessivenumberofcontinuityeditswouldreduceopportunitiesforfreeexploration.Intherestofthesection,wesummarizeourmainfindings,andoutlineinterestingareasoffutureworkahead.CognitionandeventsegmentationinVR.WehavefirstreplicatedanexistingcognitivestudycarriedoutontheTheRedBalloonmovie,andfoundmanysimilaritiesinVR.Likeintraditionalcinemato-graphy,actiondiscontinuitiesdominateeventsegmentationinVR,becomingthestrongestpredictorsofeventboundaries.ContinuityeditsdosucceedinmaintainingtheperceivedcontinuityalsoinVR,despitethevisualdiscontinuityacrosseditboundaries.Thissuggeststhatviewersbuildamentalmodeloftheshowneventstructurethatissimilartowatchingatraditionalmovie,despitethedrasticallydifferentviewingconditions.Measuringcontinuityeffects.Ouranalysishasrevealedseveralotherinterestingfindings.Moreover,mostofourreportedfindingshavesignificantvaluesofp<0.01;thisminimizestheriskoffalsepositivesinourconclusions.TherelationbetweenhowmisalignedaROIappearsafteranedit,andhowlongittakesviewerstofixateonit,seemstobeexponential;thiscouldbeusedasaroughguidelinewhenperformingedits.Evenmoreimportantly,largemisalignmentsacrosseditboundariesdoaltertheviewers’behaviorevenaftertheyhavefixatedonthenewROI.Apossibleinterpretationisthatthemisalignmentfostersamoreexploratorybehavior,andthuscouldbeusedtocontrolattention.TwoROIsinthesameFOVbeforeaneditseemtoelicitamoreexploratorybehavioraswell,evenafterhavinglocatedoneROIaftertheedit.Othereffectsnotcaughtbyourmetricscanbeinferredbyvisualinspectionofthestatedistributions.Thereseemstobeatexplorationpeakatthebeginningofeachclip,andasimilarattentionpeakrightaftertheedit,independentofthetypeofedit.Bothsuggestthatusersrequiresometimetoadapttonewvisualcontent,beforetheirgazefixatesonROIs.Also,itappearsthattheROIattractsmoreattentionafteranE2editthanafteratypeE1,perhapsbecausetheconsistentactionbeforeandaftertheeditactsasananchor.Limitationsandfuturework.Asinallstudiesofsimilarnature,ourresultsareonlystrictlyvalidforourchosenstimuli.Wehavefocusedonshort360◦videosforseveralreasons:toisolatesimpleactions,avoidingconfoundingfactors;togaincontroloverthesti-muli,enablingasystematicexplorationoftheparameterspace;andtofacilitatetheanalysisofthegathereddata.Someofourfindingsmaythereforenotgeneralizetoconditionsoutsideourstudy.Ofcourse,manyothervariablesandparameterscanbeexploredinfuturework,suchasothertypesofcinematographiccuts,longermovies,morecomplexvisualcontent,theinfluenceofsound,ortheeffectoffatigueorfrequentexposuretoVRcontent.Morecompre-hensivesubjectivedatamayalsobeavaluablesourceofinformation,togetherwithourobjectivegazedata.Webelievethatthejointstudyofcognitivemechanismsandcinematographictechniquesprovidesasolidgroundtocarryoutthisresearch.Insummary,webelievethatourworkisatimelyeffort,sinceVRvideosareafast-growingnewmediumstillinitsinitialexploratoryphase,withmanycontentcreatorstestingwaystocommunicatesto-riesthroughit.WehopethatourfindingswillbeusefulasguidelinesforVRcontentcreators,especiallyamateurs,acrossareasonablerangeofsituations.ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.47:10•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.MasiaFig.8.RadargraphsshowingvariationofthreeofourmetricswithA,RbandRa.VariationwithEisnotshown.Ineachgraph,thethreecurvescorrespondtothethreealignmentconditions,aslabeledinthelegend.TheradiiofthegraphcorrespondtothedifferentcombinationsbetweenRaandRb.Ravaluesarewrittenateachpointintheperimeter,whilethelargecoloredsectors(blue,grayandgreen),correspondtoRb(Rb,0,Rb,1andRb,2respectively,asindicated).Top:NumberoffixationsaftertheeditafterfixatingontheROI(nFix),whichissignificantlydifferentforA80thanforA40andA0.Thescaleoftheradialaxisisenlargedforvisualizationpurposes.Middle:ValueofpercFixInsideforthedifferentconditions;percFixInsideissignificantlyaffectedbytheROIconfigurationbothbeforeandaftertheedit(seetextfordetails).Bottom:MeanRMSEwithrespecttothebaselineaftertheeditafterfixatingontheROI(scanpathError).Pleaseseethetextfordetails.ACKNOWLEDGEMENTSWewouldliketothankPazHernandoandMartaOrtinfortheirhelpwiththeexperimentsandanalyses.WewouldalsoliketoFig.9.Statedistributionfor(Rb,0,Ra,0),foralignmentsA0andA80,andforthetwodifferenttypesofeditsE1(left)andE2(right).Althoughourmetricsdidnotcapturethiseffect,itappearsthatE1editsmightbehardertounderstandthanE2,asindicatedbythedeeperbluevalleyaftertheeditintherightcolumn.thankCubeFXforallowingustoreproduceStarWars-HuntingoftheFallen,andAbacodigitalforhelpingusrecordthevideosforcreatingthestimuli,aswellasSandraMalpicaandVictorArellanoforbeinggreatactors.ThisresearchhasbeenpartiallyfundedbyanERCConsolidatorGrant(projectCHAMELEON),andtheSpanishMinistryofEconomyandCompetitiveness(projectsTIN2016-78753-P,TIN2016-79710-P,andTIN2014-61696-EXP).AnaSerranowassupportedbyanFPIgrantfromtheSpanishMinistryofEconomyandCompetitiveness.DiegoGutierrezwasadditionallyfundedbyaGoogleFacultyResearchAwardandtheBBVAFoundation.GordonWetzsteinwassupportedbyaTermanFacultyFellowship,anOkawaResearchGrant,andanNSFFacultyEarlyCareerDevelopment(CAREER)Award.REFERENCESJosephD.Anderson.1996.TheRealityofIllusion:AnEcologicalApproachtoCognitiveFilmTheory.SouthernIllinoisUniversityPress.IdoArev,HyunSooPark,YaserSheikh,JessicaK.Hodgins,andArielShamir.2014.Automaticeditingoffootagefrommultiplesocialcameras.ACMTrans.Graph.33,4(2014),81:1–81:11.FloraineBerthouzoz,WilmotLi,andManeeshAgrawala.2012.Toolsforplacingcutsandtransitionsininterviewvideo.ACMTrans.Graph.31,4(2012),67:1–67:8.I.Biederman.1987.Recognition-by-Components:ATheoryofHumanImageUnder-standing.PsychologicalReview94(1987),115–147.DavidBordwell,KristinThompson,andJeremyAshton.1997.Filmart:Anintroduction.Vol.7.McGraw-HillNewYork.W.BrowneandJ.Rasbash.2004.MultilevelModelling.InHandbookofdataanalysis.SagePublications,459–478.JohnMCarrollandThomasGBever.1976.Segmentationincinemaperception.Science191,4231(1976),1053–1055.SusanaCastillo,TilkeJudd,andDiegoGutierrez.2011.UsingEye-TrackingtoAs-sessDifferentImageRetargetingMethods.InSymposiumonAppliedPerceptioninGraphicsandVisualization(APGV).ACMPress.GaelChandler.2004.Cutbycut.MichaelWieseProductions.DavidBChristianson,SeanEAnderson,Li-weiHe,DavidHSalesin,DanielSWeld,andMichaelFCohen.1996.Declarativecameracontrolforautomaticcinematography.InAAAI/IAAI,Vol.1.148–155.MarcChristie,RumeshMachap,Jean-MarieNormand,PatrickOlivier,andJonathanH.Pickering.2005.VirtualCameraPlanning:ASurvey.InInt.SymposiumonSmartGraphics.40–52.NeilCohn.2013.VisualNarrativeStructure.CognitiveScience37,3(2013),413–452.AntoineCoutrotandNathalieGuyader.2014.Howsaliency,faces,andsoundinfluencegazeindynamicsocialscenes.Journalofvision14,8(2014),5–5.ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.MovieEditingandCognitiveEventSegmentationinVRVideo•47:11JamesCutting.2004.PerceivingSceneinFilmandintheWorld.InMovingimagetheory:ecologicalconsiderations,J.D.AndersonandB.F.Anderson(Eds.).Chapter1,9–26.JamesECutting.2014.Eventsegmentationandseventypesofnarrativediscontinuityinpopularmovies.Actapsychologica149(2014),69–77.NicholasM.Davis,AlexanderZook,BrianO’Neill,BrandonHeadrick,MarkRiedl,AshtonGrosz,andMichaelNitsche.2013.Creativitysupportfornovicedigitalfilmmaking.InProc.ACMSIGCHI.651–660.EdwardDmytryk.1984.OnFilmEditing.AnIntroductiontothèArtofFilmCon-struction.(1984).AlexisGabadinho,GilbertRitschard,NicolasMÃijller,andMatthiasStuder.2011.Ana-lyzingandVisualizingStateSequencesinRwithTraMineR.JournalofStatisticalSoftware40,1(2011).QuentinGalvane,RémiRonfard,ChristopheLino,andMarcChristie.2015.Continuityeditingfor3danimation.InAAAIConferenceonArtificialIntelligence.Li-weiHe,MichaelFCohen,andDavidHSalesin.1996.Thevirtualcinematographer:aparadigmforautomaticreal-timecameracontrolanddirecting.InProceedingsofthe23rdannualconferenceonComputergraphicsandinteractivetechniques.ACM,217–224.RachelHeck,MichaelN.Wallick,andMichaelGleicher.2007.Virtualvideography.TOMCCAP3,1(2007).JulianHochbergandVirginiaBrooks.2006.Filmcuttingandvisualmomentum.Inthemind’seye:JulianHochbergontheperceptionofpictures,films,andtheworld(2006),206–228.EaktaJain,YaserSheikh,ArielShamir,andJessicaHodgins.2014.Gaze-drivenVideoRe-editing.ACMTransactionsonGraphics(2014).ThomasCKübler,KatrinSippel,WolfgangFuhl,GuilhermeSchievelbein,JohannaAufreiter,RaphaelRosenberg,WolfgangRosenstiel,andEnkelejdaKasneci.2015.AnalysisofeyemovementswithEyetrace.InInternationalJointConferenceonBiomedicalEngineeringSystemsandTechnologies.Springer,458–471.ChristopherAKurbyandJeffreyMZacks.2008.Segmentationintheperceptionandmemoryofevents.Trendsincognitivesciences12,2(2008),72–79.OlivierLeMeurandThierryBaccino.2013.Methodsforcomparingscanpathsandsaliencymaps:strengthsandweaknesses.Behaviorresearchmethods45,1(2013),251–266.OlivierLeMeur,ThierryBaccino,andAlineRoumy.2011.PredictionoftheInter-ObserverVisualCongruency(IOVC)andapplicationtoimageranking.InProc.ACMMultimedia.ACM,373–382.ZhengLuandKristenGrauman.2013.Story-DrivenSummarizationforEgocentricVideo.InProc.IEEECVPR.2714–2721.JosephMaglianoandJeffreyM.Zacks.2011.TheImpactofContinuityEditinginNarrativeFilmonEventSegmentation.CognitiveScience35,8(2011),1489–1517.BobbieO’Steen.2009.TheInvisibleCut.MichaelWieseProductions.AbhishekRanjan,JeremyP.Birnholtz,andRavinBalakrishnan.2008.Improvingmeetingcapturebyapplyingtelevisionproductionprincipleswithaudioandmotiondetection.InProc.ACMSIGCHI.227–236.https://doi.org/10.1145/1357054.1357095S.W.RaudensbushandA.S.Bryk.2002.HierarchicalLinearModels.SagePublications.JeremyR.Reynolds,JeffreyM.Zacks,andToddS.Braver.2007.AComputationalModelofEventSegmentationFromPerceptualPrediction.CognitiveScience31,4(2007),613–643.VincentSitzmann,AnaSerrano,AmyPavel,ManeeshAgrawala,DiegoGutierrez,andGordonWetzstein.2016.SaliencyinVR:Howdopeopleexplorevirtualenviron-ments?arXivpreprintarXiv:1612.04335(2016).TimJSmith.2012.Theattentionaltheoryofcinematiccontinuity.Projections6,1(2012),1–27.TimJSmithandJohnMHenderson.2008.EditBlindness:Therelationshipbetweenattentionandglobalchangeblindnessindynamicscenes.JournalofEyeMovementResearch2,2(2008).AnhTruong,FloraineBerthouzoz,WilmotLi,andManeeshAgrawala.2016.QuickCut:AnInteractiveToolforEditingNarratedVideo.InProceedingsofthe29thAnnualSymposiumonUserInterfaceSoftwareandTechnology,UIST2016,Tokyo,Japan,October16-19,2016.497–507.Hui-YinWuandMarcChristie.2015.Stylisticpatternsforgeneratingcinematographicsequences.InWorkshoponIntelligentCinematographyandEditing.JeffreyMZacks.2010.Howweorganizeourexperienceintoevents.PsychologicalScienceAgenda24,4(2010).JeffreyMZacks,NicoleKSpeer,KhenaMSwallow,andCoreyJMaley.2010.Thebrain’scutting-roomfloor:Segmentationofnarrativecinema.Frontiersinhumanneuroscience4(2010),168.JeffreyMZacksandKhenaMSwallow.1976.Foundationsofattribution:Theperceptionofongoingbehavior.Newdirectionsinattributionresearch1,223–247.JeffreyMZacksandKhenaMSwallow.2007.Eventsegmentation.Currentdirectionsinpsychologicalscience14,2(2007),80–84.APPENDIXStimuliI.Definitionofcinematographiccuttechniquesusedfortheedits•Compressed-timecut:Itrepresentsthepassingofalongerperiodoftimebycuttingtogetherkeyshots(e.g.,acha-ractermakingcoffeeinakitchen;whilethewholeeventinreallifecanlasttwoorthreeminutes,itcanbequicklysummarizedinafewsecondswithafewquickshotssho-wingherpouringwater,grindingthecoffee,lettingitbrew,thenpouringitintoacup).•Match-on-actioncut:Acutwherethesecondshotmatchestheactioninthefirstshot(e.g.,acharactergoingthroughadoor;asthedoorstartstoopen,cuttothecharactergoingthroughthedoorfromtheotherside).•Jumpcut:Althoughtheyneedtobeavoidedingeneralwhenshootingthesamescene[Arevetal.2014],theyarecommonlyusedtocreateanabrupttransitionfromonescenetoanother.II.MoviesusedforassessingcontinuityeditinginVRWeusedfourpubliclyavailablemoviesforcarryingouttheexperi-mentdescribedinSec.4:•StarWars-Huntingofthefallenhttps://youtu.be/SeDOoLwQQGoDuration:8:00minutes.•Always-AVRstoryhttps://youtu.be/Tn_V8sVSnoUDuration:5:35minutes.•InvisibleEpisode2-BackInTheFoldhttps://youtu.be/M3FO3j2z5TkDuration:4:42minutes.•InvisibleEpisode5-IntoTheDenhttps://youtu.be/M3FO3j2z5TkDuration:4:05minutes.III.DetailsonROIalignmentWemanuallyalignedtheshotsofourcutswithAdobePremiere2015CC.Inordertodoso,wemadesurethatROIswerealignedbeforeandaftertheeditsfortheA0condition.Oncethiseditwasgenerated,wemisalignedthesecond(after-the-cut)shotwithrespecttothealignedpositionby40degreesfortheA40condition,andby80degreesfortheA80condition.Themisalignmentwasrandomlyperformedtotheleftorright,butensuringanequalnumberofstimuliineachdirection.InFig.10weshowsomeexamplesofdifferentalignmentsanddispositionoftheROIsinourcuts.GazedataprocessingWecollectedgazepointswiththeeyetrackerandheadpositionswiththeOculusDK2.Wedescribeherethemainaspectsoftheprocessingofthisdata.Gazescanpaths:First,weprocessedtheeyetrackersamples.Wediscardedfulltrialsfromtheeyetrackerwhenthemeanconfidenceforbotheyeswaslowerthan0.6(theconfidencerangesfrom0to1).ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.47:12•A.Serrano,V.Sitzmann,J.Ruiz-Borau,G.Wetzstein,D.Gutierrez,andB.MasiaFig.10.ExamplesofdifferentalignmentsanddispositionsoftheROIsinourcuts.Fromtoptobottom:Alignmentscorrespondingto0degreesforA0,40degreesforA40,and80degreesforA80.Fromlefttoright:Editsalignedfrom1ROIto1ROI,to2ROIsinthesamefieldofview,andto2ROIsindifferentfieldofview.Wethenlinearlyinterpolatedallmeasurementsfromtheeyetrackerwhoseconfidencewasbelow0.9.Additionally,theheadpositiontrackerfromtheOculusDK2hasalowersamplingratethantheeyetracker,soinordertomatchthedifferentsamplingrates,wematchedeachgazemeasurementwiththeclosesttimestampoftheheadposition.Second,wematchedgazepositionstoframesinourvideos.Sincevideoshadaframerateof60fpsandtheeyetrackerrecordedat120Hz,2gazepositionswererecordedperframe.Weassignedasinglegazepointtoeachframebycomputingthemeanofthegazepointscorrespondingtothatframe[CoutrotandGuyader2014].Finally,wedefineagazescanpathastheresultingtemporalsequenceofgazepositions.Fixationdetection:Weperformfixationdetectionforourgazescan-pathswithavelocity-basedfixationdetector.Weconsiderthatagazepointisafixationwhenitsvelocityisbelowacertainthreshold.Wecalculatethisthresholdforeachscanpathas20%ofthemaximumvelocity,afterdiscardingthe2ndpercentileoftopvelocities[Kübleretal.2015].Outlierrejection:Wediscardoutliersundertwocriteria.First,wediscardobservationswhenlessthan40%ofthetotalnumberoffixationsbeforethecutoccurredinsidetheROI.Weconsiderthatinsuchcasesuserswerenotpayingattention,ordidnotunderstandthetask.Second,wediscardobservationsthatdifferedsignificantlyfromotherusers’behavior.Wedothisbyfollowingaconservativestandardoutlierrejectionapproach,inwhichanobservationisdiscardedifitfulfilsoneofthefollowingconditions:observation<(Q1−kd∗Qd)observation>(Q3+kd∗Qd)(1)whereQ1andQ3arethefirstandthethirdquartile,respectively;Qd=Q3−Q1;andkd=1.5.ACMTransactionsonGraphics,Vol.36,No.4,Article47.Publicationdate:July2017.