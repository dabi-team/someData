8
1
0
2

y
a
M
0
1

]

V
C
.
s
c
[

2
v
5
0
5
6
0
.
9
0
7
1
:
v
i
X
r
a

SalNet360:SaliencyMapsforomni-directionalimageswithCNNRafaelMonroy∗,SebastianLutz∗,TejoChalasani∗,AljosaSmolic{monroyrr,lutzs,chalasat,smolica}@scss.tcd.ieTrinityCollegeDublin,IrelandAbstractThepredictionofVisualAttentiondatafromanykindofmediaisofvaluableusetocontentcreatorsandusedtoeﬃcientlydriveencodingalgorithms.WiththecurrenttrendintheVirtualReality(VR)ﬁeld,adaptingknowntechniquestothisnewkindofmediaisstartingtogainmomentum.Inthispaper,wepresentanarchitecturalextensiontoanyConvolutionalNeuralNetwork(CNN)toﬁne-tunetraditional2DsaliencypredictiontoOmnidirectionalImages(ODIs)inanend-to-endmanner.Weshowthateachstepintheproposedpipelineworkstowardsmakingthegeneratedsaliencymapmoreaccuratewithrespecttogroundtruthdata.Keywords:saliency,omnidirectionalimage(ODI),convolutionalneuralnetwork(CNN),virtualreality(VR)1.IntroductionTheﬁeldofVirtualReality(VR)hasexperiencedaresurgenceinthelastcoupleofyears.MajorcorporationsarenowinvestingconsiderableeﬀortsandresourcestodelivernewHead-MountedDisplays(HMDs)andcontentinaﬁeldthatisstartingtobecomemainstream.DisplayingOmni-DirectionalImages(ODIs)isanapplicationforVRhead-sets.Theseimagesportrayanentiresceneasseenfromastaticpointofview,∗Theseauthorscontributedequallytothiswork.PreprintsubmittedtoElsevierSignalProcessingJournal:ImageCommunicationsMay9,2018 
 
 
 
 
 
andwhenviewedthroughaVRheadset,allowforanimmersiveuserexperience.ThemostcommonmethodforstoringODIsisbyapplyingequirectangular,cylindricalorcubicprojectionsandsavingthemasstandardtwo-dimensionalimages[1].OneofthemanydirectionsofresearchintheVRﬁeldisVisualAttention,themaingoalofwhichistopredictthemostprobableareasinapicturetheaveragepersonwilllookat,byanalysinganimage.AsshownbyRaielat.[2],visualattentionistheresultoftwokeyfactors:bottom-upsaliencyandtop-downperceived.Inordertocollectgroundtruthdata,experimentsareperformedinwhichsubjectslookatpictureswhileaneye-tracker,togetherwiththeInertialMeasurementUnitoftheheadsetinuse,recordsthelocationintheimagetheuserislookingat[3].Bycollectingthisdatafromseveralsubjects,itispossibletocreateasaliencymapthathighlightstheregionswheremostpeoplelookedat.Knowingwhichportionsinanimagearethemostobservedcanbeused,forexample,todrivecompressionandsegmentationalgorithms[4][5].Earlierworksonimagesaliencymadeuseofmanually-designedfeaturemapsthatrelatetosalientregionsandwhencombinedinsomeformproduceaﬁnalsaliencymap[6][7][8].Withtheadventofdeepneuralnetworks,GPU-optimisedcodeandavailabilityofvastamountsofannotateddata,researcheﬀortsincomputervisiontaskssuchasrecognition,segmentationanddetectionhavebeenfocusedoncreatingdeeplearningnetworkarchitecturestolearnandcombinefeaturesmapsautomaticallydrivenbydata[9][10][11].AsevidentfromtheMITSaliencyBenchmarks[12],variantsofConvolutionalNeuralNetworks(CNNs)arethetop-performingalgorithmsforgeneratingsaliencymapsintraditional2Dimages.Goodtechniquesfortransferlearning[13]madesurethatCNNscanbeusedforpredictingusergazeevenwitharelativelysmallamountofdata[14][15][16][17].Saliencypredictiontechniquesoriginallydesignedfortraditional2DimagescannotbedirectlyusedonODIsduetotheheavydistortionspresentinmostprojectionsandadiﬀerentnatureinobservedbiases.DeAbreuetal.[18]demonstratedthatremovingthecentrebiaspresentonmosttechniquesfor2traditional2DimagessigniﬁcantlyimprovestheperformancewhenestimatingsaliencymapsforODIs.ArecenteﬀorttoattractattentiontotheproblemofcreatingsaliencymapsforODIswaspresentedintheSalient360!GrandChallengeattheICME2017Conference[19].ParticipantswereprovidedwithaseriesofODIs(40intotal)forwhichhead-andeye-trackinggroundtruthdatawasgivenintheformofsaliencymaps.ThedetailsonhowthedatasetwasbuiltaredescribedbyRaietal.[3].Twenty-ﬁvepairedimagesandgroundtruthwerelaterprovidedtoproperlyevaluatethesubmissionstothechallenge.Inthepresentedpaperwefollowtheexperimentalconditionsofthechallenge,i.e.weusedtheinitial40imagestotraintheproposedCNNandtheresultswepresentwerecalculatedusingthe25testimages.TheapproachwepresentissimilartothatofDeAbreuetal.[18]inthatthecoreCNNtocreatethesaliencymapscanbeswitchedoncebetternetworksbecomeavailable.WestartwiththepremisethattheheavydistortionsnearthepolesinanequirectangularODInegativelyaﬀecttheﬁnalsaliencymapandthenatureofthebiasesneartheequatorandpolesdiﬀersfromthoseinatraditional2Dimage.Toaddresstheseissues,wemakethefollowingcontributions:•SubdividingtheODIintoundistortedpatches.•ProvidingtheCNNwiththesphericalcoordinatesforeachpixelinthepatches.Thispaperisorganisedasfollows.Section2enlistsworkrelatedtosaliencymapsfortraditional2DimagesandODIs.Then,Section3describesthepipelineandtherequiredpre-andpost-processingsteps.Theend-to-endtrainableCNNarchitectureweusehereisthendescribedinSection4.ResultsarepresentedinSection5.Finally,inSection6ideasforfutureresearchopportunitiesandconclusionsareprovided.32.PreviousworkUntiltheadventofCNNs,mostsaliencymodelsreliedonextractingfeaturemaps,whichdepicttheresultofdecomposingtheimage,highlightingaparticu-larcharacteristic.Thesefeaturemapsarethenlinearlycombinedbycomputingaseriesofweightstobeappliedtoeachmap.FeatureslikethosepresentedbyIttiandKoch[6]wereinitiallybasedonknowledgeofthelow-levelhumanvision,e.g.color,orientationandfrequencyresponses.Thecatalogueoffeaturesusedtocalculatesaliencymapshasincreasedovertheyears,includingforexample,globalfeatures[7],facedetectionfeatures[20],locationbiasfeatures[21]andothers.Juddetal.[8]createdaframeworkandmodelthatcombinedmanyofthesefeaturestocomputesaliencymaps.TheMITSaliencyBenchmarkwaslaterdevelopedbyBylinskiietal.,allow-ingresearcherstosubmitandcomparetheirsaliencycomputationalgorithms[12].WiththeincreasingavailabilityofVRheadsets,saliencycomputationmeth-odsspeciﬁcallytailoredforODIshavestartedtosurface.DuetothefactthatODIsdescribe,inactuality,asphere,saliencymodelsdesignedfortraditional2Dimagescannotbedirectlyused.Bogdanovaetal.analysethespheredepictedinanODI,creatinglow-levelvisionfeaturemapsbasedonintensity,colourandorientationfeatures[22].Inrecentyears,theinterestinCNNshasgrownandmostofthetop-performingsubmissionsintheMITSaliencyBenchmarksystemcorrespondtoCNN-basedapproaches.OneoftheﬁrstmethodstouseCNNsforsaliencywaspresentedbyViget.al.[23],whichusedfeaturemapsfromconvolutionlayersastheﬁnalfeaturesofalinearclassiﬁer.K¨ummereret.al.[13]publishedanextensionofthisapproachtouseCNNsasfeatureextractors.Theyextendedtheirapproachin[24],wheretheyusetheVGG[25]featurestopredictsaliencywithnoﬁne-tuning.InsteadtheyonlytrainafewreadoutlayersontopoftheVGGlayers.ThemethoddevelopedbyLiuet.al.[14]usedaMulti-ResolutionCNN,wherethenetworkwastrainedwithimageregionscentredonﬁxation4Figure1:ODISaliencyDetectionPipeline.andnon-ﬁxationeyelocationsatseveralscales.AnotherapproachusingCNNstoextractfeatureswasshownbyLiet.al.[26],wheretheyobtainedfeaturesfromconvolutionallayersatseveralscalesandcombinedthemwithhandcraftedfeatureswhichweretrainedonaRandomForest.Oneoftheﬁrstend-to-endtrainedCNNsforsaliencypredictionwasintroducedbyPanet.al.[17].SimilarworkfromKruthiventietal.[16]addedanovelLocationBiasedConvolutionalLayer,whichallowedthemtomodellocationdependentpatterns.AsCNNsarebecomingmorecomplexandsophisticatedinotherComputerVisiondomains,theseadvancesarebeingtransferredtosaliencypredictionap-plications,forexample,themethodproposedbyPanet.al.[27],introducingadversarialexamplestotraintheirnetwork.TheworkofLiuet.al.[28]in-troducesadeepspatialcontextualLSTMtoincorporateglobalfeaturesintheirsaliencyprediction.TheamountofpublishedresearchfocusingonCNNsappliedtoODIsisatthemomentverysmall.DeAbreuet.al.[18]usestranslatedversionsoftheODIasinputtoaCNNtrainedfortraditional2Dimagestogenerateanomnidirectionalsaliencymap.5(a)FullODI(b)NadirviewFigure2:ExampleofdistortionspresentinequirectangularODIs.3.MethodInthissectionwedetailthepipelineusedtoobtainthesaliencymapforagivenODI.Fig.1showsadiagramwiththecompletepipeline.OurmethodtakesanODIasinputandsplitsitintosixpatchesusingthepre-processingstepsdescribedin3.1.EachofthesesixpatchesissentthroughtheCNN,thedetailsofwhichwedelveintoinSection4.TheoutputoftheCNNforallthepatchesarethencombinedusingthepost-processingtechniquementionedinSection3.23.1.Pre-processingMappingasphereontoaplanerequiresintroducingheavydistortionstotheimage,whicharemostevidentatthepoles.WhenlookingatthosedistortedareasintheODI,itbecomesveryhardtoidentifywhattheyaresupposedtobedepicting.Fig.2givesanexampleofthedistortionsobservedonanequirectangularODI.FromFig.(2a)alone,itisnotpossibletorecognisetheobjectonthetable.OncethenadirviewisundistortedasseeninFig.(2b),itisclearthattheobjectinquestionisacake.Inordertoreducetheeﬀectofthesedistortionsonsaliencyestimation,wedividetheODIintoequally-sizedpatchesbyrenderingviewingfrustumswithaﬁeldofviewof(FOV)approximately90degreeseach.Thisﬁeldofviewwasselectedtokeepdistortionslow,covertheentirespherewithsixpatchesand6xyzθϕ(a)SphericalCoordinatesxyz(b)SlidingfrustumFigure3:Sphericalcoordinatesdeﬁnitionandslidingfrustumusedtocreatethepatches.haveasimilarFOVtothatoftheOculusRift,whichwasusedtocreatethedataset(approximately100degrees).Byspecifyingtheﬁeldofviewperpatchanditsresolution,itispossibletocalculatethesphericalcoordinatesofeachpixelinthepatch.ThesearethenusedtoﬁndthecorrespondingpixelsintheODIbyapplyingthefollowingequations:x=sw(cid:16)θ+π22π(cid:17)(1)y=sh(cid:16)1−φ+π2π(cid:17)(2)Whereθandφarethesphericalcoordinatesofeachpixel,seeFig.3a.ThevariablesswandshcorrespondtotheODI’swidthandheightrespectively.AgraphicalrepresentationofhowpatcheslooklikewhensamplingthespherecanbeseeninFig.3b.Theprocessofgeneratingpatchesisalsoappliedduringtraining,whichwillbediscussedinSection4.2.Duringthesaliencymapcomputationsixpatcheswithﬁxedviewsaregenerated.Twooftheseviewsareorientedtowardsthenadirandzenith,theotherfourarepointedtowardsthehorizonbutrotated7Nadir PatchZenith PatchPatch 1Patch 1Patch 2Patch 4Patch 3Figure4:PatchesextractedfromtheODI.horizontallytocovertheentirebandatthesphere’sequator.Fig.4illustratessuchpartitionswhenappliedtoanequirectangularODI.Thoughonecouldthinkoftheseviewsasacubemap,ourdeﬁnitionismoregenericinthesensethattheFOVcanbeadjustedtobeclosertothatofthedeviceusedtovisualisetheODI,orincreasethenumberofpatchestofurtherdecreasethedistortions.3.2.Post-processingAspreviouslymentioned,theCNNtakesthesixpatchesandtheirsphericalcoordinatesasinputs.Asresult,theCNNgeneratesasaliencymapforeachofthesepatches.Consequently,theyhavetobecombinedtoasinglesaliencymapasoutput.Forthat,weprojecteachpixelofeachpatchtotheequirectangularODI,usingtheirper-pixelsphericalcoordinates.Weapplyforward-projectionwithnearest-neighbourinterpolationforsimplicity.Inordertoﬁllholesandsmooththeresult,weapplyaGaussianﬁlterwithakernelsizeof64pixels.Thiskernelsizewasselectedbecauseitwasfoundtogivethebestresultsintermsofcorrelationwiththegroundtruth.Suchprocessingiseﬃcientandsuﬃcient,aswedonotcomputeoutputimagesforviewing,butestimatesaliencymaps.8Figure5:NetworkArchitecture.4.SalNet360SinceODIsdepictanentire360-degreeview,theirsizetendtobeconsider-ablylarge.Thisfactor,togetherwiththecurrenthardwarelimitations,prohibitsusingthemdirectlyasinputsinaCNNwithoutheavilydown-scalingtheODI.DeepCNNsneedlargeamountsofdatatoavoidover-ﬁtting.Sincetrainingdataconsistedofonly40images,theresultingamountofdatawouldnotbeenoughforproperlytrainingaCNN.Toaddressthisproblemwegeneratedonehundredpatches(pairedcolourimageandgroundtruthsaliencymap)perODIbyplacingtheviewingfrustumtorandomlocations.ThesaliencymapsperpatchareusedaslabelsintheCNN.Fig.5illustratesthearchitectureoftheproposednetwork.Ournetworkconsistsoftwoparts,theBaseCNNandthereﬁnementarchi-tecture.TheBaseCNNistrainedtodetectsaliencymapsfortraditional2Dimages.Ithasbeenpre-trainedusingtheSALICONdataset[29].ThesecondpartisareﬁnementarchitecturethatisaddedaftertheBaseCNN.Ittakesa3-channelfeaturemapasinput:theoutputsaliencymapoftheBaseCNNandthesphericalcoordinatesperpixelastwochannels.Thiscombinationof9Table1:Networkparameters.LayerInputdepthOutputdepthKernelsizeStridePaddingActivationconv13967×713ReLUpool1333×320-conv2962565×512ReLUpool22562563×320-conv32565123×311ReLUconv45122565×512ReLUconv52561287×713ReLUconv61283211×1115ReLUconv732113×1316ReLUdeconv1118×842-merge------conv83325×512ReLUpool332323×320-conv932643×312ReLUconv1064325×512ReLUconv113217×713ReLUdeconv2114×421-theBaseCNNandtheSaliencyReﬁnementhasbeentrainedaswillbedis-cussedinSection4.2.Omnidirectionalsaliencymapsdiﬀerfromtraditional2Dsaliencymapsinthattheyareaﬀectedbythevieworpositionoftheusershead.CombiningthesphericalcoordinatesofeachpixelasextrainputprovidesthenetworkinthesecondstagetheinformationtohighlightorlowerthealreadycomputedsalientregionsdependingontheirplacementintheODI.Wedelveintothedetailsofnetworkarchitectureandtraininginthenextsubsections.4.1.NetworkArchitectureThearchitectureofourBaseCNNwasinspiredbythedeepnetworkintro-ducedbyPanet.al.[17]andsharesthesamelayoutastheVGGCNNMarchitecturefrom[30]intheﬁrstthreelayers.ThisallowedustoinitialisetheweightsoftheselayerswiththeweightsthathavebeenlearnedontheIma-geNetclassiﬁcationtask.Aftertheﬁrstthreelayers,fourmoreconvolutionlayersfollow,eachofthemfollowedbyaRectiﬁedLinearUnit(ReLU)activa-tionfunction.Twomax-poolinglayersaftertheﬁrstandsecondconvolutionlayersreducethesizeofthefeaturemapsforsubsequentconvolutionsandaddsometranslationinvariance.Toupscaletheﬁnalfeaturemaptoasaliencymapwiththesamedimensionsastheinputimage,adeconvolutionlayerisaddedat10theend.AswillbediscussedinSection4.2,wetrainourwholenetworkintwostages.IntheﬁrststageonlythisBaseCNNistrainedontraditional2Dim-ageswiththehelpofanEuclideanLossfunctiondirectlyafterthedeconvolutionlayer.Inourﬁnalnetwork,however,thislossfunctionismovedtotheendofourreﬁnementarchitectureandtheoutputofthedeconvolutionlayerismergedwiththe2-channelper-pixelsphericalcoordinatesasinputtothesecondpartofourarchitecture,theSaliencyReﬁnement.Ashasbeenstatedabove,ourarchitecturedoesnotusetheentireODIatthesametime.Instead,theODIissplitintopatchesandforeachofthesepatchesthesaliencymapiscalculatedindividually.Theideaofthesecondpartofourarchitecture,theSaliencyReﬁnement,istotakethesaliencymapgeneratedfromtheBaseCNNandreﬁneitwiththeinformationonwhereintheODIthispatchislocated.Forexample,thisallowsthemodeltoinferthestrongbiastowardsthehorizonthatthegroundtruthsaliencymapsforODIsshow.ThewholeSaliencyReﬁnementstageconsistsoffourconvolutionlayers,onemax-poolinglayeraftertheﬁrstconvolutionandonedeconvolutionlayerattheend.AllactivationfunctionsareReLUsandthelossfunctionattheendisEuclideanLoss.OurfullarchitectureisvisualisedinFig.5andthehyperparametersforalllayersarepresentedinTable1.Weexperimentedwithdiﬀerentactivationlayersanddiﬀerentlossfunctionsinthenetwork,however,wefoundthatusingReLUsandEuclideanLossyieldedthebestmetricsonthedataset.4.2.TrainingWeperformedtrainingintwostages.Intheﬁrststageweonlytraintheﬁrstpartofthenetwork,theBaseCNN.Theﬁrstthreelayersareinitialisedusingpre-trainedweightsfromVGGCNNMnetworkfrom[30].WethenuseSALICONdata[29]totraintheﬁrstpartofthenetwork.Thedata(bothimagesandtheirsaliencies)isnormalisedbyremovingthemeanofpixelintensityvaluesandre-scalingthedatatoa[-1,1]interval.Wealsoscalealltheimagesandsaliencymapsto360x240resolutionandsplitthedatasetintotwosetsof800011imagesfortrainingand2000imagesfortesting.Thenetworkisthentrainedfor20,000iterationsusingthestochasticgradientdescentmethodandwithbatchsizeoffour.ForthesecondstageofthetrainingweaddtheSaliencyReﬁnementparttothenetworkandusethedatasetwecreatedwithODIsfrom[3].AsdescribedinSection3.1,100patchesarerandomlysampledperODItogenerateadatasetof4000images,theirgroundtruthsaliencyandsphericalcoordinates.ThisdataaugmentationstrategyallowsustotrainthenetworkeventhoughweonlyhadafewODIsavailable.Wepre-processalltheimagesusingthesametechniquesweusedfortheﬁrststageoftraining.TheweightsfortheBaseCNNareinitialisedusingtheweightsweobtainedfromtheﬁrststagetraining.Westartedwithabaselearningrateof1.3e-7andreduceditby0.7afterevery500iterations.Thenetworkistrainedfor22,000iterationswitha10%splitfortestdataandtherestoftheimagesareusedfortraining.Thebatchsizeissettobeﬁvewhilethetesterrorismonitoredevery100iterationstolookfordivergence.Toavoidoverﬁttingweadditionallyusestandardweightdecayasregularization.Wepresentourexperimentsandresultsinthenextsection.5.ResultsInthissectionwedescribetheexperimentsandresultsthatindicatethatourmethodenhancesaCNNtrainedwithtraditional2DimagesandallowsittobeappliedtoODIs.Beforewediscusstheactualnumbers,weprovideashortintroductiontoeachofthemetricsusedtoevaluatethegeneratedsaliencymaps.5.1.PerformancemeasuresBylinskiietal.[31]diﬀerentiatebetweentwotypesofperformancemetricsbasedontheirrequiredgroundtruthformat.Location-basedmetricsneedagroundtruthwithvaluesatdiscreteﬁxationlocations,whiledistribution-basedmetricsconsiderthegroundtruthandsaliencymapsascontinuousdistributions.12Fourmetricswereusedtoevaluatetheresultsofoursystem:theKullback-Leiblerdivergence(KL),thePearson’sCorrelationCoeﬃcient(CC),theNor-malizedScanpathSaliency(NSS)andtheAreaunderROCcurve(AUC).BoththeKLandtheCCaredistribution-basedmetrics,whereastheNSSandAUCarelocation-based.Inthecaseoftheformertwometrics,thepredictedsaliencymapofourapproachisnormalisedtogenerateavalidprobabilitydistribution.Allthesemetricsweredesignedwithtraditional2Dimagesinmind.Guti´errezetal.[32]developedatoolboxthatisspeciﬁcallytailoredtoanalyseODIs.Theresultswepresentwereobtainedusingthesenewtools.WerefertoGuti´errezetal.workforfurtherdetails.5.2.ExperimentsInordertotesttheimprovementsobtainedfromourproposedmethod,wedeﬁnedthreescenariostobecompared.TheﬁrstoneconsistsofapplyingourBaseCNN,seeFig.5whiletakingtheentireODIasinputbydownscalingittoaresolutionof800×400pixels.Thepredictedsaliencymapisthenupscaledtotheresolutionoftheoriginalimage.Inoursecondscenario,wedividetheODIinsixundistortedpatchesasdescribedinSection3.1andrunthesepatchesseparatelythroughtheBaseCNN.Afterwardswerecombinethepredictedsalienciesofthesepatchestoformtheﬁnalsaliencymap.Finally,thethirdscenarioconsistsofthewholepipelineasdescribedinSection3,wherethesphericalcoordinatesofthepatchesarealsoconsideredbytheCNNduringinference.TheresultsofourexperimentscanbeseeninTable2,whereweshowtheaverageKL,CC,NSSandAUCforthe25testimages.Ascanbeseenfromthetable,onlyrunningtheentireODIthroughtheBaseCNNgivesrelativelypoorresultscomparedtothelaterscenarios.BydividingtheODIintopatchesandusingthemindividuallybeforebeingrecombined,ourresultsareimprovedinmostofthemetrics.Afteraddinginthesphericalcoordinatestotheinferenceofeachpatch,theresultsclearlyimproveconsiderablyinallthemetrics.Avisualexampleoftheresultsofthethreescenarioscanbeseeninﬁgure6.Inthetoprowfromlefttoright,theinputODIandablendedversionof13Table2:Comparisonofthethreeexperimentalscenarios.∗indicatesasigniﬁcantimprovementinperformancecomparedtotheBaseCNN(t-test,p<0.01).KLCCNSSAUCBaseCNN1.5970.4160.6300.648Above+Patches0.6250.4740.5660.659Above+SphericalCoords.0.487*0.536*0.757*0.702*Figure6:Comparisonofthethreeexperimentalscenarios.Toprow:OnthelefttheinputODI,ontherightthegroundtruthsaliencymapblendedwiththeimage.Bottomrow:Fromlefttoright,theresultofthethreeexperimentalscenarios:BaseCNN,BaseCNN+Patches,BaseCNN+Patches+SphericalCoords.theODIandgroundtruthareshown.InthebottomrowagainfromlefttorightthepredictedsaliencymapsblendedintotheinputODIforeachscenarioarepresented.Theﬁrstscenarioonthefarleft(BaseCNNonly)predictedtwobigsalientcentres,whicharenotfoundinthegroundtruthmap.Thesecondscenarioimprovedonthesepredictionsbyﬁndingsalientareasthatalsocoversalientareasinthegroundtruth.However,itstillpredictstwohighlysalientareasinwronglocationsandintroducedsomesalientareasoflowimpactatthebottompartoftheimagethatareincorrectlylabelledassalient.Finally,thethirdscenarioclearlycoversallthesalientareasinthegroundtruthandremovestheincorrectlylabelledsalientregionsatthebottomoftheimagethatthesecondscenariointroduced.Itis,however,toogenerousintheamountofareathatthesalientregionscover,whichseemstobethemainissueinthisscenario.InFig.8,someofthebest-performingresultsareshown.Asacompari-1400.10.20.30.40.50.60.70.80.9181619202648506065697172737478798591939495969798KLScoreAverage00.10.20.30.40.50.60.70.80.9181619202648506065697172737478798591939495969798CCScoreAverage00.20.40.60.811.21.4181619202648506065697172737478798591939495969798NSSScoreAverage00.10.20.30.40.50.60.70.80.9181619202648506065697172737478798591939495969798AUCScoreAverageFigure7:PlotsforeachofthemetricsappliedtothetestODIs.son,twoofthelowest-performingresultsareshowninFig.9.AsummaryoftheresultsofallimagescanbeseeninAppendixA.Fig.7providesavisualrepresentationofthevaluesobtainedforeachmetric.5.3.Salient360!GrandChallengeAsmentionedinSection1,achallengewasorganisedduringtheICME2017Conference,inwhichparticipantswereprovidedwithtrainingdataconsistingof40ODIspairedwithhead-andeye-trackinggroundtruthdata.Participantswereallowedtosubmittheirworkonthreediﬀerentcategories:head,head+eyeandscanpath.Thegoaloftheﬁrstcategorybeingtheestimationofasaliencymapconsideringonlytheorientationofthehead;theobjectiveofthesecondcategorycorrespondedtotheestimationofasimilarsaliencymapbutaddingeye-trackinginformation,leadingtomorelocalisedpredictions;ﬁnallythethirdonehadthegoalofestimatingacollectionofscanningpathsthatwouldbecomparedtothescanningpathscollectedbytheorganisers.Theworkherepresentedwassubmittedandparticipatedinthesecondcategory:head+eye.Atotalof16individualsubmissionswereevaluatedinthehead+eyecategory.Table3showstheresultsforthetop-5performersinthechallengefromthesame15(a)Imageindex:69,KL:0.433,CC:0.686,NSS:1.064,AUC:0.769(b)Imageindex:79,KL:0.360,CC:0.818,NSS:1.303,AUC:0.779Figure8:Twoofthebest-performingexamples.Toprow:Fromlefttoright:Inputimage,sixextractedpatches,groundtruthblendedwithinputimage.Bottomrow:Fromlefttoright:Predictedsaliencymap,predictedsaliencymapsfromeachpatch,groundtruthsaliencymap.institution.WedemonstratedthataugmentingtheCNN’sinputwiththecorrespondingsphericalcoordinatesandusingundistortedpatchesoftheODIleadstobettersaliencypredictions.Thisapproachcouldeasilybeappliedtomoreaccuratepredictorstrainedfortraditional2Dimages.5.4.ChallengesAscanbeseenonthepredictedsaliencymapsinFigs.8and9,themergingofthepredictedpatchesleavesadistinctpatternintheﬁnalsaliencymap,leavinginsomecasesalattice-likepattern.Wetriedtomitigatethisissueby16Table3:Top-5performersinthechallengeModelnameKLCCNSSAUCTUMunich[33]0.4490.5790.8050.726SJTU[34]0.4810.5320.9180.735Wuhan[35]0.5080.5380.9360.736ProSal/Zhejihang[36]0.6980.5270.8510.714SalNet360(ours)0.4870.5360.7570.702applyingaGaussianBlur,butitisstillnoticeableinsomeoftheresultsandconsequentlyhasanegativeeﬀectontheKLandCCscores.Inthesaliencypredictionoftheindividualpatchesinbothﬁgures,anothersetofartefactscanalsobeobserved.Wespeculatethattheselinepatternsstemfromtheadditionofthesphericalcoordinates,becausethesepredictionscorrespondtothepatchesthatshowthepoles.However,whenrecombiningthepatchesandapplyingthepost-processingstepsdiscussedabove,theseartefactsarenolongernoticeable.Weexpectthatincreasingtheamountoftrainingdatacouldhelpalleviatesomeoftheissues.6.ConclusionsWeshowedinSection5,thatdividinganomnidirectionalimageintopatchesandaddingaSaliencyReﬁnementarchitecturethattakesintoconsiderationsphericalcoordinatestoanexistingBaseCNNcanconsiderablyimprovetheresultsinomnidirectionalsaliencyprediction.Weenvisionseveralpotentialimprovementsthatcouldbemadetoincreaseperformance.OurnetworkwastrainedusingtheEuclideanLossfunction,whichisarelativelysimplelossfunc-tionthatisapplicabletoawidevarietyofcases,suchassaliencyprediction.Ittriestominimisethepixel-wisediﬀerencebetweenthecalculatedsaliencymapandthegroundtruth.However,tooptimisebasedonanyoftheperformancemetricsmentionedin5.1,customlossfunctionscouldbeused.Inthisway,thenetworkwouldbetrainedtospeciﬁcallyminimiseinregardstotheKL,CCorNSS.AsmentionedinSection5.4,oneofthebiggerissuesthataﬀectourresultsaretheartefactsthatarecreatedwhenrecombiningthepatchestocreatethe17(a)Imageindex:72,KL:0.459,CC:0.271,NSS:0.353,AUC:0.609(b)Imageindex:98,KL:0.652,CC:0.309,NSS:0.126,AUC:0.529Figure9:Twoofthelowest-performingexamples.Toprow:Fromlefttoright:Inputimage,sixextractedpatches,groundtruthblendedwithinputimage.Bottomrow:Fromlefttoright:Predictedsaliencymap,predictedsaliencymapsfromeachpatch,groundtruthsaliencymap.ﬁnalsaliencymap.InsteadofjustusingGaussianBlurtoremovetheseartefacts,amoresophisticatedprocesscouldbeimplementedsincetheseartefactsalwaysappearinthesamewayandlocation.InmostDeepLearningapplications,improvementsareoftenmadebymak-ingthenetworksdeeper.OurnetworkisbasedontheDeepConvolutionalNetworkbyPanet.al.[17],butcomparedtothecurrentstate-of-the-artinotherComputerVisiontaskse.g.classiﬁcationandsegmentation,thisnetworkisstillrelativelysimple.WeareconﬁdentthatupdatingtheBaseCNNtorecentadvanceswillimprovetheresultsinomnidirectionalsaliency.Finally,aswithallDeepLearningtasks,alargeamountofdataisneeded18toachievegoodresults.Unfortunatelythereiscurrentlyonlyarelativelysmallamountofgroundtruthsaliencymapsavailable,andevenlessdataforomni-directionalsaliency.Itisourhopethatmoredatawillbecomeavailableinthefuturewhichcanbeusedtogetbetterresults.Inthiswork,wepresentanend-to-endCNNthatisspeciﬁcallytailoredforestimatingsaliencymapsforODIs.WeprovideevidencewhichindicatesthatpartofthediscrepanciesfoundwhenusingCNNstrainedontraditional2Dim-agesisduetotheheavydistortionsfoundontheprojectedODIs.TheseissuescanbeaddressedbydividingtheODIinundistortedpatchesbeforecalculat-ingthesaliencymap.Furthermore,inadditiontothepatches,biasesduetothelocationofobjectsonthespherecanbeconsideredbyusingthesphericalcoordinatesofthepixelsinthepatchesbeforecomputingtheﬁnalsaliencymap.AcknowledgementThepresentworkwassupportedbytheScienceFoundationIrelandundertheProjectID:15/RP/2776andwiththetitleV-SENSE:ExtendingVisualSensationthroughImage-BasedVisualComputing.19References[1]M.Yu,H.Lakshman,B.Girod,Aframeworktoevaluateomnidirectionalvideocodingschemes,in:IEEEInt.SymposiumonMixedandAugmentedReality,2015.[2]Y.Rai,P.L.Callet,G.Cheung,Quantifyingtherelationbetweenper-ceivedinterestandvisualsalienceduringfreeviewingusingtrellisbasedoptimization,in:2016IEEE12thImage,Video,andMultidimensionalSig-nalProcessingWorkshop(IVMSP),2016,pp.1–5.doi:10.1109/IVMSPW.2016.7528228.[3]Y.Rai,J.Gutirrez,P.LeCallet,Adatasetofheadandeyemovementsforomni-directionalimages,in:Proceedingsofthe8thInternationalConfer-enceonMultimediaSystems,ACM,2017.[4]L.Itti,Automaticfoveationforvideocompressionusinganeurobiologicalmodelofvisualattention,in:IEEETransactionsonImageProcessing,Vol.13,2004.[5]B.C.Ko,J.-Y.Nam,Object-of-interestimagesegmentationbasedonhu-manattentionandsemanticregionclustering,in:JournalofOpticalSoci-etyofAmericaA,Vol.23,2006.[6]L.Itti,C.Koch,Asaliency-basedmechanismforovertandcovertshiftofvisualattention,in:VisionResearch,Vol.40,2000.[7]A.Torralba,A.Oliva,M.Castelhano,J.Henderson,Contextualguidanceofeyemovementsandattentioninreal-worldscenes:Theroleofglobalfeaturesinobjectsearch,in:PsychologicalReview,Vol.113,2006.[8]T.Judd,K.Ehinger,F.Durand,A.Torralba,Learningtopredictwherehumanslook,in:ICCV,2009.[9]A.Krizhevsky,I.Sutskever,G.E.Hinton,Imagenetclassiﬁcationwithdeepconvolutionalneuralnetwork,in:Advancesinneuralinformationprocessingsystems,2012,pp.1097–1105.20[10]R.Girshick,J.Donahue,T.Darrell,J.Malik,Richfeaturehierarchiesforaccurateobjectdetectionandsemanticsegmentation,in:CVPR,2014,pp.580–587.[11]J.Long,E.Shelhamer,T.Darrell,Fullyconvolutionalnetworksforseman-ticsegmentation,in:CVPR,2015,pp.3431–3440.[12]Z.Bylinskii,T.Judd,A.Borji,L.Itti,A.O.F.Durand,A.Torralba,Mitsaliencybenchmark,http://saliency.mit.edu/results_mit300.html.[13]M.K¨ummerer,L.Theis,M.Bethge,Deepgazei:Boostingsaliencypredic-tionwithfeaturemapstrainedonimagenet,in:InternationalConferenceonLearningRepresentations,2015.[14]N.Liu,J.Han,D.Zhang,S.Wen,T.Liu,Predictingeyeﬁxationsusingconvolutionalneuralnetworks,in:CVPR,2015.[15]M.Cornia,L.Baraldi,G.Serra,R.Cucchiara,Adeepmulti-levelnetworkforsaliencyprediction,in:CoRR,2016.[16]S.Kruthiventi,K.Ayush,R.Babu,Deepﬁx:Afullyconvolutionalneu-ralnetworkforpredictinghumaneyeﬁxations,in:IEEETransactionsonImageProcessing,2017.[17]J.Pan,E.Sayrol,X.G.iNieto,K.McGuinness,N.O’Connor,Shallowanddeepconvolutionalnetworksforsaliencyprediction,in:CVPR,2016.[18]A.D.Abreu,C.Ozcinar,A.Smolic,Lookaroundyou:Saliencymapsforomnidirectionalimagesinvrapplications,in:QoMEX,2017.[19]Salient360!:Visualattentionmodelingfor360◦imagesgrandchallenge,http://www.icme2017.org/grand-challenges/,accessed:2017-06-06.[20]M.Cerf,J.Harel,W.Einhauser,C.Koch,Predictinghumangazeusinglow-levelsaliencycombinedwithfacedetection,in:AdvancesinNeuralInformationProcessingSystems,Vol.20,2008.21[21]B.W.Tatler,Thecentralﬁxationbiasinsceneviewing:Selectinganop-timalviewingpositionindependentlyofmotorbiasedandimagefeaturedistribution,in:JournalofVision,Vol.7,2007.[22]I.Bogdanova,A.Bur,H.Hugli,Visualattentiononthesphere,in:IEEETransactionsonImageProcessing,Vol.17,2008.[23]E.Vig,M.Dorr,D.Cox,Large-scaleoptimizationofhierarchicalfeaturesforsaliencypredictioninnaturalimages,in:CVPR,2014.[24]M.K¨ummerer,T.S.Wallis,M.Bethge,Deepgazeii:Readingﬁxa-tionsfromdeepfeaturestrainedonobjectrecognition,in:arXivpreprintarXiv:1610.01563,2016.[25]K.Simonyan,A.Zisserman,Verydeepconvolutionalnetworksforlarge-scaleimagerecognition,in:CoRR,Vol.abs/1409.1556,2014.[26]G.Li,Y.Yu,Visualsaliencydetectionbasedonmultiscaledeepcnnfea-tures,in:IEEETransactionsonImageProcessing,Vol.25,2016.[27]J.Pan,C.Canton,K.McGuinness,N.O’Connor,J.Torres,E.Sayrol,X.G.iNieto,Salgan:Visualsaliencypredictionwithgenerativeadversarialnetworks,in:ArXiv,2017.[28]N.Liu,J.Han,Adeepspatialcontextuallong-termrecurrentconvolutionalnetworkforsaliencydetection,in:arXivpreprintarXiv:1610.01708,2016.[29]M.Jiang,S.Huang,J.Duan,Q.Zhao,Salicon:Saliencyincontext,in:CVPR,2015.[30]K.Chatﬁeld,K.Simonyan,A.Vedaldi,A.Zisserman,Returnofthedevilinthedetails:Delvingdeepintoconvolutionalnets,in:BritishMachineVisionConference,2014.[31]Z.Bylinskii,T.Judd,A.Oliva,A.Torralba,F.Durand,Whatdodiﬀerentevaluationmetricstellusaboutsaliencymodels?,in:ArXiv,2016.22[32]J.Guti´errez,E.David,Y.Rai,P.LeCallet,Toolboxanddatasetforthedevelopmentofsaliencyandscanpathmodelsforomnidirectional/360◦stillimages,SignalProcessing:ImageCommunication.[33]M.Startsev,M.Dorr,360-awaresaliencyestimationwithconventionalim-agesaliencypredictors,SignalProcessing:ImageCommunication.[34]Y.Zhu,G.Zhai,X.Min,Thepredictionofheadandeyemovementfor360degreeimages,SignalProcessing:ImageCommunication.[35]Y.Fang,X.Zhang,Anovelsuperpixel-basedsaliencydetectionmodelfor360-degreeimages,SignalProcessing:ImageCommunication.[36]P.Lebreton,A.Raake,GBVS360,BMS360,ProSal:Extendingexistingsaliencypredictionmodelsfrom2Dtoomnidirectionalimages,SignalPro-cessing:ImageCommunication.23AppendixASummaryofresultsperimageIndividualresultsforeachODI.Thetwobestandworstresultshavebeenhighlightedingreenandredrespectively.ImageindexKLCCNSSAUC10.6610.4760.5760.65880.4920.6571.1050.773160.2570.6130.4570.632190.4250.4560.7310.702200.6110.4850.8060.739260.2990.5730.7170.705480.6560.2390.5780.695500.4860.6091.0780.766600.2730.6430.4520.659650.5350.5290.8030.732690.4070.6871.0850.773710.4390.3470.5810.678720.5390.3140.4030.601730.3790.4820.6090.681740.3910.6700.5240.654780.5330.5761.0430.721790.4110.7951.2660.778850.3520.7190.9620.751910.4810.6580.9490.713930.4930.4840.7860.700940.6140.4640.8340.751950.5260.5580.8640.706960.7650.6901.2300.807970.4860.3720.3560.628980.6500.3060.1420.547Mean0.4870.5360.7570.702Std.Dev.0.1280.1440.2900.06124