Cascaded Mutual Modulation for Visual Reasoning

Yiqun Yao123, Jiaming Xu12∗, Feng Wang1 and Bo Xu1234
1Institute of Automation, Chinese Academy of Sciences (CASIA). Beijing, China
2Research Center for Brain-inspired Intelligence, CASIA
3University of Chinese Academy of Sciences
4Center for Excellence in Brain Science and Intelligence Technology, CAS. China
{yaoyiqun2014,jiaming.xu,feng.wang,xubo}@ia.ac.cn

8
1
0
2

p
e
S
6

]

R

I
.
s
c
[

1
v
3
4
9
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Visual reasoning is a special visual ques-
tion answering problem that is multi-step and
compositional by nature, and also requires
intensive text-vision interactions. We pro-
pose CMM: Cascaded Mutual Modulation as
a novel end-to-end visual reasoning model.
CMM includes a multi-step comprehension
process for both question and image. In each
step, we use a Feature-wise Linear Modula-
tion (FiLM) technique to enable textual/visual
pipeline to mutually control each other. Ex-
periments show that CMM signiﬁcantly out-
performs most related models, and reach state-
of-the-arts on two visual reasoning bench-
marks: CLEVR and NLVR, collected from
both synthetic and natural languages. Ab-
lation studies conﬁrm that both our multi-
step framework and our visual-guided lan-
guage modulation are critical
to the task.
Our code is available at https://github.
com/FlamingHorizon/CMM-VR.

and

Figure 1: Connections and differences between
our
“program-generating” works
previous
model:
other models generate/control multi-step
image-comprehension processes with single question
representation, while we put more attention on lan-
guage logics and let multi-modal information modulate
each other in each step. The question and image are
taken as a visual-reasoning example from CLEVR
dataset.

1

Introduction

It is a challenging task in artiﬁcial intelligence to
perform reasoning with both textual and visual in-
puts. Visual reasoning task is designed for re-
searches in this ﬁeld. It is a special visual ques-
tion answering (VQA) (Antol et al., 2015) prob-
lem, requiring a model to infer the relations be-
tween entities in both image and text, and gen-
erate a textual answer to the question correctly.
Unlike other VQA tasks, questions in visual rea-
soning often contain extensive logical phenomena,
and refer to multiple entities, speciﬁc attributes
and complex relations. Visual reasoning datasets
such as CLEVR (Johnson et al., 2017a) and NLVR
(Suhr et al., 2017) are built on unbiased, synthetic
images, with either complex synthetic questions
or natural-language descriptions, facilitating in-
depth analyses on reasoning ability itself.

* Corresponding Author

Most previous visual reasoning models focus on
using the question to guide the multi-step comput-
ing on visual features (which can be deﬁned as a
image-comprehension “program”). Neural Mod-
ule Networks (NMN) (Andreas et al., 2016a,b)
and Program Generator + Execution Engine
(PG+EE) (Johnson et al., 2017b) learn to com-
bine speciﬁc image-processing modules, guided
by question semantics. Feature-modulating meth-
ods like FiLM (De Vries et al., 2017; Perez et al.,
2018) control image-comprehension process using
modulation-parameters generated from the ques-
tion, allowing models to be trained end-to-end.
However, the image-comprehension program in
visual reasoning tasks can be extremely long and
sophisticated. Using a single question represen-
tation to generate or control the whole image-
comprehension process raises difﬁculties in learn-
ing. Moreover, since information comes from
multiple modalities, it is not intuitive to assume

 
 
 
 
 
 
that one (language) is the “program generator”,
and the other (image) is the “executor”. One way
to avoid making this assumption is to perform
multiple steps of reasoning with each modality
being generator and executor alternately in each
step. For these two reasons, we propose Cascaded
Mutual Modulation (Figure 1), a novel visual rea-
soning model to solve the problem that previous
“program-generating” models lack a method to
use visual features to guide multi-step reasoning
on language logics. CMM reaches state-of-the-
arts on two benchmarks: CLEVR (complex syn-
thetic questions) and NLVR (natural-language).

2 Related Work

Perez et al. (2018) proposed FiLM as an end-
to-end feature-modulating method.
The orig-
inal ResBlock+GRU+FiLM structure uses sin-
gle question representation, and conditions all
image-modulation-parameters on it, without suf-
ﬁciently handling multi-step language logics. In
contrast, we modulate both image and language
features alternately in each step, and condition
the modulation-parameters on the representations
from previous step. We design an image-guided
language attention pipeline and use it in combina-
tion with FiLM in our CMM framework, and sig-
niﬁcantly outperform the original structure.

Other widely-cited works on CLEVR/NVLR
include Stacked Attention Networks (SAN) (Yang
et al., 2016), NMN (Andreas et al., 2016b),
N2NMN (Hu et al., 2017), PG+EE (Johnson et al.,
2017b) and Relation Networks (RN) (Santoro
et al., 2017). The recent CAN model (Hudson
and Manning, 2018) also uses multiple question
representations and has strong performances on
CLEVR. However, these representations are not
modulated by the visual part as in our model.

In other VQA tasks, DAN (Nam et al., 2017)
is the only multi-step dual framework related to
ours. For comparison, in every time step, DAN
computes textual and visual attention in parallel
with the same key-vector, while we perform tex-
tual attention and visual modulation (instead of at-
tention) in a cascaded manner.

3 Model

We review and extend FiLM in Section 3.1-3.2,
and introduce CMM model in Section 3.3-3.4.

3.1 Visual Modulation

Perez et al. (2018) proposed Feature-wise Linear
Modulation (FiLM), an afﬁne transformation on
intermediate outputs of a neural network (v stands
for visual):

F iLM v(Fi,c|γi,c, βi,c) = γi,cFi,c + βi,c,

(1)

where Fi,c is the c-th feature map (C in to-
tal) generated by Convolutional Neural Networks
(CNN) in the i-th image-comprehension step.
Modulation-parameters γi,c and βi,c can be condi-
tioned on any other part of network (in their work
the single question representation q). If the output
tensor Vi of a CNN block is of size C × H × W ,
then Fi,c is a single slice of size 1×H ×W . H and
W are the height and width of each feature map.

Unlike (Perez et al., 2018), in each step i, we
compute a new question vector qi. Modulation-
parameters γi and βi
(C × 1 vectors, γi =
[γi,1, ...,γi,C], etc.) are conditioned on the previ-
ous question vector qi−1 instead of a single q:

γi, βi = M LP i(qi−1).

(2)

MLP stands for fully connected layers with lin-
ear activations. The weights and biases are not
shared among all steps.

3.2 Language Modulation

In each step i, we also apply FiLM to modulate
every language “feature map”. If the full question
representation is a D × T matrix, a question “fea-
ture map” fi,d is deﬁned as a 1 × T vector gather-
ing T features along a single dimension. D is the
hidden-state dimension of language encoder, and
T is a ﬁxed maximum length of word sequences.

F iLM l(fi,d|γi,d, βi,d) = γi,dfi,d + βi,d,

(3)

language.

where l stands for
Concatenated
modulation-parameters γi and βi (D × 1) are con-
ditioned on the visual features Vi computed in the
same step:

γi, βi = gm(Vi),

(4)

where gm (Section 3.4) is an interaction func-
tion that converts 3-d visual features to language-
modulation-parameters. The weights in gm are
shared among all N steps.

Figure 2: Details in CMM step i (middle), with a modulated ResBlock (right) and a modulated textual attention
pipeline (top). Visual and textual features modulate each other in each step to compute new representations.

3.3 Cascaded Mutual Modulation

and compute visual-guided attention weights:

The whole pipeline of our model is built up with
multiple steps. In each step i (N in total), previous
question vector qi−1 and the visual features Vi−1
are taken as input; qi and Vi are computed as out-
put. Preprocessed questions/images are encoded
by language/visual encoders to form q0 and V0.

In each step, we cascade a FiLM-ed ResBlock
with a modulated textual-attention. We feed Vi−1
into the ResBlock modulated by parameters from
qi−1 to compute Vi, and then control the tex-
tual attention process with modulation-parameters
from Vi, to compute the new question vector qi.
(Figure 2, middle).

Each ResBlock contains a 1 × 1 convolution,
a 3 × 3 convolution, a batch-normalization (Ioffe
and Szegedy, 2015) layer before FiLM modula-
tion, followed by a residual connection (He et al.,
2016). (Figure 2, right. We keep the same Res-
Block structure as (Perez et al., 2018)). To be con-
sistent with (Johnson et al., 2017b; Perez et al.,
2018), we concatenate the input visual features
Vi−1 of each ResBlock i with two “coordinate
feature maps” scaled from −1 to 1, to enrich rep-
resentations of spatial relations. All CNNs in our
model use ReLU as activation functions; batch-
normalization is applied before ReLU.

After the ResBlock pipeline, we apply lan-
guage modulation on the full language features
{h1, ..., hT } (D × T ) conditioned on Vi and
rewrite along the time dimension, yielding:

ei,t = F iLM l(ht|gm(Vi)),

(5)

αi,t = softmaxt(Watt

i ei,t + batt

i ),

and weighted summation over time:

qi =

T
(cid:88)

t=1

αi,tht.

(6)

(7)

In equation (6), W att

i ∈ R1×D and batt

i ∈ R1×1
are network weights and bias; ht is the t-th lan-
guage state vector (D × 1), computed using a bi-
directional GRU (Chung et al., 2014) from word
embeddings {w1, ..., wT }. In each step i, the lan-
guage pipeline does not re-compute ht, but re-
modulate it as ei,t instead. (Figure 2, top.)

3.4 Feature Projections

We use a function gp to project the last visual fea-
tures VN into a ﬁnal representation:

uf inal = gp(VN ).

(8)

gp includes a convolution with K 1 × 1 ker-
nels, a batch-normalization afterwards, followed
by global max pooling over all pixels (K = 512).
We also need a module gm (equation (4)) to
compute language-modulations with Vi, since Vi
is 3-d features (not a weighted-summed vector as
in traditional visual-attention). We choose gm to
have the same structure as gp, except that K equals
to the total number of modulation-parameters in
each step. This design is critical (Section 4.3).

We use a fully connected layer with 1024 ReLU
It takes
hidden units as our answer generator.
uf inal as input, and predicts the most probable an-
swer in the answer vocabulary.

Model

Overall Count

Exist

Human
SAN (Yang et al., 2016)
N2NMN (Andreas et al., 2016a)
PG+EE-9K
PG+EE-700K (Johnson et al., 2017b)
RN (Santoro et al., 2017)
COG-model (Yang et al., 2018)
FiLM
FiLM-raw (Perez et al., 2018)
DDRprog (Suarez et al., 2018)
CAN (Hudson and Manning, 2018)
CMM-single (ours)
CMM-ensemble (ours)

92.6
76.7
83.7
88.6
96.9
95.5
96.8
97.7
97.6
98.3
98.9
98.6
99.0

86.7
64.4
68.5
79.7
92.7
90.1
91.7
94.3
94.3
96.5
97.1
96.8
97.6

96.6
82.7
85.7
89.7
97.1
97.8
99.0
99.1
99.3
98.8
99.5
99.2
99.5

Compare
Numbers
86.5
77.4
84.9
79.1
98.7
93.6
95.5
96.8
93.4
98.4
99.1
97.7
98.5

Query
Attribute
95.0
82.6
90.0
92.6
98.1
97.9
98.5
99.1
99.3
99.1
99.5
99.4
99.6

Compare
Attribute
96.0
75.4
88.7
96.0
98.9
97.1
98.8
99.1
99.3
99.0
99.5
99.1
99.4

Table 1: Accuracies on CLEVR test set. N2NMN and PG+EE need extra supervision to train with reinforcement
learning. FiLM-raw uses raw image as input (others use pre-extracted features). Another work (Mascharka et al.,
2018) gets 99.1% accuracy but uses strong program supervision, which is a totally different setting.

4 Experiments

We are the ﬁrst to achieve top results on both
datasets (CLEVR, NLVR) with one structure. See
Appendix for more ablation and visualization re-
sults.

4.1 CLEVR

CLEVR (Johnson et al., 2017a) is a commonly-
used visual
reasoning benchmark containing
700,000 training samples, 150,000 for validation
and test. Questions in CLEVR cover several typ-
ical elements of reasoning: counting, comparing,
querying the memory, etc. Many well-designed
models on VQA have failed on CLEVR, revealing
the difﬁculty to handle the multi-step and compo-
sitional nature of logical questions.

On CLEVR dataset, we embed the question
words into a 200-dim continuous space, and use a
bi-directional GRU with 512 hidden units to gen-
erate 1024-dim question representations. Ques-
tions are padded with NULL token to a maximum
length T = 46. As the ﬁrst-step question vector in
CMM, q0 can be arbitrary RNN hidden state in the
set {h1, ..., hT } (Section 3.3). We choose the one
at the end of the unpadded question.

In each ResBlock, the feature map number C
is set to 128.
Images are pre-processed with
a ResNet101 network pre-trained on ImageNet
(Russakovsky et al., 2015) to extract 1024 × 14 ×
14 visual features (this is also common practice on
CLEVR). We use a trainable one-layer CNN with
128 kernels (3×3) to encode the extracted features
into V0 (128 × 14 × 14). Convolutional paddings
are used to keep the feature map size to be 14 × 14
through the visual pipeline.

We train the model with an ADAM (Kingma
and Ba, 2014) optimizer using a learning rate of
2.5e-4 and a batch-size of 64 for about 90 epoches,
and switch to an SGD with the same learning rate
and 0.9 momentum, ﬁne-tuning for another 20
epoches. SGD generally brings around 0.3 points
gains to CMM on CLEVR.

We achieve 98.6% accuracy with single model
(4-step), signiﬁcantly better than FiLM and other
related work, only slightly lower than CAN, but
CAN needs at least 8 model-blocks for >98% (and
12 for best). We achieve state-of-the-art of 99.0%
with ensemble of 4/5/6 step CMM models. Table
1 shows test accuracies on all types of questions.
The main improvements over program-generating
models come from “Counting” and “Compare
Numbers”, indicating that CMM framework sig-
niﬁcantly enhances language (especially numeric)
reasoning without sophisticated memory design
like CAN.

4.2 NLVR

NLVR (Suhr et al., 2017) is a visual reason-
ing dataset proposed by researchers in NLP ﬁeld.
NLVR has 74,460 samples for training, 5,940 for
validation and 5,934 for public test. In each sam-
ple, there is a human-posed natural language de-
scription on an image with 3 sub-images, and re-
quires a false/true response.

We use different preprocessing methods on
NLVR. Before training, we reshape NLVR images
into 14 × 56 raw pixels and use them directly as
visual inputs V0. For language part, we correct
some obvious typos among the rare words (fre-
quency < 5) in the training set, and pad the sen-
tences to a maximum length of 26. Different from

CLEVR, LSTM works better than GRU on the
real-world questions. For training, we use ADAM
with a learning rate of 3.5e-4 and a batch-size of
128 for about 200 epoches, without SGD ﬁne-
tuning.

Our model (3-step, 69.9%) outperforms all pro-
posed models on both validation and public test
set, showing that CMM is also suitable for real-
world languages (Table 2).

Words
<START>
Is
there
a
big
brown
object
of
the
same
shape
as
the
green
thing
<END>

Block 1
0.017
0.006
0.010
0.006
0.008
0.049
0.149
0.032
0.030
0.062
0.345
0.233
0.000
0.003
0.012
0.006

Block 2
0.030
0.048
0.041
0.031
0.031
0.040
0.265
0.063
0.029
0.032
0.152
0.031
0.005
0.014
0.075
0.036

Block 3
0.052
0.501
0.299
0.015
0.001
0.002
0.003
0.003
0.004
0.002
0.007
0.001
0.004
0.004
0.011
0.025

Visual Attention Map

Model
Text only
Image only
CNN+RNN (Suhr et al., 2017)
NMN (Andreas et al., 2016b)
FiLM (our run)
CNN-BiAtt (Tan and Bansal, 2018)
CMM-3-steps (ours)

Dev
56.6
55.4
56.6
63.1
59.0
66.9
68.0

Test-P
57.2
56.1
58.0
66.1
61.3
69.7
69.9

Table 2: Accuracies on valid and test set of NLVR.

4.3 Ablation Studies

We list CMM ablation results in Table 3. Abla-
tions on CLEVR show that CMM is robust to step
number but sensitive to gm structure because it’s
the key to multi-modal interaction. Section 3.4 is
temporarily a best choice. CMM performs over 7-
point higher than FiLM on NLVR in a setting of
same hyper-parameters and ResBlocks, showing
the importance of handling language logics (see
also difﬁcult CLEVR subtasks in Table 1).

CLEVR

Val
Model
98.4
5-step
98.4
6-step
gm-CNN
93.3
gm w/o BN 94.1
97.0
98.6

gm-NS
4-step

NLVR

Model
FiLM-hyp
1-step
2-step
3-step
4-step

Dev
59.0
65.3
67.7
68.5
67.2
3-step-LSTM 68.0

Test-P
61.3
66.9
66.8
68.4
66.5
69.9

Table 3: Ablation studies on CLEVR/NLVR. gm-CNN
means using 2-layer-CNN with 3 × 3 kernels, followed
by concatenation and MLP, as gm. BN means batch-
normalization in gm. NS means not sharing weights.
“FiLM-hyp” uses all the same hyper-parameters as the
3-step CMM (both use GRU as question encoder).

4.4 A Case Study

We select an image-question pair from the vali-
dation set of CLEVR for visualization. In Table
4, we visualize the multi-step attention weights on
question words, and the distribution of argmax po-
sition in the global max-pooling layer of gp (equiv-

According to NLVR rules, we will run on the unreleased

test set (Test-U) in the near future.

Table 4: Visualization of CMM intermediate outputs
on a sample from CLEVR validation set. We colour
the largest attention weight with dark gray, and top four
attention weights in the rest with light gray.

alent to the last visual “attention map” although
there isn’t explicit visual attention in our image-
comprehension pipeline). On the bottom right is
the original image, and on the top right is the dis-
tribution of argmax positions in the global max-
pooling, multiplied by the original image.

Our model attends to phrases “same shape as”
and “brown object” in the ﬁrst two reasoning
steps. These phrases are meaningful because
“same shape as” is the core logic in the question,
and “brown object” is the key entity to generat-
ing the correct answer. In the last step, the model
attends to the phrase “is there”. This implicitly
classiﬁes the question into question-type “exist”,
and directs the answer generator to answer “no” or
“yes”. The visual map, guided by question-based
modulation parameters, concentrates on the green
and brown object correctly.

The result shows that visual features can guide
the comprehension of question logics with textual
modulation. On the other hand, question-based
modulation parameters enable the ResBlocks to
ﬁlter out irrelative objects.

5 Conclusion

We propose CMM as a novel visual reasoning
model cascading visual and textual modulation
in each step. CMM reaches state-of-the-arts on
visual reasoning benchmarks with both synthetic
and real-world languages.

Acknowledgements

We thank the reviewers for their insightful com-
ments. This work is supported by the National
Natural Science Foundation of China (61602479)
and the Strategic Priority Research Program of the
Chinese Academy of Sciences (XDBS01070000).

References

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016a. Learning to compose neural net-
In Proceedings of
works for question answering.
NAACL-HLT, pages 1545–1554.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016b. Neural module networks.
In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 39–48.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2425–2433.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Harm De Vries, Florian Strub, J´er´emie Mary, Hugo
Larochelle, Olivier Pietquin, and Aaron C Courville.
2017. Modulating early visual processing by lan-
guage. In Advances in Neural Information Process-
ing Systems, pages 6597–6607.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition, pages 770–
778.

Ronghang Hu, Jacob Andreas, Marcus Rohrbach,
Trevor Darrell, and Kate Saenko. 2017. Learning
to reason: End-to-end module networks for visual
question answering. CoRR, abs/1704.05526, 3.

Drew A Hudson and Christopher D Manning. 2018.
Compositional attention networks for machine rea-
In International Conference on Learning
soning.
Representations.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
In International
reducing internal covariate shift.
conference on machine learning, pages 448–456.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017a. Clevr: A diagnostic dataset for
compositional language and elementary visual rea-
In IEEE Conference on Computer Vi-
soning.
sion and Pattern Recognition (CVPR), 2017, pages
1988–1997. IEEE.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
nick, and Ross Girshick. 2017b. Inferring and ex-
ecuting programs for visual reasoning. In Proceed-
ings of the IEEE International Conference on Com-
puter Vision.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

David Mascharka, Philip Tran, Ryan Soklaski, and Ar-
jun Majumdar. 2018. Transparency by design: Clos-
ing the gap between performance and interpretabil-
ity in visual reasoning. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion.

Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim.
2017. Dual attention networks for multimodal rea-
In Proceedings of the IEEE
soning and matching.
Conference on Computer Vision and Pattern Recog-
nition, pages 299–307.

Ethan Perez, Florian Strub, Harm De Vries, Vincent
Dumoulin, and Aaron Courville. 2018. Film: Visual
reasoning with a general conditioning layer. In Pro-
ceedings of the 32nd AAAI Conference on Artiﬁcial
Intelligence.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition chal-
International Journal of Computer Vision,
lenge.
115(3):211–252.

Adam Santoro, David Raposo, David G Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Tim Lillicrap. 2017. A simple neural network
In Advances in
module for relational reasoning.
neural information processing systems, pages 4974–
4983.

Joseph Suarez, Justin Johnson, and Fei-Fei Li. 2018.
Ddrprog: A clevr differentiable dynamic reasoning
programmer. arXiv preprint arXiv:1803.11361.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
2017. A corpus of natural language for visual rea-
In Proceedings of the 55th Annual Meet-
soning.
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 217–223.

Hao Tan and Mohit Bansal. 2018. Object ordering with
bidirectional matchings for visual reasoning. In Pro-
ceedings of NAACL-HLT.

Guangyu Robert Yang,

Igor Ganichev, Xiao-Jing
Wang, Jonathon Shlens, and David Sussillo. 2018.
reason-
A dataset and architecture for visual
arXiv preprint
ing with a working memory.
arXiv:1803.06092.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
for image question answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 21–29.

