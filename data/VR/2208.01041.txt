Voice Analysis for Stress Detection and Application in Virtual Reality to 

Improve Public Speaking in Real-time: A Review  

Arushi 

James Cook University, Singapore 387380, arushi@my.jcu.edu.au 

Roberto Dillon 

James Cook University, Singapore 387380, roberto.dillon@jcu.edu.au 

Ai Ni Teoh 

James Cook University, Singapore 387380, aini.teoh@jcu.edu.au 

Denise Dillon 

James Cook University, Singapore 387380, denise.dillon@jcu.edu.au 

Stress during public speaking is common and adversely affects performance and self-confidence. Extensive research has 

been carried out to develop various models to recognize emotional states. However, minimal research has been conducted 

to detect stress during public speaking in real time using voice analysis. In this context, the current review showed that the 

application  of  algorithms  was  not  properly  explored  and  helped  identify  the  main  obstacles  in  creating  a  suitable  testing 

environment while accounting for current complexities and limitations. In this paper, we present our main idea and propose a 

stress detection computational algorithmic model that could be integrated into a Virtual Reality (VR) application to create an 

intelligent virtual audience for improving public speaking skills. The developed model, when integrated with VR, will be able 

to detect excessive stress in real time by analysing voice features correlated to physiological parameters indicative of stress 

and help users gradually control excessive stress and improve public speaking performance. 

CCS  CONCEPTS  •  Human-centred  computing  →  Human  computer  interaction  (HCI)  →  Virtual  reality  • 
Computing methodologies → Machine learning → Machine learning algorithms → Feature selection → 

Real-time simulation → Artificial intelligence → Intelligent agents  

Additional  Keywords  and  Phrases:  Virtual  reality,  Affect  sensing  and  analysis,  Nonverbal  signals,  Real-time 

feedback system, Voice analysis, Intelligent virtual agents, Signal processing 

1  INTRODUCTION 

Public speaking is considered a vital skill [Schreiber 2011]. It has several benefits to individuals at various levels, 
such as rapport building and networking in professional as well as social contexts. The predominant functional 

state  of  speakers  during  public  speaking  is  the  experience  of stress  [Koroleva  et  al.  2014].  Several  studies 

1 

 
 
  
 
 
 
found that public speaking causes and induces excessive stress in speakers and therefore compromises the 
quality  of  speech  being  delivered  [Beatty  and  Behnke  1991; Dowd  et  al.  2010;  Elfering  and  Grebner  2011; 

Elfering  and  Grebner  2012;  Garcia-Palacios  et  al.  2002;  Georgiades  et  al.  2000;  Gramer  and  Saria  2007; 
Gramer and Sprintschnik 2008; Kotlyar et al. 2008; Pisanski et al. 2016; Westenberg et al. 2009; Wiemers et 

al. 2015; Wirtz et al. 2013]. According to the American Psychological Association (APA), stress is defined as a 
physiological  or  psychological  response  to  any  internal  or  external  stressor  [American  Psychological 

Association 2022]. Stress manifests in every system of the body, for example, through heart palpitations, dry 
mouth, shortness of breath, accelerated speech, and changes in breathing patterns leading to changes in voice 

[American  Psychological  Association  2022].  Although  a  moderate  level  of  stress  is  beneficial  for  optimal 
performance, high levels of stress lead to fatigue and reduced performance [Dongrui et al. 2010]. Stress during 

public speaking not only impacts speaking ability in large social gatherings, but it also affects daily interpersonal 
communication skills. Ultimately, it impacts opportunities for learning and growth.  

Despite  public  speaking  commonly  triggering  stress  among  individuals,  it  has  received  relatively  little 

attention in the fields of affective computing and human-computer interaction (HCI). This is possibly due to the 
limitations  and  complexities  of  creating  a  suitable  testing  environment.  Specifically,  little  attention  has  been 

given to the real-time identification of stress during public speaking. Automatic detection of stress from audio 
and voice is one of the most challenging tasks because emotions (such as anxiety) and stress are basically 

indistinguishable in terms of reaction patterns and response to an event [American Psychological Association 
2022]. 

In  the  past,  researchers  have  tested  various  classification  algorithms  to  analyse  human  voice  for  the 

identification of six basic, distinct categorical emotions, such as happiness, anger, sadness, surprise, disgust, 
and fear, and other dimensional emotions, such as anxiety, boredom, calmness,  and sarcasm [Bitouk et al. 

2010; Deng et al. 2013; Kwon et al. 2003; Lin and Wei 2005; Poorna et al. 2018; Soltani and Ainon 2007; Zhou 
et al. 2001]. Emotions are fundamental in human nature and impact both verbal and non-verbal vocalisations. 

Therefore, emotion classification algorithms can be developed by creating emotionally significant events which 
induce physiological, psychological, and complex behavioural and reactive patterns [American Psychological 

Association 2022]. The algorithms discern emotions using voice analysis and extracting specific features that 
are indicative of affective states. For instance, some studies [Ang et al. 2002; Liscombe et al. 2005; Lugger and 

Yang 2007] utilised prosodic features of voice which are essentially non-verbal patterns of tune in speech, such 
as speaking rate, pitch, energy and pause duration to help detect the speaker’s state beyond the literal spoken 

words.  Other studies  [Rong  et  al.  2009;  Sanchez et  al.  2010;  Schuller  et  al.  2007;  Wang  et  al.  2015]  used 
spectral features which are obtained from the time domain signals (temporal features) through the application 

of Fourier transforms to describe the signal in the frequency domain. 

The  process  of  feature  extraction  (e.g.,  prosodic  or  spectral)  for  detection/identification  of  specific 

emotional/affective  state  by  voice  analysis  can  be  a  difficult  task  considering  both  the  event  and  the 
environment. Specifically, identifying features to detect stress in real-time during the event of public speaking 

is a complex and challenging problem to be solved, as the associated physiological changes, such as increased 

2 

 
 
 
 
heart rate and blood pressure, are not exclusively indicative of stress experience [Clinton et al. 2017; Kappas 
et al. 1991; Westenberg, Bokhorst, Miers, Sumter, Kallen, van Pelt and Blöte 2009] Therefore, stress detection 

should include changes in voice and breathing patterns [Droppleman and McNair 1971]. Stress during public 
speaking can be reduced by participating in training programs as well as receiving social support [Fredrick et 

al.  2018; Teoh  and Hilmert  2018; Thorsteinsson and  James  1999; Uchino  and  Garvey  1997]. Furthermore, 
stress can be detected by organising speaking tasks in laboratory environments, for example, by using recent 

exponential  technologies like  Virtual Reality  (VR). Application of the latest technologies like VR may  help in 
overcoming limitations and complexities involved in creating an environment to simulate public speaking. Being 

a human-computer interface designed to immerse users in a simulated environment [Azwar et al. 2016], VR 
provides a safe space for HCI for various purposes, such as training [Fruchter et al. 2007] and applications in 

different industries. Since VR has the capability to engineer stress responses to create a more confident public 
speaker in real life situations, it could therefore be utilised as a training tool to help reduce excessive stress of 

public speaking in real-time.  

This  literature  review  aims  to  give  readers  a  general  overview  of  the  various  algorithms  used  to  identify 

different affective states by analysis of various voice features. In this paper, our key contribution is as follows: 
•  We emphasise and present how a computational algorithmic model that specifically detects stress 

in real-time from voice analysis could be developed for improving public speaking. Its innovative 
features will include analysis of prosodic features such as intensity, frequency, the pitch of voice, 

and  spectral  features  which  are  identified  based  on  correlation  established  with  physiological 
symptoms of stress such as heart rate, blood pressure and psychological responses obtained from 

questionnaires (Section 3). 

•  Next,  we  discuss  how  to  implement  (theoretically)  a  real-time  stress  detection  model  in  VR  for 

additional  user  immersion  and  engagement.  Thereby,  overcoming  the  limitation  of  testing 
environment  for  improving  public  speaking  skills  by  effectively  and  efficiently  reducing  the  gap 

between affective computing and human-computer interaction fields (Section 4). 

•  Additionally, virtual audience in VR can be trained using reinforcement learning techniques or other 

ways to detect stress  and  provide  feedback in real time. The final developed VR application will 
include  a  virtual  audience  enabled  to  provide  virtual  social  support  in  two  different  modes: 

supportive feedback mode where only positive feedback will be provided regardless of performance 
(measured in terms of detected stress levels), or realistic positive or negative feedback based on 

actual  performance  (Section  5).  This  will  allow  the  speaker  to  practice  techniques  designed  to 
reduce stress during public speaking first in a VR environment and then progressing on to refining 

public-speaking in real-life. 

2  ARTICLE SEARCH AND SCREENING PROCESS  

Numerous  studies  have  examined  the  relationship  between  the  voice  and  emotions.  This  review  primarily 

focused on emotion detection from voice and VR for public speaking. Therefore, peer-reviewed articles and 

3 

 
 
 
 
book chapters that refer to emotion/affective state detection, public speaking and voice analysis were explored 
by using ScienceDirect, Web of Science and IEEE Xplore databases. Only English-language publications were 

selected.  Several  different  searches  were  conducted  for  the  different  topics  using  various  keywords  and 
combinations of keywords.  

We  conducted  the  first  round  of  literature  search  (Search  1)  to  understand  and  identify  the  relationship 

between emotional/affective state and its effect on voice. The search included keywords voice and  emotion, 
public speaking, and virtual reality. The setting of the timeframe for 50 years yielded important earlier research 

papers  such  as  by  Scherer  [Scherer  1995],  Wallbott  &  Scherer  [Wallbott  and  Scherer  1986],  and  Kappas 
[Kappas, Hess and Scherer 1991], which were helpful in the basic understanding of the relationship between 

voice  and  emotional/affective state  in  humans.  Similarly, some  of  the  research  conducted  by  Slater  and  his 
colleagues [Slater et al. 2006; Slater et al. 1999] helped elaborate the earlier research methods and experiments 

related to public speaking in virtual reality. Search 1 resulted in a total of 686 articles. These articles provided 
us with the necessary introduction to public speaking, voice, emotions, virtual social support, and virtual reality. 

Before we made our Search 2, we were fairly confirmed in finding that little research has been conducted to 

detect stress  in  real-time  by  voice  analysis.  Therefore,  our  Search  2 focused  on speech  emotional/affective 
state recognition models and included keywords audio and voice analysis, speech emotion detection and voice 

stress detection. A timeframe of 20 years  (i.e., 2000  – 2020) was set for Search 2 to understand the recent 
research related to affective state detection from voice. This was done mainly to get an overview of application 

and improvements in emotional state recognition models in the past 20 years. This provided more insights into 
changes that came in by using machine learning technology for emotion detection. Search 2 resulted in 438 

articles. 

Items from both searches were screened by using EndNote software [The EndNote Team 2013] was used 
to manage the references. After a screening of the total 1,124 abstracts and conclusions, 241 articles were 

selected. Articles which were less than a single page were eliminated, as were commentaries and reflective 
viewpoints. Furthermore, short papers with no data were excluded. Numerous articles which were not relevant 

and  coherent  with  our  research  area  were  also  removed.  Research  articles  without  a  focus  on 
emotional/affective state detection models, voice analysis features and VR for public speaking were excluded. 

Taken together, all search and screening methods resulted in a total of 169 articles. 

3  LITERATURE SURVEY FINDINGS  

3.1  Voice Analysis  

Human voice is capable of conveying emotions through speech  [Banse and Scherer 1996; Darwin and Prodger 
1998;  Scherer  et  al.  1991;  Wallbott  and  Scherer  1986]  and  non-speech  vocalisations  [Cowen  et  al.  2019]. 

Furthermore, analysis of non-speech vocalisations (voice) and speech has received much recognition during 
the past decade to detect  various emotional/affective states [El Ayadi et al. 2011; Schuller et al. 2011]. The 

state detection process from voice involves various algorithms which require training to work correctly.  

4 

 
 
 
 
An  algorithm  is  a  well-defined  set  of  instructions  that  maps  an  input  to  an  output  using  a  function.  For 
instance,  if  a  given  input  is  X  (e.g.,  voice  features)  and  we  want  to  know  if  the  output  is  Y  (e.g.,  specific 

emotion/affective  state),  then  the  function  involves  mapping  the  given  input  X  to  Y  as  f(X)=Y.  Detecting 
emotional/affective state via voice analysis also involves some preliminary steps such as pre-processing, signal 

framing, windowing, detection of voice activity, normalisation, and noise reduction of the data [Hamid 2018]. 
These steps  play an essential role in extracting the  specific  features of voice capable of identifying different 

affective states and training the algorithmic models for classifying and recognising emotions.  

3.2  Voice Features and Emotion Detection  

The communication of emotions in humans takes place beyond the literal spoken words, such as in non-verbal 
patterns of rhythm, timbre, and tone in speech. Overall, many non-lexical patterns can be identified by features 

such as intensity, fundamental frequency (F0) and duration of the sounds. The prosodic features, which are a 
combination of spectral, also identified as frequency-based features and including time-domain features, also 

known  as  temporal  features,  are  of  paramount  importance  for  detecting  affective  states  have  also  been 
discussed in the literature  [Mitchell and Ross 2013].  

Temporal features include intensity, which signifies the energy of the sound waves (signal) travelling through 
the air. It is the representation of a change in the amplitude of speech signals over time. Amplitude here in this 

process is the displacement of the sound waves produced, which determines the loudness of the voice [Kappas, 
Hess and Scherer 1991]. These features are relatively simple to extract and are easier to interpret than spectral 

features.  

Spectral  features  specifically  include  features  associated  with  the  fundamental  frequency  (F0)  such  as 

frequency components, formants, and coefficients. Vibrations of the vocal cords cause F0, which denotes the 
lowest oscillation of sound waves to be produced, and it is related to the pitch of voice. F0 one of the most-used 

parameters for detecting emotions [Scherer 1995]. The pitch comprises characteristics related to rhythm and 
tone of the speech. The variation of the F0 over time yields its other statistical properties and F0 contour, which 

can  also  be  used  as  features  [Kamiloğlu  et  al.  2020].  More  specifically,  formants  indicate  the  location  of 
resonance  occurring  in  any  vocal  tract.  They  are  particularly  described  with  their  specific  frequencies,  their 

amplitude  at  the  peak,  and  their  bandwidth.  The  localisation  of  these  formants’  resonance  on  frequencies 
causes amplification or attenuation. The resonance of the formants regulates articulated sound quality and are 

assessed as the peak in the frequency spectrum of the sound [Koolagudi and Rao 2012]. Spectral features are 
obtained by converting the time-based signals into the frequency domain signals using a Fourier Transform 

[Kappas, Hess and Scherer 1991; Scherer 1995].  

One of the essential spectral features is the Mel Frequency Cepstral Coefficients (MFCC), which represents 

the speech signal’s short term power spectrum. It is obtained by division of the utterances into segments. Then 
a short-time discrete Fourier Transform is applied on each segment to convert it into the frequency domain. 

Next, the Mel filter bank is utilised for the calculation of the number of sub-band energies, which is then followed 
by taking logarithms and finally the application of inverse Fourier Transform obtains MFCC [Kuchibhotla et al. 

2014]. Other spectral features include Linear Prediction Cepstral Coefficients (LPCC), Log-Frequency Power 
Coefficients (LFPC) and Gammatone Frequency Cepstral Coefficients (GFCC) that can be acquired by a Linear 

5 

 
Prediction  Coefficient  (LPC),  by  measurement  of  spectral  band  energies  using  Fast  Fourier  Transform  and 
Gammatone filter-bank respectively [Nwe et al. 2003]. 

Furthermore, an algorithm documented by Kaiser [Kaiser 1993] as  

Ψ [𝑋(𝑛)]=𝑥2 (𝑛)− 𝑥 (𝑛 +1) 𝑥(𝑛 − 1) 

was  also  developed  to  measure  the  energy  from  speech  by  a  non-linear  process,  where  Ψ[]  is  the  Teager 
Energy Operator (TEO), and x( n ) represents the sample of a speech signal. TEO has various features, such 

as  TEO  decomposed  FM  (frequency  modulation),  TEO  decomposed FM variation  (TEO-FM-Var),  and  TEO 
auto-correlation envelope area (TEO-Auto-Env), as well as a critical band based TEO auto-correlation envelope 

area (TEO-CB-Auto-Env). These features explore the change in the characteristics of energy’s airflow of speech 
under stress [Sun and Moore 2011]. These features can be particularly utilised for detection of stress. 

Another set of features called prosodic features are a combination of both temporal and spectral features. 
For instance, intensity, pitch, F0, formants and duration are also categorised as prosodic features. Duration is 

defined as the time to construct vowels and other factors present in speech, such as words. Speech pace, the 
time duration of voiced, unvoiced, and silenced region and longest voiced speech are some of the commonly 

used  duration  related  features.  Furthermore,  some  other  properties  of  a  voice  include  shimmer,  jitter  and 
harmonics to noise ratio (HNR). These features might also help differentiate the emotions produced in a speech 

signal.  While  jitter  represents  the  unevenness  of  F0  between  successive  oscillations,  the  shimmer  is  the 
unevenness  of  the  amplitude.  Furthermore,  the  HNR  is  the  representation  of  the  ratio  between  periodic  to 

aperiodic  components  in  a  voiced  speech  signal  [Li  et  al.  2007].  In  Table  1,  we  summarise  common  voice 
features used in emotion/affective state detection. 

Temporal features  
(Time based features)  

Spectral features  
(Frequency based 
features) 

Mel Frequency Cepstral 
Coefficients (MFCCs) 

TABLE 1. Different types of voice features  

Features pertaining to intensity 

Represents the signal in terms of frequency  

Represents short term power spectrum 

Teager Energy Based 
features  

Represents energy in signal by following a non-linear 
process  

Minimum energy, zero-crossing 
rate, maximum amplitude  
Fundamental frequency (F0), 
formants, pitch, fundamental 
frequency contour  

Linear Prediction Cepstral 
Coefficients (LPCC), Log-
Frequency Power Coefficients 
(LFPC) and Gammatone 
Frequency Cepstral Coefficients 
(GFCC), Linear prediction 
Coefficient (LPC) 

TEO decomposed FM 
(frequency modulation), TEO 
decomposed FM variation 
(TEO-FM-Var), and TEO auto-
correlation envelope area (TEO-
Auto-Env), as well as a critical 
band based TEO auto-

6 

 
 
 
Prosodic Features  

Combination of Temporal and Spectral features  

correlation envelope area (TEO-
CB-Auto-Env) 
Intensity, pitch, F0, formants, 
duration  
Unvoiced and silenced region,  

Some other voice 
features   

Represents unevenness of F0   between successive 
oscillations 

Shimmer  

Represents unevenness in amplitude  

Jitter  

Represents representation of the ratio between 
periodic to aperiodic components in a voiced speech 
signal 

Harmonics to noise ratio (HNR) 

It  has  been  found  that  listeners can  judge five  types  of  emotions  –  anger, fear,  happiness, sadness  and 

sensitivity – with up to 70% accuracy [Juslin and Laukka 2003]. Researchers [Kappas, Hess and Scherer 1991] 
suggested that emotions such as anger, fear, anxiety, happiness or surprise, which are considered to be high 

arousal emotions, result in increased intensity as well as a high mean F0. In comparison, disgust, and sadness, 
which are considered to be low arousal emotions, have the opposite effect. Kamiloğlu et al. [Kamiloğlu, Fischer 

and Sauter 2020] suggest that pitch is high for emotions such as amusement, interest, and relief, but moderate 
for savouring emotions like contentment and pleasure, and low for prosocial emotions such as admiration. Low 

et al. [Low et al. 2010] used prosodic, spectral voice quality as well as TEO-based features for detecting clinical 
depression  in  adolescents.  The  results  showed  that  TEO-based  features,  specifically  TEO  CB  AutoEnv, 

outperformed all other features and all other combinations of features as well. 

Despite a range of voice features, most of the early studies in the last decade were focused on analysing 

the  prosodic  features  including  pitch, formant frequencies  and  intensity  [Busso  et  al.  2009;  Nogueiras  et  al. 
2001; Nwe, Foo and De Silva 2003; Schuller et al. 2003]. Recent literature surveys showed extensive use of 

spectral  features  and  cepstral  measurements  (e.g.,  MFCCs),  voice  quality  features  including  HNR,  jitter,  or 
shimmer, and Teager energy operator such as TEO-FM-Var and TEO-CB-Auto-Env [Bhavan et al. 2019; Hao 

et al. 2020; Semwal et al. 2017]. This might be since various features perform differently in the detection of 
emotional/affective state based on the newly developed algorithms.  

There exist various algorithms for analysing the voice and audio for emotion detection. The most significant 

ones  are the  Gaussian Mixture Model  (GMM),  the  Hidden Markov  Model  (HMM),  Support  Vector Machines 
(SVM), Artificial Neural Networks (ANN), the Deep Neural Network (DNN) and the Convolutional Neural Network 

(CNN) [Hansen and Liu 2016; Julião et al. 2015; Přibil and Přibilová 2013; Schuller, Batliner, Steidl and Seppi 
2011; Zeng et al. 2019]. 

3.3   Models for detecting Affective States from Audio and Voice Features 

In recent years, the Gaussian Mixture Model (GMM) is one of the well-known models for emotion detection 

[Anagnostopoulos et al. 2015; Busso, Lee and Narayanan 2009; Tashev et al. 2017; Truong and van Leeuwen 

7 

  
 
 
 
 
 
 
 
 
2007].  GMMs  are  efficient  in  modelling  multi-modal  distribution  that  best  model  any  input  data,  by  finding 
probabilistic cluster assignments [Douglas-Cowie et al. 2007]. A GMM uses expectation – maximization [E-M] 

approach, where it chooses starting guesses for setting location and shape. It repeats the E-M steps until each 
cluster is connected with a smooth Gaussian model which can be in any form from stretched circular (Figure 1) 

to even oblong stretched out clusters (Figure 2) [VanderPlas 2016]. 

        Figure 1: Circular stretched out clusters using GMM                         Figure 2: Oblong stretched out clusters using GMM 

A  group  of  researchers  developed  and  tested  the  various  GMMs  with  spectral  features  such  as  MFCC, 

MFCC-low, and pitch. Neiberg et al. combined three different models for recognising emotions. The developed 
models  were  tested  on voice  controlled telephonic service  recordings  recorded  by  Swedish company  Voice 

Provider  (VP)  and  on  the  meeting  recordings  from  the  ISL  Meeting  corpus.  The  results  showed  that  the 
combination of developed GMMs based on the spectral features obtained the best results [Neiberg et al. 2006].  

In another study, Schuller et al.[Schuller, Rigoll and Lang 2003] compared two models: GMM and HMM. 

GMM was used to classify the utterances using global statistics of raw pitch and energy contour. On the other 
hand, HMM classified the utterances using low-level statistical features (which represent a stationary state of 

emotions in voice). Each emotion in HMM is modelled by a single state HMM [Casale et al. 2007]. The single 
state HMM is trained by increasing the minimizing distance margin between emotions, and a loss function is 

used to scale the margin. In comparison to GMMs, the HMM is based on the Markov chain model and in its 
basic formulation, the Markov chain model states that the future state of any pattern is decided by the current 

state and not by the past state. 

Markov Assumption:  

P (qi = a|q1...qi−1) = P (qi = a|qi−1) 
Where q1 …qi are the random i states [Martin. 2021]. 

 It  is similar to  predicting  tomorrow’s  weather  based  on today’s weather  only,  without  looking  at the  past 
weather. Furthermore, in HMM, the various states are kept hidden from the observer [Li, Tao, Johnson, Soltis, 

Savage, Leong and Newman 2007]. The hidden states of the HMM model indicate the temporal state of the 
voice  data,  where  the  temporal  state  is  created  because  the  observation  sequence  is  formed  due  to  its 

association with each state randomly. However, the calculation of the optimal number of states is an issue of 
HMM classifier design. The results indicated that the average accuracy in identifying discrete emotions was 

more  than  86%  by  using  global  statistics  on  the  features.  Furthermore,  GMMs  are  considered  to  be  more 

8 

                          
 
 
appropriate  for  emotion  detection  as  GMMs  training  and  testing  requirements  are  less  than  the  continuous 
Hidden Markov Model (HMM). 

Another HMM built model showed 70% recognition rate for identifying six emotions; happiness, anger, joy, 

fear, disgust, sadness; using low-level pitch, energy features of voice, and their contours of voice [Nogueiras, 
Moreno, Bonafonte and Mariño 2001]. Nwe et al. [Nwe, Foo and De Silva 2003] showed that the HMM yields 

better performance by achieving the best recognition rate of 89% by using spectral features MFCC and LPCC.  
Similarly, HMM and Support Vector Machine (SVM) were used to classify five emotions: anger, happiness, 

sadness, surprise, and neutral emotion. The results from the study showed a 99.5% accuracy rate for HMM 
whereas SVM obtained an accuracy rate of only 88.9% in recognising the emotions [Lin and Wei 2005].  

However,  generally,  in  contrast  to  GMM  and  HMM,  the  SVM  models  are  more  recently  used  [Bhavan, 

Chauhan, Hitkul and Shah 2019; Bitouk, Verma and Nenkova 2010; Hao, Cao, Liu, Wu and Xiao 2020; Jain et 
al. 2020; Semwal, Kumar and Narayanan 2017; Truong and van Leeuwen 2007; Wang, An, Li, Zhang and Li 

2015].  SVM seems  to  be  promising  in many  research studies  for the  classification  of stress  or  not stressed 
[Barreto et al. 2007]. It is used to find the best hyperplane for linearly separable patterns of any given data in 

an N-dimensional space (where N is number of features) which distinctly classify the data. Hyperplane here 
refers to the boundaries upon which a decision is made, whereby data falling on either side of the hyperplane 

can be considered with differed classes. Some of the possible hyperplanes for given data points are shown in 
Figure 3 [Gandhi 2018]. 

Figure 3: Possible hyperplanes 

In case it is not possible to separate the patterns linearly, a kernel function is utilised to map the original 

data to a new space. However, since there is no particular defined way of choosing the kernel, the separation 
of transferred features is not guaranteed. Despite this, SVM also offers some specific advantages over the 

HMM and GMM such as global optimality of training algorithm and memory efficiency, since it uses a subset 
of training points in the decision-making function [Scikit Learn 2022].  

9 

 
 
 
 
 
Similarly,  in  the  study  conducted  by  Hu  et  al.  [Hu  et  al.  2007]  GMM  supervector  based  on  SVM  for 

classification of emotions using spectral features reported an accuracy rate of 82.5%, while an accuracy rate of 
77.9% was obtained by only using GMM.  

Bhavan and colleagues [Bhavan, Chauhan, Hitkul and Shah 2019] recognised emotions such as happiness, 

fear, sadness, anger, disgust, boredom, calm, surprise, and sarcasm on three databases: the Berlin EmoDB 
[Burkhardt et al. 2005], the Indian Institute of Technology Kharagpur Simulated Emotion Hindi Speech Corpus 

(IITKGP-SEHSC) [Koolagudi et al. 2009], and the Ryerson Audio-Visual Database of Emotional Speech and 
Song  (RAVDESS)  [Livingstone  and  Russo  2018].  A  combination  of  spectral features  was  extracted,  further 

processed,  and  reduced  to  have  the  required  features  set  including  MFCC,  spectral  centroids,  and  MFCC 
derivatives. The group of researchers then further utilised a bagged ensemble model consisting of SVM with a 

Gaussian kernel. The results showed an improved accuracy rate of 92.45% (EmoDB), 75.69% (RAVDESS), 
and 84.11% (IITKGP-SHSC) respectively. Similarly, other researchers [Truong and van Leeuwen 2007] also 

used SVM for emotion recognition and found that SVM performs better with  acoustic and linguistic features 
ultimately increasing the recognition of emotions by 8%.  

Recently, several new computational models are reported in the literature. This includes the Neural Network 

for the classification of emotions/affective states from audio [Palo et al. 2015; Soltani and Ainon 2007]. Neural 
Network or Artificial Neural Network (ANN) consists of an input layer, one or more hidden layers and an output 

layer as shown in Figure 4.  

Figure 4: Neural Network or Artificial Neural Network (ANN) 

The layers are made up of nodes, and each layer is connected to the next layer. Once the training of the 
ANN is completed with raw data, the ANN can be used to classify new data. Each node works based on its own 

10 

 
 
 
 
 
 
linear regression model, which is composed of input data, weights and a threshold (bias) and an output. Machine 
Learning  (ML)  and  Deep  Learning  (a  subset  of  ML),  which  are  part  of  Artificial  Intelligence  (AI),  are  also 

emerging fields for recognising affective states. Though ANN has various specific advantages over the HMM 
and GMM it still lacks rules for the optimal setting of ANN topology, and the learning and training makes the 

classification procedure difficult. 

Agarwalla  and  Sarma  used Feed-Forward  (FF)  network  and Deep  Neural Network  (DNN)  approaches to 
extract relevant data samples from big data space and used them for automatic speech recognition using soft 

computing techniques for Assamese speech with dialectal variations. The Feed-Forward Network is ANN where 
the information only moves in one direction without forming a loop between different nodes in layers. The Multi-

Layer  Perceptron  (MLP),  which  is  a  Feed-Forward  ANN,  was  configured  with  inputs  to  learn  emotion 
classification information using clustering and manual labelling. The extracted features comprised spectral and 

prosodic features. The obtained features were applied to Recurrent Neural Network (RNN) and Fully Focused 
Time  Delay  Neural  Network  (FFTDNN)  for  the  evaluation  of  their  performance  to  recognise  mood,  dialect, 

speaker, and gender variations in Assamese speech. Agarwalla and Sarma tested models by considering the 
recognition rates which were obtained by using confusion matrices and manually calculating time. The results 

showed that their proposed Machine Learning (ML) based sentence extraction techniques and the composite 
feature set when used with RNN as a classifier outperformed all other approaches [Agarwalla and Sarma 2016]. 

The Recurrent Neural Network (RNN) is one of the commonly used deep learning algorithms. RNN is a family 
of neural networks specialised for sequential data processing and has short term internal memory.  Although 

RNNs use training data to learn similar to other ANNs, they are distinguished as they can also learn from prior 
given inputs to improve current outputs. Furthermore, RNNs also share their parameters across each network 

layer and share the same weight within each layer of the network [IBM 2020].  On the other hand, FFTDNN is 
a multilayer ANN which classify pattens based on shift-invariance, without needing explicit segmentation before 

classification.  

Various  deep  learning  methods  contribute  to  emotion  recognition  from  speech  or  song.  Deep  learning 
involves the creation of algorithms with multiple layers, working similarly to the human brain process of sensing 

(message travels within many neurons before making a decision). DNN is the hidden network of algorithms as 
seen in the ANN’s middle layers highlighted in turquoise colour in Figure 4. The input data go through different 

layers of DNN. DNNs then form high-level invariant appropriate features called identifiers from raw input data 
and then go on to classify audio and voice data for detecting emotions [Bengio et al. 2013]. Therefore, DNNs 

perform very well in ML tasks such as speech recognition [Dahl et al. 2011]. In recent years, the performance 
of the DNN has surpassed the traditional ML algorithms. Furthermore, the feature is automatically selected with 

the use of deep learning algorithms.  

Ooi et al. [Ooi et al. 2014] used prosodic and spectral features for recognising six different emotions: anger, 
happiness, sadness, disgust, fear and surprise. The performance of the proposed architecture, which included 

two  main  paths,  were  evaluated  on  eNTER-FACE’05  [Martin  et  al.  2006]  and  RML  database  [Ryerson 
Multimedia Research Lab. 2017]. It revealed that the  Radial  Basis Functions Neural Network (RBFNN) was 

11 

 
 
 
more accurate in recognising emotions than in comparison to RNN. RBFNN transforms the input signal to  a 
different form, that can be fed into the single hidden layer (here called feature vector) to get linear separability. 

Hao et al. [Hao, Cao, Liu, Wu and Xiao 2020] used an ensemble visual-audio emotion recognition algorithm 

to identify different emotions. A multi-task model (which included four sub-models) was built based on the CNN 
networks which is another type of DNN. The authors performed two different experiments using eNTERFACE 

database,  showing  that  multi-task  CNN  recognised  3%  over  the  CNN  model  in  speaker-independent 
experiments and an average of 2% more over the CNN model in speaker-dependent experiments respectively. 

The emotion recognition accuracy rate was also  reported to be increased to 81.36% in speaker-independent 
and 78.42% in speaker-dependent experiments. 

Issa et al. [Issa et al. 2020] used one-dimensional deep CNN. The input features included MFCC, Mel-scale 

spectrogram,  and  spectral  contrast  features.  The  features  were  extracted  using  samples  from  the  Ryerson 
Audio-Visual  Database  of  Emotional  Speech  and  Song  (RAVDESS)  [Livingstone  and  Russo  2018],  Berlin 

(EMO-DB)  [Burkhardt,  Paeschke,  Rolfes,  Sendlmeier  and  Weiss  2005],  and  Interactive  Emotional  Dyadic 
Motion Capture (IEMOCAP) datasets [Busso et al. 2008]. They utilized an incremental process to modify initial 

models to improve classification accuracy. The proposed framework obtained an accuracy rate of 71.61% for 
RAVDESS, 86.1% for EMO-DB, 95.71% for EMO-DB and 64.3% for IE-MOCAP in speaker-independent audio 

classification tasks. 

We  summarise  most  common  algorithms  used  by  researchers  for  detecting  emotions  in  Table  2. 
Furthermore, most of the research related to emotion recognition and affective state detection has emphasised 

detection of six universal emotions of disgust, sadness, happiness, fear, anger, and surprise. Moreover, it is 
quite  evident  from  the  table  that  a  vast  range  of  corpora  such  as  SUSAS  [Hansen  et  al.  1997],  Emo-

DB[Burkhardt, Paeschke, Rolfes, Sendlmeier and Weiss 2005] and Interactive emotional dyadic motion capture 
database (IEMOCAP) [Busso, Bulut, Lee, Kazemzadeh, Mower, Kim, Chang, Lee and Narayanan 2008] and 

Simulated Telegu Speech Corpus (IITKGP-SEHSC) [Koolagudi, Maity, Kumar, Chakrabarti and Rao 2009] are 
commonly utilised databases for research work. 

12 

 
 
 
 
TABLE 2 

Various Models of Detection of Emotions from the Human Voice 
*Anger=An, Happiness=Hp, Sadness=Sd, Boredom=Bo, Disgust=Ds, Fear=Fr, Panic=Pc, Elation=El, Shame=Sh, Pride=Pr, Despair=De, Interest=In, Contempt=Co, Neutral = Ne, Excitement = Ex, 
Frustration = Fu, Lombard=Lo, Loud= Lu, Surprise =Sr, Joy= Jo, Sarcastic=Sc 
Here Lombard (Lo) may refer to the Lombard effect in consideration with the database used, the environment in which it was created, and how the researchers applied it in their research. 

Algorithms  

Features Used 

Affective States and 
Emotions Detected* 

Performance  

Reference 

GMM 

Fundamental frequency contours, pitch contours 

An, Hp, Sd, Bo, Ds, Fr, Pc, 
El, Sh, Pr, De, In, Co 

Over  77%  accuracy  on  three  different  databases  collected  at 
various universities  

 [Busso,  Lee  and  Narayanan 
2009] 

Spectral features 

An, Fr, Ne, Hp, Sd 

82.5% accuracy on the dataset collected by hiring participants  

[Hu, Xu and Wu 2007] 

HMM 

MFCC,  Pitch,  TEO-CB-AUTO-ENV,  16-GA  feature, 
48-GA feature 

Ne, An, Lm, Lo 

Increased  performance  of  emotion  detection  using  the  16-GA 
features on SUSAS database  

[Casale,  Russo  and  Serrano 
2007] 

Pitch, log energy, formant, MFCCs,  

Ne, An, St, Lm, Lo 

SUSAS  using  GSVM  90%  and  92%  for  neutral  and  stress 
speech, 96.3% of recognition rate obtained by using HMM while 
70% was obtained for 4-class style classification  

[Kwon  [Kwon,  Chan,  Hao  and 
Lee 2003] 

Prosodic and spectral features  

Sr, Jo, An, Fr, Ds, Sd, Ne 

Log frequency power coefficients (LFPC) 

Lo, and Lu, St Ne 

SVM  

MFCC, LPCC, TEO-AutoCor 

An, Ex, Fu, Hp, Ne, Sd 

Energy,  pitch, MFCC  coefficients,  LPCC coefficients 
and speaker rate 

An, Fr, Hp, Sd 

Fundamental frequency, formant, short-term  energy, 
loudness and (MFCCs) Line Spectral Pairs (LPSs)  

Sr, An, Fr, Ds, Hp, Sd, Ne 

MFCCs, Spectral centroids, and MFCC derivatives 

Sr, An, Fr, Ds, Hp, Sd, Ne, 
Sr, Bo, Ne, Ca 

13 

70% accuracy on Spanish corpus of INTERFACE 
Emotional  Speech  Synthesis  Database  [Barra  Chicote  et  al. 
2008] 
For Burmese:78.5% accuracy  

For the Mandarin utterance: 75.7% accuracy  
On the constructed database  
SVM 
90.12% on EMO-DB, 83.2% on IEMOCAP 

k-NN  

89.3% on EMO-DB, 78% on IEMOCAP 

90.08%  accuracy  with  Linguistic  data  consortium  (LDC) 
datasets  and  of  65.97%  with  University  of  Georgia  (UGA) 
datasets 

MFCCs features gave an accuracy of 85.085% in comparison 
to LPCC features accuracy of 73.125 %  

Accuracy reaches 81.36% and 78.42% in speaker-
independent and speaker-dependent experiments conducted 
on eNTERFACE database [Martin, Kotsia, Macq and Pitas 
2006]  

An accuracy of: 
92.45% on EmoDB 
75.69% on RAVDESS 
and 84.11% on IITKGP-SEHSC 

 [Nogueiras,  Moreno,  Bonafonte  and 
Mariño 2001] 

 [Nwe, Foo and De Silva 2003] 

[Bandela and Kumar 2019] 

[Jain, 
Bhowmick and Muthu 2020] 

Narayan, 

Balaji, 

[Hao,  Cao,  Liu,  Wu  and  Xiao 
2020] 

[Bhavan,  Chauhan,  Hitkul  and 
Shah 2019] 

 
 
 
 
 
 
 
 
 
 
 
 
Spectral features  

Prosodic features  

An, Fr, Ds, Hp, Sd, Ne 

46.1% on LDC 

81.3% for Berlin database of German emotional speech 

[Bitouk,  Verma  and  Nenkova 
2010] 

Global and local Prosodic features 

An, Fr, Ds, Hp, Sd, Sc, Sr 

MFCC 

St 

Local  prosodic  features  performed  better  compared  to  the 
global prosodic features on IITKGP-SEHSC 
66.4% on manually collected data 

[Rao et al. 2013] 

[Han et al. 2018]  

Prosodic and spectral features 

An, Hp, Sd, Ds, Fr, Su 

68.57 by using the RBF neural network 

DNN 

MFCC feature, pitch related features, 

Ex, Hp, Ne, Sd 

54.3% average recognition rate 

k-NN 
CNN  rectangular 
kernels  

Pitch, spectral features, ZCR, Intensity, MFCCs  
Spectrograms  

Chroma  gram,  spectral  contrast  features,  Mel-scale 
spectrogram, Tonnetz representation 

Hp, An, Sd, Fr and Ne 
An, Hp, Sd, Bo, Ds, Fr, Ne 
An, Ca, Ds, Fr, Hp, Ne, Sd, 
Su 

66.24% average recognition rate on Chinese emotional data set  

80.79 % on Emo-DB and Korean Speech Database  
71.61% for RAVDESS 
86.1% on EMO-DB  
95.71% for EMO-DB  

and 64.3% for IEMOCAP with 4 classes 

[Ooi,  Seng,  Ang  and  Chew 
2014] 
[Han et al. 2014] 

[Rong, Li and Chen 2009] 
[Badshah et al. 2019] 

[Issa,  Fatih  Demirci  and  Yazici 
2020] 

Hierarchical 
classifiers  

Spectral, prosodic features, mean of the log spectrum 

Jo, An, Fr, Ds, Bo, Ne, Sd 

An average 71.5% recognition rate on Berlin Emo-DB 

[Albornoz et al. 2011] 

14 

 
 
 
 
 
 
 
 
 
 
Few studies extended the research in the area of detecting stress during any stressful event using either non-
verbal  features  [Han,  Byun  and  Kang  2018;  Han,  Yu  and  Tashev  2014;  Pfister  and  Robinson  2011]  or 

physiological features [Barreto, Zhai and Adjouadi 2007; Gillespie et al. 2017]. 

Barreto et al. was one of the limited studies that detected stress by monitoring physiological signals, such 
as galvanic skin response, temperature of skin, blood pulse volume, and left eye pupil diameter of the subjects, 

during a determined computer-based, paced Stroop test. The Stroop incongruent segment was considered to 
be related to the stressed state in participants, while congruent Stroop was linked to a non-stressed state in 

participants. Twelve out of 32 samples were used to train the classifiers while the remaining 20 samples were 
used to test the classifiers. All the extracted features of the physiological signals, such as the mean amplitude 

of the individual blood pulse volume beat and the mean value of pupil diameter, which were expected to increase 
under stress, were given as the input to various machine learning algorithms, such as Naïve Bayes, Decision 

Tree,  and  SVM  classifiers,  to  classify  the  relaxed  versus  stressed  state  in  participants  [Barreto,  Zhai  and 
Adjouadi 2007]. 

Another study conducted by Han et al. proposed an algorithm to determine the state of stress (stressed or 

unstressed) via voice features using a binary decision criterion involving two layers of Long Short-Term Memory 
Recurrent  Neural Networks (LSTM-RNN) and a classifier. The authors evaluated the proposed algorithm by 

obtaining the speech, video, and bio-signal data under stressful (interview of the Korean speaking subjects in 
English  by  a  foreigner)  and  non-stressful  (comfortable  conditions  such  as  watching  videos)  conditions  and 

surveyed participants about the stress they felt. Based on the survey results and cortisol levels, the speech 
signals from 25 subjects whose salivary cortisol level changed more than 10% were labelled as stress or non-

stress in the database. Additionally, 15 subjects were used to train the model, while five stressed and five un-
stressed speech samples from the database were used to test the data. The authors pre-processed the data 

before extracting the MFCC, which were used to determine stressed and unstressed state from the testing data. 
The proposed LSTM-RNN with SVM classifier achieved 66.4% accuracy in stress detection [Han, Byun and 

Kang 2018]. 

Furthermore, Pfister and Robinson presented a new classification algorithm to assess public speaking skills. 
They trained the classifier using the Mind Reading corpus [Junek 2007] for the detection of both simple and 

complex emotions. For the classifier to detect the emotions, the authors chose a wide variety of emotions such 
as joy, interestedness, unfriendliness, excitement, unsureness, and sureness from the Mind Reading corpus. 

They pre-processed the corpora, extracted the non-verbal features of speech and computed the SVM model. 
Then, to assess public speaking skills, Pfister and Robinson first retrained the classifier using six labels which 

are  commonly  used  by  professional  experts.  For  this  purpose, they  asked  an  experienced speech coach to 
label  124  one-minute-long  speech  samples  obtained  from  31  speakers  attending  speech  coaching.  Finally, 

Pfister  and  Robinson  segmented  the  live  audio,  extracted  the  audio  features,  and  ran  the  classifiers.  For 
pairwise machine, the results yielded an average cross-validation accuracy of 89% while for the fused machine, 

86%  of  accuracy  was  obtained.  For  assessing  public  speaking  skills,  the  novel  application  of  the  classifier 
achieved 81% cross-validation accuracy and 61% accuracy when performed using a leave-one-speaker-out 

method [Pfister and Robinson 2011].  

15 

 
 
 
 
Specifically,  stress  is  common  during  public  speaking.  Although  public  speaking  is  recognised  as  an 

important skill, it has received little attention for real-time voice analysis. This is most possibly due to limitations 
and complexities involved in creating a suitable testing environment. To overcome this, technologies like VR 

will be more helpful in creating a public speaking simulation. 

4  VIRTUAL REALITY FOR PUBLIC SPEAKING  

VR has also been utilised for improving public speaking skills [Kimani and Bickmore 2019; Lister et al. 2010; 

North et al. 1998; Takac et al. 2019], and other interpersonal skills. The benefit of the VR environment is the 
immersive experience it brings to users, which provides the experience of learning in a way that is like real-life 

learning  [Azwar,  Alam,  Kazmi,  Zain-ul-abidin  and  khan  2016;  Gavish  et  al.  2015;  Nijholt  2014].Therefore, 
learning skills using VR is becoming increasingly popular as well [Dascalu et al. 2017].  It is therefore useful in 

prototyping,  scientific  visualization,  training,  engineering,  manufacturing,  and  learning  [Dillon  et  al.  2006; 
Kassem et al. 2017; Le et al. 2015; Pedro et al. 2016; Schmid Mast et al. 2018]. 

Researchers have applied VR in various settings, and some of them are summarized in Table 3 [Bhagat et al. 

2016; Bouchlaghem et al. 2005; Carl et al. 2019; Chittaro and Sioni 2015; Crocetta et al. 2018; Dechant et al. 
2017; Garcia-Palacios, Hoffman, Carlin, Furness and Botella 2002; Gebara et al. 2016; Gerardi et al. 2010; 

Hilfert and König 2016; Kim et al. 2009; Laver et al. 2015; Levy et al. 2016; Lindner et al. 2019; Manju et al. 
2017; Maskey et al. 2014; Miloff et al. 2016; Molina et al. 2014; Motraghi et al. 2014; Mujber et al. 2004; Opriş 

et al. 2012; Ordaz et al. 2015; Parsons and Rizzo 2008; Ticknor 2018; Ticknor 2019; Valmaggia et al. 2016]. 

TABLE 3 
Various Applications of Virtual Reality (VR) 

Category  

Industrial 

Educational  

General  

Broad Applications 

Gaming 
Architectural  
Design Construction  
Manufacturing 
Healthcare 

Learning  
Overcoming fears  

Relaxation therapies  

Clinical Psychology  

Assessing & treating disorders  

Complement to standard CBT treatments   

Specific applications 

Stroke Rehabilitation 

Diagnostic tool 

Social Skills Training 
Fear of heights 

Fear of spiders  

Fear of public speaking  
Anxiety, Posttraumatic Stress Disorder  

Schizophrenia 

Depression 

Eating disorders 

Furthermore, there has been much research interest in using VR to explore feedback strategies to provide 
social support in a VR environment for public speaking [Brundage and Hancock 2015; Chollet et al. 2015]. It is 

reported that psychosocial stress due to public speaking is a common occurrence across the world, which may 

16 

 
 
 
 
 
 
 
hinder the quality of the speech delivered [Koroleva, Bakhchina, Shyshalov, Parin and Polevaia 2014; Ruscio 
et al. 2008]. Studies suggest that interactions with virtual entities and their behaviours may be able to elicit both 

social stress and social support [Kothgassner et al. 2016; Kothgassner et al. 2019; Kotlyar, Donahue, Thuras, 
Kushner, O'Gorman, Smith and Adson 2008; Pan et al. 2012]. Conducting public speaking tasks in a laboratory 

setting can help in the evaluation of real-life stress and finding ways to overcome it as well [Feldman et al. 2004; 
Jezova et al. 2016; Owens et al. 2015]. 

Brundage  &  Hancock  in  2015  investigated  VR  exposure  therapy  for  treating  people  who  stutter.  In  their 

study, ten people who stutter first delivered a speech to a real audience and then, on another day, performed 
in front of two virtual audience members. The study results indicated that participants had an experience in the 

virtual setting similar to their experience with the real audience [Brundage and Hancock 2015]. Therefore, VR 
can be used as a standardised tool for stress [Jönsson et al. 2010] and fear [Powers et al. 2013] induction in 

real life. Notably, these studies show how people around us, even in a virtual setting, can influence our feelings, 
behaviour,  and  performance.  In  another  research study,  the  authors  explored  feedback  strategies  for  public 

speaking  by  training  a  virtual  audience  to  be  non-interactive  (control  condition),  or  to  provide  direct  visual 
feedback, or non-verbal feedback from non-verbal behaviour such as eye contact and pause fillers [Chollet et 

al. 2015; Wörtwein et al. 2015]. 

Furthermore, research on VR exposure therapy was investigated by Hartanto et al. [Hartanto et al. 2014] to 
be used for the treatment of anxiety disorders by controlling social stress. The intervention included a one-to-

one dialogue session between the speaker and the virtual audience. The participants were exposed to three 
virtual scenarios of a neutral virtual scene, a blind date condition and a job interview session. The participants’ 

anxiety levels were significantly increased when the social situation changed. 

In one of the earliest studies conducted by Slater et al.1999, studied the psychotherapeutic effects of a virtual 
environment  on social  phobia for  public  speaking.  The  study  had  an  experimental  design  of  a virtual  public 

scenario  with  avatars.  The  avatars  displayed  random  behaviours  such  as  nodding,  twitching,  and  blinking. 
Avatars could also yawn, clap, and show hostile reactions. Ten graduate students were recruited from University 

College London (UCL). The participants either gave their speech to the audience on the monitor or were entirely 
‘immersed’ in a VR setting with a headset. Each participant repeated their speech three times. At the first time, 

the participants faced either a hostile or a friendly audience. At the second time, participants faced whichever 
audience they did not face during the first speech. At the third time, participants faced an audience with hostile 

reactions, which eventually become friendly. A participant immersed using the VR headset reported a high co-
presence with a hostile audience (felt as though the participant was together with the listeners in real-life) but 

did not feel as though the audience was interested. This shows that co-presence amplified the situation and a 
low-interest audience reduced self-rating and increased anxiety in the speaker [Slater, Pertaub and Steed 1999]  

In a separate study by Slater et al. 2006, therapeutic intervention in VR was used to reduce the fear of public 

speaking. The experiment included 16 participants with a fear of public speaking and 20 participants with a 
lower amount of fear of public speaking to give a speech in an empty seminar room in a VR setting. Later, the 

same  room  was  populated  by  a  neutrally  behaving  virtual  audience  of  five  people.  The  responses  from 

17 

 
 
  
 
participants with a fear of public speaking showed a significant increase in signs of anxiety when speaking to 
the virtual audience in comparison to when speaking to the empty room. In contrast, the participants with a 

lower amount of fear of public speaking reported no anxiety at all [Slater, Pertaub, Barker and Clark 2006].  

Significant differences were detected between groups undergoing virtual reality therapy (VRT) sessions and 
a  control  group,  with  VRT  found  to  be  effective  in  reducing  the  fear  of  public  speaking  [North  et  al.  1998]. 

Furthermore,  another study  showed  that  a  hostile,  negative  audience  scenario  generated  a strong effect  in 
speakers, regardless of whether the participants feared public speaking. The results suggested that induced 

anxiety is directly related to the virtual audience’s feedback [Pertaub et al. 2001].  

A group of researchers developed a virtual character system, which accepts an audio signal’s transcription as 
the input [Pellett and Zaidi 2019]. Based on the analysis of the audio signal received, the system generates an 

animated  performance  as  output.  A  framework  for  virtual  reality  training  to  improve  public  speaking  was 
proposed, which involved assessing the speech using a speech-to-text approach [Marsella et al. 2013], using 

the Windows dictation recogniser. The speech performance results were shown to speakers at the end of their 
speech  to  grade  their  overall  performance.  Another  more  inventive  presentation  trainer  was  developed  to 

assess the public speaking performance by giving the user real-time feedback about different aspects of the 
participant’s non-verbal communication. It tracks the participant’s voice and body to interpret performance and 

present an intervention to give feedback to the user [Schneider et al. 2015]. In another research study, a group 
of  researchers  developed  an  interface  using  Google  Glass  to  help  people  to  improve  public  speaking  by 

automatically detecting the speaker’s volume and speaking rate in real time. Based on the speaker’s volume 
and speaking rate, the system provides feedback during the speech [Tanveer et al. 2015].   

Similarly,  El-Yamri et al. 2019, created a VR system, which included a virtual audience character model. 

Emotions such as stress elicited in the speaker by the virtual audience character model were analysed using a 
third-party emotion detection application. The detected emotional states, such as stress and happiness, were 

weighted using the authors’ common sense only. This may not be accurate due to insufficient and unclear in-
formation for the basis of common sense used. Moreover, this model lacks a learning process  for the virtual 

agent based on the analysis of voice features and other factors such as physiological parameters. For example, 
the authors could have detected emotions using VR in real time and/or integrating the developed VR application 

with another built emotion detection model to analyse voice features and physiological parameters. Based on 
the detected emotions, the authors could have then given the score to the speaker. This score could have been 

used  to train the  virtual  audience  to  provide  real-time  feedback to the  speaker  based  on  a certain score  of 
speakers [El-Yamri et al. 2019]. 

Table 4 shows some of the relevant studies on public speaking in VR. 

18 

 
 
 
 
 
Article 

Main issue focused on 

Task 

Responses of virtual audience 

TABLE 4 
Various Relevant Studies of Public Speaking in Virtual Reality (VR) 

[Brundage and Hancock 2015] 

Affect generation  

[Chollet, Wörtwein, Morency, Shapiro and Scherer 2015] 

Anxiety 

[El-Yamri, Romero-Hernandez, Gonzalez-Riojo and 
Manero 2019] 

[Kothgassner, Felnhofer, Hlavacs, Beutl, Palme, Kryspin-
Exner and Glenk 2016; Kothgassner, Goreis, Kafka, 
Kaufmann, Atteneder, Beutl, Hennig-Fast, Hlavacs and 
Felnhofer 2019] 
[Pertaub, Slater and Barker 2001] 

[Slater, Pertaub, Barker and Clark 2006; Slater, Pertaub 
and Steed 1999] 

Fear/stress 

Stress 

Fear 

Anxiety 

Speech 

Speech 

Speech 

Speech 

Speech 

Speech 

Neutral and challenging virtual audience 

Non-verbal indirect feedback 

Real-time feedback voice tone, gaze, and 
speech content 

Real  audience,  Virtual  audience,  Empty 
virtual lecture hall 

Positive, 
behavior 

Positive 
feedback 

negative 

comments 

and 

and 

negative 

evaluation 

[Hartanto, Kampmann, Morina, Emmelkamp, Neerincx and 
Brinkman 2014] 

Stress 

One-to-one conversation 
session 

Neutral and conversing audience 

19 

 
 
Overall, most studies related to public speaking in VR show a lack of clarity in terms of how the affective state 
of the speaker was determined in real time during the tasks. Furthermore, changes in the speaker’s affective 

state  due  to the  virtual  audience’s  feedback  is  or  was solely  understood  based  either  only  on  self-reported 
questionnaires or physiological change. An additional way to determine speaker’s affective state is to assess 

emotions  in  real time  by  analysing  the voice.  Interestingly,  emotion/affective  state  detection  from  voice  has 
unique traits and also common elements shared with the analysis of emotional contents in music, as discussed 

in one of earlier works by Scherer [Scherer 1995]. Particularly for detection of stress from voice, a approach 
similar to discussed by Dillon in research conducted can also be investigated  [Dillon 2001; Dillon 2003]. 

In  next  Section  5  we  will  discuss  the  gaps,  opportunities,  potential  barriers,  and  suggestions  based  on  the 
literature survey in more details. 

5  DISCUSSION 

It is well-known and established that communication by speech is a powerful means of human expression 

and  connection.  The  human voice  has  proven to be  a  good  indicator  of  emotional/affective  state  in  several 
studies [Cowen, Elfenbein, Laukka and Keltner 2019; Juslin and Laukka 2003; Mitchell and Ross 2013]. In the 

areas of HCI and affective computing, recognition of emotions and human affective states is one of the most 
challenging tasks. Emotions such as anxiety and affective states such as stress are subjective to individuals. 

Therefore,  the  development  of  a  detection  model  depends  on  the  affective  state  causing  event  and  its 
application area. 

Various  models  exist  and  are  utilised  for  analysing  the  human  voice  to  detect  emotions/affective  states. 

Some studies suggested that prosodic features work more appropriately in the identification of emotion with the 
GMM [Li, Tao, Johnson, Soltis, Savage, Leong and Newman 2007]. Previous research also suggests that the 

HMM  approach  achieves  a  higher  recognition  rate  when  detecting  happiness,  anger,  joy,  fear,  disgust,  and 
sadness using spectral features such as MFCC and LPCC [Nogueiras, Moreno, Bonafonte and Mariño 2001; 

Nwe, Foo and De Silva 2003]. Furthermore, in a comparison of HMM and SVM to classify five emotions (anger, 
happiness,  sadness,  surprise,  and  neutral  emotion),  the  results  showed  a  99.5%  accuracy  rate  for  HMM, 

whereas  SVM  obtained  an  accuracy  rate  of  only  88.9%  in  recognizing  the  emotions  [Lin  and  Wei  2005]. 
However, other research studies indicate SVM as promising for the classification of stress or not stressed or 

other states [Barreto, Zhai and Adjouadi 2007; Truong et al. 2012]. Other computational models such as Artificial 
Neural Network (ANN), Recurrent Neural Network (RNN), and Convolutional Neural Network (CNN) are also 

reported to perform well. However, in recent years, the performance of the Deep Neural Network (DNN) has 
surpassed the traditional Machine Learning (ML) algorithm [Dahl, Yu, Deng and Acero 2011].  

Most  of  the  research  related  to  emotion  recognition  and  affective  state  detection  has  emphasized  the 

detection of six universal emotions (i.e., disgust, sadness, happiness, fear, anger, and surprise) using spectral, 
temporal, and prosodic features [Bhavan, Chauhan, Hitkul and Shah 2019; Busso, Lee and Narayanan 2009; 

Hao, Cao, Liu, Wu and Xiao 2020; Hu, Xu and Wu 2007; Jain, Narayan, Balaji, Bhowmick and Muthu 2020; 
Nogueiras, Moreno, Bonafonte and Mariño 2001; Nwe, Foo and De Silva 2003; Ooi, Seng, Ang and Chew 2014; 

Schuller  et  al.  2003;  Semwal,  Kumar  and  Narayanan  2017].  Furthermore,  the  extraction  of  voice  features 

20 

 
 
 
 
depends  upon  emotion  or  affective  state that  needs  identification. For  example,  Low  et  al.  [Low,  Maddage, 
Lech,  Sheeber  and  Allen  2010]  concluded  that  specifically  for  the  identification  of  stress,  Teager  Energy 

Operators (TEO) based features, and especially the TEO CB AutoEnv, are more effective in comparison to all 
other types of features. Furthermore, sometimes the voice features may overlap between different emotions, 

and therefore, it is crucial to extract correct features carefully.  

 From these studies, it can be inferred that various voice features, and various datasets and models play a 
different role in emotion recognition and affective state identification. It depends on the dataset we choose the 

emotion we want to detect, and the preference of algorithm. A combination of carefully extracted features, with 
a proper noise-free training data set and with the appropriate classification algorithm, is needed to develop an 

accurate detection model. Once the application of emotion detection is validated within a specific context, it is 
possible to port it into other settings to detect stressful events of different nature with an improved recognition 

rate. 

5.1  Gaps and Opportunities 

Public speaking is recognised as an essential skill as well as a cause of stress. Acquiring public speaking skills 

for the presentation of ideas is important in various professional contexts [Docan-Morgan and Nelson 2015]. It 
is reported that psychosocial stress due to public speaking is a common occurrence across the world, which 

hinders the quality of the speech delivered [Koroleva, Bakhchina, Shyshalov, Parin and Polevaia 2014; Ruscio, 
Brown, Chiu, Sareen, Stein and Kessler 2008]. Although much recent research has been done in the field of 

emotion detection, not enough research has been carried out to  specifically detect stress in the context of a 
potentially stressful event such as public speaking in real time.  

More  specifically,  public  speaking  has  received  little  attention  for  real-time  stress  detection  by  analysing 

voice  features  and  physiological  parameters.  Very  few  studies  have  extended  research  to  the  matter  of 
detecting stress during any potentially stressful event using either non-verbal features [Han, Byun and Kang 

2018; Pfister and Robinson 2011] or physiological features [Barreto, Zhai and Adjouadi 2007; Gillespie, Moore, 
Laures-Gore, Farina, Russell, Logan and Ieee 2017] or self-reported indices [Bandela and Kumar 2019].  

For example, Pfister and Robinson [2011] utilised non-verbal features to detect the affective state and apply 

it to  assess  public  speaking  only,  leaving  out  the physiological  parameters. The  accuracy  of  this method  of 
detecting  stress  may  not  be  high  because  of  the  restriction  to  just  non-verbal  features,  leaving  out  the 

physiological parameters in the context of public speaking.  

In  another  example,  in  the  study  conducted  by  El-Yamri  et  al.  [El-Yamri, Romero-Hernandez,  Gonzalez-
Riojo and Manero 2019] where the authors created a VR system, which included a virtual audience character 

model.   Stress  elicited in  a speaker was  identified and analysed using a third-party  affective state  detection 
application. Furthermore, the emotional states of the speaker were weighted using the authors’ common sense 

only. This is not an appropriate and accurate option due to insufficient and unclear information for the basis of 

21 

 
 
 
 
 
 
‘common sense’ used, which can be culturally dependent and hence could potentially bias the recognition of 
certain emotions over others. Moreover, this model lacks a learning process for the virtual agent based on the 

analysis  of  voice  features  and  other  factors  such  as  physiological  features.  Also,  knowing  how  the  virtual 
audience were made to work can help in improving the experiment and real-time feedback to the speaker during 

public speaking.  

The literature review points out that real-time stress detection in the areas of affective computing and HCI 
has not received much attention, and this is possibly due to the limitations and complexities in creating a suitable 

testing environment for stress during public speaking. While conducting public speaking tasks in a laboratory 
setting can help in the evaluation of real-life stress [Jezova, Hlavacova, Dicko, Solarikova and Brezina 2016], 

technologies like VR can be employed to create a simulation for public speaking. Previous studies have also 
indicated that interactions with virtual entities are able to elicit social stress. It can also be inferred from research 

that VR can be used to provide Virtual Social Support (VSS) during a stressful activity [Kothgassner, Goreis, 
Kafka, Kaufmann, Atteneder, Beutl, Hennig-Fast, Hlavacs and Felnhofer 2019]. In this regard, research has 

been carried out using VR to examine the effects of VSS for public speaking [Chollet, Stefanov, Prendinger and 
Scherer  2015;  Chollet,  Wörtwein,  Morency,  Shapiro  and  Scherer  2015;  El-Yamri,  Romero-Hernandez, 

Gonzalez-Riojo and Manero 2019; Hartanto, Kampmann, Morina, Emmelkamp, Neerincx and Brinkman 2014; 
Pertaub, Slater  and Barker 2001; Slater, Pertaub, Barker and Clark 2006; Slater, Pertaub and Steed 1999]. 

However,  most  of  the  research  (e.g.,  [Barreto,  Zhai  and  Adjouadi  2007;  Hartanto,  Kampmann,  Morina, 
Emmelkamp, Neerincx and Brinkman 2014; Slater, Pertaub, Barker and Clark 2006]) related to VR for public 

speaking focused on testing the effects of virtual audience behaviour on a speaker’s anxiety and fear of public 
speaking  by  only  measuring  the  speaker’s  physiological  parameters  such  as  heart  rate  or  blood  pressure. 

Despite  being  important  indices  to  emotional  experience, changes  in  heart  rate  and  blood  pressure  are  not 
exclusively indicative of specific emotional experience.  The research could have been improved by detecting 

emotions from speakers’ voices, which could have made the detection of stress more accurate since the human 
voice is a powerful channel to convey emotions and this has not been done using VR before as accurately as 

it could be done.   

Based on the gaps identified, we propose the development of a stress detection model. This model would 
be able to analyse the voice of users in real time. The selected voice features to develop the stress detection 

model will be correlated to the physiological parameters of stress such as heart rate and blood pressure as well 
as psychological results obtained from self-reported questionnaires such as stress arousal checklist [Cox and 

Mackay 1985] and the task specific anxiety checklist. 

22 

 
 
Figure 5: Data collection in process for developing stress detection model 

After data collection (Figure 6), the first step would be to analyse the self-reported responses in pre- and post-test questionnaires for each 
condition for all speakers. We will then check the corresponding heart-rate readings and blood pressure readings of the participants for both 

VR scenarios. If both the psychological responses and physiological responses indicate that the person was stressed during public speaking 
in VR, we will proceed to extract the voice features, which will be counted as our stressed voice data. On the other hand, the speeches provided 

by participants who did not show signs of stress during their speech will be kept as our confident voice data set. Once the features of voice are 
extracted, we will evaluate various algorithms and choose the one that can provide us with the best accuracy for developing the stress detection 

model. 

23 

 
 
 
 
Figure 6: Steps in Development of Stress Detection Model to be Integrated in VR application  

24 

 
 
 
The developed stress detection model will then be integrated or connected to a virtual audience in the VR 
application to provide real-time feedback in two modes (social supportive and realistic feedback mode) (Figure 

7). 

Figure 7: VR game application for improving public speaking skills 

This would allow users to practice their public speaking in social supportive mode (when less certain about 
their speaking abilities) or they could choose to practice in realistic feedback mode (when sure about their public 

speaking skills and wanting to check their performance). This will allow users to gradually learn to overcome 
stress caused by public speaking and therefore improve their public speaking skills. 

5.2  Potential Barriers and Solutions 

Several issues may arise in the implementation procedure of the suggestion presented in 5.1. For example, 

there may be barriers in the development software for extraction and analysis of all necessary voice features. 
Issues may also arise if we want to analyse the physiological parameters of stress in real time. We can start by 

pre-processing of recorded voice data, followed by extracting the spectral features of voice such as frequency 
contours, and prosodic features using Python libraries like My Voice Analysis [Shabahi 2020], LibROSA [McFee 

25 

 
 
 
 
 
et al. 2015] or pyAudioAnalysis [Giannakopoulos 2015]. Furthermore, if needed, similar to approach labelled by 
Batrinca, we can also seek expert advice in deciding various prosodic features [Batrinca et al. 2013]    

In case issues arise with the analysis of the physiological parameters of stress in real time, we can also 

develop another application to record the physiological parameters separately for analysis of stress levels. We 
may  then  try  to  combine  the  results  within  the  emotion  detection  model  by  training  it  with  voice  features 

combined with physiological parameters specifically for detecting stress. This would allow the integration of the 
whole developed model into the VR application using TensorFlow. The built emotion detection model would be 

based on analysis of voice features such as prosodic features, spectral features, and physiological parameters 
of stress.  

This  model  could  be  used  to  train  the  virtual  audience  present  in  the  VR  application  using  the  Machine 

Learning Agents SDK (ML-Agents) using various machine learning methods such as reinforcement learning. 
This  would  allow  the  virtual  agents  to  provide  real-time  feedback  to  the  speaker  with  respect  to  voice  and 

physiological features.  

Therefore, analysis of stress elicited in a public speaker by a virtual audience should be accomplished by 
using stress detection models in VR. Furthermore, the developed stress detection model combined with voice 

analysis and physiological symptoms of stress could be used to train the virtual audience in real time to provide 
feedback. From the research surveyed here, it is evident that stress detection models in VR have not yet been 

utilised for providing real-time feedback based on both voice analysis and physiological symptoms. 

6  CONCLUSION 

Results  and  findings  from  the  present  review  are  encouraging.  There  are  opportunities  to  advance  the 
research  in  voice  analysis  for  specifically  detection  of  stress  in  a  VR  setup  during  public  speaking.  Limited 

research has been carried out to detect the real-time presence of stress during a stressful event like public 
speaking.  This  is  likely  due  to  the  complexities  and  limitations  involved  in  creating  a  suitable  environment. 

Exponential  technologies  such  as  VR  can  help  provide  an  immersive  environment  for  this  very  purpose. 
Therefore, the detection of stress from the voice should be explored by developing a stress-detection model 

and implementing it in VR. However, the potential barriers include analysis of prosodic features of voice and 
especially real-time analysis of physiological parameters of stress during public speaking in VR. The issues can 

be overcome by either analysing physiological signals within the VR application or by using another application 
and then importing the physiological data into the VR application. This review has its limitations such as in terms 

of limited number of articles surveyed. However, it shows that there are opportunities for integrating real-time 
audio and voice analysis within VR, which can help detect stress during public speaking and, ultimately, provide 

a valid training tool to improve public speaking skills by the suggested idea. 

26 

 
 
 
 
 
 
REFERENCES 

Agarwalla, S. and Sarma, K.K. 2016. Machine learning based sample extraction for automatic speech recognition using 

dialectal Assamese speech. Neural Networks 78, 97-111. 

Albornoz, E.M., Milone, D.H. and Rufiner, H.L. 2011. Spoken Emotion Recognition Using Hierarchical Classifiers. Computer 

Speech & Language 25, 25. 

American Psychological Association 2022. Anxiety. In APA Dictionary APA Website. 

American Psychological Association 2022. Emotion. In APA Dictionary, APA website. 

American Psychological Association 2022. Stress. In APA Dictionary APA Webiste. 

Anagnostopoulos, C.-N., Iliou, T. and Giannoukos, I. 2015. Features And Classifiers For Emotion Recognition From Speech: 

A Survey From 2000 To 2011. Artificial Intelligence Review 43, 155-177. 

Ang, J., Dhillon, R., Krupski, A., Shriberg, E. and Stolcke, A. 2002. Prosody-based automatic detection of annoyance and 

frustration in human-computer dialog. In INTERSPEECH Citeseer. 

Azwar, H., Alam, N., Kazmi, B., Zain-Ul-Abidin, S. and Khan, S.A. 2016. VIRTUAL REALITY BASED IMMERSION SYSTEMS. 

International Journal of Technology and Research 4, 82-84. 

Badshah, A.M., Rahim, N., Ullah, N., Ahmad, J., Muhammad, K., Lee, M.Y., Kwon, S. and Baik, S.W. 2019. Deep features-

based speech emotion recognition for smart affective services. Multimedia Tools and Applications 78, 5571-5589. 

Bandela, S.R. and Kumar, T.K. 2019. Speech emotion recognition using semi-NMF feature optimization. Turkish Journal 

of Electrical Engineering and Computer Sciences 27, 3741-3757. 

Banse,  R.  and  Scherer,  K.R.  1996.  Acoustic  Profiles  In  Vocal  Emotion  Expression.  Journal  of  personality  and  social 

psychology 70, 614. 

Barreto, A., Zhai, J. and Adjouadi, M. 2007.  Non-Intrusive Physiological Monitoring For Automated Stress Detection In 

Human-Computer Interaction. Springer. 

Batrinca, L., Stratou, G., Shapiro, A., Morency, L.-P. and Scherer, S. 2013. Cicero-towards a multimodal virtual audience 

platform for public speaking training. In International workshop on intelligent virtual agents Springer, 116-128. 

27 

 
Beatty, M.J. and Behnke, R.R. 1991. Effects of Public Speaking Trait Anxiety and Intensity of Speaking Task on Heart Rate 

During Performance. Human Communication Research 18, 147-176. 

Bengio, Y., Courville, A. and Vincent, P. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions 

on Pattern Analysis and Machine Intelligence 35, 1798-1828. 

Bhagat, K.K., Liou, W.-K. and Chang, C.-Y. 2016. A Cost-Effective Interactive 3d Virtual Reality System Applied To Military 

Live Firing Training. Virtual Reality 20, 127-140. 

Bhavan, A., Chauhan, P., Hitkul and Shah, R.R. 2019. Bagged Support Vector Machines For Emotion Recognition From 

Speech. Knowledge-Based Systems 184, 104886. 

Bitouk,  D.,  Verma,  R.  and  Nenkova,  A.  2010.  Class-Level  Spectral  Features  For  Emotion  Recognition.  Speech 

Communication 52, 613-625. 

Bouchlaghem, D., Shang, H., Whyte, J. and Ganah, A. 2005. Visualisation in architecture, engineering and construction 

(AEC). Automation in Construction 14, 287-295. 

Brundage, S.B. and Hancock, A.B. 2015. Real Enough: Using Virtual Public Speaking Environments To Evoke Feelings And 

Behaviors Targeted In Stuttering Assessment And Treatment. American journal of speech-language pathology 24, 139-

149. 

Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F. and Weiss, B. 2005. A database of German emotional speech. In 

Interspeech, 1517-1520. 

Busso,  C.,  Bulut,  M.,  Lee,  C.-C.,  Kazemzadeh,  A.,  Mower,  E.,  Kim,  S.,  Chang,  J.N.,  Lee,  S.  and  Narayanan,  S.S.  2008. 

IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation 42, 335-359. 

Busso, C., Lee, S. and Narayanan, S. 2009. Analysis of Emotionally Salient Aspects of Fundamental Frequency for Emotion 

Detection. IEEE transactions on audio, speech, and language processing 17, 582-596. 

Carl, E., Stein, A.T., Levihn-Coon, A., Pogue, J.R., Rothbaum, B., Emmelkamp, P., Asmundson, G.J.G., Carlbring, P., Powers, 

M.B., Stockholms, U., Samhällsvetenskapliga, F., Psykologiska, I. and Klinisk, P. 2019. Virtual Reality Exposure Therapy For 

Anxiety And Related Disorders: A Meta-Analysis Of Randomized Controlled Trials. Journal of Anxiety Disorders 61, 27-36. 

Casale, S., Russo, A. and Serrano, S. 2007. Multistyle classification of speech under stress using feature subset selection 

based on genetic algorithms. Speech Communication 49, 801-810. 

28 

Chittaro,  L.  and  Sioni,  R.  2015.  Serious  Games For  Emergency  Preparedness:  Evaluation  Of  An  Interactive  Vs.  A  Non-

Interactive Simulation Of A Terror Attack. Computers in Human Behavior 50, 508-519. 

Chollet,  M.,  Stefanov,  K.,  Prendinger,  H.  and  Scherer,  S.  2015.  Public  speaking  training  with  a  multimodal  interactive 

virtual audience framework - Demonstration, 367-368. 

Chollet, M., Wörtwein, T., Morency, L.-P., Shapiro, A. and Scherer, S. 2015. Exploring Feedback Strategies To Improve 

Public  Speaking:  An  Interactive  Virtual  Audience  Framework.  In  Proceedings  of  the  Proceedings  of  the  2015  ACM 

International  Joint  Conference  on  Pervasive  and  Ubiquitous  Computing,  Osaka,  Japan2015  Association  for  Computing 

Machinery, 1143–1154. 

Clinton,  E.,  Cookson,  G.,  Brown,  S.  and  Woods,  D.  2017.  The  Stress  of  Public  Speaking  Increases  Cortisol  Levels  in 

Undergraduates: Is increased Preparation Really the Best Remedy? 

Cowen, A.S., Elfenbein, H.A., Laukka, P. and Keltner, D. 2019. Mapping 24 emotions conveyed by brief human vocalization. 

American Psychologist 74, 698. 

Cox, T. and Mackay, C. 1985. The measurement of self‐reported stress and arousal. British journal of psychology 76, 183-

186. 

Crocetta, T.B., De Araújo, L.V., Guarnieri, R., Massetti, T., Ferreira, F.H.I.B., De Abreu, L.C. and De Mello Monteiro, C.B. 

2018. Virtual Reality Software Package For Implementing Motor Learning And Rehabilitation Experiments. Virtual Reality 

22, 199-209. 

Dahl,  G.E.,  Yu,  D.,  Deng,  L.  and  Acero,  A.  2011.  Context-Dependent  Pre-Trained  Deep  Neural  Networks  For  Large-

Vocabulary Speech Recognition. IEEE transactions on audio, speech, and language processing 20, 30-42. 

Darwin, C. and Prodger, P. 1998. The Expression Of The Emotions In Man And Animals. Oxford University Press, USA. 

Dascalu, M.-I., Bagis, S., Nitu, M., Ferche, O.-M. and Alin Dragos Bogdan, M. 2017. Experiential Learning VR System for 

Studying Computer Architecture. Romanian Journal of Human - Computer Interaction 10, 197-215. 

Dechant, M., Trimpl, S., Wolff, C., Mühlberger, A. and Shiban, Y. 2017. Potential Of Virtual Reality As A Diagnostic Tool 

For Social Anxiety: A Pilot Study. Computers in Human Behavior 76, 128-134. 

29 

Deng,  J.,  Zhang,  Z.,  Marchi,  E.  and  Schuller,  B.  2013.  Sparse  autoencoder-based  feature  transfer  learning  for  speech 

emotion recognition. In  2013 humaine association conference on affective computing and intelligent interaction IEEE, 

511-516. 

Dillon, R. 2001. Extracting audio cues in real time to understand musical expressiveness. In Proceedings “Current research 

directions in computer music”, MOSART Workshop, Barcelona, Spain, 41-44. 

Dillon, R. 2003. A statistical approach to expressive intention  recognition in violin performances. In Proceedings of the 

Stockholm Music Acoustics Conference (SMAC’03) Stockholm, 529-532. 

Dillon,  R.,  Wong,  G.  and  Ang,  R.  2006.  Virtual  orchestra:  An  immersive  computer  game  for  fun  and  education.  In 

Proceedings of the 2006 international conference on Game research and development, 215-218. 

Docan-Morgan, T. and Nelson, L.L. 2015. Chapter 11: The Benefits and Necessity of Public Speaking Education  – Tony 

Docan-Morgan & Laura L. Nelson. In Public Speaking for the Curious. 

Dongrui, W., Courtney, C.G., Lance, B.J., Narayanan, S.S., Dawson, M.E., Oie, K.S. and Parsons, T.D. 2010. Optimal Arousal 

Identification  and  Classification  for  Affective  Computing  Using  Physiological  Signals:  Virtual  Reality  Stroop  Task.  IEEE 

Transactions on affective computing 1, 109-118. 

Douglas-Cowie,  E.,  Cowie,  R.,  Sneddon,  I.,  Cox,  C.,  Lowry,  O.,  Mcrorie,  M.,  Martin,  J.-C.,  Devillers,  L.,  Abrilian,  S.  and 

Batliner,  A.  2007.  The  HUMAINE  database:  Addressing  the  collection  and  annotation  of  naturalistic  and  induced 

emotional data. In International conference on affective computing and intelligent interaction Springer, 488-500. 

Dowd, H., Dowd, H., Zautra, A., Zautra, A., Hogan, M. and Hogan, M. 2010. Emotion, Stress, and Cardiovascular Response: 

An Experimental Test of Models of Positive and Negative Affect. International Journal of Behavioral Medicine 17, 189-

194. 

Droppleman, L.F. and Mcnair, D.M. 1971. An experimental analog of public speaking.  Journal of consulting and clinical 

psychology 36, 91. 

El-Yamri, M., Romero-Hernandez, A., Gonzalez-Riojo, M. and Manero, B. 2019. Designing a VR game for public speaking 

based on speakers features: a case study. Smart Learning Environments 6, 1-15. 

El Ayadi, M., Kamel, M.S. and Karray, F. 2011. Survey on speech emotion recognition: Features, classification schemes, 

and databases. Pattern Recognition 44, 572-587. 

30 

Elfering, A. and Grebner, S. 2011. Ambulatory Assessment of Skin Conductivity During First Thesis Presentation: Lower 

Self-Confidence Predicts Prolonged Stress Response. Applied Psychophysiology and Biofeedback 36, 93-99. 

Elfering, A. and Grebner, S. 2012. Getting Used to Academic Public Speaking: Global Self-Esteem Predicts Habituation in 

Blood Pressure Response to Repeated Thesis Presentations. Applied Psychophysiology and Biofeedback 37, 109-120. 

Feldman, P.J., Cohen, S., Hamrick, N. and Lepore, S.J. 2004. Psychological Stress, Appraisal, Emotion And Cardiovascular 

Response In A Public Speaking Task. Psychology & Health 19, 353-368. 

Fredrick,  S.S.,  Demaray,  M.K.,  Malecki,  C.K.  and  Dorio,  N.B.  2018.  Can  social  support  buffer  the  association between 

depression and suicidal ideation in adolescent boys and girls? Psychology in the Schools 55, 490-505. 

Fruchter, R., Reidsma, D., Op Den Akker, H.J.A., Nishida, T., Rienks, R.J., Rosenberg, D., Poppe, R.W., Nijholt, A., Heylen, 

D.K.J. and Zwiers, J. 2007. Virtual Meeting Rooms: From Observation to Simulation. AI & society 22, 133-144. 

Gandhi, R. 2018. Support Vector Machine — Introduction to Machine Learning Algorithms. 

Garcia-Palacios, A., Hoffman, H., Carlin, A., Furness, T.A. and Botella, C. 2002. Virtual Reality In The Treatment Of Spider 

Phobia: A Controlled Study. Behaviour research and therapy 40, 983-993. 

Gavish, N., Gutiérrez, T., Webel, S., Rodríguez, J., Peveri, M., Bockholt, U. and Tecchia, F. 2015. Evaluating Virtual Reality 

And Augmented Reality Training For Industrial Maintenance And Assembly Tasks. Interactive Learning Environments 23, 

778-798. 

Gebara,  C.M.,  Barros-Neto,  T.P.D.,  Gertsenchtein,  L.  and  Lotufo-Neto,  F.  2016.  Virtual  Reality  Exposure  Using  Three-

Dimensional Images For The Treatment Of Social Phobia. Revista brasileira de psiquiatria (Sao Paulo, Brazil : 1999) 38, 

24-29. 

Georgiades,  A.,  Sherwood,  A.,  Gullette,  E.C.D.,  Babyak,  M.A.,  Hinderliter,  A.,  Waugh,  R.,  Tweedy,  D.,  Craighead,  L., 

Bloomer, R. and Blumenthal, J.A. 2000. Effects of Exercise and Weight Loss on Mental Stress–Induced Cardiovascular 

Responses in Individuals With High Blood Pressure. Hypertension: Journal of the American Heart Association 36, 171-176. 

Gerardi, M., Cukor, J., Difede, J., Rizzo, A. and Rothbaum, B.O. 2010. Virtual Reality Exposure Therapy for Post-Traumatic 

Stress Disorder and Other Anxiety Disorders. Current Psychiatry Reports 12, 298-305. 

Giannakopoulos,  T.  2015.  pyAudioAnalysis:  An  Open-Source  Python  Library  for  Audio  Signal  Analysis.  PloS  one  10, 

e0144610. 

31 

Gillespie, S., Moore, E., Laures-Gore, J., Farina, M., Russell, S., Logan, Y.Y. and Ieee 2017. Detecting Stress And Depression 

In Adults With Aphasia Through Speech Analysis. In 2017 Ieee International Conference on Acoustics, Speech and Signal 

Processing, 5140-5144. 

Gramer, M. and Saria, K. 2007. Effects Of Social Anxiety And Evaluative Threat On Cardiovascular Responses To Active 

Performance Situations. Biological Psychology 74, 67-74. 

Gramer, M. and Sprintschnik, E. 2008. Social Anxiety And Cardiovascular Responses To An Evaluative Speaking Task: The 

Role Of Stressor Anticipation. Personality and Individual Differences 44, 371-381. 

Hamid, O.K. 2018. Frame blocking and windowing speech signal. Journal of Information, Communication, and Intelligence 

Systems (JICIS) 4, 87-94. 

Han,  H.,  Byun,  K.  and  Kang,  H.-G.  2018.  A  Deep  Learning-based  Stress  Detection  Algorithm  with  Speech  Signal.  In 

International Multimedia Conference ACM, 11-15. 

Han, K., Yu, D. and Tashev, I. 2014. Speech emotion recognition using deep neural network and extreme learning machine. 

In Interspeech 2014. 

Hansen, J.H., Bou-Ghazale, S.E., Sarikaya, R. and Pellom, B. 1997. Getting started with SUSAS: a speech under simulated 

and actual stress database. In Eurospeech, 1743-1746. 

Hansen,  J.H.L.  and  Liu,  G.  2016.  Unsupervised  Accent  Classification  For  Deep  Data  Fusion  Of  Accent  And  Language 

Information. Speech Communication 78, 19-33. 

Hao,  M.,  Cao,  W.-H.,  Liu,  Z.-T.,  Wu,  M.  and  Xiao,  P.  2020.  Visual-audio  emotion  recognition  based  on  multi-task  and 

ensemble learning with multiple features. Neurocomputing 391, 42-51. 

Hartanto, D., Kampmann, I.L., Morina, N., Emmelkamp, P.G.M.,  Neerincx, M.A. and Brinkman, W.P. 2014. Controlling 

Social Stress In Virtual Reality Environments. PloS one 9, e92804. 

Hilfert, T. and König, M. 2016. Low-Cost Virtual Reality Environment For Engineering And Construction. Visualization in 

Engineering 4, 1-18. 

Hu, H., Xu, M. and Wu, W. 2007. GMM Supervector Based SVM with Spectral Features for Speech Emotion Recognition. 

In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07, IV-413-IV-416. 

32 

Ibm 2020. Recurrent Neural Networks, https://www.ibm.com/cloud/learn/recurrent-neural-networks. 

Issa, D., Fatih Demirci, M. and Yazici, A. 2020. Speech  emotion recognition with deep convolutional neural networks. 

Biomedical signal processing and control 59, 101894. 

Jain, M., Narayan, S., Balaji, P., Bhowmick, A. and Muthu, R.K. 2020. Speech emotion recognition using support vector 

machine. arXiv preprint arXiv:2002.07590. 

Jezova, D., Hlavacova, N., Dicko, I., Solarikova, P. and Brezina, I. 2016. Psychosocial Stress Based On Public Speech In 

Humans: Is There A Real Life/Laboratory Setting Cross-Adaptation? Stress 19, 429-433. 

Jönsson, P., Wallergård, M., Österberg, K., Hansen, Å.M., Johansson, G. and Karlson, B. 2010. Cardiovascular and cortisol 

reactivity  and  habituation  to  a  virtual  reality  version  of  the  Trier  Social  Stress  Test:  A  pilot  study. 

Psychoneuroendocrinology 35, 1397-1403. 

Julião, M., Silva, J., Aguiar, A., Moniz, H. and Batista, F. 2015. Speech features for discriminating stress using branch and 

bound wrapper search. In International Symposium on Languages, Applications and Technologies Springer, 3-14. 

Junek,  W.  2007.  Mind  reading:  The  interactive  guide  to  emotions.  Journal  of  the  Canadian  Academy  of  Child  and 

Adolescent Psychiatry 16, 182. 

Juslin,  P.N.  and  Laukka,  P.  2003.  Communication  of  emotions  in  vocal  expression  and  music  performance:  different 

channels, same code? Psychol Bull 129, 770-814. 

Kaiser, J.F. 1993. Some useful properties of Teager's energy operators. In 1993 IEEE international conference on acoustics, 

speech, and signal processing IEEE, 149-152. 

Kamiloğlu, R.G., Fischer, A.H. and Sauter, D.A. 2020. Good vibrations: A review of vocal expressions of positive emotions. 

Psychonomic Bulletin & Review 27, 237-265. 

Kappas, A., Hess, U. and Scherer, K.R. 1991. Voice and emotion. Fundamentals of nonverbal behavior 200. 

Kassem, M., Benomran, L. and Teizer, J. 2017. Virtual Environments For Safety Learning In Construction And Engineering: 

Seeking Evidence And Identifying Gaps For Future Research. Visualization in Engineering 5, 1-15. 

Kim, K., Kim, C.-H., Kim, S.-Y., Roh, D. and Kim, S.I. 2009. Virtual Reality for Obsessive-Compulsive Disorder: Past and the 

Future. Psychiatry Investigation 6, 115-121. 

33 

Kimani, E. and Bickmore, T. 2019. Addressing Public Speaking Anxiety in Real-time Using a Virtual Public Speaking Coach 

and Physiological Sensors. In  Proceedings of the Proceedings of the 19th ACM International Conference on Intelligent 

Virtual Agents, Paris, France2019 Association for Computing Machinery, 260–263. 

Koolagudi, S.G., Maity, S.,  Kumar, V.A., Chakrabarti, S. and Rao, K.S. 2009. IITKGP-SESC: speech database for emotion 

analysis. In International conference on contemporary computing Springer, 485-492. 

Koolagudi,  S.G.  and  Rao,  K.S.  2012.  Emotion  Recognition  From  Speech:  A  Review.  International  journal  of  speech 

technology 15, 99-117. 

Koroleva, M., Bakhchina, A., Shyshalov, I., Parin, S.B. and Polevaia, S.A. 2014. Influence Of The Context Of Public Speaking 

On Human Functional State. International Journal of Psychophysiology 94, 230-231. 

Kothgassner, O.D., Felnhofer, A., Hlavacs, H., Beutl, L., Palme, R., Kryspin-Exner, I. and Glenk, L.M. 2016. Salivary Cortisol 

And Cardiovascular Reactivity To A Public Speaking Task In A Virtual And Real-Life Environment. Computers in Human 

Behavior 62, 124-135. 

Kothgassner,  O.D.,  Goreis,  A.,  Kafka,  J.X.,  Kaufmann,  M.,  Atteneder,  K.,  Beutl,  L.,  Hennig-Fast,  K.,  Hlavacs,  H.  and 

Felnhofer, A. 2019. Virtual Social Support Buffers Stress Response: An Experimental Comparison Of Real-Life And Virtual 

Support Prior To A Social Stressor. Journal of behavior therapy and experimental psychiatry 63, 57-65. 

Kotlyar,  M.,  Donahue,  C.,  Thuras,  P.,  Kushner,  M.G.,  O'gorman,  N.,  Smith,  E.A.  and  Adson,  D.E.  2008.  Physiological 

Response To A Speech Stressor Presented In A Virtual Reality Environment. Psychophysiology 45, 1034-1037. 

Kuchibhotla,  S.,  Vankayalapati,  H.D.,  Vaddi,  R.  and  Anne,  K.R.  2014.  A  comparative  analysis  of  classifiers  in  emotion 

recognition through acoustic features. International journal of speech technology 17, 401-408. 

Kwon, O.-W., Chan, K., Hao, J. and Lee, T.-W. 2003. Emotion recognition by speech signals. In Eighth European conference 

on speech communication and technology. 

Laver, K.E., George, S., Thomas, S., Deutsch, J.E. and Crotty, M. 2015. Virtual reality for stroke rehabilitation. The Cochrane 

database of systematic reviews, CD008349. 

Le, Q.T., Le, Q.T., Pedro, A., Pedro, A., Park, C.S. and Park, C.S. 2015. A Social Virtual Reality Based Construction Safety 

Education System for Experiential Learning. Journal of Intelligent & Robotic Systems 79, 487-506. 

34 

Levy, F., Leboucher, P., Rautureau, G. and Jouvent, R. 2016. E-Virtual Reality Exposure Therapy In Acrophobia: A Pilot 

Study. Journal of Telemedicine and Telecare 22, 215-220. 

Li, X., Tao, J., Johnson, M.T., Soltis, J., Savage, A., Leong, K.M. and Newman, J.D. 2007. Stress and emotion classification 

using  jitter  and shimmer  features.  In  2007  IEEE  International  Conference  on  Acoustics,  Speech  and  Signal  Processing-

ICASSP'07 IEEE, IV-1081-IV-1084. 

Lin, Y.-L. and Wei, G. 2005. Speech emotion recognition based on HMM and SVM. In 2005 international conference on 

machine learning and cybernetics IEEE, 4898-4901. 

Lindner, P., Miloff, A., Fagernäs, S., Andersen, J., Sigeman, M., Andersson, G., Furmark, T., Carlbring, P., Linköpings, U., 

Institutionen För Beteendevetenskap Och, L., Filosofiska, F. and Psykologi 2019. Therapist-led and self-led one-session 

virtual  reality  exposure  therapy  for  public  speaking  anxiety  with  consumer  hardware  and  software:  A  randomized 

controlled trial. Journal of Anxiety Disorders 61, 45-54. 

Liscombe, J., Riccardi, G. and Hakkani-Tur, D. 2005. Using context to improve emotion detection in spoken dialog systems. 

Lister, H.A., Piercey, C.D. and Joordens, C. 2010. The effectiveness of 3-D video virtual reality for the treatment of fear of 

public speaking. Journal of CyberTherapy and Rehabilitation 3, 375. 

Livingstone, S.R. and Russo, F.A. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A 

dynamic, multimodal set of facial and vocal expressions in North American English. PloS one 13, e0196391. 

Low, L.-S.A., Maddage, N.C., Lech, M., Sheeber, L.B. and Allen, N.B. 2010. Detection Of Clinical Depression In Adolescents’ 

Speech During Family Interactions. IEEE Transactions on Biomedical Engineering 58, 574-586. 

Lugger, M. and Yang, B. 2007. The relevance of voice quality features in speaker independent emotion recognition. In 

2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07 IEEE, IV-17-IV-20. 

Manju, T., Padmavathi, S. and Tamilselvi, D. 2017. A Rehabilitation Therapy For Autism Spectrum Disorder Using Virtual 

Reality. In International Conference On Intelligent Information Technologies Springer, 328-336. 

Marsella, S., Xu, Y., Lhommet, M., Feng, A., Scherer, S. and Shapiro, A. 2013. Virtual character performance from speech. 

In Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 25-35. 

Martin, O., Kotsia, I., Macq, B. and Pitas, I. 2006. The eNTERFACE'05 audio-visual emotion database. In 22nd International 

Conference on Data Engineering Workshops (ICDEW'06) IEEE, 8-8. 

35 

Martin., D.J.J.H. 2021. Hidden Markov Models. In Speech and Language Processing. 

Maskey, M., Lowry, J., Rodgers, J., Mcconachie, H. and Parr, J.R. 2014. Reducing specific phobia/fear in young people with 

autism spectrum disorders (ASDs) through a virtual reality environment intervention. PloS one 9, e100374. 

Mcfee, B., Raffel, C., Liang, D., Ellis, D.P., Mcvicar, M., Battenberg, E. and Nieto, O. 2015. librosa: Audio and music signal 

analysis in python. In Proceedings of the 14th python in science conference, 18-25. 

Miloff, A., Lindner, P., Hamilton, W., Reuterskiöld, L., Andersson, G., Carlbring, P., Stockholms, U., Samhällsvetenskapliga, 

F.,  Psykologiska,  I.  and  Klinisk,  P.  2016.  Single-session  gamified  virtual  reality  exposure  therapy  for  spider  phobia  vs. 

traditional exposure therapy: study protocol for a randomized controlled non-inferiority trial. Trials 17, 60. 

Mitchell, R.L. and Ross, E.D. 2013. Attitudinal Prosody: What We Know And Directions For Future Study. Neuroscience & 

Biobehavioral Reviews 37, 471-479. 

Molina,  K.I.,  Ricci,  N.A.,  De  Moraes,  S.A.  and  Perracini,  M.R.  2014.  Virtual  reality  using  games  for  improving  physical 

functioning in older adults: a systematic review. Journal of NeuroEngineering and Rehabilitation 11, 156. 

Motraghi, T.E., Seim, R.W., Meyer, E.C. and Morissette, S.B. 2014. Virtual Reality Exposure Therapy for the Treatment of 

Posttraumatic Stress Disorder: A Methodological Review Using CONSORT Guidelines. Journal of Clinical Psychology 70, 

197-208. 

Mujber, T.S., Szecsi, T. and Hashmi, M.S. 2004. Virtual Reality Applications In Manufacturing Process Simulation. Journal 

of materials processing technology 155, 1834-1838. 

Neiberg, D., Elenius, K., Laskowski, K. and Isca 2006. Emotion Recognition in Spontaneous Speech Using GMMs. 

Nijholt, A. 2014. Breaking Fresh Ground in Human-Media Interaction Research. Frontiers in ICT 2014. 

Nogueiras,  A.,  Moreno,  A.,  Bonafonte,  A.  and  Mariño,  J.B.  2001.  Speech  emotion  recognition  using  hidden  Markov 

models. In Seventh European Conference on Speech Communication and Technology. 

North,  M.M.,  North, S.M.  and  Coble,  J.R.  1998.  Virtual  Reality  Therapy:  an  Effective  Treatment  for  the  Fear of  Public 

Speaking. International Journal of Virtual Reality 3, 1-6. 

36 

Nwe, T.L., Foo, S.W. and De Silva, L.C. 2003. Detection of stress and emotion in speech using traditional and FFT based 

log energy features. In Fourth International Conference on Information, Communications and Signal Processing, 2003 and 

the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint IEEE, 1619-1623. 

Ooi, C.S., Seng, K.P., Ang, L.-M. and Chew, L.W. 2014. A New Approach Of Audio Emotion Recognition.  Expert Systems 

with Applications 41, 5858-5869. 

Opriş, D., Pintea, S., García‐Palacios, A., Botella, C., Szamosközi, Ş. and David, D. 2012. Virtual reality exposure therapy in 

anxiety disorders: a quantitative meta‐analysis. Depression and Anxiety 29, 85-93. 

Ordaz,  N.,  Romero,  D.,  Gorecky,  D.  and  Siller,  H.R.  2015.  Serious  Games  and  Virtual  Simulator  for  Automotive 

Manufacturing Education & Training. Procedia Computer Science 75, 267-274. 

Owens, M.E., Owens, M.E., Beidel, D.C. and Beidel, D.C. 2015. Can Virtual Reality Effectively Elicit Distress Associated with 

Social Anxiety Disorder? Journal of Psychopathology and Behavioral Assessment 37, 296-305. 

Palo, H., Mohanty, M.N. and Chandra, M. 2015. Design of neural network model for emotional speech recognition. In 

Artificial intelligence and evolutionary algorithms in engineering systems Springer, 291-300. 

Pan, X., Gillies, M., Barker, C., Clark, D.M. and Slater, M. 2012. Socially anxious and confident men interact with a forward 

virtual woman: an experimental study. PloS one 7, e32931. 

Parsons,  T.D.  and  Rizzo,  A.A.  2008.  Affective  Outcomes  Of  Virtual  Reality  Exposure  Therapy  For  Anxiety  And  Specific 

Phobias: A Meta-Analysis. Journal of behavior therapy and experimental psychiatry 39, 250-261. 

Pedro, A., Le, Q.T. and Park, C.S. 2016. Framework for Integrating Safety into Construction Methods Education through 

Interactive Virtual Reality. Journal of Professional Issues in Engineering Education and Practice 142, 4015011. 

Pellett, K. and Zaidi, S.F.M. 2019. A Framework for Virtual Reality Training to Improve Public Speaking. In Proceedings of 

the 25th ACM Symposium on Virtual Reality Software and Technology, Parramatta, NSW, Australia2019 Association for 

Computing Machinery, Article 44. 

Pertaub, D., Slater, M. and Barker, C. 2001. An Experiment On Fear Of Public Speaking In Virtual Reality. Studies in health 

technology and informatics, 372-378. 

Pfister, T. and Robinson, P. 2011. Real-Time Recognition of Affective States from Nonverbal Features of Speech and Its 

Application for Public Speaking Skill Analysis. IEEE Transactions on affective computing 2, 66-78. 

37 

Pisanski, K., Nowak, J. and Sorokowski, P. 2016. Individual Differences In Cortisol Stress Response Predict Increases In 

Voice Pitch During Exam Stress. Physiology & Behavior 163, 234-238. 

Poorna, S.S., Anuraj, K. and Nair, G.J. 2018. A Weight Based Approach for Emotion Recognition from Speech: An Analysis 

Using South Indian Languages. In Soft Computing Systems, Icscs 2018, I. Zelinka, R. Senkerik, G. Panda and P.S.L. Kanthan 

Eds., 14-24. 

Powers, M.B., Briceno, N.F., Gresham, R., Jouriles, E.N., Emmelkamp, P.M.G. and Smits, J.a.J. 2013. Do conversations with 

virtual avatars increase feelings of social anxiety?? Journal of Anxiety Disorders 27, 398-403. 

Přibil, J. and Přibilová, A. 2013. Evaluation Of Influence Of Spectral And Prosodic Features On Gmm Classification Of Czech 

And Slovak Emotional Speech. EURASIP Journal on Audio, Speech, and Music Processing 2013, 1-22. 

Rao, K.S., Rao, K.S., Koolagudi, S.G., Koolagudi, S.G., Vempada, R.R. and Vempada, R.R. 2013. Emotion Recognition From 

Speech Using Global And Local Prosodic Features. International journal of speech technology 16, 143-160. 

Rong,  J.,  Li,  G.  and  Chen,  Y.-P.P.  2009.  Acoustic  Feature  Selection  For  Automatic  Emotion  Recognition  From  Speech. 

Information Processing and Management 45, 315-328. 

Ruscio, A.M., Brown, T.A., Chiu, W.T., Sareen, J., Stein, M.B. and Kessler, R.C. 2008. Social Fears And Social Phobia In The 

USA: Results From The National Comorbidity Survey Replication. Psychological medicine 38, 15-28. 

Ryerson  Multimedia 

Research 

Lab., 

R.U. 

2017. 

RML 

Emotion  Database 

R.M.R. 

Lab. 

Ed., 

http://shachi.org/resources/4965. 

Sanchez, M.H., Tur, G., Ferrer, L. and Hakkani-Tür, D. 2010. Domain adaptation and compensation for emotion detection. 

In Eleventh Annual Conference of the International Speech Communication Association. 

Scherer, K.R. 1995. Expression of Emotion in Voice and Music. Journal of Voice. 

Scherer, K.R. 1995. Expression of emotion in voice and music. Journal of Voice 9, 235-248. 

Scherer, K.R., Banse, R., Wallbott, H.G. and Goldbeck, T. 1991. Vocal Cues In Emotion Encoding And Decoding. Motivation 

and Emotion 15, 123-148. 

Schmid  Mast,  M.,  Kleinlogel,  E.P.,  Tur,  B.  and  Bachmann,  M.  2018.  The  Future  Of  Interpersonal  Skills  Development: 

Immersive Virtual Reality Training With Virtual Humans. Human Resource Development Quarterly 29, 125-141. 

38 

 
Schneider, J., Börner, D., Rosmalen,  P.V. and Specht, M. 2015. Presentation Trainer, your Public Speaking Multimodal 

Coach.  In  Proceedings  of  the  Proceedings  of  the  2015  ACM  on  International  Conference  on  Multimodal  Interaction, 

Seattle, Washington, USA2015 Association for Computing Machinery, 539–546. 

Schreiber, L. 2011. Informative Speaking. The Public Speaking Project (Ed.). Public speaking. The virtual text, 15-11. 

Schuller, B., Batliner, A., Steidl, S. and Seppi, D. 2011. Recognising realistic emotions and affect in speech: State of the art 

and lessons learnt from the first challenge. Speech Communication 53, 1062-1087. 

Schuller, B., Rigoll, G. and Lang, M. 2003. Hidden Markov Model-based Speech Emotion Recognition. 

Schuller,  B.,  Rigoll,  G.  and  Lang,  M.  2003.  Hmm-Based  Music  Retrieval  Using  Stereophonic  Feature  Information  And 

Framelength  Adaptation.  In  2003  International  Conference  on  Multimedia  and  Expo.  ICME'03.  Proceedings  (Cat.  No. 

03TH8698) IEEE, II-713. 

Schuller, B., Seppi, D., Batliner, A., Maier, A. and Steidl, S. 2007. Towards more reality in the recognition of emotional 

speech. In 2007 IEEE international conference on acoustics, speech and signal processing-ICASSP'07 IEEE, IV-941-IV-944. 

Scikit Learn 2022. Support Vector Machines. 

Semwal, N., Kumar, A. and Narayanan, S. 2017. Automatic Speech Emotion Detection System using Multi-domain Acoustic 

Feature Selection and Classification Models. 

Shabahi, S. 2020. my-voice-analysis 0.7, https://pypi.org/project/my-voice-analysis/. 

Slater, M., Pertaub, D.-P., Barker, C. and Clark, D.M. 2006. An Experimental Study On Fear Of Public Speaking Using A 

Virtual Environment. Cyberpsychology & behavior 9, 627-633. 

Slater,  M.,  Pertaub,  D.-P.  and  Steed,  A.  1999.  Public  Speaking  In  Virtual  Reality:  Facing  An  Audience  Of  Avatars.  IEEE 

Computer Graphics and Applications 19, 6-9. 

Soltani, K. and Ainon, R.N. 2007. Speech emotion detection based on neural networks. 

Sun,  R.  and  Moore,  E.  2011.  Investigating  glottal  parameters  and  teager  energy  operators  in  emotion  recognition.  In 

International Conference on Affective Computing and Intelligent Interaction Springer, 425-434. 

39 

Takac,  M.,  Collett,  J.,  Blom,  K.J.,  Conduit,  R.,  Rehm,  I.  and  De  Foe,  A.  2019.  Public  speaking  anxiety  decreases  within 

repeated virtual reality training sessions. PloS one 14, e0216288. 

Tanveer, M.I., Lin, E. and Hoque, M. 2015. Rhema: A Real-Time In-Situ Intelligent Interface to Help People with Public 

Speaking. In Proceedings of the Proceedings of the 20th International Conference on Intelligent User Interfaces, Atlanta, 

Georgia, USA2015 Association for Computing Machinery, 286–295. 

Tashev, I.J., Wang, Z.Q., Godin, K. and Ieee 2017. Speech Emotion Recognition based on Gaussian Mixture Models and 

Deep Neural Networks. In Proceedings of the 2017 Information Theory and Applications Workshop2017. 

Teoh, A.N. and Hilmert, C. 2018. Social support as a comfort or an encouragement: A systematic review on the contrasting 

effects of social support on cardiovascular reactivity. British Journal of Health Psychology 23, 1040-1065. 

The Endnote Team 2013. EndNote Clarivate, Philadelphia, PA. 

Thorsteinsson, E.B. and James, J.E. 1999. A Meta-analysis of the effects of experimental manipulations of social support 

during laboratory stress. Psychology & Health 14, 869-886. 

Ticknor,  B.  2018.  Using  Virtual  Reality  to  Treat  Offenders:  An  Examination.  International  Journal  of  Criminal  Justice 

Sciences 13, 316-325. 

Ticknor, B. 2019. Virtual Reality and Correctional Rehabilitation: A Game Changer. Criminal Justice and Behavior 46, 1319-

1336. 

Truong,  K.P.  and  Van  Leeuwen,  D.A.  2007.  Automatic  Discrimination  Between  Laughter  And  Speech.  Speech 

Communication 49, 144-158. 

Truong,  K.P.,  Van  Leeuwen,  D.A.  and  De  Jong,  F.M.  2012.  Speech-based  recognition  of  self-reported  and  observed 

emotion in a dimensional space. Speech Communication 54, 1049-1063. 

Uchino,  B.N.  and  Garvey,  T.S.  1997.  The  Availability  of  Social  Support  Reduces  Cardiovascular  Reactivity  to  Acute 

Psychological Stress. Journal of Behavioral Medicine 20, 15-27. 

Valmaggia, L.R., Latif, L., Kempton, M.J. and Rus-Calafell, M. 2016. Virtual reality in the psychological treatment for mental 

health problems: An systematic review of recent evidence. Psychiatry Research 236, 189-195. 

Vanderplas, J. 2016. Python Data Science Handbook. In Python Data Science Handbook O'Reilly Media, Inc. 

40 

Wallbott, H.G. and Scherer, K.R. 1986. How Universal And Specific Is Emotional Experience? Evidence From 27 Countries 

On Five Continents. Information (International Social Science Council) 25, 763-795. 

Wang,  K.,  An,  N.,  Li,  B.N.,  Zhang,  Y.  and  Li,  L.  2015.  Speech  Emotion  Recognition  Using  Fourier  Parameters.  IEEE 

Transactions on affective computing 6, 69-75. 

Westenberg, P.M., Bokhorst, C.L., Miers, A.C., Sumter, S.R., Kallen, V.L., Van Pelt, J. and Blöte, A.W. 2009. A Prepared 

Speech In Front Of A Pre-Recorded Audience: Subjective, Physiological, And Neuroendocrine Responses To The Leiden 

Public Speaking Task. Biological Psychology 82, 116-124. 

Wiemers,  U.S.,  Schultheiss,  O.C.  and  Wolf,  O.T.  2015.  Public  speaking  in  front  of  an  unreceptive  audience  increases 

implicit power motivation and its endocrine arousal signature. Hormones and Behavior 71, 69-74. 

Wirtz, P.H., Ehlert, U., Kottwitz, M.U., La Marca, R. and Semmer, N.K. 2013. Occupational Role Stress is Associated With 

Higher Cortisol Reactivity to Acute Stress. Journal of Occupational Health Psychology 18, 121-131. 

Wörtwein, T., Chollet, M., Schauerte, B., Morency, L.-P., Stiefelhagen, R. and Scherer, S. 2015. Multimodal Public Speaking 

Performance Assessment. In Proceedings of the Proceedings of the 2015 ACM on International Conference on Multimodal 

Interaction, Seattle, Washington, USA2015 Association for Computing Machinery, 43–50. 

Zeng, Y., Mao, H.,  Peng, D. and Yi, Z. 2019. Spectrogram Based Multi-Task Audio Classification.  Multimedia Tools and 

Applications 78, 3705-3722. 

Zhou,  G.,  Hansen,  J.H.  and  Kaiser,  J.F.  2001.  Nonlinear  feature  based  classification  of  speech  under  stress.  IEEE 

Transactions on speech and audio processing 9, 201-216. 

41 

 
