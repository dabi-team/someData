MediaPipe Hands: On-device Real-time Hand Tracking

Fan Zhang Valentin Bazarevsky Andrey Vakunov
Andrei Tkachenka George Sung Chuo-Ling Chang Matthias Grundmann
Google Research
1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA
{zhafang, valik, vakunov, atkach, gsung, chuoling, grundman}@google.com

0
2
0
2

n
u
J

8
1

]

V
C
.
s
c
[

1
v
4
1
2
0
1
.
6
0
0
2
:
v
i
X
r
a

Abstract

We present a real-time on-device hand tracking solution
that predicts a hand skeleton of a human from a single
RGB camera for AR/VR applications. Our pipeline con-
sists of two models: 1) a palm detector, that is providing
a bounding box of a hand to, 2) a hand landmark model,
that is predicting the hand skeleton. It is implemented via
MediaPipe[12], a framework for building cross-platform
ML solutions. The proposed model and pipeline architec-
ture demonstrate real-time inference speed on mobile GPUs
with high prediction quality. MediaPipe Hands is open
sourced at https://mediapipe.dev.

1. Introduction

Hand tracking is a vital component to provide a natu-
ral way for interaction and communication in AR/VR, and
has been an active research topic in the industry [2] [15].
Vision-based hand pose estimation has been studied for
many years. A large portion of previous work requires
specialized hardware, e.g. depth sensors [13][16][17][3][4].
Other solutions are not lightweight enough to run real-time
on commodity mobile devices[5] and thus are limited to
platforms equipped with powerful processors. In this pa-
per, we propose a novel solution that does not require any
additional hardware and performs in real-time on mobile
devices. Our main contributions are:

• An efﬁcient two-stage hand tracking pipeline that can
track multiple hands in real-time on mobile devices.

• A hand pose estimation model that is capable of pre-

dicting 2.5D hand pose with only RGB input.

• And open source hand tracking pipeline as a ready-to-
go solution on a variety of platforms, including An-
droid, iOS, Web (Tensorﬂow.js[7]) and desktop PCs.

Figure 1: Rendered hand tracking result. (Left): Hand land-
marks with relative depth presented in different shades. The
lighter and larger the circle, the closer the landmark is to-
wards the camera. (Right): Real-time multi-hand tracking
on Pixel 3.

2. Architecture

Our hand tracking solution utilizes an ML pipeline con-

sisting of two models working together:

• A palm detector that operates on a full input image and
locates palms via an oriented hand bounding box.
• A hand landmark model that operates on the cropped
hand bounding box provided by the palm detector and
returns high-ﬁdelity 2.5D landmarks.

Providing the accurately cropped palm image to the hand
landmark model drastically reduces the need for data aug-
mentation (e.g. rotations, translation and scale) and allows
the network to dedicate most of its capacity towards land-
mark localization accuracy. In a real-time tracking scenario,
we derive a bounding box from the landmark prediction
of the previous frame as input for the current frame, thus
avoiding applying the detector on every frame. Instead, the
detector is only applied on the ﬁrst frame or when the hand
prediction indicates that the hand is lost.

1

 
 
 
 
 
 
Figure 3: Architecture of our hand landmark model. The
model has three outputs sharing a feature extractor. Each
head is trained by correspondent datasets marked in the
same color. See Section 2.2 for more detail.

Figure 2: Palm detector model architecture.

2.2. Hand Landmark Model

2.1. BlazePalm Detector

To detect initial hand locations, we employ a single-
shot detector model optimized for mobile real-time appli-
cation similar to BlazeFace[1], which is also available in
MediaPipe[12]. Detecting hands is a decidedly complex
task: our model has to work across a variety of hand sizes
with a large scale span (∼20x) and be able to detect oc-
cluded and self-occluded hands. Whereas faces have high
contrast patterns, e.g., around the eye and mouth region, the
lack of such features in hands makes it comparatively difﬁ-
cult to detect them reliably from their visual features alone.
Our solution addresses the above challenges using dif-

ferent strategies.

First, we train a palm detector instead of a hand detector,
since estimating bounding boxes of rigid objects like palms
and ﬁsts is signiﬁcantly simpler than detecting hands with
articulated ﬁngers.
In addition, as palms are smaller ob-
jects, the non-maximum suppression algorithm works well
even for the two-hand self-occlusion cases, like handshakes.
Moreover, palms can be modelled using only square bound-
ing boxes [11], ignoring other aspect ratios, and therefore
reducing the number of anchors by a factor of 3∼5.

Second, we use an encoder-decoder feature extractor
similar to FPN[9] for a larger scene-context awareness even
for small objects.

Lastly, we minimize the focal loss[10] during training
to support a large amount of anchors resulting from the
high scale variance. High-level palm detector architecture
is shown in Figure 2. We present an ablation study of our
design elements in Table 1.

After running palm detection over the whole image, our
subsequent hand landmark model performs precise land-
mark localization of 21 2.5D coordinates inside the detected
hand regions via regression. The model learns a consis-
tent internal hand pose representation and is robust even to
partially visible hands and self-occlusions. The model has
three outputs (see Figure 3):

1. 21 hand landmarks consisting of x, y, and relative

depth.

2. A hand ﬂag indicating the probability of hand presence

in the input image.

3. A binary classiﬁcation of handedness, e.g. left or right

hand.

We use the same topology as [14] for the 21 landmarks.
The 2D coordinates are learned from both real-world im-
ages as well as synthetic datasets as discussed below, with
the relative depth w.r.t. the wrist point being learned only
from synthetic images. To recover from tracking failure, we
developed another output of the model similar to [8] for pro-
ducing the probability of the event that a reasonably aligned
hand is indeed present in the provided crop. If the score is
lower than a threshold then the detector is triggered to reset
tracking. Handedness is another important attribute for ef-
fective interaction using hands in AR/VR. This is especially
useful for some applications where each hand is associated
with a unique functionality. Thus we developed a binary
classiﬁcation head to predict whether the input hand is the
left or right hand. Our setup targets real-time mobile GPU
inference, but we have also designed lighter and heavier ver-
sions of the model to address CPU inference on the mobile
devices lacking proper GPU support and higher accuracy
requirements of accuracy to run on desktop, respectively.

3. Dataset and Annotation

To obtain ground truth data, we created the following

datasets addressing different aspects of the problem:

• In-the-wild dataset: This dataset contains 6K images
of large variety, e.g. geographical diversity, various
lighting conditions and hand appearance. The limita-
tion of this dataset is that it doesn’t contain complex
articulation of hands.

• In-house collected gesture dataset: This dataset con-
tains 10K images that cover various angles of all phys-
ically possible hand gestures. The limitation of this
dataset is that it’s collected from only 30 people with
limited variation in background. The in-the-wild and
in-house dataset are great complements to each other
to improve robustness.

• Synthetic dataset: To even better cover the possi-
ble hand poses and provide additional supervision for
depth, we render a high-quality synthetic hand model
over various backgrounds and map it to the corre-
sponding 3D coordinates. We use a commercial 3D
hand model that is rigged with 24 bones and includes
36 blendshapes, which control ﬁngers and palm thick-
ness. The model also provides 5 textures with differ-
ent skin tones. We created video sequences of trans-
formation between hand poses and sampled 100K im-
ages from the videos. We rendered each pose with a
random high-dynamic-range lighting environment and
three different cameras. See Figure 4 for examples.

For the palm detector, we only use in-the-wild dataset,
which is sufﬁcient for localizing hands and offers the high-
est variety in appearance. However, all datasets are used for
training the hand landmark model. We annotate the real-
world images with 21 landmarks and use projected ground-
truth 3D joints for synthetic images. For hand presence, we
select a subset of real-world images as positive examples
and sample on the region excluding annotated hand regions
as negative examples. For handedness, we annotate a subset
of real-world images with handedness to provide such data.

4. Results

For the hand landmark model, our experiments show that
the combination of real-world and synthetic datasets pro-
vides the best results. See Table 2 for details. We evaluate
only on real-world images. Beyond the quality improve-
ment, training with a large synthetic dataset leads to less
jitter visually across frames. This observation leads us to
believe that our real-world dataset can be enlarged for bet-
ter generalization.

Our target is to achieve real-time performance on mobile
devices. We experimented with different model sizes and
found that the “Full” model (see Table 3) provides a good

Figure 4: Examples of our datasets. (Top): Annotated real-
world images. (Bottom): Rendered synthetic hand images
with ground truth annotation. See Section 3 for details.

Model Variation
No decoder + cross entropy loss
Decoder + cross entropy loss
Decoder + focal loss

Average Precision
86.22%
94.07%
95.7%

Table 1: Ablation study of palm detector design elements of
palm detector.

Dataset
Only real-world
Only synthetic
Combined

MSE normalized by palm size
16.1%
25.7%
13.4%

Table 2: Results of our model
datasets.

trained from different

trade-off between quality and speed. Increasing model ca-
pacity further introduces only minor improvements in qual-
ity but decreases signiﬁcantly in speed (see Table 3 for de-
tails). We use the TensorFlow Lite GPU backend for on-
device inference[6].

Model

Light
Full
Heavy

Params
(M)

1
1.98
4.02

MSE

11.83
10.05
9.817

Time(ms)
Pixel 3

Time(ms)
Samsung
S20

Time(ms)
iPhone11

6.6
16.1
36.9

5.6
11.1
25.8

1.1
5.3
7.5

Table 3: Hand landmark model performance characteristics.

5. Implementation in MediaPipe

With MediaPipe[12], our hand tracking pipeline can be
built as a directed graph of modular components, called Cal-
culators. Mediapipe comes with an extensible set of Calcu-
lators to solve tasks like model inference, media process-
ing, and data transformations across a wide variety of de-
vices and platforms. Individual Calculators like cropping,
rendering and neural network computations are further opti-
mized to utilize GPU acceleration. For example, we employ
TFLite GPU inference on most modern phones.

Our MediaPipe graph for hand tracking is shown in Fig-
ure 5. The graph consists of two subgraphs one for hand
detection and another for landmarks computation. One key
optimization MediaPipe provides is that the palm detector
only runs as needed (fairly infrequently), saving signiﬁcant
computation. We achieve this by deriving the hand location
in the current video frames from the computed hand land-
marks in the previous frame, eliminating the need to apply
the palm detector on every frame. For robustness, the hand
tracker model also outputs an additional scalar capturing the
conﬁdence that a hand is present and reasonably aligned in
the input crop. Only when the conﬁdence falls below a cer-
tain threshold is the hand detection model reapplied to the
next frame.

6. Application examples

Our hand tracking solution can readily be used in many
applications such as gesture recognition and AR effects. On
top of the predicted hand skeleton, we employ a simple al-
gorithm to compute gestures, see Figure 6. First, the state
of each ﬁnger, e.g. bent or straight, is determined via the ac-
cumulated angles of joints. Then, we map the set of ﬁnger
states to a set of predeﬁned gestures. This straightforward,
yet effective technique allows us to estimate basic static ges-
tures with reasonable quality. Beyond static gesture recog-
nition, it is also possible to use a sequence of landmarks to
predict dynamic gestures. Another application is to apply
AR effects on top of the skeleton. Hand based AR effects
currently enjoy high popularity. In Figure 7, we show an
example AR rendering of the hand skeleton in neon light
style.

7. Conclusion

In this paper, we proposed MediaPipe Hands, an end-to-
end hand tracking solution that achieves real-time perfor-
mance on multiple platforms. Our pipeline predicts 2.5D
landmarks without any specialized hardware and thus, can
be easily deployed to commodity devices. We open sourced
the pipeline to encourage researchers and engineers to build
gesture control and creative AR/VR applications with our
pipeline.

Figure 5: The hand landmark models output controls when
the hand detection model is triggered. This behavior is
achieved by MediaPipes powerful synchronization building
blocks, resulting in high performance and optimal through-
put of the ML pipeline.

Figure 6: Screenshots of real-time gesture recognition. Se-
mantics of gestures are rendered at top of the images.

References

[1] Valentin Bazarevsky, Yury Kartynnik, Andrey Vakunov,
Karthik Raveendran, and Matthias Grundmann. Blaze-
face: Sub-millisecond neural face detection on mobile gpus.

der C. Berg. SSD: single shot multibox detector. CoRR,
abs/1512.02325, 2015. 2

[12] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-
Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh
Chang, Wei Hua, Manfred Georg, and Matthias Grundmann.
Mediapipe: A framework for building perception pipelines.
volume abs/1906.08172, 2019. 1, 2, 4

[13] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A Argy-
ros. Efﬁcient model-based 3d tracking of hand articulations
using kinect. 1

[14] Tomas Simon, Hanbyul Joo, Iain A. Matthews, and Yaser
Sheikh. Hand keypoint detection in single images using mul-
tiview bootstrapping. CoRR, abs/1704.07809, 2017. 2

[15] Snapchat. Lens Studio by Snap Inc.

https://lensstudio.snapchat.com/
templates/object/hand/. 1

[16] Andrea Tagliasacchi, Matthias Schr¨oder, Anastasia Tkach,
Soﬁen Bouaziz, Mario Botsch, and Mark Pauly. Robust
In Computer
articulated-icp for real-time hand tracking.
Graphics Forum, volume 34, pages 101–114. Wiley Online
Library, 2015. 1

[17] Chengde Wan, Thomas Probst, Luc Van Gool, and Angela
Yao. Self-supervised 3d hand pose estimation through train-
In Proceedings of the IEEE Conference on
ing by ﬁtting.
Computer Vision and Pattern Recognition, pages 10853–
10862, 2019. 1

Figure 7: Example of real-time AR effects based on our
predicted hand skeleton.

CoRR, abs/1907.05047, 2019. 2

[2] Facebook. Oculus Quest Hand Tracking.

https://www.oculus.com/blog/oculus-
connect-6-introducing-hand-tracking-on-
oculus-quest-facebook-horizon-and-more/.
1

[3] Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann.
Robust 3d hand pose estimation in single depth images: from
single-view cnn to multi-view cnns. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3593–3601, 2016. 1

[4] Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann.
Robust 3d hand pose estimation from single depth images
IEEE Transactions on Image Pro-
using multi-view cnns.
cessing, 27(9):4422–4436, 2018. 1

[5] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying
Wang, Jianfei Cai, and Junsong Yuan. 3d hand shape and
pose estimation from a single rgb image. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 10833–10842, 2019. 1

[6] Google. Tensorﬂow lite on GPU.

https://www.tensorflow.org/lite/
performance/gpu advanced. 3

[7] Google. Tensorﬂow.js Handpose.

https://blog.tensorflow.org/2020/03/
face-and-hand-tracking-in-browser-with-
mediapipe-and-tensorflowjs.html. 1

[8] Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko,
and Matthias Grundmann. Real-time facial surface ge-
ometry from monocular video on mobile gpus. CoRR,
abs/1907.06724, 2019. 2

[9] Tsung-Yi Lin, Piotr Doll´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. CoRR, abs/1612.03144, 2016.
2

[10] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll´ar. Focal loss for dense object detection.
CoRR, abs/1708.02002, 2017. 2

[11] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexan-

