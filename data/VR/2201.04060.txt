VR Viewport Pose Model for Quantifying and
Exploiting Frame Correlations

Ying Chen∗, Hojung Kwon∗, Hazer Inaltekin†, Maria Gorlatova∗
∗Duke University, Durham, NC, †Macquarie University, North Ryde, NSW, Australia
@duke.edu, †hazer.inaltekin@mq.edu.au
ying.chen151, hojung.kwon, maria.gorlatova
}
{

∗

2
2
0
2

b
e
F
7

]

Y
S
.
s
s
e
e
[

2
v
0
6
0
4
0
.
1
0
2
2
:
v
i
X
r
a

Abstract—The importance of the dynamics of the viewport pose,
i.e., the location and the orientation of users’ points of view,
for virtual reality (VR) experiences calls for the development
of VR viewport pose models. In this paper, informed by our
experimental measurements of viewport trajectories across 3
different types of VR interfaces, we ﬁrst develop a statistical
model of viewport poses in VR environments. Based on the
developed model, we examine the correlations between pixels
in VR frames that correspond to different viewport poses, and
obtain an analytical expression for the visibility similarity (ViS)
of the pixels across different VR frames. We then propose a
lightweight ViS-based ALG-ViS algorithm that adaptively splits
VR frames into the background and the foreground, reusing
the background across different frames. Our implementation of
ALG-ViS in two Oculus Quest 2 rendering systems demonstrates
ALG-ViS running in real time, supporting the full VR frame rate,
and outperforming baselines on measures of frame quality and
bandwidth consumption.

Index Terms—Virtual reality, pose model, frame correlation,

game engine-based simulations

I. INTRODUCTION

Virtual reality (VR), which immerses users into computer-
generated virtual environments [1], has been showing promise
in many applications including gaming, education, and health-
care [2]. VR is expected to boost global GDP by $450 billion
by 2030 [3]. High expectation for VR, coupled with its known
resource-hungry nature [4], spurred a wide range of recent
research that optimizes VR systems to reduce their communi-
cation and computing resource consumption [5]–[10].

A particular feature of VR is the tight coupling of user’s
actions and the generated frames. In traditional visual media,
the frames that are shown to the users are ﬁxed. By contrast, in
VR, to allow the users to independently explore virtual worlds,
each frame is generated for the speciﬁc point of view of the
user at a given time, i.e., for the speciﬁc viewport pose, namely
the x, y, and z coordinates, and polar and azimuth orientation
angles θ and φ, of user’s VR headset or another interface
to the virtual world (mobile phone [5], [8], [11]; computer
monitor [12]). Hence, the correlations between different VR
frames, and the performance of approaches that exploit them
to reduce resource consumption in VR [8]–[10], are intimately
tied to the dynamics of user behavior within the VR experience.
We examine and exploit this phenomenon in this work.

First, we develop a statistical model of users’ VR viewport
pose, comprised of the models of pose components, orientation
and position. To develop this model, we collected a dataset of
VR viewport trajectories in 3 VR games and across 3 different

types of VR user interfaces, with over 5.5 hours of user data
in total.

To characterize the correlation of viewport orientations
between VR frames that are ∆t seconds apart, we obtain
models of the change of azimuth and polar angles over ∆t. To
characterize the displacement between VR viewport positions
that are ∆t apart, we propose a modiﬁed random waypoint
model (RWP) with random pause times (‘paused-MRWP’). We
demonstrate a close ﬁt of the developed VR viewport pose
model to the experimental data. To the best of our knowledge,
this is the ﬁrst statistical model of viewport pose in VR.

Fig. 1: VR frames gener-
ated for similar VR de-
vice viewport poses.

Next, we apply the developed
pose model to quantify the simi-
larity of pixels across VR frames.
For similar poses, the VR frames
are highly redundant, as shown in
Fig. 1. It is thus possible to reduce
resource consumption by render-
ing a set of ‘reference’ frames,
and generating other,
‘novel’,
frames by rendering only a por-
tion of the frame while generating
the rest by reusing the reference
frame via view projection [8],
[13]. In this paper we derive ana-
lytical expressions for the visibil-
ity similarity (ViS) of pixels across different VR frames, re-
lating the poses of the reference and novel frames through the
developed viewport pose model, and accounting for the mis-
alignments of the ﬁelds of view (FoVs) and the VR contents-
to-viewport distance differences between the novel and the ref-
erence frames. We verify our analysis via Unity 3D [14] game
engine-based simulations. Finally, we exploit the formulated
ViS to adaptively divide VR frame contents into background
and foreground, in order to render the foreground for the novel
frames, while reusing the background. Separate treatment of
background and foreground in VR frame generation has been
considered in multiple lines of work [9]–[11], [15], [16], which
use heuristics for this separation. In this work, we propose a
lightweight algorithm, ALG-ViS, that uses the analytical ViS
to adaptively determine the distance threshold beyond which
the contents are treated as background. We incorporate the
developed ALG-ViS in two rendering systems based on Oculus
Quest 2 (also known as Meta Quest 2), one on-device and
one supported by edge computing. In both systems, ALG-ViS

 
 
 
 
 
 
runs in real time, supporting the full VR frame rate, and
outperforming a set of baselines on measures of frame quality
and resource consumption.

To summarize, the main contributions of this paper are: (i)
the ﬁrst statistical model of viewport pose in VR, (ii) the analy-
sis of the visibility similarity between different VR frames, and
(iii) the analytically grounded algorithm for determining which
contents to reuse across different frames. We make the VR
viewport pose dataset and our implementation codes publicly
available via GitHub.1

The rest of this paper is organized as follows. We review the
related work in §II, propose the viewport pose model in §III,
and analyze the ViS and propose the ALG-ViS in §IV. We
present the evaluation in §V and conclude the paper in §VI.

II. RELATED WORK
Device pose modeling: VR frame generation requires infor-
mation about the pose (position and orientation) of user’s point
of view. The vast body of work that has, over the years,
modeled human mobility in many different applications [17]–
[19] focused on human positions but not orientations. Orien-
tations of handheld mobile devices are starting to be modeled
in context of visible light communications [20], [21]. We are
unaware of existing statistical models of users’ viewport pose
in VR. The position component of our developed model builds
on the modiﬁed RWP proposed in [22], and one of the orienta-
tion components is related to the observations previously made
in [23]. The comprehensive model we propose signiﬁcantly
modiﬁes and extends these approaches.
Predicting VR viewport pose: Recently, several approaches
that predict pose or its components in VR systems have been
developed [11], [24]–[28]. Unfortunately highly immersive VR
experiences are known to be negatively affected by errors in
the pose prediction [8]. Our statistical approach can be seen
as making decisions based on the distribution of viewport
poses, rather than the speciﬁc predicted pose. Our evaluation
demonstrates that this approach improves image quality and
bandwidth variability over prediction-based approaches.
Exploiting redundancy across VR frames: Multiple methods
for reducing the required bandwidth and transmission latency
in VR have been developed [5]–[10]. In a rich body of
work [9]–[11], [15], [16], a VR frame is classiﬁed into
background that is relatively static across VR frames and
foreground that is less similar from one frame to the next. The
background can be rendered on the edge and prefetched by the
VR device, while the foreground is rendered on the mobile
device [11], [15], [16]; the background can also be reused
across multiple frames [9], [10]. These studies use heuristics to
separate the background and the foreground. Complementing
this work, we derive an analytical expression for the inter-
frame pixel similarity, which we use to split the background
and the foreground adaptively, via a lightweight algorithm
that can run on-device or on the edge server. Our evaluation
demonstrates that our approach improves the VR image quality
while consuming fewer resources.

1https://github.com/VRViewportPose/VRViewportPose.

TABLE I: Main characteristics of VR games.
VR game
Viking village (VK) [33]
Lite [34]
Ofﬁce [35]

Number of triangles
2,400 K
65.7 K
207.6 K

Number of vertices
1,600 K
52.4 K
143.7 K

III. VR VIEWPORT POSE MODEL

We introduce our collected dataset in §III-A, describe our
orientation model in §III-B and our position model in §III-C,
and model the correlation between them in §III-D.

A. Collected Dataset

To complement existing datasets of users’ head orientation
in 360◦ videos [23], [29], [30] and a small-scale single-
interface dataset of users’ head pose in untethered VR [31],
we collected a dataset of users’ viewport pose in 3 different
VR games listed in Table I, across 3 different common
VR user interface types. Speciﬁcally, we examined: (i) VR
experienced through a VR headset and controlled through
user head rotation and a VR controller (“headset VR”), (ii)
“desktop VR” [12], experienced through the user’s desktop
monitor and controlled through desktop’s mouse and keyboard,
and (iii) VR experienced through a mobile phone, controlled
via moving the phone and tapping on it [32]. Our institutional
review board (IRB)-approved data collection, conducted un-
der COVID-19 restrictions, involved remote desktop VR and
phone-based VR data collection via apps that we distributed
to remote users, and a small number of socially distanced in-
lab experiments for headset and phone-based VR. In total,
we recorded experiences of 5 users with headset and phone-
based VR, and 20 users with desktop VR. For desktop and
phone-based VR, each user explored the 3 VR games for 2–5
minutes (per game). For headset VR, the users explored each
game for 2 minutes to avoid simulator sickness. Additional
data collection protocol details and the dataset are provided
via GitHub.1

B. Orientation Model

A VR viewport

is depicted in
Fig. 2, where n is the unit vector
along the optical axis of the camera.
We ﬁrst introduce the representation
for the viewport orientation using
variables related to n, followed by
the deﬁnition of the statistical view-
port orientation model.

Y

nq

Z

X

f
Fig. 2: VR viewport
orientation representa-
tion.

Deﬁnition 1 (Viewport orientation representation). The view-
port orientation is the tuple (θ, φ), where polar angle θ
∈
[0, π] is the angle between n and the positive direction of
π, π) is the angle
the Y -axis, and the azimuth angle φ
between the projection of n in the XZ-plane and the positive
direction of the X-axis in the Earth coordinates XY Z. θ and
φ characterize how users look vertically and horizontally.

−

∈

[

Deﬁnition 2 (Viewport orientation model). The viewport ori-
entation model is the tuple (pθ(θ), p∆θ(∆θ), p∆φ(∆φ)), where
pθ(θ), p∆θ(∆θ), and p∆φ(∆φ) are the probability density

functions (PDFs) of the polar angle θ, the polar angle change
π, π] over the time interval ∆t, and the azimuth angle
∆θ
[
∈
π, π] over ∆t. In this paper we assume that
change ∆φ
pθ(θ), p∆θ(∆θ), and p∆φ(∆φ) are independent.

−

−

∈

[

∆θ is given by ∆θ = θnov −

θref , where θref and θnov
are the polar angles of the reference and novel frames taken
∆t seconds apart. ∆φ is calculated as ∆φ =
π + mod
and
b
(φnov −
⌊·⌋
is the ﬂoor function, and φref and φnov are the azimuth angles
of the reference and novel frames taken ∆t seconds apart.

φref + π, 2π), where mod(a, b) = a

−
a
b

−

(cid:4)

(cid:5)

We examine azimuth angle change ∆φ rather than φ itself
because φ can be assumed to be uniformly distributed when
VR contents are scattered along different longitudes in the
VR systems and there are no viewing preferences. Hence,
the distribution of φ does not provide information about the
correlation between different VR frames.

1) Distribution ﬁt: We evaluate the distribution ﬁt for the
experimental measurements of θ, ∆θ, and ∆φ. In analyzing
pθ(θ) and p∆θ(∆θ), we ﬁt the experimental data to a set
of common statistical distributions. In analyzing p∆φ(∆φ),
the PDFs of which have irregular shapes for some values
of ∆t, we ﬁt the experimental data to the set of common
distributions and mixed distributions of two different common
distributions. As the error metric, we use the sum of squared
errors (SSE) [36] between the data and the ﬁtted distribution.
2) Statistical distribution of the polar angle θ: We found
Laplace distributions, with means close to 90◦ (89.0◦–92.3◦)
and scales ranging from 3.5 to 7.4, to best ﬁt experimental
data (see a summary in Table II and a ﬁt example in Fig. 3).
This is intuitive: it corresponds to humans having a bias for
looking straight ahead, to the central parts of VR contents,
without frequently tilting their heads. Among the 3 VR games,
the scale values are the largest for VK (4.3–7.4), which
has more contents scattered along different latitudes than the
other games. Among the 3 interface types, the scale values
are the smallest for headset VR (3.5–4.3), corresponding to
users looking straight ahead rather than up and down. We
hypothesize that this is due to the discomfort associated with
tilting the head drastically while wearing a headset.

3) Statistical models for polar angle change ∆θ: The ex-
perimental distributions of ∆θ for different ∆t values closely
ﬁt zero-mean Laplace distributions. The scales b1,θ of the
Laplace distributions that yield the best ﬁt for different ∆t
values in Lite are shown in Table III. As ∆t increases, the
correlation between polar angles decreases, leading to the
increase of the scale with ∆t (e.g., from 0.305 for ∆t = 5/60 s
to 3.407 for ∆t = 100/60 s for desktop VR). Among the
3 interface types, headset VR has the largest bl,θ when ∆t
is small (e.g., 1.92 vs. 1.48 and 1.41 for ∆t = 30/60 s),
indicating that the polar angle changes more rapidly. This is
due to the ease of changing viewport orientation over a small
time interval in headset VR.

4) Statistical models for azimuth angle change ∆φ: In our
examinations, for some ∆t the distributions of ∆φ appeared
to have canonical shapes, while for others they appeared as

a mixture of distributions. Thus we ﬁt the experimental data
to both common and mixed distributions. We present the best
distribution ﬁts and their parameters, for a subset of ∆t values,
in desktop VR for all 3 games jointly, in Table IV. For the
cases of mixed distributions, logistic and Laplace in these
examples, the PDF of the mixed distribution is written as

−1

1

−

(cid:19)

fmixed(x) = pl

1
2bl

exp

µl

x
|

|−
bl

−

(1

−

pl)

blo

exp

−
(cid:16)
1+exp

(cid:16)
|x|−µlo
blo
(cid:17)
|x|−µlo
blo

−

1

1

−

(cid:17)

exp

−

(cid:16)

+

180−µl
bl

(cid:17)

1

2

2

1+exp

180−µlo
blo

−

(cid:18)

(cid:16)

(cid:16)

(cid:17)(cid:17)
where µl and bl are the mean and the scale of the Laplace
distribution, µlo and blo are the mean and the scale of the
logistic distribution, and pl is used to alter the fractions of the
logistic and the Laplace distributions.

(cid:17)(cid:17)

(cid:16)

(cid:16)

The best distribution ﬁt for ∆φ changes with ∆t. When
∆t is small (i.e., when ∆t < β1), ∆φ is best modeled by
a Laplace distribution with a relatively small scale. When
β1 6 ∆t < β2 , ∆φ is best modeled by a mixture of logistic
and Laplace distributions, corresponding to users’ tendency to
change their head orientations only slightly over these time
15◦ < ∆φ < 15◦). Finally, when ∆t > β2,
intervals (i.e.,
the individual angle observations become uncorrelated and are
180◦,180◦). From the
best modeled by a uniform distribution
collected desktop VR pose data, we obtain β1 = 189/60 s
and β2 = 1549/60 s. Examples of these three cases are
shown in Fig. 4. For the other 2 VR interfaces, we observe
similar patterns, but β1 and β2 are different: β1 = 244/60 s
and β2 = 1003/60 s for headset VR, β1 = 496/60 s and
β2 = 1006/60 s for phone-based VR.

U[

−

−

C. Position Model

1) MRWP model with random pause times: In this section
we introduce our model for VR viewport position. We focus
on viewport position change over time interval ∆t, in order
to analyze the ViS of VR frames that are ∆t apart.

Adopting the axis notation common in computer graphics
[14], [37], the viewport positions in the Earth coordinates
XY Z (shown in Fig. 2) are denoted as (x, y, z), where y
is the height of the viewport, and x and z are the coordinates
of the viewport positions in the XZ-plane (i.e., the ground
plane). We assume that y is constant, e.g., y can be set to
the human eye level. While changing y is important in some
speciﬁc contexts, such as exergames [38], [39], y is ﬁxed in the
vast majority of typical VR experiences, and in native Oculus
Integration app development, to avoid disorienting the users
when they sink below or ﬂoat above the ground in the virtual
environment [40].

To model the change of x and z, we propose a paused-
MRWP position model in the inﬁnite plane, based on our
collected pose data and the modiﬁed RWP [22]. The model
N+, called
consists of an inﬁnite sequence of points Wn, n
waypoints, the pause time Sn at each waypoint, the duration
Tn to move along a straight line from Wn to Wn+1 with
a constant velocity v, and the included angle αn between
WnWn+1 and the abscissa. The waypoint is expressed as
−−−−−−→

∈

TABLE II: Mean and scale of the Laplace distribution that has the smallest SSE with the experimental measurements of polar
angle θ, for different interface types and VR games.

Interface type
VR game
Mean
Scale

Desktop VR
Lite
90.037
6.057

Ofﬁce
89.979
6.204

VK
90.575
7.356

Headset VR
Lite
89.654
3.454

Ofﬁce
88.999
3.646

VK
92.331
4.319

Phone-based VR
Lite
90.548
6.195

Ofﬁce
89.944
6.856

VK
90.896
6.797

TABLE III: Scale of best-ﬁtting Laplace distributions for polar angle change ∆θ, in Lite for different interface types.

∆t (1/60 s)

Interface
type

Desktop VR
Headset VR
Phone-based VR

1
0.0748
0.0841
0.0740

5
0.305
0.450
0.332

10
0.571
0.825
0.664

15
0.826
1.149
0.727

20
1.049
1.438
0.880

25
1.276
1.699
1.013

30
1.480
1.920
1.141

100
3.407
3.516
2.589

200
4.984
4.510
3.495

600
8.005
5.816
4.792

5000
9.926
6.186
12.648

0.10

0.08

F
D
P

0.06

0.04

0.02

0.00

0

Laplace fit
Exp. data

30 60 90 120 150 180

Polar angle ( ∘ ∘

F
D
P

0.175
0.150
0.125
0.100
0.075
0.050
0.025
0.000

−180 −90

Laplace
Exp.Δdata

0.020

0.015

F
D
P

0.010

0.005

Logistic
+Laplace
Exp.Δdata

0.008

0.006

F
D
P

0.004

0.002

Uniform
Exp.Δdata

0
ΔϕΔ( ∘ ∘

90

180

0.000

−180 −90

0
ΔϕΔ( ∘ ∘

90

180

0.000

−180 −90

0
ΔϕΔ( ∘ ∘

90

180

Fig. 3: Experimentally ob-
tained θ (desktop VR, Lite) is
Laplace distributed.

(a) ∆t = 10/60 s

(b) ∆t = 500/60 s

(c) ∆t = 2000/60 s

Fig. 4: The distribution of the experimental data for azimuth change ∆φ after ∆t, and the
best distribution ﬁt among common distributions and mixed distributions.

Collected trajectory
Extracted flights

1.0

0.8

1.0

0.8

Exponential dist. 
and a "bump"
Exp. data

30

20

10

0

)

m

(

z

−10

−20

−30

−20 −10

10

0
x (m)

20

Fig. 5: The collected trajec-
tory for one user in Lite and
the extracted ﬂights.

F
D
C

0.6

0.4

0.2

0.0

Flight samples
Paused-MRWP
Classical RWP

0 5 10 15 20 25 30 35 40
Flight time (s)

Fig. 6: CDF of the ﬂight time
for collected samples, paused-
MRWP, and classical RWP.

F
D
P

0.6

0.4

0.2

0.0

0

2

4

6

8

10

10−5

Pause time (s)

Fig. 7: The pause time for all
users and all games.

1

2

3

4

k

Fig. 8: The analytical and em-
pirical results of the k-th mo-
ment of position displacement.

Δt=1/6 s, mΔk)
̃mΔk)
Δt=1/6 s, 

Δt=1 s, mΔk)
̃mΔk)
Δt=1 s, 

103

101

ψ
f
o

t
n
e
m
o
m
h
t
-
k

10−1

10−3

TABLE IV: Distributions that best ﬁt the experimental az-
imuth angle change ∆φ over ∆t, and the SSE between the
experimental and the ﬁtted distributions.

∆t (s)

10/60
60/60
200/60

500/60

Distribution
Laplace
Laplace
Logistic+
Laplace

Logistic+
Laplace

2000/60

Uniform

Best ﬁt
Parameters
µl = 0.0, bl = 3.130
µl = 0.0, bl = 13.868
µlo = −0.1, blo = 28.54,
µl = 0.1, bl = 0.24,
pl = 0.36
µlo = −0.4, blo = 53.35,
µl = 4.2, bl = 0.34,
pl = 0.13

∆φ ∼ U[−180◦,180◦)

SSE
1.465 × 10−3
7.016 × 10−3
3.650 × 10−3

2.499 × 10−3

1.739 × 10−4

Wn = (xn, zn) where xn and zn are the coordinates of
waypoints in the XZ-plane. The vector −−−−−−→
WnWn+1 is called the
n-th ﬂight, and αn is called the direction of the n-th ﬂight.
At time 0, the viewport is at W1, and starts to move towards
W2. Due to the constant velocity, the ﬂight time Tn for the n-
th ﬂight is proportional to its length
. The reason
for assuming a constant velocity is that although acceleration
can be used to produce more realistic movements, constant-
velocity VR movement is known to be more comfortable
than the movement with acceleration or deceleration [41]. We
further assume that Tn, Sn, and αn are all i.i.d. distributed

WnWn+1
−−−−−−→

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

over n. Based on the experimental data in desktop VR, we
will propose the models for Tn, Sn, and αn.

The collected data shows that the viewpoint movement in
the XZ-plane is well approximated by a sequence of ﬂights.
We apply the standard angle model proposed in [19] to extract
ﬂights from the trajectories. Fig. 5 plots the trajectory in XZ-
plane of one user in Lite and the extracted ﬂights. Although
the viewport does not move in a perfectly straight line during
each ﬂight, the trajectory is close to it.

Modeling ﬂight duration. Our experimental data demon-
strates that Tn is exponentially distributed, and conﬁrms that
the paused-MRWP better models the ﬂight time in VR than
the classical RWP models [17]. Speciﬁcally, the PDF of the
µt.
ﬂight times Tn, denoted as fTn(t), is modeled as µe−
Fig. 6 shows the cumulative distribution function (CDF) of
ﬂight times for our collected ﬂight samples, the paused-MRWP
model with the best ﬁtted µ, and the classical RWP with a
constant velocity in VK game. We see that the ﬂight times
of the paused-MRWP model match the measurements better
statistically, with the SSE as low as 0.0021. The exponential
distribution with the best ﬁtted µ will be used to model the
ﬂight times in §IV and §V.

Modeling pause time. We model the pause time Sn accord-

 
 
 
 
ing to the collected data. Fig. 7 shows that the exponential
distribution with a “bump” around zero is a good ﬁt to its
distribution. The PDF of the pause times, denoted as fSn (s), is
λs stands for the
modeled as (1
PDF of the exponential distribution with parameter λ, Dirac
delta function δ(s) models the “bump” around zero pause time,
and c represents the fraction of the “bump”.

λs + cδ(s), where λe−

c)λe−

−

Modeling ﬂight direction. From our pose dataset, the ﬂight

angles αn follow the uniform distribution on [0, 2π).

With the developed paused-MRWP model, we will focus on
observation intervals of duration ∆t, and derive an analytical
expression for the moment generating function (MGF) of the
displacement between two viewport positions.

Deﬁnition 3 (MGF of position displacement). Consider
[ts, ts + ∆t], where ts is a sam-
an observation interval
U[0,T ]. Let Xref and Xnov be the viewport po-
ple from
the posi-
sitions at ts and ts + ∆t, respectively. Then,
2, and the
Xref k
Xnov −
tion displacement ψ is deﬁned as
k
is given by
Mψ(τ ),
MGF of ψ, denoted by
Mψ(τ ) =
2
Xref k
.
exp
limT
We note that during a small ∆t in VR systems, Xref
and Xnov are not necessarily at the waypoints. This is in
contrast to conventional applications of RWP models, where
the movement is observed at a larger timescale [17], [19].

Xnov −

→∞

(cid:17)i

· k

E

(cid:16)

h

τ

2) Analysis: In analyzing

Mψ(τ ), there are two mutually
exclusive and exhaustive cases to consider. Setting T0 =
S0 = 0 as the auxiliary variables, the Case 1 is the case

j

N+,

j

1

−

∈

that

j
∃

Tn +

Sn < ts <

Sn, i.e.,
we start observing the process when the movement is paused.
Let Λ denote the event that Case 1 holds. Let Λ′ denote the
complement of Λ, and Λ′ is the event that Case 2 holds. The

n=0
P

n=0
P

n=0
P

n=0
P

Tn +

j

j

j

1

−

N+,

j

1

−

Tn +

Sn < ts <

Case 2 is the case that

j
∃

∈

j

j

1

−

Tn +

n=0
P
Sn, i.e., we start observing the process during
n=0
a ﬂight. We will ﬁrst obtain the k-th (k > 1) moment of
P
ψ, mΛ(k) and mΛ′ (k), for Cases 1 and 2 in Lemmas 1 and
2. Combined with the probability that Case 1 holds given in
Lemma 3, we will obtain

n=0
P

n=0
P

Mψ(τ ) in Theorem 1.

Lemma 1. Assume Case 1 holds. Let T ′j = 0, and S′j be the
remaining pause duration after ts in the same pause interval.
Let T ′i = Ti and S′i = Si for i > j. We deﬁne the events An
and Bn as

, n > 2

An =

, n = 1
1 > 0 & ξn < 0

{

ξ1 < 0
}
˜ξn
(
−
n
ξn > 0 & ˜ξn < 0
n
∆= ∆t

j+n

−

1

o
j+n
−

Bn =

S′i −

o
, n > 1

−

i=j
P

i=j
P

1

T ′i and ˜ξn , ∆t

−

T ′i . An is the event that we end the observation

where ξn

j+n

−

1

i=j
P

S′i −

j+n

i=j
P

N+,

j′

j′

1

−

Tn +

Sn < ts +

in a pause interval (

j′

∃

∈

j′

j′

∆t <

Tn +

n=0
P
Sn) and that there are n

n=0
P

n=0
P
j

n=0
P

ﬂights in [ts, ts + ∆t]. Bn is the event
j
−

−

1

1

j

that
1
j

−

Tn +

Sn < ts + ∆t <

Tn +

−

1 complete
N+,

j
∃
∈
Sn and that

n=0
there are n
P
is given by

n=0
P
−

n=0
1 complete ﬂights in [ts, ts + ∆t]. Then, mΛ(k)
P

n=0
P

mΛ(k) =

∞

E

ψk1(An)

+

∞

E

ψk1(Bn)

(1)

n=2
X

(cid:2)

(cid:3)

n=1
X

(cid:2)

(cid:3)

where

E

ψk1(An)

=

(cid:3)

(cid:2)
ψk1(Bn)

E

=

n + k
n

−
n + k
n

−

(cid:18)

(cid:18)

−
2

−
1

2

n

−

2

(cid:19)
1

Xh=0
1
n
−

(cid:19)

Xh=0

µλgn

−

2,h,h+2,k,2k+n+h+1,

λgn

−

1,h,h+1,k,2k+n+h+1

j+n

(cid:2)
−

i=j+1
P

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:3)

(cid:2)
and gi,h,n,k,m , (2k)!µiλh(∆t)m−1e−µ∆t
v2k
hypergeometric function of the ﬁrst kind.

1F1(n;m;
1)!
c)h, with 1F1(n; m; z) being the conﬂuent

h(1

µ)∆t)

ci

×

−

(m

i
h

(λ

−

−

−

−

(cid:0)

(cid:1)

Proof. See Appendix A. Proof sketch: Since the collec-
tions of events
are mutually disjoint and
Bn}
{
collectively exhaustive, we write mΛ(k) as in (1). To cal-
culate E
, we express the k-th moment of ψ as

and

An}
{
ψk1(An)
2k
1

v2k

Tiei

(cid:3)
, where ei denotes the unit vector whose

direction represents the moving direction αi of the i-th ﬂight.
Based on the property that αi is i.i.d. uniformly distributed on
[0, 2π) and on the distributions of ﬂight and pause times, we
ψk1(An)
get the expressions of E
. Similar techniques are
used to obtain E

.

ψk1(Bn)
(cid:2)

(cid:3)

(cid:2)

(cid:3)

Lemma 2. Assume Case 2 holds. Let S′j
1 = 0, and T ′j be the
remaining ﬂight duration after ts in the same ﬂight interval.
Let T ′i = Ti for i > j and S′i = Si for i > j. Let the events
An and Bn be deﬁned as

−

An =

(

{

ξ1 < 0
, n = 0
}
˜ξn > 0 & ξn+1 < 0

, n > 1

Bn =

n
ξn > 0 & ˜ξn < 0

o
n > 1

n

where ξn , ∆t

o
j+n

−

1

j+n

−

2

S′i −

T ′i and ˜ξn , ∆t

−

1

i=j
−
P

i=j
P
T ′i . Then, mΛ′ (k) is given by

−

j+n

−

1

1

i=j
−
P

S′i −

−
1

−

j+n

i=j
P
∞

mΛ′ (k) =

E

ψk1(An)

+

∞

E

ψk1(Bn)

n=0
X

(cid:2)

(cid:3)

n=1
X

(cid:2)

(cid:3)

where

E

ψk1(An)

=

(cid:2)

(cid:3)

E

ψk1(Bn)

=

cn(2k)!µn(∆t)2k+n exp(

µ∆t)

+

−

(2k + n)!
n

n + k
n

(cid:18)
1

n

−

(cid:19)
1

Xh=1
µgn

−

1(n > 0)

n + k
n

−
1

gn,h,h,k,2k+n+h+1,

1,h,h+1,k,2k+n+h+1.

−
Proof. The proof is similar to that of Lemma 1.

Xh=0

(cid:3)

(cid:2)

(cid:19)

(cid:18)

ψk1(An)

Lemmas 1 and 2 yield the expressions for the k-th moment
of the position displacement for arbitrary ∆t. In VR systems,
we analyze the ViS for small ∆t. In this case, the number of
ﬂights and pauses in the observation interval is limited. The
terms of E
, n 6 N , dominate
mΛ(k) and mΛ′ (k). N = 2 a good choice for N because
(cid:2)
(cid:2)
ψk1(An)
the sum of the terms E
), n 6
2, accounts for more than 98% of mΛ(k) (or mΛ′ (k)) when
t < 1 s and k 6 4. Hence, we can simplify the calculation of
mΛ(k) and mΛ′(k) by discarding many terms corresponding
to the cases of n > 2 when ∆t is small (e.g., ∆t < 1 s).

ψk1(Bn)

ψk1(Bn)

(cid:3)
(or E

and E

(cid:2)

(cid:3)

(cid:3)

(cid:2)

(cid:3)

Lemma 3. Let ts be a sample from
probability that ts falls in a ﬂight interval. Then, p = lim
→∞
exists and is equal to p = λ/(1
c)
c)+µ .
−
λ/(1
−

U[0,T ] and pT be the
pT

T

Proof. See Appendix B.

∞k=0 m(k)τ k

Theorem 1 (MGF of ψ).

Mψ(τ ) is given by
Mψ(τ ) =
k! where m(k) is the k-th moment of the posi-
p)mΛ(k) + pmΛ′ (k)].

tion displacement ψ and m(k) = [(1
P
Proof. The proof follows directly from Lemmas 1-3.

−

(cid:14)

Fig. 8 compares the empirical results of the k-th moment
of ψ, ˜m(k), with the analytical results m(k). We obtain ˜m(k)
by randomly sampling 5000 pairs of viewport positions that
are ∆t apart and calculating the position displacement. The
6 when ∆t = 1/6 s
gap
and k = 4, and is smaller than 6% of ˜m(k) in other cases.
Theorem 1 will be used to calculate the ViS in §IV.

is smaller than 5

m(k)
|

˜m(k)

10−

−

×

|

D. Correlations between Orientations and Positions

VR viewports’ position and orientation are correlated. Ob-
serving the azimuth angles φ and the walking directions
Xref Xnov and the positive
(i.e., the included angle between −−−−−−→
direction of X-axis) in our collected data, we ﬁnd that the
azimuth angles ﬁxate around the walking direction. Similar
observations have been made about human walking patterns
in non-virtual worlds [42], [43]. Supported by the pose data,
we assume that the azimuth angle at observation start time ts
is the same as the walking direction.

IV. VISIBILITY SIMILARITY

We introduce the model for the average visibility similarity
given the viewport-to-content distance d (ViS(d)) in §IV-A.
Then we apply the developed VR pose model to analyze

the ViS(d) in §IV-B, and propose the ViS-based VR content
splitting algorithm, ALG-ViS, in §IV-C.

A. ViS Model

The analytical model for ViS(d) we develop in this section
characterizes the statistical average of the inter-frame pixel
similarity over different pose changes given the viewport-
to-content distance d. We deﬁne the ViS(d) formally after
introducing the camera model to present how viewport pose
determines the rendered pixels.

Camera model. In VR, the virtual environment is con-
structed as computer-generated 3D contents, where the pixels
in VR frames are generated by capturing the scenes with
the camera. The camera is modeled as a standard pinhole
camera following [44]. A 3D point in the virtual environment
is projected through the pinhole to a pixel on the VR frame.
We denote the camera’s angle of view (AoV) as wf v.
In
addition, we assume that the far plane df p of the camera,
i.e., the largest viewport-to-content distance beyond which the
contents cannot be rendered in the VR frame, is much larger
than the viewport position change.

Consider two VR frames generated at ts and ts + ∆t, called
reference and novel frames. The cameras that capture these
frames are called reference and novel cameras, respectively.
A pixel in the reference frame is projected back to the 3D
point in the virtual environment, and the 3D point is projected
to the corresponding pixel in the novel frame. The viewport-to-
content distance d is formally deﬁned as the distance between
the viewport position of the reference frame and the 3D point.
The ViS(d) represents the average similarity of the pixels (of
distance d) in the reference frame and their corresponding
pixels in the novel frame, where the average is taken over
different pose changes of reference and novel frames.

q

(x3D −

xr)2 + (z3D −

Sd denote the number of

Deﬁnition 4 (ViS(d)). Let
the
reference frame’s pixels that are projected to the 3D points
with distance d from the reference camera. d is expressed as
zr)2, where X3D = (x3D, z3D),
d =
Xref = (xr, zr) are the positions of the 3D point and the
SV iS,d,
reference camera in the XZ-plane, respectively. Let
SV iS,d ⊆ Sd, denote the set of the reference frame’s pixels
that have the same pixel value as the corresponding pixels
in the novel frame. The visibility similarity given d is deﬁned
as ViS(d) , E
, which is the average percentage of
pixels that have the same pixel value as the corresponding
h
is
pixels in the novel frame among
taken over the possible viewport pose changes from reference
to novel cameras modeled in §III.

Sd. The mean of |SV iS,d|
|Sd|

|SV iS,d|
|Sd|

i

We break down the ViS(d) into two terms: (1) the FoV
term ViSf ov, which represents the fraction of the VR contents
contained in the FoVs of both the novel and the reference
cameras, and (2) the distance term ViSdst(d), which quantiﬁes
the ratio of the number of pixels representing the same 3D
points (of distance d) in the novel and reference frames. In VR
systems, the viewport moving closer to the VR contents will

result in the use of more pixels to represent the contents. Note
that ViSf ov is independent of d while ViSdst(d) is a function
of d. In our analysis, we ignore the inﬂuence of occlusion in
ViS(d), i.e., we do not consider the case where the occluded
objects are in the FoV of both reference and novel frames, are
rendered in the novel frame, but occluded by the other contents
in the reference frame. In our numerical results, we show that
their effect is small. We have ViS(d) = ViSf ovViSdst(d). The
ViSf ov and ViSdst(d) are deﬁned formally below.

Deﬁnition 5 (FoV term). ViSf ov is deﬁned as the statistical
average of the multiplication of the fraction of overlapping
polar and azimuth angles of reference and novel cameras.

We analyze ViSf ov
with the distributions
of ∆θ = θnov −
θref
and ∆φ = φnov −
φref
§III-B.
in
obtained
the
depicts
9
Fig.
positions
and angles
of reference and novel
camera viewports
in
the XZ-plane. Since
plane
the
df p
is much larger
than
−−−−−→XrefXnov
(cid:13)
(cid:13)
is
. The
(cid:13)
(cid:13)
(cid:13)
(cid:13)
fraction of the overlapping polar angles is obtained similarly.
Hence, the FoV term is

Fig. 9: Schematics of camera posi-
tions and angles in the XZ-plane.

, the fraction of the overlapping azimuth angles

wf v
2 ,φnov−

wf v
2 )−
wf v

max(φref −

min(φref +

2 ,φnov+

wf v
2 )

far

wf v

ViSf ov = E∆θ

1
(cid:20)

−

∆θ
|
|
wf v (cid:21)

·

E∆φ

1

(cid:20)

∆φ
|
|
wf v (cid:21)

.

−

Deﬁnition 6 (Distance term). ViSdst(d) is deﬁned as

d2 + ψ

ViSdst(d) , Eψ,ϑ


  p

−

2d√ψ cos(ϑ)
d

2

!





(2)

(3)

d2 + ψ

2d√ψ cos(ϑ) is the distance between the
where
−
VR content and the novel camera, and ϑ is the included angle
between −−−−−−→Xref X3D and −−−−−−→

Xref Xnov, as shown in Fig. 9.

p

B. The Analysis of ViS(d)

We ﬁrst provide a closed-form expression for ViSf ov, and

then approximate ViSdst(d) tightly with a small error.

Theorem 2 (FoV term). ViSf ov is equal to

bl,θ −

exp

(cid:16)

−

wf v
bl,θ
wf v

(cid:17)

(bl,θ + wf v)

ViSf ov =

1

−









(cid:16)

pφ
f

1

−

(cid:17)

where bl,θ is the scale of the ﬁtted Laplace distribution of ∆θ,
wf v
and pφ
f = 2
0 ∆φp∆φ(∆φ)d(∆φ), where p∆φ(∆φ) is
wf v
the PDF of ∆φ. pφ

f is equal to

R

bl−

exp

(cid:16)

−

wf v
bl
wf v

(bl+wf v )

(cid:17)

, ∆t < β1

pφ
f =

2(1

pl)

−
wf v

wf v +blo log

2



−

1+e

wf v
blo

wf v



−

−

1+e

wf v
blo


1+e

− π
blo

2

(cid:18)

(bl+wf v)

−1


1

−

(cid:19)
, β1 6 ∆t < β2

(4)

exp

+pl

bl−

wf v
bl
exp
wf v
wf v
2π , ∆t > β2.

−
(cid:16)
1

−

(cid:16)

(cid:17)

(cid:16)

−

π
bl

(cid:17)(cid:17)

Proof. See Appendix C.






We focus on small ∆t (e.g., ∆t 6 1 s which belongs to the
ﬁrst case in (4)), as the most relevant, in practice, to exploiting
VR frame correlation. In this case, the FoV term only depends
on wf v, bl, and bl,θ.

Theorem 3 (Distance term). For ε > 0, we can approximate
4 sin( wf v
2 )
ViSdst(d) within error of κε
.
Mψ
wf v d√π
Speciﬁcally, ViSdst(d) can be approximated according to

, where κ =

1
ε2

−
(cid:0)

(cid:1)

1

−

−

ViSdst(d)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where g(i) = (

−

m(1)
d2 + κ

∞

i=0
X
1)iε−(2i+1)m(i+1)
2 )i!

(i+ 1

.

< κε

Mψ

−

(cid:18)

1
ε2

(cid:19)

(5)

g(i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. See Appendix D. Proof
erage over ϑ and substituting m(1),
4 sin( wf v
2 )
ten as ViSdst(d) = 1 + m(1)
wf v d
Eψ

d2 −
τ ψτ −

1
2 dτ

√ψ

Eψ

= 1
√π

(3)
Eψ

sketch: Taking the av-
rewrit-

is

, we then prove that

(cid:2)

(cid:3)

√ψ

. Since

0 ψe−
∞
= ∞
i=0
1
P
2 dτ

(cid:21)
τ ψτ −

Eψ

(cid:2)

1
(cid:3)
ε2
0 ψe−

τ ψτ −

hR
1
2 dτ

(cid:20)

tion error Eψ
R
ψe−
to conclude the proof.

∞1
ε2

hR

g(i) and that the approxima-

i

is smaller than ε

i

Mψ

1
ε2

−
(cid:0)

(cid:1)

From (5), ViSdst(d) can be approximated by 1 + m(1)

d2 −
can be made

(cid:1)
is convergent
g(i)
|

g(i)

. Hence, ∞
i=0
P
< 0.01 in §V.

κ ∞
i=0
P

g(i). The approximation error κε

Mψ
arbitrarily small by choosing a small ε. ∞

−
(cid:0)
i=0 |
is bounded by 2ε−(2i+1)(v∆t)2(i+1)
P

since

1
ε2

i!

g(i)
|

|

1
ε2

is convergent, and the sum of the ﬁrst 30 terms provides a good
approximation with an error ε
Mψ
The time taken to calculate the ViS(d) is dominated by
calling the conﬂuent hypergeometric function 1F1(n; m; z)
in Lemmas 1 and 2. The average time, over 100 iterations,
for calculating ViS(d) on a Lenovo laptop (equipped with an
AMD Ryzen 7 4800H CPU and an NVIDIA GTX 1660 Ti
GPU) in MATLAB is only 4.4 ms. Hence, the ViS(d) can be
calculated for each frame in real time.

−
(cid:0)

(cid:1)

Algorithm 1 ALG-ViS.

1: count = 0;
2: for each new frame do
3:
4:

if mod (count, R) == 0 then

5:
6:

7:

8:
9:

10:

11:
12:

13:

14:
15:

The frame is selected as a reference frame;
Classify all VR contents as foreground contents;

else

df p;

dtr ←
for (d = 0.5; d < df p; d
Calculate ViS(d);
if ViS(d) > ViStr then
d;

←

dtr ←
break;

d + 0.5) do

Classify the VR contents with d 6 dtr as foreground
contents, and other contents as background contents;

count

count + 1;

←

Render the foreground contents. Reuse the background
pixels from the reference frame by view projection;

−

C. ViS-Based Foreground and Background Content Splitting
Based on the average ViS for a given d, i.e., ViS(d), we
adaptively split the contents to background and foreground,
where the background (with a larger d) has a high ViS(d)
and can be reused to reduce the resource consumption. To
this end, we propose ALG-ViS given in Algorithm 1. For
every R consecutive frames, the ﬁrst frame is selected as the
1 frames are the novel
reference frame; the remaining R
frames. In reference frames, all VR contents are classiﬁed as
foreground contents. In novel frames, we calculate a distance
threshold dtr such that ViS(dtr) = ViStr, where ViStr is
the threshold and a ViS(d) larger than ViStr indicates high
similarity of pixels in reference and novel frames. The VR
contents that have the distance d 6 dtr from the reference
camera are classiﬁed as foreground contents, the other contents
– as background contents. VR system renders the foreground
contents and reuses the pixels for the background contents
from the reference frame by view projection [8], [13]. We
only need to calculate ViS(d) for R
1 values of ∆t when
the inter-frame interval is ﬁxed (e.g., when the system supports
the full frame rate as in §V), which makes ALG-ViS even more
lightweight.

−

V. EVALUATION

We verify the analysis of ViS via simulations in §V-A
and examine the performance of ALG-ViS in real-world VR
implementations in §V-B. The parameters are listed in Table V
unless otherwise speciﬁed.

A. Examining ViS

1) Simulation settings: We verify the ViS analysis via
simulations using Unity Engine 2019.2.14f1 [14] with 3 VR
games listed in Table I. The results are analyzed in MATLAB.
To simulate the ViS, we randomly sample 5000 pairs of
viewport poses from the collected pose trajectories for refer-
ence and novel cameras. We obtain the pristine novel frame

TABLE V: Parameters

Parameter
VR frame resolution
AoV
Depth encoding
Far clipping plane df p
Unity unit: meter

Value
1080×1080
90◦
Linear
50 m
1:1

Parameter
VR frame rate
Mapping scheme
Depth map precision
Eye level height
Pairs of viewport poses

Value
60 fps
Equiangular
8 bits
1.6 m
5000

∪

g

ViS(d) =

rendered by Unity, and the generated novel frame by view
projection from the reference frame and its depth map, where
each pixel in the depth map represents the distance of the VR
contents to the reference camera. Among the generated novel
frame’s pixels whose corresponding 3D points are at a distance
d from the reference camera, the pixels with RGB values with
indistinguishable differences from the pristine novel frame
constitute the set Φ(d); the other pixels form the set Φc(d).
The ViS given d is calculated as the proportion of matched
Φ(d)
pixels
. We consider pixel values as
|
|
Φc(d)
Φ(d)
|
|
indistinguishable when the difference of the pixel values is
N+ in all RGB channels2.
less than Cth ∈
g
2) Numerical results: Fig. 10 shows the ViS obtained in our
ViS(d), and the analytically derived ViS, ViS(d),
simulations,
for headset VR and phone-based VR, for different d and ∆t.
The results for desktop VR are similar to the results for phone-
based VR and are omitted. As expected, the ViS declines with
∆t (i.e., frames that are separated by a longer time interval
are less similar), and increases with d (i.e., contents that are
farther from the camera change less across different frames).
We observe differences between VR interfaces as well: the
ViS for headset VR (Fig. 10(a)) is smaller than the ViS for
the other interface types (Fig. 10(b)). This is explained by the
difference in movement dynamics we observed in our dataset:
in headset VR, the users change their viewport orientations and
ﬂight directions more rapidly than in the other VR interfaces.
Smaller ViS for headset VR implies that fewer VR contents will
be classiﬁed as background contents. Finally, we note that the
gap between the analytical results and the simulations is small,
4.1% on average for d < 20m and only 1.6% on average
for d > 20m. The gap can be attributed to the omission of
object occlusions from our analytical derivations. The achieved
highly accurate analytical ViS(d) in the high ViS regime (e.g.,
d > 20m) is crucial for selecting the distance threshold dtr to
ensure that the background has a high ViS.

The obtained ViS for the three VR games in desktop VR is
shown in Fig. 11. The results for the other 2 interface types
exhibit the same trends and are omitted. Although VK has a
larger number of triangles and vertices, which manifests in
higher visual scene complexity, the ViS(d) of VK is larger
than the ViS(d) of Ofﬁce and Lite. This is because users’
pose trajectories in VK have relatively smaller bl,θ and bl, and
smaller µ and λ, corresponding to larger ﬂight lengths and
pause durations. In other words, users tend to change both
their viewport orientations and positions more slowly in VK.

2Cth is selected as the maximum value such that the structural similarity
index measure (SSIM) between the combined and the pristine novel frame
is larger than 0.95. The combined novel frame is obtained by replacing
Φc(d) in the generated novel frame with the
the mismatched pixels ∪
d
corresponding pixels in the pristine novel frame.

1

0.8

0.6

0.4

0.2

i

S
V
e
h
T

t=1/6 s

t=1 s

ViS(d)
~
ViS(d)

40

50

1

10

20

30
d (m)

1

0.8

0.6

0.4

0.2

i

S
V
e
h
T

t=1/6 s

t=1 s

0

1

10

20

30
d (m)

ViS(d)
~
ViS(d)

40

50

1

0.8

0.6

0.4

0.2

)
d
(

S
V

i

0

1

10

20

30
d (m)

VK
Lite
Office

40

50

ALG-FX-S
ALG-ML
ALG-ViS

1

F
D
C

0.5

0
20
80
Required bandwidth (Mbps)

60

40

(a) Headset VR

(b) Phone-based VR

Fig. 10: Analytical and simulation results of the ViS for VK
game in headset VR and phone-based VR.

Fig. 11: The analytical ViS
versus d for different games in
desktop VR.

Fig. 12: CDF of required band-
width for ALG-ViS and the base-
lines in the edge-assisted system.

ALG-FX-T
ALG-ML
ALG-ViS

1

F
D
C

0.5

0
0.85

0.9
0.95
SSIM score

1

1

F
D
C

0.5

0

ALG-FX-S
ALG-ML
ALG-ViS

0
15
Frame processing time (ms)

10

5

1

F
D
C

0.5

0

0

1

F
D
C

0.5

0

0

ALG-FX-S
ALG-ML
ALG-ViS

50
CPU usage (%)

100

ALG-FX-S
ALG-ML
ALG-ViS

50
GPU usage (%)

100

(a) VR frame quality

(b) Frame processing time

(c) CPU usage

(d) GPU usage

Fig. 13: CDFs of 4 different metrics for ALG-ViS and the baselines in the local rendering system.

We hypothesize that higher-complexity games may potentially
be more engaging, which encourages the users to explore them
in a slower, more deliberate fashion. The observed differences
between the ViS for different games suggest that it is important
to take speciﬁc game’s pose characteristics into account.

B. Performance of ALG-ViS

We implement ALG-ViS and a set of baselines in two VR
rendering systems, an on-device (“local”) one and one sup-
ported by edge computing. Both systems display the generated
frames in an Oculus Quest 2 VR headset [45]. On-device
rendering system is implemented with build 30.0; its target
frame rate is set to the default 72 fps. The edge-assisted
system generates the VR frames in a Lenovo laptop, with
Unity Engine 2019.1.14f1 and Google VR SDK [46], and
sends them to the headset over IEEE 802.11ac WiFi. The
laptop is equipped with an AMD Ryzen 7 4800H CPU and
an NVIDIA GTX 1660 Ti GPU. The target frame rate of
this system is set to the laptop’s default 60 fps. To ensure
reproducibility, for all algorithms our evaluation is based on
replaying 30 min of headset pose trajectories we collected
(see §III-A). We examine the performance for all 3 games,
and present the results for Lite, which are representative. We
examine the required bandwidth for the edge-supported system
in Fig. 12, and the achieved SSIM, frame processing time,
and CPU and GPU usage (monitored using the OVR Metrics
Tool [47]) for the local system in Fig. 13.

We compare ALG-ViS to different approaches with the
same reference frame interval R: ALG-FX-S, ALG-FX-T, and
ALG-ML. In ALG-ViS, ViStr is set to 0.945 to ensure high
ViS values for background contents. We set R as the minimum
integer such that dtr < df p for every frame. In ALG-FX-S
and ALG-FX-T, dtr is determined by the number of triangles
Ntr in the VR frame, similar to the near and far background

qNtr, df p}

splitting in [10]. Speciﬁcally, dth = max
, where
{
q is ﬁxed for each game. For a fair comparison, in ALG-FX-
T, we set q to make the average frame processing time of
ALG-FX-T and ALG-ViS the same to compare the SSIM; in
ALG-FX-S, we set q to make the average SSIM the same
as ALG-ViS when comparing other metrics. In ALG-ML,
we adopt an online ridge regression model, which has been
shown to achieve state-of-the-art accuracy in 360◦ video pose
prediction [24], [25]. Following [11], we predict x, y, z, θ, and
φ separately. We set the history and prediction windows as in
[11]. We split the VR contents by calculating the ViS of the
reference frame and the predicted VR frame.

The CDF of the required bandwidth of the edge-assisted
rendering system shown in Fig. 12 demonstrates that ALG-ViS
requires less bandwidth on average than ALG-FX-S (11.3%
difference; ALG-FX-S consistently transmits more pixels to
maintain the same SSIM as ALG-ViS), and has signiﬁcantly
smaller bandwidth variance than ALG-ML (88.4% reduction).
Although ALG-ML can save bandwidth when it accurately
predicts the viewport pose, the required bandwidth increases
drastically when the prediction is erroneous. Generating less
bursty trafﬁc, ALG-ViS prevents transmission resource over-
provisioning and potential TCP incast problems in edge-
assisted VR systems, which helps supporting these systems
better when compared with ALG-ML.

Fig. 13 shows the CDFs of the SSIM, frame processing
time, and CPU and GPU usage for the local rendering system.
It shows that ALG-ViS improves frame quality and frame
processing time while consuming fewer resources. The ALG-
ViS ensures high average SSIM, outperforming ALG-FX-T by
3.2% and ALG-ML by 5.9%. The ALG-ML exhibits lower
frame quality. This is because ALG-ML splits the foreground
and background contents based on the predicted pose, and
prediction errors lead to severe performance degradation [8].

 
 
The ALG-ViS decreases the frame processing time by 16.1%
and 33.4% compared to ALG-FX-S and ALG-ML. The CPU
usage of ALG-ML is 20.5% higher than that of ALG-ViS due
to the extra computation required to tune the regularization
parameter and conduct the pose prediction. The GPU usage
of ALG-FX-S is 33.7% higher than that of ALG-ViS because
ALG-FX-S classiﬁes more VR contents as foreground contents
on average. These results demonstrate that
the developed
ALG-ViS is lightweight yet effective.

VI. CONCLUSION

In this paper, we ﬁrst propose a viewport pose model for VR
systems based on the experimental measurements. We apply
the pose model to adaptively select background contents that
are reused across VR frames to reduce the communication
and computation resource consumption, via quantifying the
similarity of pixels across VR frames. Numerical results verify
the pose model and the inter-frame pixel similarity analy-
sis. Oculus Quest 2-based implementations of our adaptive
background content selection approach show that it improves
the image quality by 5.6% and reduces the variance of the
required bandwidth by 88.4% compared to the method based
on viewport pose prediction.

ACKNOWLEDGMENTS

This work is supported in part by NSF grants CSR-1903136,
CNS-1908051, and CAREER-2046072, and by an IBM Fac-
ulty Award.

REFERENCES

[1] S. M. LaValle, Virtual Reality. Cambridge University Press, 2016.
[2] 3GPP.

speciﬁcation group services

(2021) Technical

and sys-
in 5G, TR 26.928 V16.1.0.

tem aspects; Extended reality (XR)
https://www.3gpp.org/ftp/Specs/archive/26 series/26.928.
is

[3] PricewaterhouseCoopers.

Seeing

(2019)

believing.

https://www.pwc.com/seeingisbelieving .

[4] E. Cuervo, K. Chintalapudi, and M. Kotaru, “Creating the perfect
illusion: What will it take to create life-like virtual reality headsets?” in
Proc. ACM HotMobile, 2018.

[5] Z. Tan, Y. Li, Q. Li, Z. Zhang, Z. Li, and S. Lu, “Supporting mobile
VR in LTE networks: How close are we?” in Proc. ACM SIGMETRICS,
2018.

[6] J. Jeong, S. Lee, I. Ryu, T. Le, and E. Ryu, “Towards viewport-dependent
6DoF 360 video tiled streaming for virtual reality systems,” in Proc.
ACM MM, 2020.

[7] L. Liu, R. Zhong, W. Zhang, Y. Liu, J. Zhang, L. Zhang, and
M. Gruteser, “Cutting the cord: Designing a high-quality untethered VR
system with low latency remote rendering,” in Proc. ACM MobiSys,
2018.

[8] Y. Li and W. Gao, “DeltaVR: Achieving high-performance mobile VR
dynamics through pixel reuse,” in Proc. ACM/IEEE IPSN, 2019.
[9] Y. Li and W. Gao, “MUVR: Supporting multi-user mobile virtual reality
with resource constrained edge cloud,” in Proc. IEEE/ACM SEC, 2018.
[10] J. Meng, S. Paul, and Y. C. Hu, “Coterie: Exploiting frame similarity
to enable high-quality multiplayer VR on commodity mobile devices,”
in Proc. ACM ASPLOS, 2020.

[11] X. Liu, C. Vlachou, F. Qian, C. Wang, and K.-H. Kim, “Fireﬂy:
Untethered multi-user VR for commodity mobile devices,” in Proc.
USENIX ATC, 2020.

[12] P. Srivastava, A. Rimzhim, P. Vijay, S. Singh, and S. Chandra, “Desktop
learning,”

VR is better than non-ambulatory HMD VR for spatial
Frontiers Front. Robot. AI, vol. 6, no. 50, pp. 1–15, 2019.

[13] Y. Liu, J. Liu, A. Argyriou, L. Wang, and Z. Xu, “Rendering-aware
VR video caching over multi-cell MEC networks,” IEEE Trans. Veh.
Technol., vol. 70, no. 3, pp. 2728–2742, 2021.

[14] Unity Technologies. (2021) The leading platform for creating interactive,

real-time content. [Online]. Available: https://unity.com/

[15] Z. Lai, Y. C. Hu, Y. Cui, L. Sun, and N. Dai, “Furion: Engineering
high-quality immersive virtual reality on today’s mobile devices,” in
Proc. ACM MobiCom, 2017.

[16] I.-H. Hou, N. Z. Naghsh, S. Paul, Y. C. Hu, and A. Eryilmaz, “Predictive
scheduling for virtual reality,” in Proc. IEEE INFOCOM, 2020.
[17] C. Bettstetter, G. Resta, and P. Santi, “The node distribution of the
random waypoint mobility model for wireless ad hoc networks,” IEEE
Trans. Mobile Comput., vol. 2, no. 3, pp. 257–269, 2003.

[18] S. Ioannidis and P. Marbach, “A brownian motion model for last

encounter routing,” in Proc. IEEE INFOCOM, 2006.

[19] I. Rhee, M. Shin, S. Hong, K. Lee, and S. Chong, “On the Levy-walk

nature of human mobility,” in Proc. IEEE INFOCOM, 2008.

[20] Y. S. Ero˘glu, Y. Yapıcı, and I. G¨uvenc¸, “Impact of random receiver
orientation on visible light communications channel,” IEEE Trans.
Commun., vol. 67, no. 2, pp. 1313–1325, 2019.

[21] M. D. Soltani, A. A. Purwita, Z. Zeng, H. Haas, and M. Safari,
“Modeling the random orientation of mobile devices: Measurement,
analysis and LiFi use case,” IEEE Trans. Commun., vol. 67, no. 3, pp.
2157–2172, 2019.

[22] X. Lin, R. K. Ganti, P. J. Fleming, and J. G. Andrews, “Towards
understanding the fundamentals of mobility in cellular networks,” IEEE
Trans. Wireless Commun., vol. 12, no. 4, pp. 1686–1698, 2013.
[23] V. Sitzmann, A. Serrano, A. Pavel, M. Agrawala, D. Gutierrez, B. Masia,
and G. Wetzstein, “Saliency in VR: How do people explore virtual
environments?” IEEE Trans. Vis. Comput. Graph., vol. 24, no. 4, pp.
1633–1642, 2018.

[24] F. Qian, B. Han, Q. Xiao, and V. Gopalakrishnan, “Flare: Practical
viewport-adaptive 360-degree video streaming for mobile devices,” in
Proc. MobiCom, 2018.

[25] S. Afzal, J. Chen, and K. Ramakrishnan, “Viewing the 360◦ future:
Trade-off between user ﬁeld-of-view prediction, network bandwidth, and
delay,” in Proc. IEEE ICCCN, 2020.

[26] X. Hou and S. Dey, “Motion prediction and pre-rendering at the edge
to enable ultra-low latency mobile 6DoF experiences,” IEEE Open J.
Commun. Soc., vol. 1, pp. 1674–1690, 2020.

[27] X. Hou, S. Dey, J. Zhang, and M. Budagavi, “Predictive adaptive
streaming to enable mobile 360-degree and VR experiences,” IEEE
Trans. Multimedia., vol. 23, pp. 716–731, 2021.

[28] X. Feng, Z. Bao, and S. Wei, “LiveObj: Object semantics-based viewport
prediction for live mobile virtual reality streaming,” IEEE Trans. Vis.
Comput. Graphics, vol. 27, no. 5, pp. 2736–2745, 2021.

[29] E. J. David, J. Guti´errez, A. Coutrot, M. P. da Silva, and P. L. Callet,
“A dataset of head and eye movements for 360◦ videos,” in Proc. ACM
MMSys, 2018.

[30] W. Lo, C. Fan, J. Lee, C. Huang, K. Chen, and C. Hsu, “360◦ video
viewing dataset in head-mounted virtual reality,” in Proc. ACM MMSys,
2017.

[31] J. Chakareski, M. Khan, T. Ropitault, and S. Blandino, “6DOF virtual
reality dataset and performance evaluation of millimeter wave vs. free-
space-optical indoor communications systems for lifelike mobile VR
streaming,” in Proc. IEEE ACSSC, 2020.

[32] B. Han, Y. Liu, and F. Qian, “ViVo: Visibility-aware mobile volumetric

video streaming,” in Proc. ACM MobiCom, 2020.

[33] Unity Technologies. (2015) Viking village. https://assetstore.unity.com/

packages/essentials/tutorial-projects/viking-village-29140.

[34] Xiaolianhua Studio. (2017) Lite. https://assetstore.unity.com/packages/

3d/environments/fantasy/make-your-fantasy-game-lite-8312.

[35] Unity Asset Store. (2020) Ofﬁce. https://assetstore.unity.com/packages/

3d/environments/snaps-prototype-ofﬁce-137490.

[36] B. Everitt and A. Skrondal, The Cambridge dictionary of statistics, 4th

ed. Cambridge University Press, 2010.

[37] E. Lengyel, Foundations of Game Engine Development, Volume 2:

Rendering. Terathon Software LLC, 2019.

[38] Supernatural.

(2021) Burn More. Sweat More. Have More Fun.

https://www.getsupernatural.com/.

[39] FITXR. (2021) A new way to exercise. https://ﬁtxr.com/.
[40] Oculus for Developers. (2021) Oculus Integration for Unreal Engine ba-

sics. https://developer.oculus.com/documentation/unreal/unreal-engine-
basics/.

[41] L. Terenzi and P. Zaal, “Rotational and translational velocity and
acceleration thresholds for the onset of cybersickness in virtual reality,”
in Proc. AIAA Scitech Forum, 2020.

Android.
android.
[47] Oculus.

[42] K. A. Turano, D. R. Geruschat, F. H. Baker, J. W. Stahl, and M. D.
Shapiro, “Direction of gaze while walking a simple route: Persons
with normal vision and persons with retinitis pigmentosa,” Lippincott
Williams & Wilkins Optom. Vis. Sci., vol. 78, no. 9, pp. 667–675, 2001.
[43] M. A. Hollands, A. E. Patla, and J. N. Vickers, ““Look where you’re
going!”: Gaze behaviour associated with maintaining and changing the
direction of locomotion,” Springer Exp. Brain Res., vol. 143, no. 2, pp.
221–230, 2002.

[44] R. Tsai, “A versatile camera calibration technique for high-accuracy 3D
machine vision metrology using off-the-shelf TV cameras and lenses,”
IEEE J. Robot. Autom., vol. 3, no. 4, pp. 323–344, 1987.
[Online].

[45] Oculus.

Available:

Oculus

(2021)

Quest

2.

https://www.oculus.com/quest-2/

[46] Google VR. (2021) Quickstart for Google VR SDK for Unity with
https://developers.google.com/vr/develop/unity/get-started-

(2021) OVR Metrics Tool. https://developer.oculus.com/

downloads/package/ovr-metrics-tool/.

APPENDIX

A. Proof of Lemma 1

−

j′

j′

j′

∃

j′

Tn +

Tn +

n=0
P

the observation interval at
j′
1

When Case 1 holds, there are two possible ways to end
N+,

time ts + ∆t. If

Sn < ts + ∆t <

∈
Sn, the obser-
n=0
vation end time ts + ∆t lies in a pause interval. Otherwise,
P
ts + ∆t lies in a ﬂight interval. If it lies in a pause interval, the
movement includes some number of complete ﬂights without
any fractional ﬂights. We do not observe any movement when
n = 1. For n > 1, An is the event that ts + ∆t lies in a pause
1 complete ﬂights in [ts, ts + ∆t].
interval and there are n
This leads to the ﬁrst summation in (1).

n=0
P

n=0
P

−

If ts + ∆t lies in a ﬂight interval, the movement includes
some number of complete ﬂights and a fraction of the last
ﬂight. Bn is the event that ts + ∆t lies in a ﬂight interval and
there are n
1 complete ﬂights in [ts, ts + ∆t]. This leads to
the second summation in (1).

−

It only remains to calculate E

ψk1(An)
to conclude the proof. We start with E

(cid:2)

and E
ψk1(An)
(cid:3)
(cid:2)
1
j+n
(cid:2)
−

ψk1(Bn)
. Given
(cid:3)
2k

(cid:3)
Tiei

.

Ti, ei, the k-th moment of ψ is given by v2k

i=j+1
According to the ﬂight direction model in §III-C,
P
rection of ei
Thus, Eei,ek [eiek]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
the di-
(cid:13)
(cid:13)
(cid:13)
(cid:13)
is i.i.d. uniformly distributed on [0, 2π).
(cid:13)
(cid:13)
to 1 for i = k and 0 for
1

is equal

j+n

2k

i

= k. Thus, expanding

−

Tiei

and averaging over

· · ·

i=j+1
P

(cid:13)
(cid:13)
1, only the terms in which all Tiei
, ej+n
ej+1, ej+2,
(cid:13)
(cid:13)
−
(j + 1 6 i 6 j + n
1) have even powers are non-zero.
(cid:13)
−
. Further,
The number of these non-zero terms is
let h(h 6 n
1) denote the number of non-zero complete
pause intervals (excluding the ﬁrst and last fractional pause
intervals). Denote the set of the non-zero complete pause

n+k
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−
2

(cid:1)

(cid:0)

−

2

intervals as

S∗i }

{

, 1 6 i 6 h. Let s = ∆t

−

ψk1(An)

be the sum of pause intervals in the observation interval.
E
is given by (6), shown at the top of next page.
Using these observations, the expressions for E
can
be obtained similarly, which completes the proof.

ψk1(Bn)

(cid:3)

(cid:2)

j+n

−

1

Ti

i=j+1
P

(cid:2)

(cid:3)

B. Proof of Lemma 3

Let Γn be the end time of the n-th ﬂight, i.e., Γn =
n
1
N
i=0 Si +
−
such that Γn 6 T < Γn+1, and we can upper and lower bound
P
P
pT according to

n
i=0 Ti. Then, for any T > 0, there exists n

∈

n

Ti

i=0
P
Si +

E

n





i=0
P

Ti

n

i=0
P







6 pT 6 E





n+1

Ti

i=0
P
Si +

.






Ti

n+1

i=0
P

1

n

−

i=0
P

The lower bound includes an extra pause interval between
Γn and Γn+1 by ignoring any possible fractional ﬂight du-
ration. On the other hand, the upper bound includes one
complete ﬂight duration between Γn and Γn+1 by ignoring
Sn.

→ ∞

When T

, using Lebesgue’s dominated convergence
theorem to replace the order of limit and expectation operators
and the law of large numbers (as T grows large, n tends to
inﬁnity), it can be seen that both upper and lower bounds
converge to p = λ/(1
c)
c)+µ .
−
λ/(1
−

C. Proof of Theorem 2

Seen from Fig. 9, the overlapping azimuth angle is

2 −

wf v
(cid:0)

wf v
2

−

#

(cid:1)

min(φref , φnov) + wf v

max(φref , φnov)

E∆φ

|

0

−

−

−

wf v

=1

∆φ

(a)
=1

wf v |
wf v

∆φp∆φ(∆φ)d(∆φ) = pφ
f .

p∆φ(∆φ)d(∆φ)

"
1
wf v Z
2
wf v Z
where ∆φ
min(φref , φnov) + ωf v
φref + ωf v
φnov −
2 −
min(φref , φnov) + ωf v
(cid:0)
φnov + ωf v
φref −
2 −
the PDF of ∆φ obtained in Section III-B:
(cid:0)

2 −
ωf v
2
(cid:0)
max(φref , φnov)
2 −
(cid:1)
ωf v
2
(cid:0)
(cid:1)

max(φref , φnov)

φref −

=

if

is

(a)

and

φnov,

(7)
because
=
and
=
−
if φref > φnov. p∆φ(∆φ) is

φnov
(cid:1)

ωf v
2

ωf v
2

φref

−
<

(cid:1)

p∆φ(∆φ)

1
2bl

exp

|

−

∆φ
|bl
(cid:17)
exp
−
(cid:16)
1+exp

, ∆t < β1
|∆φ|−µlo
blo
(cid:17)
|∆φ|−µlo
blo

(1

−

(cid:16)
pl)

blo

1
2bl

(cid:16)
exp

+pl

−
(cid:16)
π
1+exp
blo
−
wf v
2π , ∆t > β2.

(cid:16)

(cid:16)

−
|∆φ|
(cid:16)
bl

−1
(cid:17)

1

−

(cid:17)(cid:17)

=






2

2

1+exp

(cid:17)(cid:17)

(cid:16)
, β1 6 ∆t < β2

(cid:16)

1

π
blo

−

−1

(cid:17)(cid:17)

1

−

Substituting p∆φ(∆φ) into (7), we obtain the results for pφ
f
(the fraction of overlapping azimuth angles) in (4).

6
E

ψk1(An)

=

2

n

−

n

2

−
h

(1

(cid:19)

−

c)hcn
−

2

−

hv2k

1[n = 2]

(

·

0
Z

∆t

(∆t

−

s)2k + 1[n > 3]

∆t

s

∆t

−

·

0
Z

0
Z

· · ·

2

2k

j+n

−

2

i=j+1
X

T ′i

ei +

∆t

s

−

−





j+n

−

i=j+1
X
h

µn

−

1e−

µ(∆t

−

s)λh+2e−

λ

i=1
P

T ′i

i +S′
S∗

ej+n
−

j +S′

j+n−1

1

dT ′j+1 · · ·




!dS′jdS∗1 · · ·

dT ′j+n

−

2


dS∗hdS′j+n


1 ·

−

(6)

ds

c)hcn
−

2

−

hv2k

(1

(cid:19)

−

0
Z

1e−

µ∆tλh+1

−

n + k
n

−

(cid:18)

2

−
2

(cid:19)

(∆t

2(2k)!
s)2k+n
−
2)!

−
(2k + n

−

e−

(λ

−

µ)s sh+1

(h + 1)!

ds

1
ε2
0 ψe−

τ ψτ −

1

2 dτ

is smaller

(cid:21)

∞

"Z

1
ε2

ψe−

τ ψdτ

#

The approximation error Eψ
than ε

, i.e., we have

(cid:20)

R

1
ε2

√π Mψ

0 < Eψ

−
(cid:0)

∞

"Z

1
ε2

(cid:1)
ψe−

τ ψτ −

1
2 dτ

< εEψ

#

= εEψ

ψ
ε2

e−

Mψ
h
which concludes the proof.

i

= ε

1
ε2

(cid:19)

−

(cid:18)

< ε

Xh=0 (cid:18)
T ′
i

Eej+1,ej+2,

,ej+n−1 



···

s

−

S′

j −

S∗
i

h−1

i=1
P

∞

s
Z

−

S′

j −






h

S∗
i

∆t

i=1
P
µn

µλgn

−

2,h,h+2,k,2k+n+h+1

∆t

−

s

−

(cid:3)
j+n−3

i=j
P

(cid:2)

0
Z

s

S′
j

s

−

0
0 Z
Z
2
n

−

n

Xh=0 (cid:18)
n + k
n

(cid:18)

−

=

=

2

2

−
h

−
2

· · ·

0
Z

2

n

−

(cid:19)

Xh=0

Similarly, we have

min(θref , θnov) + wf v

max(θref, θnov)

2 −

wf v
(cid:0)

wf v
2

−

#

(cid:1)

wf v

2
wf v Z
bl,θ −

0

−

∆θp∆φ(∆θ)d(∆θ)

exp

−

(cid:16)

wf v
bl,θ
wf v

(bl,θ + wf v)

(cid:17)



E∆θ

"

=1

−

=

1




(8)

is the PDF of ∆θ

∆φ
|
|
bl,θ

where p∆θ(∆θ) = 1
2bl,θ
obtained in §III-B.

exp

−

(cid:16)
Combining fractions of overlapping azimuth angle and
overlapping polar angle in (4) and (8), we get the ﬁnal result
of ViSf ov.

(cid:17)

D. Proof of Theorem 3

wf v

2 , wf v

2
−
the same as φref
(cid:0)

. This is be-
ϑ is uniformly distributed on
cause the walking direction is
(see
§III-D), and the included angle of −−−−−−→Xref X3D and pos-
itive direction of X-axis
is uniformly distributed on
2 , φref + wf v
. Taking the average over ϑ and
φref
substituting m(1), (3) is rewritten as ViSdst(d) = 1 +
(cid:0)
4 sin( wf v
2 )
m(1)
is calculated as
d2 −
wf v d
Eψ
= 1
√ψ
√π

, where Eψ
τ ψτ −
2 dτ

(cid:1)
√ψ

Eψ

Eψ

√ψ

wf v

−

(cid:1)

2

1

0 ψe−
∞
(cid:2)
(cid:3)
hR
Eψ
by 1
√π

1
ε2
0 ψe−

(cid:3)

. We further approxi-
(cid:2)
i
τ ψτ −

with the error

2 dτ

1

mate Eψ
(cid:2)
(cid:3)

√ψ

Eψ

1
√π
as

(cid:3)
ψe−

τ ψτ −

1

2 dτ

(cid:2)
∞1
ε2

(cid:20)

R
. Eψ
i

1
ε2
0 ψe−

(cid:21)
τ ψτ −

1

2 dτ

(cid:20)

R

is given

(cid:21)

hR

Eψ

"Z
0

1
ε2

ψe−

τ ψτ −

1

2 dτ

1
ε2

(

−

ψ

#
τ ψ)i
i!

Eψ

=

∞

i=0 (
X

"Z
0

τ −

1

2 dτ

#)

∞

=

g(i).

i=0
X

 
