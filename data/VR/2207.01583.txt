2
2
0
2

l
u
J

8
1

]

V
C
.
s
c
[

3
v
3
8
5
1
0
.
7
0
2
2
:
v
i
X
r
a

LaTeRF: Label and Text Driven Object
Radiance Fields

Ashkan Mirzaei, Yash Kant, Jonathan Kelly, and Igor Gilitschenski

University of Toronto

Abstract. Obtaining 3D object representations is important for cre-
ating photo-realistic simulations and for collecting AR and VR assets.
Neural fields have shown their effectiveness in learning a continuous vol-
umetric representation of a scene from 2D images, but acquiring ob-
ject representations from these models with weak supervision remains
an open challenge. In this paper we introduce LaTeRF, a method for ex-
tracting an object of interest from a scene given 2D images of the entire
scene, known camera poses, a natural language description of the object,
and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends
the NeRF formulation with an additional ‘objectness’ probability at each
3D point. Additionally, we leverage the rich latent space of a pre-trained
CLIP model combined with our differentiable object renderer, to inpaint
the occluded parts of the object. We demonstrate high-fidelity object ex-
traction on both synthetic and real-world datasets and justify our design
choices through an extensive ablation study.

Keywords: image-based rendering, neural radiance fields, 3D recon-
struction, 3D computer vision, novel view synthesis

1

Introduction

Extracting a partially-occluded object of interest (OOI) from a 3D scene re-
mains an important problem with numerous applications in computer vision and
elsewhere. For example, OOI extraction enables the creation of faithful photo-
realistic simulators for robotics, supports the creation of digital content (e.g.
animated movies), and facilitates the building of AR and VR tools. Although 3D
scene reconstruction from 2D images has been extensively explored [23,14,46,35,18],
the ability to extract semantically consistent and meaningful objects from com-
plex scenes remains an open challenge.

Early approaches [34,56] attempted to segment objects from real images, but
could not extract their 3D geometry and did not consider the underlying im-
age formation process. Recent work has attempted to disentangle object-centric
representations from a scene by learning to decompose the scene into a graph
and train object representations at the leaf nodes [27], or via the use of joint 3D
semantic segmentation and neural volumetric rendering [55]. The approach de-
scribed in [27] is specifically designed for automotive data and is not applicable

 
 
 
 
 
 
2

A. Mirzaei et al.

Fig. 1: An overview of LaTeRF showing the extraction of a plate. The method
takes as input 2D images with camera parameters and poses, a few manually
annotated pixel labels corresponding to the object, and labels pointing to the
places in the images that do not belong to the object (foreground or background
parts). LaTeRF is able to extract the object from the scene while also filling the
missing details of the object that are not visible in the input views.

to object instances from other categories, whereas [55] leads to non-photorealistic
object extraction due to training constraints required for real-time deployment.
Moreover, none of these works allow for reasoning about occlusions of the OOI
in the scene. Approaches that rely on unsupervised discovery of 3D objects from
a single image [51,38] typically require optimizing a non-trivial loss function and
do not work well with high-resolution real-world scenes. Other approaches [47]
require large-scale prior category-level videos that are expensive to collect and
might not generalize well to novel categories.

We propose LaTeRF, a novel object extraction method that requires a set
of 2D images (with known camera parameters) containing the OOI mixed with
other clutter, a textual description of the OOI, and a minimal set of pixel an-
notations distinguishing the object and non-object boundaries. We assign an
objectness probability to each point in the 3D space and use this to guide the
disentanglement of the OOI and non-object parts in the scene during training.
Once trained, we use an objectness threshold to filter points belonging to the
OOI while ignoring non-object points. Moreover, we find that reasoning about
occlusion is an important aspect of extracting objects without artifacts (such
as holes, or clutter). Thus, we leverage CLIP [31], a large cross-modal vision-
language model, for formulating a loss to fill in the occluded parts using supervi-
sion derived from a textual prompt. Combining these two loss functions we can
perform high-fidelity object extraction from real-world. An overview of our ap-
proach is visualized in Figure 1. We evaluate LaTeRF on real and synthetic data
and study the effects of individual design choices in extensive ablation studies.
We find that, in challenging settings with significant occlusions, LaTeRF leads
to 11.19 and 0.63 gains in PSNR and SSIM metrics respectively compared to
Mask+NeRF.

Pixel-Level AnnotationsTextual Object Description "A plain empty plate" CLIP Loss Classification Loss Reconstruction Loss   Input ImagesLaTeRFObject of Interest Non-object Model OutputsLaTeRF: Label and Text Driven Object Radiance Fields

3

2 Related Work

3D object reconstruction: Understanding 3D structure from 2D images has
been a long-standing problem in computer vision. Several studies have tried to
learn category-specific information from a set of videos [10,47] and use this infor-
mation to generalize to new instances within the same category. Recent work [17]
captures objects from images with varying illumination and backgrounds, but is
unable to inpaint potential missing parts and relies on an object mask for every
input image. Neural scene graphs have been utilized for scene decomposition into
background and object instances for automotive applications [27]. Another line
of work involves recovering the 3D structure of a scene from a single image only
and in an unsupervised manner [51,38], but is limited to simple scenarios and
performs poorly in complex scenes. Although these methods can recover the 3D
shape and appearance of an object with a few 2D images, most rely on training
on a large set of either category-specific or general objects; collecting such data
can be expensive for certain categories. We aim to overcome the need for prior
training and instead employ a minimal set of test-time clues given by a human
annotator to extract a photo-realistic object radiance field from the scene.
Representing scenes with neural fields: Volumetric rendering of 3D objects
and scenes via neural fields has attracted significant attention recently [41]. In the
past few years and based on differentiable volume rendering [42,9], NeRF-style
methods [23,53,25,33,28,49,50] have achieved impressive results in reconstructing
high-quality 3D models and novel views learned from 2D inputs of bounded
static scenes. The success of NeRF in capturing the details of scenes is due in
part to the use of positional encoding [43,7] and periodic activations [36,5,37]
that help to increase the model’s capacity. The backbone of our method is a
neural field[23,48] that we use to learn the density and view-dependent color of
the scene and the object simultaneously based on the camera parameters and
input images of the scene containing the object of interest. It is worthwhile to
mention that our method is not limited to a specific neural field method and can
be extended easily to faster [19,40,15] and better-quality NeRFs [2,24].
Semantic segmentation in 3D: Semantic segmentation in 3D has been stud-
ied using multi-view fusion-based representations [1,22,11,20,39,21,44,52] that
require only 2D supervision when training, and a separate 3D mesh at testing
time, unlike implicit methods like ours. Recently, there have been promising at-
tempts to recover 3D semantic maps from 2D inputs using NeRFs. NeSF [45]
recovers the 3D geometry as density fields from posed RGB images and uses
2D semantic maps to train a semantic field to assign a probability of belonging
to each of the semantic classes to every point in the space. Another proposed
approach is to extend the NeRF MLP and add a view-independent semantic
head to it to get the logits for each of the semantic classes [54,55]. Our idea is
conceptually similar to these, and we propose using binary partitioning of 3D
points into two categories (object of interest vs. non-object). In contrast to the
previous works, we introduce a differentiable rendering scheme to only render
the object of interest using these objectness probabilities, which enables us to
fill the visual gaps in the object.

4

A. Mirzaei et al.

Language-guided NeRFs: In the representation learning [3] literature, self-
supervised approaches [6,8,26,12] are some of the most compelling settings due to
the rather cheap availability of unlabelled data. CLIP [31] is a contrastive-based
representation learning method that learns visual and textual feature extraction
by looking at a massive set of diverse image-caption pairs scraped from the in-
ternet. Following the success of CLIP in finding the similarity of image and text
samples, recent works have used it for image generation guided by language [32].
More recently, fusing CLIP with NeRF has been explored. In [14], CLIP similar-
ity of the rendered scene from different views is utilized to reduce the amount of
data needed to train the model. This is based on CLIP’s ability to assign similar
features to different views of a single object. CLIP-NeRF [46] makes use of the
joint image-text latent space of CLIP and provides a framework to manipulate
neural radiance fields using multi-modal inputs. Dream field [13] uses the poten-
tial of CLIP in finding the similarity between a text and an image and optimizes
a NeRF to increase the similarity of its different renders to a text prompt. This
way, starting from a phrase, a 3D object closely representing the text is created.
Motivated by these results, which demonstrate the possibility and benefits of
using CLIP alongside neural radiance fields. We leverage the rich multi-modal
feature space of the pre-trained CLIP model to give our object extractor semantic
information as text. It helps to inpaint the points of the object of interest that
are invisible and obscured by other elements in the scene. Notably, we only
use CLIP as a good, recent instance of a joint image-language model, and our
results can be improved with richer joint embedding functions in the future. The
language module of our method is closely related to dream field [13] but we use
it to generate 3D objects that are consistent with the ones provided in a scene.

3 Background

Neural radiance fields (NeRFs) [23] use a multi-layer perceptron (MLP) to im-
plicitly capture the geometry and visual appearance of a 3D scene. A scene
is encoded as a mapping between the 3D coordinates x and view direction d,
to a view-dependent color c and view-independent density σ using an MLP
f : (x, d) → (c, σ). For simplicity, for every point x and view direction d, we
write the outputs of the MLP as c(x, d) and σ(x), respectively.

Consider a ray r with origin o and direction d characterized by r(t) = o + td
with near and far bounds tn and tf respectively. Similar to the original NeRF
formulation [23], the rendering equation to calculate the expected color for the
ray C(r) is

C(r) =

(cid:90) tf

tn

T (t)σ(cid:0)r(t)(cid:1)c(cid:0)r(t), d(cid:1) dt,

(1)

σ(r(s)) ds(cid:1) is the transmittance. This integral is numer-
where T (t) = exp (cid:0)−(cid:82) t
tn
ically estimated via quadrature. The interval between tn and tf is partitioned
into N equal sections, and ti is uniformly sampled from the i-th section. In this
way, the continuous ray from tn to tf is discretized and the estimated rendering

LaTeRF: Label and Text Driven Object Radiance Fields

5

integral can be obtained as

ˆC(r) =

N
(cid:88)

i=1

(cid:0)1 − exp(−σiδi)(cid:1)ci,

Ti

(2)

j=1 σjδj

(cid:1) is the discretized estimate of the transmittance
where Ti = exp (cid:0) − (cid:80)i−1
and δi = ti+1 − ti is the distance between two adjacent sample points along the
ray. Note that for the sake of simplicity, σ(cid:0)r(ti)(cid:1) and c(cid:0)r(ti), d(cid:1) are shown as
σi and ci, respectively. The rendering scheme given by Eq. 2 is differentiable,
allowing us to train our MLP by minimizing the L2 reconstruction loss between
the estimated color C(r) and the ground-truth color CGT(r). We use a variant
of stochastic gradient descent [16], and minimize the following loss term:

Lrec. =

(cid:88)

r∈R

(cid:13)C(r) − CGT(r)(cid:13)
(cid:13)
2
(cid:13)

,

(3)

where R is a batch of rays sampled from the set of rays where the corresponding
pixels are available in the training data. In this paper, we use the reconstruction
loss Lrec. to ensure the consistency of the extracted object radiance field with
respect to that of the original scene. The goal is to make the resulting object
look similar to the one represented in the 2D input images of the scene, and not
just to generate an object within the same category.

4 Method

In this paper, we propose a simple framework to extract 3D objects as radi-
ance fields from a set of 2D input images. Built on top of the recent advances in
scene representation via neural fields [41,23], our method aims to softly partition
the space into the object and the foreground/background by adding an object
probability output to the original NeRF MLP. We leverage pixel-level annota-
tions pointing to the object or the foreground/background and a text prompt
expressing the visual features and semantics of the object.

4.1 Objectness Probability

We wish to extract an OOI from a neural radiance field guided by a minimal
set of human instructions. Our approach is to reason about the probability of
each point in the space being part of the object. Recent work proposed to ap-
pend a view-independent semantic classifier output [54] to the original NeRF
architecture. Inspired by this, we extend the NeRF MLP to return an additional
objectness score s(x) for every point x in the space. The objectness probability
p(x) is obtained from the MLP as

p(x) = Sigmoid(cid:0)s(x)(cid:1).

(4)

For an overview of the architecture of the MLP used in LaTeRF please refer to
our supplementary material.

6

A. Mirzaei et al.

4.2 Differentiable Object Volume Renderer

The most straightforward approach to render the object using the NeRF model
with the objectness probabilities is to apply a threshold on the probability values
and to zero out the density of all points in the space that have an objectness
probability less than 0.5. In other words, for every point x, the new density
function would be σ′(x) = σ(x)1(cid:0)p(x) ≥ 0.5(cid:1) and the naive object rendering
integral would be

Cobj(r) =

(cid:90) tf

tn

Tobj(t)σ(cid:0)r(t)(cid:1)1

(cid:16)
p(cid:0)r(t)(cid:1) ≥ 0.5

(cid:17)

c(cid:0)r(t), d(cid:1) dt,

where the object transmittance Tobj is defined as

(cid:18)

Tobj(t) = exp

−

(cid:90) t

tn

(cid:16)
σ(cid:0)r(s)(cid:1)1

p(cid:0)r(s)(cid:1) ≥ 0.5

(cid:17)

(cid:19)

ds

.

(5)

(6)

This approach leads to a hard-partitioning of the scene into two categories:
1) The extracted object, 2) background and foreground. Although this method
works in practice and gives a rough outline of the object, if we apply numerical
quadrature to Eq. 5 to evaluate the pixel values for the object, the gradients of
the resulting pixel colors with respect to the weights of the MLP become zero
in most of the domain due to the use of the indicator function. As a result, it
is not possible to define a loss function that enforces properties on the rendered
images of the object (more on this loss function later).

We fix the aforementioned issue with a minor tweak. The transmittance (den-
sity) σ(x) can be interpreted as the differential probability of an arbitrary ray
passing through x being terminated at the particle at point x in the space.
Let Tx represent the event that a ray terminates at point x, that is, we have
σ(x) = P(Tx). In addition, denote the event that location x contains a particle
belonging to the OOI as Ox (p(x) = P(Ox)). With this change, the object can
be rendered with a new density function σobj(x) defined as the joint probability
of Tx and Ox by σobj(x) = P(Tx, Ox). With the assumption that Tx and Ox are
independent, the object density function is:

σobj(x) = P(Tx, Ox) = P(Tx)P(Ox) = σ(x)p(x).

(7)

Given the above equation, the object rendering integral in Eq. 5 can be

changed to

Cobj(r) =

(cid:90) tf

tn

Tobj(t)σ(cid:0)r(t)(cid:1)p(cid:0)r(t)(cid:1)c(cid:0)r(t), d(cid:1) dt,

and the object transmittance Tobj in Eq. 6 becomes

(cid:18)

Tobj(t) = exp

−

(cid:90) t

tn

σ(cid:0)r(s)(cid:1)p(cid:0)r(s)(cid:1) ds

(cid:19)

.

(8)

(9)

LaTeRF: Label and Text Driven Object Radiance Fields

7

Now, similar to the previous quadrature formulation, the integral in Eq. 8 is

approximated as follows,

ˆCobj(r) =

N
(cid:88)

i=1

T obj
i

(cid:0)1 − exp(−σipiδi)(cid:1)ci,

(10)

i = exp (cid:0) − (cid:80)i−1

(cid:1) and
where the discretized object transmittance is T obj
pi = p(cid:0)r(ti)(cid:1). Note that with this new object rendering method, the gradients
are smoother and it is possible to use iterative optimization to minimize common
loss functions defined over the rendered views of the object. Compared to the
previous rendering equation with the indicator function, the new equation can be
interpreted as a soft-partitioning of the space where every point belongs to one of
the two classes (the object or the foreground/background) and the partitioning
is stochastic.

j=1 σjpjδj

4.3 Object Classification using Pixel Annotations

The first source of ‘clues’ obtained from a human annotator to extract the object
is pixel-level annotation information. The user selects a few of the 2D input
images of the scene and, for each of them, chooses a few pixels on the OOI and
a few pixels corresponding to the background or the foreground. After collecting
these data, for a ray r corresponding to one of the annotated pixels, the label t(r)
is defined as 1 if the pixel is labelled as the object, and t(r) = 0 if it is labelled as
either foreground or background. The objectness score S(r) is calculated using
the classical volume rendering principles:

S(r) =

(cid:90) tf

tn

T (t)σ(cid:0)r(t)(cid:1)s(cid:0)r(t)(cid:1) dt.

(11)

The objectness probability for a ray r is obtained as P (r) = Sigmoid(cid:0)S(r)(cid:1).
In our implementation, the numerical estimate of the integral above is deter-
mined as

N
(cid:88)

ˆS(r) =

(cid:0)1 − exp(−σiδi)(cid:1)si,

Ti

(12)

where si = s(cid:0)r(ti)(cid:1). Consequently, the approximated objectness probability is
ˆP (r) = Sigmoid(cid:0) ˆS(r)(cid:1) and following [54], the classification loss Lclf is defined as

i=1

Lclf = −t(r) log ˆP (r) − (cid:0)1 − t(r)(cid:1) log (cid:0)1 − ˆP (r)(cid:1).

(13)

This loss function guides the model to tune s(x) to be large for points that
belong to the object, and to decrease the value for the rest of the points. The
MLP is able to propagate the sparse pixel-level data over the OOI and the
background/foreground [54], resulting in a binary classification of points in the
space. However, this loss function alone is not able to reason about the occluded
parts of the object, which results in an extracted object with holes and other
possible artifacts in areas that are not visible in any of the input images or that
are visible in only a small number of the inputs.

8

A. Mirzaei et al.

4.4 CLIP Loss

To mitigate the shortcomings of the classification loss in inpainting the unseen
parts of the object, we propose the use of a phrase describing the object as the
additional input signal. This signal helps to train the MLP to extract the OOI
from the scene with any holes being filled in. The user is asked to describe the
object in a few words by pointing out its semantics and appearance. Let ttext
represent this user input. Subsequently, the CLIP loss is used to make sure that
the object, when rendered from random views, is similar to the textual clue.

The contrastive language-image pre-training (CLIP) [31] model is a multi-
modal feature extractor trained on a large dataset of image and caption pairs
collected from the Internet. It includes two encoders with a shared normalized
latent (output) space. For an image I and the text t (typically a sentence or
phrase), the similarity of their features is proportional to the probability of the
text t being associated with I. CLIP has a massive and diverse training dataset
and has been shown to be useful in zero-shot transfer applications for visual
and textual data, for example, object recognition and image synthesis. Recent
results suggest that it is applicable to and beneficial for novel view synthesis
tasks [14,13,46].

The object can be rendered from a random view v pixel-by-pixel using the
differentiable object rendering scheme in Eq. 10, resulting in an image Iobj(v)
based on the current estimate of the object by the model. In order to maximize
the similarity between the rendered object Iobj(v) and the clue phrase ttext, the
CLIP loss function LCLIP is defined as

LCLIP = −SimCLIP(Iobj(v), ttext),

(14)

where SimCLIP(I, t) is the similarity of the features of the image I and the
text t extracted by the pre-trained CLIP model. This loss is influenced by the
recent work [13] that utilizes the CLIP model to generate 3D shapes from text
descriptions. Our proposed method begins with a complete scene as a starting
point and already has a template of the object, and so uses the CLIP loss to fix
the obscured regions and surfaces and to reason about the shape and color of
the missing parts.

4.5 Training Details

In summary, the reconstruction loss Lrec. is applied to make sure that the final
object shape and appearance are consistent with the training images of the scene,
while the classification loss Lclf is used to guide the model to find the OOI in the
scene and to resolve any potential ambiguities and redundancies that may occur
when only using the text query. Meanwhile, the CLIP loss LCLIP facilitates the
inpainting of potential holes and occluded parts of the object through a high-
level description of semantics and appearance. The final loss function L used to
train the model is

L = Lrec. + λclfLclf + λCLIPLCLIP,

(15)

LaTeRF: Label and Text Driven Object Radiance Fields

9

where the constants λclf and λCLIP are the hyperparameters of the model. Having
both λclf and λCLIP set to zero (or close to zero) yields a reproduction of the
whole scene without any object extraction.

5 Experiments

Training an object radiance field using LaTeRF can be done with either 360◦
inward-facing posed 2D images or forward-facing posed views of the scene, in
addition to the visual and textual cues for OOI selection. Besides qualitative re-
sults, we study the consequences of applying the different loss terms, the number
of visual cues, and the underlying NeRF representation quality on the quality
of the resulting object asset on our synthetic dataset that includes challenging
scenes with object occlusions.
Datasets In our experiments, we use a
subset of real-world scenes borrowed from
NeRD [4], scenes that we collected our-
selves of real-world objects, and a collec-
tion of synthetic scenes full of occlusions
that we specifically designed to conduct
our quantitative studies. Our synthetic
data are rendered using Blender and con-
sists of views of the scene, ground-truth
object masks, and ideal extracted objects,
as illustrated in Figure 2. The lighting be-
tween different views of most of the scenes
provided by NeRD [4] (scenes which are
partly from the British Museum’s pho-
togrammetry dataset [30]) is inconsistent
because NeRD was designed for the task
of decomposing a scene into the under-
lying shape, illumination, and reflectance
components. As a result, we manually select a subset of the images in these
datasets that are roughly similar in terms of lighting and train LaTeRF on
them.
Baseline To the best of our knowledge, LaTeRF is the only method that is able
to extract and inpaint a 3D object from 2D views of a scene, without having a
huge category-specific multi-view or video data prior to extraction. As a baseline,
we use a NeRF [23,48] model (implemented in PyTorch [29]) trained on the
images of each scene after applying object masks, which we call Mask+NeRF. In
Mask+NeRF, prior to training, we substitute every pixel marked as non-object
in the ground-truth mask with a black pixel and then train a NeRF on the new
masked images.
Metrics For quantitative comparison of the synthetic dataset, we report the
peak signal-to-noise ratio (PSNR) and the structural similarity index measure
(SSIM); in both cases, higher is better.

Fig. 2: Sample data from 3 of our
synthetic scenes.

SceneObject of InterestObject Mask10

A. Mirzaei et al.

5.1 Real-World Scenes

Qualitative results
that demonstrate
LaTeRF extracting objects from real-
world scenes are shown in Figure 3 (the
text queries used in these examples are
”A dark green book”, ”A mug”, and
”A wooden ukulele”, respectively). In
the real-world scenes with textured and
complex backgrounds, we observed that
small particles (points with small densi-
ties) emerge throughout the scene that
are almost invisible from the training
views. These particles blend into the
Fig. 3: Extracting occluded objects.
background or the OOI and disappear,
but when filtering the object, they become visible. This leads to object render-
ings full of noise in the background (see the soft threshold results in Figure 4).
In order to remove this noise, we first make the sampled densities along each
ray smoother by substituting the density of each point with the average of its
density and the density of its direct neighbors; we do this for five steps. After
smoothing the densities, most of the small particles, which are abrupt peaks in
the value of density, become close to zero. As a result, they can be filtered by
applying a hard threshold based on the density and filtering all the points with
a value below this threshold. Later, the object mask is rendered by substituting
the objectness score instead of the color in Eq. 10 and applying the sigmoid
function to the result, while assuming that, along each ray and at an infinitely
far point, there is a particle with high density and low objectness score. As a
result of this process, the denoised object renderings can be obtained by apply-
ing object masks to the noisy images (the result of each step of this denoising
pipeline is shown in Figure 4). Please refer to the supplementary material for
more details on the denoising process.

5.2 Synthetic Scene Evaluations

In order to be able to quantitatively
evaluate our proposed method in dif-
ferent scenarios, we generate synthetic
scenes using Blender, each containing
the following information: 1) Multi-
-view 400 × 400 pixel images of the
scene that contains different objects
including an OOI, 2) the ground-truth images of the OOI so that we can eval-
uate the reconstruction quality of our object selection method, 3) the mask for
the OOI in each of the 100 training images of the scene, used instead of manual
pixel-level annotations for evaluations, 4) and the ground-truth camera intrinsic
and extrinsic parameters for each of the input images.

Fig. 4: The denoising pipeline.

Original SceneLaTeRF renderings1.00.50.0Soft ThresholdHard ThresholdRendered MaskFinal ResultLaTeRF: Label and Text Driven Object Radiance Fields

11

Having ground-truth object masks in the dataset makes it easy to automat-
ically sample a certain number of pixels from the object and from the fore-
ground/background to study the effects of pixel labels on the results. Sample
data from some of the challenging cases of our synthetic dataset can be found in
Figure 2. As shown in the figure, the synthetic scenes have been designed such
that parts of the OOI are obscured with other objects, making the extraction
more challenging.

Table 1: The effect of the label
count on reconstruction quality
of the OOI

Pixel Labels # PSNR↑ SSIM↑
26.93 0.95
16,000,000
0.94
26.90
1,600,000
0.90
26.48
160,000
0.90
26.36
16,000
0.94
26.10
1,600
0.85
26.00
160
0.86
20.81
16

Pixel Label Count We ablate along the
number of pixel labels used during training
to demonstrate the effect on reconstruction
quality. Since we have the ground-truth object
masks in the synthetic dataset, at each step,
we simply sample a different number of pixel
labels from the mask, based on a uniform dis-
tribution, while making sure that the number
of positive labels (labels on the OOI) and nega-
tive ones (non-objects) are the same. Using the
sampled subset of the pixel labels, we extract
the OOI from the scene. In order to calculate
the reconstruction probability, we compare the
results of our method to the ground-truth ren-
derings of the OOI in the dataset from 30 dif-
ferent viewpoints and report the average PSNR and SSIM. Table 1 shows the
results of this experiment with different numbers of pixel labels. Since each of
the 100 training images has 400 × 400 pixels and the ground-truth object mask
includes labels for each of these pixels, there are overall 16, 000, 000 labels for
each of the scenes. As is evident in Table 1, the reconstruction quality of the OOI
stays within the same range when reducing the number of labels from 16, 000, 000
to only 160 labels (an overall of 80 positive labels and 80 negative labels that are
uniformly distributed across 100 training images). Note that a total of 160 labels
can easily be obtained within a few minutes by a human annotator and that this
is much easier than having someone mask out the entire object for every single
training image.

Importance of Boundary Pixel Labels Intuitively, it is clear that not all
pixel labels have the same importance. Pixels close to the boundary of the object
and non-object portions of each view can help the model to extract the OOI with
sharper and more accurate edges. In this experiment, we show that selecting
labels close to the boundaries of the object helps to improve the reconstruction
quality. This way, the human annotator can be asked to spend most of their
time labelling pixels around the edges. Moreover, we can increase the annotator’s
brush size (allow the user to label a larger area at a time instead of a single pixel)
to allow them quickly annotate the points further from the boundaries and then
to focus on the more important boundary labels.

12

A. Mirzaei et al.

Table 2: The effect of the number
of boundary labels on the recon-
struction quality

Table 3: The effect of the loss
terms against Mask+NeRF

Boundary Labels # PSNR↑ SSIM↑
26.78 0.93
16,000
0.91
26.50
1,600
0.92
26.36
160
0.90
24.82
16

PSNR↑ SSIM↑
Model
15.74
Mask+NeRF
12.91
Lrec.
Lrec. + λclfLclf
14.10
Lrec. + λCLIPLCLIP 21.95
LaTeRF (ours)

0.32
0.79
0.82
0.84
26.93 0.95

The boundary labels are computed by applying minimum and maximum
filters with a kernel size of 3 on the object masks, reducing the number of labels
from 16, 000, 000 to less than 500, 000 for our scenes. The pixel labels for this
experiment are then uniformly sampled among these boundary pixels. Table 2
contains the results with different numbers of boundary labels. As is shown in the
table, the reconstruction quality of the model is less affected than the experiment
with uniform labels all around the images when reducing the number of boundary
labels. The results show an improvement of up to 4.81 in the PSNR compared
to the previous experiment, and this happens when using only 16 pixel labels.
It is worth mentioning that this case has 16 labels total across all of the input
images and not 16 labels per image.

Effects of Different Loss Functions As shown in Eq. 15, the loss function
used to train the main model contains three different parts: 1) The reconstruction
loss Lrec., which is the same loss function used in NeRF [23], to let the model
learn the whole scene, 2) the classification loss Lclf, which is defined over the
pixel-level annotations to detect the OOI, 3) and the CLIP loss LCLIP that
guides the model to fill in occluded portions of the OOI. In this section, the
effect of each of these loss terms on the final reconstruction quality of the model
is studied.

The reconstruction loss is the main loss enforcing the 3D consistency of the
learned scene and OOI. Thus, it can not be removed and must be present in all
of the baselines. Our experiment scenarios involve four cases including: 1) Lrec.,
where the classification loss and the CLIP loss are not used. This is similar to the
original NeRF [23] in that there are no clues guiding the model to find the OOI.
2) Lrec. + λclfLclf, where the CLIP loss is ignored and the object is extracted
using the pixel labels only. The model has no clue to help reason about missing
parts in this case. 3) Lrec.+λCLIPLCLIP, where only the text clue is used to ‘push’
the model towards finding the OOI in the scene. 4) Lrec. + λclfLclf + λCLIPLCLIP,
which is our complete proposed method. We compare the results with the base-
line, which is Mask+NeRF.

Table 3 compares the results of this experiment with the aforementioned
baselines. When only the reconstruction loss is used, the semantic head of the
MLP can not be trained and the results are unsatisfactory, as expected. The
model with the classification loss only is unable to produce results on par with

LaTeRF: Label and Text Driven Object Radiance Fields

13

Fig. 5: A qualitative representation of the effectiveness of the CLIP loss in filling
in object parts that are invisible in the input images. The OOI is the empty
plate. The text prompt used here is ”A plain empty plate”.

our proposed method, since the synthetic dataset includes objects of interest
that are partly covered by other objects (by design), while the CLIP loss is the
only semantic clue that is used to fill in the occluded regions. The best result is
obtained when using all of the loss terms together; each of them contributes to
extracting high-quality objects.

Fig. 6: The effect of the weight
of the CLIP loss on the ex-
tracted plate.

Figure 5 visually compares the effect of the
presence or absence of the CLIP loss on the ren-
dered object produced. The scene used in this
example is a challenging scenario where the goal
is to extract the empty plate and remove the
contents of the plate. There are various parts
of the plate’s surface for which no visual in-
formation is available in the 2D input images.
This example demonstrates that the CLIP loss,
making use of a text phrase like ”A plain empty
plate”, is able to reason about the missing area
on the surface of the plate and render a plau-
sible empty plate. In addition, the CLIP loss is able to remove the reflectance
of the hotdogs on the plate to some extent. However, there are still are limited
artifacts on the plate even after applying the CLIP loss. This result is due to
the challenging nature of the task, since a large portion of the plate’s surface is
covered in the input images. As another example, the model has difficulty re-
moving the shadow of the hotdogs from the original plate and tries to complete
the shadow instead of removing it. This issue can be mitigated by increasing the
CLIP loss, but the result is a plate that is less consistent with the one in the
original scene, as shown in Figure 6. Notice that not only the depth of the plate is
reduced compared to the one in the original scene when the CLIP weight λCLIP
is increased to 0.02; the lighting highlight on the plate surface is also rotated and
differs from the original radiance of the plate. For the case without the CLIP
loss (the middle row in Figure 5), the background has been set to black to better
visualize the holes in the plate.

Importance of the Number of Input Views The neural volumetric render-
ing method that is used in LaTeRF is not limited to a certain implementation

Original Scene14

A. Mirzaei et al.

of NeRF. We argue that with better quality and faster neural rendering ap-
proaches, the reconstruction quality and the speed of LaTeRF can be increased.
In this experiment, the effect of the quality of the base neural renderer on fi-
nal object reconstruction quality is studied. In order to mimic the behavior of
a lower-quality neural rendering model, we limit the number of training input
views fed when calculating the reconstruction loss Lrec.. The results in Table 4
indicate that the detail and quality of the rendered object produced by LaTeRF
is closely related to the reconstruction quality of the base NeRF model used to
ensure the consistency of the extracted object with the one present in the scene.
Consequently, as better volumetric rendering approaches for 3D reconstruction
are introduced, LaTeRF will be able to take advantage of them to enable the
creation of better object extractors.

6 Conclusion

Table 4: The effect of the
number of training views
for the calculation of Lrec.

# Views PSNR↑ SSIM↑
26.93 0.96
100
0.95
26.62
80
0.92
26.50
60
0.95
26.20
40
0.94
25.35
20

We have presented LaTeRF, a method to extract
digital objects via neural fields from 2D input im-
ages of a scene, and a minimal set of visual and
natural language clues that guide the model to the
object of interest. The geometry and color of dif-
ferent points are captured via NeRF, while the ob-
jectness probability of scene points is mainly de-
termined via pixel labels provided by the user that
identify whether some of the input pixels belong to
the object of interest or not. Additionally, a CLIP
loss is defined to ensure high similarity between the
rendered images of the object and a text prompt
that expresses the appearance and semantics of the
object, enabling the model to reason about any missing parts of the object. The
effectiveness of each of the components of the training scheme is shown through
our experiments. However, we observed a need for per-scene fine-tuning of the
hyper-parameters λclf and λCLIP . An additional challenge for LaTeRF is to ex-
tract transparent objects or objects with shiny surfaces that reflect other objects.
Moreover, LaTeRF naturally comes with the general limitations associated with
NeRFs, including the need for per-scene training and data intensity. Addition-
ally, the rendered images used to calculate the CLIP loss had to be downsized
to avoid memory shortages. Applying the CLIP loss to higher-resolution images
will result in better inpainting.

References

1. Armeni, I., He, Z.Y., Gwak, J., Zamir, A.R., Fischer, M., Malik, J., Savarese, S.:
3d scene graph: A structure for unified semantics, 3d space, and camera. In: ICCV
(2019) 3

LaTeRF: Label and Text Driven Object Radiance Fields

15

2. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance
fields. arXiv (2021) 3

3. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new
perspectives. In: IEEE transactions on pattern analysis and machine intelligence
(2013) 4

4. Boss, M., Braun, R., Jampani, V., Barron, J.T., Liu, C., Lensch, H.: Nerd: Neural

reflectance decomposition from image collections. In: ICCV (2021) 9, 19

5. Cand`es, E.J.: Harmonic analysis of neural networks. Applied and Computational

Harmonic Analysis (1999) 3

6. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-

trastive learning of visual representations. In: ICML (2020) 4

7. Gehring, J., Auli, M., Grangier, D., Yarats, D., Dauphin, Y.N.: Convolutional

sequence to sequence learning. In: ICML (2017) 3

8. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised

visual representation learning. In: CVPR (2020) 4

9. Henzler, P., Mitra, N.J., Ritschel, T.: Escaping plato’s cave: 3d shape from adver-

sarial rendering. In: ICCV (2019) 3

10. Henzler, P., Reizenstein, J., Labatut, P., Shapovalov, R., Ritschel, T., Vedaldi, A.,
Novotny, D.: Unsupervised learning of 3d object categories from videos in the wild.
In: CVPR (2021) 3

11. Hermans, A., Floros, G., Leibe, B.: Dense 3d semantic mapping of indoor scenes

from rgb-d images. In: ICRA (2014) 3

12. H´enaff, O.J., Srinivas, A., Fauw, J.D., Razavi, A., Doersch, C., Eslami, S.M.A.,
van den Oord, A.: Data-efficient image recognition with contrastive predictive cod-
ing. In: ICML (2020) 4

13. Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided

object generation with dream fields. arXiv (2021) 4, 8

14. Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent

few-shot view synthesis. In: ICCV (2021) 1, 4, 8

15. Jiang, G., Kainz, B.: Deep radiance caching: Convolutional autoencoders deeper

in ray tracing. Computers & Graphics (2021) 3

16. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015) 5, 18

17. Kuang, Z., Olszewski, K., Chai, M., Huang, Z., Achlioptas, P., Tulyakov, S.:
NeROIC: Neural object capture and rendering from online image collections. arXiv
(2022) 3

18. Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radi-

ance fields. In: ICCV (2021) 1

19. Liu, L., Gu, J., Lin, K.Z., Chua, T.S., Theobalt, C.: Neural sparse voxel fields. In:

NeurIPS (2020) 3

20. Ma, L., St¨uckler, J., Kerl, C., Cremers, D.: Multi-view deep learning for consistent

semantic mapping with rgb-d cameras. In: IROS (2017) 3

21. Mascaro, R., Teixeira, L., Chli, M.: Diffuser: Multi-view 2d-to-3d label diffusion

for semantic scene segmentation. In: ICRA (2021) 3

22. McCormac, J., Handa, A., Davison, A., Leutenegger, S.: Semanticfusion: Dense 3d

semantic mapping with convolutional neural networks. In: ICRA (2017) 3

23. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV
(2020) 1, 3, 4, 5, 9, 12, 18, 19, 20

16

A. Mirzaei et al.

24. M¨uller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives

with a multiresolution hash encoding. arXiv (2022) 3

25. Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative

neural feature fields. In: CVPR (2021) 3

26. van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive

predictive coding. arXiv (2019) 4

27. Ost, J., Mannan, F., Thuerey, N., Knodt, J., Heide, F.: Neural scene graphs for

dynamic scenes. In: CVPR (2021) 1, 3

28. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin-

Brualla, R.: Nerfies: Deformable neural radiance fields. In: ICCV (2021) 3

29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:
Pytorch: An imperative style, high-performance deep learning library. In: Wallach,
H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett, R. (eds.)
NeurIPS (2019) 9, 18

30. Pett, D.: BritishMuseumDH/moldGoldCape: First release of the Cape in 3D

(2017). https://doi.org/10.5281/zenodo.344914 9, 20

31. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable
visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.)
ICML (2021) 2, 4, 8

32. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,

Sutskever, I.: Zero-shot text-to-image generation. arXiv (2021) 4

33. Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: Derf: De-

composed radiance fields. In: CVPR (2020) 3

34. Rubinstein, M., Joulin, A., Kopf, J., Liu, C.: Unsupervised joint object discovery

and segmentation in internet images. In: CVPR (2013) 1

35. Sara Fridovich-Keil and Alex Yu, Tancik, M., Chen, Q., Recht, B., Kanazawa, A.:

Plenoxels: Radiance fields without neural networks. In: CVPR (2022) 1

36. Sitzmann, V., Martel, J.N., Bergman, A.W., Lindell, D.B., Wetzstein, G.: Implicit
neural representations with periodic activation functions. In: NeurIPS (2020) 3
37. Sonoda, S., Murata, N.: Neural network with unbounded activation functions is
universal approximator. Applied and Computational Harmonic Analysis (2017) 3
38. Stelzner, K., Kersting, K., Kosiorek, A.R.: Decomposing 3d scenes into objects via

unsupervised volume segmentation. arXiv (2021) 2, 3

39. Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view convolutional

neural networks for 3d shape recognition. In: ICCV (2015) 3

40. Takikawa, T., Litalien, J., Yin, K., Kreis, K., Loop, C., Nowrouzezahrai, D., Ja-
cobson, A., McGuire, M., Fidler, S.: Neural geometric level of detail: Real-time
rendering with implicit 3D shapes. In: CVPR (2021) 3

41. Tewari, A., Thies, J., Mildenhall, B., Srinivasan, P., Tretschk, E., Wang, Y., Lass-
ner, C., Sitzmann, V., Martin-Brualla, R., Lombardi, S., Simon, T., Theobalt, C.,
Niessner, M., Barron, J.T., Wetzstein, G., Zollhoefer, M., Golyanik, V.: Advances
in neural rendering. In: SIGGRAPH (2021) 3, 5

42. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view

reconstruction via differentiable ray consistency. In: CVPR (2017) 3

43. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,

L.u., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 3

LaTeRF: Label and Text Driven Object Radiance Fields

17

44. Vineet, V., Miksik, O., Lidegaard, M., Nießner, M., Golodetz, S., Prisacariu, V.A.,
K¨ahler, O., Murray, D.W., Izadi, S., P´erez, P., et al.: Incremental dense semantic
stereo fusion for large-scale semantic scene reconstruction. In: ICRA (2015) 3
45. Vora, S., Radwan, N., Greff, K., Meyer, H., Genova, K., Sajjadi, M.S.M., Pot,
E., Tagliasacchi, A., Duckworth, D.: Nesf: Neural semantic fields for generalizable
semantic segmentation of 3d scenes. arXiv (2021) 3

46. Wang, C., Chai, M., He, M., Chen, D., Liao, J.: Clip-nerf: Text-and-image driven

manipulation of neural radiance fields. arXiv (2021) 1, 4, 8

47. Wu, S., Jakab, T., Rupprecht, C., Vedaldi, A.: Dove: Learning deformable 3d ob-

jects by watching videos. arXiv (2021) 2, 3

48. Yen-Chen, L.: Nerf-pytorch. https://github.com/yenchenlin/nerf-pytorch/

(2020) 3, 9, 18

49. Yen-Chen, L., Florence, P., Barron, J.T., Rodriguez, A., Isola, P., Lin, T.Y.: iNeRF:

Inverting neural radiance fields for pose estimation. In: IROS (2021) 3

50. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from

one or few images. In: CVPR (2021) 3

51. Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields.

In: ICLR (2022) 2, 3

52. Zhang, C., Liu, Z., Liu, G., Huang, D.: Large-scale 3d semantic mapping using

monocular vision. In: ICIVC (2019) 3

53. Zhang, K., Riegler, G., Snavely, N., Koltun, V.: Nerf++: Analyzing and improving

neural radiance fields. arXiv (2020) 3

54. Zhi, S., Laidlow, T., Leutenegger, S., Davison, A.: In-place scene labelling and
understanding with implicit scene representation. In: ICCV (2021) 3, 5, 7, 18
55. Zhi, S., Sucar, E., Mouton, A., Haughton, I., Laidlow, T., Davison, A.J.: Ilabel:

Interactive neural scene labelling (2021) 1, 2, 3

56. Zhu, J.Y., Wu, J., Xu, Y., Chang, E., Tu, Z.: Unsupervised object class discovery
via saliency-guided multiple class learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2015) 1

18

A. Mirzaei et al.

Fig. 7: The architecture used for LaTeRF is a multi layer perceptron which in-
spired by [54], extends the original NeRF model [23] to contain an additional
output s to reason about the probability of different points in the space being
part of the OOI. As evident in the figure, s is independent of the view d and
only depends on the location x. Following the literature, both location x and
view direction d are passed through a positional encoding function (PE).

A Additional Implementation Details

Our proposed method is implemented in PyTorch [29], and the network is op-
timized using the Adam optimizer [16] to learn the view-dependent radiance
(color) and view-independent densities and objectness probabilities for points in
the space. The network is randomly initialized and trained from scratch for each
scene individually. Hyperparameters related to the NeRF model and the opti-
mizer are set based on the PyTorch implementation [48] in the original NeRF
paper [23]. Training is done on one NVIDIA GeForce RTX 3090 GPU. Due to
the high GPU memory usage for the CLIP loss calculations, the object renders
used for computing LCLIP are 1
3 of the size of the original input images, e.g., for
400×400 pixel synthetic views, the object is rendered at 133×133 pixels and the
CLIP loss is defined over the downsized rendering. Moreover, in order to speed
up training, the CLIP loss is only calculated every 10 steps. For test-time object
renderings, instead of using the soft-partitioning approach, a hard-thresholding
method is used to denoise the background. In the next section, we introduce the
details of this denoising mechanism. An overview of the architecture of the MLP
with the additional objectness score s as output is shown in Figure 7.

For real-world scenes, a minimal user interface is designed to capture the
pixel annotations from an end-user. In this procedure, instead of limiting the
user to give visual cues pixel-by-pixel, we allow brush size changes to enable the
selection of areas corresponding to either the OOI or non-objects. This allows for
quick annotation of points far from the object boundaries, resulting in a better
non-object removal and object discovery in the scene. Using dynamic brush sizes,
we were able to collect millions of pixel-level annotations in just a few minutes.
By being able to change the brush size, the end-user can coarsely label the points
that are not close to the boundaries, and then reduce the annotation area as they
get closer to the boundaries to finely label the more important pixel-level data

256256RGBLogitDensity256256256256256256256256128256256256ViewLocationLocationLaTeRF: Label and Text Driven Object Radiance Fields

19

(i.e., the boundary of the object and the foreground/background, as shown in
the experiments).

B Denoising the Views

As mentioned in section 5.1, we use a post-processing approach to denoise the
rendered images of the objects. Noise is mostly caused by small particles that
emerge in the training of the NeRF model and that blend in with the background
in the training views, but become visible as the non-objects (including the back-
ground) are removed from the scene. In addition, for the points in the space that
are inside of a dense object or behind the background, the objectness score is not
trained well since the densities of the surface points block the training signals.
All these reasons contribute to noisy renderings of the OOI when using LaTeRF
without additional denoising (see soft threshold results in Fig 8). Our first step
to mitigate this issue is to smooth the densities. Because the noisy particles are
mostly steep ‘jumps’ in density compared to the neighbouring points, substitut-
ing the value of every density with the average of its neighbours, including itself,
will smooth these bumps. We repeat this averaging for 5 steps. Afterward, we
filter the points with densities lower than a small threshold (which was set to
0.2 in the example in Fig 8). We call this approach hard thresholding and it is
evident in Fig 8 that it has helped to reduce the noise, but that there are still
some unpleasant gray artifacts in the renderings. However, we do not directly
use hard thresholding to render the RGB images of the object; we only use it to
render the silhouette of the object to mask it out from the soft threshold results.
After applying the hard threshold, we render the object mask by substituting
the color with the objectness scores in Eq. 10 and applying the sigmoid function:

ˆPobj(r) = Sigmoid

T obj
i

(cid:0)1 − exp(−σipiδi)(cid:1)si

(cid:33)
.

(cid:32) N
(cid:88)

i=1

(16)

Note that we assume a background with 0 objectness probability when rendering
the object so that only object points with high densities will dominate this
background and, as such, we obtain the object mask as an output. The rendered
mask is then applied to the soft-threshold results to yield the rendered images
of the object without background artifacts (final results are shown in Figure 8).

C Additional Real-world Results

Novel-view renderings of additional objects (partly borrowed from [23,4]) are
shown in Figure 9. These examples include objects with detailed textures and
geometries and objects with challenging shiny surfaces.

D Relighting the Object

It is possible to leverage the dependence of color of a point on the view direction
to ‘trick’ the learned object radiance field to render the OOI under novel lighting

20

A. Mirzaei et al.

Fig. 8: More examples of the effectiveness of our denoising approach for removing
the background artifacts using rendered objectness probabilities which act as
object masks on the goldcape scene [30].

conditions [23]. The view direction fed to the network to find the radiance (color)
of points in the space can be manually rotated while keeping the camera fixed.
The effect caused by this alteration is similar to changing the lighting of the
scene, and it is possible to fit the novel lighting to be consistent with certain
lighting conditions. Figure 10 shows some of the real-world objects with three
different illumination choices for a given, fixed view.

D.1 Blending the Object in Novel Scenes

The lighting setting can later be optimized with respect to the desired lighting in
a novel scene, making an inserted object look consistent in a new scene. Figure 11
shows an example of placing a 3D asset extracted by LaTeRF into a scene under
two different illumination conditions.

1.00.50.01.00.50.01.00.50.01.00.50.01.00.50.01.00.50.01.00.50.01.00.50.01.00.50.01.00.50.0Soft ThresholdHard ThresholdRendered MaskFinalResultSoft ThresholdHard ThresholdRendered MaskFinal ResultLaTeRF: Label and Text Driven Object Radiance Fields

21

Fig. 9: Additional real-world object extraction results.

Fig. 10: Relighting objects under three
different illumination conditions.

Fig. 11: An example of placing an ex-
tracted object in a scene with two dif-
ferent lighting conditions.

TextOriginal SceneNovel Scene #1Novel Scene #2