1
2
0
2

v
o
N
3
2

]

R
G
.
s
c
[

1
v
8
1
0
2
1
.
1
1
1
2
:
v
i
X
r
a

Distortion Reduction for Off-Center Perspective Projection of
Panoramas

Chi-Han Peng1 and Jiayao Zhang2

1National Chiao Tung University
2King Abdullah University of Science and Technology

Figure 1: (a) Off-center perspective projections of a panorama contain barrel distortions and break realism. (b) With our distortion-reduction
measures, distortions are signiﬁcantly reduced and the visualization is more like a genuine perspective projection in which linear lines in 3D
remain linear in 2D. In comparison, augmenting the panorama with (c) depth or (d) room layouts do not lead to visually convincing results.
Observe the broken lamp wires and doorway in (d). We show the panorama and room layout (predicted by LED2-Net [WYS*21]) and the
camera position and looking direction on the left. The panorama and the ground-truth depth are from the Structure3D dataset [ZZL*20].

Abstract
A single Panorama can be drawn perspectively without distortions in arbitrary viewing directions and ﬁeld-of-views when the
camera position is at the origin. This is a key advantage in VR and virtual tour applications because it enables the user to freely
"look around" in a virtual world with just a single panorama, albeit at a ﬁxed position. However, when the camera moves away
from the center, barrel distortions appear and realism breaks. We propose modiﬁcations to the equirectangular-to-perspective
(E2P) projection that signiﬁcantly reduce distortions when the camera position is away from the origin. This enables users to
not only "look around" but also "walk around" virtually in a single panorama with more convincing renderings. We compare
with other techniques that aim to augment panoramas with 3D information, including: 1) panoramas with depth information
and 2) panoramas augmented with room layouts, and show that our approach provides more visually convincing results.

CCS Concepts
• Computing methodologies → Virtual reality;

1. Introduction

A spherical panorama stores incoming ray intensities toward a ﬁxed
camera point in all possible directions. Commonly, the directions
are sampled on a sphere (centered at the point) in a 2D equirect-
angular format. Using just a single panorama, one can render ac-
curate perspective projections from the camera point in arbitrary
viewing directions and ﬁeld-of-views through sampling strategies
commonly known as the equirectangular-to-perspective (E2P) pro-
jection. In VR and virtual tour applications, this is known as the
3DoF (i.e., rotations along three axis) of a perspective camera in a
virtual environment, albeit at a ﬁxed position.

It is actually straightforward to render perspective projections
from an off-center position, although the results would contain bar-
rel distortions. One way is to map the panorama onto a spheri-
cal mesh through projective texture mapping and draw perspective
views using standard perspective camera models such as OpenGL.
An analytical form of the off-center E2P projection is described
in Section 3.1. The rendering results (see the accompanying video
for examples) are "intuitive" in general, i.e., scenery becomes big-
ger when the camera moves closer and the scenery moves to the
right when the camera moves to the left, and vice versa. Parallax
effect and occlusion on-and-off are lacking. However, by our ob-

 
 
 
 
 
 
2

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

servations, lacking them usually doesn’t ruin the plausibility of the
rendering, unless there are large variations in the depth disparity
(e.g., having objects very close to the camera).

2. Related work

2.1. Panoramic 3D modeling and datasets

We opt for solving another major defect of the rendering - the
barrel distortions (e.g., Figure 1 (a)), which are inevitable when
viewing a textured sphere from an interior position away from
the center. The reason why barrel distortions break realism is that
straight lines in 3D no longer remain straight in the 2D view,
breaching a key assumption of perspective projections.

Such task is often called the novel view synthesis problem in
visual computing. Powerful methods such as [XZX*21] leverage
deep learning (DL) models and comprehensive data to synthesize
novel views of the same scene that extrude parallax effect and
even contain scenery that was previously occluded. These DL-
based methods either take panoramic videos as input (e.g., "one-
shot 3D photography" [KMA*20]), or need a dataset trained on
such ([XZX*21]). In comparison, our method takes a lightweight
approach to the problem (no DL training/inference nor video-
sequence inputs are needed, and is very computationally light),
with a focus on tackling barrel distortions.

We introduce two modiﬁcations to the standard E2P projection
process. First, in order to preserve the linearity of vertically straight
lines in 3D in the 2D view, we re-project the panorama to a cylin-
der so that all vertically straight lines in 3D, which are necessarily
mapped to meridians on the sphere, would be projected to verti-
cally straight lines in 3D again (i.e., the straight lines on the cylin-
der), which are guaranteed to be projected to straight lines in 2D.
Second, we introduce computational dolly-zoom effects to ﬁnd al-
ternative camera position and ﬁeld-of-view for the same viewing
region that minimizes barrel distortions. In summary, the ﬁrst mea-
sure effectively eliminates barrel distortions of vertical lines while
the second measure reduces distortions in both horizontal and ver-
tical directions to a degree.

The endgame for the novel view synthesis problem is to build an
accurate 3D model of the scene. To our best knowledge, [XZX*21]
is the only DL-based work that takes a single panorama as input
for such a goal. However, their code is not yet available. There-
fore, we opt for comparing to two common strategies to build a
rough 3D model out of a single panorama: 1) panoramas with per-
pixel depths (ground-truth or predicted by neural networks), and 2)
augmenting panoramas with room layouts. In our experiments (see
Section 4), we show that both strategies are inadequate to produce
good quality rendering results.

The paper outline is as follows. In Section 2, we describe related
work in the novel view synthesis problem based on panoramic in-
puts. In Section 3, we describe our two distortion reduction mea-
sures and provide a thorough analysis. In Section 4, we compare
the renderings of off-center perspective projections of our method
to vanilla E2P projections and common approaches to build 3D
panoramas. We further compare to results of [XZX*21] (by using
panorama images taken from their paper) and discuss the pros and
cons. Results on a large variety of panoramas taken from different
datasets are shown. Finally, we conclude the paper in Section 5.

3D Modeling of indoor scenes based on spherical panoramic (also
called "360°" in commercial settings) image inputs is a popular
ﬁeld in recent years. Key tasks include depth estimation [PAA*;
AZD*21; WYS*20; LRSK19], room layout estimation [LXM*20;
XZX*21; YWP*19], object detection and segmentation [SSC21;
XZH*18], and more generally 3D reconstruction tasks such as reg-
istration of multiple panoramas [YLC*20; CCYC21].

A number of panoramic image datasets have been pro-
duced to aid the research. Matterport3D [CDF*17] and Stand-
ford2D3D [ASZS17] are real-world large-scale RGB-D datasets,
which provide panoramic views of diverse indoor scenes. They
include various kinds of 2D and 3D semantics, meshes, and
even video walkthroughs. Structure3D [ZZL*20] and SunCG
[ZKZD18] dataset are synthetic datasets with richly decorated in-
door scenes. They provide realistically rendered indoor RGB-D im-
ages and annotations of 3D structures. 3D60 [ZKZ*19] is a col-
lective dataset with 3 different modalities (color, depth and nor-
mal) and comprises of realistic and synthetic 3D datasets (Matter-
port3D, Stanford2D3D, and SunCG). Gibson [XZH*18] is a real-
world dataset that includes high-quality RGB panoramas, global
camera poses, and 3D meshes.

2.2. Novel view synthesis

Novel view synthesis is one of the core tasks in visual comput-
ing. We limit our scope to methods rely on panoramic image in-
puts. Layered Depth Images (LDI) [SGHS98] and Multiplane Im-
ages (MPI) [ZTF*18] are used as image-based representations for
novel view synthesis, but for large translations, they might lack
sufﬁcient information to render correctly. Multi Depth Panora-
mas (MDPs) [LXM*20] and PerspectiveNet [NGR19] comprise
of multi-RGBDα images for high-quality and efﬁcient novel view
generations. Hedman et al. [HK18] input burst of aligned color-
and-depth photos to generate 3D panorama, and their 3D effects
could also interact with the scene. Gao et al. [GSKH21] propose an
algorithm to generate novel views from dynamic monocular videos.
Xu et al. [XZX*21] make the ﬁrst attempt to generate a target-
view panorama from one single source-view panorama with a large
camera translation. Jin et al. [JXZ*20] and Zeng et al. [ZKG20]
leverage the geometric structure of a 360° indoor image for depth
estimation.

Attal et al. [ALG*20] simultaneously learn depth and occlusions
via a multi-sphere image representation, which could greatly han-
dle occluded regions in dynamic scenes. With 46 input light ﬁeld
video, Broxton et al. [BFO*20] present a system that is able to re-
produce view-dependent reﬂections, semi-transparent surfaces, and
near-ﬁeld objects. Tobias et al. introduce OmniPhotos [BYLR20]
for quickly and casually capturing 360° VR panoramas, and im-
prove the visual rendering quality by alleviating distortion using a
novel deformable proxy geometry. Serrano et al. [SKC*19] present
a device which enable head motion parallax in 360°video, thus
tackled silhouettes and occlusions. Other works extend these ap-
proach to point clouds, Aliev et al. [ASK*20] present a point-
based approach to generate novel views of the scene. Voxel

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

3

"left" direction as Le f t = U p × Dir by right-handed cross product.
See Figure 2 for an example.

To retrieve the ray intensity (e.g., color or depth) at a normalized
2D position on the image plane, (X,Y ), 0 ≤ X,Y ≤ 1, X is left
to right direction and Y is top to bottom direction, we derive the
corresponding 3D ray direction, Ray = (rayx, rayy, rayz), as:

|Dir · near + Le f t · (0.5 − X) · width +U p · (0.5 −Y ) · height|,

which can be shortened as:

|Dir + Le f t · (1 − 2 ˙X) ·tan( f ovx/2) +U p · (1 − 2 ˙Y ) ·tan( f ovy/2)|.
(1)

Figure 2: An off-center E2P projection. P is the camera center
and I is the intersection of the looking ray and the unit sphere. We
use a right-handed system with +z direction at azimuth=0° and
zenith=0° and +x direction at azimuth=0° and zenith=90°. The
viewing pyramid is drawn in blue. Red lines show a column in the
view plane that depicts a non-straight geometry in the 3D scene
due to barrel distortions.

grid-based methods such as DeepVoxels [STH*19] encodes the
view-dependent appearance of a 3D scene as 3D voxels. Implicit
function-based methods such as Sitzmann et al [SZW19] propose
a continuous, 3D structure-aware scene representation that encodes
both geometry and appearance.

Overall, nearly all existing methods reply on input data that con-
sists of multiple panoramas (taken at different camera positions),
often as panoramic videos shoot either from a single moving 360°
camera or an array of ﬁxed (or even moving) cameras (capturing
rigs). One exception is [XZX*21], in which they rely on DL-based
methods to synthesize novel views from just a single panorama.
Our work is most similar to theirs.

3. Method

We ﬁrst describe the analytical form of the vanilla off-center E2P
projection in Section 3.1. We then describe our two measures to
reduce barrel distortions in Section 3.2 and 3.3. Each approach can
be applied independently. We assume a system would apply both
unless otherwise speciﬁed.

3.1. Off-center E2P projection

Without loss of generality, we assume the panorama is mapped to
a unit sphere centered at the origin. We assume the perspective
camera’s position is P = (px, py, pz), looking direction is Dir =
(dirx, diry, dirz), and up direction is U p = (upx, upy, upz). U p is
orthogonal to Dir. We assume the image plane is rectangular and
its local y-axis is aligned to the up direction. The dimensionality
of the image plane can be described by two angles f ovx and f ovy,
i.e., the horizontal and vertical angles of the pyramid formed by
the camera center and the view plane. Therefore, the width and
height of the image plane are width = near · tan( f ovx/2) · 2 and
height = near · tan( f ovy/2) · 2, respectively. near is the distance
between the camera position and the view plane. We denote the

To draw a perspective projection, for each 2D position (X,Y )
on the image plane (e.g., a pixel), our goal is to ﬁnd the spherical
coordinate, (θ, φ), of the point I on the unit sphere that intersects
with the ray from the camera position to the point on the 2D image
plane. Recall that θ is the zenith (angle from the +z axis), φ is the
azimuth (angle of counterclockwise rotation along +z-axis from the
+x axis), and the corresponding 2D coordinate in equirectangular
projection can be trivially derived as ( φ

2π , θ

π ).

x + ray2
|I| = 1. We have a = ray2
rayy + posz · rayz), and c = pos2
√

We ﬁnd the 3D coordinate of I by solving t in I = P + t · Ray,
y + ray2
z , b = 2 · (posx · rayx + posy ·
y + pos2
x + pos2
z − 1. t equals (−b +
b2 − 4ac)/2a (i.e., we take the positive-signed solution). Finally,

convert I’s 3D coordinate to spherical coordinate (θ, φ).

Note that the above calculation can be done in the OpenGL ren-
dering pipeline by setting up a unit spherical model centered at ori-
gin with the panorama as the 2D texture and equirectangular co-
ordinates as UV coordinates, and drawing the 2D view deﬁned by
a view frustum deﬁned by the aforementioned view pyramid cut
through the near plane. A pixel shader is used to calculate the UV
coordinates per pixel.

3.2. Cylindrical projection

Modern 360° cameras have strong image stabilization features to
ensure that the shoot panoramas are nearly in upright position, i.e.,
the -z direction in 3D or the +y direction in equirectangular projec-
tion are aligned to the direction of gravity. There also exist algo-
rithms to further transform a panorama to make it in upright posi-
tion (e.g., the "camera rotation pose alignment" in [SHSC19]). This
means that vertical features in 3D in the scene, such as wall corners,
are nearly always mapped to vertical lines in the panorama. How-
ever, this doesn’t mean that they are mapped to vertical lines in 3D
through the equirectangular projection. Instead, they are mapped
to meridians on the unit sphere, which are curved in 3D and are
perspectively drawn as curved lines in 2D except when the camera
position is at the origin.

We propose a simple solution to ensure that vertical features in
the scene are perspectively drawn as straight lines in 2D even when
the camera is not at the origin: projecting the panorama to a cylin-
der. We choose the cylinder to be of radius 1, centered at the origin,
in the "upright" position (i.e., axis is aligned to the z axis), and of an
inﬁnite height. We now have off-center equirectangular-to-cylinder
(E2C) projection as follows.

4

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

Figure 3: Off-center cylindrical projections. (a): A rendering using the same camera position and looking direction as in Figure 2. All
vertical features in the scene are now drawn as straight lines. (b): After the camera is pitched downward. The vertical features are still drawn
as 2D straight lines, but not vertical. (c): After the camera is rolled clockwise. Again, vertical features are drawn as 2D straight lines.

Equation 1 again describes the 3D ray direction, Ray, of a 2D
position (X,Y ) on the image plane of the camera. Its intersec-
tion point with the cylinder, Ic, is calculated by solving t(cid:48) in
Ic = P + t(cid:48) · Ray, |Ic
x + Ic
x + ray2
y,
y −1. t(cid:48) equals
b(cid:48) = 2·(posx ·rayx + posy ·rayy), and c = pos2
b(cid:48)2 − 4a(cid:48)c(cid:48))/2a(cid:48). Next, convert |Ic| to spherical coordi-
(−b(cid:48) +
nate (θ, φ), and use it to sample a ray intensity in the panorama.

y | = 1. We now have a(cid:48) = ray2

x + pos2

√

We have the following lemma:

Lemma 3.1 A vertical feature in 3D in the scene is perspectively
drawn as a 2D straight line under a cylindrical projection.

Proof Recall that a vertical feature in 3D is necessarily drawn as
a vertical line in the 2D panorama under equirectangular projec-
tions, which is then mapped to a part of a meridian on the sphere.
Trivially, a cylindrical projection maps every meridian (and parts of
it) on the sphere to a vertical straight line on the cylinder. Finally,
recall that perspective projection preserves the linearity of any 3D
straight lines in 2D perspective views.

Note that Lemma 3.1 applies for cameras with arbitrary rolls,
pitches, and yaws, not just "upright" cameras with a left direction
perpendicular to the z axis. See Figure 3 for an example.

To realize the above calculation in the OpenGL rendering
pipeline, replace the spherical model with a cylindrical model.
The model is textured using the panorama as the 2D texture and
"normalized" equirectangular coordinates, which are based on the
spherical coordinates of the normalized 3D point positions, as UV
coordinates.

In summary, Lemma 3.1 states that any vertical features in the
3D scene are guaranteed to be drawn as straight lines in the 2D per-
spective views. However, the inverse is not necessarily true - every-
thing depicted on a column in the panorama would remain straight
in 2D perspective views, no matter they are genuinely vertically
aligned in 3D or not. In practice, we observe that the renderings
largely remain smooth and intuitive.

3.3. Computational dolly-zoom effect

In cinematography, a "dolly-zoom" effect refers to the act that the
ﬁeld-of-view (FOV) angle is continuously narrowed while the cam-
era is moving away from an object in the scene, or vice versa, in
a calibrated manner such that the object would appear at the same
size on the 2D frame during the movement. The goal is to create so

Inspired by the dolly-zoom effects in cinematography,
Figure 4:
we ﬁnd an alternative camera position and FOV angles setting that
with which the perspective camera draws roughly the same subset
of the panorama but with less barrel distortions. The original cam-
era is shown in blue and the adjusted camera is shown in cyan.
Observe that the two drawn images have roughly the same bound-
ary and center. Just that the images are stretched in different ways.

called "perspective distortions" in which the relative sizes of other
objects in the scene change w.r.t. to the size of the particular object.

Recall that in perspective drawing of panoramas, in general, the
amount of distortions is proportional to the distance between the
camera position and the origin (zero when they coincide). There-
fore, our main idea is to leverage dolly-zoom principles to draw
roughly the same subset of the panorama in the image plane but
from a camera position that is as close to the origin as possible.
We call the new camera position, Ph, the heuristic solution to the
computational dolly-zoom problem.

To elaborate, Ph is the solution to the following optimization

problem:

argmin
t

|Ph|

subject to Ph = P + t · Dir

(2)

where P is the original camera position and Dir is the look-
ing direction. Given the new camera position Ph, we solve the
new FOV angles as follows. First, we denote the "left-middle"
and "right-middle" viewing rays, Rayle f t and Rayright , as the rays

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

5

Figure 5: Distortion analysis of perspective views of a original
camera (a) and the dolly-zoom adjusted version (b). The original
camera has a distortion value of 0.00012 while the adjusted ver-
sion’s amount is just 4.413e − 5. Blue grids come from bilinear in-
terpolations of the four projected corners of the viewing pyramids.
Red grids are their projections to the cylinder. Grey lines connect
the grid vertices to the origin while cyan lines connect them to the
camera positions. In short, the cyan grids in perspective views vi-
sualize how regular grids on each camera’s image plane (blue) are
ﬁrst projected to the cylinder (red) and then projected back to the
respective image planes, causing distortions. We used a coarser
grid for visualization with less clutter.

from P toward the left-middle and right-middle points of the orig-
inal image plane. See Figure 4 for an example. Next, we ﬁnd
the intersections of Rayle f t and Rayright to the sphere or cylin-
der (depending on which projection scheme is used), denoted as
Ile f t and Iright , respectively. Our goal is that in the new view-
ing pyramid, the new left-middle and right-middle viewing rays
should intersect the sphere/cylinder at the same positions. There-
fore, the new left-middle viewing ray, Raynew
le f t , is the ray from
Ph toward Ile f t . The new right-middle viewing ray, Raynew
right , is
the ray from Ph toward Iright . Finally, we calculate the new
"left-horizontal" and "right-horizontal" FOV angles, f ovxle f t and
f ovxright , as the angles between Dir and Raynew
le f t and between Dir
and Raynew
le f t , respectively. The new vertical FOV angle is calculated
as atan( (tan( f ovxle f t )+tan( f ovxright ))
), aspect is the aspect ratio of the
original image plane. Note that the new viewing pyramid could be
skewed as f ovxle f t and f ovxright are not necessarily the same.

aspect

3.3.1. Performance analysis

We propose the following scheme to evaluate how the heuristic so-
lution reduces distortions. In short, for a given camera position,
looking direction, and FOV angles (a "camera pose" in short), we
estimate a subset of the sphere/cylinder that is projected to roughly
cover the whole image plane. We subdivide the subset to be a regu-
lar grid. We then calculate the curvature of the grid’s 2D projection
on the image plane as the way to measure the "distortion value" of
the camera.

How to estimate such a subset of the sphere/cylinder? We
ﬁrst project
the four corners of the view plane onto to the
sphere/cylinder. Afterwards, we build a nearly-planar regular grid
through bilinear interpolation of the four projected corners. We de-

Figure 6: The derivation of the 3D-to-2D perspective projection
formula (Equation 3). We show the derivation of Vx of a point on
the cylinder (red dot) and assume Aspect = 1. The distance to the
view plane, near, is 1. Left: an original camera without position
v(cid:48)(cid:48)
offsets (i.e., t = 0). h equals
is negative). Therefore, Vx
x
−v(cid:48)(cid:48)
z
equals h
plane). Right: the camera is moved along the z-axis by amount t. h
v(cid:48)(cid:48)
z +t . The new half-width of the view plane becomes
now equals
x
−v(cid:48)(cid:48)
H
.
B+t . Therefore, we have Vx =

(i.e., we need to normalize h by the half-width of the view

(v(cid:48)(cid:48)
z

H
B

v(cid:48)(cid:48)
x
H
B+t ·(−v(cid:48)(cid:48)

z +t)

note the regular grid as a 2D array of vertices v[i, j] ∈ R3, 0 ≤ i ≤
ROW S, 0 ≤ j ≤ COLS. In practice, the vertices are projected onto
the sphere/cylinder in a panoramic image, so the 3D position of the
(i, j)-th vertex is actually v(cid:48)[i, j] = |v[i, j]|. Note that this means that
the grid becomes a discrete curved surface and the 2D projections
of its rows and columns of vertices would not be straight unless it
is viewed exactly from the origin. Finally, to evaluate the curvature
of the grid’s 2D projection on the image plane, we sum up a linear-
ity measurement (Equation 3.3.2) of the consecutive edges of every
rows and columns in the grid. See Figure 5 for an example.

At a particular camera pose, the distortion improvement is the
distortion value of the original camera pose minus the distortion
value of the dolly zoom-adjusted camera pose. As shown in Fig-
ure 7, we measure the distortion improvements at every possible
camera poses up to rotational and reﬂective symmetries in a sphere,
sampled to avoid clutter. We can see that the improvements vary
greatly at different camera poses. For example, distortions are re-
duced to zero when we "dolly" the camera (i.e., moving forward or
backward without changing the looking direction). However, im-
provements are non-existent if we "truck" the camera (i.e., moving
sideways while ﬁxing the looking direction). We leave ﬁnding an
analytical form to explain the distribution of improvements among
different camera poses to future work.

3.3.2. Optimization-based solution

The aforementioned evaluation scheme inspired us to formulate the
task of ﬁnding t as an optimization problem to minimize barrel dis-
tortions. In short, we have a single variable t (i.e., offset of the
camera position along the looking direction). Given the 3D posi-
tions of the grid vertices, v(cid:48)[i, j], as constants, we ﬁrst transform
them to a new coordinate space that is the same as the view space
in OpenGL. That is, the camera position is aligned to the origin, the
three axis, Le f t, U p, and Dir, are aligned to the -x-axis, +y-axis,
and -z-axis, respectively, through a rotation and a translation (e.g.,

6

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

Figure 7: Comparing distortions of various camera models. (a) to (c): the distortion values of the original camera model, heuristic solution,
and optimized solutions with cylindrical projection. We only show results up to rotational and reﬂective symmetrices in a sphere. Therefore,
we sample the camera positions on a radially sampled ﬁrst quartile of the Y-Z plane intersect with the unit sphere. At each position, we
sample all possible looking directions on the half hemisphere facing the +x direction. The distortion values are converted to colors from blue
(smallest) to yellow to red (biggest). For clarity, we cap the upper bound (there exist some very large distortion values so using the full range
would result in all blue-ish colors). We can see that optimized solutions clearly have the lowest distortion values among the three. While not
clearly shown by the visualizations here, heuristic solutions actually win over the original camera models (see Table 4.1 for statistics). In (d)
and (e), we show the distortion improvements of the optimized solutions over the original and the heuristic solutions, respectively.

by applying OpenGL’s MODELVIEW transform matrix). We de-
note the transformed grid vertex positions as v(cid:48)(cid:48)[i, j] ∈ R3. In this
manner, we can concisely formulate their 2D projected positions
onto the image plane, V [i, j] ∈ R2, as a function of t as follows:

V [i, j]x =

v(cid:48)(cid:48)[i, j]x

Aspect · H

B+t · (−v(cid:48)(cid:48)[i, j]z + t)

,

V [i, j]y =

v(cid:48)(cid:48)[i, j]y
H
B+t · (−v(cid:48)(cid:48)[i, j]z + t)
where B denotes the distance of the grid plane to the origin in the
new coordinate space. H equals B · tan( f ovy/2). B and H can be
understood as the base and height of the triangle from the origin to
half of the grid plane (see Figure 6 for an illustration).

,

(3)

The objective function to minimize the sum of the linearity mea-
surement of consecutive edges of every rows and columns in the
grid is formulated as follows:

Ob j = ∀i Σ j=[1,COLS−1]angle(V [i, j − 1],V [i, j],V [i, j + 1])+
∀ j Σi=[1,ROW S−1]angle(V [i − 1, j],V [i, j],V [i + 1, j]),

(4)
where linearity(a, b, c), a, b, c ∈ R2, denotes the linearity measure-
ment of edge a, b and edge b, c. We formulate it as:

linearity(a, b, c) = pow((bx − ax)(cy − ay)−
(cx − ax)(by − ay), 2).

To sum up, the optimization problem takes the form:

argmin
t

Equation 4

subject to Equation 3 ∀ V [i, j] ∈ grid.

(5)

After t is solved, the new camera position and FOV angles are de-
rived accordingly.

In Figure 7, we compare the distortion improvements of the orig-
inal camera, heuristic solutions, and the optimized solutions at dif-

ferent camera poses. We ﬁnd that the optimized solutions improves
upon both the original and the heuristic solutions. In our compar-
isons, we used 10 for ROW S and COLS. We experimented with
other grid resolutions and the results are similar. We conclude that
using the optimization approach led to better results at a small com-
putational cost.

4. Results and comparisons

We ﬁrst compare our method to [XZX*21]. As shown in Figure 8,
we attempt to create the same novel view synthesis results as done
in their paper. In general, we ﬁnd their results to be quite good
in getting the overall 3D layouts correct. However, two shortcom-
ings are: 1) the occasional blurs and ghosts, and 2) the resolution
of their predicted panoramas is only 512x256, too low for drawing
perspective views. In comparison, our results are not as geomet-
rically accurate as theirs, but are of higher resolutions (since the
source panoramas tend to have higher resolutions than the predicted
panoramas), and largely free of any visual artifacts.

In Figure 9, we compare off-center perspective projection results
of our method with the vanilla E2P projection and two common
approaches to augment panoramas with 3D information (per-pixel
depth and room layouts). We test on panoramas from the Struc-
ture3D dataset [ZZL*20] (synthetic), the Gibson dataset [XZH*18]
(real-world), and several panoramas shoot by ourselves (we used a
RICOH THETA Z1 360◦ camera). Note that the teaser (Figure 1)
is also based on the Structure3D dataset. The depth information
are either ground truth (available in the Structure3D dataset only)
or predicted by a state-of-the-art depth prediction model ("MiDaS
v3.0" [RBK21]) that we found to be accurate and robust in gen-
eral. We also tried another modern depth prediction model specif-
ically for panoramas ( [WYS*20]) but found the results to be less
accurate. The layouts are either ground truth (available in the Struc-
ture3D dataset only) or predicted by LED2-Net [WYS*21]. In sum-
mary, our results signiﬁcantly reduced the barrel distortions pro-
duced by the vanilla E2P projection. Augmenting panoramas with
depths or room layouts provides more realistic 3D effects such as

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

7

original cameras, heuristic solutions, and optimized solutions are
shown in Table 4.1. In summary, both heuristic and optimized solu-
tions improve upon the original camera model in terms of distortion
values, while the optimized solutions win over the heuristic ones by
a sizeable margin.

4.2. Limitations

Our method can be summarized as using a cylinder as the proxy
mesh to deﬁne the per-pixel depths, which eliminates distortions
of vertical features in 3D, and then using the computational dolly-
zoom effect to ﬁnd optimal alternative camera positions and FOV
angles to render the same view-able regions but with minimized
distortions. Our method does not create true 3D effects such as
parallax and occlusion on-and-offs. Nevertheless, the illusion of a
scene in 3D remains to a degree when the camera movement is
small w.r.t. the depth disparity of the scene. Our hypothesis is that
human brains can still deduct 3D depths by the image features (se-
mantics, shading, straight lines, etc). Realism of the rendering be-
gins to fray when the relative positions of the camera to the other
objects signiﬁcantly changed. One example is the last row in Fig-
ure 8 (the camera moved to the other side of the table-chairs set).

5. Conclusion and future work

In this paper, we explore how to tackle barrel distortions in E2P
projections when the camera moves away from the center. The
experiments show that our two proposed approaches, namely the
cylindrical projection and the computational dolly-zoom effect, sig-
niﬁcantly reduced distortions in different indoor scene datasets.
The resulting renderings are smooth and intuitive, albeit lacking
true 3D effects such as parallax and occlusion on-and-offs. Com-
paring to the state-of-the-art DL-based method [XZX*21], one key
advantage of our method is that the resolution of perspective views
is much higher. We compare our method with other methods that
aim to augment panoramas with 3D information (i.e., depth infor-
mation and room layouts), and show that these methods are actually
more likely to produce results with noticeable artifacts.

In summary, our approach relies on novel strategies to re-sample
the existing pixels in a panorama to synthesize off-center perspec-
tive views with minimized barrel distortions. For future work, we
would like to expand our methods to leverage readily-available 3D
information of a panorama, such as depths and room layouts pre-
dicted by neural networks. For example, creating more complex
proxy meshes according to depths or layouts instead of a cylinder.
More ambitious goals include creating datasets and deep-learning
methods that directly synthesize novel full-resolution perspective
views out of a single or a sparse set of panoramas.

References

[ALG*20] ATTAL, BENJAMIN, LING, SELENA, GOKASLAN, AARON, et
al. “Matryodshka: Real-time 6dof video view synthesis using multi-
sphere images”. European Conference on Computer Vision. Springer.
2020, 441–459 2.

[ASK*20] ALIEV, KARA-ALI, SEVASTOPOLSKY, ARTEM, KOLOS,
MARIA, et al. “Neural point-based graphics”. Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXII 16. Springer. 2020, 696–712 2.

Figure 8: Our attempts to recreate the same novel view synthe-
sis results in Figure 5 in [XZX*21]. For each row, left: a perspec-
tives view of a ground truth "target" panorama. Middle: the same
perspective view of the synthesized panorama generated by their
method using a different "source" panorama and the camera pose
of the "target" panorama. Right: our result with our best effort to
ﬁnd the corresponding camera pose. Our results aren’t as geomet-
rically correct as theirs but are free of blurs and ghosts. The red
markers can be ignored.

Method
Ori.
Heu.
Opt.

Zeroth-q
1.832e-05
7.254e-15
9.450e-31

First-q
0.00038
0.00019
0.00015

Second-q
0.00172
0.00098
0.00071

Third-q
0.00689
0.00375
0.00255

Fourth-q
5.837e+11
6.293e+09
1.546e+06

Table 1: The zeroth- (minimum), ﬁrst-, second- (median), third-,
and fourth-quartiles (maximum) of the distortion values of the orig-
inal camera model, the heuristic solutions, and the optimized solu-
tions of the computational dolly-zoom effects.

parallax and occlusion on-and-offs. However, glaring artifacts, such
as blurs (happen when viewing "extruded" pixels by depths from
sideways) and broken images features (happen when room layouts
mismatch 3D objects, such as furniture, in the scene), may happen.
See the accompanying video for animated versions of the results.
We also provide a computer program for readers to try out on their
own panoramas.

4.1. Statistics

We tested on a laptop computer with 6-core 2.6GHZ CPU, 16GB
ram, NVidia GTX 1650 Ti graphics card, and Windows system. We
use Google Ceres-Solver to solve the computational dolly-zoom ef-
fect optimization problem. We measure the times to solve the prob-
lem in all possible camera poses (the same sampling as in Figure 7).
The average and largest times are 0.53 and 7 milliseconds. This
means that the method is suitable for real-time applications on a
reasonable computer. The computational costs of the heuristic so-
lutions are negligible. The quartiles of the distortion values of the

8

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

Figure 9: Off-center perspective projection results. From second left to right columns: 1) by vanilla E2P projection, 2) our method, panora-
mas augmented with 3) depths and 4) room layouts. The ﬁrst two rows are based on real-world indoor panoramas shoot by ourselves with
predicted depths ("MiDaS v3.0" [RBK21]) and predicted room layouts (LED2-Net [WYS*21]). The third row is based on a panorama from
the Structure3D dataset (synthetic) with ground truth depths and room layout. The last three rows are based on Gibson dataset (real-world)
with predicted depths and room layouts. We show the panoramas annotated with room layouts and visualization of the off-center cameras in
the left column. Bigger versions of the images can be found in the Additional Materials.

Chi-Han Peng & Jiayao Zhang / Distortion Reduction for Off-Center Perspective Projection of Panoramas

9

[ASZS17] ARMENI, IRO, SAX, SASHA, ZAMIR, AMIR R, and SAVARESE,
SILVIO. “Joint 2d-3d-semantic data for indoor scene understanding”.
arXiv preprint arXiv:1702.01105 (2017) 2.

[STH*19] SITZMANN, VINCENT, THIES, JUSTUS, HEIDE, FELIX, et al.
“DeepVoxels: Learning Persistent 3D Feature Embeddings”. Proc. Com-
puter Vision and Pattern Recognition (CVPR), IEEE. 2019 3.

[AZD*21] ALBANIS, GEORGIOS, ZIOULIS, NIKOLAOS, DRAKOULIS,
PETROS, et al. “Pano3D: A Holistic Benchmark and a Solid Baseline
for 360deg Depth Estimation”. Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. 2021, 3727–3737 2.

[BFO*20] BROXTON, MICHAEL, FLYNN, JOHN, OVERBECK, RYAN, et
al. “Immersive light ﬁeld video with a layered mesh representation”.
ACM Transactions on Graphics (TOG) 39.4 (2020), 86–1 2.

[BYLR20] BERTEL, TOBIAS, YUAN, MINGZE, LINDROOS, REUBEN,
and RICHARDT, CHRISTIAN. “OmniPhotos: casual 360° VR photogra-
phy”. ACM Transactions on Graphics (TOG) 39.6 (2020), 1–12 2.

[CCYC21] CHEN, KUO-WEI, CHANG, FELIX, YAO, CHIH-YUAN, and

CHU, HUNG-KUO. “Sio-Keong Si”. (2021) 2.

[CDF*17] CHANG, ANGEL, DAI, ANGELA, FUNKHOUSER, THOMAS, et
al. “Matterport3d: Learning from rgb-d data in indoor environments”.
arXiv preprint arXiv:1709.06158 (2017) 2.

[GSKH21] GAO, CHEN, SARAF, AYUSH, KOPF,

JOHANNES, and
HUANG, JIA-BIN. “Dynamic View Synthesis from Dynamic Monocu-
lar Video”. arXiv preprint arXiv:2105.06468 (2021) 2.

[HK18] HEDMAN, PETER and KOPF, JOHANNES. “Instant 3d photogra-

phy”. ACM Transactions on Graphics (TOG) 37.4 (2018), 1–12 2.

[JXZ*20] JIN, LEI, XU, YANYU, ZHENG, JIA, et al. “Geometric structure
based and regularized depth estimation from 360 indoor imagery”. Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2020, 889–898 2.

[KMA*20] KOPF, JOHANNES, MATZEN, KEVIN, ALSISAN, SUHIB, et al.
“One Shot 3D Photography”. Transactions on Graphics (Proceedings of
SIGGRAPH) 39.4 (2020) 2.

[LRSK19] LASINGER, KATRIN, RANFTL, RENÉ, SCHINDLER, KONRAD,
and KOLTUN, VLADLEN. “Towards robust monocular depth estima-
tion: Mixing datasets for zero-shot cross-dataset transfer”. arXiv preprint
arXiv:1907.01341 (2019) 2.

[LXM*20] LIN, KAI-EN, XU, ZEXIANG, MILDENHALL, BEN, et al.
“Deep multi depth panoramas for view synthesis”. Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XIII 16. Springer. 2020, 328–344 2.

[SZW19] SITZMANN, VINCENT, ZOLLHÖFER, MICHAEL, and WET-
ZSTEIN, GORDON. “Scene representation networks: Continuous
scene representations”. arXiv preprint
3d-structure-aware neural
arXiv:1906.01618 (2019) 3.

[WYS*20] WANG, FU-EN, YEH, YU-HSUAN, SUN, MIN, et al. “Bifuse:
Monocular 360 depth estimation via bi-projection fusion”. Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion. 2020, 462–471 2, 6.

[WYS*21] WANG, FU-EN, YEH, YU-HSUAN, SUN, MIN, et al. “LED2-
Net: Monocular 360deg Layout Estimation via Differentiable Depth
Rendering”. Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition. 2021, 12956–12965 1, 6, 8.

[XZH*18] XIA, FEI, ZAMIR, AMIR R, HE, ZHIYANG, et al. “Gibson env:
Real-world perception for embodied agents”. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2018, 9068–
9079 2, 6.

[XZX*21] XU, JIALE, ZHENG, JIA, XU, YANYU, et al. “Layout-Guided
Novel View Synthesis from a Single Indoor Panorama”. Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
2021, 16438–16447 2, 3, 6, 7.

[YLC*20] YANG, SHENG, LI, BEICHEN, CAO, YAN-PEI, et al. “Noise-
resilient reconstruction of panoramas and 3D scenes using robot-
mounted unsynchronized commodity RGB-D cameras”. ACM Transac-
tions on Graphics (TOG) 39.5 (2020), 1–15 2.

[YWP*19] YANG, SHANG-TA, WANG, FU-EN, PENG, CHI-HAN, et al.
“Dula-net: A dual-projection network for estimating room layouts from
a single rgb panorama”. Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2019, 3363–3372 2.

[ZKG20] ZENG, WEI, KARAOGLU, SEZER, and GEVERS, THEO. “Joint
3d layout and depth prediction from a single indoor panorama image”.
European Conference on Computer Vision. Springer. 2020, 666–682 2.

[ZKZ*19] ZIOULIS, NIKOLAOS, KARAKOTTAS, ANTONIS, ZARPALAS,
DIMITRIOS, et al. “Spherical view synthesis for self-supervised 360
depth estimation”. 2019 International Conference on 3D Vision (3DV).
IEEE. 2019, 690–699 2.

[NGR19] NOVOTNY, DAVID, GRAHAM, BEN, and REIZENSTEIN,
JEREMY. “PerspectiveNet: A scene-consistent image generator for new
view synthesis in real indoor environments”. Advances in Neural Infor-
mation Processing Systems 32 (2019), 7601–7612 2.

[ZKZD18] ZIOULIS, NIKOLAOS, KARAKOTTAS, ANTONIS, ZARPALAS,
DIMITRIOS, and DARAS, PETROS. “Omnidepth: Dense depth estimation
for indoors spherical panoramas”. Proceedings of the European Confer-
ence on Computer Vision (ECCV). 2018, 448–465 2.

[PAA*] PINTORE, GIOVANNI, AGUS, MARCO, ALMANSA, EVA, et al.
“SliceNet: deep dense depth estimation from a single indoor panorama
using a slice-based representation: Supplementary Material”. () 2.

[ZTF*18] ZHOU, TINGHUI, TUCKER, RICHARD, FLYNN, JOHN, et al.
“Stereo magniﬁcation: Learning view synthesis using multiplane im-
ages”. arXiv preprint arXiv:1805.09817 (2018) 2.

[ZZL*20] ZHENG, JIA, ZHANG, JUNFEI, LI, JING, et al. “Structured3D: A
Large Photo-realistic Dataset for Structured 3D Modeling”. Proceedings
of The European Conference on Computer Vision (ECCV). 2020 1, 2, 6.

[RBK21] RANFTL, RENÉ, BOCHKOVSKIY, ALEXEY, and KOLTUN,
VLADLEN. “Vision Transformers for Dense Prediction”. ArXiv preprint
(2021) 6, 8.

[SGHS98] SHADE, JONATHAN, GORTLER, STEVEN, HE, LI-WEI, and
SZELISKI, RICHARD. “Layered depth images”. Proceedings of the 25th
annual conference on Computer graphics and interactive techniques.
1998, 231–242 2.

[SHSC19] SUN, CHENG, HSIAO, CHI-WEI, SUN, MIN, and CHEN,
HWANN-TZONG. “HorizonNet: Learning Room Layout With 1D Rep-
resentation and Pano Stretch Data Augmentation”. IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA,
USA, June 16-20, 2019. 2019, 1047–1056 3.

[SKC*19] SERRANO, ANA, KIM, INCHEOL, CHEN, ZHILI, et al. “Motion
parallax for 360 RGBD video”. IEEE Transactions on Visualization and
Computer Graphics 25.5 (2019), 1817–1827 2.

[SSC21] SUN, CHENG, SUN, MIN, and CHEN, HWANN-TZONG. “Ho-
honet: 360 indoor holistic understanding with latent horizontal features”.
Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition. 2021, 2573–2582 2.

