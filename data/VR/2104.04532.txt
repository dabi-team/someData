Neural RGB-D Surface Reconstruction

Dejan Azinovi´c1 Ricardo Martin-Brualla2 Dan B Goldman2 Matthias Nießner1

Justus Thies1,3

1Technical University of Munich 2Google Research 3Max Planck Institute for Intelligent Systems

Figure 1. Our method obtains a high-quality 3D reconstruction from an RGB-D input sequence by training a multi-layer perceptron. The
core idea is to reformulate the neural radiance field definition in NeRF [48], and replace it with a differentiable rendering formulation based
on signed distance fields which is specifically tailored to geometry reconstruction.

Abstract

Obtaining high-quality 3D reconstructions of room-
scale scenes is of paramount importance for upcoming ap-
plications in AR or VR. These range from mixed reality
applications for teleconferencing, virtual measuring, vir-
tual room planing, to robotic applications. While current
volume-based view synthesis methods that use neural radi-
ance fields (NeRFs) show promising results in reproducing
the appearance of an object or scene, they do not recon-
struct an actual surface. The volumetric representation of
the surface based on densities leads to artifacts when a sur-
face is extracted using Marching Cubes, since during opti-
mization, densities are accumulated along the ray and are
not used at a single sample point in isolation. Instead of
this volumetric representation of the surface, we propose to
represent the surface using an implicit function (truncated
signed distance function). We show how to incorporate this
representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such
as a Kinect. In addition, we propose a pose and camera re-
finement technique which improves the overall reconstruc-

tion quality.
In contrast to concurrent work on integrat-
ing depth priors in NeRF which concentrates on novel view
synthesis, our approach is able to reconstruct high-quality,
metrical 3D reconstructions.

1. Introduction

Research on neural networks for scene representations
and image synthesis has made impressive progress in re-
cent years [73]. Methods that learn volumetric represen-
tations [42, 48] from color images captured by a smart-
phone camera can be employed to synthesize near photo-
realistic images from novel viewpoints. While the focus
of these methods lies on the reproduction of color images,
they are not able to reconstruct metric and clean (noise-
free) meshes. To overcome these limitations, we show that
there is a significant advantage in taking additional range
measurements from consumer-level depth cameras into ac-
count.
Inexpensive depth cameras are broadly accessible
and are also built into modern smartphones. While classical
reconstruction methods [9, 33, 53] that purely rely on depth
measurements struggle with the limitations of physical sen-

1

sors (noise, limited range, transparent objects, etc.), a neu-
ral radiance field-based reconstruction formulation allows
to also leverage the dense color information. Methods like
BundleFusion [14] take advantage of color observations to
compute sparse SIFT [44] features for re-localization and
refinement of camera poses (loop closure). For the actual
geometry reconstruction (volumetric fusion), only the depth
maps are taken into account. Missing depth measurements
in these maps, lead to holes and incomplete geometry in
the reconstruction. This limitation is also shared by learned
surface reconstruction methods that only rely on the range
data [47, 67].
In contrast, our method is able to recon-
struct geometry in regions where only color information is
available. Specifically, we adapt the neural radiance field
(NeRF) formulation of Mildenhall et al. [48] to learn a trun-
cated signed distance field (TSDF), while still being able to
leverage differentiable volumetric integration for color re-
production. To compensate for noisy initial camera poses
which we compute based on the depth measurements, we
jointly optimize our scene representation network with the
camera poses. The implicit function represented by the
scene representation network allows us to predict signed
distance values at arbitrary points in space which is used
to extract a mesh using Marching Cubes.

Concurrent work that incorporates depth measurements
in NeRF focuses on novel view synthesis [16, 49, 81], and
uses the depth prior to restrict the volumetric rendering to
near-surface regions [49,81] or adds an additional constraint
on the depth prediction of NeRF [16]. NeuS [76] is also
a concurrent work on novel view synthesis which uses a
signed distance function to represent the geometry, but takes
only RGB images as input, and thus fails to reconstruct the
geometry of featureless surfaces, like white walls. In con-
trast, our method aims for high-quality 3D reconstructions
of room-scale scenes using an implicit surface representa-
tion and direct SDF-based losses on the input depth maps.
Comparisons to state-of-the-art scene reconstruction meth-
ods show that our approach improves the quality of geome-
try reconstructions both qualitatively and quantitatively.

In summary, we propose an RGB-D based scene recon-
struction method that leverages both dense color and depth
observations.
It is based on an effective incorporation of
depth measurements into the optimization of a neural radi-
ance field using a signed distance-based surface represen-
tation to store the scene geometry. It is able to reconstruct
geometry detail that is observed by the color images, but
not visible in the depth maps.
In addition, our pose and
camera refinement technique is able to compensate for mis-
alignments in the input data, resulting in state-of-the-art re-
construction quality which we demonstrate on synthetic as
well as on real data from ScanNet [12].

2. Related Work

Our approach reconstructs geometry from a sequence of
RGB-D frames, leveraging both dense color and depth in-
formation. It is related to classical fusion-based 3D recon-
struction methods [9, 14, 50, 53, 93], learned 3D reconstruc-
tion [7, 15, 47, 58, 79], as well as to recent coordinate-based
scene representation models [48, 69, 74].

Classical 3D Reconstruction. There exists a wide range
of methods for RGB and RGB-D based 3D reconstruc-
tion that are not based on deep learning. Reconstruct-
ing objects and scenes can be done using passive stereo
systems that rely on stereo matching from two or multi-
ple color views [29, 62], Structure-from-Motion [63], or
SLAM-based [20, 21, 23] methods. These approaches may
use disjoint representations, like oriented patches [25], vol-
umes [38], or meshes [32] to reconstruct the scene or object.
Zollh¨ofer et al. [93] review the 3D reconstruction meth-
ods that rely on range data from RGB-D cameras like the
Kinect. Most of these methods are based on [9], where
multiple depth measurements are fused using a signed dis-
tance function (SDF) which is stored in a uniform 3D grid.
E.g., KinectFusion [50] combines such representation with
real-time tracking to reconstruct objects and small scenes
in real-time. To handle large scenes Nießner et al. [53]
propose a memory-efficient storage of the SDF grid using
spatial hashing. To handle the loop closure problem when
scanning large-scale scenes, bundle adjustment can be used
to refine the camera poses [14]. In addition, several regu-
larization techniques have been proposed to handle outliers
during reconstruction [19, 61, 89].

Deep Learning for 3D Reconstruction. To reduce artifacts
from classical reconstruction methods, a series of meth-
ods was proposed that use learned spatial priors to predict
depth maps from color images [24, 28, 39], to learn multi-
view stereo using 3D CNNs on voxel grids [34, 68, 82], or
multi-plane images [22], to reduce the influence of noisy
depth values [79], to complete incomplete scans [13, 15],
to learn image features for SLAM [2, 10, 91] or feature fu-
sion [4,71,80], to predict normals [90], or to predict objects
or parts of a room from single images [11, 18, 27, 51, 75].
Most recently coordinate-based models have become pop-
ular [74]. These models use a scene representation that is
based on a deep neural network with fully connected layers,
i.e., a multi-layer perceptron (MLP) [73, 74]. As input the
MLP takes a 3D location in the model space and outputs
for example, occupancy [7, 47, 52, 55, 58–60], density [48],
radiance [48], color [54], or the signed distance to the sur-
face [56,83,84]. Scene Representation Networks [69] com-
bine such a representation with a learned renderer which is
inspired by classical sphere tracing, to reconstruct objects
from single RGB images.
Instead, Mildenhall et al. [48]
propose a method that represents a scene as a neural ra-

2

Figure 2. Differentiable volumetric rendering is used to reconstruct a scene that has been captured using an RGB-D camera. The scene
is represented using multi-layer perceptrons (MLPs), encoding a signed distance value Di and a viewpoint-dependent radiance value
ci per point pi. We perform volumetric rendering by integrating the radiance along a ray, weighing the samples as a function of their
signed distance Di and their visibility. We also learn a per-frame latent corrective code to account for exposure or white balance changes
throughout the capture, which is passed to the radiance MLP alongside the ray direction d. We optimize the scene representation’s MLPs,
together with the per-frame corrective codes, the input camera poses, and an image-plane deformation field (not shown) by computing
losses for the signed distance Di of the samples, and the final integrated color C with respect to the input depth and color views.

diance field (NeRF) using a coordinate-based model, and
a classical, fixed volumetric rendering formulation [46].
Based on this representation, they show impressive novel
view synthesis results, while only requiring color input im-
ages with corresponding camera poses and intrinsics. Be-
sides the volumetric image formation, a key component of
the NeRF technique is a positional encoding layer, that uses
sinusoidal functions to improve the learning properties of
the MLP. In follow-up work, alternatives to the positional
encoding were proposed, such as Fourier features [72] or si-
nusoidal activation layers [67]. NeRF has been extended to
handle in-the-wild data with different lighting and occlud-
ers [45], dynamic scenes [40, 57], avatars [26], and adapted
for generative modeling [5, 66] and image-based render-
ing [77, 87]. Others have focused on resectioning a camera
given a learned NeRF [85], and optimizing for the camera
poses while learning a NeRF [41, 78].

In our work, we take advantage of the volumetric render-
ing of NeRF and propose the usage of a hybrid scene rep-
resentation that consists of an implicit surface representa-
tion (SDF) and a volumetric radiance field. We incorporate
depth measurements in this formulation to achieve robust
and metric 3D reconstructions. In addition, we propose a
camera refinement scheme to further improve the quality of
the reconstruction. In contrast to NeRF which uses a den-
sity based volumetric representation of the scene, our im-
plicit surface representation leads to high quality geometry
estimates of entire scenes.

Concurrent Work. In concurrent work, Wang et al. [76]
present NeuS which uses an implicit surface representation
to improve novel view synthesis of NeRF. Wei et al. [81]

propose a multi-view stereo approach to estimate dense
depth maps which they use to constrain the sampling region
when optimizing a NeRF. Similarly, Neff et al. [49] restrict
the volumetric rendering to near surface regions. Additional
constraints on the depth predictions of NeRF were proposed
by Deng et al. [16]. In contrast to these, our method focuses
on accurate 3D reconstructions of room-scale scenes, with
explicit incorporation of depth measurements using an im-
plicit surface representation.

3. Method

We propose an optimization-based approach for geome-
try reconstruction from an RGB-D sequence of a consumer-
level camera (e.g., a Microsoft Kinect). We leverage both
the N color frames Ii as well as the corresponding aligned
depth frames Di to optimize a coordinate-based scene rep-
resentation network. Specifically, our hybrid scene rep-
resentation consists of an implicit surface representation
based on a truncated signed distance function (TSDF) and
a volumetric representation for the radiance. As illustrated
in Fig. 2, we use differentiable volumetric integration of the
radiance values [46] to compute color images from this rep-
resentation. Besides the scene representation network, we
optimize for the camera poses and intrinsics. We initialize
the camera poses Ti using BundleFusion [14]. At evalua-
tion time, we use Marching Cubes [43] to extract a triangle
mesh from the optimized implicit scene representation.

3.1. Hybrid Scene Representation

Our method is built upon a hybrid scene representation
which combines an implicit surface representation with a

3

volumetric appearance representation. Specifically, we im-
plement this representation using a multi-layer perceptron
(MLP) which can be evaluated at arbitrary positions pi in
space to compute a truncated signed distance value Di and
view-dependent radiance value ci. As a conditioning to the
MLP, we use a sinusoidal positional encoding γ(·) [48] to
encode the 3D query point pi and the viewing direction d.
Inspired by the recent success of volumetric integration
in neural rendering [48], we render color as a weighted sum
of radiance values along a ray.
Instead of computing the
weights as probabilities of light reflecting at a given sample
point based on the density of the medium [48], we compute
weights directly from signed distance values as the product
of two sigmoid functions:

wi = σ

(cid:18) Di
tr

(cid:19)

(cid:18)

· σ

−

(cid:19)

,

Di
tr

(1)

where tr is the truncation distance. This bell-shaped func-
tion has its peak at the surface, i.e., at the zero-crossing of
the signed distance values. A similar formulation is used
in concurrent work [76], since this function produces unbi-
ased estimates of the signed distance field. The truncation
distance tr directly controls how quickly the weights fall to
zero as the distance from the surface increases. To account
for the possibility of multiple intersections, weights of sam-
ples beyond the first truncation region are set to zero. The
color along a specific ray is approximated as a weighted
sum of the K sampled colors:

C =

1
i=0 wi

(cid:80)K−1

K−1
(cid:88)

i=0

wi · ci.

(2)

This scheme gives the highest integration weight to the
point on the surface, while points farther away from the
surface have lower weights. Although such an approach
is not derived from a physically-based rendering model, as
is the case with volumetric integration over density values,
it represents an elegant way to render color in a signed dis-
tance field in a differentiable manner, and we show that it
helps deduce depth values through a photometric loss (see
Sec. 4).
In particular, this approach allows us to predict
hard boundaries between occupied and free space which
results in high-quality 3D reconstructions of the surface.
In contrast, density-based models [48] can introduce semi-
transparent matter in front of the actual surface to represent
view-dependent effects when integrated along a ray. This
leads to noisy reconstructions and artifacts in free space, as
can be seen in Sec. 4.

Network Architecture Our hybrid scene representation
network is composed of two MLPs which represent the
shape and radiance, as depicted in Fig. 2. The shape MLP
takes the encoding of a queried 3D point γ(p) as input and

outputs the truncated signed distance Di to the nearest sur-
face. The task of the second MLP is to produce the sur-
face radiance for a given encoded view direction γ(d) and
an intermediate feature output of the shape MLP. The view
vector conditioning allows our method to deal with view-
dependent effects like specular highlights, which would
otherwise have to be modeled by deforming the geome-
try. Since color data is often subject to varying exposure or
white-balance, we learn a per-frame latent corrective code
vector as additional input to the radiance MLP [45].
Pose and Camera Refinement The camera poses Ti, rep-
resented with Euler angles and a translation vector for ev-
ery frame, are initialized with BundleFusion [14] and re-
Inspired by [92], an addi-
fined during the optimization.
tional image-plane deformation field in form of a 6-layer
ReLU MLP is added as a residual to the pixel location be-
fore unprojecting into a 3D ray to account for possible dis-
tortions in the input images or inaccuracies of the intrin-
sic camera parameters. Note that this correction field is the
same for every frame. During optimization, camera rays are
first shifted with the 2D vector retrieved from the deforma-
tion field, before being transformed to world space using the
camera pose Ti.

3.2. Optimization

We optimize our scene representation network by ran-
domly sampling a batch of Pb pixels from the input dataset
of color and depth images. For each pixel p in the batch, a
ray is generated using its corresponding camera pose and Sp
sample points are generated on the ray. Our global objective
function L(P) is minimized w.r.t. the unknown parameters
P (the network parameters Θ and the camera poses Ti) over
all B input batches and is defined as:

L(P) =

B−1
(cid:88)

b=0

λ1Lb

rgb(P) + λ2Lb

f s(P) + λ3Lb

tr(P).

(3)

Lb
rgb(P) measures the squared difference between the ob-
served pixel colors ˆCp and predicted pixel colors Cp of the
b-th batch of rays:

Lb

rgb(P) =

1
|Pb|

(cid:88)

p∈Pb

(Cp − ˆCp)2.

(4)

f s is a ‘free-space’ objective, which forces the MLP to
p which lie between

Lb
predict a value of tr for samples s ∈ Sf s
the camera origin and the truncation region of a surface:

Lb

f s(P) =

1
|Pb|

(cid:88)

p∈Pb

1
|Sf s
p |

(cid:88)

s∈Sf s
p

(Ds − tr)2.

(5)

For samples within the truncation region (s ∈ Str
ply Lb

p ), we ap-
tr(P), the signed distance objective of samples close

4

to the surface:

Lb

tr(P) =

1
Pb

(cid:88)

p∈Pb

1
|Str
p |

(cid:88)

s∈Str
p

(Ds − ˆDs)2,

(6)

where Ds is the predicted signed distance of sample s, and
ˆDs the signed distance observed by the depth sensor, along
the optical axis. In our experiments, we use a truncation
distance tr = 5 cm, and scale the scene so that the trunca-
tion region maps to [−1, 1] (positive in front of the surface,
negative behind).

The Sp sample points on the ray are generated in two
steps. In the first step S′
c sample points are generated on the
ray using stratified sampling. Evaluating the MLP on these
S′
c sample points allows us to get a coarse estimate for the
ray depth by explicitly searching for the zero-crossing in the
predicted signed distance values. In the second step, another
S′
f sample points are generated around the zero-crossing
and a second forward pass of the MLP is performed with
these additional samples. The output of the MLP is con-
catenated to the output from the first step and color is inte-
grated using all S′
f samples, before computing the ob-
jective loss. It is important that the sampling rate in the first
step is high enough to produce samples within the trunca-
tion region of the signed distance field, otherwise the zero-
crossing may be missed.

c + S′

We implement our method in Tensorflow using the
ADAM optimizer [36] with a learning rate of 5 × 10−4 and
set the loss weights to λ1 = 0.1, λ2 = 10 and λ3 = 6×103.
We run all of our experiments for 2 × 105 iterations, where
in each iteration we compute the gradient w.r.t. |Pb| = 1024
randomly chosen rays. We set the number of S′
f samples to
16. S′
c is chosen such that there is on average one sample for
every 1.5 cm of the ray length. The ray length itself needs to
be greater than the largest distance in the scene that is to be
reconstructed and ranges from 4 to 8 meters in our scenes.

4. Results

In the following, we evaluate our method on real, as well
as on synthetic data. For the shown results, we use March-
ing Cubes [43] with a spatial resolution of 1 cm to extract a
mesh from the reconstructed signed distance function.

Results on real data. We test our method on the ScanNet
dataset [12] which provides RGB-D sequences of room-
scale scenes. The data has been captured with a StructureIO
camera which provides quality similar to that of a Kinect
v1. The depth measurements are noisy and often miss struc-
tures like chair legs or other thin geometry. To this end our
method proposes the additional usage of a dense color re-
construction loss, since regions that are missed by the range
sensor are often captured by the color camera. To compen-
sate for the exposure and white balancing of the used cam-
era, our approach learns a per-frame latent code as proposed

Method

C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

0.062
0.057
0.057
0.077
0.060
0.065

0.049
0.044

0.594
0.615
0.619
0.461
0.603
0.550

0.655
0.747

0.892
0.864
0.901
0.849
0.893
0.768

0.908
0.918

0.805
0.838
0.839
0.643
0.816
0.782

0.868
0.924

Table 1. Reconstruction results on a dataset of 10 synthetic scenes.
The Chamfer ℓ1 distance, normal consistency and the F-score [37]
are computed between point clouds sampled with a density of 1
point per cm2, using a threshold of 5 cm for the F-score. We
voxelize the mesh to compute the intersection-over-union (IoU)
between the predictions and ground truth.

In Fig. 3, we compare our method to the origi-
in [45].
nal ScanNet BundleFusion reconstructions which often suf-
fer from severe camera pose misalignment. Our approach
jointly optimizes for the scene representation network as
well as the camera poses, leading to substantially reduced
misalignment artifacts in the reconstructed geometry.

Quantitative evaluation. We perform a quantitative eval-
uation of our method on a dataset of 10 synthetic scenes
for which the ground truth geometry and camera trajectory
are known. Note that the ground truth camera trajectory is
only used for the rendering and evaluation, and not for the
reconstruction. For each frame, we render a photo-realistic
image using Blender [8, 17]. We apply noise and artifacts,
similar to those of a real depth sensor [1, 3, 30, 31]. On this
data, we compare our technique to several state-of-the-art
methods that use either depth input only, or both color and
depth data to reconstruct geometry (see Tab. 1).

BundleFusion. BundleFusion [14] uses the color and
depth input to reconstruct the scene. It is a classical depth
fusion approach [9] which compensates misalignments us-
ing a global bundle adjustment approach.

RoutedFusion. RoutedFusion [79] uses a routing net-
work which filters sensor-specific noise and outliers from
depth maps and computes pixel-wise confidence values,
which are used by a fusion network to produce the final
SDF. It takes the depth maps and camera poses as input.

COLMAP with screened Poisson surface reconstruction.
We obtain camera poses using COLMAP [63–65] and use
these to back-project depth maps into world space. We ob-
tain a mesh by applying screened Poisson surface recon-
struction [35] on the resulting point cloud.

Convolutional Occupancy Networks. We accumulate the
point clouds from the depth maps using BundleFusion poses
and evaluate the pre-trained convolutional occupancy net-
works model [58] provided by the authors (which has been
used on similar data [6]).

5

Figure 3. We compare our model without pose optimization and our full model with both the pose optimization and image-plane de-
formation field to BundleFusion, RoutedFusion, SIREN and a NeRF optimized with depth supervision in scenes 2, 5, 12, and 50 of the
ScanNet dataset. Our model without pose optimization recovers smoother meshes than the density-based NeRF model, but still suffers
from misalignment artifacts. These are solved by our full model to recover a clean reconstruction.

6

Figure 4. Accuracy shows how close ground truth points are to predicted points, while completeness shows how close predicted points are
to ground truth points. Geometry reconstructed purely through the photometric loss has slightly lower accuracy than geometry for which
depth observations were also available. Furthermore, the accuracy and completeness drop in distant areas, which had less multi-view
constraints and more noise in the depth measurements.

SIREN. We optimize a SIREN [67] per scene using the
back-projected point cloud data. The ICL-NUIM [31] scene
on which the method was originally tested, is also included
in our synthetic dataset.

NeRF with an additional depth loss. NeRF [48] proposes
using the expected ray termination distance as a way to vi-
sualize the depth of the scene. In our baseline, we add an
additional loss to NeRF, where this depth value is compared
to the input depth using an L2 loss. Note that this baseline
still uses NeRF’s density field to represent geometry.

As can be seen in Tab. 1, our approach with camera re-
finement results in the lowest Chamfer distance, and the
highest IoU, normal consistency (mean of the dot product of
the ground truth and predicted normals), and F-score [37].
Especially, the comparison to the density-based NeRF with
an additional depth constraint shows the benefit of our pro-
posed hybrid scene representation.
Ablation studies. We conduct ablation studies to justify our
choice of network architecture and training parameters. In
Fig. 3, we show the difference between a volumetric repre-
sentation (density field, ‘NeRF with Depth’) to an implicit
surface representation (signed distance field, ‘Ours-Full’)
on real data from ScanNet [12]. While representing scenes
with a density field works great for color integration, ex-
tracting the geometry itself is a challenging problem. Al-
though small variations in density may not affect the inte-
grated color much, they cause visible noise in the extracted
geometry and produce floating artifacts in free space. These
artifacts can be reduced by choosing a different iso-level for
geometry extraction with Marching Cubes, but this leads to
less complete reconstructions. In contrast, a signed distance

field models a smooth boundary between occupied and free
space, and we show that it can be faithfully represented by
an MLP. However, the reconstruction quality is still limited
by the provided camera poses, as can be seen in Fig. 3 (e.g.,
the cabinet in the left column). Optimizing for pose correc-
tions further improves the quality of our reconstructions.

Effect of the photometric term. A fundamental compo-
nent of our method is the use of a photometric term to infer
depth values which are missing from camera measurements.
We analyze the effect of this term on the synthetic scene in
Fig. 4, where we simulate missing geometry of the table
legs and the meshed basket. In the figure, we visualize the
completeness and accuracy. In contrast to a model without
the photometric term, our method is still able to reconstruct
the missing geometry leveraging the RGB observations.

For our full approach, we also separately evaluate the
reconstruction quality of geometry where depth measure-
ments were available and where they were missing. Re-
gions that relied only on color have a somewhat worse aver-
age accuracy of 11 mm, compared to 8 mm for regions that
had access to depth measurements. We refer the reader to
the supplemental material for more details and a qualitative
comparison on real data.

Effect of pose refinement. We show that initial camera
pose estimates can be further improved by jointly optimiz-
ing for the rotation and translation parameters of the cam-
eras which are initialized with BundleFusion [14]. We
quantitatively evaluate this on all scenes in our synthetic
dataset. An aggregate of the positional and rotational errors
of different methods is presented in Tab. 2. A detailed per-

7

Figure 5. Our method improves the camera alignment over the baseline, as visible in the tiles of the floor. The additional image-plane
distortion correction results in straight and aligned edges in the reconstruction.

Method

Pos. error (meters) ↓ Rot. error (degrees) ↓

Method

C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑

BundleFusion
COLMAP

Ours

0.033
0.038

0.021

0.571
0.692

0.144

Table 2. Based on our synthetic dataset, we evaluate the average
positional and rotational errors of the estimated camera poses. Our
method is able to further increase the pose estimation accuracy
compared to its BundleFusion initialization.

scene breakdown is given in the supplemental material. In
Tab. 1 and Fig. 3, we show that optimizing camera poses
reduces geometry misalignment artifacts and improves the
overall reconstruction, both quantitatively and qualitatively.

Effect of the image-plane deformation field. To evaluate
the effect of the pixel-space deformation field, we initial-
ize the camera with an incorrect focal length and optimize
our model with and without the deformation field. Tab. 3
shows that the deformation field mitigates this inaccuracy
in the camera’s intrinsic parameters which leads to signifi-
cantly better reconstruction results compared to the model
that does not use the deformation field. Fig. 5 showcases
the effects of our camera pose and image-plane deformation
field [14]. Blurry frames and sparse features lead to system-
atic camera pose errors in BundleFusion. Our method im-
proves these camera poses and the camera distortion model,
and, thus, is able to better align scene features, resulting in
higher reconstruction quality.

Limitations and future work. Similar to other methods
that are based on a scene representation which uses a scene-
specific MLP, our method runs offline (around 9 hours for
2 × 105 iterations using an NVIDIA RTX 3090). Re-
cent methods that utilize voxel grids to optimize a radi-
ance field [70, 86] have shown significantly faster conver-
gence compared to earlier MLP-based methods and we be-
lieve that they would also be good candidates for improv-
ing our method. Nonetheless, our proposed method offers a
high-quality scene reconstruction which outperforms online
fusion approaches. Another limitation is the global MLP

Ours (w/o IPDF)
Ours (w/ IPDF)

0.061
0.031

0.266
0.609

0.886
0.911

0.406
0.904

Table 3. Ablation of the image-plane deformation field (IPDF)
which compensates image space distortions and incorrect intrinsic
parameters. The experiment is based on a synthetic scene, where
we assume an incorrect focal length of 570 instead of 554.26 (GT).

which stores the entire scene information which comes at
the cost of missing high-frequency local detail in very large
scenes. Approaches like IF-Nets [7] or Convolutional Oc-
cupancy Networks [58] benefit from locally-conditioned
MLPs and can be integrated in future work. Finally, our
method was designed to handle only opaque surfaces.

5. Conclusion

We have presented a new method for 3D surface re-
construction from RGB-D sequences by introducing a hy-
brid scene representation that is based on an implicit sur-
face function and a volumetric representation of radiance.
This allows us to efficiently incorporate depth observations,
while still benefiting from the differentiable volumetric ren-
dering of the original neural radiance field formulation. As
a result, we obtain high-quality surface reconstructions, out-
performing traditional and learned RGB-D fusion methods.
Overall, we believe our work is a stepping stone towards
leveraging the success of implicit, differentiable represen-
tations for 3D surface reconstruction.

Acknowledgements

This work was supported by a Google Gift Grant, a
TUM-IAS Rudolf M¨oßbauer Fellowship, an NVidia Pro-
fessorship Award,
the ERC Starting Grant Scan2CAD
(804724), and the German Research Foundation (DFG)
Grant Making Machine Learning on Static and Dynamic 3D
Data Practical. We would also like to thank Angela Dai for
the video voice-over.

8

References

[1] Jonathan T. Barron and Jitendra Malik. Intrinsic scene prop-

erties from a single rgb-d image. CVPR, 2013. 5, 14

[2] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan
Leutenegger, and Andrew J. Davison. Codeslam — learn-
ing a compact, optimisable representation for dense visual
slam. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018. 2
[3] Jeannette Bohg, Javier Romero, Alexander Herzog, and Ste-
fan Schaal. Robot arm pose estimation through pixel-wise
part classification. ICRA, 2014. 5, 14

[4] Aljaˇz Boˇziˇc, Pablo Palafox, Justus Thies, Angela Dai, and
Matthias Nießner. Transformerfusion: Monocular rgb scene
reconstruction using transformers. Proc. Neural Information
Processing Systems (NeurIPS), 2021. 2

[5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and
Gordon Wetzstein. pi-gan: Periodic implicit generative ad-
versarial networks for 3d-aware image synthesis. In arXiv,
2020. 3

[6] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-
d data in indoor environments. International Conference on
3D Vision (3DV), 2017. 5

[7] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.
Implicit functions in feature space for 3d shape reconstruc-
tion and completion. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR). IEEE, jun 2020. 2,
8

[8] Blender Online Community. Blender - a 3D modelling and
rendering package. Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 5

[9] Brian Curless and Marc Levoy. A volumetric method for
In Proceed-
building complex models from range images.
ings of the 23rd Annual Conference on Computer Graphics
and Interactive Techniques, SIGGRAPH ’96, page 303–312,
New York, NY, USA, 1996. Association for Computing Ma-
chinery. 1, 2, 5

[10] J Czarnowski, T Laidlow, R Clark, and AJ Davison. Deep-
factors: Real-time probabilistic dense monocular slam. IEEE
Robotics and Automation Letters, 5:721–728, 2020. 2
[11] Manuel Dahnert, Ji Hou, Matthias Nießner, and Angela Dai.
Panoptic 3D scene reconstruction from a single RGB image.
2021. 2

[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE, 2017. 2, 5, 7

[13] Angela Dai, Christian Diller, and Matthias Nießner. Sg-nn:
Sparse generative neural networks for self-supervised scene
In Proc. Computer Vision and
completion of rgb-d scans.
Pattern Recognition (CVPR), IEEE, 2020. 2

[14] Angela Dai, Matthias Nießner, Michael Zollh¨ofer, Shahram
Izadi, and Christian Theobalt. Bundlefusion: Real-time
globally consistent 3d reconstruction using on-the-fly sur-

face reintegration. ACM Transactions on Graphics (TOG),
36(4):76a, 2017. 2, 3, 4, 5, 7, 8, 13, 14, 16

[15] Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin,
and Matthias Nießner. Spsg: Self-supervised photometric
scene generation from rgb-d scans. In Proc. Computer Vision
and Pattern Recognition (CVPR), IEEE, 2021. 2

[16] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. arXiv preprint arXiv:2107.02791, 2021. 2, 3

[17] Maximilian Denninger, Martin Sundermeyer, Dominik
Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad El-
badrawy, Ahsan Lodhi, and Harinandan Katam. Blender-
proc. arXiv preprint arXiv:1911.01911, 2019. 5, 14

[18] Maximilian Denninger and Rudolph Triebel. 3d scene re-
construction from a single viewport. In Proceedings of the
European Conference on Computer Vision (ECCV), 2020. 2
[19] Wei Dong, Qiuyuan Wang, Xin Wang, and Hongbin Zha.
PSDF fusion: Probabilistic signed distance function for on-
the-fly 3d data fusion and scene reconstruction. CoRR,
abs/1807.11034, 2018. 2

[20] J. Engel, T. Sch¨ops, and D. Cremers. LSD-SLAM: Large-
scale direct monocular SLAM. In European Conference on
Computer Vision (ECCV), September 2014. 2

[21] J. Engel, J. Sturm, and D. Cremers. Semi-dense visual odom-
In 2013 IEEE International
etry for a monocular camera.
Conference on Computer Vision, pages 1449–1456, 2013. 2
[22] John Flynn, Ivan Neulander, James Philbin, and Noah
Snavely. Deepstereo: Learning to predict new views from the
world’s imagery. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5515–5524,
2016. 2

[23] C. Forster, M. Pizzoli, and D. Scaramuzza. Svo: Fast semi-
In 2014 IEEE Inter-
direct monocular visual odometry.
national Conference on Robotics and Automation (ICRA),
pages 15–22, 2014. 2

[24] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2002–2011, 2018. 2

[25] Y. Furukawa and J. Ponce. Accurate, dense, and robust multi-
view stereopsis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(8):1362–1376, 2010. 2

[26] Guy Gafni, Justus Thies, Michael Zollh¨ofer, and Matthias
Nießner. Dynamic neural radiance fields for monocular 4d
facial avatar reconstruction. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2021.
3

[27] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh

R-CNN. CoRR, abs/1906.02739, 2019. 2

[28] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 270–279,
2017. 2

[29] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M.
Seitz. Multi-view stereo for community photo collections.

9

In 2007 IEEE 11th International Conference on Computer
Vision, pages 1–8, 2007. 2

[30] Ankur Handa. Simulating kinect noise for the icl-nuim
https : / / github . com / ankurhanda /

dataset.
simkinect. Accessed: 2021-11-15. 5, 14

[31] Ankur Handa, Thomas Whelan, John McDonald, and An-
drew J Davison. A benchmark for rgb-d visual odometry, 3d
reconstruction and slam. ICRA, 2014. 5, 7, 14

[32] Vu Hoang Hiep, Renaud Keriven, Patrick Labatut, and Jean-
Philippe Pons. Towards high-resolution large-scale multi-
view stereo. In 2009 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1430–1437. IEEE, 2009. 2

[33] Shahram Izadi, David Kim, Otmar Hilliges, David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568. ACM, 2011. 1

[34] Abhishek Kar, Christian H¨ane,

Learning a multi-view stereo machine.
arXiv:1708.05375, 2017. 2

and Jitendra Malik.
arXiv preprint

[35] Michael Kazhdan and Hugues Hoppe. Screened poisson sur-
face reconstruction. ACM Trans. Graph., 32(3), July 2013.
5

[36] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014. 5, 13
[37] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. ACM Trans. Graph., 36(4), July 2017. 5, 7,
18, 19

[38] Kiriakos N Kutulakos and Steven M Seitz. A theory of shape
by space carving. International journal of computer vision,
38(3):199–218, 2000. 2

[39] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 2016 Fourth
international conference on 3D vision (3DV), pages 239–
248. IEEE, 2016. 2

[40] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene flow fields for space-time view synthesis of dy-
namic scenes. arXiv preprint arXiv:2011.13084, 2020. 3
[41] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
mon Lucey. Barf: Bundle-adjusting neural radiance fields. In
IEEE International Conference on Computer Vision (ICCV),
2021. 3

[42] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural
volumes. ACM Transactions on Graphics, 38(4):1–14, Jul
2019. 1

[43] William E. Lorensen and Harvey E. Cline. Marching cubes:
A high resolution 3d surface construction algorithm.
In
Proceedings of the 14th Annual Conference on Computer
Graphics and Interactive Techniques, SIGGRAPH ’87, page
163–169, New York, NY, USA, 1987. Association for Com-
puting Machinery. 3, 5

10

[44] D. G. Lowe. Object recognition from local scale-invariant
features. In Proceedings of the Seventh IEEE International
Conference on Computer Vision, volume 2, pages 1150–
1157 vol.2, 1999. 2

[45] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,
Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. NeRF in the Wild: Neural Radiance Fields for Un-
constrained Photo Collections. In CVPR, 2021. 3, 4, 5
[46] N. Max. Optical models for direct volume rendering.
IEEE Transactions on Visualization and Computer Graph-
ics, 1(2):99–108, 1995. 3

[47] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
In Proceed-
Learning 3d reconstruction in function space.
ings IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2019. 2

[48] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV, 2020. 1, 2, 4, 7, 13, 16

[49] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller,
C. R.A. Chaitanya, A. Kaplanyan, and M. Steinberger. DON-
eRF: Towards Real-Time Rendering of Compact Neural Ra-
diance Fields using Depth Oracle Networks. Computer
Graphics Forum, 40(4):45–59, 2021. 2, 3

[50] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J Davison, Pushmeet
Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.
Kinectfusion: Real-time dense surface mapping and track-
In Mixed and augmented reality (ISMAR), 2011 10th
ing.
IEEE international symposium on, pages 127–136. IEEE,
2011. 2

[51] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng,
Jian Chang, and Jian Jun Zhang. Total3DUnderstanding:
Joint layout, object pose and mesh reconstruction for indoor
scenes from a single image. pages 55–64, 2020. 2

[52] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision.
In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2020. 2

[53] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger.
Real-time 3d reconstruction at scale using voxel hashing.
ACM Transactions on Graphics (TOG), 2013. 1, 2

[54] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo
Strauss, and Andreas Geiger. Texture fields: Learning tex-
ture representations in function space. In International Con-
ference on Computer Vision, Oct. 2019. 2

[55] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In International Con-
ference on Computer Vision (ICCV), 2021. 2

[56] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019. 2

[57] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-
Martin Brualla. Deformable neural radiance fields. arXiv
preprint arXiv:2011.12948, 2020. 3

[58] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
In European Conference on Computer Vision
networks.
(ECCV), Cham, Aug. 2020. Springer International Publish-
ing. 2, 5, 8

[59] Shunsuke Saito, , Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. arXiv preprint arXiv:1905.05172, 2019. 2

[60] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, June 2020. 2

[61] Nikolay Savinov, Christian H¨ane, L’ubor Ladick´y, and Marc
Pollefeys. Semantic 3d reconstruction with continuous reg-
ularization and ray potentials using a visibility consistency
In 2016 IEEE Conference on Computer Vision
constraint.
and Pattern Recognition (CVPR), pages 5460–5469, 2016. 2
[62] D. Scharstein, R. Szeliski, and R. Zabih. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms. In Proceedings IEEE Workshop on Stereo and Multi-
Baseline Vision (SMBV 2001), pages 131–140, 2001. 2
[63] Johannes L. Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2016. 2, 5

[64] Johannes Lutz Sch¨onberger, True Price, Torsten Sattler, Jan-
Michael Frahm, and Marc Pollefeys. A vote-and-verify strat-
egy for fast spatial verification in image retrieval. In Asian
Conference on Computer Vision (ACCV), 2016. 5

[65] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys,
and Jan-Michael Frahm. Pixelwise view selection for un-
In European Conference on
structured multi-view stereo.
Computer Vision (ECCV), 2016. 5

[70] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. arXiv preprint arXiv:2111.11215, 2021. 8

[71] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and
Hujun Bao. NeuralRecon: Real-time coherent 3D recon-
struction from monocular video. CVPR, 2021. 2

[72] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-
tures let networks learn high frequency functions in low di-
mensional domains. NeurIPS, 2020. 3

[73] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,
Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-
Brualla, Tomas Simon, Jason Saragih, Matthias Nießner,
In Computer
et al. State of the art on neural rendering.
Graphics Forum, volume 39, pages 701–727. Wiley Online
Library, 2020. 1, 2

[74] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-
vasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,
Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-
bardi, Tomas Simon, Christian Theobalt, Matthias Niess-
ner, Jonathan T. Barron, Gordon Wetzstein, Michael Zoll-
hoefer, and Vladislav Golyanik. Advances in neural render-
ing, 2021. 2

[75] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh
models from single rgb images. In ECCV, 2018. 2

[76] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
arXiv preprint arXiv:2106.10689, 2021. 2, 3, 4, 16

[77] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-
vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser.
Ibrnet:
Learning multi-view image-based rendering. CVPR, 2021.
3

[78] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-
tor Adrian Prisacariu. Nerf–: Neural radiance fields without
known camera parameters, 2021. 3

[66] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance fields for 3d-aware image
In Advances in Neural Information Processing
synthesis.
Systems (NeurIPS), 2020. 3

[79] Silvan Weder, Johannes L. Sch¨onberger, Marc Pollefeys, and
Martin R. Oswald. Routedfusion: Learning real-time depth
map fusion. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), June 2020. 2, 5

[67] Vincent Sitzmann,

Julien N.P. Martel, Alexander W.
Bergman, David B. Lindell, and Gordon Wetzstein. Implicit
neural representations with periodic activation functions. In
arXiv, 2020. 2, 3, 7

[68] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Nießner, Gordon Wetzstein, and Michael Zollhofer. Deep-
voxels: Learning persistent 3d feature embeddings. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 2437–2446, 2019. 2

[80] Silvan Weder, Johannes L. Schonberger, Marc Pollefeys, and
Martin R. Oswald. Neuralfusion: Online depth fusion in
In Proceedings of the IEEE/CVF Conference
latent space.
on Computer Vision and Pattern Recognition (CVPR), pages
3162–3172, June 2021. 2

[81] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu,
and Jie Zhou. Nerfingmvs: Guided optimization of neural
radiance fields for indoor multi-view stereo. In ICCV, 2021.
2, 3

[69] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wet-
Scene representation networks: Continuous 3d-
In Advances

zstein.
structure-aware neural scene representations.
in Neural Information Processing Systems, 2019. 2

[82] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.
Mvsnet: Depth inference for unstructured multi-view stereo.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 767–783, 2018. 2

11

[83] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-
ume rendering of neural implicit surfaces. Advances in Neu-
ral Information Processing Systems, 34, 2021. 2, 16

[84] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and ap-
pearance. Advances in Neural Information Processing Sys-
tems, 33, 2020. 2

[85] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto
Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting
neural radiance fields for pose estimation. 2020. 3

[86] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox-
els: Radiance fields without neural networks. arXiv preprint
arXiv:2112.05131, 2021. 8

[87] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images. In
CVPR, 2021. 3

[88] Cem Yuksel. A class of c2 interpolating splines. ACM Trans.

Graph., 39(5), aug 2020. 14

[89] Christopher Zach, Thomas Pock, and Horst Bischof. A glob-
ally optimal algorithm for robust tv-l1 range image integra-
tion. In 2007 IEEE 11th International Conference on Com-
puter Vision, pages 1–8, 2007. 2

[90] Yinda Zhang and Thomas Funkhouser. Deep depth comple-
In Proceedings of the IEEE
tion of a single rgb-d image.
Conference on Computer Vision and Pattern Recognition,
pages 175–185, 2018. 2

[91] Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, and
Andrew J. Davison. Scenecode: Monocular dense semantic
reconstruction using learned encoded scene representations.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019. 2
[92] Qian-Yi Zhou and Vladlen Koltun. Color map optimization
for 3d reconstruction with consumer depth cameras. 33(4),
July 2014. 4, 16

[93] M. Zollh¨ofer, P. Stotko, A. G¨orlitz, C. Theobalt, M. Nießner,
R. Klein, and A. Kolb. State of the Art on 3D Reconstruc-
tion with RGB-D Cameras. Computer Graphics Forum (Eu-
rographics State of the Art Reports 2018), 37(2), 2018. 2

12

Figure 6. Our method obtains a high-quality 3D reconstruction from an RGB-D input sequence by training a multi-layer perceptron.
In comparison to state-of-the-art methods like BundleFusion [14] or the theoretical NeRF [48] with additional depth constraints, our
approach results in cleaner and more complete reconstructions. As can be seen, the pose optimization of our approach is key to resolving
misalignment artifacts.

APPENDIX

In this appendix we show a per-scene breakdown of the
quantitative evaluation from Tab. 1, an ablation study on
additional scenes from the ScanNet dataset (see Fig. 6), as
well as further ablation studies on synthetic data. For the
purpose of reproducibility, we also provide further details
on the parameters that were used for optimization in each
of the scenes.

A. Implementation Details

We implement our method in TensorFlow v2.4.1 using
the ADAM [36] optimizer with a learning rate of 5 × 10−4
and an exponential learning rate decay of 10−1 over 2.5 ×
105 iterations.
In each iteration, we compute a gradient
w.r.t. |Pb| = 1024 randomly chosen rays. We set the num-
ber of S′
c is chosen so that there is on av-
erage one sample for every 1.5 cm of the ray length. Tab. 4
gives an overview of ray length and number of samples for
each of the experiments. Internally, we translate and scale
each scene so that it lies within a [−1, 1]3 cube. Depending

f samples to 16. S′

on scene size, our method takes between 9 and 13 hours to
converge on a single NVIDIA RTX 3090 (see Sec. F). We
set the loss weights to λ1 = 0.1, λ2 = 10 and λ3 = 6×103.
We use 8 bands for the positional encoding of the point co-
ordinates and 4 bands to encode the view direction vector.

To account for distortions or inaccuracies of the intrin-
sic parameters, a 2D deformation field of the camera pixel
space in form of a 6-layer MLP, with a width of 128, is used.

B. Per-scene Quantitative Evaluations

In Tab. 8 and Tab. 9 we present a per-scene breakdown
of the quantitative analysis from the main paper (see Sec. 4,
Tab. 1 and Tab. 2 in the main paper). The corresponding
qualitative results are shown in Fig. 14 and Fig. 15.

Reconstruction Evaluation. The goal of our method is
to reconstruct a scene from color and depth data, i.e., we
do not aim for scene completion. To evaluate the recon-
struction quality, we evaluate the quality of reconstruc-
tions w.r.t. Chamfer distance (C-ℓ1),
intersection-over-
union (IoU), normal consistency (NC) based on cosine sim-
ilarity, and F-score. These metrics are computed on surfaces

13

Scene

Scene 0
Scene 2
Scene 5
Scene 12
Scene 24
Scene 50
Scene 54

Breakfast room
Green room
Grey-white room
ICL living room
Kitchen 1
Kitchen 2
Morning apartment
Staircase
Thin geometry
White room

S′
c

512
256
256
320
512
256
256

320
512
512
320
512
640
256
512
256
512

ray length
(m)

#f rames

8
4
4
5
8
4
4

5
8
8
5
8
10
4
8
4
8

1394
1299
1159
1335
849
1163
1250

1167
1442
1493
1510
1517
1221
920
1149
395
1676

Table 4. We list the number of samples S′
c and the ray length in
meters that were used to reconstruct each of the ScanNet scenes
and the synthetic scenes. Note that these settings are dependent on
the scene size.

which were visible in the color and depth streams (geometry
within the viewing frusta of the input images). Specifically,
we subdivide all meshes to have a maximum edge length of
below 1.5 cm and use the ground truth trajectory to detect
vertices which are visible in at least one camera. Triangles
which have no visible vertices, either due to not being in any
of the viewing frusta or due to being occluded by other ge-
ometry, are culled. This is necessary to avoid computing the
error in regions such as occluded geometry in the synthetic
ground truth mesh or in regions where the network output
is unpredictable because the region was never seen at train-
ing time. The culled geometry is sampled with a density of
1 point per cm2 and the error metrics are evaluated on the
sampled point clouds. To evaluate the IoU, we voxelize the
reconstruction using voxels with an edge length of 5 cm.
The F-score is also computed using a 5 cm threshold.

Synthetic Dataset. Our synthetic dataset which we use
for numeric evaluation purposes consists of 10 scenes pub-
lished under either the CC-BY or CC-0 license (see Tab. 5).
We define a trajectory by a Catmull-Rom spline interpola-
tion [88] on several manually chosen control points. We
use BlenderProc [17] to render color and depth images for
each camera pose in the interpolated trajectory. Noise is ap-
plied to the depth maps to simulate sensor noise of a real
depth sensor [1, 3, 30, 31]. For the ICL scene [31], we use
the color and noisy depth provided by the authors and do
not render our own images. The scenes in the dataset have

Scene

ScanNet

URL

http://www.scan-net.org/

License

MIT

https://blendswap.com/blend/13363
Breakfast room
Green room
https://blendswap.com/blend/8381
Grey-white room https://blendswap.com/blend/13552
ICL living room
Kitchen 1
Kitchen 2
Morning apart.
Staircase
Thin geometry
White room

CC-BY
CC-BY
CC-BY
https://www.doc.ic.ac.uk/ ahanda/VaFRIC/iclnuim.html CC-BY
CC-BY
https://blendswap.com/blend/5156
CC-0
https://blendswap.com/blend/11801
CC-0
https://blendswap.com/blend/10350
CC-BY
https://blendswap.com/blend/14449
CC-BY
https://blendswap.com/blend/8381
CC-BY
https://blendswap.com/blend/5014

Table 5. Source and license information of the used data.

various sizes, complexity and materials like highly specu-
lar surfaces or mirrors. BundleFusion [14] is used to get an
initial estimate of the camera trajectory. This estimated tra-
jectory is used by all methods other than COLMAP to allow
a fair comparison.

Figure 7. The photometric energy term encourages correct depth
prediction in areas where the depth sensor did not capture any
depth measurements.

C. Ablation Studies

In this section, we present additional details for the ab-
lation studies described in the main paper, and show fur-
ther studies to test the robustness and the limitations of our
method. In Fig. 6, the additional results on real data demon-
strate the advantages of the signed distance field and our
camera refinement.

C.1. Effect of the Photometric Energy Term

In Tab. 6, we list the quantitative evaluation of the exper-
iment on the effectiveness of the photometric energy term
from Fig. 4 in the main paper. Fig. 7 shows the effect of
the term on a real scene from the ScanNet dataset. The

14

Method

C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑

and leading to large errors in geometry reconstruction.

Ours (depth-only)
Ours (full)

0.017
0.009

0.791
0.865

0.910
0.910

0.944
0.995

Table 6. Detailed reconstruction results for Fig. 4 from the main
paper. Our method reconstructs geometry visible only in color im-
ages, leading to significantly better reconstruction results in scenes
with geometry which is not captured by the depth sensor.

legs of the piano stool were not visible in any of the depth
maps. Nevertheless, our method is able to reconstruct them
by making use of the corresponding color data.

C.2. Number of Input Frames

The reconstruction quality of any reconstruction method
is dependent on the number of input frames. We evalu-
ate our method on the ‘whiteroom’ synthetic scene through
multiple experiments in which we remove different num-
bers of frames in the dataset used for optimization. Re-
construction results are presented in Fig. 8. Note that for
these experiments we use the camera poses initialized with
BundleFusion which uses all 1676 depth frames.

C.3. Robustness to Noisy Pose Initialization

To analyze the robustness of our method w.r.t. presence
of inaccuracies in camera alignment, we apply Gaussian
noise to every camera’s position and direction in the ‘white-
room’ scene.
In Fig. 9 we present reconstruction results
for poses of increasing inaccuracy. We separately show the
pose errors of the refined cameras in Fig. 10. On the recon-
struction metrics, our method is robust to camera position
and orientation errors of up to 5 cm and 5◦ respectively.
The pose refinement is robust up to a noise level of 3 cm
and 3◦. At noise levels with a standard deviation of 10 cm
and higher, some cameras are initially positioned inside ge-
ometry, preventing our method from refining their position

Figure 9. We test
the robustness of our reconstructions to
noise in the initial camera position and direction. Our method
achieves good results even in the presence of significant noise. At
σ = 10 cm, some of the cameras intersect geometry, degrading the
reconstruction quality.

Figure 10. We test the robustness of our pose refinement to noise
in the initial camera position and direction. The rotation error has
been scaled by a factor of 10 for better visibility. Our method
is able to correct poses even in the presence of significant noise.
At σ = 10 cm, some of the cameras start intersecting geometry,
making refinement impossible.

Truncation (cm) C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑

2
3
5
10

0.053
0.023
0.021
0.024

0.671
0.766
0.786
0.742

0.855
0.901
0.912
0.908

0.862
0.930
0.933
0.912

Figure 8. We test the robustness of our method by remov-
ing frames from the dataset used for optimization. Our method
achieves good reconstruction results using as few as 13 frames.

Table 7. Impact of the truncation region width on reconstruction
quality.

15

of the advantages and drawbacks of classic reconstruction
methods [14, 92] and MLP-based radiance fields [48] when
synthesizing unseen views. Classic reconstruction methods
usually do not try to decouple intrinsic material parame-
ters [14, 92] and instead optimize a texture that represents
the average observation of all the input views. The result-
ing texture is usually high-resolution (bounded by the res-
olution of the input images), but does not allow for correct
synthesis of view-dependent effects. Furthermore, inaccu-
racies in camera calibration may lead to visible seams in
the optimized texture. Methods like NeRF that focus purely
on high-quality novel view synthesis do not explicitly re-
construct geometry and may thus produce images riddled
with artifacts for views that are too far from the input views.
We believe that it is possible to combine both of these ap-
proaches to improve novel view synthesis on views far away
from the ones used during the optimization and would like
to encourage research in this direction. Fig. 13 shows an
example view synthesis result on the ScanNet dataset, for
an out-of-trajectory camera position and orientation.

Figure 11. Reconstruction quality with varying batch size.

C.4. Batch Size

Optimization with a lower batch size leads to more noise
and might miss areas without depth supervision due to a
lower number of multi-view constraints within the batch.
A batch size that is too large will slow down the optimiza-
tion and consume more GPU memory, while not offering
improvements in reconstruction quality (see Fig. 11).

C.5. Truncation Size

The reconstruction quality is dependent on the width of
the truncation region, as shown in Tab. 7. The truncation re-
gion needs to account for the noise in the input (i.e., needs to
be greater than the noise of the depth camera). In our experi-
ments a truncation radius of tr = 5 cm gives the best results
(evaluated based on the mean across multiple scenes).

D. Comparison to RGB-based methods

NeuS [76] and VolSDF [83] are concurrent works that
propose learning a signed distance field of an object from a
set of RGB images. In contrast to these methods, our focus
lies on reconstructing indoor scenes which often have large
textureless regions (e.g., a white wall). Methods which
use only color input will not have enough multi-view con-
straints to properly reconstruct these regions. In Fig. 12, we
show a case where methods that rely only on color input
struggle to reconstruct high-quality geometry.

Figure 13. We compare the color synthesis of BundleFusion and
NeRF-style methods. NeRF without any depth constraints shows
severe fogging when rendering an image from a novel view. This
gets resolved after adding depth constraints to the optimization.
BundleFusion produces the sharpest results, but suffers from in-
correct view-dependent effects and misalignment artifacts. Our
method produces results similar to NeRF with a depth constraint.
A combination of classic and NeRF-style methods may yield both
high-quality geometry and high-quality view synthesis and we en-
courage further research in this direction.

Figure 12. Comparison between NeuS and our method on the
‘morning apartment’ scene.

E. Color Reproduction of Classic and NeRF-

style Methods

While our focus lies on geometry reconstruction and
not accurate view synthesis, we conducted a brief analysis

16

F. Runtime and Memory Requirements

Our method. The runtime and memory requirements of our
method are dependent on the scene size. For smaller scenes
where it is enough to have S′
c = 256 samples, our method
completes 2 × 105 iterations in 9 hours on an NVIDIA RTX
3090 and requires 8.5 GB of GPU memory. When S′
c is set
to 512, the runtime increases to 13 hours and the memory
requirement to 10.5 GB. The memory consumption can be
reduced by using smaller batches.

BundleFusion. We run BundleFusion at a voxel resolution
of 1 cm for all scenes. On an NVIDIA GTX TITAN Black,
depending on the size of the scene and number of frames in
the camera trajectory, it takes 10 to 40 minutes to integrate
the depth frames into a truncated signed distance field and
extract a mesh using Marching Cubes. The memory usage
is around 5.8 GB.

RoutedFusion. To train and test RoutedFusion, we used an
NVIDIA RTX 3090. The routing network was trained for
24 hours on images with a resolution of 320×240 pixels. As
per suggestion of the authors, we train the fusion network
for 20 epochs which takes about 1.5 hours. We reconstruct
all scenes at a voxel resolution of 1 cm for a fair compari-
son to other methods. The runtime ranges from 40 minutes
to 6 hours depending on scene size and number of frames.
The memory usage also heavily depends on scene size and
ranges from 5.5 GB to 23 GB.

COLMAP + Poisson. In the COLMAP + Poisson base-
line, the bottleneck is the global bundle adjustment process
performed by COLMAP. The total runtime depends on the
number of frames in the trajectory. Using all 8 cores of
an Intel i7-7700K CPU, it took us about 4 hours to align
all 1167 cameras in the ‘breakfast room’. The couple of
minutes needed to backproject all depth maps at full reso-
lution and run the screened Poisson surface reconstruction
are negligible in comparison.

Convolutional Occupancy Networks. We reconstruct
each scene using the pre-trained model provided by the au-
thors. This takes about 2 minutes per scene and requires
about 10 GB of memory.

SIREN. We train SIREN for 104 epochs on each scene.
SIREN is trained over the complete point cloud in each
epoch, so the runtime depends on the number of points in
the point cloud. In our experiments on an NVIDIA RTX
3090, this ranged from 6 to 12 hours with 12 GB of mem-
ory being in use.

NeRF + Depth. We optimize NeRF using 64 samples for
the coarse network and 128 samples for the fine network.
On an NVIDIA RTX 3090 it takes 6 hours for 2 × 105 iter-
ations to run. The memory usage is 4.7 GB.

17

Scene

Breakfast room

Green room

Grey-white room

ICL living room

Kitchen 1

Method

C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑ Pos. error ↓ Rot. error ↓

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

0.033
0.033
0.033
0.047
0.060
0.041

0.031
0.030

0.024
0.018
0.018
0.053
0.023
0.030

0.014
0.013

0.038
0.033
0.029
0.048
0.033
0.040

0.032
0.015

0.018
0.019
0.023
0.112
0.020
0.021

0.014
0.011

0.234
0.265
0.252
0.262
0.265
0.271

0.255
0.252

0.698
0.714
0.668
0.474
0.566
0.619

0.720
0.793

0.694
0.755
0.849
0.554
0.746
0.668

0.766
0.921

0.567
0.606
0.727
0.480
0.635
0.563

0.640
0.886

0.743
0.698
0.727
0.352
0.768
0.689

0.790
0.905

0.368
0.401
0.459
0.352
0.357
0.336

0.420
0.447

0.944
0.918
0.935
0.879
0.922
0.811

0.930
0.934

0.923
0.904
0.925
0.855
0.913
0.748

0.931
0.932

0.860
0.850
0.899
0.841
0.868
0.764

0.864
0.924

0.956
0.939
0.947
0.841
0.950
0.900

0.964
0.969

0.860
0.805
0.888
0.839
0.850
0.710

0.887
0.886

0.890
0.901
0.893
0.780
0.822
0.854

0.914
0.920

0.926
0.969
0.967
0.737
0.940
0.871

0.982
0.990

0.751
0.790
0.899
0.601
0.812
0.697

0.806
0.987

0.958
0.976
0.966
0.507
0.967
0.956

0.992
0.994

0.620
0.680
0.748
0.483
0.575
0.600

0.700
0.718

0.037
-
0.009
-
-
-

-
0.007

0.027
-
0.014
-
-
-

-
0.012

0.056
-
0.029
-
-
-

-
0.014

0.022
-
0.029
-
-
-

-
0.007

0.038
-
0.103
-
-
-

-
0.030

0.697
-
0.210
-
-
-

-
0.135

0.546
-
0.227
-
-
-

-
0.104

1.891
-
0.296
-
-
-

-
0.146

0.382
-
0.836
-
-
-

-
0.109

0.327
-
0.941
-
-
-

-
0.114

Table 8. We compare the quality of our reconstruction on several synthetic scenes for which ground truth data is available. The Chamfer ℓ1
distance, normal consistency and the F-score [37] are computed between point clouds sampled with a density of 1 point per cm2. We use
a threshold of 5 cm for the F-score. We further voxelize each mesh to compute the intersection-over-union (IoU) between the predictions
and ground truth.

18

Scene

Kitchen 2

Morning apartment

Staircase

Thin geometry

White room

Method

C-ℓ1 ↓

IoU ↑ NC ↑

F-score ↑ Pos. error ↓ Rot. error ↓

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

BundleFusion
RoutedFusion
COLMAP + Poisson
Conv. Occ. Nets
SIREN
NeRF + Depth

Ours (w/o pose)
Ours

0.089
0.059
0.037
0.052
0.055
0.051

0.034
0.032

0.012
0.013
0.017
0.045
0.013
0.022

0.011
0.011

0.091
0.069
0.074
0.069
0.067
0.087

0.057
0.045

0.019
0.023
0.047
0.022
0.021
0.014

0.009
0.009

0.062
0.038
0.036
0.061
0.046
0.073

0.034
0.028

0.441
0.572
0.675
0.484
0.453
0.435

0.488
0.637

0.767
0.815
0.668
0.450
0.727
0.587

0.787
0.716

0.373
0.340
0.322
0.315
0.432
0.396

0.457
0.565

0.764
0.708
0.440
0.723
0.733
0.825

0.857
0.865

0.528
0.545
0.652
0.424
0.617
0.385

0.631
0.738

0.856
0.842
0.919
0.861
0.898
0.708

0.908
0.903

0.885
0.870
0.877
0.802
0.873
0.838

0.887
0.888

0.860
0.864
0.895
0.838
0.885
0.644

0.899
0.920

0.909
0.829
0.820
0.882
0.887
0.847

0.911
0.910

0.869
0.817
0.904
0.853
0.888
0.716

0.902
0.911

0.687
0.787
0.818
0.653
0.735
0.630

0.796
0.890

0.968
0.976
0.959
0.784
0.966
0.975

0.983
0.982

0.623
0.622
0.628
0.508
0.676
0.624

0.704
0.853

0.922
0.881
0.721
0.910
0.913
0.989

0.995
0.995

0.701
0.799
0.796
0.470
0.752
0.619

0.813
0.915

0.050
-
0.043
-
-
-

-
0.083

0.008
-
0.017
-
-
-

-
0.005

0.039
-
0.043
-
-
-

-
0.016

0.009
-
0.079
-
-
-

-
0.010

0.045
-
0.018
-
-
-

-
0.028

0.566
-
1.154
-
-
-

-
0.450

0.165
-
0.380
-
-
-

-
0.093

0.643
-
0.305
-
-
-

-
0.123

0.126
-
2.400
-
-
-

-
0.037

0.375
-
0.167
-
-
-

-
0.133

Table 9. We compare the quality of our reconstruction on several synthetic scenes for which ground truth data is available. The Chamfer ℓ1
distance, normal consistency and the F-score [37] are computed between point clouds sampled with a density of 1 point per cm2. We use
a threshold of 5 cm for the F-score. We further voxelize each mesh to compute the intersection-over-union (IoU) between the predictions
and ground truth.

19

Figure 14. We show a qualitative comparison of synthetic scene reconstructions obtained using our method and several baseline methods.
The BundleFusion reconstruction is incomplete in some regions, screened Poisson and SIREN attempt to fit noise in the depth data, while
the NeRF reconstruction suffers from noise in the density field. Our method manages to fill in gaps in geometry, while maintaining the
smoothness of classic fusion approaches.

20

Figure 15. We show a qualitative comparison of synthetic scene reconstructions obtained using our method and several baseline methods.
The BundleFusion reconstruction is incomplete in some regions, screened Poisson and SIREN attempt to fit noise in the depth data, while
the NeRF reconstruction suffers from noise in the density field. Our method manages to fill in gaps in geometry, while maintaining the
smoothness of classic fusion approaches.

21

