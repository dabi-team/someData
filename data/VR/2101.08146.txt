Validation of the Virtual Reality Neuroscience Questionnaire: 
Maximum Duration of Immersive Virtual Reality Sessions Without the 
Presence of Pertinent Adverse Symptomatology. 

Panagiotis Kourtesis1,2,3,4, Simona Collina3,4, Leonidas A.A. Doumas2, and Sarah E. 
MacPherson1,2 

1 Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, 
United Kingdom 
2 Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom 

3 Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy 
4 Interdepartmental Centre for Planning and Research "Scienza Nuova", Suor Orsola Benincasa 
University of Naples, Naples, Italy 

* Correspondence:  
Panagiotis Kourtesis  
pkourtes@exseed.ed.ac.uk 

Keywords: Virtual Reality1, VRISE2, VR Sickness3, Cybersickness4, Neuroscience5, 
Neuropsychology6, Psychology7, Motion Sickness8. 

Abstract 

There are major concerns about the suitability of immersive virtual reality (VR) systems (i.e., head-
mounted display; HMD) to be implemented in research and clinical settings, because of the presence 
of nausea, dizziness, disorientation, fatigue, and instability (i.e., VR induced symptoms and effects; 
VRISE). Research suggests that the duration of a VR session modulates the presence and intensity of 
VRISE, but there are no suggestions regarding the appropriate maximum duration of VR sessions. 
The implementation of high-end VR HMDs in conjunction with ergonomic VR software seems to 
mitigate the presence of VRISE substantially.  However, a brief tool does not currently exist to 
appraise and report both the quality of software features and VRISE intensity quantitatively.  

The Virtual Reality Neuroscience Questionnaire (VRNQ) was developed to assess the quality of VR 
software in terms of user experience, game mechanics, in-game assistance, and VRISE. Forty 
participants aged between 28 and 43 years were recruited (18 gamers and 22 non-gamers) for the 
study. They participated in 3 different VR sessions until they felt weary or discomfort and 
subsequently filled in the VRNQ.  

Our results demonstrated that VRNQ is a valid tool for assessing VR software as it has good 
convergent, discriminant, and construct validity. The maximum duration of VR sessions should be 
between 55-70 minutes when the VR software meets or exceeds the parsimonious cut-offs of the 
VRNQ and the users are familiarized with the VR system. Also. the gaming experience does not 
seem to affect how long VR sessions should last. Also, while the quality of VR software substantially 
modulates the maximum duration of VR sessions, age and education do not. Finally, deeper 

 
 
 Virtual Reality Neuroscience Questionnaire 

immersion, better quality of graphics and sound, and more helpful in-game instructions and prompts 
were found to reduce VRISE intensity.  

The VRNQ facilitates the brief assessment and reporting of the quality of VR software features 
and/or the intensity of VRISE, while its minimum and parsimonious cut-offs may appraise the 
suitability of VR software for implementation in research and clinical settings. The findings of this 
study contribute to the establishment of rigorous VR methods that are crucial for the viability of 
immersive VR as a research and clinical tool in cognitive neuroscience and neuropsychology.  

1 

Introduction 

Immersive virtual reality (VR) has emerged as a novel tool for neuroscientific and 
neuropsychological research (Bohil et al., 2011; Parsons, 2015; Parson et al., 2018). Nevertheless, 
there are concerns pertinent to implementing VR in research and clinical settings, especially 
regarding the head-mounted display (HMD) systems (Sharples et al., 2008; Bohil et al., 2011; de 
Franca & Soares, 2017; Palmisano et al., 2017). A primary concern is the presence of adverse 
physiological symptoms (i.e., nausea, dizziness, disorientation, fatigue, and postural instability), 
which are referred to as motion-sickness, cybersickness, VR sickness or VR induced symptoms and 
effects (VRISE) (Sharples et al., 2008; Bohil et al., 2011; de Franca & Soares, 2017; Palmisano et 
al., 2017).  

Longer durations in a virtual environment have been associated with a higher probability of 
experiencing VRISE, while the intensity of VRISE also appears to increase proportionally with the 
duration of the VR session (Sharples et al., 2008). However, extensive linear and angular 
accelerations provoke intense VRISE, even in a short period of time (McCauley & Sharkey, 1992; 
LaViola, 2000; Gavgani et al., 2018). VRISE may place the health and safety of the participants or 
patients at risk of experiencing adverse physiological symptoms (Parsons et al., 2018). Research has 
also shown that VRISE induce significant decreases in reaction times and overall cognitive 
performance (Nalivaiko et al., 2015; Nesbitt et al., 2017; Mittelstaedt et al., 2019), as well as 
substantially increasing body temperatures and heart rates (Nalivaiko et al., 2015), which may 
compromise physiological data acquisition. Furthermore, the presence of VRISE has been found to 
significantly augment cerebral blood flow and oxyhemoglobin concentration (Gavgani et al., 2018), 
electrical brain activity (Arafat et al., 2018), and the connectivity between stimulus-response regions 
and nausea-processing regions (Toschi et al., 2017).  Thus, VRISE appear to confound the reliability 
of neuropsychological, physiological, and neuroimaging data (Kourtesis et al., 2019). 

To our knowledge, there do not appear to be any guidelines as to the appropriate maximum duration 
of VR research and clinical sessions to evade or alleviate the presence of VRISE. Recently, our work 
has suggested that VRISE are substantially reduced or prevented by VR software that facilitates 
ergonomic navigation (e.g., physical movement) and interaction (e.g., direct-hand tracking) 
facilitated by the hardware capabilities (e.g., motion tracking) of commercial, contemporary VR 
HMDs comparable to or more advanced than the HTC Vive and/or Oculus Rift (Kourtesis et al., 
2019).However, there are other factors such as the type of display and its features that may also 
induce or reduce VRISE (Mittelstaedt et al., 2018; Kourtesis et al., 2019). Nevertheless, we note that 
adequate technological competence is required to be able to implement appropriate VR hardware 
and/or software. In an attempt to reach a methodological consensus, we have proposed minimum 
hardware and software features, which appraise the suitability of VR hardware and software (see 
Table 1; Kourtesis et al., 2019).  

2 

 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

While VRISE may occur for various reasons, they are predominantly the undesirable outcomes of 
hardware and software insufficiencies (e.g., low resolution and refresh rates of the image, a narrow 
field of view, non-ergonomic interactions, and inappropriate navigation modes) (de Franca & Soares, 
2017; Palmisano et al., 2017; Kourtesis et al., 2019). In terms of hardware, the technical 
specifications of the computer (e.g., processing power and graphics card), and VR HMD (e.g., the 
field of view, refresh rate, and resolution) suffice to appraise their suitability (Kourtesis et al., 2019). 
However, there is not a tool to quantify the software’s recommended features, as well as the intensity 
of VRISE (Kourtesis et al., 2019). Currently, the most frequently used measure of VRISE is the 
simulator sickness questionnaire (SSQ), which only considers the symptoms pertinent to simulator 
sickness (Kennedy et al., 1993). However, the SSQ does not assess software attributes (Kennedy et 
al., 1993), and there is an argument that simulator sickness symptomatology may not be identical to 
VRISE (Stanney et al., 1997). There is thus a need for a tool, which will enable researchers to assess 
both the suitability of VR software, as well as the intensity of VRISE.  

Our recent technological literature review of VR hardware and software pinpointed four domains that 
should be considered in the development or selection of VR research/clinical software (Kourtesis et 
al., 2019). The domains are user experience, game mechanics, in-game assistance, and VRISE. Each 
domain has five criteria that should be met to ensure the appropriateness of the software (see Table 
1). Also, in the same study, the meta-analysis of 44 VR neuroscientific studies revealed that most of 
the studies did not report quantitatively VR software’s quality and/or VRISE intensity (Kourtesis et 
al., 2019). In an attempt to provide a brief tool for the appraisal of VR research/clinical software 
features and VRISE intensity, we developed the virtual reality neuroscience questionnaire (VRNQ), 
which includes twenty questions that address five criteria under each domain. This study aimed to 
validate the VRNQ and provide suggestions for the duration of VR research/clinical sessions. We 
also considered the gaming experience of the participants to examine whether this may affect the 
duration of the VR sessions. Lastly, we investigated the software predictors of VRISE as measured 
by the VRNQ. 

Table 1. Domains and Criteria for VR Research/Clinical Software 

Domains 

User Experience 

Game Mechanics 

An Adequate Level of 
Immersion 

A Suitable Navigation 
System (e.g., 
Teleportation) 

In-Game 
Assistance 
Digestible Tutorials 

I

A
R
E
T
I
R
C

Pleasant VR Experience 

Availability of 
Physical Movement 

Helpful Tutorials 

High Quality Graphics 

Naturalistic 
Picking/Placing of 
Items 

Adequate Duration 
of Tutorials 

High Quality Sounds 

Naturalistic Use of 
Items 

Helpful In-game 
Instructions 

Suitable Hardware (HMD 
and Computer) 

Naturalistic 
2-Handed Interaction 

Helpful In-game 
Prompts 

Derived from Kourtesis et al. (2019) 

VRISE 

Absence or 
Insignificant Presence 
of Nausea 

Absence or 
Insignificant Presence 
of Disorientation 

Absence or 
Insignificant Presence 
of Dizziness 

Absence or 
Insignificant Presence 
of Fatigue 

Absence or 
Insignificant Presence 
of Instability 

3 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

2  Methods 

2.1  Participants 

Forty participants (21 males) aged between 28 and 43 years (M = 32.08; SD = 3.54) and an 
educational level between 12 and 16 full-time years of education (M = 14.25; SD = 1.37) were 
recruited for the study. Eighteen participants (10 males) identified themselves as gamers through self-
report and 22 as non-gamers (11 males). The gamer experience was a dichotomous variable (i.e., 
gamer or non-gamer) based on the participants’ response to a question asking whether they played 
games on a weekly basis. The participants responded to a call disseminated through mailing lists at 
the University of Edinburgh and social media. The study was approved by the Philosophy, 
Psychology and Language Sciences Research Ethics Committee of the University of Edinburgh. All 
participants provided written informed consent prior to taking part. 

2.2  Material 

2.2.1 Hardware 

       An HTC Vive HMD with two lighthouse-stations for motion tracking was used with two HTC 
Vive’s wands with 6 degrees of freedom (DoF) to facilitate navigation and interactions within the 
environment (Kourtesis et al., 2019). The VR area where the participants were immersed and 
interacted with the virtual environments was 4.4 m2. Additionally, the HMD was connected to a 
laptop with an Intel Core i7 7700HQ processor at 2.80GHz, 16 GB RAM, a 4095MB NVIDIA 
GeForce GTX 1070 graphics card, a 931 GB TOSHIBA MQ01ABD100 (SATA) hard disk, and 
Realtek High Definition Audio.  

2.2.2 Software 

       Three VR games were selected, which included ergonomic navigation (i.e., teleportation and 
physical mobility) and interactions (i.e., 6 DoF wands simulating hand movements) with the virtual 
environment. In line with Kourtesis et al. (2019), the VR software inclusion criteria (see Table 1) 
were: 1) ergonomic interactions which simulate real-life hand movements; 2) a navigation system 
which uses teleportation and physical mobility; 3) comprehensible tutorials pertinent to the controls; 
and 4) in-game instructions and prompts which assist the user in orientating and interacting with the 
virtual environment. The suitability of the VR software for both gamers and non-gamers was also 
considered. The selected VR games which met the above software criteria were: 1) “Job Simulator” 
(Session 1) [https://store.steampowered.com/app/448280/Job_Simulator/]; 2) “The Lab” (Session 2) 
[https://store.steampowered.com/app/450390/The_Lab/]; and 3) “Rick and Morty: Virtual Rick-ality” 
(Session 3) [https://store.steampowered.com/app/469610/Rick_and_Morty_Virtual_Rickality/]. In 
“Job Simulator”, the participant becomes an employee who has several occupations, such as a cook 
(preparing simply recipes), car mechanic (doing rudimentary tasks e.g., replacing faulty parts), and 
an office worker (making calls and sending emails). In “The Lab”, the participant needs to complete 
several mini-games like slingshot (shooting down piles of boxes), longbow (shooting down 
invaders), xortex (spaceship-battles), postcards (visiting exotic places), human medical scan 
(exploring the human body), solar system (exploring the solar system), robot repair (repairing a 
robot), and secret shop (exploring a magical shop). In “Rick and Morty: Virtual Rick-ality”, the 
participant needs to complete several imaginary home-chores as in “Job Simulator”, though, in this 
case, the participant is required to follow a sequence of tasks according to a fictional storyline.  

4 

 
 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

2.2.3 Virtual Reality Neuroscience Questionnaire (VRNQ)  

      The VRNQ measures the quality of user experience, game mechanics, and in-game assistance, as 
well as the intensity of VRISE.  The VRNQ involves 20 questions where each question corresponds 
to one of the criteria for appropriate VR research/clinical software (e.g., the level of immersion; see 
Table 1). The 20 questions are grouped under four domains, where each domain encompasses five 
questions. Hence, VNRQ produces a total score corresponding to the overall quality of VR software, 
as well as four sub-scores (i.e., user experience, game mechanics, in-game assistance, VRISE). The 
user experience score is based on the intensity of the immersion, the level of enjoyment, as well as 
the quality of the graphics, sound, and VR technology (i.e., internal and external hardware). The 
game mechanics’ score depends on the ease to navigate, physically move, and interact with the 
virtual environment (i.e., use, pick & place, and hold items; two-handed interactions). The in-game 
assistance score appraises the quality of the tutorial(s), in-game instructions (e.g., description of the 
aim of the task), and prompts (e.g., arrows showing the direction). The VRISE are evaluated by the 
intensity of primary adverse symptoms and effects pertinent to VR (i.e., nausea, disorientation, 
dizziness, fatigue, and instability). VRNQ responses are indicated on a 7-point Likert style scale, 
ranging from 1= extremely low to 7 = extremely high. The higher scores indicate a more positive 
outcome; this also applies to the evaluation of VRISE intensity. Hence, the higher VRISE score 
indicates a lower intensity of VRISE (i.e., 1 = extremely intense feeling, 2 = very intense feeling, 3 = 
intense feeling, 4 = moderate feeling, 5 = mild feeling, 6 = very mild feeling, 7 = absent).  The 
VRNQ also includes space under each question, where the participant may provide optional 
qualitative feedback. For further details, please see the VRNQ in the supplementary materials. 

2.3  Procedure  

The participants individually attended 3 separate VR sessions; in each session, they were immersed 
in different VR software. The period between each session was one week for each participant (i.e., 3 
weeks in total). The participants went through an induction pertinent to the VR software for that 
session and the specific HMD and controllers used (i.e., HTC Vive and its 6DoF wands-controllers) 
before being immersed. Subsequently, the participants were asked to play the respective VR game 
until they completed it, or they felt any discomfort or fatigue. The duration of each VR session was 
recorded from the time the software was started until the participant expressed that they wanted to 
discontinue. At the end of each session, participants were asked to complete the VRNQ. The “Job 
Simulator” was always used in the 1st session, “The Lab” was always used in the 2nd session, and 
“Rick and Morty: Virtual Rick-ality” was always used in the 3rd session. 

2.4  Statistical Analyses  

A reliability analysis of the VRNQ was conducted to calculate Cronbach’s alpha and inspect whether 
the items have adequate internal consistency for research and clinical purposes. A Cronbach's alpha 
of 0.70-1.00 indicates good to excellent internal consistency (Nunally & Bernstein, 1994). A 
confirmatory factor analysis (CFA) was performed to examine the construct validity of the VRNQ in 
terms of convergent and discriminant validity (Cole, 1987). The reliability analysis and CFA were 
conducted using AMOS (version 24) (Arbuckle, 2014), and IBM Statistical Package for the Social 
Sciences (SPSS) 24.0 (IBM Corp. Released, 2016). Several tests for goodness of fit were 
implemented to allow the evaluation of VRNQ’s structure. The (CFI), Tuckere Lewis index (TLI), 
standardized root mean square residual (SRMR), and the root mean squared error of approximation 
(RMSEA) were used to assess model fit. A CFI and TLI equal to or greater than 0.90 indicate good 
structural model fit to the data (Hu & Bentler, 1999; Jackson et al., 2009; Hopwood & Donnellan, 
2010). An SRMR and RMSEA less than 0.08 postulate a good fit to the data (Hu & Bentler, 1999; 

5 

 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

Hopwood & Donnellan, 2010). Lastly, the variance of the results was assessed by dividing the χ2 by 
the degrees of freedom (df), which is an indicator of the sample distribution (Hu & Bentler, 1999; 
Jackson et al., 2009; Hopwood & Donnellan, 2010).  

The reliability and confirmatory factor analyses were conducted based on 120 observations (40 
participants * 3 sessions with different software). The a priori sample size calculator for structural 
equation models was used to calculate the minimum sample size for model structure. This calculator 
uses the error function formula, the lower bound sample size formula for a structural equation model, 
and the normal distribution cumulative distribution function (Soper, 2019a), which are in perfect 
agreement with the recommendations for statistical power analysis for the behavioral sciences 
(Cohen, 2013). A sample size of 100 observations was suggested as the minimum for conducting 
CFA to examine the model structure with statistical power equal to or greater than 0.80. Hence, the 
120 observations in our sample appear adequate to conduct a CFA with statistical power equal to or 
greater than 0.80. 

Bayesian Pearson correlation analyses were conducted to examine whether any of the demographic 
variables were significantly associated with the VRNQ total score and sub-scores, or the length of the 
VR sessions. Bayesian paired samples t-tests were performed to investigate possible differences 
between each session’s duration, as well as the VRNQ results for each VR game. Also, a Bayesian 
independent samples t-test examined whether there were any differences between gamers and non-
gamers in the duration of the session. Lastly, a Bayesian linear regression was performed to examine 
the predictors of VRISE, where the Jeffreys–Zellner–Siow (JZS) mixed g-prior was used for the 
selection of the best model. JZS has the computational advantages of a g-prior in conjunction with 
the theoretical advantages of a Cauchy prior, which are valuable in variable selection for the best 
model (Liang et al., 2008; Rouder & Morey, 2012).  For all the analyses, a Bayes Factor (BF10) ≥ 10 
was set for statistical inference, which indicates strong evidence in favor of the alternative hypothesis 
(Rouder & Morey, 2012; Wetzels & Wagenmakers, 2012; Marsman & Wagenmakers, 2017). All the 
Bayesian analyses were performed using JASP (Version 0.8.1.2) (JASP Team, 2017). The Bayesian 
Pearson correlation analyses and Bayesian linear regression analysis were conducted based on 120 
observations (40 participants * 3 different software sessions). The post-hoc statistical power 
calculator was used to calculate the observed power of the best model using Bayesian linear 
regression analysis (Soper, 2019b). 

3 

Results 

3.1  Reliability Analysis & CFA  

The reliability analysis demonstrated good to excellent Cronbach’s α for each domain of the VRNQ 
(i.e., user experience - α = 0.89, game mechanics - α = 0.89, in-game assistance - α =  0.90, VRISE - 
α = 0.89; see Table 2), which indicate very good internal reliability (Nunally & Bernstein, 1994). 
VRNQ’s fit indices are displayed in Table 2 with their respective thresholds.  The χ2/df was 1.61, 
which indicates good variance in the sample (Hu & Bentler, 1999; Jackson et al., 2009; Hopwood & 
Donnellan, 2010).  Both CFI and TLI were close to 0.95, which suggest a good fit for the VRNQ 
model (Hu & Bentler, 1999; Jackson et al., 2009; Hopwood & Donnellan, 2010). Comparably, 
SPMR and RMSEA values were between 0.06 and 0.08, which also support a good fit (Hu & 
Bentler, 1999; Jackson et al., 2009; Hopwood & Donnellan, 2010). The VRNQ’s path diagram is 
displayed in Figure 1, where from left to right are depicted the correlations among the 
factors/domains of the VRNQ, the correlations between each factor/domain and its items, and the 
error terms for each item. The VRNQ items/questions are efficiently associated with their respective 

6 

 
 
 
 
factor/domain, which shows good convergent validity (Cole, 1987).  Furthermore, there was not any 
significant correlation amongst the factors/domains, which indicates good discriminant validity 
(Cole, 1987).  

  Virtual Reality Neuroscience Questionnaire 

Table 2. Internal Reliability and Goodness of Fit for the VRNQ 

Statistics 

Thresholds 

Results 

Cronbach’s α 

≥ 0.70 

GM – 0.888 

USER – 0.886 

χ 2/df 

Comparative Fit Index (CFI) 

Tuckere Lewis Index (TLI) 

≤ 2.00 

≥ 0.90 

≥ 0.90 

Standardised root mean square residual (SRMR) 

< 0.08 

Root mean square error of approximation (RMSEA) 

≤ 0.08 

GA – 0.895 

VR – 0.891 

1.610 

0.954 

0.938 

0.076 

0.071 

VRNQ Domains: USER = user experience; GM = game mechanics; GA = in-game assistance; VR = VRISE 

7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. CFA: Model’s Path Diagram 

 Virtual Reality Neuroscience Questionnaire 

8 

 From left to right: the structural model illustrates the associations between VRNQ domains (paths with double headed arrows) and between each VRNQ domain and its items. At the right there are the error terms (e) for each item; USER = user experience; GM = game mechanics; GA = in-game assistance; VR = VRISE  
 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

3.2  Descriptive Statistics of Sessions’ Duration and VRNQ Scores 

The descriptive statistics for the sessions’ durations and the VRNQ scores are displayed in Table 3. 
In session 1, the participants were immersed for 59.65 (8.42) minutes. In session 1, the average time 
of gamers seems more than the average time of non-gamers (Table 3). In session 2, the participants 
spent 64.72 (6.24) minutes (Table 3).  In session 3, gamers spent 70.44 (7.78) minutes, while non-
gamers spent 65.73 (6.75) minutes (Table 3). The average total score of the VRNQ for all software 
was 126.30 (7.55) (maximum score is 140), where gamers and non-gamers scores did not appear to 
differ. Similarly, the median scores for each domain were 30-32 out of 35, where again gamers and 
non-gamers scores did not appear to differ. Importantly, all the VRISE scores (per item) for both 
gamers and non-gamers were equal to 5 (i.e., mild feeling), or 6 (i.e., very mild feeling), or 7 (absent 
feeling). The vast majority of scores were equal to 6 (i.e., very mild feeling) or 7 (absent feeling) (see 
Figure 2).       

3.3  Minimum and Parsimonious Cut-off Scores of VRNQ 

Cut-off scores were calculated for the VRNQ total score and sub-scores to inspect the suitability of 
the assessed VR software (see Table 4). In the VRNQ, the ordinal 1-3 responses are paired with 
negative qualities, response 4 is paired with neutral/moderate qualities, and 5-7 responses are paired 
with positive qualities (see Supplementary Material). The minimum cut-offs suggest that if the 
median of the responses is 25 for every sub-score, and 100 in the total score (i.e., at least a median of 
5 for every item), then the VRNQ outcomes indicate that the evaluated VR software is of an adequate 
quality not to cause any significant VRISE. Furthermore, the parsimonious cut-offs suggest that, if 
the median of the responses is 30 for every sub-score, and 120 for the total score (i.e., at least a 
median of 6 for every item) then the utilization of the parsimonious cut-offs more robustly supports 
the suitability of the VR software. The minimum and parsimonious cut-offs hence appear adequate to 
guarantee the safety, pleasantness, and appropriateness of the VR software for research and/or 
clinical purposes. 

9 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

Table 3. Descriptive Statistics: Duration of VR Sessions and VRNQ Scores 

Group 

N 

Mean (SD) 

SE 

Total Duration 

Duration of Session 1 

Duration of Session 2 

Duration of Session 3 

VRNQ Total Score 
Out of 140 
(Across 3 Sessions) 

User’s Experience (Across 3 Sessions) 
Out of 35 

Game Mechanics (Across 3 Sessions) 
Out of 35 

In-Game Assistance 
(Across 3 Sessions) 
Out of 35 

VRISE 
(Across 3 Sessions) 
Out of 35 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

18  
22 

40 

18 
22 

40 

18 
22 

40 

18 
22 

40 

18  
22  

40  

18  
22  

40  

18  
22  

40  

18  
22  

40  

18  
22  

40  

199.39 (13.63) 
186.36 (11.76) 

192.2 (14.09) 

65.61 (7.14) 
54.77 (5.91) 

59.65 (8.42) 

63.33 (6.16) 
65.86 (6.21) 

64.72 (6.24) 

70.44 (7.78) 
65.73 (6.75) 

67.85 (7.52) 

127.2 (7.32)  
125.6 (7.71) 

126.3 (7.55) 

31.37 (2.73)  
30.91 (2.73) 

31.12 (2.73) 

31.50 (2.68)  
31.32 (2.61) 

31.40 (2.63) 

31.70 (2.59) 
31.65 (2.52)  

31.68 (2.54) 

32.67 (2.17)  
31.71 (2.56)  

32.14 (2.43) 

3.21 
2.51 

2.23 

1.68 
1.26 

1.33 

1.45 
1.32 

0.99 

1.83 
1.44 

0.69 

0.99 
0.95 

0.69 

0.34 
0.37 

0.25 

0.37 
0.32 

0.24 

0.35 
0.31 

0.23 

0.30 
0.32 

0.22 

10 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. VRISE Intensity in VR Sessions as Measured by VRNQ 

  Virtual Reality Neuroscience Questionnaire 

7

6

5

4

Nausea

Disorientation

Dizziness

Fatigue

Instability

1st Session

All Sessions

2nd Session

3rd Session

Minimum Cut-off

Parsimonious Cut-off

 Median scores of VRISE items of VRNQ; VRNQ Minimum Cut-off ( ≥ ); VRNQ Parsimonious Cut-off ( ≥ );                                                         
1 = Extreme intense feeling; 2 = Very intense feeling; 3 = Intense feeling; 4 = Moderate feeling;                                                       
5 = Mild feeling; 6 = Very mild feeling; 7 = Absent feeling 

Table 4. VRNQ Cut-offs 

Score 

Minimum Cut-offs 

Parsimonious Cut-offs 

User Experience 

Game Mechanics 

In-Game Assistance 

VRISE 

≥ 25/35 

≥ 25/35 

≥ 25/35 

≥ 25/35 

≥ 30/35 

≥ 30/35 

≥ 30/35 

≥ 30/35 

VRNQ Total Score 

≥ 100/140 

≥ 120/140 

The median of each sub-score and totals scores should meet the suggested cut-offs to support that the 
evaluated VR software has an adequate quality without any significant VRISE.  The utilisation of the 
parsimonious cut-offs more robustly supports the suitability of the VR software. 

11 

 
 
 
 
     
 Virtual Reality Neuroscience Questionnaire 

3.4  Bayesian T-Tests  

The Bayesian independent samples t-test between gamers and non-gamers indicated that the former 
spent significantly more time in VR across the total duration for the 3 sessions (BF10 = 14.99), as 
well as the duration of the 1st session (BF10 = 2,532; see Table 4) (Wetzels & Wagenmakers, 2012; 
Marsman & Wagenmakers, 2017). The difference is much smaller in the total duration than the 
difference in the 1st session. Thus, the difference between the gamers and non-gamers in the total 
duration appears to be driven by the substantial difference in the1st session’s duration (see Table 5).  
Conversely, the Bayesian paired samples t-test (i.e., differences between the VR games) indicated 
significant differences in the total score and every sub-score of VRNQ (see Table 6) between the VR 
software. The VR software in the 3rd session was evaluated higher than the VR software in the 1st 
and 2nd sessions, while the VR software in the 2nd session was rated better than the VR software in 
the 1st session.  There was also an important difference between the duration of the 3rd session 
(longer) and the duration of the 1st session (shorter; BF10 = 103,568), while there was not a 
substantial difference between the duration of the 2nd and 3rd sessions (BF10 = 2.78), as well as 
between the duration of 1st and 2nd sessions (BF10 = 7.05; see Table 6) (Wetzels & Wagenmakers, 
2012; Marsman & Wagenmakers, 2017). 

Table 5. Bayesian Independent Samples T-Test: Gamers against Non-Gamers 

Significance 

* 

*** 

Variables 

Age 

Education 

Total Duration 

Session 1 Duration 

Session 2 Duration 

Session 3 Duration 

VRNQ Total 

User’s Experience 

Game Mechanics 

In-Game Assistance 

VRISE 

BF₁₀ 

0.323 

0.325 

14.987 

2531.886 

0.595 

1.580 

0.425 

0.359 

0.315 

0.315 

0.745 

error % 

0.006 

0.006 

7.044e -6 

7.491e -8 

0.006 

0.003 

0.007 

0.006 

0.006 

0.006 

0.003 

12 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 6. Bayesian Paired Samples T-Tests: Differences between the VR Software 

  Virtual Reality Neuroscience Questionnaire 

                                         Pairs 

Significance 

Session 2 Duration 

Session 1 Duration 

Session 3 Duration 

Session 2 Duration 

Session 3 Duration 

Session 1 Duration 

S3 VRNQ Total 

S3 VRNQ Total 

S2 VRNQ Total 

S3 VRISE 

S3 VRISE 

S2 VRISE 

S2 VRNQ Total 

S1 VRNQ Total 

S1 VRNQ Total 

S2 VRISE 

S1 VRISE 

S1 VRISE 

S3 In-Game Assistance 

S2 In-Game Assistance 

S2 In-Game Assistance 

S1 In-Game Assistance 

S3 In-Game Assistance 

S1 In-Game Assistance 

S3 Game Mechanics 

S2 Game Mechanics 

S2 Game Mechanics 

S1 Game Mechanics 

S3 Game Mechanics 

S1 Game Mechanics 

S3 User’s Experience 

S2 User’s Experience 

S3 In-Game Assistance 

S1 User’s Experience 

S2 User’s Experience 

S1 User’s Experience 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

BF₁₀ 

7.049  

2.783  

error % 

~ 0.001  

~ 3.276e -4  

103568.858  

6.942e +12  

3.520e +20  

8.500e +17  

22075.036  

1.322e +10  

1.160e +7  

207216.904  

1.197e +7  

8.028e +10  

274310.417  

4.883e +14  

2.876e +14  

2.873e +7  

2.597e +7  

1.708e +6  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

BF₁₀ = Bayes Factor; * BF₁₀ > 10, ** BF₁₀ > 30, *** BF₁₀ > 100; S1 = Session 1; S2 = Session 2; 

 S3 = Session 3.  

3.5  Bayesian Pearson Correlation Analyses & Regression Analysis 

The Bayesian Pearson correlation analyses did not show any significant correlation between age and 
any of the VRNQ scores, between age and duration of the sessions, between education and any of the 
VRNQ scores, or between education and duration of the sessions. However, the duration of the 
session was positively correlated with the total VRNQ score (BF10 = 81.54; r(120) = 0.310, p < .001). 
Furthermore, the VRISE score substantially correlated with the following VRNQ items: immersion, 
pleasantness, graphics, sound, pick & place, tutorial’s difficulty, tutorial’s usefulness, tutorial’s 
duration, instructions, and prompts (see Table 7). In contrast, VRISE did not significantly correlate 
with the following VRNQ items: VR tech, navigation, physical movement, use items, or two-handed 
interactions (see Table 7). Moreover, the Bayesian regression analysis indicated the five best models 
that predicted the VRNQ’s VRISE score (see Table 8). The best model includes the following items 
from the VRNQ: immersion, graphics, sound, instructions, and prompts. All the predictors exceeded 
the prior inclusion probabilities (see Figure 3). The best model showed a BFM = 117.42, whereas the 
second-best model displayed a BFM = 56.40 (see Table 8); hence, the difference between the best 
model compared to the second-best model was robust (Rouder & Morey,2012; Wetzels & 
Wagenmakers, 2012; Marsman & Wagenmakers, 2017). Also, the best model has an R² = 0.324 (see 
Table 8), which postulates that the model explains the 32.4% of the variance of VRISE score (Rouder 
& Morey,2012; Wetzels & Wagenmakers, 2012). Lastly, the post-hoc statistical power analysis for 

13 

 
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

the best model indicated an observed statistical power of 0.998, p <.001, which postulates a high 
efficiency, precision, reproducibility, and reliability of the regression analysis and results (Button et 
al., 2013; Cohen, 2013). 

Table 7. Bayesian Pearson Correlations Analyses: VRISE Score with VRNQ Items 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

Pairs 

Significance 

BF₁₀ 

Immersion 

Pleasantness 

Graphics 

Sound 

VR Tech 

Navigation 

Physical Movement 

Pick & Place 

Use Items 

Two-Handed Interaction 

Tutorial Difficulty 

Tutorials Usefulness 

Tutorials’ Duration 

Instructions 

Prompts 

*** 
* 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

1226.538 

20.504 

1629.195 

18586.578 

5.094 

4.808 

2.229 

175.087 

0.405 

0.506 

28252.587 

161.949 

128.539 

952.871 

706510.726 

r 

0.371 

0.273 

0.377 

0.421 

0.228 

0.226 

0.197 

0.329 

0.109 

0.123 

0.428 

0.327 

0.322 

0.366 

0.476 

BF₁₀ = Bayes Factor; * BF₁₀ > 10, ** BF₁₀ > 30, *** BF₁₀ > 100; 

14 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

Table 8. Models’ Comparison: Predictors of VRISE score 

Models 

P(M)  P(M|data) 

BFM 

BF10 

R² 

Prompts + Sound + Graphics + Immersion + Instructions 

0.004 

0.304 

Prompts + Graphics + Immersion + Instructions + Pleasantness 

0.004 

0.173 

Prompts + Sound + Graphics + Immersion + Instructions + Pick & 
Place 

0.004 

0.161 

117.42 
*** 

56.47  
** 

43.15 
 * 

1.000 

0.324 

0.571 

0.317 

0.443 

0.330 

Prompts + Sound + Graphics + Immersion + Instructions + Pick 
&Place + Tutorials Usefulness + Pleasantness 

0.021 

0.123 

6.62 

0.072 

0.337 

Prompts + Graphics + Immersion + Instructions + Pick & Place + 
Tutorials Usefulness + Pleasantness 

0.008 

0.077 

10.72  
* 

0.121 

0.329 

P = Probability; M = Model; BFM = Model’s Bayesian Factor; * BFM >10, ** BFM >30, *** BFM >100;                
BF₁₀ = BF against null model 

15 

Figure 3. Variables’ Prior Inclusion Probabilities           
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

4 

Discussion 

4.1  The VRNQ as a Research and Clinical Tool  

The VRNQ is a short questionnaire (5-10 minutes administration time) which assesses the quality of 
VR software in terms of user experience, game mechanics, in-game assistance, and VRISE. The 
values of the fit indices of CFA (i.e., CFI, TLI, SPMR and RMSEA) indicated that the VRNQ’s 
structure was a good fit to the data, which postulates good construct validity for the VRNQ (Hu & 
Bentler, 1999; Jackson et al., 2009; Hopwood & Donnellan, 2010). In addition, the construct validity 
of the VRNQ was supported by its convergent and discriminant validity (Cole, 1987). VRNQ items 
were strongly correlated with their grouping factor, which indicates robust convergent validity, while 
there were substantially poor correlations between the factors, which postulates very good 
discriminant validity (Cole, 1987). Furthermore, the Cronbach’s α for each VRNQ domain (i.e., user 
experience - α = 0.89, game mechanics - α = 0.89, in-game assistance - α = 0.90, VRISE - α = 0.89; 
see Table 2) suggest very good construct validity (Nunally & Bernstein, 1994). Henceforth, the 
VRNQ emerges as a valid and suitable tool to evaluate the quality of the VR research/clinical 
software as well as the intensity of the adverse VRISE.  

Furthermore, minimum and parsimonious cut-off scores were calculated for the VRNQ total score 
and sub-scores to inspect the suitability of the assessed VR software. The minimum cut-offs indicate 
the lowest acceptable quality that VR research/clinical software should be, while the parsimonious 
cut-offs are offered for more robust support of the VR software’s suitability, which may be required 
in experimental and clinical designs with more conservative standards. However, the individual 
scores from the VRNQ may be modulated by individual differences and preferences unrelated to the 
quality of the software (Kortum & Peres, 2014). In addition, the VRNQ produces ordinal data; 
therefore, the median is the appropriate measure for their analysis (Harpe, 2015). Hence, the median 
VRNQ scores for the whole sample should be used to assess the VR software’s quality effectively. 
Also, the medians of the VRNQ total score and sub-scores allow the generalisation of the results and 
comparison between different VR software (Kortum & Peres, 2014; Harpe, 2015).  Researchers, 
clinicians, and/or research software developers should use the medians of the VRNQ total score and 
sub-scores to assess whether the implemented VR software exceed the minimum or parsimonious 
cut-offs.  Hence, if the medians of the VRNQ sub-scores and totals score for VR research software 
meet the minimum cut-offs, then these results support the VR software’s suitability. Likewise, if the 
medians of VRNQ sub-scores and totals score for VR research software meet the parsimonious cut-
offs, then these results provide even stronger support for its suitability. However, median scores 
below these cut-offs suggest that the suitability of the VR software is questionable, but they do not 
indicate that this VR software is certainly unsuitable.   

 Also, VRNQ appears as an appropriate tool to measure both VRISE and VR software features 
compared to other questionnaires. The SSQ is the most implemented questionnaire in VR studies. 
However, the SSQ only considers the symptoms pertinent to simulator sickness and it does not assess 
software attributes (Kennedy et al., 1993), while there is a dispute that simulator sickness 
symptomatology may not be the same as VRISE (Stanney et al., 1997). Alternatively, Virtual reality 
sickness questionnaire (VRSQ) was recently developed (Kim et al., 2018). The development of 
VRSQ was based on the SSQ, where the researchers attempted to isolate the items which are 
pertinent to VRISE (Kim et al., 2018).  However, their sample size was relatively small (i.e., 24 
participants * 4 sessions = 96 observations) (Kim et al., 2018).  Notably, the factor analyses of Kim 
et al., (2018) accepted only items pertinent to oculomotor and disorientation components of SSQ, and 
rejected all the items pertinent to nausea (i.e.,7 items) (Kim et al., 2018), while nausea is the most 

16 

 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

frequent symptom in VRISE (Stanney et al., 1997; Sharples et al., 2008; Bohil et al., 2011; de Franca 
& Soares, 2017; Palmisano et al., 2017). Also, comparable to SSQ, VRSQ does not consider 
software features. Hence, the VRNQ appears to be the only valid and suitable tool to evaluate both 
the intensity of predominant VRISE and the quality of VR software features.  

The VRNQ allows researchers to report the quality of VR software and/or the intensity of VRISE in 
their VR studies. However, an in-depth assessment of the numerous software features requires a 
questionnaire with more than the 20 questions of the VRNQ (Zarour et al., 2015). For an in-depth 
software analysis, questionnaires with more questions pertinent to the whole spectrum of software 
features should be preferred (Zarour et al., 2015).  Additionally, the VRNQ has solely five items 
pertinent to VRISE. Hence, it does not offer an exhaustive assessment of VRISE. Studies that aim to 
investigate VRISE in depth should opt for a tool which contains more items pertinent to VRISE than 
VRNQ (e.g., SSQ). The VNRQ is a brief questionnaire (5-10 minutes administration time) including 
20 items, which enables researchers, clinicians, and research software developers to evaluate and 
report the quality of the VR software and the intensity of VRISE for research and clinical purposes. 

4.2  Maximum Duration of VR Sessions 

The duration of the VR session is a crucial factor in research and/or clinical design. In our sample, 
the participants discontinued the VR session due to loss of interest, while none discontinued due to 
VRISE. In the 1st session, gamers spent significantly more time immersed than the non-gamers; a 
difference which modulated the difference between the two groups in the summed duration across all 
sessions. However, it is worth noting that there was not a significant difference between the two 
groups in the time spent in VR for the 2nd and 3rd sessions. The observed difference in the 1st 
session and the absence of a difference in the later sessions’ durations postulates that when users are 
familiarized with the VR technology, while the influence of their gaming experience on the session’s 
duration becomes insignificant.  In support of this, a recent study showed that user gaming 
experience does not affect the perceived workload of the users in VR (Lum et al., 2018). Hence, the 
level of familiarization of the participants with the VR technology appears to affect substantially the 
duration of the VR session. 

Nevertheless, in the whole sample, irrespective of participants’ gaming experience, the durations of 
the 2nd and 3rd sessions are sufficiently longer than the duration of the 1st session. The duration of 
the 3rd session is not significantly longer than the duration of the 2nd session. Furthermore, given 
that in each session, a different VR software was administered, the VRNQ correspondingly 
pinpointed significant differences amongst the implemented VR software’ quality. All the VRNQ 
scores for the 3rd session’s VR software are greater than the 2nd session’s VR software scores. 
Similarly, all the VRNQ scores for the 2nd session’s VR software are greater than the 1st session’s 
VR software scores. Also, the duration of VR session was positively correlated with the total score of 
VRNQ. Thus, the quality of the VR software as measured by the VRNQ seems to be significantly 
associated with the duration of the VR session.  

Overall, in every session, the intensity of VRISE was reported as very mild to absent by the vast 
majority of the sample. However, comparable to the rest of the VRNQ scores, the VRISE score for 
the 3rd VR session was significantly higher (i.e., milder feeling) than the 2nd and 3rd sessions. 
Similarly, the VRISE score for the 2nd session’s VR software was substantially higher than the 1st 
session’s VR software score. Notably, there was not any difference between gamers and non-gamers 
in the VRNQ scores across the three sessions. Equally, the age and education of participants did not 
correlate with any of the VRNQ scores or the duration of sessions. Thus, the age, education, and 

17 

 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

gaming experience of the participants did not affect the responses in the VRNQ. Therefore, the 
observed differences in the VRISE scores between the VR sessions support that the quality of the VR 
software as measured by the VRNQ and the level of familiarization of the participants with the VR 
technology also affect the intensity of VRISE.  

The findings postulate that the implementation of VR software with a maximum duration between 
55-70 minutes is substantially feasible. However, long exposures in VR have been found to increase 
the probability of experiencing VRISE and the intensity of VRISE (Sharples et al., 2008). In our 
sample, especially in the 3rd session, which was substantially longer than the other sessions, the 
intensity of VRISE was significantly lower than the rest of the sessions. As discussed above, the 
substantially lower intensity of VRISE in the 3rd session appears to be a result of increased VR 
familiarity, and the better quality of the implemented VR software as measured by the VRNQ. 
Hence, researchers and/or clinicians should consider the quality of their VR software to define the 
appropriate duration of their VR session. In research and clinical designs where the duration of the 
VR session is required to be between 55-70 minutes, the researchers and/or clinicians should opt for 
the parsimonious cut-offs of the VRNQ to ensure adequate quality of their VR software to facilitate 
longer sessions without significant VRISE. Additionally, an extended introductory tutorial which 
allows participants to familiarize themselves with the VR technology and mechanics would assist 
with the implementation of longer (i.e., 55-70 minutes) VR sessions, where the presence and 
intensity of VRISE would not be significant. 

4.3  The Quality of VR Software and VRISE 

The VRISE score substantially correlated with almost every item under the section of user experience 
and in-game assistance (see Table 6). However, the VRISE score did not correlate with VR tech (the 
item under the user experience’s domain) or most of the items under the section of game mechanics. 
The quality of VR hardware (i.e., the HMD and its controllers) and interactions (i.e., ergonomic or 
non-ergonomic) with the virtual environment are crucial for the alleviation or evasion of VRISE 
(Kourtesis et al., 2019). Nevertheless, in this sample, the VR tech item (i.e., the quality of the internal 
and external VR hardware) was not expected to correlate with the VRISE score, because the HMD 
and its 6DoF controllers were the same for all 3 VR software versions and sessions. Hence, the 
variance in the responses to this item was limited. Also, the three VR software games share common 
game mechanics, especially the same navigation system (i.e., teleportation) and a similar amount of 
physical mobility. Likewise, apart from some controls (i.e., the button to grab items), the interaction 
systems of the implemented VR software were very proximal. Therefore, the absence of a correlation 
between VRISE scores and most of the items in the game mechanics’ section was also an expected 
outcome. Nonetheless, the VRISE score was strongly associated with the level of immersion and 
enjoyment, the quality of graphics and sound, the comfort to pick and place 3D objects, and the 
usefulness of in-game assistance modes (i.e., tutorials, instructions, and prompts).  

The items which correlated with the VRISE score were also included in the best models of predicting 
its value (see Table 7). Importantly, the best model includes as predictors of VRISE, the level of 
immersion, the quality of graphics and sound, and the helpfulness of in-game instructions and 
prompts (see Table 7). The higher scores for prompts and instructions indicate that the user was 
substantially assisted by the in-game assistance (e.g., an arrow showing the direction that the user 
should follow) to orientate and guide his or herself from one point of interest to the next in 
accordance with the scenario of the VR experience. This may be interpreted as ease to orient and 
interact with the virtual environment, as well as a significant decrease in confusion (Brade et al., 
2018). The quality of the in-game assistance methods is essential for the usability and enjoyment that 

18 

 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

VR software offers (Brade et al., 2018). Equally, the quality of the graphics is predominantly 
dependent upon rendering which encompasses the in-game quality of the image known as perceptual 
quality, and the exclusion of redundant visual information known as occlusion culling (Lavoué & 
Mantiuk, 2015). The improvement of these two factors not only results in improved quality of the 
graphics but also in improved performance of the software (Brennesholtz, 2018). Furthermore, the 
spatialized sound of VR software, which assists the user to orient his or herself (Ferrand et al., 2017), 
deepens the experienced immersion (Riecke et al., 2011), and enriches the geometry of the virtual 
space without affecting the performance of the software (Kobayashi et al., 2015).  Lastly, the level of 
immersion appears to be negatively correlated with the frequency and intensity of VRISE (Milleville-
Pennel & Charron, 2015; Weech et al., 2019). The best model hence aligns with the relevant 
literature and provides further evidence in support of the utility of the VRNQ as a valid and efficient 
tool to appraise the quality of the VR software and intensity of VRISE. 

4.4  Limitations and Future Studies 

This study also has some limitations. In this study, construct validity for the VRNQ is provided. 
However, future work should endeavor to provide convergent validation of the VRNQ with tools that 
measure VRISE symptomatology (e.g., SSQ) and/or VR software attributes. Moreover, the sample 
size was relatively small, but it offered an adequate statistical power for the conducted analyses. 
Also, the VRNQ does not directly quantify linear or angular accelerations, which may induce intense 
VRISE in a relatively short period of time (McCauley & Sharkey, 1992; LaViola, 2000; Gavgani et 
al., 2018). However, the VRNQ quantifies the effect(s) of linear and angular accelerations (i.e., 
VRISE), where VR software with a highly provocative content (e.g., linear and angular accelerations) 
would fail to meet or exceed the VRNQ cut-offs for the VRISE domain. Furthermore, the study 
utilized only one type of VR hardware, which did not allow us to inspect the effect of VR HMD’s 
quality on VRISE presence and intensity. Similarly, our VR software did not allow us to compare 
different ergonomic interactions or levels of provocative potency pertaining to VRISE. Future studies 
with a larger sample, various types of VR hardware, and VR software with substantially more diverse 
features will offer further insights on the impact of software features on VRISE intensity, as well as 
provide additional support for the VRNQ’s structural model. Lastly, neuroimaging (e.g., 
electroencephalography) and physiological data (e.g., heart rates) may correlate, classify, and predict 
VRISE symptomatology (Kim et al., 2005; Dennison et al., 2016; Dennison et al., 2019). Hence, 
future studies should consider collecting neuroimaging and/or physiological data that could further 
elucidate the relationship between VRNQ’s VRISE score(s) and brain region activation or 
cardiovascular responses (e.g., heart rate). 

4.5  Conclusion 

This study showed that the VRNQ is a valid and reliable tool which assesses the quality of VR 
software and intensity of VRISE. Our findings support the viability of VR sessions with a duration 
up to 70 minutes, when the participants are familiarized with VR tech through an induction session, 
and the quality of the VR software meets the parsimonious cut-offs of VRNQ.  Also, our results 
offered insights on the software-related predictors of VRISE intensity, such as the level of 
immersion, the quality of graphics and sound, and the helpfulness of in-game instructions and 
prompts. Finally, the VRNQ enables researchers to quantitatively assess and report the quality of VR 
software features and intensity of VRISE, which are vital for the efficacious implementation of 
immersive VR systems in cognitive neuroscience and neuropsychology. The minimum and 
parsimonious cut-offs of VRNQ may appraise the suitability of VR software for implementation in 
research and clinical settings. The VRNQ and the findings of this study contribute to the endeavor of 

19 

 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

establishing thorough VR research and clinical methods that are crucial to guarantee the viability of 
implementing immersive VR systems in cognitive neuroscience and neuropsychology. 

5 

Conflict of Interest 

The authors declare that the research was conducted in the absence of any commercial or financial 
relationships that could be construed as a potential conflict of interest. 

6 

Author Contributions 

The primary author had the initial idea and contributed to every aspect of this study. The rest of the 
authors contributed to the methodological aspects and the discussion of the results. The VRNQ may 
be downloaded from supplementary material. 

7 

References 

Arafat, I. M., Ferdous, S. M. S., & Quarles, J. (2018, March). Cybersickness-Provoking Virtual 
Reality Alters Brain Signals of Persons with Multiple Sclerosis. In 2018 IEEE Conference on 
Virtual Reality and 3D User Interfaces (VR) (pp. 1-120). IEEE. 

Arbuckle, J. L. (2014). Amos (Version 23.0) [Computer Program]. Chicago: IBM SPSS. 

Bohil, C. J., Alicea, B., & Biocca, F. A. (2011). Virtual reality in neuroscience research and therapy. 

Nature reviews neuroscience, 12(12), 752. 

Brade, J., Dudczig, M., & Klimant, P. (2018, September). Using Virtual Prototyping Technologies to 

Evaluate Human-Machine-Interaction Concepts. In aw&I Conference (Vol. 3). 

Brennesholtz, M. S. (2018, May). 3‐1: Invited Paper: VR Standards and Guidelines. In SID 

Symposium Digest of Technical Papers (Vol. 49, No. 1, pp. 1-4). 

Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munafò, M. R. 

(2013). Confidence and precision increase with high statistical power. Nature Reviews 
Neuroscience, 14(8), 585. 

Cohen, J. (2013). Statistical power analysis for the behavioral sciences. Routledge. 

Cole, D. A. (1987). Utility of confirmatory factor analysis in test validation research. Journal of 

consulting and clinical psychology, 55(4), 584. 

de França, A. C. P., & Soares, M. M. (2017, July). Review of Virtual Reality Technology: An 

Ergonomic Approach and Current Challenges. In International Conference on Applied Human 
Factors and Ergonomics (pp. 52-61). Springer, Cham.  

Dennison Jr, M., D'Zmura, M., Harrison, A., Lee, M., & Raglin, A. (2019, May). Improving motion 
sickness severity classification through multi-modal data fusion. In Artificial Intelligence and 
Machine Learning for Multi-Domain Operations Applications (Vol. 11006, p. 110060T). 
International Society for Optics and Photonics. 

Dennison, M. S., Wisti, A. Z., & D’Zmura, M. (2016). Use of physiological signals to predict 

cybersickness. Displays, 44, 42-52. 

20 

 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

Ferrand, S., Alouges, F., & Aussal, M. (2017, May). Binaural Spatialization Methods for Indoor 

Navigation. In Audio Engineering Society Convention 142. Audio Engineering Society. 

Gavgani, A. M., Wong, R. H., Howe, P. R., Hodgson, D. M., Walker, F. R., & Nalivaiko, E. (2018). 
Cybersickness-related changes in brain hemodynamics: A pilot study comparing transcranial 
Doppler and near-infrared spectroscopy assessments during a virtual ride on a roller coaster. 
Physiology & behavior, 191, 56-64. 

Harpe, S. E. (2015). How to analyze Likert and other rating scale data. Currents in Pharmacy 

Teaching and Learning, 7(6), 836-850. 

Hopwood, C. J., & Donnellan, M. B. (2010). How should the internal structure of personality 

inventories be evaluated? Personality and Social Psychology Review, 14, 332–346. 

Hu, L. T., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: 

Conventional criteria versus new alternatives. Structural Equation Modeling, 6, 1–55.  

IBM Corp. Released (2016). IBM SPSS Statistics for Windows, Version 24.0. Armonk, NY: IBM 

Corp. 

Jackson, D. L., Gillaspy Jr, J. A., & Purc-Stephenson, R. (2009). Reporting practices in confirmatory 

factor analysis: An overview and some recommendations. Psychological methods, 14(1), 6. 

JASP Team (2017). JASP (Version 0.8.1.2) [Computer software]. 

Kennedy, R. S., Lane, N. E., Berbaum, K. S., & Lilienthal, M. G. (1993). Simulator sickness 

questionnaire: An enhanced method for quantifying simulator sickness. The international journal 
of aviation psychology, 3(3), 203-220. 

Kim, H. K., Park, J., Choi, Y., & Choe, M. (2018). Virtual reality sickness questionnaire (VRSQ): 

Motion sickness measurement index in a virtual reality environment. Applied ergonomics, 69, 66-
73. 

Kim, Y. Y., Kim, H. J., Kim, E. N., Ko, H. D., & Kim, H. T. (2005). Characteristic changes in the 

physiological components of cybersickness. Psychophysiology, 42(5), 616-625. 

Kobayashi, M., Ueno, K., & Ise, S. (2015). The effects of spatialized sounds on the sense of presence 

in auditory virtual environments: a psychological and physiological study. Presence: 
Teleoperators and Virtual Environments, 24(2), 163-174. 

Kortum, P., & Peres, S. C. (2014). The relationship between system effectiveness and subjective 
usability scores using the System Usability Scale. International Journal of Human-Computer 
Interaction, 30(7), 575-584. 

Kourtesis, P., Collina, S., Doumas, L. A. A., & MacPherson, S. E. (2019). Technological competence 
is a precondition for effective implementation of virtual reality head mounted displays in human 
neuroscience: a technological review and meta-analysis. Frontiers in Human Neuroscience, 13, 
342.  

21 

 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

LaViola Jr, J. J. (2000). A discussion of cybersickness in virtual environments. ACM Sigchi Bulletin, 
32(1), 47-56. 

Lavoué, G., & Mantiuk, R. (2015). Quality assessment in computer graphics. In Visual Signal 

Quality Assessment (pp. 243-286). Springer, Cham  

Liang, F., Paulo, R., Molina, G., Clyde, M. A., & Berger, J. O. (2008). Mixtures of g priors for 

Bayesian variable selection. Journal of the American Statistical Association, 103(481), 410-423. 

Lum, H. C., Greatbatch, R., Waldfogle, G., & Benedict, J. (2018, September). How Immersion, 
Presence, Emotion, & Workload Differ in Virtual Reality and Traditional Game Mediums. In 
Proceedings of the Human Factors and Ergonomics Society Annual Meeting (Vol. 62, No. 1, pp. 
1474-1478). Sage CA: Los Angeles, CA: SAGE Publications. 

Marsman, M., & Wagenmakers, E. J. (2017). Bayesian benefits with JASP. European Journal of 

Developmental Psychology, 14(5), 545-555.  

McCauley, M. E., & Sharkey, T. J. (1992). Cybersickness: Perception of self-motion in virtual 
environments. Presence: Teleoperators & Virtual Environments, 1(3), 311-318. 

Milleville-Pennel, I., & Charron, C. (2015). Do mental workload and presence experienced when 
driving a real car predispose drivers to simulator sickness? An exploratory study. Accident 
Analysis & Prevention, 74, 192-202. 

Mittelstaedt, J., Wacker, J., & Stelling, D. (2018). Effects of display type and motion control on 

cybersickness in a virtual bike simulator. Displays, 51, 43-50. 

Mittelstaedt, J. M., Wacker, J., & Stelling, D. (2019). VR aftereffect and the relation of cybersickness 

and cognitive performance. Virtual Reality, 1-12. 

Nalivaiko, E., Davis, S. L., Blackmore, K. L., Vakulin, A., & Nesbitt, K. V. (2015). Cybersickness 
provoked by head-mounted display affects cutaneous vascular tone, heart rate and reaction time. 
Physiology & behavior, 151, 583-590. 

Nesbitt, K., Davis, S., Blackmore, K., & Nalivaiko, E. (2017). Correlating reaction time and nausea 

measures with traditional measures of cybersickness. Displays, 48, 1-8 

Nunally, J. C., & Bernstein, I. H. (1994). Psychometric theory, 3rd. New Yokr: Mcgraw-Hill. 

 Palmisano, S., Mursic, R., & Kim, J. (2017). Vection and cybersickness generated by head-and-

display motion in the Oculus Rift. Displays, 46, 1-8. 

 Parsons, T. D. (2015). Virtual reality for enhanced ecological validity and experimental control in 

the clinical, affective and social neurosciences. Frontiers in human neuroscience, 9.  

Parsons, T. D., McMahan, T., & Kane, R. (2018). Practice parameters facilitating adoption of 

advanced technologies for enhancing neuropsychological assessment paradigms. The Clinical 
Neuropsychologist, 32(1), 16-41. 

22 

 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

Riecke, B. E., Feuereissen, D., Rieser, J. J., & McNamara, T. P. (2011, May). Spatialized sound 

enhances biomechanically-induced self-motion illusion (vection). In Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems (pp. 2799-2802). ACM. 

Rouder, J. N., & Morey, R. D. (2012). Default Bayes factors for model selection in 

regression. Multivariate Behavioral Research, 47(6), 877-903. 

Sharples, S., Cobb, S., Moody, A., & Wilson, J. R. (2008). Virtual reality induced symptoms and 
effects (VRISE): Comparison of head mounted display (HMD), desktop and projection display 
systems. Displays, 29(2), 58-69. 

Soper, D.S. (2019a). A-priori Sample Size Calculator for Structural Equation Models [Software]. 

Available   from http://www.danielsoper.com/statcalc  

Soper, D.S. (2019b). Post-hoc Statistical Power Calculator for Multiple Regression [Software]. 

Available from http://www.danielsoper.com/statcalc  

Stanney, K. M., Kennedy, R. S., & Drexler, J. M. (1997, October). Cybersickness is not simulator 

sickness. In Proceedings of the Human Factors and Ergonomics Society annual meeting (Vol. 41, 
No. 2, pp. 1138-1142). Sage CA: Los Angeles, CA: SAGE Publications.  

Toschi, N., Kim, J., Sclocco, R., Duggento, A., Barbieri, R., Kuo, B., & Napadow, V. (2017). Motion 
sickness increases functional connectivity between visual motion and nausea-associated brain 
regions. Autonomic Neuroscience, 202, 108-113. 

Weech, S., Kenny, S., & Barnett-Cowan, M. (2019). Presence and cybersickness in virtual reality are 

negatively related: a review. Frontiers in psychology, 10, 158. 

Wetzels, R., & Wagenmakers, E. J. (2012). A default Bayesian hypothesis test for correlations and 

partial correlations. Psychonomic bulletin & review, 19(6), 1057-1064. 

Zarour, M., Abran, A., Desharnais, J. M., & Alarifi, A. (2015). An investigation into the best 

practices for the successful design and implementation of lightweight software process assessment 
methods: A systematic literature review. Journal of Systems and Software, 101, 180-192. 

23 

 
 
 
 
 
 
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

Table 2. Domains and Criteria for VR Research/Clinical Software 

Domains 

User Experience 

Game Mechanics 

An Adequate Level of 
Immersion 

A Suitable Navigation 
System (e.g., 
Teleportation) 

In-Game 
Assistance 
Digestible Tutorials 

I

A
R
E
T
I
R
C

Pleasant VR Experience 

Availability of 
Physical Movement 

Helpful Tutorials 

High Quality Graphics 

Naturalistic 
Picking/Placing of 
Items 

Adequate Duration 
of Tutorials 

High Quality Sounds 

Naturalistic Use of 
Items 

Helpful In-game 
Instructions 

Suitable Hardware (HMD 
and Computer) 

Naturalistic 
2-Handed Interaction 

Helpful In-game 
Prompts 

Derived from Kourtesis et al. (2019) 

VRISE 

Absence or 
Insignificant Presence 
of Nausea 

Absence or 
Insignificant Presence 
of Disorientation 

Absence or 
Insignificant Presence 
of Dizziness 

Absence or 
Insignificant Presence 
of Fatigue 

Absence or 
Insignificant Presence 
of Instability 

Table 3. Domains and Criteria for VR Research/Clinical Software 

Domains 

User Experience 

Game Mechanics 

In-Game 

VRISE 

Assistance 

a
i
r
e
t
i
r
C

An Adequate Level of 

A Suitable Navigation 

Comprehensible 

Absence or Insignificant 

Immersion 

System (e.g., 

Tutorials 

Presence of Nausea 

Teleportation) 

Pleasant VR 

Availability of 

Helpful Tutorials 

Absence or Insignificant 

Experience 

Physical Movement 

Presence of Disorientation 

High-Quality 

Naturalistic 

Adequate Duration 

Absence or Insignificant 

Graphics 

Picking/Placing of Items 

of Tutorials 

Presence of Dizziness 

High-Quality Sounds 

Naturalistic Use of Items 

Helpful In-game 

Absence or Insignificant 

Instructions 

Presence of Fatigue 

Suitable Hardware 

Naturalistic 

Helpful In-game 

Absence or Insignificant 

(HMD and Computer) 

2-Handed Interaction 

Prompts 

Presence of Instability 

Derived from Kourtesis et al., 2019 

24 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 2. Internal Reliability and Goodness of Fit for the VRNQ 

  Virtual Reality Neuroscience Questionnaire 

Statistics 

Thresholds 

Results 

Cronbach’s α 

χ 2/df 

Comparative Fit Index (CFI) 

Tuckere Lewis Index (TLI) 

≥ 0.70 

≤ 2.00 

≥ 0.90 

≥ 0.90 

Standardised root mean square residual (SRMR) 

< 0.08 

Root mean square error of approximation (RMSEA) 

≤ 0.08 

0.889 

1.610 

0.954 

0.938 

0.076 

0.071 

25 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3. Descriptive Statistics: Duration of VR Sessions and VRNQ Scores 

 Virtual Reality Neuroscience Questionnaire 

Total Duration 

Duration of Session 1 

Duration of Session 2 

Duration of Session 3 

VRNQ Total Score 
Out of 140 
(Across 3 Sessions) 

User’s Experience (Across 3 Sessions) 
Out of 35 

Game Mechanics (Across 3 Sessions) 
Out of 35 

In-Game Assistance 
(Across 3 Sessions) 
Out of 35 

VRISE 
(Across 3 Sessions) 
Out of 35 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

Gamers 
Non-Gamers 

Total 

18  
22 

40 

18 
22 

40 

18 
22 

40 

18 
22 

40 

18  
22  

40  

18  
22  

40  

18  
22  

40  

18  
22  

40  

18  
22  

40  

199.39 (13.63) 
186.36 (11.76) 

192.2 (14.09) 

65.61 (7.14) 
54.77 (5.91) 

59.65 (8.42) 

63.33 (6.16) 
65.86 (6.21) 

64.72 (6.24) 

70.44 (7.78) 
65.73 (6.75) 

67.85 (7.52) 

127.2 (7.32)  
125.6 (7.71) 

126.3 (7.55) 

31.37 (2.73)  
30.91 (2.73) 

31.12 (2.73) 

31.50 (2.68)  
31.32 (2.61) 

31.40 (2.63) 

31.70 (2.59) 
31.65 (2.52)  

31.68 (2.54) 

32.67 (2.17)  
31.71 (2.56)  

32.14 (2.43) 

3.21 
2.51 

2.23 

1.68 
1.26 

1.33 

1.45 
1.32 

0.99 

1.83 
1.44 

0.69 

0.99 
0.95 

0.69 

0.34 
0.37 

0.25 

0.37 
0.32 

0.24 

0.35 
0.31 

0.23 

0.30 
0.32 

0.22 

26 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  Virtual Reality Neuroscience Questionnaire 

Table 4. VRNQ Cut-offs 

Score 

Minimum Cut-offs 

Parsimonious Cut-offs 

User Experience 

Game Mechanics 

In-Game Assistance 

VRISE 

≥ 25/35 

≥ 25/35 

≥ 25/35 

≥ 25/35 

≥ 30/35 

≥ 30/35 

≥ 30/35 

≥ 30/35 

VRNQ Total Score 

≥ 100/140 

≥ 120/140 

The median of each sub-score and totals scores should meet the suggested cut-offs to support that the 
evaluated VR software has an adequate quality without any significant VRISE.  The utilisation of the 
parsimonious cut-offs more robustly supports the suitability of the VR software. 

Table 5. Bayesian Independent Samples T-Test: Gamers against Non-Gamers 

Significance 

* 

*** 

Variables 

Age 

Education 

Total Duration 

Session 1 Duration 

Session 2 Duration 

Session 3 Duration 

VRNQ Total 

User’s Experience 

Game Mechanics 

In-Game Assistance 

VRISE 

BF₁₀ 

0.323 

0.325 

14.987 

2531.886 

0.595 

1.580 

0.425 

0.359 

0.315 

0.315 

0.745 

error % 

0.006 

0.006 

7.044e -6 

7.491e -8 

0.006 

0.003 

0.007 

0.006 

0.006 

0.006 

0.003 

27 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Virtual Reality Neuroscience Questionnaire 

Table 6. Bayesian Paired Samples T-Tests: Differences between the VR Software 

Pairs 

Significance 

Session 2 Duration 

Session 1 Duration 

Session 3 Duration 

Session 2 Duration 

Session 3 Duration 

Session 1 Duration 

S3 VRNQ Total 

S3 VRNQ Total 

S2 VRNQ Total 

S3 VRISE 

S3 VRISE 

S2 VRISE 

S2 VRNQ Total 

S1 VRNQ Total 

S1 VRNQ Total 

S2 VRISE 

S1 VRISE 

S1 VRISE 

S3 In-Game Assistance 

S2 In-Game Assistance 

S2 In-Game Assistance 

S1 In-Game Assistance 

S3 In-Game Assistance 

S1 In-Game Assistance 

S3 Game Mechanics 

S2 Game Mechanics 

S2 Game Mechanics 

S1 Game Mechanics 

S3 Game Mechanics 

S1 Game Mechanics 

S3 User’s Experience 

S2 User’s Experience 

S3 In-Game Assistance 

S1 User’s Experience 

S2 User’s Experience 

S1 User’s Experience 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

BF₁₀ 

7.049  

2.783  

error % 

~ 0.001  

~ 3.276e -4  

103568.858  

6.942e +12  

3.520e +20  

8.500e +17  

22075.036  

1.322e +10  

1.160e +7  

207216.904  

1.197e +7  

8.028e +10  

274310.417  

4.883e +14  

2.876e +14  

2.873e +7  

2.597e +7  

1.708e +6  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

NaN  

BF₁₀ = Bayes Factor; * BF₁₀ > 10, ** BF₁₀ > 30, *** BF₁₀ > 100; S1 = Session 1; S2 = Session 2; 

 S3 = Session 3.  

28 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 7. Bayesian Pearson Correlations Analyses: VRISE Score with VRNQ Items 

  Virtual Reality Neuroscience Questionnaire 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

VRISE 

Pairs 

Significance 

BF₁₀ 

Immersion 

Pleasantness 

Graphics 

Sound 

VR Tech 

Navigation 

Physical Movement 

Pick & Place 

Use Items 

Two-Handed Interaction 

Tutorial Difficulty 

Tutorials Usefulness 

Tutorials’ Duration 

Instructions 

Prompts 

*** 
* 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

*** 

1226.538 

20.504 

1629.195 

18586.578 

5.094 

4.808 

2.229 

175.087 

0.405 

0.506 

28252.587 

161.949 

128.539 

952.871 

706510.726 

r 

0.371 

0.273 

0.377 

0.421 

0.228 

0.226 

0.197 

0.329 

0.109 

0.123 

0.428 

0.327 

0.322 

0.366 

0.476 

BF₁₀ = Bayes Factor; * BF₁₀ > 10, ** BF₁₀ > 30, *** BF₁₀ > 100; 

Table 8. Models’ Comparison: Predictors of VRISE score 

Models 

P(M)  P(M|data) 

BFM 

BF10 

R² 

Prompts + Sound + Graphics + Immersion + Instructions 

0.004 

0.304 

Prompts + Graphics + Immersion + Instructions + Pleasantness 

0.004 

0.173 

Prompts + Sound + Graphics + Immersion + Instructions + Pick & 
Place 

0.004 

0.161 

117.42 
*** 

56.47  
** 

43.15 
 * 

1.000 

0.324 

0.571 

0.317 

0.443 

0.330 

Prompts + Sound + Graphics + Immersion + Instructions + Pick 
&Place + Tutorials Usefulness + Pleasantness 

0.021 

0.123 

6.62 

0.072 

0.337 

Prompts + Graphics + Immersion + Instructions + Pick & Place + 
Tutorials Usefulness + Pleasantness 

0.008 

0.077 

10.72  
* 

0.121 

0.329 

P = Probability; M = Model; BFM = Model’s Bayesian Factor; * BFM >10, ** BFM >30, *** BFM >100;                
BF₁₀ = BF against null model 

29 

 
 
 
 
 
 
 
 
 
 
 
