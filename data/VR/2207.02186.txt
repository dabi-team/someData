2
2
0
2

l
u
J

5

]

V
C
.
s
c
[

1
v
6
8
1
2
0
.
7
0
2
2
:
v
i
X
r
a

NeuralPassthrough: Learned Real-Time View Synthesis for VR

Lei Xiao
Reality Labs Research, Meta
United States of America
lei.xiao@fb.com

Salah Nouri
Reality Labs Research, Meta
United States of America
snouri@fb.com

Joel Hegland
Reality Labs Research, Meta
United States of America
hegland@fb.com

Alberto Garcia Garcia
Reality Labs, Meta
Switzerland
agarciagarcia@fb.com

Douglas Lanman
Reality Labs Research, Meta
United States of America
douglas.lanman@fb.com

Figure 1: We demonstrate the first neural view synthesis method that is optimized to meet the unique requirements for VR
passthrough, synthesizing perspective-correct viewpoints in real time and with high visual fidelity. Left: We demonstrate
performance using a custom-built VR headset, containing a stereo RGB camera rig with an adjustable baseline. Right: Our
method runs in real-time and supports dynamic scenes (top) and near-field objects (bottom).

ABSTRACT
Virtual reality (VR) headsets provide an immersive, stereoscopic
visual experience, but at the cost of blocking users from directly
observing their physical environment. Passthrough techniques are
intended to address this limitation by leveraging outward-facing
cameras to reconstruct the images that would otherwise be seen by
the user without the headset. This is inherently a real-time view syn-
thesis challenge, since passthrough cameras cannot be physically
co-located with the userâ€™s eyes. Existing passthrough techniques
suffer from distracting reconstruction artifacts, largely due to the
lack of accurate depth information (especially for near-field and
disoccluded objects), and also exhibit limited image quality (e.g., be-
ing low resolution and monochromatic). In this paper, we propose

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9337-9/22/08.
https://doi.org/10.1145/3528233.3530701

the first learned passthrough method and assess its performance
using a custom VR headset that contains a stereo pair of RGB cam-
eras. Through both simulations and experiments, we demonstrate
that our learned passthrough method delivers superior image qual-
ity compared to state-of-the-art methods, while meeting strict VR
requirements for real-time, perspective-correct stereoscopic view
synthesis over a wide field of view for desktop-connected headsets.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Mixed / augmented reality;
Machine learning; Image-based rendering.

KEYWORDS
Passthrough, Real-Time View Synthesis, Virtual Reality

ACM Reference Format:
Lei Xiao, Salah Nouri, Joel Hegland, Alberto Garcia Garcia, and Douglas
Lanman. 2022. NeuralPassthrough: Learned Real-Time View Synthesis for
VR. In Special Interest Group on Computer Graphics and Interactive Techniques
Conference Proceedings (SIGGRAPH â€™22 Conference Proceedings), August 7â€“11,
2022, Vancouver, BC, Canada. ACM, New York, NY, USA, 9 pages. https:
//doi.org/10.1145/3528233.3530701

 
 
 
 
 
 
SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Xiao, L. et al

Figure 2: Left: Typically, VR passthrough cameras are located
several centimeters away from the userâ€™s eyes. Right: Sep-
aration between the cameras and the eyes requires signifi-
cant viewpoint changes to be resolved by the passthrough
algorithm. Here the right-eye camera image (shown in red)
is compared to what should be seen by the userâ€™s right eye
(shown in cyan).

1 INTRODUCTION
Virtual reality (VR) head-mounted displays (HMDs) provide nearly
complete visual immersion, using a pair of near-eye displays to
create wide-field-of-view, stereoscopic images. However, such im-
mersion comes at the cost of visual isolation from the userâ€™s physical
environment. For certain applications, a direct view of the nearby
surroundings is necessary. To this end, augmented reality (AR) uses
near-eye displays to support optical see-through. Yet, modern AR
displays achieve limited fields of view, unlike blocked-light VR;
thus, video see-through VR has been proposed as a solution, using
passthrough algorithms to transform imagery collected by outward-
facing cameras to enable the user to see their surroundings.

In practice, VR passthrough systems do not directly pass through
anything. Rather, they must accomplish the difficult task of repro-
jecting camera images to appear from the userâ€™s perspective. This
is often approximated, with Krajancich et al. [2020] showing the
value of updating reconstructions to track the userâ€™s moving pupils.
While pupil-tracked passthrough may be the ultimate goal, state-
of-the-art techniques, such as Passthrough+ [Chaurasia et al. 2020],
reproject camera data to the nominal position of the eyes, while
accepting any artifacts resulting from the computational limits.

Real-time view synthesis lies at the core of achieving compelling
passthrough experiences. While this is a challenging problem it-
self [Mildenhall et al. 2019, 2020; Zhou et al. 2018], VR headsets
present particularly daunting requirements that cannot be sup-
ported by many modern methods. Namely, commercial VR displays
are stereoscopic, refresh at 72-144 frames per second, and support
wide fields of view (>90â—¦, horizontally). For VR passthrough, a
typical scenario involves a userâ€™s manipulating near-field objects
with their own hands and observing dynamic environments, result-
ing in large regions with missing data, due to disocclusions, and
preventing offline reconstruction from prior observations.

Given these algorithmic challenges, headset designers can assist
passthrough by placing cameras as close to the userâ€™s eyes as pos-
sible, asking the algorithm to make only modest changes. Yet, as
shown in Figure 2, cameras cannot be co-located with the userâ€™s
eyes. Thus, in this paper, we optimize the performance of a minimal
passthrough architecture: placing a stereo pair of RGB cameras on
the front of a VR headset (see Figure 1). Consistent with Chaurasia
et al. [2020], we identify that such a configuration offers a practical
trade-off between hardware and algorithmic complexity. However,

unlike prior works, we consider the optimal placement of the cam-
eras to work in concert with the passthrough algorithm, exploring
how the baseline can be adjusted to mitigate reprojection artifacts.
We aim to find an efficient, high-quality method for real-time
stereoscopic view synthesis from stereo inputs. Being the closest
prior work, we seek to address the limitations of Chaurasia et
al. [2020], which applies a traditional 3D computer vision pipeline:
for each frame, a sparse point cloud is reconstructed and pro-
cessed to produce meshes for reprojection. In contrast, we introduce
NeuralPassthrough to leverage recent advances in deep learning,
solving passthrough as an image-based neural rendering problem.
Specifically, we jointly apply learned stereo depth estimation and
image reconstruction networks to produce the eye-viewpoint im-
ages via an end-to-end approach, one that is tailored for todayâ€™s
desktop-connected VR headsets and their strict real-time require-
ments. The source code and pretrained models will be available
at https://research.facebook.com/publications/neural-passthrough/,
pending institutional approval. Our primary technical contributions
include the following:

â€¢ We build a VR headset with an adjustable stereo camera
baseline, optimized for evaluating view synthesis methods.
â€¢ We analyze the impact of camera placement on VR passthrough
image quality; our analysis reveals that key disocclusions can
be mitigated by adopting wider camera baselines than the
userâ€™s interpupillary distance (IPD) â€” a design optimization
that diverges from current consumer products.

â€¢ We demonstrate the first learned view synthesis method tai-
lored for real-time VR passthrough, suppressing key artifacts
and achieving higher image quality than prior methods.

2 RELATED WORK
2.1 Classical Methods for View Synthesis
Early work on light field imaging addresses view synthesis as an
interpolation problem, resampling captured rays to generate novel
views [Gortler et al. 1996; Levoy and Hanrahan 1996]. However,
by requiring a dense set of input views, such methods are not well
suited for VR passthrough. View synthesis from sparse views gained
much attention in graphics rendering [Chen and Williams 1993;
Oculus 2016], computational photography [Hedman et al. 2017;
Shade et al. 1998; Zitnick et al. 2004] and displays [Chapiro et al.
2014; Didyk et al. 2013]. Most of these methods require rendered
depth as input, allow limited viewpoint changes, or run offline.

To our knowledge, Passthrough+ [Chaurasia et al. 2020] is the
only prior work that directly addresses VR passthrough. It is specifi-
cally tailored for mobile VR applications, but achieves limited image
quality. In contrast, our method focuses on desktop-connected VR
headsets, leveraging more compute resources while continuing to
be strictly constrained by the same real-time requirements.

2.2 Learning-based Methods for View Synthesis
Significant recent progress has been made in learning-based view
synthesis, which can be grouped according to the inputs: single
views [Kopf et al. 2020; Shih et al. 2020; Wiles et al. 2020]; stereo
views [Zhou et al. 2018]; multiple views [Flynn et al. 2019; Kalantari
et al. 2016; Mildenhall et al. 2019, 2020]; multiview videos [Broxton
et al. 2020]; and RGB-D captures [Aliev et al. 2020; Martin-Brualla

EYESPASSTHROUGHCAMERASNeuralPassthrough: Learned Real-Time View Synthesis for VR

SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

et al. 2018]. In closely related work, Zhou et al. [2018] represent a
static scene with multiplane images (MPIs) generated from stereo
pairs. Once generated, MPIs can be reused to efficiently render a
range of output views. DeepView [Flynn et al. 2019] generates MPIs
from multiview images using learned gradient descent. While MPIs
allow for efficient rendering of dynamically varying viewpoints,
generating an MPI from input images can be computationally ex-
pensive. Furthermore, MPIs can only render a limited range of
views, i.e., perceptible artifacts often appear when the output view
is relatively far from the reference views. Mildenhall et al. [2019]
mitigate this problem by building multiple MPIs from a broad set
of input views, which are blended together to create the output
view, albeit with significant computational overhead (due to mul-
tiple MPI generation/rendering steps and costly 3D convolutional
networks). Martin-Brualla et al. [2018] does real-time neural view
synthesis, but it requires textured 3D reconstruction from a per-
formance capture system as input and is limited to human scenes.
Aliev et al. [2020] introduces neural point-based representation but
it requires a raw point cloud as input and presents limited quality.
A prominent line of recent research relates to neural radiance
fields (NeRFs) [Mildenhall et al. 2020]. NeRFs represent a scene as an
implicit function encoded by multilayer perceptrons (MLPs). NeRF-
inspired approaches have led to rapid advances in view synthesis;
however, none of these recent works directly address the unique
challenges of VR passthrough. Specifically, many NeRF-derived
methods rely on a dense set of views. More significantly, these
methods incur prohibitive computational costs for per-scene opti-
mization and volume rendering. Despite recent work to decrease
these costs [Garbin et al. 2021; Hedman et al. 2021; Liu et al. 2020;
Reiser et al. 2021; Yu et al. 2021a], NeRF-based methods still do not
appear practical for VR passthrough. Similarly, other works have
attempted to reduce the number and range of input views [Jain
et al. 2021; Yu et al. 2021b; Zhang et al. 2020], to support dynamic
scenes [Gao et al. 2021; Pumarola et al. 2021] and to be general-
izable [Guo et al. 2022; Yu et al. 2021b]; however, these lines of
research have also fallen short on the crucial real-time requirement.

3 NEURAL PASSTHROUGH
In this section, we describe our neural passthrough system, includ-
ing both optimizing the configuration of our hardware prototype
(Section 3.1) and the aspects of designing and training our real-time
neural view synthesis method (Sections 3.2 and 3.3, respectively).

3.1 Optimizing the Hardware Configuration
As outlined in Section 1, we focus on constructing a passthrough
capture system with minimal hardware (i.e., using a pair of stereo
RGB cameras as the only input). As such, the hardware design
challenge can be posed as an optimization problem: where should
the cameras be placed to best support view synthesis algorithms?
We define the objective as maximizing the information captured
from the 3D scene, by the stereo cameras, that is necessary for recon-
structing the target novel views. In other words, we want to select
the stereo camera placement to minimize the extent of the disoc-
clusion regions (i.e., the set of 3D points that would be visible in
the target novel views but that are occluded in the input views and,
thus, cannot be faithfully recovered by view synthesis).

Figure 3: The extent of disocclusion regions (i.e., scene points
that should be seen by the userâ€™s eyes but are occluded from
the cameraâ€™s perspective) depends on the VR headset con-
struction, the position of the userâ€™s eyes, and the physical
scene geometry. In this simplified model, the width ğ›½ of
the disocclusion region is determined for a scene containing
a near-field occluder and a far background plane. See Sec-
tion 3.1 for an application of this model to the optimization
of stereo camera baselines for VR passthrough.

In Figure 3, we analyze the parameters that affect disocclusion.
Note that, since we only consider a pair of cameras on the front sur-
face of a HMD, we constrain both cameras to be vertically located on
the same plane as the nominal eye positions and the optical centers
of the viewing optics. Both cameras face directly forward and are
further constrained to be symmetric about the HMD center. Under
these constraints, the free parameters defining the camera place-
ment reduce to just the horizontal offset ğ›¼ between each camera
and its corresponding eye. Intuitively, one might consider setting
ğ›¼ to zero, minimizing the distance between the input and target
viewpoints. However, we propose to increase ğ›¼ (to a certain ex-
tent) to reduce the degree of disocclusions and, thus, provide some
assistance to the view synthesis algorithm.

Applying the model in Figure 3, disocclusion appears in the
target view due to the viewpoint difference between the camera
and the eye. The width of the disocclusion region ğ›½ is given by
(cid:19)
(cid:18) ğœƒ

(cid:19)

(cid:18)

(cid:19)

ğ›½ = max

0, ğœ‘ tan

âˆ’ ğ›¼

Â·

âˆ’ 1

(1)

(cid:18) ğ‘‘ğ‘“
ğ‘‘ğ‘›

2

where ğœ‘ denotes the distance between the camera and the eye
along the depth axis (i.e., the HMD thickness), ğ‘‘ğ‘› and ğ‘‘ğ‘“ denote
the depth of the near occluder and the background respectively
(ğ‘‘ğ‘› < ğ‘‘ğ‘“ ), and ğœƒ âˆˆ [0, ğœ‹) measures the angular region within
which the disocclusion is aimed to be eliminated. Note that, under
our stereo camera constraints, only horizontal disocclusion can be
ğœƒ
2 , disocclusion ğ›½
reduced. From Eq. (1), clearly, when ğ›¼ â‰¥ ğœ‘ tan
will disappear. Given ğœŒ is the target interpupillary distance (IPD),
ğœƒ
the required minimal stereo camera baseline becomes ğœŒ + 2 ğœ‘ tan
2 .
From Eq. (1), we also note that reducing HMD thickness ğœ‘ could
reduce disocclusion ğ›½. This suggests that the passthrough problem
can benefit from more compact headset designs, such as those
introduced by Maimone and Wang [2020]. In addition, disocclusion
ğ›½ increases when foreground objects are closer.

DISOCCLUSION ğœ·ğœ·NEAR OCCLUDERFAR BACKGROUNDOFFSETğœ½ğœ½FOREGROUND DEPTH ğ’…ğ’…ğ’ğ’BACKGROUND DEPTH ğ’…ğ’…ğ’‡ğ’‡HMD THICKNESS ğ‹ğ‹IPDğ†ğ†ğœ¶ğœ¶SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Xiao, L. et al

Figure 4: Overview of the NeuralPassthrough algorithm.

While this simplified model motivates slightly increasing the
camera baseline in comparison to the userâ€™s IPD, this design choice
does potentially introduce other issues. Following Figure 3, as the
left-hand camera moves to the left, it may reduce or even eliminate
disocclusion on the left side of the occluder. However, disocclusion
may be introduced on the right side. We observe that, since we have
a stereo camera pair, increasing disocclusion in this manner may
be somewhat compensated by utilizing observations from the other
camera. Many anticipated near-field objects in VR use cases, such as
hands and handheld implements, are compact enough such that the
other camera can contribute to observing what would otherwise
become a hidden surface. Thus, we have elected to increase our cam-
era baseline in our prototype HMD in this manner. Experimental
results are given in the supplementary material.

As shown in Figure 1, our prototype includes stereo RGB cam-
eras (from Azure Kinect DK [2021]) attached to an Oculus Rift S
headset [2021]. The camera enclosures are removed and some elec-
trical components are folded away from the front of the headset.
We place the stereo cameras on a linear translation stage, allowing
the baseline to be adjusted. The supported baseline ranges from
5.4cm to 10cm. For the results shown in this paper, we selected a
10cm baseline. That supports an angular region ğœƒ = 25â—¦, where the
disocclusion is eliminated for an IPD of ğœŒ = 6cm, or, equivalently,
ğœƒ = 18â—¦ for an IPD of ğœŒ = 7cm. The distance between the cameras
and the eyes (along the depth axis) is ğœ‘ = 9.3cm. Each RGB camera
runs at 30Hz with a resolution of 1280Ã—720 and a 90â—¦ field of view.

3.2 Learned, Real-Time View Synthesis
In this section, we define the NeuralPassthrough algorithm. This
method addresses passthrough as an image-based rendering prob-
lem, solved separately per frame, taking stereo camera images as
input and producing stereo images for the target eye views.

A block diagram of the method is provided in Figure 4. At a high
level, the method represents the scene with 2D color and depth
(RGB-D) images. A depth map is estimated at each of the input views
by deep-learning-based disparity estimation (Section 3.2.1). The
RGB-D pixels of both input views are then splatted to each target
view (Section 3.2.3) before being fed into a neural network for final
view reconstruction (Section 3.2.5). To reduce splatting artifacts due
to the ambiguity of depth at discontinuities (e.g., â€œflyingâ€ pixels),

Figure 5: Depth estimation example. Note that the depths, es-
timated by the algorithm in Section 3.2.1, well approximate
the reference depths. The arrows indicate regions that are
only visible in one of the stereo inputs, but where plausible
depth is estimated from monocular depth cues.

Figure 6: Applying the RGB-D sharpening operation, follow-
ing Section 3.2.2, suppresses â€œflying-pixelâ€ artifacts and pro-
duces cleaner depth maps. As shown on the left-hand side,
without RGB-D sharpening, â€œflying pixelsâ€ are produced
within the disocclusion regions (i.e., the background areas
surrounding the edge of the lamp shade and the boundaries
of the characters).

the method filters the RGB-D data at each input view (Section 3.2.2)
before the splatting operation. The method further applies pro-
cessing to reduce disocclusion artifacts in the splatted RGB values
(Section 3.2.4) before passing them to the final reconstruction.

3.2.1 Depth Estimation. We first rectify the input color image pairs,
reducing disparity estimation from a 2D correspondence-matching
problem to a more efficient 1D matching problem. In contrast to
Passthrough+ [Chaurasia et al. 2020], which estimates scene depth
from motion vectors produced by video encoding hardware, we

Stereo Rectification(Left) Disparity Estimation(Left) RGB-D Sharpening(Right) Disparity Estimation(Right) RGB-D Sharpening(Left) RGB-D Forward SplattingFusion(Right) RGB-D Forward SplattingDisocclusion Filtering(Left) RGB-D Forward SplattingFusion(Right) RGB-D Forward SplattingDisocclusion FilteringLeft EyeRight EyeStereo ImagesEstimated Depth - LeftReference Depth - LeftEstimated Depth - RightReference Depth - RightInput View - LeftInput View - RightWithout RGB-D SharpeningWith RGB-D SharpeningNeuralPassthrough: Learned Real-Time View Synthesis for VR

SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

leverage neural approaches to produce higher quality depth maps.
Specifically, we apply the RAFT-Stereo algorithm [Lipson et al. 2021]
to estimate a disparity map at each of stereo input views, which
are then converted to inverse depth maps using pre-calibrated
camera parameters. We denote the rectified input color and the
estimated inverse depth as {cğ‘™ , cğ‘Ÿ } and {dğ‘™ , dğ‘Ÿ } respectively, where
the subscript ğ‘™ and ğ‘Ÿ indicate left and right views respectively.

Figure 5 shows an example of the estimated depth maps recov-
ered from stereo inputs, which accurately approximate the reference
depth maps. Importantly, for regions that are only visible in one
of the input views, the depth estimation network can still produce
reasonable results from neighboring pixels and from monocular
depth cues learned during training â€” differing from the motion-
vector-based depth in Passthrough+ [Chaurasia et al. 2020] and the
plane-sweep-volume approach in MPI-based methods [Zhou et al.
2018]. This is one of the key reasons that we choose to estimate
the depth at each input view, since the two depth maps provide
complementary information of the scene geometry.

3.2.2 RGB-D Sharpening. While the estimated depth maps visually
align with the corresponding color images, if they were directly
used for view reprojection (Section 3.2.3), â€œflying pixelsâ€ would
occur at the disoccluded regions in the reprojected images, due
to depth ambiguity at depth discontinuities (see Figure 6). To re-
duce this problem, we propose sharpening the color images and
estimated depth maps along depth discontinuities. Specifically, we
detect the depth edges with Sobel filter followed by morphological
dilation, and then set the RGB-D values of the edge pixels to that of
their nearest-neighbor, non-edge pixels. We emphasize that another
benefit of such RGB-D sharpening is that it helps produce clean
depths in the splatted image space, which are important for the fol-
lowing disocclusion filtering step to work properly (Section 3.2.4).

Forward Splatting. We elect to apply a neural network to
3.2.3
reconstruct the color image, at each target eye viewpoint, from the
color and recovered depth for the input stereo views. To reduce
the required receptive field of the neural network, we first warp
each input view to the target view. Since the depths are estimated
for the input views, forward warping is required. Compared to
backward warping, forward warping is more prone to introducing
holes due to disocclusion; similarly, with forward warping, multiple
source pixels can map to the same pixel in the warped image space,
due to newly introduced occlusions. Both failure cases often occur
for VR passthrough applications. In this section, we focus on the
issue caused by newly introduced occlusions, leaving the discussion
regarding disocclusion holes until Section 3.2.4.

Fortunately, we have the estimated depth at each input view,
providing visibility information for each 3D point. We employ the
softmax splatting method that was originally developed for video
frame interpolation [Niklaus and Liu 2020]. This method blends
the pixels that were mapped to the same target pixel, applying
pixel-wise importance weights defined as a measure of occlusion.
In our implementation, we define the importance weights w to be
a function of the estimated inverse depth d, given by

w = 36

(cid:18) d âˆ’ dğ‘šğ‘–ğ‘›
dğ‘šğ‘ğ‘¥ âˆ’ dğ‘šğ‘–ğ‘›

+

(cid:19)

1
9

(2)

ALGORITHM 1: Full Disocclusion Filtering

Input: Color image (cid:98)c, inverse depth d, occlusion mask (cid:98)m, kernel k
Output: Filtered color image câˆ—

for each pixel i do
if (cid:98)m(ğ‘–) is 0 then
câˆ— (ğ‘–) = (cid:98)c(ğ‘–)

else
N
ğ‘šğ‘–ğ‘›, d
d
for each pixel j in local neighborhood Nğ‘– do

N
ğ‘šğ‘ğ‘¥ , ğ‘ğ‘ğ‘ğ‘, ğ‘¤ğ‘ğ‘ğ‘ = MAX, MIN, 0, 0

if d( ğ‘—) > 0.01 then
N
ğ‘šğ‘ğ‘¥ = ğ‘šğ‘–ğ‘› (d

N
ğ‘šğ‘–ğ‘›, d
d

N
ğ‘šğ‘–ğ‘›, d( ğ‘—)), ğ‘šğ‘ğ‘¥ (d
for each pixel j in local neighborhood Nğ‘– do
N
ğ‘šğ‘ğ‘¥ ) then

if d( ğ‘—) > 0.01 and d( ğ‘—) < 0.5(d

N
ğ‘šğ‘–ğ‘› + d

N
ğ‘šğ‘ğ‘¥ , d( ğ‘—))

ğ‘ğ‘ğ‘ğ‘ += (cid:98)c( ğ‘—) Â· k(ğ‘–, ğ‘—)
ğ‘¤ğ‘ğ‘ğ‘ += k(ğ‘–, ğ‘—)

if ğ‘¤ğ‘ğ‘ğ‘ > 0 then

câˆ— (ğ‘–) = ğ‘ğ‘ğ‘ğ‘ /ğ‘¤ğ‘ğ‘ğ‘

else

câˆ— (ğ‘–) = (cid:98)c(ğ‘–)

where dğ‘šğ‘–ğ‘› and dğ‘šğ‘ğ‘¥ are the minimum and maximum of the in-
verse depth map d, and the heuristic constants are chosen to map the
weights to the range [4, 40], which works well in our experiments.
The metric w assigns higher weights to the source pixels closer to
the cameras (in the warped image space). We denote the splatted
color and inverse depth as {cğ‘™ , cğ‘Ÿ } and {dğ‘™ , dğ‘Ÿ }, respectively.

3.2.4 Disocclusion Filtering. The forward-splatted images at the
target views typically contain holes due to disocclusions, as shown
in Figure 7. In this section, we describe our approach to address
this issue. We divide the disocclusion holes into two categories and
treat them separately: partial disocclusion, defined as the holes that
occur in only one of the splatted images (i.e., either cğ‘™ or cğ‘Ÿ ), and
full disocclusion, defined as the holes that occur in both cğ‘™ and cğ‘Ÿ .
Partial disocclusion can be removed by blending cğ‘™ and cğ‘Ÿ :

(cid:98)cğ‘™ = (1 âˆ’ mğ‘™ ) âŠ™ cğ‘™ + mğ‘™ âŠ™ cğ‘Ÿ
(cid:98)cğ‘Ÿ = (1 âˆ’ mğ‘Ÿ ) âŠ™ cğ‘Ÿ + mğ‘Ÿ âŠ™ cğ‘™

(3)

where âŠ™ denotes the Hadamard product, and the pixel-wise masks
mğ‘™ and mğ‘Ÿ are defined on the splatted inverse depth dğ‘™ and dğ‘Ÿ , as

mğ‘™ =

(cid:40)

if dğ‘™ < ğœ–
1
0 otherwise

, mğ‘Ÿ =

(cid:40)

if dğ‘Ÿ < ğœ–
1
0 otherwise

(4)

where ğœ– = 0.1 and {mğ‘™ , mğ‘Ÿ } indicate the zero-valued pixels in the
splatted inverse depth maps {dğ‘™ , dğ‘Ÿ }. Partial disocclusion removal
results are shown in Figure 7.

Full disocclusions can not be faithfully recovered as the input
stereo inputs contain no information for those regions. As reviewed
in Section 2.2, prior work in 3D photography [Shih et al. 2020] re-
solves disocclusions via context-aware image inpainting. We find
that for static images visually acceptable results can be hallucinated;
but, if applied to our dynamic passthrough problem, this approach
will introduce temporal flickering into the output videos. Further-
more, advanced image inpainting techniques [Liu et al. 2018; Shih
et al. 2020] are typically computationally expensive, hindering their
utility for our real-time application.

SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Xiao, L. et al

Table 1: Each layer of the fusion network (Section 3.2.5) is
a 2D convolution followed by relu activation. The operators
concat, down, and up, represent concatenation, average pool-
ing, and bilinear upsampling, respectively.

Layer
conv0
conv1
conv2
conv3
conv4
conv5
conv6
conv7
conv8
conv9
conv10

Input Tensor
concat(câˆ—
ğ‘™ , câˆ—
ğ‘Ÿ )
conv0
down(conv1)
conv2
down(conv3)
conv4
concat(cid:0)up(conv5), conv(cid:1))
layer6
concat(cid:0)up(conv7), conv1(cid:1)
conv8
conv9

Channels In/Out
6/16
16/16
16/32
32/32
32/64
64/64
96/32
32/32
48/16
16/16
16/3

Table 2: Quality comparisons for NeuralPassthrough (Ours)
on synthetic datasets. Average PSNR (dB), SSIM and ST-
RRED are reported. Higher PSNR and SSIM, and lower ST-
RRED indicate better quality.

MPI [Zhou et al. 2018]
Ours
Ours (trained without Eq. 5)
Ours (trained without Eq. 3, 5)

PSNRâ†‘
27.38
30.74
28.66
29.02

SSIMâ†‘
0.8818
0.9579
0.9475
0.9456

ST-RREDâ†“
105.74
51.78
95.33
99.33

in turn, could degrade the user experience, especially due to the
limited capacity (by design) of the fusion network in Section 3.2.5).
The stereo depth network reapplies the pre-trained RAFT-Stereo
model with frozen weights during training.

We train the method on a synthetic dataset with 80 random
scenes, similar to the ones in Xiao et al. [2018]. Each scene includes
3D scans of sculptures from the Louvre as well as spheres and
cubes, that are randomly textured and placed in 3D space with
varying depths. The unnatural geometry and appearance makes
the training data substantially different from the synthetic and
real scenes we test in the paper. Example scenes are provided in
the supplementary material. For each scene, we render 20 image
sequences with a resolution of 512 Ã— 512 and rendered at varying
viewpoints (i.e., two views serve as the input stereo pair, with a
10cm baseline, and the rest are the target output views that are
9.3cm behind the input views and with baselines ranging from
4.8cm to 8.0cm). Note that the trained network can be applied to
other camera and IPD configurations, as well as to different input
resolutions, at test time. We train the method in Pytorch using the
ADAM optimizer with default parameters for 120 epoches.

4 RESULTS
4.1 Real-Time Implementation
After training, we implement our full, optimized inference method
in C++ using CUDA/CuDNN, integrating it with the Oculus Rift
SDK for real-time passthrough demonstrations. We test the method

Figure 7: Example results for disocclusion filtering, as de-
fined in Section 3.2.4. cğ‘™ and cğ‘Ÿ are the forward-splatted color
images, as taken from the input left and right view, respec-
tively (see Section 3.2.3). (cid:98)cğ‘Ÿ and câˆ—
ğ‘Ÿ are the right view after
partial (Eq. (3)) and full disocclusion filtering (Eq. (5)), respec-
tively. câ€  is the final reconstruction (Eq. 6), and cğ‘Ÿğ‘’ ğ‘“ is the ref-
erence for the target view. The arrows point to regions with
full disocclusion holes.

As an alternative, we propose depth-assisted, anisotropic low-
pass filtering to produce stable results efficiently. We observe that
the disoccluded regions are more often missing information from
background objects rather than from foreground occluders; as a
result, our method fills in the disoccluded pixels using only the
smoothed colors of relatively far objects within the local neighbor-
hood. The method details are given by in Eq. (5) and Algorithm 1:

(cid:98)m = mğ‘™ âŠ™ mğ‘Ÿ
câˆ—
ğ‘™ = full_disocclusion_filtering((cid:98)cğ‘™ , dğ‘™ ,
câˆ—
ğ‘Ÿ = full_disocclusion_filtering((cid:98)cğ‘Ÿ , dğ‘Ÿ ,

(cid:98)m, k)
(cid:98)m, k)

(5)

The mask (cid:98)m indicates whether a pixel is fully disoccluded. k denotes
a low-pass kernel (which in our implementation is a zero-mean
2D Gaussian filter with size 29 Ã— 29 and a standard deviation of 7
pixels). For experimental evaluation of the benefits of the filtering
operations, see Table 2.

Fusion. Our pipeline concludes by feeding the disocclusion-
3.2.5
filtered images to a neural network for final reconstruction at the
target eye views, as given by

câ€  = fusion(câˆ—

ğ‘™ , câˆ—
ğ‘Ÿ )

(6)

where the fusion network is a lightweight U-Net with skip connec-
tions, comprising the specific architecture in Table 1. Note that the
fusion network runs once for each of the two target eye views, as
illustrated in Figure 4. We find that fusion is necessary to further
reduce reprojection errors and aliasing artifacts in câˆ—

ğ‘™ and câˆ—
ğ‘Ÿ .

3.3 Training
The training loss function for NeuralPassthrough is defined as
10 ||(1 âˆ’ (cid:98)m) âŠ™ (câ€  âˆ’ c
where ssim is the structural similarity index measure [Wang et al.
2004]. We apply the mask (1 âˆ’ (cid:98)m) to exclude the full disocclusion
regions from the loss, preventing inpainting at those regions (which
may lead to inconsistent temporal and left/right completions that,

ğ‘Ÿğ‘’ ğ‘“ )||1 âˆ’ ||(1 âˆ’ (cid:98)m) âŠ™ ssim(câ€ , c

ğ‘Ÿğ‘’ ğ‘“ )||1

(7)

NeuralPassthrough: Learned Real-Time View Synthesis for VR

SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Figure 10: Visual comparisons between our method
and Oculus Quest 2, which is
closely related to
Passthrough+ [Chaurasia et al. 2020].

Figure 8: Comparisons on synthetic data. For the DanceS-
tudio scene (top), our PSNR and SSIM are 34.28dB and 0.97,
while MPI [Zhou et al. 2018] achieves 29.69dB and 0.90. For
the ElectronicRoom scene (bottom), ours are 30.06dB and
0.95, while MPI achieves 24.42dB and 0.86. The MPI results
present obvious artifacts (e.g., stretching and repeated tex-
tures at disocclusions).

Figure 9: Qualitative comparisons on data captured by our
prototype headset. Our method does not produce texture
stretching artifacts, while MPI [Zhou et al. 2018] tends to,
while also failing to reconstruct the table leg.

on a desktop with an Intel Xeon W-2155 CPU and dual Nvidia Titan
V GPUs. Each GPU is responsible for one depth estimation and one
eye-view reconstruction. The method runs at 32ms per frame at a
resolution of 1280Ã—720 for the two eye views, with depth estimation
at 24.0ms, RGB-D sharpening at 0.3ms, forward-splatting at 0.8ms,
disocclusion filtering at 0.9ms, and the fusion network at 5.6ms.

While we report these run times for our per-frame processing
pipeline, we see opportunities for further improvement. For exam-
ple, we could run the depth estimation at a lower frame rate (e.g.,
30Hz), but run the color reconstruction operations (i.e., forward
splatting, disocclusion filtering, and fusion, which take 7.3ms in
total) at a higher frame rate (e.g., 72Hz or the native refresh rate of
the displays). In this manner, the more efficient color reconstruction
pipeline could account for the current estimate of the head pose,
while reusing the latest depth estimates. Notably, such a decou-
pled framework is similar to Passthrough+, which reconstructs the
textured meshes at 30Hz, but renders the eye buffers at 72Hz.

Figure 11: Visual comparisons between our method and Ocu-
lus Quest 2 on a dynamic scene. Our method does not show
severe distortions, whereas Quest 2 does as highlighted.

4.2 Quality Comparisons
As there is little recent work on real-time view synthesis, we com-
pare to the representative MPI method [Zhou et al. 2018], which
also takes stereo images as inputs. MPI requires several seconds
to generate the multiplane representation and another several sec-
onds to render stereo eye views (at a resolution of 1280Ã—720 using
TensorFlow with our system). Although follow-up MPI works exist
with improved quality [Mildenhall et al. 2019; Srinivasan et al. 2019],
these approaches are substantially slower due to the need to gener-
ate/render multiple MPIs per frame and the use of 3D convolutional
networks, making them even less applicable to VR passthrough.

For image quality comparisons, we render two datasets for 3D en-
vironments with dynamic objects (DanceStudio and ElectronicRoom).
Each dataset contains 5 videos with simulated VR head movements,
each containing 30 frames at a resolution of 1280Ã—720. For each
frame, we render input stereo views (with a 10cm baseline) and
target eye views (with a 6cm IPD and depth-axis offset of 9.3cm).
These scenes differ in appearance from our static training datasets.
We evaluate the methods using PSNR, SSIM [Wang et al. 2004]
and Spatio-Temporal Entropic Difference (ST-RRED) [Soundarara-
jan and Bovik 2012], where the latter is for video quality and tem-
poral stability assessment. As reported in Table 2, our method
outperforms MPI by a large margin on all metrics. Example re-
sult are shown in Figure 8. Notably, MPI presents more obvious

OursOursMPIReferenceOursOursMPIPatch From Input OursOculus Quest2OursOculus Quest2SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Xiao, L. et al

artifacts, especially distortion, stretching, and repeated textures at
disocclusion regions.

We qualitatively compare these methods using real data captured
by our prototype, as shown in Figure 9. Since we can not capture the
ground truth reference images at the target eye views, we provide
the closest patches from the input views as a visual reference.

As reviewed in Sections 1, the closest related work to our Neu-
ralPassthrough is Passthough+ [Chaurasia et al. 2020], which is the
predecessor to the current commercial solution applied in Oculus
Quest 2. As this commercial implementation is unavailable, and its
camera images are not accessible via the Oculus SDK, we capture
scenes with similar camera trajectories using our prototype and
the Quest 2, allowing for qualitative visual comparisons. Exam-
ple results are shown in Figures 10 and 11. We observe that the
main limitation of Oculus Quest 2 is that the reconstructed mesh
can be inaccurate at depth discontinuities and within disocclusion
regions, causing noticeable distortion and stretching artifacts. In
contrast, our method produces more accurate results, while also
allowing for color outputs with higher resolution. Please refer to
the supplementary material for video results and comparisons.

5 LIMITATIONS AND FUTURE WORK
The quality of our results is partly affected by the quality of our
real-time depth estimation. While the depth estimation module
produces reasonable results in most circumstances, it may fail for
objects with challenging geometric details, for view-dependent
materials, or when the monocular depth cues are lacking. Our
results may also present blending artifacts when the input stereo
views contain mismatched colors due to severe view-dependent
reflections, especially when they appear together with disocclusion.
Examples of these failure cases are shown in Figure 12.

Future improvements in real-time depth estimation may benefit
our approach, updating the associated step of our pipeline. In future
work, active depth sensors may provide instant depth generation
and improve depth estimation for regions where stereo recovery
does not work well (e.g. regions with little texture, difficult partial
disocclusions, or view-dependent effects). To enable this work, our
hardware prototype already incorporates depth sensors.

Oculus Quest 2 passthrough results suffer from severe distortion
artifacts, however they typically appear temporally stable, partly
due to their low resolution, but also due to their multi-frame recon-
struction method. As future work, we anticipate leveraging multiple
frames to further improve spatial quality and temporal consistency.

6 CONCLUSION
This paper takes a first step towards bringing the latest devel-
opments in neural view synthesis to the specific domain of VR
passthrough. With VR recently seeing wider adoption, we believe
improved passthrough technologies are necessary to unlock a broad
set of mixed reality applications â€” seamlessly blending virtual ob-
jects with the userâ€™s physical surroundings. Throughout this paper
we have emphasized that, while neural view synthesis is an increas-
ingly well studied topic, VR applications set a much higher bar on
performance. To deliver compelling VR passthrough, the field will
need to make significant strides both in image quality (i.e., suppress-
ing notable warping and disocclusion artifacts), while meeting the

Figure 12: Example failure cases on real data. Our system
may produce artifacts for objects with significant view-
dependent reflections (monitor corner) or with complex ge-
ometry and ambiguities in stereo matching (mug handle).

strict real-time, stereoscopic, and wide-field-of-view requirements.
Tacking on the further constraint of mobile processors for wearable
computing devices means that there truly is a long road ahead.

REFERENCES
2021. Azure Kinect DK. https://azure.microsoft.com/en-us/services/kinect-dk/
2021. Rift-S VR. https://www.oculus.com/rift-s/features/
Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lem-
pitsky. 2020. Neural point-based graphics. In European Conference on Computer
Vision. Springer, 696â€“712.

Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew
Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. 2020. Im-
mersive light field video with a layered mesh representation. ACM Transactions on
Graphics (TOG) 39, 4 (2020), 86â€“1.

Alexandre Chapiro, Simon Heinzle, TunÃ§ Ozan AydÄ±n, Steven Poulakos, Matthias
Zwicker, Aljosa Smolic, and Markus Gross. 2014. Optimizing stereo-to-multiview
conversion for autostereoscopic displays. In Computer graphics forum, Vol. 33. Wiley
Online Library, 63â€“72.

Gaurav Chaurasia, Arthur Nieuwoudt, Alexandru-Eugen Ichim, Richard Szeliski, and
Alexander Sorkine-Hornung. 2020. Passthrough+ Real-time Stereoscopic View
Synthesis for Mobile Mixed Reality. Proceedings of the ACM on Computer Graphics
and Interactive Techniques 3, 1 (2020), 1â€“17.

Shenchang Eric Chen and Lance Williams. 1993. View interpolation for image synthesis.
In Proceedings of the 20th annual conference on Computer graphics and interactive
techniques. 279â€“288.

Piotr Didyk, Pitchaya Sitthi-Amorn, William Freeman, FrÃ©do Durand, and Wojciech
Matusik. 2013. Joint view expansion and filtering for automultiscopic 3D displays.
ACM Transactions on Graphics (TOG) 32, 6 (2013), 1â€“8.

John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan
Overbeck, Noah Snavely, and Richard Tucker. 2019. Deepview: View synthesis with
learned gradient descent. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2367â€“2376.

Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. 2021. Dynamic View
Synthesis from Dynamic Monocular Video. arXiv preprint arXiv:2105.06468 (2021).
Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien
Valentin. 2021. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of
the IEEE/CVF International Conference on Computer Vision. 14346â€“14355.

Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. The
lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and
interactive techniques. 43â€“54.

Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht,
Joshua M Susskind, and Qi Shan. 2022. Fast and Explicit Neural View Synthesis. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.
3791â€“3800.

Peter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes Kopf. 2017. Casual 3D

photography. ACM Transactions on Graphics (TOG) 36, 6 (2017), 1â€“15.

Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul
Debevec. 2021. Baking neural radiance fields for real-time view synthesis. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 5875â€“
5884.

Ajay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting nerf on a diet: Semantically
consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International
Conference on Computer Vision. 5885â€“5894.

Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. 2016. Learning-
based view synthesis for light field cameras. ACM Transactions on Graphics (TOG)
35, 6 (2016), 1â€“10.

Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge, Yangming
Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu, et al. 2020. One
shot 3d photography. ACM Transactions on Graphics (TOG) 39, 4 (2020), 76â€“1.
Brooke Krajancich, Petr Kellnhofer, and Gordon Wetzstein. 2020. Optimizing Depth
Perception in Virtual and Augmented Reality through Gaze-Contingent Stereo

Input LeftInput RightOutputInput LeftInput RightOutputNeuralPassthrough: Learned Real-Time View Synthesis for VR

SIGGRAPH â€™22 Conference Proceedings, August 7â€“11, 2022, Vancouver, BC, Canada

Rendering. ACM Trans. Graph. 39, 6, Article 269 (nov 2020), 10 pages.

interactive techniques. 231â€“242.

Marc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the 23rd

annual conference on Computer graphics and interactive techniques. 31â€“42.

Lahav Lipson, Zachary Teed, and Jia Deng. 2021. Raft-stereo: Multilevel recurrent field

transforms for stereo matching. arXiv preprint arXiv:2109.07547 (2021).

Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan
Catanzaro. 2018. Image inpainting for irregular holes using partial convolutions.
In Proceedings of the European Conference on Computer Vision (ECCV). 85â€“100.
Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.

Neural sparse voxel fields. arXiv preprint arXiv:2007.11571 (2020).

Andrew Maimone and Junren Wang. 2020. Holographic Optics for Thin and Light-
weight Virtual Reality. ACM Trans. Graph. 39, 4, Article 67 (jul 2020), 14 pages.
Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan
Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter
Lincoln, et al. 2018. LookinGood: enhancing performance capture with real-time
neural re-rendering. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1â€“14.
Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari,
Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local light field fusion:
Practical view synthesis with prescriptive sampling guidelines. ACM Transactions
on Graphics (TOG) 38, 4 (2019), 1â€“14.

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-
mamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields
for view synthesis. In European conference on computer vision. Springer, 405â€“421.
Simon Niklaus and Feng Liu. 2020. Softmax splatting for video frame interpolation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
5437â€“5446.

Oculus. 2016. asynchronous spacewarp. https://www.oculus.com/blog/introducing-

asw-2-point-0-better-accuracy-lower-latency/

Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021.
D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10318â€“10327.

Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF:
Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. arXiv preprint
arXiv:2103.13744 (2021).

Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. 1998. Layered depth
images. In Proceedings of the 25th annual conference on Computer graphics and

Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3d photography
using context-aware layered depth inpainting. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 8028â€“8038.

Rajiv Soundararajan and Alan C Bovik. 2012. Video quality assessment by reduced
reference spatio-temporal entropic differencing. IEEE Transactions on Circuits and
Systems for Video Technology 23, 4 (2012), 684â€“694.

Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren
Ng, and Noah Snavely. 2019. Pushing the boundaries of view extrapolation with
multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 175â€“184.

Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. 2004. Image
quality assessment: from error visibility to structural similarity. IEEE Transactions
on Image Processing 13, 4 (2004), 600â€“612.

Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. Synsin:
End-to-end view synthesis from a single image. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 7467â€“7477.

Lei Xiao, Anton Kaplanyan, Alexander Fix, Matthew Chapman, and Douglas Lanman.
2018. DeepFocus: learned image synthesis for computational displays. ACM
Transactions on Graphics (TOG) 37, 6 (2018), 1â€“13.

Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021a.
Plenoctrees for real-time rendering of neural radiance fields. arXiv preprint
arXiv:2103.14024 (2021).

Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021b. pixelnerf: Neural
radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 4578â€“4587.

Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. Nerf++: Analyzing
and improving neural radiance fields. arXiv preprint arXiv:2010.07492 (2020).
Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018.
Stereo magnification: Learning view synthesis using multiplane images. arXiv
preprint arXiv:1805.09817 (2018).

C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard
Szeliski. 2004. High-quality video view interpolation using a layered representation.
ACM transactions on graphics (TOG) 23, 3 (2004), 600â€“608.

