Variance Reduction in Deep Learning: More Momentum is All You Need.

Lionel Tondji 1 2 Sergii Kashubin 3 Moustapha Cisse 3

1
2
0
2

v
o
N
3
2

]

G
L
.
s
c
[

1
v
8
2
8
1
1
.
1
1
1
2
:
v
i
X
r
a

Abstract

Variance reduction (VR) techniques have con-
tributed signiﬁcantly to accelerating learning with
massive datasets in the smooth and strongly con-
vex setting (Schmidt et al., 2017; Johnson &
Zhang, 2013; Roux et al., 2012). However, such
techniques have not yet met the same success
in the realm of large-scale deep learning due to
various factors such as the use of data augmenta-
tion or regularization methods like dropout (De-
fazio & Bottou, 2019). This challenge has re-
cently motivated the design of novel variance re-
duction techniques tailored explicitly for deep
learning (Arnold et al., 2019; Ma & Yarats, 2018).
This work is an additional step in this direction.
In particular, we exploit the ubiquitous clustering
structure of rich datasets used in deep learning to
design a family of scalable variance reduced opti-
mization procedures by combining existing opti-
mizers (e.g., SGD+Momentum, Quasi Hyperbolic
Momentum, Implicit Gradient Transport) with a
multi-momentum strategy (Yuan et al., 2019). Our
proposal leads to faster convergence than vanilla
methods on standard benchmark datasets (e.g., CI-
FAR and ImageNet). It is robust to label noise and
amenable to distributed optimization. We provide
a parallel implementation in JAX.

1. Introduction

Given a data generating distribution P, a parameterized func-
tion fθ (e.g. a deep network) and a loss function (cid:96)(·), we
consider the traditional risk minimization problem represen-
tative of most settings in ML (Shalev-Shwartz & Ben-David,
2014):

θ∗ = arg min

θ

E(x∼P)(cid:96)(fθ(x))

(1)

The celebrated optimization algorithm for solving this prob-
lem in large-scale machine learning is Stochastic Gradient

1Institute of Analysis and Algebra, TU Braunschweig 2This
work was done during the AI Resideny at Google Brain 3Google
Research, Brain Team. Correspondence to: Lionel Tondji
<l.ngoupeyou-tondji@tu-braunschweig.de>.

Descent (SGD) (Robbins, 2007; Bottou et al., 2018). In fa-
vorable cases (e.g., smooth and strongly convex functions),
SGD converges to a good solution by iteratively applying the
following ﬁrst-order update rule: θt+1 ← θt − µtg(xt, θt)
where xt is an instance drawn from P, µt is the learning rate
and g(xt, θt) is the (approximate) gradient. SGD enjoys
several desirable properties. Indeed, It is fast: when one has
access to the actual gradient, its convergence rate is O(e−νt)
for some ν > 0. Also, it is memory efﬁcient, robust, and
amenable to distributed optimization (Bottou et al., 2018;
Nemirovski et al., 2009; Dean et al., 2012). However, we
generally do not know the data-generating distribution P;
consequently, we do not have access to the actual gradient.
Instead, we resort to empirical risk minimization and rely
on a stochastic approximation of the gradient for a given
xt and θt. Unfortunately, the gradient noise due to using
the approximate instead of the exact gradient slows down
the convergence speed (which becomes O(1/t) instead of
O(e−νt)) (Bottou et al., 2018). It also makes the algorithm
harder to tune. Variance Reduction (VR) techniques allow
us to mitigate the impact of using noisy gradients in stochas-
tic optimization (Roux et al., 2012; Defazio et al., 2014;
Johnson & Zhang, 2013; Mairal, 2013).

Variance reduction methods have been successfully applied
to convex learning problems (Roux et al., 2012; Defazio
et al., 2014; Shalev-Shwartz & Zhang, 2013; Johnson &
Zhang, 2013). However, this success does not readily trans-
late to large-scale non-convex settings such as deep learning
due to colossal memory requirements (Roux et al., 2012; De-
fazio et al., 2014; Shalev-Shwartz & Zhang, 2013) and the
use of normalization and data augmentation procedures (De-
fazio & Bottou, 2019). This has motivated the design of
novel VR strategies that are better suited for deep learn-
ing (Cutkosky & Orabona, 2019; Arnold et al., 2019; Ma &
Yarats, 2018). In the next section, we present in more detail
Implicit Gradient Transport (Arnold et al., 2019) and Quasi
Hyperbolic Momentum (Ma & Yarats, 2018), two instances
of successful application of VR to deep learning.

Large scale datasets used in deep learning problems come
with a rich clustering structure (Deng et al., 2009). For
example, the data can result from the aggregation of several
datasets collected by different individuals or organizations
(e.g., this is typical in the federated learning setting (Li et al.,
2019)). When there is an underlying clustering structure

 
 
 
 
 
 
Discover

in the data, the variance due to gradient noise decomposes
into an in-cluster variance and a between-cluster variance.
Yuan et al. (2019) exploit this insight to reduce the between
cluster variance in the convex setting by considering every
data point as a separate cluster. Unfortunately, such an
approach requires storing as many models as data points
and therefore has prohibitive memory requirements. Also,
it does not consider the use of mini-batches during training.
Consequently, it is not directly applicable to deep learning.
In this work, we make the following contributions:

• We leverage the idea of using multiple momentum
terms and introduce a family of variance reduced op-
timizers for deep learning. These new approaches
(termed Discover 1) improve upon widely used meth-
ods such as SGD with Momentum, IGT, and QHM.
They are theoretically well-motivated and can exploit
the clustering structures ubiquitous in deep learning.

• Using simple clustering structures, we empirically val-
idate that Discover optimizers lead to faster conver-
gence and sometimes (signiﬁcantly) improved gener-
alization on standard benchmark datasets. We provide
parallel implementations of our algorithms in JAX2.

In the next section, we provide a brief overview of variance
reduction methods with an emphasis on Implicit Gradient
Transport (Arnold et al., 2019) and Quasi Hyperbolic Mo-
mentum (Ma & Yarats, 2018), two VR optimizers speciﬁ-
cally designed for deep learning. We then discuss (section 3)
the clustering structure present in most deep learning prob-
lems and how we can leverage it to design scalable variance
reduction strategies. The experiments section demonstrates
our proposals’ effectiveness and provides several insights
regarding their behavior.

2. Variance Reduction in Deep Learning

We consider Variance Reduction (VR) in the realm of large-
scale deep learning (Goodfellow et al., 2016), i.e. when we
are in the presence of signiﬁcant amounts of (streaming)
data, and the parameterized function fθ is a massive deep
neural network. Most existing approaches do not naturally
scale to this setting. Indeed, dual methods for VR, such as
SDCA (Shalev-Shwartz & Zhang, 2013), have prohibitive
memory requirements due to the necessity of storing large
dual iterates. Primal methods akin to SAGA (Roux et al.,
2012) are also memory inefﬁcient because they need to store
past gradients. Other approaches like SARAH (Nguyen
et al., 2017) are computationally demanding; they entail a
full snapshot gradient evaluation at each step and two mini-
batch evaluations. While some recent approaches, such as

stochastic MISO (Bietti & Mairal, 2017), can handle inﬁ-
nite data in theory. However, their memory requirement
scales linearly with the number of examples. In addition
to these limitations, Defazzio & Bottou (Defazio & Bottou,
2019) have shown that several other factors such as data
augmentation (Krizhevsky et al., 2012), batch normaliza-
tion (Ioffe & Szegedy, 2015), or dropout (Srivastava et al.,
2014b) impede the success of variance reduction methods
in deep learning. Despite these negative results, some recent
approaches such as Quasi Hyperbolic Momentum (Ma &
Yarats, 2018) and Implicit Gradient Transport with tail av-
eraging (IGT) (Arnold et al., 2019) have shown promising
results in variance reduction in the context of large scale
deep learning. In the sequel, we present them in more detail.
We ﬁrst present the Momentum or Heavy Ball method since
the above approaches and our proposal rely on it.

The Momentum or Heavy Ball (Polyak, 1964; Sutskever
et al., 2013) method uses the following update rule:

vt = βvt−1 + (1 − β) · g(θt, xt)

θt+1 = θt − µvt

where vt is called the momentum buffer and βt balances the
buffer and the approximate gradient compute at iteration t.
SGD with Momentum is widely used in deep learning due
to its simplicity and its robustness to variations of hyper-
parameters (Sutskever et al., 2013; Zhang & Mitliagkas,
2017). It is an effective way of alleviating slow convergence
due to curvature (Goh, 2017) and has been recently shown
also perform variance reduction (Roux et al., 2018).

The Quasi Hyperbolic Momentum (QHM) (Ma & Yarats,
2018) method uses the following update rule:

vt = βvt−1 + (1 − β) · g(θt, xt)
θt+1 = θt − µ · [νvt + (1 − ν)g(θt, xt)]

QHM extends Momentum by using in the update a combina-
tion of the approximate gradient and the momentum buffer.
When ν = 1 (resp. ν = 0 ), it reduces to Momentum (resp.
SGD). QHM inherits the efﬁciency of Momentum and also
performs VR. In addition, it alleviates the potential staleness
of the momentum buffer (when choosing ν < 1).

The Implicit Gradient Transport (IGT) (Arnold et al.,
2019) method uses the following update rule:

γt = t/(t + 1)

vt = γt · vt−1 + (1 − γt) · g

θt +

(cid:18)

(cid:19)

(θt − θt−1), xt

γt
1 − γt

wt = β · wt−1 − µ · vt

θt+1 = θt + wt

1Deep SCalable Online Variance Reduction
2https://github.com/google/jax

IGT effectively achieves variance reduction by combining
several strategies maintaining a buffer of past gradients (us-

Discover

ing iterative tail averaging) and transporting them to equiva-
lent gradients at the current point. The resulting update rule
can also be combined with Momentum as shown above.

3. Exploiting Clusters in Variance Reduction

The Ubiquitous Clustering Structure Large-scale ma-
chine learning datasets come with a rich clustering structure
that arises in different ways depending on the setting. For
example, when training deep neural networks, we combine
various data augmentation strategies, resulting in a mixture
with clusters deﬁned by the transformations (Zhang et al.,
2017; Yun et al., 2019b; Krizhevsky et al., 2012). In multi-
class classiﬁcation problems, one can consider each class as
a separate cluster. Therefore, the overall dataset becomes a
mixture deﬁned by the classes. In all these cases, the data
is not independent and identically distributed across clus-
ters. Indeed, different data augmentation strategies lead to
different training data distributions. To make the clustering
structure apparent, we can rewrite the minimization prob-
lem in equation 1 as a combination of risks on the different
clusters. To this end, we denote Pn the data distribution cor-
responding to the n-th cluster and pn the probability with
which we sample from that cluster, with (cid:80)N
i=1 pn = 1. We
also denote xn
t the realization of the data point xt belonging
to the cluster n for clarity. The risk minimization problem 1
becomes:

θ∗ = arg min

θ

N
(cid:88)

n=1

pnE(x∼Pn)(cid:96)(fθ(x))

(2)

While one can pool all the data to apply traditional empir-
ical risk minimization 1, this would ignore valuable prior
clustering information. In the next section, we show how,
when solving the equivalent problem 2, we can leverage the
clustering information to speed the learning procedure by
reducing the variance due to gradient noise. We start with
stochastic gradient descent and present a decomposition of
such variance, which considers the clustering structure.

Gradient noise for SGD with clusters Here, we re-
derive the variance of the gradient noise for SGD in presence
of clustered data (Sayed, 2014b; Yuan et al., 2019). To this
end, we introduce the ﬁltration Ft = {θi<t+1} and denote
gn(θ) = E(xn∼Pn)g(θ, xn) for convenience. We assume the
loss (cid:96)(f (xn)) is δ-Lipschitz with respect to θ and the clus-
tered risk (cid:96)n(θ) = E(x∼Pn)(cid:96)(fθ(xn)) is ν-strongly convex,
that is for any θ1, θ2 it holds:

(gn(θ1) − gn(θ2))T (θ1 − θ2) ≥ ν(cid:107)θ1 − θ2(cid:107)2

For a given example xn
t , the update rule for stochastic gradi-
ent descent is θt+1 ← θt − µtg(xn
t , θt). The gradient noise
resulting from this update depends both on the probability
distribution on the clusters and the data distribution Pn for

the considered cluster. For a given example, the gradient
noise writes: st+1(θt) = g(xn
t , θt) − g(θt) and the gradient
noise within the cluster n is: sn
t+1(θt) = g(xn
t , θt) − gn(θt).
The following result bounds the ﬁrst and second moment of
the within-cluster variance of the gradient noise for SGD.

Lemma 1 The ﬁrst and second order moments of the gra-
dient noise st+1(θt) satisfy: E(cid:0)st+1(θt)|Ft
(cid:1) = 0 and
E(cid:0)(cid:107)st+1(θt)(cid:107)2|Ft
(cid:1) ≤ β2(cid:107)˜θt(cid:107)2 + σ2 where β2 = 2δ2,
˜θt = θt − θ∗ and σ2 = 2E(cid:0)(cid:107)g(xn
t , θ∗)(cid:107)2|Ft

(cid:1).

We can now decompose the variance of the gradient noise
into a sum of within-cluster and between-cluster variance.

Lemma 2 Assuming the gradient is unbiased and the vari-
ance of the gradient noise within the cluster n is bounded
as in shown in lemma 1, the following inequality holds:

E((cid:107)st+1(θ∗)(cid:107)2|Ft) ≤

N
(cid:88)

pnσ2
n

+

N
(cid:88)

pn(cid:107)gn(θ∗)(cid:107)2

n=1
(cid:125)
(cid:123)(cid:122)
(cid:124)
in-cluster variance

n=1
(cid:125)
(cid:123)(cid:122)
(cid:124)
between-cluster variance
(3)

Lemma 2 captures the structure of the problem. The LHS
represents the variance of the gradient noise. The ﬁrst
term of the RHS is the within-cluster variance σ2
in, and
the second term is the between-cluster variance σ2
bet. In
the limit, the mean square deviation of the steady-state de-
pends on the in-cluster and between-cluster variances as
follows lim supt→∞ (cid:107)θt − θ∗(cid:107) = O(µ(σ2
bet)). There-
fore when the clustering structure is known and σ2
in (cid:28)
σ2
in + σ2
bet, which is our working assumption, we can signif-
icantly reduce the overall variance by reducing the between-
cluster variance. In the next section, we present an update
rule exploiting this fact and generalizing the approach pre-
sented in (Yuan et al., 2019) to the minibatch setting3. We
also prove its gradient noise properties and convergence4

in + σ2

4. Discover Algorithms

To re-iterate, we assume we know the clustering structure
and the probability pn of observing data from a given cluster
n such that (cid:80)
n pn = 1. As we will show in the experiments,
this is a realistic assumption, and straightforward design
choices such as using labels or data augmentation strategies
as clusters lead to improved results. We do not have access
to the data distribution given a cluster n. We consider a

3It is worth noting that Yuan et al. (2019) have assumed the
results in lemma 1 and state lemma 2 without proof. We provide
full proofs in the appendix.

4The proof assumes a smooth and strongly convex setting.
Though we consider in the experiments non-convex problems, this
assumption may be valid locally in a basin of attraction of the loss
landscape of a deep neural network

Discover

learning setting where at each round t, we observe a batch
of examples Bt = {xn
t } coming from the different groups
and sampled according to the mixture distribution induced
by the clustering structure. We propose to achieve between
cluster variance reduction in minibatch stochastic gradient
descent by recursively applying the following update rule:

θt+1 = θt −

µ
|Bt|

·

xn

t ∈Bt

(cid:18)

(cid:88)

g(xn

t , θt) − g(n)

t +

(cid:19)

pkg(k)
t

N
(cid:88)

k=1

(4)
The update rule stems from the traditional use of control
variates for variance reduction (Fishman, 1996) with the ad-
ditional trick to use one control variate for each cluster given
that the clustering structure is known. In Equation (4), each
g(n)
is an approximation of the actual cluster gradient gn(θt)
t
k=1 pkg(k)
to which the example xn
is the average cluster gradient. The gradient noise resulting
from the update rule 4 depends both on the probability dis-
tribution on the clusters and the data distribution Pn of each
considered cluster. It can be written as follows:

t belongs, and ¯gt = (cid:80)N

t

ut+1(θt) =

1
|Bt|

·

(cid:18)

(cid:88)

xn

t ∈Bt

g(xn

t , θt) − g(n)

t + ¯gt

(cid:19)

− g(θt)

In the sequel, we show how a recursive application of the
above update rule ultimately leads to reduced between-
cluster variance of the gradient noise ut+1(θt). Before,
we ﬁrst state a lemma exposing the properties of the gra-
dient noise ut+1(θt). Similarly to Lemma 2, this results
highlights how the variance of the gradient noise decom-
poses into an in-cluster and a between-cluster variance. We
provide a proof of the lemma in the appendix Section 9.1.

Lemma 3 (gradient noise properties) Under the same
assumptions as in Section 3, for a batch of size |Bt| the
gradient is unbiased E[ut+1(θt)|Ft] = 0. Denoting
˜θt := θ∗ − θt, C1 = 4δ2 and σ2
t , θ∗)(cid:107)2)
the variance of the gradient noise is bounded as:

n = 2 · E((cid:107)g(xn

(cid:32)

(cid:33)

E

(cid:107)ut+1(θt)(cid:107)2|Ft

≤

1
|Bt|

· C1(cid:107) ˜θt(cid:107)2 +

1
|Bt|
(cid:124)

N
(cid:88)

·

pnσ2
n

n=1
(cid:123)(cid:122)
in-cluster variance

(cid:125)

+

2
|Bt|
(cid:124)

N
(cid:88)

·

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

n

(cid:123)(cid:122)
between-cluster variance

Algorithm 1 Discover

Initialization: ¯g0 = 0, α ∈ (0, pmin), g(n)
for t = 0, . . . , T − 1 do

0 = 0, αn = α/pn

Get the cluster indexes C in the current batch Bt and
update θt+1, {g(n)

n=1 and ¯gt+1:

θt+1 = θt − µ

g(xn

t , θt) − g(n)

t+1}N
|Bt| · (cid:80)

(cid:18)

xn

t ∈Bt

(cid:19)

t + ¯gt
(cid:46) in parallel

for n ∈ C do
Bn
t = {xk
t+1 = (1 − αn)g(n)
g(n)

t ∈ Bt | k = n}

t + αn
|Bn
t |

(cid:80)
t ∈Bn
xn
t

g(xn

t , θt)

end for
g(n)
t+1 = g(n)
¯gt+1 = ¯gt − α
|Bt|

t

for each n /∈ C
(cid:16)

(cid:80)

xn

t ∈Bt

g(n)
t − g(xn

t , θt)

(cid:17)

end for
return θT

that when we use the same approximate gradient buffer
gM for all the clusters, the update rule 4 reduces to that of
SGD with Momentum (Polyak, 1964; Goh, 2017). Con-
sequently, the between cluster variance term in the above
t − gn(θ∗)(cid:107)2 and may van-
bound becomes (2/|Bt|) · (cid:107)gM
ish as t → ∞. Therefore, Momentum also can perform
between-cluster variance reduction and can be seen as a
special case of the method proposed here, albeit operating at
a coarser level (maintaining one general approximate cluster
gradient buffer instead of one for each cluster). Momen-
tum has been mainly considered as a method for ﬁghting
curvature (Goh, 2017). Recent work has shown its variance
reduction capabilities in speciﬁc cases (Roux et al., 2018).
We show in our experiments that Momentum indeed per-
forms between-cluster variance reduction. We now show
that recursive applications of the update rule 4 indeed leads
to vanishing between-cluster variance. The full proof of
Theorem 1 is provided in the appendix Section 9.3.

Theorem 1 Under the same assumptions as in Section 3,
we denote |Bt| as the batch size and σ2
n=1 pnσ2
n
t , θ∗)(cid:107)2),
as the in-cluster variance with σ2
pmin = min{p1, . . . , pN }. For any step size satisfying
(cid:41)

in = (cid:80)N
n = 2 · E((cid:107)g(xn

(cid:40)

µ ≤ min

ν|Bt|
3δ2(|Bt|+5) , α

6ν

where α ∈ (0, pmin) and

γ = 3µ2
α|Bt| , the iterate θt from Discover 1 converge in
expectation to the solution θ∗ with the contraction factor
q = 1 − µν < 1 with G0 = (cid:80)N
n=1 pn(cid:107)gn(θ∗)(cid:107)2. it holds:

(cid:125)
(5)

E((cid:107)θt−θ∗(cid:107)2) ≤ qt

E((cid:107)˜θ0(cid:107)2)+γG0

+

(cid:32)

(cid:33)

4µ
ν|Bt−1|

σ2
in (6)

Remark The second and the last term of the right-hand
side of this bound are respectively the within-cluster and the
between-cluster variance. If the approximate cluster gradi-
ent converges to the actual one g(n)
t → gn(θ∗) as t → ∞,
the between-cluster variance vanishes. It is worth noting

lim sup
t→+∞

E((cid:107)θt+1 − θ∗(cid:107)2) = O(µ · σ2

in/|Bt|)

(7)

Theorem 1 shows that when the step size is small, Discover 1
eliminates the between-cluster variance in the limit, hence

Discover

reducing the overall variance of the gradient noise to the
in-cluster variance. Therefore, when the latter is signiﬁ-
cantly smaller than the between-cluster variance, Discover
can be an effective variance reduction strategy. We show
in the experiments section that when the clustering struc-
ture to exploit is carefully chosen, Discover leads to faster
convergence and sometimes results in improved general-
ization thanks to the cluster information. We now show
how to leverage the idea of using multiple momentum terms
based on a given clustering to improve Implicit Gradient
Transport (Arnold et al., 2019) and Quasi Hyperbolic Mo-
mentum (Ma & Yarats, 2018) (both relying on a single mo-
mentum in their vanilla version). The update rules for such
extensions (respectively called Discover-IGT and Discover-
QHM) follow. Our experiments will demonstrate the effec-
tiveness of these methods.

Discover-IGT (D-IGT) update rule

:

γt = t/(t + 1)

vt = γt · vt−1 + (1 − γt) · g

θt +

(cid:18)

(cid:19)

(θt − θt−1), xt

γt
1 − γt

(cid:40)

g(n)
t+1 =

θt+1 = θt −

(1 − αn)g(n)
g(n)
t
µ
|Bt|

(cid:88)

·

xn

t ∈Bt

t + αnvt

if n ∈ C,
otherwise
(cid:19)

(cid:18)

vt − g(n)

t + ¯gt

The Discover-QHM (D-QHM) method uses the following
update rule:

g(n)
t+1 =






(1 − αn)g(n)

t + αn
|Bn
t |

g(n)
t

(cid:80)
t ∈Bn
xn
t

g(xn

t , θt)

if n ∈ C,

otherwise

t+2 = νg(n)
g(n)

t+1 + (1 − ν) ·

1
|Bn
t |

(cid:88)

t ∈Bn
xn
t

g(xn

t , θt)

θt+1 = θt −

µ
|Bt|

·

(cid:88)

xn

t ∈Bt

(cid:18)

g(xn

t , θt) − g(n)

t+1 + ¯gt+1

(cid:19)

t

t = {xk

where Bn
t ∈ Bt | k = n} is the batch at iteration
t, C is the cluster indexes present in the batch Bt, and g(n)
are updated in parallel for n ∈ C. Discover-IGT combines
gradient transport and the multi-momentum strategy. D-
QHM extends QHM and Discover by using in the update a
combination of the approximate gradient and the momentum
buffer, together with a multi-momentum approach similar to
Discover. When ν = 1, and N = 1 it reduces to Momentum.
When N = 1, it reduces to QHM. When ν = 1, it reduces to
Discover. Choosing ν < 1 alleviates the potential staleness
of the buffer.

5. Experiments

We validate that the Discover algorithms coupled with a
careful clustering structure lead to faster convergence com-
pared to their vanilla counterparts (SGD+Momentum, IGT,
and QHM5). We provide additional insights and show that
the runtime is comparable to competing methods thanks to
a careful parallel implementation.

We implement all the methods used in our experiments
using JAX (Bradbury et al., 2018), and FLAX (Flax Devel-
opers, 2020) and perform our experiments on Google Cloud
TPU (Jouppi et al., 2017) devices. We compare Discover
with widely used minibatch SGD with Momentum, and
Adam (Kingma & Ba, 2014), as well as with the more re-
cently introduced IGT and QHM on ImageNet (Deng et al.,
2009) and CIFAR-10 (Krizhevsky et al., 2009) datasets. Dis-
cover is amenable to parallelization. For example, at each
round, we can update the cluster gradients in parallel, as
shown in Algorithm 1. Our implementation exploits this
feature and groups clusters by cores to enable parallel up-
dates. That is, examples of each core belong to the same
cluster. In this section we treat αn as an independent hyper-
parameter instead of using an exact equation αn = α/pn
to simplify the parallel implementation. We provide more
details about the practical implementation in appendix Sec-
tion 7.1. We conﬁrm that using the same strategy does not
make a practical difference for all the other optimizers we
consider. For each optimizer, we select the hyperparameters
either as suggested by its authors or by running a sweep to
obtain the highest accuracy at the end of training across ﬁve
random seeds. The details of the training setup, HP search,
and the best values found for each optimizer are given in
appendix Section 7.

ImageNet: Data augmentation methods as clusters
We ﬁrst consider an image classiﬁcation and train a ResNet-
v1-50 (He et al., 2016) model on the ImageNet (Deng et al.,
2009; Russakovsky et al., 2015) dataset for 31200 steps (90
epochs). We use cosine learning rate schedule with mini-
batches of a batch size of 4096, weight decay regularization
of 0.001, group normalization (Wu & He, 2018), and weight
standardization (Qiao et al., 2019). We use a standard pre-
processing pipeline consisting of cropping (Szegedy et al.,
2015) with size 224x224, pixel value scaling, and a random
horizontal ﬂipping.

We train our Resnet-50 using three data augmentation
methods: random ﬂipping, Mixup (Zhang et al., 2017),
and CutMix (Yun et al., 2019a). For each image, we ap-
ply all transformations separately, producing three differ-
ently augmented examples (as shown in Figure 5). Conse-
quently, each of the augmentation methods induces a differ-

5we restrict QHM to ν (cid:54)= 0 and ν (cid:54)= 1 so it is not reduced to

SGD or SGD+Momentum

Discover

ent cluster and the probability of each is 1/3. We compare
the multi-momentum strategies (Discover, Discover-IGT,
and Discover-QHM), exploiting this clustering information,
with their vanilla counterparts (SGD with Momentum, IGT,
and QHM). For the sake of completeness, we also add Adam
to the comparison as a baseline. Figure 1 shows the results
for the different methods. Using all these data augmenta-
tion methods can make the learning problem more difﬁcult
because the resulting cluster data distributions can differ sig-
niﬁcantly. In this setting, we observe that Discover variants
initially converge faster than their corresponding vanilla op-
timizers. Discover, and Discover-IGT reach similar ﬁnal
performance to SGD with Momentum and IGT, respectively,
while Discover-QHM outperforms the vanilla QHM.

CIFAR: Classes as clusters. Next, we consider the
CIFAR-10 (Krizhevsky et al., 2009) classiﬁcation and use
the classes as the clusters, therefore having ten different
clusters. We train a WideResNet26-10 (Zagoruyko & Ko-
modakis, 2016) on CIFAR-10 for 400 epochs using cosine
learning rate schedule, batch size of 256, group normal-
ization, L2-regularization of 5 × 10−4 and dropout rate of
0.3. The preprocessing consists of 4-pixel zero-padding, a
random crop of size 32x32, scaling the pixels to [0, 1] range
and random horizontal ﬂip.

Figure 2 shows the results for the different optimizers.
CIFAR-10 is an easy task and all the methods eventually con-
verge to high accuracy. IGT converges faster than Discover-
IGT and reaches higher ﬁnal accuracy (95.3% vs 94.2%).
Discover (resp. Discover-QHM) converge similarly to SGD
with Momentum (resp. QHM). These single momentum
strategies also reach a similar (for QHM, 95.1% vs 95.4%)
or higher (for Momentum, 95.3% vs 94.2%) ﬁnal perfor-
mance. To highlight the importance of carefully choosing
the clustering structure, we also performed an experiment
on CIFAR-10 where we assigned each point to 1 of 10 clus-
ters uniformly at random and train the model using discover
and such clustering. The resulting test accuracy is 91.09%
down from 94.2% when using classes as clusters. Therefore
it is essential to use a good clustering structure.

Table 1. Training performance (in steps/second) with different opti-
mizers. On ImageNet a single number is reported, while on CIFAR
- Mean and std deviation across 5 runs.

Optimizer

SGD
Adam
Momentum
IGT
QHM
Discover
Discover-QHM
Discover-IGT

ImageNet, CIFAR-10 (µ ± σ),
4 TPUv2
64 TPUv3

−.−
8.2
8.7
7.8
8.6
7.4
7.6
7.3

14.3 ± 3.8
9.8 ± 0.4
10.4 ± 0.7
9.3 ± 0.6
12.5 ± 2.7
6.3 ± 0.3
5.8 ± 0.02
5.6 ± 0.02

setting as in the previous ImageNet and CIFAR-10 exper-
iments. Still, We perform a hyperparameter (HP) search
de novo to determine the best learning for each method in
the presence of noisy labels. The details of the HP search,
and the best values found for each optimizer are given in
appendix Section 7.

When the label noise is low (p = 0.2), all the methods
achieve high accuracy on the non-corrupted training exam-
ples and the test examples with only a slight deterioration
compared to a clean setting p = 0 (exact results are given
in appendix Section 7.5). Figure 3 and Figure 4 show the
results of this experiment in the high noise level setting
(p = 0.8) on ImageNet and CIFAR-10, respectively. On
CIFAR-10, the multi-momentum optimizers’ loss curves
are almost superimposed with the curves of SGD with Mo-
mentum, QHM, and IGT. However, the former general-
ize signiﬁcantly better at each time step and reach higher
ﬁnal performance than all their single momentum coun-
terparts. It is worth noting that Discover outperforms all
the multi-momentum optimizers by a large margin on the
validation accuracy, achieving 85.9%. The superiority of
multi-momentum optimizers over the single momentum
ones translates to ImageNet, suggesting that our Discover
algorithms ﬁnd better local optima even in challenging set-
tings. The other interesting ﬁnding is that while Adam
converges faster in training loss, it generalizes poorly.

Learning with noisy labels We now consider the more
challenging ImageNet and CIFAR-10 classiﬁcation task in
the presence of label noise: At each round, the label of the
considered example is ﬂipped with probability p (the noise
level) and assigned to a different class selected uniformly at
random among the other classes. In this setting, the variance
of the gradient noise is increased due to the presence of la-
bel noise (see Appendix Section 8). We compare Discover,
Discover-QHM, and Discover-IGT with their vanilla coun-
terparts (SGD with Momentum, QHM, IGT) and Adam. We
use Data augmentation methods as clusters for ImageNet
and classes as clusters for CIFAR-10. We use the same

Impact of clustering structure. We also performed an
experiment on CIFAR-10 where we compare three cluster-
ing structures: (1) Classes as clusters: This is the setting we
have considered so far for CIFAR-10. (2) Transformations
as clusters. (3) Random clusters: assigning each point to 1
of 10 clusters uniformly at random. In each case, we train
the model using Discover and the chosen clustering. When
the clusters are selected uniformly at random, the result-
ing test accuracy is 91.09%. When we use transformation
as clusters instead, the test accuracy improves to 93.6%.
The best accuracy with Discover on CIFAR-10 (94.2%) is
achieved when using classes as clusters. However, using this

Discover

(a)

(b)

(c)

Figure 1. Results of training ResNet-v1-50 model on ImageNet dataset: train loss (a), validation accuracy (b) and validation accuracy on
the last step in % (c). Discover consistently stays on par with vanilla optimizers (IGT, Momentum) or outperforms them (QHM) in the
end of the training while always converging faster in the beginning.

(a)

(c)

(b)

Figure 2. Results of training WideResNet26-10 model on CIFAR-10 dataset (classes as clusters): train loss (a), validation accuracy (b) and
validation accuracy on the last step in % (c). For each step a mean value across 5 random seeds is plotted, black whiskers in (c) indicate
standard deviation. Discover variants for IGT and Momentum are slightly worse than vanilla optimizers, while being on par for QHM.

clustering structure comes at the cost of a higher memory.

previous experiments.

Between-Cluster Variance Reduction In this experi-
ment, we assess the between-cluster variance reduction ca-
pabilities of Discover compared to SGD and SGD with
Momentum on CIFAR-10 (with classes as clusters) both in
the clean (p = 0) and noisy (p = 0.8) settings. We measure
at each step of the optimization an approximation of the
between-cluster variance obtained by substituting gn(θt+1)
to gn(θ∗) in the between-cluster variance formula. For SGD
we use Equation (3). For Discover and SGD with Momen-
tum – Equation (5). However, for SGD with Momentum,
one global gradient buffer is used for all the clusters. Equa-
tion (5) shows that in both the clean and the noisy setting,
Discover reduces the between-cluster variance faster. The
ﬁgure also highlights the between-cluster variance reduction
capabilities of SGD with Momentum, which, though not as
fast as Discover, also leads to quickly vanishing variance.
This explains the good performance of Momentum in our

6. Conclusion and Perspectives
We introduced a set of scalable VR algorithms exploiting
the ubiquitous clustering structure in the data and relying on
a multi-momentum strategy. We demonstrated that simple
choices of clustering structure (e.g., transformations) can
lead to improved results. Our framework gives the designer
signiﬁcant ﬂexibility allowing them to leverage prior infor-
mation to select good clustering structure. The experiments
demonstrated that the proposed strategy often leads to faster
convergence and sometimes to improved generalization, es-
pecially in challenging settings (e.g., label noise).

Table 1 shows there are only minor differences between
the multi-momentum strategies and their vanilla counter-
parts thanks to our efﬁcient implementation that exploits the
parallel structure of the algorithms.

Discover

(a)

(b)

(c)

Figure 3. Results of training ResNet-v1-50 model on ImageNet dataset in a high noise setting p = 0.8: train loss (a), validation accuracy
(b) and validation accuracy on the last step in % (c). Discover variants outperforms all vanilla optimizers by a large margin.

(a)

(b)

(c)

Figure 4. Results of training WideResNet26-10 model on CIFAR-10 dataset (classes as clusters) in a high noise setting p = 0.8: train loss
(a), validation accuracy (b) and validation accuracy on the last step in % (c). For each step a mean value across 5 random seeds is plotted,
black whiskers in (c) indicate standard deviation. Discover modiﬁcations outperform all vanilla optimizers by a large margin, once again
suggesting high noise robustness. Adam achieves only 19.8% mean (27.7% max) ﬁnal accuracy and thus not shown on (c) for clarity.

(a)

(b)

(c)

Figure 5. Between-cluster variance estimate (mean per 100 train steps) for CIFAR-10 with clean labels (a) and noisy labels (b) with
p = 0.8. Example augmentations (c) used as clusters for ImageNet experiments.

Discover

References

TensorFlow Datasets, a collection of ready-to-use datasets.
https://www.tensorflow.org/datasets.

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-
enberg, J., Mané, D., Monga, R., Moore, S., Murray, D.,
Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M.,
Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015.
URL https://www.tensorflow.org/. Software
available from tensorﬂow.org.

Arnold, S., Manzagol, P.-A., Harikandeh, R. B., Mitliagkas,
I., and Le Roux, N. Reducing the variance in online opti-
mization by transporting past gradients. In Advances in
Neural Information Processing Systems, pp. 5392–5403,
2019.

Bietti, A. and Mairal, J. Stochastic optimization with vari-
ance reduction for inﬁnite datasets with ﬁnite sum struc-
ture. In Advances in Neural Information Processing Sys-
tems, pp. 1623–1633, 2017.

Bottou, L., Curtis, F. E., and Nocedal, J. Optimization
methods for large-scale machine learning. Siam Review,
60(2):223–311, 2018.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., and Wanderman-Milne, S. JAX: com-
posable transformations of Python+NumPy programs,
2018. URL http://github.com/google/jax.

Cutkosky, A. and Orabona, F. Momentum-based variance
In Advances in Neural
reduction in non-convex sgd.
Information Processing Systems, pp. 15236–15245, 2019.

Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,
Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K.,
et al. Large scale distributed deep networks. In Advances
in neural information processing systems, pp. 1223–1231,
2012.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248–255. Ieee, 2009.

Fishman, G. S. Monte Carlo: Concepts, Algorithms and
Applications. Springer Verlag, New York, NY, USA,
1996.

Flax Developers.

Flax: A neural network library
URL

jax designed for ﬂexibility, 2020.
for
https://github.com/google-research/
flax/tree/prerelease.

Goh, G. Why momentum really works. Distill, 2(4):e6,

2017.

Goodfellow, I., Bengio, Y., and Courville, A. Deep
http://www.

MIT Press, 2016.

Learning.
deeplearningbook.org.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
June 2016.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in neural information processing systems, pp. 315–323,
2013.

Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,
G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,
A., et al. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th Annual Inter-
national Symposium on Computer Architecture, pp. 1–12,
2017.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers

of features from tiny images. 2009.

Defazio, A. and Bottou, L. On the ineffectiveness of vari-
In Ad-
ance reduced optimization for deep learning.
vances in Neural Information Processing Systems, pp.
1753–1763, 2019.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems,
pp. 1097–1105, 2012.

Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A
fast incremental gradient method with support for non-
strongly convex composite objectives. In Advances in
neural information processing systems, pp. 1646–1654,
2014.

Krogh, A. and Hertz, J. A. A simple weight decay can im-
prove generalization. In Moody, J. E., Hanson, S. J., and
Lippmann, R. P. (eds.), Advances in Neural Information
Processing Systems 4, pp. 950–957. Morgan-Kaufmann,
1992. URL http://papers.nips.cc/paper/

563-a-simple-weight-decay-can-improve-generalization.
pdf.

Sayed, A. H. Adaptation, learning, and optimization over
networks. Foundations and Trends in Machine Learning,
7(ARTICLE):311–801, 2014a.

Discover

Li, T., Sahu, A. K., Talwalkar, A., and Smith, V. Feder-
ated learning: Challenges, methods, and future directions.
arXiv preprint arXiv:1908.07873, 2019.

Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
In International Conference on Learning
ularization.
Representations, 2019. URL https://openreview.
net/forum?id=Bkg6RiCqY7.

Ma, J. and Yarats, D. Quasi-hyperbolic momentum and
adam for deep learning. arXiv preprint arXiv:1810.06801,
2018.

Mairal, J. Optimization with ﬁrst-order surrogate functions.
In International Conference on Machine Learning, pp.
783–791, 2013.

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Ro-
bust stochastic approximation approach to stochastic pro-
gramming. SIAM Journal on optimization, 19(4):1574–
1609, 2009.

Nguyen, L. M., Liu, J., Scheinberg, K., and Takáˇc, M. Sarah:
A novel method for machine learning problems using
stochastic recursive gradient. In Proceedings of the 34th
International Conference on Machine Learning-Volume
70, pp. 2613–2621. JMLR. org, 2017.

Polyak, B. T. Some methods of speeding up the convergence
of iteration methods. USSR Computational Mathematics
and Mathematical Physics, 4(5):1–17, 1964.

Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Weight
standardization. arXiv preprint arXiv:1903.10520, 2019.

Robbins, H. E. A stochastic approximation method. Annals

of Mathematical Statistics, 22:400–407, 2007.

Roux, N. L., Schmidt, M., and Bach, F. R. A stochastic
gradient method with an exponential convergence _rate
for ﬁnite training sets. In Advances in neural information
processing systems, pp. 2663–2671, 2012.

Roux, N. L., Babanezhad, R., and Manzagol, P.-A. Online
variance-reducing optimization, 2018. URL https://
openreview.net/forum?id=r1qKBtJvG.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015. doi:
10.1007/s11263-015-0816-y.

Sayed, A. H. Adaptive networks. Proceedings of the IEEE,

102(4):460–497, 2014b.

Schmidt, M., Le Roux, N., and Bach, F. Minimizing ﬁnite
sums with the stochastic average gradient. Mathematical
Programming, 162(1-2):83–112, 2017.

Shalev-Shwartz, S. and Ben-David, S. Understanding ma-
chine learning: From theory to algorithms. Cambridge
university press, 2014.

Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordinate
ascent methods for regularized loss minimization. Jour-
nal of Machine Learning Research, 14(Feb):567–599,
2013.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res.,
15(1):1929–1958, January 2014a. ISSN 1532-4435.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014b.

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the
importance of initialization and momentum in deep learn-
ing. In International conference on machine learning, pp.
1139–1147, 2013.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
In Computer
A. Going deeper with convolutions.
Vision and Pattern Recognition (CVPR), 2015. URL
http://arxiv.org/abs/1409.4842.

Wu, Y. and He, K. Group normalization.

In The Euro-
pean Conference on Computer Vision (ECCV), September
2018.

Yuan, K., Ying, B., and Sayed, A. H. Cover: A cluster-based
variance reduced method for online learning. In ICASSP
2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 3102–3106.
IEEE, 2019.

Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y.
Cutmix: Regularization strategy to train strong classiﬁers
In The IEEE International
with localizable features.
Conference on Computer Vision (ICCV), October 2019a.

Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y.
Cutmix: Regularization strategy to train strong classiﬁers

Discover

with localizable features. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 6023–
6032, 2019b.

Zagoruyko, S. and Komodakis, N. Wide residual networks.

arXiv preprint arXiv:1605.07146, 2016.

Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz,
D. Mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412, 2017.

Zhang, J. and Mitliagkas, I. Yellowﬁn and the art of mo-
mentum tuning. arXiv preprint arXiv:1706.03471, 2017.

7. Appendix A: Experimental details

7.3. ImageNet (augmentations as clusters)

Discover

7.1. Implementation

All models, training and evaluation pipelines are imple-
mented using JAX (Bradbury et al., 2018) and FLAX (Flax
Developers, 2020). The data loading and preprocessing is
implemented in Tensorﬂow v2 (Abadi et al., 2015) and
TFDS (TFD). The SGD, Momentum and Adam optimizers
are taken from FLAX libraries, QHM is implemented
following (Ma & Yarats, 2018) and IGT is written in
FLAX following an example at https://github.
com/google-research/google-research/
tree/master/igt_optimizer (Arnold et al., 2019).
Discover optimizers are implemented with single-program
multiple-data training in mind. Our implementation expects
a training data processed during a single training step by
a given device core to contain examples from the same
cluster. After shufﬂing the dataset, our input pipeline
selects the next batch_size/num_devices examples
from the same randomly picked cluster to satisfy this
condition. This translates into a single cluster per batch on
one-host one-core training (e.g. a workstation with one
CPU), but easily scales to more clusters by using multiple
cores even within a single-host setting. For examples, the
CIFAR experiments use one host machine with 4 Google
Cloud TPUv2 (Jouppi et al., 2017) devices, thus having 8
cores in total, so each batch can contain examples from
up to 8 different clusters. Discover optimizer averages
g∆,t = g(xn
across devices to perform identical
¯gt updates locally on each device and performs local g(n)
updates in parallel, synchronizing across devices at the
end of each training step (see Algorithm 1 for notation).
This averaging makes effective αn value depend on the
distributed training setup and to account for it we chose
to tune αn as a direct hyperparameter instead of trying
to incorporate the training setup into the formula from
Algorithm 1.This implementation is well suited to a good
balance of clusters within a single global batch, which is the
usual case for a small number of clusters and a randomly
shufﬂed dataset. We have not tested the performance when
the cluster distribution in a global batch is signiﬁcantly
skewed.

t , θt) − g(n)

t

t

7.2. Datasets

(Krizhevsky et al., 2009).

The datasets used in this work are ImageNet-1000
(Deng et al., 2009; Russakovsky et al., 2015) and
CIFAR-10
The datasets
were downloaded from https://www.tensorflow.
org/datasets/catalog/imagenet2012
and
https://www.tensorflow.org/datasets/
catalog/cifar10 respectively. The detailed informa-
tion about the size and format of the train and validation
splits are available at the corresponding web pages.

We trained a ResNet-v1-50 (He et al., 2016) model on the
ImageNet (Deng et al., 2009; Russakovsky et al., 2015)
dataset for 31200 steps, corresponding to 100 epochs on the
unaugmented dataset. We used:

• Cosine learning rate schedule with 5 warmup steps.
• Batch size of 4096.
• Weight decay regularization setting of λ = 0.001. We
used decoupled weight decay inspired by (Loshchilov
& Hutter, 2019) and each step update all network pa-
rameters by Θt+1 = Θt ∗ (1 − λµ) where µ is the
learning rate.

• Group normalization with 32 groups (Wu & He, 2018).
• Weight standardization.

The preprocessing consists of Inception-style cropping
(Szegedy et al., 2015) with size 224x224, scaling the pixel
values to [−1; 1] range and random horizontal ﬂip. In this
setting the model achieves 0.76 accuracy with Momen-
tum (Polyak, 1964; Sutskever et al., 2013) optimizer when
µ = 0.1, β = 0.9. We extend the setting to use Mixup
(Zhang et al., 2017) and Cutmix (Yun et al., 2019a) aug-
mentations instead of random ﬂipping, choosing the same
mixing ratio for all examples in the single local batch of
each distributed training host. We correspondingly assign
all examples augmented with random horizontal ﬂip to clus-
ter 0, with Mixup - to cluster 1, Cutmix - to cluster 2. The
clusters are used for Discover optimizer.

The hyperparameters are selected from a hyperparameter
sweep to obtain the highest mean accuracy at 31200 steps
across 5 random seeds. The learning rate µ is chosen from
a set of {0.03, 0.1, 0.3} for all optimizers. The further
hyperparameters are selected from the following sets:

• Discover: α, αn ∈ {0.001, 0.01, 0.1, 0.9}
• Momentum: β ∈ {0.85, 0.9, 0.95, 0.99}
• QHM: β = 0.999, γ = 0.7 (as reported default for

ImageNet in (Ma & Yarats, 2018))

• IGT: β = 0.9, tail_f raction = 90 (as reported to be

selected for ImageNet in (Arnold et al., 2019))

• Adam: µ = 0.001, β1 = 0.9, β2 = 0.999
• Discover-QHM: α ∈ {0.1, 0.6, 0.7, 0.8, 0.9, 0.99},

αn ∈ {0.01, 0.1, 0.9}, β ∈ {0, 1, 0.6, 0.7, 0.8}

• Discover-IGT: α ∈ {0.1, 0.6, 0.7, 0.8, 0.9, 0.99},
∈

{0.01, 0.1, 0.9},

tail_fraction

∈

αn
{18, 45, 50, 60, 90, 180, 360}

The exact hyperparameters selected for each optimizer from
the hyperparameters sweep are:

Discover

• Discover: µ = 0.1, α = 0.1, αn = 0.01
• Momentum: µ = 0.1, β = 0.9
• QHM: µ = 0.1
• IGT: µ = 0.1
• Adam: µ = 0.001, β1 = 0.9, β2 = 0.999
• Discover-QHM: µ = 0.1, α = 0.9, αn = 0.1, γ = 0.9
• Discover-IGT: µ = 0.1, α = 0.9, αn = 0.1,

tail_f raction = 180

The training is setup in the multi-device data-parallel
fashion on a pod of 8x8 Google Cloud TPUv3 (Jouppi
et al., 2017). There are 16 host machines, each hav-
(Deng
ing access to 4 TPUv3 devices. The ImageNet
et al., 2009; Russakovsky et al., 2015) training and valida-
tion splits obtained from https://www.tensorflow.
org/datasets/catalog/imagenet2012 are di-
vided into equal parts and each host has access to its own
separate part. At most num_hosts = 16 examples are ex-
cluded this way to ensure the equal division. The training
examples inside each batch are arranged such that each de-
vice core receives only examples from a single cluster (see
Section 7.1). We conﬁrmed that this does not make a prac-
tical difference for any other optimizer examined: the loss
and accuracy curves look identical and the ﬁnal accuracy is
the same independent of whether this technique is applied
or not.

7.4. CIFAR (classes as clusters)

We trained a WideResNet26-10 (Zagoruyko & Komodakis,
2016) model on the CIFAR10 dataset (Krizhevsky et al.,
2009) for 400 epochs. We used:

• Cosine learning rate schedule with 5 warmup steps.
• Batch size of 256.
• L2-regularization (Krogh & Hertz, 1992) set to 0.0005.
• Dropout (Srivastava et al., 2014a) rate of 0.3.
• Group normalization (Wu & He, 2018) with 16 groups
in the ﬁrst layer in each block and 32 groups in the rest
of the layers.

The preprocessing consists of 4 pixel zero-padding followed
by a random crop of size 32x32, scaling the pixels to [0; 1]
range and random horizontal ﬂip. We correspondingly as-
sign all examples with the same class to the same cluster
(for Discover optimizer).

The hyperparameters are selected from a hyperparameter
sweep to obtain the highest mean accuracy at 400 epochs
across 5 runs with different random seeds. The learning
rate µ is chosen from a set of {0.001, 0.01, 0.03, 0.1, 0.175}
for all optimizers. The further hyperparameters are selected
from the following sets:

• Discover: α, αn ∈ {0.001, 0.01, 0.015, 0.02, . . .

0.095, 0.1, 0.15, 0.2, . . . 0.95, 1.0}

• Momentum: β = 0.9 (as used for WideResNet on

CIFAR-10 in (Zagoruyko & Komodakis, 2016))
• QHM: β ∈ {0.8, 0.9}, γ ∈ {0.1, 0.2, . . . , 0.8, 0.9}
• IGT: β = 0.9, tail_f raction = 18 (as determined to

be best for CIFAR-10 in (Arnold et al., 2019))

• Adam: β1 = 0.9, β1 = 0.999, (cid:15) = 1e − 8 (as a good
default settings reported in (Kingma & Ba, 2014))
• Discover-QHM: α ∈ {0.6, 0.7, 0.8, 0.9}, αn ∈

{0.01, 0.1, 0.9}, β ∈ {0.6, 0.7, 0.8, 0.9, 0.99}
• Discover-IGT: α ∈ {0.095, 0.1, 0.6, 0.7, 0.8, 0.9,

0.99}, αn ∈ {0.01, 0.1, 0.9}, tail_fraction ∈ {18, 45,
50, 60, 90, 180, 360}

The exact hyperparameters selected for each optimizer from
the hyperparameters sweep are:

• SGD: µ = 0.01
• Discover: µ = 0.01, α = 0.095, αn = 0.1
• Momentum: µ = 0.03, β = 0.9
• QHM: µ = 0.1, γ = 0.9, β = 0.9
• IGT: µ = 0.01, β = 0.9, tail_f raction = 18
• Adam: µ = 0.001
• Discover-QHM: µ = 0.01, α = 0.9, αn = 0.1, β =

0.9

• Discover-IGT: µ = 0.01, α = 0.095, αn = 0.1,

tail_f raction = 18

The training setup is the same as for ImageNet experiments
(see appendix Section 7.3) but using only one host machine
with 4 Google cloud TPUv2 (Jouppi et al., 2017). The
CIFAR-10 (Krizhevsky et al., 2009) training and test splits
are obtained from https://www.tensorflow.org/
datasets/catalog/cifar10.

7.5. CIFAR (nosiy labels)

We also run the above-mentioned CIFAR-10 (Krizhevsky
et al., 2009) experiments with partially corrupted labels,
where the label of each image is ﬂipped to a random differ-
ent class independently with probability p every time the
example is seen during training. E.g. the same example
might get different labels in different training epochs. We
have tuned the hyperparameters anew in a same way as the
clean CIFAR-10 experiment above Section 7.4 each value
of p ∈ {0.2, 0.8}. The results for the low-noise setting
(p = 0.2) and high-noise setting (p = 0.8) are reported in
Figure 6 and Figure 4 respectively.

The exact hyperparameters selected for p = 0.2 setting are:

• SGD: µ = 0.175

(a)

(b)

(c)

Discover

Figure 6. Results of training WideResNet26-10 model on CIFAR-10 dataset (classes as clusters) in a low noise setting p = 0.2: train loss
(a), validation accuracy (b) and validation accuracy on the last step in % (c). For each step a mean value across 5 random seeds is plotted,
black whiskers in (c) indicate standard deviation.

7.7. CIFAR (augmentations as clusters)

We also run the above-mentioned CIFAR-10 experiments
when using Mixup, Cutmix and left-right ﬂipping aug-
mented examples as clusters to mirror the setting used for
ImageNet experiments above. We did not tune any hyper-
parameters anew, instead for both clean and noisy labels
setting we have used the same hyperparameters as for the ex-
periments above (when classes were used as clusters). When
using clean labels the results for Discover modiﬁcations
shown on Figure 7 were very similar to those of vanilla opti-
mizers as expected. When using noisy labels (with p = 0.8)
Discover modiﬁcations performed signiﬁcantly worse than
vanilla optimizers as shown on Figure 8. These results
highlight the importance of the careful clustering structure
choice and suggest that Discover hyperparameters chosen
for one clustering structure does not necessarily transfer to
a different clustering choice in case of noisy labels.

• Discover: µ = 0.01, α = 0.095, αn = 0.1
• Momentum: µ = 0.1, β = 0.9
• QHM: µ = 0.175, γ = 0.9, β = 0.9
• IGT: µ = 0.1, β = 0.9, tail_f raction = 18
• Adam: µ = 0.001
• Discover-QHM: µ = 0.1, α = 0.9, αn = 0.1, γ = 0.7
• Discover-IGT: µ = 0.1, α = 0.095, αn =

0.1, tail_f raction = 18

The exact hyperparameters selected for p = 0.8 setting are:

• SGD: µ = 0.1
• Discover: µ = 0.01, α = 0.095, αn = 0.1
• Momentum: µ = 0.01, β = 0.9
• QHM: µ = 0.1, γ = 0.6, β = 0.9
• IGT: µ = 0.01, β = 0.9, tail_f raction = 18
• Adam: µ = 0.001
• Discover-QHM: µ = 0.1, α = 0.6, αn = 0.1, γ = 0.6
• Discover-IGT: µ = 0.1, α = 0.095, αn =

0.1, tail_f raction = 18

7.6. ImageNet (nosiy labels)

We also run the above-mentioned ImageNet (Deng et al.,
2009; Russakovsky et al., 2015) experiments with partially
corrupted labels, where the label of each image is ﬂipped to
a random different class independently with probability p =
0.8 every time the example is seen during training. E.g. the
same example might get different labels in different training
epochs. We used the same hyperparameters as selected for
the clean ImageNet experiment, except the learning rate
which was selected again for each optimizer value of p as
described in Section 7.3. The best values were exactly the
values chosen in Section 7.3.

(a)

(b)

(c)

Discover

Figure 7. Results of training WideResNet26-10 model on CIFAR-10 dataset (Mixup, Cutmix and left-right ﬂipping augmented examples
as clusters) in a clean labels setting: train loss (a), validation accuracy (b) and validation accuracy on the last step in % (c).

(a)

(b)

(c)

Figure 8. Results of training WideResNet26-10 model on CIFAR-10 dataset (Mixup, Cutmix and left-right ﬂipping augmented examples
as clusters) in a high noise (p = 0.8) setting: train loss (a), validation accuracy (b) and validation accuracy on the last step in % (c).

8. Variance due to label noise

(cid:96)(fθ(x))

:=
Suppose that we optimize the loss
(cid:96)(θ; x, fθ(x)) with gradient equal to g(x, θ). Let denote
by ˜g(x, θ) the gradient of the loss (cid:96)(Fθ(x)) where F (x)
is the true labeling function. On an instance x, we can
write it as: (cid:96)(fθ(x)) = a + b , with a = (cid:96)(Fθ(x)) and
b = (cid:96)(fθ(x)) − (cid:96)(Fθ(x)) .

E(cid:107)g(x, θ)(cid:107)2 = E(cid:107)˜g(x, θ) + g(x, θ) − ˜g(x, θ)(cid:107)2

≤ 2E(cid:107)˜g(x, θ)(cid:107)2 + 2E(cid:107)g(x, θ) − ˜g(x, θ)(cid:107)2

The ﬁrst term is the traditional variance of the gradient noise,
and the second term is due to the label noise the larger the
noise the larger it is.

9. Appendix B: Proofs

Proof 1 Before proving, we denote E[.] = En,t[.|xn
t ] and
Et = E[.|Ft] where the superscript n indicate the cluster
index x arises from.

In this part we want to give a proof of Lemma 1. We want to
show that the ﬁrst and second order moments of the gradient
noise st+1(θt) satisfy :

(cid:32)

(cid:33)

E

st+1(θt)|Ft

= 0

(cid:32)

(cid:33)

E

(cid:107)st+1(θt)(cid:107)2|Ft

≤ β2(cid:107)˜θt(cid:107)2 + σ2

(8)

(9)

where ˜θt = θt − θ∗, β2 = 2δ2 and :

(cid:32)

(cid:33)

σ2 = 2E

(cid:107)g(xn

t , θ∗)(cid:107)2

= 2

N
(cid:88)

n=1

(cid:32)

(cid:33)

pnEt

(cid:107)g(xn

t , θ∗)(cid:107)2

where σ2 is referred to as the magnitude of gradient noise.
In case all the clusters have the same probability that is

Discover

We recall that the gradient noise within the cluster n is given
by:

t+1(θt) = g(xn
sn

t , θt) − gn(θt)

Assuming the gradient noise within the cluster n is fol-
lowing assumptions similar to the result in Lemma 1
meaning that it is unbiased and it variance is bounded :
n, where ˜θt = θ∗ − θt,
n(cid:107)˜θt(cid:107)2 + σ2
E((cid:107)sn
t+1(θt)(cid:107)2|Ft) ≤ γ2
and γn, σn > 0, where σ2
n is referred to as the magnitude of
gradient noise in cluster n. we want to prove Lemma 2 :

E((cid:107)st+1(θ∗)(cid:107)2|Ft) ≤

N
(cid:88)

pnσ2
n

+

N
(cid:88)

pn(cid:107)gn(θ∗)(cid:107)2

n=1
(cid:125)
(cid:123)(cid:122)
(cid:124)
in-cluster variance

n=1
(cid:124)

(cid:123)(cid:122)
between-cluster variance

(cid:125)
(10)

p1 = p2, ..., pN = 1

N , we have that:

(cid:32)

(cid:33)

σ2 = 2E

(cid:107)g(xn

t , θ∗)(cid:107)2

= 2

N
(cid:88)

n=1
(cid:32)

(cid:32)

(cid:33)

pnEt

(cid:107)g(xn

t , θ∗)(cid:107)2

(cid:33)

= 2Et

(cid:107)g(xt, θ∗)(cid:107)2

which correspond to the magnitude of gradient noise without
considering the clustering structure.
we recall that the gradient noise for SGD is given by :
st+1(θt) = g(xn

t , θt) − g(θt).

(cid:32)

(cid:33)

(cid:32)

(cid:33)

E

st+1(θt)|Ft

= En,t

g(xn

t , θt)|Ft

− g(θt)

pngn(θt) − g(θt)

=

N
(cid:88)

n=1

= 0

Where we use the fact that :

(cid:32)

(cid:33)

gn(θt) = Et

g(xn

t , θt)|Ft

g(θt) = En(gn(θt))

Using the following Jensen’s inequality :

E((cid:107)a + b(cid:107)2) = 4E((cid:107)

1
2

a +

1
2

b(cid:107)2)

≤ 2E((cid:107)a(cid:107)2) + 2E((cid:107)b(cid:107)2)

It holds that :
(cid:32)

(cid:33)

(cid:32)

(cid:33)

E

(cid:107)st+1(θt)(cid:107)2|Ft

= E

(cid:107)g(xn

t , θt) − g(θt)(cid:107)2|Ft

(cid:32)

(cid:33)

≤ 2E

(cid:32)

(cid:33)

+2E

(cid:107)g(xn

t , θ∗)(cid:107)2|Ft

(cid:107)g(xn

t , θt) − g(θt) − g(xn

t , θ∗)(cid:107)2|Ft

Using the fact that for any random variable x,

E(cid:107)x − E[x](cid:107)2 = E(cid:107)x(cid:107)2 − (cid:107)E[x](cid:107)2 ≤ E(cid:107)x(cid:107)2

and g(θ∗) = 0, we have :

(cid:32)

(cid:33)

(cid:32)

(cid:33)

E

(cid:107)g(xn

t , θt) − g(θt) − g(xn

t , θ∗)(cid:107)2|Ft

≤ E

(cid:107)g(xn

t , θt) − g(xn

t , θ∗)(cid:107)2|Ft

≤ δ2(cid:107)˜θt(cid:107)2

Proof 2

From (Yuan et al., 2019) it holds that :

Discover

E((cid:107)st+1(θ∗)(cid:107)2|Ft) = E((cid:107)g(xn
= E((cid:107)g(xn
= E((cid:107)sn
= E((cid:107)sn
N
(cid:88)

t , θ∗)(cid:107)2|Ft)
t , θt) − gn(θ∗) + gn(θ∗)(cid:107)2|Ft)

t+1(θ∗) + gn(θ∗)(cid:107)2|Ft)
t+1(θ∗)(cid:107)2|Ft) + E((cid:107)gn(θ∗)(cid:107)2|Ft)

≤

pn(γ2

n(cid:107)0(cid:107)2 + σ2

n) +

(cid:32)

(cid:33)

E

(cid:107)un

t+1(θt)(cid:107)2|Ft

≤ C1(cid:107) ˜θt(cid:107)2 + C2 +

2

N
(cid:88)

n=1

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

Now we can give properties of DISCOVER gradient noise
by proving Lemma 3:

(cid:19)

N
(cid:88)

n=1

pn(cid:107)gn(θ∗)(cid:107)2

E((cid:107)st+1(θ∗)(cid:107)2|Ft) ≤

n=1

N
(cid:88)

n=1

pnσ2

n +

N
(cid:88)

n=1

pn(cid:107)gn(θ∗)(cid:107)2

where the expectation is also taken over the different clusters
with probabilities pn.

9.1. Proof of Lemma 3

Under the same assumptions as in Section 3, for a batch
of size |Bt| the gradient is unbiased E[ut+1(θt)|Ft] = 0.
Denoting ˜θt := θ∗ − θt, C1 = 4δ2, C2 = (cid:80)N
n=1 pnσ2
n
t , θ∗)(cid:107)2).
and σ2

n = 2 · E((cid:107)g(xn

The gradient noise of DISCOVER is given by the following:

ut+1(θt) =

=

1
|Bt|

1
|Bt|

where

(cid:18)

(cid:88)

·

xn

t ∈Bt

g(xn

t , θt) − g(n)

t + ¯gt − g(θt)

(cid:88)

xn

t ∈Bt

un
t+1(θt)

t+1(θt) = g(xn
un

t , θt) − g(n)

t + ¯gt − g(θt)

To this end, we introduce the ﬁltration Ft = {θi<t+1}.
Since θt ∈ Ft and the cluster n is selected with probability
pn, it holds that

(cid:32)

(cid:33)

E[un

t+1(θt)|Ft] = E

g(xn

t , θt) − g(n)

t + ¯gt − g(θt)|Ft

(cid:32)

(cid:33)

(cid:32)

(cid:33)

= E

g(xn

t , θt) − g(θt)|Ft

+ E

¯gt − g(n)

t

|Ft

(cid:32)

(cid:33)

= E

st+1(θt)|Ft

+ ¯gt −

N
(cid:88)

n=1

png(n)
t

= 0

Because
¯gt = (cid:80)N

st+1(θt)
n=1 png(n)
.

t

=

g(xn

t , θt) − g(θt)

and

Discover

(cid:33)

(cid:33)

E[ut+1(θt)|Ft] = E

(cid:32)

1
|Bt|

(cid:88)

un
t+1(θt)|Ft

=

1
|Bt|

= 0

xn

t ∈Bt
(cid:32)

(cid:88)

E

un
t+1(θt)|Ft

xn

t ∈Bt

(cid:32)

E

(cid:107)ut+1(θt)(cid:107)2

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ft

(cid:32)
1
|Bt|2 · E

(cid:88)

(cid:88)

xn

t ∈Bt

t (cid:54)=xn
xm
t

(cid:32)

(cid:33)T (cid:32)

un
t+1(θt)

um
t+1(θt)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

Ft

=

=

=

=

(cid:32)

= E

(cid:107)

1
|Bt|

(cid:88)

t+1(θt)(cid:107)2
un

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ft

t+1(θt)(cid:107)2
un

Ft

t ∈Bt

(cid:88)

xn
(cid:32)
1
|Bt|2 · E
(cid:107)
(cid:32)
1
|Bt|2 · E

xn

t ∈Bt
(cid:13)
(cid:13)
un
(cid:13)
t+1(θt)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

xn

t ∈Bt

(cid:32)
1
|Bt|2 · E

(cid:88)

xn

t ∈Bt

1
|Bt|2 ·

(cid:88)

xn

t ∈Bt

E

(cid:13)
(cid:13)
2(cid:12)
(cid:13)
(cid:13)
(cid:12)
un
(cid:13)
(cid:13)
(cid:12)
t+1(θt)
(cid:13)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
2(cid:12)
(cid:13)
(cid:32)(cid:13)
(cid:12)
(cid:13)
(cid:13)
un
(cid:12)
(cid:13)
(cid:13)
t+1(θt)
(cid:12)
(cid:13)
(cid:13)
(cid:12)
(cid:13)
(cid:13)

Ft

(cid:33)

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ft

+

(cid:33)

Ft

(cid:33)

In the above proof we use the fact that the gradient noise
are independent from one cluster to another meaning that
for n (cid:54)= m:

(cid:18)

E

t+1(θt)um
un

t+1(θt)|Ft

(cid:19)

(cid:18)

= E

un
t+1(θt)|Ft

(cid:19)

(cid:18)

E

um
t+1(θt)|Ft

(cid:19)

= 0

So we have :

(cid:32)

(cid:33)

E

(cid:107)ut+1(θt)(cid:107)2|Ft

=

≤

≤

1
|Bt|2 ·

1
|Bt|2 ·

(cid:88)

xn

t ∈Bt

(cid:88)

xn

t ∈Bt

(cid:32)

(cid:33)

(cid:107)un

t+1(θt)(cid:107)2|Ft

E

(cid:32)

C1(cid:107) ˜θt(cid:107)2 + C2 + 2

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

(cid:33)

N
(cid:88)

n=1

1
|Bt|

· C1(cid:107) ˜θt(cid:107)2 +

1
|Bt|

· C2 +

2
|Bt|

·

N
(cid:88)

n=1

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

Lemma 4 Under the same assumptions as in Section 3, for
a batch of size |Bt|, deﬁning Gt to be :

Gt =

N
(cid:88)

n=1

(cid:32)

(cid:33)

pnE

(cid:107)g(n)

t − gn(θ∗)(cid:107)2

The two inequalities hold:

(cid:32)

(cid:33)

(cid:32)

E

(cid:107)˜θt+1(cid:107)2

≤

1 − 2µν + µ2(δ2 +

(cid:33)
)

E((cid:107)˜θt(cid:107)2)

C1
|Bt|

+

µ2
|Bt|

C2 +

2µ2
|Bt|

· Gt

Discover

|Bt| and using the particular result of Lemma 5 we have:

θt+1 = θt −

µ
|Bt|

·

(cid:18)

(cid:88)

xn

t ∈Bt

−θt+1 = −θt +

= −θt +

µ
|Bt|

µ
|Bt|

·

·

(cid:88)

xn

t ∈Bt

(cid:88)

xn

t ∈Bt

(cid:19)

+g(θt)

= −θt +

µ
|Bt|

·

(cid:88)

xn

t ∈Bt

(cid:18)

(cid:18)

g(xn

t , θt) − g(n)

t + ¯gt

g(xn

t , θt) − g(n)

t + ¯gt

(cid:19)

(cid:19)

(cid:18)

g(xn

t , θt) − g(n)

t + ¯gt − g(θt)

g(xn

t , θt) − g(n)

t + ¯gt − g(θt)

(cid:19)

Gt+1 ≤ (1 − α) · Gt + 3αδ2 · E((cid:107)˜θt(cid:107)2) + αC2

(11)

+µ · g(θt)

9.2. Proof of Lemma 4

Before proving Lemma 4, we are going to use the following
lemma from (Sayed, 2014a):

Lemma 5 (Mean-value theorem: Real arguments)

Consider a real-valued and twice-differentiable function
g(z) ∈ R, where z ∈ RM is real-valued. Then for any
M-dimensional vectors z0 and ∆z, the following increment
equalities hold:

= −θt + µ · g(θt) + µ · ut+1(θt)
˜θt+1 = ˜θt − µ · Ht · ˜θt + µ · ut+1(θt)
˜θt+1 = (I − µ · Ht)˜θt + µ · ut+1(θt)

Since :

0 < νId < ∇g(θ) < δId

where d is the dimension of θ and ∇g(θ) is the Hessian
Matrix, we can derive:

(1 − µδ)Id < Id − µHt < (1 − µν)Id

g(z0 + ∆z) − g(z0) =

(cid:32) (cid:90) 1

0

(cid:33)

∇zg(z0 + t∆z)dt

∆z (12)

for all t. Using the fact that Id − µHt is a symmetric matrix,
we have that its 2-induced norm is equal to its spectral radius
so that:

∇zg(z0+∆z)−∇zg(z0) = (∆z)T

(cid:32) (cid:90) 1

0

∇2

zg(z0+r∆z)dr

(13)

From Lemma 5, denoting ˜θt
∇(cid:96)(f (θ, xn)), we have:

:= θ∗ − θt, g(θ) =

(cid:32) (cid:90) 1

g(θt) = −

(cid:33)

∇θg(θ∗ − t˜θt)dt

˜θt

0
˜θt

:= −Ht

(cid:107)I − µ · Ht(cid:107)2 =

(cid:33)

(cid:18)

(cid:19)2

ρ(I − µ · Ht)

(cid:40)

(cid:41)

≤ max

(1 − µδ)2, (1 − µν)2

(cid:40)

(cid:41)

= max

1 − 2µδ + µ2δ2, 1 − 2µν + µ2ν2

(cid:107)I − µ · Ht(cid:107)2 ≤ 1 − 2µδ + µ2δ2

where we are introducing the symmetric time-variant matrix
which is deﬁned in terms of the Hessian of the cost function.

Ht :=

(cid:90) 1

0

∇θg(θ∗ − t˜θt)dt

Considering DISCOVER recursion in Algorithm 1 under
the same assumptions as in Section 3, for a batch of size

Discover

We got the last inequality because ν ≤ δ

Using the previous result, we have :

(cid:107)˜θt+1(cid:107)2 ≤ (cid:107)I − µ · Ht(cid:107)2 · (cid:107)˜θt(cid:107)2 + µ2 · (cid:107)ut+1(θt)(cid:107)2

(cid:32)

(cid:33)

(cid:32)

(cid:33)

E

(cid:107)˜θt+1(cid:107)2|Ft

≤ (cid:107)I − µ · Ht(cid:107)2 · E((cid:107)˜θt(cid:107)2|Ft) + µ2E

(cid:107)ut+1(θt)(cid:107)2|Ft

≤ (1 − 2µν + µ2δ2)E((cid:107)˜θt(cid:107)2|Ft) + µ2

(cid:32)

1
|Bt|

C1E((cid:107)˜θt(cid:107)2|Ft) +

1
|Bt|

C2 +

2
|Bt|

N
(cid:88)

n=1

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

(cid:33)

(cid:32)

≤

1 − 2µν + µ2(δ2 +

(cid:33)
)

E((cid:107)˜θt(cid:107)2|Ft) +

C1
|Bt|

µ2
|Bt|

C2 +

2µ2
|Bt|

·

N
(cid:88)

n=1

pn(cid:107)g(n)

t − gn(θ∗)(cid:107)2

Then taking the expectation on both side give us:

(cid:32)

(cid:33)

(cid:32)

E

(cid:107)˜θt+1(cid:107)2

≤

1 − 2µν + µ2(δ2 +

(cid:33)
)

E((cid:107)˜θt(cid:107)2) +

C1
|Bt|

µ2
|Bt|

C2 +

2µ2
|Bt|

·

N
(cid:88)

n=1

pnE((cid:107)g(n)

t − gn(θ∗)(cid:107)2)

which conclude the proof. The second equation of Lemma 4
is obtained from Lemma 2 of (Yuan et al., 2019).

9.3. Proof of Theorem 1

From Lemma 4, we have :

Discover

E((cid:107)˜θt+1(cid:107)2) + γGt+1 ≤

1 − 2µν + µ2(δ2 +

(cid:32)

(cid:33)
)

E((cid:107)˜θt(cid:107)2) +

C1
|Bt|

µ2
|Bt|

C2 +

2µ2
|Bt|

· Gt +

(cid:32)

(cid:33)

γ

(1 − α) · Gt + 3αδ2 · E((cid:107)˜θt(cid:107)2) + αC2

(cid:34)

=

1 − 2µν + µ2(δ2 +

(cid:35)

) + 3αγδ2

E((cid:107)˜θt(cid:107)2) + (

C1
|Bt|

µ2
|Bt|

+ αγ)C2 +

(

2µ2
|Bt|

+ γ(1 − α)) · Gt

(cid:34)

≤

1 − 2µν + K

(cid:35)
E((cid:107)˜θt(cid:107)2) + (

+ αγ)C2 + (

+ γ(1 − α)) · Gt

µ2
|Bt|
(cid:18)

2µ2
|Bt|
(cid:19)

(cid:34)

(cid:35)(cid:32)

≤

1 − 2µν + K

E((cid:107)˜θt(cid:107)2) +

2µ2
|Bt| + γ(1 − α)

1 − 2µν + K

(cid:33)

· Gt

+

(

µ2
|Bt|

+ αγ)C2

(

µ2
|Bt|

+ αγ)C2

(cid:34)

(cid:35)(cid:32)

≤

1 − 2µν + K

E((cid:107)˜θt(cid:107)2) +

(cid:18)

2µ2
|Bt| + γ(1 − α)

(cid:19)

1 − 2µν

(cid:33)

· Gt

+

(cid:32)

≤ (1 − µν)

E((cid:107)˜θt(cid:107)2) +

(cid:32)

(cid:18)

2µ2
|Bt| + γ(1 − α)

(cid:19)

1 − 2µν
(cid:33)

µ2
|Bt|

≤ (1 − µν)

E((cid:107)˜θt(cid:107)2) + γ · Gt

+ (

+ αγ)C2

(cid:33)

· Gt

+ (

µ2
|Bt|

+ αγ)C2

E((cid:107)˜θt+1(cid:107)2) + γGt+1 ≤ (1 − µν)

E((cid:107)˜θt(cid:107)2) + γ · Gt

+

(cid:32)

(cid:33)

4µ2
|Bt|

· C2

where K = 3δ2(µ2 + 2µ2

|Bt| + αγ).

Since µ satisﬁes µ ≤ α
6ν

α − 6µν ≥ 0

Discover

3α − 6µν ≥ 2α
3
α
3µ2
α|Bt|

≥

≥

γ =

2
α − 2µν
2µ2
|Bt|(α − 2µν)

γ ≥

2µ2
|Bt|(α − 2µν)

⇔ γ ≥

(cid:18)

2µ2
|Bt| + γ(1 − α)

(cid:19)

1 − 2µν

(14)

Because

(cid:18)

2µ2
|B| + γ(1 − α)

(cid:19)

γ =

1 − 2µν

⇔ γ =

2µ2
|Bt|(α − 2µν)

(15)

Also since µ satisﬁes µ ≤

ν|Bt|
3δ2(|Bt|+5)

It implies that :

) ≤

) ≤

5
|Bt|
5
|Bt|
3µ2
|Bt|

ν
3δ2
µν
3δ2
µν
3δ2
µν
3δ2

+ αγ) ≤ µν

µ2 +

+

≤

µ2 +

+ αγ ≤

µ(1 +

µ2(1 +

2µ2
|Bt|
2µ2
|Bt|
2µ2
|Bt|
2µ2
|Bt|

3δ2(µ2 +

1 − 2µν + 3δ2(µ2 +

+ αγ) ≤ 1 − 2µν + µν

1 − 2µν + K ≤ 1 − µν

So in conclusion :

1 − 2µν + K ≤ 1 − µν

(16)

In summary we have :

E((cid:107)˜θt+1(cid:107)2) + γGt+1 ≤ (1 − µν) ·

E((cid:107)˜θt(cid:107)2) + γGt

+

(cid:32)

(cid:33)

4µ2

|Bt|

C2

Discover

Iterating recursion above we get :

E((cid:107)˜θt+1(cid:107)2) ≤ E((cid:107)˜θt+1(cid:107)2) + γGt+1 ≤ (1 − µν) ·

E((cid:107)˜θt(cid:107)2) + γGt

+

(cid:32)

(cid:33)

4µ2
|Bt|

C2

≤ (1 − µν)t+1 ·

E((cid:107)˜θ0(cid:107)2) + γG0

+

(cid:32)

(cid:33)

≤ (1 − µν)t+1 ·

E((cid:107)˜θ0(cid:107)2) + γG0

+

(cid:32)

(cid:33)

4µ2
|Bt|

C2

t
(cid:88)

(1 − µν)k

k=0

1
1 − (1 − µν)

4µ2
|Bt|

C2

≤ (1 − µν)t+1 ·

E((cid:107)˜θ0(cid:107)2) + γG0

+

(cid:32)

(cid:33)

4µ
ν|Bt|

C2

That implies :

E((cid:107)˜θt+1(cid:107)2) ≤ (1−µν)t+1·

E((cid:107)˜θ0(cid:107)2)+γG0

+

(cid:32)

(cid:33)

4µ
ν|Bt|

C2

(17)

So that :

lim sup
t→+∞

E((cid:107)θ∗ − θt+1(cid:107)2) = O(

µ
|Bt|

C2) = O(µ · σ2

in/|Bt|)
(18)

