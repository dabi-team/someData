Improving X-ray Diagnostics through Eye-Tracking and XR

Catarina Moreira*
INESC-ID / School of Information Systems,
Queensland University of Technology, Australia

Isabel Blanco Nobre†
Imagiology Department
Grupo Lus´ıadas, Lisboa, Portugal

Sandra Costa Sousa‡
Imagiology Department
Grupo Lus´ıadas, Lisboa, Portugal

Jo ˜ao Madeiras Pereira§
INESC-ID / Instituto Superior T ´ecnico
Universidade de Lisboa, Portugal

Joaquim Jorge¶
INESC-ID / Instituto Superior T ´ecnico
Universidade de Lisboa, Portugal

2
2
0
2

r
a

M
3

]

C
H
.
s
c
[

1
v
3
4
6
1
0
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT

There is a growing need to assist radiologists in performing X-ray
readings and diagnoses fast, comfortably, and effectively. As radiol-
ogists strive to maximize productivity, it is essential to consider the
impact of reading rooms in interpreting complex examinations and
ensure that higher volume and reporting speeds do not compromise
patient outcomes. Virtual Reality (VR) is a disruptive technology
for clinical practice in assessing X-ray images. We argue that con-
jugating eye-tracking with VR devices and Machine Learning may
overcome obstacles posed by inadequate ergonomic postures and
poor room conditions that often cause erroneous diagnostics when
professionals examine digital images.

Index Terms:
Applied Computing—Life and medical
Sciences—Health informatics Computing methodologies—Artiﬁcial
intelligence—Knowledge representation and reasoning—Causal rea-
soning and diagnostics Human-centered computing—Virtual reality

1 INTRODUCTION

The outbreak of recent pandemics affecting patients’ lungs has re-
minded the scientiﬁc community that there is a growing need to
assist radiologists in performing X-ray readings and diagnoses fast,
comfortably, and effectively. As radiologists strive to maximize
productivity, it is essential to consider the impact of radiologists’
reading rooms in the interpretation of increasingly complex exam-
inations and ensure that higher volume and reporting speeds do
not compromise patient outcomes [37]. In this pressured setting,
inadequate ergonomic postures and improper room conditions can
cause erroneous diagnostics when professionals examine 3D digital
images using common 2D desktop displays [27].

Current radiology reading rooms still face many limitations. The
most signiﬁcant is that the quality of the X-ray images is seriously
affected by the quality of the display devices, which impacts the
radiologists reading ability. Additionally, the room’s improper light
and luminance conditions together with pressures in radiologists to
increase productivity can lead to visual tiredness, cognitive overload,
and decision fatigue [23, 31], which makes the radiologists’ readings
more susceptible to biases and errors [29].

Recently, Sousa et al. [22, 32] showed that the application of Vir-
tual Reality (VR) technologies for radiology reading rooms consti-
tutes a viable, ﬂexible, portable, and cost-effective option to address

*e-mail: catarina.pintomoreira@qut.edu.au
†e-mail: isabel.blanco.nobre@lusiadas.pt
‡e-mail: sandra.costa.sousa@lusiadas.pt
§e-mail: jap@inesc-id.pt
¶e-mail: jorgej@tecnico.ulisboa.pt

Figure 1: Using VR for XRay Diagnostics. Courtesy of rawpixel.com
(https://www.freepik.com/photos/technology)

the limitations associated with traditional settings. However, one
signiﬁcant aspect that the authors have not explored is the potential
of using radiologists’ eye-tracking data to build decision models that
could support, amplify, augment and enhance expert radiologists’
performance when assessing X-ray images.

1.1 The Importance of Eye Tracking in Radiology

Eye-tracking data in X-ray images have been receiving much inter-
est in the scientiﬁc community, particularly in the ﬁeld of Human-
Computer Interaction (HCI) and Artiﬁcial Intelligence (AI). Al-
though there is a rich literature correlating eye-tracking data with
user cognitive load [3, 11], the connection between eye movements
and the cognitive state has mainly been under-exploited in Human-
Computer Interaction, and a deeper understanding of its usability in
practice is still required [38].

In AI, eye tracking ﬁxation data could provide an automated
approach to label large amounts of data without asking expert radiol-
ogists to manually annotate them [18]. Although AI cannot replace
clinicians in medical diagnosis, it can support expert radiologists
in time-consuming tasks, such as examining chest X-rays for signs
of pneumonia. AI models can outperform humans at speciﬁc diag-
noses in X-ray images. However, many times these models get the
predictions correct, but not for the right clinical reasons [24, 28].
Consequently, radiologists are reluctant to blindly accept the an-
swers produced by AI without understanding how those predictions
came to be. This uncertainty poses a signiﬁcant barrier to adopting
AI-based technologies in healthcare because they are highly suscep-
tible to well-documented biases (such as automation bias), leading

 
 
 
 
 
 
to a misdiagnosis that may erode trust in model predictions, limiting
clinical translation and adoption. This has led to new research paths,
such as eXplainable Artiﬁcial Intelligence (XAI), which aims to
allow human users to understand and trust the results and outputs of
machine learning algorithms [13].

One of the main points that we argue in this paper is that eye-
tracking data can enable novel AI architectures that, instead of learn-
ing solely from X-ray pixel data, will use radiologists’ classiﬁcation
patterns encoded in their eye movements. Ultimately, eye-tracking
data can be instrumental in teaching machines how humans learn
and using that knowledge to help clinicians better understand how
machines make decisions. However, current (desktop) eye-tracking
technologies still suffer from signiﬁcant problems that prevent the
widespread collection of high-quality ocular data.

1.2 Limitations of Desktop Eye-Tracking Devices

Related works address the lack of eye-tracking data in X-ray images
by using desktop devices. However, these devices pose several
concerns in data quality and data collection (Figure 2):
Fixed distance from screen. To collect eye-tracking data, radiolo-
gists need to be in a ﬁxed position with a speciﬁc distance from the
reading screen to ensure the system’s calibration. This can cause
discomfort, increase task difﬁculty, decrease concentration, and limit
interaction with X-ray images (e.g., zooming).
Frequent Calibration. Eye-tracker calibrations are necessary for
ﬁnding the correspondence between pupil and cornea positions in
the image captured by the eye tracker’s camera and locations on the
screen where the radiologists are looking [18].
Limited Head Movements and Loss of Line of Sight. Desktop
eye-tracking devices require that the user is ﬁxed to a particular
position. Lateral body movements, e.g. neck rotations cause tracking
loss and consequently lead to noise and data loss.
Oculomotor drift may contribute to misinterpretations. Mea-
suring the eyes’ drift and tremor with high precision and accuracy
is challenging [21]. These movements appear to be beyond the
resolution of the most available eye trackers.
Difﬁculty of Interpreting Eye Tracking Data. Eye-tracking anal-
ysis is based on the assumption that there is a relationship between
ﬁxation points and what the radiologist is thinking about. However,
that is often not the case, and ﬁxations do not necessarily translate
into a conscious cognitive process.
1.3 Using VR technologies for clinical practice

In this paper, we argue that VR can overcome most of the desktop
eye-tracking limitations and become a disruptive technology for
clinical practice in assessing X-ray images. VR headsets enable ra-
diologists to move comfortably and good ergonomic postures when

Figure 2: EyeLink 1000 Plus ®desktop eye tracker with chin and
forehead support to avoid user doing lateral movements (https:
//www.sr-research.com/eyelink-1000-plus/).

diagnosing, provide high-resolution screens that help radiologists
analyze X-ray images, and can extract high-quality eye-tracking data
and pupil dilations that AI frameworks can use to learn human classi-
ﬁcation patterns or that can be explored in HCI to infer correlations
between eye movements and cognitive load. The following sections
discuss how this could be achieved based on recent advances.

2 RELATED WORK

Eye-tracking has been used extensively in HCI-related domains
and is routinely applied to assess the impact of interface and web
page designs on usability and content presentation. However, its
application to improving human performance is still incipient. Addi-
tionally, eye movement data are notoriously difﬁcult to analyze and
heavily task-dependent. Duchowski’s comprehensive book provides
an excellent perspective of the ﬁeld and its evolution [6]. Recent
surveys proposed measuring pupillary activity index as a base to
models and estimate cognitive load [7, 38]. Other authors have
looked at how perceived task difﬁculty in mental arithmetic affects
microsaccadic rates, and magnitudes [30]. Older studies looked at
using task-induced pupil diameter and blink rate to infer cognitive
load [4]. Kretjz et al. attempted to separate Ambient illumination,
and Focal Attention [17], whereas [12] looked at Evidence from
Static and Dynamic Scene Perception and its inﬂuence on eye move-
ments. Minamoto et al. [2] studied how memory load affected eye
movement control using the reading span test. Other authors have
looked at Gaze Analytic methods to understand human behavior.
More recently, [1, 15, 19, 33] have used eye-tracking data to improve
segmentation and disease classiﬁcation in Computed Tomography
(CT) by integrating them in deep learning techniques. We argue that
eye-movement data from trained radiologists can offer deep insights
into how they analyze images and formulate diagnostics [10].

3 ADVANTAGES OF VR USAGE IN RADIOLOGY ROOMS

A recent systematic literature review conducted by Trevia et al. [34]
analyzed the potential of VR technologies in radiology rooms to
assist clinicians in their practice. The authors found some important
points where VR technologies could be advantageous in clinical
radiologist practices:
Reduced Lightning and Ergonomics Equipment Costs. The use
of VR in radiology cuts the equipment and maintenance costs of a
traditional radiology reading room, and it could potentially improve
accuracy in radiological diagnosis [8].
VR Renders High Quality Data Visualisations. This can enable
more detailed surgery planning and communications among expert
radiologists and patients in medical appointments. [35, 36].
VR can Promote Medical Training. Students and experienced ra-
diologists can leverage VR technologies for medical training, which
can be reviewed at the learners’ pace [16]. This can provide a cost-
effective alternative to face-to-face training sessions or simulations.
To complement the above list, we add the following points, which
we argue are key missing ingredients that provide advantages in the
adoption of VR technologies for clinical practice:
Accurate in Eye-Tracking Mechanisms. VR technologies can
overcome the limitations of current desktop eye-tracking devices
and enable the collection of ﬁxation points and eye gaze without the
problem of losing line of sight or having body movements.
Insights about Radiologists Cognitive Load. The duration of ﬁx-
ation points, the length of the saccades, pupil dilation, or even the
number of blinks that a radiologist makes during a reading can
provide insights into how complex the diagnosis is.
Insights about Radiologists’ Fatigue. A signiﬁcant amount of
literature suggests correlations between ﬁxation points and pupil di-
lations with how tired a radiologist is [7]. With the high expectations
in terms of pressure and speed in delivering a diagnosis, radiologists
are affected by fatigue, which could be detected with the state of the
art metrics and could alert the radiologist to take a break.

Promotes the Development of Multi-Modal Learning AI. Eye-
tracking data and cognitive load features could be incorporated in
advanced deep learning architectures to learn human classiﬁcation
patterns instead of focusing solely on the pixels of X-ray images.
Novel Human-Centred XAI Mechanisms. Current explainable AI
algorithms fail to accurately identify the regions in x-ray images that
are responsible for a given prediction. Incorporating eye-tracking
data can show how radiologists learn and how they assess x-ray
images. This could provide human-centric explanations to expert
radiologists and promote trust in AI-based technologies.
Simulations of healthcare environments for clinical practice.
VR technologies can use AI mechanisms to teach young student
radiologists how to read an X-ray image based on the classiﬁcation
patterns of more senior radiologists.
VR Promotes Remote Collaboration. VR can also support joint
work as it creates many opportunities for collaboration. In situations
like pandemics where traveling needs to be minimized, VR tech-
nologies can bring together world radiologist experts to analyze and
assess complex X-ray images in collaboration in a virtual reading
room. This is different from tele-medicine or tele-diagnostics which
are already feasible today. Indeed, real-time collaboration via shared
virtual worlds can provide a fertile ground for research.

In what follows, we discuss how eye-tracking can be combined
with AI technologies to support, amplify, augment and enhance
expert radiologists’ performance when assessing X-ray images.

4 EYE-TRACKING AND AI FOR X-RAY IMAGIOLOGY

The reduced cost of VR equipment promoted the adoption of eye-
tracking technologies and the creation of novel X-ray datasets with
multi-modal data such as ﬁxation points, pupil dilations, radiolo-
gists audio utterances, etc [14, 18]. Figure 3 shows an example of
heatmaps generated from radiologists’ manual annotations in chest
X-rays and an example of a radiologist’s scan-path and ﬁxation
points generated from eye-tracking data [18].

Currently, some works in the literature combined different data
modalities in deep learning architectures to achieve higher accuracy
in predicting pneumonia in chest X-ray images [9]. For instance,
Moon et al. [20] combined chest X-ray images and radiologists’
text reports to predict the patient’s disease and to generate diagnos-
tics automatically. However, these architectures are opaque and do
not explain the algorithms’ internal workings to the expert radiol-
ogist. This uncertainty makes radiologists skeptical and resistant
to adopting AI-based technologies in X-ray diagnosis. Therefore,

Figure 3: Example of heatmaps generated from radiologists annota-
tions for different lesions (left). Radiologist eye-tracking scan paths
and ﬁxation points (right) [18].

there is a need for human-centered explanations that are both human-
understandable and domain-speciﬁc in AI-based predictions for
X-ray images [5]. In addition, there is a growing need to design
interactive explainable models that allow radiologists to drill down
or ask for different explanations until they are satisﬁed.

A thought experiment carried out in 2018 at the Neural Informa-
tion Processing Systems Conference asked hundreds of participants
to imagine that they had a cancerous tumor and needed to choose a
surgeon to operate them. The ﬁrst option was a human surgeon who
could explain the details of the operation but had a 15% chance of
causing death during the surgery. The second option was a robot that
could achieve a 98% success rate but not provide any explanations
or answers to any questions. In the end, the robot surgeon received
just one vote [26]. This thought experiment demonstrates the skep-
ticism amongst people in accepting AI-based technologies and the
need for human-centred AI technologies, particularly in contexts of
high-stakes decision-making such as healthcare [25].

4.1 Eye-Tracking as a Foundation for Explainable

Human-Centric AI

Very few studies in the literature have attempted to use eye-tracking
radiologist data combined with chest X-ray images. We argue that
eye-tracking is a key missing ingredient that can make machines
learn how radiologists learn and serve as a foundation for developing
human-centric AI technologies that can learn from human classiﬁ-
cation patterns. Eye-tracking can provide unique insights into AI
learning architectures of how conﬁdent (and tired) the radiologist is
in his readings, the degree of expertise, and the task complexity.

Eye-tracking and pupil dilation data can help radiologists better
understand the difﬁculty of assessing a speciﬁc X-ray image. Ac-
cording to Duchowski et al. [7] ﬁxation duration is proportional
to the duration of the underlying difﬁculty of the cognitive opera-
tion, longer ﬁxations and shorter saccades may suggest increased
cognitive load, and more complex problems evoke large pupillary
dilations. Additionally, eye-tracking scan paths can also indicate the
degree of expertise of the radiologist: targeted ﬁxation points corre-
late with expertise. Studies suggest that the more expert a radiologist
is, the more targeted the eye-tracking scan paths are. In Borys and
Plechawska-W´ojcik [3], the authors asked a group of students and
an expert radiologist to read a chest X-ray image. They found that
the scan paths of the expert radiologist were sharp and targeted at
the regions of the X-ray with lesions. Students who could read the
image correctly had messier scan paths with larger ﬁxation points in
the regions with lesions. Those who could not read the image had
random scan paths and shorter duration ﬁxation points.

5 CONCLUSIONS

A VR headset provides high-resolution screens that help radiologists
analyze X-ray images and extract high-quality eye-tracking data and
pupil dilations. AI frameworks can use this data to learn human
classiﬁcation patterns and discover correlations between eye move-
ments and cognitive load. In this paper, we provide a discussion of
the advantages of VR equipment in the collection and generation of
eye-tracking data to support, amplify, augment and enhance expert
radiologists’ performance when assessing X-ray images. We put
forward two main arguments: (1) eye-tracking data has the potential
to enable novel AI learning architectures that, instead of learning
solely from X-ray pixel data, will learn from radiologists’ classi-
ﬁcation patterns that are encoded in their eye movements; and (2)
VR can overcome most of the limitations highlighted in traditional
desktop eye-trackers and serve as a disruptive technology for clinical
practice in assessing X-ray images.

We ﬁnish by reﬂecting on major challenges affecting the adoption
of VR technologies in clinical practice. The ﬁrst is device physical
characteristics and resolution. While radiologists complained about
the display resolution of earlier VR head-mounted displays (HMDs),

have progressed at a faster clip. They are now competitive in terms
of pixels/radian with conventional displays. We expect time to tilt
the scales even more in favor of HMDs. While over ten million
Oculus Quest 2 units were shipped in 2021, Reading Room displays
remain an expensive niche market. In contrast, commodity HMDs
now cost comparatively much less. As for eye-tracking interfaces,
the required hardware is not part of commodity headsets. However,
this is changing. Many commercial headsets now feature infrared
cameras for this purpose, at a twentieth of the cost of desktop eye-
tracking a decade ago. Finally, the most signiﬁcant barrier to the
adoption of VR HMDs as medical devices has to do with regula-
tory approval. Indeed, while Computer Science research requires
comparatively small user studies, establishing the clinical validity of
a novel medical device or procedure requires extensive testing and
validation to warrant regulatory approval. Still, given the many ad-
vantages and promises outlined herein, we believe that the future of
clinical radiology diagnostics lies in Extended Reality Technologies.

ACKNOWLEDGMENTS
This work was supported in part by the Portuguese Govt. Fundac¸ ˜ao
para a Ciˆencia e a Tecnologia, under project UIDB/50021/2020.

REFERENCES

[1] G. Aresta, C. Ferreira, J. Pedrosa, et al. Automatic lung nodule detec-
tion combined with gaze information improves radiologists’ screening
performance. IEEE Journal of Biomedical and Health Informatics,
24:2894–2901, 2020.

[2] M. Azuma, T. Minamoto, K. Yaoi, M. Osaka, and N. Osaka. Effect
of memory load on eye movement control: A study using the reading
span test. Journal of Eye Movement Research, 7, 2014.

[3] M. Borys and M. Plechawska-Wojcik. Eye-tracking metrics in per-
ception and visual attention research. European Journal of Medical
Technologies, 3:11–23, 2017.

[4] S. Chen and J. Epps. Using task-induced pupil diameter and blink rate

to infer cognitive load. Human–Computer Inter., 29:390–413, 2014.

[5] M. Chromik and A. Butz. Human-XAI Interaction: A Review and
Design Principles for Explanation User Interfaces. In C. Ardito et al.,
eds., Human-Computer Interaction. 2021.

[6] A. Duchowski. Eye Tracking Methodology: Theory and Practice.

Springer Mihe (3rd edition), 2017.

[7] A. Duchowski, K. Krejtz, I. Krejtz, et al. The Index of Pupillary
Activity: Measuring Cognitive Load Vis-`a-Vis Task Difﬁculty with
Pupil Oscillation, p. 1–13. ACM, 2018.

[8] M. Elsayed, N. Kadom, C. Ghobadi, et al. Virtual and augmented
reality: potential applications in radiology. Acta Radiologica, 61:1258–
1265, 2020.

[9] W. Gale, L. Oakden-Rayner, G. Carneiro, et al. Producing radiologist-
quality reports for interpretable deep learning. In IEEE 16th Intern.
Symp. on Biomedical Imaging (ISBI), pp. 1275–1279, 2019.

[10] N. A. Gehrer, M. Sch¨onenberg, A. Duchowski, and K. Krejtz. Imple-
menting innovative gaze analytic methods in clinical psychology: A
study on eye movements in antisocial violent offenders. In Proceedings
of the 2018 ACM Symp. on Eye Tracking Research & Apps., 2018.
[11] K. Harezlak and P. Kasprowski. Application of eye tracking in
medicine: A survey, research issues and challenges. Computerized
Medical Imaging and Graphics, 65:176–190, 2018.

[12] J. R. Helmert, M. Joos, S. Pannasch, and B. M. Velichkovsky. Two
visual systems and their eye movements: Evidence from static and
dynamic scene perception. In Proceedings of the 27th Annual Meeting
of the Cognitive Science Society (CogSci), 2005.

[13] A. Holzinger et al. Causability and explainability of artiﬁcial intelli-
gence in medicine. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 9:e1312, 2019.

[14] A. Karargyris, S. Kashyap, I. Lourentzou, et al. Creation and validation
of a chest x-ray dataset with eye-tracking and report dictation for ai
development. Scientiﬁc Data, 8, 2021.

[15] N. Khosravan, H. Celik, B. Turkbey, et al. A collaborative computer
aided diagnosis (c-cad) system with eye-tracking, sparse attentional
model, and deep learning. Medical Image Analysis, 51:101–115, 2019.

[16] M. M. Knodel, B. Lemke, M. Lampe, et al. Virtual reality in advanced
medical immersive imaging: a workﬂow for introducing virtual reality
as a supporting tool in medical imaging. Computing and Visualization
in Science, 18:203–212, 2018.

[17] K. Krejtz, A. Duchowski, I. Krejtz, A. Szarkowska, and A. Kopacz. Dis-
cerning ambient/focal attention with coefﬁcient k. ACM Transactions
on Applied Perception, 13, 2016.

[18] R. B. Lanfredi, M. Zhang, W. F. Auffermann, et al. Reﬂacx, a dataset
of reports and eye-tracking data for localization of abnormalities in
chest x-rays, 2021.

[19] S. Mall, P. C. Brennan, and C. Mello-Thoms. Modeling visual search
behavior of breast radiologists using a deep convolution neural network.
Journal of Medical Imaging, 5(3):035502, 2018.

[20] J. H. Moon, H. Lee, W. Shin, and E. Choi. Multi-modal understanding
and generation for medical images and text via vision-language pre-
training. arXiv, 2021.

[21] D. C. Niehorster, R. Zemblys, and K. Holmqvist. Is apparent ﬁxational
drift in eye-tracking data due to ﬁlters or eyeball rotation? Behavior
Research Methods, 53:311–324, 2021.

[22] S. F. Paulo, D. Medeiros, D. Lopes, and J. Jorge. Controlling camera
movement in VR colonography. Virtual Reality, 2022. doi: 10.1007/
s10055-021-00620-4

[23] B. I. Reiner and E. Krupinski. The insidious problem of fatigue in
medical imaging practice. Journal of Digital Imaging, 25:3–6, 2012.
[24] J. G. Richens, C. M. Lee, and S. Johri. Improving the accuracy of med-
ical diagnosis with causal machine learning. Nature Communications,
11:3923–3932, 2020.

[25] C. Rudin. Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead. Nature Machine
Intelligence, 1:206–215, 2019.

[26] C. Rudin and J. Radin. Why are we using black box models in ai
when we don’t need to? a lesson from an explainable ai competition.
Harvard Data Science Review, 1(2), 2019.

[27] E. Samei, A. Badano, D. Chakraborty, et al. Assessment of display
performance for medical imaging systems: Executive summary of
aapm tg18 report. Medical Physics, 32(4):1205–1225, 2005.

[28] A. Saporta, X. Gui, A. Agrawal, et al. Benchmarking saliency methods

for chest x-ray interpretation. medRxiv, 2021.

[29] G. Saposnik, D. Redelmeier, C. C. Ruff, and P. N. Tobler. Cognitive
biases associated with medical decisions: a systematic review. BMC
Medical Informatics and Decision Making, 16, 2016.

[30] E. Siegenthaler, F. M. Costela, et al. Task difﬁculty in mental arithmetic
affects microsaccadic rates and magnitudes. European Journal of
Neuroscience, 39:287–294, 2014.

[31] E. Sokolovskaya, T. Shinde, R. B. Ruchman, et al. The effect of
faster reporting speed for imaging studies on the number of misses and
interpretation errors: A pilot study. Journal of the American College of
Radiology, 12, 2015.

[32] M. Sousa, D. Mendes, S. Paulo, N. Matela, J. Jorge, and D. S. Lopes.
Vrrrroom: Virtual reality for radiologists in the reading room. In Pro-
ceedings of the 2017 CHI Conference on Human Factors in Computing
Systems, p. 4057–4062. ACM, 2017.

[33] J. N. Stember, H. Celik, E. Krupinski, et al. Eye tracking for deep
learning segmentation using convolutional neural networks. Journal of
digital imaging, 32(4):597–604, 2019.

[34] R. Trevia and M. S. Pinho. Virtual reality in radiology: A systematic
mapping study. In SVR21 - International Symposium on Virtual and
Augmented Reality, p. 177–181. SBC, 2021.

[35] J. E. Venson, J. C. A. Berni, C. E. da Silva Maia, A. M. da Silva,
M. d’Ornelas, and A. Maciel. Medical imaging vr: can immersive 3d
aid in diagnosis? In Proceedings of the 22nd ACM Conference on
Virtual Reality Software and Technology, 2016.

[36] J. E. Venson, J. C. A. Berni, C. E. da Silva Maia, et al. A case-based
study with radiologists performing diagnosis tasks in virtual reality.
Studies in health technology and informatics, 245:244–248, 2017.
[37] S. Waite, S. Kolla, J. Jeudy, et al. Tired in the reading room: The
inﬂuence of fatigue in radiology. Journal of the American College of
Radiology, 14(2):191–197, 2017.

[38] X. Wang, Z. Bylinskii, M. Castelhano, J. Hillis, and A. Duchowski.
EMICS’21: Eye Movements as an Interface to Cognitive State. 2021.

