1

0
2
0
2

r
a

M
1

]

V
C
.
s
c
[

1
v
9
2
4
0
0
.
3
0
0
2
:
v
i
X
r
a

Deep Learning for Content-based Personalized
Viewport Prediction of 360-Degree VR Videos

Xinwei Chen, Ali Taleb Zadeh Kasgari, Member, IEEE, and Walid Saad, Fellow, IEEE

Abstract—In this paper, the problem of head movement pre-
diction for virtual reality videos is studied. In the considered
model, a deep learning network is introduced to leverage position
data as well as video frame content to predict future head
movement. For optimizing data input into this neural network,
data sample rate, reduced data, and long-period prediction length
are also explored for this model. Simulation results show that
the proposed approach yields 16.1% improvement in terms of
prediction accuracy compared to a baseline approach that relies
only on the position data.

Index Terms—Virtual reality; 360-degree video; Head move-

ment prediction

I. INTRODUCTION

V IRTUAL reality (VR) is arguably one of the most excit-

ing technologies for the coming decade [1]. Major virtual
reality headset producers such as HTC VIVE, Oculus Go, and
PlayStation VR are developing advanced head mount display
(HMD) with high resolution and high refresh rate in order to
enable a plethora of innovative VR applications. Meanwhile,
360° video providers like YouTube and Facebook can provide
ultra-high-resolution (up to 16K) with 120 Hz frame rate
panoramic videos to users for a better immersion experience.
However, to deliver VR and 360° video content wireless over
the Internet, VR providers must transmit their content in a
bandwidth-efﬁcient manner over capacity-constrained wireless
networks [2]. In particular, the whole VR video is downloaded
and delivered to HMD when watching, but the observable area
in the device, called ﬁeld of view (FoV) or viewport, is only
part of the whole video downloaded. While a VR user can
adjust its orientation by changing its pitch, yaw, and roll, the
viewport displayed in HMD is based on the head movement
and FoV. Therefore, most of the VR video delivered to HMD
is not displayed in the viewport.

The FoV deﬁnes the extent of the area observable by a
VR user and this area is always a ﬁxed parameter of a VR
headset (typically 110°). To ensure a good viewing experience,
the video delivered is often transmitted in extremely high
resolution, typically 4K (3840 × 2160), while in HMDs the
VR users only see a small part of the video. Thus, in order
to save network bandwidth, it is necessary to develop new
approaches that allow a VR content provider to deliver only
the viewport to the HMD of its users.

Tile-based panoramic video streaming [3] is a special type
of panoramic video streaming which can deliver part of the

Xinwei Chen is with Department of Control Science and Engineering,

Zhejiang University, Hangzhou, China (e-mail: xwchen.zju@gmail.com).

Ali Taleb Zadeh Kasgari, is with Wireless@VT, Electrical and Computer
Engineering Department Virginia Tech, Blacksburg, VA 24060 USA. (e-mail:
alitk@vt.edu).

Walid Saad is with Wireless@VT, Electrical and Computer Engineer-
(e-mail:

ing Department, Virginia Tech, Blacksburg, VA 24060 USA.
walids@vt.edu).

This research was supported by the U.S. National Science Foundation under

Grant IIS-1633363.

video content, instead of the whole video. In order to deliver
only the viewport content with tile streaming, it is imperative
to develop new approaches to predict head movement for a VR
user. In this way, VR content providers can only deliver the
video portions which the user is most likely to view in high
quality while the remaining portions are ignored or delivered
in low quality.

Despite tremendous efforts devoted to the prediction of
user’s viewport in 360° videos [4]–[10], there are no works
considering both head movement and content of the video for
predicting future viewport of the video. By taking both head
movement and video content into account, the VR system can
be designed to outperform previous works in predicting the
viewport of a 360° video for the user.
A. Related Works

Some recent works such as in [4], [5] have studied pre-
dicting head movement using only history head orientation
data. They propose methods such as simple average [4],
linear regression [5], and weighted linear regression [4] to
predict future head movement. However, these approaches in
[4], [5] use naive models and ignore video content’s relation
to future movement, thus are less accurate. Other existing
works such as in [6]–[10] combine both the video content
features and orientation of HMD to predict the future head
movement. In [6], the authors use a pre-trained saliency model
to predict head movement. The work in [7] integrated saliency,
motion map, and prior head orientation to further improve
the prediction accuracy. The authors in [8] introduce SaltiNet
for scan-path prediction trained on 360° images. Moreover,
the work in [9] proposes panoramic saliency based on the
user’s ﬁxation along with the user’s head movement history
for prediction. The authors in [10] combine saliency map,
gaze displacement and history scan-path for a better prediction
performance.

Although the works in [6]–[10] use both video saliency
and history head orientation to predict future head movement,
these prior works do not directly treat video frame content
in much detail. Since VR videos contain various scenes, and
further, each scene has different regions of interest for users.
In fact, remarkably, to date, none of the prior works has
investigated whether video frame content can contribute to
the performance of head movement prediction. This study
will generate fresh insight into this frame content and head
trajectory-based architecture.
B. Contributions

The main contribution of this paper is a novel framework
for predicting the future head movement of VR users while
watching a 360° video. To the best of our knowledge, this is
the ﬁrst work that considers both position-related data and
content-related data VR user’s head movement prediction.

 
 
 
 
 
 
2

users because of the merit of LSTM. We will elaborate on the
model in the next section.

B. Movement Prediction Network

1) Feature extraction module: Different methods have been
used to extract features from the image. One suitable approach
is to use neural networks, particularly, CNNs to extract image
features. CNNs are suitable for this problem because they
showed a tremendous success in end-to-end learning tasks
such as object detection and image classiﬁcation. We adopt
a deep multi-level network, whose architecture is based on
MobileNetV2 [13]. MobileNetV2 is a very effective feature
extractor based on an inverted residual structure that can be
implemented on the user’s mobile devices easily due to its
lightweight. A pre-trained MobileNetV2 model follows with
three fully connect layers to extract the feature of the image.
This network extracts 32 features for each frame. The output
of CNN is now deﬁned as follows:
fi = h1(vi),
(1)
where function h1(·) represents the input-output function of

MobileNetV2.

2) Movement prediction module: The function of this net-
work is to predict where the VR user is more likely to look at
in the future video frames given a sequence of concatenated
observed video frames and position-related data. The move-
ment prediction network, which is based on a recurrent neural
network (RNN) [14], is suitable to learn useful information
from a time series of video frames. In view of the good
performance of LSTM networks in motion tracking and its
ability to learn long-term dependencies, we choose an LSTM-
based RNN for predicting future head movements.

The network takes as inputs the features of n past video
frames as well as the head orientation data in a sliding window.
Then, we can deﬁne the output set of stacked LSTM Lt+1:t+T
as:

lt+1, lt+2, ..., lt+T = h2( f1 + l1, f2 + l2, ..., fn + l n),
(2)
where function h2(·) represents the input-output function of
stacked LSTM. In addition, the function h2 is stacked LSTMs
with 2 LSTM layers and one fully connect layer, each with
256 neurons.

Hence, for this model, our objective is to predict the future T
movement using deep learning methods. The model proposed
above can minimize the prediction error while meeting the
memory constraint on mobile device and delay requirement
of the VR system.

C. Performance Metric

We propose to use the viewing angle between the predicted
orientation and the actual one to measure the performance
of head movement prediction. A small angle implies a more
accurate orientation prediction, and, hence, we use the mean
angle error (MAE) to evaluate the performance. For head ori-
entation in frame i, we convert the quaternion expression to a
Euler angle (xi, yi) which indicate the latitude and longitude of
head orientation on a 3D-Sphere where xi ∈ [−180, 180], yi ∈
[−90, 90]. We deﬁne that (xi, yi) as actual head orientation and
( ˆxi, ˆyi) as predicted one. Here the angle error σ at i frame can
be deﬁned as:

Fig. 1: The architecture of our proposed deep neural network.

First, we propose a deep learning network to predict future
head movement, this network employs a convolutional neural
network (CNN) as feature extractor of content-related data and
long short-term memory (LSTM) as the encoder of position-
related data to predict the future movement of a user. The
position-related data includes the VR user’s head orientation,
while the content-related data is each VR video frame itself.
Simulation results using real data show that the proposed
CNN-LSTM architecture can achieve high prediction accuracy,
in particular, the proposed content-based approach can yield up
to 16.1% compared to a baseline that relies only on position.

II. PROBLEM STATEMENT AND PROPOSED APPROACH

In this section, we will describe our approach toward head

movement prediction.

A. Problem Deﬁnition

The head movement prediction problem is formulated as
follows: We use a set V1:t = {v1, v2, ..., vt } to represent a
sequence of 360° video frames where vi ∈ V corresponds
to frame i. In VR,
the standard format of a VR video
frame is in equirectangular projection [11]. We use a set
L1:t = {l1, l2, ..., lt } to represent
the head orientation in
V1:t frames where li ∈ L is coordinates of head rotation.
Here li = [q0, q1, q2, q3], to be speciﬁc, is named quaternion
which has a hyper-complex number of rank 4, and often
used in VR systems to describe rotations [12]. Based on the
above formulation, movement prediction aims to regress the
future movement coordinates corresponding to the future T
frames. The predicted movement set is deﬁned as Lt+1:t+T =
{lt+1, lt+2, ..., lt+T }.

The future head movement of a VR user is related to
multiple factors. On the one hand, users are more likely to
be attracted by some features in the frame. On the other hand,
history movement is also a key factor to predict the future
movement of a user because of different movement patterns.
Based on the above observations, we can pose the head
movement prediction problem as a non-linear regression prob-
lem, in which the history head movement and corresponding
frames are independent variables and future head movement
are dependent variables. We use a deep neural network for this
prediction problem (as shown in Fig. 1). Our network includes
a feature extraction module and a movement prediction mod-
ule. This model has the following advantages: i) It can exploit
important information from the video which is neglected in
the previous model, ii) It can be implemented over a mobile
device because of its lightweight in its architectures, and iii)
It can effectively learn both short and long term movement of

Proposed model
Baseline model
Static

60

50

40

30

20

10

0

l

e
g
n
a

e
d
u
t
i
t
a
L

3

Real angle
Predicted angle by baseline model
Predicted angle by proposed model

5

10

15
Angle error

20

25

30

240

245

250

255

260

270

275

280

285

290

265
Time

F
D
C

1

0.8

0.6

0.4

0.2

0

0

Fig. 2: Comparison of the CDF of the angle error.

σi = arccos (sin xi sin ˆxi + cos xi cos ˆxi cos|yi − ˆyi |) ,
(3)
We also use cumulative distribution function (CDF) of all
head orientation for performance evaluation. A higher CDF
curve corresponds to a method with smaller MAE.

In summary, we have introduced a feature extraction module
for video content and a movement prediction module for
combining content-related features and position-related feature
to predict future movement of VR users. Then, we use
MAE and CDF to evaluate our proposed model. By adding
the content-related features to our system, we can implicitly
predict future points of interest in the video. This prediction
gives an advantage to our proposed method over previous
studies for head movement prediction [4], [5].

D. Exploiting Predictions in a Wireless Network

As discussed in [15], deploying a VR system over a wireless
network will potentially strain the capacity and bandwidth
of that network, thus requiring new techniques to improve
resource management. One promising approach to overcome
this challenge, is by enabling the VR system to predict the
head movement of VR users, as proposed here. In particular,
by performing such head movement prediction, instead of
using a very large bandwidth to transmit the entirety of a
VR content to a user (e.g., the entirety of a 360° video), the
wireless network can now adaptively decide on what portion
of the VR content to deliver to its users at any given time thus
saving signiﬁcant bandwidth and resources. Also, performing
predictions of a VR user’s head movement can enable the
wireless network to pre-fetch and cache the predicted parts of
the content at the edge of the network, thus reducing latency
and bandwidth usage, particularly at backhaul links. Clearly,
the proposed prediction scheme can be leveraged in all of these
scenarios.

III. SIMULATION RESULTS AND DISCUSSIONS

A. Dataset

There exist a few datasets of head orientation for VR users.
We explore one public head movement dataset for 360° videos
[16]. This dataset
includes 5 videos viewed by 59 users.
Each head orientation is timestamped and corresponding to a
video frame. The head orientation is recorded as a quaternion,
a four-tuple mathematical representation of head orientation
with respect to a ﬁxed reference point. We use the quaternion
expression as the input and output of our model.

B. Simulation Setup

We downsample one frame from every ﬁve frames for
model training and evaluation. Hence, the interval between two

Fig. 3: Head latitude angle prediction for the proposed model
and the baseline model

F
D
C

:
r
o
r
r

l

E
e
g
n
A

1

0.8

0.6

0.4

0.2

0

0

2 Hz
3 Hz
5 Hz
10 Hz
15 Hz
30 Hz

5

10

15
Angle error

20

25

30

Fig. 4: Effect of input sample rate on the angle error.

frames in our experiments is 1
6 seconds. This setting can reduce
the memory consumption needed for training and evaluating
our model, yet it makes the head movement prediction more
challenging compared to the original frame-rate. In what
follows, the frames mentioned correspond to the downsampled
ones. We use ﬁve history head orientation data to predict head
movement in the next ﬁve frames. In other words, we use the
ﬁrst 1 second to predict the next second.

The model is implemented using the TensorFlow frame-
work. We train our network with the following hyper-
parameters setting: mini-batch size(32), Adam optimizer,
learning rate(0.001), and decay(0.0005). Our training model
is a generalizable model that applies for all users, as per
the process that we explain next. In our simulations, for
each video, we randomly select 70% of the users’ movement
behavior for training data, out of which 10% are used for
validation and 20% for testing. The model is trained for 500
epochs and is checkpointed after the minimum validation loss.

C. Results

In Fig. 2, we compare the model we proposed with a
baseline, position-based model. The position-related model (as
shown in Fig. 1) only takes 5 past frame head orientation
data points in one second denoted by L1:5 into a stacked
LSTM to predict future movement as captured by Lt+1:t+5.
Fig. 2 shows that the proposed model yields up to 16.1%
improvement in MAE compared to the baseline. Fig. 2 also
shows that both the proposed model and position-based model
perform far better than static prediction (the system assumes
the user does not move), which validates the necessity of our
prediction. Fig. 3 shows a comparison between the real latitude
angle and predicted and the predicted angles. From this ﬁgure,
we observe that our proposed model achieves higher accuracy
than the baseline. This improvement in error stems from the
fact that MobileNetV2 can extract the user’s regions of interest
from the video content, and, hence, it enables the LSTM to use

 
 
 
4

fact that future movements over an in long period are less
relevant to the current movement, and, hence the prediction
length should be limited for better performance.

Fig. 5: Reduce k data points in a time window of length n

IV. CONCLUSION AND FUTURE WORK

F
D
C

:
r
o
r
r

l

E
e
g
n
A

1

0.8

0.6

0.4

0.2

0

0

Reduce 0 data point
Reduce 1 data point
Reduce 2 data points
Reduce 3 data points
Reduce 4 data points

5

10

15
Angle error

20

25

30

Fig. 6: Effect of reduced input dataset on the angle error.

1

0.8

0.6

0.4

0.2

F
D
C

:
r
o
r
r

l

E
e
g
n
A

0

0

Predict 1 s
Predict 2 s
Predict 3 s

5

10

15
Angle error

20

25

30

Fig. 7: Effect of input duration on the angle error.

these video features and combines them with history trajectory
data to enhance the prediction accuracy.

to our network and,

Fig. 4 shows how prediction accuracy changes as the sample
rate vary. From Fig. 4, we can see that, as the sample
rate increases, the prediction accuracy increases signiﬁcantly
around 2 Hz to 15 Hz, and at 15 Hz to 30 Hz the increase
becomes slower. This is because, as the sample rate increases,
more data will be input
thus, more
features will be extracted. Fig. 4 also shows that when the
sample rate reaches 15 Hz, the prediction accuracy remains
constant or decreases as the sample rate increases. This is
because by increasing sampling frequency beyond a certain
value, input data to the model becomes mostly redundant. This
redundancy will increase the model complexity as well as the
time to execute our algorithm, but it cannot improve model
accuracy. Thus, such sample rate cannot meet the low latency
requirement for HMD operation and guarantee user’s quality
of experience. Under a computation memory constraint and
time, we need to reduce the history data transmitted to the
model and also achieve good prediction accuracy. Taking the
sample rate as 5 Hz can be a good compromise of these two
factors.

Fig. 6 shows the performance of our proposed model on
a reduced dataset with a sample rate of 5 Hz. We remove k
data points in the middle of a time window of length n, as
shown in Fig. 5. From Fig. 6, we can see that the prediction
accuracy varies little as the reduced data length increases from
0 to 3. This implies that we can save computation memory and
upload bandwidth by reducing the input data and achieve the
same performance as before.

Fig. 7 shows how our model’s performance changes as the
prediction length vary. From Fig. 7, we observe that it is
difﬁcult to predict long-term movement. This is due to the

In this paper, we have proposed neural network architecture
for predicting the head movement of VR users. First, we have
introduced a network that uses CNN to extract the features of
video content, and an LSTM to predict the future movement
of users. Simulation results have shown that the proposed
approach yields an improvement in prediction accuracy com-
pared to the position-related only one. Next, we have found
that downsample or reduced input dataset can meet the con-
straint of computation memory and upload bandwidth while
maintaining the prediction accuracy. Extensive experiments
also validate the effectiveness of our approaches. In future
work, by considering motions between video frames, training
with a larger dataset and using eye-tracking HMD may further
boost the performance. Another important future work is to
conduct actual experiments with real VR users to further
validate our approach.

REFERENCES

[1] M. Chen, W. Saad, and C. Yin, “Virtual reality over wireless networks:
Quality-of-service model and learning-based resource management,”
IEEE Trans. Commun., vol. 66, no. 11, pp. 5621–5635, 2018.

[2] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems:
Applications, trends, technologies, and open research problems,” IEEE
Netw., 2020, to appear.

[3] A. Zare, A. Aminlou, M. M. Hannuksela, and M. Gabbouj, “Hevc-
compliant tile-based streaming of panoramic video for virtual reality
applications,” in Proc. of the ACM MM, Oct. 2016, pp. 601–605.
[4] F. Qian, L. Ji, B. Han, and V. Gopalakrishnan, “Optimizing 360 video
the 5th Workshop on

delivery over cellular networks,” in Proc. of
ATC@MobiCom, Oct. 2016, pp. 1–6.

[5] F. Duanmu, E. Kurdoglu, S. A. Hosseini, Y. Liu, and Y. Wang,
“Prioritized buffer control in two-tier 360 video streaming,” in Proc. of
the Workshop on VR/AR Network@SIGCOMM, Aug. 2017, pp. 13–18.
[6] A. D. Aladagli, E. Ekmekcioglu, D. Jarnikov, and A. M. Kondoz,
“Predicting head trajectories in 360° virtual reality videos,” in Proc.
of IC3D, Dec. 2017, pp. 1–6.

[7] C. Fan, J. Lee, W. Lo, C. Huang, K. Chen, and C. Hsu, “Fixation
prediction for 360° video streaming in head-mounted virtual reality,”
in Proc. of the 27th Workshop on NOSSDAV, June 2017, pp. 67–72.
[8] M. Assens, X. Gir´o i Nieto, K. McGuinness, and N. E. O’Connor,
“Saltinet: Scan-path prediction on 360° images using saliency volumes,”
in Proc. of IEEE ICCV Workshops, Oct. 2017, pp. 2331–2338.

[9] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique: De-
tecting 360° video saliency in head-mounted display for head movement
prediction,” in Proc. of ACM MM, Oct. 2018, pp. 1190–1198.

[10] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and S. Gao, “Gaze
prediction in dynamic 360° immersive videos,” in Proc. of IEEE CVPR,
June 2018, pp. 5333–5342.

[11] F. E. Sandnes and Y. Huang, “Translating the viewing position in single
equirectangular panoramic images,” in Proc. of IEEE Int. Conf. on SMC,
Oct. 2016, pp. 389–394.

[12] R. Mukundan, “Quaternions: From classical mechanics to computer
graphics, and beyond,” in Proc. of the 7th Asian Technol. conf. in Math.,
2002, pp. 97–105.

[13] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proc. of
IEEE CVPR, June 2018, pp. 4510–4520.

[14] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artiﬁcial neural
networks-based machine learning for wireless networks: A tutorial,”
IEEE Commun. Surv. Tut., 2019, to appear.

[15] F. Hu, Y. Deng, W. Saad, M. Bennis, and A. H. Aghvami, “Cellular-
connected wireless virtual reality: Requirements, challenges, and solu-
tions,” arXiv preprint arXiv:2001.06287, 2020.

[16] X. Corbillon, F. D. Simone, and G. Simon, “360° video head movement
dataset,” in Proc. of the 8th ACM MMSys, June 2017, pp. 199–204.

 
 
 
 
