Larger is Better: The Effect of Learning Rates

1

Enjoyed by Stochastic Optimization with

Progressive Variance Reduction

Department of Computer Science and Engineering, The Chinese University of Hong Kong

Fanhua Shang

fhshang@cse.cuhk.edu.hk

October 16, 2018

Abstract

In this paper, we propose a simple variant of the original stochastic variance reduction gradient (SVRG) [1], where

hereafter we refer to as the variance reduced stochastic gradient descent (VR-SGD). Different from the choices of the

snapshot point and starting point in SVRG and its proximal variant, Prox-SVRG [2], the two vectors of each epoch in VR-

SGD are set to the average and last iterate of the previous epoch, respectively. This setting allows us to use much larger

learning rates or step sizes than SVRG, e.g., 3/(7L) for VR-SGD vs. 1/(10L) for SVRG, and also makes our convergence

analysis more challenging. In fact, a larger learning rate enjoyed by VR-SGD means that the variance of its stochastic

gradient estimator asymptotically approaches zero more rapidly. Unlike common stochastic methods such as SVRG and

proximal stochastic methods such as Prox-SVRG, we design two different update rules for smooth and non-smooth objective

functions, respectively. In other words, VR-SGD can tackle non-smooth and/or non-strongly convex problems directly without

using any reduction techniques such as quadratic regularizers. Moreover, we analyze the convergence properties of VR-SGD

for strongly convex problems, which show that VR-SGD attains a linear convergence rate. We also provide the convergence

guarantees of VR-SGD for non-strongly convex problems. Experimental results show that the performance of VR-SGD is

signiﬁcantly better than its counterparts, SVRG and Prox-SVRG, and it is also much better than the best known stochastic

method, Katyusha [3].

Index Terms

Stochastic optimization, stochastic gradient descent (SGD), proximal stochastic gradient, variance reduction, iterate

averaging, snapshot and starting points, strongly convex and non-strongly convex, smooth and non-smooth

(cid:70)

7
1
0
2

r
p
A
7
1

]

G
L
.
s
c
[

1
v
6
6
9
4
0
.
4
0
7
1
:
v
i
X
r
a

• All the codes of VR-SGD and some related variance reduced stochastic methods can be downloaded from the author’s website: https://sites.google.com/site/

fanhua217/publications.

 
 
 
 
 
 
1 INTRODUCTION

In this paper, we focus on the following composite convex optimization problem:

F (x) def=

min
x∈Rd

1
n

n
(cid:88)

i=1

fi(x) + g(x)

2

(1)

where f (x) := 1
n

i=1fi(x), fi(x) : Rd → R, i = 1, . . . , n are the smooth convex functions, and g(x) is a relatively simple
(but possibly non-differentiable) convex function (referred to as a regularizer). The formulation (1) arises in many places

(cid:80)n

in machine learning, signal processing, data science, statistics and operations research, such as regularized empirical risk
minimization (ERM). For instance, one popular choice of the component function fi(·) in binary classiﬁcation problems is
the logistic loss, i.e., fi(x) = log(1 + exp(−biaT
i x)), where {(a1, b1), . . . , (an, bn)} is a collection of training examples, and
bi ∈ {±1}. Some popular choices for the regularizer include the (cid:96)2-norm regularizer (i.e., g(x) = (λ1/2)(cid:107)x(cid:107)2), the (cid:96)1-norm
regularizer (i.e., g(x) = λ2(cid:107)x(cid:107)1), and the elastic-net regularizer (i.e., g(x) = (λ1/2)(cid:107)x(cid:107)2 +λ2(cid:107)x(cid:107)1), where λ1 ≥ 0 and λ2 ≥ 0
are two regularization parameters. So far examples of some other applications include deep neural networks [1], [4], [5],

[6], group Lasso [7], [8], sparse learning and coding [9], [10], phase retrieval [11], matrix completion [12], [13], conditional

random ﬁelds [14], eigenvector computation [15], [16] such as principal component analysis (PCA) and singular value

decomposition (SVD), generalized eigen-decomposition and canonical correlation analysis (CCA) [17].

1.1 Stochastic Gradient Descent

In this paper, we are especially interested in developing efﬁcient algorithms to solve regularized ERM problems involving a
large sum of n component functions. The standard and effective method for solving Problem (1) is the (proximal) gradient

descent (GD) method, including accelerated proximal gradient (APG) [18], [19], [20], [21]. For smooth objective functions,

the update rule of GD is

(cid:34)

xk+1 = xk − ηk

1
n

n
(cid:88)

∇fi(xk) + ∇g(xk)

(cid:35)

(2)

i=1
for k = 1, 2, . . ., where ηk > 0 is commonly referred to as the step-size in optimization or the learning rate in machine
learning. When the regularizer g(·) is non-smooth, e.g., the (cid:96)1-norm regularizer, we need to introduce the following proximal

operator into (2),

xk+1 = Prox ηk,g(yk) := arg min

x∈Rd

(1/2ηk)·(cid:107)x − yk(cid:107)2 + g(x)

(3)

where yk = xk −(ηk/n) (cid:80)n
i=1∇fi(xk). The GD methods mentioned above have been proven to achieve linear convergence
for strongly convex problems, and APG attains the optimal convergence rate of O(1/T 2) for non-strongly convex problems,
where T denotes the number of iterations. However, the per-iteration cost of all the batch (or deterministic) methods is
O(nd), which is expensive.

Instead of evaluating the full gradient of f (·) at each iteration, an effective alternative is the stochastic (or incremental)

gradient descent (SGD) method [22]. SGD only evaluates the gradient of a single component function at each iteration, thus
it has much lower per-iteration cost, O(d), and has been successfully applied to many large-scale learning problems [4],

[23], [24], [25]. The update rule of SGD is formulated as follows:

xk+1 = xk − ηk[∇fik(xk) + ∇g(xk)]

(4)

where ηk ∝ 1/k, and the index ik is chosen uniformly at random from {1, . . . , n}. Although the expectation of ∇fik(xk) is
an unbiased estimation for ∇f (xk), i.e., E[∇fik(xk)] = ∇f (xk), the variance of the stochastic gradient estimator ∇fik(xk) may
be large due to the variance of random sampling [1]. Thus, stochastic gradient estimators are also called “noisy gradients”,

and we need to gradually reduce its step size, leading to slow convergence. In particular, even under the strongly convex
condition, standard SGD attains a slower sub-linear convergence rate of O(1/T ) [26], [27].

3

1.2 Accelerated SGD

Recently, many SGD methods with variance reduction techniques were proposed, such as stochastic average gradient

(SAG) [28], stochastic variance reduced gradient (SVRG) [1], stochastic dual coordinate ascent (SDCA) [29], SAGA [30],

stochastic primal-dual coordinate (SPDC) [31], and their proximal variants, such as Prox-SAG [32], Prox-SVRG [2] and
Prox-SDCA [33]. All these accelerated SGD methods can use a constant step size η instead of diminishing step sizes for

SGD, and fall into the following three categories: primal methods such as SVRG and SAGA, dual methods such as SDCA,
and primal-dual methods such as SPDC. In essence, many of primal methods use the full gradient at the snapshot (cid:101)x or
average gradients to progressively reduce the variance of stochastic gradient estimators, as well as dual and primal-dual

methods, which leads to a revolution in the area of ﬁrst-order methods [34]. Thus, they are also known as the hybrid gradient

descent method [35] or semi-stochastic gradient descent method [36]. In particular, under the strongly convex condition, the
accelerated SGD methods enjoy linear convergence rates and the overall complexity of O((n+L/µ) log(1/(cid:15))) to obtain an
(cid:15)-suboptimal solution, where each fi(·) is L-smooth and g(·) is µ-strongly convex. The complexity bound shows that they
converge signiﬁcantly faster than deterministic APG methods, whose complexity is O((n(cid:112)L/µ) log(1/(cid:15))) [36].

SVRG [1] and its proximal variant, Prox-SVRG [2], are particularly attractive because of their low storage requirement
compared with other stochastic methods such as SAG, SAGA and SDCA, which require to store all the gradients of the n
component functions fi(·) or dual variables. At the beginning of each epoch in SVRG, the full gradient ∇f ((cid:101)x) is computed
at the snapshot point (cid:101)x, which is updated periodically. The update rule for the smooth optimization problem (1) is given by

(cid:101)∇fik(xk) = ∇fik(xk) − ∇fik((cid:101)x) + ∇f ((cid:101)x),
xk+1 = xk − η[ (cid:101)∇fik(xk) + ∇g(xk)].

(5a)

(5b)

When g(·) ≡ 0, the update rule in (5b) becomes the original one in [1], i.e., xk+1 = xk −η (cid:101)∇fik(xk). It is not hard to verify
that the variance of the SVRG estimator (cid:101)∇fik(xk), i.e., E(cid:107) (cid:101)∇fik(xk)−∇f (xk)(cid:107)2, can be much smaller than that of the SGD
estimator ∇fik(xk), i.e., E(cid:107)∇fik(xk)−∇f (xk)(cid:107)2. However, for non-strongly convex problems, the accelerated SGD methods
mentioned above converge much slower than batch APG methods such as FISTA [21], namely, O(1/T ) vs. O(1/T 2).

More recently, many acceleration techniques were proposed to further speed up those variance-reduced stochastic

methods mentioned above. These techniques mainly include the Nesterov’s acceleration technique used in [24], [37], [38],

[39], [40], reducing the number of gradient calculations in the early iterations [34], [41], [42], the projection-free property of

the conditional gradient method (also known as the Frank-Wolfe algorithm [43]) as in [44], the stochastic sufﬁcient decrease

technique [45], and the momentum acceleration trick in [3], [34], [46]. More speciﬁcally, [40] proposed an accelerating
Catalyst framework and achieved the complexity of O((n+(cid:112)nL/µ) log(L/µ) log(1/(cid:15))) for strongly convex problems. [3]
and [46] proved that their accelerated methods can attain the best known complexity of O(n log(1/(cid:15))+(cid:112)nL/(cid:15)) for non-
strongly convex problems. The overall complexity matches the theoretical upper bound provided in [47]. Katyusha [3] and
point-SAGA [48] achieve the best-known complexity of O((n+(cid:112)nL/µ) log(1/(cid:15))) for strongly convex problems, which is
identical to the upper complexity bound in [47]. That is, Katyusha is the best known stochastic optimization method for both

strongly convex and non-strongly convex problems. Its proximal gradient update rules are formulated as follows:

xk+1 = w1yk + w2 (cid:101)x + (1 − w1 − w2)zk,

yk+1 = arg min

y∈Rd

zk+1 = arg min

z∈Rd

(cid:26) 1
2η
(cid:26) 3L
2

(cid:107)y − yk(cid:107)2 + yT (cid:101)∇fik(xk+1) + g(y)

,

(cid:27)

(cid:107)z − xk+1(cid:107)2 + zT (cid:101)∇fik(xk+1) + g(z)

(cid:27)

(6a)

(6b)

(6c)

where w1, w2 ∈ [0, 1] are two momentum parameters. To eliminate the need for parameter tuning, η is set to 1/(3w1L),
and w2 is ﬁxed to 0.5 in [3]. Unfortunately, most of the accelerated methods mentioned above, including Katyusha, require

at least two auxiliary variables and two momentum parameters, which lead to complicated algorithm design and high

per-iteration complexity [34].

4

1.3 Our Contributions

From the above discussion, one can see that most of accelerated stochastic variance reduction methods such as [3], [34],

[37], [42], [44], [45] and applications such as [9], [10], [13], [15], [16], [49] are based on the stochastic variance reduced gradient

(SVRG) method [1]. Thus, any key improvement on SVRG is very important for the research of stochastic optimization.

In this paper, we propose a simple variant of the original SVRG [1], which is referred to as the variance reduced stochastic

gradient descent (VR-SGD). The snapshot point and starting point of each epoch in VR-SGD are set to the average and last

iterate of the previous epoch, respectively. Different from the settings of SVRG and Prox-SVRG [2] (i.e., the last iterate for

the two points of the former, while the average of the previous epoch for those of the latter), the two points in VR-SGD are

different, which makes our convergence analysis more challenging than SVRG and Prox-SVRG. Our empirical results show

that the performance of VR-SGD is signiﬁcantly better than its counterparts, SVRG and Prox-SVRG. Impressively, VR-SGD

with a sufﬁciently large learning rate performs much better than the best known stochastic method, Katyusha [3]. The main

contributions of this paper are summarized below.

• The snapshot point and starting point of VR-SGD are set to two different vectors. That is, for all epochs, except the

(cid:80)m−1

ﬁrst one, (cid:101)xs = 1
m. In
particular, we ﬁnd that the setting of VR-SGD allows us take much larger learning rates or step sizes than SVRG,
e.g., 3/(7L) vs. 1/(10L), and thus signiﬁcantly speeds up the convergence of SVRG and Prox-SVRG in practice.

k (denoted by Option I) or (cid:101)xs = 1

k (denoted by Option II), and xs+1

0 = xs

k=1 xs

k=1 xs

m−1

m

(cid:80)m

Moreover, VR-SGD has an advantage over SVRG in terms of robustness of learning rate selection.

• Different from proximal stochastic gradient methods, e.g., Prox-SVRG and Katyusha, which have a uniﬁed update

rule for the two cases of smooth and non-smooth objectives (see Section 2.2 for details), VR-SGD employs two different

update rules for the two cases, respectively, as in (12) and (13) below. Empirical results show that gradient update

rules as in (12) for smooth optimization problems are better choices than proximal update formulas as in (10).

•

Finally, we theoretically analyze the convergence properties of VR-SGD with Option I or Option II for strongly convex

problems, which show that VR-SGD attains a linear convergence rate. We also give the convergence guarantees of

VR-SGD with Option I or Option II for non-strongly convex objective functions.

2 PRELIMINARY AND RELATED WORK

Throughout this paper, we use (cid:107) · (cid:107) to denote the (cid:96)2-norm (also known as the standard Euclidean norm), and (cid:107)·(cid:107)1 is the
(cid:96)1-norm, i.e., (cid:107)x(cid:107)1 = (cid:80)d
i=1|xi|. ∇f (·) denotes the full gradient of f (·) if it is differentiable, or ∂f (·) the subgradient if f (·)
is only Lipschitz continuous. For each epoch s ∈ [S] and inner iteration k ∈ {0, 1, . . . , m−1}, is
k ∈ [n] is the random chosen
index. We mostly focus on the case of Problem (1) when each component function fi(·) is L-smooth1, and F (·) is µ-strongly

convex. The two common assumptions are deﬁned as follows.

2.1 Basic Assumptions

Assumption 1 (Smoothness). Each convex function fi(·) is L-smooth, that is, there exists a constant L > 0 such that for all
x, y ∈ Rd,

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107).

(7)

Assumption 2 (Strong Convexity). The convex function F (x) is µ-strongly convex, i.e., there exists a constant µ > 0 such that for
all x, y ∈ Rd,

F (y) ≥ F (x) + (cid:104)∇F (x), y − x(cid:105) +

(cid:107)x − y(cid:107)2.

(8)

µ
2

Note that when the regularizer g(·) is non-smooth, the inequality in (8) needs to be revised by simply replacing the
gradient ∇F (x) in (8) with an arbitrary sub-gradient of F (·) at x. In contrast, for a non-strongly convex or general convex
function, the inequality in (8) can always be satisﬁed with µ = 0.

1. Actually, we can extend all the theoretical results in this paper for the case, when the gradients of all component functions have the same

Lipschitz constant L, to the more general case, when some fi(·) have different degrees of smoothness.

Algorithm 1 SVRG (Option I) and Prox-SVRG (Option II)
Input: The number of epochs S, the number of iterations m per epoch, and step size η.
Initialize: (cid:101)x0.
1: for s = 1, 2, . . . , S do
xs
0 = (cid:101)xs−1;
(cid:80)n
(cid:101)µs = 1
for k = 0, 1, . . . , m − 1 do

i=1∇fi((cid:101)xs−1);

2:

3:

4:

n

5:

6:

7:

8:

k

k

Pick is
k uniformly at random from [n];
(xs
k) = ∇fis
(cid:101)∇fis
Option I: xs
or xs
Option II: xs

(xs
k) − ∇fis
(cid:104)
k+1 = xs
k − η
k+1 = Prox η, g
k+1 = arg miny∈Rd

(cid:101)∇fis
(cid:16)
xs
k − η (cid:101)∇fis

((cid:101)xs−1) + (cid:101)µs;
(xs

(cid:105)
k) +∇g(xs
k)
(cid:17)
(xs
k)
g(y) + yT (cid:101)∇fis

(cid:110)

;

,

k

k

k

k

end for

9:
10: Option I: (cid:101)xs = xs
m;
(cid:80)m
11: Option II: (cid:101)xs = 1
12: end for
Output: (cid:101)xS

m

k=1xs
k;

(xs

k) + 1

2η (cid:107)y − xs

k(cid:107)2(cid:111)
;

5

% Initiate the variable xs
0
% Compute the full gradient

% The stochastic gradient estimator
% Smooth case of g(·)

% Non-smooth case of g(·)

% Proximal update

% Last iterate for snapshot (cid:101)x
% Iterate averaging for snapshot (cid:101)x

2.2 Related Work

To speed up standard and proximal SGD methods, many variance reduced stochastic methods [28], [29], [30], [35] have
been proposed for some special cases of Problem (1). In the case when each fi(x) is L-smooth, f (x) is µ-strongly convex,
and g(x) ≡ 0, Roux et al. [28] proposed a stochastic average gradient (SAG) method, which attains a linear convergence rate.

However, SAG needs to store all gradients as well as other incremental aggregated gradient methods such as SAGA [30],
so that O(nd) storage is required in general problems [41]. Similarly, SDCA [29] requires storage of all dual variables [1],
which scales as O(n). In contrast, SVRG [1], as well as its proximal variant, Prox-SVRG [2], has the similar convergence

rate to SAG and SDCA but without the memory requirements of all gradients and dual variables. In particular, the SVRG

estimator in (5a) (independently introduced in [1], [35]) may be the most popular choice for stochastic gradient estimators.

Besides, other stochastic gradient estimators include the SAGA estimator in [30] and the stochastic recursive gradient

estimator in [50]. Although the original SVRG in [1] only has convergence guarantees for a special case of Problem (1),
when each fi(x) is L-smooth, f (x) is µ-strongly convex, and g(x) ≡ 0, one can extend SVRG to the proximal setting by
introducing the proximal operator in (3), as shown in Line 7 of Algorithm 1. In other words, when g(·) is non-smooth, the

update rule of SVRG becomes

xs
k+1 = arg min

x∈Rd

(cid:110)

(1/2η)·(cid:107)x − [xs

k − η (cid:101)∇fis

k

(xs

(cid:111)
k)](cid:107)2 + g(x)

.

(9)

Some researchers [8], [51], [52] have borrowed some variance reduction techniques into ADMM for minimizing convex

composite objective functions subject to an equality constraint. [15], [16], [17] applied efﬁcient stochastic solvers to compute

leading eigenvectors of a symmetric matrix or generalized eigenvectors of two symmetric matrices. The ﬁrst such method

is VR-PCA by Shamir [15], and the convergence properties of the VR-PCA algorithm for such a non-convex problem are
also provided. Garber et al. [16] analyzed the convergence rate of SVRG when f (·) is a convex function that is a sum

of non-convex component functions. Moreover, [6] and [53] proved that SVRG and SAGA with minor modiﬁcations can

converge asymptotically to a stationary point of non-convex ﬁnite-sum problems. Some distributed variants [54], [55] of

accelerated SGD methods have also been proposed.

An important class of stochastic methods is the proximal stochastic gradient (Prox-SG) method, such as Prox-SVRG [2],

SAGA [30], and Katyusha [3]. Different from standard variance reduction SGD methods such as SVRG, which have a

6

stochastic gradient update as in (5b), the Prox-SG method has a uniﬁed update rule for both smooth and non-smooth cases
of g(·). For instance, the update rule of Prox-SVRG [2] is formulated as follows:

(cid:26)

xs
k+1 = arg min

y∈Rd

g(y) + yT (cid:101)∇fis

k

(xs

k) +

(cid:107)y − xs

k(cid:107)2

(cid:27)

.

1
2η

(10)

For the sake of completeness, the details of Prox-SVRG [2] are shown in Algorithm 1 with Option II. When g(·) is the
widely used (cid:96)2-norm regularizer, i.e., g(·) = (λ1/2)(cid:107) · (cid:107)2, the proximal update formula in (10) becomes

xs
k+1 =

(cid:104)

1
1 + λ1η

xs
k − η (cid:101)∇fis

k

(xs
k)

(cid:105)

.

(11)

3 VARIANCE-REDUCED STOCHASTIC GRADIENT DESCENT

In this section, we propose an efﬁcient variance reduced stochastic gradient descent (VR-SGD) method with iterate averaging.

Different from the choices of the snapshot and starting points in SVRG [1] and Prox-SVRG [2], the two vectors of each

epoch in VR-SGD are set to the average and last iterate of the previous epoch, respectively. Unlike common stochastic

gradient methods such as SVRG and proximal stochastic gradient methods such as Prox-SVRG, we design two different

update rules for smooth and non-smooth objective functions, respectively.

3.1 Iterate Averaging

Like SVRG and Katyusha, VR-SGD is also divided into S epochs, and each epoch consists of m stochastic gradient steps,
where m is usually chosen to be Θ(n) as in [1], [2], [3]. Within each epoch, we need to compute the full gradient ∇f ((cid:101)xs)
at the snapshot (cid:101)xs and use it to deﬁne the variance reduced stochastic gradient estimator (cid:101)∇fik(xs
k) as in [1]. Unlike SVRG
whose snapshot point is set to the last iterate of the previous epoch, the snapshot (cid:101)xs of each epoch in VR-SGD is set to
the average of the previous epoch, e.g., (cid:101)xs = 1
k=1 xs
k in Option I of Algorithm 2, which leads to better robustness to
gradient noise2, as also suggested in [34], [45], [59]. In fact, the choice of Option II in Algorithm 2, i.e., (cid:101)xs = 1
k=1 xs
k,
also works well in practice, as shown in Fig. 1. Therefore, we provide the convergence guarantees for both our algorithms

(cid:80)m−1

(cid:80)m

m−1

m

(including Algorithm 2) with Option I and our algorithms with Option II in the next section. In particular, we ﬁnd that

the one of the effects of the choice in Option I or Option II of Algorithm 2 is to allow taking much larger learning rates
or step sizes than SVRG in practice, e.g., 3/(7L) for VR-SGD vs. 1/(10L) for SVRG (see Fig. 2 and Section 5.3 for details).

This is the main reason why VR-SGD converges signiﬁcantly faster than SVRG. Actually, a larger learning rate enjoyed by

VR-SGD means that the variance of its stochastic gradient estimator goes asymptotically to zero faster.

Unlike Prox-SVRG [2] whose starting point is initialized to the average of the previous epoch, the starting point xs+1
m of the previous epoch. That is, the last iterate of the previous
epoch becomes the new starting point in VR-SGD, while those of Prox-SVRG are completely different, thereby leading

of each epoch in VR-SGD is set to the last iterate xs

0

to relatively slow convergence in general. It is clear that both the starting point and snapshot point of each epoch in the
original SVRG [1] are set to the last iterate of the previous epoch3, while the two points of Prox-SVRG [2] are set to the

average of the previous epoch (also suggested in [1]). Different from the settings in SVRG and Prox-SVRG, the starting and

snapshot points in VR-SGD are set to the two different vectors mentioned above, which makes the convergence analysis of

VR-SGD more challenging than SVRG and Prox-SVRG, as shown in Section 4.

2. It should be emphasized that the noise introduced by random sampling is inevitable, and generally slows down the convergence speed in

a sense. However, SGD and its variants are probably the most used optimization algorithms for deep learning [56]. In particular, [57] has shown

that by adding noise at each step, noisy gradient descent can escape the saddle points efﬁciently and converge to a local minimum of non-convex

optimization problems, as well as the application of deep neural networks in [58].

3. Note that the theoretical convergence of the original SVRG [1] relies on its Option II, i.e., both (cid:101)xs and xs+1

k, where k is randomly
chosen from {1, 2, . . . , m}. However, the empirical results in [1] suggest that Option I is a better choice than its Option II, and the convergence
guarantee of SVRG with Option I for strongly convex objective functions is provided in [60].

are set to xs

0

Algorithm 2 VR-SGD for strongly convex objectives
Input: The number of epochs S, the number of iterations m per epoch, and step size η.
Initialize: x1
1: for s = 1, 2, . . . , S do

0 = (cid:101)x0.

2:

3:

4:

5:

6:

(cid:80)n

(cid:101)µs = 1
for k = 0, 1, . . . , m − 1 do

i=1∇fi((cid:101)xs−1);

n

k

Pick is
k uniformly at random from [n];
(xs
k) = ∇fis
(cid:101)∇fis
(cid:104)
k+1 = xs
xs
k − η
or xs
k+1 = Prox η, g

((cid:101)xs−1) + (cid:101)µs;
(cid:105)
k) + ∇g(xs
k)
,
(cid:17)
xs
(xs
k − η (cid:101)∇fis
k)

k) − ∇fis
(xs
(cid:101)∇fis
k
(cid:16)

(xs

;

k

k

k

end for

7:
8: Option I: (cid:101)xs = 1
9: Option II: (cid:101)xs = 1
xs+1
0 = xs
m;
10:

m

m−1

(cid:80)m−1

k=1 xs
k;
k=1xs
k;

(cid:80)m

7

% Compute the full gradient

% The stochastic gradient estimator
% Smooth case of g(·)

% Non-smooth case of g(·)

% Iterate averaging for snapshot (cid:101)x
% Iterate averaging for snapshot (cid:101)x
% Initiate xs+1
for the next epoch

0

11: end for
Output: (cid:98)xS = (cid:101)xS if F ((cid:101)xS) ≤ F ( 1

S

(cid:80)S

s=1 (cid:101)xs), and (cid:98)xS = 1

S

(cid:80)S

s=1 (cid:101)xs otherwise.

(a) Logistic regression

(b) Ridge regression

Fig. 1. Comparison of VR-SGD with Option I (denoted by VR-SGD-I) and Option II (denoted by VR-SGD-II) for solving (cid:96)2-norm (i.e., (λ/2)(cid:107) · (cid:107)2)
regularized logistic regression and ridge regression problems on the Covtype data set. In each plot, the vertical axis shows the objective value
minus the minimum, and the horizontal axis is the number of effective passes. Note that the blue lines stand for the results where λ = 10−4, while
the red lines correspond to the results where λ = 10−5 (best viewed in colors).

3.2 The Algorithm for Strongly Convex Objectives

In this part, we propose an efﬁcient VR-SGD algorithm to solve strongly convex objective functions, as outlined in Algorithm

2. It is well known that the original SVRG [1] only works for the case of smooth and strongly convex objective functions.

However, in many machine learning applications, e.g., elastic net regularized logistic regression, the strongly convex
objective function F (x) is non-smooth. To solve this class of problems, the proximal variant of SVRG, Prox-SVRG [2],

was subsequently proposed. Unlike SVRG and Prox-SVRG, VR-SGD can not only solve smooth objective functions, but
directly tackle non-smooth ones. That is, when the regularizer g(x) is smooth, e.g., the (cid:96)2-norm regularizer, the update rule

of VR-SGD is

k+1 = xs
xs

k − η[ (cid:101)∇fis

k

(xs

k) + ∇g(xs

k)].

(12)

024681010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )VR−SGD−IVR−SGD−IIVR−SGD−IVR−SGD−II024681010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )VR−SGD−IVR−SGD−IIVR−SGD−IVR−SGD−II8

(a) Logistic regression: λ = 10−4 (left) and λ = 10−5 (right)

(b) Ridge regression: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 2. Comparison of SVRG [1] and VR-SGD with different learning rates for solving (cid:96)2-norm (i.e., (λ/2)(cid:107) · (cid:107)2) regularized logistic regression and
ridge regression problems on the Covtype data set. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal

axis is the number of effective passes. Note that the blue lines stand for the results of SVRG with different learning rates, while the red lines

correspond to the results of VR-SGD with different learning rates (best viewed in colors).

When g(x) is non-smooth, e.g., the (cid:96)1-norm regularizer, the update rule of VR-SGD becomes

xs
k+1 = Prox η, g

(cid:16)

xs
k − η (cid:101)∇fis

k

(cid:17)
(xs
k)

.

(13)

Different from the proximal stochastic gradient methods such as Prox-SVRG [2], all of which have a uniﬁed update rule
as in (10) for both the smooth and non-smooth cases of g(·), VR-SGD has two different update rules for the two cases, as

stated in (12) and (13). This leads to the following advantage over the Prox-SG methods: the stochastic gradient update

rule in (12) usually outperforms the proximal stochastic gradient update rule in (11), as well as the two classes of update

rules for Katyusha [3] (see Section 5.4 for details).

Fig. 2 demonstrates that VR-SGD has a signiﬁcant advantage over SVRG in terms of robustness of learning rate selection.
That is, VR-SGD yields good performance within the range of the learning rate between 0.1/L and 0.4/L, whereas the

performance of SVRG is very sensitive to the selection of learning rates. Thus, VR-SGD is convenient to apply in various

real-world problems of large-scale machine learning. In fact, VR-SGD can use much larger learning rates than SVRG for
logistic regression problems in practice, e.g., 6/(5L) for VR-SGD vs. 1/(10L) for SVRG, as shown in Fig. 2(a).

05101520253010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  η=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=6/5L0510152025303510−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  η=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.5/Lη=6/5L010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )η=0.05/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.05/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/L010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )η=0.05/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/Lη=0.05/Lη=0.1/Lη=0.2/Lη=0.3/Lη=0.4/L9

3.3 The Algorithm for Non-Strongly Convex Objectives

Although many variance reduced stochastic methods have been proposed, most of them, including SVRG and Prox-
SVRG, only have convergence guarantees for the case of Problem (1) when the objective function F (x) is strongly convex.
However, F (x) may be non-strongly convex in many machine learning applications such as Lasso and (cid:96)1-norm regularized

logistic regression. As suggested in [3], [61], this class of problems can be transformed into strongly convex ones by adding
0(cid:107)2, which can be efﬁciently solved by Algorithm 2. However, the reduction technique may
a proximal term (τ /2)(cid:107)x − xs
degrade the performance of the involved algorithms both in theory and in practice [42]. Thus, we present an efﬁcient

VR-SGD algorithm for directly solving the non-strongly convex problem (1), as outlined in Algorithm 3.

The main difference between Algorithm 2 and Algorithm 3 is the setting for the learning rate. Similar to Algorithm 2,
the learning rate ηs of Algorithm 3 can also be ﬁxed to a constant. Inspired by existing accelerated stochastic algorithms [3],
[34], the learning rate ηs in Algorithm 3 can be gradually increased, which in principle leads to faster convergence (see

Section 5.5 for details). Different from existing stochastic methods such as Katyusha [3], the update rule of the learning rate
ηs for Algorithm 3 is deﬁned as follows: η0 = c, where c > 0 is an initial learning rate, and for any s ≥ 1,

ηs = η0/ max{α, 2/(s + 1)}

(14)

where 0 < α ≤ 1 is a given constant, e.g., α = 0.2.

3.4 Complexity Analysis

k

((cid:101)xs−1), and ∇g(xs

k),
From Algorithms 2 and 3, we can see that the per-iteration cost of VR-SGD is dominated by the computation of ∇fis
∇fis
k) or the proximal update in (13). Thus, the complexity is O(d), as low as that of SVRG [1] and
Prox-SVRG [2]. In fact, for some ERM problems, we can save the intermediate gradients ∇fi((cid:101)xs−1) in the computation of
(cid:101)µs, which generally requires O(n) additional storage. As a result, each epoch only requires (n + m) component gradient
evaluations. In addition, for extremely sparse data, we can introduce the lazy update tricks in [36], [62], [63] to our

k

(xs

algorithms, and perform the update steps in (12) and (13) only for the non-zero dimensions of each example, rather than
all dimensions. In other words, the per-iteration complexity of VR-SGD can be improved from O(d) to O(d(cid:48)), where d(cid:48) ≤ d

is the sparsity of feature vectors. Moreover, VR-SGD has a much lower per-iteration complexity than existing accelerated

stochastic variance reduction methods such as Katyusha [3], which have at least two more update rules for additional

variables, as shown in (6a)-(6c).

3.5 Extensions of VR-SGD

It has been shown in [36], [37] that mini-batching can effectively decrease the variance of stochastic gradient estimates. In

this part, we ﬁrst extend the proposed VR-SGD method to the mini-batch setting, as well as its convergence results below.
Here, we denote by b the mini-batch size and I s
k the selected random index set Ik ⊂ [n] for each outer-iteration s ∈ [S] and
inner-iteration k ∈ {0, 1, . . . , m−1}. The variance reduced stochastic gradient estimator in (5a) becomes

(cid:101)∇fI s

k

(xs

k) =

1
b

(cid:88)

i∈I s
k

(cid:2)∇fi(xs

k)−∇fi((cid:101)xs−1)(cid:3)+∇f ((cid:101)xs−1)

where I s

k ⊂ [n] is a mini-batch of size b. If some component functions are non-smooth, we can use the proximal operator
oracle [61] or the Nesterov’s smoothing [64] and homotopy smoothing [65] techniques to smoothen them, and thereby
obtain the smoothed approximations of the functions fi(x). In addition, we can directly extend our algorithms to the

non-smooth setting as in [34], e.g., Algorithm 3 in [34].

Considering that each component function fi(x) maybe have different degrees of smoothness, picking the random index

is
k from a non-uniform distribution is a much better choice than the commonly used uniform random sampling [66], [67],
as well as without-replacement sampling vs. with-replacement sampling [27]. This can be done using the same techniques
in [2], [3], i.e., the sampling probabilities for all fi(x) are proportional to their Lipschitz constants, i.e., pi = Li/ (cid:80)n
j=1Lj.

Algorithm 3 VR-SGD for non-strongly convex objectives
Input: The number of epochs S, and the number of iterations m per epoch.
Initialize: x1
1: for s = 1, 2, . . . , S do

0 = (cid:101)x0, α > 0 and η0.

2:

3:

4:

5:

6:

7:

(cid:80)n

n

i=1∇fi((cid:101)xs−1);

(cid:101)µs = 1
ηs = η0/ max{α, 2/(s+1)};
for k = 0, 1, . . . , m − 1 do

k

Pick is
k uniformly at random from [n];
(xs
k) = ∇fis
(cid:101)∇fis
k
(cid:104)
xs
k+1 = xs
k − ηs
or xs
k+1 = Prox ηs, g

((cid:101)xs−1) + (cid:101)µs;
(cid:105)
k) + ∇g(xs
k)
,
(cid:17)
xs
(xs
k − ηs (cid:101)∇fis
k)

k) − ∇fis
(xs
(cid:101)∇fis
k
(cid:16)

(xs

;

k

k

end for

8:
9: Option I: (cid:101)xs = 1
10: Option II: (cid:101)xs = 1
xs+1
0 = xs
m;
11:

m

m−1

(cid:80)m−1

k=1 xs
k;
k=1xs
k;

(cid:80)m

10

% Compute full gradient
% Compute step sizes

% The stochastic gradient estimator
% Smooth case of g(·)

% Non-smooth case of g(·)

% Iterate averaging for snapshot (cid:101)x
% Iterate averaging for snapshot (cid:101)x
% Initiate xs+1
for the next epoch

0

12: end for
Output: (cid:98)xS = (cid:101)xS if F ((cid:101)xS) ≤ F ( 1

S

(cid:80)S

s=1 (cid:101)xs), and (cid:98)xS = 1

S

(cid:80)S

s=1 (cid:101)xs otherwise.

Moreover, our VR-SGD method can also be combined with other accelerated techniques proposed for SVRG. For instance,

the epoch length of VR-SGD can be automatically determined by the techniques in [42], [68] instead of a ﬁxed epoch

length. We can reduce the number of gradient calculations in the early iterations as in [34], [41], [42], which leads to faster

convergence in general. Moreover, we can also introduce the Nesterov’s acceleration technique as in [24], [37], [38], [39],

[40] and momentum acceleration trick as in [3], [34] to further improve the performance of VR-SGD.

4 CONVERGENCE ANALYSIS

In this section, we provide the convergence guarantees of VR-SGD for solving both smooth and non-smooth general convex

problems. We also extend these results to the mini-batch setting. Moreover, we analyze the convergence properties of VR-

SGD for solving both smooth and non-smooth strongly convex objective functions. We ﬁrst introduce the following lemma

which is useful in our analysis.

Lemma 1 (3-point property, [69]). Let ˆz be the optimal solution of the following problem,

min
z∈Rd

τ
2

(cid:107)z − z0(cid:107)2 + r(z)

where r(z) is a convex function (but possibly non-differentiable), and τ ≥ 0. Then for any z ∈ Rd, the following inequality holds

r(ˆz) +

τ
2

(cid:107)ˆz − z0(cid:107)2 ≤ r(z) +

τ
2

(cid:107)z − z0(cid:107)2 −

τ
2

(cid:107)z − ˆz(cid:107)2.

4.1 Convergence Properties for Non-strongly Convex Problems

In this part, we analyze the convergence properties of VR-SGD for solving the more general non-strongly convex problems.

Considering that the two proposed algorithms (i.e., Algorithms 2 and 3) have two different update rules for the smooth

and non-smooth problems, we give the convergence guarantees of VR-SGD for the two cases as follows.

4.1.1 Smooth Objectives

We ﬁrst give the convergence guarantee of Problem (1) when the objective function F (x) is smooth and non-strongly convex.
In order to simplify analysis, we denote F (x) by f (x), that is, fi(x) := fi(x)+g(x) for all i = 1, 2, . . . , n.

11

Lemma 2 (Variance bound of smooth objectives). Let x∗ be the optimal solution of Problem (1). Suppose Assumption 1 holds, and
let (cid:101)∇fis

((cid:101)xs−1) + ∇f ((cid:101)xs−1). Then the following inequality holds

k) := ∇fis

(xs

(xs

k

k

k

k) − ∇fis
(cid:104)

E

(cid:107) (cid:101)∇fis

k

(xs

k) − ∇f (xs

k)(cid:107)2(cid:105)

≤ 4L[f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)].

The proof of this lemma is included in APPENDIX A. Lemma 2 provides the upper bound on the expected variance

of the variance reduced gradient estimator in (5a), i.e., the SVRG estimator independently introduced in [1], [35]. For

Algorithm 3 with Option I and a ﬁxed learning rate, we give the following key result for our analysis.

Lemma 3 (Option I and smooth objectives). Let β = 1/(Lη). If each fi(·) is convex and L-smooth, then the following inequality
holds for all s = 1, 2, . . . , S,

(cid:18)

(cid:19)

1 −

2
β −1
2m
(β −1)(m−1)

≤

E[f ((cid:101)xs) − f (x∗)] +
E(cid:2)f ((cid:101)xs−1)−f (x∗)(cid:3) +

1
m−1

E[f (xs

m) − f (x∗)]

2
(β −1)(m−1)

E[f (xs

0)−f (x∗)] +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

Proof. In order to simplify notation, the stochastic gradient estimator is deﬁned as: vs
((cid:101)xs−1)+∇f ((cid:101)xs−1).
Since each component function fi(x) is L-smooth, which implies that the gradient of the average function f (x) is also L-
smooth, i.e., for all x, y ∈ Rd,

k)−∇fis

k := ∇fis

(xs

k

k

whose equivalent form is

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2.

Applying the above smoothness inequality, we have

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

(cid:11) +

k+1 − xs
k

(cid:13)
2
(cid:13)

= f (xs

k) + (cid:10)∇f (xs

k), xs

= f (xs

k) + (cid:10)vs

k, xs

k+1 − xs
k

+ (cid:10)∇f (xs

k) − vs

k, xs

(cid:13)
(cid:13)xs

(cid:13)
(cid:13)xs

L
2
Lβ
2
k+1 − xs
(cid:107)xs
L(β −1)
2

k(cid:107)2

(cid:11) +
k+1 − xs
k
Lβ
2
(cid:11) −

k+1 − xs
k

(cid:11) +

(cid:107)xs

k+1 − xs

k(cid:107)2,

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

where β = 1/(Lη) > 3 is a constant. Using Lemma 2, then we get

E

(cid:20)
(cid:10)∇f (xs
(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)f (xs
β −1

≤

k) − vs

k, xs

k+1 − xs
k

(cid:11) −

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:107)∇f (xs

k) − vs

k(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:21)

(cid:107)xs

k+1 −xs

k(cid:107)2

L(β −1)
2
L(β −1)
2

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) ,

(15)

(16)

where the ﬁrst inequality holds due to the Young’s inequality (i.e., yT z ≤ (cid:107)y(cid:107)2/(2γ)+γ(cid:107)z(cid:107)2/2 for all γ > 0 and y, z ∈ Rd),

and the second inequality follows from Lemma 2.

Substituting the inequality in (16) into the inequality in (15), and taking the expectation with respect to the random

choice is

k, we have

E[f (xs

k+1)]

≤ E[f (xs

≤ E[f (xs

(cid:20)
(cid:10)vs
k)] + E
(cid:20)
(cid:104)vs
k)] + E

k, xs

k+1 −xs
k

(cid:11) +

Lβ
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

+

2
β −1

k, x∗ −xs

k(cid:105) +

((cid:107)x∗ −xs

k(cid:107)2 − (cid:107)x∗ −xs

k+1(cid:107)2)

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)

(cid:2)f (xs
(cid:21)

Lβ
2

(cid:20) Lβ
2

+

2
β −1
(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)
2
β −1

k)−f (x∗)+f ((cid:101)xs−1)−f (x∗)(cid:3)

(cid:2)f (xs

+

≤ E[f (xs

k)] + E[(cid:104)∇f (xs

k), x∗ −xs

k(cid:105)] + E

((cid:107)x∗ −xs

k(cid:107)2 − (cid:107)x∗ −xs

≤ f (x∗) +

Lβ
2

E(cid:2)(cid:0)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) .

Here, the ﬁrst inequality holds due to the inequality in (15) and the inequality in (16); the second inequality follows
from Lemma 1 with ˆz = xs
k(cid:105); the third inequality holds
due to the fact that E[vs
k); and the last inequality follows from the convexity of the smooth function f (·), i.e.,
k), x∗ −xs
f (xs

k] = ∇f (xs
k(cid:105) ≤ f (x∗). The above inequality can be rewritten as follows:

k, τ = Lβ = 1/η, and r(z) := (cid:104)vs

k+1, z = x∗, z0 = xs

k)+(cid:104)∇f (xs

k, z − xs

12

E[f (xs

k+1)] − f (x∗) ≤

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

Summing the above inequality over k = 0, 1, . . . , m−1, then

m
(cid:88)

k=1

E[f (xs

k) − f (x∗)] ≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k−1(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k(cid:107)2(cid:3)

Due to the setting of (cid:101)xs = 1

m−1

(cid:80)m−1

k=1 xs

k in Option I, and the convexity of f (·), then we have

The left and right hand sides of the inequality in (17) can be rewritten as follows:

f ((cid:101)xs) ≤

1
m − 1

m−1
(cid:88)

k=1

f (xs

k).

m
(cid:88)

k=1

E[f (xs

k) − f (x∗)] =

m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)] ,

.

(17)

(18)

m
(cid:88)

(cid:26) 2

(cid:2)f (xs

k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k−1(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k(cid:107)2(cid:3)

β −1

2
β −1

(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3).

k=1

=

2
β −1

m−1
(cid:88)

[f (xs

k)−f (x∗)] +

k=1
(cid:80)m−1

Subtracting 2
β−1

k=1 [f (xs

k)−f (x∗)] from both sides of the inequality in (17), then we obtain

(cid:18)

1 −

2
β −1

(cid:19) m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)]

≤

2
β −1

E(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

Applying the inequality in (18), we have

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

(cid:18)

(cid:18)

1 −

1 −

≤

2
β −1

2
β −1

(cid:19)

(m−1)E[f ((cid:101)xs) − f (x∗)] + E[f (xs

m) − f (x∗)]

(cid:19) m−1
(cid:88)

k=1

E[f (xs

k) − f (x∗)] + E[f (xs

m) − f (x∗)]

≤

2
β −1

E(cid:8)f (xs

0)−f (x∗) + m[f ((cid:101)xs−1)−f (x∗)](cid:9) +

Lβ
2

Dividing both sides of the above inequality by (m−1), we arrive at

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

E[f ((cid:101)xs) − f (x∗)] +

1
m−1

E[f (xs

m) − f (x∗)]

(cid:18)

(cid:19)

1 −

2
β −1
2
(β −1)(m−1)

≤

E[f (xs

0)−f (x∗)]+

2m
(β −1)(m−1)

E(cid:2)f ((cid:101)xs−1)−f (x∗)(cid:3)+

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

This completes the proof.

The ﬁrst main result is the following theorem, which provides the convergence guarantee of VR-SGD with Option I for

solving smooth and general convex minimization problems.

Theorem 1 (Option I and smooth objectives). Suppose Assumption 1 holds. Then the following inequality holds

E

(cid:104)

(cid:105)
f ((cid:98)xS)

− f (x∗) ≤

2(m + 1)
[(β −1)(m−1)−4m+2]S

[f ((cid:101)x0) − f (x∗)] +

β(β − 1)L
2[(β −1)(m−1)−4m+2]S

(cid:107)(cid:101)x0 − x∗(cid:107)2,

where (cid:98)xS = (cid:101)xS if f ((cid:101)xS) ≤ f (xS), and xS = 1

S

(cid:80)S

s=1 (cid:101)xs. Otherwise, (cid:98)xS = xS.

Proof. Since 2/(β −1) < 1, it is easy to verify that

2
(β −1)(m−1)

{E[f (xs

m)] − f (x∗)} ≤

1
m−1

{E[f (xs

m)] − f (x∗)} .

Applying the above inequality and Lemma 3, we have

13

(19)

E[f ((cid:101)xs) − f (x∗)] +

E[f ((cid:101)xs) − f (x∗)] +

2
(β −1)(m−1)

E[f (xs

m) − f (x∗)]

1
m−1

E[f (xs

m) − f (x∗)]

(cid:19)

(cid:18)

(cid:18)

(cid:19)

1 −

2
β −1
2
β −1
2
(β −1)(m−1)

1 −

≤

≤

E[f (xs

0) − f (x∗)] +

2m
(β −1)(m−1)

E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Summing the above inequality over s = 1, 2, . . . , S, taking expectation with respect to the history of random variables is
k,
and using the setting of xs+1

m, we obtain

0 = xs
(cid:19)

2
β −1

E[f ((cid:101)xs) − f (x∗)]

S
(cid:88)

(cid:18)

1 −

s=1
S
(cid:88)

(cid:26)

s=1

≤

2
(β −1)(m−1)

E[f (xs

0) − f (x∗) − (f (xs

m) − f (x∗))] +

2m
(β −1)(m−1)

(cid:27)
E[f ((cid:101)xs−1) − f (x∗)]

+

Lβ
2(m−1)

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting

2m
(β−1)(m−1)

(cid:80)S−1

s=1 [f ((cid:101)xs)−f (x∗)] from both sides of the above inequality, we have

≤

2m
(β −1)(m−1)
2
(β −1)(m−1)
Lβ
(cid:104)
E
2(m − 1)

+

E[f ((cid:101)xS) − f (x∗)] +

S
(cid:88)

(cid:18)

1 −

s=1

4
β −1

−

2
(β −1)(m−1)

(cid:19)

E[f ((cid:101)xs) − f (x∗)]

(cid:104)
E

f (x1

0) − f (x∗) − (f (xS
m) − f (x∗))
m(cid:107)2(cid:105)

0(cid:107)2 − (cid:107)x∗ − xS

.

(cid:107)x∗ − x1

(cid:105)

+

2m
(β −1)(m−1)

E[f ((cid:101)x0) − f (x∗)]

Dividing both sides of the above inequality by S, and using the setting of (cid:101)x0 = x1
2
(β −1)(m−1)

E[f ((cid:101)xs) − f (x∗)]

4
β −1

(cid:19) S
(cid:88)

1 −

1
S

−

(cid:18)

s=1

0, we arrive at

≤

≤

=

2m
(β −1)(m−1)S

2
(β −1)(m−1)S
2(m + 1)
(β −1)(m−1)S

E[f ((cid:101)xS) − f (x∗)] +

(cid:18)

1
S

1 −

4
β −1

−

2
(β −1)(m−1)

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

[f (x1

0) − f (x∗)] +

2m
(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)x∗ − x1

0(cid:107)2

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2,

where the ﬁrst inequality holds due to the fact that f ((cid:101)xS) − f (x∗) ≥ 0; the second inequality holds due to the facts that
m(cid:107)2 ≥ 0; and the last equality follows from the setting of (cid:101)x0 = x1
f (xS
0.

m)−f (x∗) ≥ 0 and (cid:107)x∗ −xS
Due to the deﬁnition of xS and the convexity of f (·), we have f (xS) ≤ 1
S

s=1 f ((cid:101)xs), and therefore the above inequality

(cid:80)S

becomes:

≤

(cid:18)

−

1 −

4
β −1
2(m + 1)
(β −1)(m−1)S

2
(β −1)(m−1)

(cid:19)

(cid:104)
E

(cid:105)
f (xS) − f (x∗)

[f ((cid:101)x0) − f (x∗)] +

Lβ
2(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Dividing both sides of the above inequality by c1 = 1− 4

β−1 −

2

(β−1)(m−1) > 0, we have

(cid:104)
E

(cid:105)
f (xS)

− f (x∗) ≤

2(m + 1)
c1(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

Lβ
2c1(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

(20)

Due to the setting for the output of Algorithm 3, (cid:98)xS = (cid:101)xS if f ((cid:101)xS) ≤ f (xS). Then
(cid:105)

(cid:104)
E

(cid:105)
f ((cid:98)xS)

(cid:104)
− f (x∗) ≤ E

f (xS)

− f (x∗) ≤

2(m + 1)
c1(β −1)(m−1)S

[f ((cid:101)x0) − f (x∗)] +

14

Lβ
2c1(m−1)S

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Alternatively, when f ((cid:101)xS) ≥ f (xS), let (cid:98)xS = xS, and the above inequality still holds. This completes the proof.

From Lemma 3, Theorem 1, and their proofs, one can see that our convergence analysis is very different from those of

existing stochastic methods, such as SVRG [1] and Prox-SVRG [2]. For Algorithm 3 with Option II and a ﬁxed learning rate,

we give the following key lemma for our convergence analysis.

Lemma 4 (Option II and smooth objectives). If each fi(·) is convex and L-smooth, then the following inequality holds for all
s = 1, 2, . . . , S,

(cid:18)

1 −

≤

2
(β −1)

(cid:19)

E[f ((cid:101)xs) − f (x∗)] +

2
β −1
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

2
(β −1)m
2
(β −1)m

E[f (xs

m) − f (x∗)]

E[f (xs

0) − f (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

This lemma is a slight generalization of Lemma 3, and we give the proof in APPENDIX B for completeness.

Theorem 2 (Option II and smooth objectives). If each fi(·) is convex and L-smooth, then the following inequality holds

(cid:104)

E

(cid:105)

f ((cid:98)xS)

− f (x∗) ≤

2(m+1)
mS(β −5)

[f ((cid:101)x0) − f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2,

where (cid:98)xS = (cid:101)xS if f ((cid:101)xS) ≤ f (xS), and xS = 1

S

(cid:80)S

s=1 (cid:101)xs. Otherwise, (cid:98)xS = xS.

The proof of this theorem can be found in APPENDIX C. Clearly, Theorems 1 and 2 show that VR-SGD with Option I

or Option II attains a sub-linear convergence rate for smooth general convex objective functions. This means that VR-SGD

is guaranteed to have a similar convergence rate with other variance reduced stochastic methods such as SAGA [30], and

a slower theoretical rate than accelerated methods such as Katyusha [3]. Nevertheless, VR-SGD usually converges much

faster than the best known stochastic method, Katyusha [3] in practice (see Section 5.3 for details).

4.1.2 Non-Smooth Objectives

Next, we provide the convergence guarantee of Problem (1) when the objective function F (x) is non-smooth (i.e., the
regularizer g(x) is non-smooth) and non-strongly convex. We ﬁrst give the following lemma.

Lemma 5 (Variance bound of non-smooth objectives). Let x∗ be the optimal solution of Problem (1). If each fi(·) is convex and
L-smooth, and (cid:101)∇fis

(xs

(xs

k) − ∇fis

k

k

k) := ∇fis
(cid:104)
E

k

((cid:101)xs−1) + ∇f ((cid:101)xs−1), then the following inequality holds
k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)].

≤ 4L[F (xs

k)(cid:107)2(cid:105)

(cid:107) (cid:101)∇fis

k

(xs

k) − ∇f (xs

This lemma can be viewed as a generalization of Lemma 2, and is essentially identical to Corollary 3.5 in [2] and Lemma

1 in [45], and hence its proof is omitted. For Algorithm 3 with Option II and a ﬁxed learning rate, we give the following

results.

Lemma 6 (Option II and non-smooth objectives). If each fi(·) is convex and L-smooth, then the following inequality holds for all
s = 1, 2, . . . , S,

(cid:18)

1 −

(cid:19)

2
β −1

≤

2
(β −1)m

E[F (xs

E[F ((cid:101)xs) − F (x∗)] +
2
β −1

0) − F (x∗)] +

m)−F (x∗)]

E[F (xs

2
(β −1)m
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2m

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

(21)

The proof of this lemma can be found in APPENDIX D. Using the above lemma, we give the following convergence

result for VR-SGD.

Theorem 3 (Option II and non-smooth objectives). Suppose Assumption 1 holds. Then the following inequality holds

E

(cid:104)

(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

2(m+1)
(β −5)mS

[F ((cid:101)x0) − F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

The proof of this theorem is included in APPENDIX E. Similarly, for Algorithm 3 with Option I and a ﬁxed learning

rate, we give the following results.

15

Corollary 1 (Option I and non-smooth objectives). If each fi(·) is convex and L-smooth, then the following inequality holds for
all s = 1, 2, . . . , S,
(cid:18)

(cid:19)

1 −

2
β −1
2m
(β −1)(m−1)

≤

E[F ((cid:101)xs) − F (x∗)] +
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

1
m−1

E[F (xs

m) − F (x∗)]

2
(β −1)(m−1)

E[F (xs

0)−F (x∗)] +

Lβ
2(m−1)

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 − (cid:107)x∗ −xs

m(cid:107)2(cid:3) .

Corollary 2 (Option I and non-smooth objectives). Suppose Assumption 1 holds. Then the following inequality holds

E

(cid:104)

(cid:105)
F ((cid:98)xS)

− F (x∗)

2(m + 1)
[(β −1)(m−1)−4m+2]S
where (cid:98)xS = (cid:101)xS if F ((cid:101)xS) ≤ F (xS), and xS = 1

≤

S

[F ((cid:101)x0) − F (x∗)] +

β(β − 1)L
2[(β −1)(m−1)−4m+2]S

(cid:107)(cid:101)x0 − x∗(cid:107)2,

(cid:80)S

s=1 (cid:101)xs. Otherwise, (cid:98)xS = xS.

Corollaries 1 and 2 can be viewed as the generalizations of Lemma 3 and Theorem 1, respectively, and hence their

proofs are omitted. Obviously, Theorems 3 and Corollary 2 show that VR-SGD with Option I or Option II attains a sub-

linear convergence rate for non-smooth general convex objective functions.

4.1.3 Mini-Batch Settings

The upper bound on the variance of the stochastic gradient estimator (cid:101)∇fis

k

(xs

k) in Lemma 5 is extended to the mini-batch

setting as follows.

Lemma 7 (Variance bound of mini-batch). If each fi(·) is convex and L-smooth, and (cid:101)∇fI s
∇f ((cid:101)xs−1), then the following inequality holds

k

(xs

k) := 1
b

(cid:80)

i∈I s
k

(cid:2)∇fi(xs

k) − ∇fi((cid:101)xs−1)(cid:3)+

(cid:104)

E

(cid:107) (cid:101)∇fI s

k

where δ(b) = (n−b)/[(n−1)b].

(xs

k) − ∇f (xs

k)(cid:107)2(cid:105)

≤ 4Lδ(b)[F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)],

It is not hard to verify that 0 ≤ δ(b) ≤ 1. This lemma is essentially identical to Theorem 4 in [36], and hence its proof

is omitted. Based on the variance upper bound in Lemma 7, we further analyze the convergence properties of VR-SGD for

the mini-batch setting. Lemma 6 is ﬁrst extended to the mini-batch setting as follows.

Lemma 8 (Mini-batch). Using the same notation as in Lemma 7, we have

2δ(b)
(β −1)m
2δ(b)
(β −1)

≤

{E[F (xs

m)] − F (x∗)} +

(cid:18)

1 −

(cid:19)

2δ(b)
β −1

{E[F ((cid:101)xs)] − F (x∗)}

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Proof. In order to simplify notation, the stochastic gradient estimator of mini-batch is deﬁned as:

vs
k :=

1
b

(cid:88)

i∈I s
k

(cid:2)∇fi(xs

k) − ∇fi((cid:101)xs−1)(cid:3) + ∇f ((cid:101)xs−1).

F (xs

k+1) ≤ g(xs

k+1) + f (xs

k) + (cid:10)∇f (xs

k), xs

= g(xs

k+1) + f (xs

k) + (cid:10)vs

k, xs

k+1 − xs
k

k+1 − xs
k

(cid:13)
(cid:13)xs

Lβ
2
k+1 − xs
(cid:107)xs

k(cid:107)2

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

(22)

+ (cid:10)∇f (xs

k) − vs

k, xs

k+1 − xs
k

(cid:11) −

(cid:107)xs

k+1 − xs

k(cid:107)2.

(cid:11) +

(cid:11) +
k+1 − xs
k
Lβ
2
L(β −1)
2

Using Lemma 7, then we obtain

L(β −1)
2
L(β −1)
2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:107)xs

k+1 −xs

k(cid:107)2 −

16

(23)

(cid:21)

L(β −1)
2

(cid:107)xs

k+1 −xs

k(cid:107)2

k) − vs

k, xs

k+1 − xs
k

(cid:11) −

(cid:107)∇f (xs

k) − vs

k(cid:107)2 +

E

(cid:20)
(cid:10)∇f (xs
(cid:20)
≤ E

1
2L(β −1)
(cid:2)F (xs

≤

2δ(b)
β −1

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) ,

where the ﬁrst inequality holds due to the Young’s inequality, and the second inequality follows from Lemma 7. Substituting
the inequality (23) into the inequality (22), and taking the expectation over the random mini-batch set I s

k, we have

E[F (xs

k+1)]

≤ E[g(xs

k+1)] + E[f (xs

k, xs

k+1 −xs
k

(cid:20)
(cid:10)vs
k)] + E
(cid:20)
(cid:104)vs
k)] + E
(cid:20) Lβ
2

(cid:11) +

Lβ
2

(cid:107)xs

k+1 −xs

k(cid:107)2

(cid:21)

+

2δ(b)
β −1
(cid:21)

Lβ
2

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

2δ(b)
β −1

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

≤ g(x∗) + E[f (xs

k, x∗ −xs

k(cid:105) +

((cid:107)x∗ −xs

k(cid:107)2 −(cid:107)x∗ −xs

k+1(cid:107)2)

+

≤ g(x∗) + f (x∗) + E

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:2)F (xs

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

= F (x∗) +

Lβ
2

E(cid:2)(cid:0)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

2δ(b)
β − 1

k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3) ,

(cid:21)
k+1(cid:107)2)

+

2δ(b)
β −1
(cid:2)F (xs

where the second inequality holds from Lemma 1. Then the above inequality is rewritten as follows:

E(cid:2)F (xs

k+1)(cid:3) − F (x∗) ≤

2δ(b)
β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

Summing the above inequality over k = 0, 1, · · · , (m − 1), then

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

(24)

m−1
(cid:88)

k=0
m−1
(cid:88)

≤

(cid:8)E[F (xs

k+1)] − F (x∗)(cid:9)

k=0
(cid:80)m

k=1 xs

(cid:2)F (xs

(cid:26) 2δ(b)
β −1
k, we have F ((cid:101)xs) ≤ 1
m)] − F (x∗)} +
{E[F (xs

m

Since (cid:101)xs = 1

m

2δ(b)
(β −1)m
2δ(b)
(β −1)

≤

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k+1(cid:107)2(cid:3)

.

Lβ
2

(cid:80)m

k=1 F (xs
(cid:18)
2δ(b)
β −1

1 −

k), and
(cid:19)

{E[F ((cid:101)xs)] − F (x∗)}

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

This completes the proof.

Similar to Lemma 8, we can also extend Lemma 3 and Corollary 1 to the mini-batch setting.

Theorem 4 (Mini-batch). If each fi(·) is convex and L-smooth, then the following inequality holds
2δ(b)(m + 1)
(β −1−4δ(b))mS

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3) +

(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

E

(cid:104)

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3) .

(25)

Proof. Using Lemma 8, we have

(cid:18)

1 −

≤

2δ(b)
(β −1)

(cid:19)

E[F ((cid:101)xs) − F (x∗)]

2δ(b)
β −1
(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

2δ(b)
(β −1)m

[F (xs

0) − F (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Summing the above inequality over s = 1, 2, . . . , S, taking expectation over whole history of I s

k, and using xs+1

0 = xs

m, we

obtain

S
(cid:88)

(cid:18)

1 −

(cid:19)

2δ(b)
β −1

E [F ((cid:101)xs) − F (x∗)]

s=1
S
(cid:88)

s=1

≤

2δ(b)
(β −1)

(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) +

S
(cid:88)

s=1

2δ(b)
(β −1)m

E[F (xs

0) − F (xs

m)] +

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting

2δ(b)
β−1

(cid:80)S−1
s=1

E[F ((cid:101)xs)−F (x∗)] from both sides of the above inequality, we have
(cid:19)

(cid:18)

(cid:104)
E

2δ(b)
β −1

(cid:105)
F ((cid:101)xS) − F (x∗)

+

S
(cid:88)

1 −

4δ(b)
β −1

E[F ((cid:101)xs) − F (x∗)]

≤

2δ(b)
(β −1)

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

s=1

2δ(b)
(β −1)m

(cid:104)
E

F (x1

(cid:105)
0) − F (xS
m)

+

Dividing both sides of the above inequality by S and using E[F (x)] ≤ 1
S

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

(cid:104)

E

(cid:107)x∗ − x1

Lβ
2m
s=1 F ((cid:101)xs), we arrive at

(cid:80)S

2δ(b)
(β −1)S
2δ(b)
(β −1)S

≤

E[F ((cid:101)xS) − F (x∗)] +

(cid:18)

1 −

(cid:19)

4δ(b)
β −1

E[F (x) − F (x∗)]

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

2δ(b)
(β −1)mS

(cid:104)
E

F (x1

0) − F (xS
m)

(cid:105)

+

(cid:104)

E

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

17

.

Subtracting

2δ(b)
(β−1)S
(cid:18)

1 −

(cid:19)

E[F ((cid:101)xS)−F (x∗)] from both sides of the above inequality, we have
4δ(b)
β −1
(cid:104)

E[F (x) − F (x∗)]

(cid:105)

(cid:105)

≤

2δ(b)
(β −1)S

E

F ((cid:101)x0) − F ((cid:101)xS)

+

2δ(b)
(β −1)mS

(cid:104)
E

F (x1

0) − F (xS
m)

+

(cid:104)
E

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2 − (cid:107)x∗ − xS

m(cid:107)2(cid:105)

.

Dividing both sides of the above inequality by (1− 4δ(b)

β−1 ) > 0, we arrive at

E

E[F (x)] − F (x∗)
2δ(b)
(β −1−4δ(b))S
2δ(b)
(β −1−4δ(b))S
2δ(b)(m + 1)
(β −1−4δ(b))mS

≤

≤

=

(cid:104)

(cid:105)
F ((cid:101)x0)−F ((cid:101)xS)

+

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3) +

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3) +

(cid:104)
E

2δ(b)
(β −1−4δ(b))mS
2δ(b)
(β −1−4δ(b))mS
Lβ(β −1)
2(β −1−4δ(b))mS

(cid:105)
F ((cid:101)x0)−F (xS
m)

+

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3)

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3) +

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3)

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3) .

When F ((cid:101)xS) ≤ F (xS), then (cid:98)xS = (cid:101)xS, and
(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

E

(cid:104)

2δ(b)(m + 1)
(β −1−4δ(b))mS

E(cid:2)F ((cid:101)x0)−F (x∗)(cid:3) +

Lβ(β −1)
2(β −1−4δ(b))mS

E(cid:2)(cid:107)x∗ − (cid:101)x0(cid:107)2(cid:3) .

Alternatively, if F ((cid:101)xS) ≥ F (xS), then (cid:98)xS = xS, and the above inequality still holds. This completes the proof.

From Theorem 4, one can see that when b = n (i.e., the batch setting), we have δ(n) = 0, and the ﬁrst term on the

right-hand side of (25) diminishes. In other words, our VR-SGD method degenerates to the deterministic method with the
convergence rate of O(1/T ). Furthermore, when b = 1, we have δ(1) = 1, and then Theorem 4 degenerates to Theorem 3.

4.2 Convergence Properties for Strongly Convex Problems

In this part, we analyze the convergence properties of VR-SGD (i.e., Algorithm 2) for solving the strongly convex objective
function (1). According to the above analysis, one can see that if F (·) is convex, and each convex component function fi(·)
is L-smooth, both Algorithms 2 and 3 converge to the optimal solution. In the following, we provide stronger convergence

rate guarantees for VR-SGD under the strongly convex condition. We ﬁrst give the following assumption.

Assumption 3. For all s = 1, 2, . . . , S, the following inequality holds

E[F (xs

0) − F (x∗)] ≤ C E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

where C > 0 is a constant4.

Similar to Algorithm 3, Algorithm 2 also has two different update rules for the two cases of smooth and non-smooth

objective functions. We ﬁrst give the following convergence result for Algorithm 2 with Option I.

4. This assumption shows the relationship of the gaps between the function values at the starting and snapshot points of each epoch and the

optimal value of the objective function. In fact, both (cid:101)xs and xs

m (i.e., xs+1

0

) converge to x∗, and thus C is far less than m, i.e., C (cid:28) m.

18

(26)

Theorem 5 (Option I). Suppose Assumptions 1, 2, and 3 hold, and m is sufﬁciently large so that

ρI :=

2Lη(m+C)
(m−1)(1−3Lη)

+

C(1−Lη)
µη(m−1)(1−3Lη)

< 1.

Then Algorithm 2 with Option I has the following geometric convergence in expectation:
(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) .

E [F ((cid:101)xs) − F (x∗)] ≤ ρs

I

Proof. Since each fi(·) is convex and L-smooth, then Corollary 1 holds, which then implies

E[F ((cid:101)xs) − F (x∗)] +

1
m−1

E(cid:2)F (xs+1

0

) − F (x∗)(cid:3)+

Lβ
2(m−1)

E(cid:2)(cid:107)xs+1

0 −x∗(cid:107)2(cid:3)

≤

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)+
Due to the strong convexity of F (·), we have (cid:107)xs

2
(m−1)(β −1)
0 − x∗(cid:107)2 ≤ (2/µ)[F (xs

E[F (xs

0)−F (x∗)]+

Lβ
2(m−1)

E(cid:2)(cid:107)xs

0 −x∗(cid:107)2(cid:3) .

0) − F (x∗)]. Then the inequality in (26) can be

(cid:19)

(cid:18)

1 −

2
β −1
2m
(m−1)(β −1)

rewritten as follows:
(cid:18)

(cid:19)

1 −

2
β −1
2m
(m−1)(β −1)
2m
(m−1)(β −1)
(cid:18) 2(m+C)

≤

≤

=

E[F ((cid:101)xs) − F (x∗)]

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)+

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)+
CLβ
µ(m−1)

+

(cid:19)

(cid:18)

(cid:18)

2
(m−1)(β −1)
2C
(m−1)(β −1)
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)

+

+

(cid:19)

(cid:19)

Lβ
µ(m−1)
CLβ
µ(m−1)

E[F (xs

0)−F (x∗)]

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)

(m−1)(β −1)
0) − F (x∗)], and the second inequality follows
where the ﬁrst inequality holds due to the fact that (cid:107)xs
from Assumption 3. Dividing both sides of the above inequality by [1−2/(β −1)] > 0 (that is, β is required to be larger than
3) and using the deﬁnition of β = 1/(Lη), we arrive at
(cid:18) 2(m+C)

0 − x∗(cid:107)2 ≤ (2/µ)[F (xs

(cid:19)

E[F ((cid:101)xs) − F (x∗)] ≤

(m−1)(β −3)
(cid:18) 2(m+C)Lη

(m−1)(1−3Lη)

+

CLβ(β −1)
µ(m−1)(β −3)

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3)

+

C(1−Lη)
µη(m−1)(1−3Lη)

(cid:19)

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) .

This completes the proof.

=

Although the learning rate η in Theorem 5 needs to be less than 1/(3L), we can use much larger learning rates in
practice, e.g., η = 3/(7L). Similar to Theorem 5, we give the following convergence result for Algorithm 2 with Option II.

Theorem 6 (Option II). Suppose Assumptions 1, 2, and 3 hold, and m is sufﬁciently large so that

ρII :=

2Lη(m+C)
m(1−3Lη)

+

C(1−Lη)
mµη(1−3Lη)

< 1.

Then Algorithm 2 with Option II has the following geometric convergence in expectation:
(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) .

E [F ((cid:101)xs) − F (x∗)] ≤ ρs

II

The proof of Theorem 6 can be found in APPENDIX F. In addition, we can also provide the linear convergence

guarantees of Algorithm 2 with Option I or Option II for solving smooth strongly convex objective functions. From all

the results, one can see that VR-SGD attains a linear convergence rate for both smooth and non-smooth strongly convex

minimization problems.

5 EXPERIMENTAL RESULTS

In this section, we evaluate the performance of our VR-SGD method for solving various ERM problems, such as logistic

regression, Lasso, and ridge regression, and compare its performance with several related stochastic variance reduced

methods, including SVRG [1], Prox-SVRG [2], and Katyusha [3]. All the codes of VR-SGD and related methods can be
downloaded from the author’s website5.

5. https://sites.google.com/site/fanhua217/publications

19

(a) Adult: λ = 10−5 (left) and λ = 10−6 (right)

(b) Protein: λ = 10−5 (left) and λ = 10−6 (right)

(c) Covtype: λ = 10−5 (left) and λ = 10−6 (right)

(d) Sido0: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 3. Comparison of Options I, II, and III for solving ridge regression problems with the regularizer (λ/2)(cid:107) · (cid:107)2. In each plot, the vertical axis shows
the objective value minus the minimum, and the horizontal axis is the number of effective passes.

5.1 Experimental Setup

We used four publicly available data sets in the experiments: Adult (also called a9a), Covtype, Protein, and Sido0, as listed

in Table 1. It should be noted that each example of these date sets was normalized so that they have unit length as in [2], [34], which
leads to the same upper bound on the Lipschitz constants Li, i.e., L = Li for all i = 1, . . . , n. As suggested in [1], [2], [3], the epoch
length is set to m = 2n for the stochastic variance reduced methods, SVRG [1], Prox-SVRG [2], and Katyusha [3], as well as
our VR-SGD method. Then the only parameter we have to tune by hand is the step size (or learning rate), η. Since Katyusha

has a much higher per-iteration complexity than SVRG, Prox-SVRG, and VR-SGD, we compare their performance in terms

of both the number of effective passes and running time (seconds), where computing a single full gradient or evaluating
n component gradients is considered as one effective pass over the data. For fair comparison, we implemented SVRG,

Prox-SVRG, Katyusha, and VR-SGD in C++ with a Matlab interface, and performed all the experiments on a PC with an

Intel i5-2400 CPU and 16GB RAM. In addition, we do not compare with other stochastic algorithms such as SAGA [30] and

Catalyst [40], as they have been shown to be comparable or inferior to Katyusha [3].

TABLE 1

Summary of data sets used for our experiments.

Data sets

Sizes n

Dimensions d

Sparsity

Adult

32,562

Protein

145,751

Covtype

581,012

123

74

54

Sido0

12,678

4,932

11.28%

99.21%

22.12%

9.84%

The three choices of snapshot and starting points for stochastic optimization.

TABLE 2

Option I

Option II

Option III

(cid:101)xs+1 = xs

m and xs+1

0 = xs
m

(cid:101)xs+1 = 1

m

(cid:80)m

k=1xs

k and xs+1

0 = 1
m

(cid:80)m

k=1xs
k

(cid:101)xs+1 = 1

m−1

(cid:80)m−1

k=1 xs

k and xs+1

0 = xs
m

0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III20

(a) Adult: λ = 10−4 (left) and λ = 10−5 (right)

(b) Protein: λ = 10−4 (left) and λ = 10−5 (right)

(c) Covtype: λ = 10−4 (left) and λ = 10−5 (right)

(d) Sido0: λ = 10−4 (left) and λ = 10−5 (right)

Fig. 4. Comparison of Options I, II, and III for solving Lasso problems with the regularizer λ(cid:107)x(cid:107)1. In each plot, the vertical axis shows the objective
value minus the minimum, and the horizontal axis is the number of effective passes.

5.2 Different Choices for Snapshot and Starting Points
In the practical implementation of SVRG [1], both the snapshot point (cid:101)xs and the starting point xs+1
to the last iterate xs
in [1]) are set to the average point of the previous epoch, 1
m

m of the previous epoch (i.e., Option I in Algorithm 1), while the two vectors in [2] (also suggested
k (i.e., Option II in Algorithm 1). In contrast, (cid:101)xs and xs+1
m (denoted by Option III, i.e., Option I6 in Algorithms 2 and 3), respectively.
Note that Johnson and Zhang [1] ﬁrst presented the choices of Options I and II, while the setting of Option III is suggested

in this paper are set to

of each epoch are set

k and xs

k=1 xs

k=1xs

(cid:80)m−1

1
m−1

(cid:80)m

0

0

in this paper. In the following, we compare the performance of the three choices (i.e., Options I, II and III in Table 2) for

snapshot and starting points for solving ridge regression and Lasso problems, as shown in Figs. 3 and 4. Except for the

three different settings for snapshot and starting points, we use the update rules in (12) and (13) for ridge regression and

Lasso problems, respectively.

From all the results shown in Figs. 3 and 4, we can see that our algorithms with Option III (i.e., Algorithms 2 and 3 with

their Option I) consistently converge much faster than SVRG with the choices of Options I and II for both strongly convex

and non-strongly convex cases. This indicates that the setting of Option III suggested in this paper is a better choice than

Options I and II for stochastic optimization.

5.3 Logistic Regression

In this part, we focus on the following generalized logistic regression problems for binary classiﬁcation,

min
x∈Rd

1
n

n
(cid:88)

i=1

log(1 + exp(−biaT

i x)) +

λ1
2

(cid:107)x(cid:107)2 + λ2(cid:107)x(cid:107)1,

(27)

where {(ai, bi)} is a set of training examples, and λ1, λ2 ≥ 0 are the regularization parameters. Note that when λ2 > 0,
fi(x) = log(1+exp(−biaT
i x))+(λ1/2)(cid:107)x(cid:107)2. The formulation (27) includes the (cid:96)2-norm (i.e., λ2 = 0), (cid:96)1-norm (i.e., λ1 = 0), and
elastic net (i.e., λ1 (cid:54)= 0 and λ2 (cid:54)= 0) regularized logistic regression problems. Figs. 5, 6 and 7 show how the objective gap, i.e.,
F (xs)−F (x∗), decreases for the (cid:96)2-norm, (cid:96)1-norm, and elastic net regularized logistic regression problems, respectively.
From all the results, we make the following observations.

6. As Options I and II in Algorithms 2 and 3 achieve very similar performance, we only report the results of Algorithms 2 and 3 with Option I.

01020304010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III0510152025303510−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III02040608010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05101520253010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015020010−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015020010−610−410−2Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III05010015020010−610−510−410−310−210−1Gradient evaluations / nF(xs ) − F(x* )Option IOption IIOption III• When the regularization parameters λ1 and λ2 are relatively large, e.g., λ1 = 10−4 or λ2 = 10−4, Prox-SVRG
usually converges faster than SVRG for both strongly convex (e.g., (cid:96)2-norm regularized logistic regression) and
non-strongly convex (e.g., (cid:96)1-norm regularized logistic regression) cases, as shown in Figs. 5(a)–5(c) and Figs. 6(a)–

21

6(b). On the contrary, SVRG often outperforms Prox-SVRG, when the regularization parameters are relatively small,
e.g., λ1 = 10−6 or λ2 = 10−6, as observed in [45]. The main reason is that they have different initialization settings,
i.e., (cid:101)xs = xs

m for SVRG vs. (cid:101)xs = 1

k for Prox-SVRG.

m and xs+1

k and xs+1

0 = 1
m

0 = xs

k=1xs

k=1xs

(cid:80)m

(cid:80)m

m

• Katyusha converges much faster than SVRG and Prox-SVRG for the cases when the regularization parameters are
relatively small, e.g., λ1 = 10−6, whereas it often achieves similar or inferior performance when the regularization
parameters are relatively large, e.g., λ1 = 10−4, as shown in Figs. 5(a)–5(d) and Figs. 6(a)–6(b). Note that we
implemented the original algorithms with Option I in [3] for Katyusha. In other words, Katyusha is an accelerated

proximal stochastic gradient method. Obviously, the above observation matches the convergence properties of
Katyusha provided in [3], that is, only if mµ/L ≤ 3/4, Katyusha attains the best known overall complexities of
O((n+(cid:112)nL/µ) log(1/(cid:15))) for strongly convex problems.

• Our VR-SGD method consistently converges much faster than SVRG and Prox-SVRG, especially when the regular-
ization parameters are relatively small, e.g., λ1 = 10−6 or λ2 = 10−6, as shown in Figs. 5(i)–5(l) and Figs. 6(e)–6(f).
The main reason is that VR-SGD can use much larger learning rates than SVRG (e.g., 6/(5L) for VR-SGD vs. 1/(10L)

for SVRG), which leads to faster convergence. This further veriﬁes that the setting of both snapshot and starting

points in our algorithms (i.e., Algorithms 2 and 3) is a better choice than Options I and II in Algorithm 1.

•

In particular, VR-SGD generally outperforms the best-known stochastic method, Katyusha, in terms of the number
of passes through the data, especially when the regularization parameters are relatively large, e.g., 10−4 and 10−5,

as shown in Figs. 5(a)–5(h) and Figs. 6(a)–6(d). Since VR-SGD has a much lower per-iteration complexity than

Katyusha, VR-SGD has more obvious advantage over Katyusha in terms of running time. From the algorithms of
Katyusha proposed in [3], one can see that the learning rate of Katyusha is at least set to 1/(3L). Similarly, the

learning rate used in VR-SGD is comparable to Katyusha, which may be the main reason why the performance of

VR-SGD is much better than that of Katyusha. This also implies that the algorithm that enjoys larger learning rates

can yield better performance.

5.4 Common Stochastic Gradient and Prox-SG Updates

In this part, we compare the original Katyusha algorithm, i.e., Algorithm 1 in [3], with the slightly modiﬁed Katyusha

algorithm (denoted by Katyusha-I). In Katyusha-I, only the following two update rules for smooth objective functions are

used to replace the original proximal stochastic gradient update rules in (6b) and (6c).

k+1 = ys
ys
k+1 = xs
zs

k − η[ (cid:101)∇fik(xs
k+1 − [ (cid:101)∇fik(xs

k+1) + ∇g(xs
k+1) + ∇g(xs

k+1)],

k+1)]/(3L).

(28)

Similarly, we also implement the proximal versions7 for the original SVRG (also called SVRG-I) and the proposed VR-SGD

(denoted by VR-SGD-I) methods, and denote their proximal variants by SVRG-II and VR-SGD-II, respectively. Here, the

original Katyusha method is denoted by Katyusha-II.

Figs. 8 and 9 show the performance of Katyusha-I and Katyusha-II for solving ridge regression problems on the two

popular data sets: Adult and Covtype. We also report the results of SVRG, VR-SGD, and their proximal variants. It is clear

that Katyusha-I usually performs better than Katyusha-II (i.e., the original proximal stochastic method, Katyusha [3]), and
converges signiﬁcantly faster for the case when the regularization parameter is 10−4. This seems to be the main reason

why Katyusha has inferior performance when the regularization parameter is relatively large, as shown in Section 5.3. In

7. Here, the proximal variant of SVRG is different from Prox-SVRG [2], and their main difference is the choices of both the snapshot point and
m, while those of Prox-SVRG are set to the average point of the

starting point. That is, the two vectors of the former are set to the last iterate xs
previous epoch, i.e., 1
m

k=1xs
k.

(cid:80)m

22

contrast, VR-SGD and its proximal variant have similar performance, and the former slightly outperforms the latter in most

cases, as well as SVRG vs. its proximal variant. All this suggests that stochastic gradient update rules as in (12) and (28)

are better choices than proximal stochastic gradient update rules as in (10), (6b) and (6c) for smooth objective functions. We

also believe that our new insight can help us to design accelerated stochastic optimization methods. Both Katyusha-I and

Katyusha-II usually outperform SVRG and its proximal variant, especially when the regularization parameter is relatively
small, e.g., λ = 10−6, as shown in Figs. 8(d) and 9(d). Unfortunately, both Katyusha-I and Katyusha-II cannot solve the
convex objectives without any regularization term (i.e., λ = 0, as shown in Figs. 8(f) and 9(f)) or with too large and small
regularization parameters, e.g., 10−3 and 10−7, as shown in Figs. 9(a) and 8(e).

Moreover, it can be seen that both VR-SGD and its proximal variant achieve much better performance than the other

methods in most cases, and are also comparable to Katyusha-I and Katyusha-II in the remaining cases. This further veriﬁes

that VR-SGD is suitable for various large-scale machine learning.

5.5 Fixed and Varied Learning Rates

Finally, we compare the performance of Algorithm 3 with ﬁxed and varied learning rates for solving (cid:96)1-norm regularized

logistic regression and Lasso problems, as shown in Fig. 10. Note that the learning rates in Algorithm 3 are varied according

to the update formula in (14). We can observe that Algorithm 3 with varied step-sizes performs very similar to Algorithm 3

with ﬁxed step-sizes in most cases. In the remaining cases, the former slightly outperforms the latter.

6 CONCLUSIONS

In this paper, we proposed a simple variant of the original SVRG [1], called variance reduced stochastic gradient descent

(VR-SGD). Unlike the choices of the snapshot point and starting point in SVRG and its proximal variant, Prox-SVRG [2],

the two points of each epoch in VR-SGD are set to the average and last iterate of the previous epoch, respectively. This
setting allows us to use much larger learning rates than SVRG, e.g., 3/(7L) for VR-SGD vs. 1/(10L) for SVRG, and also

makes VR-SGD more robust in terms of learning rate selection. Different from existing proximal stochastic methods such as

Prox-SVRG [2] and Katyusha [3], we designed two different update rules for smooth and non-smooth objective functions,

respectively, which makes our VR-SGD method suitable for non-smooth and/or non-strongly convex problems without

using any reduction techniques as in [3], [61]. In contrast, SVRG and Prox-SVRG cannot directly solve non-strongly convex

objectives [42]. Furthermore, our empirical results showed that for smooth problems stochastic gradient update rules as in

(12) are better choices than proximal stochastic gradient update formulas as in (10).

On the practical side, the choices of the snapshot and starting points make VR-SGD signiﬁcantly faster than its

counterparts, SVRG and Prox-SVRG. On the theoretical side, the setting also makes our convergence analysis more

challenging. We analyzed the convergence properties of VR-SGD for strongly convex objective functions, which show

that VR-SGD attains a linear convergence rate. Moreover, we also provided the convergence guarantees of VR-SGD for

non-strongly convex problems, which show that VR-SGD achieves a sub-linear convergence rate. All these results imply

that VR-SGD is guaranteed to have a similar convergence rate with other variance reduced stochastic methods such as

SVRG and Prox-SVRG, and a slower theoretical rate than accelerated methods such as Katyusha [3]. Nevertheless, various

experimental results show that VR-SGD signiﬁcantly outperforms than SVRG and Prox-SVRG, and is still much better than

the best known stochastic method, Katyusha [3].

APPENDIX A: PROOF OF LEMMA 2

Before proving Lemma 2, we ﬁrst give and prove the following lemma.

Lemma 9. Suppose each fi(·) is L-smooth, and let x∗ be the optimal solution of Problem (1) when g(x) ≡ 0, then we have

(cid:104)

(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2(cid:105)

E

≤ 2L [f (x) − f (x∗)] .

Proof. Following Theorem 2.1.5 in [19] and Lemma 3.4 [2], we have

(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 ≤ 2L [fi(x) − fi(x∗) − (cid:104)∇fi(x∗), x − x∗(cid:105)] .

Summing the above inequality over i = 1, . . . , n, we obtain

(cid:104)
E

(cid:107)∇fi(x)−∇fi(x∗)(cid:107)2(cid:105)

=

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x)−∇fi(x∗)(cid:107)2 ≤ 2L [f (x) − f (x∗) − (cid:104)∇f (x∗), x − x∗(cid:105)] .

By the optimality of x∗, i.e., x∗ = arg minx f (x), we have ∇f (x∗) = 0. Then

(cid:104)
E

(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2(cid:105)

≤ 2L [f (x) − f (x∗) − (cid:104)∇f (x∗), x − x∗(cid:105)]

= 2L [f (x) − f (x∗)] .

23

Proof of Lemma 2:

Proof.

(cid:104)

E

(cid:107) (cid:101)∇fis

k

(xs

k) − ∇f (xs

k)(cid:107)2(cid:105)

k

k

= E

= E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis
≤ 4L(cid:2)f (xs

≤ 2E

≤ E

k

k

(xs

k) − ∇fis

k

(cid:13)
((cid:101)xs−1) + ∇f ((cid:101)xs−1) − ∇f (xs
(cid:13)
k)
(cid:13)

2(cid:21)

− (cid:13)

(cid:13)∇f (xs

k) − ∇f ((cid:101)xs−1)(cid:13)
2
(cid:13)

(xs

k) − ∇fis

k

(xs

k) − ∇fis

k

(xs

k) − ∇fis

2(cid:21)

2(cid:21)

(cid:13)
((cid:101)xs−1)
(cid:13)
(cid:13)
(cid:13)
((cid:101)xs−1)
(cid:13)
(cid:13)
2(cid:21)
(cid:13)
(x∗)
(cid:13)
(cid:13)

k

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fis

k

((cid:101)xs−1) − ∇fis

k

2(cid:21)

(cid:13)
(x∗)
(cid:13)
(cid:13)

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3),

where the second equality holds due to the fact that E[(cid:107)x−E[x](cid:107)2] = E[(cid:107)x(cid:107)2]−(cid:107)E[x](cid:107)2; the second inequality holds due to
the fact that (cid:107)a − b(cid:107)2 ≤ 2((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2); and the last inequality follows from Lemma 9.

APPENDIX B: PROOF OF LEMMA 5

Proof. For convenience, the stochastic gradient estimator is deﬁned as: vs
((cid:101)xs−1)+∇f ((cid:101)xs−1). Since each
component function fi(x) is L-smooth, which implies that the gradient of the average function f (x) is also L-smooth, i.e.,
for all x, y ∈ Rd,

k := ∇fis

k)−∇fis

(xs

k

k

whose equivalent form is

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

L
2

(cid:107)y − x(cid:107)2.

Using the above smoothness inequality, we have

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

(cid:11) +

k+1 − xs
k

(cid:13)
2
(cid:13)

= f (xs

k) + (cid:10)∇f (xs

k), xs

= f (xs

k) + (cid:10)vs

k, xs

k+1 − xs
k

+ (cid:10)∇f (xs

k) − vs

k, xs

(cid:13)
(cid:13)xs

(cid:13)
(cid:13)xs

L
2
Lβ
2
k+1 − xs
(cid:107)xs
L(β −1)
2

k(cid:107)2

(cid:11) +
k+1 − xs
k
Lβ
2
(cid:11) −

k+1 − xs
k

(cid:11) +

(cid:107)xs

k+1 − xs

k(cid:107)2.

k+1 − xs
k

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

Using Lemma 2, then we get

E

(cid:20)
(cid:10)∇f (xs
(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)f (xs
β −1

≤

k) − vs

k, xs

k+1 − xs
k

(cid:11) −

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:107)∇f (xs

k) − vs

k(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:21)

(cid:107)xs

k+1 −xs

k(cid:107)2

L(β −1)
2
L(β −1)
2

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) ,

(29)

(30)

where the ﬁrst inequality holds due to the Young’s inequality (i.e., yT z ≤ (cid:107)y(cid:107)2/(2θ)+θ(cid:107)z(cid:107)2/2 for all θ > 0 and y, z ∈ Rd),

and the second inequality follows from Lemma 2.

Taking the expectation over the random choice of is

k and substituting the inequality in (30) into the inequality in (29),

24

we have

E[f (xs

k+1)]

≤ E[f (xs

≤ E[f (xs

(cid:20)
(cid:10)vs
k)] + E
(cid:20)
(cid:104)vs
k)] + E

k, xs

k+1 − xs
k

(cid:11) +

Lβ
2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

+

2
β −1

k, x∗ − xs

k(cid:105) +

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)
(cid:2)f (xs
(cid:21)
k+1(cid:107)2)

(cid:2)f (xs

+

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

≤ E[f (xs

k), x∗ − xs

k(cid:105) + E

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

k)] + (cid:104)∇f (xs
(cid:20) Lβ
2
E(cid:2)(cid:0)(cid:107)x∗ − xs

((cid:107)x∗ − xs

Lβ
2

≤ f (x∗) + E

= f (x∗) +

k(cid:107)2 − (cid:107)x∗ − xs

(cid:21)
k+1(cid:107)2)

+

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:1)(cid:3) +

2
β −1
2
β −1

(cid:2)f (xs

(cid:2)f (xs

+

(cid:2)f (xs

k+1(cid:107)2)

2
β −1
k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)
k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) ,

Lβ
2

(cid:20) Lβ
2

2
β −1
(cid:21)

where the ﬁrst inequality holds due to the inequality in (29) and the inequality in (30); the second inequality follows from
Lemma 1 with ˆz = xs
k(cid:105); the third inequality holds due to the fact
that E[vs
k(cid:105) ≤ f (x∗). The

k); and the last inequality follows from the convexity of f (·), i.e., f (xs

k, τ = Lβ = 1/η, and r(z) := (cid:104)vs

k+1, z = x∗, z0 = xs

k)+(cid:104)∇f (xs

k] = ∇f (xs

k), x∗ −xs

k, z −xs

above inequality can be rewritten as follows:

E[f (xs

k+1)] − f (x∗) ≤

2
β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

Summing the above inequality over k = 0, 1, . . . , m−1, we obtain

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

m−1
(cid:88)

k=0
m−1
(cid:88)

k=0

≤

(cid:8)E[f (xs

k+1)] − f (x∗)(cid:9)

(cid:26) 2

β −1

(cid:2)f (xs

k) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3)

Then

(cid:18)

1 −

2
β −1

(cid:19) m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)} +

2
β −1

m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)}

≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

(cid:27)
k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

(cid:27)

.

(31)

Due to the setting of (cid:101)xs = 1

m

(cid:80)m

k=1 xs

k in Option II, and the convexity of f (·), then
m
(cid:88)

f ((cid:101)xs) ≤

1
m

f (xs

k).

k=1

Using the above inequality, the inequality in (31) becomes

(cid:18)

m

1 −

(cid:19)

2
β −1

E[f ((cid:101)xs) − f (x∗)] +

2
β −1

m
(cid:88)

k=1

{E[f (xs

k)] − f (x∗)}

≤

m
(cid:88)

(cid:26) 2

β −1

k=1

(cid:2)f (xs

(cid:27)
k−1) − f (x∗) + f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Dividing both sides of the above inequality by m and subtracting

2
(β−1)m

(cid:80)m−1

k=1 [f (xs

k)−f (x∗)] from both sides, we arrive

at

(cid:18)

1 −

(cid:19)

E[f ((cid:101)xs) − f (x∗)] +

2
β −1
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +

2
(β −1)m
2
(β −1)m

≤

2
(β −1)

This completes the proof.

E[f (xs

m) − f (x∗)]

E[f (xs

0) − f (x∗)] +

Lβ
2m

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

APPENDIX C: PROOF OF THEOREM 2

Proof. Using Lemma 5, we have

25

E[f (xs

m) − f (x∗)]

(1 −

2
β −1
2
(β −1)m

)E[f ((cid:101)xs) − f (x∗)] +

2
(β −1)m
2
β −1

≤

E[f (xs

E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3) +
Summing the above inequality over s = 1, 2, . . . , S, taking expectation with respect to the history of random variables is
k,
and using the setting of xs+1

0(cid:107)2 − (cid:107)x∗ − xs

m in Algorithm 3, we arrive at

0) − f (x∗)] +

E[(cid:107)x∗ − xs

0 = xs

m(cid:107)2].

Lβ
2m

S
(cid:88)

(1−

2
β −1

)E[f ((cid:101)xs) − f (x∗)]

s=1
S
(cid:88)

(cid:26)

s=1

≤

2
(β −1)m

E{f (xs

0) − f (x∗) − [f (xs

m) − f (x∗)]} +

2
β −1

(cid:27)
E(cid:2)f ((cid:101)xs−1) − f (x∗)(cid:3)

+

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

Subtracting 2
β−1

(cid:80)S

s=1[f ((cid:101)xs) − f (x∗)] from both sides of the above inequality, we obtain

(cid:19)

E[f ((cid:101)xs) − f (x∗)]

4
β −1
(cid:110)

E

f (x1

S
(cid:88)

(cid:18)

1 −

s=1

2
(β −1)m
Lβ
(cid:104)
2m

+

E

≤

0) − f (x∗) − [f (xS
m(cid:107)2(cid:105)

0(cid:107)2 − (cid:107)x∗ − xS

.

(cid:107)x∗ − x1

(cid:111)
m) − f (x∗)]

+

2
β −1

E[f ((cid:101)x0) − f ((cid:101)xS)]

It is not hard to verify that E[f ((cid:101)x0)−f ((cid:101)xS)] ≤ f ((cid:101)x0)−f (x∗). Dividing both sides of the above inequality by S, and using

the choice (cid:101)x0 = x1
0, we have
(cid:18)

1
S

1 −

4
β −1

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

≤

≤

=

2
(β −1)mS
2
(β −1)mS
2(m+1)
(β −1)mS

(cid:110)

E

f (x1

0) − f (x∗) − [f (xS

(cid:111)
m) − f (x∗)]

+

(cid:2)f ((cid:101)x0) − f (x∗)(cid:3) +

[f ((cid:101)x0) − f (x∗)] +

2
(β −1)S
Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2

2
(β −1)S
Lβ
2mS

[f ((cid:101)x0) − f (x∗)] +

(cid:107)x∗ − (cid:101)x0(cid:107)2

[f ((cid:101)x0) − f (x∗)] +

Lβ
2mS

(cid:107)x∗ − x1

0(cid:107)2

(32)

where the ﬁrst inequality holds due to the facts that E[f ((cid:101)x0)−f ((cid:101)xS)] ≤ f ((cid:101)x0)−f (x∗) and E(cid:2)(cid:107)x∗ −xS
m)−f (x∗)(cid:3) ≥ 0 and (cid:101)x0 = x1
inequality uses the facts that E(cid:2)f (xS
0.
(cid:80)S
s=1 (cid:101)xs, and using the convexity of f (·), we have f (xS) ≤ 1

Since xS = 1
S

(cid:80)S

S

s=1f ((cid:101)xs), and therefore the inequality in

m(cid:107)2(cid:3) ≥ 0, and the last

(32) becomes

(cid:18)

1 −

(cid:19)

4
β −1

(cid:104)
E

(cid:105)
f (xS) − f (x∗)

≤

(cid:18)

1 −

1
S

4
β −1

(cid:19) S
(cid:88)

s=1

E[f ((cid:101)xs) − f (x∗)]

≤

2(m+1)
(β −1)mS

[f ((cid:101)x0) − f (x∗)] +

Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Dividing both sides of the above inequality by (1− 4

β−1 ) > 0 (i.e., η < 1/(5L)), we arrive at

(cid:104)
E

f (xS) − f (x∗)

(cid:105)

≤

2(m+1)
mS(β −5)

[f ((cid:101)x0) − f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2.

If f ((cid:101)xS) ≤ f (xS), then (cid:98)xS = (cid:101)xS, and

(cid:104)
E

(cid:105)
f ((cid:98)xS) − f (x∗)

(cid:104)
≤ E

(cid:105)
f (xS) − f (x∗)

≤

2(m+1)
mS(β −5)

[f ((cid:101)x0) − f (x∗)] +

Lβ(β −1)
2mS(β −5)

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Alternatively, if f ((cid:101)xS) ≥ f (xS), then (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

APPENDIX D: PROOF OF LEMMA 6

Proof. Since the average function f (x) is L-smooth, then for all x, y ∈ Rd,
L
2

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) +

(cid:107)y − x(cid:107)2,

which then implies

f (xs

k+1) ≤ f (xs

k) + (cid:10)∇f (xs

k), xs

k+1 − xs
k

(cid:11) +

L
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2 .
(cid:13)

Using the above inequality, we have

F (xs

k+1) = f (xs

k+1) + g(xs

k+1)

≤ f (xs

k) + g(xs

k+1) + (cid:10)∇f (xs

k), xs

= f (xs

k) + g(xs

k+1) + (cid:10)vs

k, xs

k+1 − xs
k

(cid:11) +

(cid:11) +
k+1 − xs
k
Lβ
2
L(β −1)
2

k+1 − xs
k

(cid:13)
(cid:13)xs

Lβ
2
k+1 − xs
(cid:107)xs

k(cid:107)2

(cid:107)xs

k+1 − xs

k(cid:107)2.

+ (cid:10)∇f (xs

k) − vs

k, xs

k+1 − xs
k

According to Lemma 5, then we obtain

(cid:11) −

(cid:13)
2 −
(cid:13)

L(β −1)
2

(cid:13)
(cid:13)xs

k+1 − xs
k

(cid:13)
2
(cid:13)

E

(cid:20)
(cid:10)∇f (xs
(cid:20)
≤ E

1
2L(β −1)
2
(cid:2)F (xs
β −1

≤

k) − vs

k, xs

k+1 − xs
k

(cid:11) −

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

(cid:107)∇f (xs

k) − vs

k(cid:107)2 +

(cid:107)xs

k+1 −xs

k(cid:107)2 −

L(β −1)
2

(cid:21)

(cid:107)xs

k+1 −xs

k(cid:107)2

L(β −1)
2
L(β −1)
2

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) ,

26

(33)

(34)

where the ﬁrst inequality holds due to the Young’s inequality, and the second inequality follows from Lemma 5. Substituting
the inequality (34) into the inequality (33), and taking the expectation over the random choice is

k, we arrive at

E(cid:2)F (xs

k)] + E(cid:2)g(xs
k+1)(cid:3) ≤ E[f (xs
2
β −1

(cid:2)F (xs

+

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)

(cid:20)
(cid:10)vs
k+1)(cid:3) + E

k, xs

k+1 − xs
k

(cid:11) +

Lβ
2

(cid:107)xs

k+1 − xs

k(cid:107)2

(cid:21)

≤ E[f (xs

(cid:20)
(cid:104)vs
k)] + g(x∗) + E
2
β −1

(cid:2)F (xs

+

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)

k, x∗ − xs

k(cid:105) +

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:21)
k+1(cid:107)2)

Lβ
2

≤ f (x∗) + g(x∗) + E

((cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2)

(cid:21)

(cid:20) Lβ
2

+

2
β −1

(cid:2)F (xs

= F (x∗) +

Lβ
2

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3)
k+1(cid:107)2(cid:3) +
E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

2
β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) ,

where the ﬁrst inequality holds due to the inequality (33) and the inequality (34); the second inequality follows from
Lemma 1 with ˆz = xs
k(cid:105)+g(z); and the third inequality holds due to
the fact that E[vs
k(cid:105) ≤ f (x∗). Then the above inequality is

k) and the convexity of f (·), i.e., f (xs

k, τ = Lβ = 1/η, and r(z) := (cid:104)vs

k+1, z = x∗, z0 = xs

k)+(cid:104)∇f (xs

k] = ∇f (xs

k), x∗ −xs

k, z −xs

rewritten as follows:

≤

E[F (xs
2
β −1

k+1)] − F (x∗)
(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

k+1(cid:107)2(cid:3) .

(35)

Summing the above inequality over k = 0, 1, . . . , (m−1) and taking expectation over whole history, we have

27

(cid:8)E[F (xs

k+1)] − F (x∗)(cid:9)

m−1
(cid:88)

k=0
m−1
(cid:88)

≤

(cid:26) 2

β −1

(cid:2)F (xs

k) − F (x∗) + F ((cid:101)xs−1) − F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ − xs

k(cid:107)2 − (cid:107)x∗ − xs

(cid:27)
k+1(cid:107)2(cid:3)

.

Subtracting 2
β−1

k=0
(cid:80)m−2
k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3)

from both sides of the above inequality, we obtain

m−1
(cid:88)

E(cid:2)F (xs

k+1) − F (x∗)(cid:3) −

2
β −1

m−1
(cid:88)

k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3) +

2
β −1

E[F (xs

m)−F (x∗)]

k=0
m−1
(cid:88)

(cid:26) 2

β −1

k=0

≤

Then

(cid:2)F (xs

(cid:27)
k)−F (x∗)+F ((cid:101)xs−1)−F (x∗)(cid:3)

−

2
β −1

m−2
(cid:88)

k=0

E(cid:2)F (xs

k+1)−F (x∗)(cid:3)+

Lβ
2

(cid:18)

1 −

2
β −1

(cid:19) m
(cid:88)

k=1

E[F (xs

k) − F (x∗)] +

2
β −1

E[F (xs

m)−F (x∗)]

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

≤

2
β −1

E[F (xs

0) − F (x∗)] +

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

(cid:80)m

k=1 xs

k and xs+1

m, and the convexity of the objective function F (·), we have F ((cid:101)xs) ≤

2m
β −1
0 = xs

Due to the settings of (cid:101)xs = 1

m

1
m

(cid:80)m

k=1 F (xs

k), and

(cid:18)

m

1 −

(cid:19)

2
β −1

E[F ((cid:101)xs) − F (x∗)] +

2
β −1

E[F (xs

m)−F (x∗)]

≤

2
β −1

E[F (xs

0) − F (x∗)] +

2m
β −1

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2

Dividing both sides of the above inequality by m, we arrive at

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

(cid:18)

1 −

(cid:19)

2
β −1

≤

2
(β −1)m

E[F (xs

E[F ((cid:101)xs) − F (x∗)] +
2
β −1

0) − F (x∗)] +

m)−F (x∗)]

E[F (xs

2
(β −1)m
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

Lβ
2m

E(cid:2)(cid:107)x∗ −xs

0(cid:107)2 −(cid:107)x∗ −xs

m(cid:107)2(cid:3) .

This completes the proof.

APPENDIX E: PROOF OF THEOREM 3

Proof. Summing the inequality in (21) over s = 1, 2, . . . , S, and taking expectation with respect to the history of is

k, we have

S
(cid:88)

(cid:18)

1−

(cid:19)

2
β −1

E[F ((cid:101)xs) − F (x∗)] +

S
(cid:88)

s=1

2
(β −1)m

E[F (xs

m) − F (x∗)]

2
(β −1)m

E[F (xs

0) − F (x∗)] +

2
β −1

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

(cid:27)

s=1
S
(cid:88)

(cid:26)

s=1

≤

+

Lβ
2m

S
(cid:88)

s=1

E(cid:2)(cid:107)x∗ − xs

0(cid:107)2 − (cid:107)x∗ − xs

m(cid:107)2(cid:3) .

E[F (xs

m)−F (x∗)]+ 2
β−1

(cid:80)S

s=1[F ((cid:101)xs)−F (x∗)] from both sides of the above inequality, and using

2
(β−1)m

Subtracting (cid:80)S
s=1
the setting of xs+1
0 = xs
S
(cid:18)
(cid:88)

m, we arrive at

1 −

s=1

≤

2
(β −1)m

(cid:19)

4
β −1

E[F ((cid:101)xs) − F (x∗)]

(cid:104)
E

F (x1

(cid:105)
0)−F (xS
m)

+

(cid:104)
E

2
β −1

F ((cid:101)x0)−F ((cid:101)xS)

(cid:105)

+

(cid:104)
E

Lβ
2m

(cid:107)x∗ −x1

0(cid:107)2 − (cid:107)x∗ −xS

m(cid:107)2(cid:105)

.

It is easy to verify that E[F ((cid:101)x0)−F ((cid:101)xS)] ≤ F ((cid:101)x0)−F (x∗). Dividing both sides of the above inequality by S, and using the
choice (cid:101)x0 = x1

0, we obtain

28

(cid:18)

1 −

4
β −1

(cid:19) 1
S

S
(cid:88)

E[F ((cid:101)xs) − F (x∗)]

≤

≤

=

2
(β −1)mS
2
(β −1)mS
2(m+1)
(β −1)mS

s=1
(cid:105)
0)−F (xS
m)

F (x1

(cid:104)

E

(cid:2)F ((cid:101)x0) − F (x∗)(cid:3) +

[F ((cid:101)x0) − F (x∗)] +

+

2
(β −1)S

2
(β −1)S
Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2

[F ((cid:101)x0) − F ((cid:101)xS)] +

(cid:107)x∗ − x1

0(cid:107)2

(36)

[F ((cid:101)x0) − F (x∗)] +

(cid:107)x∗ − (cid:101)x0(cid:107)2

Lβ
2mS
Lβ
2mS

m(cid:107)2 ≥ 0; and the last inequality holds due to the facts that E(cid:2)F (x1

0)−F (xS

m)(cid:3)

where the ﬁrst inequality uses the fact that (cid:107)x∗−xS
≤ F (x1

0)−F (x∗), E[F ((cid:101)x0)−F ((cid:101)xS)] ≤ F ((cid:101)x0)−F (x∗), and (cid:101)x0 = x1
0.
(cid:80)S

Using the deﬁnition of xS = 1
S

s=1 (cid:101)xs and the convexity of the objective function F (·), we have F (xS) ≤ 1

S

(cid:80)S

s=1F ((cid:101)xs),

and therefore we can rewrite the above inequality in (36) as

(cid:18)

1 −

(cid:19)

4
β −1

(cid:104)
E

(cid:105)
F (xS) − F (x∗)

≤

(cid:18)

1 −

4
β −1

(cid:19) 1
S

S
(cid:88)

s=1

E[F ((cid:101)xs) − F (x∗)]

≤

2(m+1)
(β −1)mS

[F ((cid:101)x0) − F (x∗)] +

Lβ
2mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Dividing both sides of the above inequality by (1− 4

β−1 ) > 0, we have

(cid:104)
E

(cid:105)
F (xS)

− F (x∗) ≤

2(m+1)
(β −5)mS

[F ((cid:101)x0) − F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

When F ((cid:101)xS) ≤ F (xS), then (cid:98)xS = (cid:101)xS, and
(cid:104)

E

(cid:105)
F ((cid:98)xS)

− F (x∗) ≤

2(m+1)
(β −5)mS

[F ((cid:101)x0) − F (x∗)] +

β(β −1)L
2(β −5)mS

(cid:107)(cid:101)x0 − x∗(cid:107)2.

Alternatively, if F ((cid:101)xS) ≥ F (xS), then (cid:98)xS = xS, and the above inequality still holds.

This completes the proof.

APPENDIX F: PROOF OF THEOREM 6

Proof. Since each fi(·) is convex and L-smooth, then we have
(cid:19)

(cid:18)

1 −

2
β −1
E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +

E[F ((cid:101)xs) − F (x∗)] +
2
m(β −1)
(cid:18)

2
m(β −1)

E(cid:2)F ((cid:101)xs−1)−F (x∗)(cid:3) +
CLβ
mµ

2
m(β −1)
E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3)

+

+

(cid:19)

Lβ
mµ

≤

≤

≤

2
β −1
2
β −1
(cid:18) 2(m+C)
m(β −1)

E(cid:2)F (xs+1

0

)−F (x∗)(cid:3) +

E(cid:2)(cid:107)xs+1

0 −x∗(cid:107)2(cid:3)

Lβ
2m
0 −x∗(cid:107)2(cid:3)
E(cid:2)(cid:107)xs

E[F (xs

0) − F (x∗)] +

Lβ
2m

(cid:19)

E[F (xs

0) − F (x∗)]

where the ﬁrst inequality follows from Lemma 6; the second inequality holds due to the fact that (cid:107)xs
F (x∗)]; and the last inequality follows from Assumption 3.

0−x∗(cid:107)2 ≤ (2/µ)[F (xs

0)−

Due to the deﬁnition of β = 1/(Lη), the above inequality is rewritten as follows:

1−3Lη
1−Lη

E[F ((cid:101)xs) − F (x∗)] ≤

(cid:18) 2Lη(m+C)
m(1−Lη)

+

C
mµη

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) .

Dividing both sides of the above inequality by (1−3Lη)(1−Lη) > 0, we arrive at

E[F ((cid:101)xs) − F (x∗)] ≤

(cid:18) 2Lη(m+C)
m(1−3Lη)

+

C(1−Lη)
mµη(1−3Lη)

(cid:19)

E(cid:2)F ((cid:101)xs−1) − F (x∗)(cid:3) .

This completes the proof.

29

REFERENCES

[1] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent using predictive variance reduction,” in Proc. Adv. Neural Inf. Process.

Syst., 2013, pp. 315–323.

[2] L. Xiao and T. Zhang, “A proximal stochastic gradient method with progressive variance reduction,” SIAM J. Optim., vol. 24, no. 4, pp.

2057–2075, 2014.

[3] Z. Allen-Zhu, “Katyusha: The ﬁrst direct acceleration of stochastic gradient methods,” in Proc. 49th ACM Symp. Theory Comput., 2017.
[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional neural networks,” in Proc. Adv. Neural Inf.

[5]

Process. Syst., 2012, pp. 1097–1105.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of initialization and momentum in deep learning,” in Proc. 30th Int.
Conf. Mach. Learn., 2013, pp. 1139–1147.

[6] Z. Allen-Zhu and E. Hazan, “Variance reduction for faster non-convex optimization,” in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 699–707.
[7] H. Ouyang, N. He, L. Q. Tran, and A. Gray, “Stochastic alternating direction method of multipliers,” in Proc. 30th Int. Conf. Mach. Learn., 2013,

pp. 80–88.

[8] Y. Liu, F. Shang, and J. Cheng, “Accelerated variance reduced stochastic ADMM,” in Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 2287–2293.
[9] C. Qu, Y. Li, and H. Xu, “Linear convergence of SVRG in statistical estimation,” arXiv:1611.01957v2, 2017.

[10] C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui, “Catalyst acceleration for gradient-based non-convex optimization,”

arXiv:1703.10993, 2017.

[11] J. Duchi and F. Ruan, “Stochastic methods for composite optimization problems,” arXiv:1703.08570, 2017.
[12] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-scale matrix completion,” Math. Prog. Comp., vol. 5, pp. 201–226, 2013.

[13] X. Zhang, L. Wang, and Q. Gu, “Stochastic variance-reduced gradient descent for low-rank matrix recovery from linear measurements,”

arXiv:1701.00481v2, 2017.

[14] M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar, “Non-uniform stochastic average gradient method for training

conditional random ﬁelds,” in Proc. 18th Int. Conf. Artif. Intell. Statist., 2015, pp. 819–828.

[15] O. Shamir, “A stochastic PCA and SVD algorithm with an exponential convergence rate,” in Proc. 32nd Int. Conf. Mach. Learn., 2015, pp.

144–152.

[16] D. Garber, E. Hazan, C. Jin, S. M. Kakade, C. Musco, P. Netrapalli, and A. Sidford, “Faster eigenvector computation via shift-and-invert

preconditioning,” in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 2626–2634.

[17] Z. Allen-Zhu and Y. Li, “Doubly accelerated methods for faster CCA and generalized eigendecomposition,” arXiv:1607.06017v2, 2017.
[18] Y. Nesterov, “A method of solving a convex programming problem with convergence rate O(1/k2),” Soviet Math. Doklady, vol. 27, pp.

372–376, 1983.

[19] ——, Introductory Lectures on Convex Optimization: A Basic Course. Boston: Kluwer Academic Publ., 2004.
[20] P. Tseng, “On accelerated proximal gradient methods for convex-concave optimization,” Technical report, University of Washington, 2008.
[21] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM J. Imaging Sci., vol. 2, no. 1,

pp. 183–202, 2009.

[22] H. Robbins and S. Monro, “A stochastic approximation method,” Ann. Math. Statist., vol. 22, no. 3, pp. 400–407, 1951.
[23] T. Zhang, “Solving large scale linear prediction problems using stochastic gradient descent algorithms,” in Proc. 21st Int. Conf. Mach. Learn.,

2004, pp. 919–926.

[24] C. Hu, J. T. Kwok, and W. Pan, “Accelerated gradient methods for stochastic optimization and online learning,” in Proc. Adv. Neural Inf.

Process. Syst., 2009, pp. 781–789.

[25] S. Bubeck, “Convex optimization: Algorithms and complexity,” Found. Trends Mach. Learn., vol. 8, pp. 231–358, 2015.
[26] A. Rakhlin, O. Shamir, and K. Sridharan, “Making gradient descent optimal for strongly convex stochastic optimization,” in Proc. 29th Int.

Conf. Mach. Learn., 2012, pp. 449–456.

[27] O. Shamir and T. Zhang, “Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes,”

in Proc. 30th Int. Conf. Mach. Learn., 2013, pp. 71–79.

[28] N. L. Roux, M. Schmidt, and F. Bach, “A stochastic gradient method with an exponential convergence rate for ﬁnite training sets,” in Proc.

Adv. Neural Inf. Process. Syst., 2012, pp. 2672–2680.

[29] S. Shalev-Shwartz and T. Zhang, “Stochastic dual coordinate ascent methods for regularized loss minimization,” J. Mach. Learn. Res., vol. 14,

pp. 567–599, 2013.

[30] A. Defazio, F. Bach, and S. Lacoste-Julien, “SAGA: A fast incremental gradient method with support for non-strongly convex composite

objectives,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 1646–1654.

[31] Y. Zhang and L. Xiao, “Stochastic primal-dual coordinate method for regularized empirical risk minimization,” in Proc. 32nd Int. Conf. Mach.

Learn., 2015, pp. 353–361.

[32] M. Schmidt, N. L. Roux, and F. Bach, “Minimizing ﬁnite sums with the stochastic average gradient,” INRIA, Paris, Tech. Rep., 2013.
[33] S. Shalev-Shwartz and T. Zhang, “Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization,” Math. Program.,

vol. 155, pp. 105–145, 2016.

[34] F. Shang, Y. Liu, J. Cheng, and J. Zhuo, “Fast stochastic variance reduced gradient method with momentum acceleration for machine

learning,” arXiv:1703.07948, 2017.

30

[35] L. Zhang, M. Mahdavi, and R. Jin, “Linear convergence with condition number independent access of full gradients,” in Proc. Adv. Neural

Inf. Process. Syst., 2013, pp. 980–988.

[36] J. Koneˇcn ´y, J. Liu, P. Richt´arik, , and M. Tak´aˇc, “Mini-batch semi-stochastic gradient descent in the proximal setting,” IEEE J. Sel. Top. Sign.

Proces., vol. 10, no. 2, pp. 242–255, 2016.

[37] A. Nitanda, “Stochastic proximal gradient descent with acceleration techniques,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 1574–1582.
[38] G. Lan and Y. Zhou, “An optimal randomized incremental gradient method,” arXiv:1507.02000v3, 2015.

[39] R. Frostig, R. Ge, S. M. Kakade, and A. Sidford, “Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical

risk minimization,” in Proc. 32nd Int. Conf. Mach. Learn., 2015, pp. 2540–2548.

[40] H. Lin, J. Mairal, and Z. Harchaoui, “A universal catalyst for ﬁrst-order optimization,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp.

3366–3374.

[41] R. Babanezhad, M. O. Ahmed, A. Virani, M. Schmidt, J. Konecny, and S. Sallinen, “Stop wasting my gradients: Practical SVRG,” in Proc. Adv.

Neural Inf. Process. Syst., 2015, pp. 2242–2250.

[42] Z. Allen-Zhu and Y. Yuan, “Improved SVRG for non-strongly-convex or sum-of-non-convex objectives,” in Proc. 33rd Int. Conf. Mach. Learn.,

2016, pp. 1080–1089.

[43] M. Frank and P. Wolfe, “An algorithm for quadratic programming,” Naval Res. Logist. Quart., vol. 3, pp. 95–110, 1956.
[44] E. Hazan and H. Luo, “Variance-reduced and projection-free stochastic optimization,” in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1263–1271.
[45] F. Shang, Y. Liu, J. Cheng, K. W. Ng, and Y. Yoshida, “Variance reduced stochastic gradient descent with sufﬁcient decrease,” arXiv:1703.06807,

2017.

[46] L. Hien, C. Lu, H. Xu, and J. Feng, “Accelerated stochastic mirror descent algorithms for composite non-strongly convex optimization,”

arXiv:1605.06892v2, 2016.

[47] B. Woodworth and N. Srebro, “Tight complexity bounds for optimizing composite objectives,” in Proc. Adv. Neural Inf. Process. Syst., 2016,

pp. 3639–3647.

[48] A. Defazio, “A simple practical accelerated method for ﬁnite sums,” in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 676–684.

[49] X. Li, T. Zhao, R. Arora, H. Liu, and J. Haupt, “Nonconvex sparse learning via stochastic optimization with progressive variance reduction,”

in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 917–925.

[50] L. Nguyen, J. Liu, K. Scheinberg, and M. Tak´aˇc, “SARAH: A novel method for machine learning problems using stochastic recursive

gradient,” arXiv:1703.00102v1, 2017.

[51] L. W. Zhong and J. T. Kwok, “Fast stochastic alternating direction method of multipliers,” in Proc. 31st Int. Conf. Mach. Learn., 2014, pp. 46–54.
[52] S. Zheng and J. T. Kwok, “Fast-and-light stochastic ADMM,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 2407–2613.
[53] S. J. Reddi, S. Sra, B. Poczos, and A. Smola, “Proximal stochastic methods for nonsmooth nonconvex ﬁnite-sum optimization,” in Proc. Adv.

Neural Inf. Process. Syst., 2016, pp. 1145–1153.

[54] S. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, “On variance reduction in stochastic gradient descent and its asynchronous variants,” in

Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 2629–2637.

[55] J. D. Lee, Q. Lin, T. Ma, and T. Yang, “Distributed stochastic variance reduced gradient methods and a lower bound for communication

complexity,” arXiv:1507.07595v2, 2016.

[56] Y. Bengio, “Learning deep architectures for AI,” Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1–127, 2009.
[57] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points online stochastic gradient for tensor decomposition,” in Proc. 28th Conf.

Learn. Theory, 2015, pp. 797–842.

[58] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens, “Adding gradient noise improves learning for very

deep networks,” arXiv:1511.06807, 2015.

[59] N. Flammarion and F. Bach, “From averaging to acceleration, there is only a step-size,” in Proc. 28th Conf. Learn. Theory, 2015, pp. 658–695.
[60] C. Tan, S. Ma, Y. Dai, and Y. Qian, “Barzilai-Borwein step size for stochastic gradient descent,” in Proc. Adv. Neural Inf. Process. Syst., 2016,

pp. 685–693.

[61] Z. Allen-Zhu and E. Hazan, “Optimal black-box reductions between optimization objectives,” in Proc. Adv. Neural Inf. Process. Syst., 2016, pp.

1606–1614.

[62] B. Carpenter, “Lazy sparse stochastic gradient descent for regularized multinomial logistic regression,” Tech. Rep., 2008.
[63] J. Langford, L. Li, and T. Zhang, “Sparse online learning via truncated gradient,” J. Mach. Learn. Res., vol. 10, pp. 777–801, 2009.
[64] Y. Nesterov, “Smooth minimization of non-smooth functions,” Math. Program., vol. 103, pp. 127–152, 2005.
[65] Y. Xu, Y. Yan, Q. Lin, and T. Yang, “Homotopy smoothing for non-smooth problems with lower complexity than O(1/(cid:15)),” in Proc. Adv. Neural

Inf. Process. Syst., 2016, pp. 1208–1216.

[66] P. Zhao and T. Zhang, “Stochastic optimization with importance sampling for regularized loss minimization,” in Proc. 32nd Int. Conf. Mach.

Learn., 2015, pp. 1–9.

[67] D. Needell, N. Srebro, and R. Ward, “Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm,” Math.

Program., vol. 155, pp. 549–573, 2016.

[68] J. Koneˇcn ´y and P. Richt´arik, “Semi-stochastic gradient descent methods,” ArXiv Preprint: 1312.1666v2, 2015.
[69] G. Lan, “An optimal method for stochastic composite optimization,” Math. Program., vol. 133, pp. 365–397, 2012.

31

(a) Adult: λ1 = 10−4

(b) Protein: λ1 = 10−4

(c) Covtype: λ1 = 10−4

(d) Sido0: λ1 = 5∗10−3

(e) Adult: λ1 = 10−5

(f) Protein: λ1 = 10−5

(g) Covtype: λ1 = 10−5

(h) Sido0: λ1 = 10−4

(i) Adult: λ1 = 10−6

(j) Protein: λ1 = 10−6

(k) Covtype: λ1 = 10−6

(l) Sido0: λ1 = 10−5

Fig. 5. Comparison of SVRG [1], Prox-SVRG [2], Katyusha [3], and VR-SGD for solving (cid:96)2-norm regularized logistic regression problems (i.e.,
λ2 = 0). In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes (top)
or running time (bottom).

0510152025303510−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152025303510−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01020304010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD00.511.5210−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD00.511.522.5310−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01020304010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152025303510−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152025303510−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD00.511.522.5310−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD012345610−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD051015202510−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−4Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD0123456710−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD05101510−1210−1010−810−610−4Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD051015202510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010010−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD32

(a) λ2 = 10−4

(b) λ2 = 10−3

(c) λ2 = 10−5

(d) λ2 = 10−4

Fig. 6. Comparison of SVRG, Prox-SVRG [2], Katyusha [3], and VR-SGD for (cid:96)1-norm regularized logistic regression problems (i.e., λ1 = 0) on the
four data sets: Adult (the ﬁrst column), Protein (the sconced column), Covtype (the third column), and Sido0 (the last column). In each plot, the

vertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes (top) or running time (bottom).

(e) λ2 = 10−6

(f) λ2 = 10−5

010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD00.511.5210−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD024681010−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015010−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020025030010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0246810−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05101520253010−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0102030405010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020025030010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020025030035010−810−710−610−510−410−3Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020025030035010−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020025030035010−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0102030405010−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015010−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD33

(a) λ1 = 10−5 and λ2 = 10−4

(b) λ1 = 10−4 and λ2 = 10−5

(c) λ1 = 10−5 and λ2 = 10−5

(d) λ1 = 10−5 and λ2 = 10−4

(e) λ1 = 10−6 and λ2 = 10−5

(f) λ1 = 10−5 and λ2 = 10−5

Fig. 7. Comparison of SVRG, Prox-SVRG [2], Katyusha [3], and VR-SGD for solving elastic net regularized logistic regression problems on the four

data sets: Adult (the ﬁrst column), Protein (the sconced column), Covtype (the third column), and Sido0 (the last column). In each plot, the vertical

axis shows the objective value minus the minimum, and the horizontal axis is the number of effective passes (top) or running time (bottom).

0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01020304010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01020304050607010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD00.511.522.533.510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01234510−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0123456710−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD010203040506010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010012010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0102030405010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD0123410−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD01234510−1210−1010−810−610−4Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD024681010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD02040608010010−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−4Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD02040608010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015020010−1210−1010−810−610−410−2Gradient evaluations / nF (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD024681010−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD0510152010−1210−1010−810−610−4Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD0510152025303510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRGProx−SVRGKatyushaVR−SGD05010015010−1210−1010−810−610−410−2Running time (sec)F (xs ) − F (x* )  SVRGProx−SVRGKatyushaVR−SGD34

(a) λ = 10−3

(b) λ = 10−4

(c) λ = 10−5

(d) λ = 10−6

(e) λ = 10−7

(f) λ = 0

Fig. 8. Comparison of SVRG [1], Katyusha [3], VR-SGD and their proximal versions for solving ridge regression problems with different regularization

parameters on the Adult data set. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the number

of effective passes (top) or running time (bottom).

0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0102030405010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.511.510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.511.510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II00.511.510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II02040608010010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II05010015010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II05010015010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II00.511.5210−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0123456710−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II0123456710−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II35

(a) λ = 10−3

(b) λ = 10−4

(c) λ = 10−5

(d) λ = 10−6

(e) λ = 10−7

(f) λ = 0

Fig. 9. Comparison of SVRG [1], Katyusha [3], VR-SGD and their proximal versions for solving ridge regression problems with different regularization

parameters on the Covtype data set. In each plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the

number of effective passes (top) or running time (bottom).

05101520253010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II01020304010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II01020304010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II0246810−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II02468101210−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II02468101210−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010203040506010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II05010015010−1210−1010−810−610−410−2Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II010020030040050010−210−1Gradient evaluations / nF(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II05101510−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II01020304050607010−1210−1010−810−610−410−2Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIKatyusha−IKatyusha−IIVR−SGD−IVR−SGD−II02040608010012010−210−1Running time (sec)F(xs ) − F(x* )SVRG−ISVRG−IIVR−SGD−IVR−SGD−II36

(a) (cid:96)1-norm regularized logistic regression: Adult (left) and Covtype (right)

Fig. 10. Comparison of Algorithm 3 with ﬁxed and varying learning rates for solving (cid:96)1-norm (i.e., λ(cid:107)x(cid:107)1) regularized logistic regression and Lasso
problems with λ = 10−4 (blue lines) and λ = 10−5 (red lines, best viewed in colors). Note that the regularization parameter is set to 10−3 and
10−4 for solving Lasso problems on the Covtype data set.

(b) Lasso: Adult (left) and Covtype (right)

01020304010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Fixed learning rateVarying learning rateFixed learning rateVarying learning rate0510152010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Fixed learning rateVarying learning rateFixed learning rateVarying learning rate05101520253010−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Fixed learning rateVarying learning rateFixed learning rateVarying learning rate05101510−1210−1010−810−610−4Gradient evaluations / nF(xs ) − F(x* )Fixed learning rateVarying learning rateFixed learning rateVarying learning rate