BiAdam: Fast Adaptive Bilevel Optimization Methods

BiAdam: Fast Adaptive Bilevel Optimization Methods

Feihu Huang
Department of Electrical and Computer Engineering
University of Pittsburgh, Pittsburgh, USA

Heng Huang
Department of Electrical and Computer Engineering
University of Pittsburgh, Pittsburgh, USA

Editor:

huangfeihu2018@gmail.com

henghuanghh@gmail.com

Abstract

Bilevel optimization recently has attracted increased interest in machine learning due to
its many applications such as hyper-parameter optimization and policy optimization. Al-
though some methods recently have been proposed to solve the bilevel problems, these
methods do not consider using adaptive learning rates. To ﬁll this gap, in the paper, we
propose a class of fast and eﬀective adaptive methods for solving bilevel optimization prob-
lems that the outer problem is possibly nonconvex and the inner problem is strongly-convex.
Speciﬁcally, we propose a fast single-loop BiAdam algorithm based on the basic momentum
technique, which achieves a sample complexity of ˜O(ǫ−4) for ﬁnding an ǫ-stationary point.
At the same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam)
by using variance reduced technique, which reaches the best known sample complexity
of ˜O(ǫ−3). To further reduce computation in estimating derivatives, we propose a fast
single-loop stochastic approximated BiAdam algorithm (saBiAdam) by avoiding the Hes-
sian inverse, which still achieves a sample complexity of ˜O(ǫ−4) without large batches. We
further present an accelerated version of saBiAdam algorithm (VR-saBiAdam), which also
reaches the best known sample complexity of ˜O(ǫ−3). We apply the uniﬁed adaptive ma-
trices to our methods as the SUPER-ADAM (Huang et al., 2021), which including many
types of adaptive learning rates. Moreover, our framework can ﬂexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence analysis
framework for both the constrained and unconstrained bilevel optimization. To the best
of our knowledge, we ﬁrst study the adaptive bilevel optimization methods with adaptive
learning rates.

Keywords: Bilevel Optimization, Momentum, Adaptive Learning Rate, Variance Re-
duced

1. Introduction

In the paper, we consider solving the following stochastic bilevel optimization problem,
deﬁned as

1
2
0
2

n
u
J

0
3

]

C
O
.
h
t
a
m

[

2
v
6
9
3
1
1
.
6
0
1
2
:
v
i
X
r
a

F (x) = Eξ

min
x
∈X
s.t. y∗(x)

,

f (x, y∗(x); ξ)
(cid:3)
g(x, y; ζ)
(cid:3)

(cid:2)
arg min
y
∈Y

Eζ

(cid:2)

∈

,

(Outer)

(Inner)

(1)

(2)

1

 
 
 
 
 
 
Feihu and Heng

k∇

k ≤

F (x)

Table 1: Sample complexity of the representative bilevel optimization methods for ﬁnding
an ǫ-stationary point of the nonconvex (outer) strongly-convex (inner) bilevel problem
(1), i.e., E
ǫ or its equivalent variants. BSize denotes mini-batch size; ALR
denotes adaptive learning rate. C(x, y) denotes the constraint sets in variables x and y
respectively, where Y denotes the fact that there exists a convex constraint on variable, oth-
erwise is N. DD denotes dimension dependence in the gradient estimators, and p denotes
the dimension of variable y. 1 denotes Lipschitz continuous of
∇yf (x, y; ξ),
∇xf (x, y; ξ),
2
yyg(x, y; ζ) for all ξ, ζ; 2 denotes Lipschitz continuous of
∇yg(x, y; ζ),
∇
2
2
yyg(x, y); 3 denotes bounded stochas-
∇yf (x, y),
xyg(x, y) and
∇xf (x, y),
2
xyg(x, y; ζ); 4 denotes bounded stochastic partial
tic partial derivatives
2
yyg(x, y; ζ); 5 denotes the bounded true partial derivatives
derivatives
∇xf (x, y; ξ), and
2
xyg(x, y); 6 denotes Lipschitz continuous of function f (x, y; ξ); 7 denotes
∇yf (x, y) and
g(x, y; ζ) is Lg-smooth and µ-strongly convex function w.r.t. y for all ζ; 8 denotes g(x, y)
is Lg-smooth and µ-strongly convex function w.r.t. y.

∇
∇yg(x, y),
∇yf (x, y; ξ) and
∇

2
xyg(x, y; ζ) and

∇

∇

∇

∇

Algorithm
BSA
TTSA
stocBiO
STABLE
SMB
SUSTAIN
SVRB
MRBO
VRBO
BiAdam
VR-BiAdam
saBiAdam
VR-saBiAdam

Reference
Ghadimi and Wang (2018)
Hong et al. (2020)
Ji et al. (2020)
Chen et al. (2021)
Guo et al. (2021)
Khanduri et al. (2021)
Guo and Yang (2021)
Yang et al. (2021)
Yang et al. (2021)
Ours
Ours
Ours
Ours

Complexity BSize
˜O(1)
˜O(1)
˜O(ǫ−
O(1)
˜O(1)
˜O(1)
O(1)
˜O(1)
˜O(ǫ−
O(1)
O(1)
˜O(1)
˜O(1)

O(ǫ−
˜O(ǫ−
O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−
˜O(ǫ−

6)
5)
4)
4)
4)
3)
3)
3)
3)
4)
3)
4)
3)

Loop
Double
Single
2) Double
Single
Single
Single
Single
Single
2) Double
Single
Single
Single
Single

C(x, y) DD ALR Conditions
Y, N
Y, N
N, N
N, N
N, Y
N, N
N, N
N, N
N, N
Y, Y
Y, Y
Y, Y
Y, Y

2, 5, 7
1, 3, 7
1, 6, 7
1, 3, 4, 8
2, 5, 7
1, 3, 7
1, 5, 8
1, 6, 7
1, 6, 7
2, 5, 8
1, 5, 8
2, 5, 7
1, 5, 7

p2
p2
p2
p3
p2
p2
p3
p2
p2
p3
p3
p2
p2

√
√
√
√

(cid:2)

Y ⊆

Rd and

g(x, y; ζ)
(cid:3)
(cid:2)

f (x, y∗(x); ξ)
(cid:3)

where F (x) = f (x, y∗(x)) = Eξ
function, and g(x, y) = Eζ

is a diﬀerentiable and possibly nonconvex
is a diﬀerentiable and strongly convex function. Here
Rp are convex closed sets, ξ and ζ are random variables. The problem
X ⊆
(1) involves many machine learning problems with a hierarchical structure, which include
hyper-parameter optimization (Franceschi et al., 2018), meta-learning (Franceschi et al.,
2018), reinforcement learning (Hong et al., 2020) and neural network architecture search
(Liu et al., 2018). Next, we speciﬁcally provide two popular applications that can be for-
mulated as the bilevel problem (1).

Policy Optimization. Policy optimization aims to ﬁnd the optimal policy for maxi-
mizing the expected total discounted reward. Actor-critic methods (Konda and Tsitsiklis,
2000) are a class of popular policy optimization methods, in which the actor is to ﬁnd the
optimal approximated action function (Q-function), and the critic is to ﬁnd the optimal
policy based on the optimal approximated Q-function. Consider a discounted Markov de-
cision process (MDP) (
denote the state and action spaces
s, a) denotes the transition kernel to the next state s′ given the cur-
respectively, and P (s′|
[0, 1) is the
rent state s and action a, and r(s, a)
R denotes a stationary policy that is the probability of
discount factor. π(s, a) :
the agent choosing action a
s, a) denotes

[0, 1] is the reward at (s, a), and γ

, P, r, γ), where

at state s

s) =

and

π(a

A

A

∈

S

S

,

∈
s)P (s′|
|

S × A →
∈ A

. P π(s′|

∈ S

2

a

∈A

P

BiAdam: Fast Adaptive Bilevel Optimization Methods

. Qπ(s, a) = Eπ

an induced Markov transition kernel, and µπ denotes an induced stationary distribution
0 γtr(st, at)
over
s0 = s, a0 = a
denotes the action value function,
t
S
|
≥
and let E
Qπ(s,
. As in (Hong et al., 2020), the actor-critic
, s)
), π(
=
i
·
·
h
method can be represented the following bilevel problem

Qπ(s, a)
(cid:2) P
(cid:3)

π(

s)

∼

(cid:3)

(cid:2)

·|

a

min
π
∈

Π −h

Qθ∗(π), π

iρ0

s.t. θ∗(π)

∈

arg min
θ

Θ (cid:26)

∈

1
2 k

Qθ −

r

γP πQθk

2
µπ

⊗

π +

−

λ
θ
2 k

2
k

,
(cid:27)

(3)

(4)

⊗

k · kµπ

where λ > 0 denotes the tuning parameter, and ρ0 is the initial state distribution, and
π denote induced inner product and weighted norm respectively as in
·iρ0 and
,
h·
(Hong et al., 2020). Here Qθ denotes the parameterize Q-function such as a linear approx-
Rd denotes a known feature
Qθ(s, a) = φ(s, a)T θ, where φ :
imation Q(s, a)
Rd denotes a ﬁnite-dimensional parameter. The above outer problem
mapping and θ
(3) is generally nonconvex when using the neural networks to approximate policy, and the
above inner problem (4) is strongly convex.

S × A →

≈
∈

Hyper-parameter optimization. The goal of hyper-parameter optimization is to
Rd (e.g., regularization coeﬃcient, learn-
search for the optimal hyper-parameters x
∈ X ⊆
ing rate, neural network architecture). These optimal hyper-parameters used in training a
Rd on the training set aims to make the learned model achieve the low-
model θ
est risk on the validation set. Speciﬁcally, consider hyper-parameter as the regularization
coeﬃcient as in (Franceschi et al., 2018), we solve the following bilevel problem

⊆

Θ

∈

F (x) = Eξ

∼Dval[ℓ(θ∗(x); ξ)]

min
x
∈X

s.t. θ∗(x)

∈

arg min
θ

Θ (cid:26)

∈

Eζ

∼D

tra[ℓ(θ; ζ)] + λ

d

Xi=1

xiθ2

,
i (cid:27)

(5)

(6)

where λ > 0 is a tuning parameter, and ℓ(θ; ξ) is the loss function on datum ξ, and
and

Dval denote the training and validation datasets, respectively.
Since the bilevel optimization has been widely applied in machine learning, multiple

Dtra

works recently have been begun to study the bilevel optimization. For example, (Ghadimi and Wang,
2018; Ji et al., 2020) proposed a class of double-loop methods to solve the problem (1).
However, to obtain an accurate estimate, the BSA in (Ghadimi and Wang, 2018) needs to
solve the inner problem to a high accuracy, and the stocBiO in (Ji et al., 2020) requires
large batch-sizes in solving the inner problem. Subsequently, (Hong et al., 2020) proposed
a class of single-loop methods to solve the bilevel problems. More recently, (Chen et al.,
2021; Khanduri et al., 2021; Guo and Yang, 2021; Yang et al., 2021) presented some accel-
erated single-loop methods by using the momentum-based variance reduced technique of
STORM (Cutkosky and Orabona, 2019). Although these methods can eﬀectively solve the
bilevel problems, they do not consider using the adaptive learning rates and only consider
the bilevel problems under unconstrained setting. Since using generally diﬀerent learning
rates for the inner and outer problems to ensure the convergence of bilevel optimization
problems, we will consider using diﬀerent adaptive learning rates for the inner and outer
problems with convergence guarantee. Clearly, this can not follow the exiting adaptive
methods for single-level problems. Thus, there exists a natural question:

3

Feihu and Heng

How to design the eﬀective methods with adaptive learning rates for the
bilevel problems under both unconstrained and constrained settings ?

In the paper, we provide an aﬃrmative answer to this question and propose a class of
fast single-loop adaptive bilevel optimization methods based on uniﬁed adaptive matrices
as the SUPER-ADAM in (Huang et al., 2021), which including many types of adaptive
learning rates. Moreover, our framework can ﬂexibly use the momentum and variance
reduced techniques. Our main contributions are summarized as follows:

1) We propose a fast single-loop adaptive bilevel optimization algorithm (BiAdam) based
on the basic momentum technique, which achieves a sample complexity of ˜O(ǫ−
4) for
ﬁnding an ǫ-stationary point. At the same time, we propose a single-loop accelerated
version of BiAdam algorithm (VR-BiAdam) by using the variance reduced technique,
which reaches the best known sample complexity of ˜O(ǫ−
3) that matches the lower
bound of stochastic smooth nonconvex optimization.

2) To further reduce computation in estimating derivatives, we propose a fast single-
loop stochastic approximated BiAdam algorithm (saBiAdam) by avoiding the Hessian
inverse, which still achieves a sample complexity of ˜O(ǫ−
4) without large batches.
We further present a single-loop accelerated version of saBiAdam algorithm (VR-
saBiAdam), which also reaches the best known sample complexity of ˜O(ǫ−

3).

3) We provide a useful convergence analysis framework for both the constrained and
unconstrained bilevel programming under some mild conditions (Please see Table 1).

2. Related Works

In this section, we overview the existing bilevel optimization methods and adaptive methods
for single-level optimization, respectively.

2.1 Bilevel Optimization Methods

Bilevel optimization has shown successes in many machine learning problems with hierar-
chical structures such as policy optimization and model-agnostic meta-learning. Thus, its
researches have become active in the machine learning community, and some bilevel opti-
mization methods recently have been proposed. For example, one class of successful meth-
ods (Colson et al., 2007; Kunapuli et al., 2008) are to reformulate the bilevel problem as a
single-level problem by replacing the inner problem by its optimality conditions. Another
class of successful methods (Ghadimi and Wang, 2018; Hong et al., 2020; Ji et al., 2020;
Chen et al., 2021; Khanduri et al., 2021; Guo and Yang, 2021) for bilevel optimization are
to iteratively approximate the (stochastic) gradient of the outer problem either in forward
or backward. Moreover, the non-asymptotic analysis of these bilevel optimization meth-
ods has been recently studied. For example, (Ghadimi and Wang, 2018) ﬁrst studied the
6) of the proposed double-loop algorithm for the bilevel problem
sample complexity of O(ǫ−
(1) (Please see Table 1). Subsequently, (Ji et al., 2020) proposed an accelerated double-
4) relying on large batches. At
loop algorithm that reaches the sample complexity of O(ǫ−

4

BiAdam: Fast Adaptive Bilevel Optimization Methods

the same time, (Hong et al., 2020) studied a single-loop algorithm that reaches the sample
5) without relying on large batches. Moreover, (Khanduri et al., 2021;
complexity of O(ǫ−
Guo and Yang, 2021; Yang et al., 2021) proposed a class of accelerated single-loop methods
for the bilevel problem (1) by using variance-reduced technique, which achieve the best
known sample complexity of O(ǫ−

3).

2.2 Adaptive Gradient Methods

Adaptive gradient methods recently have been shown great successes in current machine
learning problems, and have been widely studied. For example, Adagrad (Duchi et al.,
2011) is the ﬁrst adaptive gradient method that shows good performances under the sparse
gradient setting. One variant of Adagrad, i.e., Adam (Kingma and Ba, 2014) is a very
popular adaptive gradient method and basically is a default method of choice for train-
ing deep learning models. Subsequently, some variants of Adam algorithm (Reddi et al.,
2019; Chen et al., 2019) have been developed and studied, and especially they have conver-
gence guarantee under the nonconvex setting. At the same time, some adaptive gradient
methods (Chen et al., 2018; Zhuang et al., 2020) have been presented to improve the gen-
eralization performance of Adam algorithm. The norm version of AdaGrad (i.e., AdaGrad-
Norm) (Ward et al., 2019) has been presented to accelerate AdaGrad without sacriﬁcing
generalization. Moreover, some accelerated adaptive gradient methods such as STORM
(Cutkosky and Orabona, 2019) and SUPER-ADAM (Huang et al., 2021) have been pro-
posed by using variance-reduced technique. In particular, SUPER-ADAM (Huang et al.,
2021) is a faster and universal framework of adaptive gradients methods, which use an
adaptive matrices including many types of adaptive learning rates.

3. Preliminaries

3.1 Notations

i

x, y
h

denotes the ℓ2 norm for vectors and spectral norm for matrices.

denotes the inner
k · k
product of two vectors x and y. For vectors x and y, xr (r > 0) denotes the element-wise
power operation, x/y denotes the element-wise division and max(x, y) denotes the element-
wise maximum. Id denotes a d-dimensional identity matrix. AT denotes transpose of matrix
) denotes function w.r.t. the second variable with ﬁxing
A. Given function f (x, y), f (x,
·
, y) denotes function w.r.t. the ﬁrst variable with ﬁxing y. a = O(b) denotes
x, and f (
·
Cb for some constant C > 0. The notation ˜O(
) hides logarithmic terms. Given a
that a
·
≤
1
2.
x
, we deﬁne a projection operation to
convex closed set
2 k
k
X
for some constant
x
∈
{
Rd
for some constant
×
∈
d : X
Sµ can be
×

as
(z) = arg minx
PX
Rd :
C
x
] denotes Euclidean projection onto the set
ΠC[
k
·
C > 0. ˆΠC[
p :
C
X
] denotes a projection on the set
k
·
C > 0.
µId}
X
Sµ denotes a projection on the set
{
implemented by using SVD and thresholding the singular values.

k ≤
}
k ≤
}
. Both ˆΠC and

X
{
Rd
∈

∈X

(cid:23)

−

X

z

3.2 Some Mild Assumptions

In the subsection, we introduce some mild assumptions on the problem (1).

5

Feihu and Heng

(cid:23)

µIp.

Assumption 1 For any x, g(x, y) is Lg-smooth and µ-strongly convex function, i.e., LgIp (cid:23)
2
yyg(x, y)
∇
Assumption 2 For functions f (x, y) and g(x, y) for all x
following conditions hold:
is Lg-Lipschitz continuous,
Lipschitz continuous. For example, for all x, x1, x2 ∈ X

, we assume the
∈ X
∇yg(x, y)
∇yf (x, y) are Lf -Lipschitz continuous,
2
yyg(x, y) is Lgyy-
, we have

2
xyg(x, y) is Lgxy-Lipschitz continuous,

∇xf (x, y) and

and y

∈ Y

∇

k∇xf (x1, y)
k∇xf (x, y1)

∇
and y, y1, y2 ∈ Y
x2k
x1 −
,
.
y2k
y1 −
Assumption 3 For functions f (x, y; ξ) and g(x, y; ζ) for all x
we assume the following conditions hold:
continuous,
uous,
∇
y, y1, y2 ∈ Y

∇yg(x, y; ζ) is Lg-Lipschitz continuous,
, we have

∇xf (x, y; ξ) and
∇

− ∇xf (x2, y)
− ∇xf (x, y2)

Lf k
Lf k

∈ X

k ≤

k ≤

2

, y

, ξ and ζ,
∈ Y
∇yf (x, y; ξ) are Lf -Lipschitz
2
xyg(x, y; ζ) is Lgxy-Lipschitz contin-
and

yyg(x, y; ζ) is Lgyy-Lipschitz continuous. For example, for all x, x1, x2 ∈ X

k∇xf (x1, y; ξ)
k∇xf (x, y1; ξ)

− ∇xf (x2, y; ξ)
− ∇xf (x, y2; ξ)

k ≤

k ≤
∇yf (x, y) and

∇

,
x2k
.
y2k

x1 −
y1 −

Lf k
Lf k
2
xyg(x, y) are bounded, i.e.,

Assumption 4 The partial derivatives
C 2

C 2

f y and

gxy.

2
2
xyg(x, y)
k

k∇

≤

2
k∇yf (x, y)
k

≤

Assumption 5 Each component functions f (x, y; ξ) and g(x, y; ζ) have unbiased stochastic
partial derivatives with bounded variance σ2, i.e.,

2
yyg(x, y; ζ)] =

E[
∇xf (x, y; ξ)] =
E[
2
xyg(x, y; ζ)] =
∇
E
k∇xf (x, y; ξ)
k∇yg(x, y; ζ)
2
yyg(x, y; ζ)
k∇
Assumptions 1-5 are commonly used in stochastic bilevel optimization problems (Ghadimi and Wang,

∇xf (x, y), E[
∇yf (x, y; ξ)] =
∇xyg(x, y), E[
∇
σ2, E
2
− ∇xf (x, y)
k
≤
σ2, E
2
− ∇yg(x, y)
k
≤
σ2.
2
2
yyg(x, y)
k
− ∇

∇yf (x, y), E[
2
yyg(x, y),
2
− ∇yf (x, y)
≤
k
2
2
xyg(x, y)
− ∇
k

∇
k∇yf (x, y; ξ)
2
xyg(x, y; ζ)
k∇

∇yg(x, y; ζ)] =

∇yg(x, y),

σ2,
σ2,

≤

≤

E

E

2018; Hong et al., 2020; Ji et al., 2020; Chen et al., 2021; Khanduri et al., 2021; Guo et al.,
2021; Guo and Yang, 2021). Here Assumption 3 is clearly stricter than Assumption 2. At
the same time, based on Assumptions 4 and 5, we also have
k∇yf (x, y; ξ)
−
2σ2 + 2C 2
2
f y and
k∇yf (x, y; ξ)
2
∇yf (x, y)
− ∇yf (x, y)
k
2
2
gxy. Thus we argue that under Assumption 5, the bounded
xyg(x, y; ζ)
≤
k∇
k
2
2
xyg(x, y; ζ)
xyg(x, y) are not milder than the bounded
∇yf (x, y) and
for all ξ and ζ.

2 =
k∇yf (x, y; ξ)
k
2
k∇yf (x, y)
k

2 + 2
− ∇yf (x, y)
k

∇yf (x, y; ξ) and

≤
2σ2 + 2C 2

∇

∇

≤

3.3 Review Basic Bilevel Optimization Method

In the subsection, we review the basic ﬁrst-order method for solving the problem (1). Nat-
urally, we give the following iteration to update the variables x, y: at the t-th step

yt+1 =

yt −

λ

∇yg(xt, yt)
(cid:1)

,

PY

(cid:0)

xt+1 =

γ

xt −

∇xf (xt, y∗(xt))
(cid:1)

,

PX

(cid:0)

(7)

6

BiAdam: Fast Adaptive Bilevel Optimization Methods

where λ > 0 and γ > 0 denote the step sizes. Clearly, if there does not exist a closed form
= y∗(xt), we can not easily obtain
solution of the inner problem in the problem (1), i.e., yt+1 6
f (xt, y∗(xt)). Thus, one of key points in solving the problem (1)
the gradient
∇
∇
is to estimate the gradient

F (xt) =

F (xt).

∇

Lemma 1 (Lemma 2.1 in (Ghadimi and Wang, 2018)) Under the above Assumption 2, we
have, for any x

∈ X

∇

F (x) =

=

∇xf (x, y∗(x)) +
∇xf (x, y∗(x))

− ∇

∇

y∗(x)T
2
xyg(x, y∗(x))[

∇yf (x, y∗(x))
∇

2
yyg(x, y∗(x))]−

1

∇yf (x, y∗(x)).

(8)

From the above Lemma 1, it is natural to use the following form to estimate
as, for all x

, y

∈ X

∈ Y

F (x), deﬁned

∇

¯
∇

f (x, y) =

∇xf (x, y)

− ∇

2
xyg(x, y)
(cid:0)

2
yyg(x, y)
(cid:1)

∇

1

−

∇yf (x, y).

(9)

Note that although the inner problem of the problem (1) is a constrained optimization, we
∇yg(x, y∗(x)) = 0 and
assume that the optimality condition of the inner problem still is
y∗(x)

.

∈ Y

Lemma 2 (Lemma 2.2 in (Ghadimi and Wang, 2018)) Under the above Assumptions (1,
2, 4), for all x, x1, x2 ∈ X
¯
∇
k

y∗(x1)
k

x1 −
κ
k

, we have

,
x2k

y∗(x2)

and y

− ∇

k ≤

k ≤

,
k

−

f (x, y)
F (x1)

F (x)
F (x2)

k∇

− ∇

k ≤

y

∈ Y
Lyk
y∗(x)
x1 −
L
k
Lgxy/µ + LgyyCgxy/µ2
(cid:0)

−
,
x2k

.

, κ = Cgxy/µ, and L = Lf + (Lf +
(cid:1)

where Ly = Lf + Lf Cgxy/µ + Cf y
Ly)Cgxy/µ + Cf y

Lgxy/µ + LgyyCgxy/µ2
(cid:0)

(cid:1)

4. Fast Adaptive Bilevel Optimization Methods

In the section, we propose a class of fast single-loop adaptive bilevel optimization methods
to solve the problem (1). Speciﬁcally, our methods adopt the universal adaptive learning
rates as in (Huang et al., 2021). Moreover, our methods can be ﬂexibly incorporate the
momentum and variance reduced techniques. In the following, we propose a class of fast
adaptive bilevel optimization methods to solve the problem (1) based on diﬀerent estimator
of

F (x).

∇

4.1 BiAdam Algorithm

In this subsection, we propose a fast single-loop adaptive bilevel optimization method (Bi-
Adam) based on the basic momentum technique. Algorithm 1 shows the algorithmic frame-
work of our BiAdam algorithm.

At the step 4 of Algorithm 1, we generate the adaptive matrices At and Bt for updating
ρId
variables x and y, respectively. Speciﬁcally, we use the general adaptive matrix At (cid:23)
for variable x, and the global adaptive matrix Bt = btIp (bt > 0). For example, we can

7

Feihu and Heng

Algorithm 1 BiAdam Algorithm

and y1 ∈ Y

;

and then compute
∇yg(x1, y1; ζ1), G1 =
1h1;

G1(H1)−

(cid:2)

(cid:2)

(cid:2)

5:

∈X

Sµ

, H1 =

Draw two independent

˜xt+1 = arg minx
˜yt+1 = arg miny

and initial input x1 ∈ X

1: Input: T , parameters
2: initialize:
u1 =
ˆΠCgxy

2γ (x
6:
2λ (y
7: Draw two independent samples ξt+1 and ζt+1, and then compute:
8:

γ, λ, ηt, αt, ˆαt, ˜αt, βt, ˆβt}
{
samples
ξ1 and ζ1,
∇xf (x1, y1; ξ1), h1 = ΠCf y
∇yf (x1, y1; ξ1)
, v1 =
(cid:3)
∇yyg(x1, y1; ζ1)
and w1 = u1 −
∇xyg(x1, y1; ζ1)
3: for t = 1, 2, . . . , T do
(cid:3)
(cid:3)
Rd
Rp
d and Bt ∈
p;
4: Generate the adaptive matrices At ∈
×
xt)T At(x
xt)
, and xt+1 = xt + ηt(˜xt+1 −
wt, x
−
−
h
yt)T Bt(y
(cid:9)
(cid:8)
vt, y
, and yt+1 = yt + ηt(˜yt+1 −
yt)
h
−
−
(cid:8)
(cid:9)
ut+1 = βt+1∇xf (xt+1, yt+1; ξt+1) + (1
ˆβt+1∇yf (xt+1, yt+1; ξt+1) + (1
ht+1 = ΠCf y
10:
vt+1 = αt+1∇yg(xt+1, yt+1; ζt+1) + (1
(cid:2)
11: Gt+1 = ˆΠCgxy
2
xyg(xt+1, yt+1; ζt+1) + (1
ˆαt+1∇
12: Ht+1 =
13:
Gt+1(Ht+1)−
14: end for
15: Output: Chosen uniformly random from

2
yyg(xt+1, yt+1; ζt+1) + (1
1ht+1;

˜αt+1∇
(cid:2)
wt+1 = ut+1 −

+ 1
i
+ 1

−
˜αt+1)Ht

−
αt+1)vt;

ˆβt+1)ht

βt+1)ut;

ˆαt+1)Gt

Sµ

−

−

−

∈Y

9:

×

(cid:3)

(cid:3)

(cid:3)

(cid:2)

i

;

;

;

xt);

yt);

xt, yt}
{

T
t=1.

generate the matrix At as in the Adam (Kingma and Ba, 2014), and generate the matrix
Bt as in the AdaGrad-Norm (Ward et al., 2019), deﬁned as

˜wt = α ˜wt
t = b2
b2
t
−

−
1 +

∇xf (xt, yt; ξt)2, ˜w0 = 0, At = diag(
1 + (1
α)
−
2, b0 > 0, Bt = btIp, t
k∇yg(xt, yt; ζt)
k

≥

1,

p

˜wt + ρ), t

1

≥

(10)

(11)

where α
(0, 1) and ρ > 0. At the steps 5 and 6 of Algorithm 1, we use the generalized pro-
jection gradient iteration with Bregman distance (Censor and Zenios, 1992; Huang et al.,
2021) to update the variables x and y, respectively.

∈

In Algorithm 1, based on the above Lemma 1, we design an estimator for each part of
f (x, y) in (9). Speciﬁcally, we use the estimators ut, ht, Ht and Gt to estimate
2
yyg(x, y), respectively. From

gradient ¯
∇
the partial derivatives
Lemma 1, we naturally use wt = ut −

∇xf (x, y),

∇yf (x, y),

∇
Gt(Ht)−

2
xyg(x, y) and
∇
1ht to estimate

F (x).

∇

At the steps 8-12 of Algorithm 1, we use the basic momentum technique used in the
Adam (Kingma and Ba, 2014) to estimate the stochastic partial derivatives ut, ht, vt, Ht
and Gt. For example, we use ht+1 = ΠCf y
to
] as
estimate the partial derivative
(cid:3)
·
in (Guo and Yang, 2021), which ensures the bounded estimator ht, i.e.,
Cf y. At
(cid:2)
the same time, we use the projection operator ˆΠCgxy
] to ensure the bounded estimator
·
Cgxy. We use the projection operator
Gt, i.e.,
Gtk ≤
] to ensure the estimator Ht is
(cid:2)
Sµ
k
·
0.
positive deﬁnite, i.e., Ht (cid:23)

ˆβt+1)ht
∇yf (xt, yt). Here we use the projection operator ΠCf y

ˆβt+1∇yf (xt+1, yt+1; ξt+1) + (1

−
htk ≤
k

µIp ≻

(cid:2)

(cid:2)

8

BiAdam: Fast Adaptive Bilevel Optimization Methods

Algorithm 2 VR-BiAdam Algorithm

and y1 ∈ Y
;
∇xf (x1, y1; ξ1),
, H1 =
∇xyg(x1, y1; ζ1)
(cid:3)

γ, λ, ηt, αt, ˆαt, ˜αt, βt, ˆβt}
1: Input: T , parameters
{
2: initialize: Draw two samples ξ1 and ζ1, and then compute u1 =

and initial input x1 ∈ X

(cid:2)

(cid:2)

×

5:

∈Y

∈X

p;

+ 1
i
+ 1

h1 = ΠCf y
Sµ

˜xt+1 = arg minx
˜yt+1 = arg miny

∇yg(x1, y1; ζ1), G1 = ˆΠCgxy
1h1;
G1(H1)−

∇yf (x1, y1; ξ1)
, v1 =
and w1 = u1 −
∇yyg(x1, y1; ζ1)
(cid:3)
(cid:2)
3: for t = 1, 2, . . . , T do
(cid:3)
Rd
4: Generate the adaptive matrices At ∈
×
xt)T At(x
−
yt)T Bt(y
−

wt, x
h
(cid:8)
vt, y
h
(cid:8)
∇xf (xt+1, yt+1; ξt+1) + (1

Rp
d and Bt ∈
2γ (x
xt)
, and xt+1 = xt + ηt(˜xt+1 −
−
(cid:9)
6:
2λ (y
, and yt+1 = yt + ηt(˜yt+1 −
yt)
−
7: Draw two independent samples ξt+1 and ζt+1, and then compute:
(cid:9)
8:
ut+1 =
;
ut − ∇xf (xt, yt; ξt+1)
βt+1)
ˆβt+1)
(cid:3)
(cid:2)
ht+1 = ΠCf y
∇yf (xt+1, yt+1; ξt+1) + (1
−
10:
;
vt − ∇yg(xt, yt; ζt+1)
αt+1)
vt+1 =
(cid:0)
(cid:2)
11: Gt+1 = ˆΠCgxy
2
(cid:3)
(cid:2)
ˆαt+1)
xyg(xt+1, yt+1; ζt+1) + (1
Gt − ∇
−
2
12: Ht+1 =
Ht − ∇
˜αt+1)
yyg(xt, yt; ζt+1)
(cid:0)
13:
(cid:0)
14: end for
15: Output: Chosen uniformly random from

∇
2
yyg(xt+1, yt+1; ζt+1) + (1
(cid:2)
∇
wt+1 = ut+1 −

∇yg(xt+1, yt+1; ζt+1) + (1

2
xyg(xt, yt; ζt+1)
;

ht − ∇yf (xt, yt; ξt+1)

Gt+1(Ht+1)−

1ht+1;

;
(cid:1)(cid:3)

Sµ

(cid:1)(cid:3)

(cid:1)(cid:3)

−

−

−

9:

(cid:2)

i

;

xt);

yt);

xt, yt}
{

T
t=1.

4.2 VR-BiAdam Algorithm

In the subsection, we propose an accelerated version of BiAdam method (VR-BiAdam)
by using the momentum-based variance reduced technique. Algorithm 2 demonstrates the
algorithmic framework of our VR-BiAdam algorithm.

At the steps 8-12 of Algorithm 2, we use the momentum-based variance reduced tech-
nique of STORM to estimate the stochastic partial derivatives ut, ht, vt, Ht and Gt. For
example, the estimator of partial derivative

∇yf (xt+1, yt+1) is deﬁned as

ht+1 =ΠCf y
=ΠCf y

(cid:2)

∇yf (xt+1, yt+1; ξt+1) + (1
−
ˆβt+1∇yf (xt+1, yt+1; ξt+1) + (1
(cid:2)
∇yf (xt+1, yt+1; ξt+1)
+

ht
−
− ∇yf (xt, yt; ξt+1)

ˆβt+1)
ht − ∇yf (xt, yt; ξt+1)
(cid:0)
ˆβt+1)
(cid:0)

(cid:1)(cid:3)

.
(cid:1)(cid:3)

(12)

Compared with the estimator ht+1 in Algorithm 1, ht+1 in Algorithm 2 adds the term
∇yf (xt+1, yt+1; ξt+1)

− ∇yf (xt, yt; ξt+1), which controls the variances of estimator.

4.3 saBiAdam Algorithm

From the above Algorithms 1 and 2, they need to compute the approximated Hessian inverse,
1. When the dimension p of variable y is large, the cost p3 of approximated
i.e., (Ht+1)−
Hessian inverse is very expensive. At the same time, there exist ﬁve tuning parameters
αt, ˆαt, ˜αt, βt, ˆβt}
f (x, y) in (9). To deal with these drawbacks,
{
we adopt a new estimator to directly estimate ¯
f (x, y) as in (Khanduri et al., 2021).
∇
Thus we propose a novel stochastic approximated bilevel Adam method (saBiAdam)
to solve the problem (1) based on a new gradient estimator. Speciﬁcally, given K + 2

for each part of gradient ¯
∇

9

Feihu and Heng

Algorithm 3 saBiAdam Algorithm

5:

compute v1 =

γ, λ, ηt, αt, βt}
{

N, parameters
1: Input: T, K
2: initialize: Draw K + 3 independent samples ¯ξ1 =

∈
∇yg(x1, y1; ζ1), and w1 = ¯
∇
3: for t = 1, 2, . . . , T do
Rd
4: Generate the adaptive matrices At ∈
×
xt)T At(x
wt, x
−
h
yt)T Bt(y
(cid:8)
6:
vt, y
h
−
∈Y
7: Draw K + 3 independent samples ¯ξt+1 =
(cid:8)
8:
vt+1 = αt+1∇yg(xt+1, yt+1; ζt+1) + (1
f (xt+1, yt+1; ¯ξt+1) + (1
wt+1 = βt+1 ¯
∇

9:
10: end for
11: Output: Chosen uniformly random from

˜xt+1 = arg minx
˜yt+1 = arg miny

αt+1)vt;
βt+1)wt;

2γ (x
2λ (y

+ 1
i
+ 1

−
−

∈X

i

xt, yt}
{

T
t=1.

and initial input x1 ∈ X
, ζ K
1 , ζ 1
1 ,
1 }
f (x1, y1; ¯ξ1) generated from (13);

ξ1, ζ 0
{

· · ·

and y1 ∈ Y
and ζ1, and then

;

×

p;

Rp
d and Bt ∈
xt)
, and xt+1 = xt + ηt(˜xt+1 −
−
(cid:9)
, and yt+1 = yt + ηt(˜yt+1 −
yt)
−
ξt+1, ζ 0
t+1,
(cid:9)
· · ·
{

and ζt+1:

t+1}

, ζ K

xt);

yt);

independent samples ¯ξ =

ξ, ζ 0, ζ 1,
{

· · ·

, ζ K

, we deﬁnite a stochastic gradient estimator:
}

f (x, y, ¯ξ) =

¯
∇

∇xf (x, y; ξ)

− ∇

xyg(x, y; ζ 0)
2
(cid:20)

K
Lg

k

Yi=1 (cid:0)

Ip −

1
Lg ∇

yyg(x, y; ζ i)
2
(cid:1)

(cid:21)∇yf (x, y; ξ),
(13)

k
i=1

where K

N+. Here we use the term K
Lg

to approximate the
∈
Q
f (x, y, ¯ξ) is a biased estimator
2
Hessian inverse, i.e.,
yyg(x, y; ζ)
−
∇
f (x, y; ¯ξ)
¯
in estimating ¯
f (x, y), i.e. E ¯ξ
In the following, we ﬁrst use
(cid:1)
∇
∇
Assumption 6 instead of Assumption 1, then give Lemma 3, which shows that the bias
(cid:3)
(cid:2)
f (x, y; ¯ξ)
R(x, y) = ¯
in the gradient estimator (13) decays exponentially
−
∇
fast with number K.
(cid:3)

(cid:0)
1. Clearly, the above ¯
∇
= ¯
∇

yyg(x, y; ζ i)
2
(cid:1)

E ¯ξ
(cid:2)

1
Lg ∇

Ip −

f (x, y).

f (x, y)

¯
∇

(cid:0)

2
yyg(x, y)

Assumption 6 For any x, ζ, g(x, y; ζ) is Lg-smooth and µ-strongly convex function, i.e.,
LgIp (cid:23) ∇
Lemma 3 (Lemma 2.1. in (Khanduri et al., 2021)) Under the about Assumptions (6, 4),
for any K

1, the gradient estimator in (13) satisﬁes

µIp.

(cid:23)

≥

R(x, y)
k

k ≤

CgxyCf y
µ

1

(cid:0)

−

K ,

µ
Lg (cid:1)

(14)

.

−

f (x, y)

f (x, y; ¯ξ)
(cid:3)

¯
where R(x, y) = ¯
E ¯ξ
∇
∇
(cid:2)
From Lemma 3, choose K = Lg
µ log(CgxyCf yT /µ) in Algorithm 3, we have
all t
mini-batch samples.

≥
In Algorithm 3, we use the estimator wt+1 to directly estimate gradient ¯
∇

T for
1. Thus, this result guarantees convergence of our algorithms only requiring a small

f (x, y) instead
1ht+1 in Algorithms 1 and 2. Thus, Algorithm 3 only uses
of wt+1 = ut+1 −
two tuning parameters αt, βt instead of ﬁve parameters αt, ˆαt, ˜αt, βt, ˆβt in Algorithms 1 and
2. Moreover, Algorithm 3 does not require to compute approximated Hessian inverse.

Gt+1(Ht+1)−

Rtk
k

= 1

10

6
BiAdam: Fast Adaptive Bilevel Optimization Methods

Algorithm 4 VR-saBiAdam Algorithm

and y1 ∈ Y
and ζ1, and then

;

5:

· · ·

ξ1, ζ 0
{

compute v1 =

γ, λ, ηt, αt, βt}
{

N, parameters
1: Input: T, K
2: initialize: Draw K + 3 independent samples ¯ξ1 =

and initial input x1 ∈ X
, ζ K
1 , ζ 1
1 ,
1 }
f (x1, y1; ¯ξ1) generated from (13);

∈
∇yg(x1, y1; ζ1), and w1 = ¯
∇
3: for t = 1, 2, . . . , T do
Rd
4: Generate the adaptive matrices At ∈
×
xt)T At(x
−
yt)T Bt(y
−

Rp
d and Bt ∈
xt)
, and xt+1 = xt + ηt(˜xt+1 −
wt, x
−
h
(cid:9)
(cid:8)
6:
vt, y
, and yt+1 = yt + ηt(˜yt+1 −
yt)
h
−
∈Y
7: Draw K + 3 independent samples ¯ξt+1 =
ξt+1, ζ 0
and ζt+1:
t+1,
(cid:8)
(cid:9)
t+1}
· · ·
{
8:
vt+1 =
αt+1)
∇yg(xt+1, yt+1; ζt+1) + (1
;
vt − ∇yg(xt, yt; ζt+1)
f (xt, yt; ¯ξt+1)
f (xt+1, yt+1; ¯ξt+1) + (1
wt+1 = ¯
¯
wt −
;
βt+1)
(cid:3)
(cid:2)
∇
∇
(cid:3)
(cid:2)
xt, yt}
{

9:
10: end for
11: Output: Chosen uniformly random from

˜xt+1 = arg minx
˜yt+1 = arg miny

2γ (x
2λ (y

+ 1
i
+ 1

T
t=1.

−
−

, ζ K

p;

∈X

×

i

xt);

yt);

4.4 VR-saBiAdam Algorithm

In the subsection, we present an accelerated version of saBiAdam method (VR-saBiAdam)
by using the momentum-based variance reduced technique. Algorithm 4 shows the algorith-
mic framework of our VR-saBiAdam algorithm. The following Lemma shows the Lipschitz
f (x1, y; ¯ξ), which is a basic point for applying the variance reduced
continuous of estimator ¯
∇
technique to our VR-saBiAdam algorithm.

Lemma 4 (Lemma B.2 in (Khanduri et al., 2021)) Under the above Assumptions (6, 3, 4),
f (x, y; ¯ξ) is Lipschitz continuous, such that for x, x1, x2 ∈ X
stochastic gradient estimate ¯
∇
and y, y1, y2 ∈ Y

,

¯
E ¯ξk
∇
¯
E ¯ξk
∇
f + 6C 2
gxyL2
f

f (x1, y; ¯ξ)
f (x, y1; ¯ξ)

−

¯
∇
¯
−
∇
µ2 + 6C 2

f (x2, y; ¯ξ)
2
k
f (x, y2; ¯ξ)
2
k
K
2µLg

f yL2

gxy

≤

L2
L2

2,
x2k
x1 −
K k
2,
y2k
y1 −
Kk
≤
µ2 + 6C 2
gxyL2
f

(Lg

K
2µLg

−

−

K 3L2
gyy
µ)2(2µLg

µ2) .

−

−

where L2

K = 2L2

At the steps 8-9 of Algorithm 4, we use the momentum-based variance reduced technique
to estimate the stochastic partial derivatives vt and wt. For example, the estimator of partial
derivative ¯
∇

f (xt+1, yt+1) is deﬁned as

wt+1 = ¯
∇
=βt+1 ¯
∇
+ ¯
∇

f (xt+1, yt+1; ¯ξt+1) + (1

−

f (xt+1, yt+1; ¯ξt+1) + (1
f (xt+1, yt+1; ¯ξt+1)
¯
∇

−

¯
wt −
βt+1)
∇
(cid:2)
wt
βt+1)
−
f (xt, yt; ¯ξt+1)
(cid:2)
(cid:3)

f (xt, yt; ¯ξt+1)
(cid:3)

.

(15)

Compared with the estimator wt+1 in Algorithm 3, wt+1 in Algorithm 4 adds the term
¯
∇

f (xt, yt; ¯ξt+1) to control the variances of estimator.

f (xt+1, yt+1; ¯ξt+1)

¯
∇

−

5. Theoretical Analysis

In this section, we study the convergence properties of our algorithms (BiAdam, VR-
BiAdam, saBiAdam and VR-saBiAdam) under some mild conditions. The detailed proofs
are provided in the following Appendix .

11

Feihu and Heng

Table 2: Some Notations

Ly = Lf + Lf Cgxy

µ + Cf y

Lgxy

µ + LgyyCgxy

L2

L = Lf + (Lf + Ly) Cgxy
(cid:0)
µ + Cf y
gxyC2
gxyC2
gyyC2
L2
(cid:0)
L2
L2
f y
f y
f +
µ2 +
µ4
L2
L2

0 = 8
(cid:0)

, κ = Lg/µ
µ2
Lgxy
µ + LgyyCgxy
(cid:1)
µ2
f C2
L2
gxy
µ2

+

(cid:1)

(cid:1)

L2

5 =

gxy

8C2

1 = 8
8C2
f
2 =
µ2
f C2
L2
3 =
µ4
8C2
L2
gxy
4 =
µ2
8L2
8L2
12µ2L2
8L2
g
gxy
f
f
+
+
+
125L2
5L2
5L2
5L2
0
2
1
4
gxy + L2
g + L2
f + L2
6 = 2L2
L2
g µ2
12L2
+ 2L2
L2
7 =
125L2
0
f yL2
µ2 + 6C 2
8 = L2
L2

K
gxy
2µLg
g + L2
K

−

−

3

0

+

8L2
gyy
5L2
3

gyy

K = 2L2
L2

f + 6C 2

gxyL2
f

K
2µLg

µ2 + 6C 2

gxyL2
f

K 3L2
gyy
µ)2(2µLg

µ2)

−

(Lg

−

5.1 Additional Mild Assumptions
Assumption 7 The estimated stochastic partial derivative ¯
∇

f (x, y; ¯ξ) satisﬁes

The stochastic partial derivative

f (x, y) + R(x, y)

¯
E ¯ξ
∇
(cid:2)
¯
E ¯ξk
∇

f (x, y; ¯ξ)
(cid:3)
f (x, y; ¯ξ)
−

= ¯
∇
¯
f (x, y)
∇
−
∇yg(x, y; ζ) satisﬁes

2
R(x, y)
k

σ2.

≤

E[

∇yg(x, y; ζ)] =

2
− ∇yg(x, y)
k
Assumption 8 In our algorithms, the adaptive matrices At for all t
variables x satisfy AT

∇yg(x, y), E

k∇yg(x, y; ζ)

t = At and λmin(At)

ρ > 0.

≥

≥

σ2.

≤

1 for updating the

Assumption 9 In our algorithms, the adaptive matrices Bt = btIp for all t
the variables y satisfy ˆb

b > 0.

bt ≥

≥

1 for updating

≥

Assumption 7 is commonly used in the stochastic bilevel optimization methods (Ji et al.,
2020; Khanduri et al., 2021). In the paper, we consider the general adaptive learning rates
(including the coordinate-wise and global learning rates) for variable x and the global learn-
ing rate for variable y. Assumption 8 ensures that the adaptive matrices At for all t
1
are positive deﬁnite as in (Huang et al., 2021). Assumption 9 guarantees the global adap-
In fact, Assumption 9 is mild. For example, in the
tive matrix Bt = btIp is bounded.
Rp E[f (x; ξ)], Ward et al. (2019) apply a global adaptive learning rate to the
problem minx
∈
2, b0 > 0, η > 0 for
1)
1 +
update form xt = xt
k
1) with Bt = btIp
1; ξt
all t
1 −
≥
b0 > 0. Li and Orabona (2019); Cutkosky and Orabona (2019) use a global
and bt ≥ · · · ≥
adaptive learning rate to the update form xt+1 = xt −
, where gt is stochastic gradient

t = b2
η ∇
t
−
1, which is equivalent to the form xt = xt

f (xt
−
1
t ∇

f (xt−1;ξt−1)
bt

1; ξt
−
f (xt

η gt
bt

1 −

ηB−

, b2

k∇

≥

−

−

−

−

12

BiAdam: Fast Adaptive Bilevel Optimization Methods

2
f (xi; ξi)
k

α/k, k > 0, ω > 0 and α

(0, 1), which is equivalent
∈
b0 = ωα
k > 0. At the same time,
f (x) = 0
f (x; ξ) = 0 for all ξ. Thus, these global adaptive learning rates are generally

t
i=1 k∇
1
t gt with Bt = btIp and bt ≥ · · · ≥
Rp f (x) = E[f (x; ξ)] approaches the stationary points, i.e.,
∈

∇

(cid:1)

(cid:0)

and bt =
ω +
ηB−
to xt+1 = xt −
P
the problem minx
or even
bounded, i.e., ˆb

∇

bt ≥

b > 0 for all t

1.

≥

≥

5.2 Reasonable Convergence Metrics

In the subsection, we provide two reasonable convergence metrics for our algorithms for
both unconstrained and constrained bilevel optimizations. We ﬁrst give a useful lemma as
follows:

Lemma 5 When the gradient estimator wt is generated from Algorithms 1 or 2, for all
t

1, we have

≥

wt − ∇
k

2
F (xt)
k

≤

y∗(xt)

L2
0k
+ L2
3k
gxyC2
f y
µ2 +

−
Ht − ∇
L2
gyyC2
gxyC2
f y
µ4

2 + L2
2 + L2
Gt − ∇
ut − ∇xf (xt, yt)
ytk
1k
2k
k
2,
2 + L2
2
ht − ∇yf (xt, yt)
yyg(xt, yt)
4k
k
k
8C2
f C2
L2
µ2 , L2
f
µ2

1 = 8, L2

, L2

2 =

+

gxy

L2

2
2
xyg(xt, yt)
k
(16)

3 =

8C2

f C2
µ4

gxy

and

(cid:1)
. When the gradient estimator wt is generated from Algorithms 3 or 4, for all

L2

f +

0 = 8
(cid:0)

where L2
8C2
gxy
µ2
1, we have

L2
t

4 =

≥

wt − ∇
k

2
F (xt)
k

≤

L2
0k

y∗(xt)

ytk

2 + 2
wt −
k

¯
∇

2.
f (xt, yt)
k

−

(17)

E[

For our Algorithms 1 and 2, based on Lemma 5, we provide a convergence metric
Mt], deﬁned as
ρ
Mt =
γ k
+ L3k

xtk
Ht − ∇
where the ﬁrst ﬁve terms of
and the last term measures the convergence of the iteration solutions
consider the existing two cases:

+ L2k
,
ytk
ht − ∇yf (xt, yt)
k
Mt measure the convergence of the iteration solutions

ut − ∇xf (xt, yt)
k
+ L4k

+ L1k
2
yyg(xt, yt)
k

T
t=1,
xt}
{
T
t=1. Next, we

2
xyg(xt, yt)
k

Gt − ∇

˜xt+1 −

+ L0k

y∗(xt)

(18)

−

yt}
{
γwt. Then we have

1) When

X
Mt =

= Rd and At = Id, we have ρ = 1 and ˜xt+1 = xt −
2
Gt − ∇
+ L2k
+ L1k
xyg(xt, yt)
wtk
k
k
2 + L0k
ht − ∇yf (xt, yt)
+ L3k
Ht − ∇
k
wt − ∇
+
wtk
k

ut − ∇xf (xt, yt)
k
2
yyg(xt, yt)
+ L4k
k
F (xt)

,
F (xt)
k

k ≥ k∇

≥ k

y∗(xt)

ytk

−

(19)

where the second last inequality holds by Lemma 5 and the inequality
for all ai ≥
obtain E
k∇
E
F (xt)
k∇
k
2020).

5
i=1 √ai
P
ǫ, we can
0. Thus if
Mt →
ǫ, i.e., we ﬁnd an ǫ-stationary point of the problem (1). The metric
F (xt)
is a common convergence metric used in (Ghadimi and Wang, 2018; Hong et al.,

qP
0. Speciﬁcally, if E[

i=1 ai ≤
Mt]
≤

0, we have

F (xt)

k →

k ≤

k∇

5

13

Feihu and Heng

2) Otherwise, i.e.,

2 xT Atx, we deﬁne
a prox-function (i.e., Bregman distance) (Censor and Lent, 1981; Censor and Zenios, 1992;
Ghadimi et al., 2016) associated with φt(x), deﬁned as

= Rd. Let φt(x) = 1

= Rd and At 6

= Id or

X 6

X

Vt(x, xt) = φt(x)

−

φt(xt) +

φt(xt), x

h∇

−

(cid:2)

xti
(cid:3)

=

1
2

(x

−

xt)T At(x

xt).

−

(20)

The step 5 of Algorithm 1 or 2 is equivalent to the following generalized projection problem,

˜xt+1 = min

x

∈X (cid:8)

wt, x
h

i

+

1
γ

Vt(x, xt)

.

(cid:9)

(21)

˜xt+1). According to the above Lemma 5, we have

As in Lemma 1 of (Ghadimi et al., 2016), we deﬁne a generalized projected gradient
1
γ (xt −
0, we have wt → ∇
k →
0, where xt is a stationary point or local minimum of the problem (1) (Ghadimi et al., 2016).
For our Algorithms 3 and 4, based on Lemma 5, we provide a convergence metric
Gt], deﬁned as

wt −∇
Mt ≥ k
0. Thus we can obtain

GX
. When
F (xt)
k
(xt,

F (xt) and ρ

(xt, wt, γ)

(xt, wt, γ) =
Mt →
F (xt), γ)

k →

kGX

kGX

E[

∇

Gt =

ρ
γ k

˜xt+1 −

xtk

+ √2

wt −
k

¯
∇

f (xt, yt)
k

+ L0k

y∗(xt)

,
ytk

−

(22)

where the ﬁrst two terms of
and the last term measures the convergence of the iteration solutions
consider the existing two cases:

Gt measure the convergence of the iteration solutions

yt}
{

T
t=1,
xt}
{
T
t=1. Similarly, we

1) When

X

= Rd and At = Id, we have ρ = 1 and ˜xt+1 = xt −
¯
y∗(xt)
+ L0k
∇
,
F (xt)
F (xt)
k

+ √2
wt −
k
wt − ∇
+
k

Gt =
≥ k

f (xt, yt)
k

wtk
k
wtk

k ≥ k∇

γwt. Then we have

−

ytk

(23)

where the second last inequality holds by Lemma 5 and the inequality √a1 + a2 ≤
for a1, a2 ≥
≤
obtain E
k∇

Gt →
= Rd and At 6
3 or 4 is equivalent to the following generalized projection problem:

Gt]
ǫ, i.e., we ﬁnd an ǫ-stationary point of the problem (1).

k →
= Rd. Similarly, the step 5 of Algorithm

k ≤
2) Otherwise, i.e.,

0. Thus if
F (xt)

0. Speciﬁcally, if E[

√a1+√a2
ǫ, we can

0, we have

= Id or

F (xt)

k∇

X 6

X

˜xt+1 = min

x

∈X (cid:8)

wt, x
h

i

+

1
γ

Vt(x, xt)

.

(cid:9)

(24)

Then we deﬁne a generalized projected gradient
the above Lemma 5, we have
wt − ∇
(xt, wt, γ)
and ρ
stationary point or local minimum of the problem (1) (Ghadimi et al., 2016).

(xt, wt, γ) = 1
γ (xt −
GX
. When
0, we have wt → ∇
F (xt)
Gt →
k
F (xt), γ)
(xt,
0. Thus we can obtain
∇
kGX

˜xt+1). According to
F (xt)
0, where xt is a

Gt ≥ k

k →

k →

kGX

5.3 Convergence Analysis of BiAdam Algorithm

In this subsection, we study the convergence properties of our BiAdam algorithm. Speciﬁ-
cally, we provide convergence analysis of our BiAdam algorithm with non-adaptive matrix
Bt = Ip and global adaptive matrix Bt = btIp (ˆb
b > 0), respectively. Note that in
bt ≥
ρId (ρ > 0) for the
our convergence analysis, we consider the general adaptive matrix At ≻
variable x. The proofs are provided in the Appendix A.1.

≥

14

BiAdam: Fast Adaptive Bilevel Optimization Methods

Theorem 6 Under the above Assumptions (1, 2, 4, 5, 8), in the Algorithm 1, given Bt =
0, βt+1 = c1ηt, ˆβt+1 = c2ηt, αt+1 = c3ηt, ˆαt+1 = c4ηt,
Ip, ηt =
m1/2
k ,

k2, (c1k)2, (c2k)2, (c3k)2, (c4k)2, (c5k)2

(m+t)1/2 for all t

≥

k

˜αt+1 = c5ηt, m
5L2
4
4 ≤
min

c2 ≤
2√6L2

max
≥
k , 125L2
m1/2
(cid:0)
0
6µ2 ≤
0κ2 , m1/2ρ
4Lk

5µ2λ2+125L2

√6λµρ

(cid:0)

k , 5L2
m1/2

2

c3 ≤

and 0 < λ

4 ≤
min

≤

c4 ≤
15L2
0
4L2

(cid:0)

5µ , 1

6Lg

(cid:1)

1

, k > 0, 5L2
k , 5L2
m1/2
(cid:1)
c5 ≤
4 ≤
, we have

3

4 ≤
m1/2

c1 ≤
k , 0 < γ

≤

(cid:1)

T

Xt=1

1
T

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

2√6G
T 1/4

.

(25)

where G = ρ(F (x1)
−
kγ
8L2
8L2
12µ2L2
g
f
f
125L2
5L2
5L2
0
4
1

+

+

+

F ∗)

+ 5L2
8L2
gxy
5L2
2

kλµ + 5σ2
0∆0
8L2
gyy
+
5L2
3

.

k + 5mσ2

k

ln(m + T ) and ∆0 =

y1 −
k

2, L2
y∗(x1)
k

5 =

≤

bt ≤

b > 0) for all t

Theorem 7 Under the above Assumptions (1, 2, 4, 5, 8), in the Algorithm 1, given Bt =
0, βt+1 = c1ηt, ˆβt+1 = c2ηt,
btIp (ˆb
k
1, ηt =
k2, (c1k)2, (c2k)2, (c3k)2, (c4k)2, (c5k)2
max
,
αt+1 = c3ηt, ˆαt+1 = c4ηt, ˜αt+1 = c5ηt, m
≥
k > 0, 5L2
k , 5L2
k , 125L2
m1/2
m1/2
m1/2
(cid:1)
(cid:0)
k ,
c2 ≤
6µ2 ≤
5L2
, m1/2ρ
b
3
,
c5 ≤
6Lg
4Lk
4 ≤
5µ2λ2+125ˆb2L2
we have

c3 ≤
and 0 < λ

k , 5L2
m1/2
min

c1 ≤
k , 0 < γ

(m+t)1/2 for all t

c4 ≤
15bL2
0
5µ ,
4L2

4 ≤
min

4 ≤
m1/2

2q6L2

4 ≤

√6λµρ

0κ2

≥

≤

≥

≤

(cid:1)

(cid:0)

(cid:1)

(cid:0)

1

4

0

2

1
T

T

Xt=1

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

2√6G
T 1/4 .

(26)

F ∗)

where G = ρ(F (x1)
−
kγ
8L2
8L2
12µ2L2
f
g
f
125L2
5L2
5L2
0
4
1

+

+

+

+ 5b1L2
8L2
gxy
5L2
2

kλµ + 5σ2
0∆0
8L2
gyy
+
5L2
3

.

k + 5mσ2

k

ln(m + T ) and ∆0 =

y1 −
k

2, L2
y∗(x1)
k

5 =

T
t=1

Remark 8 Without loss of generality, let k = O(1) and m = O(1), we have G = O(ln(m +
T )) = ˜O(1). Thus our BiAdam algorithm has a convergence rate of ˜O( 1
Mζ] =
1
4). Since our BiAdam algorithm only
T
requires two samples to estimate stochastic partial derivatives in each iteration, and needs
T iterations. Thus our BiAdam algorithm requires sample complexity of 2T = ˜O(ǫ−
4) for
ﬁnding an ǫ-stationary point of the problem (1).

Mt] = ˜O( 1
E[

ǫ, we have T = ˜O(ǫ−

T 1/4 ). Let E[

T 1/4 )

P

≤

5.4 Convergence Analysis of VR-BiAdam Algorithm

In this subsection, we study the convergence properties of our VR-BiAdam algorithm.
Speciﬁcally, we provide convergence analysis of our VR-BiAdam algorithm with non-adaptive
matrix Bt = Ip and global adaptive matrix Bt = btIp (ˆb
b > 0), respectively. Note
bt ≥
ρId (ρ > 0)
that in our convergence analysis, we consider the general adaptive matrix At ≻
for the variable x. The proofs are provided in the Appendix A.2.

≥

15

Feihu and Heng

k

Theorem 9 Under the above Assumptions (1, 3, 4, 5, 8), in the Algorithm 2, given
0, βt+1 = c1η2
Bt = Ip, ηt =
t , ˆαt+1 =
c4η2
, k > 0, c1 ≥
3k3 + 5L2
3k3 + 5L2
(cid:1)
2
4 , 0 <
min
γ

≥
2, k3, (c1k)3, (c2k)3, (c3k)3, (c4k)3, (c5k)3
4 , c5 ≥
, c4 ≥
min
≤

(m+t)1/3 for all t
t , ˜αt+1 = c5η2
t , m
max
≥
3k3 + 5L2
(cid:0)
2
4 , c3 ≥
4 , c2 ≥
0κ2 , m1/3ρ
2√24L2

3k3 + 5L2
15L2
6µ , 1
0
16L2

3k3 + 125L2
6
and 0 < λ

t , ˆβt+1 = c2η2

√6λµρ
6λ2µ2+125L2

t , αt+1 = c3η2

, we have

6Lg

4Lk

≤

2

2

2

1

4

3

0

2

(cid:1)

(cid:0)

(cid:1)

(cid:0)

1
T

T

Xt=1

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3

,

F ∗)

where M = ρ(F (x1)
−
kγ
6 = 2L2
2 and L2
y∗(x1)
k

+ 5L2
f + L2

kλµ + 5m1/3σ2
0∆0
gxy + L2
g + L2

gyy.

k2 + 2k2 ˆCσ2 ln(m + T ), ˆC =

(27)

5
i=1 c2

i , ∆0 =

y1 −
k

P

k

≥

Theorem 10 Under the above Assumptions (1, 3, 4, 5, 8), in the Algorithm 2, given Bt =
btIp (ˆb
t , αt+1 = c3η2
b > 0), ηt =
bt ≥
t ,
t , ˜αt+1 = c5η2
ˆαt+1 = c4η2
, k > 0,
3k3 + 5L2
(cid:1)
2
2
4 ,
4 , c2 ≥
c1 ≥
0 < γ
2q24L2

2, k3, (c1k)3, (c2k)3, (c3k)3, (c4k)3, (c5k)3
3k3 + 125L2
4 , c5 ≥
6
and 0 < λ

(m+t)1/3 for all t
t , m
max
≥
3k3 + 5L2
(cid:0)
4
4 , c3 ≥
, m1/3ρ
4Lk
0κ2

3k3 + 5L2
15bL2
0
6µ ,
16L2

3k3 + 5L2
min

, c4 ≥
min
≤

t , ˆβt+1 = c2η2

0, βt+1 = c1η2

6λ2µ2+125ˆb2L2

√6λµρ

b
6Lg

≥

≤

2

2

2

3

1

2

0

, we have
(cid:1)

(cid:0)

(cid:1)

(cid:0)

1
T

T

Xt=1

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3 ,

(28)

F ∗)

+ 5L2

6 = 2L2

kλµ + 5m1/3σ2
0b1∆0
k2
f + L2
g + L2
gxy + L2

+ 2k2 ˆCσ2 ln(m + T ), ˆC =
gyy.

where M = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k
Remark 11 Without loss of generality, let k = O(1) and m = O(1), we have M =
O(ln(m + T )) = ˜O(1). Thus our VR-BiAdam algorithm has a convergence rate of ˜O( 1
T 1/3 ).
Let E[
3). Since our VR-BiAdam
algorithm only requires two samples to estimate stochastic partial derivatives in each iter-
ation, and needs T iterations. Thus our VR-BiAdam algorithm requires sample complexity
of 2T = ˜O(ǫ−

3) for ﬁnding an ǫ-stationary point of the problem (1).

Mt] = ˜O( 1
E[

ǫ, we have T = ˜O(ǫ−

Mζ] = 1

i , ∆0 =

5
i=1 c2

T 1/3 )

T
t=1

P

P

≤

T

5.5 Convergence Analysis of asBiAdam Algorithm

In this subsection, we study convergence properties of our asBiAdam algorithm. Speciﬁcally,
we provide convergence analysis of our asBiAdam algorithm with non-adaptive matrix Bt =
Ip and global adaptive matrix Bt = btIp (ˆb
b > 0), respectively. Note that in our
ρId (ρ > 0) for the
convergence analysis, we consider the general adaptive matrix At ≻
variable x. The proofs are provided in the Appendix A.3.

bt ≥

≥

Theorem 12 Under the above Assumptions (6, 2, 4, 7, 8), in the Algorithm 3, given
,
Bt = Ip, ηt =

0, αt+1 = c1ηt, βt+1 = c2ηt, m

k2, (c1k)2, (c2k)2

max

k

(m+t)1/2 for all t

≥

≥

(cid:0)

(cid:1)

16

BiAdam: Fast Adaptive Bilevel Optimization Methods

k > 0, 125L2
6µ2

0

0 < λ

min

≤

(cid:0)

c1 ≤
≤
15L2
7µ , 1
0
4L2

6Lg

(cid:1)

m1/2
k , 9
2 ≤
and K = Lg

m1/2

k , 0 < γ

c2 ≤
µ log(CgxyCf yT /µ), we have

min

≤

√6λµρ
7λ2µ2+125L2

0κ2 , m1/2ρ

4Lk

√6L2

,

(cid:1)

(cid:0)

1
T

T

Xt=1

E[

Gt]

≤

2√3G′m1/4
T 1/2

+

2√3G′
T 1/4

+

√2
T

.

+ 5L2

kλµ + 2σ2
0∆0

k + 2mσ2

k

ln(m+T )+ 4(m+T )

9kT 2 + 8k

T and ∆0 =

(29)

y1−
k

2,
y∗(x1)
k

F ∗)

where G′ = ρ(F (x1)
−
kγ
g µ2
12L2
+ 2L2
L2
3 .
125L2
0

7 =

0

Theorem 13 Under the above Assumptions (6, 2, 4, 7, 8), in the Algorithm 3, given
Bt = btIp (ˆb
0, αt+1 = c1ηt, βt+1 = c2ηt,
m1/2

b > 0), ηt =

for all t

≥

bt ≥
k2, (c1k)2, (c2k)2
max
15bL2
(cid:0)
b
0
7µ ,
4L2
6Lg

, 0 < γ

≤

k
(m+t)1/2
, k > 0, 125L2
(cid:1)
√6λµρ
min
7λ2µ2+125ˆb2L2

6µ2 ≤

q6L2

0

0κ2

(cid:1)

(cid:0)

m

min

≥

(cid:0)
we have

c1 ≤
, m1/2ρ
4Lk

c2 ≤
2 ≤
and K = Lg

k , 0 < λ
≤
µ log(CgxyCf yT /µ),

≥
m1/2
k , 9

(cid:1)

1
T

T

Xt=1

E[

Gt]

≤

2√3G′m1/4
T 1/2

+

2√3G′
T 1/4

+

√2
T

.

where G′ = ρ(F (x1)
−
kγ
12L2
g µ2
2, L2
y∗(x1)
125L2
k
0

7 =

F ∗)

+ 5b1L2
+ 2L2
3 .

0

kλµ + 2σ2
0∆0

k + 2mσ2

k

ln(m + T ) + 4(m+T )

9kT 2 + 8k

T and ∆0 =

(30)

y1 −
k

T
t=1

Gζ] = 1

Remark 14 Without loss of generality, let k = O(1) and m = O(1), we have G′ =
O(ln(m + T )) = ˜O(1). Thus our saBiAdam algorithm has a convergence rate of ˜O( 1
T 1/4 ).
Let E[
4). Since our saBiAdam algo-
rithm only requires K + 3 = Lg
µ log(CgxyCf yT /µ) + 3 = ˜O(1) samples to estimate stochastic
P
partial derivatives in each iteration, and needs T iterations. Thus our saBiAdam algorithm
requires sample complexity of (K + 3)T = ˜O(ǫ−
4) for ﬁnding an ǫ-stationary point of the
problem (1).

ǫ, we have T = ˜O(ǫ−

Gt] = ˜O( 1
E[

T 1/4 )

≤

T

5.6 Convergence Analysis of VR-asBiAdam Algorithm

In this subsection, we study convergence properties of our VR-asBiAdam algorithm. Specif-
ically, we provide convergence analysis of our VR-asBiAdam algorithm with non-adaptive
matrix Bt = Ip and global adaptive matrix Bt = btIp (ˆb
b > 0), respectively. Note
bt ≥
ρId (ρ > 0)
that in our convergence analysis, we consider the general adaptive matrix At ≻
for the variable x. The proofs are provided in the Appendix A.4.

≥

k

Theorem 15 Under the above Assumptions (6, 3, 4, 7, 8), in the Algorithm 4, given Bt =
,
Ip, ηt =
k > 0, c1 ≥

2, k3, (c1k)3, (c2k)3
0κ2 , m1/3ρ

(m+t)1/3 for all t
3k3 + 125L2

≥
(cid:0)
√6λµρ
8λ2µ2+125L2

≥
6µ2 , c2 ≥

t , βt+1 = c2η2

0, αt+1 = c1η2

2 , 0 < γ

3k3 + 9

2√24L2

t , m

max

(cid:1)
≤

min

4Lk

≤

2

2

0

, 0 < λ
(cid:1)

(cid:0)

17

15L2
0
16L2

8µ , 1

6Lg

min

(cid:0)

Feihu and Heng

and K = Lg

µ log(CgxyCf yT /µ), we have

(cid:1)

1
T

T

Xt=1

E[

Gt]

≤

2√3M ′m1/6
T 1/2

+

2√3M ′
T 1/3

+

√2
T

,

where M ′ = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k

F ∗)

+ 5L2

kλµ + 2m1/3σ2
0∆0
g + L2
K.

8 = L2

k2 + 2k2(c2

1 + c2

2)σ2 ln(m + T ) + 6k(m+T )1/3

T

(31)

, ∆0 =

Theorem 16 Under the above Assumptions (6, 3, 4, 7, 8), in the Algorithm 4, given
Bt = btIp (ˆb

b > 0), ηt =

k

bt ≥

≥
2, k3, (c1k)3, (c2k)3

max

(cid:0)
0 < γ

min

≤

(cid:0)

2q24L2

, k > 0, c1 ≥
(cid:1)
√6λµρ
8λ2µ2+125ˆb2L2

0κ2

(cid:1)

(m+t)1/3 for all t
3k3 + 125L2

≥
6µ2 , c2 ≥
and K = Lg

0, αt+1 = c1η2
3k3 + 9

t , βt+1 = c2η2
t , m
15bL2
b
0
2 , 0 < λ
8µ ,
16L2
6Lg
µ log(CgxyCf yT /µ), we have

, m1/3ρ
4Lk

min

≤

(cid:0)

2

2

0

≥
,

(cid:1)

1
T

T

Xt=1

E[

Gt]

≤

2√3M ′m1/6
T 1/2

+

2√3M ′
T 1/3

+

√2
T

,

where M ′ = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k

F ∗)

+ 5b1L2

kλµ + 2m1/3σ2
0∆0
g + L2
K.

8 = L2

k2 + 2k2(c2

1 + c2

2)σ2 ln(m + T ) + 6k(m+T )1/3

T

(32)

, ∆0 =

T
t=1

Gζ] = 1

Remark 17 Without loss of generality, let k = O(1) and m = O(1), we have M ′ =
O(ln(m+T )) = ˜O(1). Thus our VR-saBiAdam algorithm has a convergence rate of ˜O( 1
T 1/3 ).
Let E[
3). Since our VR-saBiAdam
algorithm requires K + 3 = Lg
µ log(CgxyCf yT /µ) + 3 = ˜O(1) samples to estimate stochastic
P
partial derivatives in each iteration, and needs T iterations. Thus our VR-saBiAdam algo-
rithm requires sample complexity of (K + 3)T = ˜O(ǫ−
3) for ﬁnding an ǫ-stationary point of
the problem (1).

ǫ, we have T = ˜O(ǫ−

Gt] = ˜O( 1
E[

T 1/3 )

≤

T

6. Conclusion

In the paper, we presented a class of fast single-loop adaptive methods for bilevel optimiza-
tion, where the outer problem is possibly nonconvex and the inner problem is strongly-
convex. Our methods can ﬂexibly use diﬀerent adaptive learning rates and cooperate with
the momentum and variance reduced techniques. Moreover, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel optimization based
on new convergence metrics.

Acknowledgments

We thank the IT Help Desk at University of Pittsburgh. This work was partially supported
by NSF IIS 1836945, IIS 1836938, IIS 1845666, IIS 1852606, IIS 1838627, IIS 1837956.

18

BiAdam: Fast Adaptive Bilevel Optimization Methods

References

Yair Censor and Arnold Lent. An iterative row-action method for interval convex program-

ming. Journal of Optimization theory and Applications, 34(3):321–353, 1981.

Yair Censor and Stavros Andrea Zenios. Proximal minimization algorithm with d-functions.

Journal of Optimization Theory and Applications, 73(3):451–464, 1992.

Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing
the generalization gap of adaptive gradient methods in training deep neural networks.
arXiv preprint arXiv:1806.06763, 2018.

Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization

method. arXiv preprint arXiv:2102.04671, 2021.

Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of
adam-type algorithms for non-convex optimization. In 7th International Conference on
Learning Representations (ICLR), 2019.

Benoˆıt Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization.

Annals of operations research, 153(1):235–256, 2007.

Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-

convex sgd. Advances in neural information processing systems, 32, 2019.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.

Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
In Interna-

Bilevel programming for hyperparameter optimization and meta-learning.
tional Conference on Machine Learning, pages 1568–1577. PMLR, 2018.

Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv

preprint arXiv:1802.02246, 2018.

Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation
methods for nonconvex stochastic composite optimization. Mathematical Programming,
155(1-2):267–305, 2016.

Zhishuai Guo and Tianbao Yang. Randomized stochastic variance-reduced methods for

stochastic bilevel optimization. arXiv preprint arXiv:2105.02266, 2021.

Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-
average estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021.

Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale frame-
work for bilevel optimization: Complexity analysis and application to actor-critic. arXiv
preprint arXiv:2007.05170, 2020.

19

Feihu and Heng

Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order mo-
mentum methods from mini to minimax optimization. arXiv preprint arXiv:2008.08170,
2020.

Feihu Huang, Junyi Li, and Heng Huang. Super-adam: Faster and universal framework of

adaptive gradients. arXiv preprint arXiv:2106.08208, 2021.

Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Nonasymptotic analysis

and faster algorithms. arXiv preprint arXiv:2010.07962, 2020.

Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran
Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum.
arXiv preprint arXiv:2102.07367, 2021.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms.

In Advances in neural

information processing systems, pages 1008–1014. Citeseer, 2000.

Gautam Kunapuli, Kristin P Bennett, Jing Hu, and Jong-Shi Pang. Classiﬁcation model
selection via bilevel programming. Optimization Methods & Software, 23(4):475–489,
2008.

Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with
adaptive stepsizes. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pages 983–992. PMLR, 2019.

Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Diﬀerentiable architecture search.

arXiv preprint arXiv:1806.09055, 2018.

Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.

arXiv preprint arXiv:1904.09237, 2019.

Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over
nonconvex landscapes. In International Conference on Machine Learning, pages 6677–
6686. PMLR, 2019.

Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimiza-

tion. arXiv preprint arXiv:2106.04692, 2021.

Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon
Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief
in observed gradients. Advances in Neural Information Processing Systems, 33, 2020.

20

BiAdam: Fast Adaptive Bilevel Optimization Methods

A. Appendix

In this section, we provide the detailed convergence analysis of our algorithms. We ﬁrst
review and provide some useful lemmas.

Given a ρ-strongly convex function φ(x), we deﬁne a prox-function (Bregman distance)

(Censor and Lent, 1981; Censor and Zenios, 1992) associated with φ(x) as follows:

V (z, x) = φ(z)

φ(x) +

φ(x), z

h∇

−

x

.

i
(cid:3)

−

(cid:2)

Then we deﬁne a generalized projection problem as in (Ghadimi et al., 2016):

x∗ = arg min

z

∈X (cid:8)

z, w
h

i

+

1
γ

V (z, x) + h(z)

,

(cid:9)

(33)

(34)

where
At the same time, we deﬁne a generalized gradient as follows:

X ⊆

∈

Rd and γ > 0. Here h(x) is convex and possibly nonsmooth function.

Rd, w

(x, w, γ) =

GX

1
γ

(x

−

x∗).

(35)

Lemma 18 (Lemma 1 in (Ghadimi et al., 2016)) Let x∗ be given in (34). Then, for any
x

Rd and γ > 0, we have

, w

∈ X

∈

w,
h

GX

(x, w, γ)

i ≥

ρ

kGX

2 +
(x, w, γ)
k

1
γ

(cid:2)

h(x∗)

−

,

h(x)
(cid:3)

where ρ > 0 depends on ρ-strongly convex function φ(x).

When h(x) = 0, in the above lemma 18, we have

w,
h

GX

(x, w, γ)

i ≥

ρ

kGX

2.
(x, w, γ)
k

(36)

(37)

Lemma 19 When the gradient estimator wt generated from Algorithms 1 and 2, for all
t

1, we have

≥

wt − ∇
k

2
F (xt)
k

≤

y∗(xt)

L2
0k
+ L2
3k

2 + L2
2 + L2
Gt − ∇
ut − ∇xf (xt, yt)
ytk
1k
2k
k
2,
2 + L2
2
ht − ∇yf (xt, yt)
yyg(xt, yt)
4k
k
k

2
2
xyg(xt, yt)
k
(38)

L2

gxyC2
f y
µ2 +

f +
. When the gradient estimator wt generated from Algorithms 3 and 4, for

2 =

3 =

+

(cid:1)

1 = 8, L2

= 8 ¯L2, L2

gxy

gxy

8C2
µ2 , L2
f

8C2

f C2
µ4

f C2
L2
µ2

−
Ht − ∇
gxyC2
gyyC2
L2
f y
µ4

where L2

and L2
all t

≥

L2

0 = 8
8C2
(cid:0)
gxy
4 =
µ2
1, we have

wt − ∇
k

2
F (xt)
k

≤

L2
0k

y∗(xt)

ytk

2 + 2
wt −
k

¯
∇

2.
f (xt, yt)
k

−

(39)

21

Feihu and Heng

Proof We ﬁrst consider the term
we have

k∇

F (xt)

¯
∇

2. Since
f (xt, yt)
k

−

∇

f (xt, y∗(xt)) =

F (xt),

∇

k∇
=

1

1

−

−

∇

2
yyg(xt, y∗(xt))
∇yf (xt, y∗(x))
(cid:1)
2
∇yf (xt, yt)
k
∇

2
f (xt, yt)
k
2
xyg(xt, y∗(xt))
(cid:0)
2
yyg(xt, yt)
(cid:1)
2
xyg(xt, y∗(xt))
(cid:0)
∇yf (xt, y∗(xt))
− ∇
2
xyg(xt, yt)
− ∇
(cid:0)

¯
f (xt, y∗(xt))
−
∇
k∇xf (xt, y∗(xt))
− ∇
2
xyg(xt, yt)
− ∇xf (xt, yt) +
∇
∇
(cid:0)
− ∇xf (xt, yt)
k∇xf (xt, y∗(xt))
2
2
yyg(xt, y∗(xt))
xyg(xt, yt)
+
∇
(cid:1)
(cid:0)
1
2
2
∇yf (xt, y∗(xt))
yyg(xt, yt)
xyg(xt, yt)
(cid:1)
(cid:0)
2
2
2
∇yf (xt, yt)
yyg(xt, yt)
xyg(xt, yt)
k
(cid:1)
(cid:0)
4C 2
f y
2 +
− ∇xf (xt, yt)
µ2 k∇
k

2
xyg(xt, y∗(xt))

− ∇
1
−

∇

∇

∇

+

+

−

−

1

1

=

≤

2
2
xyg(xt, yt)
k

− ∇

−

∇yf (xt, y∗(xt))

2
yyg(xt, y∗(xt))
(cid:1)
2
yyg(xt, y∗(xt))
∇
(cid:1)

2
xyg(xt, yt)
(cid:0)
2
yyg(xt, yt)
(cid:1)

∇

−

−

1

1

∇yf (xt, y∗(xt))

∇yf (xt, y∗(xt))

2
yyg(xt, y∗(xt))

2 +
2
yyg(xt, yt)
k

− ∇

gxy

4C 2
µ2 k∇yf (xt, y∗(xt))

2
− ∇yf (xt, yt)
k

+

L2

gxyC 2
gyyC 2
f y
µ4

+

gxy

f C 2
L2
µ2

y∗(xt)
k
(cid:1)

2

ytk

−

(40)

∇
∇
k∇xf (xt, y∗(xt))
4
gxyC 2
4C 2
f y
µ4

+

L2

k∇
gxyC 2
f y
µ2

L2

f +

4
≤
(cid:0)
= 4 ¯L2

y∗(xt)
k

−

2,
ytk

where the second last inequality is due to Assumptions 1, 2 and 4; the last equality holds
by ¯L2 = L2

L2

L2

+

gxy

.

gxyC2
f y
µ2 +

gyyC2
gxyC2
f y
µ4

f C2
L2
µ2

f +

Then we have

wt − ∇
k

2 =
F (xt)
k

≤

≤

Next we consider the term

from Algorithms 1 and 2,

f (xt, yt) + ¯
¯
f (xt, yt)
wt −
∇
∇
k
− ∇
¯
¯
2 + 2
f (xt, yt)
wt −
2
f (xt, yt)
∇
∇
k
k
k
2 + 8 ¯L2
¯
f (xt, yt)
wt −
2
y∗(xt)
k
k
∇
k
−
2 when wt = ut −
wt −
f (xt, yt)
k
k

2
F (xt)
k
2
F (xt)
k
− ∇
2.
ytk
Gt(Ht)−

¯
∇

(41)

1ht generated

=

∇

¯
2
wt −
f (xt, yt)
∇
k
k
1ht − ∇xf (xt, yt) +
2
ut −
=
Gt(Ht)−
xyg(xt, yt)
k
(cid:0)
1ht +
2
xyg(xt, yt)
ut − ∇xf (xt, yt)
Gt(Ht)−
k
(cid:0)
1ht − ∇
2
2
xyg(xt, yt)
xyg(xt, yt)
+
∇
∇
(cid:0)
(cid:0)
2
2
xyg(xt, yt)
∇yf (xt, yt)
k
(cid:0)
Gt − ∇

−
2
yyg(xt, yt)
(cid:1)
2
yyg(xt, yt)
(cid:1)
4C 2
f y
µ2 k

∇
∇
2 +
ut − ∇xf (xt, yt)
4
k
k
4C 2

∇

+

≤

−

−

1

2 +
2
xyg(xt, yt)
k

+

gxy
µ2 k

2,
ht − ∇yf (xt, yt)
k

∇
Ht

−

2
yyg(xt, yt)
(cid:1)
1ht − ∇
(cid:1)
2
yyg(xt, yt)
∇
(cid:1)

1

−

2
∇yf (xt, yt)
k
Ht

−

2
xyg(xt, yt)
(cid:0)
1ht

−

(cid:1)

1ht

4C 2

gxyC 2
f y
µ4

Ht − ∇
k

2
2
yyg(xt, yt)
k

(42)

22

BiAdam: Fast Adaptive Bilevel Optimization Methods

where the last inequality holds by Assumptions 1, 2 and 4, and the projection operators
] and
ΠCf y [
·

] in Algorithms 1 and 2.
Sµ[
·

By combining the above inequalities (40) with (42), we have

2
wt − ∇
F (xt)
k
k
f (xt, yt) + ¯
¯
f (xt, yt)
wt −
=
∇
k
∇
¯
¯
2 + 2
f (xt, yt)
wt −
2
k
k
∇
k
∇
8C 2
f y
2 +
ut − ∇xf (xt, yt)
8
µ2 k
k
k
8C 2

≤

≤

− ∇
f (xt, yt)

2
F (xt)
k
2
F (xt)
k

− ∇
2 +
2
xyg(xt, yt)
k

Gt − ∇

8C 2

gxyC 2
f y
µ4

Ht − ∇
k

2
2
yyg(xt, yt)
k

+

gxy
µ2 k

2 + 8 ¯L2
ht − ∇yf (xt, yt)
k

y∗(xt)
k

−

2.
ytk

(43)

Lemma 20 Suppose that the sequence
4. Let 0 < ηt ≤

1 and 0 < γ

ρ
2Lηt

≤

xt, yt}
{
, then we have

T
t=1 be generated from Algorithms 1, 2, 3 or

F (xt+1)

F (xt) +

≤

ηtγ
ρ k∇

F (xt)

2

wtk

−

−

ρηt
2γ k

˜xt+1 −

2.
xtk

(44)

Proof This proof is similar to the proof of Lemma 2 in (Huang et al., 2021). According
to Lemma 2, the function F (x) is L-smooth. Thus we have

F (xt+1)

≤

F (xt) +

h∇

+

xti
F (xt), xt+1 −
F (xt), ηt(˜xt+1 −
wt, ˜xt+1 −

xti

xt)
i
+ηt h∇

L
2 k

+

2

xt+1 −
L
2 k

xtk
ηt(˜xt+1 −

2
xt)
k
xti
wt, ˜xt+1 −

F (xt)

−

(45)

+

Lη2
t
2 k

˜xt+1 −

2,
xtk

= F (xt) +

h∇
= F (xt) + ηt h

=T2
{z
where the second equality is due to xt+1 = xt + ηt(˜xt+1 −
2 xT Atx is
≥
ρ-strongly convex, then we have a prox-function associated with φt(x) as in Ghadimi et al.
(2016), deﬁned as

According to Assumption 3, i.e., At ≻

xt).
1, the function φt(x) = 1

ρId for any t

=T1
{z

|

}

|

}

Vt(x, xt) = φt(x)

φt(xt) +

φt(xt), x

h∇

=

−

xti
(cid:3)

−

(cid:2)

1
2

(x

−

xt)T At(x

xt).

−

(46)

By using Lemma 1 in Ghadimi et al. (2016) to the problem ˜xt+1 = arg minx
1
2γ (x

xt)T At(x

, we have

xt)

−

−

Then we obtain

(cid:9)

1
γ

wt,
h

(xt −

˜xt+1)

i ≥

1
γ

ρ
k

(xt −

2.
˜xt+1)
k

T1 =

wt, ˜xt+1 −
h

xti ≤ −

ρ
γ k

˜xt+1 −

2.
xtk

23

wt, x
h

i

+

∈X

(cid:8)

(47)

(48)

Feihu and Heng

Next, consider the bound of the term T2, we have

−

h∇

T2 =

F (xt)
F (xt)

wt, ˜xt+1 −
wtk · k
wtk
−
where the ﬁrst inequality is due to the Cauchy-Schwarz inequality and the last is due to
Young’s inequality. By combining the above inequalities (45), (48) with (49), we obtain

xti
˜xt+1 −
ρ
2 +
4γ k

xtk
˜xt+1 −

≤ k∇
γ
ρ k∇

−
F (xt)

2,
xtk

(49)

≤

F (xt+1)

≤

≤

F (xt) + ηth∇
ηtγ
F (xt) +
ρ k∇
ηtγ
ρ k∇
ηtγ
ρ k∇

F (xt) +

= F (xt) +

≤

F (xt)

−

F (xt)

wt, ˜xt+1 −
2 +
wtk

−

F (xt)

F (xt)

−

−

2

2

wtk
wtk

−

−

xti
ρηt
4γ k
ρηt
2γ k
ρηt
2γ k

+ ηth
˜xt+1 −

wt, ˜xt+1 −
xtk

−

2

˜xt+1 −
˜xt+1 −

−

2

xtk
2,
xtk

(cid:0)

+

Lη2
t
2 k
˜xt+1 −
Lη2
t
2

xti
ρηt
γ k
ρηt
4γ −

2

˜xt+1 −
2 +
xtk

xtk
Lη2
t
2 k

˜xt+1 −

2

xtk

˜xt+1 −
k

2

xtk

(50)

(cid:1)

where the last inequality is due to 0 < γ

ρ
2Lηt

.

≤

Lemma 21 Suppose the sequence
Under the above assumptions, given 0 < ηt ≤
have

xt, yt}
{

T
t=1 be generated from Algorithms 1, 2, 3 or 4.
, we

1, Bt = Ip for all t

1 and 0 < λ

1
6Lg

≤

≥

yt+1 −
k

2
y∗(xt+1)
k

≤

(1

+

where κ = Lg/µ.

ηtµλ
yt −
)
4
k
−
25ηtλ
6µ k∇yg(xt, yt)

2
y∗(xt)
k
−
vtk

−

2

˜yt+1 −

3ηt
4 k
25κ2ηt
6µλ k

ytk
˜xt+1 −

2 +

2,
xtk

(51)

Proof This proof follows the following proof of Lemma 22. Let bt = b = 1 for all t
can easily obtain the above result.

≥

1, we

Lemma 22 Suppose the sequence
Under the above assumptions, given 0 < ηt ≤
and 0 < λ
, we have

xt, yt}
{

T
t=1 be generated from Algorithms 1, 2, 3 or 4.
1,

1, Bt = btIp (0 < b

ˆb) for all t

bt ≤

≤

≥

b
6Lg

≤

yt+1 −
k

2
y∗(xt+1)
k

≤

(1

+

where κ = Lg/µ.

2
y∗(xt)
k

−

yt −
)
k

ηtµλ
4bt
−
25ηtλ
6µbt k∇yg(xt, yt)

3ηt
˜yt+1 −
4 k
25κ2ηtbt

2

ytk

2 +

vtk

−

6µλ k

˜xt+1 −

2,
xtk

(52)

24

BiAdam: Fast Adaptive Bilevel Optimization Methods

Proof This proof mainly follows the proof of Lemma 30 in (Huang et al., 2020). According
to Assumption 1, i.e., the function g(x, y) is µ-strongly convex w.r.t y, we have

g(xt, y)

g(xt, yt) +

≥
= g(xt, yt) +

h∇yg(xt, yt), y
˜yt+1i
+
vt, y
h
yti
h∇yg(xt, yt), ˜yt+1 −

yti
−
−
h∇yg(xt, yt)
−
µ
2.
+
ytk
y
2 k
−

−

+

+

µ
y
2 k

2

ytk
vt, y

˜yt+1i

−

According to the Assumption 2, i.e., the function g(x, y) is Lg-smooth, we have

g(xt, ˜yt+1)

yti
h∇yg(xt, yt), ˜yt+1 −
Combining the about inequalities (53) with (54), we have

g(xt, yt) +

≤

+

Lg
2 k

˜yt+1 −

2.
ytk

g(xt, y)

≥

g(xt, ˜yt+1) +

+

µ
y
2 k

2

ytk

−

vt, y
h
Lg
2 k

˜yt+1i
−
˜yt+1 −

−

+

h∇yg(xt, yt)
2.
ytk

vt, y

˜yt+1i

−

−

By the optimality of the step 6 of Algorithms 1, 2, 3 or 4, given Bt = btIp (bt ≥

we have

vt +
h

bt
λ

(˜yt+1 −

yt), y

˜yt+1i ≥

−

0,

y

∀

.

∈ Y

Then we obtain

(53)

(54)

(55)

b > 0),

(56)

vt, y
h

−

˜yt+1i ≥
=

bt
λ h
bt
λ k

˜yt+1 −
˜yt+1 −

y

yt, ˜yt+1 −
bt
2 +
ytk
λ h

i
˜yt+1 −

yt, yt −

y

.
i

(57)

By pugging the inequalities (57) into (55), we have

g(xt, y)

≥

g(xt, ˜yt+1) +

+

h∇yg(xt, yt)

−

Let y = y∗(xt), then we have

bt
λ h

˜yt+1 −
vt, y

−

y

yt, yt −
+
˜yt+1i

+

i
µ
y
2 k

bt
λ k

−

2

˜yt+1 −
ytk

−

2

ytk
Lg
2 k

˜yt+1 −

2.
ytk

(58)

g(xt, y∗(xt))

≥

g(xt, ˜yt+1) +

bt
λ h

yt, yt −

˜yt+1 −
vt, y∗(xt)

y∗(xt)
i
µ
2 k

+

+ (

bt
λ −
y∗(xt)

Lg
˜yt+1 −
)
2
k
2.
ytk

−

2

ytk

(59)

+

h∇yg(xt, yt)
, y) and y∗(xt) = arg miny
Due to the strongly-convexity of g(
·
g(xt, ˜yt+1). Thus, we obtain

˜yt+1i

−

−

g(xt, y), we have g(xt, y∗(xt))

∈Y

≤

0

≥

bt
λ h

+ (

yt, yt −
˜yt+1 −
Lg
bt
˜yt+1 −
)
2
λ −
k

y∗(xt)
i
2 +

ytk

+

h∇yg(xt, yt)
µ
y∗(xt)
2 k

−

−
2.
ytk

vt, y∗(xt)

˜yt+1i

−

(60)

25

Feihu and Heng

yt), we have

By yt+1 = yt + ηt(˜yt+1 −
2 =
y∗(xt)
yt+1 −
k
k

yt + ηt(˜yt+1 −
yt)
k
−
2 + 2ηth
y∗(xt)
yt −
k
k

2
y∗(xt)
k
yt, yt −
˜yt+1 −

=

y∗(xt)
i

+ η2
t k

˜yt+1 −

2.
ytk

(61)

Then we obtain

˜yt+1 −
h

yt, yt −

y∗(xt)
i

=

1
2ηt k

yt+1 −

2
y∗(xt)
k

−

1
2ηt k

yt −

2
y∗(xt)
k

−

ηt
2 k

˜yt+1 −

2. (62)

ytk

Consider the upper bound of the term

h∇yg(xt, yt)

−

vt, y∗(xt)

−

˜yt+1i

, we have

h∇yg(xt, yt)
=

−
h∇yg(xt, yt)

vt, y∗(xt)

−
vt, y∗(xt)

−
1
µ k∇yg(xt, yt)
2
µ k∇yg(xt, yt)

2

vtk

2

vtk

−

−

≥ −

=

−

˜yt+1i
yti
−
µ
4 k
µ
4 k

−

−

+

h∇yg(xt, yt)
ytk

−

−

2

y∗(xt)

y∗(xt)

2

ytk

−

−

˜yt+1i
vt, yt −
−
1
µ k∇yg(xt, yt)
µ
2.
˜yt+1k
4 k

yt −

−

2

vtk

−

µ
4 k

yt −

2

˜yt+1k

(63)

By plugging the inequalities (62) and (63) into (60), we obtain

2
y∗(xt)
k

(

bt

≤

2ηtλ −

bt
yt+1 −
2ηtλ k
µ
bt
4
µ
4
µ
4
µ
4

2ηtλ −

2ηtλ −

2ηtλ −

= (

≤

≤

bt

bt

(

(

yt −
)
k

2 + (
y∗(xt)
k

yt −
)
k

2 + (
y∗(xt)
k

yt −
)
k

2
y∗(xt)
k

yt −
)
k

2
y∗(xt)
k

−

−

Lg
2 −

bt
λ

+

µ
4
bt
2λ
bt
8λ −

+

btηt
2λ
3Lg

+

4 −
3bt
8λ
(cid:0)
3bt
8λ k

˜yt+1 −

ytk

˜yt+1 −
)
k
3Lg
4

(cid:1)
2 +

2 +

ytk

ytk

˜yt+1 −
)
k
2
2 +
µ k∇yg(xt, yt)
2
µ k∇yg(xt, yt)
2,
vtk

˜yt+1 −
k
2
µ k∇yg(xt, yt)

ytk

vtk

2 +

−

−

2

2
µ k∇yg(xt, yt)

2

vtk

−

2

vtk

−

(64)

where the second inequality holds by Lg ≥
to 0 < λ

. It implies that

b
6Lg ≤

bt
6Lg

≤

µ and 0 < ηt ≤

1, and the last inequality is due

yt+1 −
k

2
y∗(xt)
k

≤

(1

−

ηtµλ
2bt

yt −
)
k

2
y∗(xt)
k

−

3ηt
4 k

˜yt+1 −

2 +

ytk

4ηtλ
µbt k∇yg(xt, yt)

−

2.
vtk
(65)

Next, we decompose the term

yt+1 −
k

2 =
y∗(xt+1)
k

=

≤

≤

yt+1 −
k
yt+1 −
k
(1 +

(1 +

ηtµλ
4bt
ηtµλ
4bt

2 as follows:
y∗(xt+1)
k
2
y∗(xt+1)
k
y∗(xt), y∗(xt)

yt+1 −
k
y∗(xt) + y∗(xt)
−
2 + 2
yt+1 −
y∗(xt)
k
h
yt+1 −
)
k

2 + (1 +
y∗(xt)
k

yt+1 −
)
k

2 + (1 +
y∗(xt)
k

26

y∗(xt+1)
i

+

y∗(xt)
k

−

2
y∗(xt+1)
k

−
y∗(xt)
)
k

2
y∗(xt+1)
k

−

)κ2

xt −
k

2,
xt+1k

(66)

4bt
ηtµλ
4bt
ηtµλ

BiAdam: Fast Adaptive Bilevel Optimization Methods

where the ﬁrst inequality holds by Cauchy-Schwarz inequality and Young’s inequality, and
the second inequality is due to Lemma 3, and the last equality holds by xt+1 = xt +ηt(˜xt+1 −
xt).

By combining the above inequalities (65) and (66), we have

yt+1 −
k

2
y∗(xt+1)
k

≤

(1 +

)(1

ηtµλ
4bt
ηtµλ
4bt

)

ηtµλ
2bt

yt −
)
−
k
4ηtλ
µbt k∇yg(xt, yt)

+ (1 +

2
y∗(xt)
k

(1 +

−

3ηt
4 k

˜yt+1 −

2

ytk

2 + (1 +

vtk

−

)κ2

xt −
k

2.
xt+1k

)

ηtµλ
4bt
4bt
ηtµλ

Since 0 < ηt ≤
Then we have

1, 0 < λ

b
6Lg ≤

bt
6Lg

≤

and Lg ≥

µ, we have λ

bt
6Lg ≤

bt

6µ and ηt ≤

1

≤

≤

bt
6µλ .

(1 +

ηtµλ
4bt

)(1

ηtµλ
2bt

) = 1

ηtµλ
2bt

+

ηtµλ
4bt −

t µ2λ2
η2
8b2

t ≤

ηtµλ
4bt

,

1

−

−
ηtµλ
4bt
ηtµλ
4bt

)

−
3ηt
4

)

3ηt
4 ≤ −

4ηtλ
µbt ≤

(1 +

,

1
24

)

(1 +

−

(1 +

(1 +

4bt
ηtµλ

)κ2

≤

btκ2
6ηtµλ

+

=

4ηtλ
µbt
4btκ2
ηtµλ

,

25ηtλ
6µbt
25btκ2
6ηtµλ

,

=

where the second last inequality is due to ηtµλ
By using xt+1 = xt + ηt(˜xt+1 −

xt), then we have

bt ≤

1

6 and the last inequality holds by bt

1.

6µληt ≥

yt+1 −
k

2
y∗(xt+1)
k

≤

(1

+

2
y∗(xt)
k

−

yt −
)
k

ηtµλ
4bt
−
25ηtλ
6µbt k∇yg(xt, yt)

3ηt
˜yt+1 −
4 k
25κ2ηtbt

2

ytk

2 +

vtk

−

6µλ k

˜xt+1 −

2.
xtk

(67)

A.1 Convergence Analysis of BiAdam Algorithm

In the subsection, we provide the detail convergence analysis of our BiAdam algorithm.

Lemma 23 Assume that the stochastic partial derivatives ut+1, ht+1, vt+1, Gt+1 and Ht+1
be generated from Algorithm 1, we have

E

k∇xf (xt+1, yt+1)

−

2

ut+1k

≤

E

k∇yf (xt+1, yt+1)

−

2

ht+1k

≤

(1
−
+ 2L2

βt+1)E
f η2

t /βt+1

k∇xf (xt, yt)
˜xt+1 −
k

E

(cid:0)

(1
−
+ 2L2

ˆβt+1)E
f η2

t / ˆβt+1

k∇yf (xt, yt)
˜xt+1 −
k

E

(cid:0)

2 + β2

utk
2 + E

t+1σ2
˜yt+1 −
k

−
xtk

2 + ˆβ2

htk
2 + E

t+1σ2
˜yt+1 −
k

−
xtk

(68)

2

ytk

.

(cid:1)

2

ytk

.

(cid:1)

27

E

k∇yg(xt+1, yt+1)

−

2

vt+1k

≤

E

k∇

2
xyg(xt+1, yt+1)

2

Gt+1k

−

≤

Feihu and Heng

(1
−
+ 2L2

αt+1)E
gη2

t /αt+1

k∇yg(xt, yt)
˜xt+1 −
k

E

vtk
−
2 + E
xtk

t+1σ2
˜yt+1 −
k

2 + α2

(cid:0)

2

ytk

.
(cid:1)

(1
−
+ 2L2

ˆαt+1)E
gxyη2

k∇
t /ˆαt+1

2
xyg(xt, yt)
Gtk
−
xtk
˜xt+1 −
k

E

2 + ˆα2

t+1σ2
˜yt+1 −
k

2 + E

(cid:0)

2

ytk

.

(cid:1)

(69)

(70)

2

E

−

≤

k∇

Ht+1k

2
yyg(xt+1, yt+1)

2
2 + ˜αt+1σ2
yyg(xt, yt)
Htk
−
˜yt+1 −
xtk
˜xt+1 −
k
k
(cid:1)
Proof Without loss of generality, we only consider the term E
2.
k∇yf (xt+1, yt+1)
ht+1k
ˆβt+1∇yf (xt+1, yt+1; ξt+1) +
The other terms are similar for this term. Since ht+1 = ΠCf y
(1

˜αt+1)E
gyyη2

(1
−
+ 2L2

k∇
t /˜αt+1

2 + E

ytk

(71)

−

E

(cid:0)

(cid:2)

2

.

E

2

ˆβt+1)ht

, we have
(cid:3)

ht+1k
−
∇yf (xt+1, yt+1)
(cid:3)

−
k∇yf (xt+1, yt+1)
= E
ΠCf y
k
(cid:2)
E
k∇yf (xt+1, yt+1)
ˆβt+1(
k
+ (1

≤
= E

−

−

ΠCf y

(cid:2)
ˆβt+1∇yf (xt+1, yt+1; ξt+1)

ˆβt+1∇yf (xt+1, yt+1; ξt+1) + (1
ˆβt+1)htk
(1
−
−
ˆβt+1)(
− ∇yf (xt+1, yt+1; ξt+1)) + (1
−

2

−

∇yf (xt+1, yt+1)
ˆβt+1)
∇yf (xt+1, yt+1)
− ∇yf (xt, yt)
(cid:0)
(cid:1)
ˆβt+1)(
ˆβt+1)
∇yf (xt, yt)
ht) + (1
−
−
−
(cid:0)
E
2
− ∇yf (xt+1, yt+1; ξt+1)
k∇yf (xt+1, yt+1)
t+1
k
ˆβt+1)2(1 + ˆβt+1)E
2 + (1

k∇yf (xt, yt)

htk

2
k
∇yf (xt+1, yt+1)

ˆβt+1)2(1 +

E

t+1
ˆβt+1)E

k∇yf (xt+1, yt+1)
k∇yf (xt, yt)

−

−

(1

−

≤

ˆβt+1)E

k∇yf (xt, yt)

−

2 +

−
−
2
− ∇yf (xt+1, yt+1; ξt+1)
k
1
k∇yf (xt+1, yt+1)
htk
ˆβt+1
f η2
2L2
t
E
ˆβt+1 (cid:0)

˜xt+1 −
k

xtk

htk

2 +

E

= E

(1
k
+ ˆβ2

≤

(1

−
+ ˆβ2

(1

≤

(72)

ˆβt+1)ht

−

2
k

(cid:3)

∇yf (xt, yt)

ht)

−

− ∇yf (xt, yt)
(cid:1)

2
k

1
ˆβt+1

)E

k∇yf (xt+1, yt+1)

2
− ∇yf (xt, yt)
k

2 + ˆβ2
− ∇yf (xt, yt)
k

t+1σ2

2 + E

˜yt+1 −
k

2

ytk

(cid:1)

+ ˆβ2

t+1σ2,

where the third equality is due to Eξt+1[
last inequality holds by 0
ˆβt+1)2(1 + 1
ˆβt+1)(1 + 1
1
) =
ˆβt+1
ˆβt+1
−
−
ηt(˜xt+1 −
inequality holds by Assumption 2 and xt+1 = xt −

∇
1 such that (1
(1

ˆβt+1)2(1+ ˆβt+1) = 1
−
ˆβt+1 + 1
1
ˆβt+1 ≤
ˆβt+1
xt), yt+1 = yt −

f (xt+1, yt+1); the second
t+1+ ˆβ3
ˆβ2
t+1 ≤
, and the last
yt).
ηt(˜yt+1 −

ˆβt+1 ≤
)
≤

f (xt+1, yt+1; ξt+1)] =

ˆβt+1 and (1

ˆβt+1−

∇

−

−

−

≤

Theorem 24 Under the above Assumptions (1, 2, 4, 5, 8), in the Algorithm 1, given
0, βt+1 = c1ηt, ˆβt+1 = c2ηt, αt+1 = c3ηt, ˆαt+1 = c4ηt,
Bt = Ip, ηt =
m1/2
k ,

k2, (c1k)2, (c2k)2, (c3k)2, (c4k)2, (c5k)2

(m+t)1/2 for all t

˜αt+1 = c5ηt, m

max

≥

k

1

, k > 0, 5L2
(cid:1)

4 ≤

c1 ≤

≥

(cid:0)

28

BiAdam: Fast Adaptive Bilevel Optimization Methods

5L2
4
4 ≤
min

c2 ≤
2√6L2

(cid:0)

k , 125L2
m1/2

6µ2

0

c3 ≤

≤

k , 5L2
m1/2

2

√6λµρ

5µ2λ2+125L2

0κ2 , m1/2ρ

4Lk

and 0 < λ

4 ≤
min

≤

c4 ≤
15L2
0
4L2

(cid:0)

5µ , 1

6Lg

k , 5L2
m1/2

3

4 ≤
, we have

c5 ≤

m1/2

k , 0 < γ

≤

(cid:1)
T

Xt=1

1
T

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

(cid:1)
2√6G
T 1/4

,

(73)

where G = ρ(F (x1)
−
kγ
8L2
8L2
12µ2L2
g
f
f
125L2
5L2
5L2
0
4
1

+

+

+

F ∗)

+ 5L2
8L2
gxy
5L2
2

kλµ + 5σ2
0∆0
8L2
gyy
+
5L2
3

.

k + 5mσ2

k

ln(m + T ) and ∆0 =

y1 −
k

2, L2
y∗(x1)
k

5 =

k

Proof Here we consider convergence properties of our BiAdam algorithm with the general
ρId for variable x and the non-adaptive matrix Bt = Ip for variable
adaptive matrix At ≻
1 and
y. Since ηt =
m1/2ρ
γ
4Lk ≤
βt+1 = c1ηt ≤
ˆβt+1 ≤
i = 1, 2,

k2, we have ηt ≤
1 and m
(c2k)2, (c3k)2, (c4k)2, (c5k)2
≥
1. At the same time, we have ci ≤

(m+t)1/2 on t is decreasing and m
ρ
ρ
for any t
2Lηt
1. Similarly, due to m
1, and ˜αt+1 ≤

≥
0. Due to 0 < ηt ≤
max

(c1k)2, we have
, we have
m1/2
(cid:1)
k

, 5. According to Lemma 23, we have

2Lη0 ≤
c1k
m1/2 ≤

1, ˆαt+1 ≤

η0 = k

m1/2 ≤

for all

≥

≥

≤

(cid:0)

1, αt+1 ≤
· · ·

E

2

E

−

ut+1k
−

k∇xf (xt+1, yt+1)
≤ −
=

βt+1E
c1ηtE
5L2
1ηt
4

k∇xf (xt, yt)
k∇xf (xt, yt)
E

−
k∇xf (xt, yt)

≤ −

−

−

−
2 + 2L2
utk
2 + 2L2
utk

k∇xf (xt, yt)
f η2
t /βt+1
f ηt/c1
8L2
f ηt
5L2

utk
−
˜xt+1 −
k
(cid:0)
xtk
˜xt+1 −
k
(cid:0)
xtk
˜xt+1 −
k

utk

2 +

2

1 (cid:0)

xtk
2 +

2 +

2 +
˜yt+1 −
k
ytk
˜yt+1 −
k
ytk
˜yt+1 −
k

2

2

ytk

2

(cid:1)
+ c2

t+1σ2
+ β2
t σ2
1η2
t σ2
mη2
k2

,

(cid:1)

(cid:1)

+

4

where the above equality holds by βt+1 = c1ηt, and the last inequality is due to 5L2
k . Similarly, given 5L2
m1/2
k , we have
E
k∇yf (xt, yt)
k∇yf (xt+1, yt+1)
8L2
5L2
f ηt
4ηt
E
2 +
5L2
4

c2 ≤
ht+1k
−
k∇yf (xt, yt)

htk
−
˜xt+1 −
k

˜yt+1 −
k

−
htk

xtk

4 ≤

ytk

≤ −

2 +

m1/2

+

−

E

t σ2
mη2
k2

2

2

2

1

4 ≤

.

4 (cid:0)

(cid:1)

m1/2

k , we have

Given 125L2

0

E

c3 ≤
6µ2 ≤
k∇yg(xt+1, yt+1)
125L2
0ηt
6µ2

E

≤ −

2

E

vt+1k
−
k∇yg(xt, yt)

k∇yg(xt, yt)
12µ2L2
2 +
vtk
125L2

−
gηt

−

−

2

vtk
˜xt+1 −
k

0 (cid:0)

Given 5L2

2

m1/2

c4 ≤
4 ≤
2
xyg(xt+1, yt+1)
5L2
2ηt
4

k∇

E

E

k∇

≤ −

k , we have
2
Gt+1k

−

2
xyg(xt, yt)

E

k∇
2 +

−
Gtk

−

2

2
xyg(xt, yt)
8L2
gxyηt
5L2
2

Gtk
−
˜xt+1 −
k
(cid:0)

29

2 +

xtk

˜yt+1 −
k

2

ytk

+

t σ2
mη2
k2

.

(cid:1)

(77)

2 +

xtk

˜yt+1 −
k

2

ytk

+

t σ2
mη2
k2

.

(cid:1)

(74)

c1 ≤

(75)

(76)

Feihu and Heng

m1/2

k , we have

c5 ≤

Given 5L2

3

4 ≤

E

k∇

≤ −

2
yyg(xt+1, yt+1)
5L2
3ηt
4

k∇

E

2

Ht+1k

−

E

k∇
2 +

−
Htk

−

2
yyg(xt, yt)

2

2
yyg(xt, yt)
8L2
gyyηt
5L2
3

Htk
−
˜xt+1 −
k

(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

+

t σ2
mη2
k2

.

(cid:1)

(78)

According to Lemmas 19 and 20, we have

F (xt+1)

F (xt)

−

≤

L2
0ηtγ
yt −
ρ
k
L2
2ηtγ
ρ
L2
4ηtγ
ρ

k∇

+

+

2 +
y∗(xt)
k

L2
1ηtγ
ρ

2
xyg(xt, yt)

Gtk

−

k∇xf (xt, yt)
L2
3ηtγ
2 +
ρ

k∇

2

utk

−

(79)

2
yyg(xt, yt)

2

Htk

−

k∇yf (xt, yt)

2

htk

−

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 21, we have

yt+1 −
k

2
y∗(xt+1)
k

yt −

− k

2
y∗(xt)
k

≤ −

+

ηtµλ

2
yt −
y∗(xt)
4 k
k
25ηtλ
6µ k∇yg(xt, yt)

−
vtk

−

3ηt
4 k

2 +

2

ytk

˜yt+1 −
25κ2ηt
6µλ k

˜xt+1 −

(80)

2.
xtk

Next, we deﬁne a Lyapunov function, for any t

1

≥

Φt = E

+

5L2
0γ
λµρ k

F (xt) +
(cid:2)
k∇yg(xt, yt)

yt −
2 +
vtk

−

2 +
y∗(xt)
k

γ
ρ

(cid:0)
2
xyg(xt, yt)

k∇

k∇xf (xt, yt)
2 +
Gtk

−

k∇

2 +

utk
−
2
yyg(xt, yt)

k∇yf (xt, yt)
.
Htk

−

2

2

htk

−

(cid:1)(cid:3)

30

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

Φt

Φt+1 −
= F (xt+1)

E

−

F (xt) +

−
+ E
E

5L2
0γ
λµρ
−
k∇xf (xt, yt)
utk
k∇yg(xt+1, yt+1)
−
2
Gtk
xyg(xt, yt)
−
k∇
−
L2
0ηtγ
2 +
yt −
y∗(xt)
ρ
k
k
L2
3ηtγ
ρ
5L2
0γ
λµρ (cid:18) −
5L2
1ηt
4

2
yyg(xt, yt)

yt −

ηtµλ

4 k

k∇

+

+

−

+

E

≤

γ
ρ (cid:18) −
5L2
4ηt
4
125L2
6µ2

E

0ηt

5L2
2ηt
4
5L2
3ηt
4
γηt
4ρ (cid:18)

−

−

−

−

=

−

k∇xf (xt, yt)

−

k∇yf (xt, yt)

−

htk

2 +

E

k∇yg(xt, yt)

−

vtk

2 +

E

E

k∇

k∇

2
xyg(xt, yt)

2
yyg(xt, yt)

−

−

2 +

Gtk

2 +

Htk

L2
0k

yt −

2 + L2
y∗(xt)
1
k

E

+ L2
3

E

2
yyg(xt, yt)

2

Htk

−

(cid:19) −

k∇
15L2
0γ
4λµρ −

L2
5γ
ρ

−

≤ −

(cid:0)
γηt
4ρ (cid:18)

L2
0k

yt −

ηtk

˜yt+1 −
(cid:1)
2 + L2
y∗(xt)
1
k

ytk
E

+ L2
3

E

k∇

2
yyg(xt, yt)

2

Htk

−

(cid:19) −

γ
ρ

2

2

2
yt+1 −
y∗(xt+1)
k
k
(cid:0)
2 + E
k∇yf (xt+1, yt+1)
−
k∇yg(xt, yt)
vt+1k
−
2 + E
2
yyg(xt+1, yt+1)
L2
1ηtγ
ρ

yt −
− k
ht+1k
−
2 + E
vtk
−
2
Ht+1k
−
−
L2
2ηtγ
2 +
utk
ρ

−

E

E

(cid:1)

E

+

k∇xf (xt+1, yt+1)

2
y∗(xt)
k
(cid:0)
htk
k∇yf (xt, yt)
2
xyg(xt+1, yt+1)
Gt+1k
−
k∇
E
2
Htk
yyg(xt, yt)

−

2

2

2

k∇
2
xyg(xt, yt)

−
Gtk

2

−

k∇

(cid:1)

2

ut+1k

−

k∇
k∇xf (xt, yt)
L2
4ηtγ
2 +
ρ
3ηt
4 k

−

Htk

2
y∗(xt)
k

k∇yf (xt, yt)
2 +

−

ytk

2

htk
25ληt
6µ

˜yt+1 −
8L2
f ηt
5L2

ρηt
2γ k

˜xt+1 −

2

xtk

−

E

k∇yg(xt, yt)

−

2 +

vtk

25κ2ηt
6µλ k

˜xt+1 −

2

xtk

(cid:19)

t σ2
mη2
k2

2 +

2 +

xtk

˜xt+1 −
utk
k
1 (cid:0)
8L2
f ηt
˜xt+1 −
5L2
k
4 (cid:0)
12µ2L2
gηt
125L2
gxyηt
5L2
2
gyyηt
5L2
3

˜xt+1 −
k
0 (cid:0)

˜xt+1 −
k

˜xt+1 −
k

8L2

8L2

(cid:0)

xtk

xtk

2 +

xtk

˜yt+1 −
k

˜yt+1 −
k

ytk

2

(cid:1)
˜yt+1 −
k

2 +

xtk

2

+

ytk
(cid:1)
mη2
t σ2
k2

+

2

ytk

(cid:1)

2 +

2 +

˜yt+1 −
k

2

ytk

˜yt+1 −
k

2

ytk

+

+

(cid:1)

(cid:1)

+

mη2
t σ2
k2
mη2
t σ2
k2
mη2
t σ2
k2 (cid:19)

(cid:0)
k∇xf (xt, yt)
L2
5γ
ρ −

ρ
2γ −

−

(cid:0)
2 +

5mγσ2
k2ρ

η2
t

2 + L2
4

utk
125κ2L2
0γ
6µ2λ2ρ

E

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

ηtk

˜xt+1 −

2

xtk

(cid:1)

k∇xf (xt, yt)
ρηt
4γ k

˜xt+1 −

2 + L2
4

utk

E

−

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

2 +

xtk

5mγσ2
k2ρ

η2
t ,

(81)

where the ﬁrst inequality holds by the above inequalities (74), (75), (76), (77), (78), (79)
15L2
0
and (80); the last inequality is due to 0 < γ
5µ ;
4L2

0κ2 and 0 < λ

5µ2λ2+125L2

2√6L2

√6λµρ

≤

≤

L2

5 =

8L2
f
5L2
1

+

8L2
f
5L2
4

+

12µ2L2
g
125L2
0

+

8L2
gxy
5L2
2

+

8L2
gyy
5L2
3

.

31

Feihu and Heng

For simplicity, let Dt = L2
yt −
0k
2 + L2
2 + L2
2
Htk
yyg(xt, yt)

3k∇

−

Gtk

2 + L2
y∗(xt)
k
4k∇yf (xt, yt)
ρηt
4γ k

E[Dt]

−

γηt
4ρ

1k∇xf (xt, yt)
htk
−

utk
2. Then we have

−

2 + L2

2
xyg(xt, yt)

−

2k∇

˜xt+1 −

2 +

xtk

5mγσ2
k2ρ

η2
t .

(82)

Φt+1 −

Φt ≤ −

Taking average over t = 1, 2,

· · ·

, T on both sides of (82), we have

1
T

T

E

Xt=1

(cid:2)

ηt
4

Dt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:3)

Xt=1

Φt+1)

ρ(Φt −
T γ

+

1
T

T

Xt=1

5mσ2
k2 η2
t .

, let ∆0 =

y1 −
k

2, we have
y∗(x1)
k

Given x1 ∈ X

Φ1 = E

+

and y1 ∈ Y
5L2
0γ
λµρ k

F (x1) +
(cid:2)
k∇yg(x1, y1)

5L2

−
0γ∆0
λµρ

F (x1) +

≤

2 +
y∗(x1)
k

γ
ρ

(cid:0)
2
xyg(x1, y1)

y1 −
2 +
v1k
5γσ2
+
ρ

k∇

,

k∇xf (x1, y1)
2 +
G1k

−

2 +

u1k
−
2
yyg(x1, y1)

k∇yf (x1, y1)
H1k

−

2

k∇

2

h1k

−

(cid:1)(cid:3)

where the last inequality holds by Assumption 5. Since ηt is decreasing on t, i.e., η−
for any 0

T , we have

t

(83)

1

η−
t

1
T ≥

≤

≤

1
T

≤

≤

≤

≤

=

Dt
4

T

T

E

Xt=1

(cid:0)

ρ
T γηT

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

(cid:1)

Φt −

Xt=1 (cid:0)

F (x1) +

Φt+1

+

5L2

(cid:1)
0γ∆0
λµρ

T

1
T ηT

Xt=1
5γσ2

ρ −

+

5mσ2
k2 η2

t

F ∗)

F ∗)

+

+

F ∗)

−

+

+

+

5L2
0∆0
T ηT λµ
5L2
0∆0
T ηT λµ
5L2
0∆0
kλµ

5σ2
T ηT
5σ2
T ηT
5σ2
k

+

+

+

+

1
T ηT

T

5mσ2
k2 η2

t

Xt=1
k2
m + t

T

1

dt

ln(m + T )

F ∗

+

(cid:1)
5mσ2
T ηT k2 Z
5mσ2
T ηT
5mσ2
k

ln(m + T )

(m + T )1/2
T

,

(cid:19)

where the second inequality holds by the above inequality (83). Let G = ρ(F (x1)
−
kγ
5L2
kλµ + 5σ2
0∆0

ln(m + T ), we have

k + 5mσ2

k

1
T

T

E

Xt=1

(cid:2)

Dt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

G
T

≤

(cid:3)

(m + T )1/2.

32

(84)

F ∗)

+

BiAdam: Fast Adaptive Bilevel Optimization Methods

Since
L3k∇

Mt = ρ
γ k
2
yyg(xt, yt)

˜xt+1 −
Htk
−

yt −

+ L0k

xtk
+ L4k∇yf (xt, yt)

y∗(xt)
+ L1k∇xf (xt, yt)
k
htk
−

+
+ L2k∇
, according to the Jensen’s inequality, we have

Gtk

utk

2
xyg(xt, yt)

−

−

1
T

T

Xt=1

E[

1
2Mt]

6
T

≤ (cid:18)

T

Xt=1 (cid:0)

Dt
4

+

ρ2
4γ2 k

2

1/2

(cid:19)

(cid:1)

xtk
˜xt+1 −
√6Gm1/4
T 1/2

√6G
T 1/2

≤

(m + T )1/4

≤

+

√6G
T 1/4

,

(85)

where the last inequality is due to (a + b)1/4

a1/4 + b1/4 for all a, b > 0. Thus we obtain

≤

1
T

T

Xt=1

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

2√6G
T 1/4

.

(86)

≤

bt ≤

1, ηt =

b > 0) for all t

Theorem 25 Under the above Assumptions (1, 2, 4, 5, 8), in the Algorithm 1, given
0, βt+1 = c1ηt, ˆβt+1 = c2ηt,
Bt = btIp (ˆb
(m+t)1/2 for all t
k2, (c1k)2, (c2k)2, (c3k)2, (c4k)2, (c5k)2
max
αt+1 = c3ηt, ˆαt+1 = c4ηt, ˜αt+1 = c5ηt, m
,
≥
k > 0, 5L2
k , 5L2
k , 125L2
m1/2
m1/2
m1/2
(cid:0)
(cid:1)
k ,
c2 ≤
6µ2 ≤
5L2
, m1/2ρ
b
3
,
c5 ≤
4 ≤
6Lg
4Lk
5µ2λ2+125ˆb2L2
we have

c3 ≤
and 0 < λ

k , 5L2
m1/2
min

c1 ≤
k , 0 < γ

c4 ≤
15bL2
0
5µ ,
4L2

4 ≤
min

4 ≤
m1/2

2q6L2

4 ≤

√6λµρ

0κ2

≥

≥

≤

≤

(cid:1)

(cid:0)

(cid:1)

(cid:0)

k

1

4

0

2

1
T

T

Xt=1

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

2√6G
T 1/4

.

(87)

F ∗)

where G = ρ(F (x1)
−
kγ
8L2
8L2
12µ2L2
f
f
g
125L2
5L2
5L2
0
4
1

+

+

+

+ 5b1L2
8L2
gxy
5L2
2

kλµ + 5σ2
0∆0
8L2
gyy
+
5L2
3

.

k + 5mσ2

k

ln(m + T ) and ∆0 =

y1 −
k

2, L2
y∗(x1)
k

5 =

Proof Here we consider convergence properties of our BiAdam algorithm with the general
bt ≥
adaptive matrix At ≻
b > 0) for variable y. This proof basically follows the proof of Theorem 24.

ρId for variable x and the global adaptive matrix Bt = btIp (ˆb

≥

According to Lemma 23, we have

E

2

E

−

ut+1k
−

k∇xf (xt+1, yt+1)
≤ −
=

βt+1E
c1ηtE
5L2
1ηt
4

k∇xf (xt, yt)
k∇xf (xt, yt)
E

−
k∇xf (xt, yt)

≤ −

−

−

−
2 + 2L2
utk
2 + 2L2
utk

k∇xf (xt, yt)
f η2
t /βt+1
f ηt/c1
8L2
f ηt
5L2

utk
−
˜xt+1 −
k
(cid:0)
xtk
˜xt+1 −
k
(cid:0)
xtk
˜xt+1 −
k

utk

2 +

2

(88)

xtk
2 +

2 +

2 +
˜yt+1 −
k
ytk
˜yt+1 −
k
ytk
˜yt+1 −
k

2

2

ytk

2

(cid:1)
+ c2

t+1σ2
+ β2
1η2
t σ2
t σ2
mη2
k2

,

(cid:1)

(cid:1)

+

1 (cid:0)

33

Feihu and Heng

4

where the above equality holds by βt+1 = c1ηt, and the last inequality is due to 5L2
k . Similarly, given 5L2
m1/2
k , we have
k∇yf (xt+1, yt+1)
5L2
4ηt
E
4

c2 ≤
ht+1k
−
k∇yf (xt, yt)

k∇yf (xt, yt)
8L2
f ηt
2 +
5L2

htk
−
˜xt+1 −
k

˜yt+1 −
k

−
htk

xtk

4 ≤

ytk

≤ −

2 +

m1/2

−

+

E

E

mη2
t σ2
k2

2

2

2

1

4 ≤

.

4 (cid:0)

(cid:1)

2

E

vt+1k
−
k∇yg(xt, yt)

k∇yg(xt, yt)
12µ2L2
2 +
vtk
125L2

−
gηt

−

−

2

vtk
˜xt+1 −
k

0 (cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

+

t σ2
mη2
k2

.

(cid:1)

m1/2

k , we have

Given 125L2

0

E

c3 ≤
6µ2 ≤
k∇yg(xt+1, yt+1)
125L2
0ηt
6µ2

E

≤ −

Given 5L2

2

m1/2

c4 ≤
4 ≤
2
xyg(xt+1, yt+1)
5L2
2ηt
4

k∇

E

E

k∇

≤ −

Given 5L2

3

m1/2

c5 ≤
4 ≤
2
yyg(xt+1, yt+1)
5L2
3ηt
4

k∇

E

E

k∇

≤ −

k , we have

k , we have

2

Gt+1k

−

E

k∇
2 +

−
Gtk

−

2
xyg(xt, yt)

2

2
xyg(xt, yt)
8L2
gxyηt
5L2
2

Gtk
−
˜xt+1 −
k
(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

2

Ht+1k

−

E

k∇
2 +

−
Htk

−

2
yyg(xt, yt)

2

2
yyg(xt, yt)
8L2
gyyηt
5L2
3

Htk
−
˜xt+1 −
k

(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

According to Lemmas 19 and 20, we have

F (xt+1)

F (xt)

−

≤

L2
0ηtγ
yt −
ρ
k
L2
2ηtγ
ρ
L2
4ηtγ
ρ

k∇

+

+

2 +
y∗(xt)
k

L2
1ηtγ
ρ

2
xyg(xt, yt)

Gtk

−

k∇xf (xt, yt)
L2
3ηtγ
2 +
ρ

k∇

2
yyg(xt, yt)

2

utk

−

(93)

k∇yf (xt, yt)

2

htk

−

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 22, we have

yt+1 −
k

2
y∗(xt+1)
k
− k
2
y∗(xt)
k

yt −

ηtµλ
4bt k

yt −

−

2
y∗(xt)
k
3ηt
˜yt+1 −
4 k

2 +

ytk

≤ −

Next, we deﬁne a Lyapunov function, for any t

25ηtλ
6µbt k∇yg(xt, yt)
1

≥

2 +

vtk

−

25κ2ηtbt

6µλ k

˜xt+1 −

2.
xtk

Φt = E

+

F (xt) +
(cid:2)
k∇yg(xt, yt)

5btL2
0γ
λµρ k
vtk

−

yt −
2 +

γ
2 +
y∗(xt)
ρ
k
2
xyg(xt, yt)

k∇

(cid:0)

k∇xf (xt, yt)
2 +
Gtk

k∇

−

2 +

utk
−
2
yyg(xt, yt)

k∇yf (xt, yt)
Htk

−

2

.

(cid:1)(cid:3)

2

htk

−

34

c1 ≤

(89)

(90)

(91)

+

t σ2
mη2
k2

.

(92)

+

t σ2
mη2
k2

.

(cid:1)

(cid:1)

2

Htk

−

(94)

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

2

E

5btL2
0γ
2
yt+1 −
y∗(xt+1)
λµρ
k
k
(cid:0)
2 + E
k∇yf (xt+1, yt+1)
−
k∇yg(xt, yt)
vt+1k
−
2 + E
2
yyg(xt+1, yt+1)
L2
1ηtγ
ρ

k∇
k∇xf (xt, yt)
L2
4ηtγ
2 +
ρ

Htk

−

−

2

ut+1k

−

2

E

yt −
− k
ht+1k
−
2 + E
vtk
−
2
Ht+1k
−
−
L2
2ηtγ
2 +
utk
ρ

2
y∗(xt)
k

E

+

γ
k∇xf (xt+1, yt+1)
ρ
(cid:1)
(cid:0)
htk
k∇yf (xt, yt)
2
xyg(xt+1, yt+1)
Gt+1k
k∇
−
E
2
Htk
yyg(xt, yt)

−

2

2

2

k∇
2
xyg(xt, yt)

−
Gtk

2

−

k∇

(cid:1)

25κ2ηtbt

6µλ k

˜xt+1 −

2

xtk

(cid:19)

2

ρηt
2γ k

˜xt+1 −

htk
−
25ηtλ
6µbt k∇yg(xt, yt)

2

xtk

2 +

ytk

−

2 +

vtk
mη2
t σ2
k2

2 +

xtk

˜yt+1 −
k

˜yt+1 −
k

ytk

2

(cid:1)
˜yt+1 −
k

2 +

xtk

2

+

ytk
(cid:1)
mη2
t σ2
k2

+

2

ytk

(cid:1)

2 +

2 +

˜yt+1 −
k

2

ytk

˜yt+1 −
k

2

ytk

+

+

(cid:1)

(cid:1)

+

mη2
t σ2
k2
mη2
t σ2
k2
mη2
t σ2
k2 (cid:19)

k∇yf (xt, yt)

−

3ηt
4 k

˜yt+1 −
8L2
f ηt
5L2

2 +

2 +

xtk

utk
˜xt+1 −
k
1 (cid:0)
8L2
f ηt
˜xt+1 −
5L2
k
4 (cid:0)
12µ2L2
gηt
125L2
gxyηt
5L2
2
gyyηt
5L2
3

˜xt+1 −
k
0 (cid:0)

˜xt+1 −
k

˜xt+1 −
k

8L2

8L2

(cid:0)

xtk

xtk

−

(cid:0)
k∇xf (xt, yt)
L2
5γ
ρ −
5mγσ2
k2ρ

ρ
2γ −

2 +

η2
t

utk
125b2

t κ2L2
0γ
6µ2λ2ρ

ηtk

˜xt+1 −

2

xtk

(cid:1)

2 + L2
4

E

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

Φt

Φt+1 −
= F (xt+1)

≤

F (xt) +

E

−

−
+ E
E

−
k∇xf (xt, yt)
utk
k∇yg(xt+1, yt+1)
−
2
Gtk
xyg(xt, yt)
−
k∇
−
L2
0ηtγ
2 +
yt −
y∗(xt)
ρ
k
k
L2
3ηtγ
ρ
5btL2
0γ
λµρ (cid:18) −
5L2
1ηt
4

ηtµλ
4bt k

2
yyg(xt, yt)

k∇

+

+

+

E

yt −

2
y∗(xt)
k

−

k∇xf (xt, yt)

−

γ
ρ (cid:18) −
5L2
4ηt
4
125L2
6µ2

E

0ηt

5L2
2ηt
4
5L2
3ηt
4
γηt
4ρ (cid:18)

−

−

−

−

=

−

k∇yf (xt, yt)

−

htk

2 +

E

k∇yg(xt, yt)

−

vtk

2 +

E

E

k∇

k∇

2
xyg(xt, yt)

2
yyg(xt, yt)

−

−

2 +

Gtk

2 +

Htk

L2
0k

yt −

2 + L2
y∗(xt)
1
k

E

+ L2
3

E

2
yyg(xt, yt)

k∇
15btL2
0γ
4λµρ −

−

≤ −

(cid:0)
γηt
4ρ (cid:18)

L2
0k

yt −

2

Htk

−

(cid:19) −

L2
5γ
ρ

ηtk

(cid:0)
ytk
˜yt+1 −
(cid:1)
E
2 + L2
y∗(xt)
1
k

+ L2
3

E

2
yyg(xt, yt)

2

Htk

−

k∇

(cid:19) −

k∇xf (xt, yt)
ρηt
4γ k

˜xt+1 −

2 + L2
4

utk

E

−

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

2 +

xtk

5mγσ2
k2ρ

η2
t ,

(95)

where the ﬁrst inequality holds by the above inequalities (88), (89), (90), (91), (92), (93)
and (94); the last inequality is due to 0 < γ

√6λµρ
5µ2λ2+125b2

t L2

0κ2

2√6L2

and 0 < λ

15bL2
0
4L2
5µ ≤

≤

15btL2
0
4L2

5µ and L2

5 =

8L2
f
5L2
1

+

√6λµρ
5µ2λ2+125ˆb2L2

q6L2

+

12µ2L2
g
125L2
0

+

8L2
gxy
5L2
2

≤
8L2
f
5L2
4

0κ2 ≤
8L2
gyy
5L2
3

+

.

35

Feihu and Heng

For simplicity, let Dt = L2
yt −
0k
2 + L2
2 + L2
2
Htk
yyg(xt, yt)

3k∇

−

Gtk

2 + L2
y∗(xt)
k
4k∇yf (xt, yt)
ρηt
4γ k

E[Dt]

−

γηt
4ρ

1k∇xf (xt, yt)
htk
−

utk
2. Then we have

−

2 + L2

2
xyg(xt, yt)

−

2k∇

˜xt+1 −

2 +

xtk

5mγσ2
k2ρ

η2
t .

(96)

Φt+1 −

Φt ≤ −

Taking average over t = 1, 2,

· · ·

, T on both sides of (96), we have

1
T

T

E

Xt=1

(cid:2)

ηt
4

Dt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:3)

Xt=1

Φt+1)

ρ(Φt −
T γ

+

1
T

T

Xt=1

5mσ2
k2 η2
t .

, let ∆0 =

y1 −
k

2, we have
y∗(x1)
k

Given x1 ∈ X

F (x1) +

and y1 ∈ Y
5b1L2
0γ
λµρ k
v1k
−
0γ∆0
+

(cid:2)
k∇xg(x1, y1)
5b1L2
λµρ

F (x1) +

Φ1 = E

+

≤

y1 −
2 +

γ
2 +
y∗(x1)
ρ
k
(cid:0)
2
xyg(x1, y1)

k∇
5γσ2
ρ

,

k∇xf (x1, y1)
2 +
G1k

k∇

−

2 +

u1k
−
2
yyg(x1, y1)

k∇yf (x1, y1)
H1k

−

2

(cid:1)(cid:3)

2

h1k

−

where the last inequality holds by Assumption 5. Since ηt is decreasing on t, i.e., η−
for any 0

T , we have

t

(97)

1

η−
t

1
T ≥

≤

≤

Dt
4

T

T

E

Xt=1

(cid:2)

ρ
T γηT

1
T

≤

≤

≤

≤

=

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

(cid:3)

Φt −

Xt=1 (cid:0)

F (x1) +

Φt+1

+

(cid:1)
5b1L2
0γ∆0
λµρ

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

F ∗)

F ∗)

+

+

5b1L2
0∆0
T ηT λµ
5b1L2
0∆0
T ηT λµ
5b1L2
kλµ

1
T ηT

T

5mσ2
k2 η2

t

Xt=1
5γσ2

+

+

+

1
T ηT

T

F ∗

+

(cid:1)
5mσ2
T ηT k2 Z
1
5mσ2
T ηT
5mσ2
k

ρ −

+

+

5σ2
T ηT
5σ2
T ηT
5σ2
k

T

5mσ2
k2 η2

t

Xt=1
k2
m + t

dt

ln(m + T )

F ∗)

−

+

0∆0

+

+

ln(m + T )

(m + T )1/2
T

,

(cid:19)

where the second inequality holds by the above inequality (97). Let G = ρ(F (x1)
−
kγ
5b1L2

kλµ + 5σ2
0∆0

k + 5mσ2

k

ln(m + T ), we have

1
T

T

E

Xt=1

(cid:2)

Dt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

G
T

≤

(cid:3)

(m + T )1/2.

36

(98)

F ∗)

+

BiAdam: Fast Adaptive Bilevel Optimization Methods

Since
L3k∇

Mt = ρ
γ k
2
yyg(xt, yt)

˜xt+1 −
Htk
−

yt −

+ L0k

xtk
+ L4k∇yf (xt, yt)

y∗(xt)
+ L1k∇xf (xt, yt)
k
htk
−

+
+ L2k∇
, according to the Jensen’s inequality, we have

Gtk

utk

2
xyg(xt, yt)

−

−

1
T

T

Xt=1

E[

1
2Mt]

6
T

≤ (cid:18)

T

Xt=1 (cid:0)

Dt
4

+

ρ2
4γ2 k

2

1/2

(cid:19)

(cid:1)

xtk
˜xt+1 −
√6Gm1/4
T 1/2

√6G
T 1/2

≤

(m + T )1/4

≤

+

√6G
T 1/4

,

(99)

where the last inequality is due to (a + b)1/4

a1/4 + b1/4 for all a, b > 0. Thus we obtain

≤

1
T

T

Xt=1

E[

Mt]

≤

2√6Gm1/4
T 1/2

+

2√6G
T 1/4

.

(100)

A.2 Convergence Analysis of VR-BiAdam Algorithm

In the subsection, we provide the detail convergence analysis of our VR-BiAdam algorithm.

Lemma 26 Assume that the stochastic partial derivatives ut+1, ht+1, vt+1, Gt+1 and Ht+1
be generated from Algorithm 2, we have

E

k∇xf (xt+1, yt+1)

−

2

ut+1k

≤

E

k∇yf (xt+1, yt+1)

−

2

ht+1k

≤

E

k∇yg(xt+1, yt+1)

−

2

vt+1k

≤

E

k∇

2
xyg(xt+1, yt+1)

2

Gt+1k

−

≤

(1

−
+ 4(1

βt+1)2E

k∇xf (xt, yt)
utk
−
f η2
˜xt+1 −
t
k

E

2 + 2β2
2 + E
xtk

βt+1)2L2

−

t+1σ2

˜yt+1 −
k

2

ytk

.
(cid:1)

(1

−
+ 4(1

ˆβt+1)2E

k∇yf (xt, yt)
htk
−
f η2
˜xt+1 −
t
k

E

2 + 2 ˆβ2
2 + E
xtk

ˆβt+1)2L2

−

t+1σ2

˜yt+1 −
k

2

ytk

.
(cid:1)

(1

−
+ 4(1

αt+1)2E

vtk
k∇yg(xt, yt)
−
gη2
˜xt+1 −
t
k

E

2 + 2α2
2 + E
xtk

αt+1)2L2

−

t+1σ2

˜yt+1 −
k

2

ytk

.

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(1

−
+ 4(1

ˆαt+1)2E

k∇
ˆαt+1)2L2

2
xyg(xt, yt)
gxyη2
t

E

Gtk
−
˜xt+1 −
k

2 + 2ˆα2
2 + E
xtk

t+1σ2

−

˜yt+1 −
k

ytk

(cid:0)

2

E

−

(1

k∇

Ht+1k

2
yyg(xt+1, yt+1)

Htk
−
˜xt+1 −
k
Proof Without loss of generality, we only consider the term E
The other terms are similar for this term. Since ht+1 = ΠCf y

k∇
˜αt+1)2L2

2
yyg(xt, yt)
gyyη2
t

−
+ 4(1

˜αt+1)2E

−

≤

E

(cid:0)

2 + 2˜αt+1σ2
2 + E
xtk

.

ytk
˜yt+1 −
k
(cid:1)
2.
k∇yf (xt+1, yt+1)
ht+1k
∇yf (xt+1, yt+1; ξt+1) + (1
−

−

(cid:2)

37

(101)

(102)

(103)

(104)
2

.
(cid:1)

(105)
2

Feihu and Heng

, we have

−

ΠCf y
− ∇yf (xt+1, yt+1; ξt+1)

ht) + ˆβt+1(

ˆβt+1)
(cid:0)

E

(cid:2)

−

(cid:1)(cid:3)
2

≤
= E

ht − ∇yf (xt, yt; ξt+1)
ht+1k
−
∇yf (xt+1, yt+1)
(cid:3)

k∇yf (xt+1, yt+1)
= E
ΠCf y
k
(cid:2)
E
k∇yf (xt+1, yt+1)
ˆβt+1)(
(1
∇yf (xt, yt)
k
ˆβt+1)
(1
∇yf (xt+1, yt+1; ξt+1)
−
(cid:0)
ˆβt+1)2E
k∇yf (xt, yt)
ˆβt+1)
∇yf (xt+1, yt+1; ξt+1)
−
(cid:0)
ˆβt+1)2E
k∇yf (xt, yt)
−
k∇yf (xt+1, yt+1; ξt+1)
k∇yf (xt, yt)
−
k∇yf (xt, yt)

htk
htk

−
+ 2(1

−
= (1

ˆβt+1)2

htk

htk

−
(1

−
(1

2 + E

(1

−

−

−

−

≤

≤

≤

−
ˆβt+1)2E
ˆβt+1)2E
t+1σ2,

(1
−
+ 2 ˆβ2

2 + 2 ˆβ2
2 + 4(1

(106)
ˆβt+1)
∇yf (xt+1, yt+1; ξt+1) + (1
ht − ∇yf (xt, yt; ξt+1)
(cid:0)
ˆβt+1)
2
ht − ∇yf (xt, yt; ξt+1)
(1
−
k
(cid:1)
(cid:0)
− ∇yf (xt+1, yt+1; ξt+1))
∇yf (xt+1, yt+1)
∇yf (xt+1, yt+1)
(
− ∇yf (xt, yt; ξt+1)
−

− ∇yf (xt, yt))
(cid:1)

−

−

2
k

∇yf (xt+1, yt+1)

− ∇yf (xt+1, yt+1; ξt+1)
(cid:1)

2
k

(cid:1)(cid:3)

∇yf (xt+1, yt+1)
(

−

− ∇yf (xt, yt))
(cid:1)

2
k

(cid:3)

t+1

∇yf (xt+1, yt+1)
− ∇
∇yf (xt+1, yt+1)
(
k∇yf (xt+1, yt+1; ξt+1)
2 + E
˜yt+1 −
k

2
f (xt+1, yt+1; ξt+1)
k
2
− ∇yf (xt, yt))
k
2
− ∇yf (xt, yt; ξt+1)
k
ytk

− ∇yf (xt, yt; ξt+1)
t+1σ2 + 2(1
−
ˆβt+1)2L2
f η2
t

−
ˆβt+1)2
E

˜xt+1 −
k

xtk

−

2

(cid:0)

(cid:1)

ˆβt+1
k
(cid:2)
(cid:0)
− ∇yf (xt, yt; ξt+1)
k
(cid:0)

E

2 + 2 ˆβ2

∇

f (xt+1, yt+1; ξt+1)

where the fourth equality is due to Eξt+1[
∇
Eξt+1[
f (xt+1, yt+1)
− ∇
2 = E
E[ζ]
equality holds by Assumption 3 and the inequality E
2
ζ
k
k
k
−
and the last inequality holds by Assumption 2 and xt+1 = xt −
ηt(˜xt+1 −
yt −

f (xt+1, yt+1) and
f (xt, yt); the second in-
(E[ζ])2
E
2,
k
xt), yt+1 =

f (xt+1, yt+1; ξt+1)] =

f (xt, yt; ξt+1)] =

ηt(˜yt+1 −

− ∇

yt).

ζ
k

ζ
k

∇

∇

−

≤

k

Theorem 27 Under the above Assumptions (1, 3, 4, 5, 8), in the Algorithm 2, given
t , ˆαt+1 = c4η2
t ,
Bt = Ip, ηt =
2
˜αt+1 = c5η2
3k3 +
5L2
1
4 , c2 ≥
min
2√24L2

(m+t)1/3 for all t
2, k3, (c1k)3, (c2k)3, (c3k)3, (c4k)3, (c5k)3
, k > 0, c1 ≥
t , m
max
≥
3k3 + 5L2
3k3 + 5L2
(cid:0)
(cid:1)
2
4 , c5 ≥
, c4 ≥
4 , c3 ≥
0κ2 , m1/3ρ
min
≤

3k3 + 125L2
and 0 < λ

3k3 + 5L2
15L2
6µ , 1
0
16L2

√6λµρ
6λ2µ2+125L2

t , ˆβt+1 = c2η2

t , αt+1 = c3η2

0, βt+1 = c1η2

4 , 0 < γ

, we have

6Lg

4Lk

≥

≤

2

6

2

2

4

0

3

2

(cid:0)

(cid:1)

(cid:0)

(cid:1)
T

Xt=1

1
T

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3

,

(107)

F ∗)

where M = ρ(F (x1)
−
kγ
6 = 2L2
2 and L2
y∗(x1)
k

+ 5L2
f + L2

kλµ + 5m1/3σ2
0∆0
gxy + L2
g + L2

gyy.

k2 + 2k2 ˆCσ2 ln(m + T ), ˆC =

5
i=1 c2

i , ∆0 =

y1 −
k

P

Proof Here we consider convergence properties of our VR-BiAdam algorithm with the
ρId for variable x and the non-adaptive matrix Bt = Ip for
general adaptive matrix At ≻
1
variable y. Since ηt =
m1/3ρ
4Lk ≤

k3, we have ηt ≤
1 and m
≥

η0 = k
(c1k)3, we have

(m+t)1/3 on t is decreasing and m

0. Due to 0 < ηt ≤

for any t

m1/3 ≤

2Lη0 ≤

and γ

ρ
2Lηt

≥

≥

≤

k

ρ

38

E

1
ηt

≤

(cid:0)

E

1
ηt

≤

(cid:0)

E

1
ηt

≤

(cid:0)

E

1
ηt

BiAdam: Fast Adaptive Bilevel Optimization Methods

c1ηt ≤
1, αt+1 ≤

βt+1 = c1η2
t ≤
we have ˆβt+1 ≤
1
ηt

E

k∇xf (xt+1, yt+1)
1
1
ηt
ηt −

c1ηt

1 −

−

−

E

(cid:1)

≤

(cid:0)

c1k
m1/3 ≤

1. Similarly, due to m
1, and ˜αt+1 ≤
E
k∇xf (xt, yt)
2 + 4L2

1, ˆαt+1 ≤
1
ηt
k∇xf (xt, yt)

ut+1k

utk

f ηt

−

−

−

2

1

max

(c2k)3, (c3k)3, (c4k)3, (c5k)3
(cid:0)

1. According to Lemma 26, we have

≥

,

(cid:1)

2

utk

(108)

−

˜xt+1 −
k

(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

1η3

t σ2,

where the above inequality is due to 0 < βt+1 = c1η2
26, we can obtain

t ≤

1. Similarly, according to Lemma

k∇yf (xt+1, yt+1)
1
1
ηt
ηt −

c2ηt

1 −

−

k∇yg(xt+1, yt+1)
1
1
ηt
ηt −

c3ηt

1 −

−

−

E

(cid:1)

−

E

(cid:1)

E

1

2

−

ht+1k

1
ηt
k∇yf (xt, yt)

−

htk

−

k∇yf (xt, yt)
2 + 4L2

f ηt

2

htk

−

(109)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

2η3

t σ2.

(cid:0)

E

1

2

−

vt+1k

1
ηt
k∇yg(xt, yt)

−

vtk

−

k∇yg(xt, yt)
2 + 4L2

gηt

2

vtk

−

(110)

˜xt+1 −
k
(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

3η3

t σ2.

2
xyg(xt+1, yt+1)

k∇
1
ηt −

1
ηt

−

1 −

c4ηt

E

(cid:1)

k∇

2

Gt+1k

−

−

1
ηt

−
2
xyg(xt, yt)

2

Ht+1k

−

−

1
ηt

−
2
yyg(xt, yt)

2
yyg(xt+1, yt+1)

k∇
1
ηt −

≤

(cid:0)
By ηt =

1
ηt

−
k

1 −

c5ηt

E

(cid:1)

k∇

(m+t)1/3 , we have

E

1

k∇

2
xyg(xt, yt)

2

Gtk

−

(111)

2 + 4L2

gxyηt

Gtk

˜xt+1 −
k
(cid:0)

2 +

xtk

˜yt+1 −
k

2

ytk

−

E

1

k∇

2
yyg(xt, yt)

2

Htk

−

−

2 + 4L2

gyyηt

Htk

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

+ 2c2

4η3

t σ2.

(cid:1)

(cid:1)

(112)

+ 2c2

5η3

t σ2.

1
ηt −

1
ηt

−

1

=

1
k

(cid:0)

(m + t)

1
3

(m + t

−

1
3

1)

−

1
3k(m + t

−

≤

(cid:1)

1

1)2/3 ≤

3k

m/2 + t

22/3
3k(m + t)2/3

=

22/3
3k3

k2
(m + t)2/3

=

22/3
3k3 η2

t ≤

2
3k3 ηt,

≤

(cid:0)

2/3

(cid:1)

(113)

2

3k3 + 5L2

where the ﬁrst inequality holds by the concavity of function f (x) = x1/3, i.e., (x + y)1/3
x1/3 + y
0 < ηt ≤
1
E
ηt

3x2/3 ; the second inequality is due to m
1. Let c1 ≥
k∇xf (xt+1, yt+1)
5L2
1ηt
E
4

4 , we have
1
ηt

≤
2, and the last inequality is due to

k∇xf (xt, yt)

k∇xf (xt, yt)

−
2 + 4L2

˜xt+1 −
k

ut+1k

+ 2c2

t σ2.

utk

utk

(114)

1η3

≤ −

f ηt

−

−

≥

−

−

E

2

2

1

1

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

(cid:0)

39

Feihu and Heng

2

3k3 + 5L2

4

4 , we have

Let c2 ≥
1
E
ηt

Let c3 ≥
1
E
ηt

Let c4 ≥
1
E
ηt

≤ −

Let c5 ≥
1
E
ηt

k∇yf (xt+1, yt+1)
5L2
4ηt
E
4
3k3 + 125L2

6

2

0

, we have

≤ −

k∇yf (xt, yt)

htk

−

−
2 + 4L2

f ηt

k∇yg(xt+1, yt+1)
125L2
0ηt
E
6
3k3 + 5L2

2

2

4 , we have

≤ −

k∇yg(xt, yt)

vtk

−

1
ηt

1

E

−
2 + 4L2

gηt

2

ht+1k

−

−

1
ηt

1

E

k∇yf (xt, yt)

−

2

htk

(115)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

+ 2c2

2η3

t σ2.

2

vt+1k

−

−

k∇yg(xt, yt)

2

vtk

−

(116)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

+ 2c2

3η3

t σ2.

2
xyg(xt+1, yt+1)

2

Gt+1k

−

−

1
ηt

1

E

2
xyg(xt, yt)

2

Gtk

−

k∇

(117)

E

k∇
5L2
2ηt
4
k∇
3k3 + 5L2

2

3

4 , we have

2
xyg(xt, yt)

−

Gtk

−
2 + 4L2

gxyηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

2
yyg(xt+1, yt+1)

2

Ht+1k

−

−

1
ηt

1

E

2
yyg(xt, yt)

2

Htk

−

k∇

k∇
5L2
3ηt
4

≤ −

E

k∇

2
yyg(xt, yt)

−

Htk

−
2 + 4L2

gyyηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

According to Lemmas 19 and 20, we have

+ 2c2

4η3

t σ2.

(cid:1)

(cid:1)

(118)

+ 2c2

5η3

t σ2.

F (xt+1)

F (xt)

−

≤

L2
0ηtγ
yt −
ρ
k
L2
2ηtγ
ρ
L2
4ηtγ
ρ

k∇

+

+

2 +
y∗(xt)
k

L2
1ηtγ
ρ

2
xyg(xt, yt)

Gtk

−

k∇xf (xt, yt)
L2
3ηtγ
2 +
ρ

k∇

2

utk

−

(119)

2
yyg(xt, yt)

2

Htk

−

k∇yf (xt, yt)

2

htk

−

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 21, we have

yt+1 −
k

2
y∗(xt+1)
k

yt −

− k

2
y∗(xt)
k

≤ −

+

ηtµλ

2
yt −
y∗(xt)
4 k
k
25ηtλ
6µ k∇yg(xt, yt)

−
vtk

−

3ηt
4 k

2 +

2

ytk

˜yt+1 −
25κ2ηt
6µλ k

˜xt+1 −

(120)

2.
xtk

Next, we deﬁne a useful Lyapunov function, for any t

1

≥

Ωt = E

+

5L2
0γ
λµρ k

F (xt) +
(cid:2)
k∇yg(xt, yt)

−

yt −
2 +
vtk

2 +
y∗(xt)
k

γ
ρηt
−
2
xyg(xt, yt)

k∇

k∇xf (xt, yt)
1 (cid:0)
2 +
Gtk
−

utk
−
2
yyg(xt, yt)

k∇

2 +

2

htk

−

k∇yf (xt, yt)
.
Htk

2

(cid:1)(cid:3)

−

40

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

Ωt

Ωt+1 −
= F (xt+1)

1

E

+

+

−

+

k∇

k∇

−
E

1
ηt
1
ηt
1
ηt
−
1
−
L2
0ηtγ
yt −
ρ
k
L2
3ηtγ
ρ
5L2
0γ
λµρ (cid:18) −
5L2
γ
1ηt
ρ (cid:18) −
4
5L2
4ηt
4
125L2
6µ2
5L2
2ηt
4
5L2
3ηt
4
γηt
4ρ (cid:18)

L2
0k

0ηt

k∇

k∇

−

+

−

−

−

−

E

E

E

≤

=

5L2
0γ
λµρ

F (xt) +

−
k∇xf (xt, yt)

−

k∇yg(xt+1, yt+1)
E

2
xyg(xt, yt)

−

yt+1 −
k
(cid:0)
1
E
2 +
utk
ηt
vt+1k
2 +

2

−

−
1
ηt

Gtk
L2
1ηtγ
ρ

− k

2
yt −
y∗(xt+1)
k
k∇yf (xt+1, yt+1)
1
ηt

k∇yg(xt, yt)

E

1

−

2
y∗(xt)
k
ht+1k

2

(cid:1)

−

2 +

vtk

−

+

γ
ρ
1
ηt
1
ηt

−
E

E

−

k∇

2
yyg(xt+1, yt+1)

2

Ht+1k

−

−
L2
2ηtγ
ρ

2 +

utk

−

2 +
y∗(xt)
k

k∇xf (xt, yt)
L2
4ηtγ
2 +
ρ
3ηt
4 k

−

−

k∇yf (xt, yt)
2 +

˜yt+1 −

ytk

2

htk
25ληt
6µ

2
yyg(xt, yt)

Htk

−

ηtµλ

yt −

2
y∗(xt)
k

4 k

2
xyg(xt, yt)

2

Gtk

−

k∇

ρηt
2γ k

˜xt+1 −

2

xtk

−

E

1
ηt
k∇yf (xt, yt)

k∇xf (xt+1, yt+1)
htk

−

2

(cid:0)
E

1

2

ut+1k

−

2
xyg(xt+1, yt+1)

2

Gt+1k

−

E

k∇

2
yyg(xt, yt)

2

Htk

−

(cid:1)

k∇
1
ηt

−

1

E

k∇yg(xt, yt)

−

2 +

vtk

25κ2ηt
6µλ k

˜xt+1 −

2

xtk

(cid:19)

E

k∇xf (xt, yt)

−

utk

2 + 4L2

2 +

xtk

˜yt+1 −
k

ytk

2

+ 2c2

1η3

t σ2

k∇yf (xt, yt)
E

−
k∇yg(xt, yt)

htk
vtk

−

2 + 4L2

f ηt

2 + 4L2

f ηt

˜xt+1 −
k
(cid:0)
˜xt+1 −
k
(cid:0)
˜xt+1 −
gηt
k

xtk
xtk

2 +

2

ytk
˜yt+1 −
k
(cid:1)
2
ytk
˜yt+1 −
k

2 +

(cid:1)
2η3
+ 2c2

t σ2

2
xyg(xt, yt)

2
yyg(xt, yt)

−

Gtk
Htk
E
2 + L2
y∗(xt)
1
k

−

yt −

(cid:0)
gxyηt

2 + 4L2

2 + 4L2

(cid:0)

gyyηt

˜xt+1 −
k
˜xt+1 −
k
(cid:0)
k∇xf (xt, yt)
4L2
6γ
ρ −

ρ
2γ −

−

2 +

xtk
xtk
2 + L2
4

2 +

utk
125κ2L2
0γ
6µ2λ2ρ

+ 2c2

3η3

t σ2

+ 2c2

4η3

t σ2

(cid:1)

+ 2c2

5η3

t σ2

(cid:19)

(cid:1)
2
ytk
ytk

2

˜yt+1 −
k
˜yt+1 −
k

(cid:1)
k∇yf (xt, yt)

−

E

ηtk

˜xt+1 −

2

xtk

(cid:1)

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

2

Htk

−

(cid:19) −

+ L2
3

E

2
yyg(xt, yt)

k∇
15L2
0γ
4λµρ −

4L2
6γ
ρ

−

≤ −

(cid:0)
γηt
4ρ (cid:18)

L2
0k

yt −

ηtk

˜yt+1 −
(cid:1)
2 + L2
y∗(xt)
1
k

(cid:0)
2 +
ytk
E

2 ˆCγσ2
ρ

η3
t

+ L2
3

E

k∇

2
yyg(xt, yt)

2

Htk

−

(cid:19) −

k∇xf (xt, yt)
ρηt
4γ k

˜xt+1 −

2 + L2
4

utk

E

−

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

2 +

xtk

2 ˆCγσ2
ρ

η3
t ,

(121)

where the ﬁrst inequality holds by the above inequalities (114), (115), (116), (117), (118),
15L2
0
(119) and (120); the last inequality is due to 0 < γ
16L2
6µ

√6λµρ
6λ2µ2+125L2

0κ2 , 0 < λ

2√24L2

≤

≤

and L2

6 = 2L2

f + L2

g + L2

gxy + L2

gyy.

41

Feihu and Heng

For simplicity, let Dt = L2
yt −
0k
2 + L2
2 + L2
2
Htk
yyg(xt, yt)

3k∇

2 + L2
y∗(xt)
k
4k∇yf (xt, yt)

−

Gtk

utk
2. Then we have

−

2 + L2

2k∇

1k∇xf (xt, yt)
htk
−
ρ(Ωt −
γ

Ωt+1)

+ 2 ˆCσ2η3
t .

2
xyg(xt, yt)

−

(122)

ηt
4

E[Dt] +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

≤

Taking average over t = 1, 2,

· · ·

, T on both sides of (122), we have

1
T

T

E

Xt=1

(cid:2)

ηt
4

Dt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:3)

Xt=1

Ωt+1)

ρ(Ωt −
T γ

+

2 ˆCσ2
T

T

Xt=1

η3
t .

Given x1 ∈ X

, let ∆0 =

y1 −
k

2, we have
y∗(x1)
k

and y1 ∈ Y
5L2
0γ
λµρ k

Ω1 = E

+

F (x1) +
(cid:2)
k∇yg(x1, y1)

F (x1) +

≤

5L2

−
0γ∆0
λµρ

y1 −
2 +
v1k
5γσ2
+
ρη0

,

2 +
y∗(x1)
k

γ
ρη0 (cid:0)
2
xyg(x1, y1)
−

k∇

k∇xf (x1, y1)
2 +
G1k

k∇

2 +

u1k
−
2
yyg(x1, y1)

2

h1k

−

k∇yf (x1, y1)
H1k

−

2

(cid:1)(cid:3)

where the last inequality holds by Assumption 2. Since ηt is decreasing, i.e., η−
any 0

T , we have

t

1
T ≥

≤

≤

T

E

Xt=1

(cid:2)

ρ
T γηT

1
T

≤

≤

≤

≤

=

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

2

xtk
(cid:3)
2 ˆCσ2
T ηT

+

ρ2
4γ2 k

˜xt+1 −

Dt
4

T

Ωt −

Xt=1 (cid:0)

F (x1) +

Ωt+1

+

5L2

(cid:1)
0γ∆0
λµρ

T

Xt=1

η3
t

+

5γσ2
ρη0 −

F ∗

+

5σ2
T ηT η0
5σ2
T ηT η0

+

+

F ∗)

F ∗)

+

+

F ∗)

−

+

+

+

5L2
0∆0
T ηT λµ
5L2
0∆0
T ηT λµ
5L2
0∆0
kλµ

2 ˆCσ2
T ηT

T

(cid:1)
2 ˆCσ2
T ηT Z
1
2k3 ˆCσ2
T ηT

T

η3
t

Xt=1
k3
m + t

dt

ln(m + T )

+

5m1/3σ2
k2

+ 2k2 ˆCσ2 ln(m + T )

(cid:19)

(m + T )1/3
T

,

where the second inequality holds by the above inequality (137). Let M = ρ(F (x1)
−
kγ
5L2
kλµ + 5m1/3σ2
0∆0

k2 + 2k2 ˆCσ2 ln(m + T ), we have

1
T

T

E

Xt=1

(cid:2)

Dt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

M
T

≤

(cid:3)

(m + T )1/3.

42

(123)

1

η−
t

for

(124)

F ∗)

+

BiAdam: Fast Adaptive Bilevel Optimization Methods

Since
L3k∇

Mt = ρ
γ k
2
yyg(xt, yt)

+
+ L2k∇
, according to the Jensen’s inequality, we have

Gtk

2
xyg(xt, yt)

utk

−

−

˜xt+1 −
Htk
−
1
T

T

yt −

+ L0k

xtk
+ L4k∇yf (xt, yt)
6
T

y∗(xt)
+ L1k∇xf (xt, yt)
k
htk
−
T
Dt
4

1
2Mt]

ρ2
4γ2 k

≤ (cid:18)

E[

+

Xt=1

Xt=1 (cid:0)

√6M
T 1/2

≤

(m + T )1/6

≤

2

xtk
˜xt+1 −
√6M m1/6
T 1/2

1/2

(cid:19)

(cid:1)

+

√6M
T 1/3

,

(125)

where the last inequality is due to (a + b)1/6

a1/6 + b1/6 for all a, b > 0. Thus we obtain

≤

1
T

T

Xt=1

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3

.

(126)

k

≥

Theorem 28 Under the above Assumptions (1, 3, 4, 5, 8), in the Algorithm 2, given Bt =
btIp (ˆb
t , αt+1 = c3η2
b > 0), ηt =
bt ≥
t ,
ˆαt+1 = c4η2
t , ˜αt+1 = c5η2
, k > 0,
3k3 + 5L2
(cid:1)
2
2
4 ,
4 , c2 ≥
c1 ≥
0 < γ
2q24L2

2, k3, (c1k)3, (c2k)3, (c3k)3, (c4k)3, (c5k)3
3k3 + 125L2
4 , c5 ≥
6
and 0 < λ

(m+t)1/3 for all t
t , m
max
≥
3k3 + 5L2
(cid:0)
4
4 , c3 ≥
, m1/3ρ
4Lk
0κ2

3k3 + 5L2
15bL2
0
6µ ,
16L2

3k3 + 5L2
min

, c4 ≥
min
≤

t , ˆβt+1 = c2η2

0, βt+1 = c1η2

6λ2µ2+125ˆb2L2

√6λµρ

b
6Lg

≥

≤

2

2

2

3

1

0

2

, we have
(cid:1)

(cid:0)

(cid:1)

(cid:0)

T

1
T

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3

,

(127)

F ∗)

Xt=1
+ 5L2
kλµ + 5m1/3σ2
0b1∆0
k2
f + L2
g + L2
gxy + L2

6 = 2L2

where M = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k
Proof Here we consider convergence properties of our VR-BiAdam algorithm with the
general adaptive matrix At ≻
ρId for variable x and the global adaptive matrix Bt =
btIp (ˆb
b > 0) for variable y. This proof basically follows the proof of Theorem 27.
3k3 + 5L2
2

+ 2k2 ˆCσ2 ln(m + T ), ˆC =
gyy.

4 , we have

i , ∆0 =

5
i=1 c2

P

1

k∇xf (xt+1, yt+1)
5L2
1ηt
E
4
3k3 + 5L2

2

4

4 , we have

k∇xf (xt, yt)

≤ −

2

ut+1k

−

−

1
ηt

1

−
2 + 4L2

f ηt

utk

−

E

k∇xf (xt, yt)

−

2

utk

(128)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

bt ≥
≥
Let c1 ≥
1
E
ηt

Let c2 ≥
1
E
ηt

2

ht+1k

−

−

1
ηt

1

E

k∇yf (xt, yt)

−

2

htk

k∇yf (xt+1, yt+1)
5L2
4ηt
E
4

k∇yf (xt, yt)

≤ −

−
2 + 4L2

f ηt

htk

−

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

+ 2c2

1η3

t σ2.

(cid:1)

(cid:1)

(129)

+ 2c2

2η3

t σ2.

43

Let c5 ≥

2

3k3 + 5L2

3

4 , we have

E

1
ηt

k∇
5L2
2ηt
4

≤ −

E

1
ηt

k∇
5L2
3ηt
4

≤ −

Feihu and Heng

, we have

Let c3 ≥

2

3k3 + 125L2

6

0

E

1
ηt

≤ −

k∇yg(xt+1, yt+1)
125L2
0ηt
E
6

k∇yg(xt, yt)

vtk

−

1
ηt

1

E

−
2 + 4L2

gηt

2

vt+1k

−

−

k∇yg(xt, yt)

2

vtk

−

(130)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

+ 2c2

3η3

t σ2.

Let c4 ≥

2

3k3 + 5L2

2

4 , we have

2
xyg(xt+1, yt+1)

2

Gt+1k

−

−

1
ηt

1

E

2
xyg(xt, yt)

2

Gtk

−

k∇

(131)

E

k∇

2
xyg(xt, yt)

−

Gtk

−
2 + 4L2

gxyηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

+ 2c2

4η3

t σ2.

(132)

+ 2c2

5η3

t σ2.

(cid:1)

(cid:1)

2

Htk

−

(134)

25κ2ηtbt

6µλ k

˜xt+1 −

2.
xtk

2
yyg(xt+1, yt+1)

2

Ht+1k

−

−

1
ηt

1

E

2
yyg(xt, yt)

2

Htk

−

k∇

E

k∇

2
yyg(xt, yt)

−

Htk

−
2 + 4L2

gyyηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

According to Lemmas 19 and 20, we have

F (xt+1)

F (xt)

−

≤

L2
0ηtγ
yt −
ρ
k
L2
2ηtγ
ρ
L2
4ηtγ
ρ

k∇

+

+

2 +
y∗(xt)
k

L2
1ηtγ
ρ

2
xyg(xt, yt)

Gtk

−

k∇xf (xt, yt)
L2
3ηtγ
2 +
ρ

k∇

2
yyg(xt, yt)

2

utk

−

(133)

k∇yf (xt, yt)

2

htk

−

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 22, we have

yt+1 −
k

2
y∗(xt+1)
k

yt −

− k

ηtµλ
4bt k

≤ −

yt −

2
y∗(xt)
k

−

2
y∗(xt)
k
3ηt
˜yt+1 −
4 k

2 +

ytk

25ηtλ
6µbt k∇yg(xt, yt)

2 +

vtk

−

Next, we deﬁne a useful Lyapunov function, for any t

1

≥

Ωt = E

+

F (xt) +
(cid:2)
k∇yg(xt, yt)

5btL2
0γ
λµρ k
vtk

−

yt −
2 +

γ
ρηt

2 +
y∗(xt)
k
2
xyg(xt, yt)

k∇

−

1 (cid:0)
Gtk

−

2 +

k∇xf (xt, yt)
2 +

utk
−
2
yyg(xt, yt)

k∇

−

k∇yf (xt, yt)
.
Htk

2

(cid:1)(cid:3)

2

htk

−

44

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

Ωt

Ωt+1 −
= F (xt+1)

1

E

−

+

k∇

−
E

1
ηt
1
ηt
1
ηt
−
1
−
L2
0ηtγ
yt −
ρ
k
L2
3ηtγ
ρ
5btL2
0γ
λµρ (cid:18) −
5L2
1ηt
4

k∇

+

+

+

≤

γ
ρ (cid:18) −
5L2
4ηt
4
125L2
6µ2
5L2
2ηt
4
5L2
3ηt
4
γηt
4ρ (cid:18)

−

−

−

−

=

−

5btL2
0γ
λµρ

F (xt) +

−
k∇xf (xt, yt)

−

k∇xg(xt+1, yt+1)
E

2
xyg(xt, yt)

−

Gtk
L2
1ηtγ
ρ

2 +
y∗(xt)
k

yt+1 −
k
(cid:0)
1
E
2 +
utk
ηt
vt+1k
2 +

2

−

−
1
ηt

E

1
ηt

−

k∇

2
y∗(xt+1)
k
− k
k∇yf (xt+1, yt+1)

yt −

2
y∗(xt)
k

2

ht+1k

−

−

k∇xf (xt+1, yt+1)

2

ut+1k

−

γ
ρ

E

1
ηt
(cid:0)
E
k∇yf (xt, yt)

1

2

htk

−

E

1

k∇xg(xt, yt)

−

2 +

vtk

2
xyg(xt+1, yt+1)

2

Gt+1k

−

2
yyg(xt+1, yt+1)

2

Ht+1k

−

E

k∇

2
yyg(xt, yt)

2

Htk

−

(cid:1)

−
L2
2ηtγ
ρ

2 +

utk

−

2
xyg(xt, yt)

2

Gtk

−

k∇

+

(cid:1)

1
ηt
1
ηt

−
E

k∇
1
ηt

−

1

k∇xf (xt, yt)
L2
4ηtγ
2 +
ρ

2
yyg(xt, yt)

Htk

−

k∇yf (xt, yt)

−

ηtµλ
4bt k

yt −

2
y∗(xt)
k

−

3ηt
4 k

˜yt+1 −

2 +

ytk

2

xtk

2

ρηt
2γ k

˜xt+1 −

htk
−
25ηtλ
6µbt k∇yg(xt, yt)
2 +

2

˜yt+1 −
k

xtk

ytk

2 +

vtk

−

25κ2ηtbt

6µλ k

˜xt+1 −

2

xtk

(cid:19)

+ 2c2

1η3

t σ2

E

k∇xf (xt, yt)

−

utk

2 + 4L2

2 + 4L2

f ηt

E

0ηt

k∇yf (xt, yt)
E

−
k∇yg(xt, yt)

htk
vtk

−

f ηt

˜xt+1 −
k
(cid:0)
˜xt+1 −
k
(cid:0)
˜xt+1 −
gηt
k

xtk
xtk

2 +

2

ytk
˜yt+1 −
k
(cid:1)
2
ytk
˜yt+1 −
k

2 +

(cid:1)
2η3
+ 2c2

t σ2

+ 2c2

3η3

t σ2

+ 2c2

4η3

t σ2

(cid:1)

+ 2c2

5η3

t σ2

(cid:19)

(cid:1)
2
ytk
ytk

2

2 + 4L2

(cid:0)
gxyηt

2 + 4L2

2 + 4L2

(cid:0)

gyyηt

˜xt+1 −
k
˜xt+1 −
k
(cid:0)
k∇xf (xt, yt)
4L2
6γ
ρ −
2 ˆCγσ2
ρ

ρ
2γ −

(cid:0)
ytk

2 +

η3
t

−

˜yt+1 −
k
˜yt+1 −
k

2 +

xtk
xtk
2 + L2
4

2 +

utk
125b2

E

(cid:1)
k∇yf (xt, yt)
t κ2L2
0γ
6µ2λ2ρ

˜xt+1 −

ηtk
(cid:1)

2

xtk

E

E

k∇

k∇

2
xyg(xt, yt)

2
yyg(xt, yt)

−

Gtk
Htk
E
2 + L2
y∗(xt)
1
k

−

L2
0k

yt −

+ L2
3

E

2
yyg(xt, yt)

2

Htk

−

(cid:19) −

k∇
15btL2
0γ
4λµρ −

−

≤ −

(cid:0)
γηt
4ρ (cid:18)

L2
0k

yt −

4L2
6γ
ρ

˜yt+1 −
ηtk
(cid:1)
E
2 + L2
y∗(xt)
1
k

+ L2
3

E

k∇

2
yyg(xt, yt)

2

Htk

−

(cid:19) −

k∇xf (xt, yt)
ρηt
4γ k

˜xt+1 −

2 + L2
4

utk

E

−

k∇yf (xt, yt)

−

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

2 +

xtk

2 ˆCγσ2
ρ

η3
t ,

(135)

2 + L2
2

htk

E

k∇

2
xyg(xt, yt)

2

Gtk

−

−

where the ﬁrst inequality holds by the above inequalities (128), (129), (130), (131), (132),
(133) and (134); the last inequality is due to 0 < γ

√6λµρ

√6λµρ
6λ2µ2+125b2

t L2

0κ2

2√6L2

and 0 < λ

15bL2
0
16L2
6µ ≤

≤

15btL2
0
16L2

6µ for all t

≥

1 and L2

≤
6 = 2L2

2q6L2
f + L2

6λ2µ2+125ˆb2L2
g + L2

0κ2 ≤
gxy + L2
gyy.

45

Feihu and Heng

For simplicity, let Dt = L2
yt −
0k
2 + L2
2 + L2
2
Htk
yyg(xt, yt)

3k∇

2 + L2
y∗(xt)
k
4k∇yf (xt, yt)

−

Gtk

utk
2. Then we have

−

2 + L2

2k∇

1k∇xf (xt, yt)
htk
−
ρ(Ωt −
γ

Ωt+1)

+ 2 ˆCσ2η3
t .

2
xyg(xt, yt)

−

(136)

ηt
4

E[Dt] +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

≤

Taking average over t = 1, 2,

· · ·

, T on both sides of (122), we have

1
T

T

E

Xt=1

(cid:2)

ηt
4

Dt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:3)

Xt=1

Ωt+1)

ρ(Ωt −
T γ

+

2 ˆCσ2
T

T

Xt=1

η3
t .

Given x1 ∈ X

, let ∆0 =

y1 −
k

2, we have
y∗(x1)
k

and y1 ∈ Y
5L2
0b1γ
λµρ k
v1k
−
0b1γ∆0
+
λµρ

5L2

Ω1 = E

+

F (x1) +
(cid:2)
k∇yg(x1, y1)

F (x1) +

≤

k∇
5γσ2
ρη0

,

y1 −
2 +

2 +
y∗(x1)
k
2
xyg(x1, y1)

γ
ρη0 (cid:0)
−

2 +

k∇xf (x1, y1)
2 +
G1k

k∇

u1k
2
yyg(x1, y1)

−

k∇yf (x1, y1)
H1k

2

−

(cid:1)(cid:3)

where the last inequality holds by Assumption 2. Since ηt is decreasing, i.e., η−
any 0

T , we have

t

1
T ≥

2

h1k

−

(137)

1

η−
t

for

≤

≤

T

E

Xt=1

(cid:2)

ρ
T γηT

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

1
T

≤

≤

≤

≤

=

2

xtk

(cid:3)
2 ˆCσ2
T ηT

+

ρ2
4γ2 k

˜xt+1 −

Dt
4

T

Ωt −

Xt=1 (cid:0)

F (x1) +

Ωt+1

+

5L2

(cid:1)
0b1γ∆0
λµρ

T

η3
t

Xt=1
5γσ2
ρη0 −

+

F ∗

+

F ∗)

F ∗)

+

+

5L2
0b1∆0
T ηT λµ
5L2
0b1∆0
T ηT λµ
5L2

+

+

5σ2
T ηT η0
5σ2
T ηT η0

+

+

2 ˆCσ2
T ηT

T

(cid:1)
2 ˆCσ2
T ηT Z
1
2k3 ˆCσ2
T ηT

T

η3
t

Xt=1
k3
m + t

dt

ln(m + T )

F ∗)

−

+

0b1∆0
kλµ

+

5m1/3σ2
k2

+ 2k2 ˆCσ2 ln(m + T )

(cid:19)

(m + T )1/3
T

,

where the second inequality holds by the above inequality (137). Let M = ρ(F (x1)
−
kγ
5L2

kλµ + 5m1/3σ2
0b1∆0

k2 + 2k2 ˆCσ2 ln(m + T ), we have

1
T

T

E

Xt=1

(cid:2)

Dt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

M
T

≤

(cid:3)

(m + T )1/3.

46

(138)

F ∗)

+

BiAdam: Fast Adaptive Bilevel Optimization Methods

Since
L3k∇

Mt = ρ
γ k
2
yyg(xt, yt)

˜xt+1 −
Htk
−

yt −

+ L0k

xtk
+ L4k∇yf (xt, yt)

y∗(xt)
+ L1k∇xf (xt, yt)
k
htk
−

+
+ L2k∇
, according to the Jensen’s inequality, we have

Gtk

utk

2
xyg(xt, yt)

−

−

1
T

T

Xt=1

E[

1
2Mt]

6
T

≤ (cid:18)

T

Xt=1 (cid:0)

Dt
4

+

ρ2
4γ2 k

√6M
T 1/2

≤

(m + T )1/6

≤

2

xtk
˜xt+1 −
√6M m1/6
T 1/2

1/2

(cid:19)

(cid:1)

+

√6M
T 1/3

,

(139)

where the last inequality is due to (a + b)1/6

a1/6 + b1/6 for all a, b > 0. Thus we obtain

≤

1
T

T

Xt=1

E[

Mt]

≤

2√6M m1/6
T 1/2

+

2√6M
T 1/3

.

(140)

A.3 Convergence Analysis of saBiAdam Algorithm

In the subsection, we provide the detail convergence analysis of saBiAdam algorithm.

Lemma 29 Assume that the stochastic partial derivatives vt+1, and wt+1 be generated from
Algorithm 3, we have

E

≤

¯
wt+1 −
∇
k
βt+1)E
(1
−
3
βt+1 (cid:0)

f (xt+1, yt+1)
wt −
k
2 +
Rtk
k

¯
∇
Rt+1k
k

+

2

Rt+1k
Rtk
−
0η2
3L2
t
βt+1 (cid:0)

+

(cid:1)

2

−
f (xt, yt)

2 + β2

t+1σ2

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

,

(cid:1)

(141)

E

2
vt+1 − ∇yg(xt+1, yt+1)
k
k

≤

(1
−
+ 2L2

αt+1)E
gη2

2 + α2
vt − ∇yg(xt, yt)
k
k
E
2 + E
xtk
˜xt+1 −
t /αt+1
k

t+1σ2
˜yt+1 −
k

(142)

2

ytk

,
(cid:1)

where L2
E ¯ξ[ ¯
∇

L2

0 = 8
f (xt, yt; ¯ξ)] for all t
(cid:0)

f +

L2

gxyC2
f y
µ2 +
1.

≥

L2

gyyC2
gxyC2
f y
µ4

+

and Rt = R(xt, yt) = ¯
∇

f (xt, yt)

−

(cid:1)

(cid:0)
f C2
L2
µ2

gxy

Proof Without loss of generality, we only consider the term E
Rt+1k

¯
f (xt+1, yt+1)
wt+1 −
∇
k
¯
2. The other term is similar for this term. We ﬁrst consider the term
f (xt+1, yt+1)
∇
k

−
−

47

Feihu and Heng

¯
∇

f (xt, yt)
k

as follows:

∇yf (xt+1, yt+1)

1

−

∇yf (xt+1, yt+1)

2
yyg(xt+1, yt+1)
(cid:1)
2
yyg(xt+1, yt+1)
∇
(cid:1)
1
∇yf (xt+1, yt+1)

2
xyg(xt, yt)
(cid:0)
2
yyg(xt, yt)
(cid:1)

∇

−

−

1

∇yf (xt+1, yt+1)

1

1

−

−

∇

2
yyg(xt+1, yt+1)
(cid:1)
2
∇yf (xt, yt)
k

2
f (xt, yt)
k
2
xyg(xt+1, yt+1)
(cid:0)
2
yyg(xt, yt)
(cid:1)
2
xyg(xt+1, yt+1)
(cid:0)
∇yf (xt+1, yt+1)
− ∇
2
xyg(xt, yt)
− ∇
(cid:0)

¯
f (xt+1, yt+1)
−
∇
k∇xf (xt+1, yt+1)
− ∇
2
− ∇xf (xt, yt) +
xyg(xt, yt)
∇
∇
(cid:0)
− ∇xf (xt, yt)
k∇xf (xt+1, yt+1)
2
2
yyg(xt+1, yt+1)
xyg(xt, yt)
+
∇
(cid:1)
(cid:0)
1
2
2
∇yf (xt+1, yt+1)
yyg(xt, yt)
xyg(xt, yt)
(cid:1)
(cid:0)
2
2
2
∇yf (xt, yt)
yyg(xt, yt)
xyg(xt, yt)
k
(cid:1)
(cid:0)
4C 2
f y
2 +
− ∇xf (xt, yt)
µ2 k∇
k

2
xyg(xt+1, yt+1)

− ∇
1
−

∇

∇

∇

∇

+

+

−

−

1

∇
∇
k∇xf (xt+1, yt+1)
4
4C 2
gxyC 2
f y
µ4

k∇

+

2
yyg(xt+1, yt+1)

− ∇

¯
k
∇
=

=

≤

≤

8L2
f

xtk
xt+1 −
k
(cid:0)
8L2
gyyC 2
gxyC 2
f y
µ4
xt+1 −
k
(cid:0)

xtk

+

= L2
0

2 +

yt+1 −
k

2

ytk

(cid:1)

xt+1 −
k
(cid:0)
2 +
yt+1 −
k

xtk
ytk

2

,

(cid:1)

2
2
xyg(xt, yt)
k

− ∇

2 +
2
yyg(xt, yt)
k
gxyC 2
f y
µ2

8L2

+

2 +

yt+1 −
k

2

ytk

(cid:1)

gxy

4C 2
µ2 k∇yf (xt+1, yt+1)

2
− ∇yf (xt, yt)
k

(cid:0)

xt+1 −
k
f C 2
8L2
µ2

+

gxy

2 +

xtk

yt+1 −
k

ytk

2

xt+1 −
k
(cid:0)

2 +

xtk

(cid:1)
yt+1 −
k

2

ytk
(cid:1)
(143)

where L2

0 = 8
(cid:0)

L2

f +

L2

gxyC2
f y
µ2 +

L2

gyyC2
gxyC2
f y
µ4

+

f C2
L2
µ2

gxy

.
(cid:1)

48

BiAdam: Fast Adaptive Bilevel Optimization Methods

Since wt+1 = βt+1 ¯
∇

f (xt+1, yt+1; ¯ξt+1) + (1

βt+1)wt, we have

−

E

2

−

−

−
E

¯
wt+1 −
Rt+1k
f (xt+1, yt+1)
k
∇
f (xt+1, yt+1; ¯ξt+1) + (1
βt+1 ¯
= E
k
∇
¯
= E
βt+1)(wt −
(1
k
∇
¯
βt+1)
+ (1
∇
(cid:0)
¯
= β2
t+1
k
∇
+ E
(1
k
E
β2
t+1

f (xt+1, yt+1; ¯ξt+1)
βt+1)
wt −
(cid:0)
f (xt+1, yt+1; ¯ξt+1)

−
Rt) + βt+1
( ¯
∇
f (xt+1, yt+1)
Rt + ¯
−
∇
f (xt+1, yt+1)

f (xt, yt)
−
f (xt, yt) + Rt −
¯
−
∇
f (xt, yt)
¯
−
∇
¯
f (xt, yt) + Rt −
∇
k
2 + β2

−
¯
k
∇
βt+1)2(1 +

1
βt+1

βt+1)E

f (xt, yt)

+ (1

¯
∇

)E

−

≤

(cid:0)

−
( ¯
∇
t+1σ2 +

¯
∇
¯
∇

wt −
k
wt −
k
f (xt+1, yt+1)

f (xt, yt)

¯
∇

−

−

Rtk
Rtk
−
2 +
f (xt, yt)
k

2 + β2

wt −
k

−
0η2
3L2
t
˜xt+1 −
βt+1 (cid:0)
k

f (xt, yt)

¯
∇

−

Rtk

2 + β2

2 +

xtk

˜yt+1 −
k

ytk

2

,

(cid:1)
where the third equality is due to E ¯ξt+1[
second last inequality holds by 0
t+1 +β3
β2
βt+1 )
βt+1 and (1
≤
and the last inequality is due to Assumption 2.

∇
βt+1 ≤
βt+1)2(1+ 1

t+1 ≤

≤

−

−

1

−

βt+1)E
−
3
βt+1 k

¯
∇
βt+1)E

≤

≤

(1

(1

+

(1

≤

+

(144)

Rt+1

−

(cid:1)

¯
f (xt+1, yt+1)
βt+1)wt −
∇
−
f (xt+1, yt+1; ¯ξt+1)
¯
∇
f (xt+1, yt+1) + Rt+1)
(cid:1)

2
k

2

2

Rt+1k
¯
∇
−

f (xt+1, yt+1)

−

Rt+1k
( ¯
f (xt, yt) + Rt −
∇
2 + (1
Rt+1k
2
f (xt+1, yt+1) + Rt+1)
k

−

2
f (xt+1, yt+1) + Rt+1)
k
(cid:1)
βt+1)2(1 + βt+1)E
wt −
k

f (xt, yt)

¯
∇

2

Rtk

−

1
βt+1 k

¯
∇

f (xt, yt) + Rt −

( ¯
∇

2
f (xt+1, yt+1) + Rt+1)
k

2 +

t+1σ2
3
Rtk
βt+1 (cid:0)
k
3
t+1σ2 +
βt+1 (cid:0)

2
Rt+1)
k
k

2 +

Rtk
k

(cid:1)
Rt+1k
k

2

(cid:1)

(145)

f (xt+1, yt+1; ¯ξt+1)] =
1 such that (1
(1

−
βt+1)(1+ 1

f (xt+1, yt+1) + Rt+1; the
βt+1 −
1
βt+1 ,

βt+1)2(1 + βt+1) = 1
βt+1 + 1

βt+1 ) =

∇

−
βt+1 ≤

−

−

Theorem 30 Under the above Assumptions (6, 2, 4, 7, 8), in the Algorithm 3, given
Bt = Ip, ηt =
,
k > 0, 125L2

0, αt+1 = c1ηt, βt+1 = c2ηt, m

k2, (c1k)2, (c2k)2
0κ2 , m1/2ρ

(cid:0)
√6λµρ
7λ2µ2+125L2

√6L2

max

min

m1/2

4Lk

≥

(cid:1)

,

k

0

(m+t)1/2 for all t
≥
m1/2
k , 9
c1 ≤
2 ≤
and K = Lg
7µ , 1

6Lg

6µ2 ≤
15L2
0
min
4L2

(cid:0)

(cid:1)

0 < λ

≤

k , 0 < γ

c2 ≤
µ log(CgxyCf yT /µ), we have

≤

(cid:0)

(cid:1)

1
T

T

Xt=1

E[

Gt]

≤

2√3G′m1/4
T 1/2

+

2√3G′
T 1/4

+

√2
T

.

+ 5L2

kλµ + 2σ2
0∆0

k + 2mσ2

k

ln(m+T )+ 4(m+T )

9kT 2 + 8k

T and ∆0 =

(146)

y1−
k

2,
y∗(x1)
k

F ∗)

where G′ = ρ(F (x1)
−
kγ
12L2
g µ2
+ 2L2
L2
3 .
125L2
0

7 =

0

Proof Here we consider convergence properties of our saBiAdam algorithm with the general
ρId for variable x and the non-adaptive matrix Bt = Ip for variable
adaptive matrix At ≻

49

Feihu and Heng

≤

y. Since ηt =
m1/2ρ
γ
4Lk ≤
αt+1 = c1ηt ≤
we have c1, c2 ≤

k

(m+t)1/2 on t is decreasing and m
ρ
ρ
for any t
2Lηt
1. Similarly, due to m

≥
0. Due to 0 < ηt ≤

k2, we have ηt ≤
1 and m
(c2k)2, we have βt+1 ≤

≥

≥

2Lη0 ≤
c1k
m1/2 ≤
m1/2

k . According to Lemma 29, we have

η0 = k

m1/2 ≤

1 and

(c1k)2, we have
1. At the same time,

≥

E

≤ −

E

2
2
vt+1 − ∇yg(xt+1, yt+1)
vt − ∇yg(xt, yt)
k
k
−
k
k
2 + 2L2
˜xt+1 −
gηt/c1
vtk
k
−
(cid:0)
gµ2ηt
12L2
k∇yg(xt, yt)

k∇yg(xt, yt)
E

c1ηtE
125L2
0
6µ2

xtk
˜xt+1 −
k
0 (cid:0)

125L2

vtk

2 +

−

≤ −

2 +

xtk

˜yt+1 −
k
2 +

ytk
˜yt+1 −
k

2

+ c2

1η2

(cid:1)
ytk

2

+

(cid:1)

t σ2
mη2
t σ2
k2

,

(147)

where the above equality holds by αt+1 = c1ηt, and the last inequality is due to 125L2
c1 ≤

k . Similarly, we have

m1/2

0

6µ2 ≤

f (xt+1, yt+1)

E

¯
wt+1 −
∇
k
βt+1E
wt −
k

≤ −

¯
∇

−
f (xt, yt)

2

−
2 +

E

wt −
k
0η2
3L2
t
βt+1 (cid:0)

2

f (xt, yt)

¯
∇
˜xt+1 −
k

Rtk
−
2 +
xtk

˜yt+1 −
k

(148)

2

ytk

(cid:1)

Rt+1k
Rtk
+ β2

−

(cid:1)

+

≤ −

+

3
βt+1 (cid:0)
9ηt
E
2
2
3ηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

t+1σ2

wt −
k
Rtk
k

2 +

f (xt, yt)

¯
∇

−

2

Rt+1k
k

(cid:1)

2L2
0ηt
3

2 +

Rtk
mη2
t σ2
+
k2

,

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

where the last inequality holds by βt+1 = c2ηt and 9

According to Lemmas 19 and 20, we have

m1/2
k .

c2 ≤

2 ≤

F (xt)

F (xt+1)
2ηtγ

−
wt −

ρ k

¯
∇

2 +
f (xt, yt)
k

L2
0ηtγ
ρ

4ηtγ

wt −

f (xt, yt)

¯
∇

2 +

Rtk

−

ρ k

≤

≤

2

y∗(xt)
k
4ηtγ

Rtk

ρ k

−

ytk
2 +

−
L2
0ηtγ
ρ

ρηt
2γ k

˜xt+1 −

(149)

2

xtk

y∗(xt)
k

−

2

ytk

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 21, we have

yt+1 −
k

2
y∗(xt+1)
k

yt −

− k

2
y∗(xt)
k

≤ −

+

ηtµλ

2
yt −
y∗(xt)
4 k
k
25ηtλ
6µ k∇yg(xt, yt)

−
vtk

−

3ηt
4 k

2 +

2

ytk

˜yt+1 −
25κ2ηt
6µλ k

˜xt+1 −

(150)

2.
xtk

Next, we deﬁne a Lyapunov function, for any t

1

≥

Γt = E

F (xt) +
(cid:2)

5L2
0γ
λµρ k

yt −

2 +
y∗(xt)
k

γ
ρ

2 +
vt − ∇yg(xt, yt)
k
k
(cid:0)

wt −
k

f (xt, yt)

¯
∇

2

Rtk

−

.

(cid:1)(cid:3)

50

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

Γt

Γt+1 −
= F (xt+1)

F (xt) +

5L2
0γ
λµρ
−
2 +
vt − ∇yg(xt, yt)
k
− k
L2
0ηtγ
2 +
y∗(xt)
yt −
ρ
k
k
5L2
0γ
λµρ (cid:18) −

yt −

ηtµλ

4 k

+

yt+1 −
k
(cid:0)
¯
wt+1 −
∇
k
4ηtγ
wt −

ρ k

2
y∗(xt)
k

−

≤

+

−

+

=

−

−

≤ −

+

¯
∇

125L2
0
6µ2

wt −
k
2 +
Rtk
k

E

γ
ρ (cid:18) −
9ηt
2
2
3ηt (cid:0)
γηt
4ρ (cid:18)
15L2
0γ
4λµρ −

L2
0k

yt −
L2
7γ
ρ

(cid:0)
γηt
4ρ (cid:18)
2γ
3ρηt (cid:0)

L2
0k

yt −
2 +

Rtk
k

E

k∇yg(xt, yt)

−

2 +

vtk

2L2
0ηt
3

2 +

−

Rtk
mη2
+

t σ2
k2 (cid:19)

f (xt, yt)

2

Rt+1k
k

(cid:1)
2 + 2E
y∗(xt)
k

wt −
k

f (xt, yt)

−

¯
∇
2mγσ2
k2ρ

2 +

η2
t +

f (xt, yt)

¯
∇

−

wt −
k
4ηtγ

2,
Rtk

ρ k

ηtk

ytk
˜yt+1 −
(cid:1)
2 + 2E
y∗(xt)
k

2

Rt+1k
k

+

(cid:1)

2
y∗(xt+1)
k

− k

f (xt+1, yt+1)

f (xt, yt)

−

¯
∇
3ηt
4 k

˜yt+1 −
12L2

ytk
gµ2ηt

2
y∗(xt)
k

+

(cid:1)
wt −
Rtk

2

2

− k

4ηtγ

ρ k

yt −
Rt+1k
2 +

−
Rtk
2 +

25ληt
6µ

E

2

γ
2
vt+1 − ∇yg(xt+1, yt+1)
ρ
k
k
(cid:0)
¯
Rtk
f (xt, yt)
∇
−
ρηt
xtk
˜xt+1 −
2γ k
2 +

−

(cid:1)
2

25κ2ηt
6µλ k
t σ2
mη2
k2

125L2

0 (cid:0)
˜xt+1 −
k

(cid:0)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

+

(cid:1)

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

k∇yg(xt, yt)

vtk

−

˜xt+1 −

2

xtk

(cid:19)

2

Rtk
2γ
3ρηt (cid:0)
Rtk

2

˜xt+1 −

2

xtk

ρ
2γ −

L2
7γ
ρ −

(cid:19) −

(cid:0)
2 +
Rtk
k

2

Rt+1k
k

ρηt
4γ k

˜xt+1 −

(cid:19) −

0κ2γ
125L2
6µ2λ2ρ
4ηtγ

2

ηtk
(cid:1)
Rtk
2mγσ2
k2ρ

η2
t

ρ k

2 +

+

(cid:1)
xtk

(151)

where the ﬁrst inequality holds by the above inequalities (147), (148), (149) and (150); the

last inequality is due to 0 < γ

Let Qt = L2
0k

yt −

√6L2
≤
2 + 2
wt −
y∗(xt)
k
k

¯
∇

√6λµρ
7λ2µ2+125L2

0κ2 , 0 < λ
Rtk

−

15L2
0
4L2
≤
2, we have

f (xt, yt)

7µ and L2

7 =

12L2
g µ2
125L2
0

+ 2L2
3 .

0

γηt
4ρ

Qt +

ρηt
4γ k

˜xt+1 −

2

xtk

Γt −

≤

Γt+1 +

2mγσ2
k2ρ

η2
t +

2γ
3ρηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

+

(cid:1)

Taking average over t = 1, 2,

· · ·

, T on both sides of (152), we have

1
T

T

E

Xt=1

(cid:0)

ηt
4

Qt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:1)

Xt=1

Γt+1)

ρ(Γt −
T γ

+

1
T

T

Xt=1

2mσ2
k2 η2

t

4γηt

ρ k

2.
Rtk
(152)

+

1
T

T

(cid:18)

Xt=1

2
3ηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

(cid:1)

51

+ 4ηtk

Rtk

2

.

(cid:19)

Feihu and Heng

Given x1 ∈ X
Γ1 = E

F (xt) +

(cid:2)

F (x1) +

≤

and y1 ∈ Y
5L2
0γ
λµρ k
0γ∆0
λµρ

5L2

+

2γσ2
ρ

,

, let ∆0 =

y1 −
k
γ
2 +
y∗(x1)
ρ
k

y1 −

2, we have
y∗(x1)
k

2 +
v1 − ∇yg(x1, y1)
k
k

w1 −
k

¯
∇

f (x1, y1)

−

2

R1k

(cid:1)(cid:3)
(153)

(cid:0)

where the last inequality holds by Assumption 2. Since ηt is decreasing on t, i.e., η−
for any 0

T , we have

t

1
T ≥

1

η−
t

T

E

Xt=1

(cid:0)

ρ
T γηT

≤

≤
Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

(cid:1)

T

Γt −

Xt=1 (cid:0)

F (x1) +

Γt+1

+

5L2

(cid:1)
0γ∆0
λµρ

T

1
T ηT

+

Xt=1
2γσ2

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT
T

F ∗)

+

5L2
0∆0
T ηT λµ

+

2σ2
T ηT

+

+

4
T 2 Z
1
ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

k
(m + t)1/2 dt
5L2
0∆0
F ∗)
T ηT λµ
5L2
0∆0
kλµ

F ∗)

−

+

+

1
T

≤

≤

≤

≤

=

2 +

Rtk
k

2

Rt+1k
k

(cid:1)

+ 4ηtk

Rtk

2

(cid:19)

2mσ2
k2 η2

t +

1
T

F ∗

+

ρ −

1
T ηT

T

Xt=1
T

(cid:18)

2
3ηt (cid:0)
2mσ2
k2 η2

t +

2
3T 3

(cid:1)
2mσ2
T ηT k2 Z

T

1

Xt=1
k2
m + t

dt +

T

2
3T 3 Z
1

T

Xt=1

ηt

T

1
ηt

+

4
T 2

Xt=1
(m + t)1/2
k

dt

+

2σ2
T ηT
2σ2
k

+

+

2mσ2
T ηT
2mσ2
k

ln(m + T ) +

ln(m + T ) +

+

4
9kT 3 (m + T )3/2 +
4(m + T )

9kT 2 +

8k
T (cid:19)

8k
T 2 (m + T )1/2
(m + T )1/2
T

,

where the second inequality holds by the above inequality (164) and
by choosing K = Lg
2mσ2
k

Rtk
k
F ∗)
µ log(CgxyCf yT /µ) in Algorithm 3. Let G′ = ρ(F (x1)
kγ
9kT 2 + 8k

ln(m + T ) + 4(m+T )

T , we have

−

= 1
T for all t
1
≥
+ 5L2
kλµ + 2σ2
0∆0
k +

1
T

T

E

Xt=1

(cid:2)

Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

G′
T

≤

(cid:3)

(m + T )1/2.

According to the Jensen’s inequality, we have

1
T

T

E

Xt=1

1
2

(cid:2)
T

3
T

3
T

≤ (cid:18)

=

(cid:18)

Xt=1 (cid:0)
T

Xt=1 (cid:0)

Qt
4

+

ρ2
4γ2 k

√3G′
T 1/2

≤

(m + T )1/4

≤

L0k
(cid:0)

yt −

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk

(cid:1)(cid:3)

L2
0
4 k

yt −

2 +
y∗(xt)
k

2
4

E

wt −
k

¯
∇

f (xt, yt)

2 +

Rtk

−

ρ2
4γ2 k

˜xt+1 −

1/2

2

xtk

(cid:19)

(cid:1)

2

xtk
˜xt+1 −
√3G′m1/4
T 1/2

1/2

(cid:19)

(cid:1)

+

√3G′
T 1/4

,

52

(154)

BiAdam: Fast Adaptive Bilevel Optimization Methods

where the last inequality is due to (a + b)1/4

a1/4 + b1/4 for all a, b > 0. Thus we have

≤

T

1
T

yt −

E

L0k
Xt=1
(cid:2)
2√3G′m1/4
T 1/2

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk
(cid:3)

.

2√3G′
T 1/4
Rtk ≥ k

wt −

+

−

¯
∇

f (xt, yt)

Rtk

k − k

, by the above inequality (166),

(155)

≤
wt −
Since
k
we can obtain

f (xt, yt)

¯
∇

T

yt −

E

L0k
Xt=1
(cid:2)
2√3G′m1/4
T 1/2

2√3G′m1/4
T 1/2

1
T

≤

=

where the last inequality is due to
in Algorithm 3.

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)
k

+

ρ
γ k

˜xt+1 −

xtk
(cid:3)

+

+

2√3G′
T 1/4 +
2√3G′
T 1/4
Rtk
k

+

√2
T

T

Xt=1

Rtk
k

√2
T

,

(156)

= 1

T for all t

1 by choosing K = Lg

µ log(CgxyCf yT /µ)

≥

Theorem 31 Under the above Assumptions (6, 2, 4, 7, 8), in the Algorithm 3, given
Bt = btIp (ˆb
0, αt+1 = c1ηt, βt+1 = c2ηt,
m1/2

b > 0), ηt =

for all t

≥

bt ≥
k2, (c1k)2, (c2k)2
max
15bL2
(cid:0)
b
0
7µ ,
4L2
6Lg

, 0 < γ

≤

k
(m+t)1/2
, k > 0, 125L2
(cid:1)
√6λµρ
min
7λ2µ2+125ˆb2L2

6µ2 ≤

q6L2

0

(cid:1)

(cid:0)

c1 ≤
, m1/2ρ
4Lk

0κ2

(cid:1)

c2 ≤
2 ≤
and K = Lg

k , 0 < λ
≤
µ log(CgxyCf yT /µ),

≥
m1/2
k , 9

m

min

≥

(cid:0)
we have

T

1
T

Xt=1
+ 5b1L2
+ 2L2
3 .

0

F ∗)

where G′ = ρ(F (x1)
−
kγ
12L2
g µ2
2, L2
y∗(x1)
125L2
k
0

7 =

E[

Gt]

2√3G′m1/4
T 1/2

+

2√3G′
T 1/4

+

√2
T

.

≤

kλµ + 2σ2
0∆0

k + 2mσ2

k

ln(m + T ) + 4(m+T )

9kT 2 + 8k

T and ∆0 =

Proof Here we consider convergence properties of our saBiAdam algorithm with the general
bt ≥
adaptive matrix At ≻
b > 0) for variable y. This proof basically follows the proof of Theorem 30.

ρId for variable x and the global adaptive matrix Bt = btIp (ˆb

≥

According to Lemma 29, we have

E

≤ −

E

2
2
vt − ∇yg(xt, yt)
vt+1 − ∇yg(xt+1, yt+1)
−
k
k
k
k
2 + 2L2
˜xt+1 −
gηt/c1
vtk
k
−
(cid:0)
gµ2ηt
12L2
k∇yg(xt, yt)

k∇yg(xt, yt)
E

c1ηtE
125L2
0
6µ2

xtk
˜xt+1 −
k
0 (cid:0)

125L2

vtk

2 +

−

≤ −

2 +

xtk

˜yt+1 −
k
2 +

ytk
˜yt+1 −
k

2

+ c2

1η2

(cid:1)
ytk

2

+

(cid:1)

t σ2
t σ2
mη2
k2

,

(157)

y1 −
k

(158)

53

Feihu and Heng

where the above equality holds by αt+1 = c1ηt, and the last inequality is due to 125L2
6µ2
c1 ≤

k . Similarly, we have

m1/2

0

≤

f (xt+1, yt+1)

E

¯
wt+1 −
∇
k
βt+1E
wt −
k

≤ −

¯
∇

−
f (xt, yt)

2

−
2 +

E

wt −
k
0η2
3L2
t
βt+1 (cid:0)

2

f (xt, yt)

¯
∇
˜xt+1 −
k

Rtk
−
2 +
xtk

˜yt+1 −
k

(159)

2

ytk

(cid:1)

Rt+1k
Rtk
+ β2

−

(cid:1)

+

≤ −

+

3
βt+1 (cid:0)
9ηt
E
2
2
3ηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

t+1σ2

wt −
k
Rtk
k

2 +

f (xt, yt)

¯
∇

−

2

Rt+1k
k

(cid:1)

2L2
0ηt
3

2 +

Rtk
t σ2
mη2
+
k2

,

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

where the last inequality holds by βt+1 = c2ηt and 9

2 ≤

m1/2
k .

c2 ≤

According to Lemmas 19 and 20, we have

F (xt)

F (xt+1)
2ηtγ

−
wt −

ρ k

¯
∇

2 +
f (xt, yt)
k

L2
0ηtγ
ρ

4ηtγ

wt −

f (xt, yt)

¯
∇

2 +

Rtk

−

ρ k

≤

≤

2

y∗(xt)
k
4ηtγ

Rtk

ρ k

−

ytk
2 +

−
L2
0ηtγ
ρ

ρηt
2γ k

˜xt+1 −

(160)

2

xtk

y∗(xt)
k

−

2

ytk

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 22, we have

yt+1 −
k

2
y∗(xt+1)
k
− k
2
y∗(xt)
k

yt −

ηtµλ
4bt k

yt −

−

2
y∗(xt)
k
3ηt
˜yt+1 −
4 k

2 +

ytk

25ηtλ
6µbt k∇yg(xt, yt)

2 +

vtk

−

≤ −

(161)

25κ2ηtbt

6µλ k

˜xt+1 −

2.
xtk

Next, we deﬁne a Lyapunov function, for any t

1

≥

Γt = E

F (xt) +
(cid:2)

5btL2
0γ
λµρ k

yt −

2 +
y∗(xt)
k

γ
ρ

(cid:0)

2 +
vt − ∇yg(xt, yt)
k
k

wt −
k

¯
∇

f (xt, yt)

2

Rtk

−

.

(cid:1)(cid:3)

54

BiAdam: Fast Adaptive Bilevel Optimization Methods

Then we have

Γt

Γt+1 −
= F (xt+1)

≤

F (xt) +

5btL2
0γ
λµρ
−
2 +
vt − ∇yg(xt, yt)
k
− k
L2
0ηtγ
2 +
ytk
y∗(xt)
ρ
−
k
5btL2
0γ
ηtµλ
λµρ (cid:18) −
4bt k
125L2
0
6µ2

yt −

+

+

yt+1 −
k
(cid:0)
¯
wt+1 −
∇
k
4ηtγ
wt −

ρ k

2
y∗(xt)
k

−

k∇yg(xt, yt)

2 +

vtk

−

2
y∗(xt+1)
k

− k

f (xt+1, yt+1)

¯
∇

f (xt, yt)

−

+

yt −
Rt+1k
2 +

2

2
y∗(xt)
k
(cid:1)
wt −
Rtk

4ηtγ

− k

2

−
Rtk

−

ρ k
25ηtλ
6µbt k∇yg(xt, yt)

3ηt
4 k

˜yt+1 −
gµ2ηt

12L2

2 +

ytk

2

γ
ρ
(cid:0)
¯
f (xt, yt)
∇
ρηt
2γ k

2
vt+1 − ∇yg(xt+1, yt+1)
k
k
Rtk
xtk
2 +

−
˜xt+1 −

25κ2ηtbt

(cid:1)
2

vtk

−

6µλ k
t σ2
mη2
k2

E

E

¯
∇

wt −
k

γ
ρ (cid:18) −
9ηt
2
γηt
L2
yt −
0k
4ρ (cid:18)
L2
15btL2
7γ
0γ
ρ
4λµρ −

(cid:0)
γηt
4ρ (cid:18)
2γ
3ρηt (cid:0)

L2
0k

yt −
2 +

Rtk
k

−

=

−

−

≤ −

+

f (xt, yt)

2 +

Rtk

−

2 + 2E
y∗(xt)
k

wt −
k

¯
∇

2

Rtk

−

(cid:19) −

125L2

0 (cid:0)
˜xt+1 −
k

2L2
0ηt
3

(cid:0)
f (xt, yt)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2 +

xtk

˜yt+1 −
k
ρ
2γ −

+

2

ytk
(cid:1)
L2
7γ
ρ −

2 + +

ytk

2mγσ2
k2ρ

η2
t +

2γ
3ρηt (cid:0)

2 +

Rt+1k
k

f (xt, yt)

¯
∇

2

Rtk

−

(cid:19) −

˜xt+1 −

xtk

(cid:0)
Rtk
k
ρηt
4γ k

2

(cid:1)

+

ytk
2
Rtk
3ηt (cid:0)
k
0κ2γ
125b2
t L2
6µ2λ2ρ
4ηtγ

2

+

(cid:1)
2 +

ρ k
2mγσ2
k2ρ

ηtk

˜yt+1 −
(cid:1)
2 + 2E
y∗(xt)
k

2

Rt+1k
k

+

(cid:1)

wt −
k
4ηtγ

2,
Rtk

ρ k

ηtk

2

(cid:1)
Rtk
η2
t

(162)

˜xt+1 −

2

xtk

(cid:19)

2 +

2

Rt+1k
k

+

mη2

t σ2
k2 (cid:19)

2

xtk

(cid:1)
˜xt+1 −

where the ﬁrst inequality holds by the above inequalities (158), (159), (160) and (161); the
last inequality is due to 0 < γ

√6λµρ
7λ2µ2+125b2

0κ2 , 0 < λ

t L2

≤

15bL2
0
4L2

7µ ≤

√6λµρ
7λ2µ2+125ˆb2L2

0κ2 ≤

√6L2

≤

q6L2

15btL2
0
7µ .
4L2
Let Qt = L2
0k

yt −

2 + 2
wt −
y∗(xt)
k
k

¯
∇

f (xt, yt)

−

2, we have

Rtk

γηt
4ρ

Qt +

ρηt
4γ k

˜xt+1 −

2

xtk

Γt −

≤

Γt+1 +

2mγσ2
k2ρ

η2
t +

2γ
3ρηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

+

(cid:1)

Taking average over t = 1, 2,

· · ·

, T on both sides of (163), we have

1
T

T

E

Xt=1

(cid:0)

ηt
4

Qt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:1)

Xt=1

Γt+1)

ρ(Γt −
T γ

+

1
T

T

Xt=1

2mσ2
k2 η2

t

4γηt

ρ k

2.
Rtk
(163)

+

1
T

T

(cid:18)

Xt=1

2
3ηt (cid:0)

2 +

Rtk
k

2

Rt+1k
k

(cid:1)

55

+ 4ηtk

Rtk

2

.

(cid:19)

Feihu and Heng

Given x1 ∈ X
Γ1 = E

F (xt) +

(cid:2)

F (x1) +

≤

and y1 ∈ Y
5b1L2
0γ
λµρ k
0γ∆0

5b1L2
λµρ

, let ∆0 =

y1 −
k
γ
2 +
y∗(x1)
ρ
k

y1 −

+

2γσ2
ρ

,

2, we have
y∗(x1)
k

2 +
v1 − ∇yg(x1, y1)
k
k
(cid:0)

w1 −
k

¯
∇

f (x1, y1)

2

R1k

−

(cid:1)(cid:3)

(164)

where the last inequality holds by Assumption 2. Since ηt is decreasing on t, i.e., η−
for any 0

T , we have

t

1
T ≥

1

η−
t

1
T

≤

≤

≤

≤

=

T

E

Xt=1

(cid:0)

ρ
T γηT

≤

≤
Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

(cid:1)

T

Γt −

Xt=1 (cid:0)

F (x1) +

1
T ηT

+

Γt+1

+

(cid:1)
5b1L2
0γ∆0
λµρ

T

Xt=1
2γσ2

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT
T

F ∗)

+

5b1L2
0∆0
T ηT λµ

+

2σ2
T ηT

+

2mσ2
k2 η2

t +

1
T

F ∗

+

ρ −

1
T ηT

T

(cid:18)

Xt=1
T

Rtk
k

2
3ηt (cid:0)
2mσ2
k2 η2

t +

(cid:1)
2mσ2
T ηT k2 Z
1

T

Xt=1
k2
m + t

dt +

T

2
3T 3 Z
1

2 +

2

Rt+1k
k

(cid:1)

+ 4ηtk

Rtk

2

(cid:19)

2
3T 3

T

ηt

Xt=1

T

1
ηt

+

4
T 2

Xt=1
(m + t)1/2
k

dt

+

4
T 2 Z
1
ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

k
(m + t)1/2 dt
5b1L2
0∆0
F ∗)
T ηT λµ
5b1L2
kλµ

F ∗)

−

+

+

0∆0

+

+

ln(m + T ) +

2σ2
T ηT
2σ2
k

2mσ2
T ηT
2mσ2
k

(cid:18)

+

+

9kT 2 +
Rtk
k
µ log(CgxyCf yT /µ) in Algorithm 3. Let G′ = ρ(F (x1)

where the second inequality holds by the above inequality (164) and
by choosing K = Lg
2σ2
k + 2mσ2

ln(m + T ) + 4(m+T )

T , we have

9kT 2 + 8k

ln(m + T ) +

kγ

k

1
T

T

E

Xt=1

(cid:2)

Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

G′
T

≤

(cid:3)

(m + T )1/2.

According to the Jensen’s inequality, we have

4
9kT 3 (m + T )3/2 +
4(m + T )

8k
T 2 (m + T )1/2
(m + T )1/2
T

,

8k
T (cid:19)
= 1
F ∗)

−

T for all t
1
≥
+ 5b1L2
0∆0
kλµ +

1
T

T

E

Xt=1

1
2

(cid:2)
T

3
T

3
T

≤ (cid:18)

=

(cid:18)

Xt=1 (cid:0)
T

Xt=1 (cid:0)

Qt
4

+

ρ2
4γ2 k

√3G′
T 1/2

≤

(m + T )1/4

≤

L0k
(cid:0)

yt −

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk

(cid:1)(cid:3)

L2
0
4 k

yt −

2 +
y∗(xt)
k

2
4

E

wt −
k

¯
∇

f (xt, yt)

2 +

Rtk

−

ρ2
4γ2 k

˜xt+1 −

1/2

2

xtk

(cid:19)

(cid:1)

2

xtk
˜xt+1 −
√3G′m1/4
T 1/2

1/2

(cid:19)

(cid:1)

+

√3G′
T 1/4

,

56

(165)

BiAdam: Fast Adaptive Bilevel Optimization Methods

where the last inequality is due to (a + b)1/4

a1/4 + b1/4 for all a, b > 0. Thus we have

≤

T

1
T

≤

yt −

E

L0k
Xt=1
(cid:2)
2√3G′m1/4
T 1/2

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk
(cid:3)

+

2√3G′
T 1/4

.

(166)

wt −
Since
k
we can obtain

f (xt, yt)

¯
∇

Rtk ≥ k

wt −

−

¯
∇

f (xt, yt)

Rtk

k − k

, by the above inequality (166),

1
T

≤

=

T

yt −

E

L0k
Xt=1
(cid:2)
2√3G′m1/4
T 1/2

2√3G′m1/4
T 1/2

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)
k

+

ρ
γ k

˜xt+1 −

xtk
(cid:3)

+

+

2√3G′
T 1/4

2√3G′
T 1/4

+

+

√2
T

T

Xt=1

Rtk
k

√2
T

,

(167)

where the last inequality is due to
in Algorithm 3.

Rtk
k

= 1

T for all t

≥

1 by choosing K = Lg

µ log(CgxyCf yT /µ)

A.4 Convergence Analysis of VR-saBiAdam Algorithm

In the subsection, we detail convergence analysis of VR-saBiAdam algorithm.

Lemma 32 Under the above Assumptions , assume the stochastic gradient estimators vt
and wt be generated from Algorithm 4, we have

E

k∇yg(xt+1, yt+1)

−

2

vt+1k

≤

(1
−
+ 4L2

αt+1)E
E
gη2
t

k∇yg(xt, yt)
xtk
˜xt+1 −
k

−
2 + E

t+1σ2
2 + 2α2
vtk
2
ytk
˜yt+1 −
,
k
(cid:1)

(cid:0)

(168)

E

wt+1 −
k

¯
∇

f (xt+1, yt+1)

2

Rt+1k

−

≤

(1
−
+ 4L2

βt+1)E
K η2
t

wt −
k
˜xt+1 −
k
(cid:0)

¯
f (xt, yt)
∇
2 +
xtk

2 + 2β2
Rtk
−
2
ytk
˜yt+1 −
k

,

t+1σ2

(169)

(cid:1)

where L2

K = 2L2

f + 6C 2

gxyL2
f

K
2µLg

−

µ2 + 6C 2

f yL2

gxy

K
2µLg

−

µ2 + 6C 2

gxyL2
f

K 3L2
gyy
µ)2(2µLg

µ2) .

−

(Lg

−

57

Feihu and Heng

Proof Since wt+1 = ¯
∇

f (xt+1, yt+1; ¯ξt+1) + (1

−

βt+1)
(cid:0)

wt −

¯
∇

f (xt, yt; ¯ξt+1)
(cid:1)

, we have

2

E

¯
∇

(cid:0)

−

−

−

≤

−

−

−

(1

¯
∇

¯
∇

¯
∇

¯
∇

¯
∇

= (1

−
+ (1

−
+ 2(1

(170)
2

f (xt+1, yt+1)

Rt+1k
Rt+1
−
(cid:1)
f (xt, yt)
−
Rt+1

−
f (xt, yt; ¯ξt))
¯
∇

wt+1 −
k
¯
= E
k
∇
= E
(1
k
+ (1

f (xt+1, yt+1)
f (xt+1, yt+1; ¯ξt+1) + (1

¯
∇
f (xt+1, yt+1; ¯ξt+1)
( ¯
∇

f (xt+1, yt+1)
¯
∇
f (xt+1, yt+1)
¯
∇
f (xt+1, yt+1)

βt+1)
wt −
(cid:0)
Rt) + βt+1
¯
∇
2 + E
¯
∇
2 + 2β2
¯
−
∇
2 + 2β2
2 + 2β2
2 + 2β2

Rt+1k
−
βt+1)(wt −
f (xt, yt)
f (xt+1, yt+1; ¯ξt+1)
¯
βt+1)
−
∇
−
(cid:0)
βt+1)2E
Rtk
wt −
f (xt, yt)
k
−
f (xt+1, yt+1; ¯ξt+1)
¯
βt+1)
−
−
∇
(cid:0)
βt+1)2E
Rtk
f (xt, yt)
wt −
k
f (xt+1, yt+1; ¯ξt+1)
¯
βt+1)2
k
∇
¯
Rtk
f (xt, yt)
wt −
∇
k
¯
Rtk
f (xt, yt)
wt −
−
∇
k
¯
Rtk
f (xt, yt)
wt −
∇
k
where the third equality holds by E ¯ξ
f (xt, yt; ¯ξt))
= ¯
¯
E ¯ξ
(cid:2)
∇
∇
−
E[ζ]
E
2; the second last inequality is due to Lemma 4; the last inequality holds by
2
(cid:3)
(cid:2)
ζ
k
k
k
xt), yt+1 = yt + ηt(˜yt+1 −
1 and xt+1 = xt + ηt(˜xt+1 −
0 < βt+1 ≤

f (xt, yt; ¯ξt+1)
¯
(cid:1)
∇
(cid:0)
Rt+1 −
f (xt+1, yt+1)
−
f (xt+1, yt+1; ¯ξt+1)
¯
βt+1
∇
k
Rt+1 −
f (xt+1, yt+1)
−
f (xt+1, yt+1; ¯ξt+1)
¯
∇
f (xt, yt; ¯ξt))
( ¯
Rt+1 −
∇
f (xt+1, yt+1; ¯ξt+1))
¯
¯
βt+1)2
−
∇
∇
k
−
βt+1)2L2
2 +
yt+1 −
xtk
xt+1 −
K
k
k
(cid:0)
2 +
ytk
˜yt+1 −
˜xt+1 −
xtk
,
k
k
= ¯
∇

¯
∇
k
f (xt+1, yt+1)
t+1σ2 + 2(1
t+1σ2 + 4(1
t+1σ2 + 4L2

f (xt, yt) + Rt; the third last inequality holds by the inequality E

(cid:0)
f (xt+1, yt+1; ¯ξt+1)
(cid:3)

−
βt+1)2E
βt+1)2E
βt+1)E

f (xt+1, yt+1) + Rt+1 and

−
f (xt, yt; ¯ξt)

−
f (xt, yt)

−
f (xt, yt)

−
Kη2
t

( ¯
∇

¯
∇

¯
∇

yt).

ζ
k

t+1

(1

(1

(1

−

−

−

−

−

≤

−

≤

−

≤

≤

−

−

−

E

(cid:1)

2

2
k

Rt)
(cid:1)

2
k

(cid:1)
Rt)
−
(cid:1)
2
Rt+1k
−

2
Rt)
k
f (xt, yt; ¯ξt)
2
k
2
ytk

(cid:1)

k

Theorem 33 Under the above Assumptions (6, 3, 4, 7, 8), in the Algorithm 4, given Bt =
Ip, ηt =
,
k > 0, c1 ≥
15L2
0
min
16L2

2, k3, (c1k)3, (c2k)3
0κ2 , m1/3ρ

≥
6µ2 , c2 ≥
and K = Lg

(m+t)1/3 for all t
3k3 + 125L2

µ log(CgxyCf yT /µ), we have

≥
(cid:0)
√6λµρ
8λ2µ2+125L2

t , βt+1 = c2η2

, 0 < λ
(cid:1)

0, αt+1 = c1η2

2 , 0 < γ

3k3 + 9

2√24L2

t , m

max

(cid:1)
≤

min

6Lg

4Lk

≤

(cid:0)

2

2

0

8µ , 1

(cid:0)

(cid:1)

1
T

T

Xt=1

E[

Gt]

≤

2√3M ′m1/6
T 1/2

+

2√3M ′
T 1/3 +

√2
T

,

where M ′ = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k

F ∗)

+ 5L2

kλµ + 2m1/3σ2
0∆0
g + L2
K.

8 = L2

k2 + 2k2(c2

1 + c2

2)σ2 ln(m + T ) + 6k(m+T )1/3

T

(171)

, ∆0 =

Proof Here we consider convergence properties of our saBiAdam algorithm with the general
ρId for variable x and the non-adaptive matrix Bt = Ip for variable
adaptive matrix At ≻
1 and
y. Since ηt =
m1/3ρ
4Lk ≤
t ≤

k3, we have ηt ≤
1 and m
(c2k)3, we have βt+1 ≤

(m+t)1/3 on t is decreasing and m
ρ
for any t

≥
0. Due to 0 < ηt ≤

(c1k)3, we have
1. According

γ
αt+1 = c1η2

ρ
2Lηt
c1k
m1/3 ≤

1. Similarly, due to m

2Lη0 ≤
c1ηt ≤

η0 = k

m1/3 ≤

≥

≥

≥

≤

k

58

BiAdam: Fast Adaptive Bilevel Optimization Methods

to Lemma 32, we have

E

1
ηt

≤

=

(cid:0)

(cid:0)

k∇yg(xt+1, yt+1)
αt+1
1
1
ηt
ηt

−

−

1
ηt −

1
ηt

−

1 −

1 (cid:1)
−
c1ηt

(cid:1)

2

vt+1k

−

−

1
ηt

−

1

E

k∇yg(xt, yt)

−

2

vtk

E

k∇yg(xt, yt)
E

k∇yg(xt, yt)

vtk

vtk

−

−

2 + 4L2

2 + 4L2

gηt

˜xt+1 −
k
(cid:0)
gηt

2 +

xtk

˜xt+1 −
k
(cid:0)
t . Similarly, we have

2 +

xtk

˜yt+1 −
k

ytk

2

+

˜yt+1 −
k

ytk

(cid:1)
2

(cid:1)

(172)

2α2

t+1σ2
ηt
1η3
t σ2,

+ 2c2

where the second equality is due to αt+1 = c1η2
1
ηt

f (xt+1, yt+1)

Rt+1k

1
ηt

¯
∇

−

−

E

E

1

2

wt+1 −
k
βt+1
1
ηt

−

1
ηt −

1
ηt

(cid:0)

≤

=

1
ηt

−

1 (cid:1)
−
c2ηt

1 −

E

wt −
k

¯
∇

f (xt, yt)

E

wt −
k

f (xt, yt)

¯
∇

−

Rtk

Rtk

−

−

k

(cid:1)
(cid:0)
−
By ηt =
(m+t)1/3 , we have
1
ηt

1
ηt −

(m + t)

1
k

=

1

1
3

−

(cid:0)

wt −
k

f (xt, yt)

¯
∇

2

Rtk

−

(173)

2 + 4L2

K ηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

ytk

2

2 + 4L2

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

ytk

(cid:0)
Kηt

(cid:0)

+

t+1σ2
2β2
ηt
t σ2.
2η3

+ 2c2

(cid:1)
2

(cid:1)

(m + t

−

1
3

1)

−

1
3k(m + t

−

≤

(cid:1)

1

1)2/3 ≤

3k

m/2 + t

2/3

≤

22/3
3k3

22/3
3k3 η2

22/3
3k(m + t)2/3 =

k2
(m + t)2/3 =
where the ﬁrst inequality holds by the concavity of function f (x) = x1/3, i.e., (x + y)1/3
x1/3 + y
0 < ηt ≤
1
E
ηt

3x2/3 ; the second inequality is due to m
3k3 + 125L2
1. Let c1 ≥

≤
2, and the last inequality is due to

k∇yg(xt, yt)

6µ2 , we have

2
3k3 ηt,

vt+1k

1
ηt

(175)

(174)

vtk

t ≤

≥

−

−

−

E

2

2

2

1

0

(cid:0)

(cid:1)

k∇yg(xt+1, yt+1)
125L2
0ηt
E
6µ2
3k3 + 9

2 , we have

2

k∇yg(xt, yt)

vtk

−

−
2 + 4L2

gηt

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:0)

(cid:1)

+ 2c2

1η3

t σ2.

f (xt+1, yt+1)

¯
∇

2

Rt+1k

−

−

E

wt −
k

¯
∇

1

f (xt, yt)

2

Rtk

−

(176)

wt −
k

¯
∇

f (xt, yt)

2 + 4L2

Rtk

≤ −
According to Lemmas 19 and 20, we have

−

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

2η3

t σ2.

(cid:0)

1
ηt

−
K ηt

F (xt)

F (xt+1)
2ηtγ

−
wt −

ρ k

¯
∇

2 +
f (xt, yt)
k

L2
0ηtγ
ρ

4ηtγ

wt −

f (xt, yt)

¯
∇

2 +

Rtk

−

ρ k

≤

≤

2

y∗(xt)
k
4ηtγ

Rtk

ρ k

−

ytk
2 +

−
L2
0ηtγ
ρ

ρηt
2γ k

˜xt+1 −

(177)

2

xtk

y∗(xt)
k

−

2

ytk

−

ρηt
2γ k

˜xt+1 −

2.
xtk

59

≤ −
Let c2 ≥
1
E
ηt

wt+1 −
k
9ηt
E
2

Feihu and Heng

According to Lemma 21, we have

yt+1 −
k

2
y∗(xt+1)
k

yt −

− k

2
y∗(xt)
k

≤ −

+

ηtµλ

2
yt −
y∗(xt)
4 k
k
25ηtλ
6µ k∇yg(xt, yt)

−
vtk

−

3ηt
4 k

2 +

2

ytk

˜yt+1 −
25κ2ηt
6µλ k

˜xt+1 −

(178)

2.
xtk

Next, we deﬁne a Lyapunov function, for any t

1

≥

Θt = E

F (xt) +
(cid:2)
Then we have

5L2
0γ
λµρ k

yt −

2 +
y∗(xt)
k

γ
ρηt

−

2 +
vt − ∇yg(xt, yt)
k
k
1 (cid:0)

wt −
k

¯
∇

f (xt, yt)

2

Rtk

−

.

(cid:1)(cid:3)

Θt

Θt+1 −
= F (xt+1)

−
ηtµλ

4 k
0ηt

E

E

+

+

1
ηt
−
1
−
L2
0ηtγ
y∗(xt)
ρ
k
5L2
0γ
λµρ (cid:18) −
γ
ρ (cid:18) −
9ηt
2
γηt
4ρ (cid:18)
15L2
0γ
4λµρ −

wt −
k

L2
0k

−

−

−

E

125L2
6µ2

≤

=

(cid:0)
γηt
4ρ (cid:18)
4ηtγ

≤ −

+

L2
0k

yt −

Rtk

ρ k

2 +

yt −
4L2
8γ
ρ

5L2
0γ
λµρ

F (xt) +

yt+1 −
−
k
(cid:0)
1
E
2 +
vt − ∇yg(xt, yt)
ηt
k
k
4ηtγ

2 +

ytk

wt −

ρ k

+

− k

yt −
f (xt+1, yt+1)

2
y∗(xt)
k
(cid:1)
Rt+1k

¯
∇

−
4ηtγ

2

Rtk

−

ρ k

1
ηt

γ
ρ

2

(cid:0)

E

2
vt+1 − ∇yg(xt+1, yt+1)
k
k
1
¯
E
Rtk
ηt
−
∇
ρηt
2γ k

wt −
k

f (xt, yt)

−

−

1

2

2

2
y∗(xt+1)
k

wt+1 −
k
¯
∇
3ηt
4 k

f (xt, yt)

−

2 +

Rtk
2 +

xtk
˜xt+1 −
2 +

vtk

−

yt −

2
y∗(xt)
k

−

˜yt+1 −

ytk

25ληt
6µ

E

k∇yg(xt, yt)

25κ2ηt
6µλ k

˜xt+1 −

2

xtk

(cid:19)

k∇yg(xt, yt)

¯
∇

f (xt, yt)

−

Rtk

2 + 4L2

gηt

−

vtk
2 + 4L2

Kηt

˜xt+1 −
k
2 +

xtk

(cid:0)
˜xt+1 −
k

2 + 2E
y∗(xt)
k

wt −
k

(cid:0)
f (xt, yt)

¯
∇
4ηtγ

−

2

Rtk
2(c2

2 +

ytk

Rtk

ρ k

2 +

˜yt+1 −
k
ρ
2γ −
(cid:19) −
(cid:0)
2)γσ2
1 + c2
ρ

η3
t

2 +

xtk

˜yt+1 −
k

+ 2c2

1η3

t σ2

2

ytk
(cid:1)
2η3
+ 2c2

2

ytk
(cid:1)
4L2
8γ
ρ −

t σ2

(cid:19)
0κ2γ
125L2
6λ2µ2ρ

ηtk

˜xt+1 −

2

xtk

(cid:1)

ηtk

˜yt+1 −
(cid:1)
2 + 2E
y∗(xt)
k
1 + c2
ρ

2)γσ2

2(c2

wt −
k

f (xt, yt)

¯
∇

2

Rtk

−

(cid:19) −

ρηt
4γ k

˜xt+1 −

2

xtk

η3
t ,

(179)

where the ﬁrst inequality holds by the above inequalities (175), (176), (177) and (178); the
last inequality is due to 0 < γ

8µ and L2

8 = L2

g + L2
K .

Let Qt = L2
0k
E[Qt] +

2√24L2
≤
¯
2 + 2
wt −
y∗(xt)
k
k
∇
Θt −
˜xt+1 −
Taking average over t = 1, 2,

yt −
ρηt
4γ k

γηt
4ρ

xtk

≤

2

· · ·

√6λµρ
8λ2µ2+125L2
f (xt, yt)

Θt+1 +

0κ2 , 0 < λ
Rtk

−
4ηtγ

≤

15L2
0
16L2
2, then we have
1 + c2
ρ

2(c2

2 +

Rtk

ρ k

, T on both sides of (180), we have

2)γσ2

η3
t .

(180)

1
T

T

E

Xt=1

(cid:0)

ηt
4

Qt +

ρ2ηt
4γ2 k

˜xt+1 −

2

xtk

T

≤

(cid:1)

Xt=1

Θt+1)

ρ(Θt −
T γ

+

4
T

T

Xt=1

60

ηtk

Rtk

2 +

2(c2

2)σ2

1 + c2
T

T

Xt=1

η3
t .

BiAdam: Fast Adaptive Bilevel Optimization Methods

Given x1 ∈ X

, let ∆0 =

2, we have
y∗(x1)
k

and y1 ∈ Y
5L2
0γ
λµρ k
0γ∆0
λµρ

y1 −

+

2γσ2
ρη0

,

Θ1 = E

F (x1) +

(cid:2)

F (x1) +

≤

5L2

y1 −
k
γ
ρη0 (cid:0)

2 +
y∗(x1)
k

2 +
v1 − ∇yg(x1, y1)
k
k

w1 −
k

f (x1, y1)

¯
∇

2

R1k

−

(cid:1)(cid:3)

where the last inequality holds by Assumption 2. Since ηt is decreasing, i.e., η−
any 0

T , we have

t

1
T ≥

≤

≤

1
T

≤

≤

≤

≤

=

T

E

Xt=1

(cid:0)

ρ
T γηT

+

ρ2
4γ2 k

˜xt+1 −

Qt
4

T

2

xtk

Θt −

Xt=1 (cid:0)

F (x1) +

Θt+1

+

5L2

(cid:1)
0γ∆0
λµρ

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

F ∗)

F ∗)

+

+

F ∗)

−

+

+

+

5L2
0∆0
λµηT T
5L2
0∆0
λµηT T
5L2
0∆0
kλµ

2

ηtk

Rtk

Xt=1
2)σ2

T

(cid:1)
1 + c2
2(c2
T ηT

2)σ2

T

T

Xt=1

η3
t +

4
T

+

2γσ2
ρη0 −

F ∗

+

(cid:1)
2(c2

2(c2

1 + c2
T ηT
2)σ2

1 + c2
T ηT
1 + c2
T ηT

Z
1
2)σ2

2k3(c2

2σ2
η0ηT T
2σ2
η0ηT T

+

+

η3
t +

4
T 2

T

ηt

T

Xt=1
k3
m + t

dt +

ln(m + T ) +

k
(m + t)1/3 dt

T

Xt=1
4
T 2 Z
1
6k
T 2 (m + T )2/3

+

2m1/3σ2
k2

+ 2k2(c2

1 + c2

2)σ2 ln(m + T ) +

6k(m + T )1/3
T

(m + T )1/3
T

,

(cid:19)

(181)

1

η−
t

for

(182)

where the second inequality holds by the above inequality (181). Let M ′ = ρ(F (x1)
−
kγ
5L2
2)σ2 ln(m + T ) + 6k(m+T )1/3
kλµ + 2m1/3σ2
0∆0

k2 + 2k2(c2

, we have

1 + c2

T

F ∗)

+

1
T

T

E

Xt=1

(cid:2)

Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

M ′
T

≤

(cid:3)

(m + T )1/3.

According to Jensen’s inequality, we have

1
T

T

E

Xt=1

1
2

(cid:2)
T

3
T

3
T

≤ (cid:18)

=

(cid:18)

Xt=1 (cid:0)
T

Xt=1 (cid:0)

L0k
(cid:0)

yt −

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk

(cid:1)(cid:3)

L2
0
4 k

yt −

2 +
y∗(xt)
k

2
4

E

wt −
k

¯
∇

f (xt, yt)

2 +

Rtk

−

ρ2
4γ2 k

˜xt+1 −

1/2

2

xtk

(cid:19)

(cid:1)

Qt
4

+

ρ2
4γ2 k

2

xtk
˜xt+1 −
(cid:1)
√3M ′m1/6
T 1/2

√3M ′
T 1/2

≤

(m + T )1/6

≤

1/2

(cid:19)

+

√3M ′
T 1/3

,

61

(183)

Feihu and Heng

where the last inequality is due to (a + b)1/6

a1/6 + b1/6 for all a, b > 0. Thus we have

≤

≤
wt −
Since
k
we can obtain

T

1
T

E

yt −
L0k
Xt=1
(cid:2)
2√3M ′m1/6
T 1/2

+

f (xt, yt)

¯
∇

−

T

E

yt −

L0k
Xt=1
(cid:2)
2√3M ′m1/6
T 1/2

2√3M ′m1/6
T 1/2

1
T

≤

=

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk
(cid:3)

2√3M ′
T 1/3
Rtk ≥ k

.

(184)

wt −

¯
∇

f (xt, yt)

Rtk

k − k

, by the above inequality (184),

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)
k

+

ρ
γ k

˜xt+1 −

xtk
(cid:3)

+

+

2√3M ′
T 1/3

2√3M ′
T 1/3

+

+

√2
T

T

Xt=1

Rtk
k

√2
T

,

(185)

where the last inequality is due to
in Algorithm 4.

Rtk
k

= 1

T for all t

≥

1 by choosing K = Lg

µ log(CgxyCf yT /µ)

Theorem 34 Under the above Assumptions (6, 3, 4, 7, 8), in the Algorithm 4, given
Bt = btIp (ˆb

b > 0), ηt =

k

bt ≥

≥
2, k3, (c1k)3, (c2k)3

max

(cid:0)
0 < γ

min

≤

(cid:0)

2q24L2

, k > 0, c1 ≥
(cid:1)
√6λµρ
8λ2µ2+125ˆb2L2

0κ2

(cid:1)

(m+t)1/3 for all t
3k3 + 125L2

≥
6µ2 , c2 ≥
and K = Lg

0, αt+1 = c1η2
3k3 + 9

t , βt+1 = c2η2
t , m
15bL2
b
0
2 , 0 < λ
8µ ,
16L2
6Lg
µ log(CgxyCf yT /µ), we have

, m1/3ρ
4Lk

min

≤

(cid:0)

2

2

0

≥
,

(cid:1)

E[

Gt]

2√3M ′m1/6
T 1/2

+

2√3M ′
T 1/3

+

√2
T

,

≤

(186)

F ∗)

kλµ + 2m1/3σ2
0∆0
g + L2
K.

where M ′ = ρ(F (x1)
−
kγ
2 and L2
y∗(x1)
y1 −
k
k
Proof Here we consider convergence properties of our saBiAdam algorithm with the general
bt ≥
adaptive matrix At ≻
b > 0) for variable y. This proof basically follows the proof of Theorem 33.

ρId for variable x and the global adaptive matrix Bt = btIp (ˆb

2)σ2 ln(m + T ) + 6k(m+T )1/3

k2 + 2k2(c2

8 = L2

1 + c2

, ∆0 =

≥

T

T

1
T

Xt=1
+ 5b1L2

2

3k3 + 125L2

0

6µ2 , we have

Let c1 ≥
1
E
ηt

k∇yg(xt+1, yt+1)
125L2
0ηt
E
6µ2

≤ −

k∇yg(xt, yt)

vtk

−

1
ηt

1

E

−
2 + 4L2

gηt

2

vt+1k

−

−

k∇yg(xt, yt)

2

vtk

−

(187)

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

1η3

t σ2.

(cid:0)

62

BiAdam: Fast Adaptive Bilevel Optimization Methods

Let c2 ≥

2

3k3 + 9

2 , we have

E

1
ηt

≤ −

wt+1 −
k
9ηt
E
2

f (xt+1, yt+1)

¯
∇

2

Rt+1k

−

−

E

wt −
k

¯
∇

1

f (xt, yt)

2

Rtk

−

(188)

wt −
k

f (xt, yt)

¯
∇

−

Rtk

2 + 4L2

˜xt+1 −
k

2 +

xtk

˜yt+1 −
k

2

ytk

(cid:1)

+ 2c2

2η3

t σ2.

(cid:0)

1
ηt

−
K ηt

According to Lemmas 19 and 20, we have

F (xt)

F (xt+1)
2ηtγ

−
wt −

ρ k

¯
∇

2 +
f (xt, yt)
k

L2
0ηtγ
ρ

4ηtγ

wt −

f (xt, yt)

¯
∇

2 +

Rtk

−

ρ k

≤

≤

2

y∗(xt)
k
4ηtγ

Rtk

ρ k

−

ytk
2 +

−
L2
0ηtγ
ρ

ρηt
2γ k

˜xt+1 −

(189)

2

xtk

y∗(xt)
k

−

2

ytk

−

ρηt
2γ k

˜xt+1 −

2.
xtk

According to Lemma 22, we have

yt+1 −
k

2
y∗(xt+1)
k
− k
2
y∗(xt)
k

yt −

ηtµλ
4bt k

yt −

−

2
y∗(xt)
k
3ηt
˜yt+1 −
4 k

2 +

ytk

25ηtλ
6µbt k∇yg(xt, yt)

2 +

vtk

−

≤ −

(190)

25κ2ηtbt

6µλ k

˜xt+1 −

2.
xtk

Next, we deﬁne a Lyapunov function, for any t

1

≥

Θt = E

F (xt) +
(cid:2)

5btL2
0γ
λµρ k

yt −

2 +
y∗(xt)
k

γ
ρηt

−

1 (cid:0)

63

2 +
vt − ∇yg(xt, yt)
k
k

wt −
k

¯
∇

f (xt, yt)

2

Rtk

−

.

(cid:1)(cid:3)

Feihu and Heng

Then we have

ytk
−
ηtµλ
4bt k
0ηt

E

Θt

Θt+1 −
= F (xt+1)

≤

E

+

+

1
ηt
−
1
−
L2
0ηtγ
y∗(xt)
ρ
k
5btL2
0γ
λµρ (cid:18) −
125L2
6µ2

wt −
k

γ
ρ (cid:18) −
9ηt
2
γηt
L2
yt −
0k
4ρ (cid:18)
15btL2
0γ
4λµρ −

E

¯
∇

(cid:0)
γηt
4ρ (cid:18)
4ηtγ

L2
0k

yt −

Rtk

ρ k

2 +

−

=

−

−

≤ −

+

5btL2
0γ
λµρ

−

F (xt) +

yt+1 −
k
(cid:0)
1
E
2 +
vt − ∇yg(xt, yt)
ηt
k
k
4ηtγ

2
y∗(xt+1)
k

yt −

− k

2
y∗(xt)
k

+

wt+1 −
k

¯
∇

f (xt+1, yt+1)

(cid:1)
Rt+1k

2

−
4ηtγ

γ
ρ (cid:18)

1
ηt
1
ηt
−
ρηt
2γ k

−

E

2
vt+1 − ∇yg(xt+1, yt+1)
k
k
E

f (xt, yt)

2

wt −
k

1

Rtk

−

(cid:19)

¯
∇

2

2 +

wt −

¯
∇

f (xt, yt)

2 +

Rtk

−

ρ k

yt −

2
y∗(xt)
k

−

3ηt
4 k

˜yt+1 −

2 +

ytk

2

Rtk

−

ρ k
25ηtλ
6µbt k∇yg(xt, yt)

˜xt+1 −

xtk
2 +

vtk

−

25κ2ηtbt

6µλ k

˜xt+1 −

2

xtk

(cid:19)

k∇yg(xt, yt)

f (xt, yt)

−

Rtk

2 + 4L2

gηt

−

vtk
2 + 4L2

Kηt

˜xt+1 −
k
2 +

xtk

(cid:0)
˜xt+1 −
k

2 + 2E
y∗(xt)
k

wt −
k

¯
∇

(cid:0)
f (xt, yt)

2

Rtk

−

2 +

ytk

4ηtγ

Rtk

ρ k

2 +

2(c2

wt −
k

f (xt, yt)

¯
∇

2

Rtk

−

4L2
8γ
ρ

˜yt+1 −
ηtk
(cid:1)
2 + 2E
y∗(xt)
k
1 + c2
ρ

2)γσ2

2(c2

η3
t ,

2 +

xtk

˜yt+1 −
k

˜yt+1 −
k
ρ
2γ −
2)γσ2

(cid:19) −
(cid:0)
1 + c2
ρ
ρηt
4γ k

(cid:19) −

η3
t

˜xt+1 −

+ 2c2

1η3

t σ2

2

ytk
(cid:1)
2η3
+ 2c2

t σ2

125b2

(cid:19)
0κ2γ
t L2
6µ2λ2ρ

2

ytk
(cid:1)
4L2
8γ
ρ −

ηtk

˜xt+1 −

2

xtk

(cid:1)

2

xtk

(191)

where the ﬁrst inequality holds by the above inequalities (187), (188), (189) and (190); the
√6λµρ
last inequality is due to 0 < γ

√6λµρ
8λ2µ2+125b2

0κ2 , 0 < λ

t L2

≤

0κ2 ≤

2√24L2

15btL2
15bL2
0
0
8µ for all t
16L2
16L2
8µ ≤
Let Qt = L2
yt −
0k
ρηt
4γ k

E[Qt] +

γηt
4ρ

≤
8λ2µ2+125ˆb2L2
2q24L2
8 = L2
g + L2
1, and L2
K.
≥
¯
2 + 2
f (xt, yt)
wt −
y∗(xt)
∇
k
k

˜xt+1 −

2

xtk

Θt −

≤

Θt+1 +

Rtk

ρ k

2 +

2)γσ2

2(c2

1 + c2
ρ

η3
t .

(192)

2, then we have

Rtk

−
4ηtγ

Taking average over t = 1, 2,

· · ·

, T on both sides of (192), we have

T

E

1
T

ηt
4

Qt +

ρ2ηt
4γ2 k

˜xt+1 −

T

2

xtk

≤

(cid:1)

Xt=1

Θt+1)

ρ(Θt −
T γ

+

4
T

T

Xt=1

ηtk

Rtk

2 +

2(c2

2)σ2

1 + c2
T

T

Xt=1

η3
t .

Xt=1

(cid:0)
Given x1 ∈ X

Θ1 = E

F (x1) +

(cid:2)

F (x1) +

≤

5b1L2
λµρ

and y1 ∈ Y
5b1L2
0γ
λµρ k
0γ∆0

y1 −

+

2γσ2
ρη0

, let ∆0 =

2 +
y∗(x1)
k

2, we have
y∗(x1)
k

y1 −
k
γ
2 +
v1 − ∇yg(x1, y1)
ρη0 (cid:0)
k
k

,

64

w1 −
k

¯
∇

f (x1, y1)

2

R1k

−

(cid:1)(cid:3)

(193)

BiAdam: Fast Adaptive Bilevel Optimization Methods

where the last inequality holds by Assumption 2. Since ηt is decreasing, i.e., η−
any 0

T , we have

t

1
T ≥

≤

≤

1
T

≤

≤

≤

≤

=

T

E

Xt=1

(cid:0)

ρ
T γηT

+

ρ2
4γ2 k

˜xt+1 −

Qt
4

T

2

xtk

Θt −

Xt=1 (cid:0)

F (x1) +

Θt+1

+

(cid:1)
5b1L2
0γ∆0
λµρ

(cid:1)
1 + c2
2(c2
T ηT

2)σ2

+

2γσ2
ρη0 −

ρ
T γηT (cid:0)
ρ(F (x1)

−
T γηT

ρ(F (x1)

−
T γηT
ρ(F (x1)
kγ

(cid:18)

F ∗)

F ∗)

+

+

F ∗)

−

+

5b1L2
0∆0
λµηT T
5b1L2
0∆0
λµηT T
5b1L2
kλµ

T

T

Xt=1

η3
t +

4
T

2

ηtk

Rtk

2(c2

T

2)σ2

Xt=1
1 + c2
T ηT
2)σ2

F ∗

+

(cid:1)
2(c2

+

+

2σ2
η0ηT T
2σ2
η0ηT T

+

+

1 + c2
T ηT
1 + c2
T ηT

Z
1
2)σ2

2k3(c2

η3
t +

4
T 2

T

ηt

T

Xt=1
k3
m + t

dt +

ln(m + T ) +

k
(m + t)1/3

T

Xt=1
4
T 2 Z
1
6k
T 2 (m + T )2/3

dt

0∆0

+

2m1/3σ2
k2

+ 2k2(c2

1 + c2

2)σ2 ln(m + T ) +

6k(m + T )1/3
T

(m + T )1/3
T

,

(cid:19)

1

η−
t

for

(194)

where the second inequality holds by the above inequality (193). Let M ′ = ρ(F (x1)
−
kγ
5b1L2

kλµ + 2m1/3σ2
0∆0

k2 + 2k2(c2

1 + c2

2)σ2 ln(m + T ) + 6k(m+T )1/3

T

, we have

F ∗)

+

1
T

T

E

Xt=1

(cid:2)

Qt
4

+

ρ2
4γ2 k

˜xt+1 −

2

xtk

M ′
T

≤

(cid:3)

(m + T )1/3.

According to Jensen’s inequality, we have

1
T

T

E

Xt=1

1
2

(cid:2)
T

3
T

3
T

≤ (cid:18)

=

(cid:18)

Xt=1 (cid:0)
T

Xt=1 (cid:0)

L0k
(cid:0)

yt −

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk

(cid:1)(cid:3)

L2
0
4 k

yt −

2 +
y∗(xt)
k

2
4

E

wt −
k

¯
∇

f (xt, yt)

2 +

Rtk

−

ρ2
4γ2 k

˜xt+1 −

1/2

2

xtk

(cid:19)

(cid:1)

Qt
4

+

ρ2
4γ2 k

2

xtk
˜xt+1 −
(cid:1)
√3M ′m1/6
T 1/2

1/2

(cid:19)

+

√3M ′
T 1/3

,

(195)

√3M ′
T 1/2

≤

(m + T )1/6

≤

where the last inequality is due to (a + b)1/6

a1/6 + b1/6 for all a, b > 0. Thus we have

≤

T

1
T

≤

E

yt −
L0k
Xt=1
(cid:2)
2√3M ′m1/6
T 1/2

+

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)

+

Rtk

−

ρ
γ k

˜xt+1 −

xtk
(cid:3)

2√3M ′
T 1/3

.

65

(196)

Feihu and Heng

wt −
Since
k
we can obtain

f (xt, yt)

¯
∇

Rtk ≥ k

wt −

−

¯
∇

f (xt, yt)

Rtk

k − k

, by the above inequality (196),

1
T

≤

=

T

E

yt −

L0k
Xt=1
(cid:2)
2√3M ′m1/6
T 1/2

2√3M ′m1/6
T 1/2

y∗(xt)
k

+ √2

wt −
k

¯
∇

f (xt, yt)
k

+

ρ
γ k

˜xt+1 −

xtk
(cid:3)

+

+

2√3M ′
T 1/3 +
2√3M ′
T 1/3

+

√2
T

T

Xt=1

Rtk
k

√2
T

,

(197)

where the last inequality is due to
in Algorithm 4.

Rtk
k

= 1

T for all t

≥

1 by choosing K = Lg

µ log(CgxyCf yT /µ)

66

