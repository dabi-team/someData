Ray-VR: Ray Tracing Virtual Reality in Falcor

Vinicius Silva
IMPA
Email: dsilva.vinicius@gmail.com

Luiz Velho
IMPA
Email: lvelho@impa.br

0
2
0
2

n
u
J

9
1

]

R
G
.
s
c
[

1
v
8
4
3
1
1
.
6
0
0
2
:
v
i
X
r
a

Abstract—NVidia RTX platform has been changing and ex-
tending the possibilities for real time Computer Graphics appli-
cations. It is the ﬁrst time in history that retail graphics cards
have full hardware support for ray tracing primitives. It still a
long way to fully understand and optimize its use and this task
itself is a fertile ﬁeld for scientiﬁc progression. However, another
path is to explore the platform as an expansion of paradigms for
other problems. For example, the integration of real time Ray
Tracing and Virtual Reality can result in interesting applications
for visualization of Non-Euclidean Geometry and 3D Manifolds.
In this paper we present Ray-VR, a novel algorithm for real
time stereo ray tracing, constructed on top of Falcor, NVidia’s
scientiﬁc prototyping framework.

I. MOTIVATION

Computer Graphics history has several examples of impor-
tant hardware milestones. They changed the way real time
algorithms could be designed and implemented and created
vast opportunities for advances in research.

One aspect of graphics cards that has been advancing con-
sistently in the last decades is shader ﬂexibility. In the begin-
ning we had graphics libraries using a ﬁxed rendering pipeline,
which could only receive data and instructions from the CPU.
No GPU side programming could be done at that time. This
aspect was changed later, with the advent of programmable
shaders. Vertex and Pixel Shaders were introduced, creating
a revolution in the possibilities for real time graphics. Later
on, those capabilities were increased with the exposition of
more programmable rendering stages [1]. Applications could
implement Tesselation and Geometry Shaders to have access to
customizable geometry resolution and primitive connectivity.
With the increment in shader ﬂexibility, several algorithms
were proposed to solve general problems using graphics hard-
ware [2]. The technique was to adapt the problem descrip-
tion to ﬁt the rendering pipeline and the Single Instruction
Multiple Data (SIMD) model that shaders use. The next step
in graphics hardware was clear: a model generalization. The
result was called General Purpose Graphics Processing Units
(GPGPU), a uniﬁed way to make general parallel computing
using buffers in graphics memory and programming languages
created speciﬁcally for that purpose [3], [4]. Examples of
such languages include CUDA [5]–[7], OpenCL [8]–[10] and
OpenACC [11].

However, GPGPU applications are hard to develop in
essence. Differently from shaders in the rendering pipeline, it
needs explicit synchronization and memory control. This more
generic model came with the costs of complexity. Nonetheless,
including but
it was explored in a vast set of problems,

not limited to Collision Detection and Response [12], [13],
Physically-based Simulation, Fluid Dynamics [14], Global
Illumination [15], Image and Video Processing [16], [17],
Segmentation [18], [19], High Performance Computing and
Clusters [20], [21], Signal Processing [22], Computer Vi-
sion [23], Neural Networks [24], Cryptography [25], Cryp-
tocurrency Mining [26], Databases [27], Big Data and Data
Science [28]. All those applications proved the model robust.
However, it was the time to migrate from hard-to-develop
in-
GPGPU to application-speciﬁc platforms for the most
teresting and important problems. Machine Learning was
already experiencing a revolution at that time, with exciting
results coming from the association of graphics hardware and
deep neural networks. The ﬁrst retail graphics cards with
application-speciﬁc hardware had deep neural network train-
ing and inference sections, called tensor cores. Additionally,
frameworks created on top of GPGPU libraries provided a
simpler API for development.

Recently, the same approach was used to create a solution
for Real time Global Illumination. The so-called RTX platform
can produce faithful images using Ray Tracing (RT), which
is historically known to have prohibitive performance for real
time applications. This landmark creates interesting opportu-
nities for new visualization applications. In particular, content
makers for Virtual Reality (VR) can greatly beneﬁt from the
added realism to create immersive, meaningful experiences.

Thus,

the demand for a VR/RT integrated solution is
clear. However, realistic VR needs stereo images for parallax
sensation. The obvious consequence is a duplication of the
performance hit caused by ray tracing. A good algorithm
should balance performance and image quality, something
that can be done using RTX Ray Tracing and a proper trace
policy. The recent announcement of ray tracing support for
older architectures [29] emphasizes even more the necessity
that
of a ﬂexible algorithm for such task. Another point
must be taken into consideration is stereo camera registration.
Depending on how the ray directions are calculated based on
camera parameters, the stereo images may diverge when seen
in a head mounted display (HMD).

This paper discusses the problem of integrating VR and
RT, proposing a ﬂexible solution. Section II describes the
technological background used. Section III contains the details
of the components needed for the VR/RT integration, which
is described in depth in Section IV. Section V contains the
evaluation of the experiments. Finally, Section VI is the
conclusion.

 
 
 
 
 
 
Fig. 1. Rendering pipeline in depth. The blue boxes are programmable shaders and the green boxes are ﬁxed stages. As in [30].

Fig. 2. Ray tracing pipeline in depth. Fixed stages are in green and programmable shaders are in blue. Modiﬁed from [30].

A. RTX Ray Tracing

II. TECHNOLOGY

NVidia RTX is a hardware and software platform with
support for real time ray tracing. The ray tracing code of an
application using this architecture consists of CPU host code,
GPU device code, the memory to transfer data between them
and the Acceleration Structures for fast geometry culling when
intersecting rays and scene objects.

Speciﬁcally, the CPU host code manages the memory ﬂow
between devices, sets up, controls and spawn GPU shaders
and deﬁnes the Acceleration Structures. On one hand, the
bottom level Acceleration Structure contains the rendering
primitives (triangles for example). On the other hand, the
top level Acceleration Structure is a hierarchical grouping of
bottom level ones. Finally, the GPU role is to run instances
of the ray tracing shaders in parallel. This is similar to the
well-established rasterization rendering pipeline.

Historically, GPUs process data in a predeﬁned rendering
pipeline, which has several programmable and ﬁxed processing
stages. The main idea is to start with a group of stages that
process the vertices, feeding a ﬁxed Rasterizer, which in its
turn generates data for pixel processing in another stage group.
Finally, the result image is output by the ﬁnal ﬁxed stage.

Currently, programmable shaders are very ﬂexible in
essence. The Vertex Shader works on the input vertices, using
transformation matrices to map them to other spaces. The Hull,
Tesselator and Domain Shaders subdivide geometry and add
detail inside graphics memory, optimizing performance. The
Geometry Shader processes primitives and mesh connectivity,
possibly creating new primitives in the process. The Fragment
Shader works on the pixels coming fromt the Rasterizer and
the Output stage outputs the resulting image. Figure 1 shows
the rendering pipeline in detail.

The ray tracing GPU device code runs in a similar pipeline
scheme. The differences are the stages taken. The goal of

the ﬁrst stages is to generate the rays. Afterwards, a ﬁxed
intersection stage calculates the intersection of the rays with
the scene geometry. Then, the intersection points are reported
to the group of shading stages. Notice that more rays can be
created at this point, resulting in a recursion in the pipeline.
The ﬁnal ﬁxed stage outputs the generated image.

The details of the pipeline are as follows. A Ray Generation
Shader is responsible for creating the rays, which are deﬁned
by their origins, directions and payloads (custom user-deﬁned
data). A call to TraceRay() launches a ray. The next stage is a
ﬁxed traversal of the Acceleration Structure, which is deﬁned
by the CPU host code beforehand. The Acceleration Traversal
uses an Intersection Shader to calculate the intersections. All
hits found pass by tests to verify if they are the closest hit or if
they must be ignored because of transparent material. In case
a transparent material is detected, the Any-Hit Shader is called
for all hits so shading can be accumulated, for example. After
no additional hits are found, the Closest-Hit Shader is called
for the closest intersection point. In case no hits are found,
the Miss Shader is called as a fallback case. It is important
to note that additional rays can be launched in the Closest-
Hit and Miss shaders. Figure 2 shows an in-depth pipeline
scheme. More detailed information about RTX Ray Tracing
can be seen in [30] and applications can be found in [31].

B. Falcor

RTX Ray Tracing can be accessed in four ways. On one
hand there are the low level APIs Vulkan [32] [33], DirectX
12 [34] [30] and OptiX [15]. They provide more ﬂexibility
but
less productivity. On the other hand there is Falcor
[35], an open-source real-time rendering framework designed
speciﬁcally for rapid prototyping. It has support for ray tracing
shaders and is the recommended way to use RTX Ray Tracing
in a scientiﬁc environment.

Falcor code and installation instructions can be found at
https://github.com/NVIDIAGameWorks/Falcor [35]. The bun-
dle comes with a Visual Studio solution, structured in two
main components: a library project called Falcor with high
level components and Sample projects which use those com-
ponents to perform computations, effects or to provide tools
for other supportive purposes.

Each Sample project consists at

least of a main class
inheriting from Renderer and a Data folder. The Renderer class
deﬁnes several relevant callbacks which can be overridden
as necessary. Examples include onLoad(), onFrameRender(),
onGuiRender(), onMouseEvent() and so forth. The Data folder
is where non-C++ ﬁles necessary for the Sample (usually
HLSL Shaders) should be placed. Falcor automatically copies
them at compilation time to the binary’s folder so programs
have no access problems.

III. RAY TRACING VR

Our goal is to build a stereo-and-ray-tracing-capable ren-
derer for VR. For this purpose, we will exploit the function-
alities of Falcor that provide support for Stereo Rendering,
Simple Ray Tracing and Global Illumination Path Tracing.

Falcor is designed to abstract scene and Acceleration Struc-
ture setup so our focus will be on describing Shader code
and the CPU host code to set it up. The next subsections
explain the logic for three Falcor Samples with the objective
of using their components later on as building blocks for our
new renderer. We refer to code in the Falcor Samples, so it
is advisable to access it in conjunction with this section for a
better understanding.

A. Simple Ray Tracer

HelloDXR is a simple ray tracer with support for mirror
the

reﬂections and shadows only. As would be expected,
Sample speciﬁes two ray types: primary and shadow.

The ray generation shader rayGen() is responsible of con-
verting pixel coordinates to ray directions. This is done by
a transformation to normalized device coordinates followed
by another transformation using the inverse view matrix and
the camera ﬁeld of view. The function TraceRay() is used
to launch the rays. The ray type index and the payload are
provided as parameters.

The Closest-Hit Shader primaryClosestHit () calculates the
ﬁnal color of the pixel. It has two components: an indirect
reﬂection color and a direct color. The reﬂection color is
calculated by getReﬂectionColor () , which reﬂects the ray
direction using the surface normal and shoots an additional ray
in that direction. The payload has a ray depth value used to
limit recursion. The direct color is the sum of the contributions
of each light source at the pixel, conditioned by the shadow
check checkLightHit () . If the light source is not occluded, the
contribution is calculated by evalMaterial () , a Falcor built-in
function to shade pixels based on materials.

B. PathTracing

The PathTracer Sample implements the Path Tracing algo-
rithm [36]. It uses a RenderGraph to chain four rendering
steps: G-Buffer rasterization, global illumination, accumula-
tion and tone mapping. The graph is deﬁned in CPU host
code, using addPass() and addEdge() to create the passes and
links their input and output respectively.

Each pass has its own Shaders. The G-Buffer pass uses the
rasterization pipeline to output shading data to a set of textures.
More speciﬁcally, the built-in function prepareShadingData()
is used in a Pixel Shader to fetch and sample the material,
whose data is output to the G-Buffers. Falcor default fallback
Vertex Shader is used in this step.

The global illumination pass calculates direct and indirect
contributions as well as shadows. It uses a ray generation
shader and two ray types: indirect and shadow. Each type
is deﬁned in host code by a hit group (a closest-hit and
an any-hit shader) and a miss shader. The setup is done
using a descriptor and calling addHitGroup() and addMiss()
respectively. Method setRayGen() is called to deﬁne the ray
generation shader as well. Each ray type must have a unique
index, which is referenced in the shaders. Shadow rays have
index 0 and indirect ones have index 1.

The ray generation shader controls the path tracing. Brieﬂy,
the coordinates of each pixel are used to compute random
number generator seeds, which are used to calculate random
directions for indirect rays. The indirect sample is chosen
randomly from the diffuse hemisphere or specular direction.
An analogous idea is used for direct lighting as well, which
is computed by randomly choosing a light source to check for
visibility. As shown in [36], this integration converges to the
complete evaluation of the illumination of the scene.

Now, the Hit Group and Miss Shaders are described. The
shadow ray Shaders are very simple. The Miss Shader sets the
ray visibility factor to 1 from the default 0, which means that
the ray origin is visible to the light and should be lit by it. The
Any-Hit shader just checks if the ray origin has a transparent
material using the built-in evalRtAlphaTest() function. Case
it is a transparent material, the hit is ignored so the ray can
continue its path. There is no Closest-Hit Shader for shadows
since the visibility factor should change only if no objects are
hit.

Analogously, indirect rays have their own Shaders. The
main difference between the ray types is the Closest-Hit
Shader, which calculates the direct light at the intersection
point and shoot an additional ray case the depth is bellow
the global threshold. The Miss Shader samples a color from
the environment map, indexed by the ray direction, and the
Any-Hit Shader is equal to the shadow ray’s.

The accumulation pass is also very simple. Its CPU code
maintains a texture with the previous frame and ensures that
accumulation is done only when the camera is static. This
image is accumulated with the current frame, coming from
the global illumination pass. The accumulation consists of an
incremental average.

planes transformed. The near plane is at the square with top-
left corner at (0,0,0) and bottom-right corner at (1,1,0) and
the far plane is at the square with top-left corner at (0,0,1)
and bottom-right corner at (1,1,1). Finally, the raster space is
the normalized device coordinate space scaled by the image
resolution. Figure 3 shows how the spaces relate to each other.

Finally, Falcor built-in tone mapping is used to adjust the

colors of the image. Class ToneMapping abstracts this pass.

C. StereoRendering

The StereoRendering Sample is an application to render
stereo image pairs using rasterization. The CPU host code
issues the
ensures connection with the HMD (initVR () ),
Shaders to generate the images and transfers them to the
device (submitStereo () ). Speciﬁcally,
it maintains a struct
containing the camera matrices and properties for both eyes.
The geometry is drawn once, but it is duplicated inside the
GPU by the Shaders. A frame buffer array with two elements is
maintained for that purpose (mVrFbo). When a frame ﬁnishes,
each array slice has the view of one eye.

The

The GPU code consists of a Vertex Shader, a Ge-
ometry Shader and a Pixel Shader. The Vertex Shader
(StereoRendering . vs . hlsl ) just passes ahead the vertex po-
sitions in world coordinates and additional rendering info
such as normals, colors, bitangent,
texture and light map
coordinates, if available.
is

Shader
responsible
(StereoRendering . gs . hlsl ), which
for duplicating the geometry.
three
vertices of a triangle and outputs six vertices. Each input
vertex is projected twice, once for each of the view-projection
matrices available at
the camera struct. The geometry for
each eye is output into the related array slice by setting a
render target index at struct GeometryOut.

also
receives as input

the Geometry

projection

left

to

is

It

Finally, the Pixel Shader (StereoRendering . ps . hlsl ) is very
simple. It just samples material data using the built-in function
prepareShadingData() and accumulates the contributions of
each light source using the built-in function evalMaterial () .

IV. INTEGRATION

Our goal is to develop a new renderer that combines the
capabilities described in the previous section. It should be
capable of stereo rendering and ray tracing in real time. This
section describes the process and the possible choices and
alternatives to address the problems encountered.

A. Stereo Convergence

Fig. 3. Several camera-related coordinate spaces: camera, normalized device
coordinates and raster. As in [37].

The transformation for a projection camera can be con-
structed in two steps. First, building a canonical perspective
matrix with distance to the near plane n and distance to the
far plane f . The projected coordinates x(cid:48) and y(cid:48) are equal to
the original ones divided by the z coordinate. z(cid:48) is remapped
so the values in the near plane have z(cid:48) = 0 and the values in
the far plane have z(cid:48) = 1:

x(cid:48) = x/z
y(cid:48) = y/z

z(cid:48) =

f (z − n)
z(f − n)

.

This operation can also be encoded as a 4x4 matrix using

homogeneous coordinates:


1
0


0

0

0
1
0
0

0
0
f
f −n
1







0
0
−f ∗n
(f −n)
0

One key problem of integrating VR and RT is the stereo
image registration. Depending on how this process is done,
the images may diverge and it can be impossible for the
human vision to focus correctly on the scene objects. This
phenomenon may result in viewer discomfort or sickness.

To understand the ray generation process it is good to think
about perspective projection and the several related spaces it
involves. [37] contains an exceptional explanation of this topic,
which will be summarized here.

Conceptually, the process consists of a chain of transforma-
tions starting at the world space, passing through the camera
space and the normalized device coordinate space and ending
at the raster space. The camera space is the world space with a
translated origin to the camera position. The normalized device
coordinate space is the camera space with the near and far

As a side note, the original position of the projection planes
would be important for rasterization because the map of z to
z(cid:48) is not linear, what could possibly result in numerical issues
at depth test, for example. However, we are only interested in
the projection directions for ray tracing, thus those distances
can be totally arbitrary.

The second step is scaling the matrix so points inside the
ﬁeld of view project map to coordinates between [−1, 1] on
the view plane. For square images, both x and y lie between
the expected interval after projection. Otherwise, the direction
in which the image is narrower maps correctly, and the wider
direction maps to a proportionally larger range of screen space
values. The scaling factor that maps the wider direction to the
range [−1, 1] can be computed using the tangent of half of the
ﬁeld of view angle. More precisely, it is equal to:

1
tan( f ov
2 )

,

as can be seen in Figure 4.

Fig. 4. Relation between a ﬁeld of view and normalized device coordinates.
As in [37].

To launch rays from pixels we use the inverse transforma-
tion chain. We start at the raster space, passing through the
normalized device coordinate and camera spaces and ending at
the world space. More speciﬁcally, to compute a ray direction
we must convert the raster coordinates of its associated pixel
to normalized device coordinates, scale by the reciprocal of the
factor used to map the ﬁeld of view to the range [−1, 1] and
use the inverse view transformation matrix to map the result
to the world space. The conversion from raster coordinates
r ∈ [dx, dy] to normalized device coordinates n ∈ [−1, 1],
given the image dimensions d = (dx, dy), is expressed by the
following equation:

n =

2(r + 0.5)
d

− 1;

which is composed of a normalization by the image dimen-
sions and operations to map the resulting image space from
the interval [0, 1] to [−1, 1].

The remains of the transformation chain can be done in a
optimized way, using a precomputed tangent of half the ﬁeld
of view angle f (in dimension y), the aspect ratio a and the
basis vectors of the inverse view matrix I = [u|v|w]. The
operation is done by the expression:

normalize(af (nxu) − (f (nyv)) − w),

As can be seen, this expression transforms the normalized
device coordinates using the parts of the inverse view matrix
that would affect each of the coordinates and scales them using
the tangent of the ﬁeld of view for dimension y. The scale
value is corrected by the aspect ratio for the x dimension.

In our tests we could not generate correctly registered
stereo images using this optimized expression, because it
does not take into account stereo rendering. For this reason,
we used two other approaches: the inverse of the projection
matrix, and a rasterization G-Buffer pre-pass. Both options
ensure correct stereo images, with different pros and cons.
The ﬁrst option does not need any additional rasterization

pass or memory for the required texture. However, the G-
Buffer provides more ﬂexibility for the algorithm as will be
discussed in Section IV-B. It is important to note that the
positions in the texture are equivalent to intersection points of
rays launched from the camera position. This property comes
from the fact that the camera position is equivalent to the
projection center and each ray is equivalent to the projection
line for the associated pixel.

B. Ray Tracing Overhead

The major drawback of usual stereo rendering is the over-
head caused by the additional camera image. This problem is
emphasized even more in ray tracing, which demands heavy
computation to generate the images. Several techniques have
been proposed to address this issue. They usually create the
additional image by transforming the contents of the original
or by temporal coherence using previous frames. However,
artifacts not present when the scene is rendered twice can be
introduced in the process.

The RTX platform opens new ways to explore this problem.
Additionally, the extension of the ray tracing support for older
graphics card architectures [29] encourages new algorithms
based on smart ray usage. We beneﬁt from Falcor’s design
to explore and evaluate the possibilities using a methodology
based on fast cycles of research, prototyping, integration and
evaluation. The result is a list of several possible approaches,
generated by changing component routines of a ray tracing al-
gorithm. In summary, those changes result from the following
questions.

1) How the ﬁrst intersections are calculated?
a) Rays shot from camera position.
b) G-Buffer pre-pass.
2) Which effects are applied?

a) Direct light and shadows only.
b) Perfect-mirror specular reﬂections and shadows

only.

c) Path tracing.

The different algorithms are created by combining the
different functionalities described in Section III. We start by
integrating Simple Ray Tracing and Stereo Rendering. On one
hand, Stereo Rendering includes all logic to connect with the
HMD and to control the data ﬂow between the ray tracing
shaders and the device. On the other hand, Simple Ray Tracing
features a ray tracing shader, which is modiﬁed to launch rays
based on two view matrices or two position G-Buffers, one
for each eye. On the G-Buffer case a rasterization pre-pass is
also performed, analogously to the Path Tracing.

Next, PathTracer’s components are integrated, enabling bet-
ter control of the effects applied. We beneﬁt from Falcor’s
RenderGraph, which is extremely useful for algorithms with
multiple rendering passes. The changes needed are listed next.
1) Adding an additional mirror ray type, equivalent to the

primary ray type.

2) Including a function to compute direct light with shad-
ows only. If the G-Buffer is available, the direct contri-
bution comes for free from it.

3) Adding a branch in the Ray Generation Shader to choose
between the effects: raster, direct light plus shadows,
specular reﬂections and path tracing.

Tables I and II shows the results for the Temple and the
Bistro respectively. The Arcade runs at 90 fps in any case
because of its simplicity.

An interesting question arises when we analyse the current
algorithm. A ray tracing procedure with a G-Buffer pre-pass
is actually a hybrid algorithm based on both rasterization
and ray tracing. What if we extrapolate this hybrid paradigm
and allow materials to be raster or ray-traced in the scene?
This question generated an additional change in the integrated
renderer. We introduced a material ID to enable support for
per-material effect selection. With this feature, an user can
control performance by changing the material IDs of objects in
the scene from more complex to simpler effects and vice versa.
The ﬁnal algorithm is very ﬂexible and suited for stereo ray
tracing or for older graphics card architectures, environments
where performance matters.

V. EVALUATION

In Section IV we pointed out component routines that could
be changed in a ray tracing algorithm to achieve different
levels of performance and image quality. In this section we
are interested in measure and evaluate those changes. Our goal
is to ﬁnd the best solution of compromise between those two
indices.

The evaluation methodology consists of several tests using
three scenes: Falcor Arcade [35], Epic Games Unreal Engine
4 Sun Temple [38] and Amazon Lumberyard Bistro [39]. The
Arcade is a very simple scene with minimal geometry, used as
a toy example. The Temple and the Bistro are part of the Open
Research Content Archive (ORCA) and are more dense, with
sizes comparable to scenes actually encountered in games and
VR experiences.

All tests are done in a PC with an Intel(R) Core(TM) i7-
8700 CPU, 16GB RAM, a RTX 2080 and a HTC Vive. The
target performance is 90 fps, the HMD update frequency.
Depending on how far the application is from this value, the
device starts to reproject images and to lose frames, what can
result in motion sickness for the user. Everything that could
perform above 90 fps is also capped to that value. In our tests,
users reported that performances near 45 fps are good, with
minimal hickups noticed due to reprojection. Timings below
this value started to feel uncomfortable. The values include
the image transfer to the HMD.

A. G-Buffers and Stereo

The ﬁrst test evaluates the impact of the G-Buffer pre-pass
and stereo rendering. As discussed in Section IV, the hybrid
algorithm depends on G-Buffers, so it is important to assess
their viability early on. The methodology is:

1) We have two controls in the experiment: a mono raster

shader and a stereo raster shader.

2) All ray tracing materials use the mirror-like reﬂection

shader with one bounce.

3) When stereo is not enabled, only the images for the left

eye are generated.

Algorithm
Raster
Raster
Inverse matrix
Inverse matrix
G-Buffer
G-Buffer

Stereo
No
Yes
No
Yes
No
Yes

FPS
90
90
90
45
90
45

TABLE I
G-BUFFER PRE-PASS AND STEREO IMAGING IMPACT FOR TEMPLE.

Algorithm
Raster
Raster
Inverse matrix
Inverse matrix
G-Buffer
G-Buffer

Stereo
No
Yes
No
Yes
No
Yes

FPS
80
45
80
45
80
45

TABLE II
G-BUFFER PRE-PASS AND STEREO IMAGING IMPACT FOR BISTRO.

As expected, the stereo rendering is the bottleneck and the
G-Buffer overhead is negligible in comparison with it. Thus,
our proposal is to use the hybrid algorithm to customize scenes
and balance the indices. The next step is to measure how
materials interact with them.

B. Materials

The second test focuses on material effects. The idea is to
use the hybrid algorithm and change the material ids on-the-ﬂy
to balance quality and performance. The methodology is:

1) Stereo is always enabled.
2) The camera position is ﬁxed.
3) Materials are changed to balance image quality and

performance. There are three possibilities.

a) Raster.
b) Raster and ray-traced shadows.
c) Ray-traced mirror-like reﬂections and shadows.

The application supports an additional Path Tracing shader.
However, as discussed in Section III-B, the algorithm needs
to accumulate samples over frames from a static camera to
eliminate noise. This restriction is hard to be imposed in a
VR experience, where the camera is controlled by the user’s
head movement. Thus, we will not be using this shader in the
experiments.

The Arcade stays at 90 fps regardless of effect choice so we
only show the best result in Figure 5, obtained when using the
ray tracing shader with shadows and reﬂections. In Figure 6
we show the results for the Sun Temple, while Figure 7 contain
the results for the Bistro. Moreover, Tables III and IV quantify
the performance for each case.

Effect
Raster
Raster + ray-traced shadows
Ray-traced reﬂections on statues and wall decorations
Ray-traced Reﬂections on everything

FPS
90
90
75
45

TABLE III
EFFECT IMPACT FOR THE SUN TEMPLE.

Effect
Raster
Raster + ray-traced shadows
Ray-traced Reﬂections and shadows

FPS
45
45
45

TABLE IV
EFFECT IMPACT FOR THE BISTRO.

Fig. 5. Arcade: ray-traced reﬂections and shadows.

Fig. 6.
everything.

Sun Temple: ray-traced mirror-like reﬂections and shadows on

The use of effects drastically changes the mood and ﬁdelity
of the scenes, resulting in a much better immersive experience.
It is important to remark that Tables III and IV only show a
small subset of all possible material setups. Those are the ones
with minimal interaction to change the ids. However, with
proper material id tweak, the application can achieve even
faster frame rates while maintaining image quality. We could
think an automatic algorithm to set ids based on importance
values given by artists and distance of objects from the camera.
Non-important or very distant objects could be set as raster or
raster plus shadows, instead of other heavier materials.

Fig. 7. Bistro: ray-traced mirror-like reﬂections and shadows.

VI. CONCLUSION

In this paper we presented Ray-VR, a very ﬂexible al-
gorithm to integrate real time RT in VR. At the time of
manuscript submission, even Epic Games and Unity Technolo-
gies, big players in the VR market, do not have support for
real time VR/RT in their game engine solutions. As far as we
know, Ray-VR is the ﬁrst algorithm to be successful at such
task.

Ray-VR performance is very ﬂexible in essence. It can
adapt a VR experience to different hardware constraints. High
performance devices can beneﬁt from high quality ray-traced
images, creating more immersive environments. However,
other devices can still run the experience in real time, but
with less effects.

The algorithm is also totally compatible with current VR
creation workﬂow. The user interaction needed to change
material
ids is straight-forward, suited for artists at asset
creation time, for developers at development time and for
designers at testing time.

The human interaction needed to change the material ids
can also be considered a limitation, however. Ideally, we
want an algorithm that changes the material ids automatically.
With this in mind, we brieﬂy described improvements that
could converge to a solution in Section V-B. The artists could
assign importance values to the assets at creation time. This
methodology in conjunction with other heuristics such as
object distance, for example, could result in an automatic
algorithm for material id setup. It could also optimize the
importance value based on the original ones given by the artists
and a given fps budget. Ray-VR is ﬂexible enough to support
such operations after small changes in the current algorithm.
An intuitive example is the statue at the Sun Temple. It is
by far the most important asset in the scene and could have a
high importance value. Walls for example, could receive much
less attention, since they usually are part of the background
of the scene. A more sophisticate attempt could be to create a
neural network to learn how to set the material ids with scene
examples in order to optimize performance and image quality.

[23] Y. Allusse, P. Horain, A. Agarwal, and C. Saipriyadarshan, “Gpucv: an
opensource gpu-accelerated framework forimage processing and com-
puter vision,” in Proceedings of the 16th ACM international conference
on Multimedia. ACM, 2008, pp. 1089–1092.

[24] H. Jang, A. Park, and K. Jung, “Neural network implementation using
cuda and openmp,” in 2008 Digital Image Computing: Techniques and
Applications.

IEEE, 2008, pp. 155–161.

[25] S. A. Manavski, “Cuda compatible gpu as an efﬁcient hardware accel-
erator for aes cryptography,” in 2007 IEEE International Conference on
Signal Processing and Communications.

IEEE, 2007, pp. 65–68.

[26] M. B. Taylor, “Bitcoin and the age of bespoke silicon,” in 2013
International Conference on Compilers, Architecture and Synthesis for
Embedded Systems (CASES).

IEEE, 2013, pp. 1–10.

[27] P. Bakkum and K. Skadron, “Accelerating sql database operations on a
gpu with cuda,” in Proceedings of the 3rd Workshop on General-Purpose
Computation on Graphics Processing Units. ACM, 2010, pp. 94–103.
[28] X.-W. Chen and X. Lin, “Big data deep learning: challenges and

perspectives,” IEEE access, vol. 2, pp. 514–525, 2014.

Burnes,

[29] A.

Real-Time

“Accelerating

Tracing
The
Ecosystem: DXR For GeForce RTX and GeForce GTX,” Mar.
2019. [Online]. Available: https://www.nvidia.com/en-us/geforce/news/
geforce-gtx-ray-tracing-coming-soon/
[30] C. Wyman, S. Hargreaves, P. Shirley,

and C. Barr-Brisebois,
“Introduction to DirectX Raytracing,” in ACM SIGGRAPH 2018
Courses, Aug. 2018. [Online]. Available: http://intro-to-dxr.cwyman.org/
[31] E. Haines and T. Akenine-Mller, Eds., Ray Tracing Gems. Apress,

Ray

2019.

[32] M. Bailey, “Introduction to the vulkan graphics api,” in ACM
New York,
[Online]. Available: http:

SIGGRAPH 2018 Courses,
NY, USA: ACM, 2018, pp. 3:1–3:146.
//doi.acm.org/10.1145/3214834.3214848

ser. SIGGRAPH ’18.

[33] G. Sellers and J. Kessenich, Vulkan Programming Guide: The
Ofﬁcial Guide to Learning Vulkan, ser. Always learning. Addison-
Wesley, 2016. [Online]. Available: https://books.google.com.br/books?
id=kUJujwEACAAJ

[34] F. Luna, Introduction to 3D Game Programming with DirectX 12. USA:

Mercury Learning & Information, 2016.

[35] N. Benty, K.-H. Yao, T. Foley, M. Oakes, C. Lavelle,
C. Wyman, The Falcor Rendering Framework, 2018.
Available: https://github.com/NVIDIAGameWorks/Falcor

and
[Online].

[36] J. T. Kajiya, “The rendering equation,” in Proceedings of the 13th
Annual Conference on Computer Graphics and Interactive Techniques,
ser. SIGGRAPH ’86. New York, NY, USA: ACM, 1986, pp. 143–150.
[Online]. Available: http://doi.acm.org/10.1145/15922.15902

[37] M. Pharr, W. Jakob, and G. Humphreys, Physically based rendering:

From theory to implementation. Morgan Kaufmann, 2016.

[38] E.

Games,

“Unreal

engine

sun

content

research
http://developer.nvidia.com/orca/epic-games-sun-temple.
[Online]. Available: http://developer.nvidia.com/orca/epic-games-sun-temple

(orca),”

archive

temple,
October

open
2017,

[39] A.

Lumberyard,

content

research
http://developer.nvidia.com/orca/amazon-lumberyard-bistro.
[Online]. Available: http://developer.nvidia.com/orca/amazon-lumberyard-bistro

“Amazon
archive

lumberyard
(orca),”

bistro,
July

open
2017,

REFERENCES

[1] R. Fernando et al., GPU gems: programming techniques, tips, and tricks
for real-time graphics. Addison-Wesley Reading, 2004, vol. 590.
[2] M. Pharr and R. Fernando, Gpu gems 2: programming techniques
for high-performance graphics and general-purpose computation.
Addison-Wesley Professional, 2005.

[3] D. Luebke, M. Harris, N. Govindaraju, A. Lefohn, M. Houston,
J. Owens, M. Segal, M. Papakipos, and I. Buck, “Gpgpu: General-
purpose computation on graphics hardware,” in Proceedings of
the 2006 ACM/IEEE Conference on Supercomputing,
ser. SC
’06. New York, NY, USA: ACM, 2006.
[Online]. Available:
http://doi.acm.org/10.1145/1188455.1188672

[4] H. Nguyen, Gpu gems 3. Addison-Wesley Professional, 2007.
[5] J. Nickolls, I. Buck, M. Garland, and K. Skadron, “Scalable parallel
programming with cuda,” Queue, vol. 6, no. 2, pp. 40–53, Mar. 2008.
[Online]. Available: http://doi.acm.org/10.1145/1365490.1365500
[6] J. Sanders and E. Kandrot, CUDA by example: an introduction to
general-purpose GPU programming, portable documents. Addison-
Wesley Professional, 2010.

[7] S. Cook, CUDA Programming: A Developer’s Guide to Parallel Com-
puting with GPUs, 1st ed. San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 2013.

Symposium (HCS).

[8] A. Munshi, “The opencl speciﬁcation,” in 2009 IEEE Hot Chips 21
IEEE, 2009, pp. 1–314.
[9] J. E. Stone, D. Gohara, and G. Shi, “Opencl: A parallel programming
standard for heterogeneous computing systems,” Computing in science
& engineering, vol. 12, no. 3, p. 66, 2010.

[10] A. Munshi, B. Gaster, T. G. Mattson, and D. Ginsburg, OpenCL

programming guide. Pearson Education, 2011.

[11] S. Wienke, P. Springer, C. Terboven, and D. an Mey, “Openaccﬁrst
experiences with real-world applications,” in European Conference on
Parallel Processing. Springer, 2012, pp. 859–870.

[12] C. Lauterbach, Q. Mo, and D. Manocha, “gproximity: hierarchical
gpu-based operations for collision and distance queries,” in Computer
Graphics Forum, vol. 29. Wiley Online Library, 2010, pp. 419–428.

[13] V. da Silva, C. Esperanc¸a, and R. G. Marroquim, “Lazy work stealing
for continuous hierarchy traversal on deformable bodies,” in 2014 In-
ternational Conference on Computer Graphics Theory and Applications
(GRAPP).

IEEE, 2014, pp. 1–8.

[14] E. Riegel, T. Indinger, and N. A. Adams, “Implementation of a lattice–
boltzmann method for numerical ﬂuid mechanics using the nvidia cuda
technology,” Computer Science-Research and Development, vol. 23, no.
3-4, pp. 241–247, 2009.

[15] S. G. Parker, J. Bigler, A. Dietrich, H. Friedrich, J. Hoberock,
D. Luebke, D. McAllister, M. McGuire, K. Morley, A. Robison, and
M. Stich, “OptiX: A General Purpose Ray Tracing Engine,” in ACM
SIGGRAPH 2010 Papers, ser. SIGGRAPH ’10. New York, NY, USA:
ACM, 2010, pp. 66:1–66:13, event-place: Los Angeles, California.
[Online]. Available: http://doi.acm.org/10.1145/1833349.1778803
[16] Z. Yang, Y. Zhu, and Y. Pu, “Parallel image processing based on cuda,”
in 2008 International Conference on Computer Science and Software
Engineering, vol. 3.
IEEE, 2008, pp. 198–201.

[17] A. Colic, H. Kalva, and B. Furht, “Exploring nvidia-cuda for video
coding,” in Proceedings of the ﬁrst annual ACM SIGMM conference on
Multimedia systems. ACM, 2010, pp. 13–22.

[18] L. Pan, L. Gu, and J. Xu, “Implementation of medical image seg-
mentation in cuda,” in 2008 International Conference on Information
Technology and Applications in Biomedicine.
IEEE, 2008, pp. 82–85.
[19] C. Y. Ren and I. Reid, “gslic: a real-time implementation of slic super-
pixel segmentation,” University of Oxford, Department of Engineering,
Technical Report, 2011.

[20] M. Fatica, “Accelerating linpack with cuda on heterogenous clusters,”
in Proceedings of 2nd Workshop on General Purpose Processing on
Graphics Processing Units. ACM, 2009, pp. 46–51.

[21] D. Jacobsen, J. Thibault, and I. Senocak, “An mpi-cuda implementation
for massively parallel incompressible ﬂow computations on multi-gpu
clusters,” in 48th AIAA Aerospace Sciences Meeting Including the New
Horizons Forum and Aerospace Exposition, 2010, p. 522.

[22] M. Ujaldon and U. V. Catalyurek, “High-performance signal process-
ing on emerging many-core architectures using cuda,” in 2009 IEEE
International Conference on Multimedia and Expo.
IEEE, 2009, pp.
1825–1828.

