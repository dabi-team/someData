Noname manuscript No.
(will be inserted by the editor)

Teacher-Student Training and Triplet Loss to Reduce the
Eﬀect of Drastic Face Occlusion
Application to Emotion Recognition, Gender Identiﬁcation and Age Estimation

Mariana-Iuliana Georgescu · Georgian-Emilian Dut¸˘a · Radu Tudor
Ionescu

1
2
0
2

v
o
N
0
2

]

V
C
.
s
c
[

1
v
1
6
5
0
1
.
1
1
1
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract We study a series of recognition tasks in two
realistic scenarios requiring the analysis of faces under
strong occlusion. On the one hand, we aim to recog-
nize facial expressions of people wearing Virtual Reality
(VR) headsets. On the other hand, we aim to estimate
the age and identify the gender of people wearing sur-
gical masks. For all these tasks, the common ground
is that half of the face is occluded. In this challeng-
ing setting, we show that convolutional neural networks
(CNNs) trained on fully-visible faces exhibit very low
performance levels. While ﬁne-tuning the deep learn-
ing models on occluded faces is extremely useful, we
show that additional performance gains can be obtained
by distilling knowledge from models trained on fully-
visible faces. To this end, we study two knowledge dis-

This work was supported by a grant of the Romanian
Ministry of Education and Research, CNCS - UEFISCDI,
project number PN-III-P1-1.1-TE-2019-0235, within PNCDI
III. This article has also beneﬁted from the support of the
Romanian Young Academy, which is funded by Stiftung Mer-
cator and the Alexander von Humboldt Foundation for the
period 2020-2022.

M.I. Georgescu
SecurifAI, Bd. Mircea Vod˘a 21D, Bucharest, Romania.
Department of Computer Science, University of Bucharest,
14 Academiei, Bucharest, Romania.
E-mail: georgescu lily@yahoo.com

G.E. Dut¸˘a
SecurifAI, Bd. Mircea Vod˘a 21D, Bucharest, Romania.
Department of Computer Science, University of Bucharest,
14 Academiei, Bucharest, Romania.
E-mail: georgian.duta@securifai.ro

R.T. Ionescu
SecurifAI, Bd. Mircea Vod˘a, 21D Bucharest, Romania.
Department of Computer Science and Romanian Young
Academy, University of Bucharest, 14 Academiei, Bucharest,
Romania.
E-mail: raducu.ionescu@gmail.com

tillation methods, one based on teacher-student train-
ing and one based on triplet loss. Our main contribu-
tion consists in a novel approach for knowledge distil-
lation based on triplet loss, which generalizes across
models and tasks. Furthermore, we consider combining
distilled models learned through conventional teacher-
student training or through our novel teacher-student
training based on triplet loss. We provide empirical ev-
idence showing that, in most cases, both individual and
combined knowledge distillation methods bring statis-
tically signiﬁcant performance improvements. We con-
duct experiments with three diﬀerent neural models
(VGG-f, VGG-face, ResNet-50) on various tasks (fa-
cial expression recognition, gender recognition, age es-
timation), showing consistent improvements regardless
of the model or task.

1 Introduction

We aim to study and develop a generic framework suit-
able for solving various recognition tasks requiring the
analysis of faces under strong occlusion. We underline
that the studied framework could be useful in several
realistic scenarios. In this work, we focus on two spe-
ciﬁc yet diverse scenarios to demonstrate the practi-
cal applicability of our framework. Our ﬁrst scenario is
related to the recognition of facial expressions of peo-
ple wearing Virtual Reality (VR) headsets. A system
able to solve this task with high accuracy provides the
means to control and change the VR environment with
respect to the user’s emotions, in real time. This could
be useful for adjusting the level of exposure for VR ap-
plications designed for the treatment of various types
of phobia. Our second scenario stems from the regu-
lations imposed by many countries around the world

 
 
 
 
 
 
2

Mariana-Iuliana Georgescu et al.

to minimize the spread of the SARS-CoV-2 virus, re-
quiring people in public indoor and even outdoor en-
vironments to wear surgical masks. In the context of
the COVID-19 pandemic, estimating the age and iden-
tifying the gender of customers wearing surgical masks
is very useful to generate customer demographics for
retail stores and supermarkets. Such demographics are
necessary for businesses to estimate the impact of ad-
vertisement campaigns or to create strategic plans with
respect to current trends in customer demand.

In the scenarios enumerated above, the common de-
nominator is the fact that the automated analysis needs
to be performed on faces with an occlusion rate of about
50%, i.e. either the upper half or the lower half of the
face is occluded. We consider this level of occlusion as
drastic, given the signiﬁcant performance damage im-
plied when a deep convolutional neural network (CNN)
trained on completely-visible faces is applied on half-
visible faces. Perhaps the most natural solution to close
the performance gap is to ﬁne-tune the model on oc-
cluded faces. Yet, we conjecture that the performance
gap can be reduced even further by distilling knowledge
from a teacher model trained on fully-visible faces into
a student model ﬁne-tuned on partially-visible faces. In
this work, we study a conventional knowledge distilla-
tion technique based on teacher-student training [30,59]
as well as a newly developed technique based on triplet
loss [17]. In our novel knowledge distillation framework,
we formulate the objective such that the model reduces
the distance between an anchor embedding, produced
by a student CNN that takes occluded faces as in-
put, and a positive embedding (from the same class
as the anchor), produced by a teacher CNN trained
on fully-visible faces, so that it becomes smaller than
the distance between the anchor and a negative embed-
ding (from a diﬀerent class than the anchor), produced
by the student CNN. In addition, we consider combin-
ing distilled student models, learned with conventional
teacher-student training or triplet loss, into ensemble
models.

We conduct experiments on multiple tasks and data
sets to demonstrate the generality of using knowledge
distillation to reduce the negative eﬀect of face occlu-
sion on accuracy. More precisely, we present facial ex-
pression recognition results on AﬀectNet [51] and FER+
[5], gender identiﬁcation results on UTKFace [83] and
age estimation results on UTKFace. To simulate occlu-
sions, we replace the pixel values in the upper or lower
half of an image with zeros, blacking out the corre-
sponding region as necessary. To demonstrate the gen-
erality across models, we consider two VGG architec-
tures, namely VGG-f [7] and VGG-face [55], and one
residual architecture, namely ResNet-50 [28]. Our em-

pirical results indicate that knowledge distillation yields
superior results across the investigated tasks and mod-
els.

With respect to our preliminary work [17], we make

the following contributions:

– We provide a more comprehensive description of the

proposed methods.

– We demonstrate the generality across multiple tasks,
adding gender recognition and age estimation as
new tasks.

– We demonstrate the generality across multiple mod-
els, considering both residual and VGG-like archi-
tectures.

The rest of the paper is organized as follows. Sec-
tion 2 contains an overview of the related scientiﬁc arti-
cles. The investigated methods are detailed in Section 3.
The experiments and results are described in Section 4.
Our conclusions and future work directions are provided
in Section 5.

2 Related Work

2.1 Facial Expression Recognition

Over the past few years, the research eﬀorts on facial
expression recognition have concentrated on building
and training deep neural networks aimed at obtaining
state-of-the-art results [11,18,20,27,31, 34, 42,40,44,45,
47,46,50,52,64,65,66,67, 70, 71,74]. Engineered models
based on handcrafted features [3,35, 62, 63] have attracted
comparably less attention, since such models typically
attain less accurate results than deep learning models.
In works such as [5,25], the authors adopted VGG-like
architectures. Barsoum et al. [5] proposed a CNN par-
ticularly for the FER+ benchmark, formed of 13 layers
(VGG-13). Guo et al. [25] concentrated on recognizing
facial expressions on mobile devices, designing a light-
weight VGG architecture. In order to minimize compu-
tational costs, the authors reduced the input size, the
number of ﬁlters and the number of layers, and replaced
the fully-connected layers with global average pooling.
Their architecture is formed of 12 layers divided into 6
blocks.

Although the majority of works studied facial ex-
pression recognition from static images, there is a body
of works focusing on video [27, 39]. Hasani et al. [27] de-
signed a neural architecture that is formed of 3D convo-
lutional layers followed by a Long Short-Term Memory
(LSTM) network, extracting spatial relations within fa-
cial images and temporal relations between diﬀerent
frames in a video.

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

3

In a diﬀerent direction from the aforementioned meth-
ods, Meng et al. [50] and Liu et al. [47] presented identity-
aware facial expression recognition models. Meng et
al. [50] suggested to jointly estimate expression and
identity features using a neural architecture consist-
ing of two identical CNN streams, aiming to allevi-
ate inter-subject variations introduced by personal at-
tributes and attain superior facial expression recogni-
tion performance. Liu et al. [47] considered deep met-
ric learning, jointly optimizing a deep metric loss and
the softmax loss. They obtained an identity-invariant
model by using a scheme based on identity-aware hard-
negative mining and online positive mining. Li et al. [45]
optimized a CNN architecture using an upgraded back-
propagation algorithm that uses a locality preserving
loss aiming to pull the neighboring faces from the same
class together. Zeng et al. [81] designed a model that
addresses the labeling inconsistencies across data sets.
In their approach, images are annotated with multi-
ple pseudo-labels, either given by human annotators or
predicted by trained models. Then, a facial expression
recognition model is trained to ﬁt the latent ground-
truth from the inconsistent pseudo-labels. Hua et al. [34]
presented a deep learning algorithm consisting of three
sub-networks of diﬀerent depths. Each sub-network is
based on an independently-trained CNN.

Diﬀerent from all the works mentioned so far and
many others [3,5, 11,18,20,27,31,34,33,35,40,42,44, 45,
47,46,50,52,62, 63,64,65,66,67,70,71,73,74,80,81], which
recognize facial expressions from fully-visible faces, we
concentrate on recognizing the emotion by looking only
at the lower part of the face. The number of works that
focus on facial expression recognition under occlusion is
considerably smaller [16,29,46]. Li et al. [46] applied a
model on synthetically occluded images. They designed
an end-to-end trainable Patch-Gated CNN to automati-
cally detect the occluded regions and concentrate on the
most discriminative non-occluded regions. Unlike Li et
al. [46], we consider a more diﬃcult scenario in which
half of the face is completely occluded. In order to learn
eﬀectively in this diﬃcult scenario, we propose to trans-
fer knowledge from teacher models that are trained on
fully-visible (non-occluded) faces.

Closer to our method are the approaches designed
for the diﬃcult VR setting [16,29,32], in which a VR
headset covers the upper side of the face. Hickson et
al. [29] proposed a method that analyzes expressions
from the eye region. The eye region is captured by an in-
frared camera mounted inside the VR headset, making
the method less generic. Georgescu et al. [16] proposed
an approach that analyzes the mouth region captured
with a standard camera. The same type of approach is
adopted by Houshmand et al. [32], but the experiments

are conducted on a data set that uses face landmarks
to apply the occlusion. In this work, we consider the
same setting as [16,32], investigating the task of facial
expression recognition when the upper half of the face
is occluded. Unlike these closely related papers [16,32],
we propose to perform knowledge distillation to pro-
duce more accurate CNN models. We study two knowl-
edge distillation techniques to distill information from
CNNs trained on fully-visible faces to CNNs trained on
occluded faces. To our knowledge, we are the ﬁrst to
apply knowledge distillation in the context of facial ex-
pression recognition under strong occlusion. Moreover,
we show the generality of the studied methods across
diﬀerent tasks and neural models.

2.2 Gender Recognition

Gender prediction models are widely used across diﬀer-
ent domains such as advertising, security and human-
computer interaction. Similar to [2,6, 19,26,36, 37,58,
77], we focus on gender prediction from facial images.
Some of these works [2,19,26,36,58] proposed the use
of CNN models to predict the gender. Abirami et al. [2]
used a CNN model to jointly predict the gender and the
age of a person from facial images. Priadana et al. [57]
proposed the use of CNN models to predict the gender
based on proﬁle pictures posted on a social media plat-
form. Jhang et al. [37] proposed an ensemble of CNNs
using a weighted-softmax voting scheme to address the
gender prediction task. The weighted-softmax voting
scheme is obtained by applying a fully-connected layer
on top of the models’ predictions. The fully-connected
layer is further trained to learn the weights for each
model. The weights of the other models are frozen dur-
ing the additional training process. Georgescu et al. [19]
proposed a ResNet-50 model based on pyramidal neu-
rons with apical dendrite activations (PyNADA) to ad-
dress the gender prediction task, among several other
tasks. The neurons are equipped with a novel activation
function inspired by some recent neuroscience discover-
ies.

Similarly to the main body of recent works on gen-
der recognition in images, we also employ a CNN model.
Diﬀerent from all the works mentioned so far [2,19,26,
36,58], which identify the gender in fully-visible faces,
we focus on a harder problem, that of predicting the
gender in strongly occluded faces. More precisely, we
aim to identify the gender of a person wearing a surgi-
cal mask, half of the face being occluded. While there
are studies on face recognition under occlusion caused
by surgical masks [10], or studies on gender recognition
under random occlusions [38], to the best of our knowl-
edge, we are the ﬁrst to propose knowledge distillation

4

Mariana-Iuliana Georgescu et al.

for the gender prediction task under consistently strong
occlusion of the lower half of the face.

2.3 Age Estimation

Age estimation is a very important task across many
domains such as advertising, human-computer interac-
tion and security, among many others. There are many
works [14,15,23,24,43,53,72,75,82] addressing the prob-
lem of age estimation of people from facial images. Be-
fore the era of deep learning, the age estimation task
was tackled by extracting handcrafted features from fa-
cial images, then applying a regressor or a classiﬁer on
top of the extracted features. Guo et al. [23] proposed
the use of biologically-inspired features based on Gabor
ﬁlters to estimate the age of a person from an image. Af-
ter extracting the bio-inspired features, Guo et al. [23]
applied the PCA algorithm to reduce the number of
components. In the end, an SVR model is employed on
top of the extracted features. In another work based
on biologically-inspired features [24], people were sepa-
rated based on gender and age groups, proving that the
segregation approach improves the performance of the
model by a signiﬁcant margin.

Wang et al. [72] employed a CNN model to estimate
the age of a person, given a facial image. Instead of esti-
mating the age based on the features resulting from the
last layer, Wang et al. [72] extracted features from dif-
ferent layers of the CNN model and concatenated them
to obtain an aging pattern. In the end, both SVM and
SVR models are applied on top of the aging patterns.
Nam et al. [53] also used a CNN model to estimate the
age, but before applying the CNN model, the authors
employ a Generative Adversarial Network [22] to in-
crease the resolution of the image in order to improve
the accuracy of the age predictor.

Similarly to the recent age prediction methods [53,
72], we employ a CNN model to estimate the age of
people from facial images. However, the main diﬀerence
is that we are interested in estimating the age under
severe occlusion. To the best of our knowledge, we are
the ﬁrst to study the age estimation task on severely
occluded images (50% of the face being occluded).

2.4 Knowledge Distillation

Knowledge distillation [4,30] is a recently studied tech-
nique [13, 48,54, 76,78,79] that enables the transfer of
knowledge between neural networks. Knowledge distil-
lation is a framework that uniﬁes model compression [4,
13,30] and learning under privileged information [48,

68], the former one being more popular than the lat-
ter. In model compression, knowledge from a large neu-
ral model [4,54] or an ensemble of large neural net-
works [30,78,79] is distilled into a shallower or thinner
neural network that runs eﬃciently during inference. In
learning under privileged information, knowledge from
a learning model trained with privileged information
(some additional data representation not available at
test time) is transferred to another learning model that
does not have access to the privileged information. In
our paper, we are not interested in compressing neural
models, but in learning under privileged information.
In particular, we study teacher-student training strate-
gies, in which the teacher neural network can learn from
fully-visible faces and the student neural network can
learn from occluded faces only. In this context, hidden
(occluded) face regions represent the privileged infor-
mation.

To the best of our knowledge, we are the ﬁrst to pro-
pose the distillation of knowledge using triplet loss. We
should underline that there are a few previous works
[13,54,78,79] that distilled triplets or the metric space
from a teacher network to a student network. Diﬀer-
ent from these methods, we do not aim to transfer the
metric space learned by a teacher network, but to trans-
fer knowledge from the teacher using metric learning,
which is fundamentally diﬀerent.

3 Methods

To demonstrate the eﬀectiveness of our approach across
various models, we consider three neural network archi-
tectures namely, VGG-f [7], VGG-face [55] and ResNet-
50 [28]. Each of the considered neural architectures is
applied on three diﬀerent tasks that involve drastic face
occlusions. Regardless of the task, our training proce-
dure based on knowledge distillation [4,17, 30, 68] is or-
ganized in a curriculum composed of the following three
steps:

1. Training a teacher neural model on fully-visible faces.
2. Fine-tuning the teacher on half-visible faces, result-

ing in a pre-trained student model.

3. Distill knowledge from the teacher model at step 1

into the student model obtained at step 2.

We emphasize that, due to catastrophic forgetting
[49], the model ﬁne-tuned at step 2 forgets useful in-
formation learned on fully-visible faces. We conjecture
that the lost information can be recovered at step 3, by
distilling knowledge from the model trained on fully-
visible faces. In this context, we refer to the model
trained on fully-visible faces as the teacher and the
model ﬁne-tuned on half-visible faces as the student. In

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

5

our experiments detailed in Section 4, we present abla-
tion results after pruning steps 2 and 3, one by one.

Our ﬁrst step is to train the teacher models on fully-
visible faces. In the second step, the networks are ﬁne-
tuned on occluded faces, thus obtaining pre-trained stu-
dent models ready to undergo knowledge distillation. In
the ﬁnal training step, the students are ﬁne-tuned on
occluded faces, simulating either the setting in which
people wear a VR headset or the setting in which peo-
ple wear a surgical mask.

We underline that each student architecture is iden-
tical to the corresponding teacher architecture, as our
goal is to learn privileged information from the teacher
[68]. Thus, during the training process, we never mix
the network architectures, such that when the teacher
is a VGG-f network, the corresponding student is also
a VGG-f network. The same applies to VGG-face and
ResNet-50. We do not aim to compress the models,
i.e. to train a shallow and eﬃcient model by distill-
ing a deeper network. Instead, we aim to distill the
knowledge from the teacher as privileged information
for the student. The privileged information available for
the student is the other half of the face (the occluded
half) seen only by the teacher. More speciﬁcally, for
people wearing VR headsets, the upper half of the face
represents the privileged information, while for people
wearing surgical masks, the privileged information is
the lower half of the face.

In the remainder of this section, we describe the two
teacher-student training strategies for learning to pre-
dict the facial expression, gender or age of people from
images with strong face occlusion. The two teacher-
student strategies are alternatively employed at step
3 in our training process. As an attempt to increase
robustness and stability, we also combine the result-
ing student models into an ensemble based on meta-
learning.

3.1 Conventional Teacher-Student Training

Ba et al. [4] proposed a method to compress a deeper
model into a shallow one, their purpose being that of
training an eﬃcient network to mimic the complex rep-
resentation of a deeper neural network. Using a similar
idea, Hinton et al. [30] proposed to distill the knowledge
of an ensemble of models into a single neural archi-
tecture. In their study, the authors demonstrated that,
even when an entire class of samples is missing dur-
ing training, through knowledge distillation, the stu-
dent is still able to recognize the examples belonging to
the missing class due to the knowledge received from
the teacher. The knowledge distillation is performed by

training the student model using a soft target distribu-
tion produced by the teacher model. Besides the knowl-
edge distillation loss, the cross-entropy loss between the
ground-truth labels and the student predictions is also
added, obtaining the following weighted loss:

LKD(θS) = λL(N τ

T , N τ

S ) + (1 − λ)L(y, NS),

(1)

where θS are the weights of the student model S, y is the
ground-truth label, NT is the prediction of the teacher
network T , NS is the prediction of the student network
S and λ is a hyperparameter that controls the impor-
tance of each loss function forming the ﬁnal loss LKD.
The ﬁrst term of LKD is the cross-entropy with respect
to the soft prediction of the teacher and the second
term is the cross-entropy with respect to the ground-
truth label. N τ
S are the softened predictions of
the teacher T and the student S, respectively, where
τ > 1 is a temperature parameter for the softening op-
eration. More precisely, N τ
S are derived from
the pre-softmax activations AT and AS of the teacher
network and the student network, as follows:

T and N τ

T and N τ

N τ

T = softmax

(cid:19)

(cid:18) AT
τ

, N τ

S = softmax

(cid:18) AS
τ

(cid:19)

.

(2)

Lopez et al. [48] introduced the generalized distilla-
tion, where a model can learn from a diﬀerent teacher
model, but also from a diﬀerent data representation.
When training the student model by optimizing the loss
deﬁned in Equation (1), the student can learn privileged
information available only to the teacher. In our case,
we employ the generalized distillation method to learn
a student to recognize facial expressions on strongly oc-
cluded faces with privileged information coming from a
teacher that has access to fully-visible faces [68]. The
application of the conventional teacher-student frame-
work for facial expression recognition under drastic oc-
clusion is illustrated in Figure 1.

We underline that the aforementioned knowledge
distillation methods [4,30,48] are suitable for classiﬁca-
tion problems with multiple classes, in which the soft-
max output has suﬃcient components to contain edi-
fying information. As a solution for our regression and
binary classiﬁcation tasks, we distill the knowledge in
the penultimate layer of the student. To distill knowl-
edge at any given layer, including the penultimate one,
we can employ the approach of Romero et al. [59]. In
order to train a deeper and thinner student than the
teacher, Romero et al. [59] provided hints learned by
the teacher network to the student network. A hint is
the output of a teacher’s hidden layer, that is used to
guide the corresponding output of a student’s hidden
layer. The loss function used to guide the training of the
student is the L1 distance between the output HT of a

6

Mariana-Iuliana Georgescu et al.

Fig. 1 The standard teacher-student training pipeline for facial expression recognition on severely occluded faces. The teacher
CNN takes as input non-occluded (fully-visible) faces, having access to privileged information. The student CNN takes as
input only occluded (lower-half-visible) faces, but learns useful information from the teacher CNN model. The loss functions
L(y, NS) and L(N τ

S ) are the terms of the loss deﬁned in Equation (1). Best viewed in color.

T , N τ

teacher’s hidden layer and the output HS of a student’s
hidden layer. More precisely, the following weighted loss
is proposed by Romero et al. [59]:

LHT (θS) = λ(cid:107)HT − HS(cid:107)1 + (1 − λ)L(y, NS),

(3)

where θS are the weights of the student model S, y is
the ground-truth label, HS is the output of a student’s
hidden layer, HT is the output of the teacher’s hint
(hidden) layer, NS is the ﬁnal output of the student
network S and λ is a hyperparameter that controls the
importance of the two components.

For the age estimation and gender recognition tasks,
we employ the knowledge distillation method of Romero
et al. [59]. We prefer the hint teacher-student paradigm
[59] over the standard teacher-student method [30], due
to the low number of available predictions in the ﬁnal
layers for gender recognition (at most two components
for male versus female classiﬁcation) and age estimation
(one component for estimating the age on a continuous
scale).

3.2 Teacher-Student Training with Triplet Loss

When employing the teacher-student paradigm, aside
from training the student to predict the correct labels,
related techniques add a loss term to minimize the dif-
ference between the output of the student and that of
the teacher [4, 30, 48], or the diﬀerence between some
intermediate layers [59] of the student and teacher net-
works. To our knowledge, we are the ﬁrst to perform
distillation by adding a triplet loss term.

In general, triplet loss [60] is employed in neural net-
work training to produce close (similar) embeddings for
objects belonging to the same class and distant embed-
dings when the objects belong to diﬀerent classes. Our

approach is to employ triplet loss on the face embed-
dings (the activations from the layer immediately before
the ﬁnal classiﬁcation or regression layer). We aim to
obtain similar face embeddings when the student net-
work and the teacher network take an input image from
the same class, and diﬀerent embeddings otherwise.

We next present in detail how triplet loss can be
applied to train the student network to learn privileged
information provided by the teacher network. Through-
out the remainder of this section, we use the prime sym-
bol to denote an occluded face. Let x be a fully-visible
face and x(cid:48) be an occluded face. Let ET (x) and ES(x(cid:48))
be the face embeddings produced by the teacher net-
work T and the student network S, respectively. In or-
der to employ the triplet loss, we need triplets of input
images of the form (a(cid:48), p, n(cid:48)), where a(cid:48) is an occluded im-
age from a class k, p is a fully-visible image from class k
(the positive sample) and n(cid:48) is an occluded image from
a diﬀerent class than k (the negative sample). During
training, our goal is to reduce the distance between the
anchor embedding ES(a(cid:48)) and the positive embedding
ET (p) until it becomes smaller than the distance be-
tween the anchor embedding ES(a(cid:48)) and the negative
embedding ES(n(cid:48)). In order to accomplish this goal, we
use the following triplet loss function:

Ltriplet(θS) =

m
(cid:88)

(cid:104)

i=1

(cid:107)ES(a(cid:48)

i) − ET (pi)(cid:107)2

2−

− (cid:107)ES(a(cid:48)

i) − ES(n(cid:48)

i)(cid:107)2

2 + α

(4)

(cid:105)

,

+

where θS are the learnable parameters of the student
network S, m is the number of training examples, [·]+ =
max(0, ·) and α is the margin (minimum distance) be-
tween the positive pair of embeddings (ES(a(cid:48)
i), ET (pi))
i), ES(n(cid:48)
and the negative pair of embeddings (ES(a(cid:48)
i)).

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

7

Fig. 2 The teacher-student training based on triplet loss for facial expression recognition on severely occluded faces. During
training, we modify the weights of the student network such that the distance (cid:107)ES(a(cid:48)
2 becomes smaller than the
distance (cid:107)ES(a(cid:48)

i) − ET (pi)(cid:107)2

2. Best viewed in color.

i) − ES(n(cid:48)

i)(cid:107)2

Similarly to the standard teacher-student loss ex-
pressed in Equation (1), we want the student network
to be able to reproduce the correct labels. Thus, our
ﬁnal loss function becomes:

LKDT (θS) = (1 − λ)L(y, NS) + λLtriplet(θS),

(5)

where θS are the learnable parameters of the student
network S, L(y, NS) is the loss function with respect
to the ground-truth labels (cross-entropy for classiﬁca-
tion tasks and mean absolute error for regression tasks),
Ltriplet is the triplet loss function and λ is a hyper-
parameter that controls the importance of the second
objective function with respect to the ﬁrst one. We il-
lustrate our knowledge distillation paradigm based on
triplet loss in Figure 2. We underline that only the
weights θS of the student are updated during train-
ing, while the embeddings ET (pi) are kept unchanged
during the whole knowledge distillation process.

Following [60], we propose a fast hard example min-
ing scheme, as described next. In order to speed-up the
training process, we generate the triplets oﬄine, at the
beginning of each epoch. One by one, each sample x(cid:48)
from the training set is selected as the anchor a(cid:48). For
each anchor a(cid:48) belonging to the class k, we randomly
select a subset Spos of fully-visible training faces belong-
ing to the same class. Next, we compute the distance
between the embeddings produced by the teacher net-
work for the fully-visible faces in Spos and the embed-
ding produced by the student for the anchor a(cid:48). The
sample which is the farthest from the anchor is selected
as the positive example p:

p = argmax

j

(cid:8)(cid:107)ES(a(cid:48)) − ET (pj)(cid:107)2

2

(cid:9) , ∀pj ∈ Spos.

(6)

In order to select the negative example n(cid:48), we ran-
domly select a subset Sneg of half-visible training faces
from a diﬀerent class than k. Then, we compute the
distance between the anchor embedding and the em-
beddings produced by the student model for the half-
visible faces in Sneg. The sample which has the embed-
ding closest to the anchor embedding is selected as the
negative example n(cid:48):

n(cid:48) = argmin

j

(cid:8)(cid:107)ES(a(cid:48)) − ES(n(cid:48)

j)(cid:107)2
2

(cid:9) , ∀n(cid:48)

j ∈ Sneg.

(7)

The random subsets Spos and Sneg used in our hard
example mining scheme contain only a small percent-
age of the entire training set (10% for facial expres-
sion recognition and 20% for age and gender estima-
tion), speeding up the training time by a large margin.
For additional eﬃciency improvements, we generate the
subsets of positive and negative samples only once per
epoch. Since age estimation is a regression task, we do
not have truly positive or negative examples. Hence,
for each anchor example, we consider examples under a
diﬀerence of 5 years as positive and the other examples
as negative. The threshold is set to roughly match the
mean absolute error of the teachers on UTKFace.

4 Experiments

4.1 Organization

We hereby present experiments to demonstrate the ef-
ﬁciency of the proposed knowledge distillation meth-
ods. We consider three tasks (facial expression recog-
nition, gender recognition, age estimation) and three
models (VGG-f, VGG-face, ResNet-50), evaluating all

8

Mariana-Iuliana Georgescu et al.

models across all tasks in three scenarios (fully-visible
faces, lower-half-visible faces, upper-half-visible faces).
For each data set used in our experiments, we also
present ablation results, considering as baselines the
students trained on occluded faces (without employing
any knowledge distillation technique) and the teach-
ers trained on fully-visible faces (without ﬁne-tuning on
occluded faces). Our knowledge distillation frameworks
are employed only in the scenarios of interest (facial ex-
pression recognition on lower-half-visible faces, gender
prediction on upper-half-visible faces, age estimation on
upper-half-visible faces).

4.2 Facial Expression Recognition

by Hickson et al. [29], in which facial expressions are
recognized from the eye region, we occlude the entire
lower half of the FER+ and AﬀectNet images. All im-
ages are resized to 224 × 224 pixels, irrespective of the
data set, in order to be given as input to VGG-f, VGG-
face and ResNet-50.

4.2.3 Evaluation Metrics

Our metric for evaluating the classiﬁcation models for
facial expression recognition is the accuracy between
the ground-truth labels and the predicted labels. Due
to the fact that FER+ is highly imbalanced, we also
report the weighted accuracy for this data set.

4.2.1 Data Sets

4.2.4 Implementation Details

FER+. The FER+ data set [5] is a curated version of
the FER 2013 data set [21]. The latter data set contains
images with incorrect labels as well as images not con-
taining faces. Barsoum et al. [5] cleaned up the FER
2013 data set by relabeling images and by removing
those without faces. In the relabeling process, Barsoum
et al. [5] added a new class of emotion, contempt, while
also keeping the other 7 classes from FER 2013: anger,
disgust, fear, happiness, neutral, sadness and surprise.
The FER+ data set is composed of 25,045 training im-
ages, 3,191 validation images and 3,137 test images. The
size of each image is 48 × 48 pixels.
AﬀectNet. The AﬀectNet [51] data set is one of the
largest data sets for facial expression recognition, con-
taining 287,651 training images and 4,000 validation
images with manual annotations. The images from Af-
fectNet have various sizes. Since the test set is not yet
publicly available, methods [18,51, 64,65, 70,71, 81] are
commonly evaluated on the validation set. The data set
contains the same 8 classes of emotion as FER+. With
500 images per class in the validation set, the class dis-
tribution is balanced. In the same time, the training
data is unbalanced. As proposed by Mollahosseini et
al. [51], we down-sample the training set for classes with
more than 15,000 images. This leaves us with a training
set of 88,021 images.

4.2.2 Data Preprocessing

In order to train and evaluate the neural models in sce-
narios with drastic face occlusion, we replace the values
of speciﬁc pixels with zero to simulate occlusions.

For the setting introduced by Georgescu et al. [16],
in which facial expressions are recognized from the lower
half of the face, we occlude the entire upper half of the
FER+ and AﬀectNet images. For the setting proposed

We emphasize that all the hyperparameters speciﬁed
below are tuned on the FER+ validation set. Since the
AﬀectNet validation set is used for the ﬁnal evaluation,
on AﬀectNet, we use the hyperparameter settings found
optimal on the FER+ validation set. The VGG-f and
VGG-face models are trained with stochastic gradient
descent with momentum. We set the momentum rate to
0.9. The VGG-face model is trained on mini-batches of
64 images, while the VGG-f model is trained on mini-
batches of 512 images, since the latter model has a lower
memory footprint. We use the same mini-batch sizes
in all training stages. The ResNet-50 model is trained
using the Adam optimizer [41] on mini-batches of 16
images.
Preliminary training of teachers and students.
For the preliminary ﬁne-tuning of the teacher and the
student models, we use the MatConvNet [69] library.
The teacher VGG-face is ﬁne-tuned on facial expres-
sion recognition from fully-visible faces for a total of 50
epochs. The teacher VGG-f is ﬁne-tuned for 800 epochs.
The student VGG-face is ﬁne-tuned on facial expression
recognition from occluded faces for 40 epochs. Similarly,
the student VGG-f is ﬁne-tuned on occluded faces for
80 epochs. Further details about training these VGG-
face and VGG-f baselines on fully-visible or occluded
faces are provided in [16]. The teacher ResNet-50 is
ﬁne-tuned for 75 epochs using a learning rate of 10−4.
Similarly, the student ResNet-50 is ﬁne-tuned on lower-
half-visible faces for 75 epochs with a learning rate set
to 10−4.
Standard teacher-student training. For the con-
ventional teacher-student strategy, the student VGG-
face is trained for 50 epochs starting with a learning
rate of 10−4, decreasing it when the validation error
does not improve for 10 consecutive epochs. By the
end of the training process, the learning rate for the

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

9

student VGG-face drops to 10−5. In a similar manner,
the student VGG-f is trained for 200 epochs starting
with a learning rate of 10−3, decreasing it when the
validation error does not improve for 10 consecutive
epochs. By the end of the training process, the learning
rate for the student VGG-f drops to 10−4. The student
ResNet-50 is trained for 10 epochs starting with a learn-
ing rate of 10−5. Hinton et al. [30] suggested to use a
lower weight on the second objective function deﬁned
in Equation (1). Therefore, we set λ to a value of 0.9
for ResNet-50 and a value of 0.7 for both VGG-f and
VGG-face models. The parameter λ is validated on the
FER+ validation set.
Teacher-student training with triplet loss. To im-
plement the teacher-student training based on triplet
loss, we switch to TensorFlow [1], exporting the VGG-f
and VGG-face models from MatConvNet. We train the
student VGG-face for 10 epochs using a learning rate
of 10−6. In a similar fashion, we train the VGG-f and
ResNet-50 students for 10 epochs using a learning rate
of 10−5. The parameter λ in Equation (5) is set to a
value of 0.5 for the VGG-f and VGG-face models, and
a value of 0.1 for the ResNet-50 model. The value of the
margin α from Equation (4) is chosen based on the per-
formance measured on the validation sets, considering
values in the interval [0, 0.5] at a step of 0.1.
Combining distilled embeddings. After training the
student models using the two teacher-student strate-
gies independently, we concatenate the corresponding
face embeddings into a single embedding vector. The
concatenated embeddings are provided as input to a
Support Vector Machines (SVM) model [8]. The reg-
ularization parameter C of the resulting SVM models
is chosen according to the performance on the valida-
tion sets, considering values between 10−1 and 103, at
a step of 101. We use the SVM implementation from
Scikit-learn [56].

4.2.5 Baselines

As baselines for facial expression recognition, we con-
sider two state-of-the-art methods [16,29] designed for
facial expression recognition in the VR setting. The
key contribution of these methods resides in the region
they use to extract features, the lower half of the face
(mouth region) [16] or the upper half of the face (eye
region) [29]. In order to conduct a fair comparison, we
use the same neural architectures for both baselines and
our approach.

As reference, we include some results on FER+ and
AﬀectNet from the recent literature [5,12,18,25,35, 42,
51,64,65,66,70,71]. We underline that these state-of-
the-art methods are trained and tested on fully-visible

faces. Hence, the comparison to our approach or other
approaches applied on occluded faces [16, 29] is unfair,
but we included it as a relevant indicator of the upper
bound for the models applied on occluded faces. We
also underline that the included state-of-the-art meth-
ods [64,65,66,70,71] are not based on standard mod-
eling choices, using sophisticated loss functions, label
smoothing, ensembles of multiple neural architectures
or more of the above. Our approach is closer to the
methods trained under more simple settings [5, 25,51],
since our teachers and students are trained using fairly
well-known loss functions and the evaluation is per-
formed using only one model.

4.2.6 Results

In Table 1, we present the empirical results obtained on
AﬀectNet [51] and FER+ [5] by the VGG-f, VGG-face
and ResNet-50 models based on our teacher-student
training strategies in comparison with the results ob-
tained by the state-of-the-art methods [5,25,12, 35, 42,
51,64,65,66,70, 71] tested on fully-visible faces and by
the methods [16,29] designed for the VR setting (tested
on occluded faces).
Comparison with the state-of-the-art. First of all,
we note that it is natural for the state-of-the-art meth-
ods [5, 25, 12,35, 42, 51,64,65,66,70, 71] to achieve bet-
ter accuracy rates (on fully-visible faces) than our ap-
proach or the other approaches applied on occluded
faces [16,29]. One exceptional case is represented by
the VGG-face model of Georgescu et al. [16] and our
student VGG-face model, as both of them surpass the
bag-of-visual-words model [35] on both data sets. An-
other exception of the above observation is represented
by the ﬁne-tuned or distilled ResNet-50 students, both
surpassing the bag-of-visual-words on FER+.
Comparison between lower-half and upper-half
visible faces. With respect to the baselines [16,29]
designed for the VR setting, all our teacher-student
training strategies provide superior results. We observe
that the accuracy rates of Hickson et al. [29] are con-
siderably lower than the accuracy rates of Georgescu
et al. [16] (diﬀerences are between 5% and 12%), al-
though the neural models have identical architectures.
We hypothesize that this diﬀerence is caused by the
fact that it is signiﬁcantly harder to recognize facial
expressions from the eye region (denoted by (cid:71)(cid:35)) than
from the mouth region (denoted by (cid:72)(cid:35)). To test this
hypothesis, we evaluate the teachers (VGG-f, VGG-
face and ResNet-50) on upper-half-visible and lower-
half-visible faces. We observe even larger diﬀerences
between the results on upper-half-visible faces (accu-
racy rates are between 24% and 49%) and the results

10

Mariana-Iuliana Georgescu et al.

Table 1 Accuracy rates of various models on AﬀectNet [51] and FER+ [5], for fully-visible faces (denoted by
), lower-half-
visible faces (denoted by (cid:72)(cid:35)) and upper-half-visible faces (denoted by (cid:71)(cid:35)). The VGG-f, VGG-face and ResNet-50 models based
on our teacher-student (T-S) training strategies are compared with state-of-the-art methods [5, 18, 25, 12, 35, 42, 51, 64, 65, 66,
70, 71] tested on fully-visible faces and with methods [16, 29] designed for the VR setting (tested on occluded faces). The test
results of our student networks that are signiﬁcantly better than the stronger baseline [16], according to a paired McNemar’s
test [9], are marked with † for a signiﬁcance level of 0.05.

(cid:35)

Model

Train faces Test faces AﬀectNet

FER+
Accuracy Weighted accuracy

VGG-13 [5]
DACL [12]
CNNs+BOVW+LC [18]
VGG-12 [25]
Bag-of-visual-words [35]
MT-VGG [42]
AlexNet [51]
Res-50IBN [64]
MBCC-CNN [65]
ESR-9 [66]
SCN [71]
PSR [70]
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
VGG-f [16]
VGG-face [16]
ResNet-50
VGG-f [29]
VGG-face [29]
ResNet-50
VGG-f (standard T-S)
VGG-face (standard T-S)
ResNet-50 (standard T-S)
VGG-f (triplet loss T-S)
VGG-face (triplet loss T-S)
ResNet-50 (triplet loss T-S)
VGG-f (triplet loss + standard T-S)
VGG-face (triplet loss + standard T-S)
ResNet-50 (triplet loss + standard T-S)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)
+ (cid:72)(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

-
65.20%
59.58%
58.50%
48.30%
54.00%
58.00%
63.11%
-
59.30%
60.23%
60.68%
57.37%
59.03%
56.07%
41.58%
37.70%
40.50%
26.85%
31.23%
24.12%
47.58%
49.23%
45.90%
42.45%
43.18%
43.37%
48.75%†
49.75%
46.95%†
48.13%
49.71%
46.17%
48.70%†
50.09%†
47.00%†

84.99%
-
87.76%
-
80.65%
-
-
89.51%
88.10%
87.15%
89.35%
89.75%
85.05%
84.79%
85.91%
70.00%
68.89%
70.89%
40.07%
48.29%
44.21%
78.23%
82.28%
81.79%
66.18%
70.19%
72.26%
80.17%†
82.37%
82.37%†
80.05%†
82.57%
81.28%
81.09%†
82.75%†
82.37%†

-
-
-
-
-
-
-
-
-
-
-
-
59.71%
66.15%
65.67%
43.24%
39.69%
44.56%
32.82%
37.36%
30.01%
50.52%
58.69%
60.57%
44.66%
48.83%
54.62%
53.00%†
59.46%
59.19%†
52.87%†
59.12%
60.93%
58.90%†
61.23%†
59.30%†

on lower-half-visible faces (accuracy rates are between
37% and 71%), conﬁrming our hypothesis. We also un-
derline that the results attained by the teacher models
on occluded faces are considerably lower than the re-
sults of the baselines [16,29] designed for the VR set-
ting, although the teacher models attain results close
to the state-of-the-art methods [5,25,12,35,42,51,64,
65,66,70,71] when testing is performed on fully-visible
faces. This indicates that CNN models trained on fully-
visible faces are not particularly suitable to handle se-
vere facial occlusions, justifying the need for training
on occluded faces.
Comparison with closely related methods. The
results presented in the last nine rows of Table 1 in-

dicate that the teacher-student learning strategies pro-
vide very good results on lower-half-visible faces, sur-
passing the other methods [16,29] evaluated on occluded
faces. We believe that the accuracy gains are due to the
teacher neural networks that are trained on fully-visible
images, which bring additional (privileged) information
from the (unseen) upper half of the training faces. Our
teacher-student training strategy based on triplet loss
provides results that are comparable to the standard
teacher-student training strategy. We also achieve addi-
tional performance gains when the two teacher-student
strategies are combined through embedding concatena-
tion. Our ﬁnal models based on the concatenated dis-
tilled embeddings attain results that are close to some

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

11

Fig. 3 Fully-visible images (
) on top row, lower-half-visible faces ( (cid:72)(cid:35)) on second row, Grad-CAM [61] explanation masks
on third row and lower-half-visible faces with superimposed Grad-CAM masks on bottom row. The predicted labels provided
by the distilled VGG-face (left-hand side) or VGG-f (right-hand side) models are also provided at the bottom. The ﬁrst two
examples from each side are selected from AﬀectNet [51] and FER+ [5], respectively. The third example from each side is a
person wearing an actual VR headset. Best viewed in color.

(cid:35)

Grad-CAM [61] approach to provide visual explana-
tions for some image examples illustrated in Figure 3.
First, we notice that we, as humans, are still able to rec-
ognize the facial expressions in the presented examples,
even if the upper half of each face depicted in the sec-
ond row of Figure 3 is occluded. We observe that the
neural architectures turn their attention on the lower
part of the face, particularly on the mouth region. This
indicates that our neural networks can properly han-
dle situations in which people wear VR headsets. We
note that the predicted labels for the ﬁrst ﬁve samples
presented in Figure 3 are correct.

state-of-the-art methods [5,25,35,42,51]. For example,
our VGG-face with triplet loss and standard teacher-
student training yields an accuracy rate of 82.75% on
FER+, which is 2.24% under the state-of-the-art VGG-
13 [5]. We thus conclude that our models can recognize
facial expressions with suﬃcient reliability, despite be-
ing tested on faces that are severely occluded (the entire
upper half is occluded).
Statistical signiﬁcance testing. We also performed
statistical signiﬁcance testing to compare our models
(VGG-f, VGG-face and ResNet-50) based on teacher-
student training with the models of Georgescu et al. [16],
which are equivalent with our students before undergo-
ing distillation. Notably, the combined teacher-student
strategies provide signiﬁcant improvements for the VGG-
f, VGG-face and ResNet-50 models on both data sets,
with a signiﬁcance level of 0.05.
Grad-CAM visualizations. In order to better un-
derstand how our models make decisions, we used the

12

Mariana-Iuliana Georgescu et al.

4.3 Age and Gender Estimation

4.3.1 Data Sets

The UTKFace [83] data set contains images with faces
of people of various age, gender and ethnicity. The data
set consists of 23,689 images. We randomly divide the
data set obtaining 14,213 (60%) images for training,
4,738 (20%) images for validation and 4,738 (20%) im-
ages for testing. The size of each image is 200 × 200
pixels. We perform two types of experiments on UTK-
Face: gender recognition and age estimation.

4.3.2 Data Preprocessing

We adopt a similar technique as in the VR setting to
simulate occlusions caused by surgical masks, thus re-
placing the values of pixels in the lower half of each face
in the UTKFace data set with zero. The images from
the data set are resized to 224×224 pixels to correspond
to the input size of the convolutional networks.

4.3.3 Evaluation Metrics

Our metric for evaluating the gender prediction models
is the standard classiﬁcation accuracy. To evaluate the
regression models for age estimation, we consider the
mean absolute error (MAE) between the predicted age
and the target age of each test sample.

4.3.4 Implementation Details

The networks are trained using the same optimizer and
batch size as in the facial expression recognition ex-
periments. We tune all other hyperparameters on the
UTKFace validation set.
Preliminary training of teachers and students.
The teacher and student VGG-f models are ﬁne-tuned
for 200 epochs starting with a learning rate of 10−4. We
note that there is a teacher and a student for each of the
two tasks (gender recognition and age estimation). Both
teacher and student VGG-face networks are ﬁne-tuned
for 250 epochs with the learning rate set to 10−4. The
teacher and student ResNet-50 models are trained from
scratch to predict the gender or the age of people in
images, for 100 epochs. The learning rate for all teacher
and student ResNet-50 models is set to 10−4.
Standard teacher-student training. In order to ap-
ply the standard teacher-student strategy to estimate
the age and the gender from an image containing a face,
the VGG-f students are each trained for 20 epochs start-
ing with a learning rate of 10−6. We set the parameter
λ in Equation (3) to a value of 0.3 for gender predic-
tion and a value of 0.5 for the age estimation task. The

VGG-face students are trained for 40 epochs, setting
the learning rate to a value of 10−5 for the age estima-
tion task and a value of 10−6 for the gender prediction
task. The parameter λ in Equation (3) is set to 0.3 for
both tasks. The ResNet-50 students are each trained for
a number of 40 epochs to estimate the age or the gen-
der of a person, respectively. The starting learning rate
is 10−5 and we opted to decrease it when the valida-
tion error does not improve for 10 consecutive epochs.
The parameter λ in Equation (3) is set to 0.1 for both
tasks, based on the performance observed on the UTK-
Face validation set.
Teacher-student training with triplet loss. The
VGG-f model is trained for 30 epochs for the gender
prediction task, starting with a learning rate of 10−5.
For the age estimation task, the model is trained for 40
epochs and the learning rate is set to 10−6. The VGG-
face model is trained for 40 epochs with the learning
rate set to 10−6 for both tasks. The parameter λ in
Equation (5) is set to 0.7 for both tasks and both VGG
networks. Each of the two student ResNet-50 models is
trained for 40 epochs with a learning rate of 10−5. The
value of the parameter λ is 0.7 for the age estimation
task and 0.4 for the gender prediction task. The value of
the margin α in Equation (4) is tuned on the validation
set, considering values in the range [0, 0.5] at a step of
0.1.
Combining distilled embeddings. We concatenate
the embeddings obtained from our two teacher-student
training strategies and provide them as input either
to an SVM model [8] for the classiﬁcation task (gen-
der recognition) or to an (cid:15)-Support Vector Regression
(SVR) for the regression task (age estimation). The reg-
ularization parameter C of these models is chosen ac-
cording to the performance on the validation sets, con-
sidering values between 10−1 and 103, at a step of 101.
We keep the default value for the parameter (cid:15) of the
SVR, that is (cid:15) = 0. As for the SVM, we employ the
SVR implementation from Scikit-learn [56].

4.3.5 Baselines

As reference, we include the state-of-the-art results of
the ResNet-50 based on pyramidal neurons with api-
cal dendrite activations (PyNADA) reported in [19], al-
though the results are not directly comparable due to
the diﬀerent splits applied on UTKFace. As baselines,
we also include the student trained on lower-half-visible
faces and the teacher trained on fully-visible faces.

4.3.6 Gender Recognition Results

Comparison with the state-of-the-art. We present
the gender prediction results in Table 2. The teacher

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

13

Table 2 Accuracy rates for gender prediction on UTKFace [83], for fully-visible faces (denoted by
), lower-half-visible faces
(denoted by (cid:72)(cid:35)) and upper-half-visible faces (denoted by (cid:71)(cid:35)). A state-of-the-art model [19] is included as reference. The results
of distilled models that are signiﬁcantly better than the student trained on upper-half-visible faces, according to a paired
McNemar’s test [9] at a signiﬁcance level of 0.05, are marked with †.

(cid:35)

Method
ResNet-50+PyNADA [19]
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
VGG-f
VGG-face
ResNet-50
VGG-f
VGG-face
ResNet-50
VGG-f (standard T-S)
VGG-face (standard T-S)
ResNet-50 (standard T-S)
VGG-f (triplet loss T-S)
VGG-face (triplet loss T-S)
ResNet-50 (triplet loss T-S)
VGG-f (triplet loss + standard T-S)
VGG-face (triplet loss + standard T-S)
ResNet-50 (triplet loss + standard T-S)

Train faces Test faces Accuracy

(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

90.80%
92.78%
92.20%
90.88%
78.13%
73.05%
72.69%
85.69%
88.18%
83.20%
88.70%
90.62%
86.47%
88.92%
88.26%
88.75%
89.13%
88.45%
89.45%†
89.55%†
88.31%
89.19%
89.82%†
90.35%†
89.63%†

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:35)

ResNet-50 trained and evaluated on fully-visible faces
(denoted by
) reaches an accuracy of 90.88%, which
is quite close to the top result reported in [19]. The
VGG-f and VGG-face teachers surpass the performance
reported in [19] by at least 2%.
Comparison between lower-half and upper-half
visible faces. When we evaluate the teachers on half-
visible faces (denoted by (cid:72)(cid:35)and (cid:71)(cid:35)), the accuracy rates
drop by considerable margins. On the one hand, eval-
uating the teachers on the lower half ( (cid:72)(cid:35)) of the faces
(the upper half being occluded) induces a performance
drop between 14% and 19%. On the other hand, when
we black out the lower half of each face, evaluating the
teachers on upper-half-visible faces ( (cid:71)(cid:35)), the accuracy
decreases by less than 7.68% (the maximum drop being
observed for the ResNet-50 teacher), suggesting that it
is easier to perform gender recognition on the upper side
of the face. Despite the signiﬁcant performance drop,
this represents an encouraging result for the surgical
mask scenario.

When we train the students on half-visible faces
and evaluate them in the same setting, the accuracy
rates improve. The ResNet-50 student trained and eval-
uated on the lower-half-visible faces reaches an accu-
racy of 86.47%. The other ResNet-50 student, the one
trained and evaluated on upper-half-visible faces, yields

an accuracy of 88.75%. Both students obtain better per-
formance compared with the teacher trained on fully-
visible faces, when the evaluation is conducted on half-
visible faces.

Comparison with the baseline. Further, we observe
that, with respect to the baseline students, both teacher-
student strategies attain superior performance. When
we ﬁne-tune the students using the standard teacher-
student strategy, we obtain improvements ranging be-
tween 0.19% and 0.70%, reaching a top accuracy rate
of 89.45% with the ResNet-50 model. The privileged in-
formation received from the teacher helps the student
to outperform its ablated version trained with the stan-
dard loss (binary cross-entropy between the predicted
labels and the ground-truth labels). The VGG-f student
ﬁne-tuned using the teacher-student training strategy
based on triplet loss obtains an accuracy of 89.55%, sur-
passing its ablated version by 0.63%. Concatenating the
VGG-face embeddings of the two knowledge distillation
strategies provides an accuracy of 90.35%. We empha-
size that the performance of the ensemble formed by the
two ResNet-50 students evaluated on upper-half-visible
faces ( (cid:71)(cid:35)) is only 1.25% below the ResNet-50 teacher
trained and evaluated on fully-visible faces (
). We
thus conclude that putting on a mask does not represent

(cid:35)

14

Mariana-Iuliana Georgescu et al.

Fig. 4 Fully-visible images (
) on top row, upper-half-visible faces ( (cid:71)(cid:35)) on second row, Grad-CAM [61] explanation masks on
third row and upper-half-visible faces with superimposed Grad-CAM masks on bottom row. The predicted gender provided
by the distilled ResNet-50 model is shown at the bottom. The ﬁrst four examples are selected from the UTKFace [83] data
set. The last two examples are people wearing surgical masks. Best viewed in color.

(cid:35)

a signiﬁcant problem for gender recognition when both
ﬁne-tuning and knowledge distillation are employed.

Statistical signiﬁcance testing. Furthermore, we also
performed statistical signiﬁcance testing to compare our
distilled models with the ablated version of the student
(the version before undergoing distillation). For all net-
work types (VGG-f, VGG-face and ResNet-50), the re-
sults obtained by concatenating the two strategies and
those obtained by the model ﬁne-tuned using the stan-
dard teacher-student paradigm are indeed statistically
signiﬁcant at a signiﬁcance level of 0.05.

Grad-CAM visualizations. To further investigate how
our models make decisions, we employ Grad-CAM [61]
to visualize what parts of the image are important in
the gender recognition process. A set of representative
Grad-CAM [61] visualizations are shown in Figure 4.
We observe that our model tends to concentrate on the
upper half of the face, especially on the eye region. We
notice that the focus area usually extends until it covers
some hair. Thus, it is likely that our model considers

the shape of the eyes and the length of the hair as dis-
criminative features for gender prediction. Additionally,
we underline that the predicted classes for the samples
presented in Figure 4 are all correct.

4.3.7 Age Estimation Results

Comparison with the state-of-the-art. In Table
3, we present the results for the age estimation task.
The teacher ResNet-50 trained and evaluated on fully-
visible faces (denoted by
) obtains an error of 5.27
years, surpassing the reference model proposed in [19].
(cid:35)
Comparison between lower-half and upper-half
visible faces. When we evaluate the teacher models
on the half-visible faces, the error increases by consid-
erable margins. Indeed, evaluating the teacher models
on lower-half-visible faces (denoted by (cid:72)(cid:35)) increases the
error up to 14.23 years. Similarly, when we evaluate
them on upper-half-visible faces (denoted by (cid:71)(cid:35)), the
MAE grows up to 11.92 years.

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

15

Table 3 Mean absolute error (MAE) values for age estimation on UTKFace [83], for fully-visible faces (denoted by
),
lower-half-visible faces (denoted by (cid:72)(cid:35)) and upper-half-visible faces (denoted by (cid:71)(cid:35)). A state-of-the-art model [19] is included as
reference.

(cid:35)

Method
ResNet-50+PyNADA [19]
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
Teacher VGG-f
Teacher VGG-face
Teacher ResNet-50
VGG-f
VGG-face
ResNet-50
VGG-f
VGG-face
ResNet-50
VGG-f (standard T-S)
VGG-face (standard T-S)
ResNet-50 (standard T-S)
VGG-f (triplet loss T-S)
VGG-face (triplet loss T-S)
ResNet-50 (triplet loss T-S)
VGG-f (triplet loss + standard T-S)
VGG-face (triplet loss + standard T-S)
ResNet-50 (triplet loss + standard T-S)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

Train faces Test faces MAE
5.79
5.63
5.11
5.27
11.16
13.08
14.23
9.60
10.30
11.92
6.80
6.15
6.66
6.36
5.53
6.44
6.34
5.40
6.35
6.34
5.42
6.34
6.22
5.40
6.33

(cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)
+ (cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:72)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:71)(cid:35)

(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)

Training the neural networks on half-visible faces
and evaluating them in the same manner reduces the
error by large margins. More precisely, the MAE goes
down to 6.15 years on lower-half-visible faces and 5.53
years on upper-half-visible faces. Based on the results
discussed above, we conclude that the upper half of the
face is more informative for age estimation than the
lower half.

Comparison with the baseline. When ﬁne-tuning
the student models with the standard teacher-student
training strategy, the error decreases from 6.44 to 6.35
years for the ResNet-50 model, and from 5.53 to 5.40
years for the VGG-face model. Fine-tuning the ResNet-
50 student using our teacher-student training paradigm
based on triplet loss reduces the error to 6.34 years. By
concatenating the embeddings of the two VGG-f stu-
dents, we obtain a MAE of 6.22, this being our best
improvement over the ablated version which scores a
MAE of 6.36 years. In the end, the maximum diﬀerence
between the teacher model trained and evaluated on
fully-visible faces and the student evaluated on lower-
half-visible faces is 1.07 years. Hence, we conclude that
the negative performance impact on age estimation gen-
erated by putting on a mask can be reduced by employ-
ing both ﬁne-tuning and knowledge distillation.

Grad-CAM visualizations. To explain how our dis-
tilled ResNet-50 models make decisions, we employ Grad-
CAM [61] to observe what parts of the image are seen
as important when estimating the age. We provide some
relevant Grad-CAM [61] visualizations in Figure 5. First,
we observe that our student model tends to focus on the
upper half of the face, especially on the forehead re-
gion. We conjecture that our model views the wrinkles
formed on the forehead as a discriminative feature for
age estimation. On the bottom of the ﬁgure, we show
the predicted age (ﬁrst number) and the ground-truth
age (second number) for each image. For the selected
samples, the maximum diﬀerence between the predicted
and the actual age is 3 years. We consider these predic-
tions as fairly accurate.

4.4 Ablation Study Regarding Neural Embeddings

To understand which part of the ensemble of distilled
models brings a larger improvement, i.e. the concate-
nation of embeddings or the SVM model, we conduct
an ablation study by training an SVM on top of each
type of embedding extracted from the distilled students
(without concatenating the embeddings). We present
the corresponding results in Table 4. The experiments

16

Mariana-Iuliana Georgescu et al.

Fig. 5 Fully-visible images (
) on top row, upper-half-visible faces ( (cid:71)(cid:35)) on second row, Grad-CAM [61] explanation masks
on third row and upper-half-visible faces with superimposed Grad-CAM masks on bottom row. The estimated age provided
by the distilled ResNet-50 model is the ﬁrst number shown at the bottom. The second number is the ground-truth age. The
ﬁrst four examples are selected from the UTKFace [83] data set, while the last two examples are people wearing masks. Best
viewed in color.

(cid:35)

Table 4 The accuracy rates of SVMs trained on embeddings
extracted from students based on standard teacher-student
(TS) or triplet loss (TL) strategies. These models are com-
pared with SVMs trained on concatenated embeddings as well
as the students providing the embeddings. Results are re-
ported for two tasks: facial expression recognition (on FER+
and AﬀectNet) and gender prediction (on UTKFace).

Network Method

VGG-f

VGG-face

TS
TL

FER+ AﬀectNet UTKFace
80.17% 48.75%
80.05% 48.13%
TS+SVM 80.39% 48.52%
TL+SVM 79.06% 47.01%
TS+TL+SVM 81.09% 48.70%
82.37% 49.75%
82.57% 49.71%
TS+SVM 82.34% 48.89%
TL+SVM 82.37% 49.90%
TS+TL+SVM 82.75% 50.09%

89.13%
89.55%
89.04%
89.70%
89.82%
88.45%
88.31%
90.35%
90.27%
90.35%

TS
TL

are performed for the VGG-f and VGG-face models on
the FER+, AﬀectNet and UTKFace data sets. For the
facial expression recognition data sets (FER+, Aﬀect-
Net), we select the distilled models trained on lower-
half-visible faces, while for the gender prediction data
set (UTKFace), we select the distilled models trained
on upper-half-visible faces. For the VGG-f model, we
observe that the largest improvement is brought by the
concatenation of embeddings, not by the SVM model.
However, on the UTKFace data set, the SVM model
trained on embeddings from the VGG-face based on
triplet loss performs on par with the SVM ensemble of
distilled models. In this particular case, it seems that
the SVM itself makes the diﬀerence. Nevertheless, none
of the SVMs trained on individual embeddings is able to
surpass the performance of the SVM ensemble. We thus
conclude that concatenating the embeddings is useful.

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

17

5 Conclusion

In this paper, we presented two teacher-student meth-
ods to improve the performance of neural network mod-
els evaluated in scenarios with strong face occlusion.
We demonstrate that our methods generalize across dif-
ferent classiﬁcation and regression tasks (facial expres-
sion recognition, age estimation, gender prediction) and
neural architecture types (VGG-face, VGG-f, ResNet-
50). To the best of our knowledge, we are the ﬁrst to
study teacher-student strategies to learn privileged in-
formation aiming to cope with strong occlusions in im-
ages. We also proposed a novel teacher-student method
based on triplet loss. The empirical results suggest that
our knowledge distillation methods obtain superior per-
formance over the baselines. On facial expression recog-
nition, our ensemble of distilled models is only 2.24%
below the performance of VGG-13 [5], when the former
method is evaluated on occluded faces and the latter
method is evaluated on fully-visible faces. Similarly, on
gender recognition, our ensemble of distilled ResNet-50
models is only 1.25% below the corresponding teacher.
On age estimation, the diﬀerence between our ensemble
of distilled ResNet-50 models applied on occluded faces
and the teacher applied on fully-visible faces is 1.06
years. In conclusion, we believe that the performance
levels of our distilled models are suﬃciently high to be
used in real-life scenarios, such as changing the envi-
ronment in VR applications based on user’s emotions
and estimating the age and gender of people wearing
surgical masks in supermarkets or retail stores.

In future work, we aim to study if models trained
under occlusion can be used to boost the performance
of teachers on fully-visible faces. Our future goal is to
ﬁnd an eﬀective way to combine students specialized
on speciﬁc parts of the face with teachers looking at
the entire face. We believe that this line of research can
lead to signiﬁcant performance gains.

Acknowledgements We thank reviewers for their valuable
feedback which led to important improvements of the article.

References

1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A.,
Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,
Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,
D.G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P.,
Wicke, M., Yu, Y., Zheng, X.: TensorFlow: A system for
large-scale machine learning. In: Proceedings of OSDI,
pp. 265–283 (2016)

2. Abirami, B., Subashini, T., Mahavaishnavi, V.: Gen-
der and age prediction from real time facial images us-
ing CNN. Materials Today: Proceedings 33, 4708–4712
(2020)

3. Al Chanti, D., Caplier, A.: Improving Bag-of-Visual-
Words Towards Eﬀective Facial Expressive Image Clas-
siﬁcation. In: Proceedings of VISIGRAPP, pp. 145–152
(2018)

4. Ba, J., Caruana, R.: Do deep nets really need to be deep?

In: Proceedings of NIPS, pp. 2654–2662 (2014)

5. Barsoum, E., Zhang, C., Ferrer, C.C., Zhang, Z.: Train-
ing deep networks for facial expression recognition with
crowd-sourced label distribution.
In: Proceedings of
ICMI, pp. 279–283 (2016)

6. Bhaskar, A., Aneesh R.P.: Advanced algorithm for gender
prediction with image quality assessment. In: Proceed-
ings of ICACCI (2015)

7. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.:
Return of the Devil in the Details: Delving Deep into
Convolutional Nets. In: Proceedings of BMVC, pp. 1–12
(2014)

8. Cortes, C., Vapnik, V.: Support-Vector Networks. Ma-

chine Learning 20(3), 273–297 (1995)

9. Dietterich, T.G.: Approximate Statistical Tests for Com-
paring Supervised Classiﬁcation Learning Algorithms.
Neural Computation 10(7), 1895–1923 (1998)

10. Ding, F., Peng, P., Huang, Y., Geng, M., Tian, Y.:
Masked Face Recognition with Latent Part Detection.
In: Proceedings of ACMMM, pp. 2281–2289 (2020)
11. Ding, H., Zhou, S.K., Chellappa, R.: FaceNet2ExpNet:
Regularizing a Deep Face Recognition Net for Expression
Recognition. In: Proceedings of FG, pp. 118–126 (2017)
12. Farzaneh, A.H., Qi, X.: Facial Expression Recognition in
the Wild via Deep Attentive Center Loss. In: Proceedings
of WACV, pp. 2402–2411 (2021)

13. Feng, Y., Wang, H., Hu, R., Yi, D.T.: Triplet distillation
In: Proceedings of ICIP, pp.

for deep face recognition.
808–812 (2020)

14. Geng, X., Yin, C., Zhou, Z.: Facial age estimation by
learning from label distributions. IEEE Transactions on
Pattern Analysis and Machine Intelligence 35(10), 2401–
2412 (2013)

15. Geng, X., Zhou, Z., Smith-Miles, K.: Automatic Age Es-
timation Based on Facial Aging Patterns. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(12), 2234–2240 (2007)

16. Georgescu, M.I., Ionescu, R.T.: Recognizing Facial Ex-
pressions of Occluded Faces Using Convolutional Neu-
ral Networks. In: Proceedings of ICONIP, pp. 645–653
(2019)

17. Georgescu, M.I., Ionescu, R.T.: Teacher-Student Train-
ing and Triplet Loss for Facial Expression Recognition
In: Proceedings of ICPR, pp. 2288–
under Occlusion.
2295 (2020)

18. Georgescu, M.I., Ionescu, R.T., Popescu, M.: Local learn-
ing with deep and handcrafted features for facial expres-
sion recognition. IEEE Access 7, 64827–64836 (2019)
19. Georgescu, M.I., Ionescu, R.T., Ristea, N.C., Sebe, N.:
Non-linear Neurons with Human-like Apical Dendrite Ac-
tivations. arXiv preprint arXiv:2003.03229 (2020)

20. Giannopoulos, P., Perikos, I., Hatzilygeroudis, I.: Deep
Learning Approaches for Facial Emotion Recognition: A
Case Study on FER-2013. In: Advances in Hybridization
of Intelligent Methods, pp. 1–16. Springer (2018)

21. Goodfellow, I.J., Erhan, D., Carrier, P.L., Courville, A.,
Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,
D., Lee, D.H., Zhou, Y., Ramaiah, C., Feng, F., Li, R.,
Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov,
M., Park, J., Ionescu, R.T., Popescu, M., Grozea, C.,
Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z.,

18

Mariana-Iuliana Georgescu et al.

Bengio, Y.: Challenges in Representation Learning: A re-
port on three machine learning contests. In: Proceedings
of ICONIP, vol. 8228, pp. 117–124 (2013)

22. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.:
Generative Adversarial Nets. In: Proceedings of NIPS,
pp. 2672–2680 (2014)

23. Guo, G., Guowang Mu, Fu, Y., Huang, T.S.: Human age
estimation using bio-inspired features. In: Proceedings of
CVPR, pp. 112–119 (2009)

24. Guo, G., Mu, G., Fu, Y., Dyer, C., Huang, T.: A study
on automatic age estimation using a large database. In:
Proceedings of CVPR, pp. 1986–1991 (2009)

25. Guo, Y., Xia, Y., Wang, J., Yu, H., Chen, R.C.: Real-
time facial aﬀective computing on mobile devices. Sensors
20(3), 870 (2020)

26. Hacibeyoglu, M., Ibrahim, M.H.: Human Gender Predic-
tion on Facial Mobil Images using Convolutional Neural
Networks.
International Journal of Intelligent Systems
and Applications in Engineering 6(3), 203–208 (2018)
27. Hasani, B., Mahoor, M.H.: Facial expression recognition
using enhanced deep 3D convolutional neural networks.
In: Proceedings of CVPRW, pp. 2278–2288 (2017)
28. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learn-
ing for Image Recognition. In: Proceedings of CVPR, pp.
770–778 (2016)

29. Hickson, S., Dufour, N., Sud, A., Kwatra, V., Essa, I.:
Eyemotion: Classifying facial expressions in VR using
eye-tracking cameras.
In: Proceedings of WACV, pp.
1626–1635 (2019)

30. Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowl-
edge in a Neural Network. In: Proceedings of NIPS Deep
Learning and Representation Learning Workshop (2014)
31. Hosseini, S., Cho, N.I.: GF-CapsNet: Using Gabor Jet
and Capsule Networks for Facial Age, Gender, and Ex-
pression Recognition.
In: Proceedings of FG, pp. 1–8
(2019)

32. Houshmand, B., Khan, N.M.: Facial Expression Recogni-
tion Under Partial Occlusion from Virtual Reality Head-
In: Proceedings of
sets based on Transfer Learning.
BigMM, pp. 70–75 (2020)

33. Hu, J., Yu, B., Yang, Y., Feng, B.: Towards Facial De-
Expression and Expression Recognition in the Wild. In:
Proceedings of ACII, pp. 157–163 (2019)

34. Hua, W., Dai, F., Huang, L., Xiong, J., Gui, G.: HERO:
Human Emotions Recognition for Realizing Intelligent
Internet of Things. IEEE Access 7, 24321–24332 (2019)
35. Ionescu, R.T., Popescu, M., Grozea, C.: Local Learning
to Improve Bag of Visual Words Model for Facial Expres-
sion Recognition. In: Proceedings of ICML Workshop on
Challenges in Representation Learning (2013)

36. Ito, K., Kawai, H., Okano, T., Aoki, T.: Age and Gender
Prediction from Face Images Using Convolutional Neu-
ral Network. In: Proceedings of APSIPA ASC, pp. 7–11
(2018)

37. Jhang, K.: Voting and Ensemble Schemes Based on CNN
Models for Photo-Based Gender Prediction. Journal of
Information Processing Systems 16(4), 809–819 (2020)

38. Juefei-Xu, F., Verma, E., Goel, P., Cherodian, A., Sav-
vides, M.: DeepGender: Occlusion and Low Resolution
Robust Facial Gender Classiﬁcation via Progressively
Trained Convolutional Neural Networks with Attention.
In: Proceedings of CVPRW, pp. 136–145 (2016)

39. Kaya, H., G¨urpınar, F., Salah, A.A.: Video-based emo-
tion recognition in the wild using deep transfer learning
and score fusion. Image and Vision Computing 65, 66–75
(2017)

40. Kim, B.K., Roh, J., Dong, S.Y., Lee, S.Y.: Hierarchical
committee of deep convolutional neural networks for ro-
bust facial expression recognition. Journal on Multimodal
User Interfaces 10(2), 173–189 (2016)

41. Kingma, D.P., Ba, J.: Adam: A method for stochastic

optimization. In: Proceedings of ICLR (2015)

42. Kollias, D., Zafeiriou, S.: Expression, Aﬀect, Action Unit
Recognition: Aﬀ-Wild2, Multi-Task Learning and Arc-
Face. In: Proceedings of BMVC (2019)

43. Lanitis, A., Draganova, C., Christodoulou, C.: Com-
paring diﬀerent classiﬁers for automatic age estimation.
IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics) 34(1), 621–628 (2004)

44. Li, D., Wen, G.: MRMR-based ensemble pruning for fa-
cial expression recognition. Multimedia Tools and Appli-
cations 77, 15251–15272 (2017)

45. Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep
locality-preserving learning for expression recognition in
the wild. In: Proceedings of CVPR, pp. 2584–2593 (2017)
46. Li, Y., Zeng, J., Shan, S., Chen, X.: Patch-Gated CNN
for occlusion-aware facial expression recognition. In: Pro-
ceedings of ICPR, pp. 2209–2214 (2018)

47. Liu, X., Kumar, B., You, J., Jia, P.: Adaptive deep metric
learning for identity-aware facial expression recognition.
In: Proceedings of CVPRW, pp. 522–531 (2017)

48. Lopez-Paz, D., Bottou, L., Sch¨olkopf, B., Vapnik, V.:
Unifying distillation and privileged information. In: Pro-
ceedings of ICLR (2016)

49. McCloskey, M., Cohen, N.J.: Catastrophic Interference in
Connectionist Networks: The Sequential Learning Prob-
lem. Psychology of Learning and Motivation 24, 109–165
(1989)

50. Meng, Z., Liu, P., Cai, J., Han, S., Tong, Y.: Identity-
aware convolutional neural network for facial expression
recognition. In: Proceedings of FG, pp. 558–565 (2017)

51. Mollahosseini, A., Hasani, B., Mahoor, M.H.: AﬀectNet:
A Database for Facial Expression, Valence, and Arousal
Computing in the Wild. IEEE Transactions on Aﬀective
Computing 10(1), 18–31 (2019)

52. Mollahosseini, A., Hassani, B., Salvador, M.J., Abdollahi,
H., Chan, D., Mahoor, M.H.: Facial expression recogni-
tion from World Wild Web. In: Proceedings of CVPRW,
pp. 1509–1516 (2016)

53. Nam, S.H., Kim, Y.H., Truong, N.Q., Choi, J., Park,
K.R.: Age Estimation by Super-Resolution Reconstruc-
tion Based on Adversarial Networks.
IEEE Access 8,
17103–17120 (2020)

54. Park, W., Kim, D., Lu, Y., Cho, M.: Relational Knowl-
edge Distillation. In: Proceedings of CVPR, pp. 3962–
3971 (2019)

55. Parkhi, O.M., Vedaldi, A., Zisserman, A., et al.: Deep
Face Recognition. In: Proceedings of BMVC, pp. 6–17
(2015)

56. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., Duchesnay, E.:
Scikit-learn: Machine Learning in Python. Journal of
Machine Learning Research 12, 2825–2830 (2011)

57. Priadana, A., Maarif, M.R., Habibi, M.: Gender Predic-
tion for Instagram User Proﬁling using Deep Learning.
In: Proceedings of DASA, pp. 432–436 (2020)

58. Raﬁque, I., Hamid, A., Naseer, S., Asad, M., Awais, M.,
Yasir, T.: Age and Gender Prediction using Deep Convo-
lutional Neural Networks. In: Proceedings of ICIC, pp.
1–6 (2019)

Teacher-Student Training and Triplet Loss to Reduce the Eﬀect of Drastic Face Occlusion

19

78. You, S., Xu, C., Xu, C., Tao, D.: Learning from multiple
teacher networks. In: Proceedings of KDD, pp. 1285–1294
(2017)

79. Yu, L., Yazici, V.O., Liu, X., Weijer, J.v.d., Cheng,
Y., Ramisa, A.: Learning Metrics from Teachers: Com-
pact Networks for Image Embedding. In: Proceedings of
CVPR, pp. 2907–2916 (2019)

80. Yu, Z., Zhang, C.: Image based static facial expression
recognition with multiple deep network learning. In: Pro-
ceedings of ICMI, pp. 435–442 (2015)

81. Zeng, J., Shan, S., Chen, X.: Facial expression recognition
with inconsistently annotated datasets. In: Proceedings
of ECCV, pp. 222–237 (2018)

82. Zeng, X., Huang, J., Ding, C.: Soft-Ranking Label En-
coding for Robust Facial Age Estimation. IEEE Access
8, 134209–134218 (2020)

83. Zhang, Z., Song, Y., Qi, H.: Age progression/regression
by conditional adversarial autoencoder. In: Proceedings
of CVPR, pp. 5810–5818 (2017)

59. Romero, A., Ballas, N., Kahou, S.E., Chassang, A.,
Gatta, C., Bengio, Y.: FitNets: Hints for Thin Deep Nets.
In: Proceedings of ICLR (2015)

60. Schroﬀ, F., Kalenichenko, D., Philbin, J.: FaceNet: A uni-
In:
ﬁed embedding for face recognition and clustering.
Proceedings of CVPR, pp. 815–823 (2015)

61. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R.,
Parikh, D., Batra, D.: Grad-CAM: Visual Explanations
from Deep Networks via Gradient-based Localization. In:
Proceedings of ICCV, pp. 618–626 (2017)

62. Shah, J.H., Sharif, M., Yasmin, M., Fernandes, S.L.: Fa-
cial expressions classiﬁcation and false label reduction us-
ing LDA and threefold SVM. Pattern Recognition Letters
139, 166–173 (2017)

63. Shao, J., Gori, I., Wan, S., Aggarwal, J.: 3D dynamic
facial expression recognition using low-resolution videos.
Pattern Recognition Letters 65, 157–162 (2015)

64. She, J., Hu, Y., Shi, H., Wang, J., Shen, Q., Mei, T.: Dive
into ambiguity: Latent distribution mining and pairwise
uncertainty estimation for facial expression recognition.
In: Proceedings of CVPR, pp. 6248–6257 (2021)

65. Shi, C., Tan, C., Wang, L.: A facial expression recognition
method based on a multibranch cross-connection convo-
lutional neural network.
IEEE Access 9, 39255–39274
(2021)

66. Siqueira, H., Magg, S., Wermter, S.: Eﬃcient facial fea-
ture learning with wide ensemble-based convolutional
neural networks. Proceedings of AAAI 34(04), 5800–5809
(2020)

67. Tang, Y.: Deep Learning using Linear Support Vector
Machines. In: Proceedings of ICML Workshop on Chal-
lenges in Representation Learning (2013)

68. Vapnik, V., Vashist, A.: A new learning paradigm:
Learning using privileged information. Neural Networks
22(5–6), 544–557 (2009)

69. Vedaldi, A., Lenc, K.: MatConvNet – Convolutional Neu-
ral Networks for MATLAB. In: Proceeding of ACMMM,
pp. 689–692 (2015)

70. Vo, T.H., Lee, G.S., Yang, H.J., Kim, S.H.: Pyramid with
super resolution for in-the-wild facial expression recogni-
tion. IEEE Access 8, 131988–132001 (2020)

71. Wang, K., Peng, X., Yang, J., Lu, S., Qiao, Y.: Suppress-
ing uncertainties for large-scale facial expression recogni-
tion. In: Proceedings of CVPR, pp. 6897–6906 (2020)
72. Wang, X., Guo, R., Kambhamettu, C.: Deeply-Learned
Feature for Age Estimation. In: Proceedings of WACV,
pp. 534–541 (2015)

73. Wen, G., Hou, Z., Li, H., Li, D., Jiang, L., Xun, E.: En-
semble of deep neural networks with probability-based
fusion for facial expression recognition. Cognitive Com-
putation 9(5), 597–610 (2017)

74. Wikanningrum, A., Rachmadi, R.F., Ogata, K.: Improv-
ing Lightweight Convolutional Neural Network for Facial
Expression Recognition via Transfer Learning. In: Pro-
ceedings of CENIM, pp. 1–6 (2019)

75. Xia, M., Zhang, X., Liu, W., Weng, L., Xu, Y.: Multi-
Stage Feature Constraints Learning for Age Estimation.
IEEE Transactions on Information Forensics and Secu-
rity 15, 2417–2428 (2020)

76. Yim, J., Joo, D., Bae, J., Kim, J.: A Gift from Knowl-
edge Distillation: Fast Optimization, Network Minimiza-
tion and Transfer Learning. In: Proceedings of CVPR,
pp. 7130–7138 (2017)

77. You, Q., Bhatia, S., Sun, T., Luo, J.: The Eyes of the Be-
holder: Gender Prediction Using Images Posted in Online
Social Networks. In: Proceedings of ICDM Workshops,
pp. 1026–1030 (2014)

