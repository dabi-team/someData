9
1
0
2

g
u
A
7
1

]

V

I
.
s
s
e
e
[

1
v
9
3
2
6
0
.
8
0
9
1
:
v
i
X
r
a

Impacts of Retina-related Zones on Quality Perception of
Omnidirectional Image

Huyen T. T. Tran1, Duc V. Nguyen1, Nam Pham Ngoc2, Trang H. Hoang3, Truong Thu Huong3,
and Truong Cong Thang1

1The University of Aizu, Aizuwakamatsu, Japan
2Vin-University project, Vietnam
3Hanoi University of Science and Technology, Hanoi, Vietnam

Abstract

Virtual Reality (VR), which brings immersive experiences
to viewers, has been gaining popularity in recent years. A
key feature in VR systems is the use of omnidirectional
content, which provides 360-degree views of scenes. In
this work, we study the human quality perception of omni-
directional images, focusing on diﬀerent zones surround-
ing the foveation point. For that purpose, an extensive
subjective experiment is carried out to assess the percep-
tual quality of omnidirectional images with non-uniform
quality. Through experimental results, the impacts of dif-
ferent zones are analyzed. Moreover, nineteen objective
quality metrics, including foveal quality metrics, are evalu-
ated using our database. It is quantitatively shown that the
zones corresponding to the fovea and parafovea of human
eyes are extremely important for quality perception, while
the impacts of the other zones corresponding to the per-
ifovea and periphery are small. Besides, the investigated
metrics are found to be not eﬀective enough to reﬂect the
quality perceived by viewers.

1 Introduction

In order to bring immersive experiences to viewers, vir-
tual reality (VR) systems employ omnidirectional content
which contains 360-degree views of scenes. Unlike tra-
ditional content displayed using a ﬂat screen, omnidirec-
tional content is usually consumed using Head Mounted

Displays (HMDs). Also, only a small part of the full con-
tent (called viewport) corresponding to the current viewing
direction is actually seen by the viewer at a moment [1].

Because omnidirectional (or 360-degree) content has
very high bitrate, a key challenge in omnidirectional con-
tent delivery is how to optimize system resources while
still ensuring satisfactory user experience. For that, many
encoding and delivery solutions have been proposed in the
literature, where the (estimated) viewport is provided with
high quality and the remaining part with low quality [2–4].
Moreover, in VR systems, foveated imaging, which de-
creases quality of zones far from the viewer’s foveation
point [5, 6], can be used to further reduce resource con-
sumption [6,7]. However, the estimated viewing direction
could be very diﬀerent from the actual one when the sys-
tem delay is large [8]. Even the viewer may suddenly turn
to look at the back.
In these cases, the actual viewport
may have low quality in the central part and high qual-
ity in the periphery. In other words, the central part may
have higher quality (called scenario S#1) or lower quality
(called scenario S#2) than the periphery, both resulting in
omnidirectional content with non-uniform quality.

It is well-known that human visual acuity is spatially
variable [9, 10]. In particular, when a person gazes at a
point, called foveation point, a zone closer to this point is
perceived to be sharper than the others. This means that
the human eyes have a higher sensitivity to distortions in
the central than in the periphery. Hence, the understanding
of the impacts of diﬀerent zones on the perceptual quality

1

 
 
 
 
 
 
is obviously of indispensable necessity in the context of
omnidirectional content.

In the literature, there are only a few existing studies
on subjective quality assessments of images/videos with
non-uniform quality [7, 11, 12]. However, most of these
studies are devoted to traditional content [11, 12]. In [11],
each image is divided into four zones of equal widths. The
quality of these zones gradually decreases with a ﬁxed step
size. It is found that, when the step size is small, the diﬀer-
ence of perceptual quality between the non-uniform and
uniform videos is insigniﬁcant. In addition, the maximum
value of the step size without causing signiﬁcant quality
diﬀerences depends on content characteristics.
In [12],
each image is divided into three zones, which are foveal,
blending, and peripheral zones. Through experimental re-
sults, the ﬁnding is that participants barely notice quality
decreases at the peripheral zones of the eccentricity larger
than 7.5 degrees. Also, an evaluation of four subjec-
tive assessment methods is presented. It is indicated that
the Absolute Category Rating (ACR) method is the best
method to evaluate the subjective quality of non-uniform
images.

In the literature, there have been some studies on sub-
jective quality assessments of omnidirectional content
[13–16].
In these studies, various distortion types such
as compression and Gaussian blur are considered. How-
ever, the distortions are distributed uniformly in [13–16].
The work in [7] is the only previous study on omnidirec-
tional content with non-uniform quality. In [7], the authors
focus on answering the question of how to spatially reduce
image quality without causing impacts on user perception.
For that purpose, they propose to divide an omnidirec-
tional image into three areas according to three regions of
the human retina, namely the macula, the near periphery,
and the far periphery. The image quality corresponding
to each region is decreased step by step until participants
notice a perceptual diﬀerence. The encoding parameters
obtained just before that point are modeled and then used
as a guide for spatially reducing image quality without
perceptual loss. It is shown that this approach could save
loading time about 90% comparing to a conventional ap-
proach using uniform quality.

Over several decades, a large number of objective qual-
ity metrics have been proposed [17–21]. Some of these
metrics take into account the foveation feature, hereafter
referred to as foveal quality metrics [20, 21]. However, all

these metrics are speciﬁc to traditional content. There has
been no existing foveal quality metric for omnidirectional
content so far.

In our previous study [22], a comparison between eight
state-of-the-art quality metrics has been conducted. Ex-
perimental results show that PSNR turns out to be the most
eﬀective metric for quality assessment of omnidirectional
videos. However, it is worth to note that stimuli used in
that study have uniform quality. As shown later in this
paper, PSNR is actually not eﬀective when the quality
is spatially variable. To the best of our knowledge, no
extensive evaluation of objective quality metrics for om-
nidirectional images with non-uniform quality has been
conducted in the literature.

In this study, our purposes related to user perception of

omnidirectional content in VR systems include:

• Subjective study on the impacts of retina-related
zones on quality perception of omnidirectional im-
ages.

• Performance evaluation of existing objective quality
metrics, especially foveal quality metrics, for omni-
directional images having non-uniform quality.

To that end, our major contributions are as follows.
First, we present a detailed description of a VR viewing
geometry and the human retina. This description helps
in designing subjective experiments and in calculating pa-
rameters used in foveal quality metrics. Second, we carry
out an extensive subjective experiment with 256 stimuli of
non-uniform quality. The quality zones of the stimuli are
designed based on ﬁve regions of the human retina. Third,
using a simple zone-weighted formulation, we quantify the
impacts of diﬀerent zones on the perceptual quality. It is
quantitatively found that the zones corresponding to the
fovea and parafovea of the human retina are extremely im-
portant for quality perception. Also, the impacts of zones
are strongly aﬀected by content characteristics. Fourth,
we evaluate the correlation of nineteen objective quality
metrics against subjective scores. Experimental results in-
dicate that these metrics, even the foveal ones, are not very
eﬀective when the viewport quality is spatially variable.

The remainder of the paper is organized as follows. A
description of a VR viewing geometry and the human
retina is presented in Sect. 2. Sect. 3 presents the details
of the subjective experiment. The analysis of perceptual

2

behaviors using the experimental results is provided in
Sect. 4. Then, an evaluation of quality metrics is presented
in Sect. 5. Section 6 concludes the paper and provides an
outlook on future work.

2 Overview

In this section, the viewing geometry in VR systems is
ﬁrst presented. Then, the regions in human retina are
described.

2.1 Viewing Geometry in VR Systems

Fig. 1 illustrates a typical viewing geometry in VR sys-
tems. Assume that VP is the displayed viewport, the lens
in the HMD produces a virtual viewport VP(cid:48) that is fur-
ther formed on the retina in the human eyes. Eccentricity e
(degrees) is used to measure the angular distance from the
central gaze direction to any point in the virtual viewport
VP(cid:48).

Let F (units of length) be the focal length of the lens.
S0, S1, and S2 (units of length) respectively denote the
distances from the lens to the displayed viewport VP, the
virtual viewport VP(cid:48), and the eye. Based on lens equations,
the distance from the lens to the virtual viewport S1 is
computed by

S1 = S0 ×

F
F − S0

[units of length].

(1)

Then, the distance from the eye to the virtual viewport is
calculated by

S3 = S1 + S2

[units of length].

(2)

Let Wp×Hp (pixels) and Wl×Hl (units of length) respec-
tively be the width and height of the displayed viewport
VP in pixels and units of length. The width of the virtual
viewport VP(cid:48) (in pixels and units of length) is given by the
following equations.

Figure 1: Typical viewing geometry in VR systems

Also, the height of the virtual viewport VP(cid:48) is calculated
by

H (cid:48)
p

= Hp

and

H (cid:48)
l

= Hl ×

F
F − S0

[pixels]

(5)

[units of length].

(6)

Assume that the foveation point is the center O(cid:48) =
(xO(cid:48), yO(cid:48)) in the virtual viewport VP(cid:48). Point O = (xO, yO)
in the displayed viewport VP corresponding to point O(cid:48) is
determined by

and

xO = xO(cid:48)

yO = yO(cid:48)

[pixels]

(7)

[pixels].

(8)

Let M be a point at the position of (xM, yM ) (pixels)
in the displayed viewport VP. The position of the virtual
point M (cid:48) = (xM(cid:48), yM(cid:48)) corresponding to point M is

and

xM(cid:48) = xM

yM(cid:48) = yM

[pixels]

(9)

[pixels].

(10)

W (cid:48)
p

= Wp

[pixels].

(3)

The distance from pixel M (cid:48) to the foveation point O(cid:48) is

W (cid:48)
l

= Wl ×

F
F − S0

[units of length].

(4)

3

d (cid:48) =

(cid:118)(cid:117)(cid:116)(cid:32) (xM(cid:48) − xO(cid:48)) × W (cid:48)
W (cid:48)
p

l

(cid:33)2

+

(cid:32) (yM(cid:48) − yO(cid:48)) × H (cid:48)
l
H (cid:48)
p

(cid:33)2

(11)

[units of length].

at the center of the retina, whereas rods are located away
from the center. Visual information from photoreceptors
are then collected by the so-called ganglion cells. The
optic disk is where axons from ganglion cells exit the
retina and convey visual information to the brain.

Based on the ganglion cell layer, the retina of human
eyes can be divided into two main parts, namely macula
and periphery [24], as illustrated in Fig. 3. In particular,
the ganglion cell layer in the macula is several cells thick.
Meanwhile, the periphery is only one ganglion cell thick.
The macula is further divided into three regions, called
fovea, parafovea, and perifovea. The periphery is in turn
divided into two regions, namely near periphery and far
periphery [24, 25]. These ﬁve regions of the retina are
brieﬂy described below. It is worth noting that there has
been no standard deﬁnition of boundaries between these
regions so far [26].
In our research, the boundaries are
determined based on [26–29].

The fovea is a small central region of the macula that
represents 5 degrees of the central visual ﬁeld or an ec-
centricity interval between 0 degree and 2.5 degrees. This
region consists of densely packed cones. In addition, it
has a layer of ganglion cells, which can be up to eight
cells thick. Therefore, the fovea vision has the highest
sensitivity to ﬁne details.

The fovea is surrounded by the parafovea belt corre-
sponding to an eccentricity interval between 2.5 degrees
and 4 degrees.
In the parafovea, rods are more numer-
ous. Meanwhile, the thickness of the ganglion cell layer
decreases from eight to four cells at its outer edge [25].

The region next to the parafovea is the perifovea with
the corresponding eccentricity interval between 4 degrees
and 9 degrees. In this region, the density of rods is higher
than that of cones. The thickness of ganglion cell layer
reduces to one cell at its peripheral edge [25].

In the periphery, the region corresponding to an eccen-
tricity interval between 9 degrees and 30 degrees is the
near periphery, and the rest is the far periphery. The di-
viding line corresponding to the eccentricity of 30 degrees
is selected based on several features of visual performance.
In particular, letter visual acuity decreases linearly with ec-
centricity from 0 degree to 30 degrees. For eccentricities
larger than 30 degrees, the decrease is much steeper [9].

Based on the above description of the viewing geome-
try and the retina, stimuli used in the following subjective
experiment are designed so that the zones in the virtual

Figure 2: Density of photoreceptors in the retinal [23]

Figure 3: Five regions of the retina

The eccentricity e of point M (cid:48) in the virtual viewport

VP(cid:48) is given by

e(xM(cid:48), yM(cid:48)) = tan−1

(cid:19)

(cid:18) d (cid:48)
S3

[degrees].

(12)

It should be noted that parameters of a point on the
virtual viewport are what actually used in a foveal quality
metric. Moreover, given the knowledge of the human
visual system, points on the virtual viewport can be divided
according to the regions of the retina.

2.2 Regions in Human Retina

In the human retina, there are two types of photoreceptors,
namely rods and cones, each plays an important role in
human visual system. In particular, cones function most
eﬀectively in relatively bright light and are responsible
for color vision and visual acuity. Meanwhile, rods have
higher sensitivities to light, and thus they function mainly
in dim light.

Fig. 2 shows the density of photoreceptors in the human
It can be seen that most cones are concentrated

retina.

4

(a) I1

(b) I2

(c) I3

(d) I4

(e) I5

(f) I6

(g) I7

(h) I8

Figure 4: Eight omnidirectional images used in our experiment

Table 1: Features of source images

Image
I1
I2
I3
I4
I5
I6
I7
I8

Description

indoor scene, large conference room, containing human faces
indoor scene, train station in Japan, containing human faces
indoor scene, small kindergarten classroom, containing human faces
indoor scene, meeting room, without presence of human
outdoor scene, natural landscape, daytime, without presence of human
outdoor scene, balcony, nighttime, without presence of human
outdoor scene, festival, daytime, containing human faces
outdoor scene, outside of a cathedral, at sunset, containing human faces

Table 2: Eccentricity intervals of zones

Zone
Eccentricity interval
(degrees)

Z1

Z2

Z3

Z4

Z5

[0,2.5)

[2.5,4)

[4,9)

[9,30)

[30,+∞)

viewports will correspond to the ﬁve regions of the retina.
It is worth noting that, in this paper, we focus on the
contributions (or weights) of diﬀerent zones in the per-
ceptual quality, rather than the quality-reducing trends as
in [7, 11, 12].

3 Experiment Description

For the experiment, we used eight omnidirectional images,
denoted by I1∼ I8, as shown in Fig. 4. Two images I5 and
I7 were obtained on Flickr under of the Creative Commons
(CC) copyrights. The other six images were selected from

the SUN 360 Database [30, 31]. The characteristics of
these images are described in Table 1. It can be seen that
the selected images cover various categories of capturing
environment and presence of human. All these images
were down sampled to the resolution of 8192×4096. We
asked 10 participants to freely observe the source images
and then point out attractive objects. Based on the obtained
results, we selected a foveation point corresponding to a
viewport for each image.

In order to generate stimuli of non-uniform quality, each
image was ﬁrst spatially divided into ﬁve zones, denoted
Z1, Z2, Z3, Z4, and Z5. In particular, each zone represents
an eccentricity interval as shown in Table 2. It can be seen
that zones Z1, Z2, Z3, Z4, and Z5 respectively correspond
to the fovea, parafovea, perifovea, near periphery, and far
periphery in the retina. Fig. 5 illustrates the boundaries of
the zones in the viewports used in our experiment.

As described in Sect. 1, we consider two basic scenarios
of spatial quality changes. In the ﬁrst scenario (S#1), the
center has higher quality than the periphery; and in the
second scenario (S#2), the center has lower quality than
the periphery. For each scenario, we used four quality
variation patterns as shown in Table 3. In patterns P1, P2,
P3, and P4, which belong to scenario S#1, the number
of high quality zones gradually increases from 1 to 4. In
the remaining patterns (i.e., P5, P6, P7, and P8), which
belong to scenario S#2, the number of high quality zones
gradually reduces from 4 to 1.

In this study, we used one high quality level correspond-

5

(a) I1

(b) I2

(c) I3

(d) I4

(e) I5

(f) I6

(g) I7

(h) I8

Figure 5: Boundaries of zones in viewports used in our experiment

ing to the quality level of the source images, and four low
quality levels corresponding to four blurring levels. These
blurring levels were generated using Gaussian ﬁlters with
a ﬁxed ﬁlter size of 50 and four diﬀerent standard devia-
tions σ. For scenario S#1, the four σ values are 2, 4, 8,
and 12. For scenarios S#2, the four σ values are 1, 2, 4,
and 6. The diﬀerence between the two scenarios is due to
the fact that blurring in zones close to the foveation point
is easier to be perceived than in the others. The source
and blurred images were then blended into stimuli of non-
uniform quality. Speciﬁcally, the high quality zones in
the stimuli consist of pixels of the source images, and the
low quality zones are comprised of pixels of the blurred
images. Similar to [12], to prevent noticeable boundaries
between low and high quality zones, belts with the width
of 5 degrees between two adjacent zones having a quality
switch were used as transition belts. The quality levels
in these belts smoothly change using a linear function.
Totally, our database consists of 256 stimuli, which were
rated in the below tests.

To display the stimuli, we used a device set of a Samsung
Galaxy S6 smartphone and a Samsung Gear VR headset
with the 96 degree ﬁeld of view. The Samsung Galaxy S6
has the screen resolution of 2560×1440 and the display
size of 5.1 inches. For the Samsung Gear VR headset,
the focal length of the lens is F=62mm, and the distances
from the lens to the displayed viewports and the eyes are
approximately S0=25mm and S2=10mm respectively.

In the tests, we used the Absolute Category Rating
method [32], which is shown the best method in [12].
Before doing actual tests, participants were trained to get
accustomed to the devices and the rating procedure. In ad-
dition, they were instructed to appropriately adjust devices
to obtain the best experience. During the test process, the
stimuli were randomly displayed one at a time. Note that,

Table 3: Quality Variation Patterns (HQ: High quality and
LQ: Low quality)

Scenario

Pattern

S#1

S#2

P1
P2
P3
P4
P5
P6
P7
P8

Z1
[0◦,2.5◦)
HQ
HQ
HQ
HQ
LQ
LQ
LQ
LQ

Quality levels of zones
Z4
Z3
Z2
[9◦,30◦)
[4◦,9◦)
[2.5◦,4◦)
LQ
LQ
LQ
LQ
LQ
HQ
LQ
HQ
HQ
HQ
HQ
HQ
HQ
HQ
HQ
HQ
HQ
LQ
HQ
LQ
LQ
LQ
LQ
LQ

Z5
[30◦,+∞)
LQ
LQ
LQ
LQ
HQ
HQ
HQ
HQ

for a stimulus, the corresponding viewport displayed on
HMD was ﬁxed during the test. Participants were asked
to look straight ahead at each viewport displayed directly
in front of them to keep focusing on the center, where
has an attractive object such as a human face or a ﬂower
vase. After stabilizing the gaze direction, each participant
verbally gave a score with the grade scale from 1 (bad) to
5 (excellent) which was recorded by an assistant.

For each stimulus, the viewing duration was decided by
the participants themselves to obtain more reliable rating
scores. Commonly, the participants spent about 5 seconds
for rating a stimulus and then took a break of 5 seconds.
To avoid the negative impacts of fatigue and boredom, the
tests were divided into 6 sessions conducted in diﬀerent
weeks. Each participant took part in only two sessions.
The duration of each session was no more than 10 minutes.
There were totally 62 participants between the ages of 20
and 30. A screening analysis of the obtained results was
performed following Recommendation ITU-T P.913 [32],
and two participants were rejected. After discarding the
scores of these two participants, each stimulus was scored
by 20 valid participants. The mean opinion score (MOS)

6

G(cid:48)(xM(cid:48), yM(cid:48)) = G(xM, yM ).

(14)

The mean squared error (MSE) of pixels in zone Zk is

computed by

W (cid:48)
p(cid:213)

H(cid:48)
p(cid:213)

MSEk =

xM (cid:48) =1

yM (cid:48) =1

where

[V (cid:48)(xM(cid:48), yM(cid:48)) − G(cid:48)(xM(cid:48), yM(cid:48))]2 × Rk(xM(cid:48), yM(cid:48))

W (cid:48)
p(cid:213)

H(cid:48)
p(cid:213)

xM (cid:48) =1

yM (cid:48) =1

Rk(xM(cid:48), yM(cid:48))

,

(15)

Rk(xM(cid:48), yM(cid:48)) =

(cid:26)1,
0,

if ek−1 ≤ e(xM(cid:48), yM(cid:48)) < ek
otherwise

(16)

The zone-weighted formulation, called ZWF, is given

by

ZWF = 10 log10

(cid:32)

MAX2
k=1(wk × MSEk)

(cid:205)K

(cid:33)

[dB],

(17)

where MAX is the maximum possible pixel value. Here
we set MAX to 255 as the bit depth of pixels is 8 bits in
our experiment.

In some previous studies [22,33], it was shown that four-
parameter and ﬁve-parameter logistic functions are good
mappings between objective quality metrics and MOS.
In this work, we deployed the following ﬁve-parameter
logistic function to map the ZWF values and the MOS
values in our database.

y = β1

(cid:18) 1
2

−

(cid:19)

1
1 + eβ2(x−β3)

+ β4 x + β5,

(18)

where {βi |i ∈ {1, 2, ..., 5}} are parameters to be ﬁtted. The
values of the parameters βi’s and the weights wk’s were
determined by means of least squares ﬁtting as in [34].

4.2 Discussion

To quantify the impact of each zone taking into account
the eﬀects of content characteristics, the weights wk’s are
derived for each source image by ﬁtting using the above
ﬁve-parameter logistic function with the stimuli of that

Figure 6: 95% conﬁdence intervals of MOS values

of a stimulus is the average score of the valid participants.
The 95% conﬁdence intervals of the MOS values are
shown in Fig. 6. We can see that the scores cover fully
the value range from 1 to nearly 5. Generally, the conﬁ-
dence intervals are smaller at the two ends of the grade
scale. This is because the participants are more conﬁdent
in rating stimuli of very high (or low) quality.

4 Analysis of Perceptual Behaviors

in Zones

4.1 Quantifying impacts of zones

In this part, we present a zone-weighted formulation which
will be used to analyze the impacts of diﬀerent zones on the
perceptual quality of omnidirectional images. In general,
the virtual viewport is divided into K zones {Zk |1 ≤ k ≤
K }, each consists of Nk pixels with the corresponding
eccentricities e ∈ [ek−1, ek). Currently, we use K = 5
as described in Sect. 3. Each zone Zk is then assigned a
weight {wk |1 ≤ k ≤ K } representing the impact of that
zone on human perception of quality. Note that (cid:205)K
k=1 wk =
1.

Let V(xM, yM ) and G(xM, yM ) respectively be the val-
ues of pixel M = (xM, yM ) in the displayed viewports
of the original and distorted images. The values of the
corresponding pixel M (cid:48) = (xM(cid:48), yM(cid:48)) in the virtual view-
ports of the original and distorted images are respectively
calculated by the following equations.

V (cid:48)(xM(cid:48), yM(cid:48)) = V(xM, yM ).

(13)

7

Table 5: Performance of ﬁtting between the ZWF formu-
lation and MOS

I1
0.99
PCC
RMSE 0.15

I2
0.99
0.13

I3
0.99
0.14

I4
0.97
0.27

I5
0.98
0.24

I6
0.99
0.10

I7
0.98
0.20

I8
0.99
0.12

actually varies in a wide range. Also, with some images,
the value of w2 is insigniﬁcant. Usually, the higher the
value of w1 is, the lower the value of w2 becomes. More
speciﬁcally, with images I3 and I7, the values of w1 are
very high. This may be because the participants focus
primarily on the small face at the center of the viewports.
Such phenomenon was also observed in [11]. In particular,
it was found that a talking face is strongly attractive to
human attention [11].
In addition, in these viewports,
there are no other interesting objects near the center. With
images I1 and I4, the participants may also pay some
attention to other objects near the center (e.g., another
face in image I1), so the values of w1 are lower than those
of images I3 and I7. With images I5 and I8, the center’s
object is not very clear (small faces in image I8) or not
very attractive (a house in image I5), resulting in lower
values of w1. Especially, with images I2 and I6, the values
of w2 are comparable to those of w1. In these images, the
participants may look at a large central area rather than
zone Z1 only. The reason is that, in image I2, the clock at
the center is larger than zone Z1; and in image I6, the object
at the center does not stand out from the neighboring area.
From the above, we can see that the perceptual quality
is aﬀected by two key factors. The ﬁrst is the sensitivity
of human eyes. Especially, in the considered context,
zones Z1 and Z2 are much more important than the other
zones. The second is content characteristics. In particular,
the values of w1 and w2 vary widely according to 1) the
attractiveness and 2) the size of the central object, as well
as 3) the presence of neighboring objects.

5 Evaluation of Quality Metrics

In this part, by using our database, we evaluate the per-
formances of nineteen existing objective quality metrics
(OQM). The goal is to examine whether existing metrics,
especially foveal quality metrics, are eﬀective for quality
assessments of omnidirectional images with non-uniform

Figure 7: Weights of zones for each source image

Table 4: Weights of zones for each source image

Image

I1
I2
I3
I4
I5
I6
I7
I8

Weight
w3
0.088
0.033
0.024
0.063
0.087
0.064
0.019
0.095

w2
0.088
0.407
0.024
0.063
0.087
0.404
0.019
0.204

w4
0.048
0.033
0.024
0.063
0.087
0.064
0.019
0.095

w1
0.728
0.495
0.905
0.759
0.650
0.404
0.941
0.545

w5
0.048
0.032
0.024
0.052
0.087
0.064
0.003
0.061

image only. The obtained values of the weights are shown
in Fig. 7 and Table 4. The correlation coeﬃcients includ-
ing Pearson Correlation Coeﬃcient (PCC) and Root Mean
Square Error (RMSE), which are used to quantify the per-
formance of the ﬁtting between the ZWF formulation and
the MOS, are shown in Table 5. We can see that, for all
the source images, the PCC values are very high and the
RMSE values are very low. In particular, the lowest PCC
value is 0.97 while the highest RMSE value is 0.27. This
means that the ﬁtting to obtain the weights is reliable.

From Table 4, it can be seen that, except w1 and w2, all
the other weights are small (i.e., ≤ 0.095). That means
the zones outside the eccentricity of 4 degrees have little
impacts on the perceptual quality. Among the weights, w1
is usually highest, which is consistent with the fact that
the fovea region of the retina has the highest cone density.
Also, because w1 ≥ w2 ≥ w3 ≥ w4 ≥ w5, distortions
closer to the center have more signiﬁcant eﬀects on the
perceptual quality than distortions far from the center.

Based on Fig. 7, it is interesting that the value of w1

8

Table 6: Descriptions of objective quality metrics tested in this study. PW: Whether or not the metric diﬀerentiates
pixels’ contributions. FF: Whether or not the metric takes into account the foveation feature.

Metrics

MSE
VPSNR
SSIM [17]

MS-SSIM [18]

UQI [19]

VIFp [35]

VIF [35]

NQM [36]

PW FF
No
No
No

No

No

No

No

No

No

No

IW-PSNR [37]

Yes No

IW-SSIM [37]

FSIM [38]

FSIMc [38]

Yes No

Yes No

Yes No

RFSIM [39]

Yes No

SR-SIM [40]

Yes No

FWQI [41]

Yes Yes

WSNR [36]

Yes Yes

FWSNR [20]

Yes Yes

FPSNR [42]

Yes Yes

F-SSIM [21]

Yes Yes

Description

No

No Mean Squared Error, Calculated based on visble pixels of a viewport with equal weights
No Viewport-PSNR, Calculated based on visble pixels of a viewport with equal weights
Structural SIMilarity, Calculated based on the concept of structural similarity
No
Multi-scale SSIM, Calculated based on similar measures computed at diﬀerent resolutions (or
multi-scales) of a viewport
Universal Image Quality, Modeling any distortion as a combination of three diﬀerent factors
including loss of correlation, luminance distortion, and contrast distortion
Visual Information Fidelity in the pixel domain (VIFp) and the wavelet domain (VIF),
Calculated based on the connections between image information and visual quality

No

No

Noise Quality Measure, Signal-to-Noise Ratio of the restored distorted image with respect to
the model restored image
Information content Weighted PSNR, Combining information content weighting with PSNR
measures
Information content Weighted SSIM, Combining information content weighting with MS-SSIM
measures
Feature similarity, Combining low-level feature weighting with local similarity measures
Feature similarity incorporating the chromatic information, Combining low-level feature
weighting with local similarity measures
Riesz Transforms based Feature Similarity, Combining low-level feature weighting based on
Riesz Transforms with local similarity measures
Spectral Residual based Similarity, Calculated based on a spectral residual visual saliency
model
Foveated Wavelet image Quality Index (FWQI), Calculated based on wavelet coeﬃcients in
the discrete wavelet transform domain using the foveation-based error sensitivity model as a
weighting function.
Weighted Signal-to-Noise Ratio, the ratio of the average weighted signal power to the average
weighted noise power, where the weighting function is the contrast sensitivity function
Foveal Weighted Signal-to-Noise Ratio, Combining weighting for each pixel by the local
frequency at that pixel with WSNR measures
Foveal Peak Signal-to-Noise Ratio, Combining weighting for each pixel by the local frequency
at that pixel with PSNR measures
Foveal-SSIM, Combining weighting for each macroblock based on the local frequency of pixels
in that macroblock with SSIM measures

9

(a) PCC

(b) RMSE

Figure 8: Performances of objective quality metrics

quality.

mentioned in Subsect. 4.1.

5.1 Description of metrics

5.2 Discussion

Table 6 shows the notations and descriptions of the nine-
In this table, the
teen metrics considered in this study.
PW column indicates whether a metric diﬀerentiates the
contributions of diﬀerent pixels; and the FF column in-
dicates whether a metric takes into account the foveation
feature of the human eye. Because the implementations
of the FWQI, FWSNR, FPSNR, and F-SSIM metrics are
not publicly available, we implemented them based on the
corresponding publications [20, 21, 41, 42]. For the re-
maining metrics, we used the implementations provided
by the original authors.

It is worth noting that all of these metrics were pro-
posed to calculate for all pixels in a traditional image.
In this study, these metrics were calculated for viewports
only (i.e., visible pixels) of the omnidirectional images to
reﬂect what is actually watched by viewers. To extract the
viewports, we used 360Lib software developed by Joint
Video Experts Team (JVET) [43]. In addition, geometric
parameters in these metrics were calculated based on the
equations presented in Subsect. 2.1.

In order to evaluate the performances of the OQM met-
rics, we used two performance metrics of Pearson Cor-
relation Coeﬃcient (PCC) and Root Mean Square Error
(RMSE). Similar to [33], a nonlinear regression was ap-
plied to map the OQM values to the MOS values using
the ﬁve-parameter logistic function (i.e., Equation (18))

Fig. 8 shows the PCC and RMSE values of the OQM
metrics when ﬁtting with all the MOSs in our database.
It can be seen that all the metrics have very low PCC
values (i.e., PCC < 0.70) and very high RMSE values
(i.e., RMSE > 0.80). Even the foveal quality metrics
(namely FWQI, WSNR, FWSNR, FPSNR, and F-SSIM)
have bad PCC values (i.e., from 0.08 to 0.59). This means
that the investigated metrics are not eﬀective to assess the
perceptual quality of omnidirectional images with non-
uniform quality.

Similar to the previous analysis related to the ZWF for-
mulation, it is important to understand the performances
of the metrics for each source image. Table 7 shows the
performances of the metrics when ﬁtting with the stimuli
of each source image. It can be seen that, for all the met-
rics, the PCC and RMSE values are drastically variable
across diﬀerent source images. The bold numbers show
the metrics having the highest performance for each source
image.

Among the investigated metrics, the FPSNR metric has
the highest PCC values for ﬁve source images (i.e., I1, I2,
I4, I6, and I8). In addition, its PCC values for six images
are quite good (i.e., PCC > 0.80). Especially, with images
I6 and I8, the PCC values are nearly perfect (0.99 and
0.98). However, for two images I3 and I7, its PCC values
are very low (i.e., PCC < 0.70), even lower than those of

10

Table 7: Performances of metrics calculated with the stimuli of each source image. The bold numbers show the metrics
having the highest performance for each source image.

Metrics

MSE
VPSNR
SSIM
MS-SSIM
UQI
VIFp
VIF
NQM
IW-PSNR
IW-SSIM
FSIM
FSIMc
RFSIM
SR-SIM
FWQI
WSNR
FWSNR
FPSNR
F-SSIM

PCC

I1
0.68
0.69
0.42
0.47
0.59
0.63
0.63
0.62
0.63
0.45
0.40
0.40
0.59
0.36
0.10
0.66
0.67
0.84
0.41

I2
0.64
0.56
0.42
0.48
0.64
0.67
0.68
0.74
0.53
0.46
0.41
0.41
0.53
0.41
0.04
0.66
0.62
0.86
0.42

I3
0.76
0.69
0.48
0.52
0.72
0.79
0.71
0.73
0.53
0.49
0.48
0.48
0.55
0.43
0.01
0.53
0.63
0.59
0.44

I4
0.64
0.68
0.43
0.49
0.75
0.75
0.66
0.69
0.64
0.48
0.43
0.43
0.51
0.42
0.09
0.61
0.59
0.84
0.41

I5
0.74
0.80
0.53
0.60
0.68
0.70
0.70
0.69
0.60
0.55
0.48
0.48
0.62
0.45
0.11
0.83
0.42
0.81
0.51

I6
0.57
0.69
0.41
0.45
0.68
0.62
0.62
0.66
0.67
0.45
0.42
0.42
0.51
0.42
0.27
0.67
0.65
0.99
0.42

I7
0.60
0.57
0.38
0.43
0.54
0.61
0.64
0.69
0.65
0.43
0.39
0.38
0.50
0.35
0.31
0.51
0.62
0.59
0.40

I8
0.70
0.60
0.38
0.45
0.76
0.61
0.79
0.69
0.65
0.43
0.38
0.38
0.52
0.39
0.13
0.65
0.58
0.98
0.38

I1
0.70
0.69
0.87
0.84
0.77
0.75
0.74
0.75
0.74
0.85
0.88
0.88
0.78
0.89
0.95
0.72
0.71
0.52
0.87

I2
0.82
0.88
0.96
0.93
0.81
0.79
0.77
0.71
0.90
0.94
0.96
0.96
0.90
0.97
1.06
0.80
0.83
0.54
0.96

I3
0.75
0.83
1.01
0.99
0.80
0.70
0.81
0.79
0.98
1.01
1.02
1.02
0.97
1.04
1.16
0.98
0.90
0.93
1.04

RMSE
I5
I4
0.80
0.86
0.70
0.81
1.00
1.02
0.95
0.99
0.87
0.73
0.85
0.73
0.84
0.83
0.85
0.81
0.95
0.85
0.99
0.97
1.04
1.01
1.04
1.01
0.93
0.96
1.07
1.00
1.18
1.10
0.87
0.65
1.07
0.90
0.70
0.60
1.01
1.01

I6
0.80
0.70
0.89
0.87
0.72
0.77
0.76
0.73
0.72
0.87
0.88
0.88
0.84
0.89
0.94
0.72
0.75
0.12
0.89

I7
0.83
0.85
0.96
0.94
0.87
0.82
0.80
0.75
0.79
0.94
0.96
0.96
0.90
0.98
1.01
0.90
0.82
0.84
0.95

I8
0.68
0.76
0.88
0.84
0.62
0.75
0.58
0.69
0.72
0.85
0.87
0.87
0.81
0.87
0.94
0.72
0.77
0.20
0.87

(a) FWQI

(b) WSNR

(c) FWSNR

(d) FPSNR

(e) F-SSIM

Figure 9: Scatter plots of the values of the foveal quality metrics versus the MOS values for image I7

11

the MSE metric (i.e., 0.59 vs. 0.76 and 0.59 vs. 0.60),
which is the simplest metric in practice.

As for the other quality metrics, their performances are
mostly low. Even the other foveal quality metrics (i.e.,
except the FPSNR metric) have lower performances than
the non-foveal and simple metrics.

To understand the actual behaviors of the foveal qual-
ity metrics that cause low performances, Fig. 9 shows the
scatter plots of the values of these metrics versus the MOS
values for image I7. In this ﬁgure, we use diﬀerent leg-
ends to diﬀerentiate the stimuli of scenario S#1, where the
center has higher quality, and the stimuli of scenario S#2,
where the center has lower quality. It is well-known that
higher values of these metrics mean higher MOS values
and better perceptual quality. From Fig. 9, we can see
that the MOS values in scenario S#1 are generally higher
than those in scenario S#2. However, for the WSNR and
F-SSIM metrics, most of their values in scenario S#1 are
signiﬁcant lower than those in scenario S#2. For the re-
maining metrics, with the same MOS value, their corre-
sponding values vary in a wide range. These result in the
low performances of the foveal quality metrics.

From the above analysis, we can see that the investigated
metrics are not eﬀective to evaluate omnidirectional im-
ages of non-uniform quality. Though the FPSNR metric
(i.e the most feasible one) have very high performances
in certain images, it performs even worse than the simple
MSE metric in some other images. Moreover, the per-
formances of all the quality metrics are not good across
diﬀerent images. This suggests that it is necessary to in-
tegrate content characteristics in these quality metrics.

6 Conclusions

In this paper, we have conducted subjective and objective
quality assessments of omnidirectional images with non-
uniform quality focusing on foveation feature of human
eyes. Based on the obtained results and discussions, some
ﬁndings can be summarized as follows.

• The perceptual quality is aﬀected by two key factors,
which are the sensitivity of human eyes and content
characteristics.

• The zones of an image corresponding to the fovea and
parafovea of human eyes are extremely important for

the perceptual quality.

• Content characteristics including the attractiveness
and the size of central object, as well as the presence
of neighboring objects aﬀect the quality perception.

• The nineteen objective quality metrics considered
in this study (including foveal quality metrics) are
not eﬀective to evaluate omnidirectional images with
non-uniform quality.

• The performances of the investigated metrics vary

drastically across diﬀerent contents.

For future work, further investigations with more con-
tent types and quality variation patterns will be conducted
to derive better understanding of viewers’ perceptual be-
haviors as well as the performances of existing metrics.

References

[1] D. V. Nguyen, H. T. Tran, A. T. Pham, and
T. C. Thang, “A new adaptation approach for
viewport-adaptive 360-degree video streaming,” in
2017 IEEE International Symposium on Multimedia
(ISM), Taichung, Taiwan, Dec. 2017, pp. 38–44.

[2] J. Chakareski, R. Aksu, X. Corbillon, G. Simon, and
V. Swaminathan, “Viewport-driven rate-distortion
optimized 360o video streaming,” in 2018 IEEE In-
ternational Conference on Communications (ICC),
Kansas City MO, USA, May 2018, pp. 1–7.

[3] C. Ozcinar, A. D. Abreu, and A. Smolic, “Viewport-
aware adaptive 360o video streaming using tiles for
virtual reality,” in IEEE International Conference on
Image Processing, Beijing, China, Sept. 2017, pp.
2174–2178.

[4] D. V. Nguyen, H. T. T. Tran, A. T. Pham, and
tile-based approach
T. C. Thang, “An optimal
for viewport-adaptive 360-degree video streaming,”
IEEE Journal on Emerging and Selected Topics in
Circuits and Systems, vol. 9, no. 1, pp. 29–42, Mar.
2019.

[5] B. Guenter, S. Drucker, D. Tan, and J. Snyder,
“Foveated 3D Graphics,” ACM Transactions on
Graphics, vol. 31, pp. 164:1–164:10, Nov. 2012.

12

[6] R. Albert, A. Patney, D. Luebke, and J. Kim, “La-
tency requirements for foveated rendering in virtual
reality,” ACM Transactions on Applied Perception
(TAP), vol. 14, no. 4, pp. 25:1–25:13, 2017.

[7] P. Guo, Q. Shen, Z. Ma, D. J. Brady, and Y. Wang,
“Perceptual quality assessment of immersive im-
ages considering peripheral vision impact.” [Online].
Available: http://arxiv.org/abs/1802.09065

[8] D. V. Nguyen, H. T. T. Tran, and T. C. Thang,
“Impact of delays on 360-degree video communi-
cations,” in 2017 TRON Symposium (TRONSHOW),
Tokyo, Japan, Dec. 2017, pp. 1–6.

[9] J. Besharse and D. Bok, The Retina and Its Disorders.

Academic Press, Apr. 2011.

[10] S. Lee, A. C. Bovik, and B. L. Evans, “Eﬃcient im-
plementation of foveation ﬁltering,” in Proceedings
of Texas Instruments DSP Educator’s Conference,
1999.

[11] J. Lee, F. De Simone, and T. Ebrahimi, “Subjective
quality evaluation of foveated video coding using
audio-visual focus of attention,” IEEE Journal of
Selected Topics in Signal Processing, vol. 5, no. 7,
pp. 1322–1331, Nov. 2011.

[15] H. G. Kim, H. Lim, and Y. M. Ro, “Deep virtual real-
ity image quality assessment with human perception
guider for omnidirectional image,” IEEE Transac-
tions on Circuits and Systems for Video Technology,
vol. -, no. -, pp. 1–1, 2019.

[16] M. Huang, Q. Shen, Z. Ma, A. C. Bovik, P. Gupta,
R. Zhou, and X. Cao, “Modeling the perceptual qual-
ity of immersive images rendered on head mounted
displays: Resolution and compression,” IEEE Trans-
actions on Image Processing, vol. 27, no. 12, pp.
6039–6050, Dec. 2018.

[17] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
celli, “Image quality assessment: From error visibil-
ity to structural similarity,” IEEE Transactions on
Image Processing, vol. 13, no. 4, pp. 600–612, Apr.
2004.

[18] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Mul-
tiscale structural similarity for image quality assess-
ment,” in The Thrity-Seventh Asilomar Conference
on Signals, Systems Computers, vol. 2, Nov. 2003,
pp. 1398–1402.

[19] Z. Wang and A. C. Bovik, “A universal image quality
index,” IEEE Signal Processing Letters, vol. 9, no. 3,
pp. 81–84, Mar. 2002.

[12] C.-F. Hsu, A. Chen, C.-H. Hsu, C.-Y. Huang, C.-L.
Lei, and K.-T. Chen, “Is foveated rendering perceiv-
able in virtual reality?: Exploring the eﬃciency and
consistency of quality assessment methods,” in Pro-
ceedings of the 25th ACM International Conference
on Multimedia, Mountain View, California, USA,
Oct. 2017, pp. 55–63.

[13] H. Duan, G. Zhai, X. Min, Y. Zhu, Y. Fang, and
X. Yang, “Perceptual quality assessment of omnidi-
rectional images,” in 2018 IEEE International Sym-
posium on Circuits and Systems (ISCAS), Firenze
Fiera Spa, Florence, Italy, May 2018, pp. 1–5.

[14] M. Xu, C. Li, Z. Chen, Z. Wang, and Z. Guan,
“Assessing visual quality of omnidirectional videos,”
IEEE Transactions on Circuits and Systems for Video
Technology, vol. -, no. -, pp. 1–1, Dec. 2018.

[20] S. Lee, M. S. Pattichis, and A. C. Bovik, “Foveated
video quality assessment,” IEEE Transactions on
Multimedia, vol. 4, no. 1, pp. 129–132, Mar. 2002.

[21] H. Ha, J. Park, S. Lee, and A. C. Bovik, “Perceptually
unequal packet loss protection by weighting saliency
and error propagation,” IEEE Transactions on Cir-
cuits and Systems for Video Technology, vol. 20,
no. 9, pp. 1187–1199, Sep. 2010.

[22] H. T. Tran, C. T. Pham, N. P. Ngoc, A. T. Pham,
and T. C. Thang, “A study on quality metrics for
360 video communications,” IEICE Transactions on
Information and Systems, vol. 101, no. 1, pp. 28–36,
2018.

[23] L. Zhaoping, Understanding Vision: Theory, Mod-
els, and Data. Oxford University Press, July 2014.

13

[24] M. Yanoﬀ and J. S. Duker, Ophthalmology. Saun-

ders, Nov. 2013.

[25] A. Hendrickson, Organization of the Adult Primate
Fovea. Berlin, Heidelberg: Springer Berlin Heidel-
berg, 2005.

[26] H. Strasburger, I. Rentschler, and M. Juttner, “Pe-
ripheral vision and pattern recognition: A review,”
Journal of Vision, vol. 11, no. 5, pp. 13–13, Dec.
2011.

[27] V. Roberto, Intelligent Perceptual Systems: New Di-
Springer,

rections in Computational Perception.
Nov. 1993.

[28] E. Pöppel and L. O. Harvey, “Light-diﬀerence thresh-
old and subjective brightness in the periphery of
the visual ﬁeld,” Psychologische Forschung, vol. 36,
no. 2, pp. 145–161, 1973.

[29] J. A. Jones, J. E. Swan II, and M. Bolas, “Peripheral
stimulation and its eﬀect on perceived spatial scale
in virtual environments,” IEEE transactions on visu-
alization and computer graphics, vol. 19, no. 4, pp.
701–710, 2013.

[30] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba,
“Recognizing scene viewpoint using panoramic
place representation,” in IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR2012),
Providence, RI, USA, Jun. 2012.

[31] P.

vision

group,

“SUN360

Database.”
https://vision.princeton.edu/projects/2012/SUN360/data/

[Online].

Panorama
Available:

[32] Recommendation ITU-T P.913, “Methods for the
subjective assessment of video quality, audio quality
and audiovisual quality of Internet video and distri-
bution quality television in any environment,” Inter-
national Telecommunication Union, 2014.

[33] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, “A
statistical evaluation of recent full reference image
quality assessment algorithms,” IEEE Transactions
on image processing, vol. 15, no. 11, pp. 3440–3451,
2006.

14

[34] Y. Ou, Y. Xue, and Y.Wang, “Q-STAR: A perceptual
video quality model considering impact of spatial,
temporal, and amplitude resolutions,” IEEE Trans-
actions Image Processing, vol. 23, no. 6, pp. 2473–
2486, June 2014.

[35] H. R. Sheikh and A. C. Bovik, “Image informa-
tion and visual quality,” IEEE Transactions on Image
Processing, vol. 15, no. 2, pp. 430–444, Feb. 2006.
[36] N. Damera-Venkata, T. D. Kite, W. S. Geisler, B. L.
Evans, and A. C. Bovik, “Image quality assessment
based on a degradation model,” IEEE Transactions
on Image Processing, vol. 9, no. 4, pp. 636–650, Apr.
2000.

[37] Z. Wang and Q. Li, “Information content weight-
ing for perceptual image quality assessment,” IEEE
Transactions on Image Processing, vol. 20, no. 5, pp.
1185–1198, May 2011.

[38] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “Fsim:
A feature similarity index for image quality assess-
ment,” IEEE Transactions on Image Processing,
vol. 20, no. 8, pp. 2378–2386, Aug. 2011.

[39] L. Zhang, L. Zhang, and X. Mou, “Rfsim: A feature
based image quality assessment metric using riesz
transforms,” in 2010 IEEE International Conference
on Image Processing, Sep. 2010, pp. 321–324.
[40] L. Zhang and H. Li, “Sr-sim: A fast and high per-
formance iqa index based on spectral residual,” in
2012 19th IEEE International Conference on Image
Processing, Sep. 2012, pp. 1473–1476.

[41] Z. Wang, A. C. Bovik, L. Lu, and J. L. Kouloheris,
“Foveated wavelet image quality index,” in 46th An-
nual Meeting, Proceedings SPIE, Application of Dig-
ital Image Processing, Jul. 2001.

[42] S. Lee and A. C. Bovik, “Foveated video image anal-
ysis and compression gain measurements,” in 4th
IEEE Southwest Symposium on Image Analysis and
Interpretation, Apr. 2000, pp. 63–67.
Exploration

[43] Joint

Video

Team,
Available:

“360Lib.”
https://jvet.hhi.fraunhofer.de/svn/svn_360Lib/tags/360Lib-
2.0.1/

[Online].

