1
2
0
2

r
p
A
2

]
I

N
.
s
c
[

1
v
6
3
0
1
0
.
4
0
1
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

1

Hybrid Policy Learning for Energy-Latency
Tradeoff in MEC-Assisted VR Video Service

Chong Zheng, Student Member, IEEE, Shengheng Liu, Member, IEEE, Yongming Huang, Senior Member, IEEE,
Luxi Yang, Senior Member, IEEE

Abstract—Virtual reality (VR)

is promising to fundamen-
tally transform a broad spectrum of industry sectors and the
way humans interact with virtual content. However, despite
unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR’s full potential.
In this paper, we consider delivering the wireless multi-tile VR
video service over a mobile edge computing (MEC) network.
The primary goal
is to minimize the system latency/energy
consumption and to arrive at a tradeoff thereof. To this end,
we ﬁrst cast the time-varying view popularity as a model-free
Markov chain to effectively capture its dynamic characteristics.
After jointly assessing the caching and computing capacities on
both the MEC server and the VR playback device, a hybrid
policy is then implemented to coordinate the dynamic caching
replacement and the deterministic ofﬂoading, so as to fully utilize
the system resources. The underlying multi-objective problem is
reformulated as a partially observable Markov decision process,
and a deep deterministic policy gradient algorithm is proposed
to iteratively learn its solution, where a long short-term memory
neural network is embedded to continuously predict the dynamics
of the unobservable popularity. Simulation results demonstrate
the superiority of the proposed scheme in achieving a trade-off
between the energy efﬁciency and the latency reduction over the
baseline methods.

Index Terms—Virtual reality, mobile edge computing, deep re-
inforcement learning, Markov decision process, wireless network.

I. INTRODUCTION

T HE past few years have witnessed the rapid evolution

of virtual reality (VR), which has branched out into
numerous application domains, from education to healthcare
to the alluring world of entertainment. VR implies immersing
users inside a synthetic ﬁctional world where the physical
environment and interactions are simulated. The ultimate goal
of VR interface design is to break the barrier that separates
both worlds by being unable to distinguish between the real
world and a simulated one. A crucial step in this direction is
to boost the resolution of the VR system to the full ﬁdelity
of human perception and to free the user from any cable
connection that limits mobility. This requires an intensive
computation and transmission of ultra-massive volume of data.
For mobile VR devices that suffers from limited battery life

Manuscript received October 16, 2020; revised XXX XX, XXXX; accepted
XXX XX, XXXX. Date of publication XXX XX, XXXX; date of current
version XXX XX, XXXX. This work was supported in part by the National
Natural Science Foundation of China under Grant Nos. 62001103 and
61720106003, and the National Key R&D Program of China under Grant No.
2018YFB1800801. Part of this work was presented at the IEEE 18th Wireless
Communications and Networking Conference (WCNC), Seoul, South Korea,
May 2020 [1]. (Corresponding author: Y. Huang.)

The authors are with the School of Information Science and Engineering,
Southeast University, Nanjing 210096, China, and also with the Purple Moun-
tain Laboratories, Nanjing 211111, China (e-mail: {czheng; s.liu; huangym;
lxyang}@seu.edu.cn).

and insufﬁcient computing capabilities, the resultant resource
request can be highly prohibitive. We thus envision that
the next steps toward the future interconnected VR service
will come from a ﬂexible use of caching, computing, and
connectivity resources. To realize this vision, innovations in
the supporting infrastructure are essential, and many trade-offs
need to be studied.

A. Related Works

From academia to industry, mobile edge computing (MEC)
is commonly viewed as a key enabler of seamless and
immersive VR experience. It addresses the above technical
bottlenecks by implementing the storage, processing, and the
service delivery at the wireless network edge, so as to reduce
bandwidth demand for backhaul to the backbone network.
Many related works on the MEC-based solution can be found
in the literatures. For instance, in [2], the delivery of VR video
is considered in a cooperative multi-node MEC system where
all the nodes share their resources by backhual links. The
optimal strategies are developed to maximize the aggregate
reward of the small base stations (SBSs). In [3], a delivery
strategy between the servers and the subscribers is further
developed to optimize the bandwidth and latency of the
wireless VR video service. Aside from the studies on content
delivery, a task scheduling strategy is proposed for the VR
video service within a queueing MEC framework [4], which
selects the computation model for the MEC server to minimize
the communication-resource consumption under the delay con-
straint. All the devices in the system are assumed to be cache-
ﬁxed. Later on, the restriction on the ﬁxed cache is further
relaxed. Based on the analysis of the VR video production
ﬂow in [3], an additional pre-caching procedure is employed
in [5], [6], where partial ﬁeld of views (FoVs) are cached at the
local VR device in advance and certain post-processing tasks
can be performed on demand either at the local or the MEC
server. Under a similar framework, the mmWave 802.11ad
wireless technology is incorporated into the MEC network
[7] to promise the use of high-bandwidth wireless VR video
service. Also, an adaptive computing ofﬂoading scheme, which
focuses on the scenario of indoor environment, is designed in
[7]. For the VR video service in the outdoor environment,
a multi-connectivity-based millimeter wave MEC network is
developed in [8], and the trade-off among link adaptation,
viewpoint rendering ofﬂoading, and chunk quality adaptation
is sought to improve the utilization of bandwidth and energy.
We observe that the existence of the viewpoints’ popularity
and the rich caching resources in the MEC system have mo-
tivated the commonly-adopted caching strategy. However, all
the caching placements in the existing studies are performed

 
 
 
 
 
 
2

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

in advance and remains constant during the service. As such,
the new tiles received for current viewing will be outright
discarded, which leads to enormous waste and repetitive
transmission in the wireless network because of the FoV
overlapping phenomenon [9]. Transmission resource allocation
in the up/down link is thus considered in [9], while the caching
and computing power at the edge is, however, omitted. In this
context, a dynamic caching replacement scheme is designed
in this work to signiﬁcantly improve the utilization of cache
resources and to reduce the waste caused by the redundant
transmission. Unlike the existing works where the caching
status remains unchanged during the service, the proposed
caching policy updates the caching status as soon as new tiles
enter and attempts to solve the following technical challenges.
On the one hand, in order to effectively realize dynamic and
real-time caching replacement, the future view requests must
be acquired in advance to support the caching update decision.
However, the view request is time-varying and depends on the
users’ interests, which is unavailable in advance and difﬁcult
to accurately predict. On the other hand, real-time caching
replacement involves both storage of the incoming prevalent
content and removal of the obsolete ones. As such, the decision
space is huge with its dimension varies constantly due to the
uncertainty of the incoming content.

The abundant computing resources in close proximity to
the VR service subscribers provided by the MEC server
can be another accelerator of the VR video service. Many
computation ofﬂoading schemes of the VR video service
have been designed to exploit the computing resource and
improve the quality of service. For example, unmanned aerial
vehicles are introduced to the wireless VR video service in
[10] to provide support in terms of computation, caching, and
transmission. To maximize the users’ reliability, a distributed
deep learning algorithm is adopted to solve the joint caching
and transmission optimization problem. In [11], a more com-
plicated computation ofﬂoading scheme is considered for
the VR applications in a multi-user edge computing system.
The mobile users’ energy minimization is formulated as a
convex problem subject
to the energy, computing latency,
and computing frequency constraints, which is solved by
using a classical Lagrangian duality. To maximize the average
tolerant delay while guaranteeing a given transmission rate
constraint, a joint communication, caching and computing
decision problem is considered in [12], which is converted
to a multiple choice multiple dimensional knapsack problem.
However, the ofﬂoaded tasks in [4], [10], [12] are treated
as unities and ofﬂoaded in a integral form either to local
VR device or the MEC server. As a result, idle computing
resources are constantly released.

Fortunately, the videos are usually sliced into multiple tiles
in wireless VR video service, which facilitates implementation
of task segmentation. In the light of this fact, a multi-tile
deterministic ofﬂoading scheme is proposed in this work,
which not only effectively avoids the release of idle computing
resources but also fully exploited the cache resources in each
device. The proposed ofﬂoading mechanism is completely
different from the integral ofﬂoading in [4], [10], [12] or
the generic proportional ofﬂoading scheme adopted in some

MEC studies. Despite remarkable beneﬁts in terms of comput-
ing/caching resource utilization, such micromanagement for
each single tile results in a dimension explosion of the decision
space, which is an important challenge to address in this
work. Collectively, caching and ofﬂoading policies constitute a
hybrid policy that is considered for the intelligent wireless VR
video service in this work. The tremendous and time-varying
dimension of the hybrid policy space represents a paramount
factor to consider, which obviously overwhelms the capacity
of the conventional optimization methods. Furthermore, as the
computing task in each device is supported by the co-located
cached tiles,
the present caching status has an important
impact on the ofﬂoading decision. Conversely, the present
ofﬂoading decision determines the tile input for each device
and consequently affects the caching policy as well as the
future caching status. This complicated interaction between the
two functions further compounds the problem and highlights
the necessity of exploration of more intelligent algorithms.

Apart from exploring the potential of MEC in terms of
caching and computing resources,
the success of wireless
VR video service also requires making full use of the multi-
perspective characteristics of VR videos. The multiple views
in VR video service allow users to watch the FoV interested
in. This is fundamentally different from the ordinary two-
dimensional (2D) single-view video service where users can
only enjoy the pre-determined contents passively. Taking into
account the FoV overlap due to the multi-view, the optimal
multi-cast schemes of tiled VR video from one server to
multiple wireless users are proposed in [13], [14], where
the minimization of transmission power and the maximiza-
tion of received video’s quality are considered. However, the
dynamics of the popularity is neglected and the distribution
parameters of the popularity is assumed known in both of these
studies. The dynamics in the view popularity is considered
in [9], [15], where learning-based resources management for
uplink/downlink transmissions is studied within small cell
networks. However, all the possible distribution parameters of
the popularity are still given as a priori knowledge during the
service. An accurate information of users’ popularity can sig-
niﬁcantly instruct the learning and optimization of the caching
policy. However, in reality, the dynamic view popularity as a
time-varying characteristics depends on the changes of users’
interests and is unavailable in advance. For more realistic
modeling, we capture the variations of the popularity using a
model-free Markov decision process (MDP). In such a model,
no a priori knowledge on the popularity is assumed aside from
the distribution type, which is more robust yet more difﬁcult
to accurately predict. Hereby lies another important technical
contribution of this work.

We know from the foregoing discussion that, a large number
of parameters and factors can, and should, be considered
in optimizing resource allocation and network performance
of the collaborative network for wireless VR service. It is
also imperative for the hybrid policy optimization to remain
ﬂexible and responsive to changes and unpredictable elements
in the network conditions. This requires assistance of intel-
ligent methodology. Artiﬁcial Intelligence has unleashed a
new era of ingenuity. There has been a consensus among

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

3

global top telecommunications companies such as Huawei,
Samsung, and Qualcomm [16]–[18] that artiﬁcial-intelligence-
based end-edge-cloud orchestrated network is the recipe for
the commercially viable wireless VR. However, as higher
requirements are posed on the computing power and latency of
communication services, the intelligent algorithms cannot be
simply used as is, without any modiﬁcation or customization.
In the sequel, our technical contributions in terms of system
modelling, problem formulation, and algorithm redesign to
cope with realistic scenarios will be showcased.

B. Main Contributions

In this paper, a joint dynamic caching replacement and
deterministic ofﬂoading scheme for MEC-assisted wireless VR
video service is proposed to minimize the service latency
as well as the system energy consumption, which achieves
a trade-off among the supporting resources in the system.
Concretely, the technical contributions of this work include:

• Multi-tile deterministic ofﬂoading is proposed in this
work, which is completely different from the proportional
ofﬂoading commonly adopted in the generic MEC re-
searches. In the designed ofﬂoading policy, we segment
the FoV tasks into tiles and assign the computation task
of each tile to a certain device as per the hybrid policy
and, thereby realize the full linkage and utilization of
multi-dimensional resources.

• In view of the FoV overlapping, a dynamic caching
replacement scheme is designed and operates collabo-
ratively with the deterministic ofﬂoading. As such, the
repetitive communication overhead is signiﬁcantly re-
duced, and the resource utilization at both the MEC server
and the local VR device is dramatically improved.

• We take into account the dynamics of the viewpoint pop-
ularity and ﬁrst model the variations of the distribution
parameter as a completely model-free Markov process,
where both the transition probabilities and the distribution
parameters are assumed unknown to the system during
the service. This model is more closely aligned with
reality and has not been studied in the related works.
• We convert the underlying optimization problem into a
partially observable model-free MDP whose state tran-
sition probability is unaccessible and the state space
is partially observable. Then, we incorporates the long
short-term memory (LSTM) network into the deep deter-
ministic policy gradient (DDPG) algorithm to overcome
the deﬁciency of the original DDPG in capturing the tem-
poral information hidden in the input data. The proposed
scheme can also handle large state/action space.

The rest of this paper is organized as follow. The system
model is presented in Section II. And the problem is for-
mulated in Section III. In Section IV, we propose a deep
reinforcement learning algorithm to get the hybrid policy for
wireless VR video service. In Section V, simulation results
are discussed. Finally, conclusions are drawn in Section VI.

II. SYSTEM MODEL

The tiling technique [6], [19] illustrated in Fig. 1 is adopted
to process the VR videos. This technique unfolds the spherical

col

tiles in column and N FOV

video into a 2D version and further divides the 2D video into
pieces of tiles. At each time instant, only the tiles in the FoV
that the user focus on will be extracted to dispose rather than
the entire spherical video. Thereby, the processed data size is
signiﬁcantly reduced. Concretely, the spherical video on the
left-hand side of Fig. 1 is converted to the 2D tiles on the
right-hand side. Let us take a closer look at the 2D tiles in
Fig. 1. The 2D VR video on the righthand side is divided
into N = Nrow × Ncol rectangular segments of the same
size, where Nrow and Ncol respectively denote the numbers
of segments in each row and each column. These rectangular
segments are referred to as tiles. One FoV is represented by
a rectangle with N FOV
row tiles in
row, and two FoVs can overlap with each other as marked
by the orange area. Suppose that one 3D FoV is requested
by the user, the related rectangular regions in the 2D plane
are extracted immediately, which is computationally simple.
The computation task considered in this paper is mainly the
conversion from the 2D tiles to the 3D FoV, i.e., 2D tiles
are the input of the computation while the 3D FoV ﬁle is the
output. In addition, we assume that all the tiles can be accessed
from the cloud but the caching capabilities of the MEC server
and the local VR device are both limited. As such, the VR
video can only be partially cached. Suppose that certain tiles
are requested whereas neither local VR device nor MEC server
has a copy, the MEC server will in turn try to access the tiles
from the cloud. For each task, dynamic caching replacement
at both the local VR device and the MEC server is considered,
and the deterministic ofﬂoading of multiple tiles is taken into
consideration in this work.

N FOV
col

FOV

N FOV
row

Filed overlap

Nrow

FOV

Ncol

Fig. 1.

Illustration of tiling technique for VR video processing.

As illustrated in Fig. 2, we consider a typical and simple
wireless VR video service system with a single VR user
wirelessly connected to a small base station (SBS) which is
equipped with one MEC server. We assume that both sides
are endowed with certain caching and computing capabilities.
The MEC server connects with the cloud by a backhual
link. When the user watches a VR video,
the local VR
device generates viewpoint request according to user’s head
movement. During the service, we assume that user makes a
new request as soon as the previous request is satisﬁed [9].
The interval between two adjacent requests is deﬁned as a
time slot. We readily see that the duration of each time slot
is not equal due to the different service delays of different
tasks. At the beginning of each time slot, the state and the
request information are uploaded to the MEC server. Next,
the computation task is extracted and divided into several sub-
tasks as per the deterministic ofﬂoading model proposed in
Section II-C, where all the sub-tasks are assigned to MEC
server or local VR device in a smart manner for subsequent

4

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

computing. Then, both the demanded but absent 2D tiles
by the local VR device as well as the outputted 3D FoV
ﬁle of the MEC server are sent to the user. In certain time
slots, the MEC server might also send request to the cloud
through the backhual link for some necessary but absent tiles
in accomplishing the computation task. In each time slots, the
dynamic caching policy is implemented in both VR device and
MEC server, which will be elaborated in section II-E. After all
the sub-tasks are completed and all the outputs are acquired
by the local VR device, the user’s request in the present time
slot is considered satisﬁed.

Cloud

SBS

Downlink: 2D tiles or 3D file

Uplink: state information

Wireless VR User

MEC Sever

Fig. 2. Architecture of an MEC-based wireless VR video system.

A. View Request Model
Let V = {Vk}K

col

k=1 and F = {Fn}N

)/∆h +1)×((Nrow −N FOV

n=1 respectively denote
the spaces of all the views and tiles in the 2D domain. As
the relationship between the number of
shown in Fig. 1,
viewpoints K and the segmentation of tiles can be expressed
as K = ((Ncol −N FOV
row )/∆v +1).
Similar to [13], we let ∆h and ∆v represent the number of
interval tiles for two adjacent views in the horizontal and the
vertical directions, respectively. The data size (in bit) of each
tile is given by τ = Q/N , where Q is the full size of all the
2D tiles. For each Vk, we assume that it consists of constant
tiles, which is in line with the characteristics of human vision
and also widely adopted in literatures such as [9], [13], [14].
We denote with FVk = {Fk1 , · · · , FkZ } the space of tiles
consistuting the view Vk, where Z is a constant. For each Vk,
its 3D FoV is computed from the 2D FoV and the input is the
set FVk . We let (Din, Dout) represent the input and the output
of the 2D-to-3D FoV conversion. Typically, in order to create a
stereoscopic vision, ϕ= Dout
Din ≥ 2 should be satisﬁed [6], [20],
[21]. Suppose that in a time slot t, the request probability for
viewpoint Vk ∈ V at the local VR device is pk(t). Additionally,
we let p(t) = [p1(t), · · · , pK(t)] denote the popularity of
K
all viewpoint. Thus, we have
k=1 pk(t) = 1. A detailed
discussion on the dynamic view popularity is presented in the
following subsection.

P

B. Dynamic Popularity Model

Viewpoint popularity of the VR video is the external mani-
festation of the user’s subjective interest. Though the changes
of popularity is complicated, its statistical representation can

be established given a large number of users’ traces. Inspired
by [22] which describes the content popularity of a ordinary
caching task as a Markov chain model, we model the dynamic
popularity in the VR video service as a completely model-free
MDP whose distribution parameters and transition probabili-
ties are both unknown. The popularity of the user’s request in
each time slot is assumed to follow the Zipf distribution [23],
[24], and the distribution parameter γt evolves dynamically
over time. As such, the probability of the k-th viewpoint in
time slot t is

pγt
k (t) =

kγt

1
K
l=1

.

1
lγt

(1)

P

As depicted in Fig. 3, we model the time-varying dynamics
of γt using a model-free Markov process with |G| states
recorded in the set G = {γi|i = 1, 2, · · · }. It
is worth
mentioning that, in this model, both the transition probabilities
and the parameter space are unknown to the system during the
VR video service, which is more realistic. The system only
has the a priori knowledge on the user’s historical requests,
which is used to assist the prediction of the time-varying
view probability. Speciﬁcally, as shown in Fig. 3, if the SBS
receives a viewpoint request in time slot t under the popularity
pγt(t), i.e., Vk(t)|pγt (t), which is abbreviated as Vk(t) for the
convenience of discussion, we have

ˆp(t) = f (R(t − 1)),

(2)

where R(t − 1) = [Vk(t − Tr), · · · , Vk(t − 1)] is the historical
request of continuous Tr time slots before time slot t. ˆp(t)
is the prediction of pγt(t). The function f (·) is a predictive
function realized by LSTM, whose implementation will be
detailed in section IV. ˆp(t) is the prediction of pγt(t).

1g

2g

Environment

ig

t tgp
( )

Popularity Markov chain

LSTM 
Network

ˆ ( )tp

Prediction

t
i

m
e

Historical requests

Fig. 3. Dynamic popularity model and the schematic of its prediction.

C. Deterministic Task Segmentation and Ofﬂoading Model

As mentioned above, unlike existing related works in MEC-
assisted VR system, deterministic task segmentation and of-
ﬂoading is implemented in this work. This means that rather
than processed as a unity, certain particular tiles in the com-
putation input set are selected and computed. Furthermore,
as will be explained later, the adopted MEC task ofﬂoading
scheme also differs from a generic paradigm where only

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

5

roughly how much proportion of a task should be ofﬂoaded
to the MEC server is considered.

consumptions of the MEC server and the local VR device
in each time slot t can be respectively represented as

Suppose that the SBS receives a viewpoint request Vk(t) and
FVk(t)= {Fk1(t), · · · , FkZ (t)}. Let o(t)= [o1(t), · · · , oZ(t)]
denote the 1 × Z binary ofﬂoading action vector in slot t,
where oz(t) = 1 means that the tile Fkz (t) is computed at
the MEC server and otherwise it is computed in the local
Z
VR device. Particularly,
z=1 oz(t) = Z indicates that all the
tiles will be computed at MEC server, and the opposite is
Z
z=1 oz(t) = 0. For the convenience of discussion, we
true if
S(t)(t)} ⊆ FVk(t) and FL(t) =
let FM(t) = {FkM
R(t)(t)} ⊆ FVk(t) respectively denote the tiles
{FkL
1 (t), · · · , FkL
sets computed in MEC server and local VR device as per the
ofﬂoading action o(t). Note that S(t) and R(t) can be different
in each slot. But S(t) + R(t) = Z and FM(t) ∩ FL(t) = ∅
should be always satisﬁed.

1 (t), · · · , FkM

P

P

Remark 1: The sliced input ﬁles produced by tiling tech-
nique facilitates the segmentation of task. In addition, the
constraint FM(t) ∩ FL(t) = ∅ ensures that the two computing
outputs are non-overlapping, such that the outputs on both
devices are splice easy.

D. Computation Model

The MEC server is assumed to run at a given computational
frequency denoted as fM, which is related to the central
processing unit (CPU) speciﬁcation. The computation task
in wireless VR service mainly comprises the projection and
rendering of the 2D tiles to the requested 3D FoV. Such
operation is commonly deemed highly computation-intensive
and,
the computation time is considered to have a
linear relation with respect to the computational frequency [3],
[6] Therefore, for request Vk(t) under o(t) the computation
latency at the MEC server is

thus,

T M
com(t) =

wτ

(cid:18)

Z

z=1

oz(t)

fM,

(3)

(cid:19)(cid:30)

X
where w is the CPU cycles involved in computing one-bit data.
The computation latency at local VR device can be given by

T L
com(t) = wτ

Z −

(cid:18)

Z

z=1

X

oz(t)

(cid:19)(cid:30)

fL,

(4)

latency,

where fL is the CPU computational frequency of the local VR
device. Obviously, we have fL < fM.
Aside from the computational

the energy con-
sumption for computation should also be taken into account,
which is not that necessary for a traditional communication
system. Let ηM and ηL denote the effective power switched
capacitances of CPUs in MEC server and local VR device,
respectively. The energy consumption for computing one cycle
at
L for the
local VR device [6], [25]1. Then the computation energy

the MEC server is ηMf 2

M and similarly ηLf 2

1Note that though the power model used in this work is simple, it has
been widely adopted in the related works and its effectiveness has been
validated (c.f., e.g., [6], [25]). We are fully aware of the fact that the power
characteristics of different devices can vary wildly and standby/static power
can account for a considerable portion of the total power. However, to avoid
further complicating the problem under investigation, the establishment of a
more realistic power model is left for our future work.

EM(t) = ηMf 2

Mwτ

X

oz(t),

Z

z=1

Z

EL(t) = ηLf 2

Lwτ

Z −

(cid:18)

oz(t)

.

(cid:19)

z=1

X

(5)

(6)

Note that, in this work, we assume that the VR device has
sufﬁcient battery power to support long-time video service.
That is, the limitation of the VR device’s battery capacity
is ignored. We will certainly consider this rather important
constraint in our future work.

E. Dynamic Caching Replacement Model

In each time slot, once the local VR device and the
MEC server receive new tiles for computation, the caching
replacement should be simultaneously considered to make
full use of the cache resources. We ﬁrst consider the cache
dynamic replacement at
the local VR device. We assume
that the cache size of the local VR device is τ ML (in bit),
where ML is an integer, and the cache state at the beginning
of slot t is represented as ˜FL(t) = {F L
ML (t)}.
Let cL(t) = (c+
L (t), c−
L (t)) denote the caching replacement
action at
L (t) =
L1(t), · · · , c+
[c+
LR(t)(t)] decides which tiles in FL(t) should
be stored while c−
(t)] decides which
tiles in ˜FL(t) should be deleted. c+
Lr(t) = 1 indicates that the
tile FkL
Lr(t) = 0,
it should be outright discarded. Similarly, c−
(t) = 1
mL (t) ∈ ˜FL(t) should be deleted; otherwise if
means that F L
c−
(t) = 0, it should be preserved. Limited by the cache
LmL
capacity of the local VR device, we have

r (t) ∈ FL(t) should be stored; otherwise if c+
LmL

the local VR device at slot t, where c+

L1(t), · · · , c−

1 (t), · · · , F L

L (t) = [c−

LML

R(t)

r=1

c+
Lr(t) =

ML

mL=1

c−
LmL

(t).

(7)

=

1, r

X

We then consider

{FkL
mL(t)|c−

X
r (t)|c+
If
Lr(t)
1, 2, · · · , R(t)} ∩
=
{F L
(t) = 1, mL = 1, 2, · · · , ML} 6= ∅, some tiles
LmL
are included in the target sets of deletion and preservation at
the same time. In this case, these tiles will remain unchanged.
the cache dynamic replacement at
the MEC server. We suppose that its cache size is τ ME
(in bit), where ME is an integer and ME > ML.
the original cache state is
At
˜FM(t) = {F M
1 (t), · · · , F M
ME (t)}. Similarly, we have cM(t) =
M1(t), · · · , c+
M(t) = [c+
M(t)), where c+
(c+
M(t), c−
MS(t)(t)] and
(t)]. c+
c−
M1(t), · · · , c−
M(t) = [c−
Ms(t) = 1 and c−
(t) =
MME
MmE
1 respectively means that the tile FkM
s (t) ∈ FM (t) should be
mE (t) ∈ ˜FM(t) should be deleted. We
preserved and the tile F M
can also obtain the following cache capacity constraint

the beginning of slot t,

S(t)

s=1

c+
Ms(t) =

ME

mE=1

c−
MmE

(t).

(8)

X

X
Supported by the caching component, this process can be
completed before the computation such that no additional
latency is induced. In addition, as the energy consumption
caused by accessing ﬁles is signiﬁcantly lower than that of
the computation, we neglect the former in the modeling.

6

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

F. Wireless Transmission Model

There are three types of transmission links in the MEC-
assisted VR service system, i.e., the uplink, the downlink, and
the backhual link. In the uplink, only the viewpoint request and
the cache state information are uploaded to the MEC server.
As such, the size of the transmitted data is negligible compared
to the size of the 2D tiles and the 3D FoV ﬁles. We therefore
ignore the latency cost in the uplink.

Regarding the downlink transmission, given the bandwidth
B and the transmit power PB, the wireless downlink rate is
derived as

(9)

Rwl(t) = B · log2(1 + ℓ(t)),
where ℓ(t) = PB · h(t)/σ2 is the signal-to-interference-plus-
noise ratio (SINR) in a slot t. σ2 denotes the variance of
the Gaussian noise and h(t) = g(t) · d−α represents the
path loss between the SBS and the VR user with g(t) being
the Rayleigh fading parameter and assumed to be constant
within one slot duration. d represents the distance between
the user and the SBS, and α is the path loss exponent.
Let CL(t) = {CL
i=1 be the vector representation of
the cache state in the local VR device at the beginning of
time slot t, where each entry corresponds to an index in F .
CL
i (t) = 1 means that the tile Fi is cached at the local, and
N
i=1 CL
i (t) ≤ ML is the cache capacity constraint. Likewise,
we have CM(t) = {CM
i (t) ≤ ME for
P
the MEC server. In a time slot t, the size of 2D tiles transmitted
through the downlink, which are demanded but absent in the
local VR device, can be denoted as

N
i=1 CM

i (t)}N

i=1 and

i (t)}N

P

Ddown

2D (t) = τ ·
Z

X

= τ ·

i=1

X
Z

Z

i=1

1(oi(t)=0&C L

i (t)=0)

1(oi(t)=0&C L

i (t)=0&CM

i (t)=1)

(10)

+τ ·

1(oi(t)=0&C L

Xi=1
mec (t) + Ddown
= Ddown

i (t)=0&CM

i (t)=0)

cloud(t),
where the ﬁrst term on the righthand side is the size of the tiles
directly accessed from the MEC server, and the second term
is the size of the tiles accessed from the cloud because neither
does the MEC server have a copy of the requested tiles. The
size of the 3D ﬁle transmitted by the downlink, which is the
computed output of the MEC server, is given by

Ddown

3D (t) = τ · ϕ ·

Z

i=1

oz(t).

(11)

X

Thus, the latency cost of the downlink transmission is

Ddown
(cid:0)

T down(t) =

2D (t) + Ddown

3D (t)

Rwl(t).

(12)

We now consider the backhual link in the wireless trans-
mission model. Given the bandwidth Rbh, the transmission
latency on the backhual link can be expressed as

(cid:1)(cid:14)

Rbh,

local(t)

mec (t)=τ ·

T back(t) =

mec (t) + Dback

Dback
(cid:0)
Z
where Dback
i=1 1(oi(t)=1&CM
i (t)=0) is the size of the
tiles absent in the MEC server but demanded by MEC server’s
computation, while Dback
local(t) = Ddown
cloud(t) denotes the size of
the tiles absent both in the local VR device and the MEC
server but computed in the local VR device. It is noteworthy

(13)

P

(cid:1)(cid:14)

that the overall latency is not simply a sum of all the latencies
introduced in this subsection. We will elaborated this issue in
following subsection.

In the wireless transmission model of this work, the system
energy is mainly used for the transmission from the SBS to
the local VR device. As such, the system energy consumption
can be approximated as

ET(t) = PB · T down(t).

(14)

G. Utility Function Model

We jointly consider both the latency and energy costs in the

utility evaluation.

We ﬁrst look into the latency cost for each task, which is
also deﬁned as the duration of each time slot. As deterministic
multi-tile ofﬂoading is implemented in the proposed scheme,
the task is decomposed into two subtasks and processed in
parallel at the both the MEC server and the local VR device.
For the MEC server, we have

T M(t) =

Dback
mec (t)
Rbh

+

Ddown
3D (t)
Rwl(t)

+ T M

com(t),

(15)

where the ﬁrst term at the righthand side is the transmission
time of the 2D tiles from the backhual link. These tiles are
used for the computation at the MEC server in slot t, which
are demanded by the MEC server but cannot be found in its
own cache. The second term is the transmission time of MEC
server’s computation output content, which is a portion of the
3D FoV requested by the user in slot t. The rightmost term
is the computation time of the MEC server. For the local VR
device, we deﬁne its corresponding latency cost as

Dback
local(t)
Rwl(t)

Dback
local(t)
Rbh

Ddown
mec (t)
Rwl(t) (cid:27)

,

(cid:26)

+

+ T L

T L(t)= max

com(t),
(16)
where Dback
local(t)
is the transmission time of the 2D tiles from
Rbh
the cloud to the SBS by backhual link, which are demanded by
the local VR device, but absent both in the local and the MEC
server. Ddown
mec (t)
is the transmission time of the 2D tiles that
Rwl(t)
cannot be found in the local cache but are stored in the MEC
server. The latency is determined by the maximum of these
two transmission durations due to the parallel transmission in
the downlink and backhual link. After receiving the 2D tiles
from the cloud, the SBS forwards them to the local VR device
by downlink. The induced latency is denoted as Dback
local(t)
Rwl(t) . The
rightmost term represents the computation time of the local VR
device. Because of the parallel processing of the two devices,
the overall service latency can be derived as

T total(t) = max

T M(t), T L(t)
(cid:9)
(cid:8)
Next, we deﬁne the energy cost generated by the computation
and the transmission, i.e.,

(17)

.

Etotal(t) = EM(t) + EL(t) + ET(t).

(18)

To ﬁnd a latency-energy tradeoff represents an important
research direction in wireless VR delivery. Many related works
suggest the weighted-sum method [26]–[28], which is intuitive

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

7

and quite effective. We follow this path and deﬁne the cost in
each time slot as

where the distribution parameters transit to themselves during
the period.

Y (t) = ωT total(t) + (1 − ω)Etotal(t),

(19)

where ω ∈ [0, 1] is the weight that indicates the relative
importance of the T total(t) and the Etotal(t). However, in
providing VR video service, we pay more attention to the
long-term experience during a continuous period of time rather
than the instantaneous experience. In this case, suppose that
Υ time slots are examined to evaluate the overall utility of the
system. Then, the cumulative utility over time is given by

U = −

Υ

t=1

X

Y (t).

(20)

III. PROBLEM FORMULATION

A. Problem Formulation

As mentioned earlier, we concerned more about the long-
term performance over a continuous period of time rather
than the instantaneous performance in wireless VR service.
Therefore, the designed hybrid policy aims at minimizing
the cumulative cost of the VR service system. As such, the
underlying optimization problem is formulated as

min
(O,CL,CM)

s.t.

Υ

E

lim
Υ→∞ X
N
CL

k=0

(cid:2)
i (t) ≤ ML,

χkY (t + k)
(cid:3)

,

X

X

i=1
N

i=1
R(t)

X

r=1
S(t)

CM

i (t) ≤ ME,

ML

X

mL=1
ME

c+
Lr(t) =

c+
Ms(t) =

c−
LmL

(t),

c−
MmE

(t),

mE=1

s=1

X
oz(t) ∈ {0, 1}, ∀z ∈ Z,
c+
Lr(t), c−
(t) ∈ {0, 1},

X

LmL
(∀r ∈ UL(t), ∀mL ∈ ML),
(t) ∈ {0, 1},

c+
Ms(t), c−

MmE

(21a)

(21b)

(21c)

(21d)

(21e)

(21f)

(21g)

(∀s ∈ UM(t), ∀mE ∈ MM),

(21h)

M(t), c−

where the optimization parameter sets O = {o(t)|t =
0, 1, 2, 3, · · · }, CL = {cL(t)|t = 0, 1, 2, 3, · · · }, and CM =
{cM(t)|t = 0, 1, 2, 3, · · · } represent the collections of actions
in each time slot t. cL(t) contains two vectors, namely
c+
L (t) and c−
L (t), which have been deﬁned in Section II-E.
Likewise, cM(t) = (c+
M(t)). Y (t) denotes the joint
latency/energy cost at any time t. χ ∈ (0, 1) is the discount
factor. The expectation is taken with respect to the measure
included by the decision variables as well as the system state.
UL(t) = {1, 2, · · · , R(t)} and UM(t) = {1, 2, · · · , S(t)}
represent the time-varying index sets of the computed tiles
sets FL(t) and FM(t), respectively. ML = {1, 2, · · · , ML}
and MM = {1, 2, · · · , ME} are the index sets of ˜FL(t) and
˜FM(t), respectively. Additionally, Z = {1, 2, · · · , Z}. Here
we assume that the unknown viewpoint request distribution
of the user varies over time. Note that, in a realistic environ-
ment, the request distribution may remain unchanged during
a period. This is equivalent to a special case of our model

The constraints in (21b) and (21c) indicate that the number
of cached ﬁles in the local VR device and the MEC server are
both limited by the cache capacity. To maximize the utilization
of the caching resources, the caches in both of the devices
should be fully ﬁlled. On the other hand, the constraints in
(21d) and (21e) respectively ensures a balance in the sizes of
the cached ﬁles at the local VR device and the MEC device
after the caching replacement, in order to keep the caches full
but not overﬂowed. The constraint in (21f) implies that each
tile of the computation input Fkz (t) can be computed at either
the local VR device or the MEC server. (21g) indicates that
the caching replacement in each time slot in the local VR
device is limited by FL(t) and ˜FL(t), which guarantees that
the caching replacement in the local VR device does not bring
any additional resource consumption. (21h) plays a similar role
for the caching operation in the MEC server. The following
facts and technical challenges should be noted:

• The solution of the problem under investigation is a
dynamic strategy over time rather than a transient one.
• The system states and actions conform to chain property

over time.

• The dimensions of the two actions cL(t) and cM(t) are

both time-varying.

• The problem (21) corresponds to an inﬁnite-horizon-cost
MDP problem [28] and a priori knowledge on state
transition probabilities is unavailable.

To overcome the above fourfold technical challenges, we ex-
ploit deep reinforcement learning in this work instead of tradi-
tional optimization techniques such as dynamic programming.
The proposed scheme offers inherent advantages in address-
ing the inﬁnite-horizon-cost MDP problems, complicated and
time-varying system states, continuous strategy development,
and etc.

Theorem 1: Without any knowledge of state transition
s(t)→s(t+1) = P {s(t + 1)|s(t), a}, the problem

probabilities P a
(21) is an inﬁnite-horizon-cost MDP problem.

Proof: Let π(s(t), a) denote the probability of selecting
action a under strategy π when the system state is s(t)
. The objective function in (21) is denoted as V (s(t)) =
Υ
k=0 π[χkY (t +k)|s(t)]. The derivation is given in
lim
Υ→∞
(22), where Ra
P
s(t)→s(t+1) is the reward feedback when the
system state transits from s(t) to s(t + 1) under action a. We
observe from the above derivation that, the analytical solution
of V (s(t)) is unobtainable since P a
s(t)→s(t+1) is unknown and
the term V (s(t + 1)) is recursive in a given time slot t. As
such, we cannot get the exact value of the objective function
in problem (21) even if the decision is executed. Therefore,
the problem (21) is an inﬁnite-horizon-cost MDP problem.

B. MDP Description

To address the above difﬁculty, we ﬁrstly convert our
problem into a MDP which consists of four components, i.e,
state space, action space, state transition probabilities, and
reward. The MDP description of the optimization problem is
denoted as < S, A, P, R >. Note that in this work, we assume

8

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

V (s(t)) = lim
Υ→∞

= Eπ

(cid:20)

Υ

Eπ

χkY (t + k)|s(t)
Pk=0
(cid:2)
Y (t)|s(t) +

∞

(cid:3)

= Eπ

∞

(cid:20)
Pk=0
= Eπ

χkY (t + k)|s(t)(cid:21)
+ Eπ
Y (t)|s(t)

Pk=0

χkY (t + k)|s(t)(cid:21)
s(t)→s(t+1)Ra
P a

(cid:2)
s(t)→s(t) + χ

(cid:3)
π(s(t), a)

∞

(cid:20)

Pk=0

π(s(t), a)

π(s(t), a)

=

=

Pa

Pa

Ps(t+1)

Ps(t+1)

P a

s(t)→s(t+1)

Pa
s(t)→s(t+1) + χV (s(t + 1))
i

Ps(t+1)

Ra
h

χkY (t + k + 1)|s(t)(cid:21)
Eπ
P a
lim
Υ→∞

s(t)→s(t)

Υ

Pk=0

χkY (t + 1+k)|s(t+1)
(cid:2)

(cid:3)

(22)

that the state transition probabilities of the Markov model is
unknown and the state is partly observable.

1) State: At the beginning of each slot, the local VR device
will upload its current cache index information ˜FL(t) along
with its viewpoint request. At the same time, the MEC server
also awares of its own cache index information ˜FM(t). We
assume that the path loss h(t) between the SBS and the VR
user remains unchanged in a slot duration and can be sensed by
the SBS in realtime. As already deﬁned in Section II, ˆp(t +
1) is used as a prediction of the unknown view popularity,
whose implementation details will be explained in Section IV.
Then, the current system state is modelled by the following
combination of variables:

where χ ∈ (0, 1) is the discount factor. The discounted
deﬁnition is common adopted in MDP problems and is similar
to the deﬁnition of value function in reinforcement learning.

IV. METHODOLOGY

In this section, we ﬁrst develop an LSTM neural network to
predict the unknown and time-varying viewpoint popularity.
Then, we integrate the LSTM network [29], [30] into the
DDPG algorithm [31], [32] to iteratively learn the optimal
hybrid policy for cooperative multi-tile ofﬂoading and caching
replacement such that the system cumulative cost is mini-
mized. The network architecture is illustrated in Fig. 4.

s(t) =

˜FL(t), ˜FM(t), ˆp(t + 1), h(t)

n(cid:16)

.

(23)

A. Popularity Prediction Based on LSTM

(cid:17)o

2) Action: The set of possible actions a(t) ∈ A is com-
posed of the set of deterministic ofﬂoading action o(t), and
the cache replacement policies cL(t) and cM(t) at the local
VR device and the MEC server. Let π be the policy and it
will output actions in any state s(t) ∈ S. Hence, the actions
under state s(t) in slot t is given by

a(t) = π(s(t)) = {(o(t), cL(t), cM(t))} .

(24)

3) State Transition: ∀s(t), s(t + 1) ∈ S and ∀a(t) ∈ A, the
state transition probability P a(t+1)
s(t)→s(t) describes the probability
that a system transits from the state s(t) to the state s(t+1)
under the action a(t). As for the solution of traditional
optimization approach or the dynamic programming, the state
transition probability is an essential component which however
is often not available in the real world due to the complicated
and changeable environment. Here we just explain the meaning
of the state transition probability in Markov problem. In our
problem, on account of the view popularity’s dynamics and
the channel condition’s randomness, we assume that the state
transition probability is unknown.

4) Reward Function: The reward function assigns each
perceived state to a value associated with an explicit goal in
the reinforcement learning problem. For an MDP, when an
action is taken under a state, the state will transfer to another
state and the environment will return an instantaneous reward
as a feedback immediately, which is derived as a negative of
the cost Y (t) in our problem, i.e.,

It is more reasonable and realistic to assume that the a priori
knowledge on the viewpoint popularity cannot be acquired. As
mentioned above, in this paper the dynamic popularity pγt(t)
is modeled as a Zipf distribution whose parameters follow a
Markov stochastic chain and transfer in a certain parameter
space. However, neither the parameter set nor the transition
probability is accessible for the system. The only information
the system knows exactly is the user’s requests under the
popularity in each slot, which is written as Vk(t)|pγt (t) and
abbreviated as Vk(t). Taking full advantage of the request
information, we design a multilayer LSTM neural network
with three layers including two LSTM layers and one dense
layer to predict the viewpoints popularity.

At the beginning of time slot t, the current viewpoint request
Vk(t) is added to the recorder and meanwhile the oldest
request Vk(t−Tr) is removed in order to keep the length of the
recorder constant. The requests vector R(t) is then obtained by
stacking a series of Tr requests in successive time slots, i.e.,
as R(t) = [Vk(t − Tr + 1), · · · , Vk(t)]. Subsequently, these
requests are fed to the LSTM neural network in sequence.
Note that, in the LSTM neural network, the input in each slot
contains not only the external input Vk(t − 1) but also the
internal input, which consists of the output and the memory
state from the previous slot. After the feeding of Tr successive
requests, the estimated popularity of the viewpoints request is
outputted in the next time slot t+1. Referring to the deﬁnition
in (2), we have

r(t) = −Y (t).

On this basis, we deﬁne the cumulative reward as

R =

Υ

t=1

X

χtr(t),

(25)

(26)

ˆp(t + 1) = fΘL (R(t)) ,

(27)

where ΘL is the collection of the trainable parameters and
ˆp(t + 1) is the predicted popularity. The training set of the

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

9

( )ta%

AN

layers

CN

layers

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

(cid:17)
(cid:17)
(cid:17)

pQ

( )tn

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

Neural 
Cell

(cid:17)
(cid:17)
(cid:17)

QQ

Actor network

Critic network

Target Q

.
.
.

.
.
.

t
i

m
e

( )h t

¢F

( )M t

L t¢F
( )

Neural 
Cell

t +p
ˆ (

1)

LN layers

LSTM 
Cell

LSTM 
Cell

LSTM 
Cell

LSTM 
Cell

LSTM 
Cell

LQ

LSTM network

Fig. 4. Network architecture of the LSTM-DDPG algorithm.

LSTM network can be denotes as {(xi, yi)|xi = R(ti), yi =
pγt (ti + 1)}, which is acquired from the historical experience.
The time slot set corresponding to the training set is repre-
sented as {t1, · · · , tL}, and the batch average of the MSE
loss function is adopted to yield a more stable convergence.
Thus, for any batch set {ti1 , · · · , tiX } ⊆ {t1, · · · , tL} where
ix ∈ {1, · · · , L}, we have

L(ΘL) =

1
X X

X

x=1

|ˆp(tix + 1) − pγt (tix + 1)|2,

(28)

where X denotes the batch size. The primary goal of the
LSTM is to minimize L(ΘL) and ﬁnd the optimal ΘL via
gradient descent. After the convergence of training, the LSTM
networks module can be embedded into the DDPG algorithm
for subsequent learning. The future popularity prediction af-
fects the caching replacement policy and affects the ofﬂoading
action in turn. Generally, high prediction accuracy yields high
system performance.

B. Learning Hybrid Policy Using LSTM-DDPG

The optimal policy π∗ determines which action should be
selected from the action space at any time t to maximize the
cumulative reward function. For traditional dynamic program-
ming algorithms, such policy is derived recursively from the
Bellman’s Equation and its solution can be written as

In this paper we focus on the deep reinforcement learn-
ing approach for the formulated problem (21). Conventional
learning approaches such as Q-learning usually build a Q-
table to record the Q-value whose size increase exponentially
along with the size of state/action spaces. The deep Q-
network (DQN) algorithm [33], [34] replaces the Q-table with
a neural network and can be used for the situations with
large/continuous state spaces. However, it still needs to select
the actions according to the maximal Q-value based on the
output of the neural network, which makes it computational
intensive for the situations with large/continuous action space.
In our problem, the state space S is a mixed space with
the caching state belonging to a discrete space while the
popularity and channel states belonging to a continuous spaces.
The action space A is extremely large because the multi-
tile has 22Z+ML+ME possible values in a single time slot.
Thus, neither can the DQN algorithm be applied. The DDPG
algorithm directly parameterizes the policy πΘ2 rather than
the Q-value function as in DQN or Q-learning, which enables
the LSTM-DDPG algorithm to handle problems with large
and continuous action space. Moreover, the user’s request
distributions, as one important component of the state, will
change as time elapses and is totally unknown to the agent.
We embed the LSTM neural network into the DDPG algorithm
to counteract with this limitation and improve the system
performance and convergence speed simultaneously.

π∗ =
arg max
a(t)∈A P

s(t+1)∈S P a(t)

s(t)→s(t+1) (r (t) + χV π (s (t + 1))) ,
(29)
where V π (s (t + 1)) is the value function which indicates the
cumulative reward under policy π at state s(t + 1). To obtain
the optimal π∗ in such a manner, the transition probability
P a(t)
s(t)→s(t+1) is essential. As a component of the system state,
although the future popularity of the viewpoint request can be
predicted, the system still has no idea about the distributed
parameter space, or even the number of different states.
Therefore,
the formulated Markov domain lacks the state
transition mapping as addressed in Remark 1. As such, the
traditional dynamic programming algorithms are inapplicable
to the problem under investigation in this work.

the tile Fu is in the viewpoint Vk(t); and xu

The structure of the proposed algorithm to solve problem
(21) is shown in Fig. 4. The deep neural network is an
important building block of the entire network structure.
k (t)]N
FVk(t) is vectorized as vec(FVk(t)) = [xu
u=1 before being
fed into the neural network, where xu
k(t) = 1 indicates
that
k (t) =
the recorded request vector
0 otherwise. Then, we input
R(t) =
into the NL-
layer LSTM neural network and obtain ˆp(t+ 1). Likewise, we
vectorize the ˜FL(t) and ˜FM(t) as vec( ˜FL(t)) =
and
vec( ˜FM(t)) =
, respectively. Subsequently, the vec-
torial state s(t) = (vec( ˜FL(t)), vec( ˜FM(t)), p′(t + 1), h(t))
is fed into the actor network, which is also a neural network
with NA dense layers. The actor network’s output consists of

vec(FVk(t−Tr+1)), · · · , vec(FVk(t))
(cid:3)
(cid:2)

N
i=1
(cid:3)

j=1
i

xj
M

xi
L

h

N

(cid:2)

10

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

three elements, i.e., o(t), cL(t), and cM(t), which constitute
a(t).

Algorithm 1 LSTM-DDPG approach training for our problem
1: Initialize: Initialize ΘL, Θπ, ΘQ and memory buffer Ω.

Remark 2:

In practice,

the actor network outputs a
continuous-valued vector and we need to convert the numeric
values to the binary values of 0/1.

L (t), c−

L (t), c+

In this paper, we further add ﬁve neural cells to the
output of the actor network to generate the threshold output
for o(t), c+
M(t), and c−
M(t) rather than manually
setting thresholds. Therefore, the actual output of the actor
network can be denoted as ⌣a(t)=
.
Nevertheless, we keep on using the notation a(t) in the
(cid:3)
following discussions for better illustration.

a(t), εo, ε+

M , ε−
M

L , ε−

L , ε+

(cid:2)

In order for the agent to fully explore the environment,
exploration-exploitation method is adopted. Different from the
ε-greedy exploration [35] which is effective for the small or
discrete action space. In this work, we balance the exploration
and the exploitation by adding a gaussian noise vector on the
policy output, i.e.,

˜a(t) = a(t) + n(t)|ni(t)∼N (0,σ2

n).

(30)

Then the action ˜a(t) will be immediately sent to the critic
neural network which contains NC dense layers together
with the vectorial state s(t). Consequently, the critic network
outputs the target-Q Q
which is a
step forward for estimating the Q-value. ΘQ is the collection
of trainable parameters of the critic network. After a linear
transformation, the output of the critic network is sent to the
actor network and contribute to the actor’s loss function. In
this way, the LSTM neural network, the actor network, and
the critic network are connected and work collaboratively.

s(t + 1), ˜a(t + 1)|ΘQ
(cid:0)

(cid:1)

In reinforcement learning, there are two important function
called state value function denoted as V (s) and state-action
value function (Q function) denoted as Q(s, a). V (s) rep-
resents the cumulative reward starting from state s under a
certain policy π. In this paper, we adopt the discount method
and the value function is thus deﬁned as

V π
γ (s) = Eπ

+∞

t=0

hX

χtr(t + 1)|s(0) = s
i

,

(31)

where r(t) is the instantaneous reward. Besides, after the agent
taking an action a under the initial state s, the cumulative
reward under a certain policy π can be given by another
function Q(s, a), deﬁned as

+∞

t=0

Qπ

hX

γ (s, a) = Eπ

χtr(t + 1)|s(0) = s, a(0) = a

.
i
(32)
These two function are usually used for policy evaluation. We
can see from their deﬁnitions that, the speciﬁc expectation
cannot be obtained if the state transition probability is un-
known. Actually, in reinforcement learning, they are given by
sampling estimates.

Let π(s|Θπ) denote the output of the actor network which
is a policy on parameters Θπ and let Q(s, a|ΘQ) be the critic
network’s output on parameters ΘQ. In practical training, these
two networks are called online networks. Correspondingly,
there are two clone networks respectively called target ac-
tor network and target critic network, whose parameters are

Copy the weights of online networks to the target networks:
Θπ

← Θπ, ΘQ

← ΘQ

′

′

2: Train the LSTM neural network using (28). Acquiring the con-

vergent model ΘL

∗

.

3: For episode = 1, · · · , T do:
4:
5:
6:
7:
8:

Initialize a random process N for the exploration.
Initialize user request recorder R(0).
Observe the initial state s(1)
For t = 1, · · · , Υ do:

Obtain current viewpoint request Vk(t). Select action
˜a(t)=π(s(t)|Θπ) + n(t)|ni(t)∼N (0,σ2
n)
Execute action ˜a(t), observe the instant reward r(t)
Obtain next viewpoint request Vk(t + 1)
Update the request recorder R(t) → R(t + 1).
Obtain ˆp(t + 2) by (27).
Observe the new state s(t + 1) and vectorize it.
Store tracing point (s(t), ˜a(t), r(t), s(t + 1)) in Ω
Randomly sample a mini-batch of X2 points
(s(t), ˜a(t), r(t), s(t + 1)) from Ω.
Compute yi = ri + γQ′(s′
i, π′(s′
)|ΘQ
Update the online critic network ΘQ by (33).
Update the online actor network Θπ by (36).
Update the target actor/critic network every ψ steps:

i|Θπ

)

′

′

9:
10:
11:
12:
13:
14:
15:

16:
17:
18:
19:

′

ΘQ
Θπ

′

← ϑ · ΘQ+(1−ϑ)ΘQ
← ϑ · Θπ+(1−ϑ)Θπ

′

′

soft update

0<ϑ<1 (

End For

20:
21: End For

′

′

) and Q′(s, a|ΘQ

copied from their online counterparts every a few steps. We de-
note their output as π′(s|Θπ
), respectively.
These target networks are employed for a stabler convergence.
During the training phase, experience replay is adopted. We
i)}X2
randomly take X2 samples {(si, ˜ai, ri, s′
i=1 as a mini-batch
from the replay memory buffer Ω, where s′
i is the next state
following state si. Then, we train the online critic network to
let the Q(s, a|ΘQ) approach the real Q value function which
will be used to guide the update of the Θπ. The loss function
of the critic network in an MSE sense is deﬁned as

L(ΘQ) =

X2

1
X2 X

i=1

yi − Q(si, ˜ai|ΘQ)
(cid:0)
(cid:1)
i|Θπ
i, π′(s′

)|ΘQ

′

′

2

,

(33)

where yi = ri + χQ′(s′
) can be seen as a
label. However, the concept of label is quite different from
that of the supervised learning. Here the label is not given by
handcraft but determined by the output of the target critic as
well as the environment reward feedback. Thus, ΘQ can be
updated by ∇ΘQ L and Q(s, a|ΘQ) will gradually approach
the real Q-value.

The actor is aimed at producing a optimal policy which
can acquire maximum cumulative reward, which is equivalent
to maximum Q-value. yi is sent to the actor network as a
current true Q-value. Then, for the current policy evaluation,
a performance objective function is designed as

Q(s, a|ΘQ)
(cid:2)
(cid:3)
which estimates the expectation of Q(s, a|ΘQ) under the state

Jβ(π) = Es∼ρβ

(34)

,

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

11

distribution s ∼ ρβ. Hence, the gradient in the view of Θπ is

∇Θπ Jβ(π) =
Es∼ρβ

∇aQ(s, a|ΘQ)|a=π(s|Θπ ) · ∇Θπ π(s|Θπ)
(cid:3)
Monte Carlo method is adopted to estimate this expectation
using the mini-batch samples with a size of I, which yields a
unbiased estimation:

(cid:2)

.

(35)

∇Θπ Jβ(π) ≈
1
I

i
P

∇aQ(si, ˜ai|ΘQ)|˜ai=π(si|Θπ)+n(t) · ∇Θπ π(si|Θπ)
.
(36)
(cid:1)
(cid:0)
We can see that Θπ is learned by following the direction of
∇Θπ Jβ(π).

As described in Algorithm 1, the target actor network Θπ
are soft-updated in every a few

′

′

and target critic network ΘQ
steps.

The overall training process of the proposed scheme is
also summarized in Algorithm 1. In Algorithm 1, T is the
total training episodes and ψ is the step interval between
the target networks and the online networks in parameters
update. ϑ is the coefﬁcient of the soft-update which is normally
set to 0.001. Note that, the proposed algorithm is executed
centrally in the MEC server. Although different users has
slightly different interests on the same content of VR video, the
statistical trend of the viewpoints’ popularity remains constant
[6]. Hence, the trained model can generally cater to different
users without updates if the same VR video is delivered.

V. NUMERICAL SIMULATIONS AND ANALYSES

In this section, we evaluate the performance of the proposed
scheme via numerical examples. In the simulation, we divide
the 2D domain into a ﬁxed number of 2D tiles with Ncol = 7
and Nrow = 5. For one viewpoint, Z = 4 with N FOV
col = 2 and
N FOV
row = 2. In addition, for less shadow effect [36], we set
both ∆h and ∆v equal to 1. Then, the number of viewpoints
K = 24. The total size of all the 2D tiles is set to Q =
5.25 Gbits [9], and the parameters of cache capacity are set
to ML = 3 and ME = 8, respectively. Adam optimizer [37] is
used to learn the parameters Θπ, ΘQ, and ΘL with a learning
rate of 10−4, 10−4 and 10−5 respectively. In addition, a 0.35
dropout rate is used in the hidden layers to avoid overﬁtting.
Other parameters are set by referring to the related works, for
example [38], [39], which are listed in Table I.

TABLE I
SIMULATION PARAMETERS

Parameter
PB
σ2
Rbh
fL
fM
ML

Value
30 dBm
−105 dBm
10 Gbit/s
3 GHz
10 GHz
3

Parameter
ME
ϕ
α
d
ω
γ

Value
8
3
2
100 m
15 cycle/bit
0.85

In order to imitate the real environment, the Zipf parameter
space is set as γt ∈ G = {0.7, 1, 1.5, 2.5} [22], with
a transition probability matrix P = [Pij ]4
i,j=0, where Pij
denotes the transition probability from γj to γi. Note that,
the transition probability matrix above is randomly generated
and the entries can be any values, which has no impact on

the system performance with our method. It should also be
emphasized that the Zipf parameter space and the transition
probability matrix are both unknown to our system during the
simulated VR service process. Here they are just provided
to demonstrate the operating mechanism of the simulation
environment and do not involve in the problem solving.

Note that, prior to training the entire network, the LSTM
network is pre-trained in advance. The experiment is run on an
Intel Core i5-7500 CPU with 3.40 GHz × 4 cores. We provide
the training statistics of the networks in Table II.

TABLE II
TRAINING STATISTICS

Training Phase

Iterations

Total Time

Pre-training
Training
Summation

1750
50000
51750

71 s
775.867 s
846.867 s

Average Time
(pre iteration)
0.0406 s
0.0155 s
0.0164 s

A. Baselines

1) Random Approach: In this baseline approach, the policy
is randomly formulated and a random action is executed
regardless of the current state.

In this approach,

2) Traditional DDPG Approach:

the
LSTM neural network is removed but the caching replacement
and task segmentation are reserved. The traditional DDPG
algorithm is used to solve the learning problem. Because of
the absence of LSTM neural network, we replace the ˆp(t + 1)
with the R(t) in order to ensure the fairness of the input
information compared with our method.

3) Normalized Advantage Functions (NAF) Approach:
NAF algorithm [40] is developed based on DQN and, sim-
ilar to the DDPG algorithm, it is applicable to the high-
dimensional or even continuous action control problem. But
different from the DDPG, which uses two networks to output
action and Q value, NAF integrates the action and Q value
into one neural network by dividing the Q value into advan-
tage term and state-value term. The caching replacement and
deterministic task segmentation/ofﬂoading are also considered
in implementing the NAF.

B. Results and Discussions

In Fig. 5, we investigate the impacts of four different conﬁg-
urations of the proposed algorithm on the system performance,
and we let ω = 0.8 in the simulations:

• Conﬁg. 1: Both the dynamic caching replacement and the
deterministic task segmentation/ofﬂoading are enabled in
both the local VR device and the MEC server. This
conﬁguration is the standard version of the proposed
scheme and is expected to achieve a best performance.
• Conﬁg. 2: The deterministic task segmentation/ofﬂoading
is enabled in both devices; while the dynamic caching
replacement is disabled in both devices.

• Conﬁg. 3: The the dynamic caching replacement
is enabled; while the deterministic task segmenta-
tion/ofﬂoading is disabled.

12

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

(a)

(b)

(c)

Fig. 5. Performance comparison of different conﬁgurations of the proposed algorithm with respect to different metrics. (a) Total reward. (b) Average latency
of each request. (c) Total energy consumption for one episode.

(a)

(b)

(c)

Fig. 6. Impact of the weight ω on the system performance. (a) Total reward versus ω. (b) Average latency of each request versus ω. (c) Energy consumption
versus ω.

Impact of the device cache capacities on the total reward. (a) Total reward versus the MEC server’s cache capacity with ML = 3. (b) Total reward

Fig. 7.
versus the local VR device’s cache capacity with ME = 8.

(a)

(b)

• Conﬁg. 4: Neither of deterministic task segmenta-
tion/ofﬂoading and the dynamic caching replacement is
enabled.

To guarantee fairness, the performance comparisons with
the four different conﬁgurations share the same parameter
setting as listed in Table I. From Fig. 5(a), we observe
that the standard Conﬁg. 1 yields maximum gain in terms
of the total reward. In addition, Conﬁg. 2 is superior to
Conﬁg. 3, which implies that the task segmentation/ofﬂoading
module contributes more to gain than the dynamic caching
replacement. This is due to the fact that the task segmentation
concurrently mobilizes computing and caching resources of

all devices in the system, while the caching replacement only
leverages the caching resources to improve hit ratio. The
performance gain is higher and more stable after adding the
caching replacement, because the cache hit ratio in each slot is
higher with the aid of the dynamic caching replacement owing
to the ﬁeld overlap. As a result, the transmitted date size is
smaller and the ﬂuctuation is lower. In terms of latency, we plot
the average latency curves in Fig. 5(b) which is equal to total
latency of one episode divided by T . It can be observed that,
the average latency of each task can decline and converges to
9 ms by adopting caching replacement and task segmentation
with ω = 0.8. Fig. 5(c) indicates that the proposed model

ZHENG et al.: HYBRID POLICY LEARNING FOR ENERGY-LATENCY TRADEOFF IN MEC-ASSISTED VR VIDEO SERVICE

13

Fig. 8. Performance comparison of different conﬁgurations and different algrithoms with respect to 95th percentile latency. (a) Different conﬁgurations. (b)
Different weights.

(a)

(b)

maintains the lowest energy consumption after convergence.
It also shows that the caching replacement has no effect on
the amount of energy expenditure but still contributes to the
stability. From the perspective of convergent behavior of the
proposed approach, as shown in Fig. 5, we ﬁnd that the agent
can earn a stabilized mean of reward after 2500 episodes,
which means that the agent has acquired the knowledge of the
entire system and the proposed algorithm gradually converges.
Note that we set Υ = 100 in the simulation and,
thus,
one episode corresponds to 100 iterations according to the
Algorithm 1.

The performance comparisons between the proposed
method and all the baseline algorithms versus the weight ω
are presented in Fig. 6. It can be seen from Fig. 6(a) that the
proposed algorithm outperforms all the baselines regardless
the value of ω. Note that a notch can be identiﬁed on each
of the total reward curves. This is due to the tradeoff between
the latency and the energy consumption. The larger the weight
is, the more important the latency is in the system. When the
weight is around 0.5, the agent choose to make actions that
ensures the equivalent signiﬁcance of these two considerations.
From Figs. 6(b) and (c), we can see that, the average latency
decreases with the increase of ω, at the expense of the system’s
energy consumption. This also reveals the tradeoff between the
two considerations. On the other hand, in Figs. 6(b) and (c),
we can ﬁnd that, when ω is at a higher value which means
the latency is paid more attention to (e.g. the system is fully
charged) the proposed algorithm consumes more energy to
ensure a lower latency than the other three algorithm. This
demonstrates that the proposed approach is more sensitive to
the weight variation and more ﬂexible to the different emphasis
of the system. We also readily observe from Figs. 6(b) and (c)
that, the random approach has no sense of the weight variation,
which is reasonable since random policies and actions are
selected and executed.

Fig. 7 illustrates the impact of the cache capacity at MEC
server and local VR device. We investigate the relationship
between the total reward and the variations of the cache
capacities ML and ME, and the other system parameters
remain unchanged. When the MEC server’s cache capacity is
restricted to 8τ , we change the local VR device’s capacity from

τ bit to 7τ bit in order to satisfy the inequality ML ≤ ME.
Similarly, we change the MEC server’s cache capacity from
3τ bit to 12τ bit and limit the cache capacity of the local
VR device to 3τ . Both Figs. 7(a) and (b) suggest that the
proposed algorithm outperforms all the other baseline methods
in terms of total reward performance and the total reward of
all considered algorithms increases with the cache capacity.
This is because that the MEC server or the local VR playback
device can cache more content with the improvement of the
cache capacity such that the agent is able to reduce the latency
with the same energy consumption. Fig. 7(b) shows that the
advantage of the proposed method becomes more signiﬁcant as
the cache capacity of the local device increases. This implies
that the cache resource at local VR playback device is very
helpful for the service, especially when the specialized VR
playback device with larger cache capacity is used. However, it
is noted that the curves in Fig. 7(b) grows slightly slower than
those of Fig. 7(a). The reason is that the computing latency and
energy dissipation of the local VR device is larger than that
of the MEC server, which reduces the gain from the cache
capacity improvement. We further evaluate the performance
of the proposed models and algorithm with respect to 95th
percentile latency during the consecutive T requests, which is
presented in Fig. 8. From Fig. 8(a), we again conﬁrm that the
Conﬁg. 1 signiﬁcantly outperforms the other conﬁgurations.
We also readily observe from Fig. 8(b) that, the proposed
algorithm outperforms all the other baseline methods with
respect to 95th percentile latency with different weight ω.

VI. CONCLUSION

VR is among the killer applications of the future wireless
networks, which will offer unprecedented experiences and
possibilities. In this paper, we consider realizing wireless
VR video service by exploiting a MEC-assisted network.
The problem of deterministic ofﬂoad and dynamic caching
replacement are ﬁrst jointly examined both at the MEC server
and the local VR device. A hybrid policy is formulated to
minimize the system latency and the energy consumption
as well as seek a trade-off between them. To solve the
yielded challenging multi-objective optimization problem and

14

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. XX, NO. X, XXX 2021

overcome the uncertainty of the viewpoint popularity, a mode-
free MDP is established and an LSTM-DDPG algorithm is
proposed to learn the optimal policy. The superior performance
of the proposed scheme compared to the baseline methods
and the gain contributed by each module are conﬁrmed by
the numerical simulations. Our future work will concentrate
on more complicated scenarios such as multiple wireless VR
users and multiple MEC nodes, where schemes for transmis-
sion resource management and interference suppression must
be investigated.

REFERENCES

[1] C. Zheng, S. Liu, Y. Huang, and L. Yang, “MEC-enabled wireless
VR video service: A learning-based mixed strategy for energy-latency
tradeoff,” in Proc. 18th IEEE Wireless Commun. Networking Conf.
(WCNC’20), Seoul, South Korea, May 2020, pp. 1–6.

[2] J. Chakareski, “VR/AR immersive communication: Caching, edge com-
puting, and transmission trade-offs,” in Proc. Workshop Virtual Reality
and Augmented Reality Network (VR/AR Network’17), Los Angeles, CA,
USA, Aug. 2017, pp. 36–41.

[3] S. Mangiante, G. Klas, A. Navon, G. Zhuang, R. Ju, and M. D. Silva,
“VR is on the edge: How to deliver 360◦ videos inmobile networks,” in
Proc. Workshop Virtual Reality and Augmented Reality Network (VR/AR
Network’17), Los Angeles, CA, USA, Aug. 2017, pp. 30–35.

[4] X. Yang, Z. Chen, K. Li, Y. Sun, N. Liu, W. Xie, and Y. Zhao,
“Communication-constrained mobile edge computing systems for wire-
less virtual reality: Scheduling and trade-off,” IEEE Access, vol. 6, pp.
16665–16677, Mar. 2018.

[5] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Communication, computing and
caching for mobile vr delivery: Modeling and trade-off,” in Proc. 17th
IEEE Int. Conf. Conference on Commun. (ICC’18), Kansas, MO, USA,
May 2018, pp. 1–6.

[6] Y. Sun, Z. Chen, M. Tao and H. Liu, “Communications, caching and
computing for mobile virtual reality: Modeling and tradeoff,” IEEE
Trans. Commun., to be published, doi: 10.1109/TCOMM.2019.2920594.
[7] T. T. Le, D. V. Nguyen, and E. Ryu, “Computing ofﬂoading over
mmWave for mobile VR: Make 360 video streaming alive,” IEEE
Access, vol. 6, pp. 66576–66589, Oct. 2018.

[8] Y. Liu, J. Liu, A. Argyriou, and S. Ci, “MEC-assisted panoramic VR
video streaming over millimeter wave mobile networks,” IEEE Trans.
Multimedia, vol. 21, no. 5, pp. 1302–1316, May 2019.

[9] M. Chen, W. Saad, C. Yin, and M. Debbah, “Data correlation-aware
resource management in wireless virtual reality (VR): An echo state
transfer learning approach,” IEEE Trans. Commun., vol. 67, no. 6 pp.
4267–4280, Feb. 2019.

[10] M. Chen, W. Saad and C. Yin, “Echo-liquid state deep learning for
360◦ content transmission and caching in wireless VR networks with
cellular-connected UAVs,” IEEE Trans. Commun., vol. 67, no. 9, pp.
6386–6400, Sept. 2019.

[11] X. He, H. Xing, Y. Chen and A. Nallanathan, “Energy-efﬁcient mobile-
edge computation ofﬂoading for applications with shared data,” in Proc.
61st IEEE Global Commun. Conf. (GLOBECOM’18), Abu Dhabi, UAE,
Dec. 2018, pp. 1–6.

[12] T. Dang and M. Peng, “Joint radio communication, caching, and
computing design for mobile virtual reality delivery in fog radio access
networks,” IEEE J. Sel. Areas Commun., vol. 37, no. 7, pp. 1594–1607,
July 2019.

[13] C. Guo, Y. Cui, and Z. Liu, “Optimal multicast of tiled 360 VR video in
OFDMA systems,” IEEE Commun. Lett., vol. 22, no. 12, pp. 2563–2566,
Dec. 2018.

[14] C. Guo, Y. Cui, and Z. Liu, “Optimal multicast of tiled 360 VR video,”

IEEE Wireless Commun. Lett., vol. 8, no. 1, pp. 145–148, Feb. 2019.

[15] M. Chen, W. Saad, and C. Yin, “Virtual reality over wireless networks:
Quality-of-service model and learning-based resource management,”
IEEE Trans. Commun., vol. 66, no. 11, pp. 5621–5635, Nov. 2018.
[16] “Cloud VR Solution White Paper (2018),” Huawei OptiX Lab, 2018,

https://www-ﬁle.huawei.com/-/media/corporate/pdf/ilab/2018/cloud vr
network solution white paper 2018 en v1.pdf
Immersive

[17] “White
Possible
2016,
whitepaper-making-immersive-virtual-reality-possible-in-mobile.pdf

Reality
Inc.,
Mobile,”
https://www.qualcomm.com/media/documents/ﬁles/

Virtual
Technologies

Paper:
in

Qualcomm

Making

[18] “White Paper: The Ultimate Guide to 360 Video Production,” Samsung,
https://image-us.samsung.com/SamsungUS/samsungbusiness/

2018,
short-form/the-ultimate-guide-to-360-video-production/
WHP-360-ROUND-PRODUCTION-GUIDE-MAR18SW.pdf

[19] Z. Jiang, X. Zhang, W. Huang, H. Chen, Y. Xu, J. Hwang, Z. Ma, and J.
Sun, “A hierarchical buffer management approach to rate adaptation for
360-degree video streaming,” IEEE Trans. Veh. Technol., vol. 69, no. 2,
pp. 2157–2170, Feb. 2020.

[20] S. Reichelt, R. Hussler, G. Ftterer, and N. Leister, “Depth cues in human
visual perception and their realization in 3D displays,” in Proc. SPIE
Defense Secur. Sens., Orlando, FL, USA, May 2010, art. no. 76900B.

[21] X. Cao, A C. Bovik, Y. Wang, and Q. Dai, “Converting 2D video to
3D: An efﬁcient path to a 3D experience,” IEEE MultiMedia, vol. 18,
no. 4, pp. 12–17, Apr. 2011.

[22] A. Sadeghi, F. Sheikholeslami, and G. B. Giannakis, “Optimal and
scalable caching for 5G using reinforcement
learning of space-time
popularities,” IEEE J. Sel. Top. Signal Process., vol. 12, no. 1, pp. 180–
190, Feb. 2018.

[23] J. Li, W. Chen, M. Xiao, F. Shu and X. Liu, “Efﬁcient video pricing and
caching in heterogeneous networks,” IEEE Trans. Veh. Technol., vol. 65,
no. 10, pp. 8744–8751, Oct. 2016.

[24] L. Yang, F.-C. Zheng, W. Wen, and S. Jin, “Analysis and optimization
of random caching in mmWave heterogeneous networks,” IEEE Trans.
Veh. Technol., vol. 69, no. 9, pp. 10140–10154, Sept. 2020.

[25] T. D. Burd and R. W. Brodersen, “Processor design for portable
systems,” J. VLSI Sig. Proc. Syst., vol. 13, no. 2, pp. 203–221, Aug.
1996.

[26] H. Trinh et al., “Energy-aware mobile edge computing and routing for
low-latency visual data processing,” IEEE Transactions on Multimedia,
vol. 20, no. 10, pp. 2562–2577, Oct. 2018.

[27] Y. Zhang, L. Jiao, J. Yan, and X. Lin, “Dynamic service placement for
virtual reality group gaming on mobile edge cloudlets,” IIEEE J. Sel.
Areas Commun., vol. 37, no. 8, pp. 1881–1897, Aug. 2019.

[28] Z. Zhang, Y. Yang, M. Hua, C. Li, Y. Huang, and L. Yang, “Proactive
caching for vehicular multi-view 3D video streaming via deep rein-
forcement learning,” IEEE Trans. Wireless Commun., vol. 18, no. 5, pp.
2693–2706, May 2019.

[29] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learn-
ing with neural networks,” in Proc. 28th Neural Inf. Process. Syst.
(NIPS’14), Montreal, Canada, Dec. 2014, pp. 3104–3112.

[30] J. Su, J. Zeng, D. Xiong, Y. Liu, M. Wang, and J. Xie, “A hierarchy-
to-sequence attentional neural machine translation model,” IEEE/ACM
Trans. Audio Speech Lang. Process., vol. 26, no. 3, pp. 623–632, Mar.
2018.

[31] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
“Deterministic policy gradient algorithms,” in Proc. 31st Int. Conf.
Mach. Learn. (ICML’14), Beijing, China, June 2014, pp. 387–395.
[32] T. P. Lillicrap, J J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D.
Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” in Proc. 4th Int. Conf. Learn. Represent. (ICLR’16), San Juan,
Puerto Rico, May 2016.

[33] M. Volodymyr, K. Koray, S. David, A. A. Rusu, V. Joel, M G. Bellemare,
G. Alex, R. Martin, A K. Fidjeland, and O. Georg, “Human-level control
through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp.
529–533, Feb. 2015.

[34] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D.
Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement
learning,” in Proc. 27th Neural Inf. Process. Syst. (NIPS’13), Lake
Tahoe, Nevada, USA, Dec. 2013.

[35] R. Sutton and A. Barto, “Reinforcement Learning: An Introduction,” ,

Cambridge, MA, USA: MIT press, 1998.

[36] S. Sukhmani, M. Sadeghi, M. Erol-Kantarci, and A. El Saddik, “Edge
caching and computing in 5G for mobile AR/VR and tactile internet,”
IEEE MultiMedia, vol. 26, no. 1, pp. 21–30, Jan. 2019.

[37] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
Proc. 3rd Int. Conf. Learn. Represent. (ICLR’15), San Diego, CA, USA,
May 2015.

[38] Z. Yang, C. Pan, W. Xu, Y. Pan, M. Chen, and M. Elkashlan, “Power
control for multi-cell networks with non-orthogonal multiple access,”
IEEE Trans. Wireless Commun., vol. 17, no. 2, pp. 927–942, Feb. 2018.
[39] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Unmanned aerial
vehicle with underlaid device-to-device communications: Performance
and tradeoffs,” IEEE Trans. Wireless Commun., vol. 15, no. 6, pp. 3949–
3963, June 2016.

[40] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep Q-
learning with model-based acceleration,” in Proc. 33rd Int. Conf. Mach.
Learn. (ICML’16), New York, NY, USA, June 2016, pp. 2829–2838.

