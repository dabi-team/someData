A NOVEL STEREO MATCHING PIPELINE WITH ROBUSTNESS AND UNFIXED
DISPARITY SEARCH RANGE

Jiazhi Liu∗†, Feng Liu∗†

∗State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy
of Sciences; †School of Cyber Security, University of Chinese Academy of Sciences
Email: {liujiazhi, liufeng}@iie.ac.cn

2
2
0
2

y
a
M
0
1

]

V
C
.
s
c
[

2
v
5
6
8
4
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT

Stereo matching is an essential basis for various applications,
but most stereo matching methods have poor generalization
performance and require a ﬁxed disparity search range. More-
over, current stereo matching methods focus on the scenes
that only have positive disparities, but ignore the scenes that
contain both positive and negative disparities, such as 3D
In this paper, we present a new stereo matching
movies.
pipeline that ﬁrst computes semi-dense disparity maps based
on binocular disparity, and then completes the rest depend-
ing on monocular cues. The new stereo matching pipeline
have the following advantages: It 1) has better generalization
performance than most of the current stereo matching meth-
ods; 2) relaxes the limitation of a ﬁxed disparity search range;
3) can handle the scenes that involve both positive and nega-
tive disparities, which has more potential applications, such as
view synthesis in 3D multimedia and VR/AR. Experimental
results demonstrate the effectiveness of our new stereo match-
ing pipeline.

Index Terms— Stereo matching, Cross-domain general-

ization, View synthesis

1. INTRODUCTION

Recently, “metaverse” has attracted many people’s attention
for its unlimited possibilities in the future. Compared with
current 2D display techniques, “metaverse” additionally uti-
lizes binocular disparity [1] that refers to the slight difference
between the left and right retina images, making people enjoy
more realistic and immersive experience. Therefore, disparity
estimation plays a basic role in the techniques of “metaverse”.
Since we have plenty of available stereoscopic content (such
as 3D movies), there is a clear need for methods that can esti-
mate disparity from such stereo pairs.

Given a position p in the 3D world space, if its projections
into the rectiﬁed left and right views are pL and pR, then their

This work was supported by the National Key R&D Program of China
with No. 2018YFC0806900, Beijing Municipal Science & Technology Com-
mission with Project No. Z191100007119009, NSFC No.61902397, NSFC
No. U2003111 and NSFC No. 61871378.

x − pR

disparity is deﬁned as DL(pL) = DR(pR) = pL
x where
pL
x denote the horizontal components of pL and pR.
x and pR
Current state-of-the-art stereo matching methods are designed
and trained to match only over positive disparity ranges be-
cause the prior methods 1) mainly focused on the applica-
tions of autonomous driving and robot navigation that only
require positive disparities; 2) need plenty of data to super-
vise the network, but there is not enough labeled data for the
scenes containing both positive and negative disparities; 3)
usually have poor generalization performance, so these meth-
ods have difﬁculties in utilizing large amounts of labeled data
from known datasets.
In this paper, we explore the above
challenges and present a new stereo matching pipeline: ﬁrst
computing semi-dense disparity maps based on binocular dis-
parity, and then estimating the rest based on monocular cues.

In contrast, the traditional stereo matching pipeline [2]
usually contains four steps: matching cost computation, cost
aggregation, disparity computation, and reﬁnement. Recent
learning-based stereo matching methods also follow the tra-
ditional pipeline and design various network architectures.
Here we review matching cost computation and cost aggre-
gation that prior methods mainly involve.

Matching costs are calculated based on hand-crafted or
learning-based feature maps extracted from the left and right
images, which expresses how well a pixel in the reference
image matches its candidates in the other. Feature descriptors
in traditional stereo matching methods are hand-crafted, usu-
ally based on gradients [3,4] and mutual information [5]. The
early deep-learning-based methods focus on designing fea-
ture extraction networks to characterize the left and right im-
ages [6, 7], and then utilize non-learned cost aggregation and
reﬁnement to determine the ﬁnal disparity map. The recent
end-to-end methods also design various Siamese-like net-
works to extract features, such as GCNet [8], StereoNet [9],
PSMNet [10], and GANet [11], and their feature extractors
are integrated into the whole network and trained in an end-
to-end manner. After obtaining matching costs, we intuitively
want to take the candidate with the smallest matching cost
(or the largest similarity) as the best-matched pixel, but re-
searchers generally do not adopt the intuitive operation for

 
 
 
 
 
 
the following reasons: 1) the left and right features have poor
ability to characterize the texture-less or repetitive regions; 2)
the single-visible pixels do not have corresponding pixels in
the other image. Therefore, for these ticklish pixels, the prior
methods usually estimate their disparities from their neigh-
boring tractable pixels, which is the job of cost aggregation.

Cost aggregation denotes we aggregate space context in-
formation in the cost volume, so that we can compute more
reliable disparities. Most traditional and some learning-
based methods aggregate matching costs in a 3D cost vol-
ume (disparity×height×width). Cost aggregation in a 3D
cost colume, including SGM [5], CostFilter [4], DispNet [12],
and AANet [13], resembles the votes of neighboring pix-
els. Recent learning-based methods, such as PSMNet [10],
GANet [11], and CFNet [14] execute cost aggregation in a
4D cost volume (feature×disparity×height×width), because
researchers believe that 4D cost volumes contain more infor-
mation than the 3D form.

Motivated by the neuroscience fact that binocular dispar-
ity is a low-level, pre-attentive cue of human depth cues, we
propose a new stereo matching pipeline that highlights the
role of feature extractors. Different from the traditional stereo
matching pipeline [2], our proposed pipeline no longer needs
cost aggregation: It ﬁrst computes accurate semi-dense dis-
parity maps directly from feature maps, and then generates
the ﬁnal disparity maps by disparity completion networks.

In summary, our proposed stereo matching pipeline has
the following advantages: 1) It has better generalization per-
formance than most state-of-the-art stereo matching methods;
2) It relaxes the limitation of a ﬁxed disparity search range; 3)
It can handle the scenes that involves both positive and neg-
ative disparities, whereas prior learning-based methods can
only deal with the scenes with positive disparities; 4 It does
not store and process cost volumes at the inference time, so it
needs less GPU memory.

2. METHOD

Our goal is to compute the disparity map from a known rec-
tiﬁed stereo pair. As mentioned above and shown in Figure
1, here we introduce the two steps of our stereo matching
pipeline in detail.

3D cost volume C = (c(d)
i,j ) for pre-deﬁned disparity candi-
dates D = {d ∈ Z | αdmin (cid:54) d (cid:54) αdmax}. The 3D cost
volume C is a tensor with the size of αh × αw × m where
m = α(dmax − dmin) + 1, and each element c(d)
i,j records how
well the position (i/α, j/α) in the left image matches its dis-
parity candidate (i/α, j/α − d/α) in the right image.

After building a 3D cost volume, prior methods usually
execute cost aggregation on the cost volume, so they need
to store the volume at the whole inference process. In con-
trast, we generate an initial disparity map Dinit = (dinit
i,j ) ∈
[αdmin, αdmax]αh×αw and the corresponding reliability map
R = (ri,j) ∈ [0, 1]αh×αw directly from the cost volume, and
thus there is no need to store the volume at the subsequent
steps. The reliability map records how reliable a predicted
disparity is, so we can obtain semi-dense disparity maps from
initial disparity maps and reliability maps.

We take the computation of disparity at the position (i, j)
as an example. We ﬁrst convert the 3D cost volume C into the
3D probability volume P = (p(d)
i,j ) by executing the softmax
operation along the disparity dimension. Like the sub-pixel
maximum a posteriori (MAP) [15,16], we compute the initial
disparity map according to

i,j = ˆdi,j + oi,j
dinit

(1)

where ˆdi,j = argmaxd∈D p(d)
i,j denotes we chose the disparity
candidate with the largest probability and oi,j is a reﬁnement
term to achieve sub-pixel precision. The reﬁnement term oi,j
is computed according to the possibilities at ˆdi,j −1 and ˆdi,j +
1:

oi,j =

1
(cid:88)

δ · p( ˆdi,j +δ)

i,j

(cid:44) 1

(cid:88)

p( ˆdi,j +δ)

i,j

(2)

δ=−1

δ=−1

where we regard (cid:80)1
initial disparity dinit
i,j .

δ=−1 p( ˆdi,j +δ)

i,j

as the reliability ri,j of the

Finally, we ﬁlter the initial disparity map Dinit to obtain
a semi-dense disparity map by left-right consistency check-
ing [4] and reliability checking (see Figure 1). The latter
checking refers to that the reliability of a predicted disparity
has to be larger than the reliability threshold τ .

2.1.1. Loss Function

2.1. Computing Semi-dense Disparity Maps directly from
Feature Maps

Like prior methods, we adopt a shared feature extractor to ex-
tract the feature maps from the left and right images. Each
feature map is a tensor with the size of αh × αw × n where
h, w, n, α represent image height, width, feature channels and
the down-scaled scale. Here we introduce the symbol α to
emphasize that current feature extractors generally extract
feature maps at smaller resolution than the original image.
Like AANet [13], we utilize a full correlation to generate a

Unlike prior stereo matching methods that train the feature
extractor and rest modules in an end-to-end way, our method
ﬁrst trains the feature extractor since we have been highlight-
ing its importance. The loss function contains three compo-
nents: (cid:96)dvr + (cid:96)ur + (cid:96)trr.

Double-visible-region loss (cid:96)dvr. For the position (i, j) in
the probability volume P , we hope its maximal probability
along the disparity dimension occurs at the ground truth dis-
parity. Moreover, if the position (i, j) is located in the oc-
cluded region, we cannot compute its disparity directly from
feature maps since it has no corresponding pixel in the other.

Fig. 1. Overview of our proposed stereo matching pipeline at the inference time. The pipeline contains two stages: ﬁrst com-
puting semi-dense disparity maps directly from feature maps; then generating ﬁnal disparity maps by two disparity completion
networks. At the training time, we ﬁrst train the ﬁrst stage using the loss functions in Sec. 2.1.1, and then the second stage.

parity has as low reliability as possible if it is located in the
occluded region or its value diverges from the ground truth
((cid:62) 1). Such the region is called the unreliable region U. The
unreliable-region loss is deﬁned as follows:

(cid:96)ur = −

1
|U|

(cid:88)

(i,j)∈U

log(1 − ri,j).

(4)

To-reﬁne-region loss (cid:96)trr. If a predicted disparity dinit
i,j is
close to the ground truth, then we use this loss function to
make the predicted disparity achieve more accurate sub-pixel
precision. Formally, the loss function is

(cid:96)trr =

1
|T |

(cid:88)

(i,j)∈T

|dinit

i,j − dgt
i,j|

(5)

where T = {(i, j) ∈ V(cid:12)

(cid:12)|dinit

i,j − dgt

i,j| (cid:54) 1}.

2.2. Generating Final Disparity Maps by Disparity Com-
pletion Networks

In the last subsection, we obtain a semi-dense disparity map,
an initial disparity map, and a reliability map. As shown
in Figure 1, we utilize two disparity completion networks
to complete hierarchically the semi-dense disparity and then
acquire the ﬁnal disparity map. The ﬁrst disparity comple-
tion network is similar to the monocular depth estimation net-
work [17]. It takes the initial disparity map, the semi-dense

Fig. 2. An example of weights of the double-visible-region
loss.

Therefore, we only execute this loss function in the double-
visible region that can be derived from the ground-truth dis-
parity map Dgt = (dgt
i,j). Formally, the double-visible-region
loss is deﬁned as

(cid:96)dvr = −

1
|V|

(cid:88)





(cid:88)

w(t+δ)
i,j

· log

(cid:16)

p(t+δ)
i,j

(cid:17)



 (3)

(i,j)∈V

δ=−1,0,1

where t = [dgt
i,j] indicates the integral disparity closest to the
ground truth and the set V denotes all the pixels in the double-
visible region. The coefﬁcient w(t+δ)
is a weight function,
and the closer to the ground truth a disparity is, the larger its
weight will be (see Figure 2). We give its detailed deﬁnition
in the supplementary material.

i,j

Unreliable-region loss (cid:96)ur. We hope that a predicted dis-

Feature ExtractorSemi-dense Disparity MapFinal Disparity MapRight Initial Disparity MapThe First Disparity Completion NetworkThe Second Disparity Completion NetworkLeft ImageRight ImageReliability CheckingLeft-right ConsistencyChecking3Left Initial Disparity Map2Reliability Map11. Computing Semi-Dense Disparity Map2. Generating Final Disparity Mapdisparity map, the reliability map, and the reference color im-
age as inputs, and outputs a full disparity map with the 1/2
resolution. The second disparity completion network is simi-
lar to the reﬁnement module in AANet [13]. It takes the full
disparity map from the ﬁrst network and the left image as in-
puts and outputs the ﬁnal disparity map.

At the training time, the ﬁrst disparity completion net-
work also outputs the disparity maps with the resolution
1/4, 1/8, 1/16. We train the two networks by supervising
the ﬁve disparity maps with different resolutions. The cor-
responding loss function is the smooth L1 loss [10] that mea-
sures the difference between output disparity maps and the
ground truths. More details are provided in the supplemen-
tary material.

3. EXPERIMENTS

3.1. Datasets

We train the networks only in SceneFlow [12], and evaluate
them qualitatively and quantitatively on the training sets of
KITTI2012 [18], KITTI2015 [19], Middlebury [20], which
only contain positive disparities. We also use the dataset 3D
Movie that includes negative disparities besides positive ones.
SceneFlow. SceneFlow is a large synthetic dataset contain-
ing 35,454 training and 4,370 test images with a resolution of
960 × 540, which have dense ground truth disparity maps.
KITTI2012&2015. KITTI2012 and KITTI2015 are real-
world datasets with street views captured from a driving car,
providing 394 stereo pairs of outdoor driving scenes with
sparse ground-truth disparities for training, and 395 pairs for
testing.
Middlebury. Middlebury is a small indoor dataset containing
less than 50 stereo pairs with three different resolutions. Here
we use its half version.
3D Movie. We chose stereo images that contain both neg-
ative and positive disparities from the 3D movie ”Big Buck
Bunny” ©by Blender Foundation (https://peach.blender.org/).
Their resolution is 1080 × 1920. The dataset does not have
available ground truth disparities, so we just give a qualitative
evaluation and visualized results.

Fig. 3. Ablation study results for left-right consistency check-
ing, sub-pixel precision, and different reliability threshold
values for reliability checking. “StereoNet-Ours” denotes we
utilized the feature extractor of StereoNet [9] in our proposed
stereo matching pipeline. “PSMNet-Ours” denotes we uti-
lized the feature extractor of PSMNet [10] in our proposed
stereo matching pipeline. “w/o sub-pixel precision” means
we just use ˆdi,j in Eq.(2) as the initial disparity map.

cropped to the size 288 × 576, and their color was ran-
domly augmented. We also used asymmetric data augmenta-
tion [21]. We ﬁrst trained the feature extractor for 18 epochs
with a batch size 4, and the learning rate started at 0.0005 and
was decreased by half every 3 epochs after 6 epochs. After
training the feature extractor, we then trained the rest module
networks for 24 epochs, and the learning rate started at 0.001
and was decreased by half every 4 epochs after 8 epochs. All
the models are trained and tested on one NVIDIA Tesla-P100
GPU.

3.2. Implementation Details

3.3. Ablation Study

We implemented the feature extractors in PyTorch and using
Adam (β1 = 0.9, β2 = 0.999) as optimizer. At training
time we set the minimum disparity dmin = 0 and the max-
imum disparity dmax = 320, whereas at test time we adap-
tively adjusted the minimum and maximum disparity for dif-
ferent datasets. Speciﬁcally, we set dmin = 0, dmax = 192
for KITTI2012 and KITTI2015, dmin = 0, dmax = 320 for
Middlebury, and set dmin = −100, dmax = 100 for 3D Movie.
We performed color normalization with ImageNet mean and
standard deviation. During training, images were randomly

We conducted experiments with several settings to evaluate
our proposed stereo matching pipeline, including the usage
of left-right consistency checking, sub-pixel precision, differ-
ent reliability threshold values for reliability checking, and
different feature extractors (StereoNet [9] and PSMNet [10]).
Left-right consistency checking and reliability checking work
together to ﬁlter possibly-wrong disparities in the initial dis-
parity map. Higher reliability threshold τ means more strict
reliability checking. As shown in Figure 3, if τ is lower
than about 0.4, left-right consistency checking has a signiﬁ-

KITTI2012LR Consistentcy Checking;         Sub-pixel PrecisionThe BestMethodStereoNet-OursPSMNet-OursKITTI20150.00.2Reliability Threshold0.40.60.86789101112D1_all (%)_0.00.2Reliability Threshold0.40.60.86789101112D1_all (%)_0.00.2Reliability Threshold0.40.60.86789101112D1_all (%)_0.00.2Reliability Threshold0.40.60.86789101112D1_all (%)_The Bestw/o LR Consistentcy Checking;  Sub-pixel PrecisionLR Consistentcy Checking;         w/o Sub-pixel Precision6.4256.0206.8266.5326.7967.0106.5416.147Method

CostFilter [4]
SGM [5]

PSMNet [10]
GWCNet [22]
AANet [13]

KITTI2012
D1 all(%)↓

KITTI2015 Middlebury
bad2.0(%)↓
D1 all(%)↓
Traditional Methods

18.9
7.6

21.7
7.1
Domain-speciﬁc Methods
15.1
20.2
18.3
Domain-invariant Methods

16.3
22.7
12.2

DSMNet [23]
STTR [21]

6.2
8.4

6.5
7.7

Our new stereo matching pipeline

StereoNet-Ours
PSMNet-Ours

6.5
6.0

6.8
6.4

40.5
25.2

25.1
34.2
31.0

13.8
oom

22.8
21.2

Time
(s)↓

240∗
3.7∗

0.337
0.32∗
0.123

1.6∗
1.416

0.095
0.198

Table 1. Generalization performance evaluation. “oom” de-
notes “out of memory”. We tested the running time on a
NVIDIA P100 GPU for the KITTI resolution 384 × 1344.
The symbol “*” in Time denotes that the running time value
is taken from the stereo evaluation system of KITTI.

cant effect on the ﬁnal result; otherwise, left-right consistency
checking no longer work. Removing the sub-pixel precision
module leads to a slight performance drop.

For the methods “StereoNet-Ours” and “PSMNet-Ours”,
we get the best result when using left-right consistency check-
ing, reliability checking with τ = 0.3, and sub-pixel precision
module. If we do not adopt left-right consistency checking,
we can also obtain competitive results just depending on sub-
pixel precision and reliability checking with τ = 0.5 (the blue
dot v.s. the orange dot in Figure 3).

3.4. Evaluation

3.4.1. Cross-domain generalization performance

Current state-of-the-art methods can achieve impressive per-
formance in some special datasets, but usually have poor gen-
eralization performance. Here we evaluate the generalization
performance of our stereo matching pipeline. Speciﬁcally,
all the methods are trained in the synthesized dataset Scene-
Flow [12], and tested in three real-world datasets KITTI2012
[18], KITTI2015 [19], and Middlebury [20]. As shown in
Table 1, our method outperforms traditional methods [4, 5],
and the learning-based domain-speciﬁc methods [10, 13, 22]
on three datasets. STTR [21] adopts the prevailing trans-
former architecture and is designed for cross-domain gener-
alization. Our method outperforms STTR on two datasets.
Note that STTR failed in the half-resolution version of Mid-
dlebury, because it requires too much GPU memory. DSM-
Net [23] is also designed for cross-domain generalization,
and our method can surpass it in two datasets. Besides, our
method is far faster than the two domain-invariant methods
STTR [21] and DSMNet [23].

Method

Unﬁxed
Range

Sub-pixel
Precision

Negative
Disparities

Robust

CostFilter [4]
SGM [5]

PSMNet [10]
AANet [13]

DSMNet [23]
STTR [21]

Ours

(cid:51)
(cid:51)

(cid:51)(cid:55)
(cid:55)

(cid:51)(cid:55)
(cid:51)

(cid:51)

(cid:55)
(cid:55)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:55)
(cid:55)

(cid:55)
(cid:55)

(cid:51)

(cid:55)
(cid:51)

(cid:55)
(cid:55)

(cid:51)
(cid:51)

(cid:51)

Table 2. Qualitative evaluation of our method and prior meth-
ods. The symbol (cid:51)(cid:55) denotes in the column “Unﬁxed Range”
denotes that the performance will be unstable if we change
their disparity search range [15].

Fig. 4. Visualized results. In reliability maps, bright colors
means high reliability, whereas dark colors denotes low relia-
bility.

not generate sub-pixel disparity maps because they use the
scheme “winner takes all”. For learning-based methods, 1) if
a method (such as AANet [13]) adopt cost aggregation on a
3D cost volume, its disparity search range is ﬁxed and cannot
be changed; 2) if a method (such as PSMNet [10] and DSM-
Net [23]) executes cost aggregation on a 4D cost volume, its
disparity range can be changed, but the performance is un-
stable if the range is changed. These learning-based methods
are limited to supporting the scenes with only positive dispar-
ities, whereas our stereo matching pipeline can also support
the scenes with both negative and positive disparities.

3.4.2. Visualized Results

Table 2 lists the qualitative evaluation results of our
method and prior methods. For traditional methods, they can-

We give an illustration of our method on KITTI2012 and 3D
Movie. As Figure 4 shows, although our method is only

KITTI 20123D MovieLeft ImageReliability MapDisparity MapLeft ImageSemi-dense Disparity MapReliability MapSemi-dense Disparity MapDisparity Maptrained in a synthesized dataset and over positive disparities, it
still has a satisfying visualized result. Note that the stereo im-
ages of 3D Movie contain both positive and negative dispar-
ity, but it does not matter for our method. Figure 4 also shows
reliability maps and semi-dense disparity maps. Surprisingly,
the disparities of more than 80% pixels can be computed ac-
curately just depending on feature extractors, and most un-
known disparities occur in the occluded region. For more vi-
sualized results, please see the supplementary material.

4. CONCLUSION

In this paper, we have presented a new stereo matching
pipeline that highlights binocular disparity of human depth
cues, and no longer needs cost aggregation:
It ﬁrst com-
putes accurate semi-dense disparity maps directly from fea-
ture maps, and then generates the ﬁnal disparity maps by
disparity completion networks. The new stereo matching
pipeline 1) has superior generalization performance, 2) avoids
the need to pre-specify a ﬁxed disparity search range, and 3)
can handle the scenes with both positive and negative dispari-
ties. We experimentally demonstrate that our stereo matching
pipeline generalizes to different domains without ﬁne-tuning
and give visualized results on the scenes with both negative
and positive disparities.

5. REFERENCES

[1] Piotr Didyk, Tobias Ritschel, Elmar Eisemann, Karol
“A perceptual

Myszkowski, and Hans-Peter Seidel,
model for disparity,” in SIGGRAPH, 2011.

[2] Daniel Scharstein and Richard Szeliski, “A taxonomy
and evaluation of dense two-frame stereo correspon-
dence algorithms,” IJCV, vol. 47, no. 1, 2002.

[3] Andreas Klaus, Mario Sormann, and Konrad Karner,
“Segment-based stereo matching using belief propaga-
tion and a self-adapting dissimilarity measure,” in ICPR,
2006, vol. 3.

[4] Asmaa Hosni, Christoph Rhemann, Michael Bleyer,
Carsten Rother, and Margrit Gelautz, “Fast cost-volume
ﬁltering for visual correspondence and beyond,” IEEE
TPAMI, vol. 35, no. 2, 2012.

[5] Heiko Hirschmuller, “Stereo processing by semiglobal
matching and mutual information,” IEEE TPAMI, vol.
30, no. 2, 2007.

[6] Jure Zbontar, Yann LeCun, et al., “Stereo matching by
training a convolutional neural network to compare im-
age patches,” J. Mach. Learn. Res., vol. 17, no. 1, 2016.
[7] Wenjie Luo, Alexander G Schwing, and Raquel Urta-
sun, “Efﬁcient deep learning for stereo matching,” in
CVPR, 2016.

[8] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta,
Peter Henry, Ryan Kennedy, Abraham Bachrach, and

Adam Bry, “End-to-end learning of geometry and con-
text for deep stereo regression,” in ICCV, 2017.

[9] Sameh Khamis, Sean Fanello, Christoph Rhemann,
Adarsh Kowdle, Julien Valentin, and Shahram Izadi,
“Stereonet: Guided hierarchical reﬁnement for real-time
edge-aware depth prediction,” in ECCV, 2018.

[10] Jia-Ren Chang and Yong-Sheng Chen, “Pyramid stereo

matching network,” in CVPR, 2018.

[11] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and
Philip HS Torr, “Ga-net: Guided aggregation net for
end-to-end stereo matching,” in CVPR, 2019.

[12] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fis-
cher, Daniel Cremers, Alexey Dosovitskiy, and Thomas
Brox, “A large dataset to train convolutional networks
for disparity, optical ﬂow, and scene ﬂow estimation,” in
CVPR, 2016.

[13] Haofei Xu and Juyong Zhang, “Aanet: Adaptive aggre-
gation network for efﬁcient stereo matching,” in CVPR,
2020.

[14] Zhelun Shen, Yuchao Dai, and Zhibo Rao,

“CFNet:
Cascade and fused cost volume for robust stereo match-
ing,” in CVPR, 2021.

[15] Stepan Tulyakov, Anton Ivanov, and Francois Fleuret,
“Practical deep stereo (pds): Toward applications-
friendly deep stereo matching,” Advances in neural in-
formation processing systems, vol. 31, 2018.

[16] Tyler Nuanes, Matt Elsey, Aswin Sankaranarayanan,
and John Shen, “Soft cross entropy loss and bottleneck
tri-cost volume for efﬁcient stereo depth prediction,” in
CVPR, 2021.

[17] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J.
“Unsupervised monocular depth estimation

Brostow,
with left-right consistency,” in CVPR, 2017.

[18] Andreas Geiger, Philip Lenz, and Raquel Urtasun, “Are
the kitti vision

we ready for autonomous driving?
benchmark suite,” in CVPR, 2012.

[19] Moritz Menze and Andreas Geiger, “Object scene ﬂow

for autonomous vehicles,” in CVPR, 2015.

[20] Daniel Scharstein, Heiko Hirschm¨uller, York Kitajima,
Greg Krathwohl, Nera Neˇsi´c, Xi Wang, and Porter West-
ling,
“High-resolution stereo datasets with subpixel-
accurate ground truth,” in German conference on pat-
tern recognition, 2014.

[21] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy
Ding, Francis X Creighton, Russell H Taylor, and Math-
ias Unberath, “Revisiting stereo depth estimation from
a sequence-to-sequence perspective with transformers,”
in ICCV, 2021.

[22] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang
“Group-wise correlation

Wang, and Hongsheng Li,
stereo network,” in CVPR, 2019.

[23] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor
Prisacariu, Benjamin Wah, and Philip Torr, “Domain-
invariant stereo matching networks,” in ECCV, 2020.

