PoVRPoint: Authoring Presentations in Mobile Virtual Reality

Verena Biener, Travis Gesslein, Daniel Schneider, Felix Kawala, Alexander Otte, Per Ola Kristensson,
Michel Pahud, Eyal Ofek, Cuauhtli Campos, Matjaˇz Kljun, Klen ˇCopiˇc Pucihar and Jens Grubert

2
2
0
2

n
a
J

7
1

]

C
H
.
s
c
[

1
v
7
3
3
6
0
.
1
0
2
2
:
v
i
X
r
a

Fig. 1. Interaction techniques for authoring presentations in mobile VR. a) 3D object manipulation technique, b) occlusion handling, c)
animations with time represented through height, d) working across slides and displaying different content resources

Abstract—Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with
a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such
as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint—a set of tools coupling pen-
and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study
the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects
on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our
results indicate that 1) the wide ﬁeld of view afforded by VR results in signiﬁcantly faster target slide identiﬁcation times compared to a
tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables signiﬁcantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study further conﬁrmed that the interaction techniques were
found to be usable and enjoyable.

Index Terms—Virtual Reality, Presentation Authoring, Mobile Knowledge Work, Pen and Touch Interaction

1 INTRODUCTION

Using slide-based presentation tools, such as Apple’s Keynote and
Microsoft’s PowerPoint, has widespread use across many sectors, such
as education, business and academia. In recent years, more people have
had the need to work in less than perfect environments, at home, in
makeshift ofﬁces and on the go. To enable workers to be productive
everywhere, even the small space of a middle seat on a coach ﬂight, we
need to focus on using small, portable devices such as tablet computers
and the small input space where the user may move their hands without
interfering with their physical environment.

Virtual Reality (VR) as an instance of Extended Reality (XR), can
beneﬁt users immensely, as it provides a much larger display space than
traditional mobile devices, independent of the physical environment
and allows rendering of 3D elements above the screen, potentially miti-
gating effects of a small physical screen. [4, 15, 18]. HMDs can also
increase privacy and reduce environmental clutter [18,57]. Using VR to
support knowledge workers on the go, and therefore in limited spaces,
has already been discussed in previous work [18, 43, 77] and interaction
techniques for such scenarios have been developed for multi-screen
environments [4] or spreadsheet applications [15]. This approach is
also supported by the advancement of new hardware, as it is already
possible to use inside-out tracking HMDs on the go and multiple manu-
facturers work on light HMDs as a replacement for physical screens.
(While our prototype system relies on a stationary tracking system,
we expect tracking systems to be available in mobile scenarios in the
foreseeable future). Furthermore, new HMDs are designed as monitor
replacements, increasing mobility of information workers in the near
future [1, 48], which we hope will increase the popularity of immersive

Author’s version.
Contact authors: verena.biener@hs-coburg.de and jens.grubert@hs-coburg.de

displays for productivity.

In this work, we focus on exploring how to support the editing
process of slide-based presentations using VR in a mobile scenario and
present our prototype called PoVRPoint. However, common interaction
techniques for VR relying on in-air interaction with controllers or hands
might not be well-suited for the limited interaction space in mobile
settings, especially regarding practicality and social acceptability [43].
In addition to the restricted space, fatigue can limit the applicability of
in-air interaction [23]. As hand and eye gaze tracking technologies are
already being incorporated into commercial HMDs this opens up new
interaction possibilities. Along with mobile devices such as tablets,
touch-based interaction can be augmented with spatial interaction above
and around the screen [4,15] to facilitate knowledge workers’ tasks and
potentially improve the overall interaction experience in limited spaces.
Therefore, we combine a VR HMD with eye-tracking and bimanual
pen and touch techniques, where pen-based input can be sensed on and
above mobile devices, requiring less interaction space [15].

We are concentrating on facilitating the authoring process of presen-
tations by expanding typical 2D presentation editing interfaces, such as
Microsoft PowerPoint, with an increased 3D output and input space,
instead of inherently changing the nature of how presentations are au-
thored today. This makes it possible to leverage familiarity of existing
tools and increases compatibility between VR and non-VR working
modes.

In this paper we do not present a complete presentation authoring ap-
plication, but we investigate common tools used in presentation editing
that are concerned with both, editing individual slides and interacting
with presentations across multiple elements (multiple slide-sets, PDFs,
browsers) and how they can be enhanced using VR. Speciﬁcally, we
explored if an extended display space can help users identify target
slides (or images) faster, as this is a common task when editing a larger
slide-set. In addition, we also investigated how to improve reordering of
multiple shapes on a slide in presence of occlusions which can get prob-

 
 
 
 
 
 
lematic as the number of objects on a slide increases. We evaluated the
performance of these techniques to understand how they can facilitate
authoring presentations in VR. For additional concepts and techniques
presented in this work, we report a usability study (n = 18), conﬁrming
the designed interaction techniques are usable and enjoyable.

In summary, this paper makes four central contributions. First, we
present the design and implementation of four VR-based techniques
for authoring presentations including manipulating objects, occlusion
handling, animations, and working across slides. Second, we quan-
tify the beneﬁts of using the increased display space of a VR HMD
compared to a tablet in a visual search task, e.g., when searching for
slides or images. Our results indicate the superiority of VR when the
matching task is easy (pre-attentive visual search). However, when the
matching difﬁculty is high (attentive search), VR performs similarly
to the tablet. Third, we show that the VR-based reordering technique
supports signiﬁcantly faster arranging of occluded objects (such as
multiple overlapping shapes on a single slide) compared to the two
existing baseline techniques used in PowerPoint. Fourth, we report that
the VR-based techniques have been found usable and enjoyable in a
usability study (n = 18).

2 RELATED WORK
Our work draws on the areas of supporting knowledge workers in XR,
pen-based interaction, gaze-based interaction and in-air interaction as
well as authoring and presenting in XR.

2.1 Knowledge Workers in VR
The use of XR for supporting knowledge work has attracted recent
research interest, e.g., [18, 20, 42, 56]. Early research focused on
projection systems to extend stationary physical ofﬁce environments,
e.g., [55, 72]). With the rise of affordable VR and AR HMDs these
devices also have been explored as tools for assisting users when in-
teracting with physical documents, e.g., [16, 37]. Further, Grubert
et al. [18, 45] and McGill et al. [43] explored the positive and nega-
tive qualities that VR introduces in mobile knowledge work scenar-
ios. Desktop-based environments have been studied for tasks such
as text entry [19, 31, 41], system control [75, 76] and visual analyt-
ics [6, 70]. Research on productivity-oriented desktop-based VR has
concentrated on the use of physical keyboards and mouse-based input
along with HMDs [57, 71], controllers and hands [33, 76], and, recently,
tablets [4, 15, 63].

Our work complements these prior studies by investigating the po-
tential of editing slide-based presentations in VR using mobile devices
such as tablets.

2.2 Pen-based, In-air and Gaze-based Interaction
Besides the commonly used single-point input with pens, enhanced
interaction techniques have been explored. Examples include using
touch input on the non-dominant hand, supporting pen input in bi-
manual interaction [26, 50], unimodal surface-based pen-postures [7],
bending [14] or using sensors in or around the pen [24, 40] for gestures
and postures, and examining pen-grips [61]. Our work was inspired by
tilting [66] and hovering [17] the pen above interactive surfaces, which
we use in a VR context.

The use of pens in AR and VR has also been investigated as a
standard input device on physical props [64], as well as using grip-
speciﬁc gestures for in-air interaction [36]. The accuracy of pen-based
in-air pointing has also been studied [52]. In AR, pen-based interaction
was speciﬁcally investigated for an object manipulation task [69].

In prior work on combining in-air with touch interaction, Marquardt
et al. [39] investigated the use of on and above surface input on a
tabletop. Chen et al. [10] explored in-air use of on and above surface
input on a tabletop. They propose that interactions can be composed
by interweaving in-air gestures before, between, and after touch on
a prototype smartphone augmented with hover sensing. Hilliges et
al. [22] have been using hover to allow more intuitive interaction with
virtual objects that represent physical objects. More recently, Hinckley
et al. [25] have been exploring a pre-touch modality on a smartphone
including the approach to record trajectories of ﬁngers to distinguish

between different operations. Such technology can be used to connect
3D tracking and touchscreen digitizer for better accuracy of tracking.
Most VR in-air interaction typically aims at using unsupported hands.
To enable reliable selection, targets are designed to be sufﬁciently large
and spaced apart [62]. Our focus on mobile knowledge workers on the
move implies small gestures to reduce working fatigue and to retain
usability in potentially cramped environments, such as airplane seats or
tiny work places such as touchdown spaces. We utilize gestures to be
used by a hand, resting on the screen of a tablet and holding a pen.

In addition, the combination of eye-gaze with other modalities such
as touch [49], in-air gestures [51, 58] and head-movements [34, 59] has
been investigated for interaction in spatial user interfaces. For a recent
survey on gaze-based interaction in AR and VR, see Hirzle et al. [27].
Speciﬁcally, our techniques were inspired by gaze-based interaction
with virtual screens [4] as well as the combination of pen-based and
touch-based interaction for mode switching [50] but adapted those
techniques speciﬁcally for the use case of editing presentations.

2.3 Presenting and Authoring in XR

XR has been explored for complementing or substituting established
methods for presenting materials, e.g., in the medical domain [46],
general education [29] or training [9]. For example, Kockro et al. [32]
compared VR and (2D) PowerPoint lectures in an anatomy context, and
found no performance differences but found VR to be rated higher in
domains such as spatial understanding and enjoyability. With the recent
rise of online conferences, VR has also been explored for delivering
oral presentations and poster sessions [35]. However, the beneﬁts and
drawbacks of presenting in VR compared to 2D conferencing tools such
as Zoom, are yet to be explored in depth. XR has also been proposed as
an aid for training public speaking [60] as well as in-situ support [47].
Besides using XR for presenting content to an audience, consider-
able work was invested in creating content for and in XR [3, 38, 44].
Speciﬁcally, XR was investigated for supporting modelling [11, 54],
sketching [12] and creating animations [2, 8, 68].

Complementary to these previous approaches, our work focuses on

utilizing VR as a tool for authoring 2D presentations.

3 INTERACTION TECHNIQUES FOR AUTHORING PRESENTA-

TIONS IN MOBILE VR

We looked at several challenging aspects of using a 2D slide editing
program, such as dealing with 3D orientation and ordering, dealing
with temporal data, and retrieving information from a large corpus of
graphics data. Then, we designed a set of interaction and visualization
techniques using the advantages that VR provides, such as a large
display space, a depth display and in-air interaction. Those techniques
are just sample points in the entire space of tasks used for presentation
authoring, but they can already show the advantages of using a VR
environment. With our techniques, we want to support knowledge
workers on the go or other conﬁned spaces, limiting the choices of
hardware. Therefore, our setup includes a tablet lying in front of the
users, who hold a stylus with two buttons in their dominant hand, while
their non-dominant hand is used for mode switches on the touchscreen
in the area near the border. Both tablet and stylus are spatially tracked
to represent them in VR. For designing these interaction techniques, we
followed an iterative approach with multiple design iterations consisting
of conceptualization, implementation and initial user tests (eating your
own dog food) [67].

As the techniques are designed to support users when working in
adverse conditions, such as conﬁned spaces, lack of privacy and limited
display size [4, 18], VR provides multiple advantages. First, users
are no longer restricted to their available physical displays and can
view information in the space around them, beyond the bounds of
a mobile device. Second, the three-dimensional VR display enables
depth visualization to allow utilization of the space above (or below) a
2D surface. Third, touch-based interaction can be complemented with
further modalities such as in-air or gaze-based interaction. Fourth, the
entire display is seen only by the users, maintaining their privacy and
reducing visual disturbance from the environment.

Because we want to support small (mobile) work spaces, we use
the tablet’s surface as the main interaction space. This provides space
for hand motions above it while the touchscreen supports easy and
accurate input. However, in many cases it might not be comfortable to
look at the tablet lying on a table. In such cases, the slide and also the
pen could be re-projected to be in front of the user’s head for indirect
manipulations as suggested by prior work [19].

In the following subsections, we will present concepts for editing
an individual slide, as well as concepts for working with multiple
slides and other resources. Note: For video description of interaction
techniques refer to the accompanying video.

3.1 Editing Slides

Preparing presentations requires the user to create slides that contain
a collection of information, arranged both along the area of the slide,
as well as in depth (layering of items) and in time (animation of the
items). In the following, we propose techniques for such tasks using a
pen and a tablet in VR.

3.1.1 Manipulation of Objects

Common presentation tools, such as Microsoft PowerPoint, let the
user create slides that may contain a collection of items such as text,
images, videos, three-dimensional objects and more. Such items can be
selected, and dragged to change their position on the slide. They can
be rotated and scaled using dedicated widgets and may even be rotated
in three-dimensions using additional input ﬁelds or symbolic input.

We explore how to use a pen alongside touch input in VR to provide
a unifying interface for 2D and 3D object manipulation of elements
on the slide. We propose to use a pen that is spatially tracked which
expands the interaction space to include not only the tablets surface
but also the space around the user. This can potentially make object
manipulations more intuitive.

Translating a selected object is supported by standard drag and drop
using a stylus or the user’s ﬁnger. An object can be rotated by rotating
the pen (as if it is attached to the pen) and it can be scaled by moving the
pen further away from or closer to the tablet (as if pulling to increase
the size). To differentiate between translation, rotation and scaling, a
ﬁnger of the non-dominant hand on the bezel of the touchscreen is used
to control the modality of the manipulation, as shown in Figure 2. We
chose a bezel-based technique, as they have already been successfully
used for mode-switches in other scenarios (e.g. [4,15]). When no ﬁnger
touches the designated area on the bezel of the screen, the stylus is used
to select an object and to drag it to a new position (Figure 2, a and e).
Touching the bezel with the non-dominant hand, while still touching
an object with the stylus, activates the 2D rotation mode. Rotating the
stylus around its axis, while still touching the surface with the tip of
the stylus, will rotate the object in the screen plane (Figure 2, b and f).
Lifting the pen away from the surface, while still touching the bezel
in rotation mode, enables the user to perform 3D rotations by rotating
the stylus in space. When performing a 3D rotation, two instances of
the object are displayed - one as a ﬂat projection on the slide, and a full
3D display of the object in the air above the screen, enabling the user
to better see the three-dimensional pose (Figures 2, c and g and 1, a).
The position of the 3D display is ﬁxed above the objects position on
the slide and its rotation is determined by the rotation of the stylus.

To scale an object, the non-dominant hand touches the bezel, while
the tip of the stylus is in the air not touching the screen. Moving the
stylus’ tip up, away from the screen, increases the scale of the object
uniformly, while bringing the tip closer to the screen reduces the its
scale (Figure 2, d and h).

The sensitivity of these manipulations can be controlled by mov-
ing the ﬁnger’s vertical location on the bezel (see Figure 2, f, g and
h). Moving the touch point up increases the control-display gain of
the rotation and the scaling, enabling larger changes with small pen
movements. Moving the ﬁnger down enables better accuracy. With this
form of object manipulation there is no need to select potentially small
scale and rotation handles on already small display sizes because of the
bimanual mode activation technique. For the case of 3D rotations in

Fig. 2. Tracking of the stylus’ six degrees of freedom in VR enables
complex manipulation of an object on the slide. The dominant hand
holding the stylus is used to drag, rotate, scale and 3D rotate the object,
while the non-dominant hand’s touch on the screen’s bezel-area (green
area on the left) modulates the interaction. Without any bezel touch (a,
e) objects are dragged by the stylus. Bezel touch and rotation of the
stylus along its axis while it touches the screen (b, f) rotates the object on
the screen’s plane. Rotating the stylus in the air while keeping the bezel
touch (c, g) rotates the objects in 3D. Finally, touching the bezel while
the stylus is in the air and moving the stylus towards or away from the
screen (d, h) decreases or increases the scale of the object uniformly.

particular, the actual 3D preview of the rotation above the object com-
bined with VR-based head movement and three-dimensional display is
something not possible in classical 2D presentation tools and has the
potential to make 3D rotations more intuitive for the user.

3.1.2 Occlusion Handling

Most presentation tools support explicit ordering of objects, where
objects of a higher layer occlude objects of lower layers. As a result,
objects may become partially or even completely hidden behind other
objects, making visual identiﬁcation difﬁcult or impossible, and increas-
ing selection difﬁculty. While some applications such as PowerPoint
display a separate list of all objects on a slide, sorted by their levels,
such a list requires the user to create a mental map, matching the lo-
cation of each item on the list to the visual objects on the slide. An
example can be seen in Figure 9, c which shows a simpliﬁed version of
the PowerPoint interface used in our study.

The Mac version of PowerPoint uses a dynamic reordering mode1
that displays objects with their layers slightly rotated, similar to Figure
9, b. In this mode, the layers can be grabbed and moved to rearrange
their order. However, with increasing number of objects this can also
become challenging as the layers can potentially overlap and make it
harder to see which object belongs to which layer, impacting selection
accuracy.

Inspired by this dynamic reordering mode, we propose to use the
space above the tablet in VR to present the object-layers to the user in a
way that facilitates assessing how the objects are ordered. Speciﬁcally,
we propose to rotate the object-layers by 90 degrees, move them up
to stand on top of the tablet and slightly separate them (see Figures
3 and 1, b). The tablet and the user’s head can be repositioned in
VR to resolve any potential occlusion issues that might arise from a
ﬁxed viewpoint. We also conducted informal experiments with further
degrees of rotation. Zero degrees of rotation (layer parallel to the
display) did not scale beyond a few layers due to the imprecise nature
of in-air selection compared to touch-based selection. When comparing
different amounts of rotations (0, 45 and 90 degrees), 90 degrees was
perceived as most comfortable and efﬁcient. Furthermore, the selection
process can be supported by showing the intersections of the layers
with the touchscreen (lines in Figures 3 and 1, b). Thus, instead of in-air
selection, layers can be selected by touching their intersection lines,
and precisely dragged to a new position (Figure 3, b). Using touch on
the tablet display allows for precise interaction, even when the number
of objects increases and the layers get closer to each other. There is also
a projection of the complete slide to the right of the object layers which
presents the current layout in 2D and facilitates the understanding of

1https://www.indezine.com/products/powerpoint/learn/shapes/dynamic-
reorder-of-overlapping-shapes-in-ppt2011-mac.html Last access September 1,
2021

Fig. 3. Reordering layered objects. The layers are displayed with the top
one on the far left of the tablet, and the bottom one on the right. On the
right end of the tablet, we display the full composited slide for reference.
a) An example of side view with a slide containing a yellow circle on top,
than a red square followed by a blue triangle. b) A user dragging the blue
triangle to place it on the top layer.

the ordering. In our implementation, the layer reordering mode is
toggled by touching the lower left corner of the bezel area that is also
used for mode switching when manipulating objects. An experiment
showing superior performance of this technique compared to baseline
Powerpoint implementations is presented in section 4.2.

3.1.3 Animations

Presentation applications such as PowerPoint show keyframes of object
animations as locations on a slide. As the area of the slide is limited
and may quickly be overloaded with information, PowerPoint displays
only the start and end keyframes and only for the currently selected
object. We facilitate visualizing and editing animations through a 3D
visualization where the extra spatial dimension represents time.

By selecting an entry in an in-air menu opened by the second pen
button, the user enters an explicit animation mode. As a base 2D
visualization, we employ a similar approach to PowerPoint, visible in
Figure 4, a, but showing all keyframes of all objects by default, instead
of showing it only for the selected objects as in PowerPoint. However,
the keyframes of certain objects can be hidden via a menu entry, which
appears next to the pen after pressing the second pen button, as can
be seen in Figure 4, b. The keyframes may be manipulated (dragged,
rotated and scaled) like any other item on the slide. To represent time,
the keyframes are numbered and connected by lines representing the
sequence of the animation (Figure 4).

By using the space above the screen, we can also visualize the time
of each keyframe by locating it at the corresponding height above the
screen. The further up the keyframe is placed, the later the associated
object state will be reached in the animation. This enables us to display
all objects and the relations between their animations. All keyframes
of an animation are connected with a colored curve that shows the path
the animation takes in time (height), as visible in Figures 4, c and 1, c.
Similar to our reordering technique, the three-dimensional time view
of the animation is toggled by touching the lower-left corner of the
tablet’s bezel area while the animation mode is active. A keyframe
can be grabbed in-air using the ﬁrst button on the stylus and moved
up and down to change the corresponding time (Figure 4, d). For
the animation interface, non-rotated layers were used (in contrast to
occlusion handling), because object manipulation on the screen is
possible while the 3D time view is active, overloading the touch inputs
that would otherwise need to be used to move rotated layers. Also,
animating depends, in contrast to pure reordering, much more on the
actual position of the objects on the slide. Therefore, we prioritized
spatial consistency over ease of selection.

Vertical timelines, displayed left of the tablet, help to indicate a
keyframe’s precise time. Applying a two ﬁnger pinch or move gesture
in the bezel area can scale or scroll through the displayed animation
along the time axis to better see parts of the animation. We display two
timelines: one that displays the global animation timeline, and the other
displays the currently viewed section of the timeline (Figure 4, c and d).
In addition to toggling visibility of animations, the menu opened by the
second pen button is also used to add and remove keyframes (Figure 4,
b). A play button that can be touched by the stylus can play or pause the
animation. The default display of the animation is on the tablet screen,
but it is also possible to render it on a virtual screen placed away from

Fig. 4. a) Animation mode showing the keyframes of two objects on the
slide. b) Menu that can be used to add or delete keyframes or make
keyframes invisible. c) 3D animation mode which shows the keyframes
as layers with their position in z-direction representing time. d) Moving
the second keyframe up so the movement from the ﬁrst to the second
will be slower.

the tablet for easier viewing.

3.2 Working Across Slides

While the tablet screen is the default surface to display and edit the
current slide, authoring presentations requires to also access further
data sources, e.g., the slides of the currently edited slide-set, fetching
resources from other presentations, or browsing external content such
as images or PDF ﬁles. In the following, we discuss means for viewing
further media concurrently to the active slide, and transferring content
between these displays and the active slide screen.

3.2.1 Slide Overview

When authoring presentations, it is a common task to copy content
between slides or to go back and forth between slides to review the
content. Especially on a small display in presentations with several
slides, it is often not possible to ﬁt all slides on the screen with sizes
that allow the user to recognize them, and the user has to scroll through
the slide overview (or slide sorter view) to ﬁnd the target slide. The
large display space in VR can be used to mitigate this problem. We use
two different ways to access other slides on the slideshow. First, while
the current slide is displayed on the tablet screen, slides before and
after the current slide are displayed to the left and right of the tablet,
see Figure 1, d. This technique is inspired by similar visualizations of
Gesslein et al. [15]. Second, a slide overview of the current slide-set
is displayed in front of the user, as can also be seen in Figure 1, d.
This overview can be scrolled and zoomed using a two-ﬁnger swipe
or pinch gesture in the border area of the tablet while gazing at the
overview area. While it would be possible to select a slide from the
overview using in-air gestures, they may be exhaustive and may not ﬁt
limited work spaces. Instead, we use eye gaze to pre-select a slide of
interest, indicated by a green frame, and while maintaining the gaze,
swipe down on the touchscreen to conﬁrm selection. The selected slide
is then displayed as the active slide in the current slide set.

3.2.2 Multiple Content Sources

When creating a presentation, it is very common to use additional
content such as text, images or videos. Just like displaying the slide
overview, the space in front of the user can be used to display a myriad
of content like images, videos or web pages that can be added to the
presentation. The user can add such content areas through a menu
opened, again, by a the second pen button. It is also possible to fade
out some of these areas while they are not used by clicking on the
corresponding toggle button with the pen (red buttons in front of the
tablet in Figure 1, d).

3.2.3 Copying Content

The user can select objects from the external content displays described
in Section 3.2.2 and copy them to the currently active slide. As the
user gazes at an element such as a slide, a semi-transparent copy of the
pen is visualized in the same relative pose to the gazed element as the

Fig. 5. a) The user is looking at an object (image) and sees the position
of the pen above the tablet as a red dot. b) The user touches the tablet
with the pen to select the image. c) The user looks back down and the
selected object is copied to the currently edited slide.

Fig. 6. a) Study setup with 6 OptiTrack cameras and two VIVE lighthouse
basestations (one is behind). OptiTrack cameras were used for the
reordering task and the usability evaluation, while lighthouse basestations
were used for the search task. b) Participant in the reordering task and
the usability evaluation. c) Participant in the tablet search task condition.

physical pen is located above the physical tablet, and the touch location
which it is hovering over is shown as a red dot (Figure 5, a). As the
user moves the stylus above the tablet screen, the corresponding copy
above the gazed element moves as well. The user can select an object
from content displays in the same way as selecting an object on the
tablet – by touching it as visualized in Figure 5, b. Looking back at
the tablet while still touching the screen with the stylus will copy the
selected object to the current slide (Figure 5, c). Similarly, the user can
copy an entire slide from the current or another slide set by touching
this slide at a position where no object is placed. Looking down will
insert this slide after the currently edited slide. Both objects and slides
can also be copied through the menu opened by the second pen button.

4 PERFORMANCE EVALUATION

The two main advantages of VR for knowledge worker tasks that
we focus on in this paper are the larger display space and the three-
dimensional viewing of 3D content. To evaluate their advantages for
presentation authoring, we chose two tasks, each representing one of
the above mentioned advantages: using the large ﬁeld of view for
displaying an overview of content and using the 3D space above the
tablet for ordering slide objects that occlude each other. These two
tasks represent issues that current users are often faced with: searching
for a slide on a limited screen and ordering occluded objects. However,
the presented concepts can also be generalized to other tasks. The
evaluation was done by conducting two separate lab-based studies
using a within-subjects design in each case.

4.1 Search Study

Virtual Reality HMDs enable the user to have a large display space
around them, which can be used to show a large number of objects, such
as slides in a slide sorter view or images during an image search, at the
same time as shown in section 3.2.2. In contrast, using a small tablet
screen may force the user to scroll to be able to view a similar amount
of items with a comparative size. We selected a search task to quantify
the possible advantages of VR HMDs compared to a tablet screen. The
participants were presented with a target image and then had to ﬁnd
and select it among 63 other images. We chose a corpus of images of
different animals. While this kind of visual stimuli are commonly used
assets in presentations, they represent challenging content due to the
amount of details in naturalistic images.

The experiment consisted of two independent variables: INTERFACE
and DIFFICULTY. We used three types of INTERFACEs. The ﬁrst, VR-
FULL used the maximum ﬁeld of view (FoV) provided by the HMD. The

Fig. 7. Conditions for the visual search task: HARD conditions, with
colored images only with a) VR-FULL, b) VR-LIMITED and c) TABLET
INTERFACE; EASY conditions with only the target image colored with d)
VR-FULL e) VR-LIMITED and f) TABLET INTERFACE. The green frame in d)
indicates the image that is currently selected via gaze.

second, VR-LIMITED, artiﬁcially limited the users’ FoV in VR to reﬂect
a similar FoV to that of the tablet’s display. And the third was TABLET,
using the actual tablet display without VR. The second independent
variable was DIFFICULTY. The levels were HARD, using the original
colored images, leading to an attentive search, and EASY, where all
images but the target image were reduced to grayscale making the
search in this condition pre-attentive, so the target could be immediately
spotted [74]. The combination of these two variables leads to six
conditions which are depicted in Figure 7. The dependent variables
measured in this experiment were task completion time, number of
errors, usability (System Usability Scale, SUS) [5], workload (NASA
TLX unweighted version) [21], and simulator sickness (SSQ) [30].

4.1.1 Participants
Twenty participants took part in this study (6 female, 14 male), with a
mean age of 28.85 (SD = 5.2). Five participants wore glasses during
the experiment and all but one had at least some prior VR experience.

4.1.2 Apparatus
In all conditions the participants were presented with a set of images.
Each image was set to occupy approximately 8 degrees horizontally of
the participant’s ﬁeld of view, both displayed on the tablet’s screen or
HMD. While it is known that image size impacts image search [28],
we empirically determined this size to be both legible and selectable
in VR and on the tablet. With an approximate distance to the tablet of
40 cm this resulted in an image width of 5.8 cm on the tablet. Sixty-four
images were arranged in four columns, such that 16 images were fully
visible without a need for scrolling.

In both VR conditions (VR-FULL and VR-LIMITED) the images were
placed on a sphere around the user at a distance of 75 cm which resem-
bles the focal distance of the HTC Vive HMD, resulting in an image
width of 10 cm. In the VR conditions, the images were arranged in 8
columns covering 65 degrees horizontally and 45 degrees vertically to
enable participants to comfortably reach all images with eye-gaze and
only slight head movements. The ﬁeld of view of the VR-LIMITED con-
dition was artiﬁcially limited using black planes in all four directions,
resulting in a ﬁeld of view of about 36 x 24 degrees, resembling the ﬁeld
of view of the tablet (26 x 17.5 cm at a distance of 40 cm). Both the
tablet and the VR applications were implemented using Unity 2019.4.
The TABLET conditions were performed on a Microsoft Surface Pro 4
as shown in Figure 6, c. For the VR conditions, we used a HTC Vive
Pro Eye, which provides built-in eye-tracking and two lighthouse base
stations (Figure 6, a). We combined it with the Microsoft Surface to
enable touch input. In contrast, to the further studies, the Optitrack
system depicted in 6, a, was not used in this study.

4.1.3 Procedure
The study started by asking the participants to sign a consent form and
ﬁll out a demographic questionnaire. The order of the six conditions
was balanced using a balanced Latin square. In each condition the
participants were ﬁrst presented with the target image, either on the
tablet screen in the TABLET conditions or in the air in front of the user

Search Task - Subjective Ratings

I
D
I × D

d f1
2
1
2

d f2
38
19
38

TS-SS
F
3.59
7.23
2.43

p
0.04
0.01
0.1

η2
p
0.16
0.28
0.11

d f1
2
1
2

d f2
38
19
38

SUS
F
0.24
8.25
0.7

p
0.79
0.01
0.5

η2
p
0.01
0.3
0.04

I
D
I × D

d f1
2
1
2

Overall task load

d f2
38
19
38

F
0.42
40.51
1.98

p
0.66
< 0.001
0.15

η2
p
0.02
0.68
0.09

Search Task - Performance Data

d f1
2
1
2

d f2
38
19
38

Task Completion Time
p
< .001
< .001
< .001

I
D
I × D

F
32.3
782.5
33.3

η2
p
.63
.98
.64
Table 1. RM-ANOVA results for the search task. Gray rows show signif-
icant ﬁndings. I = INTERFACE, D = DIFFICULTY. TS-SS: Total Severity
Dimension of the Simulator Sickness Questionnaire. SUS: System Us-
ability Scale. d f1 = d fe f f ect and d f2 = d ferror.

p
< .001
< .001
.08

Errors
F
33.96
22.27
2.74

d f1
2
1
2

d f2
38
19
38

η2
p
.64
.54
.13

conditions was rather low and higher error rates in the VR conditions
could be explained by the eye-gaze technique, relying on off-the shelf
gaze-tracking in the HTC Vive Pro Eye. It is not always perfectly
accurate and if the user is not fully concentrated, the gaze might lose
the target in the moment of the conﬁrmation, resulting in a wrong se-
lection. However, there are possibilities for improvement like a delay
before switching the selected images or a dwell time. More errors in
the HARD conditions can be explained by user mistakes, for example
when looking for an orange ﬁsh they chose the yellow one.

Analyzing the results from the total severity dimension of the sim-
ulator sickness questionnaire indicated a signiﬁcant inﬂuence of IN-
TERFACE and DIFFICULTY. However, pairwise comparisons showed
no signiﬁcant differences. Unsurprisingly, a signiﬁcant inﬂuence of
DIFFICULTY on the usability score was detected in such a way that the
HARD (M = 85.88, SD = 13.07) conditions had a signiﬁcantly lower
usability than the EASY (M = 89.75, SD = 10.69) conditions. However,
no signiﬁcant inﬂuence of INTERFACE could be detected, regarding us-
ability. The DIFFICULTY also had a signiﬁcant inﬂuence on the overall
task load, such that the task load was signiﬁcantly higher for the HARD
conditions (M = 29.65, SD = 16.54) than for the EASY conditions
(M = 17.81, SD = 12.73). This makes sense, because compared to the
HARD conditions, in the EASY conditions less mental effort is required
to ﬁnd the target. Again, no signiﬁcant inﬂuence of INTERFACE could
be detected.

4.2 Reordering Study

To quantify the beneﬁts of a three-dimensional visualization of lay-
ered information, we used the reordering task to compare the standard
POWERPOINT tool for reordering object layers with the DYNAMIC
REORDERING tool implemented in PowerPoint for Mac and our VR
tool. The participants were presented with a number of objects on a
slide. The displayed objects always included one yellow square and
one red circle while the remaining objects were blue triangles. The task
for the participants was to bring the red circle directly in front of the
yellow square, by moving the corresponding layers to the front or to the
back. To reproduce many different scenarios, the objects were placed
with different amounts of overlap, the number of layers was varied and
the target object was placed at different depths.

This experiment had one independent variable, INTERFACE, with
three levels. First, the VR condition, a simpliﬁed version of the layer
visualization and manipulation interface described in section 3.1.2 dis-
plays the layers in the air above the tablet (Figure 9, a). Second, the
non-VR DYNAMIC REORDERING technique available in PowerPoint
for Mac was used (Figure 9, b). Third, the non-VR baseline technique
available in POWERPOINT was used. It presents the layers in a list next
to the slide, where they can be dragged up and down, and buttons to
bring a layer to the front or back (Figure 9, c). The dependent variables
for this experiment were task completion time, usability (System Us-
ability Scale, SUS) [5], workload by using NASA TLX (unweighted
version) [21], simulator sickness (SSQ) [30] and user preferences.

Fig. 8. Task completion times for the six conditions, which is signiﬁcantly
inﬂuenced by the interface in the easy conditions, but not in the hard.

in VR. Upon touching the tablet’s screen the target image vanished and
the image search started. In the TABLET condition, the participants had
to scroll through the images by touching and dragging on the display.
When they found the target, they selected it by touching it. In the VR
(VR-FULL and VR-LIMITED) conditions, the participants had to search
for the target by moving their eyes and head and select the target by eye-
gaze and conﬁrm the selection by taping anywhere on the tablet. Prior
to the start of the VR conditions, users conducted eye-gaze calibration
using the built-in calibration routine of the HTC Vive Pro Eye. After
selecting an image, the next target image was displayed.
In each
condition, participants had to ﬁnd 30 images, which were always the
same but in randomized order and positions while ensuring that targets
are positioned in all regions. After completing a condition, participants
answered the simulator sickness questionnaire [30], the system usability
scale questionnaire [5] and the NASA task load index [21]. Also, we
recorded the task completion times and errors for each task. On average,
it took 45 minutes to complete this experiment.

4.1.4 Results

Repeated measures analysis of variance (RM-ANOVA) was used to
analyze task completion times, which were non-normal and therefore
log transformed. For multiple comparisons Bonferroni adjustments
were used at an initial signiﬁcance level of α = 0.05. Aligned Rank
Transform [73] was used for subjective data and errors that are not
normally distributed (or could not be normalized using log transform).
The main results are displayed in Table 1.

For each participant an average task completion was computed from
the 30 tasks, including trials with errors. Analyzing the task completion
times indicated signiﬁcant simple main effects of INTERFACE and DIF-
FICULTY on task completion time and that there were also signiﬁcant
interaction effects. Speciﬁcally, the VR-FULL (M = 4.7s, SD = 3.52)
conditions were signiﬁcantly faster than the VR-LIMITED (M = 5.47s,
SD = 3.34) and TABLET (M = 5.75s, SD = 2.56) conditions. Also,
a signiﬁcant difference could be found between VR-LIMITED and
TABLET. As expected, the EASY (M = 2.65s, SD = 1.02) conditions
were signiﬁcantly faster than the HARD (M = 7.97s, SD = 2.2) condi-
tions.

The interaction effect is visible in Figure 8. Post-hoc comparisons
showed that for the EASY conditions, VR-FULL (M = 1.71s, SD = 0.61)
is signiﬁcantly faster than VR-LIMITED (M = 2.7s, SD = 0.8) and
both VR methods are signiﬁcantly faster than TABLET (M = 3.53s,
SD = 0.66). This suggests that the wider FoV makes the search faster,
as indicated by prior work [53]. The different input techniques (gaze vs.
scrolling) are likely to contribute to difference between the VR-LIMITED
and TABLET. However, for the HARD conditions, no signiﬁcant differ-
ences between the INTERFACES were indicated. This could suggest that
the larger FoV does not necessarily provide an advantage in a search
were all possible targets have to be looked at in detail.

INTERFACE also signiﬁcantly inﬂuenced the number of errors.
Speciﬁcally, participants made signiﬁcantly less errors in the TABLET
(M = 0.8, SD = 1.11) conditions compared to both the VR-FULL
(M = 5.08, SD = 4.65) and VR-LIMITED (M = 3.58, SD = 3.64) con-
ditions. There was no signiﬁcant difference between VR-LIMITED and
VR-FULL. Also, the analysis showed that DIFFICULTY had a signiﬁcant
effect on the number of errors in such a way that the EASY conditions
(M = 2.63, SD = 3.81) resulted in signiﬁcantly less errors than the
HARD conditions (M = 3.67, SD = 3.9). However, the error rate in all

Reordering Task
d f1
2
2
2

F
71.8
0.68
22.8

d f2
26
26
26

p
< 0.001
0.52
< 0.001

2

26

18.98

< 0.001

η2
p
0.85
0.05
0.64

0.59

TCT
TS-SS
SUS
Overall task
load

Fig. 9. Conditions in the reordering task: a) our VR technique. b) a re-
implementation of the DYNAMIC REORDERING technique from PowerPoint
for Mac c) a re-implementation of the standard POWERPOINT technique.

4.2.1 Participants

Fourteen volunteers (2 female, 12 male, mean age 27.1, SD = 4.2) took
part in the study. All but one had prior VR experience and normal or
corrected to normal vision.

4.2.2 Apparatus

We implemented all three conditions using Unity 2019.4, in order to
facilitate the task design, to get consistent logging data for all three
techniques and to avoid potential confounds due to different input and
output devices. In the VR condition, the object layers were rotated
90 degrees and lifted up above the tablet, so that the edge of each
layer just touched the display. Therefore, it is possible to move a layer
by touching the intersection on the tablet and moving it around. For
the DYNAMIC REORDERING condition, the reordering feature from
PowerPoint for Mac was re-implemented. The object layers were
presented in 3D on the tablet and could be moved by touching a layer
and dragging it to its new position. For the POWERPOINT condition,
we implemented an application resembling the PowerPoint interface
for rearranging object layers. Re-implementing this interface allowed
us also to exclude potential confounds, such as visual noise added by
extraneous buttons and submenus not relevant for the experiment. The
two interfaces showed the same content as the original would, just
leaving blank functionalities that are not used in the study.

For the VR condition we used a HTC Vive Pro Eye in combination
with a Microsoft Surface Pro 4 tablet and an AZLink stylus pen. The
pen, the tablet and the HMD were tracked via an OptiTrack motion-
tracking system, using 4 Optitrack Prime 13 cameras placed above the
user and two Prime 13W wide angle cameras placed closer to the user to
support tracking for the stylus which was equiped with smaller markers.
The VIVE-based HMD tracking was overriden to prevent interference
between VIVE lighthouses and OptiTrack cameras. Optitrack was
chosen for the study setup, because it provides very accurate tracking
for the HMD, tablet and pen. Using Optitrack, we could utilize a
normal lightweight pen in contrast to rather heavy trackable pens that
are currently available. In future work this could also be implemented as
a mobile prototype and be evaluated in-the-wild. Both the pen and the
tablet were visualized in the virtual environment using 3D models of the
real objects. The commodity PC running the VR application received
touch inputs remotely from the Microsoft Surface via UDP. The two
non-VR conditions (DYNAMIC REORDERING and POWERPOINT) were
run directly on the Microsoft Surface Pro 4 tablet. The study setup can
be seen in Figure 6, a and b.

4.2.3 Procedure

Upon arrival, the participants were ﬁrst asked to sign a consent form and
to ﬁll out a demographic questionnaire. Then the participants started
with one of the three conditions. The condition order was counterbal-
anced between the subjects to avoid effects due to fatigue or learning.
With 14 participants, it was not possible to exactly counterbalance,
but no signiﬁcant order effects were detected. For each conditions the
task was repeated 32 times. After each condition, participants com-
pleted the Simulator Sickness questionnaire [30], the System Usability
Scale [5] and the NASA TLX questionnaire [21]. After all conditions
were successfully completed, participants ﬁlled out a questionnaire
about their preferred technique. We then conducted a semi-structured
interview to understand their choice and give them the opportunity to
give comments. In addition, we recorded the task completion times for

Table 2. RM-ANOVA results for the reordering task. Gray rows show
signiﬁcant ﬁndings. TCT: Task Completion Time. TS-SS: Total Sever-
ity Dimension of the Simulator Sickness Questionnaire. SUS: System
Usability Scale d f1 = d fe f f ect and d f2 = d ferror.

all conditions. On average, this study took 30 minutes. Volunteers did
not receive a compensation for participating.

4.2.4 Results

Repeated measures analysis of variance (RM-ANOVA) was used to
analyze task completion times, which were non-normal and therefore
log transformed. For multiple comparisons Bonferroni adjustments
were used at an initial signiﬁcance level of α = 0.05. Aligned Rank
Transform [73] was used for subjective data and errors that are not
normally distributed (or could not be normalized using log transform).
The main results of the reordering task are shown in Table 2. Due to
logging errors, we lost 12 samples for the task completion time from
the ﬁrst participant in the reordering task, so the mean task comple-
tion time for this participant only consisted of 20 samples. Statistical
signiﬁcance tests showed that the task completion time was signiﬁ-
cantly inﬂuenced by INTERFACE in such a way that the VR method
(M = 4.51s, SD = 2.46) was signiﬁcantly faster than both the DYNAMIC
REORDERING (M = 14.5s, SD = 7.4) and POWERPOINT (M = 16.1s,
SD = 6.39) methods. But no signiﬁcant difference between DYNAMIC
REORDERING and POWERPOINT was detected.

The NASA task load index also showed a signiﬁcant inﬂuence of the
INTERFACE on the overall task load. Pairwise comparisons showed that
the VR (M = 20.1, SD = 11.46) interface resulted in a signiﬁcantly
lower task load than the DYNAMIC REORDERING (M = 49.29, SD =
22.15) interface. But no signiﬁcant difference was detected between
VR and POWERPOINT (M = 34.23, SD = 21.56) or between DYNAMIC
REORDERING and POWERPOINT. No signiﬁcant differences between
the INTERFACEs regarding simulator sickness were detected.

The usability was also signiﬁcantly inﬂuenced by the INTERFACE in
such a way that the usability of the VR method (M = 89.11, SD = 7.76)
was signiﬁcantly higher than for the DYNAMIC REORDERING (M =
53.39, SD = 21.63) and POWERPOINT (M = 69.64, SD = 18.76) meth-
ods. Again, no signiﬁcant difference between DYNAMIC REORDERING
and POWERPOINT could be detected.

All but one participant preferred the VR method. One preferred the
DYNAMIC REORDERING method. Six participants that preferred the
VR techniques said that ”it results in the fastest overview” (P1, P2, P3,
P4, P7, P8) and ”if something was occluded you could move your head”
(P2, P7, P14). Five also mentioned that ”it was easy to select the layers”
(P1, P4, P6, P8, P12), three that ”the interaction was more convenient”
(P8, P9, P12,) and three that ”it was easier and faster” (P1, P2, P11).
Three participants also complained about the DYNAMIC REORDERING
condition. One said ”I was confused which slide is selected” (P1), one
that ”it was hard to identify the layer” (P2) and one that ”I often selected
the wrong layer” (P6). Another one mentioned that ”it is a problem
that it is only displayed in 2D, so it is hard to see where to tap” (P12).
One participant proposed to highlight the slides when touching them
and selecting them by for example pressing the pens button. This is in
line with our observations that the two baseline techniques required a
lot more trial and error to select the right objects, as they were partially
occluded and it was hard to see which object belongs to which layer. In
contrast, the three-dimensional view and head movement in VR helps
with assigning objects to layers.

5 USABILITY EVALUATION
In addition to the performance evaluation that we presented in Section
4, we conducted a usability study on our prototype, which consists
of the techniques presented in Section 3. Our objective was to ﬁnd
out if they are easy to understand and to use for regular users. Also,

through participant’s comments we gained valuable insights into how
to improve our prototype. This study was divided into four parts repre-
senting the techniques from section 3—object manipulation, handling
occlusions, animations and working across slides. The three concepts
(slide overview, multiple content sources, copying content) on working
across slides, as presented in section 3, were presented to the partici-
pants jointly as one coherent workﬂow.

5.1 Participants

Eighteen participants (5 female, 13 male) took part in this study. Their
mean age was 28.94 years (SD = 5.3). All had normal or corrected to
normal vision and all but one had prior VR experience.

Fig. 10. Answers from questionnaire about Ease of use, utility and
enjoyment for the four concept with 7 being the highest and 1 the lowest
possible score. ObjMan = Object Manipulation, Anim = Animation, Occ =
Occlusion Handling, Across = Across Slides

5.2 Apparatus

The VR applications described in this paper were implemented us-
ing the Unity 2019.4. We used a HTC Vive Pro Eye, which provides
built-in eye tracking in combination with a Microsoft Surface Pro 4
tablet and an AZLink stylus pen. For the user study, the pen, tablet and
HMD were tracked via an OptiTrack motion-tracking system with 6
Optitrack Prime 13 cameras (Figure 6, a and b), since pen tracking via
VIVE-trackers or other VIVE-based tracking devices was unfeasible
due to pen weight concerns. Therefore, the VIVE-based HMD tracking
was overriden to prevent interference between VIVE lighthouses and
OptiTrack cameras. Pen, tablet and the two ﬁngers of the non-dominant
hand, that were used for pinching, were visualized in the virtual envi-
ronment, using 3D models of the real pen and tablet and spheres for the
ﬁngers. The tablet was connected to the PC running the VR application
which received the touch inputs via UDP.

5.3 Procedure

First, participants were asked to sign a consent form and ﬁll out a
demographic questionnaire. Then the eye-tracking was calibrated using
the built-in routine of the HTC Vive Pro Eye. All participants started
with object manipulation, because it is a prerequisite for the animation
techniques. The order of the remaining parts - handling occlusions,
animations and working across slides - was counterbalanced. For each
concept, the participants were walked through the possibilities and
interaction techniques that are provided in the prototype. Then they
had time to try out the technique as long as they liked. On average
participants spent about 5 minutes exploring each technique. Following
each interaction concept, the participants orally graded three statements
(while wearing the HMD) by giving a score on a seven-item Likert
scale, regarding ease of use (”I would ﬁnd the application easy to use”),
utility (”I would ﬁnd the application to be useful”) and enjoyment (”I
would have fun interacting with the application”) (1: totally disagree,
7: totally agree). Also they were encouraged to think out loud about
their experience and make suggestions. At the end, when all concepts
were explored, the participants completed the Simulator Sickness ques-
tionnaire [30], the System Usability Scale [5] and the NASA TLX
questionnaire [21]. Also, they were asked to rank the four techniques
by popularity and we conducted a semi-structured interview to give the
participants a chance to further express their thoughts. The whole study
took about 45 minutes on average.

5.4 Results

The results from the three questions on ease of use, utility and enjoy-
ment that were asked after each technique are presented in Figure 10.
It can be seen that the ratings for utility and enjoyment are high with
more than 75% of the answers being at least a ﬁve on the seven-item
Likert scale. Even tough the participants had to learn a lot in a short
time, the ease of use rating was also high.

After each technique was presented, participants were asked to ﬁll
out three questionnaires (SUS, NASA TLX, SSQ). The system usability
questionnaire reported an average usability score of 71.53 (SD = 20.13)
which indicates that our prototype has an average usability. This is an
acceptable result for a prototype. The average total severity dimension
of the Simulator Sickness Questionnaire was 12.26 (SD = 17.34) (Nau-
sea: M = 5.83, SD = 13.15, Oculo-motor: M = 11.79, SD = 19.49,

Fig. 11. Left: percentage of people ranking the technique either 1st, 2nd,
3rd or 4th. Right: boxplot of the ranking. ObjMan = Object Manipulation,
Anim = Animation, Occ = Occlusion handling, Across = Across Slides

Disorientation: M = 15.47, SD = 15.76) , which means the participants
were not suffering from severe simulator sickness.

The average overall task load measured by the NASA task load index
was 25.54 (SD = 11.18), indicating that they were not overwhelmed
(due to technical problems, we lost the data for TLX of one participant
and computed the average with one less value).

Participants were also asked to rank the techniques based on prefer-
ence, see Figure 11. There is no clear trend visible, but animation and
occlusion handling seem to be more popular than the object manipula-
tion technique or working across slides.

Participants also had the chance to comment on the techniques. For
the object manipulation technique, four participants mentioned that it
requires some practice and P8 suggested an additional explicit mode
switch. P2 would rather do the scaling in 2D, as it only affects two
dimensions. P7 thought the 3D rotation was intuitive, and P15 said it
was very useful, because ”you can better see what happens”.

Regarding the animation, two participants mentioned that it is easier
and more intuitive than in a standard slide authoring application. They
also suggested additional features such as the snapping of two frames
(P5), other types of animation (P8) and the possibility to exactly enter
time (P16). However, P2 and P12 also mentioned that a certain amount
of practice time is needed.

P1 and P17 thought that the occlusion handling method makes it
easier and more natural to ﬁnd a speciﬁc layer. P8 suggested to add the
possibility to see the ﬁnal image not only on the right but also on the
tablets surface or standing on the upper edge. To be more consistent
with the animation mode, P4 suggested to move the layers in-air and
P15 was concerned it could get confusing with a very large number of
objects, but for standard slides this should not be an issue.

When working across slides, participants also mentioned that many
new techniques need to be remembered (P1, P2, P12). Only one
participant thought that too much content is displayed that confuses
her. P2 liked the possibility to make the content in front of the user
invisible and P16 would also like this feature for the slides presented
to the sides of the tablet. P3 suggested to add the possibility to bring
resources (like a PDF) closer, so that it is easier to read it. P15 found it
challenging to hit the bezel while not looking at it. P10 mentioned to
feel some motion-sickness when the content was moving around (when
adding or closing content).

Participants were also asked to comment on the overall system and
explained their choices for the rating. Four people said that they liked
the system. P2, P10 and P14 said they had fun using the system. P6

and P17 could not imagine themselves using a VR HMD for work, but
nevertheless, they liked the system. Five participants mentioned that
they would need to practice to get used to all techniques. However, P11
mentioned that ”with a little practice it would be faster than in standard
PowerPoint”. P8 suggested to display a cheat sheet somewhere, as
VR provides a lot of space. Three participants mentioned that 3D
view is useful and ”makes things easier to see than in 2D” (P10).
Four people mentioned the system seems viable and P14 said ”I could
imagine to use this system”. P15 and P17 liked the display of additional
information and the increased workspace. P14 was interested in seeing
how these interaction techniques could be integrated in a standard
desktop setup. P4, P17 and P18 especially liked the occlusion handling
technique, because ”it saves us a lot of work” (P4) and ”it is what I
always missed in PowerPoint” (P17). P14 and P15 especially liked the
animation, because ”it was fun” (P14) and ”implemented well and easy
to understand” (P15).

6 DISCUSSION

As people work more remotely, from touchdown ofﬁces, on the go or
from home, the importance of mobility and privacy increases. This
paper, and other prior works [4, 15] show that this mode of work can
be more efﬁcient than the current use of 2D displays, as HMDs can ﬁll
the user’s full ﬁeld of view with potentially very large virtual displays
and show world grounded stereoscopic information, which the user can
interact with directly.

To examine the effect of such a display on presentation authoring
applications, we focused on four speciﬁc techniques in this paper: The
use of the large ﬁeld of view for selection and interactions with content
catalogs, use of 3D visualization for animation, handling object order-
ing and occlusions, and tracking the user’s stylus in 6 DOF, enabling
complex manipulations of objects.

We evaluated the performance of two techniques utilizing the larger
display space and the 3D view provided by VR. First, we examined the
effect of the extended display space where we expected that a larger
ﬁeld of view would speed up a visual search task as indicated by prior
work [53]. The results of the search study showed that this was true for
the EASY conditions where the target was identiﬁable pre-attentively.
Yet, contrary to our expectations, as matching required more mental
effort per item, the ﬁeld of view did not seem to signiﬁcantly inﬂuence
search time. Nevertheless, we argue that a wider ﬁeld of view is
desirable, as it performs at least as good or better when compared to a
smaller ﬁeld of view when identifying targets and has the potential to
skip interactions for switching or toggling displayed information.

Our second performance study showed that VR-based 3D visualiza-
tion for resolving occlusion outperformed two baseline techniques in
terms of speed and usability. Follow-up interviews suggested that the
technique was well received, as ”easy”, ”fast” and ”provided a good
overview”. Similar techniques for embedding 2D data in 3D could
also be used in other applications from animation (representing time
as the third dimension) or image editing (showing semantic layers and
versioning in space) to displaying alignment constraints and relations
between slide objects and more. We showed the beneﬁts of VR as a
work space, and hope to encourage more work in this direction.

Additionally, a usability study in a walk-up-and-use scenario was
conducted to evaluate these techniques. Subjects were confronted with
a large amount of new interaction techniques and input modalities
that differ from traditional touch input techniques in several ways. In
spite of this, participants gave positive ratings for ease of use, which
is also reﬂected in the results of the system usability scale indicating
an average usability of 71.53. It showed the feasibility of our approach
and the techniques were rated usable and enjoyable by participants.
Many participants also expressed the feeling that their level of comfort
with the techniques would improve with more training, indicating their
unfamiliarity due to the walk-up-and-use scenario but general level of
comfort with the techniques. Future work could look at ways to further
improve the techniques to be even more intuitive, discoverable, and
require less setup and explanation to use or to determine the actual
learning curve of the techniques. No severe levels of simulator sickness
were measured among participants and their perceived task load was

also not high, indicating that they had no major issues with the basic
functionality of our prototype. We hope that these ﬁndings are precur-
sors for supporting further tasks in presentation authoring beyond the
tested ones. Our goal was to gain initial insights into the usability, yet it
will be important for future work to evaluate them in a more extensive
way.

In this paper we focus on the graphic organization of slides. Text
entry is an important issue for presentation authoring, yet we did not
address it in this paper, as prior works has already addressed typing in
VR, e.g., [13, 19]. These techniques can be used in conjunction with
the presented techniques, for example, after entering the text, it could
be manipulated like any other object (translated, rotated, scaled).

One of our main objectives in designing interactions was to use small
hand movements, to allow for longer interaction times without fatigue.
One option to further extend the input space but still keep small hand
movements would be to remap physical pen and ﬁnger movements
using a C-D ratio and visulizing copies of the pen and ﬁngers (similar
to our technique used when working across slides), for example to
allow users to reach higher times in the 3D time view of the animation
mode without scrolling through the timeline.

Another outcome from this work is the use of eye-gaze along with
retargeting the input of a stylus on the tablet. The use of such techniques
allows the user to interact with a very large display space while working
in a limited cluttered physical environment, as their hands are located on
a small tablet screen. This skips the need to physically reach displayed
content sources.

Finally, this paper has only looked at the authoring side of pre-
sentation applications. There is another aspect of these applications
which is the presentation process. While being out of the scope of
this work, there are many similar advancements that can come to play
while presenting, from the use of the large display space to presenting
information useful for the presenter such as upcoming slides or notes.
Also the presented techniques could be used to control the presentations
such as quickly switching between slides that are not neighbours.

7 CONCLUSIONS AND FUTURE WORK

In this work we prototyped an experience called PoVRPoint: a set of
tools that couple pen-, touch- and gaze-based authoring of presenta-
tions on mobile devices with the interaction possibilities afforded by
VR. We studied the utility of extended display space in VR for tasks
such as visual search, spatial manipulation of shapes, animations and
ordering overlapping shapes. The results showed that VR can improve
usability and performance of common authoring tasks and are liked by
participants.

We see multiple avenues of future work. First, we aim at investigat-
ing the knowledge worker experience within the ofﬁce of the future
in VR [18], with multiple applications in use at any given time. To
achieve this, we plan to explore techniques for transferring content
across applications. Second, we want to explore how to expand the
knowledge worker experience in VR by opportunistically leveraging
available physical objects in-situ, such as a tray surface or an armrest
in an airplane. It has been shown that VR can improve the usability
and performance of editing presentations during the limited time period
of the study. Still, future work should evaluate the effects of working
in VR for prolonged time periods. Finally, we would like to extend
the work into a collaborative one, and see how we can further use the
advantages of VR to create experiences with awareness of the context
and the remote participants. For example, VR can enable remote col-
laboration that represents both the shared document (the task space), as
well as a representation of collaborators with a reference space to show
where they are pointing in relation to the shared document, and private
spaces [65].

REFERENCES
[1] Microsoft research mt. rogers. https://www.microsoft.com/en-us/
research/project/mt-rogers-head-worn-virtual-displays/,
2021.

[2] R. Arora, R. H. Kazi, D. M. Kaufman, W. Li, and K. Singh. Magicalhands:
Mid-air hand gestures for animating in vr. In Proc. of the 32nd Annual

ACM Symp. on User Interface Software and Technology, pp. 463–477,
2019.

[3] N. Ashtari, A. Bunt, J. McGrenere, M. Nebeling, and P. K. Chilana.
Creating augmented and virtual reality applications: Current practices,
challenges, and opportunities. In Proc. of the 2020 CHI Conf. on Human
Factors in Computing Systems, pp. 1–13, 2020.

[4] V. Biener, D. Schneider, T. Gesslein, A. Otte, B. Kuth, P. O. Kristensson,
E. Ofek, M. Pahud, and J. Grubert. Breaking the screen: Interaction across
touchscreen boundaries in virtual reality for mobile knowledge workers.
IEEE Trans. on Visualization and Computer Graphics, 26(12):3490–3502,
2020. doi: 10.1109/TVCG.2020.3023567

[5] J. Brooke. Sus: a “quick and dirty’usability. Usability evaluation in

industry, p. 189, 1996.

[6] W. B¨uschel, J. Chen, R. Dachselt, S. Drucker, T. Dwyer, C. G¨org, T. Isen-
berg, A. Kerren, C. North, and W. Stuerzlinger. Interaction for immersive
analytics. In Immersive Analytics, pp. 95–138. Springer, 2018.

[7] D. Cami, F. Matulic, R. G. Calland, B. Vogel, and D. Vogel. Unimanual
pen+ touch input using variations of precision grip postures. In Proc. of
the 31st Annual ACM Symp. on User Interface Software and Technology,
pp. 825–837, 2018.

[8] A. Cannav`o, C. Demartini, L. Morra, and F. Lamberti. Immersive virtual
reality-based interfaces for character animation. IEEE Access, 7:125463–
125480, 2019.

[9] D. Checa and A. Bustillo. A review of immersive virtual reality serious
games to enhance learning and training. Multimedia Tools and Applica-
tions, 79(9):5501–5527, 2020.

[10] X. Chen, J. Schwarz, C. Harrison, J. Mankoff, and S. E. Hudson. Air+
touch: interweaving touch & in-air gestures. In Proc. of the 27th annual
ACM symp. on User interface software and technology, pp. 519–525, 2014.
[11] M. F. Deering. Holosketch: a virtual reality sketching/animation tool.
ACM Trans. on Computer-Human Interaction (TOCHI), 2(3):220–238,
1995.

[12] T. Drey, J. Gugenheimer, J. Karlbauer, M. Milo, and E. Rukzio. Vrsketchin:
Exploring the design space of pen and tablet interaction for 3d sketching
in virtual reality. In Proc. of the 2020 CHI Conf. on Human Factors in
Computing Systems, pp. 1–14, 2020.

[13] T. J. Dube and A. S. Arif. Text entry in virtual reality: A comprehensive
review of the literature. In M. Kurosu, ed., Human-Computer Interac-
tion. Recognition and Interaction Technologies, pp. 419–437. Springer
International Publishing, Cham, 2019.

[14] N. Fellion, T. Pietrzak, and A. Girouard. Flexstylus: Leveraging bend
input for pen interaction. In Proc. of the 30th Annual ACM Symp. on User
Interface Software and Technology, UIST ’17, p. 375–385. Association for
Computing Machinery, New York, NY, USA, 2017. doi: 10.1145/3126594
.3126597

[15] T. Gesslein, V. Biener, P. Gagel, D. Schneider, E. Ofek, M. Pahud, P. O.
Kristensson, and J. Grubert. Pen-based interaction with spreadsheets in
mobile virtual reality. In 2020 IEEE Int. Symp. on Mixed and Augmented
Reality (ISMAR), 2020.

[16] R. Grasset, A. Duenser, H. Seichter, and M. Billinghurst. The mixed
reality book: a new multimedia reading experience. In CHI’07 extended
abstracts on Human factors in computing systems, pp. 1953–1958. ACM,
2007.

[17] T. Grossman, K. Hinckley, P. Baudisch, M. Agrawala, and R. Balakrishnan.
Hover widgets: Using the tracking state to extend the capabilities of pen-
operated devices. In Proc. of the SIGCHI Conf. on Human Factors in
Computing Systems, CHI ’06, p. 861–870. Association for Computing
Machinery, New York, NY, USA, 2006. doi: 10.1145/1124772.1124898
[18] J. Grubert, E. Ofek, M. Pahud, P. O. Kristensson, F. Steinicke, and C. San-
dor. The ofﬁce of the future: Virtual, portable, and global. IEEE computer
graphics and applications, 38(6):125–133, 2018.

[19] J. Grubert, L. Witzani, E. Ofek, M. Pahud, M. Kranz, and P. O. Kristensson.
Text entry in immersive head-mounted display-based virtual reality using
standard keyboards. In 2018 IEEE Conf. on Virtual Reality and 3D User
Interfaces (VR), pp. 159–166. IEEE, 2018.

[20] J. Guo, D. Weng, Z. Zhang, H. Jiang, Y. Liu, Y. Wang, and H. B.-L.
Duh. Mixed reality ofﬁce system based on maslow’s hierarchy of needs:
Towards the long-term immersion in virtual environments. In 2019 IEEE
Int. Symp. on Mixed and Augmented Reality (ISMAR), pp. 224–235. IEEE,
2019.

[21] S. G. Hart and L. E. Staveland. Development of nasa-tlx (task load index):
Results of empirical and theoretical research. In Advances in psychology,
vol. 52, pp. 139–183. Elsevier, 1988.

[22] O. Hilliges, S. Izadi, A. Wilson, S. Hodges, A. Garcia-Mendoza, and
A. Butz.
Interactions in the air: Adding further depth to interactive
tabletops. In UIST ’09 Proc. of the 22nd annual ACM symp. on User
interface software and technology, pp. 139–148. ACM, October 2009.

[23] J. D. Hincapi´e-Ramos, X. Guo, P. Moghadasian, and P. Irani. Consumed
endurance: a metric to quantify arm fatigue of mid-air interactions. In
Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pp. 1063–1072, 2014.

[24] K. Hinckley, X. A. Chen, and H. Benko. Motion and context sensing
techniques for pen computing. In Proceedings of Graphics Interface 2013,
GI ’13, p. 71–78. Canadian Information Processing Society, CAN, 2013.
[25] K. Hinckley, S. Heo, M. Pahud, C. Holz, H. Benko, A. Sellen, R. Banks,
K. O’Hara, G. Smyth, and B. Buxton. Pre-touch sensing for mobile
interaction. In CHI ’16 Proc. of the 2016 CHI Conference on Human
Factors in Computing Systems, pp. 2869–2881. ACM, May 2016.
[26] K. Hinckley, K. Yatani, M. Pahud, N. Coddington, J. Rodenhouse, A. Wil-
son, H. Benko, and B. Buxton. Pen+ touch= new tools. In Proc. of the
23nd annual ACM symp. on User interface software and technology, pp.
27–36, 2010.

[27] T. Hirzle, J. Gugenheimer, F. Geiselhart, A. Bulling, and E. Rukzio. A
design space for gaze interaction on head-mounted displays. In Proc. of
the 2019 CHI Conf. on Human Factors in Computing Systems, p. 625.
ACM, 2019.

[28] W. H¨urst, C. G. Snoek, W.-J. Spoel, and M. Tomin. Size matters! how
thumbnail number, size, and motion inﬂuence mobile video retrieval. In
Int. Conf. on MultiMedia Modeling, pp. 230–240. Springer, 2011.
[29] D. Kami´nska, T. Sapi´nski, S. Wiak, T. Tikk, R. E. Haamer, E. Avots,
A. Helmi, C. Ozcinar, and G. Anbarjafari. Virtual reality and its applica-
tions in education: Survey. Information, 10(10):318, 2019.

[30] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal. Simulator
sickness questionnaire: An enhanced method for quantifying simulator
sickness. The int. journal of aviation psychology, 3(3):203–220, 1993.

[31] P. Knierim, V. Schwind, A. M. Feit, F. Nieuwenhuizen, and N. Henze.
Physical keyboards in virtual reality: Analysis of typing performance and
effects of avatar hands. In Proc. of the 2018 CHI Conference on Human
Factors in Computing Systems, p. 345. ACM, 2018.

[32] R. A. Kockro, C. Amaxopoulou, T. Killeen, W. Wagner, R. Reisch,
E. Schwandt, A. Gutenberg, A. Giese, E. Stofft, and A. T. Stadie. Stereo-
scopic neuroanatomy lectures using a three-dimensional virtual reality
environment. Annals of Anatomy-Anatomischer Anzeiger, 201:91–98,
2015.

[33] P. G. Kry, A. Pihuit, A. Bernhardt, and M.-P. Cani. Handnavigator: Hands-
on interaction for desktop virtual reality. In Proc. of the 2008 ACM symp.
on Virtual reality software and technology, pp. 53–60. ACM, 2008.
[34] M. Kyt¨o, B. Ens, T. Piumsomboon, G. A. Lee, and M. Billinghurst. Pin-
pointing: Precise head-and eye-based target selection for augmented re-
ality. In Proc. of the 2018 CHI Conf. on Human Factors in Computing
Systems, p. 81. ACM, 2018.

[35] D. A. Le, B. MacIntyre, and J. Outlaw. Enhancing the experience of
virtual conferences in social virtual environments. In 2020 IEEE Conf. on
Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),
pp. 485–494. IEEE, 2020.

[36] N. Li, T. Han, F. Tian, Huang, S. Jin, P. Minhui, Irani, and J. Alexander.
Get a grip: Evaluating grip gestures for vr input using a lightweight pen.
In Proc. of the 2020 CHI Conf. on Human Factors in Computing Systems.
ACM, 2020.

[37] Z. Li, M. Annett, K. Hinckley, K. Singh, and D. Wigdor. Holodoc: En-
abling mixed reality workspaces that harness physical and digital content.
In Proc. of the 2019 CHI Conf. on Human Factors in Computing Systems,
p. 687. ACM, 2019.

[38] B. MacIntyre, M. Gandy, S. Dow, and J. D. Bolter. Dart: a toolkit for
rapid design exploration of augmented reality experiences. In Proc. of the
17th annual ACM symp. on User interface software and technology, pp.
197–206, 2004.

[39] N. Marquardt, R. Jota, S. Greenberg, and J. A. Jorge. The continuous
interaction space: interaction techniques unifying touch and gesture on and
above a digital surface. In IFIP Conf. on Human-Computer Interaction,
pp. 461–476. Springer, 2011.

[40] F. Matulic, R. Arakawa, B. Vogel, and D. Vogel. Pensight: Enhanced
interaction with a pen-top camera. In Proc. of the 2020 CHI Conf. on
Human Factors in Computing Systems. ACM, 2020.

[41] M. McGill, D. Boland, R. Murray-Smith, and S. Brewster. A dose of
reality: Overcoming usability challenges in vr head-mounted displays. In

1145/1978942.1979138

[62] M. Speicher, A. M. Feit, P. Ziegler, and A. Kr¨uger. Selection-based text
entry in virtual reality. In Proc. of the 2018 CHI Conf. on Human Factors
in Computing Systems, CHI ’18, p. 1–13. Association for Computing
Machinery, New York, NY, USA, 2018. doi: 10.1145/3173574.3174221
[63] H. B. Surale, A. Gupta, M. Hancock, and D. Vogel. Tabletinvr: Exploring
the design space for using a multi-touch tablet in virtual reality. In Proc.
of the 2019 CHI Conf. on Human Factors in Computing Systems, pp. 1–13,
2019.

[64] Z. Szalav´ari and M. Gervautz. The personal interaction panel–a two-
handed interface for augmented reality. In Computer graphics forum,
vol. 16, pp. C335–C346. Wiley Online Library, 1997.

[65] A. Tang, M. Pahud, K. Inkpen, H. Benko, J. C. Tang, and B. Buxton.
Three’s company: Understanding communication channels in three-way
distributed collaboration. In Proc. of the 2010 ACM Conf. on Computer
Supported Cooperative Work, CSCW ’10, p. 271–280. Association for
Computing Machinery, New York, NY, USA, 2010. doi: 10.1145/1718918
.1718969

[66] F. Tian, L. Xu, H. Wang, X. Zhang, Y. Liu, V. Setlur, and G. Dai. Tilt
menu: Using the 3d orientation information of pen devices to extend the
selection capability of pen-based user interfaces. In Proc. of the SIGCHI
Conf. on Human Factors in Computing Systems, CHI ’08, p. 1371–1380.
Association for Computing Machinery, New York, NY, USA, 2008. doi:
10.1145/1357054.1357269

[67] R. Unger and C. Chandler. A Project Guide to UX Design: For user
experience designers in the ﬁeld or in the making. New Riders, 2012.
[68] D. Vogel, P. Lubos, and F. Steinicke. Animationvr-interactive controller-

based animating in virtual reality. pp. 1–6. IEEE, 2018.

[69] P. Wacker, O. Nowak, S. Voelker, and J. Borchers. Arpen: Mid-air object
manipulation techniques for a bimanual ar system with pen & smartphone.
In Proc. of the 2019 CHI Conf. on Human Factors in Computing Systems,
pp. 1–12, 2019.

[70] J. A. Wagner Filho, C. M. D. S. Freitas, and L. Nedel. Virtualdesk: a
comfortable and efﬁcient immersive information visualization approach.
In Computer Graphics Forum, vol. 37, pp. 415–426. Wiley Online Library,
2018.

[71] X. Wang, L. Besanc¸on, D. Rousseau, M. Sereno, M. Ammi, and T. Isen-
berg. Towards an understanding of augmented reality extensions for
existing 3d data analysis tools. In Proc. of the 2020 CHI Conf. on Human
Factors in Computing Systems, pp. 1–13, 2020.

[72] P. D. Wellner. Interacting with paper on the digitaldesk. Technical report,

University of Cambridge, Computer Laboratory, 1994.

[73] J. O. Wobbrock, L. Findlater, D. Gergle, and J. J. Higgins. The aligned
rank transform for nonparametric factorial analyses using only anova
procedures. In Proc. of the SIGCHI Conf. on human factors in computing
systems, pp. 143–146, 2011.

[74] J. M. Wolfe and T. S. Horowitz. Five factors that guide attention in visual

search. Nature Human Behaviour, 1(3):1–8, 2017.

[75] D. Zielasko, M. Kr¨uger, B. Weyers, and T. W. Kuhlen. Menus on the desk?
system control in deskvr. In 2019 IEEE Conf. on Virtual Reality and 3D
User Interfaces (VR), pp. 1287–1288. IEEE, 2019.

[76] D. Zielasko, M. Kr¨uger, B. Weyers, and T. W. Kuhlen. Passive haptic
menus for desk-based and hmd-projected virtual reality. In 2019 IEEE 5th
Workshop on Everyday Virtual Reality (WEVR), pp. 1–6. IEEE, 2019.
[77] D. Zielasko, B. Weyers, and T. W. Kuhlen. A non-stationary ofﬁce desk
substitution for desk-based and hmd-projected virtual reality. In 2019
IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp.
1884–1889. IEEE, 2019.

Proc. of the 33rd Annual ACM Conf. on Human Factors in Computing
Systems, pp. 2143–2152. ACM, 2015.

[42] M. Mcgill, A. Kehoe, E. Freeman, and S. Brewster. Expanding the bounds
of seated virtual workspaces. ACM Trans. on Computer-Human Interac-
tion (TOCHI), 27(3):1–40, 2020.

[43] M. McGill, J. Williamson, A. Ng, F. Pollick, and S. Brewster. Challenges
in passenger use of mixed reality headsets in cars and other transportation.
Virtual Reality, pp. 1–21, 2019.

[44] M. Nebeling, K. Lewis, Y.-C. Chang, L. Zhu, M. Chung, P. Wang, and
J. Nebeling. Xrdirector: A role-based collaborative immersive authoring
system. In Proc. of the 2020 CHI Conf. on Human Factors in Computing
Systems, pp. 1–12, 2020.

[45] E. Ofek, J. Grubert, M. Pahud, M. Phillips, and P. O. Kristensson. Towards
a practical virtual ofﬁce for mobile knowledge workers. Microsoft New
Future of Work Symposium, 2020.

[46] P. Pantelidis, A. Chorti, I. Papagiouvanni, G. Paparoidamis, C. Drosos,
T. Panagiotakopoulos, G. Lales, and M. Sideris. Virtual and augmented
reality in medical education. In Medical and Surgical Education-Past,
Present and Future, pp. 77–97. 2018.

[47] D. Parmar and T. Bickmore. Making it personal: Addressing individual
audience members in oral presentations using augmented reality. Proc. of
the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,
4(2):1–22, 2020.

[48] L. Pavanatto, C. North, D. A. Bowman, C. Badea, and R. Stoakley. Do we
still need physical monitors? an evaluation of the usability of ar virtual
monitors for productivity work. In 2021 IEEE Virtual Reality and 3D User
Interfaces (VR), pp. 759–767. IEEE, 2021. doi: 10.1109/VR50410.2021.
00103

[49] K. Pfeuffer, J. Alexander, M. K. Chong, Y. Zhang, and H. Gellersen. Gaze-
shifting: Direct-indirect input with pen and touch modulated by gaze.
In Proc. of the 28th Annual ACM Symp. on User Interface Software &
Technology, pp. 373–383, 2015.

[50] K. Pfeuffer, K. Hinckley, M. Pahud, and B. Buxton. Thumb+ pen interac-

tion on tablets. In CHI, pp. 3254–3266, 2017.

[51] K. Pfeuffer, B. Mayer, D. Mardanbegi, and H. Gellersen. Gaze+ pinch
interaction in virtual reality. In Proc. of the 5th Symp. on Spatial User
Interaction, pp. 99–108. ACM, 2017.

[52] D.-M. Pham and W. Stuerzlinger. Is the pen mightier than the controller? a
comparison of input devices for selection in virtual and augmented reality.
In 25th ACM Symp. on Virtual Reality Software and Technology, pp. 1–11,
2019.

[53] E. D. Ragan, D. A. Bowman, R. Kopper, C. Stinson, S. Scerbo, and
R. P. McMahan. Effects of ﬁeld of view and visual complexity on virtual
reality training effectiveness for a visual scanning task. IEEE trans. on
visualization and computer graphics, 21(7):794–807, 2015.

[54] P. Reipschl¨ager and R. Dachselt. Designar: Immersive 3d-modeling
combining augmented reality with interactive displays. In Proc. of the
2019 ACM Int. Conf. on Interactive Surfaces and Spaces, pp. 29–41, 2019.
[55] J. Rekimoto and M. Saitoh. Augmented surfaces: a spatially continuous
work space for hybrid computing environments. In Proc. of the SIGCHI
Conf. on Human Factors in Computing Systems, pp. 378–385. ACM, 1999.
[56] A. Ruvimova, J. Kim, T. Fritz, M. Hancock, and D. C. Shepherd. ”
transport me away”: Fostering ﬂow in open ofﬁces through virtual reality.
In Proc. of the 2020 CHI Conf. on Human Factors in Computing Systems,
pp. 1–14, 2020.

[57] D. Schneider, A. Otte, T. Gesslein, P. Gagel, B. Kuth, M. S. Damlakhi,
O. Dietz, E. Ofek, M. Pahud, P. O. Kristensson, et al. Reconviguration:
Reconﬁguring physical keyboards in virtual reality. IEEE trans. on visual-
ization and computer graphics, 2019.

[58] R. Schweigert, V. Schwind, and S. Mayer. Eyepointing: A gaze-based
selection technique. In Proc. of Mensch und Computer 2019, pp. 719–723.
ACM, 2019.

[59] L. Sidenmark, D. Mardanbegi, A. Ramirez Gomez, C. Clarke, and
H. Gellersen. Bimodalgaze: Seamlessly reﬁned pointing with gaze and
ﬁltered gestural head movement. In Proc. of Eye Tracking Research and
Applications, 2020.

[60] M. Slater, D.-P. Pertaub, C. Barker, and D. M. Clark. An experimental
study on fear of public speaking using a virtual environment. CyberPsy-
chology & Behavior, 9(5):627–633, 2006.

[61] H. Song, H. Benko, F. Guimbretiere, S. Izadi, X. Cao, and K. Hinckley.
Grips and gestures on a multi-touch pen. In Proc. of the SIGCHI Conf.
on Human Factors in Computing Systems, CHI ’11, p. 1323–1332. Asso-
ciation for Computing Machinery, New York, NY, USA, 2011. doi: 10.

