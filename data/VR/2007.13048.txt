Trick the Body Trick the Mind: Avatar representation
aﬀects the perception of available action possibilities in
Virtual Reality

Tugce Akkoc · Emre Ugur · Inci Ayhan

0
2
0
2

l
u
J

6
2

]

C
H
.
s
c
[

1
v
8
4
0
3
1
.
7
0
0
2
:
v
i
X
r
a

Abstract In immersive Virtual Reality (VR), your brain
can trick you into believing that your virtual hands are
your real hands. Manipulating the representation of the
body, namely the avatar, is a potentially powerful tool
for the design of innovative interactive systems in VR.
In this study, we investigated interactive behavior in
VR by using the methods of experimental psychology.
Objects with handles are known to potentiate the af-
forded action. Participants tend to respond faster when
the handle is on the same side as the responding hand
in bi-manual speed response tasks. In the ﬁrst exper-
iment, we successfully replicated this aﬀordance eﬀect
in a Virtual Reality (VR) setting. In the second ex-
periment, we showed that the aﬀordance eﬀect was in-
ﬂuenced by the avatar, which was manipulated by two
diﬀerent hand types: 1) hand models with full ﬁnger
tracking that are able to grasp objects, and 2) capsule-
shaped - ﬁngerless- hand models that are not able to
grasp objects. We found that less than 5 minutes of
adaptation to an avatar, signiﬁcantly altered the aﬀor-
dance perception. Counter intuitively, action planning
was signiﬁcantly shorter with the hand model that is
not able to grasp. Possibly, fewer action possibilities
provided an advantage in processing time. The presence
of a handle speeded up the initiation of the hand move-
ment but slowed down the action completion because
of ongoing action planning. The results were examined
from a multidisciplinary perspective and the design im-
plications for VR applications were discussed.

T. Akkoc
Bogazici University, Cognitive Science MA Program
E-mail: akkoctugce@gmail.com

E. Ugur
Bogazici University, Computer Engineering Department

I. Ayhan
Bogazici University, Psychology Department

Keywords Virtual Reality · Avatars · Aﬀordance ·
Virtual Hands

1 Introduction

With recent breakthroughs in Virtual Reality technol-
ogy, VR has the power to redeﬁne the way we interact
with digital media. Today, VR devices are low-cost and
accessible. There is a growing number of researchers, de-
velopers, VR enthusiasts, and tech companies exploring
and enhancing the user experience and interactivity in
immersive virtual environments. Although much eﬀort
has been put on the exploration of the perception of vir-
tual bodies and virtual environments in VR separately,
little attention has been given to their interaction.

Gibson (1966) coined the term aﬀordance, which be-
came a commonly used concept in explaining the inter-
action between an agent and its environment. The same
object can aﬀord diﬀerent actions for diﬀerent agents. A
bottle, for example, is ‘grasp-able’ for a person, rather
‘climb-able’ for an ant. Thus, it is the relationship be-
tween the agent and the environment that determines
the possible ways of interaction. Without technologies
like VR, on the other hand, it is not easy to manipulate
human body to observe the impact of morphological
changes on perception. In fact, the literature on aﬀor-
dance is heavily based on behavioral studies where bod-
ily morphology remains unaltered, whereas the proper-
ties of the environment and/or objects are changed in
various ways [19,29,59,47, 69].

Warren conducted the ﬁrst empirical study to show
that aﬀordance perception is based on bodily percep-
tion by studying human stair climbing with short and
tall people [65]. Warren and Whang provided further
evidence that aﬀordance perception is body-scaled by
studying the passability of an aperture with large and

 
 
 
 
 
 
2

Tugce Akkoc et al.

small subjects [66]. However, body perception is not
limited to innate diﬀerences in body morphology be-
tween individuals. Tool-use is known as a way of manip-
ulating body perception. People identify tools as exten-
sions of their own bodies [52,58], which in turn changes
the way they perceive their environment [6,21,67,10].
Just holding a stick, for example, can create a dramatic
eﬀect on our body perception such that it remaps our
body form and the environment around us and the in-
teractivity in between. It has been shown that tool-use
as brief as 15 minutes is enough for people to overes-
timate their arm length [58]. Through the interactions
with the environment, the brain constantly updates the
neural representation of the body parts and their po-
sitions [55]. Therefore, humans do not perceive their
bodies in stable metrics. Instead, our body perception
is dynamic and changes through action.

Both VR and tool-use studies provide evidence for
the ﬂexibility of body perception. It is possible to con-
trol altered bodies in VR through the manipulation of
sensory feedback. As sensory feedback changes, our per-
ception of our bodies and the world around us change,
too. The perception of body and environment are in-
terdependent (For a review, see [30]). Yet measuring
this interaction between the body and aﬀordance per-
ception requires controlled manipulation of the body
form and its capabilities, which is a highly challenging
task in experimental design. VR technology allows us
to manipulate human body through the embodiment
of avatars, and measure aﬀordance perception while in
action.

Here, by using diﬀerent avatar hands in a VR envi-
ronment, we demonstrate for the ﬁrst time in literature
- that reaction time to act upon an object is modu-
lated diﬀerentially for restricted versus capable avatars
and that this aﬀordance eﬀect depends on the phase
of action, whether it is during action preparation or
action execution. Aﬀordance eﬀect is measured by the
change in response time according to the orientation
of the graspable part of an object with respect to the
response hand. Contrary to our expectation, when we
removed the avatar’s capability of grasping in the pres-
ence of a graspable object, we saw a reduction in the re-
action time that indicates faster action planning. These
results are in line with the aﬀordance competition hy-
pothesis [13] and will be further discussed in detail in
the discussion section. Partitioning the response time
measurement into diﬀerent phases of action was a criti-
cal decision for our study. Strikingly, we found that the
aﬀordance eﬀect shows a diﬀerent pattern depending on
the phase of the action. Furthermore, having ﬁngers or
not only matters during the planning time of a grasp-
ing action and creates no diﬀerence for the overall time

to complete the action. Rosenbaum showed that plan-
ning occurs before the initiation time [49]. Therefore,
only in the planning time, we saw the eﬀect of having a
hand that can grasp while acting on a graspable object.
Fleming et al. suggested that planning and preparation
do not completely stop and continue during and even
after action execution [25]. People adjust their hands
and ﬁnger positions according to the object properties.
In our study, we found that the existence of a handle
accelerated the initiation of the hand movement but
slowed down the completion of the movement because
of ongoing action planning. Last but not least of all, we
showed that less than 5 minutes of adaptation time in
VR is enough to alter aﬀordance perception.

1.1 Related Work

1.1.1 Redesigning the human body

Avatars are 3D representations of the body and its
movements in Virtual Reality (VR). They create the
feeling that a virtual body is one’s own and one has
control over the movements of that body [18]. In im-
mersive virtual environments, avatars provide a tool for
manipulating the body and its capabilities. In a series
of informal studies, Lanier et al. explored the limits of
our ability to adapt to novel bodies in Virtual Reality
(VR) by testing awkward avatars [38]. For more than 10
years, they prototyped and tested avatars that are radi-
cally diﬀerent from the standard human body form but
still controllable. These studies have shown that peo-
ple are good at adapting to avatars with diﬀerent body
forms to such an extent that they can even control a
lobster body with eight arms.

Mastering the use of a novel body and being able
to control it skillfully in VR requires a certain level of
embodiment. In this context, embodiment refers to the
sense of ownership and agency created by the avatar.
In other words, the avatar becomes “the new source of
sensations” as if the actual body of the user is substi-
tuted by a virtual one [27].

Avatars in immersive virtual environments are em-
bodied through a ﬁrst-person perspective, synchronous
visuotactile, and/or sensory-motor feedback. Embodi-
ment can occur with avatars of diﬀerent ages [5], gender
[24], and race [51]. In fact, people can learn to control
avatars with radically diﬀerent characteristics than the
standard human body form such as avatars with dis-
proportionate body parts [35], in the form of diﬀerent
species [1], or even abstract representations [50].

Seeing virtual body parts in isolation is enough to
create a sense of ownership in the absence of full-body
tracking. The idea of perceptually replacing body parts

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

3

with artiﬁcial objects dates back to the well-known rub-
ber hand experiment. The Rubber Hand Illusion [8] is
a phenomenon where people feel like a rubber hand is
their own when they observe it is stroked in the same
way as their hidden hand. Recently, Aldhous et al. repli-
cated the rubber hand illusion in a VR setting [2]. In
a setup where there has been a mismatch between the
positions of the hidden real and visible virtual hands,
participants were asked to close their eyes and nail the
spot where they thought their hand was located. Bias
in the responses was indicated by the proprioceptive
drift, which is the deviation of the reported position
from the original spot. Results demonstrated that the
proprioceptive drift was towards the virtual hand, sug-
gesting that participants rely more on the visual cues
than on the proprioceptive sensation, although the lat-
ter clearly signals where their actual hand actually is.
Therefore, the visual representation of the hand that
the user relies on can potentially mislead the user about
the position, form, or abilities of the hand. Passive ob-
servation of the changes in the body representation can
create perceptual alterations on the position and form
of the body parts. Understanding these changes in per-
ceived abilities, however, requires experimental designs
where a participant actively interacts with the virtual
objects.

Won et al. emphasized the diﬀerence between “being
versus doing in a novel body” [68]. In an experiment,
they demonstrated that participants learned to control
an avatar with three arms, with an extra arm coming
out of their chest. Participants with a third arm per-
formed better than those in the control group who had
avatars with two arms in a virtual reach and touch task,
where having a long third arm is advantageous. These
results suggest that task requirements are crucial in ad-
justing to a novel avatar body. Thus, here, in order to
ensure the embodiment of novel avatar hands, we im-
plemented an adaptation procedure, where participants
were asked to carry out simple tasks using a virtual ob-
ject such as reaching, pushing, and picking before the
experimental trials.

Redesigning the human body means redesigning the
capabilities of that speciﬁc body form in VR. For ex-
ample, users can reach distant objects and grasp them
with a “magical” hand, so that the distant objects can
suddenly become graspable and within reach. Or “ghost
hands” can pass through virtual objects so the objects
that look and feel solid are no longer graspable. Aﬀor-
dances are thus highly ﬂexible in VR. This idea brought
up the main question behind our study: Can altered
avatar abilities aﬀect a VR user’s perception of action
possibilities, known as aﬀordances, in VR? Simply, can

putting a person in a new body change their belief in
what they can or cannot do?

1.1.2 Altered abilities altered perception

Our actual or even perceived abilities with respect to
the actions available to us in a given context play an
important role in our perception of the environment.
Older, heavier or shorter people, and women, for ex-
ample, overestimate the steepness of a staircase [20].
Fatigue or being low on physical ﬁtness [20] or wearing
a heavy backpack [7] also alter perception - hills appear
to be steeper to the participants. Similarly, with ankle
weights, participants perceive gaps as wider than they
physically are [41]. Therefore, changing the body form
can change the perceived action possibilities in a given
interactive virtual environment.

Within the context of aﬀordance, “the way living
beings perceive the world is deeply inﬂuenced by the
actions they are able to perform” [31]. The available
actions we perceive when looking at an object are de-
pendent on the object features. The percept of these
features, on the other hand, are not invariant as Tur-
vey suggested [62] but rather relative to the size of our
body (or body parts) with reference to the perceived
eye-height (as an index of scale). Thus, there is no in-
ternal measurement of absolute metric values based on
object geometry. As we interact with the objects in
daily life, for example, we intrinsically decide if a chair
is sittable [42] or a stair is climbable [65] according to
our leg length. Similarly, as the users interact with the
environment in VR, their perception of object features
such as distance, scale or orientation may change since
they recalibrate their movements according to the re-
lationship between the virtual objects and their virtual
body-avatar. In fact, Banakou et al. provided evidence
that adults inhabiting 4-year-old avatar bodies in VR
feel strong body ownership and overestimate the size of
virtual objects [5].

As the body form changes, the perception of the re-
lationship between the body and an object like distance
may change, say if a person has longer arms. In a study,
where they tested the eﬀect of extended arms on one’s
perceived action capabilities, Day et al. found that cal-
ibration to an avatar with long arms is possible as long
as participants receive feedback on their actions [17].
Day et al. also emphasized the diﬀerence between adap-
tation and calibration. Whereas the former occurs in a
longer period of time (e.g. adapting to the use of a pros-
thetic arm), the latter occurs rather quickly (e.g. recali-
brating responses according to the form and capabilities
of an avatar body). Not only visual feedback changes
one’s perceived action capabilities, but perceived action

4

Tugce Akkoc et al.

capabilities, and thus, the embodiment of avatars can
also aﬀect the way we see and react to the world around
us. Testing this hypothesis, on the other hand, requires
a reliable measurement of aﬀordance perception. In the
next section, we provide a background for the selected
methodology.

1.1.3 How to measure aﬀordance perception in Virtual
Reality

Most studies examining avatar embodiment or interac-
tivity in VR are in the form of formal/informal user
studies based on user feedback, questionnaires, semi-
structured interviews or observation [40,39,43,1,46]. On
the other hand, some embodiment studies manipulating
the body morphology in VR are limited to quantitative
changes like size [56] or the number of the limbs [68],
the scale of the body [5]. In our study, we focused on
changing the avatar functionally by giving participants
normal (able to grasp) or ﬁngerless hands (not able to
grasp) while they are presented with a stimulus that has
a graspable handle and we measured aﬀordance percep-
tion with a robust methodology.

Aﬀordance perception is known to be formed in a
short while, exerting its eﬀects during action planning,
even before an observer recognizes an object and makes
a conscious decision on what to do with it. In a review of
the aﬀordance literature, Jamone et al. provided three
conclusive insights from a multidisciplinary perspective:
“(A) perception of action-related object properties is
fast; (B) perception and action are tightly linked and
share common representations; (C) object recognition
and semantic reasoning are not required for aﬀordance
perception.” [31]. If the process of action-related per-
ception is fast, then the investigation should also be
made by fast-paced measurement techniques. Within
the context of perceptual-motor behavior, the fastest
responses are also thought to be the most accurate [23]).
Aﬀordance perception can be measured with a fast-
paced reaction time task by using the Stimulus-Response
Compatibility (SRC) paradigm, where the compatibil-
ity between a stimulus feature and a given response in
a task creates a measurable eﬀect on the speed and/or
accuracy of the response [23]. For example, a circle can
be presented on the right or the left side of the screen
while a participant is responding according to the color
of the stimulus. A participant may be responding with
the right hand if the stimulus is red and responding
with the left hand if the stimulus is blue regardless of
its location. This is a classic example of the Simon Ef-
fect [54] that is based on the spatial relationship be-
tween the location of the stimulus and the response.
Note that what is measured here (the compatibility be-

tween the stimulus position and the response) is inde-
pendent of the task (left or right press contingent upon
the color) which allows researchers to study the poten-
tially unconscious eﬀects of the stimulus on response
time, which is also known as the SRC paradigm. This
relationship between the stimulus and the response is
called the compatibility eﬀect. The same methodology
can be used by changing the location of a part of the
object (e.g. the handle), rather than the location of the
whole object. For instance, the handle of a mug can be
presented either on the right or left-hand side. When a
stimulus is a signiﬁer for action, then the compatibility
eﬀect is called the aﬀordance eﬀect [3].

Instead of the color or shape of an object, Tucker
and Ellis used object inversion as a criterion for the re-
sponding hand selection [60]. In this task, participants
responded with the right hand if the object was upright
and with the left hand if it was upside down. In contrast
with a color-based task, where the response depends on
the color of the object, making a decision on the ob-
ject orientation requires mental processing of the form,
which carries information about the potential actions.
Handled objects like a mug or a frying pan, for example,
automatically potentiate a reach-and-grasp response to-
wards their handles [69]. In a bimanual response task,
left-hand responses are faster when the handle is on the
left side, whereas right-hand responses are faster when
the handle is on the right side [11].

Overall, this methodology allows us to measure the
aﬀordance eﬀect while participants are engaged in an
independent task and unaware of the main purpose of
the experiment. There are also other advantages of this
methodology in a VR setting. Firstly, the participant
is isolated from the outside distractors and unable to
see anything other than the experimental setup that is
designed by the experimenter. This provides the exper-
imental conditions to be precisely controlled. Secondly,
the stimuli (3D models of the objects with a handle)
is more realistic than the 2D pictures used in prior
studies. Thirdly, the responses like grasping and reach-
ing can be measured during both action preparation
and action execution by dividing the measurement into
subcategories of action. Thus, here, using a stimulus-
response compatibility paradigm in a VR setting, we
investigated the eﬀect of the avatar representation on
aﬀordance perception.

1.2 Signiﬁcance of the current study

In this study, we present a robust methodology to mea-
sure interactivity in VR by quantifying the perceived
action possibilities. In the ﬁrst experiment, in a virtual
room environment, participants were presented with a

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

5

mug on a table that appeared either in an upright or
upside-down orientation across diﬀerent trials. The task
of the participants was to respond as quickly as possi-
ble by rotating their wrist and making a grasp action
using their left (i.e., for upside down decisions) or right
(i.e., for upright decisions) hands. This compatibility
between the reacted hand and the handle orientation
was the ﬁrst independent variable. In blocked trials, the
distance between the participant and the mug was also
manipulated - as a second independent variable - such
that the mug could appear in 3 diﬀerent locations - at a
near, middle or far distance from the participant. The
response times of the participants were recorded from
the moment when the stimulus had ﬁrst appeared until
the grasping response was ﬁnalized. The main purpose
of the ﬁrst experiment was to replicate the aﬀordance
eﬀect in a VR setting and the methodology was found to
be eﬀective. In the second experiment, the mug stimulus
was replaced by a frying pan, the position of which was
ﬁxed at a single coordinate (the middle position used in
the ﬁrst experiment). In two diﬀerent conditions of the
independent variable, the avatar hand was either a real-
istic hand with both grasping and pushing abilities or a
capsule-like restricted hand, which lacked any grasping
abilities. Response times were recorded using two in-
dices, namely the lift-oﬀ and movement times. Lift-oﬀ
time was the time it took for participants to lift their
hands oﬀ a rest-state-key on a keyboard following the
presentation of the stimulus on the screen. Movement
time, on the other hand, was recorded from the lift-
oﬀ time until the hand model contacted the invisible,
virtual detection box on the target location. Recording
the lift-oﬀ time, as well as the movement time increased
the reliability of the reaction time measurement. The
results revealed diﬀerent patterns for diﬀerent phases
of action.

2 Methods and Results

2.1 Experiment 1: Using Stimulus-Response
Compatibility (SRC) paradigm to measure the
aﬀordance eﬀect in VR

The main purpose of this experiment was to validate
our experimental setup and methodology. We aimed to
test if we can create an aﬀordance eﬀect with a virtual
object in a virtual environment when participants only
see virtual hands mimicking the movements of their ac-
tual hands, which are out of sight. Objects with handles
are known to potentiate aﬀorded action. Participants
tend to respond faster when the handle is on the same
side as the responding hand in a bimanual speed re-
sponse task [60]. Additionally, this eﬀect, also known

as the aﬀordance eﬀect, is found to be aﬀected by the
reachability of the object [16].

In the literature, distance is usually divided into
two categories: peripersonal -within reach- and extrap-
ersonal space -out of reach- [15, 16,22]. In this experi-
ment, we also introduced a mid-range position between
the two in order to see whether the aﬀordance eﬀect
is categorical (i.e. reaction times clustering around the
two categories the eﬀect is present or absent) or gradi-
ent (the strength of the eﬀect varies gradually).

H1: On a SRC paradigm measuring the aﬀordance
eﬀect in VR, participants will respond faster when the
object handle is on the same side as the responding hand
than when the object handle is on the opposite side.

H2: The SRC eﬀect in the virtual environment will
only be found when the object is at a reachable distance
(in the peripersonal space) than when it is at an out-of-
reach distance (in the extrapersonal space) leading to a
categorical rather than a gradual change.

2.1.1 Methods and Materials

Participants: Sixteen graduate and undergraduate stu-
dents (13 male and 3 female) from Istanbul Techni-
cal University participated in Experiment 1. The mean
age of the participants was 25 ranging from 22 to 33
years old. Fifty-six percent of the subjects had previ-
ously tried VR at least once. Seventy-ﬁve percent of the
participants were right-handed and the rest were left-
handed. The study was compliant with the Bogazici
University research ethics requirements, as well as the
Declaration of Helsinki.

Apparatus: Virtual hands are usually tracked by VR
controllers. The position and rotation of the controllers
are mapped onto the virtual representation of the hands.
Here, we used the Leap Motion hand tracking device1
so that the movement of the participant’s bare hands
and ﬁngers could be tracked and modeled in VR. The
hand tracking device was connected to a laptop (HP
OMEN) and physically attached to the front of a head-
mounted display (Acer Windows Mixed Reality Head-
set2). Participants saw the stimulus via a head-mounted
display. Since the VR goggles physically covered their
eyes, they did not see the real experimental room but
rather the virtual room from a ﬁrst-person perspective.
In both the real and virtual experimental rooms, par-
ticipants were sitting on a chair in front of a table. A
Sony 310AP Wired Headphones was connected to the
VR headset for auditory feedback. A red mug with a
handle on the right-hand or left-hand side was used as

1 https://www.ultraleap.com/
2 https://www.acer.com/ac/en/US/content/series/wmr

6

Tugce Akkoc et al.

Fig. 1 Outline of a trial in Experiment 1.a) At the beginning of each trial, a black cube appeared in one of the three locations
on the long table (Near, Middle, Far). b) After 500 milliseconds, the cube disappeared and the stimulus randomly appeared at
the same location in one of four diﬀerent orientations (Right-Up, Right-Down, Left-Up, or Left-Down). Up and Down represent
the vertical orientation of the stimulus, while Right and Left represent the handle orientation. The handles were rotated 15◦
towards the participant. c) Participants responded according to the vertical orientation of the stimulus by making a grasping
gesture either with their right or left hand. d) At the end of the grasping gesture, the target disappeared and the trial ended.
See the video of the experimental procedure: https://youtu.be/7vc_zBAQxMM

a stimulus that randomly appeared in diﬀerent orienta-
tions (upright or inverted). In blocked trials, the mug
was presented either at a Near (30 cm), Middle (60 cm),
or Far (150 cm) distance from the participant.

Procedure: First, the experimenter demonstrated the
resting position of the hands. In the default resting po-
sition, participants held both of their hands open (in
front of the tracking device) with their palms facing
away from them. They were then shown the required
hand movement to respond to the stimulus: rotating
the wrist inward and grasping. The experimenter then
presented the upright and inverted orientations of the
stimulus with a real physical red mug which was similar
to the virtual mug (which acted as the stimulus) they
were about to see in the virtual environment.

Participants initiated the experiment by pressing
the space bar on a keyboard. They were instructed to
ﬁxate their eyes on a black cube (with the side lengths
of 10 cm) that appeared in one of the three positions (at
the Near, Middle or Far distance) for 500 milliseconds.
Right after the cube disappeared, a red mug appeared
at the same position as the black cube. The task of the
participants was to make a grasping gesture using the
correct hand according to the vertical orientation of the
mug (whether upright or upside down), independent of
the handle orientation (Fig. 1). Following each grasp re-
sponse was faint auditory feedback to mark the end of
an individual trial. This stereo sound always originated

from the same location (left or right) of the respond-
ing hand. After every trial, participants returned their
hands back to their default position. Response times
were automatically recorded during the experiment. For
every individual trial, the response time counter started
at the appearance of the object and ended when the
thumb and index ﬁnger touched each other in order to
complete the grasping gesture.

For the ﬁrst part of the experiment, participants
completed a practice session with 25 trials during which
they were given auditory error feedback. Participants
who made errors in more than 10% of the trials repeated
the practice session. At the beginning of the experimen-
tal session, participants were instructed to respond as
quickly and accurately as possible. Each experimental
block was composed of 60 test trials. Half of the par-
ticipants responded with the right hand when the ob-
ject is upright and with the left hand when the object
is upside down in the ﬁrst block, and responded with
the left hand when the object is upright and with the
right hand when it is upside down in the second block.
The mapping of response hand to object inversion was
changed and counterbalanced across participants. Be-
fore the second block, the participants completed an-
other 25 practice trials with the new instruction. After-
wards, they completed 60 test trials. Participants thus
completed 120 trials in two blocks. Each test trial took
approximately two minutes. In each block, the mug ap-
peared at every combination of position and orientation

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

7

demonstrates that the stimulus-response compatibility
paradigm can also be used to understand the interac-
tion between an object and a virtual hand.

Although we could not ﬁnd a signiﬁcant interaction
eﬀect between handle orientation and distance, there
has been a clear trend in the data. In the compatible
conditions, participants reacted faster only when the
object was in a reachable region category (Near and
Middle) than when it was in the out-of-reach (Far) re-
gion (Fig. 2). This is compatible with the Constantini
et al. ﬁndings[16], where object features were found to
evoke actions only when presented in the peripersonal
space.

In summary, in Experiment 1, we showed that the
aﬀordance eﬀect can be created in VR. In Experiment
2, we used this methodology to look at the eﬀect of
avatar representation on how people perceive the avail-
able action possibilities.

2.2 Experiment 2: Manipulating the virtual hand
representation

The purpose of the second experiment was to see if the
aﬀordance eﬀect is modulated by the body representa-
tion. Previous studies showed that avatar abilities can
be changed in VR [68, 17]. We used two diﬀerent hand
types: one with fully animated ﬁngers and one without
ﬁngers. The former is the able hand that it is able to
grasp virtual objects, and the latter is the restricted
hand that it is not able to grasp objects. Using the
same SRC paradigm that we tested in Experiment 1,
here, in Experiment 2, we manipulated the avatar hand
representation to measure the aﬀordance eﬀect.

H3: On a SRC paradigm measuring the aﬀordance
eﬀect in VR, participants will respond faster when the
object handle is on the same side as the responding hand
than when the object handle is on the opposite side.
H4: The aﬀordance eﬀect in VR will be aﬀected by the
avatar hand type such that reaction times will diﬀer
for the restricted hand avatars than for the able hand
avatars.

2.2.1 Methods and Materials

Participants: Thirty-two undergraduate students (14 fe-
male, 18 male) from Boazii University participated in
Experiment 2. The mean age of the participants was 23
ranging from 19 to 35 years old. Sixty-two percent of the
subjects had previously tried VR at least once. Ninety-
four percent of the participants were right-handed and
the rest were left-handed. None of the participants had

Fig. 2 Overall results of Experiment 1. Response time is rep-
resented on the y-axis in seconds. On the x-axis, two levels
of the handle orientation eﬀect (the compatible and incom-
patible conditions) are shown. The purple, orange, and green
lines represent the conditions where the stimulus was shown
at near, middle, and far distances respectively. Error bars in-
dicate the standard errors of the mean (+/- 1 SEM).

(right-up, right-down, left-up, left-down) for 5 times.
The order of these states was randomized.

2.1.2 Results and Discussion

Error trials and response times 2 standard deviation
above or below from the condition means were excluded
from the analysis. A 2 x 3 repeated measures analysis of
variance (ANOVA) was conducted with the Handle Ori-
entation (compatible and incompatible) and Distance
(Near, Middle, and Far) as within-subject factors. The
main eﬀects for both Handle Orientation and Distance
were signiﬁcant, F (1, 15) = 7.690, p = .014, η2 = .339
and F (2, 15) = 7.290, p = .003, η2 = .327, respectively.
Participants reacted faster in the compatible conditions
(Mean = .575 , SD = .048) than in the incompatible
conditions (Mean = .583, SD = .052). A Tukey’s HSD
post hoc analysis on the main distance eﬀect showed
that participants responded signiﬁcantly faster when
the object is presented in the Near (Mean = .576, SD
= .048) or Middle (Mean = .572, SD = .051) positions
compared to the Far (Mean = .588, SD = .052) po-
sition. The diﬀerence between Near and Middle con-
ditions were not signiﬁcant. The interaction eﬀect be-
tween Handle Orientation and Distance was found to
be insigniﬁcant, F (1, 31) = 0.435, p = .651, η2 = .028
(Fig. 2).

These results provide further evidence for the ﬁnd-
ings of [16], where they showed the main eﬀects of
handle orientation and distance of the object on reac-
tion time in a 3D stereoscopic viewing setup combined
with colored hand pictures. That we found a signiﬁ-
cant handle orientation eﬀect in VR with hand tracking

8

Tugce Akkoc et al.

Fig. 3 Experimental setup for experiment 2. ‘X’ and ‘2’ on
the numeric keypad were used as response keys. All the other
keys surrounding them were pulled out of the keyboard to pre-
vent possible interfering inputs during the experiment. The
two response keys were physically enlarged. The keyboard
was raised and aligned with the handrests. The Leap Mo-
tion hand tracking device was ﬁxed at the edge of the table.
The response was given by lifting the responding hand oﬀ the
response key and reaching forward.

participated in Experiment 1. The study was compli-
ant with the Bogazici University research ethics require-
ments, as well as the Declaration of Helsinki.

Apparatus: The methodology was the same as the ﬁrst
experiment whereas the experimental setup and the
measurement technique were changed (Fig. 3). In this
experiment, the hand tracking device was ﬁxed at the
edge of the table rather than the VR headset to pre-
vent the errors that might be introduced by the reach-
ing movements. An extra keyboard was connected to
the same computer to divide response time measure-
ment into two parts: lift-oﬀ time and movement time.
Whereas the lift-oﬀ time indicated the interval between
the appearance of the target stimulus and the release of
the response key, movement time indicated the interval
between the release of the response key and the touch
over the target stimulus.

A frying pan, designed in Autodesk Maya 20183, was
used as the stimulus object. The bottom texture of the
pan was chosen so as to be diﬀerent from that of the top
part to make it easier for participants to discriminate
the upright oriented pan from an inverted pan. Object
positioning was done using the base-centered approach
[11] as cited in [36] where the pivot of the virtual object
was placed at the center of the body of the pan.

The distance of the pan from the participant was
ﬁxed at the ‘middle’ distance used in Experiment 1. To
the right and left sides of the pan were two invisible

3 https://www.autodesk.com/products/maya/

Fig. 4 Virtual hand representations. a) The able hand and
b) the restricted hand.

boxes to detect the contact of the hand models where
the reaching response was completed. Two audio source
objects were placed in the same locations for audio feed-
back to mark a successful reaching response.

The main diﬀerence between Experiment 1 and Ex-
periment 2 was the second independent variable - avatar
hand type. Two types of hands were used: Able and Re-
stricted hand. The Able hand model was a full ﬁnger
tracking hand model which was also used in Exper-
iment 1 (Fig. 4a), while the Restricted model was a
capsule or a pill-shaped 3D model (Fig. 4b). The re-
stricted hand had the same size, texture, and color as
the normal hand. The able hand was designed to allow
grasping behavior as opposed to the restricted hand,
which was not able to grasp. Thus, two avatar hands al-
lowed diﬀerent abilities to the participants. There were
no physical restrictions on the real hand movements of
the participants, but when they grasped with their real
hands, only the full hand model provided visual feed-
back of the grasping action. Likewise, the Restricted
hand lacked a grasping ability and simply collided with
the virtual objects.

2.2.2 Procedure

Prior to each experimental session, participants spent
time with the respective hand model for adaptation. In
the adaptation phase, they performed 4 sessions com-
posed of simple tasks as instructed by the experimenter
16 times each. Adaptation Task 1A was “Open and
Close Hands A”. With palms facing inwards partici-
pants opened and closed their hands while looking at
their palms and counting out loud. Adaptation Task
1B was “Open and Close Hands B”. With palms facing
outwards participants opened and closed their hands
16 times while looking at the back of their hands and

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

9

Fig. 5 Outline of a trial in Experiment 2. a) At the beginning of each trial, a black cube appeared at the same location
on the table. b) After 500 milliseconds, the cube disappeared and the stimulus appeared at the same location in one of
four diﬀerent orientations randomly (Right-Up, Right- Down, Left-Up, or Left-Down). Up and Down represent the vertical
orientation of the stimulus, while Right and Left refer to the handle orientation. The handles were rotated 15◦ towards the
participant. c) Participants responded according to the vertical orientation of the stimulus by lifting their right or left hand oﬀ
the response key and reaching forward until the target point, represented by the wooden areas on the table. d) As the virtual
hand contacted the invisible detection box, the target disappeared and the trial ended. Please see the video of experimental
procedure: https://youtu.be/eyru1BVJShU

counting out loud. Adaptation Task 2 was “Reach and
Touch”. Participants reached and touched red cubes
that appeared either on the right-hand side or left-hand
side with the corresponding hand and then brought
their hand back to the default position. Adaptation
Task 3 was “Push”. Participants pushed the cubes for-
ward with the back of their hands with a ﬂick of the
wrist. Adaptation Task 4 was “Pick and Place”. Par-
ticipants picked the red cubes up and placed them on
black cubes.

In the experimental phase, participants initiated the
experiment by pressing both response keys simultane-
ously. In a manner similar to Experiment 1, as both
keys were pressed down, a black cube appeared for 500
ms. Participants were told to ﬁxate their eyes on the po-
sition of the black cube. Right after the ﬁxation object
disappeared, the stimulus (the frying pan) appeared
with the handle either on the right or the left-hand side.
Participants’ task was to respond according to the ver-
tical orientation of the object (whether it was upright or
upside down) (Fig. 5). Responses were given by lifting
the responding hand oﬀ the response key and reaching
forward (Fig. 3).

The instruction was counterbalanced between sub-
jects. Half of the participants were given instruction A
only, which was ‘Respond with your right hand if the
pan is upright and respond with your left hand if the

pan is upside down’ and the other half of the partici-
pants received the instruction B only, which is ‘Respond
with your right hand if the pan is upside down and re-
spond with your left hand if the pan is upright’.

Participants ﬁrst completed 24 practice trials with
error feedback as in Experiment 1. Once the practice
session was completed, they started the ﬁrst test ses-
sion, which was composed of 24 trials without error
feedback. Participants completed 6 blocks of 24 trials.
In each block, the pan appeared at every location, and
in every orientation (right-up, right-down, left-up, left-
down) twice. The order of these states was randomized.
Overall the experiment had 7 sessions: one practice and
six test sessions in two parts composed of three sessions
for each hand type. Prior to each test session was an
adaptation phase. Depending on the pace of the partici-
pant, each adaptation phase took three to ﬁve minutes.
Each test session took approximately one and a half
minutes. Between the two parts of the experiment, par-
ticipants were allowed to rest as long as they needed
it. They were allowed to talk to the experimenter, and
drink water but they were not permitted to have screen
time or engage in cognitively or physically demand-
ing tasks. The order of hand type was counterbalanced
among the participants. After each session, data includ-
ing lift-oﬀ time and movement-time for each trial was
automatically saved.

10

Tugce Akkoc et al.

Fig. 6 Results of experiment two (lift-oﬀ time). Lift-oﬀ time
is represented on the y-axis in seconds. On the x-axis, two
levels of the handle orientation eﬀect (the compatible and in-
compatible conditions) are shown. The green and orange lines
represent the conditions where the participants have able or
restricted hands respectively. Error bars indicate the standard
errors of the mean (+/-1 SEM).

Fig. 7 Results of experiment two (movement time). Move-
ment time is represented on the y-axis in seconds. On the
x-axis, two levels of the handle orientation eﬀect (the com-
patible and incompatible conditions) are shown. The green
and orange lines represent the conditions where the partici-
pants have able or restricted hands respectively. Error bars
indicate the standard errors of the mean (+/- 1 SEM).

Immediately after the completion of the experiment,
participants ﬁlled out two Body Ownership Question-
naires, one for Able hand (see Appendix A) and one for
Restricted hand (see Appendix B). The questionnaire
was prepared by modifying the questions that were pre-
viously used in three diﬀerent VR studies [4,17,53]. The
questionnaires were given in the same order of the given
hand type. Finally, participants were debriefed.

2.2.3 Results

For two diﬀerent dependent variables (Lift-oﬀ Time and
Movement Time), two Repeated measures analysis of
variance (ANOVA) were conducted with the Hand Type
(Able vs. Restricted) and Handle Orientation (compat-
ible vs. incompatible) as within-subject factors.

Lift-oﬀ time was the interval between the appear-
ance of the target stimulus and the release of the re-
sponse key. Error trials and response times 2 standard
deviation above or below from the condition means were
excluded from the analysis. A 2 x 2 within-subjects
ANOVA indicated a signiﬁcant handle orientation ef-
fect, F (1, 31) = 34.456, p = .001, η2 = .547 such that
participants lifted their hand oﬀ the response key faster
in the compatible conditions (when the handle is on the
same side as the responding hand) (Mean = .897, SD =
.204) than in incompatible conditions (where the handle
is on the opposite side of the responding hand) (Mean
= .947 , SD = .183). There was no signiﬁcant main ef-
fect of hand type, F (1, 31) = 0.769, p = .387, η2 = .024
which means that participants lifted their hands oﬀ the
response keys at similar times with both able (Mean =

.930, SD = .221) and restricted hand (Mean = .914, SD
= .165).

More interestingly, there was a signiﬁcant interac-
tion eﬀect between Handle Orientation and Hand Type,
F (1, 31) = 7.129, p = .012, η2 = .187, demonstrating
that the handle orientation eﬀect was modulated by
the hand type. As shown in Fig. 6, in the incompatible
conditions, it took a similar amount of time for partici-
pants to respond using able (Mean = .947, SD = .203)
and restricted hand (Mean = .947, SD = .162 ). How-
ever, participants reacted signiﬁcantly faster with the
restricted hand (Mean = .881, SD = .164) than with the
able hand (Mean = .913, SD = .239) in the compatible
conditions.

Movement time was the interval between the re-
lease of the response key and the touch over the target
stimulus. Error trials and response times 2 standard de-
viation above or below from the condition means were
excluded from the analysis. As well as in the lift-oﬀ
time, handle orientation was also signiﬁcant for move-
ment time, F (1, 31) = 9.414, p = .004, η2 = .233, but
with the reverse trend that it took longer to respond in
the compatible trials (Mean = .581, SD = .178) than
in the incompatible trials (Mean = .566, SD = .181)
(Fig. 7).

As in the lift-oﬀ time, the main eﬀect of hand type
was not statistically diﬀerent for movement time, ei-
ther, F (1, 31) = 1.561, p = .221, η2 = .048, thus, it took
similar time to respond with the able (Mean = .564,
SD = .171) and restricted hand (Mean = .583, SD =
.188). Similarly, the interaction eﬀect between the han-
dle orientation and the hand type was also statistically
insigniﬁcant, F (1, 31) = 0.572, p = .455, η2 = .018.

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

11

Body ownership questionnaire included questions
such as “I felt as if the virtual hands that I saw dur-
ing the experiment were part of my body” to measure
to what extent virtual body parts felt like one’s own.
Participants answered the same questions for both the
able and restricted hand on a Likert scale from 0 to 10
corresponding to ‘completely disagree’ and ‘completely
agree’. The results demonstrated that both restricted
and able hand provided a sense of ownership, although
a paired-samples t-test, conducted to compare the sense
of ownership created by diﬀerent hand types, indicated
that the able hand with full ﬁnger tracking created sig-
niﬁcantly more ownership (M = 7.99, SD = 1.14) than
the restricted hand (M = 5.67, SD = 1.74) condition,
t(31) = 7.70, p = .001, d = 1.36. Since participants
answered the questionnaires at the end of the experi-
ment, however, they may have thought that they were
supposed to compare the two hands and answer ques-
tions relative to one another. Thus, this diﬀerence be-
tween the hand types may not have appeared if it were a
between-subject design, where participants would com-
plete the task with only one hand type.

Although participants reported that the able hand
felt more like their own, however, the movement time
results did not reveal a diﬀerence between the two hand
types. Similarly, the lift-oﬀ time results did not re-
veal a diﬀerence between the able and restricted hands
in the incompatible conditions, either. The only dif-
ference appeared in the compatible conditions, where
the hand was approaching towards the handle of the
object, implying that the eﬀect of hand type is re-
lated to the relationship between the virtual hand’s
capability (able to grasp or not able to grasp) and
object aﬀordance (graspability) rather than the sense
of ownership created by the hand type. In fact, in or-
der to see whether the level of ownership has an eﬀect
on the signiﬁcant interaction result in the lift-oﬀ time
measures, we conducted a median-split analysis, where
participants were divided into two groups (the ones
who felt strong ownership and the ones who felt less
strong ownership). An ANOVA analysis with the level
of ownership given as a between-subject factor revealed
that the eﬀect of the level of ownership is insigniﬁcant,
F (1, 30) = 3.03, p = .092, η2 = .092, indicating that the
ownership cannot explain the diﬀerences in the lift-oﬀ
time.

3 Discussion

The ﬁndings of the experiments presented signiﬁcant
outcomes:

1. Virtual objects can potentiate aﬀordance eﬀect in
VR such that participants tend to respond faster
when the handle of a target object is on the same
side as the responding hand in a bimanual speed
response task (Experiment 1 and 2). This aﬀordance
eﬀect, on the other hand, depends on the phase of
action: Whereas the trend is as described during
the action planning, it is reversed during the action
execution (Experiment 2).

2. The eﬀect of avatar hand type on aﬀordance percep-
tion also depends on the phase of action. Whereas
the lift-oﬀ time is faster for the restricted hand avatar,
the movement time is faster for the able hand avatar
(Experiment 2).

3. The restricted avatar hand potentiates greater aﬀor-
dance eﬀect than the able avatar hand (Experiment
2).

4. Adaptation to a new avatar hand is fairly fast in
VR. Approximately a 5-minutes-adaptation which
involves various interactions with objects is enough
to embody a new body part.

5. Reaction times in a bimanual speed response task
are faster for virtually closer targets than for further
targets (Experiment 1).

6. Having radically diﬀerent hand designs in VR did
not show any signiﬁcant diﬀerence in the overall re-
action time. Only the action planning time was dif-
ferent for diﬀerent hand models (Experiment 2).

Ecological approach to vision asserts that the envi-
ronment is perceived in terms of behavioral aﬀordances
deﬁned as what the environment oﬀers to the agent.
In this context, vision may be seen as an active ex-
ploration of external world mediated by the sensorimo-
tor contingencies [26,44,45] as well as on-line guidance
while executing actions. How these action possibilities
inherent in the visual objects are represented in the
brain, however, is a question yet to be answered. In a
plausible account, the percept of an object automat-
ically triggers the activation of motor representations
of a list of potential competing actions (see [13]) even
when the observer has no explicit intention to act [60].
These potentiated actions, on the other hand, depends
on the particular state of a perceiver with respect to
the object: A reach-and-grasp act, for example, can-
not be executed if the object is beyond the reaching
distance [15]. Consistently with the previous ﬁndings,
we hereby conﬁrmed the stimulus-response compatibil-
ity eﬀect this time - in a virtual reality environment,
where participants interacted with the object using a
virtual hand, together with further evidence that the ef-
fect size is not gradual across the peripersonal towards
the extrapersonal space but rather categorical. Finding
a signiﬁcant handle orientation eﬀect in VR with hand

12

Tugce Akkoc et al.

tracking supported the idea that the stimulus-response
compatibility paradigm can be used to understand the
interaction between the components of cyberspace (e.
g. a virtual mug) and cyberbody (e.g. virtual hand) as
well as the computer-generated objects and real body
parts as has been investigated in the previous studies
[3,16,60,61].

In both Experiment 1 and Experiment 2, we used
objects that would potentiate reach-and-grasp action.
Even though reaching and grasping seem like comple-
mentary actions that would occur in sequence, they are
known to be processed at separate parts of the mon-
key dorsal stream, reaching in the medial intraparietal
area [12,33,57] and grasping in the anterior intrapari-
etal area [32,48], the human equivalents of which have
also been assessed using functional magnetic resonance
imaging [28]. In his aﬀordance competition account [13],
Cisek argues that potential actions compete alongside
the dorsal stream, and only those that survive this com-
petition are transmitted into the frontal areas such as
the dorsal premotor cortex, the activity of which re-
ﬂects the ﬁnal action decision [34]. In this context, the
restricted avatar hand potentiating greater aﬀordance
eﬀect than the able avatar hand in our study may be
explained by a lack of competing action in the case
of restricted hand, as it does not allow any grasping
movement but only reaching. This is consistent with
recent neurophysiological evidence that when monkeys
are provided with two potential reaching targets, action
execution is relatively delayed, implying that multiple
reach options are all represented at the beginning, and
then gradually eliminated until a ﬁnal decision is made
[14].

Whereas we conﬁrmed the aﬀordance eﬀect during
the action planning, as indexed by the lift-oﬀ time, the
handle orientation eﬀect was still signiﬁcant but inter-
estingly reversed during the action execution as indi-
cated by the movement time measure. The reverse com-
patibility eﬀect, which is rare but still existing in the
literature [9,37, 64, 70], was explained by a horizontal
object positioning, where object centralization could ei-
ther be made according to the mass or the length of ob-
ject, the latter of which would yield an uneven distribu-
tion of pixels favoring the body [37]. This would render
the handle insigniﬁcant, or the body of object contain-
ing more task-relevant information than the handle, in
either case of which the endogenous attention would
shift towards the body and thus, the reaction times
would be faster when the body and not the handle -
of a target object is on the same side as the responding
hand. In a task, where participants were asked to in-
dicate whether the presented objects are man-made or
natural images, Yu et al. have also found a reverse com-

patibility eﬀect, where the reaction times were faster
when the response button was situated on the opposite
side as the depicted object’s handle [70]. Replicating
Tucker and Ellis’ original stimulus-response compati-
bility eﬀect in a paradigm which rather required an ex-
plicit instruction of imagining picking up each object,
Yu et al. concluded that it is not merely observing an
object, but rather engaging in an action-relevant motor
task which facilitates action for compatible responses.
Similarly, in an eﬀort to explain the negative aﬀordance
eﬀect, Vainio drew attention to the presentation time
[63], revealing reversed stimulus-response compatibil-
ity during brief presentations of target objects (30 or
70 ms), which turned into a positive compatibility ef-
fect when the object was displayed for 370 ms. The
authors argued that it takes a build-up time to grad-
ually activate associated motor representations. None
of these accounts, however, are adequate in explaining
the reversed compatibility eﬀect in our paradigm, as
we found both positive and negative stimulus-response
compatibility eﬀects using the same stimuli and proce-
dure in the same setup. The discrepancy in our study
could rather be originating from two response mea-
sures having been collected at diﬀerent phases of action.
Whereas participants started the action faster in the
compatible conditions than in the incompatible ones,
they rather moved more rapidly in the incompatible
conditions than in the compatible ones. This is in agree-
ment with Cisek’s argument that action planning is
never complete [13]. Cisek argues that reaching a cer-
tain information threshold is enough to start an ac-
tion. However, “even in the cases of highly practised
behaviours” (p. 1586), an action begins without a com-
plete trajectory. That is, there is an ongoing cycle of
planning and observation during the action execution,
with continuous feedback loops that allow ﬁne motor
adjustments while interacting with objects. Therefore,
actions are better thought of as processes than discrete
contained events. Within this theoretical framework, in
our experiment, no ﬁne motor adjustments are needed
in the conditions where the handle orientation does not
correspond to the target location. If the handle is on the
same side as the reaching trajectory, however, action
planning may still continue to adjust the hand move-
ment to the handle position during the action execu-
tion, even though the initial plan may not be to grasp
it. This may explain why we found an approximately 14
milliseconds of slow-down in the movement time for the
compatible compared to the incompatible conditions.

We rely on the sensory feedback to perceive our
bodies and the environment around us, which makes it
possible for us to control novel bodies in VR [38]. The
results of Experiment 2 supported the idea of ﬂexibil-

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

13

ity in body perception. In order to adapt participants
to their novel cyber body parts, we introduced a 5-
minutes adaptation procedure, where participants were
asked to interact with virtual objects in various ways,
including pushing, pulling and lifting actions. Our pre-
liminary data demonstrated that this adaptation phase
was crucial in obtaining the relevant stimulus-response
compatibility eﬀects. It is known that it is possible to
embody a virtual hand via synchronous visuo-tactile
feedback [2]. Here, we demonstrated that synchronous
visual feedback, even in the absence of haptic input, is
enough to create such embodiment eﬀects.

The purpose of this study was to evaluate the per-
ception of aﬀordances in VR and the eﬀect of avatars on
aﬀordance perception. Results indicated that the aﬀor-
dance eﬀect can be created and measured in VR while
the participant is interacting with virtual objects. In
other words, virtual objects in VR trigger speciﬁc ac-
tions like real-world objects do. However, it is not only
dependent on the properties of the objects. People per-
ceive potential actions in relation to their perceived
abilities. We have shown that the representation of the
virtual body aﬀects the way people interact with vir-
tual objects. Understanding the eﬀects of embodiment
on interactivity in virtual environments is highly impor-
tant for commercial VR applications to provide better
user experience and also for research in computer sci-
ence, psychology, neuroscience, and cognitive science to
better understand human perception through action. In
the ﬁnal section, we will provide implications for inter-
action design in VR.

4 Implications for interaction design in VR

Designing interactions in VR is a challenging task. First
of all, users need to know what is interact-able and how
to interact with it. Just as visual representations of vir-
tual objects drive their use, the visual representation of
the body determines the available actions with which
objects can be interacted. Thus, changing virtual hands
and their capabilities can make it easier for the user to
understand potential actions in virtual environments.
Altering the body form can help lead users to under-
stand what is possible in a VR setting and thereby im-
prove user experience.

The idea of having shape-shifting hands provides
potentially inﬁnite aﬀordances for virtual objects in
VR. Through the alteration of avatar abilities, far ob-
jects could now become reachable, heavy objects liftable,
small objects stretchable or big objects scalable. En-
hancing the ways of interaction by giving users magical
abilities provides creative freedom for developers, de-
signers, and users.

Having multiple action possibilities, however, can
also distract users, make the application hard to learn
and use, and make the decision making harder (as shown
by the results of Experiment 2). Therefore, virtual in-
teractions can sometimes be enhanced by restricting
potential actions to direct users to perform a task in
a certain way. Restricting action possibilities may also
provide precise results with imprecise actions. Unlike
real object interactions, a user can interact with vir-
tual objects by making actions in the vicinity of the
object that is expected to be interacted on. For exam-
ple, in VR, the detection of contact between any part of
the user’s hand with a virtual object can be enough to
grasp the object. In real life, on the other hand, grasp-
ing an object requires a complex sequence of actions
and proper positioning of the ﬁngers. This diﬀerence
may provide advantages in some tasks for VR over the
real-world tasks.

VR provides an environment for the curious to ex-
plore the limits of body perception and its eﬀects on
interactivity in Human-Computer Interaction. Virtual
Reality represents a new frontier for extending repre-
sentations of the human form, creates new areas of ap-
plication, and open or even design new perceptions of
reality. This will, in turn, create new horizons of re-
search in perception, blurring the lines between the real
and the virtual.

Acknowledgements We provide our thanks to Erhan Oztop,
Albert Ali Salah and Esra Mungan for reading and providing
comments to an earlier version of this manuscript.
Also, we are grateful for the support of Teleporter Realities
Inc. which provided the devices used in this study.

References

1. Ahn, S.J., Bostick, J., Ogle, E., Nowak, K.L.,
McGillicuddy, K.T., Bailenson, J.N.: Experiencing na-
ture: Embodying animals in immersive virtual environ-
ments increases inclusion of nature in self and involve-
ment with nature. Journal of Computer-Mediated Com-
munication 21(6), 399–419 (2016)

2. Aldhous, J., Hetherington, R., Turner, P.: The digital
rubber hand illusion. In: Proceedings of the 31st Inter-
national BCS Human Computer Interaction Conference
(HCI 2017) 31, pp. 1–5 (2017)

3. Ambrosecchia, M., Marino, B.F., Gawryszewski, L.G.,
Riggio, L.: Spatial stimulus-response compatibility and
aﬀordance eﬀects are not ruled by the same mechanisms.
Frontiers in Human Neuroscience 9(283), 1–11 (2015)
4. Argelaguet, F., Hoyet, L., Trico, M., L´ecuyer, A.: The
role of interaction in virtual embodiment: Eﬀects of the
virtual hand representation. In: 2016 IEEE Virtual Re-
ality (VR), pp. 3–10. IEEE (2016)

5. Banakou, D., Groten, R., Slater, M.: Illusory ownership of
a virtual child body causes overestimation of object sizes
and implicit attitude changes. Proceedings of the Na-
tional Academy of Sciences 110(31), 12846–12851 (2013)

14

Tugce Akkoc et al.

6. Berti, A., Frassinetti, F.: When far becomes near: Remap-
ping of space by tool use. Journal of cognitive neuro-
science 12(3), 415–420 (2000)

7. Bhalla, M., Proﬃtt, D.R.: Visual–motor recalibration in
geographical slant perception. Journal of experimental
psychology: Human perception and performance 25(4),
1076–1096 (1999)

8. Botvinick, M., Cohen, J.: Rubber hands feeltouch that

eyes see. Nature 391(6669), 756–756 (1998)

9. Bub, D.N., Masson, M.E.: On the dynamics of action
representations evoked by names of manipulable objects.
Journal of Experimental Psychology: General 141(3),
502–517 (2012)

10. Canzoneri, E., Ubaldi, S., Rastelli, V., Finisguerra, A.,
Bassolino, M., Serino, A.: Tool-use reshapes the bound-
aries of body and peripersonal space representations. Ex-
perimental brain research 228(1), 25–42 (2013)

11. Cho, D.T., Proctor, R.W.: The object-based simon eﬀect:
Grasping aﬀordance or relative location of the graspable
part? Journal of Experimental Psychology: Human Per-
ception and Performance 36(4), 853–861 (2010)

12. Cisek, P.: Embodiment is all in the head. Behavioral and

Brain Sciences 24(1), 36–38 (2001)

13. Cisek, P.: Cortical mechanisms of action selection: the
aﬀordance competition hypothesis. Philosophical Trans-
actions of the Royal Society B: Biological Sciences
362(1485), 1585–1599 (2007)

14. Cisek, P., Kalaska, J.F.: Neural correlates of reaching de-
cisions in dorsal premotor cortex: speciﬁcation of multi-
ple direction choices and ﬁnal selection of action. Neuron
45(5), 801–814 (2005)

15. Costantini, M., Ambrosini, E., Scorolli, C., Borghi, A.M.:
When objects are close to me: aﬀordances in the periper-
sonal space. Psychonomic bulletin & review 18(2), 302–
308 (2011)

16. Costantini, M., Ambrosini, E., Tieri, G., Sinigaglia, C.,
Committeri, G.: Where does an object trigger an action?
an investigation about aﬀordances in space. Experimen-
tal brain research 207(1-2), 95–103 (2010)

17. Day, B., Ebrahimi, E., Hartman, L.S., Pagano, C.C.,
Robb, A.C., Babu, S.V.: Examining the eﬀects of altered
avatars on perception-action in virtual reality. Journal of
Experimental Psychology: Applied 25(1), 1–24 (2019)
18. Debarba, H.G., Boulic, R., Salomon, R., Blanke, O., Her-
belin, B.: Self-attribution of distorted reaching move-
ments in immersive virtual reality. Computers & Graph-
ics 76, 142–152 (2018)

19. Ellis, R., Tucker, M.: Micro-aﬀordance: The potentiation
of components of action by seen objects. British journal
of psychology 91(4), 451–471 (2000)

20. Eves, F.F.: Is there any proﬃtt in stair climbing? a head-
count of studies testing for demographic diﬀerences in
choice of stairs. Psychonomic Bulletin & Review 21(1),
71–77 (2014)

21. Farn`e, A., Iriki, A., L`adavas, E.: Shaping multisensory
action–space with tools: evidence from patients with
cross-modal extinction. Neuropsychologia 43(2), 238–248
(2005)

22. Ferri, F., Riggio, L., Gallese, V., Costantini, M.: Objects
and their nouns in peripersonal space. Neuropsychologia
49(13), 3519–3524 (2011)

23. Fitts, P.M., Seeger, C.M.: Sr compatibility: spatial char-
acteristics of stimulus and response codes. Journal of
experimental psychology 46(3), 199–210 (1953)

24. Fizek, S., Wasilewska, M.: 4 embodiment and gender
identity in virtual worlds. Creating Second Lives: Com-
munity, Identity and Spatiality as Constructions of the
Virtual pp. 75–98 (2011)

25. Fleming, J., Klatzky, R.L., Behrmann, M.: Time course
of planning for object and action parameters in visually
guided manipulation. Visual Cognition 9(4-5), 502–527
(2002)

26. Gibson, J.J.: The ecological approach to the visual per-
ception of pictures. Leonardo 11(3), 227–235 (1978)
27. Gonzalez-Franco, M., Peck, T.C.: Avatar embodiment.
Frontiers in

towards a standardized questionnaire.
Robotics and AI 5(74), 1–9 (2018)

28. Grefkes, C., Fink, G.R.: The functional organization of
the intraparietal sulcus in humans and monkeys. Journal
of anatomy 207(1), 3–17 (2005)

29. Handy, T.C., Grafton, S.T., Shroﬀ, N.M., Ketay, S., Gaz-
zaniga, M.S.: Graspable objects grab attention when the
potential for action is recognized. Nature neuroscience
6(4), 421–427 (2003)

30. Harris, L.R., Carnevale, M.J., DAmour, S., Fraser, L.E.,
Harrar, V., Hoover, A.E., Mander, C., Pritchett, L.M.:
How our body inﬂuences our perception of the world.
Frontiers in psychology 6, 1–10 (2015)

31. Jamone, L., Ugur, E., Cangelosi, A., Fadiga, L.,
Bernardino, A., Piater, J., Santos-Victor, J.: Aﬀordances
in psychology, neuroscience, and robotics: A survey.
IEEE Transactions on Cognitive and Developmental Sys-
tems 10(1), 4–25 (2016)

32. Jeannerod, M., Arbib, M., Rizzolatti, G., Sakata, H.:
Grasping objects: the cortical mechanisms. Trends Neu-
rosci 18, 314–332 (1995)

33. Kalaska, J.F.: Parietal cortex area 5 and visuomotor be-
havior. Canadian journal of physiology and pharmacol-
ogy 74(4), 483–498 (1996)

34. Kalaska, J.F., Crammond, D.J.: Deciding not to go: neu-
ronal correlates of response selection in a go/nogo task in
primate premotor and parietal cortex. Cerebral Cortex
5(5), 410–428 (1995)

35. Kilteni, K., Groten, R., Slater, M.: The sense of embodi-
ment in virtual reality. Presence: Teleoperators and Vir-
tual Environments 21(4), 373–387 (2012)

36. Kostov, K.: The handle orientation eﬀect: critical atten-
tional factors that have received little to no attention:
Authors extended abstract of the thesis submitted in par-
tial fulﬁllment of the requirements for the degree of ph.
d. in psychology. Ph.D. thesis, New Bulgarian University
(2017)

37. Kostov, K., Janyan, A.: Reversing the aﬀordance eﬀect:
negative stimulus–response compatibility observed with
images of graspable objects. Cognitive processing 16(1),
287–291 (2015)

38. Lanier, J.: Homuncular ﬂexibility. Edge: The World

Question Center (2006)

39. Le Ch´en´echal, M., Duval, T., Lacoche, J., Gouranton, V.,
Royan, J., Arnaldi, B.: When the giant meets the ant an
asymmetric approach for collaborative object manipula-
tion. In: 2016 IEEE Symposium on 3D User Interfaces
(3DUI), pp. 277–278. IEEE (2016)

40. Lee, P.W., Wang, H.Y., Tung, Y.C., Lin, J.W., Valstar,
A.: Transection: hand-based interaction for playing a
game within a virtual reality game. In: Proceedings of the
33rd Annual ACM Conference Extended Abstracts on
Human Factors in Computing Systems, pp. 73–76 (2015)
41. Lessard, D.A., Linkenauger, S.A., Proﬃtt, D.R.: Look
before you leap: Jumping ability aﬀects distance percep-
tion. Perception 38(12), 1863–1866 (2009)

42. Mark, L.S., Vogele, D.: A biodynamic basis for perceived
categories of action: A study of sitting and stair climbing.
Journal of Motor Behavior 19(3), 367–384 (1987)

Avatar representation aﬀects the perception of available action possibilities in Virtual Reality

15

62. Turvey, M.T.: Aﬀordances and prospective control: An
outline of the ontology. Ecological psychology 4(3), 173–
187 (1992)

63. Vainio, L.: Negative stimulus–response compatibility ob-
served with a brieﬂy displayed image of a hand. Brain
and Cognition 77(3), 382–390 (2011)

64. Vainio, L., Symes, E., Ellis, R., Tucker, M., Ottoboni, G.:
On the relations between action planning, object identi-
ﬁcation, and motor representations of observed actions
and objects. Cognition 108(2), 444–465 (2008)

65. Warren, W.H.: Perceiving aﬀordances: Visual guidance of
stair climbing. Journal of experimental psychology: Hu-
man perception and performance 10(5), 683–703 (1984)
66. Warren Jr, W.H., Whang, S.: Visual guidance of walk-
ing through apertures: body-scaled information for af-
fordances. Journal of experimental psychology: human
perception and performance 13(3), 371–383 (1987)
67. Witt, J.K., Proﬃtt, D.R., Epstein, W.: Tool use aﬀects
perceived distance, but only when you intend to use it.
Journal of experimental psychology: Human perception
and performance 31(5), 880–888 (2005)

68. Won, A.S., Bailenson, J., Lee, J., Lanier, J.: Homuncu-
lar ﬂexibility in virtual reality. Journal of Computer-
Mediated Communication 20(3), 241–259 (2015)

69. Yamani, Y., Ariga, A., Yamada, Y.: Object aﬀordances
potentiate responses but do not guide attentional priori-
tization. Frontiers in integrative neuroscience 9(74), 1–6
(2016)

70. Yu, A.B., Abrams, R.A., Zacks, J.M.: Limits on action
priming by pictures of objects. Journal of Experimental
Psychology: Human Perception and Performance 40(5),
1861–1873 (2014)

43. Miura, T., Urakawa, S., Isojima, M., Yu, J., Yoshii, A.,
Nakajima, T.: Natural user interaction requires good af-
fordance when using with a head-mounted display.
In:
Proceedings of the Eighth International Conferences on
Advances in Multimedia (2016)

44. No¨e, A., No¨e, A., et al.: Action in perception. MIT press

(2004)

45. O’Regan, J.K., No¨e, A.: A sensorimotor account of vision
and visual consciousness. Behavioral and brain sciences
24(5), 939–973 (2001)

46. Pfeuﬀer, K., Mayer, B., Mardanbegi, D., Gellersen, H.:
Gaze+ pinch interaction in virtual reality. In: Proceed-
ings of the 5th Symposium on Spatial User Interaction,
pp. 99–108 (2017)

47. Ren, S., Sun, Y.: Human-object-object-interaction af-
In: 2013 IEEE Workshop on Robot Vision

fordance.
(WORV), pp. 1–6. IEEE (2013)

48. Rizzolatti, G., Luppino, G., Matelli, M.: The organiza-
tion of the cortical motor system: new concepts. Elec-
troencephalography and clinical neurophysiology 106(4),
283–296 (1998)

49. Rosenbaum, D.A.: Human movement initiation: speciﬁ-
cation of arm, direction, and extent. Journal of Experi-
mental Psychology: General 109(4), 444–474 (1980)
50. Roth, D., Lugrin, J.L., Galakhov, D., Hofmann, A.,
Bente, G., Latoschik, M.E., Fuhrmann, A.: Avatar re-
alism and social interaction quality in virtual reality.
In: 2016 IEEE Virtual Reality (VR), pp. 277–278. IEEE
(2016)

51. Salmanowitz, N.: The impact of virtual reality on implicit
racial bias and mock legal decisions. Journal of Law and
the Biosciences 5(1), 174–203 (2018)

52. Serino, A., Bassolino, M., Farne, A., Ladavas, E.: Ex-
tended multisensory space in blind cane users. Psycho-
logical science 18(7), 642–648 (2007)

53. Sikstr¨om, E., De G¨otzen, A., Seraﬁn, S.: The role of sound
in the sensation of ownership of a pair of virtual wings in
immersive vr. In: Proceedings of the 9th Audio Mostly:
A Conference on Interaction With Sound, pp. 1–6 (2014)
54. Simon, J.R.: Reactions toward the source of stimula-
tion. Journal of experimental psychology 81(1), 174–176
(1969)

55. Sirigu, A., Grafman, J., Bressler, K., Sunderland, T.:
Multiple representations contribute to body knowledge
processing: Evidence from a case of autotopagnosia.
Brain 114(1), 629–642 (1991)

56. Slater, M., P´erez Marcos, D., Ehrsson, H., Sanchez-Vives,
M.V.: Towards a digital body: the virtual arm illusion.
Frontiers in human neuroscience 2, 1–6 (2008)

57. Snyder, L.H., Batista, A.P., Andersen, R.A.: Intention-
related activity in the posterior parietal cortex: a review.
Vision research 40(10-12), 1433–1441 (2000)

58. Sposito, A., Bolognini, N., Vallar, G., Maravita, A.: Ex-
tension of perceived arm length following tool-use: clues
to plasticity of body metrics. Neuropsychologia 50(9),
2187–2194 (2012)

59. Tipper, S.P., Paul, M.A., Hayes, A.E.: Vision-for-action:
The eﬀects of object property discrimination and action
state on aﬀordance compatibility eﬀects. Psychonomic
bulletin & review 13(3), 493–498 (2006)

60. Tucker, M., Ellis, R.: On the relations between seen ob-
jects and components of potential actions. Journal of
Experimental Psychology: Human perception and per-
formance 24(3), 830–846 (1998)

61. Tucker, M., Ellis, R.: Action priming by brieﬂy presented
objects. Acta psychologica 116(2), 185–203 (2004)

16

5 Appendix A

6 Appendix B

Tugce Akkoc et al.

Appendix A

Body Ownership Questionnaire

for Able Hands

Appendix B

Body Ownership Questionnaire

for Restricted Hands

1. I felt as if the virtual hands that I saw during the

experiment were part of my body.

1. I felt as if the virtual capsule hands that I saw
during the experiment were part of my body.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

2. I felt as if the virtual hands were moving

2. I felt as if the virtual capsule hands were moving

independently of my movements.

independently of my movements.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

3. I felt as if my body was immersed in the virtual

3. I felt as if my body was immersed in the virtual

environment.

environment.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

4. I felt as if the virtual hands were someone elses

4. I felt as if the virtual capsule hands were someone

hands.

elses hands.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

5. I felt as if I was the one who is controlling the

5. I felt as if I was the one who is controlling the

virtual hands.

virtual capsule hands.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

6. During the experiment, I felt as if was watching the

6. During the experiment, I felt as if I was watching

scene from a third-person perspective.

the scene from a third-person perspective.

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

Strongly Disagree 0 1 2 3 4 5 6 7 8 9 10 Strongly Agree

