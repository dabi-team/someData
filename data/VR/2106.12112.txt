2
2
0
2

r
a

M
6
1

]

G
L
.
s
c
[

3
v
2
1
1
2
1
.
6
0
1
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2022

BREGMAN GRADIENT POLICY OPTIMIZATION

Feihu Huang∗ †, Shangqian Gao∗, Heng Huang†
Department of Electrical and Computer Engineering
University of Pittsburgh
Pittsburgh, PA 15261, USA
huangfeihu2018@gmail.com, shg84@pitt.edu, heng.huang@pitt.edu

ABSTRACT

In the paper, we design a novel Bregman gradient policy optimization framework
for reinforcement learning based on Bregman divergences and momentum tech-
niques. Speciﬁcally, we propose a Bregman gradient policy optimization (BGPO)
algorithm based on the basic momentum technique and mirror descent iteration.
Meanwhile, we further propose an accelerated Bregman gradient policy optimiza-
tion (VR-BGPO) algorithm based on the variance reduced technique. Moreover,
we provide a convergence analysis framework for our Bregman gradient policy
optimization under the nonconvex setting. We prove that our BGPO achieves a
sample complexity of O((cid:15)−4) for ﬁnding (cid:15)-stationary policy only requiring one
trajectory at each iteration, and our VR-BGPO reaches the best known sample
complexity of O((cid:15)−3), which also only requires one trajectory at each iteration.
In particular, by using different Bregman divergences, our BGPO framework uni-
ﬁes many existing policy optimization algorithms such as the existing (variance
reduced) policy gradient algorithms such as natural policy gradient algorithm. Ex-
tensive experimental results on multiple reinforcement learning tasks demonstrate
the efﬁciency of our new algorithms.

1

INTRODUCTION

Policy Gradient (PG) methods are a class of popular policy optimization methods for Reinforce-
ment Learning (RL), and have achieved signiﬁcant successes in many challenging applications (Li,
2017) such as robot manipulation (Deisenroth et al., 2013), the Go game (Silver et al., 2017) and
autonomous driving (Shalev-Shwartz et al., 2016). In general, PG methods directly search for the
optimal policy by maximizing the expected total reward of Markov Decision Processes (MDPs) in-
volved in RL, where an agent takes action dictated by a policy in an unknown dynamic environment
over a sequence of time steps. Since the PGs are generally estimated by Monte-Carlo sampling,
such vanilla PG methods usually suffer from very high variances resulted in slow convergence rate
and destabilization. Thus, recently many fast PG methods have been proposed to reduce variances
in vanilla stochastic PGs. For example, Sutton et al. (2000) introduced a baseline to reduce vari-
ances of the stochastic PG. Konda & Tsitsiklis (2000) proposed an efﬁcient actor-critic algorithm
by estimating the value function to reduce effects of large variances. (Schulman et al., 2016) pro-
posed the generalized advantage estimation (GAE) to control both the bias and variance in policy
gradient. More recently, some faster variance-reduced PG methods (Papini et al., 2018; Xu et al.,
2019a; Shen et al., 2019; Liu et al., 2020; Huang et al., 2020) have been developed based on the
variance-reduction techniques in stochastic optimization.

Alternatively, some successful PG algorithms (Schulman et al., 2015; 2017) improve convergence
rate and robustness of vanilla PG methods by using some penalties such as Kullback-Leibler (KL)
divergence penalty. For example, trust-region policy optimization (TRPO) (Schulman et al., 2015)
ensures that the new selected policy is near to the old one by using KL-divergence constraint, while
proximal policy optimization (PPO) (Schulman et al., 2017) clips the weighted likelihood ratio
to implicitly reach this goal. Subsequently, Shani et al. (2020) have analyzed the global conver-
gence properties of TRPO in tabular RL based on the convex mirror descent algorithm. Liu et al.

∗Feihu and Shangqian contributed equally.
†Corresponding Authors.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2022

Table 1: Sample complexities of the representative PG algorithms based on mirror descent algo-
rithm for ﬁnding an (cid:15)-stationary policy of the nonconcave performance function. Although Liu
et al. (2019); Shani et al. (2020) have provided the global convergence of TRPO and PPO under
some speciﬁc policies based on convex mirror descent, they still obtain a stationary point of noncon-
cave performance function. Note that our convergence analysis does not rely any speciﬁc policies.

Algorithm
TRPO

Reference
Shani et al. (2020)
Regularized TRPO Shani et al. (2020)
Liu et al. (2019)
Yang et al. (2019)
Tomar et al. (2020)
Ours
Ours

TRPO/PPO
VRMPO
MDPO
BGPO
VR-BGPO

Complexity Batch Size

O((cid:15)−4)
O((cid:15)−3)
O((cid:15)−8)
O((cid:15)−3)
Unknown
O((cid:15)−4)
O((cid:15)−3)

O((cid:15)−2)
O((cid:15)−2)
O((cid:15)−6)
O((cid:15)−2)
Unknown
O(1)
O(1)

(2019) have also studied the global convergence properties of PPO and TRPO equipped with over-
parametrized neural networks based on mirror descent iterations. At the same time, Yang et al.
(2019) tried to propose the PG methods based on the mirror descent algorithm. More recently, mir-
ror descent policy optimization (MDPO) (Tomar et al., 2020) iteratively updates the policy beyond
the tabular RL by approximately solving a trust region problem based on convex mirror descent
algorithm. In addition, Agarwal et al. (2019); Cen et al. (2020) have studied the natural PG methods
for regularized RL. However, Agarwal et al. (2019) mainly focuses on tabular policy and log-linear,
neural policy classes. Cen et al. (2020) mainly focuses on softmax policy class.

Although these speciﬁc PG methods based on mirror descent iteration have been recently studied,
which are scattered in empirical and theoretical aspects respectively, it lacks a universal framework
for these PG methods without relying on some speciﬁc RL tasks.
In particular, there still does
not exist the convergence analysis of PG methods based on the mirror descent algorithm under the
nonconvex setting. Since mirror descent iteration adjusts gradient updates to ﬁt problem geometry,
and is useful in regularized RL (Geist et al., 2019), there exists an important problem to be addressed:

Could we design a universal policy optimization framework based on the mirror descent
algorithm, and provide its convergence guarantee under the non-convex setting ?

In the paper, we ﬁrmly answer the above challenging question with positive solutions and propose an
efﬁcient Bregman gradient policy optimization framework based on Bregman divergences and mo-
mentum techniques. In particular, we provide a convergence analysis framework of the PG methods
based on mirror descent iteration under the nonconvex setting. In summary, our main contributions
are provided as follows:

a) We propose an effective Bregman gradient policy optimization (BGPO) algorithm based
on the basic momentum technique, which achieves the sample complexity of O((cid:15)−4) for
ﬁnding (cid:15)-stationary policy only requiring one trajectory at each iteration.

b) We propose an accelerated Bregman gradient policy optimization (VR-BGPO) algorithm
based on the variance-reduced technique of STORM (Cutkosky & Orabona, 2019). More-
over, we prove that the VR-BGPO reaches the best known sample complexity of O((cid:15)−3).
c) We design a uniﬁed policy optimization framework based on mirror descent iteration and
momentum techniques, and provide its convergence analysis under nonconvex setting.

In Table 1 shows that sample complexities of the representative PG algorithms based on mirror
descent algorithm. Shani et al. (2020); Liu et al. (2019) have established global convergence of a
mirror descent variant of PG under some pre-speciﬁed setting such as over-parameterized networks
(Liu et al., 2019) by exploiting these speciﬁc problems’ hidden convex nature. Without these special
structures, global convergence of these methods cannot be achieved. However, our framework does
not rely on any speciﬁc policy classes, and our convergence analysis only builds on the general
nonconvex setting. Thus, we only prove that our methods convergence to stationary points.

Geist et al. (2019); Jin & Sidford (2020); Lan (2021); Zhan et al. (2021) studied a general theory
of regularized MDPs based on policy space such as a discrete probability space that generally is
discontinuous. Since both the state and action spaces S and A generally are very large in practice,
the policy space is large. While our methods build on policy’ parameter space that is generally
continuous Euclidean space and relatively small. Clearly, our methods and theoretical results are
more practical than the results in (Geist et al., 2019; Jin & Sidford, 2020; Lan, 2021; Zhan et al.,
2021). (Tomar et al., 2020) also proposes mirror descent PG framework based on policy parameter

2

Published as a conference paper at ICLR 2022

space, but it does not provide any theoretical results and only focuses on Bregman divergence taking
form of KL divergence. While our framework can collaborate with any Bregman divergence forms.

2 RELATED WORKS

In this section, we review some related works about mirror descent-based algorithms in RL and
variance-reduced PG methods, respectively.

2.1 MIRROR DESCENT ALGORITHM IN RL
Due to easily deal with the regularization terms, mirror descent (a.k.a., Bregman gradient) algorithm
(Censor & Zenios, 1992; Beck & Teboulle, 2003) has shown signiﬁcant successes in regularized RL,
which is ﬁrst proposed in (Censor & Zenios, 1992) based on Bregman distance (divergence) (Breg-
man, 1967; Censor & Lent, 1981). For example, Neu et al. (2017) have shown both the dynamic
policy programming (Azar et al., 2012) and TRPO (Schulman et al., 2015) algorithms are approx-
imate variants of mirror descent algorithm. Subsequently, Geist et al. (2019) have introduced a
general theory of regularized MDPs based on the convex mirror descent algorithm. More recently,
Liu et al. (2019) have studied the global convergence properties of PPO and TRPO equipped with
overparametrized neural networks based on mirror descent iterations. At the same time, Shani et al.
(2020) have analyzed the global convergence properties of TRPO in tabular policy based on the
convex mirror descent algorithm. Wang et al. (2019) have proposed divergence augmented policy
optimization for off-policy learning based on mirror descent algorithm. MDPO (Tomar et al., 2020)
iteratively updates the policy beyond the tabular RL by approximately solving a trust region problem
based on convex mirror descent algorithm.

(VARIANCE-REDUCED) PG METHODS

2.2
PG methods have been widely studied due to their stability and incremental nature in policy op-
timization. For example, the global convergence properties of vanilla policy gradient method in
inﬁnite-horizon MDPs have been recently studied in (Zhang et al., 2019). Subsequently, Zhang et al.
(2020) have studied asymptotically global convergence properties of the REINFORCE (Williams,
1992), whose policy gradient is approximated by using a single trajectory or a ﬁxed size mini-batch
of trajectories under soft-max parametrization and log-barrier regularization. To accelerate these
vanilla PG methods, some faster variance-reduced PG methods have been proposed based on the
variance-reduction techniques of SVRG (Johnson & Zhang, 2013), SPIDER (Fang et al., 2018) and
STORM (Cutkosky & Orabona, 2019) in stochastic optimization. For example, fast SVRPG (Papini
et al., 2018; Xu et al., 2019a) algorithm have been proposed based on SVRG. Fast HAPG (Shen
et al., 2019) and SRVR-PG (Xu et al., 2019a) algorithms have been presented by using SPIDER
technique. Subsequently, the momentum-based PG methods, i.e., ProxHSPGA (Pham et al., 2020)
and IS-MBPG (Huang et al., 2020), have been developed based on variance-reduced technique of
STORM/Hybrid-SGD (Cutkosky & Orabona, 2019; Tran-Dinh et al., 2019). More recently, (Ding
et al., 2021) studied the global convergence of momentum-based policy gradient methods. (Zhang
et al., 2021) proposed a truncated stochastic incremental variance-reduced policy gradient (TSIVR-
PG) method to relieve the uncheckable importance weight assumption in above variance-reduced
PG methods and provided the global convergence of the TSIVR-PG under overparameterizaiton of
policy assumption.

3

PRELIMINARIES

In the section, we will review some preliminaries of Markov decision process and policy gradients.

3.1 NOTATIONS
Let [n] = {1, 2, · · · , n} for all n ∈ N+. For a vector x ∈ Rd, let (cid:107)x(cid:107) denote the (cid:96)2 norm of x, and
(cid:107)x(cid:107)p = (cid:0) (cid:80)d
(p ≥ 1) denotes the p-norm of x. For two sequences {ak} and {bk}, we
denote ak = O(bk) if ak ≤ Cbk for some constant C > 0. E[X] and V[X] denote the expectation
and variance of random variable X, respectively.

i=1 |xi|p(cid:1)1/p

3.2 MARKOV DECISION PROCESS
Reinforcement learning generally involves a discrete time discounted Markov Decision Process
(MDP) deﬁned by a tuple {S, A, P, r, γ, ρ0}. S and A denote the state and action spaces of the

3

Published as a conference paper at ICLR 2022

agent, respectively. P(s(cid:48)|s, a) : S × A → (cid:52)(S) is the Markov kernel that determines the transition
probability from the state s to s(cid:48) under taking an action a ∈ A. r(s, a) : S × A → [−R, R] (R > 0)
is the reward function of s and a, and ρ0 = p(s0) denotes the initial state distribution. γ ∈ (0, 1) is
the discount factor. Let π : S → (cid:52)(A) be a stationary policy, where (cid:52)(A) is the set of probability
distributions on A.

Given the current state st ∈ S, the agent executes an action at ∈ A following a conditional proba-
bility distribution π(at|st), and then the agent obtains a reward rt = r(st, at). At each time t, we
can deﬁne the state-action value function Qπ(st, at) and state value function V π(st) as follows:

Qπ(st, at) = Est+1,at+1,...

∞
(cid:88)

(cid:2)

l=0

γlrt+l

(cid:3), V π(st) = Eat,st+1,...

(cid:2)

∞
(cid:88)

l=0

γlrt+l

(cid:3).

(1)

We also deﬁne the advantage function Aπ(st, at) = Qπ(st, at) − V π(st). The goal of the agent is
to ﬁnd the optimal policy by maximizing the expected discounted reward
max
π
Given a time horizon H, the agent collects a trajectory τ = {st, at}H−1
t=0 under any stationary policy.
Then the agent obtains a cumulative discounted reward r(τ ) = (cid:80)H−1
t=0 γtr(st, at). Since the state
and action spaces S and A are generally very large, directly solving the problem (2) is difﬁcult.
Thus, we let the policy π be parametrized as πθ for the parameter θ ∈ Θ ⊆ Rd. Given the initial
distribution ρ0 = p(s0), the probability distribution over trajectory τ can be obtained

J(π) := Es0∼ρ0[V π(s0)].

(2)

p(τ |θ) = p(s0)

H−1
(cid:89)

t=0

P(st+1|st, at)πθ(at|st).

Thus, the problem (2) will be equivalent to maximize the expected discounted trajectory reward:

max
θ∈Θ
In fact, the above objective function J(θ) has a truncation error of O( γH
inﬁnite-horizon MDP.

J(θ) := Eτ ∼p(τ |θ)[r(τ )].

1−γ ) compared to the original

POLICY GRADIENTS

3.3
The policy gradient methods (Williams, 1992; Sutton et al., 2000) are a class of effective policy-
based methods to solve the above RL problem (4). Speciﬁcally, the gradient of J(θ) with respect to
θ is given as follows:

Given a mini-batch trajectories B = {τi}n
stochastic policy gradient ascent update at (k + 1)-th step, deﬁned as

∇J(θ) = Eτ ∼p(τ |θ)

(cid:2)∇ log (cid:0)p(τ |θ)(cid:1)r(τ )(cid:3).

(5)
i=1 sampled from the distribution p(τ |θ), the standard

θk+1 = θk + η∇JB(θk),
(cid:80)n

where η > 0 is learning rate, and ∇JB(θk) = 1
n
H = O( 1
gradient of J(θ), i.e., E[g(τ |θ)] = ∇J(θ), where

i=1 g(τi|θk) is stochastic policy gradient. Given
1−γ ) as in (Zhang et al., 2019; Shani et al., 2020), g(τ |θ) is the unbiased stochastic policy

g(τ |θ) = (cid:0)

H−1
(cid:88)

t=0

∇θ log πθ(at, st)(cid:1)(cid:0)

H−1
(cid:88)

t=0

γtr(st, at)(cid:1).

(7)

Based on the gradient estimator in (7), we can obtain the existing well-known policy gradient esti-
mators such as REINFORCE (Williams, 1992), policy gradient theorem (PGT (Sutton et al., 2000)).
Speciﬁcally, the REINFORCE obtains a policy gradient estimator by adding a baseline b, deﬁned as

g(τ |θ) = (cid:0)

H−1
(cid:88)

t=0

∇θ log πθ(at, st)(cid:1)(cid:0)

H−1
(cid:88)

t=0

γtr(st, at) − bt

(cid:1).

The PGT is a version of the REINFORCE, deﬁned as

g(τ |θ) =

H−1
(cid:88)

H−1
(cid:88)

t=0

j=t

(cid:0)γjr(sj, aj) − bj

(cid:1)∇θ log πθ(at, st).

4

(3)

(4)

(6)

Published as a conference paper at ICLR 2022

Algorithm 1 BGPO Algorithm
1: Input: Total iteration K, tuning parameters {λ, b, m, c} and mirror mappings (cid:8)ψk

ν-strongly convex functions;

(cid:9)K
k=1 are

2: Initialize: θ1 ∈ Θ, and sample a trajectory τ1 from p(τ |θ1), and compute u1 = −g(τ1|θ1);
3: for k = 1, 2, . . . , K do
4:

Update ˜θk+1 = arg minθ∈Θ
Update θk+1 = θk + ηk(˜θk+1 − θk) with ηk =
Sample a trajectory τk+1 from p(τ |θk+1), and compute uk+1 = −βk+1g(τk+1|θk+1) + (1 −
βk+1)uk with βk+1 = cηk;

λ Dψk (θ, θk)(cid:9);

(cid:8)(cid:104)uk, θ(cid:105) + 1

b
(m+k)1/2 ;

5:

6:

7: end for
8: Output: θζ chosen uniformly random from {θk}K

k=1.

4 BREGMAN GRADIENT POLICY OPTIMIZATION

In this section, we propose a novel Bregman gradient policy optimization framework based on Breg-
man divergences and momentum techniques. We ﬁrst let f (θ) = −J(θ), the goal of policy-based
RL is to solve the problem: maxθ∈Θ J(θ) ⇐⇒ minθ∈Θ f (θ), so we have ∇f (θ) = −∇J(θ).

Assume ψ(x) is a continuously-differentiable and ν-strongly convex function, i.e., (cid:104)x − y, ∇ψ(x) −
∇ψ(y)(cid:105) ≥ ν(cid:107)x − y(cid:107), ν > 0, we deﬁne a Bregman distance:

Dψ(y, x) = ψ(y) − ψ(x) − (cid:104)∇ψ(x), y − x(cid:105), ∀x, y ∈ Rd
(8)
Then given a function h(x) deﬁned on a closed convex set X , we deﬁne a proximal operator (a.k.a.,
mirror descent):

P ψ

λ,h(x) = arg min
y∈X

(cid:8)h(y) +

Dψ(y, x)(cid:9),

1
λ

(9)

where λ > 0. Based on this proximal operator P ψ
we can deﬁne a Bregman gradient of function h(x) as follows:
λ,h(x)(cid:1).

(cid:0)x − P ψ

λ,h(x) =

Bψ

1
λ

λ,h as in (Ghadimi et al., 2016; Zhang & He, 2018),

(10)

If ψ(x) = 1
0. Thus, this Bregman gradient can be regarded as a generalized gradient.

2 (cid:107)x(cid:107)2 and X = Rd, x∗ is a stationary point of h(x) if and only if Bψ

λ,h(x∗) = ∇h(x∗) =

4.1 BGPO ALGORITHM
In the subsection, we propose a Bregman gradient policy optimization (BGPO) algorithm based on
the basic momentum technique. The pseudo code of BGPO Algorithm is provided in Algorithm 1.

In Algorithm 1, the step 4 uses the stochastic Bregman gradient descent (a.k.a., stochastic mirror
descent) to update the parameter θ. Let h(θ) = (cid:104)θ, uk(cid:105) be the ﬁrst-order approximation of function
f (θ) at θk, where uk is an approximated gradient of function f (θ) at θk. By the step 4 of Algorithm
1 and the above equality (10), we have

Bψk

λ,h(θk) =

(cid:0)θk − ˜θk+1

(cid:1),

1
λ

(11)

where λ > 0. Then by the step 5 of Algorithm 1, we have
θk+1 = θk − ληkBψk

(12)
where 0 < ηk ≤ 1. Due to the convexity of set Θ ⊆ Rd and θ1 ∈ Θ, we choose the parameter
ηk ∈ (0, 1] to ensure the updated sequence {θk}K

λ,h(θk),

k=1 in Θ.

In fact, our BGPO algorithm uniﬁes many popular policy optimization algorithms. When the mir-
ror mappings ψk(θ) = 1
2 (cid:107)θ(cid:107)2 for ∀k ≥ 1, the update (12) will be equivalent to a classic policy
gradient iteration. Then our BGPO algorithm will become a momentum version of the policy gra-
dient algorithms (Sutton et al., 2000; Zhang et al., 2019). Given ψk(θ) = 1
2 (cid:107)θ(cid:107)2 and βk = 1, i.e.,
uk = −g(τk|θk), we have Bψk

λ,h(θk) = −g(τk|θk) and

θk+1 = θk + ληkg(τk|θk).

(13)

5

Published as a conference paper at ICLR 2022

Algorithm 2 VR-BGPO Algorithm
1: Input: Total iteration K, tuning parameters {λ, b, m, c} and mirror mappings (cid:8)ψk

ν-strongly convex functions;

(cid:9)K
k=1 are

2: Initialize: θ1 ∈ Θ, and sample a trajectory τ1 from p(τ |θ1), and compute u1 = −g(τ1|θ1);
3: for k = 1, 2, . . . , K do
4:

Update ˜θk+1 = arg minθ∈Θ
Update θk+1 = θk + ηk(˜θk+1 − θk) with ηk =
Sample a trajectory τk+1 from p(τ |θk+1), and compute uk+1 = −βk+1g(τk+1|θk+1) + (1 −
βk+1)(cid:2)uk − g(τk+1|θk+1) + w(τk+1|θk, θk+1)g(τk+1|θk)(cid:3) with βk+1 = cη2
k;

λ Dψk (θ, θk)(cid:9);

(cid:8)(cid:104)uk, θ(cid:105) + 1

b
(m+k)1/3 ;

5:

6:

7: end for
8: Output: θζ chosen uniformly random from {θk}K

k=1.

2 θT F (θk)θ with F (θk) = E(cid:2)∇θπθk (s, a)(cid:0)∇θπθk (s, a)(cid:1)T (cid:3),
When the mirror mappings ψk(θ) = 1
the update (12) will be equivalent to a natural policy gradient iteration. Then our BGPO will become
a momentum version of natural policy gradient algorithms (Kakade, 2001; Liu et al., 2020). Given
ψk(θ) = 1

2 θT F (θk)θ, βk = 1, i.e., uk = −g(τk|θk), we have Bψk
θk+1 = θk + ληkF (θk)+g(τk|θk),
(14)
where F (θk)+ denotes the Moore-Penrose pseudoinverse of the Fisher information matrix F (θk).
When given the mirror mapping ψk(θ) = (cid:80)
s∈S πθ(s) log(πθ(s)), i.e., Boltzmann-Shannon en-
tropy function (Shannon, 1948) and Θ = (cid:8)θ ∈ Rd | (cid:80)
s∈S πθ(s) = 1(cid:9), we have Dψk (θ, θk) =
(cid:1), which is the KL divergence. Then our BGPO will
KL(cid:0)πθ(s), πθk (s)(cid:1) = (cid:80)
s∈S πθ(s) log (cid:0) πθ(s)
πθk (s)
become a momentum version of mirror descent policy optimization (Tomar et al., 2020).

λ,h(θk) = −F (θk)+g(τk|θk) and

4.2 VR-BGPO ALGORITHM
In the subsection, we propose a faster variance-reduced Bregman gradient policy optimization (VR-
BGPO) algorithm based on a variance-reduced technique. The pseudo code of VR-BGPO algorithm
is provided in Algorithm 2.

Consider the problem (4) is non-oblivious that the distribution p(τ |θ) depends on the variable θ
varying through the whole optimization procedure, we apply the importance sampling weight (Pap-
ini et al., 2018; Xu et al., 2019a) in estimating our policy gradient uk+1, deﬁned as

w(τk+1|θk, θk+1) =

p(τk+1|θk)
p(τk+1|θk+1)

=

H−1
(cid:89)

t=0

πθk (at|st)
πθk+1(at|st)

.

Except for different stochastic policy gradients {uk} and tuning parameters {ηk, βk} using in Al-
gorithms 1 and 2, the steps 4 and 5 in these algorithms for updating parameter θ are the same.
Interestingly, when choosing mirror mapping ψk(θ) = 1
2 (cid:107)θ(cid:107)2, our VR-BGPO algorithm will reduce
to a non-adaptive version of IS-MBPG algorithm (Huang et al., 2020).

5 CONVERGENCE ANALYSIS

In this section, we will analyze the convergence properties of our BGPO and VR-BGPO algorithms.
All related proofs are provided in the Appendix A. Here we use the standard convergence metric
(cid:107)Bψk
λ,(cid:104)θ,∇f (θk)(cid:105)(θ)(cid:107) used in (Zhang & He, 2018; Yang et al., 2019) to evaluate the convergence Breg-
man gradient-based (a.k.a., mirror descent) algorithms. To give the convergence analysis, we ﬁrst
give some standard assumptions.
Assumption 1. For function log πθ(a|s), its gradient and Hessian matrix are bounded, i.e., there
exist constants Cg, Ch > 0 such that (cid:107)∇θ log πθ(a|s)(cid:107) ≤ Cg, (cid:107)∇2
Assumption 2. Variance of stochastic gradient g(τ |θ) is bounded, i.e., there exists a constant σ > 0,
for all πθ such that V(g(τ |θ)) = E(cid:107)g(τ |θ) − ∇J(θ)(cid:107)2 ≤ σ2.
Assumption 3. For importance sampling weight w(τ |θ1, θ2) = p(τ |θ1)/p(τ |θ2), its variance is
bounded, i.e., there exists a constant W > 0, it follows V(w(τ |θ1, θ2)) ≤ W for any θ1, θ2 ∈ Rd
and τ ∼ p(τ |θ2).

θ log πθ(a|s)(cid:107) ≤ Ch.

6

Published as a conference paper at ICLR 2022

Assumption 4. The function J(θ) has an upper bound in Θ, i.e., J ∗ = supθ∈Θ J(θ) < +∞.

Assumptions 1 and 2 are commonly used in the PG algorithms (Papini et al., 2018; Xu et al.,
2019a;b). Assumption 3 is widely used in the study of variance reduced PG algorithms (Papini
et al., 2018; Xu et al., 2019a). In fact, the bounded importance sampling weight might be violated in
some cases such as using neural networks as the policy. Thus, we can clip this importance sampling
weights to guarantee the effectiveness of our algorithms as in (Papini et al., 2018). At the same time,
the importance weights actually also have some nice properties, e.g., in soft-max policy it is bounded
by ec(cid:107)θ1−θ2(cid:107)2
for all θ1, θ2 ∈ Θ. More recently, (Zhang et al., 2021) used a simple truncated update
to relieve this uncheckable importance weight assumption. Assumption 4 guarantees the feasibility
of the problem (4). Note that Assumptions 2 and 4 are satisﬁed automatically given Assumption 1
and the fact that all the rewards are bounded, i.e., |r(s, a)| ≤ R for any s ∈ S and a ∈ A. For
example, due to |r(s, a)| ≤ R, we have |J(θ)| ≤ R

1−γ . So we have J ∗ = R

1−γ .

5.1 CONVERGENCE ANALYSIS OF BGPO ALGORITHM
In the subsection, we provide convergence properties of the BGPO algorithm. The detailed proof is
provided in Appendix A.1.
Theorem 1. Assume the sequence {θk}K
all k ≥ 1, 0 < λ ≤ νm1/2
K
(cid:88)

k=1 be generated from Algorithm 1. Let ηk =

, and m ≥ max{b2, (cb)2}, we have

ν ≤ c ≤ m1/2

(m+k)1/2 for

√
2

b

b

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

2M m1/4
K 1/2

+

√
2M
2
K 1/4

,

νλLb ln(m + K).

where M = J ∗−J(θ1)
Remark 1. Without loss of generality, let b = O(1), m = O(1) and λ = O(1), we have M =
O(ln(m + K)) = ˜O(1). Theorem 1 shows that the BGPO algorithm has a convergence rate of
˜O(
4 ≤ (cid:15), we have K = ˜O((cid:15)−4). Since the BGPO algorithm only needs one
trajectory to estimate the stochastic policy gradient at each iteration and runs K iterations, it has
the sample complexity of 1 · K = O((cid:15)−4) for ﬁnding an (cid:15)-stationary point.

K1/4 ). Let K − 1

1

9Lb , b > 0, 8Lλ
1
K
νλb + σ2

νλLb + mσ2

E(cid:107)Bψk

k=1

5.2 CONVERGENCE ANALYSIS OF VR-BGPO ALGORITHM
In the subsection, we give convergence properties of the VR-BGPO algorithm. The detailed proof
is provided in Appendix A.2.
Theorem 2. Suppose the sequence {θk}K
, b > 0, c ∈ (cid:2) 2
all k ≥ 0, 0 < λ ≤ νm1/3
5 ˆLb
we have

k=1 be generated from Algorithm 2. Let ηk =
3b3 + 20 ˆL2λ2

(cid:3) and m ≥ max (cid:0)2, b3, (cb)3, ( 5

(m+k)1/3 for
6b )2/3(cid:1),

, m2/3
b2

ν2

b

1
K

K
(cid:88)

k=1

E(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

√
2

2M (cid:48)m1/6
K 1/2

+

√
2

2M (cid:48)
K 1/3

,

(15)

where M (cid:48) = J ∗−J(θ1)
(cid:113)

H(2HC 2

g + Ch)(W + 1).

bνλ + m1/3σ2

16b2 ˆL2λ2 + c2σ2b2

8 ˆL2λ2 , ˆL2 = L2 + 2G2C 2

w, G = CgR/(1 − γ)2 and Cw =

Remark 2. Without loss of generality, let b = O(1), m = O(1) and λ = O(1), we have M =
O(ln(m + K)) = ˜O(1). Theorem 2 shows that the VR-BGPO algorithm has a convergence rate
of ˜O(
3 ≤ (cid:15), we have K = ((cid:15)−3). Since the VR-BGPO algorithm only needs
one trajectory to estimate the stochastic policy gradient at each iteration and runs K iterations, it
reaches a lower sample complexity of 1 · K = ˜O((cid:15)−3) for ﬁnding an (cid:15)-stationary point.

K1/3 ). Let K − 1

1

6 EXPERIMENTS

In this section, we conduct some RL tasks to verify the effectiveness of our methods. We ﬁrst
study the effect of different choices of Bregman divergences with our algorithms (BGPO and VR-
BGPO), and then we compare our VR-BGPO algorithm with other state-of-the-art methods such
as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017), ProxHSPGA (Pham et al., 2020),
VRMPO (Yang et al., 2019), and MDPO (Tomar et al., 2020). Our code is available at https:
//github.com/gaosh/BGPO.

7

Published as a conference paper at ICLR 2022

(a) CartPole-v1

(c) MountainCarContinuous
(b) Acrobat-v1
Figure 1: Effects of two Bregman Divergences: lp-norm and diagonal term (Diag).

(a) CartPole-v1

(c) MountainCarContinuous
(b) Acrobat-v1
Figure 2: Comparison between BGPO and VR-BGPO on different environments.

(cid:107)x(cid:107)p−2
p

, ∇ψ∗

i=1 |yi|q)

k(y) = ((cid:80)d

k(yj) are p-norm link functions, and ∇ψk(xj) = sign(xj )|xj |p−1

6.1 EFFECTS OF BREGMAN DIVERGENCES
In the subsection, we examine how different Bregman divergences affect the performance of our
algorithms. In the ﬁrst setting, we let mirror mapping ψk(x) = (cid:107)x(cid:107)p (p ≥ 1) with different p
1
to test the performance our algorithms. Let ψ∗
q be the conjugate mapping of
ψk(x), where p−1 + q−1 = 1, p, q > 1. According to (Beck & Teboulle, 2003), when Θ = Rd,
the update of ˜θk+1 in our algorithms can be calculated by ˜θk+1 = ∇ψ∗
k(∇ψk(θk) + λuk), where
∇ψk(xj) and ∇ψ∗
k(yj) =
sign(yj )|yj |q−1
(cid:107)y(cid:107)q−2
q

, and j is the coordinate index of x and y. In the second setting, we apply diagonal
term on the mirror mapping ψk(x) = 1
2 xT Mkx, where Mk is a diagonal matrix with positive
values. In the experiments, we generate Hk = diag(
k, and
α > 0, β ∈ (0, 1), as in Super-Adam algorithm (Kingma & Ba, 2014; Huang et al., 2021). Then
2 (y − x)T Hk(y − x). Under this setting, the update of ˜θk+1 can also be
we have Dψk (y, x) = 1
analytically solved ˜θk+1 = θk − λH −1
To test the effectiveness of two different Bregman divergences, we evaluate them on three
classic control environments from gym Brockman et al. (2016): CartPole-v1, Acrobat-v1, and
MountainCarContinuous-v0. In the experiment, categorical policy is used for CartPole and Acrobot
environments, and Gaussian policy is used for MountainCar. Gaussian value functions are used in
all settings. All policies and value functions are parameterized by multilayer perceptrons (MLPs).
For a fair comparison, all settings use the same initialization for policies. We run each setting ﬁve
times and plot the mean and variance of average returns. For lp-norm mapping, we test three dif-
ferent values of p = (1.50, 2.0, 3.0). For diagonal mapping, we set β = 0.999 and α = 10−8. We
set hyperparameters {b, m, c} to be the same. λ still needs to be tuned for different p to achieve
relatively good performance. For simplicity, we use BGPO-Diag to represent BGPO with diagonal
mapping, and we use BGPO-lp to represent BGPO with lp-norm mapping. Details about the setup
of environments and hyperparameters are provided in the Appendix C.

vk + α), vk = βvk−1 + (1 − β)u2

k uk.

√

From Fig. 1, we can ﬁnd that BGPO-Diag largely outperforms BGPO-lp with different choices of p.
The parameter tuning of BGPO-lp is much more difﬁcult than BGPO-Diag because each p requires
an individual λ to achieve the desired performance.

6.2 COMPARISON BETWEEN BGPO AND VR-BGPO
To understand the effectiveness of variance reduced technique used in our VR-BGPO algorithm, we
compare BGPO and VR-BGPO using the same settings introduced in section. 6.1. Both algorithms
use the diagonal mapping for ψ, since it performs much better than lp-norm. From Fig. 2 given
in the Appendix C, we can see that VR-BGPO can outperform BGPO in all three environments.

8

Published as a conference paper at ICLR 2022

(a) Pendulum-v2

(b) DoublePendulum-v2

(c) Walker2d-v2

(e) Reacher-v2
Figure 3: Experimental results of our algorithms and other baseline algorithms on six environments.

(f) HalfCheetah-v2

(d) Swimmer-v2

In CartPole, both algorithms converge very fast and have similar performance, and VR-BGPO is
more stable than BGPO. The advantage of VR-BGPO becomes large in Acrobot and MountainCar
environments, probably because the task is more difﬁcult compared to CartPole.

6.3 COMPARE TO OTHER METHODS
In this subsection, we apply our BGPO and VR-BGPO algorithms to compare with the other meth-
ods. For our BGPO and VR-BGPO, we use diagonal mapping for ψ. For VRMPO, we follow their
implementation and use lp-norm for ψ. For MDPO, ψ is the negative Shannon entropy, and the
Bregman divergence becomes KL-divergence.

To evaluate the performance of these algorithms, we test them on six gym (Brockman et al., 2016)
environments with continuous control tasks: Inverted-DoublePendulum-v2, Walker2d-v2, Reacher-
v2, Swimmer-v2, Inverted-Pendulum-v2 and HalfCheetah-v2. We use Gaussian policies and Gaus-
sian value functions for all environments, and both of them are parameterized by MLPs. To ensure a
fair comparison, all policies use the same initialization. For TRPO and PPO, we use the implementa-
tions provided by garage (garage contributors, 2019). We carefully implement MDPO and VRMPO
following the description provided by the original papers. All methods include our method, are
implemented with garage (garage contributors, 2019) and pytorch (Paszke et al., 2019). We run
all algorithms ten times on each environment and report the mean and variance of average returns.
Details about the setup of environments and hyperparameters are also provided in the Appendix C.

From Fig. 3, we can ﬁnd that our VR-BGPO method consistently outperforms all the other meth-
ods. Our BGPO basically reaches the second best performances. From the results of our BGPO,
we can ﬁnd that using a proper Bregman (mirror) distance can improve performances of the PG
methods. From the results of our VR-BGPO, we can ﬁnd that using a proper variance-reduced tech-
nique can further improve performances of the BGPO. ProxHSPGA can reach some relatively good
performances by using the variance reduced technique. MDPO can achieve good results in some
environments, but it can not outperform PPO or TRPO in Swimmer and InvertedDoublePendulum.
VRMPO only outperforms PPO and TRPO in Reacher and InvertedDoublePendulum. The unde-
sirable performance of VRMPO is probably because it uses lp norm for ψ, which requires careful
tuning of learning rate.

7 CONCLUSION

In the paper, we proposed a novel Bregman gradient policy optimization framework for reinforce-
ment learning based on Bregman divergences and momentum techniques. Moreover, we studied
convergence properties of the proposed methods under the nonconvex setting.

ACKNOWLEDGMENT

This work was partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002,
OIA 2040588.

9

Published as a conference paper at ICLR 2022

REFERENCES

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J Kappen. Dynamic policy programming.

The Journal of Machine Learning Research, 13(1):3207–3245, 2012.

Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters, 31(3):167–175, 2003.

Lev M Bregman. The relaxation method of ﬁnding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200–217, 1967.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. Openai gym, 2016.

Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558,
2020.

Yair Censor and Arnold Lent. An iterative row-action method for interval convex programming.

Journal of Optimization theory and Applications, 34(3):321–353, 1981.

Yair Censor and Stavros Andrea Zenios. Proximal minimization algorithm withd-functions. Journal

of Optimization Theory and Applications, 73(3):451–464, 1992.

Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.

In Advances in neural information processing systems, pp. 442–450, 2010.

Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.

In Advances in Neural Information Processing Systems, pp. 15210–15219, 2019.

Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.

Foundations and Trends® in Robotics, 2(1–2):1–142, 2013.

Yuhao Ding, Junzi Zhang, and Javad Lavaei. On the global convergence of momentum-based policy

gradient. arXiv preprint arXiv:2110.10116, 2021.

Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689–699, 2018.

The garage contributors. Garage: A toolkit for reproducible reinforcement learning research.

https://github.com/rlworkgroup/garage, 2019.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision

processes. In Thirty-sixth International Conference on Machine Learning, 2019.

Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267–305, 2016.

Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient meth-

ods. In International Conference on Machine Learning, pp. 4422–4433. PMLR, 2020.

Feihu Huang, Junyi Li, and Heng Huang. Super-adam: Faster and universal framework of adaptive

gradients. Advances in Neural Information Processing Systems, 34, 2021.

Yujia Jin and Aaron Sidford. Efﬁciently solving mdps with stochastic mirror descent. In Interna-

tional Conference on Machine Learning, pp. 4890–4900. PMLR, 2020.

10

Published as a conference paper at ICLR 2022

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS, pp. 315–323, 2013.

Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,

14:1531–1538, 2001.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information

processing systems, pp. 1008–1014, 2000.

Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling

complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135, 2021.

Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.

Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimiza-

tion attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.

Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. Advances in Neural Information Processing
Systems, 33, 2020.

Gergely Neu, Anders Jonsson, and Vicenc¸ G´omez. A uniﬁed view of entropy-regularized markov

decision processes. arXiv preprint arXiv:1705.07798, 2017.

Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. In 35th International Conference on Machine Learn-
ing, volume 80, pp. 4026–4035, 2018.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8024–8035, 2019.

Nhan Pham, Lam Nguyen, Dzung Phan, Phuong Ha Nguyen, Marten Dijk, and Quoc Tran-Dinh. A
hybrid stochastic policy gradient algorithm for reinforcement learning. In International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 374–385. PMLR, 2020.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897, 2015.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In International Con-
ference on Learning Representations (ICLR), 2016.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement

learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.

Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pp. 5668–5675, 2020.

Claude E Shannon. A mathematical theory of communication. The Bell system technical journal,

27(3):379–423, 1948.

Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy

gradient. In International Conference on Machine Learning, pp. 5729–5738, 2019.

11

Published as a conference paper at ICLR 2022

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057–1063, 2000.

Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy

optimization. arXiv preprint arXiv:2005.09814, 2020.

Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradi-
ent descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920,
2019.

Qing Wang, Yingru Li, Jiechao Xiong, and Tong Zhang. Divergence-augmented policy optimization.

In Advances in Neural Information Processing Systems, pp. 6099–6110, 2019.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artiﬁcial
Intelligence, pp. 191, 2019a.

Pan Xu, Felicia Gao, and Quanquan Gu. Sample efﬁcient policy gradient methods with recursive

variance reduction. arXiv preprint arXiv:1909.08610, 2019b.

Long Yang, Gang Zheng, Haotian Zhang, Yu Zhang, Qian Zheng, Jun Wen, and Gang Pan. Policy

optimization with stochastic mirror descent. arXiv preprint arXiv:1906.10462, 2019.

Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D Lee, and Yuejie Chi. Policy
mirror descent for regularized reinforcement learning: A generalized framework with linear con-
vergence. arXiv preprint arXiv:2105.11066, 2021.

Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. On the con-
vergence and sample efﬁciency of variance-reduced policy gradient method. arXiv preprint
arXiv:2102.08607, 2021.

Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efﬁcient reinforce-

ment learning with reinforce. arXiv preprint arXiv:2010.11364, 2020.

Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Bas¸ar. Global convergence of policy gradient

methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383, 2019.

Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth

nonconvex optimization. arXiv preprint arXiv:1806.04781, 2018.

12

Published as a conference paper at ICLR 2022

A APPENDIX

In this section, we study the convergence properties of our algorithms. We ﬁrst provide some useful
lemmas.
Lemma 1. (Proposition 4.2 in Xu et al. (2019b)) Suppose g(τ |θ) is the PGT estimator. Under
Assumption 1, we have

1) g(τ |θ) is L-Lipschitz differential, i.e., (cid:107)g(τ |θ1)−g(τ |θ2)(cid:107) ≤ L(cid:107)θ1 −θ2(cid:107) for all θ1, θ2 ∈ Θ,

where L = ChR/(1 − γ)2;

2) J(θ) is L-smooth, i.e., (cid:107)∇2J(θ)(cid:107) ≤ L;

3) g(τ |θ) is bounded, i.e., (cid:107)g(τ |θ)(cid:107) ≤ G for all θ ∈ Θ with G = CgR/(1 − γ)2.

Lemma 2. (Lemma 6.1 in Xu et al. (2019a)) Under Assumptions 1 and 3, let w(τ |θk−1, θk) =
g(τ |θk−1)/g(τ |θk), we have

V[w(τ |θk−1, θk)] ≤ C 2

w(cid:107)θk − θk−1(cid:107)2,

(16)

where Cw =

(cid:113)

H(2HC 2

g + Ch)(W + 1).

Lemma 3. (Lemma 1 in (Ghadimi et al., 2016)) Let X ⊆ Rd be a closed convex set, and φ : X → R
be a convex function but possibly nonsmooth, and Dψ : X × X → R is Bregman divergence related
to the ν-strongly convex function ψ. Then we deﬁne

x+ = arg min
z∈X

(cid:8)(cid:104)g, z(cid:105) +

1
λ

Dψ(z, x) + φ(z)(cid:9), ∀x ∈ X

(17)

PX (x, g, λ) =

(18)
where g ∈ Rd, λ > 0 and Dψ(z, x) = ψ(z) − (cid:0)ψ(x) + (cid:104)∇ψ(x), z − x(cid:105)(cid:1). Then the following
statement holds

(x − x+),

1
λ

(cid:104)g, PX (x, g, λ)(cid:105) ≥ ν(cid:107)PX (x, g, λ)(cid:107)2 +

1
λ
1 and x+
Lemma 4. (Proposition 1 in Ghadimi et al. (2016)) Let x+
2 be given in (17) with g re-
placed by g1 and g2 respectively. Then let PX (x, g1, λ) and PX (x, g2, λ) be deﬁned in (18) with x+
replaced by x+

(cid:2)φ(x+) − φ(x)(cid:3).

2 respectively. we have

1 and x+

(19)

(cid:107)PX (x, g1, λ) − PX (x, g2, λ)(cid:107) ≤

1
ν
Lemma 5. (Lemma 1 in (Cortes et al., 2010)) Let w(x) = P (x)
Q(x) be the importance weight for dis-
tributions P and Q. The following identities hold for the expectation, second moment, and variance
of w(x)

(cid:107)g1 − g2(cid:107).

(20)

E[w(x)] = 1, E[w2(x)] = d2(P ||Q),
V[w(x)] = d2(P ||Q) − 1,

(21)

where d2(P ||Q) = 2D(P ||Q), and D(P ||Q) is R´enyi divergence between distributions P and Q.
Lemma 6. Suppose that the sequence {θk}K
and 0 < λ ≤ ν

k=1 be generated from Algorithms 1 or 2. Let 0 < ηk ≤ 1

, then we have

2Lηk

f (θk+1) − f (θk) ≤

ηkλ
ν

(cid:107)∇f (θk) − uk(cid:107)2 −

νηk
2λ

(cid:107)˜θk+1 − θk(cid:107)2.

(22)

f (θk+1) ≤ f (θk) + (cid:104)∇f (θk), θk+1 − θk(cid:105) +

Proof. According to Assumption 1 and Lemma 1, the function f (θ) is L-smooth. Then we have
L
2
= f (θk) + ηk(cid:104)∇f (θk), ˜θk+1 − θk(cid:105) +

(cid:107)˜θk+1 − θk(cid:107)2

(cid:107)θk+1 − θk(cid:107)2

(23)

Lη2
k
2

= f (θk) + ηk(cid:104)∇f (θk) − uk, ˜θk+1 − θk(cid:105) + ηk(cid:104)uk, ˜θk+1 − θk(cid:105) +

Lη2
k
2

(cid:107)˜θk+1 − θk(cid:107)2,

13

Published as a conference paper at ICLR 2022

where the second equality is due to θk+1 = θk + ηk(˜θk+1 − θk). By the step 4 of Algorithm 1 or 2,
λ Dψk (θ, θk)(cid:9). By using Lemma 3 with φ(·) = 0, we have
we have ˜θk+1 = arg minθ∈Θ

(cid:8)(cid:104)uk, θ(cid:105) + 1

Thus, we can obtain

(cid:104)uk,

1
λ

(θk − ˜θk+1)(cid:105) ≥ ν(cid:107)

1
λ

(θk − ˜θk+1)(cid:107)2.

(cid:104)uk, ˜θk+1 − θk(cid:105) ≤ −

ν
λ

(cid:107)˜θk+1 − θk(cid:107)2.

(24)

(25)

According to the Cauchy-Schwarz inequality and Young’s inequality, we have

(cid:104)∇f (θk) − uk, ˜θk+1 − θk(cid:105) ≤ (cid:107)∇f (θk) − uk(cid:107)(cid:107)˜θk+1 − θk(cid:107)

≤

λ
ν

(cid:107)∇f (θk) − uk(cid:107)2 +

ν
4λ

(cid:107)˜θk+1 − θk(cid:107)2.

(26)

Combining the inequalities (23), (25) with (26), we obtain

f (θk+1) ≤ f (θk) + ηk(cid:104)∇f (θk) − uk, ˜θk+1 − θk(cid:105) + ηk(cid:104)uk, ˜θk+1 − θk(cid:105) +

Lη2
k
2

(cid:107)˜θk+1 − θk(cid:107)2

≤ f (θk) +

(cid:107)∇f (θk) − uk(cid:107)2 +

ηkλ
ν
ηkλ
ν
ηkλ
ν
where the last inequality is due to 0 < λ ≤ ν

(cid:107)∇f (θk) − uk(cid:107)2 −

(cid:107)∇f (θk) − uk(cid:107)2 −

= f (θk) +

≤ f (θk) +

νηk
4λ
νηk
2λ
νηk
2λ
.

2Lηk

νηk
(cid:107)˜θk+1 − θk(cid:107)2 −
λ
(cid:107)˜θk+1 − θk(cid:107)2 − (cid:0) νηk
4λ

(cid:107)˜θk+1 − θk(cid:107)2 +

Lη2
k
2

(cid:107)˜θk+1 − θk(cid:107)2

−

Lη2
k
2

(cid:1)(cid:107)˜θk+1 − θk(cid:107)2

(cid:107)˜θk+1 − θk(cid:107)2,

(27)

A.1 CONVERGENCE ANALYSIS OF BGPO ALGORITHM

In this subsection, we analyze the convergence properties of BGPO algorithm.
Lemma 7. Assume the stochastic policy gradient uk+1 be generated from Algorithm 1, given 0 <
βk ≤ 1, we have

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 ≤ (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 +

2
βk+1

L2η2
k

E(cid:107)˜θk+1 − θk(cid:107)2 + β2

k+1σ2.

Proof. By the deﬁnition of uk+1 in Algorithm 1, we have

uk+1 − uk = −βk+1uk − βk+1g(τk+1|θk+1).

(28)

Since ∇f (θk) = −J(θk) for all k ≥ 1, we have
E(cid:107)∇f (θk+1) − uk+1(cid:107)2
= E(cid:107) − ∇J(θk) − uk − ∇J(θk+1) + ∇J(θk) − (uk+1 − uk)(cid:107)2
= E(cid:107) − ∇J(θk) − uk − ∇J(θk+1) + ∇J(θk) + βk+1uk + βk+1g(τk+1|θk+1)(cid:107)2
= E(cid:107)(1 − βk+1)(−∇J(θk) − uk) + βk+1(−∇J(θk+1) + g(τk+1|θk+1))

+ (1 − βk+1)(cid:0) − ∇J(θk+1) + ∇J(θk)(cid:1)(cid:107)2

= (1 − βk+1)2E(cid:107)∇J(θk) + uk + ∇J(θk+1) − ∇J(θk)(cid:107)2 + β2

k+1

≤ (1 − βk+1)2(1 + βk+1)E(cid:107)∇J(θk) + uk(cid:107)2 + (1 − βk+1)2(1 +

+ β2

k+1

E(cid:107)∇J(θk+1) − g(τk+1|θk+1)(cid:107)2

E(cid:107)∇J(θk+1) − g(τk+1|θk+1)(cid:107)2
1
βk+1

)E(cid:107)∇J(θk+1) − ∇J(θk)(cid:107)2

≤ (1 − βk+1)E(cid:107)∇J(θk) + uk(cid:107)2 +

≤ (1 − βk+1)E(cid:107)∇J(θk) + uk(cid:107)2 +

≤ (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 +

2
βk+1
2
βk+1
2
βk+1

E(cid:107)∇J(θk+1) − ∇J(θk)(cid:107)2 + β2

k+1

E(cid:107)∇J(θk+1) − g(τk+1|θk+1)(cid:107)2

L2E(cid:107)θk+1 − θk(cid:107)2 + β2

k+1

E(cid:107)∇J(θk+1) − g(τk+1|θk+1)(cid:107)2

L2η2
k

E(cid:107)˜θk+1 − θk(cid:107)2 + β2

k+1σ2,

(29)

14

Published as a conference paper at ICLR 2022

where the fourth equality holds by Eτk+1∼p(τ |θk+1)[g(τk+1|θk+1)] = ∇J(θk+1); the ﬁrst inequality
holds by Young’s inequality; the second inequality is due to 0 < βk+1 ≤ 1 such that (1−βk+1)2(1+
βk+1) = 1 − βk+1 − β2
;
βk+1
the last inequality holds by Assumption 2.

k+1 ≤ 1 − βk+1 and (1 − βk+1)2(1 + 1
βk+1

) ≤ 1 + 1

k+1 + β3

≤ 2

βk+1

Theorem 3. Assume the sequence {θk}K
all k ≥ 1, 0 < λ ≤ νm1/2

9Lb , b > 0, 8Lλ

ν ≤ c ≤ m1/2

b

k=1 be generated from Algorithm 1. Let ηk =

b

(m+k)1/2 for

, and m ≥ max{b2, (cb)2}, we have

1
K

K
(cid:88)

k=1

E(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

√
2

2M m1/4
K 1/2

+

√
2
2M
K 1/4

,

where M = J ∗−J(θ1)

νλb + σ2

νλLb + mσ2

νλLb ln(m + K).

b

(m+k)1/2 is decreasing on k, we have ηk ≤ η0 = b

Proof. Since ηk =
the same time, let m ≥ (cb)2, we have βk+1 = cηk ≤ cη0 = cb
have c ≤ m1/2
According to Lemma 7, we have

9Lb , we have λ ≤ νm1/2

. Since 0 < λ ≤ νm1/2

9Lb ≤ νm1/2

m1/2 ≤ 1 for all k ≥ 0. At
m1/2 ≤ 1. Consider m ≥ (cb)2, we
≤ ν
2Lb = ν
for all k ≥ 0.
2Lη0

2Lηk

b

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 − E(cid:107)∇f (θk) − uk(cid:107)2

≤ −βk+1E(cid:107)∇f (θk) − uk(cid:107)2 +

2
βk+1

L2η2
k

E(cid:107)˜θk+1 − θk(cid:107)2 + β2

k+1σ2

= −cηkE(cid:107)∇f (θk) − uk(cid:107)2 +

2L2
c

ηkE(cid:107)˜θk+1 − θk(cid:107)2 + c2η2

= −

8Lλ
ν

ηkE(cid:107)∇f (θk) − uk(cid:107)2 +

Lν
4λ

ηkE(cid:107)˜θk+1 − θk(cid:107)2 +

kσ2
mη2
kσ2
b2

,

(30)

where the ﬁrst equality is due to βk+1 = cηk and the last equality holds by 8Lλ
Next we deﬁne a Lyapunov function Φk = E(cid:2)f (θk) + 1
have

L (cid:107)∇f (θk) − uk(cid:107)2(cid:3) for any t ≥ 1. Then we

ν ≤ c ≤ m1/2

b

.

≤

1
L
E(cid:107)∇f (θk) − uk(cid:107)2 −

Φk+1 − Φk = E(cid:2)f (θk+1) − f (θk) +
ληk
ν
νηk
4λ
λ
4ν

E(cid:107)˜θk+1 − θk(cid:107)2 +

≤ −

+

ηkE(cid:107)∇f (θk) − uk(cid:107)2 −

E(cid:107)˜θk+1 − θk(cid:107)2 −

(cid:0)(cid:107)∇f (θk+1) − uk+1(cid:107)2 − (cid:107)∇f (θk) − uk(cid:107)2(cid:1)(cid:3)
νηk
2λ
mη2
Lb2
ν
4λ

ηkE(cid:107)˜θk+1 − θk(cid:107)2 +

mη2
Lb2

8ληk
ν

kσ2

kσ2

,

E(cid:107)∇f (θk) − uk(cid:107)2

(31)

where the ﬁrst inequality follows by the Lemma 6 and the above inequality (30).

15

Published as a conference paper at ICLR 2022

Summing the above inequality (31) over k from 1 to K, we can obtain

K
(cid:88)

k=1

E[

λ
4ν

ηkE(cid:107)∇f (θk) − uk(cid:107)2 +

ν
4λ

ηkE(cid:107)˜θk+1 − θk(cid:107)2]

≤ Φ1 − ΦK+1 +

mσ2
Lb2

K
(cid:88)

k=1

η2
k

= f (θ1) − f (θK+1) +

≤ J(θK+1) − J(θ1) +

1
L

1
L

(cid:107)∇f (θ1) − u1(cid:107)2 −

1
L

(cid:107)∇f (θK+1) − uK+1(cid:107)2 +

mσ2
Lb2

K
(cid:88)

k=1

η2
k

(cid:107)∇J(θ1) − g(τ1|θ1)(cid:107)2 +

mσ2
Lb2

K
(cid:88)

k=1

η2
k

≤ J ∗ − J(θ1) +

≤ J ∗ − J(θ1) +

σ2
L
σ2
L

+

+

mσ2
L
mσ2
L

(cid:90) K

1

1
m + k

dk

ln(m + K),

where the last second inequality holds by Assumptions 2 and 4.

Since ηk is decreasing, we have

1
K

≤

≤

K
(cid:88)

E[

1
4ν2

+

k=1
J ∗ − J(θ1)
KνληK
(cid:18) J ∗ − J(θ1)
νλb

E(cid:107)∇f (θk) − uk(cid:107)2 +

1
4λ2

E(cid:107)˜θk+1 − θk(cid:107)2]

σ2
KνλLηK
σ2
νλLb

+

+

+

mσ2
νλLKηK

ln(m + K)

mσ2
νλLb

ln(m + K)

(cid:19) (m + K)1/2
K

.

(32)

(33)

Let M = J ∗−J(θ1)

νλb + σ2

νλLb + mσ2

νλLb ln(m + K), the above inequality (33) reduces to

1
K

K
(cid:88)

k=1

E[

1
4ν2

E(cid:107)∇f (θk) − uk(cid:107)2 +

1
4λ2

E(cid:107)˜θk+1 − θk(cid:107)2] ≤

M
K

(m + K)1/2.

(34)

According to Jensen’s inequality, we have

1
K

K
(cid:88)

k=1

E(cid:2) 1
2ν

(cid:107)∇f (θk) − uk(cid:107) +

1
2λ

(cid:107)˜θk+1 − θk(cid:107)(cid:3)

K
(cid:88)

k=1

≤ (cid:0) 2
K
√

≤

2M
K 1/2

E(cid:2) 1

4ν2 (cid:107)∇f (θk) − uk(cid:107)2 +
√
2M m1/4
K 1/2

+

(m + K)1/4 ≤

1

4λ2 (cid:107)˜θk+1 − θk(cid:107)2(cid:3)(cid:1)1/2

√

2M
K 1/4

,

(35)

where the last inequality is due to the inequality (a + b)1/4 ≤ a1/4 + b1/4 for all a, b ≥ 0. Thus we
have

1
K

K
(cid:88)

k=1

E(cid:2) 1
ν

(cid:107)∇f (θk) − uk(cid:107) +

(cid:107)˜θk+1 − θk(cid:107)(cid:3) ≤

1
λ

√
2

2M m1/4
K 1/2

+

√
2
2M
K 1/4

.

By the step 4 of Algorithm 1, we have

Bψk
λ,(cid:104)uk,θ(cid:105)(θk) = PΘ(θk, uk, λ) =

(cid:0)θk − ˜θk+1

(cid:1).

1
λ

(36)

(37)

16

Published as a conference paper at ICLR 2022

At the same time, as in Ghadimi et al. (2016), we deﬁne

Bψk
λ,(cid:104)∇f (θk),θ(cid:105)(θk) = PΘ(θk, ∇f (θk), λ) =

1
λ

(cid:0)θk − θ+

k+1

(cid:1),

where

θ+
k+1 = arg min
θ∈Θ

(cid:8)(cid:104)∇f (θk), θ(cid:105) +

Dψk (θ, θk)(cid:9).

1
λ

(38)

(39)

According to the above Lemma 4, we have (cid:107)Bψk
Then we have

λ,(cid:104)uk,θ(cid:105)(θk)−Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤ 1

ν (cid:107)uk −∇f (θk)(cid:107).

(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤ (cid:107)Bψk
≤ (cid:107)Bψk

λ,(cid:104)uk,θ(cid:105)(θk)(cid:107) + (cid:107)Bψk

λ,(cid:104)uk,θ(cid:105)(θk) − Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107)

1
λ,(cid:104)uk,θ(cid:105)(θk)(cid:107) +
ν
1
ν

(cid:107)˜θk+1 − θk(cid:107) +

=

1
λ

(cid:107)uk − ∇f (θk)(cid:107)

(cid:107)uk − ∇f (θk)(cid:107).

By the above inequalities (36) and (40), we have

1
K

K
(cid:88)

k=1

E(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

√
2

2M m1/4
K 1/2

+

√
2
2M
K 1/4

.

(40)

(41)

A.2 CONVERGENCE ANALYSIS OF VR-BGPO ALGORITHM

In this subsection, we will analyze convergence properties of the VR-BGPO algorithm.
Lemma 8. Assume that the stochastic policy gradient uk+1 be generated from Algorithm 2, given
0 < βk ≤ 1, we have

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 ≤ (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 + 4 ˆL2η2

k(cid:107)˜θk+1 − θk(cid:107)2 + 2β2

k+1σ2,

where ˆL2 = L2 + 2G2C 2

w and Cw =

(cid:113)

H(2HC 2

g + Ch)(W + 1).

Proof. By the deﬁnition of uk+1 in Algorithm 2, we have

uk+1 − uk
= −βk+1uk − βk+1g(τk+1|θk+1) + (1 − βk+1)(cid:0) − g(τk+1|θk+1) + w(τk+1|θk, θk+1)g(τk+1|θk)(cid:1).

Since ∇f (θk+1) = −∇J(θk+1), we have
E(cid:107)∇f (θk+1) − uk+1(cid:107)2
= E(cid:107)∇J(θk) + uk + ∇J(θk+1) − ∇J(θk) + (uk+1 − uk)(cid:107)2
= E(cid:107)∇J(θk) + uk + ∇J(θk+1) − ∇J(θk) − βk+1uk − βk+1g(τk+1|θk+1)

+ (1 − βk+1)(cid:0) − g(τk+1|θk+1) + w(τk+1|θk, θk+1)g(τk+1|θk)(cid:1)(cid:107)2
= E(cid:107)(1 − βk+1)(∇J(θk) + uk) + βk+1(∇J(θk+1) − g(τk+1|θk+1))

(42)

− (1 − βk+1)(cid:0)g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk) − (∇J(θk+1) − ∇J(θk))(cid:1)(cid:107)2

= (1 − βk+1)2E(cid:107)∇J(θk) + uk(cid:107)2 + E(cid:107)βk+1(∇J(θk+1) − g(τk+1|θk+1))

− (1 − βk+1)(cid:0)g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk) − (∇J(θk+1) − ∇J(θk))(cid:1)(cid:107)2

≤ (1 − βk+1)2E(cid:107)∇J(θk) + uk(cid:107)2 + 2β2

k+1

E(cid:107)∇J(θk+1) − g(τk+1|θk+1)(cid:107)2

+ 2(1 − βk+1)2E(cid:107)g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk) − (∇J(θk+1) − ∇J(θk))(cid:107)2

≤ (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2β2

k+1σ2 + 2 E(cid:107)g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk)(cid:107)2
,
(cid:125)

(cid:124)

(cid:123)(cid:122)
=T1

17

Published as a conference paper at ICLR 2022

where the forth equality holds by Eτk+1∼p(τ |θk+1)[g(τk+1|θk+1)] = ∇J(θk+1) and
Eτk+1∼p(τ |θk+1)[g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk)] = ∇J(θk+1) − ∇J(θk); the sec-
ond last inequality follows by Young’s inequality; and the last inequality holds by Assumption 2,
and the inequality E(cid:107)ζ − E[ζ](cid:107)2 = E(cid:107)ζ(cid:107)2 − (E[ζ])2 ≤ E(cid:107)ζ(cid:107)2, and 0 < βk+1 ≤ 1.
Next, we give an upper bound of the term T1 as follows:

T1 = E(cid:107)g(τk+1|θk+1) − w(τk+1|θk, θk+1)g(τk+1|θk)(cid:107)2

= E(cid:107)g(τk+1|θk+1) − g(τk+1|θk) + g(τk+1|θk) − w(τk+1|θk, θk+1)g(τk+1|θk)(cid:107)2
≤ 2E(cid:107)g(τk+1|θk+1) − g(τk+1|θk)(cid:107)2 + 2E(cid:107)(1 − w(τk+1|θk, θk+1))g(τk+1|θk)(cid:107)2
≤ 2L2(cid:107)θk+1 − θk(cid:107)2 + 2G2E(cid:107)1 − w(τk+1|θk, θk+1)(cid:107)2
= 2L2(cid:107)θk+1 − θk(cid:107)2 + 2G2V(cid:0)w(τk+1|θk, θk+1)(cid:1)
w)(cid:107)θk+1 − θk(cid:107)2,
≤ 2(L2 + 2G2C 2

(43)

where the second inequality holds by Lemma 1, and the third equality holds by Lemma 5, and the
last inequality follows by Lemma 2.
Combining the inequalities (42) with (43), let ˆL2 = L2 + 2G2C 2

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 ≤ (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2β2
= (1 − βk+1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2β2

w, we have
k+1σ2 + 4 ˆL2(cid:107)θk+1 − θk(cid:107)2
k+1σ2 + 4 ˆL2η2

k(cid:107)˜θk+1 − θk(cid:107)2.

Theorem 4. Suppose the sequence {θk}K
all k ≥ 0, 0 < λ ≤ νm1/3
5 ˆLb
we have

, b > 0, 2

k=1 be generated from Algorithm 2. Let ηk =

3b3 + 20 ˆL2λ2

ν2 ≤ c ≤ m2/3

b2 and m ≥ max (cid:0)2, b3, (cb)3, ( 5

b

(m+k)1/3 for
6b )2/3(cid:1),

1
K

K
(cid:88)

k=1

E(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

√
2

2M (cid:48)m1/6
K 1/2

+

√
2

2M (cid:48)
K 1/3

,

(44)

where M (cid:48) = J ∗−J(θ1)

bνλ + m1/3σ2

16b2 ˆL2λ2 + c2σ2b2

8 ˆL2λ2 and ˆL2 = L2 + 2G2C 2
w.

b

(m+k)1/3 on k is decreasing and m ≥ b3, we have ηk ≤ η0 = b

m1/3 ≤ 1. Due
2Lb = ν
for any k ≥ 0.
2Lη0
2Lηk
m2/3 ≤ 1. At the same time, we have

≤ νm1/3
k ≤ cb2

≤ ν

w ≥ L, we have 0 < λ ≤ νm1/3
5 ˆLb

Proof. Since ηk =
to ˆL = (cid:112)L2 + 2G2C 2
Consider 0 < ηk ≤ 1 and m ≥ (cb)3, we have βk+1 = cη2
c ≤ m2/3
b2
1
ηk
≤ (cid:0) 1 − βk+1
ηk

1
ηk−1
(cid:1)E(cid:107)∇f (θk) − uk(cid:107)2 +

. According to Lemma 8, we have

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 −

E(cid:107)∇f (θk) − uk(cid:107)2

1
ηk−1

−

2β2

k+1σ2
ηk
(cid:1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2c2η3
kσ2 + 4 ˆL2ηk(cid:107)˜θk+1 − θk(cid:107)2

+ 4 ˆL2ηk(cid:107)˜θk+1 − θk(cid:107)2

− cηk

1
3 − (m + k − 1)

1

3 (cid:1) − cηk

(cid:1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2c2η3

kσ2 + 4 ˆL2ηk(cid:107)˜θk+1 − θk(cid:107)2

(cid:1)E(cid:107)∇f (θk) − uk(cid:107)2 + 2c2η3

kσ2 + 4 ˆL2ηk(cid:107)˜θk+1 − θk(cid:107)2,

(45)

where the last inequality holds by the following inequality

(m + k)

1

3 − (m + k − 1)

1

3 ≤

1
3(m + k − 1)2/3

≤

≤

22/3
3(m + k)2/3

=

22/3
3b2

18

1
3(cid:0)m/2 + k(cid:1)2/3
b2
(m + k)2/3

=

22/3
3b2 η2

k ≤

2
3b2 ηk,

(46)

−

1
= (cid:0) 1
ηk
ηk−1
= (cid:0) 1
(cid:0)(m + k)
b
≤ (cid:0) 2
3b3 ηk − cηk

Published as a conference paper at ICLR 2022

where the ﬁrst inequality holds by the concavity of function f (x) = x1/3, i.e., (x + y)1/3 ≤
x1/3 + y
3x2/3 ; the second inequality is due to m ≥ 2, and the last inequality is due to 0 < ηk ≤ 1.
3b3 + 20 ˆL2λ2
Let c ≥ 2

, we have

ν2

1
ηk

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 −

1
ηk−1

E(cid:107)∇f (θk) − uk(cid:107)2

≤ −

20 ˆL2λ2
ν2

ηkE(cid:107)∇f (θk) − uk(cid:107)2 + 2c2η3

kσ2 + 4 ˆL2ηk(cid:107)˜θk+1 − θk(cid:107)2.

(47)

Here we simultaneously consider c ≥ 2

3b3 + 20 ˆL2λ2

ν2

, c ≤ m2/3
b2

and 0 < λ ≤ νm1/3
5 ˆLb

, we have

2
3b3 +

20 ˆL2λ2

ν2 ≤

2
3b3 +

20 ˆL2
ν2

ν2m2/3
25 ˆL2b2

=

2
3b3 +

4m2/3
5b2 ≤

m2/3
b2

.

(48)

6b )2/3.

Then we have m ≥ ( 5
Next we deﬁne a Lyapunov function Ωk = E(cid:2)f (θk) +
According to Lemma 6, we have

ν
16 ˆL2ληk−1

(cid:107)∇f (θk) − uk(cid:107)2(cid:3) for any k ≥ 1.

Ωk+1 − Ωk = f (θk+1) − f (θk) +

E(cid:107)∇f (θk+1) − uk+1(cid:107)2 −

E(cid:107)∇f (θk) − uk(cid:107)2

(cid:19)

1
ηk−1

≤

(cid:107)∇f (θk) − uk(cid:107)2 −

(cid:107)˜θk+1 − θk(cid:107)2 −

5ληk
4ν

E(cid:107)∇f (θk) − uk(cid:107)2

+

ηkλ
ν
νηk
4λ
ληk
4ν

E(cid:107)˜θk+1 − θk(cid:107)2 +

≤ −

E(cid:107)∇f (θk) − uk(cid:107)2 −

E(cid:107)˜θk+1 − θk(cid:107)2 +

kσ2
νc2η3
8 ˆL2λ

,

(cid:18) 1
ηk

ν
16 ˆL2λ
νηk
2λ
νc2η3
kσ2
8 ˆL2λ
νηk
4λ

where the ﬁrst inequality is due to the above inequality (47). Thus, we can obtain

ληk
4ν

E(cid:107)∇f (θk) − uk(cid:107)2 +

νηk
4λ

E(cid:107)˜θk+1 − θk(cid:107)2 ≤ Ωk − Ωk+1 +

νc2η3
kσ2
8 ˆL2λ

.

(49)

(50)

Taking average over k = 1, 2, · · · , K on both sides of (50), we have

1
T

K
(cid:88)

k=1

E(cid:2) ληk
4ν

E(cid:107)∇f (θk) − uk(cid:107)2 +

νηk
4λ

E(cid:107)˜θk+1 − θk(cid:107)2(cid:3)

≤

≤

≤

f (θ1) − f (θK+1)
K

+

ν(cid:107)∇f (θ1) − u1(cid:107)2
16 ˆL2η0λK

−

ν(cid:107)∇f (θK+1) − uK+1(cid:107)2
16 ˆL2ηKλK

+

1
K

K
(cid:88)

k=1

νc2η3
kσ2
8 ˆL2λ

J(θK+1) − J(θ1)
K

+

νσ2
16 ˆL2η0λK

+

1
K

K
(cid:88)

k=1

kσ2
νc2η3
8 ˆL2λ

J ∗ − J(θ1)
K

+

νσ2
16 ˆL2η0λK

+

1
K

K
(cid:88)

k=1

νc2η3
kσ2
8 ˆL2λ

,

(51)

where the second inequality is due to u1 = −g(τ1|θ1), ∇f (θ1) = −∇J(θ1) and Assumption 1,
and the last inequality holds by Assumption 2. Since ηk is decreasing, i.e., η−1
for any

K ≥ η−1

k

19

Published as a conference paper at ICLR 2022

0 < k ≤ K, we have

E(cid:107)∇f (θk) − uk(cid:107)2 +

1
4λ2

E(cid:107)˜θk+1 − θk(cid:107)2(cid:3)

1
K

≤

≤

≤

=

K
(cid:88)

E(cid:2) 1
4ν2

k=1
J ∗ − J(θ1)
KνληK

+

+

+

J ∗ − J(θ1)
KνληK
J ∗ − J(θ1)
KληK
(cid:18) J ∗ − J(θ1)
bνλ

σ2
16 ˆL2ηKη0λ2K
m1/3σ2
16b ˆL2λ2ηKK
m1/3σ2
16b ˆL2λ2ηKK
m1/3σ2
16b2 ˆL2λ2

+

+

+

1
KληK

K
(cid:88)

k=1

+

+

c2σ2
8 ˆL2λ2KηK
c2σ2b3
8 ˆL2λ2KηK

c2η3
kσ2
8 ˆL2λ
(cid:90) K

b3
m + k

dk

1

ln(m + K)

c2σ2b2
8 ˆL2λ2

(cid:19) (m + K)1/3
K

,

(52)

where the second inequality holds by (cid:80)K
16b2 ˆL2λ2 + c2σ2b2

Let M (cid:48) = J ∗−J(θ1)

bνλ + m1/3σ2

k=1 η3

kdk ≤ (cid:82) K

1 η3

kdk = b3 (cid:82) K

1 (m + k)−1dk.

8 ˆL2λ2 , the above inequality (52) reduces to

1
K

K
(cid:88)

k=1

E(cid:2) 1

4ν2 (cid:107)∇f (θk) − uk(cid:107)2 +

1

4λ2 (cid:107)˜θk+1 − θk(cid:107)2(cid:3) ≤

M (cid:48)
K

(m + K)1/3.

(53)

According to Jensen’s inequality, we have

1
K

K
(cid:88)

k=1

E(cid:2) 1
2ν

(cid:107)∇f (θk) − uk(cid:107) +

1
2λ

(cid:107)˜θk+1 − θk(cid:107)(cid:3)

K
(cid:88)

E(cid:2) 1

≤ (cid:0) 2
K
√

≤

k=1
2M (cid:48)
K 1/2

4ν2 (cid:107)∇f (θk) − uk(cid:107)2 +
√
2M (cid:48)m1/6
K 1/2

(m + K)1/6 ≤

1

4λ2 (cid:107)˜θk+1 − θk(cid:107)2(cid:3)(cid:1)1/2

√

2M (cid:48)
K 1/3

,

+

(54)

where the last inequality is due to the inequality (a + b)1/6 ≤ a1/6 + b1/6 for all a, b ≥ 1. Thus we
have

1
K

K
(cid:88)

k=1

E(cid:2) 1
ν

(cid:107)∇f (θk) − uk(cid:107) +

(cid:107)˜θk+1 − θk(cid:107)(cid:3) ≤

1
λ

√
2

2M (cid:48)m1/6
K 1/2

+

√
2

2M (cid:48)
K 1/3

.

Then by using the above inequality (40), we can obtain

1
K

K
(cid:88)

k=1

E(cid:107)Bψk

λ,(cid:104)∇f (θk),θ(cid:105)(θk)(cid:107) ≤

√
2

2M (cid:48)m1/6
K 1/2

+

√
2

2M (cid:48)
K 1/3

.

(55)

(56)

B ACTOR-CRITIC STYLE BGPO AND VR-BGPO ALGORITHMS

In the experiments, we use the advantage-based policy gradient estimator:

g(τ |θ) =

H−1
(cid:88)

t=0

∇ log πθ(at|st) ˆAπθ (st, at),

(57)

20

Published as a conference paper at ICLR 2022

Algorithm 3 BGPO Algorithm (Actor-Critic Style)
1: Input: Total iteration K, tuning parameters {λ, b, m, c} and mirror mappings (cid:8)ψk

ν-strongly convex functions;

(cid:9)K
k=1 are

2: Initialize: θ1 ∈ Θ, θv

1 ∈ Θv and sample a trajectory τ1 from p(τ |θ1), and compute u1 =

−g(τ1|θ1);

3: for k = 1, 2, . . . , K do
4:
5:

(cid:8)(cid:104)uk, θ(cid:105) + 1

λ Dψk (θ, θk)(cid:9);

# Update the policy network
Update ˜θk+1 = arg minθ∈Θ
Update θk+1 = θk + ηk(˜θk+1 − θk) with ηk =
# Update the value network
Update θv
k+1 by solving the subproblem (58);
# Sample a new trajectory and compute policy gradients
Sample a trajectory τk+1 from p(τ |θk+1), and compute uk+1 = −βk+1g(τk+1|θk+1) + (1 −
βk+1)uk with βk+1 = cηk;

b
(m+k)1/2 ;

11: end for
12: Output: θζ chosen uniformly random from {θk}K

k=1.

Algorithm 4 VR-BGPO Algorithm (Actor-Critic Style)
1: Input: Total iteration K, tuning parameters {λ, b, m, c} and mirror mappings (cid:8)ψk

ν-strongly convex functions;

(cid:9)K
k=1 are

2: Initialize: θ1 ∈ Θ, θv

1 ∈ Θv and sample a trajectory τ1 from p(τ |θ1), and compute u1 =

−g(τ1|θ1);

3: for k = 1, 2, . . . , K do
4:
5:

(cid:8)(cid:104)uk, θ(cid:105) + 1

λ Dψk (θ, θk)(cid:9);

# Update the policy network
Update ˜θk+1 = arg minθ∈Θ
Update θk+1 = θk + ηk(˜θk+1 − θk) with ηk =
# Update the value network
Update θv
k+1 by solving the subproblem (58);
# Sample a new trajectory and compute policy gradients
Sample a trajectory τk+1 from p(τ |θk+1), and compute uk+1 = −βk+1g(τk+1|θk+1) + (1 −
βk+1)(cid:2)uk − g(τk+1|θk+1) + w(τk+1|θk, θk+1)g(τk+1|θk)(cid:3) with βk+1 = cη2
k;

b
(m+k)1/3 ;

11: end for
12: Output: θζ chosen uniformly random from {θk}K

k=1 .

6:
7:
8:
9:
10:

6:
7:
8:
9:
10:

where θ(∈ Θ ⊆ Rd) denotes parameters of the policy network, and ˆAπθ (s, a) is an estimator of
the advantage function Aπθ (s, a). In using advantage-based policy gradient, we also need the state-
value function V πθ (s). Here, we use a value network Vθv (s) to approximate the state-value function
V πθ (s). Speciﬁcally, we solve the following problem to obtain the value network:

min
θv∈Θv

L(θv) :=

H−1
(cid:88)

t=0

(cid:0)Vθv (st) − ˆV πθ (st)(cid:1)2

,

(58)

where θv(∈ Θv ⊆ Rdv ) denotes parameters of the value network, and ˆV πθ (s) is an estimator of the
state-value function V πθ (s), which is obtained by the GAE Schulman et al. (2016). Then we use
the GAE to estimate ˆAπθ based on value network Vθv . We describe the actor-critic style BGPO and
VR-BGPO algorithms in Algorithm 3 and Algorithm 4, respectively.

C DETAILED SETUP OF EXPERIMENTAL ENVIRONMENTS AND

HYPER-PARAMETERS

In this section, we provide the detailed setup of experimental environments and hyper-parameters.
We ﬁrst provide the detailed setup of our experiments in Tab. 2 and Tab. 3. We use ADAM optimizer
to optimize value functions for all methods and settings, which is a common practice. The impor-

21

Published as a conference paper at ICLR 2022

Environments
Horizon
Value function Network sizes
Policy network sizes
Number of timesteps
Batchsize
VR-BGPO/BGPO {b, m, c}
BGPO-lp {λp=1.5, λp=2.0, λp=3.0}
BGPO-Diag/VR-BGPO-Diag λ
Value function learning rate

CartPole-v1
100
32 × 32
8 × 8
5 × 105
50
{1.5, 2.0, 25}
{0.0064, 0.0016, 0.0008}
1 × 10−3
2.5 × 10−3

Acrobat-v1
500
32 × 32
8 × 8
5 × 106
100
{1.5, 2.0, 25}
{0.016, 0.004, 0.001}
1 × 10−3
2.5 × 10−3

MountainCar-v0
500
32 × 32
64 × 64
7.5 × 106
100
{1.5, 2.0, 25}
{0.016, 0.004, 0.001}
1 × 10−3
2.5 × 10−3

Table 2: Setups of environments and hyper-parameters for experiments in section 6.2 and section 6.3.
The learning rate of value functions are the same for all methods.

Environments
Horizon
Value function Network sizes
Policy network sizes
Number of timesteps
Batchsize
VR-BGPO {b, m, c}
VR-BGPO λ
TRPO/PPO learning rate
MDPO learning rate
VRMPO learning rate
Value function learning rate

Pendulum-v2
500
32 × 32
64 × 64
5 × 106
100
{1.50, 2.0, 25}
1 × 10−2
2.5 × 10−3
3 × 10−3
5 × 10−3
2.5 × 10−3

DoublePendulum-v2 Walker2d-v2

500
32 × 32
64 × 64
5 × 106
100
{1.50, 2.0, 25}
1 × 10−2
2.5 × 10−3
3 × 10−3
3 × 10−4
2.5 × 10−3

500
32 × 32
64 × 64
1 × 107
100
{1.50, 2.0, 25}
1 × 10−2
2.5 × 10−3
3 × 10−3
1 × 10−2
2.5 × 10−3

Swimmer-v2
500
32 × 32
64 × 64
1 × 107
100
{1.50, 2.0, 25}
5 × 10−4
2.5 × 10−3
3 × 10−3
2 × 10−4
2.5 × 10−3

Reacher-v2
500
32 × 32
64 × 64
1 × 107
100
{1.50, 2.0, 25}
5 × 10−4
2.5 × 10−3
3 × 10−3
5 × 10−5
2.5 × 10−3

HalfCheetah-v2
500
32 × 32
64 × 64
1 × 107
100
{1.50, 2.0, 25}
5 × 10−4
2.5 × 10−3
3 × 10−3
5 × 10−5
2.5 × 10−3

Table 3: Setups of environments and hyper-parameters for experiments in section 6.4. The learning
rate of value functions are the same for all methods.

tance sampling weight used for VR-BGPO algorithm is clipped within [0.5, 1.5]. The momentum
term βk is set to be less or equal than one (βk = min(βk, 1.0) ) through the whole training process.

BGPO and VR-BGPO algorithms involve 4 hyper-parameters {λ, b, m, c}, which may bring addi-
tional efforts for hyper-parameter tuning. However, the actual hyper-parameter tuning is not so hard,
and we only use one set of {b, m, c} for 9 environments. The strategy of hyper-parameter tuning is
to separate the four hyper-parameters into two parts. The ﬁrst part is {b, m, c}, which mainly decide
when the momentum term βk actually affects (βk < 1.0) updates. The second part, λ, only affects
how fast the policy is learning. To further reduce the complexity of hyper-parameter tuning, we
always set m = 2. By grouping hyper-parameters, we only consider λ and how βk changes, which
largely simpliﬁes the process of hyper-parameter tuning.

22

