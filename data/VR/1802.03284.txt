9
1
0
2

n
u
J

4
2

]

C
O
.
h
t
a
m

[

3
v
4
8
2
3
0
.
2
0
8
1
:
v
i
X
r
a

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

1

Mini-batch Stochastic ADMMs for Nonconvex
Nonsmooth Optimization

Feihu Huang and Songcan Chen

Abstract—With the large rising of complex data, the nonconvex models such as nonconvex loss function and nonconvex regularizer
are widely used in machine learning and pattern recognition. In this paper, we propose a class of mini-batch stochastic ADMMs
(alternating direction method of multipliers) for solving large-scale nonconvex nonsmooth problems. We prove that, given an
appropriate mini-batch size, the mini-batch stochastic ADMM without variance reduction (VR) technique is convergent and reaches a
convergence rate of O(1/T ) to obtain a stationary point of the nonconvex optimization, where T denotes the number of iterations.
Moreover, we extend the mini-batch stochastic gradient method to both the nonconvex SVRG-ADMM and SAGA-ADMM proposed in
our initial manuscript [1], and prove these mini-batch stochastic ADMMs also reaches the convergence rate of O(1/T ) without
condition on the mini-batch size. In particular, we provide a speciﬁc parameter selection for step size η of stochastic gradients and
penalty parameter ρ of augmented Lagrangian function. Finally, extensive experimental results on both simulated and real-world data
demonstrate the effectiveness of the proposed algorithms.

Index Terms—ADMM, stochastic gradient, nonconvex optimization, graph-guided fused Lasso, overlapping group Lasso.

(cid:70)

1 INTRODUCTION

S Tochastic optimization [2] is a class of powerful opti-

mization tool for solving large-scale problems in ma-
chine learning, pattern recognition and computer vision. For
example, stochastic gradient descent (SGD [2]) is an efﬁcient
method for solving the following optimization problem,
which is a fundamental to machine learning,

min
x∈Rd

f (x) + g(x)

(1)

(cid:80)n

where f (x) = 1
i=1 fi(x) denotes the loss function, and
n
g(x) denotes the regularization function. The problem (1)
includes many useful models such as support vector ma-
chine (SVM), logistic regression and neural network. When
sample size n is large, even the ﬁrst-order methods become
computationally burdensome due to their per-iteration com-
plexity of O(nd). While SGD only computes gradient of one
sample instead of all samples in each iteration, thus, it has
only per-iteration complexity of O(d). Despite its scalability,
due to the existence of variance in stochastic process, the
stochastic gradient is much noisier than the batch gradi-
ent. Thus, the step size has to be decreased gradually as
stochastic learning proceeds, leading to slower convergence
than the batch method. Recently, a number of accelerated
algorithms have successfully been proposed to reduce this
variance. For example, stochastic average gradient (SAG [3])
obtains a fast convergence rate by incorporating the old
gradients estimated in the previous iterations. Stochastic
dual coordinate ascent (SDCA [4]) performs the stochastic
coordinate ascent on the dual problems to obtain also a
fast convergence rate. Moreover, an accelerated randomized
proximal coordinate gradient (APCG [5]) method acceler-
ates the SDCA method by using Nesterov’s accelerated

• The authors are with the College of Computer Science and Technology,
Nanjing University of Aeronautics and Astronautics, Nanjing 210016,
China. E-mail: {huangfeihu, s.chen}@nuaa.edu.cn

method [6]. However, these fast methods require much
space to store old gradients or dual variables. Thus, stochas-
tic variance reduced gradient (SVRG [7], [8]) methods are
proposed, and enjoy a fast convergence rate with no extra
space to store the intermediate gradients or dual variables.
Moreover, [9] proposes the SAGA method, which extends
the SAG method and enjoys better theoretical convergence
rate than both SAG and SVRG. Recently, [10] presents
an accelerated SVRG by using the Nesterov’s acceleration
technique [6]. Moreover, [11] proposes a novel momentum
accelerated SVRG method (Katyusha) via using the strongly
convex parameter, which reaches a faster convergence rate.
In addition, [12] specially proposes a class of stochastic
composite optimization methods for sparse learning, when
g(·) is a sparsity-inducing regularizer such as (cid:96)1-norm and
nuclear norm.

Though the above methods can effectively solve many
problems in machine learning, they are still difﬁcultly to be
competent for some complicated problems with the nonsep-
arable and nonsmooth regularization function as follows

min
x∈Rd

f (x) + g(Ax)

(2)

(cid:80)n

where A ∈ Rd×p is a given matrix, f (x) = 1
i=1 fi(x)
n
denotes the loss function, and g(x) denotes the regulariza-
tion function. With regard to g(·), we are interested in a
sparsity-inducing regularization functions, e.g. (cid:96)1-norm and
nuclear norm. The problem (2) includes the graph-guided
fuzed Lasso [16], the overlapping group Lasso [17], and
generalized Lasso [18]. It is well known that the alternating
direction method of multipliers (ADMM [19], [20], [21]) is
an efﬁcient optimization method for the problem (2). Specif-
ically, we can use auxiliary variable y = Ax to make the
problem (2) be suitable for the general ADMM form. When
sample size n is large, due to the need of computing the
empirical risk loss function on all training samples at each

 
 
 
 
 
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

2

TABLE 1
Summary of the existing stochastic ADMMs for the nonconvex optimization. (cid:88)denotes that the proposed methods can optimize the corresponding
nonconvex problems.

Convergence rate

Problems

Methods
Nonconvex incremental ADMM [13]
NESTT [14]
Nonconvex mini-batch stochastic ADMM (ours)
Nonconvex SVRG-ADMM (ours and [15])
Nonconvex SAGA-ADMM (ours)

minx

iteration, the ofﬂine or batch ADMM is unsuitable for large-
scale learning problems. Thus, the online and stochastic
versions of ADMM [22], [23], [24] have been successfully de-
veloped for the large-scale problems. Due to the existence of
variance in the stochastic process, these stochastic ADMMs
also suffer from the slow convergence rate. Recently, some
accelerated stochastic ADMMs are effectively proposed to
reduce this variance. For example, SAG-ADMM [25] is
proposed by additionally using the previous estimated gra-
dients. An accelerated stochastic ADMM [26] is proposed
by using Nesterov’s accelerated method [6]. SDCA-ADMM
[27] obtains linearly convergence rate for the strong problem
by solving its dual problem. SCAS-ADMM [28] and SVRG-
ADMM [29] are developed, and reach the fast convergence
rate with no extra space for the previous gradients or dual
variables. Moreover, [30] proposes an accelerated SVRG-
ADMM by using the momentum accelerated technique.
More recently, [31] proposes a fast stochastic ADMM, which
achieves a non-ergodic convergence rate of O(1/T ) for the
convex problem. In addition, an adaptive stochastic ADMM
[32] is proposed by using the adaptive gradients. Due to that
the penalty parameter in ADMM can affect convergence
[33], another adaptive stochastic ADMM [34] is proposed
by using the adaptive penalty parameters.

So far, the above study on stochastic optimization meth-
ods relies heavily on strongly convex or convex problems.
However, there exist many useful nonconvex models in ma-
chine learning such as nonconvex empirical risk minimiza-
tion models [35] and deep learning [36]. Thus, the study
of nonconvex optimization methods is much needed. Re-
cently, some works focus on studying the stochastic gradient
methods for the large-scale nonconvex optimizations. For
example, [37], [38] have established the iteration complexity
of O(1/(cid:15)2) for the SGD to obtain an (cid:15)-stationary solution of
the nonconvex problems. [39], [40], [41] have proved that the
variance reduced stochastic gradient methods such as the
nonconvex SVRG and SAGA reach the iteration complexity
of O(1/(cid:15)). At the same time, [42] has proved that the
variance reduced stochastic gradient methods also reach the
iteration complexity of O(1/(cid:15)) for the nonconvex nonsmooth
composite problems. More recently, [43] propose a faster
nonconvex stochastic optimization method (Natasha) via
using the strongly non-convex parameter. [44] proposes
a faster gradient-based nonconvex optimization by using
catalyst approach in [45].

Similarly, the above nonconvex methods are difﬁcult to
be competent to some complicated nonconvex problems,
such as nonconvex graph-guided regularization risk loss
minimizations [1] and tensor decomposition [46]. Recently,

i=1 fi(x) + g(x) minx

(cid:80)n

1
n
(cid:88), Unknown
(cid:88), O(1/T )
(cid:88), O(1/T )
(cid:88), O(1/T )
(cid:88), O(1/T )

1
n

(cid:80)n

i=1 fi(x) + g(Ax)

(cid:88), O(1/T )
(cid:88), O(1/T )
(cid:88), O(1/T )

some works [46], [47], [48], [49], [50] have begun to study
the ADMM method for the nonconvex optimization, but
they only focus on studying the deterministic ADMMs for
the nonconvex optimization. Due to the need of computing
the empirical loss function on all the training examples at
each iteration, these nonconvex ADMMs are not yet well
competent to the large-scale learning problems. Recently,
[13] has proposed a distributed, asynchronous and incre-
mental algorithm based on the ADMM method for the large-
scale nonconvex problems, but this method is difﬁcult for
the nonconvex problem (2) with the nonseparable and non-
smooth regularizers such as graph-guided fused lasso and
overlapping group lasso. A nonconvex primal dual splitting
(NESTT [14]) method is proposed for the distributed and
stochastic optimization, but it is also difﬁcult for the non-
convex problem (2). More recently, our initial manuscript
[1] proposes the stochastic ADMMs with variance reduc-
tion (e.g., nonconvex SVRG-ADMM and nonconvex SAGA-
ADMM) for optimizing these nonconvex problems with
some complicated structure regularizers such as graph-
guided fuzed Lasso, overlapping group Lasso, sparse plus
low-rank penalties. In addition, our initial manuscript [1]
and Zheng and Kwok’s paper [15] simultaneously propose
the nonconvex SVRG-ADMM method1. At present, to our
knowledge, there still exist two important problems needing
to be addressed:

1) Whether the general stochastic ADMM without VR tech-
nique is convergent for the nonconvex optimization?
2) What is convergence rate of the general stochastic ADMM

for the nonconvex optimization, if convergent?

In the paper, we provide the positive answers to them
by developing a class of mini-batch stochastic ADMMs
for the nonconvex optimization. Speciﬁcally, we study the
mini-batch stochastic ADMMs for optimizing the nonconvex
nonsmooth problem below:

min
x,y

1
n

n
(cid:88)

fi(x) + g(y)

(3)

i=1
s.t. Ax + By = c,

1.

of

our

The

ﬁrst

version

manuscript
[1](https://arxiv.org/abs/1610.02758v1) proposes both non-convex
SVRG-ADMM and SAGA-ADMM, which is online available in Oct. 10,
2016 . The ﬁrst version of [15] (https://arxiv.org/abs/1604.07070v1)
only proposes the convex SVRG-ADMM, which is online available in
Apr. 24, 2016 and named as ’Fast-and-Light Stochastic ADMM’. While,
the second version of [15] (https://arxiv.org/abs/1604.07070v2) adds
the non-convex SVRG-ADMM, which is online available in Oct. 12,
2016 and renamed as ’Stochastic Variance-Reduced ADMM’.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

3

(cid:80)n

where x ∈ Rd, y ∈ Rp, f (x) = 1
i=1 fi(x), each fi(x)
n
is a nonconvex and smooth loss function, g(y) is nonsmooth
and possibly nonconvex, and A ∈ Rq×d, B ∈ Rq×p and c ∈
Rq denote the given matrices and vector, respectively. The
problem (3) is inspired by the structural risk minimization in
machine learning [51]. In summary, our main contributions
are four-fold as follows:

1) We propose the mini-batch stochastic ADMM for the
nonconvex nonsmooth optimization. Moreover, we
prove that, given an appropriate mini-batch size, the
mini-batch stochastic ADMM reaches a fast conver-
gence rate of O(1/T ) to obtain a stationary point.
2) We extend the mini-batch stochastic gradient method
to both the nonconvex SVRG-ADMM and SAGA-
ADMM, proposed in our initial manuscript
[1].
Moreover, we prove that these stochastic ADMMs
also reach a convergence rate of O(1/T ) without
condition on the mini-batch size.

3) We provide a speciﬁc parameter selection for step
size η of stochastic gradients and penalty parameter
ρ of the augmented Lagrangian function.
Some numerical experiments demonstrate the effec-
tiveness of the proposed algorithms.

4)

In addition, Table 1 shows the convergence rate summary
of the stochastic/incremental ADMMs for optimizing the
nonconvex problems.

1.1 Notations
(cid:107) · (cid:107) denotes the Euclidean norm of a vector or the spectral
norm of a matrix. Ip denotes an p-dimensional identity
matrix. H (cid:31) 0 denotes a positive deﬁnite matrix H, and
H = xT Hx. Let A+ denote the generalized inverse of
(cid:107)x(cid:107)2
matrix A. φA
min denotes the smallest eigenvalues of ma-
max and φH
trix AT A. φH
min denotes the largest and smallest
eigenvalues of positive matrix H, respectively. The other
notations used in this paper is summarized as follows:
max)2)
˜L = L + 1, φH = (φH
,
and ζ1 = 5(φH
max)2
minη2 .
φA

max)2, ζ = 5(L2η2+(φH
minη2

min)2 + 20(φH

φA

2 NONCONVEX MINI-BATCH STOCHASTIC ADMM
WITHOUT VR
In this section, we propose a mini-batch stochastic ADMM
to optimize the nonconvex problem (3). Moreover, we study
convergence of the mini-batch stochastic ADMM. In partic-
ular, we prove that, given an appropriate mini-batch size, it
reaches the convergence rate of O(1/T ).

First, we review the deterministic ADMM for solving
the problem (3). The augmented Lagrangian function of (3)
is deﬁned as follows:

Lρ(x, y, λ) =f (x) + g(y) − (cid:104)λ, Ax + By − c(cid:105)

+

(cid:107)Ax + By − c(cid:107)2,

ρ
2

Next, we give a mild assumption, as in the general
stochastic optimization [37], [38] and the initial convex
stochastic ADMM [24].

Assumption 1. For smooth function f (x), there exists a stochas-
tic ﬁrst-order oracle that returns a noisy estimation to the gradient
of f (x), and the noisy estimation G(x, ξ) satisﬁes

E[G(x, ξ)] = f (x),
E(cid:2)(cid:107)G(x, ξ) − ∇f (x)(cid:107)2(cid:3) ≤ σ2,

(8)

(9)

where the expectation is taken with respect to the random variable
ξ.

Let M be the size of mini-batch I, and ξI =
{ξ1, ξ2, · · · , ξM } denotes a set of i.i.d. random variables, and
the stochastic gradient is given by

G(x, ξI) =

1
M

(cid:88)

i∈I

G(x, ξi).

Clearly, we have

E[G(x, ξI)] = ∇f (x),
E(cid:2)(cid:107)G(x, ξI) − ∇f (x)(cid:107)2(cid:3) ≤ σ2/M.

(10)

(11)

Algorithm 1 Mini-batch Stochastic ADMM (STOC-ADMM)
for Nonconvex Nonsmooth Optimization

1: Input: Number of iteration T , Mini-batch size 0 < M <

n and ρ > 0;

2: Initialize: x0, y0 and λ0;
3: for t = 0, 1, · · · , T − 1 do
4: Uniformly randomly pick a mini-batch It

from

5:

{1, 2, · · · , n};
yt+1 = arg miny Lρ(xt, y, λt);
xt+1 = arg minx ˜Lρ
λt+1 = λt − ρ(Axt+1 + Byt+1 − c);

6:
7:
8: end for
9: Output: Iterate x and y chosen uniformly random from

(cid:0)x; yt+1, λt, xt, G(xt, ξIt)(cid:1)

;

{xt, yt}T

t=1.

In the stochastic ADMM algorithm, we can update y
and λ by (5) and (7), respectively, as in the deterministic
ADMM. However, to update the variable x, we will deﬁne
an approximated function of the form:
˜Lρ

(cid:0)x; yt+1, λt,xt, G(xt, ξIt)(cid:1) = f (xt) + G(xt, ξIt)T (x − xt)

+

+

1
2η
ρ
2

(cid:107)x − xt(cid:107)2

H − (cid:104)λt, Ax + Byt+1 − c(cid:105)

(cid:107)Ax + Byt+1 − c(cid:107)2,

(12)

where E[G(xt, ξIt)] = ∇f (xt), η > 0 and H (cid:31) 0. By
minimizing (12) on the variable x, we have

xt+1 = (

(4)

H
η

+ρAT A)−1(cid:2) H
η

xt −G(xt, ξIt)−ρAT (Byt+1 −c−

)(cid:3).

λt
ρ

where λ is the Lagrange multiplier, and ρ is the penalty
parameter. At t-th iteration, the ADMM executes the update:

yt+1 = arg min

y

Lρ(xt, y, λt),

xt+1 = arg min

x

Lρ(x, yt+1, λt),

λt+1 = λt − ρ(Axt+1 + Byt+1 − c).

(5)

(6)

(7)

When AT A is large, computing ( H
η +ρAT A)−1 is expensive,
and storage of this matrix may still be problematic. To avoid
them, we can use the inexact Uzawa method [52] to linearize
the last term in (12). In other words, we set H = rI −ρηAT A
with

r ≥ rmin ≡ ηρ(cid:107)AT A(cid:107) + 1

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

4

to ensure H (cid:23) I. Then we have
η
r

xt+1 = xt −

(cid:2)G(xt, ξIt) + ρAT (At + Byt+1 − c −

)(cid:3).

λt
ρ

(13)

Lemma 1. Suppose the sequence {xt, yt, λt}T
Algorithm 1. The following inequality holds

t=1 is generated by

E(cid:107)λt+1 −λt(cid:107)2 ≤ ζ(cid:107)xt −xt−1(cid:107)2 +ζ1E(cid:107)xt+1 −xt(cid:107)2 +

10σ2
M φA

min

,

Finally, we give the algorithmic framework of the mini-
batch stochastic ADMM (STOC-ADMM) in Algorithm 1.

2.1 Convergence Analysis of Nonconvex Mini-batch
STOC-ADMM

In the subsection, we study the convergence and iteration
complexity of the nonconvex mini-batch STOC-ADMM.
First, we give some mild assumptions as follows:

Assumption 2. For smooth function f (x), its gradient is Lips-
chitz continuous with the constant L > 0, such that

(cid:107)∇f (x1) − ∇f (x2)(cid:107) ≤ L(cid:107)x1 − x2(cid:107), ∀x1, x2 ∈ Rd,

(14)

and this is equivalent to

f (x1) ≤ f (x2) + ∇f (x2)T (x1 − x2) +

L
2

(cid:107)x1 − x2(cid:107)2.

(15)

Assumption 3. Gradient of loss function f (x) is bounded, i.e.,
there exists a constant δ > 0 such that for all x, it follows
(cid:107)∇f (x)(cid:107)2 ≤ δ2.

Assumption 4. f (x) and g(y) are all lower bounded, and
denoting f ∗ = inf x f (x) and g∗ = inf y g(y).

Assumption 5. A is a full row or column rank.

Assumption 2 has been widely used in the convergence
analysis of nonconvex algorithms [39], [40]. Assumptions 3,4
have been used in study of ADMM for nonconvex optimza-
tions [46]. Assumption 5 has been used in the convergence
analysis of ADMM [46], [53]. Assumption 5 guarantees
the matrix AT A or AAT is non-singular. Without loss of
generality, we will use the full column rank matrix A below.
Next, we deﬁne the (cid:15)-stationary point of the nonconvex
problem (3) below:
Deﬁnition 1. For (cid:15) > 0, the point (x∗, y∗, λ∗) is said to be an
(cid:15)-stationary point of the nonconvex problem (3) if it holds that




E(cid:107)Ax∗ + By∗ − c(cid:107)2 ≤ (cid:15),
E(cid:107)∇f (x∗) − AT λ∗(cid:107)2 ≤ (cid:15),
E[dist(cid:0)BT λ∗, ∂g(y∗)(cid:1)2
where dist(cid:0)y0, ∂g(y)(cid:1) := inf{(cid:107)y0 − z(cid:107) : z ∈ ∂g(y)}, and
∂g(y) denotes the subgradient of g(y). If (cid:15) = 0, the point
(x∗, y∗, λ∗) is said to be a stationary point of (3).

] ≤ (cid:15),

(16)



Note that the above inequalities (16) are equivalent to
dist

(cid:0)0, ∂L(x∗, y∗, λ∗)(cid:1)2(cid:3) ≤ (cid:15), where

E(cid:2)

∂L(x, y, λ) =





∂L(x, y, λ)/∂x
∂L(x, y, λ)/∂y
∂L(x, y, λ)/∂λ


 ,

where ζ = 5(L2η2+(φH
minη2

φA

max)2)

and ζ1 = 5(φH
max)2
minη2 .
φA

A detailed proof of Lemma 1 is provided in Appendix
A.1. Lemma 1 gives the upper bound of E(cid:107)λt+1 − λt(cid:107)2.
Given a sequence {xt, yt, λt}T
t=1 generated from Algorithm
1, then we deﬁne a useful sequence

(cid:9)T
t=1 as follows:

(cid:8)Ψt

Ψt = E(cid:2)Lρ(xt, yt, λt) +

5(L2η2 + (φH
ρφA
minη2

max)2)

(cid:107)xt − xt−1(cid:107)2(cid:3).

(17)

For notational simplicity, let ˜L = L + 1, φH = (φH
20(φH

max)2 and ϕ = ( ˜L + 10L2/(ρφA

minρ.
Lemma 2. Suppose that the sequence {xt, yt, λt}T
ated by Algorithm 1. Let ρ∗ =
20(φH
minρ − ( ˜L + 10L2
ρφA
ρφA

40L2+ ˜L2
2φA
)(cid:1), and

min)) − φA

, (cid:52) = (φH

(cid:0)φA

max)2

˜L+

√

min

min

min

t=1 is gener-
min)2 +

min)2 +

ρ0 =

10φH

max

(cid:0) ˜LφH

max +

(cid:113)

˜L2(φH

max)2 + 2L2φH (cid:1)

φA
minφH

and suppose the parameters ρ and η, respectively, satisfy






√

(cid:52)

η ∈ (cid:0) φH
min −
ϕ
max)2
η ∈ (cid:0) 10(φH
minφH
ρφA
min
√
η ∈ (cid:0) φH
min −
ϕ

,

(cid:52)

√

,

φH
min +
ϕ
r − 1
ρ(cid:107)AT A(cid:107)
r − 1
ρ(cid:107)AT A(cid:107)

,

(cid:3),

(cid:52)

(cid:1),

ρ ∈ (cid:0)ρ0, ρ∗

(cid:1);

ρ = ρ∗;

(18)

(cid:3),

ρ ∈ (ρ∗, +∞).

Then we have γ = φH
and it holds that

min

η + φA

2 − ˜L
minρ

2 − 5(L2η2+2(φH
minη2

ρφA

max)2)

> 0,

1
T

T −1
(cid:88)

t=0

E(cid:107)xt − xt+1(cid:107)2 ≤

Ψ0 − Ψ∗
γT

+

(φA

minρ + 20)σ2
2γφA
minρM

. (19)

where Ψ∗ is a lower bound of sequence (cid:8)Ψt

(cid:9)T
t=1.

A detailed proof of Lemma 2 is provided in Appendix
(cid:9)T
A.2. Lemma 2 gives a property of the sequence
t=1.
Moreover, (18) provides a speciﬁc parameter selection on the
step size η and the penalty parameter ρ, in which selection
of the step size η depends on the parameter ρ. Next, we
deﬁne a useful variable θt deﬁned by:

(cid:8)Ψt

θt = (cid:2)(cid:107)xt+1 − xt(cid:107)2 + (cid:107)xt − xt−1(cid:107)2(cid:3).

(20)

Theorem 1. Suppose the sequence {xt, yt, λt}T
by Algorithm 1. Deﬁne κ1 = 3(L2 + (φH
ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2, and κ4 = φA

max)2
η2

minρ+20
2φA

minρ . Let

t=1 is generated
ρ2 , κ3 =

), κ2 = ζ

where L(x, y, λ) = f (x) + g(y) − (cid:104)λ, Ax + By − c(cid:105) is the
Lagrangian function of (3). In the following, based the above
assumptions and deﬁnition, we study the convergence and
iteration complexity of the mini-batch stochastic ADMM.

M ≥

2σ2
(cid:15)

max{κ1κ4 + 3, κ2κ4 +

T =

max{κ1, κ2, κ3}
(cid:15)γ

(Ψ1 − Ψ∗),

10
φA
minρ2

, κ3κ4},

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

5

where Ψ∗ is a lower bound of the sequence {Ψt}T
t=1. Let t∗ =
arg min2≤t≤T +1 θt, then (xt∗ , yt∗ ) is an (cid:15)-stationary point of
the problem (3).

Algorithm 2 Mini-batch SVRG-ADMM for Nonconvex Non-
smooth Optimization

1: Input: Mini-batch size M , epoch length m, T , S =

A detailed proof of Theorem 1 is provided in Ap-
pendix A.4. Theorem 1 shows that, given an mini-batch
size M = O(1/(cid:15)), the mini-batch stochastic ADMM has the
convergence rate of O( 1
T ) to obtain an (cid:15)-stationary point of
the nonconvex problem (3). Moreover, the IFO(Incremental
First-order Oracle [40]) complexity of
the mini-batch
stochastic ADMM is O(M/(cid:15)) = O(1/(cid:15)2) for obtaining an
(cid:15)-stationary point. While, the IFO complexity of the deter-
ministic proximal ADMM [46] is O(n/(cid:15)) for obtaining an
(cid:15)-stationary point. When n > 1
(cid:15) , the mini-batch stochastic
ADMM needs less IFO complexity than the deterministic
ADMM.

In the convergence analysis, given an appropriate mini-
batch size M satisﬁes the condition (21), the step size η only
need satisﬁes the condition (18) instead of η = O( 1√
) used
t
in the convex stochastic ADMM [24].

3 NONCONVEX MININ-BATCH SVRG-ADMM
In the subsection, we propose a mini-batch noncon-
vex stochastic variance reduced gradient ADMM (SVRG-
ADMM) to solve the problem (3), which uses a multi-stage
strategy to progressively reduce the variance of stochastic
gradients.

(cid:80)

(cid:0)∇fit (xs+1

Algorithm 2 gives an algorithmic

the stochastic gradient ˆ∇f (xs+1

framework of
mini-batch SVRG-ADMM for nonconvex optimizations.
) =
In Algorithm 2,
) − ∇fit(˜xs)(cid:1) + ∇f (˜xs) is unbiased,
1
it∈It
M
i.e., E[ ˆ∇f (xs+1
). In the following, we give
an upper bound of variance of the stochastic gradient
ˆ∇f (xs+1
Lemma 3. In Algorithm 2, set ∆s+1
then it holds

t = ˆ∇f (xs+1

)] = ∇f (xs+1

)−∇f (xs+1

).

),

t

t

t

t

t

t

t

E(cid:107)∆s+1
t

(cid:107)2 ≤

L2
M

(cid:107)xs+1

t − ˜xs(cid:107)2,

(21)

where E(cid:107)∆s+1
ˆ∇f (xs+1

).

t

t

(cid:107)2 denotes variance of the stochastic gradient

) has an upper bound O((cid:107)xs+1

A detailed proof of Lemma 3 is provided in Appendix
B.1. Lemma 3 shows that the variance of the stochastic
gradient ˆ∇f (xs+1
t − ˜xs(cid:107)2).
t
Due to ˜xs = xs
m, as number of iterations increases, both
xs+1
and ˜xs approach the same stationary point, thus the
t
variance of stochastic gradient vanishes. In fact, the variance
of stochastic gradient ˆ∇f (xs+1

) is progressively reduced.

t

3.1 Convergence Analysis of Nonconvex Mini-batch
SVRG-ADMM

ρ0 =

In the subsection, we study the convergence and iteration
complexity of the mini-batch nonconvex SVRG-ADMM.
First, we give an upper bound of E(cid:107)λs+1
t+1 − λs+1
t
t=1}S
t )m
t , λs
Lemma 4. Suppose the sequence {(xs
ated by Algorithm 2. The following inequality holds
5L2
φA
minM
+ ζ(cid:107)xs+1

5L2
φA
minM
t+1 − xs+1

t−1 (cid:107)2 + ζ1E(cid:107)xs+1

t − ˜xs(cid:107)2 +

t − xs+1

t+1 −λs+1

s=1 is gener-

E(cid:107)λs+1

E(cid:107)xs+1

(cid:107)xs+1

t , ys

(cid:107)2 ≤

(cid:107)2.

(cid:107)2.

t

t

t−1 − ˜xs(cid:107)2

m and λ0
m;

m, y0

[T /m], ρ > 0;
2: Initialize: ˜x0 = x0
3: for s = 0, 1, · · · , S − 1 do
xs+1
m, ys+1
0 = xs
0 = ys
4:
(cid:80)n
5: ∇f (˜xs) = 1
n
6:
7:

i=1 ∇fi(˜xs);
for t = 0, 1, · · · , m − 1 do

m and λs+1

0 = λs
m;

Uniformly randomly pick a mini-batch It from
{1, 2, · · · , n};
ys+1
t+1 = arg miny Lρ(xs+1
ˆ∇f (xs+1
) = 1
M
∇f (˜xs);
t+1 = arg minx ˜Lρ
xs+1

, y, λs+1
);
t
(cid:0)∇fit(xs+1

) − ∇fit(˜xs)(cid:1) +

t+1 , λs+1

, ˆ∇f (xs+1

(cid:0)x; ys+1

, xs+1
t

it∈It

(cid:80)

)(cid:1)

t

t

t

t

t

;

8:

9:

10:

t+1 + Bys+1

t − ρ(Axs+1

λs+1
t+1 = λs+1
end for
˜xs+1 = xs+1
m ;

11:
12:
13:
14: end for
15: Output: Iterate x and y chosen uniformly random from

t+1 − c);

{(xs

t , ys

t )m

t=1}S

s=1.

t )m

t , ys

t , λs

t )m
t=1

A detailed proof of Lemma 4 is provided in Ap-
pendix B.2. Given the sequence {(xs
s=1 gener-
ated from Algorithm 2, then we deﬁne a useful sequence
(cid:8)(Φs
t =E(cid:2)Lρ(xs
Φs
ζ
(cid:107)xs
ρ

(cid:9)S
s=1 as follows:
t , λs
t ) + hs
t−1(cid:107)2(cid:3),

t − ˜xs−1(cid:107)2 + (cid:107)xs

t , ys
t − xs

t−1 − ˜xs−1(cid:107)2)

t=1}S

t ((cid:107)xs

(22)

+

t )m

t=1}S

where {(hs
Lemma 5. Suppose the sequence {(xs
s=1 is gen-
erated from Algorithm 2, and suppose the positive sequence
{(hs

s=1 is a positive sequence.
t , λs

t=1}S

t , ys

t )m

t )m

t=1}S




hs
t =

s=1 satisﬁes, for s = 1, 2, · · · , S
minρ)L2
minM

(10 + φA
2ρφA

(2 + β)hs

t+1 +

10L2
φA
minρM

,

t = m,

, 1 ≤ t ≤ m − 1,

where β > 0. Let ˆh = mint{(1 + 1
max)2
(φH
˜L+2ˆh+

t+1, hs+1
minρ − ( ˜L + 2ˆh + 10L2
ρφA

min)2 + 20(φH

β )hs

(cid:0)φA

√

min

1

min

ρφA
40L2+( ˜L+2ˆh)2
2φA

min

, and

(23)
}, (cid:52)1 =
)(cid:1), ρ∗ =

(cid:18)

10φH

max

( ˜L + 2ˆh)φH

max +

(cid:113)

( ˜L + 2ˆh)2(φH

max)2 + 2L2φH

(cid:19)

φA
minφH

and suppose the parameters ρ and η, respectively, satisfy

√

√

(cid:52)1






η ∈ (cid:0) φH
min −
ϕ1
η ∈ (cid:0) 10(φH
max)2
minφH
ρφA
min
√
η ∈ (cid:0) φH
min −
ϕ1

,

(cid:52)1

,

φH
min +
ϕ1
(cid:3),

r − 1
ρ(cid:107)AT A(cid:107)
r − 1
ρ(cid:107)AT A(cid:107)

,

(cid:52)1

(cid:1),

ρ ∈ (cid:0)ρ0, ρ∗

(cid:1);

ρ = ρ∗;

(24)

(cid:3),

ρ ∈ (ρ∗, +∞).

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
where ϕ1 = ( ˜L + 2ˆh + 10L2/(ρφA
that the sequence {(Γs

min)) − φA
s=1 is positive, deﬁned by

minρ. Then it holds

t )m

Γs

t =





φH
min
η
φH
min
η

+

+

φA
minρ
2
φA
minρ
2

−

−

t=1}S
˜L
2
˜L
2

−

−

ζ +ζ1
ρ
ζ + ζ1
ρ

−(1+

1
β

)ht+1, 1 ≤ t ≤ m − 1

−hs+1
1

,

t = m

and the sequence (cid:8)(Φs

t )m
t=1

(cid:9)S
s=1 monotonically decreases.

(25)

(cid:8)(Φs

t )m
t=1

A detailed proof of Lemma 5 is provided in Appendix
(cid:9)S
B.3. Lemma 5 shows that the sequence
s=1 mono-
tonically decreases. Moreover, (24) provides a speciﬁc pa-
rameter selection on the step size η and the penalty param-
eter ρ in Algorithm 2.
Lemma 6. Suppose the sequence {(xs
s=1 is gener-
ated by Algorithm 2. Under the same conditions as in Lemma 5,
the sequence (cid:8)(Φs

t=1}S

t , λs

t , ys

t )m

t )m
t=1

Lemma 6 shows that the sequence

(cid:9)S
s=1 has a
lower bound. The proof of Lemma 6 is the same as the proof
of Lemma 7 in [1]. Next, we deﬁne a useful variable ˆθs
t as
follows:

t )m
t=1

(cid:9)S
s=1 has a lower bound.
(cid:8)(Φs

t =E(cid:2)(cid:107)xs
ˆθs
+ (cid:107)xs

t − ˜xs−1(cid:107)2 + (cid:107)xs
t−1(cid:107)2(cid:3).
t − xs

t−1 − ˜xs−1(cid:107)2 + (cid:107)xs

t+1 −xs

t (cid:107)2

(26)

In the following, we will analyze the convergence proper-
ties of the nonconvex SVRG-ADMM based on the above
lemmas.
t=1}S
Theorem 2. Suppose the sequence {(xs
s=1 is
generated by Algorithm 2. Denote κ1 = 3(cid:0)L2 + (φH
max)2
(cid:1),
η2
κ2 = ζ
ρ2 , κ3 = ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2, and γ = min(t,s) Γs
t and
t+1 + L2
ω = min(s,t){(2 + β)hs
2M ,

5L2
minρM }. Let
φA

t , λs

t , ys

t )m

mS = T =

max{κ1, κ2, κ3}
τ (cid:15)
where τ = min(γ, ω), and Φ∗ is a lower bound of the sequence
(cid:8)(Φs

1 − Φ∗),

(Φ1

(27)

t )m
t=1

(cid:9)S
s=1 . Let

(t∗, s∗) = arg min

1≤t≤m, 1≤s≤S

ˆθs
t ,

then (xs∗

t∗ , ys∗

t∗ ) is an (cid:15)-stationary point of the problem (3).

A detailed proof of Theorem 2 is provided in Appendix
B.4. Theorem 2 shows that the mini-batch SVRG-ADMM for
nonconvex optimizations has a convergence rate of O( 1
T ).
Moreover, the IFO complexity of the mini-batch SVRG is
O(cid:0)( n
m + M < n, the mini-batch SVRG-
ADMM needs less IFO complexity than the deterministic
ADMM.

m + M )/(cid:15)(cid:1)

. When n

6

4 NONCONVEX MINI-BATCH SAGA-ADMM

In the subsection, we propose a mini-batch nonconvex
stochastic average gradient ADMM (SAGA-ADMM) by ad-
ditionally using the old gradients estimated in the previous
iteration, which is inspired by the SAGA method [9].

(cid:80)

it∈It

The algorithmic framework of the SAGA-ADMM is
given in Algorithm 3. In Algorithm 3, the stochastic gradient
)(cid:1) + ψt is unbiased
(cid:0)∇fit(xt) − ∇fit(zt
ˆ∇f (xt) = 1
it
M
(cid:80)n
(i.e., E[ ˆ∇f (xt)] = ∇f (xt)), where ψt = 1
i=1 ∇fi(zt
i ). In
n
the following, we give an upper bound of the variance of
the stochastic gradient ˆ∇f (xt).
Lemma 7. For Algorithm 3, Let ∆t = ˆ∇f (xt) − ∇f (xt), then
it holds

E(cid:107)∆t(cid:107)2 ≤

L2
M n

n
(cid:88)

i=1

(cid:107)xt − zt

i (cid:107)2,

(28)

where E(cid:107)∆t(cid:107)2 denotes variance of
ˆ∇f (xt).

the stochastic gradient

A detailed proof of Theorem 7 is provided in Appendix
C.1. Lemma 7 shows that the variance of the stochastic
gradient ˆ∇f (xt) has an upper bound O( 1
i (cid:107)2).
n
As the number of iteration increases, both xt and the stored
points {zt}n
i=1 approach the same stationary point, so the
variance of stochastic gradient progressively reduces. In
fact, the variance of stochastic gradient ˆ∇f (xt) is progres-
sively reduced via additionally using the old gradients in
the previous iterations.

i=1 (cid:107)xt −zt

(cid:80)n

Algorithm 3 Mini-batch SAGA-ADMM for Nonconvex
Nonsmooth Optimization
1: Input: x0 ∈ Rd, y0 ∈ Rq, z0
number of iterations T ;

i = x0 for i ∈ {1, 2, · · · , n},

2: Initialize: ψ0 = 1
n
3: for t = 0, 1, · · · , T − 1 do
4: Uniformly randomly pick a mini-batch It

i=1 ∇fi(z0

i );

(cid:80)n

from

5:

6:

7:
8:
9:

{1, 2, · · · , n};
yt+1 = arg miny Lρ(xt, y, λt);
(cid:80)
ˆ∇f (xt) = 1
it∈It
M
(cid:80)n
ψt = 1
i=1 ∇fi(zt
n
xt+1 = arg minx ˜Lρ
λt+1 = λt − ρ(Axt+1 + Byt+1 − c);
zt+1
it
ψt+1 = ψt − 1
n

i = zt
it∈It

(cid:0)∇fit(xt) − ∇fit(zt
it
i );
(cid:0)x; yt+1, λt, xt, ˆ∇f (xt)(cid:1)

= xt+1 and zt+1

(cid:0)∇fit(zt
it

(cid:80)

;

i for i (cid:54)= it, for all it ∈ It;
) − ∇fit (zt+1

)(cid:1)

;

)(cid:1) + ψt with

10:
11: end for
12: Output: Iterate x and y chosen uniformly random from

it

{xt, yt}T

t=1.

Since the mini-batch SVRG-ADMM uses VR technique,
its convergence does not depend on the mini-batch size
M . In other words, when M = 1, the mini-batch noncon-
vex SVRG-ADMM reduces to the initial nonconvex SVRG-
ADMM in [1], which also has a convergence rate of O( 1
T ).
However, by Lemma 3, the variance of stochastic gradient
in the mini-batch SVRG-ADMM decreases faster than that
in the initial nonconvex SVRG-ADMM.

4.1 Convergence Analysis of Nonconvex Mini-batch
SAGA-ADMM

In the subsection, we study the convergence and iteration
complexity of the nonconvex mini-batch SAGA-ADMM.
First, we give same useful lemmas as follows:

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

7

Lemma 8. Suppose the sequence {xt, yt, λt}T
Algorithm 3. The following inequality holds

t=1 is generated by

E(cid:107)λt+1 − λt(cid:107)2 ≤

n
(cid:88)

(cid:0)E(cid:107)xt − zt

5L2
φA
minM n
+ ζ(cid:107)xt − xt−1(cid:107)2 + ζ1E(cid:107)xt+1 − xt(cid:107)2.

i (cid:107)2 + (cid:107)xt−1 − zt−1

i=1

i

(cid:107)2(cid:1)

Lemma 8 gives an upper bound of E(cid:107)λt+1 − λt(cid:107)2. Its
proof is the same as that of Lemma 4. Given the sequence
{xt, yt, λt}T
t=1 generated by Algorithm 3, then we deﬁne a
useful sequence {Θt}T

t=1 below:

TABLE 2
Comparing the best IFO and EI complexity of different algorithms. The
complexity is measured in terms of the number of oracle calls required
to achieve an (cid:15)-stationary point (see Deﬁnition 2).

EI
Algorithms
O(1/(cid:15))
Deterministic ADMM
O(1/(cid:15))
Mini-batch STOC-ADMM
Mini-batch SVRG-ADMM O(cid:0)n + M/(cid:15)(cid:1) O(1/(cid:15))
O(1/(cid:15))
Mini-batch SAGA-ADMM O(n + M/(cid:15))

IFO
O(n/(cid:15))
O(1/(cid:15)2)

Fixed Step Size ?
(cid:88)
(cid:88)
(cid:88)
(cid:88)

iteration complexity of the SAGA-ADMM based on the
above lemmas. We deﬁne a useful variable ˜θt deﬁned by:

((cid:107)xt − zt

i (cid:107)2 + (cid:107)xt−1 − zt−1

i

(cid:107)2)

˜θt =(cid:2)(cid:107)xt+1 − xt(cid:107)2 + (cid:107)xt − xt−1(cid:107)2 +

(29)

+ (cid:107)xt−1 − zt−1

i

(cid:107)2)(cid:3).

1
n

n
(cid:88)

i=1

((cid:107)xt − zt

i (cid:107)2

(33)

Θt =E(cid:2)Lρ(xt, yt, λt) +

αt
n

n
(cid:88)

i=1

+

ζ
ρ

(cid:107)xt − xt−1(cid:107)2(cid:3),

where {αt}T
t=1 is a decreasing positive sequence.
Lemma 9. Suppose that the sequence {xt, yt, λt}T
ated by Algorithm 3, and the positive sequence {αt}T

t=1 is gener-
t=1 satisfy

αt =

10L2 + φA
2ρφA

minρL2

+ (

2n − M
n

minM
where β > 0. Let ˆα = mint{ n−M
max)2
(φH
˜L+2 ˆα+

min)2 + 20(φH

(cid:0)φA

√

ρφA
40L2+( ˜L+2 ˆα)2
2φA

, and

min

+

n − M
n
n (1 + 1

minρ − ( ˜L + 2ˆα + 10L2
ρφA

min

β)αt+1, (30)

β )αt+1}, (cid:52)2 =
)(cid:1), ρ∗ =

min
(cid:18)

10φH

max

( ˜L + 2ˆα)φH

max +

(cid:113)

( ˜L + 2ˆα)2(φH

max)2 +2L2φH

(cid:19)

ρ0 =

φA
minφH

and suppose the parameters ρ and η, respectively, satisfy

(cid:1),

ρ ∈ (cid:0)ρ0, ρ∗

(cid:1);

√

(cid:52)2

√



,

,

(cid:52)2




φH
min +
ϕ2
(cid:3),

η ∈ (cid:0) φH
min −
ϕ2
max)2
η ∈ (cid:0) 10(φH
r − 1
minφH
ρφA
ρ(cid:107)AT A(cid:107)
min
√
η ∈ (cid:0) φH
min −
r − 1
ρ(cid:107)AT A(cid:107)
ϕ2
where ϕ2 = ( ˜L + 2ˆα + 10L2/(ρφA
the sequence {Γt}T

(cid:52)2

,

(cid:3),

t=1 is positive, deﬁned by

ρ = ρ∗;

(31)

ρ ∈ (ρ∗, +∞),

min)) − φA

minρ. Then it holds

Γt =

φH
min
η

+

φA
minρ
2

−

˜L
2

−

ζ + ζ1
ρ

−

n − M
n

(1 +

1
β

)αt+1,

(32)

and the sequence {Θt}T

t=1 monotonically decreases.

A detailed proof of Lemma 9 is provided in Appendix
C.2. Lemma 9 shows that the sequence {Θt}T
t=1 monotoni-
cally decreases. Moreover, (31) provides a speciﬁc parameter
selection on the step size η and the penalty parameter ρ in
Algorithm 3.
Lemma 10. Suppose the sequence {xt, yt, λt}T
t=1 is generated
by Algorithm 3. Under the same conditions as in Lemma 9, the
sequence {Θt}T

t=1 has a lower bound.
Lemma 10 shows that the sequence {Θt}T

t=1 has a lower
bound. Its proof is the same as the proof of Lemma 7 in
[1]. In the following, we will study the convergence and

Theorem 3. Suppose the sequence {xt, yt, λt}T
by Algorithm 3. Denote κ1 = 3(cid:0)L2 + (φH
ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2, and γ = mint Γt and ω = mint
n−M

max)2
η2

t=1 is generated
(cid:1), κ2 = ζ
ρ2 , κ3 =
(cid:8) L2
2M +( 2n−M
n +

n β)αt+1

(cid:9). Let

T =

max{κ1, κ2, κ3}
τ (cid:15)

(Θ1 − Θ∗),

(34)

where τ = min (cid:8)γ, ω(cid:9) > 0, and Θ∗ is a lower bound of the
(cid:9)T
sequence (cid:8)Θt
˜θt, then (xt∗ , yt∗ )
t=1. Let t∗ = arg min2≤t≤T +1
is an (cid:15)-stationary point of the problem (3).

A detailed proof of Theorem 3 is provided in Appendix
C.3. Theorem 3 shows that the mini-batch SAGA-ADMM for
nonconvex optimizations has a convergence rate of O( 1
T ).
Moreover, the IFO complexity of the mini-batch SAGA-
ADMM is O(M/(cid:15)) for obtaining an (cid:15)-stationary point.
Clearly, due to 1 ≤ M < n, the mini-batch SAGA-ADMM
needs less IFO complexity than the deterministic ADMM.

Since the mini-batch SAGA-ADMM also uses VR tech-
nique, its convergence does not depend on the mini-batch
size M . In other words, when M = 1, the mini-batch
nonconvex SAGA-ADMM reduces to the initial nonconvex
SAGA-ADMM in [1], which also has the convergence rate of
O( 1
T ). However, by Lemma 7, the variance of stochastic gra-
dient in the mini-batch nonconvex SAGA-ADMM decreases
faster than that in the initial nonconvex SAGA-ADMM.

Finally, in Table 2, we give the IFO(Incremental First-
order Oracle [40]) and EI (Effective Iteration) of both the
mini-batch stochastic ADMMs and the deterministic (or
batch) ADMM. Speciﬁcally, the deﬁnition of EI is given in
Deﬁnition 2. From Table 2, we can ﬁnd that though both the
mini-batch stochastic and deterministic ADMMs have the
same EI complexity, the mini-batch stochastic ADMMs has
lower IFO complexity than the deterministic ADMM when
M < n. In the above theoretical analysis, the mini-batch size
M of the nonconvex STOC-ADMM may be very large when
(cid:15) is small. However, the following extensive experimental
results show that STOC-ADMM still has good performances
given a moderate M , and is comparable with both SVRG-
ADMM and SAGA-ADMM.

Deﬁnition 2. For ADMM and its variants, an EI describes the
fact that all the primal and dual variables in the algorithm are
updated once.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

8

5 EXPERIMENTS

In this section, we perform some numerical experiments
on both simulated and real-world data to examine per-
formances of the proposed algorithms for the noncon-
vex nonsmooth optimization2.
In the experiments, we
compare nonconvex mini-batch stochastic ADMM (STOC-
ADMM) with nonconvex mini-batch SVRG-ADMM, non-
convex mini-batch SAGA-ADMM and deterministic ADMM
(DETE-ADMM). In the experiments, we use the inexact
Uzawa method to both mini-batch stochastic ADMMs and
deterministic (or batch) ADMM. In the following, all algo-
rithms are implemented in MATLAB, and all experiments
are performed on a PC with an Intel E5-2630 CPU and 32GB
memory.

5.1 Simulated Data

In the subsection, we compare the performances in some
synthetic data. Here we focus on the binary classiﬁcation
task problem with the graph-guided fused lasso and the
overlapping group lasso regularization functions, respec-
tively. Given a set of training samples (ai, bi)n
i=1, where
ai ∈ Rd, bi ∈ {−1, +1}, then we solve the following
nonconvex nonsmooth optimization problem:

min
x∈Rd

1
n

n
(cid:88)

i=1

fi(x) + ν(cid:107)Ax(cid:107)1,

(35)

1

1+exp(biaT

where fi(x) =
i x) is the sigmoid loss function [39],
which is nonconvex and smooth, and ν denotes a nonegative
regularization parameter. When using graph-guided fused
lasso [16] in (35), we let A to decode the sparsity pattern
of graph, which is obtained by sparse precision matrix
estimation [54], [55]. When using overlapping group lasso
[16] in (35), we let Ax be concatenation of k−repetitions of
x (i.e., Ax = [x; . . . ; x] ) as in [23], where k denotes the
number of overlapping group of unknown parameter.

5.1.1 Graph-guided Fused Lasso

Here we compare the performances in some simulated data,
where a graph-guided fused lasso regularization is imposed.
First, we generate a sparse precision matrix Λ ∈ Rd×d with
elements

Λij

i.i.d.∼

(cid:40)

0,
prob. 0.95
U nif ([−0.75, −0.25] (cid:83)[0.25, 0.75]), otherwise.

Then the input feature vectors {ai}n
i=1 are i.i.d. generated
from multivariate normal distribution N (0, Λ−1). The true
vector parameter x∗ ∈ Rd is generated from the standard
normal distribution. The output label is generated as bi =
i x∗ + (cid:15)i), where (cid:15)i is chosen uniformly at random
sign(aT
from [0, 1].

In the experiment, we set d = 200, and then generate
n = {20000, 40000, 60000} samples (ai, bi)n
i=1, respectively.
For each dataset, we choose half of the samples as training
data, while use the rest as testing data. In the problem (35),
we use a graph-guided fused lasso, and ﬁx the regulariza-
tion parameter ν = 10−5. In the algorithms, we use the same
initial solution x0 from the standard normal distribution

2. We will put our code online once this paper is accepted.

and choose the step size η = 1. In addition, we choose
the mini-batch size M = 100 in the stochastic algorithms,
and m = [n/M ] in the mini-batch SVRG-ADMM. Finally,
all experimental results are averaged over 10 repetitions.

Figs.1 and 2 show that both the objective values and test
loss of these stochastic ADMMs faster decrease than those
of the deterministic ADMM, as CPU time consumed in-
creases. In particular, though the nonconvex STOC-ADMM
uses a ﬁxed step size η, it shows good performance in
the nonconvex optimization with graph-guided fused lasso
regularization, and is comparable with both the nonconvex
SVRG-ADMM and SAGA-ADMM.

5.1.2 Overlapping Group Lasso

Here we compare the performances in some simulated
data, where an overlapping group lasso regularization is
imposed. First, we generate n input feature vector {ai}n
i=1
with the dimension d = 400, where each feature is i.i.d.
generated from the standard normal distribution. Next, we
generate a sparse matrix X ∈ R20×20 , where only the ﬁrst
column is non-zero (generated i.i.d. from standard normal
distribution) and other columns are zero, and the true
parameter vector x∗ is vectorization of the matrix X. The
output label bi is generated as bi = sign(aT
i x∗ + (cid:15)i), where
(cid:15)i is the standard normal distribution. Then, we generate
n = {20000, 40000, 60000} samples (ai, bi)n
i=1, respectively.
In the experiment, we choose the mini-batch size M = 200
in these stochastic algorithms, and the other settings are
the similar as the above graph-guided fused lasso task. In
the problem (35), we use a overlapping group lasso penalty
function g(x) = ν((cid:80)20
i=1 (cid:107)Xi,.(cid:107) + (cid:80)20
j=1 (cid:107)X.,j(cid:107)). Then we let
A = [I; I] as in [23], and ﬁx the parameter ν = 10−5.

Figs. 3 and 4 show that both the objective values and test
loss of these stochastic ADMMs faster decrease than those
of the deterministic ADMM, as CPU time consumed in-
creases. In particular, though the nonconvex STOC-ADMM
uses a ﬁxed step size η, it shows good performance in
the nonconvex optimization with overlapping group lasso
regularization, and is comparable with both the nonconvex
SVRG-ADMM and SAGA-ADMM.

5.2 Real Data

In the subsection, we compare the performances in some
real data. Speciﬁcally, we perform the binary classiﬁcation
task and multitask learning, respectively.

5.2.1 Graph-guided Fused Lasso

Here we perform the binary classiﬁcation task with the
graph-guided fused lasso penalty function as in (35). In
the experiment, we use some publicly available datasets3,
which are summarized in Table 3. In the algorithms, we
use the same initial solution x0 from the standard normal
distribution and choose a ﬁxed step size η = 1. In the
problem (35), we ﬁx the parameter ν = 10−5. In addition,
we choose the mini-batch size M = 100 in these stochastic
algorithms. The following experimental results are averaged
over 10 repetitions.

3. 20news is from the website (https://cs.nyu.edu/ roweis/data.html);
ijcnn1 and covtype.binary are from the LIBSVM website

a9a, w8a,
(www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/).

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

9

(a) n = 20, 000

(b) n = 40, 000

(c) n = 60, 000

Fig. 1. Objective value versus CPU time on the simulated nonconvex model with graph-guided fused Lasso.

(a) n = 20, 000

(b) n = 40, 000

(c) n = 60, 000

Fig. 2. Test error versus CPU time on the simulated nonconvex model with graph-guided fused Lasso.

(a) n = 20, 000

(b) n = 40, 000

(c) n = 60, 000

Fig. 3. Objective value versus CPU time on the simulated nonconvex model with overlapping group Lasso.

(a) n = 20, 000

(b) n = 40, 000

(c) n = 60, 000

Fig. 4. Test error versus CPU time on the simulated nonconvex model with overlapping group Lasso.

00.511.522.533.544.5510−610−510−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101510−610−510−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101510−610−510−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM00.511.522.533.544.5500.050.10.150.20.250.30.350.40.450.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101500.050.10.150.20.250.30.350.4CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101500.050.10.150.20.250.30.350.4CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0246810121416182010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101520253010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0510152025303540455010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0246810121416182000.10.20.30.40.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101520253000.10.20.30.40.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0510152025303540455000.10.20.30.40.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMMIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

10

TABLE 3
Real data for graph-guided fused lasso

datasets
20news
a9a
w8a
ijcnn1
covtype.binary

#training
8,121
16,281
32,350
63,351
290,506

#test
8121
16,280
32,350
63,351
290,506

#f eatures #classes

100
123
300
22
54

2
2
2
2
2

TABLE 4
Real data for multitask learning

datasets #training

letter
sensorless
mnist
covtype
mnist8m

7,500
29,255
30,000
290,506
4,050,000

#test
7,500
29,254
30,000
290,506
4,050,000

#f eatures #classes

16
48
780
54
780

26
11
10
7
10

Figs. 5 and 6 show that both the objective values and test
loss of these stochastic ADMMs faster decrease than those
of the deterministic ADMM, as CPU time consumed in-
creases. In particular, though the nonconvex STOC-ADMM
uses a ﬁxed step size η, it shows good performance in
the nonconvex optimization with graph-guided fused lasso
regularization, and is comparable with both the nonconvex
SVRG-ADMM and SAGA-ADMM. Due to that ijcnn1 is a
severely imbalanced data set (i.e., includes large negative
samples), the ﬁrst iteration solution of all algorithms shows
good performance on the testing error.

5.2.2 Multi-task Learning
Here we perform the multi-task learning with both sparse
and low-rank penalty functions. Speciﬁcally, given a set
i=1, where ai ∈ Rd and bi ∈
of training samples (ai, bi)n
{1, 2, · · · , m}. Let ¯bi,c = 1 if bi = c ∈ {1, 2, · · · , m}, and
¯bi,c = 0 otherwise. Then we solve the following nonconvex
problem

min
X∈Rm×d

1
n

n
(cid:88)

fi(X) + ν1

m,d
(cid:88)

κ(|Xi,j|) + ν2(cid:107)X(cid:107)∗,

(36)

i=1

i,j=1
where fi(X) = log((cid:80)m
c=1 exp(X T
c,.ai is
a multinomial logistic loss function, κ(α) = β log(1 + α
θ )
is the nonconvex log-sum penalty function [56], and (cid:107)X(cid:107)∗
denotes the nuclear norm of matrix X. Here ν1 and ν2 are
nonegative regularization parameters. Following [57], we
can transform the problem (36) into the following problem

c,.ai)) − (cid:80)m

¯bi,cX T

c=1

min
X∈Rm×d

1
n

n
(cid:88)

¯fi(X) + ˘g(X)

(37)

(cid:0) (cid:80)m,d

i,j=1 κ(|Xi,j|) − κ0(cid:107)X(cid:107)1

i=1
(cid:1)
where ¯fi(X) = f (X) + ν1
,
˘g(X) = ν1κ0(cid:107)X(cid:107)1 + ν2(cid:107)X(cid:107)∗, and κ0 = κ(cid:48)(0). By Propo-
sition 2.3 in [57], ¯fi(X) is nonconvex and smooth, and
˘g(X) is nonsmooth and convex. To solve the problem (37)
by using ADMMs, we introduce an auxiliary variable Y
with the constraint X = Y , and given A = [I; I], then
˘g(AX) = ν1κ0(cid:107)X(cid:107)1 + ν2(cid:107)Y (cid:107)∗.

In the experiment, we use some publicly available
datasets4, which are summarized in Table 4. We use the

4.

letter, sensorless, covtype, mnist and mnist8m are from the LIBSVM

website (www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/).

mini-batch size of M = 100 on letter, M = 300 on sensorless
and mnist, M = 500 on covtype, and M = 1000 on mnist8m.
In the algorithms, we use the same initial solution x0 from
the standard normal distribution and choose a ﬁxed step
size η = 0.8. In the problem (36), we ﬁx the regularization
parameters ν1 = 10−5 and ν2 = 10−4.

Figs. 7 and 8 show that both objective values and test
loss of the stochastic ADMMs faster decrease than those
of the deterministic ADMM, as CPU time consumed in-
creases. In particular, though the nonconvex STOC-ADMM
uses a ﬁxed step size η, it shows good performance in
the nonconvex multi-task learning with spare and low-rank
regularization functions, and is comparable with both the
nonconvex SVRG-ADMM and SAGA-ADMM. Due to large
training samples, the stochastic gradient of SAGA-ADMM
includes many old gradients, and slowly updates.

5.3 Varying ρ

In the subsection, we demonstrate the speciﬁc parameter
selection for step size η of stochastic gradient and penalty
parameter ρ of augmented Lagrangian function. Speciﬁcally,
we give a ﬁxed η, then ﬁnd an optimal ρ. In the experiment,
we use the above simulated data imposed the overlapping
group lasso regularization function, and set n = 40, 000, d =
400. In the problem (35), we ﬁx the regularization parameter
ν = 10−5. In the algorithms, we ﬁx the step size η = 1.

Figs. 9, 10 and 11 show the objective value and test error
versus CPU time with different ρ. From these results, we
can ﬁnd that given an appropriate step size η, the proposed
mini-batch stochastic algorithms have good performances
in a wide-range of parameter ρ. In particular, when the
parameter ρ satisﬁes the above conditions (18), (24) and (31),
these mini-batch algorithms show good performances.

6 CONCLUSION

In the paper, we have studied the mini-batch stochas-
tic ADMMs for the nonconvex nonsmooth optimization.
We have theoretically proved that, give mini-batch size
M = O(1/(cid:15)), the mini-batch stochastic ADMM without
VR (STOC-ADMM) has the convergence rate of O(1/T )
to obtain an (cid:15)-stationary point. In theoretical analysis, the
mini-batch size M may be very large when (cid:15) is small.
However, the above extensive experimental results show
that STOC-ADMM still has good performances given a
moderate M , and is comparable with both SVRG-ADMM
and SAGA-ADMM. In particular, as long as the step size η
and the penalization parameter ρ satisfy the above condition
(18) instead of η = O( 1
t ) used in the convex stochastic
ADMM [24], STOC-ADMM is convergent, and reaches a
convergence rate of O(1/T ).

Moreover, we have extended the mini-batch stochastic
gradient method to both the non-convex SVRG-ADMM
and SAGA-ADMM proposed in our initial manuscript [1],
and also proved that these mini-batch stochastic ADMMs
reach the convergence rate of O(1/T ). Though both SVRG-
ADMM and SAGA-ADMM reach the convergence rate of
O(1/T ) without the condition on M , SVRG-ADMM re-
quires frequently compute gradients over the full data, and
SAGA-ADMM requires memory of the same size for storing

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

11

(a) 20news

(b) a9a

(c) w8a

(d) ijcnn1

(e) covtype.binary

Fig. 5. Objective value versus CPU time of the nonconvex graph-guided binary classiﬁcation model on some real datasets.

(a) 20news

(b) a9a

(c) w8a

(d) ijcnn1

(e) covtype.binary

Fig. 6. Test loss versus CPU time of the nonconvex graph-guided binary classiﬁcation model on some real datasets.

(a) letter

(b) sensorless

(c) mnist

(d) covtype

(e) mnist8m

Fig. 7. Objective value versus CPU time of the nonconvex multi-task learning on some real datasets.

(a) letter

(b) sensorless

(c) mnist

(d) covtype

(e) mnist8m

Fig. 8. Test loss versus CPU time of the nonconvex multi-task learning on some real datasets.

(a) Objective value

(b) Test error

(a) Objective value

(b) Test error

Fig. 9. Performance of nonconvex STOC-ADMM at different ρ.

Fig. 10. Performance of nonconvex SVRG-ADMM at different ρ.

the old gradients. In the future work, we will develop a
more efﬁcient stochastic ADMM algorithm for automati-
cally adapting to the system resources, and yield the best

performance in practice. In addition, we will propose some
accelerated stochastic ADMMs for nonconvex optimization
by using the momentum techniques.

00.511.522.533.5410−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM01234567891010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101520253035404510−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0510152025303540455010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM02040608010012014016018020010−410−310−210−1CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM00.511.522.533.5400.050.10.150.20.250.30.350.40.450.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0123456789100.10.150.20.250.30.350.4CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0510152025303540450.10.1050.110.1150.120.1250.130.1350.140.145CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM051015202530354045500.050.10.150.2CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0204060801001201401601802000.20.250.30.350.40.450.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM01234567891010−410−310−210−1100CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101520253010−410−310−210−1100CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM02040608010012010−410−310−210−1100CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05010015020025030010−410−310−210−1100CPU time (second)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM02040608010012010−410−310−210−1100101CPU time (minute)Objective minus best  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0123456789100.30.40.50.60.70.80.9CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0510152025300.30.40.50.60.70.8CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0204060801001200.10.150.20.250.30.350.40.450.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0501001502002503000.250.30.350.40.450.5CPU time (second)Test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM0204060801001200.10.20.30.40.50.60.70.8CPU time (minute)test error  DETE−ADMMSAGA−ADMMSVRG−ADMMSTOC−ADMM05101520253010−410−310−210−1CPU time (second)Objective minus best  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=10005101520253000.050.10.150.20.250.30.350.40.450.5CPU time (second)Test error  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=10005101520253010−410−310−210−1CPU time (second)Objective minus best  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=10005101520253000.050.10.150.20.250.30.350.40.450.5CPU time (second)Test error  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=100IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

12

[20] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction
method of multipliers,” Foundations and Trends R(cid:13) in Machine Learn-
ing, vol. 3, no. 1, pp. 1–122, 2011.

[21] C. Lu, J. Feng, S. Yan, and Z. Lin, “A uniﬁed alternating direction
method of multipliers by majorization minimization,” IEEE trans-
actions on pattern analysis and machine intelligence, vol. 40, no. 3, pp.
527–541, 2018.

[22] H. Wang and A. Banerjee, “Online alternating direction method,”
in Proceedings of the 29th International Conference on Machine Learn-
ing, 2012, pp. 1119–1126.

[23] T. Suzuki, “Dual averaging and proximal gradient descent for on-
line alternating direction multiplier method.” in 30st International
Conference on Machine Learning, 2013, pp. 392–400.

[24] H. Ouyang, N. He, L. Tran, and A. G. Gray, “Stochastic alternating

direction method of multipliers.” ICML, vol. 28, pp. 80–88, 2013.

[25] L. W. Zhong and J. T. Y. Kwok, “Fast stochastic alternating di-
rection method of multipliers,” in 31st International Conference on
Machine Learning, Beijing, China, 2014.

[26] S. Azadi and S. Sra, “Towards an optimal stochastic alternating
direction method of multipliers,” in Proceedings of the 31st Interna-
tional Conference on Machine Learning, 2014, pp. 620–628.

[27] T. Suzuki, “Stochastic dual coordinate ascent with alternating
direction method of multipliers,” in Proceedings of The 31st Inter-
national Conference on Machine Learning, 2014, pp. 736–744.

[28] S.-Y. Zhao, W.-J. Li, and Z.-H. Zhou, “Scalable stochastic
alternating direction method of multipliers,” arXiv preprint
arXiv:1502.03529, 2015.

[29] S. Zheng and J. T. Kwok, “Fast and light stochastic admm,” in The
25th International Joint Conference on Artiﬁcial Intelligence, 2016, pp.
2407–2613.

[30] Y. Liu, F. Shang, and J. Cheng, “Accelerated variance reduced

stochastic admm.” in AAAI, 2017, pp. 2287–2293.

[31] C. Fang, F. Cheng, and Z. Lin, “Faster and non-ergodic o (1/k)
stochastic alternating direction method of multipliers,” arXiv
preprint arXiv:1704.06793, 2017.

[32] P. Zhao, J. Yang, T. Zhang, and P. Li, “Adaptive stochastic alter-
nating direction method of multipliers,” in Proceedings of The 32nd
International Conference on Machine Learning, 2015, pp. 69–77.
[33] R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. I. Jordan,
“A general analysis of the convergence of admm,” in Proceedings
of The 32nd International Conference on Machine Learning, 2015, pp.
343–352.

[34] Y. Xu, M. Liu, Q. Lin, and T. Yang, “Admm without a ﬁxed penalty
parameter: Faster convergence with new adaptive penalization,”
in Advances in Neural Information Processing Systems, 2017, pp.
1267–1277.

[35] A. Aravkin and D. Davis, “A smart stochastic algorithm for non-
convex optimization with applications to robust machine learn-
ing,” arXiv preprint arXiv:1610.01101, 2016.

[36] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.

521, no. 7553, pp. 436–444, 2015.

[37] S. Ghadimi and G. Lan, “Accelerated gradient methods for non-
convex nonlinear and stochastic programming,” Mathematical Pro-
gramming, vol. 156, no. 1-2, pp. 59–99, 2016.

[38] S. Ghadimi, G. Lan, and H. Zhang, “Mini-batch stochastic approx-
imation methods for nonconvex stochastic composite optimiza-
tion,” Mathematical Programming, vol. 155, no. 1-2, pp. 267–305,
2016.

[39] Z. Allen-Zhu and E. Hazan, “Variance reduction for faster non-
convex optimization,” in International Conference on Machine Learn-
ing, 2016, pp. 699–707.

[40] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, “Stochastic
variance reduction for nonconvex optimization,” in International
conference on machine learning, 2016, pp. 314–323.

[41] S.

J. Reddi, S. Sra, B. Poczos, and A. Smola, “Fast

incre-
mental method for nonconvex optimization,” arXiv preprint
arXiv:1603.06159, 2016.

[42] ——, “Fast stochastic methods for nonsmooth nonconvex opti-

mization,” arXiv preprint arXiv:1605.06900, 2016.

[43] Z. Allen-Zhu, “Natasha: Faster non-convex stochastic opti-
mization via strongly non-convex parameter,” arXiv preprint
arXiv:1702.00763, 2017.

[44] C. Paquette, H. Lin, D. Drusvyatskiy, J. Mairal, and Z. Harchaoui,
“Catalyst acceleration for gradient-based non-convex optimiza-
tion,” arXiv preprint arXiv:1703.10993, 2017.

(a) Objective value

(b) Test error

Fig. 11. Performance of nonconvex SAGA-ADMM at different ρ.

REFERENCES

[1] F. Huang, S. Chen, and Z. Lu, “Stochastic alternating direction
method of multipliers with variance reduction for nonconvex
optimization,” arXiv preprint arXiv:1610.02758, 2016.

[2] L. Bottou, “Stochastic learning,” in Advanced lectures on machine

learning. Springer, 2004, pp. 146–168.

[3] N. L. Roux, M. Schmidt, and F. R. Bach, “A stochastic gradient
method with an exponential convergence rate for ﬁnite training
sets,” in Advances in Neural Information Processing Systems, 2012,
pp. 2663–2671.
S. Shalev-Shwartz and T. Zhang, “Stochastic dual coordinate as-
cent methods for regularized loss minimization,” Journal of Ma-
chine Learning Research, vol. 14, no. Feb, pp. 567–599, 2013.

[4]

[5] Q. Lin, Z. Lu, and L. Xiao, “An accelerated randomized proximal
coordinate gradient method and its application to regularized em-
pirical risk minimization,” SIAM Journal on Optimization, vol. 25,
no. 4, pp. 2244–2273, 2015.

[6] Y. Nesterov, Introductory Lectures on Convex Programming Volume I:

Basic course. Kluwer, Boston, 2004.

[7] R. Johnson and T. Zhang, “Accelerating stochastic gradient de-
scent using predictive variance reduction,” in Advances in Neural
Information Processing Systems, 2013, pp. 315–323.

[8] L. Xiao and T. Zhang, “A proximal stochastic gradient method
with progressive variance reduction,” SIAM Journal on Optimiza-
tion, vol. 24, no. 4, pp. 2057–2075, 2014.

[9] A. Defazio, F. Bach, and S. Lacoste-Julien, “Saga: A fast incre-
mental gradient method with support for non-strongly convex
composite objectives,” in Advances in Neural Information Processing
Systems, 2014, pp. 1646–1654.

[10] A. Nitanda, “Stochastic proximal gradient descent with accel-
eration techniques,” in Advances in Neural Information Processing
Systems, 2014, pp. 1574–1582.

[11] Z. Allen-Zhu, “Katyusha: The ﬁrst direct acceleration of stochastic

gradient methods,” arXiv preprint arXiv:1603.05953, 2016.

[12] W. Zhang, L. Zhang, Z. Jin, R. Jin, D. Cai, X. Li, R. Liang, and
X. He, “Sparse learning with stochastic composite optimization,”
IEEE transactions on pattern analysis and machine intelligence, vol. 39,
no. 6, pp. 1223–1236, 2017.

[13] M. Hong, “A distributed, asynchronous and incremental algo-
rithm for nonconvex optimization: An admm based approach,”
arXiv preprint arXiv:1412.6058, 2014.

[14] D. Hajinezhad, M. Hong, T. Zhao, and Z. Wang, “Nestt: A noncon-
vex primal-dual splitting method for distributed and stochastic
optimization,” arXiv preprint arXiv:1605.07747, 2016.

[15] S. Zheng and J. T. Kwok, “Stochastic variance-reduced admm,”

arXiv preprint arXiv:1604.07070, 2016.

[16] S. Kim, K.-A. Sohn, and E. P. Xing, “A multivariate regression
approach to association analysis of a quantitative trait network,”
Bioinformatics, vol. 25, no. 12, pp. i204–i212, 2009.

[17] L. Yuan, J. Liu, and J. Ye, “Efﬁcient methods for overlapping
group lasso,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 9, pp. 2104–2116, 2013.

[18] R. J. Tibshirani, J. Taylor et al., “The solution path of the general-
ized lasso,” The Annals of Statistics, vol. 39, no. 3, pp. 1335–1371,
2011.

[19] D. Gabay and B. Mercier, “A dual algorithm for the solution of
nonlinear variational problems via ﬁnite element approximation,”
Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17–40,
1976.

0510152025303510−410−310−210−1CPU time (second)Objective minus best  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=1000510152025303500.050.10.150.20.250.30.350.40.450.5CPU time (second)Test error  ρ=0.001ρ=0.01ρ=0.1ρ=1ρ=10ρ=100IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

13

[45] H. Lin, J. Mairal, and Z. Harchaoui, “A universal catalyst for ﬁrst-
order optimization,” in Advances in Neural Information Processing
Systems, 2015, pp. 3384–3392.

[46] B. Jiang, T. Lin, S. Ma, and S. Zhang, “Structured nonconvex and
nonsmooth optimization: Algorithms and iteration complexity
analysis,” arXiv preprint arXiv:1605.02408, 2016.

[47] F. Wang, W. Cao, and Z. Xu, “Convergence of multi-block breg-
man admm for nonconvex composite problems,” arXiv preprint
arXiv:1505.03063, 2015.

[48] L. Yang, T. K. Pong, and X. Chen, “Alternating direction method
of multipliers for nonconvex background/foreground extraction,”
arXiv preprint arXiv:1506.07029, 2015.

[49] Y. Wang, W. Yin, and J. Zeng, “Global

convergence of
admm in nonconvex nonsmooth optimization,” arXiv preprint
arXiv:1511.06324, 2015.

[50] M. Hong, Z.-Q. Luo, and M. Razaviyayn, “Convergence analysis
of alternating direction method of multipliers for a family of
nonconvex problems,” SIAM Journal on Optimization, vol. 26, no. 1,
pp. 337–364, 2016.

[51] V. Vapnik, The nature of statistical learning theory. Springer Science

& Business Media, 2013.

[52] X. Zhang, M. Burger, and S. Osher, “A uniﬁed primal-dual algo-
rithm framework based on bregman iteration,” Journal of Scientiﬁc
Computing, vol. 46, no. 1, pp. 20–46, 2011.

[53] W. Deng and W. Yin, “On the global and linear convergence of the
generalized alternating direction method of multipliers,” Journal
of Scientiﬁc Computing, vol. 66, no. 3, pp. 889–916, 2016.

[54] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covari-
ance estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3,
pp. 432–441, 2008.

[55] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. D. Ravikumar,
“Quic: quadratic approximation for sparse inverse covariance
estimation.” Journal of Machine Learning Research, vol. 15, no. 1,
pp. 2911–2947, 2014.

[56] E. J. Candes, M. B. Wakin, and S. P. Boyd, “Enhancing sparsity
by reweighted (cid:96)1 minimization,” Journal of Fourier analysis and
applications, vol. 14, no. 5-6, pp. 877–905, 2008.

[57] Q. Yao and J. T. Kwok, “Efﬁcient learning with a family of noncon-
vex regularizers by redistributing nonconvexity,” in Proceedings
of the 33rd International Conference on International Conference on
Machine Learning-Volume 48.

JMLR. org, 2016, pp. 2645–2654.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

14

APPENDIX A
CONVERGENCE ANALYSIS OF NONCONVEX MINI-BATCH STOC-ADMM

A.1 Proof of Lemma 1

Proof. By the optimal condition of step 6 in Algorithm 1, we have

0 = G(xt, ξIt) − AT λt + ρAT (Axt+1 + Byt+1 − c) −

H
η

(xt − xt+1)

= G(xt, ξIt) − AT λt+1 −

H
η

(xt − xt+1),

where the second equality is due to step 7 in Algorithm 1. Thus, we have

It follows that

AT λt+1 = G(xt, ξIt) −

H
η

(xt − xt+1).

λt+1 = (AT )+(cid:0)G(xt, ξIt) +

H
η

(xt+1 − xt)(cid:1),

(38)

(39)

where (AT )+ is the pseudoinverse of AT . By Assumption 5, without loss of generality, we use the full column matrix A.
Then we have (AT )+ = A(AT A)−1. Using the equality (39), we have

(cid:107)λt+1 − λt(cid:107)2 = (cid:107)(AT )+(cid:0)G(xt, ξIt) +

H
η

(xt+1 − xt) − G(xt−1, ξIt−1 ) −

(xt − xt−1)(cid:1)(cid:107)2

H
η
H
η

≤ (φA

(xt+1 − xt) −

min)−1(cid:107)G(xt, ξIt) − G(xt−1, ξIt−1) +

H
η
= (φA
min)−1(cid:107)G(xt, ξIt) − ∇f (xt) + ∇f (xt) − ∇f (xt−1) + ∇f (xt−1) − G(xt−1, ξIt−1)
H
η
5
φA

(cid:107)G(xt−1, ξIt−1) − ∇f (xt−1)(cid:107)2 +

(cid:107)G(xt, ξIt) − ∇f (xt)(cid:107)2 +

(xt − xt−1)(cid:107)2

(xt − xt−1)(cid:107)2

(xt+1 − xt) −

5
φA

max)2
5(φH
φA
minη2

H
η

min

+

(i)
≤

(cid:107)xt − xt+1(cid:107)2

(cid:107)xt−1 − xt(cid:107)2,

(40)

min
5((φH

+

max)2 + η2L2)
φA
minη2

where the inequality (i) holds by Assumption 2.

Taking expectation conditioned on information It to (40), we have

E(cid:107)λt+1 − λt(cid:107)2 ≤

E(cid:107)G(xt, ξIt) − ∇f (xt)(cid:107)2 +

E(cid:107)G(xt−1, ξIt−1) − ∇f (xt−1)(cid:107)2

5
φA

min
5η2(φH
φA

min

max)2

+

5
φA
min
5(η2(φH

max)2 + L2)
φA
min
5((φH

(cid:107)xt − xt+1(cid:107)2 +

(cid:107)xt−1 − xt(cid:107)2

(i)
≤

10σ2
M φA

min

+

max)2
5(φH
φA
minη2

E(cid:107)xt+1 − xt(cid:107)2 +

max)2 + η2L2)
φA
minη2

(cid:107)xt − xt−1(cid:107)2,

= ζ(cid:107)xt − xt−1(cid:107)2 + ζ1E(cid:107)xt+1 − xt(cid:107)2 +

10σ2
M φA

min

where the inequality (i) holds by Assumption 1.

A.2 Proof of Lemma 2

Proof. By the step 5 of Algorithm 1, we have

Lρ(xt, yt+1, λt) ≤ Lρ(xt, yt, λt).

(41)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

15

By the optimal condition of step 6 in Algorithm 1, we have

0 = (xt − xt+1)T (cid:2)G(xt, ξIt) − AT λt −

H
η

(xt − xt+1) + ρAT (Axt+1 + Byt+1 − c)(cid:3)

= (xt − xt+1)T (cid:2)G(xt, ξIt ) − ∇f (xt) + ∇f (xt) − AT λt −

H
η

(xt − xt+1) + ρAT (Axt+1 + Byt+1 − c)(cid:3)

(i)
≤ f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0)G(xt, ξIt) − ∇f (xt)(cid:1) −

1
η

(cid:107)xt+1 − xt(cid:107)2
H

− λT

t (Axt − Axt+1) + ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c)

(ii)
= f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0)G(xt, ξIt) − ∇f (xt)(cid:1) −

1
η

(cid:107)xt+1 − xt(cid:107)2
H

− λT
t (Axt + Byt+1 − c) + λT
ρ
2

(cid:107)Axt+1 + Byt+1 − c(cid:107)2 −

−

t (Axt+1 + Byt+1 − c) +

ρ
2

(cid:107)Axt − Axt+1(cid:107)2

ρ
2

(cid:107)Axt + Byt+1 − c(cid:107)2

= Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) + (xt − xt+1)T (cid:0)G(xt, ξIt) − ∇f (xt)(cid:1) +

−

1
η

(cid:107)xt+1 − xt(cid:107)2

H −

ρ
2

(cid:107)Axt − Axt+1(cid:107)2

(iii)
≤ Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) +

1
2

(cid:107)G(xt, ξIt) − ∇f (xt)(cid:107)2

− (

φH
min
η

+

ρφA
min
2

−

L + 1
2

)(cid:107)xt − xt+1(cid:107)2,

L
2

(cid:107)xt+1 − xt(cid:107)2

(42)

where the inequality (i) holds by (15); the equality (ii) holds by using the equality (a − b)T (b − c) = 1
2 ((cid:107)a − c(cid:107)2 − (cid:107)a − b(cid:107)2 −
(cid:107)b − c(cid:107)2) on the term ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c); the inequality (iii) holds by the Cauchy inequality. Taking
expectation conditioned on information It to (42), we have

E[Lρ(xt+1, yt+1, λt)] ≤ Lρ(xt, yt+1, λt) − (

φH
min
η

+

ρφA
min
2

−

L + 1
2

)E(cid:107)xt+1 − xt(cid:107)2 +

σ2
2M

.

By the step 7 of Algorithm 1, we have

E[Lρ(xt+1, yt+1, λt+1) − Lρ(xt+1, yt+1, λt)] =

≤

E(cid:107)λt − λt+1(cid:107)2

1
ρ
5(L2η2 + (φH
φA
minη2ρ
10σ2
M φA

minρ

,

+

max)2)

(cid:107)xt − xt−1(cid:107)2 +

max)2
5(φH
φA
minη2ρ

E(cid:107)xt+1 − xt(cid:107)2

where the inequality (i) holds by the Lemma 1. Combining (41), (43) and (44), we have

E[Lρ(xt+1, yt+1, λt+1)] ≤Lρ(xt, yt, λt) +

max)2)

(cid:107)xt − xt−1(cid:107)2

5(L2η2 + (φH
φA
minη2ρ
L + 1
2

−

−

− (

φH
min
η

+

ρφA
min
2

max)2
5(φH
φA
minη2ρ

)E(cid:107)xt − xt+1(cid:107)2 +

(φA

minρ + 20)σ2
2φA
minρM

.

Next, we deﬁne a useful sequence

(cid:8)Ψt

(cid:9)T
t=1 as follows:

Ψt = E(cid:2)Lρ(xt, yt, λt) +

5(L2η2 + (φH
ρφA
minη2

max)2)

(cid:107)xt − xt−1(cid:107)2(cid:3).

Then we have

Ψt+1 − Ψt ≤ − (cid:0) φH
min
η

+

ρφA
min
2

−

L + 1
2

−

(cid:124)

(cid:123)(cid:122)
γ

5(L2η2 + 2(φH
φA
minη2ρ

max)2)

(cid:1)

(cid:125)

E(cid:107)xt − xt+1(cid:107)2 +

(φA

minρ + 20)σ2
2φA
minρM

,

= −γE(cid:107)xt − xt+1(cid:107)2 +

(φA

minρ + 20)σ2
2φA
minρM

.

Finally, using (18) and the properties of quadratic equation in one unknown, we have γ > 0.

(43)

(44)

(45)

(46)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

Since A is a full column rank matrix, we have (AT )+ = A(AT A)−1. It follows that σmax

σmax((AT A)−1) = 1
φA

min

. using (39), we have

16
(cid:0)((AT )+)T (AT )+(cid:1) =

Lρ(xt+1, yt+1, λt+1) = f (xt+1) + g(yt+1) − λT

t+1(Axt+1 + Byt+1 − c) +

(cid:107)Axt+1 + Byt+1 − c(cid:107)2

ρ
2

= f (xt+1) + g(yt+1) − (cid:104)(AT )+(cid:0)G(xt, ξIt) +

H
η

(xt+1 − xt)(cid:1), Axt+1 + Byt+1 − c(cid:105) +

ρ
2
(xt+1 − xt)(cid:1), Axt+1 + Byt+1 − c(cid:105)

(cid:107)Axt+1 + Byt+1 − c(cid:107)2

= f (xt+1) + g(yt+1) − (cid:104)(AT )+(cid:0)G(xt, ξIt) − ∇f (xt) + ∇f (xt) +

+

ρ
2

(cid:107)Axt+1 + Byt+1 − c(cid:107)2

H
η

≥ f (xt+1) + g(yt+1) −

(cid:107)G(xt, ξIt) − ∇f (xt)(cid:107)2 −

+

ρ
8

2
φA
minρ
(cid:107)Axt+1 + Byt+1 − c(cid:107)2
2σ2
M φA
minρ
2δ2
φA
minρ

2σ2
M φA

minρ

−

≥ f (xt+1) + g(yt+1) −

≥ f ∗ + g∗ −

−

−

2δ2
φA
minρ
max)2
2(φH
φA
minη2ρ

−

max)2
2(φH
φA
minη2ρ

(cid:107)xt+1 − xt(cid:107)2

2
φA
minρ

(cid:107)∇f (xt)(cid:107)2 −

max)2
2(φH
φA
minη2ρ

(cid:107)xt+1 − xt(cid:107)2

(cid:107)xt+1 − xt(cid:107)2,

(47)

where the ﬁrst inequality is obtained by applying (cid:104)a, b(cid:105) ≤ 1
∇f (xt)), Axt+1 + Byt+1 − c(cid:105), (cid:104)(AT )+G(xt, ξIt), Axt+1 + Byt+1 − c(cid:105) and (cid:104)(AT )+ H
with β = ρ
holds by Assumption 4. Using the deﬁnition of Ψt, we have

2 (cid:107)b(cid:107)2 to the terms (cid:104)(AT )+(G(xt, ξIt) −
η (xt+1 − xt), Axt+1 + Byt+1 − c(cid:105)
4 , respectively, and the second inequality follows by the inequality and Assumption 3, and the third inequality

2β (cid:107)a(cid:107)2 + β

Ψt+1 ≥ f ∗ + g∗ −

2σ2
M φA

minρ

−

2δ2
φA
minρ

, ∀ t = 0, 1, 2, · · · .

It follows that the function Ψt is bounded from below. Let Ψ∗ denotes a lower bound of sequence {Ψt}T

t=1.

Telescoping inequality (46) over t from 0 to T , we have

1
T

T −1
(cid:88)

t=0

E(cid:107)xt − xt+1(cid:107)2 ≤

Ψ0 − Ψ∗
γT

+

(φA

minρ + 20)σ2
2γφA
minρM

.

A.3 Proof of Theorem 1
Proof. First, we deﬁne a useful variable θt = E(cid:2)(cid:107)xt − xt+1(cid:107)2 + (cid:107)xt−1 − xt(cid:107)2(cid:3)
(φA

t∗ = arg min
2≤t≤T +1

E[θt] ≤

(Ψ1 − Ψ∗) +

2
γT

minρ + 20)σ2
2γφA
minρM

.

. By (49), then we have

By (39), we have

E(cid:107)AT λt+1 − ∇f (xt+1)(cid:107)2 = E(cid:107)G(xt, ξIt) − ∇f (xt+1) −

H
η

(xt − xt+1)(cid:107)2

= E(cid:107)G(xt, ξIt) − ∇f (xt) + ∇f (xt) − ∇f (xt+1) −

H
η

(xt − xt+1)(cid:107)2

≤ 3(cid:0)L2 +

≤ 3(cid:0)L2 +

max)2
(φH
η2
(φH
max)2
η2

(cid:1)(cid:107)xt − xt+1(cid:107)2 +

3σ2
M

(cid:1)θt +

3σ2
M

.

By the step 7 of Algorithm 1, we have

E(cid:107)Axt+1 + Byt+1 − c(cid:107)2 =

≤

≤

=

E(cid:107)λt+1 − λt(cid:107)2

1
ρ2
5(L2η2 + (φH
φA
minη2ρ2
5(L2η2 + (φH
φA
minρ2η2
10σ2
φA
minρ2M

ζ
ρ2 θt +

max)2)

max)2)

.

5(φH
max)2
φA
minη2ρ2

E(cid:107)xt+1 − xt(cid:107)2 +

10σ2

M φA

minρ2

,

(cid:107)xt − xt−1(cid:107)2 +

θt +

10σ2
φA
minρ2M

(48)

(49)

(50)

(51)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

By the step 5 of Algorithm 1, there exists a subgradient µ ∈ ∂g(yt+1) such that

E(cid:2)

dist(BT λt+1, ∂g(yt+1))2(cid:3) ≤ (cid:107)µ − BT λt+1(cid:107)2

= (cid:107)BT λt − ρBT (Axt + Byt+1 − c) − BT λt+1(cid:107)2
= (cid:107)ρBT A(xt+1 − xt)(cid:107)2
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2(cid:107)xt+1 − xt(cid:107)2
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2θt.

17

(52)

Finally, using the above bounds (50), (51) and (52), and the deﬁnition 1, an (cid:15)-stationary point of the problem (3) holds in
expectation.

APPENDIX B
CONVERGENCE ANALYSIS OF NONCONVEX MINI-BATCH SVRG-ADMM

B.1 Proof Lemma 4

Proof. Since ˆ∇f (xs+1

t

) = 1
M

(cid:80)

it∈It

(cid:0)∇fit(xs+1

t

) − ∇fit(˜xs)(cid:1) + ∇f (˜xs), we have

E(cid:107) ˆ∇f (xs+1

t

) − ∇f (xs+1
(cid:0)∇fit(xs+1

)(cid:107)2
) − ∇fit(˜xs)(cid:1) + ∇f (˜xs) − ∇f (xs+1

t

t

t

(cid:88)

)(cid:107)2

it∈It
(cid:88)

(cid:0)∇fit(xs+1

t

) − ∇fit(˜xs)(cid:1)(cid:107)2 − (cid:107)∇f (xs+1

t

) − ∇f (˜xs)(cid:107)2

= E(cid:107)

1
M

(i)
= E(cid:107)

1
M

≤

=

1
M 2

1
M 2

it∈It
(cid:88)

E(cid:107)∇fit(xs+1

t

) − ∇fit(˜xs)(cid:107)2

it∈It

(cid:88)

it∈It

1
n

n
(cid:88)

i=1

(cid:107)∇fit (xs+1

t

) − ∇fit(˜xs)(cid:107)2

(ii)
≤

L2
M

(cid:107)xs+1

t − ˜xs(cid:107)2.

where the equality (i) holds by the equality E(ξ − Eξ)2 = Eξ2 − (Eξ)2 for random variable ξ; the inequality (ii) holds by
(14).

B.2 Proof of Lemma 5

Proof. For simplicity, let xs+1
we have

t = xt, ys+1

t = yt, λs+1

t = λt, and ˜x = ˜xs. By the optimal condition of step 10 in Algorithm 2,

0 = ˆ∇f (xt) − AT λt + ρAT (Axt+1 + Byt+1 − c) −

H
η

(xt − xt+1)

= ˆ∇f (xt) − AT λt+1 −

H
η

(xt − xt+1),

where the second equality is due to step 11 in Algorithm 2. Then we have

AT λt+1 = ˆ∇f (xt) −

H
η

(xt − xt+1).

It follows that

λt+1 = (AT )+(cid:0) ˆ∇f (xt) +

H
η

(xt+1 − xt)(cid:1),

(53)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

18

where (AT )+ is the pseudoinverse of AT . By Assumption 5, without loss of generality, we use the full column matrix A.
So we have (AT )+ = A(AT A)−1. By (53), we have

(cid:107)λt+1 − λt(cid:107)2 = (cid:107)(AT )+(cid:0) ˆ∇f (xt) +

H
η

(xt+1 − xt) − ˆ∇f (xt−1) −

(xt − xt−1)(cid:1)(cid:107)2

H
η
H
η

H
η

≤ (φA

(xt+1 − xt) −

min)−1(cid:107) ˆ∇f (xt) − ˆ∇f (xt−1) +
min)−1(cid:107) ˆ∇f (xt) − ∇f (xt) + ∇f (xt) − ∇f (xt−1) + ∇f (xt−1) − ˆ∇f (xt−1)
= (φA
H
η
5
φA

(cid:107) ˆ∇f (xt−1) − ∇f (xt−1)(cid:107)2 +

(cid:107) ˆ∇f (xt) − ∇f (xt)(cid:107)2 +

(xt − xt−1)(cid:107)2

(xt − xt−1)(cid:107)2

(xt+1 − xt) −

max)2
5(φH
φA
minη2

5
φA

H
η

min

+

(i)
≤

(cid:107)xt − xt+1(cid:107)2

min
5(L2η2 + (φH
φA
minη2

+

max)2)

(cid:107)xt−1 − xt(cid:107)2,

(54)

where the inequality (i) holds by Assumption 2.

Taking expectation conditioned on information It to (54), we have

E(cid:107) ˆ∇f (xt) − ∇f (xt)(cid:107)2 +

5
φA

min

E(cid:107) ˆ∇f (xt−1) − ∇f (xt−1)(cid:107)2 +

5φ2
max
φA
minη2

(cid:107)xt − xt+1(cid:107)2

E(cid:107)λt+1 − λt(cid:107)2 ≤

(i)
≤

5
φA

min
5(L2η2 + φ2
φA
minη2

+

5L2
φA
minM
5(L2η2 + φ2
φA
minη2

+

max)

(cid:107)xt−1 − xt(cid:107)2

max)

(cid:107)xt−1 − xt(cid:107)2,

E(cid:107)xt − ˜x(cid:107)2 +

5L2
φA
minM

(cid:107)xt−1 − ˜x(cid:107)2 +

5φ2
max
φA
minη2

E(cid:107)xt − xt+1(cid:107)2

where the inequality (i) holds by Lemma 3.

B.3 Proof of Lemma 6
t )m
Proof. This proof includes two parts: First, we will prove the sequence {(Φs
m ≥ Φs+1
{1, 2, · · · , m} in each epoch s ∈ {1, 2, · · · , S}. Second, we will prove Φs
1
For simplicity, we omit the label of each epoch in the ﬁrst part, i.e., let xs+1

t=1}S

s=1 monotonically decreases over t ∈

for s ∈ {1, 2, · · · , S}.
t = yt, λs+1

t = xt, ys+1

t = λt and ˜xs = ˜x. By

the step 8 of Algorithm 2, we have

Lρ(xt, yt+1, λt) ≤ Lρ(xt, yt, λt).

(55)

By the optimal condition of step 10 in Algorithm 2, we have

0 = (xt − xt+1)T (cid:2) ˆ∇f (xt) − AT λt + ρ(Axt+1 + Byt+1 − c) −

H
η

(xt − xt+1)(cid:3)

= (xt − xt+1)T (cid:2) ˆ∇f (xt) − ∇f (xt) + ∇f (xt) − AT λt + ρAT (Axt+1 + Byt+1 − c) −

H
η

(xt − xt+1)(cid:3)

(i)
≤ f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1) −

1
η

(cid:107)xt+1 − xt(cid:107)2
H

− λT

t (Axt − Axt+1) + ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c)

(ii)
= f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1) −

(cid:107)xt+1 − xt(cid:107)2
H

− λT
t (Axt + Byt+1 − c) + λT
ρ
2

(cid:107)Axt+1 + Byt+1 − c(cid:107)2 −

−

ρ
2

t (Axt+1 + Byt+1 − c) +

(cid:107)Axt − Axt+1(cid:107)2

= Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1)

1
η
(cid:107)Axt + Byt+1 − c(cid:107)2

ρ
2

+

L
2

(cid:107)xt+1 − xt(cid:107)2 −

1
η

(cid:107)xt+1 − xt(cid:107)2

H −

ρ
2

(cid:107)Axt − Axt+1(cid:107)2

(iii)
≤ Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) +

1
2

(cid:107) ˆ∇f (xt) − ∇f (xt)(cid:107)2

− (

φH
min
η

+

φA
minρ
2

−

L + 1
2

)(cid:107)xt − xt+1(cid:107)2,

(56)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

19

where the inequality (i) holds by (15); the equality (ii) holds by applying the equality (a − b)T (b − c) = 1
2 ((cid:107)a − c(cid:107)2 − (cid:107)a −
b(cid:107)2 − (cid:107)b − c(cid:107)2) on the term ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c); the inequality (iii) holds by the Cauchy inequality.
Taking expectation conditioned on information It to (56), we have

E[Lρ(xt+1, yt+1, λt)] ≤ Lρ(xt, yt+1, λt) − (

φH
min
η

+

φA
minρ
2

−

L + 1
2

)(cid:107)xt − xt+1(cid:107)2 +

L2
2M

(cid:107)xt − ˜x(cid:107)2.

(57)

By the step 11 of Algorithm 2, we have

E[Lρ(xt+1, yt+1, λt+1) − Lρ(xt+1, yt+1, λt)] =

+

where the inequality (i) holds by Lemma 4.
Combining (55), (57) and (58), we have

(i)
≤

ρφA

E(cid:107)λt+1 − λt(cid:107)2

1
ρ
5L2
minM
5(L2η2 + (φH
ρφA
minη2

(cid:107)xt − ˜x(cid:107)2 +

max)2)

5L2
minM

ρφA

(cid:107)xt−1 − ˜x(cid:107)2 +

5(φH
ρφA

max)2
minη2

(cid:107)xt+1 − xt(cid:107)2

(cid:107)xt − xt−1(cid:107)2,

(58)

E[Lρ(xt+1, yt+1, λt+1)] ≤Lρ(xt, yt, λt) +

(10 + φA
2ρφA

minρ)L2
minM

(cid:107)xt − ˜x(cid:107)2 +

5L2
minM

ρφA

(cid:107)xt−1 − ˜x(cid:107)2

max)2)

+

− (

5(L2η2 + (φH
φA
minη2ρ
φA
minρ
2

φH
min
η

+

(cid:107)xt − xt−1(cid:107)2

−

L + 1
2

−

max)2
5(φH
φA
minη2ρ

)(cid:107)xt+1 − xt(cid:107)2.

Next, considering E(cid:107)xt+1 − ˜x(cid:107)2, we have

E(cid:107)xt+1 − ˜x(cid:107)2 = E(cid:107)xt+1 − xt + xt − ˜x(cid:107)2

= E[(cid:107)xt+1 − xt(cid:107)2 + 2(xt+1 − xt)T (xt − ˜x) + (cid:107)xt − ˜x(cid:107)2]
(i)
≤ E[(cid:107)xt+1 − xt(cid:107)2 + 2(

(cid:107)xt+1 − xt(cid:107)2 +

(cid:107)xt − ˜x(cid:107)2) + (cid:107)xt − ˜x(cid:107)2]

1
2β

β
2

= (1 +

1
β

)(cid:107)xt+1 − xt(cid:107)2 + (1 + β)(cid:107)xt − ˜x(cid:107)2,

where the inequality (i) is due to the Cauchy-Schwarz inequality, and β > 0. Combining (59) and (60), then, we have

(59)

(60)

E(cid:2)Lρ(xt+1, yt+1, λt+1) +

(cid:107)xt+1 − xt(cid:107)2 + hs

t+1((cid:107)xt+1 − ˜x(cid:107)2 + (cid:107)xt − ˜x(cid:107)2)(cid:3)

max)2(cid:1)

5(cid:0)L2η2 + (φH
φA
minη2ρ
5(cid:0)L2η2 + (φH
max)2(cid:1)
φA
minη2ρ
L + 1
2
(cid:1)(cid:107)xt−1 − ˜x(cid:107)2,

−

−

L2
2M

≤ Lρ(xt, yt, λt) +

− (cid:2) φH
min
η

+

φA
minρ
2

− (cid:0)(2 + β)hs

t+1 +

(cid:107)xt − xt−1(cid:107)2 + (cid:0)(2 + β)hs

t+1 +

(cid:1)((cid:107)xt − ˜x(cid:107)2 + (cid:107)xt−1 − ˜x(cid:107)2)

5(cid:0)L2η2 + 2(φH
φA
minη2ρ

max)2(cid:1)

− (1 +

1
β

)ht+1

minρ)L2

(10 + φA
2φA

minM ρ
(cid:3)E(cid:107)xt+1 − xt(cid:107)2

where hs

t+1 > 0. By the deﬁnition of the sequence {(Φs

t )m

t=1}S

s=1 22, we have

Φs

t+1 ≤ Φs

t − Γs
t

E(cid:107)xs

t+1 − xs

t (cid:107)2 − (cid:0)(2 + β)hs

t+1 +

L2
2M

(cid:1)(cid:107)xs

t−1 − ˜xs−1(cid:107)2.

(61)

Then using (24) and the properties of quadratic equation in one unknown, we have Γs
prove the ﬁrst part.

t > 0, ∀t ∈ {1, 2, · · · , m}. Thus, we

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

20

1

m − λs+1
E(cid:107)AT λs

Next, we will prove the second part. Since λs+1
0 = λs
(cid:107)2

(cid:107)2 = E(cid:107)λs

E(cid:107)λs+1

0 − λs+1

1

m and xs+1

0 = xs

m = ˜xs, we have

0

1

≤

(cid:107)2

(i)
=

) −

(ii)
=

H
η

E(cid:107) ˆ∇f (xs

m − AT λs+1

m−1) − ˆ∇f (xs+1

1
φA
min
1
φA
min
1
φA
min
H
η
5L2
φA
minM
max)2
5(φH
φA
minη2
where the equality (i) holds by (53), and the equality (ii) holds by the following result:

0 − xs+1
5(L2η2 + (φH
φA
minη2

m−1 − ˜xs−1(cid:107)2 +

m−1) − ∇f (xs

m−1) + ∇f (xs

0 − xs+1

m−1 − xs

m−1 − xs

E(cid:107) ˆ∇f (xs

max)2)

(cid:107)xs+1

(xs+1

m) −

H
η

(cid:107)xs

(cid:107)xs

(xs

(xs

)(cid:107)2

(cid:107)2,

+

≤

−

1

1

m) −

m−1) − ∇f (xs

m)

m−1 − xs

m(cid:107)2

H
η

(xs+1

0 − xs+1

1

)(cid:107)2

ˆ∇f (xs+1

0

) =

=

1
M

1
M

(cid:88)

it∈It
(cid:88)

(cid:0)∇fit(xs+1

0

) − ∇fit(˜xs)(cid:1) + ∇f (˜xs)

(cid:0)∇fit(xs

m) − ∇fit(xs

m)(cid:1) + ∇f (xs
m)

= ∇f (xs

it∈It
m).

By (55), we have

By (57), we have

Lρ(xs+1
0

, ys+1
1

, λs+1
0

) ≤ Lρ(xs+1

0

, ys+1
0

, λs+1
0

) = Lρ(xs

m, ys

m, λs

m).

E[Lρ(xs+1

1

, ys+1
1

, λs+1
0

0

)] ≤ Lρ(xs+1
− (cid:0) φH
min
η

+

, ys+1
1
φA
minρ
2

, λs+1
0

)
L + 1
2

−

(cid:1)E(cid:107)xs+1

1 − xs+1

0

(cid:107)2.

By (62), we have

E[Lρ(xs+1

1

, ys+1
1

, λs+1
1

) − Lρ(xs+1

1

, ys+1
1

, λs+1
0

)] =

1
ρ

E(cid:107)λs+1

0 − λs+1

1

(cid:107)

(cid:107)xs

m−1 − ˜xs−1(cid:107)2 +

5(L2η2 + (φH
φA
minη2ρ

max)2)

(cid:107)xs

m−1 − xs

m(cid:107)2

≤

5L2
φA
minρM
max)2
5(φH
φA
minη2ρ

+

(cid:107)xs+1

0 − xs+1

1

(cid:107)2.

Combining (63), (64) and (65), we have

E[Lρ(xs+1

1

, ys+1
1

, λs+1
1

)] ≤

5L2
φA
minρM
− (cid:0) φH
min
η

(cid:107)xs

m−1 − ˜xs−1(cid:107)2 +

+

φA
minρ
2

−

L + 1
2

−

5(L2η2 + (φH
φA
minη2ρ
max)2
5(φH
φA
minη2ρ

max)2)

(cid:107)xs

m−1 − xs

m(cid:107)2

(cid:1)E(cid:107)xs+1

1 − xs+1

0

(cid:107)2.

(62)

(63)

(64)

(65)

Using hs+1

1 = 10L2

minρM , then, we have
φA

E(cid:2)Lρ(xs+1

1

, ys+1
1

, λs+1
1

) + hs+1

1

(cid:0)(cid:107)xs+1

1 − ˜xs(cid:107)2 + (cid:107)xs+1

0 − ˜xs(cid:107)2(cid:1) +

≤ Lρ(xs

m, ys

m, λs

m) +

10L2
φA
minρM

(cid:0)(cid:107)xs

m − ˜xs−1(cid:107)2 + (cid:107)xs

m−1 − ˜xs−1(cid:107)2(cid:1) +

(cid:107)xs

m−1 − xs

m(cid:107)2

+

− (cid:0) φH
min
η
10L2
φA
minρM

−

φA
minρ
2

− hs+1

1 −

L + 1
2

−

5(L2η2 + 2(φH
φA
minη2ρ

max)2)

(cid:107)xs

m − ˜xs−1(cid:107)2.

(cid:1)E(cid:107)xs+1

0 − xs+1

1

(cid:107)2 −

5L2
φA
minρM

(cid:107)xs

m−1 − ˜xs−1(cid:107)2

(cid:107)xs+1

1 − xs+1

0

(cid:107)2(cid:3)

max)2)

5(L2η2 + (φH
φA
minη2ρ
5(L2η2 + (φH
φA
minη2ρ

max)2)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

By the deﬁnition of the sequence

(cid:8)(Φs

t )m
t=1

(cid:9)S
s=1, we have

Φs+1

1 ≤ Φs

m − Γs
m

E(cid:107)xs+1

0 − xs+1

1

(cid:107)2 −

5L2
φA
minρM

(cid:107)xs

m−1 − ˜xs−1(cid:107)2.

21

(66)

Then using (24) and the properties of quadratic equation in one unknown, we have Γs
the sequence {(Φs

s=1 monotonically decreases.

t=1}S

t )m

m > 0, ∀s ≥ 1. Finally, we prove that

B.4 Proof of Theorem 2
Proof. By (61) and (66), we have, for s ∈ {1, 2, · · · , S} and t ∈ {1, 2, · · · , m},

Φs

t+1 ≤ Φs

t − Γs
t

and

E(cid:107)xs

t+1 − xs

t (cid:107)2 − (cid:0)(2 + β)hs

t+1 +

L2
2M

(cid:1)(cid:107)xs

t−1 − ˜xs−1(cid:107)2,

5L2
φA
minρM
Summing (67) and (68) over s ∈ {1, 2, · · · , S} and t ∈ {1, 2, · · · , m}, we have

0 − xs+1

1 ≤ Φs

m − Γs
m

E(cid:107)xs+1

Φs+1

(cid:107)2 −

1

(cid:107)xs

m − ˜xs−1(cid:107)2.

ΦS

m − Φ1

1 ≤ −γ

S
(cid:88)

m
(cid:88)

E(cid:107)xs

t − xs

t−1(cid:107)2 − ω

S
(cid:88)

m
(cid:88)

s=1

t=1

(cid:107)xs

t−1 − ˜xs−1(cid:107)2

where γ = min(s,t) Γs
Φs

t ≥ Φ∗. By (26) and (69), then, we have

t=1
t , and ω = min(s,t){(2 + β)hs

s=1

t+1 + L2
2M ,

5L2
minρM }. By Lemma 6, there exists a constant Φ∗ such that

φA

(s∗, t∗) = arg min

1≤s≤S, 1≤t≤m

ˆθs
t ≤

2
τ T

(Φ1

1 − Φ∗),

where τ = min(γ, ω), and T = mS.

By (53), we have

E(cid:107)AT λs

t+1 − ∇f (xs

t+1)(cid:107)2

= E(cid:107) ˆ∇f (xs

t ) − ∇f (xs

t+1) −

H
η

(xs

t − xs

t+1)(cid:107)2

= E(cid:107) ˆ∇f (xs

t ) − ∇f (xs

t ) + ∇f (xs

t ) − ∇f (xs

t+1) −

H
η

(xs

t − xs

t+1)(cid:107)2

(cid:107)xs

≤

3L2
M
≤ 3(cid:0)L2 +

t − ˜xs−1(cid:107)2 + 3(cid:0)L2 +
max)2
(φH
η2

(cid:1)ˆθs
t .

max)2
(φH
η2

By Lemma 4, we have

(cid:1)(cid:107)xs

t − xs

t+1(cid:107)2

E(cid:107)Axs

t+1 + Bys

t+1 − c(cid:107)2 =

≤

+

≤

t+1 − λs

t (cid:107)2

1
ρ2 (cid:107)λs
5L2
φA
minρ2M
5(φH
max)2
φA
minρ2η2
5(L2η2 + φ2
φA
minρ2η2

E(cid:107)xs

t+1 − xs

t (cid:107)2 +

max)

ˆθs
t =

ζ
ρ2

ˆθs
t .

E(cid:107)xs

t − ˜xs−1(cid:107)2 +

t−1 − ˜xs−1(cid:107)2

(cid:107)xs

5L2
φA
minρ2M
5(L2η2 + (φH
φA
minρ2η2

max)2)

(cid:107)xs

t − xs

t−1(cid:107)2

By the step 8 of Algorithm 2, there exists a sub-gradient µ ∈ ∂g(ys

t+1) such that

E(cid:2)

dist(BT λs

t+1, ∂g(ys

t+1(cid:107)2
t − ρBT (Axs
t+1 − xs

t+1))2(cid:3) ≤ (cid:107)µ − BT λs
= (cid:107)BT λs
t + Bys
= (cid:107)ρBT A(xs
t )(cid:107)2
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2(cid:107)xs
t+1 − xs
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2 ˆθs
t .

t (cid:107)2

t+1 − c) − BT λs

t+1(cid:107)2

(67)

(68)

(69)

(70)

(71)

(72)

(73)

Finally, using the above bounds (71), (72) and (73), and the deﬁnition 1, an (cid:15)-stationary point of the problem (3) holds in
expectation.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

22

APPENDIX C
CONVERGENCE ANALYSIS OF NONCONVEX MINI-BATCH SAGA-ADMM

C.1 Proof of Lemma 8

Proof. Since ψt = 1
n

(cid:80)n

j=1 ∇fj(zt

j), we have

E(cid:107)∆t(cid:107)2 = E(cid:107)

1
M

(cid:88)

it∈It
(cid:88)

(cid:0)∇fit(xt) − ∇fit(zt
it

)(cid:1) + ψt − ∇f (xt)(cid:107)2

(cid:0)∇fit(xt) − ∇fit (zt
it

)(cid:1)(cid:107)2 − (cid:107)∇f (xt) − ψt(cid:107)2

(i)
= E(cid:107)

1
M

≤

=

1
M 2

1
M 2

(ii)
≤

L2
nM

it∈It

(cid:88)

it∈It
n
(cid:88)

i=1

it∈It
(cid:88)

E(cid:107)∇fit (xt) − ∇fit(zt
it

)(cid:107)2

1
n

n
(cid:88)

i=1

(cid:107)∇fit(xt) − ∇fit(zt

i )(cid:107)2

(cid:107)xt − zt

i (cid:107)2.

where the equality (i) holds by the equality E(ξ − Eξ)2 = Eξ2 − (Eξ)2 for random variable ξ, and E[∇fit (zt
it
1
n

j) = ψt; the inequality (ii) holds by (14).

j=1 ∇fj(zt

(cid:80)n

C.2 Proof of Lemma 10

Proof. By the step 5 of Algorithm 3, we have

By the optimal condition of step 7 in Algorithm 3, we have

Lρ(xt, yt+1, λt) ≤ Lρ(xt, yt, λt).

0 = (xt − xt+1)T (cid:2) ˆ∇f (xt) + ρAT (Axt+1 + Byt+1 − c) − AT λt −

H
η

(xt − xt+1)(cid:3)

= (xt − xt+1)T (cid:2) ˆ∇f (xt) − ∇f (xt) + ∇f (xt) − AT λt −

H
η

(xt − xt+1) + ρAT (Axt+1 + Byt+1 − c)(cid:3)

(i)
≤ f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1) −

1
η

(cid:107)xt+1 − xt(cid:107)2
H

− λT

t (Axt+1 − Axt) + ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c)

(ii)
= f (xt) − f (xt+1) +

L
2

(cid:107)xt+1 − xt(cid:107)2 + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1) −

(cid:107)xt+1 − xt(cid:107)2
H

1
η
(cid:107)Axt + Byt+1 − c(cid:107)2

ρ
2

t (Axt + Byt+1 − c) + λT
− λT
ρ
2

(cid:107)Axt+1 + Byt+1 − c(cid:107)2 −

−

ρ
2

t (Axt+1 + Byt+1 − c) +

(cid:107)Axt − Axt+1(cid:107)2

= Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) + (xt − xt+1)T (cid:0) ˆ∇f (xt) − ∇f (xt)(cid:1)

+

L
2

(cid:107)xt+1 − xt(cid:107)2 −

1
η

(cid:107)xt+1 − xt(cid:107)2

H −

ρ
2

(cid:107)Axt − Axt+1(cid:107)2

(iii)
≤ Lρ(xt, yt+1, λt) − Lρ(xt+1, yt+1, λt) +

1
2

(cid:107) ˆ∇f (xt) − ∇f (xt)(cid:107)2

− (

φH
min
η

+

φA
minρ
2

−

L + 1
2

)(cid:107)xt − xt+1(cid:107)2,

)] =

(74)

(75)

where the inequality (i) holds by (15); the equality (ii) holds by applying the equality (a − b)T (b − c) = 1
2 ((cid:107)a − c(cid:107)2 − (cid:107)a −
b(cid:107)2 − (cid:107)b − c(cid:107)2) on the term ρ(Axt − Axt+1)T (Axt+1 + Byt+1 − c); the inequality (iii) holds by the Cauchy inequality.
Taking expectation conditioned on information It to (75), we have

E[Lρ(xt+1, yt+1, λt)] ≤Lρ(xt, yt+1, λt) +

L2
2nM

n
(cid:88)

i=1

E(cid:107)xt − zt

i (cid:107)2 − (

φH
min
η

+

φA
minρ
2

−

L + 1
2

)(cid:107)xt − xt+1(cid:107)2.

(76)

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

23

By the step 8 of Algorithm 3, and taking expectation conditioned on It, we have

E[Lρ(xt+1, yt+1, λt+1) − Lρ(xt+1, yt+1, λt)] =

1
ρ

E(cid:107)λt − λt+1(cid:107)2

(i)
≤

+

5L2
ρφA
minM n
5(φH
max)2
φA
minη2ρ

n
(cid:88)

i=1

E(cid:107)xt − zt

i (cid:107)2 +

5L2
minM n

ρφA

n
(cid:88)

(cid:107)xt−1 − zt−1

i

(cid:107)2

E(cid:107)xt+1 − xt(cid:107)2 +

(cid:107)xt − xt−1(cid:107)2,

(77)

i=1
max)2)

5(L2η2 + (φH
φA
minη2ρ

where the inequality (i) holds by Lemma 8. Combining (74), (76) and (77), we have

E[Lρ(xt+1, yt+1, λt+1)] ≤Lρ(xt, yt, λt) +

minρL2

n
(cid:88)

10L2 + φA
2ρφA

i=1

minM n
(cid:107)xt − xt−1(cid:107)2 − (cid:0) φH
min
η

ρφA

5L2
minM n
L + 1
2

−

n
(cid:88)

i=1

−

+

φA
minρ
2

E(cid:107)xt − zt

i (cid:107)2 +

(cid:107)xt−1 − zt−1

i

(cid:107)2

max)2
5(φH
φA
minη2ρ

(cid:1)E(cid:107)xt+1 − xt(cid:107)2.

+

5(L2η2 + (φH
φA
minη2ρ

max)2)

Next, we give an upper bound of 1
n

(cid:80)n

i=1

E(cid:107)xt+1 − zt+1

i

(cid:107)2. Using the step 9 in Algorithm 3, we have

1
n

n
(cid:88)

i=1

E(cid:107)xt+1 − zt+1

i

(cid:107)2 =

=

n
(cid:88)

1
n

i=1
n − M
n

1
n

n
(cid:88)

i=1

E(cid:107)xt+1 − zt

i (cid:107)2.

(cid:2) M
n

E(cid:107)xt+1 − xt+1(cid:107)2 +

n − M
n

E(cid:107)xt+1 − zt

i (cid:107)2(cid:3)

The term E(cid:107)xt+1 − zt

i (cid:107)2 in (79) can be bounded below:

E(cid:107)xt+1 − zt

i (cid:107)2 = E(cid:107)xt+1 − xt + xt − zt

i (cid:107)2

= E[(cid:107)xt+1 − xt(cid:107)2 + 2(xt+1 − xt)T (xt − zt
(i)
≤ E[(cid:107)xt+1 − xt(cid:107)2 + 2(

E(cid:107)xt+1 − xt(cid:107)2 +

1
2β

= (1 +

1
β

)E(cid:107)xt+1 − xt(cid:107)2 + (1 + β)(cid:107)xt − zt

i (cid:107)2]

i ) + (cid:107)xt−1 − zt
β
2
i (cid:107)2,

(cid:107)xt − zt

i (cid:107)2) + (cid:107)xt − zt

i (cid:107)2]

where β > 0, and the inequality (i) is due to Cauchy-Schwarz inequality. Thus, we have

1
n

n
(cid:88)

i=1

E(cid:107)xt+1 − zt+1

i

(cid:107)2 ≤

n − M
n

(1 +

1
β

)E(cid:107)xt+1 − xt(cid:107)2 +

n − M
n

(1 + β)

1
n

n
(cid:88)

i=1

(cid:107)xt − zt

i (cid:107)2

Combining (78) and (81), we have

E(cid:2)Lρ(xt+1, yt+1, λt+1) +

max)2)

(cid:107)xt+1 − xt(cid:107)2 +

αt+1
n

n
(cid:88)

i=1

((cid:107)xt+1 − zt+1

i

(cid:107)2 + (cid:107)xt − zt

i (cid:107)2)(cid:3)

5(L2η2 + (φH
φA
minη2ρ
5(L2η2 + (φH
max)2)
φA
minη2ρ
2n − M
n

+ (

+

≤ Lρ(xt, yt, λt) +

(cid:107)xt − xt−1(cid:107)2

minρL2

+ (cid:0) 10L2 + φA
2ρφA
minM
φA
minρ
2

+

− (cid:0) φH
min
η
− (cid:0) L2
2M

+ (

2n − M
n

+

n − M
n

−

L + 1
2

−

5L2η2 + 10(φH
φA
minη2ρ
n
(cid:1) 1
(cid:88)
n

β)αt+1

i=1

n − M
n

β)αt+1

max)2

(cid:1) 1
n

−

n
(cid:88)

(cid:0)(cid:107)xt − zt

i (cid:107)2 + (cid:107)xt−1 − zt−1

i

(cid:107)2(cid:1)

i=1
n − M
n

(1 +

1
β

)αt+1

(cid:1)(cid:107)xt+1 − xt(cid:107)2

(cid:107)xt−1 − zt−1

(cid:107)2.

i

By the deﬁnition of the sequence {Θt}T

t=1 (29), we have

Θt+1 ≤ Θt − Γt(cid:107)xt+1 − xt(cid:107)2 − (cid:0) L2
2M

+ (

2n − M
n

+

n − M
n

β)αt+1

(cid:1) 1
n

n
(cid:88)

i=1

(cid:107)xt−1 − zt−1

i

(cid:107)2.

(78)

(79)

(80)

(81)

(82)

(83)

Using (31) and the properties of quadratic equation in one unknown, then we have Γt > 0. Finally, we prove that the sequence
{Θt}T

t=1 monotonically decreases.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

C.3 Proof of Theorem 3
Proof. By (83), we have, for t ∈ {1, 2, · · · , T }

Θt+1 ≤ Θt − Γt(cid:107)xt+1 − xt(cid:107)2 − (cid:0) L2
2M

+ (

2n − M
n

+

n − M
n

β)αt+1

(cid:1) 1
n

n
(cid:88)

i=1

(cid:107)xt−1 − zt−1

i

(cid:107)2.

Summing (84) over t = 1, 2, · · · , T , we have

ΘT ≤ Θ1 − γ

T
(cid:88)

t=1

E(cid:107)xt+1 − xt(cid:107)2 − ω

T
(cid:88)

t=1

1
n

n
(cid:88)

i=1

(cid:107)xt−1 − zt−1

i

(cid:107)2,

24

(84)

(85)

where γ = mint Γt and ω = mint
holds for ∀t ≥ 1. By (33) and (85), then, we have

L2
2M + ( 2n−M

n + n−M

n β)αt+1. By Lemma 10, there exists a constant Θ∗ such that Θt ≥ Θ∗

where τ = min(γ, ω).

Next, by the optimal condition of step 7 in Algorithm 3, we have

t∗ = arg min
2≤t≤T +1

˜θt ≤

2
τ T

(Θ1 − Θ∗),

E(cid:107)AT λt+1 − ∇f (xt+1)(cid:107)2
= E(cid:107) ˆ∇f (xt) − ∇f (xt+1) −

H
η

(xt − xt+1)(cid:107)2

= E(cid:107) ˆ∇f (xt) − ∇f (xt) + ∇f (xt) − ∇f (xt+1) −

H
η

(xt − xt+1)(cid:107)2

n
(cid:88)

(cid:107)xt − zt

i (cid:107)2 + 3(cid:0)L2 +

max)2
(φH
η2

(cid:1)(cid:107)xt − xt+1(cid:107)2

≤

3L2
nM

i=1
≤ 3(cid:0)L2 +

max)2
(φH
η2

(cid:1)˜θt.

By Lemma 8, we have

E(cid:107)Axt+1 + Byt+1 − c(cid:107)2 =

≤

+

≤

1
ρ2 (cid:107)λt+1 − λt(cid:107)2
n
(cid:88)

5L2
φA
minnM ρ2
max)2
5(φH
φA
minη2ρ2
5(L2η2 + (φH
φA
minη2ρ2

i=1

E(cid:107)xt − zt

i (cid:107)2 +

n
(cid:88)

(cid:107)xt−1 − zt−1

i

(cid:107)2

5L2
φA
minnM ρ2
5(L2η2 + (φH
φA
minη2ρ2

i=1
max)2)

E(cid:107)xt+1 − xt(cid:107)2 +

(cid:107)xt − xt−1(cid:107)2

max)2)

˜θt =

ζ
ρ2

˜θt.

By the step 5 of Algorithm 3, there exists a subgradient µ ∈ ∂g(yt+1) such that

E(cid:2)

dist(BT λt+1, ∂g(yt+1))2(cid:3) ≤ (cid:107)µ − BT λt+1(cid:107)2

= (cid:107)BT λt − ρBT (Axt + Byt+1 − c) − BT λt+1(cid:107)2
= (cid:107)ρBT A(xt+1 − xt)(cid:107)2
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2(cid:107)xt+1 − xt(cid:107)2
≤ ρ2(cid:107)B(cid:107)2(cid:107)A(cid:107)2 ˜θt.

(86)

(87)

(88)

(89)

Finally, using the above bounds (87), (88) and (89), and the deﬁnition 1, an (cid:15)-stationary point of the problem (3) holds in
expectation.

