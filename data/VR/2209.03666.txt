R3LIVE++: A Robust, Real-time, Radiance
reconstruction package with a tightly-coupled
LiDAR-Inertial-Visual state Estimator

Jiarong Lin and Fu Zhang

1

2
2
0
2

p
e
S
8

]

V
C
.
s
c
[

1
v
6
6
6
3
0
.
9
0
2
2
:
v
i
X
r
a

Fig. 1 – The radiance map of HKU (a and b) and HKUST campuses (c) reconstructed by R3LIVE++ in real-time (see our accompanying
video on YouTube: youtu.be/qXrnIfn-7yA).

Abstract—This work proposed a LiDAR-inertial-visual fusion framework termed R3LIVE++ to achieve robust and accurate state
estimation while simultaneously reconstructing the radiance map on the ﬂy. R3LIVE++ consists of a LiDAR-inertial odometry (LIO) and
a visual-inertial odometry (VIO), both running in real-time. The LIO subsystem utilizes the measurements from a LiDAR for
reconstructing the geometric structure, while the VIO subsystem simultaneously recovers the radiance information of the geometric
structure from the input images. R3LIVE++ is developed based on R3LIVE and further improves the accuracy in localization and
mapping by accounting for the camera photometric calibration and the online estimation of camera exposure time. We conduct more
extensive experiments on public and private datasets to compare our proposed system against other state-of-the-art SLAM systems.
Quantitative and qualitative results show that R3LIVE++ has signiﬁcant improvements over others in both accuracy and robustness.
Moreover, to demonstrate the extendability of R3LIVE++, we developed several applications based on our reconstructed maps, such as
high dynamic range (HDR) imaging, virtual environment exploration, and 3D video gaming. Lastly, to share our ﬁndings and make
contributions to the community, we release our codes, hardware design, and dataset on our Github:
github.com/hku-mars/r3live.

Index Terms—SLAM, 3D reconstruction, State estimation, Sensor fusion

(cid:70)

1 INTRODUCTION

S IMULTANEOUS localization and mapping (SLAM) is a

technology that utilizes a sequence of sensor (e.g., cam-
era, LiDAR, IMU, etc.) data to estimate the sensor poses
and simultaneously reconstruct the 3D map of surrounding
environments. Since SLAM can estimate poses in real-time,
it has been widely applied in localization and feedback con-
trol for autonomous robots (e.g., unmanned aerial vehicles
[1], [2], automated ground vehicles [3], [4], [5], and self-
driving cars [6], [7], [8]). Meanwhile, with the capacity to
reconstruct the map in real-time, SLAM is also crucial in
various robots navigation, virtual and augmented reality
(VR/AR), surveying, and mapping applications. Different
applications usually require a different level of mapping
details: sparse feature map, 3D dense point cloud map, and
3D radiance map (i.e., a 3D point cloud map with radiance

‚

J. Lin and F. Zhang are with the Department of Mechanical Engineering,
The University of Hong Kong, Hong Kong SAR, China.
E-mail: tjiarong.lin, fuzhangu@hku.hk

information). For example, the sparse visual feature map
is suitable and has been widely used for camera-based
localization, where the sparse features observed in images
can be used for calculating the camera’s pose [9], [10]. The
3D dense point cloud can capture the geometrical structure
of the environment even for tiny objects. Hence it is widely
used in robot navigation and obstacle avoidance [2], [11]. Fi-
nally, radiance maps containing both geometry and radiance
information are used in mobile mapping, AR/VR, video
gaming, 3D simulation, and surveying. These applications
require both geometric structures and textures to provide
virtual environments alike the real world [12], [13].

Existing SLAM systems can be mainly categorized into
two classes based on the used sensor: visual SLAM and
LiDAR SLAM. Visual SLAM is based on low-cost and SWaP
(size, weight, and power)-efﬁcient camera sensors and has
achieved satisfactory results in localization accuracy. The
rich colorful information measured by cameras also makes
the reconstructed map suitable for human interpretation.
However, due to the lack of direct accurate depth mea-
surements, the mapping accuracy and resolution of visual

 
 
 
 
 
 
SLAM are usually lower than LiDAR SLAM. To be more
speciﬁc, visual SLAM maps the environments by triangulat-
ing disparities from multi-view images (e.g., structure from
motion for mono-camera, stereo-vision for stereo-camera),
an extremely computationally expensive process that often
requires hardware acceleration or server clusters. Moreover,
limited by the measurement noises and the baseline of
multi-view images, the computed depth accuracy drops
quadratically with the measurement distance, making visual
SLAM difﬁcult to reconstruct large-scale outdoor scenes.
Furthermore, visual SLAM can only work in scenarios with
good illuminations and will degenerate in high-occlusion or
texture-less environments.

On the other hand, LiDAR SLAM is based on LiDAR
sensors. Beneﬁting from the high measurement accuracy (a
few centimeters) and the long measurement range (hun-
dreds of meters) of LiDAR sensors, LiDAR SLAM can
achieve much higher accuracy and efﬁciency on both lo-
calization and map reconstruction than visual SLAM. How-
ever, LiDAR SLAM easily fails in scenarios with insufﬁcient
geometry features, such as in long tunnel-like corridors,
facing a single big wall, etc. Moreover, LiDAR SLAM can
only reconstruct the geometric structure of the environment,
but lacks the color information.

Fusing both LiDAR and camera measurements in the
SLAM could overcome the degeneration issues of each
sensor in localization and produce an accurate, textured,
and high-resolution 3D map that sufﬁces the needs of var-
ious mapping applications. Motivated by this, we propose
R3LIVE++, which has the following features:

‚

‚

‚

It is a LiDAR-Inertial-Visual fusion framework that
tightly couples two subsystems: the LiDAR-inertial
odometry (LIO) subsystem and the visual-inertial
odometry (VIO) subsystem. The two subsystems
jointly and incrementally build a 3D radiance map
of the environment in real-time. In particular, the
LIO subsystem reconstructs the geometric structure
by registering new points in each LiDAR scan to the
map, and the VIO subsystem recovers the radiance
information by rendering pixel colors in each image
to points in the map.
It has a novel VIO design, which tracks the camera
pose (and estimates other system state) by minimiz-
ing the radiance difference between points from the
radiance map and a sparse set of pixels in the current
image. The frame-to-map alignment effectively low-
ers the odometry drift, and the direct photometric
error on a sparse set of individual pixels effectively
constrains the computation load. Moreover, based on
the photometric errors, the VIO is able to estimate
the camera exposure time online, which enables the
recovery of environments’ true radiance information.
It is extensively validated in real-world experiments
in terms of localization accuracy, robustness, and
radiance map reconstruction accuracy. Benchmark
results on 25 sequences from an open dataset (the
NCLT-dataset) show that R3LIVE++ achieves the
highest overall accuracy among all other state-of-the-
art SLAM systems (e.g., LVI-SAM, LIO-SAM, FAST-
LIO2, etc) under comparison. The evaluations on

2

our private dataset show that R3LIVE++ is robust to
extremely challenging scenarios that LiDAR and/or
camera measurements degenerate (e.g., when the
device is facing a single texture-less wall). Finally,
compared with other counterparts, R3LIVE++ esti-
mates the camera exposure time more accurately
and reconstructs the true radiance information of the
environment with signiﬁcantly smaller errors when
compared to the measured pixels in images.
It is, to our best knowledge, the ﬁrst radiance map
reconstruction framework that can achieve real-time
performance on a PC equipped with a standard
CPU without any hardware or GPU accelerations.
The system is completely open sourced to ease the
reproduction of this work and beneﬁt the following-
up researches. Based on a set of ofﬂine utilities for
mesh reconstruction and texturing further devel-
oped, the system shows high potentials in a variety
of real-world applications, such as 3D HDR imaging,
physics simulation, and video gaming.

‚

2 RELATED WORKS

In this chapter, we review existing works related to our
method or system, including LiDAR SLAM, visual SLAM,
and LiDAR-visual fused SLAM. Due to the large number of
existing works, any attempts to give a full review would be
incomplete, hence we only select the most relevant ones of
each branch for review.

2.1 LiDAR(-inertial) SLAM

In recent years, with the rapid development of LiDAR tech-
nologies, the reliability and performance of LiDAR sensors
have been greatly improved while the cost signiﬁcantly low-
ered, drawing increasing amount of researches on LiDAR
SLAM [14]. Zhang et al. propose a real-time LiDAR odom-
etry and mapping framework, LOAM [15], which achieves
localization by scan-to-scan point registration and mapping
by scan-to-map registration. In both registrations, only edge
and plane feature points are considered to lower the com-
putation load. To make the algorithm run in real-time at
computation-limited platforms, Shan et al. [16] propose a
lightweight and ground-optimized LOAM (LeGO-LOAM),
which discards unreliable features in the step of ground
plane segmentation. These works [15], [16] are mainly based
on multi-line spinning LiDARs. For emerging solid-state Li-
DARs that have irregular scanning and very small FoV, our
previous works [14], [17] use direct scan-to-map registration
to achieve localization and mapping.

To further improve the accuracy and robustness of Li-
DAR SLAM systems, many frameworks that fuse LiDAR
measurements with inertial sensors have been proposed.
In LOAM [15], an IMU could be used to de-skew the
LiDAR scan and give a motion prior for the scan-to-scan
registration. It is a loosely-coupled method since the IMU
bias (and the full state vector) is not involved in the scan
registration process. Compared with loosely-coupled meth-
ods, tightly-coupled methods show higher robustness and
accuracy, therefore drawing increasing research interests
recently. Authors in [18] propose LIOM, which uses a graph

optimization based on priors from LiDAR-Inertial odometry
and a rotation-constrained reﬁnement method. Compared
with the former algorithms, LIO-SAM [19] optimizes a
sliding window of keyframe poses in a factor graph to
achieve higher accuracy. Similarly, Li et al. propose LiLi-OM
[20] for both multi-line and solid-state LiDARs based on
a sliding window optimization technique. LINS [21] is the
ﬁrst tightly-coupled LIO that solves the 6 DOF ego-motion
via iterated Kalman ﬁltering. To lower the high computa-
tion load in calculating the Kalman gain, FAST-LIO [22]
proposes a new formula for the Kalman gain computation.
The resultant computation complexity depends on the state
dimension instead of measurement dimension. Its successor
FAST-LIO2 [23] further improves the computation efﬁciency
by proposing an incremental k-d tree. Such a data structure
can signiﬁcantly reduce the time cost of nearest points
search and allow the registration of raw points (instead of
feature points, such as planes and edges, in the past works).
The method using raw points is termed as a “direct” method
and could exploit subtle features in the environment and
hence increase the localization accuracy and robustness.

The LIO subsystem of R3LIVE++ is largely based on
FAST-LIO2 [23] since it achieves the best overall perfor-
mance among its counterparts in terms of accuracy, efﬁ-
ciency, and robustness. Moreover, to address the LiDAR
degeneration problem and further improve the localization
accuracy, we fuse the LIO subsystem with our VIO subsys-
tem in a tightly-coupled manner.

2.2 Visual(-inertial) SLAM

Depending on how a camera measurement is formulated,
we review the works of visual SLAM by categorizing them
into two branches based on the criteria proposed in [24]:
indirect and direct. These two types of methods have very
different pipelines: the former one (indirect method) in-
cludes feature extraction, data association, and minimiza-
tion of feature re-projection error. In contrast, the latter one
(direct method) directly minimizes the photometric error (or
intensity discrepancy) between consecutive images.

Indirect visual SLAM is also called the feature-based
method, which has a quite long history. MonoSLAM [25]
proposed by Davison et al. is the ﬁrst monocular visual
SLAM, which recovers the 3D trajectory of a camera in
real-time by creating a sparse but persistent map of nat-
ural landmarks within a probabilistic framework. PTAM
[26] proposed by Klein and Murray split the tracking and
mapping in parallel threads. Visual landmarks in the map
are selected from only a few frames to allow efﬁcient
bundle-adjustment (BA) optimization that estimates the
camera pose and landmark position. Following this idea,
a more complete and reliable framework ORB-SLAM [27]
was proposed. ORB-SLAM utilizes the same feature (i.e.,
ORB feature) for all the involved tasks, including tracking,
mapping, relocalization, and loop closing. Its further work
ORB-SLAM2 [28] improves the accuracy by utilizing the
metric scale provided by stereo or RGB-D cameras. The
scale issue in pure visual SLAM can also be addressed by
fusing inertial sensor data, such as VINS-mono [29] and
ORB-SLAM3 [30], which achieve high-accuracy localization
by fusing the IMU measurements and image features in a
sliding window bundle adjustment optimization.

3

Direct visual SLAM is also called photometric-based
method, which minimizes the intensity differences rather
than a geometric error. It has been successfully applied in
2D sparse feature tracking (e.g., Lucas–Kanade optical ﬂow
[31]) and then extended to visual SLAM. LSD-SLAM [32]
proposed by Engel et al. is a direct monocular SLAM algo-
rithm with both tracking and mapping directly operating on
image intensities. It incrementally tracks the camera pose us-
ing direct image alignment and simultaneously performs a
pose graph optimization to keep the entire camera trajectory
globally consistent. In DSO [24], authors proposed a fully
direct probabilistic model that integrates a full photometric
calibration. By incorporating a photometric bundle adjust-
ment, the system outperforms other state-of-the-art works in
terms of both accuracy and robustness. To achieve real-time
performance on a standard CPU, the authors also exploit
the sparsity structures of the corresponding Hessian matrix.
While the photometric model provides accurate pose esti-
mation over short-term tracking without data association,
the geometric model gives robustness for a large baseline.
Hybrid approaches that use both photometric and geometric
errors have been proposed, with the most representative
work SVO [33] proposed by Forster et al., where the short-
term tracking is solved by minimizing the photometric error,
while the long-term drift is constrained by a windowed
bundle adjustment on visual landmarks.

There have been many discussions in the literature to
answer the question: Which is better? While it is difﬁcult to
answer this question now, it is true that the direct method
often shows better short-term performance in low-textured
environments [24], [30]. Besides, the direct method is often
more computation efﬁcient due to the removal of feature ex-
traction [33]. To leverage these advantages, R3LIVE++ uses
a photometric-based VIO subsystem. Unlike the pure visual
(or visual-inertial) direct SLAM systems, which perform
bundle adjustment on photometric errors [24] or feature
reprojection errors [33] to restrain long-term drift, the VIO
in R3LIVE++ makes fully use of the geometry structure
reconstructed from LiDAR point cloud by minimizing the
radiance errors between map points and image pixels. Such
a frame-to-map alignment effectively lowers the odometry
drift at a low computation cost. Moreover, pure visual (or
visual-inertial) direct methods construct photometric errors
on dense images [32] or a sparse set of image patches
[24], [33], while the photometric errors of R3LIVE++ VIO
subsystem are on a sparse set of individual pixels. Fur-
thermore, the VIO in R3LIVE++ accounts for the camera
photometric calibration (e.g., non-linear response function
and lens vignetting) and estimates the camera exposure
time online, which help improve the odometry accuracy and
recover the true radiance information of the environment.

2.3 LiDAR-visual fused SLAM

On the basis of LiDAR-inertial methods, LiDAR-inertial-
visual odometry incorporating measurements from visual
sensors shows higher robustness and accuracy. Zhang and
Singh in [34] propose a LiDAR-inertial-visual system that
uses a loosely-coupled VIO as the motion model to initial-
ize the LiDAR mapping subsystem. Similarly, Shao et al.

4

Fig. 2 – The overview of our proposed system.

in [35] propose a stereo visual-inertial LiDAR SLAM that
incorporates the tightly-coupled stereo VIO with LiDAR
mapping and LiDAR-enhanced visual loop closure. The
overall system is still a loosely-coupled fusion since the
LiDAR data are not jointly optimized along with the visual-
inertial measurements.

There are also some RGB-D-inertial SLAM systems, such
as [35], [36]. Designed for RGB-D cameras, these methods
are difﬁcult to be applied on LiDARs due to the signiﬁcant
differences in the measurement pattern, range and density
between RGB-D cameras and LiDAR sensors. In [37], LiDAR
measurements are used to provide depth information for
camera images at each frame, forming a system similar
to RGB-D camera and hence can leverage existing visual
SLAM works such as ORB-SLAM2 [28]. This is also a
loosely-coupled method as it ignores the direct constraints
imposed by LiDAR measurements.

For works mentioned above, the measurement of LiDAR
and camera are fused in a loosely-coupled manner. To
achieve higher accuracy and robustness, frameworks that
fuse sensor data in a tightly-coupled way are proposed in
recent years. Zuo et al. [38] propose a LIC-fusion framework
combining IMU measurements, sparse visual features, and
LiDAR plane and edge features with online spatial and
temporal calibration based on the MSCKF framework. The
system exhibits higher accuracy and robustness than other
state-of-the-art methods in their experiment results. Later
on, their further work termed LIC-Fusion 2.0 [39] reﬁnes
a novel plane-feature tracking algorithm across multiple
LiDAR scans within a sliding window to make LiDAR scan
matching more robust. Shan et al. in [40] propose LVI-SAM
that fuses the LiDAR, visual and inertial sensors in a tightly-
coupled smooth and mapping framework, which is built
atop a factor graph. The LiDAR-inertial and visual-inertial
subsystems of LVI-SAM can function independently when
failure is detected in one of them, or jointly when enough
features are detected. A similar tightly-coupled system is
our previous work R2LIVE [41], which fuses the LiDAR and
camera measurements in an on-manifold iterated Kalman
ﬁlter. R2LIVE can run in various challenging scenarios even
with small LiDAR FoV, aggressive motions, sensor failures,
and narrow tunnel-like environments with moving objects.
The above LiDAR-inertial-visual systems all use feature-

based methods in both LIO and VIO. In contrast, R3LIVE++
uses direct methods in both LIO and VIO to best exploit
any subtle features in the environments even in case of
extreme scenarios (e.g., structure-less and/or texture-less
environments). Moreover, the above LiDAR-inertial-visual
systems mainly focus on the localization part and has very
limited consideration on the mapping efﬁciency and accu-
racy. Hence, their visual and LiDAR subsystem often main-
tains two separate map for the LIO and VIO, preventing the
data fusion at a deeper level and the reconstruction of high-
accuracy colored 3D maps. R3LIVE++ is designed to per-
form both localization and radiance map reconstruction in
real-time. The central of these two tasks is a single radiance
map shared among and maintained by both LIO and VIO.
In particular, the LIO subsystem reconstructs the geometric
structure of the map and the VIO subsystem recovers the
radiance information of the map.

This paper is an extension of the previously published
work R3LIVE [42]. The extended works of this paper in-
clude (1) a full incorporation of the camera photometric
calibration, which corrects the camera nonlinear response
function and lens vignetting effect; (2) online estimation of
the camera exposure time. The estimated exposure time and
the camera photometric calibration enables the system to
recover the true radiance information of the environment;
(3) a more comprehensive evaluation of the system on both
open and private dataset in terms of localization accuracy,
robustness and radiance map reconstruction accuracy; and
(4) release of the system codes, private dataset, and the in-
house designed hardware devices for collecting this dataset.

3 BASIC MODELS
3.1 Notations

In this paper, we use notations shown in Table 1.

3.2 System overview

To simultaneously estimate the sensor pose and reconstruct
the environment radiance map, we design a tightly-coupled
LiDAR-inertial-visual sensor fusion framework, as shown in
Fig. 2. The proposed framework contains two subsystems:
the LIO subsystem (upper part) and the VIO subsystem

IMU inputPlanar-feature extraction &  motion compensationFast corners detection & KLT optical flowPlane-to-plane residual computationPnP reprojection errorState propagationLiDAR features mapsError-state iterated Kalman filter updatePriorestimationLiDAR rate odometryIMU rateodometryCamera rateodometryLiDARmeasurementVisualmeasurementLiDARmeasurementVisualmeasurementTrackedFactor graph optimizationAppendRetriveMarginalizationFactors within the sliding windowFactors within the sliding window     Filter-based odometryProjectR3LIVE systemoverviewVisual landmarksTriangulationCamera input10~30 HzCamera input10~30 HzLiDAR input10~50 HzLiDAR input10~50 HzLiDAR inputLiDAR inputCamera inputCamera inputIMU inputMotion compensationPoint to plane LIO updatePerspective-n-Point VIO updatePhotometric VIO update Frame-to- frame optical flow R3LIVE systemoverviewLIO VIO Frame-to-frameFrame-to-mapGeometry structurePoint cloud textureRetriveAppendRetriveAppendProjectRenderProjectRenderGeometry structureMapsState propagationIMU rateodometryCamera rateodometryLiDAR rate odometryLiDAR inputLiDAR inputCamera inputCamera inputIMU inputMotion compensationPoint to plane LIO updateFrame-to-map VIO updatePhotometric calibrationR3LIVE++ systemoverviewVIO Radiance map reconstructionRadiance informationRetriveAppendRetriveAppendProjectUpdateProjectUpdateGeometric structureState propagationIMU rateodometryCamera rateodometryLiDAR rate odometryLIO TrackedFrame-to-frame VIO updateRadiance mapTABLE 1 – NOMENCLATURE

Notation

Explanation

Expressions

‘{a The encapsulated “boxplus” and

“boxminus” operations on manifold [43]
Gp¨q The value of p¨q expressed in global frame
C p¨q The value of p¨q expressed in camera frame
Expp¨q{Logp¨q The Rodrigues’ transformation between the

rotation matrix and rotation vector

δ p¨q The estimated error of p¨q parameterized

in tangent space.

Σp¨q The covariance matrix of vector p¨q

Variables

bg, ba The bias of gyroscope and accelerometer in an IMU
Gg The gravitational acceleration in global frame
Gv The linear velocity in global frame

pGRI , GpI q The IMU attitude and position w.r.t. global frame
pI RC , I pC q The extrinsic between camera and IMU

x The ground-true state
ˆx The prior estimation of x
ˇx The current estimate of x in each ESIKF iteration

(lower part). The LIO subsystem constructs the geometric
structure of the radiance map by registering point cloud
measurements of each input LiDAR scan. The VIO subsys-
tem recovers the radiance information of the map in two
steps: the frame-to-frame VIO update estimates the system
state by minimizing the frame-to-frame PnP reprojection
error, while the frame-to-map VIO update minimizes the ra-
diance error between map points and the current image. The
two subsystems are tightly coupled within an on-manifold
error-state iterated Kalman ﬁlter framework (ESIKF) [43],
where the LiDAR and camera visual measurements are
fused to the same system state (Section 3.4) at their respec-
tive data reception time (Section 4 and Section 5).

3.2.1 Point

Our radiance map is composed of map points in the global
frame, each point P is a structure as below:

“

P “

Gpx, Gpy, Gpz, γr, γg, γb

‰T

“

GpT , γT

‰T

“
“

P R6 (1)
‰

T

Gpx, Gpy, Gpz

P R3
where the head sub-vector Gp “
denotes the point 3D position, and the tail sub-vector
γ “ rγr, γg, γbsT P R3 is the point radiance consisting
of three independent channels (i.e., red, green, and blue
channel) accounting for the camera photometric calibration
(see Section 3.3). Besides, we also record other necessary
information of this point, such as the 3 ˆ 3 matrix Σp and
Σγ , which denote the covariance of the estimation errors of
Gp and γ, respectively, and the timestamps when this point
was created and updated.

3.2.2 Voxel

To inquiry a point in the radiance map efﬁciently (e.g., for
camera pose tracking in Section 5.3 and map point radiance
recovery in Section 5.4), we put map points in ﬁx-size (e.g.
0.1 mˆ0.1 mˆ0.1 m) voxels. If a voxel has points appended
recently (e.g. in recent 1 second), we mark this voxel as
activated. Otherwise, this voxel is marked as deactivated.

5

Fig. 3 – The image formation process of a color camera.

3.3 Color camera photometric model

A camera observes the radiance of the real world in the
form of images that consists of 2D arrays of pixel intensities.
In our work, we model the image formation process of a
camera based on [44] and further extend the gray camera
model to a color camera. As shown in Fig. 3, for a point P
in the world, it reﬂects the incoming lights emitted from a
light source (e.g., the sun). The reﬂected lights then pass
through the camera lens and ﬁnally arrive at the CMOS
sensor, which records the intensity of the reﬂected lights and
creates a pixel channel in the output image. The recorded
intensity is determined by the radiance, a measure of the
power reﬂected at the point P.

To model the above imaging process, we denote γ the
radiance at point P. Since a color camera has three channels
in its CMOS sensor: red, green, and blue, the radiance γ has
three components: γr, γg, γb, respectively. For each channel
i, the lights passing through the camera lens has power

Oipρq “ V pρqγi

(2)

where V pρq P r0, 1s is called the vignetting factor account-
ing for the lens vignetting effect. Since the vignetting effect
is different at different area of the lens, the vignetting factor
V pρq is a function of the pixel location ρ.

Oipρq is the amount of power that can be received
by the sensor and is called the irradiance. When taking
an image, the captured irradiance Opρq is integrated over
time (i.e., the exposure time τ ). The accumulated irradiance
θi “ τ V pρqγi is then converted as the output of pixel
intensity Iipρq via the camera response function (CRF) fip¨q:

Iipρq “ fipτ V pρqγiq, Ii P r0, 255s .

(3)

Since a real camera sensor has a limited dynamic range and
that the physical scale of the radiance γ can not be recovered
anyway, the pixel intensities can be normalized within r0, 1s
without loss of generality.

As noted in (3), different channels often has different
non-linear response function (CRF) fip¨q and they can be cal-
ibrated ofﬂine along with the vignetting factor V pρq based
on the method in [45]. The exposure time τ is estimated
online in our work. With the calibration and estimation

RGBGRBGGRBGGBRGGBRGGBRGGBRG  Light sourceCameralenseVignettingGBRGGBRGGBRGGBRGGBRGGBRGGBRGV(ρ)VignettingV(ρ)VignettingLight sourceLight sourcef(I)Camera response function f(·)Accumulative irradiancePixel intensityCamera response function f(·)Accumulative irradiancePixel intensityCamera response function f(·)Accumulative irradiancePixel intensityγIresults, the radiance of point P from the observed pixel
value Ipρq can be computed as:

γi “

f ´1
i

pIipρqq

τ V pρq

(4)

Remark: Under the assumption of constant continuous
light sources and a Lambertian reﬂection model, the radi-
ance at point P is a constant physical value that is invariant
to the camera pose. Such invariance to time and camera pose
enables us to infer the camera ego-motion from the radiance
difference between the map and the current image (with
photometric calibration), as detailed in Section 5.3.

3.4 State
In our work, we deﬁne the full state x as:

`

x “

GRI , GpI , Gv, bg, ba, Gg, I RC, I pC, (cid:15), I tC, φ

˘

(5)

T

‰

“

fx, fy, cx, cy

where the notations GRI , GpI , Gv, bg, ba, Gg, I RC , I pC
are explained in Table 1, I tC is the time-offset between IMU
and camera while LiDAR is assumed to be synced with
the IMU already, (cid:15) “ 1{τ is the inverse camera exposure
time, φ “
are the camera intrinsics, where
pfx, fyq denote the camera focal length and pcx, cyq the
offsets of the principal point from the top-left corner of the
image plane. The camera extrinsic pI RC, I pCq, intrinsic φ
and time-offset I tC would usually have their rough values
available (e.g., ofﬂine calibration, CAD model, manufac-
turer’s manual). To cope with the possible calibration errors
(e.g., extrinsic pI RC, I pCq and intrinsic φ) or online drifting
(e.g., time-offset I tC ), we also include them in the state x
such that they will be estimated online. Beside, we also
estimate the camera exposure time online in order to recover
the true radiance value of each map point.

4 LIDAR-INERTIAL ODOMETRY (LIO)

Our LIO subsystem reconstructs the geometry structure of
the environment by registering each new LiDAR scan to the
global map. We use the generalized-iterative closet point
(GICP) method [46] to iteratively estimate the LiDAR pose
(and other system state) by minimizes the distance of each
point in the scan to a plane ﬁtted from the corresponding
points in the map. The estimated state estimate is then used
to append the new points to the map.

4.1 LiDAR point-to-plane residual

(cid:32)
Lp1, ...,L pm

As shown in Fig. 2, our LIO subsystem constructs the
geometric structure of the global map. For the k-th in-
put LiDAR scan, we ﬁrst compensate the in-frame motion
with a IMU backward propagation introduced in [22]. Let
Lk “
be the set of m LiDAR points after
motion compensation, we compute the residual of each raw
point (or a downsampled subset) of Lps P Lk where s is the
index of point and the superscript L denotes that the point
is represented in the LiDAR-reference frame.

(

With ˇxk being the estimate of xk at the current iteration,

we transform Lps from LiDAR frame to the global frame:

Gps “ G ˇRIk pI RL

Lps ` I pLq ` G ˇpIk

(6)

6

To register the point to the global map, we search for the
nearest ﬁve points in the map. To accelerate the nearest
neighbor search, map points are organized into an incre-
mental k-d tree (see [23]). The found nearest neighbor points
are used to ﬁt a plane with normal us and centroid qs, then
the LiDAR measurement residual rlpˇxk, Lpsq is:
rlpˇxk, Lpsq “uT
s

Gps ´ qs

(7)

˘

`

4.2 LIO ESIKF update

The residual in (7) should be zero ideally. However, due
to the estimation error in ˇxk and the LiDAR measurement
noise, this residual is often not zero and can be used to reﬁne
the state estimate ˇxk. Speciﬁcally, let ns be the measurement
noise of the point Lps, we have the relation between the true
point location Lpgt

s and the measured one Lps as below:

Lps “ Lpgt

s ` ns, ns „ N p0, Σns q.

(8)

This true point location together with the true state xk
should lead to zero residual in (7), i.e.,

0 “ rlpxk, Lp

gt
s q « rlpˇxk, Lpsq ` Hl

sδˇxk ` αs,

(9)

where xk is parameterized by its error δˇxk in the tangent
space of ˇxk (i.e., xk “ ˇxk ‘ δˇxk), αs „ N p0, Σαs q is
the lumped noise due to ns, and Hl
s is the Jacobian of the
residual w.r.t. δˇxk.

Equation (9) constitutes an observation distribution for
xk (or equivalently δˇxk ﬁ xk a ˇxk), which is combined with
the prior distribution from the IMU propagation:
´
}pˇxk ‘ δˇxkq a ˆxk}2
ÿm

min
δˇxk

(10)

¯

Σδˆxk
›
›rlpˇxk, Lpsq ` Hl

sδˇxk

`

›
›2
Σαs

s“1

where }x}2
Σ “ xT Σ´1x is the squared Mahalanobis dis-
tance with covariance Σ, ˆxk is the IMU propagated state
estimate, and Σδˆxk is the IMU propagated state covariance.
The detailed derivation of ﬁrst item in (10) can be found in
Section IV-E of R2LIVE [41].

Solving (10) leads to the Maximum A-Posteriori (MAP)
k which is then added to ˇxk as below

estimate of δˇxo

ˇxk Ð ˇxk ‘ δˇxo
k

(11)

The above iteration process is iterated until convergence
(i.e., the update δˇxo
k is smaller than a given threshold). The
converged state estimate ˇxk is then used as the starting
point of the IMU propagation until the reception of the
next LiDAR scan or camera image. Furthermore, the con-
verged estimate ˇxk is used to append points in the current
LiDAR scan to the global map as follows. For the s-th point
Lps P Lk, its position in global frame Gps is ﬁrst obtained
by (6). If Gps has nearby points in the map with distance
1 cm (see Section 3.2.1), Gps will be discarded to maintain a
spatial resolution of 1 cm. Otherwise, a new point structure
Ps will be created in the map with:

‰T

“
Gps, 0

‰T

“

GpT

Ps “

s , γT
s
where the radiance vector γs is set as zero and will be ini-
tialized at the ﬁrst time it is observed in forthcoming images
(see Section 5.4). Finally, we mark the voxel containing Gps
as activated such that the radiance of points in this voxel can
be updated by the forthcoming images (see Section 5.4).

(12)

“

5 VISUAL-INERTIAL ODOMETRY (VIO)

While our LIO subsystem reconstructs the geometric struc-
ture of the environment, our VIO subsystem recovers the
radiance information from the input color images. To be
more speciﬁc, our VIO subsystem projects a certain number
of points (i.e., tracked points) from the global map to the
current image, then it iteratively estimates the camera pose
(and other system state) by minimizing the radiance error
of these points. Only a sparse set of tracked map points is
used for the sake of computation efﬁciency.

Our proposed framework is different from previous
photometric-based methods [33], [47], which constitute the
residual of a point by considering the photometric error
over all its neighborhood pixels (i.e., a patch). These patch-
based methods achieve stronger robustness and faster con-
vergence speed than that without. However, the patch-
based method is not invariant to either translation or rota-
tion, which requires estimating the relative transform when
aligning one patch to another. Plus, the calculation of the
residual is not completely precise by assuming the depths
of all pixels in the patch are the same as the mid-point. On
the other hand, our VIO is operated at an individual pixel,
which utilizes the radiance of a single map point to compute
the residual. The radiance, which is updated simultaneously
in the VIO, is an inherent property of a point in the world
and is invariant to both camera translation and rotation. To
ensure a robust and fast convergence, we design a two-step
pipeline shown in Fig. 2, where in the ﬁrst step (i.e., frame-
to-frame VIO) we leverage a frame-to-frame optical ﬂow
to track map points observed in the last frame and obtain
a rough estimate of the system’s state by minimizing the
Perspective-n-Point (PnP) reprojection error of the tracked
points (Section 5.2). Then, in the second step (i.e., frame-to-
map VIO), the state estimate is further reﬁned by minimiz-
ing the difference between the radiance of map points and
the pixel intensities at their projected location in the current
image (Section 5.3). With the converged state estimate and
the raw input image, we ﬁnally update map points radiance
according to the current image measurement (Section 5.4).

5.1 Photometric correction
For each incoming image I, we ﬁrst correct the image
non-linear CRF fip¨q and the vignetting factor V p¨q, which
are calibrated in advance (see Section 3.3), to obtain the
photometrically corrected image Γ, whose i-th channel at
pixel location ρ is:

Γipρq “

f ´1
i

pIipρqq
V pρq

.

(13)

The photometrically corrected image Γ is then used in the
following VIO pipelines including the frame-to-frame VIO,
frame-to-map VIO and radiance recovery.

7

Fig. 4 – Frame-to-frame VIO estimates the system’s state by mini-
mizing the PnP reprojection error of map points observed in the last
frame.

Lucas´Kanade optical ﬂow to ﬁnd out their corresponding
location in the current image Ik, denoted as tρ1k , ..., ρmk u.
Then, we iteratively minimize the reprojection errors of P to
obtain a rough estimate of the state (see Fig. 4). Speciﬁcally,
“
taking the s-th point Ps “
P P as example,
let ˇxk be the state estimate at the current iteration, the
projection error rc

ˇxk, ρsk , Gps

GpT
˘

s , γT
s

is

`

‰

T

`

rc

˘

ˇxk, ρsk , Gps

“ ρsk ´ πpGps, ˇxkq

(14)

where πpGps, ˇxkq P R2 is the predicted pixel location
computed as below:

πpGps, ˇxkq “ πphpGps, ˇxkq `

I ˇtCk
∆tk´1,k

pρsk ´ ρsk´1 q (15)

where the ﬁrst term πphpGps, ˇxkq is the standard camera
pin-hole model, the second one is the temporal correction
factor [48], and ∆tk´1,k is the time interval between the last
and current image.

5.2.2 Frame-to-frame VIO Update
Similar to the LIO update, the state estimation error in ˇxk
and the camera measurement noise will lead to a certain
residual in (14), from which we can update the state estimate
ˇxk as follows. First, the measurement noise in the residual
(14) consists of two sources: one is the pixel tracking error in
ρsk and the other lies in the map point location error Gps,

Gps “ Gpgt
ρsk “ ρgt

s ` nps, nps „ N p0, Σnps q
„ N p0, Σnρsk
sk ` nρsk

, nρsk

(16)

(17)

q

s and ρgt

sk are the true values of Gps and ρsk ,
where Gpgt
respectively. Then, correcting such noises and using the true
system state should lead to zero residual, i.e.,

0 “ rcpxk, ρgt
sk

, Gpgt

s q « rc

ˇxk, ρsk , Gps

`

˘

` Hr

sδˇxk ` βs

(18)

5.2 Frame-to-frame Visual-Inertial odometry

5.2.1 Perspective-n-Point reprojection error
Assume we have tracked m map points P “ tP1, ..., Pmu
image frame Ik´1 with their projected lo-
in the last
cation in Ik´1 being tρ1k´1, ..., ρmk´1u, we leverage the

where Hr
N p0, Σβs q is the lumped noise due to nps and nρs .

s is the Jacobian of the residual w.r.t. δˇxk and βs „

Equation (18) constitutes an observation distribution for
xk, which is combined with the IMU propagation to obtain
the MAP estimate of the state in the same way as the LIO

ΓkΓk-1Gpsxkxk-1ρskρskΔtk-1,kρsk-1ρsk-1ρsk-1ρsk-1^xk︿xk︿xk︿xk︿π(Gps , xk)︿π(Gps , xk)︿ρsk-1ρsk-1ρskρskρsk-1ρsk-18

Fig. 6 – We update the radiance γs of a map point via Bayesian
update.

where γgt
s is the ground truth of γs, the ﬁrst noise nγs is due
to the radiance estimation error (Section 5.4), and the second
noise nic accounts for the radiance temporal change caused
by illumination change. Since the illumination often changes
slowly over time, we model it as a random walk, hence
its covariance is linear to ∆tγs , the time interval between
current time and last update time of Ps.

`

For the second component Φs in (19), it is computed
from the state estimate ˇxk and the current image Γk as Φs “
ˇ(cid:15)kΓk
, hence its noise consists of two sources:
one is the state estimation error (from ˇxk) and the other is
the image measurement noise (from Γk):

πpGps, ˇxkq

˘

Φs “ Φgt

s ` nΦs, nΦs „ N p0, ΣnΦs q

(21)

where ΣnΦs denotes the covariance due to these two noise
sources.

Combining (19), (20) and (21), we obtain the ﬁrst order
s q:

Taylor expansion of the true zero residual rcpxk, Gpgt

s , γgt

0 “ rcpxk, Gpgt

s , γgt

s q « rcpˇxk, Gps, csq ` Hc

sδˇxk ` ζs (22)

where Hr
s is the Jacobian of the residual w.r.t. δˇxk and ζs „
N p0, Σζsq is the lumped noise due to noises in γs and Φs.
Similar as before, (22) constitutes an observation dis-
tribution for state xk, which is combined with the IMU
propgation to obtain the MAP estimate of the state.

Remark: Since the Φs in (19) is related to camera expo-
sure time (cid:15), it will cause Hc
s to contain nonzero elements
corresponding to the exposure time and hence an update of
them in the state estimation.

5.4 Recovery of radiance information

After the frame-to-map VIO update, we have the precise
pose of the current image. Then, we perform the Bayesian
update to determine the optimal radiance of all map points
such that the average radiance error between each point and
its viewed images is minimal.

First of all, we retrieve all the points in all activated voxels
(activated in Section 4.2). Assume the retrieved point set is
Q “ tP1, ..., Pnu. For the s-th point Ps “
P Q
falling in the current image FoV, we ﬁrst can obtain the
observed radiance vector Φs by (19) and its covariance
ΣnΦs by (21). Then, if Ps is a new point appended by the
LIO subsystem (see Section 4.2) with γs “ 0, we set:

s , γT
s

GpT

“

‰

T

γs “ Φs, Σnγs “ ΣnΦs

(23)

Fig. 5 – Frame-to-map VIO reﬁnes the state estimate by minimizing
the radiance error between the map point and the observed radiance
in the current image.

update detailed in Section 4.2. The converged state estimate
is then reﬁned in the frame-to-map VIO in the next section.
Remark: Since the camera pin-hole model πphpGps, xkq
in (15) is related to camera pose (consisting of the IMU pose
pGRI , GpI q and camera extrinsic pI RC, I pCq) and intrinsic
φ, so the projection model πpGps, xkq is also related to these
state components. In addition, πpGps, xkq is also related
to the temporal offset I tC due to the temporal correction
factor. This will cause Hr
s to contain nonzero elements cor-
responding to the IMU pose pGRI , GpI q, camera extrinsic
pI RC, I pCq, intrinsic φ, and temporal offset I tC , and hence
an update of them in the state estimation.

5.3 Frame-to-map Visual-Inertial odometry

5.3.1 Frame-to-map radiance error

The frame-to-frame VIO update can provide a good state
estimate ˇxk, which is further reﬁned by the frame-to-map
VIO update by minimizing the radiance error of the tracked
map points P. Let Γk the photometrically calibrated image
at the k-th step (see (13)). With the state estimate at the
current iteration, ˇxk, which contains the estimated camera
pose, extrinsic, intrinsic, and exposure time, we project a
tracked map point Ps P P to the image plane to obtain its
pixel location ˇρsk “ πpGps, ˇxkq (see (15) and Fig. 5). Then,
the observed radiance denoted by Φs can be computed from
the exposure time component ˇ(cid:15)k in ˇxk as: Φs “ ˇ(cid:15)kΓkp ˇρsk q.
Finally, the frame-to-map radiance error is the difference
between the radiance component γs of the point Ps and
the observed value Φs:

rcpˇxk, Gps, γsq “ Φs ´ γs, Φs “ ˇ(cid:15)kΓkp ˇρsk q,

(19)

where Φs, Γkp ˇρsk q and γs both contain three channels: red,
green, and blue.

5.3.2 Frame-to-map VIO update

The measurement noise in (19) come from both the compo-
nent γs and Φs. For the component γs, we model it as:

γs “ γgt

s ` nγs ` nic, nγs „ N p0, Σnγs q

nic „ N p0, σ2

ic ¨ ∆tγs q

(20)

xkΓkRadiance MapγsΦsΦsΣnΦsΣnγsΣnγsΣnγsΣnγsΣn+σic2·∆tγsLast updated  color Prior estimated  color Last updated  color Prior estimated  color Observed  color Current updated  color Observed  color Current updated  color γsγsΣnγsΣncscsΣncsΣncsΣncsΣn+σs2·∆tcsΣn+σs2·∆tcs~cs~csΣncsΣn~csΣn~Last updated  color Last updated  color Observed  color Observed  color Current updated  color Current updated  color Last updated  color Observed  color Current updated  color Last updated radiance vector Last updated radiance vector Observed radiance vector Observed radiance vector Current updatedradiance vectorCurrent updatedradiance vector+=Last updated radiance vector Observed radiance vector Current updatedradiance vector+=ΦsΦsΣnΦsΣnγsγsΣnγsΣnγsΣnγsΣn+σic2·∆tγsΣn+σic2·∆tγs~γs~γsΣnγsΣn~γsΣn~ΦsΦsΣnγsγsΣnγsΣn+σic2·∆tγs~γsΣn~γsOtherwise, the radiance vector γs saved in the map (see
(20)) is fused with newly observed radiance vector Φs with
covariance ΣnΦs via Bayesian update (see Fig. 6):
¯

´`

˘

´1

´`

˜γs “

Σn˜γs “

Σnγs ` σ2

Σnγs ` σ2
˘
´1

ic ¨ ∆tγs

γs ` Σ´1
nΦs

ic ¨ ∆tγs
γs “ ˜γs, Σnγs “ Σn˜γs

Φs

` Σ´1
nΦs
¯
´1

´1

Σn˜γs

(24)

(25)

(26)

5.5 Update of the tracking points

After the recovery of radiance information, we update the
tracked point set P for the next frame of image use. Firstly,
we remove points from current P if their projection error in
(14) or radiance error in (19) are too large, and also remove
the points which does not fall into the current image FoV.
Secondly, we project each point in Q to the current image
and add it to P if no other tracked points already existed in
a neighborhood of 50 pixels.

6 EXPERIMENTS
In this chapter, we conduct extensive experiments to vali-
date the advantages of our proposed system against other
counterparts in threefold: 1) To verify the accuracy in lo-
calization, we quantitatively compare our system against
existing state-of-the-art SLAM systems on a public dataset
(NCLT-dataset). 2) To validate the robustness of our frame-
work, we test it under various challenging scenarios where
camera and LiDAR sensor degeneration occurs. 3) To evalu-
ate the accuracy of our system in reconstructing the radiance
map, we compare it against existing baselines in estimat-
ing the camera exposure time and calculating the average
photometric error w.r.t. each image. In the experiments, two
datasets are used for evaluation: the NCLT-dataset and the
R3LIVE-dataset.

6.1 NCLT-dataset

To compare the accuracy of our proposed method against
other state-of-the-art SLAM systems, we perform quanti-
tative evaluations on NCLT dataset [49]. NCLT-dataset is
a large-scale, long-term autonomy dataset for robotics re-
search that was collected on the University of Michigan’s
North Campus. The dataset is comprised of 27 sequences
that are collected by exploring the campus, both indoors and
outdoors, on varying paths, and at different time of a day
across all four seasons. Each sequence includes data from
the omnidirectional camera, 3D lidar, planar lidar, GPS, and
wheel encoders on a Segway robot.

We choose NCLT-dataset for three reasons: 1) NCLT-
dataset is currently the largest public dataset with ground-
truth trajectories of high quality. 2) NCLT-dataset provides
all raw data sampled by the sensors, which meets our
requirement for the input data. 3) NCLT-dataset has many
challenging scenarios, such as moving obstacles (e.g., pedes-
trians, bicyclists, and cars), illumination changing, vary-
ing viewpoint, seasonal and weather changes (e.g., falling
leaves and snow), and long-term structural changes caused
by construction projects.

In the experiments, the front-facing camera data (one of
ﬁve) and the 3D LiDAR data are used for all systems under

9

evaluation. Moreover, we notice some time synchronization
errors in two sequences (i.e., 2012-03-17 and 2012-08-04),
where the LiDAR timestamp is 100ms delayed from the IMU
timestamp (about one LiDAR-frame). Therefore, we exclude
these two sequences from the evaluation. As a result, 25
sequences are evaluated with total traveling length up to
138 km and duration up to 33 h:34 min.

6.2 Our private dataset: R3LIVE-dataset

While the large-scale NCLT-dataset is suitable for evaluating
the localization accuracy, it didn’t cover any scenarios with
sensor degeneration, preventing us from evaluating the
system robustness, which is one of the major motivations
of this work. Moreover, the camera photometric calibration
and the ground-true exposure time are not available in the
NCLT-dataset, which are essential for the reconstruction
of the radiance maps and the evaluation of the online
exposure time estimation. To ﬁll this gap, we designed a
handheld data collection device and made a new dataset
named R3LIVE-dataset. The dataset and hardware device
are released along with the codes of this work to facilitate
the reproduction of our work.

6.2.1 Handheld device for data collection

Fig. 7 – (a) shows our handheld device for data collection. (b) shows
the ArUco marker board to provide the ground-truth for evaluating
the system accuracy. (c) shows our open source schematics model.

Our handheld device for data collection is shown in Fig.
7(a), which includes a power supply unit, an onboard com-
puter DJI manifold-2c (equipped with an Intel i7-8550u CPU
and 8 GB RAM), a FLIR Blackﬂy BFS-u3-13y3c global shutter
camera, and a LiVOX AVIA 3D LiDAR. The camera FoV is
82.9 °ˆ66.5° and the LiDAR FoV is 70.4 °ˆ77.2°. To quanti-
tatively evaluate the accuracy of our algorithm (Section 6.5)
even in GPS denied environments, we use an ArUco marker
[50] as a reference to calculate the sensor pose when returns
to the starting point, which enables to evaluate the local-
ization drift. All of the mechanical modules of this device
are designed as FDM printable, schematics of the design are
opened on our Github: github.com/hku-mars/r3live.
To correct the camera’s nonlinear response function and
the vignette effect, we perform photometric calibration on

the camera based on the method in [45]. The calibrated re-
sults are shown in Fig.8, which are also released on Github.

Fig. 8 – The left ﬁgure shows the calibrated nonlinear response
function in three channels (red, green, and blue). The right one plots
the calibrated vignetting factors at each image pixel.

6.2.2 The R3LIVE-dataset
The R3LIVE-dataset was collected within the campuses
of the University of Hong Kong (HKU) and the Hong
Kong University of Science and Technology (HKUST). As
summarized in Table 2, the dataset includes 13 sequences
that are collected by exploring both indoor and outdoor
environments, in various scenes (e.g., walkway, park, forest,
etc) at different time in a day (i.e., morning, noon, and
evening). This allows the dataset to capture both structured
urban buildings and cluttered ﬁeld environments with dif-
ferent lighting conditions. The dataset also includes three
sequences (degenerate seq 00/01/02) where the LiDAR or
camera (or both) degenerate by occasionally facing the
device to a single and/or texture-less plane (e.g., wall,
the ground) or visually. The total traveling length reaches
8.4 km, duration reaching 2.4 h. More details of each se-
quence will be provided in sequel when it is used.

6.3 System conﬁgurations

fair comparison,

For the sake of
in the evaluation of
our systems and their counterparts, each system uses
the same parameters for all sequences in the same
dataset. For the counterpart systems (e.g., LIO-SAM, LVI-
SAM, FAST-LIO2, etc), we use their default conﬁgura-
tions on their Github repository except for some nec-
essary adjustments to match the hardware setup. For
our system, we also make its conﬁguration available on
Github, “r3live_config_nclt.yaml” for NCLT-dataset
and “r3live_config.yaml” for R3LIVE-dataset.

10

deactivated the loop closure of LIO-SAM and LVI-SAM for
the sake of fair comparison. Since the camera photometric
calibration of the NCLT-dataset is not available, we disable
the photometric calibration modules of our VIO-subsystem
by using V p¨q “ 1 and fip¨q “ 1.

Table 3 shows the absolute position error (APE) [51] of
these methods, where Our-LIO is the LIO subsystem of our
system. LIO-SAM and LVI-SAM failed in some sequences,
and these sequences are excluded from the computation of
the average APE. As can be seen from this table, with the
average APE only 8.51 m, our proposed system achieves the
best overall performance than feature-based LiDAR-inertial-
visual systems R2LIVE and LVI-SAM. The performance im-
provement mainly come from the direct method used in the
LIO subsystem and the tight-coupling of the LIO and VIO
subsystems, the former can be seen by comparing the direct
method FAST-LIO2 to the feature-based method LIO-SAM
in Table 3 (and also detailed in [23]), the latter improves
the accuracy of the VIO subsystem (hence the complete
system) by leveraging the high-accuracy geometry structure
reconstructed from the LiDAR. Furthermore, the overall
APE of our system is lower than its LIO subsystem Our-
LIO and the other LIO systems (i.e., FAST-LIO2 and LIO-
SAM), which conﬁrms the effectiveness of fusing camera
data. Indeed, we found that in the evaluated sequences,
the LiDAR sensor may occasionally face the sun, which
creates a large number of noisy LiDAR points and adversely
affect the LIO accuracy. There are certain sequences where
our system is slightly outperformed by its LIO subsystem
and the other LIO systems, the reason residues in moving
objects (e.g., pedestrians, bicyclists, and cars ) which may
adversely affect the VIO (hence the overall system). In Fig.
10, we overlay all the 25 ground-true trajectories (in the
left ﬁgure) and ours (the right one). As can be seen, the
overlaid trajectories estimated by our system agree with the
ground-truth well and each trajectory can still be clearly
distinguished without noticeable errors. Note that these 25
trajectories are collected on the same campus area across
different time in a day and seasons in a year, still our system
can produce consistent and reliable trajectory estimation
with these illumination and scene changes, demonstrating
the robustness of our system.

6.5 Experiment-2: Evaluation of robustness

Besides illumination and scene change, we also test the
robustness of our system to extreme scenarios where sensor
degeneration occurs. We use the R3LIVE-dataset, which
contains such extreme scenarios.

6.4 Experiment-1: Evaluation of localization accuracy

In this experiment, we benchmark the localization accu-
racy of our systems against other state-of-the-art SLAM
systems, including LIO-SAM [19], LVI-SAM [40], FAST-LIO2
[23], and our previous work R2LIVE [41], on the NCLT-
dataset [49]. LIO-SAM and FAST-LIO2 are LiDAR-inertial
systems without fusing image data (Section 2.1), while LVI-
SAM and R2LIVE are two feature-based LiDAR-inertial-
visual systems (see Section 2.3). Since our work is a state
estimator without any loop detection and correction, we

6.5.1 Evaluation of robustness in LiDAR degenerated sce-
narios

In this experiment, we evaluate the robustness of our pro-
posed system by testing our system on the sequence “degen-
erate seq 00” and “degenerate seq 01” of R3LIVE-dataset
(see Section 6.2.2). These two sequences were collected in
front of a stairway with the LiDAR occasionally facing
against the ground and a side wall (see Fig. 9(a) and (b)).
When facing a wall, the LiDAR only observes a single plane,
which is insufﬁcient to determine the LiDAR pose, causing

0.000.250.500.751.00Accumulated sensor irradiance050100150200250Brightness valueR-channelG-channelB-channel0.00.20.40.60.81.0Vignette factorTABLE 2 – Overview of the R3LIVE-dataset.

Sequence

degenerate seq 00
degenerate seq 01
degenerate seq 02
hku campus seq 00
hku campus seq 01
hku campus seq 02
hku campus seq 03
hku main building
hku park 00
hku park 01
hkust campus 00
hkust campus 01
hkust campus 02
hkust campus 03
Total

Duration
(s)
101
86
85
202
304
323
173
1170
351
228
1073
1162
478
1618
7354

Traveling
Length (m)
74.9
53.3
75.2
190.6
374.6
354.3
181.2
1036.9
401.8
247.3
1317.2
1524.3
503.8
2112.2
8447.6

Sensor
Degeneration
Camera, LiDAR
LiDAR
LiDAR
—-
—-
—-
—-
—-
—-
—-
—-
—-
—-
—-

Return to
origin1
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Aruco marker2

Camera exposure
time3

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

1 Sequences are collected by traveling a loop, with starting from and ending with the same position.
2 Sequences with ArUco marker for providing the ground-truth relative pose.
3 Sequences with ground-truth camera exposure time read from camera’s API.

11

Scenarios

Indoor
Outdoor
Outdoor
Indoor
Outdoor
Indoor, Outdoor
Indoor, Outdoor
Indoor, Outdoor
Outdoor, Cluttered
Outdoor, Cluttered
Indoor, Outdoor
Indoor, Outdoor
Indoor, Outdoor
Outdoor

degenerate. We use sequence “degenerate seq 02” of the
R3LIVE-dataset, where the sensor device passes through a
narrow “T”-shape passage (see Fig. 11) while occasionally
facing against the side walls, causing LiDAR degeneration.
Moreover, the visual texture on the white walls is very
limited (Fig. 11(a) and Fig. 11(c)), especially for the wall-
1, which has only changes in illumination. The absence of
available LiDAR and visual features makes such scenarios
rather challenging for both LiDAR-based and visual-based
SLAM methods.

Taking advantage of the raw pixel color information and
tightly fusing it with the LiDAR point cloud measurements,
our proposed algorithm can “survive” in such extremely
difﬁcult scenarios. Fig. 12 shows our estimated pose, with
the phases of passing through “wall-1” and “wall-2” shaded
with purple and yellow, respectively. The estimated covari-
ance is also shown in Fig. 12, which is bounded over the
entire estimated trajectory, indicating that our estimation
quality is stable over the entire process. The sensor is moved
to the starting point, where an ArUco marker board is used
to obtain the ground-true relative pose between the starting
and end poses. Compared with the ground-true end pose,
our algorithm drifts 1.62˝ in rotation and 4.57 cm in transla-
tion. We recommend the readers to the accompanying video
on YouTube (youtu.be/qXrnIfn-7yA?t=461) for better
visualization of the experiment.

6.6 Experiment-3: Evaluation of radiance map recon-
struction

In this experiment, we evaluate the accuracy of our pro-
posed algorithm in reconstructing the radiance map. Since
the ground-true radiance map of the environment can not
be measured, we evaluate the accuracy based on two indi-
cators: one is the estimation quality of the camera exposure
time and the other is the average photometric error between
the reconstructed radiance map and the measured images.

Fig. 9 – Tests in LiDAR degenerated environments.

LiDAR degeneration. The device starts from and ends at
the same location, enabling the evaluation of localization
drift. The estimated trajectories of our proposed system,
our LIO-subsystem Our-LIO, and another two LIO systems,
FAST-LIO2 and LIO-SAM, are shown in Fig. 9(c) and (d). As
can be seen, due to the LiDAR degeneration when facing
a single plane, all three LiDAR-inertial odometry systems
failed and did not return to the starting point. In contrast,
by exploiting clues from the visual images, our proposed
system works well in these two sequences and successfully
returned to the starting point with drift down to 4.1 cm
and 4.6 cm on sequences “degenerate seq 00” and “de-
generate seq 01”, respectively. To obtain a more intuitive
comprehension of the LiDAR degenerated scenarios, we
recommend our readers to watch the accompanying video
on YouTube: youtu.be/qXrnIfn-7yA?t=390.

6.5.2 Evaluate of robustness in simultaneously LiDAR de-
generated and visual texture-less environments

In this experiment, we challenge one of the most difﬁ-
cult scenarios in SLAM, where both LiDAR and camera

10.07.55.02.50.02.55.00246810degenerate_seq_0005101520251050510152025degenerate_seq_01OurOur-LIOFAST-LIO2LIO-SAMOrigin(c)(d)TABLE 3 – The comparison of absolute position errors (APE, meters) on NCLT-dataset

12

Sequence
(date)

2012-01-08
2012-01-15
2012-01-22
2012-02-02
2012-02-04
2012-02-05
2012-02-12
2012-02-18
2012-02-19
2012-03-17
2012-03-31
2012-04-29
2012-05-11
2012-05-26
2012-06-15
2012-08-04
2012-08-20
2012-09-28
2012-10-28
2012-11-04
2012-11-17
2012-12-01
2013-01-10
2013-02-23
2013-04-05
Total:
Average

Length
(m)

6495.69
7499.80
6183.07
6315.78
5641.00
6649.26
5829.12
6249.20
6232.68
5907.19
6073.71
3183.09
6116.74
6340.70
4085.89
5492.13
6014.51
5574.41
5682.10
4788.33
5751.89
4991.93
1137.32
5235.27
4523.65
137994.46

Duration
(hour:minute:second)

1 hr:25 min:35 sec
1 hr:52 min:19 sec
1 hr:27 min:22 sec
1 hr:38 min:36 sec
1 hr:18 min:30 sec
1 hr:34 min:17 sec
1 hr:25 min:35 sec
1 hr:29 min:55 sec
1 hr:29 min:11 sec
1 hr:22 min:53 sec
1 hr:27 min:53 sec
43 min:18 sec
1 hr:25 min:5 sec
1 hr:28 min:34 sec
55 min:10 sec
1 hr:20 min:32 sec
1 hr:23 min:48 sec
1 hr: 17 min:59 sec
1 hr:26 min:10 sec
1 hr:20 min:39 sec
1 hr:29 min:44 sec
1 hr:16 min:48 sec
17 min:4 sec
1hr: 20min:08 sec
1 hr:9 min:27 sec
33 hr: 34 min: 52 sec

Our

R2LIVE

LVI-SAM Our-LIO Fast-LIO2

LIO-SAM

10.81
6.64
9.23
5.33
5.58
8.52
4.50
40.50
8.52
4.83
4.94
6.32
3.70
4.55
7.74
3.85
4.50
7.89
7.71
7.48
8.68
11.25
3.41
11.64
8.82

22.43
5.10
12.64
6.05
8.36
7.58
6.47
59.25
6.58
5.94
9.96
6.43
3.79
6.30
6.29
3.73
4.46
6.59
7.95
9.31
6.48
14.21
4.57
13.39
11.91

23.43
—-
8.29
18.08
9.63
—-
40.01
—-
8.87
12.58
19.04
5.94
4.23
18.34
—-
11.00
11.20
34.42
—-
3.42
21.92
6.93
4.88
12.60
9.83

20.07
6.19
12.39
7.44
7.78
7.67
10.48
53.53
6.19
6.77
9.33
6.27
4.21
6.13
5.65
4.53
4.18
6.84
8.61
12.55
6.13
16.96
5.30
13.79
11.55

18.50
4.81
7.14
9.12
7.19
7.80
8.30
56.97
5.98
4.70
7.27
6.44
4.13
6.43
5.27
6.96
6.02
10.42
7.68
3.33
5.83
7.41
3.50
11.85
6.38

21.66
—-
8.99
15.63
10.78
—-
45.02
—-
9.63
11.82
18.34
5.67
4.16
18.38
—-
12.73
11.43
36.71
—-
3.37
24.17
7.21
5.08
12.20
9.01

8.51

10.58

15.03

10.75

9.59

15.39

1 Some systems fail in midway in some sequences and are marked as ”—-”.

Fig. 10 – The overlay of ground-true trajectory and ours on NCLT-dataset.

13

Fig. 11 – Tests in simultaneously LiDAR degenerated and visual
texture-less environments.

Fig. 12 – The estimated poses and their 3-σ bound with 5 times
ampliﬁcation for better visualization (the light-colored area around
the trajectory) of the test in simultaneously LiDAR degenerated and
visual texture-less environments. The shaded areas in purple and
yellow are the phases of the sensors facing against the white “wall-
1” and “wall-2”, respectively.

6.6.1 Evaluation of exposure time estimation

In this experiment, we evaluate the accuracy of the es-
timated camera exposure time by comparing it with the
ground-true value read from the camera’s API. We use
four sequences (see Fig. 13) of the R3LIVE-dataset, where
the data were collected by traveling through both interior
and exterior of a complex building to ensure signiﬁcant
changes in lighting conditions. We compare our estimated
results with Tum-cali [44], which is currently the only work
that can estimate the camera’s exposure time online to our
knowledge. Both our system and [44] are initialized by
assuming the exposure time of the ﬁrst image frame is at
a default value 1 ms.

The results are shown in Fig. 13, where the estimated
exposure time of both our method and Tum-cali [44] is
re-scaled to match with the ground-truth for better visu-
alization. The average and maximum error of estimated
exposure time w.r.t. the ground-truth is listed in Table 4.
As shown in Fig. 13 and Table 4, our proposed method
shows signiﬁcantly lower estimation error than [44]. This
is mainly because our method estimates the exposure time
by minimizing the scan-to-map radiance error, while [44]
recovers the exposure times from consecutive frames. The

Fig. 13 – Estimation of camera exposure time.

TABLE 4 – The comparison of estimation error of exposure time over
5 sequences.

Sequence

hku campus seq 02
hku campus seq 03
hku main building
hkust campus seq 02
hkust campus seq 03

Our
Mean / Max (ms)
3.460 / 20.311
1.460 / 10.653
2.572 / 16.855
0.302 / 3.514
0.189 / 1.185

Tum cali
Mean / Max (ms)
7.082 / 36.175
6.400 / 37.126
5.196 / 26.775
5.225 / 13.361
0.341 / 1.451

consequence is that our method can better utilize longer-
term temporal intensity changes to restrain the drift of the
exposure time estimation.

6.6.2 Evaluation of radiance map

In this experiment, we evaluate the accuracy of our pro-
posed algorithm in reconstructing the radiance map. Cur-
rently, LiDAR point cloud colorization remains one of the
most challenging problems in the ﬁeld of 3D reconstruction.

White wall-1White wall-2Current LiDARscanCurrent LiDARscan(a)(b)(c)(d)(e)−200−1000100200Attitude (∘)Ang_xAng_yAng_zWall-1Wall-20102030405060708090100Time (s)-10.00-5.000.005.0010.00Position (m)Pos_xPos_yPos_zWall-1Wall-2Our estimated pose on sequence degenerate_seq_020501001502002503000204060Exposure (ms)hku_campus_seq_0202550751001251500204060Exposure (ms)hku_campus_seq_030200400600800100002040Exposure (ms)hku_main_building010020030040001020Exposure (ms)hkust_campus_seq_0202004006008001000120014001600Time (s)0123Exposure (ms)hkust_campus_seq_03OurTum-caliGround-truth14

Fig. 14 – Photometric errors between the reconstructed radiance map and image pixels.

Fig. 15 – Closeup of a few scenes in the radiance map reconstructed by the baseline, R3LIVE and R3LIVE++.

TABLE 5 – The average photometric error among all sequences of
R3LIVE-dataset.

Sequence
degenerate seq 00
degenerate seq 01
degenerate seq 02
hku campus seq 00
hku campus seq 01
hku campus seq 02
hku campus seq 03
hku main building
hku park 00
hku park 01
hkust campus 00
hkust campus 01
hkust campus 02
hkust campus 03
Average

Frames
1694
1715
3315
3016
4502
2595
4845
12157
5251
3410
16075
17426
4031
24270
7413.57

baseline
30.58
34.24
27.14
34.78
34.97
39.01
42.95
43.12
43.86
43.29
37.64
39.09
41.74
35.60
38.60

R3LIVE
21.36
21.28
20.30
22.56
22.47
23.73
24.78
22.26
27.01
25.17
24.19
24.67
23.77
22.37
23.58

R3LIVE++
16.19
16.55
15.97
14.57
16.30
16.85
16.93
19.04
20.07
19.02
18.00
18.53
18.92
18.65
18.01

The most common way is using the most recent image
frame in time to give the color of each LiDAR frame [52],
[53]. The preliminary implementation of our system R3LIVE
published previously [42] colorized the point cloud by min-
imizing the photometric error similar to our current system
but does not consider any exposure time estimation or
photometric calibration. In this experiment, we compare our
system against the previous implementation R3LIVE [42]
to show the effectiveness of the exposure time estimation
and photometric calibration and against the current baseline
[52], [53] to show the advantage of the overall system.

To assess the radiance reconstruction error, after the
map reconstruction, we re-project all points in the map

05002040degenerate_seq_0005002040degenerate_seq_0105010002040degenerate_seq_0201002000204060hku_campus_seq_0002000204060hku_campus_seq_0102000204060hku_campus_seq_0201000204060hku_campus_seq_03050010000255075hku_main_building02000204060hku_park_0001002000204060hku_park_01050010000255075hkust_campus_seq_00050010000204060hkust_campus_seq_0102004000255075hkust_campus_seq_020500100015000204060hkust_campus_seq_03Time (s)Photometric errorBaselineR3LIVER3LIVE++15

Fig. 16 – Our reconstructed radiance map of the main building of HKU. (a) The bird’s view of the map, with its details shown in (b„n). (b„g)
closeup of outdoor scenarios and (h„n) closeup of indoor scenarios. To see the real-time reconstruction process of the map, please refer to the
video on YouTube: youtu.be/qXrnIfn-7yA?t=55.

the map point color and the RGB values of the image
at the projected pixel location. The average photometric
error of each image frame is calculated to evaluate the
accuracy of the reconstructed radiance map. We perform
the evaluation on all R3LIVE-dataset sequences, with the
results of each sequence are given in Fig. 14, and the average
photometric of each sequence is listed in Table 5. As can
be seen, our system has consistently achieved the lowest
photometric errors in all sequences and the next best is
R3LIVE. Fig. 15 shows a few closeups of the reconstructed
radiance map, from which we can clearly tell the words on
objects. Moreover, in Fig. 16, we present the reconstructed
radiance map of the sequence “hku main building” in
R3LIVE-dataset,
the data in both
interior and exterior of the main building of HKU. As
shown in Fig. 16, both the indoor and outdoor details
(e.g., marks on the road) are very clear, demonstrating that
our proposed algorithm is of high accuracy. What worth
mentioning is, this 3D radiance map is reconstructed on
the ﬂy as the data is being acquired (see the accompa-
nying video on YouTube: youtu.be/qXrnIfn-7yA?t=55).
For more qualitative results of other sequences, we
refer our
Supplementary Material:
github.com/hku-mars/r3live/blob/master/supply/
r3live_plus_plus_supplementary_material.pdf

in which we collect

to our

readers

Fig. 17 – (a„ d) Images rendered from the reconstructed radiance
map at different exposure time: 1 ms, 2 ms, 5 ms and 10 ms. (e) The
HDR image merged from (a„d). Notice that for the sake of better
visualization, those points that are over-exposure are not displayed.

with radiance information to each image frame with the
estimated camera pose and calibrated photometric param-
eters. Then, we calculate the photometric error between

6.7 Run time analysis

In this section, We investigate the average time consumption
of our proposed system on a CPU-Only PC (equipped with
an Intel i7-9700K CPU and 64 GB RAM). We counts the
average time consumption on all sequences of both datasets
(i.e., the NCLT-dataset and R3LIVE-dataset), whose results
are shown in Table 6 and Table 7, respectively. For the NCLT-
dataset, each LiDAR scan takes an average of 34.3 ms and
each camera image takes an average of 16.6 ms processing

TABLE 6 – The average time consumption per LiDAR or camera
frame of our system on NCLT-dataset.

NCLT-dataset
Sequence (date)
2012-01-08
2012-01-15
2012-01-22
2012-02-02
2012-02-04
2012-02-05
2012-02-12
2012-02-18
2012-02-19
2012-03-17
2012-03-31
2012-04-29
2012-05-11
2012-05-26
2012-06-15
2012-08-04
2012-08-20
2012-09-28
2012-10-28
2012-11-04
2012-11-17
2012-12-01
2013-01-10
2013-02-23
2013-04-05
Average

LiDAR frame
Mean / Std (ms)
33.852 / 9.110
35.517 / 11.602
34.392 / 11.719
33.812 / 11.188
32.599 / 10.498
34.823 / 11.147
32.738 / 11.765
37.284 / 10.169
38.004 / 9.928
32.196 / 11.154
36.283 / 10.377
33.652 / 9.014
34.044 / 8.964
37.623 / 11.315
29.07 / 8.807
29.231 / 10.427
28.561 / 10.952
36.692 / 10.738
36.147 / 11.497
37.066 / 11.430
38.153 / 9.187
36.357 / 11.660
32.977 / 10.355
36.171 / 10.720
29.542 / 10.052
34.271 / 10.551

Camera frame
Mean / Std (ms)
15.911 / 4.180
17.613 / 4.275
16.551 / 4.591
16.926 / 4.361
15.09 / 4.164
17.09 / 4.276
17.071 / 3.846
17.087 / 3.973
18.179 / 4.099
17.304 / 4.370
16.228 / 4.312
16.487 / 4.018
17.357 / 3.811
15.228 / 4.384
17.254 / 3.881
17.267 / 4.629
15.73 / 3.931
15.458 / 4.346
16.081 / 4.102
16.242 / 4.061
16.931 / 3.904
16.492 / 4.165
18.328 / 3.971
15.368 / 4.358
15.725 / 4.189
16.600 / 4.168

TABLE 7 – The average time consumption per LiDAR or camera
frame of our system on R3LIVE-dataset.

Sequence
(date)
degenerate seq 00
degenerate seq 01
degenerate seq 02
hku campus seq 00
hku campus seq 01
hku campus seq 02
hku campus seq 03
hku main building
hku park 00
hku park 01
hkust campus seq 00
hkust campus seq 01
hkust campus seq 02
hkust campus seq 03
Average

LiDAR frame
Mean / Std (ms)
17.927 / 9.080
8.111 / 3.622
21.638 / 6.793
27.659 / 8.159
25.328 / 11.246
20.757 / 5.109
21.705 / 4.903
25.123 / 10.246
26.908 / 7.196
24.412 / 6.598
28.160 / 9.703
27.058 / 10.135
22.857 / 6.135
30.757 / 5.109
23.453 / 7.431

Camera frame
Mean / Std (ms)
13.057 / 2.675
12.111 / 2.502
13.543 / 3.014
18.829 / 3.719
18.187 / 2.860
16.928 / 2.518
16.040 / 2.338
17.023 / 2.512
17.882 / 2.545
17.479 / 2.485
16.624 / 2.756
16.138 / 2.813
16.518 / 2.618
16.928 / 2.518
16.234 / 2.705

time. Since the data rate of the LiDAR and camera sensors
are 10 Hz and 5 Hz, the total processing time per second is
426 ms, comprising of the time for processing 10 lidar scans
(i.e., 343 ms) and 5 images (83 ms). For R3LIVE-dataset,
each LiDAR scan takes an average of 22.7 ms and each
camera image takes an average of 16.2 ms processing time.
Since the data rate of the LiDAR and camera sensors are
10 Hz and 15 Hz, the total processing time per second is
470 ms, comprising of the time for processing 10 lidar scans
(i.e., 235 ms) and 15 images (244 ms). In both cases, the
processing time required per second is below half a second,
indicating that our system runs in real-time (even two times
faster than real-time) in both pose estimation and radiance
map reconstruction. In addition, the processing time per
image is similar across the two datasets while that for each
lidar scan is quite different. The reason is that the lidar in
R3LIVE-dataset has a much lower data rate than that of the
NCLT-dataset (240 k versus 695 k points per second) while

16

the cameras have similar resolution.

7 APPLICATIONS WITH R3LIVE
7.1 High dynamic range (HDR) imaging

After the reconstruction of the radiance map, we are able to
render an image by projecting the map to an image plane
with a given pose and exposure time with equation (3).
Taking the sequence “hku main building” as an example,
Fig. 17(a), (b), (c) and (d) are the rendered images with
global exposure time of 1 ms, 2 ms, 5 ms and 10 ms, respec-
tively. These images rendered at different exposure times
can be merged into a HDR image shown in Fig. 17 (e).

7.2 Mesh reconstruction and texturing

Fig. 18 – (a) show the RGB-colored 3D points reconstructed by
R3LIVE++. (b) and (c) show the wireframe and surface of our
reconstructed mesh. (d) show the mesh after texture rendering.

While R3LIVE++ reconstructs the colored 3D map in
real-time, we also develop software utilities to mesh and tex-
ture the reconstructed map ofﬂine (see Fig. 18). For meshing,
we make use of the Delaunay triangulation and graph cuts
[54] implemented in CGAL [55] and openMVS [56]. After
the mesh construction, we texture the mesh with the vertex
(point) colors, with are rendered by our VIO subsystem.

Our developed utilities also export the colored point
map from R3LIVE++ or the ofﬂine meshed map into com-
monly used ﬁle formats such as “pcd”, “ply”, “obj”, etc. As
a result, the maps reconstructed by R3LIVE++ can be im-
ported into various 3D software, including but not limited
to CloudCompare [57], Meshlab [58], AutoDesk 3ds Max
[59].

7.3 Toward various of 3D applications

With the developed software utilities, we can export the re-
constructed 3D maps to Unreal Engine [60] to enable a series
of 3D applications. For example, in Fig. 19(a), we built a car
simulator with the AirSim [61]. In Fig. 19(b) and Fig. 19(c),
we used our reconstructed maps to develop video games
for mobile platforms and desktop PCs, respectively. To get
more details about our demos, we refer the readers to watch
our video on YoutuBe: youtu.be/qXrnIfn-7yA?t=516.

Fig. 19 – In (a), we built a car simulator with our maps and AirSim.
The images in yellow and blue frame-boxes are the depth and RGB
image query from the AirSim’s API. In (b) and (c), we developed
video games for mobile platforms and desktop PCs. The player in (b)
is controlling the actor to explore the campus of HKU, and in (c) is
ﬁghting against a dragon by shooting rubber balls at HKUST.

8 CONCLUSIONS AND FUTURE WORK
8.1 Conclusions

In this paper, we proposed a novel LiDAR-inertial-visual
fusion framework termed R3LIVE++ to achieve robust and
accurate state estimation while simultaneously reconstruct-
ing the radiance map on the ﬂy. This framework consists
of two subsystems (i.e., the LIO and the VIO) that jointly
and incrementally build a 3D radiance map of the envi-
ronment in real-time. By tightly fusing the measurement
of three different types of sensors, R3LIVE++ can achieve
higher localization accuracy while being robust enough to
scenarios with sensor degenerations.

In our experiments, we extensively validated our pro-
posed algorithm with real-world experiments in terms of
localization accuracy, robustness, and radiance map recon-
struction accuracy. The benchmark results on 25 sequences
from an open dataset (the NCLT-dataset) showed that
R3LIVE++ achieved the highest overall accuracy among
all other state-of-the-art SLAM systems under comparison.
The evaluation on a private dataset (the R3LIVE-dataset)
showed that R3LIVE++ was robust to extremely challenging
scenarios that LiDAR and/or camera measurements degen-
erate (e.g., when the device is facing a single texture-less
wall). Finally, compared with other counterparts, R3LIVE++
estimates the camera exposure time more accurately and
reconstructs the true radiance information of the environ-
ment with signiﬁcantly smaller errors when compared to
the measured values in images.

To demonstrate the extendability of our work, we de-
veloped several applications based on our reconstructed
radiance maps, such as high dynamic range (HDR) imaging,
virtual environment exploration, and 3D video gaming.
Finally, to share our ﬁndings and make contributions to
the community, we made our codes, hardware design, and
dataset publicly available on our Github.

8.2 Future work
In R3LIVE++, the radiance map is reconstructed with 3D
points that contain radiance information, which prevents
us from rendering high-resolution images from the radi-
ance map due to the limited point cloud density of the
radiance map (1 cm in our current implementation). While
further increasing this point cloud density is possible, it will
further increase the processing time. This point density is
also limited by the density of the raw points measured by
lidar sensors. Noticing that images often have much higher
resolution, in the future, we could explore how to make full
use of such high-resolution images in the fusion framework.

17

REFERENCES

[1] F. Gao, W. Wu, W. Gao, and S. Shen, “Flying on point clouds: On-
line trajectory generation and autonomous navigation for quadro-
tors in cluttered environments,” Journal of Field Robotics, vol. 36,
no. 4, pp. 710–733, 2019.

[2] F. Kong, W. Xu, Y. Cai, and F. Zhang, “Avoiding dynamic small
obstacles with onboard sensing and computation on aerial robots,”
IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7869–7876,
2021.

IEEE, 2015, pp. 245–248.

[3] H. Lategahn, A. Geiger, and B. Kitt, “Visual slam for autonomous
ground vehicles,” in 2011 IEEE International Conference on Robotics
and Automation.

IEEE, 2011, pp. 1732–1737.
[4] P. Beinschob and C. Reinke, “Graph slam based mapping for agv
localization in large-scale warehouses,” in 2015 IEEE International
Conference on Intelligent Computer Communication and Processing
(ICCP).
J. Lin, X. Liu, and F. Zhang, “A decentralized framework for
simultaneous calibration, localization and mapping with multiple
lidars,” in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).
J. Levinson, J. Askeland, J. Becker, J. Dolson, D. Held, S. Kammel,
J. Z. Kolter, D. Langer, O. Pink, V. Pratt, et al., “Towards fully
autonomous driving: Systems and algorithms,” in 2011 IEEE Intel-
ligent Vehicles Symposium (IV).

IEEE, 2020, pp. 4870–4877.

IEEE, 2011, pp. 163–168.

[5]

[6]

[7] G. Ros, A. Sappa, D. Ponsa, and A. M. Lopez, “Visual slam for
driverless cars: A brief survey,” in Intelligent Vehicles Symposium
(IV) Workshops, vol. 2, 2012, pp. 1–6.

[8] A. Singandhupe and H. M. La, “A review of slam techniques and
security in autonomous driving,” in 2019 third IEEE international
conference on robotic computing (IRC).

IEEE, 2019, pp. 602–607.

[9] O. Pink, “Visual map matching and localization using a global
feature map,” in 2008 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition Workshops.

IEEE, 2008, pp. 1–7.

[10] M. B ¨urki, M. Dymczyk, I. Gilitschenski, C. Cadena, R. Siegwart,
and J. Nieto, “Map management for efﬁcient long-term visual
localization in outdoor environments,” in 2018 IEEE Intelligent
vehicles symposium (IV).

IEEE, 2018, pp. 682–688.

[11] B. Nagy and C. Benedek, “Real-time point cloud alignment for
vehicle localization in a high resolution 3d map,” in Proceedings
of the European Conference on Computer Vision (ECCV) Workshops,
2018, pp. 0–0.

[12] W. Li, C. Pan, R. Zhang, J. Ren, Y. Ma, J. Fang, F. Yan, Q. Geng,
X. Huang, H. Gong, et al., “Aads: Augmented autonomous driving
simulation using data-driven algorithms,” Science robotics, vol. 4,
no. 28, p. eaaw0863, 2019.

[13] Z. Bao, S. Hossain, H. Lang, and X. Lin, “High-deﬁnition map
generation technologies for autonomous driving: A review,” arXiv
preprint arXiv:2206.05400, 2022.

[14] J. Lin and F. Zhang, “Loam livox: A fast, robust, high-precision
lidar odometry and mapping package for lidars of small fov,”
in 2020 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2020, pp. 3126–3131.

[15] J. Zhang and S. Singh, “Loam: Lidar odometry and mapping in
real-time.” in Robotics: Science and Systems, vol. 2, no. 9, 2014.
[16] T. Shan and B. Englot, “Lego-loam: Lightweight and ground-
optimized lidar odometry and mapping on variable terrain,” in
2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).

IEEE, 2018, pp. 4758–4765.
[17] J. Lin and F. Zhang, “A fast, complete, point cloud based
loop closure for lidar odometry and mapping,” arXiv preprint
arXiv:1909.11811, 2019.

[18] H. Ye, Y. Chen, and M. Liu, “Tightly coupled 3d lidar inertial
odometry and mapping,” in 2019 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2019.

[19] T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus, “Lio-
sam: Tightly-coupled lidar inertial odometry via smoothing and
mapping,” in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2020, pp. 5135–5142.

[20] K. Li, M. Li, and U. D. Hanebeck, “Towards high-performance
solid-state-lidar-inertial odometry and mapping,” arXiv preprint
arXiv:2010.13150, 2020.

[21] C. Qin, H. Ye, C. E. Pranata, J. Han, S. Zhang, and M. Liu, “Lins: A
lidar-inertial state estimator for robust and efﬁcient navigation,”
in 2020 IEEE International Conference on Robotics and Automation
(ICRA).

IEEE, 2020, pp. 8899–8906.

(b)(a)(a)(b)(c)(a)(b)(c)[22] W. Xu and F. Zhang, “Fast-lio: A fast, robust lidar-inertial odom-
etry package by tightly-coupled iterated kalman ﬁlter,” IEEE
Robotics and Automation Letters, 2021, in press.

[23] W. Xu, Y. Cai, D. He, J. Lin, and F. Zhang, “Fast-lio2: Fast direct
lidar-inertial odometry,” IEEE Transactions on Robotics, pp. 1–21,
2022.

[24] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,”
IEEE transactions on pattern analysis and machine intelligence, vol. 40,
no. 3, pp. 611–625, 2017.

[25] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “Monoslam:
Real-time single camera slam,” IEEE transactions on pattern analysis
and machine intelligence, vol. 29, no. 6, pp. 1052–1067, 2007.

[26] G. Klein and D. Murray, “Parallel tracking and mapping for small
ar workspaces,” in 2007 6th IEEE and ACM international symposium
on mixed and augmented reality.

IEEE, 2007, pp. 225–234.

[27] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a
versatile and accurate monocular slam system,” IEEE transactions
on robotics, vol. 31, no. 5, pp. 1147–1163, 2015.

[28] R. Mur-Artal and J. D. Tard ´os, “Orb-slam2: An open-source slam
system for monocular, stereo, and rgb-d cameras,” IEEE Transac-
tions on Robotics, vol. 33, no. 5, pp. 1255–1262, 2017.

[29] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile
monocular visual-inertial state estimator,” IEEE Transactions on
Robotics, vol. 34, no. 4, pp. 1004–1020, 2018.

[30] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D.
Tard ´os, “Orb-slam3: An accurate open-source library for visual,
visual–inertial, and multimap slam,” IEEE Transactions on Robotics,
vol. 37, no. 6, pp. 1874–1890, 2021.

[31] B. D. Lucas, T. Kanade, et al., “An iterative image registration
technique with an application to stereo vision.” Vancouver, 1981.
[32] J. Engel, T. Sch ¨ops, and D. Cremers, “Lsd-slam: Large-scale di-
rect monocular slam,” in European conference on computer vision.
Springer, 2014, pp. 834–849.

[33] C. Forster, M. Pizzoli, and D. Scaramuzza, “Svo: Fast semi-direct
monocular visual odometry,” in 2014 IEEE international conference
on robotics and automation (ICRA).

IEEE, 2014, pp. 15–22.

[34] J. Zhang and S. Singh, “Laser–visual–inertial odometry and map-
ping with high robustness and low drift,” Journal of Field Robotics,
vol. 35, no. 8, pp. 1242–1264, 2018.

[35] W. Shao, S. Vijayarangan, C. Li, and G. Kantor, “Stereo visual iner-
tial lidar simultaneous localization and mapping,” arXiv preprint
arXiv:1902.10741, 2019.

[36] T. Laidlow, M. Bloesch, W. Li, and S. Leutenegger, “Dense rgb-d-
inertial slam with map deformations,” in 2017 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS).
IEEE,
2017, pp. 6741–6748.

[37] Y. Zhu, C. Zheng, C. Yuan, X. Huang, and X. Hong, “Camvox:
A low-cost and accurate lidar-assisted visual slam system,” arXiv
preprint arXiv:2011.11357, 2020.

[38] X. Zuo, P. Geneva, W. Lee, Y. Liu, and G. Huang, “Lic-fusion:
Lidar-inertial-camera odometry,” in 2019 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2019,
pp. 5848–5854.

[39] X. Zuo, Y. Yang, J. Lv, Y. Liu, G. Huang, and M. Pollefeys, “Lic-
fusion 2.0: Lidar-inertial-camera odometry with sliding-window
plane-feature tracking,” in IROS 2020, 2020.

[40] T. Shan, B. Englot, C. Ratti, and D. Rus, “Lvi-sam: Tightly-coupled
lidar-visual-inertial odometry via smoothing and mapping,” arXiv
preprint arXiv:2104.10831, 2021.

[41] J. Lin, C. Zheng, W. Xu, and F. Zhang, “R2live: A robust, real-time,
lidar-inertial-visual tightly-coupled state estimator and mapping,”
IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7469–7476,
2021.

[42] J. Lin and F. Zhang, “R3live: A robust, real-time, rgb-colored, lidar-
inertial-visual tightly-coupled state estimation and mapping pack-
age,” in 2022 International Conference on Robotics and Automation
(ICRA), 2022, pp. 10 672–10 678.

[43] D. He, W. Xu, and F. Zhang, “Embedding manifold structures into

kalman ﬁlters,” arXiv preprint arXiv:2102.03804, 2021.

[44] P. Bergmann, R. Wang, and D. Cremers, “Online photometric
calibration of auto exposure video for realtime visual odometry
and slam,” IEEE Robotics and Automation Letters, vol. 3, no. 2, pp.
627–634, 2017.

[45] J. Engel, V. Usenko, and D. Cremers, “A photometrically cali-
brated benchmark for monocular visual odometry,” arXiv preprint
arXiv:1607.02555, 2016.

18

[46] A. Segal, D. Haehnel, and S. Thrun, “Generalized-icp.” in Robotics:
science and systems, vol. 2, no. 4. Seattle, WA, 2009, p. 435.
[47] R. Wang, M. Schworer, and D. Cremers, “Stereo dso: Large-scale
direct sparse visual odometry with stereo cameras,” in Proceedings
of the IEEE International Conference on Computer Vision, 2017, pp.
3903–3911.

[48] T. Qin and S. Shen, “Online temporal calibration for monocular
visual-inertial systems,” in 2018 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 3662–
3669.

[49] N. Carlevaris-Bianco, A. K. Ushani, and R. M. Eustice, “University
of michigan north campus long-term vision and lidar dataset,” The
International Journal of Robotics Research, vol. 35, no. 9, pp. 1023–
1035, 2016.

[50] S. Garrido-Jurado, R. Mu ˜noz-Salinas, F. J. Madrid-Cuevas, and
M. J. Mar´ın-Jim´enez, “Automatic generation and detection of
highly reliable ﬁducial markers under occlusion,” Pattern Recog-
nition, vol. 47, no. 6, pp. 2280–2292, 2014.

[51] Z. Zhang and D. Scaramuzza, “A tutorial on quantitative trajec-
tory evaluation for visual (-inertial) odometry,” in 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 7244–7251.

[52] Y. Zhu, C. Zheng, C. Yuan, X. Huang, and X. Hong, “Camvox: A
low-cost and accurate lidar-assisted visual slam system,” in 2021
IEEE International Conference on Robotics and Automation (ICRA).
IEEE, 2021, pp. 5049–5055.

[53] C. Zheng, Q. Zhu, W. Xu, X. Liu, Q. Guo, and F. Zhang, “Fast-
livo: Fast and tightly-coupled sparse-direct lidar-inertial-visual
odometry,” arXiv preprint arXiv:2203.00893, 2022.

[54] P. Labatut, J.-P. Pons, and R. Keriven, “Efﬁcient multi-view re-
construction of large-scale scenes using interest points, delaunay
triangulation and graph cuts,” in 2007 IEEE 11th international
conference on computer vision.

IEEE, 2007, pp. 1–8.

[55] A. Fabri and S. Pion, “Cgal: The computational geometry
algorithms library,” in Proceedings of the 17th ACM SIGSPATIAL
international conference on advances in geographic information systems,
2009, pp. 538–539. [Online]. Available: https://www.cgal.org
[56] D. Cernea, “OpenMVS: Multi-view stereo reconstruction library,”
2020. [Online]. Available: https://cdcseacave.github.io/openMVS
[57] D. Girardeau-Montaut, “Cloudcompare,” France: EDF R&D
[Online]. Available: https://www.

Telecom ParisTech, 2016.
danielgm.net/cc

[58] P. Cignoni, G. Ranzuglia, M. Callieri, M. Corsini, F. Ganovelli,
N. Pietroni, and M. Tarini, “Meshlab,” 2011. [Online]. Available:
https://www.meshlab.net

[59] Autodesk, “Autodesk 3ds max.” [Online]. Available: https:

//www.autodesk.com/products/3ds-max

[60] Epic Games, “Unreal

engine.” [Online]. Available: https:

//www.unrealengine.com

[61] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity
visual and physical simulation for autonomous vehicles,” in
Field and service robotics.
Springer, 2018, pp. 621–635. [Online].
Available: https://microsoft.github.io/AirSim

Jiarong Lin received a B.S. degree in Optical
Information Science and Technology from the
University of Electronic Science and Technology
of China (UESTC) in 2015. He is currently a
Ph.D. candidate in the Department of Mechan-
ical Engineering, the University of Hong Kong
(HKU), Hong Kong, China.

His research interests include light detection
and ranging (LiDAR) mapping and sensor fusion.

Fu Zhang received the B.E. degree in automa-
tion from the University of Science and Tech-
nology of China (USTC), Hefei, Anhui, China,
in 2011, and the Ph.D. degree in controls from
the University of California at Berkeley, Berkeley,
CA, USA, in 2015.

He joined the Department of Mechanical En-
gineering, The University of Hong Kong (HKU),
Hong Kong, as an Assistant Professor in Au-
gust 2018. His current research interests are on
robotics and controls, with a focus on unmanned
aerial vehicle (UAV) design, navigation, control, and light detection and
ranging (LiDAR)-based simultaneous localization and mapping (SLAM).

Supplementary Material for R3LIVE++: The qualitative results of our reconstructed radiance
map on R3LIVE-dataset

1

Fig. 1 – Sequence “hku campus seq 01” are collected by walking along the drive way of the HKU campus. (a) is the
birdview of the whole radiance map, with its details shown in (b„ d).

Fig. 2 – Sequence “hku campus seq 00/02/03” are sampled at the same place but at different times of day (evening,
noon and morning, respectively) and with different traveling trajectories. (a) is the birdview of map of sequence
“hku campus seq 02”, with the closeup view of details are shown in (b) and (c).

2

Fig. 3 – Sequence “hku park 00” is collected by walking along the pathway of a garden of HKU. (a) is the birdview of
the whole radiance map, with its details shown in (b„ d).

Fig. 4 – Sequence “hku park 01” is collected in a cluttered environment with many trees and bushes. (a) is the birdview
of the whole radiance map, with its details are shown in (b) and (c).

3

Fig. 5 – Sequence “hku campus seq 00/01” are collected within the campus of HKUST with two different traveling
trajectories. In (a), we merge the point cloud of sequence “hku campus seq 00” with the GoogleEarth satellite image and
ﬁnd them aligned well. The details of our reconstructed radiance map are selectively shown in (b„d).

Fig. 6 – Sequence “hku campus seq 02” is collected by exploring the entrance piazza of HKUST, traveling both the
interior and exterior of the buildings. (a) is the birdview of the whole radiance map, with the outdoor and indoor
scenarios selectively shown in (b) and (c), respectively.

4

(
a
)
)

a
n
d
e
n
d
i
n
g
a
t

t
h
e

e
n
t
r
a
n
c
e
p
i
a
z
z
a

(
t
h
e
u
p
p
e
r

r
i
g
h
t
o
f

(
a
)
)
o
f

H
K
U
S
T

.

I
n
(
a
)
,

w
e
m
e
r
g
e
o
u
r

r
e
c
o
n
s
t
r
u
c
t
e
d
p
o
i

n
t

c
l
o
u
d
m
a
p
(
p
o
i

n
t
s

a
r
e

c
o
l
o
r
e
d
b
y
t
h
e
i
r
h
e
i
g
h
)

w

i
t
h
t
h
e
G
o
o
g
l
e
E
a
r
t
h
s
a
t
e
l
l
i
t
e

i

m
a
g
e

a
n
d

F
i
g

.

7

–

S
e
q
u
e
n
c
e

“
h
k
u
s
t

c
a
m
p
u
s

s
e
q

0
3
”

c
a
p
t
u
r
e
s

m
o
s
t
p
a
r
t

o
f

t
h
e
H
K
U
S
T
’
s

c
a
m
p
u
s
,

w

i
t
h

t
h
e

t
r
a
v
e
l
i

n
g

l
e
n
g
t
h

r
e
a
c
h

i

n
g

2
.
1
k
m

.

W

e

c
o
l
l
e
c
t
e
d
t
h
e
d
a
t
a

s
t
a
r
t
i
n
g

f
r
o
m

t
h
e

s
e
a

f
r
o
n
t

(
s
e
e

t
h
e

l
o
w
e
r

l
e
f
t

o
f

Y
o
u
T
u
b
e
:

y
o
u
t
u
.
b
e
/
q
X
r
n
I
f
n
-
7
y
A
?
t
=
2
6
1

.

ﬁ
n
d
t
h
e
m
a
l
i
g
n
e
d
w
e
l
l
.

(
b
)

s
h
o
w
s

t
h
e

s
i
d
e

v
i
e
w
o
f

t
h
e
m
a
p

.

(
c
„
h
)

a
r
e

t
h
e

c
l
o
s
e
u
p
v
i
e
w
s

o
f

t
h
e
d
e
t
a
i
l
s

m
a
r
k
e
d

i

n

(
a
)
.

T
o

s
e
e

t
h
e

r
e
a
l
-
t
i

m
e

r
e
c
o
n
s
t
r
u
c
t
i
o
n
p
r
o
c
e
s
s

o
f

t
h
e
m
a
p

,

p
l
e
a
s
e

r
e
f
e
r

t
o

t
h
e

v
i
d
e
o

o
n

