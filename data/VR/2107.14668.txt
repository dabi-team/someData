1
2
0
2

l
u
J

0
3

]

O
L
.
s
c
[

1
v
8
6
6
4
1
.
7
0
1
2
:
v
i
X
r
a

Interleaving & Reconﬁgurable Interaction: Separating Choice
from Scheduling using Glue⋆

Yehia Abd Alrahman1, Mauricio Martel1, and Nir Piterman1

University of Gothenburg, Gothenburg, Sweden
{yehia.abd.alrahman,nir.piterman,mauricio.martel}@gu.se

Abstract. Reconﬁgurable interaction induces another dimension of nondeterminism in concurrent
systems which makes it hard to reason about the diﬀerent choices of the system from a global
perspective. Namely, (1) choices that correspond to concurrent execution of independent events;
and (2) forced interleaving (or scheduling) due to reconﬁguration. Unlike linear order semantics
of computations, partial order semantics recovers information about the interdependence among
the diﬀerent events for ﬁxed interaction, but still is unable to handle reconﬁguration. We introduce
glued partial orders as a way to capture reconﬁguration. Much like partial orders capture all possible
choices for ﬁxed systems, glued partial orders capture all possible choices alongside reconﬁguration.
We show that a glued partial order is suﬃcient to correctly capture all partial order computations
that diﬀer in forced interleaving due to reconﬁguration. Furthermore, we show that computations
belonging to diﬀerent glued partial orders are only diﬀerent due to non-determinism.

1

Introduction

Reconﬁgurable concurrent systems [3,5,4] are a class of computational systems, consisting of a collection
of processes (or agents) that interact and exchange information in nontrivial ways. Agents interact using
message-passing [22] (or token-passing [12]) and based on dynamic notions of connectivity where agents
may only observe, inhibit or participate in interactions happening on links they are connected to. Agents
may get connected or disconnected to links as side-eﬀects of the interaction, and thus providing dynamic
and sophisticated scoping mechanisms of interaction through reconﬁgurable interfaces.

Reconﬁguration induces another dimension of nondeterminism in concurrent systems where it be-
comes hard to reason about the diﬀerent choices of the system from a global perspective. It creates a
situation where some events must be ordered with respect to sequences of other events dynamically dur-
ing execution, and thus forcing interleaving in a non-trivial way. That is, from the point of an event, a
sequence of other events is considered as a single block and can only happen before or after it. Note that
reconﬁguration is an internal event, and is totally hidden from the perspective of an external observer [10]
who may only observe message-/token- passing. Indeed, messages or tokens can only indicate the occur-
rence of exchange but cannot help with noticing that a reconﬁguration has happened and what are the
consequences of reconﬁguration. Knowing the reason why some event is scheduled before some others and
the causal dependencies among the diﬀerent events is crucial to facilitate reasoning about speciﬁc internal
aspects from a global perspective [9]. It also becomes very relevant when applying correct-by-construction
techniques [26] to synthesise such systems.

Clearly, linear order semantics of computations [7,27] cannot be used to globally distinguish a system
choice due to concurrent execution of independent events and a forced interleaving due to reconﬁguration.
It cannot be even used to recover information about the participants of an event and the interdepen-
dence of the diﬀerent events. Therefore, a partial order semantics of computations is in-order. Existing
approaches to partial order semantics (cf. Process semantics of Petri nets [24,21,27] and Mazurkiewicz
traces of Zielonka automata [29,17,20]) proved useful in recovering information about the participants
of events and independence of concurrent events. For instance, in the Process semantics of Petri nets,
two concurrent events can be executed in any order or even simultaneously, and thus we can distinguish
concurrent execution from mere nondeterminism. However, these formalisms have ﬁxed interaction struc-
tures that deﬁne interdependence of events in a static way, and thus leads to a straightforward partial
order semantics. Indeed, while the interdependence of events is statically deﬁned based on the structure
of a Petri net, it is also deﬁned based on the domains of events of Zielonka automata which are ﬁxed in
advance.

⋆

This work is funded by the ERC consolidator grant D-SynMA (No. 772459) and the Swedish research council
grants: SynTM (No. 2020-03401) and VR project (No. 2020-04963).

 
 
 
 
 
 
2

Y. Abd Alrahman et al.

In this paper, we propose a partial order semantics of computations under reconﬁguration. In such
settings, dependencies among events emerge dynamically as side-eﬀects of interaction, and thus we handle
these emergencies while ensuring that the semantics deﬁnes the actual behaviour of the system. Our ap-
proach consists of characterising reconﬁguration points and their corresponding scheduling decisions in a
single structure, while preserving a true-concurrent execution of independent events. Our semantics allows
reasoning about the individual behaviour of agents composing the system and their interaction informa-
tion. We test our results on Petri net with inhibitor arcs (PTI-nets) [16,12] and Channeled Transition
Systems (CTS) [6,5]. These modelling frameworks cover a wide range of interaction capabilities alongside
reconﬁguration from two diﬀerent schools of concurrency. In fact, inhibitor arcs add a restricted form
of reconﬁguration to Petri nets while CTS can be considered as a generalisation of Zielonka automata,
supporting rich interactions alongside reconﬁguration.
Contributions. We deﬁne specialised partial orders, that we call labelled partial orders (LPO for short),
to represent computations. An LPO is a representation of a speciﬁc computation. That is, given a system
consisting of a set of agents, we can construct an LPO by only considering the local views of individual
agents and their interaction information. An LPO deﬁnes how the individual computations of agents
are related, and also how diﬀerent events are related. In the spirit of Mazurkiewicz traces, the states of
diﬀerent agents are (strictly) incomparable, that is there is no notion of a global state. This way we can
easily single out ﬁnite sequences of computation steps where an agent or a (small) group of agents execute
independently. We can also distinguish individual events from joint ones. Despite the fact that an LPO
may refer to reconﬁguration points, it cannot fully characterise reconﬁguration in a single structure. For
this reason, we introduce glued labeled partial orders (g-LPO, for short), that is an extension of LPO with
glue to separate a non-deterministic choice from forced scheduling due to reconﬁguration. Intuitively, two
elements are glued from the point of view of another element if they both happen either before or after
said element. We show that a g-LPO is suﬃcient to represent LPO computations that diﬀer in scheduling
due to reconﬁguration. We also show that LPO computations belonging to diﬀerent g-LPO(s) are diﬀerent
due to nondeterministic selection of independent events.

The paper is organised as follows: In Sect. 2, we informally present our partial order semantics and
in Sect. 3, we introduce the necessary background. In Sect. 4, we provide LPO semantics for PTI-nets
and CTSs. In Sect. 5 we deﬁne glued partial orders and the corresponding extension to both PTI-nets
and CTSs. We show, for both, that every LPO computation is only a reﬁnement of some g-LPO of
the same system. In Sect. 6 we prove important results on g-LPO with respect to reconﬁguration and
nondeterminism. In Sect. 7 we present concluding remarks, related works, and future directions. All proofs
are included in the appendix.

2 Labelled Partial Order Computations in a Nutshell

In this section, we use a fragment of a PTI-net to informally illustrate the LPO semantics under recon-
ﬁguration and the idea behind g-LPO.

We consider the PTI-net in Fig. 1(a), where we interpret reconﬁguration and concurrency in the
following way: each token represents an individual agent and the structure of the net deﬁnes the combined
behaviour. The places of the net, denoted by circles, deﬁne the states of the diﬀerent agents during
execution. The transitions, denoted by squares, can either refer to synchronisation points (e.g., t1 and t2)
or individual computation steps (e.g., t3 and t4).

Arrows deﬁne which places require to have tokens to enable a transition and the places to put tokens
after ﬁring. In our examples all arrows consume/produce one token. For instance, transition t1 may ﬁre
when there is at least one token in both p1 and p2. Transition ﬁring induces removal of tokens from input
places and addition of tokens in output places. Thus, when t1 ﬁres, one token is removed from p1 and one
from p2 and one token is placed in p3 and p4, each. Sometimes a place can choose nondeterministically
which transition to participate in (e.g., p4 chooses t2 or t3). A place can inhibit the ﬁring of some transition
(e.g., p3 inhibiting t4) using an inhibitor arc (p3 ⊸ t4). While the place contains a token it inhibits the
transition. We interpret this as the agent represented by the token (e.g., in p3) starting to listen to the
transition (t4), but it cannot participate, and thus it inhibits its execution. In our example, in p1 the
agent is not listening to t4, but once t1 is executed the agent reconﬁgures its interaction interface and
starts listening. This means that t4 may only ﬁre either before a token is placed in p3 or after the token is
removed. Clearly, this can only happen when t4 either happens before t1 or after t2. Thus from the point
of view of t4 both t1 and t2 are considered as a single block, and their execution cannot be interrupted.
Namely, the only viable sequences of execution (in case t2 is scheduled later) are t4, t1, t2 or t1, t2, t4.
Note that this is only from the point of view of t4 and has no implications for other transitions. Indeed,

Separating Choice from Scheduling using Glue

3

p1

p2

p1

p2

p1

p2

p7

t1

p7

t1

p7

t1

t4

p3

p4

t4

p3

p4

t4

p3

p4

p8

t2

t3

p8

t2

p8

p5

p6

p5

p6

t3

p6

(a) Perti net with Inhibitor arcs

(b) Two possible g-LPO computations

Fig. 1. Petri net with inhibitor arcs

other transitions can have a diﬀerent point of view (e.g., t3). This creates a forced interleaving in a non-
trivial way due to the occurrence of non-observable events (i.e., reconﬁguration) that we cannot reason
about from a global perspective. Furthermore, these dependencies among events emerge dynamically as
side-eﬀects of interaction, and thus put the correctness of partial order semantics at stake.

To handle this issue, we introduce a partial order semantics of computations under reconﬁguration. We
handle the above mentioned emergences by characterising reconﬁguration points and their corresponding
scheduling decisions in a single structure, while preserving a true-concurrent execution of independent
events. Our semantics allows reasoning about the individual behaviour of agents composing the system
and their interaction information.

We illustrate our LPO and g-LPO semantics in Fig. 1(b), which characterises all possible (maximal)

computations of the net. Here, we use the arrow → to indicate a happen before relation.

The two ﬁgures succinctly encode three possible LPOs: (i) the LPO obtained from Fig. 1(b) left
structure with the dashed arrow from t4 to t1; (ii) the LPO obtained from Fig. 1(b) left structure with
the dashed arrow from t2 to t4; and (iii) the LPO obtained from Fig. 1(b) right structure with the dashed
arrow from t4 to t1. LPOs (i) and (ii) agree that the token in p4 nondeterministically chooses the transition
t2 while in (iii) the nondeterministic choice is t3. All LPOs capture information about interaction and
interdependence among events. Indeed, in all cases we see that both p1 and p2 synchronize through the
transition t1. Places that are not strictly ordered with respect to a common transition are considered
concurrent. Thus, as in Mazurkiewicz traces there is no notion of a global state. Notice that LPOs (i) and
(ii) diﬀer only in the forced interleaving of t4 with respect to the block t1, t2.

Notice that both LPOs (i) and (ii) have information both on reconﬁguration and nondeterminism,
but each individually cannot be used to distinguish the hidden reconﬁguration. In fact, t4 → t1 in (i)
indicates that t4 happened before a reconﬁguration caused by t1, and t2 → t4 in (ii) indicates that t4
happened after the reconﬁguration. In (iii), due to the diﬀerent nondeterminsitic choice, the only possible
case we have to consider is that of t4 happening before t1.

This suggests that we can actually isolate reconﬁguration from nondeterminism by using a more
sophisticated structure than LPO, and thus expose the diﬀerence in a way that allows reasoning about
these hidden events from a global perspective. For this reason, we deﬁne g-LPO computations, that are
an extension of LPO with a notion of glue.

For PTI-nets like ﬁxed systems, a g-LPO simply drops strict ordering of events with respect to each
other (like t4 → t1 or t2 → t4), and instead assigns each event a (possibly empty) glue relation deﬁning the
glued elements from the point of view of that event. The glue relation is deﬁned based on reconﬁguration
points, and in case of Petri nets is based on inhibitor arcs. We will see later how this is deﬁned in a more
dynamic and compositional model like CTS, where structural information does not simply exist. There,
the g-LPO has to account also to event-to-event ordering when sharing the same communication channel.
Consider now the structures in Fig. 1(b) without the dashed arrows and, now, with an explanation
of the red arrows. These two structures are each a g-LPO. For the one on the left, since p3 inhibits t4
all existing incoming and outgoing edges from p3 are glued to p3. Thus, t4’s glue relation includes these
edges (in red). All other transitions have empty glue relations because they are not inhibited. As they are
not inhibited, their interdependence is well-captured statically based on the structure of the net. Note
that the glue relation is not required to be transitive and the glue only relates places and transitions. In

4

Y. Abd Alrahman et al.

the structure on the right of the ﬁgure, t1 is glued only to p4. As t3 is scheduled rather than t2, then p3
remains as a maximal element.

As we show later, a single g-LPO can be used to characterise reconﬁguration and separate it from

other sources of nondeterminism in the system.

3 Preliminaries: Labeled Partial Orders

We use partial orders to represent computations. We specialize notations to match our needs.

A partial order (PO, for short) is a binary relation ≤ over a set O that is reﬂexive, antisymmetric,
and transitive. We use a < b for a ≤ b and a 6= b. We use a#b for a 6≤ b and b 6≤ a, i.e., a and b are
incomparable.

A labelled partial order (LPO, for short) is (O, →c, →i, Σ, Υ, L), where O = V

E is a set of elements
partitioned to nodes and edges, respectively, →c and →i are disjoint, anti-reﬂexive, anti-symmetric, and
non-transitive communication and interleaving order relations over O. We have →c⊆ V × E ∪ E × V and
→i⊆ E × E. When →i= ∅ we omit it from the tuple. The relation ≤ is the reﬂexive and transitive closure
of the union of →c and →i. We require that ≤ is a partial order. Moreover, Σ is a node alphabet, Υ is
an edge alphabet, and L : O → Σ ∪ Υ such that L(V ) ⊆ Σ and L(E) ⊆ Υ is the labelling function.

U

Intuitively, elements in V can denote states or execution histories of individual agents and elements in
E denote transitions or events. Thus, a history belongs to an individual agent and a transition corresponds
to either an individual computational step or a synchronisation point among multiple agents. The relation
→c captures participation in communication and the relation →i captures order requirements.

We denote →=→c ∪ →i. Given an element a ∈ O we write •a for {b | b → a} and a• for {b | a → b}.

4 LPO Semantics

In this section, we present Petri Nets with inhibitor arcs [16,12] and Channeled Transition Systems [6,5]
and we provide each with a labelled partial order semantics. The labelled partial order semantics of Petri
nets extends occurrence nets [21] with event-to-event connections that allow to capture reconﬁgurations.
We include in appendix the labelled partial order semantics of asynchronous automata, which do not
require the relation →i, and, thus, show that the separation of results in this paper only make sense in
reconﬁgurable systems.

4.1 Petri Nets with Inhibitor Arcs (PTI-nets)

A Petri net N with inhibitor arcs is a bipartite directed graph N = hP, T, F, Ii, where P and T are the
set of places and transitions such that P ∩ T = ∅, F : (P × T ) ∪ (T × P ) → N is the ﬂow relation, and
I ⊆ (P × T ) is the inhibiting relation. We write (s, s′) ∈ F for F (s, s′) > 0. We restrict attention to Petri
nets where all transitions have a non-empty preset.

The conﬁguration of a Petri net at a time instant is deﬁned by means of a marking. Formally, let N
be a Petri net with a set of places P = {p1, . . . , pk}. A marking is a function m : P → N and is deﬁned
as a vector m = m[1], . . . , m[k] where m[i] corresponds to the number of tokens in pi, for i = 1, . . . , k.
Vectors can be added, subtracted, and compared in the usual way. We assume some initial marking m0.
For p ∈ P let ~p be the singleton vector ~p : P → {0, 1} such that ~p(p) = 1 and ~p(p′) = 0 for every p′ 6= p.
For a transition t ∈ T we deﬁne the pre-vector of t, denoted by •t, to represent the vector •t[1], . . . , •t[k],

where •t[i] = F (pi, t). Similarly, the post-vector of t is t• = t•[1], . . . , t•[k], where t•[i] = F (t, pi).

An inhibitor arc from a place to a transition means that the transition can only ﬁre if no token is on
that place. The inhibitor set of a transition t is the set ◦t = {p ∈ P | (p, t) ∈ I}, and represents the places
to be “tested for absence” of tokens. That is, an inhibiting place allows to prevent the transition ﬁring.
A transition t is enabled at m if for every p ∈ •t we have m(p) ≥ F (p, t) and all inhibitor places are
empty, i.e., for every p ∈ ◦t we have m(p) = 0. Note that if for some t and p ∈ ◦t we have (p, t) ∈ F then
t can never ﬁre, thus it is called blocked.

A transition t enabled at marking m can ﬁre and produce a new marking m′ such that m′ = m−•t+t•,
denoted m|tim′. That is, for every place p ∈ P , the ﬁring transition t consumes F (p, t) tokens and produces
F (t, p) tokens.

Deﬁnition 1 (History). We deﬁne the set of histories of a net N by induction.

We deﬁne a special transition tǫ such that tǫ

• = m0. The pair (∅, tǫ) is a t-history. Note that tǫ is not

a transition in T .

Separating Choice from Scheduling using Glue

5

For a place p, let h = (S, t) be a t-history such that t•(p) > 0. Then we have (h, p, t•(p)) is a p-history.
That is, given a t-history h ending in transition t, where p is in t•, then the combination of h, p, and the
number of tokes that t puts in p form a p-history.

Consider a transition t ∈ T . A t-history is a pair (S, t), where S = {(h1, i1), . . . , (hn, in)} is a multiset
satisfying the following. For every j we have hj = (−, p, cj) is a p-history, where cj ≥ ij and •t =
Pj ij · ~pj.
That is, the t-history identiﬁes the set of p-histories from which t takes tokens with the multiplicity of
tokens taken from every p-history.

Let hist(N ) be the set of all histories of N partitioned to histp(N ) and histt(N ) in the obvious way.
Given a t-history h = (S, t) and a p-history h′ we write h(h′) for the number of appearances of h′ in the
multiset S.

Now, everything is in place to deﬁne the labelled partial order semantics of a PTI-net.

Deﬁnition 2 (LPO-computation). A computation of N is an LPO (O, →c, →i, Σ, Υ, L), where V ⊆
histp(N ), E ⊆ histt(N ), Σ = P , Υ = T , for a p-history v = (−, p, i) we have L(v) = p and for a
t-history (S, t) we have L(e) = t, and such that:

N1. The t-history (∅, tǫ) is the unique minimal element according to ≤.
N2. For a p-history v = (e, p, i) ∈ V we have e ∈ E and e is the unique edge such that e →c v.
N3. For a p-history v = (h, p, i) ∈ V , let e1, . . . , ej be the t-histories such that v →c ej. Then, for every j
Pj ej(v) ≤ i. That is, v leads to t-histories that contain it with the multiplicity

we have ej(v) > 0 and
of v being respected.

N4. For every e ∈ E, where e = ({(v1, i1), . . . , (vn, in)}, t), all the following hold:

(a) •e ∩ V = {v1, . . . , vn} and e• ∩ V = {(e, p, t•(p)) | t•(p) > 0}.
(b) For every v ∈ V such that L(v) ∈ ◦L(e) we have e ≤ v or v ≤ e.
(c) If e →i e′ then there is some v such that either (i) v →c e and (L(v), L(e′)) ∈ I or (ii) e′ →c v

and (L(v), L(e)) ∈ I.

That is, a computation starts from the dummy transition tǫ, which establishes the initial marking.
Every other transition is a t-history that connects the p-histories that it contains. If a place inhibits a
transition then either the transition happens before a token arrives to the place or after the token left
that place. This is possible by adding direct interleaving dependencies (→i) between edges. Namely, if p
inhibits t then either t happens before the transition putting token in p or after the transition taking the
token from p.

4.2 Channelled Transition Systems (CTS)

A Channelled Transition System (CTS) is a tuple of the form T = hC, Λ, B, S, S0, R, L, lsi, where C is a
set of channels, including the broadcast channel (⋆), Λ is a state alphabet, B is a transition alphabet, S is
a set of states, s0 ∈ S is an initial state, R ⊆ S × B × S is a transition relation, L : S → Λ is a labelling
function, and ls : S → 2C is a channel-listening function such that for every s ∈ S we have ⋆ ∈ ls(s).
That is, a CTS is listening to the broadcast channel in every state. We assume that B = B+ × {!, ?} × C,
for some set B+. That is, every transition labeled with some b ∈ B is either a message send (!) or a
message receive (?) on some channel c ∈ C.

Given (b+, !, c) ∈ B we write ?(b+, !, c) for (b+, ?, c) and ch(b+, −, c) for c. That is, ?(b) is the corre-

sponding receive transition of a send transition b and ch(b) is the channel of b.

For a receive transition b = (b+, ?, c) and a state s ∈ S we write s →b if c ∈ ls(s) and there is some
s′ such that (s, b, s′) ∈ R. That is, s is listening on channel c and can participate, i.e., has an outgoing
receive transition for b. We write s 6→b if c ∈ ls(s) and it is not the case that s →b. That is, s is listening
on channel c and is not able to participate.

A history h = s0, . . . , sn is a ﬁnite sequence of states such that s0 ∈ S0 and for every 0 ≤ i < n
we have that (si, bi, si+1) ∈ R for some bi ∈ B. The length of h is n + 1, denoted |h|. For convenience
we generalise notations applying to states to apply to histories. For example, we write c ∈ ls(h) when
c ∈ ls(sn), h →b when sn →b and h 6→b for sn 6→b. Similarly, if h = s0, . . . , sn and h′ = s0, . . . , sn, sn+1
where (sn, bn, si+1) ∈ R, we write (h, bn, h′) ∈ R. Let hist(T ) be the set of all histories of T . An execution
π = s0, b0, s1 . . . is an inﬁnite sequence such that for every i ≥ 0 we have (si, bi, si+1) ∈ R and bi ∈ B.
Thus, every preﬁx of π (projected on states) is a history.

The linear semantics for CTS is given by a parallel composition operator over a set of CTSs. We
include the full deﬁnition in appendix and refer the reader to [6]. Intuitively, multicast channels are
blocking. All agents who are listening to the channel must be able to participate in the communication

6

Y. Abd Alrahman et al.

1 {⋆}

1 {⋆, d}

(v2, !, d)

(v2, ?, d)

1 {⋆}

2 {⋆, c}

2 {⋆, e}

(v1, !, c)

(v3, !, e)

(v3, ?, e)

(v4, !, b)

2 {⋆}

3 {⋆}

3 {⋆}

4 {⋆}

(a) Agent T1

(b) Agent T2

(c) Agent T3

Fig. 2. CTS representation of the running example.

in order for a send to be possible. The broadcast channel, on the other hand, is non-blocking. Agents
always listen to the broadcast channel. However, if they cannot participate in a communication it still
goes on without them.

The PTI-net in Fig. 1(a) can be modelled as the parallel composition of the CTSs in Fig. 2, where
we label states with the listening function. Starting from the initial states, we have that either (v1, !, c)
or (v2, !, d) can be sent. The former is an individual transition of agent T1 while the latter is a joint
transition between T2 and T3 where T2 sends and T3 receives. Note that T3 is initially connected to
channel d. If (v2, !, d) is scheduled ﬁrst then the listening function of both T2 and T3 is reconﬁgured where
T2 starts listening to channel c and T3 starts listening to e. This way, (v1, !, c) is blocked until (v3, !, e) is
sent. It is not hard to see that a reconﬁguration due to changes in the listening function is equivalent to
token passing. However, here we can model a more interesting compositional interactions with meaningful
message exchange.

Now, everything is in place to deﬁne the labelled partial order semantics of a CTS. Consider a system

S = T1 k · · · k Tn, where Ti = hCi, Λi, Bi, Si, Si

0, Ri, Li, lsii. We denote C =

Si Ci, and B =

Si Bi.

Deﬁnition 3 (LPO-computation). A computation of S is an LPO (O, →c, →i, Σ, Υ, L), where V ⊆
Si hist(Ti), Σ = V , →c=→s
→r is the disjoint union of the send and receive relations, Υ = {(υ, !, c) ∈
B}, and for h ∈ V we have L(h) = h. In addition we require the following:

U

C1. The edge eǫ such that L(eǫ) = (b, !, ⋆) is the unique minimal element according to ≤. For every i, we

have s0

i ∈ V and eǫ →r s0
i .

C2. If h ∈ V ∩ hist(Ti) there is a unique e ∈ E such that e →c h. If |h| > 1, there is also a unique h′ ∈ V

such that h′ →c e and either (h′, L(e), h) ∈ Ri or (h′, ?(L(e)), h) ∈ Ri.

C3. For every h ∈ V there is at most one e ∈ E such that h →c e.
C4. For every e ∈ E \ {eǫ} there is I ⊆ [n] such that all the following hold:
(a) For every i ∈ I we have |•e ∩ hist(Ti)| = 1 and |e• ∩ hist(Ti)| = 1.
(b) There is a unique i ∈ I and h, h′ ∈ V ∩ hist(Ti) such that (h, L(e), h′) ∈ Ri and h →s e →s h′
and for every i′ ∈ I \ {i} there are h′′, h′′′ ∈ V ∩ hist(Ti′ ) such that h′′ →r e →r h′′′ and
(h′′, ?(L(e), h′′′) ∈ Ri′ .

(c) If L(e) = (υ, !, c) for c 6= ⋆ then for every h ∈ V such that c ∈ ls(h) we have h ≤ e or e ≤ h.
(d) If L(e) = (υ, !, ⋆) then for every h ∈ V such that h →?(L(e)) we have h ≤ e or e ≤ h.

C5. For every e 6= e′ such that ch(e) = ch(e′) we have e ≤ e′ or e′ ≤ e.
C6. If e →i e′ then there is some h = s0, . . . , sj such that one of the following holds:

(a) ch(e) = ch(e′).
(b) L(e′) = (υ, !, c) for c 6= ⋆, h →c e and ch(L(e′)) ∈ ls(h).
(c) L(e) = (υ, !, c) for c 6= ⋆, e′ →c h and ch(L(e′)) ∈ ls(h).
(d) L(e′) = (υ, !, ⋆), h →c e and h →?(L(e′)).
(e) L(e) = (υ, !, ⋆), e′ →c h and h →?(L(e)).

Note that an LPO computation relates histories of individual CTSs, and thus allows to draw relations
among ﬁnite sequences of individual computation steps of one CTS (or a group of CTSs) with respect to
others; Furthermore, a CTS is always listening to the broadcast channel, and thus, it becomes mandatory
to order broadcast messages that enable/disable participation to each other.

More precisely, C1 ensures that a unique broadcast initiates all the initial states of Ti for all i and that
nothing happens before that. As expected, C2 and C3 ensure that an LPO deﬁnes a unique resolution
of a nondeterministic choice in every single step. Moreover, C4 models interactions, where (a) and (b)
model synchronisation while (c)-(f) model ordering due to schedule imposed by using global resources and

Separating Choice from Scheduling using Glue

7

restrictions due to reconﬁguration. First, communications on the same channel must be ordered. Then,
a multicast must be ordered with respect to every individual history that listens to it. Furthermore, a
broadcast must be ordered with respect to every individual history that can participate in it. Clearly,
the last two requirements are crucial to preserve the blocking semantics of multicasts and the input
enabledness of broadcasts.

Thus, for a multicast, if a history h blocks the multicast execution then either h can be extended
so that the multicast is released or the multicast happens directly before h is reached. We solve this by
adding a strict ordering between multicasts. The same holds for a broadcast, but in this case we handle
input enabledness of broadcast rather.

We will use comp(S) for S being a Petri net or CTS, to denote the set of LPO computations of S.

5 Partial Order with Glue

In this section we extend labeled partial orders with glue. Intuitively, two elements are glued from the
point of view of another element if they both happen either before or after said element.

Deﬁnition 4 (Glue relation). A Glue over a set O and a relation →c⊆ O × O is a relation R ⊆→c.

Intuitively, a glue relation R over the set O and a relation →c deﬁnes pairs of elements that are glued

together.
Deﬁnition 5 (Glued LPO). A glued labeled partial order (g-LPO, for short) is lpg = (P, G, E), where
E, →c, →i, Σ, Υ, L) is an LPO, G = {G1, . . . , Gk} is a set of Glue relations over O and
P = (O = V
→c, and E : Υ ֒→ G labels elements in E (through their edge labels) by glue relations.
Deﬁnition 6 (g-LPO-reﬁnement). An LPO lpo = (O, →c, →i, Σ, Υ, L) where O = V
g-LPO lpg = (Pg, G, E), denoted lpo (cid:22) lpg, where Pg = (O, →c, →g
hold:

E reﬁnes a
i Σ, Υ, L) if the following conditions

U

U

i ) implies (e′, v) ∈ E(L(e)) for some v or (v, e) ∈ E(L(e′)) for some

i ⊆ →i and (e, e′) ∈ (→i \ →g

– For every e ∈ E and (a, b) ∈ E(L(e)) we have e ≤ a or b ≤ e.
– →g
v.
That is, the two share the relation →c, the relation →g

i is preserved and extended by extra interleaving
to capture the glue. In order to respect the glue, an edge that is glued to a pair (a, b) must happen either
before a or after b.

We show now that g-LPOs enable to remove parts of the interleaving order relation for both PTI-nets
and CTSs. g-LPOs capture better reconﬁguration by combining multiple order choices due to the same
reconﬁguration into the same g-computation.

5.1 Glue Computations for PTI-nets

Let N = hP, T, F, Ii be a PTI-net and m0 its initial marking. We now deﬁne a g-computation. The
diﬀerences from the deﬁnition of LPO (Deﬁnition 2) are highlighted with a “∗”.
Deﬁnition 7 (g-computation). A g-computation of N is a g-LPO (P, G, E), where P = (O, →c
, Σ, Υ, L), the components V , E, Σ, Υ , and L are as for LPO, and the following holds.
∗N1. The t-history (∅, tǫ) is the unique minimal element according to ≤.
∗N2. For a p-history v = (e, p, i) ∈ V we have e ∈ E and e is the unique edge such that e →c v.
∗N3. For a p-history v = (h, p, i) ∈ V , let e1, . . . , ej be the t-histories such that v →c ej. Then, for every j
Pj ej(v) ≤ i. That is, v leads to t-histories that contain it with the multiplicity

we have ej(v) > 0 and
of v being respected.

∗N4. For every e ∈ E, where e = ({(v1, i1), . . . , (vn, in)}, t) the following holds:

∗(a) •e = {v1, . . . , vn} and e• = {(e, p, t•(p)) | t•(p) > 0}.

∗N5. For every t ∈ T we have:

E(t) = {(v, e) | v →c e and (L(v), t) ∈ I} ∪

{(e, v) | e →c v and (L(v), t) ∈ I}

That is, we drop →i and assign each inhibited event (or transition) with a glue relation. Namely, for

every transition t add all existing ingoing and outgoing transitions of places that inhibit t.

We use compg(N ) to denote the set of g-computations of Petri net N .
Theorem 1. Given a PTI-net N , comp(N ) = {π | π (cid:22) πg ∧ πg ∈ compg(N )}.

8

Y. Abd Alrahman et al.

5.2 Glue Computations for CTSs

Consider a system S = T1 k · · · k Tn, where Ti = hCi, Λi, Bi, Si, Si
B =

Si Ci and
Si Bi.
We now deﬁne a g-computation for CTS. As before, the diﬀerences from the deﬁnition of LPO (Deﬁ-

0, Ri, Li, lsii. We denote C =

nition 3) are highlighted with a “∗”.

Deﬁnition 8 (g-computation). A g-computation of S is a g-LPO (P, G, E), where P = (O, →i, →c
, Σ, Υ, LV , LE) and V , E, Σ, Υ , and L are as before, →c=→s

→r, and in addition:

U

∗C1. The edge eǫ such that L(eǫ) = (b, !, ⋆) is the unique minimal element according to ≤. For every i,

we have s0

i ∈ V and eǫ →r s0
i .

∗C2. If h ∈ V ∩ hist(Ti) there is a unique e ∈ E such that e →c h. If |h| > 1, there is also a unique

h′ ∈ V such that h′ →c e and either (h′, L(e), h) ∈ Ri or (h′, ?(L(e)), h) ∈ Ri.

∗C3. For every h ∈ V there is at most one e ∈ E such that h →c e.
∗C4. For every e ∈ E \ {eǫ} there is I ⊆ [n] such that all the following hold:
(a) For every i ∈ I we have |•e ∩ hist(Ti)| = 1 and |e• ∩ hist(Ti)| = 1.
(b) There is a unique i ∈ I and h, h′ ∈ V ∩ hist(Ti) such that (h, L(e), h′) ∈ Ri and h →s e →s h′
and for every i′ ∈ I \ {i} there are h′′, h′′′ ∈ V ∩ hist(Ti′ ) such that h′′ →r e →r h′′′ and
(h′′, ?(L(e), h′′′) ∈ Ri′ .

∗C5. For every e 6= e′ such that ch(e) = ch(e′) we have e ≤ e′ or e′ ≤ e.
∗C6. If e →i e′ then the following holds:

(a) ch(e) = ch(e′).

∗C7. For every (υ, !, c) ∈ B then

E((υ, !, c)) = {(h, e) | for c 6= ⋆, h →c e and c ∈ ls(h)} ∪
{(e, h) | for c 6= ⋆, e →c h and c ∈ ls(h)} ∪
{(h, e) | for c = ⋆, h →c e and h →?(υ,!,c)} ∪
{(e, h) | for c = ⋆, e →c h and h →?(υ,!,c)}

We drop from the interleaving relation all order relations that correspond to reconﬁguration and keep
only those that correspond to the usage of a common resource. Furthermore, we assign each broadcast
and multicast message with a glue relation. Namely, for every multicast m add all existing ingoing and
outgoing messages of histories that blocks m execution; for every broadcast b add all existing ingoing
and outgoing messages of histories that may participate in m. Note that, for the case of broadcast, the
rationale is that if such histories can participate in a broadcast then they cannot be enabled independently
from the broadcast. Notice that ∗C6 adds one glue for every multicast channel but one for every broadcast
message.

We use compg(S) to denote the set of g-computations of CTS S.

Theorem 2. Given a CTS T , comp(T ) = {π | π (cid:22) πg ∧ πg ∈ compg(T )}.

6 Separating Choice and Reconﬁguration-Forced Interleaving

We show that g-LPOs capture the diﬀerences between nondeterministic choice, which corresponds to
diﬀerent g-LPOs, and interleaving choices due to reconﬁguration, which correspond to diﬀerent ways to
refer to glue. For both PTI-nets and CTS we show that distinct g-LPOs contain diﬀerent nondeterministic
or order choices.

6.1 Choice vs Interleaving in PTI-nets

A choice is a situation where a set of tokens have exactly the same history and they do a diﬀerent
exchange.

We show that every two distinct g-computations of the same net have a set of tokens that “see the
diﬀerence”. That is, they participate in a diﬀerent transition in the two g-computations. This includes the
option of tokens in one g-computation participating in a transition and tokens in the other g-computation
not continuing.

Theorem 3. Given a Petri net P and two diﬀerent g-LPOs G1, G2 ∈ compg(N ) then there exists a set
of nodes v1, . . . , vn appearing in both G1 and in G2 such that one of the following holds:

1. There is a node vi such that the number of tokens not taken from vi in G1 and G2 is diﬀerent.
2. There is a set of p-histories v1, . . . , vn that participate in some transition t in Gi but not in G3−i.

Separating Choice from Scheduling using Glue

9

Notice that item 2 includes the case where the transition t happens in both G1 and G2 but takes a
diﬀerent number of tokens from every node. This diﬀerence is indeed signiﬁcant as the nodes communicate
via the identiﬁed transition and share the knowledge about the diﬀerence.

Theorem 3 is not true for LPOs. This is already shown by the very simple examples in Figure 1(b).
Indeed, in the two LPOs corresponding to each of the dashed arcs in the ﬁgure all sets of nodes participate
in exactly the same transitions.

We note that by the proof of Theorem 1 all the LPOs that disagree only on forced interleavings are

reﬁned by the same g-LPO.

6.2 Choice vs Interleaving in CTSs

We now proceed with CTS. Here, a choice is either a situation where all the agents have exactly the
same history and at least one agent participates in a diﬀerent communication or communications on the
same channel are ordered in a diﬀerent way. Notice that as channels are global resources, the case that
changing the order of communications on a channel does not have side eﬀects is accidental. Indeed, such
a change of order could have side eﬀects and constitutes a diﬀerent choice.

We show that every two distinct g-computations of the same CTS have a joint history of some agent
that “sees the diﬀerence” or a channel that transfers messages in a diﬀerent order. Diﬀerence for a
history is either maximality in one and not the other or extension by diﬀerent communications in the two
g-computations.

Theorem 4. Given a CTS T and two diﬀerent g-LPOs G1, G2 ∈ compg(N ) then one of the following
holds:

1. For some agent i there exists a history hi in both G1 and G2 such that either hi is maximal in Gi

and not maximal G3−i;

2. For some agent i there exists a history hi in both G1 and G2 such that the edges e1 and e2 such that
E(e2);
3. or; There is a pair of agents i and i′ and histories hi and hi′ in both G1 and G2 such that the order

hi →c1 e1 and hi →c2 e2 we have L1

E(e1) 6= L2

between the communications of i and i′ is diﬀerent in G1 and G2.

As for PTI-nets, Theorem 4 is not true for LPOs. This does not hold as shown by the LPOs and g-LPO

of the CTS in Figure 2. Recall, that this CTS has the same LPOs and g-LPOs depicted in Figure 1(b).

We note that by the proof of Theorem 2 all the LPOs that disagree only on forced interleavings are

reﬁned by the same g-LPO.

7 Concluding Remarks

In this paper, we laid down the basis to reason about reconﬁguration in concurrent systems from a global
perspective. We showed how to isolate forced interleaving decisions of the system due to reconﬁguration,
and other decisions due to standard concurrent execution of independent events. To test our results, we
considered PTI-nets [16,12] and CTS [6,5] which cover a wide range of interaction capabilities alongside
reconﬁguration from two diﬀerent schools of concurrency. We proposed, for both, a partial order se-
mantics, named LPO, of computations under reconﬁguration. An LPO extends occurrence nets [27] with
event-to-event connections that allows to refer to reconﬁguration points. Moreover, to fully characterise
reconﬁguration in a single structure, we proposed a glued LPO semantics, named g-LPO. The latter is able
to fully isolate scheduling decisions due to reconﬁguration from the ones due to standard concurrency.
We show that any LPO computation is only a reﬁnement of some g-LPO of the same system. Finally, we
prove important results on g-LPO with respect to reconﬁguration and nondeterminism.

For future work, we would like to exploit g-LPO semantics to verify properties about reconﬁguration
and interaction in general. Namely, we would like to deﬁne a speciﬁcation logic that considers g-LPO
computations as the underlying structure rather than the standard linear computations. Clearly, logics
over linear structures easily distinguish diﬀerent interleavings of the same LPO. However, diﬀerent lin-
earizations of the same LPO are either all computations of a system or none of them is. Similarly, a logic
deﬁned over LPOs would easily distinguish diﬀerent schedules that relate to reconﬁguration. Again, these

10

Y. Abd Alrahman et al.

diﬀerent LPOs are either all computations of a system or none of them is. By considering g-LPOs as the
underlying structure we can create speciﬁcations that do not distinguish between diﬀerent schedules that
correspond to the same choices of the system. Our view is that such a speciﬁcation language that incorpo-
rates elements of Strategy logic [14] and ltol [5] would not only allow us to reason about interaction and
reconﬁguration, but also to reason about the local views of agents as well as their combined behaviour.

Related works The prevalent approach to semantics of reconﬁgurable interactions is based on linear
order semantics (cf. Pi-calculus [23,15], Mobile Ambients [13], Applied Pi-calculus [1], Psi-calculus [11,8],
concurrent constraint programming [25,18], fusion calculus [28], the AbC calculus [3,4], ReCiPe [5] etc.).
This semantics cannot distinguish the diﬀerent choices of the system from a global perspective, and thus
does not facilitate reasoning about reconﬁguration from an external observer’s point of view. It also hides
information about interactions and possible interdependence among events. In fact, linear order semantics
ignores the possible concurrency of events, which can be important e.g. for judging the temporal eﬃciency
of the system [27]. However, it still provides a correct abstraction of the system behaviour, while hiding
such details.

Partial order semantics (cf. Process semantics of Petri nets [24,21,27] and Mazurkiewicz traces of
Zielonka automata [29,17,20]), on the other hand, is able to refer to the interaction and event dependen-
cies, but does not deal very well with reconﬁguration. This is because the latter formalisms have ﬁxed
interaction structures, and thus the interdependence of events is deﬁned structurally. Reconﬁguration, on
the other hand, enforces reordering of events dynamically in non-trivial ways, and thus makes deﬁning
correct partial order semantics very challenging. As shown in [19], some aspects of concurrency are almost
impossible to tackle in both linear-order and partial-order causality-based models, and one of them is
PTI-nets [16]. In fact, reconﬁguration increases the expressive power of the formalism, e.g., adding in-
hibitor arcs to Petri nets makes them Turing Powerful [2]. However, this expressive power does not come
without expenses. In fact, it prevents most analysis techniques for standard Petri nets [12].

To the best of our knowledge, the closest to our LPO semantics is Relational Structures [19]. In order
to capture inhibition they add an additional “not later than” relation to partial orders. Much like our
LPOs, this allows to represent the diﬀerent forced interleavings separately. The emphasis in [19] is on
providing a general semantic framework for concurrent systems. Thus, relational structures handle issues
like priority and error recovery, which we do not handle. However, relational structures are not concerted
directly with separation of choice from interleaving as we are. So the two works serve diﬀerent purposes
and it would be interesting to investigate mutual extensions.

References

1. Abadi, M., Blanchet, B., Fournet, C.: The applied pi calculus: Mobile values, new names, and secure communi-
cation. J. ACM 65(1), 1:1–1:41 (2018). https://doi.org/10.1145/3127586, https://doi.org/10.1145/3127586
2. Agerwala, T.: A complete model for representing the coordination of asynchronous processes. Tech. rep.,

Johns Hopkins Univ., Baltimore, Md.(USA) (1974)

3. Alrahman, Y.A., Nicola, R.D., Loreti, M.: A calculus

behavioural

its
Comput.
theory.
https://doi.org/10.1016/j.ic.2019.104457

Inf.

268

(2019).

collective-adaptive

for
and
https://doi.org/10.1016/j.ic.2019.104457,

systems

4. Alrahman, Y.A., Nicola, R.D., Loreti, M.: Programming interactions

in collective adaptive sys-
tems by relying on attribute-based communication. Sci. Comput. Program. 192, 102428 (2020).
https://doi.org/10.1016/j.scico.2020.102428, https://doi.org/10.1016/j.scico.2020.102428

5. Alrahman, Y.A., Perelli, G., Piterman, N.: Reconﬁgurable interaction for MAS modelling. In: Proceedings of
the 19th International Conference on Autonomous Agents and Multiagent Systems, AAMAS ’20, Auckland,
New Zealand, May 9-13, 2020. pp. 7–15. International Foundation for Autonomous Agents and Multiagent
Systems (2020)

6. Alrahman, Y.A., Piterman, N.: Modelling and veriﬁcation of reconﬁgurable multi-agent systems (2021)
7. Baeten,

J.C.M., Basten, T.: Partial-order process

algebra

(and its

relation to petri nets).
(eds.) Handbook of Process Algebra, pp. 769–
https://doi.org/10.1016/b978-044482830-9/50031-x,

In: Bergstra, J.A., Ponse, A., Smolka, S.A.
(2001).
/
872.
https://doi.org/10.1016/b978-044482830-9/50031-x

North-Holland

Elsevier

8. Bengtson, J., Johansson, M., Parrow, J., Victor, B.: Psi-calculi: a framework for mobile pro-
(2011).
logic. Logical Methods

cesses with
https://doi.org/10.2168/LMCS-7(1:11)2011, https://doi.org/10.2168/LMCS-7(1:11)2011

Science 7(1)

in Computer

nominal

data

and

9. Best, E., Desel, J.: Partial order behaviour and structure of petri nets. Formal Aspects Comput. 2(2), 123–138

(1990). https://doi.org/10.1007/BF01888220, https://doi.org/10.1007/BF01888220

10. Boreale, M., Nicola, R.D., Pugliese, R.: Basic observables for processes. Inf. Comput. 149(1), 77–98 (1999).

https://doi.org/10.1006/inco.1998.2755, https://doi.org/10.1006/inco.1998.2755

Separating Choice from Scheduling using Glue

11

11. Borgstr¨om, J., Huang, S., Johansson, M., Raabjerg, P., Victor, B., Pohjola, J.˚A., Parrow, J.: Broadcast
psi-calculi with an application to wireless protocols. Software and System Modeling 14(1), 201–216 (2015).
https://doi.org/10.1007/s10270-013-0375-z, https://doi.org/10.1007/s10270-013-0375-z

12. Busi, N.: Analysis issues in petri nets with inhibitor arcs. Theor. Comput. Sci. 275(1-2), 127–177 (2002).
https://doi.org/10.1016/S0304-3975(01)00127-X, https://doi.org/10.1016/S0304-3975(01)00127-X
13. Cardelli, L., Gordon, A.D.: Mobile ambients. Electr. Notes Theor. Comput. Sci. 10, 198–201 (1997).

https://doi.org/10.1016/S1571-0661(05)80699-1

14. Chatterjee, K., Henzinger, T.A., Piterman, N.: Strategy logic. Inf. Comput. 208(6), 677–693 (2010).

https://doi.org/10.1016/j.ic.2009.07.004, https://doi.org/10.1016/j.ic.2009.07.004

15. Ene, C., Muntean, T.: Expressiveness of point-to-point versus broadcast communications. In: Fundamentals

of Computation Theory. pp. 258–268. Springer (1999)
16. Flynn, M.J., Agerwala, T.: Comments on capabilities,

limitations and correctness of petri nets. In:
Lipovski, G.J., Szygenda, S.A. (eds.) Proceedings of the 1st Annual Symposium on Computer Architec-
ture, Gainesville, FL, USA, December 1973. pp. 81–86. ACM (1973). https://doi.org/10.1145/800123.803973,
https://doi.org/10.1145/800123.803973

17. Genest, B., Gimbert, H., Muscholl, A., Walukiewicz, I.: Optimal zielonka-type construction of deterministic
asynchronous automata. In: Abramsky, S., Gavoille, C., Kirchner, C., auf der Heide, F.M., Spirakis, P.G. (eds.)
Automata, Languages and Programming, 37th International Colloquium, ICALP 2010, Bordeaux, France,
July 6-10, 2010, Proceedings, Part II. Lecture Notes in Computer Science, vol. 6199, pp. 52–63. Springer
(2010). https://doi.org/10.1007/978-3-642-14162-1 5, https://doi.org/10.1007/978-3-642-14162-1_5
18. Gilbert, D.R., Palamidessi, C.: Concurrent constraint programming with process mobility. In: Computational
Logic - CL 2000, First International Conference, London, UK, 24-28 July, 2000, Proceedings. pp. 463–477
(2000). https://doi.org/10.1007/3-540-44957-4 31, https://doi.org/10.1007/3-540-44957-4_31
J., Koutny, M., Mikulski, L.: Relational

19. Janicki, R., Kleijn,

structures

concurrent be-
for
https://doi.org/10.1016/j.tcs.2020.10.019,

haviours. Theor. Comput.
https://doi.org/10.1016/j.tcs.2020.10.019

Sci.

862,

174–192

(2021).

20. Krishna, S., Muscholl, A.: A quadratic construction for zielonka automata with acyclic communi-
cation structure. Theor. Comput. Sci. 503, 109–114 (2013). https://doi.org/10.1016/j.tcs.2013.07.015,
https://doi.org/10.1016/j.tcs.2013.07.015

21. Meseguer, J., Montanari, U., Sassone, V.: On the semantics of petri nets. In: International Conference on

Concurrency Theory. pp. 286–301. Springer (1992)

22. Milner, R., Parrow, J., Walker, D.: A calculus of mobile processes, I. Inf. Comput. 100(1), 1–40 (1992).

https://doi.org/10.1016/0890-5401(92)90008-4

23. Milner, R., Parrow, J., Walker, D.: A calculus of mobile processes, ii. Information and computation 100(1),

41–77 (1992)

24. Petri, C.A., Reisig, W.: Petri net. Scholarpedia 3(4), 6477 (2008). https://doi.org/10.4249/scholarpedia.6477,

https://doi.org/10.4249/scholarpedia.6477

25. Saraswat, V.A., Rinard, M.C.: Concurrent constraint programming. In: Conference Record of the Seventeenth
Annual ACM Symposium on Principles of Programming Languages, San Francisco, California, USA, January
1990. pp. 232–245 (1990). https://doi.org/10.1145/96709.96733, https://doi.org/10.1145/96709.96733
26. Stefanescu, A., Esparza, J., Muscholl, A.: Synthesis of distributed algorithms using asynchronous au-
In: Amadio, R.M., Lugiez, D. (eds.) CONCUR 2003 - Concurrency Theory, 14th Inter-
tomata.
national Conference, Marseille, France, September 3-5, 2003, Proceedings. Lecture Notes in Com-
puter Science, vol. 2761, pp. 27–41. Springer
(2003). https://doi.org/10.1007/978-3-540-45187-7 2,
https://doi.org/10.1007/978-3-540-45187-7_2

27. Vogler, W.: Partial order

semantics and read arcs. Theor. Comput. Sci. 286(1), 33–63 (2002).

https://doi.org/10.1016/S0304-3975(01)00234-1, https://doi.org/10.1016/S0304-3975(01)00234-1

28. Wischik, L., Gardner, P.: Explicit

fusions. Theor. Comput.

Sci.

340(3),

606–630

(2005).

https://doi.org/10.1016/j.tcs.2005.03.017, https://doi.org/10.1016/j.tcs.2005.03.017

29. Zielonka, W.: Notes on ﬁnite asynchronous automata. RAIRO Theor. Informatics Appl. 21(2), 99–135 (1987)

12

Y. Abd Alrahman et al.

A Proofs for Section 4 (LPO Semantics)

A.1 Asynchronous automata

We include the LPO semantics of asynchronous automata. As asynchronous automata do not have recon-
ﬁgurations of communication we only need the communication relation and do not use the interleaving
order relation. This makes the notion of glue not relevant for asynchronous automata.

A process is P = (Act, S, s0, δ), where Act is a ﬁnite non-empty alphabet, S is a ﬁnite and non-empty
set of states, s0 ∈ S is an initial state, and δ ⊆ S × Act × S. We also write δ : S × Act → 2S when
convenient.

A history h = s0, . . . , sn is a ﬁnite sequence such that s0 = s0 and for every 0 ≤ i < n we have
si+1 ∈ δ(si, ai) for some ai ∈ Act. The length of h is n + 1, denoted |r|. For convenience, if h1 = s0, . . . , sn
and h2 = s0, . . . , sn, sn+1 such that sn+1 ∈ δ(sn, an) we write h2 ∈ δ(h1, an) or δ(h1, an, h2). We deﬁne
hist(P ) to be the set of histories of P .

A ﬁnite asynchronous automaton A with n processes is A = (P1, . . . , Pn) such that each Pi =

(Acti, Si, s0

i , δi) is a process. Let Act =

Acti.

Si

Deﬁnition 9 (computation). A computation of A is an LPO (O, →c, Σ, Υ, L), where V ⊆
Σ = V , L(h) = h, and Υ = Act such that:

Si hist(Pi),

1. The edge eǫ such that L(eǫ) = a for some a is the unique minimal element according to ≤. For every

2. If h ∈ V there is a unique e ∈ E such that e →c h. If |h| > 1, there is also a unique h′ ∈ V such that

i, we have s0

i ∈ V and eǫ →c s0
i .

h ∈ δ(h′, L(e)) and h′ →c e.

3. For every h ∈ V there is at most one e ∈ E such that h →c e.
4. For every e ∈ E there is I ⊆ [n] such that all the following hold:
Acti

(a) LE(e) ∈
(b) •e, e• ∈
(c) For every i ∈ I we have |•e ∩ hist(Pi)| = 1 and |e• ∩ hist(Pi)| = 1

Acti \
Ti∈I
Si∈I hist(Pi)

Si /∈I

That is, a computation starts from an arbitrary joint edge that leads to the initial states of all
processes. For every transition, the set of participating processes is all those having the transition’s label
in their alphabet. Each participating process has a history that is a predecessor of the transition and a
history that is a successor of the transition. The pair of histories that belong to one process satisfy the
transition relation of that process.

A.2 Composition for Channeled Transition Systems

We include the deﬁnition of the parallel composition operator over CTS. A parallel composition of CTSs
is again a CTS.

0, Ri, Li, lsii, where i ∈
Deﬁnition 10 (Parallel Composition). Given two CTS Ti = hCi, Λi, Bi, Si, si
{1, 2} their composition T1 k T2 is the following CTS T = hC, Λ, B, S, s0, R, L, lsi, where the components
of T are:

– C = C1 ∪ C2
– B = B1 ∪ B2
0, s2
– s0 = (s1
0)
– ls(s1, s2) = ls1(s1) ∪ ls2(s2)

– R =

– Λ = Λ1 × Λ2
– S = S1 × S2
– L(s1, s2) = (L1(s1), L2(s2))

((s1, s2), (υ, !, c), (s′

1, s′

2))






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(s1, (υ, !, c), s′

1) ∈ R1, c ∈ ls2(s2) and (s2, (υ, ?, c), s′

2) ∈ R2

or

(s1, (υ, ?, c), s

′
1) ∈ R1, c ∈ ls1(s1), and (s2, (υ, !, c), s

′
2) ∈ R2 or

(s1, (υ, !, c), s

′
1) ∈ R1, c /∈ ls2(s2), and s2 = s

′
2

c /∈ ls1(s1), s1 = s′

1, and (s2, (υ, !, c), s′
2) ∈ R2
′
c ∈ ls1(s1), (s1, (υ, ?, c), s
1) ∈ R1, c ∈ ls2(s2)
′
and (s2, (υ, ?, c), s
2) ∈ R2




((s1, s2), (υ, ?, c), (s′


1, s′

2))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(s1, (υ, ?, c), s

′
1) ∈ R1, c /∈ ls2(s2), and s2 = s

′
2 or

c /∈ ls1(s1), s1 = s′

1, and (s2, (υ, ?, c), s′

2) ∈ R2

or

or

∪

∪










Separating Choice from Scheduling using Glue

13




((s1, s2), (υ, γ, ⋆), (s′

1, s′

2))



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

γ ∈ {!, ?}, (s1, (υ, γ, ⋆), s
2 . (s2, (υ, ?, ⋆), s′′
∀s′′

2 ) /∈ R2

′
1) ∈ R1, s2 = s

′
2 and

γ ∈ {!, ?}, s1 = s
(s2, (υ, γ, ⋆), s′

′
1, ∀s
2) ∈ R2,

′′
1 . (s1, (υ, ?, ⋆), s

or

′′
1 ) /∈ R1 and






The transition relation R of the composition deﬁnes two modes of interactions, namely multicast
and broadcast. In both interaction modes, the composition T sends a message (υ, !, c) on channel c (i.e.,
((s1, s2), (υ, !, c), (s′
1) ∈ R1
or (s2, (υ, !, c), s′

2)) ∈ R) if either T1 or T2 is able to generate this message, i.e, (s1, (υ, !, c), s′

1, s′
2) ∈ R2.

Consider the case of a multicast channel. A multicast is blocking. Thus, a multicast message is sent
if either it is received or the channel it is sent on is not listened to. Suppose that a message originates
from T1, i.e., (s1, (υ, !, c), s′
1) ∈ R1. Then, T2 must be able to either receive the message or, in the case
that T2 does not listen to the channel, discard it. CTS T2 receives if (s2, (υ, ?, c), s′
2) ∈ R2. It discards if
c /∈ ls2(s2) and s2 = s′
2. The case of T2 sending is dual. Note that T2 might be a composition of other
CTS(s), say T2 = T3kT4. In this case, T2 listens to channel c if at least one of T3 or T4 is listening. That is,
it could be that either c ∈ (ls(s3) ∩ ls(s4)), c ∈ (ls(s2)\ls(s3)), or c ∈ (ls(s2)\ls(s4)). In the ﬁrst case,
both must receive the message. In the latter cases, the listener receives and the non-listener discards.
Accordingly, when a message is sent by one system, it is propagated to all other connected systems in a
joint transition. A multicast is indeed blocking because a connected system cannot discard an incoming
message on a channel it is listening to. More precisely, a joint transition ((s1, s2), (υ, !, c), (s′
2)) where
c ∈ ls(s2) requires that (s2, (υ, ?, c), s′
2) is supplied. In other words, message sending is blocked until all
connected receivers are ready to participate in the interaction. Clearly, the latter correspond to inhibition
arcs in Petri nets.

1, s′

Consider now a broadcast. A broadcast is non-blocking. Thus, a broadcast message is either received
or discarded. Suppose that a message originates from T1, i.e., (s1, (υ, !, ⋆), s′
1) ∈ R1. If T2 is receiving,
2) ∈ R2 the message is sent. However, by deﬁnition, we have that ⋆ ∈ ls(s) for every s
i.e., (s2, (υ, ?, ⋆), s′
in a CTS. Namely, a system may not disconnect the broadcast channel ⋆. For this reason, the last part
of the transition relation R deﬁnes a special case for handling (non-blocking) broadcast. Accordingly, a
joint transition ((s1, s2), (υ, γ, ⋆), (s′
2)) ∈ R where γ ∈ {!, ?} is always possible and may not be blocked
by any receiver. In fact, if (γ = !) and (s1, (υ, !, ⋆), s′
1) ∈ R1 then the joint transition is possible whether
(s2, (υ, ?, ⋆), s′
2) ∈ R2 or not. In other words, a broadcast can happen even if there are no receivers.
Furthermore, if (γ = ?) and (s1, (υ, ?, ⋆), s′
1) ∈ R1 then also the joint transition is possible regardless of
the other participants. In other words, a broadcast is received only by interested participants.

1, s′

B Proofs for Section 5 (Partial Order with Glue)

Lemma 1. Given an LPO π ∈ comp(N ), there exists a corresponding g-LPO ⌊π⌋ ∈ compg(N ) such
that π (cid:22) ⌊π⌋.

Proof. Let ⌊π⌋ be the g-LPO obtained from π by using →c of π, setting →g
relations according to Deﬁnition 7.

i = ∅, and adding the glue

We have to show that the conditions of Deﬁnition 6 hold. Note that by construction both π and ⌊π⌋

agree on →c⊆ V × E ∪ E × V and only disagree in terms of →i⊆ E × E and the glue.

Consider some t ∈ T and (a, b) ∈ E(t). We have to show that e ≤ a or b ≤ e. By deﬁnition we know

that a →c b. We have the following cases.

– If a ∈ V and b ∈ E then (L(a), t) ∈ I. By N 4(b) in deﬁnition 2 we have that either e ≤ a or a ≤ e. If

e ≤ a we are done. If a ≤ e then from a →c b it follows that either e = b or b < e.

– If a ∈ E and b ∈ V then by deﬁnition (L(a), t) ∈ I. By N 4(b) in deﬁnition 2 we have that either e ≤ b

or b ≤ e. If b ≤ e we are done. If e ≤ b then from a →c b it follows that either e = a or e < a.

Consider some (e, e′) ∈→i. We have to show that either (e′, v) ∈ E(L(e)) for some v or (v, e) ∈ E(L(e′)).

By deﬁnition, we have that there exists v ∈ V such that one of the following holds.

– (L(v), L(e′)) ∈ I and v →c e. By ∗N 5 in Deﬁnition 7, we have that (v, e) ∈ E(L(e′)) as required.
– (L(v), L(e)) ∈ I and e′ →c v. By ∗N 5 in Deﬁnition 7, we have that (e′, v) ∈ E(L(e)) as required.

Lemma 2. Given a g-LPO π1 ∈ compg(N ) and an LPO π2 such that π2 (cid:22) π1 then π2 ∈ comp(N ).

14

Y. Abd Alrahman et al.

Proof. Given that π2 (cid:22) π1, it follows that both π1 and π2 agree on →c⊆ V × E ∪ E × V and only disagree
in terms of →i⊆ E × E and the glue.

It is suﬃcient to prove that N 4, items (b) and (c) in Deﬁnition 2 hold for π2. Consider some e ∈ E.

We have the following cases.

– Consider some v ∈ V and e ∈ E such that L(v) ∈ ◦L(e). In order to show that π2 ∈ comp(N ) we
have to show that e ≤ v or v ≤ e. Let e′ be the edge such that e′ →c v. By Deﬁnition 7 (∗N 5) we
have that (e′, v) ∈ E(L(e)). By reﬁnement, we have that either e ≤ e′, which implies e ≤ v, or v ≤ e
as required.

– Consider some e′ ∈ E such that e →i e′. By reﬁnement, we have one of the following cases holds.

• (e′, v) ∈ E(L(e)) for some v. By Deﬁnition 7 (∗N 5), we have that e′ →c v and (L(v), L(e)) ∈ I as

required.

• (v, e) ∈ E(L(e′)) for some v. By Deﬁnition 7 (∗N 5), we have that v →c e and (L(v), L(e′)) ∈ I as

required.

Theorem 1. Given a PTI-net N , comp(N ) = {π | π (cid:22) πg ∧ πg ∈ compg(N )}.

Proof. The proof follows directly from Lemma 1 and Lemma 2.

Lemma 3. Given an LPO π ∈ comp(T ), there exists a corresponding g-LPO ⌊π⌋ ∈ compg(T ) such that
π (cid:22) ⌊π⌋.

Proof. Let ⌊π⌋ be the g-LPO obtained from π by using the partial order induced by →c of π, by →i of π
whenever e →i e′ implies ch(e) = ch(e′), and adding the glue relations according to Deﬁnition 8.

Note that by construction both π and ⌊π⌋ agree on →c⊆ V × E ∪ E × V and only agree on →i⊆ E × E

whenever e →i e′ implies ch(e) = ch(e′). We have to show that the conditions of Deﬁnition 6 hold.

Consider some e ∈ E and (a, b) ∈ E(L(e)). We have to show that e ≤ a or b ≤ e. By deﬁnition we

know that a →c b. We show that either e ≤ a or b ≤ e. We have the following cases.

– ch(L(e)) is a multicast channel:

• If a ∈ V and b ∈ E then by deﬁnition ch(L(e)) ∈ ls(a). By C4(c) in deﬁnition 3 we have that
either e ≤ a or a ≤ e. If e ≤ a we are done. If a ≤ e then from a →c b it follows that either e = b
or b < e.

• If a ∈ E and b ∈ V then by deﬁnition ch(L(e)) ∈ ls(b). By C4(c) in deﬁnition 3 we have that
either e ≤ b or b ≤ e. If b ≤ e we are done. If e ≤ b then from a →c b it follows that either e = a
or e < a.

– ch(L(e)) is the broadcast channel:

• If a ∈ V and b ∈ E then a →?(L(e)). By C4(d) in deﬁnition 3 we have that either e ≤ a or a ≤ e.

If e ≤ a we are done. If a ≤ e then from a →c b it follows that either e = b or b < e.

• If a ∈ E and b ∈ V then b →?(L(e)). By C4(d) in deﬁnition 8 we have that either e ≤ b or b ≤ e.

If b ≤ e we are done. If e ≤ b then from a →c b it follows that either e = a or e < a.

Consider e, e′ ∈ E such that (e, e′) ∈→g
Consider e, e′ ∈ E such that (e, e′) ∈ (→i \ →g
that one of the following holds.
– ch(L(e′)) 6= ⋆, ch(L(e′)) ∈ ls(v) and v →c e. By ∗C7 in Deﬁnition 8, we have that (v, e) ∈ E(L(e′))

i ⊆→i.
i ). By C6(b) − (e) in Deﬁniton 3 there exists v ∈ V such

i . By construction we have that (e, e′) ∈→i, and thus →g

as required.

– ch(e) 6= ⋆, ch(L(e)) ∈ ls(v) and e′ →c v. By ∗C7 in Deﬁnition 8, we have that (e′, v) ∈ E(L(e)) as

required.

– ch(e′) = ⋆, v →?(L(e′)) and v →c e. By ∗C7 in Deﬁnition 8, we have that (v, e) ∈ E(L(e′)) as required.
– ch(e) = ⋆, v →?(L(e)) and e′ →c v. By ∗C7 in Deﬁnition 8, we have that (e′, v) ∈ E(L(e)) as required.
Lemma 4. Given a g-LPO π1 ∈ compg(T ) and an LPO π2 such that π2 (cid:22) π1 then π2 ∈ comp(T ).

Proof. Given that π2 (cid:22) π1, it follows that both π2 and π2 agree on →c⊆ V × E ∪ E × V and only agree
on →i⊆ E × E whenever e →i e′ implies ch(e) = ch(e′). Hence, it is suﬃcient to prove that C4(c) − (d)
and C6(b) − (e) in Deﬁnition 3 hold for π2.
We prove C4(c) − (d). Consider some e ∈ E. We have the following cases.

– ch(L(e)) is a multicast channel:

Consider some v ∈ V such that ch(L(e)) ∈ ls(v). We have to show that e ≤ v or v ≤ e. By Deﬁnition 8
(∗C7), there is some e′ such that one of the following cases holds.

Separating Choice from Scheduling using Glue

15

• (v, e′) ∈ E(L(e)) where v →c e′. By reﬁnement, we have that if (v, e′) ∈ E(L(e)) then either e ≤ v

as required or e′ ≤ e, which implies that v ≤ e.

• (e′, v) ∈ E(L(e)) where e′ →c v. By reﬁnement, we have that if (e′, v) ∈ E(L(e)) then either v ≤ e

as required or e ≤ e′, which implies that e ≤ v.

– ch(L(e)) is a broadcast channel:

Consider some v ∈ V such that v →?(L(e)). We have to show that e ≤ v or v ≤ e. By Deﬁnition 8
(∗C7), there is some e′ such that one of the following cases holds.

• (v, e′) ∈ E(L(e)) where v →c e′. By reﬁnement, we have that if (v, e′) ∈ E(L(e)) then either e ≤ v

as required or e′ ≤ e, which implies that e ≤ v.

• (e′, v) ∈ E(L(e)) where e′ →c v. By reﬁnement, we have that if (e′, v) ∈ E(L(e)) then either v ≤ e

as required or e ≤ e′, which implies that e ≤ v.

We prove C6(b) − (e). Consider (e, e′) ∈→i such that (e, e′) ∈ (→i \ →g
the following cases hold.

i ). By reﬁnement, we have one of

– ch(L(e)) is a multicast channel:

• (e′, v) ∈ E(L(e)) for some v. By Deﬁnition 8 (∗C7), we have that e′ →c v and ch(L(e)) ∈ ls(v)

as required.

• (v, e) ∈ E(L(e′)) for some v. By Deﬁnition 8 (∗C7), we have that v →c e and ch(L(e)) ∈ ls(v) as

required.

– ch(L(e)) is a broadcast channel:

• (e′, v) ∈ E(L(e)) for some v. By Deﬁnition 8 (∗C6), we have that e′ →c v and v →?(L(e)) as

required.

• (v, e) ∈ E(L(e′)) for some v. By Deﬁnition 8 (∗C6), we have that v →c e and v →?(L(e′)) as

required.

Theorem 2. Given a CTS T , comp(T ) = {π | π (cid:22) πg ∧ πg ∈ compg(T )}.

Proof. The proof follows by Lemma 3 and Lemma 4.

C Proofs for Section 6 (Separating Choice and Reconﬁguration-Forced

Interleaving)

Theorem 3. Given a Petri net P and two diﬀerent g-LPOs G1, G2 ∈ compg(N ) then there exists a set
of nodes v1, . . . , vn appearing in both G1 and in G2 such that one of the following holds:

1. There is a node vi such that the number of tokens not taken from vi in G1 and G2 is diﬀerent.
2. There is a set of p-histories v1, . . . , vn that participate in some transition t in Gi but not in G3−i.

Proof. We deﬁne the depth of a history to be the maximal number of transitions taken by some token
in the history. Formally, the depth (∅, tǫ) is 0. The depth of a p-history (h, p, j) is depth(h) + 1. For a
t-history e ∈ E, let •h be {e1, . . . , en}, then the depth of e is maxj depth(ej). Notice, that a t-history e
could have other edges in its preset.

We order the elements in a g-LPO by increasing depth. In addition, elements of the same depth are
ordered so that edges appear before vertices and there is some arbitrary order between edges of the same
depth and between vertices of the same depth. Clearly, in this order every element appears after all the
elements that are smaller than it according to ≤. Indeed, if a →c b or a →i b, then the depth of b is at
least the depth of a plus one. As every element has a ﬁnite depth and there is a ﬁnite number of elements
in every depth, it follows that this order is some linearisation of all the elements in the g-LPO.

We prove the theorem by induction according to the order mentioned above. We are going to mark
nodes and edges that appear in both G1 and G2. Nodes are marked by the number of tokens in them that
we have not handled yet. When this number is 0 the node is called closed. Otherwise, it is open. Edges
are simply marked (or unmarked). For all marked nodes, we “handle” tokens that are participating in
the same transitions in G1 and G2. Nodes could have tokens that do not participate in transitions. As
we “handle” tokens we mark transitions continuing from the node as not forming part of the diﬀerence
between G1 and G2. Once we mark nodes as closed they are also equivalent in G1 and G2. As we go
through the nodes in G1 in induction order either we ﬁnd a diﬀerence or, if not, the induction proves
that G1 and G2 are equivalent in contradiction to the assumption.

16

Y. Abd Alrahman et al.

Both G1 and G2 have the t-history hǫ = (∅, tǫ) as minimal element. Mark it as closed. The p-histories
of the form (hǫ, p, m0(p)) such that m0(p) > 0 are marked by m0(p). Clearly, as both G1 and G2 start
from the initial marking m0 both G1 and G2 have the same nodes marked and they have the same positive
number of tokens.

Assume that we have marked a preﬁx of G1 and G2 such that all closed nodes have all their outgoing
transitions marked. Furthermore, the number marking a node is suﬃcient for all unmarked transitions
existing from the node. Clearly, this is true of the marking of the minimal nodes.

Suppose that there are some open nodes. Choose the minimal open node v according to the induction
order. If there are no unmarked edges connected to v in both G1 and G2 then mark v as closed. If there
is no unmarked edge connected to v in G1 and there is some unmarked edge connected to v in G2 then
we have found a diﬀerence as the number of tokens “left” in v in G1 is larger than in G2. In this case,
we have identiﬁed the diﬀerence between G1 and G2. Similarly for the other way around.

The remaining case is when both in G1 and G2 there are unmarked edges connected to v. Let e be
the minimal unmarked edge connected to v in G1. If e is not connected to v in G2 we are done. Indeed,
the preset of e either participate in e in G1 and not in G2 or participate in a transition LE(e) in diﬀerent
ways in G1 and G2.

Otherwise, e is connected to v both in G1 and G2. By its construction as a multiset of place histories,
e “takes” the same number of tokens from v in G1 and G2. As e is unmarked, all the other nodes that
e takes tokens from have a suﬃcient number of unhandled tokens. Again, by e’s structure as a pair of a
multiset and a transition, e connects to exactly the same nodes in G1 and G2 in the same way. Reduce
the marking of all predecessors of e by the number of tokens taken by e from them. If some of them are
reduced to 0 then they are closed. Mark e as well.

If there are no open nodes, then both G1 and G2 are ﬁnite and equivalent. Otherwise, continue

handling open nodes by induction.

Theorem 4. Given a CTS T and two diﬀerent g-LPOs G1, G2 ∈ compg(N ) then one of the following
holds:

1. For some agent i there exists a history hi in both G1 and G2 such that either hi is maximal in Gi

and not maximal G3−i;

2. For some agent i there exists a history hi in both G1 and G2 such that the edges e1 and e2 such that
E(e2);
3. or; There is a pair of agents i and i′ and histories hi and hi′ in both G1 and G2 such that the order

hi →c1 e1 and hi →c2 e2 we have L1

E(e1) 6= L2

between the communications of i and i′ is diﬀerent in G1 and G2.

Proof. As before, we deﬁne the depth of elements in a partial order as their distance from a minimal
element. Formally, the depth of the minimal element is 0 and all the initial states (runs of length 1) have
depth of 1. The depth of a non-minimal element o is maxo′∈•o depth(o′) + 1.

As before, we order the elements in a g-LPO by increasing depth. In addition, elements of the same
depth are ordered so that edges appear before vertices and there is some arbitrary order between edges of
the same depth and between vertices of the same depth. In this order, every element appears after all the
elements that are smaller than it according to ≤. As before, every element has a ﬁnite depth and there
is a ﬁnite number of elements in every depth. Hence, if we follow this order constitutes a linearization of
the elements of the g-LPO.

We prove the theorem by induction according to the order mentioned above. As before, we are going
to mark elements in the partial order as “equivalent” in both G1 and G2. The marking here is simpler
(immediately closed marking).

Consider the minimal element edges in G1 and G2 and their post-sets of runs of length 1 (depth 1).
By deﬁnition, these correspond to the initial states of the diﬀerent agents. It follows that they are the
same. Mark all of them.

Assume that we have marked up to a point in G1 and G2 according to the induction order. We build
the marking so that the maximal marked elements according to ≤ are all nodes. Obviously, all maximal
(according to ≤) marked elements are incomparable. It follows that we maintain the minimal unmarked
element (in induction order) as an edge. Clearly, this is true for the marking of the minimal nodes.

Consider the set of unmarked edges in G1 and G2. If both are empty, then G1 and G2 are the same.
Suppose that the set of unmarked edges in (wlog) G1 is empty and G2 is not empty. Consider the sender
participating in the communication of the ﬁrst unmarked edge in G2. It must be the case that we have
found an agent i and a history hi that is maximal in G1 and not maximal in G2. The remaining case is
that both G1 and G2 have unmarked edges.

Separating Choice from Scheduling using Glue

17

Consider the g-LPO G1. Let e be the minimal unmarked edge in G1 according to the induction order.
Let h1, . . . , hn be •e in G1 with h1 being the sender. As all elements of smaller depth than e have been
marked, it follows that h1, . . . , hm have been marked and that they appear also in G2.

Consider a history hi ∈ •e. If hi is maximal in G2 we are done. Otherwise, let ei be the edge such
E(ei) we are done as hi does something diﬀerent in G1 and G2. The same

that hi →≤2 ei. If L1
holds for every j ∈ {1, . . . , m}. Hence, for every j we have ej exists and LE(ej) = L(e).

E(e) 6= L2

Suppose that G1 and G2 are diﬀerent here. This can only happen if there are at least two agents j
and j′ for which ej and ej′ are distinct edges labeled by the same communication. In particular, n ≥ 2
and the agents in histories hi for i > 1 are listening to channel ch(LE(e)).

However, for ej and ej′ each, there is a unique sender. If h1 is not sending in G2 then h1 does something
diﬀerent in G1 and G2 and we are done. Wlog, assume that h1 is the sender of ej. Consider the following
options.

– Suppose that one of the agents hi for i > 1 is the sender of ej′ . Then, hi is a history that receives in

G1 and sends in G2. Thus, hi does something diﬀerent in G1 and G2.

– Suppose that there exists an additional agent k and a history hk such that hk is the sender for ej′ .
In order not to ﬁnd a diﬀerence between G1 and G2, it must be the case that hk is a sender of ej′
also in G1 and the set of agents that participate in ej and ej′ together is the same and they have the
same roles. That is, every agent that is a receiver in G1 is a receiver in G2 and vice versa. However,
as we assumed that G1 and G2 are diﬀerent, there are again two options:

• Either the order between ej and ej′ in G1 and G2 is reversed. This matches the diﬀerence 3,

where the senders are the agents witnessing the diﬀerence.

• Or the order between ej and ej′ is the same in both G1 and G2. Then, the matching between
senders and receivers in G1 and G2 to ej and ej′ is diﬀerent. Consider a receiver that moved from
listening to (wlog) ej to ej′ . It follows that this agent participates in an early communication in
G1 (ej) and a later communication in G2 (ej′ ). This receiving agent and the sender of ej see a
diﬀerent order of the communication they participate in (from equal to one before the other).

By induction, unless this process terminates prematurely by ﬁnding a diﬀerence, it will visit all of G1

and G2 and show that they are, in fact, equivalent.

