0
2
0
2

g
u
A
6
2

]

V
C
.
s
c
[

1
v
9
8
7
1
1
.
8
0
0
2
:
v
i
X
r
a

Expressive Telepresence via
Modular Codec Avatars

Hang Chu1,2

Shugao Ma3

Fernando De la Torre3

Sanja Fidler1,2

Yaser Sheikh3

1University of Toronto

2Vector Institute

3Facebook Reality Lab

Abstract. VR telepresence consists of interacting with another human
in a virtual space represented by an avatar. Today most avatars are
cartoon-like, but soon the technology will allow video-realistic ones. This
paper aims in this direction, and presents Modular Codec Avatars (MCA),
a method to generate hyper-realistic faces driven by the cameras in the
VR headset. MCA extends traditional Codec Avatars (CA) by replacing
the holistic models with a learned modular representation. It is impor-
tant to note that traditional person-speciﬁc CAs are learned from few
training samples, and typically lack robustness as well as limited expres-
siveness when transferring facial expressions. MCAs solve these issues by
learning a modulated adaptive blending of diﬀerent facial components as
well as an exemplar-based latent alignment. We demonstrate that MCA
achieves improved expressiveness and robustness w.r.t to CA in a variety
of real-world datasets and practical scenarios. Finally, we showcase new
applications in VR telepresence enabled by the proposed model.

Keywords: Virtual Reality, Telepresence, Codec Avatar

1

Introduction

Telepresence technologies aims to make a person feel as if they were present, to
give the appearance of being present, or to have an eﬀect via telerobotics, at a
place other than their true location. Telepresence systems can be broadly cate-
gorized based on the level of immersiveness. The most basic form of telepresence
is video teleconferencing (e.g., Skype, Hangouts, Messenger) that is widely-used,
and includes both audio and video transmissions. Recently, a more sophisticated
form of telepresence has become available featuring a smart camera that follows
a person (e.g., the Portal from Facebook).

This paper addresses a more immersive form of telepresence that utilizes
a virtual reality (VR) headset (e.g., Oculus, VIVE headsets). VR telepresence
aims to enable a telecommunication system that allows remote social interac-
tions more immersive than any prior media. It is not only a key promise of VR,
but also has vast potential socioeconomic impact such as increasing communi-
cation eﬃciency, lowering energy footprint, and such a system could be timely
for reducing inter-personal disease transmission [2]. VR telepresence has been
an important active area of research in computer vision [3,1,4,5,6,7,8]. In VR

 
 
 
 
 
 
2

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

Fig. 1. Train and test pipeline for our VR telepresence system. In the ﬁrst stage, we
capture facial expressions of a user using both a multi-view camera dome and a VR
headset (mounted with face-looking cameras). Correspondences between VR headset
recording and full face expressions are established using the method described in [1].
Finally, once the person-speciﬁc face animation model is learned using these correspon-
dences, a real-time photo-realistic avatar is driven from the VR headset cameras.

telepresence, the users wear the VR headset, and a 3D face avatar is holographi-
cally projected in realtime, as if the user teleports himself/herself into the virtual
space. This allows immersive bidirectional face-to-face conversations, facilitating
instant interpersonal communication with high ﬁdelity.

Fig. 1 illustrates our VR telepresence system that has three main stages:
(1) Appearance/shape capture. The person-speciﬁc avatar is built by captur-
ing shape and appearance of the person from a multi-camera system (i.e., the
dome). The user performs the same set of scripted facial expressions siting in
the dome and wearing a headset mounted with face-looking cameras respectively.
The 3D faces are reconstructed from the dome-captured multi-view images, and
a variational autoencoder (VAE) is trained to model the 3D shape and appear-
ance variability of the person’s face. This model is referred to as Codec Avatar
(CA) [4,1], since it decodes the 3D face from low-dimensional code. The CA
is learned from 10k-14k 3D shapes and texture images. (2) Learning the corre-
spondence between the infra-red (IR) VR cameras and the codec avatar. In the
second stage, the CA method establishes the correspondence between the head-
set cameras and the 3D face model using a image-based synthesis approach [1].
Once a set of IR images (i.e., mouth, eyes) in the VR headset are in correspon-
dence with the CA, we learn a network to map the IR VR headset cameras to
the codec avatar codes, that should generalize to unseen situations (e.g. expres-
sions, environments). (3) Realtime inference. Given the input images from the
VR headset, and the network learned in step two, we can drive a person-speciﬁc
and photo-realistic face avatar. However, in this stage the CA has to satisfy two
properties for authentic interactive VR experience:

– Expressiveness: The VR system needs to transfer the subtle expressions
of the user. However, there are several challenges: (1) The CA model has
been learned from limited training samples of the user (∼10k-14k), and the
system has to precisely interpolate/extrapolate unseen expressions. Recall
that is impractical to have a uniform sample of all possible expressions in
the training set, because of their long-tail distribution. This requires careful
ML algorithms that can learn from few training samples and long-tail dis-

Expressive Telepresence via Modular Codec Avatars

3

Codec Avatar [1,4]

Modular Codec Avatar (MCA)

Fig. 2. Model diagrams comparing the previous CA and the proposed MCA. K denote
the number of head-mounted cameras. In CA, images of all headset cameras are feed
together to the single encoder E to compute the full face code which is subsequently
decoded into 3D face using deocoder D. In MCA, the images of each camera are encoded
separately into a modular code cpart
k with the encoder Ek, which is feed to a synthesizer
Sk to estimate a camera speciﬁc full face code cfull
k , and blending weights. Finally all
these camera speciﬁc full face codes are decoded into 3D faces and blended together
to form the ﬁnal face avatar.

tributions. (2) In addition, CA is an holistic model that typically results in
rigid facial expression transfers.

– Robustness: To enable VR telepresence at scale in realistic scenarios, CAs
have to provide robustness across diﬀerent sources of variability, that in-
cludes: (1) iconic changes in the users’ appearance (e.g., beard, makeup),
(2) variability in the headset camera position (e.g., head strap), (3) diﬀer-
ent lighting and background from diﬀerent room environments, (4) hardware
variations within manufacturing speciﬁcation tolerance (e.g., LED intensity,
camera placement).

This paper proposes Modular Codec Avatar (MCA) that improves robust-
ness and expressiveness of traditional CA. MCA decomposed the holistic face
representation of traditional CA into learned modular local representations. Each
local representation corresponds to one headset-mounted camera. MCA learns
from data the automatic blending across all the modules. Fig. 2 shows a diagram
comparing the CA and MCA models. In MCA, a modular encoder ﬁrst extracts
information inside each single headset-mounted camera view. This is followed by
a modular synthesizer that estimates a full face expression along with its blend-
ing weights from the information extracted within the same modular branch.
Finally, multiple estimated 3D faces are aggregated from diﬀerent modules and
blended together to form the ﬁnal face output.

Our contributions are threefold. First, we present MCA that introduces mod-
ularity into CA. Second, we extend MCA to solve the expressivity and robustness
issues by learning the blending as well as new constraints in the latent space.

4

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

e
r
a
w
d
r
a
h

s
e
g
a
m

i

T
G

. headset
t
a
t
s

3

person

room

capture

4

6

14

video

1,691

frame

image

203,729

1,833,561

Fig. 3. Hardware and dataset examples. First row: capture dome [4], training headset,
tracking headset, and sample images [1]. Second row: head-mounted camera examples
of left eye, right eye, and mouth from diﬀerent capture runs. Third row: examples of
head-mounted images and ground-truth correspondences. Last row: dataset statistics.

Finally, we demostrate MCA’s robustness and expressiveness advantages on a
real-world VR dataset.

2 Related Work

Morphable 3D Facial Models: Part-based models have been widely used for
modeling facial appearance because of their elasticity and the ability to han-
dle occlusion. Cootes et al. [9] introduced Active Appearance Models (AAM)
for locating deformable objects such as faces. The facial shape and part-based
appearance are iteratively matched to the image, using parameter displacement
estimated from residual errors. The 3D morphable face model by Blanz and Vet-
ter [10] decomposes the face into four parts to augment expressiveness, despite
the fact that PCA decomposition is still computed holistically on the whole face.
Tena et al. [11] proposed region-based face models that use a collection of PCA
sub-models with shared boundaries. Their method achieved semantically mean-
ingful expression bases, while generalising better to unseen data compared to
the holistic approach. Neumman et al. [12] extended sparse matrix decomposi-
tion theory to face mesh sequence processing, and a new way to ensure spatial
locality. Cao et al. [13] proposed to learn part-based rigidity prior from existing

Expressive Telepresence via Modular Codec Avatars

5

facial capture data [14], which was used to impose rigid stability and avoid head
pose jittery in real-time animation. Recently, Ghafourzadeh et al. [15] presented
local PCA-based model combined with anthropometric measurement to ensure
expressiveness and intuitive user control. Our approach also decomposes the face
into part-based modules. However, instead of using linear or shallow features on
the 3D mesh, our modules take place in latent spaces learned by deep neural
networks. This enables capturing of complex non-linear eﬀects, and producing
facial animation with a new level of realism.

Deep Codec Avatars: Human perception is particularly sensitive to detecting
the realism of facial animation, e.g. the well-known Uncanny Valley Eﬀect [16].
Traditional approaches such as morphable 3D models usually fail to pass the
uncanny valley. Recent deep codec avatars have brought new hope to overcome
this issue. Lombardi et al. [4] introduced Deep Appearance Models (DAM) that
use deep Variational Auto-Encoders (VAE) [17] to jointly encode and decode
facial geometry and view-dependent appearances into a latent code. The usage
of deep VAE enabled capturing complex non-linear animation eﬀects, while pro-
ducing a smooth and compact latent representation. View-dependent texture
enabled modeling view-dependent eﬀects such as specularity, as well as allowing
correcting from imperfect geometry estimation. Their approach enabled realis-
tic facial animation without relying on expensive light-transport rendering. Re-
cently, Lombardi et al. [18] extended their prior work to Neural Volumes (NV).
They presented a dynamic, volumetric representation learned through encoder-
decoder neural networks using a diﬀerentiable ray-marching algorithm. Their
approach circumvents the diﬃculties in conventional mesh-based representation
and does not require explicit reconstruction or tracking. The work in [4] is an
important cornerstone that our work is built upon. Our work diﬀers in that our
main focus is producing robust and accurate facial animations in realtime.

VR Telepresence: The previous work that is most closely related to ours is
Wei et al. [1]. VR telepresence presents many unique challenges due to its novel
hardware setup, e.g. unaccommodating camera views that can only see parts
of the face, as well as the image domain gap from head-mounted cameras to
realistic avatars. In Thies et al. [6] and Cao et al. [19], a well-posed, unobstructed
camera is used to provide image input for realtime reenactment. VR telepresence
is diﬀerent in that clear view of the full face is unavailable because the user has
to wear the VR headset. Li et al. [20] presented an augmented VR headset that
contains an outreaching arm holding an RGB-D camera. Lombardi et al. [4] used
cameras mounted on a commodity VR headset, where telepresence was achieved
by image-based rendering to create synthetic head-mounted images, and learning
a common representation of real and synthetic images. Wei et al. [1] extended
this idea by using CycleGAN [21] to further alleviate the image domain gap.
A training headset with 9 cameras was used to establish correspondence, while
a standard headset with 3 cameras was used to track the face. Existing work
in VR telepresence are based on holistic face modeling. In our work, we revive
the classic module-based approach and combine it with codec avatars to achieve
expressive and robust VR telepresence.

6

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

3 Hardware & Dataset

In this section, we ﬁrst describe our hardware setup. After that, we provide more
details about the dataset that we use to train and evaluate our model.

Gathering high quality facial image data is the foundation of realistic facial
animation. For this purpose, we use a large multi-camera dome that contains 40
cameras capturing images at a resolution of 2560×1920 pixels. Cameras lie on
the frontal hemisphere of the face at a distance of about 1 meter. The cameras
are synchronized with a frame rate of 30fps. 200 LED lights are directed at the
face to create uniform illumination. To collect headset images, we used a two-
headset design that has a training headset and a tracking headset. The training
headset contains 9 cameras, which ensures establishing high-quality expression
between head-mounted cameras and the avatar. The tracking headset contains
a subset of 3 cameras, it is a consumer-friendly design with minimally intrusive
cameras for real-time telepresence. The images are captured by IR cameras with
a resolution of 640×480 and frame rate of 30fps (down-sampled from 90fps). We
refer to [4] and [1] for more details about the camera dome and the VR headsets.
To obtain the high-ﬁdelity facial avatar, we train a person-speciﬁc view-
dependent VAE using 3D face meshes reconstructed and tracked from dome-
captured data similar to [4]. The mesh contains 7306 vertices and a texture
map of 1024×1024. We then use the method in [1] to establish correspondences
between training headset frames and avatar latent codes. We decode these cor-
responding latent codes into 3D face meshes. These serve as the ground truth
outputs in our dataset. The 9-camera training headset is only used for obtaining
accurate ground truth. Both training and evaluation of our model in later sec-
tions only uses the subset of 3 cameras on the tracking headset. Note that the
method for establishing correspondences provides accurate ground truth output,
but it is infeasible for real-time animation due to its expensive computational
complexity.

We construct a dataset that covers common variations in practical usage
scenarios: varying users, varying headsets and varying environments with diﬀer-
ent backgrounds and lighting conditions. Fig. 3 shows some example data and
statistics of the dataset. Speciﬁcally, our dataset contains four diﬀerent users.
We used three diﬀerent headsets, and captured a total of 14 sessions (half an
hour for each session) in three diﬀerent indoor environments: a small room with
weak light, an oﬃce environment in front of a desk under bright white light,
and in front of a large display screen. In the last environment, we capture some
sections while playing random high-frequency ﬂashing patterns on the screen, to
facilitate the evaluation under extreme lighting condition. Sessions of the same
person may be captured on diﬀerent dates that are months apart, resulting in
potential appearance changes in the captured data. For example, for one person,
we captured him with heavy beard in some sessions and the other sessions are
captured after he shaved.

For each dome capture or headset capture, we recorded a predeﬁned set of
73 unique facial expressions, recitation of 50 phonetically-balanced sentences,
two range-of-motion sequences where the user is asked to move jaw or whole

Expressive Telepresence via Modular Codec Avatars

7

face randomly with maximum extend, and 5-10 minutes conversation. Finally,
we split the dataset by assigning one full headset capture session of each person
as the testing set. This means the testing set does not have overlap in terms of
capture sessions. We use only sequences of sentence reading and conversation for
testing as they reﬂect the usual facial behaviors of a user in social interactions.

4 Method

The goal of our head-only VR telepresence system is to faithfully reconstruct
full faces from images captured by the headset-mounted cameras in realtime. In
this section, we will ﬁrst describe the formulation of our model, followed by two
important techniques that are important for successfully training the models and
lastly implementation details.

4.1 Model Formulation

We denote the images captured by the headset-mounted cameras at each time
instance as X={xk | k ∈ {1, ..., K}}, with K the total number of cameras and
xk∈IRI where I is the number of pixels in each image. Note the time subscript
t is omitted to avoid notation clutter. We denote v∈IR3 as the direction from
which the avatar is to be viewed in VR. Given X, the telepresence system needs
to compute the view-dependent output yv which contains the facial geometry
yg∈IR3G of 3D vertex positions and the facial texture yt
v∈IR3T corresponding
to view direction v. G and T are the number of vertices on the facial mesh and
number of pixels on the texture map respectively.

The holistic Codec Avatar (CA) [1,4] is formulated as a view-dependent

encoder-decoder framework, i.e.

yv = D (c, v) , c = E (X)

(1)

where headset cameras’ images X are feed into an image encoder E to produce
the expression code of the whole face, and a decoder D produces the view-
dependent 3D face. D is trained on dome-captured data to ensure animation
quality, and E is trained using the correspondences between X and c that are
established using the method in [1]. Because D is a neural network trained with a
limited set of facial expressions, CA has limited expressiveness in out-of-sample
expressions due to the long tail distribution nature of human facial expressions.
In this work we propose Modular Codec Avatar (MCA), where the 3D face
modules are estimated by an image encoder followed by a synthesizer from each
headset camera view, and blended together to form the ﬁnal face. MCA can be
formulated as:

yv =

K
(cid:88)

k=1

wk (cid:12) Dk

(cid:0)cfull

k , v(cid:1)

(2)

8

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

(cid:2)cfull

k , wk

(cid:3) = Sk

(cid:0)cpart

k

(cid:1) , cpart

k = Ek (xk)

(3)

where each camera view xk is processed separately. This computation consists of
three steps. Firstly, a modular image encoder Ek estimates the modular expres-
sion code cpart
k which only models the facial part visible in the kth camera view,
e.g., left eye. Subsequently, a modular synthesizer Sk estimates a latent code for
the full-face denoted as cfull
. The syn-
thesizer also estimates blending weights wk. Lastly, we aggregate the results from
all K modules to form the ﬁnal face: decode the 3D modular faces using Dk, and
blend them together using the adaptive weights. (cid:12) represents the element-wise
multiplication. In this way, MCA learns part-wise expressions inside each face
module, while keeping full ﬂexibility of assembling diﬀerent modules together.

k , based only on the information from cpart

k

The objective function for training the MCA model consists of three loss

terms for the reconstruction and intermediate latent codes:

LMCA = (cid:107)yv0 − ˆyv0(cid:107)2 + λ1

K
(cid:88)

k=1

(cid:13)
(cid:13)cfull

k − ˆc(cid:13)

(cid:13)2 + λ2

K
(cid:88)

k=1

(cid:13)
(cid:13)cpart

k − ˆcpart

k

(cid:13)
(cid:13)2

(4)

k

where ˆyv0 , ˆc, and ˆcpart
denote diﬀerent supervision signals. The ﬁrst term mea-
sures the reconstruction error in facial geometry and texture. We set ˆyv0 =D(ˆc, v0)
as the facial geometry and texture decoded from the supervision code ˆc from
frontal view direction v0. Note the supervision ˆc is estimated from the correspon-
dence stage using the method in [1]. We impose equal weights for reconstruction
error in geometry and texture. For the texture, we average pool the reconstruc-
tion errors of the pixels corresponding to the same closest mesh vertex instead of
averaging over all pixels on the texture map. This vertex-based pooling ensures
that texture loss has a geometrically uniform impact. The second term encour-
ages each module to produce independent estimation of the correct full-face and
the last term directs each encoder to produce correct modular expression code.
Section 4.2 describes in detail how we generate the supervision ˆcpart

.

k

4.2 Exemplar-based Latent Alignment

k

k have diﬀerent purposes. cpart

The two latent codes cpart
and cfull
represents infor-
k
mation only within its responsible modular region, while cfull
further synthesizes
a full-face based on the single-module expression information. It is important to
have cpart
, because otherwise the modules will only collectively try to recover
the same full-face code through cfull
k , which essentially degrades to the holistic
CA. The key to ensure the eﬀectiveness of cpart
is through crafting proper super-
vision signal ˆcpart
. The main challenge is ˆcpart
resides in an inexplicitly deﬁned
latent space. We address this problem with exemplar-based latent alignment.

k

k

k

k

k

To obtain ˆcpart

k

, we ﬁrst train a separate VAE for each module from dome-
captured data. It has a similar architecture as CA, but only uses the region
within the module by applying a modular mask on both the VAE input and
output. We denote the masked modular VAE decoder as Dmask
, and the set
of codes corresponding to dome-captured modular faces as Cmask
. The main

k

k

Expressive Telepresence via Modular Codec Avatars

9

k

challenge to obtain ˆcpart
is the domain gap between dome-captured and headset-
captured data. Directly applying the trained modular VAE on masked ground
truth often result in spurious code vectors, i.e., the dome code and headset code
corresponding to similar expression content do not match. This is caused by
lighting and appearance diﬀerences between the two drastically diﬀerent capture
setup. Moreover, the domain gap also exist between diﬀerent headset capture
runs. To overcome this mismatch, we use exemplar-based latent alignment that
replace the headset-captured codes produced by the decoder by their nearest
dome-captured exemplar code. This eﬀectively calibrates ˆcpart
from diﬀerent
capture runs to a consistent base, i.e.

k

ˆcpart
k = argmin
c∈Cmask
k

(cid:13)
(cid:13)Dmask
k

(c, v0) − ˆymask
v0,k

(cid:13)
(cid:13)2

(5)

where ˆymask
v0,k is the modular masked ˆyv0 . This diﬀers from pure image-based
synthesis in that result must come from a set of known dome-captured exemplars
Cmask
k

is then used in Eq.(4) to train MCA.

. The resulting ˆcpart

k

4.3 Modulated Adaptive Blending

The blending weights wk can be fully learned from data. However, we ﬁnd auto-
matically learned blending weights lead to animation artifacts. This is because
module correlation exists in training data, e.g. left and right eyes often open and
close together. Therefore, the dominant module interchanges between left and
right eyes across nearby pixels, which results in jigsaw-like artifacts in the eye-
ball region. To overcome this issue and promote spatial coherence in the modular
blending weights, we add a multiplicative and additive modulation signals to the
adaptively learned blending weights. This ensures blending weight is constant
near the module centroid, and the importance of adaptively learned weights
gradually increases, i.e.

wk =

(cid:18)

1
w

(cid:12)

wS

k (cid:12) e−

max{(cid:107)u−uk(cid:107)2−ak ,0}
σ2

+ bk1

(cid:110)
(cid:107)u − uk(cid:107)2 ≤ ak

(cid:111)(cid:19)

(6)

where wS
k denotes the adaptive blending weights produced by the synthesizer Sk,
u denotes the 2D texture map coordinates corresponding to each vertex, uk, ak,
and bk are constants denoting the module’s centroid, area in the texture map,
and constant amplitude within its area. w is computed vertex-wise to normalize
the blending weights across all modules.

4.4 Implementation Details

We use K=3 modules following our hardware design, with three head-mounted
cameras capturing left eye, right eye, and mouth. Each xk is resized to a dimen-
sion of 256×256. We use a latent dimension of 256 for both ˆcpart
and ˆc. Similar
neural network architectures to [1] are used for our encoder Ek and decoder Dk.

k

10

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

For the decoder, we reduce the feature dimensions to remedy computation cost
due to decoding K times. We also set all Dk to share the same weights to save the
capacity requirement for storage and transmission of the model. The synthesizer
Sk consists of three temporal convolution layers [22] (TCN), with connections
to a small temporal receptive ﬁeld of 4 previous frames. The blending weights
wS
k is predicted through transposed convolution layers with a sigmoid function
at the end to ﬁt the texture map resolution. The texture weights were then
reordered to produce geometry weights using the correspondence between ver-
tices and texture map coordinates. We also add cross-module skip connections
that concatenates the last layer features in Ek to allow the model to exploit
correlations between images from diﬀerent headset-mounted cameras.

We train the model with the Adam optimizer with both λ1 and λ2 set to
1. We augment the headset images by randomly cropping, zooming, and rotat-
ing the images. We also add random gaussian noise to the modular code ˆcpart
with diagonal covariance determined by the distance to the closest neighbour
to prevent overﬁtting. The training is completed using four Tesla V100 GPUs.
During test time, the telepresence system using MCA takes in average 21.6ms to
produce one frame in VR, achieving real-time photo-realistic facial animation.

k

5 Experiments

The ﬁrst experiment illustrates the advantage of MCA over CA modeling un-
trained facial expressions. We then provide detailed evaluation results of the
full VR telepresence via MCA comparing to CA. Our experiment evaluates the
performance from extensive perspectives, including expression accuracy and per-
ceptive quality. We also provide detailed ablation study and discuss failure cases.
Finally, we show two extensions of our method which may be useful under certain
usage scenarios.

5.1 Facial Expression Modeling: Modular (MCA) vs. Holistic (CA)

In the ﬁrst experiment, we show the advantage of modeling modular expressions
in real world VR telepresence scenarios. Towards this goal, we train a holistic
VAE [1,4] as well as modular VAEs on dome-captured data. For both approach,
we apply agglomerative clustering on the resulting latent codes of the training
set data with varying number of clusters to represent the corresponding model’s
capacity of representing facial expressions. Recall that for headset-captured data,
we have their estimated ground truth 3D face through the correspondence stage
using the method in [1]. We then use these 3D faces to retrieve the closest
cluster centers by matching the pixel values within each modular region of the
rendered frontal faces, and report the root mean squared error (RMSE) of the
pixel values in Fig. 4. For fair comparison, we use K=3 times more clusters for
the CA baseline. Fig. 4 shows the result.

It can be seen from Fig. 4 that MCA consistently produces lower matching
errors than CA throughout all subjects and model capacity levels. Intuitively

Expressive Telepresence via Modular Codec Avatars

11

person1

person2

person3

person4

Fig. 4. Comparing holistic versus modular by isolating and evaluating the importance
expressiveness. X-axis shows diﬀerent capacities of the face expressions by the number
of clusters. Y-axis shows the RMSE photometric error. Note that CA uses proportion-
ally K times more clusters than each module of MCA for fair comparison.

this implies that MCA generalizes better to untrained expressions. The gap
between CA and MCA increases as the number of clusters increases, indicating
MCA’s advantage increases as the face modeling becomes more ﬁne-grained. It
is then clear that by modeling modular expressions in the MCA, we can more
truthfully interpolate/extrapolate facial expressions that are not contained in
the dome-captured data, achieving better expressiveness in telepresence.

5.2 Full VR Telepresence

We evaluate the performance of the full VR telepresence system. We feed headset
images as input to the model, and evaluate the 3D face output against the
ground-truth. Six metrics are used for comprehensive evaluation and the results
are reported in Table 1.
Expression Accuracy The most important metrics are MAE (i.e. Mean Ab-
solute Error) and RMSE on pixels in the rendered frontal view face, as they
directly and metrically measure the accuracy of VR telepresence. To further de-
compose the error into geometry and texture components, we compute RMSE
on vertex position (Geo.) and texture map (Tex.) respectively as well. MCA con-
sistently outperform CA on all these metrics on almost all identities’ test data.
These results suggest that MCA can more truthfully recover the 3D face given
only the partial views of the face from headset-camera images than CA, leading
to more expressive and robust telepresense.
Improvement Consistency As these metric values are computed from average
of all frames, we need to verify that the improvement of MCA is not due to large
performance diﬀerences on only a small set of frames. To do that, we compute
the percentage of frames for which MCA produces more accurate results than
CA and vice versa. This is reported as %-better in Table 1. From the results, it
is clear that MCA consistently outperform CA across frames. For example, for
person3, on 99.7% frames MCA produces more accurate results than CA.
Perceptive Quality We also want to verify that such accuracy improvements
align with human perception, i.e. whether a user may actually feel the improve-
ment. Quantitatively, we compute structural similarity index on the grey-level
frontal rendering (SSIM) [23] which is a perception-based metric that consid-
ers image degradation as perceived change in structural information, while also

12

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

Geo.↓

MAE↓

RMSE↓

8.82
4.44
9.09
3.33
6.54

method CA Ours CA Ours
7.47
person1
3.84
person2
6.66
person3
3.04
person4
5.48
overall

1.14
1.46
0.84
0.64
1.17
Table 1. Quantitative results of our main experiment for evaluating the full VR telep-
resence system robustness. MCA outperforms CA across various metrics. Please refer
to the text for details.

%-better↑
Tex.↓
CA Ours CA Ours CA Ours
63.7
1.26
72.7
1.82
99.7
1.14
58.9
0.54
70.7
1.37

Ours
CA
0.954 0.957
0.949 0.951
0.933 0.942
0.984
0.984
0.953 0.956

3.02
2.04
3.43
0.85
2.44

8.69
4.26
6.97
3.21
6.17

7.67
4.00
8.36
3.08
5.81

3.40
2.05
4.58
0.86
2.72

36.3
27.3
0.3
41.1
29.3

SSIM↑

blend
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

end2end
-
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

soft-ex.
-
-
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)

dimen.
-
-
-
-
(cid:88)
(cid:88)
(cid:88)

skip-mod.
-
-
-
-
-
(cid:88)
(cid:88)

tconv.
-
-
-
-
-
-
(cid:88)

RMSE ∆↓

7.33
6.67
6.10
5.71
5.64
5.52
5.48

-
0.66
0.57
0.39
0.07
0.08
0.04

Table 2. An ablation study of our method showing the progression and contribution
to the overall performance improvement on the main RMSE metric.

incorporating important perceptual phenomena [24]. These results are reported
in the last column in Table 1: MCA outperforms CA on three persons while is
on-par on the last one. This hints that the accuracy improvements of MCA over
CA align with perception improvement. Fig. 5 shows a qualitative comparison
between MCA and CA. It can be seen that MCA is better at handling subtle
combined expressions, e.g. mouth open while eyes half-open, showing teeth while
eyes shut, and mouth stretched while looking down. Renderings from diﬀerent
viewpoint by varying v are shown in Fig. 6. It can be seen that MCA produces
consistent results from diﬀerent viewing directions.

Albation Study Table 2 shows an ablation study of MCA. Ablation factors
range from the usage of synthesized blending weights instead of equal weights
(blend), training the network end-to-end (end2end), using soft latent part vectors
with gaussian noise instead of hard categorical classes (soft-ex.), expanding the
dimension of latent codes (dimen.), using skip-module connections so all images
are visible to the modular encoder (skip-mod.), and using temporal convolution
on the synthesizer (tconv.). It can be seen the former three techniques improve
performance signiﬁcantly, while extra connections such as skip and temporal
convolution lead to further improvements. We refer to supplemental material for
more details.

Failure Cases Fig. 7 shows example failure cases. Strong background ﬂash is a
challenging problem even for MCA, leading to inaccurate output (left of Fig. 7).
Although MCA can produce more expressive 3D faces, extreme asymmetric ex-
pressions like one pupil in the center while the other roll all the way to the corner
as in the right of Fig. 7 still remains challenging to be faithfully reconstructed.

Expressive Telepresence via Modular Codec Avatars

13

1
n
o
s
r
e
p

2
n
o
s
r
e
p

3
n
o
s
r
e
p

4
n
o
s
r
e
p

CA [1,4]

MCA(ours)

GT

CA [1,4]

MCA(ours)

GT

Fig. 5. Qualitative VR telepresence results. Compared to the holistic approach (CA),
MCA handles untrained subtle expressions better.

Fig. 6. Qualitative results of MCA from diﬀerent viewing directions by varying v.
MCA produces natural expressions that are consistent across viewpoints.

5.3 Extensive Applications

Flexible animation: Making funny expressions is part of social interaction. The
MCA model can naturally better facilitate this task due to stronger expressive-
ness. To showcase this, we shuﬄe the head-mounted image sequences separately

14

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

image

MCA

GT

image

MCA

GT

Fig. 7. Failure cases of MCA. Typical failure cases include interference from strong
background ﬂash, extreme asymmetry between the eyes, and weakened motion.

image

MCA

image

MCA

original

ampliﬁed

original

ampliﬁed

Fig. 8. Two new applications enabled by the MCA model. Left side shows two examples
of ﬂexible animation. Right side shows two examples of eye ampliﬁcation.

for each module, and randomly match them to simulate ﬂexible expressions. It
can be seen from Fig. 8 that MCA produces natural ﬂexible expressions, even
though such expressions have never been seen holistically in the training set.
Eye ampliﬁcation: In practical VR telepresence, we observe users often do not
open their eyes to the full natural extend. This maybe due to muscle pressure
from the headset wearing, and display light sources near the eyes. We introduce
an eye ampliﬁcation control knob to address this issue. In MCA, this can be
simply accomplished by identifying the base cpart
that correspond to closed
eye, and amplifying the latent space distance by multiplying a user-provided
ampliﬁcation magnitude. Fig. 8 shows examples of amplifying by a factor of 2.

k

6 Conclusion

We addressed the problem of VR telepresence, which aimed to provide re-
mote and immersive face-to-face telecommunication through VR headsets. Codec
Avatar (CA) utilized view-dependent neural networks to achieve realistic facial
animation. We presented a new formulation of codec avatar named Modular
Codec Avatar (MCA). This paper combines classic module-based face mod-
eling with codec avatars in VR telepresence. We presented several important
techniques to realize MCA eﬀectively. We demonstrated that MCA achieves im-
proved expressiveness and robustness through experiments on a comprehensive
real-world dataset that emulated practical scenarios. New applications in VR
telepresence enabled by the proposed model were ﬁnally showcased.

Expressive Telepresence via Modular Codec Avatars

15

References

1. Wei, S.E., Saragih, J., Simon, T., Harley, A.W., Lombardi, S., Perdoch, M., Hypes,
A., Wang, D., Badino, H., Sheikh, Y.: Vr facial animation via multiview image
translation. In: SIGGRAPH. (2019)

2. Heymann, D.L., Shindo, N.: Covid-19: what is next for public health? The Lancet

(2020)

3. Orts-Escolano, S., Rhemann, C., Fanello, S., Chang, W., Kowdle, A., Degtyarev,
Y., Kim, D., Davidson, P.L., Khamis, S., Dou, M., et al.: Holoportation: Virtual
3d teleportation in real-time. In: UIST. (2016)

4. Lombardi, S., Saragih, J., Simon, T., Sheikh, Y.: Deep appearance models for face

rendering. In: SIGGRAPH. (2018)

5. Tewari, A., Bernard, F., Garrido, P., Bharaj, G., Elgharib, M., Seidel, H.P., P´erez,
P., Zollhofer, M., Theobalt, C.: Fml: face model learning from videos. In: CVPR.
(2019)

6. Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C., Nießner, M.: Face2face:

Real-time face capture and reenactment of rgb videos. In: CVPR. (2016)

7. Elgharib, M., BR, M., Tewari, A., Kim, H., Liu, W., Seidel, H.P., Theobalt,
C.: Egoface: Egocentric face performance capture and videorealistic reenactment.
arXiv:1905.10822 (2019)

8. Nagano, K., Seo, J., Xing, J., Wei, L., Li, Z., Saito, S., Agarwal, A., Fursund, J.,
Li, H., Roberts, R., et al.: Pagan: real-time avatars using dynamic textures. In:
SIGGRAPH. (2018)

9. Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active appearance models. TPAMI

23(6) (2001) 681–685

10. Blanz, V., Vetter, T., et al.: A morphable model for the synthesis of 3d faces. In:

SIGGRAPH. (1999)

11. Tena, J.R., De la Torre, F., Matthews, I.: Interactive region-based linear 3d face

models. In: SIGGRAPH. (2011)

12. Neumann, T., Varanasi, K., Wenger, S., Wacker, M., Magnor, M., Theobalt, C.:

Sparse localized deformation components. TOG 32(6) (2013) 1–10

13. Cao, C., Chai, M., Woodford, O., Luo, L.: Stabilized real-time face tracking via a

learned dynamic rigidity prior. TOG 37(6) (2018) 1–11

14. Cao, C., Weng, Y., Zhou, S., Tong, Y., Zhou, K.: Facewarehouse: A 3d facial

expression database for visual computing. TVCG 20(3) (2013) 413–425

15. Ghafourzadeh, D., Rahgoshay, C., Fallahdoust, S., Aubame, A., Beauchamp, A.,
Popa, T., Paquette, E.: Part-based 3d face morphable model with anthropometric
local control. In: EuroGraphics. (2020)

16. Seyama, J., Nagayama, R.S.: The uncanny valley: Eﬀect of realism on the impres-
sion of artiﬁcial human faces. Presence: Teleoperators and virtual environments
16(4) (2007) 337–351

17. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv:1312.6114

(2013)

18. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.:
Neural volumes: Learning dynamic renderable volumes from images. TOG 38(4)
(2019) 65

19. Cao, C., Hou, Q., Zhou, K.: Displaced dynamic expression regression for real-time

facial tracking and animation. TOG 33(4) (2014) 1–10

20. Li, H., Trutoiu, L., Olszewski, K., Wei, L., Trutna, T., Hsieh, P.L., Nicholls, A.,
Ma, C.: Facial performance sensing head-mounted display. TOG 34(4) (2015) 1–9

16

H. Chu, S. Ma, F. De la Torre, S. Fidler, and Y. Sheikh

21. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation

using cycle-consistent adversarial networks. In: ICCV. (2017)

22. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional

and recurrent networks for sequence modeling. arXiv:1803.01271 (2018)

23. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from

error visibility to structural similarity. TIP 13(4) (2014) 600–612

24. Wikipedia: Structural similarity. https://en.wikipedia.org/wiki/structural_

similarity

