0
2
0
2

r
a

M
7
2

]

V

I
.
s
s
e
e
[

1
v
2
2
3
2
1
.
3
0
0
2
:
v
i
X
r
a

LIGHT FIELD IMAGE CODING USING DUAL DISCRIMINATOR GENERATIVE
ADVERSARIAL NETWORK AND VVC TEMPORAL SCALABILITY

Nader Bakir∗†, Wassim Hamidouche∗, Sid Ahmed Fezza§, Khouloud Samrouth† and Olivier D´eforges∗

∗Univ. Rennes, INSA Rennes, CNRS, IETR - UMR 6164, Rennes, France
†Lebanese University, Tripoli, Lebanon
§National Institute of Telecommunications and ICT, Oran, Algeria
whamidou@insa-rennes.fr

ABSTRACT
Light ﬁeld technology represents a viable path for providing
a high-quality VR content. However, such an imaging system
generates a high amount of data leading to an urgent need for
LF image compression solution.
In this paper, we propose
an efﬁcient LF image coding scheme based on view synthe-
sis. Instead of transmitting all the LF views, only some of
them are coded and transmitted, while the remaining views
are dropped. The transmitted views are coded using Versatile
Video Coding (VVC) and used as reference views to synthe-
size the missing views at decoder side. The dropped views are
generated using the efﬁcient dual discriminator GAN model.
The selection of reference/dropped views is performed us-
ing a rate distortion optimization based on the VVC tempo-
ral scalability. Experimental results show that the proposed
method provides high coding performance and overcomes the
state-of-the-art LF image compression solutions.

Index Terms— Light Field, Deep Learning, D2GAN,

VVC, Coding Structure, RDO.

1. INTRODUCTION

Light Field (LF) image can be captured and sampled by a
plenoptic camera composed of an array of microlens such
as Lytro and Raytrix cameras. LF imaging has many advan-
tages over traditional 2D imaging systems, as its allows the
user to change various camera settings after capture, thus pro-
viding more ﬂexibility. Speciﬁcally, LF image captures both
spatial and angular information enabling several multimedia
services: multi-focus, multi-perspective, viewpoint rendering
and even 6-Degree of Freedom (6-DoF) viewing [1, 2]. A
light ﬁeld can be described by a 4 dimensional function with
2 parallel plans s, t and u, v denoted by L(u, v, s, t). There
are several ways to represent a LF image, including micro-
image, epipolar image and sub-aperture image [3]. The latter,
illustrated in Fig. 1, is the the widely used representation.

However, in sight of the huge amount of data involved
by LF image, its processing, storage and transmission raise a
real challenge that received increased research attention. In

response, several solutions have been proposed to encode a
LF image in sub-aperture representation. The straight for-
ward coding approach organizes the LF views in a pseudo
video sequence, which is then encoded with a classical 2D
hybrid video encoder [4, 5, 6]. Another approach consists in
encoding a spare set of views using a video encoder, while the
rest of views are synthesized at the decoder side. The latter
solution has been followed by several authors [7, 8, 9], for in-
stance, linear approximation has been investigated in [7] to es-
timate the views at the decoder from neighbour views, while a
combination of linear approximation and convolutional neural
network has been proposed in [8] to synthesize missing views
at the decoder side. In the same way, Jia et al. [9] proposed
to use the generative adversarial network to generate unsam-
pled views. To enhance the coding efﬁciency, the authors pro-
posed to encode and transmit the residual error between the
generated uncoded views and their original versions. Jiang
et al. [10] proposed a coding method called Homography-
based Low Rank Approximation. This method jointly op-
timizes multiple homographies that align the LF views and
low rank approximation matrices. Hou et al. [11] proposed a
method that exploits the inter- and intra-view correlations ef-
fectively by characterizing its particular geometrical structure
using both learning and advanced video coding techniques.

In this paper, we propose an efﬁcient approach to encode
the LF images, which consists in encoding a sparse set of
views, and estimate the rest of views at the decoder side. In
particular, the ﬁrst set of selected reference views are coded
with the next generation video coding standard called Ver-
satile Video Coding (VVC). While the second set of views
are either synthesized from the ﬁrst decoded set of views
using a Dual Discriminator Generative Adversarial Network
(D2GAN) or decoded by a VVC decoder. The D2GAN have
been trained with a large set of LF images coded at different
distortions. The architecture offered by the D2GAN, com-
posed by a generator and two discriminators, enables better
training and thus synthesizes views with high visual quality.
In addition, to increase the coding efﬁciency, a rate distor-
tion optimization (RDO) is adopted to select which views

 
 
 
 
 
 
player minimax optimization game

max
D1,D2

 (G, D1, D2) = α Ex(cid:118)Pdata [log D1(x)]

min
G
+ Ez(cid:118)Pz [−D1(G(z))] + Ex(cid:118)Pdata [−D2(x)]
+ β Ez(cid:118)Pz [log D2(G(z))],

(1)

where z is a noise vector, E represents expected value, x is the
real data, P represents the probability distribution, α and β
are two hyper-parameters (0 < α, β ≤ 1) to stabilize the
learning of the model and control the effect of KL and reverse
KL divergences on the optimization problem [15].

More speciﬁcally, with a batch of M noise samples
z(1), z(2), ..., z(M ) given as inputs, the generator generates
M artiﬁcial samples, and this process is deﬁned as G(z(i)).
While, x(1), x(2), ..., x(M ) represents a batch of M real data
samples.

Three cost functions deﬁned in (2), (3) and (4) are com-
puted to obtain the error that should be transmitted respec-
tively to D1, D2 and G for their backward weights updating,
as shown in Fig. 2 (dash lines).

∇θD1

∇θD2

1
M

1
M

M
(cid:88)

[α log D1(x(m)) − D1(G(z(m)))],

(2)

m=1

M
(cid:88)

[β log D2(G(z(m))) − D2(x(m))],

(3)

m=1

∇θG

1
M

M
(cid:88)

[β log D2(G(z(m))) − D1(G(z(m)))].

(4)

m=1

In this work, we use D2GAN to synthesize the dropped
LF views, where the generator consists of two conventional
neural networks (CNN) [16]. The ﬁrst CNN estimates the
disparity and the second one generates the color image.

2.2. Versatile Video Coding

Based on High Efﬁciency Video Coding (HEVC), the Joint
Video Exploration Team (JVET) is developing a new video
coding standard called Versatile Video Coding (VVC) [17].
VVC already enables a bitrate saving of 35% to 40% with re-
spect to HEVC for the same visual quality [18]. VVC intro-
duces several new coding tools at different levels of the cod-
ing chain including frame partitioning, Intra/Inter predictions,
transform, quantization, entropy coding and in-loop ﬁlters.
For more details about the VVC coding tools the reader can
refer to [19]. VVC supports by design the temporal scalability
through the Random Access (RA) coding conﬁguration. This
latter, illustrated in Fig. 3, enables different temporal layers
and each temporal layer uses as reference only frames from
lower temporal resolution, i.e., lower layer. Therefore, frames
of each temporal layer ti can be removed without impacting
the decoding of frames of lower temporal resolution tj with
ti > tj.

Fig. 1. Illustration of the Light Field image as an array of u,
v slices arranged in s, t.

should be encoded and transmitted and which ones should be
dropped and synthesized at the decoder side.

The remainder of this paper is organized as follows. Sec-
tion 2 describes the concepts of D2GAN and VVC. Then, in
Section 3, we describe the proposed LF image compression
solution. Section 4 presents and discusses the experimental
results. Finally, Section 5 concludes this paper.

2. BACKGROUND

As mentioned in Section 1, the proposed coding approach is
based on Dual Discriminator Generative Adversarial Network
(D2GAN) and Versatile Video Coding (VVC) standard.
In
this section, we brieﬂy introduce these two concepts.

2.1. Dual Discriminator Generative Adversarial Nets

Generative Adversarial Networks (GANs) are deep neural net
architectures composed of two consecutive neural network
models, namely generator G and discriminator D. GAN en-
ables to simultaneously train the two models: the generative
model G that captures the data distribution, and the discrim-
inative model D that estimates the probability that a sample
came from the training data rather than from the generator
G [12]. GAN has recently achieved great success in various
ﬁelds, especially in fake video generation, super-resolution
and objects detection [13, 14].

Dual discriminator generative

adversarial network
(D2GAN), is a novel framework based on GAN, which uses
two discriminators D1 and D2, where D1 tries to assign high
scores for real data, and D2 tries to assign high scores for
the fake data. This technique uses the two discriminators to
minimize the Kullback-Leibler (KL) divergence and reverse
KL between the generated image and the target image [15].

Formally, D1, D2 and G now play the following three

Fig. 2. Dual discriminator generative adversarial networks (D2GAN) architecture.

Fig. 3. Hierarchical prediction structure in VVC in Random
Access (RA) coding conﬁguration.

In the proposed coding approach, we exploit the concept
of temporal resolution to drop views at the encoder without
impacting the decoding process and thus performing the best
rate distortion performance.

3. PROPOSED LF IMAGE COMPRESSION METHOD

The idea behind the proposed coding method is, instead of
transmitting all the LF views, to drop a sub-set of views at
the encoder side and synthesize them at decoder side, thus
considerably reducing the required bitrate for LF images. To
efﬁciently achieve that, we exploit the temporal scalability of
VVC and use the D2GAN model, all within a rate distortion
optimization process (RDO).

At the encoder side, ﬁrst, LF sub-aperture views are or-
ganized into groups of 16 views that form Groups of Pictures
(GOPs), as illustrated in Fig. 3. Next, in each GOP, the im-
ages of temporal levels 0, 1 and 2 are encoded using the VVC
codec, which constitute the reference views used later in the
synthesis process at the decoder side. Then, the images at
the remaining levels 3 and 4 are either coded using the VVC
codec or dropped. In contrast to ﬁx the number of dropped
views, in our approach this is done adaptively on the basis of
the proposed RDO process described in the Algorithm 1 and

Algorithm 1 RDO block based Lagrangian optimization
Require: J ← { ∀ m, ∀ v ∈ T L#[3 or 4], J = D + λ R}

m: metod {VVC, D2GAN}
for all v ∈ T L#4 do

if J (V V C) < J (D2GAN ) then

Encode v by VVC
ﬂag(v) ← False

else

generate v by D2GAN
ﬂag(v) ← True

end if
end for
for all v ∈ T L#3 do

if J (V V C) < J (D2GAN ) then

Encode v by VVC
ﬂag(v) ← False

else {ﬂag(previous(v)) and ﬂag(next(v))}

generate v by D2GAN
ﬂag(v) ← True

end if
end for

explaining in the following.

As illustrated in Fig. 3, we apply RDO process on the 3
consecutive frames, i.e., frame i at level 4, frame i + 1 at level
3 and frame i+2 at level 4. It should be noted that if one of the
views at temporal level 4 (frame i or i + 2) must be encoded
using VVC, then the frame i + 1 at level 3 is also encoded
using VVC, because this layer will be used as a reference for
the frames at temporal level 4.

Main reasons behind only considering the 2 upper levels
exclusively to the RDO block are, ﬁrstly, after an extensive
study, we found that these levels occupy together around 28%

Convolution LayerReLUBatch Normalization + ReLUFully ConnectedX Real1x1 Conv, 50ReLU3x3 Conv, 100ReLU5x5 Conv, 100ReLU7x7 Conv, 2001x1 Conv, 50 x 3ReLU3x3 Conv, 100ReLU5x5 Conv, 100ReLU7x7 Conv, 5116 ViewsColor CNNDisparity CNNGeneratorQPOriginalReconstructedReLU3x3 Conv, 64BN 64 + ReLU3x3 Conv, 1283x3 Conv, 1283x3Conv,2563x3 Conv,2563x3 Conv, 5123x3 Conv, 5121x1 Conv, 1024BN 128 + ReLUBN 128 + ReLUBN 256 + ReLUBN 256 + ReLUBN 512 + ReLUBN 512 + ReLU3x3 Conv, 64D 1ReLU3x3 Conv, 64BN 64 + ReLU3x3 Conv, 1283x3 Conv, 1283x3Conv,2563x3 Conv, 2563x3 Conv, 5123x3 Conv, 5121x1 Conv, 10241024, FCBN 128 + ReLUBN 128 + ReLUBN 256+ ReLUBN 256 + ReLUBN 512 + ReLUBN 512 + ReLU3x3 Conv, 64D 21024, FCDiscriminatorsBBBBBBBBBBBBBBBBTL#4TL#3TL#2TL#1TL#0POC012345678910111213141516BReferenceRDOViewsTable 1. The average coding gains in terms of BD-BR of
D2GAN, trained with reconstructed views, in comparison
with the anchor D2GAN training with original views.

vs. D2GAN
Reconstructed
vs. D2GAN
Recons. separately

wPSNR-based

SSIM-based

BD-BR

BD-PSNR

BD-BR

BD-SSIM

-11.0%

-16.6%

0.25

0.39

-20.3%

0.013

-25.5%

0.022

of the total bitrate. Second, the views at the upper levels are
not used as references in the VVC coding scheme.

Thus, we proposed a RDO block deciding which views
from the upper level can be encoded using VVC or dropped
and synthesized using D2GAN. To reach this goal, the en-
coder computes the rate distortion (RD) cost function J given
by (5) for both the VVC decoded view and the one synthe-
sized by the D2GAN.

J = D + λ R

(5)

where λ is the Lagrangian multiplier, D is the distortion and
R is the rate in bits per pixel (bpp). To set the Lagrangian
multiplier (λ), we empirically determine its value by testing a
large set of LF images. We found that the value of 0.1 for λ is
optimal and for which the Lagrangian optimization is giving
the best performance.

At the decoder side, the dropped views are synthesized
using D2GAN block. As a reminder, the D2GAN is com-
posed of a generator G and two discriminators D1 and D2.
G consists of two CNNs [16], the ﬁrst CNN estimates the
disparity and the second one generates the color image. A set
of features (mean and standard deviation) of a sparse set of
views (16 views) are fed to the disparity CNN that estimates
the disparity at an intermediate view, and then used it to warp
(backward) all the input views to the intermediate view. The
second color CNN uses all the warped images, derived from
the ﬁrst CNN, to predict the color and synthesizes the dropped
views.

Given that the generator G and discriminators (D1 and
D2) are CNN-based blocks, a training phase is required to ﬁx
respectively their parameters θG, θD1 and θD2. Unlike GAN,
in D2GAN, the scores returned by G are values in R+ rather
than probabilities in [0, 1]. The discriminators and genera-
tor are alternatively updated using stochastic gradient ascent
and descent, respectively. The backward propagation of er-
rors (i.e., cost functions) is applied to update the discrimina-
tors and generator with mini-batch size equal to M , as shown
in Fig. 2.

For the training phase of D2GAN, 3 conﬁgurations were
considered : 1) training with the original views , 2) training
with reconstructed views at multiple distortion levels includ-
ing the original views and 3) one training for each distortion
level separately. We compared the three conﬁgurations, and

the obtained results are given in Table 3. Based on these re-
sults, the third conﬁguration, i.e., D2GAN reconstructed sep-
arately, outperforms the other conﬁgurations and hence we
used it for the D2GAN training.

4. RESULTS AND DISCUSSIONS

4.1. Experimental setup

The proposed deep learning-based architecture described in
the previous section was trained with 140 LF images, where
70 LF images are from EPFL dataset [20], 50 LF images are
from Stanford Lytro LF image dataset [21] and 20 LF im-
ages are from HCI dataset [22]. Each sub-aperture view was
split into patches of size 60×60, thus resulting in more than
150,000 patches that were used in the training phase. For
the testing phase, 9 LF images are selected, 6 LF images are
from EPFL dataset [20], 1 LF image from Stanford Lytro LF
dataset [21] and 2 LF images from HCI dataset [22]. Each
of these LF images is composed of 8×8 sub-aperture views.
These views are rearranged in a pseudo sequence using spi-
ral order scan and coded using VVC in random access (RA)
coding conﬁguration at 4 QP values of 18, 24, 28 and 32.

The training conﬁguration of D2GAN was set as fol-
lows: we trained the generator G and two discriminators (D1
and D2) with the ADAM optimizer by setting β1 = 0.9,
β2 = 0.999, learning rate = 0.0002, batch-size of 10 and ker-
nel size of convolutional layers as depicted in Figure 2. The
regularization coefﬁcients of D1 and D2 were set as α = 0.2
and β = 0.2, respectively. For the generator, we used input
patch of 60×60, stride of 16, and output patch equal to 36×36
(reduced size is due to the convolutions).

4.2. Evaluations

We compared the proposed scheme with four state-of-the-art
methods: 1) VVC-All that encodes all views with the VVC
in RA coding conﬁguration, 2) LF-GAN method proposed
in [9], where a sub-set of views are coded with HEVC, while
the remaining views are generated by GAN and the residual
error of views are transmitted to the decoder, 3) the method
proposed in [4] encodes the views as a pseudo-video sequence
using speciﬁc order scan, 4) the method of Hou el al. [11]
that exploits the Inter- and Intra-views correlation to encode
the views using HEVC. The latter method is considered as the
anchor method.

4.3. Results

The BD-BR [23] is a Peak Signal to Noise Ratio (PSNR)
based metric. It is used in this paper to assess the gain of the
proposed approach compared to the anchor solution. A neg-
ative BD-BR value refers to a bitrate reduction compared to
the anchor method, while a positive value expresses a bitrate
overhead.

Fig. 4. RD curves of the ﬁve considered solutions for the 9 LF images using four QP values.

Table 2. BD-BR and BD-PSNR gains calculated against anchor method described in [4].
Jia et al. [9]
Proposed

Hou et al. [11]

VVC-All

Image

Bikes
DangerDeMort
Flowers
Ankylosaurus Dip1
Aloe
Stone pillars outside
Bedroom
Desktop
Herbs
Average

BD-BR
–11.7%
–7.8%
–12.3%
–13.2%
–26.4%
–18.3%
–5.3%
–19.6%
–26.0%
–15.6%

BD-PSNR
0.72
0.22
0.56
0.44
0.85
0.61
0.46
0.32
1.14
0.59

BD-BR
–6.3%
–10.8%
–11.9%
–14.9%
–9.1%
–15.1%
–4.0%
–7.5%
–4.4%
–8.3%

BD-PSNR
0.48
0.28
0.54
–0.72
0.31
0.52
0.32
0.11
–0.11
0.35

BD-BR
–6.9%
–8.7%
–16.2%
–12.3%
–2.46%
–11.9%
–2.3%
44.1%
6.9%
–0.54%

BD-PSNR
0.49
0.26
0.72
0.39
–0.12
0.28
0.18
–0.61
–0.20
0.15

BD-BR
–22.4%
–16.5%
–16.6%
–18.0%
–42.3%
–35.6%
–9.5%
–26.3%
–29.8%
–24.1%

BD-PSNR
0.96
0.40
0.74
0.57
1.23
0.98
0.85
0.45
1.32
0.83

R-D curves based on PSNR for the 9 LF images are pro-
vided in Fig. 4. We can notice that for all considered im-
ages, the proposed coding method provides the highest perfor-
mance for all bitrates. The previous conclusion is conﬁrmed
by Table 2, providing the Bjøntegaard results of the four cod-
ing solutions compared to the anchor one [4]. The proposed
method achieved an average BD-BR gain of -24.1% and BD-
PSNR of 0.83 dB compared to the anchor method [4].

The complexity of the proposed coding approach is also
evaluated and compared to the other methods on both CPU
and GPU platforms. The performance has been carried-out on
an Intel core i9-7900X CPU running at 3.3GHz PC with 64
GB memory and a TITAN Xp NVDIA GPU. It is important
to note that the GPU is only used when the D2GAN block is

involved in the coding scheme.

Table 3 gives the encoding and decoding run times in
seconds. We can notice that the proposed solution requires
almost the same complexity in the encoding for all QP com-
pared to [9] and [11] methods. The GPU enables to speedup
the encoding part related to the D2GAN block. However, the
decoder of the proposed solution is more complex than the
other solutions due the D2GAN block.

5. CONCLUSION

In this paper, we have proposed a view synthesis based LF im-
age compression approach. In the proposed coding scheme,

0.020.030.040.050.060.070.080.09bitrate (bpp)3637383940YPSNR (dB)Bikes VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.010.020.030.040.050.06bitrate (bpp)3738394041YPSNR (dB)Danger de Mort VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.0250.0300.0350.0400.045bitrate (bpp)38.539.039.540.040.541.041.5YPSNR (dB)Flowers VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.00300.00350.00400.00450.00500.00550.00600.0065bitrate (bpp)44.545.045.546.046.5YPSNR (dB)AnkylosaurusDiplodocus1 VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.0050.0100.0150.0200.0250.0300.0350.0400.045bitrate (bpp)3536373839404142YPSNR (dB)Aloe VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.0050.0100.0150.0200.0250.0300.0350.0400.045bitrate (bpp)35363738394041YPSNR (dB)Stone_Pillars_Outside VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.00350.00400.00450.00500.00550.00600.00650.0070bitrate (bpp)3031323334353637YPSNR (dB)Bedroom VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.0100.0150.0200.0250.0300.0350.0400.045bitrate (bpp)38.539.039.540.040.541.041.5YPSNR (dB)Desktop VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] Proposed0.010.020.030.040.050.060.070.08bitrate (bpp)30323436YPSNR (dB)Herbs VVC_All Jiaetal.[9] Liuetal.[4] Houetal.[11] ProposedTable 3. Processing time in seconds of the four LF image
coding methods.

QP

18
22
28
34
Average

VVC-All
CPU
259
152
101
66
66

Jia et al. [9]
GPU
450
350
220
142
291

Encoder

Hou et al. [11]
CPU
6028
6028
6028
6028
6028

Decoder

Our

CPU
559
452
401
366
445

GPU
449
342
291
256
335

Average

4

53

583

124

94

a set of views are encoded using VCC, while the remaining
views are dropped. The dropped views are synthesized us-
ing enhanced GAN-based approach known as D2GAN. The
transmitted and dropped views are selected using RDO pro-
cess. In addition, in order to avoid impacting the decoder with
the dropped views, the latter are determined according to the
temporal scalability of VCC. All these features allow reduc-
ing bitrate required by LF image, while providing views with
high visual quality.

The experimental results showed the efﬁciency of our
scheme, which achieved bitrate reduction of -24.1% in terms
of BD-BR and increased the visual quality by 0.83 dB in BD-
PSNR with respect to the state-of-the-art solution.

6. REFERENCES

[1] C. Guillemot and R. A. Farrugia, “Light Field Image Process-
ing : Overview and Research Issues,” IEEE COMSOC MMTC
Communications - Frontiers, vol. 14, no. 4, pp. 37–43, 2017.

[2] G. Wu, B. Masia, A. Jarabo, Y. Zhang, L. Wang, Q. Dai,
T. Chai, and Y. Liu,
“Light ﬁeld image processing: An
overview,” IEEE Journal of Selected Topics in Signal Process-
ing, vol. 11, no. 7, pp. 926–954, Oct 2017.

[3] D. Dansereau, “Plenoptic signal processing for robust vision in
ﬁeld robotics,” Ph.D. dissertation, University of Sydney Grad-
uate School of Engineering and IT School of Aerospace, Me-
chanical and Mechatronic Engineering, 2014.

[4] D. Liu, L. Wang, L. Li, Zhiwei X., Feng W., and Wenjun
Z., “Pseudo-sequence-based light ﬁeld image compression,”
in 2016 IEEE International Conference on Multimedia Expo
Workshops (ICMEW), July 2016, pp. 1–4.

[5] S. Zhao, Z. Chen, K. Yang, and H. Huang, “Light ﬁeld image
coding with hybrid scan order,” in 2016 Visual Communica-
tions and Image Processing (VCIP), Nov 2016, pp. 1–4.
[6] W. Ahmad, R. Olsson, and M. Sjostrom, “Interpreting plenop-
tic images as multi-view sequences for improved compres-
sion,” in IEEE International Conference on Image Processing
(ICIP), Sep. 2017, pp. 4557–4561.

[7] S. Zhao and Z. Chen, “Light ﬁeld image coding via linear
approximation prior,” in IEEE International Conference on
Image Processing (ICIP), Sep. 2017, pp. 4562–4566.

[8] N. Bakir, W. Hamidouche, O. D´eforges, K. Samrouth, S. A.
Fezza, and M. Khalil, “Rdo-based light ﬁeld image coding us-
ing convolutional neural networks and linear approximation,”

in 2019 Data Compression Conference (DCC), March 2019,
pp. 554–554.

[9] C. Jia, X. Zhang, S. Wang, S. Wang, S. Pu, and S. Ma, “Light
ﬁeld image compression using generative adversarial network
based view synthesis,” IEEE Journal on Emerging and Se-
lected Topics in Circuits and Systems, pp. 1–1, 2018.

[10] X. Jiang, M. Le Pendu, R. A. Farrugia, and C. Guillemot,
“Light ﬁeld compression with homography-based low-rank ap-
proximation,” IEEE Journal of Selected Topics in Signal Pro-
cessing, vol. 11, no. 7, pp. 1132–1145, Oct 2017.

[11] J. Hou, J. Chen, and L. Chau, “Light ﬁeld image compression
based on bi-level view compensation with rate-distortion opti-
mization,” IEEE Trans. Cir. and Sys. for Video Technol., vol.
29, no. 2, pp. 517–530, Feb. 2019.

[12] I. Goodfellow and et. all, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp.
2672–2680.

[13] C. Ledig, L. Theis, F. Huszr, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi,
“Photo-realistic single image super-resolution using a genera-
tive adversarial network,” in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), July 2017, pp. 105–
114.

[14] Y. Bai, Y. Zhang, M. Ding, and B. Ghanem,

“Sod-mtgan:
Small object detection via multi-task generative adversarial
network,” in The European Conference on Computer Vision
(ECCV), September 2018.

[15] T. Nguyen, T. Le, H. Vu, and D. Phung, “Dual discriminator
generative adversarial nets,” in Advances in Neural Informa-
tion Processing Systems, 2017, pp. 2670–2680.

[16] N. Kalantari, T. Wang, and R. Ramamoorthi, “Learning-based
view synthesis for light ﬁeld cameras,” ACM Trans. Graph.,
vol. 35, no. 6, pp. 193:1–193:10, Nov. 2016.

[17] M. Wien, V. Baroncini, J. Boyce, A. Segall, and T. Suzuki,
“Preliminary joint call for evidence on video compression with
capability beyond hevc,” Janvier 2017.

[18] N. Sidaty, W. Hamidouche, O. D´eforges, P. Philippe, and
J. Fournier, “Compression Performance of the Versatile Video
Coding: HD and UHD Visual Quality Monitoring,” in Picture
Coding Symposium (PCS), November 2019.

[19] K. Reuze, W. Hamidouche, P. Philippe, and O. Deforges, “Dy-
namic lists for efﬁcient coding of intra prediction modes in the
future video coding standard,” in 2019 Data Compression Con-
ference (DCC), March 2019, pp. 601–601.

[20] M. Rerabek and T. Ebrahimi,

“New light ﬁeld image
in https://mmspg.epﬂ.ch/EPFL-light-ﬁeld-image-

dataset,”
dataset, 2016.

[21] S. Raj, L. Michael, and A. Sunder, “Stanford lytro light ﬁeld

archive,” in http://lightﬁelds.stanford.edu/, 2016.

[22] K. Honauer, O. Johannsen, D. Kondermann, and B. Goldl¨ucke,
“A dataset and evaluation methodology for depth estimation on
4d light ﬁelds,” in Computer Vision - ACCV 2016 : 13th Asian
Conference on Computer Vision, Cham, 2016, Springer.
[23] G. Bjøntegaard, “Improvements of the bd-psnr model,” ITU-T

SG16 Q, vol. 6, pp. 35, 2008.

