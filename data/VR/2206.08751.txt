A Database for Perceived Quality Assessment of
User-Generated VR Videos

Yuming Fang, Senior Member, IEEE, Yiru Yao, Xiangjie Sui, and Kede Ma, Member, IEEE

2
2
0
2

n
u
J

3
1

]

V
C
.
s
c
[

1
v
1
5
7
8
0
.
6
0
2
2
:
v
i
X
r
a

Fig. 1: Illustration of how people explore a virtual reality (VR) video. Under varying viewing conditions (e.g., starting points and
exploration times), users may exhibit different viewing behaviors in the form of scanpaths, leading to different portions of the video
being explored. As user-generated VR videos often come with localized authentic distortions, the perceived quality may vary with
user viewing behaviors constrained by viewing conditions. Therefore, incorporation of viewing conditions would be the key to the
success of computational quality prediction of user-generated VR videos.

Abstract—Virtual reality (VR) videos (typically in the form of 360° videos) have gained increasing attention due to the fast development
of VR technologies and the remarkable popularization of consumer-grade 360° cameras and displays. Thus it is pivotal to understand
how people perceive user-generated VR videos, which may suffer from commingled authentic distortions, often localized in space
and time.
In this paper, we establish one of the largest 360° video databases, containing 502 user-generated videos with rich
content and distortion diversities. We capture viewing behaviors (i.e., scanpaths) of 139 users, and collect their opinion scores
of perceived quality under four different viewing conditions (two starting points × two exploration times). We provide a thorough
statistical analysis of recorded data, resulting in several interesting observations, such as the signiﬁcant impact of viewing conditions
on viewing behaviors and perceived quality. Besides, we explore other usage of our data and analysis, including evaluation of
computational models for quality assessment and saliency detection of 360° videos. We have made the dataset and code available at
https://github.com/Yao-Yiru/VR-Video-Database.

Index Terms—360° videos, virtual reality, scanpaths, video quality assessment, saliency detection.

1 INTRODUCTION

As virtual reality (VR) acquisition and display systems become widely
accessible, people are getting used to capture, edit, and interact with
VR content, which is evidenced by the accelerated proliferation of
360° videos1 uploaded to popular video sharing and social media plat-
forms (e.g., Bilibili and Youtube). A practical issue arising from user-
generated 360° videos is that they are often born with complex visual
artifacts (i.e., the so-called authentic distortions) due to scene complex-
ity, lens imperfection, sensor limitation, non-professional shooting, and
stitching inaccuracy. The acquired videos may subsequently undergo
several stages of processing, including compression, editing, trans-

• Yuming Fang, Yiru Yao, and Xiangjie Sui are with the School of Information
Management, Jiangxi University of Finance and Economics, Nanchang
330032, Jiangxi, China (e-mail: yaoyiru1998@foxmail.com,
suixiangjie2017@163.com, fa0001ng@e.ntu.edu.sg).

• Kede Ma is with the Department of Computer Science, City University of
Hong Kong, Kowloon, Hong Kong (e-mail: kede.ma@cityu.edu.hk).

1In this paper, we use the terms “360°”, “omnidirectional”, “spherical”, and

“panoramic”, interchangeably.

mission, and transcoding, leading to additional video impairments [1].
Thus understanding how people perceive 360° video distortions in
virtual environments is central to many VR-enabled video applications.

Different from traditional planar videos, an omnidirectional video,
by its name, records/generates the scene of interest by capturing/trac-
ing light from all directions at possibly varying viewpoints through
time. This gives rise to a 360° × 180° spherical ﬁeld of view (FoV) at
any time instance. With the help of a head-mounted display (HMD),
users can freely explore the virtual scene using head and gaze move-
ments as if they were in the real world. Such immerse and interactive
viewing experience renders existing quality assessment methods for
planar videos [11–17] ineffective in predicting the perceived quality
of 360° videos. Although several subjective quality studies [2–10]
on omnidirectional videos have been conducted, they may have three
limitations. First, most of the resulting databases contain synthetic
distortions only, with compression artifacts being the most represen-
tative. This is an oversimpliﬁcation of the real-world situation, where
user-generated 360° videos may suffer from commingled authentic
distortions, often localized spatiotemporally. The extensively studied
compression artifacts may no longer dominate the perceptual quality.
Second, the databases assume the viewing conditions such as the start-
ing point and the exploration time to be ﬁxed, which is over-constrained

......Starting point IStarting point IIExploration time IExploration time IIt1t2Different viewports from different viewing conditions......Viewing conditionsDifferent viewports Scoring ...t1ExcellentGood FairPoorBadExcellentGood FairPoorBadPlanar viewport sequence 1Starting point 1Starting point 2Scanpath  1Scanpath 2Planar viewport sequence 2Rating 1 Rating 2ExcellentGood FairPoorBadExcellentGood FairPoorBad 
 
 
 
 
 
Projection

#videos

#subjects

Database

Singla et al. [2]

Curcio et al. [3]

Tran et al. [4]

Year

2017

2017

2017

IVQAD2017 [5]

2017

Zhang et al. [6]

Zhang et al. [7]

Lopes et al. [8]

2017

2018

2018

ERP

ERP

ERP

ERP

ERP

ERP

ERP

6 / 60

3 / 24

3 / 60

10 / 150

16 / 384

10 / 50

6 / 79

VQA-ODV [9]

2018

ERP, RCMP, TSP

60 / 540

VOD-VQA [10]

2021

Proposed

2021

ERP

ERP

18 / 774

- / 502

30

12

37

13

23

30

37

221

160

139

Resolution
1, 920 × 1, 080 to
3, 840 × 2, 160
3, 840 × 1, 920
1, 440 × 720 to
3, 840 × 1, 920
4, 096 × 2, 048

4, 096 × 2, 048

3, 600 × 1, 800
960 × 480 to
7, 680 × 3, 840
3, 840 × 1, 920 to
7, 680 × 3, 840
3, 840 × 1, 920
1, 280 × 720 to
5, 120 × 2, 560

Duration (sec)

HM/EM data

Distortion type

10

21

30

15

10

10

10

HM

HM

N/A

N/A

N/A

N/A

N/A

H.265 compression, Downsampling

Tile-based H.265 compression

H.264 compression

MPEG-4 compression, Downsampling
H.264 compression H.265 compression,
VP9 compression, Simulated packet loss
H.265 compression, Downsampling

H.265 compression, Downsampling

10 to 23

HM + EM

H.265 compression, Projection

10

15

N/A

H.264 compression, Downsampling

HM + EM

Authentic distortion

Table 1: Summary of VR VQA databases. ERP, RCMP, and TSP stand for the equirectangular projection, the reshaped cubemap projection,
and truncated square pyramid projection, respectively. The numbers in the “#videos” column are in the form of “#reference videos / #distorted
videos”.

when viewing virtual scenes with HMDs. If such constraints are re-
laxed, the visible distortions of a 360° video are probably not perceived
for some viewing conditions, and thus it is reasonable to rate the per-
ceptual quality as perfect. Third, given a ﬁxed human labeling budget,
the number of unique reference 360° videos in the databases is deter-
mined by the number of synthetic distortion types and levels, which is
limited to a few dozens (if not fewer). As such, these databases fail to
sufﬁciently represent real-world videos with diverse content, distortion,
and motion complexities.

In view of the above limitations of existing 360° video databases,
many important problems remain under-explored for understanding
the perceived quality of 360° videos. For example, how consistent are
human behaviors under the same viewing condition? How are human
behaviors affected by viewing conditions? How does perceived quality
change with viewing conditions? Can we effectively make quality
prediction and saliency detection under different viewing conditions?
In an attempt to answer these questions, we establish a large 360°
video database, which contains 502 panoramic video sequences to
span diverse video content, including cityscape, landscape, shows,
sports, and computer-generated (CG) content. The videos exhibit a
wide range of complex authentic distortions, covering the full quality
spectrum. To assess the perceived quality of a panoramic video as a
function of viewing conditions, we invite subjects to watch the video
from different starting points and time durations. By doing so, a total
of 40, 268 perceptual opinion scores together with the scanpaths (as
viewing behaviors) from 139 users are recorded. We then provide
an in-depth analysis of our data, investigating the impact of viewing
conditions on viewing behaviors and perceived quality. In addition,
we leverage our data and analysis to evaluate existing computational
models for quality prediction and saliency detection. We show how to
adapt planar video quality models to 360° videos, which demonstrate
competitive performance.

quality levels and two resolutions) were carefully selected to probe
whether the background tile should be encoded with higher resolu-
tion or higher ﬁdelity given the same bitrate budget. Tran et al. [4]
established a small database containing 60 mobile distorted videos by
ﬁve levels of H.264 compression and four resolutions. Duan et al. [5]
studied how MPEG-4 compression and spatial resolution affect the
perceptual quality of 360° videos. Zhang et al. [6] proposed a large
omnidirectional video dataset, including 16 reference and 384 distorted
videos, covering H.264, H.265, VP9 compression, and simulated packet
loss. Along with the dataset, they proposed a standardized subjective
procedure with improved efﬁciency. Zhang et al. [7] gave a compre-
hensive treatment of the interaction between subsampling and H.264
compression to panoramic video quality. They computed an optimal
resolution, 3, 600 × 1, 800, for the HTC VIVE display. Lopes et al. [8]
studied the individual and combined effects of spatial resolution, frame
rate, and H.265 compression to 360° videos. Li et al. [9] introduced the
VQA-ODV dataset, consisting of 540 impaired 360° videos from 60 ref-
erences by different levels of H.265 compression and map projections.
VQA-ODV also includes head movement (HM) and eye movement
(EM) data supplied with an analysis of human behavior consistency.
VOD-VQA [10] is currently the largest panoramic video database, in
which 18 reference videos are divided into two groups to generate a
total of 774 distorted videos with different compression levels, spatial
resolutions, and frame rates.

We list existing 360° video databases in Table 1, where we ﬁnd
that they include only synthetic distortions with the assumption that
original undistorted videos are available for database construction and
model development. By contrast, we are interested in user-generated
panoramic videos, many of which suffer from authentic distortions
during video acquisition. Moreover, it is not uncommon to see that these
distortions are localized in space and time, making viewing conditions
indispensable for determining VR video quality [19].

2 RELATED WORK
In this section, we ﬁrst introduce existing 360° video quality assessment
(VQA) databases with mean opinion scores (MOSs). We then review
objective VQA models that are adapted to or speciﬁcally designed
for assessing panoramic content. Last, we summarize 360° saliency
detection models.

2.1 Subjective Quality Assessment of Panoramic Videos
Singla et al. [2] constructed one of the ﬁrst databases to study the
impact of H.265 compression and spatial resolution on 360° video
quality. The database contains six reference videos and 60 distorted
videos at two resolutions and ﬁve bitrates. Curcio et al. [3] performed
a subjective quality experiment of 360° videos under the tile-based
streaming setting [18]. The visual stimuli (24 distorted videos at four

2.2 Objective Quality Assessment of Panoramic Videos

Existing computational models for evaluating panoramic content are
mainly adapted from planar image quality assessment (IQA) and VQA
methods to one of three data formats: (projected) 2D plane, spherical
surface, and (projected) rectilinear viewport.

Methods in the planar domain [20–22] aim to compensate for the
non-uniform sampling caused by the sphere-to-plane projection. If
the equirectangular projection (ERP) is used, current planar IQA/VQA
methods can be augmented by latitude-dependent weighting schemes.
Craster parabolic projection can also be used to ensure uniform sam-
pling density [21]. Kim et al. [22] explored an adversarial loss for
learning patch-based quality estimators using content and position fea-
tures. Li et al. [9] trained a convolutional neural network (CNN) for
panoramic video quality assessment, making use of HM and EM data.

Fig. 2: Thumbnails of user-generated 360° video sequences in the proposed dataset.

Methods in the spherical domain (e.g., S-PSNR [23] and S-SSIM [24])
calculate and aggregate local quality estimates over the sphere. Yu
et al. [23] incorporated importance weighting derived from the em-
pirical distributions of HM and EM data. Methods in the viewport
domain focus on extracting viewports that are likely to be seen for
quality computation. Xu et al. [25] used graph convolutional networks
to model spatial relations of extracted viewports, which, however, does
not necessarily reﬂect the human viewing process. Li et al. [26] pro-
posed a two-stage approach - viewport proposal and quality assessment
using spherical convolution [27]. Recently, Sui et al. [19] suggested to
convert a panoramic image to planar videos by sampling, along users’
scanpaths, sequences of rectilinear projections of viewports. By doing
so, mature planar IQA/VQA methods can be directly applied. In this
work, we will continue along this path to take into account viewing
conditions, with the goal of boosting existing quality predictors for
panoramic videos.

2.3 Saliency Detection of Panoramic Videos

360° saliency detection aims at predicting objects/regions of interest
in VR environments. It identiﬁes visually important information in
panoramic videos, and plays a key role in panoramic video streaming
and rendering. Saliency detection may also be important to visual
quality assessment [9, 23, 26].

Current saliency detection methods for 360° videos can also be
divided according to their operating domain: (projected) 2D plane,
spherical surface, and (projected) rectilinear viewport. At present, most
panoramic saliency detection algorithms build up traditional planar
methods, while taking into account some speciﬁc properties of the 2D
projection [28–30]. Nguyen et al. [28] proposed PanSalNet by ﬁne-
tuning a planar image saliency detector on two 360° video databases [31,
32]. Cheng et al. [29] trained a CNN-based saliency detection model for
360° video with a cube padding trick to alleviate projection distortions
and boundary discontinuities. Xu et al. [30] predicted HM positions
for 360° videos based on deep reinforcement learning. In the spherical
domain, Bogdanova et al. [33] extended the classic saliency model [34]
by extracting and combining intensity, chroma, and orientation features
from the spherical Gaussian pyramid. Zhang et al. [35] proposed
the spherical U-Net for saliency detection, where the sliding of the
convolution kernel “translates” to kernel rotation on the sphere. In the

Fig. 3: Empirical distribution of 360° videos (denoted by blue crosses)
in the SI-TI space with the ﬁtted convex hull and 1D histograms.

viewport domain, Lebreton et al. [36] extracted features from viewports
based on orientation analysis, which were back-projected to the ERP
domain. Concurrently, they [37] extended BMS360 [36] to V-BMS360,
enabled by the optical ﬂow-based motion detection. A recent subjective
user study on stereoscopic 360° images [38] suggests weak effect of
viewing conditions on the gathered saliency maps. However, the results
may no longer be valid when watching user-generated 360° videos,
which are thoroughly analyzed in the paper.

3 PSYCHOPHYSICAL EXPERIMENT

In this section, we summarize our effort towards recording a database
that contains human perceptual data - MOSs and scanpaths for users
watching 360° videos under different viewing conditions.

CityscapeLandscapeShowsSportsCGOthers0153045607590105120TI010203040506070SI0204060Frequency050100Frequencydistortions are mainly due to the limitation of the stitching algorithm
itself and the negative inﬂuence of visual distortions from the previous
acquisition step (e.g., stitching images of different luminance levels
tends to create artiﬁcial boundaries, as shown in Fig. 5 (d)). Visually,
stitching distortions are abrupt luminance/structure change, object with
missing parts, ghosting, and motion discontinuity localized in space
and time. Of particular interest is the artiﬁcial converging points visible
at two poles (see Fig. 5 (h)). These authentic distortions inevitably
affect the whole video processing pipeline, and ultimately be perceived
by end users.

Viewing Conditions We use an HTC Eye Pro to display 360°
videos, which provides an FoV of 110° and a binocular resolution of
2, 880 × 1, 600 pixels. Subjects are asked to seat on a swivel chair
using the HMD to watch videos. EM and HM data can be collected
by a built-in Tobii Pro eye-tracking system with a sampling rate of 2×
fps. Video playback is supported by a high-performance server with
an AMD Ryzen 9 3950X 16-Core CPU, 128 GB RAM, and NVIDIA
GeForce RTX 2080 Ti GPU. The graphical user interface is customized
using the Unity Game Engine.

An important consideration in our psychophysical experiment is that
we vary two viewing conditions: the starting point and the exploration
time. We intentionally choose Starting Point I to give users a poor ini-
tial viewing experience. This includes viewports that contain localized
distortions or intensive spatiotemporal information. Another example
is the initial viewport from the side when there is strong camera motion
guidance. On the contrary, Starting Point II is selected to encourage
a good initial viewing experience, and is at least 120° (in longitude)
apart from Starting Point I. An example is shown in Fig. 6, where the
viewport extracted from Starting Point I contains visible over-exposure
and color cast distortions, while the viewport extracted from Starting
Point II is of high quality. We also set two exploration times: one
spanning the entire duration (i.e., about 15 seconds) and the other set
to the half of the former (i.e., 7 seconds). More exploration time al-
lows more viewports to extracted and viewed. As shown in Fig. 7, the
“F BridegOpening2” video records a cruise ship sailing out of a dark
cave. Viewing from Starting Point I in the ﬁrst seven seconds, the user
may be exposed to distortions like under-exposure and over-exposure,
which injures her/his viewing experience. In the subsequent eight sec-
onds, the cruise ship has sailed out of the cave, and the viewer may
see high-quality content without artifacts, which improves the viewing
experience. The starting points and exploration times are combined in
pairs, giving rise to a total of four viewing conditions.

Subjective Methodology The single stimulus continuous quality
evaluation method described in the ITU-R BT 500.13 recommenda-
tion [44] is employed for our psychophysical study. Subjects need to
rate the perceived quality of a 360° video on a continuous scale of
[1, 5], labeled by ﬁve quality levels (“bad”, “poor”, “fair”, “good”, and
“excellent”). To reliably collect MOSs, the experimental procedure
consists of three phases: pre-training, training, and testing, as shown in
Fig. 8.

In the pre-training phase, basic non-sensitive user information such
as age, gender, and whether to wear glasses are recorded. Subjects are
familiarized with the experiment procedure and the rating guideline.
We ﬁnd it relatively time-consuming to teach subjects to use the hand
controller for rating. Therefore, one of the authors is responsible for
recording the opinion scores, read out by the subjects.

In the training phase, we select six video sequences that are not
included in the proposed dataset. For the ﬁrst four videos, we let
subjects freely view the virtual scenes, and point out the distortions they
encounter during exploration. We ﬁnd that this level of instruction is
necessary to familiarize subjects with distortions that are likely to occur
in the testing phase. We try not to over-instruct the subjects, for example
avoiding providing a reference MOS for each of the four videos. For the
remaining two videos, we ask the subjects to give quality scores with no
instructions. A discussion on how the subjects arrive at such ratings is
held to make sure they understand the evaluation process. No feedback
is provided on their scores. More importantly, if the subjects feel any

consistently with the background.

Fig. 4: 360° video processing pipeline, from optical acquisition to
content consumption via an HMD. The optical acquisition and stitching
are two main steps for 360° video creation, where authentic distortions
arise.

3.1 Data Gathering

Visual Stimuli Our database contains 502 unique user-generated
360° video sequences, with frame rate ranging from 20 to 60 frames per
second (fps) and resolution ranging from 1, 280 × 720 to 5, 120 × 2, 560
pixels. All videos are downloaded from the Internet, carrying Creative
Commons licenses. Each video is cropped to a duration of about 15
seconds and stored in the ERP format without further compression. In
our dataset, we mainly include 360° videos shot by a (nearly) static
camera. This is because videos captured by a moving camera have a
higher probability to cause dizziness [39], reducing the reliability of the
collected MOSs. We only select 32 videos with strong camera motion,
and guarantee the visual comfort by a posteriori questionnaire [40].
These moving-camera videos typically receive a considerable number
of likes on video sharing platforms, and bring users a stronger sense of
immersion and interaction.

The videos are selected to span a diversity of scenes that are ideal for
VR shooting: Cityscape, Landscape, Shows, Sports, CG, and Others
(see Fig. 2). Cityscape contains different places of interest around the
world like the Roman Colosseum and other famous historical sites.
Landscape includes beautiful natural scenes, such as waterfalls, moun-
tains, volcanoes, etc. Shows represent different forms of entertainment,
including band performance, living theatre, and street improvisation.
Sports gather various sporting events, e.g., car racing, skiing, and rid-
ing. CG is a collection of rendered videos by mature CG techniques.
Finally, the Others category is reserved for scenes that do not belong
to the previous ﬁve classes. As suggested by Winkler [41], we quan-
tify the content diversity of the proposed database using two low-level
statistics: spatial information (SI) and temporal information (TI), with
higher values indicating more complexities. Fig. 3 shows the 2D scatter
plot together with 1D histograms, from which we see that the selected
stimuli provide fairly wide coverage in the SI-TI space.

Different from existing 360° video databases (see Table 1), we
primarily focus on authentic distortions, manifesting themselves as
complex mixtures of multiple visual artifacts that arise during 360°
video creation [42]. Fig. 4 shows the entire 360° video processing
pipeline, and we see that the creation of 360° videos consists of two
steps: optical acquisition with a multi-camera rig and stitching of
multiple planar videos with limited and overlapping FoVs. Visual
distortions from the optical acquisition are often the consequences
of the combination of scene complexity, lens imperfection, sensor
limitation, and non-professional shooting, which include under/over-
exposure, out-of-focus and motion blurring, sensor noise, annoying
shaky motion, ﬂickering2, jerkiness3, and ﬂoating4 [43]. Stitching

2Flickering generally refers to unwanted frequent luminance or chrominance

changes along the temporal dimension.

3Jerkiness appears when the temporal resolution is too low to catch up with

the speed of moving objects, resulting in discontinuous object motion.

4Floating denotes the erroneously perceived motion in certain regions rel-
ative to their surrounding background, which are supposed to stay or move

405OpticalacquisitionStitchingProjectionERPCompressionStorage/TransmissionDecompressionReprojectionERPRendering/Extracting viewportHead-mounted display312405312CMPCMP(a) Under-exposure

(b) Over-exposure

(c) Motion blurring

(d) Abrupt luminance change

(e) Abrupt structure change

(f) Object with missing parts

(g) Ghosting

(h) Artiﬁcial poles

Fig. 5: Visual examples of authentic distortions in our database.

Fig. 6: We consider two types of starting points. Starting Point I (de-
noted by the light orange dot) and Starting Point II (denoted by the dark
green dot) offer poor and good initial viewing experiences, respectively.
The video name in our proposed database is “D ConfucianTemple”.

Fig. 8: Procedure of our psychophysical experiment. Period A: the ﬁrst
7- second viewing. Period B: the second duration of viewing, separated
by a voice prompt. After each video sequence is displayed, subjects
need to give two scores, indicating their viewing experience in Period
A, and both Periods A and B.

exploration periods. Speciﬁcally, a voice prompt is played when the
subject has viewed the half of a 360° video (about 7 seconds) to remind
her/him of giving a quality score based on the viewing experience so
far. When the subject ﬁnishes viewing the video, s/he is required to
give another quality score according to the overall viewing experience.
It is noteworthy that each video is viewed only once by one subject to
ensure that user data is collected without prior knowledge of the scene.

3.2 Data Processing

Opinion Scores After obtaining the raw human scores, we detect
and remove outliers using the method in [46]. Speciﬁcally, we ﬁrst
determine whether the subjective scores given to a 360° video are
normally distributed by calculating the kurtosis coefﬁcient:

κ =

µ4
(σ )4 ,

(1)

where µ4 is the fourth central moment and σ is the standard deviation.
If they are normally distributed (i.e., κ ∈ [2, 4]), an outlier is detected
if the score is out of range [µ − 2σ , µ + 2σ ]. If they are not well ﬁtted
by Gaussian, we extend the valid range to [µ −
20σ ]. We
compute the MOS by

20σ , µ +

√

√

q j =

1
M

M
∑
i=1

q(i)
j ,

(2)

where q(i)
is the opinion score of the i-th observer for the j-th video
j
sequence. In our study, each of the 502 videos in each viewing con-
ditions receives at least 20 ratings. Fig. 9 shows the MOSs with the
corresponding 95% conﬁdence intervals.

Fig. 7: We consider two exploration times, one spanning the entire
duration (i.e., 15 seconds) and the other set to the half of the former
(i.e., 7 seconds). The initial viewport is from Starting Point I. The video
name in our proposed database is “F BridegOpening2”.

discomfort during this phase, the experiment is interrupted immediately.
These are not invited to conduct the subsequent experiments.

In the testing phase, we divide the 502 videos into eight sessions to
reduce fatigue and discomfort caused by possible long-time viewing.
To further minimize such effects, the subjects can take a break at any
time during this phase. Each session contains about 60 videos with
a 5-second mid-gray screen in between. We gather human data from
139 subjects (75 females and 64 males with ages between 17 and 26).
All participants report normal or corrected-to-normal color vision. The
subjects are divided into two groups, according to two different sets
of starting points. Each subject takes part in at least two sessions.
Each video is rated by no less than 20 subjects. We employ the well-
established rating strategy in [19, 45] to collect MOSs for different

Omnidirectional videoInitial viewports......Initial viewport7-second15-secondHalf duration Entire duration ...11-secondv…ARatingB0 s7 s14 or 15sPre-trainingTrainingTestingTimeBreak4.1 Viewing Behavior Metrics
To compare multiple scanpaths, we adopt two wide-used metrics: tem-
poral correlation [48] and similarity ring metric (SRM) [49]. We also
consider comparing the saliency maps (i.e., the heatmaps) as spatial
aggregations of scanpaths for further analysis.

Temporal correlation uses PLCC to calculate the correlations be-
tween the longitude values and the latitude values of two scanpaths
s(i) = [φ (i), θ (i)] and s( j) = [φ ( j), θ ( j)], followed by simple averaging:

TC(s(i), s( j)) =

1
2

(cid:16)

PLCC

φ (i), φ ( j)(cid:17)
(cid:16)

+ PLCC

θ (i), θ ( j)(cid:17)(cid:17)
(cid:16)

,

(3)

where φ (i) and θ (i) represent the longitudes and latitudes of the i-th
scanpath, respectively. The mean temporal correlation over M subjects
exploring the video is calculated by

Fig. 9: MOSs (in the number of 502 × 4) with the corresponding 95%
conﬁdence intervals in our database.

mTC =

2 ∑M−1

i=1 ∑M

j=i+1 TC(s(i), s( j))
M(M − 1)

.

(4)

Similarity ring metric (SRM) measures whether different subjects
have been watching the same video parts at the same time. It is less
likely that all scanpaths completely overlap, but it is reasonable to
determine if they fall within a certain range, i.e., passing through the
same ring. As suggested by [49], we focus on the longitude of the
scanpath, and set the radius and the center of the ring to be r = FoV/2
and the mode of longitude values from M scanpaths at the same time
instance:

ct = mode

(cid:16)

(1)
φ
t

(2)
, φ
t

(M)
, . . . , φ
t

(cid:17)

.

(5)

A longitude value out of the ring means that the corresponding subject
does not watch the same content with respect to other subjects at t-th
time instance. The instantaneous similarity at the t-th time instance for
the i-th scanpath is then deﬁned as

(cid:40)

IS(i)

t =

1,
if φ
0, otherwise,

(i)

t ∈ (cid:2)ct − FoV

2 , ct + FoV
2

(cid:3) ,

(6)

based on which we compute the SRM by averaging across all scanpaths
and over all time instances:

SRM =

100
MT

T
∑
t=1

M
∑
i=1

IS(i)
t

.

(7)

SRM is scaled to lie within [0, 100], with a larger value indicating
higher consistency.

Heatmap reﬂects the salient areas that users pay attention to, and
can be considered as a spatial aggregation of scanpaths. To generate
dynamic heatmaps for omnidirectional videos, we apply the density-
based spatial clustering (DBSCAN) algorithm [50] to scanpaths of
all subjects, and deﬁne ﬁxations as the cluster centroids that span at
least 200 ms [51], during which the gaze direction remains roughly
unchanged. Noisy ﬁxation points will be automatically ﬁltered out. For
each second of the video clip, we compute a ﬁxation map by DBSCAN.
The saliency of each location in the ﬁxation map is determined by
the total spherical (i.e., great-circle) distances from the location to all
ﬁxation points [30], normalized by the computed maximum distance.
To compare the similarity of two heatmaps, we follow [38] and use
PLCC as the quantitative measure.

4.2 Does the Viewing Condition Affect Viewing Behav-

iors?

To assess whether viewing behaviors are affected by the viewing condi-
tions, we calculate mTC in Eq. (4) and SRM in Eq. (7) under different
viewing conditions, as listed in Table 2. We also employ analysis of
variance (ANOVA) to see whether such differences in viewing behavior
consistency as measured by mTC and SRM are statistically signiﬁcant,
as listed in Table 3. From the tables, we ﬁnd that human viewing

Fig. 10: Correlations between ratings from individual subjects and
MOSs.

To validate the reliability of the collected MOSs in the less con-
trollable VR viewing environment, we measure the Pearson linear
correlation coefﬁcient (PLCC) between the ratings from an individ-
ual subject and the MOSs. Fig. 10 shows the correlation values from
135 subjects (four subjects are detected as outliers), with a median
correlation of 0.784, which is reasonably high compared to previous
image/video quality databases with authentic distortions [16, 47]. This
suggests that the adopted subjective rating strategy (with a voice prompt
in between) is reliable for collecting MOSs in the VR environment.

Viewing Scanpaths With the built-in eye-tracking system, we are
able to gather both HM and EM data. The HM of a subject is in the
form of a sequence of three Euler angles [pitch, yaw, roll]. Pitching
up/down the head gives a positive/negative pitch value, in the range of
[−90°, 90°]; rotating the head to the left/right evokes a positive/negative
yaw value, in the range of [−180°, 180°]; tilting the head to the left/right
results in a positive/negative roll value. For current VR HMDs, only
pitch and yaw values matter, corresponding to the center latitude and
longitude coordinates of the extracted viewport. Similarly, the EM of
a subject is in the direct form of a [latitude, longitude] sequence,
representing the positions at which the eye is looking. The sampling
rate of HM and EM is 2× fps, with a maximum frequency of 90 Hz
constrained by the HMD. Compared to HM, EM is relatively noisier
due to two different behavioral modes in alternation: attention and
re-orientation [38]. Thus, we deﬁne the scanpath of a user as the
[pitch, yaw] sequence from HM.

4 UNDERSTANDING VIEWING BEHAVIORS IN VR

With the recorded data, we gather insights and investigate a number of
questions about the behaviors of users when watching user-generated
VR videos. Here we focus on analyzing one particular type of viewing
behaviors - the scanpath - because it is the most relevant to the perceived
quality of a 360° video by the corresponding user.

0500100015002000Video index0123456MOSMOS95% confidence interval020406080100120140Subject number00.20.40.60.81PLCCmTC

SRM

7-second
15-second
7-second
15-second

Starting Point I
0.394 (±0.046)
0.289 (±0.038)
72.997 (±7.221)
63.721 (±5.371)

Starting Point II
0.395 (±0.041)
0.286 (±0.034)
74.702 (±7.670)
65.128 (±5.603)

Table 2: Viewing behavior consistency in terms of mTC and SRM (and
the associated standard error) under different viewing conditions.

Fig. 11: Viewing behavior consistency in terms of PLCC between
heatmaps from Starting Point I and Starting Point II, averaged over all
360° videos. The dash line represents the initial PLCC.

behavior consistency is relatively low as measured globally by mTC
for all viewing conditions. When measured more locally by SRM, the
consistency improves, and the differences in consistency for different
starting points and exploration times are statistically signiﬁcant, as
evidenced by p-values close to zero.

We also compute the mean PLCC values between the heatmaps from
Starting Point I and Starting Point II, averaged over different videos,
as shown in Fig. 11. We ﬁnd that, in the ﬁrst four seconds, the PLCC
is lower than the initial, indicating divergent and different heatmaps,
which is explained by different viewing conditions. Second, the PLCC
increases over time, but is still not sufﬁciently high to eliminate the
impact of viewing conditions.

5 UNDERSTANDING PERCEIVED QUALITY IN VR
Understanding how people perceive visual distortions in VR is chal-
lenging due to the differences in viewing conditions between planar
2D and immersive 360° videos. In this section, we analyze the effects
of VR viewing conditions as well as video attributes on the perceived
quality of omnidirectional videos.

5.1 Does the Viewing Condition Affect Perceived Quality?
Previous studies [2–10] assume that viewing conditions have a negligi-
ble impact on the perceived quality of 360° videos, which is reasonable
if synthetic artifacts (e.g., video compression) with global distortion
appearances are considered. This is because for any head orientation
and at any time instance, the extracted viewport has a high probability
of containing the same main artifacts. However, this is not the case
when it comes to user-generated VR videos, where we are dealing with
authentic distortions, localized in space and time. Whether and when
to encounter such spatiotemporal local distortions may have a different
inﬂuence on the perceived quality. To test the hypothesis, we average
the MOSs in the proposed dataset for different viewing conditions in
Table 4. Several useful ﬁndings are worth mentioning. First, compared
to 15-second exploration, seven seconds mean fewer viewports of the
scene to be observed, highlighting the importance of the starting point
to the perceived video quality. Second, if a longer exploration time is
allowed, the viewports close to the end are more likely to affect the
overall viewing experience due to the recency effect [52]. This explains
that for the 7-second exploration, the subjects tend to give low-quality
scores when viewing from Starting Point I, where distortions appear in

Factor
Starting point
Exploration time

mTC
0.615
≈ 0

SRM
≈ 0
≈ 0

Table 3: p-values in the ANOVA test for mTC and SRM. A p-value
below the threshold of 0.05 represents that the corresponding factor
has a signiﬁcant impact on mTC and SRM, i.e., viewing behavior
consistency.

7-second
15-second

Starting Point I
2.551 (± 0.034)
3.119 (± 0.036)

Starting Point II
3.059 (± 0.034)
2.562 (± 0.038)

Table 4: Perceived quality analysis in terms of MOS (and the associated
standard error) under different viewing conditions.

the initial viewports (see Sec. 3.1 for the deﬁnitions of the two types
of starting points). If they are allowed more time, the subjects would
consciously re-orient their heads to avoid viewing distorted viewports,
and look for those with better quality, leading to an improved viewing
experience. On the contrary, from Starting Point II where the distortions
may not be viewed initially, the subjects are less likely to observe dis-
tortions with less allowable time, explaining the higher average MOS
of the 7-second exploration.

5.2 Does the Video Attribute Affect Perceived Quality?

We consider two video attributes: camera motion and spatial resolution.
The camera motion is often generated by body-mounted cameras. We
select 32 videos with complex camera motion (e.g., camera mounted
on the roller coaster or held by a surfer). From Table 5, we ﬁnd, as
expected, that videos with camera motion generally obtain lower MOSs
irrespective of viewing conditions. This validates that using camera
motion as strong visual guidance may be accompanied by annoying
shaky motion, impairing the user viewing experience. We also show in
Table 5 that high-resolution videos receive a higher average MOS than
low-resolution ones (for all viewing conditions) despite being down-
sampled in the HMD. It remains to be seen that such downsampling has
a positive impact on the perceived quality by “concealing” certain types
of distortions (e.g., compression artifacts and high-frequency noise).

5.3 Signiﬁcant Impact Analysis on Perceived Quality

To test whether the four factors: the starting point, the exploration
time, the spatial resolution, and the camera motion have a statistically
signiﬁcant effect on perceived quality, we apply the multi-factorial
ANOVA to the MOS values between factors. The results are listed in
Table 6, from which we conﬁrm that spatial resolution is a signiﬁcant
individual factor. The effect of the camera motion alone is not statis-
tically signiﬁcant due in part to the limited inclusion of such videos
to avoid visual discomfort. Two viewing conditions (i.e., the starting
point and the exploration time) together play a compound decisive role
in the perceived quality of user-generated VR videos. It turns out that
the spatial resolution and camera motion also have an interplay effect.

6 EVALUATING VQA MODELS FOR VR VIDEOS

In this section, we ﬁrst enable existing planar IQA/VQA methods to
assess the perceived quality of 360° videos, taking advantage of human

Video attribute

Low-resolution
High-resolution
No camera motion
Camera motion

Average MOS
2.176 (± 0.024)
3.081 (± 0.021)
2.856 (± 0.021)
2.779 (± 0.037)

Table 5: Perceived quality analysis in terms of MOS (and the associated
standard error) under different video attributes.

0246810121416Time00.20.40.60.81PLCCSource of variation
Starting point
Exploration time
Spatial resolution
Camera motion
Starting point
Exploration time
Starting point
Spatial resolution
Starting point
Camera motion
Exploration time
Spatial resolution
Exploration time
Camera motion
Spatial resolution
Camera motion
Error
Total

SS
1.600
0
207.250
0.200

71.360

0.300

0.720

0.110

0.890

3.750

d. f .
1
1
1
1

1

1

1

1

1

1

897.900
1386.140

1992
2007

MS
1.604
0.004
207.250
0.201

F
3.560
0.010
459.79
0.450

p
0.059
0.927
≈ 0
0.505

71.362

158.32

≈ 0

0.295

0.650

0.419

0.716

1.590

0.208

0.105

0.230

0.629

0.888

1.970

0.161

8.310

0.004

3.746

0.451

Table 6: The results of the multi-factorial ANOVA test for the effects
of the starting point, the exploration time, the spatial resolution, and
the camera motion on the perceived quality. SS: sum of squares. d. f .:
degrees of freedom. MS: mean square. F: F-value. p: p-value for
the null hypothesis. We omit three- and four-factorial analysis results,
which are statistically insigniﬁcant.

viewing behaviors [19]. We then evaluate several representative quality
models, including one that is speciﬁcally designed for omnidirectional
content, on the proposed dataset. Note that full-reference IQA/VQA
models such as the mean squared error (MSE) and the structural sim-
ilarity (SSIM) index [53] are not applicable here because no original
360° videos of pristine quality are available as reference.

6.1 Model Selection

The implementations of existing IQA/VQA methods are fairly unam-
biguous for planar images and videos. But there are many ways one
can adapt these methods to 360° videos. The primary issues are:
Resolution. To span the entire 360°×180° FoV with high ﬁdelity, the
spatial resolution of an omnidirectional video is extraordinarily high.
To speed up computational prediction, it is necessary to consider spatial
downsampling as one of the preprocessing steps. But, how should we
determine the downsampling factor?
Frame rate. Similar to spatial resolution, the frame rate of an om-
nidirectional video is suggested to be as high as possible to enhance
the sense of presence and to reduce motion sickness. Should we also
perform temporal downsampling for computational reasons?
Projection. Omnidirectional videos are typically stored in ERP format,
which introduces severe geometric distortions at high latitudes. Thus
the direct application of existing planar IQA/VQA models may achieve
suboptimal quality prediction performance. Among many map projec-
tion methods in cartography, which one should be adopted in tandem
with current quality models?
Temporal Pooling. When applying IQA models for VQA, temporal
pooling is a necessary step to aggregate frame-level quality estimates
into an overall quality score. Among popular temporal pooling strate-
gies [59], which one should be implemented?

We use bicubic interpolation to downsample the 360° videos, and
ﬁnd that the quality prediction performance degrades gracefully with
the increasing downsampling factor. To strike a balance between speed
and accuracy on the proposed dataset, we choose a downsampling
factor of two. Due to the fact that our database does not include high
frame rate videos (e.g., > 60 fps), we choose not to downsample in
the temporal dimension. Moreover, to reproduce the user viewing
process, we transform an omnidirectional video into a set of planar
videos by sampling, along the users’ scanpaths, sequences of rectilinear
projections of viewports [19]. The pixel value in the viewport can be
easily retrieved by ﬁrst projecting its positions onto the unit sphere and

then onto the ERP plane. In [59], temporal pooling has demonstrated
effectiveness in boosting IQA models for VQA. As pursuing the best
temporal pooling strategy is not our focus, we decide to use simple
average pooling to give prominence to the adopted IQA models.

In this paper, we select eight representative blind IQA/VQA models.

• BRISQUE [54], the Blind/Referenceless Image Spatial QUality
Evaluator, is a blind IQA model that extracts nature scene statis-
tics (NSS) in the spatial domain to quantify the “unnaturalness”
of the test image.

• NIQE [55], the Natural Image Quality Evaluator, is a completely
blind IQA model without training on MOSs. It measures the
deviation of the test image from statistical regularities observed
in natural undistorted images.

• UNIQUE [56], the Uniﬁed No-reference Image Quality and Un-
certainty Evaluator, is a DNN-based model trained on multiple
image quality datasets simultaneously. It can assess both synthetic
and realistic distortions.

• VSFA [14], the Video Semantic Feature Aggregation, is a blind
VQA model exploiting the content-dependency and temporal
memory effects. The content-aware features are extracted by a
pre-trained DNN model for object recognition, and the temporal
memory is modeled by gated recurrent units [60].

• TLVQM [15], the Two-Level Video Quality Model, is a blind
VQA model based on two sets of features of different complex-
ities. Support vector regression (SVR) is implemented for ﬁnal
quality prediction.

• PVQ [16], the Patch Video Quality, is a blind VQA model that
uses PaQ2PiQ [61] and ResNet3D [62] to compute 2D and 3D
video features, respectively. The pooled features are fed into an
InceptionTime [63] network for quality estimation.

• MLSP-VQA [17], the Multi-Level Spatially Pooled deep features
for VQA, is a blind model based on the multi-level features from
the pre-trained Inception network.

• MC360IQA [57], the Multi-Channel CNN for blind 360° IQA,
uses six viewports that cover the panoramic scene as input. Six
parallel hyper-ResNet34 networks [64] are used to extract features,
which are fed into an image quality regressor.

The implementations of all competing models are obtained from the
original authors. We apply planar IQA/VQA models directly in ERP
format as baselines. To distinguish the two types of methods, we add
a “V-” in front to name the viewport-based methods that make use of
viewing behaviors. Given the scanpath of the i-th user, the viewport-
based methods are able to sample a planar video, and produce a quality
estimate, denoted by ˆq(i). The ﬁnal score is computed by averaging the
estimated scores across subjects:

ˆq =

∑M
i=1 ˆq(i)
M

.

(8)

6.2 Performance Comparison
In addition to PLCC, we use a second evaluation metric to quantify
the quality prediction performance that has been widely adopted in
the IQA/VQA literature: Spearman rank-order correlation coefﬁcient
(SRCC). A higher SRCC value indicates better prediction monotonicity.
To compensate for the nonlinearity between model predictions and
MOSs, we ﬁt a four-parameter logistic function before computing
PLCC:

˜q = (β1 − β2)

1
− ˆq−β3
|β4|

1 + e

+ β2,

(9)

where {βi}4

i=1 are the parameters to be ﬁtted.

Model

BRISQUE [54]
NIQE [55]
UNIQUE [56]
VSFA [14]
TLVQM [15]
PVQ [16]
MLSP-VQA [17]
MC360IQA [57]
V-BRISQUE
V-NIQE
V-UNIQUE
V-VSFA
V-TLVQM
V-PVQ
V-MLSP-VQA

Starting Point I
15s
0.240
0.461
0.270
0.251
0.668
0.562
0.388
0.602
0.358
0.637
0.623
0.555
0.899
0.602
0.933

7s
0.176
0.368
0.260
0.259
0.583
0.453
0.377
0.510
0.260
0.520
0.504
0.443
0.842
0.480
0.911

PLCC
Starting Point II

7s
0.279
0.471
0.213
0.316
0.690
0.591
0.363
0.622
0.342
0.613
0.591
0.407
0.895
0.524
0.935

15s
0.289
0.483
0.230
0.305
0.709
0.572
0.414
0.601
0.388
0.628
0.579
0.540
0.903
0.572
0.945

Overall

0.236
0.424
0.229
0.266
0.778
0.513
0.791
0.554
0.317
0.567
0.551
0.457
0.824
0.517
0.891

Starting Point I
15s
0.257
0.536
0.271
0.207
0.669
0.562
0.441
0.618
0.378
0.686
0.633
0.543
0.898
0.610
0.933

7s
0.180
0.420
0.268
0.209
0.578
0.447
0.412
0.530
0.268
0.575
0.526
0.516
0.841
0.491
0.913

SRCC
Starting Point II

7s
0.291
0.551
0.221
0.279
0.694
0.592
0.412
0.637
0.357
0.656
0.586
0.511
0.896
0.524
0.938

15s
0.299
0.565
0.222
0.272
0.710
0.564
0.470
0.610
0.396
0.665
0.573
0.530
0.906
0.568
0.945

Overall

0.246
0.492
0.232
0.230
0.776
0.513
0.791
0.569
0.330
0.613
0.556
0.435
0.825
0.520
0.891

Table 7: Performance comparison of different IQA/VQA methods on the proposed dataset under different viewing conditions. The best results are
highlighted in bold.

Starting
Point I
0.523
0.784
0.815
0.665
0.685
0.552
0.711
0.800

AUC-Judd ↑
Starting
Point II
0.535
0.769
0.811
0.680
0.694
0.559
0.711
0.795

Overall

0.529
0.777
0.813
0.673
0.690
0.556
0.711
0.798

Starting
Point I
0.502
0.525
0.507
0.523
0.587
0.607
0.536
0.754

s-AUC ↑
Starting
Point II
0.500
0.501
0.494
0.541
0.595
0.593
0.540
0.800

Overall

0.501
0.513
0.501
0.532
0.591
0.600
0.538
0.777

Starting
Point I
0.018
1.103
1.203
0.546
0.577
1.607
0.714
4.512

NSS ↑
Starting
Point II
0.024
1.015
1.206
0.639
0.612
1.538
0.753
4.626

Overall

0.021
1.059
1.205
0.593
0.595
1.573
0.734
4.569

Starting
Point I
0.005
0.309
0.336
0.165
0.178
0.356
0.188
0.694

PLCC ↑
Starting
Point II
0.006
0.283
0.334
0.185
0.183
0.344
0.194
0.654

Overall

0.006
0.296
0.335
0.175
0.181
0.350
0.191
0.674

Chance
Equator
Constant
PanoSalNet [28]
CP360 [29]
DHP [30]
ATSal [58]
Human

Table 8: Performance comparison of different panoramic saliency detection models under different viewing conditions.

(a) Chance

(b) Equator

(c) Constant

Fig. 12: Visualization of three saliency detection baselines.

The results under different viewing conditions are listed in Table 7,
where we make several interesting observations. First, viewport-based
methods that incorporate viewing conditions signiﬁcantly outperform
the baselines that directly work with the ERP format. This is consistent
with the observations in [19], indicating the importance of explicitly
modeling viewing conditions during the quality assessment. Second,
among all tested models, V-MLSP-VQA is the best, even outperform-
ing MC360IQA speciﬁcally designed for omnidirectional content. The
primary reason may be that MC360IQA is trained for synthetic dis-
tortions (e.g., compression artifacts) rather than authentic distortions
in user-generated VR videos. Third, despite using the same feature
representation, V-BRISQUE does not deliver the same level of perfor-
mance as V-NIQE, indicating weak synthetic-to-real generalization.
Fourth, the model performance from Starting Point II is generally better
than that of Starting Point I. In our experimental setting, users from
Starting Point I have higher chances of seeing localized distortions.
The computational models, however, may fail to spot these distortions,
therefore overestimating the perceived quality. This inaccuracy is more
pronounced given a shorter time of viewing.

7 EVALUATING SALIENCY DETECTION MODELS FOR VR

VIDEOS

In this section, we explore additional use of the proposed dataset for
evaluating panoramic saliency detection models.

7.1 Model Selection
We choose three baselines and four state-of-the-art saliency detectors
for omnidirectional videos.

• Chance model assigns a uniformly distributed value from [0, 1] to

each pixel in the heatmap (see Fig. 12 (a)).

• Equator model consists of a symmetric Gaussian around the equa-
tor with a variance to cover 20% of the equator in the θ -direction
and a degenerate Gaussian with inﬁnite variance in the φ -direction
(see Fig. 12 (b)).

• Constant model is the average heatmap across the whole dataset

(see Fig. 12 (c)).

• PanoSalNet [28] employs transfer learning to adapt an existing
saliency model [65] for ERP-based saliency detection. A prior ﬁl-
ter that encodes inductive viewing biases (e.g., center and equator
biases) is used to reﬁne the prediction.

• CP360 [29] is a cubemap-based weakly-supervised model with
a cube padding trick to reduce projection distortions and image
border discontinuities.

• DHP [30] applies M workﬂows in parallel to predict the HM
positions of M subjects for a 360° video. We set M = 20, which
is the number of subjects in our database. For each video frame, a
heatmap can be obtained by Gaussian blurring the estimated HM
positions.

Fig. 13: Saliency detection performance changes over time for different starting points.

• ATSal [58] is a two-stream CNN model, where the ERP-based
stream is dedicated to extracting global attention statistics, while
the cubemap-based stream aims at learning local saliency fea-
tures.

The implementations of the four panoramic saliency detectors are again
obtained from the original authors, and tested with the default settings.

7.2 Performance Comparison
We use four metrics to evaluate the saliency detection performance,
including the Judd variant of the area under curve (AUC-Judd) [66],
shufﬂe-AUC (s-AUC) [67], normalized scanpath saliency (NSS) [68],
and PLCC [69]. It is noteworthy that in order to avoid the over-sampling
problem in ERP format when calculating the metrics, we uniformly
sample 1, 000 points on the sphere, whose saliency values can be re-
trieved from the corresponding heatmap of size 180 × 360, as suggested
in [70].

We show the overall performance for different starting points in
Table 8. To make the results more comparable and interpretable, for
each metric, we compute the human consistency as a realistic upper
bound for model performance [71]. Speciﬁcally, we ﬁrst compare the
ﬁxations of two groups of M observers, where M varies from 1 to 10
(i.e., half of the total 20 observers). We then ﬁt the 10 performance
scores to a power function (i.e., aMb + c), and predict the human
performance as that of two groups of inﬁnite observers (which is equal
to c, for b < 0). We also take a closer look at the performance changes
over time for different starting points in Fig. 13. We ﬁnd that DHP [30]
that explicitly models the viewing conditions performs the best among
all models in terms of s-AUC, NSS, and PLCC (see also Fig. 14).
Moreover, equator and constant models conﬁrm the effectiveness of
the equator bias, and are even top-2 performers under AUC-Judd.
Nevertheless, there is signiﬁcant room for improvements as evidenced
by a large performance gap between computational models and humans.

8 CONCLUSION AND DISCUSSION
We have put together the ﬁrst user-generated VR video dataset that
includes MOSs and viewing behavioral data. This dataset encompasses
139 users viewing 360° videos from four different conditions, resulting
in a total of 40, 268 opinion scores and scanpaths. We conducted a
statistical analysis of various effects on viewing behaviors and perceived
quality in VR, identifying viewing conditions to be crucial. We last
evaluated several quality assessment and saliency detection models on
our dataset.

Our work presents an initial effort to understand the perceived qual-
ity of user-generated VR videos. Many important research problems
are left unexplored. First, in our psychophysical experiment, we man-
ually minimize the adverse physiological reactions by means of ques-

tionnaires. It would be interesting to design clever psychophysical
experiments to disentangle visual discomfort and visual quality in a
quantitative way. Second, when collecting viewing behaviors, we ex-
pose users to a VR scene only once to eliminate the prior knowledge
about the scene conﬁguration. How do viewing behaviors change with
multiple exposures to the same scene and how these changes affect
the perceived quality are worth exploring. Third, currently, objective
quality models tailored to user-generated 360° videos are largely lack-
ing. Our study suggests that a key step in the model development
is the incorporation of viewing conditions that faithfully reﬂect how
humans explore VR videos. Fourth, when evaluating existing objective
IQA/VQA models for 360° videos, we directly make use of human
scanpaths, which have to be estimated in practice. However, scanpath
prediction for 360° videos is still in its infancy, especially for long-term
prediction (e.g., ≥ 10 seconds). It is thus desirable to develop better
scanpath prediction models that deliver accurate short-term and long-
term results, and meanwhile capture the diversity of human scanpaths.
Last but not least, the current work considers 360° videos as the sole
visual stimuli, it is of interest to investigate audio-visual perception
of user-generated 360° videos [51] and its consequences on perceived
quality.

REFERENCES

[1] R. G. D. A. Azevedo, N. Birkbeck, F. De Simone, I. Janatra, B. Adsumilli,
and P. Frossard, “Visual distortions in 360° videos,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 30, no. 8, pp. 2524–2537,
2020.

[2] A. Singla, S. Fremerey, W. Robitza, P. Lebreton, and A. Raake, “Compari-
son of subjective quality evaluation for HEVC encoded omnidirectional
videos at different bit-rates for UHD and FHD resolution,” in ACM Multi-
media Workshops, 2017, pp. 511–519.

[3] I. D. Curcio, H. Toukomaa, and D. Naik, “Bandwidth reduction of om-
nidirectional viewport-dependent video streaming via subjective quality
assessment,” in International Workshop on Multimedia Alternate Realities,
2017, pp. 9–14.

[4] H. T. T. Tran, N. P. Ngoc, C. M. Bui, M. H. Pham, and T. C. Thang,
“An evaluation of quality metrics for 360 videos,” in Ninth International
Conference on Ubiquitous and Future Networks, 2017, pp. 7–11.

[5] H. Duan, G. Zhai, X. Yang, D. Li, and W. Zhu, “IVQAD 2017: An
immersive video quality assessment database,” in International Conference
on Systems, Signals and Image Processing, 2017, pp. 1–5.

[6] B. Zhang, J. Zhao, S. Yang, Y. Zhang, J. Wang, and Z. Fei, “Subjective
and objective quality assessment of panoramic videos in virtual reality
environments,” in IEEE International Conference on Multimedia Expo
Workshops, 2017, pp. 163–168.

[7] Y. Zhang, Y. Wang, F. Liu, Z. Liu, Y. Li, D. Yang, and Z. Chen, “Subjective
panoramic video quality assessment database for coding applications,”
IEEE Transactions on Broadcasting, vol. 64, no. 2, pp. 461–473, 2018.

Starting Point IStarting Point IIChanceEquatorConstantPanoSalNetCP360DHPATSalFig. 14: Heatmaps produced by different saliency detection models. From left to right show the results of the 1-st, 4-th, 7-th, 10-th, and 15-th
second, respectively.

[8] F. Lopes, J. Ascenso, A. Rodrigues, and M. P. Queluz, “Subjective and
objective quality assessment of omnidirectional video,” in Applications of
Digital Image Processing, 2018, pp. 249–265.

[9] C. Li, M. Xu, X. Du, and Z. Wang, “Bridge the gap between VQA and
human behavior on omnidirectional video: A large-scale dataset and a
deep learning model,” in ACM Multimedia, 2018, pp. 932–940.

[10] Y. Meng and Z. Ma, “Viewport-based omnidirectional video quality assess-
ment: Database, modeling and inference,” IEEE Transactions on Circuits
and Systems for Video Technology, vol. 32, no. 1, pp. 120–134, 2022.
[11] Z. Wang, L. Lu, and A. C. Bovik, “Video quality assessment based on
structural distortion measurement,” Signal Processing: Image Communi-
cation, vol. 19, no. 2, pp. 121–132, 2004.

[12] Y. Wang, T. Jiang, S. Ma, and W. Gao, “Novel spatio-temporal structural
information based video quality metric,” IEEE Transactions on Circuits
and Systems for Video Technology, vol. 22, no. 7, pp. 989–998, 2012.
[13] M. Xu, J. Chen, H. Wang, S. Liu, G. Li, and Z. Bai, “C3DVQA: Full-
reference video quality assessment with 3D convolutional neural network,”
in IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing, 2020, pp. 4447–4451.

[14] D. Li, T. Jiang, and M. Jiang, “Quality assessment of in-the-wild videos,”

in ACM Multimedia, 2019, pp. 2351–2359.

Visualization and Computer Graphics, pp. 1–11, 2021.

[20] Y. Sun, A. Lu, and L. Yu, “Weighted-to-spherically-uniform quality evalu-
ation for omnidirectional video,” IEEE Signal Processing Letters, vol. 24,
no. 9, pp. 1408–1412, 2017.

[21] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical
panoramic video,” in Optics and Photonics for Information Processing,
2016, pp. 57–65.

[22] H. G. Kim, H. Lim, and Y. M. Ro, “Deep virtual reality image quality
assessment with human perception guider for omnidirectional image,”
IEEE Transactions on Circuits and Systems for Video Technology, vol. 30,
no. 4, pp. 917–928, 2019.

[23] M. Yu, H. Lakshman, and B. Girod, “A framework to evaluate omnidirec-
tional video coding schemes,” in IEEE International Symposium on Mixed
and Augmented Reality, 2015, pp. 31–36.

[24] S. Chen, Y. Zhang, Y. Li, Z. Chen, and Z. Wang, “Spherical structural
similarity index for objective omnidirectional video quality assessment,”
in IEEE International Conference on Multimedia and Expo, 2018, pp. 1–6.
[25] J. Xu, W. Zhou, and Z. Chen, “Blind omnidirectional image quality as-
sessment with viewport oriented graph convolutional networks,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 31, no. 5,
pp. 1724–1737, 2021.

[15] J. Korhonen, “Two-level approach for no-reference consumer video quality
assessment,” IEEE Transactions on Image Processing, vol. 28, no. 12, pp.
5923–5938, 2019.

[26] C. Li, M. Xu, L. Jiang, S. Zhang, and X. Tao, “Viewport proposal CNN for
360° video quality assessment,” in IEEE Conference on Computer Vision
and Pattern Recognition, 2019, pp. 10169–10178.

[16] Z. Ying, M. Mandal, D. Ghadiyaram, and A. Bovik, “Patch-VQ: ‘Patching
Up’ the video quality problem,” in IEEE Conference on Computer Vision
and Pattern Recognition, 2021, pp. 14019–14029.

[17] F. G¨otz-Hahn, V. Hosu, H. Lin, and D. Saupe, “Konvid-150k: A dataset
for no-reference video quality assessment of videos in-the-wild,” IEEE
Access, vol. 9, pp. 72139–72160, 2021.

[18] M. Graf, C. Timmerer, and C. Mueller, “Towards bandwidth efﬁcient
adaptive streaming of omnidirectional video over HTTP: Design, imple-
mentation, and evaluation,” in ACM on Multimedia Systems Conference,
2017, pp. 261–271.

[19] X. Sui, K. Ma, Y. Yao, and Y. Fang, “Perceptual quality assessment of
omnidirectional images as moving camera videos,” IEEE Transactions on

[27] T. Cohen, M. Geiger, J. K¨ohler, and M. Welling, “Spherical CNNs,” in

International Conference on Learning Representations, 2018.

[28] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique: Detecting
360-degree video saliency in head-mounted display for head movement
prediction,” in ACM Multimedia, 2018, pp. 1190–1198.

[29] H.-T. Cheng, C.-H. Chao, J.-D. Dong, H.-K. Wen, T.-L. Liu, and M. Sun,
“Cube padding for weakly-supervised saliency prediction in 360° videos,”
in IEEE Conference on Computer Vision and Pattern Recognition, 2018,
pp. 1420–1429.

[30] M. Xu, Y. Song, J. Wang, M. Qiao, L. Huo, and Z. Wang, “Predicting head
movement in panoramic video: A deep reinforcement learning approach,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41,

PanoSalNetCP360ATSalDynamicstimuliGT-IIGT-IDHP-IDHP-IIno. 11, pp. 2693–2708, 2019.

[31] X. Corbillon, F. De Simone, and G. Simon, “360-Degree video head
movement dataset,” in ACM on Multimedia Systems Conference, 2017, pp.
199–204.

[32] C. Wu, Z. Tan, Z. Wang, and S. Yang, “A dataset for exploring user be-
haviors in VR spherical video streaming,” in ACM on Multimedia Systems
Conference, 2017, pp. 193–198.

[33] I. Bogdanova, A. Bur, and H. Hugli, “Visual attention on the sphere,” IEEE
Transactions on Image Processing, vol. 17, no. 11, pp. 2000–2014, 2008.
[34] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
for rapid scene analysis,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 20, no. 11, pp. 1254–1259, 1998.

[35] Z. Zhang, Y. Xu, J. Yu, and S. Gao, “Saliency detection in 360° videos,”
in European Conference on Computer Vision, 2018, pp. 488–503.
[36] P. Lebreton and A. Raake, “GBVS360, BMS360, ProSal: Extending
existing saliency prediction models from 2D to omnidirectional images,”
Signal Processing: Image Communication, vol. 69, pp. 69–78, 2018.
[37] P. Lebreton, S. Fremerey, and A. Raake, “V-BMS360: A video extention
to the BMS360 image saliency model,” in IEEE International Conference
on Multimedia Expo Workshops, 2018, pp. 1–4.

[38] S. Vincent, S. Ana, P. Amy, A. Maneesh, G. Diego, M. Belen, and W. Gor-
don, “Saliency in VR: How do people explore virtual environments?”
IEEE Transactions on Visualization and Computer Graphics, vol. 24,
no. 4, pp. 1633–1642, 2018.

[39] H. G. Kim, H.-T. Lim, S. Lee, and Y. M. Ro, “VRSA Net: VR sickness
assessment considering exceptional motion for 360° VR video,” IEEE
Transactions on Image Processing, vol. 28, no. 4, pp. 1646–1660, 2019.

[40] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal, “Simula-
tor sickness questionnaire: An enhanced method for quantifying simulator
sickness,” The International Journal of Aviation Psychology, vol. 3, no. 3,
pp. 203–220, 1993.

[41] W. Stefan, “Analysis of public image and video databases for quality
assessment,” IEEE Journal of Selected Topics in Signal Processing, vol. 6,
no. 6, pp. 616–625, 2012.

sessment in the spatial domain,” IEEE Transactions on Image Processing,
vol. 21, no. 12, pp. 4695–4708, 2012.

[55] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a ‘completely
blind’ image quality analyzer,” IEEE Signal Processing Letters, vol. 20,
no. 3, pp. 209–212, 2013.

[56] W. Zhang, K. Ma, G. Zhai, and X. Yang, “Uncertainty-aware blind image
quality assessment in the laboratory and wild,” IEEE Transactions on
Image Processing, vol. 30, pp. 3474–3486, 2021.

[57] W. Sun, X. Min, G. Zhai, K. Gu, H. Duan, and S. Ma, “MC360IQA: A
multi-channel CNN for blind 360-degree image quality assessment,” IEEE
Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp. 64–77,
2020.

[58] Y. Dahou, M. Tliba, K. McGuinness, and N. O’Connor, “ATSal: An
attention based architecture for saliency prediction in 360° videos,” in
International Conference on Pattern Recognition, 2021, pp. 305–320.
[59] Z. Tu, C.-J. Chen, L.-H. Chen, N. Birkbeck, B. Adsumilli, and A. C.
Bovik, “A comparative evaluation of temporal pooling methods for blind
video quality assessment,” in IEEE International Conference on Image
Processing, 2020, pp. 141–145.

[60] K. Cho, B. van Merrienboer, C¸ . G¨ulc¸ehre, F. Bougares, H. Schwenk, and
Y. Bengio, “Learning phrase representations using RNN encoder-decoder
for statistical machine translation,” CoRR, vol. abs/1406.1078, 2014.
[Online]. Available: http://arxiv.org/abs/1406.1078

[61] Z. Ying, H. Niu, P. Gupta, D. Mahajan, D. Ghadiyaram, and A. Bovik,
“From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space
of picture quality,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2020, pp. 3575–3585.

[62] K. Hara, H. Kataoka, and Y. Satoh, “Learning spatio-temporal features
with 3D residual networks for action recognition,” in IEEE International
Conference on Computer Vision Workshops, 2017, pp. 3156–3160.
[63] H. Ismail Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt,
J. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, “Incep-
tionTime: Finding AlexNet for time series classiﬁcation,” Journal of Data
Mining and Knowledge Discovery, vol. 34, no. 6, pp. 1936–1962, 2020.

[42] D. Ghadiyaram and A. C. Bovik, “Massive online crowdsourced study
of subjective and objective picture quality,” IEEE Transactions on Image
Processing, vol. 25, no. 1, pp. 372–387, 2016.

[64] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for im-
age recognition,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 770–778.

[43] K. Zeng, T. Zhao, A. Rehman, and Z. Wang, “Characterizing perceptual
artifacts in compressed video streams,” in Human Vision and Electronic
Imaging XIX, vol. 9014, 2014, pp. 173–182.

[65] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in International Conference on Learning
Representations, 2015.

[44] B. Series, “Methodology for the subjective assessment of the quality of

[66] Z. Bylinskii, T. Judd, A. Borji, L. Itti, F. Durand, A. Oliva, and A. Torralba,

television pictures,” Recommendation ITU-R BT 500-13, 2012.

“MIT saliency benchmark,” 2015.

[67] B. W. Tatler, “The central ﬁxation bias in scene viewing: Selecting an
optimal viewing position independently of motor biases and image feature
distributions,” Journal of Vision, vol. 7, no. 14, pp. 1–17, 2007.

[68] R. J. Peters, A. Iyer, L. Itti, and C. Koch, “Components of bottom-up
gaze allocation in natural images,” Vision Research, vol. 45, no. 18, pp.
2397–2416, 2005.

[69] O. Le Meur, P. Le Callet, and D. Barba, “Predicting visual ﬁxations on
video based on low-level visual features,” Vision Research, vol. 47, no. 19,
pp. 2483–2498, 2007.

[70] E. David, J. Guti´errez, A. Coutrot, M. P. Da Silva, and P. Le Callet, “A
dataset of head and eye movements for 360° videos,” in ACM Multimedia
Systems Conference, 2018, pp. 432–437.

[71] Z. Bylinskii, T. Judd, A. Oliva, A. Torralba, and F. Durand, “What do
different evaluation metrics tell us about saliency models?” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, vol. 41, no. 3, pp.
740–757, 2019.

[45] Z. Duanmu, K. Ma, and Z. Wang, “Quality-of-experience for adaptive
streaming videos: An expectation conﬁrmation theory motivated approach,”
IEEE Transactions on Image Processing, vol. 27, no. 12, pp. 6135–6146,
2018.

[46] VQGE, “Final report from the video quality experts group on the
validation of objective models of video quality assessment,” 2000.
[Online]. Available: http://www.vqeg.org

[47] V. Hosu, H. Lin, T. Sziranyi, and D. Saupe, “KonIQ-10k: An ecologically
valid database for deep learning of blind image quality assessment,” IEEE
Transactions on Image Processing, vol. 29, pp. 4041–4056, 2020.
[48] N. C. Anderson, F. Anderson, A. Kingstone, and W. F. Bischof, “A com-
parison of scanpath comparison methods,” Behavior Research Methods,
vol. 47, no. 4, pp. 1377–1392, 2015.

[49] I. D. Curcio, H. Toukomaa, and D. Naik, “360-degree video streaming
and its subjective quality,” in SMPTE Annual Technical Conference and
Exhibition, 2017, pp. 1–23.

[50] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-based algorithm
for discovering clusters in large spatial databases with noise,” in Interna-
tional Conference on Knowledge Discovery and Data Mining, 1996, pp.
226–231.

[51] F.-Y. Chao, C. Ozcinar, C. Wang, E. Zerman, L. Zhang, W. Hamidouche,
O. Deforges, and A. Smolic, “Audio-visual perception of omnidirectional
video for virtual reality applications,” in IEEE International Conference
on Multimedia Expo Workshops, 2020, pp. 1–6.

[52] D. S. Hands and S. E. Avons, “Recency and duration neglect in subjective
assessment of television picture quality,” Applied Cognitive Psychology,
vol. 15, no. 6, pp. 639–657, 2001.

[53] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assess-
ment: From error visibility to structural similarity,” IEEE Transactions on
Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

[54] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality as-

