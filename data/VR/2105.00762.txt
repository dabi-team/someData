VECA : A Toolkit for Building Virtual Environments to Train and Test
Human-like Agents

Kwanyoung Park, Hyunseok Oh, Youngki Lee∗
Department of Computer Science and Engineering
Seoul National University, South Korea
william202@snu.ac.kr, ohsai@snu.ac.kr, youngkilee@snu.ac.kr

1
2
0
2

y
a
M
3

]
I

A
.
s
c
[

1
v
2
6
7
0
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

Building human-like agent, which aims to learn and
think like human intelligence, has long been an im-
portant research topic in AI. To train and test human-
like agents, we need an environment that imposes the
agent to rich multimodal perception and allows com-
prehensive interactions for the agent, while also easily
extensible to develop custom tasks. However, existing
approaches do not support comprehensive interaction
with the environment or lack variety in modalities. Also,
most of the approaches are difﬁcult or even impossible
to implement custom tasks. In this paper, we propose a
novel VR-based toolkit, VECA, which enables building
fruitful virtual environments to train and test human-
like agents. In particular, VECA provides a humanoid
agent and an environment manager, enabling the agent
to receive rich human-like perception and perform com-
prehensive interactions. To motivate VECA, we also
provide 24 interactive tasks, which represent (but are
not limited to) four essential aspects in early human de-
velopment: joint-level locomotion and control, under-
standing contexts of objects, multimodal learning, and
multi-agent learning. To show the usefulness of VECA
on training and testing human-like learning agents, we
conduct experiments on VECA and show that users can
build challenging tasks for engaging human-like algo-
rithms, and the features supported by VECA are critical
on training human-like agents.

Introduction

It has long been an important research topic to understand
how humans learn and build human-like agents (Lake et al.
2017; Sloman 1999). In particular, human intelligence has
been a role model for many modern learning machines as an
interpretable and data-efﬁcient general intelligence. For ex-
ample, deep learning methods, inspired by the human brain
structure, have outperformed previous state-of-the-art algo-
rithms and even human intelligence in various domains such
as image classiﬁcation (He et al. 2015), complex games such
as Go or Starcraft 2 (Silver et al. 2016; Vinyals et al. 2019).

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

∗Corresponding author.

Although there is no need to precisely duplicate human in-
telligence (which is error-prone and imperfect), human intel-
ligence is still an attractive target to learn and get inspiration
on how learning works (Marcus 2018).

Human intelligence learns by experience; collecting rich
multimodal perception (such as vision, audio, tactile) from
an environment (Landau, Smith, and Jones 1998; Tacca
2011) and actively interacting (Franchak, van der Zalm,
and Adolph 2010; Vogt et al. 2018) with it. For instance,
developmental psychologists have studied that general un-
derstanding of objects develops in an early stage of the tod-
dler without any supervision, by receiving multimodal feed-
back during interaction on objects such as mouthing, chew-
ing, and rotating (Gibson 1988; Piaget and Cook 1952). Like
human intelligence, it has also been argued that artiﬁcial in-
telligence could beneﬁt from multimodal (De Vries et al.
2017; Ngiam et al. 2011) and interactive (Caselles-Dupr´e,
Ortiz, and Filliat 2019; Hermann et al. 2017) learning. To
build agents that learn like human, it is crucial to provide
rich multimodal perception and interactions for the agent.

Although prior works reveal that human-like agents learn
from rich multimodal perception and active interaction with
the environment, we still lack an understanding of how and
what the agent learns. In particular, there are important un-
explored problems, such as how human-like agents should
be evaluated, and through what tasks human-like agents are
trained. Such limitations motivate to develop an extensible
toolkit, rather than a ﬁxed set of tasks, to support the re-
search community to train and test novel approaches and al-
gorithms for human-like agents.

However, it is challenging to design an environment that
can provide the agent with rich multimodal perception and
comprehensive interactions while also being extensible to
develop custom tasks. An intuitive way would be to develop
a robot with an actionable body and sensors, but building
such a human-like robot is highly costly. Also, it is difﬁ-
cult to test premature agents that may break robot hardware.
Moreover, accelerating the training process is not straight-
forward since the robot has to perceive and interact in the
real world. Another promising approach is to train the agent
in a virtual environment. Existing environments such as
game environments for reinforcement learning (Bellemare
et al. 2013; OpenAI 2016; Beattie et al. 2016; Johnson et al.
2016; Kempka et al. 2016; Song et al. 2020) have diverse

 
 
 
 
 
 
Environments

ALE (Bellemare et al. 2013)
DeepMind Lab (Beattie et al. 2016)
OpenAI Universe (OpenAI 2016)
VizDoom (Kempka et al. 2016)
Arena (Song et al. 2020)
Malmo (Johnson et al. 2016)
Gibson (Xia et al. 2018)
MINOS (Chang et al. 2017)
House3D (Wu et al. 2018)
HoME (Brodeur et al. 2017)
AI2-THOR (Kolve et al. 2017)
VECA

3D

X
O
O
O
O
O
O
O
O
O
O
O

Large-
scale
X
X
O
X
X
O
O
O
O
O
X
X

Exten
sible
X
(cid:52)
O
O
O
O
X
X
(cid:52)
(cid:52)
O
O

Physics

X
X
X
X
O
X
X
X
X
O
O
O

FPP
Vision
X
O
O
O
X
O
O
O
O
O
O
O+

Audio

X
X
X
X
X
X
X
X
X
O
X
O+

Tactile Multi-
agent
X
X
X
O
O
X
X
X
X
O
O
O

X
X
X
X
X
X
X
X
X
X
X
O

Inter
action
X
X
X
X
X
O
X
X
X
X
O
O

Table 1: Comparison of various virtual environments with VECA. O+ indicates that the environment supports the perception and
also it reﬂects the characteristic of human perception. 3D : Supports 3D environment. Large-scale : Number of environments
is more than 103. Extensible : User can implement and add novel environments and tasks. (cid:52) indicates that the user is possible,
but difﬁcult to add novel environment and tasks due to limitations such as lack of visual editor or heavily specialized API.
Physics : Supports physical properties (e.g. collision, friction) FPP Vision : Renders ﬁrst person perspective vision. Audio :
Renders audio perception. Tactile : Renders tactile perception. Multi-agent : Supports multiple agents in single environment.
Interaction : Supports comprehensive interactions. For example in Malmo, hitting with a pickaxe breaks the block when it
targets block while it gives damage when it targets monsters.

games to evaluate the agent with some of the platforms are
extensible. However, those environments oversimplify dy-
namics and perceptions of game avatars, which makes them
inappropriate for developing human-like agents. There have
been recent efforts to simulate realistic indoor environments
(Xia et al. 2018; Kolve et al. 2017; Brodeur et al. 2017; Wu
et al. 2018; Chang et al. 2017), but they do not support active
interactions or lack of rich multimodal perceptions, and also
hard or even impossible to implement custom tasks.

In this light, we propose a novel VR-driven toolkit,
VECA, which enables to train and test emerging human-like
agents in a virtual environment. VECA provides essential
features to train human-like agents: 1) rich human-like per-
ceptions, 2) comprehensive interaction capability with the
environment, 3) extensibility for implementing various cus-
tom tasks. Using VECA, developers can easily create a cus-
tom environment where an agent can take rich sensory in-
puts, learn cognition while interacting with the environment,
and perform necessary actions to solve complex tasks.

More speciﬁcally, VECA is implemented over the Unity
engine, which provides not only realistic physics and ren-
dering but also an user-friendly visual editor to the users.
VECA provides a humanoid agent equipped with a hu-
manoid avatar, which receives human-like perception and
supports joint-level physical actions, as well as animation-
based actions. To accurately simulate the agent’s percep-
tion and action, VECA internally simulates the environ-
ment using low-level APIs of Unity instead of using the de-
fault simulation loop of Unity. Moreover, VECA provides a
network-based python API, which enables training python-
coded agents in VECA from external servers.

which reﬂect essential features in human learning (e.g., un-
derstanding contexts of objects, multimodal learning, multi-
agent learning, joint-level locomotion and control). Those
environments can be directly used or modiﬁed to train and
test human-like learning algorithms.

We show the usefulness of VECA on training and testing
human-like learning agents with various use cases. In par-
ticular, we analyze the results on the proposed tasks with
widely used reinforcement learning algorithms and study
the effect of the quality of perceptions. Our analysis shows
that the performance across various tasks has noticeable dif-
ferences, varying from easily solvable tasks to challenging
tasks. We also show that the agent’s performance can be re-
duced by 50%, 97% if the spatialization, stereo feature of au-
dio perception is removed, and by 20% when the tactile per-
ception is removed. Those results show that users can build
challenging tasks for engaging human-like algorithms using
VECA, and features of perceptions supported by VECA are
critical to building environments for human-like agents.

The contribution of this work can be summarized as fol-

lows:

• We propose a novel VR-based toolkit named VECA to
train and test human-like agents in a virtual environment.
VECA is the very ﬁrst tool that provides rich human-
like perceptions and interaction capability with a human
avatar, which will serve as the cornerstone to develop in-
novative human-like models and algorithms.

• We provide a network-based python API, which can be
used to train agents from external servers without graphic
devices.

To motivate and formulate VECA, we showcase a set of
environments equipped with 24 tasks for human-like agents,

• We provide a set of tasks, datasets, and playgrounds,
which can be directly used or modiﬁed to train and test

various human-like learning algorithms.

• By conducting various experiments on VECA, we show
that users can build challenging tasks for engaging
human-like algorithms using VECA, and features of
VECA are important for training human-like agents.

Background & Related Works
Researchers have used various ways to train artiﬁcial intelli-
gence: (1) datasets, (2) real robots, (3) virtual environments.
However, they are not suitable for developing human-like
agents, motivating us to build VECA to promote research on
next-generation human-like agents.

Pre-collected datasets. The most common approach to
train an agent is to use pre-collected datasets such as Ima-
genet (Deng et al. 2009), Audioset (Gemmeke et al. 2017).
However, this approach requires developers to collect a large
volume of data to build accurate models, which is costly and
time-consuming. Moreover, this approach is inappropriate
for training agents that learn by interaction.

Training in reality. Another approach is to use a robot
and train it in the real world (Metta et al. 2010; Maiolino
et al. 2013). However, building robot hardware requires sig-
niﬁcant time, effort, monetary cost. Furthermore, training
agents in reality makes the training process hard to paral-
lelize or accelerate, since resources and time scale of train-
ing process are bounded to those of the real-world. Those
difﬁculties motivate the usage of virtual environments.

Game-based environments. Recently,
several game-
based environments (Bellemare et al. 2013; OpenAI 2016;
Beattie et al. 2016; Johnson et al. 2016; Kempka et al. 2016;
Song et al. 2020) have been proposed and adopted to train
and test agents with various learning algorithms (e.g., rein-
forcement learning (Schulman et al. 2017), imitation learn-
ing (Ho and Ermon 2016)). However, tasks, dynamics, and
perceptions from such game environments are oversimpli-
ﬁed and do not apply to real-world problems.

Realistic Indoor Simulators. To overcome the problems
of game-based environments, realistic virtual environments
such as AI2-THOR (Kolve et al. 2017), MINOS (Chang et al.
2017), House3D (Wu et al. 2018), Gibson (Xia et al. 2018),
HoME (Brodeur et al. 2017) have been proposed. On top
of those, VECA aims to take one step further by including
critical points in human learning.

Among those, the most related environments with VECA
is HoME and AI2-THOR. HoME provides a large-scale mul-
timodal environment, which renders audio perception and
enables physical interactions. However, it lacks tactile per-
ception and diversity of interactions (only physical interac-
tion). Also, it is hard to implement novel tasks due to lack
of visual editor. AI2-THOR supports object-speciﬁc interac-
tions, which enables the agent to experience diverse circum-
stances. However, it does not support various modalities.
Moreover, those environments do not reﬂect human char-
acteristics, since those are not designed for this purpose. In
short, VECA aims to support human-like modalities and di-

Figure 1: VECA architecture.

verse interactions, also extensible for easily implementing
custom tasks.

VECA Toolkit
VECA toolkit provides useful features for developing virtual
environments to train and test human-like agents. As shown
in Fig. 1, VECA provides a humanoid agent and an envi-
ronment manager, enabling the agent to receive rich human-
like perception and perform comprehensive interactions. To
provide an extensible toolkit for the users, we chose Unity
as an engine to simulate and develop the environment, a
state-of-the-art game engine equipped with realistic render-
ing, physics, and user-friendly visual editor. On top of that,
users can train and test python-coded agents from external
servers using network-based communication interface pro-
vided by VECA.

Humanoid Agent
This component simulates an actionable agent with multi-
modal perception, which can be trained by human-like learn-
ing algorithms. The agent interacts with the virtual envi-
ronment through its humanoid avatar, capable of perform-
ing joint-level actions and pre-deﬁned actions such as walk-
ing, kicking. The agent receives four main perceptions: vi-
sion, audio, tactile, and proprioception (including vestibular
senses), which can be perceived with various conﬁgurations.

Humanoid Avatar & Action. We modeled a humanoid
avatar with an accurate collider and human-like joint-level
motion capability, which allows the agent to perform joint-
level actions and physically interact with the environment.
Although there are many humanoid assets in Unity, they are
mainly designed for games and do not precisely model bod-
ies and actions. In this light, we model a humanoid agent
with Blender3D software and integrate it into a Unity asset,

which is easy to customize. Speciﬁcally, the avatar has 47
bones with 82 degrees of freedom with hard constraints sim-
ilar to human joints, and skin mesh represented with 1648
triangles, which adaptively changes according to the orien-
tation of the bones.

However, training an agent from joint-level actions and
physical interactions may not be practically plausible for
complex tasks. To mitigate the user’s burden of training
those tasks, we also support an animation-based agent,
which supports stable primitive actions (e.g., walk, rotate,
crawl) and interactions (e.g., grab, open, step on) by trading
off its physical plausibility. Speciﬁcally, we classify objects
with their mass: For the light objects, the agent is relatively
kinematic and apply the collision force only to the object.
For a heavy object, the object is relatively kinematic and
pushes the agent out of the collision.

On top of that, VECA provides an user-friendly interface
where the users can implement interactions between agent
and object. Similar to AI2-THOR (Kolve et al. 2017), the
object is interactive with the agent when the object is visible
(rendered by agent’s vision and also closer than 1.5m) and
not occluded by transparent objects. When there are multiple
interactable objects, the agent could choose among those ob-
jects or follow the default order (closest to the center of the
viewport).

Vision. To mimic human vision, we implement a binocu-
lar vision using two Unity RGB cameras. The core challenge
for vision implementation lies in simulating the diverse vari-
ants of human vision in the real-world. For instance, hu-
man vision varies with age: the infant’s vision has imper-
fect color and sharpness (Adams, Maurer, and Cashin 1990;
Dobson and Teller 1978). To simulate these various factors,
we design multiple visual ﬁlters (e.g., changing focal length,
grayscale, blur) and allow the users to use these features in
combination to simulate a particular vision system.

Audio. The auditory sense allows human to recognize
events in the blind spot and even roughly estimate the audio
source’s position. This is possible because the auditory sen-
sor uses the time and impulse difference between two ears
and receives the sound affected by its head structure (Po-
tisk 2015). A common approach to simulate hearing is to
use AudioListener provided by Unity and utilize the Unity
SDK for 3D spatialization. However, we found that this ap-
proach makes it impossible to design tasks with multiple
agents or to simulate multiple environments in a single ap-
plication since Unity supports only one listener per scene.

To provide human-like rich auditory sense to the
agent, we address the problems in two folds. First, we
adopt the LISTEN HRTF (head-related transfer function)
dataset (Olivier Warusfel 2003) to simulate the spatial au-
dio according to the human’s head structure. Second, we al-
low multiple listeners in the scene and supports various ef-
fects such as 3D spatialization or reverb. To support those
features, each listener maintains the list of audio sources,
calculates the distance from the agent and the room im-
pulse (Allen and Berkley 1979) of the audio source, and sim-
ulates the audio using those pieces of information. Note that
we also enable the developer to listen to the agent’s audio

data using the audio devices for debugging purposes.

Tactile. Tactile perception takes an important role when
the agent physically controls and interacts with the object,
but it is challenging to model in Unity. A common approach
to implement tactile perception is to model the body as a
system of rigid bodies and calculate the force during the
collision with other objects by dividing the collision im-
pulse with the simulation interval. However, this approach
makes the tactile perception sensitive to the simulation in-
terval since rigid body collision happens instantly. For ex-
ample, if we halve the simulation interval for accurate simu-
lation, then the collision force is doubled, although the same
collision happened. Moreover, Unity only imposes the to-
tal impulse of the collision, so the tactile data would be in-
correctly calculated when there are multiple contact points,
which frequently happens in real-life situations (e.g., hand
grabbing a phone).

To simulate tactile perception, we approximate the col-
lision force using Hooke’s law, which has not only been
adopted for implementing virtual tactile sensors (El Bab
et al. 2008) but also used for real tactile sensors (Ren et al.
2018). Speciﬁcally, we model the human body as rigid bones
covered with ﬂexible skin (which obeys Hook’s law). When
the collision occurs in the soft skin, we calculate the force
using Hooke’s law. If the collision occurs in the bone, the
collision is handled by the physics engine and the force is
calculated as the maximum force. We normalize the tactile
input by dividing the force with the maximum force, which
is calculated by Equation 1 with the spring displacement d
with the maximum displacement dmax.

T (d) = min(1,

d
dmax

)

(1)

For the position of sensors, we distribute six sensors on
triangle formulation for each triangle in the mesh of the
agent. Each sensor senses the normal component of the nor-
malized pressure to its triangle using Eq. 1.

Proprioception. VECA also provides proprioceptive sen-
sory data. The raw forms of human proprioceptive sense,
such as pressure sensors from muscles and joints, are hard
to simulate and give unnecessary noise to the desired pro-
prioceptive information. Thus, we directly provide physical
information, such as bone orientation, current angle, and an-
gular velocity of the joints, as proprioception.

Environment Manager
We design an environment manager that collects observa-
tions and performs actions with the avatar in the Unity
environment. The main challenge is that the time inter-
val between consecutive frames ﬂuctuates in Unity. This
is because Unity focuses on providing perceptually natural
scenes to the user and adapt the frame rate to available com-
puting power. Also, communication latency affects the time
intervals; for instance, when there is a large communication
delay, the simulation time is fast-forwarded to compensate
for the delay. Although the simplest way would be to limit
the users to implement the environment in FixedUpdate()

Figure 2: Workﬂow diagram about how VECA can be used in training and testing human-like agents. On the environment side,
the user designs tasks and implement environment using assets of Unity. Using VECA, the users can import humanoid agent
with human-like perception and interaction, deﬁne interactions between object and agent, with no concern about management
of the inner simulation loop for the environment. On the algorithm side, the user designs and implements novel human-like
learning algorithms using python. Using VECA, the user can realize those algorithms with the environment.

(which is called in a ﬁxed rate), this approach makes Up-
date() function, which is mainly used in Unity, out of sync.
To address the problem, we separate the clock of the vir-
tual environment from the actual time by implementing the
time manager class VECATime, which replaces the Time
class in Unity. Speciﬁcally, we enable VECA to simulate
time-dependent features (e.g., physics, audio clip, anima-
tion) with constant time steps. For the physics, VECA adopts
the physics simulator provided by Unity to simulate physics
for the constant time step. For other time-dependent fea-
tures, VECA searches for all objects with corresponding fea-
tures in the environment and explicitly controls the simula-
tion time to enforce a constant time interval.

Communication Interface
This component manages the data ﬂow between the agent
algorithm (Python) and the environment (Unity), and pro-
vides a socket-based network connection to support train-
ing agents on remote servers. Since Unity does not support
rendering visual information in servers without graphics de-
vices, the Unity environment needs to be executed in a lo-
cal machine with displays to get visual data or visualize the
training status. This component allows the communication
between the agent in the remote server with the Unity envi-
ronment in the local machine.

How to Use VECA
VECA provides user-friendly interfaces to design custom
environments, tasks, agents, and object-agent interactions
as shown in Fig. 2. Firstly, users need to create a virtual
environment. For this, the user could rely on Unity, which
has many existing assets and APIs to implement virtual en-
vironments. Secondly, users need to deﬁne the perceptions
and actions of an agent. For this, VECA provides a hu-
manoid avatar and useful APIs to customize it. In speciﬁc,
users need to implement three main functions (similar to
ML-Agents (Juliani et al. 2018)): (1) AgentAction(action)
to make the agent perform a particular action, (2) CollectO-
bservation() to collect various sensory inputs, and (3) Agen-
tReset() to reset the agent. To be more speciﬁc, users can im-
plement AgentAction(action) by importing existing interac-
tions or implementing new interactions over VECAObjectIn-
teract interface. Finally, the user builds the environment (in-

cluding the agent) as a standalone application. Those envi-
ronments could be used to train the agents with novel learn-
ing algorithms in python, where many machine learning li-
braries exist, using the python API provided by VECA.

Various Tasks Towards Human Intelligence
To motivate VECA, we provide various tasks that represent
building blocks of human learning, as shown in Fig 3. We fo-
cused on four essential aspects in early human development:
joint-level locomotion and control, understanding contexts
of objects, multimodal learning, and multi-agent learning.
Note that these categories are neither disjoint nor unique.

Joint-level Locomotion & Control. We provide a set of
tasks that feature learning joint-level locomotion and object-
humanoid interaction with diverse objectives. Humanoid
agent control has been studied for human assistance and also
to understand the underlying human cognition. However,
developing a human-level control is extremely challeng-
ing (Akkaya et al. 2019). Prior environments are domain-
speciﬁc and difﬁcult to integrate the human-like aspects.
VECA supports joint-level dynamics down to ﬁnger knuck-
les, physical interactions, and tactile perception. With this
capability, we build a set of challenging joint-level control
tasks {TurnBaby, RunBaby, CrawlBaby, SitBaby, Rotate-
Cube, SoccerOnePlayer}. We also incorporate multimodal
learning tasks (GrabObject) and tasks related to understand-
ing objects (PileUpBlock, PileDownBlock).

Understanding Contexts of Objects. We provide various
tasks {ColorSort, ShapeSort, ObjectNav, BabyZuma’s Re-
venge, MazeNav, FillFraction} which aims to understand
contexts of objects. Learning abstract contexts of objects
is one of the key features in human learning (Cohen and
Cashon 2007; Entwistle 2007), and is getting increasing
attention in artiﬁcial intelligence (Vercauteren et al. 2019;
Mikolov et al. 2013). Thanks to the extensibility of VECA,
users can implement tasks and environments with vari-
ous useful contexts, including properties (ColorSort, Shape-
Sort, ObjectNav), functionality (BabyZuma’s Revenge), and
mathematical meanings (FillFraction).

Multimodal Learning. For multimodal learning, we pro-
vide tasks {KickTheBall, ObjectPhysNav, MoveToTarget}

Figure 3: Set of tasks and playgrounds supported by VECA.

which features in learning how to incorporate multiple sen-
sory inputs. Multimodal learning plays a big role in human
learning(Tacca 2011; Landau, Smith, and Jones 1998), and
have also been suggested to be beneﬁcial for training arti-
ﬁcial intelligence(De Vries et al. 2017; Ngiam et al. 2011).
Using rich perceptions provided by VECA, users can de-
velop tasks and environments for multimodal learning, such
as vision-audio (KickTheBall) and vision-tactile (Object-
PhysNav, MoveToTarget) learning.

Multi-Agent Reinforcement Learning. We provide a set
of multi-agent RL tasks in which agents need to cooper-
ate or compete with others to solve the tasks. Multi-agent
learning is critical in human development
(Eckerman and
Whatley 1977; Trevarthen 1979) and is a rising topic for
artiﬁcial general intelligence (Lowe et al. 2017; Foerster
et al. 2017). However, it is difﬁcult to generate a multi-
agent task and to integrate the multimodal perception and
active interactions (including interactions between agents)
to the task. With the multi-agent support of VECA, we build
a competitive task (SoccerTwoPlayer), a cooperative task
(MultiAgentNav), and a mixed competitive-cooperative task
(PushOut). Some of them also include other features such as
multimodality (MultiAgentNav) or joint-level physics (Soc-
cerTwoPlayer).

Supervised Learning. Although VECA is designed for
interactive agents, supervised learning still plays an impor-
tant role in training/testing machine-learning agents. For ex-
ample, some users may pre-train their agent with supervised
tasks or evaluate their agent using supervised downstream
tasks. In this light, we also provide labeled datasets collected
from VECA for several supervised learning problems, such
as image classiﬁcation, object localization, sound localiza-
tion, and depth estimation. See the appendix for a detailed

description of the datasets.

Playgrounds
VECA provides four exemplar playgrounds, as shown in
Fig 3. Those playgrounds vary in numbers of props, furni-
ture, and its structure. It allows controlling the complexity of
tasks such as ObjectNav, KickTheBall, MultiAgentNav, i.e.,
the same tasks could be tested in different complexity. For
the details of the playgrounds, please refer to the appendix.

Experiments
We performed a set of experiments to show and evaluate
the effectiveness of VECA on training and testing human-
like agents. Please note that although it is ideal to evalu-
ate the agents with human-like learning algorithms, we used
conventional reinforcement/supervised learning algorithms
since they are yet to be fully developed due to a lack of tools
like VECA.

Results on Tasks for Human-like Agents
Experiment Setup. Among various RL algorithms, we
applied PPO (Schulman et al. 2017) and SAC (Haarnoja
et al. 2018), widely used state-of-the-art reinforcement
learning algorithms that can be used to learn VECA-
supported example tasks. For the tasks, we picked one rep-
resentative task per each aforementioned task categories:
GrabObject, ObjectNav, KickTheBall, MultiAgentNav. We
experimented with all compatible playgrounds to also ob-
serve the effect of playgrounds on the agent’s performance.

Results. As shown in Fig. 4, performance of learning al-
gorithms show noticeable differences, varying from easily
solvable tasks (ObjectNav, KickTheBall) to highly challeng-
ing tasks (MultiAgentNav, ObjectNav and KickTheBall in

Figure 4: Visualization of training progress over average reward of agents trained by PPO (Proximal Policy Optimization) and
SAC (Soft Actor-Critic). The agents are trained on subset of tasks provided by VECA: ObjectNav, KickTheBall, MultiAgentNav,
GrabObject, which represents essential aspects in early human development. Best viewed in color.

(without spatialization). For tactile perception, we trained
an agent to perform the GrabObject task trained with and
without tactile perception. We evaluated the performance of
the agent with the average reward of the agent. For technical
details, please refer to the appendix.

Results. As shown in Fig. 5, the existence and the qual-
ity of the perception play an important role in the agent’s
performance. On the auditory perception, the agent without
spatialized audio got a low score since the agent couldn’t use
the audio information to ﬁgure out the direction of the ball.
The agent without HRTF was able to use the information of
audio to get a high score, but it required much more training
time to learn it. The agent with full spatialized audio could
get the maximum score with short training time. On the tac-
tile perception, there exists a noticeable difference between
agents with and without tactile perception in terms of ﬁnal
performance and its training speed. These results show that
the features of perceptions provided by VECA are critical to
the agent’s performance.

Conclusion

We proposed VECA, a virtual toolkit for building virtual
environments to train and test emerging human-like agents.
VECA provides a virtual humanoid agent with rich human-
like perceptions, a joint-level physics, and an environment
for the agent to interact, facilitating the development of
the human-like agents. VECA also provides an environ-
ment manager managing the internal simulation loop of
the environment for the agent, and a communication in-
terface, which enables the users to train the agent using
python-based learning algorithms. Our experiments show
that various tasks towards human intelligence can be eas-
ily generated with VECA, and they are challenging to solve
with recent RL algorithms. Moreover, multimodal percep-
tion of VECA plays an important role in training human-like

Figure 5: Learning curve (average reward by step) for Kick-
TheBall task with different quality of auditory perception
and GrabObject task with/without tactile perception. Best
viewed in color.

VECAHouseEnv, VECAHouseEnv2). It shows that there is a
room for novel human-like learning algorithms to strike in,
and also indicates that using VECA, users can build chal-
lenging tasks for engaging human-like agents.

Also, for the KickTheBall and ObjectNav tasks, the per-
formance of the agent and its training speed signiﬁcantly
differ with the playground. In detail, learning KickTheBall
task in the VECASimpleEnv and VECASingleRoomEnv play-
ground shows noticeable performance enhancement, but the
performance increase is negligible in the VECAHouseEnv
and VECAHouseEnv2 playground. It shows that the users
can moderate the difﬁculty of the task using the playgrounds.

Effectiveness of Multimodal Perception
Experiment Setup. We compared the performance of the
agents trained with PPO in various quality of auditory and
tactile perception, to show that the perception in VECA
takes an essential role in training human-like agents. For au-
ditory perception, we trained an agent to perform the Kick-
TheBall task in VECASingleRoomEnv with varying audio
quality: stereo + HRTF, stereo (without HRTF), and mono

agents. We believe that the features of VECA would be use-
ful for developing environments to train and test human-like
agents.

Acknowledgement
This work was supported by Institute of Information &
Communications Technology Planning & Evaluation (IITP)
grant funded by the Korea government (MSIT) (No. 2019-
0-01371, Development of brain-inspired AI with human-like
intelligence).

References
Adams, R. J.; Maurer, D.; and Cashin, H. A. 1990. The
inﬂuence of stimulus size on newborns’ discrimination of
chromatic from achromatic stimuli. Vision research 30(12):
2023–2030.
Akkaya, I.; Andrychowicz, M.; Chociej, M.; Litwin, M.;
McGrew, B.; Petron, A.; Paino, A.; Plappert, M.; Powell,
G.; Ribas, R.; et al. 2019. Solving rubik’s cube with a robot
hand. arXiv preprint arXiv:1910.07113 .
Allen, J. B.; and Berkley, D. A. 1979.
Image method for
efﬁciently simulating small-room acoustics. The Journal of
the Acoustical Society of America 65(4): 943–950.
Beattie, C.; Leibo, J. Z.; Teplyashin, D.; Ward, T.; Wain-
wright, M.; K¨uttler, H.; Lefrancq, A.; Green, S.; Vald´es,
V.; Sadik, A.; et al. 2016. Deepmind lab. arXiv preprint
arXiv:1612.03801 .
Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.
2013. The arcade learning environment: An evaluation plat-
form for general agents. Journal of Artiﬁcial Intelligence
Research 47: 253–279.
Brodeur, S.; Perez, E.; Anand, A.; Golemo, F.; Celotti,
L.; Strub, F.; Rouat, J.; Larochelle, H.; and Courville, A.
2017. Home: A household multimodal environment. arXiv
preprint arXiv:1711.11017 .
Caselles-Dupr´e, H.; Ortiz, M. G.; and Filliat, D. 2019.
Symmetry-Based Disentangled Representation Learning re-
quires Interaction with Environments. In Advances in Neu-
ral Information Processing Systems, 4606–4615.
Chang, A.; Dai, A.; Funkhouser, T.; Halber, M.; Niessner,
M.; Savva, M.; Song, S.; Zeng, A.; and Zhang, Y. 2017.
Matterport3D: Learning from RGB-D Data in Indoor En-
vironments. International Conference on 3D Vision (3DV)
.
Cohen, L. B.; and Cashon, C. H. 2007.
Handbook of child psychology 2.
De Vries, H.; Strub, F.; Chandar, S.; Pietquin, O.;
Larochelle, H.; and Courville, A. 2017. Guesswhat?! vi-
sual object discovery through multi-modal dialogue. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 5503–5512.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
Imagenet: A large-scale hierarchical image
Fei, L. 2009.
database. In 2009 IEEE conference on computer vision and
pattern recognition, 248–255. Ieee.

Infant cognition.

Dobson, V.; and Teller, D. Y. 1978. Visual acuity in human
infants: a review and comparison of behavioral and electro-
physiological studies. Vision research 18(11): 1469–1483.
Duan, Y.; Schulman, J.; Chen, X.; Bartlett, P. L.; Sutskever,
I.; and Abbeel, P. 2016. RL2: Fast reinforcement learn-
arXiv preprint
ing via slow reinforcement
arXiv:1611.02779 .
Eckerman, C. O.; and Whatley, J. L. 1977. Toys and social
interaction between infant peers. Child Development 1645–
1656.

learning.

El Bab, A. M. F.; Tamura, T.; Sugano, K.; Tsuchiya, T.;
Tabata, O.; Eltaib, M. E.; and Sallam, M. M. 2008. De-
sign and simulation of a tactile sensor for soft-tissue com-
pliance detection. IEEJ Transactions on Sensors and Micro-
machines 128(5): 186–192.
Entwistle, N. 2007. Conceptions of learning and the expe-
rience of understanding: Thresholds, contextual inﬂuences,
and knowledge objects. .

Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and
Whiteson, S. 2017. Counterfactual multi-agent policy gra-
dients. arXiv preprint arXiv:1705.08926 .
Franchak, J. M.; van der Zalm, D. J.; and Adolph, K. E.
2010. Learning by doing: Action performance facilitates af-
fordance perception. Vision research 50(24): 2758–2765.
Gemmeke, J. F.; Ellis, D. P.; Freedman, D.; Jansen, A.;
Lawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M.
2017. Audio set: An ontology and human-labeled dataset
In 2017 IEEE International Conference
for audio events.
on Acoustics, Speech and Signal Processing (ICASSP), 776–
780. IEEE.

Gibson, E. J. 1988. Exploratory behavior in the develop-
ment of perceiving, acting, and the acquiring of knowledge.
Annual review of psychology 39(1): 1–42.
Haarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.
Soft actor-critic: Off-policy maximum entropy deep rein-
forcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290 .
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Delving deep
into rectiﬁers: Surpassing human-level performance on im-
agenet classiﬁcation. In Proceedings of the IEEE interna-
tional conference on computer vision, 1026–1034.
Hermann, K. M.; Hill, F.; Green, S.; Wang, F.; Faulkner, R.;
Soyer, H.; Szepesvari, D.; Czarnecki, W. M.; Jaderberg, M.;
Teplyashin, D.; et al. 2017. Grounded language learning in
a simulated 3d world. arXiv preprint arXiv:1706.06551 .
Ho, J.; and Ermon, S. 2016. Generative adversarial imita-
tion learning. In Advances in neural information processing
systems, 4565–4573.
Johnson, M.; Hofmann, K.; Hutton, T.; and Bignell, D. 2016.
The Malmo Platform for Artiﬁcial Intelligence Experimen-
tation. In IJCAI, 4246–4247.
Juliani, A.; Berges, V.-P.; Vckay, E.; Gao, Y.; Henry, H.;
Mattar, M.; and Lange, D. 2018. Unity: A general platform
for intelligent agents. arXiv preprint arXiv:1809.02627 .

Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;
Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;
Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the
game of Go with deep neural networks and tree search. na-
ture 529(7587): 484–489.
Sloman, A. 1999. What sort of architecture is required for
a human-like agent? In Foundations of rational agency, 35–
52. Springer.

Song, Y.; Wojcicki, A.; Lukasiewicz, T.; Wang, J.; Aryan,
A.; Xu, Z.; Xu, M.; Ding, Z.; and Wu, L. 2020. Arena: A
General Evaluation Platform and Building Toolkit for Multi-
Agent Intelligence. In AAAI, 7253–7260.
Tacca, M. C. 2011. Commonalities between perception and
cognition. Frontiers in psychology 2: 358.
Trevarthen, C. 1979. Communication and cooperation in
early infancy: A description of primary intersubjectivity. Be-
fore speech: The beginning of interpersonal communication
1: 530–571.

Vercauteren, T.; Unberath, M.; Padoy, N.; and Navab, N.
2019. Cai4cai: the rise of contextual artiﬁcial intelligence
in computer-assisted interventions. Proceedings of the IEEE
108(1): 198–214.

Vinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;
Dudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds,
T.; Georgiev, P.; et al. 2019. Grandmaster level in Star-
Craft II using multi-agent reinforcement learning. Nature
575(7782): 350–354.

Vogt, F.; Hauser, B.; Stebler, R.; Rechsteiner, K.; and Urech,
C. 2018. Learning through play–pedagogy and learning out-
comes in early childhood mathematics. European Early
Childhood Education Research Journal 26(4): 589–603.
Wu, Y.; Wu, Y.; Gkioxari, G.; and Tian, Y. 2018. Building
generalizable agents with a realistic and rich 3D environ-
ment. arXiv preprint arXiv:1801.02209 .
Xia, F.; R. Zamir, A.; He, Z.-Y.; Sax, A.; Malik, J.; and
Savarese, S. 2018. Gibson env: real-world perception for
embodied agents. In Computer Vision and Pattern Recogni-
tion (CVPR), 2018 IEEE Conference on. IEEE.

Kempka, M.; Wydmuch, M.; Runc, G.; Toczek, J.; and
Ja´skowski, W. 2016. Vizdoom: A doom-based ai re-
search platform for visual reinforcement learning. In 2016
IEEE Conference on Computational Intelligence and Games
(CIG), 1–8. IEEE.

Kolve, E.; Mottaghi, R.; Han, W.; VanderBilt, E.; Weihs, L.;
Herrasti, A.; Gordon, D.; Zhu, Y.; Gupta, A.; and Farhadi,
A. 2017. Ai2-thor: An interactive 3d environment for visual
ai. arXiv preprint arXiv:1712.05474 .

Lake, B. M.; Ullman, T. D.; Tenenbaum, J. B.; and Gersh-
man, S. J. 2017. Building machines that learn and think like
people. Behavioral and brain sciences 40.

Landau, B.; Smith, L.; and Jones, S. 1998. Object perception
and object naming in early development. Trends in cognitive
sciences 2(1): 19–24.

Lowe, R.; Wu, Y. I.; Tamar, A.; Harb, J.; Abbeel, O. P.;
and Mordatch, I. 2017. Multi-agent actor-critic for mixed
cooperative-competitive environments. In Advances in neu-
ral information processing systems, 6379–6390.

Maiolino, P.; Maggiali, M.; Cannata, G.; Metta, G.; and Na-
tale, L. 2013. A ﬂexible and robust large scale capacitive tac-
tile system for robots. IEEE Sensors Journal 13(10): 3910–
3917.

Marcus, G. 2018. Deep learning: A critical appraisal. arXiv
preprint arXiv:1801.00631 .

Metta, G.; Natale, L.; Nori, F.; Sandini, G.; Vernon, D.;
Fadiga, L.; Von Hofsten, C.; Rosander, K.; Lopes, M.;
Santos-Victor, J.; et al. 2010. The iCub humanoid robot:
An open-systems platform for research in cognitive devel-
opment. Neural Networks 23(8-9): 1125–1134.

Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013. Distributed representations of words and
phrases and their compositionality. In Advances in neural
information processing systems, 3111–3119.

Ngiam, J.; Khosla, A.; Kim, M.; Nam, J.; Lee, H.; and Ng,
A. Y. 2011. Multimodal deep learning. In ICML.

Warusfel,

Olivier
I.
http://recherche.ircam.fr/equipes/salles/listene.

Room
LISTEN

HRTF

2003.

Acoustics

Team,
DATABASE.

OpenAI.
https://universe.openai.com.

2016.

Openai

universe.

Piaget, J.; and Cook, M. 1952. The origins of intelligence
in children, volume 8. International Universities Press New
York.

Potisk, T. 2015. Head-related transfer function.

Ren, Z.; Nie, J.; Shao, J.; Lai, Q.; Wang, L.; Chen, J.; Chen,
X.; and Wang, Z. L. 2018. Fully elastic and metal-free tac-
tile sensors for detecting both normal and tangential forces
based on triboelectric nanogenerators. Advanced Functional
Materials 28(31): 1802989.

Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 .

Dataset for Supervised Learning
We collected the data in VECASimpleEnv to prevent multi-
ple objects is included in the vision datasets. All dataset con-
sists of 80000 image/audio/tactile data, collected in various
perspectives, orientations.

Object classiﬁcation (Vision). The agent has to classify
images by its included object. There are three types of ob-
jects: doll, ball, and pyramid.

Object classiﬁcation (Tactile). The agent s to classify
four shapes: pyramid, sphere, cube, cylinder. The object is
dropped from above the hand. The agent receives the tactile
data sensed by hand during 128 time-steps (total 0.512s).

Distance estimation (Vision). The agent has to estimate
the distance between the camera and the object. Note that
the distance uses the meter unit, so it may not be practical to
apply regression without normalization.

Object recognition (Vision). The agent has to estimate
the bounding box of the object projected to the camera. The
bounding box is represented as (x, y, h, w), which indicates
x and y coordinate of the center, height, and width.

Sound localization (Audio). The agent has to localize the
direction of the sound. The agent receives spatialized audio
data for 0.2s.

Training Details & Baseline results
We used NVIDIA GeForce GTX 1060 6GB to train the
agent in Ubuntu 16.04. Results are averaged with ﬁve runs.
We used the architecture the same as those of reinforcement
learning experiments. We trained the model for 50 epochs.
Baseline results are shown in Table 2.

Task(Perception)
Classiﬁcation (Vision)
Classiﬁcation (Tactile)
Recognition (Vision)
Distance (Vision)
Localization (Audio)

Result (metric)
99.7% ± 0.02% (top-1 accuracy)
94.8% ± 0.64% (top-1 accuracy)
76.1% ± 1.52% (top-1 accuracy)
1.90 ± 0.30 (relative abs. error)
0.80% ± 0.03 (cosine similarity)

Table 2: Baseline result of datasets.

Training Details: RL
For our experiments, we used Intel(R) Core(TM) i5-9600KF
with 3.70GHz for simulating the environment in Windows
10 and used NVIDIA GeForce GTX 1060 6GB to train the
agent in Ubuntu 16.04. Results are obtained with single run.

Perception/Action
Vision. We sampled binocular RGB vision data in a reso-
lution of 84x84. For the GrabObject task, the agent always
looks toward the object (i.e., the object is always located at
the center of each vision).

Audio. We sampled the audio data at the rate of 22050Hz
and converted the audio data to frequency-domain by FFT
with the window size of 1024.

Figure 6: Architecture used for experiments. For SAC, the
model outputs the Q value. For supervised learning prob-
lems, the model outputs the corresponding results.

Tactile. For the GrabObject task, the agent can sense the
tactile perception sensed by taxels from bones in hand. We
say the taxel is from a bone when the taxel is closer to that
bone than any other bones.

Action. For the GrabObject task, we set the torque applied
to each joint as action and used applyTorque() function to
move the agent according to the action. For other tasks, we
set the velocity vector of walking as the agent’s action and
used the walk() function to move the agent.

Helper Rewards
To accelerate the training process of RL tasks, we gave
helper rewards to the agent.

KickTheBall. We gave a helper reward calculated as
0.01cosθ, where θ is the angle between the velocity vector
and the displacement vector to the ball.

ObjectNav & MultiAgentNav. We gave a helper reward
calculated as V is·(0.05·af +0.03·al ·L). af and al is value
of velocity vector projected to forward and left direction.
V is is 1 when the object is visible to the agent. L is 1 / −1
when the object is left / right side of the agent.

GrabObject. To encourage the agent to move its hands to
the object, we gave a helper reward calculated as dt−1 − dt,
while sum-of-distance dt = |Rt−Ot|+|Lt−Ot| (Lt, Rt, Ot
is the position vector of left hand/right hand/object). On top
of that, we also gave a negative quadratic penalty, which is
calculated as −0.004 ∗ |at|2 where at is the action vector, to
prevent agent from performing extreme actions.

Hyperparameters & Architectures
Throughout the experiment, we used the architecture as
shown in Fig. 6. For the unused perceptions, the correspond-
ing component is ignored. For the hyper-parameters used in
RL algorithms, please see Table 3.

Locomotions with Humanoid Agent
To show the physical capability of provided humanoid agent,
we demonstrate locomotions with the humanoid agent: turn-
ing its body (Fig. 7) and sitting (Fig. 8).

Figure 7: Humanoid agent turning its body.

Figure 8: Humanoid agent sitting from lying down.

Parameter
Learning rate
Number of workers
λ (GAE)
Clipping ratio
Entropy coefﬁcient
Gradient norm clipping
γ Discount factor
Optimizer
Training epoch/batch per update
Value function Coefﬁcient
Batch size

PPO
Adaptive to KL Div.
8
0.95
0.2
0.03
5
0.99
Adam
4/4
0.5
N/A

SAC
2.5e-4
8
N/A
N/A
0.01
5
0.99
Adam
N/A
N/A
64

Table 3: Hyperparameters of algorithms used in RL experiments.

ObservationUtils

ﬂoat[] getImage (camera, height, width,
grayscale)

ﬂoat[] getSpatialAudio (head, earL, earR,
env)

ﬂoat[] getSpatialAudio (head, earL, earR,
env, Vector3 roomSize, ﬂoat beta)

ﬂoat[] getTactile (mesh)

ﬂoat[] getTactile (taxels, taxelratio, debug)

GeneralAgent

void AddObservations (key, observation)

Return the image of the camera with the size
of (height, width). Returns grayscale image if
grayscale is true, otherwise returns RGB image.
Return the raw spatilized audio data with audio
sources included in env based on the position of
head and both ears (earL, earR).
Return the raw spatilized audio data, also including
the room impulse generated by the room size of
roomSize and reﬂection coffecient beta.
Return the tactile data with the mesh of the agent.
Return the tactile data with the taxels of the agent.
If debug is true, each taxel displays a blue line
which represents the direction and impulse of tac-
tile perception. To prevent massive display of lines
during debugging, user could disable some of the
taxels using taxelratio.
Add the observation vector in its data buffer with
its key.

Table 4: Example of VECA APIs related to perception of the agent.

Class Name

Function

VECAHumanoid
ExampleInteract

void walk (walkSpeed, turnSpeed)

void kick (obj)
void grab (obj)
void release ()
void adjustFocalLength(newFocalLength)

void lookTowardPoint(pos)

void rotateUpDownHead(deg)

void rotateLeftRightHead(deg)

void makeSound(audiodata)

bool isVisible(obj, cam)

bool isInteractable(obj)

List<VECAObjectInteract>
actableObjects()

getInter-

VECAObjectInteract
ject()

getInteractableOb-

VECAHumanoid
PhysicalExample
Interact

List<Vector3> GetVelocity()
List<Vector3> GetAngVelocity()

List<ﬂoat> GetAngles()

void ApplyTorque(normalizedTorque)

void updateTaxels()

Description
Make the agent walk. Speed and trajectory of the
agent is determined by walkSpeed and turnSpeed.
Make the agent kick the object (obj).
Make the agent grab the object (obj).
Make the agent release the grabbed object.
Adjust the focal distance of the cameras.
Make the agent look at 3-dimensional point. Note
that this function doesn’t rotate the head : it only
rotates the camera(eye). Also, the agent will look at
the point until releaseTowardPoint() is called, even
when the agent is moving.
Rotate the head in the Up-Down plane. Head goes
down when deg>0. Please note that this function
doesn’t have any constraints: Excessive rotation
might distort the mesh of the agent.
Rotate the head in the Left-right plane. Head goes
right when deg>0. Please note that this function
doesn’t have any constraints: Excessive rotation
might distort the mesh of the agent.
Make a voice according to the audiodata. Please
note that the agent also percieves its voice.
Check if the object obj is visible by the camera
cam. Note that the object is considered visible even
if it is occluded by transparent objects.
Check if the object obj is interactable with the
agent. The object is considered interactable when
1) the object is visible, 2) there is no transparent
objects between the object and the agent 3) the dis-
tance is closer than 2.5m.
Returns the list of objects which is interactable
with the agent.
Returns a single object which is interactable with
the agent. If there are multiple interactable objects,
it chooses the object which is closest to the center
of agent’s viewport.
Returns the velocity of the bones.
Returns the angular velocity of the bones.
Returns the current angle for all joints (length is
same to degree of freedom).
Apply the torques to the joint according to the nor-
malizedTorque. Input must be the length same to
the degree of freedom and within [-1, 1].
Update the taxels (used in tactile calculation) ac-
cording to the orientation of the bones and the
mesh of the skin. In default, 6 taxels are distributed
in triangle formulation for each triangle of the
mesh.

Table 5: Example of VECA APIs related to action of the agent.

Figure

Task

Observation

Action space

KickTheBall

Vision, Audio

Continuous
Discrete

ObjectNav

Vision

Continuous
Discrete

/

/

Description
A ball with a buzzing sound continuously comes
out from one of the sidewalls. The agent needs to
go closer to the ball to kick it. The agent has to
use auditory perception to predict the ball’s posi-
tion since its ﬁeld of view is limited.

In the environment, there are multiple objects, in-
cluding the target object. When the agent has nav-
igated to the target object, then the agent receives
+1 reward. If the agent has navigated to the wrong
object, then the agent receives -1 reward.

ObjectPhysNav

Vision, Tactile,
Proprioception

Continuous

Same as ObjectNav, but have to learn the task with
joint-level actions.

MultiAgentNav

Vision, Tactile,
Proprioception

Continuous
Discrete

/

GrabObject

Vision, Tactile,
Proprioception

Continuous

MoveToTarget

Vision, Tactile,
Proprioception

Continuous

Agents are distributed in each room, and the ob-
ject is in one of those rooms. All agents receive
+1 reward when each agent navigates to the ob-
ject. Without cooperation, the agent must traverse
all rooms and ﬁnd the desired object. The agent
who found the object must notify other agents by
making a sound, enabling other agents to navigate
the object using auditory perception.
Learn how to recognize and grab an object with
joint-level actions. The agent is rewarded by the
vertical position difference when the object is close
enough to each hand (preventing the agent from
”throwing” the object).

Learn how to recognize and move the object to the
target position with joint-level actions. The agent
is rewarded by the difference of distance between
the target position and the object’s position.

TurnBaby

Tactile, Propri-
oception

Continuous

Learn how to turn its body with joint-level actions.
The agent is rewarded by the orientation of its torso
and hip.

RunBaby

Tactile, Propri-
oception

Continuous

Learn how to run with joint-level actions. The
agent is rewarded by the velocity to the direction
of its torso.

CrawlBaby

Tactile, Propri-
oception

Continuous

Learn how to crawl with joint-level actions. The
agent is rewarded by the velocity and the orienta-
tion of its torso and hip.

SitBaby

Tactile, Propri-
oception

Continuous

Learn how to sit with joint-level actions. The agent
is rewarded by the position of its torso and head.

RotateCube

Tactile, Propri-
oception

Continuous

Learn how to rotate the cube to target rotation us-
ing its hand with joint-level actions. The agent is
rewarded by the difference in distance between the
target orientation and the cube’s orientation.

Babyzuma’s
revenge

Vision, Audio

Continuous &
Discrete

MazeNav

Vision

Continuous
Discrete

/

Similar to Montezuma’s revenge, the agent has to
learn complex sequence of interactions to complete
the level in sparse reward settings, with vision and
audio senses. For instance, the agent has to: 1) nav-
igate to the drawer, 2) open the drawer, 3) grab the
key (in the drawer), 4) open the door to clear the
ﬁrst room.

the

navigation

Adopted
from
RL2(Duan et al. 2016). The agent is rewarded
when the agent navigates to the object. The agent
can repeat ﬁve trials for each maze.

experiment

ShapeSort

Vision

Continuous &
Discrete

The agent has to sort the objects according to their
shape. The agent is rewarded when the agent grabs
the objects and puts the object to the correct shape
basket.

ColorSort

Vision

Continuous &
Discrete

The agent has to sort the objects according to their
color. The agent is rewarded when the agent grabs
the objects and puts the object to the correct color
basket.

FillFraction

Vision

Continuous &
Discrete

The agent has to ﬁll three circles using the frac-
tion circle objects. The agent can not put the object
when the sum of the fraction exceeds the circle.
The agent is rewarded when the agent puts the ob-
ject to the circle and is additionally rewarded when
it is full.

PileUpBlock

Vision, Tactile,
Proprioception

Continuous

The agent has to pile up the (red) blocks with joint-
level actions. The agent is rewarded according to
the sum of the y-axis position of red blocks.

PutDownBlock

Vision, Tactile,
Proprioception

Continuous

The agent has to put down (only the red) blocks
with joint-level actions. The agent is rewarded ac-
cording to the negative sum of the y-axis position
of red blocks. Additionally, the agent gets a nega-
tive reward when the blue blocks fall.

SoccerOnePlayer

Vision, Tactile,
Proprioception

Continuous

Put the ball into the goal with joint-level actions.
Note that there is no rule: e.g., the agent may use
hands.

SoccerTwoPlayer

Vision, Tactile,
Proprioception

Continuous

The attacker must put the ball into the goal, and
the defender must block the ball from the goal with
joint-level actions. Note that there is no rule: e.g.,
the agent may use hands and block or tackle the
opponent player.

DodgeCar

Vision

Continuous
Discrete

/

The toy car moves around in the room fast. The
agent must dodge the car.

ChaseCar

Vision, Tactile,
Proprioception

Continuous &
Discrete

The toy car moves around in the room fast. The
agent must chase and interact with the car, but must
not collide with the car.

PushOut1v1

Vision, Tactile,
Proprioception

Continuous

The agent must push other agents out of the ring to
win. The agent is equipped with joint-level actions.

PushOut2v2

Vision, Tactile,
Proprioception

Continuous

Agents must push agents of the opponent team out
of the ring to win. The agent is equipped with joint-
level actions.

Table 6: Description of various tasks provided by VECA.

