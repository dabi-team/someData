7
1
0
2

t
c
O
1
3

]

G
L
.
s
c
[

1
v
4
0
0
0
0
.
1
1
7
1
:
v
i
X
r
a

Mining the Unstructured: Accelerate RNN-based Training with Importance
Sampling

Fei Wang∗

Xiaofeng Gao⋆

Weichen Li†

Guihai Chen⋆

Jun Ye‡

Abstract

1 Introduction

Importance sampling (IS) as an elegant and efﬁcient variance
reduction (VR) technique for the acceleration of stochastic
optimization problems has attracted many researches recently.
Unlike commonly adopted stochastic uniform sampling in
stochastic optimizations, IS-integrated algorithms sample train-
ing data at each iteration with respect to a weighted sampling
probability distribution P , which is constructed according to
the precomputed importance factors. Previous experimental
results show that IS has achieved remarkable progresses in
the acceleration of training convergence. Unfortunately, the
calculation of the sampling probability distribution P causes
a major limitation of IS:
it requires the input data to be
well-structured,
the feature vector is properly deﬁned.
Consequently, recurrent neural networks (RNN) as a popular
learning algorithm is not able to enjoy the beneﬁts of IS due to
the fact that its raw input data, i.e., the training sequences, are
often unstructured which makes calculation of P impossible. In
considering of the the popularity of RNN-based learning appli-
cations and their relative long training time, we are interested
in accelerating them through IS. This paper propose a novel
Fast-Importance-Mining algorithm to calculate the importance
factor for unstructured data which makes the application of IS in
RNN-based applications possible. Our experimental evaluation
on popular open-source RNN-based learning applications
validate the effectiveness of IS in improving the convergence
rate of RNNs.

i.e.,

Keywords: Unstructured Data Mining,
Importance
Sampling, Convergence Acceleration, Recurrent Neural
Networks

∗Department of Computer Science and Engineering, Shanghai Jiao
Tong University. bomber@sjtu.edu.cn, {gao-xf, gchen}@cs.sjtu.edu.cn

†Facebook, Inc. weichenli@fb.com
‡Intel Asia-Paciﬁc R&D Ltd. jason.y.ye@intel.com

For the optimization of general ﬁnite-sum problems, e.g.,
empirical risk minimization (ERM), stochastic gradient
descent (SGD) may be the most widely adopted optimizer
algorithm. Assume φi,
are vector func-
1, 2, ..., N
∈ {
tions that map Rd
P means i is drawn itera-
∼
tively with respect to sampling probability distribution P .
This paper studies the following optimization problem:

i
∀
R, i

→

}

(1)

min
w∈Rd

F (w) := Ei∼P (φi(w) + ηr(w))

where r(w) is the regularizer and η is the regularization
factor. Denote fi(w) = φi(w) + ηr(w), for stochastic
gradient descent optimization, w is updated as:

(2)

wt+1 = wt

λ

fit (wt)

∇

−
where it is the index sampled at t-th iteration and λ is the
step-size. The key advantage of SGD is that the compu-
tation of the stochastic gradient is much faster than the
true gradient. It is easy to notice that with a proper sam-
pling distribution P (typically a uniform distribution), the
stochastic gradient equals to the true gradient in expecta-
tion. Assume pt
i as the probability of selecting xi as the
training sample at iteration t, we denote by
(3)
V

= E
k
the gradient variance. It is commonly known that the con-
vergence procedure is severely slowed down when gradi-
ent variance is large.

fit (wt)
(cid:1)

(npt
(cid:0)

fit (wt)

it )−1

it )−1

(npt

−∇

∇

∇

F (wt)
k

2
2

To cope with this problem, original importance sam-
pling (IS) algorithms [1, 8, 10, 14] are developed for vari-
ance reduction in stochastic optimizations by adjusting
the sampling probability distribution P iteratively. De-
spite of its theoretical effectiveness, in industrial applica-

1

 
 
 
 
 
 
tions such algorithms are seldom adopted for their itera-
tively re-estimation of the sampling probability is compu-
tationally infeasible.

fi(w)

Recently, optimized IS algorithms inspired by random-
ized Kaczmarz algorithm were proposed by authors of
[5, 15, 17, 19]. This kind of IS algorithms sample training
data at each iteration t w.r.t to a pre-constructed proba-
bility distribution instead of iterative re-estimation of P .
The sampling distribution P is constructed based on the
supremem of the gradient norm of the data samples, i.e.,
k2. When cooperated with a proper step-
sup
size, the complexity of the upper-bound of training it-
erations to achieve certain accuracy is proved to be de-
creased.
In the rest of this paper, we call this kind of
gradient-norm-supremum based importance sampling al-
gorithm directly as IS for clarity. With the success of
IS, many stochastic optimizations have adopted it and
show impressive convergence results improvements, e.g.,
SVM-based classiﬁcation etc.

k∇

On the other hand, with the fast development of neural
network (NN)s, they have been widely used in modeling
many large-scale optimization problems and have demon-
strated great advantages. Among all these research ﬁelds,
recurrent neural networks (RNN) have unique capabilities
in modeling context-conditioned sequences such as natu-
ral language processing (NLP), acoustic sequences pro-
cessing, time series prediction and achieves much better
training performance (accuracy, convergence rate) than
previous state-of-the-art.

Since we consider IS as an elegant VR technique and
RNN a widely used learning algorithm, we are very inter-
ested in deploying IS in RNN applications which yields
both practical signiﬁcance and novelty. On the other hand,
as is mentioned above, the sampling probability pi is
based on the supremum of
k2, which in turn in-
fi(w)
k∇
volves the calculation of
k2. This poses a major prob-
xi
lem of the current IS algorithms, i.e., the calculation of the
sampling distribution P requires the input data to be well-
structured, i.e., the feature vector of the data samples are
properly deﬁned, otherwise
k2 can not be calculated.
k
Unfortunately, for RNN applications such as NLP, its raw
data are typically randomly mapped into a d-dimensional
space which is in fact unstructured. In fact, in this kind
of RNN-based optimization problems, the feature vec-
tor is only structured by multiplying a mapping matrix
W , which is also to be learned during the optimization,

xi

k

}

{

}

i
∀

∈ {

e.g., the embed matrix W emb for the embedding (struc-
turing) of the input word vector xi in RNN-based NLP.
As a consequence, the sampling probability distribution
P =

can not be constructed.

1, 2, ..., N

,

pi

To solve this bottlenecking problem, we propose a
novel algorithm which mines the importance of RNN
training samples w.r.t which the sampling probability pi
can be calculated and thus the IS can be proceeded in
RNN-based applications, i.e., IS-RNN. Our experimen-
tal results validate that our IS-RNN algorithm is able to
decrease the gradient variance and achieves better conver-
gence results than conventional non-IS RNN.

This paper is organized as follows. In section 2 we give
brief descriptions about necessary preliminaries and con-
cepts of the IS algorithm. We discuss the difﬁculty of
applying current IS in RNN in Section 3. In section 4,
we propose and analyze our optimized IS algorithm for
RNN in detail. The evaluation results of IS on some most
popular RNN-based applications for convergence acceler-
ation are shown in Section 5. In the last section, we make
conclusion of this paper.

2 Importance

Sampling
Stochastic Optimizations

for

We ﬁrst brieﬂy introduce some key concepts of IS. Like
most previous related literatures, we make the following
necessary assumptions for the convergence analysis of the
target stochastic optimization problems.

•

•

fi is strongly convex with parameter µ, that is:
(4)
x
h

fi(x)

fi(y)

− ∇

x, y

i ≥

2
2,

∇

y,

−

−

µ

x

∀

k

k

y

Rd

∈

Each fi is continuously differentiable and
Lipschitz constant Li w.r.t
(5)

⋆, i.e.,

k · k

fi has

∇

fi(x)

k∇

fi(y)
k

− ∇

⋆

≤

Li

x

k

⋆,

y

k

−

x, y

∀

∈

Where

i
∀

∈ {

1, 2, ..., N

.

}

2.1 Importance Sampling

Rd

Importance sampling reduces the gradient variance
through a non-uniform sampling procedure instead of

2

drawing sample uniformly. For conventional stochastic
optimization algorithms, the sampling probability of i-th
sample at t-th iteration, i.e., pt
i, always equal to 1/N while
in an IS scheme, pt
i is endowed with an importance fac-
tor I t
i and thus the i-th sample is sampled at t-th iteration
with a weighted probability:

(6)

i = I t
pt

i /N,

s.t.

pt
i = 1

N

Xi=1

where N is the number of training samples. With this
non-uniform sampling procedure, to obtain an unbiased
expectation, the update of w is modiﬁed as:

(7)

wt+1 = wt

−

λ
npt
it ∇

fit (wt)

where it is drawn i.i.d w.r.t the weighted sampling proba-
pt
bility distribution P t =
,
i}

1, 2, ..., N

i
∀

∈ {

}

{

.

2.2 Importance Sampling for Variance Re-

duction

Recall the optimization problem in Equation 1, using the
analysis result from [19], we have the following lemma:

Algorithm 1 Practical Importance Sampling for SGD

1: procedure IS-SGD(T )
2:

Construct Sampling Distribution P According to

Equation 10

3:
4:

5:

6:
7:

Generate Sample Sequence S w.r.t distribution P .
Sample it from
for i = 0; i

n
i=0 w.r.t distribution P .

i
}
= T ; i++ do

{

it = S[i]
wt+1 = wt

λ
npit ∇

fit (wt)

−

fi, assume
∇
k
RLi, i.e., sup

L-Lipschitz of
fi(wt)
k2 ≤

R for any t, we get
k2 = RLi. Thus
k∇
the actual sampling probability pi is calculated according
to:

k ≤
fi(wt)

k∇

wt

(10)

pi =

Li
N
j=1 Lj

,

i
∀

∈ {

1, 2, ..., N

.

}

P

The authors prove that IS accelerated SGD achieves a
convergence bound as:
(11)
T
1
T

n
i=1 Li
n

E[F (wt)

≤ r k

F (w⋆)]

w0k

(cid:18) P

−
σ

w⋆

−

2
2

(cid:19)

1
T

,

Xt=1

w

k∇

fi(w⋆)
k

Lemma 2.1 Set σ2 = E
2
2 where w⋆ =
1
µ , with the update scheme de-
F (w). Let λ
arg min
ﬁned in Equation 7, the following inequality satisfy:
(8)
E[F (wt+1)

F (w⋆)]

w⋆

w⋆

E[

wt

≤

1
2λ

≤

k

−

−

µE
k

w⋆

wt

k

−

2
2 +

−

λt
µ

EV

2
2 − k
k
(npt

it )−1

−

∇

(cid:16)
it )−1

where the variance is deﬁned as V
(npt
=
∇
E
2
2, and the expectation is
F (wt)
fit (wt)
(cid:0)
k
k
estimated w.r.t distribution P t.

fit (wt)
(cid:1)

it )−1

(npt

−∇

∇

In order to minimize the gradient variance, it is easy to
verify that the optimal sampling probability pt

i is:

(9) pt

i =

fi(wt)

k2
fj(wt)

k∇
N
j=1 k∇

,

i
∀

∈ {

1, 2, ..., N

.

}

P

k2
Obviously, such iteratively re-estimation of P t is com-
pletely impractical. The authors propose to use the supre-
k2 as an approximation. Since we have
mum of

fi(wt)

k∇

while for standard non-IS SGD optimizers that actually
samples xi w.r.t uniform distribution, the convergence
bound is:
(12)
T
1
2
2]
wt+1k
T
fit (wt)

2
w0k
2
σn

n
i=1(L2
i )

≤ r k

E[F (wt)

F (w⋆)]

when λ is set as
(cid:17)
−
cording to Cauchy-Schwarz inequality, we always have:

w0k

i=1 Li
n

. Ac-

√T

Xt=1

1
T

2
2/

w⋆

w⋆

Pn

P

p

−

−

(cid:16)

(cid:17)

σ

k

,

1,

(13)

n
(
P

n
i=1 L2
i
n
i=1 Li)2 ≥
P
which implies that IS does improve convergence bound
and the improvements get more signiﬁcant when
(Pn
Pn

i=1 Li)2
i=1(L2
Clearly, pi of each fi can be analyzed beforehand. For
example, for L2-regularized optimization SVM problem
with squared hinge loss, i.e.,

i ) ≪

n.

(14)

fi(w) = (
⌊

1

−

yiwT xi

⌋+)2 +

λ
2 k

w

2
2

k

3

6
where xi is the i-th sample and yi
corresponding label,

fi(w)

k∇

is the

1, +1
∈ {−
}
k2 can be bounded as
k2 + √λ
xi

k2/√λ)
k

xi

(15)

fi(w)

k2 ≤

2(1 +

k

k

k∇
Since
k2 is the only variable in the calculation of pi,
xi
the sampling distribution P can be constructed off-line
completely. The pseudo code of practical IS-SGD algo-
rithm can be written as Algorithm 1. As can be seen that,
the core procedure of IS is the construction of P . Once P
is constructed, IS-SGD works as same as SGD except that
the training sample is selected w.r.t to P and the step-size
is adjusted with 1/pi. This means that IS as an effective
VR technique can be implemented with almost no extra
online computation which makes it very suitable for VR
of large scale optimization problems.

3 Recurrent Neural Network with

IS

Recurrent neural networks [6, 13, 16, 18] have been devel-
oped with great efforts due to their strong advantages in
modeling large-scale sequences. However, comparing to
convolutional neural networks (CNN), its training is typi-
cally much difﬁcult and slower.

As we have discussed previously, IS has been recently
studied in many optimization problems such as SVM-
based linear-regression or even CNN applications [2].
However we found that the research of applying IS in
RNN is still left missing. In considering of the popularity
and the novelty, we are motivated to study the application
of IS in RNN. Our goal is to improve the convergence re-
sults of RNN training through IS not only effectively but
also efﬁciently. To discuss our algorithm, we ﬁrst give
a brief introduction of RNN since some important con-
cepts are used in designing our optimized IS algorithm
for RNNs.

3.1 Recurrent Neural Network

RNNs are proposed for sequence modeling. Its variants
e.g., long short term memory (LSTM) and gated recurrent
units (GRU) have achieved signiﬁcant progresses in ma-
chine learning tasks such as NLP and acoustic sequences
analyze due to its capability in conditioning the model on

Figure 1: Unrolled Architecture of RNN with 3 Iterations

yt

yt+1

yt+2

W

ht−1

W

ht

W

ht+1

ht+2

xit

xit+1

xit+2

X 1

i , X 2

all previous inputs. Figure 1 shows the general architec-
ture of RNN. For the i-th raw training sample input se-
quence, i.e., Xi =
where n is the size
of the training sample Xi. RNN embeds Xi to xi and
reads in one element of the input vector, i.e., xt
i at each it-
eration t and calculates the prediction yt and hidden state
ht. The calculation step is shown in the following Equa-
tion:

i , ..., X n
i }

{

(16)

xi = W embXi
ht = σ(W hht−1 + W xxt
yt = sof tmax(W sht + by)

i + bh)

RNv ×d: embedding matrix used to convert
W emb
raw word xraw into embedded value, to be learned.

∈

Rd: the t-th element of embedded training sam-

xt
i ∈
ple xi.

RDh×d: weight matrix used to condition the

W x
input xt, to be learned.

∈

RDh×Dh : weight matrix used to condition

W h
the previous hidden state ht−1, to be learned.

∈

RNv ×Dh: weight matrix used to performed

W s
the classiﬁcation ht, to be learned.

∈

RDh : hidden state output of last iteration.

ht−1 ∈
σ: non-linear activation functions.

RDh , by

bh

∈

∈

RN

v : bias terms.

•

•

•

•

•

•

•

•

where d is the size of embedding, Dh is the number of
hidden neurons and Nv is the size of vocabulary. With

4

the corresponding output vector of xi denoted as yi =
as the true
yi1, yi2 , ..., yin }
{
{
probability distribution. The objective function is calcu-
lated as multiclass cross-entropy, i.e.,:

i2, ..., y′

i1 , y′
y′

and y′

i =

in }

(17)

T
i (yi, y′
L

i) =

1
T

n

Xt=1

y′
it log(yit )

3.2 Problems of Applying IS in RNN

During the research of applying IS in RNN to acceler-
ate its training procedure, we met two main bottlenecking
problems caused by the special architecture of RNN. We
give detailed analysis as the following.

3.2.1 Base-Gradient

k∇

fi(wt)

Recall that in IS-SGD, the sampling probability pi of
training data xi is approximated by the supremum of its
gradient norm sup
k2. The ﬁrst question is that
in RNN based on what gradient the sampling distribution
is computed. For example, in a simple linear-regression
problem y = W T x + b, we have only one model (linear
regression matrix W ) to be trained and thus we have no
ambiguousness in sup
k2. However in RNN, it
fi(wt)
can be seen from equation 16 that we have four models
W emb, W h, W x and W s to be learned (assume bh and
by are not considered) and consequently 4 corresponding
i = Li
∂W emb , gh
gradients, i.e., gemb
∂W x
i = Li
and gs

i = ∂Li

∂W h , gx

= ∂Li

k∇

∂W s .

i

, gh

i , gs

Intuitively we have two choices,

the ﬁrst
the four gradients,

is to
i.e.,
use the combination of
G(gemb
i , gx
i ) where G is the combination function.
i
Another way is to choose one of the four gradients based
on which its norm supremum is used to calculate pi ac-
cording to Equation 10. While we have no guidance in de-
signing the combination function G, the second choice is
simpler and most importantly, more easy to explain, e.g.,
we may base the calculation of P on gemb if we consider
the update of W emb affects the convergence more signif-
icantly than others. For clarity, we denote by
the
gradient based on which pi is calculated, i.e., the base-
gradient.

base
i
∇

3.2.2 Unstructured Input

The second bottlenecking problem is that once the
base is selected, obtaining its norm supremum
form
∇
k2 is very difﬁcult due to the fact that W emb
base
sup
i
k∇
is not learned at the beginning which means that the
embedded value of the input xi, based on which the
all four gradients are actually computed, is not decided.
In other words,
is not
properly structured until W emb is converged. With the
learning of W emb, the structuring of the feature vector
keeps proceeding and the value of xi varies accordingly
which brings in much uncertainty in the analyzing of
sup

the feature vector of input xi

In brief, the special architecture of RNN makes the at-
tempt to accelerates its convergence through IS very difﬁ-
cult. To enjoy the beneﬁts of IS, a special modiﬁed variant
of IS algorithm that solves the mentioned two bottleneck-
ing problems efﬁciently for RNN is needed.

base
i
k∇

k2.

4 Mining the Importance for RNN

We propose an optimized IS algorithm which can be ap-
plied in RNN to achieve accelerated convergence proce-
dure by targeting at the two above mentioned bottleneck-
ing problems.

4.1 Base-Gradient Selection

base. As we have
Our ﬁrst step is to select a proper
as one of the four gradi-
discussed above, we select
ents, i.e., gemb
i , gs
i for simplicity and easy explana-
tion. We ﬁrst look into the detailed derivation of the four
gradients:

base
i
∇

i , gx

, gh

∇

i

(18)

gh
i =

T

t

Xt=1

Xk=1

t
∂
i
L
∂yt

∂yt
∂ht

t





Yj=k+1

∂hj
∂hj−1





∂hk
∂W h

gx
i =

T

Xt=1

t
∂
i
L
∂yt

∂yt
∂ht

∂ht
∂W x

gs
i =

T

Xt=1

t
∂
i
L
∂yt

∂yt
∂W s

(19)

(20)

5

(21)

gemb
i =

T

Xt=1

t
∂
i
L
∂yt

∂yt
∂ht

∂ht
∂xit

∂xit
∂W emb

∇

base as the gradient
A reasonable option is to select
which has the fundamental effect for the whole train-
ing procedure. Obviously, according to Equation 16 and
Equation 18 to 21, xi is the only input conditioned by
W x and ht is an intermediate memory value that is actu-
ally decided by h0 and the training sequence < xi, y′
i >
. In fact, it is easy to verify that with
,
i
}
∀
determined initialization models, i.e., bh
0 and
h0, all gradients are totally decided by the sequence of
W xxi, y′
. We thus empirically choose
, i
i}
{
W x as our base-gradient
1, that is, pi is calculated
as:

}
base
∇

1, 2, ..., T

1, 2, ..., t

0 , W s

0, W h

0 , by

∈ {

∈ {

k

(22)

pi =

gx
sup
i k2
N
gx
j=1 sup
j k2
k
base
After the selection of
i
∇
tain, or construct an appropriate proxy of sup

= gx

i
∀

∈ {

P

,

1, 2, ..., N

.

}

i , we are now left to ob-
k2.

base
i
k∇

4.2 Mining the Importance

Since we have selected the base-gradient, obtaining or
ﬁnding proxy of sup
k2 is the following step. As
we have discussed above, xi = W embXi varies with the
proceeding of the training which makes the bounding of

base
i
k∇

k2 technically difﬁcult.

base
i
k∇
Intuitively, one possible method is to periodically re-
calculate P according to the snapshot of W emb. That
the snapshot of W emb at iteration
is, denote by W emb
s
s, calculate embedded dataset xs
1, 2, ..., N

s Xi
,
∈
}
i respectively.
{
Indeed, it seems that with the structuring of the feature
vector (i.e., the training of W emb) getting more accu-
rate, the corresponding ps
i is getting closer to the opti-
mum. However, this procedure is based on an impor-
tant assumption, which is, the initial sampling distribu-
tion P 0 =
calculated based on

i =
i according to xs

and estimate ps

1, 2, ..., N

W emb

i
∀

{

}

,

p0
i }

{

i
∀

∈ {

}

1In fact, we also tested other gradients during the experimental eval-
uations. The result veriﬁes our analysis, i.e., choosing gx
i as the ∇base
based on which P is constructed makes IS in RNN effective while others
are either non-effective or less effective.

Algorithm 2 Fast Importance Mining

1: procedure CONSTRUCT_SAMPLING_DISTRIBUTION(ǫ)
2:

Parallel do for

1, 2, ..., N

:

∈ {
RN Ni=new RNN(W (0), b(0))

}

i
∀

3:

4:
5:

6:

7:

8:

9:

i = MAX_FLOAT

L
While

ǫ

L

≥

i
L
i=Train_RN Ni(xi)
N
W x
j k2
j=1 k
i
∀
∈ {
i k2
Wsum

1, 2, ..., N

Wsum =
Parallel do for
P
pi = kW x

:

}

i = W emb
x0
0 Xi is better than the uniform distribution.
Otherwise, the training procedure is more likely to be
slowed down due to an inferior initial sampling distribu-
tion P 0. Unfortunately, as is obvious, this requirement of
the initially-structured feature vector is in fact not guaran-
teed.

the

fact

obtaining

base
i
k∇

considering
base
i
k∇

In
that
exact
k2 is complex and difﬁcult, we here propose
sup
another simple yet effective Fast-Importance-Mining
(FIM) algorithm for
the construction of proxy of
k2. See Figure 2 for illustration, the FIM
sup
algorithm trains each data sample in parallel to obtain
the proxy value which serves as the same function with
base
k2. Each xi has a private trained RN Ni with
i
k∇
itself as the only training sample. The training for each
RN Ni starts with the same initial value of RNN and
ends with the same accuracy, ǫ, i.e.,
ǫ. Empirically,
we set ǫ much smaller (typically two magnitude lower)
than the standard training for RNN, i.e., trained with the
whole dataset. The accuracy is easy to be met since it
has only one training sample. Denote by W x
the matrix
i
W x in corresponding RN Ni, with the trained W x set
W x
i k2 as
1, 2, ..., N
W =
i }
∈ {
base
k2 and thus the sampling
the proxy value of sup
i
k∇
is calculated as:
probability P =
i
∀

, we use

1, 2, ...N

W x

i
∀

∈ {

i
L

pi

≤

k

}

{

}

}

{

,

,

W x

k
N
j=1 k

i k2
W x

j k2

,

i
∀

∈ {

1, 2, ..., N

.

}

(23)

pi =

P

6

Figure 2: Fast Importance Mining

x1

x2

RN N1

RN N2

W x

1 k2

k

W x

2 k2

k

P

p1 = kW x
PN

1 k2

j=1

p2 = kW x
PN

2 k2

j=1

· · ·

· · ·

· · ·

· · ·

xN

RN NN

W x

N k2

k

pN = kW x
PN

N k2

j=1

4.3 Analysis

ai
j=1 aj

In considering the fact that pi is calculated in the form of
rather an absolute value directly, it is more easy

PN
to ﬁnd a proxy value that faithfully reﬂects the sampling
importance of xi rather than approximate the exact value
of sup

gx
i k2 which is technically difﬁcult.
According to our proposed FIM algorithm, W x
i

is ac-
tually the sum of all history gradients and initial value
W x(0), i.e.,

k

(24)

W x

i = W x(0)

Ti

−

Xt=1

gx
i (t)

where we denote by gx
i (t) as gx
i at iteration t and Ti the
iterations used for RN Ni to be trained with accuracy ǫ.
Due to the triangle inequality, we further have:

(25)

W x

i k2 ≤ k

k

W x(0)

k2 +

Ti

k

Xt=1

gx
i (t)

k2

k

W x

i k2 is to have larger

It is reasonable to conclude that data samples xi with
gx
larger
i k2 which complies
with our expectation of a proxy value and our intuition of
gx
i k2.
giving more sampling importance to xi with larger
We still face the unstructured feature vector prob-
lem in FIM, however it is reasonable to say that once
RN Ni is trained, the feature vector of xi is structured

k

k

i

i

,

,

}

∈ {

i
∀

1, 2, ..., N

(at least for itself). Consequently, since each RN Ni,
∈
is trained separately with its only training
1, 2, ..., N
{
}
sample xi, the corresponding structuring rules that have
been learned, i.e., W emb
, are different
i
∀
from each other and the global rule W emb trained with
the whole dataset. Such differences make Xi embedded
into different xi when in whole dataset training and FIM.
However according to our statistical evaluation, the dif-
and W emb is
ference between W emb
not signiﬁcant and thus we empirically assume that such
inconsistency will not hurt the performance of IS heavily.
Meanwhile, since the structuring of xi is proceeded
with the training process, it is reasonable to condition
the whole structuring procedure as a method to avoid
the effect of structuring variance during the FIM. Thus
we consider W x
i (t) and use its
−
gx
W x
k2 as
i k2 instead of just recording max
norm
i (t)
k
gx
k2. The FIM algorithm of
the proxy value of sup
i (t)
P is rather empirical, however it works surprisingly well
according to our evaluation as to be shown in next section.

i = W x(0)

1, 2, ..., N

Ti
t=1 gx

∈ {

i
∀

P

}

k

k

5 Experimental Results

To validate the effectiveness of IS in RNN, we conduct ex-
perimental evaluations2 on two popular RNN-based ma-
chine learning tasks: LSTM for sentence classiﬁcation
and RNN-RBM for polyphonic music sequence model-
ing. The source code we based on is from the well-
developed deep learning tutorials of Theano[3] which is
popularly used in academic deep learning researches. Par-
ticularly, since the networks of our target evaluation tasks
are variants of standard RNN thus the implementation of
IS changes accordingly as we will describe in detail for
each case. All source code of our experimental evaluation
along with the visualization script can be accessed from
the author’s git repository3.

5.1 LSTM for Sentence Classiﬁcation

LSTM [7, 9, 12] is an important variant of RNN which
solves the gradient-vanishing/explosion problem to a

2Our testbed is a server with Intel Xeon E5-2699 v4 CPU and 128G

DDR3 RAM.

3https://github.com/FayW/DeepLearningTutorials.

7

Figure 3: Architecture of Standard LSTM Network

ht−1

xt

ht−1

xt

ht−1

xt

Uz

Wz

Uc

Wc

Uf

Wf

σ

zt

tanh
˜Ct
Ct−1

σ

ft

Uo

Wo

σ

ot

Ct

+

tanh

⋆

ht

⋆

⋆

Selected as

base

∇

large extent and is widely used in sequence analysis (clas-
siﬁcation, prediction, etc.) tasks. Our evaluation case is
for the sentence analysis which reads in sentences and
predict whether it is a negative or positive comment for
a movie.

Dataset: 25,000 sentences for training set and 1998
for test set.4

•

•

•

Objective function: Multiclass cross-entropy.

(28)

pi =

Evaluation metrics: Error rate.

5.1.1 Base-Gradient Selection

To see how IS is implemented in this variant of RNN, it is
necessary to have a look at the detailed training step:

(26)

i + U zht−1 + bz)
i + U f ht−1 + bf )

i + U cht−1 + bc)

Ct−1
i + U oht−1 + bo)

∗

zt = σ(W zxt
ft = σ(W f xt
˜Ct = tanh(W cxt
˜Ct + ft
Ct = zt
ot = σ(W oxt
ht = ot

∗

tanh(Ct)

∗

4Original code uses 1,998 sentences for training and 25,000 for vali-

dation. We reverse this for better evaluation.

8

where xit is the t-th word of training sentence xi, zt is the
input gate, ˜Ct is the buffered memory cell, ft is the for-
get gate, Ct is the output memory cell, ot is the exposure
gate and ht is the output hidden state. See Figure 3 for
illustration.

ht−1

xt

It can be seen that there are 3 classes of parameters to
be learned, i.e., matrices that condition the input xit , W z,
W c, W f , W o, matrices that condition the output state
ht−1, U z, U c, U f , U o and bias terms bz, bc, bf , bo. Dif-
ferent from base RNN as we discussed above, in LSTM
the output state ht is further gated by the memory cell Ct.
As we have discussed in section 4, we would like to
choose the base-gradient from matrices that conditions xi
rather than hi. We consider W c as a more important pa-
rameters that affects ˜Ct which has the major impact on
the output hidden state ht and thus we choose ∂LT
∂W c as our
base-gradient which can be derived as:

i

(27)

T
∂
i
∂W c =
L

T

Xt=1

t
∂
i
L
∂yt

∂yt
∂ht

∂ht
∂Ct

∂Ct
∂ ˜Ct

∂ ˜Ct
∂W c

According to our proposed algorithm, we train each sam-
ple xi separately in parallel to obtain its private LSTM
i k2. And thus
model and retrieve its corresponding
the sampling distribution P =
is
1, 2, ..., N
calculated as:

k
i
∀

W c

∈ {

pi

}

{

}

,

W c

k
N
j=1 k

i k2
W c

,

i
∀

∈ {

1, 2, ..., N

}

P

j k2
With a pre-constructed P , we generate the training se-
quence S and adjust step-size with (N pi)−1 correspond-
ingly.

5.1.2 Convergence Acceleration Results of IS-LSTM

Denote by IS-LSTM as the proposed IS accelerated RNN
algorithm, the curves of the error rates of training set and
validation set are shown in Figure 4. As can be seen that
in all step-size settings, the error rate of IS-LSTM of both
training set and validation set drops much faster than tra-
ditional LSTM. Particularly, IS-LSTM gains most signif-
icant convergence result improvement when λ = 0.5 and
decreases when λ getting smaller or higher. When step-
size is small, e.g., λ = 0.1, the convergence improvement
It is reasonable to conclude that there should
is small.

0.6

0.4

0.2

e
t
a
R
r
o
r
r
E

i

n
a
r
T

e
t
a
R
r
o
r
r
E

i

n
a
r
T

e
t
a
R
r
o
r
r
E

i

n
a
r
T

e
t
a
R
r
o
r
r
E

i

n
a
r
T

0.50

0.25

0.00

0.50

0.25

0.00

0.50

0.25

0.00

0

0

0

0

λ = 0.1

10
Epoch

20

λ = 0.3

10
Epoch

20

λ = 0.5

10
Epoch

20

λ = 0.7

e
t
a
R
r
o
r
r
E

l

a
V

e
t
a
R
r
o
r
r
E

l

a
V

e
t
a
R
r
o
r
r
E

l

a
V

e
t
a
R
r
o
r
r
E

l

a
V

0.4

0.2

0.4

0.2

0.4

0.2

0.4

0.2

0

0

0

10
Epoch

20

λ = 0.5

10
Epoch

20

λ = 0.7

10
Epoch

20

0

10
Epoch

20

LSTM

IS-LSTM

Figure 4: Results of Convergence Acceleration of IS for
LSTM. We compare the epoch error rate (calculated at
the end of each epoch) between LSTM and IS accelerated
LSTM with four different step sizes.

be an optimal step-size with which IS achieves its maxi-
mum performance in accelerating the convergence rate for
LSTM. Finding the optimal λ for a given LSTM network
is a complex problem and we leave this research to future
work.

Another noticeable difference is that the error rate of
IS-LSTM suffers less variance than LSTM. As can be
seen that in the last three rows, signiﬁcant increase of the
error rate happens only once for IS-LSTM while twice for
LSTM.

5.2 RNN-RBM for Polyphonic Music Mod-

λ = 0.1

Figure 5: Architecture of RNN-RBM

10
Epoch

20

λ = 0.3

W uh

h1

bh
1

W

h2

· · ·

vT

bh
2

bh
T

bv
1

v1

bv
2

v2

· · ·

bv
T

hT

W uv

W vu

W uu

u0

u1

u2

· · ·

uT

Selected
as

base

∇

thus choose RNN-based Restricted Boltzmann Machines
(RNN-RBM) polyphonic music modeling [4] as our sec-
ond evaluation case of IS for RNN. Particularly, in this
case RNN is used to condition a speciﬁed modeling struc-
ture RBM[11] which can be seen as another variant of
RNN. It is every interesting to observe how IS performs
in this kind of multilayer-RNN architectures.

Dataset: 147,737 musical sequences with dimen-
sions as 88.

Objective function: Cross-entropy.

Evaluation metrics: Object function value.

•

•

•

5.2.1 Restricted Boltzmann Machines

RBM models complicate distributions based on its energy
function (which is to be learned during training). Denote
W (W T as its transpose) represents the weights connect-
ing hidden units h and visible units v, and bv, bh are the
offsets of the visible and hidden layers respectively. The
update rule of the parameters are deﬁned as:

(29)

ht+1 ∼
vt+1 ∼

σ(W T vt + bh
t )
σ(W ht+1 + bv
t )

where σ is typically set as sigmoid function. The above
equations mean that ht+1 is activated with probability
σ(W T vt + bh

t ) and similar for vt+1.

eling

5.2.2 RNN-RBM

Besides of NLP related tasks, acoustic sequence process-
ing is another important application ﬁeld for RNNs. We

RNN extends the ability of RBM which makes it able
to model multi-modal conditional distributions. With

9

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
the combination of RNN, an extra recurrent procedure is
added as:

(30)

bv
t = bv + W uvut−1
bh
t = bh + W uhut−1
ut = tanh(bu + W uuut−1 + W vuvt)

where vt is the input vector, ut is the output hidden state
of the added RNN and bv
t , bh
t are intermediate results to
be calculated. And bv, bh, bu, W uv, W uh, W uu, W vu are
parameters to be learned. With the calculated bv
t and bh
t ,
v serves as the initial input of Equation 29, i.e., v0. The
unrolled architecture of RNN-RBM is shown in Figure 5.

5.2.3 Base-Gradient Selection

y
p
o
r
t
n
E

s
s
o
r
C

8

7

6

5

0

10

y
p
o
r
t
n
E

s
s
o
r
C

8

6

4

batch-size=50
λ = 0.005

batch-size=50
λ = 0.003

y
p
o
r
t
n
E

s
s
o
r
C

8

7

6

5

100
Epoch

200

0

50

100 150

Epoch

batch-size=100
λ = 0.003

batch-size=100
λ = 0.0015

10

y
p
o
r
t
n
E

s
s
o
r
C

8

6

4

From an architectural point of view, RNN-RBM is actu-
ally a two layer RNNs with the RBM (seen as a variant
of RNN) as the second layer where the output is gener-
ated. In considering the fact that the ﬁrst layer RNN is
for the calculation of auxiliary parameters bt and bv as
the parameter of the second RNN, we choose W of Equa-
tion 29 (the RBM recursion) which conditions both in-
put vt and hidden state ht as the base-gradient since we
consider it serves similar function with W x in base RNN
which is a proper proxy of
base. Accordingly, we have
pi = kWik2
PN

1, 2, ..., N

∇

,

.

j=1 kWj k2

i
∀

∈ {

}

5.2.4 Convergence Acceleration Results of IS-RNN-

RBM

We use two different batch-sizes to comply with the orig-
inal version of this case5. We evaluate two different step-
sizes for each batch-size, Figure 6 shows the curves of
cost per epoch6. As can be seen that IS accelerated RNN-
RBM achieves better convergence results than standard
RNN-RBM in both batch-sizes. In the ﬁrst conﬁguration,
i.e. batch-size = 50 and λ = 0.3, the objective cost of
standard RNN-RBM encounters heavy regression when
the cost is low while for IS accelerated RNN-RBM, the
objective cost continues to decrease which is a very sig-
niﬁcant improvement. Similar result can also be observed
when batch-szie = 100 and λ = 0.003.

5In this case, the batch-size actually indicates how many consecutive

musical slices are combined as one training sample.

6By summing the objective cost for each sample (without model up-

date) and then return its average at the end of each epoch.

0

500
Epoch

1000

0

200 400 600

Epoch

RNN-RBM

IS-RNN-RBM

Figure 6: Convergence Results. We compare the epoch
objective function between standard non-IS RNN-RBM
and IS accelerated RNN-RBM with two different batch-
sizes.

This clearly shows the effect of IS, it makes the stochas-
tic training procedure more robust since its searching di-
rection is statistically better (by choosing important data)
than Non-IS trainings. Such robustness does not only ben-
eﬁt the convergence rate, but also very helpful in avoiding
accuracy regression that frequently happens when step-
size is relative large (as shown in this case). In fact, this
actually means that IS accelerated RNN can use larger
step-sizes and less-likely to be trapped in local-minimum
while non-IS RNN is not able to. This surely leads to a
faster convergence rate and sometimes higher ﬁnal accu-
racy.

5.3 Remark

Although we have analyzed the expected attributes of
proper base-gradient in theoretical, its selection is still
somehow empirical.
In general, the matrix that condi-
tions the (embedded) input performs the best. Another
important experience is that the accuracy of the FIM al-
gorithm, i.e., ǫ in Algorithm 2 has signiﬁcant effect on the
performance of IS. By setting a higher accuracy, i.e., a

10

 
 
 
 
lower ǫ, the total iterations Ti needed by each separate
fast approximation training thread varies largely which
potentially incurs larger differences between the trained
models and consequently the target gradients. Empiri-
cally, we consider this will reﬂect the relative importance
of data sample for training more signiﬁcantly and beneﬁts
the performance of IS.

6 Conclusion

Due to the practical signiﬁcance and novelty, we are mo-
tivated to apply IS in RNN for convergence acceleration.
The calculation of the sampling distribution P w.r.t which
the IS is based on requires the training data to be well-
structured. However for RNNs the input data are often
randomly mapped before training which is the major bot-
tlenecking problem that prevents the effective application
of IS in RNNs. To break this obstacle, we propose an op-
timized IS procedure based on Fast-Importance-Mining
(FIM) algorithm which trains each input data xi,
∈
separately with its private model until cer-
1, 2, ..., N
{
tain convergence accuracy ǫ is met. IS then use the se-
lected base-gradient’s norm
k2 as the proxy value
of the sampling importance upon on which the sampling
distribution P can be constructed.

i
∀

k∇

base

}

We evaluate our optimized IS accelerated RNN on two

∇

popular applications. In both cases, we the select
base
accordingly and the results show that IS accelerated RNN-
based optimizations achieves better convergence results
(convergence rate or ﬁnal accuracy) than its non-IS coun-
terparts. We also notice that certain relationship exists
between convergence improvements and step-size, batch-
size, etc. In practical, FIM incurs small additional time
cost since the algorithm can be totally parallelized and
the training for single xi is fast. Related source code of
FIM and the evaluation applications are all accessible on
the author’s github repository.

Acknowledgements

The authors thank Jason Ye, Professor Guihai Chen and
Professor Xiaofeng Gao for their important helps.

References

[1] G. ALAIN,

A.

LAMB,

SANKAR,
A. COURVILLE, AND Y. BENGIO, Variance
reduction in sgd by distributed importance sam-
pling, arXiv preprint arXiv:1511.06481, (2015).

C.

[2] L. A. S. C. C. A. ALAIN, GUILLAUME AND
Y. BENGIO, Variance reduction in sgd by dis-
tributed importance sampling.,
in arXiv preprint
arXiv:1511.06481., 2015.

[3] J. BERGSTRA, O. BREULEUX, F. BASTIEN,
P. LAMBLIN, R. PASCANU, G. DESJARDINS,
J. TURIAN, D. WARDE-FARLEY, AND Y. BENGIO,
Theano: A cpu and gpu math compiler in python, in
Proc. 9th Python in Science Conf, 2010, pp. 1–7.

[4] N. BOULANGER-LEWANDOWSKI, Y. BENGIO,
AND P. VINCENT, Modeling temporal dependencies
in high-dimensional sequences: Application to poly-
phonic music generation and transcription, arXiv
preprint arXiv:1206.6392, (2012).

[5] D. CSIBA AND P. RICHTÁRIK,

sampling
arXiv:1602.02283, (2016).

for minibatches,

Importance
preprint

arXiv

[6] K.-I. FUNAHASHI AND Y. NAKAMURA, Approxi-
mation of dynamical systems by continuous time re-
current neural networks, Neural networks, 6 (1993),
pp. 801–806.

[7] F. A. GERS, J. SCHMIDHUBER, AND F. CUMMINS,
Learning to forget: Continual prediction with lstm,
(1999).

[8] S. GOPAL., Adaptive sampling for sgd by exploiting
side information., in Proceedings of the 33 rd Inter-
national Conference on Machine Learning., 2016.

[9] A. GRAVES ET AL., Supervised sequence labelling
with recurrent neural networks, vol. 385, Springer,
2012.

[10] E. HAZAN, T. KOREN, AND N. SREBRO, Beat-
ing sgd: Learning svms in sublinear time, in Ad-
vances in Neural Information Processing Systems,
2011, pp. 1233–1241.

11

[11] G. HINTON, A practical guide to training restricted
boltzmann machines, Momentum, 9 (2010), p. 926.

[12] S. HOCHREITER AND J. SCHMIDHUBER, Long
short-term memory, Neural computation, 9 (1997),
pp. 1735–1780.

[13] T. MIKOLOV, M. KARAFIÁT, L. BURGET, J. CER-
NOCK `Y, AND S. KHUDANPUR, Recurrent neural
network based language model.,
in Interspeech,
vol. 2, 2010, p. 3.

[14] E. MOULINES AND F. R. BACH, Non-asymptotic
analysis of stochastic approximation algorithms for
machine learning, in Advances in Neural Informa-
tion Processing Systems, Curran Associates, Inc.,
2011, pp. 451–459.

[15] D. NEEDELL, R. WARD, AND N. SREBRO,
Stochastic gradient descent, weighted sampling, and
the randomized kaczmarz algorithm, in Advances in
Neural Information Processing Systems, Curran As-
sociates, Inc., 2014, pp. 1017–1025.

[16] F. J. PINEDA, Generalization of back-propagation
to recurrent neural networks, Physical review let-
ters, 59 (1987), p. 2229.

[17] T.STROHMER AND R. VERSHYNIN., A randomized
kaczmarz algorithm with exponential convergence,
in The Journal of Fourier Analysis and Applica-
tions., vol. 2, 2009, pp. 262–278.

[18] J. WANG, Analysis and design of a recurrent neural
network for linear programming, IEEE Transactions
on Circuits and Systems I: Fundamental Theory and
Applications, 40 (1993), pp. 613–618.

[19] P. ZHAO AND T. ZHANG., Stochastic optimization
with importance sampling for regularized loss min-
imization., in Proceedings of the 32nd International
Conference on Machine Learning., 2015.

12

