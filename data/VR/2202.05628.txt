2
2
0
2

n
u
J

7
1

]

R
G
.
s
c
[

3
v
8
2
6
5
0
.
2
0
2
2
:
v
i
X
r
a

Artemis: Articulated Neural Pets with Appearance and Motion
Synthesis

HAIMIN LUO, ShanghaiTech University, China
TENG XU, ShanghaiTech University, China
YUHENG JIANG, ShanghaiTech University, China
CHENGLIN ZHOU, ShanghaiTech University, China
QIWEI QIU, ShanghaiTech University and Deemos Technology Co., Ltd., China
YINGLIANG ZHANG, DGene Digital Technology Co., Ltd., China
WEI YANG, Huazhong University of Science and Technology, China
LAN XU, ShanghaiTech University, China
JINGYI YUâˆ—, ShanghaiTech University, China

Fig. 1. Demonstration of our neural pet in a VR environment. We present an approach, we call â€œArtemisâ€, for generating photo-realistic and interactable
virtual pets. Traditional approaches animate animals with rigged mesh models and skeleton-based skinning techniques, but they can not handle furs well.
Instead, we represent animals as an animatable neural volume and render animal appearance and furs in real-time (see the right figure for rendering results).
Moreover, we use the motion synthesis approach, i.e., the Local Motion Phase, to generate skeletal motion for the animal according to the userâ€™s control
signals. We demonstrate this concept in the left figure, where the user gives a destination point, and the virtual lion follows the skeleton and motion and
moves to the destination.

âˆ—corresponding author

Authorsâ€™ addresses: Haimin Luo, ShanghaiTech University, China,
luohm@
shanghaitech.edu.cn; Teng Xu, ShanghaiTech University, China, xuteng@shanghaitech.
edu.cn; Yuheng Jiang, ShanghaiTech University, China, jiangyh2@shanghaitech.edu.cn;
Chenglin Zhou, ShanghaiTech University, China, zhouchl@shanghaitech.edu.cn; Qi-
wei Qiu, ShanghaiTech University and Deemos Technology Co., Ltd., China, qiuqw@
shanghaitech.edu.cn; Yingliang Zhang, DGene Digital Technology Co., Ltd., China,
yingliang.zhang@dgene.com; Wei Yang, Huazhong University of Science and Tech-
nology, China, weiyangcs@hust.edu.cn; Lan Xu, ShanghaiTech University, China,
xulan1@shanghaitech.edu.cn; Jingyi Yu, ShanghaiTech University, China, yujingyi@
shanghaitech.edu.cn.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed

We, humans, are entering into a virtual era and indeed want to bring animals
to the virtual world as well for companion. Yet, computer-generated (CGI)
furry animals are limited by tedious off-line rendering, let alone interactive
motion control. In this paper, we present ARTEMIS, a novel neural mod-
eling and rendering pipeline for generating ARTiculated neural pets with
appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion

for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
Â© 2022 Copyright held by the owner/author(s).
0730-0301/2022/7-ART164
https://doi.org/10.1145/3528223.3530086

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

 
 
 
 
 
 
164:2

â€¢ Luo, H. et al

control, real-time animation, and photo-realistic rendering of furry animals.
The core of our ARTEMIS is a neural-generated (NGI) animal engine, which
adopts an efficient octree-based representation for animal animation and
fur rendering. The animation then becomes equivalent to voxel-level de-
formation based on explicit skeletal warping. We further use a fast octree
indexing and efficient volumetric rendering scheme to generate appearance
and density features maps. Finally, we propose a novel shading network to
generate high-fidelity details of appearance and opacity under novel poses
from appearance and density feature maps. For the motion control module
in ARTEMIS, we combine state-of-the-art animal motion capture approach
with recent neural character control scheme. We introduce an effective opti-
mization scheme to reconstruct the skeletal motion of real animals captured
by a multi-view RGB and Vicon camera array. We feed all the captured
motion into a neural character control scheme to generate abstract control
signals with motion styles. We further integrate ARTEMIS into existing
engines that support VR headsets, providing an unprecedented immersive
experience where a user can intimately interact with a variety of virtual
animals with vivid movements and photo-realistic appearance. Extensive
experiments and showcases demonstrate the effectiveness of our ARTEMIS
system in achieving highly realistic rendering of NGI animals in real-time,
providing daily immersive and interactive experiences with digital animals
unseen before. We make available our ARTEMIS model and dynamic furry
animal dataset at https://haiminluo.github.io/publication/artemis/.

CCS Concepts: â€¢ Computing methodologies â†’ Image-based rendering;
Motion capture; Volumetric models.

Additional Key Words and Phrases: neural volumetric animal, novel view
syntheis, neural rendering, neural representation, dynamic scene modeling,
motion synthesis

ACM Reference Format:
Haimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei Qiu, Yingliang
Zhang, Wei Yang, Lan Xu, and Jingyi Yu. 2022. Artemis: Articulated Neural
Pets with Appearance and Motion Synthesis . ACM Trans. Graph. 41, 4,
Article 164 (July 2022), 19 pages. https://doi.org/10.1145/3528223.3530086

1

INTRODUCTION

Our love for animals is a great demonstration of humanity. Digitiza-
tion of fascinating animals such as Aslan the Lion in the Chronicles
of Nardia to Richard Parker the Tiger in Life of Pie brings close en-
counters to viewers, unimaginable in real life. Despite considerable
successes in feature films, accessing computer-generated imagery
(CGI) digital animals and subsequently animating and rendering
them at high realism have by far been a luxury of VFX studios
where the process of creating them requires tremendous artistsâ€™
efforts and high computational powers, including the use of render
farms. In fact, even with ample resources, photo-realistic rendering
of animated digital animals still remains off-line and hence is not
yet ready for primetime on Metaverse, where a user should be able
to guide the movement of the animal, interact with it, and make
close observations, all at high photo-realism in real-time.

The challenges are multi-fold, but the conflict demand between
real-time and photo-realistic rendering is at the core. First, animals
are covered with furs traditionally modeled with hundreds of thou-
sands of hair fiber/strands, using videos and photos as references.
The traditional modeling process is tedious and requires exquisite
artistic skills and excessive labor. The rendering process, on the other
hand, is equally time-consuming: off-line ray tracing is typically
adopted to produce photo-realism in translucency, light scattering,

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

volumetric shadows, etc, and far from real-time even with the most
advanced graphics hardware. In addition to rendering, animating
the model at an interactive speed and with high realism remains
challenging. For example, animations of animals in the latest remake
of the Lion King were crafted by hand, based on the reference clips
to match skeletal and muscle deformations. An automated tool is
in urgent need for facilitating motion controls and even transfers
(e.g., from dogs to wolves). Finally, for digital animals to thrive in
the virtual world, they should respond to usersâ€™ instructions, and
therefore, both the rendering and animation components need tight
integration with interaction.

In this paper, we address these critical challenges by presenting
ARTEMIS, a novel neural modeling and rendering framework for
generating ARTiculated neural pets with appEarance and Motion
synthesIS. In stark contrast with existing off-line animation and
rendering systems, ARTEMIS supports interactive motion control,
realistic animation, and high-quality rendering of furry animals, all
in real-time. Further, we extend ARTEMIS to OpenVR to support
consumer-level VR headsets, providing a surreal experience for
users to intimately interact with various virtual animals as if in the
real world (see Fig. 1).

On appearance modeling and rendering, ARTEMIS presents a
neural-generated imagery (NGI) production pipeline to replace the
traditional realistic but time-consuming off-line CGI process to
model animated animals. NGI is inspired by the Neural Radiance
Field (NeRF) [Mildenhall et al. 2020], and various forms of its ex-
tensions for producing photo-realistic rendering of real-world ob-
jects [Peng et al. 2021a; Su et al. 2021; Tretschk et al. 2021; Zhang
et al. 2021]. These approaches, however, cannot yet handle elastic
motions while maintaining visual realism. In our case, we train on
CGI animal assets (dense multi-view RGBA videos rendered using
Blender) under a set of pre-defined motion sequences, where each
animal model contains a skeleton with Linear Blend Skinning (LBS)
weights. Using Spherical Harmonics factorization, we embed the
animalâ€™s appearance in a high-dimensional latent space. Specifi-
cally, we follow the same strategy as PlenOctree [Yu et al. 2021]
and build a canonical voxel-based octree for the whole sequence
while maintaining the latent descriptors and the density field. Under
this design, we can apply LBS skeletal warping to locate features
under the canonical pose on live poses. To ensure image quality, we
further add a convolutional decoder to enhance spatial texture de-
tails of furs. The complete process can be trained as a differentiable
renderer where the result enables the real-time, photo-realistic, free-
viewpoint rendering of dynamic animals.

Besides rendering, ARTEMIS provides realistic motion synthesis
so that a user can control the animal in virtual space. We com-
bine state-of-the-art animal motion capture techniques with the
recent neural character control [Starke et al. 2020]. Since there
still lacks a comprehensive 3D mocap database, we collect several
new datasets using a hybrid RGB/Vicon mocap dome. Similar to
human pose priors [Pavlakos et al. 2019], we pre-train an animal
pose prior from the data via Variational AutoEncoder (VAE) and use
it as a regularizer for motion estimation. The resulting motion se-
quences are used to drive neural character animations [Holden et al.
2017; Starke et al. 2019, 2020, 2021; Zhang et al. 2018]. In particular,
we employ the local motion phase (LMP) technique [Starke et al.

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:3

Fig. 2. Overview of our ARTEMIS system for generating articulated neural pets with appearance and motion synthesis. ARTEMIS consists of two
core components. In the first module, given skeletal rig and skinning weights of CGI animal assets and corresponding multi-view rendered RGBA images
in representative poses, we build a dynamic octree-based neural representation to enable explicit skeletal animation and real-time rendering for dynamic
animals, which supports real-time interactive applications; In the second module, we build a hybrid animal motion capture system with multi-view RGB and
VICON cameras to reconstruct realistic 3D skeletal poses, which supports to training a neural motion synthesis network to enable a user to interactively
guide the movement of the neural animals. The ARTEMIS system is further integrated into existing consumer-level VR headset platforms for immersive VR
experience of neural-generated animals.

2020] to guide the movement of the neural animal under common
commands by real-world pet owners. Since both neural rendering
and motion controls of ARTEMIS achieve real-time performance,
we further integrate ARTEMIS into existing consumer-level VR
headset platforms. Specifically, we fine-tune our neural training
process for binocular rendering and develop a hybrid rendering
scheme so that the foreground NGI results can be fused with the
rasterization-produced background. Comprehensive experiments
show that ARTEMIS produces highly realistic virtual animals with
convincing motions and appearance. Coupled with a VR headset, it
provides a surreal experience where users can intimately interact
with a variety of digital animals in close encounters.
To summarize, our main contributions include:

â€¢ We propose a novel neural modeling and rendering system,
ARTEMIS, that supports natural motion controls and user
interactions with virtual animals on 2D screens or in VR set-
tings. In particular, we collect a new motion capture dataset
on animals of different scales and train a new skeleton detec-
tor.

â€¢ We present a differentiable neural representation tailored for
modeling dynamic animals with furry appearances. Our new
approach can effectively convert traditional CGI assets to NGI
assets to achieve real-time and high-quality rendering.

â€¢ We provide controllable motion synthesis schemes for various
NGI animals under the VR setting. A user can send commands
to or intimately interact with the virtual animals as if they
were real.

2 RELATED WORK
2.1 Fuzzy object modeling and rendering

Traditional methods focus on physical simulation-based and image-
based modeling to model fuzzy objects like human hair and ani-
mal fur. Physical simulation-based modeling methods directly rep-
resent hair as existing physical models. Hair rendering can date
back to texel object method [Kajiya and Kay 1989], which proposed
anisotropic lighting model object to be rendered as furry surfaces.
This model is still used in some modern CG rendering engines. Light
scattering model [Marschner et al. 2003] of hair is then proposed,
which modeling hair lighting with two specular layer effects, and is
improved by considering energy-conserving [dâ€™Eon et al. 2011]. A
more complex microcosmic hair model is proposed, which adds a
medulla kernel in hair and fur [Yan et al. 2015] and is accelerated by
directly performing path tracing [Chiang et al. 2015]. Mass spring
model [Selle et al. 2008] is proposed to simulate hair which repre-
sents straight hair better. Floating Tangents algorithm [Derouet-
Jourdan et al. 2013] approximates hair curves with helices model.
Grid hair model starts from NVIDIA HairWorks technology and is
improved to mesh model which provides direct control of overall
hair shape [Yuksel et al. 2009]. On the other hand, image-based
modeling methods try to reconstruct hair models directly from
multi-view or single view images. Multi-view images based hair
reconstruction relies on direction field and hair tracking growth
algorithm [Bao and Qi 2016; Luo et al. 2013; Paris et al. 2004, 2008;
Wei et al. 2005]. Single view image-based hair reconstruction, which
relies on direction field and needs hair guideline as input [Chai et al.
2015; Ding et al. 2016], the data-driven based method rely on hair
model database to perform hair information matching [Chai et al.
2016; Hu et al. 2015].

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Neural Motion Capture and SynthesisNeural Animal Modeling and RenderingMotion Synthesis Network164:4

â€¢ Luo, H. et al

Fig. 3. The algorithm pipeline of our Animatable Neural Animals. Given multi-view RGBA images of traditional modeled animals rendered in canonical
space, we first extract a sparse voxel grid and allocate a corresponding feature look-up table as a compact representation, together with an octree for quick
feature indexing. Then we pose the character to training poses using the rig of the traditional animal asset and conduct efficient volumetric rendering to
generate view-dependent appearance feature maps and coarse opacity maps. We next decode them into high-quality appearance and opacity images with the
convolutional neural shading network. We further adopt an adversarial training scheme for high-frequency details synthesis.

Besides human hair, other methods focus on reconstructing furry
objects. IBOH uses the multi-background matting method to con-
struct opacity hull, which represents furry objects as opacity point
clouds [Matusik et al. 2002]. Neural representation methods are also
proved to perform well on furry objects like hair and fur. NOPC
method implements a neural network algorithm to get similar opac-
ity point clouds in neural network representation [Wang et al. 2020],
which can efficiently generate high-quality alpha maps even with
low-quality reconstruction from small data. The latter work Con-
vNeRF changes point clouds representation to opacity radiance
field [Luo et al. 2021], which enables high quality, global-consistent,
and free-viewpoint opacity rendering for fuzzy objects.

Recall all furry objects modeling method, it is seen that tradi-
tional physical modeling and rendering method is able to generate
high-quality method, but cost much times computation than simple
mesh due to the complex ray reflection property of hair and fur,
which is difficult to perform real-time rendering. Image-based meth-
ods directly reconstruct hair model from images, which is more
efficient to generate static hair model but lose view consistency and
cost much on dynamic scene effects. Recent neural representation
methods perform well on fuzzy object modeling and rendering but
still cannot generate real-time dynamic hair and fur effects.

2.2 Virtual animals

Animal parametric models have gradually been investigated in re-
cent years, corresponding to general human parametric models. A
series of works are proposed to capture and reconstruct animals like
dogs [Biggs et al. 2020], horses [Zuffi et al. 2019], birds [Kanazawa
et al. 2018] and other medium animals [Biggs et al. 2018; Cashman
and Fitzgibbon 2012; Zuffi et al. 2017]. The seminal work [Cash-
man and Fitzgibbon 2012] of Cashman and Fitzgibbon learns a
low-dimensional 3D morphable animal model by estimating the
shape of dolphins from images. This approach was limited to spe-
cific classes(dolphins, pigeons) and suffered from an overly smooth

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

shape representation. [Kanazawa et al. 2016] learns a model to de-
form the template model to match hand clicked correspondences.
However, their model is also animal-specific. SMAL [Zuffi et al.
2017] extends SMPL [Loper et al. 2015] model to the animal domain;
they create a realistic 3D model of animals and fit this model to 2D
data, overcoming the lack of motion and shape capture for animal
subjects with prior knowledge. [Kanazawa et al. 2018] combine the
benefits of the classically used deformable mesh representations
with a learning-based prediction mechanism. [Kearney et al. 2020]
built the first open-source 3D motion capture dataset for dogs using
RGBD cameras and Vicon optical motion capture system. Compara-
bly, our method builds a hybrid capture system with marker-based
capture for docile and small pets(e.g., dogs and cats) and only RGB
footage for dangerous and large animals. Besides, we reconstruct
the realistic 3D skeletal poses for each kind of animal above.

2.3 Neural Modeling and Rendering

Various explicit representations have been incorporated into the
deep learning pipeline for geometry and appearance learning via
differentiable renderings, such as pointclouds [Aliev et al. 2020; Anqi
et al. 2021; Insafutdinov and Dosovitskiy 2018; Kolos et al. 2020;
Lin et al. 2018; Roveri et al. 2018; Wu et al. 2020; Yifan et al. 2019],
textured meshes [Chen et al. 2019; Habermann et al. 2021; Kato et al.
2018; Liu et al. 2020b, 2019; Shysheya et al. 2019] and volumes [Lom-
bardi et al. 2019; Mescheder et al. 2019; Peng et al. 2020; Sitzmann
et al. 2019]. However, they suffer from holes, topological changes,
or cubic memory footprint. Recent implicit representations employ
Multi-Layer Perceptrons (MLP) to learn a continuous implicit func-
tion that maps spacial locations to distance field [Chabra et al. 2020;
Jiang et al. 2020; Park et al. 2019], occupancy field [He et al. 2021;
Huang et al. 2020; Mescheder et al. 2019; Peng et al. 2020; Saito et al.
2019] or radiance field [Bi et al. 2020; Liu et al. 2020a; Martin-Brualla
et al. 2021; Mildenhall et al. 2020]. They can naturally handle com-
plicated scenes, but they usually suffer from high query time for

Volume RenderingReal?Fake?DiscriminatorGTNeural  Shading NetworkOpacityForegroundRGBAppearance FeaturemapCoarse Opacity Mapâ„’ğºğºğºğºğºğºâ„’ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ+â„’ğ‘ƒğ‘ƒWarpâ„’ğ‘‰ğ‘‰Canonical FrameSkeletal PosesLive FramesFeature LUTrendering. For real-time rendering, explicit primitives [Lombardi
et al. 2021] or spheres [Lassner and Zollhofer 2021] representations
are employed, together with acceleration data structures [Yu et al.
2021]. However, they require good initialization or high memory for
dynamic scenes. MonoPort [Li et al. 2020a,b] achieves impressive
real-time reconstruction and rendering with an RGBD camera by
combining PiFu [Saito et al. 2019] and octree for fast surface localiza-
tion, but it does not support animation and suffers from insufficient
rendering quality.

Recently NeRF [Mildenhall et al. 2020] has achieved impressive
progress by learning radiance fields from 2D images. The following
variants of NeRF aim to learn generalizable radiance fields [Chen
et al. 2021; Wang et al. 2021a], train with unposed cameras [Meng
et al. 2021; Wang et al. 2021b; Yen-Chen et al. 2021], model high-
frequency details [Luo et al. 2021; Sitzmann et al. 2020; Tancik et al.
2020] and opacity [Luo et al. 2021], handle relighting and shading [Bi
et al. 2020; Boss et al. 2021; Kuang et al. [n.d.]] or accelerate for real-
time applications [Garbin et al. 2021; Hedman et al. 2021; Neff et al.
2021; Reiser et al. 2021; Yu et al. 2021]. Most recent approaches
extend NeRF to dynamic scenes by learning a deformation field [Li
et al. 2021; Park et al. 2021a; Pumarola et al. 2021; Tretschk et al.
2021; Xian et al. 2021] or training a hypernet [Park et al. 2021b].
Such methods can only achieve playback or implicit interpolation.
With estimated skeleton motion and parametric model, Animat-
ableNeRF [Peng et al. 2021a] achieves pose control by inversely
learning blending weights fields; however, it struggles for large mo-
tion due to the limited capacity of MLPs. NeuralBody [Peng et al.
2021b] proposes structured latent codes for dynamic human perfor-
mance rendering and is then extended to be more pose-generalizable
by adopting pixel-aligned features [Kwon et al. 2021]. H-NeRF [Xu
et al. 2021] constrains a radiance field by a structured implicit hu-
man body model to robustly fuse information from sparse views
for better pose generalization. NeuralActor [Liu et al. 2021] fur-
ther introduces dynamic texture to neural radiance fields for better
pose-dependent details. These works, however, still suffer from high
query time consumption without acceleration. Another line of re-
search employs explicit volume [Lombardi et al. 2019], points [Wu
et al. 2020], mesh [Habermann et al. 2021; Liu et al. 2020b, 2019;
Shysheya et al. 2019], but suffers from low resolution representa-
tions. MVP [Lombardi et al. 2021] achieves real-time fine detail
rendering but still cannot realize pose extrapolation.

3 ANIMATABLE NEURAL ANIMALS

Animating digital animals have long relied on experienced artists. To
produce photo-realistic appearance and opacity of furry animals, it
is commonly a group effort to model tens of thousands of hair fibers
and simulate their movements with physical soundness. Finally, real-
time rendering is difficult to achieve even on high-end render farms.
For interactive applications such as virtual pets, it is essential to
simultaneously maintain photo-realism and real-time performance
on both motion and appearance.

Neural Opacity Radiance Fields. In contrast to physically modeling
translucency of furs, we adopt the neural rendering approach and
cast the problem as view synthesis on the radiance field. The seminal
work of the Neural Radiance Field conceptually provides a natural

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:5

Fig. 4. Dynamic Animal Rendering Details. Given a target skeletal pose,
we deform the canonical index octree to live frames, then perform ray
marching for efficient volume rendering to integrate the features along the
ray to generate feature maps for the further rendering process.

solution to fur rendering: NeRF represents a scene in terms of the
color and density on each point along a ray where the density
naturally reflects the opacity of the point. However, the alpha matte
produced from NeRF tends to be noisy and less continuous than
natural fur or hair. The recent ConvNeRF [Luo et al. 2021] addresses
this issue by processing the image in feature space than directly in
RGB colors. They formulate a neural opacity field as:

ğ¼ = Î¦(ğ¹ğ‘ ), ğ¹ğ‘ =

|S |
âˆ‘ï¸

ğ‘–

ğ‘‡ğ‘– (cid:0)1 âˆ’ exp(âˆ’ğœğ‘–ğ›¿ğ‘– )(cid:1) ğ‘“ğ‘–

(1)

where Î¦ is the opacity synthesis network, and ğ¹ğ‘ is the aggregated
image features, ğ‘‡ğ‘– = exp(âˆ’ (cid:205)ğ‘–
ğœ ğ‘— ğ›¿ ğ‘— ). S is the set of sampled
ğ‘—=1
points, and ğœğ‘– , ğ›¿ğ‘– are density and distance between sampled points,
respectively. The key here is the use of feature ğ‘“ğ‘– to represent the
opacity at each spatial point instead of directly using the density.
The original ConvNeRF can only handle static objects. The brute-
force approach is to conduct per-frame optimization for dynamic
objects, which is clearly infeasible on neural animals that generally
perform long motion sequences.

3.1 Animatable Neural Volumes

We first extend the neural opacity radiance field to dynamic animals.
Notice that our goal is not only to accelerate training but, more im-
portantly, to realize real-time rendering. Inspired by PlenOctree [Yu
et al. 2021] for static scenes, we set out to store the opacity fea-
tures in a volumetric octree structure for real-time rendering but
under animations. Specifically, we devise a feature indexing scheme
for quick feature fetching and rendering. We further introduce a
skeleton-based volume deformation scheme that employs skinning
weights derived from the original CGI model to bridge the canonical
frame with live frames for animation. In addition, we devise a neural

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

dCanonicalPoseTarget PoseğœV2V1V3V2V3V1FeatureFFLUTğˆğ’‡164:6

â€¢ Luo, H. et al

shading network to handle animated object modeling and adopt an
efficient adversarial training scheme for model optimization.

Octree Feature Indexing. Unlike the original NeRF or PlenOctree,
where the objectâ€™s geometry is unknown, we have the CGI animal
models as inputs. Therefore, we first convert a CGI animal char-
acter, such as a tiger or a lion, to the octree-based representation.
Again, the original CGI model contains very detailed furs, and direct
conversion onto discrete voxels can cause strong aliasing and signif-
icant errors in subsequent neural modeling. If we remove the furs
and use only the bare model, the voxel representation will deviate
significantly from the actual ones. Instead, we prefer "dilated" voxel
representations that encapsulate the model. We apply a simple trick
to resolve this issue: we use the rendered alpha matte from a dense
set of views as inputs and conduct conservative volume carving to
construct the octree: we initialize a uniform volume and carve it
using the masks dilated from the alpha mattes. Notice, though, that
we also need the rendered multi-view alpha matte later for training
the neural opacity field. The resulting octree contains a voxel array
P occupied in 3D space. Using this volumetric representation, we
aim to store a view-dependent feature ğ‘“ at each voxel. We, there-
fore, allocate a separate data array F called Features Look-up Table
(FLUT) to store features and density values as in the original PlenOc-
tree. Here FLUT is used to efficiently query features at an arbitrary
3D location to accelerate training and inference. For a given query
point in space at the volume rendering process, we can index into
FLUT in constant time and assign the corresponding feature and
density to the point.

Inspired by PlenOctree [Yu et al. 2021] that factorizes view-dep-
endent appearance with Spherical Harmonics (SH) basis, we model
our opacity feature ğ‘“ also as a set of SH coefficients, i.e., ğ‘“ = {ğ‘˜ğ‘–
â„=1,
where ğ‘˜ğ‘–
â„ âˆˆ Rğ¶ correspond to the coefficients for ğ¶ components,
and ğ» is the number of SH functions. Given a query ray direction
ğ‘‘ = (ğœƒ, ğœ™), the view dependent feature S âˆˆ Rğ¶ can be written as:

â„ }ğ»

S(ğ‘“ , ğ‘‘) =

ğ»
âˆ‘ï¸

ğ‘˜ğ‘–
â„ğ‘Œâ„ (ğœƒ, ğœ™)

(2)

â„=1
where ğ‘Œâ„ : S2 â†’ R is the SH bases. In our implementation, we set
the dimension of SH bases to 91.

Rigging and Deformation. To make the octree adapt to animated
animals, we directly "rig" the octree Vğ‘ in terms of its voxels from
the canonical pose to the target skeleton S, where ğ‘† is the skeleton
parameter S = {r, ğ‘…,ğ‘‡ }, r âˆˆ Rğ½ Ã—3 is the joint rotation angles,
ğ‘… âˆˆ R3 is global rotation and ğ‘‡ âˆˆ R3 is global translation.

To rig the octree under skeletal motion, the brute force way is to
rig all voxels in terms of their distance to the skeletons. Following
Linear Blending Skinning (LBS), we instead apply skinning weights
to voxels using the skinned mesh provided by the CGI models. Specif-
ically, given mesh vertices and corresponding skinning weights, we
generate per-voxel skinning weight by blending the weights of ğ‘š
closest vertices {ğ‘£ ğ‘— }ğ‘š
ğ‘—=1 instead of only one point [Huang et al. 2020]
on the mesh as:

ğ‘¤ (pğ‘– ) =

ğ‘š
âˆ‘ï¸

ğ‘—=1

ğ›¼ ğ‘—ğ‘¤ ğ‘— , ğ›¼ ğ‘— = ğ‘’ğ›¿ ğ‘— /Î£ğ‘š

ğ‘˜ ğ‘’ğ›¿ğ‘˜

(3)

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Fig. 5. Neural Shading Network. Our four blocks appearance branch
translates the feature map to the foreground texture image, while the two
blocks opacity branch takes the texture image to refine the coarse opacity
map.

where ğ‘¤ ğ‘— is the skinning weight of ğ‘£ ğ‘— and ğ›¿ ğ‘— = ğ‘‘ ğ‘— âˆ’ ğ‘šğ‘–ğ‘›ğ‘š
ğ‘˜
ğ‘‘ ğ‘— as the Euclidean distance between ğ‘£ ğ‘— and voxel position pğ‘– .

ğ‘‘ğ‘˜ with

With the voxel set Vğ‘ , their skinning weights W and a corre-
sponding skeleton Sğ‘ ready, we then conduct deformation from the
canonical pose Sğ‘ to the target pose Sğ‘¡ with the transformation
matrices ğ‘€ğ‘, ğ‘€ğ‘¡ âˆˆ Rğ½ Ã—4Ã—4 following LBS as:

ğ‘¡
ğ‘– =
p

ğ½
âˆ‘ï¸

ğ‘—=1

ğ‘¤ ğ‘— (vğ‘– )ğ‘€ğ‘¡

ğ‘— (ğ‘€ğ‘

ğ‘
ğ‘— )âˆ’1p
ğ‘–

(4)

with canonical voxel position pğ‘

ğ‘– and ğ½ joints.

Dynamic Volume Rendering. Once we deform the octree to the
target pose, we can render the neural animal via volume rendering
and subsequently enable end-to-end training. We employ a differen-
tiable volumetric integration scheme, as shown in Fig. 4. Given a
ğ‘¡
ray ğ‘Ÿğ‘¡
ğ‘ that
ğ‘ and ray direction d
corresponds to pixel ğ‘ viewing the volume under pose Sğ‘¡ , we can
compute the view-dependent feature of ğ‘ as:

ğ‘¡
ğ‘ ) with starting point oğ‘¡
ğ‘, d

ğ‘ = (oğ‘¡

ğ¹ ğ‘¡
ğ‘ =

ğ‘š
âˆ‘ï¸

ğ‘–

ğ‘
ğ›¼ğ‘– S(ğ‘“ğ‘–, d
ğ‘ )

ğ›¼ğ‘– = ğ‘‡ğ‘– (cid:0)1 âˆ’ exp(âˆ’ğœğ‘–ğ›¿ğ‘– )(cid:1), ğ‘‡ğ‘– = exp(âˆ’

(5)

(6)

ğ‘–
âˆ‘ï¸

ğ‘—=1

ğœ ğ‘— ğ›¿ ğ‘— )

where ğ›¿ğ‘– is the distance of the ray traveling through a voxel vğ‘– ,
and ğ‘“ğ‘– is the corresponding SHs coefficients fetched using the index
stored in vğ‘– , ğ‘š is the number of hit voxels. To further preserve
consistency over view-dependent effects, we map the ray directions
ğ‘¡
to the canonical space with a rotation matrix ğ‘…ğ‘¡
ğ‘– )âˆ’1d
ğ‘ .
Finally, we generate a view-dependent feature map F by applying
Eqn.3.1 to each pixel and a coarse opacity map A by accumulating ğ›¼ğ‘–
along the rays. Since our neural animal model is dense and compact
in space, uniform sampling used in NeuralVolumes [Lombardi et al.
2019] and NeRF [Mildenhall et al. 2020] can lead to rendering empty

ğ‘
ğ‘ = (ğ‘…ğ‘¡
ğ‘– by d

Course Opacity MapOpacity BranchAppearance BranchDownsampling BlockUpsampling BlockInput BlockOutput BlockTextureOpacityAppearance Feature Mapvoxels. To tackle this problem, we employ a highly optimized data-
parallel octree construction technique [Karras 2012] that only uses
around 10ms to build an octree for millions of voxels. With this
speed, we manage to rebuild the octree under pose variations and
perform a ray-voxel intersection test for dynamic scenes. We further
apply an early-stop strategy based on the accumulated alpha, i.e.,
we stop the integration once alpha is greater than 1 âˆ’ ğœ†ğ‘¡â„ (we use
0.01 in our implementation).

Neural Shading. We have by far rasterized the volume to a neu-
ral appearance feature map F and opacity A at the target pose
and viewpoint. The final step is to convert the rasterized volume
to a color image with a corresponding opacity map resembling a
classical shader. To preserve high frequency details of the fur, it is
essential to consider spatial contents in the final rendered image.
Notice, though, that neither NeRF nor PlenOctree considers spatial
correlation as all pixels are rendered independently. We instead
adopt an additional U-Net architecture Î¦ following ConvNeRF [Luo
et al. 2021] to perform image rendering. Note that our ray-marching-
based sampling strategy enables full-image rendering, in contrast to
the patch-based strategy of ConvNeRF. Precisely, our neural shading
network ğº consists of two encoder-decoder branches for RGB and
alpha channels, respectively. The RGB branch converts F to texture
images Iğ‘“ with rich fur details. The alpha branch refines the coarse
opacity map A and Iğ‘“ to form a super-resolved opacity map A. The
process enforces multi-view consistency by explicitly utilizing the
implicit geometry information encoded in A.

Training. Recall that all features ğ‘“ are stored in FLUT. For accel-
eration, we randomly initialize FLUT and then jointly optimize the
dense feature array F and the parameters of ğº from the multi-view
animal motion videos. Our network aims to recover the appear-
ances and opacity values of furry animals under free viewpoint. We
therefore adopt the pixel-level ğ¿1 loss as:

Lğ‘Ÿğ‘”ğ‘ğ‘ =

1
ğ‘ğ‘ƒ

ğ‘ğ‘ƒ
âˆ‘ï¸

ğ‘–

(âˆ¥ ^Iğ‘– âˆ’ Iğ‘– âˆ¥1 + âˆ¥ ^ğ›¼ğ‘– âˆ’ ğ›¼ğ‘– âˆ¥1)

(7)

where I, ğ›¼ are ground truth color and alpha rendered from ground
truth fuzzy CGI animal models, and ^Iğ‘–, ^ğ›¼ğ‘– are synthesized color and
alpha values from our network. In particular, ^Iğ‘– is the blended result
using the predicted alpha matte. ğ‘ğ‘ƒ is the number of sampled pixels.
To recover fine texture and geometry details, we employ the

VGG19 perceptual loss [Johnson et al. 2016]:

Lğ‘ƒ =

1
ğ‘ğ¼

ğ‘ğ¼
âˆ‘ï¸

âˆ‘ï¸

ğ‘–

ğ‘™ âˆˆ {3,8}

(âˆ¥ğœ™ğ‘™ (^Iğ‘– ) âˆ’ğœ™ğ‘™ (Iğ‘– )âˆ¥1 + âˆ¥ğœ™ğ‘™ ( ^Ağ‘– ) âˆ’ğœ™ğ‘™ (Ağ‘– )âˆ¥1) (8)

where ğœ™ğ‘™ denotes the ğ‘™ğ‘¡â„ layer feature map of VGG19 backbone, and
ğ‘ğ¼ is the number of sampled images.

To encourage cross view consistency as well as maintain temporal
coherency, we impose geometry priors encoded in the ground truth
alpha maps on the coarse alpha map A:

Lğ‘‰ =

1
ğ‘ğ¼

ğ‘ğ‘‰
âˆ‘ï¸

ğ‘–

âˆ¥Ağ‘– âˆ’ Ağ‘– âˆ¥1

(9)

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:7

Fig. 6. Neural Rendering Engine. We develop a neural rendering engine
based on our animatable neural volumetric representation. It can render
our neural animals into standard frame buffers (e.g., texture, alpha, depth)
used in the traditional 3D rendering pipeline.

Under motion, voxels may be warped to same locations that oc-
clude previous frames. This leads to voxel collisions and feature
overwrites of a grid and breaks the temporal consistency. To ad-
dress this issue, we propose a voxel regularization term (VRT) that
enforces features falling onto the same grid after warping should
have identical values as:

Lğ‘£ğ‘Ÿğ‘¡ = ğœ†ğ‘£ğ‘Ÿğ‘¡

1
ğ‘ğ‘£

ğ‘ğ‘£
âˆ‘ï¸

ğ‘–

ğ‘¡
ğ‘– , T
âˆ¥Q(p

ğ‘
ğ‘¡ ) âˆ’ f
ğ‘– âˆ¥1

(10)

Q is a query operator that outputs the corresponding feature of
voxel at position pğ‘¡

ğ‘– in octree Tğ‘¡ . ğ‘ğ‘£ is the voxel number.

To further improve the visual quality of high-frequency appear-
ance of furs, we employ the PatchGAN [Isola et al. 2017] discrimi-
nator ğ· to perform adversarial training with a GAN loss as:

Lğºğ´ğ‘ =

ğ‘ğ¼
âˆ‘ï¸

ğ‘–

(âˆ¥ğ· (^Iğ‘– )âˆ¥2 + âˆ¥ğ· (Iğ‘– ) âˆ’ 1âˆ¥2)

(11)

Hybrid Rendering. The processes of warping, octree construc-
tion, and volume rendering form a differentiable neural rendering
pipeline and can be paralleled using C++ and CUDA C for efficient,
end-to-end training. Once trained, the resulting octree structure
supports real-time free-view rendering under animation. The so-
lution can be further integrated with OpenGL and Unity engines,
where a standard 3D rendering pipeline can conveniently render
environments that host virtual animals. It is essential to address
shading and occlusions in order to correctly fuse the environment
with the neural rendering results to provide a realistic viewing
experience. We, therefore, extend the volume rendering process
described in Sec. 3.1 to render to different frame buffers. Firstly,
the original outputs F and A correspond to neural textures for
neural shading, and the final rendered images are stored in texture
buffers. Neural rendered animals can also produce a depth buffer
using a ray marching process: we cast a ray from each pixel from

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

OpacityFeatureNeural PetsEnvironmentsTextureDepthNormalRay MarchingOctreeNeuralShadingHybridShadingVertexShadingFragment ShadingDepthNormal164:8

â€¢ Luo, H. et al

Fig. 7. Setup of our animal motion capture system. The system consists of
12 Vicon cameras and 22 Z-CAM cinema cameras surrounding the animal.

the image plane to our trained volumetric model, trace along the
ray and locate the first voxel with a density larger than a thresh-
old, and directly assign the distance as the depth. In contrast, we
observe that directly applying such ray marching schemes on NeRF
produces large errors near the boundary of the object, particularly
problematic for furry objects. The rendered neural frame buffers
can be combined with the ones produced by the standard rendering
pipeline for resolving occlusions and alpha blending with the back-
ground. For example, the translucency of fur blends accurately with
the floor or the background as the animal walks or jumps using our
scheme.

4 NEURAL ANIMAL MOTION SYNTHESIS

For ARTEMIS to provide convincing motion controls, e.g., to guide
an animal to walk or jump from one location to another, it is essential
to synthesize realistic intermediate motions as the animal moves.
For human motions, data driven approaches now serve as the gold
standard. However, a sufficiently large animal mocap dataset is
largely missing. We hence first set out to acquire a new animal
mocap dataset along with a companion deep animal pose estimation
that we intend to share with research community. Next, we tailor a
neural character animator [Starke et al. 2020] for animals that maps
the captured skeletal motions to abstract control commands and
motion styles.

4.1 Animal Motion Capture

For data acquisition, we have constructed two types of animal mo-
tion capture systems, first composed of an array of RGB cameras
for animals of large scales and the second combines RGB cameras
and Vicon cameras for docile and small pets, as shown in Fig. 8.

MoCap Processes. We observe that quadrupeds, despite having
similar skeletal structures across species, exhibit drastically different
shapes and scales. It is simply impossible to capture a mocap dataset
suitable for all types of quadrupeds. We hence set out to learn
motion priors from docile and small pets and transfer the priors to
larger animals such as tigers and wolves. For the latter, we further
enhance the prediction accuracy using a multi-view RGB dome in
partnership with the zoo and circus.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Fig. 7 shows our capture system for dogs and cats where we use
12 Vicon Vantage V16 cameras evenly distributed surrounding the
animal subject, each capturing moving IR reflective markers at 120
Hz. We add additional 22 Z-CAM cinema cameras interleaved with
Vicon and capturing at a 1920 Ã— 1080 resolution at 30 fps. Cross-
calibrations and synchronization are conducted in prior of actual
motion capture. For small animals, we place the hybrid capture
system closer to the ground so that they can clearly acquire limb
movements while resolving occlusions between legs. We hire a
professional trainer to guide these small animals to perform different
movements including jumping and walking. Vicon produces highly
accurate motion estimations that are used as priors for RGB camera
based estimation.

For large animals, it is infeasible to place markers on large animals
such as horses, tigers, elephants and etc. We partner with several
zoos and circuses to deploy the RGB dome system where we use 22
Ëœ60 cameras and adjust their heights and viewing directions, depend-
ing on the size of the performance area. It is worth mentioning that
several circuses further provided us surveillance footages of their
animals where we manually select the usable ones and label their
poses.

2D Key-points and Silhouettes Estimation. To automatically ex-
tract skeletons on multi-view RGB images for animal motion in-
ference, we first estimate the 2D key-points and silhouettes in im-
ages [Biggs et al. 2020; Zuffi et al. 2019]. For key-points, we combine
DeepLabCut [Mathis and Warren 2018] and the SMAL [Zuffi et al.
2017] model for processing imagery data towards quadrupeds, aug-
mented with light manual annotations. Specifically, we annotate
joints defined in the SMAL model on the first 10% frames and then
allow DeepLabCut to track and estimate 2D poses for remaining
frames. For silhouette extractions, we directly use the off-the-shelf
DeepLab2 [Weber et al. 2021] with their pre-trained models.

Animal Pose Estimator. We adopt the parametric SMAL animal
pose model. To briefly reiterate, SMAL is represented as a function
ğ‘€ (ğ›½, ğœƒ, ğ›¾), where ğ›½ is the shape parameter, ğœƒ the pose, and ğ›¾ the
translation. Î (ğ‘¥, ğ¶ ğ‘— ) denote the projection of a 3D point ğ‘¥ on the ğ‘—-
th camera whereas Î (ğ‘€, ğ¶ğ‘– ) represents the projection of the SMAL
model to camera ğ¶ğ‘– . Our goal is to recover SMAL parameters ğœƒ, ğœ™, ğ›¾
from the observed 2D joints and silhouette. Assume all RGB and
Vicon cameras in our system are calibrated. We extend the key point
reprojection error ğ¸ğ‘˜ğ‘ , silhouette error ğ¸ğ‘  as presented in [Zuffi
et al. 2017] to multiview inputs. In our setup, we assume known
animal species and therefore append shape constraints ğ¸ğ›½ from
[Zuffi et al. 2017] to enforce the distribution of ğ›½ to match the pre-
trained species type. For our multi-view setup, we further add a 3D
key-point constraint ğ¸3ğ‘‘ and a mocap constraint ğ¸ğ‘š as:

ğ¸3ğ‘‘ (Î˜; ğ‘‰ ) =

(cid:13)
(cid:13)ğ‘‰ğ‘˜ âˆ’ Tr(ğ‘£ğ‘–

ğ‘˜ )(cid:13)
(cid:13)2

âˆ‘ï¸

ğ‘˜

(12)

where ğ‘‰ğ‘˜ denotes the ğ‘˜th SMAL joint in 3D space and ğ‘£ğ‘–
ğ‘˜ the ob-
servation of ğ‘‰ğ‘˜ in camera ğ‘–. Tr(Â·) corresponds to the triangulation
operator. We hence have:

ğ¸ğ‘š (Î˜; ğ‘€) =

âˆ¥ğ‘€ğ‘˜ âˆ’

âˆ‘ï¸

ğ‘˜

1
|Î©|

âˆ‘ï¸

ğ‘– âˆˆÎ©ğ‘˜

ğ‘˜ (Tr(ğ‘£ğ‘–
T ğ‘–

ğ‘˜ )âˆ¥2

(13)

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:9

Fig. 8. We estimate the animal skeleton, SMAL parameters more specif-
ically, from the multi-view video data and Vicon MoCap data. We adopt
an optimization framework to regress SMAL parameter from multiview
observations of 2D keypoints and silhouettes and Vicon Observations.

where ğ‘€ğ‘˜ is position of the ğ‘˜th mocap marker. Î©ğ‘˜ corresponds to
the set of joints used for recovering ğ‘€ğ‘˜ . T ğ‘–
ğ‘˜ is the transformation
between the ğ‘˜th marker and vertex ğ‘–. Notice not all vertices are
usable for recovering ğ‘€ğ‘˜ because the animal motion is non-rigid.
We thus find Î©ğ‘˜ by identifying the vertices that have correspond to
constant T ğ‘–

ğ‘˜ across the motion sequence.

For human pose estimation, the SMPL-X human model [Pavlakos
et al. 2019] introduces a prior using Variational AutoEncoder [Kingma
and Welling 2014] to penalize on impossible poses. We adopt a
similar idea: we per-train an animal motion prior using the same
framework with all recovered skeletal poses of quadrupeds in our
dataset and then use ğ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ to penalize on the deviations from the
prior. We thus can formulate the animal pose estimator in terms of
an optimization problem as:

ğ›½, ğœƒ, ğ›¾ â† arg min(ğ¸ğ‘˜ğ‘ + ğ¸ğ‘  + ğ¸3ğ‘‘ + ğ¸ğ‘š + ğ¸ğ›½ + ğ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ )

(14)
On RGB only captured images, we remove the marker loss term ğ¸ğ‘š.
With recovered ğ›½, ğœƒ, ğ›¾ for all frames, we can construct an animal
motion sequence {ğœƒ ğ‘— } and {ğ›¾ ğ‘— }.

4.2 Motion Synthesis

Our task is to generate virtual pets that exhibit realistic motions in
response to a userâ€™s command. Physics-based controls are widely
used in industry to generate vivid periodical motion patterns. How-
ever, those physical rules such as momentum conservation and
reaction force can be confusing to users as they provide less direct
guidance. Hence, we leverage the recent advances in data-driven
methods and human movement animation, i.e., Local Motion Phases
(LMP) [Starke et al. 2020], for our animal motion synthesis to learn
movements from asynchronous behaviors of different body parts.
Different from [Starke et al. 2019] that requires tedious manual
annotation, LMP features can be automatically extracted and subse-
quently trained on unstructured motion data collected on animals.

Controlled Motion Synthesis with Local Motion Phases. To enable
interactions between the user and the virtual animal, it is critical
to synthesize animal motions according to user commands, e.g., for

Fig. 9. Neural Animal Controller Pipeline. Leveraging ideas of Local
Motion Phases, the neural animal controller is composed of a gating network
and a motion prediction network. The gating network inputs local phases
and calculates the expert blending coefficients. The coefficients are used to
generate the motion prediction network. Then, the user-given control signals
and motion information from previous frames are sent into the motion
prediction network, which uses them to predict the motion information of
the next frame.

accomplishing specific tasks. We adopt the LMP framework that
consists of a gating network and a motion prediction network, as
shown in Fig. 9. The gating network takes the motion states (joint
locations, rotations, and velocities) of the current frame and the
past frames as inputs and computes expert activation, which is then
dynamically blended with motion states and sent to the motion
prediction network. Taking the expert weights and control signals
from users, the motion prediction network would be able to calculate
the motion state for future frames. The process can be expressed as:

Mğ‘– = Î¦(Mğ‘–âˆ’1, ğ‘)
(15)
where Mğ‘– is the motion state at frame ğ‘–, ğ‘ is the user control signal.
In our implementation, we define the set of control signals as {â€˜Idleâ€™,
â€˜Moveâ€™, â€˜Jumpâ€™, â€˜Sitâ€™, â€˜Restâ€™}. Under these control signals, we extract
the LMP feature accordingly, in terms of the contact state between
the animalâ€™s feet or hips with the ground. The contact label is also
extracted automatically by calculating the difference in positions
and velocities between end-effectors and the environment collider.
Furthermore, we process our motion data to calculate the action
labels automatically.

For example, we compute Idle and Move states by calculating the
root velocities and mapping them into values from 0 to 1. We detect
Jump, Sit and Rest labels by jointly checking the position, velocity,
and contact information and mapping them to values between 0
and 1. Specifically, Jump labels have no contact and y-axis velocity,
whereas the Sit and Rest Labels have no velocity and all end-effectors
contact with the environment but differ in the position of the head
of the animal.

Motion transfer to articulated model. Notice that the generated
motion state from LMP is bound to the skeleton captured in our
Mocap systems. Yet the base models used for our neural pets are
created by artists, and their skeletons do not match with the captured
animals. We need to transfer the motions generated by LMP to
animal models created by artists while keeping the model shape.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

OutputOptimizerInputLocal PhaseFrame iFrame i+1MotionPrediction NetworkExpert PoolGating NetworkUser Commands164:10

â€¢ Luo, H. et al

Therefore, for each type of virtual animal, we manually apply offsets
constraining the rotation and translation components in the rest
pose. We then apply forward-kinematics and inverse-kinematics to
calculate the target motion state and use transformation limits to
fine-tune the impossible states. With the transformed motion data,
we can transfer motion states from one to another and drive the
neural animals to move freely.

5 NEURAL ANIMALS IN IMMERSIVE ENVIRONMENTS

In previous sections, we have described how to train our animat-
able neural volumetric representation to synthesize animals under
arbitrary poses and views, along with a hybrid rendering pipeline.
Our neural motion controls and synthesis, enabled by our animal
mocap data, provide convenient interfaces to guide the animalâ€™s
movements. In this section, we assemble all these modules coher-
ently under the VR setting as the ultimate ARTEMIS system, where
a user can interact with the animal.

Interactions. Our motion synthesis module guides virtual animal
movements by explicitly pointing to the target location and pro-
viding action types. We further explore high-level control patterns,
emulating a commonly used set of commands by pet owners:

â€¢ Companion: As the user can move freely in virtual space, so

will the virtual animal. It will follow the user.

â€¢ Go to: The user points to a 3D location in virtual space, and
the animal reaches the target destination automatically. In
a complicated environment where obstacles are presented,
we use the A-Star algorithm to find the path for the animal
to follow. The user can also control the speed of the move-
ment, i.e., the animal can either walk to or run/jump to the
destination.

â€¢ Go around a Circle: The user specifies a location, and the
animal will reach the location and continue going around in
circle. The user can even point themselves as the destination
and ended being surrounded by single or multiple animals.
â€¢ Size and Speed Adjustment: Since our neural animal represen-
tation, as a continuous implicit function, can support infinite
resolution, the user can adjust the animalâ€™s size by simply
adjusting its octree accordingly. The user can also adjust the
movement speed, either slowing it down or speeding it up. In
particular, slow motion provides a fancy visual effect where
the user can observe the animal at a close distance as if time
freezes.

â€¢ Free Mode: when no command is given, the animal can take
any reasonable movements, such as exploring the virtual
world themselves. In our implementation, we randomly set
destinations within a time interval where the animal reaches
the destination one after another.

Implementations Details. The entire control flow for producing
neural animals under the VR setting is as follows: We first obtain
the pose of the VR headsets along with the controller status from
OpenVR. Next, we then use LMP control signals and the current
state to generate the estimated motion parameters. Based on the
parameters, we rig the canonical model using LBS and then construct
an octree for each frame. Finally, our neural rendering pipeline

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

traces into the octree to generate the feature maps and subsequently
conduct rendering where the background environments, obstacles
in 3D environments, etc, are rendered using a standard graphics
pipeline and fused with the neural rendering results.

Note that for VR applications, it is critical to generate two con-
sistent views for presenting to the left and right eyes of a user,
respectively, in the VR headsets. Inconsistent views between eyes
may cause discomforts, such as nausea and dizziness. Thus when
we deploy the trained neural animals to our system, we adopt the
stereo loss proposed in LookinGood [Martin-Brualla et al. 2018], to
finetune the trained model for better view consistency between left
and right eyes in a self-supervised learning manner.

We follow the high cohesion and low coupling strategy to distrib-
ute time consumption as even as possible where the communication
between core parts is lightweight via shared CUDA memory. The
average computation time breakdown of individual parts are: 10ms
for handle input (e.g., update the left and right camera poses, han-
dle the controller action, controller environment), 29ms for motion
synthesis (motion generation and motion transfer), 10ms for octree
construction, 25ms for stereo neural rendering and environment
synthesis, 10ms for output assembly and submit. By tailoring the
assembly line of various components, we manage to produce the
rendering at around 30 fps. The supplementary video shows live
recordings of several examples.

6 RESULTS

In this section, we evaluate our ARTEMIS under various challeng-
ing scenarios. We first report our dynamic furry animal datasets,
which are used for training our NGI animals, and our training and
experimental details. Then, we provide the comparison of our neural
rendering scheme with current state-of-the-art (SOTA) methods for
both dynamic appearance rendering and static opacity generation,
which demonstrate that our method better preserves high-frequency
details. We also conduct a detailed runtime performance analysis
of our rendering approach and provide extra evaluation for our
motion capture scheme. We further provide the VR applications
of our ARTEMIS using consumer-level VR headsets where a user
can intimately interact with various virtual animals. Finally, the
limitation and discussions regarding our approach are provided in
the last subsection.

Dynamic Furry Animal Dataset. To evaluate our ARTEMIS (neu-
ral pets) system, especially for our NGI animals, we seek help from
traditional CGI animal animation and rendering pipelines to gen-
erate a comprehensive training dataset. Specifically, our Dynamic
Furry Animal (DFA) dataset contains nine high-quality CGI animals
with fiber/strands based furs and skeletal rigs modeled through
tedious artistsâ€™ efforts, including panda, lion, cat, etc. We utilize the
commercial rendering engine (e.g., MAYA) to render all these CGI
animal characters into high-quality multi-view 1080 Ã— 1080 RGBA
videos under various representative skeletal motions. Specifically,
we adopted 36 camera views which are uniformly arranged around
a circle towards the captured animal, and the number of represen-
tative poses ranges from 700 to 1000 for each animal. Note that on
average it takes about 10 minutes to render a single frame of our

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:11

Fig. 10. Neural-generated animals in our ARTEMIS system. We show our synthesized results of different neural volumetric animals in representative
poses.

high-quality DFA dataset using commercial render farms. For train-
ing an NGI animal from the corresponding CGI asset, we uniformly
select 30 views to conduct per-animal training and leave six views
as test data for evaluation. We train our NGI animals at 960 Ã— 540
image resolution using 4 Nvidia TITAN RTX GPUs, and it takes
about two days for the training process. Fig. 10 demonstrates the
representative rendering results of our NGI animals under various
novel skeletal poses, where the details of appearance, fur, and opac-
ity are faithfully reconstructed with real-time performance. Besides,
our high-quality DFA dataset will be made publicly available to
the community to stimulate future research about realistic animal
modeling.

6.1 Comparison to Dynamic Neural Rendering

Here we compare our neural rendering pipeline in ARTEMIS with
recent SOTA methods for dynamic scene rendering. For thorough
comparison, we compare against both the radiance field based
methods, including NeuralBody [Peng et al. 2021b] and Animat-
ableNeRF [Peng et al. 2021a], as well as the volume-based approach
NeuralVolumes [Lombardi et al. 2019]. Note that our NGI animal
requires an additional pre-defined skeletal rig with skinning weights
from a corresponding CGI digital asset. Thus, for fair comparisons,
we train the baseline models using the same experiment settings as
our approach and adopt the ground truth mesh vertices and skin-
ning weights of our CGI animal models to train NeuralBody and
AnimatableNeRF.

Fig. 11 provides the qualitative comparison results of our approach
against the above baselines. Note that only fur with color much
different from the background can be recognized as fur, like lion

manes and cat forehead. Non-surprisingly, the three baseline meth-
ods suffer from artifacts of blurry fur and loss of appearance details.
Specifically, NeuralVolumes suffers from severe noises and blurring
effects due to its limited modeling ability, which is most obviously
on the cat forehead. NeuralBody can only generate low-frequency
fur details as blurry effects, while AnimatableNeRF behaves slightly
better but still cannot faithfully recover the furry details. In stark
contrast, our approach generates much more clear details favorably,
especially for those furry regions which are common in animals.
For quantitative evaluation, we adopt the peak signal-to-noise ratio
(PSNR), structural similarity index (SSIM), and Learned Perceptual
Image Patch Similarity (LPIPS) as metrics to evaluate our rendering
accuracy. As shown in Tab. 1, our approach significantly outper-
forms the other baselines for all the above metrics, demonstrating
the superiority of our approach in preserving rendering details.

6.2 Comparison to Static RGBA Rendering

Here we evaluate our approach for generating free-view opacity
maps, which is important for fur modeling. To the best of our knowl-
edge, there are no previous works that can generate opacity maps for
dynamic scenes. Thus, we compare our approach against the recent
SOTA methods on the opacity rendering task for static scenes. For
thorough comparison, we select three baselines, including the ex-
plicit point-cloud based method called Neural Opacity Point Clouds
(NOPC) [Wang et al. 2020], as well as the radiance field based meth-
ods NeRF [Mildenhall et al. 2020] and ConvNeRF [Luo et al. 2021].
Fig. 12 shows several RGB and alpha results of our method and
other static methods. NOPC suffers from aliasing and ghosting on
the boundary regions due to the interpolation of point-cloud based

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

164:12

â€¢ Luo, H. et al

Fig. 11. Qualitative comparison for dynamic appearance rendering against NeuralVolumes, NeuralBody and AnimatableNeRF. Note that our approach
achieves more photo-realistic rendering with finer details for appearance and fur.

features. NeRF suffers from severe blur artifacts because of the
limited representation ability of its MLP network, especially for high-
frequency details, while ConvNeRF improves the alpha and RGB
details but still causes grid-like artifacts due to patch-based training
strategy on limited sparse training views. In contrast, our approach
achieves the best performance where we manage to compensate for
missing views at a specific frame using views from other frames.

Differently, our approach generates more realistic opacity render-
ing and even supports dynamic scenes in real-time. The correspond-
ing quantitative result is provide in Tab. 2. For the evaluation of the
alpha map, we further adopt Sum of Absolute Distance (SAD) as

metrics besides PSNR and SSIM. Our approach outperforms all the
baselines for alpha-related metrics and maintains comparable per-
formance for RGB texture rendering against ConvNeRF. All these
evaluations illustrate the effectiveness of our approach for high-
quality fur and opacity detail rendering.

6.3 Ablation Study

We conduct extensive ablation studies on the â€œBearâ€ data. Let w/o
VRT and w/o GAN denote our models trained without voxel regu-
larization term (VRT) and GAN loss. Fig. 13 shows GAN loss helps
to preserve more high-frequency details and synthesized sharper

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Ground TruthOursNeural BodyNeural VolumesAnimatable NeRFArtemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:13

Fig. 12. Qualitative comparison for free-view appearance and opacity rendering of static scene with NOPC, NeRF and ConvNeRF. Our approach
achieves modeling high-frequency geometry details and generates almost the same RGBA images with the ground truth.

Fig. 13. Visualization of the ablation study on w/o the NS, GAN loss and VRT loss modules (corresponding to the Tab. 3).

alpha map, and VRT leads to slightly smoothed texture while main-
taining temporal consistency. We further train our model to directly
render RGBA images through volumetric without neural shading
network (NS) (w/o NS). It suffers from noises and artifacts around
the legs and abdomen due to geometry misalignment and limited
volume resolution. The qualitative result in Tab. 3 well supports the
contribution of each module.

pose and rendering, with other baselines. Our method achieves
real-time novel pose synthesis and rendering. For static scene, we
compare our free-view rendering runtime. Our method is much
faster (in order of 10 times) than other methods, especially achieves
three to four orders of magnitude rendering speed up to traditional
CGI animal character, which further supports our novel ARTEMIS
system.

6.4 Runtime Performance Evaluation

We run our method on an Nvidia TITAN RTX GPU. Tab. 4 compares
the running time of our method for one frame with others. For
dynamic scenes, we compare our full runtime, i.e., warping to target

Runtime Analysis. We further analyze the runtime of each part
and the performance-speed trade-off on a single Nvidia GeForce
RTX3090 GPU. As shown in Tab. 5, for a general case, where the res-
olution of our sparse volume is 512, animating (warp + build octree)

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Ground TruthOursConvNeRFNeRFNOPCGround TruthOursW/o VRTW/o GANW/o NS 164:14

â€¢ Luo, H. et al

Table 1. Quantitative comparisons of synthesized appearance images on
different dynamic animals. Compared with NeuralVolumes, NeuralBody,
and AnimableNeRF, our method achieves the best performance in all metrics.

Table 3. Quantitative evaluation of the ablation studies on w/o the NS, GAN
loss and VRT loss modules. Our full model achieves the best performance.

Method

Panda

Cat

Dog

Lion

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

Neural
Body

Neural
Volumes

Animatable
NeRF

30.38
0.970
0.110

30.77
0.972
0.067

32.37
0.978
0.075

30.11
0.956
0.111

30.11
0.965
0.116

28.14
0.951
0.087

26.80
0.945
0.129

29.59
0.947
0.123

26.51
0.957
0.112

31.37
0.973
0.061

31.19
0.975
0.074

27.87
0.944
0.123

Ours

33.63
0.985
0.031

37.54
0.989
0.012

38.95
0.989
0.022

33.09
0.966
0.035

Table 2. Quantitative comparisons of appearance and alpha on a single
representative frame of different animals. Our approach achieves the best
performance in almost all alpha-related metrics and comparable perfor-
mance for RGB texture against ConvNeRF.

RGB

Bear

Duck

Fox

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

â†‘ PSNR
â†‘ SSIM
â†“ LPIPS

NOPC NeRF ConvNeRF Ours

18.43
0.886
0.140

25.45
0.967
0.075

17.42
0.914
0.106

28.12
0.954
0.113

30.35
0.978
0.091

27.53
0.966
0.099

32.34
0.953
0.063

34.31
0.985
0.052

33.42
0.973
0.047

30.95
0.967
0.038

37.14
0.986
0.026

30.94
0.976
0.029

Alpha

NOPC NeRF ConvNeRF Ours

Bear

Duck

Fox

â†‘ PSNR
â†‘ SSIM
â†“ SAD

â†‘ PSNR
â†‘ SSIM
â†“ SAD

â†‘ PSNR
â†‘ SSIM
â†“ SAD

17.89
0.918
144.2

19.77
0.849
110.6

15.68
0.903
192.4

31.65
0.986
199.2

30.09
0.923
36.17

23.81
0.968
52.32

36.37
0.992
11.80

33.02
0.990
12.76

34.90
0.993
11.30

40.13
0.995
8.072

36.81
0.994
8.558

36.43
0.995
9.555

costs around 10ms, volume rendering costs around 5ms, and the
neural shading network costs around 13ms with half floating-point
precision. It costs 27.43ms in total. To further accelerate, we design

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Models

ğ‘…ğºğµ

PSNR â†‘

SSIM â†‘

LPIPS â†“

PSNR â†‘

(a) w/o NS
(b) w/o GAN
(c) w/o VRT
(d) ours

29.47
34.29
33.81
34.43

0.939
0.964
0.961
0.965

0.152
0.051
0.044
0.045

29.27
35.84
35.74
36.18

Alpha
SSIM â†‘

0.981
0.991
0.992
0.992

SAD â†“

24.68
12.47
12.23
11.92

Table 4. Runtime comparisons different Methods. Our method is signifi-
cantly faster than existing methods and enables real-time applications.

Dynamic

CGI

Neural
Body

Neural
Volumes

Animatable
NeRF

runtime (ms)
fps

âˆ¼ 5 Ã— 105
âˆ’

2353
0.425

181.7
5.504

18142
0.055

Ours

34.29
29.16

Static

CGI

NOPC

NeRF

ConvNeRF

Ours

runtime (ms)
fps

âˆ¼ 5 Ã— 105
âˆ’

51.23
19.52

18329
0.055

2599
0.385

20.38
49.07

Table 5. Runtime (in milliseconds) analysis of components of our model
and performance-speed trade-off.

Model Normal
PSNR â†‘
33.01
SSIM â†‘
0.991
LPIPS â†“
0.010
warp â†“
1.950
build otree â†“
7.990
volume render â†“
4.521
neural render â†“
12.97
total â†“
27.43

Light
32.80
0.990
0.011
1.975
8.321
4.520
10.03
24.84

256 + Light
32.47
0.988
0.012
1.915
6.285
2.478
10.01
22.69

a light shading network by removing the opacity branch and adding
an output channel to the appearance branch to predict the opacity
map, denoted as Light. The rendering time reduces to 24.84ms with
a slight performance downgrade. Finally, by combining the light net-
work with 256 volume resolution, denoted as 256 + Light, it takes
22.69ms with a slight performance drop. We adopt this scheme to
provide a better experience in our ARTEMIS system.

6.5

Interactive NGI Animals in VR.

As shown in Fig. 14, we exhibit our rendering results in VR applica-
tions, including different level interactions and different perspec-
tives. The first line, â€™viewingâ€™ shows the vivid virtual panda we
observed from the third and first view(VR headset), respectively. We
can clearly see good fur effects even in VR headsets. The second
line, â€™Instructionsâ€™ shows the low-level control animation results.
We can explicitly drive our neural pets using control signals like
â€™Jumpâ€™, â€™Moveâ€™ and â€™Sitâ€™. â€™Commandsâ€™ illustrates our high-level con-
trol pattern â€™Go toâ€™. The user points to a 3D location in virtual space,
and the wolf reaches the target destination automatically. We can

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:15

Fig. 14. Example interactive modes of our ARTEMIS. From top to bottom are different levels of interactions: viewing - we can observe the animal and see
the fur clearly; instructions - drive the animals with explicit signals, such as â€˜jumpâ€™ or â€˜sitâ€™; commands - high-level control patterns, such as user points at a 3D
location and the animal moves to there; companion - the animal follows the user; â€˜explorationâ€™ - animals show random movements.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Viewing Neural Panda with motion in VR Headsets High-Level Command-based Control in VR Headsets Combining with MarkerlessHuman MoCapLow-Level Control with Neural PetsImplicit Control with Neural Pets, Companion Mode Implicit Control with Neural Pets, Exploring ModeViewingInstructionsCommandsCompanionExploring164:16

â€¢ Luo, H. et al

Fig. 15. A concept demo of animal and user interaction in a VR environment. We develop a VR demo to exhibit how users can interact with a virtual
pet in a VR environment. We obtain the coordinate of the user in VR according to the controller, the wolf pet then plays with the user, e.g., jumping around
the user.

also call back the wolf by waving. Note that the speed of the move-
ment is also controlled by the user. In â€™Companionâ€™, our virtual pet
will follow and accompany the user like a real pet. Finally, â€™Explor-
ingâ€™ shows our free mode, when no command is given, the animal
can make any reasonable movements, exploring the virtual world
themselves.

We show more interactive results in Fig. 15.

6.6 Limitations and Discussions

We have demonstrated the compelling capability of interactive
motion control, real-time animation, and photo-realistic render-
ing of neural-generated (NGI) furry animals even with modern
commercial-level VR headsets. However, as a first trial to bring
photo-realistic NGI animals into our daily immersive applications
with us humans, our ARTEMIS system still owns limitations as
follows.

First, our NGI animal still heavily relies on a pre-defined skeletal
rig and skinning weights of the corresponding CGI animal character.
Despite the considerable improvement in terms of rendering time
speed up from CGI to NGI animals, the problem of fully automatic
neural animal generation from captured data without the artistâ€™s
effort remains unsolved. In future work, we plan to combine the
non-rigid tracking technique [Xu et al. 2020] to alleviate our reliance
on skeletal parameters. Moreover, within the paradigm of neural
rendering, our approach suffers from appearance artifacts for those
unobserved animal body regions during training and under those
challenging rendering views that are too different compared to the
captured camera views. It is promising to combine the appearance
prior of certain kinds of animals to alleviate such neural rendering
shortcomings. Our current rendering results still suffer from flick-
ering artifacts when the rendering view or the captured animal is
moving, even using our voxel regularization. We suspect that itâ€™s

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

partially caused by the voxel collision of our dynamic Octree re-
construction. Besides, our technique cannot yet handle self-contact
between distant body parts. In future work, we plan to explore a
more elegant Octree indexing scheme and employ more temporal
regularization to handle the flickering. Moreover, our current NGI
animal can only generate results within the baked lighting envi-
ronment while rendering the CGI asset and thus cannot quickly
adopt the lighting in a new environment compared to traditional
rendering engines. It is interesting to enable more explicit lighting
control on top of the current NGI animal design.

From ARTEMIS system side, our current interactions with NGI
animals are still limited by the heavy reliance on human motion cap-
ture and pre-defined rule-based strategies. Fundamentally, current
ARTEMIS design is based on only several relatively basic interac-
tive modes. Despite the unprecedented interaction experience of
ARTEMIS, a more advanced and knowledge-based strategy for the
interaction between NGI animals and us humans are in urgent need.
Moreover, more engineering effort is needed to provide a more
immersive experience for current VR applications, especially for
supporting the rendering of multiple animals.

Discussion. ARTEMIS has the potential to create the neural ani-
mal directly from captured real animal data if some obstacles are
conquered: it requires high-quality multi-view videos of furry ani-
mals with rich fur details while obtaining such videos of real animals
remains difficult. Animals occupy a much smaller portion within the
images than human performers, and therefore their captured videos
are generally of a low resolution. Further, balancing the exposure
setting (aperture vs. shutter vs. gain) to capture fast motion and
shape images simultaneously can be very tricky. In contrast, skele-
ton extraction is generally robust if the camera setting is dense, as
shown in the paper. Recent excellent work BANmo [Yang et al. 2021]
also provides promising insights for animals from casual videos.

On the other hand, compared to rasterization-based schemes for
fast rendering hair and fur (e.g., EEVEE), our work, instead, aims to
show that neural rendering provides a promising alternative that
can tackle shape deformations similar to traditional meshes but
implicitly. In addition, such neural representations can potentially
handle real animations without manual model building: by building
a neural representation from a multi-view video sequence of a real
animal, one can potentially conduct photo-realistic rendering of the
animal under unseen poses without explicitly modeling the under-
lying geometry, let alone fur. Besides, furry objects are challenging
to model using meshes: it requires extensive labor by artists and
the use of ultra-dense meshes where implicit representation such
as NeRF and its extensions can relieve artists from such labor.

7 CONCLUSION

We have presented a neural modeling and rendering pipeline called
ARTEMIS for generating articulated neural pets with appearance
and motion synthesis. Our ARTEMIS system has interactive motion
control, realistic animation, and high-quality rendering of furry
animals, all in real-time. At the core of our ARTEMIS, our neural-
generated (NGI) animals renew the rendering process of traditional
CGI animal characters, which can generate real-time photo-realistic
rendering results with rich details of appearance, fur, and opacity
with notably three to four orders of magnitude speed-up. Our novel
dynamic neural representation with a tailored neural rendering
scheme enables highly efficient and effective dynamic modeling for
our NGI animal. Our robust motion capture scheme for real animals,
together with the recent neural character control scheme, provides
the controllable ability to interact with our NGI animals with more
realistic movements. Moreover, our hybrid rendering engine enables
the integration of ARTEMIS into existing consumer-level VR head-
set platforms so as to provide a surreal and immersive experience
for users to intimately interact with virtual animals as if in the real
world. Extensive experimental results and VR showcases demon-
strate the effectiveness of our approach for neural animal modeling
and rendering, supporting immersive interactions unseen before.
We believe that our approach renews the way humans perceive and
interact with virtual animals, with more immersive, realistic, and
responsive interaction experiences, with many potential applica-
tions for animal digitization and protection or fancy human-animal
interactions in VR/AR, gaming, or entertainment.

ACKNOWLEDGMENTS

The authors would like to thank Junyu Zhou and Ya Gao from
DGene Digital Technology Co., Ltd. for processing the CGI ani-
mals models and motion capture data. Besides, we thank Zhenxiao
Yu and Heyang Li from ShanghaiTech University for producing a
supplementary video and figures.

This work was supported by NSFC programs (61976138, 61977047),
the National Key Research and Development Program (2018YFB2100
500), STCSM (2015F0203-000-06) and SHMEC (2019-01-07-00-01-
E00003).

REFERENCES

Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempit-
sky. 2020. Neural point-based graphics. In European Conference on Computer Vision.

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:17

Springer, 696â€“712.

Pang Anqi, Chen Xin, Luo Haimin, Wu Minye, Yu Jingyi, and Xu Lan. 2021. Few-shot
Neural Human Performance Rendering from Sparse RGBD Videos. In Proceedings of
the 30th International Joint Conference on Artificial Intelligence, IJCAI-21.

Yongtang Bao and Yue Qi. 2016. Realistic Hair Modeling from a Hybrid Orientation Field.
Vis. Comput. 32, 6â€“8 (jun 2016), 729â€“738. https://doi.org/10.1007/s00371-016-1240-1
Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, MiloÅ¡ HaÅ¡an,
Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. 2020. Neural
reflectance fields for appearance acquisition. arXiv preprint arXiv:2008.03824 (2020).
Benjamin Biggs, Oliver Boyne, James Charles, Andrew Fitzgibbon, and Roberto Cipolla.
2020. Who left the dogs out? 3d animal reconstruction with expectation maximiza-
tion in the loop. In European Conference on Computer Vision. Springer, 195â€“211.
Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, and Roberto Cipolla. 2018.
Creatures great and smal: Recovering the shape and motion of animals from video.
In Asian Conference on Computer Vision. Springer, 3â€“19.

Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, and Hendrik
Lensch. 2021. Nerd: Neural reflectance decomposition from image collections. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 12684â€“
12694.

Thomas J Cashman and Andrew W Fitzgibbon. 2012. What shape are dolphins? building
3d morphable models from 2d images. IEEE transactions on pattern analysis and
machine intelligence 35, 1 (2012), 232â€“244.

Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove,
and Richard Newcombe. 2020. Deep local shapes: Learning local sdf priors for
detailed 3d reconstruction. In European Conference on Computer Vision. Springer,
608â€“625.

Menglei Chai, Linjie Luo, Kalyan Sunkavalli, Nathan Carr, Sunil Hadap, and Kun Zhou.
2015. High-Quality Hair Modeling from a Single Portrait Photo. ACM Trans. Graph.
34, 6, Article 204 (oct 2015), 10 pages. https://doi.org/10.1145/2816795.2818112
Menglei Chai, Tianjia Shao, Hongzhi Wu, Yanlin Weng, and Kun Zhou. 2016. AutoHair:
Fully Automatic Hair Modeling from a Single Image. ACM Trans. Graph. 35, 4,
Article 116 (jul 2016), 12 pages. https://doi.org/10.1145/2897824.2925961

Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and
Hao Su. 2021. MVSNeRF: Fast Generalizable Radiance Field Reconstruction from
Multi-View Stereo. arXiv:2103.15595 [cs.CV]

Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson,
and Sanja Fidler. 2019. Learning to predict 3d objects with an interpolation-based
differentiable renderer. In Advances in Neural Information Processing Systems. 9609â€“
9619.

Matt Jen-Yuan Chiang, Benedikt Bitterli, Chuck Tappan, and Brent Burley. 2015. A
Practical and Controllable Hair and Fur Model for Production Path Tracing. In ACM
SIGGRAPH 2015 Talks (Los Angeles, California) (SIGGRAPH â€™15). Association for
Computing Machinery, New York, NY, USA, Article 23, 1 pages. https://doi.org/10.
1145/2775280.2792559

Eugene dâ€™Eon, Guillaume Francois, Martin Hill, Joe Letteri, and Jean-Marie Aubry.
2011. An Energy-Conserving Hair Reflectance Model. In Proceedings of the Twenty-
Second Eurographics Conference on Rendering (Prague, Czech Republic) (EGSR â€™11).
Eurographics Association, Goslar, DEU, 1181â€“1187. https://doi.org/10.1111/j.1467-
8659.2011.01976.x

Alexandre Derouet-Jourdan, Florence Bertails-Descoubes, and JoÃ«lle Thollot. 2013.
Floating Tangents for Approximating Spatial Curves with G1 Piecewise Helices.
Comput. Aided Geom. Des. 30, 5 (jun 2013), 490â€“520. https://doi.org/10.1016/j.cagd.
2013.02.007

Zhipeng Ding, Yongtang Bao, and Yue Qi. 2016. Single-View Hair Modeling Based on
Orientation and Helix Fitting. In 2016 International Conference on Virtual Reality
and Visualization (ICVRV). 286â€“291. https://doi.org/10.1109/ICVRV.2016.54

Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin.
2021. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 14346â€“14355.

Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and
Christian Theobalt. 2021. Real-Time Deep Dynamic Characters. ACM Trans. Graph.
40, 4, Article 94 (jul 2021), 16 pages. https://doi.org/10.1145/3450626.3459749
Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. 2021. ARCH++:
Animation-ready clothed human reconstruction revisited. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 11046â€“11056.

Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul De-
bevec. 2021. Baking neural radiance fields for real-time view synthesis. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 5875â€“5884.

Daniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks
for character control. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1â€“13.
Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. 2015. Single-View Hair Modeling
Using a Hairstyle Database. ACM Trans. Graph. 34, 4, Article 125 (jul 2015), 9 pages.
https://doi.org/10.1145/2766931

Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. 2020. Arch: Ani-
matable reconstruction of clothed humans. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 3093â€“3102.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

164:18

â€¢ Luo, H. et al

Eldar Insafutdinov and Alexey Dosovitskiy. 2018. Unsupervised Learning of Shape and
Pose with Differentiable Point Clouds. In Advances in Neural Information Processing
Systems (NeurIPS).

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image
translation with conditional adversarial networks. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition. 1125â€“1134.

Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias NieÃŸner, Thomas
Funkhouser, et al. 2020. Local implicit grid representations for 3d scenes. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
6001â€“6010.

Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time
style transfer and super-resolution. In European conference on computer vision.
Springer, 694â€“711.

J. T. Kajiya and T. L. Kay. 1989. Rendering Fur with Three Dimensional Textures.
SIGGRAPH Comput. Graph. 23, 3 (jul 1989), 271â€“280. https://doi.org/10.1145/74334.
74361

Angjoo Kanazawa, Shahar Kovalsky, Ronen Basri, and David Jacobs. 2016. Learning
3d deformation of animals from 2d images. In Computer Graphics Forum, Vol. 35.
Wiley Online Library, 365â€“374.

Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. 2018. Learning
category-specific mesh reconstruction from image collections. In Proceedings of the
European Conference on Computer Vision (ECCV). 371â€“386.

Tero Karras. 2012. Maximizing parallelism in the construction of BVHs, octrees, and
k-d trees. In Proceedings of the Fourth ACM SIGGRAPH/Eurographics conference on
High-Performance Graphics. 33â€“37.

Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Neural 3d mesh renderer.
In Proceedings of the IEEE conference on computer vision and pattern recognition.
3907â€“3916.

Sinead Kearney, Wenbin Li, Martin Parsons, Kwang In Kim, and Darren Cosker. 2020.
RGBD-Dog: Predicting Canine Pose from RGBD Sensors. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR).
Diederik P. Kingma and Max Welling. 2014.

Auto-Encoding Variational
Bayes.
In 2nd International Conference on Learning Representations,
ICLR
2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.
arXiv:http://arxiv.org/abs/1312.6114v10 [stat.ML]

Maria Kolos, Artem Sevastopolsky, and Victor Lempitsky. 2020. TRANSPR: Trans-
parency Ray-Accumulating Neural 3D Scene Point Renderer. In 2020 International
Conference on 3D Vision (3DV). IEEE, 1167â€“1175.

Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and
Sergey Tulyakov. [n.d.]. NeROIC: Neural Object Capture and Rendering from Online
Image Collections. ([n. d.]).

Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. 2021. Neural human
performer: Learning generalizable radiance fields for human performance rendering.
Advances in Neural Information Processing Systems 34 (2021).

Christoph Lassner and Michael Zollhofer. 2021. Pulsar: Efficient Sphere-based Neural
Rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 1440â€“1449.

Ruilong Li, Kyle Olszewski, Yuliang Xiu, Shunsuke Saito, Zeng Huang, and Hao Li.
2020a. Volumetric Human Teleportation. In ACM SIGGRAPH 2020 Real-Time Live!
(Virtual Event, USA) (SIGGRAPH â€™20). Association for Computing Machinery, New
York, NY, USA, Article 9, 1 pages. https://doi.org/10.1145/3407662.3407756

Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, and Hao Li.
2020b. Monocular real-time volumetric performance capture. In European Conference
on Computer Vision. Springer, 49â€“67.

Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. 2021. Neural scene flow
fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 6498â€“6508.

Chen-Hsuan Lin, Chen Kong, and Simon Lucey. 2018. Learning efficient point cloud
generation for dense 3d object reconstruction. In proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 32.

Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020a.
Neural sparse voxel fields. Advances in Neural Information Processing Systems 33
(2020), 15651â€“15663.

Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and
Christian Theobalt. 2021. Neural Actor: Neural Free-view Synthesis of Human
Actors with Pose Control. ACM Trans. Graph.(ACM SIGGRAPH Asia) (2021).

Lingjie Liu, Weipeng Xu, Marc Habermann, Michael ZollhÃ¶fer, Florian Bernard,
Hyeongwoo Kim, Wenping Wang, and Christian Theobalt. 2020b. Neural Human
Video Rendering by Learning Dynamic Textures and Rendering-to-Video Transla-
tion. IEEE Transactions on Visualization and Computer Graphics PP (05 2020), 1â€“1.
https://doi.org/10.1109/TVCG.2020.2996594

Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc
Habermann, Wenping Wang, and Christian Theobalt. 2019. Neural rendering and
reenactment of human actor videos. ACM Transactions on Graphics (TOG) 38, 5
(2019), 1â€“14.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann,
and Yaser Sheikh. 2019. Neural Volumes: Learning Dynamic Renderable Volumes
from Images. ACM Trans. Graph. 38, 4, Article 65 (jul 2019), 14 pages. https:
//doi.org/10.1145/3306346.3323020

Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh,
and Jason Saragih. 2021. Mixture of Volumetric Primitives for Efficient Neural
Rendering. ACM Trans. Graph. 40, 4, Article 59 (jul 2021), 13 pages. https://doi.org/
10.1145/3450626.3459863

Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J
Black. 2015. SMPL: A skinned multi-person linear model. ACM transactions on
graphics (TOG) 34, 6 (2015), 1â€“16.

H. Luo, A. Chen, Q. Zhang, B. Pang, M. Wu, L. Xu, and J. Yu. 2021. Convolutional Neural
Opacity Radiance Fields. In 2021 IEEE International Conference on Computational
Photography (ICCP). IEEE Computer Society, Los Alamitos, CA, USA, 1â€“12. https:
//doi.org/10.1109/ICCP51581.2021.9466273

Linjie Luo, Hao Li, and Szymon Rusinkiewicz. 2013. Structure-Aware Hair Capture.

ACM Transactions on Graphics (Proc. SIGGRAPH) 32, 4 (July 2013).

Stephen R. Marschner, Henrik Wann Jensen, Mike Cammarano, Steve Worley, and Pat
Hanrahan. 2003. Light Scattering from Human Hair Fibers. ACM Trans. Graph. 22,
3 (jul 2003), 780â€“791. https://doi.org/10.1145/882262.882345

Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan
Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter
Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan B Goldman, Cem Keskin, Steve
Seitz, Shahram Izadi, and Sean Fanello. 2018. <i>LookinGood</i>: Enhancing
Performance Capture with Real-Time Neural Re-Rendering. ACM Trans. Graph. 37,
6, Article 255 (dec 2018), 14 pages. https://doi.org/10.1145/3272127.3275099

Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey
Dosovitskiy, and Daniel Duckworth. 2021. Nerf in the wild: Neural radiance fields
for unconstrained photo collections. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 7210â€“7219.

Alexander Mathis and Richard A. Warren. 2018. On the inference speed and video-
compression robustness of DeepLabCut. bioRxiv (2018). https://doi.org/10.1101/
457242 arXiv:https://www.biorxiv.org/content/early/2018/10/30/457242.full.pdf
Wojciech Matusik, Hanspeter Pfister, Addy Ngan, Paul Beardsley, Remo Ziegler, and
Leonard McMillan. 2002. Image-Based 3D Photography Using Opacity Hulls. ACM
Trans. Graph. 21, 3 (jul 2002), 427â€“437. https://doi.org/10.1145/566654.566599
Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and
Jingyi Yu. 2021. Gnerf: Gan-based neural radiance field without posed camera. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 6351â€“6361.
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas
Geiger. 2019. Occupancy networks: Learning 3d reconstruction in function space. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
4460â€“4470.

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-
mamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields
for view synthesis. In European conference on computer vision. Springer, 405â€“421.
Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller,
Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger.
2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance
Fields using Depth Oracle Networks. Computer Graphics Forum 40, 4 (2021).
https://doi.org/10.1111/cgf.14340

Sylvain Paris, Hector M. BriceÃ±o, and FranÃ§ois X. Sillion. 2004. Capture of Hair
Geometry from Multiple Images. ACM Trans. Graph. 23, 3 (aug 2004), 712â€“719.
https://doi.org/10.1145/1015706.1015784

Sylvain Paris, Will Chang, Oleg I. Kozhushnyan, Wojciech Jarosz, Wojciech Matusik,
Matthias Zwicker, and FrÃ©do Durand. 2008. Hair Photobooth: Geometric and Pho-
tometric Acquisition of Real Hairstyles. ACM Trans. Graph. 27, 3 (aug 2008), 1â€“9.
https://doi.org/10.1145/1360612.1360629

Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-
grove. 2019. Deepsdf: Learning continuous signed distance functions for shape
representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 165â€“174.

Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,
Steven M Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable neural
radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer
Vision. 5865â€“5874.

Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz,
Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021b. HyperNeRF:
A Higher-Dimensional Representation for Topologically Varying Neural Radiance
Fields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021).

Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A.
Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D
Hands, Face, and Body from a Single Image. In Proceedings IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR).

Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei
Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling

Dynamic Human Bodies. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 14314â€“14323.

Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.
2020. Convolutional occupancy networks. In Computer Visionâ€“ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part III 16. Springer,
523â€“540.

Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and
Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured
latent codes for novel view synthesis of dynamic humans. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9054â€“9063.

Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021.
D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10318â€“10327.

Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. Kilonerf: Speeding
up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 14335â€“14345.

Riccardo Roveri, Lukas Rahmann, Cengiz Oztireli, and Markus Gross. 2018. A network
architecture for point cloud classification via automatic depth images generation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
4176â€“4184.

Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa,
and Hao Li. 2019. Pifu: Pixel-aligned implicit function for high-resolution clothed
human digitization. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 2304â€“2314.

Andrew Selle, Michael Lentine, and Ronald Fedkiw. 2008. A Mass Spring Model for
Hair Simulation. ACM Trans. Graph. 27, 3 (aug 2008), 1â€“11. https://doi.org/10.1145/
1360612.1360663

Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov,
Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov,
et al. 2019. Textured neural avatars. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2387â€“2397.

Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon
Wetzstein. 2020. Implicit neural representations with periodic activation functions.
Advances in Neural Information Processing Systems 33 (2020).

Vincent Sitzmann, Justus Thies, Felix Heide, Matthias NieÃŸner, Gordon Wetzstein, and
Michael Zollhofer. 2019. Deepvoxels: Learning persistent 3d feature embeddings. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
2437â€“2446.

Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. 2019. Neural state machine

for character-scene interactions. ACM Trans. Graph. 38, 6 (2019), 209â€“1.

Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. 2020. Local motion phases
for learning multi-contact character movements. ACM Transactions on Graphics
(TOG) 39, 4 (2020), 54â€“1.

Sebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Komura. 2021. Neural animation
layering for synthesizing martial arts movements. ACM Transactions on Graphics
(TOG) 40, 4 (2021), 1â€“16.

Shih-Yang Su, Frank Yu, Michael ZollhÃ¶fer, and Helge Rhodin. 2021. A-nerf: Articulated
neural radiance fields for learning human shape, appearance, and pose. Advances in
Neural Information Processing Systems 34 (2021).

Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin
Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng.
2020. Fourier Features Let Networks Learn High Frequency Functions in Low
Dimensional Domains. NeurIPS (2020).

Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael ZollhÃ¶fer, Christoph Lass-
ner, and Christian Theobalt. 2021. Non-rigid neural radiance fields: Reconstruction
and novel view synthesis of a dynamic scene from monocular video. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 12959â€“12970.

Cen Wang, Minye Wu, Ziyu Wang, Liao Wang, Hao Sheng, and Jingyi Yu. 2020. Neural
Opacity Point Cloud. IEEE Transactions on Pattern Analysis and Machine Intelligence
42, 7 (2020), 1570â€“1581. https://doi.org/10.1109/TPAMI.2020.2986777

Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou,
Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser.
2021a. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4690â€“4699.

Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. 2021b.
NeRFâ€“: Neural Radiance Fields Without Known Camera Parameters. arXiv preprint
arXiv:2102.07064 (2021).

Mark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D Collins, Yukun Zhu,
Liangzhe Yuan, Dahun Kim, Qihang Yu, Daniel Cremers, et al. 2021. DeepLab2: A
TensorFlow Library for Deep Labeling. arXiv preprint arXiv:2106.09748 (2021).
Yichen Wei, Eyal Ofek, Long Quan, and Heung-Yeung Shum. 2005. Modeling Hair from
Multiple Views. In ACM SIGGRAPH 2005 Papers (Los Angeles, California) (SIGGRAPH
â€™05). Association for Computing Machinery, New York, NY, USA, 816â€“820. https:
//doi.org/10.1145/1186822.1073267

Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. 2020. Multi-view neural human
rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern

Artemis: Articulated Neural Pets with Appearance and Motion Synthesis

â€¢

164:19

Recognition. 1682â€“1691.

Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. 2021. Space-time neural
irradiance fields for free-viewpoint video. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 9421â€“9431.

Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. 2021. H-nerf: Neural radiance
fields for rendering and temporal reconstruction of humans in motion. Advances in
Neural Information Processing Systems 34 (2021).

Lan Xu, Zhuo Su, Lei Han, Tao Yu, Yebin Liu, and Lu Fang. 2020. UnstructuredFusion:
Realtime 4D Geometry and Texture Reconstruction Using Commercial RGBD Cam-
eras. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 10 (2020),
2508â€“2522. https://doi.org/10.1109/TPAMI.2019.2915229

Ling-Qi Yan, Chi-Wei Tseng, Henrik Wann Jensen, and Ravi Ramamoorthi. 2015.
Physically-Accurate Fur Reflectance: Modeling, Measurement and Rendering. ACM
Trans. Graph. 34, 6, Article 185 (oct 2015), 13 pages. https://doi.org/10.1145/2816795.
2818080

Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and
Hanbyul Joo. 2021. BANMo: Building Animatable 3D Neural Models from Many
Casual Videos. arXiv preprint arXiv:2112.12761 (2021).

Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and
Tsung-Yi Lin. 2021. iNeRF: Inverting Neural Radiance Fields for Pose Estimation. In
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).

Wang Yifan, Felice Serena, Shihao Wu, Cengiz Ã–ztireli, and Olga Sorkine-Hornung.
2019. Differentiable surface splatting for point-based geometry processing. ACM
Transactions on Graphics (TOG) 38, 6 (2019), 1â€“14.

Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021.

PlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.

Cem Yuksel, Scott Schaefer, and John Keyser. 2009. Hair Meshes. ACM Trans. Graph.

28, 5 (dec 2009), 1â€“7. https://doi.org/10.1145/1618452.1618512

He Zhang, Sebastian Starke, Taku Komura, and Jun Saito. 2018. Mode-Adaptive Neural
Networks for Quadruped Motion Control. ACM Trans. Graph. 37, 4, Article 145 (jul
2018), 11 pages. https://doi.org/10.1145/3197517.3201366

Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu,
Yingliang Zhang, Lan Xu, and Jingyi Yu. 2021. Editable Free-Viewpoint Video
Using a Layered Neural Representation. ACM Trans. Graph. 40, 4, Article 149 (jul
2021), 18 pages. https://doi.org/10.1145/3450626.3459756

Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. 2019. Three-D
Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images" In the
Wild". In Proceedings of the IEEE/CVF International Conference on Computer Vision.
5359â€“5368.

Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 2017. 3D
menagerie: Modeling the 3D shape and pose of animals. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 6365â€“6373.

ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022.

