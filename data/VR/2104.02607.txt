1
2
0
2

y
a
M
4
1

]

V
C
.
s
c
[

2
v
7
0
6
2
0
.
4
0
1
2
:
v
i
X
r
a

MirrorNeRF: One-shot Neural Portrait Radiance
Field from Multi-mirror Catadioptric Imaging

Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu,
Lan Xu, and Jingyi Yu, Fellow, IEEE

Abstract—Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still,
existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view
synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF – a one-shot neural portrait
free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital
camera, which is the ﬁrst to combine neural radiance ﬁeld with catadioptric imaging so as to enable one-shot photo-realistic human
portrait reconstruction and rendering, in a low-cost and casual capture setting. More speciﬁcally, we propose a light-weight catadioptric
system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online
calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the
casual capture ability for convenient daily usages. We introduce a novel neural warping radiance ﬁeld representation to learn a
continuous displacement ﬁeld that implicitly compensates for the misalignment due to our ﬂexible system setting. We further propose a
density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner,
which not only improves the training efﬁciency but also provides more effective density supervision for higher rendering quality.
Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and
high-quality appearance free-viewpoint rendering for human portrait scenes.

Index Terms—Computational Photography, Neural Rendering, View Synthesis, Catadioptric Imaging.

(cid:70)

1 INTRODUCTION

W ITH the recent popularity of virtual and augmented

reality (VR and AR) to present information in an
innovative and immersive way, the high-quality 3D human-
centric content generation evolves as a cutting-edge yet
bottleneck technique. Reconstructing a high-quality and
photo-realistic human portrait scene conveniently for bet-
ter VR/AR experience has recently attracted substantive
attention of both the computer photography and computer
graphics communities.

The recent data-driven neural rendering techniques [1],
[2], [3], [4], [5] bring huge potential for realistic human
portrait modeling and rendering in novel views using only
RGB images as input. Speciﬁcally, the recent approaches [5],
[6] utilize neural radiance ﬁelds with volume rendering to
achieve photo-realistic free-viewpoint results of complicated
scenes. However, these approaches require to dispatch cam-
era rays through various view angles to model the continues
plenoptic function space, which inherently relies on multi-

• Z. Wang, L. Wang, and F. Zhao are with the School of Information Science
and Technology, ShanghaiTech University, Shanghai, 201210, China.
E-mail: {wangzy6,wangla,zhaofq}@shanghaitech.edu.cn

• M. Wu is with the School of Information Science and Technology,
ShanghaiTech University, Shanghai 201210, China, and the Shanghai
Institute of Microsystem and Information Technology, Chinese Academy
of Sciences, Shanghai 200031, China, and also with the University of
Chinese Academy of Sciences, Beijing, 100049, China.
E-mail: wumy@shanghaitech.edu.cn.
L. Xu, and J. Yu are with the School of Information Science and Technol-
ogy, ShanghaiTech University, and also with the Shanghai Engineering
Research Center of Intelligent Vision and Imaging, Shanghai, 201210,
China.
E-mail: {xulan1,yujingyi}@shanghaitech.edu.cn

•

view capture setting spatially [5], [7], [8] or temporally [9],
[10], [11]. Thus, reconstructing a one-shot neural radiance
ﬁeld for realistic portrait scenes through only a single cam-
era remains extremely challenging.

In this paper, we propose a rescue to such problem
by using a multi-mirror catadioptric imaging system. The
key idea is that the mirror is a natural dispatcher of rays
to enable diverse sampling across the continues 3D space,
which potentially allows 3D modeling of a scene through
only one-shot capture. Catadioptric imaging systems [12],
[13], [14] consist of mirrors (cata) and lenses (dioptric)
to capture a wide view of world with a single sensor
and have been broadly used in various tasks such as
panoramic imaging [15], 3D reconstruction [16], [17], [18],
light-ﬁeld modeling [19], [20] or hyperspectral sensing [21],
[22]. Most relatively to our setting, the approaches [17],
[18] enable one-shot 3D reconstruction by utilizing multi-
stereo techniques with multiple spherical mirrors. However,
only coarse geometry can be recovered, leading to inferior
texture results. Another method [20] utilizes similar spher-
ical mirror array for wide-angle light-ﬁeld rendering but
still suffers from unrealistic blending artifacts due to the
discontinuity of the recover geometry. More importantly,
none of existing approaches explore to utilize catadioptric
imaging to strengthen the neural radiance ﬁeld generation,
so as to provide photo-realistic reconstruction. Moreover, to
utilize the neural radiance ﬁeld [5], these capture systems
above require heavy manual labor to precisely calibrate the
mirrors and the camera, leading to the high restriction of the
daily usages with casually capture.

To tackle these challenges, we present MirrorNeRF – a

 
 
 
 
 
 
Fig. 1. Our MirrorNeRF approach combines a light-weight catadioptric imaging system with a novel neural scene representation Neural Warping
Radiance Field (NeWRF), which achieves photo-realistic novel view synthesis using only a single camera shot.

one-shot neural rendering approach using a catadioptric
imaging system with multiple sphere mirrors and a single
high-resolution digital camera. As illustrated in Fig. 1, our
approach is the ﬁrst to combine neural radiance ﬁeld with
catadioptric imaging, which enables one-shot photo-realistic
human portrait reconstruction and rendering, whilst main-
taining low-cost and casual capture setting.

More speciﬁcally, from the system side we ﬁrst introduce
a light-weight catadioptric imaging system using a sphere
mirror array, where all the mirrors are arranged tightly in
hexagons and form a honeycomb pattern to provide diverse
sampling in the continues 3D space. To enable easy deploy-
ment with a low budget, our hexagon design is drawn on a
simple ﬂat A3-size white paper with red dots on the vertices
and all the mirrors are manually placed without tedious
and absolutely precise arrangement. An effective online
calibration scheme based on the red dots for the mirror array
and the camera is proposed to further enable casually one-
shot capture. Compared with previous systems [14], [18],
[20], our deployment is much more easy to setup with a
low budget and casual capture ability, yet such convenience
presents the challenges on the algorithm side to handle
the misalignment caused by both the calibration and ar-
rangement. From the algorithm perspective, we extend the
neural radiance ﬁeld into the catadioptric imaging. Inspired
by the work [5] we ﬁrst adopt the radiance representation
and volumetric integration to obtain the density and color
values along the rays dispatched by our catadioptric system.
To further handle the misalignment challenge due to our
ﬂexible system setting, we introduce a novel neural warp-
ing radiance ﬁeld representation, which utilizes per-mirror
latent code to learn a continuous displacement ﬁeld, so
as to implicitly compensate the complicated misalignment.
Moreover, a density regularization scheme is proposed to
further leverage the inherent geometry information from the
catadioptric data in a self-supervision manner, which not
only improves the training efﬁciency but also provides more
effective density supervision for higher rendering quality. To
summarize, our main contributions include:

• We present a one-shot photo-realistic neural portrait
rendering approach in novel views, which is the ﬁrst
to combine neural radiance ﬁeld with multi-mirror
catadioptric imaging.

• we propose a light-weight catadioptric system design to
enable diverse ray sampling, which is low-cost, easily
deployed and supports casual capture.

• We propose a neural warping radiance ﬁeld representa-
tion as well as a density regularization scheme, which
can handle the misalignment caused by our ﬂexible
system to enable high-quality rendering.

2 RELATED WORK
2.1 3D Scene Modeling

Recent work has made a signiﬁcant process on 3D object
modeling and realism free-viewpoint rendering with level
sets of deep networks that implicitly map spacial locations
xyz to geometric representations (i.g., distance ﬁeld [23],
occupancy Field [24], [25] etc..). In contrast to the aforemen-
tioned explicit representations which require discretization
(e.g., in terms of the number of voxels, points, or vertices),
implicitly models shapes with a continuous function and
naturally is able to handle complicated shape topologies.
The implicit geometric modeling can be easily learned from
3D point samples [26], [27], [28], and the trained models
can be used to reconstruct shapes from a single image
or 3D part. However, these models are limited by their
requirement of access to ground truth 3D geometry, typ-
ically obtained from synthetic 3D shape datasets such as
ShapeNet [29]. Subsequent works relax this requirement of
ground truth 3D prior by formulating differentiable render-
ing functions that allow neural implicit shape representa-
tions to be optimized using only 2D images [4], [30].

When the viewing angle is zoomed in and out, there
will be different degrees of voids or artifacts. Voxel oc-
cupancy provides a regular representation, but due to its
memory-cost characteristics, it often only provide rough
low-resolution object expression. In fact, our goal
is to
express the object in a continuous space that conforms to
the physical facts. Therefore, we chose a continuous model
of neural radiance ﬁeld [5], as the basis of our work.

2.2 Free-Viewpoint Rendering

Free-viewpoint synthesis methods generally model
in-
put/target images as a collection of rays and essentially
aims to recover the plenoptic function [31] from dense
samples. Previous Image-Based Rendering (IBR) works [32]
used two plane parametrization, or 2PP, to represent rays
and render new rays via ray blending. They are able to
achieve real-time interpolation but require a lot of memory
as they need to cache all rays. Following works Buehler et
al. [33] bring in proxy geometric to select suitable views and

ﬁlter occluded rays by cross-projection to the image plane
when ray fusion. However, those methods are still limited
by the linear blending function usually result in strong
ghosting and blurring artifact. [34], [35] seek to model the
radiance ﬁeld as rays emitted from the object surface, i.e., the
surface light ﬁelds. The surface-based ray parameterization
can achieve a more continuous ray interpolation and resolve
the issue that the ray sample extremely unbalance in the
previous 2PP modeling.

Most recently, seminal researches seek to implicitly rep-
resent the radiance ﬁeld and render novel views with neural
networks. Chen et al. [36] presented Deep Surface Light
Fields, which use an MLP network to ﬁx per-vertex radiance
and learns to ﬁll up the missing data across angle and ver-
tices. This et al. [37] present a novel learnable neural texture
to model rendering as image translation. Another line of
research extends the free-viewpoint to dynamic sequence or
scene relighting. Wu et al. [2] model and render dynamic
scenes through embedding special features with the sparse
dynamic point cloud. Lombardi et al. [1] use a novel volu-
metric representation to reconstruct dynamic geometry and
appearance variations jointly while requires only image-
level supervision. Chen et al. [38] model image formation
in terms of environment lighting, object intrinsic attributes,
and the light transport function (LTF) that achieve free-
viewpoint relighting. Notable exceptions are the most recent
Nerf [5] and Nerf in the wild (Nerf-W) [6]. The Nerf implic-
itly models the radiance ﬁeld and the density of a volume by
neural networks, then uses direct volume render function to
reconstruct geometric and novel views. The following work
Nerf-W relaxes the strict consistency assumptions through
modeling per-image appearance variations such as expo-
sure, lighting, weather, and post-processing with a learned
low-dimensional latent space. Our system combines neural
radiance ﬁeld with catadioptric imaging for the ﬁrst time,
which enables photo-realistic human portrait reconstruction
and rendering within a single shot.

2.3 Catadioptric Imaging

Catadioptric imaging system consists of lenses and curved
surfaces to achieve a wide ﬁeld of view captured by a single
sensor. Baker et al. [12] present in detail the modeling and
the conﬁguration of single view point catadioptric image
systems. These systems require an accurate alignment of
the sensor optical axis and the focus of the curved mir-
rors to maintain the single viewpoint property. As for the
non-single viewpoint catadioptric imaging system, there is
no strict restriction on sensor location, making it possible
for multi-mirror catadioptric system construction to realize
multi-perspective capturing. Levoy et al. [19] use a planer
mirror array to capture light ﬁeld. Taguchi et al. [20] propose
a geometric non-approximate model for spherical mirror
array light ﬁeld capture. Xue et al. [21] propose spectral
coded spherical mirror arrays for acquiring 5D light ﬁelds.
A multi-mirror catadioptric system can also be used for
geometric reconstruction. Lanman et al. [14] manually select
several corresponding points on each spherical mirror to
recover vertex positions for the reconstruction of mesh.
Ding et al. [17] use piecewise GLCs [13] approximation for
the reﬂective rays to enable stereo matching and fast projec-
tion for volumetric reconstruction. Chen et al. [18] propose a

method for the multi-mirror system which enables multiple
central and non-central compacted stereo matching for 3D
reconstruction.

Differently, our system combines the catadioptric imag-
ing with neural radiance ﬁeld for the ﬁrst time to our
knowledge, which enables one-shot photo-realistic human
portrait reconstruction and rendering whilst maintaining a
low-cost and casual capture setting.

3 OVERVIEW
The proposed MirrorNeRF marries implicit neural radiance
ﬁeld with multi-mirror catadioptric imaging, which enables
one-shot photo-realistic human portrait reconstruction and
rendering in novel views. Fig. 1 illustrates the high-level
components of our approach, which takes a single catadiop-
tric image captured by a digital camera as input, and gen-
erates high-quality novel-view synthesis results in various
challenging human portrait scenarios as output.
Catadioptric Imaging System. We ﬁrst introduce a light-
weight catadioptric system design to enable diverse ray
sampling in one-shot capture, which is low-cost, easily de-
ployed, and supports casual capture. Our system consists of
a sphere mirror array and a high-resolution digital camera,
where all the mirrors are arranged tightly in hexagons and
form a honeycomb pattern. (Sec. 4).
Neural Portrait Rendering. We then propose a neural por-
trait rendering scheme based on the catadioptric image with
dispatched rays. We adopt the radiance representation and
volumetric integration of the work [5] to obtain the density
and color values along with these rays. We introduce a
neural warping radiance ﬁeld representation which learns
a continuous displacement ﬁeld to handle the misalignment
caused by our ﬂexible system. Moreover, a density regular-
ization scheme is proposed to further leverage the inherent
geometry information from the catadioptric data, which
improves the training efﬁciency rendering quality (Sec. 5).

4 CATADIOPTRIC IMAGING SYSTEM
Our light-weight catadioptric system consists of a mirror
arary and a digital camera, as shown in Fig. 2. The camera
faces toward the mirror array at a range of angles deter-
mined by the primitive shape of the mirror to capture the
entire mirror array in one shot. We will evaluate the angle
range of our system setting in the experimental section. We
use the high-resolution SONY ILCE7RM4 camera, which
can take 61MP pictures for capturing more details. The sys-
tem is decorated with a green screen as the background to
facilitate easy foreground segmentation. During capturing,
the target is placed in front of the mirror, and we shot only
one image for reconstruction.

Mirror Array. The mirror array in MirrorNeRF consists of
25 low-cost convex spherical reﬂecting mirrors. We scan the
geometry of the mirror in advance to obtain its approxi-
mate parameters (e.g., diameter and thickness). Mirrors are
placed tightly on a ﬂat plane with an A3-size white paper.
We draw hexagons on this paper and arrange them as a
honeycomb pattern. Each hexagon has a similar size to
the mirror and marks the mirror position. Thus, we can
place a mirror over it manually without tedious and special

Then, there exists a projection matrix P = K[R|t] to enable
the following operation:

(cid:21)

(cid:20)xi
1

λ

= P

(cid:21)

(cid:20)Xi
1

= K (cid:2) r1,

r2,

r3,

t (cid:3)













xi
yi
0
1

= K (cid:2) r1,

r2,

t (cid:3)





(1)





xi
yi
1

= H


 ,





xi
yi
1

Fig. 2. Our capture system consists of a camera and a mirror array. The
red box in the lower right corner shows the calibration pattern.

where H is the homography matrix which relates the
transformation between the mirror plane and the image
plane; K is the pre-calibrated camera intrinsic matrix;
R = [r1, r2, r3] and t are the world-to-camera rotation and
translation, respectively. Note that the homography matrix
H has 8 degrees of freedom. Thus, given more than four
reliable detected pairs of xi and Xi, we can estimate H by
using the Direct Linear Transformation(DLT) algorithm.

Furthermore, we decompose H into the extrinsic matrix
[R|t] using an efﬁcient solution. According to the Eqn. 1, we
have the following formulation:
αK−1H = (cid:2) r1,

t (cid:3) .

r2,

(2)

Fig. 3. Illustrations of light paths in our catadioptric system. (a) shows
the reﬂection on a mirror surface; The mirror has a diameter of 50mm;
(b) demonstrates that our system can capture rays coming from varying
directions in a single image through a mirror array.

precise operation. Such honeycomb pattern ensures that
the captured rays have varieties of directions and span the
continuous 3D space as much as possible for neural portrait
reconstruction, as illustrated in Fig. 2.

As described above, assembling this mirror array is
convenient and requires no precise operations. We assume
all the mirrors are put in the exact place and accordingly pre-
pare a 3D geometry template of the whole mirror array as a
3D proxy for the following ray restoration process. Note that
the actual geometry of the mirror array is inaccurate due
to the manual placement since the misalignment between
mirrors and hexagons violates our geometry assumption.
We attack the misalignment challenge by introducing a
warping ﬁeld in the next section.

System Calibration. As illustrated in Fig. 3, the rays from
the target scene are captured by the camera after being
reﬂected by the sphere mirror array. To enable neural scene
reconstruction in a casual capture setting, we perform online
calibration of our catadioptric system to restore rays in the
world coordinate system from image pixels.

To this end, we utilize the red calibration points to mark
the corners of hexagons on the ﬂat paper, which helps us
to obtain the relative pose between the camera and mirror
array. Speciﬁcally, we deﬁne this honeycomb pattern in the
world coordinate system, and thus the i-th calibration point
has a known 3D coordinate, deﬁned as Xi = (xi, yi, 0)T . Let
xi = (ui, vi)T be the projection of Xi on the image plane.

To solve the scale ambiguity, we obtain [r1 r2| t] by normal-
izing K−1H with the norm of its ﬁrst column, so that (cid:107)r1(cid:107)2
is close to one. Then, the initial extrinsic matrix [R(cid:48)|t] can
be obtained as follows:

[R(cid:48)|t] = (cid:2) r1,

r2,

r1 × r2,

t (cid:3) .

(3)

However, this solution does not ensure that R(cid:48) will be
orthogonal and have a determinant of 1. Thus, we apply the
orthogonal Procrustes to R(cid:48) = UΣVT and take R = UVT
as the ﬁnal rotation matrix.

Ray Restoration. Each restored ray is a sample of the
plenoptic function of the mirror array, which consists of the
observation point on the mirror surface o, the ray direction
d and the color c. After the above online calibration between
the mirror array and the camera in our catadioptric system,
we render the geometry of the mirror array to a depth map
and a normal map. Hence we can associate the image pixels
with corresponding 3D points on mirror surfaces. For each
pixel from the mirror array, there are two rays on the light
path, namely the camera ray rc(t) and the ray r(t) we
are going to restore. rc(t) = oc + tdc casts into the scene
from the pixel whose image coordinate is (u, v), where oc
is the position of the camera. The camera ray direction dc is
formulated as:

dc =

RT K−1[u v 1]T
(cid:107)K−1[u v 1]T (cid:107)2

.

(4)

Then, we restore r(t) = o+td to sample and reconstruct the
captured neural scene. Firstly, we calculate the intersection
of ray rc(t) and the mirror surface, namely o. Since we have
the depth td for the pixel, we have o = oc + tddc directly.
Next, we compute the direction d. Let nc denotes the normal
of o in camera frame. It can be obtained from the normal

Fig. 4. Illustration of our NeWRF rendering scheme. The latent code ωm controls the warping ﬁeld for each mirror and hence compensates calibration
errors. During the novel view rendering phase, the position of the sample point is directly fed into the neural radiance ﬁeld(purple dotted line).

map. n = RT nc is the transformed normal in the world
coordinate system. Thus d can be computed as in:

the warping ﬁeld to the m-th mirror. Then, our NeWRF is
formulated as:

d = dc − 2(nT dc)n.

(5)

Finally, we assign the corresponding pixel color to the
ray r(t). Fig. 3 illustrates the catadioptric light path in our
system.

5 NEURAL PORTRAIT RENDERING

Here, we introduce our neural portrait rendering scheme
based on the one-shot image from our ﬂexible catadiop-
tric system. Speciﬁcally, a Neural Warping Radiance Field
(NeWRF) is presented to restore the 3D scene and pro-
duce photo-realistic images at arbitrary viewpoints, which
handles the misalignment caused by manual arrangement
and coarse calibration. To further improve the rendering
quality and training efﬁciency, we also propose a density
regularization scheme that leverages the inherent geom-
etry information from captured data in an effective self-
supervision manner.

5.1 Neural Warping Radiance Field

The neural radiance ﬁeld [5] is a continuous representation
for mapping each 3D point x = (x, y, z) and a viewing di-
rection d = (θ, φ) to density σ and color c = (r, g, b). Please
refer to [5] for more details. Here, we further introduce a
novel warping displacement ﬁeld to alleviate the inﬂuence
of the misalignment.

The warping displacement ﬁeld is based on the key
observation that if a mirror’s position deviates from where
it should be, the rays from this entire mirror are calculated
wrongly and share a similar error pattern. Thus, our warp-
ing displacement ﬁeld is conditioned on the mirror to model
such error pattern implicitly, which maps each sample point
from rays emitted from the same mirror to a displacement
which can warp this point into a reference space. To this
end, we assign a learnable latent code ωm that modulates

F (x + ∆x, d) = (c, σ)

∆x = Ψwarp(x, ωm),

(6)

where Ψwarp is the warping displacement ﬁeld represented
by an MLP network; x is a sample point from the m-th
mirror; ∆x is the displacement for the 3D point, while F
is the neural radiance ﬁeld described in [5]. Note that we
only warp the position of the sample point, but its viewing
direction remains unchanged. As shown in Fig. 4, all points
are warped into a reference space where the neural radiance
ﬁeld is deﬁned, which is constrained according to the Eqn. 6.
This may cause a global distortion between the reference
space and the world coordinate system. Thus, we utilize the
mirror in the array center as an anchor to regularize the
reference space. Speciﬁcally, we do not apply Ψwarp on the
sample points from the central mirror and feed them directly
into F instead. Such strategy enables that the reference
space can be aligned with the world coordinate system as
close as possible.

5.2 Density Regularization

The original NeRF [5] takes the multilayer perceptron (MLP)
training as an optimization process to regress the solution
of continuous densities and colors in the space. However,
the direction of rays in our setting is more concentrated
than those multi-image ones, which means our setting lacks
enough rays from the back-view or side-view to efﬁciently
supervise the regression, especially for the density. It will
easily cause degenerated solutions (as illustrated in Fig. 7)
and slow convergence. During volume rendering, sam-
ple points near the visible geometry surface should con-
tribute most to the rendering result, and their corresponding
weights are dominating. Thus, we introduce a series of
density regularization to alleviate these problems, which
leverages the inherent geometry from captured data in a
self-supervised manner.

Bounding Box Sampling. Instead of using near-far planes
to sample 3D points along with rays, we pre-deﬁne a 3D

bounding box in the world coordinate and only sample
points inside it. This strategy reduces the number of points
which should be mapped by our NeWRF and limits the
solution space.

Implicit Visual Hull. We further segment the captured
image into foreground and background using the Chroma
Key algorithm. Sample points Xb located on the rays which
belong to background mask pixels are in void space, and
their densities should be close to zero. Thus, we propose a
visual hull loss Lv:

Lv =

1
|Xb|

(cid:88)

x∈Xb

|σ(x)|2,

(7)

where σ(x) is the density of sample point x.

Geometry-aware Regularization. Unfortunately, the visual
hull algorithm will produce a superﬂuous convex hull in
front of objects in our setting. And our visual hull loss
above is nonfunctional on this space. Even points inside the
superﬂuous hull have a low-density level. It is enough to
cause noise rendering results. Thus, we introduce a self-
supervision approach to regularize the density in these
regions. Given the sample points along a pixel ray r(t) =
o + td, we have the parameters ti, i ∈ {1, ..., n} for the n
sampled points. Then, the depth D(r) is formulated as:

D(r) =

n
(cid:88)

i=1

Wi(1 − exp(−h(σi)δi))ti,

Wi = exp(−

i−1
(cid:88)

j=1

h(σj)δj),

(8)

where δi = ti+1 − ti
samples. h(·) is a piece-wise function as follows:

is the distance between adjacent

h(x) =

(cid:26) 0 x ≤ τ

x otherwise

(9)

where τ is a threshold. Note that h(·) ﬁlters those noises
with low density in order to obtain more accurate depth
values.

During the network training, we randomly sample void
point xg for each ray in the certain range, where xg =
o + tgd and tg ∈ [0, D(r)). Let Xg denotes the set of these
void points of a bunch of rays. Then, the geometry-aware
regularization loss is formulated as:

Lg =

1
|Xg|

(cid:88)

x∈Xg

|σ(x)|2.

(10)

5.3 Implementation Details
We model both the warping displacement ﬁeld Ψwarp and
the neural radiance ﬁeld F as continuous functions using
two separate MLPs. We train the MLP for the neural radi-
ance ﬁeld for the ﬁrst three epochs individually to warm
up our network design, and then two MLPs are trained
together afterward. Our network structure is illustrated in
Fig. 4. The warping displacement network Ψwarp has 5
layers, 128 hidden neurons, and ReLU activation, while the
activation of the last layer is removed in Ψwarp. F has the
same structure as in the original NeRF [5].

Training rays are generated from the captured image
by the principle described in Sec. 4. We also mark the
corresponding mirror index for each ray so that we can
choose the corresponding warping latent code ωm during
training. We use 16 dimensions for the warping latent
codes and optimize them through backpropagation. During
training, we use Adam to optimize network parameters
with a learning rate of 1e−4. The total loss Ltotal contains
three parts: the same photometric loss Lc in the original
NeRF [5], the visual hull loss Lv as well as the geometry-
aware regularization loss Lg. Speciﬁcally, the total loss is
formulated as:

Ltotal = Lc + λ(Lv + Lg),

(11)

where λ = 10−2 in our experiments. Since the estimated
depth D(r) is full of noise and unreliable during the begin-
ning of the training, τ is 0 in the warm-up epochs. After that,
we adjust τ progressively during training, which increases
linearly from 0 to 20, and reaches the maximum value after
ﬁve epochs.

During inference, we remove the warping displacement
ﬁeld Ψwarp from networks and directly query radiance and
densities of sample points in the reference space and adopt
the volumetric integration strategy similar to [5], so as to
enable photo-realistic one-shot novel view synthesis.

6 EXPERIMENTAL RESULTS
In this section, we evaluate the free view rendering result
of our MirrorNeRF on various scenarios, followed by the
comparison with other methods, both qualitatively and
quantitatively. We provide the limitation and discussions
regarding our approach in the last subsection. Fig. 5 shows
that our approaches can generate free-view high-resolution
rendering with high-quality details. We train our model on
Nvidia GeForce RTX3090 GPU for 10 hours with 4000 rays
per batch. Each ray uses 96 samples in coarse volume and 32
additional adaptive samples in ﬁne volume for volumetric
integration. We render images with the output resolution of
1200 × 900 in our experiments. Our approach takes about
14 seconds to render each image.

6.1 Evaluate

Neural Warping Field Evaluation.

To evaluate our neural warping radiance ﬁeld, we
train our network with/without warping displacement ﬁeld
module Ψwarp. As shown in Fig. 6, results without warping
ﬁeld exhibit strong blurring, while others with warping
ﬁeld alleviate this phenomenon and produce photo-realistic
images. This evaluation shows that calibration errors caused
by manual placement have greatly inﬂuenced the neural
radiance ﬁeld regression and lead to poor results. Original
NeRF does not have the ability to correct these errors. Our
NeWRF with warping displacement ﬁeld module success-
fully tackles this problem.
Density Regularization Evaluation. We further evaluate
our density regularization. As shown in Fig. 7, the rendered
image appears ﬁne details with density regularization. We
can observe some noises in the result images without den-
sity regularization, especially in some areas such as hands

Fig. 5. Several examples demonstrate our proposed system’s quality and ﬁdelity render results on the data we captured, including human portrait,
objects, and human with things.

Fig. 6. Demonstrations of neural warping ﬁeld evaluation. (a) captured
reference image from one mirror in the array; (b) results with warping
module; (c) results without warping module.

and eyes. The regression of neural radiance ﬁeld in these
areas will easily be trapped in a local minimum and causes
noises without density regularization. On the contrary, our
density regularization scheme uses the geometric knowl-
edge priors learned from training to optimize the result.

We also quantitatively analyze the rendering quality on

Fig. 7. Demonstration of density regularization evaluation. (a) captured
reference image from 2 mirrors in the array; (b) results with density
regularization; (c) results without density regularization.

both real and synthetic data. As for real data, we train the
same checkerboard data in three ways and render each of
them in 262 views. Fig. 9 illustrates one image of them.
Both the model in Fig. 9(a) and Fig. 9(a) appear unpleasant
artifacts. In contrast, our complete model reconstructs the
checkerboard in detail and with less noise. To quantify
the rendering quality, We deﬁne two metrics. The average
Detection Success Rate measures the rendering quality of

Fig. 8. Qualitative evaluation of the number of mirrors on both real data and synthetic data. Geometry becomes incomplete, and details are lost
when decreasing the number of mirrors.

TABLE 2
Quantitative comparison on synthetic data.

PSNR ↑

SSIM ↑

LPIPS ↓

w/o Neural warping
w/o Density Reg.
Ours

22.64
24.29
24.82

.8838
.9044
.9070

.1547
.0861
.0854

Fig. 9. Qualitative evaluation of our method to reconstruct the same
checkerboard. (a) w/o neural warping. (b) w/o density regularization. (c)
ours.

TABLE 1
Quantitative evaluation on reconstructing the same checkerboard.

Methods

w/o Neural warping
w/o Density Reg.
Ours

Detection
Success Rate
0.7595
0.8931
0.9656

Mean
Reprojection Error
0.1916
0.1249
0.1197

the reconstructed corner. For each view, only when all the
corners of the checkerboard are detected will it be consid-
ered successful. The Mean Reprojection Error measures the
geometric correctness of reconstruction. We ﬁrst obtain the
checkerboard pose using all checkerboard corners. Then, we
project the real checkerboard corners to the corresponding
view to evaluate the reprojection error. As shown in Tab. 1,
our complete model achieves the best result in both detect-
ing corners and reprojection.

As for synthetic data, we add Gaussian noise on the
mirror positions to simulate the actual situation. And we
all render the same 97 novel views for each method and
compare them to the ground truth. Tab. 2 summarizes the
effects of our model components. Our full model performs
the best in all metrics for evaluation: Peak Signal-to-Noise
Ratio (PSNR), Structural Similarity (SSIM) [39] and Learned
Perceptual Image Patch Similarity (LPIPS) [40].

We further evaluate the inﬂuence when the background
is not a green screen. In this scenario, We ﬁrst evaluate our
method using the NDC sampling strategy, which is pro-
posed in [5] to handle large depth. As shown in Fig. 10(a),

Fig. 10. Qualitative evaluation of the scene with textured background
and parallax. (a) NDC sampling w/o density regularization. (b) Ours w/o
density regularization. (c) Ours.

NDC helps reconstruct the scene with background, but it de-
creases the rendering quality of the portrait. Our approach
only without the regularization in Fig. 10(b) also reduces
the foreground’s quality while learning the background and
the foreground at the same time. In contrast, our entire
pipeline in Fig. 10(c) enables more photo-realistic rendering
of the foreground and further compositing purposes. We
provide the corresponding quantitative analysis in Table. 3.
It shows the effectiveness of our density regularization for
reconstructing the foreground, especially for handling our
unique catadioptric setting. Note that in Figure 10(b)(c), we
expand the bounding box during training and rendering to
cover the background. While in our current regularization
design with a green screen, we can only sample rays from
the foreground bounding box to avoid unnecessary sam-
pling and achieve better results.

Fig. 11. Evaluation of different camera positions. Our catadioptric system is unstructured, which can capture the image of the mirror array from
various view angles of a handheld camera.

TABLE 3
Quantitative evaluation of the scene with textured background and
parallax.

PSNR ↑

SSIM ↑

LPIPS ↓

NDC w/o Reg.
Ours w/o Reg.
Ours

23.19
24.16
24.41

.8979
.9019
.9049

.1055
.0956
.0896

TABLE 4
Quantitative evaluation on different number of mirrors.

# Mirrors

PSNR ↑

SSIM ↑

LPIPS ↓

5
11
17
25

21.72
22.38
23.15
24.82

.9009
.9001
.9030
.9070

.1356
.1159
.0948
.0854

Fig. 12. Training loss for different camera positions.

Mirror Number Evaluation. To evaluate the inﬂuence of the
number of mirrors, we only use the reﬂected rays of selected
5, 11, 17, and 25 mirrors, respectively. As Fig. 8 shows, the
reconstruction result suffers from geometry and rendering
artifacts at the non-central part of the scene when decreasing
the mirror numbers. Quantitatively, Tab. 4 shows that using
25 mirrors outperforms the best in all scenarios.

Camera Position Evaluation. To demonstrate that our sys-
tem is unstructured, we reconstruct the same facial mask
by capturing the data in 4 different views using a handheld
camera. The ﬁrst row in Fig. 11 is the raw captured data.
View 1 is the viewpoint we use most in our system, where
most of the pixels are the reﬂected rays of mirrors. View
2 and View 3 are at a certain angle to our catadioptric
system. View 4 is from a really oblique perspective. From
the second row in Fig. 11, our system can achieve similar
results in normal viewing angles, for example, View 1, View
2, and View3. In the skewed viewpoint like View 4, the
performance drops and some artifacts appears.

Fig. 12 are curves of the training loss for four different
views. View 1, View 2, and View 3 coverage similarly,
while the fourth one is slower and has a higher loss. It
quantitatively reveals that our system can achieve similar
performance in most capturing angles and has some limita-
tions in some skewed angles.

6.2 Comparison

We compare our complete model with per-vertex color and
origin NeRF. To get the per-vertex color, we use ray tracing
to query the corresponding pixel color. Its geometry proxy is
extracted from the original NeRF using marching cube with
a threshold to ﬁlter the geometry noise.

As illustrated in Fig. 13, our method achieves signif-
icantly more photo-realistic novel view rendering results
than other methods.

To quantitatively compare our method to others, we
similarly reconstruct the same checkerboard in Fig. 14, cal-
culate the Average Detection Success Rate and Mean Re-
projection Error in Tab. 5 using three methods. Also, Tab. 6
shows the performance of the three on synthetic data with
noise on positions of mirrors by comparing PSNR, SSIM,
and LPIPS. Our method surpasses others in all indicators
with distinct differences. Vertex color greatly relies on the
geometry proxy. Without an accurate geometry and enough
vertices of the 3D model, it cannot provide a high-quality
rendering. Also, it cannot support view-dependent effects.
On the other hand, origin NeRF suffers from noises in the
density ﬁeld if the view directions are not diverse enough.
It is also unable to handle the misalignment due to the
inaccurate position of mirrors. In contrast, our method is
capable of reconstructing the scene with high quality in our

Fig. 13. Qualitative comparison on both the real (the ﬁrst two rows) and synthetic data (the third row). From left to right: the results of the per-vertex
color scheme via ray tracing, the original NeRF and ours, respectively.

TABLE 6
Quantitative comparison on synthetic data.

PSNR ↑

SSIM ↑

LPIPS ↓

Vertex Color
NeRF
Ours

18.64
21.79
24.82

.8456
.8833
.9070

.1842
.1368
.0854

Fig. 14. Qualitative evaluation of our method to reconstruct the same
checkerboard. From left to right: the results of the per-vertex color
scheme via ray tracing, the original NeRF and ours, respectively.

TABLE 5
Quantitative comparison on reconstructing the same checkerboard.

Methods

Vertex Color
NeRF
Ours

Detection
Success Rate
0.1870
0.7061
0.9656

Mean
Reprojection Error
0.6116
0.2104
0.1197

catadioptric setting.

6.3 Limitation and Discussion

As the ﬁrst trial to explore the problem of combining neural
radiance ﬁeld with catadioptric imaging to reconstruct and
render a scene in a convenient capture setting, the proposed
MirrorNeRF still owns limitations as follows.

First, our system requires that all the mirrors are almost
on a plane. Thus, our method can only reconstruct the front
view of the human or object. One of our further steps
is to make our mirror arrangement unstructured. If the
mirrors are placing around the human portrait or object, for
example, in a sphere structure, the full body of the human
or object can be reconstructed. Our method is also restricted
to the number of distinct reﬂected light rays from the scene
to be reconstructed. The high-resolution camera, number of
mirrors, and not skewed viewing angle are used to provide
enough light rays such that the rendering result has high
ﬁdelity even in details. It is interesting that the requirements
of our system can be reduced to portable. We expect that our
system will enable everyone to reconstruct anything they
want anytime and anywhere by his or her cellphone and just
several mirrors in the pocket. Besides, it still needs hours
to train for a static scene. Although our system is suitable
for video-rate image capturing, it is hard to reconstruct a

dynamic scene in a short time. It’s a promising direction
to use some neural prior on human portrait or the kind
of objects we will reconstruct or some other techniques
to speed up training without decreasing the performance.
Another direction is to enable our neural warping ﬁeld to be
deform-able in time series, so the dynamic scene is recorded
in our radiance ﬁeld.

7 CONCLUSION

We have presented the ﬁrst approach to combine neural
radiance ﬁeld with catadioptric imaging, which enables
one-shot, photo-realistic and free-viewpoint human portrait
reconstruction and rendering. Our light-weight catadioptric
system design enables diverse ray sampling in the 3D space,
which is low-cost, easily deployed and supports casual cap-
ture. Our novel neural warping radiance ﬁeld enables im-
plicit compensation of the misalignment due to our ﬂexible
system setting via a continuous displacement ﬁeld learning.
Our self-supervised density regularization scheme further
improves the training efﬁciency and provides more effective
density supervision for higher rendering quality. Our exper-
imental results demonstrate the effectiveness of MirrorNeRF
for compelling one-shot neural portrait rendering in various
challenging scenarios, which compares favorably to the
state-of-the-arts. Given the aforementioned distinctiveness,
we believe that our approach is a critical step to enable
conveniently and photo-realistic human portrait modeling,
with many potential applications in VR/AR like gaming,
entertainment and immersive telepresence.

ACKNOWLEDGMENTS

This work was supported by NSFC (grant nos. 61976138 and
61977047), and STCSM (2015F0203-000-06).

REFERENCES

[1]

S. Lombardi, T. Simon, J. Saragih, G. Schwartz, A. Lehrmann,
and Y. Sheikh, “Neural volumes: Learning dynamic renderable
volumes from images,” arXiv preprint arXiv:1906.07751, 2019.
[2] M. Wu, Y. Wang, Q. Hu, and J. Yu, “Multi-view neural human
rendering,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2020.

[3] G. Riegler and V. Koltun, “Free view synthesis,” in Computer Vision
– ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.
Cham: Springer International Publishing, 2020.

[4] V. Sitzmann, M. Zollh ¨ofer, and G. Wetzstein, “Scene representation
networks: Continuous 3d-structure-aware neural scene represen-
tations,” in Advances in Neural Information Processing Systems, 2019.
[5] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-
mamoorthi, and R. Ng, “Nerf: Representing scenes as neural
radiance ﬁelds for view synthesis,” arXiv preprint arXiv:2003.08934,
2020.

[6] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Doso-
vitskiy, and D. Duckworth, “Nerf in the wild: Neural radi-
ance ﬁelds for unconstrained photo collections,” arXiv preprint
arXiv:2008.02268, 2020.

[7] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, “Nerf++: An-
alyzing and improving neural radiance ﬁelds,” arXiv preprint
arXiv:2010.07492, 2020.

[8] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt, “Neural sparse

voxel ﬁelds,” NeurIPS, 2020.

[9] W. Xian, J.-B. Huang, J. Kopf, and C. Kim, “Space-time neu-
irradiance ﬁelds for free-viewpoint video,” arXiv preprint

ral
arXiv:2011.12950, 2020.

[10] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M.
Seitz, and R.-M. Brualla, “Deformable neural radiance ﬁelds,”
arXiv preprint arXiv:2011.12948, 2020.

[11] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer,
“D-nerf: Neural radiance ﬁelds for dynamic scenes,” arXiv preprint
arXiv:2011.13961, 2020.

[12] S. Baker and S. K. Nayar, “A theory of single-viewpoint cata-
dioptric image formation,” International journal of computer vision,
vol. 35, no. 2, pp. 175–196, 1999.

[13] J. Yu and L. McMillan, “General linear cameras,” in European

Conference on Computer Vision. Springer, 2004, pp. 14–27.

[14] D. Lanman, D. Crispell, M. Wachs, and G. Taubin, “Spherical
catadioptric arrays: Construction, multi-view geometry, and cal-
ibration,” in Third International Symposium on 3D Data Processing,
Visualization, and Transmission (3DPVT’06).
IEEE, 2006, pp. 81–88.
[15] G. Krishnan and S. K. Nayar, “Cata-ﬁsheye camera for panoramic
imaging,” in 2008 IEEE Workshop on Applications of Computer Vision.
IEEE, 2008, pp. 1–8.

[16] B. Micusik and T. Pajdla, “Autocalibration 3d reconstruction with
non-central catadioptric cameras,” in Proceedings of the 2004 IEEE
Computer Society Conference on Computer Vision and Pattern Recogni-
tion, 2004. CVPR 2004., vol. 1, 2004, pp. I–I.

[17] Y. Ding, J. Yu, and P. Sturm, “Multiperspective stereo matching
and volumetric reconstruction,” in 2009 IEEE 12th International
Conference on Computer Vision.

IEEE, 2009, pp. 1827–1834.

[18] S. Chen, Z. Xiang, N. Zou, Y. Chen, and C. Qiao, “Multi-stereo
3d reconstruction with a single-camera multi-mirror catadioptric
system,” Measurement Science and Technology, vol. 31, no. 1, p.
015102, 2019.

[19] M. Levoy, B. Chen, V. Vaish, M. Horowitz, I. McDowall, and
M. Bolas, “Synthetic aperture confocal imaging,” ACM Transactions
on Graphics (ToG), vol. 23, no. 3, pp. 825–834, 2004.

[20] Y. Taguchi, A. Agrawal, A. Veeraraghavan, S. Ramalingam, and
R. Raskar, “Axial-cones: Modeling spherical catadioptric cameras
for wide-angle light ﬁeld rendering,” ACM Transactions on Graphics
(ToG), vol. 29, no. 6, p. 172, 2010.

[21] Y. Xue, K. Zhu, Q. Fu, X. Chen, and J. Yu, “Catadioptric hyper-
spectral light ﬁeld imaging,” in Proceedings of the IEEE International
Conference on Computer Vision, 2017, pp. 985–993.

[22] D. O. Baskurt, Y. Bastanlar, and Y. Y. Cetin, “Catadioptric hyper-
spectral imaging, an unmixing approach,” IET Computer Vision,
vol. 14, no. 7, pp. 493–504, 2020.

[23] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Love-
grove, “Deepsdf: Learning continuous signed distance functions
for shape representation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2019, pp. 165–174.

[24] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and
A. Geiger, “Occupancy networks: Learning 3d reconstruction in
function space,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2019, pp. 4460–4470.

[25] Z. Chen and H. Zhang, “Learning implicit ﬁelds for generative
shape modeling,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2019, pp. 5939–5948.

[26] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and
A. Geiger, “Convolutional occupancy networks,” arXiv preprint
arXiv:2003.04618, 2020.

[27] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and
H. Li, “Pifu: Pixel-aligned implicit function for high-resolution
clothed human digitization,” in Proceedings of the IEEE International
Conference on Computer Vision, 2019, pp. 2304–2314.

[28] S. Saito, T. Simon, J. Saragih, and H. Joo, “Pifuhd: Multi-level pixel-
aligned implicit function for high-resolution 3d human digitiza-
tion,” in Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020, pp. 84–93.

[29] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,
Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., “Shapenet:
repository,” arXiv preprint
An information-rich 3d model
arXiv:1512.03012, 2015.

[30] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, “Differen-
tiable volumetric rendering: Learning implicit 3d representations
without 3d supervision,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020, pp. 3504–3515.
[31] P. Debevec, C. Bregler, M. Cohen, and L. Mcmillan, “Image-based

modeling and rendering,” 1998, p. 299.

[32] M. Levoy, “Light ﬁeld rendering,” in Conference on Computer Graph-

ics and Interactive Techniques, 1996, pp. 31–42.

[33] C. Buehler, M. Bosse, L. Mcmillan, S. Gortler, and M. Cohen,
“Unstructured lumigraph rendering,” in Conference on Computer
Graphics and Interactive Techniques, 2001, pp. 425–432.

[34] D. N. Wood, D. I. Azuma, K. Aldinger, B. Curless, T. Duchamp,
D. H. Salesin, and W. Stuetzle, “Surface light ﬁelds for 3d pho-
tography,” in Proceedings of the 27th annual conference on Computer
graphics and interactive techniques. ACM Press/Addison-Wesley
Publishing Co., 2000, pp. 287–296.

[35] W.-C. Chen, J.-Y. Bouguet, M. H. Chu, and R. Grzeszczuk, “Light
ﬁeld mapping: efﬁcient representation and hardware rendering of
surface light ﬁelds,” ACM Transactions on Graphics (TOG), vol. 21,
no. 3, pp. 447–456, 2002.

[36] A. Chen, M. Wu, Y. Zhang, N. Li, J. Lu, S. Gao, and J. Yu, “Deep
surface light ﬁelds,” Proceedings of the ACM on Computer Graphics
and Interactive Techniques, vol. 1, no. 1, pp. 1–17, 2018.

[37] J. Thies, M. Zollh ¨ofer, and M. Nießner, “Deferred neural render-
ing: Image synthesis using neural textures,” ACM Transactions on
Graphics (TOG), vol. 38, no. 4, pp. 1–12, 2019.

[38] Z. Chen, A. Chen, G. Zhang, C. Wang, Y. Ji, K. N. Kutulakos,
and J. Yu, “A neural rendering framework for free-viewpoint
relighting,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 5599–5610.

[39] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,”
IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600–612,
2004.

[40] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The
unreasonable effectiveness of deep features as a perceptual met-
ric,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2018, pp. 586–595.

