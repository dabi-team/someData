A Survey on Applications of Digital Human Avatars
toward Virtual Co-presence

Matthew Korban · Xin Li

2
2
0
2

n
a
J

1
1

]

C
H
.
s
c
[

1
v
8
6
1
4
0
.
1
0
2
2
:
v
i
X
r
a

Latest Revision: May 2021

Abstract This paper investigates diﬀerent approaches to build and use digital
human avatars toward interactive Virtual Co-presence (VCP) environments.
We evaluate the evolution of technologies for creating VCP environments and
how the advancement in Artiﬁcial Intelligence (AI) and Computer Graphics
aﬀect the quality of VCP environments. We categorize diﬀerent methods in the
literature based on their applications and methodology and compare various
groups and strategies based on their applications, contributions, and limita-
tions. We also have a brief discussion about the approaches that other forms
of human representation, rather than digital human avatars, have been uti-
lized in VCP environments. Our goal is to ﬁll the gap in the research domain
where there is a lack of literature review investigating diﬀerent approaches for
creating avatar-based VCP environments. We hope this study will be useful
for future research involving human representation in VCP or Virtual Real-
ity (VR) environments. To the best of our knowledge, it is the ﬁrst survey
research that investigates avatar-based VCP environments. Speciﬁcally, the
categorization methodology suggested in this paper for avatar-based methods
is new.

Keywords Co-presences, Avatar, Virtual Reality, Motion Retargeting

1 Introduction

Remote communication has a crucial role in modern societies. An essential
aspect of eﬀective remote communication is co-presence, where multiple par-
ticipants can see and interact with each other in a shared virtual environment
so-called Virtual co-presence (VCP). The VCP is a sense of being with others
in a virtual world that people are psychologically connected, and are available

Department of Electrical and Computer Engineering, Louisiana State University, Baton
Rouge 70803, U.S.A.
E-mail: acw6ze@virginia.edu (Matthew Korban), xinli@lsu.edu (Xin Li)

 
 
 
 
 
 
2

Matthew Korban, Xin Li

and accessible to others [1]. The VCP environments can be more engaging than
text or voice-based chat in communications, collaborations, and training. Re-
cently, such a virtual co-presence has been studied in remote education [2],
training simulations [3,4], therapy treatments [5] and social interaction venues
[6].

A VCP environment can be created using a Virtual Reality (VR) platform.
A well-known approach to represent the human character in VR environments
is using “Avatar” models. An avatar is a digital form of human character that
can be represented as 2D or 3D. 3D avatars have some advantages over 2D
avatars, such as being more human-like, having realistic motions, and oﬀering
an immersive experience in VR and VCP environments.

In this paper, we investigate diﬀerent technologies designed toward creating
VCP environments based on digital human avatar models. We evaluate how the
evolution of technologies in Artiﬁcial Intelligence (AI) and Computer Graphics
aﬀect the human representation quality in VCP environments. We mainly
study two types of researches: (1) works that create or use digital avatars
models in VCP environments (2) works that build the avatar models or the
pipelines that are highly beneﬁcial for developing VCP environments. For both
types of research, we will explain and compare their advantages, limitations,
and also applications. We also will have a short discussion about the methods
that used other forms of human character representation rather than digital
human avatar to create VCP environments.

Fig. 1 shows our methodology of categorizing of the works carried out in
the literature to construct VCP environments. As can be seen in the ﬁgure,
we classify non-avatar based methods into three categories of Robotics, Mobile
systems, and Images and videos. We also compare non-avatar based researches
based on their applications, advantages and disadvantages.

On the other hand, we classify the avatar-based approaches into two main
types of Direct-motion retargeting and Pre-rendered motions. In the methods
that used Direct-motion retargeting, the motion is transferred directly from
users to digital avatars. In contrast, in the approaches that exploited Pre-
rendered motions, the motions are pre-deﬁned by developers. Direct-motion
retargeting methods provide authentic motions resembling users’ actions. Al-
though some of these strategies are not used directly in VCP environments,
their algorithm pipelines or avatar models are highly beneﬁcial in creating
prospective VCP environments based on digital human avatars.

Next, we further divide the Direct-motion retargeting category into two
classes of Image-based and Sensor-based approaches. In the image-based strate-
gies, the input is commonly RGB images, video, or depth images On the
other hand, in the sensor-based approaches, the input is obtained from wear-
able sensor devices, tracking sensors, and tracking markers. Both Pre-rendered
motion and Sensor-based categories mostly include similar application-based
approaches. As a result, we compare these two categories based on their ap-
plications, beneﬁts, and drawbacks.

The Image-based category consists of two groups of Oﬄine motion retar-
geting and Online motion retargeting. Oﬄine motion retargeting approaches

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

3

often oﬀer more accurate results than online methods by reconstructing high-
quality body shape, pose, and textures. Online motion retargeting is, however,
faster and more suitable for a real-time VCP or VR experiments. Furthermore,
the online motion retargeting approaches consist of two types of (1) 3D model
reconstruction, where they reconstruct the human character model or scene,
and (2) the methods that used pre-designed Rigged avatar models. In this pa-
per, we compare diﬀerent Image-based strategies based on their applications,
advantages, and disadvantages in VCP environments. Since the image-based
methods commonly made technical contributions, we also will illustrate the
gradual evolution of technology in each sub-category. This illustration explains
how new approaches resolve previous issues.

There have been a few research reviewing diﬀerent approaches to build
VCP environments. Krist et al. [7] are the only researchers that conducted
a comprehensive study on VCP environments. However, they discussed the
strategies that utilized non-avatar/robotics methods. On the other hand, in
our research, we focus on using the digital human avatar, which is the state-of-
the-art form of human representation in VCP environments. Moreover, there
has also been a lack of thorough research on human avatars in general. Hud-
son et al. [8] conducted a short study about general applications of digital
human avatars. Our literature review, however, investigates the strategies to-
ward building VCP environments. We also carried out a signiﬁcantly more
comprehensive reviewing survey than [8] by categorizing diﬀerent methods, in-
vestigating their advantages, limitations and applications, comparing various
approaches and categories and evaluating the evolution of this new technology.

Fig. 1 Our categorization methodology of diﬀerent methods in the literature toward cre-
ating Virtual co-presence (VCP) environments.

4

Matthew Korban, Xin Li

This research’s contributions are: This literature review is the ﬁrst re-
search that investigates diﬀerent approaches toward building VCP environ-
ments upon digital human avatar models to the best of our knowledge. No-
tably, the categorization methodology of avatar-based methods in this paper
is new. Moreover, we conducted a comprehensive analysis on diﬀerent cate-
gories and methods based on their advantages, limitations, applications, and
the evolution of technologies used in VCP environments.

2 Non-Avatar-based Approaches

Non-avatar-based approaches usually rely on physical hardware to create a
VCP environment. The hardware can be an interactive robot, a mobile system
installed on a moving platform, or traditional video-based telepresence. While
this research’s primary focus is on avatar-based approaches, we brieﬂy explain
non-avatar methods as follows:

2.1 Robotics

Robotics has been used for various VCP/telepresence purposes, such as com-
munication, environmental visualization, and training physicians. However, its
widespread usage has been to improve the interactions of a social VCP envi-
ronment. For example, in [9] Minato et al. suggested a portable human-like to
users can feel others’ presence. This human-like sensation is achieved by us-
ing human-like voice, appearance, and touch that users can communicate and
talk to others remotely. The experiments showed that people quickly started
conversation with the robot and are impressed by its shape and feeling. In
[10], Yoon et al. proposed a robotic telepresence system with some modern
features such as a projector and a head tracker system. These features make
the communication between the user and robot more interactive. The sug-
gested features are unique in telepresence applications that led the system
to be more eﬀective than traditional robotics system. Robotics also has been
used for environmental visualization. For instance, in [11] Macharet et al. de-
signed a telepresence robot to visualize the environment for users. The robot
is controlled by a remote human operator and can smoothly navigate through
a house with capability of handling complicated situations such as narrow cor-
ridors and doors. The results showed that using the proposed method helps
to reduce the number of environmental collisions during navigating with the
robot.

The main drawbacks of using robots are the need for regular maintenance,
limitation of the robot to perform human-like interactions, the diﬃculty of
adding new features and users’ discomfort to use robots.

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

5

2.2 Mobile Systems

A mobile system is a set of interactive tools integrated on moving frames to
create a VCP environment. Compared to conventional robots, mobile systems
are more focused on mobility, accessibility, and practical applications [12]. As
an example, in [13] Beer et al. designed a mobile system consisting of several
modules such as a touch screen installed on a phone frame, a microphone, a web
camera, and speakers to help elders to interact with visitors. The experimental
results showed that elders have good experience of interactivity and visibility in
the designed mobile system that will reduce traveling costs and social isolation.
In [14] Lee et al. suggested a mobile system to enable remote workers to live
and work with local coworkers similar to how they do it physically in real life.
They exploited a Texai Alpha prototype [15] mobile system for this purpose.
The experimental results obtained from the surveys indicated that the remote
pilots have a similar experience of working with real local coworkers.

While mobile systems can be equipped with state-of-the-art electronic
tools, they share the similar problems with robotics systems. Moreover, they
strictly rely on manufacturer’s available development kit tools for adding any
new features [16].

2.3 Images and Videos

Using images and videos is the traditional way of creating a co-presence envi-
ronment. As an example of such an approach, in [17] Noda et al. proposed a
telecommunication system to connect diﬀerent users using a conﬁgurable tile
display. The proposed scheme includes several features to oﬀer a realistic sen-
sation of co-presence such as life-size processing, subtracting the background,
and using multiple cameras. The designed VCP system provides a higher sense
of presence compared to the traditional video-based approaches. An advanced
video-based VCP environment is designed in [18] called the “real-world” where
users can view each other from diﬀerent perspectives. The real-world imple-
mentation is accomplished by using a multiview video capturing system and
eighteen PCs in a cylindrical chamber. The experimental results showed that
the suggested system can be delivered eﬃciently to users who watch others in
real-time. Compared to robotics and mobile systems, using images and videos
for creating VCP environments has some advantages and drawbacks. For ex-
ample, while large displays can provide more human-like sensation and require
minimum maintenance, they lack interactivity and a real 3D experience.

Table 1 summarizes the diﬀerent non-avatar-based methods have been sug-

gested to create VCP environments.

3 Avatar-based Approaches

In avatar-based approaches, commonly, a digital 3D human model represents
real humans in VCP environments [19]. Compared to non-avatar methods, us-

6

Matthew Korban, Xin Li

(a)

(b)

(c)

Fig. 2 Examples of (a) robotics [9] , (b) mobile system [14] and (c) image and videos-based
[17] VCP environments.

Table 1 Summary of non-avatar methods.

Category
Robotics

Applications
telepresence [9, 10]
and remote
navigation [11]

Advantages
compactness and
conﬁgurable
modular systems

Mobile Systems

Images and Videos

elderly healthcare
assistance [13] and
remote
working [14]
tele-communication
[17, 18]

mobility,
accessibility and
practical
applications
large displays and
minimum
maintenance

Disadvantages
regular
maintenance,
limited interactions
and features and
users’ discomfort
regular
maintenance and
limited features
and interactions
lack of interactivity
and real 3D
experience

ing an avatar doesn’t require any special maintenance and oﬀers more human-
like sensations and interactions in a 3D world. Moreover, in contrast to non-
avatar approaches that rely on hardware, avatar-based strategies depend on
software to create a VCP environment. Hence, they are more ﬂexible in adding
new features and can be upgraded based on state-of-the-art AI and Computer
Graphics algorithms to fulﬁll users’ needs. In this paper, we ﬁrst categorize
the avatar-based approaches to two types of Pre-rendered motions where the
animations are pre-deﬁned and Direct motion retargeting where users directly
control the avatars. We will give more explanation about the aforementioned
categories as follows:

3.1 Pre-Rendered Motions

A 3D avatar in a VCP environment can be animated by pre-designed motions
and scenes. For example, in [20], Pazour et al. simulate a conference room
with user-deﬁned avatars that can communicate with each other remotely.
This simulation’s primary goal is to evaluate users’ realistic feeling of co-
presence in a virtual environment. Two experiments were conducted, where
two scenarios control head motions: (1) tracking by a Head-Mounted Device
(HMD), HTC Vive, and (2) using mouse movements in a desktop PC. The
avatars’ upper and lower body are animated using pre-rendered animations

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

7

developed by Mixamo [21]. The experimental results showed that using the
VR headset outperforms the desktop (mouse movement) in terms of feeling
other users’ presence.

Pre-rendered motions have also been used to simulate metropolitan struc-
tures such as college [22], airport [23] and Museum [24]. For example, in [22],
Monahan et al. simulated a college environment with students and teachers. In
the designed scenario, students select a unique 3D character during the regis-
tration process to represent them onscreen in the 3D university environment.
The avatars are human-like and can perform a variety of pre-designed actions
associated with their roles. The results illustrated the eﬀectiveness of the pro-
posed technique, and users’ realistic feeling of others’ activities and presence.
In another example, in [23], Li et al. simulated taking oﬀ an aircraft in an
airport runway environment. Five users can interact with each other while
wearing VR goggles to see other users‘ avatars. The users can take the role of
a pilot, a support oﬃce, a tractor guide, a carrier aircraft guide, and a trac-
tor driver. The carried surveys illustrated that the proposed type of exhibit
is exciting and attractive for users. In [24] Mu et al. designed a VR environ-
ment for multi-user learning in Museum. The primary way of the interaction
between users is pre-deﬁned gestures that can be customized by users using a
Graphical User Interface (GUI). Participants can select their appearance and
clothing before entering the VR environment. The results indicated that the
method was eﬀective for users to transfer their motion in the virtual location.
Pre-rendered motions also have been used for psychological testing pur-
poses. For example in [25], Brown et al. presented a narrative story in a VR
environment. Users can play the game over a network connection wearing a
head-mounted display (Oculus Rift). The goal is to study a set of guided cam-
era and gaze distracting techniques to determine how to attract unfocused
individuals to the same story. The results showed a better understanding of
factors that cause users’ attraction to a narrative story.

Using pre-rendered motions and scenes can be suitable for the scenarios
that speciﬁc animations satisfy users’ needs. However, in most cases, a more
authentic approach to create movements directly from users’ actions is desir-
able. To eﬀectuate this, many researchers directly retarget motions from users
to 3D avatars.

3.2 Direct-Motion Retrageting

In this category, the motion data is directly transferred from humans to 3D
avatars. The surrounding environment can be pre-designed [26,27,28] or re-
constructed scenes [29,30]. We divide the Direct-motion retargeting strategies
into two types of Image-based methods, where the inputs are image, video, or
depth data and Sensors-based approaches where the inputs are obtained from
sensor equipment. While some of the researches in this category are directly
implemented in VCP environments, some others suggested avatar modeling
pipelines that are profoundly beneﬁcial toward creating VCP environments.

8

Matthew Korban, Xin Li

(a)

(b)

Fig. 3 Pre-rendered motions used for creating VCP environments to simulate an airport [23]
and a psychological test [25].

Sensor-based Approaches
To animate human avatars, diﬀerent sensors have been used; such as wear-
able sensors [31,32,33], head-mounted sensors [34, 35,36], and motion captur-
ing systems with tracking markers [37].

There have been diﬀerent applications for sensor-based human avatar ani-
mation generation. Some researchers utilized sensors to evaluate the eﬀective-
ness of human avatars [35,37,32]. As an example, in [35] Wang et al. adopted
HMD sensors (HTC Vive) to investigate the performance of diﬀerent seg-
ment levels of a human avatar such as partial hands, full hands and full-body
avatar. The experimental results indicated that the full-body avatar leads to
the highest performance and satisfaction when they used HMD sensors. As an-
other example, in [37] Camporesi et al. evaluated the performance of avatars
and non-avatars techniques using a motion capturing system with ten cam-
eras that can track users’ head and body based on trackable markers. The
results indicated that exploiting avatar-based categories can improve users’
quality and speed on ﬁnishing their assigned tasks. In [32] Han et al. stim-
ulated human’s upper body motions using head and hand motion capturing
sensors whose data are transferred in two diﬀerent channels. The experimental
results show an 80% consistency rate between real human actions and digital
avatar motions. In [34], Herder et al. created a VCP environment to simulate
a large factory with industrial machines. Users’ roles are as new workers and
are trained based on a basic tutorial. Participants are tracked and animated
in real-time using HMD sensors and development kits provided by HMD man-
ufacturers. The results indicated that using a human avatar helps stimulating
communication between users by having high immersive interactions and en-
gagements.

Sensors-based animated avatars also have been used to simulate social
events [33,38]. For example, in [33] Andreadis et al. suggested a VCP en-
vironment to simulate a theater containing actors and other scenery subjects
that are streamed in a multi-screen display. While the main actors’ interac-
tions are captured using a real-time motion capturing system and wearable
magnet sensors, other subjects such as animals are animated using automatic
AI-assisted motions. The eﬀectiveness of the proposed VCP environment is val-

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

9

idated by receiving positive feedback from both regular audiences and experts.
As another example, in [38]. De et al. utilized Facebook Space, a pre-designed
commercial software, to evaluate user’s interaction. Participants interact with
other human-like avatars by talking to them and listening to their conversation
while watching movies. The ﬁnal results showed that the users have similar ex-
periences of watching movies together when using 3D avatars and traditional
video-based environment.

Another application of sensor-based animated avatars is to analyze human
behavior and motions in diﬀerent scenarios [39, 36]. As an example, in [39]
Park et al. created a system to capture and analyze users’ walking-in-place
movement in VR environments. The motion sensors are installed on users’
lower legs, and the avatar’s lower leg is animated when any motion is de-
tected. They concluded that the avatar’s movement is natural and accurate.
In [36], a mixed-reality environment is designed to investigate the interaction
between users and objects. They combined the real world and virtual objects
using depth sensors and an HTC Vive Pro HMD. The experimental results in-
dicated that creating such a scenario to interact with virtual near-real objects
in a mixed-reality environment is highly viable for real-world applications. As
another instance, in [40] Maloney suggested a VR environment where users
can embody the avatar with diﬀerent races to measure their racial bias. Users
need to shoot human-like targets that are explained to be aliens invading the
earth. They concluded that the proposed strategy is successful in decreasing
users’ implicit bias against diﬀerent races.

Compared to pre-rendered animations, generating motions by using sensors
is more ﬂexible, allowing users to perform various actions. However, using sen-
sors has some limitations such as the need of proper maintenance, calibration,
training and diﬃculty of wearing or using the sensor devices.

Both of senor-based and pre-rendered motion-based approaches are more
focused on application rather than techniques. So, in Table 2 we compare these
two categories based on their applications, advantages and disadvantages.

(a)

(b)

Fig. 4 Sensor-based VCP environments created to (a) simulate a large factory [34] and (b)
watching movies [38].

10

Matthew Korban, Xin Li

Table 2 Comparison between Pre-rendered motion and Sensor-based (direct motion retar-
geting)

Category
Pre-rendered
motions

Sensor-based direct
motion retargeting

Advantages
minimal motion
transfer error, ease
of design based on
the application, no
need for
maintenance and
calibrations or
wearing extra
equipment

authentic motion
transfer directly
from users, no
limitation on
variety of motions

Disadvantages
limited number of
interactions and
lack of authentic
motion directly
transferred from
users

need for regular
maintenance,
calibration and
training and
diﬃculty of using
wearable devices

Applications
virtual conference
room [20],
simulating
metropolitan
structures:
university [22],
airport [23] and
Museum [24],
psychological test
[25]
evaluating the
eﬀectiveness of
human avatars [35,
37, 32], simulating
social events [33,
38], analyzing
human behavior
and motions [39,
36]

Image-based Approaches
Images (including RGB and depth) have been widely used as the primary
inputs for motion synthesis and retargeting [41, 42, 43,44]. As a result, images
can be utilized as the inputs to animate human avatars in a VCP environment.
In contrast to wearing/using sensors, capturing images does not require speciﬁc
maintenance, training, and calibration. Moreover, users have more freedom to
perform any action without the limitations caused by wearable devices. Image-
based strategies can be further divided into two Oﬄine and Online retargeting
approaches. In Oﬄine methods, the process of retargeting can be computa-
tionally expensive and might not be suitable for real-time applications. On
the other hand, they might oﬀer better accuracy by reconstructing human’s
body shape, pose, face, and textures. Nevertheless, Online motion retargeting
is capable of being used in real-time since they are optimized and sometimes
implemented with minimal realization [30]. We summarize the comparison be-
tween diﬀerent image-based strategies in Table 3 and then will explain each
sub-category with details as follows.
Oﬄine Motion Retargeting
Oﬄine motion retargeting is mainly used to reconstruct high-quality hu-
man body shape, pose and textures [45,46,47,48, 49, 50,51]. As an example, in
[48] Lim et al. fused several approaches such as KinectFusion [52] and ICP-
based registration algorithm to generate the 3D avatar. The proposed method
can create a human avatar with textures automatically. They enforce posi-
tional constraints to avoid motion artifacts. However, the ICP-based registra-
tion algorithm might not be suitable for highly deformable subjects such as
humans.

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

11

Table 3 Brief comparison of diﬀerent Image-based Direct motion retargeting approaches

Category
Image-based
Oﬄine motion
retargeting

Image-based
Online motion
retargeting (Rigged
avatar)

Image-based
Online motion
retargeting (3D
model
reconstruction)

Applications
non-real time
applications such
as creating
pre-rendered
avatars, repairing
avatar model
damages caused by
scanning error
VCP environments
with generic
avatars such as
workers in training
environments,
system with simple
setup

VCP environments
with avatar
resembling users:
tele-presence,
tele-conference and
meeting

Disadvantages
expensive
computational cost
and diﬃculty of
being implemented
in real-time
applications

lack of authentic
avatar that
resembles the
user’s appearance,
motion retargeting
error caused by
diﬀerent skeleton
structures
high computational
cost, complex
system design and
setup

Advantages
accurate and high
resolution
geometry
reconstruction and
texturing, accurate
oﬄine inpainting
to ﬁx the 3D model
damages
low computational
cost, simple system
setup, ﬂexibility of
selecting the
avatars based on
the application

authentic avatar
resembling users
appearance,
capability of
reconstructing the
surrounding
enviroment in
real-time

To overcome such a non-rigid deformation in human body geometry, [46],
Cui et al. suggested an approach based on a non-rigid registration algorithm
to reconstruct 3D human body geometry. First, several images are captured
using a depth camera (Microsoft Kinect). Then, multiple stages mainly includ-
ing a super-resolution-based algorithm and the Poisson mesh reconstruction
[53], are suggested to reconstruct the human body geometry and textures. The
proposed method can automatically reconstruct non-rigid subjects such as hu-
mans smoothly and with high accuracy due to adding color constraints. Nev-
ertheless, the suggested approach might be sensitive to issues such as holes or
missing regions and soft tissue deformation since they utilized low-dimensional
statistical models.

To handle the soft tissue deformation and hole ﬁlling problems, in [45] Bogo
et al. exploited both high and low dimensional models to reconstruct human
body shape and pose from RGB-D video sequences. While the low dimensional
model is used for initial pose estimation, the high dimensional model is utilized
to repose and accurately reconstruct the geometry. The reconstruction process
is done by displacement mapping between the local and global geometries. The
results showed that their method is reliable even in the challenging situations
such as varying resolution and soft tissue deformation. Still, the facial details
can not be encoded accurately because the statistical model is built upon
human body landmarks.

There have been some methods that developed strategies to encode human
facial features to be used in VR environments. These methods have been based

12

Matthew Korban, Xin Li

on recognizing Action units [54,55], motion tracking of facial landmarks [56]
and bone marker tools [57]. Yet, these approaches only render faces while most
VCP environments require full human body avatar models.

To include eﬀective facial features in full human body avatar models, in
[49] Malleson et al. designed a system to create full-body avatars replicating
the person’s body shape, face and textures. When the body shape is created
using blendshapes based on the body dimensions obtained from the depth im-
ages, the face is reconstructed using blendweights and the facial landmarks
obtained from images. The experimental results illustrated that the recon-
structed avatars look real enough for users to feel other’s real presence in a
VR environment. However, the computational cost of the whole reconstruc-
tion process is expensive (around 10 seconds). We summarize the evolution of
technology (as mentioned above) in the Image-based Oﬄine motion retarget-
ing category in Table 4.

Table 4 Gradual evolution of technology in the Image-based Oﬄine motion retargeting
approaches

Paper/Year

Cui et al. [46] 2012

Solved issue(s)
from previous
paper(s)
highly deformable
human body

non-rigid
registration
algorithm

Proposed method
to solve the issue(s)

Other used
algorithms

Bogo et al. [45]
2015

soft tissue
deformation [46]

high dimensional
models

Malleson et al. [49]
2017

lack of encoding
facial
landmarks [45]

blendweights and
facial landmarks

super-resolution-
based algorithm,
Poisson -based
mesh
reconstruction
low dimensional
model for initial
pose estimation,
displacement
mapping
blendshapes for
body shape
reconstruction

Online Motion Retargeting
Compared to Oﬄine motion retargeting, Online methods are optimized
and can be utilized in real-time. We categorize the Online motion retargeting
strategies into two types of Rigged-avatar-based motion retargeting and 3D
avatar model reconstruction. We will give more explanation as follows:

Rigged-Avatar-Based Motion Retargeting: In the Rigged-avatar-based

motion retargeting category, the main focus is on the quality of motion trans-
fer rather than quality of 3D avatar models. In this group, often pre-designed
3D avatar models depending on the application are used. As a result, they are
more suitable for VCP environments that don’t require the human avatars re-
sembling users’ appearance. They also can be implemented cheaper and faster.
For example, in [58] Jo et al. created a 3D teleconference in an Augmented
Reality (AR) environment using a generic rigged avatar model whose motion
information is obtained by the Microsoft Kinect. They preserve the spatial

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

13

property of objects that users can sit on digital chairs similarly to a real envi-
ronment. The obtained surveys indicated that the users have more impressive
and realistic experience than the traditional video-based communication ap-
proaches. Still, they evaluated their AR system based on limited motions,
while complex movements are inevitable in a real-time VCP environment.

This limitation [58], is solved in [59] and [60] by considering various motions
to have a more reliable interactive environment. Speciﬁcally, in, [59] Lugrin
et al. suggested a strategy to evaluate the impact of diﬀerent types of avatars
on the performance of ﬁtness training. They exploited the Microsoft Kinect
to capture the motion data in real-time and transfer it to human-like rigged
avatars. As another example, in [60] Choi et al. evaluated the impact of dif-
ferent types of motions on users’ feelings of body ownership and presence.
They utilized a OptiTrack Motion Capturing System [61] with six cameras for
real-time motion tracking and retargeting. The experimental results indicated
the eﬀectiveness of these approaches [59,60] for training and testing purposes,
respectively. However, the Kinect or motion capturing system cannot extract
the facial features eﬀectively because of the limited resolution of commercial
motion cameras.

To tackle the shortage of facial features [59, 60], in [62] Roth et al. designed
an immersive environment that uses a tracking system (OptiTrack sensors
[61]), an eye gaze tracking and facial expression tracking (Faceshift software)
to evaluate users’ social interactions. The proposed strategy oﬀers various
interactive features that help users’ immersion in the environment based on
a stereoscopic projected display. Still, the real-time tracking errors caused by
complex motions are not evaluated while these errors can negatively aﬀect a
high quality VCP experiment.

To reduce the artifacts caused by complex motions such as turning around
or partial view Kang et al. [63] suggested an adjustable ﬁlter integrated with
multiple Inverse Kinematics (IK) constraints. The motion information is ob-
tained from a Kinect device and is transferred to a rigged avatar using quater-
nions calculations. We summarize the aforementioned evolution of technology
in the Rigged-avatar-based Online motion retargeting group in Table 5.

Table 5 Gradual evolution of technology in the Rigged-avatar-based Online motion retar-
geting approaches

Paper/Year

Jo et al. [58] 2014

Lurgin et al.
2015
Roth et all.
2017
Kang et al.
2019

[59]

[62]

[63]

Solved issue(s)
from previous
paper
preserving spatial
properties
limited motions
[58]
lack of facial
features [59]
artifacts in
complex
motions [62]

Proposed method
to solve the issue(s)

Application

global and local
motion adaptation
advanced tracking

Faceshift software

adjustable ﬁlters

tele-conference

virtual ﬁtness
training
virtual social
gathering
virtual ﬁtness
training

14

Matthew Korban, Xin Li

Fig. 5 Rigged avatars used to simulate a 3D tele-conference [58]

3D Model Reconstruction: 3D reconstruction of avatar models has
some advantages over rigged avatars, such as generating avatars that resemble
users’ body shape, face, and reconstructing the environmental scene. In this
category, some researchers suggested approaches to reconstruct the human
3D model [26,27,28] or the whole scene [29,30]. The image-based 3D Model
reconstruction category often includes the most advanced state-of-the-art al-
gorithms for human avatar modelling.

As an example in, [26] Shapiro et al. combined several methods such as
KinectFusion [64] and Poisson surface reconstruction algorithm [53] to re-
construct the human body geometry and animation based on key-poses and
superresolution range scana. Although they can reconstruct the whole human
geometry in a few minutes, the scanning process and auto-rigging of the recon-
structed mesh are performed oﬄine. As a result, the entire motion retargeting
process can not be implemented in real-time.

To resolve the oﬄine scanning issue [26], in [29], Newcombe et al. proposed
a scene reconstruction algorithm that can reconstruct the whole scene in real-
time. The suggested approach is based on a DynamicFusion scheme consisting
of three stages of parameter estimation, fusion and structure adaptation, to
fuse the RGBD data and reconstruct the subjects. The suggested strategy
can handle reconstructing highly deforming subjects such as human body in
real-time. Nevertheless, they did not suggest any solution to add textures and
handle the tracking errors caused by complex geometry, partial view, occluded
or shadowed parts.

To resolve the tracking errors and real-time texture mapping issues, [26,
29], Orts et al. [30] developed a system called Holoportation that can han-
dle the occlusion using a temporal reconstruction algorithm. The proposed
system includes multiple RGB and infrared cameras to capture and transmit
the moving human body’s dynamic 3D geometry and the surrounding scene.

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

15

While the method can reconstruct high-quality human body 3D models with
textures, there are some drawbacks such as expensive and sophisticated setup
due using multiple RGB and depth cameras to reconstruct the whole scene.

To overcome the diﬃculty of complex setup [26], in [27] Li et al. pro-
posed a strategy to reconstructs human body model and textures from a sin-
gle RGB image. The suggested approach includes multiple stages, including
human body segmentation, ﬁtting the segmented body to a parametric model,
wrapping the initial geometry to the ﬁnal model based on a dense correspon-
dence and silhouette. They suggested a new network called InferGAN to in-
terpret the textures of invisible parts from users behind. Yet, they concluded
some limitations such as limited camera view and sensitivity to occluded body
limbs. We summarize the aforementioned evolution of technology in the 3D
model reconstruction-based online motion retargeting category in Table 6

Table 6 Gradual evolution of technology in the 3D model reconstruction-based online
motion retargeting approaches

Paper/Year

Shapiro et al.
2014

[26]

Solved issue(s)
from previous
paper
slow human
geometry
reconstruction [48]

Newcombe et
al. [29] 2015

oﬄine scanning
[26]

Orts et al.
2016

[30]

real-time tracking
errors and texture
mapping [29, 26]

Li et al

[27] 2019

expensive and
complex setup [30]

Proposed method
to solve the issue(s)

Other used
algorithms

representative
frames (key-poses)

DynamicFusion
(online-scan-
streaming)

temporal
reconstruction
algorithm and
textures
compressing
single RGB
image-based
human body
reconstruction and
texturing

Poisson Surface
Reconstruction,
superresolution
range scanning
algorithm
parameters
estimation, fusion
and structure
adaptation
dynamic 3D
geometry
reconstruction

InferGAN to
interpret the
textures of
invisible parts,
parametric model

Fig. 6 Online Image-based 3D reconstruction of avatar and scene models [30],

16

4 Conclusion

Matthew Korban, Xin Li

In this survey paper, we reviewed the methods which created and used human
avatar models toward designing VCP environments. After a short discussion
about the non-avatar strategies, we discussed the avatar-based techniques,
their advantages and disadvantages, and the gradual advancement of technol-
ogy for each method and category. We conclude that the recent advancement
in computer graphics/vision algorithms profoundly has improved the quality
of human avatar representation in VCP environments.

References

1. Saniye Tugba Bulu. Place presence, social presence, co-presence, and satisfaction in

virtual worlds. Computers & Education, 58(1):154–161, 2012.

2. Mustufa Abidi, El-Tamimi A.M., and Abdulrahman Al-Ahmari. Virtual reality: Next
generation tool for distance education. International Journal of Advanced Science and
Engineering Technology, 2:95–100, 12 2012.

3. Jessica Hooper, Eleftherios Tsiridis, James E Feng, Ran Schwarzkopf, Daniel Waren,
William J Long, Lazaros Poultsides, William Macaulay, George Papagiannakis, Eu-
stathios Kenanidis, et al. Virtual reality simulation facilitates resident training in total
hip arthroplasty: A randomized controlled trial. The Journal of arthroplasty, 2019.
4. Mona W Schmidt, Karl-Friedrich Kowalewski, Marc L Schmidt, Erica Wennberg,
Carly R Garrow, Sang Paik, Laura Benner, Marlies P Schijven, Beat P M¨uller-Stich,
and Felix Nickel. The heidelberg vr score: development and validation of a compos-
ite score for laparoscopic virtual reality training. Surgical endoscopy, 33(7):2093–2103,
2019.

5. B. Wiederhold and M. Wiederhold. Virtual Reality Therapy for Anxiety Disorders:
Advances in Evaluation and Treatment. American Psychological Association, 2005.
6. Sarah Hudson, Sheila Matson-Barkat, Nico Pallamin, and Guillaume J´egou. With or
without you? interaction and immersion in a virtual reality experience. Journal of
Business Research, 100:459–468, 2019.

7. Annica Kristoﬀersson, Silvia Coradeschi, and Amy Loutﬁ. A review of mobile robotic

telepresence. Advances in Human-Computer Interaction, 2013, 2013.

8. Irwin Hudson and Jonathan Hurter. Avatar types matter: review of avatar literature
for performance purposes. In International conference on virtual, augmented and mixed
reality, pages 14–21. Springer, 2016.

9. Takashi Minato, Shuichi Nishio, Kohei Ogawa, and Hiroshi Ishiguro. Development of
cellphone-type tele-operated android. In Proceedings of the 10th Asia Paciﬁc Confer-
ence on Computer Human Interaction, 2012.

10. Dae-Keun Yoon, Shin-Young Kim, Jai-Hi Cho, Ji-Yong Lee, Jung-Heum Kwon, Kwang-
Kyu Lee, and Bum-Jae You. Control and applications of smart projector based tele-
presence robot. In 2015 12th International Conference on Ubiquitous Robots and Am-
bient Intelligence (URAI), pages 549–550. IEEE, 2015.

11. Douglas G Macharet and Dinei A Florencio. A collaborative control system for telep-
resence robots. In 2012 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pages 5105–5111. IEEE, 2012.

12. Lynne E Parker, Daniela Rus, and Gaurav S Sukhatme. Multiple mobile robot systems.

In Springer Handbook of Robotics, pages 1335–1384. Springer, 2016.

13. Jenay M Beer and Leila Takayama. Mobile remote presence systems for older adults:
acceptance, beneﬁts, and concerns. In 2011 6th ACM/IEEE International Conference
on Human-Robot Interaction (HRI), pages 19–26. IEEE, 2011.

14. Min Kyung Lee and Leila Takayama. Now, i have a body: Uses and social norms for
mobile remote presence in the workplace. In Proceedings of the SIGCHI conference on
human factors in computing systems, pages 33–42. ACM, 2011.

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

17

15. Willow garage texai remote presence system. http://http://www.willowgarage.com/

pages/texai, 2020.

16. Sebastian Hening, Peter Cottrell, Mircea Teoderescu, Sri Kurniawan, and Pat Mantey.
Assistive living robot: a remotely controlled robot for older persons living alone.
In
Proceedings of the 6th International Conference on PErvasive Technologies Related to
Assistive Environments, pages 1–4, 2013.

17. Satoshi Noda, Yasuo Ebara, Tomoyuki Ishida, Koji Hashimoto, and Yoshitaka Shibata.
Implementation of high presence video communication system for multiple users using
tiled display environment. In 2015 IEEE 29th International Conference on Advanced
Information Networking and Applications Workshops, pages 494–499. IEEE, 2015.
18. Hiroyuki Maeda, Tomohiro Tanikawa, Jun Yamashita, Koichi Hirota, and Michitaka
Hirose. Real world video avatar: Transmission and presentation of human ﬁgure. In
IEEE Virtual Reality 2004, pages 237–238. IEEE, 2004.

19. B´eAtrice S Hasler, Peleg Tuchman, and Doron Friedman. Virtual research assistants:
Replacing human interviewers by automated avatars in virtual worlds. Computers in
Human Behavior, 29(4):1608–1616, 2013.

20. Patrick David Pazour, Andreas Janecek, and Helmut Hlavacs. Virtual reality confer-
encing. In 2018 IEEE International Conference on Artiﬁcial Intelligence and Virtual
Reality (AIVR), pages 84–91. IEEE, 2018.
21. Mixamo. https://www.mixamo.com, 2020.
22. Teresa Monahan, Gavin McArdle, and Michela Bertolotto. Virtual reality for collabo-

rative e-learning. Computers & Education, 50(4):1339–1353, 2008.

23. Lu Li and Ji Zhou. Virtual reality technology based developmental designs of
multiplayer-interaction-supporting exhibits of science museums: taking the exhibit of”
virtual experience on an aircraft carrier” in china science and technology museum as an
example. In Proceedings of the 15th ACM SIGGRAPH Conference on Virtual-Reality
Continuum and Its Applications in Industry-Volume 1, pages 409–412, 2016.

24. Bo Mu, YuHui Yang, and JianPing Zhang. Implementation of the interactive gestures of
virtual avatar based on a multi-user virtual learning environment. In 2009 International
Conference on Information Technology and Computer Science, volume 1, pages 613–
617. IEEE, 2009.

25. Cullen Brown, Ghanshyam Bhutra, Mohamed Suhail, Qinghong Xu, and Eric D Ragan.
Coordinating attention and cooperation in multi-user virtual reality narratives. In 2017
IEEE Virtual Reality (VR), pages 377–378. IEEE, 2017.

26. Ari Shapiro, Andrew Feng, Ruizhe Wang, Gerard Medioni, Mark Bolas, and Evan A
Suma. Automatic acquisition and animation of virtual avatars. In 2014 IEEE Virtual
Reality (VR), pages 185–186. IEEE, 2014.

27. Zhong Li, Lele Chen, Celong Liu, Yu Gao, Yuanzhou Ha, Chenliang Xu, Shuxue Quan,
and Yi Xu. 3d human avatar digitization from a single image. In The 17th International
Conference on Virtual-Reality Continuum and its Applications in Industry, pages 1–8,
2019.

28. Stephan Beck, Andre Kunert, Alexander Kulik, and Bernd Froehlich. Immersive group-
to-group telepresence. IEEE Transactions on Visualization and Computer Graphics,
19(4):616–625, 2013.

29. Richard A Newcombe, Dieter Fox, and Steven M Seitz. Dynamicfusion: Reconstruction
and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 343–352, 2015.

30. Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kow-
dle, Yury Degtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou,
et al. Holoportation: Virtual 3d teleportation in real-time. In Proceedings of the 29th
Annual Symposium on User Interface Software and Technology, pages 741–754. ACM,
2016.

31. Masoud Zadghorban Lifkooee, Celong Liu, Yongqing Liang, Yimin Zhu, and Xin Li.
Real-time avatar pose transfer and motion generation using locally encoded laplacian
oﬀsets. Journal of Computer Science and Technology, 34(2):256–271, 2019.

32. Dustin T Han, Shyam Prathish Sargunam, and Eric D Ragan. Simulating anthropo-
morphic upper body actions in virtual reality using head and hand motion data. In
2017 IEEE Virtual Reality (VR), pages 387–388. IEEE, 2017.

18

Matthew Korban, Xin Li

33. Anthousis Andreadis, Alexander Hemery, Andronikos Antonakakis, Gabriel Gour-
doglou, Pavlos Mauridis, Dimitrios Christopoulos, and John N Karigiannis. Real-time
motion capture technology on a live theatrical performance with computer generated
scenery. In 2010 14th Panhellenic Conference on Informatics, pages 148–152. IEEE,
2010.

34. Jens Herder, Nico Brettschneider, Jeroen De Mooij, and Bektur Ryskeldiev. Avatars for
co-located collaborations in hmo-based virtual environments. In 2019 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), pages 968–969. IEEE, 2019.

35. Tzu-Yang Wang, Yuji Sato, Mai Otsuki, Hideaki Kuzuoka, and Yusuke Suzuki. Eﬀect of
full body avatar in augmented reality remote collaboration. In 2019 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), pages 1221–1222. IEEE, 2019.

36. Michael Rauter, Christoph Abseher, and Markus Safar. Augmenting virtual reality
In 2019 IEEE Conference on Virtual Reality and 3D

with near real world objects.
User Interfaces (VR), pages 1134–1135. IEEE, 2019.

37. Carlo Camporesi and Marcelo Kallmann. The eﬀects of avatars, stereo vision and
display size on reaching and motion reproduction. IEEE transactions on visualization
and computer graphics, 22(5):1592–1604, 2015.

38. Francesca De Simone, Jie Li, Henrique Galvan Debarba, Abdallah El Ali, Simon NB
Gunkel, and Pablo Cesar. Watching videos together in social virtual reality: An experi-
mental study on user’s qoe. In 2019 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR), pages 890–891. IEEE, 2019.

39. Chanho Park and Kyungho Jang.

walking-in-place navigation system in virtual reality.
Virtual Reality and 3D User Interfaces (VR), pages 1114–1115. IEEE, 2019.

Investigation of visual self-representation for a
In 2019 IEEE Conference on

40. Divine Maloney. [dc] embodied virtual avatars and potential negative eﬀects on implicit
racial bias. In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),
pages 1373–1374. IEEE, 2019.

41. Sen Wang, Xinxin Zuo, Runxiao Wang, and Ruigang Yang. A generative human-robot
motion retargeting approach using a single rgbd sensor. IEEE Access, 7:51499–51512,
2019.

42. Zhuoqian Yang, Wentao Zhu, Wayne Wu, Chen Qian, Qiang Zhou, Bolei Zhou, and
Chen Change Loy. Transmomo: Invariance-driven unsupervised video motion retarget-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5306–5315, 2020.

43. Masoud Zadghorban Lifkooee, Celong Liu, Maoqing Li, and Xin Li. Image-based human
In 2018
character modeling and reconstruction for virtual reality exposure therapy.
13th International Conference on Computer Science & Education (ICCSE), pages 1–5.
IEEE, 2018.

44. Carl Doersch and Andrew Zisserman. Sim2real transfer learning for 3d human pose es-
timation: motion to the rescue. In Advances in Neural Information Processing Systems,
pages 12949–12961, 2019.

45. Federica Bogo, Michael J Black, Matthew Loper, and Javier Romero. Detailed full-body
reconstructions of moving people from monocular rgb-d sequences. In Proceedings of
the IEEE International Conference on Computer Vision, pages 2300–2308, 2015.
46. Yan Cui, Will Chang, Tobias N¨oll, and Didier Stricker. Kinectavatar: fully automatic
body capture using a single kinect. In Asian Conference on Computer Vision, pages
133–147. Springer, 2012.

47. Matthew Korban, Xin Li, Kehua Miao, and Yimin Zhu. 3d human body inpainting using
intrinsic statistical shape models. In 2019 14th International Conference on Computer
Science & Education (ICCSE), pages 1105–1110. IEEE, 2019.

48. Hwasup Lim, Junseok Kang, and Sang Chul Ahn. Rapid 3d avatar creation system
using a single depth camera. In 2019 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR), pages 1329–1330. IEEE, 2019.

49. Charles Malleson, Maggie Kosek, Martin Klaudiny, Ivan Huerta, Jean-Charles Bazin,
Alexander Sorkine-Hornung, Mark Mine, and Kenny Mitchell. Rapid one-shot acquisi-
tion of dynamic vr avatars. In 2017 IEEE Virtual Reality (VR), pages 131–140. IEEE,
2017.

A Survey on Applications of Digital Human Avatars toward Virtual Co-presence

19

50. Ari Shapiro, Andrew Feng, Ruizhe Wang, Hao Li, Mark Bolas, Gerard Medioni, and
Evan Suma. Rapid avatar capture and simulation using commodity depth sensors.
Computer Animation and Virtual Worlds, 25(3-4):201–211, 2014.

51. Qing Zhang, Bo Fu, Mao Ye, and Ruigang Yang. Quality dynamic human body mod-
eling using a single low-cost depth camera. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 676–683, 2014.

52. Diana Pagliari, Fabio Menna, R Roncella, Fabio Remondino, and Livio Pinto. Kinect
fusion improvement using depth camera calibration. The International Archives of
Photogrammetry, Remote Sensing and Spatial Information Sciences, 40(5):479, 2014.
53. Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction.
In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7,
2006.

54. Miguel ´A Vicente-Querol, Arturo S Garc´ıa, Patricia Fern´andez-Sotos, Roberto
Rodriguez-Jimenez, and Antonio Fern´andez-Caballero. Development and validation
of basic virtual human facial emotion expressions. In International Work-Conference
on the Interplay Between Natural and Artiﬁcial Computation, pages 222–231. Springer,
2019.

55. Masoud Z Lifkooee, ¨Omer M Soysal, and Kazim Sekeroglu. Video mining for facial
action unit classiﬁcation using statistical spatial–temporal feature image and log deep
convolutional neural network. Machine Vision and Applications, 30(1):41–57, 2019.
56. Lorena C Kegel, Peter Brugger, Sascha Fr¨uhholz, Thomas Grunwald, Peter Hilﬁker,
Oona Kohnen, Miriam L Loertscher, Dieter Mersch, Anton Rey, Teresa Sollfrank, et al.
Dynamic human and avatar facial expressions elicit diﬀerential brain responses. Social
cognitive and aﬀective neuroscience, 15(3):303–317, 2020.

57. Kevin El Haddad, Francois Zajega, and Thierry Dutoit. An open-source avatar for
real-time human-agent interaction applications. In 2019 8th International Conference
on Aﬀective Computing and Intelligent Interaction Workshops and Demos (ACIIW),
pages 79–80. IEEE, 2019.

58. Dongsik Jo, Ki-Hong Kim, and Gerard J Kim. Avatar motion adaptation for ar based 3d
tele-conference. In 2014 International Workshop on Collaborative Virtual Environments
(3DCVE), pages 1–4. IEEE, 2014.

59. Jean-Luc Lugrin, Maximilian Landeck, and Marc Erich Latoschik. Avatar embodiment
realism and virtual ﬁtness training. In 2015 IEEE Virtual Reality (VR), pages 225–226.
IEEE, 2015.

60. Changyeol Choi, Joohee Jun, Jiwoong Heo, and Kwanguk Kenny Kim. Eﬀects of virtual-
In Proceedings of the 34th

avatar motion-synchrony levels on full-body interaction.
ACM/SIGAPP Symposium on Applied Computing, pages 701–708. ACM, 2019.

61. Optitrack motion capturing system. https://optitrack.com, 2020.
62. Daniel Roth, Kristoﬀer Waldow, Marc Erich Latoschik, Arnulph Fuhrmann, and Gary
Bente. Socially immersive avatar-based communication. In 2017 IEEE Virtual Reality
(VR), pages 259–260. IEEE, 2017.

63. Ning Kang, Junxuan Bai, Junjun Pan, and Hong Qin. Real-time animation and motion
retargeting of virtual characters based on single rgb-d camera. In 2019 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), pages 1006–1007. IEEE, 2019.

64. Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim,
Andrew J Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew W
Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In ISMAR,
volume 11, pages 127–136, 2011.

