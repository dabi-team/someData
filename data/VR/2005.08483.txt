Improving Reverse k Nearest Neighbors Queries

Lixin Ye
School of Computer Science, China University of Geosciences

2
2
0
2

t
c
O
5

]

B
D
.
s
c
[

2
v
3
8
4
8
0
.
5
0
0
2
:
v
i
X
r
a

Abstract—The reverse k nearest neighbor query ﬁnds all points
that have the query point as one of their k nearest neighbors,
where the kNN query ﬁnds the k closest points to its query
point. Based on conics, we propose an efﬁcent RkNN veriﬁcation
method. By using the proposed veriﬁcation method, we imple-
ment an efﬁcient RkNN algorithm on VoR-tree, which has a
computational complexity of O(k1.5 · log k). The comparative
experiments are conducted between our algorithm and other
two state-of-the-art RkNN algorithms. The experimental results
indicate that the efﬁciency of our algorithm is signiﬁcantly higher
than its competitors.

Index Terms—RkNN, conic section, Voronoi, Delaunay

I. INTRODUCTION
As a variant of nearest neighbor (NN) query, RNN query
is ﬁrst introduced by Korn and Muthukrishnan [1]. A direct
generalization of NN query is the reverse k nearest neighbors
(RkNN) query, where all points having the query point as
one of their k closest points are required to be found. Since
its appearance, RkNN has received extensive attention [2],
[3], [4], [5], [6], [7] and been prominent in various scientiﬁc
ﬁelds including machine learning, decision support, intelligent
computation and geographic information systems, etc.

At ﬁrst glance, RkNN and kNN queries appear to be
the results for RkNN and kNN
equivalent, meaning that
may be the same for the same query point. However, RkNN
is not as simple as it seems to be. It is a very different
kind of query from kNN, although their results are similar
in many cases. So far, RkNN is still an expensive query
for its computational complexity at O(k2) [6], whereas the
computational complexity of kNN queries has been reduced
to O(k · log k) [7].

In order to solve the RNN/RkNN problem, a large number
of approaches have been proposed. Some early methods [8],
[1], [9] speed up RNN/RkNN queries by pre-computation.
Their disadvantage is that it is difﬁcult to support queries on
dynamic data sets. Therefore, many RkNN algorithms without
pre-computation are proposed.

Most existing non-pre-computation RkNN algorithms have
two phases: the ﬁltering phase and the reﬁning phase (also
known as the pruning phase and the veriﬁcation phase). In the
pruning phase, the majority of points that do not belong to
RkNN should be ﬁltered out. The main goal of this phase is to
generate a candidate set as small as possible. In the veriﬁcation
phase, each candidate point should be veriﬁed whether it
belongs to the RkNN set or not. For most algorithms, the
candidate points are veriﬁed by issuing kNN queries or range
queries, which are very computational expensive. The state-
of-the-art RkNN technique SLICE, provides a more efﬁcient
veriﬁcation method with a computational complexity of O(k)

for one candidate. The size of the candidate set of SLICE
varies form 2k to 3.1k. However, it is still time consuming to
perform such a veriﬁcation for each candidate point.

There seems to be a consensus in the past studies that for an
RkNN technique, the number of veriﬁcation points cannot be
smaller than the size of the result set. Such an idea, however,
limits our understanding of the RkNN problem. Hence we
amend our thought and come up with a conjecture that whether
a point could be directly determined as belonging to the RkNN
set according to its location. Given the query point q, our
intuition tells us that if a point p is closer to q than a point p+
belonging to the RkNNs of q, then p is highly likely to also
belong to the RkNN of q. Conversely, if p is further away
from q than a point p− that does not belong to the RkNN
set of q, then p is probably not a member of the RkNN set.
Along with this idea, we further study and obtain a set of
veriﬁcation methods for RkNN queries. Based VoR-tree, we
use this veiriﬁcation method implement an efﬁcient RkNN
algorithm, which out performs most mainstream algorithms.

TABLE I
COMPARISON OF COMPUTATIONAL COMPLEXITY

Operation

VR-RkNN

SLICE

Our approach

Generate candidates
Verify a candidate

|Veriﬁed candidates|

Overall

O(k · log k)
O(k · log k)
O(k)
(=6k)
O(k2 · log k)

O(k · log k)
O(k)
O(k)
(2k∼3.1k)
O(k2)

O(k · log k)
O(k · log k)
√

O(
(≤ 7.1

k)
√

k)

O(k1.5 · log k)

Table I shows the comparison of computational complexity
among VR-RkNN , SLICE and our approach. It can be seen
the bottleneck of both VR-RkNN and SLICE is the
that
veriﬁcation phase. The computational complexity of verifying
a candidate of our approach is O(k·log k), which is higher than
that of SLICE. However, the number of candidates veriﬁed by
√
k, which is much less than that
our approach is only about 7.1
of SLICE. In addition, the overall computational complexity
of our approach is much lower than that of SLICE.

The rest of the paper is organized as follows. In Section 2,
we introduce the major related work of RkNN since its
appearance. In Section 3, we formally deﬁne the RkNN
problem and introduce the concepts and knowledge related to
our approach. Our approach and its principles are described
in section 4. Section 5 provides a detailed theoretical analysis.
Experimental evaluation is demonstrated in Section 6. The last
two sections are conclusions and acknowledgements.

 
 
 
 
 
 
II. RELATED WORK

A. RNN-tree

Reverse nearest neighbor (RNN) queries are ﬁrst introduced
by Korn and Muthukrishnan where RNN queries are imple-
mented by preprocessing the data [1]. For each point p in
the database, a circle with p as the center and the distance
from p to its nearest neighbor as the radius is pre-calculated
and these circles are indexed by an R-tree. The RNN set of
a query point q includes all the points whose circle contains
q. With the R-tree, the RNN set of any query point can be
found efﬁciently. Soon after, several techniques [10], [11] are
proposed to improve their work.

B. Six-regions

Six-regions [2] algorithm, proposed by Stanoi et al., is the
ﬁrst approach that does not need any pre-computation. They
divide the space into six equal segments using six rays starting
at the query point, so that the angle between the two boundary
rays of each segment is 60◦. They suggest that only the nearest
neighbor (NN) of the query point in each of the six segments
may belong to the RNN set. It ﬁrstly performs six NN queries
to ﬁnd the closest point of the query point q in each segments.
Then it launches an NN query for each of the six points to
verify q as their NN. Finally the RNN of q is obtained.

Generalizing this theory to RkNN queries leads to a corol-
lary that, only the members of kNN of the query point in
each segment have the possibility of belonging to the RkNN
set. This corollary is widely adopted in the pruning phase of
several RkNN techniques.

C. TPL

TPL [3], proposed by Tao et al., is one of the prestigious
algorithms for RkNN queries. This technique prunes the space
using the bisectors between the query point and other points.
The perpendicular bisector is denoted by Bp:q. Bp:q is between
a point p and the query point q. Bp:q divides the space into two
half-spaces. The half-space that contains p is denoted as Hp:q.
Another one is denoted as Hq:p. If a point p(cid:48) lies in Hp:q, p(cid:48)
must be closer to p than to q. Then p(cid:48) cannot be the RNN of
q and we can say that p prunes p(cid:48). If a point is pruned by at
least k other points, then it cannot belong to the RkNN of q.
An area that is the intersection of any combination of k half-
spaces can be pruned. The total pruned area corresponds to
the union of pruned regions by all such possible combinations

of k bisectors (total

combinations). TPL also uses an

alternative computational cheaper pruning method which has
a less pruning power. All the points are sorted by their Hilbert
values. Only the combinations of k consecutive points are used
to prune the space (total m combinations).

(cid:19)

(cid:18)m
k

D. FINCH

FINCH is another famous RkNN algorithm proposed by
Wu et al. [4]. The authors of FINCH think that it is too
computational costly to use m combinations of k bisectors

to prune the points. They utilize a convex polygon that ap-
proximates the unpruned region to prune the points instead of
using bisectors. All points lying outside the polygon should be
pruned. Since the containment can be achieved in logarithmic
time for convex polygons, the pruning of FINCH has a higher
efﬁciency than TPL. However, the computational complexity
of computing the approximately unpruned convex polygon
is O(m3), where m is the number of points considered for
pruning.

E. InfZone

Previous techniques can reduce the candidate set

to an
extent by different pruning methods. However, their veriﬁca-
tion methods for candidates are very inefﬁcient. It is quite
computational costly to issue an inefﬁcient veriﬁcation for
each point in a candidate set with a size of O(k). In order to
overcome this issue, a novel RkNN technique which is named
as InfZone is proposed by Cheema et al. [5]. The authors of
InfZone introduce the concept of inﬂuence zone (denoted as
Zk), which also can be called RkNN region. The inﬂuence
zone of a query point q is a region that, a point p belongs to
the RkNN set of q, if and only if it lies in the Zk of q. The
inﬂuence zone is always a star-shaped polygon and the query
point is its kernel point. A number of properties are detailed.
These properties are aimed to shrink the number of points
which are crucial to compute the inﬂuence zone. They propose
an inﬂuence zone computing algorithm with a computational
complexity of O(k · m2), where m is the number of points
accessed during the construction of the inﬂuence zone. Every
points that
lies inside the inﬂuence zone are accessed in
the pruning phase, since they cannot be ignored during the
construction of the inﬂuence zone. Namely, all the potential
members of the RkNN are accessed during the pruning phase.
Hence, for monochromatic RkNN queries, InfZone does not
require to verify the candidates. It is indicated that the expected
size of RkNN set is k. Evidently, the size of RkNN must not
be greater than m, i.e., k ≤ m. Therefore, the computational
complexity of InfZone must be no less than O(k3).

F. SLICE

SLICE [6]

is the state-of-the-art approach for RkNN
queries. In recent years, several well-known techniques [2]
have been proposed to address the limitations of half-space
pruning[3] (e.g., FINCH [4], InfZone [5]). While few re-
searcher carries out further research based on the idea of Six-
regions. Yang et al. suggests that the regions-based pruning
approach of Six-regions has great potential and proposed an
efﬁcient RkNN algorithm SLICE [6]. SLICE uses a more
powerful and ﬂexible pruning approach that prunes a much
larger area as compared to Six-regions with almost similar
computational complexity. Furthermore, it signiﬁcantly im-
proves the veriﬁcation phase by computing a list of signiﬁcant
points for each segment. These lists are named as sigLists.
Each candidate can be veriﬁed by accessing sigList instead of
issuing a range query. Therefore, SLICE is signiﬁcantly more
efﬁcient than the other existing algorithms.

G. VR-RkNN

For most RkNN algorithms, data points are indexed by R-
tree [12]. However, R-tree is originally designed primarily
for range queries. Although some approaches [13], [3], [14],
[15] are proposed afterwards to make it also suitable for NN
queries and their variants: the NN derived queries are still
disadvantageous. When answering an NN derived query, all
nodes in the R-tree intersecting with the local neighborhood
(Search Region) of the query point need to be accessed to ﬁnd
all the members of the result set. Once the candidate set of
the query is large, the cost of accessing the nodes can also
become very large. In order to improve the performance of R-
tree on NN derived queries, Sharifzadeh and Shahabi proposes
a composite index structure composed of an R-tree and a
Voronoi diagram, and named it as VoR-Tree [7]. VoR-Tree
beneﬁts from both the neighborhood exploration capability
of Voronoi diagrams and the hierarchical structure of R-tree.
By utilizing VoR-tree, they propose VR-RkNN to answer the
RkNN query. Similar to the ﬁlter phase of Six-regions [2], Vor-
RkNN divides the space into 6 equal segments and selects k
candidate points from each segment to form a candidate set
of size 6k. During the reﬁning phase, each candidate point is
veriﬁed to be a member of the RkNN through issuing a kNN
query (VR-kNN). The expected computational complexity of
VR-RkNN is O(k2 · log k)

III. PRELIMINARIES

A. Problem deﬁnition

Deﬁnition 1. Euclidean Distance: Given two points A =
{a1, a2, ..., ad} and B = {b1, b2, ..., bd} in Rd, the Euclidean
distance between A and B, dist(A, B), is deﬁned as follows:
(cid:118)
(cid:117)
(cid:117)
(cid:116)

dist(A, B) =

(ai − bi)2.

d
(cid:88)

(1)

i=1

Deﬁnition 2. kNN Queries: A kNN query is to ﬁnd the
k closest points to the query point from a certain point set.
Mathematically, this query in Euclidean space can be stated
as follows. Given a set P of points in Rd and a query point
q ∈ Rd,

kNN(q) = {p ∈ P | dist(p, q) ≤ dist(pk, q)}
where pk is the kth closest point to q in P.

(2)

Deﬁnition 3. RkNN Queries: A RkNN query retrieves all
the points that have the query point as one of their k nearest
neighbors from a certain point set. Formally, given a set P of
points in Rd and a query point q ∈ P , the RkNN of q in P
can be deﬁned as

Fig. 1. a) Voronoi Diagram, b) Delaunay Graph

into n regions corresponding to these points, which are called
Voronoi cells. For each of these n points, the corresponding
Voronoi cell consists of all locations closer to that point than
to any other. In other words, each point is the nearest neighbor
of all the locations in its corresponding Voronoi cell. Formally,
the above description can be stated as follows.

Deﬁnition 4. Voronoi cell & Voronoi diagram: Given a set
P of n points, the Voronoi cell of a point p ∈ P , denoted as
V (P, p) or V (p) for short, is deﬁned as Equation (4)

V (P, p) = {q | ∀p(cid:48) ∈ P \ {p} : dist(p, q) ≤ dist(p(cid:48), q)}

(4)

and the Voronoi diagram of P , denoted as V D(P ), is deﬁned
as Equation (5).

V D(P ) = {V (P, p) | p ∈ P }

(5)

The Voronoi diagram of a certain set P of points, V D(P ), is
unique.

Deﬁnition 5. Voronoi neighbor: Given the Voronoi diagram
of P , for a point p, its Voronoi neighbors are the points in P
whose Voronoi cells share an edge with V (P, q). It is denoted
as V N (P, q) or V N (q) for short. Note that the nearest point
in P to p is among V N (q).

Lemma 1. Let pk be the k-th nearest neighbor of q, then pk
is a Voronoi neighbor of at least one point of the k − 1 nearest
neighbors of q (where k > 1).

Proof. See [7].

Lemma 2. For a Voronoi diagram, the expected number of
Voronoi neighbors of a generator point does not exceed 6.

Proof. Let n, ne and nv be the number of generator points,
Voronoi edges and Voronoi vertices of a Voronoi diagram in
R2, respectively, and assume n ≥ 3. According to Euler’s
formula,

RkNN(q) = {p ∈ P | q ∈ kNN(p)}.

(3)

n + nv − ne = 1

(6)

B. Voronoi diagram & Delaunay graph

Voronoi diagram [16], proposed by Rene Descartes in 1644,
is a spatial partition structure widely applied in many science
domains, especially spatial database and computational geom-
etry. In a Voronoi diagram of n points, the space is divided

Every Voronoi vertex has at least 3 Voronoi edges and each
Voronoi edge belongs to two Voronoi vertices. Hence the
number of Voronoi edges is not less than 3(nv + 1)/2, i.e.,
3
2

(nv + 1)

ne ≥

(7)

According to Equation (6) and Equation (7), the following
relationships holds:

ne ≤ 3n − 6

(8)

When the number of generator points is large enough, the
average number of Voronoi edges per Voronoi cell of a Voronoi
diagram in Rd is a constant value depending only on d. When
d = 2, every Voronoi edge is shared by two Voronoi Cells.
Hence the average number of Voronoi edges per Voronoi cell
does not exceed 6, i.e., 2 · ne/n ≤ 2(3n − 6)/n = 6 − 12/n ≤
6.

For set of points P , a dual graph of its Voronoi Diagram is
the Delaunay graph (denoted as DG(P )) [17] of it. For P , its
nearest neighbor graph is a subgraph of its Delaunay graph.

Deﬁnition 6. Delaunay graph distance: Given the Delaunay
graph DG(P ),
the Delaunay graph distance between two
vertices p and p(cid:48) of DG(P ) is the minimum number of edges
connecting p and p(cid:48) in DG(P ). It is denoted as distDG(p, p(cid:48)).

pk represents the kth closest point to q. This region is denoted
as RGkNN(q). The radius of RGkNN(q) is called the kNN
radius of q and is denoted as rq.

Note that a point p must be one kNN(q) if it

lies in
RGkNN(q), i.e., the kNN region of q. Conversely, if a point
p(cid:48) lies out of RGkNN(q), it cannot be any one of kNN(q). In
Figure 2, q is the query point and the gray region within the
circle centered on q represents RGkNN(q). As we can see, p1,
p2 and p3 lie inside RGkNN(q), then we can determine that
they belong to kNN(q). while p4 and p5 lie outside. So they
are not the members of kNN(q).

Lemma 4. Given a query point q, a point p must be one of
RkNN(q) if it satisﬁes

dist(p, q) ≤ rp.

(11)

Conversely, a point p(cid:48) cannot be any one of RkNN(q) if it
satisﬁes

dist(p(cid:48), q) > rp(cid:48).

(12)

Lemma 3. Given the query point q, if a point p belongs to
RkNN(q), then we have distDG(p, q) ≤ k in Delaunay graph
DG(p).

Simply, for a point p, if the query point q lies in its kNN region,
p must be one of RkNN(q), otherwise it must not belong to
RkNN(q).

Proof. See [7].

C. Conic section

Deﬁnition 7. Ellipse: An ellipse is a closed curve on a plane,
such that the sum of the distances from any point on the curve
to two ﬁxed points p1 and p2 is a constant C. Formally, it is
denoted as Ec

p1:p2 deﬁned as follows:

Ec

p1:p2

= {p | dist(p, p1) + dist(p, p2) = C}

(9)

Deﬁnition 8. Hyperbola: A hyperbola is a geometric ﬁgure
such that the difference between the distances from any point
on the ﬁgure to two ﬁxed points p1 and p2 is a constant C.
Formally, it is denoted as H c

p1:p2 deﬁned as follows:

H c

p1:p2

= {p | |dist(p, p1) − dist(p, p2)| = C}

(10)

IV. METHODOLOGIES

A. Veriﬁcation approach

Fig. 2. kNN region

Deﬁnition 9. kNN region: Given a query point q, the kNN
region of q is the inner region of Cq:dist(q,pk), i.e., the circle
with q as center and dist(q, pk) as the length of radius, where

Proof. The lemma is easily proved by the deﬁnition of kNN
and RkNN, see Equation (2) and Equation (3).

According to Lemma 4, we can determine whether a point
p belongs to the RkNN of the query point q by calculating
the kNN region of p. Obviously, q lying in RGkNN(p) is a
necessary and sufﬁcient condition for p to be one of RkNN(q).
In the reﬁning phase of some RkNN algorithms, the candidates
are veriﬁed by this condition. In this veriﬁcation method,
kNN region is required, so a kNN query must be conducted.
The computational complexity of the state-of-the-art kNN
algorithm is O(k · log k). Thus, the computational complexity
of the veriﬁcation method based on Lemma 4 is O(k · log k).
For most RkNN algorithms, the size of candidate set is often
several times much as that of the result set. Therefore, issuing
a RkNN veriﬁcation of which the computational complexity
is O(k · log k) for each candidate is obviously expensive. In
order to reduce the computational cost of the reﬁning phase of
RkNN queries, we introduce several more efﬁcient veriﬁcation
approaches in the following.

Lemma 5. Given a query point q and a point p+ ∈ RkNN(q),
a point p must be one of RkNN(q) if it satisﬁes

dist(p, q) + dist(p, p+) ≤ rp+.

(13)

Proof. As shown in Figure 3, the larger circle takes p+ as
the center and rp+ as the radius, which represents the kNN
region of p+. Lp×,p+ is a line segment passing through the
point p with a length of rp+. The smaller circle takes p as
the center and dist(p, p×) as the radius. Let p(cid:48) be an arbitrary
point inside Cp:dist(p,p×), then it must satisfy that

dist(p, p(cid:48)) ≤ dist(p, p×).

(14)

Fig. 3. Lemma 5

Fig. 4. Positive determine region

According to the triangle inequality, we can obtain

From the triangle inequality, it can be shown that

dist(p(cid:48), p+) ≤ dist(p, p(cid:48)) + dist(p, p+).

(15)

dist(p(cid:48), q) + dist(p(cid:48), p) ≥ dist(p, q).

(19)

Combining Inequality (14) and Inequality (15), we can obtain

If p /∈ RkNN(q), i.e., dist(p, q) > rp,

dist(p(cid:48), p+) ≤ dist(p, p×) + dist(p, p+)
= dist(p×, p+)
= rp+.

(16)

From above, we can construct a corollary that any point lying
in Cp:dist(p,p×) must belong to kNN(p+). Speciﬁcally, the
number of points lying in Cp:dist(p,p×) must not be greater
than k, i.e., the size of kNN(p+). Equivalently, there is no
more than k points closer to p than p×. Thus, pk (the kth
to p) cannot be closer than p× to p. Then
closest point
dist(p, p×) ≤ dist(p, pk) = rp. Suppose Inequality (13)
holds,

dist(p, q) ≤ rp+ − dist(p, p+)

= dist(p×, p+) − dist(p, p+)
= dist(p, p×) ≤ rp.

(17)

From Lemma 4 and Inequality (17), we can deduce that p ∈
RkNN(q). Therefore Lemma 5 proved to be true.

Lemma 5 provides a sufﬁcient but unnecessary condition
for determining that a point belongs to RkNN(q), where q
represents the query point. That means if a point p satisﬁes
the condition of Inequality (13),
it can be determined as
one of RkNN(q) without issuing a kNN query. In the case
that rp+ is known, we can verify whether Inequality (13)
holds by only calculating the Euclidean distance from p to
q and p+ respectively. Calculating the Euclidean distance
between two points can be regarded as an atomic operation.
Hence the computational complexity of the veriﬁcation method
corresponding to Lemma 5 is O(1).

Deﬁnition 10. Positive determine region: Given the query
point q and a point p, the positive determine region of p is the
internal region of Erp
det(p)
and is deﬁned as follows:

p:q. Formally, it is denoted as RG+

RG+

disc(p) = {p(cid:48) | dist(p(cid:48), q) + dist(p(cid:48), p) ≤ rp}.

(18)

dist(p(cid:48), q) + dist(p(cid:48), p) > rp

(20)

det(p) = ∅. Therefore, if RG+

then RG+
det(p) (cid:54)= ∅, p must
belong to RkNN(q). In consequence, from Lemma 5, we can
construct a corollary that, for any point p, if RG+
det(p) is not
empty, all the points lying inside of RG+
det(p) must belong to
RkNN(q).

As shown in Figure 4, q represents the query point, the
internal region of the circle Cp:rp indicates RGkNN(p), and
the gray region within the ellipse Erp
det(p). As
p1 and p2 lies in RG+
det(p), we can know p1, p2 ∈ RkNN(q).
Whereas p3, p4 and p5 lie out of RG+
det(p), so we cannot
directly determine whether or not they belong to RkNN(q) by
Lemma 5.

p:q is for RG+

Lemma 6. Given a query point q and a point p− /∈ RkNN(q),
a point p cannot be any one of RkNN(q) if it satisﬁes

dist(p, q) − dist(p, p−) > rp− .

(21)

Fig. 5. Lemma 6

Proof. As shown in Figure 5, the smaller circle takes p− as
the center and rp− as the radius, which represents the kNN
region of p−. The point p× is the intersection of an extension
of Lp,p− (a line segment between p and p−) with Cp−:rp− .
The larger circle takes p as the center and dist(p, p×) as the

radius. Let p(cid:48) be an arbitrary point inside of Cp−:rp−, then it
must satisfy that

For an arbitrary point p(cid:48),from the triangle inequality in

(cid:52)pqp(cid:48), it can be known that

dist(p−, p(cid:48)) ≤ dist(p−, p×) = dist(p−, p×).

(22)

dist(p(cid:48), p) + dist(p, q) ≥ dist(p(cid:48), q).

(27)

According to the triangle inequality, we can obtain

If p ∈ RkNN(q), i.e., dist(p, q) ≤ rp,

dist(p, p(cid:48)) ≤ dist(p, p−) + dist(p−, p(cid:48)).

(23)

From Inequality.(22) and Inequality.(23), we can get that

dist(p, p(cid:48)) ≤ dist(p, p−) + dist(p−, p×)

= dist(p, p×)
= rp.

(24)

Then we realize that all the points lying in RGkNN(p−) must
lie inside Cp:dist(p,p×), namely the number of points lying
inside of Cp:dist(p,p×) must be no less than k, i.e., the number
of points lying in RGkNN(p−). That is to say, there exist at
least k points no further than p× away from p. Equivalently,
dist(p, p×) ≥ dist(p, pk) = rp (where pk represents the
kth closest point to p). If the condition of Inequality (21) is
satisﬁed,

dist(p, q) > dist(p, p−) + rp−

= dist(p, p−) + dist(p−, p×)
= dist(p, p×) ≥ rp.

(25)

From Lemma 4 and Inequality (25), we can deduce that p /∈
RkNN(q). Therefore, Lemma 6 proved to be true.

From Lemma 6, we can know that, if a point is determined
not to be one of RkNN(q) and its kNN radius is known,
then there may exist some other points that can be sufﬁciently
determined to belong to RkNN(q) without performing a kNN
query but by performing two times of simple Euclidean
distance calculation. That means the computational complexity
of the veriﬁcation method based on Lemma 6 is O(1).

dist(p(cid:48), q) − dist(p(cid:48), p) ≤ dist(p, q) ≤ rp
det(p) = ∅. Therefore, if RG−

then RG−
det(p) is not empty, p
must belong to RkNN(q). Hence from Lemma 6, we can draw
such a corollary that, for an arbitrary point p, if RG−
det(p) is
not empty, any point lying inside RG−
det(p) cannot belong to
RkNN(q).

(28)

As shown in Figure 6, q represents the query point, the
region within the circle centered on p represents RGkNN(p),
and the gray region separated by the hyperbola H rp
p:q on the
right represents RG−
det(p). As in the ﬁgure, p1 and p2 lie inside
RG−
det(p), while p3 and p4 do not. Then we can determine that
p1 and p2 must not belong to RkNN(q), whereas we cannot
tell by Lemma 6 whether p3 or p4 belongs to RkNN(q) or not.

Deﬁnition 12. Positive/Negative determine point: Given the
query point q and two other points p and p(cid:48), if p(cid:48) lies in
RG+
det(p), we claim that p is a positive determine point of p(cid:48)
and p can positive determine p(cid:48). It is denoted as p +det−−−→ p(cid:48).
Similarity, if p(cid:48) lies in RG−
det(p), we name that p is a negative
determine point of p(cid:48) and p can negative determine p(cid:48). It is
denoted as p −det−−−→ p(cid:48). If not speciﬁed, both of these two types
of points may be collectively referred to as determine points
and we can use p det−−→ p(cid:48) to express that p can dedermine p(cid:48).

Whether a point belongs to the RkNN set of the query point
or not, the corresponding veriﬁcation method with low com-
putational complexity is provided. However, when performing
the veriﬁcation of Lemma 5 or Lemma 6, the distance from
the point to be determined to the query point and the posi-
tive/negative determine point should be calculated respectively.
In order to further improve the veriﬁcation efﬁciency of some
points, we propose Lemma 7.

Lemma 7. Given a query point q, a point p must be one of
RkNN(q) if it satisﬁes

dist(p, q) ≤ rq/2.

Fig. 6. Negative determine region

Deﬁnition 11. Negative determine region: Given the query
point q and a point p, H rp
p:q divides the space into three regions
of which the one contains p is the negative determine region of
p. Formally, this region is denoted as RG−
det(p) and is deﬁned
as follows:

RG−

det(p) = {p(cid:48) | dist(p(cid:48), q) − dist(p(cid:48), p) > rp}.

(26)

Fig. 7. Lemma 7

Proof. In Figure 7, there are three circles, two of which are
centered on q and take rq and rq/2 as the length of their
radii, respectively. The other circle takes p as the center and
dist(p, q) as the length of the radius, where p lies in cq:rq/2,
i.e., dist(q, p) ≤ rq/2. Let p(cid:48) be an arbitrary point inside of
Cp:dist(p,q), then it must satisfy that

dist(p, p(cid:48)) ≤ dist(q, p).

(29)

From the triangle inequality of (cid:52)pqp(cid:48), it can be obtained that

dist(q, p(cid:48)) ≤ dist(q, p) + dist(p, p(cid:48)).

Then we can get that,

dist(q, p(cid:48)) ≤ 2 · dist(q, p).

Because dist(q, p) ≤ rq/2,

dist(q, p(cid:48)) ≤ 2 · rq/2 = rq

(30)

(31)

(32)

That means, any point lying in Cp:dist(p,q) must belong to
kNN(q). Therefore, the number of points lying in Cp:dist(p,q)
must not be greater than k, i.e., the size of kNN(q), which
means there is no more than k points closer to p than q. Hence
pk (kth closest point to p) cannot be closer than q to p. Then

B. Selection of determine points

Theoretically, when using Lemma 4, 5, 6 and 7 to verify the
candidates, any RkNN point can be considered as a positive
determine point. Similarly, if a point is not a member of
RkNNs, then it can be considered as a negative determine
point. In other words, all points in the candidate set are eligible
to be selected as determine points. Our aim is to issue as few
kNN queries as possible in the process of RkNN queries, that
is, to use as few determine points as possible to determine all
the other points in the candidate set. Therefore, the selection
of determine points is very important for improving the
efﬁciency of RkNN queries. Which points should be selected
as determine points is what we will scrutinize next.

Deﬁnition 14. Determine point set: For a RkNN query, given
a set Scnd of candidates and denoted as Sdist, a determine set
is such a set that the following condition is satisﬁed:
∀p ∈ Scnd \ Sdist, ∃p(cid:48) ∈ Sdist : p(cid:48) det−−→ p.

(35)

Because it

is not certain how many points and which
points need to be selected as determine points,
the total
number of schemes for selecting determine points can be as

dist(p, q) ≤ dist(p, pk) = rp.

(33)

large as

, where |Scnd| means the number of

|Scnd|
(cid:80)
i=1

(cid:19)

(cid:18)|Scnd|
i

According to Lemma 4, p ∈ RkNN(q), then Lemma 7 is
proved.

candidates. Hence the computational complexity of ﬁnding the
absolute optimal one out of all the schemes is as much as
O(k!). However, it is not difﬁcult to come up with a relatively
good determine points selecting scheme, of which the size of
the determine set |Sdist| is just about O(

k).

√

For a positive determine point, most of the points in its
determine region are closer to the query point than itself.
Furthermore, any negative determine point is closer to the
query point than most of the points in its own determine
region. Therefore, a point belonging to RkNNs can rarely be
determined by a point closer to the query point than itself,
and the probability that a point not belonging to RkNNs can
be determined by a point further than itself away from the
query point is also very low. Therefore, the points which are
extremely close to the boundary of the RkNN region (i.e.,
inﬂuence zone [5]) are rarely able to be determined by other
points. Thus, these points should be selected as determine
points in preference. However, it is impossible to directly
ﬁnd these points near the boundary without pre-calculating
the RkNN region. Calculating the RkNN region is a very
computational costly process for its computational complexity
of O(k3). While the kNN region of the query point is easy to
obtained by issuing a kNN query. Assuming that the points are
uniformly distributed, the kNN region and the RkNN region
of a query point are extremely approximate and the difference
between them is negligible. Hence it is a good strategy to
preferentially select the points near the boundary of kNN
region as the determine points to some extent.

As shown in Figure 9, there are some points distributed.
The region inside the circle with q as the center represents
the kNN region of q. In general, only the points near the
boundary of RGkNN(q) need to be selected as the determine

Fig. 8. Semi-kNN region

Deﬁnition 13. Semi-kNN region: Given the query point q,
the semi-kNN region of q is the internal region of Cq:rq/2.
is denoted as SRGkNN(q) and is deﬁned as
Formally,
Equation (34).

it

SRGkNN(q) = {p | dist(p, q) ≤ rq/2}

(34)

As shown in Figure 8, q represents the query point, the
region within the larger circle represents RGkNN(q), and the
gray region within the smaller circle represents SRGkNN(q).
It can be observed from the ﬁgure, p1 and p2 lie in the gray
region, while p3, p4 and p5 do not. Then p1 and p2 can be
determined as members of RkNN(q). Nevertheless, we cannot
determine whether p3, p4 or p5 belongs to RkNN(q) or not by
Lemma 7

With Lemma 4, 5, 6 and 7, we can ﬁnd all the points in the
RkNNs of the query point by verifying only a small portion
of points in the candidates.

can be determined whether belongs to the RkNNs. Otherwise,
we say that this point is almost impossible to be determined
by any known determine point and it should be marked
as a determine point. Recall Lemma 2, in two dimensions,
the expected number of Voronoi neighbors per point is 6,
which is a constant. By using the above approach we can
ﬁnd the determine point for a non-determine point with a
computational complexity of O(1).

D. Algorithm

In this subsection, we will introduce the implementation of

the RkNN algorithm based the above approaches.

The pseudocode for the veriﬁcation methood is shown in
Algorithm 1. When verifying a point, we ﬁrst try to determine
whether the point belongs to RkNNs by Lemma 4 (line 2). If
this fails, we visit the Voronoi neigbors of the point and try
to use Lemma 2 or Lemma3 to determine it (line 10 and line
13). If none of the three lemmas above apply to this point,
then we issue a kNN query for it and use Lemma 4 to verify
it (line 18).

Algorithm 1: verify(p, q, k, rq, Sv, Sdet, Ddet)
Input: the point p to be veriﬁed, the query point q, the

parameter k, the kNN radius rq of q, the set
Sv of points that have been visited , the
determine point set Sdet and the dictionary
Ddet that records the corresponding determine
points for non-determine points

Output: whether p ∈ RkNN(q).

1 Sv.add(p);
2 if dist(p, q) ≤ rq/2 then
3

return true;

4 foreach pn ∈ VN(p) do
if pn ∈ Sv then
5

/* Lemma 7 */

6

7

8

9

10

11

12

13

14

15

if pn ∈ Sdet then
pdet ←− pn;

else

pdet ←− Ddet[pn];

if pdet ∈ RkNN(q) and dist(p, q) +
dist(p, pdet) ≤ rpdet then /* Lemma 5 */

Ddet[p] ←− pdet;
return true;

if pdet /∈ RkNN(q) and dist(p, q) −
dist(p, pdet) > rpdet then /* Lemma 6 */

Ddet[p] ←− pdet;
return false;

16 rp ←− calculate the kNN radius of p;
17 Sdet.add(p);
18 if rp ≥ dist(p, q) then
return true;
19
20 else
21

return false;

/* Lemma 4 */

Fig. 9. Determine point set

points and all the other candidate points can be determined
by these determine points. In other words, if the points are
evenly distributed, the points near the boundary of RGkNN(q)
are enough to form a valid determine set of q. Because the
distribution of points is not guaranteed to be absolute uniform,
it is not always reliable if only the points near the boundary
of the kNN region of the query point are taken as determine
points for a RkNN query.

In order to ensure the reliability of the selection, we propose
a strategy to dynamically construct the determine set while
verifying the candidate points. First,
the candidate points
belonging to kNN(q) are accessed in descending order of
distance to q. Then the other candidate points are accessed
in ascending order of distance to q. During the process of
accessing candidates, once the currently accessed point cannot
be determined by any point in the determine point set, this
point should be selected as a determine point and put into the
determine point set. Otherwise, we can use a corresponding
point in the determine point set to determine whether it belongs
to RkNNs or not.

C. Matching candidate points with determine points

√

Under the above strategy, it is sufﬁcient to ensure that
any point not belonging to Sdist can be determined by at
least one point in Sdist. Since the expected size of Sdist
k) (see Section 5), the computational complexity of
is O(
ﬁnding a determine point for a point by exhaustive searching
the determine set is O(
k). Obviously, it is not a good idea to
match candidate points with their determine point in this way.
Therefore, we propose a method based on Voronoi diagrams
to improve the efﬁciency of this process.

√

Given a Voronoi diagram V D(P ) of a point set P and
a continuous region RG, the vast majority of points in RG
have at least one Voronoi neighbor lying in RG [18]. For any
determine point, its determine region is a continuous region
(ellipse region or hyperbola region). So for a non-determine
point, there is high probability that at least one of its Voronoi
neighbors can determine it or shares a determine point with
it. Therefore, when accessing a candidate point, if the point
can be determined by one of its Voronoi neighbors or the
determine point of one of its Voronoi neighbors, this point

Algorithm 2: RkNN(q)
Input: the query point q
Output: RkNN(q

1 Scnd ←− generateCandidates(q, k);
2 Sort Scnd in ascending order by the distance to q;
3 rq ←− calculate the kNN radius of q;
4 Sv ←− ∅;
5 Sdet ←− ∅;
6 Ddet ←− generate an empty dictionary;
7 SRkNN ←− ∅;
8 for i ←− k to 1 do
9

if verify(Scnd[i], q, k, rq, Sv, Sdet, Ddet) then

10

SRkNN.add(Scnd[i]);

11 for i ←− k + 1 to 6k do
12

if verify(Scnd[i], q, k, rq, Sv, Sdet, Ddet) then

13

SRkNN.add(Scnd[i]);

14 return SRkNN;

Using the veriﬁcation approach in Algorithm 1, we imple-
ment an efﬁcient RkNN algorithm, as shown in Algorithm 2.
First we generate the candidate set in the same way as VR-
RkNN [7], where the size of candidate is 6k (line 1). Next,
the candidate set is sorted in ascending order by the distance
to the query point (line 2). Then the ﬁrst k elements of the
candidate set and the rest of the elements are divided into two
groups. The elements in the two groups are veriﬁed one by
one in the order from back to front and from front to back,
respectively (line 8 and line 11). After all candidate points are
veriﬁed, the RkNNs of the query point is obtained.

We used the same algorithm as VR-RkNN to generate the
candidate set, and we do not improve it. The core of this
algorithm is still from the Six-regions [2]. In addition, it uses
a Voronoi diagram to ﬁnd the candidate points incrementally
according to Lemma 1. By Lemma 3, only the points whose
Delaunay distance to the query point is not larger than k are
eligible to be selected as candidate points. Hence the number
of points accessed for ﬁnding candidates in the algorithm
is guaranteed to be no more than O(k2). The pseudocode
of the algorithm for generating candidates is presented in
Algorithm 3.

V. THEORETICAL ANALYSIS

In this section, we analyze the expected size of determine
point set, the expected number of accessed points and the
computational complexity of our algorithm.

A. Expected size of determine point set

The query point is q, the number of points in RkNN(q)
is |RkNN|, and the number of points near the boundary of
RGkNN(q) is |Sb|. The area and circumference (total length
of the boundary) of RGkNN(q) are denoted as ARkNN(q) and
C RkNN(q), respectively. The expected size of the determine
point set of q is |Sdet|.

Algorithm 3: pruning(q, k)
Input: the query point q and the parameter k
Output: the candidates of RkNN(q)

1 H ←− M inHeap();
2 V isited ←− ∅;
3 for i ←− 1 to 6 do
4

Scnd[i] ←− M inHeap();

5 foreach p ∈ VN(q) do
6

H.push([1, p]);
V isited.add(p);

7

8 while |H| > 0 do
9

[distDG(p), p] ←− H.pop();
for i ←− 1 to 6 do

10

11

12

13

14

15

16

17

18

19

20

21

22

if Segmenti contains p then
if |Scnd[i]| > 0 then

pn ←− the last point in Scnd[i];

else

pn ←− a point inﬁnitely away from q;

if distDG(p) ≤ k and
dist(q, p) ≤ dist(q, pn) then

Scnd[i].push([dist(p, q), p]);
foreach p(cid:48) ∈ VN(p) do

if p(cid:48) /∈ V isited then

distDG(p(cid:48)) ←− distDG(p) + 1;
H.push([distDG(p(cid:48)), p(cid:48)]);
V isited.add(p(cid:48));

23 Candidates ←− ∅;
24 for i ←− 1 to 6 do
25

for j ←− 1 to k do

26

Candidates.add(Scnd[i].pop());

27 return Candidates;

It is shown that the expected value of |RkNN| is k [5]. Thus,
the radius of the approximate circle of RGkNN(q) is equal to
rq. Then

ARkNN(q) = π · rq

2

CRkNN(q) = 2π · rq.

(36)

(37)

The following equation can be obtained from Equation (36)
and Equation (37).

CRkNN(q) = 2(cid:112)π · ARkNN(q)
As the points around the boundary of RGkNN(q) consists of
two sets of points where one is inside RGkNN(q) and the other
is outside, |Sb| is to |RkNN| what 2·C RkNN(q) is to ARkNN(q),
i.e.,

(38)

|Sb| = 2 · 2(cid:112)π · |RkNN| = 4

√

√

π · k ≈ 7.1

k.

(39)

If all the points near the boundary are selected as the determine
points, there must be some redundancy, i.e., the determine

region of some points will overlap. Hence the size of the
determine point set generated under our strategy is less than
the number of the points near the boundary of the RkNN
region, i.e, |Sdet| ≤ 7.1

√

k.

B. Expected number of accessed points

For an RkNN query of q, the candidate points are distributed
in an approximately circular region RGcnd(q) centered around
q, which has an area Acnd(q) and a circumference Ccnd(q).
The expected number of accessed points is |Sac|. In the
ﬁltering phase of our approach, the points accessed include all
the the candidate points and their Voronoi neighbors. Except
for the points in the candidate set, the other accessed points
are distributed outside RGcnd(q) and adjacent to the boundary
of RGcnd(q). Hence |Sac| − |Scnd| is to |Scnd| what Ccnd(q)
is to Acnd(q), i.e.,

|Sac| − |Scnd| = 2(cid:112)π · |Scnd|

|Sac| = |Scnd| + 2(cid:112)π · |Scnd|
√

= 6k + 2

π · 6k ≈ 6k + 8.7

(40)

(41)

√

k

Therefore, if the points are distributed uniformly, the expected
k. When
number of accessed points is approximately 6k+8.7
|Sac| becomes larger.
the points are distributed unevenly,
However, it has an upper bound. Recall Lemma 3, we can
make deduce that only the points whose Delaunay graph
distance to q is not larger than k are eligible to be selected as
candidate points. Then

√

|Sac| ≤

k
(cid:88)

i=1

2π · i = (k2 + k)π.

(42)

C. Computational complexity

The expected computational complexity of the ﬁltering
phase of our approach is O(k ·log k) [7]. In the reﬁning phase,
we have to issue a kNN query with O(k ·log k) computational
complexity for each determine point, and the size of the
determine point set is about 7.1
k. The other candidates
only need to be veriﬁed by our efﬁcient veriﬁcation method.
Thus, the computational complexity of the reﬁning phase is
O(k1.5 · log k). Hence the overall computational complexity
of our RkNN algorithm is O(k1.5 · log k).

√

VI. EXPERIMENTS

In the previous section, we discussed the theoretical perfor-
mance of our algorithm. In this section, we intend to evaluate
the performance of aspects through comparison experiments.

A. Experimental settings

In the experiments, we let VR-RkNN [7] and the state-of-
the-art RkNN approach SLICE [6] to be the competitors of
our method.

The settings of our experiment environment are as follows.
The experiment is conducted on a personal computer with
Python 2.7. The CPU is Intel Core i5-4308U 2.80GHz and
the RAM is DDR3 8G.

To be fair, all three methods in the experiment are imple-
mented in Python, with six partitions in the pruning phase. We
use two types of experimental data sets: simulated data set and
real data set1. To decrease the error of the experiments, we
repeat each experiment for 30 times and calculate the average
of the results. The query point for each time of the experiment
is randomly generated.

Our experiments are designed into four sets. The ﬁrst set
of experiments is used to evaluate the effect of the data size
on the time cost of the RkNN algorithms. The data size is
from 103 to 106 and the value of k is ﬁxed at 200. The rest
of sets are used to evaluate the effect of the value of k on
the time cost, the number of veriﬁed points and the number
of the accessed points of the RkNN algorithms, respectively.
For these three sets of experiments, the size of the simulated
data is ﬁxed at 106, the size of the real data is 49,601 and the
value of k varies from 101 to 104.

B. Experimental results

TABLE II
TOTAL TIME COST(IN MS) OF DIFFERENT RkNN ALGORITHMS WITH
VARIOUS SIZES OF DATA SETS.

Algorithm

VR-RkNN
SLICE
Our approach

103

510
232
59

Data size

104

725
397
65

105

728
438
69

106

732
441
72

Fig. 10. Effect of data size on efﬁciency of RkNN queries

Figure 10 shows the time cost of the three RkNN algorithms
with various data sizes. As we can see, when the number of
points in the database is signiﬁcantly much lager than k, the
impact of the data size on the time cost of RkNN queries is
very limited. If the number of points in the database is small
enough to be on the same order of magnitude as k, all points
in the database become candidate points. Then the smaller the
database size, the less time cost of the RkNN query. When the
number of points in the database is above 10,000 and the value
of k is ﬁxed at 200, the time cost of our approach is always
around 84% and 90% less than that of SLICE and VR-RkNN,

149,601 non-duplicative data points on the geographic coordinates of the
National Register of Historic Places (http://www.math.uwaterloo.ca/tsp/us/
ﬁles/us50000 latlong.txt)

respectively. The detailed experimental results are presented
in Table II.

TABLE IV
NUMBER OF CANDIDATES VERIFIED BY RkNN ALGORITHMS WITH
VARIOUS VALUES OF k.

TABLE III
TOTAL TIME COST (IN MS) OF RkNN QUERIES WITH VARIOUS VALUES OF
k.

k

101
102
103
104

Simulated data

Real data

VR-RkNN

SLICE

Our approach

VR-RkNN

SLICE

Our approach

5
199
20576
2118391

26
193
3759
321233

2
39
801
22077

4
194
17212
1829742

28
283
4610
226959

2
29
813
23911

(a) Simulated data

(b) Real data

Fig. 11. Effect of k on efﬁciency of RkNN queries

Figure 11 shows the inﬂuence of k on the efﬁciency of these
three RkNN algorithms, where sub-ﬁgure (a) and (b) shows the
time cost of RkNN queries from simulated data and real data,
respectively. As k varies from 10 to 10,000, the time cost of
these three algorithms increases. With both synthetic data and
real data, the query efﬁciency of our approach is signiﬁcantly
higher than that of the other two competitors. With the increase
of k, this advantage becomes more and more obvious. When
k is 10,000, the time cost of our approach is only about 1/10
of that of the state-of-the-art algorithm SLICE. The detailed
experimental results are presented in Table III.

(a) Simulated data

(b) Real data

Fig. 12. Effect of k on the number of candidates veriﬁed

k

101
102
103
104

Simulated data

Real data

VR-RkNN

SLICE

Our approach

VR-RkNN

SLICE

Our approach

60
600
6000
60000

25
257
2572
25675

20
57
186
599

60
600
6000
49601

20
203
2257
23874

17
46
156
627

points veriﬁed with different values of k. During the execution
of our algorithm, only the points in the determine point set
are veriﬁed by issuing kNN queries. Therefore, the number
of candidates veriﬁed is equal to the size of the determine
point set. As we discussed in section V-A, the size of the
k.
determine point set is theoretically not larger than 7.1
In consequence, the theoretical number of veriﬁed candidates
k. It can be seen from the ﬁgure that
in Figure 12 is 7.1
the actual number of points veriﬁed is slightly less than the
theoretical value, 7.1
k. It indicates that the experimental
results are consistent with our analysis. It is also obvious from
the ﬁgure that the number of veriﬁed candidate points of our
approach is much smaller than that of the other two algorithms.
The detailed experimental results are presented in Table IV.

√

√

√

(a) Simulated data

(b) Real data

Fig. 13. Effect of k on the number of points accessed

TABLE V
NUMBER OF ACCESSED POINTS OF RkNN QUERIES WITH VARIOUS
VALUES OF k.

k

101
102
103
104

Simulated data

Real data

VR-RkNN

SLICE

Our approach

VR-RkNN

SLICE

Our approach

76
725
6721
63782

119
1052
10211
102206

75
728
6731
63721

153
1108
32031
49601

181
876
14359
49601

162
1193
31717
49601

Figure 12 reﬂects the relationship between k and the number
of candidate points veriﬁed of the three algorithms in the
experiments. Sub-ﬁgure (a) and (b) show the experimental
results on simulated data and real data, respectively. These
two sub-ﬁgures also show the theoretical number of candidate

Figure 13 shows the number of accessed points of the three
algorithms in the experiments and the theoretical number of
accessed points of our approach with various values of k,
which indirectly reﬂects their IO cost. It can be seen from
sub-ﬁgure (a), the number of accessed points of the three
algorithms is almost equal in terms of magnitude, and so is

[6] Shiyu Yang, Muhammad Aamir Cheema, Xuemin Lin, and Ying Zhang.
SLICE: reviving regions-based pruning for reverse k nearest neighbors
queries. In IEEE 30th International Conference on Data Engineering,
Chicago, ICDE 2014, IL, USA, March 31 - April 4, 2014, pages 760–
771, 2014.

[7] Mehdi Sharifzadeh and Cyrus Shahabi. Vor-tree: R-trees with voronoi
diagrams for efﬁcient processing of spatial nearest neighbor queries.
Proc. VLDB Endow., 3(1-2):1231–1242, September 2010.

[8] Anil Maheshwari, Jan Vahrenhold, and Norbert Zeh. On reverse nearest
neighbor queries. In Proceedings of the 14th Canadian Conference on
Computational Geometry, University of Lethbridge, Alberta, Canada,
August 12-14, 2002, pages 128–132, 2002.

[9] Congjun Yang and King-Ip Lin. An index structure for efﬁcient reverse
In Proceedings of the 17th International
nearest neighbor queries.
Conference on Data Engineering, April 2-6, 2001, Heidelberg, Germany,
pages 485–492, 2001.

[10] Congjun Yang and King-Ip Lin. An index structure for efﬁcient reverse
In Proceedings of the 17th International
nearest neighbor queries.
Conference on Data Engineering, April 2-6, 2001, Heidelberg, Germany,
pages 485–492, 2001.

[11] King-Ip Lin, Michael Nolen, and Congjun Yang. Applying bulk
insertion techniques for dynamic reverse nearest neighbor problems. In
7th International Database Engineering and Applications Symposium
(IDEAS 2003), 16-18 July 2003, Hong Kong, China, pages 290–297,
2003.

[12] Antonin Guttman. R-trees: A dynamic index structure for spatial
In Beatrice Yormark, editor, SIGMOD’84, Proceedings of
searching.
Annual Meeting, Boston, Massachusetts, USA, June 18-21, 1984, pages
47–57. ACM Press, 1984.

[13] G´ısli R. Hjaltason and Hanan Samet. Distance browsing in spatial

databases. ACM Trans. Database Syst., 24(2):265–318, 1999.

[14] Dimitris Papadias, Yufei Tao, Kyriakos Mouratidis, and Chun Kit Hui.
Aggregate nearest neighbor queries in spatial databases. ACM Trans.
Database Syst., 30(2):529–576, 2005.

[15] Dimitris Papadias, Yufei Tao, Greg Fu, and Bernhard Seeger. Progressive
skyline computation in database systems. ACM Trans. Database Syst.,
30(1):41–82, 2005.

[16] Cyrus Shahabi and Mehdi Sharifzadeh. Voronoi diagrams for query
processing. In Encyclopedia of GIS., pages 2446–2452. Springer, 2017.
[17] B. Delaunay. Sur la sph`ere vide. a la m´emoire de georges vorono¨ı.
Bulletin de I’Acad´emie des Sciences de I’URSS. Classe des Sciences
Math´ematiques et Naturelles, 6:793–800, 1934.

[18] Yang Li.

Area queries based on voronoi diagrams.

CoRR,

abs/1912.00426, 2019.

the theoretical value of our approach. Speciﬁcally, the number
of accessed points of our approach is slightly smaller than
that of SLICE. As shown in sub-ﬁgure (b), our approach
needs to access more points than SLICE. The reason is
that
the distribution of real data is very uneven, and our
algorithm is more sensitive to the distribution of data than
SLICE. Note that our approach and VR-RkNN use the same
candidate set generation method, so they have almost the same
number of accessed points. The detailed experimental results
are presented in Table V.

From the above three experiments, it can be seen that RkNN
query efﬁciency is little affected by the data size, but greatly
affected by the value of k. Our approach is signiﬁcantly
more efﬁcient than other algorithms because it requires less
veriﬁcation of candidate points. For data sets with very uneven
distribution of points, the candidate set of our approach is
relatively large, which will affect the IO cost to some extent.
However, the main time cost of the RkNN query is caused
by a large number of veriﬁcation operations rather than IO.
Therefore, the distribution of points has little impact on the
overall performance of our approach.

VII. CONCLUSIONS AND FUTURE WORKS

In this paper, we propose an efﬁcient approach to verify
potential RkNN points without issuing any queries with non-
constant computational complexity. With the proposed veriﬁ-
cation approach, an efﬁcient RkNN algorithm is implemented.
The comparative experiments are conducted between the pro-
posed RkNN and other two RkNN algorithms of the state-
of-the-art. The experimental results show that our algorithm
signiﬁcantly outperforms its competitors in various aspects,
except that our algorithm needs to access more points to
generate the candidate set when the distribution of points is
very uneven. However, our algorithm does not require costly
validation of each candidate point. Hence the distribution of
data has very limited impact on its overall performance.

REFERENCES

[1] Flip Korn and S. Muthukrishnan.

Inﬂuence sets based on reverse
In Proceedings of the 2000 ACM SIGMOD
nearest neighbor queries.
International Conference on Management of Data, May 16-18, 2000,
Dallas, Texas, USA, pages 201–212, 2000.

[2] Ioana Stanoi, Divyakant Agrawal, and Amr El Abbadi. Reverse nearest
In 2000 ACM SIGMOD
neighbor queries for dynamic databases.
Workshop on Research Issues in Data Mining and Knowledge Discovery,
Dallas, Texas, USA, May 14, 2000, pages 44–53, 2000.

[3] Yufei Tao, Dimitris Papadias, and Xiang Lian. Reverse knn search in
arbitrary dimensionality. In Proceedings of the Thirtieth International
Conference on Very Large Data Bases - Volume 30, VLDB ’04, page
744–755. VLDB Endowment, 2004.

[4] Wei Wu, Fei Yang, Chee Yong Chan, and Kian-Lee Tan. FINCH:
evaluating reverse k-nearest-neighbor queries on location data. PVLDB,
1(1):1056–1067, 2008.

[5] Muhammad Aamir Cheema, Xuemin Lin, Wenjie Zhang, and Ying
Zhang. Inﬂuence zone: Efﬁciently processing reverse k nearest neighbors
queries. In Proceedings of the 27th International Conference on Data
Engineering, ICDE 2011, April 11-16, 2011, Hannover, Germany, pages
577–588, 2011.

