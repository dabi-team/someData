1
2
0
2

l
u
J

2
1

]

C
H
.
s
c
[

1
v
4
9
1
5
0
.
7
0
1
2
:
v
i
X
r
a

Monoscopic vs. Stereoscopic Views and Display Types in the
Teleoperation of Unmanned Ground Vehicles for Object Avoidance

Yiming Luo1, Jialin Wang1, Hai-Ning Liang1,∗, Shan Luo2, and Eng Gee Lim3

reality

Abstract— Virtual

(VR) head-mounted displays
(HMD) have recently been used to provide an immersive,
ﬁrst-person vision/view in real-time for manipulating remotely-
controlled unmanned ground vehicles (UGV). The teleoperation
of UGV can be challenging for operators when it is done in real
time. One big challenge is for operators to perceive quickly and
rapidly the distance of objects that are around the UGV while
it is moving. In this research, we explore the use of monoscopic
and stereoscopic views and display types (immersive and non-
immersive VR) for operating vehicles remotely. We conducted
two user studies to explore their feasibility and advantages.
Results show a signiﬁcantly better performance when using an
immersive display with stereoscopic view for dynamic, real-
time navigation tasks that require avoiding both moving and
static obstacles. The use of stereoscopic view in an immersive
display in particular improved user performance and led to
better usability.

I. INTRODUCTION

The teleoperation of unmanned ground vehicles (UGV) or
unmanned aerial vehicles (UAV) is challenging for operators
because it is not easy to have a good understanding of the
terrain and surrounding objects given that they are away from
the actual environment [1]. With advances in real-time image
transmission, issues such as low image quality, long delays
in image transmission, and unstable signal
transmission
have been improved signiﬁcantly, making their teleoperation
stable and practical in a wide range of situations [2]. These
improvements have allowed for the exploration of different
image views that can be shown on different display types to
improve the ability of operators to control UGV in a precise
and efﬁcient manner.

Current displays used in the teleoperation of UGV include
primarily traditional 2D screens (such as those on mobile
phones/tablets and typical desktop computers). More recently
immersive displays, such as virtual reality head-mounted
displays (VR HMD), have started to make their way for UGV
teleoperations. VR HMD allow a higher level of immersion
and presence (that is, the feeling of being “there” in the
environment) [3], which can enhance performance [4], [5].
The disadvantage of VR HMD is that
they have small
displays and their use may increase the chance of operators

The work has been supported in part by Xi’an Jiaotong-Liverpool
University (XJTLU) Key Special Fund (KSF-A-03; KSF-P-02) and XJTLU
Research Development Fund.

1Yiming Luo, Jialin Wang, Hai-Ning Liang are with the Department of

Computing, Xi’an Jiaotong-Liverpool University, Suzhou, China.

2Shan Luo is with the Department of Computer Science, The University

of Liverpool, UK.

3Eng Geen Lim is with The Department of Communications and Net-

working, Xi’an Jiaotong-Liverpool University, Suzhou, China.

∗Corresponding author (haining.liang@xjtlu.edu.cn).

having motion sickness, a common issue with HMD that is
not present in normal displays [6], [7].

Whether it is a traditional screen or a VR HMD, it is not
easy for operators to gauge the distance and depth perception
of objects from the video images shown in the displays in
real time because, unlike being in the real environment, they
often do not provide enough visual stereoscopic information
for the operators to see.

Depending on the placement of the camera in the drone,
the operator can see images based on the ﬁrst-person view
(FPV) or the third-person view (TPV) [1], [8], [9]. Main-
stream drones in the market can supply a FPV perspective
primarily. Based on cost factors and the stability of image
transmission, most drones use a single camera, typically of
high-quality, to transmit monoscopic images to a 2D non-
immersive screen display. In the absence of stereoscopic
cues, these 2D images do not often provide enough visual
information for operators to perceive clearly and accurately
distance and depth of obstacles surrounding the drone [1],
[10]. This situation could be worse when there are moving
objects while the drone is moving towards them.

Using binocular cameras could provide additional infor-
mation to assist operators gauge distance information of
objects. The use of binocular cameras has been studied on
UAV mainly but not so much on UGV [1], [10]–[12]. At low
altitudes but high speed, a UAV with an on-board computer
that can provide stereoscopic FPV rendered in a VR HMD
could enhance the control of the ﬂying drone [1]. In our
research, we explore the use of binocular cameras in UGV,
which tend to go slower than UAV and could have more
objects on its path, and whether they can enhance teleporters’
stereoscopic perception and their performance in obstacle
avoidance tasks in real-time. In addition, we want to compare
the usability and performance in both normal displays and
immersive VR HMD.

In this paper, we ﬁrst review related work about viewing
modes and display types as well as obstacle avoidance tasks
and their performance metrics. We then present two user
studies. The ﬁrst study explores four viewing modes in a
normal 2D display and VR HMD (see Fig. 1). We compared
these modes based on participants’ subjective feedback and
objective results in a pre-designed obstacle avoidance ex-
periment. The version with stereoscopic view in VR had
best performance but participants still faced challenges with
moving objects. These results led us modify this viewing
mode and run a second study comparing the new mode
against it. The results show that the new ﬁfth mode could
lead to an improved overall performance. The results of these

 
 
 
 
 
 
Fig. 1. The four display modes tested in the ﬁrst experiment. (A, a) D1: a regular monitor; using one of the binocular cameras’ images. (B, b) D2: a
ﬁxed virtual screen in VR; using one of the binocular cameras’ images. (C, c) D3: a virtual screen in VR moving along with head motions; using one of
the binocular cameras’ images (monoscopic). (D, d) D4: a virtual screen in VR moving along with head motions; using both of the binocular cameras’
images (stereoscopic).(e) The picture of UGV with the camera facing a board on the monitor.

two studies can inform the design of future teleoperator-
UGV interaction that offers better performance and user
experience.

II. RELATED WORK

In this section, we ﬁrst introduce previous work on view-
ing modes and display types for UGV/UAV. Then we present
the tasks for testing robot performance and the metrics for
their evaluation.

A. Viewing Modes and Display Types

Viewing modes (e.g., 2D vs. 3D) and display types (VR
vs. 2D displays) can affect users’ levels of immersion,
ﬂow, and performance [4], [13], [14]. Most
teleoperated
unmanned systems use a monocular camera that displays
video streams captured from the camera attached to a robot.
The camera supplies images for operators to see and make
decisions for where to go. Because the camera tends to
have a small ﬁeld-of-view (FOV), the video images could
only provide limited visual details which in turn can lead
to lower performance on tasks such as target detection and
identiﬁcation [15]. It requires operators to put extra effort to
survey the environment (e.g., by manipulating and/or rotating
the robot to get different views) [16], an inefﬁcient process
that can increase mental and visual load and the feeling of
motion sickness. Also, important distance cues may not be
provided while depth perception may be degraded when the
FOV is restricted [17].

When driving a UGV, operators have more difﬁculties in
judging the speed of the vehicle, time-to-collision, perception
of objects, locations of obstacles, and the start of a sharp
curve [18]. The level of difﬁculty can increase if the objects
around the drone are moving and not static. However, simply
increasing the FOV may lead to other issues. For example, in
a typical 2D display, this could mean that the users may need
to scan a wide view of the environment, requiring moving
their head left and right frequently. In addition, a larger
FOV may cause additional usability issues with VR HMD
as rotation head movements might induce greater motion
sickness [6], [7]. The challenge is to provide additional visual

cues without signiﬁcantly changing the FOV and causing
large head movements, especially rotational ones.

3D Stereoscopic (3DS) views have been found to be better
than traditional non-stereo (2D) views on manipulation tasks
with either virtual or real objects [5], [19]. In [1] the authors
explored immersive 3DS views for ﬂying drones which
enable higher accurate depth perception and led to better
teleoperation and navigation performance. Stereoscopic FPV
presents signiﬁcant advantages over monocular FPV [20],
[21]. The distance between the binocular cameras used to
achieve 3DS also has an important effect. The best per-
formance was achieved when the inter-camera distance was
less than the inter-ocular distance, which is 2–3 cm and 6
cm, respectively [22]. However, artiﬁcially induced binocular
stereo-vision may increase motion sickness and perceived
stress [23]. Latency in image transmission is also associated
with motion sickness [24], [25]. Low image quality caused
by reduced frames per second (fps), reduced resolution of
the display (pixels per frame), while a lower gray-scale
(number of levels of brightness or bits per frame) [26] can
also increase the motion sickness. While motion sickness
is possible to have high-
can be an issue and when it
resolution images without transmission delays [16], the depth
information and stereoscopic perception provided by the
binocular cameras can signiﬁcantly improve the operator’s
teleoperation performance for obstacle avoidance and precise
maneuverability.

Distance underestimation and overestimation can occur
when objects are viewed in normal displays and especially in
immersive VR environments (VE) [22], [27]. In experiments
with teleoperated UGV, it has been found that operators
underestimated the distances from obstacles and landmarks
[28]. As such, given the beneﬁts of 3DS views in VR,
this research aims to explore whether the combination of
3DS and VR could support operators to gauge distance
information during teleoperation manipulation and improve
their performance in obstacle avoidance tasks.

The binocular camera consists of 2 monocular cameras
with a resolution of 1280 × 720, a 71° FOV, and distortion
less than ± 0.3 % [33]. The binocular camera has a stereo-
scopic view with a 2560 × 720 resolution. It was installed
on a mounting platform with Inter Pupillary Distance (IPD)
adjustment with a range of about 26mm to 84mm. The
average human IPD is 65mm, with a range of 54 to 75
millimeter for young adults, between 16 to 24 years [34].
Therefore, the IPD was set to 65mm in the stereoscopic FPV
condition. The right camera was moved to a central position
as the input source of the other monoscopic conditions. The
FPV system used WebRTC as the video streaming protocol
which has a 214 ± 7 (ms) latency at 2560 x 720 resolution
(1280 x 720 per eye) and 30 FPS in a Wi-Fi6 connection
[35]. A mini desktop with 16GB RAM, an i7-10710U CPU,
and an Intel UHD Graphics was installed on the robot as
the WebRTC server which was powered by a 100w power
bank. The HMD and monitor were connected to a desktop
with 16GB RAM, an i7-9700k CPU, a GeForce GTX 2080Ti
dedicated GPU.

Participants used an Xbox wireless controller3 as the
input device to control the UGV. Fig. 4 (next page) shows
the elements of the control and image transmission system
consisting of a custom control protocol, robot controller
software for Windows, Inter-Process-Communication (IPC),
VR and PC interface built in Unity3D, and Xbox controller
for controlling the robot remotely.

B. Conditions

In this study, the following four conditions were explored
(see Fig. 1). Fig. 1e shows the UGV’s camera pointing at
a board with black dots and words to show the difference
between monoscopic and stereoscopic images.

• D1: Non-immersive display with ﬁxed screen using
monoscopic images (see Fig. 1A, 1a). We used a
traditional 27-inch monitor as the display screen. The
content of the display was from monoscopic images
(fed from one of the binocular cameras placed on
the robot). This is the non-immersive display and the
baseline condition as this is what is provided typically in
current UGV/UAV via a mobile phone, tablet or regular
desktop.

• D2: Immersive display with ﬁxed screen using mono-
scopic images (see Fig. 1B, 1b). We constructed a
ﬁxed virtual screen in VR. All irrelevant information is
blacked out, except for what is displayed. This condition
is the monoscopic view in VR.

• D3: Immersive display with head tracking screen using
monoscopic images (see Fig. 1C, 1c). We had this con-
dition because this is the same as the ones that are used
for current UGV or UGV that provide VR capabilities.
We turned the ﬁxed screen in into a moving screen that
follows users’ head motion. It used monoscopic images
captured from one of the binocular cameras.

3Xbox wireless

controller:

https://www.microsoft.com/

en-us/p/xbox-wireless-controller/8xn59crbsqgz?cid=
msft_web_collection.

Fig. 2.
(a) The prototype used in this research. It contains a UGV (DJI
RoboMaster S1) and a FPV system which was built with a binocular camera,
an on-board mini desktop, and a power bank. (b) Implementation of 3DS
view.

B. Tasks and Metrics

Teleoperation tasks that require real-time obstacle avoid-
ance have been researched in the context of human-drone
interaction. For example, in [29], [30], researchers explored
a remotely controlled robot that has been integrated with a
laser sensor and a monocular camera to capture distance in-
formation. This combination provided operators with images
displayed in a virtual UI. The operator was asked to drive
this robot through a series of mazes and avoid obstacles. Its
performance was measured using a set of metrics [31]: (a)
Obstacle encounter, the number of collisions of the robot
against obstacles; (b) Efﬁciency, the time to complete tasks;
and (c) Subjective ratings, usability issues with controls and
the interface. We followed a similar approach and used these
metrics in this research.

In addition, it has been shown that moving objects require
a greater depth information of the objects to avoid colliding
with them [32]. As such, the maze used in our research
(see Fig. 3 on the next page) incorporated both static and
dynamic objects of different types. Their combination allows
us to explore in detail
the comparative performance of
teleoperating a UGV based on different viewing modes and
display types.

III. USER STUDY 1

To explore how monoscopic and stereoscopic views and
display types affect the distance perception of users, we
conducted a 4-condition, 8-person within-subjects study. The
main task consisted of participants driving a UGV in real
time using a game controller through a maze that had both
static and moving obstacles.

A. Prototype

Fig. 2a shows the prototype we developed to conduct this
research. A DJI RoboMaster s11 was used as the mobile
UGV. The transmission system consisted of a binocular
camera, a mini desktop, and a power bank. The ﬁrst-person
view (FPV) images from the camera were transmitted to a
PC and rendered in a traditional 27-inch 4K monitor or VR
HMD, which in our case was a HTC Vive Pro Eye2.

1DJI RoboMaster S1: https://www.dji.com/robomaster-s1
2HTC

Pro:https://www.vive.com/uk/product/

Vive

vive-pro-eye/specs/

Fig. 3. Overview of the maze. (a) The distribution of the different tasks in the maze (T1, T2, T3, T4, and T5); (b) A picture of the actual maze used in
the study.

prevent collisions.

• T3: This task had a cardboard box that rotated at a con-
stant speed. Participants would need to judge distance
and time their move accordingly to avoid colliding with
this spinning obstacle.

• T4: This task had a circular moving obstacle and a cross
shaped, static obstacle. Participants had to judge the
distance between the robot and the obstacles and time
its move accordingly to avoid hitting both the moving
obstacle and horizontal cross at the same time.

• T5: This task consisted of an obstacle that would move
in a straight line in a backward and forward manner and
a cube placed vertically. Participants needed to judge the
robot’s distance and time its move to avoid colliding
with either object.

A simple driving training outside the maze was given to
the participants before they started the formal trials. The
purpose of this training was to give participants the chance to
become familiar with the controls, the HMD, and controlling
the robot. After this training, the participants were asked to
run the formal trials. Each participant had three trials in each
display methods and the order of display methods was pre-
determined by a 4 × 4 Latin square to reduce any learning
effect.

D. Participants

Eight participants (5 males and 3 females, aged between
20-29, mean = 24.5) were recruited for this experiment. Data
collected from the pre-experiment demographics question-
naire show that that they all declared to be healthy and did
not have any health issues, physical and otherwise. They all
had normal or corrected-to-normal vision and did not suffer
from any known motion sickness issues in their normal daily
activities. None of them had any experience driving a UGV
using HMD in FPV. As such, it was the ﬁrst time for all 8
participants to drive a remote a UGV using an HMD in FPV.
This experiment has been approved by the University

Ethics Committee at Xi’an Jiaotong-Liverpool University.

Fig. 4. Overview of the components of the control and image transmission
system.

• D4: Immersive display with head tracking screen us-
ing stereoscopic images (see Fig. 1d). We used the
same moving screen as D3, but the content of display
was stereoscopic images captured using both binocular
cameras. Each camera would provide images to the
corresponding eye (see Fig. 2b).

C. Tasks and Procedures

To investigate the performance of the four different types
of views and display methods, participants had to drive the
robot through a customized maze (Fig. 3b). We installed a
high-deﬁnition video camera and 8 motion tracking VICON4
cameras to capture the movement of the robot along the maze
and to detect any collisions with the objects in the maze.

For each condition, participants had to drive the robotic
car remotely and maneuver it through the maze as fast as
possible but without hitting or colliding with the obstacles.
The driving maze was designed to have ﬁve different tasks
(see Fig. 3a):

• T1: This task consisted of 36 small and static wooden
cubes. As the UGV moves deeper into the maze, the
distance between the two cubes would become smaller
(from 32cm to 30cm to 28cm).

• T2: This task had two big cubes placed horizontally.
The user needs to drive UGV in a side-way manner to

4Vicon system: https://www.vicon.com/

E. Hypotheses

Based on our review of the literature and experiment

design, we formulated the following four hypotheses:

• H1: D4 would lead to the best overall performance in
distance perception; D1 would lead to the worse overall
performance than D2 and D3;

• H2: D4 would lead to the best local performance in the
complex tasks (T3, T4, and T5); D1 would lead to the
worse local performance than D2 and D3 in these three
tasks (T3, T4, and T5);

• H3: D4 would signiﬁcantly reduce user demands and
would be the most popular display method; D1 would
have more user demands and less user preferences than
D2 and D3.

F. Results

1) Overall Performance: All participants understood the
nature of the tasks and all recorded data were valid. If there
was a collision and it lasted less than 1 second, then it was
considered as one collision only. If it lasted longer than 1
second, for each 1 second of collision time, we counted it as
one collision. We recorded the number of collisions for each
trial by checking frame by frame the high-deﬁnition videos
from the camera and VICON tracking system. A Shapiro-
Wilk test for normality was performed on each of measures
separately for each condition and show that they followed a
normal distribution.

Fig. 5a shows the average number of collisions per
condition. A repeated measures ANOVA with Greenhouse-
Geisser correction showed that
the mean of the number
of collisions differed signiﬁcantly between display types
(F(1.722, 12.054) = 10.691, p <.05). A Bonferroni post-hoc
test revealed that the number of collisions was signiﬁcantly
lower for D2 and D4 (p <.05) compared to D1. There was
no signiﬁcant difference between D2, D3 and D4 (p >.05).
A repeated measures ANOVA with Greenhouse-Geisser
there was no signiﬁcant difference

correction found that
between conditons (F(1.481, 10.365) = 1.327, p >.05).

2) Local Performance: Fig. 5b shows the average number
of collisions for each condition in each task. A repeated
measures ANOVA with Greenhouse-Geisser correction found
that there was a signiﬁcant difference in T1 (F(1.464, 10.248)
= 14.477, p <.05). A Bonferroni post-hoc test showed that
the number of collisions was signiﬁcantly lower in T3 for
D1 (p <.05) and D3 (p <.001) when compared to C4.

There was no signiﬁcant difference among D1, D2, D3

and D4 in completion time.

G. Subjective Results

1) NASA-TLX Workload: Kruskal-Wallis H Test was con-
ducted and found no signiﬁcant difference in any sub-scales
of the NASA-TLX workload (Mental, Physical, Temporal,
Effort, Performance, and Frustration).

2) User Experience Questionnaire (UEQ): Fig. 5c shows
the results of the UEQ. They were evaluated with the
Kruskal-Wallis H Test comparing the four conditions for the
six UEQ elements (Attractiveness, Perspicuity, Efﬁciency,

Dependability, Stimulation, and Novelty). There was a sig-
niﬁcant difference in Novelty (χ2(3) = 9.077, p <.05). Dunn’s
post-hoc analysis showed a signiﬁcant difference (p <.05) for
D1 vs D4 in Novelty.

3) Interviews Results: Overall, all participants had a pos-
itive experience in the experiment. None of the participants
commented that they had any serious discomfort or simula-
tion sickness in the four conditions. Most said that the most
difﬁcult task was T3, the section with a cardboard box that
rotated at a constant speed.

H. Discussion

The results conﬁrmed some of our initial hypotheses but
also revealed some different effects. We discuss these next.
1) Overall Performance: From the evaluation of the over-

all number of collisions, we have following effects:

• D2 signiﬁcantly reduced the number of collisions com-

pared to D1. This supports in part H1.

• D3 did not signiﬁcantly reduce the number of collisions

compared to D1. This seems to contradict H1.

• D4 signiﬁcantly reduced the number of collisions com-

pared to D1. This also supports in part H1.

D2, D3, and D4 all used VR. Their advantage over a
normal monitor was that
they could increase immersion
and eliminate the reﬂection of light on the 2D display
screen. Participants seem to have had better concentration
and immersion when the off-screen information was all
blackout. According to H1, all conditions with VR (D2, D3,
and D4) should have improved the performance compared
to D1. However, we only found signiﬁcant improvements
in D2 vs D1 and D4 vs D1. This seems to indicate that
the existing display modes from products in the market
(D3 vs D1), as mentioned in Section II.A (Condition), did
not show signiﬁcant difference in performance for real-time
obstacle avoidance tasks. That is, they did not lead to a good
performance overall.

For overall completion time, our analysis indicated that the
different conditions did not signiﬁcantly improve efﬁciency
in the tasks, which contradicted H1.

In interviews after the experiment, participants commented
that they needed more effort in T3 because estimating the
distance of the UGV to a moving obstacle with frequent
changes in depth perception was very challenging.

2) Local Performance: Users behaved differently in dif-
ferent tasks in our experiment. Based on the analysis of colli-
sions, we found that only one task (T3) presented signiﬁcant
differences on performance, while the other tasks did not,
which supported part of H2 (only T3 showed differences).
In particular, we found the following:

• Using the virtual screen in VR (D2) and monoscopic
view in VR (D3) did not signiﬁcantly reduce the number
of collisions compared to the 2D screen (D1) in T3.
• Having stereoscopic images in VR (D4) signiﬁcantly
reduced the number of collisions compared to the 2D
screen (D1) and the monoscopic view in VE (D3),
respectively.

(a) Overall mean and std. deviation of number of collisions for the four conditions (D1, D2, D3, and D4); (b) Local mean and std. deviation of
Fig. 5.
number of collisions for the four conditions (D1, D2, D3, and D4) in each task (T1, T2, T3, T4, T5). All Error bars show +1 SD; (c) Average of response
scores for each element of UEQ. Error bars show +1 SD. Higher scores represent more preferences in all cases. The ’×’ symbol represents the mean
value. Error bars show ±1 SD.

T3 required participants to judge the distance between the
UGV and a spinning box. The difference between this task
(T3) and other tasks with dynamic obstacles (T4 and T5) was
that it not only changed the motion states of the obstacle
but also signiﬁcantly changed the depth of the obstacle that
user needed to perceive from the views. This was where the
advantages of 3DS view was observed the most because it
allowed participants to obtain a better depth perception and
3D effect. Therefore, the participants could have a better
sense of the distance between the UGV and the obstacles,
which helped improve their performance. This was supported
by the interview data. Participants commented that
they
needed more effort on T3 because estimating the distance
to a moving obstacle with frequent changes in perceptual
depth was quite challenging.

3) User Demands and User Preferences: The positive
results from NASA-TLX data do not fully show equal prefer-
ence of users for the four conditions, which contradicted the
ﬁrst part of H3. The results of UEQ gave us further insights
into participants’ preferences. From the results of UEQ, D4
was considered more creative and innovative when compared
to D1, which supported the second part of H3.

A. Conditions

We decided to compare the best-performing version (D4)

in Study 1 with the new mode (D5):

• D4: Immersive display with head tracking screen using

stereoscopic images. The same as in Study 1.

• D5: Immersive display with ﬁxed screen using stereo-
scopic images. We used a ﬁxed virtual screen in VR.
The display content was stereoscopic (using both binoc-
ular cameras; each camera provides different images to
their corresponding eye).

B. Participants

Another 8 participants (4 males and 4 females, aged
between 21-27, mean = 22.5) were recruited for this ex-
periment. Data from the pre-experiment questionnaire show
that none of the participants had major physical discomfort,
health problems, simulator sickness, or vision issues. All of
them were able to complete the pre-training successfully.
Similar to participants in Study 1, none of these participants
had any experience with driving a UGV using HMD in FPV.
As such, it was also the ﬁrst time for all 8 participants to
drive a UGV using an HMD in FPV.

IV. USER STUDY 2

C. Experiment setup

Based on the above results of Study 1, we combined D2
and D4 to create a new display mode - D5. This version
would show participants a ﬁxed big screen in VR and
allow them to have 3DS vision by giving each eye different
images from the corresponding binocular camera. The only
difference between D4 and D5 was whether the screen was
movable or ﬁxed (see Fig. 6).

Fig. 6.
(a) A picture of the UGV’s camera(s) watching a board. (b) View
of the left camera of the binocular cameras. (c) View of the right camera
of the binocular cameras. (d) Images for the two eyes in D4 and (e) in D5.

The rest of experiment setup was the same as in Study 1.

D. Hypotheses

Based on Study 1, we formulated the following two

hypotheses:

• H4: According to the objective results of Study 1, D5
would perform better than D4 overall and especially in
T3.

• H5: According to the subjective results of Study 1, D5
would be more preferred by participants than D4.

E. Results

A Shapiro-Wilk test for normality was performed on each
of measures separately for each condition and showed that
they all followed a normal distribution. Fig. 7a and 7b show
the performance of the two modes. From these ﬁgures, we
can see that:

• For overall performance, the independent-samples t-test
showed that D5 signiﬁcantly reduced the number of
collisions (t(78) = 3.473, p <.001) compared to D4.

(a) Overall mean and std. deviation of number of collisions for the two conditions (D4 and D5); (b) Local mean and std. deviation of number
Fig. 7.
of collisions for the two conditions (D4 and D5). All Error bars show +1 SD; (c) Average of response scores for each element of NASA-TLX workload.
Higher scores represent more demands in all cases; (d) Average of response scores for each element of UEQ. Higher scores represent more preferences in
all cases. The ’×’ symbol represents the mean value. Error bars show ±1 SD.

• In local performance, the t-test showed that D5 signif-
icantly reduced the number of collisions compared to
D4 in T1 (t(14)=3.473 = 3.631, p <.05), T3 (t(14) =
2.395, p <.05), and T5 (t(14) = 3.147, p <.05).

Fig. 7c and 7d show the summary of the NASA-TLX and

UEQ questionnaire data. An analysis shows that:

• There was a signiﬁcant difference in the NASA-TLX
workload in Effort (χ2(1) = 7.345, p <.05). Participants
needed more effort using D4.

• There was also a signiﬁcant difference in the UEQ data
in Attractiveness (χ2(1) = 10.678, p <.05). Participants
preferred D5 signiﬁcantly more.

F. Discussion

From the results, we can observe that participants per-
formed better in D5, as indicated by an overall lower number
of collisions (especially in T3). This result conﬁrms H4.
However, we also found that D5 had better performance
in T1 and T5, which we had not thought it would be the
case. The NASA-TLX results also show a similar trend.
Participants did not need much effort to ﬁnish the tasks
using D5. In addition, the UEQ data showed that participants
signiﬁcantly liked D5 more, which supported H5. All the
above results show that D5 led to better performance than D4
in overall (local) performance, required lower demands from
participants, and led to an enhanced user experience. These
results suggest that an immersive display with a ﬁxed screen
using stereoscopic images (like D5) is a viable approach
to allow operators remotely control a UGV when obstacle
avoidance is essential.

V. LIMITATIONS AND FUTURE WORK

From the results of the two studies, we can observe that an
immersive display with stereoscopic images can signiﬁcantly
increase user performance in obstacle avoidance tasks. They
also indicate that a ﬁxed screen in VR led to a better
performance than a screen that moves together with head
motions.

This research has the following two limitations, which can
serve as directions for future work. Even with binocular
cameras,
is still not possible to signiﬁcantly increase
the environmental information because of the limitations of
the camera’s FOV. If users do not manipulate the camera,
they are not able to see a wide view of the environment

it

on a single screen. Cameras that can capture wide views,
including 360° panoramic ones, around the robot could be
one possible solution but further research is needed to assess
their suitability because such views will require users to
move their heads frequently, which could lead to higher
levels of motion sickness.

In this research, we explored views on two types of
displays and did not
investigate image distortion or en-
hancement approaches. Given that the immersive VR display
with stereoscopic view led to better performance and higher
usability, it will be interesting to see if distorting the images
(to enhance certain elements for example) could allow even
better performance. Applying edge enhancement could po-
tentially improve the perception of the contours of obstacles
and may let operators gauge distance information in a more
precise way. This line of research could produce useful and
interesting results and applications.

In addition, another way to help improve manoeuvra-
bility to avoid obstacles is to use operators’ physiological
information that can be captured during teleoperation. For
example, eye gaze is readily available and implicitly provided
by operators. Recent research shows that gaze data can be
used to improve object manipulation in VR environments
[36]. Typically, when approaching an obstacle to be avoided,
operators’ gaze would likely be ﬁxated on that object and,
when a ﬁxation is longer than a threshold, the system can
provide additional information to help steer the UGV towards
the most optimal path. As eye trackers are becoming a
common feature of HMD, using gaze data could represent a
low-cost and efﬁcient approach for improving operator-UGV
interaction. Further research is needed to explore how this
can be achieved.

VI. CONCLUSION

In this paper, we have explored four viewing/display
modes for real-time unmanned ground vehicle (UGV) con-
trol in obstacle avoidance tasks. The aim is to investigate
whether these modes allow users to determine UGV-to-
obstacle distances. Study 1 evaluated the performance and
user preference of the four modes in a maze with moving
and static objects that the UGV had to avoid. Results from
this study show that the version with stereoscopic images
displayed in a virtual reality head-mounted display (VR
HMD) led to better performance and usability. In Study

2, a new display mode that combines two modes from
Study 1 was compared with the best-performing mode (VR
with stereoscopic view). Overall, our results indicate that an
immersive VR display with a ﬁxed screen using stereoscopic
images is an applicable and suitable approach for improving
depth perception when controlling a UGV in real-time in
obstacle avoidance tasks, whether static or moving. It also
helped participants lower their workload levels and led to an
enhanced user experience.

ACKNOWLEDGMENT

The authors would like to thank the participants for
their time and the reviewers for their reviews and useful
comments.

REFERENCES

[1] N. Smolyanskiy and M. Gonzalez-Franco, “Stereoscopic ﬁrst person
view system for drone navigation,” Frontiers in Robotics and AI, vol. 4,
p. 11, 2017.

[2] J. S. McCarley and C. D. Wickens, “Human factors concerns in
uav ﬂight,” University of Illinois at Urbana-Champaign Institute of
Aviation, Aviation Human Factors Division, 2004.

[3] R. Pausch, D. Profﬁtt, and G. Williams, “Quantifying immersion in
the 24th annual conference on

virtual reality,” in Proceedings of
Computer graphics and interactive techniques, 1997, pp. 13–18.
[4] W. Xu, H.-N. Liang, K. Yu, and N. Baghaei, “Effect of gameplay
uncertainty, display type, and age on virtual reality exergames,” in
Proceedings of
the 2021 CHI Conference on Human Factors in
Computing Systems, 2021, pp. 1–14.

[5] J. P. McIntire, P. R. Havig, and E. E. Geiselman, “Stereoscopic 3d
displays and human performance: A comprehensive review,” Displays,
vol. 35, no. 1, pp. 18–26, 2014.

[6] A. B. Oving and J. B. van Erp, “Driving with a head-slaved camera
system,” in Proceedings of the Human Factors and Ergonomics Society
Annual Meeting, vol. 45, no. 18. SAGE Publications Sage CA: Los
Angeles, CA, 2001, pp. 1372–1376.

[7] C. Smyth, V. Paul, A. Meldrum, and K. McDowell, “Examining alter-
native display conﬁgurations for an indirect vision driving interface,”
US Army Research Laboratory: Aberdeen Proving Ground, MD, to be
published.

[8] D.-H. Kim, Y.-G. Go, and S.-M. Choi, “First-person-view drone ﬂying

in mixed reality,” in SIGGRAPH Asia 2018 Posters, 2018, pp. 1–2.

[9] A. C. Plascencia, V. Beran, and K. Sedlmajer, “Drone sensory data
processing for advanced drone control for augmented reality.” 2019.
[10] Z. Chen, X. Luo, and B. Dai, “Design of obstacle avoidance system
for micro-uav based on binocular vision,” in 2017 International Con-
ference on Industrial Informatics-Computing Technology, Intelligent
Technology, Industrial Information Integration (ICIICII).
IEEE, 2017,
pp. 67–70.

[11] H. Duan, H. Li, Q. Luo, C. Zhang, C. Li, P. Li, and Y. Deng, “A
binocular vision-based uavs autonomous aerial refueling platform,”
Science China Information Sciences, vol. 59, no. 5, pp. 1–7, 2016.

[12] Y. Ma, Q. Li, L. Chu, Y. Zhou, and C. Xu, “Real-time detection and
spatial localization of insulators for uav inspection based on binocular
stereo vision,” Remote Sensing, vol. 13, no. 2, p. 230, 2021.

[13] W. Xu, H.-N. Liang, Z. Zhang, and N. Baghaei, “Studying the effect
of display type and viewing perspective on user experience in virtual
reality exergames,” Games for health journal, vol. 9, no. 6, pp. 405–
414, 2020.

[14] D. Monteiro, H.-N. Liang, J. Wang, H. Chen, and N. Baghaei, “An
in-depth exploration of the effect of 2d/3d views and controller
types on ﬁrst person shooter games in virtual reality,” in 2020 IEEE
International Symposium on Mixed and Augmented Reality (ISMAR).
IEEE, 2020, pp. 713–724.

[15] R. P. Darken, K. Kempster, and B. Peterson, “Effects of streaming
video quality of service on spatial comprehension in a reconnaissance
task,” 2001.

[16] J. Y. Chen, E. C. Haas, and M. J. Barnes, “Human performance issues
and user interface design for teleoperated robots,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
vol. 37, no. 6, pp. 1231–1245, 2007.

[17] B. G. Witmer and W. J. Sadowski Jr, “Nonvisually guided locomotion
to a previously viewed target in real and virtual environments,” Human
factors, vol. 40, no. 3, pp. 478–488, 1998.

[18] J. B. Van Erp and P. Padmos, “Image parameters for driving with
indirect viewing systems,” Ergonomics, vol. 46, no. 15, pp. 1471–
1499, 2003.

[19] D. Drascic, “An investigation of monoscopic and stereoscopic video

for teleoperation.” 1993.

[20] ——, “Skill acquisition and task performance in teleoperation using
monoscopic and stereoscopic video remote viewing,” in Proceedings
of the Human Factors Society Annual Meeting, vol. 35, no. 19. SAGE
Publications Sage CA: Los Angeles, CA, 1991, pp. 1367–1371.
[21] J. Y. Chen, E. C. Haas, and M. J. Barnes, “Human performance issues
and user interface design for teleoperated robots,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
vol. 37, no. 6, pp. 1231–1245, 2007.

[22] D. R. Lampton, D. P. McDonald, M. Singer, and J. P. Bliss, “Distance
estimation in virtual environments,” in Proceedings of the human
factors and ergonomics society annual meeting, vol. 39, no. 20. SAGE
Publications Sage CA: Los Angeles, CA, 1995, pp. 1268–1272.
[23] D. R. Scribner and J. W. Gombash, “The effect of stereoscopic and
wide ﬁeld of view conditions on teleoperator performance,” ARMY
RESEARCH LAB ABERDEEN PROVING GROUND MD HUMAN
RESEARCH AND ENGINEERING . . . , Tech. Rep., 1998.

[24] K. M. Stanney, R. R. Mourant, and R. S. Kennedy, “Human factors
issues in virtual environments: A review of the literature,” Presence,
vol. 7, no. 4, pp. 327–351, 1998.

[25] E. M. Kolasinski, Simulator sickness in virtual environments. US
Army Research Institute for the Behavioral and Social Sciences, 1995,
vol. 1027.

[26] A. Rastogi, “Design of an interface for teleoperation in unstructured

environments using augmented reality displays.” 1997.

[27] B. G. Witmer and P. B. Kline, “Judging perceived and traversed
distance in virtual environments,” Presence, vol. 7, no. 2, pp. 144–
167, 1998.

[28] D. E. McGovern, “Experience and results in teleoperation of land
vehicles,” in Pictorial communication in virtual and real environments
(2nd ed.), 1993, pp. 182–195.

[29] C. W. Nielsen, B. Ricks, M. A. Goodrich, D. Bruemmer, D. Few,
and M. Few, “Snapshots for semantic maps,” in 2004 IEEE Interna-
tional Conference on Systems, Man and Cybernetics (IEEE Cat. No.
04CH37583), vol. 3.

IEEE, 2004, pp. 2853–2858.

[30] B. Ricks, C. W. Nielsen, and M. A. Goodrich, “Ecological displays for
robot interaction: A new perspective,” in 2004 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No.
04CH37566), vol. 3.

IEEE, 2004, pp. 2855–2860.

[31] A. Steinfeld, T. Fong, D. Kaber, M. Lewis, J. Scholtz, A. Schultz,
and M. Goodrich, “Common metrics for human-robot interaction,” in
Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-
robot interaction, 2006, pp. 33–40.

[32] J. O. Merritt, V. G. CuQlock-Knopp, M. Kregel, J. Smoot, and
W. Monaco, “Perception of terrain drop-offs as a function of lr view-
point separation in stereoscopic video,” in Helmet-and Head-Mounted
Displays X: Technologies and Applications, vol. 5800.
International
Society for Optics and Photonics, 2005, pp. 169–176.

[33] J. Weng, P. Cohen, M. Herniou et al., “Camera calibration with
distortion models and accuracy evaluation,” IEEE Transactions on
pattern analysis and machine intelligence, vol. 14, no. 10, pp. 965–
980, 1992.

[34] H. B. Pryor, “Objective measurement of interpupillary distance,”

Pediatrics, vol. 44, no. 6, pp. 973–977, 1969.

[35] B. Sredojev, D. Samardzija, and D. Posarac, “Webrtc technology
overview and signaling solution design and implementation,” in 2015
38th international convention on information and communication
technology, electronics and microelectronics (MIPRO).
IEEE, 2015,
pp. 1006–1009.

[36] D. Yu, X. Lu, R. Shi, H.-N. Liang, T. Dingler, E. Velloso, and
J. Goncalves, “Gaze-supported 3d object manipulation in virtual
reality,” in Proceedings of the 2021 CHI Conference on Human Factors
in Computing Systems, 2021, pp. 1–13.

