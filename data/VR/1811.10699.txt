8
1
0
2

v
o
N
9
2

]

V
C
.
s
c
[

2
v
9
9
6
0
1
.
1
1
8
1
:
v
i
X
r
a

Time-Aware and View-Aware Video Rendering
for Unsupervised Representation Learning

Shruti Vyas
shruti@crcv.ucf.edu

Yogesh S Rawat
yogesh@crcv.ucf.edu

Mubarak Shah
shah@crcv.ucf.edu

Center for Research in Computer Vision
University of Central Florida

November 30, 2018

Abstract

The recent success in deep learning has lead to various ef-
fective representation learning methods for videos. How-
ever, the current approaches for video representation re-
quire large amount of human labeled datasets for effec-
tive learning. We present an unsupervised representation
learning framework to encode scene dynamics in videos
captured from multiple viewpoints. The proposed frame-
work has two main components: Representation Learning
Network (RL-NET), which learns a representation with
the help of Blending Network (BL-NET), and Video Ren-
dering Network (VR-NET), which is used for video syn-
thesis. The framework takes as input video clips from
different viewpoints and time, learns an internal represen-
tation and uses this representation to render a video clip
from an arbitrary given viewpoint and time. The ability
of the proposed network to render video frames from ar-
bitrary viewpoints and time enable it to learn a meaning-
ful and robust representation of the scene dynamics. We
demonstrate the effectiveness of the proposed method in
rendering view-aware as well as time-aware video clips
on two different real-world datasets including UCF-101
and NTU-RGB+D. To further validate the effectiveness
of the learned representation, we use it for the task of
view-invariant activity classiﬁcation where we observe a
signiﬁcant improvement (∼ 26%) in the performance on
NTU-RGB+D dataset compared to the existing state-of-
the art methods.

Figure 1: An overview of the proposed video rendering frame-
work. An activity is captured from different viewpoints (v1, v2,
and v3) providing observations (o1, o2, and o3). Video clips
from these viewpoints (v1 and v2) at arbitrary times (t1 and t2)
are used to learn a scene and dynamics representation (r) for
this activity, employing the proposed RL-NET. The learned rep-
resentation (r) is then used to render a video from an arbitrary
query viewpoint (v3) and time (t3) using proposed VR-NET.

1

Introduction

In recent years, we have seen some success in the research
on video synthesis. The proposed methods are mainly fo-
cused on future frame prediction [18, 1, 3], future clip
prediction [33, 37, 12], or conditioned video generation
[31, 2, 28, 23]. The future frame/clip prediction meth-
ods can synthesize frames from near future in the video
with some realism. The video generation works mainly

1

 
 
 
 
 
 
leverage the Generative Adversarial Network [7] for real-
istic video synthesis based on some conditioning. All of
these methods tackle the problem of video synthesis from
a single viewpoint and focus on the time dimension of the
video. However, wouldn’t be interesting if we can bring
in the notion of viewpoint into video synthesis as well?
This is what we address in this paper.

The diversity in a scene and objects dynamics, along
with camera motion, makes video synthesis of real-world
scenarios a very challenging problem. Adding the notion
of viewpoint makes it even more difﬁcult, as the observed
video of the same scene and dynamics will vary from one
viewpoint to another. Historically, view-point in-variance
has been a very active research area in computer vision,
and is currently also important from the perspective of
representation learning. There have been some work in
view-aware image synthesis [22, 5], where given images
captured from different views one can synthesize an im-
age from an unseen view. Most intriguing recent work
in this area is by Eslami et. al[5], who proposed a sim-
ple neural network approach for rendering images from
unseen views for synthetic data. However, to the best of
our knowledge, the problem of view-aware video synthe-
sis has not been addressed yet.
Inspired by Eslami et.
al[5], in this paper, we explore generalization of their idea
to video and propose time-aware and view-aware video
rendering for real-world videos instead of synthetic im-
ages.

The proposed framework, shown in Figure 1, takes
multiple video clips from varying view-points and times
as input, and renders a video from a given an arbitrary
viewpoint and time. The learned representation has an
understanding of temporal and view-point signiﬁcance of
the input videos. The ability of the network to render a
video from any given viewpoint and time, enables the net-
work to learn a robust view and time aware representation,
which can be employed for view-invariant activity classi-
ﬁcation.

We make the following contributions in this paper.
We introduce a novel view and time-aware video render-
ing problem and propose an unsupervised representation
learning framework to solve this. The proposed frame-
work bears an understanding of time and view-point and
therefore the learned representation can be used to gener-
ate videos from arbitrary given viewpoint and time. We
also demonstrate the effectiveness of the proposed frame-

work for future video prediction. We further validate the
robustness of the learned representation for view-invariant
action classiﬁcation, where we observe a signiﬁcant im-
provement (∼ 26%) in the performance, when compared
with the state-of-the art methods on NTU-RGB+D dataset
using RGB modality.

2 Related Work

The research in image synthesis [19, 9, 34] has recently
seen a great progress, and it is mainly attributed to the suc-
cess of Generative Adversarial Networks (GAN) [7]. This
includes methods for generating high-resolution images
[13, 34] along with image-to-image translation [38, 15, 4].
However, video synthesis is still a landmark challenge and
the research is in preliminary stage.

Our work is closely related to the research in video syn-
thesis, which explores future frame prediction [18, 1, 3],
future clip prediction [37, 12], and conditioned video gen-
eration [33, 31, 2, 28, 23]. In one of the early attempts
on video prediction, the authors in [18] proposed a GAN
based approach for next-frame prediction, where they ex-
plore different types of loss functions along with adversar-
ial loss. Similarly, the authors in [33, 2, 37, 12, 31, 23, 28]
explored the GAN framework for video synthesis, where
they focus on future frame prediction and conditional
video generation. The authors in [1] recently proposed
a variational latent space learning framework for video
synthesis, and similarly the authors in [3] also explored a
recurrent approach for next frame prediction. Our work
is distinctly different from these approaches as we have a
notion of viewpoint, which has not been addressed ear-
lier. Apart from this, our proposed method is time-aware,
and therefore given a single-view video it can also be used
for future frame prediction.

In addition, our work is also related to unsupervised
video representation learning [29, 35, 8]. The research
in unsupervised video representation learning mainly fo-
cuses on encoder-decoder [11] kind of networks, where
the decoder is used for reconstruction of the input video.
The authors in [29] proposed a recurrent network for both
encoding and decoding which was based on LSTM. Sim-
ilarly, the authors in [8] proposed to learn a latent distri-
bution instead of encoding for video representation. Our
approach resembles to these methods as we also learn a

2

representation. However, instead of learning a represen-
tation for a single video, we learn a representation for
the whole scene and its dynamics captured from differ-
ent viewpoints. Also, instead of trying to generate the
input video, we aim to generate a video from a different
viewpoint and time.

Cross-view synthesis of data is an interesting problem,
which can have multiple applications including view-
invariant representation learning. There are some exist-
ing works focusing on this problem for image synthesis
[22, 5]. In [21], the authors proposed a GAN based ap-
proach, where they perform a image-to-image translation
from aerial to ground view images. In [5], the authors pro-
posed a scene representation learning framework where
a representation is learned for a scene and a view of the
same scene is generated from a different viewpoint. How-
ever, there has been no work in cross-view video synthe-
sis. Our work is inspired from [5], which was mainly fo-
cused on synthetic images and we generalize it to real-
world videos. Apart from this, we also propose the notion
of time awareness in the viewpoint which is intuitive from
the perspective of videos.

We demonstrate the effectiveness of the learned repre-
sentation for view-invariant activity classiﬁcation.
It is
an active research topic in computer vision community
[26] and most of the existing works are focused on multi-
ple modalities, such as depth [20] and pose [25], besides
RGB data [17, 14]. We mainly focus on RGB modality
and present a comparison with existing works.

3 Method

al[5],

Following pioneering work of Eslami et.
the
proposed method consists of two components, a repre-
sentation learning network, f (RL-NET), and a video
rendering network, g (VR-NET). The RL-NET takes
multiple video clips captured from varying viewpoints
termed as observations oi =
and time of an event
{(xk
i represents kth video
clip captured from viewpoint vk
i and time ti for any event
i. The RL-NET take these observations and learns a com-
prehensive representation, ri, of the event with the help
of Blending Network (BL-NET), preserving the view and
time notion. BL-NET is a recurrent network maintain-
ing an internal representation, which is updated as more

i , ti)}k=1,2,...,K, where, xk

i , vk

observations are seen by the network before providing a
holistic representation r of the scene and its dynamics.
The VR-NET, then, use this representation, r, along with
stochastic latent variables, z, to render a video clip from
an arbitrary viewpoint, vk

i , and time, ti.

Formally, we can deﬁne the representation learning as,

ri = fθ(oi), and the video rendering network as,

gθ(x|vq, tq, r) =

(cid:90)

gθ(x, z|vq, tq, r)dz,

(1)

where, gθ(x|vq, tq, r) represents a probability density of
a video x observed from a viewpoint vq at time tq, for
a scene with representation r and latent variable z. We
train the two networks, RL-NET and VR-NET, jointly in
an end-to-end fashion to maximize the likelihood of ren-
dering the ground-truth video, observed from the query
viewpoint and timestep. A detailed overview of the pro-
posed framework is shown in Figure 2.

3.1 Representation Learning Network (RL-

NET)

We use a convolution based neural network to learn the
scene representation. RL-NET is shown in Figure 2C, the
input consists of multiple video clips along with view-
point and time conditioning. The view-point and time
conditioning is applied on encodings generated after few
convolutions on input clip. We use concatenation of view-
point, and time with the convolution features from the
video for conditioning. It is followed by some more con-
volution layers. The ﬁnal encodings from each observa-
tion are then combined together to learn a uniﬁed scene
and dynamics representation, r, using a blending network.
The encoding network is shared among all the observa-
tions.

We explore both 2D and 3D convolution based net-
works, with conditioning as additional input and gener-
ate an encoding for each observation. The learned en-
codings are, then, used to learn the scene representation,
r, with size, (X, Y, Z) (Figure 2.E). The scene represen-
tation is learned using a convolutional recurrent network
(BL-NET), which accumulates information from all the
observations effectively (Figure 2.E). The work in [5] pro-
posed a simple addition of encodings to learn the scene
representation. We propose a recurrent network, instead,

3

Figure 2: Outline of the proposed view and time-aware video rendering framework. A : A collection of observations (o) for a given
activity from different viewpoints. B: Randomly selected clips from the set of observations captured from different viewpoints and
at different times. C: Representation learning network (RL-NET), which takes video clips from different viewpoint and time as
input and learns a representation (r). It consists of two parts: Basenet and Blending network. D: Basenet is used to learn individual
video encodings (ei) conditioned on its viewpoint vi and time ti and it is based on 3D convolutions. E: The blending network
combines encodings learned from different video clips into a uniﬁed representation r. This blending is performed using a recurrent
network which learns a correlation between individual encodings and to get r. F: Finally, the representation r is used to synthesize
a video from arbitrary viewpoint vq and time tq using VR-NET which is based on 3D convolution. (ConvA and ConvB refers to
convolution and U refers to Upsampling layers.)

which we found more effective both in terms of perfor-
mance and training efﬁciency.

Blending Network (BL-NET) We want to learn a rep-
resentation which holistically represents the scene, and its
dynamics as viewed from varying viewpoints. We pro-
pose a recurrent network which updates its representa-
tion after looking at each observation (Figure 2E). More
speciﬁcally, we utilize an LSTM architecture [6], where
the memory cell, c, acts as an accumulator of state in-
formation and is updated by the input (i), output (o) and
forget (f ) gates, which are self-parameterized controlling
gates. The order in which the observations are seen by
the cell should not have any role in the learned represen-
tation, therefore we propose to use bi-directional layers
[24] in the network for a more effective learning. Also,
to preserve the spatial information in the embeddings, we
make use of convolutional LSTM [36]. For a given video
embedding, ei, after seeing all other observations in a for-
ward and a backward pass, we get an updated hidden rep-

resentation hr
i .

i = (of
hr

i ◦ tanh(cf

i ))(cid:95)(ob

i ◦ tanh(cb

i )).

(2)

i and ob

i and cb

Here, of
i are the output gates of the forward pass
and backward pass respectively, cf
i are the cor-
responding memory cell states, ◦ denotes the Hadamard
product, and (cid:95) denotes a concatenation operation be-
tween learned representations from the forward and back-
ward pass. The updated intermediate representation from
each observation is then passed to a uni-directional LSTM
layer, which accumulates these to get a holistic represen-
tation r.

r = on ◦ tanh(cn).

(3)

Here, on is the output gate, cn is the memory cell state of
the network after seeing all the n observations.

3.2 Video Rendering Network (VR-NET)

The representation, r, learned based on the given observa-
tions, o, is used to render a video with a video rendering
network (VR-NET). The VR-NET, shown in Figure 2F, is

4

also a convolution based network which takes as input the
learned representation, r, along with query viewpoint, vq,
time tq, and latent noise z. The viewpoint vq, time tq, and
noise z are feed to the network as conditioning, for which
we use concatenation operation with the representation
features. The VR-NET consists of 2D convolutions fol-
lowed by 3D convolutions to render the video clips. The
convolution layers are used in combination with upsam-
pling of features to generate video clips with resolution
same as the input observations.

The two networks, RL-NET and VR-NET, are trained
jointly in an end-to-end fashion minimizing the recon-
struction loss Lr as the objective function. The recon-
struction loss Lr is computed as mean squared error be-
tween the predicted V p and the ground truth video clip
V g.

Lr =

1
N

N
(cid:88)

F
(cid:88)

H
(cid:88)

W
(cid:88)

C
(cid:88)

n

i

j

k

m

||V p

ijkm − V g

ijkm||2.

(4)

Here, N is the number of samples, F is the number of
frames in the clip, H, W is height and width of the video
frames, and C = 3 for three RGB color channels (more
details in supplementary ﬁle).

4 Activity Recognition

The learned representation can render view-aware as well
as time-aware video clips. To further explore the effec-
tiveness of the learned representation, we use it for the
task of view-invariant action recognition. We modify the
same RL-NET and VR-NET framework and add convo-
lution layers followed by fully connected layers on top of
the representation features. This branch of the network
(CL-NET) predicts probabilities for each action classes
(Figure 7b). We use categorical cross entropy to compute
the loss Lc for the classiﬁcation branch.

Lc = −

1
N

N
(cid:88)

C
(cid:88)

n

c

1yi∈Cc log(ˆp[yi ∈ Cc]).

(5)

Here, C is the number of action categories, and ˆp[yi ∈ Cc]
is the predicted probability for this sample corresponding
to category c. The modiﬁed network is trained end-to-end
with the two loss functions (Lr and Lc) in a multi-task

5

setting and the overall loss of the network is deﬁned as,

L = λ1 × Lr + λ2 × Lc.

(6)

In all our experiments, we use λ1 = λ2 = 1. The network
is trained using observations captured from certain views
and later tested on observations from unseen views.
In
another variant, we also explore the proposed framework
for multi-view action recognition, where all the views are
provided during the classiﬁcation training.

5 Experiments

We perform our experiments on two different real-world
datasets: UCF-101 [27], and NTU-RGB+D [25]. UCF-
101 does not have the notion of viewpoint, therefore, we
use it for time-aware video rendering experiments. NTU-
RGB+D is a large scale dataset with videos captured from
multiple viewpoints. We use it for view-ware video ren-
dering and view-invariant representation learning, which
is further explored for view-invariant activity classiﬁca-
tion.

5.1 Datasets

UCF-101:The UCF-101 dataset [27] covers a wide range
of activities and has around 13K video samples with 101
action classes. There are three different splits in this
dataset and we use split-1 for our experiments. NTU-
RGB+D: This human activity recognition dataset con-
tains more than 56K videos and 4 millions frames with
60 different actions, including individual activities, inter-
actions between 2 people and health related events. There
are a total of 40 different actors, who perform actions
captured from 80 different viewpoints. The authors pro-
posed two different splits in this dataset, cross-view and
cross-subject. We perform our classiﬁcation experiments
on both the splits as suggested in [25]. Apart from this,
we use cross-view split for view-invariant rendering and
cross-subject split for other rendering experiments.

5.2 Training

We perform the pre-processing of video frames in UCF-
101 as suggested in [30], and take a random crop of
112x112 on the resized frames. In the time-aware frame

Method
[18]
[32]
[16]
[3]
Ours

PSNR
32
31
33.4
34.9
31.9

SSIM
0.92
0.91
0.94
0.92
0.95

Figure 3: Images generated with time conditioning using 2D-
CNN representation on UCF-101. Row-1: LSTM generator and,
Row-2: CNN generator and Row-3: Ground truth.

rendering experiments, the RL-NET takes 6 frames in a
video randomly and the VR-NET generates a video frame
from arbitrary time in the clip. In video rendering experi-
ments, the input video clips have 6 frames and we feed in
3 randomly selected clips to the RL-NET for representa-
tion learning, and VR-NET generates a video clip with 6
frames from arbitrary time.

In all our experiments on NTU-RGB+D dataset, we use
subject split, except for the view-split based classiﬁcation
task. For view-based rendering, the subject training split
is further divided to get the desired view. We resize the
video frames to 240x135, preserving the aspect ratio, and
then take a random 112x112 crop for training. The input
clips with 6 frames are randomly selected from each view,
with varying time, and are used for representation learn-
ing, and subsequently the VR-NET generates a video clip
with 6 frames from arbitrary view and time. The addi-
tional view-point information is used along with time for
representation learning as well as video rendering.

In all our experiments we use Adam optimizer [10]
with a learning rate of 1e-4 and a batch size of 6. We
implemented our code in Keras with Tensorﬂow backend
and use Titan-X GPU for training our network.

5.3 Time-aware Rendering

Building upon view-invariant neural representation for
image generation using synthetic data [5], we learn a
representation encompassing the temporal domain. The
video representation r is learned conditioned on the time
factor using a convolution network (RL-NET). The rep-
resentation r, with an understanding of the temporal do-

Table 1: Comparison of the quality of future-frame prediction
on 10% of UCF-101 test dataset. The network is provided 4
frames to predict the next frame.
It is important to note that
the other methods were trained speciﬁcally to predict the future
frame whereas our network was trained to predict any arbitrary
frame in the video.

main, is used to generate an image/clip at arbitrary time
in the video with the help on VR-NET.

In their work [5], related to view-invariant image rep-
resentation, the authors use an iterative LSTM generator.
Drawing on similar lines, we explore a recurrent LSTM
generator (VR-NET) for image generation. Apart from
this, we also explore a CNN generator (VR-NET) with
similar training conditions. The images generated by both
the networks are shown in Figure 3 for a comparison. In
the absence of any marked difference between the quality
of generated images, we choose a CNN generator (VR-
NET) for further experiments, which is faster to train in
comparison with an LSTM network. The representation
learned by this network can also predict 3-4 frames in fu-
ture or the whole video frame by frame. A comparison of
the generated image quality with other methods is shown
in Table 1. We use the same testing setup as suggested
in [3]. The method proposed by [3] performs better in
terms of PSNR values, however, it is important to note
that this method was speciﬁcally trained for future frame
prediction, whereas our network was trained for predict-
ing arbitrary frames in the video. Moreover, our network
outperforms all the other methods in terms of SSIM score.
Next, we explore the video generation capability of the
proposed framework. We experimented with both 2D and
3D convolutions to learn a representation with 3D-CNN
based RL-NET. The input to RL-NET is a set of frames
from arbitrary time in case of 2D-CNN and a set of video
clips in case of 3D-CNN. The frames from the generated
videos for some of the UCF-101 action classes are shown
in Figure 4. The videos generated using 3D-CNN based

6

FF

FV

LF
PSNR SSIM PSNR SSIM PSNR SSIM
21.4
24.68
19.9
19.9
23.3

-
22.97
19.8
19.8
23.3

17.7
21.66
19.8
19.9
23.2

-
0.73
0.68
0.68
0.79

0.58
0.67
0.68
0.69
0.79

0.69
0.79
0.68
0.69
0.79

[18]
Ours
O-1
O-2
O-3

Table 2: Quantitative evaluation of the future video predictions
on 10% UCF-101 and time as well as view-aware prediction on
NTU-RGB+D test set. The ﬁrst two rows are for UCF-101 and
the bottom 3 rows are our results on NTU-RGB+D dataset. FF:
ﬁrst frame, LF: last frame, and FV: full video. O-1: input is 2-
random views from (1,2,3) and output is the left out view, O-2:
input is views 2 and 3 and output is view 1, O-3: input is from
all 3 views and output is from random view. The query time is
randomly chosen for all the experiments.

RL-NET are relatively better when compared with 2D-
CNN. We compared our method with [18] for future video
prediction and observed an improvement in the predicted
video quality based on PSNR and SSIM measures (Table
2).

5.4 View Invariant Video Generation

We perform our view-invariant video rendering experi-
ments on NTU-RGB+D dataset. We train the network
with view-point conditioning along with time condition
to render videos from arbitrary view-points. A viewpoint
is deﬁned using 6 different parameters: camera height,
camera distance, view-point, horizontal-pan, vertical-pan,
and actor position (refer to supplementary ﬁles for more
details).

We ﬁrst explore the effectiveness of the proposed
blending network (BL-NET) for representation learning.
We use a 2D-CNN based RL-NET along with proposed
BL-NET to learn a representation r for a given set of
frames captured from different view-points and time. The
learned representation r is used then used to render a
frame from arbitrary viewpoint and time. We perform
similar experiment with addition in place of BL-NET for
representation learning. Some of the generated images
with these experiments are shown in Figure 5. We can
observe that the quality of generated images is much bet-

Figure 4: Future video frames rendered for UCF101 dataset,
with a time conditioned 3D convolution VR-NET; top-three-
rows are generated with: 2D convolution RL-NET, Row-1: Ap-
ply lipstick, Row-2: Writing on board, and Row-3: Playing vio-
lin; and bottom-three-rows use 3D convolution RL-NET: Row-
4: Apply lipstick, Row-5: Writing on board, and Row-6: Play-
ing violin.

Figure 5: Images generated with time and view conditioning
using 2D-CNN network on NTU-RGB+D dataset, where rep-
resentation uses a 2D-CNN network with 2 different blending
techniques: Row-1: Addition; Row-2: BL-NET; and Row-3:
Ground truth.

ter with BL-NET representation. Apart from this, we also
compare the variation in loss and learning curve for these
two blending techniques (Figure 6). We observe a faster
learning and a better performance for BL-NET.

We use a 3D-CNN based RL-NET along with BL-NET
blending to learn r for a given set of video clips from

7

Figure 6: A performance comparison between two blending
techniques: Addition and BL-NET. Left plot shows the change
in loss and the right plot shows the change in accuracy as train-
ing progresses. The accuracy is computed using per-pixel pre-
diction at a certain threshold. We observe a faster convergence
and a better performance with BL-NET as compared to addition
blending.

Figure 8: Generated and ground truth video frames (action
class- 55, hugging other person) at different time-steps gener-
ated using models trained under different conditions. Row-1-2:
Given two random views generates third unseen view, Row-3-
4: Given view 2 and 3 generates unseen view 1, and Row-5-6:
Given all three views generates one given view at an arbitrary
query time.

views (row 5 & 6). We observe that all the three varia-
tions generate video-clips with correct view-point on the
test-set. Although we observe some blur in the regions
with motion, the quality of the videos generated is best
with the third variation. Some more synthesized video
frames for this variation are shown in Figure 9.

We also perform a quantitative evaluation of the pro-
posed view and time aware rendering approach. We com-
pute PSNR and SSIM measures to evaluate the quality of
generated videos. The evaluation scores are shown in row
3-5 of Table 2. We observe that the quality of generated
videos is better when all the views are seen by the net-
work as compared with generating unseen views which
was expected.

5.5 Activity Classiﬁcation

We further explore the effectiveness of the proposed rep-
resentation learning approach for view-invariant activity

Figure 7: (a) Generalized network architecture for video ren-
dering using 3D-CNN RL-NET and VR-NET. (b) Multitasking
network for classiﬁcation along with video rendering.

different viewpoints and time, which is then passed to a
3D-CNN based VR-NET for video rendering from an ar-
bitrary viewpoint and time. (Figure 7a). We experimented
with three different variations in the training setup based
on the input and rendered videos (Figure 8). We start
with a challenging setup, where the input clips are from
(cid:1) (1,2,3), i.e. two randomly selected views from three
(cid:0)3
2
views (1, 2, 3) and VR-NET renders the third unseen view
(Figure 8 row 1 & 2). In the second variation, the input
clips are from view 2 and 3 and VR-NET always gener-
ates view 1 (row 3 & 4). In the third variation, the in-
put was given from all three views and VR-NET renders
a video from a randomly selected view among the three

8

Figure 9: Video frames generated with time and view condi-
tioned network with three views. Video generated contains fu-
ture frames for one of the given views and action classes, Row-1:
class 18(wear on glasses) , Row-2: class 57(touch other person’s
pocket), and Row-3: class 43(falling).

classiﬁcation. We modify the proposed framework for
multi-tasking (Figure 7b) where the network is jointly
trained for video rendering as well as activity classiﬁca-
tion. Although, multi-tasking with classiﬁcation does not
improve rendering quality, we obtain state-of-the-art ac-
tivity classiﬁcation results on NTU-RGB+D dataset using
RGB modality. We experimented with different training
variations based on the input to RL-NET (different com-
binations of views and number of clips) and query to VR-
NET.

The RL-NET performs representation learning based
on a set of input observations. These observations can
be from different viewpoints (varying views) and differ-
ent time in a video (Figure 10). The visual appearance
of any activity changes with viewpoint as well as time.
This analogy between time and viewpoint for variation
in visual appearance of any activity allows us to use the
two concepts interchangeably during representation learn-
ing. This idea makes the proposed architecture even more
powerful as a network trained under some settings can be
tested on different set of parameters in terms of number
of views and number of clips. The proposed BL-NET
supports this further due to its recurrent structure which
allows it to learn a representation for different varying ob-
servations.

We perform an ablation study on activity classiﬁcation
to explore this idea further. In Table 3, we show the classi-
ﬁcation accuracy on a cross subject-split using three dif-
ferent network variations. The classiﬁcation accuracy is
highest, when all three views are available which provides

Figure 10: Interwoven time and view dimensions: time
and view dimensions can be used interchangeably while
making predictions with our network.

Testing View

(1,2,3)
(1,2)
(2,3)
(1,3)
(cid:1) (1,2,3)
(1)
(2)
(3)

(cid:0)3
2

(1,2,3)
67.36
62.88
65.96
62.01
63.13
57.28
56.16
55.25

(cid:1) (1,2,3)

Training View
(cid:0)3
(2,3)
2
83.79
78.06
80.44
62.0
80.67
69.6
80.09
63.1
79.67
64.93
77.26
59.8
74.17
59.7
72.23
58.4

Table 3: Ablation experiments to study the effectiveness of the
learned representation. The table shows classiﬁcation accuracy
with different trained networks and inputs. Variation in the train-
ing and testing views and number of input clips leads to different
(cid:1) (1,2,3) refers to two randomly
classiﬁcation accuracy. Here, (cid:0)3
picked views from the three views (1,2,3).

2

more activity information. Please note that the actors are
either facing view 2 or view 3 in this dataset, which pro-
vides uniformity to the input (2, 3) thus accuracy is second
highest when we use this input view.

We also observe that the representation learned with
(cid:1) (1,2,3)) provides the highest
two random input views ((cid:0)3
classiﬁcation accuracy of 83.79%. When network gener-
ates the third unseen view during the training the repre-
sentation learned is more robust. The t-SNE embedding

2

9

Method

Modality

D
D
D
S

Luo et al. [17]
Li et al. [14]
Shufﬂe & learn [20]
Shahroudy et al.[25]
DSSCA - SSLM[26] RGB-D-S
Luo et al. [17]
Li et al. [14]
Proposed
Proposed-AV

RGB
RGB
RGB
RGB

Cross-
subject
66.2
68.1
61.4
62.9
74.9
56
55.5
79.23
86.78

Cross-
view
53.2
63.9
53.2
70.3
-
-
49.3
75.59
-

Figure 11: t-SNE visualization of activity representations
for a subset of 10 activities (out of 60) on NTU-RGB+D
dataset. The representation is learned with two random in-
put views to RL-NET where VR-NET generates the third
view and activity classiﬁcation is performed. Most of the
actions are well separated and the actions with similar ap-
pearance and dynamics are close to each other.

Table 4: Quantitative evaluation in terms of classiﬁcation ac-
curacy on NTU-RGB+D dataset. Note that rows from 1-5 use
different modalities such as depth (D) and/or skeleton (S) in ad-
dition to RGB; we only use RGB. Proposed-AV method takes all
the views at once for classiﬁcation and we observe a signiﬁcant
improvement with this prior on testing samples.

for this model are shown in the Figure 11. We observe
that the learned embedding for a given class are clustered
together and the activity classes which are visually similar
are close to each other.

We compare the performance of our proposed method
with the recent works on view-invariant activity recog-
nition (Table 4). We observe that the proposed method
outperforms the existing methods with a signiﬁcant mar-
gin (∼ 26%). Moreover, the performance is comparable
with the state-of-the-art approaches employing depth and
skeleton modalities [17][14]. Our model performs well
in both cross-subject (79.23% with one view) and cross-
view split (75.59% with unseen view 1) and outperforms
the state-of-the-art methods using only RGB modality.
The weight sharing form of our network provides great
ﬂexibility in increasing the input clips to our network.
Thus, when we increase the number of input clips from
a video, the best subject-split classiﬁcation accuracy of
83.79% with three views (Table 3) increases to 86.78%,
when we input multiple clips.

learning based framework to solve this problem. The
proposed framework consists of two components, repre-
sentation learning network (RL-NET): which learns a ro-
bust representation of a time-varying event captured from
multiple viewpoints, and a video rendering network (VR-
NET) which synthesizes a video for a given viewpoint and
time of the same event. We demonstrate the effectiveness
of the proposed framework to render view and time aware
videos on two different real world datasets. The video
rendering network forces the representation network to
learn a more robust and effective representation of the
event. To validate this further, we utilize the learned rep-
resentation for view-aware activity classiﬁcation task and
show state-of-the-art results on NTU-RGB+D dataset. We
believe the idea of rendering unseen view is really power-
ful and it enables the network to learn a meaningful rep-
resentation which can be used for multiple other tasks. In
this work, we mainly focused on visual data and we plan
to explore this further for other modalities and problem
domains.

6 Conclusion and Future Work

Acknowledgments

In this work, we introduce a novel view and time aware
video rendering problem. We propose a simple deep

This research is based upon work supported in parts
by the National Science Foundation under Grant No.
1741431; and the Ofﬁce of the Director of National

10

Intelligence (ODNI), Intelligence Advanced Research
Projects Activity (IARPA), via IARPA R&D Contract No.
D17PC00345. The views, ﬁndings, opinions, and conclu-
sions or recommendations contained herein are those of
the authors and should not be interpreted as necessarily
representing the ofﬁcial policies or endorsements, either
expressed or implied, of the NSF, ODNI, IARPA, or the
U.S. Government. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon.

References

[1] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and
S. Levine. Stochastic variational video prediction. arXiv
preprint arXiv:1710.11252, 2017.

[2] W. J. Baddar, G. Gu, S. Lee, and Y. M. Ro. Dynamics
transfer gan: Generating video by transferring arbitrary
temporal dynamics from a source video to a single target
image. arXiv preprint arXiv:1712.03534, 2017.

[3] W. Byeon, Q. Wang, R. K. Srivastava, P. Koumout-
sakos, P. Vlachas, Z. Wan, T. Sapsis, F. Raue, S. Palacio,
T. Breuel, et al. Contextvp: Fully context-aware video pre-
diction. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, pages
1122–1126, 2018.

[4] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and
J. Choo.
Stargan: Uniﬁed generative adversarial net-
works for multi-domain image-to-image translation. arXiv
preprint, 1711, 2017.

[5] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Mor-
cos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka,
K. Gregor, et al. Neural scene representation and render-
ing. Science, 360(6394):1204–1210, 2018.

[6] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to

forget: Continual prediction with lstm. 1999.

[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In Advances in neural infor-
mation processing systems, pages 2672–2680, 2014.
[8] P. Goyal, Z. Hu, X. Liang, C. Wang, E. P. Xing, and
C. Mellon. Nonparametric variational auto-encoders for
hierarchical representation learning. In ICCV, pages 5104–
5112, 2017.

[9] K. Gregor, I. Danihelka, A. Graves, D. Rezende, and
D. Wierstra. Draw: A recurrent neural network for im-
age generation. In International Conference on Machine
Learning, pages 1462–1471, 2015.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[11] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013.

[12] B. Kratzwald, Z. Huang, D. P. Paudel, and L. Van Gool.
Towards an understanding of our world by ganing videos
in the wild. arXiv preprint arXiv:1711.11453, 2017.
[13] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunning-
ham, A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang,
et al. Photo-realistic single image super-resolution using
In CVPR, volume 2,
a generative adversarial network.
page 4, 2017.

[14] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli. Unsu-
pervised learning of view-invariant action representations.
arXiv preprint arXiv:1809.01844, 2018.

[15] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-
to-image translation networks. In Advances in Neural In-
formation Processing Systems, pages 700–708, 2017.
[16] Z. Liu, R. A. Yeh, X. Tang, Y. Liu, and A. Agarwala. Video
In ICCV, pages

frame synthesis using deep voxel ﬂow.
4473–4481, 2017.

[17] Z. Luo, B. Peng, D.-A. Huang, A. Alahi, and L. Fei-Fei.
Unsupervised learning of long-term motion dynamics for
videos. arXiv preprint arXiv:1701.01821, 2, 2017.
[18] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. International
Conference on Learning Representations, 2016.

[19] M. Mirza and S. Osindero. Conditional generative adver-
sarial nets. arXiv preprint arXiv:1411.1784, 2014.
[20] I. Misra, C. L. Zitnick, and M. Hebert. Shufﬂe and learn:
Unsupervised learning using temporal order veriﬁcation.
In B. Leibe, J. Matas, N. Sebe, and M. Welling, editors,
Computer Vision – ECCV 2016, pages 527–544, Cham,
2016. Springer International Publishing.

[21] H. Rahmani and A. Mian. 3d action recognition from novel
In Proceedings of the IEEE Conference on
viewpoints.
Computer Vision and Pattern Recognition, pages 1506–
1515, 2016.

[22] K. Regmi and A. Borji. Cross-view image synthesis using
conditional gans. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3501–
3510, 2018.

[23] M. Saito, E. Matsumoto, and S. Saito. Temporal genera-
tive adversarial nets with singular value clipping. In IEEE
International Conference on Computer Vision (ICCV), vol-
ume 2, page 5, 2017.

[24] M. Schuster and K. K. Paliwal. Bidirectional recurrent
IEEE Transactions on Signal Process-

neural networks.
ing, 45(11):2673–2681, 1997.

11

[38] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired
image-to-image translation using cycle-consistent adver-
sarial networks. arXiv preprint, 2017.

[25] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang. Ntu rgb+
d: A large scale dataset for 3d human activity analysis. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1010–1019, 2016.

[26] A. Shahroudy, T.-T. Ng, Y. Gong, and G. Wang. Deep
multimodal feature analysis for action recognition in rgb+
d videos. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 40(5):1045–1058, 2018.

[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild.
2012.

[28] C. Spampinato, S. Palazzo, P. D’Oro, F. Murabito, D. Gior-
dano, and M. Shah. Vos-gan: Adversarial learning of
visual-temporal dynamics for unsupervised dense predic-
tion in videos. arXiv preprint arXiv:1803.09092, 2018.
[29] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsu-
pervised learning of video representations using lstms. In
International conference on machine learning, pages 843–
852, 2015.

[30] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Learning spatiotemporal features with 3d con-
volutional networks. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 4489–4497,
2015.

[31] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz. Moco-
gan: Decomposing motion and content for video genera-
tion. arXiv preprint arXiv:1707.04993, 2017.

[32] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. De-
composing motion and content for natural video sequence
prediction. International Conference on Learning Repre-
sentations, 2017.

[33] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating
videos with scene dynamics. In Advances In Neural Infor-
mation Processing Systems, pages 613–621, 2016.
[34] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and se-
mantic manipulation with conditional gans. arXiv preprint
arXiv:1711.11585, 2017.

[35] X. Wang and A. Gupta. Unsupervised learning of visual
In The IEEE International
representations using videos.
Conference on Computer Vision (ICCV), December 2015.
[36] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-c. Woo. Convolutional lstm network: A machine
In Ad-
learning approach for precipitation nowcasting.
vances in neural information processing systems, pages
802–810, 2015.

[37] W. Xiong, W. Luo, L. Ma, W. Liu, and J. Luo. Learn-
ing to generate time-lapse videos using multi-stage dy-
arXiv preprint
namic generative adversarial networks.
arXiv:1709.07592, 2017.

12

A Appendices

This document provides additional details supplementary
to the main manuscript. It covers network architectures,
training details and some additional results which were
not included in the main manuscript.

B Model Details

The proposed approach consists of two main components:
Representation Learning network (RL-NET) and Video
Rendering Network (VR-NET) (shown in Figure 12). RL-
NET is used to learn a representation r given some ob-
servations oi. It consists of a base network (BASENET),
which is shared among all the observations to encode indi-
vidual observations, and a Blending Network (BL-NET),
which aggregates encodings from all the observations to
learn a uniﬁed representation r. The representation r is
then passed to the VR-NET which renders a video from
arbitrary view and time. The multi-tasking architecture,
which also performs action classiﬁcation, consists of an-
other branch caled Classiﬁcation Network (CL-NET). It
takes the learned representation r as input and predict
conﬁdence scores for the activity classes. We will dis-
cuss the architecture details for all these networks in the
following subsections.

B.1 Representation Learning Network

(RL-NET)

The overview of the RL-NET is shown in Figure 2.C
(main manuscript). BASENET is the encoding part of
RL-NET which is shared among all the observations and
the corresponding encodings are passed to the BL-NET
for representation learning. We experimented with two
different networks for representation learning. We ﬁrst
explored 2D convolutions for single view videos (UCF-
101), where each observation is a single video frame. The
BASENET for this architecture utilizes 2D convolution in
combination with max-pooling to get frame level encod-
ings. We further extend this to short video clips where
each observation is a set of consecutive video frames with
some skip rate. In this case we use 3D convolutions in-
stead of 2D and the structure of the architecture remains
almost similar. A detailed architecture for this variation

Figure 12: An outline of the proposed framework for rep-
resentation learning and video rendering for single view
videos.

is shown in Figure 13. The 2D variant of representation
learning network has similar architecture with 3D convo-
lutions substituted with 2D convolutions.

The encodings learn by the base network are passed
on to the blending network (BL-NET), which is a re-
current network with bi-directional convolutional LSTM
cells. The architecture of BL-NET is shown in Figure 2.E
(main manuscript). The number of recurrent steps in the
network will be equal to the number of observations (oi)
provided to the RL-NET. The ﬁrst layer of BL-NET com-
prise of 128 3x3 kernels in each direction with a total of
256 kernels for the convolution operation. The ﬁnal layer
has 256 3x3 kernels which produces a representation of
size 28x28x256.

B.2 Video Rendering Network (VR-NET)

The video rendering network takes the learned represen-
tation r along with query viewpoint v and latent noise z to
synthesize a video clip. A detailed architecture of the pro-
posed VR-NET is shown in Figure 14. It ﬁrst integrates
the representation r with viewpoint v and latent noise z
followed by convolution operation. A video is generated
using convolutions along with upsampling of features. A
similar network architecture is used for the experiments
where images are generated using the learned representa-

13

Figure 13: Network architecture of BASENET used in RL-NET for representation learning r for set of video observa-
tions oi. The diagram on the left side shows the change in feature volumes, along with size, as the input to the network
ﬂows through the layers of the network. The network takes two input, video clips and corresponding viewpoints, and
generates a video. The table on the right shows details of the layers in the network including kernel sizes and layer
types. Conv: 3D convolution in all the layers except the last two layers where it will be 2D convolution as the temporal
extent has been compressed to single channel, v: viewpoint, e: embeddings for observations.

Figure 14: Network architecture of VR-NET used for video rendering. VR-NET takes the learned representation r
along with query viewpoint v and latent noise z as input and generates a video. The representation r, viewpoint v
and latent noise z are ﬁrst integrated together followed by a convolution operation. A video is generated using 3D
convolution along with batch-normalization and upsampling. BN: batch normalization, z: latent noise, i: generated
video.

14

Figure 15: Network architecture of the classiﬁcation branch (CL-NET) of the multi-task approach. This network takes
the learned representation r as input and predicts class probabilities. FC: fully connected layer, c: predicted class
probabilities.

tion. It uses 2D convolutions instead of 3D convolutions
with no upsampling along the temporal axis in the feature
space.

B.3 Classiﬁcation Network (CL-NET)

We also propose to use the learned representation for ac-
tivity classiﬁcation. The network is trained for both video
rendering as well as activity classiﬁcation simultaneously
in a multi-task learning. We added a classiﬁcation branch
(CL-NET) on top of representation which predicts class
probabilities for activities. A detailed architecture for this
branch is shown in Figure 15. It consists of 2D convolu-
tions followed by fully connected layers for class predic-
tions.

C Experimental Details

C.1 Hyperparameters

We implement the proposed method on Keras with Ten-
sorﬂow backend. We train all our networks using Adam
optimizer [10] with a learning rate of 2e-5, β1=0.9,
β2=0.999, and a decay of 1e-6. The networks were
trained until convergence of loss. The batch normaliza-
tion layers use a momentum of 0.9 with (cid:15)=1e-3 and no
scaling. The CL-NET branch has dropouts after every
fully connected layer where we use dropout rate of 0.5.
We use a skip rate of 3 frames for all our experiments.

C.2 Viewpoint

The NTU-RGB+D dataset was captured using three cam-
eras positions (CAM1, CAM2 and CAM3) and action
videos were recorded in two conditions: each actor fac-
ing CAM2, and each actor facing CAM3. The viewpoint
involved 6 parameters including, camera height, camera
distance, camera number, horizontal-pan, vertical-pan,
and actor orientation. Horizontal and vertical pan in our
training set was determined based on the cropping of the
input frames (112, 112, 3) which was done randomly. The
camera position was encoded based on its location using
V values (-π/2, -π/4, 0, π/4, π/2) (Figure 16). The other
viewpoint parameters were normalized between 0-1.

Figure 16: Assimilation of viewpoint and actor orientation in
NTU-RGB+D dataset. There are three different cameras (la-
beled as CAM1, CAM2 and CAM3) in the set-up. R represents
which camera the actor is facing: R=1 when actor faces CAM3
and R=2 when actor faces CAM2.

15

Method
[33]
[23]
[31]
Proposed
Original vidoes

Inception score
8.18±0.05
11.85±0.07
12.42±0.03
17.18±0.19
52.20±0.82

Table 5: Comparison of inception score with state-of-the-
art approaches for video generation on UCF-101 dataset.
Last row shows the inception score for the ground truth
real videos.

C.3 PSNR and SSIM Evaluation

The quantitative evaluation presented in Table 1 and Table
2 of the manuscript was performed over 378 test videos
from UCF-101 dataset for a fair comparison with exist-
ing methods. We compute the Peak Signal to Noise Ra-
tio (PSNR) and the Structural Similarity Index Measure
(SSIM) of [?]. This experimental setup was suggested
in [18] where every 10th video was selected for evalua-
tion from the test list. Apart from this, the evaluation for
Table 1 was performed only in the moving areas which
was determined using optical ﬂow [?]. We use the opti-
cal ﬂow images provided by [18] for our evaluation. In
Table 2, the evaluation was performed over the full image
for a fair comparison with [18]. The evaluation on NTU-
RGB+D dataset was performed on all the testing videos
where the target video was sampled randomly from each
instance.

D Additional Results

D.1

Inception Score

We have recently seen some works for video synthesis
which make use of adversarial loss in the GAN frame-
work [33, 23, 31]. We compare the quality of the gener-
ated videos with these works in terms of inception score
[23]. We need a trained classiﬁer to compute the incep-
tion score and we use a C3D model [30] pre-trained on
Sports1M dataset [?] and ﬁne-tuned on UCF-101 as sug-
gested in [23]. We generated around 50K video samples
for future prediction using the test set of UCF-101 with

Figure 17: t-SNE visualization of activity representations
for a subset of 10 activities (out of 60) on NTU-RGB+D
dataset. The representation is learned with one input view
to RL-NET. Most of the actions are well separated and the
actions with similar appearance and dynamics are close
to each other. The separation is not as good as when we
use multiple views for representation learning (Figure 11,
main manuscript) in case of confusing classes, however
for others there is a clear visible boundary.

6 frames each. A comparison is shown in Table 5 and
we observe that the quality of videos generated using our
proposed approach is much better as compared with these
methods in terms of inception score.

D.2 Visualizing Embeddings

In the main manuscript, we discuss the 2-dimensional t-
distributed stochastic neighbor embedding (t-SNE) [?] vi-
sualization of the learned embeddings with two random
views as inputs while generating the third view along with
activity classiﬁcation. Here we present the t-SNE visual-
ization of the learned embedding from single views. The
visualization of the embeddings from the ﬁnal fully con-
nected layer of CL-NET is shown in Figure 17. We ob-
serve that the separation for the activity classes which are
confusing (such as drinking and eating) is slightly over-
lapping and it is not as sharp as we observe for multi-view
embeddings. However, the network is still able to separate
other activity classes very well.

We also compare the representation learned by the pro-
posed RL-NET with autoencoding density models such as
Variational Autoencoder (VAE) [11]. The VAE was im-

16

with the viewpoint, and later focuses on reﬁning the actor
and the corresponding action.

D.4 Qualitative Results

Here we are showing some additional qualitative results
for future frame generation on UCF-101 dataset and
view and time aware video generation on NTU-RGB+D
dataset. The generated frames are shown in Figure 20,
21, and 22. We also provide the videos (gif animations)
created using these frames in the supplementary material.
The videos show the six frames generated by the model
and their name corresponds to the activity class.

Figure 18: The variation in training and validation loss as
training progresses. The images are the ﬁrst frame of the
generated videos using the trained model at corresponding
epoch.

plemented by replacing the RL-NET model with a CNN
network (similar to BASENET) and keeping the rest of
the network similar to ours. We observe that the pro-
posed method was able to place the instances from sim-
ilar classes close to each other despite the change in the
viewpoint. VAE on the other hand failed to capture any
structure in the representations with varying viewpoints
and activity classes. A t-SNE comparison plot is shown
in Figure 19 where we use samples only from one scene
(scene S001) of NTU-RGB+D dataset.

D.3 Quality of Generated Videos

We also explore the variation in the quality of generated
videos as the training progresses. The variation in the
training and validation loss at each epoch is shown in
Figure 18. The graph also shows the ﬁrst frame of the
videos generated after each epoch with our trained model.
The videos are generated with a 3D convolution RL-NET
and VR-NET architecture, which was trained with input
videos from two randomly selected views for each video
and the third view was used for generation. We render the
same view point (view 1) for the shown graph. We ob-
serve a gradual improvement in the visual quality of the
generated video frames as the loss goes down during the
training. The network ﬁrst learns to synthesize the scene

17

Figure 19: View-invariant representation learning. t-SNE visualization of representations learned with VAE (left) and
RL-NET (right) for a subset of 10 activities (out of 60, shown in Figure 6) on NTU-RGB+D dataset. The representation
is inferred with one input view to both VAE and RL-NET from only one scene (scene S001) in NTU-RGB+D dataset.
The shown images are the ﬁrst frame of the video clip. We observe that VAE is indifferent to view awareness of
activities and mostly clusters videos with similar visual content. On the other hand, the proposed method is able to
cluster activities from different views close to each other even if they have different viewpoints.

(a) Generated video frames

(b) Ground-truth video frames

Figure 20: Future video frames generated (on Left) with 3D-Convolution Network (RL-NET and VR-NET) and their
ground truth (on Right) on UCF-101 dataset. Row-1: Baby crawling, Row-2: Archery, Row-3: Apply eye make-up,
Row-4: Cutting in kitchen, and Row-5: Band marching.

18

Figure 21: Video frames generated for arbitrary selected
query time and view on NTU-RGB+D dataset. The net-
work is given video clips from three views at randomly
selected time. This network was trained on input video
clips from all the three views and a video was generated
from any one of these views at arbitrary time. view 3D-
Convolution Network for action classes. Row-1: Stom-
achache/heart pain, Row-2: Eat meal/snack, and Row-3:
Nod head/bow.

Figure 22: Video frames generated for future time for
a given view on NTU-RGB+D dataset. Row-1: Wear
on glasses, Row-2: Touch other person’s pocket, Row-3:
Brushing hair, Row-4: Brushing hair, and Row-5: Touch
other person’s pocket.

19

