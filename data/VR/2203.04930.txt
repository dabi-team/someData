Triangular Character Animation Sampling with Motion, Emotion, and Relation

Yizhou Zhao1, Liang Qiu1, Wensi Ai2, Pan Lu1, Song-Chun Zhu1
1UCLA Center for Vision, Cognition, Learning, and Autonomy
2Department of Computer Science, UCLA
yizhouzhao@g.ucla.edu

2
2
0
2

r
a

M
9

]

R
G
.
s
c
[

1
v
0
3
9
4
0
.
3
0
2
2
:
v
i
X
r
a

Abstract

Dramatic progress has been made in animating individual
characters. However, we still lack automatic control over ac-
tivities between characters, especially those involving inter-
actions. In this paper, we present a novel energy-based frame-
work to sample and synthesize animations by associating the
characters’ body motions, facial expressions, and social re-
lations. We propose a Spatial-Temporal And-Or graph (ST-
AOG), a stochastic grammar model, to encode the contextual
relationship between motion, emotion, and relation, forming
a triangle in a conditional random ﬁeld. We train our model
from a labeled dataset of two-character interactions. Exper-
iments demonstrate that our method can recognize the so-
cial relation between two characters and sample new scenes
of vivid motion and emotion using Markov Chain Monte
Carlo (MCMC) given the social relation. Thus, our method
can provide animators with an automatic way to generate 3D
character animations, help synthesize interactions between
Non-Player Characters (NPCs), and enhance machine emo-
tion intelligence (EQ) in virtual reality (VR).

Introduction
Traditional 3D animation is time-consuming. From sculpt-
ing 3D meshes, building rigging and deformation, to assign-
ing skin weights, those preparatory steps make a 3D charac-
ter ready to move. Beyond those efforts, an animator designs
detailed movement for each body joint by setting keyframes
or obtaining live actions from a motion capture device. That
complicated work requires precise adjustment and careful
consideration of many details.

Recent years have witnessed the rapid evolution of
machine learning methods that facilitate animation mak-
ing (Lee, Lee, and Lee 2018; Zhang and van de Panne
2018; Taylor et al. 2017). However, few works aim to make
multi-character animation from a data-driven approach. In
multi-character animations, movements of one character
along with others, which bring out complicated combina-
tions of body poses, hand gestures, and facial expressions.
To synthesize meaningful animations, especially for multi-
character social interactions, we argue there exist three main
difﬁculties:
• Multi-character animation should interpret meaningful
social interactions. (e.g., two characters say greetings
with a high ﬁve.)

• Body motions of one character must correspond with
its facial emotions. For example, one character applauds
with a happy face.

• Multi-character animation brings additional constraints
between characters to match their body movements and
facial expressions temporally.

To address these challenges, ﬁrst, we use the Spatial-
Temporal And-Or Graph (ST-AOG) (Xiong et al. 2016)
that samples a two-character key-frame animation from a
stochastic grammar. The ST-AOG allows us to set up the
contextual constraints for (body) motions, (facial) emotions,
and the (social) relation between the two characters across
time. Then, we propose Iterative Data Generating And La-
beling (IDGAL) to alternatively sample scenes, label sam-
ples, and update model parameters, allowing us to efﬁ-
ciently train the ST-AOG from limited human-annotated
data. Next, to make detailed animation for single-character
motion and emotion, we apply the Eigenface method (Turk
and Pentland 1991) to encode facial joints for generating
facial expressions, and a Variational Recurrent Neural Net-
work (VRNN) (Chung et al. 2015) to control body joints for
generating body poses. Finally, using Markov Chain Monte
Carlo (MCMC), the well-trained ST-AOG helps the Eigen-
face and VRNN to sample animations.

Our work makes two major contributions: (1) we are
the pioneer to jointly and automatically sample the motion,
emotion, and social relation for a multi-character conﬁgura-
tion; (2) we present IDGAL to collect data while training a
stochastic grammar. We plan to make our work (including
our model and collected dataset) open-source to encourage
researches on synthesizing multi-character animation.

The following sections ﬁrst review some related works
and then deﬁne the ST-AOG representing a two-character
scene. Next, we formulate the stochastic grammar of the ST-
AOG and propose the learning algorithm with IDGAL. Fi-
nally, after pre-training the Eigenface and VRNN, we can
sample animations using MCMC 1.

1The animation part

in our work is done by Autodesk

Maya (Autodesk, INC.)

 
 
 
 
 
 
Figure 1: When animating a two-character social interaction scene, we need to consider the consistency of each character’s
body movements and facial expressions and the interplay between them. The ﬁgure on the left shows the initial frame of the
animation; the ﬁgures on the right plot the sampling results of our method for the body movements and facial expressions of
the two characters every 0.5 second.

Related Work

2019) or dialogue-based (Yu et al. 2020).

Machine learning for animation
Recent advances in machine learning have made it possible
for animators to automatize some of the tough processes of
generating human motion sequences. From early approaches
such as hidden Markov models (Tanco and Hilton 2000; Ren
et al. 2005), Gaussian processes (Wang, Fleet, and Hertz-
mann 2007; Fan, Zhang, and Ding 2011) and restricted
Boltzmann machines (Taylor and Hinton 2009), to more re-
cent neural architectures such as Convolutional Neural Net-
works (CNN) (Holden, Saito, and Komura 2016) and Re-
current Neural Networks (RNN) (Fragkiadaki et al. 2015),
synthesizing human motions relies on framing poses or ges-
tures (Pavllo et al. 2019). More recent work focuses on im-
proving animation quality by considering physical environ-
ment (Holden, Komura, and Saito 2017), character-scene in-
teractions (Starke et al. 2019) or social activities (Shu, Ryoo,
and Zhu 2016).

Emotion animation often comes together with speech an-
imation (Taylor et al. 2017). Producing high-quality speech
animation relies on speech-driven audios (Oh et al. 2019)
and performance-driven videos obtained by facial motion
captures (Maurer et al. 2001; Cao et al. 2015). Then, anima-
tions for the designated actor can be synthesized by trans-
ferring facial features (Taylor et al. 2017) or Generative Ad-
versarial Networks (GAN) (Vougioukas, Petridis, and Pantic
2019).

Animation with social interaction
Involving animations with social interactions has been stud-
ied for a long time (Takeuchi and Naito 1995; Waters et al.
1997; Arafa et al. 2002; Silvio 2010). Animations with so-
cial meanings are created by the interaction between the
individuals in a group, which build better adoption for the
characters in VR (Wei et al. 2019), AR (Miller et al. 2019),
and Human-robot Interaction (HRI) (Shu, Ryoo, and Zhu
2016). Existing datasets for two-person interactions can be
video-based (Yun et al. 2012), skeleton-based (Liu et al.

Stochastic grammar model

Stochastic grammar models are useful for parsing the hierar-
chical structures for creating indoor scenes (Qi et al. 2018),
predicting human activities (Taylor et al. 2017), and con-
trolling robots (Xiong et al. 2016). In this paper, we forward
sampling from a grammar model to generate large variations
of two-character animations. We also attempt to outline a
general multi-character animation model to bring together
motions, emotions, and social relations.

Representation and formulation
In this section, we ﬁrst deliver a brief mathematical deﬁ-
nition of skeletal animation. Then, we introduce norms of
valence, arousal, dominance and intimacy, which set con-
straints between motion, emotion, and social relation. Fi-
nally, we conduct the stochastic grammar for animations.

Skeletal animation
Skeletal animation is a computer animation technique that
enables a hierarchical set of body joints to control a char-
acter. Let j denote one body joint that is characterized by
its joint type, rotation, and position. A body pose p is de-
ﬁned as a set of joints {ju}u=1,2,...,n controlling the whole
body, where n is the total number of joints. Similarly, a fa-
cial expression f is characterized by facial rigging. We can
make an animation by designing the body pose p and facial
expression f at k-th keyframe at time tk.

We deﬁne a motion m as a sequence of body poses and an
emotion e as a sequence of facial expressions corresponding
to the keyframes.

Valance, arousal, dominance, and intimacy

Norms of valence, arousal, and dominance (VAD) are
standardized to assess environmental perception, experi-
ence, and psychological responses (Warriner, Kuperman,

Figure 2: Scene grammar as an ST-AOG. We ﬁrst initialize a social interaction scene between two characters by setting up the
relative transform (position, rotation, and scale) and the type of the social relation between them. Then, for each character, we
sample the animation that consists of one motion and one emotion.

and Brysbaert 2013). Valence v describes a stimulus’s pleas-
antness, arousal a quantiﬁes the intensity provoked by a
stimulus, and dominance d evaluates the degree of con-
trol (Bakker et al. 2014). Norms of VAD can also describe a
facial expression (Ahn et al. 2012). Figure 3 plots different
facial expressions with varying degrees of valence, arousal,
and dominance.

Figure 3: Norms of VAD and facial expressions (Ahn et al.
2012)

Bringing the concept of intimacy, we extend VAD norms
as VADI norms. Speciﬁcally, intimacy i describes the close-
ness of the relationship (Karakurt and Cumbie 2012). We de-
scribe the social relation r between two characters by their
relative dominance and intimacy (d, i). Figure 4 plots dif-
ferent types social relations along with their dominance and
intimacy.

Norms of valance, arousal, dominance, and intimacy form
the space to set constraints between motion, emotion, and
social relation. We will discuss it in detail in the next section.

Stochastic grammar for two-character animations
We deﬁne a Spatial-Temporal And-Or Graph (ST-AOG)

G = (R, V, C, P, S, T )

(1)

to represent the social interaction scene of two characters,
where R is the root node for representing the scene, V the
node set, C the set of production rules (see Figure 2), P the
probability model. The spatial relation set S represents the

Figure 4: Examples of different social relations and their cor-
responding dominance-intimacy scores.

contextual relations between terminal nodes and the tempo-
ral relation set T represents the time dependencies.

Node Set V can be decomposed into a ﬁnite set of non-
terminal and terminal nodes: V = V N T ∪ V T . The non-
terminal nodes V N T consists of two subsets V And and V Or.
A set of And-nodes V And is a node set in which each node
represents a decomposition of a larger entity (e.g., one char-
acter’s animation) into smaller components (e.g., emotion
and motion). A set of Or-nodes V Or is a node set in which
each node branches to alternative decompositions (e.g., the
intimacy score between two characters can be low, medium,
or high). The selection rule of Or-nodes follows the proba-
bility model P . In our work, we select the child nodes with
equal. The terminal nodes V T represent entities with dif-
ferent meanings according to context: terminal nodes under
the relation branch identify the relationship between the two
characters; the ones under motion branch determine their
motions, and the ones under emotion branch depict emo-
tions.

Spatial Relations S among nodes are represented by the
horizontal links in ST-AOG forming Conditional Random
Fields (CRFs) on the terminal nodes. We deﬁne different

potential functions to encode pairwise constraints between
motion m, emotion e, and social relation r:

S = Sme ∪ Sre ∪ Srm.

(2)

Sme sets constraints on the motion and emotion to ensure
that the body movement supports one emotion according to
the social affordance. For example, the crying action (rub-
bing eyes) can hardly be compatible with a big smile. Sre
regulates the emotion when social relation is considered. For
instance, an employee has few chances to laugh presumptu-
ously in front of the boss. Similarly, Srm manages to select
the suitable body motion under social relation. For example,
kissing is allowed for couples.

Temporal Relations T among nodes are also represented
by the links in ST-AOG to address time dependencies. Tem-
poral relations are divided into two subsets:

T = Tme ∪ Tr.

(3)

Tme encodes the temporal associations between motion and
emotion. Tr describes the extent to which the two charac-
ters’ animations match temporally.

A hierarchical parse tree pt is an instantiation of the ST-
AOG by selecting a child node for the Or-nodes and deter-
mining the terminal nodes. A parse graph pg consists of a
parse tree pt, a number of spatial relations S, and several
temporal relations T on the parse tree:

pg = (pt, Spt, Tpt).

(4)

Probabilistic model of ST-AOG
A scene conﬁguration is represented by a parse graph pg,
including animations and social relations of the two charac-
ters. The probability of pg generated by an ST-AOG param-
eterized by θ is formulated as a Gibbs distribution:

p(pg | Θ) =

=

1
Z
1
Z

exp {−E(pt | Θ) − E (Spt | Θ) − E (Tpt | Θ)} ,

(5)

where E(pg | Θ) is the energy function of a parse graph, and
E(pt | Θ) of a parse tree. E(Spt | Θ) and E(Tpt | Θ) are the
energy terms of spatial and temporal relations. E(pt | Θ) can
be further decomposed into the energy functions of different
types nodes:

E(pt | Θ) =

(cid:88)

E Or
Θ (v)

+

(cid:88)

E T
Θ (v)

.

v∈V
(cid:125)
(cid:123)(cid:122)
(cid:124)
non-terminal nodes

v∈V r
T
(cid:125)
(cid:123)(cid:122)
(cid:124)
terminal nodes

(6)

We apply the norms of valence, arousal, dominance and inti-
macy (VADI) to quantify the triangular constraints between
social relation, emotion and motion:
• Social relation r is characterized by its dominance and

intimacy (dr, ir).

• For emotion e, which is a sequence facial expression, we
consider its valance, arousal, and dominance scores as
the different between the beginning facial expression f0
and ending facial expression f1:

(ve, ae, de) = (vf1, af1, df1) − (vf0, af0, df0).

(8)

• To get the scores of a motion m, we ﬁrst label the name
Nm of the motion, such as talk, jump and cry. Then we
can obtain the valance, arousal, and dominance scores
from NRC-VAD Lexicon (Mohammad 2018), which in-
cludes a list of more than 20,000 English words and their
valence, arousal, and dominance scores:

m → Nm → (vm, am, dm).

Therefore, the relation Sme and its potential φme on the
clique Cme = {(m, e)} contain all the motion-emotion pairs
in the animation, and we deﬁne

φme(c) =

1
Z s

me

exp{λs

me · (vm, am, dm) · (ve, ae, de)(cid:62)}.
(9)

Calculating potentials φrm on clique Crm = {(m, r)}
and φre on Cre = {(e, r)} needs another variable ime sug-
gesting the intimacy score. ime is deﬁned as the distance
dist between the two characters compared with a standard
social distance dist0 :

ime = (dist0 − dist)/dist0.

(10)

φre(c) =

φrm(c) =

1
Z s
re
1
Z s

rm

exp{λs

re · (dr, ir) · (de, ime)(cid:62)},

(11)

exp{λs

rm · (dr, ir) · (dm, ime)(cid:62)}.

(12)

Temporal potential E (Spt | Θ) combines two potentials

for time control, and we have

p (Tpt | Θ) =

=

1
Z
(cid:89)

c∈CT

me

exp {−E (Tpt | Θ)}

ψme(c)

(cid:89)

c∈CT
r

ψr(c).

(13)

exp{−E(pg | Θ)}

Then we can deﬁne

Spatial potential E (Spt | Θ) combines the potentials of
three types of cliques Cme, Cre, Crm in the terminal layer,
integrating semantic contexts mentioned previously for mo-
tion, emotion and relation:

Potential ψme is deﬁned on clique C T
me = {(tm, te)} rep-
resenting the time to start a motion and an emotion. We as-
sume the time discrepancy between them follows a Gaussian
distribution, then we can get

p (Spt | Θ) =

=

1
Z
(cid:89)

exp {−E (Spt | Θ)}

φme(c)

(cid:89)

φre(c)

(cid:89)

φrm(c).

c∈Cme

c∈Cre

c∈Crm

(7)

exp (cid:0)λt

ψme(c) =

1
Z t
me
Notice that so far the training parameters λs
λt
me and partition functions Z s
doubled since we have two characters in the scene.

re · (tm − te)2(cid:1) .

me, Z s

rm, Z t

re, Z s

me, λs

rm, λs
rm,
me should be

(14)

At last, to match the animation for both characters, we as-
sume that the time differences between ending time of their
motions t1,m, t2,m and emotions t1,e, t2,e follow the Gaus-
sian distribution:

ψr(c) =

+

1
Z t
m
1
Z t
e

exp (cid:0)λt

m · (t1,m − t2,m)2(cid:1)

exp (cid:0)λt

e · (t1,e − t2,e)2(cid:1) .

(15)

Here we have two additional parameters λt
more partition functions Z t

m, Z t
e.

m, λt

e, and two

Data collection
Traditional methods (Creswell et al. 2018) for training a gen-
erative model prefer using a well-labeled dataset. Therefore,
a model hardly modiﬁes the original dataset after prepa-
ration steps such as data preprocessing and augmentation.
However, there is not a suitable dataset to train our model.
Fortunately, our ST-AOG is a generative model that can sam-
ple animations.

We start from low-quality animations sampled by our ST-
AOG, since our ST-AOG is initiated from a random state and
probably sample distorted body poses, weird facial expres-
sions, and meaningless interpretations for the two-character
social interactions. Then, we introduce iterative data gen-
erating and labeling (IDGAL) to render two-character in-
teraction scenes, making more samples while improving the
data sampling quality by training model. Each round of ID-
GAL contains three steps: sampling scenes from our ST-
AOG, labeling scenes, and updating ST-AOG using the la-
beled data.

Figure 5: Comparison between (a) regular machine learning
pipeline and (b) iterative data generating and labeling.

Sampling scenes
Sampling pg from our ST-AOG requires selecting Or-nodes
and terminal nodes from the transform branch, relation
branch, emotion branch, and motion branch. We assume that
each character in a scene has one motion, one starting facial
expression and one ending facial expression.

The transform branch samples the initial relative distance
between the two characters and we assume that they stand
face to face. The relation branch sets up their social rela-
tion (see Figure 4). The ST-AOG samples emotions from a
pool of 21 basic facial expressions (e.g. smile, happy face,
and sad face) by facial rigging from Facial Action Coding
System (Cohn, Ambadar, and Ekman 2007) and annotate
their VAD scores by the NRC-VAD Lexicon (Mohammad

2018). The motion pool we gathered is from Adobe Mixamo
(Blackman 2014). We select a total number of 65 single-
person animations and label their names (such as kissing,
bowing and yelling) in order to get their VAD scores by the
NRC-VAD Lexicon (Mohammad 2018).

The above process speciﬁes the conﬁguration of pg and
VADI scores that determine the spatial potential E (Spt | Θ).
To obtain temporal potential E (Tpt | Θ), We further assume
the time difference between motion start and emotion start,
and the time misalignments between their motions and emo-
tions (t1,m − t2,m) and (t1,e − t2,e) follow the standard nor-
mal distribution.

Labeling scenes
Training our ST-AOG requires labeling expert data (see
Equation 16 and 18): the model learns to sample those ex-
pert data with higher probabilities. Therefore, we label the
scene’s quality by asking experimenters’ subjective judg-
ments on whether the scene is reasonable and meaningful.

Updating ST-AOG
We rewrite the probability distribution of cliques formed on
terminal nodes as

p (Spt, Tpt, | Θ) =

=

1
Z
1
Z

exp {−E (Spt | Θ) − E (Tpt | Θ)}

exp {− (cid:104)λ, l (STpt)(cid:105)} .

(16)

where λ is the weight vector and l (STpt) is the loss vector
given by Equations 9, 11, 12, 14 and 15. To learn the weight
vector, the standard maximum likelihood estimation (MLE)
maximizes the log-likelihood:
N
(cid:88)

(cid:104)λ, l (STpt)(cid:105) − log Z.

(17)

L (Spt, Tpt, | Θ) = −

1
N

n=1

It is usually maximized by gradient descent:

∂L (STpt | Θ)
∂λ

= −

1
N

N
(cid:88)

n=1

l (STptn ) −

∂ log Z
∂λ

= −

1
N

N
(cid:88)

l (STptn ) +

˜N
(cid:88)

l (STpt ˜n) ,

(18)

1
˜N

n=1
where {l(STpt ˜n )}
(cid:101)n=1,··· , (cid:101)N is calculated by synthesized ex-
amples from the current model, and {l(STptn )}n=1,··· ,N by
expert samples (gold labels) from the labeled dataset.

˜n=1

A dataset of two-character animations
We apply Equation 18 to train our model from the labeled
data. We truncate the low-likelihood samples, and in each
updating ST-AOG round, we take 100 epochs of the gradi-
ent descent (equation 18) with the learning rate as 1e − 3.
After getting the sampled scenes in each round, experi-
menters were asked to examine scenes and label them with
good/medium/bad. In total, three rounds of IDGAL give
us 1, 240 well-labeled animations with a total length of
9, 285 seconds. Table 1 shows the improvement along with
three rounds of IDGAL: the percent of good samples rises
whereas the percent of bad samples decreases.

Count
Good rate
Bad rate

Round one Round two Round three
400
39.5%
26.5%

440
36.8%
36. 0%

400
40.0%
24.0%

Table 1: Quality labeling results of different rounds.

Synthesizing scenes
Training our ST-AOG requires sampling motions and emo-
tions from existing motion and emotion pools, limiting it to
a small range of applications with little ﬂexibility. To over-
come this problem, we present the idea to make the learned
stochastic grammar suitable for all kinds of generative mod-
els for facial animation and body movement so that our ST-
AOG can be useful for a broader range of applications.

Markov chain dynamics
First, we design three types of Markov chain dynamics:
(1) qr to make proposal sample social relations; (2) qe to
sample emotions; (3) qm to sample motions.

Relation dynamic qr makes transition of social relations

directly from ST-AOG’s Or-nodes on relation branch:

Noder1 → Noder2.
Emotion dynamic qe changes the VAD scores of one of

(19)

the facial expression at keyframe tk:

(v, a, d) → (v(cid:48), a(cid:48), d(cid:48)).

(20)

Besides, to synthesize emotions, we train a linear model
based on our 21 basic faces with manually labeled VAD
scores. Speciﬁcally, we ﬁrst get the eigenfaces through prin-
cipal component analysis (PCA) based on the positions of
eyebrows, eyes, mouth and cheeks. We show some con-
structed examples in Figure 6 and leave the detailed analysis
in the appendix.

Figure 6: Reconstructed facial expressions and VAD scores.

Motion dynamic qm regards motion as a sequence of
body poses {pi}i=1,2,.... Therefore, we train a generative
model for motions, where the model takes the inputs of
poses {pi}i=1,2,...,k for every δt time interval and predicts
the next body pose pk+1 according to the maximum likeli-
hood:

arg max
pk+1

P(pk+1 | pk, pk−1, ..., p1; δt).

(21)

We train a model to generate animations for a single
character. The motion samples collected from Adobe Mix-
amo (Blackman 2014) are manually ﬁltered. The ﬁltered

database contains 149 animations such as yelling, waving
a hand, shrugging, and talking. The complete list of anima-
tions is attached to the Appendix.

The data augmentation process mirrors each animation
from left to right, resulting in a total number of 36, 622
keyframes of animations with 24 frames in a second. We
set 0.5s as the time interval between poses, and each pose
is represented by the rotations of 65 joints and the position
of the root joint (see Appendix for the rigging). Therefore,
each pt is a 198-dimensional (65 × 3 + 3) vector.

We apply the variational

recurrent neural network
(VRNN) (Chung et al. 2015) as our motion generative model
and we set δt = 0.5s. As Figure 7 shows, the model ﬁrst en-
codes the historical poses {pu}u=1,2,...,k−1 into a state vari-
able hk−1, whereby the prior on the latent random variable
zk is calculated. zk usually follows a Gaussian distribution.
Finally, the generating process takes the inputs hk−1 and zk
to generate pose pk.

Figure 7: The variational recurrent neural network (VRNN).

The training procedure follows that of a standard VAE
(Doersch 2016). The objective is to minimize the reconstruc-
tion loss (mean square error between original and recon-
structed data) and KL-divergence loss. The results show that
VRNN performs well: 56.9% of the reconstructed errors for
joint rotations are less than 1.0 degree and 87.3% less than
5 degrees. See to the Aappendix for detailed analysis.

Then, dynamic qm generates a body pose pk by sampling

the latent random variable zk:

pk → p(cid:48)
k,
by zk → z(cid:48)
k.
(23)
To map the pose to the VADI norms, we train a linear model:

(22)

regression
−−−−−→ (v, a, d, i).
(24)
Finally, according to Equations (9), (11), and (12), we can

pk

calculate the probability of sampling pg(cid:48).

MCMC
Adopting the Metropolis-Hastings algorithm (Chib and
Greenberg 1995), the proposed new parse graph pg(cid:48) is sam-
pled according to the following acceptance probability:
(cid:19)
(cid:18)

α (pg(cid:48) | pg, Θ) = min

1,

p (pg(cid:48) | Θ) p (pg | pg(cid:48))
p(pg | Θ)p (pg(cid:48) | pg)

,

(25)

Scenario
wave hands
high-ﬁve
shake hands
apologize
criticize
quarrel

Social relation
friends (medium, medium)
brothers (medium, high)
strangers (medium, low)
employee to employer (low, medium)
teacher to student (high, close)
colleagues (medium, medium)

Character one emotion
neutral → happy
happy → excited
neutral → excited
neutral → sad
neutral → angry
neutral → dissatisﬁed

Character two emotion
(0.5, 0.5, 0.5)[neutral] →(0.9, 0.6, 0.6)[delight]
(0.5, 0.5, 0.5)[neutral] →(0.9, 0.7, 0.7)[glad]
(0.8, 0.7, 0.6)[joyful] →(0.9, 0.4, 0.8)[respectf ul]
(0.5, 0.5, 0.5)[neutral] →(0.3, 0.6, 0.4)[concerned]
(1.0, 0.7, 0.8)[happy] →(0.3, 0.8, 0.3)[scared]
(0.5, 0.5, 0.5)[neutral] →(0.1, 0.8, 0.3)[annoyed]

Table 2: Emotion sampling results. The social relation is labeled with its dominance and intimacy scores. MCMC generates the
emotion to sample from the starting facial expression (the blue text describes its valence, arousal, and dominance scores) to the
ending facial expression (VAD norms in red). We also pick a word to describe a facial expression according to the NRC-VAD
Lexicon (Warriner, Kuperman, and Brysbaert 2013).

where p (pg | Θ) and p (pg(cid:48) | Θ) are calculated by energy
terms exp(E(pg | Θ)) and exp(E(pg(cid:48) | Θ)) from Equation
(5). p(pg | pg(cid:48)) and p(pg(cid:48) | pg) are the proposal probabilities
deﬁne by dynamics qr, qe, and qm.

Application

We present two applications of our model:
• Emotion sampling: given two characters’ social rela-
tions and motions, our model can complement anima-
tions of facials expressions.

• Motion completion: given two characters’ social rela-
tion, emotions, and part of the movement information,
our model can complement the subsequent body move-
ments of characters.

Emotion sampling
Facial expressions are among universal forms of body lan-
guage and nonverbal communication. Currently, there is a
lack of study on how a virtual character could interplay
his/her facial expression to another character, resulting in
poor playability in a game or a virtual reality (VR) platform.
For example, a user is shaking hands with a VR character,
who offers his/her hand but does not show any facial ex-
pressions. His/Her expressionless face may make the user
confused or even scared.

Our work proposes a method to automatically generate
facial expressions for virtual characters. Given two charac-
ters’ motions, their social relation, and the emotion of one
of the characters, our task is to sample the ending facial
expression of the other character from his/her starting fa-
cial expression. We consider six scenarios: wave hands (be-
tween friends), high-ﬁve (between brothers), shake hands
(between strangers), apologize (employee to employer), crit-
icize (teacher to student), and quarrel (between colleagues).
Figure 8 depicts the snapshots of motions of two characters.
We apply the emotion dynamic qe for a total number of
20 times for each scenario, and change the value v, a, or d
by ±0.1 for each proposed pg(cid:48). Table 2 shows the sampling
results. With the help of VAD scores, facial expression can
be synthesized accurately.

Motion completion
Common methods to control the motions of a character in-
cluding ﬁnite state machines and behavior trees highly rely

on manually created animations. Our model also provides
an innovative way to help generate body poses for charac-
ters. Given the two characters’ emotions and their social re-
lation, our task is to sample the other character’s next body
pose based on his/her previous poses. We consider the same
six scenarios as those in emotion sampling. However, in this
case, we only keep the motions of the left character and set
up the starting pose of the right one as standing in those
scenarios. The social relation and emotions follow those in
Table 2.

Figure 8 plots the skeletons of sampled poses. For each
scenario, we sample two pose sequences by VRNN that rep-
resent the character’s body movement after 0.5 second and
1.0 second. We can see that in most cases, our sampled mo-
tions can interpret some social interaction meanings. For ex-
ample, to respond to the left character’s high-ﬁve proposal,
the right character may raise his hand or jump happily.

Figure 8: Motion sampling results. Our model samples the
animations (shown in the rectangle) that represent the re-
sponse of a character in different scenarios. For each sce-
nario, we sample two sets of body poses.

Conclusion
We propose a method to generate animations of two-
character social interaction scenes automatically. Our ap-
proach can be useful for the tasks including but not limited
to: i) assisting animators to make keyframe animations in-
cluding character poses and facial expressions. ii) helping
game developers generate vivid NPC interaction events; iii)
offering better emotional intelligence for VR agents.

In Unity for

References
Ahn, J.; Gobron, S.; Garcia, D.; Silvestre, Q.; Thalmann, D.;
and Boulic, R. 2012. An NVC emotional model for con-
versational virtual humans in a 3D chatting environment.
In International Conference on Articulated Motion and De-
formable Objects, 47–57. Springer.
Arafa, Y.; Kamyab, B.; Mamdani, E.; Kshirsagar, S.;
Magnenat-Thalmann, N.; Guye-Vuill`eme, A.; and Thal-
mann, D. 2002. Two approaches to scripting character ani-
mation. Technical report.
Autodesk, INC. ???? Maya.
Bakker, I.; Van der Voordt, T.; Vink, P.; and de Boon, J. 2014.
Pleasure, arousal, dominance: Mehrabian and Russell revis-
ited. Current Psychology, 33(3): 405–421.
Blackman, S. 2014. Rigging with Mixamo.
Absolute Beginners, 565–573. Springer.
Cao, C.; Bradley, D.; Zhou, K.; and Beeler, T. 2015. Real-
time high-ﬁdelity facial performance capture. ACM Trans-
actions on Graphics (ToG), 34(4): 1–9.
Chib, S.; and Greenberg, E. 1995. Understanding the
metropolis-hastings algorithm. The american statistician,
49(4): 327–335.
Chung, J.; Kastner, K.; Dinh, L.; Goel, K.; Courville, A. C.;
and Bengio, Y. 2015. A recurrent latent variable model for
sequential data. In Advances in neural information process-
ing systems, 2980–2988.
Cohn, J. F.; Ambadar, Z.; and Ekman, P. 2007. Observer-
based measurement of facial expression with the Facial Ac-
tion Coding System. The handbook of emotion elicitation
and assessment, 1(3): 203–221.
Creswell, A.; White, T.; Dumoulin, V.; Arulkumaran, K.;
Sengupta, B.; and Bharath, A. A. 2018. Generative adversar-
ial networks: An overview. IEEE Signal Processing Maga-
zine, 35(1): 53–65.
Doersch, C. 2016. Tutorial on variational autoencoders.
arXiv preprint arXiv:1606.05908.
Fan, G.; Zhang, X.; and Ding, M. 2011. Gaussian process
for human motion modeling: A comparative study. In 2011
IEEE International Workshop on Machine Learning for Sig-
nal Processing, 1–6. IEEE.
Fragkiadaki, K.; Levine, S.; Felsen, P.; and Malik, J. 2015.
Recurrent network models for human dynamics. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, 4346–4354.
Holden, D.; Komura, T.; and Saito, J. 2017.
Phase-
functioned neural networks for character control. ACM
Transactions on Graphics (TOG), 36(4): 1–13.
Holden, D.; Saito, J.; and Komura, T. 2016. A deep learning
framework for character motion synthesis and editing. ACM
Transactions on Graphics (TOG), 35(4): 1–11.
Karakurt, G.; and Cumbie, T. 2012. The relationship be-
tween egalitarianism, dominance, and violence in intimate
relationships. Journal of Family Violence, 27(2): 115–122.
Lee, K.; Lee, S.; and Lee, J. 2018. Interactive character an-
imation by learning multi-objective control. ACM Transac-
tions on Graphics (TOG), 37(6): 1–10.

Liu, J.; Shahroudy, A.; Wang, G.; Duan, L.-Y.; and Kot,
A. C. 2019. Skeleton-based online action prediction using
scale selection network. IEEE transactions on pattern anal-
ysis and machine intelligence, 42(6): 1453–1467.
Maurer, T.; Elagin, E. V.; Nocera, L. P. A.; Steffens, J. B.;
and Neven, H. 2001. Wavelet-based facial motion capture
for avatar animation. US Patent 6,272,231.
Miller, M. R.; Jun, H.; Herrera, F.; Yu Villa, J.; Welch, G.;
and Bailenson, J. N. 2019. Social interaction in augmented
reality. PloS one, 14(5): e0216290.
Mohammad, S. 2018. Obtaining reliable human ratings of
valence, arousal, and dominance for 20,000 english words.
In Proceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
174–184.
Oh, T.-H.; Dekel, T.; Kim, C.; Mosseri, I.; Freeman, W. T.;
Rubinstein, M.; and Matusik, W. 2019.
Speech2Face:
In Proceedings of
Learning the Face Behind a Voice.
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR).
Pavllo, D.; Feichtenhofer, C.; Auli, M.; and Grangier, D.
2019. Modeling human motion with quaternion-based neu-
ral networks. International Journal of Computer Vision, 1–
18.
Qi, S.; Zhu, Y.; Huang, S.; Jiang, C.; and Zhu, S.-C.
2018. Human-centric indoor scene synthesis using stochas-
In Proceedings of the IEEE Conference on
tic grammar.
Computer Vision and Pattern Recognition, 5899–5908.
Ren, L.; Patrick, A.; Efros, A. A.; Hodgins, J. K.; and Rehg,
J. M. 2005. A data-driven approach to quantifying natu-
ral human motion. ACM Transactions on Graphics (TOG),
24(3): 1090–1097.
Shu, T.; Ryoo, M. S.; and Zhu, S.-C. 2016. Learning so-
cial affordance for human-robot interaction. arXiv preprint
arXiv:1604.03692.
Silvio, T. 2010. Animation: The new performance? Journal
of Linguistic Anthropology, 20(2): 422–438.
Starke, S.; Zhang, H.; Komura, T.; and Saito, J. 2019. Neural
state machine for character-scene interactions. ACM Trans.
Graph., 38(6): 209–1.
Takeuchi, A.; and Naito, T. 1995. Situated facial displays:
In Proceedings of the SIGCHI
towards social interaction.
conference on Human factors in computing systems, 450–
455.
Tanco, L. M.; and Hilton, A. 2000. Realistic synthesis of
novel human movements from a database of motion cap-
ture examples. In Proceedings Workshop on Human Motion,
137–142. IEEE.
Taylor, G. W.; and Hinton, G. E. 2009. Factored conditional
restricted Boltzmann machines for modeling motion style.
In Proceedings of the 26th annual international conference
on machine learning, 1025–1032.
Taylor, S.; Kim, T.; Yue, Y.; Mahler, M.; Krahe, J.; Ro-
driguez, A. G.; Hodgins, J.; and Matthews, I. 2017. A deep
learning approach for generalized speech animation. ACM
Transactions on Graphics (TOG), 36(4): 1–11.

Turk, M.; and Pentland, A. 1991. Eigenfaces for recognition.
Journal of cognitive neuroscience, 3(1): 71–86.
Vougioukas, K.; Petridis, S.; and Pantic, M. 2019. Realis-
tic speech-driven facial animation with gans. International
Journal of Computer Vision, 1–16.
Wang, J. M.; Fleet, D. J.; and Hertzmann, A. 2007. Gaussian
process dynamical models for human motion. IEEE trans-
actions on pattern analysis and machine intelligence, 30(2):
283–298.
Warriner, A. B.; Kuperman, V.; and Brysbaert, M. 2013.
Norms of valence, arousal, and dominance for 13,915 En-
glish lemmas. Behavior research methods, 45(4): 1191–
1207.
Waters, R. C.; Anderson, D. B.; Barrus, J. W.; Brogan, D. C.;
Casey, M. A.; McKeown, S. G.; Nitta, T.; Sterns, I. B.; and
Yerazunis, W. S. 1997. Diamond park and spline: Social vir-
tual reality with 3d animation, spoken interaction, and run-
time extendability. Presence: Teleoperators & Virtual Envi-
ronments, 6(4): 461–481.
Wei, S.-E.; Saragih, J.; Simon, T.; Harley, A. W.; Lombardi,
S.; Perdoch, M.; Hypes, A.; Wang, D.; Badino, H.; and
Sheikh, Y. 2019. VR facial animation via multiview image
translation. ACM Transactions on Graphics (TOG), 38(4):
1–16.
Xiong, C.; Shukla, N.; Xiong, W.; and Zhu, S.-C. 2016.
Robot learning with a spatial, temporal, and causal and-or
graph. In 2016 IEEE International Conference on Robotics
and Automation (ICRA), 2144–2151. IEEE.
Yu, D.; Sun, K.; Cardie, C.;
Dialogue-Based Relation Extraction.
arXiv:2004.08056.
Yun, K.; Honorio, J.; Chattopadhyay, D.; Berg, T. L.; and
Samaras, D. 2012. Two-person interaction detection using
body-pose features and multiple instance learning. In 2012
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition Workshops, 28–35. IEEE.
Zhang, X.; and van de Panne, M. 2018. Data-driven auto-
completion for keyframe animation. In Proceedings of the
11th Annual International Conference on Motion, Interac-
tion, and Games, 1–11.

and Yu, D. 2020.
arXiv preprint

