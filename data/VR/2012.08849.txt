Affective visualization in Virtual Reality 

Affective visualization in Virtual Reality: An integrative review 

Andres Pinilla1, 2 *, Jaime Garcia2, William Raffe2, Jan-Niklas Voigt-Antons1, 3, 
Robert P. Spang1, Sebastian Möller1, 3 

1 Quality and Usability Lab, Institute for Software Technology and Theoretical Computer 
Science, Faculty of Electrical Engineering and Computer Science, Technische Universität 
Berlin, Germany 

2 UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS, 
Sydney, Australia 

3 German Research Center for Artificial Intelligence (DFKI), Berlin, Germany 

* Correspondence:  
Andres Pinilla 
andres.pinilla@qu.tu-berlin.de 

Keywords: virtual reality, affect, emotion, electrophysiology, visual design 

Abstract 

A  cluster  of  research  in  Affective  Computing  suggests  that  it  is  possible  to  infer  some 
characteristics of users’ affective states by analyzing their electrophysiological activity in real-
time. However, it is not clear how to use the information extracted from electrophysiological 
signals  to  create  visual  representations  of  the  affective  states  of  Virtual  Reality  (VR)  users. 
Visualization of users’ affective states in VR can lead to biofeedback therapies for mental health 
care.  Understanding  how  to  visualize  affective  states  in  VR  requires  an  interdisciplinary 
approach  that  integrates  psychology,  electrophysiology,  and  audio-visual  design.  Therefore, 
this review aims to integrate previous studies from these fields to understand how to develop 
virtual  environments  that  can  automatically  create  visual  representations  of  users’  affective 
states.  The  manuscript  addresses  this  challenge  in  four  sections:  First,  theories  related  to 
emotion and affect are summarized. Second, evidence suggesting that visual and sound cues 
tend to be associated with affective states are discussed. Third, some of the available methods 
for  assessing  affect  are  described.  The  fourth  and  final  section  contains  five  practical 
considerations for the development of virtual reality environments for affect visualization. 

1 

 
 
 
 
 
 
 
 
 
Affective visualization in Virtual Reality 

Virtual  Reality  (VR)  systems  offer  endless  possibilities  for  the  development  of  interactive 
experiences. They are used for the development of tools in diverse areas such as rehabilitation 
therapy (Garcia & Navarro, 2014), exergames (Arndt et al., 2018; Greinacher et al., 2020), and 
robotics (Burdea et al., 2013). Their potential is particularly promising when combined with 
technological advances in Affective Computing, allowing to interpret users’ affective states as 
computer commands (Hernandez et al., 2014; Leslie et al., 2015; Picard et al., 2001; Sitaram et 
al., 2011) and adapt the content of a virtual environment accordingly (Bermudez i Badia et al., 
2019). 

Traditional  psychological  tasks  for  the  treatment  and  diagnosis  of  mental  disorders  can  be 
replaced by VR systems (Belger et al., 2019; J. Blum et al., 2019, 2020; Koenig et al., 2011). 
These new tools are less time-consuming and provide more realistic environments, hence higher 
ecological  validity.  Furthermore,  VR  might  be  helpful  for  at  least  two  types  of  therapy: 
exposure therapy and biofeedback therapy. Exposure therapy is commonly used to treat anxiety 
disorders caused by phobias. Patients are systematically exposed to the stimuli that trigger the 
phobia in a controlled environment and with a therapist's guidance. VR is useful for exposure 
therapy  because  it  allows  delivering  realistic  experiences  while  providing  control  over  the 
stimuli.  Previous  research  suggests  that  exposure  therapy  in  VR  might  be  effective  for  the 
treatment of a least three types of phobias: social phobia (Shiban et al., 2015), claustrophobia 
(Shiban et al., 2016), and spider-phobia (Peperkorn et al., 2014). 

Biofeedback  therapy  is  used  to  provide  real-time  feedback  to  the  patient  about  their 
physiological activity while they perform a task (J. Blum et al., 2020). The characteristics of 
the task depend on the purpose of the therapy. For example, Blandón et al. (2016) developed a 
biofeedback game for training attention control in children with Attention Deficit Hyperactivity 
Disorder (ADHD). The player performed tasks in a virtual farm, such as collecting fruits and 
repairing a pathway. Participants were challenged to increase their concentration, impulsivity 
control,  and  sustained  attention  to  do  these  tasks.  Simultaneously,  electroencephalography 
(EEG) signals were processed in real-time to identify EEG activity associated with attention 
state. The player obtained a game score if attention state was detected in the EEG signals. 

Similarly, Cavazza et al. (2014) developed an interactive experience to enhance empathy using 
neurofeedback.  The  participant  interacted  with  a  fictional  doctor  that  was  going  through  a 
difficult situation. Simultaneously, the participant's EEG signals were analyzed to estimate the 
affective response towards the doctor. If the system detected a positive affective response in 
the  player,  the  storyline  would  change  positively  (the  doctor  would  struggle  less).  It  was 
expected  that  those  changes  in  the  story  line  would  reinforce  the  supportive,  empathetic 
behavior of the player. 

Patients who lack affective self-regulation could benefit from VR biofeedback therapy to train 
affective self-control, fostering mood regulation (Desmet, 2015). Li et al. (2016) conducted an 
experiment  where  twenty-three  participants'  brain  activity  was  analyzed  in  real-time  using 
functional Magnetic Brain Imaging (fMRI). They asked participants to evoke a happy or sad 
memory and provided feedback about their affective state. The feedback was provided with a 
bar on a screen. The bar’s level increased when the fMRI data indicated that the participant 
successfully evoked the happy or sad memory. Results suggest that providing visual feedback 
allowed participants to learn how to modulate their neural activity. But it is not clear how to 
implement this finding in a therapeutic application that can be accessible for a large population 
because  (1)  fMRI  is  an  expensive  technology  that  is  not  accessible  for  most  people,  (2) 
participants must remain motionless for long periods during fMRI recordings; otherwise, the 
data is corrupted, and (3) an ideal therapeutic application should consist of engaging content 
that motivates users to use the system. These challenges could be solved by measuring brain 
than  fMRI,  such  as 
activity  with  a 

less  expensive  and  more  portable  method 

2 

 
Affective visualization in Virtual Reality 

electroencephalography  (EEG).  The  visual  feedback  could  be  provided  using  game-like 
elements. 

The  development  of  an  affective  visualization  tool  in  VR  would  require  at  least  two 
components: (1) A set of VR stimuli with affective content whose properties can be adjusted in 
real-time, and (2) a technique to continuously assess affective states in an online fashion without 
interrupting  the  VR  experience.  This  literature  review  was  elaborated  to  understand  how  to 
develop  those  components.  Both  requirements  are  addressed  in  three  subsections.  Firstly, 
theories related to emotions and affect are presented. Secondly, findings related to visual and 
sound cues that are associated with affective responses are analyzed. And thirdly, some of the 
most common methods for detecting affective states are summarized. 

1 

Theoretical models of emotion and affect 

The terms emotion and affect are often used interchangeably in the literature, but they are not 
exactly the same. There is not a general agreement about how to define these concepts. In this 
manuscript,  emotions  are  defined  as  mental  states  that  coordinate  the  operation  of  cognitive 
processes.  This  definition  is  based  on  the  assumption  that  the  human  mind  is  designed  as  a 
computational  system  that  consists  of  a  series  of information-processing  programs  (Putnam, 
1967).  Emotions  are  a  particular  type  of  program  that  coordinate  other  programs’  operation 
(Cosmides  &  Tooby,  1994).  Affect  is  defined  as  the  cognitive  representation  of  the  bodily 
changes  that  come  with  emotions  (Barrett  &  Bliss-Moreau,  2009;  Wundt,  1897).  Neither 
emotion  nor  affect  can  be  directly  observed  or  measured.  However,  affect  is  conceptually 
associated  with  the  physiological  changes  of  the  body.  Therefore,  it  is  reasonable  to  use 
electrophysiological  signals  to  infer  affective  states,  which  might  allow  to  infer  some 
characteristics of emotional states. 

1.1  Emotion theories 

The Ortony, Clore & Collins (OCC) theory of emotions (Ortony et al., 1988) has been widely 
used in the field of computer science to model users’ emotional responses (e.g., Conati & Zhou, 
2002; Jaques & Vicari, 2007). This theory describes emotions in terms of twenty-two categories 
and  assumes  a  clear  distinction  between  each  category.  This  approach  is  compatible  with 
existing  emotion  recognition  algorithms  because  these  are  usually  based  on  categorizing 
emotions (e.g., Harischandra & Perera, 2012; Mavridou et al., 2017). According to the OCC 
theory  (Ortony  et  al.,  1988),  the  first  step  in  an  emotional  response  is  the  perception  of  the 
situation.  Then  the  situation  is  evaluated  (appraisal),  and  finally,  the  emotional  response 
emerges.  However,  this  theory  does  not  consider  the  physiological  changes  associated  with 
emotions. 

Similarly,  Robert  Plutchnik  proposed  a  structural  model  of  emotions  (Plutchik,  1982), 
commonly  known  as  Plutchnik’s  wheel  of  emotions.  This  model  consists  of  eight  primary 
states: ecstasy, adoration, terror, amazement, grief, loathing, rage, and vigilance. According to 
Pluthnik’s theory, any emotion can be described as a combination of a subset of those basic 
states. Here emotions are defined as a sequence of reactions towards a stimulus. This sequence 
includes a cognitive evaluation of the stimuli (appraisal), feelings (subjective experience of the 
emotion), autonomic neural activity, and behavioral responses. 

There are at least three other major emotion theories: the James-Lang Theory (Lange & James, 
1922), the Cannon-Bard Theory (Bard, 1934; Cannon, 1927), and the Schachter-Singer Theory 
(Schachter  &  Singer,  1962).  According  to  Shiota  &  Kalat  (2012),  these  theories  have  in 
common the assumption that emotional responses have four components but differ in the order 
those components take place during an emotional response. The components are: 

3 

 
Affective visualization in Virtual Reality 

•  Appraisal:  The  cognitive,  rationalized  evaluation  of  the  context  where  the  emotional 

response is produced. 

•  Feeling: The subjective, momentary experience of the emotion. 
•  Physiological change: The bodily changes produced by the emotional response. 
•  Behavior: The observable conduct that comes with the emotion. 

According to the James-Lang Theory (Lange & James, 1922), the first step in an emotional 
response is the cognitive evaluation of the situation. Then, physiological changes are produced 
in the body, at the same time that a behavioral response is produced. Lastly, feelings take place. 

The  Cannon-Bard  theory  (Bard,  1934;  Cannon, 1927)  proposes  that  all  the  elements  of  an 
emotional response are independent of each other, and there is no particular order in which they 
occur. This theory is not compatible with the convincing amount of evidence indicating that 
emotional  stimuli  tend  to  trigger  automatic  changes  in  the  body  (e.g.,  Dimberg  et  al.,  2000; 
Huster et al., 2009; Thayer et al., 2009). Overall, these previous studies suggest interdependence 
between physiological changes and the other components of emotion. 

According to the Schachter-Singer theory (Schachter & Singer, 1962), physiological changes 
occur first. Then the user tries to find an explanation in the environment for those physiological 
changes.  Depending  on  the  explanation  found,  a  label  is  assigned  to  the  bodily  changes 
perceived.  Therefore,  the  physiological  changes  indicate  the  intensity  of  the  emotional 
experience, but cognitive factors determine the emotion’s valence (pleasant vs. unpleasant).  

1.2  Theoretical models of affect 

Theoretical models of affect can be classified into two major groups: discrete and dimensional 
models.  Discrete  models  are  based  on  a  categorical  division  of  affective  responses,  while 
dimensional models represent affect as an array of continuous variables. Both types of models 
are commonly used in Affective Computing to build affect recognition models (e.g., Hernandez 
et al., 2014; Leslie et al., 2015; Sitaram et al., 2011). 

In broad terms, discrete models propose the existence of a few primary states, such as happiness, 
sadness,  and  anger.  Affective  responses  are  a  combination  of  a  subset  of  those fundamental 
states.  Evidence  obtained  by  Ekman  and  Friesen  (1971)  during  an  experiment  conducted  in 
New Guinea supports discrete models. In this experiment, stories with emotional content were 
told to 153 participants. One hundred thirty of them (84.97%) had no previous contact with the 
western culture. After each story had been told, participants saw a series of pictures of facial 
expressions  and  were  asked  to  choose  the  more  coherent  face  with  the  story.  Interestingly, 
participants associated similar facial expressions to the same stories, regardless of their cultural 
background. Based on this evidence, it was proposed that there are at least six facial expressions 
that  are  universal  (i.e.,  they  are  not  affected  by  culture):  happiness,  anger,  sadness,  disgust, 
surprise, and fear. These results are consistent with earlier contributions from Charles Darwin, 
who pointed out the existence of activation patterns in facial muscles which are associated with 
affective states (Darwin, 1872; Ekman, 2006). 

Dimensional  models  have  their  roots  in  the  early  contributions  of  Wilhelm  Wundt,  who 
proposed  that  affective  responses  have  three  dimensions:  valence  (pleasant  –  unpleasant), 
arousal (arousing – subduing), and intensity (strain – relaxation) (Wundt, 1897). On this basis, 
the Circumplex Model of Affect (Russell, 1980) was developed, representing affect in a two-
dimensional  space,  where  valence  and  arousal  are  equivalent  to  the  x-axis  and  y-axis, 
respectively. 

Other authors have proposed the Evaluative Space Model (ESM) (Cacioppo et al., 1997), which 
has three dimensions: Negativity in the x-axis, Positivity in the y-axis, and Net predisposition 

4 

 
Affective visualization in Virtual Reality 

(to  withdraw  or  approach  a  stimulus)  in  the  z-axis.  Unlike  the  Circumplex  Model  of  Affect 
(Russell,  1980),  the  ESM  (Cacioppo  et  al.,  1997)  contemplates  the  existence  of  affective 
responses with simultaneous negative and positive activation (“bitter-sweet” affective states). 
For example, while playing a terror video game, the user might feel fear, and at the same time, 
might feel excited because there is not a real danger. An analysis about dimensional models of 
affect can be found in Mattek et al. (2017). 

The ESM proposes the existence of the negativity bias and the positivity offset. The negativity 
bias implies that negative activation produces more changes in the motivation to withdraw or 
approach stimuli than positive activation. Evidence supporting the existence of the negativity 
bias indicates that negative stimuli tend to produce more salient behaviors than positive stimuli 
(Sutherland & Mather, 2012), and negative stimuli tend to be associated with higher arousal 
than positive stimuli (Lang et al., 2008). The negativity bias suggests that terror video games 
should  trigger  higher  arousal  than  video  games  associated  with  positive  affective  states. 
However,  a  recent  study  indicates  that  the  arousal  level  triggered  by  terror  video  games  is 
slightly lower than the arousal triggered by video games associated with positive affective states 
(Martínez-Tejada et al., 2021). 

The  positivity  offset  implies  a  slight  positive  motivation  to  approach  unknown  stimuli  in  a 
neutral  environment.  This  mechanism  has  been  associated  with  humans'  natural  tendency  to 
explore  new,  unthreatening  environments,  even  when  that  behavior  is  not  associated  with  a 
reward (Cacioppo et al., 1997, p. 12). Further research about the positivity offset could help 
understand  how  to  motivate  VR  users  to  explore  virtual  environments.  For  example,  to 
stimulate engagement of players with VR games. 

2 

Visual and sound cues 

Building  a  virtual  environment  for  affective  visualization  requires  content  that  any  user  can 
associate with a wide range of affective states, regardless of cultural differences or personal 
preferences. Therefore, this section presents recent studies suggesting an association between 
some characteristics of graphical elements and affective states. We do not intend to define a set 
of  rules  about  how  to  communicate  affect  with  audio-visual  elements.  Instead,  we  aim  to 
provide  guidelines  about  visual  and  sound  features  when  creating  visual  representations  of 
affective states. 

2.1  Visual cues 

Rounded objects are associated with higher valence and lower arousal than sharp objects (Bar 
& Neta, 2006). And rounded lines are perceived as more attractive than straight or angular lines 
(Aronoff,  2006;  Aronoff  et  al.,  1992).  Given  that  attractiveness  is  associated  with  positive 
affective states, rounded lines are likely associated with positive valence. Additional studies 
suggest  that  visual  complexity  plays  a  role  in  the  likability  of  objects.  People  tend  to  prefer 
extremely  simple  or  extremely  complex  objects  (Norman  et  al.,  2010).  Given  that  likability 
tends to trigger positive valence (Ryali et al., 2020), an intermediate level of complexity is more 
likely associated with negative valence. 

A cross-cultural study showed that the most critical factors in the affective meaning of color 
are brightness and saturation, while hue has a secondary role (Gao et al., 2007). These results 
are consistent with evidence reported in Valdez & Mehrabian (1994) but contrast with recent 
studies indicating that hue has a significant role in the affective state associated with a color 
palette  (Bartram  et  al.,  2017).  Additional  evidence  suggests  that  blue,  green,  and  purple  are 
among the most pleasant hues, while yellow is among the most unpleasant. Green-yellow, blue-
green, and green are the most arousing, while purple-blue and yellow-red are among the least 
arousing (Palmer & Schloss, 2010). Similarly, it has been found that the most pleasant colors 

5 

 
Affective visualization in Virtual Reality 

are those with higher saturation and brightness (Camgöz et al., 2002; Wilms & Oberfeld, 2018). 
However,  other  studies  suggest  that  there  are  not  universal  associations  between  colors  and 
affective states. People tend to like colors associated with objects they like and dislike colors 
associated with objects they dislike (Palmer & Schloss, 2010). Additional evidence indicates 
that color associations change according to the context where colors are used (Lipson-Smith et 
al., 2020), supporting the hypothesis that there are not universal associations between colors 
and  affective  states.  Yet,  it  is  possible  to  establish  color  palettes  that  allow  to  communicate 
affective states. For example, bright, unsaturated colors are more suitable to communicate calm, 
while dark, red colors are more suitable to communicate disturbance (Bartram et al., 2017). 

Textures may influence the affective meaning of color (Ebe & Umemuro, 2015; Lucassen et 
al., 2011). This has been demonstrated by pairing colors with computer-generated textures and 
asking  participants  to  rate  the  color-texture  pairs using  four  scales:  Warm-Cool,  Masculine-
Feminine, Hard-Soft, and Heavy-Light. Results suggest that texture significantly influences the 
evaluation on the Hard-Soft scale and has a minor impact on the other scales. However, this 
evidence  does  not  allow  to  identify  associations  between  particular  texture  patterns  and 
affective responses. 

Non-static visual elements have other visual properties besides color, shape, and texture. Some 
of  these  additional  properties  are  speed,  motion  shape,  direction,  and  path  curvature.  Fast-
moving objects are associated with higher arousal than slow-moving objects (Feng et al., 2014; 
Piwek et al., 2015). But there are contradictory findings regarding the type of valence associated 
with speed. One study suggest that fast movements are related to positive affective states (Piwek 
et al., 2015), while other study indicates the opposite (Feng et al., 2014). 

Linear motion with straight paths is associated with low arousal and positive valence (Feng et 
al., 2014). Jerky paths are associated with higher arousal than straight paths in linear motion 
(Feng  et  al.,  2014;  Lockyer  et  al.,  2011).  But  the  curvature  of  the  path  has  no  incidence  in 
affective associations when applied to spherical or radial motion (Feng et al., 2014). Inward 
movements are related to more positive affective states than outward movements (Feng et al., 
2014). Downwards-right motion tends to be linked to positive states, while upwards-left motion 
tends to be associated with negative states (Lockyer et al., 2011). In general, angular paths are 
related to more negative affective states than linear paths (Lockyer et al., 2011). And spherical 
motion patterns tend to be associated with higher arousal than linear motion patterns. 

2.2  Sound cues 

Previous research indicates that the location of a sound source influences the affective states 
associated with that sound. When the user cannot see where the object is (outside of the field 
of view), it is often associated with more arousing affective states than when the user can see it 
(inside  the  field  of  view)  (Drossos  et  al.,  2015;  Tajadura-Jiménez,  Larsson,  et  al.,  2010). 
Similarly,  sounds  located  further  away  in  the  space  are  related  to  less  arousing  responses 
(Tajadura-Jiménez  et  al.,  2008).  The  perception  of  an  approaching  sound  is  associated  with 
more arousing responses than the perception of it moving away (Tajadura-Jiménez, Väljamäe, 
et al., 2010). These phenomena are likely to be linked to mechanisms enforced by evolution 
(Cosmides & Tooby, 1994). Our primitive ancestors had more chances to survive if they were 
aware of the most potentially dangerous objects, such as those they could not see, were closer 
to them, or were approaching them. 

The reverberation of the sound, which is associated with space’s size, can influence affective 
associations (Tajadura-Jiménez, Larsson, et al., 2010). Lower reverberation (smaller rooms) is 
linked to more pleasant states than higher reverberation (larger rooms). Perhaps, because the 
primitive  human  being  was  better  protected  from  predators  in  closed  spaces,  leading  to  an 

6 

 
Affective visualization in Virtual Reality 

evolutionary  process  that  favors  the  activation  of attentional  resources  when  we  are  in  open 
areas.  

Other studies indicate that asking people to rate pictures with affective content while listening 
to the sound of a heartbeat can influence their affective evaluations, as well as their heart rate 
(Tajadura-Jiménez et al., 2008). Here, the sound of a heart rate faster than the listener’s one 
tends to increase their heart rate, while a slower sound seems to relax the listener’s heart rate. 
Therefore, playing a fast heartbeat in the background might be an effective way of representing 
an increase in arousal. 

On the other hand, music is pivotal for affective visualization because it can contribute to create 
more immersive experiences. However, it is a vast topic that will not be fully covered in this 
manuscript. Yet, it is important to mention that tempo influences music’s affective perception 
(Fernández-Sotos et al., 2016). Faster tempo tends to be associated with higher arousal ratings, 
while  slower  tempo  tends  to  be  associated  with  lower  arousal  ratings.  To  the  extent  of  our 
knowledge, there is no evidence suggesting that tempo influences valence ratings. 

Major and minor chords are associated with positive and negative affective states, respectively 
(Gerardi & Gerken, 1995). Similarly, dissonant harmonies tend to be strongly associated with 
anger, and to a lesser extent, with fear (Petri, 2009). And it is possible to compose music based 
on  people’s  affective  states  (Williams  et  al.,  2017).  However,  it  remains  an  open  question 
whether it is feasible to do it in real-time, based on the user’s electrophysiological signals. 

Table 1. Summary of audio-visual cues associated with affective states, according to previous 
studies. 

Static 
visual cues 

Shape 

Lines 

Hue 

High 
Arousal 

Low 
Arousal 

High 
(Positive) 
Valence  

Low 
(Negative) 
Valence  

Angular 

Rounded 

Rounded 

Green-yellow, 
blue-green, and 
green 

Purple-blue 
and yellow-
red 

Rounded 

Blue, green, 
and purple 

Angular 

Straight 

Yellow 

Saturation 

Brightness  

Visual complexity  

N/A 

N/A 

N/A 

N/A 

N/A 

N/A 

Non-static 
visual cues 

Speed 

Fast 

Slow 

Motion shape 

Spherical 

Direction 

N/A 

Linear 

N/A 

Path curvature 

Jerky 

Straight  

Sound cues  Source location 

Outside field 
view 

Inside field 
view 

Saturated 

Unsaturated 

Bright 

Dark 

Extremely 
complex or 
extremely 
simple  

Neither 
complex nor 
simple  

Some studies suggest that fast 
motion is associated with 
positive valence, while others 
suggest the opposite. 

N/A 

N/A 

Downwards-
right, inward 

Upwards-left, 
outward 

Jerky 

N/A 

Straight 

N/A 

7 

 
 
 
 
 
Affective visualization in Virtual Reality 

Distance to the  
sound source 

Sound source 
movement 

Heartbeat sound 

Near 

Far 

N/A 

Approaching 

Receding 

N/A 

Fast (above 
100 bpm) 

Slow (below 
60 bpm) 

N/A 

N/A 

N/A 

N/A 

N/A 

N/A 

Major scale 

Minor scale 

Music 

Tempo 

Harmony 

Fast 

N/A 

Slow 

N/A 

2.3  Personalized affective visualizations 

There might be individual differences in the affective states that each user associates with the 
same audio-visual stimuli. These individual differences could be amplified as a consequence of 
personal  experiences.  An  ideal  system  for  affective  visualization  should  account  for  those 
individual differences, delivering personalized visual representations of affective states, similar 
to Bermudez i Badia (2019).  

Semertzidis  et  al.  (2020)  developed  an  Augmented  Reality  (AR)  system  that  automatically 
creates  visual  representations  of  the  user's  affective  states.  The  visualizations  consisted  of 
fractals  generated  using  Procedural  Content  Generation  (PCG).  The  visual  properties  of  the 
fractals  varied  according  to  the  affective  state  detected  in  the  user.  However,  the  evidence 
reported by Semertzidis et al. (2020) does not allow to establish whether participants perceived 
that the fractals' graphical properties represented their affective states. 

Additional  studies  indicate  that  it  is  possible  to  use  PCG  to  create  content  dynamically,  
adjusting  it  to  the  preferences  of  the  user.  This  approach  is  known  as  experience-driven 
procedural content generation (EDPCG) (Raffe et al., 2015; Yannakakis & Togelius, 2011). In 
broad terms, EDPCG consists of an iterative process where the content is constantly modified 
based on the user's feedback. 

The general functioning of EDPCG is the same as an evolutionary algorithm (EA), which is an 
optimization process inspired by natural evolution. In a natural environment, the organisms that 
are better adapted to their habitat tend to have more reproductive success, hence more likely to 
pass their genes to the next generation. Similarly, objects can be created programmatically in a 
virtual  environment  and  tested  to  identify  the  most  successful  ones.  The  criteria  to  identify 
which objects are more successful is based on a previously defined goal. This goal is defined 
by the developer based on the purpose of the application. During each iteration, the objects that 
are more successful at reaching the goal are identified. In the following iterations, new sets of 
objects  are  created,  and  the  characteristics  of  the  most  successful  objects  tend  to  remain, 
whereas the characteristics of the least successful tend to disappear. It is assumed that repeating 
this process several times allows to reach the optimal parameters required to achieve the goal. 
For example, if the goal is to create personalized visual representations of positive affective 
states, and the EA detects that the user tends to associate red, rounded objects with positive 
affective  states,  the  game  would  produce  objects  that  would  tend  to  be  more  red  and  more 
angular. An introduction to EA can be found in Eiben and Smith (Eiben & Smith, 2015). 

Additional research indicates that it is possible to create automatically visual compositions in 
VR  using  Deep  Convolutional  Neural  Networks  (DCNN)  (Kitson  et  al.,  2019).  Overall,  the 
process consists of merging features from two images to create a third image. This approach 
could be combined with EDPCG (Raffe et al., 2015; Yannakakis & Togelius, 2011) to create 
personalized affective visualizations. The process would involve at least three steps: (1) Create 
a set of VR content that all users will observe and used that content as a baseline. This initial 
set of content could be developed following the guidelines described in Table 1; (2) Capture 

8 

 
Affective visualization in Virtual Reality 

user feedback about the visual stimuli to establish the affective state that each user associates 
to each piece of VR content; And (3) use DCNNs to merge features of the initial content onto 
new, personalized VR content. 

3 

Assessment of affective states 

Users’  feedback  should  be  captured  using  methods  that  do  not  interrupt  the  VR  experience, 
such as body movements (see Section 4.2) or electrophysiological signals, similar to Georgiou 
and Demiris (2017). Methods for assessing affective states can be grouped into three categories: 
self-report questionnaires, behavioral measures, and electrophysiological signals. Each method 
has advantages and disadvantages that will be discussed below. 

3.1  Self-reports 

Self-reports  allow  participants  to  evaluate  their  affective  state  by  answering  a  series  of 
questions. They can be used to verify the accuracy of the acquired information through other 
methods,  such  as  behavioral  and  electrophysiological  signals.  Data  collected  through  self-
reports are often used as a ground-truth in the field of HCI. 

In general, self-report measures are relatively easy to implement because they only require to 
display  a  series  of  questions  on  a  paper  sheet  or  a  screen.  Unlike  behavioral  and 
electrophysiological methods, self-reports are considered a direct measure because they allow 
asking participants directly about their mental states (Perkis et al., 2020). However, they are 
susceptible to be biased by rational processes. For example, participants who believe that it is 
expected  from  them  to  respond  in  a  certain  way  might  adjust  their  responses  to  fulfill  that 
expectation, causing a phenomenon known as experimenter bias (Fisher, 1993). Some available 
tools for the assessment of affective responses are the Positive and Negative Affect Schedule 
(PANAS) (Watson et al., 1988), Self-Assessment Manikin (SAM) (Bradley & Lang, 1994), and 
Pick a Mood (PAM) (Desmet et al., 2016). The PANAS consists of 20 words related to negative 
and positive feelings (ten negatives and ten positives). Participants use those words to report 
their affective state. Each word can receive a rating from 1 to 5. 

The SAM (Bradley & Lang, 1994) is an instrument that uses three scales: valence (pleasant / 
unpleasant), arousal (tension / relaxation) and dominance (inhibition / uninhibition). Each scale 
has five pictograms. Participants can select the blank spaces between each pictogram to indicate 
intermediate  states.  Therefore,  answers  to  each  scale  can  take  values  between  1  and  9  (see 
Figure 1). Given that this instrument is based on dimensions, it is compatible with dimensional 
models of affect. The SAM (Bradley & Lang, 1994) is one of the most established instruments 
for assessing affect (over 7.000 citations) and has been used for the development of batteries of 
stimuli  with  emotional  content,  such  as  the  International  Affective  Pictures  System  (IAPS) 
(Lang et al., 2008) and the DEAP dataset (Koelstra et al., 2012). 

9 

 
 
Affective visualization in Virtual Reality 

Fig 1 From top to bottom: valence, arousal, and dominance scales of the Self-Assessment Manikin 
(SAM). Taken from (Bradley & Lang, 1994) 

On the other hand, the PAM (Desmet et al., 2016) is based on discrete states. Therefore, it is 
compatible  with  discrete  models  of  affect.  This  instrument  also  uses  pictorial  cues  to  assess 
participant’s states. There are eight mood types plus a neutral one: excited, cheerful, relaxed, 
calm, bored, sad, irritated, and tense. There are three characters for each of these states: a man, 
a woman, and a robot (gender-neutral character). In comparison to the SAM (Bradley & Lang, 
1994), PAM’s  characters  (Desmet  et  al.,  2016)  are  more  similar  to  a  real  human  being  (see 
Figure  2),  which  might  be  an  advantage  because  it  could  be  easier  for  participants  to  feel 
identified with the characters of the PAM (Desmet et al., 2016). 

Fig 2 Female character of Pick A Mood (PAM), taken from Desmet et al. (2016). Eight discrete states 
are represented, plus a neutral one. 

The  PAM  has  been  used  to  understand  how  to  design  objects  and  experiences  that  could 
stimulate  mood  regulation  (Desmet,  2015),  analyze  the  effect  of  immersive  virtual 
environments  on  gaming  Quality  of  Experience  (QoE)  (Hupont  et  al.,  2015),  and  analyze 
whether the effect of color on affective states varies across different VR rooms (Lipson-Smith 
et  al.,  2020).  Using  scales  to  analyze  experiences  in  virtual  environments  might  require  to 
interrupt  the  VR  experience.  This  limitation  can  potentially  be  counterbalanced  by  using 
subjective rating scales inside the virtual environment (Voigt-Antons et al., 2020). 

3.2  Behavioral measures 

Behavioral measures allow inferring affective states from observable conducts, such as body 
movements (Bull, 1978; Robitaille & McGuffin, 2019), voice patterns (Cordaro et al., 2016; 
Scherer  &  Oshinsky,  1977),  and  facial  expressions  (Ekman  &  Friesen,  1971).  During  an 
experiment conducted by Bull (1978), participants listened to a series of audio recordings with 
emotional content while their body movements were videotaped. Results suggested that sadness 
is associated with dropping the head while boredom is related to leaning the face in one hand. 
Building  on  that,  recent  research  indicates  that  it  is  possible  to  infer  arousal  from  body 
movements  in  virtual  reality  users  (Kapur  et  al.,  2005;  Robitaille  &  McGuffin,  2019).  In 
general, faster body movements are associated with higher arousal. 

It is possible to automatically analyze users’ affective states based on their voice patterns (Vogt 
et  al.,  2008).  Usually,  a  set  of features  are  defined  and  used  to  build  a  classification  model. 
Some of the features used for automatic speech emotion recognition are pitch, loudness, and 
tempo  (Polzehl  et  al.,  2011;  Vogt  et  al.,  2008).  This  approach  is  coherent  with  evidence 

10 

 
 
Affective visualization in Virtual Reality 

suggesting that changes in vocalization patterns have an effect on the affective evaluation of 
speech, e.g. (Banse & Scherer, 1996; Scherer & Oshinsky, 1977).  

Eye-tracking  has  been  an  essential  measure  of  various  individual  states  or  even  personality 
traits (Hoppe et al., 2018). Greinacher & Voigt-Antons (2020) demonstrated recently how this 
measure  could  be  easily  obtained  from  modern  smartphones  using  built-in  system  libraries 
(Greinacher  &  Voigt-Antons,  2020).  The  accuracy  of  this  approach  is  comparable  to  other 
webcam or selfie-cam-based systems. However, as the authors pointed out, having eye-tracking 
systems easily accessible in millions of devices opens up opportunities for remote or in-the-
field studies with a much higher ecological validity than studies relying on heavy equipment 
traditionally used in laboratory investigations. 

As mentioned in section 3.1, facial expressions are associated with affective states (Ekman & 
Friesen, 1971). These expressions can be analyzed visually and described in terms of the Facial 
Action  Coding  System  (FACS)  (Ekman  &  Friesen,  1976).  The  FACS  is  an  instrument  that 
describes all the possible movements of the facial muscles. Each movement is defined as an 
Action Unit (AU). Facial expressions can be described as a combination of a subset of all the 
Action Units defined in the FACS (Ekman & Friesen, 1976). In a study conducted by Porcu et 
al. (2020), AUs were used for real-time analysis of the facial expressions of video streaming 
users.  Additional  studies  suggest  that  human  facial  expressions  can  be  collected  using 
crowdsourcing techniques (D. McDuff et al., 2012), and its analysis can be optimized using 
statistical models that adapt automatically to the characteristics of the data (Feffer et al., 2018). 
However,  facial  recognition  might  be  challenging  to  implement  in  VR  because  the  Head-
Mounted Display (HMD) covers the user’s face. Therefore, facial electromyography (fEMG), 
a technique introduced in the following section, might be more suitable for capturing VR users' 
facial expressions. 

3.3  Electrophysiology 

facial 

electromyography 

Electrophysiological methods allow measuring changes in the electrical potentials of the body. 
Usually, 
and 
(fEMG), 
electroencephalography  (EEG)  are  used  to  record  facial  muscle,  heart,  and  brain  activity, 
respectively.  This  section  focuses  on  methods  to  infer  emotions  in  terms  of  the  Circumplex 
Model of Affect (Russell, 1980) (see Section 2.2.). Therefore, the focus is on techniques that 
can be used to infer valence and arousal. There are many approaches for affect detection using 
electrophysiological  signals  that  are  not  based  on  the  Circumplex  Model  of  Affect (Russell, 
1980) and are not included in this manuscript. 

electrocardiography 

(ECG), 

Arousal can be inferred from features extracted from ECG signals. The beat-to-beat intervals 
of the ECG signal (often referred to as RR-Intervals, RRI) are extracted, detecting its peaks and 
calculating the time lapse between each peak. These RRIs are used to analyze the heart rate 
variability (HRV). Prominent examples of time-domain features used to analyze HRV are the 
root  mean  square  of  successive  differences  (RMSSD)  and  the  standard  deviation  of  NN 
intervals  (SDNN).  It  has  been  found  that  higher  HRV  is  associated  with  higher  emotional 
arousal  (Thayer  et  al.,  2009).  It  is  possible  to  extract  features  from  the  ECG  signal  in  the 
frequency domain by calculating the LF/HF ratio. The low-frequency component (LF) (0.04 to 
0.15 Hz) is associated with parasympathetic activity, while the high-frequency component (HF) 
(0.15 to 0.4 Hz) is associated with sympathetic activity (Malik et al., 1996). The activation of 
the  parasympathetic  system  is  associated  with  relaxation,  and  activation  of  the  sympathetic 
system  is  associated  with  arousal.  Therefore,  more  activity  in  the  HF  component  indicates 
higher  arousal  (Pagani  et  al.,  1984).  Further  research  has  shown  that  it  is  possible  to  infer 
arousal from EEG signals in VR users employing long short-term memory (LSTM) recurrent 
neural networks (RNN) (Hofmann et al., 2018). 

11 

 
Affective visualization in Virtual Reality 

A recent study compared the benefits of implementing HRV biofeedback in virtual reality with 
a  traditional  HRV  biofeedback  therapy  (Blum  et  al.,  2019),  suggesting  that  the  VR 
implementation produces more benefits for users in terms of relaxation self-efficacy, reduced 
mind wandering, and control of attentional resources. A similar approach was proposed in Blum 
et al. (2020), introducing a breathing biofeedback algorithm. This algorithm combines features 
extracted from electrocardiography activity with data inferred from diaphragm movements. The 
experiment  was  conducted  using  a  chest  band  (Polar  H10),  which  is  a  reliable,  relatively 
inexpensive  sensor.  Results  suggest  that  this  approach  can  help  to  foster  more  regular  and 
slower breathing in VR users. 

Valence  can  be  inferred  from  EMG  and  EEG  signals.  Previous  evidence  suggests  that  the 
Corrugator Supercilii muscle activity (located above the eyebrows) is associated with negative 
affective states. In contrast, the Zygomaticus Major muscle activity (located in the cheeks) is 
related to positive affective states (Dimberg, 1982). Changes in facial muscle activity can occur 
without conscious awareness of the participant (Dimberg et al., 2000; Dimberg & Thunberg, 
2012).  However,  it  might  be  challenging  to  implement  EMG  in  a  VR  system  because  the 
pressure  of  the  Head-Mounted  Display  (HDM)  on  the  electrodes  can  create  artifacts  on  the 
recorded signal. 

Asymmetry in the cortical activity of the frontal cortex is also associated to valence . It has been 
found  that  positive  and  negative  emotions  are  processed  in  the  left  and  right  frontal  cortex, 
respectively (Huster et al., 2009; Ray & Cole, 1985; Antons, 2015). Additionally, it has been 
found that cortical activity decreases as the alpha power (frequencies between 8 and 13 Hz) 
increases (Pfurtscheller & Lopes da Silva, 1999). Therefore, increased processing of positive 
stimuli is associated with decreased alpha power in the left frontal cortex (higher activity in the 
left  side  of  the  brain).  Similarly, increased  processing  of  negative  stimuli  is  associated  with 
decreased alpha power in the right frontal cortex (higher activity in the right side of the brain) 
(Davidson, 1992; Huster et al., 2009; Pfurtscheller & Lopes da Silva, 1999). 

These findings are coherent with results obtained by Reuderink et al. (2013) in a study where 
the brain activity of video game players was recorded using EEG. Participants were asked to 
report their affective state using the SAM (Bradley & Lang, 1994) after the game session ended. 
Results  indicated  a  positive  correlation  between  self-reported  valence  and  alpha  asymmetry. 
Likewise, Koelstra et al. (2012) analyzed the brain activity of 32 participants who watched forty 
musical videos and rated their emotional reactions to each video using the SAM (Bradley & 
Lang, 1994). A positive correlation was found between self-reported valence and alpha power 
in the right occipital region of the brain. 

Eye-movements and eye-blinks cause artifacts in the EEG signals and are usually reflected in 
the activity of the frontal region of the brain. In non-stationary VR applications, it is particularly 
challenging  to  remove  artifacts  caused  by  muscle  activity,  head  movements,  or  electrical 
activity from the VR headset (Klug & Gramann, 2020). It is possible to remove these artifacts 
using  Independent  Component  Analysis  (ICA).  This  technique  allows  to  identify  the 
components of an EEG signal that are not produced by brain activity (Makeig et al., 1997). The 
maximum number of independent components (ICs) that can be identified using ICA depends 
on the number of electrodes used. For example, a recording with 32 electrodes will allow to 
identify up to 32 ICs. Therefore, increasing the number of electrodes might help identify the 
artifacts in the signal with more precision. For a complete analysis about using ICA in non-
stationary and stationary settings, see Klug and Gramman (2020). 

An additional challenge is to process the EEG signals in real-time. ICA can be used in real-time 
(Pion-Tonachini et al., 2015), but it was not designed for that purpose. An alternative is Artifact 
Subspace  Reconstruction  (ASR)  (S.  Blum  et  al.,  2019;  Mullen  et  al.,  2015),  a  technique 
designed for online artifact removal. ASR uses data recorded from the user as a baseline. Then, 

12 

 
Affective visualization in Virtual Reality 

principal  component  analysis  (PCP)  is  applied  to  identify  the  EEG  channels  that  contain 
artifacts.  The  data  of  the  corrupted  channels  are  reconstructed  using  the  baseline  data  as  a 
reference. There is software available that can facilitate the implementation of ASR, such as 
BCILAB (Kothe & Makeig, 2013), OpenBiVE (Renard et al., 2010) and Neuropype (Intheon 
Labs, California). 

3.4  Brain-Computer Interfaces 

The implementation of electrophysiological signals in VR systems leads to the development of 
interfaces that allow interpreting users’ brain activity as computer commands (Wolpaw et al., 
2002). One of the basic assumptions underlying the development of Brain-Computer Interfaces 
(BCIs)  is  that  mental  processes  originate  in  the  brain.  But  there  are  BCIs  that  measure 
electrophysiological responses in other places of the body (e.g., Cassani et al., 2018), such as 
the heart and facial muscles, because cognitive processes that originate in the brain can produce 
changes in the activity of other body parts. 

functional  Magnetic  Resonance 

There are different techniques for measuring brain activity that can be used for the development 
of BCIs. For example, electrocorticography (ECoG), Positron Emission Tomography (PET), 
and 
(fMRI),  among  others.  However, 
electroencephalography  (EEG)  is  the  method  most  frequently  used  in  BCIs  because  (1)  it 
provides  high  temporal  resolution  (i.e.,  relatively  large  amount  of  data  points  recorded  per 
second); (2) does not create health risks for the user because the electrodes can be easily placed 
and removed from the scalp; (3) can be portable, which is important for applications where the 
user  is  moving;  and (4)  is  less  expensive  than  most  of the  other  methods  (Zander  &  Kothe, 
2011). 

Imaging 

According  to  Zander  &  Kothe  (2011),  there  are  three  types  of  BCIs:  active,  passive,  and 
reactive.  Active  BCIs  require  the  active  participation  of  the  user  to  generate  an  action.  For 
example,  patients  who  lack  motor  control  can  use  mental  commands  to  move  a  wheelchair 
(Voznenko et al., 2018). Passive BCIs do not require the conscious involvement of the user. 
They  can  be  used,  for  example,  to  analyze  the  cognitive  load  of  car  drivers  automatically 
(Almahasneh  et  al.,  2014).  Reactive  BCIs  use  mental  activity  that  occurs  as  a  response  to 
external  stimuli.  An  example  is  a  neurofeedback  video  game  where  threatening  stimuli  are 
presented, and players have to control their anxiety to obtain game score (Schoneveld et al., 
2016). A VR application for affective visualization, would usually involve either a passive or a 
reactive BCI. 

The typical workflow in a BCI involves at least four steps (Antons et al., 2014; Zander & Kothe, 
2011): 

1.  Preprocessing pipeline: Filter out the signal's noise and keep only the components that 
reflect brain activity. This process involves (but is not limited to) filtering frequency 
bands  and  removing  artifacts  caused  by  eye-movements  or  muscle  activity.  An 
introduction to signal processing can be found in Unpingco (2014). 

2.  Feature extraction: Isolate the information related to the psychological construct of 

interest based on previous neuroscience studies (see Section 4.3). 

3.  Classifier  definition:  A  classification  model  is  created  using  prerecorded  data.  The 
classifier  is  tested  offline,  and  an  estimate  of  the  accuracy  of  the  classification  is 
calculated. In general, classifiers are trained using data that has been previously labeled 
by humans. Machine Learning algorithms are used to identify patterns in the data that 
tend to be associated with each label. 

4.  Classification  application:  The  classification  is  implemented  in  the  BCI  to  perform 
online  analysis  of  the  brain  activity.  The  outputs  of  the  classification  are  used  as 
computer commands. 

13 

 
Affective visualization in Virtual Reality 

4 

Practical considerations 

This section contains five practical considerations that might help during the development of 
a VR system for affective visualization. 

1.  Which are the initial steps for designing a virtual environment? First, define who 
will  use  the  virtual  environment  (target  group)  and  what  the  user  will  do  inside  that 
environment. This will help to have a more clear idea about the interaction events that 
will occur during the experience. Look for other interactive experiences, such as games 
and art installations, that can serve as inspiration. This will trigger ideas and will help 
to  understand  how  to  implement  them.  Then,  define  the  graphical  layout  of  your 
environment (color palette, typographies, and textures). 

2.  Which software should I use for VR development? Unity is probably one of the best 
options. There are alternatives, such as Vizard, a virtual reality software for research. 
However, to the extent of our knowledge, Unity is the only game engine compatible 
with open-source solutions, such as LSL and Excite-O-Meter. Therefore, it is relatively 
easy  and  inexpensive  to  develop  virtual  environments  that  rely  on  the  user's 
physiological data using Unity. 

3.  How to integrate Unity with electrophysiological equipment? One possibility is to 
use  LabStreamingLayer  (LSL),  a  tool for  collecting  time-series  data  in  experimental 
settings. Essentially, LSL allows to collect and synchronize the data and stream it into 
Unity. At the same time, it allows to send data from Unity (e.g., markers) to the signal 
processing software. Another option is to setup a UDP Broadcast to send information 
through your network. 

4.  Is  there  a  ready-to-use  solution  for  integrating  Unity  with  electrophysiological 
equipment?  Yes.  Excite-O-Meter  is  a  Unity  plugin  for  visualizing  cardiovascular 
activity, which is built on top of LSL. It can be used to visualize Heart Rate Variability 
(HRV) (see Section 4.3). By default, the Excite-O-Meter provides a time-series graph 
of the data. But you can customize it to build other types of visualizations. 

5.  How  to  define  the  sampling  rate  for  recording  electrophysiological  signals? 
According the Nyquist-Shannon sampling theorem, the sampling rate should be twice 
the maximum frequency of interest. For example, if you are interested in frequencies of 
up to 128 Hz, you should use a sampling rate of at least 256 Hz. A sampling rate of 256 
Hz means that you are collecting 256 data points per second. 

The usable information for each type of signal is located in a different frequency range. 
Therefore, the maximum frequency of interest for each signal is different. For example, 
the  usable  information  in  an  ECG  signal  is  up  to  100  Hz.  Therefore,  the  sampling 
frequency for ECG signals should be at least 200 Hz. However, previous studies indicate 
that ECG recordings at 200 Hz contain noise in the high-frequency components (Malik 
et  al.,  1996).  This  noise  can  be  reduced  by  recording  at  a  higher  sampling  rate. 
Therefore,  it  is  considered  a  good  practice  to record  ECG  signals  at  a  sampling  rate 
between 256 Hz and 512 Hz, EMG signals at a sampling rate between 512 Hz and 1024, 
and EEG signals at a sampling rate between 256 Hz and 512 Hz. 

5 

Discussion 

This  manuscript  aims  to  understand  how  to  develop  VR  systems  for  affective  visualization. 
These  systems  would  involve  the  development  of  at  least  two  components:  a  virtual 
environment and an affect detection technique. The development of both components requires 
the understanding of theories related to emotion and affect. Therefore, the manuscript analyses 
previous research related to (1) theories of emotion and affect, (2) audio-visual cues associated 
with affective states, and (3) methods for assessment of affective states.  

14 

 
 
Affective visualization in Virtual Reality 

Studies discussed in Section 3 suggest that specific visual and sound cues can represent users’ 
emotions. However, most of these studies were conducted in experimental settings where the 
stimuli were carefully controlled. It is unclear whether the same psychological responses 
would occur if a combination of these cues were used simultaneously. For example, a 
particular combination of “happy” colors may result in an unbalanced visual composition that 
produces negative affective states. Or there might be motion patterns that are more prone to 
produce motion sickness in VR users, triggering negative states. Moreover, the novelty of a 
VR system in new users might bias the emotions they associate with the audio-visual stimuli. 

Other studies mentioned in Section 3 suggest that leftwards linear motion tends to be associated 
with negative valence (Feng et al., 2014; Lockyer et al., 2011). This finding was obtained during 
experiments conducted in a western society, where time is represented as a progression to the 
right (Fuhrman & Boroditsky, 2010). Therefore, it is likely that western users associate leftward 
motion with negative affective states because that type of motion is culturally associated with 
regressing in time. However, in other cultures, such as the Hebrew culture, people represent 
time as a progression to the left (Fuhrman & Boroditsky, 2010). Therefore, it is possible that 
Hebrew users would associate leftward linear motion with positive valence. This hypothesis 
can be tested in future experiments. 

Recent studies have demonstrated that affective states can be elicited by triggering psychogenic 
shivering (PS) (Haar et al., 2020; Schoeller, Haar, et al., 2019), using a device that controls the 
temperature in the upper back of the participants. Additional research indicates that the ability 
to be empathetic with others’ emotions can be influenced by delivering electrical stimulation in 
the vagus nerve (Colzato et al., 2017), and by inducing affective states in the observer through 
videos (Pinilla et al., 2020). It remains an open question how to use those findings to develop 
Mixed  Reality  (MR)  technologies  for  empathy  enhancement,  as  proposed  by  Schoeller, 
Bertrand, et al. (2019). 

Most of the existing techniques for inferring affective states from electrophysiological signals 
require the usage of previously annotated data to train a classifier, e.g. (Harischandra & Perera, 
2012; Mavridou et al., 2017). But the amount of distinct affective states that can be detected 
using this approach is limited. Therefore, it might be convenient to formulate affect detection 
problems in terms of statistical regression. This approach would allow creating a model capable 
of describing affective states in terms of a continuum containing an infinite amount of distinct 
affective states. Previous studies suggest that it is possible to infer arousal from EEG signals 
(Hofmann et al., 2018) as a continuous variable. Future studies could investigate whether it is 
possible to use a similar approach to express valence in terms of a continuous variable. 

Finally, it is possible to use a programmatic approach to create virtual reality content in real-
time, using procedural content generation (PCG) (Bermudez i Badia et al., 2019; Raffe et al., 
2015; Semertzidis et al., 2020; Yannakakis & Togelius, 2011). PCG allows to create content 
dynamically that adjusts to user feedback. Electrophysiological signals could be used to capture 
user  feedback  without  interrupting the  VR  experience.  This  approach  would  allow  to  create 
personalized virtual environments for emotion visualization, similar to Kitson et al. (2019) or 
Bermudez i Badia (2019). 

6 

Declarations 

6.1  Conflict of Interest 

The  authors  declare  that  the  research  was  conducted  in  the  absence  of  any  commercial  or 
financial relationships that could be construed as a potential conflict of interest. 

6.2  Author Contributions 

15 

 
Affective visualization in Virtual Reality 

All authors contributed to the study conception and analysis. Jaime Garcia and William Raffe 
contributed  with  analysis  of  data  related  to  gaming  and  Virtual  Reality.  Jan-Niklas  Voigt-
Antons contributed to the analysis of data related to psychology and electrophysiology. Robert 
Philipp Greinacher contributed with the redaction of the manuscript and with data related to 
electrocardiography  and  eye-tracking.  Sebastian  Möller  contributed  to  the  analysis  of  data 
related to sound design and Machine Learning. Andres Pinilla performed the literature search, 
data  analysis  and  wrote  the  first  draft.  All  authors  commented  on  previous  versions  of  the 
manuscript. 

6.3  Funding 

We acknowledge the support of the German Research Foundation and the Open Access 
Publication Fund of TU Berlin. 

6.4  Acknowledgments 

This  work  was  supported  by  the  strategic  partnership  between  the  Technische  Universität 
Berlin, Germany and the University of Technology Sydney, Australia. 

We appreciate the generosity of Nick Busietta in giving us access to the Psydocs of LiminalVR 
(liminalvr.com). That documentation was crucial for writing Section 2 of this manuscript. 

6.5  Data availability statement 

Not applicable. 

16 

 
 
Affective visualization in Virtual Reality 

7 

References 

Antons, J.-N., Arndt, S., Schleicher, R., & Möller, S. (2014). Brain Activity Correlates of 
Quality of Experience. In S. Möller & A. Raake (Eds.), Quality of Experience (pp. 
109–119). Springer International Publishing. https://doi.org/10.1007/978-3-319-
02681-7_8 

Arndt, S., Perkis, A., & Voigt-Antons, J.-N. (2018). Using Virtual Reality and Head-Mounted 
Displays to Increase Performance in Rowing Workouts. Proceedings of the 1st 
International Workshop on Multimedia Content Analysis in Sports  - MMSports’18, 
45–50. https://doi.org/10.1145/3265845.3265848 

Aronoff, J. (2006). How We Recognize Angry and Happy Emotion in People, Places, and 

Things. Cross-Cultural Research, 40(1), 83–105. 
https://doi.org/10.1177/1069397105282597 

Aronoff, J., Woike, B. A., & Hyman, L. M. (1992). Which are the stimuli in facial displays of 
anger and happiness? Configurational bases of emotion recognition. Journal of 
Personality and Social Psychology, 62(6), 1050–1066. https://doi.org/10.1037/0022-
3514.62.6.1050 

Banse, R., & Scherer, K. R. (1996). Acoustic profiles in vocal emotion expression. Journal of 

Personality and Social Psychology, 70(3), 614–636. https://doi.org/10.1037/0022-
3514.70.3.614 

Bar, M., & Neta, M. (2006). Humans Prefer Curved Visual Objects. Psychological Science, 

17(8), 645–648. https://doi.org/10.1111/j.1467-9280.2006.01759.x 

Bard, P. (1934). On emotional expression after decortication with some remarks on certain 

theoretical views: Part I. Psychological Review, 41(4), 309–329. 
https://doi.org/10.1037/h0070765 

Barrett, L. F., & Bliss-Moreau, E. (2009). Chapter 4 Affect as a Psychological Primitive. In 
Advances in Experimental Social Psychology (Vol. 41, pp. 167–218). Elsevier. 
https://doi.org/10.1016/S0065-2601(08)00404-8 

Bartram, L., Patra, A., & Stone, M. (2017). Affective Color in Visualization. Proceedings of 

the 2017 CHI Conference on Human Factors in Computing Systems, 1364–1374. 
https://doi.org/10.1145/3025453.3026041 

Belger, J., Thone-Otto, A., Krohn, S., Finke, C., Tromp, J., Klotzsche, F., Villringer, A., 

Gaebler, M., Chojecki, P., & Quinque, E. (2019). Immersive Virtual Reality for the 
Assessment and Training of Spatial Memory: Feasibility in Individuals with Brain 
Injury. 2019 International Conference on Virtual Rehabilitation (ICVR), 1–2. 
https://doi.org/10.1109/ICVR46560.2019.8994342 

Bermudez i Badia, S., Quintero, L. V., Cameirao, M. S., Chirico, A., Triberti, S., Cipresso, P., 
& Gaggioli, A. (2019). Toward Emotionally Adaptive Virtual Reality for Mental 
Health Applications. IEEE Journal of Biomedical and Health Informatics, 23(5), 
1877–1887. https://doi.org/10.1109/JBHI.2018.2878846 

Blandón, D. Z., Muñoz, J. E., Lopez, D. S., & Gallo, O. H. (2016). Influence of a BCI 

neurofeedback videogame in children with ADHD. Quantifying the brain activity 
through an EEG signal processing dedicated toolbox. 2016 IEEE 11th Colombian 
Computing Conference (CCC), 1–8. 
https://doi.org/10.1109/ColumbianCC.2016.7750788 

17 

 
Affective visualization in Virtual Reality 

Blum, J., Rockstroh, C., & Göritz, A. S. (2019). Heart Rate Variability Biofeedback Based on 

Slow-Paced Breathing With Immersive Virtual Reality Nature Scenery. Frontiers in 
Psychology, 10, 2172. https://doi.org/10.3389/fpsyg.2019.02172 

Blum, J., Rockstroh, C., & Göritz, A. S. (2020). Development and Pilot Test of a Virtual 
Reality Respiratory Biofeedback Approach. Applied Psychophysiology and 
Biofeedback, 45(3), 153–163. https://doi.org/10.1007/s10484-020-09468-x 

Blum, S., Jacobsen, N. S. J., Bleichner, M. G., & Debener, S. (2019). A Riemannian 
Modification of Artifact Subspace Reconstruction for EEG Artifact Handling. 
Frontiers in Human Neuroscience, 13. https://doi.org/10.3389/fnhum.2019.00141 

Bradley, M. M., & Lang, P. J. (1994). Measuring emotion: The self-assessment manikin and 
the semantic differential. Journal of Behavior Therapy and Experimental Psychiatry, 
25(1), 49–59. https://doi.org/10.1016/0005-7916(94)90063-9 

Bull, P. (1978). The interpretation of posture through an alternative methodology to role play. 

British Journal of Social and Clinical Psychology, 17(1), 1–6. 
https://doi.org/10.1111/j.2044-8260.1978.tb00888.x 

Burdea, G. C., Cioi, D., Kale, A., Janes, W. E., Ross, S. A., & Engsberg, J. R. (2013). 

Robotics and Gaming to Improve Ankle Strength, Motor Control, and Function in 
Children With Cerebral Palsy—A Case Study Series. IEEE Transactions on Neural 
Systems and Rehabilitation Engineering, 21(2), 165–173. 
https://doi.org/10.1109/TNSRE.2012.2206055 

Cacioppo, J. T., Gardner, W. L., & Berntson, G. G. (1997). Beyond Bipolar 

Conceptualizations and Measures: The Case of Attitudes and Evaluative Space. 
Personality and Social Psychology Review, 1(1), 3–25. 
https://doi.org/10.1207/s15327957pspr0101_2 

Camgöz, N., Yener, C., & Güvenç, D. (2002). Effects of hue, saturation, and brightness on 

preference. Color Research & Application, 27(3), 199–207. 
https://doi.org/10.1002/col.10051 

Cannon, W. B. (1927). The James-Lange Theory of Emotions: A Critical Examination and an 

Alternative Theory. The American Journal of Psychology, 39(1/4), 106. 
https://doi.org/10.2307/1415404 

Cassani, R., Moinnereau, M.-A., & Falk, T. H. (2018). A Neurophysiological Sensor-

Equipped Head-Mounted Display for Instrumental QoE Assessment of Immersive 
Multimedia. 2018 Tenth International Conference on Quality of Multimedia 
Experience (QoMEX), 1–6. https://doi.org/10.1109/QoMEX.2018.8463422 

Cavazza, M., Aranyi, G., Charles, F., Porteous, J., Gilroy, S., Klovatch, I., Jackont, G., Soreq, 
E., Keynan, N. J., Cohen, A., Raz, G., & Hendler, T. (2014). Towards Empathic 
Neurofeedback for Interactive Storytelling [Application/pdf]. 19 pages. 
https://doi.org/10.4230/OASICS.CMN.2014.42 

Colzato, L. S., Sellaro, R., & Beste, C. (2017). Darwin revisited: The vagus nerve is a causal 
element in controlling recognition of other’s emotions. Cortex, 92, 95–102. 
https://doi.org/10.1016/j.cortex.2017.03.017 

Conati, C., & Zhou, X. (2002). Modeling Students’ Emotions from Cognitive Appraisal in 

Educational Games. In S. A. Cerri, G. Gouardères, & F. Paraguaçu (Eds.), Intelligent 
Tutoring Systems (pp. 944–954). Springer Berlin Heidelberg. 

18 

 
Affective visualization in Virtual Reality 

Cordaro, D. T., Keltner, D., Tshering, S., Wangchuk, D., & Flynn, L. M. (2016). The voice 
conveys emotion in ten globalized cultures and one remote village in Bhutan. 
Emotion, 16(1), 117–128. https://doi.org/10.1037/emo0000100 

Cosmides, L., & Tooby, J. (1994). Origins of domain specificity: The evolution of functional 
organization. In L. A. Hirschfeld & S. A. Gelman (Eds.), Mapping the Mind (1st ed., 
pp. 85–116). Cambridge University Press. 
https://doi.org/10.1017/CBO9780511752902.005 

D. McDuff, R. E. Kaliouby, & R. W. Picard. (2012). Crowdsourcing Facial Responses to 
Online Videos. IEEE Transactions on Affective Computing, 3(4), 456–468. 
https://doi.org/10.1109/T-AFFC.2012.19 

Darwin, C. (1872). The expression of the emotions in man and animals. John Murray. 

https://doi.org/10.1037/10001-000 

Davidson, R. J. (1992). Emotion and Affective Style: Hemispheric Substrates. Psychological 

Science, 3(1), 39–43. https://doi.org/10.1111/j.1467-9280.1992.tb00254.x 

Desmet. (2015). Design for mood: Twenty activity-based opportunities to design for mood 

regulation. International Journal of Design, 9 (2), 2015. 

Desmet, Vastenburg, M. H., & Romero, N. (2016). Mood measurement with Pick-A-Mood: 

Review of current methods and design of a pictorial self-report scale. J. of Design 
Research, 14(3), 241. https://doi.org/10.1504/JDR.2016.079751 

Dimberg, U. (1982). Facial Reactions to Facial Expressions. Psychophysiology, 19(6), 643–

647. https://doi.org/10.1111/j.1469-8986.1982.tb02516.x 

Dimberg, U., & Thunberg, M. (2012). Empathy, emotional contagion, and rapid facial 

reactions to angry and happy facial expressions: Empathy and rapid facial reactions. 
PsyCh Journal, 1(2), 118–127. https://doi.org/10.1002/pchj.4 

Dimberg, U., Thunberg, M., & Elmehed, K. (2000). Unconscious Facial Reactions to 
Emotional Facial Expressions. Psychological Science, 11(1), 86–89. 
https://doi.org/10.1111/1467-9280.00221 

Drossos, K., Floros, A., Giannakoulopoulos, A., & Kanellopoulos, N. (2015). Investigating 
the Impact of Sound Angular Position on the Listener Affective State. IEEE 
Transactions on Affective Computing, 6(1), 27–42. 
https://doi.org/10.1109/TAFFC.2015.2392768 

Ebe, Y., & Umemuro, H. (2015). Emotion Evoked by Texture and Application to Emotional 
Communication. Proceedings of the 33rd Annual ACM Conference Extended 
Abstracts on Human Factors in Computing Systems, 1995–2000. 
https://doi.org/10.1145/2702613.2732768 

Eiben, A. E., & Smith, J. E. (2015). Introduction to evolutionary computing (2. ed). Springer. 

Ekman, P. (2006). Darwin and facial expression: A century of research in review. Ishk. 

Ekman, P., & Friesen, W. V. (1971). Constants across cultures in the face and emotion. 

Journal of Personality and Social Psychology, 17(2), 124–129. 
https://doi.org/10.1037/h0030377 

Ekman, P., & Friesen, W. V. (1976). Measuring facial movement. Environmental Psychology 

and Nonverbal Behavior, 1(1), 56–75. https://doi.org/10.1007/BF01115465 

19 

 
Affective visualization in Virtual Reality 

Feffer, M., Rudovic, O. (Oggi), & Picard, R. W. (2018). A Mixture of Personalized Experts 

for Human Affect Estimation. In P. Perner (Ed.), Machine Learning and Data Mining 
in Pattern Recognition (pp. 316–330). Springer International Publishing. 

Feng, C., Bartram, L., & Riecke, B. E. (2014). Evaluating affective features of 3D 

motionscapes. Proceedings of the ACM Symposium on Applied Perception - SAP ’14, 
23–30. https://doi.org/10.1145/2628257.2628264 

Fernández-Sotos, A., Fernández-Caballero, A., & Latorre, J. M. (2016). Influence of Tempo 

and Rhythmic Unit in Musical Emotion Regulation. Frontiers in Computational 
Neuroscience, 10. https://doi.org/10.3389/fncom.2016.00080 

Fisher, R. J. (1993). Social Desirability Bias and the Validity of Indirect Questioning. Journal 

of Consumer Research, 20(2), 303. https://doi.org/10.1086/209351 

Fuhrman, O., & Boroditsky, L. (2010). Cross-Cultural Differences in Mental Representations 

of Time: Evidence From an Implicit Nonlinguistic Task. Cognitive Science, 34(8), 
1430–1451. https://doi.org/10.1111/j.1551-6709.2010.01105.x 

Gao, X.-P., Xin, J. H., Sato, T., Hansuebsai, A., Scalzo, M., Kajiwara, K., Guan, S.-S., 
Valldeperas, J., Lis, M. J., & Billger, M. (2007). Analysis of cross-cultural color 
emotion. Color Research & Application, 32(3), 223–229. 
https://doi.org/10.1002/col.20321 

Garcia, J. A., & Navarro, K. F. (2014). The Mobile RehAppTM: An AR-based mobile game 
for ankle sprain rehabilitation. 2014 IEEE 3nd International Conference on Serious 
Games and Applications for Health (SeGAH), 1–6. 
https://doi.org/10.1109/SeGAH.2014.7067087 

Georgiou, T., & Demiris, Y. (2017). Adaptive user modelling in car racing games using 

behavioural and physiological data. User Modeling and User-Adapted Interaction, 
27(2), 267–311. https://doi.org/10.1007/s11257-017-9192-3 

Gerardi, G. M., & Gerken, L. (1995). The Development of Affective Responses to Modality 

and Melodic Contour. Music Perception: An Interdisciplinary Journal, 12(3), 279–
290. https://doi.org/10.2307/40286184 

Greinacher, R., Kojic, T., Meier, L., Parameshappa, R. G., Moller, S., & Voigt-Antons, J.-N. 
(2020). Impact of Tactile and Visual Feedback on Breathing Rhythm and User 
Experience in VR Exergaming. 2020 Twelfth International Conference on Quality of 
Multimedia Experience (QoMEX), 1–6. 
https://doi.org/10.1109/QoMEX48832.2020.9123141 

Greinacher, R., & Voigt-Antons, J.-N. (2020). Accuracy Assessment of ARKit 2 Based Gaze 
Estimation. In M. Kurosu (Ed.), Human-Computer Interaction. Design and User 
Experience (Vol. 12181, pp. 439–449). Springer International Publishing. 
https://doi.org/10.1007/978-3-030-49059-1_32 

Haar, A. J. H., Jain, A., Schoeller, F., & Maes, P. (2020). Augmenting aesthetic chills using a 
wearable prosthesis improves their downstream effects on reward and social 
cognition. Scientific Reports, 10(1), 21603. https://doi.org/10.1038/s41598-020-
77951-w 

Harischandra, J., & Perera, M. U. S. (2012). Intelligent emotion recognition system using 

brain signals (EEG). 2012 IEEE-EMBS Conference on Biomedical Engineering and 
Sciences, 454–459. https://doi.org/10.1109/IECBES.2012.6498050 

Hernandez, J., Li, Y., Rehg, J. M., & Picard, R. W. (2014). BioGlass: Physiological parameter 
estimation using a head-mounted wearable device. 2014 4th International Conference 

20 

 
Affective visualization in Virtual Reality 

on Wireless Mobile Communication and Healthcare - Transforming Healthcare 
Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH), 55–58. 
https://doi.org/10.1109/MOBIHEALTH.2014.7015908 

Hofmann, S. M., Klotzsche, F., Mariola, A., Nikulin, V. V., Villringer, A., & Gaebler, M. 

(2018). Decoding Subjective Emotional Arousal during a Naturalistic VR Experience 
from EEG Using LSTMs. 2018 IEEE International Conference on Artificial 
Intelligence and Virtual Reality (AIVR), 128–131. 
https://doi.org/10.1109/AIVR.2018.00026 

Hoppe, S., Loetscher, T., Morey, S. A., & Bulling, A. (2018). Eye Movements During 

Everyday Behavior Predict Personality Traits. Frontiers in Human Neuroscience, 12, 
105. https://doi.org/10.3389/fnhum.2018.00105 

Hupont, I., Gracia, J., Sanagustin, L., & Gracia, M. A. (2015). How do new visual immersive 
systems influence gaming QoE? A use case of serious gaming with Oculus Rift. 2015 
Seventh International Workshop on Quality of Multimedia Experience (QoMEX), 1–6. 
https://doi.org/10.1109/QoMEX.2015.7148110 

Huster, R. J., Stevens, S., Gerlach, A. L., & Rist, F. (2009). A spectralanalytic approach to 
emotional responses evoked through picture presentation. International Journal of 
Psychophysiology, 72(2), 212–216. https://doi.org/10.1016/j.ijpsycho.2008.12.009 

Jaques, P. A., & Vicari, R. M. (2007). A BDI approach to infer student’s emotions in an 
intelligent learning environment. Computers & Education, 49(2), 360–384. 
https://doi.org/10.1016/j.compedu.2005.09.002 

Kapur, A., Kapur, A., Virji-Babul, N., Tzanetakis, G., & Driessen, P. F. (2005). Gesture-
Based Affective Computing on Motion Capture Data. In J. Tao, T. Tan, & R. W. 
Picard (Eds.), Affective Computing and Intelligent Interaction (pp. 1–7). Springer 
Berlin Heidelberg. 

Kitson, A., DiPaola, S., & Riecke, B. E. (2019). Lucid Loop: A Virtual Deep Learning 

Biofeedback System for Lucid Dreaming Practice. Extended Abstracts of the 2019 
CHI Conference on Human Factors in Computing Systems, 1–6. 
https://doi.org/10.1145/3290607.3312952 

Klug, M., & Gramann, K. (2020). Identifying key factors for improving ICA-based 

decomposition of EEG data in mobile and stationary experiments. European Journal 
of Neuroscience. https://doi.org/10.1111/ejn.14992 

Koelstra, S., Muhl, C., Soleymani, M., Jong-Seok Lee, Yazdani, A., Ebrahimi, T., Pun, T., 

Nijholt, A., & Patras, I. (2012). DEAP: A Database for Emotion Analysis ;Using 
Physiological Signals. IEEE Transactions on Affective Computing, 3(1), 18–31. 
https://doi.org/10.1109/T-AFFC.2011.15 

Koenig, S. T., Crucian, G. P., Duenser, A., Bartneck, C., & Dalrymple-Alford, J. C. (2011). 

Validity evaluation of a spatial memory task in virtual environments. 

Kothe, C. A., & Makeig, S. (2013). BCILAB: A platform for brain–computer interface 

development. Journal of Neural Engineering, 10(5), 056014. 
https://doi.org/10.1088/1741-2560/10/5/056014 

Lang, P. J., Bradley, M. M., & Cuthbert, B. N. (2008). International affective picture system 

(IAPS): Affective ratings of pictures and instruction manual. Technical Report A-8. 
University of Florida, Gainesville, FL. 

Lange, C. G., & James, W. (Eds.). (1922). The emotions, Vol. 1. Williams & Wilkins Co. 

https://doi.org/10.1037/10735-000 

21 

 
Affective visualization in Virtual Reality 

Leslie, G., Picard, R., & Lui, S. (2015). An EEG and Motion Capture Based Expressive Music 

Interface for Affective Neurofeedback. https://doi.org/10.13140/RG.2.1.4378.6081 

Li, Z., Tong, L., Wang, L., Li, Y., He, W., Guan, M., & Yan, B. (2016). Self-regulating 

positive emotion networks by feedback of multiple emotional brain states using real-
time fMRI. Experimental Brain Research, 234(12), 3575–3586. 
https://doi.org/10.1007/s00221-016-4744-z 

Lipson-Smith, R., Bernhardt, J., Zamuner, E., Churilov, L., Busietta, N., & Moratti, D. 

(2020). Exploring colour in context using Virtual Reality: Does a room change how 
you feel? Virtual Reality. https://doi.org/10.1007/s10055-020-00479-x 

Lockyer, M., Bartram, L., & Riecke, B. E. (2011). Simple Motion Textures for Ambient 

Affect. Computational Aesthetics in Graphics, Visualization, 8 pages. 
https://doi.org/10.2312/COMPAESTH/COMPAESTH11/089-096 

Lucassen, M. P., Gevers, T., & Gijsenij, A. (2011). Texture affects color emotion. Color 
Research & Application, 36(6), 426–436. https://doi.org/10.1002/col.20647 

Makeig, S., Jung, T.-P., Bell, A. J., Ghahremani, D., & Sejnowski, T. J. (1997). Blind 

separation of auditory event-related brain responses into independent components. 
Proceedings of the National Academy of Sciences, 94(20), 10979–10984. 
https://doi.org/10.1073/pnas.94.20.10979 

Malik, M., Bigger, J. T., Camm, A. J., Kleiger, R. E., Malliani, A., Moss, A. J., & Schwartz, 
P. J. (1996). Heart rate variability: Standards of measurement, physiological 
interpretation, and clinical use. European Heart Journal, 17(3), 354–381. 
https://doi.org/10.1093/oxfordjournals.eurheartj.a014868 

Martínez-Tejada, L. A., Puertas-González, A., Yoshimura, N., & Koike, Y. (2021). Exploring 

EEG Characteristics to Identify Emotional Reactions under Videogame Scenarios. 
Brain Sciences, 11(3), 378. https://doi.org/10.3390/brainsci11030378 

Mattek, A. M., Wolford, G. L., & Whalen, P. J. (2017). A Mathematical Model Captures the 

Structure of Subjective Affect. Perspectives on Psychological Science, 12(3), 508–
526. https://doi.org/10.1177/1745691616685863 

Mavridou, I., McGhee, J. T., Hamedi, M., Fatoorechi, M., Cleal, A., Ballaguer-Balester, E., 

Seiss, E., Cox, G., & Nduka, C. (2017). FACETEQ interface demo for emotion 
expression in VR. 2017 IEEE Virtual Reality (VR), 441–442. 
https://doi.org/10.1109/VR.2017.7892369 

Mullen, T. R., Kothe, C. A. E., Chi, Y. M., Ojeda, A., Kerth, T., Makeig, S., Jung, T.-P., & 
Cauwenberghs, G. (2015). Real-time neuroimaging and cognitive monitoring using 
wearable dry EEG. IEEE Transactions on Biomedical Engineering, 62(11), 2553–
2567. https://doi.org/10.1109/TBME.2015.2481482 

Norman, J. F., Beers, A., & Phillips, F. (2010). Fechner’s Aesthetics Revisited. Seeing and 

Perceiving, 23(3), 263–271. https://doi.org/10.1163/187847510X516412 

Ortony, A., Clore, G. L., & Collins, A. (1988). The cognitive structure of emotions. 

Cambridge university press. 

Pagani, M., Lombardi, F., Guzzetti, S., Sandrone, G., Rimoldi, O., Malfatto, G., Cerutti, S., & 

Malliani, A. (1984). Power spectral density of heart rate variability as an index of 
sympatho-vagal interaction in normal and hypertensive subjects. Journal of 
Hypertension. Supplement: Official Journal of the International Society of 
Hypertension, 2(3), S383-385. 

22 

 
Affective visualization in Virtual Reality 

Palmer, S. E., & Schloss, K. B. (2010). An ecological valence theory of human color 

preference. Proceedings of the National Academy of Sciences, 107(19), 8877–8882. 
https://doi.org/10.1073/pnas.0906172107 

Peperkorn, H. M., Alpers, G. W., & Mühlberger, A. (2014). Triggers of Fear: Perceptual Cues 

Versus Conceptual Information in Spider Phobia: Cues Versus Information in Spider 
Phobia. Journal of Clinical Psychology, 70(7), 704–714. 
https://doi.org/10.1002/jclp.22057 

Perkis, A., Timmerer, C., Baraković, S., Husić, J. B., Bech, S., Bosse, S., Botev, J., 

Brunnström, K., Cruz, L., Moor, K. D., Saibanti, A. de P., Durnez, W., Egger-Lampl, 
S., Engelke, U., Falk, T. H., Hameed, A., Hines, A., Kojic, T., Kukolj, D., … 
Zadtootaghaj, S. (2020). QUALINET White Paper on Definitions of Immersive Media 
Experience (IMEx). 

Petri, T. (2009). Exploring relationships between audio features and emotion in music. 

Frontiers in Human Neuroscience, 3. 
https://doi.org/10.3389/conf.neuro.09.2009.02.033 

Pfurtscheller, G., & Lopes da Silva, F. H. (1999). Event-related EEG/MEG synchronization 

and desynchronization: Basic principles. Clinical Neurophysiology, 110(11), 1842–
1857. https://doi.org/10.1016/S1388-2457(99)00141-8 

Picard, R. W., Vyzas, E., & Healey, J. (2001). Toward machine emotional intelligence: 

Analysis of affective physiological state. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 23(10), 1175–1191. https://doi.org/10.1109/34.954607 

Pinilla, A., Tamayo, R. M., & Neira, J. (2020). How Do Induced Affective States Bias 

Emotional Contagion to Faces? A Three-Dimensional Model. Frontiers in 
Psychology, 11. https://doi.org/10.3389/fpsyg.2020.00097 

Pion-Tonachini, L., Sheng-Hsiou Hsu, Makeig, S., Tzyy-Ping Jung, & Cauwenberghs, G. 
(2015). Real-time EEG Source-mapping Toolbox (REST): Online ICA and source 
localization. 2015 37th Annual International Conference of the IEEE Engineering in 
Medicine and Biology Society (EMBC), 4114–4117. 
https://doi.org/10.1109/EMBC.2015.7319299 

Piwek, L., Pollick, F., & Petrini, K. (2015). Audiovisual integration of emotional signals from 

others’ social interactions. Frontiers in Psychology, 9. 
https://doi.org/10.3389/fpsyg.2015.00611 

Plutchik, R. (1982). A psychoevolutionary theory of emotions. Social Science Information, 

21(4–5), 529–553. https://doi.org/10.1177/053901882021004003 

Polzehl, T., Schmitt, A., Metze, F., & Wagner, M. (2011). Anger recognition in speech using 
acoustic and linguistic cues. Speech Communication, 53(9–10), 1198–1209. 
https://doi.org/10.1016/j.specom.2011.05.002 

Porcu, S., Floris, A., Voigt-Antons, J.-N., Atzori, L., & Moller, S. (2020). Estimation of the 

Quality of Experience during Video Streaming from Facial Expression and Gaze 
Direction. IEEE Transactions on Network and Service Management, 1–1. 
https://doi.org/10.1109/TNSM.2020.3018303 

Putnam, H. (1967). The nature of mental states. In W. H. Capitan & D. D. Merrill (Eds.), Art, 

Mind, and Religion (pp. 1--223). Pittsburgh University Press. 

Raffe, W. L., Zambetta, F., Li, X., & Stanley, K. O. (2015). Integrated Approach to 

Personalized Procedural Map Generation Using Evolutionary Algorithms. IEEE 

23 

 
Affective visualization in Virtual Reality 

Transactions on Computational Intelligence and AI in Games, 7(2), 139–155. 
https://doi.org/10.1109/TCIAIG.2014.2341665 

Ray, W., & Cole, H. (1985). EEG alpha activity reflects attentional demands, and beta 

activity reflects emotional and cognitive processes. Science, 228(4700), 750–752. 
https://doi.org/10.1126/science.3992243 

Renard, Y., Lotte, F., Gibert, G., Congedo, M., Maby, E., Delannoy, V., Bertrand, O., & 

Lécuyer, A. (2010). OpenViBE: An Open-Source Software Platform to Design, Test, 
and Use Brain–Computer Interfaces in Real and Virtual Environments. Presence: 
Teleoperators and Virtual Environments, 19(1), 35–53. 
https://doi.org/10.1162/pres.19.1.35 

Reuderink, B., Mühl, C., & Poel, M. (2013). Valence, arousal and dominance in the EEG 

during game play. International Journal of Autonomous and Adaptive 
Communications Systems, 6(1), 45. https://doi.org/10.1504/IJAACS.2013.050691 

Robitaille, P., & McGuffin, M. J. (2019). Increased affect-arousal in VR can be detected from 

faster body motion with increased heart rate. Proceedings of the ACM SIGGRAPH 
Symposium on Interactive 3D Graphics and Games, 1–6. 
https://doi.org/10.1145/3306131.3317022 

Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social 

Psychology, 39(6), 1161–1178. https://doi.org/10.1037/h0077714 

Ryali, C. K., Goffin, S., Winkielman, P., & Yu, A. J. (2020). From likely to likable: The role 
of statistical typicality in human social assessment of faces. Proceedings of the 
National Academy of Sciences, 117(47), 29371–29380. 
https://doi.org/10.1073/pnas.1912343117 

Schachter, S., & Singer, J. (1962). Cognitive, social, and physiological determinants of 

emotional state. Psychological Review, 69(5), 379–399. 
https://doi.org/10.1037/h0046234 

Scherer, K. R., & Oshinsky, J. S. (1977). Cue utilization in emotion attribution from auditory 
stimuli. Motivation and Emotion, 1(4), 331–346. https://doi.org/10.1007/BF00992539 

Schoeller, F., Bertrand, P., Gerry, L. J., Jain, A., Horowitz, A. H., & Zenasni, F. (2019). 

Combining Virtual Reality and Biofeedback to Foster Empathic Abilities in Humans. 
Frontiers in Psychology, 9, 2741. https://doi.org/10.3389/fpsyg.2018.02741 

Schoeller, F., Haar, A. J. H., Jain, A., & Maes, P. (2019). Enhancing human emotions with 

interoceptive technologies. Physics of Life Reviews, 31, 310–319. 
https://doi.org/10.1016/j.plrev.2019.10.008 

Semertzidis, N., Scary, M., Andres, J., Dwivedi, B., Kulwe, Y. C., Zambetta, F., & Mueller, 
F. F. (2020). Neo-Noumena: Augmenting Emotion Communication. Proceedings of 
the 2020 CHI Conference on Human Factors in Computing Systems, 1–13. 
https://doi.org/10.1145/3313831.3376599 

Shiban, Y., Peperkorn, H., Alpers, G. W., Pauli, P., & Mühlberger, A. (2016). Influence of 
perceptual cues and conceptual information on the activation and reduction of 
claustrophobic fear. Journal of Behavior Therapy and Experimental Psychiatry, 51, 
19–26. https://doi.org/10.1016/j.jbtep.2015.11.002 

Shiban, Y., Reichenberger, J., Neumann, I. D., & Mühlberger, A. (2015). Social conditioning 
and extinction paradigm: A translational study in virtual reality. Frontiers in 
Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00400 

24 

 
Affective visualization in Virtual Reality 

Shiota, M. N., & Kalat, J. W. (2012). Emotion (Second Edition). Linda Schreiber-Ganster. 

Sitaram, R., Lee, S., Ruiz, S., Rana, M., Veit, R., & Birbaumer, N. (2011). Real-time support 

vector classification and feedback of multiple emotional brain states. NeuroImage, 
56(2), 753–765. https://doi.org/10.1016/j.neuroimage.2010.08.007 

Sutherland, M. R., & Mather, M. (2012). Negative arousal amplifies the effects of saliency in 

short-term memory. Emotion, 12(6), 1367–1372. https://doi.org/10.1037/a0027860 

Tajadura-Jiménez, A., Larsson, P., Väljamäe, A., Västfjäll, D., & Kleiner, M. (2010). When 
room size matters: Acoustic influences on emotional responses to sounds. Emotion, 
10(3), 416–422. https://doi.org/10.1037/a0018423 

Tajadura-Jiménez, A., Väljamäe, A., Asutay, E., & Västfjäll, D. (2010). Embodied auditory 
perception: The emotional impact of approaching and receding sound sources. 
Emotion, 10(2), 216–229. https://doi.org/10.1037/a0018422 

Tajadura-Jiménez, A., Väljamäe, A., & Västfjäll, D. (2008). Self-Representation in Mediated 

Environments: The Experience of Emotions Modulated by Auditory-Vibrotactile 
Heartbeat. CyberPsychology & Behavior, 11(1), 33–38. 
https://doi.org/10.1089/cpb.2007.0002 

Thayer, J. F., Hansen, A. L., Saus-Rose, E., & Johnsen, B. H. (2009). Heart Rate Variability, 
Prefrontal Neural Function, and Cognitive Performance: The Neurovisceral 
Integration Perspective on Self-regulation, Adaptation, and Health. Annals of 
Behavioral Medicine, 37(2), 141–153. https://doi.org/10.1007/s12160-009-9101-z 

Unpingco, J. (2014). Python for signal processing: Featuring IPython notebooks. Springer. 

Valdez, P., & Mehrabian, A. (1994). Effects of color on emotions. Journal of Experimental 

Psychology: General, 123(4), 394–409. https://doi.org/10.1037/0096-3445.123.4.394 

Vogt, T., André, E., & Bee, N. (2008). EmoVoice—A Framework for Online Recognition of 
Emotions from Voice. In E. André, L. Dybkjær, W. Minker, H. Neumann, R. 
Pieraccini, & M. Weber (Eds.), Perception in Multimodal Dialogue Systems (pp. 188–
199). Springer Berlin Heidelberg. 

Voigt-Antons, J.-N., Lehtonen, E., Palacios, A. P., Ali, D., Kojic, T., & Moller, S. (2020). 

Comparing Emotional States Induced by 360° Videos Via Head-Mounted Display and 
Computer Screen. 2020 Twelfth International Conference on Quality of Multimedia 
Experience (QoMEX), 1–6. https://doi.org/10.1109/QoMEX48832.2020.9123125 

Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief 

measures of positive and negative affect: The PANAS scales. Journal of Personality 
and Social Psychology, 54(6), 1063–1070. https://doi.org/10.1037/0022-
3514.54.6.1063 

Williams, D., Kirke, A., Miranda, E., Daly, I., Hwang, F., Weaver, J., & Nasuto, S. (2017). 

Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music 
Composition System. ACM Transactions on Applied Perception, 14(3), 1–13. 
https://doi.org/10.1145/3059005 

Wilms, L., & Oberfeld, D. (2018). Color and emotion: Effects of hue, saturation, and 

brightness. Psychological Research, 82(5), 896–914. https://doi.org/10.1007/s00426-
017-0880-8 

Wolpaw, J. R., Birbaumer, N., McFarland, D. J., Pfurtscheller, G., & Vaughan, T. M. (2002). 
Brain–computer interfaces for communication and control. Clinical Neurophysiology, 
113(6), 767–791. https://doi.org/10.1016/S1388-2457(02)00057-3 

25 

 
Affective visualization in Virtual Reality 

Wundt, W. (1897). Outlines of psychology. (C. H. Judd, Trans.). Williams and Norgate. 

https://doi.org/10.1037/12908-000 

Yannakakis, G. N., & Togelius, J. (2011). Experience-Driven Procedural Content Generation. 
IEEE Transactions on Affective Computing, 2(3), 147–161. https://doi.org/10.1109/T-
AFFC.2011.6 

Zander, T. O., & Kothe, C. (2011). Towards passive brain–computer interfaces: Applying 

brain–computer interface technology to human–machine systems in general. Journal 
of Neural Engineering, 8(2), 025005. https://doi.org/10.1088/1741-2560/8/2/025005 

26 

 
 
 
 
 
8 

List of abbreviations 

Affective visualization in Virtual Reality 

functional Magnetic Brain Imaging (fMRI) 

International Affective Pictures System (IAPS)  

•  Human Computer Interaction (HCI) 
•  Virtual Reality (VR) 
• 
•  electroencephalography (EEG) 
•  Ortony, Clore & Collins (OCC) theory of emotions 
•  Evaluative Space Model (ESM) 
•  Positive and Negative Affect Schedule (PANAS)  
•  Self-Assessment Manikin (SAM) 
•  Pick a Mood (PAM) 
• 
•  Facial Action Coding System (FACS) 
•  Action Unit (AU) 
•  Head–Mounted Display (HMD) 
•  Electromyography (EMG) 
•  electrocardiography (ECG) 
•  electroencephalography (EEG) 
•  RR-Intervals (RRI) 
•  heart rate variability (HRV) 
• 
•  standard deviation of NN intervals (SDNN). 
•  Low Frequency / High Frequency ratio (LF/HF ratio) 
• 
• 
• 
•  Artifact Subspace Reconstruction (ASR)  
•  principal component analysis (PCP)  
•  Brain–Computer Interfaces (BCIs)  
•  electrocorticography (ECoG), 
•  Positron Emission Tomography (PET) 

root mean square of successive differences (RMSSD)  

long short-term memory recurrent neural networks (LSTMRNN)  
Independent Component Analysis (ICA) 
independent components (ICs) 

27 

 
