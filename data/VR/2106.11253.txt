1
2
0
2

n
u
J

1
2

]

V

I
.
s
s
e
e
[

1
v
3
5
2
1
1
.
6
0
1
2
:
v
i
X
r
a

Applying VertexShuﬄe Toward 360-Degree Video Super-Resolution
on Focused-Icosahedral-Mesh

Na Li and Yao Liu

Department of Computer Science, Binghamton University

Abstract

With the emerging of 360-degree image/video, aug-
mented reality (AR) and virtual reality (VR), the de-
mand for analysing and processing spherical signals
get tremendous increase. However, plenty of eﬀort
paid on planar signals that projected from spherical
signals, which leading to some problems, e.g. waste of
pixels, distortion. Recent advances in spherical CNN
have opened up the possibility of directly analysing
spherical signals. However, they pay attention to
the full mesh which makes it infeasible to deal with
situations in real-world application due to the ex-
tremely large bandwidth requirement. To address the
bandwidth waste problem associated with 360-degree
video streaming and save computation, we exploit Fo-
cused Icosahedral Mesh to represent a small area and
construct matrices to rotate spherical content to the
focused mesh area. We also proposed a novel Ver-
texShuﬄe operation that can signiﬁcantly improve
both the performance and the eﬃciency compared
to the original MeshConv Transpose operation intro-
duced in UGSCNN [1]. We further apply our pro-
posed methods on super resolution model, which is
the ﬁrst to propose a spherical super-resolution model
that directly operates on a mesh representation of
spherical pixels of 360-degree data. To evaluate our
model, we also collect a set of high-resolution 360-
degree videos to generate a spherical image dataset.
Our experiments indicate that our proposed spheri-
cal super-resolution model achieves signiﬁcant bene-
ﬁts in terms of both performance and inference time
compared to the baseline spherical super-resolution

model that uses the simple MeshConv Transpose op-
eration. In summary, our model achieves great super-
resolution performance on 360-degree inputs, achiev-
ing 32.79 dB PSNR on average when super-resoluting
16x vertices on the mesh.

1

Introduction

360-degree image/video, also known as spherical im-
age/video, is an emerging format of media that cap-
tures views from all directions surrounding the cam-
era. Unlike traditional 2D image/video that limits
the user’s view to wherever the camera is facing dur-
ing capturing, a 360-degree image/video allows the
viewer to freely navigate a full omnidirectional scene
around the camera position.

Despite its substantial promise of immersiveness,
the utility of streaming 360-degree video is limited
by the huge bandwidths required by most streaming
implementations. When watching a 360-degree video,
users can only watch a small portion of the full omni-
directional view. That is, while the 360-degree video
encodes frames that cover the full 360◦ × 180◦ ﬁeld-
of-view (FoV), the user may only observe a “view”
of 100◦ × 100◦ FoV of the omnidirectional frame at
a time. If the omnidirectional frame is projected to
the 2D frame using the equirectangular projection [2],
then only roughly 15% of the pixels of the frame is
viewed. The rest 85% pixels are not viewed, and are
thus wasted.

To allow the users to observe “views” in high
enough quality, full omnidirectional frames must be

1

 
 
 
 
 
 
equirectangular projection. Further, training a CNN
directly on the distorted representation could cause
CNN models to learn characteristics of the planar dis-
tortion rather than relevant details of the high reso-
lution representation.

Recent work [1, 4] pay attention to play Convolu-
tional operation directly on spherical signals to pre-
vent the distortion problem. Their works show that it
is possible to analyze spherical signals directly with-
out 2D projections. Furthermore, extensive exper-
iments were conducted in these works to show the
eﬃciency of their proposed spherical CNNs.

Motivated by 2D PixelShuﬄe [5], we also pro-
posed our VertexShuﬄe that achieves great perfor-
mance and parameter eﬃciency on mesh representa-
tion, which improved a lot based on MeshConv Trans-
pose proposed in UGSCNN [1].

To illustrate the eﬃciency of our proposed Focused
Icosahedral Mesh representation and VertexShuﬄe,
we apply our methods on a popular problem in com-
puter vision, Super-Resolution [6], which aims at re-
covering high-resolution images and videos from low-
resolution images and videos.

In this paper,

inspired by recent advances in
spherical CNNs [1, 4] and state-of-the-art 2D super-
resolution methods [7, 8, 9, 10, 11], We proposed
an eﬃcient Focused Icosahedral Mesh representation
to better utilize the computational resources and a
novel VertexShuﬄe operation that can signiﬁcantly
improve both the performance and the eﬃciency com-
pared to the original MeshConv Transpose operation
introduced in UGSCNN [1]. For evaluation, due to
the lack of former spherical super resolution dataset,
we also created a spherical super-resolution dataset
from ten 360-degree videos in high resolution.

In summary, our paper makes the following main

contributions:

• We create a Focused Icosahedral Mesh represen-
tation of the sphere to eﬃciently represent spher-
ical data, which not only saves computational
resources but also improves memory eﬃciency.

• We create a novel VertexShuﬄe operation, in-
spired by the 2D PixelShuﬄe [5] operation. The
Vertex operation signiﬁcantly increases both the

Figure 1: This left shows the same 360-degree content
as pixels on a sphere, which is the most natural way
for representing 360-degree data. The right shows a
360-degree image encoded in the equirectangular pro-
jection, which is a widely used spherical projection for
representing 360-degree images. However, projecting
spherical signals to the 2D plane introduces distor-
tion, e.g., the north and south pole areas.2

transmitted at 4K or 8K resolution. Streaming videos
at 4K or 8K resolution requires a signiﬁcant amount
of network bandwidth (e.g., 100 Mbps for 8K video
streaming) that may not be supported by most users’
network connections.

To address the bandwidth-waste problem, we pro-
posed a eﬃcient mesh representation, Focused Icosa-
hedral Mesh, supports our model to focus on the
more interesting portion of a sphere instead of the
full mesh. It is more ﬂexible and eﬃcient.

Another problem is that the omnidirectional views
captured by 360-degree cameras are most naturally
represented as uniformly dense pixels over the sur-
face of a sphere (as shown in Figure 1 (left)). When
spherical pixels are projected to planar surfaces, dis-
tortions are introduced. For example, the equirectan-
gular projection [2] is a widely used spherical projec-
tion for representing 360-degree data. However, sig-
niﬁcant distortions can be observed around the north
and south pole areas, as shown in Figure 1 (right).

Such distortions can reduce the eﬃciency of CNN
operations by adding “over-represented” pixels, e.g.,
the regions near the north and south poles in the

2The original

image is from the following video in the
360-degree video head movement dataset [3]: https://www.
youtube.com/watch?v=8lsB-P8nGSM

2

visual quality metric (peak signal-to-noise ra-
tio (PSNR)) and inference time over comparable
transposed convolution operations.

• We are the ﬁrst to propose a super-resolution
model that directly operates on a mesh repre-
sentation of spherical pixels of 360-degree data.

• We create a 360-degree super-resolution dataset
from a set of high resolution 360-degree videos
for evaluation.

• Results show that our proposed SSR model
achieves great super-resolution performance on
360-degree inputs, achieving 32.79 dB PSNR on
average when super-resoluting 16x vertices on
the mesh.

2 Related Work

2.1 360-degree video

Despite its potential for delivering more-immersive
viewing experiences than standard video streams,
current 360-degree video implementations requires
bandwidths that are too high to deliver adequate ex-
periences for many users.

Numerous approaches have been proposed for im-
proving 360-degree bandwidth eﬃciency. These ap-
proaches have both attempted to improve the eﬃ-
ciency of how the 360-degree view is represented dur-
ing transmission [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
as well as improving a system’s ability to avoid de-
livering unviewed pixels [22]. Only recently have
super-resolution (SR) approaches been proposed in
conjunction with 360-degree video delivery [23, 24].
To avoid distortion problem in projecting 360-
degree video to 2D planes, Xiong et al. [25] devel-
oped a reinforcement learning approach to select a
sequence of rotation angles to minimize interest area
near or on the cube boundaries.

Marc et al. [26] proposed a spherical image repre-
sentation that mitigates spherical distortion by ren-
dering tangent icosahedron faces to a a set of ori-
ented,
low-distortion images to icosahedron faces.
They also presented utilities of applying standard
CNN on spherical data.

2.2 Spherical
networks

convolutional neural

Spherical CNN has been studied by the computer vi-
sion community recently as a number of real-world
applications require processing signals in the spher-
ical domain, including self-driving cars, panoramic
videos, omnidirectional RGBD images, and climate
science.

Recently works such as Cohen et al.

[4] gave
theoretical support of spherical CNNs for rotation-
invariant learning problems, which is important for
problems where orientation is crucial to the model
performance. They ﬁrst introduced the concepts of
S2 and SO(3). S2 can be deﬁned as the set of points
on a unit sphere, and SO(3) is the rotation group in
Euclidean three dimensional space.

They replaced planar correlation with spherical
correlation, which can be understood as the value of
output feature map evaluated at rotation R ∈ SO(3)
computed as an inner product between the input fea-
ture map and a ﬁlter, rotated by R. Furthermore,
they implemented the generalized Fourier transform
for S2 and SO(3).

Later, Cohen et al. [27] introduced a theory that is
equi-variance to symmetry transformations on man-
ifolds. They further prompt a gauge equi-variant
CNN for signals on the icosahedron using the icosahe-
dral CNN, which implements gauge equi-variant con-
volution using a single conv2d call, making it a highly
scalable and practical alternative to spherical CNNs.
UGSCNN [1] is another recent work in spherical
CNN. It presents a novel CNN approach on unstruc-
tured grids using parameterized diﬀerential operators
for spherical signals. They introduce a basic convolu-
tion operation, called MeshConv, that can be applied
on meshes rather than planar images.
It achieves
signiﬁcantly better performance and parameter eﬃ-
ciency compared to state-of-the-art network architec-
tures for 3D classiﬁcation tasks since it does not re-
quire large amounts of geodestic computations and
interpolations.

Zhang et al. [28] proposed to perform semantic seg-
mentation on omnidirectional images by designing an
orientation-aware CNN framework for the icosahe-
dron mesh. They introduced fast interpolation of ker-

3

nel convolutions and presented weight transfer from
learned through classical CNNs to perspective data.
Recently, Eder et al. [26] proposed a spherical image
representation that mitigates spherical distortion by
rendering a set of oriented, low-distortion images tan-
gent to icosahedron faces. They also presented util-
ities of their approaches by applying standard CNN
to the spherical data.

While these existing works demonstrate their ef-
fectiveness in classiﬁcation and segmentation tasks,
the super-resolution task was not considered. In this
work, we found it possible to apply their work on
the super-resolution task. Our work is based on the
proposed MeshConv operation since it achieves bet-
ter performance and parameter eﬃciency than other
spherical convolutional networks. We also conduct
experiment to show signiﬁcant improvements over the
baseline spherical super-resolution model that uses
the simple MeshConv Transpose operation.

2.3 Super resolution

The super-resolution ﬁeld has advanced rapidly from
its origins in the deep learning age. The SRCNN [7, 8]
model was the ﬁrst to apply CNNs to SR. FSR-
CNN [9] was an evolution of SRCNN. It operated di-
rectly on a low-resolution input image and applied a
a deconvolution layer to generate the high-resolution
output. VDSR [29] was the ﬁrst to apply residual
layers [30] to the SR task, allowing for deeper SR
networks. DRCN [31] introduced recursive learning
in a very deep network for parameter sharing. Shi
et al. [5] proposed “PixelShuﬄe”, a method for map-
ping values at low-resolution positions directly to po-
sitions in a higher-resolution image more eﬃciently
than the deconvolution operation. SRResNet [32] in-
troduced a modiﬁed residual layer tailored for the
SR application. EDSR [33] further modiﬁed the SR-
speciﬁc residual layer from SSResNet and introduced
a multi-task objective in MDSR. SRGAN [32] applied
a Generative Adversarial Network (GAN) [34] to SR,
allowing better resolution of high-frequency details.
These works focus on 2D planar data, which may not
be ideal for 360-degree image super-resolution due
to the distortions introduced in the projected repre-
sentation. Our proposed model, however, operates

directly on spherical signals so that we can avoid the
distortion problem.

Focusing on optimizing 360-degree video stream-
ing, Chen et al. [35] focused to apply super-resolution
on 360-degree video tiles. Their work mainly focused
on the overall video streaming system rather than
the super-resolution model implementation. This is
diﬀerent from our work that mainly focuses on the
implementation of a novel spherical super-resolution
model for 360-degree videos.

3 Methodology

3.1 Focused icosahedral mesh

In this section, we ﬁrst introduce our proposed fo-
cused icosahedral mesh. The icosahedral spherical
mesh [36] is a common discretization of the spherical
surface. The mesh can be obtained by progressively
sub-dividing each face of the unit icosahedron into
four equal triangles.

Operations on a full spherical mesh, reﬁned to a
granularity that can include all pixels from a planar
representation of a 360-degree video frame, however,
requires a signiﬁcant amount of computation. In ad-
dition, operations on the full mesh cannot easily sup-
port operations on sub-areas of the spherical surface.
Performing super-resolution on “sub-areas” of the
spherical surface can be beneﬁcial for real-world 360-
degree applications. This is because human eyes as
well as their viewing devices (e.g., the head-mounted
display) have limited ﬁelds-of-view (FoV), usually
represented as the angular extent of the ﬁeld that can
be observed. For example, Figure 2 represents a 80-
degree by 80-degree FoV. To render the view shown
in this ﬁgure, only part of the sphere is required.

Such “sub-areas” would be useful

in “tiling”
schemes that can be used to support spatial-adaptive
super-resolution over the 360-degree view. That is, if
only a small area on the sphere will be viewed by the
user, we may only need to apply super-resolution to
a sub-portion of the sphere instead of the full sphere.
As a result, performing super-resolution on the full
icosahedral mesh may no longer be necessary as it
requires more computation resources.

4

3.1.1 Rotating content to the Focused Level-

1 origin

Our model operates on a single focused icosahedral
mesh, instead of operating on separate meshes for
diﬀerent Level-1 reﬁned icosahedral faces. To allow
our model to perform super-resolution for any area
on the sphere, we need to map spherical pixel content
that belongs to any arbitrary full Level-1 mesh face
to the face that is selected to be reﬁned. To do so, we
pre-compute a rotation matrix M ∈ RF ×V ×C, where
F represents the total number of faces in a full Level-
1 mesh, which is 80. V is the number of vertices in
a Level-1 face, as shown in ﬁgure3. A Level-1 mesh
consists of 80 triangles, each containing 3 vertices.
C represents the number of dimensions of Euclidean
coordinates in sphere, namely xyz.

We denote the Level-1 face selected to be reﬁned
as face F0. To rotate an arbitrary face Fi, i ∈ (0, 80)
on the Level-1 mesh to the reﬁned face F0, we need
to ﬁnd a rotation matrix Mi for face Fi such that
Fi = Mi · F0, where Fi and F0 are 3 × 3 matrices that
represent the xyz coordinates of three vertices of a
triangle face.

0

Therefore, we can obtain Mi as: Mi = Fi · F (−1)
.
We ﬁrst rotate the vertices in the Focused Level-X
Mesh with the rotation matrix M , and then compute
a mapping from each pixel in the input planar repre-
sentation (e.g., equirectangular image) to the rotated
Focused Level-X vertex. In this way, we can represent
all 80 diﬀerent faces on the full Level-1 mesh through
a single Focused Mesh ﬁle, which has the potential to
save a signiﬁcant amount of computation and storage
resources and achieves better parameter eﬃciency.

Figure 4 visualizes how one focused icosahedral
mesh can be used to represent all 80 diﬀerent Level-1
faces. Figure 4(a) shows an original equirectangular-
projected 360-degree image. In this image, we high-
light two areas marked by magenta circles. In Fig-
ure 4(b), the left-hand-side image shows the Focused
Level-9 mesh visualized on an equirectangular image.
Magenta points in this ﬁgure represent vertices in the
full Level-1 mesh. There are 42 vertices in the full
Level-1 mesh. The right-hand-side image in Figure
4(b) magniﬁes the reﬁned face in the Focused icosa-
hedral mesh to show details. We can see that content

Figure 2: The user can only observe a sub-portion of
the sphere at a time. For example, this ﬁgure shows
a 80-degree by 80-degree ﬁeld-of-view.

To support both faster operation and super-
resolution on a sub-portion of the sphere, we pro-
pose a partial reﬁnement scheme to generate “Fo-
cused Icosahedral Mesh”.

To generate a focused icosahedral mesh, we ﬁrst
create a Level-1 icosahedral mesh by reﬁning each
face on a unit icosahedron into 4 faces. In this way,
the 20-face icosahedron is reﬁned into a Level-1 icosa-
hedral mesh with 80 faces. An example full Level-1
mesh with 80 faces is shown in Figure 3(a).

We then select one face out of the 80 faces of the
Level-1 icosahedral mesh and only reﬁne triangles lo-
cated inside the selected Level-1 face.

Speciﬁcally, in our focused mesh representation, we
select the face of the Level-1 mesh that covers the po-
sition of <latitude=0, longitude=0> on the sphere
since very little distortion is introduced when pixels
near this area are projected to the 2D plane. Fig-
ure 3(b) shows the Focused Level-2 mesh where the
selected Level-1 face is reﬁned into 4 smaller faces.
Figures 3(c) and 3 (d) show the Focused Level-3 and
Focused Level-5 meshes, respectively.

5

(a) Full Level-1 Mesh

(b) Focused Level-2 Mesh

(c) Focused Level-3 Mesh

(d) Focused Level-5 Mesh

Figure 3: Example of meshes in Level-1, Level-2, Level-3 and Level-5. To create “Focused icosahedral
meshes”, we select one face in the full Level-1 mesh and repeatedly reﬁne triangles in this Level-1 face to
obtain Focused Level-X meshes.

in this face are in the same position as in the original
equirectangular-projected image.

Figure 4(c) shows the resulting visualization when
we rotate a diﬀerent Level-1 face to the reﬁned face.
The image on the right magniﬁes the reﬁned face to
show details.

3.2 MeshConv Transpose

UGSCNN[1] also proposed MeshConv Transpose op-
eration in their UNet architecture. MeshConv Trans-
pose takes level-i mesh for input and outputs a level-
i + 1 mesh, which can be described as follows:

3.1.2 Mesh sizes

Table 1 shows the number of vertices in both Full
and Focused icosahedral meshes in diﬀerent levels of
reﬁnement. A Full Level-9 mesh has more than 2.6
million vertices and requires more than 1.9 GB space
for storage. On the other hand, a Focused Level-
9 mesh has only about 33K vertices, requiring only
about 31 MB storage space.

We know that the area of a unit spherical sur-
face is 4π. A frame generated through the equirect-
angular projection covers a corresponding area of
2π × π = 2π2. Suppose there are Nx vertices in
the Full Level-X mesh, given that vertices on the
icosahedral mesh are roughly uniformly distributed
on the sphere, we can estimate the equivalent 2D
equirectangular-projected frame resolution as follows:
Nx × π, H = W/2, where W and H are the
W =
width and height of the equirectangular projection,
respectively. The results are listed in Table 1. We
ﬁnd that Level-6 mesh is roughly equivalent to the
2D equirectangular projection in 360x180 resolution,
and that Level-9 mesh is roughly equivalent to the 2D
equirectangular projection in 2880x1440 resolution.

√

Mi+1 = MeshConv(Padding(Mi))

where P represents zero padding, Mi+1 and Mi are
level-i+1 mesh and level-i mesh, respectively. In gen-
eral, MeshConv Transpose simply padding 0s on new
vertexes in level-i + 1 mesh, then apply MeshConv on
the new zero-padding level-i + 1 mesh. However, it’s
easy to implement but ineﬃcient.

3.3 VertexShuﬄe

Motivated by PixelShuﬄe [5] commonly used in 2D
super-resolution models, we proposed VertexShuﬄe
in our spherical super-resolution model, which can
be described as follows:

Mi+1 = VertexShuﬄe(Mi)
Mi = Mi0, Mi1, Mi2, Mi3
N (cid:48)
i = (Mij + Mi(j+1))/2
Ni = unique(N (cid:48)
i )
VertexShuﬄe(Mi) = concat(Mi0, Ni)

The input of our basic VertexShuﬄe operation can
be represented as Mi ∈ RF ×Vi, where F is the feature

6

Table 1: Number of vertices in Full icosahedral mesh, Focused icosahedral mesh, and their roughly-
equivalent 2D planar resolution in the equirectangular projection.

Level

Full
Focused

Level-6

Level-7

Level-8

Level-9

40,962
600

163,842
2,184

655,362
8,424

2,621,442
33,192

2D planar

360x180

720x360

1440x720

2880x1440

dimension in Level-i, and Vi represents the number
of vertices of Level-i mesh. The output is Mi+1 ∈
RF (cid:48)×Vi+1 , where F (cid:48) is the feature dimension in Level-
i + 1, which is F/4 in our work, and Vi+1 represents
the number of vertices of the Level-i + 1 mesh.
split Mi

parts
{Mi0, Mi1, Mi2, Mi3} on feature map dimension,
where Mij ∈ RF (cid:48)×Vi, j = 0, 1, 2, 3, F (cid:48) here is F/4.
We keep Mi0 as our Level-i mesh, which will be used
later. Mi1, Mi2, Mi3 are used to reﬁned vertices in
Level-i + 1 mesh.

ﬁrstly

four

into

We

As we introduced before, a spherical mesh can
be obtained by progressively sub-dividing each face
of the unit icosahedron into four equal triangles.
Here, we treat a single triangle face as a se-
quence of vertices, v0, v1, v2 and a sequence of edges
(v0, v1), (v1, v2), (v2, v0). The reﬁnement process can
be regarded as progressively construct midpoint ver-
tex on associated edges, and new edges in Level-i + 1
are created between each pair of midpoint vertices,
thus a single face in Level-i are reﬁned into four new
faces in Level-i + 1.

To fully make use of feature maps in Level-i, we
use Mi1, Mi2, Mi3 to reﬁne vertices in Level-i + 1
mesh. Speciﬁcally, we use Mi1 to calculate mid-
point between (v0, v1), Mi2 to calculate midpoint be-
tween (v1, v2), and Mi3 to calculate midpoint be-
tween (v2, v0). Midpoint vertex values are con-
structed by averaging the values associated with the
original two vertices on a edge, which can be de-
scribed as follows:

N (cid:48)
N (cid:48)
N (cid:48)

i0 = (Mi1(v0) + Mi1(v1))/2
i1 = (Mi2(v1) + Mi2(v2))/2
i2 = (Mi3(v2) + Mi3(v0))/2

7

Thus, we can get a set of midpoint vertices N (cid:48)
i ,
which are new vertices generated in Level-i + 1 mesh.
However, there exits redundant midpoints due to the
shared edges that may be calculated twice. We have
to performs deduplication on the set of midpoint ver-
tices. There are plenty of ways to select midpoint
between the two calculated midpoint, such as, max,
min, average, weighted average. In our paper, we
simply select the ﬁrst instance of a midpoint, which
achieves best results in our experiments.

Then, we have a set of unique midpoint vertices
that used to reﬁne the next level mesh Ni ∈ RF (cid:48)×Ai,
where Ai = Vi+1 − Vi.

Finally, we concatenate partial of feature map in
Level-i Mi0 with the new calculated midpoint vertices
Ni to formulate our Level-i + 1 mesh.

Compared to MeshConv Transpose, we do not have
extra learnable parameters. In other word, the imple-
mentation of VertexShuﬄe is not only more param-
eters eﬃcient, but also achieves signiﬁcantly better
performance.

3.4 Model architecture

We apply our Focused icosahedral mesh and Ver-
texShuﬄe in super resolution. The architecture of
our model is shown in Figure 5.
In this ﬁgure, we
show the input of our model as a Level-7 Focused
icosahedral mesh, it ﬁrst goes through a MeshConv
layer with Batch Normalization [37] followed by a
ReLU [38] activation function. Then, we use two
adapted Residual Blocks [30] to further extract fea-
tures, which we will explain in depth later. We fur-
ther concatenate the output of the ﬁrst MeshConv
and the output from the two ResBlocks by element-
wise addition. After that, we apply two VertexShuﬄe

residual block. Other settings are similar to the reg-
ular residual block [30].

MeshConv. The MeshConv operation introduced
by Jiang [1] et al.
is performed by taking a linear
combination of linear operator values computed on a
set of input mesh vertex values. MeshConv can be
formulated as follows:

MeshConv(F ; θ) = θ0IF +θ1∇latF +θ2∇lngF +θ3∇2F

where I represents for the identity, which can be
regarded as the 0th order diﬀerential, same as ∇00.
∇x and ∇y are derivatives in two orthogonal spatial
dimensions respectively, which can be viewed as the
1st order diﬀerential. ∇2 stands for the Laplacian
operator, which can be regarded as the 2nd order
diﬀerential.

At a high-level, these linear operators can be
viewed as computing a set of local information near
each vertex of the mesh. The standard 3×3 cross cor-
relation operation can be viewed as a set of nine linear
operators. Each of the linear operators returning a
value of either the pixel itself or an adjacent pixel.
Compared to the 3 × 3 convolution, it is clear that
the set of four linear operators used by MeshConv
are less expressive. They not only extract less infor-
mation per pixel, but this information also can drop
information about a vertex’s surrounding. For ex-
ample, the gradient operation on the mesh computes
a 3-dimensional average of either six or seven val-
ues. Another degree-of-freedom is dropped from the
gradient when taking only the east-west and north-
south components of the gradient. We hypothesize
that some of the information excluded from the linear
operator computations could be useful for the super-
resolution task. To attempt to mitigate this infor-
mation loss, rather than including single MeshConv
ops in our network architecture, we include pairs of
composed MeshConv ops. These paired operations
aggregate more local information around a vertex be-
fore the non-linearity is applied, allowing the network
to capture more-useful characteristics needed for the
super-resolution task.

(a) This ﬁgure shows an equirectangular-projected 360-degree
image. Magenta circles, b and c, in this ﬁgure mark areas
corresponding to two diﬀerent reﬁned faces. (Original photo
by Timothy Oldﬁeld on Unsplash: https://unsplash.com/
photo/luufnHoChRU)

(b) The left-hand-side image displays the Focused Level-9 mesh
visualized on an equirectangular image. The right-hand-side
image displays a magniﬁed view of the reﬁned face. In both
images, magenta points represent vertices in the full Level-1
mesh.

(c) This ﬁgure displays a diﬀerent Level-1 icosahedral face ro-
tated to the face reﬁned in the Focused Mesh. Pixel values from
the original image are attached to rotated vertices by inverting
the rotation for positions of the mesh vertices then ﬁnding the
nearest neighbor pixel of this rotated position.

Figure 4: Visualizing the Focused Icosahedral mesh.

operations to upscale the features. Finally, our model
ends up with a MeshConv layer. Thus, a Level-9 Fo-
cused icosahedral mesh is generated.
Adapted Residual Block. We adapt a regular
residual block by adding two MeshConv layers in the

8

Figure 5: This ﬁgure shows the architecture of our proposed Spherical Super-Resolution (SSR) model
that uses MeshConv and VertexShuﬄe operations. Here, L7 represents the input Level-7 mesh, and L9
represents the output Level-9 mesh. Our model starts with a MeshConv layer followed by 2 ResBlocks and
2 VertexShuﬄe layers, it then ends with a ﬁnal MeshConv layer.

video datasets: the 360-Degree Video Head Move-
ment Dataset [3] and VR User Behavior Dataset [39]
to generate a spherical super resolution dataset with
high quality. The 360-Degree Video Head Movement
Dataset [3] contains 5 videos in 4K quality, and the
VR User Behavior Dataset contains 5 videos with
2560 × 1440 resolution. We use FFmpeg [40] to ex-
tract the key frames of each video in the dataset.

We ﬁrst construct a small dataset with the 360-
Degree Video Head Movement Dataset [3]. This
small dataset contains 345 images of 2880 × 1440 res-
olution. We randomly split the dataset with 80%
training set and 20% test set. The training set pro-
vide roughly 21,440 training items, and the test set
provide 6,160 testing items. We evaluate our model
with the upscaling factor of 4, that is, 16x super-
resolution.

We also generate a larger dataset with both the
360-Degree Video Head Movement Dataset [3] and
the VR User Behavior Dataset [39]. The large dataset
contains 1,532 images in total with 2560 × 1440 reso-
lution. As with the small dataset, we split the dataset
with 80% training set and 20% test set. The training
set provide roughly 95,360 training items, and the
test set provide 27,200 testing items. We evaluate
our model with the same upscaling factor of 4.

4.2 Implementation details

With our generated Focused Icosahedral mesh, we
map the 2D equirectangular-projected frame to the
partial mesh sphere.
In both the small and large
datasets, the input data is in Level-7, which is

Figure 6: The adapted ResBlock used in our model.

3.5 Loss function

Similar to general super-resolution tasks, our goal is
minimizing the loss between the reconstructed im-
ages Yi and the corresponding ground truth high-
resolution images Hi. Given a set of high-resolution
images Hi and their corresponding low-resolution im-
ages Xi, we represent the loss as follows:

MSE =

1
N

N
(cid:88)

i=1

(F (Xi) − Yi)2

L = 10 × log10(MSE)

where N is the number of training samples. Here,
we adapt mean square error as our loss function. It
can be regarded as the negative peak signal-to-noise
(PSNR) value, which is more straightforward in our
task.

4 Experiments

4.1 Dataset

Due to the lack of oﬃcial spherical super-resolution
datasets, we collect two publicly-available 360-degree

9

L7L7Input signalsMeshConvL7L7VertexShuffleL8ResBlockL9Output signals+Feed ForwardResBlockVertexShuffleMeshConvL9Element-wise AdditionTable 2: PSNR (dB) results for small and large dataset.

Model

Small Large Average

Spherical: MeshConv with transposed MeshConv
Spherical: MeshConv with VertexShuﬄe (SSR)

18.52
31.44

16.57
34.13

17.54
32.79

Table 3: Comparison of total number of model parameters, and per-frame inference time.

Model

Total # of Parameters Per-image/frame Inference Time

Spherical: MeshConv with transposed MeshConv
Spherical: MeshConv with VertexShuﬄe (SSR)

1001225
734905

5883 ms
578 ms

roughly equivalent to a 2D equirectangular-projected
frame in 720 × 360 and 640 × 360 resolution. The
output data is in Level-9, which is roughly equiva-
lent to 2880 × 1440 and 2560 × 1440 equirectangular-
projected frame, respectively. The upscaling factor
in our experiment set up is ×4. That is, the number
of output vertices is 16x the number of input vertices.
In our experiments, we train our model with 50
epochs with batch size of 64. we set learning rate
as 0.01 and use Adam [41] as our optimizer. We use
the PSNR as the performance metric to evaluate our
models.

4.3 Comparison

with MeshConv

Transpose

We compare our spherical super-resolution (SSR)
model that uses the VertexShuﬄe operation with a
baseline model that uses the MeshConv Transposed
proposed in the original UGSCNN paper
[1]. We
conduct experiments on both of the small and large
datasets with same conﬁguration, i.e., using Focused
Icosahedral mesh with an upscaling factor of ×4.

Table 2 shows the performance of two models on
both datasets. As we can see, our model achieves
signiﬁcantly higher performance than the baseline
MeshConv transpose model. We also found that with
the increasing volume of dataset, the performance of
our model gets better. Our model can achieve the
PSNR result on larger dataset of 34.13 dB, 31.44 dB
on small on, and 32.79 dB on average, while the base-

line method performs not so good in spherical super
resolution task, it can only achieve 17.54 dB on av-
erage.

4.4 Inference time and parameters

For fair comparison of model inference time, we ﬁxed
the batch sizes of both models to 16. We ﬁrst com-
pute the average inference time for each batch, then
divide it by the batch size, and ﬁnally, we multiple
the number of data items for a full frame, which is
80 here. This allows us to roughly obtain the per-
frame inference time. We collected our results via
experiments on a Tesla P100 GPU.

Table 3 compares the inference time and total num-
ber of parameters of our proposed model with the
baseline model that uses MeshConv transpose.

Our model can save roughly 20% parameters com-
pared to the baseline model, which improves the ef-
ﬁciency signiﬁcantly.
In addition, MeshConv with
transposed MeshConv requires nearly 6 seconds to
process a full image/frame. With our proposed Ver-
texShuﬄe operation, our SSR model can achieve
more than 10x acceleration in processing a full im-
age/frame, which is signiﬁcantly faster than the
spherical baseline model with transposed MeshConv
operations.

In addition, given that a user can only watch a
sub-portion of the spherical image/frame at a time,
there is no need to perform super-resolution for all 80
faces of a frame at the same time. This indicates that

10

Figure 7: Visualization of results of our proposed SSR model (represented as “Ours”) and the MeshConv
transpose model (represented as “Transpose”). “HR” represents the high resolution version of the data, i.e.,
the groundtruth.

our SSR model can be used in real-world applications
with even faster per-image/frame processing time.

4.5 Quantitative results

our model

outperforms

the baseline
Overall,
MeshConv-transpose-based model in all of PSNR re-
sults, inference time, and the total number of param-
eters. Since we are the ﬁrst to directly apply spheri-
cal convolutional neural network on super-resolution
task, we have no benchmark to compare with. Sim-
ply comparing the PSNR results with the 2D super-
resolution task would be unfair due to the diﬀerent
data format convolution method. Hence, we only
compare our method with the MeshConv transpose
model, which is a fair comparison to show our con-
tributions.

4.6 Qualitative results

Figure 7 visualizes the results of our proposed SSR
model and the MeshConv transpose model. We use

two images with signiﬁcantly diﬀerent PSNR results
as examples.3 Here, we ﬁrst show the full image
frame via two formats, in both the spherical domain
and the 2D planar domain. Moreover, in ﬁgures (a)
to (d), we select two focused icosahedral meshes from
diﬀerent locations on a sphere to demonstrate the eﬃ-
ciency of directly applying spherical super-resolution
to spherical data.

4.7 Discussion

The computer vision community has made tremen-
dous progress in 2D super-resolution while there is
little precedent eﬀort in directly applying spherical
super-resolution. There are signiﬁcant challenges in
directly performing super-resolution on spherical sig-
nals, e.g., how to perform convolution operations in
3D space, how to perform deconvolution operations

3These images are from the following two videos in
the 360-degree video head movement dataset [3]:
https:
//www.youtube.com/watch?v=2OzlksZBTiA and https://www.
youtube.com/watch?v=sJxiPiAaB4k

11

in 3D space, how to perform PixelShuﬄe in 3D space
with other spherical convolution method, etc. In this
paper, we provide a straightforward approach to di-
rectly apply 3D convolution to spherical signals and
can achieve good results, which shows great potential
in the 3D super-resolution area. In addition, we show
that it is feasible to directly apply spherical super-
resolution on spherical signals, which can avoid issues
in applying 2D super-resolution in 3D space, such as
distortion, oversampled pixels, etc. We believe there
are a great number of interesting directions to exploit
ahead on directly apply super resolution on spherical
signals.

5 Conclusion

In this paper, we proposed a memory- and
bandwidth-eﬃcient representation of the spherical
mesh – the Focused Icosahedral Mesh, which is
more ﬂexible than full meshes and saves a signiﬁ-
cant amount of computation resources, and a novel
VertexShuﬄe operation to further improve the per-
formance compared to the baseline model that uses
the MeshConv Transpose operation. To illustrate our
proposed idea, we present a spherical super resolu-
tion model and show great capacity and potential to
apply the traditional 2D computer vision tasks on
spherical signals. For evaluation, we create a new
high-resolution spherical super resolution dataset by
extracting key frames from a set of collected 360-
degree videos. Experiments on the dataset show that
our proposed model is superior to the baseline model
in performing spherical super-resolution tasks with
remarkable eﬃciency.

References

[1] Chiyu

Jiang,

Jingwei Huang,

Karthik
Kashinath, Philip Marcus, Matthias Niess-
ner, et al.,
“Spherical cnns on unstructured
grids,” arXiv preprint arXiv:1901.02039, 2019.

[2] “Equirectangular

Projection,”

http://mathworld.wolfram.com/
EquirectangularProjection.html.

[3] Xavier Corbillon, Francesca De Simone, and
Gwendal Simon, “360-degree video head move-
ment dataset,” in Proceedings of the 8th ACM on
Multimedia Systems Conference, 2017, pp. 199–
204.

[4] Taco Cohen, Mario Geiger, Jonas K¨ohler,
“Convolutional net-
arXiv preprint

and Max Welling,
works for spherical signals,”
arXiv:1709.04893, 2017.

[5] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Jo-
hannes Totz, Andrew P Aitken, Rob Bishop,
Daniel Rueckert, and Zehan Wang, “Real-time
single image and video super-resolution using
an eﬃcient sub-pixel convolutional neural net-
work,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016,
pp. 1874–1883.

[6] Michal Irani and Shmuel Peleg, “Improving res-
olution by image registration,” CVGIP: Graph-
ical models and image processing, vol. 53, no. 3,
pp. 231–239, 1991.

[7] Chao Dong, Chen Change Loy, Kaiming He, and
Xiaoou Tang, “Learning a deep convolutional
network for image super-resolution,” in Euro-
pean conference on computer vision. Springer,
2014, pp. 184–199.

[8] Chao Dong, Chen Change Loy, Kaiming He,
and Xiaoou Tang, “Image super-resolution us-
ing deep convolutional networks,” IEEE trans-
actions on pattern analysis and machine intelli-
gence, vol. 38, no. 2, pp. 295–307, 2015.

[9] Chao Dong, Chen Change Loy, and Xiaoou
Tang, “Accelerating the super-resolution con-
volutional neural network,” in European con-
ference on computer vision. Springer, 2016, pp.
391–407.

[10] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng
Zhong, and Yun Fu, “Residual dense network for
image super-resolution,” in Proceedings of the
IEEE conference on computer vision and pattern
recognition, 2018, pp. 2472–2481.

12

[11] Ying Tai, Jian Yang, and Xiaoming Liu, “Image
super-resolution via deep recursive residual net-
work,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017,
pp. 3147–3155.

[12] Stefano Petrangeli, Viswanathan Swaminathan,
Mohammad Hosseini, and Filip De Turck, “An
http/2-based adaptive streaming framework for
in Proceedings of
360 virtual reality videos,”
the 2017 ACM on Multimedia Conference. ACM,
2017, pp. 306–314.

[13] Lan Xie, Zhimin Xu, Yixuan Ban, Xinggong
Zhang, and Zongming Guo, “360probdash: Im-
proving qoe of 360 video streaming using tile-
based http adaptive streaming,” in Proceedings
of the 2017 ACM on Multimedia Conference.
ACM, 2017, pp. 315–323.

[14] Afshin Taghavi Nasrabadi, Anahita Mahzari,
Joseph D Beshay, and Ravi Prakash, “Adaptive
360-degree video streaming using scalable video
coding,” in Proceedings of the 2017 ACM on
Multimedia Conference. ACM, 2017, pp. 1689–
1697.

[15] Alireza Zare, Alireza Aminlou, Miska M Han-
nuksela, and Moncef Gabbouj, “Hevc-compliant
tile-based streaming of panoramic video for vir-
tual reality applications,” in Proceedings of the
2016 ACM on Multimedia Conference. ACM,
2016, pp. 601–605.

[16] Xavier Corbillon, Gwendal Simon, Alisa Devlic,
and Jacob Chakareski, “Viewport-adaptive nav-
igable 360-degree video delivery,” in Communi-
cations (ICC), 2017 IEEE International Confer-
ence on. IEEE, 2017, pp. 1–7.

[17] Mario Graf, Christian Timmerer, and Christo-
pher Mueller,
“Towards bandwidth eﬃcient
adaptive streaming of omnidirectional video over
http: Design, implementation, and evaluation,”
in Proceedings of the 8th ACM on Multimedia
Systems Conference. ACM, 2017, pp. 261–271.

[18] Liyang Sun, Fanyi Duanmu, Yong Liu, Yao
Wang, Yinghua Ye, Hang Shi, and David Dai,

“Multi-path multi-tier 360-degree video stream-
ing in 5g networks,” in Proceedings of the 9th
ACM Multimedia Systems Conference. ACM,
2018, pp. 162–173.

[19] Anahita Mahzari, Afshin Taghavi Nasrabadi,
Aliehsan Samiei, and Ravi Prakash, “Fov-aware
edge caching for adaptive 360 ˆA° video stream-
ing,” in 2018 ACM Multimedia Conference on
Multimedia Conference. ACM, 2018, pp. 173–
181.

[20] Feng Qian, Bo Han, Qingyang Xiao, and Vi-
jay Gopalakrishnan, “Flare: Practical viewport-
adaptive 360-degree video streaming for mobile
devices,”
in Proceedings of the 24th Annual
International Conference on Mobile Computing
and Networking. ACM, 2018, pp. 99–114.

[21] Yu Guan, Chengyuan Zheng, Xinggong Zhang,
Zongming Guo, and Junchen Jiang, “Pano: Op-
timizing 360 video streaming with a better un-
derstanding of quality perception,” in Proceed-
ings of the ACM Special Interest Group on Data
Communication, pp. 394–407. 2019.

[22] Chao Zhou, Zhenhua Li, and Yao Liu,

“A
measurement study of oculus 360 degree video
streaming,”
in Proceedings of the 8th ACM
on Multimedia Systems Conference. ACM, 2017,
pp. 27–37.

[23] Mallesham Dasari, Arani Bhattacharya, Santi-
ago Vargas, Pranjal Sahu, Aruna Balasubrama-
nian, and Samir R Das, “Streaming 360-degree
videos using super-resolution,” .

[24] Jiawen Chen, Miao Hu, Zhenxiao Luo, Zelong
Wang, and Di Wu, “Sr360: boosting 360-degree
video streaming with super-resolution,” in Pro-
ceedings of the 30th ACM Workshop on Network
and Operating Systems Support for Digital Au-
dio and Video, 2020, pp. 1–6.

[25] Bo Xiong and Kristen Grauman, “Snap angle
prediction for 360 panoramas,” in Proceedings
of the European Conference on Computer Vision
(ECCV), 2018, pp. 3–18.

13

[26] Marc Eder, Mykhailo Shvets, John Lim, and
Jan-Michael Frahm, “Tangent images for mit-
igating spherical distortion,” in Proceedings of
the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020, pp. 12426–12434.

[27] Taco S Cohen, Maurice Weiler, Berkay Ki-
canaoglu, and Max Welling, “Gauge equivari-
ant convolutional networks and the icosahedral
cnn,” arXiv preprint arXiv:1902.04615, 2019.

[28] Chao Zhang, Stephan Liwicki, William Smith,
and Roberto Cipolla,
“Orientation-aware se-
mantic segmentation on icosahedron spheres,” in
Proceedings of the IEEE International Confer-
ence on Computer Vision, 2019, pp. 3533–3541.

[29] Jiwon Kim, Jung Kwon Lee, and Kyoung
Mu Lee, “Accurate image super-resolution using
very deep convolutional networks,” in Proceed-
ings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 1646–1654.

[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun,
“Deep residual learning for image
recognition,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition,
2016, pp. 770–778.

[31] Jiwon Kim, Jung Kwon Lee, and Kyoung
Mu Lee, “Deeply-recursive convolutional net-
work for image super-resolution,” in Proceedings
of the IEEE conference on computer vision and
pattern recognition, 2016, pp. 1637–1645.

[32] Christian Ledig, Lucas Theis, Ferenc Husz´ar,
Jose Caballero, Andrew Cunningham, Alejan-
dro Acosta, Andrew Aitken, Alykhan Tejani,
Johannes Totz, Zehan Wang, et al., “Photo-
realistic single image super-resolution using a
generative adversarial network,” in Proceedings
of the IEEE conference on computer vision and
pattern recognition, 2017, pp. 4681–4690.

[33] Bee Lim, Sanghyun Son, Heewon Kim, Se-
ungjun Nah, and Kyoung Mu Lee, “Enhanced
deep residual networks for single image super-
resolution,” in Proceedings of the IEEE confer-

ence on computer vision and pattern recognition
workshops, 2017, pp. 136–144.

[34] Ian Goodfellow, Jean Pouget-Abadie, Mehdi
Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio,
“Generative adversarial nets,” in Advances in
neural information processing systems, 2014, pp.
2672–2680.

[35] Jiawen Chen, Miao Hu, Zhenxiao Luo, Zelong
Wang, and Di Wu, “Sr360: Boosting 360-degree
video streaming with super-resolution,” in Pro-
ceedings of the 30th ACM Workshop on Net-
work and Operating Systems Support for Digital
Audio and Video, New York, NY, USA, 2020,
NOSSDAV ’20, p. 1–6, Association for Comput-
ing Machinery.

[36] John R Baumgardner and Paul O Frederickson,
“Icosahedral discretization of the two-sphere,”
SIAM Journal on Numerical Analysis, vol. 22,
no. 6, pp. 1107–1115, 1985.

[37] Sergey Ioﬀe and Christian Szegedy, “Batch nor-
malization: Accelerating deep network training
by reducing internal covariate shift,” in Pro-
ceedings of the 32nd International Conference
on International Conference on Machine Learn-
ing - Volume 37. 2015, ICML’15, p. 448–456,
JMLR.org.

[38] Abien Fred Agarap,

“Deep learning us-
ing rectiﬁed linear units (relu),” 2018,
cite
arxiv:1803.08375Comment: 7 pages, 11 ﬁgures,
9 tables.

[39] Chenglei Wu, Zhihao Tan, Zhi Wang, and
Shiqiang Yang, “A dataset for exploring user
behaviors in vr spherical video streaming,” in
Proceedings of the 8th International Conference
on Multimedia Systems, Taipei, Taiwan, 2017,
MMSys ’17, ACM.

[40] “FFmpeg,” http://www.ffmpeg.org/.

[41] Diederik P Kingma and Jimmy Ba,

“Adam:
A method for stochastic optimization,” arXiv
preprint arXiv:1412.6980, 2014.

14

