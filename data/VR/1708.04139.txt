PhyShare: Sharing Physical Interaction in Virtual Reality
Fengyuan Zhu
60 5th Ave, Ofﬁce 342
New York, NY 10011
zhufyaxel@gmail.com

Ken Perlin
60 5th Ave, Ofﬁce 342
New York, NY 10011
ken.perlin@gmail.com

Zhenyi He
60 5th Ave, Ofﬁce 342
New York, NY 10011
zh719@nyu.edu

7
1
0
2

g
u
A
0
1

]

C
H
.
s
c
[

1
v
9
3
1
4
0
.
8
0
7
1
:
v
i
X
r
a

Figure 1: Sharing Physical Interaction When Clinking the Mugs and Pushing the Wall

ABSTRACT
Virtual reality has recently been gaining wide adoption. Yet
the absence of effective haptic feedback in virtual reality sce-
narios can strongly detract from the suspension of disbelief
needed to bridge the virtual and physical worlds. PhyShare
proposes a new haptic user interface based on actuated robots.
Because participants do not directly observe these robotic
proxies, multiple mappings between physical robots and vir-
tual proxies can be supported. PhyShare bots can act either as
directly touchable objects or invisible carriers of physical ob-
jects, depending on the scenario. They also support distributed
collaboration, allowing remotely located VR collaborators to
share the same physical feedback. A preliminary user study
evaluates PhyShare for user awareness, preference, acknowl-
edgement, reliability and related factors.

ACM Classiﬁcation Keywords
H.5.m. Information Interfaces and Presentation: Miscella-
neous; H.5.2. Information Interfaces and Presentation: User
Interfaces

Author Keywords
Virtual Reality; Haptic User Interfaces; Robots;

INTRODUCTION
This work aims to add to the growing ﬁeld of "Robotic Graph-
ics", described by William A. McNeely [19] as "robots simu-
lating the feel of an object and graphics displays simulating
its appearance".

Several signiﬁcant steps have recently been made towards
William A. McNeely’s concept, particularly through research
on passive objects like MAI Painting Brush[21, 33], Normal-
Touch[3], human actuated systems like TurkDeck[5] and actu-
ated or wheeled robots such as Zooids[15], CirculaFloor[13],
Snake Charmer[1], and Tangible Bots[22].

However, current systems suffer from a number of limitations.
First, passive objects and human actuated systems only sup-
port static haptic feedback, which can be insufﬁcient when
interacting with a dynamic environment. On the other hand,
actuated systems in real or in augmented reality (AR) envi-
ronments aim to control movable props that correspond to
particular virtual objects. However, current implementations
do not support dynamic mapping between physical props and
their virtual proxies, and might therefore require large num-
bers of actuated objects[15], with a corresponding increase in
system complexity.

In addition, [16, 27, 28] which allow remote collaboration
based on distributed actuated tangible objects, provide only a
limited sense of visual feedback rather than a fully immersive

 
 
 
 
 
 
experience. Likewise, many such systems require expensive
hardware to function, and are thus are primarily meant to
be operated in special laboratory environments rather than
embedded in the physical world.

Our research focuses on variable mappings between virtual
objects in a VR world and their physical robotic proxies, mul-
tiple modes of physical manipulation, and a solution for dis-
tributed collaboration based on wheeled robots (see Figure 1).
PhyShare is a hardware and software system: a wheeled robot
can behave either as a haptic proxy or a invisible carrier of
haptic proxies, thereby supporting both direct manipulation
and the illusion of telekinesis in a shareable virtual world.
PhyShare utilizes the Holojam multi-user VR framework1
integrated with wheeled robots (m3pi robot2, iRobot3), and
the OptiTrack motion tracking system.

Because all interaction takes place in a virtual reality environ-
ment, PhyShare can choose what to display and what to hide.
Therefore, its mobile robots can provide haptic feedback when
touched, and move in a way that is invisible to the user at other
times. This "invisibility" allows much room for optimization
in path planning of PhyShare robotic proxies. For example,
if two remote collaborators are using PhyShare to play tic-
tac-toe in VR, each user can see the actual movement of the
remotely located controller of their opponent. Meanwhile, in
their own physical space, a haptic proxy of that controller is
invisibly moving on the desk in a path that can vary from that
actual movement, as long as it arrives at the correct location
in time for the local user to touch it.

In summary, our contributions are:

• Multiple mapping designs between physical props and vir-
tual proxies to expand the possibilities of robot assistants,
• Multiple manipulation methods for controlling actuated

robots, and

• A new interaction approach supporting haptic feedback for

distributed collaboration in virtual reality.

RELATED WORK
Our work touches on several research areas, including physical
objects in VR and AR, tabletop tangible user interfaces (TUIs)
and distributed collaboration.

Physical Objects in VR and AR
Physical objects have the ability to offer tactile feedback for
immersive experiences. While tactile feedback through stim-
ulating skin receptors alone is relatively straightforward and
continues to enjoy ongoing development, a number of VR and
AR researchers have focused on use of physical objects for
tactile feedback in recent years.

Passive Objects
SkyAnchor focuses on attaching virtual images to physical
objects that can be moved by the user[14]. Liao et al. imple-
mented a telemanipulation system that simulates force feed-
back when controlling a remote robot manipulator[18]. In both
1https://holojamvr.com/
2https://www.pololu.com
3http://store.irobot.com/default/create-programmable/

cases, the tangible objects in the scene are passive, only mov-
ing when moved by the user. This approach is not sufﬁcient
for general simulation of dynamic environments.a

Robotic Shape Display
As described in Robotic Graphics[19], Robotic Shape Dis-
play(RSD) described a scenario in which actuated robots pro-
vide feedback when users come into contact with a virtual
desktop. This scenario requires the robot to be ready to move
at any time to meet the user’s touch.

Some recent work has implemented this idea. TurkDeck[5]
simulates a wide range of physical objects and haptic effects
by using human actuators. While good for prototyping, this
approach has clear limitations in terms of scalability. A scal-
able approach requires a technology that can move without
human operators.

NormalTouch and TextureTouch[3] offer haptic shape ren-
dering in VR, using very limited space to simulate feedback
of different surfaces of various objects. Snake Charmer[1]
offers a robotic arm that moves based on its user’s position
and provides corresponding haptic feedback of different vir-
tual proxies with different textures and temperatures. Our
work shifts the focus to promoting a variety of manipulation
methods between user and objects. Therefore we focus on
direct manipulation, remote synchronization, and creating the
illusion of telekinesis.

Roboxels
Roboxels (a contraction of robotic volume elements), are able
to dynamically conﬁgure themselves into a desired shape and
size. Some recent work extends this concept. CirculaFloor[13]
provides the illusion of a ﬂoor of inﬁnite extent. It uses mov-
able ﬂoor elements, taking advantage of the inability of a VR
user to see the actual movement of these ﬂoor elements. Our
work also exploits the invisibility of a physical robot to a
person who is immersed in VR, but takes this concept in a
different direction.

Tabletop TUI and Swarm UI
Tangible user interfaces (TUIs) allow their users to manipu-
late physical objects that either embody virtual data or act as
handles for virtual data[27]. TUIs can assume several differ-
ent forms, including passive sensors[21, 33, 7] and actuated
pieces[1]. Table TUIs incorporate physical objects moving on
a ﬂat surface as input[8]. These objects are used to simulate
autonomous physical objects[4, 16, 23, 28, 29].

Tangible Bots[22] use wheeled robots as input objects on a
tabletop rear-projection display. When the user directly manip-
ulates Bots, the Bots in turn react with movement according to
what the user has done. Tangible Bits[11, 12] is general frame-
work proposed by Hiroshi Ishiii to bridge the gap between
cyberspace and the physical environment by incorporating
active tangible elements into the interface. Our work extends
this idea of combining the physical world with digital behavior
in immersive VR.

Zooids[15] provides a large number of actuated robots that
behave as both input and output. Since Zooids merge the char-
acters of controller and haptic display, users can perform ma-

nipulations more freely. A large number of robots are required
in Zooids because the display requires relatively high resolu-
tion. This is not as necessary in VR environments, because the
same haptic proxy can be used to stand in for different virtual
objects. In our work, we can use one haptic feedback object
as the physical proxy for multiple virtual objects.

Distributed Collaboration
Some work offers different views for different users[9, 10,
16, 17, 30, 31]. Among these, [10] provides only spatial an-
notations for local users as guidance. Other work shares the
same environment without haptic feedback, including Holo-
portation[20] and MetaSpace[32], in which all interactions
are established as belonging either to local physical objects
or remote virtual objects. InForm[9, 16, 17] introduces an
approach to physical telepresence that includes capturing and
remotely rendering the shape of objects in shared workspaces.
Local actuated movables behave both as input by manipulation
and output by shape changing. This approach is limited by the
disparity between visual and haptic feedback for remote users.

PSyBench[4] ﬁrst suggested the possibility of distributing TUI.
The approach was to virtually share the same objects when
users are remote using Synchronized Distributed Physical Ob-
jects. A similar idea called Planar Manipulator Display[29]
was developed to interact with movable physical objects. A
furniture arrangement task was implemented for this bidirec-
tional interaction.

Tangible Active Objects[28] extends the idea by adding audio
feedback. This research was based around the constraint of
What You Can See is What You Can Feel[34]. Our own
work adapts this core concept and puts it into VR to open up
more possibilities, including multiple mappings between what
the user can see (virtual proxies) and what the user can feel
(physical objects).

Some work mentions different mapping possibilities in dis-
tributed collaboration[25, 26, 27]. TwinSpace[25, 26] extends
the idea of the "ofﬁce of the future" [24] by presenting a frame-
work for collaborative cross-reality. It supports multiple mixed
reality clients that can have different conﬁgurations.

One inspiration for our approach of dynamic mapping was
[27], which implemented a remote active tangible interaction
based on a furniture placement task. This work mentions that
a robot could represent different pieces of furniture at different
points in time, likening this to changing costumes. In our
research, we extend this idea of changeable representation and
identity to VR.

HARDWARE AND SOFTWARE DESIGN

Hardware
We use an OptiTrack for position and rotation tracking. Rigid
body markers are attached to each object to be tracked, in-
cluding the head-mounted display (HMD) on each user and
gloves on each user’s hands. For the HMD, we use a GearVR
with Samsung Galaxy Note 44, and use sensor fusion between
the GearVR’s IMU and the OptiTrack. For the robots, we use

4http://www.samsung.com/global/galaxy/gear-vr/

(a) Tabletop Robot Conﬁguration
a: physical objects, b: rigid body
marker, c: XBee wireless module,
d: battery, e: micro control board,
f: pololu m3pi robot with mbed
socket

(b) Floor Based Robot Conﬁgura-
tion
a: wall, b: counter weight, c:
serial communication module, d:
battery, e: raspberry pi

Figure 2: Hardware Conﬁguration

small Pololu m3pi robots equipped with mbed sockets5 and
iRobot Create 2 as actuated wheeled robots.

There are two different robots used in our system, one for small
tabletop manipulation and one for large ﬂoor based movement.

For the tabletop robot, the m3pi robot can function as an
interaction object itself–an invisible holder for carrying an
interaction object (see ﬁgure 2). We equip each m3pi robot
with an XBee module for communication. We pair another
XBee board to send commands to each m3pi robot and receive
their replies.

The larger ﬂoor based robot is based on an iRobot Create 2,
to which a rigid "wall" is afﬁxed (see ﬁgure 2b). For commu-
nication, we add a raspberry pi6 onto the iRobot to receive
commands through WiFi.

Software
To connect our robots together and synchronize their actions
with multiple users in virtual reality, we use Holojam, a com-
bined networking and content development framework for
building location-based multiplayer VR experiences. Holojam
provides a standard protocol for two-way, low-latency wireless
communication, which is ﬂexible enough to use for any kind
of device on the network, including our robots. Essentially,
Holojam acts as the central relay node of a low latency local
Wiﬁ network, while other nodes can behave as either emitter,
sink or both. Each client receives updates from the Holojam
relay node (see in ﬁgure3).

DESIGN PRINCIPLES AND CHALLENGES

Multiple Mapping
One challenge of physically representing a virtual environment
is the method used to map virtual objects to their physical
5https://www.pololu.com/product/2151
6https://www.raspberrypi.org/

Figure 3: networking architecture based on Holojam

counterparts (or vice versa). In non-VR TUI systems, there
is generally a direct mapping between what users see and
what they feel. In tangible VR systems, there is more room to
experiment with the relationship between the real and virtual
environments. Conventionally, virtual objects have no physical
representation and can only be controlled indirectly or through
the use of a standard input device (such as a game controller).
Also, the set of physically controlled objects that are visually
represented in VR have traditionally been limited to the input
devices themselves. In contrast, our system provides three
different mapping mechanisms to augment the relationship
between physical and virtual representation, in order to explore
how each mapping affects user experience and interaction
design.

(a) telekinesis

(b) city builder

(c) tic-tac-toe

Figure 4: Use Cases

One-to-One Mapping
This is the standard and most straightforward option. When
users interact with a virtual object in the scene, they simulta-
neously interact with a physical proxy at the same location.
In the ’telekinesis’ use case (see ﬁgure 4a), the m3pi robot is
represented by the virtual mug.

One-to-Many mapping
Various constraints, including cost and space, limit the capa-
bility of maintaining a physical counterpart for every one of
a large number of virtual objects. When fewer proxies are
available than virtual objects, one of the total available proxies
could represent a given virtual object as required. For example,
a user with a disordered desk in VR may want to reorganize
all of their virtual items. In each instance of the user reaching
to interact with a virtual item, the single (or nearest, if there
are multiple) proxy would relocate to the position of the target
item, standing by for pickup or additional interactions. The
virtual experience is seamless, while in the physical world a
small number of proxies move into position as needed to main-
tain that seamless illusion. In the ’city builder’ use case (see
ﬁgure 4b), a proxy is ﬁtted behind-the-scenes to the virtual
building which is visually nearest to a given user’s hand. In

this implementation, the movement and position of the robots
is completely invisible to the user.

In the ’escape the room’ use case, the iRobot carries a physical
wall which moves with the user to provide haptic feedback.
The virtual wall that the user perceives in VR is much longer
than the touchable wall section that represents it in the physical
world (see in ﬁgure 1).

Many-to-One mapping
When multiple proxies represent one virtual object, we deﬁne
the mapping as "many-to-one." This is useful for remote-space
applications: A virtual object could exist in the shared envi-
ronment, which could then be manipulated by multiple users
via their local proxies.

The ’clink the mugs’ use case simulates the effect of two
users striking their mugs together while in separate physical
locations (see in Figure 1). In each local environment, the
user’s mug’s position is governed by a non-robotic tracked
physical object, and the force of the strike is simulated via a
synchronized moving proxy.

Multiple Manipulation Methods
Our system supports several methods of interacting with vir-
tual proxies via physical objects.

We support direct (one-to-one) manipulation, for comparison
purposes. We also employ a custom gesture recognition tech-
nique, enabling users to command the position of a target us-
ing hand motions. Utilizing a simple conﬁguration of tracked
markers that reports each user’s wrist position and orientation
via the OptiTrack system, users can push a nearby(proximate)
object across the table via a pushing gesture, pull a distant ob-
ject towards them with a pulling gesture, or motion in another
direction to slide the object towards another part of the table.

Retargeting in Physical Distributed Collaboration
Inspired by Synchronized Distributed Physical Objects[4], we
extend the idea and adapt it to a VR environment, using a
novel retargeting[2] system. In the ’Tic-Tac-Toe’ use case,
(ﬁgure 4c), the virtual object does not necessarily represent
the physical location of the robot. While the user is handling
the object, remote users see this exact movement. During this
operation, the remote robot invisibly moves to the predicted
location where the user will set down the virtual object.

For the Tic-Tac-Toe experience, we added "artiﬁcial" latency
to improve the user experience when synchronizing remote
actions, since the robotic proxy takes time to move into po-
sition. Given the speed of the m3pi robot and the size of the
tabletop workspace, we concluded that 1.5 seconds is ideal.
Movements by the local user of their proxy object are visually
delayed in the remote collaborator’s view to give the remotely
located robot sufﬁcient time to catch up with the local user’s
actions. This smooths the haptic operations considerably.

In our user study, we test the perception of this latency, with
positive results. When we asked users directly whether they
had experienced any latency, the answer was generally no (see
details below).

PRELIMINARY USER STUDY
The goals of our study were (1) Necessity. How do users
feel about remote physical interaction? (2) Awareness of la-
tency. How much latency do users perceive when interacting
remotely? (3) Preference. Which manipulation approach does
the user prefer? (4) Awareness of remote location of the other
participant. Does the user feel that their opponent is sitting at
the same physical table, or does it feel as though their opponent
is remotely located?

Sixteen participants took part in our study, three female. Ages
ranged from twenty-two to ﬁfty-two, and the median age was
twenty-six. All participants were right-handed. Seventy-ﬁve
percent had experienced VR prior to the study.

(a) physical
tic-tac-toe
which move controller to
make the move

(b) pure tic-tac-toe which
use only hand to make the
move

(c) table split by 9 areas
which mug is in area 2 for
example

Figure 5: Experiment Sketch

Experiment Design
We designed two tests for our user study. The ﬁrst was a
recreation of the classic board game, Tic-Tac-Toe, in VR. We
utilized a "controller" object, allowing players to select a tile
for each game step. When one player is holding the controller
and considering their next move, the other player can observe
this movement in their view as well. We included a version of
Tic-Tac-Toe that was purely virtual (no haptics) for comparison
(ﬁgure 5a and 5b).

In the second experiment, "Telekinesis" (ﬁgure 5c), we split
the virtual table into nine parts, sequentially placing a target in
each one of the subsections. Its purpose was to measure users’
ability to learn our gesture system and to use it to command
a target at various locations. Participants were ﬁrst given up
to two minutes to learn the gestures, then we observed their
command choice (including non-gestural physical interaction)
for each target position.

Procedure
We ﬁrst explained to participants the purpose of the study.
Before each task, the interaction techniques involved were
described.

Completion time and error rate were logged for each ex-
periment. In a questionnaire, the participants rated their in-
teraction using four questions from QUIS[6]: difﬁcult/easy,
dull/stimulating, never/always, too slow/fast enough. The
questionnaire also included additional questions, using a ﬁve-
step scale.

Figure 6: QUIS Question Results

Results
The results from our QUIS questions were positive (see ﬁg-
ure 6). For gap awareness (M = 3.93, SD = 1.13), 12 out of
16 (75.0%) felt the experience was fast enough. Regarding the
learning curve of system (M = 4.47, SD = 0.54), 15 out of 16
(93.8%) thought it was easy to learn. For the telekinesis exper-
iment (M = 3.47, SD = 1.00), 7 out of 16 (43.8%) thought the
interaction was straight-forward. Although this result is not
high, keep in mind that only 6 out of 16 (37.5%) scored a 3,
and (M = 4.27, SD = 0.52), 14 out of 16 (87.5%) enjoyed the
interaction.

Regarding necessity, 10 out of 16 (62.5%) preferred to play
Tic-Tac-Toe with a physical controller (ﬁgure 8). Regarding
acknowledgement of physical placement, 14 out of 16 (87.5%)
felt they were playing on the same table rather than performing
a remote task (ﬁgure 8).

Figure 7: Preference of Telekinesis Manipulation

For tiles one, two, and three (far from the player) in Figure
5, forty-three out of forty-eight (89.6%) were moved using
gestures. For tiles four, ﬁve, and six (at a medium distance
from the player), thirty-nine out of forty-eight (81.3%) were
moved using gestures. For the close tiles, including seven,
eight, and nine, thirty-three out of forty-eight (68.8%) were
moved using gestures.

Annual Symposium on User Interface Software and
Technology. ACM, 717–728.

4. Scott Brave, Hiroshi Ishii, and Andrew Dahley. 1998.

Tangible interfaces for remote collaboration and
communication. In Proceedings of the 1998 ACM
conference on Computer supported cooperative work.
ACM, 169–178.

5. Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven
Köhler, Patrick Schmidt, Robert Kovacs, Johannes Jasper,
Jonas Kemper, and Patrick Baudisch. 2015. Turkdeck:
Physical virtual reality based on people. In Proceedings
of the 28th Annual ACM Symposium on User Interface
Software & Technology. ACM, 417–426.

6. John P Chin, Virginia A Diehl, and Kent L Norman. 1988.

Development of an instrument measuring user
satisfaction of the human-computer interface. In
Proceedings of the SIGCHI conference on Human factors
in computing systems. ACM, 213–218.

7. Inrak Choi and Sean Follmer. 2016. Wolverine: A
Wearable Haptic Interface for Grasping in VR. In
Proceedings of the 29th Annual Symposium on User
Interface Software and Technology. ACM, 117–119.

8. Katherine M Everitt, Scott R Klemmer, Robert Lee, and
James A Landay. 2003. Two worlds apart: bridging the
gap between physical and virtual media for distributed
design collaboration. In Proceedings of the SIGCHI
conference on Human factors in computing systems.
ACM, 553–560.

9. Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu
Hogge, and Hiroshi Ishii. 2013. inFORM: dynamic
physical affordances and constraints through shape and
object actuation.. In Uist, Vol. 13. 417–426.

10. Steffen Gauglitz, Benjamin Nuernberger, Matthew Turk,
and Tobias Höllerer. 2014. World-stabilized annotations
and virtual scene navigation for remote collaboration. In
Proceedings of the 27th annual ACM symposium on User
interface software and technology. ACM, 449–459.

11. Hiroshi Ishii. 2008. Tangible bits: beyond pixels. In
Proceedings of the 2nd international conference on
Tangible and embedded interaction. ACM, xv–xxv.

12. Hiroshi Ishii and Brygg Ullmer. 1997. Tangible bits:
towards seamless interfaces between people, bits and
atoms. In Proceedings of the ACM SIGCHI Conference
on Human factors in computing systems. ACM, 234–241.

13. Hiroo Iwata, Hiroaki Yano, Hiroyuki Fukushima, and

Haruo Noma. 2005. CirculaFloor [locomotion interface].
IEEE Computer Graphics and Applications 25, 1 (2005),
64–67.

14. Hajime Kajita, Naoya Koizumi, and Takeshi Naemura.

2016. SkyAnchor: Optical Design for Anchoring Mid-air
Images onto Physical Objects. In Proceedings of the 29th
Annual Symposium on User Interface Software and
Technology. ACM, 415–423.

Figure 8: Necessity and Acknowledgement of placement

While most participants preferred to use gestures for distant
targets and physical interaction for proximate targets, one par-
ticipant mentioned that they preferred gestures over physical
interaction because the gestures were more fun. Two other
users both chose to use gestures to push proximate targets
away before pulling them back again. We concluded from this
that the interaction we designed is not only meaningful and
useful, but enjoyable as well.

LIMITATIONS AND FUTURE WORK
One limitation is our hardware conﬁguration. Currently, repur-
posing our robots after installation is not easy. Additionally,
our robots cannot assume dynamic shapes. For remote col-
laboration, we would like to try a more sophisticated task, in
order to determine how tasks with varying levels of challenge
require different methods of design.

CONCLUSION
In this paper, we proposed a new approach for interaction in
virtual reality via robotic haptic proxies, speciﬁcally targeted
towards collaborative experiences, both remote and local. We
presented several prototypes utilizing our three mapping deﬁ-
nitions, demonstrating that robotic proxies can be temporarily
assigned to represent different virtual objects, that our system
can allow remotely located users to have the experience of
touching on the same virtual object, and that users can alter-
nately use gestures to command objects without touching them.
Our preliminary experiments returned positive results. In the
future we plan to undertake a more comprehensive study focus-
ing on deeper application interactions and expanded hardware
capability.

REFERENCES
1. Bruno Araujo, Ricardo Jota, Varun Perumal, Jia Xian Yao,
Karan Singh, and Daniel Wigdor. 2016. Snake Charmer:
Physically Enabling Virtual Objects. In Proceedings of
the TEI’16: Tenth International Conference on Tangible,
Embedded, and Embodied Interaction. ACM, 218–226.

2. Mahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal
Ofek, and Andrew D Wilson. 2016. Haptic retargeting:
Dynamic repurposing of passive haptics for enhanced
virtual reality experiences. In Proceedings of the 2016
CHI Conference on Human Factors in Computing
Systems. ACM, 1968–1979.

3. Hrvoje Benko, Christian Holz, Mike Sinclair, and Eyal

Ofek. 2016. NormalTouch and TextureTouch:
High-ﬁdelity 3D Haptic Shape Rendering on Handheld
Virtual Reality Controllers. In Proceedings of the 29th

15. Mathieu Le Goc, Lawrence H Kim, Ali Parsaei,

25. Derek Reilly, Anthony Tang, Andy Wu, Niels Mathiasen,

Jean-Daniel Fekete, Pierre Dragicevic, and Sean Follmer.
2016. Zooids: Building Blocks for Swarm User
Interfaces. In Proceedings of the 29th Annual Symposium
on User Interface Software and Technology. ACM,
97–109.

16. Daniel Leithinger, Sean Follmer, Alex Olwal, and

Hiroshi Ishii. 2014. Physical telepresence: shape capture
and display for embodied, computer-mediated remote
collaboration. In Proceedings of the 27th annual ACM
symposium on User interface software and technology.
ACM, 461–470.

17. Daniel Leithinger, Sean Follmer, Alex Olwal, and

Hiroshi Ishii. 2015. Shape displays: Spatial interaction
with dynamic physical form. IEEE computer graphics
and applications 35, 5 (2015), 5–11.

18. Yuan-Yi Liao, Li-Ren Chou, Ta-Jyh Horng, Yan-Yang

Luo, Kuu-Young Young, and Shun-Feng Su. 2000. Force
reﬂection and manipulation for a VR-based telerobotic
system. PROCEEDINGS-NATIONAL SCIENCE
COUNCIL REPUBLIC OF CHINA PART A PHYSICAL
SCIENCE AND ENGINEERING 24, 5 (2000), 382–389.

19. William A McNeely. 1993. Robotic graphics: A new

approach to force feedback for virtual reality. In Virtual
Reality Annual International Symposium, 1993., 1993
IEEE. IEEE, 336–341.

20. Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello,
Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David
Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou,
and others. 2016. Holoportation: Virtual 3D Teleportation
in Real-time. In Proceedings of the 29th Annual
Symposium on User Interface Software and Technology.
ACM, 741–754.

21. Mai Otsuki, Kenji Sugihara, Asako Kimura, Fumihisa
Shibata, and Hideyuki Tamura. 2010. MAI painting
brush: an interactive device that realizes the feeling of
real painting. In Proceedings of the 23nd annual ACM
symposium on User interface software and technology.
ACM, 97–100.

22. Esben Warming Pedersen and Kasper Hornbæk. 2011a.
Tangible bots: interaction with active tangibles in
tabletop interfaces. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems.
ACM, 2975–2984.

23. Esben Warming Pedersen and Kasper Hornbæk. 2011b.
Tangible Bots: Interaction with Active Tangibles in
Tabletop Interfaces. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems
(CHI ’11). ACM, New York, NY, USA, 2975–2984. DOI:
http://dx.doi.org/10.1145/1978942.1979384

24. Ramesh Raskar, Greg Welch, Matt Cutts, Adam Lake,

Lev Stesin, and Henry Fuchs. 1998. The ofﬁce of the
future: A uniﬁed approach to image-based modeling and
spatially immersive displays. In Proceedings of the 25th
annual conference on Computer graphics and interactive
techniques. ACM, 179–188.

Andy Echenique, Jonathan Massey, Hafez Rouzati, and
Shashank Chamoli. 2011. Toward a Framework for
Prototyping Physical Interfaces in Multiplayer Gaming:
TwinSpace Experiences. Springer Berlin Heidelberg,
Berlin, Heidelberg, 428–431. DOI:
http://dx.doi.org/10.1007/978-3-642-24500-8_58

26. Derek F Reilly, Hafez Rouzati, Andy Wu, Jee Yeon

Hwang, Jeremy Brudvik, and W Keith Edwards. 2010.
TwinSpace: an infrastructure for cross-reality team
spaces. In Proceedings of the 23nd annual ACM
symposium on User interface software and technology.
ACM, 119–128.

27. Jan Richter, Bruce H Thomas, Maki Sugimoto, and
Masahiko Inami. 2007. Remote active tangible
interactions. In Proceedings of the 1st international
conference on Tangible and embedded interaction. ACM,
39–42.

28. Eckard Riedenklau, Thomas Hermann, and Helge Ritter.

2012. An integrated multi-modal actuated tangible user
interface for distributed collaborative planning. In
Proceedings of the Sixth International Conference on
Tangible, Embedded and Embodied Interaction. ACM,
169–174.

29. Dan Rosenfeld, Michael Zawadzki, Jeremi Sudol, and
Ken Perlin. 2004. Physical objects as bidirectional user
interface elements. IEEE Computer Graphics and
Applications 24, 1 (2004), 44–49.

30. Misha Sra. 2016. Asymmetric Design Approach and
Collision Avoidance Techniques For Room-scale
Multiplayer Virtual Reality. In Proceedings of the 29th
Annual Symposium on User Interface Software and
Technology. ACM, 29–32.

31. Misha Sra, Dhruv Jain, Arthur Pitzer Caetano, Andres
Calvo, Erwin Hilton, and Chris Schmandt. 2016.
Resolving Spatial Variation And Allowing Spectator
Participation In Multiplayer VR. In Proceedings of the
29th Annual Symposium on User Interface Software and
Technology. ACM, 221–222.

32. Misha Sra and Chris Schmandt. 2015. Metaspace:

Full-body tracking for immersive multiperson virtual
reality. In Adjunct Proceedings of the 28th Annual ACM
Symposium on User Interface Software & Technology.
ACM, 47–48.

33. Kenji Sugihara, Mai Otsuki, Asako Kimura, Fumihisa
Shibata, and Hideyuki Tamura. 2011. MAI Painting
Brush++: Augmenting the feeling of painting with new
visual and tactile feedback mechanisms. In Proceedings
of the 24th annual ACM symposium adjunct on User
interface software and technology. ACM, 13–14.

34. Yasuyoshi Yokokohji, Ralph L Hollis, and Takeo Kanade.
1996. What you can see is what you can feel-development
of a visual/haptic interface to virtual environment. In
Virtual Reality Annual International Symposium, 1996.,
Proceedings of the IEEE 1996. IEEE, 46–53.

