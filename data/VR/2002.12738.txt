Human-like Planning for Reaching in Cluttered Environments

Mohamed Hasan1, Matthew Warburton2, Wisdom C. Agboh1, Mehmet R. Dogar1,
Matteo Leonetti1, He Wang1,3, Faisal Mushtaq2,3, Mark Mon-Williams2,3 and Anthony G. Cohn1

0
2
0
2

r
a

M
3

]

O
R
.
s
c
[

2
v
8
3
7
2
1
.
2
0
0
2
:
v
i
X
r
a

Abstract— Humans,

in comparison to robots, are remark-
ably adept at reaching for objects in cluttered environments.
The best existing robot planners are based on random sam-
pling of conﬁguration space- which becomes excessively high-
dimensional with large number of objects. Consequently, most
planners often fail to efﬁciently ﬁnd object manipulation plans
in such environments. We addressed this problem by identifying
high-level manipulation plans in humans, and transferring these
skills to robot planners. We used virtual reality to capture
human participants reaching for a target object on a tabletop
cluttered with obstacles. From this, we devised a qualitative
representation of the task space to abstract the decision making,
irrespective of the number of obstacles. Based on this repre-
sentation, human demonstrations were segmented and used to
train decision classiﬁers. Using these classiﬁers, our planner
produced a list of waypoints in task space. These waypoints
provided a high-level plan, which could be transferred to an
arbitrary robot model and used to initialise a local trajectory
optimiser. We evaluated this approach through testing on
unseen human VR data, a physics-based robot simulation, and
a real robot (dataset and code are publicly available1). We
found that the human-like planner outperformed a state-of-the-
art standard trajectory optimisation algorithm, and was able
to generate effective strategies for rapid planning- irrespective
of the number of obstacles in the environment.

I. INTRODUCTION

Imagine grasping a yoghurt tub from the back of a cluttered
fridge shelf. For humans, this is a trivial task and one that
even young children are able to perform exquisitely. Yet,
there are a number of non-trivial questions that a robot must
resolve to execute the same action with a similar ease. Should
the robot try to navigate through the available free space? Or
should it start by moving obstacles? In which case, which
obstacle should be moved ﬁrst and to where?

Standard robot motion planning approaches focus on
identifying a collision-free trajectory that satisﬁes a set of
given constraints [1], and the majority of current planning
techniques are based on random sampling of the conﬁguration
space [2], [3], [4], [5]. A deﬁning feature of these sampling-
based planners (SBPs) is the use of a set of probing samples
drawn to uniformly cover the state space. To accelerate
the planning process, it is desirable to devise non-uniform
sampling strategies that favour sampling in regions where
an optimal solution might lie [6]. Finding such regions is
non-trivial but, as we highlighted earlier, humans are able to
ﬁnd near-optimal solutions very quickly.

1School of Computing, University of Leeds, UK.
2School of Psychology, University of Leeds, UK.
3Centre for Immersive Technologies, University of Leeds, UK.
1https://github.com/m-hasan-n/hlp.git

Fig. 1: Overview of the HLP approach.

Predicated on human expertise, imitation learning from
demonstration (LfD)
techniques are increasingly being
adopted by researchers for robot motion planning [7], [8] [9],
[10]. For example, researchers have demonstrated the use of
neural networks for learning the dynamics of arm motion
from human data [11], whilst others have shown the utility of
combining planning and learning-based approaches to facili-
tate goal-directed behaviour during human-robot interactions
[12]. Alternative approaches to utilising human data include
learning qualitative task representations. Evidence indicates
that humans recognise and interact with spatio-temporal
space in a more qualitative than quantitative manner [13],
[14], [15]. Previous work has therefore integrated qualitative
spatial representations (QSRs) with manipulation planning
at different levels [16], [17]. Importantly, these approaches
avoid the pitfalls of SPBs which only allow a small number
of objects due to the curse of dimensionality [18], [19], [20].
We propose a novel approach to the problem of reaching
through a cluttered environment based on geometric reasoning
and workspace partitioning [1] (similar to cell decomposition)
augmented by QSRs [14]. To achieve this, we collected data
from human participants for our scenario using virtual reality
(VR)[21]. Demonstrations were segmented based on a spatial
model of the task space. This segmentation resulted in a set of
state-action pairs used to train classiﬁers via standard machine
learning techniques. These trained classiﬁers are used by a
human-like planner (HLP) which, during testing, generates a
high-level plan in task space. Finally, the generated high-level
plan is forwarded to a robot-speciﬁc low-level controller for
inverse kinematics (IK) and local optimisation if required. The
resulting method is a human-like planning (HLP) algorithm
for reaching towards objects in cluttered environments (see
Fig. 1 for an overview of this framework).

Our novel contributions include:

• The development of a new high-level planning algorithm

that learns from humans operating within VR

• A new qualitative space/action representation to mitigate

 
 
 
 
 
 
Initial scene

Moving an object

Going through a gap

Approaching the target

Fig. 2: Sample keyframes in a VR trial.

the problem of high dimensionality

• Empirical demonstrations of the utility of this high-level
planner - showing that it is scalable and can work in
conjunction with any existing low-level planner in a
seamless manner.

II. AN OVERVIEW OF THE HLP

i , ak

labelled by a set of associated actions ak

Given a cluttered environment ξ = {X s, X t, X o

i } repre-
sented by a start position X s, a target position X t and the
positions of N movable objects X o
i , i = 1, ..., N , we consider
the problem of planning a high-level path ρ from X s to X t
that is most likely to be selected by a human. A high-level
plan ρ = {X k
i } is represented by a set of M keypoints
X k
i , i = 1, ..., M .
i
Our framework (Fig. 1) starts by learning from human
demonstrations collected through VR. Each demonstration is
given the form of an environment representation ξ in addition
to an associated human trajectory τ h. This framework runs
in two phases, training and testing. In the training phase
(green arrow path of Fig. 1): given pairs of {ξtrain, τ h},
demonstrations are segmented into abstracted state-action
pairs {S, A} that are used to train decision classiﬁers. In
the testing phase, given ξtest of a new scene, the HLP uses
the trained decision models to generate ρ. The generated
high-level plan can then be transferred to an arbitrary robot
model. A robot-speciﬁc low-level controller solves for IK
and performs local optimisation if required.

Demonstration segmentation is based on our modelling of
the task space in order to extract required state-action pairs.
Modelling of the task space is detailed in Sec. IV. Feature
extraction and decision classiﬁers are explained in Sec. V
whilst the planning algorithm used in the testing phase is
given in Sec. VI. Demonstration of the transfer from high-
level planing to robot control is provided in the Experiments
section.

III. VR DATASET COLLECTION
A dataset2 of human demonstrations was collected by
asking 24 participants to perform 90 reaching trials towards
objects placed on a cluttered table top in a virtual environment
(Fig. 2). The table top surface was surrounded by transparent
screens from all but the front side and the work space
dimensions were tailored to suit adult human arm movements.
The position and rotation of the hand, elbow and upper arm
(nearest point to shoulder) were tracked and sampled at 90Hz.

2https://doi.org/10.5518/780

Participants were asked to initiate each trial by ﬁrst moving
towards a home position, which was indicated by a transparent
pink ball. After onset, a target end-point (a pink-coloured can)
appeared, along with a total of six obstacles placed in two
rows. Participants were instructed to interact with the scene
to pick up the can and bring it back to the home position.
Participants could achieve this by either navigating around
the obstacles or picking up and moving the obstacles to a new
position. Trials were failed and restarted if: (i) any part of
the arm interacted with the obstacles; (ii) any moved object
hit the edge of the workspace; (iii) the can touched any of
the obstacles; (iv) participants knocked over an obstacle with
their hand.

IV. MODELLING THE TASK SPACE

Devising a model of the task space that enables learning
from demonstrations and generalising with different environ-
ment settings was of critical importance. To this end, we
designed the row-based structure shown in Fig. 2 and Fig. 3,
with two rows each containing three objects and aligned
start-target positions. This structured environment helped
model the task space and train the decision learners. All VR
demonstrations used for training were generated according
to this structure and we show in the Experiments section
how the HLP was able to generalise to different environment
conﬁgurations.

We model the environment task space ξ qualitatively as
a union of three disjoint sets: ξo representing the occupied
space, while the free space is given by the union of gaps ξg
and connecting space ξc sets. Occupied space represents all
objects in the scene in addition to walls3 of the surface
on which objects rest. A gap gi ∈ ξg is a qualitative
representation of the free space existing between two objects
oj and ok, j (cid:54)= k on the same row. The connecting space
models the remaining free space used to connect from the
start to the ﬁrst row, between rows, and from the second row
to the target.

Based on this representation of the task space, the action
space A is discretized into two primitive actions: Ao moving
an object and Ag going through a gap (Fig. 2). A primitive
action occurs at a keypoint in the ξo ∪ ξg space, i.e. an action
applies to an object or a gap at a speciﬁc row. Actions at
different rows are connected through the connecting space
ξc.

3A set of four virtual objects are used to represent top, left, right and
bottom walls and added to the occupied space to avoid objects slipping off
the surface.

Keypoints at objects and gaps are conceptually analogous
to the idea of keyframe-based learning from demonstration
[22]. They are also comparable to nodes in RRTs and PRMs
[23], [19]. Keypoints are locally connected in ξc in a similar
manner to interpolating between keyframes or connecting
graph/tree nodes.

The high-level plan is generated hierarchically at three
levels: path, segment and action. At the top level, a path ρ
comprises a sequence of consecutive segments. Each segment
is a qualitative description of a subset of ξc in which the
planner (connects) interpolates between two consecutive
actions. Finally, an action is one of the two primitive actions
in A. For example, in an environment structured in NR rows,
a path consists of NR-1 segments. Each segment connects
between a pair of consecutive rows. One action takes place
at a row and applies to a gap or an object. Start and target
points are directly connected to the actions at the ﬁrst and
last rows respectively.

V. DECISION CLASSIFIERS

Decision learners are used to locally capture rules un-
derlying human planning skills. We design such learners
as classiﬁers that map from state (feature) domain X to
action (class) domain Y . Training examples are drawn from
a distribution D over X × Y where the goal of each classiﬁer
[24], [25], [26] is ﬁnding a mapping C : X (cid:55)→ Y that
minimises prediction errors under D. The classiﬁer output
can be represented by the posterior probability P (Y |X).

In this section, we introduce the features extraction required
for decision classiﬁers. Features4 are extracted based on the
spatial representations used by humans to recognise the task
space. We heavily use relational spatial representations such
as distance, size, orientation and overlap. In our working
scenario, there are four main decisions during a plan: (1)
which gap to go through?; (2) which object to move?; (3)
which direction should the object be moved in?; (4) which
segment should connect two consecutive actions? We designed
our learners as binary classiﬁers with the exception of ’object-
moving direction’.

A. Gap and Object Classiﬁers (Cg, Co)

These classiﬁers have a binary output deﬁning the prob-
ability of either selecting or rejecting a gap or an object.
Intuitively, humans prefer gaps close to their hand, of an
adequate size, leading to the target, and not requiring acute
motions. Hence, the features vector Xg input to gap classiﬁer
Cg is deﬁned by the distances from the gap to initial dxg,xs
and target dxg,xt positions, gap (size) diagonal lg, orientation
of the gap-to-start θg,s and gap-to-target θg,t lines.

Xg = [dxg,xs , dxg,xt, lg, θg,i, θg,t]T

(1)

Similarly, object features are given by distances dxo,xs and
dxo,xt, object diagonal lo, the orientation angles θo,s, θo,t
in addition to the object’s overlap with the target lo,t and a
measure of the free space area around the object aof s .

4Features are appropriately normalised to relevant factors such as the table

size, object area, number of objects and arm dimensions.

Fig. 3: Modelling the task space. A 2D projection is shown
for a structure deﬁned by two rows each with three objects
(hence four gaps), start location, and target object. Space
around an object is discretized into eight direction classes.
The size of each direction block depends on the object size
and a scaling factor α.

Xo = [dxo,xs, dxo,xt, lo, θo,i, θo,t, lo,t, aof s ]T

(2)

Space around a given object is discretized into a set ∆ of
eight classes (forward FF, forward left FL, left LL, back left
BL, back BB, back right BR, right RR and forward right FR)
covering the object’s neighborhood (Fig. 3). The size of each
direction block depends on the object size and an arbitrary
scaling factor. The free space in each block is computed and
aof s is given by the sum of free space in the eight blocks.

B. Object Direction Classiﬁer (Cd)

If the action is to move an object, we learn appropriate
moving direction from the human data. To estimate the
moving direction, an object is described by the direction
of the human hand hd ∈ ∆ when approaching the object,
orientation of an object-to-target line θo,t and the amount
of free space af s in each surrounding direction around the
object. This is a multi-class classiﬁer whose output Yd = ∆.
Xd = [hd, θo,t, af s]T
C. Connecting Segments Classiﬁer (Cc)

(3)

Each segment connects two actions applied on gaps and/or
objects. Let e1 and e2 denote the elements representing these
gaps/objects at the ﬁrst and second rows respectively. It is
desirable to avoid actions that are considerably apart from
each other, having no or small overlap with the target and
expected to result in a large collision. Hence, a segment
feature vector consists of the signed horizontal d(e1,e2)x
and
distances, overlap lc,t between d(e1,e2)x
vertical d(e1,e2)y
and
the target object, segment orientation θc w.r.t a straight line
connecting initial and target positions, and the collision cζ
expected to accompany motion through this segment.

Segment collision is computed as the overall overlapping
volume (area) between human forearm (link) and ξo. The
overlapping area is approximated by the intersection polygon
area and found by scanning the surrounding area using
sampling lines.5

Xc = [d(e1,e2)x

, d(e1,e2)y
5Although there are standard graphics polygon clipping algorithms to solve
this kind of problem, we preferred an approximate but faster line-sampling
approach.

, lc,t, θccs]T

(4)

VI. HLP ALGORITHM

as the one with least likely collision.

The HLP algorithm uses the trained classiﬁer models
explained in the previous section to generate the high-level
plan in the testing phase. The algorithm starts by locating
rows Ri, i = 1, ..., NR and gaps ξg in the scene.

For each i-th row, object and gap classiﬁers are called
(Lines 2-3) to identify the best Nio objects and Nig gaps
respectively. Selected gaps and objects in the (i)-th row
are connected to their counterparts in the next (i + 1)-th
row through connecting segments. Selecting a total Ng =
(cid:80)i+1
j=i Njo objects in a pair of
consecutive rows results in NgNo segment combinations.
These combinations are constructed and passed to the segment
classiﬁer that selects the best Nc connecting segments (Lines
4-6).

j=i Njg gaps and No = (cid:80)i+1

Algorithm 1 The Human-Like Planner (HLP)
Input: Environment representation ξ = {X s, X t, X o
i }
Output: High-level path ρ

Locate rows R and gaps ξg

1: for all R do
2:

Compute gaps feature vector Xg
Gselected ← Cg(Xg)
Compute objects feature vector Xo
Oselected ← Co(X o
i )

3: end for
4: for all pairs of consecutive rows do
5:

C ← Segment Constructor (Gselected, Oselected)
Compute segments feature vector Xc
Cselected ← Cc(Xc)

6: end for
7: for all Cselected do
8:
9:
10:

if ao ∈ Cselected then

Compute object-direction feature vector Xd
Object direction = Cd(Xd)
Augment Cselected by expected object’s location

11:

end if
Compute arm conﬁguration feature vector Xa
Estimate arm conﬁguration: Ra(Xa)
Compute expected path collision ρζ

12: end for

Select the path with minimum collision score

A segment connects two actions, one of which may belong
to the moving-object class. Hence, the object moving direction
is estimated by the Cd classiﬁer using the same convention of
space discretization in Sec. V-B. The expected object location
after moving is found and added to the segment’s sequence
of keypoints (Lines 7-11).

Next, the human-arm conﬁguration is estimated (see next
section) at each keypoint, and estimated conﬁgurations are
used to evaluate the overall expected collision of each segment
(Lines 11-12). To this end, we get candidate segments between
rows that are labelled by a measure of their overall expected
collision. For a two-row structure, the best segment is picked

A. Arm Conﬁguration and Collision Detection

In the 2-row structure, a candidate path has one segment
that is connected to the start and target points. Having a
number of candidate paths, the algorithm selects the one with
minimum overall expected collision. Overall expected path
collision is found by computing collision of full-arm motion
with ξo between path keypoints. Therefore, arm conﬁgurations
are ﬁrstly estimated at each keypoint and then expected arm
collision is computed.

A human arm is modeled as a planar arm with four joints
at neck, shoulder, elbow and hand. The arm conﬁguration is
represented by two angles: θsh between neck-shoulder and
upper arm links, and θel between upper arm and forearm links.
The arm conﬁguration at a given keypoint Kt is estimated
by regression. Input features to the conﬁguration regression
model Ra are: hand direction hd ∈ ∆ when approaching Kt,
arm conﬁguration at the previous keypoint Kt−1 and signed
horizontal and vertical distances between the two keypoints.

Xa = [hd, θsht−1 , θelt−1 , d(k1,k2)x

, d(k1,k2)y

]T

(5)

Through estimating arm conﬁgurations at keypoints (and
hence joint positions), full arm collision is computed as the
sum of its link collisions during motion along keypoints from
the start to target. The collision of each link is computed
using the same approach as in Sec. V-C. Intersections between
sampling lines and objects are found and the area of the
resulting intersection polygon deﬁnes the collision amount.
To this end, a number of candidate paths are generated, each
labelled with a measure of its overall expected collision
ρζ. This step completes the HLP algorithm and generates a
plan deﬁned by the path having minimum expected overall
collision.

VII. EXPERIMENTS AND RESULTS

The HLP approach was tested through three protocols:
human VR data, robot simulation and real-robot experiments.
The VR dataset was randomly split into two disjoint sets:
approximately 80% (19 participants and 1710 trials) for
training, and 20% (5 participants and 450 trials) for testing.
This cross-participant splitting, (i.e. no overlap between
training and testing data in terms of participants) allowed
the generalisation of the proposed approach
us to test
with participants who had not been seen in training. This
splitting was repeated for ﬁve folds of randomly selected
participants for both sets. Standard (MATLAB) support
vector machines with Gaussian kernel and Gaussian process
regression were used to implement classiﬁers and regression
models respectively. The same trained models and planning
algorithm were used for all experiment protocols.

A. Experiments on VR Data

This protocol tested the HLP on unseen, but similarly
distributed data to the training set. The objective was to
measure the similarity of the generated plans to the “ground

Fig. 4: A human-like manipulation plan for reaching a target amongst 8 obstacles. HLP is shown to generalize to different
conﬁgurations in terms of table size, number and shape of objects.

truth” human plans. This similarity was computed for a scene
having NR rows by the following metric:

sHLP =

1
2NR

NR(cid:88)

n=1

I(Dn)(I(Dn) + I(En))

(6)

where I(.) is the indicator function which is 1 when its
argument is true and 0 otherwise, Dn is a Boolean variable
that is true if HLP action is same as human action at the
n-th row, and En is a Boolean variable that is true if the
same element (gap or object) is selected by both HLP and
human action. To illustrate, if both the HLP and the human
participant decided to move an obstacle in the ﬁrst row and
then go through a gap in the second row, then this would be
quantiﬁed as a 50% similarity rate. This could increase to
100% if both selected the same obstacle and the same gap.
Mean and standard deviation of the 5-fold results are shown
in Table I. Accuracy of the gap and object classiﬁers are 95%
and 85% respectively. It is worth noting that we compared
similarity of the HLP output to a speciﬁc participant’s plan
at a time and then reported the mean similarity. This means
that HLP similarity is counted if it exactly matches a speciﬁc
testing participant who was never seen during training. On
average, our planner was 70% similar to the test participant
plans. More speciﬁcally, the HLP selected the same action as
a human 79% of the time, while it selected the same element
(speciﬁc gap or object) 67% of the time.

B. Robot Experiments

Through robot experiments, we compared the HLP with
a standard physics-based stochastic trajectory optimisation
(STO) approach [27], [28], [29]. These algorithms were
implemented using the Mujoco [30] physics engine and the
Deepmind control suite [31]. We assumed a world consisting
of objects on a table with a 10-DOF robot as shown in Fig.
4. As a baseline algorithm, we used a state-of-the-art physics-
based stochastic trajectory optimiser [27], initialised with a

TABLE I: Results (mean and standard deviation) of the 5-fold
VR test experiment.

Metric
Cg accuracy
Co accuracy
sHLP (overall)
sHLP (I(Dn))
sHLP (I(En))

Mean
0.95
0.85
0.70
0.79
0.67

STD
0.002
0.005
0.011
0.016
0.012

TABLE II: Generalisation I Simulation Scene Results.

Success rate(%)
94
84

Init. time(s)
0.59
0.04

Opt. time(s)
0.97
17.84

Total time(s)
1.56
17.88

HLP
STO

straight line control sequence from the end-effector’s start
pose to the target object.

IK solutions for the high-level plan keypoints were found
and connected with straight lines in the robot’s conﬁguration
space to generate a control sequence. This was passed
to a trajectory optimiser as an initial candidate control
sequence. Thus, for the HLP, the number of actions in a
given control sequence varied depending on the high level
plan. In contrast, the baseline approach (STO) initialised the
trajectory optimiser using a straight line control sequence to
the goal.

We compared performance of the HLP and STO quantita-
tively through success rates and planning times of simulation
experiments, and qualitatively through visual inspection of
manipulation plans in real robot experiments.

1) Robot Simulation Experiments: Simulation experiments
evaluated the performance of our planner in scenes generated
from a distribution considerably different to that of the training
data. Generalisation was investigated across different numbers,
sizes and types of objects. A plan was classiﬁed as being
successful if the target object was inside the gripper’s pre-
grasp region at the ﬁnal state, and if no other object dropped
off the table during the trial. We recorded the total planning
time for successful trials of each planner. Planning time
comprised an initial time used to generate the high level
plan (straight line for ST O) and an optimisation time. Two
simulation categories were considered:

Generalisation I– Performance of the HLP was tested in
simulation scenes with the same two-row structure used in the
VR demonstrations. Here, the generalisation element involved
varying the dimensions of the table and objects. Table II
summarises results of 100 scenes from this category. The
HLP substantially outperformed the STO by a large margin,
indexed through success rates and a reduction in planning
time.

Generalisation II– Our second test involved investigating
how our approach would fare when generalising to different
numbers of objects, different types of objects, and different
environment settings. We considered conditions with 5, 7
and 9 objects with two shape classes: cylinders and boxes.

)

X
1
(
P
L
H

)

.

X
5
1
(
O
T
S

Fig. 5: A human-like manipulation plan (Top; HLP), and a baseline plan (Bottom; STO). HLP can reason over task space
and navigate through the available free space. On the other hand, STO is biased towards its straight-line initialization and
thus had to push objects around.

TABLE III: Planning times (mean) for Generalisation II.
The main planning time of HLP (Init. time) is almost ﬁxed
irrespective of the number of obstacles.

No. of Objects

5
7
9

Init. time(s)
0.60
0.61
0.63

HLP

Opt. time(s)
1.70
2.65
5.90

Total(s)
2.30
3.26
6.53

STO
Total(s)
1.85
3.68
4.98

Importantly, the start and target positions were no longer
aligned in this experiment. 100 randomly sampled scenes were
generated for each object-number condition. For each random
scene, we selected a random6 initial robot conﬁguration.

Success rates for 100 random scenes for each of the
three object categories were computed. The rates for both
planners were relatively similar, (93%, 93% and 95%) for
HLP and (96%, 96% and 98%) for STO for 5, 7 and 9 objects
respectively. Planning time comparisons are given in Table III.
The time required for generating a high-level plan by HLP
(Init. Time) is ﬁxed irrespective of the number of objects.

2) Real Robot Experiments: We used a Robotiq two-ﬁnger
gripper attached to a UR5 arm mounted on an omnidirectional
robot (Ridgeback). Object positions were obtained using a
depth camera mounted above the table. We conducted a
limited number of experiments - 4 sample scenes per 7, 9
and 11 objects. We then ran the HLP and the STO, producing
a total of 24 robot runs7. Sample results are shown in Fig. 5
where the HLP favoured going through the free space avoiding
obstacles, while the STO was biased to its initialised straight-
line path. Other scenes are presented in the accompanying
video8.

6We uniformly sampled a start point along the front edge of the table, and
a corresponding random end-effector orientation (in the plane) and found
the initial robot conﬁguration using inverse kinematics

74 scenes, 3 number-of-object categories, and 2 methods.
8https://youtu.be/aMIZP_SYa0I

VIII. DISCUSSION

This work speaks to the potential of human-like computing
i.e. the endowment of systems with capabilities derived
from modelling human perception, cognition and action.
Humans are known to recognise the spatio-temporal world
in a qualitative manner. Thus, instead of cloning the human
behaviour from demonstrations, we used QSR in order to
segment demonstrations in the action space. We have shown
that extracting human skills at such a high level helps to
model a planner that can: (1) generalise to different numbers
of objects without increasing the actual planning time; and
(2) seamlessly connect with an arbitrary robot model.

Many cluttered environments can be clustered into regions
that geometrically approximate to our deﬁnition of rows. For
an arbitrary number of rows, the HLP can be run recursively
for row pairs by deﬁning a set of sub-targets. Moreover,
generalisation may be improved by augmenting training data
with more generalised scenes, using more powerful classiﬁers,
and running in a closed-loop manner. Finally, we note that
further gains for this human-like approach may be made
through a more detailed mechanistic understanding of the
processes underlying human planning and execution. These
topics will be addressed in our future work.

IX. CONCLUSIONS

We used VR to collect data from human participants whilst
they reached for objects on a cluttered table-top. From these
data, we devised a qualitative representation of the task space
to segment demonstrations into keypoints in the action space.
State-action pairs were used to train decision classiﬁers that
constituted the building blocks of the HLP algorithm. VR,
robot simulation and real robot experiments tested against a
state-of-the-art planner has shown that this HLP is able to
generate effective strategies for planning, irrespective of the
number of objects in a cluttered environment. We conclude
that the optimisation of robot planning has much to gain by
extracting features from human action selection and execution.

[19] R. Geraerts and M. H. Overmars, “A comparative study of probabilistic
roadmap planners,” in Algorithmic Foundations of Robotics V. Springer,
2004, pp. 43–57.

[20] S. Karaman, M. R. Walter, A. Perez, E. Frazzoli, and S. Teller, “Anytime
motion planning using the rrt,” in 2011 IEEE International Conference
on Robotics and Automation.

IEEE, 2011, pp. 1478–1483.

[21] J. Brookes, M. Warburton, M. Alghadier, M. Mon-Williams, and
F. Mushtaq, “Studying human behavior with virtual reality: The unity
experiment framework,” Behavior research methods, pp. 1–9, 2019.

[22] B. Akgun, M. Cakmak, K. Jiang, and A. L. Thomaz, “Keyframe-based
learning from demonstration,” International Journal of Social Robotics,
vol. 4, no. 4, pp. 343–355, 2012.

[23] A. Bry and N. Roy, “Rapidly-exploring random belief trees for motion
planning under uncertainty,” in 2011 IEEE international conference
on robotics and automation.

IEEE, 2011, pp. 723–730.

[24] B. Moldovan, P. Moreno, D. Nitti, J. Santos-Victor, and L. De Raedt,
“Relational affordances for multiple-object manipulation,” Autonomous
Robots, vol. 42, no. 1, pp. 19–44, 2018.

[25] E. Alpaydin, Introduction to machine learning. MIT press, 2009.
[26] J. Langford and B. Zadrozny, “Reducing t-step reinforcement learning

to classiﬁcation,” 2003.

[27] W. C. Agboh and M. R. Dogar, “Pushing fast and slow: Task-
adaptive planning for non-prehensile manipulation under uncertainty,”
in Workshop on the Algorithmic Foundations of Robotics (WAFR),
2018.

[28] W. C. Agboh and M. R. Dogar, “Real-time online re-planning for
grasping under clutter and uncertainty,” in IEEE-RAS 18th International
Conference on Humanoid Robots (Humanoids), 2018.

[29] W. C. Agboh, D. Ruprecht, and M. R. Dogar, “Combining coarse
and ﬁne physics for manipulation using parallel-in-time integration,”
International Symposium on Robotics Research (ISRR), 2019.
[30] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for

model-based control,” in IROS.

IEEE, 2012.

[31] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de Las Casas,
J. Merel, A. Lefrancq, T. P.
“Deepmind control
suite,”
[Online]. Available: http:

D. Budden, A. Abdolmaleki,
Lillicrap,
CoRR, vol.
//arxiv.org/abs/1801.00690

and M. A. Riedmiller,
abs/1801.00690, 2018.

ACKNOWLEDGMENTS

This work is funded by the EPSRC (EP/R031193/1) under
their Human Like Computing initiative. The 3rd author was
supported by an EPSRC studentship (1879668) and the EU
Horizon 2020 research and innovation programme under grant
agreement 825619 (AI4EU). The 7th, 8th and 9th authors are
supported by Fellowships from the Alan Turing Institute.

REFERENCES

[1] J.-C. Latombe, Robot motion planning. Springer Science & Business

Media, 2012, vol. 124.

[2] J. E. King, J. A. Haustein, S. Srinivasa, and T. Asfour, “Nonprehensile
whole arm rearrangement planning on physics manifolds,” in ICRA,
2015.

[3] M. Moll, L. Kavraki, J. Rosell et al., “Randomized physics-based
motion planning for grasping in cluttered and uncertain environments,”
IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 712–719,
2017.

[4] S. Karaman and E. Frazzoli, “Sampling-based algorithms for optimal
motion planning,” The international journal of robotics research,
vol. 30, no. 7, pp. 846–894, 2011.

[5] M. Elbanhawi and M. Simic, “Sampling-based robot motion planning:

A review,” Ieee access, vol. 2, pp. 56–77, 2014.

[6] B. Ichter, J. Harrison, and M. Pavone, “Learning sampling distributions
for robot motion planning,” in 2018 IEEE International Conference
on Robotics and Automation (ICRA).

IEEE, 2018, pp. 7087–7094.

[7] R. Fox, R. Berenstein, I. Stoica, and K. Goldberg, “Multi-task
hierarchical imitation learning for home automation,” in 2019 IEEE
15th International Conference on Automation Science and Engineering
(CASE).

IEEE, 2019, pp. 1–8.

[8] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg,
and P. Abbeel, “Deep imitation learning for complex manipulation
tasks from virtual reality teleoperation,” in 2018 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2018, pp.
1–8.

[9] M. Rana, M. Mukadam, S. R. Ahmadzadeh, S. Chernova, and
B. Boots, “Towards robust skill generalization: Unifying learning from
demonstration and motion planning,” in Intelligent robots and systems,
2018.

[10] H. Tan and K. Kawamura, “A computational framework for integrating
robotic exploration and human demonstration in imitation learning,” in
2011 IEEE International Conference on Systems, Man, and Cybernetics.
IEEE, 2011, pp. 2501–2506.

[11] H. Ravichandar and A. Dani, “Learning contracting nonlinear dynamics
from human demonstration for robot motion planning,” in ASME 2015
Dynamic Systems and Control Conference. American Society of
Mechanical Engineers Digital Collection, 2016.

[12] M. Lawitzky, J. R. Medina, D. Lee, and S. Hirche, “Feedback motion
planning and learning from demonstration in physical robotic assistance:
differences and synergies,” in 2012 IEEE/RSJ International Conference
on Intelligent Robots and Systems.

IEEE, 2012, pp. 3646–3652.

[13] J. O. Wallgr¨un, F. Dylla, A. Klippel, and J. Yang, “Understanding
human spatial conceptualizations to improve applications of qualita-
tive spatial calculi,” in 27th International Workshop on Qualitative
Reasoning, 2013, p. 131.

[14] J. Chen, A. G. Cohn, D. Liu, S. Wang, J. Ouyang, and Q. Yu, “A survey
of qualitative spatial representations,” The Knowledge Engineering
Review, vol. 30, no. 1, pp. 106–136, 2015.

[15] A. G. Cohn, J. Renz, and M. Sridhar, “Thinking inside the box: A
comprehensive spatial representation for video analysis,” in Thirteenth
International Conference on the Principles of Knowledge Representa-
tion and Reasoning, 2012.

[16] M. Mansouri and F. Pecora, “More knowledge on the table: Planning
with space, time and resources for robots,” in 2014 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2014, pp.
647–654.

[17] M. Westphal, C. Dornhege, S. W¨olﬂ, M. Gissler, and B. Nebel, “Guiding
the generation of manipulation plans by qualitative spatial reasoning,”
Spatial Cognition & Computation, vol. 11, no. 1, pp. 75–102, 2011.
[18] I. A. S¸ ucan and L. E. Kavraki, “A sampling-based tree planner for
systems with complex dynamics,” IEEE Transactions on Robotics,
vol. 28, no. 1, pp. 116–131, 2011.

