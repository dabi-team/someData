initial arXiv draft 

Enhancing User Experience in Virtual Reality with Radial Basis 
Function Interpolation Based Stereoscopic Camera  Control 

Emre Avan, Ufuk Celikcan, Tolga K. Capin, and Hasmet Gurcay 

Fig.  1:  An  overview  of  our  approach.  Clockwise  from  the  top  left:  (1)  The  user  creates  a  unique  design  of  stereoscopic 
cinematography, which is composed of a series of waypoint locations with designated camera angles and stereoscopic camera 
parameters. (2) Then, the method calculates a smooth camera path and interpolates the stereoscopic camera parameters for the rest 
of the scene using radial basis functions. (3) Finally, during the VR experience, automated stereoscopic camera control is realized 
on the path by projection matrix manipulations using the fitted surfaces of parameters. 

Abstract— Providing a depth-rich Virtual Reality (VR) experience to users without causing discomfort remains to be a challenge with 
today’s commercially available head-mounted displays (HMDs), which enforce strict measures on stereoscopic camera parameters for 
the sake of keeping visual discomfort to a minimum. However, these measures often lead to an unimpressive VR experience with 
shallow depth feeling. We propose the first method ready to be used with existing consumer HMDs for automated stereoscopic camera 
control in virtual environments (VEs). Using radial basis function interpolation and projection matrix manipulations, our method makes 
it possible to significantly enhance user experience in terms of overall perceived depth while maintaining visual discomfort on a par with 
the default arrangement. In our implementation, we also introduce the first immersive interface for authoring a unique 3D stereoscopic 
cinematography for any VE to be experienced with consumer HMDs. We conducted a user study that demonstrates the benefits of our 
approach in terms of superior picture quality and perceived depth. We also investigated the effects of using depth of field (DoF) in 
combination with our approach and observed that the addition of our DoF implementation was seen as a degraded experience, if not 
similar. 

Index Terms—Virtual Reality, Stereoscopy 

1 INTRODUCTION AND MOTIVATION 
In recent years, Virtual Reality (VR) has become more commonplace 
with recent advances in hardware technology that have led to the pro- 
duction of consumer appropriate head-mounted displays (HMDs), such 
as HTC Vive and Oculus Rift. A wide field-of-view (FOV) HMD that 
immerses a user in computer-generated virtual worlds is a key enabling 

•  Emre Avan is with Hacettepe University, Department of Computer Graphics. 

E-mail: emreavan@gmail.com. 

•  Ufuk Celikcan is with Hacettepe University, Department of Computer 

Engineering. E-mail:  ufuk.celikcan@gmail.com. 

•  Tolga K. Capin is with TED University, Department of Computer 

Engineering. E-mail: tolga.capin@tedu.edu.tr. 

•  Hasmet Gurcay is with Hacettepe University, Department of Mathematics. 

E-mail: gurcay@hacettepe.edu.tr. 

technology to VR applications. 

The immersive nature of HMDs creates a strong presence illusion, 
where users perceive virtual environments (VEs) as real and not medi- 
ated through technology. Therefore, it finds a myriad of applications in 
gaming, entertainment, simulation and training, defense, education, and 
other fields. Support from the  makers of the commercially available 
HMDs with extensive software development kits has resulted in an un- 
seen and rapidly expanding ecosystem of such applications specifically 
designed for VR. As VR has been increasingly accessible and popu- 
lar, comfortable, high-quality stereo 3D has become an important and 
timely requirement for real-time VR applications. There are still issues 
that remain to be resolved in order to provide a thoroughly realistic and 
comfortable experience to VR users. Immersive VEs can be visually 
constraining. The foremost contributing factor to visual discomfort is 
the accommodation-convergence conflict (ACC), which arises due to 

1 

 
 
 
 
 
the dissonance between accommodation, adjustment of the eye lenses 
to focus at the observed depth, and eye convergence [21, 25, 34, 39]. 
While these two cues are cross coupled in normal viewing conditions, 
in stereoscopic displays, the viewer always focuses at the screen level 
regardless of where eyes actually converge, which leads to ACC. Due to 
the ensuing discomfort, users commonly report symptoms such as eye- 
strain, nausea, dizziness and headaches after using HMDs for extended 
periods. Enhancing user experience and perceived depth together with- 
out  invoking  discomfort  has  been  a  major  challenge  in  stereoscopic 
content production. 

To  create  stereo  vision  for  VR  applications,  there  are  two  main 
parameters that need to be set. These are interaxial separation, which is 
the distance between the two cameras, and their convergence distance 
in  the  VE.  These  stereoscopic  camera  parameters  play  a  major  role  
in the VR experience as they produce the disparity between left and 
right images, and therefore impact the amount of perceived depth [16], 
as  well  as  visual  comfort.  Nonetheless,  no  single  setting  exists  that 
minimizes the fusion effort and leads to optimized depth perception for 
varying viewing circumstances and depth ranges [24]. 

In current commercially available HMDs, stereo camera parameters 
are set to be fixed at the API level, such that, they are kept constant no 
matter how the scene contents change or the depth composition of the 
scene varies. This is a convenient solution to avoid issues regarding 
visual discomfort, however it reduces the depth perceived, and, in turn, 
level of immersion of the users. 

In this work, we aim to enhance user experience in VR with con- 
sumer HMDs in terms of overall perceived depth without sacrificing 
picture quality or visual comfort. Addressing the challenges of chang- 
ing depth composition dynamically while maintaining visual discomfort 
to a minimum, we propose a new method for automated stereoscopic 
camera control in VEs. To the best of our knowledge, this is the first 
such method ready to be used with existing consumer HMDs without 
additional  hardware  requirements  such  as  embedded  eye  trackers  or 
focus-adjustable lenses. 

Our proposed method demands a stereoscopic cinematography, that 
is, a particular arrangement of stereoscopic camera settings consisting 
of a series of waypoint locations with designated camera angles and 
stereoscopic  camera  parameters.  The  stereoscopic  parameters  asso- 
ciated  with  these  locations  constitute  a  set  of  scattered  data.  Using 
radial basis function interpolation on this small set of data, our method 
produces a smooth surface fit of parameters for the rest of a given VE 
and  provides  automated  stereoscopic  camera  control  by  continuous 
projection matrix manipulations using the fitted parameters. 

We also introduce a VR interface for creating the required stereo- 
scopic cinematography. Our survey of the relevant literature indicates 
that this is the first immersive interface that can readily be used with all 
commercially available HMDs for authoring a unique 3D stereoscopic 
cinematography. With the proposed interface, users can author unique 
depth narratives for any given VE directly from the first-person per- 
spective exactly as the VE will be experienced using the same HMD 
that it will be experienced with. While it can be used by VR content 
creators to design signature depth narratives, the easy-to-use interface 
lets even the most novice users to quickly create their own unique VR 
experiences. 

As the highly subjective nature of stereoscopic camera control neces- 
sitates, we evaluated our method in comparison to the default HMD set- 
tings in different configurations. The results illustrate that our method 
is  able  to  significantly  enhance  user  experience  in  terms  of  overall 
perceived  depth  while  also  boosting  picture  quality  and  maintaining 
visual discomfort on a par with the default arrangement. We also an- 
alyzed  whether  using  Depth-of-Field  blur  in  addition  to our  method 
improves  user  experience  further  and  found  that,  as  in  some  similar 
earlier studies, the addition of our Depth-of-Field blur implementation 
was seen as a degraded experience notably in terms of visual comfort 
and similar in terms of depth perception. 

The remainder of this paper is organized as follows. In Sections 2 
and 3, we briefly give some background information on the components 
of our work and present an overview of previous works on the subject 
matter.  Our  proposed  method  is  elaborated  in  Section  4.  Then,  in 

2 

Fig. 2: Cross coupling between accommodation and convergence is 
broken in stereoscopic display systems such as HMDs. 

Section 5,  the details  of  our user  evaluation  study  are  given  and  its 
results are illustrated and discussed. Finally, Section 6 concludes the 
paper. 

2 BACKGROUND 
2.1 Stereo Vision and Stereoscopic Rendering 
Interaxial  separation  and  zero-parallax  distance  are  the  two  major 
parameters in stereoscopic rendering. For the binocular vision, there 
are  two  virtual  cameras  rendering  the  scene  with  a  slight  distance 
between each other. This distance is called interaxial separation and 
lets  the  human  visual  system  to  create  depth  perception.  Secondly, 
oculomotor  muscles  enable  eyes  to  converge  into  a  plane  which  is 
called zero-parallax plane, also known as convergence plane, and the 
distance  between  this  plane  and  the  cameras  is  called  zero-parallax 
distance. 

If  the  object  is  far  behind  the  zero-parallax  plane,  it  has  positive 
disparity  and  appears  as  inside  the  display.    For  an  object  that  is  at     
a closer distance than the zero-parallax plane,  it appears as in front  
of  the  display  and  this  situation  is  called  negative  disparity.  As  the 
zero-parallax  distance  gets  closer  to  the  virtual  cameras,  image  for 
the left eye shifted to left and right for the right eye  for the HMDs. 
For the objects that have negative disparity, fatigue may occur more 
easily than the ones with positive disparity since oculomotor muscle 
needs to contract in order to rotate eyes inwards to focus with negative 
disparity.  Therefore,  developers  and  designers  carefully  control  the 
scene  composition  and  cinematography  in  order  to  keep  users’  eyes 
rested as much as possible [13]. 

When looking at the screen, viewer’s eyes converge or diverge ac- 
cording to the depth of the object in the scene while they are focused 
on the display. In real-world, accommodation and convergence systems 
are cross-coupled, which means that eyes both converge and accommo- 
date at the same position as seen in the Fig 2. Crystalline lens of the 
eye make accommodation possible by refracting the light inwards or 
outwards. In stereoscopic display systems, on the other hand, coupling 
between accommodation and convergence is broken causing ACC. 

In order  to  solve  this  issue  which  can be  seen  in Figure 2,  focus 
object needs to be in stereoscopic comfort zone. This zone is defined 
by Percival [31] to be 1/3 diopter distance from each side (negative 
disparity and positive disparity) to the accommodation distance. To 

Fig. 3: Difference between Asymmetric Frustum (on the left) and 
Toe-in (on the right) camera setups. 

 
VR Authoring 

Add New 
Waypoint 

Edit 
Waypoint 

Remove 
Waypoint 

VR Experience 

Camera Path 
Calculation 

Adjust Stereo 
Camera 
Parameters 

End VR Experience 

YES 

Is Path 
Completed? 

NO 

Translate and Rotate 
Stereo Camera Pair 
along the Path 

Update Stereo 
Camera Parameters 

Camera 
Path 
Traversal 

Fig. 4: Flow diagram of our RBF interpolation-based stereoscopic camera control approach. 

and eye tracking data from a video game. After finding the zero-parallax 
plane, off-center projection matrix shifts that plane into the comfort 
zone. 

While some gaze prediction-based methods have been shown to be 
effective at improving VR experience of users within some restricted 
settings, each has certain shortcomings such as limited usability, low 
accuracy or low frame rate. While equipping HMDs with eye trackers 
can  solve  most  of  these  issues  by  foveated  rendering,  they  are  not 
available  with current consumer  HMDs, with the exception of HTC 
Vive Pro Eye, which remains to be a niche product mainly due to its 
considerably  higher  price.  To  the  best  of  our  knowledge,  our  work 
presented in this paper is the first that does not require gaze detection 
or prediction and can readily be used with existing consumer HMDs 
for automated stereoscopic camera control in VEs towards enhancing 
the VR experience. 

4 RBF INTERPOLATION-BASED STEREOSCOPIC CAMERA CONTROL 

The  overview  of  our  RBF  interpolation-based  stereoscopic  camera 
control  approach  is  shown  in  Figure  4.    The  approach  consists  of  
two  parts,  namely,  VR  Authoring  and  VR  Experience.  While  VR 
Experience is the main part where the automatic camera control takes 
place,  it  requires  an  arrangement  of  stereoscopic  camera  settings,  a 
depth layout, that is created in VR Authoring. Users can promptly start 
their VR Experience with one of the preset depth layouts or use VR 
Authoring to create their own depth layouts. 

4.1 VR Authoring 

In  VR  Authoring,  the  user  designs  the  depth  narrative  of  the  VR 
experience by creating a depth layout. A depth layout is a unique design 
of stereoscopic cinematography that consists of a path for the stereo 
camera pair to follow with designated camera angles and stereoscopic 
camera parameters. A depth layout is composed of a series of waypoints. 
By placing a waypoint, the user defines a position and an angle for the 
stereo camera pair and sets the stereoscopic camera parameters. 

During authoring, the user is placed in the VE that they are creating 
a depth layout for. This way, they can tailor a depth narrative to that VE 
from the first-person perspective in the same immersive setting using 
the same display (HMD) viewers will experience it with. This aspect of 
our approach constitutes a significant improvement over the traditional 
stereoscopic editing paradigm where users are bound to work with a 
two-dimensional interface on a two-dimensional display. 

The  user  can  roam  the  VE  and  place  a  waypoint  anywhere  in  it 
freely. Movement in the VE is realized by either materially  moving 
in the physical space or virtually teleporting within the VE. With our 
editing in fist-person view paradigm, position and angle of the stereo 
camera  pair  are  controlled by  the  user  via  HMD  in  the  same  way  a 
cameraman operates their camera in live shooting. Upon locking the 
position and view angle for a waypoint, the user adjusts the stereoscopic 

4 

(a) 

(b) 

(c) 

(d) 

(e) 

Fig. 5: The graphical user interfaces in VR Authoring: (a) main menu 
(b) submenu for Edit Waypoint (c) the user is restricted to changing the 
position of an existing waypoint within a certain radius (d) close-up of 
a waypoint marker with its saved zero-parallax distance and interaxial 
separation hovering above (e) preview of a sample camera path. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
initial arXiv draft 

according to the viewer’s own IPD, the distance between the centers of 
their pupils. When the IPD is set incorrectly, perception errors and eye 
strain may follow [12]. Therefore, at the beginning of the tests, the IPD 
of a subject is measured using a digital pupilometer and set the IPD 
value of the HMD correspondingly. 

During the tests, our approach was evaluated by the subjects in pairs 
of sessions for each scene in three different configurations, as given 
below. Order of the configurations was randomized during the tests. 

5.5.1  Preset Experience vs. Default Experience 

First, the subjects experienced each of the two scenes once in a session 
with our approach via a preset depth layout (Preset Experience) and 
once in another session with Vive’s fixed stereoscopic camera parame- 
ters (Default Experience). During the Default Experience session, the 
virtual stereo camera pair follows the same path, with the same camera 
positions and viewpoints, as the one that was created in the preset depth 
layout picked for our approach. Order of sessions was randomized at 
run time and the subjects were not informed about the order. 

5.5.2  Self-Authored Experience vs. Default Experience 

The second configuration is similar to the first one, except this time, 
the subjects first used VR Authoring to create their own depth layout. 
They were asked to design two depth layouts in total, one for each VR 
scene. While the subjects were free to shape their layouts according to 
their liking, they were told to keep the threshold to a minimum of 5 for 
the number of waypoints to create within a layout. 

Afterwards, they again experienced each scene in two sessions, once 
with our approach via their self-authored depth layout (Self-Authored 
Experience) and once with the Default Experience setting, in a random 
order. 

DoF blur), via the same preset depth layout (Fig.9). Order of sessions 
was randomized again without informing the subjects. 

5.6 Evaluation Criteria 

Following each session, the subjects were asked to evaluate the session 
in terms of image quality, perceived depth and visual comfort in a 5-
point  Likert  scale  with  the  labels  “bad”,  “poor”,  “fair”,  “good”,  and 
“excellent”.  These  criteria,  which  are  detailed  below,  are  frequently 
resorted to for perceptual assessment of stereoscopic contents [38]. 

Image Quality: expresses the overall visual quality of the displayed 
content  as  perceived  by  the  user.  Since  our  approach  dynamically 
modifies  the  degree  of  horizontal  asymmetry  of  the  frusta,  proper 
fusion of the resulting left and right images and proper scaling of the 
scene contents in these images should be established by validating the 
image quality. 
Perceived Depth: denotes the apparent depth of the displayed content 
perceived  by  the  user.  With  stereoscopic  camera  control  methods 
aiming at a dynamic depth narrative, as the stereo camera parameters 
change over time, so does the perceived depth. Accordingly, providing 
a method that brings upon a feeling of realistic depth is essential for 
an  enhanced VR  experience  since  it  contributes  a  great  deal  to  user 
immersion. 
Visual Comfort: is to measure the subjective feeling of visual comfort 
of the user. Improperly set stereoscopic camera parameters cause visual 
discomfort in the form of eye strain, which can lead to adverse side 
effects  including  visual  fatigue,  nausea  and  headaches  and  yield  a 
dissatisfactory VR experience. Hence, first of all, it is vital to ensure 
that a proposed stereoscopic camera control method does not invoke 
visual distress. 

5.5.3 Preset Experience with DoF blur vs. Preset Experience 

In the last configuration, the subjects experienced the two scenes using 
our approach only, once without DoF blur (Preset Experience, as in the 
first configuration) and once with DoF blur (Preset Experience with 

Once both sessions and their respective individual assessments are 
done, the subjects were then asked to evaluate the two sessions vis-a-vis 
each other. This time, in addition to the previous three, they were also 
to make a comparison in terms of overall quality. 

Fig. 10: Individual session ratings collected with the questionnaires and their averages are given for each scene per configuration. The averages 
are indicated in rectangles. 

7 

 
 
Fig. 11: Relative user preferences collected with the questionnaires are given in percentages. Scores are relative to the compared setting. 

5.7 Results 

Figure 10 illustrates the results of the individual session evaluations for 
image quality, depth perception and visual comfort for all three configu- 
rations. The user preferences in percentages in the three configurations 
are given in Figure 11. 

For  the  first  two  configurations  where  our  approach  is  evaluated 
against the Default Experience setting, it is seen that, in both Preset 
Experience and Self-Authored Experience settings, our approach was 
rated  significantly  higher  on  average  in  terms  of  image  quality  and 
perceived depth in both scenes. 9 subjects, for both settings and both 
scenes, indicated their preference of our approach to the Default Ex- 
perience in terms of image quality. Similarly, in the outdoor scene, 7 
subjects preferred our approach to the Default Experience in terms of 
perceived depth with both the Preset and the Self-Authored Experiences. 
For the indoor scene, 7 subjects favored the Preset Experience and 9 
subjects favored the Self-Authored Experience in terms of perceived 
depth. 

In terms of visual comfort, again for the first two configurations, our 
approach was rated only slightly better in the outdoor scene with both 
settings. In the indoor scene, however, it was rated slightly worse with 
the Preset Experience and the same with the Self-Authored Experience. 
It is seen that while the Default Experience was favored in terms of 
visual comfort to the Preset Experience by 2 and 3 of the subjects in 
the  outdoor  and  indoor  scenes,  respectively,  4  and  5  of  the  subjects 
preferred  it  to  their  own  Self-Authored  Experiences  in  the  outdoor 
and  indoor  scenes,  respectively.  The  subject  with  VR  application 
development background, on the other hand, was among the group who 
found their own Self-Authored Experience more visually comforting. 
These findings imply that novice VR users may still have a hard time 
when  they  first  start  authoring VR  experiences  for  their  ideal  visual 
comfort level, however easy the interface is to grasp and use. 

The  questionnaire  item  that  queries  the  overall  preference  of  the 
subjects garnered responses demonstrating that our approach was con- 
siderably well-received with both the Preset Experience and the Self- 
Authored Experience settings. The breakdown of the results show that 
while  the  Self-Authored  Experience  was  preferred  by  more  subjects 
(9 in both scenes) than the Preset Experience (8 in both scenes), more 
subjects regarded the Preset Experience as ”much better” with respect 

to the Default (5 in the outdoor scene and 6 in the indoor scene) than 
they did the Self-Authored Experience (3 in both scenes). 

The results for the third configuration, which facilitates to evaluate 
the impact of our DoF blur implementation in combination with our 
approach, show that the combination was generally found to degrade 
both image quality and visual comfort. While, more subjects indicated 
their overall preference towards the session with DoF blur, the average 
ratings of the session without DoF blur are higher in all three evaluation 
criteria for the outdoor scene and similar for the indoor scene. 

Figure 12 presents a sample of depth charts that contrast the depth 
distributions resulting from our approach with the results of the default 
arrangement  in  the  two  scenes.  That  is,  the  pair  of  charts  for  each 
scene is obtained by following the same camera path using the default 
settings once and once using our approach. On the charts, minimum and 
maximum depth values along the camera path are given with respect 
to  the  HMD  display.  It  is  seen  that  while  the  default  arrangement 
constrains the zero-parallax plane to a fixed short distance to the stereo 
camera pair leading to a very narrow band of negative disparity region, 
our  approach  allows  a  dynamic  yet  smooth  depth  narrative  in  both 
negative and positive disparity regions. 

6 CONCLUSION 

In this paper we have described an effective method of real-time au- 
tomated stereoscopic camera control in VEs towards providing users 
with impressive VR experiences that are rich in depth. We have also 
introduced  an  immersive  design  interface  for  authoring  unique  VR 
experiences to be used with the proposed method. We believe both of 
these novel contributions that are ready to be used with existing con- 
sumer HMDs will stimulate new directions in stereoscopic rendering 
research. Moreover, we have addressed the issues with the stereoscopic 
parameter  extrema  that  may  cause  undesired  effects  and  proposed  a 
dynamic limiting scheme. 

The results of the user evaluation study demonstrate that our method 
is able to enhance user experience, as intended, especially in terms of 
overall perceived depth and image quality. The  method is also seen 
to slightly improve the visual comfort in the scene with wider depth 
range and to keep it at similar levels in the scene with narrower depth 
range. The  use  of  DoF  blur  added  to  our  proposed  method  did not 

Indoor Scene with Preset Experience 

Indoor Scene with Default 

Outdoor Scene with Preset Experience 

Outdoor Scene with Default 

40 

0 

-11 

40 

0 

-11 

151 

0 
-9 

151 

0 

-9 

Max Depth 

Min Depth 

Screen 

Max Depth 

Min Depth 

Screen 

(a) 

(b) 

Fig. 12: Relative maximum and minimum depth value chart of (a) the indoor scene, (b) the outdoor scene. 

8 

 
 
initial arXiv draft 

help to improve the experience further, since the majority of the user 
assessments were in accord with some earlier studies [5, 9, 10]. Surely, 
a further standalone study with higher number of configurations and 
larger sample size in which users can be grouped as first time VR users, 
VR  enthusiasts  and  VR  developers  would  be  beneficial  for  a  more 
thorough analysis. 

While it is observed that using our approach in an interactive setting 
where the user can roam the scene freely without being restricted to the 
camera path set in the selected depth layout lead to acceptable results, 
we believe further improvements to the method are crucial for proper 
adaptation to use in interactive VR applications. 

REFERENCES 
[1]  About  the  VIVE  controllers.  https://www.vive.com/nz/support/ 
[Online; 

vive/category_howto/about-the-controllers.html. 
accessed 30-August-2019]. 
OpenGL  Projection  Matrix.  http://www.songho.ca/opengl/gl_ 
projectionmatrix.html. [Online; accessed 30-August-2019]. 
Radial Basis Function Interpolation in 2D. http://people.sc.fsu. 
edu/˜jburkardt/cpp_src/rbf_interp_2d/rbf_interp_2d.html. 
[Online; accessed 14-January-2015]. 
R. K. Andrew J. Woods, Tom Docherty. Image distortions in stereoscopic 
video systems, 1993. doi: 10.1117/12.157041 
J. P. Brooker and P. M. Sharkey. Operator performance evaluation of con- 
trolled depth of field in a stereographically displayed virtual environment. 
In Stereoscopic Displays and Virtual Reality Systems VIII, vol. 4297, pp. 
408–417. International Society for Optics and Photonics, 2001. 
K.  Carnegie  and  T.  Rhee.    Reducing  visual  discomfort  with  hmds  us-  
ing  dynamic  depth  of  field.  Virtual  Reality  Software  and  Technology, 
September-October:34–41, 2015. 
J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. 
McCallum,  and  T.  R.  Evans.  Reconstruction  and  representation  of  3d 
objects  with  radial  basis  functions.  In  Proceedings  of  the  28th  annual 
conference on Computer graphics and interactive techniques, pp. 67–76. 
ACM, 2001. 
U.  Celikcan,  G.  Cimen,  E.  B.  Kevinc,  and  T.  Capin.  Attention-aware 
disparity control in interactive environments. The Visual Computer, 29(6- 
8):685–694, 2013. 
J.  Conti,  B.  Ozell,  E.  Paquette,  and  P.  Renaud.  Adjusting  stereoscopic 
parameters  by  evaluating  the  point  of  regard  in  a  virtual  environment. 
Computers & Graphics, 69:24–35, 2017. 
A. T. Duchowski, D. H. House, J. Gestring, R. I. Wang, K. Krejtz, I. Krejtz, 
R. Mantiuk, and B. Bazyluk.  Reducing  visual  discomfort  of  3d  stereo- 
scopic displays with gaze-contingent depth-of-field. In Proceedings of the 
ACM Symposium on Applied Perception, SAP ’14, pp. 39–46. ACM, New 
York, NY, USA, 2014. doi: 10.1145/2628257.2628259 
R. Franke. Scattered data interpolation: Tests of some methods. Mathe- 
matics of computation, 38(157):181–200, 1982. 
B. Gerschu¨ tz,  M. Fechter,  B. Schleich,  and S. Wartzack.   A review of 
requirements  and  approaches  for  realistic  visual  perception  in  virtual 
reality. In Proceedings of the Design Society: International Conference on 
Engineering Design, vol. 1, pp. 1893–1902. Cambridge University Press, 
2019. 
S.-H. Guan, Y.-C. Lai, K.-W. Chen, H.-T. Chou, and Y.-Y. Chuang. A 
tool  for  stereoscopic  parameter  setting  based  on  geometric  perceived 
depth percentage. IEEE Transactions on Circuits and Systems for Video 
Technology, 26(2):290–303, 2016. 
J.-P. Guertin and D. Nowrouzezahrai. High performance non-linear motion 
blur.  2015. 
P.  Hanhart  and  T.  Ebrahimi.  Subjective  evaluation  of  two  stereoscopic 
imaging  systems  exploiting  visual  attention  to  improve  3d  quality  of 
experience. In Stereoscopic Displays and Applications XXV, vol. 9011, p. 
90110D. International Society for Optics and Photonics, 2014. 
G. R. Jones, D. Lee, N. S. Holliman, and D. Ezra. Controlling perceived 
depth in stereoscopic images. In Stereoscopic Displays and Virtual Reality 
Systems VIII, vol. 4297, pp. 42–53. International Society for Optics and 
Photonics, 2001. 
P. Kellnhofer, P. Didyk, K. Myszkowski, M. M. Hefeeda, H.-P. Seidel, 
and W. Matusik. Gazestereo3d: Seamless disparity manipulations. ACM 
Transactions on Graphics (TOG), 35(4):68, 2016. 
G.-A. Koulieris, B. Bui, M. Banks, and G. Drettakis. Accommodation and 
comfort in head-mounted displays. 36:1–11, 07 2017. 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

9 

[27] 

[26] 

[28] 

[29] 

[20] 

[21] 

[23] 

[25] 

[22] 

[24] 

[19]  G.  A.  Koulieris,  G.  Drettakis,  D.  Cunningham,  and  K.  Mania.  Gaze 
prediction  using  machine  learning  for  dynamic  stereo  manipulation  in 
games. In 2016 IEEE Virtual Reality (VR), pp. 113–120. IEEE, 2016. 
G.  Kramida.  Resolving  the  vergence-accommodation  conflict  in  head- 
mounted  displays.  IEEE  transactions  on  visualization  and  computer 
graphics, 22(7):1912–1931, 2016. 
M.  Lambooij,  M.  Fortuin,  I.  Heynderickx,  and  W.  IJsselsteijn.  Visual 
discomfort and visual fatigue of stereoscopic displays: A review. Journal 
of Imaging Science and Technology, 53(3):30201–1, 2009. 
M. Lang, A. Hornung, O. Wang, S. Poulakos, A. Smolic, and M. Gross. 
Nonlinear disparity  mapping  for  stereoscopic 3D. ACM  Trans.  Graph., 
29(4):75:1–75:10, July 2010. 
E. Langbehn, T. Raupp, G. Bruder, F. Steinicke, B. Bolte, and M. Lappe. 
Visual  blur  in  immersive  virtual  environments:  does  depth  of  field  or 
motion blur affect distance and speed estimation? In Proceedings of the 
22nd ACM Conference on Virtual Reality Software and Technology, pp. 
241–250. ACM, 2016. 
P. Milgram and M. Kru¨ ger.  Adaptation effects in stereo due to on-line 
changes  in  camera  configuration.  In  Stereoscopic  Displays  and  Appli- 
cations III, vol. 1669, pp. 122–134. International Society for Optics and 
Photonics, 1992. 
G. Moreau. Visual immersion issues in virtual reality: A survey. In 2013 
26th Conference on Graphics, Patterns and Images Tutorials, pp. 6–14, 
Aug 2013. doi: 10.1109/SIBGRAPI-T.2013.9 
J.-y.  Noh,  D.  Fidaleo,  and  U.  Neumann.  Animated  deformations  with 
radial basis functions. In Proceedings of the ACM symposium on Virtual 
reality software and technology, pp. 166–174. ACM, 2000. 
H. Oh, J. Kim, J. Kim, T. Kim, S. Lee, and A. C. Bovik. Enhancement 
of visual comfort and sense of presence on stereoscopic 3d images. IEEE 
Transactions on Image Processing, 26(8):3789–3801, 2017. 
L. O’Hare, T. Zhang, H. T. Nefs, and P. B. Hibbard. Visual discomfort 
and depth-of-feld. i-Perception, 4:156–169, 2013. 
T. Oskam, A. Hornung, H. Bowles, K. Mitchell, and M. Gross. OSCAM- 
optimized stereoscopic camera control for interactive 3D. In SA’11 Pro- 
ceedings of the 2011 SIGGRAPH Asia Conference, vol. 30, p. 189. ACM 
New York, 2011. 
N. Padmanaban, R. Konrad, T. Stramer, E. A. Cooper, and G. Wetzstein. 
Optimizing virtual reality for all users through gaze-contingent and adap- 
tive  focus  displays.  Proceedings  of  the  National  Academy  of  Sciences, 
114(9):2183–2188, 2017. 
A.  S.  Percival.  The  relation  of  convergence  to  accommodation  and  its 
practical bearing. Ophthal. Rev., 11:313–328, 1892. 
M.  J.  Powell.  Radial  basis  functions  for  multivariable  interpolation:  a 
review. In Algorithms for approximation, pp. 143–167. Clarendon Press, 
1987. 
M. J. Powell. The theory of radial basis function approximation in 1990. 
Advances in numerical analysis, 2:105–210, 1992. 
S. K. Rushton and P. M. Riddell. Developing visual systems and exposure 
to  virtual  reality  and  stereo  displays:  some  concerns  and  speculations 
about the demands on accommodation and vergence. Applied Ergonomics, 
30(1):69 – 78, 1999. doi: 10.1016/S0003-6870(98)00044-1 
R. Salomon and J. Weissmann. Using Radial Basis Function Networks for 
Hand Gesture Recognition, pp. 37–58. Physica-Verlag HD, Heidelberg, 
2001. doi: 10.1007/978-3-7908-1826-0  2 
A. Sherstyuk and A. State. Dynamic eye convergence for head-mounted 
displays, 01 2010. 
R. Summer. The Impact of Dynamic Convergence on the Human Visual 
System  in  Head  Mounted  Displays.  PhD  thesis,  Victoria  University  of 
Wellington, 2017. 
I.  Union.  Subjective  methods  for  the  assessment  of  stereoscopic  3dtv 
systems. Recommendation ITU-R BT, 2021, 2015. 
M. Vinnikov and R. S. Allison. Gaze-contingent depth of field in realistic 
scenes:  The  user  experience.  In  Proceedings  of  the  Symposium on  Eye 
Tracking Research and Applications, ETRA ’14, pp. 119–126. ACM, New 
York, NY, USA, 2014. doi: 10.1145/2578153.2578170 
M.  Wang,  X.-J.  Zhang,  J.-B.  Liang,  S.-H.  Zhang,  and  R.  R.  Martin. 
Comfort-driven  disparity  adjustment  for  stereoscopic  video.  Computa- 
tional Visual Media, 2(1):3–17, 2016. 
F. Zilly, J. Kluger, and P. Kauff. Production rules for stereo acquisition. 
Proceedings of the IEEE, 99(4):590–606, 2011. 

[33] 

[34] 

[30] 

[31] 

[32] 

[41] 

[36] 

[35] 

[40] 

[39] 

[38] 

[37] 

 
