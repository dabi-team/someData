1
2
0
2

p
e
S
2
2

]

G
L
.
s
c
[

3
v
9
4
9
8
0
.
2
0
0
2
:
v
i
X
r
a

Improving Sampling Accuracy of Stochastic Gradient
MCMC Methods via Non-uniform Subsampling of Gradients

Ruilin Li1, Xin Wang2, Hongyuan Zha3, and Molei Tao∗1

1Georgia Institute of Technology
2Google Inc.
3The Chinese University of Hong Kong, Shenzhen

Abstract

Many Markov Chain Monte Carlo (MCMC) methods leverage gradient informa-
tion of the potential function of target distribution to explore sample space eﬃciently.
However, computing gradients can often be computationally expensive for large scale
applications, such as those in contemporary machine learning. Stochastic Gradi-
ent (SG-)MCMC methods approximate gradients by stochastic ones, commonly via
uniformly subsampled data points, and achieve improved computational eﬃciency,
however at the price of introducing sampling error. We propose a non-uniform sub-
sampling scheme to improve the sampling accuracy. The proposed exponentially
weighted stochastic gradient (EWSG) is designed so that a non-uniform-SG-MCMC
method mimics the statistical behavior of a batch-gradient-MCMC method, and
hence the inaccuracy due to SG approximation is reduced. EWSG diﬀers from
classical variance reduction (VR) techniques as it focuses on the entire distribu-
tion instead of just the variance; nevertheless, its reduced local variance is also
proved. EWSG can also be viewed as an extension of the importance sampling
idea, successful for stochastic-gradient-based optimizations, to sampling tasks. In
our practical implementation of EWSG, the non-uniform subsampling is performed
eﬃciently via a Metropolis-Hastings chain on the data index, which is coupled to the
MCMC algorithm. Numerical experiments are provided, not only to demonstrate
EWSG’s eﬀectiveness, but also to guide hyperparameter choices, and validate our
non-asymptotic global error bound despite of approximations in the implementation.
Notably, while statistical accuracy is improved, convergence speed can be compara-
ble to the uniform version, which renders EWSG a practical alternative to VR (but
EWSG and VR can be combined too).

1

Introduction

Consider the construction of algorithms that sample a target probability distribution
π ∼ Z−1ρ(x)dx, where Z is a normalization constant and the unnormalized density ρ

∗mtao@gatech.edu

1

 
 
 
 
 
 
is assumed to be nonzero on the domain. Let V (x) := − log ρ(x) and then the target
density can be rewritten in the form of Gibbs distribution, i.e. Z−1 exp(−V (x)), where
V will be referred to as the potential function.

For this purpose, many MCMC algorithms use physics-inspired evolution such as
Langevin dynamics [Brooks et al., 2011] to utilize gradient information (i.e., ∇V ) in order
to eﬃciently explore the target distribution over continuous parameter space. However,
gradient-based MCMC methods are often limited by the computational cost of evaluating
the gradient on large data sets, which often correspond to speciﬁc potentials of the form
V (x) = (cid:80)n
i=1 Vi(x), where n is very large; this type of additive potential with many
terms will be the setup of this paper.

Motivated by the great success of stochastic gradient methods for optimization, which
uses a stochastic estimator of the batch gradient ∇V instead of evaluating all ∇Vi terms,
stochastic gradient MCMC methods (SG-MCMC) for sampling have also been gaining
increasing attention. More precisely, when the accurate but expensive-to-evaluate batch
gradients in a MCMC method are replaced by computationally cheaper estimates based
on a subset of the data, the method is turned to a stochastic gradient version. Classical
examples include SG (overdamped) Langevin Dynamics [Welling and Teh, 2011] and SG
Hamiltonian Monte Carlo [Chen et al., 2014], both designed for scalability suitable for
machine learning tasks.

However, directly replacing the batch gradient by a (uniform) stochastic one without
additional mitigation generally causes a MCMC method to sample from a statistical
distribution diﬀerent from the target, because the transition kernel of the MCMC method
gets corrupted by the noise of subsampled gradient.
In general, the additional noise
is tolerable if the learning rate/step size is tiny or decreasing. However, when large
steps are used for better eﬃciency, the extra noise is non-negligible and undermines the
performance of downstream applications such as Bayesian inference.

In this paper, we present a state-dependent non-uniform SG-MCMC algorithm termed
Exponentially Weighted Stochastic Gradients method (EWSG), which continues the ef-
forts of uniform SG-MCMC methods for scalable sampling. Our approach is based on
designing the transition kernel of a SG-MCMC method to approximate the transition
kernel of a full-gradient-based MCMC method. This approximation leads to non-uniform
(in fact, exponential) weights that aim at capturing the entire state-variable distribu-
tion of the full-gradient-based MCMC method, rather than providing unbiased gradient
estimator and reducing its variance. Nevertheless, if focusing on the variance, the ad-
vantage of EWSG is the following: recall the stochasticity of a SG-MCMC method can
be decomposed into the intrinsic randomness of MCMC and the extrinsic randomness
introduced by gradient subsampling; in conventional uniform subsampling treatments,
the latter randomness is independent of the former, and thus when they are coupled to-
gether, variances add up; EWSG, on the other hand, dynamically chooses the weight of
each datum according to the current state of the MCMC, and thus the variances do not
add up due to dependence. However, the gained accuracy is beyond reduced variance, as
EWSG, when converged, samples from a distribution close to the invariant distribution
of the full-gradient MCMC method (which has no variance contributed by the extrinsic

2

randomness), because its transition kernel (of the corresponding Markov process) is close
to that of the full-gradient-MCMC method. This is how better sampling accuracy can
be achieved.

Our main demonstration of EWSG is based on 2nd-order Langevin equations (a.k.a.
inertial, kinetic, or underdamped Langevin), although it works for other MCMC meth-
ods too (e.g., Appendix E,F). To concentrate on the role of non-uniform SG weights,
we will work with constant step sizes only. The fact that EWSG has locally reduced
variance than its uniform counterpart is rigorously shown in Theorem 2. Furthermore, a
global non-asymptotic error analysis is given in Theorem 3 to quantify the convergence
and improved accuracy of EWSG, as well as to provide insights about hyperparameter
choices.

Practically, the non-uniform gradient subsampling of EWSG is eﬃciently imple-
mented via a Metropolis-Hastings chain over the data index. A number of experiments on
synthetic and real world data sets, across downstream tasks including Bayesian logistic
regression and Bayesian neural networks, are conducted to demonstrate the eﬀectiveness
of EWSG and validate our theoretical results, despite the approximation used in the im-
plementation. In addition to improved accuracy, the convergence speed was empirically
observed, in a fair comparison setup based on the same data pass, to be comparable to its
uniform counterpart when hyper-parameters are appropriately chosen. The convergence
(per data pass) was also seen to be clearly faster than a classical Variance Reduction
for sampling, not optimization), and EWSG hence provides a
(VR) approach (note:
useful alternative to VR. Additional theoretical study of EWSG convergence speed is
provided in Appendix H.

Notation-wise, ∇V will be referred to as the full/batch-gradient, n∇VI with random
I ∈ [n], which is a statistical estimator of ∇V , will be called stochastic gradient (SG),
and when I is uniformly distributed it will be called a uniform SG/subsampling, oth-
erwise non-uniform. When uniform SG is used to approximate the batch-gradient in
underdamped Langevin, the method will be referred to as (vanilla) Stochastic Gradient
Underdamped Langevin Dynamics (SGULD/SGHMC1), and it serves as a baseline in
experiments.

2 Related Works

Stochastic Gradient MCMC Methods (SG-MCMC) Based on approximating
gradients by uniformly subsampled ones, stochastic gradient methods are computation-
ally more favorable than their full gradient counterparts and have been widely studied
and used in the ﬁeld of optimization. Inspired by the great success of stochastic gradient
methods in optimization, people also have also applied stochastic gradient methods to
sampling problems. Since the seminal work of Stochastic Gradient Langevin Dynamics
(SGLD) [Welling and Teh, 2011], much progress [Ahn et al., 2012, Patterson and Teh,

1SGULD is the same as the well-known SGHMC with ˆB = 0, see eq. (13) and Sec. 3.3 in Chen
et al. [2014] for details. To be consistent with existing literature, we will refer SGULD as SGHMC in
the sequel.

3

2013] has been made in the ﬁeld of SG-MCMC. Teh et al. [2016] theoretically justiﬁed
the convergence of SGLD and oﬀered practical guidance on tuning step size. Li et al.
[2016] introduced a preconditioner and improved stability of SGLD. We also refer to
Maclaurin and Adams [2015] and Fu and Zhang [2017] which will be discussed in Sec. 5.
While these work were mostly based on 1st-order (overdamped) Langevin, other dynam-
ics were considered too. For instance, Chen et al. [2014] proposed Stochastic Gradient
Hamiltonian Monte Carlo (SGHMC), which is closely related to 2nd-order Langevin dy-
namics [Bou-Rabee and Sanz-Serna, 2018, Bou-Rabee et al., 2018], and Ma et al. [2015]
put it in a more general framework. 2nd-order Langevin was recently shown to be faster
than the 1st-order version in appropriate setups [Cheng et al., 2018b,a, Li et al., 2021]
and began to gain more attention.

Variance Reduction (VR) For optimization, vanilla SG methods usually ﬁnd ap-
proximate solutions quickly but the convergence slows down (due to variance) when an
accurate solution is needed [Bach, 2013, Johnson and Zhang, 2013]. SAG [Schmidt et al.,
2017] improved the convergence speed of stochastic gradient methods to linear, which is
the same as gradient descent methods with full gradient, at the expense of large memory
overhead. SVRG [Johnson and Zhang, 2013] successfully reduced this memory overhead.
SAGA [Defazio et al., 2014] furthers improved convergence speed over SAG and SVRG.
For sampling, Dubey et al. [2016] applied VR techniques to SGLD (see also [Baker et al.,
2019, Chatterji et al., 2018]). However, many VR methods have large memory overhead
and/or periodically use the whole data set for gradient estimation calibration, and hence
can be resource-demanding.

EWSG is derived based on matching transition kernels of MCMC and improves the
accuracy of the entire distribution rather than just the variance. However, it does have a
consequence of variance reduction and thus can be implicitly regarded as a VR method.
When compared to the classic work on VR for SG-MCMC [Dubey et al., 2016], EWSG
converges faster when the same amount of data pass is used, although its sampling
accuracy is below that of VR for Gaussian targets (but well above vanilla SG; see Sec.
5.1).
In this sense, EWSG and VR suit diﬀerent application domains: EWSG can
replace vanilla SG for tasks in which the priority is speed and then accuracy, as it keeps
the speed but improves the accuracy; on the other hand, VR remains to be the heavy
weapon for accuracy-demanding scenarios.
Importantly, EWSG, as a generic way to
improve SG-MCMC methods, can be combined with VR too (e.g., Sec. F); thus, they
are not exclusive or competing with each other.

Importance Sampling (IS)
IS methods employ nonuniform weights to improve the
convergence speed of stochastic gradient methods for optimization. Traditional IS meth-
ods use ﬁxed weights that do not change along iterations, and the weight computation
requires prior information of gradient terms, e.g., Lipschitz constant of the gradient
[Needell et al., 2014, Schmidt et al., 2015, Csiba and Richt´arik, 2018], which are usually
unknown or diﬃcult to estimate. Adaptive IS was also proposed in which the impor-
tance was re-evaluated at each iteration, whose computation usually required the entire

4

data set per iteration and may also require information like the upper bound of gradient
[Zhao and Zhang, 2015, Zhu, 2016].

For sampling, it is not easy to combine IS with SG [Fu and Zhang, 2017]; the same
paper is, to our knowledge, the closest to this goal and will be compared with in Sec. 5.3.
EWSG can be viewed as a way to combine (adaptive) IS with SG for eﬃcient sampling. It
require no oracle about the gradient, nor any evaluation over the full data set. Instead,
an inner-loop Metropolis chain maintains a random index that approximates a state-
dependent non-uniform distribution (i.e. the weights/importance).

Other Mini-batch MCMC Methods Besides SG-MCMC methods, there are also
many non-gradient-based MCMC methods that use only a subset of data in each iteration
so that the MCMC methods can scale to large data sets. For example, austerity MH
[Korattikara et al., 2014] formulates Metropolis-Hastings step as a statistical hypothesis
testing problem and proposes to use only a subset of data to make statistically signiﬁcant
accept/reject decision. Using a subsampled unbiased estimator of the likelihood in a
pseudo-marginal framework to accelerate the Metropolis-Hastings algorithm is proposed
in Bardenet et al. [2017]. A notable exact MCMC method is FlyMC Maclaurin and
Adams [2015], which introduces an auxiliary binary random variable for each datum
and only the subset of data whose corresponding auxiliary binary indicator ”light” up,
are used in iteration. Some more recent advances on exact MCMC methods include
Zhang and De Sa [2019], Zhang et al. [2020]. We also refer to Bardenet et al. [2017] for
an excellent review on subsampling MCMC methods.

3 Underdamped Langevin: the continuous time backbone

of a MCMC method

Underdamped Langevin Dynamics (ULD) is given by the SDE

(cid:40)

dθ = rdt
dr = −(∇V (θ) + γr)dt + σdW

(1)

where θ, r ∈ Rd are state and momentum variables, V is a potential energy function
which in our context is, as originated from cost minimization or Bayesian inference over
many data, the sum of many terms V (θ) = (cid:80)n
i=1 Vi(θ), γ is a friction coeﬃcient, σ
is intrinsic noise amplitude, and W is a standard d-dimensional Wiener process. Un-
der mild assumptions on V , Langevin dynamics admits a unique invariant distribution
π(θ, r) ∼ exp
and is in many cases geometric ergodic [Pavliotis,
2014]. T is the temperature of system determined via the ﬂuctuation dissipation theo-
rem σ2 = 2γT [Kubo, 1966].

(cid:17)
T (V (θ) + (cid:107)r(cid:107)2
2 )

− 1

(cid:16)

We consider ULD instead of the overdamped version mainly for two reasons: (i) one
may think ULD is more complicated, and we’d like to show it is still easy to be paired with
EWSG (EWSG can work for many MCMC methods; Appendix E has an overdamped

5

version); (ii) it is believed that ULD has faster convergence than overdamped Langevin
for instance in high-dimensions where (local) condition number is likely to be larger
(e.g., Cheng et al. [2018b,a], Tao and Ohsawa [2020]). Like the overdamped version,
numerical integrators for ULD with well captured statistical properties of the continuous
process have been extensively investigated (e.g, Roberts et al. [1996], Bou-Rabee and
Owhadi [2010]), and both the overdamped and underdamped integrators are friendly to
derivations that will allow us to obtain explicit expressions of the non-uniform weights.

4 Method

4.1 Motivation: An Illustration of Non-optimality of Uniform Sub-

sampling

Uniform subsampling of gradients have long been the dominant way of stochastic gradi-
ent approximations mainly because it is intuitive, unbiased and easy to implement.

However, uniform gradient subsampling can introduce large noise, and is sub-optimal
even in the family of unbiased stochastic gradient estimator, as the following Theorem 1
will show. One intuition is, consider for example cases where data size n is larger than
dimension d. In such cases, {∇Vi}i=1,2,··· ,n ⊂ Rd are linearly dependent and hence it
is likely that there exist probability distributions {pi}i=1,2,··· ,n other than the uniform
one such that the gradient estimate is unbiased, however with smaller variance because
linearly dependent terms need not to be all used. This is a motivation for us to develop
non-uniform subsampling schemes (weights may be θ dependent), although we will not
require n > d later.

Theorem 1 Suppose given θ ∈ Rd, the errors of SG approximation bi = n∇Vi(θ) −
∇V (θ), 1 ≤ i ≤ n are i.i.d. absolutely continuous random vectors with possibly-θ-
dependent density p(·|θ) and n > d. We call p ∈ Rn a sparse vector if the number
of non-zero entries in p is no greater than d + 1, i.e. (cid:107)θ(cid:107)0 ≤ d + 1. Then with probability
1, the optimal probability distribution p(cid:63) that is unbiased and minimizes the trace of the
covariance of n∇VI (θ), i.e. p(cid:63) which solves the following, is a sparse vector.

min
p

Tr(EI∼p[bI bT

I ])

s.t. EI∼p[bI ] = 0,

(2)

Despite the sparsity of p(cid:63), which seemingly suggests one only needs at most d + 1
gradient terms per iteration when using SG methods, it is not practical because p(cid:63)
requires solving the linear programming problem (2) in Theorem 1, for which an entire
data pass is needed. Nevertheless, this result motivates us to seek alternatives to uniform
SG. For example, the EWSG method we will develop will have reduced local variance
with high probability, and at the same time remain eﬃciently implementable without
having to use all data per parameter update; it can be biased though, but a global error
analysis (Thm.3) will show that trading bias for variance can still be worthy.

6

4.2 Exponentially Weighted Stochastic Gradient

MCMC methods are characterized by their transition kernels. In traditional SG-MCMC
methods, uniform SG is used, which is independent of the intrinsic randomness of MCMC
methods (e.g. diﬀusion in ULD), as a result, the transition kernel of SG-MCMC is quite
diﬀerent from that with full gradient. Therefore, it is natural to ask — is it possible
to couple the two originally independent randomness, so that the transition kernel of
the SG-MCMC better matches that of the batch-gradient-MCMC, and the sampling
accuracy is thus improved?

Here is one way to do so. Consider Euler-Maruyama (EM) discretization2 of Eq. (1):

(cid:40)

θk+1 = θk + rkh
rk+1 = rk − (∇V (θk) + γrk)h + σ

√

hξk+1

(3)

where h is step size and ξk+1’s are i.i.d. d-dimensional standard Gaussian random vari-
ables. Denote the transition kernel of EM discretization with full gradient by P EM (θk+1, rk+1|θk, rk).

Then consider a SG version: replace ∇V (θk) by a weighted SG n∇VIk (θk), where
Ik is the index chosen to approximate full gradient and has p.m.f. P(Ik = i|θk, rk) = pi.
Denote the new transition kernel by ˜P EM (θk+1, rk+1|θk, rk).

It is not hard to see that

P EM (θk+1, rk+1|θk, rk)
(cid:18)

=1{θk+rkh}(θk+1)

=1{θk+rkh}(θk+1)

and

1
Z
1
Z

exp

−

(cid:18)

exp

−

(cid:107)rk+1 − rk + (∇V (θk) + γrk)h(cid:107)2
2σ2h
(cid:19)

(cid:107)x + (cid:80)n
i=1 ai(cid:107)2
2

(cid:19)

˜P EM (θk+1, rk+1|θk, rk) = 1{θk+rkh}(θk+1)

1
˜Z

n
(cid:88)

j=1

(cid:18)

pi exp

−

(cid:107)x + nai(cid:107)2
2

(cid:19)

,

where Z and ˜Z are normalization constants, x (cid:44) rk+1−rk+hγrk
From these two expressions, one can see that if we could choose

√

h

σ

and ai (cid:44)

√

h∇Vi(θk)
σ

.

(cid:18)

pi ∝ exp

−

(cid:107)x + (cid:80)n
i=1 ai(cid:107)2
2

+

(cid:107)x + nai(cid:107)2
2

(cid:19)

,

(4)

we would have P EM (θk+1, rk+1|θk, rk) = ˜P EM (θk+1, rk+1|θk, rk) and be able to recover
the transition kernel of full gradient with that of stochastic gradient. However, Eq.(4)
is only formal and infeasible, because x is dependent on the future state variable rk+1

2EM is not the most accurate or robust discretization, see e.g., [Roberts et al., 1996, Bou-Rabee and
Owhadi, 2010], but since it may still be the most used method, demonstrations here will be based on
EM. The same idea of EWSG can easily apply to most other discretizations such as GLA [Bou-Rabee
and Owhadi, 2010].

7

which we do not know. Therefore, to obtain a practically implementable algorithm, we
will ﬁx x as a hyper-parameter and hope that the approximation is good enough so that
we still have P EM (θk+1, rk+1|θk, rk) ≈ ˜P EM (θk+1, rk+1|θk, rk).

We refer to the choice of pi in eq.4 Exponentially Weighted Stochastic Gradient
(EWSG). Unlike Thm.1, EWSG does not require n > d to work. Note the idea of
designing non-uniform weights of SG-MCMC to match the transition kernel of full gra-
dient can be suitably applied to a wide class of gradient-based MCMC methods; for
example, Sec. E shows how EWSG can be applied to Langevin Monte Carlo (over-
damped Langevin), and Sec. F shows how it can be combined with VR. Therefore,
EWSG complements a wide range of SG-MCMC methods.

Since the weight choice of EWSG is motivated by approximating the transition ker-
nel of a full-gradient MCMC method, we anticipate EWSG to be statistically more
accurate than a uniformly-subsampled stochastic gradient estimator. As a special but
commonly interested accuracy measure, the smaller variance of EWSG is shown with
high probability3:

Theorem 2 Assume {∇Vi(θ)}i=1,2,··· ,n are i.i.d random vectors and |∇Vi(θ)| ≤ R
for some constant R almost surely. Denote the uniform distribution over [n] by pU ,
the exponentially weighted distribution by pE, and let ∆ = Tr[covI∼pE [n∇VI (θ)|θ] −
h), we have E[∆] < 0, and ∃C > 0 independent of n
covI∼pU [n∇VI (θ)|θ]]. If x = O(
or h such that ∀(cid:15) > 0,

√

P(|∆ − E[∆]| ≥ (cid:15)) ≤ 2 exp

(cid:18)

−

(cid:15)2
nCh2

(cid:19)

.

It is not surprising that less non-intrinsic local variance correlates with better global
statistical accuracy, which will be made explicit and rigorous in the next subsection.

4.3 Non-asymptotic Error Bound

We now establish a non-asymptotic global sampling error bound (in mean square distance
between arbitrary test observables) of SG underdamped Langevin algorithms (the bound
applies to both EWSG and other methods e.g., SGHMC). The full proof is deferred to
the Appendix C, but the main tool we will be using is the Poisson equation machinery
[Mattingly et al., 2010, Vollmer et al., 2016, Chen et al., 2015]. A brief overview is the
following:

Let X =

(cid:19)

(cid:18)θ
r

. The generator L of diﬀusion process (1) is

L(f (X t)) = lim
h→0

E[f (X t+h)] − E[f (X t)]
h

=rT ∇θf − (γr + ∇V (θ))T ∇rf + γ∆rf.

3‘With high probability’ but not almost surely because Theorem 2 in fact suits a class of weights,

which includes but is not limited to EWSG.

8

Given a test function φ(x), its posterior average is ¯φ = (cid:82) φ(x)π(x)dx, approximated
(cid:80)K
by its time average of samples (cid:98)φK = 1
k is the sample path
K
given by EM integrator. Then the Poisson equation Lψ = φ − ¯φ can be a useful tool for
the weak convergence analysis of SG-MCMC. The solution ψ characterizes the diﬀerence
between φ and its posterior average ¯φ.

k ), where X E

k=1 φ(X E

Our main theoretical result is the following:

Theorem 3 Assume E[(cid:107)∇Vi(θE
and ∀k ≥ 0. Assume the solution to the Poisson equation, ψ, exists, and its derivatives
up to 3rd-order are uniformly bounded (cid:107)Dlψ(cid:107)∞ < M3, l = 0, 1, 2, 3. Then there exist
constants C1, C2, C3 > 0 depending on M1, M2, M3, such that

k (cid:107)l] < M2, ∀l = 1, 2, · · · , 12, ∀i = 1, 2, · · · , n

k )(cid:107)l] < M1, E[(cid:107)rE

E(cid:0)

(cid:98)φK − ¯φ(cid:1)2 ≤ C1

1
T

+ C2

h
T

(cid:80)K−1
k=0

E[Tr[cov(n∇VIk |Fk)]]

K

+ C3h2

(5)

where T = Kh is the corresponding time in the underlying continuous dynamics, Ik is
the index of the datum used to estimate the gradient at k-th iteration, and cov(n∇VIk |Fk)
is the covariance of stochastic gradient at k-th iteration conditioned on the current sigma
algebra Fk in the ﬁltration.

Remark: (interpreting the three terms in the bound ) Unlike a typical VR method which
aims at ﬁnding unbiased gradient estimator with reduced variance, EWSG aims at bring-
ing the entire density closer to that of a batch-gradient MCMC. As a consequence, its
practical implementation may correspond to SG that has reduced variance but a small
bias too. Eq.(5) quantiﬁes this bias-variance trade-oﬀ. How the extrinsic local variance
and bias contribute to the global error is respectively reﬂected in the 2nd and 3rd terms,
although the 3rd term also contains a contribution from the numerical discretization
error. With or without bias, the 3rd term remains O(h2) because of this discretization
error. However, for moderate T , the 2nd term is generally larger than the 3rd due to its
lower order in h, which means reducing local variance can improve sampling accuracy
even if at the cost of introducing a small bias. Since EWSG has a smaller local variance
than uniform SG (Thm.2, as a special case of improved overall statistical accuracy),
its global performance is also favorable. The 1st term is for the convergence of the
continuous process (eq.1 in this case).
Remark: (innovation and relation with the literature) Thm.3, to the best of our knowl-
edge, is the ﬁrst that incorporates the eﬀects of both local bias and local variance of
a SG approximation (previous SOTA bounds are only for unbiased SG). It still works
when restricting to unbiased SG, and in this case our bound reduces to SOTA Vollmer
et al. [2016], Chen et al. [2015]. Some more facts include: Mattingly et al. [2010], being
the seminal work from which we adapt our proof, only discussed the batch gradient case,
whereas our theory has additional (non-uniform) SG. Vollmer et al. [2016], Chen et al.
[2015] studied the eﬀect of SG, but the SG considered there did not use state-dependent
weights, which would destroy several martingales used in their proofs. Unlike in Mat-
tingly et al. [2010] but like in Vollmer et al. [2016], Chen et al. [2015], our state space is
not the compact torus but Rd. Also, the time average (cid:98)φK, to which our results apply, is

9

a commonly used estimator, particularly when using a long time trajectory of Markov
chain for sampling. However, if one is interested in an alternative of using an ensemble
for sampling, techniques in Cheng et al. [2018b], Dalalyan and Karagulyan [2017] might
be useful to further bound diﬀerence between the law of X k and the target distribution.

4.4 Practical Implementation

(cid:26)

+ (cid:107)x+nai(cid:107)2
In EWSG, the probability of each gradient term is pi = (cid:98)Z−1 exp
2
Although the term (cid:107)x + (cid:80)n
j=1 aj(cid:107)2/2 depends on the full data set, it is shared by all pi’s
and can be absorbed into the normalization constant ˆZ−1 (we still included it explicitly
due to the needs in proofs); unique to each pi is only the term (cid:107)x + nai(cid:107)2/2. This mo-
tivates us to run a Metropolis-Hastings chain over the possible indices i ∈ {1, 2 · · · , n}:
at each inner-loop step, a proposal of index j is uniformly drawn, and then accepted
with probability P (i → j) =

−

(cid:107)x+(cid:80)n
j=1 aj (cid:107)2
2

(cid:27)

.

(cid:26)

min

1, exp

(cid:18) (cid:107)x + naj(cid:107)2
2

−

(cid:107)x + nai(cid:107)2
2

(cid:19)(cid:27)

;

(6)

if accepted, the current index i is replaced by j. When the chain converges, the index
will follow the distribution given by pi. The advantage is, we avoid passing through the
entire data sets to compute each pi, but the index will still approximately sample from
the non-uniform distribution.

In practice, we often perform only M = 1 step of the Metropolis index chain per
integration step, especially if h is not too large. The rationale is, when h is small, the
outer iteration evolves slower than the index chain, and as θ does not change much in,
say, N outer steps, eﬀectively N × M inner steps take place on almost the same index
chain, which makes the index r.v. equilibrate better. Regarding the larger h case (where
the eﬃcacy of local variance reduction via non-uniform subsampling is more pronounced;
see e.g., Thm. 3), M = 1 may no longer be optimal, but improved sampling with large
h and M = 1 is still clearly observed in various experiments (Sec. 5).

√

Another hyper-parameter is x (see earlier discussion in Sec.4.2). Our heuristic rec-
ommendation is x =
. The rationale is, as long as rk+1 − rk’s density is maximized
at 0 (which will be the case at least for large k as rk will converge to a Gaussian), this
choice of x is a maximum likelihood estimator. This approximation appeared to be a
good one in all our experiments with medium h and M = 1.

hγrk
σ

Sec. 5.1 further investigates hyperparameter selection empirically and shows that
approximations due to M and x is not detrimental to our non-asymptotic theory in Sec.
4.3.

Practical EWSG is summarized in Algorithm 1. For simplicity of notation, we restrict
the description to mini batch size b = 1, but an extension to b > 1 is straightforward.
See Sec. D in appendix. Practical EWSG has reduced variance but does not completely
eliminate the extrinsic noise created by SG due to its approximations. A small bias was
also created by these approximations, but its eﬀect is dominated by the variance eﬀect

10

Algorithm 1 EWSG

Input: {the number of data terms n, gradient functions Vi(·), i = 1, 2, · · · , n, step size
h, the number of data passes K, index chain length M , friction and noise coeﬃcients
γ and σ}
Initialize θ0, r0 (arbitrarily, or use an informed guess)
for k = 0, 1, · · · , (cid:100) Kn

M +1 (cid:101) do

i ← uniformly sampled from 1, · · · , n, compute and store n∇Vi(θk)
I ← i
for m = 1, 2, · · · , M do

j ← uniformly sampled from 1, · · · , n, compute and store n∇Vj(θk)
I ← j with probability in Equation 6

end for
Evaluate ˜V (θk) = nVI (θk)
Update (θk+1, rk+1) ← (θk, rk) via one step of Euler-Maruyama integration using
˜V (θk)
end for

(see Sec. 4.3). In practice, if needed, one can combine EWSG with other VR technique
to further improve accuracy. Appendix F describes how EWSG can be combined with
SVRG.

5 Experiments

In this section, the proposed EWSG algorithm will be compared with SGHMC [Chen
et al., 2014], SGLD [Welling and Teh, 2011], as well as several more recent popular
approaches, including FlyMC [Maclaurin and Adams, 2015], pSGLD [Li et al., 2016], CP-
SGHMC [Fu and Zhang, 2017] (a method closest to the goal of applying IS idea to SG-
based sampling) and SVRG-LD [Dubey et al., 2016] (overdamped Langevin improved by
VR). Sec. 5.1 is a detailed empirical study of EWSG on simple models, with comparison
and implication of two important hyper-parameters M and x, and veriﬁcation of the
non-asymptotic theory (Theorem 3). Sec. 5.2 demonstrates EWSG for Bayesian logistic
regression on a large-scale data set. Sec. 5.3 is a Bayesian Neural Network (BNN)
example. It serves only as a high-dimensional, multi-modal test case, and we do not
intend to compare Bayesian and non-Bayesian neural nets. As FlyMC requires a tight
lower bound of likelihood, known for only a few cases, it will only be compared against
in Sec. 5.2 where such a bound is obtainable. CP-SGHMC requires heavy tuning on the
number of clusters which diﬀers across data sets/algorithms, so it will only be included
in the BNN example, for which the authors empirically found a good hyper parameter
for MNIST [Fu and Zhang, 2017]. SVRG-LD is only compared to in Sec. 5.1, because
SG-MCMC methods can converge within only one data pass in Sec. 5.2, rendering
control-variate based VR technique inapplicable, and it was suggested that VR leads to
poor results for deep models (e.g., Sec. 5.3) [Defazio and Bottou, 2019].

For fair comparison, all algorithms use constant step sizes and are allowed ﬁxed

11

(a) Sample quality in KL (b) Performance for various
x

(c) Performance for various
M

(d) MSE against time T (1st
and 2nd terms in Eq. (5))

(e) MSE against step size h
with ﬁxed ﬁnite T (2nd term
in Eq. (5))

(f) MSE against step size h
with T ≈ ∞ (3rd term in Eq.
(5))

Figure 1: Sampling from Gaussian target

computation budget, i.e., for L data passes, all algorithms can only call gradient function
nL times. All experiments are conducted on a machine with a 2.20GHz Intel(R) Xeon(R)
E5-2630 v4 CPU and an Nvidia GeForce GTX 1080 GPU. If not otherwise mentioned,
σ =
2γ so only γ needs speciﬁcation, the length of the index chain is set M = 1
for EWSG and the default values of two hyper-parameters required in pSGLD are set
λ = 10−5 and α = 0.99, as suggested in Li et al. [2016].

√

5.1 Gaussian Examples

Consider sampling from a simple 2D Gaussian whose potential function is

V (θ) =

n
(cid:88)

i=1

Vi(θ) =

n
(cid:88)

i=1

1
2

(cid:107)θ − ci(cid:107)2.

We set n = 50 and randomize ci from a two-dimensional standard normal N (0, I2).
Due to the simplicity of V (θ), we can write the target density analytically and will use
KL divergence KL(p(cid:107)q) = (cid:82) p(θ) log p(θ)
q(θ) dθ to measure the diﬀerence between the target
distribution and generated samples.

For each algorithm, we generate 10,000 independent realizations for empirical esti-
mation. All algorithms are run for 30 data passes with minibatch size of 1. Step size is
tuned from 5 × {10−1, 10−2, 10−3, 10−4} and 5 × 10−3 is chosen for SGLD and pSGLD,
5 × 10−2 for SGHMC and EWSG and 5 × 10−4 for SVRG-LD. SGHMC and EWSG use

12

51015Number of Data Pass100101KL DivergenceSGLDpSGLDSVRG-LDSGHMCEWSG246810Number of Data Pass2×1013×1014×101KL DivergenceEWSG(x=hrk)EWSG(x=0)EWSG(x=1)EWSG(x=(1+h)rkh)5101520Number of Data Pass100101102KL DivergenceEWSG(M=0) (SGHMC)EWSG(M=1)EWSG(M=9)EWSG(M=19)0.02.55.07.510.0T103102101[()2]EWSG(M=1)C11T+C2hT+C3h24×1035×1036×1037×1038×1039×103h2×105[()2]EWSG(M=1)C1h+C21017×1028×1029×102h1044×1056×105[()2]EWSG(M=1)O(h2)Method
Accuracy(%)
Log Likelihood
Wall Time (s)

SGLD
75.283 ± 0.016
-0.525 ± 0.000
3.085 ± 0.283

pSGLD
75.126 ± 0.020
-0.526 ± 0.000
4.312 ± 0.359

SGHMC
75.268 ± 0.017
-0.525 ± 0.000
3.145 ± 0.307

EWSG
75.306 ± 0.016
-0.523 ± 0.000
3.755 ± 0.387

FlyMC
75.199 ± 0.080
-0.523 ± 0.000
291.295 ± 56.368

Table 1: Accuracy, log likelihood and wall time of various algorithms on test data after
one data pass (mean ± std).

γ = 10. Results are shown in Fig. 1a and EWSG outperforms SGHMC, SGLD and
pSGLD in terms of accuracy. Note SVRG-LD has the best accuracy4 but the slowest
convergence, and that is why EWSG is a useful alternative to VR: its light-weight suits
situations with limited computational resources better.

Figure 1b shows the performance of several possible choices of the hyper-parameter
√
hγrk/σ, and x = 0, x = 1, x = (−1 +
h (which corresponds to rk+1 = 0). Step size h = 7 × 10−2 is used for this

x, including the recommended option x =
hγ)rk/σ
experiment. The recommended option performs better than the others.

√

Another important hyper-parameter in EWSG is M . As the length of index chain M
increases, the subsampling distribution approaches that given by Eq.4. Considering that
larger M means more gradient evaluations per step5, there could be some M value that
achieves the best balance between speed and accuracy. Fig.1c shows a fair comparison
of four values of M = 0, 1, 9, 19, and the recommended M = 1 case converges as fast as
SGHMC (when M = 0, EWSG does not run the Metropolis-Hastings index chain and
hence degenerates to SGHMC) but improves its accuracy.
It is also clear that as M
increases, sampling accuracy gets improved.

As approximations are used in Algorithm 1, it is natural to ask if results of Thm.
3 still hold. We empirically investigate this question (using M = 1 and variance as
the test function φ). Eq.5 in Thm.3 is a nonasymptotic error bound consisting of three
parts, namely an O( 1
T ) term corresponding to the convergence at the continuous limit,
an O(h/T ) term coming from the SG variance, and an O(h2) term due to bias and
numerical error. Fig.1d plots the mean squared error (MSE) against time T = Kh
to conﬁrm the 1st term. Fig.1e plots the MSE against h with ﬁxed T in the small h
regime (so that the 3rd term is negligible when compared to the 2nd) to conﬁrm that
the 2nd term scales like O(h). For the 3rd term in Eq. (5), we run suﬃciently many
iterations to ensure all chains are well-mixed, and Fig.1f conﬁrms the ﬁnal MSE to scale
like O(h2) even for large h (as the 2nd term vanishes due to T → ∞). In this sense,
despite the approximations introduced by the practical implementation, the performance
of Algorithm 1 is still approximated by Thm. 3, even when M = 1. Thm. 3 can thus
guide the choices of h and T in practice.

4For Gaussians, mean and variance completely determine the distribution, so appropriately reduced

variance leads to great accuracy for the entire distribution.

5in each iteration of the outer MCMC loop, EWSG consumes M + 1 data points, and hence in a fair
M +1 iterations

comparison with ﬁxed computation budget (e.g. E total gradient calls), EWSG runs E
which is decreasing in M .

13

(a) Test Accuracy

(b) Test Log Likelihood

Figure 2: BLR learning curve

(a) MLP architecture (b) CNN architecture

Figure 3: BNN learning curve. Shade: one
standard deviation.

5.2 Bayesian Logistic Regression (BLR)

Consider Bayesian logistic regression for the binary classiﬁcation problem. The prob-
abilistic model for predicting a label yk given a feature vector xk is p(yk = 1|xk, θ) =
1/(1 + exp(−θT xk)). We set a Gaussian prior with zero mean and covariance Σ = 10Id
for θ, and hence the potential function of the posterior distribution of θ is

V (θ) = 5(cid:107)θ(cid:107)2 −

n
(cid:88)

i=1

yi log p(yi = 1|xi, θ) + (1 − yi) log (1 − p(yi = 1|xi, θ)) .

We conduct our experiments on Covertype data set6, which contains n = 581, 012 data
points and 54 features (which is the dimension of θ). Given the large size of this data
set, SG is needed to scale up MCMC methods. We use 80% of data for training and the
rest 20% for testing.

The FlyMC algorithm7 uses a lower bound derived in Maclaurin and Adams [2015]
for likelihood function. For underdamped Langevin based algorithms, we set friction
coeﬃcient γ = 50. After tuning, we set the step size as {1, 3, 0.02, 5, 5} × 10−3 for
SGULD, EWSG, SGLD, pSGLD and FlyMC. All algorithms are run for one data pass,
with minibatch size of 50 (for FlyMC, it means 50 data are sampled in each iteration
to switch state). 100 independent samples are drawn from each algorithm to estimate
statistics. To further smooth out noise, all experiments are repeated 1000 times with
diﬀerent seeds.

Results are in Fig. 2a and 2b and Table 1. EWSG outperforms others, except for
log likelihood being comparable to FlyMC, which is an exact MCMC method. The wall
time consumed by EWSG is only slightly more than that of SGLD and SGHMC, but
less than pSGLD and orders-of-magnitude less than FlyMC.

5.3 Bayesian Neural Network (BNN)

Bayesian neural network is a compelling model for deep learning [Wilson, 2020]. Here
two popular architectures of BNN are experimented – multilayer perceptron (MLP) and
convolutional neural nets (CNN). In MLP, a hidden layer with 100 neurons followed

6https://archive.ics.uci.edu/ml/datasets/covertype
7https://github.com/HIPS/ﬁreﬂy-monte-carlo/tree/master/ﬂymc

14

0.00.20.40.60.81.0Number of Data Pass50.055.060.065.070.075.0Test Accuracy (%)SGLDpSGLDSGHMCEWSG0.00.20.40.60.81.0Number of Data Pass-0.70-0.68-0.65-0.63-0.60-0.58-0.55-0.53Log LikelihoodSGLDpSGLDSGHMCEWSG50100150200Number of Data Pass0.00.10.20.30.40.50.60.70.8Training Error (%)SGLDpSGLDSGHMCCP-SGHMCEWSG50100150200Number of Data Pass0.00.10.20.30.40.50.60.7Training Error (%)SGLDpSGLDSGHMCCP-SGHMCEWSGby a softmax layer is used.
In CNN, we use standard network conﬁguration with 2
convolutional layers followed by 2 fully connected layers [Jarrett et al., 2009]. Both
convolutional layers use 5 × 5 convolution kernel with 32 and 64 channels, 2 × 2 max
pooling layers follow immediately after convolutional layer. The last two fully-connected
layers each has 200 neurons. We set the standard normal as prior for all weights and
bias.

We test algorithms on the MNIST data set, consisting of 60,000 training data and
10,000 test data, each datum is a 28 × 28 gray-scale image with one of the ten possible
labels (digits 0 ∼ 9). For ULD based algorithms , we set friction coeﬃcient γ = 0.1 in
MLP and γ = 1.0 in CNN. In MLP, the step sizes are set h = {4, 2, 2} × 10−3 for EWSG,
SGHMC and CP-SGHMC, and h = {0.001, 1} × 10−4 for SGLD and pSGLD, via grid
search. For CP-SGHMC , (clustering-based preprocessing is conducted [Fu and Zhang,
2017] before SGHMC) we use K-means with 10 clusters to preprocess the data set. In
CNN, the step sizes are set h = {4, 2, 2} × 10−3 for EWSG, SGHMC and CP-SGHMC,
and h = {0.02, 8} × 10−6 for SGLD and pSGLD, via grid search. All algorithms use
minibatch size of 100 and are run for 200 epoches. For each algorithm, we generate
100 independent samples to make posterior prediction. To smooth out noise and obtain
more signiﬁcant results, we repeat experiments 10 times with diﬀerent seeds.

The learning curve of training error is shown in Fig. 3a and 3b. EWSG consistently
improves over its uniform counterpart (i.e., SGHMC) and CP-SGHMC (an approximate
IS SG-MCMC). Moreover, EWSG also outperforms two standard benchmarks SGLD
and pSGLD. The improvement over baseline on MNIST data set is comparable to some
of the early works [Chen et al., 2014, Li et al., 2016].

Note:

in the MLP setup, the model has d > 78, 400 parameters whereas there are
n = 60, 000 data points, which shows EWSG does not require n > d to work and can still
outperform its uniform counterpart in the overparametrized regime (Thm.1 demonstrates
the underparametrized case only because the sparsity result is easy to understand, but
EWSG doesn’t only work for underparameterized models).

Acknowledgment

We thank Yian Ma and Xuefeng Gao for insightful discussions. MT is grateful for
the partial support by NSF DMS-1847802 and ECCS-1936776. This work was mainly
conducted when HZ was a professor at Georgia Institute of Technology.

15

References

Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via
stochastic gradient ﬁsher scoring. In 29th International Conference on Machine Learn-
ing, ICML 2012, pages 1591–1598, 2012.

Francis Bach. Stochastic gradient methods for machine learning. Technical report,
INRIA - Ecole Normale Superieur, 2013. URL http://lear.inrialpes.fr/people/
harchaoui/projects/gargantua/slides/bach_gargantua_nov2013.pdf.

Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates

for stochastic gradient mcmc. Statistics and Computing, 29(3):599–615, 2019.

R´emi Bardenet, Arnaud Doucet, and Christopher C Holmes. On markov chain monte
carlo methods for tall data. Journal of Machine Learning Research, 18(47), 2017.

Vivek S Borkar and Sanjoy K Mitter. A strong approximation theorem for stochastic
recursive algorithms. Journal of optimization theory and applications, 100(3):499–513,
1999.

Nawaf Bou-Rabee and Houman Owhadi. Long-run accuracy of variational integrators
in the stochastic context. SIAM Journal on Numerical Analysis, 48(1):278–297, 2010.

Nawaf Bou-Rabee and Jes´us Mar´ıa Sanz-Serna. Geometric integrators and the Hamil-

tonian Monte Carlo method. Acta Numerica, 27:113–206, 2018.

Nawaf Bou-Rabee, Andreas Eberle, and Raphael Zimmer. Coupling and convergence for

Hamiltonian Monte Carlo. arXiv preprint arXiv:1805.00452, 2018.

Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov

chain monte carlo. CRC press, 2011.

Niladri S Chatterji, Nicolas Flammarion, Yi-An Ma, Peter L Bartlett, and Michael I
Jordan. On the theory of variance reduction for stochastic gradient monte carlo.
ICML, 2018.

Changyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradi-
ent mcmc algorithms with high-order integrators. In Advances in Neural Information
Processing Systems, pages 2278–2286, 2015.

Tianqi Chen, Emily B Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte

Carlo. International Conference on Machine Learning, pages 1683–1691, 2014.

Xiang Cheng, Niladri S Chatterji, Yasin Abbasi-Yadkori, Peter L Bartlett, and Michael I
Jordan. Sharp convergence rates for langevin dynamics in the nonconvex setting. arXiv
preprint arXiv:1805.01648, 2018a.

16

Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped
langevin mcmc: A non-asymptotic analysis. Proceedings of the 31st Conference On
Learning Theory, PMLR, 2018b.

Dominik Csiba and Peter Richt´arik. Importance sampling for minibatches. The Journal

of Machine Learning Research, 19(1):962–982, 2018.

Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the langevin

monte carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095, 2017.

Aaron Defazio and L´eon Bottou. On the ineﬀectiveness of variance reduced optimization
for deep learning. In Advances in Neural Information Processing Systems, pages 1755–
1765, 2019.

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gra-
dient method with support for non-strongly convex composite objectives. In Advances
in neural information processing systems, pages 1646–1654, 2014.

Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos,
Alexander J Smola, and Eric P Xing. Variance reduction in stochastic gradient
langevin dynamics. In Advances in neural information processing systems, pages 1154–
1162, 2016.

Tianfan Fu and Zhihua Zhang. Cpsg-mcmc: Clustering-based preprocessing method
for stochastic gradient mcmc. In Artiﬁcial Intelligence and Statistics, pages 841–850,
2017.

Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is
the best multi-stage architecture for object recognition? In 2009 IEEE 12th interna-
tional conference on computer vision, pages 2146–2153. IEEE, 2009.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction. In Advances in neural information processing systems, pages 315–
323, 2013.

Anoop Korattikara, Yutian Chen, and Max Welling. Austerity in mcmc land: Cutting
the metropolis-hastings budget. In International Conference on Machine Learning,
pages 181–189. PMLR, 2014.

Rep Kubo. The ﬂuctuation-dissipation theorem. Reports on progress in physics, 29(1):

255, 1966.

Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned
stochastic gradient langevin dynamics for deep neural networks. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.

Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modiﬁed equations and adaptive
In International Conference on Machine Learning,

stochastic gradient algorithms.
pages 2101–2110, 2017.

17

Ruilin Li, Hongyuan Zha, and Molei Tao. Mean-square analysis with an application to
optimal dimension dependence of Langevin Monte Carlo. preprint arXiv:2109.03839,
2021.

Moshe Lichman et al. UCI machine learning repository, 2013.

Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc.

In Advances in Neural Information Processing Systems, pages 2917–2925, 2015.

Dougal Maclaurin and Ryan Prescott Adams. Fireﬂy monte carlo: Exact mcmc with
subsets of data. In Twenty-Fourth International Joint Conference on Artiﬁcial Intel-
ligence, 2015.

Stephan Mandt, Matthew D Hoﬀman, and David M Blei. Stochastic gradient descent as
approximate bayesian inference. The Journal of Machine Learning Research, 18(1):
4873–4907, 2017.

Jonathan C Mattingly, Andrew M Stuart, and Michael V Tretyakov. Convergence of nu-
merical time-averaging and stationary measures via poisson equations. SIAM Journal
on Numerical Analysis, 48(2):552–577, 2010.

Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted
sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information
Processing Systems, pages 1017–1025, 2014.

Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics
on the probability simplex. Advances in Neural Information Processing Systems, pages
3102–3110, 2013.

Grigorios A Pavliotis. Stochastic processes and applications: diﬀusion processes, the

Fokker-Planck and Langevin equations, volume 60. Springer, 2014.

Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of langevin

distributions and their discrete approximations. Bernoulli, 2(4):341–363, 1996.

Mark Schmidt, Reza Babanezhad, Mohamed Ahmed, Aaron Defazio, Ann Clifton, and
Anoop Sarkar. Non-uniform stochastic average gradient method for training condi-
tional random ﬁelds. In artiﬁcial intelligence and statistics, pages 819–828, 2015.

Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the

stochastic average gradient. Mathematical Programming, 162(1-2):83–112, 2017.

Molei Tao and Tomoki Ohsawa. Variational optimization on lie groups, with examples

of leading (generalized) eigenvalue problems. AISTATS, 2020.

Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and ﬂuctu-
ations for stochastic gradient langevin dynamics. The Journal of Machine Learning
Research, 17(1):193–225, 2016.

18

Sebastian J Vollmer, Konstantinos C Zygalakis, and Yee Whye Teh. Exploration of the
(non-) asymptotic bias and variance of stochastic gradient langevin dynamics. The
Journal of Machine Learning Research, 17(1):5504–5548, 2016.

Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin

dynamics. International Conference on Machine Learning, pages 681–688, 2011.

Andrew Gordon Wilson. The case for bayesian deep learning.

arXiv preprint

arXiv:2001.10995, 2020.

Ruqi Zhang and Christopher De Sa. Poisson-minibatching for gibbs sampling with

convergence rate guarantees. NeurIPS, 2019.

Ruqi Zhang, A Feder Cooper, and Christopher De Sa. Asymptotically optimal exact

minibatch metropolis-hastings. NeurIPS, 2020.

Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for reg-
ularized loss minimization. In International Conference on Machine Learning, pages
1–9, 2015.

Rong Zhu. Gradient-based sampling: An adaptive importance sampling for least-squares.

In Advances in Neural Information Processing Systems, pages 406–414, 2016.

19

A Proof of Theorem 1

Proof: Denote the set of all n-dimensional probability vectors by Σn, the set of sparse
probability vectors by S, and the set of non-sparse (dense) probability vectors by D =
Σn \ S. Denote B = [b1, · · · , bn], then the optimization problem can be written as

min

n
(cid:88)

i=1

pi(cid:107)bi(cid:107)2

s.t.






Bp = 0
pT 1n = 1
pi ≥ 0, i = 1, 2, · · · , n

Note that the feasible region is always non-empty (take p to be a uniform distribution)
and is also closed and bounded, hence this linear programming is always solvable. Denote
the set of all minimizers by M. Note that M depends on b1, · · · , bn and is in this sense
random.

The Lagrange function is

L(p, λ, µ, ω) = pT s − λT Bp − µ(pT 1n) − ωT p

where s = [(cid:107)b1(cid:107)2, (cid:107)b2(cid:107)2, · · · , (cid:107)bn(cid:107)2]T and λ, µ, ω are dual variables. The optimality
condition reads as

∂L
∂p

= s − BT λ − µ1n − ω = 0

Dual feasibilty and complementary slackness require

ωi ≤ 0, i = 1, 2, · · · , n
ωT p = 0

Consider the probability of the event {a dense probability vector can solve the above

minimization problem}, i.e., P(M ∩ D (cid:54)= ∅). It is upper bounded by

P(M ∩ D (cid:54)= ∅) ≤ P(p ∈ D and p solves KKT condition)

Since p ∈ D, complementary slackness implies that at least d + 2 entries in ω are
zero. Denote the indices of these entries by J . For every j ∈ J , by optimality condition,
we have sj − λT bj − µ = 0, i.e.,

(cid:107)bj(cid:107)2 − λT bj − µ = 0

Take the ﬁrst d + 1 indices in J , and note a geometric fact that d + 1 points in a d-
dimensional space must be on the surface of a hypersphere of at most d − 1 dimension,

20

which we denote by S = Sq−1 + x for some vector x and integer q ≤ d. Because bi’s
distribution is absolutely continuous, we have

P(p ∈ D and p solves KKT condition)

≤P(p ∈ D and bj ∈ S, ∀j ∈ J )
≤P(bj ∈ S, ∀j ∈ J )
=P(bjk ∈ S, k = d + 2, · · · , |J |)

|J |
(cid:89)

=

k=d+2

P(bjk ∈ S)

(independence)

=0

(absolute continuous)

Hence P(M ∩ D (cid:54)= ∅) = 0 and

1 = P(M (cid:54)= ∅)

= P((M ∩ S) ∪ (M ∩ D) (cid:54)= ∅)
≤ P(M ∩ S (cid:54)= ∅) + P(M ∩ D (cid:54)= ∅)
= P(M ∩ S (cid:54)= ∅)

Therefore we have

P(M ∩ S (cid:54)= ∅) = 1

B Proof of Theorem 2

Proof: Let bi = n∇Vi and assume (cid:107)bi(cid:107)2 ≤ R for some constant R. Denote B =
[b1, b2, · · · , bn]. For any probability distribution p over {1, · · · , n}, we have

covI∼p[bI |b1, · · · , bn]

n
(cid:88)

i=1

n
(cid:88)

i=1
(cid:88)

=

=

=

pibibT

i −

(cid:32) n
(cid:88)

pibi

(cid:33) (cid:32) n
(cid:88)

pibi

(cid:33)T

pibibT
i

n
(cid:88)

i=1

i=1

i=1

(cid:32) n
(cid:88)

pibi

(cid:33) (cid:32) n
(cid:88)

pibi

(cid:33)T

pi −

i=1

i=1

(bi − bj)(bi − bj)T pipj

i<j

Therefore we let

f (B) := Tr



(cid:88)



i<j

(bi − bj)(bi − bj)T pipj −

(bi − bj)(bi − bj)T 1
n2

(cid:88)

i<j





=

(cid:88)

i<j

(cid:107)bi − bj(cid:107)2pipj −

(cid:88)

i<j

(cid:107)bi − bj(cid:107)2 1
n2

(Tr[AB] = Tr[BA])

21

and use it to compare the trace of covariance matrix of uniform- and nonuniform- sub-
samplings.

First of all,

E[f (B)]

=E[(cid:107)bi − bj(cid:107)2]

(cid:88)

i<j


(cid:18)

pipj −

(cid:19)

1
n2

=E[(cid:107)bi − bj(cid:107)2]

(cid:88)



pipj −





n − 1
2n

i<j
(cid:18) 1 − (cid:80)n
i=1 p2
i
2

(cid:19)

−

n − 1
2n

(cid:32)

1 − 1
n
2

−

n − 1
2n

(cid:33)

=E[(cid:107)bi − bj(cid:107)2]

≤E[(cid:107)bi − bj(cid:107)2]

=0

where the inequality is due to Cauchy-Schwarz and it is a strict inequality unless all
pi’s are equal, which means uniform subsampling on average has larger variablity than
a non-uniform scheme measured by the trace of covariance matrix.

Moreover, concentration inequality can help show f (B) is negative with high proba-
√

bility if h is small. To this end, plug x = O(

h) in and rewrite

(cid:40)

(cid:34)

exp

F h

pi =

1
Z

(cid:107)y + 1
n

(cid:80)n

i=1 bi(cid:107)2

2

(cid:35)(cid:41)

−

(cid:107)y + bi(cid:107)2
2

where y = σ√
h
unnormalized probability by

x = O(1), F = − 1

σ2 and Z is the normalization constant. Denote the

(cid:40)

(cid:34)

(cid:101)pi = exp

F h

(cid:107)y + 1
n

(cid:80)n

i=1 bi(cid:107)2

2

(cid:35)(cid:41)

−

(cid:107)y + bi(cid:107)2
2

and we have

f (B) =

=

1
2

1
2

n
(cid:88)

n
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1

j=1

(cid:107)bi − bj(cid:107)2

(cid:18)

pipj −

(cid:19)

1
n2

(cid:107)bi − bj(cid:107)2

(cid:101)pi (cid:101)pj
k=1 (cid:101)pk]2 −

[(cid:80)n

1
2

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:107)bi − bj(cid:107)2 1
n2

To prove concentration results, it is useful to estimate

Ci =

sup
b1,··· ,bn∈B(0,R)
(cid:98)bi∈B(0,R)

|f (b1, · · · , bi, · · · , bn)

22

−f (b1, · · · , (cid:98)bi, · · · , bn)|

where B(0, R) is a ball centered at origin with radius R in Rd.
Due to the mean value theorem, we have Ci ≤ 2R sup | ∂f
∂bi

to compute sup | ∂f
∂b1

| to upper bound C1. Note that

|. By symmetry, it suﬃces

∂ (cid:101)pj
∂b1

= 2(cid:101)pjF h[

1
n

(y +

1
n

n
(cid:88)

i=1

bi) − (y + bj)δ1j] = O(h)(cid:101)pj

where δ1j is the Kronecker delta function. Thus

∂f
∂b1

(cid:88)

=

(b1 − bj)

[(cid:80)n

n
(cid:88)

j=1
n
(cid:88)

(cid:101)p1 (cid:101)pj
k=1 (cid:101)pk]2 −
(cid:101)pi (cid:101)pj
k=1 (cid:101)pk]3
(cid:101)pj
k=1 (cid:101)pk]2 −

[(cid:80)n

[(cid:80)n

k=1
n
(cid:88)

j=1

(cid:107)b1 − bj(cid:107)2

(b1 − bj)

) + O(h) + O(h)

j=1
n
(cid:88)

− 2

i,j=1
n
(cid:88)

= (cid:101)p1

= O(

j=1
h
n
= O(h)

(cid:101)pkO(h)

(b1 − bj)

1
n2 +

n
(cid:88)

i,j=1

(cid:107)b1 − bj(cid:107)2 O(h)(cid:101)pi (cid:101)pj
k=1 (cid:101)pk]2

[(cid:80)n

(b1 − bj)

1
n2 +

O(n2)O(h)
O(n2)

+

O(n2)
O(n3)

O(n)O(h)

n ) in the 2nd last equation comes from the diﬀerence of the ﬁrst two terms in

where O( h
the 3rd last equation. This estimation shows that Ci ≤ 2RO(h) = O(h).
Therefore, by McDiarmid’s inequality, we conclude for any (cid:15) > 0,

P(|f − E[f ]| > (cid:15)) ≤ 2 exp

(cid:19)

(cid:18) −2(cid:15)2
(cid:80)n

i=1 C2
i

= 2 exp

(cid:18) −2(cid:15)2
nO(h2)

(cid:19)

.

Any choice of h(n) = o(n−1/2) will render this probability asymptotically vanishing as
n grows, which means that f will be negative with high probability, which is equivalent
to reduced variance per step.

C Proof of Theorem 3

Proof: We rewrite the generator of underdamped Langevin with full gradient as

Lf (X) = F (X)T

(cid:21)
(cid:20)∇θf (X)
∇rf (X)

+

1
2

A : ∇∇f (X)

where

F (X) =

(cid:20)

(cid:21)

r
−γr − ∇V (θ)

, A = GGT and G =

(cid:20)Od×d
Od×d

(cid:21)

√

Od×d
2γId×d

23

Rewrite the discretized underdamped Langevin with stochastic gradient in variable

X, i.e.,

where

X E

k+1 − X E

k = hF k(X E

k ) +

√

hGkηk+1

F k(X) =

(cid:20)

r
−γr − n∇VIk (θ)

(cid:21)

,

Gk = G =

(cid:20)Od×d
Od×d

√

Od×d
2γId×d

(cid:21)

,

and ηk+1 is a 2d dimensional standard Gaussian random vector. Note that this repre-
sentation include both SGHMC and EWSG, for SGHMC Ik follows uniform distribution
and for EWSG, Ik follows the MCMC-approximated exponentially weighted distribution.
Denote the generator associated with stochastic gradient underdamped Langevin at

the k-th iteration by

Lkf (X) = F k(X)T

(cid:21)
(cid:20)∇θf (X)
∇rf (X)

+

1
2

A : ∇∇f (X)

and the diﬀerence of the generators of full gradient and stochastic gradient underdamped
Langevin at k-th interation is denoted by

∆Lkf (X) = (Lk−L)f (X) = (F k(X)−F (X))T

(cid:21)

(cid:20)∇θf (X)
∇rf (X)

= (cid:104)∇V (θ)−n∇VIk (θ), ∇rf (X)(cid:105)

For brevity, we write φk = φ(X E

k ) and Dlφk =
k ) where (Dlψ)(z) is the l-th order derivative. We write (Dlψ)[s1, s2, · · · , sl]

(Dlψ)(X E
for derivative evaluated in the direction sj, j = 1, 2, · · · , l. Deﬁne

k ), ψk = ψ(X E

k = F k(X E

k ), F E

δk = X E

k+1 − X E

k = hF E

k +

√

hGkηk+1

Under the assumptions of Theorem 3, we show that the vector ﬁeld F E

k also has

bounded momentum up to p-th order.

Lemma 4 Under the assumption of Theorem 3, there exists a constant M such that up
to p

2 -th order moments of random vector ﬁeld F E

k are bounded

E(cid:107)F E

k (cid:107)j

2 ≤ M, ∀j = 0, 1, 2, · · · ,

p
2

, ∀k = 0, 1, 2 · · · ,

Proof: It suﬃces to bound the highest moment, as all other lower order moments are
bounded by the highest one by Holder’s inequality.

First notice that

(cid:107)F E

k (cid:107)2 =

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

rE
k
k − ∇VIk (θE
k )

−γrE

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:112)

≤

1 + γ2(cid:107)rE

k (cid:107)2 + (cid:107)∇VIk (θE

k )(cid:107)2

24

Hence

E(cid:107)F E
k (cid:107)

p
2

2 ≤E

=E

(cid:16)(cid:112)

1 + γ2(cid:107)rE

k (cid:107)2 + (cid:107)∇VIk (θE

k )(cid:107)2

(cid:17) p

2




p

2(cid:88)

(cid:19)

(cid:18) p
2
i

(cid:107)rE

k (cid:107)i

2(cid:107)∇VIk (θE

k )(cid:107)

p
2 −i
2








i=0

p

2(cid:88)

i=0
p

2(cid:88)

i=0

(cid:18) p
2
i

(cid:18) p
2
i

=

≤

(cid:19)

E

(cid:104)
(cid:107)∇VIk (θE

k )(cid:107)

p
2 −i
2

(cid:107)rE

k (cid:107)i
2

(cid:105)

(cid:19)(cid:114)

E

(cid:104)
(cid:107)∇VIk (θE

k )(cid:107)p−2i

2

(cid:105)(cid:113)

E (cid:2)(cid:107)rE

k (cid:107)2i
2

(cid:3)

(Cauchy-Schwarz inequality)

By assumption, we know each E (cid:2)(cid:107)∇VIk (θE
we conclude there exists a constant M > 0 that bounds the p
F E

(cid:3) , E(cid:107)rE

k )(cid:107)l
2

k , ∀k = 0, 1, · · · ,

k (cid:107)l

2, l = 0, 1, · · · , p is bounded, so
2 -th order moment of

Using Taylor’s expansion for ψ, we have

ψk+1 = ψk + Dψk[δk] +

D2ψk[δk, δk] +

1
2

1
6

D3ψk[δk, δk, δk] + Rk+1

where

Rk+1 =

(cid:18) 1
6

(cid:90) 1

0

s3D4ψ(sX E

k + (1 − s)X E

k+1)ds

(cid:19)

[δk, δk, δk, δk]

is the remainder term. Therefore, we have

ψk+1 =ψk + hLkψk + h

1

2 Dψk[Gkηk+1] + h

3

2 D2ψk[F E

k , Gkηk+1]

(7)

+

1
2

h2D2ψk[F E

k , F E

k ] +

D3ψk[δk, δk, δk] + rk+1 + Rk+1

1
6

where

rk+1 =

h
2

(cid:0)D2ψk[Gkηk+1, Gkηk+1] − A : ∇∇ψk

(cid:1)

Summing Equation (7) ove the ﬁrst K terms, dividing by Kh and use Poisson equa-

tion, we have

1
Kh

(ψK − ψ0) =

1
K

K−1
(cid:88)

(φk − ¯φ) +

k=0

1
K

K−1
(cid:88)

k=0

∆Lkψk +

1
Kh

3
(cid:88)

i=1

(Mi,K + Si,K),

(8)

where

M1,K =

K−1
(cid:88)

k=0

rk+1, M2,K = h

K−1
(cid:88)

1
2

k=0

Dψk[Gkηk+1], M3,K = h

K−1
(cid:88)

3
2

k=0

D2ψk[F E

k , Gkηk+1],

S1,K =

h2
2

K−1
(cid:88)

k=0

D2ψk[F E

k , F E

k ], S2,K =

K−1
(cid:88)

k=0

Rk+1, S3,K =

1
6

K−1
(cid:88)

k=0

D3ψk[δk, δk, δk]

25

Furthermore, it will be convenient to decompose

S3,K = M0,K + S0,K

where

S0,K =h2

M0,K =h

3
2

K−1
(cid:88)

k=0
K−1
(cid:88)

k=0

(cid:0)hD3ψk[F E

k , F E

k , F E

k ] + 3D3ψk[F E

k , Gkηk+1, Gkηk+1](cid:1)

(cid:0)D3ψk[Gkηk+1, Gkηk+1, Gkηk+1] + 3hD3ψk[F E

k , F E

k , Gkηk+1](cid:1)

Rearrange terms in Equation (7), square on both sides, use Cauchy-Schwarz inequal-

ity and take expectation, we have

E(cid:0) ˆφK − ¯φ(cid:1)2 ≤C



E (ψK − ψ0)2

(Kh)2 +

E

1
K2



E (ψK − ψ0)2
T 2

=C

+

E

1
K2

(cid:32)K−1
(cid:88)

k=0
(cid:32)K−1
(cid:88)

k=0

(cid:33)2

(∆Lkψk)

+

1
(Kh)2

2
(cid:88)

i=0

ES2

i,K +

1
(Kh)2

3
(cid:88)

i=0



EM 2

i,K



(cid:33)2

(∆Lkψk)

+

1
T 2

2
(cid:88)

i=0

ES2

i,K +



EM 2

i,K



1
T 2

3
(cid:88)

i=0

(9)

where T = kh, the corresponding time of the underlying continuous dynamics.
We now show how each term is bounded. By boundedness of ψ, we have

E (ψK − ψ0)2
T 2
K2 E(cid:0) (cid:80)K−1

k=0 (∆Lkψk)(cid:1)2

≤

4(cid:107)ψ(cid:107)2
∞

T 2 = O(

1
T 2 )

The second term 1

is critical in showing the advantage of

EWSG, and we will show how to derive its bound in detail later.

The technique we use to bound 1

i,K, i = 0, 1, 2 are all similar, we will ﬁrst show
an upper bound for |Si,K| in terms of powers of (cid:107)F E
k (cid:107), then take square and expectation,
and ﬁnally expand squares and use Lemma 4 extensively to derive bounds. As a concrete
example, we will show how to bound 1
0,K. Other bounds follow in a similar fashion
and details are omitted.

T 2 ES2

T 2 ES2

To bound the term containing S0,K, we ﬁrst note that

|S0,K| ≤h2

K−1
(cid:88)

k=0

(cid:0)h|D3ψk[F E

k , F E

k , F E

k ]| + 3|D3ψk[F E

k , Gkηk+1, Gkηk+1]|(cid:1)

≤h2(cid:107)D3ψ(cid:107)∞

K−1
(cid:88)

k=0

(cid:0)h(cid:107)F E

k (cid:107)3

2 + 3(cid:107)F E

k (cid:107)2(cid:107)Gkηk+1(cid:107)2
2

(cid:1)

26

Square both sides of the above inequality and take expectation, we obtain

K−1
(cid:88)

E(cid:0)

h(cid:107)F E

k (cid:107)3

2 + 3(cid:107)F E

k (cid:107)2(cid:107)Gkηk+1(cid:107)2
2

(10)

(cid:1)2

k=0
K−1
(cid:88)

k=0
K−1
(cid:88)

k=0
K−1
(cid:88)

k=0

E(h(cid:107)F E

k (cid:107)3

2 + 3(cid:107)F E

k (cid:107)2(cid:107)Gkηk+1(cid:107)2

2)2

(Cauchy-Schwarz inequality)

E[h2(cid:107)F E

k (cid:107)6

2 + 6(cid:107)F E

k (cid:107)4

2(cid:107)Gkηk+1(cid:107)2

2 + 9(cid:107)F E

k (cid:107)2

2(cid:107)Gkηk+1(cid:107)2
4]

h2E(cid:107)F E

k (cid:107)6

2 + 6E(cid:107)F E

k (cid:107)4
2

E(cid:107)Gkηk+1(cid:107)2

2 + 9E(cid:107)F E

k (cid:107)2
2

E(cid:107)Gkηk+1(cid:107)2
4

E|S0,K|2

1
T 2
h4
T 2 (cid:107)D3ψ(cid:107)2

∞

≤

≤

h4
T 2 (cid:107)D3ψ(cid:107)2

∞K

=

h4
T 2 (cid:107)D3ψ(cid:107)2

∞K

∞K

=

h4
T 2 (cid:107)D3ψ(cid:107)2
1
T 2 O(K2h4)
=O(h2)

=

To bound the term containing S1,K and S2,K, we have

|S1,K| ≤

|S2,K| ≤

h2
2

1
24

K−1
(cid:88)

k=0

(cid:107)D2ψ(cid:107)∞(cid:107)F E

k (cid:107)2
2

(cid:107)D4ψ(cid:107)∞

K−1
(cid:88)

k=0

(cid:107)δk(cid:107)4

2 ≤

1
24

h2(cid:107)D4ψ(cid:107)∞

K−1
(cid:88)

√

(cid:107)

k=0

hF E

k + Gkηk+1(cid:107)4
2

Then we can obtain the following bound in a similar fashion as in Equation (10)

1
T 2
1
T 2

ES2

1,K =O(h2)

ES2

2,K =O(h2)

Now we will use martingale argument to bound 1

i,K, i = 0, 1, 2, 3. There are
two injected randomness at k-th iteration, the Gaussian noise ηk+1 and the stochastic
gradient term determined by the stochastic index Ik. Denote the sigma algebra at k-th
iteration by Fk. For both SGHMC and EWSG we have

T 2 EM 2

hence

ηk+1 ⊥ Fk and Ik ⊥ ηk+1

E[ηk+1|Fk] =0
E[D3ψk[Gkηk+1, Gkηk+1, Gkηk+1]|Fk] =0
k , Gkηk+1]|Fk] =0
k , Gkηk+1]|Fk] =0

E[D2ψk[F E
k , F E

E[D3ψk[F E

27

Therefore, it is clear that Mi,K, i = 0, 1, 2, 3 are all martingales. Due to martingale

properties, we have

1
T 2

1
T 2

1
T 2

1
T 2

EM 2

0,K =

h3
T 2

K−1
(cid:88)

k=0

E(cid:0)D3ψk[Gkηk+1, Gkηk+1, Gkηk+1] + 3hD3ψk[F E

k , F E

k , Gkηk+1](cid:1)2

=

EM 2

1,K =

1
T 2 O(h3K) = O(
1
Er2
T 2

k+1 =

K−1
(cid:88)

)

h2
T
1
T 2 O(h2K) = O(

h
T

)

k=0
K−1
(cid:88)

k=0

EM 2

2,K =

h
T 2

E(Dψk[Gkηk+1])2 =

1
T 2 O(hK) = O(

1
T

)

EM 2

3,K =

1
T 2 h3

K−1
(cid:88)

k=0

E(D2ψk[F E

k , Gkηk+1])2 =

1
T 2 O(h3K) = O(

h2
T

)

We now collect all bounds derived so far and obtain



E(cid:0) ˆφK − ¯φ(cid:1)2 ≤C

O(

1
T 2 ) +

1
K2

(cid:32)K−1
(cid:88)

E

(cid:33)2

(∆Lkψk)

+ O(h2) + O(

h
T

) + O(

1
T

) + O(



≤C

O(

1
T

) +

E

1
K2

k=0
(cid:32)K−1
(cid:88)

(∆Lkψk)

k=0

(cid:33)2



+ O(h2)





)



h2
T

(11)

In the above inequality, we use 1
T (cid:29) 1 and h (cid:28) 1 in non-asymptotic analysis.
Now we focus on the remaining term 1

T 2 < 1

T and h

T ≤ 1
K2 E(cid:0) (cid:80)K−1

that E[∆Lkψk|Fk] = 0, hence (cid:80)K−1
we have

. For SGHMC, we have
k=0 ∆Lkψk is a martingale. By martingale property,

k=0 ∆Lkψk

(cid:1)2

T , h2

T ≤ 1

T as typically we assume

E

1
K2

(cid:32)K−1
(cid:88)

k=0

(cid:33)2

∆Lkψk

=

1
K2

K−1
(cid:88)

k=0

E(∆Lkψk)2

For EWSG, (cid:80)K−1

k=0 ∆Lkψk is no longer a martingale, but we still have the following

E

1
K2

(cid:32)K−1
(cid:88)

k=0

(cid:33)2

∆Lkψk

=

1
K2

=

1
K2

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

E(∆Lkψk)2 +

E(∆Lkψk)2 +

2
K2

(cid:88)

i<j

2
K2

(cid:88)

i<j

E(∆Liψi)(∆Ljψj)

E[(∆Liψi)E[∆Ljψj|Fj]]

(12)

For the term E[∆Ljψj|Fj], we have

E[∆Ljψj|Fj] = E[(cid:104)∇V (θE

j )−n∇VIj (θE

j ), ∇rψj(cid:105)|Fj] = (cid:104)E[∇V (θE

j )−n∇VIj (θE

j )|Fj], ∇rψj(cid:105)

28

as ψj ∈ Fj. Then by Cauchy-Schwarz inequality, boundedness of ψ and the fact
(cid:107)∇V (θE
j )|Fj](cid:107)2 = O(h) as shown in the proof of Theorem 2, we conclude
E[∆Ljψj|Fj] = O(h).

j )−E[n∇VIj (θE

Now plug the above result in Equation (12), we have

E

1
K2

(cid:32)K−1
(cid:88)

k=0

(cid:33)2

∆Lkψk

=

1
K2

=

1
K2

=

1
K2

=

1
K2

=

1
K2

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

E(∆Lkψk)2 +

E(∆Lkψk)2 +

E(∆Lkψk)2 +

E(∆Lkψk)2 +

2
K2

2
K2

2
K2

(cid:88)

i<j

(cid:88)

i<j

(cid:88)

i<j

2
K2

(cid:88)

i<j

E(∆Lkψk)2 + O(h2)

E[(∆Liψi)E[∆Ljψj|Fj]]

E[∆Liψi]O(h)

O(h2)

O(h2)

Combine both cases of SGHMC and EWSG, we obtain

E

1
K2

(cid:32)K−1
(cid:88)

k=0

(cid:33)2

∆Lkψk

=

1
K2

K−1
(cid:88)

k=0

E(∆Lkψk)2 + O(h2)

Note that O(h2) term will later be combined with other error terms with the same order.

The ﬁnal piece is to bound 1
K2

(cid:80)K−1
k=0

E(∆Lkψk)2, and we have

1
K2

K−1
(cid:88)

k=0

E(∆Lkψk)2 =

≤

≤

=

≤

1
K2

1
K2

M 2
3
K2

M 2
3
K2

K−1
(cid:88)

k=0
K−1
(cid:88)

k=0
K−1
(cid:88)

k=0
K−1
(cid:88)

E(cid:104)∇V (θE

k ) − n∇VIk (θE

k ), ∇rψk(cid:105)2

E[(cid:107)∇V (θE

k ) − n∇VIk (θE

k )(cid:107)2

2 · (cid:107)∇rψk(cid:107)2
2]

(Cauchy-Schwarz inequality)

E[(cid:107)∇V (θE

k ) − n∇VIk (θE

k )(cid:107)2
2]

E[E[(cid:107)∇V (θE

k ) − n∇VIk (θE

k )(cid:107)2

2 | Fk]]

k=0
K−1
(cid:88)

2M 2
3
K2

(cid:124)

k=0

(cid:124)

E[ E[ (cid:107)∇V (θE

k ) − E[n∇VIk (θE
(cid:123)(cid:122)
Q1
k ) | Fk] − n∇VIk (θE

k )(cid:107)2

+ E[ (cid:107)E[n∇VIk (θE

k ) | Fk](cid:107)2 | Fk]
(cid:125)

]

2 | Fk]
]
(cid:125)

(cid:123)(cid:122)
Q2

29

The term Q1 captures the bias of stochastic gradient. For SGHMC, uniform gradient
subsamping leads to an unbiased gradient estimator, so Q1 = 0 for SGHMC. For EWSG,
same as in the proof of Theorem 2, we have that

E (cid:2) (cid:107)∇V (θE

k ) − E[n∇VIk (θE

k ) | Fk](cid:107)2 | Fk

(cid:3) = O(h2)

Combining two cases, we have

Q1 = O(h2)

For a random vector v with mean E[v] = 0, we have

E[(cid:107)v(cid:107)2] = E (cid:2)Tr[vvT ](cid:3) = Tr (cid:2)E[vvT ](cid:3) = Tr [cov(v)]

where cov(v) is the covariance matrix of random vector v. Therefore, we have that

Q2 = Tr [cov(n∇VIk |Fk)] ,

i.e., Q2 is the trace of the covariance matrix of stochastic gradient estimate conditioned
on current ﬁltration Fk.

Combining Q1 and Q2, we have that

E

1
K2

(cid:32)K−1
(cid:88)

k=0

∆Lkψk

(cid:33)2

≤

2M 2
3
K2

K−1
(cid:88)

(cid:2)E[Tr[cov(n∇VIk |Fk)]] + O(h2)(cid:3)

k=0
(cid:80)K−1
k=0

=

2M 2
3 h
T

E[Tr[cov(n∇VIk |Fk)]]

K

+ O(

h3
T

)

Now plug this bound into Equation (11) and we obtain

E(cid:0) ˆφK − ¯φ(cid:1)2 ≤ C1

1
T

+ C2

h
T

(cid:80)K−1
k=0

E [Tr[cov(n∇VIk |Fk)]]

K

+ C3h2

for some constants C1, C2, C3 > 0 depending on M1, M2, M3.

D Mini Batch Version of EWSG

When mini batch size b > 1, for each mini batch {i1, i2, · · · , ib}, we use n
j=1 ∇Vij
b
to approximate full gradient ∇V , and assign the mini batch {i1, i2, · · · , ib} probability
pi1i2,··· ,ib. We can easily extend the transition probability of b = 1 to general b, simply
by replacing n∇Vi with n
b

j=1 ∇Vij and end up with

(cid:80)b

(cid:80)b

˜P (θk+1, rk+1|θk, rk) = δ(θk+1 = θk + rkh)×

(cid:88)

i1,i2,··· ,ib

pi1i2···ibΦ (x + nai1i2···ib)

1
√

h

σ

30

where

x =

rk+1 − rk + hγrk
√

σ

h

, ai1i2···ib =

√

h
σ

1
b

b
(cid:88)

j=1

∇Vij (θk)

Therefore, to match the transition probability of underdamped Langevin dynamics with
stochastic gradient and full gradient, we let pi1i2···ib =
(cid:34)
(cid:107)x + nai1i2···ib (cid:107)2 − (cid:107)x +

ai1i2···ib (cid:107)2

exp

(cid:88)

(cid:35)(cid:41)

(cid:40)

1
Z

1
2

i1i2···ib

where Z is a normalization constant.

To sample multidimensional random data indices I1, · · · , Ib from pi1i2···ib, we again
use a Metropolis chain, whose acceptance probability only depends on ai1i2···ib and
aj1j2···jb but not the full gradient.

E EWSG Version for Overdamped Langevin

Overdamped Langevin equation is the following SDE
√

dθt = −∇V (θt)dt +

2dBt

where V (θ) = (cid:80)n
Maruyama discretization is

i=1 Vi(θ) and Bt is a d-dimensional Brownian motion. The Euler-

θk+1 = θk − h∇V (θk) +

√

2hξk+1

where ξk+1 is a d-dimensional random Gaussian vector. When stochastic gradient is
used, the above numerical schedme turns to

θk+1 = θk − h∇VIk (θk) +

√

2hξk+1

where Ik is the datum index used in k-th iteration to estimate the full gradient.
h∇Vi(θk)
2

Denote x = θk+1−θk√

. If we set

and ai =

√

√

2h

pi = P(Ik = i) ∝ exp (cid:8) −

(cid:107)x + (cid:80)n
j=1 aj(cid:107)2
2

+

(cid:107)x + nai(cid:107)2
2

(cid:9)

and follow the same steps in Sec.4.2, we will see the transition kernel of the full gradient
method being approximated by that of the stochastic gradient version.

F Variance Reduction (VR)

We have seen that when step size h is large, EWSG still introduces extra variance. To
further mitigate this inaccuracy, we provide in this section a complementary variance
reduction technique.

31

Locally (i.e., conditioned on the state of the system at the current step), we have

increased variance

cov[rk+1|rk] = E[cov[rk+1|I]] + cov[E[rk+1|I]]

= h(Σ2

k+1 + h cov[n∇VI (θk)])

(13)

k+1 = 1
h

E[cov[rk+1|I]]. The extra randomness due to the randomness of the
where Σ2
index I enters the parameter space through the coupling of θ and r and eventually
deviates the stationary distribution from that of the original dynamics. Adopting the
perspective of modiﬁed equation [Borkar and Mitter, 1999, Mandt et al., 2017, Li et al.,
2017], we model this as an enlarged diﬀusion coeﬃcient. To correct for this enlargement
and still sample from the correct distribution, we can either, in each step, shrink the size
of intrinsic noise to Σk ∈ Rd×d such that σ2I = Σ2
k + hcov[n∇VI (θk−1)], or alternatively
increase the dissipation. More precisely, due to the matrix version ﬂuctuation dissipation
theorem Σ2 = 2ΓT , one could instead increase the friction coeﬃcient Γ ∈ Rd×d rather
than shrinking the intrinsic noise. The second approach is computationally more eﬃcient
because it no longer requires square-rooting / Cholesky decomposition of (possibly large-
scale) matrices. Therefore, in each step, we set

Γk =

1
2T

(σ2I + hcov[n∇VI (θk−1)]).

Accurately computing cov[n∇VI (θk−1)] is expensive as it requires running I through
1, · · · , n, which defeats the purpose of introducing a stochastic gradient. To downscale
the computation cost from O(n) to O(1), we use an SVRG type estimation of the this
variance instead. More speciﬁcally, we periodically compute cov[n∇VI (θk−1)] only every
L data passes, in an outer loop. In every iteration of an inner loop, which integrates
the Langevin, an estimate of cov[n∇VI (θk−1)] is updated in an SVRG fashion. sof See
Algorithm 2 for detailed description. We refer variance reduced variant of EWSG as
EWSG-VR.

Figure 4: KL divergence

To demonstrate the performance of EWSG-VR, we reuse the setup of simple Gaus-
sian example in subsection 5.1. As shown in Algorithm 2, the only hyper-parameter

32

051015202530Number of Data Pass0.0000.0250.0500.0750.1000.1250.1500.1750.200KL DivergenceSGLDpSGLDSGULDEWSGEWSG-VRFGAlgorithm 2 EWSG-VR
1: Input: {number of data terms n, gradient functions ∇Vi(·), step size h, number of
data passes K, period of variance calibration L, index chain length M , friction and
noise coeﬃcients γ and σ}

2: initialize θ0, r0, γ0 = γ
3: initialize inner loop index k = 0
4: for l = 1, 2, · · · , K do
5:

if (l − 1) mod L = 0 then

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

compute m1 ← EI [n∇VI (θk)], m2 ← EI [n2∇VI (θk)∇VI (θk)T ]
ω ← θk

else

for t = 1, 2, · · · , (cid:100) n

M +1 (cid:101) do

i ← uniformly sampled from 1, · · · , n, compute and store n∇Vi(θk)
for m = 1, 2, · · · , M do

j ← uniformly sampled from 1, · · · , n, compute and store n∇Vj(θk)
i ← j with probability in Equation 6

end for
update (θk+1, rk+1) ← (θk, rk) according to Equation 3, using n∇Vi(θk) as
gradient and Γk as friction
m1 ← m1 + ∇Vi(θk) − ∇Vi(ω)
m2 ← m2 + n∇Vi(θk)∇Vi(θk)T − n∇Vi(ω)∇Vi(ω)T
covar ← m2 − m1mT
1
Γk+1 ← 1
k ← k + 1

2T (σ2I + h covar)

end for

end if
22:
23: end for

of EWSG-VR additional to EWSG is the period of variance calibration, for which we
set L = 1. All other hyper-parameters (e.g. step size h, friction coeﬃcient γ) are set
the same as EWSG. We also run underdamped Langevin dynamics with full gradient
(FG) using the same hyper-parameters of EWSG. We plot the KL divergence in Figure
4. We see that EWSG-VR further reduces variance and achieves better statistical accu-
racy measured in KL divergence. Although EWSG-VR periodically use full data set to
calibrate variance estimation, it is still signiﬁcantly faster than the full gradient version.
Note that KL divergence of SGLD, pSGLD and SGHMC are too large so that we can
not even see them in Figure 4

We also consider applying EWSG-VR to Bayesian logistic regression problems. We
run experiments on two standard classiﬁcation data sets parkinsons 8, pima9 from UCI
repository [Lichman et al., 2013].

From Figure 5, we see stochastic gradient methods (SGHMC, EWSG and EWSG-

8https://archive.ics.uci.edu/ml/datasets/parkinsons
9https://archive.ics.uci.edu/ml/datasets/diabetes

33

(a) parkinsons

(b) pima

Figure 5: Posterior prediction of mean (left) and standard deviation (right) of log like-
lihood on test data set generated by SGHMC, EWSG and EWSG-VR on two Bayesian
logistic regression tasks. Statistics are computed based on 1000 independent simulations.
Minibatch size b = 1 for all methods except FG. M = 1 for EWSG and EWSG-VR.

VR) only take tens of data passes to converge while full gradient version (FG) requires
hundreds of data passes to converge. Compared with SGHMC, EWSG produces closer
results to FG for which we treat as ground truth, in terms of statistical accuracy. With
variance reduction, EWSG-VR is able to achieve even better performance, signiﬁcantly
improving the accuracy of the prediction of mean and standard deviation of log likeli-
hood. It, however, converges slower than EWSG without VR.

One downside of EWSG-VR is that it periodically use whole data set to calibrate
variance estimation, so it may not be suitable for very large data sets (e.g. Covertype
data set used in subsection 5.2) for which stochastic gradient methods could converge
within one data pass.

G Additional Experiments

G.1 A Misspeciﬁed Gaussian Case

In this subsection, we follow the same setup as in [Bardenet et al., 2017] and study
a misspeciﬁed Gaussian model where one ﬁts a one-dimensional normal distribution
p(θ) = N (θ|µ0, σ2
0) to 105 i.i.d points drawn according to Xi ∼ log N (0, 1), and ﬂat
prior is assigned p(µ0, log σ0) ∝ 1. It was shown in [Bardenet et al., 2017] that FlyMC
algorithm behaves erratically in this case, as “bright” data points with large values are
rarely updated and they drive samples away from the target distribution. Consequently
the chain mixes very slowly. One important commonality FlyMC shares with EWSG is
that in each iteration, both algorithms select a subset of data in a non-uniform fashion.
Therefore, it is interesting to investigate the performance of EWSG in this misspeciﬁed
model.

For FlyMC10, a tight lower bound based on Taylor’s expansion is used to minimize
“bright” data points used per iteration. At each iteration, 10% data points are resam-
pled and turned “on/oﬀ” accordingly and the step size is adaptively adjusted. FlyMC
algorithm is run for 10000 iterations. Figure 6a shows the histogram of number of data

10https://github.com/rbardenet/2017JMLR-MCMCForTallData

34

0102030395400number of data pass0.40.60.81.01.21.4mean of posterior log likelihoodSGULDEWSGEWSG-VRFG0102030395400number of data pass0.00.10.20.30.40.50.60.70.80.9std of posterior log likelihoodSGULDEWSGEWSG-VRFG0246810398400number of data pass0.40.60.81.01.21.4mean of posterior log likelihoodSGULDEWSGEWSG-VRFG0246810398400number of data pass0.00.10.20.30.40.5std of posterior log likelihoodSGULDEWSGEWSG-VRFGpoints used in each iteration for FlyMC algorithm. On average, FlyMC consumes 10.9%
of all data points per iteration. For fair comparison, the minibatch size of EWSG is
hence set 105 × 10.9% = 10900 and we run EWSG for 1090 data passes. We set step
size h = 1 × 10−4 and friction coeﬃcient γ = 300 for EWSG. An isotropic random walk
Metropolis Hastings (MH) is also run for suﬃciently long and serves as the ground truth.
Figure 6b shows the autocorrelation of three algorithms. The autocorrelation of
FlyMC decays very slowly, samples that are even 500 iterations away still show strong
correlation. The autocorrelation of EWSG, on the other hand, decays much faster,
suggesting EWSG explores parameter space eﬃciently than FlyMC does. Figure 6c
and 6d show the samples (the ﬁrst 1000 samples are discarded as burn-in) generated
by EWSG and FlyMC respectively. The samples of EWSG center around the mode
of the target distribution while the samples of FlyMC are still far away from the true
posterior. The experiment shows EWGS works quite well even in misspeciﬁed models,
and hence is an eﬀective candidate in combining importance sampling with scalable
Bayesian inference.

G.2 Additional Results of BNN Experiment

We report the test error of various SG-MCMC methods after 200 epochs in Table 2. For
both MLP and CNN architecture, EWSG outperforms its uniform counterpart SGHMC
as well as other benchmarks SGLD, pSGLD and CP-SGHMC. The results clearly demon-
strate the eﬀectiveness of the proposed EWSG on deep models.

Table 2: Test error (mean ± standard deviation) after 200 epoches.

Method
SGLD
pSGLD
SGHMC
CP-SGHMC
EWSG

Test Error(%), MLP Test Error(%), CNN

1.976 ± 0.055
1.821 ± 0.061
1.833 ± 0.073
1.835 ± 0.047
1.793 ± 0.100

0.848 ± 0.060
0.860 ± 0.052
0.778 ± 0.040
0.772 ± 0.055
0.753 ± 0.035

G.3 Additional Experiment on BNN: Tuning M

In each iteration of EWSG, we run an index Markov chain of length M and select a
“good” minibatch to estimate gradient, therefore EWSG essentially uses b × (M + 1)
data points per iteration where b is minibatch size. How does EWSG compare with its
uniform gradient subsampling counterpart with a larger minibatch size (b × (M + 1))?
We empirically answer this question in the context of BNN with MLP architecture.
We use the same step size for SGHMC and EWSG and experiment a large range of
values of minibatch size b and index chain length M . Each algorithm is run for 200
data passes and 10 independent samples are drawn to estimate test error. The results
are shown in Table 3. We ﬁnd that EWSG beats SGHMC with larger minibatch in 8

35

(a) Histogram

(b) Autocorrelation

(c) Samples of EWSG

(d) Samples of FlyMC

Figure 6: (a) Histogram of data used in each iteration for FlyMC algorithm. (b) Au-
tocorrelation plot of FlyMC, EWSG and MH. (c) Samples of EWSG. (d) Samples of
FlyMC.

out of 9 comparison groups, which suggests in general EWSG could be a better way
to consuming data compared to increasing minibatch size and may shed light on other
areas where stochastic gradient methods are used (e.g. optimization).

H EWSG does not necessarily change the speed of conver-

gence signiﬁcantly

Changing the weights of stochastic gradient from uniform to non-uniform, as we saw, can
increase the statistical accuracy of the sampling; however, it does not necessarily increase
or decrease the speed of convergence to the (altered) limiting distribution. Numerical
examples already demonstrated this fact, but on the theoretical side, we note the non-

36

1000012000140001600018000NumberofDataUsedperIteration0500100015002000250030003500Count0100200300400500Lag0.00.20.40.60.81.0AutocorrelationFlyMCEWSG1.631.641.651.661.671.682.1502.1552.1602.1652.1702.1752.1802.185EWSGGroundTruth1.621.631.641.651.661.671.682.152.162.172.182.19FlyMCGroundTruthb M + 1 = 2 M + 1 = 5 M + 1 = 10

1.86%
1.94%

1.90%
1.87%

1.79%
1.97%

100

200

500

1.83%
1.92%

1.87%
1.97%

2.01%
2.17%

1.80%
1.97%

1.80%
2.07%

2.36%
2.37%

Table 3: Test errors of EWSG (top of each cell) and SGHMC (bottom of each cell) after
200 epoches. b is minibatch size for EWSG, and minibatch size of SGHMC is set as
b × (M + 1) to ensure the same number of data used per parameter update for both
algorithms. Step size is set h = 10
b(M +1) as suggested in [Chen et al., 2014], diﬀerent from
that used to produce Table 2. Results with smaller test error is highlighted in boldface.

asymptotic bound provided by Theorem 3 may not be tight in terms of the speed of
convergence due to its generality. Therefore, here we quantify the convergence speed on
a simple quadratic example:
Consider Vi(θ) = 1

n (θ−µi)2/2 where µi’s are constant scalars. Assume without loss of
generality that (cid:80)
i=1 Vi(θ) = θ2/2 + some constant. We will
show the convergence speed of Eθ is comparable for uniform and a class of non-uniform
SG-MCMC (including EWSG) applied to second-order Langevin equation (overdamped
Langevin will be easier and thus omitted):

i µi = 0, and thus V (θ) = (cid:80)n

Theorem 5 Consider, for 0 < γ < 2, respectively SGHMC and EWSG,

(cid:40)

k+1 = θ(cid:48)
θ(cid:48)
k+1 = r(cid:48)
r(cid:48)

k + hr(cid:48)
k
k − hγr(cid:48)

k − h(θ(cid:48)

k − µI (cid:48)

k

√

) +

hσξ(cid:48)

k+1

and

(cid:40)

θk+1 = θk + hrk
rk+1 = rk − hγrk − h(θk − µIk ) +

√

,

hσξk+1

where I (cid:48)
variable on [n] satisfying P(Ik = i) = 1/n + O(hp), and ξk+1, ξ(cid:48)
Gaussian random variables. Denote by ¯θ(cid:48)
k = [ ¯θ(cid:48)
x(cid:48)

k are i.i.d. uniform random variable on [n], Ik are [θ, r] dependent random
k+1 are standard i.i.d.
k, ¯θk = Eθk, ¯rk = Erk,

k]T , and xk = [¯θk, ¯rk]T , then

k = Eθ(cid:48)

k = Er(cid:48)

k, ¯r(cid:48)

k, ¯r(cid:48)

k = (I + Ah)kx(cid:48)
x(cid:48)

0, where A =

(cid:21)

(cid:20) 0
1
−1 −γ

,

(14)

for small enough h, (cid:107)x(cid:48)
a comparable speed in the sense that (cid:107)xk − x(cid:48)

k(cid:107) converges to 0 exponentially with k → ∞, and xk converges at
k(cid:107) = O(hp) if x0 = x(cid:48)
0.

Proof: Taking the expectation of the [θ(cid:48), r(cid:48)] iteration and using the fact that (cid:80)
and hence EµI (cid:48)
= 0, one easily obtains (14). The geometric convergence of x(cid:48)

i µi = 0
k thus

k

37

follows from the fact that eigenvalues of I + Ah have less than 1 modulus for small
enough h.

Let ek = [0, EµIk ]T and then

ek = [0,

n
(cid:88)

i=1

P(Ik = i)µi]T = [0, O(hp)]T

Now we take the expectation of both sides of the [θ, r] iteration and obtain xk+1 =
(I + Ah)xk + hek. Therefore

xk =(I + Ah)kx0 + (I + Ah)k−1he0 + · · · + (I + Ah)hek−2 + hek−1

=x(cid:48)

k + h(cid:0)(I + Ah)k−1e0 + · · · + (I + Ah)ek−2 + ek−1

(cid:1)

To bound the diﬀerence, note I + Ah is diagonalizable with complex eigenvalues λ1,2
satisfying

|λ1| = |λ2| =

1 − hγ + h2 = 1 − γh/2 + O(h2).

(cid:112)

Projecting ej to the corresponding eigenspaces via ej = v1,j + v2,j, we can get

h(cid:107)(I + Ah)k−1e0 + · · · + ek−1(cid:107) ≤ h

(cid:16)

(cid:107)(I + Ah)k−1e0(cid:107) + · · · + (cid:107)ek−1(cid:107)

(cid:17)

(cid:16)

|λ1|k−1(cid:107)v1,0(cid:107) + |λ2|k−1(cid:107)v2,0(cid:107) + · · · + (cid:107)v1,k−1(cid:107) + (cid:107)v2,k−1(cid:107)

(cid:17)

= h
≤ hChp(|λ1|k−1 + · · · + 1) = hChp 1 − |λ1|k
1 − |λ1|
≤ ˆChp

≤ hChp

1
1 − |λ1|

for some constant C and ˆC.

Important to note is, although this is already a nonlinear example for EWSG (as
nonlinearity enters through the µIk term), it is a linear example for SGHMC. For the
fully nonlinear cases, a tight quantiﬁcation of EWSG’s convergence speed remains to
be an open theoretical challenge (a loose quantiﬁcation is already given by the general
Theorem 3).

38

