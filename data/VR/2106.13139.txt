FaDIV-Syn: Fast Depth-Independent View Synthesis
using Soft Masks and Implicit Blending

Andre Rochow
University of Bonn
rochow@ais.uni-bonn.de

Max Schwarz
University of Bonn
schwarz@ais.uni-bonn.de

Michael Weinmann
Delft University of Technology

Sven Behnke
University of Bonn

2
2
0
2

y
a
M
3
1

]

V
C
.
s
c
[

3
v
9
3
1
3
1
.
6
0
1
2
:
v
i
X
r
a

Abstract—Novel view synthesis is required in many robotic
applications, such as VR teleoperation and scene reconstruction.
Existing methods are often too slow for these contexts, cannot
handle dynamic scenes, and are limited by their explicit depth
estimation stage, where incorrect depth predictions can lead to
large projection errors. Our proposed method runs in real time
on live streaming data and avoids explicit depth estimation by
efﬁciently warping input images into the target frame for a range
of assumed depth planes. The resulting plane sweep volume
(PSV) is directly fed into our network, which ﬁrst estimates
soft PSV masks in a self-supervised manner, and then directly
produces the novel output view. This improves efﬁciency and
performance on transparent, reﬂective, thin, and feature-less
scene parts. FaDIV-Syn can perform both interpolation and
extrapolation tasks at 540p in real-time and outperforms state-of-
the-art extrapolation methods on the large-scale RealEstate10k
dataset. We thoroughly evaluate ablations, such as removing
the Soft-Masking network, training from fewer examples as
well as generalization to higher resolutions and stronger depth
discretization. Our implementation is available1.

I. INTRODUCTION
Novel view synthesis (NVS) aims to estimate images from
novel viewpoints of a scene from images captured from one
or more reference views.

This is of great relevance for numerous applications in
virtual reality, 3D movies, computer games, and other areas
where images have to be generated efﬁciently under arbitrarily
chosen viewpoints. VR teleoperation of robots is a speciﬁc
application (see Fig. 1). Here NVS can be employed to re-
duce latencies (e.g., instantaneously reacting to the viewpoint
changes by the operator), to compensate camera workspace
limits, or to generate third-person views. However, this ap-
plication places special constraints on the NVS algorithm:
Movement latency needs to be minimal to avoid VR sickness,
i.e. novel views have to be generated in real time. On the other
hand, scene latency, i.e. the time from image capture to display
also needs to be minimal to allow efﬁcient telemanipulation.
This effectively rules out approaches which have a costly
preprocessing step that generates a suitable representation for
later interpolation. Note that NVS for VR teleoperation is of
high importance for recently emerging Avatar systems [1, 2].
However, synthesizing novel views of a sparsely captured
scene is challenging, as scene geometry and surface properties
are unknown a priori and have to be inferred from the input
views. Additionally, viewpoint changes induce both occlu-
sions, where foreground objects occlude previously visible

1https://github.com/AIS-Bonn/fadiv-syn

?

? L R

Fig. 1: FaDIV-Syn extrapolates from two cameras (L,R) mounted in
a stereo setup with human baseline (6.5 cm) to a pose outside of the
baseline (denoted ?). The network was trained on RealEstate10k [3]
and generalizes to robotic teleoperation scenarios.

backgrounds, and disocclusions, which uncover previously
invisible backgrounds. In the latter case,
the disoccluded
content has to be guessed from its context.

Our work is focused on the problem setting of real-time
view interpolation and extrapolation from two RGB images,
which show the scene from roughly the same direction—as,
for example, when captured from a stereo camera mounted
on a robotic system. We note, though, that we also show
applicability to more input views.

Many stereo [4–7] and view synthesis [3, 8, 9] methods
use Plane Sweep Volumes (PSVs) [10], which warp the input
views on a range of parallel planes and thus pre-transform the
input data under the assumption of a range of discrete depths.
Usually, a disparity or depth map is estimated from the PSV,
which is then used to project the input views into the target
frame [9] or to generate representations such as multi-plane
images [3, 8]. This approach exhibits a fundamental bottle-
neck, however: Imprecise or wrong depth estimates, which
occur especially on uniform, transparent, thin, or reﬂective
surfaces, will result in loss of information and lead to failures
later on in the synthesis pipeline.

Our proposed method is related to Image-Based Rendering
(IBR) approaches, but forgoes the geometry estimation step by
directly operating on the PSV in the target view to produce

 
 
 
 
 
 
the output RGB image, without computing explicit depth. To
this end, we learn an RGB generator network which processes
the PSV to directly synthesize the novel view and equip it
with operations such as group convolutions and gated con-
volutions [11, 12], which are suitable for detecting layer-wise
correspondences, masking of irrelevant areas within the layers,
and blending. Unlike most IBR approaches, FaDIV-Syn has no
explicit blending or inpainting stage, which avoids early and
hard decisions, allowing the distribution of the blending and
inpainting operations throughout the learned network. Only
a single forward pass is required for view interpolation and
extrapolation. Further, we demonstrate how gated convolutions
can be used to learn self-supervised soft masks, which are
predicted by a lightweight CNN to give a soft correspondence
measure, which improves accuracy and provides insight on the
network’s method of operation.

In summary, our contributions include 1) a real-time view
synthesis method operating on plane sweep volumes without
requiring explicit geometry, 2) self-supervised learning of soft-
masks by introducing a gating module, and 3) a detailed
evaluation on the large-scale RealEstate10k dataset [3], where
we demonstrate our approach to outperform existing methods
in terms of accuracy and runtime.

II. RELATED WORK

Novel view synthesis has gained much attention in recent
years and a large variety of approaches has been presented.
For a broader review, we refer to surveys by Nguyen et al.
[13] and Tewari et al. [14].

Image Based Rendering (IBR): In contrast to classical
rendering of 3D scenes using textured geometry, IBR methods
render novel views by combining input images in the target
pose [15–19, 9, 13, 20–24]. To be able to project the input
images correctly, IBR methods still require geometry, often
in the form of depth maps, which are either available or
estimated. Recent approaches use blending to combine the
images [15–17, 22]. Hedman et al. [16] learn the blending
operation end-to-end. Riegler and Koltun [22] use a recurrent
blending decoder in order to deal with a varying number
of input images. In recent work [23], heuristic input image
selection is replaced with a fully-differentiable synthesis block.
Penner and Zhang [15] introduce soft visibility volumes, which
encode occlusion probabilities and thus avoid early decisions,
but require a larger number of input views. Kalantari et al.
[9] synthesize novel views in light ﬁeld datasets. They use the
corner cameras to predict depth in the target view, which is
then used to warp the input views. Nguyen et al. [13] introduce
RGBD-Net, which ﬁrst estimates depth using a multi-scale
PSV, warps the input images into the target frame, performs
explicit blending, and reﬁnes the warped image using a depth-
aware network. Similar to our approach, Flynn et al. [24] also
directly build a PSV in the target view. Color is fused for each
PSV plane pair and a separate network estimates depth prob-
abilities for blending. Our method also works directly on the
input images, but does not compute or require depth explicitly.

Blending is learned implicitly by the network, together with
detection and inpainting of extrapolated/disoccluded regions.
Geometry-based Approaches: Recent geometry-based ap-
proaches [25–28] use depth features to spatially project pixel
information and reﬁne these projections to a target view. Wiles
et al. [27] and Chen et al. [29] process single input images,
estimating monocular depth in an end-to-end fashion. Wiles
et al. [27] implement a differentiable point cloud renderer
that allows z-buffering and splatting. Chen et al. [29] predict
depth in the target view using a transforming auto-encoder
that explicitly transforms the latent code before entering the
decoder. Similarly, Olszewski et al. [30] learn implicit voxel
representations and transform encoded representations explic-
itly. Srinivasan et al. [28] predict RGB-D light ﬁelds from
a single RGB image. They estimate precise scene geometry,
render it to the target frame, and predict occluded rays using a
second CNN. Choi et al. [26] predict depth probabilities along
camera rays in multiple input images and unite them in the
target camera pose. They discretize the number of possible
depth values and therefore reduce the depth estimation prob-
lem to a classiﬁcation problem. A more recent approach [31]
learns novel view synthesis without target view supervision
by performing two synthesis steps, initially to an arbitrary
target pose and from there to a pose where ground truth
is available. In contrast to these methods, FaDIV-Syn does
not feature an explicit geometry representation. We argue that
explicit geometry—besides requiring a higher computational
effort—forces early resolution of ambiguities, which can lead
to loss of information.

Multiplane and Layered Depth Images (MPIs/LDIs):

A multiplane image consists of multiple depth planes, which
store RGB and alpha values. Once computed for a set of input
images, novel views can be synthesized very efﬁciently by
warping and blending the individual layers. One can attempt
to predict MPIs from single input images [32, 33]. Tucker
and Snavely [33] train a network to estimate scale-invariant
depth and require additional sparse point clouds to recover
scale. Multiview approaches [8, 3, 34] use information from
additional camera poses to place surfaces at the correct MPI
layer. Plane sweeping [3] or warping [8] at different depths
creates a suitable representation for the network. Mildenhall
et al. [34] blend the layers of multiple MPIs to generate
novel views with local
light ﬁelds. Recently, Attal et al.
[35] extended the key idea of multiplane images to multi-
sphere images in order to synthesize 360◦ images in real-time,
however, at lower resolution.

Other approaches [36–38] build on Layered Depth Images
(LDI) [39], which store multiple RGB and depth values per
pixel. Snavely et al. [36] train a CNN to predict a two-layered
LDI, where the network learns to predict occluded pixels.
Shih et al. [37] use an LDI representation to turn a single
image into a 3D photo by inpainting color and depth of the
occluded areas. While inspired by MPI approaches, FaDIV-
Syn bypasses MPI generation and instead determines a novel
view from multiple warped planes directly. It is applicable for
dynamic scenes in real-time.

Fig. 2: The FaDIV-Syn architecture. (a) Input images (gray) are projected into the target camera (black) for each depth plane (red/green)
deﬁned in the target frame. For a particular surface in the scene (blue circle), there will be a depth plane where the projections most closely
align (green). This plane corresponds to the approximate object depth. (b) The resulting projected images are stacked and fed into the view
synthesis network (c), which directly predicts the target image.

Neural Rendering: In recent years, view synthesis ap-
proaches based on Neural Radiance Fields (NeRF) [40] have
been introduced, which employ a neural network as a learnable
density and radiance function over the scene volume. Novel
views can be synthesized using classical volume rendering
techniques. The sub-sequent improvements [41, 42, 19, 43]
show impressive results on a variety of scenes and recent de-
velopments also achieve real-time inference [44–47]. However,
with the exception of Wang et al. [19] and Sitzmann et al. [47],
NeRFs typically have to be trained with hundreds of images of
the target scene, making them unsuitable for dynamic scenes.
While methods designed for dynamic scenes exist [43, 48–50],
they require ofﬂine training or processing phases as well.

III. PROPOSED METHOD

The key idea of FaDIV-Syn, illustrated in Fig. 2, is to pre-
process and transform the input images into the target frame,
without making early and hard decisions. Of course,
this
transformation requires depth information. Instead of explicitly
estimating depth, which exhibits problems for transparent,
reﬂective, thin, and featureless surfaces, we sample multiple
depth values and present the resulting possibilities to a deep
neural network for learning image synthesis. The induced
multi-plane representation is well-suited for view synthesis.

We present and evaluate our method for two input images
(in the following denoted by I1 and I2) and one output image
(IO). This scenario commonly occurs in many contexts, such
as robotics, AR/VR, and mobile devices. We note though, that
additional input views enhance the performance further (see
Sec. IV-B). For all input cameras, we sample N depth planes
and for each plane assume that the entire image lies on it.
When projecting these planes into the target view, one could,
for N → ∞, determine each pixel’s 3D position by searching
for correspondences in the warped planes (see Figs. 2 and 4).
When performing the synthesis task with a learned network,
we can reduce N to a small number, since the network can
learn to interpolate between planes with adjacent depth levels.

A. Plane Sweep Volumes for View Synthesis

Planar geometry is especially well-suited for camera-to-
camera projection, since the resulting warping operation can be

performed efﬁciently. We deﬁne the planes in the target image
IO (see Fig. 2), as it is commonly done in plane sweeping
multi-view stereo approaches [10]. For each plane i with depth
di, we deﬁne P (i)
as the image resulting from projecting Ik
k
onto the plane, and then into IO. Using this representation,
we can deﬁne a “hard-wired” view synthesis method f :

f (I1, I2, p) =

(cid:40)

1

P (D(p))
(p)
g(I1, I2, p)

if p visible in I1, I2
otherwise ,

D(p) = arg max

j

Q(P (j)

1 , P (j)

2 , p),

(1)

(2)

where p = (x, y) is a pixel in the target image IO, g is an
inpainting method, D is the (internal) depth estimate, Q is
a correspondence quality estimator, and j denotes the plane
with optimal correspondence. Note that only images in P
of the same depth need to be compared. Figure 4 shows
an exemplary PSV where the idea presented in Eq. (1) is
immediately apparent. Since perfect Q and g are, however,
not known, we will learn a CNN approximating f .

Depth Discretization: For increasingly distant objects,
the spatial error of projected pixels caused by wrong depth
decreases. Therefore, it is legitimate to set a maximum depth.
Our most efﬁcient network uses 19 depth planes, which is
much less compared to related methods [8, 3] but achieves
signiﬁcantly better accuracy (see Sec. IV). The depths are
sampled in disparity space, where we linearly interpolate
within a chosen disparity range.

B. View Synthesis Network

When generating a novel view directly from a PSV, dis-
occlusion areas are not trivial to determine. Thus, our net-
work must
learn to distinguish between (1) areas of suf-
ﬁcient correspondences in the warped planes, (2) areas of
no correspondence but sufﬁcient correspondence in different
planes, and (3) areas of disocclusion and occlusion. Hence,
the network must
learn under the constraint of geometric
consistency to fuse and correct sufﬁciently corresponding areas
from warped planes and recognize inpainting areas to ﬁll them
with realistic content. In contrast to Thies et al. [21], who also
learn implicit blending, we avoid early and hard decisions in a

Input 2
Fig. 3: View extrapolation on RealEstate10k [3] test set. Please see our supplementary video for an animated version of this ﬁgure that
shows both inter- and extrapolation along a trajectory.

Extrapolated view

Input 1

1.35 m

2.20 m

4.00 m

7.60 m

Fig. 4: Plane sweep volume (PSV). Four planes of the scene in Fig. 3 with α-blended projections of the two input images. Note that
RealEstate10k only provides estimated global scale, so the given plane distances are up to a scale. Exemplary areas with good correspondence
between the two input views are marked in red. Forwarding the corresponding regions would already yield an approximate solution.

depth estimation stage, which may lead to quality degradation
later in the pipeline.

Our network is divided into several components (see Fig. 5):
A soft-masking network, a gating section, and the ﬁnal fusion
network.

1) Soft-Masking Network: The Soft-Masking (SM) network
operates on the input PSV and generates approximate PSV
masks that guide the further synthesis process. Ideally, the
output mask m should be the correspondence quality measure
Q(P (j)
2 ) (see Eq. (1)). In practice, the entire pipeline is
trained end-to-end without direct supervision on the masks.

1 , P (j)

As shown in Fig. 6, the SM network learns meaningful
correspondence masks that correlate closely with depth in the
scene. Since there is a large amount of overlap, we conclude
that the later fusion network uses the inferred masks mostly
to eliminate areas of poor correspondence, but keeps multiple
possibilities to make ﬁnal decisions in deeper layers. We thus
denote these representations as soft masks.

To generate m, we implement an hourglass network module
with four downsampling and four upsampling blocks. Each
block has one convolution followed by average pooling or
bilinear upsampling. The module processes the PSV (P1, P2)
as 2 × N input planes in grayscale and outputs 2 × N
feature maps. For each plane, we encourage the network to

PSV

SM-Net

Gating

m

˜m

Fusion-Net

GGC2

·

Output

P1, P2

GGC1

P1

P2

Fig. 5: View synthesis network architecture. The Soft-Masking (SM)
network computes layer-wise masks from the PSV, which are used
for gating. The fusion network then produces the ﬁnal output image
from the gated PSV.

learn a binary classiﬁcation mask by applying the softmax
function (separately for each plane). For details we refer to
the supplementary material.

2) Learned Gating: We send the entire PSV through a
gating section. This way, the network can eliminate areas of
low correspondence.

Gated convolutions [11] have recently shown promising
results in image inpainting [12]. Instead of C(Wf , I) =
σ(Wf (cid:126) I), a gating layer calculates the non-linear output

GC(Wf , Wg, I) = σ1(Wf (cid:126) I) (cid:12) σ2(Wg (cid:126) I),

(3)

where (cid:12) denotes the element-wise multiplication and (cid:126) the
convolution. Wg, Wf are two different convolutional ﬁlters,
and σ1, σ2 are activation functions. The formula shows that
gating directly inﬂuences the actual feature extraction and can
thus adapt it to the context. Gating layers can help the network
especially with performing masking-like operations (e.g. when
recognizing corresponding depth planes), performing blend-
ing, or determining inpainting areas [12].

To avoid hard decisions, we allow the gating stage to
incorporate the mask m in a learned fashion by adding
convolutional layers that can modify the mask and features
before the actual multiplication (see Fig. 5). We note that the
network may also learn to ignore the provided mask.

Since the depth planes are aligned, we can be sure that
corresponding areas only appear in corresponding planes (see
Fig. 2). Hence, we can restrict
the convolutions through
grouping under the assumption that other planes are initially
irrelevant to the considered plane pair.

We concatenate the estimated masks m to the input plane
and activate the gating feature extraction with

pairs P (j)
a sigmoid (σ), which creates a mask ˜m ∈ [0, 1]:

1 , P (j)

2

9.5 m

4.3 m

3.8 m

3.0 m

2.8 m

2.5 m

2.2 m

Fig. 6: Self-supervised soft masking. Predicted image (left) and learned mask activations (right). The masks are normalized (dark = high
activation). The PSV layer depth is up to scale. Note how the learned masks correlate with depth in the scene.

2.0 m

1.9 m

1.7 m

1.6 m

1.5 m

1.4 m

1.3 m

GGC (j)

1 = ReLU(W1 (cid:126) [P (j)
1 , P (j)
˜m(j) = σ(W2 (cid:126) GGC (j)
1 ),

2 , m(j)]),

GGC (j)

2 = ReLU(W3 (cid:126) GGC (j)

1 ) (cid:12) ˜m(j),

(4)

(5)

(6)

where W denotes the learned weights.

By allowing self-supervised learning of a suitable mask
presentation and incorporation, we thus (i) keep the key idea
of our approach to not estimate depth explicitly, (ii) keep the
complexity of the mask generator bounded, and (iii) avoid
early projection errors due to the depth discretization.

3) Fusion Network: Finally, the gated PSV features enter
the fusion and inpainting module (Fusion-Net
in Fig. 5).
is based on a U-Net [51] architecture and consists of
It
four downsampling and four upsampling blocks with skip
connections and a dilated convolution in the middle. For
upsampling, we use bilinear interpolation. Each downsampling
block consists of two convolutions with batch normalization
and ReLU activation. The second convolution has a stride of
two for downsampling. The ﬁnal output of the fusion network,
following a tanh activation, is the predicted RGB image ˆIO.
Variant without soft masking (NoSM): To validate the
effect of soft masking, we also tested generating novel views
without the SM network. Here, the gating section receives only
P1, P2 as input. In this setup, the network must learn to spread
the correspondence estimation throughout the layers. We there-
fore use Gated Convolutions in each downsampling layer and
increase the number of feature-maps in deeper layers. Note
that both manipulations are parameter-intensive and increase
the accumulated number of parameters by approximately 33%-
55%, depending on the number of PSV planes.

Architecture details can be found in the supplementary.

C. Training

The whole pipeline is trained end-to-end in a supervised
manner from a triple (I1, I2, IO) with known camera poses
and intrinsics. We deﬁne the loss function

L( ˆIO, IO) = λ1L1( ˆIO, IO) + λpLperceptual ( ˆIO, IO),

(7)

where the perceptual loss is based on a VGG-19 [52] network
Ψ pretrained on ImageNet. It is deﬁned as

Lperceptual ( ˆIO, IO) =

L
(cid:88)

l

wl
NΨl

|Ψl( ˆIO) − Ψl(IO)|,

(8)

where Ψl(·) is the activation of the l-th layer, wl is a weight
factor of the l-th layer and NΨl are the number of elements

in the l-th layer. We train the network with a batch size of
20, a learning rate of 1e-4, and the Adam optimizer with
β1,2 = (0.4, 0.9) on two NVIDIA A6000 GPUs with 48 GiB
RAM. Training takes four days for images of 288p resolution.
Training for a higher resolution of 576p is only possible with
a batch size of six and takes up to three weeks.

For 288p images, we train the networks for 300k-350k
iterations, whereas for 576p images we increase the number
of iterations accordingly to adjust to the smaller batch size.
Within this range, we use early stopping based on the valida-
tion score to select the model for evaluation.

IV. EVALUATION

We train and evaluate our method on the challenging
RealEstate10k dataset introduced by Zhou et al. [3], which
contains approx. 75k video clips extracted from YouTube
videos, showing mostly indoor scenes. The large variety in the
dataset allows generalizing to different scenarios (see Fig. 1).
All videos have been automatically annotated with camera
intrinsics and camera trajectories using ORB-SLAM2 [53] and
bundle adjustment. Monocular SLAM cannot recover global
scale, so the sequences have been scaled so that the near
geometry lies at approx. 1.25 m [3]. We divide the ofﬁcial train
split further into 54k training and 13.5k validation sequences.
All our tests are done using the ofﬁcial test split, where the
extrapolation experiments use the data provided by Shih et al.
[37]. For all experiments, we evaluate the quality of generated
images with the PSNR, SSIM [54], and LPIPS [55] metrics.

A. Extrapolation

The ﬁrst, more challenging task is extrapolation, i.e. the
target view is outside of the provided input views. We start
from a pre-trained interpolation network (see Sec. IV-B) and
add extrapolation sequences. Extrapolation and interpolation
triplets are mixed in the ratio 80% to 20% during training.
These are randomly sampled from video sequences, ensuring
a distance d1 ∈ [3, 5] between the input frames I0, I1, as well
as that the target frame IO is d2 ∈ [5, 7] frames after I1.

Shih et al. [37] evaluated an array of related methods for
extrapolation tasks. In order to test view synthesis accuracy,
they generated 1500 random triplets from the test data set.
We follow the same evaluation protocol. Note that
these
experiments are carried out at a resolution of 1024×576.

SSIM↑ PSNR↑ LPIPS↓ Params min max

Depth [m]

p
6
7
5

Method
Stereo-Mag [3] (cid:88) .8906
(cid:88) .8773
PB-MPI [8]
(cid:88) .8062
LLFF [34]
(cid:88) .8628
Xview [26]
3D-Photo [37] (cid:88) .8887
(cid:88) .9036
Ours-32
Ours-32
.9020
Ours-32-NoSM (cid:88) .8891
.8865
Ours-32-NoSM
(cid:88) .9007
Ours-19
.8985
Ours-19
Ours-19-NoSM
.8801
Ours-17-NoSM (cid:88) .8790
.8750
Ours-17-NoSM

Input 1

26.71
25.51
23.17
24.75
27.29

29.41
29.39
28.83
28.67
29.25
29.04
28.30
28.13
27.96

.0826
.0902
.1323
.0822
.0724

.0521
.0556
.0613
.0650
.0531
.0583
.0679
.0674
.0695

17M 1.0 100
6M 1.0 100
682K 1.0 100
-
58M
-
119M

-
-

12M 1.0 100
12M 1.0 100
16M 1.0 100
16M 1.0 100
9M 1.0 100
9M 1.0 100
14M 1.0 100
16
14M 0.3
16
14M 0.3

Input 1

Input 1

Input 2

Extrapolation

Input 2

Input 1

Input 2

Extrapolation

Input 2

Extrapolation

Extrapolation

TABLE I: Extrapolation results on RealEstate10k [3]. All variants
without (cid:88)are trained with 288p, but evaluated on 576p. Ours-N
denotes a network with N PSV planes, while NoSM refers to
ablations without the Soft-Masking network.

∆t=5

Depth [m]

Method

PSNR↑ SSIM↑ LPIPS↓ min max

Stereo-Mag [3]

28.20

.9209

.0783

32.71
Ours-32
Ours-32-NoSM 32.02
Ours-19
32.17
Ours-19-NoSM 31.05

.9448
.9343
.9433
.9241

.0361
.0429
.0386
.0464

1.0

1.0
1.0
1.0
1.0

100

100
100
100
100

TABLE II: Interpolation results. Our networks are trained for 288p
but evaluated in twice the resolution, i.e. 576p. We compare against
Stereo-Mag [3], which was trained for higher resolution.

Results: As shown in Table I, we outperform current
state-of-the-art methods on RealEstate10k by signiﬁcant mar-
gins in all metrics. Reducing the number of depth layers
decreases the performance only slightly (Ours-19). To evaluate
the Soft-Masking network, we also evaluate ablations without
it (NoSM). However, removing the Soft-Masking network
results in a noticeable drop in performance. We note that
NoSM variants still achieve signiﬁcantly higher PSNR and
LPIPS values than related methods (see Table I), however, they
score lower (or similar) in SSIM compared to Stereo-Mag [3]
and 3D-Photo [37].

We also note that FaDIV-Syn can be trained at half reso-
lution and evaluated at full resolution without compromising
much performance. Interestingly, ablations without the SM
network generalize less well to higher resolutions.

All our variants have less or equal PSV planes compared to
other approaches based on plane sweeping (Stereo-Mag: 32,
PB-MPI: 64, LLFF: 32). Our main competitors Stereo-Mag [3]
and 3D-Photo [37] use signiﬁcantly more learned parameters
(17M and 119M), as shown in Table I.

We show exemplary extrapolated views in Figs. 1 and 7.

B. Interpolation

The second interesting problem setting is interpolation, i.e.
when the target camera pose is roughly between the two input
frames of a video sequence. For this, we randomly choose a

Fig. 7: Extrapolation on the RealEstate10k test set.

target image IO and source frames I1 and I2 before and after
it, respectively. The distances ∆t1, ∆t2 are uniformly sampled
from the interval [4, 13]. Note that extrapolation may still occur
in this mode, since the camera seldomly moves perfectly on a
straight line. We perform both training and evaluation based on
the image resolution of 518×288 unless otherwise mentioned.
In Table II we compare our approach to our main competitor
Stereo Magniﬁcation [3] for interpolation, which we outper-
form in all metrics. Note that our networks generalize from
a training image size of 512×288 to 1024×576 at inference,
while Stereo-Mag was trained for higher resolution explicitly.
Looking at the ablations, we again achieve better generaliza-
tion ability in the variants with soft masking (especially in
comparison with Table III).

In our ablation study in Table III, reducing the number of
PSV planes clearly reduces performance. However, removing
the soft-masking network (NoSM variant) has a much bigger
impact, so that the F-19 network outperforms the F-32-NoSM
variant with signiﬁcantly more depth planes.

Three source views: In order to demonstrate FaDIV-Syn’s
capability to handle more than two input views, we also report

∆t = 5

∆t = 10

Variant

PSNR↑ SSIM↑ LPIPS↓

PSNR↑ SSIM↑ LPIPS↓

33.61
F-32
F-32-NoSM 33.16
F-19
33.03
F-19-NoSM 32.21
F-17-NoSM 32.80
F-13-NoSM 31.67

.9541
.9493
.9524
.9415
.9452
.9376

.0242
.0272
.0263
.0293
.0304
.0326

29.93
29.34
29.52
28.53
28.82
28.06

.9205
.9071
.9164
.8949
.8983
.8881

.0462
.0538
.0497
.0579
.0606
.0634

F-19-3view

33.50

.9545

.0248

29.97

.9215

.0452

TABLE III: Interpolation results of ablations on RealEstate10k with
an image resolution of 512×288 pixels. F-13 and F-17 networks
are trained with a depth distribution of [0.3, 16] m, while the other
networks use [1, 100] m. The 3view variant uses three input views
(which might not be available in certain scenarios).

Ours

GT

Shih et al. [37]

Fig. 8: Extrapolation results of our method (left) compared with ground truth (center) and Shih et al. [37] (right).

Torch [ms]

TRT-32 [ms]

TRT-16 [ms]

Model

288p 540p

288p

540p

288p

540p

F-32
28.7 96.8
F-32-NoSM 27.1 94.1
F-19
16.3 55.2
F-19-NoSM 15.1 51.5
F-17-NoSM 13.5 46.3
F-13-NoSM 10.7 35.8

27.1
25.9
14.7
12.1
11.2
9.0

92.2
89.2
49.3
39.8
35.8
29.7

12.3
11.3
7.7
5.2
4.8
3.9

40.4
39.7
25.3
19.3
17.4
14.0

Input 1

TABLE IV: Inference times of our networks on RTX 3090. We show
native PyTorch and TensorRT (TRT) [56] ﬂoat32/ﬂoat16 versions.
The times do not include PSV generation (1.5 ms @ 540p).

Input 1

Input 2

Input 1

Input 2

Extrapolation

Input 1

Input 2

Extrapolation

Input 2

Extrapolation

Extrapolation

the results of an F-19 variant that is trained with three input
views (F-19-3view). We note that inference times and the
number of parameters grow with each additional PSV plane.
In training and evaluation, we always choose a third frame
outside of the two original source frames. The frame is chosen
with at least four frames distance to the other frames. Table III
shows that FaDIV-Syn beneﬁts from more views, with the
mentioned drawbacks.

Data Efﬁciency: FaDIV-Syn generalizes well with signif-
icantly smaller training subsets (35%, 5%, 1%). This experi-
ment can be found in our supplementary material.

C. Inference Time

Table IV shows the inference time of the different models on
one NVIDIA RTX 3090 GPU. Our full model achieves 34fps
on 288p input. Removing the soft-masking network results in
slightly faster inference speed. Reducing the PSV plane count
yields larger gains. We especially note that F-19 is 43% faster
than F-32, but attains nearly the same accuracy (see Table I).
The fastest model is F-13-NoSM, as it only uses 13 depth
planes and no SM network.

In addition to results on vanilla PyTorch, Table IV also
shows inference times in TensorRT [56], which already boosts
performance for ﬂoat32 precision without losing accuracy.
TensorRT offers possibilities to quantize the weights of neural
networks to ﬂoat16. This quantization of our models retains
almost 100% accuracy for SSIM and PSNR and approximately
99% for LPIPS. Further, it leads to a signiﬁcant acceleration:
We can achieve up to 71 fps on 960×540 images and 256 fps
for 512×288 resolution. In comparison, 3D-Photo and Stereo-
Mag process 2-3 min and 93 ms per 540p image, respectively.
If faster processing times are desired without removing the
soft-masking network, one can also estimate the soft-masks
in half resolution and upsample them without losing much
accuracy (see Table V).

Fig. 9: FaDIV-Syn is able to represent multiple layers of depth at one
location and thus handles reﬂections correctly.

D. Qualitative Results

Continuous Depth: The ﬁxed plane depths do not con-
strain FaDIV-Syn. This effect can particularly be seen on
straight lines across different depths, which are preserved by
our approach (see Fig. 3 and suppl. material). We conclude
that FaDIV-Syn does not directly propagate information from
the PSV, but properly interpolates between planes.

Occlusions & Disocclusions: Figs. 1, 3 and 7 show
examples of disocclusions. Our method handles disocclusions
for both interpolation and extrapolation. As shown in Figs. 1
and 9, FaDIV-Syn can also handle and represent occlusions.
Reﬂections and Transparency: Often methods that use
depth have problems in representing transparencies and reﬂec-
tions (see suppl. material), since there is often more than one
depth value at a certain pixel location. FaDIV-Syn is designed

Accuracy

540p [ms]

Method

SSIM↑ PSNR↑ LPIPS↓

Torch TRT-32 TRT-16

F-32
F-32 (mixed scale)
F-19
F-19 (mixed scale)

.9020
.8967
.8985
.8943

29.39
28.99
29.04
28.74

.0556
.0623
.0583
.0611

96.8
76.5
55.2
43.4

89.2
71.9
49.3
37.4

39.7
32.8
25.3
19.4

TABLE V: Mixed scale inference with 288p mask generation and
576p fusion as described in Sec. IV-C. We show extrapolation
performance and timings.

Input 1

Input 2

Prediction at 512×288

Prediction at 1024×576

Fig. 10: Interpolation from two reference views with a moving object
(elevator door). See supplementary video for animation. This ﬁgure
has been created with a 17-NoSM network.

in such a way that there is not only one depth for each pixel,
but a multitude of information in the different depth layers.
This allows recognition of the correct position of both the
surface and the reﬂection on it, as shown in Fig. 9. Examples
for transparencies can be found in Figs. 3 and 8.

Moving Objects: While dynamic scenes are more difﬁcult
for view synthesis (and one could argue that the problem is ill-
posed), FaDIV-Syn nonetheless shows plausible behavior here.
For example, it has learned to interpolate between positions of
movable objects (see Fig. 10). This would be hard to achieve
if a method estimates depth ﬁrst or only blends pixels.

E. Limitations

Our approach sometimes struggles with inpainting under
large camera movements. One example is the fountain/pool
boundary in Fig. 7. We believe that adversarial losses could
help to encourage realistic inpainting. Furthermore, we expect
that semi-supervised techniques such as proposed by Hani
et al. [31] could be used to increase the robustness of the
method against arbitrary target poses, since the supervision
offered by RealEstate10k only covers inter- and extrapolation
on smooth camera trajectories. Additionally, the PSV depth
distribution is currently ﬁxed and techniques such as depth
plane resampling [13] could be advantageous for varying
geometries. Another issue we observed is that our network
shows very small learning progress after several epochs. While
we stop the training after a ﬁxed number of epochs,
the
validation and testing score still improves for a long time,
indicating that further gains are possible through changes in
the training regime. Finally, the inference time is limited by
the network itself, where compression techniques [57] could
be applied to reduce network runtime even further. Note that
additional demonstrations of failure cases can be found in the
supplementary material.

V. CONCLUSION

We introduced FaDIV-Syn, a fast depth-independent novel
view synthesis method that exceeds state-of-the-art perfor-
mance in interpolation and extrapolation on the RealEstate10k

dataset. The method generalizes well to larger resolutions. Fur-
thermore, its lightweight architecture makes our method real-
time-capable with 25-256 fps, depending on output resolution
and desired quality. The fast inference times make it applicable
for live applications. The proposed gating module encour-
ages self-supervised learning of soft masks, which noticeably
improves performance and provides valuable insight into the
network operation. Overall, we conclude that the direct usage
of the PSV for RGB view synthesis is a promising approach
especially for real-time applications and will inspire further
research in this direction.

ACKNOWLEDGMENTS
This work was funded by grant BE 2556/16-2 (Research
Unit FOR 2535 Anticipating Human Behavior) of the German
Research Foundation (DFG) and the Federal Ministry of
Education and Research of Germany as part of the competence
center for machine learning ML2R (01IS18038C).

REFERENCES
[1] David Whitney, Eric Rosen, Elizabeth Phillips, George
Konidaris, and Stefanie Tellex. Comparing robot grasping
teleoperation across desktop and virtual reality with ros reality.
In Robotics Research, pages 335–350. Springer, 2020.

[2] Max Schwarz, Christian Lenz, Andre Rochow, Michael
Schreiber, and Sven Behnke. NimbRo Avatar: Interactive im-
mersive telepresence with force-feedback telemanipulation. In
2021 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 5312–5319. IEEE, 2021.

[3] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and
Noah Snavely. Stereo magniﬁcation: Learning view synthesis
In ACM Transactions on Graphics
using multiplane images.
(TOG), 2018.

[4] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon.
Dpsnet: End-to-end deep plane sweep stereo. In International
Conference on Learning Representations (ICLR), 2018.

[5] B. Ruf, Bastian Erdn¨uß, and Martin Weinmann. Determining
plane-sweep sampling points in image space using the cross-
ratio for image-based depth estimation. International Archives
of the Photogrammetry, Remote Sensing and Spatial Informa-
tion Sciences (ISPRS), XLII-2/W6:325–332, 2017.

[6] P. Huang, K. Matzen, J. Kopf, N. Ahuja, and J. Huang.
DeepMVS: Learning multi-view stereopsis. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
[7] S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, and
H. Su. Deep stereo using adaptive thin volume representation
with uncertainty awareness. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2020.

[8] P. P. Srinivasan, R. Tucker, J. T. Barron, R. Ramamoorthi,
Pushing the boundaries of view
R. Ng, and N. Snavely.
extrapolation with multiplane images. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2019.
[9] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-
mamoorthi.
Learning-based view synthesis for light ﬁeld
cameras. Transactions on Graphics (TOG), 35(6):1–10, 2016.
[10] Robert T Collins. A space-sweep approach to true multi-image
matching. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 358–363, 1996.

[11] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grang-
ier. Language modeling with gated convolutional networks. In
International Conference on Machine Learning (ICML), 2017.
[12] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S. Huang. Free-form image inpainting with gated
convolution. In Int. Conf. on Computer Vision (ICCV), 2019.

[13] Phong Nguyen, Animesh Karnewar, Lam Huynh, Esa Rahtu,
Jiri Matas, and Janne Heikkila. RGBD-Net: Predicting color
and depth images for novel views synthesis. arXiv preprint
arXiv:2011.14398, 2020.

[14] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,
Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla,
Tomas Simon, Jason Saragih, Matthias Nießner, et al. State
of the art on neural rendering. In Computer Graphics Forum,
volume 39, pages 701–727, 2020.

[15] Eric Penner and Li Zhang. Soft 3D reconstruction for view
synthesis. ACM Transactions on Graphics (TOG), 36(6), 2017.
[16] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,
George Drettakis, and Gabriel Brostow. Deep blending for
free-viewpoint image-based rendering. ACM Transactions on
Graphics (TOG), 37(6):1–15, 2018.

[17] Peter Hedman, Tobias Ritschel, George Drettakis, and Gabriel
Brostow. Scalable inside-out image-based rendering. ACM
Transactions on Graphics (TOG), 35(6):1–11, 2016.

[18] R. O. Cayon, A. Djelouah, and G. Drettakis. A bayesian
approach for selective image-based rendering using superpixels.
In International Conference on 3D Vision (3DV), 2015.
[19] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srini-
vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser.
IBRNet:
Learning multi-view image-based rendering. 2021.

[20] Junyuan Xie, Ross Girshick, and Ali Farhadi. Deep3D: Fully
automatic 2D-to-3D video conversion with deep convolutional
neural networks. In European Conference on Computer Vision
(ECCV), pages 842–857, 2016.

[21] Justus Thies, Michael Zollh¨ofer, Christian Theobalt, Marc Stam-
minger, and Matthias Nießner.
Image-guided neural object
rendering. In International Conference on Learning Represen-
tations (ICLR), 2020.

[22] Gernot Riegler and Vladlen Koltun. Free view synthesis.
European Conference on Computer Vision (ECCV), 2020.
[23] Gernot Riegler and Vladlen Koltun. Stable view synthesis. In
Conf. on Computer Vision and Pattern Rec. (CVPR), 2021.
[24] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely.
DeepStereo: Learning to predict new views from the world’s
In Conference on Computer Vision and Pattern
imagery.
Recognition (CVPR), 2016.

In

[25] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry
Ulyanov, and Victor Lempitsky. Neural point-based graphics.
arXiv preprint arXiv:1906.08240v3, 2020.

[26] Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H Kim,
and Jan Kautz. Extreme view synthesis. In IEEE International
Conference on Computer Vision (CVPR), 2019.

[27] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin
Johnson. SynSin: End-to-end view synthesis from a single
In IEEE Conference on Computer Vision and Pattern
image.
Recognition (CVPR), pages 7467–7477, 2020.

[28] Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi
Ramamoorthi, and Ren Ng. Learning to synthesize a 4D
RGBD light ﬁeld from a single image. In IEEE International
Conference on Computer Vision (ICCV), 2017.

[29] Xu Chen, Jie Song, and Otmar Hilliges. Monocular neural
image based rendering with continuous view control.
In
International Conference on Computer Vision (ICCV), 2019.

[30] K. Olszewski, S. Tulyakov, O. Woodford, H. Li, and L. Luo.
In IEEE International

Transformable bottleneck networks.
Conference on Computer Vision (ICCV), 2019.

[31] Nicolai Hani, Selim Engin, Jun-Jee Chao, and Volkan Isler.
Continuous object representation networks: Novel view synthe-
International Conference
sis without target view supervision.
on Neural Information Processing Systems (NeurIPS), 33, 2020.
[32] Diogo C Luvizon, Gustavo Sutter P Carvalho, Andreza A dos
Santos, Jhonatas S Conceicao, Jose L Flores-Campana, Luis GL

Decker, Marcos R Souza, Helio Pedrini, Antonio Joia, and
Otavio AB Penatti. Adaptive multiplane image generation from
a single internet picture. In Winter Conference on Applications
of Computer Vision (WACV), 2021.

[33] Richard Tucker and Noah Snavely. Single-view view synthesis
In IEEE Conference on Computer
with multiplane images.
Vision and Pattern Recognition (CVPR), pages 551–560, 2020.
[34] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light ﬁeld fusion: Practical view synthesis
with prescriptive sampling guidelines. ACM Transactions on
Graphics (TOG), 38(4):1–14, 2019.

[35] Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian
Richardt, and James Tompkin. MatryODShka: Real-time 6DoF
video view synthesis using multi-sphere images. In European
Conference on Computer Vision (ECCV), pages 441–459, 2020.
[36] Noah Snavely, Richard Tucker, and Shubham Tulsiani. Layer-
structured 3D scene inference via view synthesis. In European
Conference on Computer Vision (ECCV), pages 302–317, 2018.
[37] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin
Huang.
3D photography using context-aware layered depth
inpainting. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 8028–8038, 2020.

[38] C. Zitnick, Sing Bing Kang, Matt Uyttendaele, Simon Winder,
and Richard Szeliski. High-quality video view interpolation
using a layered representation. ACM Transactions on Graphics
(TOG), 23(3):600–608, 2004.

[39] Jonathan Shade, Steven Gortler, Li-wei He, and Richard
Szeliski. Layered depth images. In Conf. on Computer Graphics
and Interactive Techniques (SIGGRAPH), 1998.

[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance ﬁelds for view synthesis.
In European Conf. on Computer Vision (ECCV), 2020.
[41] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun.
NeRF++: Analyzing and improving neural radiance ﬁelds. arXiv
preprint arXiv:2010.07492, 2020.

[42] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
In
IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 4578–4587, 2021.

[43] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Soﬁen
Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-
Brualla. Deformable neural radiance ﬁelds. arXiv preprint
arXiv:2011.12948, 2020.

[44] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger.
KiloNeRF: Speeding up neural radiance ﬁelds with thousands
of tiny MLPs. arXiv preprint arXiv:2103.13744, 2021.
[45] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. FastNeRF: High-ﬁdelity neural
rendering at 200FPS. In International Conference on Computer
Vision (ICCV), 2021.

[46] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
PlenOctrees for real-time rendering of

Angjoo Kanazawa.
neural radiance ﬁelds. In ICCV, 2021.

[47] Vincent Sitzmann, Semon Rezchikov, William T. Freeman,
Joshua B. Tenenbaum, and Fredo Durand. Light ﬁeld networks:
Neural scene representations with single-evaluation rendering.
In Proc. NeurIPS, 2021.

[48] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-NeRF: Neural radiance ﬁelds for
dynamic scenes. arXiv preprint arXiv:2011.13961, 2020.
[49] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias
Nießner. Dynamic neural radiance ﬁelds for monocular 4D
facial avatar reconstruction. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2021.

[50] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael

Zollh¨ofer, Chris toph Lassner, and Christian Theobalt. Non-rigid
neural radiance ﬁelds: Reconstruction and novel view synthesis
of a deforming scene from monocular video. arXiv preprint
arXiv:2012.12247, 2020.

[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:
Convolutional networks for biomedical image segmentation. In
International Conference on Medical Image Computing and
Computer-Assisted Intervention (MICCAI), 2015.

[52] Karen Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition.
In Yoshua
Bengio and Yann LeCun, editors, International Conference on
Learning Representations, (ICLR), 2015.

[53] Raul Mur-Artal and Juan D Tard´os. ORB-SLAM2: An open-
source SLAM system for monocular, stereo, and RGB-D cam-
eras. IEEE Transactions on Robotics (T-RO), 33(5), 2017.
[54] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: From error visibility to structural
similarity. IEEE Transactions on Image Processing (TIP), 13
(4):600–612, 2004.

[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep fea-
tures as a perceptual metric. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 586–595, 2018.

[56] NVIDIA. TensorRT. https://developer.nvidia.com/tensorrt.
[57] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A ﬁlter
level pruning method for deep neural network compression. In
Int. Conference on Computer Vision (ICCV), 2017.

APPENDIX A
NETWORK ARCHITECTURE DETAILS
Here, we provide more details regarding the architectures of
our Soft-Masking network (see Table VI), our fusion network
(see Table VII) as well as the NoSM network architecture (see
Table VIII).

Input

k

c Output

gray(P SV )
Pool(down1)
Pool(down2)
Pool(down3)
Pool(down4)
Up(up1), down3
Up(up2), down2
Up(up3), down1
Up(up4), P
softmax(masking)

3 min(2 · N, 256)
3 min(23 · N, 256)
3 min(24 · N, 256)
3 min(25 · N, 256)
3 min(24 · N, 256)
3 min(23 · N, 256)
3 min(22 · N, 256)
3 min(21 · N, 256)
3
-

down1
down2
down3
down4
up1
up2
up3
up4
2 · N f inal
pred

N

TABLE VI: Soft-Masking network architecture for N grayscale depth
planes in the PSV. Each row denotes a convolutional layer, where k
is the kernel size and c is the number of output features. Pool is 2×2
average pooling, and Up denotes bilinear upsampling with a factor
of 2.

Input

k1

c1 k2

c2

Output

P SV
G(groupconv)
bottleneck
down1
down2
down3
down4
Up(dilated), down3
Up(up1), down2
Up(up2), down1
Up(up3), bottleneck
up4

3 6 · N + N 3 12 · N groupconv
3 · N bottleneck
3
6 · N
3
128
3
256
3
256
3
256
3
128
3
6 · N
3
3 · N
3
32
3
−
1

6 · N 3
6 · N 3
3
3
3
3
3
3
6 · N 3
32
3
3 −

down1
down2
down3
down4
dilated
up1
up2
up3
up4
pred

128
256
256
256
256
128

TABLE VII: Fusion network with N depth planes in the PSV. Each
row shows 2 convolutional layers, where k is the kernel size and c
is the number of output features. G denotes the gating operation and
Up denotes upsampling.

Input

k1

c1 k2

c2

Output

12 · 6 groupconv
6 · N bottleneck

P SV
G(groupconv)
G(bottleneck)
G(down1)
G(down2)
G(down3)
G(down4)
Up(dilated), G(down3)
Up(up1), G(down2)
Up(up2), G(down1)
Up(up3), G(bottleneck)
up4

128
256
256
512
256

6 · N 3
3
6 · N 3
3
6 · N 3 12 · N
3
256
3
3
512
3
3
1024
3
3
512
3
3
256
3
3
3 12 · N 3 12 · N
6 · N
3
32
3
−
1

6 · N 3
32
3
3 −

down1
down2
down3
down4
dilated
up1
up2
up3
up4
pred

TABLE VIII: NoSM architecture with N depth planes in the PSV.
Each row shows 2 convolutional layers, where k is the kernel size
and c is the number of output features. G denotes the gating operation
which reduces the number of feature maps by factor 2 and Up denotes
upsampling.

R
N
S
P

30

28

26

24

22

20

0.1%
1%
5%

0

50k

100k

150k

200k

Iterations

Fig. 11: PSNR on reduced training (solid) and validation (dotted)
splits of the RealEstate10k dataset during 17-NoSM network training.

s
s
o
l

l
a
u
t
p
e
c
r
e
P

0.3

0.25

0.2

0.15

0.1

0.1%
1%
5%

0

50k

100k

150k

200k

Iterations

Fig. 12: Perceptual loss on reduced training (solid) and validation
(dotted) splits of the RealEstate10k dataset during 17-NoSM network
training.

APPENDIX B
EXTENDED DATA EFFICIENCY RESULTS

Due to lack of space in the main paper, we shifted details
on the data efﬁciency experiments that belong to Section 4.2
into the supplemental. We train our 17-NoSM ablation on
smaller fractions of the full RealEstate10k training dataset,
and evaluate on the full test set. The dataset size is reduced by
randomly choosing scenes until the speciﬁed size is met (35%,
5%, 1%, and 0.1%). As shown in Table IX, all sizes from 35%
to 1% give sufﬁciently good results, where 35% even performs
similarly or slightly better than our model trained on the full
dataset. It is possible that further training may yield advan-
tages, since we set an upper bound on the training iterations
as described in Section 3.3. However, we conclude that 35% of
RealEstate10k still contains enough scene and pose variance
to prevent the network from overﬁtting (see Fig. 14). This
is to be expected, since the triplet sampling during training

Model

PSNR ↑

SSIM ↑

LPIPS ↓

PSNR ↑

SSIM ↑

LPIPS ↓

PSNR ↑

SSIM ↑

LPIPS ↓

∆t = 2

∆t = 5

∆t = 10

17-NoSM

31.91−11.7% .9361−3.19% .0381+140%

28.03−13.5% .8825−6.32% .0712+122%

24.70−13.4% .8129−9.07% .1257+103%

n
i
a
r
T

%
1
0

.

% 17-NoSM

1

% 17-NoSM

5

34.93−3.31% .9607−0.64% .0191+20.1%

31.38−3.20% .9320−1.06% .0360+12.2%

27.60−3.24% .8789−1.69% .0695+12.1%

35.19−2.58% .9616−0.55% .0173+8.81%

31.92−1.54% .9366−0.57% .0327+1.87%

28.20−1.16% .8874−0.74% .0631+1.77%

%

5
3

%

0
0
1

17-NoSM

36.15+0.08% .9658−0.11% .0158−0.63%

32.52+0.32% .9417−0.03% .0307−4.36%

28.59+0.24% .8944+0.05% .0605−2.42%

17-NoSM

36.12+0.00% .9669+0.00% .0159+0.00%

32.42+0.00% .9420+0.00% .0321+0.00%

28.53+0.00% .8940+0.00% .0620+0.00%

TABLE IX: Data efﬁciency experiment. The Train column shows the training dataset size relative to the full RealEstate10k train split.

greatly augments the number of training samples. The 1%
network maintains good performance for SSIM and PSNR but
starts losing signiﬁcantly in LPIPS. Finally, the 0.1% network
loses signiﬁcant performance in all metrics and seems to be
outside of the boundary for satisfactory results. As Figs. 11
and 12 show, we observed signiﬁcant drops in validation
performance for the 1% and 0.1% training split. Starting
with 35%, we observe that the validation score is actually
better than the training score (see Figs. 13 and 14), which is
caused by batch normalization: The average parameters used
during evaluation seem to work more robustly than the on-line
statistics computed for each batch during training. Overall, we
conclude that above 35% there are no indications of overﬁtting
at all.

APPENDIX C
LIMITATIONS AND FAILURE CASES

We present some examples for failure cases in Figs. 16

to 18.

Inpainting: Even though our method is designed in such
inpainting and PSV fusion can be performed

a way that

35%
100%

0.3

s
s
o
l

l
a
u
t
p
e
c
r
e
P

0.25

0.2

0.15

0

50k

100k

150k

200k

Iterations

Fig. 13: Perceptual loss on reduced training (solid) and validation
(dotted) splits of the RealEstate10k dataset during 17-NoSM network
training.

R
N
S
P

30

28

26

24

22

20

35%
100%

0

50k

100k

150k

200k

Iterations

Fig. 14: PSNR on reduced training (solid) and validation (dotted)
splits of the RealEstate10k dataset during 17-NoSM network training.

simultaneously it often struggles to inpaint
large missing
regions. We visualize two examples in Fig. 16. We expect
that this may be a result of (1) the limited receptive ﬁeld and
(2) a number of learned parameters which is insufﬁcient for
ﬁlling in large regions with reasonable and realistic content.
We also show 3D-Photo [37] results in Fig. 16, which uses a
separate inpainting network. However, we believe that both our
method and 3D-Photo perform similar in inpainting regions,
so this does not seem to be an architectural advantage.

Camera pose errors: The RealEstate10k dataset has been
annotated with ORB-SLAM2 [53] and bundle adjustment. This
sometimes leads to inaccurate camera poses. While our method
can generally handle small misalignments, larger errors can
cause blurred regions as demonstrated in Fig. 17. We expect
that it could be advantageous to allow small camera pose
corrections instead of assuming that they are ﬁxed. However,
predicting camera offsets must be embedded into the pipeline
in a learned fashion, unless per scene optimization is desired.
Biased training data: Our method is trained in such a
way that target poses are always on the camera trajectory of the
RealEstate10k dataset, where ground truth is available. How-
ever, this induces a bias, which may result in less performance

Method

Iterations

SSIM↑

PSNR↑

LPIPS↓

Ours-19
Ours-19
Ours-19

330k
660k
990k

.8985
.8989
.9008

29.04
29.27
29.19

.0583
.0558
.0552

TABLE X: Long-time training behavior. The table shows extrapola-
tion results on the RealEstate10k [3] test set. All variants are trained
with 288p, but evaluated on 576p.

R
N
S
P

30.5

30

29.5

29

28.5

28

0

200k

400k

600k

800k

1,000k 1,200k

Iterations

Fig. 15: Long-time training behavior. We show training (solid) and
validation (dotted) PSNR during training for more iterations.

for target views outside of the smooth camera trajectories.
It could be possible to improve generalization to such poses
using semi-supervised techniques [31].

Slow convergence: We note that the experiments in the
main paper were achieved with a limited number of training
iterations, i.e., we did not train until there was no improvement
anymore. Table X compares the accuracy of three equivalent
networks which were trained for an ascending number of
iterations on the test set provided by [37]. Table X and Fig. 15
illustrate that more training still improves the testing/validation
accuracy and the optimal performance is not yet achieved.
It would be desirable to reach the optimal performance in
signiﬁcantly less training iterations to achieve better results
without changing the method itself.

APPENDIX D
ADDITIONAL QUALITATIVE RESULTS
In addition to the exemplary results already shown, we
present more qualitative examples here. Figures 19 and 20
show extrapolation examples for our full network variants
(Ours-32, Ours-19) that use Soft-Masking, in comparison to
ground truth, 3D Photo [37], and Stereo-Mag [3]. In Fig. 21
we illustrate that our method ablation Ours-17-NoSM with-
out Soft-Masking achieves also reasonable results. To fur-
ther demonstrate the generalization capability of our method
to higher resolutions, we show interpolation sequences in
Figs. 22 and 23 with models that are trained in 288p but
inferred in 576p (twice the training resolution).

Input 1

Input 1

Input 2

Input 1

Input 2

Extrapolation Ours

Input 1

Input 2

Input 1

Input 1

Input 2

Input 1

Input 2

Input 1

Extrapolation Ours

Extrapolation 3D Photo
Input 1
Input 2

Extrapolation Ours

Extrapolation 3D Photo
Input 2
Extrapolation Stereo-Mag

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation Ours

Input 1

Input 2

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Fig. 16: Limitations of FaDIV-Syn: Inpainting at borders.

Input 1

Input 2

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Fig. 17: Limitations of FaDIV-Syn: Insufﬁcient pose alignment.

Input 1

Input 1

Input 2

Input 1

Input 2

Extrapolation Ours

Input 1

Input 2

Extrapolation Ours

Extrapolation 3D Photo
Input 2
Input 1

Extrapolation Ours

Extrapolation 3D Photo
Input 2
Extrapolation Stereo-Mag

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation Stereo-Mag

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Extrapolation 3D Photo

Extrapolation 3D Photo

Input 2
Extrapolation Ours

Input 2
Input 1
Extrapolation Ours

Input 2
Extrapolation Ours

Extrapolation Ours

Extrapolation Ours

Input 1

Input 2

Extrapolation Ours

Extrapolation 3D Photo

Extrapolation Stereo-Mag

Fig. 18: Limitations of FaDIV-Syn: General failure cases.

Input

Input

Ours-32

Input

Ours-32

Ground Truth

Input

Ours-32

Ground Truth

3D Photo [37]

Ours-32

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Input

Ours-32

Input

Ours-32

Ground Truth

Input

Ours-32

Ground Truth

3D Photo [37]

Ours-32

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Ours-32

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Input

Input

Ours-32

Input

Ours-32

Input

Ground Truth

Ours-32

Ground Truth

Ours-32

3D Photo [37]

Ground Truth

3D Photo [37]

Ground Truth

Stereo-Mag [3]

3D Photo [37]

Input

Input

Ours-32

Input

Ours-32

Input

Input

Ground Truth
Ours-32
Fig. 19: Extrapolation comparison of our method with 32 planes (Ours-32) against ground truth, 3D Photo [37], and Stereo-Mag [3]. The
input frame closer to the target frame is marked in green for easier comparison.

Ground Truth
Ours-32

Ground Truth

Ground Truth

3D Photo [37]

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Ground Truth

Stereo-Mag [3]

3D Photo [37]

Ours-32

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Input

Ours-19

Input

Ours-19

Ground Truth

Input

Ours-19

Ground Truth

3D Photo [37]

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Input

Ours-19

Input

Ours-19

Ground Truth

Input

Ours-19

Ground Truth

3D Photo [37]

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Input

Input

Ours-19

Input

Ours-19

Ground Truth

Input

Ground Truth

3D Photo [37]

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Input

Ours-19

Input

Ours-19

Ground Truth

Input

Ours-19

Ground Truth

3D Photo [37]
Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]
Ground Truth

Fig. 20: Extrapolation comparison of our method with 19 planes (Ours-19) against ground truth, 3D Photo [37], and Stereo-Mag [3]. The
input frame closer to the target frame is marked in green for easier comparison.

Ours-19

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Input

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Ours-17-NoSM

Input

Ours-17-NoSM

Ground Truth

Input

Ours-17-NoSM

Ground Truth

3D Photo [37]

Input

Ours-17-NoSM

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Input

Ours-17-NoSM

Input

Ours-17-NoSM

Ground Truth

Ours-17-NoSM

Input

Ground Truth

Ours-17-NoSM

3D Photo [37]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Input

Ours-17-NoSM

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Input

Input

Input

Ours-17-NoSM

Input

Ours-17-NoSM

Input

Ours-17-NoSM

Ground Truth

Input

Ours-17-NoSM

Input

Ours-17-NoSM

Ground Truth

Input

Ours-17-NoSM

Ground Truth

3D Photo [37]

Ours-17-NoSM

Ground Truth

Fig. 21: Extrapolation comparison of our method ablation Ours-17-NoSM without soft masks against ground truth, 3D Photo [37], and
Stereo-Mag [3]. The input frame closer to the target frame is marked in green for easier comparison.

3D Photo [37]

Stereo-Mag [3]

Ground Truth

Ours-17-NoSM

Stereo-Mag [3]

3D Photo [37]

Ground Truth

Input

Ground Truth

3D Photo [37]

Ours-17-NoSM

Ground Truth

3D Photo [37]

Stereo-Mag [3]

Ground Truth

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

3D Photo [37]

Stereo-Mag [3]

Stereo-Mag [3]

Fig. 22: Interpolation using the fast Ours-19 network. The network runs inference in 576p while being trained in 288p. In every block, the top
row (green) shows the ground truth trajectory from RealEstate10k, while the bottom row (blue) presents the interpolated result corresponding
to the ground truth camera poses.

Fig. 23: Interpolation using the Ours-32 network. The network runs inference in 576p while being trained in 288p. In every block, the top
row (green) shows the ground truth trajectory from RealEstate10k, while the bottom row (blue) presents the interpolated result corresponding
to the ground truth camera poses.

