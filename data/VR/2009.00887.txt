0
2
0
2

v
o
N
3
1

]

R
G
.
s
c
[

2
v
7
8
8
0
0
.
9
0
0
2
:
v
i
X
r
a

Inspection of histological 3D reconstructions in
virtual reality

Oleg Lobacheva,b,c,∗, Moritz Bertholda,1, Henriette Pfeﬀerd, Michael Guthea,
Birte S. Steinigerd

aUniversity of Bayreuth, Visual Computing, Bayreuth, Germany
bHannover Medical School, OE 4120, Carl-Neuberg-Straße 1, 30625 Hannover, Germany
cLeibniz-Fachhochschule School of Business, Expo Plaza 11, 30539 Hannover, Germany
dPhilipps-University Marburg, Anatomy and Cell Biology, Marburg, Germany

Abstract

3D reconstruction is a challenging current topic in medical research. We perform
3D reconstructions from serial sections stained by immunohistological methods. This
paper presents an immersive visualisation solution to quality control (QC), inspect, and
analyse such reconstructions. QC is essential to establish correct digital processing
methodologies. Visual analytics, such as annotation placement, mesh painting, and
classiﬁcation utility, facilitates medical research insights. We propose a visualisation in
virtual reality (VR) for these purposes. In this manner, we advance the microanatomical
research of human bone marrow and spleen. Both 3D reconstructions and original data
are available in VR. Data inspection is streamlined by subtle implementation details
and general immersion in VR.

Keywords: virtual reality, 3D reconstruction, scientiﬁc visualisation, histological serial
sections
2020 MSC: 68U05, 68U35, 92C55, 94A08

1. Introduction

Visualisation is a signiﬁcant part of modern research. It is important not only to
obtain images, but to be able to grasp and correctly interpret data. There is always the
question, whether the obtained model is close enough to reality. Further, the question
arises how to discern important components and derive conclusions from the model.
The method we present in this paper is a typical example of this very generic problem

∗Corresponding author
Email address: oleg.lobachev@leibniz-fh.de (Oleg Lobachev)
URL: https://orcid.org/0000-0002-7193-6258 (Oleg Lobachev)
1Present address: BCM Solutions GmbH, Roteb¨uhlpl. 23, 70178 Stuttgart, Germany
cbed Licensed under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 Interna-

tional licence.

 
 
 
 
 
 
microanatomical
insights

conclusions
conclusions

biological processing,
registration, mesh
construction, etc.

sample of a
lymphatic
organ

specimen
specimen

acquisition

reconstructed
meshes

visual decisions
visual decisions

visual
computing

digital copy
digital copy

coloured mesh,
decision on quality

visual analytics
visual analytics

simulation
simulation

quality control,
annotations,
mesh painting

VRVR

the medium

real-time
rendering,
tracking, etc.

Figure 1: The general life cycle of visual computing (black) and our speciﬁc case (blue). The intensity of blue
shading in the circle highlights the major focus of this paper. Blue arrows show facets of our implementation.

statement, but it is quite novel in the details. We use virtual reality for quality control
(QC) and visual analytics (VA) of our 3D reconstructions in medical research. Figure 1
positions our tool in the visual computing methodology.

3D reconstruction from histological serial sections closes a gap in medical research
methodology. Conventional MRI, CT, and ultrasound methods do not have the desired
resolution. This is also true for micro-CT and similar methods. Even worse, there is no
way to identify the structures of interest in human specimens under non-invasive imaging
techniques. In contrast, immunohistological staining provides a reliable method to mark
speciﬁc kinds of molecules in the cells as long as these molecules can be adequately
ﬁxed after specimen removal. It is possible to unequivocally identify, e.g., the cells
forming the walls of small blood vessels, the so-termed endothelial cells. Only a thin
section of the specimen—typically about 7 µm thick—is immunostained to improve
recognition in transmitted light microscopy. If larger structures, such as microvessel
networks, are to be observed, multiple sections in a series (‘serial sections’) need to be
produced. These series are necessary, because paraﬃn sections cannot be cut at more
than about 30 µm thickness due to mechanical restrictions. The staining solution can
penetrate more than 30 µm of tissue. It is not possible up to now to generate focused

2

z-stacks from immunostained thick sections in transmitted light. Hence, the information
gathered from single sections is limited. Thus, registration is a must.

For 3D reconstruction, serial sections are digitally processed after obtaining large
images of each section with a special optical scanning microscope. The resolution is
typically in the range 0.11–0.5 µm/pixel, the probe may cover up to 1 cm2. With a our
registration method (Lobachev et al., 2017b) we produce stacks of serial sections that
are spatially correct. After some post-processing (e. g., morphological operations, but
also an interpolation Lobachev et al., 2017a), a surface polygon structure (a mesh) is
obtained from volume data with the marching cubes algorithm (Lorensen and Cline,
1987; Ulrich et al., 2014). Both the actual mesh construction and the post-processing
operations feature some subjective decisions, most prominently, choice of an iso-value
for mesh construction. Thus, it is necessary to demonstrate that the 3D reconstruction is
correct.

We now present a method for controlling that 3D reconstructions tightly correspond
to the original immunostained sections by directly comparing the reconstructions to
the original serial sections. This method accelerates QC and mesh colouring. QC is
facilitated by showing single sections in the visualised mesh, without volume rendering.
We inspect, annotate, and colour 3D models (directly compared to original data, the
serial sections) in virtual reality (VR). Figure 2 documents a QC session from ‘outside’.
The presented method has been extensively used in microanatomical research (Steiniger
et al., 2018a,b; Lobachev, 2018; Lobachev et al., 2019; Steiniger et al., 2020).

Our domain experts are much better trained in distinguishing and analysing details
in stained histological sections than in reconstructed meshes. However, only 3D re-
constructions provide an overview of continuous structures spanning multiple sections,
e.g., blood vessels.

Further, the reconstructed mesh permits novel ﬁndings. In our prior experience,
domain experts often encounter problems when trying to understand 3D arrangements
using conventional mesh viewers. For this reason, we previously showed pre-rendered
videos to the experts to communicate the reconstruction results and to enable detailed
inspection. Videos are, however, limited by the ﬁxed direction of movement and the
ﬁxed camera angle. Our experience with non-immersive interactive tools has been
negative, both with standard (Cignoni et al., 2008) and with custom (Fig. 16) software.
Further, our data suﬀer from a high degree of self-occlusion. In VR the user can move
freely and thus intuitively control the angle of view and the movement through the model.
In our experience, immersion allows for much easier and more thorough inspection of
visualised data. Occlusion of decisive structures in the reconstruction does no longer
pose a problem.

Contributions

We present a modern, immersive VR approach for inspection, QC, and VA of histo-
logical 3D reconstructions. Some of the reconstructed meshes are highly self-occluding,
we are able to cope with this problem. For QC, the original data is simultaneously dis-
played with the reconstruction. User annotations and mesh painting facilitate VA. With
our application, novel research results concerning the micoanatomy of human spleens
became viable for the ﬁrst time; our ﬁndings have been established and published in a
series of papers (Steiniger et al., 2018a,b, 2020).

3

Figure 2: A user in VR. The large display mirrors the headset image.

2. Related work

2.1. Immersive visualisation

Immersive visualisation is not at all a new idea, Brooks Jr. (1999) quotes a vision
of Sutherland (1965, 1968, 1970). However, an immersive scientiﬁc visualisation was
quite hard to obtain in earlier years, if a multi-million-dollar training simulation was
to be avoided (van Dam et al., 2000). The availability of inexpensive hardware such
as Oculus Rift or HTC Vive head-mounted displays (HMD) has massively changed
the game recently. This fact (and the progress in GPU performance) allows for good
VR experiences on commodity hardware. Immersive visualisation has been previously
suggested for molecular visualisation (Stone et al., 2010), for medical volumetric data
(Shen et al., 2008; Scholl et al., 2018), for dentistry (Shimabukuro and Minghim, 1998;
Xia et al., 2013), and for computational ﬂuid dynamics (Quam et al., 2015). More
relevant to our approach are the visualisations of the inside of large arterial blood vessels
(Forsberg et al., 2000; Egger et al., 2020). There is a trend is to utilise VR in medical
education and training (Walsh et al., 2012; Chan et al., 2013; Mathur, 2015; Moro
et al., 2017; Bouaoud et al., 2020; L´opez Ch´avez et al., 2020; Pieterse et al., 2020);
Uruthiralingam and Rea (2020) and Duarte et al. (2020) provide an overview. The
availability of head-mounted displays has sparked some new research (Chen et al., 2015;
Choi et al., 2016; Inoue et al., 2016) in addition to already covered ﬁelds. Mann et al.
(2018) present a taxonomy of various related approaches (virtual reality, augmented
reality, etc.). Checa and Bustillo (2020) review VR applications in the area of serious
games.

A radically diﬀerent approach is to bypass mesh generation altogether and to render
the volumes directly. Scholl et al. (2018) do so in VR, although with quite small volumes

4

(a)

(c)

(b)

(d)

(e)

(f)

Figure 3: From section images to ﬁnal results: A human spleen section is stained for SMA (brown), CD34
(blue), and either CD271 (red) or CD20 (red), this is the ‘Sheaths alternating’ data set. (a): The region of
interest (ROI), staining of B-lymphocytes (CD20) in red. (b): The ROI, staining of capillary sheaths (CD271)
in red. (c): Result of colour deconvolution for CD271 of (b), a single image. (d): Same, but for CD34. (e):
A volume rendering of the ﬁrst 30 sheath-depicting sections of the ROI. (f): Final meshes. The colours
highlight the diﬀerent functions. The arterial blood vessels are in blue and red. The red colour is highlighting
a speciﬁc tree of blood vessels. The sheaths related to this tree are green, the unrelated sheaths are dark green.
The follicular dendritic cells (that are also weakly CD271+) are depicted in light green. The SMA mesh was
used for a heuristics to ﬁnd arterioles among blood vessels. SMA and B-lymphocytes are not shown in the
rendering.

5

(with largest side of 256 or 512 voxels). Faludi et al. (2019) also use direct volume
rendering and add haptics. In this case the largest volume side was 512 voxels. The key
issue in such direct volume renderings is the VR-capable frame rate, which is severely
limited in volume rendering of larger volumes. The volumes we used for the mesh
generation had 2000 pixels or more at the largest side.

Zoller et al. (2020) use volume rendering for haptic feedback in VR. Their goal was

to simulate pedicle screw tract palpation. This work is not concerned with haptics.

Wißmann et al. (2020) use reprojection to improve binocular ray tracing. The idea
is to use the image from one eye to create the image for the other eye faster. We do
not use ray tracing in this paper, our visualisations are conventional OpenGL-based
rasterizations in VR. However, future improvements to our work both in the visuals and
in rendering times, might base on the results by Wißmann et al..

In the next section we discuss additional modern medical applications of VR, how-
ever, VR visualisation is a much broader topic (Berg and Vance, 2017; Misiak et al.,
2018; Rizvic et al., 2019; Slater et al., 2020).

2.2. Virtual reality in medical research

Although there have been precursors long before the ‘VR boom’, e. g., Tomikawa
et al. (2010), most relevant publications on the use of VR in medical research, training,
and in clinical applications appeared after 2017. This section focuses on medical
research.

Stets et al. (2017) work with a point cloud. We work with surface meshes. Esfahlani
et al. (2018) reported on non-immersive VR in rehab. We use immersive VR in medical
research. Uppot et al. (2019) describe VR and AR for radiology, we use histological
sections as our input data. Knodel et al. (2018) discuss the possibilities of VR in medical
imaging. Stefani et al. (2018) show confocal microscopy images in VR. We use images
from transmitted light microscopy. Cal`ı et al. (2019) visualise glial and neuronal cells
in VR. We visualise blood vessels and accompanying cell types in lymphatic organs,
mostly in the spleen.

A visualisation support for HTC Vive in popular medical imaging toolkits has been
presented before (Egger et al., 2017). Unlike our approach, this method is tied into
existing visualisation libraries. Our method is a stand-alone application, even if easily
usable in our tool pipeline. Further, visualising both reconstructed meshes and original
input data was a must in our use case. We also implemented a customised mesh painting
module for visual analytics. Both our approach and the works of Egger et al. (2017,
2020) generate meshes prior to the visualisation. We discuss the diﬀerences between
Egger et al. (2020) and our approach on page 25.

El Beheiry et al. (2019) analyse the consequences of VR for research. In their
opinion, VR means navigation, but also allows for better education and provides options
for machine learning. They can place annotations in their program, but focus on
(immersed) measurements between the selected points. El Beheiry et al. perform some
segmentations in VR, but primarily work with image stacks. Our mesh painting in VR
can be seen as a form of segmentation, but we perform it on the 3D models, not on
image data. Mesh painting uses geodesic distances, as detailed in Section 5.7.

Daly (2018; 2019a; 2019b) has similar goals to this work, however he uses a radically
diﬀerent tool pipeline, relying more on oﬀ-the-shelf software—which alleviates a larger

6

part of software development, but is also a limitation in the amount of features. Daly
(and also others, e.g., Preim and Saalfeld, 2018) also focus a lot on teaching, we use our
system at the moment mostly for research purposes.

Dorweiler et al. (2019) discusses the implications of VR, AR, and further techno-
logies in blood vessel surgery. We are concerned with analysis of microscopic blood
vessels in removed probes.

The work by Liimatainen et al. (2020) allows the user to inspect 3D reconstructions
from histological sections (created in a radically diﬀerent manner from how we section,
they skip a lot of tissue in an eﬀort to cover a larger volume). The user can view the
sections and ‘interact with single areas of interest’. This is elaborated to be a multi-scale
selection of the details and allowing the user to zoom in. We stay mostly at the same
detail level, but allow for more in-depth analysis. They put histological sections of
tumors in their correct location in the visualisation, which was also one of the ﬁrst
requirements to our tool, as Section 3.1 details.

We are not aware of other implementations of advanced VR- and mesh-based in-
teractions, such as our mesh paining that follows blood vessels (Section 5.7). To our
knowledge, annotations have never before been implemented in the manner we use:
The markers are preserved after the VR session and can be used in a mesh space for
later analysis. This paper presents both those features. In general, most VR-based
visualisations focus on presentation and exploration of the data. We do not stop there,
but also perform a lot of visual analytics.

2.3. Why non-VR tools do not suﬃce

While enough non-VR tools for medical visualisation exist, such as 3D Slicer
(Pieper et al., 2004; Kikinis et al., 2014), ParaView (Ahrens et al., 2005; Ayachit,
2015), or MeVisLab (Silva et al., 2009; Ritter et al., 2011), we are proponents of
VR-based visualisation. Rudimentary tasks in QC can be done, e. g., in 3D Slicer or
using our previous work, a custom non-VR tool (detailed below on page 23), but in our
experience, our VR-based QC was much faster and also easier for the user. (Bouaoud
et al. (2020) and L´opez Ch´avez et al. (2020) report similar experiences.) The navigation
and generation of insights are a larger problem with non-VR tools. The navigation in
VR is highly intuitive. A lot of insight can be gathered by simply looking at the model
from various views.

The relation of implementation eﬀorts to the usability impact was favourable for our
VR tool. The complexity of software development of large standard components also
plays a role here. We base our VR application heavily on available components, such
as Steam VR and VCG mesh processing library, as Section 4.1 details. However, our
tool is not an amalgamation of multiple existing utilities (e.g., using Python or shell as
a glue), but a stand-alone application, written in C++.

Merely paging through the registered stack of serial sections does not convey a
proper 3D perception. Single entities in individual sections (e.g., capillaries) have a
highly complex shape and are entangled among similar objects. While it is possible to
trace a single entity through a series, gaining a full 3D perception is impossible without
a full-ﬂedged 3D reconstruction. An inspection of our reconstructions in VR (Steiniger
et al., 2018a,b, 2020) was much faster than a typical inspection of 3D data without VR
(Steiniger et al., 2016), as Section 6 details.

7

3. Background

3.1. VR visualisation: Requirements

Our domain experts provided feedback on the earlier versions of the software in
order to shape our application. The following features were deemed necessary by
medical researchers:

• Load multiple meshes corresponding to parts of the model and to switch between
them. This allows for the analysis of multiple ‘channels’ from diﬀerent stainings.

• Load the original data as a texture on a plane and blend it in VR at will at the
correct position. The experts need to discriminate all details in the original stained
sections.

• Remove the reconstructed mesh to see the original section underneath.

• Provide a possibility to annotate a 3D position in VR. Such annotations are crucial

for communication and analysis.

• Adjust the perceived roles of parts of the meshes by changing their colour. Colour

changes form the foundation of visual analytics.

• Cope with very complex, self-occluding reconstructions. Otherwise it is im-
possible to analyse the microvasculature in thicker reconstructions (from about
200 µm in z axis onward).

• Free user movement. This issue is essential for long VR sessions. Basically, no
movement control (e.g., ﬂight) is imposed on the user. In our experience, free
user movement drastically decreases the chances of motion sickness.

• Provide a possibility for voice recording in annotations (work in progress).

• Design a method for sharing the view point and current highlight with partners
outside VR (trivial with Steam VR and its display mirroring), and for communic-
ating the ﬁndings from live VR sessions as non-moving 2D images in research
papers (an open question).

3.2. A 3D reconstruction pipeline for serial sections

In a short summary our method includes:

1. Biological processing: tissue acquisition, ﬁxation, embedding, sectioning, stain-

ing, coverslipping.

2. Digital data acquisition: serial scanning in an optical scanning microscope.
3. Coarse registration: ﬁtting the sections to each other.
4. Selection of regions of interest (ROIs).
5. Fine-grain registration (Lobachev et al., 2017b). The decisive step for maintaining

the connectivity of capillaries.

6. Colour processing, e.g., channel selection or colour deconvolution.
7. Optional: healing of damaged regions (Lobachev, 2020).

8

8. Interpolation to reduce anisotropy (Lobachev et al., 2017a).
9. Volume ﬁltering, e.g., a closing ﬁlter and a blur.

10. Mesh construction (Ulrich et al., 2014).
11. Mesh processing, e.g., decimation or repair (Ju, 2004).
12. Mesh colouring, e. g., colouring of selected components (Steiniger et al., 2018a,b,

2020) or visualisation of shape diameter function (Steiniger et al., 2016).

13. QC as initial visual analytics. If the reconstruction, e. g., interrupts microvessels
or includes non-informative components, identify the cause and repeat from there.

14. Further visual analytics, e.g., mesh colouring.
15. Final rendering.

Figure 3 showcases some important steps of this pipeline.

3.3. Histological background

The human spleen is a secondary lymphatic organ which serves to immunologically
monitor the blood. In order to intensify the contact between blood-borne molecules,
which provoke immune reactions, and the speciﬁc immunocompetent lymphocytes and
macrophages, the spleen harbours a so-termed ‘open circulation system’. This system
is unique to the spleen. It does not comprise continuous arteries, arterioles, capillaries,
venules and veins as in other organs, but there is an open vessel-free space between
capillaries and the beginning of the draining venous system, which is passed by all
constituents of the blood. In addition, the initial venous vessels which re-collect the
blood into the circulation system are also organ-speciﬁc and are termed ‘sinuses’.

In humans, the initial parts of the splenic capillary network is covered by peculiar
multicellular arrangements termed ‘capillary sheaths’. The detailed function of these
sheaths is unknown, but comparative anatomy suggests, that they collect certain foreign
molecules from the blood and guide the immigration of special immunocompetent
lymphocytes (B-lymphocytes) into the spleen.

Prior to our works (Steiniger et al., 2018a,b), arrangement of capillary sheaths
has never been shown in three dimensions. It has been unknown, whether all splenic
capillaries are covered by sheaths, how long the sheaths are, what shape they have
and, ﬁnally, which cell types they consist of. In addition, the location of the sheaths
with respect to the open ends of capillaries feeding the ‘open circulation’ has remained
enigmatic.

During recent years our research (Steiniger et al., 2018a,b, 2020) has clariﬁed
many of these questions. We now report an advanced study comprising 3D models
derived from up to 150 serial paraﬃn sections stained for conventional transmitted light
microscopy utilising three diﬀerent chromogens (brown, blue and red) to visualise four
molecules by immunohistological methods. In detail, we demonstrate smooth muscle
alpha-actin (SMA), CD34, CD271 and CD20 (Table 1).

3.4. Input data

The input data was generated from the registered stack of serial sections. Typical
data volume was 2.3k × 2.3k × 161 voxels, with z-axis interpolation. The original
data typically featured 21 to 24 sections, but we have used up to 150 sections in some

9

Table 1: Cell distribution of the molecules detected in human spleens.

Target

Expression

Colour in (Steiniger et al., 2018a)
Colour in (Steiniger et al., 2018b)
Colour in (Steiniger et al., 2020)
Colour in ‘sinus’ data set

endothelial cells (inner vessel lining)
smooth muscle cells (wall of arteries, arteri-
oles)
ﬁbroblasts (connective tissue cells) at sur-
face of follicles
ubiquitous ﬁbroblasts in red pulp
ﬁbroblastic capillary sheath cells
ﬁbroblasts in trabeculae
B-lymphocytes
sinus endothelia

SMA
–
brown
brown
–

–
+

+

+/–
–3
+
–
–

1 stained in alternating serial sections
2 except endothelial cells of most sinuses
3 most cells
4 expression varies by proximity to a follicle

blue
blue
red1
red

CD34 CD271 CD20 CD141
brown
brown
blue
blue
+2
–

–
–
–
brown

–
–
red1
–

–
–

–
–

–
–

–

–
–
+/–
–
+/–4

+/–

+/–
+
–
–
–

–

–
–
–
+
–

+

–
–
–
–
+

reconstructions. The ﬁnal meshes had typically 1.7M–2.3M vertices. (Most of the
GPU memory used by the application was occupied by textures anyway.) Real time
rendering was possible with Vive-native resolution at 90 fps. Our experiments with
Valve Index ran at even higher frame rates. Original data were projected as textures,
typically at 2.3k × 2.3k. Although we experimented with showing all sections at once,
in a productive use only one section was shown at a time.

We quality control the following data sets derived from the bone marrow of a 53-
year-old male and from the spleen of a 22-year-old male. Acquisition of the specimens
complied with the ethical regulations of Marburg University Hospital at the time of
processing.

1. ‘Bone marrow’: stained with anti-CD34 plus anti-CD141 (both brown), 4 ROI
3500 × 3500 pixel at 0.5 µm/pixel, 21 serial sections (Steiniger et al. (2016); Fig. 4,
16).

2. ‘Follicle-double’: spleen sections stained with anti-CD34 (brown) followed by
anti-CD271 (blue), ROI 2300 × 2300 pixel at 0.416 µm/pixel, 24 serial sections
(Steiniger et al. (2018a); Fig. 7);

3. ‘Follicle-single’: spleen sections stained with anti-CD34 (brown), a ROI 4k × 4k

pixel, 0.3 µm/pixel, 24 serial sections (Steiniger et al. (2018a); Fig. 8);

4. ‘Red pulp’: spleen sections stained with anti-CD34 plus anti-SMA (both brown),
followed by anti-CD271 (violet-blue, diﬀerent pigment than above), 11 ROI
2k × 2k pixel at 0.5 µm/pixel, 24 serial sections (Steiniger et al. (2018b); Figs. 9,
13);

10

(a)

(b)

(c)

(d)

(e)

(f)

Figure 4: Images, renderings, and VR screenshots showing mesh reconstructions of blood vessels in a human
bone marrow specimen, stained with anti-CD34 plus anti-CD141. (a): A single section. The staining colour is
brown for both molecules in the original data. (b): A volume rendering of 21 consecutive serial sections. (c):
The reconstructed mesh. It shows shape diameter function values, colour-coded from red to green. (d): We
annotate a position of interest in the mesh in VR. An original section is seen in the background. (e): We have
found the section containing the annotation, the mesh is still visible. (f): Only the section with the annotation
is shown in VR. Domain experts can now reason on the stained tissue at the marked position.

5. ‘Sheaths alternating’: 148 immunostained sections plus 2 HE sections stained
with anti-SMA, anti-CD34 and anti-CD271. In every other section CD271 was
replaced by CD20. We processed 4 ROI at 2k × 2k pixel at 0.5 µm/pixel, of which
from 82 to 148 sections were used (Lobachev et al., 2019; Steiniger et al., 2020;
Lobachev, 2020), see also Figs. 3, 5, 10, 14, 15.

6. ‘Sinus’: 21 spleen sections immunostained with anti-CD141 (brown, not shown
here), anti-CD34 (blue), anti-CD271 (red), work in progress with currently two
ROI 2k × 2k pixel at 0.44 µm/pixel, shown in Figs. 11, 12. A phenotypical
investigation was performed in Steiniger et al. (2007), but without serial sections
and capillary sheaths.

4. System architecture and features

4.1. Components

Our application makes use of existing software libraries. We load meshes with the
VCG library. Multiple meshes with vertex colours are supported. We utilise Open GL 4.2.

11

(a)

(b)

Figure 5: Working with our annotation tool. We show VR screenshots of our application, in a human spleen
‘sheaths alternating’ data set (Steiniger et al., 2020). In Fig. (b) the front plane clipping is evident, viz. Fig. 10.
Notice the Valve Index controller with a ball, showing an anatomical structure. In this manner, the VR user
can clarify some morphological details or demonstrate an issue to an audience outside VR.
All images are produced with our VR tool. Similar illustrations can be found in (Steiniger et al., 2020).

We enable back face culling, multisampling, and mipmaps. The section textures are
loaded with the FreeImage library. The Steam VR library is used for interaction with
Vive controllers and the headset.

With respect to hardware, the system consists of a desktop computer with a Steam VR-

capable GPU and a Steam VR-compatible headset with controllers.

4.2. Controls

For control, a simple keyboard and mouse interface (for debugging outside VR),
XBox One controller, and Steam VR-compatible controllers can all be used. Our initial
idea was to use an XBox 360 or an XBox One controller, as such controllers provide
a simple control metaphor. However, the expert users were not acquainted to gaming
controllers and could not see the XBox One controller in VR. Thus, initial error rates
were high when they e. g., tried to simultaneously use an ‘X’ key and a ‘D-Pad’ in blind.
Hence, a more intuitive approach with the native Vive controllers was targeted. We
have kept the keyboard-and-mouse and the XBox controller options, but duplicated
required input actions with Vive controllers. Native HTC Vive controllers proved their
beneﬁts. Although the metaphors were much more complicated, the intuitive control
payed oﬀ immediately. Further, the visibility of the tracked controllers in VR helped a
lot. Later on, we extended the application support to Valve Index knuckle controllers.
Further spectators can follow the domain expert from ‘outside’ of the immersion,
as the HMD feed is mirrored on a monitor. Further, annotations signiﬁcantly improve
communication (Figs. 7–9). The main controller actions of the domain expert are:

• Blending the original data (the stained sections) in or out;

12

Figure 6: This ﬁgure demonstrates why we need geodesic distances for mesh painting in our VR application.
The yellow circle is the painting tool. We would like to mark the green blood vessels inside the circle, but
do not want to co-mark the red blood vessel, even if it is also inside the circle. Red and green blood vessels
might even be connected somewhere outside the circle, but the geodesic distance from the centre of the circle
(the black dot) to any vertex of the red blood vessel is too large, even if they are reachable. Conversely, a
part of the green blood vessel is selected, as a vertex of the green mesh is closest to the centre of the circle.
As many vertices are selected as the geodesic distance (corresponding to the radius of the circle with some
heuristics) allows for.

• Blending the mesh(es) in or out;

• Advancing the currently displayed section of original data;

• Placing an annotation;

• Mesh painting.

The most signiﬁcant user interaction happens with intuitive movements of the

immersed user around (and through) the displayed entities in VR.

4.3. Communication

Without a beacon visible in VR it is almost impossible to understand what the expert
tries to show. With a VR controller and our annotation tool, interesting areas in the
visualisation can be shown to the outside spectators in real time.

4.4. Annotation markers

We designed a spherical selection tool for marking points in space (Fig. 5). The
sphere is located at the top front of a Vive or Index controller and can be seen in the
virtual space (and, by proxy, also in the mirrored display, Fig. 2). We need to note,
however, that the annotation sphere appears much more vivid to the VR user than it
appears on screenshots. User’s movement and live feedback are in our opinion a major

13

reason for such a diﬀerence in perception. Figures 4d–4f, 5, 7b, 8, show our annotation
tool in images captured from VR sessions.

The annotations and mesh modiﬁcations are saved for further analysis. For example,
after the domain expert has marked suspicious areas, the 3D reconstruction expert can
inspect them in a later VR session. Reconstruction improvements can be deduced from
this information.

4.5. Anti-aliasing

If a ‘direct’ rendering approach is used, there is a very dominant aliasing eﬀect at
certain points. We used multisampling (MSAA) on meshes and mipmaps on textures to
alleviate this problem.

4.6. Front face culling

Consider the interplay between the model and original serial sections. A section is
not an inﬁnitely thin plane. We show the original data as an opaque cuboid that is one
section thick in the z direction and spans over the full surface in the xy plane. The actual
data points of the mesh, corresponding to the displayed section, are inside the opaque
block. Decisive parts of the mesh are occluded by the front face of the cuboid. On the
one hand, this is, of course, not desired, and requires correction. On the other hand, the
behaviour of the model spanning for multiple sections in front of the current section is
best studied when it ends at the front face of the cuboid. The solution is to enable or
disable front face culling of the original data display at will.

With front face culling enabled, the user can look inside the opaque block with
original section texture. This is well suited for the inspection of lesser details and small
artefacts. (Figure 9, (d), (e) features a real-life example, observe the edge of the section
shown.) The general behaviour of the model across multiple sections can be tracked
more easily with front faces of the original section on display. The presence of both
representations accelerates QC.

4.7. Geodesic distances for mesh painting

We also implemented a VR-based mesh painting facility, mostly based on MeshLab
code base (Cignoni et al., 2008). In this mode the colour spheres, which our user can
place with the controller, produce a geodesically coloured region on the mesh instead of
an annotation. These two functions, annotations and mesh painting, are conveyed to be
clearly diﬀerent to the user.

The selected colour is imposed on all vertices inside the geodesic radius from the
centre of the sphere. We would like to paint on, for example, a part of a blood vessel
that has a speciﬁc property. At the same time, we would like not to colour other blood
vessels that might be inside the painting sphere, but are not immediately related to the
selected blood vessel. This is facilitated with geodesic distances, as Figures 6 shows.

The markings from mesh painting lead to the ﬁnal separation of the entities (such as

blood vessel types, kinds of capillary sheaths, etc.) in the visualisation.

14

(a)

(b)

(c)

(d)

(e)

(f)

Figure 7: Real or artefact? The models are derived from human spleen sections from the ‘follicle-double’
data set. These sections were stained for CD34 (brown in staining, yellow in the reconstruction) and for
CD271 (blue). In VR we spotted and annotated putative capillaries inside follicles (large blue structures, a, b).
We can look at the meshes only (c) or also show the original data (d). A closer view (e), (f) conﬁrms: the
reconstruction is correct, these structures are CD34+ objects inside the follicle. As the structures in question
continue through multiple sections, they do not represent single CD34+ cells. Hence the objects in question
must be blood vessels. The reconstruction is correct, the brown structures are real.
All images in this ﬁgure are screenshots from our application. Similar results can be found in (Steiniger et al.,
2018a).

Figure 8: A VR screenshot showing mesh reconstructions of blood vessels in a human spleen specimen,
anti-CD34 staining, ‘follicle-single’ data set (Steiniger et al., 2018a). Unconnected mesh components were
set to distinct colours. The user is highlighting a smaller blood vessel that follows larger ones with the HTC
Vive controller.

4.8. Front plane clipping

The classic view frustum in computer graphics consists of six planes, four ‘display
edges’ building the frustum sides, a back plane (the farthest visible boundary) and the
front plane, the closest boundary. The clipping planes are recomputed when the camera
(i.e., the user) is moving. In the cases when there are too many self-occluding objects
in the scene, the observer cannot ‘pierce through’ further than few closest objects. In
other words, the observer can only see the closest objects. (This fact motivates occlusion
culling.)

Such an occlusion was the case with our denser data sets. With a simple change in
the code, we moved the front plane of the view frustum further away, in an adjustable
manner. Basically, the user ‘cuts’ parts of the reconstruction in front of their eyes,
allowing for the detailed inspection of the inside of the reconstruction.

This adjustment is very minor from the computer graphics point of view, but it was
very much welcomed by our actual users, the medical experts. With appropriate front
plane clipping set at about 60 cm from the camera, it becomes possible to inspect very
dense medical data sets from ‘inside’. (Figs. 5, 10, 14, 15 demonstrate this eﬀect.) The
user ‘cuts away’ the currently unneeded layers with their movements.

5. Results

5.1. Hardware

We conducted most of our investigations on a 64-bit Intel machine with i7-6700K
CPU at 4 GHz, 16 GB RAM, and Windows 10. We used NVidia GTX 1070 with 8 GB
VRAM, and HTC Vive.

16

Our VR application was initially developed with HTC Vive in mind; it performed
well on other headsets, such as HTC Vive Pro Eye and Valve Index. We observed
convincing performance on Intel i7-9750H at 2.6 GHz, 64 GB RAM (MacBook Pro 16 (cid:48)(cid:48))
and NVidia RTX 2080 Ti with 11 GB VRAM in Razor Core X eGPU with HTC Vive
Pro Eye, as well as on AMD Ryzen 2700X, 32 GB RAM, NVidia RTX 2070 Super with
8 GB VRAM with Valve Index. Our application also should perform well with further
headsets such as Oculus Rift. It was possible to use previous-generation GPUs, we also
tested our application with NVidia GTX 960. Overall, it is possible to work with our
application using an inexpensive setup.

The largest limitation factor seems to be the VRAM used by the uncompressed
original image stack. The second largest limitation is the number of vertices of the
visualised meshes and the rasteriser performance in case of very large, undecimated
reconstructions.

5.2. ‘Bone marrow’ data set

We have reconstructed the 3D shape of smaller and larger microvessels in hard,
undecalciﬁed, methacylate-embedded human bone serial sections (Steiniger et al., 2016).
Shape diameter function on the reconstructed mesh allows to distinguish capillaries from
sinuses. Figure 4 shows (a) a single section (part of the input data to reconstruction), (b)
a volume rendering of all 21 sections, and (c) our 3D reconstruction. In Steiniger et al.
(2016) we did not use VR. Here, we use the same data set to showcase some features
of our VR-based method. It took us much more manpower and time to validate the
reconstructions then, without VR, as Section 6 details (Fig. 16).

The process of annotation is demonstrated in Fig. 4, (d). The next subﬁgures show
further investigation of the annotated area in VR either in combined mesh-section
view (e), or showing the corresponding section only (f). To discriminate between
capillaries (smaller, coloured red) and sinuses (larger, coloured green), we computed
shape diameter function on the reconstructed meshes and colour-coded resulting values
on the mesh, as shown in (c)–(e). The handling of the reconstruction and serial section
data in VR showcases the annotation process.

5.3. ‘Follicle-double’ data set

The human spleen contains accumulations of special migratory lymphocytes, the
so-termed follicles. We reconstructed the capillaries inside and outside the follicles
(Steiniger et al., 2018a). We show some results from this work in this section and in the
next. Fig. 7 presents one of three ROIs that were quality controlled.

Our 3D reconstruction demonstrates that follicles are embedded in a superﬁcial
capillary meshwork resembling a basketball basket. Figure 7 shows that our VR tool
enables easy annotation and projection of the original data leading to further results
(Steiniger et al., 2018a). In Fig. 7, (e), some brown dots have been marked inside a
follicle. The 3D model shows, that the dots indeed represent capillaries cut orthogonally
to their long axis. Thus, we additionally ﬁnd that infrequent capillaries also occur inside
the follicles. The superﬁcial capillary network of the follicles is thus connected to very
few internal capillaries and to an external network of capillaries in the red pulp. We
observed the latter two networks to have a shape which is totally diﬀerent from the

17

(a)

(b)

(c)

(d)

(e)

Figure 9: Investigating the annotated regions, VR screenshots of our application.The human spleen ‘red
pulp’ data set is used (Steiniger et al., 2018b); we have annotated some ends of capillary sheaths in meshes
reconstructed from human spleen data. (a): Overview. (b)–(d): Original data and an annotation. Experts can
more easily reason on such visualisations because of 3D perception and intuitive navigation. (e): The same
annotation as in (d), showing additionally the mesh for sheaths.

18

superﬁcial follicular network. The external network is partially covered by capillary
sheaths stained in blue colour. In total, we examined three ‘follicle-double’ data sets in
VR.

5.4. ‘Follicle-single’ data set

To continue the investigation of capillaries inside and outside the follicles, Fig. 8
shows that the annotated elongated structures in the follicles and in the T-cell zone at
least partially belong to long capillaries, which accompany the outside of larger arteries,
so-termed vasa vasorum. With our VR-based method, we investigated this 4k × 4k
ROI at 0.3 µm/pixel and three further ROIs (not shown) with 1600 × 1600 pixels at
0.6 µm/pixel (Steiniger et al., 2018a). Fig. 8 also shows a Vive controller tracing one
of the longer capillaries with the annotation ball as a form of communication of the
ﬁndings to spectators outside VR.

5.5. ‘Red pulp’ data set

The location of capillary sheaths in human spleens has not been clariﬁed in detail
until recently (Steiniger et al., 2018b). Our 3D reconstructions indicate that sheaths
primarily occur in a post-arteriolar position in the part of the organ, which does not
contain lymphocyte accumulations (so-termed red pulp), although length and diameter
of the sheaths are variable. Many sheaths are interrupted by the boundaries of the
ROI. (The remedy was a longer series of sections, as presented in Section 5.6.) For
this reason it makes sense to collect only sheaths which are completely included in the
reconstruction. Such a selection was done with our VR classiﬁcation tool.

Figure 9, (a) shows an overview of the annotations. In Figs. 9, (b)–(d) it becomes
clear, that the sheaths indeed end at the marked positions. Notice the enabled front
face culling on the section cuboid in the closeups. Figure 9, (e) additionally shows the
reconstructed meshes for the sheaths. We show a single ROI at 2k × 2k pixels. We have
inspected 11 such ROIs in VR.

5.6. ‘Sheaths alternating’ data set and clipping

The ‘sheaths alternating’ data set with up to 150 sections was created to further
investigate the morphology and (to some extent) the function of capillary sheaths
(Steiniger et al., 2020). The resulting 3D data set was extremely dense. The increased
amount of ‘channels’ and the nature of the study (tracking the blood vessels) was a
big challenge. The amount of the reconstructed blood vessels and their self-occlusion
prohibited any possible insight when viewing them from the outside. Here we utilised
front plane clipping (Section 4.8). Figures 5b, 10, 14 (and also Fig. 11 for ‘sinus’ data
set) showcase this minor, but important adjustment. Figs. 14, 15 further demonstrate the
complexity of the ‘sheaths alternating’ data set.

5.7. Mesh painting and obtaining insights

As already seen in Figs. 5, 7, (a), (b), 8, we can point to anatomical structures with
the Valve Index controller. Similarly, annotations can be placed and mesh fragments
can be painted in diﬀerent colours. An example of real-life mesh painting with geodesic

19

(a)

(b)

(c)

(d)

Figure 10: Showcasing front plane clipping on ‘sheaths alternating’ data sets (Steiniger et al., 2020). (a):
A complete data set in a frontal visualisation. (b): The user cuts into objects of interest using clipping. (c)–(d):
Utilisation of clipping during the exploration of the data set.
All images are produced with our VR tool either directly or with Steam VR interface. (a), (b) were featured in
a poster (Lobachev et al., 2019). (a), (c), (d): Similar illustrations can be found in (Steiniger et al., 2020).

distances is in Figure 12. The arrows show a part of a structure already painted by user
in red in (a). It is painted back to blue in (b).

In this manner we have reﬁned an automatic heuristics for arterioles (larger blood

vessels, red in Fig. 13) to be always correct and to lead up to the capillary sheaths.

With a similar tool, working on unconnected components, we changed the colour
of the sheaths in the ‘red pulp’ and ‘sheaths alternating’ data sets. The colour change
eﬀected the classiﬁcation of the sheaths. The sheaths were initially all blue in our
visualisations of the ‘red pulp’ data set. Sheaths around capillaries, following known
arterioles, were then coloured green. We also annotated very few capillaries that should
have a sheath, but did not (white). Figure 13 shows one of the ﬁnal results, a connected
vascular component with accompanying sheaths, follicle structures, and smooth muscle
actin.

20

Figure 11: Cutting structures open with front clipping plane, using the ‘sinus’ data set. Capillaries are blue,
capillary sheaths are green in the reconstruction. An original section is visible on the right.

Figure 14 underlines the complexity of the ‘sheaths alternating’ data set. Both
images in this ﬁgure show the ﬁnal results of mesh painting, the actual work is already
done. Still, they convey how interwoven and obscure the original situation is. CD271+
cells, mostly present in capillary sheaths, are in various shades of green in this ﬁgure.
Figure 14 is highly complex; the fact that something can be seen in still images is the
merit of applied annotations and mesh painting, of an active front plane clipping, and of
a proper choice of a view point. A viewer in VR has no problem navigating such data
sets because of the active control of view direction and of movement dynamics. The
latter also means a control of the front plane clipping dynamics.

Figure 15 shows a further development: a separated arteriole with its further capillary
branches and capillary sheaths (Steiniger et al., 2020). The sheath in the ﬁgure was cut
open by front plane clipping. This ‘separation’ was generated from user inputs in VR,
similar to the previous ﬁgure. Now, however, the complexity is reduced to a degree, that
allows showing the still image in a medical research publication.

Summarising, our visualisations in VR were used to obtain insights (Steiniger et al.,
2018b, 2020) on the position of the capillary sheaths—a problem that was initially
discussed (Schweigger-Seidel, 1862) more than 150 years ago!

6. Discussion

The novelty of this work stems from how VR streamlines and facilitates better QC
and VA of our reconstructions. The presented VR-based tool plays an important role in
our pipeline. ‘Usual’ 3D reconstructions from serial histological sections are known,
but are quite rare, because they involve cumbersome tasks. The reasons for this are
threefold: technical diﬃculties to create them; struggles to QC the reconstructions;

21

(a)

(b)

(c)

Figure 12: An ongoing session of mesh painting with geodesic distances as VR screenshots. We use the
‘sinus’ data set. (a): Notice the huge annotation ball on the controller, the bright red dot is its centre. This
centre is the starting point of the geodesic computation, initiated by the trigger on the controller. The large
radius of the marking tool is bounded by the connectivity: the vertices which are within the radius, but are not
connected to the starting point or are ‘too far’ geodesically, are not painted.
(b): For better visibility, we show a crop from the left eye view of (a). The white arrow shows a point of
interest.
(c): An excessive marking (white arrow), is removed with a repeated painting operation. On the bottom left
in (a), (c) a Valve Index controller is visible. The background colour in this ﬁgure signiﬁes the highlighting
mode used.

Figure 13: Final result of mesh annotation and painting, ‘red pulp’ data set (Steiniger et al., 2018b). Blood
vessels are yellow. Certain support structures in the spleen that feature smooth muscle actin are also
reconstructed and displayed in yellow. (A trained histologist can discern these structures from various kinds of
blood vessels though.) Some of the blood vessels (red) lead from larger blood vessels (arterioles) to capillary
sheaths (green). Some sheaths are fed by arterioles not traced in the reconstruction. These sheaths are marked
blue. Finally, while some capillaries are red (having green sheaths), some other capillaries, coming from the
same arteriole, do not have a sheath at all. Such capillaries are coloured in white. The background is black to
better discern the white colour.
A similar, but diﬀerent image appeared in (Steiniger et al., 2018b) under CC-BY 4.0 license.

investigation and comprehension problems in dense, self-occluding reconstructions.
A proper registration, correct reconstruction options, and possibly also inter-slice in-
terpolation are necessary for creating a satisfying reconstruction. For QC we need to
visually ensure the correctness of processing, identify what the reconstruction actually
shows, keep artefacts at bay by creating a better reconstruction if needed. Finding a
good reconstruction from previously unexplored data with a new staining is an iterative
process. While we create the reconstructions quite eﬃciently, QC was a lot of work
in the past. With an immersive VR application, QC is much easier and faster, in our
experience.

Annotations and mesh colouring provide for visual analytics abilities and these
facilitate better distinction between various aspects of the reconstruction. To give some
example, capillary sheaths, surely following arterioles, can be separated from capillary
sheaths, the origins or ends of which lie outside of the ROI. Such distinctions allow for
better understanding in microanatomical research.

Our experience emphasises the importance of VR-based QC. Our older 3D recon-
struction study (Steiniger et al., 2016) featured 3500 × 3500 × 21 voxels in four regions.

23

(a)

(b)

Figure 14: The natural complexity of the ‘sheaths alternating’ data set (Steiniger et al., 2020) is very high.
With mesh annotation, painting, and removal of irrelevant details we were able to keep the complexity at a
tolerable level.
(a): The capillary network of the splenic red pulp is blue. Arterioles have been highlighted in red. Capillary
sheaths are green and dark green, depending on whether they belong to the red vessels. Special supportive
cells in a follicle are light green. An arteriole (red) is entering from left, one of the branches is splitting up
into capillaries (still red) that immediately enter capillary sheaths. One such sheath (green) is is cut open,
showing the capillary inside.
(b): Arterioles and sheathed capillaries are light blue, capillary sheaths are green. The open-ended side
branches of sheathed capillaries are red. This ﬁgure shows a single system, starting with an arteriole. It has
been separated from other arterial vessel systems in the surroundings. Front plane clipping opens the capillary
sheath and shows the blue capillary inside. We see some open green capillary sheaths with light blue ‘main
line’ blood vessels inside.
Similar, but diﬀerent ﬁgures can be found in Steiniger et al. (2020).

Figure 15: Final result of mesh annotation, painting, and removal of irrelevant details. The complexity is now
greatly reduced. This is the ‘sheaths alternating’ data set (Steiniger et al., 2020). The meshes are cut open by
the front clipping plane. Blood vessels are red, capillary sheaths are green, cells in follicle are light green.
An arteriole (red) is entering from left in the proximity of a follicle, this arteriole splits further, one of the
branches is splitting up into capillaries (still red) that immediately enter capillary sheaths. One such sheath
(green) is curved around the follicle. The sheath is cut open, showing the capillary inside.
A similar, but diﬀerent ﬁgure, depicting a diﬀerent sheath, can be found in Steiniger et al. (2020).

From each reconstruction a further version was derived. They did not need to be quality
controlled again, but their inspection was crucial to produce a diagnosis for medical
research. We used both a non-VR visualisation tool for QC and pre-rendered videos for
inspection. It took a group of 3 to 5 experts multiple day-long meetings to QC these
reconstructions with the non-VR tool (Fig. 16). Deducing the anatomical ﬁndings from
pre-rendered videos was also not easy for the domain experts.

We found free user movement essential for long-term usability of our application—
our users spend hours immersed in consecutive sessions. Basically, the model is not
otherwise translated, rotated, or scaled in the productive use, but only in response to
tracking and reacting to user’s own movements. Such free user movement allows the
immersed user to utilise their brain’s systems for spatial orientation and spatial memory.
In their turn, the recognition and annotation of structures become easier. Free user
movement also distinguishes our application from Egger et al. (2020): they used a VR
ﬂight mode on 3D models from CT.

We ﬁrst found the beneﬁts of VR-based visualisation during the preparation of

25

Steiniger et al. (2018a). Unlike the bone marrow data set (Steiniger et al., 2016), in our
next work (Steiniger et al., 2018b), the total number of voxels was slightly larger, and
QC was much faster with the new method. Our domain expert alone quality controlled
with our VR-based method eleven regions with 2000 × 2000 × 24 voxels per day in one
instance (Steiniger et al., 2018b) and two to four ≈ 2000 × 2000 × 84 regions per day in
another instance (Steiniger et al., 2020). These sum up to slightly more than 109 voxels
per day in the ﬁrst case and up to 1.36 · 109 voxels per day in the second case. We would
like to highlight, that these amounts of data were routinely quality controlled by a single
person in a single day. Thus VR immersion saved an order of magnitude of man-hours
for QC of our medical research 3D reconstructions (Steiniger et al., 2018a,b, 2020).

Our immersive application also enabled VA of the same reconstructions. Without
immersive VA and (later on) interactive ‘cutting’ into the reconstructions with front
plane clipping in VR, it would be exorbitantly harder or even impossible for us to obtain
the research results, summarised in Figs. 13–15 (Steiniger et al., 2018b, 2020).

7. Conclusions

3D reconstructions from histological serial sections require quality control (QC)
and further investigations. Domain experts were not satisﬁed by previously existing QC
methods. We present a VR-based solution to explore mesh data. Our application also
allows to superimpose the original serial sections. Such display is essential for QC. In our
experience, immersion accelerates QC by an order of magnitude. Our users can annotate
areas of interest and communicate the annotations. VR-powered VA allowed for a more
exact and fast distinction and classiﬁcation of various microanatomical entities, such as
post-arteriolar capillaries and other kinds of capillaries. The classiﬁcation of arterial
blood vessels in its turn facilitated the classiﬁcation of capillary sheaths. Summarising,
our VR tool greatly enhances productivity and allows for more precise reconstructions
that enable new insights (Steiniger et al., 2018a,b, 2020) in microanatomical research.

7.1. Future work

Making our application an even a better visual analytics tool is always viable. Minor
improvements at user input handling include more input combinations and gestures.
A planned feature is to record spoken annotations for every annotation marker. Recorded
memos would facilitate better explanation of markings at their revision. The application
has a potential to evolve in the direction of a non-medical 3D sculpting utility. A better
maintainability of the code base through an excessive use of software product lines
(Apel et al., 2016) is an important goal. Not all builds need all features and software
product lines can accommodate this point.

Improvements of the rendering performance are both important and viable. Possible
points of interest are better occlusion culling (e. g., Mattausch et al., 2008; Hasselgren
et al., 2016) and progressive meshes (e.g., Derzapf and Guthe, 2012). There are further
ways to improve the anti-aliasing and thus even further improve the immersive user
experience. A possibility to consider is an advanced interpolation for higher internal
frame rates.

A promising idea is to learn better view angles (similar to Burns et al., 2007) from
the transformation matrices saved as parts of annotations. Better pre-rendered videos

26

(a)

(b)

Figure 16: Showcasing our non-VR volume renderer. Endothelia of blood vessels are stained brown in
‘bone marrow’ data set. The blended-in mesh is blue. The volume renderer played an important role in data
veriﬁcation for our publication (Steiniger et al., 2016). (a) shows volume data representation, (b) presents the
visualisation of the ﬁnal, ﬁltered mesh vs. corresponding single section.

might be produced in this manner. (Guti´errez et al., 2018, have a similar motivation.)
Texture compression in general and volume compression techniques in particular, [e.g.,
Guthe et al. (2002);Guthe and Goesele (2016);Guarda et al. (2017)}, would help to
reduce the GPU memory consumption caused by data for original slices.

VR might be the pivotal instrument for better understanding in teaching complex 3D
structures (Philippe et al., 2020), e.g., in medicine or in machine engineering. An eﬀect
of VR in training and education in such professions (and also in other areas, e.g., Calvert
and Abadia, 2020) might need a more detailed assessment.

Of course, viable future work includes applications of our visualisations to recon-
structions of further organs and tissues (e.g., future bone marrow, lung, heart, or tonsil
probes) and expansion to further modalities of medical (such as MRI or CT) or non-
medical data. Recently, we experimented with VR presentation of serial block face
electron microscopy data. Multi-modality is an interesting topic, too (Tang et al., 2020).
Possible examples of further applications include materials science, computational ﬂuid
dynamics, and, most surely, computer graphics.

Acknowledgements

Most of this work was done when the ﬁrst two authors were members of University
of Bayreuth. We would like to thank Vitus Stachniss, Verena Wilhelmi, and Christine
Ulrich (Philipps-University of Marburg) for their eﬀorts with non-VR QC tool. We thank
Christian M¨uhlfeld (Hannover Medical School) for the possibility to test our application
on a MacBook Pro 16 (cid:48)(cid:48). Paul Schmiedel (then: University of Bayreuth) worked on the
codebase of the VR tool in Bayreuth. Figure 2 depicts Lena Voß, we would like to thank
her for the permission to use this image.

References

Ahrens, J., Geveci, B., Law, C., 2005. ParaView: An end-user tool for large data visual-
ization. The visualization handbook 717. URL: http://datascience.dsscale.
org/wp-content/uploads/2016/06/ParaView.pdf. LA-UR-03-1560.

27

Apel, S., Batory, D., K¨astner, C., Saake, G., 2016. Feature-oriented software product

lines. Springer. doi:10.1007/978-3-642-37521-7.

Ayachit, U., 2015. The ParaView guide: a parallel visualization application. Kitware,

Inc.

Berg, L.P., Vance, J.M., 2017. Industry use of virtual reality in product design and man-
ufacturing: a survey. Virtual Real. 21, 1–17. doi:10.1007/s10055-016-0293-9.

Bouaoud, J., El Beheiry, M., Jablon, E., Schouman, T., Bertolus, C., Picard, A., Mas-
son, J.B., Khonsari, R.H., 2020. DIVA, a 3D virtual reality platform, improves
undergraduate craniofacial trauma education. Journal of Stomatology, Oral and Max-
illofacial Surgery URL: http://www.sciencedirect.com/science/article/
pii/S2468785520302214, doi:10.1016/j.jormas.2020.09.009.

Brooks Jr., F.P., 1999. What’s real about virtual reality? IEEE Comput. Graph. 19,

16–27. doi:10.1109/38.799723.

Burns, M., Haidacher, M., Wein, W., Viola, I., Gr¨oller, M.E., 2007. Feature emphasis
and contextual cutaways for multimodal medical visualization, in: Proceedings of the
9th Joint Eurographics / IEEE VGTC Conference on Visualization, EG. pp. 275–282.
doi:10.2312/VisSym/EuroVis07/275-282.

Calvert, J., Abadia, R., 2020. Impact of immersing university and high school students in
educational linear narratives using virtual reality technology. Computers & Education
159, 104005. doi:10.1016/j.compedu.2020.104005.

Cal`ı, C., Kare, K., Agus, M., Veloz Castillo, M.F., Boges, D., Hadwiger, M., Magistretti,
P., 2019. A method for 3D reconstruction and virtual reality analysis of glial and
neuronal cells. JoVE 151, e59444. doi:10.3791/59444.

Chan, S., Conti, F., Salisbury, K., Blevins, N.H., 2013. Virtual reality simulation in
neurosurgery: Technologies and evolution. Neurosurgery 72, A154–A164. doi:10.
1227/NEU.0b013e3182750d26.

Checa, D., Bustillo, A., 2020. A review of immersive virtual reality serious games
to enhance learning and training. Multimed. Tools Appl. 79, 5501–5527. doi:10.
1007/s11042-019-08348-9.

Chen, X., Xu, L., Wang, Y., Wang, H., Wang, F., Zeng, X., Wang, Q., Egger, J.,
2015. Development of a surgical navigation system based on augmented reality using
an optical see-through head-mounted display. J. Biomed. Inform. 55, 124–131.
doi:10.1016/j.jbi.2015.04.003.

Choi, K.S., Chan, S.T., Leung, C.H.M., Chui, Y.P., 2016.

Stereoscopic three-
dimensional visualization for immersive and intuitive anatomy learning, in: IEEE
8th International Conference on Technology for Education, IEEE. pp. 184–187.
doi:10.1109/T4E.2016.046.

28

Cignoni, P., Callieri, M., Corsini, M., Dellepiane, M., Ganovelli, F., Ranzuglia, G., 2008.
Meshlab: an open-source mesh processing tool., in: Eurographics Italian Chapter
Conference, pp. 129–136.

Daly, C.J., 2018. The future of education? Using 3D animation and virtual reality in
teaching physiology. Physiology News 111, 43. URL: http://www.physoc.org/
magazine.

Daly, C.J., 2019a. From confocal microscope to virtual reality and computer games;
Infocus Magazine 54, 51–59. URL:

technical and educational considerations.
http://eprints.gla.ac.uk/188105/.

Daly, C.J., 2019b. Imaging the vascular wall: From microscope to virtual reality, in:
Touyz, R.M., Delles, C. (Eds.), Textbook of Vascular Medicine. Springer, Cham, pp.
59–66. doi:10.1007/978-3-030-16481-2_6.

van Dam, A., Forsberg, A.S., Laidlaw, D.H., LaViola, J.J., Simpson, R.M., 2000.
Immersive VR for scientiﬁc visualization: a progress report. IEEE Comput. Graph.
20, 26–52. doi:10.1109/38.888006.

Derzapf, E., Guthe, M., 2012. Dependency-free parallel progressive meshes. Comput.

Graph. Forum 31, 2288–2302. doi:10.1111/j.1467-8659.2012.03154.x.

Dorweiler, B., Vahl, C.F., Ghazy, A., 2019. Zukunftsperspektiven digitaler Visu-
Gef¨asschirurgie 24, 531–538.

alisierungstechnologien in der Gef¨aßchirurgie.
doi:10.1007/s00772-019-00570-x.

Duarte, M.L., Santos, L.R., Guimar˜aes J´unior, J.B., Peccin, M.S., 2020. Learning
anatomy by virtual reality and augmented reality. A scope review. Morphologie
doi:10.1016/j.morpho.2020.08.004.

Egger, J., Gall, M., Wallner, J., Boechat, P., Hann, A., Li, X., Chen, X., Schmalstieg, D.,
2017. HTC Vive MeVisLab integration via OpenVR for medical applications. PLOS
ONE 12, 1–14. doi:10.1371/journal.pone.0173972.

Egger, J., Gunacker, S., Pepe, A., Melito, G.M., Gsaxner, C., Li, J., Ellermann, K., Chen,
X., 2020. A comprehensive workﬂow and framework for immersive virtual endoscopy
of dissected aortae from CTA data, in: Fei, B., Linte, C.A. (Eds.), Medical Imaging
2020: Image-Guided Procedures, Robotic Interventions, and Modeling, SPIE. pp.
774–779. doi:10.1117/12.2559239.

El Beheiry, M., Doutreligne, S., Caporal, C., Ostertag, C., Dahan, M., Masson, J.B.,
J. Mol. Biol. 431, 1315–1321.

2019. Virtual reality: Beyond visualization.
doi:10.1016/j.jmb.2019.01.033.

Esfahlani, S.S., Thompson, T., Parsa, A.D., Brown, I., Cirstea, S., 2018. ReHabgame: A
non-immersive virtual reality rehabilitation system with applications in neuroscience.
Heliyon 4, e00526. doi:10.1016/j.heliyon.2018.e00526.

29

Faludi, B., Zoller, E.I., Gerig, N., Zam, A., Rauter, G., Cattin, P.C., 2019. Direct visual
and haptic volume rendering of medical data sets for an immersive exploration in
virtual reality, in: Shen, D., Liu, T., Peters, T.M., Staib, L.H., Essert, C., Zhou, S., Yap,
P.T., Khan, A. (Eds.), Medical Image Computing and Computer Assisted Intervention
– MICCAI 2019, Springer, Cham. pp. 29–37. doi:10.1007/978-3-030-32254-0_
4.

Forsberg, A.S., Laidlaw, D.H., van Dam, A., Kirby, R.M., Karniadakis, G.E., Elion,
Immersive virtual reality for visualizing ﬂow through an artery, in:

J.L., 2000.
Visualization, IEEE Computer Society Press. pp. 457–460.

Guarda, A.F.R., Santos, J.M., da Silva Cruz, L.A., Assunc¸ ˜ao, P.A.A., Rodrigues,
N.M.M., de Faria, S.M.M., 2017. A method to improve HEVC lossless coding
of volumetric medical images. Signal Process.-Image 59, 96–104. doi:10.1016/j.
image.2017.02.002.

Guthe, S., Goesele, M., 2016. Variable length coding for GPU-based direct volume
rendering, in: Vision, Modeling & Visualization, EG. pp. 77–84. doi:10.2312/vmv.
20161345.

Guthe, S., Wand, M., Gonser, J., Strasser, W., 2002.

Interactive rendering of large
volume data sets, in: IEEE Visualization, pp. 53–60. doi:10.1109/VISUAL.2002.
1183757.

Guti´errez, J., David, E., Rai, Y., Callet, P.L., 2018. Toolbox and dataset for the
development of saliency and scanpath models for omnidirectional/360° still images.
Signal Process.-Image 69, 35–42. doi:10.1016/j.image.2018.05.003.

Hasselgren, J., Andersson, M., Akenine-M¨oller, T., 2016. Masked software occlusion

culling, in: Proceedings of High Performance Graphics, EG. pp. 23–31.

Inoue, A., Ikeda, Y., Yatabe, K., Oikawa, Y., 2016. Three-dimensional sound-ﬁeld
visualization system using head mounted display and stereo camera. Proceedings of
Meetings on Acoustics 29, 025001–1–13. doi:10.1121/2.0000381.

Ju, T., 2004. Robust repair of polygonal models. ACM T. Graphic. 23, 888–895.

Kikinis, R., Pieper, S.D., Vosburgh, K.G., 2014. 3D Slicer: A platform for subject-
speciﬁc image analysis, visualization, and clinical support, in: Jolesz, F.A. (Ed.),
Intraoperative Imaging and Image-Guided Therapy, Springer. pp. 277–289. doi:10.
1007/978-1-4614-7657-3_19.

Knodel, M.M., Lemke, B., Lampe, M., Hoﬀer, M., Gillmann, C., Uder, M., Hillengaß,
J., Wittum, G., B¨auerle, T., 2018. Virtual reality in advanced medical immersive
imaging: a workﬂow for introducing virtual reality as a supporting tool in medical
imaging. Comput. Visual Sci. 18, 203–212. doi:10.1007/s00791-018-0292-3.

Liimatainen, K., Latonen, L., Valkonen, M., Kartasalo, K., Ruusuvuori, P., 2020. Virtual
reality for 3D histology: multi-scale visualization of organs with interactive feature
exploration. arXiv:2003.11148 [cs, eess] URL: http://arxiv.org/abs/2003.
11148. arXiv: 2003.11148.

30

Lobachev, O., 2018. On Three-dimensional reconstruction. Habilitation thesis.

University of Bayreuth.

Lobachev, O., 2020. The tempest in a cubic millimeter: Image-based reﬁnements
necessitate the reconstruction of 3D microvasculature from a large series of damaged
alternately-stained histological sections.
IEEE Access 8, 13489–13506. doi:10.
1109/ACCESS.2020.2965885.

Lobachev, O., Pfeﬀer, H., Guthe, M., Steiniger, B.S., 2019.

Inspecting human 3D
histology in virtual reality. Mesoscopic models of the splenic red pulp microvas-
culature computed from immunostained serial sections, in: 114th Annual Meeting,
Anatomische Gesellschaft, Institut f¨ur Anatomie und Zellbiologie der Universit¨at
W¨urzburg. Poster.

Lobachev, O., Steiniger, B.S., Guthe, M., 2017a.

Compensating anisotropy in
histological serial sections with optical ﬂow-based interpolation, in: Proceedings
of the 33rd Spring Conference on Computer Graphics, ACM. pp. 14:1–14:11.
doi:10.1145/3154353.3154366.

Lobachev, O., Ulrich, C., Steiniger, B.S., Wilhelmi, V., Stachniss, V., Guthe, M., 2017b.
Feature-based multi-resolution registration of immunostained serial sections. Med.
Image Anal. 35, 288–302. doi:10.1016/j.media.2016.07.010.

Lorensen, W.E., Cline, H.E., 1987. Marching cubes: A high resolution 3D surface
construction algorithm. ACM SIGGRAPH Computer Graphics 21, 163–169. doi:10.
1145/37402.37422.

L´opez Ch´avez, O., Rodr´ıguez, L.F., Gutierrez-Garcia, J.O., 2020. A comparative
case study of 2D, 3D and immersive-virtual-reality applications for healthcare edu-
cation. International Journal of Medical Informatics 141, 104226. doi:10.1016/j.
ijmedinf.2020.104226.

Mann, S., Furness, T., Yuan, Y., Iorio, J., Wang, Z., 2018. All reality: Virtual, augmented,
mixed (x), mediated (x, y), and multimediated reality. arXiv:1804.08386. arXiv
preprint.

Mathur, A.S., 2015. Low cost virtual reality for medical training, in: 2015 IEEE Virtual

Reality (VR), pp. 345–346. doi:10.1109/VR.2015.7223437.

Mattausch, O., Bittner, J., Wimmer, M., 2008. CHC++: Coherent hierarchical culling
revisited. Comput. Graph. Forum 27, 221–230. doi:10.1111/j.1467-8659.2008.
01119.x.

Misiak, M., Schreiber, A., Fuhrmann, A., Zur, S., Seider, D., Nafeie, L., 2018. IslandViz:
A tool for visualizing modular software systems in virtual reality, in: 2018 IEEE
Working Conference on Software Visualization (VISSOFT), IEEE. pp. 112–116.
doi:10.1109/VISSOFT.2018.00020.

31

Moro, C., ˇStromberga, Z., Raikos, A., Stirling, A., 2017. The eﬀectiveness of virtual
and augmented reality in health sciences and medical anatomy. Anat Sci Educ 10,
549–559. doi:10.1002/ase.1696.

Philippe, S., Souchet, A.D., Lameras, P., Petridis, P., Caporal, J., Coldeboeuf, G., Duzan,
H., 2020. Multimodal teaching, learning and training in virtual reality: a review and
case study. Virtual Reality & Intelligent Hardware 2, 421–442. doi:10.1016/j.
vrih.2020.07.008.

Pieper, S., Halle, M., Kikinis, R., 2004. 3D Slicer, in: IEEE International Symposium
on Biomedical Imaging: Nano to Macro, pp. 632–635. doi:10.1109/ISBI.2004.
1398617.

Pieterse, A.D., Hierck, B.P., de Jong, P.G.M., Kroese, J., Willems, L.N.A., Reinders,
M.E.J., 2020. Design and implementation of ‘AugMedicine: Lung Cases,’ an
augmented reality application for the medical curriculum on the presentation of
dyspnea. Front. Virtual Real. 1. doi:10.3389/frvir.2020.577534. publisher:
Frontiers.

Preim, B., Saalfeld, P., 2018. A survey of virtual human anatomy education systems.

Comput. Graph. 71, 132–153. doi:10.1016/j.cag.2018.01.005.

Quam, D.J., Gundert, T.J., Ellwein, L., Larkee, C.E., Hayden, P., Migrino, R.Q., Otake,
Immersive visualization for enhanced computational
J. Biomech. Eng.-T. ASME 137, 031004–031004–12.

H., LaDisa, Jr., J.F., 2015.
ﬂuid dynamics analysis.
doi:10.1115/1.4029017.

Ritter, F., Boskamp, T., Homeyer, A., Laue, H., Schwier, M., Link, F., Peitgen, H.O.,
2011. Medical image analysis. IEEE Pulse 2, 60–70. doi:10.1109/MPUL.2011.
942929.

Rizvic, S., Boskovic, D., Okanovic, V., Kihic, I.I., Sljivo, S., 2019. Virtual reality
experience of Sarajevo war heritage, in: Rizvic, S., Rodriguez Echavarria, K. (Eds.),
Eurographics Workshop on Graphics and Cultural Heritage, EG. doi:10.2312/gch.
20191340.

Scholl, I., Suder, S., Schiﬀer, S., 2018. Direct volume rendering in virtual reality,
in: Maier, A., Deserno, T.M., Handels, H., Maier-Hein, K.H., Palm, C., Tolxdorﬀ,
T. (Eds.), Bildverarbeitung f¨ur die Medizin 2018, Springer, Berlin, Heidelberg. pp.
297–302. doi:10.1007/978-3-662-56537-7_79.

Schweigger-Seidel, F., 1862. Untersuchungen ¨uber die Milz. Arch. Pathol. Anat. Ph.

23, 526–570. doi:10.1007/BF01939038.

Shen, R., Boulanger, P., Noga, M., 2008. MedVis: A real-time immersive visualization
environment for the exploration of medical volumetric data, in: Information Visualiz-
ation in Medical and Biomedical Informatics, pp. 63–68. doi:10.1109/MediVis.
2008.10.

32

Shimabukuro, M.H., Minghim, R., 1998. Visualisation and reconstruction in dentistry,

in: Information Visualization, pp. 25–31. doi:10.1109/IV.1998.694195.

Silva, S., Santos, B.S., Madeira, J., Silva, A., 2009. Processing, visualization and
analysis of medical images of the heart: An example of fast prototyping using
MeVisLab, in: 2009 2nd International Conference in Visualisation, pp. 165–170.
doi:10.1109/VIZ.2009.40.

Slater, M., Gonzalez-Liencres, C., Haggard, P., Vinkers, C., Gregory-Clarke, R., Jelley,
S., Watson, Z., Breen, G., Schwarz, R., Steptoe, W., Szostak, D., Halan, S., Fox, D.,
Silver, J., 2020. The ethics of realism in virtual and augmented reality. Front. virtual
real. 1, 1. doi:10.3389/frvir.2020.00001.

Stefani, C., Lacy-Hulbert, A., Skillman, T., 2018. ConfocalVR: Immersive visualization
for confocal microscopy. J. Mol. Biol. 430, 4028–4035. doi:10.1016/j.jmb.
2018.06.035.

Steiniger, B., Stachniss, V., Schwarzbach, H., Barth, P.J., 2007. Phenotypic diﬀerences
between red pulp capillary and sinusoidal endothelia help localizing the open splenic
circulation in humans. Histochem. Cell. Biol. 128, 391–398. doi:10.1007/
s00418-007-0320-8.

Steiniger, B.S., Pfeﬀer, H., Guthe, M., Lobachev, O., 2020. Exploring human splenic
red pulp vasculature in virtual reality. Details of sheathed capillaries and the open
capillary network. Histochem. Cell Biol. URL: https://rdcu.be/b8KgZ, doi:10.
1007/s00418-020-01924-3.

Steiniger, B.S., Stachniss, V., Wilhelmi, V., Seiler, A., Lampp, K., Neﬀ, A., Guthe,
M., Lobachev, O., 2016. Three-dimensional arrangement of human bone marrow
microvessels revealed by immunohistology in undecalciﬁed sections. PLOS ONE 11,
1–25. doi:10.1371/journal.pone.0168173.

Steiniger, B.S., Ulrich, C., Berthold, M., Guthe, M., Lobachev, O., 2018a. Capillary
networks and follicular marginal zones in human spleens. Three-dimensional models
based on immunostained serial sections. PLOS ONE 13, 1–21. doi:10.1371/
journal.pone.0191019.

Steiniger, B.S., Wilhelmi, V., Berthold, M., Guthe, M., Lobachev, O., 2018b. Locating
human splenic capillary sheaths in virtual reality. Sci. Rep. 8, 15720. doi:10.1038/
s41598-018-34105-3.

Stets, J.D., Sun, Y., Corning, W., Greenwald, S.W., 2017. Visualization and labeling of
point clouds in virtual reality, in: SIGGRAPH Asia 2017 Posters — SA ’17, ACM,
Bangkok, Thailand. pp. 1–2. doi:10.1145/3145690.3145729.

Stone, J.E., Kohlmeyer, A., Vandivort, K.L., Schulten, K., 2010. Immersive molecular
visualization and interactive modeling with commodity hardware, in: Bebis, G.,
Boyle, R., Parvin, B., Koracin, D., Chung, R., Hammound, R., Hussain, M., Kar-
Han, T., Crawﬁs, R., Thalmann, D., Kao, D., Avila, L. (Eds.), Advances in Visual
Computing, Springer. pp. 382–393. doi:10.1007/978-3-642-17274-8_38.

33

Sutherland, I.E., 1965. The ultimate display, in: Proceedings of IFIP Congress,, pp.

506–508.

Sutherland, I.E., 1968. A head-mounted three dimensional display, in: Proceedings of
the December 9-11, 1968, fall joint computer conference, part I, ACM. pp. 757–764.
doi:10.1145/1476589.1476686.

Sutherland, I.E., 1970. Computer displays. Sci. Am. 222, 56–81. URL: https:

//www.jstor.org/stable/24925827.

Tang, L., Tian, C., Li, L., Hu, B., Yu, W., Xu, K., 2020. Perceptual quality assessment
for multimodal medical image fusion. Signal Process.-Image 85, 115852. doi:10.
1016/j.image.2020.115852.

Tomikawa, M., Hong, J., Shiotani, S., Tokunaga, E., Konishi, K., Ieiri, S., Tanoue, K.,
Akahoshi, T., Maehara, Y., Hashizume, M., 2010. Real-time 3-dimensional virtual
reality navigation system with open MRI for breast-conserving surgery. J. Am. Coll.
Surgeons 210, 927–933. doi:10.1016/j.jamcollsurg.2010.01.032.

Ulrich, C., Grund, N., Derzapf, E., Lobachev, O., Guthe, M., 2014. Parallel iso-
surface extraction and simpliﬁcation, in: Skala, V. (Ed.), WSCG Communications
proceedings. URL: http://hdl.handle.net/11025/26435.

Uppot, R.N., Laguna, B., McCarthy, C.J., De Novi, G., Phelps, A., Siegel, E., Courtier,
J., 2019. Implementing virtual and augmented reality tools for radiology education
and training, communication, and clinical care. Radiology 291, 570–580. doi:10.
1148/radiol.2019182210.

Uruthiralingam, U., Rea, P.M., 2020. Augmented and virtual reality in anatomical
education – a systematic review, in: Rea, P.M. (Ed.), Biomedical Visualisation.
Springer, Cham. volume 1235 of Advances in Experimental Medicine and Biology,
pp. 89–101. doi:10.1007/978-3-030-37639-0_5.

Walsh, C.M., Sherlock, M.E., Ling, S.C., Carnahan, H., 2012. Virtual reality simulation
training for health professions trainees in gastrointestinal endoscopy. Cochrane Data-
base of Systematic Reviews URL: http://doi.wiley.com/10.1002/14651858.
CD008237.pub2, doi:10.1002/14651858.CD008237.pub2.

Wißmann, N., Miˇsiak, M., Fuhrmann, A., Latoschik, M.E., 2020. Accelerated stereo
rendering with hybrid reprojection-based rasterization and adaptive ray-tracing, in:
2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 828–835.
doi:10.1109/VR46266.2020.00107. iSSN: 2642-5254.

Xia, P., Lopes, A.M., Restivo, M.T., 2013. Virtual reality and haptics for dental surgery:
a personal review. Vis Comput 29, 433–447. doi:10.1007/s00371-012-0748-2.

Zoller, E.I., Faludi, B., Gerig, N., Jost, G.F., Cattin, P.C., Rauter, G., 2020. Force
quantiﬁcation and simulation of pedicle screw tract palpation using direct visuo-haptic
volume rendering. Int. J. Comput. Ass. Rad. doi:10.1007/s11548-020-02258-0.

34

Appendix A. Short glossary of medical terms

• Histology: the science studying biological tissues at microscopic level.

• Histological section: a thin slice of a tissue that can be inspected under a micro-
scope. The sectioning happens on a device called ‘microtome’. Sections should
be stained (coloured) for better results.

• Serial sections: a consecutive series of histological sections. After registration

the series forms volume data.

• Vasculature: a network of blood vessels.

• Capillary: the smallest type of blood vessels.

• Sinus: a capillary with large diameter and specialised wall structure in speciﬁc
organs, such as spleen or bone marrow. Depending on the organ, sinuses diﬀer in
wall structure and function. Sinuses only occur in bone marrow regions where
blood cells are generated (‘hematopoietic regions’). Sinuses are ubiquitous in
the spleen. Whether (venous) spleen sinuses are connected to capillaries of the
arterial side, is an open question.

• Follicle: generally, a round structure. In spleen and tonsils, for example, follicles
are important dynamic structures where speciﬁc white blood cells often appear.
This makes follicles important for understanding immune system functions.

• Spleen: one of the organs which essentially function as a blood ﬁlter. The spleen
has a unique vasculature, where blood ﬂow also occurs outside blood vessels. It
features some unique structures both with respect to the vasculature and to the
arrangement of two major types of migratory white blood cells called lymphocytes,
which either occupy follicles or T-cell zones.

• Bone marrow: the inside of bones not only contains fatty tissue, but also regions
responsible for generation of new blood cells. Bone marrow features a unique
vasculature with capillaries and sinuses.

• Staining: for better visual inspection, speciﬁc parts of the tissue sections can be

coloured.

• Immunohistology: speciﬁc detection of diﬀerent molecules in tissue sections
using antibody solutions. Binding of an antibody to a tissue component is ﬁnally
visualised by deposition of a coloured insoluble polymerisation product of a previ-
ously uncoloured soluble stain. The stainings can, for example, detect membrane
glycoproteins in the innermost cells of blood vessels, so-termed endothelial cells.
Membrane glycoproteins have been numbered in the order of their discovery
using the CD (cluster of diﬀerentiation) nomenclature.

• MRI, magnetic resonance imaging, and CT, computed tomography from a series
of X-ray images are non-invasive imaging techniques that revolutionised the dia-
gnostics. Unfortunately, the spatial resolution and selectivity of these techniques
are not enough for our goals.

35

• Anti-CD34 staining: primarily stains endothelial cells of arterial vessels and
capillaries in human spleen and bone marrow. Typically used colour is brown.
Some stem cells in bone marrow are also stained. It is also weakly present in
sinus endothelia in the proximity of follicles.

• Anti-CD141 staining: stains sinus endothelial cells in human bone marrow and in

human spleen. Typically used colour is brown.

• Anti-SMA staining: stains smooth muscle alpha-actin.

It is present, e.g., in
walls of larger blood vessels on the ‘input’ arterial side, the so-called ‘arterioles.’
Typically used staining colour is brown.

• Anti-CD271 staining: stains capillary sheath cells and additional ﬁbroblast-like
cells in human spleen. The sheaths are multi-cellular structures around the initial
segment of human splenic capillaries. Sheath cells obviously represent the sessile
ﬁbroblast-derived part of capillary sheaths. Typically used staining colour is blue
or red. Specialised ﬁbroblasts inside the follicles are more weakly stained with
this antibody.

• Anti-CD20 staining: stains B-lymphocytes. Typically used colour is red.

Appendix B. Supplementary Material

The supplementary video for this paper is available under https://zenodo.org/
record/4268535. It shows the details of our visualisations in dynamics. The video
also showcases various approaches towards visual analytics. All imagery in the video is
a real-time feed from VR headset.

The datasets analyzed in this study can be found in the Zenodo repositories https:
//zenodo.org/record/1039241, https://zenodo.org/record/1229434, https:
//zenodo.org/record/4059595.

36

