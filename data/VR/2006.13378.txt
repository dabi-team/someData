0
2
0
2

g
u
A
2

]

C
D
.
s
c
[

2
v
8
7
3
3
1
.
6
0
0
2
:
v
i
X
r
a

To Appear in IEEE/ACM International Symposium on Microarchitecture, MICRO 2020

A Benchmarking Framework for Interactive 3D
Applications in the Cloud

Tianyi Liu1, Sen He1, Sunzhou Huang1, Danny Tsang1, Lingjia Tang2, Jason Mars2, and Wei Wang1

1The University of Texas at San Antonio
1{tianyi.liu, sen.he, sunzhou.huang, danny.tsang, wei.wang}@utsa.edu
2University of Michigan
2{lingjia, profmars}@umich.edu

ABSTRACT
With the growing popularity of cloud gaming and cloud vir-
tual reality (VR), interactive 3D applications have become
a major type of workloads for the cloud. However, despite
their growing importance, there is limited public research
on how to design cloud systems to efﬁciently support these
applications, due to the lack of an open and reliable research
infrastructure, including benchmarks and performance anal-
ysis tools. The challenges of generating human-like inputs
under various system/application randomness and dissecting
the performance of complex graphics systems make it very
difﬁcult to design such an infrastructure. In this paper, we
present the design of a novel research infrastructure, Pictor,
for cloud 3D applications and systems. Pictor employs AI
to mimic human interactions with complex 3D applications.
It can also track the processing of user inputs to provide in-
depth performance measurements for the complex software
and hardware stack used for cloud 3D-graphics rendering.
With Pictor, we designed a benchmark suite with six interac-
tive 3D applications. Performance analyses were conducted
with these benchmarks to characterize 3D applications in the
cloud and reveal new performance bottlenecks. To demon-
strate the effectiveness of Pictor, we also implemented two
optimizations to address two performance bottlenecks discov-
ered in a state-of-the-art cloud 3D-graphics rendering system,
which improved the frame rate by 57.7% on average.

1.

INTRODUCTION

The rise of cloud gaming and cloud virtual-reality (VR)
has made interactive 3D applications a major type of work-
loads for cloud computing and data centers [6, 22, 35, 46, 50].
A main beneﬁt of rendering interactive 3D applications in the
cloud is that it may reduce the installation and operational
costs for the large-scale deployments of these 3D applica-
tions. Running these 3D applications in the cloud also allows
mobile clients with less powerful GPUs to enjoy better vi-
sual effects. Moreover, cloud 3D-graphics rendering may
also simplify the development and delivery of these 3D ap-
plications. For the rest of this paper, we refer to these 3D
interactive applications simply as 3D applications.

Most prior research on virtual desktop infrastructure (VDI)
or cloud gaming focused on network latency [2, 19, 39].

However, the network latency is considerably reduced to-
day and becomes viable for cloud 3D applications [71]. This
improved network, in turn, makes the design of cloud 3D-
graphics rendering systems crucial to the efﬁciency and per-
formance of cloud 3D applications. However, there is limited
public research on this system design, largely due to the
lack of an open and reliable research infrastructure, including
benchmarks and performance analysis tools. Prior attempts to
provide such research infrastructures [12, 48, 78] have limited
success due to the following challenges.

First, for reliable evaluation, the research infrastructure
must be able to mimic human interactions with 3D applica-
tions under randomly generated/placed objects and varying
network latency. That is, the inputs used for the benchmarks
should closely resemble real human inputs, so that the per-
formance results obtained with these human-like inputs are
similar to those obtained with real human inputs. Prior re-
search generated human-like inputs from recorded human
actions [69, 78]. However, this recording does not work for
3D VR applications and games, which have irregular and
randomly placed/generated objects in their frames. Addi-
tionally, variations in network latency may affect when a
particular object will be shown on the screen, further limiting
the usefulness of the recorded actions [78].

Second, to reliably measure performance, the research
infrastructure must be able to accurately measure the round
trip time/latency to respond to a user input [48], which in
turn, relies on the accurate association of a user input and its
response frame. However, this association is very difﬁcult
due to the need to track the handling of a user input and the
rending of its response frame across the network, across the
CPU and GPU, and across multiple software processes (i.e.,
tracking from the client to the server, and back to the client).
Third, to effectively identify performance bottlenecks, the
research framework must be able to measure the performance
of every stage involved in the handling of a user input and
the rendering of its response frame. The framework should
be able to properly measure the performance of all the com-
ponents involved, including those from the complex graphics
software stack and heterogeneous hardware devices. Addi-
tionally, the research framework must have low overhead to
ensure these measurements are reliable.

Fourth, the research infrastructure should be extensible

1

 
 
 
 
 
 
to easily include new 3D applications. 3D applications are
typically refreshed every one or two years and most of them
are proprietary. Therefore, the research infrastructure should
be constantly refreshed with new 3D benchmarks without
requiring to modify their source code.

In this paper, we present a novel benchmarking framework,
called Pictor, which overcomes the above challenges to allow
reliable and effective performance evaluation of 3D applica-
tions and cloud graphics rendering systems. Pictor has two
components: 1) an intelligent client framework that can gen-
erate human-like inputs to interact with 3D applications; and
2) a performance analysis framework that provides reliable
input tracking and performance measurements. Inspired by
autonomous driving, the intelligent client framework employs
computer vision (CV) and recurrent neural network (RNN)
to simulate human actions [37,66,68]. The performance anal-
ysis framework tracks inputs with tags and combines various
performance monitoring techniques to measure the process-
ing latency and resource usage of each hardware and software
component. Additionally, Pictor is carefully designed to have
low overhead and require no modiﬁcation to 3D applications.
With Pictor, we designed a benchmark suite with four com-
puter games and two VR applications. Through experimental
evaluation with these benchmarks, we show that Pictor can
indeed accurately mimic human action with an average er-
ror of 1.6%. We also conducted an extensive performance
analysis on a state-of-the-art cloud 3D-graphics rendering
system [61, 75] to characterize the 3D benchmarks and the
rendering system, analyze the impact of co-locating multi-
ple 3D applications, and study the overhead of rendering 3D
applications in containers. This performance analysis demon-
strated the beneﬁts of cloud graphics rendering and revealed
new performance bottlenecks. At last, to demonstrate that the
in-depth performance analysis allowed by Pictor can indeed
lead to performance improvements, we implemented two
optimizations which improved average frame-rate by 57.7%.

The contributions of this paper include:
1. A novel intelligent client framework that can faithfully
mimic human interactions with complex 3D applications
with randomly generated/placed objects and under varying
network latency.

2. A novel performance analysis framework that can ac-
curately track the processing of user inputs, and measure
the performance of each step involved in the processing of
user inputs in various software and (heterogeneous) hardware
components. This framework is also carefully designed to
have low overhead and require no application source code.

3. A comprehensive performance analysis of a state-of-the-
art cloud graphics rendering system, the 3D benchmarks and
containerization. This analysis also shows the beneﬁt of cloud
3D applications and reveal new optimization opportunities.

4. Two new optimizations for current cloud graphics ren-
dering system with signiﬁcant performance improvements,
which also demonstrate the effectiveness of Pictor.

The rest of this paper is organized as follows: Section 2 dis-
cusses a typical cloud graphics rendering system; Section 3
presents the design of Pictor; Section 4 evaluates the accuracy
and overhead of Pictor; Section 5 provides the performance
analysis on a current cloud graphics rendering system; Sec-
tion 6 presents two new optimizations; Section 7 discusses

related work and Section 8 concludes the paper.

2. CLOUD 3D RENDERING SYSTEM

Figure 1 illustrates the typical system architecture for cloud
graphics rendering. This architecture employs a server-client
model where the servers on the cloud execute 3D applica-
tions and serve most of their rendering requests. The client is
mainly responsible for displaying UI frames and capturing
user inputs. The client may also perform less-intensive graph-
ics rendering, depending on the system design. In this work,
we focus on Linux-based systems and open-source software
which are easy to modify and free to distribute.

The system in Figure 1 operates in the following steps.
When the client’s interactive device captures a user input
(e.g., a keystroke, mouse movement or head motion), it sends
the input through the network to a proxy on the cloud server
(step (cid:202)), which forwards the input to the application (step
(cid:203)). The proxy is usually a server application that handles
media communication protocols, such as a Virtual Network
Computing (VNC) server with Remote Frame Buffer (RFB)
protocol or a video streaming server with extended Real
Time Streaming Protocol (RTSP) [29, 61, 64]. After receiving
the input, the application starts frame rendering (step (cid:204)).
A 3D application may use a rendering engine that provides
functions for drawing complex objects (e.g., the “Application
1”), or it may directly call a 2D/3D library to draw objects
from scratch (e.g., “App 2”). The rendering engine, in turn,
invokes the 2D/3D library. On Linux, the 2D/3D library is
typically Mesa 3D Graphics Library, which implements the
APIs of OpenGL and Vulkan [30, 56, 65]. Examples of the
rendering engine include Unity, OSVR and OpenVR [13, 67,
72]. To ensure 2D/3D calls are indeed invoked on the server,
a graphics interposer library is employed [17]. The 2D/3D
library (and the GPU driver) then translates the drawing APIs
into GPU commands to perform the rendering on the GPU
(step (cid:205)). After the frame is rendered on the GPU, the graphics
interposer copies the frame from the GPU (step (cid:206)-(cid:207)) and
push the newly rendered frame to the server proxy (step (cid:208)).
The proxy then compresses and sends the frame over the
network to the client for display (step (cid:209)).

Moreover, as shown in Figure 1, multiple 3D applications
can execute simultaneously on the same machine and share
hardware components, such as CPU, memory, GPU and PCIe
buses. Each application is executed in a virtual machine (VM)
or a container with virtualized GPUs (vGPU) [24, 38, 51].

This system architecture has two implications for the bench-
marking of cloud graphics rendering systems. First, as the
behaviors of 3D applications are heavily inﬂuenced by user in-
puts, reliably benchmarking 3D applications requires generat-
ing human-like inputs. Second, cloud graphics rendering sys-
tem includes complex and heterogeneous software/hardware
components, which must be properly handled/measured when
analyzing performance. In the rest of this paper, we will de-
scribe the design of Pictor, which overcomes the challenges
mention in Section 1 and the above two issues.

3. THE DESIGN OF Pictor

Figure 2 shows the components of Pictor benchmarking
framework. A main component of Pictor is the intelligent

2

Figure 1: System architecture for cloud graphics rendering.

Figure 2: Overview of the Pictor benchmarking framework (surrounded by the dashed box).

client framework that is used to generate clients with human-
like actions to interact with 3D applications. The other main
component of Pictor is the performance analysis framework,
which spans over the client and the server to provide reliable
performance measurements. The rest of this section describes
these two components in detail.

3.1

Intelligent Client Framework Design
Overview The intelligent client framework allows building
an intelligent client for a 3D application by learning how to
properly interact with this application from recorded human
actions. More speciﬁcally, for a 3D application, an RNN
model is trained based on recorded human actions under a
scene of this application [66]. To improve the RNN model’s
accuracy, the objects in the frames are ﬁrst recognized using
CV with Convolutional Neural Network (CNN) [37].

Figure 3 gives an overview of a client obtained with the
intelligent client framework, which operates in the following
steps. After a compressed frame is sent over the network to
the intelligent client (step (cid:202)), it is ﬁrst decompressed (step (cid:203)).
The decompressed frame is then processed by a CNN model
to recognize its objects (step (cid:204)). The types and coordinates
of the recognized objects are then sent to an RNN model
to generate user inputs that mimic real human actions (step
(cid:205)). These inputs are eventually sent back to the client proxy,
which encodes these actions into network packages and sends
them to the benchmark (step (cid:206)). With CNN and RNN models,
the clients can properly interact with 3D applications with
random frames and under random networking/system latency.
By generating actions purely based on frames, the clients can
be built for 3D applications without knowing their internal
designs or modifying these applications.

Model Training Each 3D application/benchmark has its
own CNN/RNN models, which are trained from a recorded
session of human actions under an application scene. The
intelligent client framework provides tools to perform this
recording. Each recorded session includes a sequence of
frames and the corresponding human actions to each frame.
To train a CNN model, the objects in the frames need to

be manually labeled. The labeled frames are then fed into
a machine-learning (ML) package to train the CNN model.
The manual frame labeling is generally fast and takes about
4 hours for one 3D application in our experience, as only the
objects that can determine the user inputs need to be labeled.
An RNN model can also be trained using the recorded
session. The recorded frames are ﬁrst processed by its CNN
model to recognize the objects. After the recognition, the
recorded data are converted into a training data set where
the features are the objects in a frame and the labels are the
corresponding human actions. An RNN model can then be
trained to learn how to respond to the objects in a frame like
a real human. Note that, our goal is not to train an AI to com-
pete with human. Instead, we aim at training an RNN model
to mimic human actions under varying system latency and
frame randomness, so that the performance results obtained
with the RNN-generated actions are similar to those obtained
with real human users. Because a trained RNN model is
executed on the same scene where it is trained, the model is
likely to work well as long as it has low training loss.

Implementation We implemented the training and infer-
ence of the CNN and RNN models with Tensorﬂow [1]. The
actual CNN model used is MobileNets [28]. The actual RNN
model used is Long Short-Term Memory (LSTM) [27].

3.2 Performance Analysis Framework Design
Overview The performance analysis framework provides
performance measurements of 3D applications and cloud
graphics rendering systems. Performance measurements
include frame rate (FPS, frames-per-second), the latencies
of each stage involved in the handling of a user input and
the delivery its response frame, as well as system-level and
architecture-level resource usages. As stated in Section 1,
designing this framework has two difﬁculties. The ﬁrst dif-
ﬁculty is to accurately track and associate the processing of
an user input and the rendering of its response frame. The
second difﬁculty is to measure the performance of the com-
plex and heterogeneous software/hardware components. This
section describes how Pictor overcomes these difﬁculties.

3

Figure 3: Overview of the intelligent client. The image is obtained from a racing game, SuperTuxKart [26].

Figure 4: Using API hooks to track the processing of a user input and the rendering of its reponse frame.

Intercepted APIs

Hooks
Hook4 XNextEvent, glutKeyboardFunc
glxSwapBuffer, glutSwapBuffers
Hook5
glReadBuffer, glReadPixel
Hook6
Hook7 XShmPutImage, glMapBuffer

Table 1: Some of the APIs intercepted at the API hooks.

Tracking User Input Processing. To track the input pro-
cessing, we tag the input from the client and use the tag to
identify every stage of the input processing. More speciﬁ-
cally, at the beginning of each processing stage, the tag of the
corresponding user input is extracted from the input data. At
the end of the stage, the tag is added to the output data, allow-
ing the next stage to extract it. For graphics rendering, the
begin and end of each stage can be determined based on the
invocation of speciﬁc OpenGL and X-Window APIs, and the
tags can be passed along as input/output data to these APIs.
The invocations to these APIs can be intercepted with API
hooks, allowing extracting/adding the tags in these hooks.

Figure 4 illustrates the API-hook-based input-tracking tech-
nique. For now, we assume a sequential graphics rendering
process. To track an input, hook1 at the client proxy gives ev-
ery input a unique tag and sends the tag with the input to the
server proxy. Upon receiving the input, hook2 at the server
proxy extracts the tag from the network package. The tag
is then forwarded to the application with its input by hook3.
When the application receives the input, the tag is extracted
at hook4 and saved. Hook5 marks the start of the GPU ren-
dering, there is no need to send the tag to GPU. At hook6, the
saved tag is embedded into the pixels of the rendered frame
(the old pixels are stored in shared memory). Embedding the
tag in pixels ensures that the tag survives the inter-process
communications between the application and server proxy.
After the server proxy receives the tagged frame at hook8,
it extracts the tag, restores the modify pixels and sends the
frame with the tag to the client. Once hook10 at the client
proxy receives the tagged frame, it matches the tag with a
previously sent user input, which ﬁnishes the tracking. Ta-
ble 1 gives some examples of the APIs that can be intercepted
from hook4 to hook7. The other hooks in the server and client

proxies can be easily identiﬁed using their source code.

However, instead of the above sequential rendering pro-
cess, modern graphics applications typical employ software
pipelines to parallelize the rendering for better performance.
Figure 5 shows the typical stages of this pipeline for remote
3D-graphics rendering when rendering two frames, framei
and framei+1. As Figure 5 shows, in each pass of the pipeline,
a new frame is rendered, and the previous frame is copied and
sent to the clients. For example, in the ﬁrst row of Figure 5,
framei is rendered based on inputi, while framei−1 is copied
from the GPU and sent to the client.

Note that, Figure 5 shows the pipeline for the cloud render-
ing system analyzed in Section 5. In this system, the stages
of application-logic (AL) and frame-copy (FC) are carried
out by the same thread due to the difﬁculty to know when
a frame is completely rendered in the GPU. Therefore, AL
and FC stages cannot overlap, and the next AL stage must
start after the previous FC stage is ﬁnished. Nonetheless, any
other two stages in this pipeline can overlap, as they are not
carried out by the same thread/processor.

The main beneﬁt of this software pipelining is that it allows
the CPU and GPU to execute simultaneously. For example,
as shown in Figure 5, when framei is being rendered on the
GPU (stage RDi), the CPU is working on the application
logic for framei+1 (stage ALi+1) and sending framei−1 (stage
ASi−1) using two threads/cores. The tag-based input tracking
still works for this parallel rendering, as long as the tracking
implementation is aware that the processing/rendering of an
input spans over two passes of the pipeline.

Performance Measurements for Diverse Components.
The API hooks also allow measuring the execution times
(latencies) of each stage involved in the rendering. A hook
records a timestamp when it intercepts an API call. The
differences between the timestamps of two hooks then give
the time spent in each stage. For example, the time difference
between the hook10 and hook1 with matching tag gives the
round-trip time (RTT) to handle a user input.

However, the time measured with the hook’s CPU times-
tamps cannot give GPU processing time. To obtain GPU

4

Figure 5: A typical software pipeline for open-source remote 3D-graphics rendering on Linux.

Application Area
Game: Racing
Game: Real-time Strategy
Game: First-person Shoot
Game: Online Battle Arena DoTA2 (D2) [76]
InMind (IM) [49]
VR: Education/Game
IMHOTEP (ITP) [57]
VR: Heatlh

Benchmark
SuperTuxKart (STK) [26]
0 A.D. (0AD) [21]
Red Eclipse (RE) [59]

Table 2: Applications included in our benchmark suite.

time, we use the time-querying functionality of OpenGL [23].
Start and stop querying statements are inserted into the hooks
to measure the GPU time spent in each stage. For example,
the time query starts at a hook5 and ends at the subsequently-
invoked hook6 gives the GPU time to render a frame.

Pictor also measures FPS and resource usages. The FPS
is obtained by counting the frames at the server and client
proxies. System-level resource usages, such as CPU/GPU
and memory utilizations, are obtained from the OS and GPU
drivers [8, 52]. Architecture-level resource utilization is mea-
sured using hardware performance monitoring units (PMU).
CPU PMU readings for each stage are obtained by using
PAPI inside the API hooks [73]. The PMUs on AMD GPUs
are queried using AMD’s GPU Performance API [7]. For
NVidia GPU, an external tool, NSight Graphics, is used to
read PMUs, as NVidia does not support programmable PMUs
reading for graphics rendering on Linux.

Performance Measurement Extensibility and Overhead.

One beneﬁt of using API hooks is that it does not require
modifying 3D applications. Our performance analysis frame-
work can be applied to any proprietary 3D applications, as
long as these applications invoke standard 3D APIs, such
as those given in Table 1. As later shown in the experimen-
tal evaluation (Section 4), these API hooks also incur little
overhead. However, the time queries used to measure the
GPU performance may stall the CPU and thus incur a high
overhead. To mitigate the impact of these stalls, we used two
query buffers and switched them between frames.

3.3 The Benchmark Suite

With Pictor, we designed a benchmark suite, which con-
tains four computer games and two VR applications. All
benchmarks are from real applications and cover popular
game genres and usage cases. Table 2 lists these benchmarks.
Among the six benchmarks, Dota2 and InMind are closed-
source. Note that, as Pictor is designed to be extensible, new
3D applications can be easily added in the future.

4. EVALUATION

This section provides the experimental evaluation of the

Figure 6: The performance (RTT) distributions obtained
with human users (H), Pictor’s intelligent clients (IC),
DeskBench [78] (DB), Chen et al. [15] (CH), and Slow-
Motion [48] (SM).

Pictor
DB
CH
SM

RE
2.5%

0AD
0.1%

Avg
ITP
D2
STK
2.0%
1.6%
0.8%
3.2%
5.4% 42.9% 14.6% 3.3%
2.3% 11.6%
39.5% 38.9% 32.6% 29.9% 11.4% 27.8% 30.0%
39.8% 30.1% 28.2% 32.7% 13.7% 22.7% 27.9%

IM
1.3%
1.3%

Table 3: Percentage errors for the means of the RTTs
obtained with Pictor’s IC, DeskBench [78] (DB), Chen
et al. [15] (CH), and Slow-Motion [48] (SM), when com-
pared to the mean RTTs obtained with human users.

reliability/accuracy and overhead of Pictor.

Experiment Setup The benchmarks were executed on a
server with an 8-core Intel i7-7820x CPU, 16GB memory
and an NVIDIA GTX1080Ti GPU with 11GB GPU memory.
The clients consisted of four machines each with a 4-core
Intel i5-7400 CPU and 8GB memory. The server and clients
were connected using 1Gbps networks. 1Gbps network was
chosen because it behaved similarly to 5G cellular network
in terms of the frame-transmitting latency as shown later
in Section 5.1.2. Precision Time Protocol [20] was used to
synchronize the time between the server and clients.

The server and clients run Ubuntu 16.04 as the OS and
TurboVNC 2.1.90 [75] as the rendering system. We chose
VNC as it has complete support for 3D rendering. The other
open-source solution, GamingAnywhere [29], failed to run
all ofour benchmarks. To the best of our knowledge, all
VNC implementations (and even the non-VNC proprietary

Figure 7: Computer vision and input generation time.

5

NX technology [58]) required TurboVNC’s graphics inter-
poser, VirtualGL [17], to support 3D rendering. Therefore,
we evaluated TurboVNC, as it represents the state-of-the-art
open-source remote 3D rendering. We modiﬁed TurboVNC
to support VR device inputs. All benchmarks were executed
at a resolution of 1920×1080 with maximized visual effects.
Intelligent Client Accuracy Evaluation. To evaluate if
the intelligent clients (ICs) indeed allow reliable and accurate
performance results, we compared the benchmarks’ behav-
iors under the ICs and human interactions. More speciﬁ-
cally, each benchmark was executed using its IC and was
also played/used by a real human user for three 15-minute
sessions each (results were stable after 10 min). We then com-
pared the performance results obtained from the two types
of executions, including the latency, FPS, and CPU/GPU
utilization. Figure 6 shows the round-trip time (RTT) that
it took to process input for each benchmark when executed
with the IC and the human user. For each execution, Figure 6
shows the mean, 1%-tile, 25%-tile, 75%-tile and 99%-tile of
the measured RTTs. As Figure 6 shows, the RTTs obtained
with IC were very similar to those from the human. Table 3
also gives the percentage errors of the means of the RTTs
obtained with our IC. The maximum percentage error for
the mean-RTT for IC is only 2.5%, and average error for IC
is only 1.6%, the RTTs from the IC and human runs were
also similar. The data for other performance metrics were
also similar for both runs. However, limited by space, other
performance metric results are omitted.

Intelligent Client Speed Evaluation. Figure 7 gives the
average times that it took to conduct CV (CNN) and generate
input (RNN) for each benchmark. As the ﬁgure shows, the
clients have fast inference times, with an overall average of
72.7ms for CV and 1.9ms for input generations. This fast
inference allows the ICs to generate 804 actions per minute
(APM) on average, which is faster than professional game
players (about 300 APM) [45], showing that the ICs can
generate inputs fast enough to mimic human reaction speed.
Pictor Overhead Evaluation. To evaluate the overhead
of Pictor, we executed each benchmark with and without the
performance analysis framework. For the run without the
performance analysis framework, native TurboVNC is used
with our ICs. As the native TurboVNC does not provide RTT
readings, we compared the FPS of both runs. Our results show
the performance analysis framework has low overhead. The
FPS reduction was only 2.7% on average (5% at maximum)
for all benchmarks. This low overhead is partially due to our
use of double-buffers when querying GPU execution times.
Without these double-buffers, the overhead was up to 10%.
Comparison with Prior Work. To show the importance
of properly handling irregular/random objects and tracking
inputs, we also compared Pictor with three prior performance
measuring techniques for VDI and cloud gaming.

We ﬁrst compared Pictor with DeskBench [60]. DeskBench
was based on VNCPlay [78] and replayed recorded human
actions to generate inputs. However, DeskBench did not
only record an action, it also recorded the screen frame when
this action was issued. During replay, the action was only
issued when the displayed frame was similar to the recorded
frame. With this frame comparison, DeskBench (and VNC-
Paly) only issued an action when the expected object was

displayed, and thus, was capable to handle network latency
variation. Note that, the “similarity” between frames was a
tune-able parameter for DeskBench. We tested with several
parameter values following the methodology presented by
DeskBench and reported the DeskBench’s results using the
best parameter we found. Additionally, as DeskBench did
not provide input tracking, it was only used to generate in-
puts, and Pictor’s performance framework was used to collect
performance data. Figure 6 and Table 3 also give the RTT
distributions and errors obtained with DeskBench. The av-
erage error of the mean-RTT obtained with DeskBench was
11.64%, which was considerably higher than the 1.6% error
of Pictor. DeskBench was designed for 2D applications with
well-shaped and placed objects (e.g., icons and texts), where
simply comparing pixels can determine if an object is shown
or not. However, for 3D games, even the same object can
have different pixels and locations depending on the viewing
angle and the ﬂow of events. Hence, simply comparing the
pixels is practically impossible to determine the existence of
an object, causing DeskBench to frequently delay an action.
We also compared Pictor with a cloud gaming performance
analysis methodology presented by Chen et al. [15]. In this
methodology, the authors generated inputs with human play-
ers. This methodology did not provide input tracking, and
hence, could not measure RTT at the client. Therefore, it had
to compute the RTT by summing the time of the stages of CS,
SP, AL, CP, and SS of the software pipeline. Figure 6 and
Table 3 also give the RTT distributions and errors obtained
with this methodology. The average error of the mean-RTT
obtained with this methodology was 30.0%, which was also
much higher than the 1.6% error of Pictor. There are two
issues with this methodology because of the lack of input
tracking. First, the AL latency in this methodology was ob-
tained ofﬂine without the VNC server proxy. This ofﬂine
measurement gave lower AL latency than that obtained dur-
ing online execution, because it eliminated the resource con-
tention between the game and the VNC server proxy. Second,
with input tracking, the methodology could not measure the
delays of the inter-process communication stages, includ-
ing PS, FC, and AS. Because of these two issues, Chen et
al.’s methodology usually reported smaller RTTs than those
directly measured at the client.

The last comparison was conducted with a VDI perfor-
mance measuring technique call Slow-Motion [12, 48]. Slow-
Motion was designed to determine the RTT of one frame.
Slow-Motion injected delays into the cloud rendering system
to only allow one input/frame being processed at a time –
only after an input was processed, its frame was rendered and
sent to the client, could the processing of the next input/frame
start. By allowing only one frame at a time, it was trivial
to associate an input with its response frame. Note that, as
Slow-Motion did not include an input generation technique,
Pictor’s IC was used to generate the inputs. Figure 6 also
shows the RTT distributions obtained with Slow-Motion. The
average error of the mean-RTT obtained with this methodol-
ogy was 27.9%, which was also higher than the 1.6% error of
Pictor. The main issue of Slow-Motion was that the injected
delay changed the resource usage and the behavior of the
benchmark and the VNC server proxy (which was also noted
by the original authors [48]). Because only one frame was

6

Figure 8: CPU and GPU utilization for each benchmark.

Figure 10: Server and client FPS when executing one to
four instances of the same benchmark on the server.

Figure 9: Network and PCIe (send-to and received-at the
GPU) bandwidth usages for each benchmark.

rendered at a time, the resource contention caused by parallel
processing/rendering of the inputs and frames was eliminated,
and the resource contention between the benchmark and the
server proxy was also reduced. Consequently, Slow-Motion
typically reported smaller RTTs than those observed with a
system executing at full capacity.
5. PERFORMANCE ANALYSIS OF CLOUD
RENDERING SYSTEM AND 3D APPLI-
CATIONS

5.1 Perf. Analysis with A Single Benchmark

This section provides the performance analysis results with
a single benchmark, which was executed using the same
methodology given in Section 4.

5.1.1

System-level Resource Utilization

Figure 8 gives the CPU and GPU utilization of each bench-
mark. The CPU utilization of these benchmarks had a high
variation, ranging from 68% (RedEclipse) to 266% (Dota2).
The GPU utilization also had a high variation, ranging from
22% to 53%. The VNC server also had considerable CPU
utilization, which varied from 169% to 243%, depending
on the FPS and frame compression difﬁculty. The CPU
memory usages also vary considerably, ranging from 600MB
(Dota2) to nearly 4GB (InMind). The GPU memory usages
of these benchmarks were less than 800MB, which is sim-
ilar to the 1GB-2GB GPU memory requirements of recent
popular games.

Figure 9 shows the network and PCIe bandwidth usages
for each benchmark. For network usage, only the bandwidth
usage of sending the frames to the client is shown, as sending
the inputs from the clients used only 1.5Mpbs. The network
usages of these benchmarks were below 600Mpbs, which is
lower than the maximum bandwidth of the coming 5G cellular
network and 10Gpbs broadband. Similarly, all benchmarks
used less than 5GB/s on the PCIe bus, which is well below
the 31.5GB/s maximum bandwidth of PCIe3. Except for
SuperTuxKart, all benchmarks sent limited amount of data
from the CPU to GPU, suggesting most of their rendering data
were stored on the GPU. The exception of SuperTuxKart was
likely due to its frequent and drastic changes in the rendered

7

Figure 11: RTT breakdown when executing one to four
instances of the same benchmark on the server.

frames. For all benchmarks, there is high PCIe bandwidth
usage from GPU to CPU, which represented the data used for
copying rendered frames from GPU to CPU.

5.1.2 Application Performance

Figure 10 gives the server and client FPS for each bench-
mark when one to four instances of the same benchmark was
executed on the server. Here, we focus on the FPS for one
instance (i.e., the bars with x-axis label “1”). Server FPS mea-
sured the number of frames that were generated at the server
in one second. Client FPS measured the number of frames
the client received in one second. The lowest client FPS was
27 (for 0AD), which is still higher than the minimum 25 FPS
quality-of-service (QoS) requirement for 3D applications,
showing the feasibility of cloud graphics rendering [62].

Figure 11 gives the average RTTs of handling an input
for each benchmark. Again, we focus on the RTTs for one
instance (i.e., the bars with x-axis label “1”). These RTTs
are broken down into the time the sever spent on handling
the input and the network times for sending the inputs and
frames. For all benchmarks, the network latency for sending
inputs (stage CS) was very small (< 10ms). The network
latency for sending frames (stage SS) ranged from 14ms to
35ms, which was similar to those reported by prior work
with 4G/5G cellular network, suggesting our 1Gpbs network
is close to the real use case [71]. The largest component
of RTT was always the time that the server took to process
inputs, which include all stages from SP to CP. This server
processing time ranged from 61ms to 106ms. Such high
server time indicates that cloud system design is crucial to
ensuring good performance.

In Figure 12, the server time is further broken down into
the time of VNC sending inputs to the benchmark (stage
PS), the application execution time (stage AL, FC, and RD),
benchmark sending frame to VNC (stage AS) and the time
of VNC compressing frames (stage CP). Note that, the time
for stage SP is omitted because it was too small (< 1m) to be
visible in the ﬁgure. As Figure 12 shows, for all benchmarks,
the main component of the server processing time is the
application execution time, where the execution times for

Figure 12: Server time breakdown when executing one
to four instances of the same benchmark on the server.

Figure 15: L3 cache miss rates when executing one to
four instances of a benchmark on the same server.

Figure 13: Application time breakdown when executing
one to four instances of a benchmark on a server.

other stages (i.e, PS, AS and CP) were less than 18ms.

The application execution time was further broken down in
Figure 13. As GPU rendering (RD) executes in parallel with
the application logic (AL) and frame copy (FC) stages, the
GPU rendering times are shown as separate bars in Figure 13.
Surprisingly, many benchmarks spent most of their time on
copying frames. This long frame-copy time was due to the
long PCIe transporting time and inefﬁciency implementation,
which were addressed with new optimizations in Section 6.
Moreover, because of the long frame-copy, GPU rendering
was never the performance bottleneck in our experiments.

5.1.3 Architecture-level Resource Usages

Figure 14 shows the CPU cycles for each benchmark us-
ing the Top-Down analysis [33]. The CPU cycles are bro-
ken down into front-end stalls, back-end stalls, bad specu-
lation stalls, and the cycles for instruction retirements. As
Figure 14 shows, all benchmarks had long back-end stalls
and low instructions-per-cycle, indicating these benchmarks
were likely memory-bound. As their L3 cache miss rates
(L3-misses/L3-accesses) were also very high (> 70%) ( Fig-
ure 15), it can be deduced that these benchmarks are also
off-chip memory bound. This behavior is consistent with
typical graphics rendering implementation, where uncached
memory is used for CPU-to-GPU communications [32].

As shown in Figure 16, all benchmarks, except InMind,
had moderate GPU cache miss rates. These moderate cache
miss rates suggested most benchmarks can use GPU caches

Figure 16: GPU L2 and texture cache miss rates when
executing one to four instances of a benchmark.

relatively effectively. Note that, 0AD used OpenGL v1.3,
which is not supported by NVidia PMU reading tools. There-
fore, we could not obtain GPU cache miss rates for 0AD.

5.1.4

Single Benchmark Analysis Summary

1) Executing 3D applications in the cloud can provide
reasonable QoS with current hardware and network. 2)
Cloud/server performance can be a major limitation on FPS
and RTT. Therefore, optimizing the cloud system design is
crucial for cloud graphics rendering. 3) 3D applications have
a wide range of resource demands and behaviors, suggesting
cloud system optimizations may need to consider individual
application’s characteristics. 4) 3D applications intensively
utilized the CPU, GPU, memory and PCIe buses. Conse-
quently, cloud system optimizations need to consider the
impacts of all these resources. For instance, we designed
an optimization to handle the long frame-copy time over the
PCIe bus in Section 6.

5.2 Perf. Analysis with Multiple Benchmarks
To investigate the feasibility and analyze the performance
of multiple 3D applications sharing hardware in the cloud,
we also conducted experiments with multiple 3D benchmarks.
More speciﬁcally, we executed one to four instances of the
same benchmark on our server. Each benchmark instance
interacted with its own client machine. To ensure enough
network bandwidth, each benchmark instance used its own
1Gpbs network card on the server.

5.2.1

Server Power Consumption

We obtained the server power consumption using a Klein
Tools CL110 meter. Overall, adding a new instance only in-
creased the total server power consumption by less than 20%.
As shown in Figure 17, This small increase in total power
usage translated into per-instance power usage reductions of

Figure 14: CPU cycles breakdown when executing one to
four instances of a benchmark on the same server.

Figure 17: Per-instance power usage when executing one
to four benchmark instances on one machine.

8

33%, 50%, and 61%, when running two to four instances
(comparing to one instance). These power usage reductions
demonstrate a main beneﬁt of executing 3D applications in
the cloud – the reduced energy cost and operational cost.

5.2.2 Application Performance

Figure 10 also shows the FPS for each benchmark, when
two to four instances of the same benchmark were executed
on the same server. Here, we focus on the FPS for 2 to
4 instances (i.e., the bars with x-axis labels “2”, “3” and
“4”). As Figure 10 shows, for all benchmarks, executing with
two instances could still provide an acceptable (i.e., ≥ 25)
FPS. For three benchmarks, RE, IM, and ITP, executing three
instances could still achieve an FPS higher than 25. These
FPS results show that consolidating multiple 3D applications
on one server can still provide acceptable QoS, and thus
reduce infrastructure cost.

Figure 11, 12 and 13 also give the breakdown of the RTT,
server processing time and benchmark processing time, when
two to four instances of the same benchmark were executed
on the same server. Again, we focus on the results for 2
to 4 instances (i.e., the bars with x-axis labels “2”, “3” and
“4”). As Figure 11 shows, there was no signiﬁcant increase in
network time due to the use of multiple graphics.

However, there were signiﬁcant increases in server exe-
cution times. As shown in Figure 12 and Figure 13, nearly
every execution stage on the server experienced increased
execution time with more instances. We have observed high
increase (up to 96%) in execution time for the stages with
inter-process communications (IPC), including the stages
of PS and AS. There were also signiﬁcantly increase in the
stages that do computations on the CPU and GPU, including
the stages of AL, FC, RD, and CP. In particular, the average
application logic (stage AL) time increased by 235% when
executing with four instances, and the average GPU render-
ing (stage RD) time increased by 133% when executing with
four instances. These increased execution time on CPU and
GPU were main caused by two issues – the oversubscribed
CPU/GPU when executing three or four instances, and the
hardware resource contention that happened in the CPU, GPU
and PCIe buses. This contention is discussed in detail in the
following section.

5.2.3 Architecture-level Resource Usages

Figure 14 and Figure 15 gives the CPU cycle breakdown
and the L3 cache miss rates of one benchmark instance, when
it executed with other benchmark instances. As the ﬁgures
show, both the back-end stalls and the L3 miss rates increased
considerably with more benchmark instances, indicating that
there was heavy contention in the memory system.

Memory contention was also observed in the GPU. As
shown in Figure 16, all benchmarks experienced increased
GPU L2 miss rates, which contributed to the increase in
their GPU rendering time. This L2 miss increases may be
explained with the GPU internal graphics pipelines [43]. Be-
cause of this pipeline, there may be frames from different
benchmark instances rendered simultaneously, thus causing
the L2 cache contention. The texture cache miss rates, how-
ever, did not change signiﬁcantly, as it is a private cache.
Note that, for both CPU and GPU, the contention may exist

Figure 18: Client FPS for 15 pairs of benchmarks.

Figure 19: Performance loss and cache miss increases of
Dota2 when executing with other benchmarks. Higher
values indicate higher loss and contention.
beyond cache and extend to DRAM and PCIe buses. Re-
source contention also existed between the benchmarks and
VNC proxies. However, a full contention analysis is beyond
the scope of this paper and will be conducted in the future.

5.2.4 Multi-benchmark Analysis Summary

1) Executing multiple 3D applications on the same server
in the cloud can provide acceptable QoS while signiﬁcantly
reduced energy consumption. Therefore, cloud graphics ren-
dering may considerably reduce the infrastructure and opera-
tional costs for the large-scale deployment of 3D applications.
2) Resource contention and slowed IPC can severely degrade
the performance of 3D applications in the cloud, and thus
should be properly managed. Moreover, resource contention
simultaneously exists in the CPU and GPU (and potentially in
the PCIe buses). Contention also exists between the applica-
tions and the server proxies. Therefore, resource contention
and IPC management for cloud graphics rendering should be
designed with heterogeneity in mind.

5.3 Perf. Analysis with Mixed Benchmarks

To study the impact of colocating different 3D applica-
tions, we also conducted experiments where two different
benchmarks were executed simultaneously. As there were 6
benchmarks, a total of 15 pairs of them were evaluated.

5.3.1 Application Performance, Power Consumption
and Architectural-level Resource Usages
Figure 18 gives the client FPS of the 15 pairs of bench-
marks. Server FPS was just slightly higher than client FPS
and was omitted due to space limitation. As Figure 18 shows,
11 pairs of benchmarks had client FPS higher than 25, sug-
gesting different 3D applications can also share hardware
while ensuring acceptable QoS. We also observed that adding
an additional benchmark only increase the total server energy
consumption by no more than 25%. Therefore, comparing to
running two applications on two servers, executing two dif-
ferent 3D applications on the same server can reduce energy
consumption by at least 37%.

Similar to the observations in Section 5.2, oversubscribed

9

Figure 20: Server FPS/RTT overheads of containers.
Negative overheads are performance improvements.

CPU/GPU, prolonged IPC, and resource contention signiﬁ-
cantly increased the server execution time. Nonetheless, we
also observe that the contentiousness of these benchmarks
varies considerably. Figure 19 gives the performance and
CPU/GPU cache misses of Dota2 when it was executed with
different benchmarks. Other benchmark pairs showed similar
results and were omitted due to space limitation. As Figure 19
shows, there was signiﬁcant variation in Dota2’s performance
depending on its co-runners, with SuperTuxKart causing the
highest contention and 0AD causing the least contention. This
high variation in the contentiousness may be utilized in op-
timizations (e.g., selecting the proper set of 3D applications
to share hardware). It is also interesting to observe that the
contentiousness for CPU cache and GPU cache seemed to
have high correlation. This correlation may be due to the
rendering data being shared between CPU and GPU, and may
be exploited when managing 3D applications contention (e.g.,
predicting a 3D application’s contentiousness).

5.3.2 Mixed Benchmark Analysis Summary

1) Executing multiple different 3D applications on the
same server in the cloud can provide acceptable QoS while
signiﬁcantly reducing energy consumption. 2) The con-
tentiousness of 3D applications varies considerably, which
may be utilized in system optimizations. 3) There may also
be a correlation between the contentiousness for the CPU
cache and GPU cache. which may also be exploited in sys-
tem optimizations.

5.4 Container Overhead

So far, all experiments were conducted on bare-metal sys-
tems. However, as we target cloud computing, it is also
important to analyze the performance impact of virtualization
and containerization. Hence, we repeated the above the per-
formance analysis experiments using Docker containers to
study the overhead of containerization. More speciﬁcally, we
executed each instance of the benchmark and its VNC server
inside an NVidia Docker container [51]. We chose container
instead of VM because docker container currently supports
most GPUs, while VM-based virtualization require special
GPUs. We will evaluate VM-based systems in the future.

Figure 20 shows the percentages of reduced FPS and in-
creased RTT (i.e., FPS and RTT overheads), comparing to the
runs without virtualization. On average, Docker containers
incurred little overhead. The average overhead for RTT was
only 1.3%, and the average overhead for server FPS was only
1.5%. This low average overhead further shows the feasibility
of executing 3D applications in the cloud.

Nonetheless, the overhead can still be as high as 8.5%
for RTT (or 6% for FPS). We observed that these overheads

10

Figure 21: Optimizing frame copy with two-step copy.

were usually due to increased execution time for the stages
with IPC (stages PS and AS). These high overheads show
the need to optimize containerized cloud 3D applications to
ensure that worst case performance still meets QoS goals.
Besides the overheads for RTT and FPS, the GPU rendering
time was also increased by 2.9% on average and 8% on
maximum, illustrating the overhead of GPU virtualization
with containers.

It is also worth noting that container also improved per-
formance in certain cases, as shown with the negative over-
heads in Figure 20. A preliminary analysis showed that these
performance improvements were mainly due to container-
ization reduced resource contention among the benchmarks
and VNC servers. Although further analysis is still required
to identify the exact cause of the reduced contention, these
performance improvements illustrate the potential beneﬁts of
container-based run-time optimizations.

Container Overhead Summary 1) On average, Docker
containers incur limited overhead, further showing the feasi-
bility of executing 3D applications in the cloud. 2) Nonethe-
less, high performance overhead may still be observed in
certain cases, suggesting that container overhead reduction
is still required. 3) Container overheads are mainly associ-
ated with IPC and GPU virtualization. 4) Containerization
may also improve performance, suggesting the potential of
additional run-time optimizations.

6. OPTIMIZED FRAME COPY

As discussed in Section 5.1.2, the frame-copy (FC) stage
was a major performance bottleneck in TurboVNC. This sec-
tion presents the optimizations we invented and implemented
to reduce the frame-copying time.

Further analysis of the TurboVNC’s graphics interposer re-
vealed two inefﬁciencies. First, the interposer called the func-
tion XGetWindowAttributes before copying a frame. XGetWin-
dowAttributes was extremely slow and consumed 6~9ms.
This function was only used to get the benchmark’s reso-
lution to determine the size of the frame to copy. As the
resolution of a game or VR application is rarely changed dur-
ing execution, there is no need to call XGetWindowAttributes
for every frame copy. Therefore, in the ﬁrst optimization,
we applied memoization to this function. That is, we inter-
cepted the invocation to XGetWindowAttributes and returned
the cached resolution instead of actually calling it. XGetWin-
dowAttributes is only actually invoked when the benchmark’s
resolution changes, which is determined by monitoring X
events at Hook4.

The second inefﬁciency is that the benchmarks were halted
during the copy, waiting from the GPU to send the frame, as
shown with the blank in the FC stage in Figure 5. Inspired
by deep CPU pipelining, we broke the frame copy into two
smaller steps – the start-copy and ﬁnish-copy. As shown in
Figure 21, after issuing the frame copying command to GPU

VNCPlay [78]

Chen et al. [15]

Slow-Motion [48]

Login-VSI [69]

DeskBench [60]

VDBench [12]

Dusi et al. [19]

Pictor

Features
Random UI Objects Tolerant
Varying Net Latency Tolerant
User-input Tracking
CPU Perf. Measurement
Network Perf. Measurement
GPU Perf. Measurement
PCIe frame-copy Perf. Measure.
Unaltered 3D App Behaviors

Table 4: Comparison between Pictor and prior work on VDI and cloud gaming performance analysis.

Figure 22: Improved FPS/RTT with our optimizations.

for framei−1 (FCStarti−1), the graphics interposer does not
wait for framei−1’s copy to ﬁnish. Instead, it goes on to ﬁnish
the copying of framei−2 (FCEndi−2) and works on sending
framei−2 to the VNC server. The actual ﬁnish of copying
framei−1 happens after the application logic for framei+1 is
computed (FCEndi−1). By making the frame copy into two
asynchronous steps, the halt in the benchmark is removed.

Figure 22 gives the performance improvements from our
two optimizations when one benchmark instance was exe-
cuted. Our optimization improved server FPS by 57.7% on
average and 115.2% at maximum. The client FPS was im-
proved by 7.4% on average and 19.5% at maximum. The RTT
was reduced by 8.5% on average and 15.1% at maximum.
Note that, in Figure 22, the client FPS of ITP had 3% reduc-
tion due to the increased benchmark performance causing
more contention with the VNC proxy. We were able to re-
move this extra contention and improve ITP’s client FPS with
an additional optimization. However, due to space limitation,
this additional optimization cannot be covered here.

7. RELATED WORK

VDI and Cloud Gaming System Benchmarking There
have been several studies providing benchmarking tools or
methodologies to analyze the performance of VDI systems
and cloud gaming systems. Table 4 compares the function-
alities and features of the major prior work with Pictor. All
of the studies summarized in Table 4, expect Chen et al. [15],
were designed to measure VDI systems with 2D applications.
Moreover, none of these studies considered the random and
irregular UI objects in cloud games and VR applications. And
none of these studies provided methods to measure the per-
formance of GPU rendering and frame-copy over the PCIe
connections. These studies also did not provided means to
track input processing without changing the 3D application’s
resource usage and behavior, as shown in Section 4. In sum-
mary, without the abilities to handle irregular and random
objects, associate an input and its response, and measure the
GPU and frame-copy performance, existing benchmarking
tools/methodologies cannot provide reliable and effective
performance measurements for cloud 3D rendering systems.
GPU Benchmarks. There are also many GPU benchmarks,
such as GraalBench [10], SPECviewperf [18], GFXBench [31]

Rodinia [14] and MGMark [70]. Mitra and Chiueh also an-
alyzed three 3D benchmark suites [47]. These benchmarks
and analysis focused on evaluating GPU performance with-
out user actions. However, as 3D applications’ behaviors
are heavily affected by user actions, user inputs must be
considered in cloud 3D benchmarks to ensure realistic re-
sults. Moreover, interactive 3D applications have intensive
usage for both CPU and GPU. Only focusing on GPU can-
not provide the insights needed to manage interactive 3D
applications and their use of heterogeneous hardware.
Graphics Rendering Software. Many software supports re-
mote desktops, such as VNC, NX and THINC [11, 58, 61, 63].
These remote desktops usually do not support 3D applica-
tions by default. Additionally, cloud graphics rendering also
requires additional support on GPU virtualization and man-
agement for co-running 3D applications. Consequently, addi-
tional research is required to efﬁciently support 3D applica-
tions in cloud. CloudVR and Furion were two programming
frameworks to support cloud VR [36, 40]. Abe et al. em-
ployed data prefetching to speed up the cloud processing time
for interactive applications [3]. Ha et al. investigated the im-
pact of consolidating multimedia and machine-learning appli-
cations in the cloud [25]. AppStreamer dynamically predicted
and downloaded useful portions of a game to mobile de-
vices [74]. Our research is inspired by these studies and aims
at facilitating these graphics system design studies. Google,
Microsoft, NVidia and PARSEC are also building their propri-
etary cloud gaming systems, whose designs may be different
than the system analyzed in this paper [22, 46, 50, 55]. Pic-
tor aims at facilitating the public research on cloud graphics
rendering, so that open-source solutions can be as good as
proprietary solutions. We will constantly update Pictor to
follow the advances in these open-source solutions.

Other Related Work GUI testing frameworks [4,5,16,42,
44] may also be used for benchmarking remotely-rendered
applications. However, these GUI testing frameworks were
not designed for 3D applications with irregularly-shaped and
random UI objects. Google DeepMind and OpenAI Five
have also built AI bots to play games [34, 53]. These bots
were built to compete with human. Therefore, their execu-
tion may require thousands of processors [54]. Additionally,
the AI models used by these bots required complex training
processes for new games, and existing model are not publicly
available. Therefore, these AI bots are not suitable for 3D
application benchmarking. Moreover, prior research on non-
cloud VR architecture and systems [9, 40, 41, 77] may also
beneﬁt from Pictor’s intelligent clients and benchmarks.

8. CONCLUSION

11

This paper presents Pictor, a benchmarking framework for
cloud 3D applications and systems. Pictor includes an intelli-
gent client to mimic human interactions with 3D applications
with 1.6% error, and a performance analysis framework that
provides detailed performance measurements for cloud graph-
ics rendering systems. With Pictor, we designed a benchmark
suite with six 3D benchmarks. Using these benchmarks, we
characterized a current cloud graphics rendering system and
cloud 3D applications, which also showed beneﬁts of cloud
graphics rendering. We also designed new optimizations with
Pictor to address two newly-found bottlenecks.

REFERENCES
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan,
P. Warden, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: A System
for Large-scale Machine Learning,” in Proceedings of the 12th
USENIX Conference on Operating Systems Design and
Implementation, 2016.

[2] O. Abari, D. Bharadia, A. Dufﬁeld, and D. Katabi, “Enabling

High-Quality Untethered Virtual Reality,” in USENIX Symp. on
Networked Systems Design and Implementation, 2017.

Apps with Minimal Restart and Approximate Learning,” in Proc. of
Int’l Conf. on Object Oriented Programming Systems Languages and
Applications, 2013.

[17] D. Commander, “VirtualGL: 3D without boundaries–the VirtualGL

project,” 2007.

[18] S. P. E. Corporation, “SPECviewperf 12,”

https://www.spec.org/gwpg/gpc.static/vp12info.html, [Online;
accessed 11-Nov-2018].

[19] M. Dusi, S. Napolitano, S. Niccolini, and S. Longo, “A Closer Look at
Thin-client Connections: Statistical Application Identiﬁcation for QoE
Detection,” IEEE Communications Magazine, vol. 50, no. 11, 2012.

[20] J. Eidson and K. Lee, “IEEE 1588 Standard for A Precision Clock
Synchronization Protocol for Networked Measurement and Control
Systems,” in ISA/IEEE Sensors for Industry Conference, 2002.

[21] W. Games, “0 A.D.” https://play0ad.com/, [Online; accessed

11-Nov-2018].

[22] Google, “Stadia,” https://stadia.dev/, [Online; accessed 10-Jul-2019].

[23] K. Group, “Query Object,”

https://www.khronos.org/opengl/wiki/Query_Object, [Online;
accessed 11-Jul-2019].

[24] V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar,
and P. Ranganathan, “GViM: GPU-Accelerated Virtual Machines,” in
Proceedings of the 3rd ACM Workshop on System-level Virtualization
for High Performance Computing. ACM, 2009, pp. 17–24.

[3] Y. Abe, R. Geambasu, K. Joshi, H. A. Lagar-Cavilla, and

M. Satyanarayanan, “vTube: Efﬁcient Streaming of Virtual
Appliances over Last-Mile Networks,” in Proc. of the Annual Symp. on
Cloud Computing, 2013.

[25] K. Ha, P. Pillai, G. Lewis, S. Simanta, S. Clinch, N. Davies, and

M. Satyanarayanan, “The Impact of Mobile Multimedia Applications
on Data Center Consolidation,” in IEEE Int’l Conf. on Cloud
Engineering, 2013.

[4] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, B. D. Ta, and A. M.

Memon, “MobiGUITAR: Automated Model-Based Testing of Mobile
Apps,” IEEE Software, vol. 32, no. 5, 2015.

[5] D. Amalﬁtano, A. R. Fasolino, P. Tramontana, S. De Carmine, and
A. M. Memon, “Using GUI Ripping for Automated Testing of
Android Applications,” in Proc. of Int’l Conf. on Automated Software
Engineering, 2012.

[6] Amazon, “Sumerian,” https://aws.amazon.com/sumerian/, [Online;

accessed 11-Nov-2018].

[7] AMD, “GPU Performance API for AMD GPUs,”

https://github.com/GPUOpen-Tools/GPA, [Online; accessed
11-Jul-2019].

[8] AMD, “ROCm System Management Interface (ROCm SMI) Library,”
https://github.com/RadeonOpenCompute/rocm_smi_lib, [Online;
accessed 11-Jul-2019].

[9] M. Anglada, E. de Lucas, J. Parcerisa, J. L. AragÃ¸sn, and

A. GonzÃ ˛alez, “Early Visibility Resolution for Removing Ineffectual
Computations in the Graphics Pipeline,” in IEEE Int’l Symp. on High
Performance Computer Architecture, 2019.

[10] I. Antochi, B. Juurlink, S. Vassiliadis, and P. Liuha, “GraalBench: A

3D Graphics Benchmark Suite for Mobile Phones,” in Proc. of the
Conf. on Languages, Compilers, and Tools for Embedded Systems,
2004.

[11] R. A. Baratto, L. N. Kim, and J. Nieh, “Thinc: A virtual display

architecture for thin-client computing,” in Proc. of ACM Symp. on
Operating Systems Principles, 2005.

[12] A. Berryman, P. Calyam, M. Honigford, and A. M. Lai, “VDBench: A
Benchmarking Toolkit for Thin-Client Based Virtual Desktop
Environments,” in IEEE Int’l Conf. on Cloud Computing Technology
and Science, 2010.

[13] Y. S. Boger, R. A. Pavlik, and R. M. Taylor, “OSVR: An Open-source

Virtual Reality Platform for Both Industry and Academia,” in IEEE
Virtual Reality (VR), 2015.

[14] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron, “Rodinia: A Benchmark Suite for Heterogeneous
Computing,” in Proc. of Int’l Symp. on Workload Characterization,
2009.

[15] K. Chen, Y. Chang, H. Hsu, D. Chen, C. Huang, and C. Hsu, “On the
Quality of Service of Cloud Gaming Systems,” IEEE Transactions on
Multimedia, vol. 16, no. 2, 2014.

[16] W. Choi, G. Necula, and K. Sen, “Guided GUI Testing of Android

[26] J. Henrichs, “SuperTuxKart,” https://supertuxkart.net/Main_Page,

[Online; accessed 11-Nov-2018].

[27] S. Hochreiter and J. Schmidhuber, “Long Short-term Memory,”

Neural Computation, vol. 9, no. 8, 1997.

[28] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,

T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient
Convolutional Neural Networks for Mobile Vision Applications,”
arXiv preprint arXiv:1704.04861, 2017.

[29] C.-Y. Huang, C.-H. Hsu, Y.-C. Chang, and K.-T. Chen,

“GamingAnywhere: An Open Cloud Gaming System,” in Proc. of
ACM Multimedia Systems Conference, 2013.

[30] T. K. G. Inc., “Vulkan 1.1.92 - A Speciﬁcation,”

https://www.khronos.org/registry/vulkan/specs/1.1/html/vkspec.html,
[Online; accessed 12-Nov-2018].

[31] K. Informatics, “GFXBench - Uniﬁed graphics benchmark based on
DXBenchmark (DirectX) and GLBenchmark (OpenGL ES),”
https://gfxbench.com/, [Online; accessed 22-July-2019].

[32] Intel, “Write Combining Memory Implementation Guidelines,” 1998.

[33] Intel, “Intel 64 and IA-32 Architectures Optimization Reference

Manual,” 2016.

[34] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman
et al., “Human-level performance in 3d multiplayer games with
population-based reinforcement learning,” Science, vol. 364, no. 6443,
pp. 859–865, 2019.

[35] G. R. James, Citrix XenDesktop Implementation: A Practical Guide

for IT Professionals. Elsevier, 2010.

[36] T. Kämäräinen, M. Siekkinen, J. Eerikäinen, and A. Ylä-Jääski,

“CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality,” in
Proceedings of the 26th ACM International Conference on Multimedia,
2018.

[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet

Classiﬁcation with Deep Convolutional Neural Networks,” in
Advances in Neural Information Processing Systems 25, F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran
Associates, Inc., 2012.

[38] H. A. Lagar-Cavilla, N. Tolia, M. Satyanarayanan, and E. De Lara,

“VMM-Independent Graphics Acceleration,” in Proceedings of the 3rd
International Conference on Virtual Execution Environments. ACM,
2007, pp. 33–43.

12

Desktop Benchmarking Toolkit,” in International Symposium on
Integrated Network Management, 2009.

[61] T. Richardson, Q. Stafford-Fraser, K. R. Wood, and A. Hopper,

“Virtual Network Computing,” IEEE Internet Computing, vol. 2, no. 1,
Jan 1998.

[62] F. Rumsey, P. Ward, and S. K. Zielinski, “Can Playing A Computer
Game Affect Perception of Audio-Visual Synchrony?” in Audio
Engineering Society Convention. Audio Engineering Society, 2004.

[63] R. W. Scheiﬂer and J. Gettys, “The X window system,” ACM

Transactions on Graphics, vol. 5, no. 2, 1986.

[64] H. Schulzrinne, A. Rao, and R. Lanphier, “Real Time Streaming

Protocol (RTSP),” Tech. Rep., 1998.

[65] D. Shreiner and T. K. O. A. W. Group, OpenGL programming guide:
the ofﬁcial guide to learning OpenGL, versions 3.0 and 3.1. Pearson
Education, 2009.

[66] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing Natural

Scenes and Natural Language with Recursive Neural Networks,” in
Proc. of Int’l Conf. on Machine Learning, 2011.

[67] V. Software, “OpenVR SDK,”

https://github.com/ValveSoftware/openvr, [Online; accessed
12-Nov-2018].

[68] A. Sperduti and A. Starita, “Supervised Neural Networks for the

Classiﬁcation of Structures,” IEEE Transactions on Neural Networks,
vol. 8, no. 3, 1997.

[69] R. Spruijt, J. Kamp, and S. Huisman, “Login Virtual Session Indexer
(VSI) Benchmarking,” Virtual Reality Check Project-Phase II
Whitepaper, 2010.

[70] Y. Sun, T. Baruah, S. A. Mojumder, S. Dong, R. Ubal, X. Gong,

S. Treadway, Y. Bao, V. Zhao, J. L. Abellán, J. Kim, A. Joshi, and
D. R. Kaeli, “MGSim + MGMark: A Framework for Multi-GPU
System Research,” CoRR, vol. abs/1811.02884, 2018.

[71] Z. Tan, Y. Li, Q. Li, Z. Zhang, Z. Li, and S. Lu, “Supporting Mobile
VR in LTE Networks: How Close Are We?” Proc. ACM Meas. Anal.
Comput. Syst., vol. 2, no. 1, Apr. 2018.

[72] U. Technologies, “Unity,” https://unity3d.com/, [Online; accessed

12-Nov-2018].

[73] D. Terpstra, H. Jagode, H. You, and J. Dongarra, “Collecting

Performance Data with PAPI-C,” in Tools for High Performance
Computing 2009, M. S. Müller, M. M. Resch, A. Schulz, and W. E.
Nagel, Eds. Springer Berlin Heidelberg, 2010, pp. 157–173.

[74] N. Theera-Ampornpunt, S. Suryavansh, S. Manchanda, R. Panta,

K. Joshi, M. Ammar, M. Chiang, and S. Bagchi, “Appstreamer:
Reducing storage requirements of mobile games through predictive
streaming,” in Proc. of Int’l Conf. on Embedded Wireless Systems and
Networks, 2020.

[75] TurboVNC, “TurboVNC,” https://turbovnc.org/, [Online; accessed

22-July-2018].

[76] Valve, “InMind VR,” http://blog.dota2.com/?l=english, [Online;

accessed 22-July-2018].

[77] C. Xie, X. Zhang, A. Li, X. Fu, and S. Song, “PIM-VR: Erasing

Motion Anomalies In Highly-Interactive Virtual Reality World with
Customized Memory Cube,” in IEEE Int’l Symp. on High
Performance Computer Architecture, 2019.

[78] N. Zeldovich and R. Chandra, “Interactive Performance Measurement
with VNCplay,” in Proc. of the USENIX Annual Technical Conference,
2005.

[39] A. M. Lai and J. Nieh, “On the Performance of Wide-area Thin-client
Computing,” ACM Transactions on Computer Systems, vol. 24, no. 2,
May 2006.

[40] Z. Lai, Y. C. Hu, Y. Cui, L. Sun, and N. Dai, “Furion: Engineering

high-quality immersive virtual reality on today’s mobile devices,” in
Proc. of Int’l Conf. on Mobile Computing and Networking, 2017.

[41] Y. Leng, C.-C. Chen, Q. Sun, J. Huang, and Y. Zhu, “Energy-Efﬁcient
Video Processing for Virtual Reality,” in Proc. of Int’l Symp. on
Computer Architecture, 2019.

[42] Y. Lin, C. Radoi, and D. Dig, “Retroﬁtting Concurrency for Android
Applications Through Refactoring,” in Proc. of Int’l Symp. on
Foundations of Software Engineering, 2014.

[43] D. Luebke and G. Humphreys, “How GPUs Work,” IEEE Computer,

vol. 40, no. 2, Feb 2007.

[44] K. Mao, M. Harman, and Y. Jia, “Sapienz: Multi-objective automated

testing for android applications,” in Proc. of Int’l Symp. on Software
Testing and Analysis, 2016.

[45] J. McCoy and M. Mateas, “An integrated agent for playing real-time

strategy games.” in AAAI, vol. 8, 2008, pp. 1313–1318.

[46] Microsoft, “Project xCloud,”

https://www.xbox.com/en-US/xbox-game-streaming/project-xcloud,
[Online; accessed 10-Jul-2019].

[47] T. Mitra and T.-c. Chiueh, “Dynamic 3D Graphics Workload

Characterization and the Architectural Implications,” in Proc. of the
Int’l Symposium on Microarchitecture, 1999.

[48] J. Nieh, S. J. Yang, and N. Novik, “Measuring Thin-client

Performance Using Slow-motion Benchmarking,” ACM Transactions
on Computer Systems, vol. 21, no. 1, Feb. 2003.

[49] Nival, “InMind VR,” https://luden.io/inmind/, [Online; accessed

22-July-2018].

[50] NVIDIA, “Geforce Now,”

https://www.nvidia.com/en-us/geforce/products/geforce-now/,
[Online; accessed 11-Nov-2018].

[51] NVidia, “NVIDIA Container Toolkit,”

https://github.com/NVIDIA/nvidia-docker, [Online; accessed
08-Aug-2019].

[52] NVIDIA, “NVIDIA Management Library (NVML),”

https://developer.nvidia.com/nvidia-management-library-nvml,
[Online; accessed 11-Jul-2019].

[53] OpenAI, “OpenAI Five,” https://openai.com/projects/ﬁve/, [Online;

accessed 11-Apr-2020].

[54] OpenAI, “Openai ﬁve,” https://blog.openai.com/openai-ﬁve/, 2018.

[55] PARSEC, “Game, Work, and Play Together from Anywhere,”
https://parsecgaming.com/, [Online; accessed 11-Apr-2020].

[56] B. Paul, “The Mesa 3D Graphics Library,” https://mesa3d.org/,

[Online; accessed 12-Nov-2018].

[57] M. Pfeiffer, H. Kenngott, A. Preukschas, M. Huber, L. Bettscheider,

B. Müller-Stich, and S. Speidel, “IMHOTEP: virtual reality framework
for surgical applications,” International Journal of Computer Assisted
Radiology and Surgery, vol. 13, no. 5, May 2018.

[58] G. F. Pinzari, “Introduction to NX technology,” 2003.

[59] Q. Reeves and L. Salzman, “Red Eclipse: A Free Arena Shooter

Featuring Parktour,” https://www.redeclipse.net/, [Online; accessed
11-Nov-2018].

[60] J. Rhee, A. Kochut, and K. Beaty, “DeskBench: Flexible Virtual

13

