3
1
0
2

n
a
J

9
1

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
5
3
5
4
.
1
0
3
1
:
v
i
X
r
a

Applications and a Three-dimensional Desktop
Environment for an Immersive Virtual Reality
System

Akira Kageyama and Youhei Masada

Graduate School of System Informatics,
Kobe University, Kobe 657-8501, Japan

Abstract

We developed an application launcher called Multiverse for scientiﬁc
visualizations in a CAVE-type virtual reality (VR) system. Multiverse can
be regarded as a type of three-dimensional (3D) desktop environment. In
Multiverse, a user in a CAVE room can browse multiple visualization ap-
plications with 3D icons and explore movies that ﬂoat in the air. Touching
one of the movies causes “teleportation” into the application’s VR space.
After analyzing the simulation data using the application, the user can
jump back into Multiverse’s VR desktop environment in the CAVE.

1 Introduction

CAVE is a room-sized virtual reality (VR) system, which was developed in the
early 1990s at the University of Illinois, Chicago [1]. In a CAVE room, the viewer
is surrounded by wall screens and a ﬂoor screen. Stereo-images are projected
onto the surfaces. Tracking systems are used to capture the viewer’s head
position and direction. The wide viewing angle provided by the surrounding
screens on the walls and ﬂoor generates a high-quality immersive VR experience.
The viewer can interact with three-dimensional (3D) virtual objects using a
portable controller known as wand, in which the tracking system is installed.

CAVE systems have been used for scientiﬁc visualizations from the ﬁrst
system [1] until the latest generation (StarCAVE) [2]. For example, visual-
ization applications in CAVE systems have been developed to analyze general
computational ﬂuid dynamics (CFD) [3], turbulence simulations [4], CFD of
molten iron [5], CFD of wind turbines [6], seismic simulation [7], meteorological
simulation [8], biomedical ﬂuid simulation [9], magnetic resonance imaging [10],
geomagnetic ﬁelds [11], archaeological studies [12], and geophysical surveys [13].
Recently, a new CAVE system was installed at the Integrated Research Cen-
ter (IRC) at Kobe University. This CAVE system was named “π-CAVE” after

1

 
 
 
 
 
 
Figure 1: Overview of the π-CAVE sys-
tem installed at Kobe University.

Figure 2: Projectors and mirrors used
by the π-CAVE system.

the IRC’s location on Port Island (PI). Fig. 1 shows a front view of the π-CAVE
while Fig. 2 shows the conﬁguration of its projectors and mirrors.

The original CAVE system had a cubic geometry with a side length of 3
m. A straightforward extension to enlarge the VR space of a CAVE is to use a
rectangular parallelepiped shape. More sophisticated conﬁgurations have been
proposed for advanced CAVE systems, such as StarCAVE [2], but we used the
rectangular parallelepiped approach for π-CAVE to maximize the VR volume
in the space allowed in the IRC building. The side lengths of π-CAVE were 3 m
× 3 m × 7.8 m. As far as we know, this is the largest CAVE system in Japan.
We have developed several VR applications for the scientiﬁc visualization
of large-scale simulation data. Of these, Virtual LHD [14] was our ﬁrst VR
visualization application. This application was developed for the CompleXcope
CAVE system installed at the National Institute for Fusion Science, Japan.
Currently, Virtual LHD is used to visualize the magnetohydrodynamic (MHD)
equilibrium state of a nuclear fusion experiment. We also developed a general-
purpose visualization application, VFIVE [15, 16, 17, 17], for 3D scalar/vector
ﬁeld data. Recently, we added a new visualization method to VFIVE at π-CAVE
for visualizing magnetic ﬁeld lines frozen into a ﬂuid [18]. The original VFIVE
only accepted a structured grid data format as the input, but an extension
of VFIVE for unstructured grid data was developed at Chuo University [19].
The development and its applications of VFIVE are summarized in our recent
papers [20, 21].

In addition to improvements of VFIVE, we also developed the following four
types of novel CAVE visualization applications for π-CAVE. (1) IonJetEngine:
for VR visualization of plasma particle in cell (PIC) simulations of an ion jet
engine in space probes (2) RetinaProtein: for molecular dynamics (MD) simu-
lations of proteins (3) SeismicWave: for the simulation of seismic wave propaga-
tion (4) CellDivision: to simulate three-dimensional time sequence microscope
images of mouse embryos. All of these new CAVE visualization programs were
written using OpenGL and CAVElib. We started developing these visualization

2

applications when the construction of π-CAVE was underway.

Several problems occur if multiple CAVE visualization applications are ex-
ecuted one after another, as follows. First, the command has to be typed in
to launch the ﬁrst application using the keyboard beside the CAVE room. The
user then enters the CAVE room wearing stereo glasses. After analyzing the
data from the ﬁrst application in the CAVE, the user leaves the CAVE room
and takes oﬀ the glasses. Next, the user types in the command to launch the
second application and enters the CAVE room wearing the stereo glasses. These
steps have to be repeated if there are many applications. This inconvenience
occurs because the CAVE must be used for single tasks.

To resolve this inconvenience, we developed an application launcher for
CAVE. This program, Multiverse, is a CAVE application written in CAVElib
and OpenGL. Multiverse can control other VR applications. These sub-applications
are depicted in CAVE’s VR space using 3D icons or panels. If the user in the
CAVE room touches one of the panels using the wand, they are “teleported” to
the corresponding VR application.

In this paper, we report the hardware used by the π-CAVE system in sec-
tion 2, and we describe the design and implementation of Multiverse in section 3.
The visualization applications loaded into Multiverse are described in section 4.

2 π-CAVE system

π-CAVE has a rectangular parallelepiped conﬁguration with side lengths of 3
m × 3 m × 7.8 m (Fig. 3). The large width (7.8 m) is one of the characteristic
features of the CAVE system. The large volume of π-CAVE allows several
people to stand on the ﬂoor at the same time, without any mutual occlusion of
the screen views in the room.

Figure 3: Alternative view of the π-CAVE system.

Like many other CAVE systems, π-CAVE has four screens: three wall screens
(front, right, and left) and a ﬂoor screen. Soft, semi-transparent screens are used

3

on the walls. The images are rear-projected onto these screens. The ﬂoor is a
hard screen where the stereo image is projected from the ceiling. Two projectors
are used to generate the front wall image (Fig. 4). An edge blending technique
is applied to the interface between the two images. Another pair of projectors
is used for the ﬂoor screen. Each side wall screen (right and left) is projected
onto using a projector. In total, six projectors are used.

Figure 4: Projector settings for π-CAVE, viewed from above.

The resolution of the projector (Christie WU12K-M) shown in Fig. 5 with
the counterpart mirror, is 1920 × 1200 pixels. The brightness is 10,500 lumens.
An optical motion tracking system (Vicon) is used for head and wand tracking.
Ten cameras with 640 × 480 resolution are installed on top of the wall screens.
A commonly used API (Trackd) is used for the interface to CAVElib.

Figure 5: Pair of projectors and mirrors.

Two computer systems are used for computations and for rendering π-CAVE.
One is a Linux PC (HP Z800) with 192 GB of shared memory. Three sets
of GPUs (NVIDIA QuadroPLEX) are used for real-time stereoscopic image
generation by the six projectors. The other computer system is a Windows PC
cluster system.

We used OpenGL for the graphics API and CAVElib for the VR API. We
are also aiming to use VR Juggler [22] for the VR API. Some of our ﬁrst trials
using VR Juggler can be found in our report [23].

4

3 Multiverse

We developed an applications launcher, Multiverse, for the π-CAVE system. At
the start of this Multiverse environment, the viewer in the π-CAVE stands in
the virtual building in IRC where π-CAVE is installed. The 3D CAD model
data of the IRC building (Fig. 6) is loaded into Multiverse and rendered in 3D
in real time. This is the Multiverse’s start-up environment known as World.
In the World mode of Multiverse, the viewer can walk through the building.
7(a) shows a snapshot where the user is approaching the IRC building.
Fig.
In Fig. 7(b), the viewer is (literally) walking into the (virtual) IRC building.
Some ﬁne structures of the building, including the virtual π-CAVE is shown in
Fig. 7(c) and (d), are also loaded from CAD data ﬁles.

Figure 6: Three-dimensional CAD data for the IRC building loaded in Multi-
verse.

Figure 7: A snapshot sequence of Multiverse in the World mode. (a) The viewer
is entering the virtual IRC building. (b) The viewer walks into the building. (c)
Rear of the (virtual) π-CAVE. The CAD models of the projectors behind the
CAVE screens can be seen. (d) Virtual π-CAVE in the real π-CAVE.

5

In Multiverse, there are two methods of showing the application list loaded in
Multiverse. The ﬁrst is to use “ribbons” that connect the wand and application
icons. In the “ribbons” mode, the user in the World ﬁnds one or more curves
or wires that start from the wand tip. Each wire is a type of guide that leads
the user to a Gate.

A Gate is an entrance to the VR world of the corresponding application. If
multiple visualization applications are loaded into Multiverse, this automatically
generates the corresponding number of Gates. All of these are connected to the
user (or the wand) via guide wires (Fig. 8). If the user walks or “ﬂies” into a
place in front of a Gate, they will ﬁnd an exploratory movie near the Gate (see
the rectangular panel in the center of the blue, torus-shaped Gate in Fig. 8).
This explains the type of application that will be executed when the user selects
the Gate. To select the application, the user (literally) walks through the Gate
when the corresponding VR application program loads and the user feels as if
they have been “teleported” to the visualization space. Each VR world is known
as a Universe in Multiverse.

Figure 8: Gates to the individual visualization applications ﬂoating in the
World.

Another method of showing the application lists loaded in the Multiverse
is to use a virtual elevator. When the user enters the elevator in the (virtual)
IRC building, they are automatically taken upward by the elevator into the sky
above the IRC building. The spatial scale of the view changes rapidly from the
building, to the city, country, and ﬁnally the globe. The user ﬁnds that they
are “ﬂoating” in space surrounded by stars. Several panels then appear in front
of the viewer. Each panel represents a visualization application (Fig. 9).

When the user touches one of the panels, the corresponding VR application
is launched and the user is “teleported” to the selected visualization Universe.
In short, Multiverse is composed of the World and several Universes. World
is a type of 3D desktop environment and a Universe is a visualization application
loaded onto Multiverse.

In the program code, each Universe is simply a standard CAVE application

6

Figure 9: Virtual touch screens in the World mode. Each panel represents a
VR visualization program or Universe. A Universe is executed when the user
touches the panel using the wand.

with a uniﬁed interface to the Multiverse class. A Universe is an instance
of a class that is derived from a virtual class known as Vacuum. Vacuum
represents an empty space, which only has an interface to the Multiverse class
through the member functions initialize(), draw(), update_per_frame(),
and compute(). These function names convey their roles to readers who are
familiar with CAVElib programming.

4 Applications

In this section, we describe ﬁve applications, or Universes, which we developed
as the ﬁrst applications for the Multiverse environment.

4.1 Universe::GeomagField

We converted VFIVE, which is described in section 1, into a class of Universe.
VFIVE is a general-purpose visualization tool, so we can visualize any vec-
tor/scalar ﬁeld provided that the data are legitimate for VFIVE’s input data
format in the Multiverse framework.

Fig. 10 shows a snapshot of an example of a Universe based on VFIVE,
known as GeomagField. The input data used by GeomagField was a geodynamo
simulation performed by one of the authors and his colleagues [24, 25, 26]. The
purpose of this simulation was to understand the mechanism that generates the
Earth’s magnetic ﬁeld (or geomagnetic ﬁeld).

Fig. 11 shows another snapshot of GeomagField in which two VFIVE visu-
alization methods were applied. The temperature distribution was visualized
by volume rendering (colored in orange to yellow). The 3D arrow glyphs show
the ﬂow velocity vectors around the wand position. The arrows followed the
motion when the viewer moved the wand, which changed the directions and

7

Figure 10: GeomagField, a Universe in Multiverse. This VR application was
used to visualize an MHD simulation of geomagnetic ﬁeld generation in Earth’s
liquid core.

Figure 11: Another snapshot of GeomagField. The scalar ﬁeld distribution of
the temperature is visualized by volume rendering. The convection ﬂow velocity
is visualized using 3D arrow glyphs while tracer particles are under the spotlight.

lengths (vector amplitudes) in real time. The white balls are tracer particles
that also visualized the ﬂow velocity. These balls were highlighted in a spot-
light or cone-shaped region, the apex of which was the wand. This visualization
method is known as Snowﬂakes in VFIVE. The viewer can change the focus
of the ﬂow visualization by changing the direction of the spotlight via wand
direction movements.

4.2 Universe::IonJetEngine

The second example of a Universe is known as IonJetEngine and a snapshot is
shown in Fig. 12.

This Universe visualized a plasma PIC simulation of the ion jet engine of a
space probe. The positions of the particles (ions and electrons) were represented
by balls (yellow for ions and blue for electrons). The velocity distribution of the
jet was visualized as the set of the individual motions of the particles. A 3D
model of the virtual space probe from which the plasma jet beams were ejected
is also shown in Fig. 12.

8

Figure 12: A snapshot of the Universe known as IonJetEngine. Plasma jets
from a space probe were visualized by moving ions (yellow) and electrons (blue)
as particles. The simulation data were provided by Prof. Usui.

4.3 Universe::RetinaProtein

Fig. 13 shows a Universe known as RetinaProtein, which was a molecular dy-
namics simulation of rhodopsin [27], a protein in the human retina. At the start
of this Universe, the viewer observed a 3D model of a human (see the top panel
of Fig. 13). As the viewer approached the model’s face, the ﬁne structures of
the eyes became visible until MD simulation visualization appeared.

Figure 13: Snapshots of the RetinaProtein Universe. The molecular structure
of rhodopsin was visualized in the human retina. The MD simulation data were
provided by Prof. Ten-no of Kobe University and his colleagues.

4.4 Universe::SeismicWave

In this Universe, a simulation of seismic wave propagation [28]was visualized,
which was performed by Prof. Furumura of the University of Tokyo by animated
volume rendering (see Fig. 14). In this Universe, we implemented rapid volume
rendering based on the 3D texture mapping technique in CAVEs. The full
details of this implementation will be reported elsewhere.

9

4.5 Universe::CellDivision

The ﬁnal Universe described here is CellDivision and a snapshot is shown in
Fig. 15. The target data used for this visualization were not simulation data.
Instead, they were microscope images of live mouse embryos. The data were
provided by Dr. Yamagata of Osaka University. The time sequence of micro-
scope images was visualized as an animated volume rendering using the same
tool used for SeismicWave in the previous subsection.

5 Summary

In many CAVE systems, VR applications are executed as single tasks. Thus,
the user has to type in each command one after another outside the CAVE
room. To convert a CAVE into a more convenient tool for scientiﬁc visualiza-
tion, we developed an application launcher known as Multiverse. Multiverse
comprises a World and Universes. World, which correspond to the desktop of
a PC operating system, where the user can select visualization applications by
touching icons ﬂoating in the World. Using the virtual touch screen interface,
the speciﬁed application program is launched and the user is “teleported” to an-
other VR space containing the corresponding visualization application, which is
known as a Universe. We developed ﬁve Universes, which can be launched from
the Multiverse environment. Multiverse was designed as a general application
framework, so it can read and control other Universes. A user can jump back
to a World and switch to another Universe at any time from any Universe.

During the implementation of Multiverse, we developed several new fun-
damental tools and methods for the CAVE environment, such as a fast speed
volume renderer, a 3D model (CAD) data loader/renderer, and a 2D movie
ﬁle loader/renderer. Details of these fundamental tools and methods will be
reported elsewhere.

Figure 14: Time sequence of snapshots of the SeismicWave Universe.

10

Figure 15: A snapshot of CellDivision. Animated volume rendering of live cell
images of a mouse embryo. The data were provided by Dr. K. Yamagata, Osaka
University.

Acknowledgements

We thank the undergraduate students at our laboratory at Kobe University
(Toshiaki Morimoto, Yasuhiro Nishida, Yuta Ohno, Tomoki Yamada, and Mana
Yuki) for contributing to the development of Multiverse. The plasma particle
simulation data were provided by Prof. H. Usui, Dr. Y. Miyake, and Mr. A.
Hashimoto (Kobe University). The MD simulation data were provided by
Prof. S. Ten-no and Dr. Y. Akinaga. The simulation data for seismic wave
propagation were provided by Prof. T. Furumura (University of Tokyo). The
microscope images were provided by Dr. K. Yamagata (Osaka University).

This work was supported by JSPS KAKENHI Grant Numbers 23340128
and 30590608, and also by the Takahashi Industrial and Economic Research
Foundation.

References

[1] Cruz-neira C, Sandin D J and Defanti T A 1993 Proc. SIGGRAPH ’93 pp

135–142

[2] Defanti T, Dawe G, Sandin D, Schulze J, Otto P, Girado J, Kuester F,
Smarr L and Rao R 2009 Future Generation Computer Systems 25 pp
169–178

[3] Jaswal V 1997 Proc. Visualization ’97 pp 301–308

[4] Tufo H M, Fischer P F, Papka M E and Blom K 1999 Proc. ACM/IEEE

Conf. Supercomputing 1999 pp 62–76

[5] Fu D, Wu B, Chen G, Moreland J, Tian F, Hu Y and Zhou C Q 2010

Proc. 14th Int. Heat Transfer Conf., pp 1–8

11

[6] Yan N, Okosun T, Basak S K, Fu D, Moreland J and Zhou C Q 2011
Proc. ASME 2011 Int. Design Engineering Technical Conf.& Computers
and Information in Engineering Conf., pp 1–8

[7] Chopra P, Meyer J and Fernandez A 2002 IEEE Visualization, 2002 , pp

497–500

[8] Ziegeler S, Moorhead R J, Croft P J and Lu D 2001 Proc. Conf. Visualiza-

tion ’01 , pp 489–493

[9] Forsberg A, Laidlaw D, Van Dam A, Kirby R, Kafniadakis G and Elion J

2000 Proc. Conf. Visualization ’00 , pp 457–460

[10] Zhang S, Demiralp C, Keefe D, DaSilva M, Laidlaw D, Greenberg B, Basser
P, Pierpaoli C, Chiocca E and Deisboeck T 2001 Proc. Visualization, 2001,
pp 437–584

[11] Bidasaria H B 2005 Proc. 43rd Annual Southeast Regional Conf. on ACM-

SE 43 , p 355

[12] Acevedo D, Vote E, Laidlaw D and Joukowsky M 2001 Proc. Visualization,

pp 493–597

[13] Lin A Y m, Novo A, Weber P P, Morelli G and Goodman D 2011 Advances

in Visual Computing (Springer Berlin Heidelberg) pp 229–238

[14] Kageyama A, Hayashi T, Horiuchi R, Watanabe K and Sato T 1998
Proc. 16th Int. Conf. Numerical Simulation Plasmas (Santa Barbara, CA,
USA) pp 138–142

[15] Kageyama A, Tamura Y and Sato T 2000 Prog. Theor. Phys., Suppl. 138

pp 665–673

[16] Ohno N and Kageyama A 2007 Phys. Earth Planet. Inter. 163 pp 305–311

[17] Ohno N and Kageyama A 2010 Comput. Phys. Comm. 181 pp 720–725

[18] Murata K and Kageyama A 2011 Plasma Fusion Res. 6 2406023–1–5

[19] Kashiyama K, Takada T, Yamazaki T, Kageyama A, Ohno N and Miyachi
H 2009 Proc. 9th Int. Conf. Construction Applications of Virtual Reality
(Sydney) pp 1–6

[20] Kageyama A, and Ohno N submitted to Int. J. Modeling Simulation &

Scientiﬁc Comput.

[21] Kageyama A, Ohno N, Kawahara S, Kashiyama K and Ohtani H submitted

to Int. J. Modeling Simulation & Scientiﬁc Comput.

[22] Bierbaum A, Just C, Hartling P, Meinert K, Baker A and Cruz-Neira C

2001 Proc. IEEE Virtual Reality 2001 , pp 89–96

12

[23] Meno D, Kageyama A and Masada Y 2012 Proc. Int. Conf. Simulation

Technology pp 387–389

[24] Kageyama A, Miyagoshi T and Sato T 2008 Nature 454 pp 1106–1109

[25] Miyagoshi T, Kageyama A and Sato T 2010 Nature 463 pp 793–796

[26] Miyagoshi T, Kageyama A and Sato T 2011 Phys. Plasmas 18 p 072901

[27] Akinaga Y, Jung J and Ten-no S 2011 Phys. Chem. Chem. Phy. 13 pp

14490-14499

[28] Furumura T, Kennett B L N and Koketsu K 2003 Bul. Seismological

Soc. America 93 pp 870–881

13

