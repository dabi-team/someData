9
1
0
2

g
u
A
8

]

G
L
.
s
c
[

2
v
3
5
9
7
0
.
5
0
9
1
:
v
i
X
r
a

Cluster-GCN: An Efficient Algorithm for Training Deep and
Large Graph Convolutional Networks

Wei-Lin Chiang∗
National Taiwan University
r06922166@csie.ntu.edu.tw

Xuanqing Liu∗
University of California, Los Angeles
xqliu@cs.ucla.edu

Si Si
Google Research
sisidaisy@google.com

Yang Li
Google Research
liyang@google.com

Samy Bengio
Google Research
bengio@google.com

Cho-Jui Hsieh
University of California, Los Angeles
chohsieh@cs.ucla.edu

ABSTRACT
Graph convolutional network (GCN) has been successfully applied
to many graph-based applications; however, training a large-scale
GCN remains challenging. Current SGD-based algorithms suffer
from either a high computational cost that exponentially grows
with number of GCN layers, or a large space requirement for keep-
ing the entire graph and the embedding of each node in memory. In
this paper, we propose Cluster-GCN, a novel GCN algorithm that is
suitable for SGD-based training by exploiting the graph clustering
structure. Cluster-GCN works as the following: at each step, it sam-
ples a block of nodes that associate with a dense subgraph identified
by a graph clustering algorithm, and restricts the neighborhood
search within this subgraph. This simple but effective strategy leads
to significantly improved memory and computational efficiency
while being able to achieve comparable test accuracy with previous
algorithms. To test the scalability of our algorithm, we create a
new Amazon2M data with 2 million nodes and 61 million edges
which is more than 5 times larger than the previous largest publicly
available dataset (Reddit). For training a 3-layer GCN on this data,
Cluster-GCN is faster than the previous state-of-the-art VR-GCN
(1523 seconds vs 1961 seconds) and using much less memory (2.2GB
vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our
algorithm can finish in around 36 minutes while all the existing
GCN training algorithms fail to train due to the out-of-memory
issue. Furthermore, Cluster-GCN allows us to train much deeper
GCN without much time and memory overhead, which leads to
improved prediction accuracy—using a 5-layer Cluster-GCN, we
achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while
the previous best result was 98.71 by [16]. Our codes are publicly
available at https://github.com/google-research/google-research/
tree/master/cluster_gcn.

ACM Reference Format:
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui
Hsieh. 2019. Cluster-GCN: An Efficient Algorithm for Training Deep and

∗This work was done during the first and the second author’s internship at Google
Research.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’19, August 4–8, 2019, Anchorage, AK, USA
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6201-6/19/08.
https://doi.org/10.1145/3292500.3330925

Large Graph Convolutional Networks. In The 25th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’19), August 4–
8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3292500.3330925

1 INTRODUCTION
Graph convolutional network (GCN) [9] has become increasingly
popular in addressing many graph-based applications, including
semi-supervised node classification [9], link prediction [17] and
recommender systems [15]. Given a graph, GCN uses a graph con-
volution operation to obtain node embeddings layer by layer—at
each layer, the embedding of a node is obtained by gathering the
embeddings of its neighbors, followed by one or a few layers of
linear transformations and nonlinear activations. The final layer
embedding is then used for some end tasks. For instance, in node
classification problems, the final layer embedding is passed to a
classifier to predict node labels, and thus the parameters of GCN
can be trained in an end-to-end manner.

Since the graph convolution operator in GCN needs to propagate
embeddings using the interaction between nodes in the graph, this
makes training quite challenging. Unlike other neural networks
that the training loss can be perfectly decomposed into individual
terms on each sample, the loss term in GCN (e.g., classification
loss on a single node) depends on a huge number of other nodes,
especially when GCN goes deep. Due to the node dependence,
GCN’s training is very slow and requires lots of memory – back-
propagation needs to store all the embeddings in the computation
graph in GPU memory.

Previous GCN Training Algorithms: To demonstrate the
need of developing a scalable GCN training algorithm, we first
discuss the pros and cons of existing approaches, in terms of 1)
memory requirement1, 2) time per epoch2 and 3) convergence
speed (loss reduction) per epoch. These three factors are crucial for
evaluating a training algorithm. Note that memory requirement
directly restricts the scalability of algorithm, and the later two
factors combined together will determine the training speed. In the
following discussion we denote N to be the number of nodes in the
graph, F the embedding dimension, and L the number of layers to
analyze classic GCN training algorithms.
• Full-batch gradient descent is proposed in the first GCN pa-
per [9]. To compute the full gradient, it requires storing all the

1Here we consider the memory for storing node embeddings, which is dense and
usually dominates the overall memory usage for deep GCN.
2An epoch means a complete data pass.

 
 
 
 
 
 
intermediate embeddings, leading to O(N F L) memory require-
ment, which is not scalable. Furthermore, although the time per
epoch is efficient, the convergence of gradient descent is slow
since the parameters are updated only once per epoch.
[memory: bad; time per epoch: good; convergence: bad]

• Mini-batch SGD is proposed in [5]. Since each update is only
based on a mini-batch gradient, it can reduce the memory re-
quirement and conduct many updates per epoch, leading to
a faster convergence. However, mini-batch SGD introduces a
significant computational overhead due to the neighborhood
expansion problem—to compute the loss on a single node at
layer L, it requires that node’s neighbor nodes’ embeddings at
layer L − 1, which again requires their neighbors’ embeddings
at layer L − 2 and recursive ones in the downstream layers. This
leads to time complexity exponential to the GCN depth. Graph-
SAGE [5] proposed to use a fixed size of neighborhood samples
during back-propagation through layers and FastGCN [1] pro-
posed importance sampling, but the overhead of these methods
is still large and will become worse when GCN goes deep.
[memory: good; time per epoch: bad; convergence: good]

• VR-GCN [2] proposes to use a variance reduction technique
to reduce the size of neighborhood sampling nodes. Despite
successfully reducing the size of samplings (in our experiments
VR-GCN with only 2 samples per node works quite well), it
requires storing all the intermediate embeddings of all the nodes
in memory, leading to O(N F L) memory requirement. If the num-
ber of nodes in the graph increases to millions, the memory
requirement for VR-GCN may be too high to fit into GPU.
[memory: bad; time per epoch: good; convergence: good.]

In this paper, we propose a novel GCN training algorithm by
exploiting the graph clustering structure. We find that the efficiency
of a mini-batch algorithm can be characterized by the notion of “em-
bedding utilization”, which is proportional to the number of links
between nodes in one batch or within-batch links. This finding mo-
tivates us to design the batches using graph clustering algorithms
that aims to construct partitions of nodes so that there are more
graph links between nodes in the same partition than nodes in dif-
ferent partitions. Based on the graph clustering idea, we proposed
Cluster-GCN, an algorithm to design the batches based on efficient
graph clustering algorithms (e.g., METIS [8]). We take this idea
further by proposing a stochastic multi-clustering framework to im-
prove the convergence of Cluster-GCN. Our strategy leads to huge
memory and computational benefits. In terms of memory, we only
need to store the node embeddings within the current batch, which
is O(bF L) with the batch size b. This is significantly better than
VR-GCN and full gradient decent, and slightly better than other
SGD-based approaches. In terms of computational complexity, our
algorithm achieves the same time cost per epoch with gradient de-
scent and is much faster than neighborhood searching approaches.
In terms of the convergence speed, our algorithm is competitive
with other SGD-based approaches. Finally, our algorithm is simple
to implement since we only compute matrix multiplication and no
neighborhood sampling is needed. Therefore for Cluster-GCN, we
have [memory: good; time per epoch: good; convergence: good].
We conducted comprehensive experiments on several large-scale

• Cluster-GCN achieves the best memory usage on large-scale
graphs, especially on deep GCN. For example, Cluster-GCN
uses 5x less memory than VRGCN in a 3-layer GCN model on
Amazon2M. Amazon2M is a new graph dataset that we construct
to demonstrate the scalablity of the GCN algorithms. This dataset
contains a amazon product co-purchase graph with more than 2
millions nodes and 61 millions edges.

• Cluster-GCN achieves a similar training speed with VR-GCN
for shallow networks (e.g., 2 layers) but can be faster than VR-
GCN when the network goes deeper (e.g., 4 layers), since our
complexity is linear to the number of layers L while VR-GCN’s
complexity is exponential to L.

• Cluster-GCN is able to train a very deep network that has a
large embedding size. Although several previous works show
that deep GCN does not give better performance, we found that
with proper optimization, deeper GCN could help the accuracy.
For example, with a 5-layer GCN, we obtain a new benchmark
accuracy 99.36 for PPI dataset, comparing with the highest re-
ported one 98.71 by [16].

Implementation of our proposed method is publicly available.3

2 BACKGROUND
Suppose we are given a graph G = (V, E, A), which consists of
N = |V | vertices and |E | edges such that an edge between any
two vertices i and j represents their similarity. The corresponding
adjacency matrix A is an N × N sparse matrix with (i, j) entry equal-
ing to 1 if there is an edge between i and j and 0 otherwise. Also,
each node is associated with an F -dimensional feature vector and
X ∈ RN ×F denotes the feature matrix for all N nodes. An L-layer
GCN [9] consists of L graph convolution layers and each of them
constructs embeddings for each node by mixing the embeddings of
the node’s neighbors in the graph from the previous layer:

Z (l +1) = A′X (l )W (l ), X (l +1) = σ (Z (l +1)),

(1)

where X (l ) ∈ RN ×Fl
is the embedding at the l-th layer for all
the N nodes and X (0) = X ; A′ is the normalized and regularized
adjacency matrix and W (l ) ∈ RFl ×Fl +1 is the feature transformation
matrix which will be learnt for the downstream tasks. Note that for
simplicity we assume the feature dimensions are the same for all
layers (F1 = · · · = FL = F ). The activation function σ (·) is usually
set to be the element-wise ReLU.

Semi-supervised node classification is a popular application of
GCN. When using GCN for this application, the goal is to learn
weight matrices in (1) by minimizing the loss function:

L =

1
|YL |

(cid:213)

i ∈YL

loss(yi , zL
i ),

(2)

is the
where YL contains all the labels for the labeled nodes; z
i-th row of Z (L) with the ground-truth label to be yi , indicating the
final layer prediction of node i. In practice, a cross-entropy loss is
commonly used for node classification in multi-class or multi-label
problems.

(L)
i

graph datasets and made the following contributions:

3https://github.com/google-research/google-research/tree/master/cluster_gcn

Table 1: Time and space complexity of GCN training algorithms. L is number of layers, N is number of nodes, ∥A∥0 is number
of nonzeros in the adjacency matrix, and F is number of features. For simplicity we assume number of features is fixed for all
layers. For SGD-based approaches, b is the batch size and r is the number of sampled neighbors per node. Note that due to the
variance reduction technique, VR-GCN can work with a smaller r than GraphSAGE and FastGCN. For memory complexity,
l =1 and the other term is for storing embeddings. For simplicity we omit the memory for storing the
LF
graph (GCN) or sub-graphs (other approaches) since they are fixed and usually not the main bottleneck.

2 is for storing {W (l )}L

Time complexity
Memory complexity

GCN [9]
O(L∥A∥0F + LN F
2)
O(LN F + LF

2)

Vanilla SGD
2)
O(d LN F
O(bd LF + LF

GraphSAGE [5]
O(r LN F

2)
2) O(br LF + LF

FastGCN [1]
2)
O(rLN F
2) O(brLF + LF

2)

VR-GCN [2]

O(L∥A∥0F + LN F

O(LN F + LF

2 + r LN F
2)

Cluster-GCN
2) O(L∥A∥0F + LN F
2)

O(bLF + LF

2)

3 PROPOSED ALGORITHM
We first discuss the bottleneck of previous training methods to
motivate the proposed algorithm.

In the original paper [9], full gradient descent is used for training
GCN, but it suffers from high computational and memory cost.
In terms of memory, computing the full gradient of (2) by back-
propagation requires storing all the embedding matrices {Z (l )}L
l =1
which needs O(N F L) space. In terms of convergence speed, since
the model is only updated once per epoch, the training requires
more epochs to converge.

It has been shown that mini-batch SGD can improve the training
speed and memory requirement of GCN in some recent works [1,
2, 5]. Instead of computing the full gradient, SGD only needs to
calculate the gradient based on a mini-batch for each update. In this
paper, we use B ⊆ [N ] with size b = |B| to denote a batch of node
indices, and each SGD step will compute the gradient estimation

1

|B|

(cid:213)

i ∈B

∇loss(yi , z

(L)
i

)

(3)

to perform an update. Despite faster convergence in terms of epochs,
SGD will introduce another computational overhead on GCN train-
ing (as explained in the following), which makes it having much
slower per-epoch time compared with full gradient descent.

(L)
i

Why does vanilla mini-batch SGD have slow per-epoch
time? We consider the computation of the gradient associated with
one node i : ∇loss(yi , z
). Clearly, this requires the embedding
of node i, which depends on its neighbors’ embeddings in the
previous layer. To fetch each node i’s neighbor nodes’ embeddings,
we need to further aggregate each neighbor node’s neighbor nodes’
embeddings as well. Suppose a GCN has L + 1 layers and each node
has an average degree of d, to get the gradient for node i, we need to
aggregate features from O(d L) nodes in the graph for one node. That
is, we need to fetch information for a node’s hop-k (k = 1, · · · , L)
neighbors in the graph to perform one update. Computing each
2) time due to the multiplication with W (l ),
embedding requires O(F
so in average computing the gradient associated with one node
requires O(d LF

2) time.

Embedding utilization can reflect computational efficiency.

If a batch has more than one node, the time complexity is less
straightforward since different nodes can have overlapped hop-
k neighbors, and the number of embedding computation can be
less than the worst case O(bd L). To reflect the computational effi-
ciency of mini-batch SGD, we define the concept of “embedding
utilization” to characterize the computational efficiency. During

(l )
i

the algorithm, if the node i’s embedding at l-th layer z
is com-
puted and is reused u times for the embedding computations at
layer l + 1, then we say the embedding utilization of z
is u. For
mini-batch SGD with random sampling, u is very small since the
graph is usually large and sparse. Assume u is a small constant
(almost no overlaps between hop-k neighbors), then mini-batch
SGD needs to compute O(bd L) embeddings per batch, which leads
to O(bd LF

2) time per update and O(Nd LF

2) time per epoch.

(l )
i

We illustrate the neighborhood expansion problem in the left
panel of Fig. 1. In contrary, full-batch gradient descent has the
maximal embedding utilization—each embedding will be reused d
(average degree) times in the upper layer. As a consequence, the
original full gradient descent [9] only needs to compute O(N L) em-
beddings per epoch, which means on average only O(L) embedding
computation is needed to acquire the gradient of one node.

To make mini-batch SGD work, previous approaches try to re-
strict the neighborhood expansion size, which however do not
improve embedding utilization. GraphSAGE [5] uniformly samples
a fixed-size set of neighbors, instead of using a full-neighborhood
set. We denote the sample size as r . This leads to O(r L) embedding
computations for each loss term but also makes gradient estima-
tion less accurate. FastGCN [1] proposed an important sampling
strategy to improve the gradient estimation. VR-GCN [2] proposed
a strategy to store the previous computed embeddings for all the
N nodes and L layers and reuse them for unsampled neighbors.
Despite the high memory usage for storing all the N L embeddings,
we find their strategy very useful and in practice, even for a small
r (e.g., 2) can lead to good convergence.

We summarize the time and space complexity in Table 1. Clearly,
all the SGD-based algorithms suffer from exponential complexity
with respect to the number of layers, and for VR-GCN, even though
r can be small, they incur huge space complexity that could go
beyond a GPU’s memory capacity. In the following, we introduce
our Cluster-GCN algorithm, which achieves the best of two worlds—
the same time complexity per epoch with full gradient descent and
the same memory complexity with vanilla SGD.

3.1 Vanilla Cluster-GCN
Our Cluster-GCN technique is motivated by the following ques-
tion: In mini-batch SGD updates, can we design a batch and the
corresponding computation subgraph to maximize the embedding
utilization? We answer this affirmative by connecting the concept
of embedding utilization to a clustering objective.

Consider the case that in each batch we compute the embeddings
for a set of nodes B from layer 1 to L. Since the same subgraph
AB, B (links within B) is used for each layer of computation, we can

final embedding matrix becomes

Z (L) = ¯A′σ ( ¯A′σ (· · · σ ( ¯A′XW (0))W (1)) · · · )W (L−1)

(6)

11σ (· · · σ ( ¯A′

11X1W (0))W (1)) · · · )W (L−1)

=

11σ ( ¯A′
¯A′






cc σ ( ¯A′
¯A′



...

cc σ (· · · σ ( ¯A′

cc XcW (0))W (1)) · · · )W (L−1)

due to the block-diagonal form of ¯A (note that ¯A′
is the correspond-
t t
ing diagonal block of ¯A′). The loss function can also be decomposed
into
L ¯A′ = (cid:213)

and L ¯A′

loss(yi , z

L ¯A′

(cid:213)

(7)

=

).

(L)
i

t t

t t

|Vt |
N

1
|Vt |

t

i ∈Vt









Figure 1: The neighborhood expansion difference between
traditional graph convolution and our proposed cluster ap-
proach. The red node is the starting node for neighbor-
hood nodes expansion. Traditional graph convolution suf-
fers from exponential neighborhood expansion, while our
method can avoid expensive neighborhood expansion.

then see that embedding utilization is the number of edges within
this batch ∥AB, B ∥0. Therefore, to maximize embedding utilization,
we should design a batch B to maximize the within-batch edges,
by which we connect the efficiency of SGD updates with graph
clustering algorithms.

Now we formally introduce Cluster-GCN. For a graph G, we par-
tition its nodes into c groups: V = [V1, · · · Vc ] where Vt consists
of the nodes in the t-th partition. Thus we have c subgraphs as

¯G = [G1, · · · , Gc ] = [{V1, E1}, · · · , {Vc , Ec }],

where each Et only consists of the links between nodes in Vt .
After reorganizing nodes, the adjacency matrix is partitioned into
2 submatrices as
c

A = ¯A + ∆ =

A11


...



Ac1



· · · A1c
...
. . .
· · · Acc









and

¯A =

A11


...



0



0
· · ·
...
. . .
· · · Acc









, ∆ =

0


...



Ac1



· · · A1c
...
. . .
0
· · ·

,









(4)

(5)

where each diagonal block At t is a |Vt | × |Vt | adjacency matrix
containing the links within Gt . ¯A is the adjacency matrix for graph
¯G; Ast contains the links between two partitions Vs and Vt ; ∆ is
the matrix consisting of all off-diagonal blocks of A. Similarly, we
can partition the feature matrix X and training labels Y according to
the partition [V1, · · · , Vc ] as [X1, · · · , Xc ] and [Y1, · · · , Yc ] where
Xt and Yt consist of the features and labels for the nodes in Vt
respectively.

The benefit of this block-diagonal approximation ¯G is that the
objective function of GCN becomes decomposible into different
batches (clusters). Let ¯A′ denotes the normalized version of ¯A, the

The Cluster-GCN is then based on the decomposition form in
(6) and (7). At each step, we sample a cluster Vt and then conduct
SGD to update based on the gradient of L ¯A′
, and this only re-
quires the sub-graph At t , the Xt , Yt on the current batch and the
models {W (l )}L
. The implementation only requires forward and
backward propagation of matrix products (one block of (6)) that is
much easier to implement than the neighborhood search procedure
used in previous SGD-based training methods.

l =1

t t

We use graph clustering algorithms to partition the graph. Graph
clustering methods such as Metis [8] and Graclus [4] aim to con-
struct the partitions over the vertices in the graph such that within-
clusters links are much more than between-cluster links to better
capture the clustering and community structure of the graph. These
are exactly what we need because: 1) As mentioned before, the em-
bedding utilization is equivalent to the within-cluster links for each
batch. Intuitively, each node and its neighbors are usually located
in the same cluster, therefore after a few hops, neighborhood nodes
with a high chance are still in the same cluster. 2) Since we replace A
by its block diagonal approximation ¯A and the error is proportional
to between-cluster links ∆, we need to find a partition to minimize
number of between-cluster links.

In Figure 1, we illustrate the neighborhood expansion with full
graph G and the graph with clustering partition ¯G. We can see that
cluster-GCN can avoid heavy neighborhood search and focus on
the neighbors within each cluster. In Table 2, we show two differ-
ent node partition strategies: random partition versus clustering
partition. We partition the graph into 10 parts by using random
partition and METIS. Then use one partition as a batch to perform
a SGD update. We can see that with the same number of epochs,
using clustering partition can achieve higher accuracy. This shows
using graph clustering is important and partitions should not be
formed randomly.

Time and space complexity. Since each node in Vt only links
to nodes inside Vt , each node does not need to perform neigh-
borhoods searching outside At t . The computation for each batch
will purely be matrix products ¯A′
(l )
t W (l ) and some element-wise
t t X
operations, so the overall time complexity per batch is O(∥At t ∥0F +
2). Thus the overall time complexity per epoch becomes O(∥A∥0F +
bF
2). In average, each batch only requires computing O(bL) embed-
N F
dings, which is linear instead of exponential to L. In terms of space
complexity, in each batch, we only need to load b samples and store
their embeddings on each layer, resulting in O(bLF ) memory for
storing embeddings. Therefore our algorithm is also more memory

Table 2: Random partition versus clustering partition of
the graph (trained on mini-batch SGD). Clustering partition
leads to better performance (in terms of test F1 score) since it
removes less between-partition links. These three datasetes
are all public GCN datasets. We will explain PPI data in the
experiment part. Cora has 2,708 nodes and 13,264 edges, and
Pubmed has 19,717 nodes and 108,365 edges.

Dataset
Cora
Pubmed
PPI

random partition
78.4
78.9
68.1

clustering partition
82.5
79.9
92.9

Figure 3: The proposed stochastic multiple partitions
scheme. In each epoch, we randomly sample q clusters (q = 2
is used in this example) and their between-cluster links to
form a new batch. Same color blocks are in the same batch.

Figure 2: Histograms of entropy values based on the la-
bel distribution. Here we present within each batch using
random partition versus clustering partition. Most cluster-
ing partitioned batches have low label entropy, indicating
skewed label distribution within each batch. In comparison,
random partition will lead to larger label entropy within a
batch although it is less efficient as discussed earlier. We par-
tition the Reddit dataset with 300 clusters in this example.

efficient than all the previous algorithms. Moreover, our algorithm
only requires loading a subgraph into GPU memory instead of the
full graph (though graph is usually not the memory bottleneck). The
detailed time and memory complexity are summarized in Table 1.

3.2 Stochastic Multiple Partitions
Although vanilla Cluster-GCN achieves good computational and
memory complexity, there are still two potential issues:

• After the graph is partitioned, some links (the ∆ part in Eq. (4))

are removed. Thus the performance could be affected.

• Graph clustering algorithms tend to bring similar nodes together.
Hence the distribution of a cluster could be different from the
original data set, leading to a biased estimation of the full gradi-
ent while performing SGD updates.

In Figure 2, we demonstrate an example of unbalanced label dis-
tribution by using the Reddit data with clusters formed by Metis.
We calculate the entropy value of each cluster based on its label
distribution. Comparing with random partitioning, we clearly see
that entropy of most clusters are smaller, indicating that the label
distributions of clusters are biased towards some specific labels.
This increases the variance across different batches and may affect
the convergence of SGD.

Figure 4: Comparisons of choosing one cluster versus multi-
ple clusters. The former uses 300 partitions. The latter uses
1500 and randomly select 5 to form one batch. We present
epoch (x-axis) versus F1 score (y-axis).

To address the above issues, we propose a stochastic multiple
clustering approach to incorporate between-cluster links and re-
duce variance across batches. We first partition the graph into p
clusters V1, · · · , Vp with a relatively large p. When constructing a
batch B for an SGD update, instead of considering only one cluster,
we randomly choose q clusters, denoted as t1, . . . , tq and include
their nodes {Vt1 ∪ · · · ∪ Vtq } into the batch. Furthermore, the links
between the chosen clusters,

{Ai j | i, j ∈ t1, . . . , tq },
are added back. In this way, those between-cluster links are re-
incorporated and the combinations of clusters make the variance
across batches smaller. Figure 3 illustrates our algorithm—for each
epochs, different combinations of clusters are chosen as a batch. We
conduct an experiment on Reddit to demonstrate the effectiveness
of the proposed approach. In Figure 4, we can observe that using
multiple clusters as one batch could improve the convergence. Our
final Cluster-GCN algorithm is presented in Algorithm 1.

3.3 Issues of training deeper GCNs
Previous attempts of training deeper GCNs [9] seem to suggest that
adding more layers is not helpful. However, the datasets used in
the experiments may be too small to make a proper justification.
For example, [9] considered a graph with only a few hundreds of
training nodes for which overfitting can be an issue. Moreover, we
observe that the optimization of deep GCN models becomes difficult
as it may impede the information from the first few layers being
passed through. In [9], they adopt a technique similar to residual

Algorithm 1: Cluster GCN
Input: Graph A, feature X , label Y ;
Output: Node representation ¯X

1 Partition graph nodes into c clusters V1, V2, · · · , Vc by

METIS;

2 for iter = 1, · · · , max_iter do
3

Randomly choose q clusters, t1, · · · , tq from V without
replacement;
Form the subgraph ¯G with nodes ¯V = [Vt1 , Vt2 , · · · , Vtq ]
and links A ¯V, ¯V
Compute д ← ∇LA ¯V, ¯V
Conduct Adam update using gradient estimator д

(loss on the subgraph A ¯V, ¯V

) ;

;

4

5

6

7 Output: {Wl }L

l =1

connections [6] to enable the model to carry the information from
a previous layer to a next layer. Specifically, they modify (1) to add
the hidden representations of layer l into the next layer.

(8)

X (l +1) = σ (A′X (l )W (l )) + X (l )
Here we propose another simple technique to improve the training
of deep GCNs. In the original GCN settings, each node aggregates
the representation of its neighbors from the previous layer. How-
ever, under the setting of deep GCNs, the strategy may not be
suitable as it does not take the number of layers into account. In-
tuitively, neighbors nearby should contribute more than distant
nodes. We thus propose a technique to better address this issue.
The idea is to amplify the diagonal parts of the adjacency matrix A
used in each GCN layer. In this way, we are putting more weights
on the representation from the previous layer in the aggregation of
each GCN layer. An example is to add an identity to ¯A as follows.
X (l +1) = σ ((A′ + I )X (l )W (l ))
(9)
While (9) seems to be reasonable, using the same weight for all the
nodes regardless of their numbers of neighbors may not be suitable.
Moreover, it may suffer from numerical instability as values can
grow exponentially when more layers are used. Hence we propose
a modified version of (9) to better maintain the neighborhoods
information and numerical ranges. We first add an identity to the
original A and perform the normalization

˜A = (D + I )−1(A + I ),

(10)

and then consider

X (l +1) = σ (( ˜A + λdiag( ˜A))X (l )W (l )).
Experimental results of adopting the “diagonal enhancement” tech-
niques are presented in Section 4.3 where we show that this new
normalization strategy can help to build deep GCN and achieve
SOTA performance.

(11)

4 EXPERIMENTS
We evaluate our proposed method for training GCN on two tasks:
multi-label and multi-class classification on four public datasets.
The statistic of the data sets are shown in Table 3. Note that the
Reddit dataset is the largest public dataset we have seen so far for

Task
Datasets
multi-label
PPI
multi-class
Reddit
multi-label
Amazon
Amazon2M multi-class

Table 3: Data statistics
#Edges
818,716
11,606,919
925,872
61,859,140

#Nodes
56,944
232,965
334,863
2,449,029

#Labels
121
41
58
47

#Features
50
602
N/A
100

Table 4: The parameters used in the experiments.
Datasets
PPI
Reddit
Amazon
Amazon2M

#clusters per batch
1
20
1
10

#hidden units
512
128
128
400

# partitions
50
1500
200
15000

GCN, and the Amazon2M dataset is collected by ourselves and is
much larger than Reddit (see more details in Section 4.2).

We include the following state-of-the-art GCN training algo-

rithms in our comparisons:
• Cluster-GCN (Our proposed algorithm): the proposed fast GCN

training method.

• VRGCN4 [2]: It maintains the historical embedding of all the
nodes in the graph and expands to only a few neighbors to
speedup training. The number of sampled neighbors is set to be
2 as suggested in [2]5.

• GraphSAGE6 [5]: It samples a fixed number of neighbors per
node. We use the default settings of sampled sizes for each layer
(S1 = 25, S2 = 10) in GraphSAGE.

We implement our method in PyTorch [13]. For the other methods,
we use all the original papers’ code from their github pages. Since
[9] has difficulty to scale to large graphs, we do not compare with
it here. Also as shown in [2] that VRGCN is faster than FastGCN,
so we do not compare with FastGCN here. For all the methods we
use the Adam optimizer with learning rate as 0.01, dropout rate as
20%, weight decay as zero. The mean aggregator proposed by [5] is
adopted and the number of hidden units is the same for all methods.
Note that techniques such as (11) is not considered here. In each
experiment, we consider the same GCN architecture for all methods.
For VRGCN and GraphSAGE, we follow the settings provided by
the original papers and set the batch sizes as 512. For Cluster-GCN,
the number of partitions and clusters per batch for each dataset
are listed in Table 4. Note that clustering is seen as a preprocessing
step and its running time is not taken into account in training.
In Section 6, we show that graph clustering only takes a small
portion of preprocessing time. All the experiments are conducted
on a machine with a NVIDIA Tesla V100 GPU (16 GB memory),
20-core Intel Xeon CPU (2.20 GHz), and 192 GB of RAM.

4.1 Training Performance for median size

datasets

Training Time vs Accuracy: First we compare our proposed
method with other methods in terms of training speed. In Figure 6,
the x-axis shows the training time in seconds, and y-axis shows the
accuracy (F1 score) on the validation sets. We plot the training time

4GitHub link: https://github.com/thu-ml/stochastic_gcn
5Note that we also tried the default sample size 20 in VRGCN package but it performs
much worse than sample size= 2.
6GitHub link: https://github.com/williamleif/GraphSAGE

Table 5: Comparisons of memory usages on different datasets. Numbers in the brackets indicate the size of hidden units used
in the model.

4-layer
3-layer
2-layer
VRGCN Cluster-GCN GraphSAGE
VRGCN Cluster-GCN GraphSAGE
VRGCN Cluster-GCN GraphSAGE
85 MB
522 MB
71 MB
373 MB
51 MB
258 MB
1076 MB
1075 MB
1074 MB
259 MB
515 MB
372 MB
1131 MB
1115 MB 2064 MB
1099 MB 1491 MB
1031 MB
N/A
N/A 1515 MB
N/A 1351 MB
1188 MB

55 MB
285 MB
308 MB
705 MB

46 MB
285 MB
300 MB
704 MB

39 MB
284 MB
292 MB
703 MB

PPI (512)
Reddit (128)
Reddit (512)
Amazon (128)

Table 6: Benchmarking on the Sparse Tensor operations in
PyTorch and TensorFlow. A network with two linear layers
is used and the timing includes forward and backward oper-
ations. Numbers in the brackets indicate the size of hidden
units in the first layer. Amazon data is used.

Avg. time per epoch (128)
Avg. time per epoch (512)

PyTorch TensorFlow
2.53s
7.13s

8.81s
45.08s

versus accuracy for three datasets with 2,3,4 layers of GCN. Since
GraphSAGE is slower than VRGCN and our method, the curves for
GraphSAGE only appear for PPI and Reddit datasets. We can see
that our method is the fastest for both PPI and Reddit datasets for
GCNs with different numbers of layers.

For Amazon data, since nodes’ features are not available, an iden-
tity matrix is used as the feature matrix X . Under this setting, the
shape of parameter matrix W (0) becomes 334863x128. Therefore,
the computation is dominated by sparse matrix operations such as
AW (0). Our method is still faster than VRGCN for 3-layer case, but
slower for 2-layer and 4-layer ones. The reason may come from
the speed of sparse matrix operations from different frameworks.
VRGCN is implemented in TensorFlow, while Cluster-GCN is im-
plemented in PyTorch whose sparse tensor support are still in its
very early stage. In Table 6, we show the time for TensorFlow and
PyTorch to do forward/backward operations on Amazon data, and
a simple two-layer network are used for benchmarking both frame-
works. We can clearly see that TensorFlow is faster than PyTorch.
The difference is more significant when the number of hidden units
increases. This may explain why Cluster-GCN has longer training
time in Amazon dataset.

Memory usage comparison: For training large-scale GCNs,
besides training time, memory usage needed for training is of-
ten more important and will directly restrict the scalability. The
memory usage includes the memory needed for training the GCN
for many epochs. As discussed in Section 3, to speedup training,
VRGCN needs to save historical embeddings during training, so it
needs much more memory for training than Cluster-GCN. Graph-
SAGE also has higher memory requirement than Cluster-GCN due
to the exponential neighborhood growing problem. In Table 5, we
compare our memory usage with VRGCN’s memory usage for
GCN with different layers. When increasing the number of layers,
Cluster-GCN’s memory usage does not increase a lot. The reason
is that when increasing one layer, the extra variable introduced is
the weight matrix W (L), which is relatively small comparing to the
sub-graph and node features. While VRGCN needs to save each
layer’s history embeddings, and the embeddings are usually dense
and will soon dominate the memory usage. We can see from Table 5

Table 7: The most common categories in Amazon2M.

Categories
Books
CDs & Vinyl
Toys & Games

number of products
668,950
172,199
158,771

that Cluster-GCN is much more memory efficient than VRGCN. For
instance, on Reddit data to train a 4-layer GCN with hidden dimen-
sion to be 512, VRGCN needs 2064MB memory, while Cluster-GCN
only uses 308MB memory.

4.2 Experimental results on Amazon2M
A new GCN dataset: Amazon2M. By far the largest public data
for testing GCN is Reddit dataset with the statistics shown in Table
3, which contains about 200K nodes. As shown in Figure 6 GCN
training on this data can be finished within a few hundreds seconds.
To test the scalability of GCN training algorithms, we constructed
a much larger graph with over 2 millions of nodes and 61 million
edges based on Amazon co-purchasing networks [11, 12]. The raw
co-purchase data is from Amazon-3M7. In the graph, each node is
a product, and the graph link represents whether two products are
purchased together. Each node feature is generated by extracting
bag-of-word features from the product descriptions followed by
Principal Component Analysis [7] to reduce the dimension to be
100. In addition, we use the top-level categories as the labels for
that product/node (see Table 7 for the most common categories).
The detailed statistics of the data set are listed in Table 3.

In Table 8, we compare with VRGCN for GCNs with a different
number of layers in terms of training time, memory usage, and test
accuracy (F1 score). As can be seen from the table that 1) VRGCN is
faster than Cluster-GCN with 2-layer GCN but slower than Cluster-
GCN when increasing one layer while achieving similar accuracy.
2) In terms of memory usage, VRGCN is using much more memory
than Cluster-GCN (5 times more for 3-layer case), and it is running
out of memory when training 4-layer GCN, while Cluster-GCN does
not need much additional memory when increasing the number of
layers, and achieves the best accuracy for this data when training a
4-layer GCN.

4.3 Training Deeper GCN
In this section we consider GCNs with more layers. We first show
the timing comparisons of Cluster-GCN and VRGCN in Table 9. PPI
is used for benchmarking and we run 200 epochs for both methods.
We observe that the running time of VRGCN grows exponentially

7http://manikvarma.org/downloads/XC/XMLRepository.html

Table 8: Comparisons of running time, memory and testing accuracy (F1 score) for Amazon2M.

Amazon2M (2-layer)
Amazon2M (3-layer)
Amazon2M (4-layer)

Time
VRGCN Cluster-GCN
1223s
1523s
2289s

337s
1961s
N/A

Figure 5: Convergence figure on a 8-layer GCN. We present
numbers of epochs (x-axis) versus validation accuracy (y-
axis). All methods except for the one using (11) fail to con-
verge.

Table 9: Comparisons of running time when using different
numbers of GCN layers. We use PPI and run both methods
for 200 epochs.

Cluster-GCN
VRGCN

2-layer
52.9s
103.6s

3-layer
82.5s
229.0s

4-layer
109.4s
521.2s

5-layer
137.8s
1054s

6-layer
157.3s
1956s

because of its expensive neighborhood finding, while the running
time of Cluster-GCN only grows linearly.

Next we investigate whether using deeper GCNs obtains better
accuracy. In Section 4.3, we discuss different strategies of modifying
the adjacency matrix A to facilitate the training of deep GCNs. We
apply the diagonal enhancement techniques to deep GCNs and run
experiments on PPI. Results are shown in Table 11. For the case
of 2 to 5 layers, the accuracy of all methods increases with more
layers added, suggesting that deeper GCNs may be useful. However,
when 7 or 8 GCN layers are used, the first three methods fail to
converge within 200 epochs and get a dramatic loss of accuracy. A
possible reason is that the optimization for deeper GCNs becomes
more difficult. We show a detailed convergence of a 8-layer GCN
in Figure 5. With the proposed diagonal enhancement technique
(11), the convergence can be improved significantly and similar
accuracy can be achieved.

State-of-the-art results by training deeper GCNs. With the
design of Cluster-GCN and the proposed normalization approach,
we now have the ability for training much deeper GCNs to achieve
better accuracy (F1 score). We compare the testing accuracy with
other existing methods in Table 10. For PPI, Cluster-GCN can
achieve the state-of-art result by training a 5-layer GCN with 2048
hidden units. For Reddit, a 4-layer GCN with 128 hidden units is
used.

Memory

Test F1 score

VRGCN Cluster-GCN VRGCN Cluster-GCN
89.00
7476 MB
90.21
11218 MB
90.41
OOM

2228 MB
2235 MB
2241 MB

89.03
90.21
N/A

Table 10: State-of-the-art performance of testing accuracy
reported in recent papers.

FastGCN [1]
GraphSAGE [5]
VR-GCN [2]
GaAN [16]
GAT [14]
GeniePath [10]
Cluster-GCN

PPI
N/A
61.2
97.8
98.71
97.3
98.5
99.36

Reddit
93.7
95.4
96.3
96.36
N/A
N/A
96.60

5 CONCLUSION
We present ClusterGCN, a new GCN training algorithm that is fast
and memory efficient. Experimental results show that this method
can train very deep GCN on large-scale graph, for instance on a
graph with over 2 million nodes, the training time is less than an
hour using around 2G memory and achieves accuracy of 90.41 (F1
score). Using the proposed approach, we are able to successfully
train much deeper GCNs, which achieve state-of-the-art test F1
score on PPI and Reddit datasets.

Acknowledgement CJH acknowledges the support of NSF via

IIS-1719097, Intel faculty award, Google Cloud and Nvidia.

REFERENCES
[1] Jie Chen, Tengfei Ma, and Cao Xiao. 2018. FastGCN: Fast Learning with Graph

Convolutional Networks via Importance Sampling. In ICLR.

[2] Jianfei Chen, Jun Zhu, and Song Le. 2018. Stochastic Training of Graph Convolu-

tional Networks with Variance Reduction. In ICML.

[3] Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, and Le Song. 2018. Learning

Steady-States of Iterative Algorithms over Graphs. In ICML. 1114–1122.

[4] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted Graph Cuts
Without Eigenvectors A Multilevel Approach. IEEE Trans. Pattern Anal. Mach.
Intell. 29, 11 (2007), 1944–1957.

[5] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation

Learning on Large Graphs. In NIPS.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual

Learning for Image Recognition. CVPR (2016), 770–778.

[7] H. Hotelling. 1933. Analysis of a complex of statistical variables into principal

components. Journal of Educational Psychology 24, 6 (1933), 417–441.

[8] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme
for partitioning irregular graphs. SIAM J. Sci. Comput. 20, 1 (1998), 359–392.
[9] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with

Graph Convolutional Networks. In ICLR.

[10] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan
Qi. 2019. GeniePath: Graph Neural Networks with Adaptive Receptive Paths. In
AAAI.

[11] Julian McAuley, Rahul Pandey, and Jure Leskovec. 2015. Inferring Networks of

Substitutable and Complementary Products. In KDD.

[12] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel.
2015. Image-Based Recommendations on Styles and Substitutes. In SIGIR.
[13] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in PyTorch. In NIPS-W.

[14] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro

Liò, and Yoshua Bengio. 2018. Graph Attention Networks. (2018).

[15] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,
and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale
Recommender Systems. In KDD.

Table 11: Comparisons of using different diagonal enhancement techniques. For all methods, we present the best validation
accuracy achieved in 200 epochs. PPI is used and dropout rate is 0.1 in this experiment. Other settings are the same as in
Section 4.1. The numbers marked red indicate poor convergence.
3-layer
97.6
97.7
96.0
97.5

Cluster-GCN with (1)
Cluster-GCN with (10)
Cluster-GCN with (10) + (9)
Cluster-GCN with (10) + (11), λ = 1

5-layer
98.3
98.4
97.6
98.3

4-layer
98.2
98.1
97.1
98.2

8-layer
43.1
42.4
43.8
96.2

6-layer
94.1
42.4
97.3
98.0

7-layer
65.4
42.4
43.9
97.4

2-layer
90.3
90.2
84.9
89.6

(a) PPI (2 layers)

(b) PPI (3 layers)

(c) PPI (4 layers)

(d) Reddit (2 layers)

(e) Reddit (3 layers)

(f) Reddit (4 layers)

(g) Amazon (2 layers)

(h) Amazon (3 layers)

(i) Amazon (4 layers)

Figure 6: Comparisons of different GCN training methods. We present the relation between training time in seconds (x-axis)
and the validation F1 score (y-axis).

[16] Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung.
2018. GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal
Graphs. In UAI.

[17] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural

Networks. In NIPS.

Table 12: The training, validation, and test splits used in the
experiments. Note that for the two amazon datasets we only
split into training and test sets.

Table 13: The running time of graph clustering algorithm
(METIS) and data preprocessing before the training of GCN.

Datasets
PPI
Reddit
Amazon
Amazon2M

#Partitions Clustering
1.6s
33s
0.3s
148s

50
1500
200
15000

Preprocessing
20.3s
286s
67.5s
2160s

While some of these preprocessing steps such as data loading or
parsing are shared across different methods, some steps are al-
gorithm specific. For instance, our method needs to run graph
clustering algorithm during the preprocessing stage.

In Table 13, we present more details about preprocessing time
of Cluster-GCN on the four GCN datasets. For graph clustering,
we adopt Metis, which is a fast and scalable graph clustering li-
brary. We observe that the graph clustering algorithm only takes
a small portion of preprocessing time, showing a small extra cost
while applying such algorithms and its scalability on large data sets.
In addition, graph clustering only needs to be conducted once to
form the node partitions, which can be re-used for later training
processes.

Datasets
Inductive
PPI
Inductive
Reddit
Amazon
Inductive
Amazon2M Inductive

Task Data splits (Tr./Val./Te.)
44906/6514/5524
153932/23699/55334
91973/242890
1709997/739032

6 MORE DETAILS ABOUT THE

EXPERIMENTS

In this section we describe more detailed settings about the experi-
ments to help in reproducibility.

6.1 Datasets and software versions
We describe more details about the datasets in Table 12. We down-
load the datasets PPI, Reddit from the website8 and Amazon from
the website9. Note that for Amazon, we consider GCN in an in-
ductive setting, meaning that the model only learns from training
data. In [3] they consider a transductive setting. Regarding software
versions, we install CUDA 10.0 and cuDNN 7.0. TensorFlow 1.12.0
and PyTorch 1.0.0 are used. We download METIS 5.1.0 via the offcial
website10 and use a Python wrapper11 for METIS library.

6.2 Implementation details
Previous works [1, 2] propose to pre-compute the multiplication
of AX in the first GCN layer. We also adopt this strategy in our
implementation. By precomputing AX , we are essentially using
the exact 1-hop neighborhood for each node and the expensive
neighbors searching in the first layer can be saved.

Another implementation detail is about the technique mentioned
in Section 3.2 When multiple clusters are selected, some between-
cluster links are added back. Thus the new combined adjacency
matrix should be re-normalized to maintain numerical ranges of
the resulting embedding matrix. From experiments we find the
renormalization is helpful.

As for the inductive setting, the testing nodes are not visible
during the training process. Thus we construct an adjacency ma-
trix containing only training nodes and another one containing all
nodes. Graph partitioning are applied to the former one and the par-
titioned adjacency matrix is then re-normalized. Note that feature
normalization is also conducted. To calculate the memory usage,
we consider tf.contrib.memory_stats.BytesInUse() for Ten-
sorFlow and torch.cuda.memory_allocated() for PyTorch.

6.3 The running time of graph clustering
algorithm and data preprocessing

The experiments of comparing different GCN training methods in
Section 4 consider running time for training. The preprocessing
time for each method is not presented in the tables and figures.

8http://snap.stanford.edu/graphsage/
9https://github.com/Hanjun-Dai/steady_state_embedding
10http://glaros.dtc.umn.edu/gkhome/metis/metis/download
11https://metis.readthedocs.io/en/latest/

