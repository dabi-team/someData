FaceEraser: Removing Facial Parts for Augmented Reality

Miao Hua*

Lijie Liu*

Ziyang Cheng*

Qian He

Bingchuan Li

Zili Yi†

Bytedance Inc.
{huamiao,liulijie.gxz,chengziyang.yy,heqian,libingchuan,yizili}@bytedance.com

1
2
0
2

t
c
O
2
2

]

V
C
.
s
c
[

2
v
0
6
7
0
1
.
9
0
1
2
:
v
i
X
r
a

Abstract

Our task is to remove all facial parts (e.g., eyebrows,
eyes, mouth and nose), and then impose visual elements onto
the “blank” face for augmented reality. Conventional ob-
ject removal methods rely on image inpainting techniques
(e.g., EdgeConnect[16], HiFill[24]) that are trained in a
self-supervised manner with randomly manipulated image
pairs. Speciﬁcally, given a set of natural images, randomly
masked images are used as inputs and the raw images are
treated as ground truths. Whereas, this technique does not
satisfy the requirements of facial parts removal, as it is hard
to obtain “ground-truth” images with real “blank” faces.
Simple techniques such as color averaging or PatchMatch
[2] fail to assure texture or color coherency. To address this
issue, we propose a novel data generation technique to pro-
duce paired training data that well mimic the “blank” faces.
In the mean time, we propose a novel network architecture
for improved inpainting quality for our task. Finally, we
demonstrate various face-oriented augmented reality appli-
cations on top of our facial parts removal model. The source
codes are released at duxingren14/FaceEraser on github for
research purposes.

1. Introduction

Augmented reality applications are becoming increas-
ingly popular among users due to the possibility of com-
bining real-world elements with virtual ones. Speciﬁcally,
face-oriented augmented reality is widely used in pictures
and videos on social media platforms. In some applications
the face is augmented by directly placing virtual elements
(e.g., cat whisker, sun glasses, virtual eyebrows or make-
ups) onto it, while some other applications require facial
parts (e.g., eyebrows, nose, mouth) to be removed before
imposing virtual ones: see Figure 1 (b). In this paper, we are
focusing on automatic facial parts removal for added value
to face-oriented augmented reality applications.

We propose to remove facial parts and obtain a “blank”

*C0-ﬁrst author; †Corresponding author

Figure 1. (a) The difference between face completing and face
erasing. Note that existing models are mostly trained to complete
a face rather than erase a face. (b) Exemplar applications of face-
oriented augmented reality that require face erasing. Please zoom
in for higher resolution.

face. However, existing image inpainting methods [16, 24,
28, 12] attempt to auto-complete facial parts rather than
removing them, which is limited to the conventional way
that generates training pairs by randomly masking real faces
and learns to complete masked pixels: see Figure 1 (a). A
more elaborate investigation implies that there is a lack of
research into problems of this type.

As one of the earliest studies into this problem, we pro-
pose to solve the problem of facial parts removal using
a novel data preparation technique. We still follow the
paradigm of image inpainting models, i.e., given a facial
image and a mask indicating the segments of facial parts to
be removed, a model is trained to generate a “blank” face

 
 
 
 
 
 
with all facial parts removed. The novel data preparation
method enables us to produce a number of images that well
simulate the “blank”’ faces we want. With these “blank”
faces, we can then generate training pairs by randomly mask-
ing the images. In addition to the data preparation method,
we also elaborate a novel network architecture speciﬁcally
for the task of facial parts removal which exhibits improved
quality over existing inpainting models. With the inpainting
model trained, we can then process facial parts removal upon
real faces, and demonstrate various interesting augmented
reality applications by superposing manipulated or virtual
elements onto the “blank” faces.

The contributions of this paper include:

al. [27] introduce a novel contextual attention layer that en-
ables borrowing features from distant spatial locations. Song
et al. [22] employ the patch-swap layer that propagates the
high-frequency texture details from the boundaries to hole
regions. Ren et al. [18] introduce the novel appearance ﬂow
to warp features from surroundings to the hole region with
the expectation to yield better image details.

Similarly, we adopt the “patch copying” idea in our model.
Whereas, different from these methods that copy patches
from outside-hole regions at the feature level, which is prone
to exert “grid patterns” or “boundary artifacts”, we are copy-
ing at the pixel level. Unlike Yu et al. [27] that soft-blend
multiple nearest neighbors, we only copy one nearest neigh-
bor, which turns out to generate sharper results.

• we propose a novel method to prepare the training data

for the task of facial parts removal.

2.2. Facial analysis

• we design a novel network architecture that introduce
the mechanism similar to patch-copying-based inpaint-
ing methods[2, 5, 4], which we call “pixel-clone”, into
neural inpainting models for improved inpainting qual-
ity in our task, effectively resolving the color or tex-
ture inconsistency issue observed in existing inpainting
methods (e.g., EdgeConnect [16] and StructureFlow
[18]).

We use a face detection model to discover faces from wild
images. 106 facial key points [13] are detected and used to
align faces, locate facial parts and generate facial masks,
which facilitates the procedure of data preparation, inference
and augmented reality applications. We use facial attributes
analyzers to predict the existence of glasses and hats. We
employ glasses segmentation and hair segmentation models
to acquire masks of glasses and hair.

• We implement and demonstrate various augmented re-
ality applications enabled by our facial parts removal
model.

3. Method

3.1. Overall pipeline

2. Related work

2.1. Image inpainting

Image inpainting has been an active research area for the
past few decades. Existing methods for image hole ﬁlling are
either rule-based (e.g., Patch-based [5, 4], diffusion-based [3,
1]) or learning-based (Contextual Attention [27, 28], HiFill
[24], Partial Convolution [12], EdgeConnect [16], Structure-
Flow [18]).

Rule-based methods [5, 4, 3, 1] attempt to ﬁll the hole by
extending and borrowing real patches from surrounding re-
gions, whereas they lack awareness of the semantic structure
of contents. Learning-based methods [27, 28, 12, 24, 16]
hallucinate missing pixels in a data-driven manner with the
use of large external databases. These approaches learn to
model the distribution of the training images and assume
that regions surrounded by similar contexts likely to pos-
sess similar contents. These methods are more content-
aware, but sometimes suffer inconsistency of colors or lack
of ﬁne-grained textures. Some researchers attempt to com-
bine the two streams of research, explicitly introducing the
“patch copying” mechanism into neural networks. These
approaches include Contextual Attention [27, 28, 24], Struc-
tureFlow [18], Patch-Swap layer [22]. Speciﬁcally, Yu et

We propose an image inpainting model consisting of
three sub-networks: Edge Completion Network Gedge, Pixel-
Clone Network Gpc and Reﬁne Network Gre f (Figure 2).
The Edge Completion Network is an edge generator that
hallucinates edges of the missing region of the image, the
Pixel-Clone Network predicts the nearest neighbors of miss-
ing pixels from the visible contents with the guidance of
hallucinated edges, and the Reﬁne Network generates the
ﬁnal result using the nearest neighbors as a priori. With
such design, we expect the hallucinated edges by Gedge can
provide some hints to the “pixel-cloning” mechanism of Gpc
so that pixels be copied from the correct region.

Speciﬁcally, given an incomplete input image I and a
mask M with value 1 indicating the hole region and 0 indi-
cating the visible region. Pixel values of the input image are
scaled to the range of [0, 1] before being processed. The Edge
Completion Network takes the grayscale version of the input
image Igray, mask M and the masked edge map E as inputs,
and generates a completed edge map ˆE = Gedge(Igray, E, M).
The masked edge map here is extracted from Igray using
canny algorithm with Gaussian width of σ = 2.0, low thresh-
old of 10% and high threshold of 20% [16].

The Pixel-Clone Network Gpc that searches the nearest
neighbor from the visible region of I for each in-hole pix-
els. The output of Pixel-Clone Network is a two-channel

Figure 2. The overall process of our face erasing model .

Figure 3. The procedure of training image preparation. (a) raw
facial image. (b) forehead image cropped from (a) based on key
points. (c) vertical ﬂip of (b). (d) stitching (b) and (c)

ﬂow map of the same size of I, indicating the x-y offsets of
the nearest neighbor of each pixel, F = Gpc( ˆE, I, M). With
the predicted offsets F, we conduct image warping upon
I at the image-level and obtain a coarse inpainting result,
Warp(I, F). Finally, the Reﬁne Network Gre f reﬁnes from
the coarse result and generates ˆI = Gre f (Warp(I, F), I, F).

3.2. Data preparation

Training image preparation Our task is to inpaint a
masked face with skin textures and obtain a “blank” face:
see Figure 1. As there are no real “blank” faces in the world,
to make the training of image inpainting models possible,
we need to generate a number of “blank” faces that well
simulate what we desire. The procedure of training image
preparation is illustrated in Figure 3.

Given a set of real face images, we remove images coming
with glasses and hats, and those whose foreheads are largely
covered by hair. After the ﬁltering, we detect the key points
of the face, and crop the forehead region of each face above
the eyebrow. We then resize the forehead image to 256×128.

Figure 4. The procedure of mask generation. (a) raw image. (b)
106 key points detected from (a). (c) initial mask obtained by con-
necting key points in (b). (d) mask dilated from (c). (e) real glasses
mask estimated with glasses segmentation model. (f) merging (e)
and (d).

Finally, we vertically ﬂip the forehead image and stitch it
with the original forehead image to acquire a 256×256 image
that looks like a “blank” face: see Figure 3.

Mask preparation The masks we used for training should
serve the task of facial part removal. We acquire the training
masks from real faces. The procedure is shown in Figure 4.
We ﬁrst extract the 106 key points from pre-aligned faces.
We then infer the masks of facial parts by connecting the key
points. Finally, we dilate the masks to assure covering of
shadows and peripheral regions. In some cases, the existence
of glasses should also be taken into account. Therefore, we
augment a fraction of masks with glasses masks. The glasses
masks are obtained from real images with an internal glasses
segmentation model.

3.3. Network architecture

3.4. Training method

Our model is comprised of three sub-networks, Edge
Completion Network Gedge, Pixel-Clone Network Gpc, and
Reﬁne Network Gre f . The Edge Completion Network
Gedge follows an encoder-decoder architecture similar to
the method proposed by Nazeri et al. [16], which proves
to be effective in connecting corrupted edges. Speciﬁcally,
the generator consists of encoders that down-sample twice,
followed by eight residual blocks [16] and decoders that
upsample images back to the original size. Dilated convolu-
tions [26] with a dilation factor of two are used in the residual
layers. Instance normalization and ReLU are used for all
convolutions except the last convolution which is activated
with Sigmoid to scale pixel values to range of (0, 1).

The Pixel-Clone Network Gpc follows almost identical
encoder-decoder architecture as the Edge Completion Net-
work except that the last activation layer is replaced with
tanh to scale output values to (−1, 1). The number of input
and output channels also differ. Gpc predicts a dense offset
map F, indicating the x-y offsets of the nearest neighbor of
each pixel. Speciﬁcally, it is a two-channel map of the same
size of the input image.

The coarse inpainting result is obtained by warping the
input image I with offset map F, i.e., ˆI = Warp(I, F). This
can be done with the grid sample operation in PyTorch,
which is differentiable over both inputs.

We utilize a U-Net architecture for the Reﬁne Network
Gre f . The Reﬁne Network is composed of 5 downsam-
pling residual blocks followed 5 upsampling residual blocks.
The skip connections are joined through addition instead of
concatenation, which proves to be more memory-efﬁcient.
We use channel attention [29] in each residual block, as
it proves to add improvements for low-level tasks such as
super-resolution, as it adaptively rescales channel-wise fea-
tures by considering interdependencies among channels. We
verify that the introduction of channel attention drives im-
provements to the inpainting quality.

Discriminators We need two discriminators for adversar-
ial training of the generator, respectively for the training
of the Edge Completion Network and the joint training of
Patch-Clone Network and Reﬁne Network. For both dis-
criminators, we use the same PatchGAN [7, 25] architecture
that rates the realness of 70×70 patches [7, 16] rather than
the entire image. We use spectral normalization [14] on dis-
criminators to stabilize the training by scaling down weight
matrices by their respective largest singular values. We use
leaky ReLU as activations except for the last layer that uses
Sigmoid.

Given the incomplete input image I, mask M and the
ground-truth image Igt , the Edge Completion Network pre-
dicts a completed edge map ˆE = Gedge(Igray, E, M), where
Igray is the grayscale version of the input image I and E is
the incomplete edge map of Igray. Similar to [16], we em-
ploy a two-phase training procedure. In Phase 1, the Edge
Completion Network is trained independently. After the
full convergence of the Edge Completion Network Gedge,
we then train the Pixel-Clone Network and Reﬁne Network
jointly while holding the weights of Gedge ﬁxed. We did
not ﬁnd joint training or ﬁnetuning of all three sub-networks
brings any signiﬁcant quality improvements.

The Edge Completion Network is trained with an ob-
jective comprised of an adversarial loss [6] and feature-
matching loss [23], which is written as

min
Gedge

max
Dedge

Ledge = min
Gedge

α edge
adv max
Dedge

Ladv + αFMLFM

(1)

where α edge

adv and αFM are the coefﬁcients of the adver-
sarial loss Ladv and feature matching loss LFM respectively.
The adversarial loss Ladv is deﬁned as

Ledge
adv = E
Igt ,M

(cid:104)
log Dedge(Egt , Igray

gt

) + log(1 − Dedge( ˆE, Igray

gt

(cid:105)
))

(2)

where Igray

gt

and Egt are the grayscale version and the edge
map of the ground truth image Igt . The feature-matching loss
LFM compares the activations of the fake and the real edge
maps in the intermediate layers of the discriminator, and
forces the generator to produce results with representations
that are similar to the real edge maps. The feature matching
loss is written as

LFM = E
Igt ,M

(cid:34) L
∑
i=1

1
Ni

(cid:13)
(cid:13)Dedge
(cid:13)

i

( ˆE, Igray
gt

) − Dedge
i

(Egt , Igray

gt

(cid:35)

(cid:13)
(cid:13)
)
(cid:13)1
(3)

i

where L is the number of convolutional layers of the
discriminator, Dedge
is the output of the i -th activation layer
of the discriminator and Ni is the number of elements in
the i-th activation layer. We use L1 norm to encourage less
blurring [7]. In our experiments, we choose α edge
adv = 1 and
αFM = 10.

The Pixel-Clone Network Gpc that searches the pixels
from the visible region to ﬁll in-hole pixels, is one of our key
contributions. The output of Pixel-Clone Network is a two-
channel offset map indicating the x-y offsets of the nearest
neighbor of each pixel, which is denoted as F = Gpc( ˆE, I, M).

The coarse inpainting result is obtained by warping the input,
i.e. Warp(I, F). The Reﬁne Network Gre f generate a reﬁned
result from the coarse, namely ˆI = Gre f (Warp(I, F), I, F).
The Pixel-Clone Network Gpc and Reﬁne Network Gre f

are trained jointly with loss Lpc,re f , which is deﬁned as

Lpc,re f = αadvLpc,re f

adv + αpercLperc + αL1LL1 + αstylLstyl + αpcLpc

(4)

where Lpc,re f

adv

, Lperc, LL1, Lstyl and Lpc are the adversarial
loss, perceptual loss, L1 loss, style loss and Pixel-Clone loss
term respectively, and αadv, αperc, αL1, αstyl and αpc are the
corresponding coefﬁcients. The adversarial loss is deﬁned as

Lpc,re f
adv = E
Igt ,M

(cid:2)logDpc,re f (Igt , ˆE) + log(1 − Dpc,re f ( ˆI, ˆE))(cid:3)

(5)

We adopt the perceptual loss Lperc and style loss Lstyl
proposed in [8] . Lperc penalizes the perceptual disparities
of the fake to the ground-truth by measuring the distance
between the activations of a pre-trained network, which is
deﬁned as

Lperc = E
Igt ,M

(cid:34) L
∑
i=1

1
Ni

(cid:35)

(cid:13)φi( ˆI) − φi(Igt )(cid:13)
(cid:13)
(cid:13)1

(6)

where φi is the output of i-th activation layer of a pre-
trained network. In our method, φi corresponds to the output
of activation layers relu11, relu21, relu31, relu41 and relu51
of the VGG-19 network [21] pre-trained on the ImageNet
dataset [20]. These activation maps are also used to compute
style loss which is deﬁned as

Lstyl = E
Igt ,M

(cid:34) L
∑
i=1

(cid:13)
(cid:13)Gφ
(cid:13)

i ( ˆI) − Gφ

(cid:13)
(cid:13)
i (Igt )
(cid:13)1

(cid:35)

(7)

where Gφ

i (·) is the Ci × Ci Gram matrix [8] computed
from i-th activation maps φi. Style loss measures the differ-
ences between covariance of the features.

The L1 loss and Pixel-Clone loss Lpc are computed as

LL1 = E
Igt ,M

(cid:13)
(cid:13) ˆI − Igt

(cid:13)
(cid:13)1

Lpc = E
Igt ,M

(cid:13)
(cid:13)WarP(I, F) − Igt

(cid:13)
(cid:13)1

(8)

(9)

We choose αadv = 0.1, αperc = 1.0, αL1 = 1.0, αstyl =

500 and αpc = 1.0 in our experiments.

3.5. Face-oriented augmented reality

On top of our face erasing model, we implement vari-
ous face-oriented augmented reality applications including
mono-eye, comic, small-face, toonized, eyebrowless effects:
see Figure 1 (b), Figure 2. For these effects, we follow the
same procedure that all facial parts are removed ﬁrst and then
certain elements are pasted back to the “blank” face. Poi-
son blending [17] is used to avoid boundary artifacts when
merging the elements with the face. Speciﬁcally, the comic
effect involves enlarged mouth, shrunk eyes and proper ad-
justment of positions of the facial parts. As for eyebrowless
effect, we simply paste back all the facial parts except for
the eyebrows. As for the toonized effect, we paste back
the eyebrows, enlarged eyes and mouth, shrunk nose and
properly adjust the positions of facial parts. As for the small
face effect, we simply shrink the face and paste it back. As
for the mono-eye effect, we paste back the mouth and nose,
and place one eye right above the nose.

In addition to the applications mentioned above, the
erased faces enable various augmented-reality applications
that require placement of any user-customized elements.

4. Experimental results

4.1. Training setup and strategies

Training images and masks are obtained from FFHQ
dataset [10] by following the procedure as described in Sec-
tion 3.2. We choose FFHQ dataset as it contains considerable
variation in terms of age, ethnicity, poses and illumination
conditions, and it has good coverage of accessories such as
eyeglasses, sunglasses and hats. We obtain about 35,000
images and 10,000 masks for training, and 4000 images and
1000 masks for validation. Our model can do inference on
images from VoxCeleb [15], CelebA-HQ [9], unseen faces
from FFHQ or any other unconstrained faces. The network
is trained using 256 × 256 images with a batch size of 8.
The model is optimized using Adam optimizer [11] with β1
= 0.0 and β2 = 0.9. All generators are trained with learning
rate 10−4, and discriminators are trained with learning rate
of 10−5.

The model is implemented in PyTorch. Each model is

trained with one Tesla V100 GPU for 2000,000 epochs.

Figure 5 illustrates the intermediate and ﬁnal results of our
face erasing results on the validation and test splits. It can be
observed that the completed edge map ˆE faithfully enforces
the copying of pixels from the valid regions. The offset map
F accurately matches the nearest neighbor for a majority of
“in-hole” pixels except for the pixels in the “crack”. The
Reﬁne Network effectively eliminates the visible artifacts in
the coarse results and ﬁlls the “crack” with plausible contents
and textures.

Figure 5. Intermediate and ﬁnal outputs of our method. Two uppers rows are results on the validation data (fake “blank” faces), and two
lower rows are inference results obtained on real faces.

Table 1. Quantitative evaluations on existing methods and dif-
ferent variations of our method. Here is the notations. Edge:
Edge Completion Network [16]. PC: Pixel-Clone Network. UNet:
Encoder-Decoder architecture with skip connections [19]. AE:
Auto-Encoder without skip connections. CA: Channel Attention
mechanism [29].

HiFill[22]
StructureFlow[18]
EdgeConnect (Edge+AE)[16]
Ours (Edge+PC+AE)
Ours (Edge+PC+UNet)
Ours (Edge+PC+UNet+CA)

PSNR
28.24 ± 0.20
29.62 ± 0.23
32.13 ± 0.25
33.01 ± 0.25
33.11 ± 0.27
33.17 ± 0.29

MAE
0.0253 ± 0.0035
0.0247 ± 0.0031
0.0231 ± 0.0022
0.0226 ± 0.0023
0.0209 ± 0.0019
0.0199 ± 0.0016

4.2. Comparisons

To verify the effectiveness of the proposed network archi-
tectures, we compared our method with the state-of-the-art
image inpainting methods including HiFill [22], Structure-
Flow [18], EdgeConnect [16]. As the ofﬁcial pre-trained
models are trained on either scene inpainting or facial com-
pletion tasks, which does not match our task, we retrained
those models on the training datasets we produced. To assure
fair comparisons, all models are trained for 2000,000 epochs
with the same batch size of 8. We attempt to use the same
experimental setup for all these models though small dispar-
ities cannot be avoided. Speciﬁcally, HiFill only supports

training and inferences on images no smaller than 512×512.
We resize all training samples to 512×512 and do the train-
ing. As for inference, we resize the cropped face to 512×512
to facilitate the inference and then resize it back after ﬁll-
ing the hole. StructureFlow requires structure smoothing
of the input images, we use python implementation of the
structure ﬁltering to replace the matlab implementation. For
EdgeConnect which consists of an Edge Completion Net-
work and Inpainting Network, we train the Edge Network
for 1000,000 epochs and jointly train the two networks for
another 1000,000 epochs.

As shown in Figure 6, we can observe that HiFill utilizes
the contextual attention, soft-blending the nearest patches
from visible regions, tends to generate blurry skin textures,
and sometimes it mistakenly copy patches from the non-
facial areas. StructureFlow can generate plausible contents
to ﬁll the holes. EdgeConnect generally outperforms HiFill
and StructureFlow and generates plausible results. Though
slight color and texture inconsistency can be observed. We
also conducted quantitative evaluations of these methods on
the validation dataset, in which we calculate the PSNR and
mean absolute error (MAE) between the inpainted images
and ground-truths.

4.3. Ablation studies

Figure 6. Comparisons of our method with prior arts on the validation and test data. The upper two rows are the inpainting results on the
validation data (fake “blank” faces produced as in Section 3.2). The bottom two are testing results on real faces.

Table 2. Quantitative evaluations on the models trained with differ-
ent αpc values.

αpc = 0.0
αpc = 0.5
αpc = 0.8
αpc = 1.0
αpc = 1.2
αpc = 1.5
αpc = 2.0

PSNR
31.21 ± 0.25
31.46 ± 0.29
32.61 ± 0.30
33.09 ± 0.28
32.87 ± 0.31
32.56 ± 0.34
32.38 ± 0.32

MAE
0.0263 ± 0.0023
0.0250 ± 0.0025
0.0221 ± 0.0021
0.0202 ± 0.0019
0.0205 ± 0.0019
0.0209 ± 0.0020
0.0214 ± 0.0017

We examine how Pixel-Clone loss term Lpc affects the
performance of our network. Figure 7 shows that the in-
painting results of models trained with different coefﬁcient
values αpc for the Pixel-Clone loss term. As shown in Figure
7, we observe that when αpc <= 0.5, the offset map F has
constant value and does not effectively match the nearest
neighbors, thus generating unsatisfactory inpainting results ˆI.
When αpc = 2.0, the “white dot” artifacts become visible in
ˆI. The quantitative evaluations further verify the conclusion
that αpc = 1.0 is close to the optimal choice.

Further, we did a quantitative evaluation of our methods

Figure 7. Comparisons of inference results of models trained with
different αpc values by providing the same input as in the topleft.
From top to bottom are F, Warp(I, F) and ˆI respectively.

in terms of different variations of the Reﬁne Network. As
the original EdgeConnect employs Edge Completion Net-
work and Inpainting Network. We replace their Inpainting
Network with a Pixel-Clone Network plus a Reﬁne Network.
In terms of the architecture of Reﬁne Network, we experi-

Figure 8. Exemplar applications supported by our FaceEraser.

mented a few variations including AE and UNet. As shown
in Table 1, the UNet [19] architecture outperforms Auto-
Encoder (AE) in our experimental setting, while Channel
Attention (CA) [29] brings noticeable quality gain.

4.4. Applications

With the facial parts removed, we can then place a sub-
set of manipulated facial parts or virtual elements on to the
“blank” face to achieve interesting and enjoyable special
effects.The augmented reality applications (e.g, mono-eye,
small-face, eyebrowless, comic and toonized) are imple-
mented and experimented on the test split of FFHQ dataset.
The results are demonstrated in Figure 8. The test results
on the CelebA-HQ [9] and Vox-Celeb [15] datasets are pro-
vided in the supplementary materials. Further, we accelerate
our model using techniques such as pruning, network slim-
ming and quantization to allow real-time inference on mobile
phones. Some exemplar video demos are presented in the
supplementary materials.

5. Conclusion

We propose a novel data preparation method and a novel
network architecture for the task of facial parts removal
(face erasing), a task that attracted insufﬁcient attention from
academia. The novel “Pixel-Clone” mechanism effectively
solves the problem of the color and texture inconsistency
issue observed in existing image inpainting models. We im-
plement and demonstrate various interesting and enjoyable
augmented-reality applications on top of our FaceEraser.

Our model suffers small chance of failure and is prone to
generate unsatisfactory results due to undersegmentation of
the facial parts (e.g., eyebrows, glasses) or interference of
beards, hair or peripherals (see the supplementary materials
for more details).

References

[1] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles,
Guillermo Sapiro, and Joan Verdera. Filling-in by joint inter-
polation of vector ﬁelds and gray levels. IEEE transactions
on image processing, 10(8):1200–1211, 2001.

[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspondence
algorithm for structural image editing. ACM Trans. Graph.,
28(3):24, 2009.

[3] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
Coloma Ballester. Image inpainting. In Proceedings of the
27th annual conference on Computer graphics and interactive
techniques, pages 417–424, 2000.

[4] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-
ject removal by exemplar-based inpainting. In 2003 IEEE
Computer Society Conference on Computer Vision and Pat-
tern Recognition, 2003. Proceedings., volume 2, pages II–II.
IEEE, 2003.

[5] Antonio Criminisi, Patrick P´erez, and Kentaro Toyama.
Region ﬁlling and object removal by exemplar-based im-
IEEE Transactions on image processing,
age inpainting.
13(9):1200–1212, 2004.

[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems, 27:2672–2680, 2014.
[7] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
Image-to-image translation with conditional adversarial net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1125–1134, 2017.
[8] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European conference on computer vision, pages 694–711.
Springer, 2016.

[9] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017.
[10] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4401–4410, 2019.
[11] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

[12] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro.
Image inpainting for
irregular holes using partial convolutions. In Proceedings of
the European Conference on Computer Vision (ECCV), pages
85–100, 2018.

[13] Yinglu Liu, Hao Shen, Yue Si, Xiaobo Wang, Xiangyu Zhu,
Hailin Shi, Zhibin Hong, Hanqi Guo, Ziyuan Guo, Yanqin
Chen, et al. Grand challenge of 106-point facial landmark
localization. In 2019 IEEE International Conference on Mul-
timedia & Expo Workshops (ICMEW), pages 613–616. IEEE,
2019.

[14] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative adver-
sarial networks. arXiv preprint arXiv:1802.05957, 2018.
[15] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman.
Voxceleb: a large-scale speaker identiﬁcation dataset. arXiv
preprint arXiv:1706.08612, 2017.

[16] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,
and Mehran Ebrahimi. Edgeconnect: Generative image

inpainting with adversarial edge learning. arXiv preprint
arXiv:1901.00212, 2019.

[17] Patrick P´erez, Michel Gangnet, and Andrew Blake. Poisson
image editing. In ACM SIGGRAPH 2003 Papers, pages 313–
318. 2003.

[18] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li, Shan
Liu, and Ge Li. Structureﬂow: Image inpainting via structure-
aware appearance ﬂow. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 181–190,
2019.

[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In International Conference on Medical image computing
and computer-assisted intervention, pages 234–241. Springer,
2015.

[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International journal of
computer vision, 115(3):211–252, 2015.

[21] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[22] Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin Huang,
Hao Li, and C-C Jay Kuo. Contextual-based image inpainting:
Infer, match, and translate. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 3–19, 2018.
[23] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 8798–8807, 2018.

[24] Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, and Zhan
Xu. Contextual residual aggregation for ultra high-resolution
image inpainting. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
7508–7517, 2020.

[25] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:
Unsupervised dual learning for image-to-image translation.
In Proceedings of the IEEE international conference on com-
puter vision, pages 2849–2857, 2017.

[26] Fisher Yu and Vladlen Koltun. Multi-scale context
arXiv preprint

aggregation by dilated convolutions.
arXiv:1511.07122, 2015.

[27] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with con-
textual attention. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5505–5514,
2018.

[28] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 4471–4480, 2019.
[29] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In Proceedings of the
European conference on computer vision (ECCV), pages 286–
301, 2018.

6. Appendix

We present more intermediate and ﬁnal results of our
method on FFHQ test split in Figure 11. The demo results
on CelebA-HQ[9] and more VoxCeleb [15] videos are pre-
sented in Figure 13 and Figure 14. We also present more
comparative results of our method and existing methods in
Figure 12.

Some failure examples are presented in Figure 15.
Some more video demos are provided in the supplemen-

tary materials.

Figure 9. Examples of training samples.

Figure 10. Examples of training masks.

Figure 11. Illustration of intermediate and ﬁnal results of our inpainting models. The test images cones from the FFHQ test split, which are
unseen by the pre-trained model.

Figure 12. Visual comparisons of our method with state-of-the-art inpainting methods. Note that the competing models are re-trained on our
training datasets with similar conﬁgurations.

Figure 13. Augmented-reality applications experimented on the CelebA-HQ dataset.

Figure 14. Augmented-reality applications experimented on the VoxCeleb dataset.

Figure 15. Exemplar failure samples generated by our FaceEraser. The failures typically result from interference of hair, shadow or other
peripherals, or undersegmentation of eyebrows or beards. The red boxes highlight the unpleasing artifacts.

