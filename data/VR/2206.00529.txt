2
2
0
2

n
u
J

1

]

G
L
.
s
c
[

1
v
9
2
5
0
0
.
6
0
2
2
:
v
i
X
r
a

Variance Reduction is an Antidote to Byzantines:
Better Rates, Weaker Assumptions and
Communication Compression as a Cherry on the Top

Eduard Gorbunov∗
MIPT
Mila & UdeM

Samuel Horváth
KAUST

Peter Richtárik
KAUST

Gauthier Gidel
Mila & UdeM
Canada CIFAR AI Chair

Abstract

Byzantine-robustness has been gaining a lot of attention due to the growth of the
interest in collaborative and federated learning. However, many fruitful directions,
such as the usage of variance reduction for achieving robustness and communica-
tion compression for reducing communication costs, remain weakly explored in
the ﬁeld. This work addresses this gap and proposes Byz-VR-MARINA–a new
Byzantine-tolerant method with variance reduction and compression. A key mes-
sage of our paper is that variance reduction is key to ﬁghting Byzantine workers
more effectively. At the same time, communication compression is a bonus that
makes the process more communication efﬁcient. We derive theoretical conver-
gence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art
for general non-convex and Polyak-Łojasiewicz loss functions. Unlike the con-
current Byzantine-robust methods with variance reduction and/or compression,
our complexity results are tight and do not rely on restrictive assumptions such as
boundedness of the gradients or limited compression. Moreover, we provide the
ﬁrst analysis of a Byzantine-tolerant method supporting non-uniform sampling of
stochastic gradients. Numerical experiments corroborate our theoretical ﬁndings.

1

Introduction

Distributed optimization algorithms play a vital role in the training of the modern machine learning
models.
In particular, some tasks require training of deep neural networks having billions of
parameters on large datasets [Brown et al., 2020, Kolesnikov et al., 2020]. Such problems may
take years of computations to be solved if executed on a single yet powerful machine [Li, 2020]. To
circumvent this issue, it is natural to use distributed optimization algorithms allowing to tremendously
reduce the training time [Goyal et al., 2017, You et al., 2020]. In the context of speeding up the
training, distributed methods are usually applied in data centers [Mikami et al., 2018]. More recently,
similar ideas have been applied to train models using open collaborations [Kijsipongse et al., 2018,
Diskin et al., 2021], where each participant (e.g., a small company/university or an individual) has
very limited computing power but can donate it to jointly solve computationally-hard problems.
Moreover, in Federated Learning (FL) applications [McMahan et al., 2017, Koneˇcný et al., 2016,
Kairouz et al., 2021], distributed algorithms are natural and the only possible choice since in such
problems, the data is privately distributed across multiple devices.

In the optimization problems arising in collaborative and federated learning, there is a high risk that
some participants deviate from the prescribed protocol either on purpose or not. For example, some
peers can maliciously send incorrect gradients to slow down or even destroy the training2. Indeed,

∗Corresponding author: eduard.gorbunov@phystech.edu.
2Such workers are usually called Byzantines [Lyu et al., 2020].

Preprint. Under review.

 
 
 
 
 
 
these attacks can break the convergence of naïve methods such as Parallel-SGD [Zinkevich et al.,
2010]. Therefore, it is crucial to use secure (a.k.a. Byzantine-robust/Byzantine-tolerant) distributed
methods for solving such problems.

However, designing distributed methods with provable Byzantine-robustness is not an easy task. The
non-triviality of this problem comes from the fact that the stochastic gradients of good/honest/regular
workers are naturally different due to their stochasticity and possible data heterogeneity. At the same
time, malicious workers can send the vectors looking like the stochastic gradients of good peers or
create small but time-coupled shifts. Therefore, as it is shown in [Baruch et al., 2019, Xie et al.,
2020, Karimireddy et al., 2021], Byzantines can circumvent popular defences based on applying
robust aggregation rules [Blanchard et al., 2017, Yin et al., 2018, Damaskinos et al., 2019, Guerraoui
et al., 2018, Pillutla et al., 2022] with Parallel-SGD. Moreover, in a broad class of problems with
heterogeneous data, it is provably impossible to achieve any predeﬁned accuracy of the solution
[Karimireddy et al., 2022].

Nevertheless, as it becomes evident from the further discussion, several works have provable Byzan-
tine tolerance and rigorous theoretical analysis. In particular, Wu et al. [2020] propose a natural
yet elegant solution to the problem of Byzantine-robustness based on the usage of variance-reduced
methods [Gower et al., 2020] and design the ﬁrst variance-reduced Byzantine-robust method called
Byrd-SAGA, which combines the celebrated SAGA method [Defazio et al., 2014] with geometric
median aggregation rule. As a result, reducing the stochastic noise of estimators used by good workers
makes it easier to ﬁlter out Byzantines (especially in the case of homogeneous data). However, Wu
et al. [2020] derive their results only for the strongly convex objectives, and the obtained convergence
guarantees are signiﬁcantly worse than the best-known convergence rates for SAGA, i.e., their results
are not tight, even when there are no Byzantine workers and all peers have homogeneous data. It is
crucial to bypass these limitations since the majority of the modern, practically interesting problems
are non-convex. Furthermore, it is hard to develop the ﬁeld without tight convergence guarantees. All
in all, the above leads to the following question:

Q1: Is it possible to design variance-reduced methods with provable Byzantine-robustness
and tight theoretical guarantees for general non-convex optimization problems?

In addition to Byzantine-robustness, one has to take into account that naïve distributed algorithms
suffer from the so-called communication bottleneck—a situation when communication is much more
expensive than local computations on the devices. This issue is especially evident in the training of
models with a vast number of parameters (e.g., millions or trillions) or when the number of workers
is large (which is often the case in FL). One of the most popular approaches to reducing the commu-
nication bottleneck is to use communication compression [Seide et al., 2014, Koneˇcný et al., 2016,
Suresh et al., 2017], i.e., instead of transmitting dense vectors (stochastic gradients/Hessians/higher-
order tensors) workers apply some quantization/sparsiﬁcation operator to these vectors and send the
compressed results to the server. Distributed learning with compression is a relatively well-developed
ﬁeld, e.g., see [Vogels et al., 2019, Gorbunov et al., 2020b, Richtárik et al., 2021, Philippenko and
Dieuleveut, 2021] and references therein for the recent advances.

Perhaps surprisingly, there are not many methods with compressed communication in the context of
Byzantine-robust learning. In particular, we are only aware of one work in these settings [Zhu and
Ling, 2021], where the authors study Byzantine-robust versions of compressed SGD (BR-CSGD)
and SAGA (BR-CSAGA) and also propose a combination of DIANA [Mishchenko et al., 2019,
Horváth et al., 2019b] with BR-CSAGA called BROADCAST. However, the derived convergence
results for these methods have several limitations. First of all, the analysis is given only for strongly
convex problems. In addition, it relies on restrictive assumptions. Namely, Zhu and Ling [2021]
assume uniform boundedness of the second moment of the stochastic gradient in the analysis of
BR-CSGD and BR-CSAGA. This assumption rarely holds in practice, and it also implies the
boundedness of the gradients, which contradicts the strong convexity assumption. Next, although the
bounded second-moment assumption is not used in the analysis of BROADCAST, Zhu and Ling
[2021] derive the rates of BROADCAST under the assumption that the compression operator is very
accurate, which implies that in theory workers apply almost no compression to the communicated
messages (see remark (5) under Table 2). Finally, even if there are no Byzantines and no compression,
similar to the guarantees for Byrd-SAGA, the rates obtained for BR-CSGD, BR-CSAGA, and
BROADCAST are outperformed with a large margin by the known rates for SGD and SAGA. All of

2

Table 1: Comparison of the state-of-the-art (in theory) Byzantine-tolerant distributed methods. Columns: “NC”
= does the theory works for general smooth non-convex functions?; “PL” = does the theory works for functions
satysfying PŁ-condition (As. 2.5)?; “Tight?” = does the theory recover tight best-known results for the version of
the method with δ = 0 (no Byzantines)?; “Compr.?” = does the method use communication compression?; “VR?”
= is the method variance-reduced?; “No UBV?” = does the theory work without assuming uniformly bounded
variance of the stochastic gradients?; “No BG?” = does the theory work without assuming uniformly bounded
second moment of the stochastic gradients?; “Non-US?” = does the theory support non-uniform sampling of the
stochastic gradients; “Het.?” = does the theory work under (B, ζ 2)-heterogeneity assumption (As. 2.2)?

Method

NC

PL

Tight?

Compr.?

VR?

No UBV?

No BG?

Non-US?

Het.?

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)(cid:51)(1)

(cid:55)(cid:51)(1)

BR-SGDm
[Karimireddy et al., 2021, 2022]
BTARD-SGD
[Gorbunov et al., 2021a]
Byrd-SAGA
[Wu et al., 2020]
BR-MVR
[Karimireddy et al., 2021]
BR-CSGD
[Zhu and Ling, 2021]
BR-CSAGA
[Zhu and Ling, 2021]
BROADCAST
[Zhu and Ling, 2021]
Byz-VR-MARINA
[This work]
(1) Strong convexity of f is assumed.
(2) Analysis is performed under As. 2.2 with B = 0.

(cid:55)(cid:51)(1)

(cid:55)(cid:51)(1)

(cid:55)(cid:51)(1)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)(cid:51)(2)

(cid:55)

(cid:55)(cid:51)(2)

(cid:55)(cid:51)(2)

(cid:55)(cid:51)(2)

(cid:51)

these limitations lead to the following question:

Q2: Is it possible to design distributed methods with compression, provable Byzantine-robustness
and tight theoretical guarantees without making strong assumptions?

In this paper, we give conﬁrmatory answers to Q1 and Q2 by proposing and rigorously analyzing
a new Byzantine-tolerant variance-reduced method with compression called Byz-VR-MARINA.
Detailed related work overview is deferred to Appendix A.

1.1 Our Contributions

Before we proceed, we need to specify the targetted problem. We consider a centralized distributed
learning in the possible presence of malicious or so-called Byzantine peers. We assume that there are
n clients consisting of the two groups: [n] = G (cid:116) B, where G denotes the set of good clients and B is
the set of bad/malicious/Byzantine workers. The goal is to solve the following optimization problem

(cid:40)

min
x∈Rd

f (x) =

1
G

(cid:88)

i∈G

(cid:41)

fi(x)

,

fi(x) =

1
m

m
(cid:88)

j=1

fi,j(x) ∀i ∈ G,

(1)

where G = |G| and functions fi,j(x) are assumed to be smooth, but not necessarily convex. Here
each good client has its dataset of the size m, fi,j(x) is the loss of the model, parameterized by vector
x ∈ Rd, on the j-th sample from the dataset on the i-th client. Following the classical convention [Lyu
et al., 2020], we make no assumptions on the workers B, i.e., Byzantines allowed being omniscent.
Our main contributions are summarized below.

(cid:5) New method: Byz-VR-MARINA. We propose a new Byzantine-robust variance-reduced method
with compression called Byz-VR-MARINA (Alg. 1). In particular, we make VR-MARINA [Gorbunov
et al., 2021b], which is a variance-reduced method with compression, applicable to the context of
Byzantine-tolerant distributed learning via using the recent tool of robust agnostic aggregation of
Karimireddy et al. [2022]. As Tbl. 1 shows, Byz-VR-MARINA and our analysis of the method have
several important improvements upon the previously best-known methods.

(cid:5) New SOTA results. Under quite general assumptions listed in Section 2, we prove theoretical
convergence results for Byz-VR-MARINA in the cases of smooth non-convex (Thm. 2.1) and Polyak-
Łojasiewicz (Thm. 2.2) functions. As Tbl. 2 shows, our complexity bounds in the non-convex case
are always better than previously known ones when the target accuracy ε is small enough. In the PŁ

3

Table 2: Comparison of the state-of-the-art complexity results for Byzantine-tolerant distributed methods. Columns: “Assumptions” =
additional assumptions to smoothness of all fi(x), i ∈ G (although our results require more reﬁned As. 2.3); “Complexity (NC)” and
“Complexity (PŁ)” = number of communication rounds required to ﬁnd such x that E(cid:107)∇f (x)(cid:107)2 ≤ ε2 in the general non-convex case and
such x that E[f (x) − f (x∗)] ≤ ε in PŁ case respectively. Dependencies on numerical constants (and logarithms in PŁ setting), smoothness
constants, and initial suboptimality are omitted in the complexity bounds. Although BR-SGDm, BR-MVR, BTARD-SGD, Byrd-SAGA, BR-
CSGD, BR-CSAGA, BROADCAST are analyzed for unit batchsize only (b = 1), one can easily generalize them to the case of b > 1 and
we show these generalizations in the table. Notation: ε = desired accuracy; δ = ratio of Byzantines; c = parameter of the robust aggregator;
n = total number of workers; b = batchsize; σ2 = uniform bound on the variance of stochastic gradients; G2 = uniform bound on the second
moment of stochastic gradients; C = the number of workers used by BTARD-SGD for the checks of computations after each step; µ =
parameter from As. 2.5 (strong convexity parameter in the case of BTARD-SGD, Byrd-SAGA, BR-CSGD, BR-CSAGA, BROADCAST);
m = size of the local dataset on workers; p = min {b/m, 1/(1+ω)} = probability of communication in Byz-VR-MARINA.

Setup

Method

Assumptions

Hom. data,
no compr.

Het. data,
no compr.

Het. data,
compr.

BR-SGDm
[Karimireddy et al., 2021, 2022]
BR-MVR
[Karimireddy et al., 2021]
BTARD-SGD
[Gorbunov et al., 2021a]
Byrd-SAGA (2)
[Wu et al., 2020]

Byz-VR-MARINA
Cor. E.1 & Cor. E.5

BR-SGDm (3)
[Karimireddy et al., 2022]
Byrd-SAGA (2),(3)
[Wu et al., 2020]

Byz-VR-MARINA (3),(4)
Cor. E.2 & Cor. E.6

BR-CSGD (2),(3)
[Zhu and Ling, 2021]

BR-CSAGA (2),(3)

[Zhu and Ling, 2021]
BROADCAST (2),(3),(5)
[Zhu and Ling, 2021]

UBV

UBV

UBV(1)

Smooth fi,j

As. 2.4

UBV

Smooth fi,j

As. 2.4

UBV, BG

Smooth fi,j
UBV, BG

Smooth fi,j

Byz-VR-MARINA (3),(6)
Cor. E.3 & Cor. E.7

As. 2.4

1

1

bε4

Complexity (NC)
ε2 + σ2(cδ+1/n)
√
cδ+1/n
ε2 +
√
bε3
ε2 + n2δσ2
(cid:55)

Cbε2 + σ2

σ

1

nbε4

(cid:114)

1+

cδm2
b3 + m
b2n
ε2

1

ε2 + σ2(cδ+1/n)
(cid:55)

bε4

Complexity (PŁ)

(cid:55)

(cid:55)

1

µ + σ2

nbµε + n2 δσ

√

C

bµε

1+

m2
b2(1−2δ)µ2
(cid:114)
cδm2
b3 + m
b2n
µ
+ m
b

(cid:55)

(cid:114)

1+

cδm2
b2 (1+ 1
ε2

b

)+ m
b2 n

(cid:114)

1+

(cid:55)

(cid:55)

(cid:55)

(cid:113)

1+

cδ(1+ω)(1+ 1
b

)

pε2

(cid:113)

+

(1+ω)(1+ 1
b
pnε2

√

)

+

m2
b2(1−2δ)µ2
cδm2
b2 (1+ 1
µ
+ m
b

b

)+ m
b2n

1
µ2

m2
b2 µ2(1−2δ)2
m2(1+ω)3/2
b2µ2(1−2δ)
(cid:113)

1+

cδ(1+ω)(1+ 1
b

)

pµ

(cid:113)

)

(1+ω)(1+ 1
b
√
pnµ
+ m
b + ω

(1) Gorbunov et al. [2021a] assume additionally that the tails of the noise distribution in stochastic gradients are sub-quadratic.
(2) Although the analyses by Wu et al. [2020], Zhu and Ling [2021] support inexact geometric median computation, for simplicity of presen-
tation, we assume that geometric median is computed exactly.
(3) BR-SGDm: ε2 = Ω(cδζ2); Byrd-SAGA: ε = Ω(ζ2/(µ2(1−2δ)2 )); Byz-VR-MARINA: ε2 = Ω(max{m/b, 1 + ω}cδζ2) for
general non-convex case and ε = Ω(max{m/b, 1 + ω}cδζ2/µ) for the case of PŁ functions (with ω = 0, where there is no compression);
BR-CSGD: ε = Ω((σ2+ζ2+ωG2)/(µ2(1−2δ)2)) (positive even when ζ2 = 0); BR-CSAGA: ε = Ω((ζ2+ωG2 )/(µ2 (1−2δ)2))
(positive even when ζ2 = 0); BROADCAST: ε = Ω((1+ω)ζ2/(µ2(1−2δ)2)).
(4) The term m
well.
(5) For this result Zhu and Ling [2021] assume that ω ≤ µ2(1−2δ)2
, which is a very restrictive assumption even when δ = 0. For
56L2(2−2δ2 )
example, even for well-conditioned problems with µ/L ∼ 10−3 and δ = 0 (no Byzantines), this bound implies that ω should be not larger
than 10−7. Such a value of ω corresponds to almost non-compressed communications.
(6) The term
similar statement holds in PŁ case as well.

√
1+ω
pnε2 is proportional to much smaller Lipschitz constant than the term
√

b3/2 ε2 does. A similar statement holds in PŁ case as

is proportional to much smaller Lipschitz constant than the term m

√
1+ω
pnbε2 does. A

cδ(1+ω)
pε2

cδ(1+ω)
bpε2

√
√

bε2

√

+

+

1+

1+

cδ

cδ

√

√

√

case, our results improve upon previously known guarantees when the problem has bad conditioning
or when ε is small enough. Moreover, we provide the ﬁrst theoretical convergence guarantees for
Byzantine-tolerant methods with compression in the non-convex case.

(cid:5) Byzantine-tolerant variance-reduced method with tight rates. Our results are tight, i.e., when
there are no Byzantines, our rates recover the rates of VR-MARINA, and when additionally no com-
pression is applied, we recover the optimal rates of Geom-SARAH [Horváth et al., 2022]/PAGE [Li
et al., 2021]. In contrast, this is not the case for previously known variance-reduced Byzantine-robust
methods such as Byrd-SAGA, BR-CSAGA, and BROADCAST that in the homogeneous data
scenario have worse rates than single-machine SAGA.

(cid:5) Support of the compression without strong assumptions. As we point out in Tbl. 2, the analysis
of BR-CSGD and BR-CSAGA relies on the bounded second-moment assumption, which contradicts
strong convexity, and the rates for BROADCAST are derived under the assumption that the compres-

4

sion operator almost coincides with the identity operator, meaning that in practice workers essentially
do not use any compression. In contrast, our analysis does not have such substantial limitations.

(cid:5) Enabling non-uniform sampling. In contrast to the existing works on Byzantine-robustness, our
analysis supports non-uniform sampling of stochastic gradients. Considering the dependencies on
smoothness constants, one can quickly notice our rates’ even more signiﬁcant superiority compared
to the previous SOTA results.

2 Byz-VR-MARINA: Byzantine-Tolerant Variance Reduction with

Communication Compression

We start by introducing necessary deﬁnitions and assumptions.

Robust aggregation. One of the main building blocks of our method relies on the notion of (δ, c)-
Robust Aggregator introduced in [Karimireddy et al., 2021, 2022].
Deﬁnition 2.1 ((δ, c)-Robust Aggregator). Assume that {x1, x2, . . . , xn} is such that there exists
a subset G ⊆ [n] of size |G| = G ≥ (1 − δ)n for δ < 0.5 and there exists σ ≥ 0 such that
E[(cid:107)xi − xl(cid:107)2] ≤ σ2 where the expectation is taken w.r.t. the randomness of {xi}i∈G.
We say that the quantity (cid:98)x is (δ, c)-Robust Aggregator ((δ, c)-RAgg) and write (cid:98)x = RAgg(x1, . . . , xn)
for some c > 0, if the following inequality holds:

1
G(G−1)

i,l∈G

(cid:80)

E (cid:2)(cid:107)(cid:98)x − x(cid:107)2(cid:3) ≤ cδσ2,

(2)

(cid:80)

i∈G xi. If additionally (cid:98)x is computed without the knowledge of σ2, we say that (cid:98)x is

where x = 1
|G|
(δ, c)-Agnostic Robust Aggregator ((δ, c)-ARAgg) and write (cid:98)x = ARAgg(x1, . . . , xn).
In fact, Karimireddy et al. [2021, 2022] propose slightly different deﬁnition, where they assume
that E(cid:107)xi − xl(cid:107)2 ≤ σ2 for all ﬁxed good workers i, l ∈ G, which is marginally stronger than what
we assume. Karimireddy et al. [2021] prove tightness of their deﬁnition, i.e., up to the constant c
one cannot improve bound (2), and prove that popular “middle-seekers” such as Krum [Blanchard
et al., 2017], Robust Federated Averaging (RFA) [Pillutla et al., 2022], and Coordinate-wise
Median (CM) [Chen et al., 2017] do not satisfy their deﬁnition. However, there is a trick called buck-
eting [Karimireddy et al., 2022] that provably robustiﬁes Krum/RFA/CM. Nevertheless, the difference
between our deﬁnition and the original one from [Karimireddy et al., 2021, 2022] is very subtle and
it turns out that Krum/RFA/CM with bucketing ﬁt Deﬁnition 2.1 as well (see Appendix D).

Compression. We consider unbiased compression operators, i.e., quantizations.
Deﬁnition 2.2 (Quantization [Horváth et al., 2019b]). Stochastic mapping Q : Rd → Rd is called
quantization operator/quantization if there exists ω ≥ 0 such that for any x ∈ Rd

E [Q(x)] = x, E (cid:2)(cid:107)Q(x) − x(cid:107)2(cid:3) ≤ ω(cid:107)x(cid:107)2.

(3)

For the given quantization operator Q(x), one can deﬁne the expected density as ζQ =
supx∈Rd E [(cid:107)Q(x)(cid:107)0] , where (cid:107)y(cid:107)0 is the number of non-zero components of y ∈ Rd.

The above deﬁnition covers many popular compression operators such as RandK sparsiﬁcation [Stich
et al., 2018], random dithering [Goodall, 1951, Roberts, 1962], and natural compression [Horváth
et al., 2019a] (see also the summary of various compression operators in [Beznosikov et al., 2020]).
There exist also other classes of compression operators such as δ-contractive compressors [Stich
et al., 2018] and absolute compressors [Tang et al., 2019, Sahu et al., 2021]. However, these types of
compressors are out of the scope of this work.

Assumptions. The ﬁrst assumption is quite standard in the literature on non-convex optimization.
Assumption 2.1. We assume that function f : Rd → R is L-smooth, i.e., for all x, y ∈ Rd we have
(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107). Moreover, we assume that f is uniformly lower bounded by f∗ ∈ R,
i.e., f∗ = inf x∈Rd f (x).

Next, we need to restrict the data heterogeneity of regular workers.
Indeed, in arbitrarily het-
erogeneous scenario, it is impossible to distinguish regular workers and Byzantines. Following
[Karimireddy et al., 2022], we make the following assumption.

5

Assumption 2.2 ((B, ζ 2)-heterogeneity). We assume that good clients have (B, ζ 2)-heterogeneous
local loss functions for some B ≥ 0, ζ ≥ 0, i.e.,

1
G

(cid:88)

i∈G

(cid:107)∇fi(x) − ∇f (x)(cid:107)2 ≤ B(cid:107)∇f (x)(cid:107)2 + ζ 2 ∀x ∈ Rd.

(4)

The following assumption is a reﬁnement of a standard assumption that fi is Li-smooth for all i ∈ G.
Assumption 2.3 (Global Hessian variance assumption [Szlendak et al., 2021]). We assume that there
exists L± ≥ 0 such that for all x, y ∈ Rd

1
G

(cid:88)

i∈G

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 − (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ L2

±(cid:107)x − y(cid:107)2.

(5)

avg − L2 ≤ L2

If fi is Li-smooth for all i ∈ G, then the above assumption is always valid for some L± ≥ 0 such
that L2
i [Szlendak et al., 2021]. Moreover, Szlendak
et al. [2021] show that there exist problems with heterogeneous functions on workers such that (5)
holds with L± = 0, while Lavg > 0.

avg, where L2

avg = 1
G

± ≤ L2

i∈G L2

(cid:80)

We propose a generalization of the above assumption for samplings of stochastic gradients.
Assumption 2.4 (Local Hessian variance assumption). We assume that there exists L± ≥ 0 such
that for all x, y ∈ Rd

1
G

(cid:88)

i∈G

E(cid:107) (cid:98)∆i(x, y) − ∆i(x, y)(cid:107)2 ≤

L2
±
b

(cid:107)x − y(cid:107)2,

(6)

where ∆i(x, y) = ∇fi(x) − ∇fi(y) and (cid:98)∆i(x, y) is an unbiased mini-batched estimator of ∆i(x, y)
with batch size b.

We notice that the above assumption covers a wide range of samplings of mini-batched stochastic
gradient differences. Below we provide two examples of situations when Assumption 2.4 holds. In
both cases, we assume that fi,j is Li,j-smooth for all i ∈ G, j ∈ [m].
Example 2.1 (Uniform sampling with replacement). Consider (cid:98)∆i(x, y) = 1
∆i,j(x, y),
b
where ∆i,j(x, y) = ∇fi,j(x) − ∇fi,j(y) and Ii,k is the set of b i.i.d. samples from the uniform
distribution on [m]. Then, Assumption 2.4 holds with L2
and L2

±,US, where L2

±,US ≤ 1
G

± = L2

i∈G L2

j∈Ii,k

i,±,US

(cid:80)

(cid:80)

i,±,US is such that

1
m

m
(cid:88)

j=1

(cid:107)∆i,j(x, y) − ∆i(x, y)(cid:107)2 ≤ L2

i,±,US(cid:107)x − y(cid:107)2.

Lemma 2 from Szlendak et al. [2021] implies that L2
such that 1
j=1 (cid:107)∆i,j(x, y)(cid:107)2 ≤ L2
m
(cid:80)m
1
j=1 L2
i,j.
m

(cid:80)m

i,US, where L2
i ≤ L2
i,US(cid:107)x − y(cid:107)2. We point out that in the worst case L2

i,±,US ≤ L2

i,US − L2

i,US is
i,US =

Example 2.2 (Importance sampling with replacement). Consider (cid:98)∆k
where ∆i,j(x, y) = ∇fi,j(x) − ∇fi,j(y), Li = 1
m
from the distribution Di,IS on [m] such that for j ∼ Di,IS we have P{j = t} = Li,t
mLi
Assumption 2.4 holds with L2

∆i,j(x, y),
j=1 Li,j, and Ii,k is the set of b i.i.d. samples
. Then,

i = 1
b

(cid:80)m

j∈Ii,k

Li
Li,j

± = L2

±,IS such that

(cid:80)

1
mG

(cid:88)

m
(cid:88)

i∈G

j=1

Li
Li,j

(cid:107)∆i,j(x, y)(cid:107)2 −

1
mG

(cid:88)

i∈G

(cid:107)∆i(x, y)(cid:107)2 ≤ L2

±,IS(cid:107)x − y(cid:107)2.

Lemma 2 from Szlendak et al. [2021] implies that 1
G
2
i = L2
i,US and in the worst case mL

2
i ≤ L2
point out that L

(cid:80)

2
±,IS ≤ 1
i − L2
i ) ≤ L2
i∈G(L
G
i,US. Therefore, typically L2

2
i . We
i∈G L
±,IS < L2
±,US.

(cid:80)

6

We notice that all previous works on Byzantine-robustness focus on uniform sampling only. However,
uniform sampling can give m times worse constant L2
± than importance sampling. As we will see
next, this difference might be signiﬁcant in the complexity bounds.

New Method: Byz-VR-MARINA. Now we are ready to present our new method—Byzantine-tolerant
Variance-Reduced MARINA (Byz-VR-MARINA). Our algorithm is based on the recently proposed
variance-reduced method with compression (VR-MARINA) from [Gorbunov et al., 2021b]. At each
iteration of Byz-VR-MARINA, good workers update their parameters xk+1 = xk − γgk using esti-
mator gk received from the parameter-server (line 7). Next (line 8), with (typically small) probability
p each good worker i ∈ G computes its full gradient, and with (typically large) probability 1 − p this
worker computes quantized mini-batched stochastic gradient difference Q( (cid:98)∆i(xk+1, xk)), where
(cid:98)∆i(xk+1, xk) satisﬁes Assumption 2.4. After that, the server gathers the results of computations
from the workers and applies (δ, c)-ARAgg to compute the next estimator gk+1 (line 10).

Let us elaborate on several important parts of the proposed algorithm. First, we point out that
with large probability 1 − p good workers need to send just quantized vectors Q( (cid:98)∆i(xk+1, xk)),
i ∈ G. Indeed, since the server knows when workers compute full gradients and when they compute
quantized stochastic gradients, it needs just to add gk to all received vectors to perform robust
aggregation from line 10. Moreover, since the server knows the type of quantization operator that
good workers apply, it can typically easily ﬁlter out those Byzantines who try to slow down the
training via sending dense vectors instead of quantized ones (e.g., if the quantization operator is
RandK sparsiﬁcation, then Byzantines cannot send more than K components; otherwise they will
be easily detected and can be banned). Next, the right choice of probability p allows equalizing the
communication cost of all steps when good workers send dense gradients and compressed gradient
differences. The same is true for oracle complexity: if p ≤ b/m, then the computational cost of
full-batch computations is not bigger than that of stochastic gradients. Finally, although the difference
between Byz-VR-MARINA and VR-MARINA is only in the choice of the aggregation rule, it allows
us to obtain vast improvements upon previously known theoretical results for Byzantine-tolerant
learning, as shown in the next subsection.

Algorithm 1 Byz-VR-MARINA: Byzantine-tolerant VR-MARINA
1: Input: starting point x0, stepsize γ, minibatch size b, probability p ∈ (0, 1], number of iterations

K, (δ, c)-ARAgg

2: Initialize g0 = ∇f (x0)
3: for k = 0, 1, . . . , K − 1 do
4:
5:
6:
7:

Get a sample from Bernoulli distribution with parameter p: ck ∼ Be(p)
Broadcast gk, ck to all workers
for i ∈ G in parallel do
xk+1 = xk − γgk

8:

Set gk+1
i

=

(cid:40)∇fi(xk+1),
gk + Q

(cid:16)

(cid:98)∆i(xk+1, xk)

(cid:17)

if ck = 1,
, otherwise,

, where minibatched estimator

(cid:98)∆i(xk+1, xk) satisﬁes Assumption 2.4; Q(·) for different i ∈ G are computed independently

end for
gk+1 = ARAgg(gk+1

9:
10:
11: end for
12: Return: ˆxK chosen uniformly at random from {xk}K−1
k=0

, . . . , gk+1

)

n

1

General Non-Convex Functions. Our main convergence result for general non-convex functions
follows. All proofs are deferred to Appendix E.

Theorem 2.1. Let Assumptions 2.1, 2.2, 2.3, 2.4 hold. Assume that

0 < γ ≤

1
√
L +

,

A

δ < min

(cid:26) p

48cB

(cid:27)

,

,

1
2

(7)

where A = 48BL2cδ
±. Then
for all K ≥ 0 the point (cid:98)xK choosen uniformly at random from the iterates x0, x1, . . . , xK produced

p + 6(1−p)

+ 6(1−p)
p

p + 1

ωL2 +

+ ω
2G

L2

2G

p

(cid:16) 4cδ(1+ω)
p

(cid:17)

(cid:17)

(1+ω)L2
±
b

(cid:16) 4cδ

(cid:17) (cid:16)

7

by Byz-VR-MARINA satisﬁes

E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤

2Φ0
(cid:17)
1 − 48Bcδ

p

(cid:16)

γ

(K + 1)

+

24cδζ 2
p − 48Bcδ

,

(8)

where Φ0 = f (x0) − f∗ + γ

p (cid:107)g0 − ∇f (x0)(cid:107)2.

We highlight here several important properties of the derived result. First of all, this is the ﬁrst
theoretical result for the convergence of Byzantine-tolerant methods with compression in the non-
convex case. Next, when ζ > 0 the theorem above does not guarantee that E[(cid:107)∇f ((cid:98)xK)(cid:107)2] can
be made arbitrarily small. However, this is not a drawback of our analysis but rather an inevitable
limitation of all algorithms in heterogeneous case. This is due to Karimireddy et al. [2022] who proved
a lower bound showing that in the presence of Byzantines, all algorithms satisfy E[(cid:107)∇f ((cid:98)xK)(cid:107)2] =
Ω(δζ 2) (when B = 0), i.e., the constant term from (8) is tight up to the factor of 1/p. However,
when ζ = 0, Byz-VR-MARINA can achieve any predeﬁned accuracy of the solution as long as (7)
holds. Finally, as Table 2 shows3 , Byz-VR-MARINA achieves E[(cid:107)∇f ((cid:98)xK)(cid:107)2] ≤ ε2 faster than all
previously known Byzantine-tolerant methods, when ε is small enough. Moreover, unlike virtually all
other results in the non-convex case, Theorem 2.1 does not rely on the uniformly bounded variance
assumption, which is known to be very restrictive [Nguyen et al., 2018].

Functions Satisfying Polyak-Łojasiewicz (PŁ) Condition. We extend our theory to the functions
satisfying Polyak-Łojasiewicz condition [Polyak, 1963, Łojasiewicz, 1963]. This assumption gen-
eralizes regular strong convexity and holds for several non-convex problems [Karimi et al., 2016].
Moreover, a very similar assumption appears in over-parameterized deep learning [Liu et al., 2022].
Assumption 2.5 (PŁ condition). We assume that function f satisﬁes Polyak-Łojasiewicz (PŁ) condi-
tion with parameter µ, i.e., for all x ∈ Rd there exists x∗ ∈ argminx∈Rd f (x) such that

(cid:107)∇f (x)(cid:107)2 ≥ 2µ (f (x) − f (x∗)) .

Under this and previously introduced assumptions, we derive the following result.
Theorem 2.2. Let Assumptions 2.1, 2.2, 2.3, 2.4, 2.5 hold. Assume that

0 < γ ≤ min




L +



1
√

,

2A

(cid:16)

p
1 − 96Bcδ

p

4µ




(cid:17)



,

δ < min

(cid:26) p

96cB

(cid:27)

,

,

1
2

(9)

(10)

p + 6(1−p)

where A = 48BL2cδ
for all K ≥ 0 the iterates produced by Byz-VR-MARINA satisfy
(cid:19)(cid:19)K

p + 1

ωL2 +

(cid:18)

(cid:18)

2G

(1+ω)L2
±
b

p

(cid:16) 4cδ

(cid:17) (cid:16)

(cid:17)

+ 6(1−p)
p

E (cid:2)f (xK) − f (x∗)(cid:3) ≤

1 − γµ

1 −

Φ0 +

96Bcδ
p

(cid:16) 4cδ(1+ω)
p

+ ω
2G

(cid:17)

L2

±. Then

24cδζ 2
µ(p − 96Bcδ)

,

(11)

where Φ0 = f (x0) − f∗ + 2γ

p (cid:107)g0 − ∇f (x0)(cid:107)2.

Similarly to the general non-convex case, in the PŁ-setting Byz-VR-MARINA is able to achieve
E[f (xK) − f (x∗)] = O(cδζ2/µ(p−96Bcδ)) accuracy, which matches (up to the factor of 1/p) the
lower bound from [Karimireddy et al., 2022] derived for µ-strongly convex objectives with B = 0.
Next, when ζ = 0 and B > 0, Byz-VR-MARINA converges linearly asymptotically to the exact
solution—this is the ﬁrst linear convergence result for the stochastic method in the literature on
Byzantine-robustness with heterogeneous data. Moreover, as Table 2 shows, our convergence
result in the PŁ-setting outperforms the known rates in more restrictive strongly-convex setting. In
particular, when ε is small enough, Byz-VR-MARINA has better complexity than BTARD-SGD.
When the conditioning of the problem is bad (i.e., L/µ (cid:29) 1) our rate dominates results of BR-CSGD,
BR-CSAGA, and BROADCAST. Furthermore, both BR-CSGD and BR-CSAGA rely on the
uniformly bounded second moment assumption (contradicting the strong convexity), and the rate of
the BROADCAST algorithm is based on the assumption that ω = O(µ2/L2) implying that Q(x) ≈ x
(no compression) even for well-conditioned problems.

3To have a fair comparison, we take p = min{b/m, 1/(1+ω)} since in this case, at each iteration each
worker sends O(ζQ) components, when ω + 1 = Θ(d/ζQ) (which is the case for RandK sparsiﬁcation and
(cid:96)2-quantization, see [Beznosikov et al., 2020]), and makes O(b) oracle calls in expectation (computations of
∇fi,j(x)). With such choice of p, the total expected (communication and oracle) cost of steps with full gradients
computations/uncompressed communications coincides with the total cost of the rest of iterations.

8

Figure 1: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on a9a dataset with uniform split over 15 workers with 5 Byzantines. The
top row displays the best performance in hindsight for a given attack.

3 Numerical Experiments

In this section, we demonstrate the practical performance of the proposed method. The main goal of
our experimental evaluation is to showcase the beneﬁts of employing SOTA variance reduction to
remedy the presence of Byzantine workers. For the task, we consider the standard logistic regression
model with (cid:96)2-regularization fi,j(x) = −yi,j log(h(x, ai,j))−(1−yi,j) log(1−h(x, ai,j))+λ(cid:107)x(cid:107)2,
where yi,j ∈ {0, 1} is the label, ai,j ∈ Rd represents the features vector, λ is the regularization
parameter and h(x, a) = 1/(1+e−a(cid:62)x). One can show that this objective is smooth, and for λ > 0, it
is also strongly convex, therefore, it satisﬁes PŁcondition. We consider a9a LIBSVM dataset [Chang
and Lin, 2011] and set λ = 0.01. We randomly shufﬂe dataset and we sequentially distribute it
among 15 good workers, where each worker has approximately the same amount of data and there
is no overlap. We include ﬁve Byzantine workers who have access to an entire dataset and the
exact updates computed at each client. We consider ﬁve different attacks: • No Attack (NA): clean
training; • Label Flipping (LF): labels are ﬂipped, i.e., yi,j → 1 − yi,j; • Bit Flipping (BF): a
Byzantine worker sends an update with ﬂipped sign; • A Little is enough (ALIE) [Baruch et al.,
2019]: the Byzantines estimate the mean µG and standard deviation σG of the good updates, and
send µG − zσG to the server where z is a small constant controlling the strength of the attack;
• Inner Product Manipulation (IPM) [Xie et al., 2020]: the attackers send − (cid:15)
i∈G ∇fi(x)
G
where (cid:15) controls the strength of the attack. For the aggregation, we consider three rules: standard
averaging (AVG), coordinate-wise median (CM) with bucketing, and robust federated averaging (RFA)
with bucketing (see the details in Appendix D). For bucketing, we use s = 2, i.e., partitioning the
updates into the groups of two, as recommended by Karimireddy et al. [2022]. We compare SGD,
BR-SGDm [Karimireddy et al., 2021], and our Byz-VR-MARINA. We do not compare against
Byrd-SAGA, which consumes large memory that scales linearly with the number of local data points
and is not well suited for memory-efﬁcient batched gradient computation (e.g., used in PyTorch). Our
implementation is based on PyTorch [Paszke et al., 2019]. We defer further details and additional
experiments with compressed methods to Appendix B.

(cid:80)

Discussion. In Figure 1, we can see that momentum (BR-SGDm) the variance reduction (Byz-VR-
MARINA) techniques consistently outperform the SGD baseline while none of them dominates for
all the attacks. Byz-VR-MARINA is particularly useful in the clean data regime and against the
ALIE and IPM attacks, and the BR-SGDm algorithm provides the best performance for label and bit
ﬂipping attacks. It would be interesting to automatically select the best technique, e.g., momentum or
VR-MARINA, that provides the best defense against any given attack. We leave this for future work.

9

05101520epochs10−710−510−310−1f(x)−f*BEST | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*BEST | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*BEST | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*BEST | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−610−410−2f(x)−f*BEST | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*AVG | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*AVG | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*AVG | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*AVG | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−610−410−2f(x)−f*AVG | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*CM | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*RFA | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*RFA | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*RFA | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*RFA | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−610−410−2f(x)−f*RFA | IPMSGDBR-SGDmByz-VR-MARINAAcknowledgments and Disclosure of Funding

This work was partially supported by a grant for research centers in the ﬁeld of artiﬁcial intelligence,
provided by the Analytical Center for the Government of the Russian Federation in accordance with
the subsidy agreement (agreement identiﬁer 000000D730321P5Q0002) and the agreement with the
Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.

References

D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efﬁcient sgd
via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30,
2017.

D. Alistarh, Z. Allen-Zhu, and J. Li. Byzantine stochastic gradient descent. In Proceedings of the
32nd International Conference on Neural Information Processing Systems, pages 4618–4628,
2018.

Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The Journal of

Machine Learning Research, 18(1):8194–8244, 2017.

Z. Allen-Zhu, F. Ebrahimian, J. Li, and D. Alistarh. Byzantine-resilient non-convex stochastic
gradient descent. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=PbEHqvFtcS.

G. Baruch, M. Baruch, and Y. Goldberg. A little is enough: Circumventing defenses for distributed

learning. Advances in Neural Information Processing Systems, 32, 2019.

D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-sgd: Distributed sgd with quantization,
sparsiﬁcation and local computations. Advances in Neural Information Processing Systems, 32,
2019.

A. Beznosikov, S. Horváth, P. Richtárik, and M. Safaryan. On biased compression for distributed

learning. arXiv preprint arXiv:2002.12410, 2020.

A. Beznosikov, P. Richtárik, M. Diskin, M. Ryabinin, and A. Gasnikov. Distributed methods with
compressed communication for solving variational inequalities, with theoretical guarantees. arXiv
preprint arXiv:2110.03313, 2021.

A. Beznosikov, E. Gorbunov, H. Berard, and N. Loizou. Stochastic gradient descent-ascent: Uniﬁed

theory and new efﬁcient methods. arXiv preprint arXiv:2202.07262, 2022.

P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries:
Byzantine tolerant gradient descent. Advances in Neural Information Processing Systems, 30,
2017.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
processing systems, 33:1877–1901, 2020.

C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM transactions on

intelligent systems and technology (TIST), 2(3):1–27, 2011.

L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos. Draco: Byzantine-resilient distributed training
via redundant gradients. In International Conference on Machine Learning, pages 903–912. PMLR,
2018.

Y. Chen, L. Su, and J. Xu. Distributed statistical machine learning in adversarial settings: Byzantine
gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1
(2):1–25, 2017.

A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex sgd. Advances in

neural information processing systems, 32, 2019.

G. Damaskinos, E.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, and S. Rouault. Aggregathor:
Byzantine machine learning via robust gradient aggregation. Proceedings of Machine Learning
and Systems, 1:81–106, 2019.

M. Danilova and E. Gorbunov. Distributed methods with absolute compression and error compensa-

tion. arXiv preprint arXiv:2203.02383, 2022.

10

A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support
for non-strongly convex composite objectives. Advances in neural information processing systems,
27, 2014.

M. Diskin, A. Bukhtiyarov, M. Ryabinin, L. Saulnier, A. Sinitsin, D. Popov, D. V. Pyrkin, M. Kashirin,
A. Borzunov, A. Villanova del Moral, et al. Distributed deep learning in open collaborations.
Advances in Neural Information Processing Systems, 34:7879–7897, 2021.

F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya. Adaptive
gradient quantization for data-parallel sgd. Advances in neural information processing systems, 33:
3174–3185, 2020.

C. Fang, C. J. Li, Z. Lin, and T. Zhang. Spider: Near-optimal non-convex optimization via stochastic
path-integrated differential estimator. Advances in Neural Information Processing Systems, 31,
2018.

W. Goodall. Television by pulse code modulation. Bell System Technical Journal, 30(1):33–49, 1951.
E. Gorbunov, A. Bibi, O. Sener, E. H. Bergou, and P. Richtárik. A stochastic derivative free
optimization method with momentum. International Conference on Learning Representations,
2020a.

E. Gorbunov, D. Kovalev, D. Makarenko, and P. Richtárik. Linearly converging error compensated

sgd. Advances in Neural Information Processing Systems, 33:20889–20900, 2020b.

E. Gorbunov, A. Borzunov, M. Diskin, and M. Ryabinin. Secure distributed training at scale. arXiv

preprint arXiv:2106.11257, 2021a.

E. Gorbunov, K. P. Burlachenko, Z. Li, and P. Richtárik. MARINA: Faster non-convex distributed
learning with compression. In International Conference on Machine Learning, pages 3788–3798.
PMLR, 2021b.

R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richtárik. SGD: General
analysis and improved rates. In International Conference on Machine Learning, pages 5200–5209.
PMLR, 2019.

R. M. Gower, M. Schmidt, F. Bach, and P. Richtárik. Variance-reduced methods for machine learning.

Proceedings of the IEEE, 108(11):1968–1983, 2020.

P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.
Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677,
2017.

R. Guerraoui, S. Rouault, et al. The hidden vulnerability of distributed learning in byzantium. In

International Conference on Machine Learning, pages 3521–3530. PMLR, 2018.

N. Gupta and N. H. Vaidya. Byzantine fault-tolerance in peer-to-peer distributed gradient-descent.

arXiv preprint arXiv:2101.12316, 2021.

N. Gupta, T. T. Doan, and N. H. Vaidya. Byzantine fault-tolerance in decentralized optimization
under 2f-redundancy. In 2021 American Control Conference (ACC), pages 3632–3637. IEEE,
2021.

F. Haddadpour, M. M. Kamani, A. Mokhtari, and M. Mahdavi. Federated learning with compression:
Uniﬁed analysis and sharp guarantees. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 2350–2358. PMLR, 2021.

L. He, S. P. Karimireddy, and M. Jaggi. Byzantine-robust decentralized learning via self-centered

clipping. arXiv preprint arXiv:2202.01545, 2022.

S. Horváth and P. Richtárik. Nonconvex variance reduced optimization with arbitrary sampling. In

International Conference on Machine Learning, pages 2781–2789. PMLR, 2019.

S. Horváth, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik. Natural compression for

distributed deep learning. arXiv preprint arXiv:1905.10988, 2019a.

S. Horváth, D. Kovalev, K. Mishchenko, S. Stich, and P. Richtárik. Stochastic distributed learning
with gradient quantization and variance reduction. arXiv preprint arXiv:1904.05115, 2019b.
S. Horváth, L. Lei, P. Richtárik, and M. I. Jordan. Adaptivity of stochastic gradient methods for
nonconvex optimization. SIAM Journal on Mathematics of Data Science, 4(2):634–648, 2022. doi:
10.1137/21M1394308.

11

R. Islamov, X. Qian, and P. Richtárik. Distributed second order methods with fast rates and com-
pressed communication. In International Conference on Machine Learning, pages 4617–4628.
PMLR, 2021.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

Advances in neural information processing systems, 26, 2013.

P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. Foundations
and Trends® in Machine Learning, 14(1–2):1–210, 2021.

H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods
under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 795–811. Springer, 2016.

S. P. Karimireddy, L. He, and M. Jaggi. Learning from history for byzantine robust optimization. In

International Conference on Machine Learning, pages 5311–5319. PMLR, 2021.

S. P. Karimireddy, L. He, and M. Jaggi. Byzantine-robust learning on heterogeneous datasets via

bucketing. International Conference on Learning Representations, 2022.

E. Kijsipongse, A. Piyatumrong, et al. A hybrid gpu cluster and volunteer computing platform for

scalable deep learning. The Journal of Supercomputing, 74(7):3236–3263, 2018.

A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer
(bit): General visual representation learning. In European conference on computer vision, pages
491–507. Springer, 2020.

A. Koloskova, S. Stich, and M. Jaggi. Decentralized stochastic optimization and gossip algorithms
In International Conference on Machine Learning, pages

with compressed communication.
3478–3487. PMLR, 2019.

J. Koneˇcný, H. B. McMahan, F. Yu, P. Richtárik, A. T. Suresh, and D. Bacon. Federated learning:
strategies for improving communication efﬁciency. In NIPS Private Multi-Party Machine Learning
Workshop, 2016.

D. Kovalev, A. Koloskova, M. Jaggi, P. Richtarik, and S. Stich. A linearly convergent algorithm for
decentralized optimization: Sending less bits for free! In International Conference on Artiﬁcial
Intelligence and Statistics, pages 4087–4095. PMLR, 2021.

G. Lan and Y. Zhou. An optimal randomized incremental gradient method. Mathematical program-

ming, 171(1):167–215, 2018.

G. Lan, Z. Li, and Y. Zhou. A uniﬁed variance-reduced accelerated gradient method for convex

optimization. Advances in Neural Information Processing Systems, 32, 2019.

C. Li. Demystifying gpt-3 language model: A technical overview, 2020. "https://lambdalabs.

com/blog/demystifying-gpt-3".

Z. Li and P. Richtárik. Canita: Faster rates for distributed convex optimization with communication

compression. Advances in Neural Information Processing Systems, 34, 2021.

Z. Li, D. Kovalev, X. Qian, and P. Richtarik. Acceleration for compressed gradient descent in
distributed and federated optimization. In International Conference on Machine Learning, pages
5895–5904. PMLR, 2020.

Z. Li, H. Bao, X. Zhang, and P. Richtárik. PAGE: A simple and optimal probabilistic gradient
estimator for nonconvex optimization. In International Conference on Machine Learning, pages
6286–6295. PMLR, 2021.

C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear

systems and neural networks. Applied and Computational Harmonic Analysis, 2022.

S. Łojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les équations aux

dérivées partielles, 117:87–89, 1963.

L. Lyu, H. Yu, X. Ma, L. Sun, J. Zhao, Q. Yang, and P. S. Yu. Privacy and robustness in federated

learning: Attacks and defenses. arXiv preprint arXiv:2012.06337, 2020.

B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient
learning of deep networks from decentralized data. In Artiﬁcial intelligence and statistics, pages
1273–1282. PMLR, 2017.

12

H. Mikami, H. Suganuma, Y. Tanaka, Y. Kageyama, et al. Massively distributed sgd: Imagenet/resnet-

50 training in a ﬂash. arXiv preprint arXiv:1811.05233, 2018.

K. Mishchenko, E. Gorbunov, M. Takáˇc, and P. Richtárik. Distributed learning with compressed

gradient differences. arXiv preprint arXiv:1901.09269, 2019.

Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization, 22(2):341–362, 2012.

L. Nguyen, P. H. Nguyen, M. Dijk, P. Richtárik, K. Scheinberg, and M. Takác. Sgd and hogwild!
convergence without the bounded gradients assumption. In International Conference on Machine
Learning, pages 3750–3758. PMLR, 2018.

L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takáˇc. Sarah: A novel method for machine learning
problems using stochastic recursive gradient. In International Conference on Machine Learning,
pages 2613–2621. PMLR, 2017.

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems, 32, 2019.

P. Patarasuk and X. Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations.

Journal of Parallel and Distributed Computing, 69(2):117–124, 2009.

J. Peng, W. Li, and Q. Ling. Byzantine-robust decentralized stochastic optimization over static and

time-varying networks. Signal Processing, 183:108020, 2021.

C. Philippenko and A. Dieuleveut. Preserved central model for faster bidirectional compression in

distributed settings. Advances in Neural Information Processing Systems, 34, 2021.

K. Pillutla, S. M. Kakade, and Z. Harchaoui. Robust aggregation for federated learning. IEEE

Transactions on Signal Processing, 70:1142–1154, 2022.

B. T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathemat-

ics and Mathematical Physics, 3(4):864–878, 1963.

B. T. Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational

mathematics and mathematical physics, 4(5):1–17, 1964.

X. Qian, Z. Qu, and P. Richtárik. SAGA with arbitrary sampling. In International Conference on

Machine Learning, pages 5190–5199. PMLR, 2019.

X. Qian, Z. Qu, and P. Richtárik. L-SVRG and L-Katyusha with arbitrary sampling. Journal of

Machine Learning Research, 22:1–49, 2021a.

X. Qian, P. Richtárik, and T. Zhang. Error compensated distributed sgd can be accelerated. Advances

in Neural Information Processing Systems, 34, 2021b.

Z. Qu and P. Richtárik. Coordinate descent with arbitrary sampling I: Algorithms and complexity.

Optimization Methods and Software, 31(5):829–857, 2016.

S. Rajput, H. Wang, Z. Charles, and D. Papailiopoulos. Detox: A redundancy-based framework for
faster and more robust gradient aggregation. Advances in Neural Information Processing Systems,
32, 2019.

J. Regatti, H. Chen, and A. Gupta. ByGARS: Byzantine SGD with arbitrary number of attackers.

arXiv preprint arXiv:2006.13421, 2020.

P. Richtárik and M. Takáˇc. On optimal probabilities in stochastic coordinate descent methods.

Optimization Letters, 10(6):1233–1243, 2016.

P. Richtárik, I. Sokolov, and I. Fatkhullin. EF21: A new, simpler, theoretically better, and practically

faster error feedback. Advances in Neural Information Processing Systems, 34, 2021.

L. Roberts. Picture coding using pseudo-random noise. IRE Transactions on Information Theory, 8

(2):145–154, 1962.

N. Rodríguez-Barroso, E. Martínez-Cámara, M. Luzón, G. G. Seco, M. Á. Veganzones, and F. Her-
rera. Dynamic federated learning model for identifying adversarial clients. arXiv preprint
arXiv:2007.15030, 2020.

M. Safaryan, R. Islamov, X. Qian, and P. Richtárik. Fednl: Making newton-type methods applicable

to federated learning. arXiv preprint arXiv:2106.02969, 2021.

13

A. Sahu, A. Dutta, A. M Abdelmoniem, T. Banerjee, M. Canini, and P. Kalnis. Rethinking gradient
sparsiﬁcation as total error minimization. Advances in Neural Information Processing Systems, 34,
2021.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming, 162(1):83–112, 2017.

F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its applica-
tion to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the
International Speech Communication Association. Citeseer, 2014.

S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsiﬁed sgd with memory. Advances in Neural

Information Processing Systems, 31, 2018.

A. T. Suresh, X. Y. Felix, S. Kumar, and H. B. McMahan. Distributed mean estimation with limited
communication. In International Conference on Machine Learning, pages 3329–3337. PMLR,
2017.

R. Szlendak, A. Tyurin, and P. Richtárik. Permutation compressors for provably faster distributed

nonconvex optimization. arXiv preprint arXiv:2110.03300, 2021.

H. Tang, C. Yu, X. Lian, T. Zhang, and J. Liu. DoubleSqueeze: Parallel stochastic gradient descent
In International Conference on Machine

with double-pass error-compensated compression.
Learning, pages 6155–6165, 2019.

T. Vogels, S. P. Karimireddy, and M. Jaggi. Powersgd: Practical low-rank gradient compression for

distributed optimization. Advances in Neural Information Processing Systems, 32, 2019.

E. Weiszfeld. Sur le point pour lequel la somme des distances de n points donnés est minimum.

Tohoku Mathematical Journal, First Series, 43:355–386, 1937.

W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad: Ternary gradients to reduce
communication in distributed deep learning. Advances in neural information processing systems,
30, 2017.

Z. Wu, Q. Ling, T. Chen, and G. B. Giannakis. Federated variance-reduced stochastic gradient descent
with robustness to byzantine attacks. IEEE Transactions on Signal Processing, 68:4583–4596,
2020.

C. Xie, O. Koyejo, and I. Gupta. Fall of empires: Breaking byzantine-tolerant sgd by inner product

manipulation. In Uncertainty in Artiﬁcial Intelligence, pages 261–270. PMLR, 2020.

X. Xu and L. Lyu. Towards building a robust and fair federated learning system. arXiv preprint

arXiv:2011.10464, 2020.

Z. Yang and W. U. Bajwa. Bridge: Byzantine-resilient decentralized gradient descent. arXiv preprint

arXiv:1908.08098, 2019a.

Z. Yang and W. U. Bajwa. Byrdie: Byzantine-resilient distributed coordinate descent for decentralized
learning. IEEE Transactions on Signal and Information Processing over Networks, 5(4):611–627,
2019b.

D. Yin, Y. Chen, R. Kannan, and P. Bartlett. Byzantine-robust distributed learning: Towards optimal
statistical rates. In International Conference on Machine Learning, pages 5650–5659. PMLR,
2018.

Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and
C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. International
Conference on Learning Representations, 2020.

H. Zhu and Q. Ling. Broadcast: Reducing both stochastic and compression noise to robustify

communication-efﬁcient federated learning. arXiv preprint arXiv:2104.06685, 2021.

M. Zinkevich, M. Weimer, L. Li, and A. Smola. Parallelized stochastic gradient descent. Advances in

neural information processing systems, 23, 2010.

14

Contents

1 Introduction

1.1 Our Contributions .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Byz-VR-MARINA: Byzantine-Tolerant Variance Reduction with Communication Com-

pression

3 Numerical Experiments

A Detailed Related Work

B Extra Experiments and Experimental details

B.1 General setup .

.

.

.

B.2 Experimental setup .

B.3 Extra Experiments

.

.

.

.

B.3.1 Compression .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3.2 Linear convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3.3 Extra dataset: w8a . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Useful Facts

D Further Details on Robust Aggregation

E Missing Proofs and Details From Section 2

E.1 General Non-Convex Functions

. . . . . . . . . . . . . . . . . . . . . . . . . . .

E.2 Functions Satisfying Polyak-Łojasiewicz Condition . . . . . . . . . . . . . . . . .

1

3

5

9

15

17

17

17

19

19

19

19

20

21

23

26

30

A Detailed Related Work

Byzantine-robustness. Classical approaches to Byzantine-tolerant optimization are based on ap-
plying special aggregation rules to Parallel-SGD [Blanchard et al., 2017, Chen et al., 2017, Yin et al.,
2018, Damaskinos et al., 2019, Guerraoui et al., 2018, Pillutla et al., 2022]. It turns out that such
defences are vulnerable to the special type of attacks [Baruch et al., 2019, Xie et al., 2020]. Moreover,
Karimireddy et al. [2021] propose a reasonable formalism for describing robust aggregation rules
(see Def. 2.1) and show that almost all previously known defences are not robust according to this
formalism. In addition, they propose and analyze new Byzantine-tolerant methods based on the usage
of Polyak’s momentum [Polyak, 1964] (BR-SDGm) and momentum variance reduction [Cutkosky
and Orabona, 2019] (BR-MVR). This approach is extended to the case of heterogeneous data and
aggregators agnostic to the noise level by Karimireddy et al. [2022], and He et al. [2022] propose an
extension to the decentralized optimization over ﬁxed networks. Gorbunov et al. [2021a] propose an
alternative approach based on the usage of AllReduce [Patarasuk and Yuan, 2009] with additional
veriﬁcations of correctness and show that their algorithm has complexity not worse than Parallel-
SGD when the target accuracy is small enough. Wu et al. [2020] are the ﬁrst who applied variance
reduction mechanism to tolerate Byzantine attacks (see the discussion above Q1). We also refer
reader to [Chen et al., 2018, Rajput et al., 2019, Rodríguez-Barroso et al., 2020, Xu and Lyu, 2020,
Alistarh et al., 2018, Allen-Zhu et al., 2021, Regatti et al., 2020, Yang and Bajwa, 2019a,b, Gupta
et al., 2021, Gupta and Vaidya, 2021, Peng et al., 2021] for other advances in Byzantine-robustness
(see the detailed summaries in [Lyu et al., 2020, Gorbunov et al., 2021a]). We further progress the
ﬁeld by obtaining new theoretical SOTA convergence results in our work.

15

Compressed communications. Methods with compression are relatively well studied in the lit-
erature. The ﬁrst theoretical results were derived in [Alistarh et al., 2017, Wen et al., 2017, Stich
et al., 2018, Mishchenko et al., 2019]. During the last several years the ﬁeld has been signiﬁcantly
developed. In particular, compressed methods are analyzed in the conjuction with variance reduc-
tion [Horváth et al., 2019b, Gorbunov et al., 2020b, Danilova and Gorbunov, 2022], acceleration [Li
et al., 2020, Li and Richtárik, 2021, Qian et al., 2021b], decentralized communications [Koloskova
et al., 2019, Kovalev et al., 2021], local steps [Basu et al., 2019, Haddadpour et al., 2021], adaptive
compression [Faghri et al., 2020], second-order methods [Islamov et al., 2021, Safaryan et al., 2021],
and min-max optimization [Beznosikov et al., 2021, 2022]. However, to our knowledge, only one
work studies communication compression in the context of Byzantine-robustness [Zhu and Ling,
2021] (see the discussion above Q2). Our work makes a further step towards closing this signiﬁcant
gap in the literature.

Variance reduction is a powerful tool allowing to speed up the convergence of stochastic methods
(especially when one needs to achieve a good approximation of the solution). The ﬁrst variance-
reduced methods were proposed by Schmidt et al. [2017], Johnson and Zhang [2013], Defazio et al.
[2014]. Optimal variance-reduced methods for (strongly) convex problems are proposed in [Lan
and Zhou, 2018, Allen-Zhu, 2017, Lan et al., 2019] and for non-convex optimization in [Nguyen
et al., 2017, Fang et al., 2018, Li et al., 2021]. Despite the noticeable attention to these kinds of
methods [Gower et al., 2020], only a few papers study Byzantine-robustness in conjunction with
variance reduction [Wu et al., 2020, Zhu and Ling, 2021, Karimireddy et al., 2021]. Moreover, as
we mentioned before, the results from Wu et al. [2020], Zhu and Ling [2021] are not better than
the known ones for non-parallel variance-reduced methods, and Karimireddy et al. [2021] rely on
the uniformly bounded variance assumption, which is hard to achieve in practice. In our work, we
circumvent these limitations.

Non-uniform sampling. Originally proposed for randomized coordinate methods [Nesterov, 2012,
Richtárik and Takáˇc, 2016, Qu and Richtárik, 2016], non-uniform sampling is extended in multiple
ways to stochastic optimization, e.g., see [Horváth and Richtárik, 2019, Gower et al., 2019, Qian et al.,
2019, Gorbunov et al., 2020b,a, Qian et al., 2021a]. Typically, non-uniform sampling of stochastic
gradients allows better dependence on smoothness constants in the theoretical results. Inspired by
these advances, we propose the ﬁrst Byzantine-robust optimization method supporting non-uniform
sampling of stochastic gradients.

16

Figure 2: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on a9a dataset with uniform split over 15 workers with 5 Byzantines. The
top row displays the best performance in hindsight for a given attack. Each method uses RandK
sparsiﬁcation with K = 0.1d.

Figure 3: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on a9a dataset, where each worker access full dataset with 4 good and 1
Byzantine workers. In the ﬁrst row, we do not use any compression, in the second row each method
uses RandK sparsiﬁcation with K = 0.1d.

B Extra Experiments and Experimental details

B.1 General setup

Our running environment has the following setup:

• 24 CPUs: Intel(R) Xeon(R) Gold 6146 CPU @ 3.20GHz ,

• GPU: NVIDIA TITAN Xp with CUDA version 11.3,

• PyTorch version: 1.11.0.

B.2 Experimental setup

For each experiment, we tune the step size using the following set of candidates {0.5, 0.05, 0.005}.
The step size is ﬁxed. We do not use learning rate warmup or decay. We use batches of size 32
for all methods. Each experiment is run with three varying random seeds, and we report the mean
optimality gap with one standard error. The optimal value is obtained by running gradient descent

17

010203040epochs10−510−410−310−210−1f(x)−f*BEST | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*BEST | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−310−210−1100101f(x)−f*BEST | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*BEST | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*BEST | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*AVG | NASGDBR-DIANAByz-VR-MARINA010203040epochs2×10−13×10−14×10−1f(x)−f*AVG | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−110310710111015f(x)−f*AVG | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*AVG | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*AVG | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*CM | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−310−210−1100101102f(x)−f*CM | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*RFA | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*RFA | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−310−210−1100101f(x)−f*RFA | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*RFA | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*RFA | IPMSGDBR-DIANAByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | NASGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | LFSGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | BFSGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | ALIESGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | IPMSGDBR-SGDmByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*CM | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*CM | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*CM | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*CM | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*CM | IPMSGDBR-DIANAByz-VR-MARINAFigure 4: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on w8a dataset with uniform split over 15 workers with 5 Byzantines. The
top row displays the best performance in hindsight for a given attack. No compression is applied.

Figure 5: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on w8a dataset with uniform split over 15 workers with 5 Byzantines. The
top row displays the best performance in hindsight for a given attack. Each method uses RandK
sparsiﬁcation with K = 0.1d.

18

05101520epochs10−710−510−310−1f(x)−f*AVG | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*CM | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*RFA | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*RFA | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*RFA | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*AVG | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−210−1f(x)−f*AVG | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*AVG | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*AVG | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*AVG | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*CM | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*CM | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−510−410−310−210−1f(x)−f*CM | IPMSGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*RFA | NASGDBR-SGDmByz-VR-MARINA05101520epochs10−310−210−1f(x)−f*RFA | LFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*RFA | BFSGDBR-SGDmByz-VR-MARINA05101520epochs10−610−510−410−310−210−1f(x)−f*RFA | ALIESGDBR-SGDmByz-VR-MARINA05101520epochs10−710−510−310−1f(x)−f*RFA | IPMSGDBR-SGDmByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*AVG | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*RFA | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−310−1101f(x)−f*RFA | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1100f(x)−f*RFA | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*AVG | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*AVG | NASGDBR-DIANAByz-VR-MARINA010203040epochs2×10−13×10−14×10−16×10−1f(x)−f*AVG | LFSGDBR-DIANAByz-VR-MARINA010203040epochs100106101210181024f(x)−f*AVG | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1100f(x)−f*AVG | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−510−410−310−210−1f(x)−f*AVG | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*CM | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−2100102104106108f(x)−f*CM | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1100f(x)−f*CM | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | IPMSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*RFA | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−1f(x)−f*RFA | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−310−1101f(x)−f*RFA | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1100f(x)−f*RFA | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*RFA | IPMSGDBR-DIANAByz-VR-MARINAFigure 6: The optimality gap f (xk) − f (x∗) of 3 aggregation rules (AVG, CM, RFA) under 5 attacks
(NA, LF, BF, ALIE, IPM) on w8a dataset, where each worker access full dataset with 4 good and 1
Byzantine workers. In the ﬁrst row, we do not use any compression, in the second row each method
uses RandK sparsiﬁcation with K = 0.1d.

(GD) on the complete dataset for 1000 epochs. Our implementation of attacks and robust aggregation
schemes is based on the public implementation from [Karimireddy et al., 2022] available at https:
//github.com/epfml/byzantine-robust-noniid-optimizer. Our codes are available via
the Github repository at https://github.com/SamuelHorvath/VR_Byzantine. We select the
same set of hyperparameters as [Karimireddy et al., 2022], i.e.,

• RFA: the number of steps of smoothed Weisﬁeld algorithm T = 8; see Section D for details,
• ALIE: a small constant that controls the strength of the attack z is chosen according to

[Baruch et al., 2019],

• IPM: a small constant that controls the strength of the attack (cid:15) = 0.1.

B.3 Extra Experiments

B.3.1 Compression

In this section, we consider the same setup as for the previous experiment in the main paper with a
difference that we employ communication compression. We choose random unbiased sparsiﬁcation
for with sparsity level 10%. We compare our Byz-VR-MARINA algorithm to compressed SGD and
DIANA (BR-DIANA).
Discussion. In Figure 2, we can see that Byz-VR-MARINA consistently outperforms both baselines
except for the bit ﬂipping attack. However, even in this case, it seems that Byz-VR-MARINA only
needs more epochs to provide the better solution while SGD cannot further improve regardless of the
number of epochs.

B.3.2 Linear convergence

In this experiment, we focus on another important feature of Byz-VR-MARINA: it guarantees linear
convergence for homogeneous datasets across clients even in the presence of Byzantine workers, as
shown in Theorem 2.2. To demonstrate this experimentally, we consider the same setup as for the
previous experiments with the difference that we have four good workers and one Byzantine, each
worker can access the entire dataset, and the server uses coordinate-wise median with bucketing as
the aggregator. Figure 3 showcases that, indeed, we observe linear convergence of our method while
no baseline achieves this fast rate. In the ﬁrst row, we display methods with no compression, and in
the second row, each algorithm uses random sparsiﬁcation.

B.3.3 Extra dataset: w8a

In Figures 4–6, we perform the same experiments, but for the different LIBSVM dataset:w8a. We
note that the obtained results are consistent with our observations for the a9a dataset.

19

0246810epochs10−610−410−2f(x)−f*CM | NASGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | LFSGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | BFSGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | ALIESGDBR-SGDmByz-VR-MARINA0246810epochs10−610−410−2f(x)−f*CM | IPMSGDBR-SGDmByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | NASGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | LFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | BFSGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | ALIESGDBR-DIANAByz-VR-MARINA010203040epochs10−410−310−210−1f(x)−f*CM | IPMSGDBR-DIANAByz-VR-MARINAC Useful Facts

For all a, b ∈ Rd and α > 0, p ∈ (0, 1] the following relations hold:

2(cid:104)a, b(cid:105) = (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2,

(cid:107)a + b(cid:107)2 ≤ (1 + α)(cid:107)a(cid:107)2 + (1 + α−1)(cid:107)b(cid:107)2,

−(cid:107)a − b(cid:107)2 ≤ −

(1 − p)

(cid:16)

1 +

(cid:17)

p
2

≤ 1 −

1
1 + α
p
2

.

(cid:107)a(cid:107)2 +

1
α

(cid:107)b(cid:107)2,

(12)

(13)

(14)

(15)

Lemma C.1 (Lemma 5 from Richtárik et al. [2021]). Let a, b > 0. If 0 ≤ γ ≤ 1√

a+b , then
a+b .

≤ 2√

(cid:111)

(cid:110) 1√

a , 1

b

aγ2 + bγ ≤ 1. The bound is tight up to the factor of 2 since

1√
a+b ≤ min

20

D Further Details on Robust Aggregation

In Section 2, we consider robust aggregation rules satisfying Deﬁnition 2.1. As we notice, this
deﬁnition slightly differs from the original one introduced by Karimireddy et al. [2022]. In particular,
we assume

1
G(G − 1)

(cid:88)

i,l∈G

E (cid:2)(cid:107)xi − xl(cid:107)2(cid:3) ≤ σ2,

(16)

while Karimireddy et al. [2022] uses E[(cid:107)xi − xl(cid:107)2] ≤ σ2 for any ﬁxed i, l ∈ G. As we show next,
this difference is very subtle and condtion (16) also allows to achieve robustness.

We consider robust aggregation via bucketing proposed by Karimireddy et al. [2022] (see Algorithm 2).
This algorithm can robustify some non-robust aggregation rules Aggr. In particular, Karimireddy

Algorithm 2 Bucketing: Robust Aggregation using bucketing [Karimireddy et al., 2022]
1: Input: {x1, . . . , xn}, s ∈ N – bucket size, Aggr – aggregation rule
2: Sample random permutation π = (π(1), . . . , π(n)) of [n]
3: Compute yi = 1
k=s(i−1)+1 xπ(k) for i = 1, . . . , (cid:100)n/s(cid:101)
s
4: Return: (cid:98)x = Aggr(y1, . . . , y(cid:100)n/s(cid:101))

(cid:80)min{si,n}

et al. [2022] show that Algorithm 2 makes Krum [Blanchard et al., 2017], Robust Federated
Averaging (RFA) [Pillutla et al., 2022] (also known as geometric median), and Coordinate-wise
Median (CM) [Chen et al., 2017] robust, in view of deﬁnition from Karimireddy et al. [2022].

Our main goal in this section is to show that Krum ◦ Bucketing, RFA ◦ Bucketing, and CM ◦
Bucketing satisfy Deﬁnition 2.1. Before we prove this fact, we need to introduce Krum, RFA, CM.

Krum. Let Si ⊆ {x1, . . . , xn} be the subset of n − |B| − 2 closest vectors to xi. Then, Krum-
estimator is deﬁned as

Krum(x1, . . . , xn) def= argmin

xi∈{x1,...,xn}

(cid:107)xj − xi(cid:107)2.

(cid:88)

j∈Si

(17)

Krum requires computing all pair-wise distances of vectors from {x1, . . . , xn} resulting in O(n2)
computation cost for the server. Therefore, Krum is computationally expensive, when number of
workers n is large.

Robust Federated Averaging. RFA-estimator ﬁnds a geometric median:

RFA(x1, . . . , xn) def= argmin
x∈Rd

n
(cid:88)

i=1

(cid:107)x − xi(cid:107).

(18)

The above problem has no closed form solution. However, one can compute approximate RFA using
several steps of smoothed Weiszfeld algorithm having O(n) computation cost of each iteration
[Weiszfeld, 1937, Pillutla et al., 2022].

Coordinate-wise Median. CM-estimator computes a median of each component separately. That
is, for t-th coordinate it is deﬁned as

[CM(x1, . . . , xn)]t

def= Median([x1]t, . . . , [xn]t) = argmin

u∈R

n
(cid:88)

i=1

|u − [xi]t|,

(19)

where [x]t is t-th coordinate of vector x ∈ Rd. CM has O(n) computation cost [Chen et al., 2017, Yin
et al., 2018].

Robustness via bucketing. The following lemma is the key to show robustness of Krum ◦
Bucketing, RFA ◦ Bucketing, and CM ◦ Bucketing in terms of Deﬁnition 2.1.

21

Lemma D.1 (Modiﬁcation of Lemma 1 from [Karimireddy et al., 2022]). Assume that
{x1, x2, . . . , xn} is such that there exists a subset G ⊆ [n], |G| = G ≥ (1 − δ)n and σ ≥ 0
such that (16) holds. Let vectors {y1, . . . , yN }, N = n/s4 be generated by Algorithm 2 and
(cid:101)G = {i ∈ [N ] | Bi ⊆ G}, where yi = 1
xj and Bi denotes the i-th bucket, i.e.,
|Bi|
Bi = {π((i − 1) · s + 1), . . . , π(min{i · s, n})}. Then, | (cid:101)G| = (cid:101)G ≥ (1 − δs)N and for any ﬁxed
i, l ∈ (cid:101)G we have

j∈Bi

(cid:80)

E[yi] = E[x] and E (cid:2)(cid:107)yi − yl(cid:107)2(cid:3) ≤

σ2
s

,

(20)

where x = 1
G

(cid:80)

i∈G xi.

Proof. The proof is almost identical to the proof of Lemma 1 from [Karimireddy et al., 2022].
Nevertheless, for the sake of mathematical rigor, we provide a complete proof. Since each Byzantine
peer is contained in no more than 1 bucket and |B| ≤ δn (here B = [n] \ G), we have that number of
“bad” buckets is not greater than δn ≤ δsN , i.e., (cid:101)G ≥ (1 − δs)N . Next, for any ﬁxed i ∈ (cid:101)G we have

Eπ[yi | i ∈ (cid:101)G] =

1
|Bi|

(cid:88)

j∈Bi

Eπ [xj | j ∈ G] =

1
G

(cid:88)

j∈G

xj = x,

where Eπ[·] denotes the expectation w.r.t. the randomness comming from the permutation. Taking the
full expectation, we obtain the ﬁrst part of (20). To derive the second part we notice that for any ﬁxed
i, l ∈ (cid:101)G the (ordered) pairs {(xk1, xk2 )}k1∈Bi,k2∈Bl are i.i.d. pairs of random vectors. Therefore, we
have

E (cid:2)(cid:107)yi − yl(cid:107)2(cid:3) = E






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
s

(cid:88)

(xk1 − xk2)

k1∈Bi,k2∈Bl

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2




=

=

E (cid:2)(cid:107)xk1 − xk2 (cid:107)2(cid:3) =

1
s

1
sG(G − 1)

(cid:88)

E (cid:2)(cid:107)xi1 − xi2 (cid:107)2(cid:3)

i1,i2∈G
i1(cid:54)=i2

1
sG(G − 1)

(cid:88)

E (cid:2)(cid:107)xi1 − xi2(cid:107)2(cid:3) (16)
≤

i1,i2∈G

σ2
s

,

which concludes the proof.

Using the above lemma, we get the following result.
Theorem D.1 (Modiﬁcation of Theorem I from [Karimireddy et al., 2022]). Let {x1, x2, . . . , xn}
satisfy the conditions of Lemma D.1 for some δ ≤ δmax. If Algorithm 2 is run with s = (cid:98)δmax/δ(cid:99), then

• Krum ◦ Bucketing satisﬁes Deﬁnition 2.1 with c = O(1) and δmax < 1/4,

• RFA ◦ Bucketing satisﬁes Deﬁnition 2.1 with c = O(1) and δmax < 1/2,

• CM ◦ Bucketing satisﬁes Deﬁnition 2.1 with c = O(d) and δmax < 1/2.

Proof. The proof is identical to the proof of Theorem I from [Karimireddy et al., 2022], since
Karimireddy et al. [2022] rely only on the general properties of Krum/RFA/CM and (20) to get the
result.

4For simplicity, we assume that n is divisible by s.

22

E Missing Proofs and Details From Section 2

We need the following lemma, wich is often used for analyzing SGD-like methods in the non-convex
case.

Lemma E.1 (Lemma 2 from Li et al. [2021]). Assume that function f is L-smooth and xk+1 =
xk − γgk. Then

f (xk+1) ≤ f (xk) −

γ
2

(cid:107)∇f (xk)(cid:107)2 −

(cid:18) 1
2γ

−

(cid:19)

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2

(cid:107)gk − ∇f (xk)(cid:107)2.

(21)

To estimate the “quality” of robust aggregation at iteration k + 1 we derive an upper bound for the
averaged pairwise variance of estimators obtained by good peers (see also Deﬁnition 2.1).

Lemma E.2 (Bound on the variance). Let Assumptions 2.1, 2.2, 2.3, 2.4 hold. Then for all k ≥ 0 the
iterates produced by Byz-VR-MARINA satisfy

1
G(G − 1)

(cid:88)

i,l∈G

E (cid:2)(cid:107)gk+1

i − gk+1

l

(cid:107)2(cid:3) ≤ A(cid:48)E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + 8BpE(cid:107)∇f (xk)(cid:107)2 + 4pζ 2,

(22)

where A(cid:48) =

(cid:16)

8BpL2 + 4(1 − p)

(cid:16)

ωL2 + (1 + ω)L2

± +

(1+ω)L2
±
b

(cid:17)(cid:17)

.

Proof. For the compactness, we introduce new notation: ∆k = ∇f (xk+1) − ∇f (xk). Then, by
deﬁnition of gk+1

for i ∈ G we have

i

1
G(G − 1)

(cid:88)

i,l∈G

E (cid:2)(cid:107)gk+1

i − gk+1

l

(cid:107)2(cid:3) =

p
G(G − 1)

(cid:88)

i,l∈G

(cid:124)

E (cid:2)(cid:107)∇fi(xk+1) − ∇fl(xk+1)(cid:107)2(cid:3)

+

1 − p
G(G − 1)

(cid:88)

E

i,l∈G

(cid:124)

(cid:123)(cid:122)
T1
(cid:104)

(cid:107)Q( (cid:98)∆k

i ) − Q( (cid:98)∆k

l )(cid:107)2(cid:105)

(cid:125)

. (23)

(cid:123)(cid:122)
T2

(cid:125)

Term T1 can be bounded via Assumption 4:

T1

=

(13)
≤

=

(4)
≤

(13)
≤

p
G(G − 1)

p
G(G − 1)

(cid:88)

i,l∈G
i(cid:54)=l

(cid:88)

i,l∈G
i(cid:54)=l

E (cid:2)(cid:107)∇fi(xk+1) − ∇fl(xk+1)(cid:107)2(cid:3)

E (cid:2)2(cid:107)∇fi(xk+1) − ∇f (xk+1)(cid:107)2 + 2(cid:107)∇fl(xk+1) − ∇f (xk+1)(cid:107)2(cid:3)

4p
G

(cid:88)

i∈G

E (cid:2)(cid:107)∇fi(xk+1) − ∇f (xk+1)(cid:107)2(cid:3)

4BpE (cid:2)(cid:107)∇f (xk+1)(cid:107)2(cid:3) + 4pζ 2

8BpE (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) + 8BpE (cid:2)(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2(cid:3) + 4pζ 2

As. 2.1
≤

8BpE (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) + 8BpL2E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + 4pζ 2.

23

Next, we estimate T2:
1 − p
G(G − 1)

T2 =

=

1 − p
G(G − 1)

(cid:88)

E

i,l∈G
i(cid:54)=l
(cid:88)

i,l∈G
i(cid:54)=l

E

(cid:104)
(cid:107)Q( (cid:98)∆k

i ) − Q( (cid:98)∆k

l )(cid:107)2(cid:105)

(cid:104)
(cid:107)Q( (cid:98)∆k

i ) − ∆k

i − (Q( (cid:98)∆k

l ) − ∆k

l )(cid:107)2(cid:105)

+

1 − p
G(G − 1)

(13)
≤

1 − p
G(G − 1)

(cid:88)

i,l∈G
i(cid:54)=l

+

1 − p
G(G − 1)

(cid:88)

E (cid:2)(cid:107)∆k

i − ∆k

l (cid:107)2(cid:3)

i,l∈G
i(cid:54)=l
(cid:104)
2(cid:107)Q( (cid:98)∆k

E

i ) − ∆k

i (cid:107)2 + 2(cid:107)Q( (cid:98)∆k

l ) − ∆k

l (cid:107)2(cid:105)

(cid:88)

E (cid:2)2(cid:107)∆k

i − ∆k(cid:107)2 + 2(cid:107)∆k

l − ∆k(cid:107)2(cid:3)

i,l∈G
i(cid:54)=l
(cid:104)

4(1 − p)
G

4(1 − p)
G

(cid:88)

E

i∈G

(cid:88)

i∈G

(cid:104)

E

=

(3)
≤

=

(cid:107)Q( (cid:98)∆k

i )(cid:107)2 − (cid:107)∆k

i (cid:107)2(cid:105)

+

4(1 − p)
G

(cid:88)

E (cid:2)(cid:107)∆k

i (cid:107)2 − (cid:107)∆k(cid:107)2(cid:3)

(1 + ω)(cid:107) (cid:98)∆k

i (cid:107)2 − (cid:107)∆k

i (cid:107)2(cid:105)

+

(cid:88)

E (cid:2)(cid:107)∆k

i (cid:107)2 − (cid:107)∆k(cid:107)2(cid:3)

i∈G
4(1 − p)
G

i∈G
4(1 − p)(1 + ω)
G

4(1 − p)(1 + ω)
G

(cid:88)

E

i∈G

(cid:104)
(cid:107) (cid:98)∆k

i − ∆k

i (cid:107)2(cid:105)

+

E (cid:2)(cid:107)∆k

i − ∆k(cid:107)2(cid:3)

(cid:88)

i∈G

+4(1 − p)ωE (cid:2)(cid:107)∆k(cid:107)2(cid:3) .

Applying Assumptions 2.4, 2.3, and 2.1, we get

T2 ≤ 4(1 − p)

(cid:18)

ωL2 + (1 + ω)

(cid:19)(cid:19)

(cid:18)

L2

± +

L2
±
b

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) .

Plugging the upper bounds for T1 and T2 in (23), we obtain the result.

Using the above lemma, we derive the following technical result, which we rely on in the proofs of
the main results.
Lemma E.3 (Bound on the distortion). Let Assumptions 2.1, 2.2, 2.3, 2.4 hold. Then for all k ≥ 0
the iterates produced by Byz-VR-MARINA satisfy

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(cid:16)

1 −

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) + 24BcδE(cid:107)∇f (xk)(cid:107)2 + 12cδζ 2

+

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) ,

(24)

where A = 48BL2cδ

p

+ 6(1−p)
p

(cid:16) 4cδ

p + 1

2G

(cid:17) (cid:16)

ωL2 +

(cid:17)

(1+ω)L2
±
b

+ 6(1−p)
p

(cid:16) 4cδ(1+ω)
p

+ ω
2G

(cid:17)

L2
±.

Proof. For convenience, we intoduce the following notation:

gk+1 = 1
G

gk+1
i =

(cid:88)

i∈G

Using the introduced notation, we derive

(cid:40)

∇f (xk+1),
(cid:80)
gk + 1
G

if ck = 1,
i ), otherwise.

i∈G Q( (cid:98)∆k

(25)

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(cid:16)

(13)
≤

(cid:17)

1 +

p
2
(cid:18)

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(cid:19)

2
p

E (cid:2)(cid:107)gk+1 − gk+1(cid:107)2(cid:3) .

(26)

+

1 +

24

(cid:17)

p
2
Ap
4

Next, we need to upper-bound the terms from the right-hand side of (26). Applying the variance
decomposition and independence of mini-batch and quantization computations on different workers,
we get

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)



(25)= (1 − p)E

(cid:13)
(cid:13)
gk +
(cid:13)
(cid:13)
(cid:13)
= (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

1
G

(cid:88)

i∈G



Q( (cid:98)∆k

i ) − ∇f (xk+1)

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+(1 − p)E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
G

(cid:88)

i∈G

(Q( (cid:98)∆k

(cid:13)
(cid:13)
i ) − ∆k
(cid:13)
i )
(cid:13)
(cid:13)

2



= (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) +

1 − p
G2

(cid:88)

E

i∈G

(cid:20)(cid:13)
(cid:13)Q( (cid:98)∆k
(cid:13)

i ) − ∆k
i

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

.

The deﬁnition of the quantization operator (Deﬁnition 2.2) implies

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) = (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) +

1 − p
G2

(cid:88)

i∈G

(cid:104)

E

(cid:107)Q( (cid:98)∆k

i )(cid:107)2(cid:105)

−

1 − p
G2

(cid:88)

i∈G

E (cid:2)(cid:107)∆k

i (cid:107)2(cid:3)

(3)

≤ (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) +

(1 − p)(1 + ω)
G2

(cid:88)

i∈G

E

(cid:104)

i (cid:107)2(cid:105)

(cid:107) (cid:98)∆k

−

1 − p
G2

(cid:88)

i∈G

E (cid:2)(cid:107)∆k

i (cid:107)2(cid:3)

= (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)
(cid:104)
(1 − p)(1 + ω)
(cid:107) (cid:98)∆k
G2

(cid:88)

+

E

i∈G

i − ∆k

i (cid:107)2(cid:105)

+

(1 − p)ω
G2

(cid:88)

i∈G

E (cid:2)(cid:107)∆k

i (cid:107)2(cid:3)

= (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)
(cid:104)
(1 − p)(1 + ω)
(cid:107) (cid:98)∆k
G2

(cid:88)

+

E

i∈G

i − ∆k

i (cid:107)2(cid:105)

+

(1 − p)ω
G2

(cid:88)

i∈G

E (cid:2)(cid:107)∆k

i − ∆k(cid:107)2(cid:3) +

(1 − p)ω
G

E (cid:2)(cid:107)∆k(cid:107)2(cid:3) .

Using Assumptions 2.4, 2.3, and 2.1, we arrive at

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤ (1 − p)E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(27)

+

1 − p
G

(cid:18)

ωL2 + ωL2

± +

(cid:19)

(1 + ω)L2
±
b

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) .

That is, we obtained an upper bound for the ﬁrst term in the right-hand side of (26). To bound the
second term, we use the deﬁnition of (δ, c)-ARAgg (Deﬁnition 2.1) and Lemma E.2:

E (cid:2)(cid:107)gk+1 − gk+1(cid:107)2(cid:3)

(2)
≤

cδ
G(G − 1)

(cid:88)

i,l∈G

E (cid:2)(cid:107)gk+1

i − gk+1

l

(cid:107)2(cid:3)

(22)

≤ 8Bpcδ + 4pζ 2cδ + A(cid:48)cδE (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) ,

(28)

25

(cid:16)

where A(cid:48) =
and using p ≤ 1, we obtain

8BpL2 + 4(1 − p)

(cid:16)

ωL2 + (1 + ω)L2

± +

(cid:17)(cid:17)

(1+ω)L2
±
b

. Plugging (27) and (28) in (26)

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤ (1 − p)

(cid:16)

1 +

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(cid:17)

p
2
(cid:18)

ωL2 + ωL2

3(1 − p)
2G
(cid:0)8Bpcδ + 4pζ 2cδ + A(cid:48)cδE (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)(cid:1)

(1 + ω)L2
±
b

± +

+

+

(cid:19)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

(cid:16)

(15)
≤

1 −

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) + 24Bcδ + 12ζ 2cδ

3
p

(cid:17)

p
2
Ap
2

+

where

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) ,

2
p

2
p

(cid:18)

A =

=

=

(cid:18)

(cid:18) 3(1 − p)
2G

ωL2 + ωL2

± +

(1 + ω)L2
±
b

(cid:19)

+

3
p

(cid:19)

A(cid:48)cδ

24BL2cδ + 3(1 − p)

(cid:18) 4cδ
p
(cid:18) 4cδ(1 + ω)
p
(cid:18) 4cδ
p
(cid:18) 4cδ(1 + ω)
p

6(1 − p)
p

+

+

+

+

1
2G
ω
2G

(cid:19)

(cid:19) (cid:18)

1
2G
(cid:19)
ω
2G

L2
±.

· 3(1 − p)

+

2
p
48BL2cδ
p
6(1 − p)
p

+

+

L2
±

(cid:19) (cid:18)

ωL2 +

(cid:19)(cid:19)

(1 + ω)L2
±
b

ωL2 +

(cid:19)

(1 + ω)L2
±
b

This concludes the proof.

E.1 General Non-Convex Functions

Theorem E.1 (Theorem 2.1). Let Assumptions 2.1, 2.2, 2.3, 2.4 hold. Assume that

0 < γ ≤

1
√
L +

,

A

δ <

p
48cB

,

(29)

where A = 48BL2cδ
for all K ≥ 0 the iterates produced by Byz-VR-MARINA satisfy

p + 6(1−p)

p + 1

ωL2 +

2G

(1+ω)L2
±
b

p

+ 6(1−p)
p

(cid:16) 4cδ

(cid:17) (cid:16)

(cid:17)

(cid:16) 4cδ(1+ω)
p

+ ω
2G

(cid:17)

L2

±. Then

E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤

(cid:16)

γ

2E[Φ0]
(cid:17)

1 − 48Bcδ

p

(K + 1)

+

24cδζ 2
p − 48Bcδ

,

(30)

where (cid:98)xK is choosen uniformly at random from x0, x1, . . . , xK, and Φ0 = f (x0) − f∗ + γ
∇f (x0)(cid:107)2.

p (cid:107)g0 −

26

Proof. For all k ≥ 0 we introduce Φk = f (xk) − f∗ + γ
Lemmas E.1 and E.3, we derive

p (cid:107)gk − ∇f (xk)(cid:107)2. Using the results of

E[Φk+1]

(21),(24)
≤

(cid:20)
f (xk) − f∗ −

E

(cid:18) 1
2γ

−

(cid:19)

L
2
γ
p

(cid:16)

1 −

(cid:17)

p
2

(cid:107)xk+1 − xk(cid:107)2 +

(cid:107)gk − ∇f (xk)(cid:107)2

γ
2
E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(cid:21)

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

−

+

γ
2
24Bcδγ
p

=

E [Φk] −

(cid:18)

1 −

(cid:19)

48Bcδ
p

γ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

12cδζ 2γ
γA
p
2
12cδζ 2γ
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +
p

−

1
2γ

(cid:0)1 − Lγ − Aγ2(cid:1) E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

≤

E [Φk] −

(cid:18)

1 −

(cid:19)

48Bcδ
p

γ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

12cδζ 2γ
p

,

(cid:16)

where in the last step we use Lemma C.1 and our choice of γ from (29). Next, in view of (29), we
have γ
1 − 48Bcδ
> 0. Therefore, summing up the above inequality for k = 0, 1, . . . , K and
2
rearranging the terms, we get

(cid:17)

p

1
K + 1

K
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

≤

(cid:16)

γ

2
1 − 48Bcδ

(cid:17)

p
24cδζ 2
p − 48Bcδ

+

K
(cid:88)

(K + 1)

k=0

(E[Φk] − E[Φk+1])

=

2 (E[Φ0] − E[ΦK+1])
(cid:16)
(K + 1)
γ

1 − 48Bcδ

(cid:17)

ΦK+1≥0
≤

(cid:16)

γ

p
2E[Φ0]
(cid:17)

1 − 48Bcδ

p

(K + 1)

+

24cδζ 2
p − 48Bcδ

+

24cδζ 2
p − 48Bcδ

.

It remains to notice, that the lef-hand side equals E[(cid:107)∇f ((cid:98)xK)(cid:107)2], where (cid:98)xK is choosen uniformly at
random from x0, x1, . . . , xK.

Before we move on to the corollaries, we ellaborate on the derived upper bound. In particular, it is
important to estimate E[Φ0]. By deﬁnition, Φ0 = f (x0) − f∗ + γ
p (cid:107)g0 − ∇f (x0)(cid:107)2, i.e., Φ0 depends
on the choice of g0. For example, one can ask good workers to compute hi = ∇fi(x0), i ∈ G and
send it to the server. Then, the server can set g0 as g0 = ARAgg(h1, . . . , hn). This gives us

E[Φ0] = f (x0) − f∗ +

E (cid:2)(cid:107)g0 − ∇f (x0)(cid:107)2(cid:3)

γ
p

(2)
≤ f (x0) − f∗ +

γcδ
pG(G − 1)

(13)
≤ f (x0) − f∗ +

2γcδ
pG(G − 1)

(cid:88)

i,l∈G
i(cid:54)=l

(cid:88)

i,l∈G
i(cid:54)=l

(cid:107)∇fi(x0) − ∇fl(x0)(cid:107)2

(cid:0)(cid:107)∇fi(x0) − ∇f (x0)(cid:107)2 + (cid:107)∇fl(x0) − ∇f (x0)(cid:107)2(cid:1)

= f (x0) − f∗ +

4γcδ
pG

(cid:88)

i∈G

(cid:107)∇fi(x0) − ∇f (x0)(cid:107)2

(4)
≤ f (x0) − f∗ +

4γcδB
p

(cid:107)∇f (x0)(cid:107)2 +

4γcδζ 2
p

.

27

Function f is L-smooth that implies (cid:107)∇f (x0)(cid:107)2 ≤ 2L (cid:0)f (x0) − f∗
and γ ≤ 1/L, we derive

(cid:1). Using this and δ < p/(48cB)

E[Φ0] ≤

1 +

(cid:18)

(cid:18)

≤

1 +

8γcδBL
p
(cid:19)

γL
6

(cid:19)

(cid:0)f (x0) − f∗

(cid:1) +

4γcδζ 2
p

(cid:0)f (x0) − f∗

(cid:1) +

4γcδζ 2
p

≤ 2 (cid:0)f (x0) − f∗

(cid:1) +

4γcδζ 2
p

.

(31)

Plugging this upper bound in (30), we get

E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤

γ

(cid:1)

4 (cid:0)f (x0) − f∗
1 − 48Bcδ

(cid:17)

(cid:16)

p

(K + 1)

+

32cδζ 2
p − 48Bcδ

.

Based on this inequality we derive following corollaries.
Corollary E.1 (Homogeneous data, no compression (ω = 0)). Let the assumptions of Theorem E.1
hold, Q(x) ≡ x for all x ∈ Rd (no compression, ω = 0), p = b/m, B = 0, ζ = 0, and

γ =

L + L±

(cid:113)

1
6 (cid:0) 4cδm2

b3 + m
b2G

(cid:1)

Then for all K ≥ 0 we have E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) of the order



(cid:18)

O





L + L±

(cid:113) cδm2

b3 + m
b2G

(cid:19)

∆0

K







,

(32)

where (cid:98)xK is choosen uniformly at random from the iterates x0, x1, . . . , xK produced by Byz-
VR-MARINA and ∆0 = f (x0) − f∗. That is, to guarantee E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤ ε2 for ε2 > 0
Byz-VR-MARINA requires

L + L±



(cid:18)

O





(cid:113) cδm2

b3 + m
b2G
ε2

(cid:19)

∆0







,

communication rounds and

bL + L±



(cid:18)

O





(cid:113) cδm2

b + m

G

ε2

(cid:19)

∆0







,

(33)

(34)

oracle calls per worker.
Corollary E.2 (No compression (ω = 0)). Let the assumptions of Theorem E.1 hold, Q(x) ≡ x for
all x ∈ Rd (no compression, ω = 0), p = b/m and

γ =

(cid:113)

L +

48L2Bcδm
b

1

+ 24cδm2

b2 L2

± + 6 (cid:0) 4cδm2

b2 + m

bG

(cid:1) L2
±
b

Then for all K ≥ 0 we have E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) of the order

(cid:113)

L +

L2Bcδm
b



(cid:18)

O





+ cδm2

b2 L2
(cid:0)1 − 48Bcδm

± + (cid:0) cδm2
(cid:1) K

b

b2 + m

bG

(cid:19)

(cid:1) L2
±
b

∆0

+

cδζ 2
b
m − 48Bcδ







,

(35)

28

where (cid:98)xK is choosen uniformly at random from the iterates x0, x1, . . . , xK produced by Byz-VR-
MARINA and ∆0 = f (x0) − f∗. That is, to guarantee E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤ ε2 for ε2 ≥ 12cδζ2
Byz-VR-MARINA requires

p−48Bcδ

(cid:113)

L +

L2Bcδm
b



(cid:18)

O





+ cδm2

b2 L2
(cid:0)1 − 48Bcδm

± + (cid:0) cδm2
(cid:1) ε2

b

b2 + m

bG

(cid:19)

(cid:1) L2
±
b

∆0







,

communication rounds and

(cid:113)

bL +



(cid:18)

O





oracle calls per worker.

L2Bcδmb + cδm2L2

± + (cid:0)cδm2 + mb

G

(cid:0)1 − 48Bcδm

b

(cid:1) ε2

(cid:19)

(cid:1) L2
±
b

∆0







,

Corollary E.3. Let the assumptions of Theorem E.1 hold, p = min{b/m, 1/1+ω} and

(36)

(37)

γ =

1
√
L +
A = 48L2Bcδ max

A

, where

(cid:111)

, 1 + ω

(cid:110) m
b
(cid:26) m2

(cid:27)

+

b2 , (1 + ω)2
(cid:26) m2

4cδ max

(cid:32)

(cid:32)

+6

+6

4cδ(1 + ω) max

b2 , (1 + ω)2

ωL2 +

(cid:33) (cid:18)

max (cid:8) m
b , 1 + ω(cid:9)
2G
ω max (cid:8) m
b , 1 + ω(cid:9)
2G

+

(cid:27)

(cid:33)

L2
±

(cid:19)

(1 + ω)L2
±
b

Then for all K ≥ 0 we have E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) of the order
(cid:16)

√

(cid:17)



O



L +

A
(cid:0)1 − 48Bcδ max (cid:8) m

∆0
b , 1 + ω(cid:9)(cid:1) K

+

min

(cid:110) b
m ,

cδζ 2
(cid:111)

1
1+ω

− 48Bcδ



 ,

(38)

where (cid:98)xK is choosen uniformly at random from the iterates x0, x1, . . . , xK produced by Byz-VR-
MARINA and ∆0 = f (x0) − f∗. That is, to guarantee E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤ ε2 for ε2 ≥ 32cδζ2
Byz-VR-MARINA requires

p−48Bcδ

communication rounds and



O





O



(cid:16)

L +

√

A

(cid:17)

(cid:0)1 − 48Bcδ max (cid:8) m

∆0
b , 1 + ω(cid:9)(cid:1) ε2

(cid:16)

√

(cid:17)

bL + b

A
b , 1 + ω(cid:9)(cid:1) ε2
(cid:0)1 − 48Bcδ max (cid:8) m

∆0



 ,



 ,

(39)

(40)

oracle calls per worker.

Corollary E.4 (Homogeneous data). Let the assumptions of Theorem E.1 hold, p = min{b/m, 1/1+ω},
B = 0, ζ = 0, and

γ =

1
√
L +
(cid:32)

A

A = 6

3cδ max

, where

(cid:26) m2

b2 , (1 + ω)2

(cid:27)

+

b , 1 + ω(cid:9)
max (cid:8) m
2G

(cid:33) (cid:18)

ωL2 +

(cid:19)

(1 + ω)L2
±
b

29

(42)

(43)

(44)

Then for all K ≥ 0 we have E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) of the order
∆0

L +



√

A

(cid:16)

(cid:17)

O



K



 ,

(41)

where (cid:98)xK is choosen uniformly at random from the iterates x0, x1, . . . , xK produced by Byz-
VR-MARINA and ∆0 = f (x0) − f∗. That is, to guarantee E (cid:2)(cid:107)∇f ((cid:98)xK)(cid:107)2(cid:3) ≤ ε2 for ε2 > 0
Byz-VR-MARINA requires
√

O





(cid:16)

O



communication rounds and

oracle calls per worker.



(cid:16)

(cid:17)

L +

A

∆0



 ,

ε2

√

bL + b

(cid:17)

A

∆0

ε2



 ,

E.2 Functions Satisfying Polyak-Łojasiewicz Condition

Theorem E.2 (Theorem 2.2). Let Assumptions 2.1, 2.2, 2.3, 2.4, 2.5 hold. Assume that

0 < γ ≤ min






1
√

,

2A

L +

(cid:16)

p
1 − 96Bcδ

p

(cid:17)

4µ






,

δ <

p
96cB

,

p + 6(1−p)

where A = 48BL2cδ
for all K ≥ 0 the iterates produced by Byz-VR-MARINA satisfy
(cid:19)(cid:19)K

p + 1

ωL2 +

(cid:18)

(cid:18)

2G

(1+ω)L2
±
b

p

(cid:16) 4cδ

(cid:17) (cid:16)

(cid:17)

+ 6(1−p)
p

E (cid:2)f (xK) − f (x∗)(cid:3) ≤

1 − γµ

1 −

Φ0 +

96Bcδ
p

(cid:16) 4cδ(1+ω)
p

+ ω
2G

(cid:17)

L2

±. Then

24cδζ 2
µ(p − 96Bcδ)

,

(45)

where Φ0 = f (x0) − f (x∗) + 2γ

p (cid:107)g0 − ∇f (x0)(cid:107)2.

Proof. For all k ≥ 0 we introduce Φk = f (xk) − f∗ + 2γ
Lemmas E.1 and E.3, we derive

p (cid:107)gk − ∇f (xk)(cid:107)2. Using the results of

E[Φk+1]

(21),(24)
≤

E

(cid:20)
f (xk) − f (x∗) −

(cid:18) 1
2γ
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

−

L
2
(cid:16)

2γ
p

1 −

−

+

γ
2
48Bcδγ
p

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(cid:19)

(cid:107)xk+1 − xk(cid:107)2 +

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

γ
2

(cid:17)

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

p
2
24cδζ 2γ
p
E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(cid:17)

+ γAE (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

=

E (cid:2)f (xk) − f (x∗)(cid:3) +

(cid:16)

1 −

2γ
p
p
4
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(cid:18)

(cid:19)

1 −

96Bcδ
p

24cδζ 2γ
p
(cid:0)1 − Lγ − 2Aγ2(cid:1) E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

−

−

γ
2
1
2γ

(9)
≤

(cid:18)

(cid:18)

1 − γµ

1 −

(cid:19)(cid:19)

96Bcδ
p

E (cid:2)f (xk) − f (x∗)(cid:3)

+

2γ
p

(cid:16)

1 −

(cid:17)

p
4

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) +

24cδζ 2γ
p

(44)
≤

(cid:18)

(cid:18)

1 − γµ

1 −

(cid:19)(cid:19)

96Bcδ
p

E [Φk] +

24cδζ 2γ
p

30

where in the last step we use Lemma C.1 and our choice of γ from (44). Unrolling the recurrence, we
obtain

E[ΦK] ≤

(cid:18)

(cid:18)

(cid:18)

1 − γµ

1 −

(cid:18)

≤

1 − γµ

1 −

(cid:18)

(cid:18)

=

1 − γµ

1 −

(cid:19)(cid:19)K

(cid:19)(cid:19)K

(cid:19)(cid:19)K

96Bcδ
p

96Bcδ
p

96Bcδ
p

E [Φ0] +

E [Φ0] +

24cδζ 2γ
p

24cδζ 2γ
p

k=0
∞
(cid:88)

(cid:18)

k=0

E [Φ0] +

24cδζ 2
µ(p − 96Bcδ)

.

K−1
(cid:88)

(cid:18)

(cid:18)

1 − γµ

1 −

(cid:19)(cid:19)k

96Bcδ
p

(cid:18)

1 − γµ

1 −

(cid:19)(cid:19)k

96Bcδ
p

Taking into account Φk ≥ f (xk) − f (x∗), we get the result.

As in the case of general non-convex smooth functions, we need to estimate Φ0 to derive complexity
results. Following exactly the same reasoning as in the derivation of (31), we get

E[Φ0] ≤ 2 (cid:0)f (x0) − f (x∗)(cid:1) +

8γcδζ 2
p

.

Plugging this upper bound in (45), we get
(cid:18)

(cid:18)

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ 2

1 − γµ

1 −

(cid:19)(cid:19)K

96Bcδ
p

(cid:0)f (x0) − f (x∗)(cid:1)

(cid:18)

(cid:18)

+

1 − γµ

1 −

(cid:19)(cid:19)K

96Bcδ
p
(cid:19)(cid:19)K

(cid:18)

(cid:18)

≤ 2

1 − γµ

1 −

96Bcδ
p

·

8γcδζ 2
p

+

24cδζ 2
µ(p − 96Bcδ)

(cid:0)f (x0) − f (x∗)(cid:1)

(cid:18)

1 − γµ

1 −

(cid:19)(cid:19)k

96Bcδ
p

·

8γcδζ 2
p

+

24cδζ 2
µ(p − 96Bcδ)

∞
(cid:88)

(cid:18)

k=0

+

(cid:18)

(cid:18)

≤ 2

1 − γµ

1 −

(cid:19)(cid:19)K

96Bcδ
p

(cid:0)f (x0) − f (x∗)(cid:1) +

32cδζ 2
µ(p − 96Bcδ)

.

Based on this inequality we derive following corollaries.
Corollary E.5 (Homogeneous data, no compression (ω = 0)). Let the assumptions of Theorem E.2
hold, Q(x) ≡ x for all x ∈ Rd (no compression, ω = 0), p = b/m, B = 0, ζ = 0, and




γ = min

1
(cid:113) 12cδm2

,

b
4mµ






.

b3 + 3m
2b2G
Then for all K ≥ 0 we have E (cid:2)f (xK) − f (x∗)(cid:3) of the order

L + 2L±







O

exp

− min






L + 2L±

µ
(cid:113) 12cδm2

b3 + 3m
2b2G






,

b
m





K

 ∆0

 ,

(46)

where ∆0 = f (x0) − f (x∗). That is, to guarantee E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε for ε > 0 Byz-VR-
MARINA requires



O

max






L + L±

(cid:113) cδm2

b3 + m
b2G

µ






,

m
b



 ,

∆0
ε

log

communication rounds and



O

max






bL + L±

oracle calls per worker.




, m





 ,

∆0
ε

log

(cid:113) cδm2

b + m

G

µ

31

(47)

(48)

Corollary E.6 (No compression (ω = 0)). Let the assumptions of Theorem E.2 hold, Q(x) ≡ x for
all x ∈ Rd (no compression, ω = 0), p = b/m and

γ = min






(cid:113)

L +

96L2Bcδm
b

1

+ 48cδm2

b2 L2

± + 12 (cid:0) 4cδm2

b2 + m

2bG

,

(cid:1) L2
±
b

Then for all K ≥ 0 we have E (cid:2)f (xK) − f (x∗)(cid:3) of the order

b
m

4µ (cid:0)1 − 96Bcδ m

b






.

(cid:1)

(cid:32)



O

exp

− min






(cid:113)

L +

96L2Bcδm
b

µ (cid:0)1 − 96Bcδ m

b

(cid:1)

+ 48cδm2

b2 L2

± + 12 (cid:0) 4cδm2

b2 + m

2bG

(cid:1) L2
±
b






,

b
m



K

 ∆0

+

cδζ2
m −96Bcδ)

µ( b

(cid:33)

, (49)

where ∆0 = f (x0) − f (x∗). That is, to guarantee E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε for ε ≥ 32cδζ2
Byz-VR-MARINA requires


(cid:113)



µ(p−96Bcδ)

+ cδm2
b2 L2
µ (cid:0)1 − 96Bcδ m

± + (cid:0) cδm2
(cid:1)

b

b2 + m

bG

(cid:1) L2
±
b






,

m
b

log

 ,

(50)

∆0
ε

L +

L2Bcδm
b

O

max

communication rounds and


(cid:113)

O

max

bL +

L2Bcδmb + cδm2L2

± + (cid:0)cδm2 + mb

G

µ (cid:0)1 − 96Bcδ m

(cid:1)

b

(cid:1) L2
±
b




, m





 ,

∆0
ε

log

(51)











oracle calls per worker.
Corollary E.7. Let the assumptions of Theorem E.2 hold, p = min{b/m, 1/1+ω} and

γ = min






1
√

,

2A

L +

(cid:110) b
m ,

min
4µ (cid:0)1 − 96Bcδ max (cid:8) m

1
1+ω

(cid:111)

b , 1 + ω(cid:9)(cid:1)






, where

A = 48L2Bcδ max

(cid:32)

(cid:111)

, 1 + ω

(cid:110) m
b
(cid:26) m2

(cid:27)

+

b2 , (1 + ω)2
(cid:26) m2

4cδ max

+6

+6

(cid:32)

4cδ(1 + ω) max

b2 , (1 + ω)2

ωL2 +

(cid:33) (cid:18)

max (cid:8) m
b , 1 + ω(cid:9)
2G
ω max (cid:8) m
b , 1 + ω(cid:9)
2G

+

(cid:27)

(cid:33)

L2
±

(cid:19)

(1 + ω)L2
±
b

Then for all K ≥ 0 we have E (cid:2)f (xK) − f (x∗)(cid:3) of the order
b , 1 + ω(cid:9)(cid:1)

µ (cid:0)1 − 96Bcδ max (cid:8) m

(cid:40)

(cid:32)

(cid:32)

O

exp

− min

√

L +

A

,

b
m

,

1
1 + ω

(cid:41)

(cid:33)

K

∆0

+

µ(min{ b

cδζ2
1+ω }−96Bcδ)
m , 1

(cid:33)
,

(52)

where ∆0 = f (x0) − f (x∗). That is, to guarantee E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε for ε ≥ 32cδζ2
Byz-VR-MARINA requires
(cid:40)

√

(cid:41)

(cid:32)

(cid:33)

µ(p−96Bcδ)

O

max

L +

A

µ (cid:0)1 − 96Bcδ max (cid:8) m

b , 1 + ω(cid:9)(cid:1) ,

m
b

∆0
ε

, 1 + ω

log

,

(53)

communication rounds and

(cid:32)

(cid:40)

O

max

oracle calls per worker.

√

A
µ (cid:0)1 − 96Bcδ max (cid:8) m

bL + b

b , 1 + ω(cid:9)(cid:1) , m, b(1 + ω)

(cid:41)

log

(cid:33)

,

∆0
ε

(54)

32

Corollary E.8 (Homogeneous data). Let the assumptions of Theorem E.2 hold, p = min{b/m, 1/1+ω},
B = 0, ζ = 0, and

γ = min






1
√

,

2A

L +

min

(cid:110) b
m ,
4µ

1
1+ω

(cid:111)




, where

(cid:32)

A = 6

4cδ max

(cid:26) m2

b2 , (1 + ω)2

(cid:27)

+


b , 1 + ω(cid:9)
max (cid:8) m
2G

(cid:33) (cid:18)

ωL2 +

(1 + ω)L2
±
b

(cid:19)

.

Then for all K ≥ 0 we have E (cid:2)f (xK) − f (x∗)(cid:3) of the order
(cid:27)
(cid:18)

(cid:26)

(cid:18)

(cid:19)

(cid:19)

O

b
m
where ∆0 = f (x0) − f (x∗). That is, to guarantee E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε for ε > 0 Byz-VR-
MARINA requires

µ
√
L +

1
1 + ω

− min

(55)

exp

∆0

K

A

,

,

,

(56)

(57)

O

max

communication rounds and

(cid:32)

(cid:40)

√

L +
µ

A

,

m
b

(cid:41)

, 1 + ω

log

(cid:33)

,

∆0
ε

(cid:32)

(cid:40)

O

max

√

bL + b
µ

A

(cid:41)

, m, b(1 + ω)

log

(cid:33)

,

∆0
ε

oracle calls per worker.

33

