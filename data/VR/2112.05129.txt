1
2
0
2
c
e
D
9

]

O
R
.
s
c
[

1
v
9
2
1
5
0
.
2
1
1
2
:
v
i
X
r
a

Assistive Tele-op: Leveraging Transformers to Collect
Robotic Task Demonstrations

Henry M. Clever1,2, Ankur Handa1, Hammad Mazhar1, Kevin Parker1, Omer Shapira1,
Qian Wan1, Yashraj Narang1, Iretiayo Akinola1, Maya Cakmak1, Dieter Fox1
1NVIDIA, USA. 2Georgia Institute of Technology, Atlanta, GA, USA.

Abstract

Sharing autonomy between robots and human operators could facilitate data collec-
tion of robotic task demonstrations to continuously improve learned models. Yet,
the means to communicate intent and reason about the future are disparate between
humans and robots. We present Assistive Tele-op, a virtual reality (VR) system
for collecting robot task demonstrations that displays an autonomous trajectory
forecast to communicate the robot’s intent. As the robot moves, the user can switch
between autonomous and manual control when desired. This allows users to collect
task demonstrations with both a high success rate and with greater ease than manual
teleoperation systems. Our system is powered by transformers, which can provide
a window of potential states and actions far into the future – with almost no added
computation time. A key insight is that human intent can be injected at any loca-
tion within the transformer sequence if the user decides that the model-predicted
actions are inappropriate. At every time step, the user can (1) do nothing and allow
autonomous operation to continue while observing the robot’s future plan sequence,
or (2) take over and momentarily prescribe a different set of actions to nudge the
model back on track. We host the videos and other supplementary material at
https://sites.google.com/view/assistive-teleop.

1

Introduction

Manually teleoperating robots to collect task demonstrations at scale is laborious and challenging.
We present a shared-autonomy-based method using neural networks to forecast robot trajectories that
substantially reduces manual teleoperation time while maintaining a high success rate. Speciﬁcally,
we adapt a learned model to do trajectory auto-complete, i.e., given an initial sequence of states
and actions, the network learns to complete the rest of the trajectory. The user can either accept
the model’s suggestions or provide manual corrections while observing their effect on the model
forecast. We leverage transformers [30] for modeling the states and actions through time, which are
well-suited to modeling long sequences of information with complex dependencies (see Fig. 1-left).
Their self-attention mechanism can holistically understand a robot trajectory, rather than emphasizing
adjacent connections between states. When taking as input a sequence of past actions, the transformer
can look far into the future and predict future actions. By integrating this into a robot manipulation
environment with VR, a user can decide if executing the future actions is appropriate, or otherwise
take momentary control of the system to provide a better demonstration.

This work explores transformer model prediction for a set of 7 manipulation tasks involving pick-
and-place across industrial, household, and caregiving robot settings (Fig. 1-right). We train on a
large number of demonstrations (> 500) from the open-source RoboTurk [18] dataset and perform
few-shot learning by ﬁne-tuning on a small number of expert demonstrations (<= 60) for each of
the 7 tasks. We show that the model is able to succeed autonomously for 67.1% of task scenarios
during test time. When the model predicts a wrong sequence of actions, the user takes control and

E-mail: ahanda@nvidia.com, henryclever@gmail.com

 
 
 
 
 
 
Figure 1: Assistive Tele-op using transformers for data collection. Left: Robot motion displaying input past
actions (yellow) and output predicted actions (blue) to the user. Right: We test our method on 7 task scenarios
across industrial (block stacking, nut assembly, kit assembly), household (cabinet drawer and bowl manipulation),
and caregiving (itch scratching and drinking) tasks.

gives the robot a nudge to get it back on track. This signiﬁcantly improves performance, resulting
in a 96.1% success rate. Importantly, our Assistive Tele-op system reduces manual control time to
collect demonstrations by a factor of 5. It is worth stressing that while the use of interventions is
similar to DAgger [27], we aid the user by displaying a live forecasting model of the robot trajectory
so corrections can be made well before the robot makes a mistake.

Our transformer embedding has a single state space of ﬁxed length that we can map a variety of
tasks into. Further, by deﬁning the robot state and actions entirely in end-effector space like [19]
and learning a world model [11] in this space, the transformer can be interchanged between robots
and simulation environments. Our transformer is pre-trained on demonstrations collected in a
different physics simulator and robot than what we use in this work. While only the predicted robot
actions are necessary for control, we use an additional loss on object pose states in the scene, which
boosts performance. Transformers are better able to parse sequence information using a positional
embedding vector, which we compute using the cumulative distanced traversed by the end effector in
Euclidean space (similar in spirit to [26]). Finally, by training with a BERT-style zero padding [6] on
the input to the transformer corresponding to future states, we can make an arbitrary number of model
predictions far into the future – which allows the user to understand what the robot is about to do.

In summary, the work makes the following contributions: (1) Evidence that pre-trained transformers
can be ﬁne-tuned for few-shot generalization to new robot manipulation tasks, and (2) Assistive Tele-
op, a VR system with live model forecasting, to assist users with collecting robotic task demonstrations
at a high success rate and with substantially reduced manual control time. As the user collects more
demonstrations, they can be fed back to the model for continual learning.

2 Related Work

Task demonstrations for robot manipulation can be collected using automatic methods such as
trajectory optimization [3, 15] and reinforcement learning [16, 27, 28], as well as manual methods
of kinesthetic teaching and teleoperation [2]. The former require carefully tuned reward functions,
while the latter can be laborious to collect. Virtual reality [7, 32] can help, but the human effort
remains considerable for complex tasks, and when many demonstrations are required. Shared
autonomy [9, 12, 14, 25] offers a better solution to collecting large-scale data. These works blend
robot and user intent using optimization [9], reinforcement learning [25], and learned coarse-to-ﬁne
user precision [14], while ours lets the user look far into the future to understand the autonomous
prediction. A similar forecasting method was proposed by Liu et al. [17], but it is used in a behavior
cloning loss function, rather than for communicating intent to the user. We take some insight from
Pérez-D’Arpino and Shah [22], who overlay a series of robot conﬁguration renderings through time
to show planned motions. Later, they used this feature for human robot teaming to allow an operator
to either accept a suggested motion plan or momentarily intervene [21].

Transformers have only recently gained traction in robotics. Janner et al. [13] reframed RL as
sequence modeling, and used transformers to control humanoid walking. Chen et al. [5] concurrently
explored this in the context of a game environment. Common transformer implementations have used

2

Figure 2: The transformer takes as input a past history of Tp states, actions, and positional embeddings, and
outputs predicted states and actions. The user can observe predictions far into the future (e.g. Tf = 300
time-steps) and may choose to execute Te <= Tf of those predictions. The input for future states is padded
with zero, forcing the model to learn future predictions only from past inputs. The total number of time-steps in
the sequence is denoted by Ts.

sinusoidal positional embeddings to better model the order of words [30]. However, Chen et al. [5]
used an episodic time-step positional embedding and Press et al. [23] added a linear bias to each
attention score. We take inspiration from these and use a cumulative distance embedding to provide
information for how far the end effector has traversed.

3 Methods

The transformer takes as input a trajectory of states and actions τx = {sx, ax} and positional encoding
vector n, and outputs a predicted trajectory (cid:98)τy = {(cid:98)sy, (cid:98)ay}. As shown in Fig. 2, the input consists
of Tp past time-steps of information, while the output contains additional information on future
predictions up to time-step Tf . For each model prediction, the user can choose to execute some
number of open-loop actions Te while observing actions far into the future.

3.1 Formulation in Robotics

The state at each time-step st is a vector consisting of a global robot end effector pose sr,t ∈ R7, con-
tinuous gripper state sg,t ∈ R1, and the local pose of J objects in the environment {so1,t . . . soJ,t} ∈
R7×J relative to the robot end effector. Each 7D pose contains a position and quaternion. At the
input, this is fed into a state embedding function Fs which we represent with a single fully connected
network layer:

semb,t = Fs(st)
(1)
where semb,t ∈ R128. The action at each time-step at is a vector consisting of the local target position
of the robot end effector ar,t ∈ R7 and the binary gripper command ag,t ∈ R1. At the input, this is
fed into an action embedding function Fa, which we represent with a single fully connected network
layer:

aemb,t = Fa(at)
(2)
where aemb,t ∈ R128. Additionally, the network contains a positional embedding to measure the
distance and rotation the end effector has traversed at each time-step along the trajectory from
time-step 1 . . . t. The positional embedding nt ∈ R1 is an integer token computed at each time-step
as:

t
(cid:88)

8
(cid:88)

nt =

||cr,j,t(cid:48) − cr,j,t(cid:48)−1||2

(3)

t(cid:48)=2

j=1

where each cr is coordinate of a corner of a 3D bounding box around the end effector at a given
time-step, and is a function of end effector position pr quaternion qr in global frame. This pose
representation is the same as that in the loss function by Allshire et al. [1]. We chose this pose
representation because it casts rotation in position space, which mitigates the problem of combining
heterogenous terms in the same function. The token nt is fed into a learned positional embedding
layer, Fn:

nemb,t = Fn(nt)

(4)

3

where nemb,t ∈ R128. This is added to each state and action embedding. At each time-step the
transformer receives the input vector computed as

xt = LN(cid:0)(semb,t + nemb,t) ⊕ (aemb,t + nemb,t)(cid:1)

(5)

where xt ∈ R256 and LN represents layer normalization. The transformer outputs predicted vector
(cid:98)yt ∈ R256, which is then decoded with linear layers on the output mirroring those on the input,
represented by gs and ga for the states and actions. The output of these decoding layers contain
predicted states and actions, the sequence of which forms trajectory (cid:98)τy. The robot is controlled with
the predicted end-effector pose actions by using Riemannian motion policies [24].

3.2 Network Training

During training, a sub-sequence of length Ts = 400 from is sampled from a task demonstration of
trajectory length Td, where Td > Ts (recall Fig. 2). The network takes input of sequence length Ts
of state and action pairs, τx, associated with each time-step. Inspired by BERT [6] masking, we only
keep the inputs from the ﬁrst Tp time-steps and mask the remaining inputs from time-steps Ts − Tp
with zeros. The network is trained to predict the corresponding state and action pairs, τy, at masked
out time-steps. The input length Tp is chosen at random during training to force the transformer to
make future predictions of an arbitrary horizon length. It is sampled from a uniform distribution
Tp ∼ U(1, 350), such that the future prediction length is at least 50 time-steps.

We denote sr,t = [pr,t, qr,t] to be the state of the robot end-effector composed of the position and
orientation at time t. We denote ar
t = [prtarget,t, qrtarget,t] to be the robot action composed of the target
end-effector position and orientation. Similarly, we deﬁne the current gripper state sg,t ∈ [0, 1]
and the target gripper state ag,t ∈ [0, 1]. Object i in the scene is also represented by its state vector
soi,t = [poi,t, qoi,t], which is composed of its position and orientation. The network takes an input
sequence of states sx={sr,t, so1,t · · · soJ,t, sg,t}k
t=0 composed of the robot end-effector state, gripper
state and states of J objects in the scene, as well as the actions ax={ar,t, ag,t}k
t=0 associated with the
robot end-effector. It then predicts the future sequence of states sy={sr,t, so1,t · · · soJ,t, sg,t}T
t=k+1 and
corresponding actions ay={ar,t, ag,t}T
t=k+1. We predict the future states and actions given the current
states and actions using a GPT-style transformer: (cid:98)sy, (cid:98)ay = GPT(sx, ax).
Loss function. The loss function used to train the transformer model has a number of components
that enable it to learn states and actions. We compute losses related to the end-effector state and
action, the gripper state and action, and to the state of each object in the scene.

The state and action space is composed of positions and orientations, which are heterogeneous terms
that would require a weighting factor if directly combined in the loss function. Instead, we compute
the loss in euclidean space in a similar way to Eq. 3, because it casts position and orientation into 3D
locations of 8 bounding box corners around the end effector or object. We compute the end-effector
state loss by mapping its position and orientation to the box corners for both the predictions and
ground truth:

(cid:98)csr = CORNERS((cid:98)psr , (cid:98)qsr )
csr = CORNERS(psr , qsr )

(6)
(7)

where csr ∈ R8×3 are the 3D positions of the 8 corners of the bounding box extents of the end effector.
The loss is computed as the L2 distance of the corresponding 8 corners of the predictions and ground
truth:

Lsr =

Ts(cid:88)

t=Tp

||(cid:98)csr,t − csr,t||2

(8)

Similarly, we can deﬁne a loss on the action space where the predictions are end-effector target
positions and orientations, as well as a loss on the state predictions of the objects.

4

To compute loss on the gripper state and action we use binary cross entropy:

Lsg =

Lag =

Ts(cid:88)

t=Tp
Ts(cid:88)

t=Tp

BCE((cid:98)sg,t, sg,t)

BCE((cid:98)ag,t, ag,t)

The total loss is sum of these components:

LTOTAL = Lsr + Lar +

J
(cid:88)

i=1

Lsoi + λ(Lsg + Lag )

4 Evaluation

(9)

(10)

(11)

We evaluated the transformer and Assistive Tele-op system across a variety of tasks representing
industrial [18, 31], household [4, 8], and caregiving [7, 10] task scenarios. For this, we used both
existing data from the Roboturk [18] simulation dataset covering pick-and-place and nut assembly
tasks, as well as new data for other tasks that we collected in VR.

4.1 Data collection

Existing data - Roboturk. The Roboturk simulation dataset was collected in the Mujoco [29]
simulator with a Baxter robot, but we found that many demonstrations played back successfully in
the NVIDIA Omniverse simulator with a Franka robot when controlling the robot in end effector
space using RMPs [24]. This dataset consists of over 6000 crowd-sourced human demonstrations
for pick-and-place tasks with 4 objects (cereal box, milk jug, bread, and coke can) and nut assembly
tasks. Of these, we selected 533 pick-and-place demonstrations for pretraining the transformer. We
chose the ﬁrst 533 demonstrations that had an overall time of less than 900 time-steps, because we
observed that shorter demonstrations had better quality. We hand-selected demonstrations for the
round nut assembly (Task B) by choosing the ﬁrst 50 that played back smoothly in our recreation of
the scene in Omniverse with the Franka robot.

New data. An HTC Vive virtual reality headset was used by a researcher to collect data in Omniverse
with Franka. To create variation in each scene, we sampled initial scene object poses from the
following uniform noise distributions for both training and testing scenes: (A) block stacking - blue
picked block from ±5cm planar translation and ±45◦ rotation, orange stacked block from ±5cm
planar translation. (C) assembly kit - pink hexagon from ±7.5cm planar translation and ±180◦
rotation. (D) cabinet, ±45◦ rotation. (E) put bowl in cabinet, cabinet from ±45◦ rotation, green bowl
from ±10cm uni-directional translation relative to the cabinet. (F) itch scratching, scratch tool from
±5 cm unidirectional translation, humanoid root pose from ±5 cm planar translation, and 19 unique
itch scratching locations on the humanoid. 3 manual demonstrations are collected for each location
(57 total). (G) humanoid drinking, mug w/straw from ±5cm unidirectional translation and ±30◦
rotation over the table, and humanoid/wheelchair root from ±7.5cm planar translation.

4.2 Transformer evaluation

The input sequence length was set to Tp = 250 and the future prediction length to Tf = 150. Each
time a forward pass runs on the transformer, the simulator executes Te = 10 actions, which are
fed back into the transformer. Predicted actions are fed directly back to the transformer, while real
simulator states resulting from the actions are fed into the model input. The transformer contains
6 layers, 8 heads, a hidden layer size of 256, and it is trained with a batch size of 128. During
pre-training, we used a learning rate of 1e-4 that linearly decreased to 5e-5, and a learning rate of
5e-5 for training on other task data.

Automatic model prediction. First, we evaluated the pretrained model. We trained it for > 2
days on raw RoboTurk data with the Baxter robot in MuJoCo, and evaluated it in a reconstructed
environment with the Franka robot in NVIDIA Omniverse. We evaluated the pretrained model on
both 50 scenes from the training data and on 50 test scenes. The training data evaluation provides a

5

Table 1: Success rate and demonstration time for models trained from scratch and pretrained models.

No.

Success rate

Manual demonstration time (s)
Assistive
Tele-op
-

Auto
w/pretr.
-

Task
RoboTurk pick/place [18]

training Manual
tele-op
demos
-
533

Assistive Manual Auto
Tele-op
-

Auto
no pretr.
0.84 / 0.66†
0.60
A. Block stacking
0.38
B. Round nut assembly [18]
0.00
C. Assembly kit - hexagon [31]
0.98
D. Cabinet drawer opening
0.52
E. Put bowl in drawer
0.46
F. Humanoid itch scratching
0.68
G. Humanoid drinking
0.517
Overall (A-G average)
*Based on approximate RoboTurk data frequency of 15 Hz. †Results on training data / results on test data. All results collected in NVIDIA Omniverse with Franka.

1.00
0.94
0.92
1.00
1.00
0.93
0.94
0.961

1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.000

14.5
72.3*
38.5
14.0
28.3
23.0
18.8
29.9

0.74
0.70
0.10
1.00
0.64
0.70
0.82
0.671

3.5
7.9
5.6
N/A
4.5
4.2
7.7
5.5

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

35
50
35
35
50
57
35
-

tele-op
-

-

Figure 3: Fully automatic prediction task examples that the transformer completes successfully. (A) the blue
block is stacked upon the orange block (B) the round nut is placed on the round peg. (C) the pink hexagon is ﬁt
into the assembly kit board. (D) the bottom cabinet drawer is opened. (E) the green bowl is put in the top cabinet
drawer. (F) an itch scratching tool is picked up and used to scratch an itch on the bottom of the left foot. (G) a
mug with a straw is picked and brought to a person in a wheelchair.

measure of the sim2sim transfer between simulators. The test data evaluation shows generalization to
previously unseen initial item locations. We conducted more tests in tasks A-G in Omniverse (see
Table 1). For each, we ﬁne-tuned a transformer from the pretrained model and trained a transformer
from scratch (no pretraining). We used a ﬁxed-time budget for this comparison. Each model is tested
on 50 new object conﬁgurations, except itch scratching, which is tested on 57.

Assistive Tele-op: We evaluated the human-in-the-loop Assistive Tele-op system using both task
success rate and manual demonstration time elapsed. A researcher used the HTC Vive VR system to
communicate with the transformer prediction when controlling the robot, as shown in Fig. 4. For
each task, the model began in automatic mode, and the user clicked a button on the interface to stage
an intervention when the robot moved in an inappropriate direction (e.g. away from the bowl rather
than toward it when picking it up). We score Assistive Tele-op success as the ability to complete a
demonstration on a new task scenario – either with fully automatic prediction, or with intervention
assistance. Assistive Tele-op can only improve the demonstration success rate. For all Assistive
Tele-op scenarios, we used the transformer with pretraining. Time taken for manual demonstration
is compared among manual, automatic, and assistive modes. In manual mode, this is the average
time to collect each full demonstration. In automatic, it is 0, because no human effort is required. In
Assistive Tele-op mode, it is the average intervention time for all demonstrations per task.

5 Results and Discussion

Pre-trained transformers can be used for few shot generalization to new tasks. For each task,
we compared models trained from scratch to those ﬁne-tuned starting with a pre-trained model. The
pre-trained and ﬁne-tuned models performed better across all tasks. Success examples with fully
automatic model prediction are shown in Fig. 3. Testing scenarios were successful in most cases,
except Task C (see section 5.1.)

Models trained with our method can transfer between different simulators and robots. The
Roboturk dataset was collected in Mujoco using a Baxter robot. However, we tested the transformer
model using a Franka robot in the Omniverse [20] simulator. These environments have a different

6

Figure 4: Assistive Tele-op in VR. When the user detects future actions that are inappropriate, they click a
button to take over control. After a momentary nudge, they return control to the transformer.

robot conﬁguration, control method, data collection rate, and simulation method, but the model
performs well, showing good sim2sim transfer. Task B, also from Roboturk, provides further
evidence for this. Formulating the model in end effector space is key to this transfer, by obviating the
conﬁguration space representation that is different between Baxter and Franka.

Human interventions can get the model back on track. When a human intervenes in event of
failure to nudge the robot back on track, success increases from 67.1% to 96.1%. See Fig. 4.

Collecting Assistive Tele-op demonstrations with model prediction is easier. For purely manual
teleoperation, the average demonstration time is 29.9 seconds. For Assistive Tele-op, the average
human demonstration time to get the robot back on track is 5.5 seconds across task scenarios that
otherwise cannot be completed with fully automatic model prediction.

Auxiliary object pose loss boosts performance. We ablated the auxiliary loss on the objects in
the scene, and found that for the round nut assembly (Task B) success for the pretrained model,
performance drops from 70% to 58%.

5.1 Limitations

The transformer model has poor generalization performance for precision tasks with few (<= 50)
examples. The Assembly Kit from TransporterNets consists of ﬁve precisely ﬁtting shapes into a
board. The performance is low for a single precisely ﬁtting shape (the Hexagon, at 10%), as shown in
Table 1. However, if the goal criteria is set more loosely (i.e. the shape is next to the goal but not
quite in the slot), then performance is 46%. It also has some difﬁculty picking the hexagon, because
the hexagon is almost as wide as the open gripper max width.

Most failures happen due to imprecise grasping when robot is unable to recover after failing at the
ﬁrst attempt. In some cases, the robot grasps the object but stops midway and never reaches the goal
or stays frozen after grasping. This may be to due to out of distribution errors as a result of limited
demonstrations.

References

[1] Arthur Allshire, Mayank Mittal, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix Widmaier,
Manuel Wüthrich, Stefan Bauer, Ankur Handa, and Animesh Garg. Transferring dexterous manipulation
from gpu simulation to a remote real-world triﬁnger. arXiv preprint arXiv:2108.09779, 2021.

[2] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from

demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.

[3] Arthur E Bryson and Yu-Chi Ho. Applied optimal control: optimization, estimation, and control. Routledge,

2018.

[4] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter
Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019
International Conference on Robotics and Automation (ICRA), pages 8973–8979. IEEE, 2019.

7

[5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.
arXiv preprint arXiv:2106.01345, 2021.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[7] Zackory Erickson, Yijun Gu, and Charles C Kemp. Assistive vr gym: Interactions with real people to
improve virtual assistive robots. In 2020 29th IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN), pages 299–306. IEEE, 2020.

[8] Caelan Reed Garrett, Chris Paxton, Tomás Lozano-Pérez, Leslie Pack Kaelbling, and Dieter Fox. Online
replanning in belief space for partially observable task and motion problems. In 2020 IEEE International
Conference on Robotics and Automation (ICRA), pages 5678–5684. IEEE, 2020.

[9] Deepak Gopinath, Siddarth Jain, and Brenna D Argall. Human-in-the-loop optimization of shared autonomy

in assistive robotics. IEEE Robotics and Automation Letters, 2(1):247–254, 2016.

[10] Phillip M Grice and Charles C Kemp. In-home and remote use of robotic body surrogates by people with

profound motor deﬁcits. Plos one, 14(3):e0212904, 2019.

[11] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. arXiv preprint

arXiv:1809.01999, 2018.

[12] Ioannis Havoutis and Sylvain Calinon. Learning from demonstration for semi-autonomous teleoperation.

Autonomous Robots, 43(3):713–726, 2019.

[13] Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling

problem. arXiv preprint arXiv:2106.02039, 2021.

[14] Hong Jun Jeon, Dylan P Losey, and Dorsa Sadigh. Shared autonomy with learned latent actions. arXiv

preprint arXiv:2005.03210, 2020.

[15] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under

unknown dynamics. In NIPS, volume 27, pages 1071–1079. Citeseer, 2014.

[16] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

[17] Yifang Liu, Diego Romeres, Devesh K Jha, and Daniel Nikovski. Understanding multi-modal perception
using behavioral cloning for peg-in-a-hole insertion tasks. arXiv preprint arXiv:2007.11646, 2020.

[18] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John
Emmons, Anchit Gupta, Emre Orbay, et al. Roboturk: A crowdsourcing platform for robotic skill learning
through imitation. In Conference on Robot Learning, pages 879–893. PMLR, 2018.

[19] Roberto Martín-Martín, Michelle A Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, and Animesh
Garg. Variable impedance control in end-effector space: An action space for reinforcement learning in
contact-rich tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pages 1010–1017. IEEE, 2019.

[20] NVIDIA. Omniverse multi-GPU real-time simulation and collaboration platform. Available at https:

//developer.nvidia.com/nvidia-omniverse-platform (2021/09/17), 2021.

[21] Claudia Pérez-D’Arpino, Rebecca P Khurshid, and Julie A Shah. Experimental assessment of human-robot
teaming for multi-step remote manipulation with expert operators. arXiv preprint arXiv:2011.10898, 2020.

[22] Claudia Pérez-D’Arpino and Julie A Shah. C-learn: Learning geometric constraints from demonstrations
for multi-step manipulation in shared autonomy. In 2017 IEEE International Conference on Robotics and
Automation (ICRA), pages 4058–4065. IEEE, 2017.

[23] Oﬁr Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables

input length extrapolation. 2021.

[24] Nathan D Ratliff, Jan Issac, Daniel Kappler, Stan Birchﬁeld, and Dieter Fox. Riemannian motion policies.

arXiv preprint arXiv:1801.02854, 2018.

[25] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Shared autonomy via deep reinforcement learning.

arXiv preprint arXiv:1802.01744, 2018.

[26] Jürgen Schmidhuber Róbert Csordás, Kazuki Irie. The devil is in the detail: Simple tricks improve

systematic generalization of transformers. arXiv preprint arXiv:2108.12284, 2021.

[27] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference on
artiﬁcial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011.

8

[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[29] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE,
2012.

[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems, pages 5998–6008, 2017.

[31] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis
Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual
world for robotic manipulation. arXiv preprint arXiv:2010.14406, 2020.

[32] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel.
Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pages 5628–5635. IEEE, 2018.

9

