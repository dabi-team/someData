JOURNAL OF LATEX CLASS FILES, VOL.

1

Development of VR Teaching System for
Engine Dis-assembly

Zhuochen Xiong

2
2
0
2

l
u
J

2
1

]

C
H
.
s
c
[

1
v
5
6
2
5
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—With the worldwide ravaging of the covid-19 epi-
demic, the traditional face-to-face education systems have been
interrupted frequently. It is demanded to develop high-quality
online education modalities. The webcasting based online class-
room is one of the popular education modalities but suffers
from poor teacher-student interactions and and low immersive
learning experiences. This thesis aims to improve the online
education quality by using the virtual reality (VR) technology.
For the purpose of automobile engine education, we develop a VR
based engine maintenance learning system. The system includes
many teaching and learning components in VR enabled by the
Unity engine. Users can immersively experience the complete
engine disassembly process through the wearable VR display and
interactive devices. The system is designed with an interactive
layer, a control layer, and a physical data layer. Such a system
architecture effectively separates the speciﬁc implementations of
different domains and improves the R&D efﬁciency. Once new
object models and process proﬁles are provided, the proposed
system architecture requires no modiﬁcation of codes for changed
learning objects and processes. The efﬁciency and efﬁctiveness
of the proposed method are verﬁed by various experiments. The
developed techniques can be useful for many other applications.

Index Terms—virtual reality, unity, education

I. INTRODUCTION

According to relevant statistics from the Ministry of Edu-
cation, there will be about 8.74 million college graduates in
2020. The number will rapidly grow to 9.03 million in 2021,
increasing 350,000 year-on-year while also reaching a new
historical high for the number of college graduates in China.
The increasing number of college graduates does not match
the demand for jobs. The employment of fresh graduates from
colleges and universities is like a thousand horses breaking
through a single wooden bridge, so many college graduates
choose to study in graduate schools, abroad, and other forms
of temporary employment pressure.

In stark contrast, the employment rate in vocational colleges
is increasing year after year. This shows that the demand for
talents with special vocational skills is increasing day by day.
This is also due to the fact that the evaluation system of talents
is becoming more and more diversiﬁed, not just "academic
only". In such a context, the state is paying more attention
to the diversion of secondary school graduates to guide more
of them to aspire to enter vocational institutions to acquire
professional skills, which will make them more competitive in
the job market. Therefore, the quality of vocational education
is essential.

Zhuochen Xiong is with the Department of Computer Science and Engi-
neering, Southern University of Science and Technology, Shenzhen, 518055,
China (e-mail: 11811806@mail.sustech.edu.cn)

For the past few decades, high school students have been
evaluated by society on their ability to get
into a good
college, while some students entering vocational institutions
have disappeared from the public eye and received no social
attention. This has also led to a lack of attention to vocational
education and relative backwardness in terms of teachers
and teaching resources. This is especially true in areas that
require teaching practice, such as automotive repair, electronic
device repair, and complex instrument manipulation. Because
of the lack of input, it is often difﬁcult for students to have
sufﬁcient access to relevant practices during the learning stage.
In particular, schools may choose not to teach in areas that
have safety risks because they do not have sufﬁcient resources
and do not want to take responsibility for safety. At the same
time, because of the lack of teachers’ resources, it is difﬁcult
for the faculty of vocational colleges to meet the needs of a
large number of students. As a result, the teaching quality
of theoretical courses also varies, which eventually makes
it difﬁcult for students in vocational colleges to master the
complete knowledge-theoretical system and practical skills.

A. Motivation

In the post-epidemic era, the face-to-face classroom format
is difﬁcult to achieve. For the sake of epidemic prevention
and control and the safety of teachers’ and students’ lives,
it is essential to explore distance learning formats that can
restore the quality of classroom teaching to the greatest extent
possible. In recent years, digital ofﬂine teaching forms have
emerged, and standard forms such as online video classes
have covered almost every stage of education. However, the
current digital
teaching scene still has problems such as
poor interaction and poor immersive experience for teachers
and students, leading to poor teaching quality and learning
experience than ofﬂine classrooms.

There is an urgent need to introduce metaverse-based virtual
teaching and learning scenarios in this context [1], [2]. Par-
ticipants will have a better sense of presence and interactive
experience when using a metaverse-based virtual
teaching
system. These advantages create a surreal virtual integration
teaching environment conducive to stimulating learners’ cu-
riosity, imagination, and creativity. Further, this is the key that
can enhance learner engagement.

Based on the above status, our team developed an engine
disassembly system based on virtual reality technology in
cooperation with FXB CO., LTD. The system is adapted to
mainstream VR devices. The teaching process and teaching
objects are stored in a parameterized form in the conﬁguration
ﬁle, which is highly scalable and migratable.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL.

2

II. RELATED WORK
Because of insufﬁcient human and material resources in
teaching, many schools have started to use systematic teaching
software to assist in teaching. Virtual reality-based simulation
teaching software has become increasingly popular in recent
years and has even been widely used in some western coun-
tries.

Currently, the application scenarios of virtual reality teach-
ing systems in China are still minimal and tend to focus on
some disciplines with high practical costs. One of the more
typical scenarios is anatomy. In some organ anatomy scenarios,
experimental materials are usually more precious and difﬁcult
for most students to perform hands-on. Students can have a
more realistic experience in the virtual reality scenario and
signiﬁcantly reduce the related material costs.

In addition, the advantages of virtual simulation teaching
are also reﬂected in the safety and high reproducibility. In
some industries with certain operation safety risks [3], [4],
using virtual reality to simulate actual operations can reduce
the occurrence of safety accidents. Furthermore, for some
scenes with the high complexity of teaching, uniﬁed teaching
software can easily replicate the teaching process and achieve
the purpose of uniﬁed standards.

A. Unity3D

Unity3D is a tool platform developed by Unity Technologies
that can be used to create any real-time interactive 2D and
3D content. The Unity platform provides a complete set of
tools to help developers quickly build projects. We choose this
tool as our primary technology stack based on the following
characteristics [5], [6].

1) Visual development

interface. As a tool

that can be
used to build 3D models, an easy-to-understand visual
interface is an excellent advantage of Unity. We can
complete project settings and script editing in the visual
programming interface, and the development efﬁciency
is extremely high.

2) High compatibility with 3D models. Unity3d supports
most 3D models and can automatically convert texture
materials to U3D format. Its good compatibility helps
us quickly import pre-built project models, including
static operating room models and high-precision engine
models [7].

3) Multi-platform compatible. The unity suite supports a
one-click compilation of projects into multiple formats,
adapting to powerful software platforms that are cur-
rently popular. We can achieve platform-independent
project development based on this feature.

4) Excellent performance. The bottom layer of Unity na-
tively supports OpenGL and Direct11, which helps to
render high-precision physical models quickly. In virtual
reality, high-precision physical models can improve the
user’s sense of presence.

B. VRTK & DoozyUI

In the project, we used VRTK and DoozyUI to implement
the teaching system. The dynamic interaction between char-

acters and models in the scene is implemented using VRTK,
and the messaging and user interface in the system is managed
using DoozyUI.

Virtual Reality Toolkit aims to make building spatial com-
puting solutions in the Unity software fast and easy for
beginners as well as experienced developers [8], [9].

In our project, VRTK is used to build the interaction
behavior of items, which has the following excellent features.

1) Good compatibility. VRTK was formerly a Steam VR
development
tool, but with the development of the
community, it gradually supports most mainstream VR
devices, such as Oculus, GearVR, etc. Good compati-
bility can help us develop teaching systems adapted to
multiple platforms so that teaching institutions using the
system do not have to be limited to the purchase models
of VR devices.

2) Rich pre-built interaction behaviors. As a mature VR
development SDK, VRTK has a variety of built-in
common interactive actions, such as: grabbing, sliding,
throwing, helmet movement, etc. For helmet movement
this kind of action, VRTK has a variety of built-in
movement methods, such as dash Movement, touch-pad
Movement, transient movement, etc. In addition, VRTK
produces sample scenes for these common interactions.
We can see how these interaction models are imple-
mented in these scenes, including clear C# code and
related settings.

3) Efﬁcient keyboard and mouse simulator. In order to
help developers to debug programs more conveniently,
VRTK implements a VR debugging simulator that can
be completely simulated by keyboard and mouse. This
tool helps us develop without equipment, which provides
great convenience for our remote collaboration during
the epidemic.

4) Rich documentation and active community. As we all
know, whether the documentation is clear and the com-
munity is active has far-reaching signiﬁcance for the
development of an SDK because it determines how many
people will be willing to use and improve its functions.
The documentation of VRTK is rich, and the developer
can directly view the documentation of related functions
by hovering the mouse on the GUI interface, which is
highly readable. These documents helped us a lot in the
project’s early days, allowing us to learn how to use the
tool quickly. In addition, VRTK’s community activity
is also very high. It has obtained thousands of stars in
the GitHub community, and many issues are constantly
to improve its
being raised and resolved, helping it
features continuously.

DoozyUI is a well-known UI management framework in the
Unity ecosystem. We use it to build a complete user interaction
system in our project. Compared with other UI frameworks,
DoozyUI has the following advantages [10].

1) Easy to learn and use. DoozyUI has a complete graph-
ical user interface and can complete most UI design
and construction through GUI. The visual interface is
very intuitive when debugging views, reducing the cost

JOURNAL OF LATEX CLASS FILES, VOL.

3

of collaboration for developers. For standard interaction
models in user interfaces, such as layouts and buttons,
DoozyUI provides services to the upper layer in the
form of APIs, reducing the cost of development and
understanding.

2) Components are persistent. In DoozyUI, each view is
treated as a container and can be managed using the
built-in container database. Developers can deﬁne any
UIView and its features in the database. These prede-
ﬁned UIViews can be easily called in any code, and the
layout can be adjusted directly in the GUI interface.
3) Signal-based event delivery. The event delivery system is
arguably one of the most powerful features of DoozyUI.
In the design of DoozyUI, all event operations are called
asynchronously in the form of signals. The advantage
of uniﬁed message events is that different nodes can
act as message producers or consumers, and the form
of event delivery is abstracted into a typical pattern. A
meta-signal in DoozyUI can be anything from a simple
value to a reference or action instruction. In our system,
meta-signals are widely used to transmit information for
system control.

C. Scenic Spheres

Scenic Spheres [11] is an online game based on virtual
reality technology. It uses AR/VR technology to immerse
users in infamous sites worldwide, encouraging them to learn
about geography through goal-based incentives. In the game
scenarios, users are placed in different locations worldwide.
In the virtual reality scenario, users must look around for
clues, and only after answering as many questions as possible
through the clues will new areas open up. The ﬁrst player to
ﬁnd the ﬂag will be the winner of the game. This incentive-
driven learning approach helps motivate students to learn [12],
[13].

This project is also a game project based on unity engine,
and C# is used as the programming language of the project. In
virtual reality technology, the biggest challenge is the user’s
depth perception of the virtual environment [14], [15]. Because
of image resolution limitations, the user may not be able
to accurately perceive the depth of the scene, which may
cause the user to experience headaches or dizziness. [16],
[17] Therefore, it is necessary to solve this problem as much
as possible. From a technical point of view, it is possible to
enhance the sense of reality and reduce user discomfort by
enhancing the picture resolution. In the ﬁeld of education,
there are more humane solutions. Teachers can reasonably
manage the time that students use VR devices and avoid
students staying in the virtual environment for too long.

III. METHODOLOGY

A. System Design

The primary modeling object of the virtual simulation teach-
ing software we have implemented is the Buick Verano engine.
Based on the 3D model, the parts are disassembled according
to the real disassembly order, and the orderly disassembly
process is realized. As a teaching system, besides the basic

interactive operations of disassembly and assembly, it also
needs to have teaching logic. For example, different parts are
divided into teaching chapters, and the difﬁculty is divided
into two stages: training and examination. In order to achieve
high-performance, scalable simulation software [18], [19], we
abstract the software architecture into the following form as
Figure 1.

Fig. 1. System Design

Speciﬁcally, our teaching software abstracts the behavior
interaction between the user and the teaching model
into
an independent behavior for coding. It then deﬁnes a series
of rules for this speciﬁc behavior. From a functional point
of view, we use to teaching and examination to distinguish
user interaction into two independent scenarios. In these two
scenarios, we endow the software with different behaviors.
The teaching mode is more focused on teaching disassembly
and assembly skills, so in this mode, we use text tutorials and
voice prompts on the way home to guide users to complete
the speciﬁed actions. Then, the examination mode focuses on
inspecting the user’s skill mastery. In this mode, we will turn
off the voice and text prompts, add a series of scoring rules
to score in different dimensions according to user operations,
and ﬁnally give a user evaluation of operational performance.
Further, we can also develop this storage system of user scores
from local records to online records so that teachers can count
students’ learning and improve teaching efﬁciency.

From the business scenarios described above, it can be
seen that the two scenarios of teaching and examination are
the difference between the education business. However, their
corresponding underlying logic is almost identical, and they
the same disassembly or installation process. This
are all
means that interaction and teaching logic can be decoupled
and placed at different architectural levels, as shown in the
Figure 1.

Architecturally, the teaching system uses a bottom-up de-
sign. Through the entity layer, the developer completes the
modeling of the demand scenario, builds the overall framework
of the entire system, and abstracts the teaching process into a
conﬁgurable data structure. After completing the construction
of the entity layer, we will enter the research and development
of the control layer, which subdivides the speciﬁc business
logic. Divide the teaching process and the speciﬁc logic of
model interaction into atomic logic, and complete the design
of these logic at the code level. After completing these logic
designs, we will eventually reassemble the basic atomic logic

VRVR Teaching SystemInteraction LayerScene BuilderUser InterfaceVR Model InteractionVRControl LayerEvent MessageTeaching SystemEntity LayerTutorialPhysical ModelJOURNAL OF LATEX CLASS FILES, VOL.

4

into complete business logic at the interaction layer accord-
ing to speciﬁc product requirements. Under this architecture,
product iteration only needs to modify the control layer in
the middle because the basic operation interaction scripts are
reusable. When encountering a new teaching scene or facing
a new interactive model, the separated entity layer can be
individually replaced with a new teaching scene. Different
processes can be used to interact with the original view.

Such a design ensures the high scalability of the system
and enables the template-based development of the auto-repair
immersive teaching software, which signiﬁcantly improves the
development efﬁciency. Similarly, such a design also helps
project team members efﬁciently divide their labor, splitting
the coupled business logic into different parts at the level of
code implementation and combining different functions more
ﬂexibly. The content of each level will be described in detail
below from a business perspective.

B. Interaction Layer

The interaction layer is the top-level structure in the soft-
ware system, which includes everything the user can see. From
the function division, we can divide it into three modules:
scene construction, model interaction, and user interface.

1) Scene Builder: Scene builder is the user’s surrounding
environment construction in the virtual reality scene. In order
to enhance the user’s sense of presence, our teaching system
sets the scene after the user enters the system as the teaching
demonstration room of FXB CO., LTD. This module belongs
to the ﬁeld of 3D modeling and does not involve coding work.
In this project, the company provided us with the 3D model
of the teaching demonstration room and its corresponding
textures and materials. We import the corresponding model
into our project in a static ﬁle and create an instance through
unity3d. These 3D object instances as operating environments
do not involve model interaction and are only rendered stati-
cally in the scene.

Fig. 2. Virtual Teaching Room in FXB CO., LTD.

2) Model Interaction: Model interaction is the core inter-
action scene of the system. This module contains two parts of
model construction and interaction. For model construction,
it is necessary to use the rendering technology provided by
Unity3D to give it natural light and shadow effects to show a
natural luster in the virtual scene.

The interaction of the model is the core part of the project.
The models can be divided into two types according to the

interaction properties in this project. One is non-interactive
static scenes, such as buildings, ﬂoors, shelves, Etc. The other
is an interactive dynamic model, a Buick Verano engine, and
a display and pad that can display content dynamically.

Fig. 3. side view

Fig. 4. exploded view

The Buick Verano engine is our teaching system’s teaching
object and the main object of model interaction. For an engine
model, we can think of it as a collection of parts. These parts
are the direct use objects of our ActionScript. Parts come in
various models and can have hundreds or thousands of shapes.
However, for a part, the combined actions with other parts are
limited, and these actions can be split into some basic actions:
rotate, press, take, hide, Etc. All teaching actions can be broken
down into combinations of these actions. The combination
of rotation and pressing is the most frequently used because
screwing is the most frequent among all actions.

Based on the above ideas, we can abstract the concept of
action into an atomic capability. For each atomic capability, we
use a C# script to describe the behavior of its object. However,
in actual business, each action in the teaching process is not
a single atomic action but a combination of some atomic
actions. Therefore, we need an efﬁcient way to organize
these combinations in an orderly manner, that is, conﬁgure
the content and order into a conﬁguration ﬁle for persistent
storage. In storage, we organize it as ﬁgure 5.

In the system operation process, we can uniquely identify a
currently ongoing task. During the running of the task, we will
read the corresponding step information under the task. When
the user triggers a step, the atomic action corresponding to the

JOURNAL OF LATEX CLASS FILES, VOL.

5

pad’s home page that carry the two main functional modules
of teaching and examination.

We provide three relevant entrances for the teaching func-
tion: grouping mode, structure display, and repair tool learning.
In the grouping mode, users can choose the chapter they wish
to enter. Entering the system in this portal will be recognized
as training mode, which does not rate the user’s operation
and will have voice prompts during the operation on how
to proceed to the next step. The remaining two portals do
not involve interaction with the model, and both present the
user with static resources in the form of tutorials. One of
them, Repair Tool Learning, is a video resource that will
automatically play an instructional video of the repair tool
when this button is clicked.

For the examination function, we provide two entrances,
task selection, and task details. The main entrance is task
selection, where users can enter the system and select the
disassembly or installation task they wish to perform. When
performing a disassembly or installation task, the large screen
will show the steps currently in progress. If the user needs
to see all the steps in this task, they can click on the Task
Details button, which triggers the screen to show all the steps
of a task.

We manage all views, buttons, and canvases in the pad
through DoozyUI. These views can be predeﬁned and stored
in DoozyUI’s database, and then their corresponding jump
relationships are organized in the scene by setting the timing
of when each view is shown and hidden.

C. Control Layer

The view layer is the direct object of user interaction with
the system, while the control layer is the inner logic that
implements all user interactions at the view layer. This section
will elaborate on how the user’s interaction actions affect the
system state.

1) Event Message: The calling relationship between each
module is very complicated in the teaching system. When the
user enters a particular chapter to study, the state of the objects
in the scene needs to be updated synchronously. Each time the
user completes an action, the corresponding action guide and
corresponding voice prompt need to be refreshed. In addition,
various multimedia teaching resources also require different
responses between the iPad and the screen according to the
user’s interactive behavior.

Because the call relationship is complex, the communication
between components will make the system more complicated
if the synchronous call method is used. Therefore, we design a
component communication method based on message passing,
which decouples message notiﬁcation and action to improve
coding efﬁciency.

The message system is implemented based on the DoozyUI
signal system. In DoozyUI, we can deﬁne the instance as the
sender or receiver of the message, and we just need to override
the SendSignal method and the Get method.

Fig. 5. conﬁg ﬁle structure

step will be executed, monitoring whether the user completes
the action as required.

3) User Interface: The user interface is an integral part
of the software system and is the entry point for the user
to interact with the system. In our virtual reality engine
disassembly system,
the user interface can be considered
everything the user can interact with within the virtual reality
scenario. However, to better abstract the user interface concept,
we only deﬁne the interaction part that affects the system state
as the user interface. The pad in the scene with the screen
displaying information satisﬁes the deﬁnition.

In a virtual reality scenario, the user interface can be a
combination of a series of views, each containing interactable
components (such as buttons, sliders, and selection boxes) and
non-interactive static resources (such as images and descrip-
tion text). These views are mainly managed and controlled
using the DoozyUI Manager in our design.

Fig. 6. Pad operation in teaching system

The pad is the main entry point for the user interface in
our system. As shown in the ﬁgure, we control the system
process through the tablet. We provide six entrances on the

In our system, a message consists of an action type and
a target object that needs to be changed. The delivery of
messages in the business has the following situations:

ActionTaskGroupGroup2task3action2action3Task4action2ActionTaskGroupGroup1task1action1action2task2action3Teaching ChapterJOURNAL OF LATEX CLASS FILES, VOL.

6

1) Display the corresponding media content on the large
screen according to the user’s click behavior on the iPad.
2) Trigger the update of ongoing task steps according to

the user’s disassembly and assembly actions

3) Automatically play the corresponding step prompt au-
dio according to the user’s disassembly and assembly
behavior

4) System state changes based on parts collision interaction

Under such an architecture, each component in the system
does not need to perceive the actions of other components and
only needs to design its behavior when sending and receiving
messages. Such a structure helps different components to
develop independently. After deﬁning the speciﬁcations of the
good news, the development can be completed independently
improving team collaboration
by different
efﬁciency.

team members,

2) Teaching System: Teaching is the core function of our
system. We have designed three teaching modes: teaching,
practice, and examination. In the teaching mode, we provide an
automatic voice guidance function. After the user completes a
step, the voice guidance of the following action will be played
automatically. At the same time, in the teaching mode, users
can also view video tutorials to learn the usage of various
tools and basic engine disassembly and assembly skills. The
practice mode is an advanced version of the teaching mode.
There is only brief operation prompts in the practice mode
instead of the complete teaching process. In this mode, users
have total freedom to practice the practical knowledge they
have learned.

Further, we designed an examination mode to assess the
user’s mastery of a speciﬁc task. In examination mode, the
weight of each step can be predeﬁned. When a user completes
a task, the completion status will be recorded, and the machine
will score the user’s performance according to the weight of
each step. There will be no prompt content in this mode, the
user needs to complete the examination independently, and the
terminal will record the results.

The teaching system is organized based on the user inter-
face, and the user can freely choose different mode entrances
on the Pad in the scene. When the user enters a speciﬁc task,
the system will recognize which teaching mode the user is
currently in and send a message indicating entering the task
to the scene. The teaching scene will determine how to refresh
the page at this time according to the content of the message,
reset the engine state to the position required by the task, and
set the prompt content of voice and text to the corresponding
teaching mode.

D. Entity Layer

Entity layer is the storage layer for data. We deﬁne our
business entities in this layer and store entity information
persistently. After decoupling system design and requirement
design, we can abstract the concept of requirement entity. In
this teaching system, the demand entities that change with the
business changes in the system can be roughly divided into
two types: tutorial entities and physical model entities.

1) Tutorials Entity: Tutorial entities represent what students
learn in our actual needs. In this project, the tutorial for the
students is the disassembly and assembly of the Buick Verano
engine. When the demand scene is changed to the engine
disassembly and assembly tutorial of other brands, or even
the disassembly and assembly of other items, we need to be
able to change the tutorial entity easily. Therefore, our system
provides the abstract concept of tutorials in a conﬁgurable
form.

2) Physical Entity: The physical entity is a collection of
models in a scene. In a user interface, the object that the user
directly interacts with is the model in the scene. However,
these models should not be coupled to the system design, and
we want our system to be usable in any simulation scenario.
The physical models in the system include static scenes: demo
room, console, toolbox, and dynamic models: engine, pad, Etc.
These contents make up our physical entities.

We build these entities in a statically modeled fashion. Each
of these models is a separate 3D model ﬁle. We preset their
shape, color, and physical properties and import them into
Unity. These model ﬁles can also be reused in other projects.

IV. EXPERIMENTAL SETUP

Evaluating our systems is also an essential part of our work.
Our system is a teaching system based on virtual reality so that
the evaluation can be carried out from the two dimensions of
virtual reality experience and teaching experience.

The teaching experience cannot be quantiﬁed using speciﬁc
performance data but must be collected from user experience
report statistics after a small-scale pilot to conclude. Therefore,
the assessment process in this report does not involve the
assessment of the teaching experience. Promoting this project
in some vocational schools and recycling the experience report
will be critical work in the future [20], [21].

In the ﬁeld of virtual reality, the quality of experience can

be assessed from the following dimensions:

1) Immersion. This is a more speciﬁc concept, indicating
whether the user’s perception of the scene after entering
the system is real. This dimension needs to be evaluated
by multiple factors, including the clarity of the video,
the spatial sense of the audio, and so on.

2) The viewing experience. Whether the information re-
ceived by the user’s optic nerve in the virtual reality
scene is close to the real scene,
this dimension is
strongly related to video quality and is also part of im-
mersion. Usually, we sample the duration and frequency
of freezes in the system to represent how smooth the
video information is. We have an intuitive performance
indicator representing the stuttering situation and the
frame rate. In addition, we also evaluate the black border
ratio of the video, which is the proportion of black
borders in the re-rendered picture when the head is
turned in the scene.

3) Interactive experience. Interaction is a further require-
than viewing. In this dimension, we must not
ment
is
only consider the smoothness of video output. It
also necessary to evaluate whether the system’s timely

JOURNAL OF LATEX CLASS FILES, VOL.

7

feedback when the user interacts. In addition to the
subjective experience, we can abstract some perfor-
mance indicators to represent the interaction quality.
The more commonly used evaluation indicators are the
number of GPU drawing calls. On the premise that the
machine performance remains unchanged, reducing the
number of GPU drawings can improve the efﬁciency
of image drawing, thereby improving the smoothness of
interaction. [22]

Based on the above evaluation ideas, we decided to evaluate
the system performance based on the number of frames
rendered and screen draw calls. Our benchmark platform is
intel 8-core i7-8700 CPU, 16G memory, NVIDIA GeForce
GTX 1080 discrete graphics, and the VR wearables used
for testing are: HTC Vive head-mounted display device and
accessories, HTC Vive infrared base station.

V. RESULTS

The frame rate test directly manifests the user’s viewing
experience, and rendering efﬁciency affects the frame rate.
[23] In Unity’s rendering engine,
the CPU is responsible
for preparing the model and related materials and textures
and passing these data to the graphics processor for graphics
rendering. This process is called DrawCall. [24], [25] In
large-scale projects, because the model properties in the scene
are more complex, the number of DrawCalls will be more.
We consider using batch processing technology to reduce the
number of draw calls per second. Batch processing means that
objects of the same material can be processed simultaneously.
In the Unity rendering engine, batch processing is divided into
dynamic and static batch processing. In the scene, dynamic
objects can be moved and transformed, and the rendering
results of these objects will change with lighting and shadows.
Unity treats all objects in the scene as dynamic objects by
default and performs dynamic batch operations. However, in
our system, there are a lot of static objects that will not be
moved and deformed after rendering in the scene so that
they can be rendered statically at a lower cost [26]. In the
system, we manually annotate these objects as static objects
so that Unity can recognize them and batch them statically,
thus reducing the number of draw calls per second. Here is
the data we get after static batch optimization [27], [28]:

all dynamic load
with static load

DrawCall/s(max)

DrawCall/s(avg)

880
795
TABLE I
DRAW CALL TEST

796.346
663.618

It can be seen from the test results that after static batch
processing of some objects,
the number of draw calls is
signiﬁcantly reduced, and a better frame rate performance is
expected.

Before running the frame rate test, we need to introduce
the concept of vertical sync. When a screen displays content,
the pixels need to be refreshed from the horizontal direction,
and each horizontal scan line is stacked in the vertical direction

to form a complete picture. The vertical synchronization pulse
signal is added between two frames of rendering. The graphics
processor pauses and waits for the refresh time of the display
after rendering a picture so that
the output frame rate is
consistent with the display’s refresh rate. It can be seen from
this that enabling vertical synchronization helps the graphics
processor to synchronize the refresh rate of the display for
graphics rendering, effectively avoids screen tearing, and can
improve the user’s viewing experience. [29] Therefore, we
conducted frame rate tests with V-Sync turned on and off,
respectively, and the results are as follows. f_t means frame
time and f_r means frame rate.

Max f_t(ms) Avg f_t(ms) Max f_r(times/ms) Avg f_r(times/ms)

V-Sync_on
V-Sync_off

470.21
445.72

15.06
3.94

519.48
444.84

65.61
65.61

TABLE II
FRAME RATE TESTING

It can be seen that the drop in frame rate after the vertical
sync is turned on is in line with expectations. It is not a
performance drop but a smoother screen refresh. The screen
will be smoother in the real machine experience after turning
on vertical sync.

VI. DISCUSSION
This project developed a virtual reality based engine disas-
sembly system collaborating with the company from a real
user scenario. The system has a highly scalable software
architecture and can be rapidly iterated through conﬁgurability
in the face of new demand scenarios [30]. The performance
test veriﬁed that the system can run smoothly on today’s
mainstream conﬁguration machines. The high scalability and
easy migration characteristics make an essential contribution
to the system’s popularity.

With the development of virtual reality technology, its prod-
uct form has gradually evolved from an immersive viewing
experience that allows for human-computer interaction [31].
With the constant maturity of the underlying technology,
developers have quickly developed human-computer interac-
tion programs through tools such as unity. In this context,
developers can stimulate creativity and apply virtual reality
technology in different scenarios, thus solving the challenges
in these ﬁelds. On the other hand, our project applies advanced
virtual reality technology to teach automotive engine repair.
Through this technology, we hope to reduce the learning
cost of these technicians, improve teaching efﬁciency, and
further promote the development of the teaching model for
this subject.

We believe that this teaching system will be deployed in
many vocational institutions and soon contribute to improving
vocational education quality. At the same time, we will fur-
ther optimize the system in the future and strive for further
improvement in interactive experience and performance.

REFERENCES

[1] G. Lan, M. van Hooft, M. De Carlo, J. M. Tomczak, and A. E. Eiben,
“Learning locomotion skills in evolvable robots,” Neurocomputing, vol.
452, pp. 294–306, 2021.

JOURNAL OF LATEX CLASS FILES, VOL.

8

[28] G. Lan, J. M. Tomczak, D. M. Roijers, and A. E. Eiben, “Time efﬁciency
in optimization with a bayesian-evolutionary algorithm,” Swarm and
Evolutionary Computation, vol. 69, p. 100970, 2022.

[29] P. Bourke, “idome: Immersive gaming with the unity3d game engine,”

Computer Games and Allied Technology, vol. 9, 2009.

[30] G. Lan, M. Jelisavcic, D. M. Roijers, E. Haasdijk, and A. E. Eiben,
“Directed locomotion for modular robots with evolvable morphologies,”
in International Conference on Parallel Problem Solving from Nature.
Springer, 2018, pp. 476–487.

[31] G. Lan, L. De Vries, and S. Wang, “Evolving efﬁcient deep neural
networks for real-time object recognition,” in 2019 IEEE Symposium
Series on Computational Intelligence (SSCI).
IEEE, 2019, pp. 2571–
2578.

[2] G. Lan, M. De Carlo, F. van Diggelen, J. M. Tomczak, D. M. Roijers,
and A. E. Eiben, “Learning directed locomotion in modular robots with
evolvable morphologies,” Applied Soft Computing, vol. 111, p. 107688,
2021.

[3] Z. Gao and G. Lan, “A neat-based multiclass classiﬁcation method
with class binarization,” in Proceedings of the Genetic and Evolutionary
Computation Conference Companion, 2021, pp. 277–278.

[4] G. Lan, J. Liang, G. Liu, and Q. Hao, “Development of a smart ﬂoor
for target localization with bayesian binary sensing,” in 2017 IEEE
31st International Conference on Advanced Information Networking and
Applications (AINA).
IEEE, 2017, pp. 447–453.

[5] J. Xie, “Research on key technologies base unity3d game engine,” in
2012 7th international conference on computer science & education
(ICCSE).

IEEE, 2012, pp. 695–699.

[6] F. Messaoudi, G. Simon, and A. Ksentini, “Dissecting games engines:
The case of unity3d,” in 2015 international workshop on network and
systems support for games (NetGames).

IEEE, 2015, pp. 1–6.

[7] S. Wang, Z. Mao, C. Zeng, H. Gong, S. Li, and B. Chen, “A new
method of virtual reality based on unity3d,” in 2010 18th international
conference on Geoinformatics.

IEEE, 2010, pp. 1–5.

[8] L. Chen and Z. Luo, “Practice and research of htc vive controller
functions in virtual reality interaction,” Advances in Social Science.
Education and Humanities Research (ASSEHR), vol. 93, 2017.

[9] ExtendRealityLtd, “Extendrealityltd/vrtk: *beta* - an example of how
to use the tilia packages to create great content with vrtk v4.” Apr
2016. [Online]. Available: https://github.com/ExtendRealityLtd/VRTK

[10] B. Kok, Beginning Unity Editor Scripting. Springer, 2021.
[11] S. J. Bryan, A. Campbell, and E. Mangina, “Scenic spheres-an ar/vr edu-
cational game,” in 2018 IEEE Games, Entertainment, Media Conference
(GEM).

IEEE, 2018, pp. 1–9.

[12] M. Lynch, “Top virtual reality apps that are changing education,” The

Tech Advocate, 20.

[13] F. J. Fowler Jr, Survey research methods. Sage publications, 2013.
[14] G. Lan, Z. Luo, and Q. Hao, “Development of a virtual reality tele-
conference system using distributed depth sensors,” in 2016 2nd IEEE
International Conference on Computer and Communications (ICCC).
IEEE, 2016, pp. 975–978.

[15] G. Lan, J. Sun, C. Li, Z. Ou, Z. Luo, J. Liang, and Q. Hao, “Development
of uav based virtual reality systems,” in 2016 IEEE International Con-
ference on Multisensor Fusion and Integration for Intelligent Systems
(MFI).

IEEE, 2016, pp. 481–486.

[16] J. P. Rolland, R. L. Holloway, and H. Fuchs, “Comparison of optical
and video see-through, head-mounted displays,” in Telemanipulator and
Telepresence Technologies, vol. 2351. SPIE, 1995, pp. 293–307.
[17] D. Van Krevelen and R. Poelman, “A survey of augmented reality
journal of

technologies, applications and limitations,” International
virtual reality, vol. 9, no. 2, pp. 1–20, 2010.

[18] G. Lan, J. Chen, and A. E. Eiben, “Simulated and real-world evolution
of predator robots,” in 2019 IEEE Symposium Series on Computational
Intelligence (SSCI).
IEEE, 2019, pp. 1974–1981.

[19] G. Lan, J. Chen, and Eiben, A. E., “Evolutionary predator-prey robot
systems: From simulation to real world,” in Proceedings of the Genetic
and Evolutionary Computation Conference Companion, 2019, pp. 123–
124.

[20] H. Xu, G. Lan, S. Wu, and Q. Hao, “Online intelligent calibration of
cameras and lidars for autonomous driving systems,” in 2019 IEEE
Intelligent Transportation Systems Conference (ITSC).
IEEE, 2019,
pp. 3913–3920.

[21] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, and A. E. Eiben,
“Real-time robot vision on low-performance computing hardware,” in
2018 15th international conference on control, automation, robotics and
vision (ICARCV).

IEEE, 2018, pp. 1959–1965.

[22] C. Ware and R. Balakrishnan, “Reaching for objects in vr displays: lag
and frame rate,” ACM Transactions on Computer-Human Interaction
(TOCHI), vol. 1, no. 4, pp. 331–356, 1994.

[23] R. T. Apteker, J. A. Fisher, V. S. Kisimov, and H. Neishlos, “Video
acceptability and frame rate,” IEEE multimedia, vol. 2, no. 3, pp. 32–
40, 1995.

[24] C. Dickinson, Unity 5 Game Optimization. Packt Publishing Ltd, 2015.
[25] ——, Unity 2017 Game Optimization: Optimize All Aspects of Unity

Performance. Packt Publishing Ltd, 2017.

[26] J. Hocking, Unity in Action: Multiplatform game development in C.

Simon and Schuster, 2022.

[27] P. Yin, “Research on design and optimization of game ui framework
based on unity3d,” in 2019 International Conference on Electronic
Engineering and Informatics (EEI).

IEEE, 2019, pp. 221–223.

