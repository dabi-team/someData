1
2
0
2

p
e
S
8
2

]
E
S
.
s
c
[

1
v
8
6
7
3
1
.
9
0
1
2
:
v
i
X
r
a

To VR or not to VR: Is virtual reality suitable to understand
software development metrics?

David Moreno-Lumbreras
d.morenolu@alumnos.urjc.es
EID @ Universidad Rey Juan Carlos, Bitergia
Móstoles, Spain

Daniel Izquierdo-Cortázar
dizquierdo@bitergia.com
Bitergia
Madrid, Spain

Gregorio Robles
gregorio.robles@urjc.es
Universidad Rey Juan Carlos
Madrid, Spain

Jesus M. Gonzalez-Barahona
jesus.gonzalez.barahona@urjc.es
Universidad Rey Juan Carlos
Madrid, Spain

ABSTRACT
Background/Context: Currently, the usual interface for visual-
izing data is based on 2-D screens. Recently, devices capable of
visualizing data while immersed in VR scenes are becoming com-
mon. However, it has not been studied in detail to which extent
these devices are suitable for interacting with data visualizations
in the specific case of data about software development.
Objective/Aim: In this registered report, we propose to answer
the following question: “Is comprehension of software development
processes, via the visualization of their metrics, better when pre-
sented in VR scenes than in 2D screens?” In particular, we will study
if answers obtained after interacting with visualizations presented
as VR scenes are more or less correct than those obtained from
traditional screens, and if it takes more or less time to produce
those answers.
Method: We will run an experiment with volunteer subjects from
several backgrounds. We will have two setups: an on-screen ap-
plication, and a VR scene. Both will be designed to be as much
equivalent as possible in terms of the information they provide.
For the former, we use a commercial-grade set of Kibana-based
interactive dashboards that stakeholders currently use to get in-
sights. For the latter, we use a set of visualizations similar to those
in the on-screen case, prepared to provide the same set of data using
the museum metaphor in a VR room. The field of analysis will be
related to modern code review, in particular pull request activity.
The subjects will try to answer some questions in both setups (some
will work first in VR, some on-screen), which will be presented
to them in random order. To draw results, we will compare and
statistically analyze both the correctness of their answers, and the
time spent until they are produced.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEM ’21, October 11–15, 2021, Bari, Italy
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

CCS CONCEPTS
• Software and its engineering → Software development pro-
cess management; Software development methods.

KEYWORDS
virtual reality, dashboards, controlled experiment, code review, pull
requests

ACM Reference Format:
David Moreno-Lumbreras, Gregorio Robles, Daniel Izquierdo-Cortázar,
and Jesus M. Gonzalez-Barahona. 2021. To VR or not to VR: Is virtual reality
suitable to understand software development metrics?. In ESEM ’21: 15th
ACM/IEEE International Symposium on Empirical Software Engineering and
Measurement, October 11-15, 2021, Bari, Italy. ACM, New York, NY, USA,
7 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Software engineers mainly interact with source code using a key-
board and a mouse, typically viewing it on 2D monitors. This way
of interacting does not take advantage of the many benefits of
movement and perception that humans have. This is not the case
of virtual reality (VR) immersion, where the subject works in a
virtual 3D environment. In the last years, affordable VR devices
have emerged, and new standards, such as WebXR [7] and We-
bGL [6], have become available, so that VR applications can be
made portable to different platforms, and easily integrable with
other applications and APIs. Therefore, we are now at a point where
using VR for interacting with data visualizations is feasible in many
environments.

In fact, in the specific case of software engineering, some schol-
ars argue that the use of VR allows for environments that may
make practitioners face lower learning curves, be more creative
and achieve higher productivity [5]. However, to our knowledge,
there is little evidence that VR can provide better, or even similar
results to on-screen visualizations, in the specific case of software
development data.

Bitergia1 is a company offering commercial services in the area
of software development analytics, specialized in offering software
development data visualizations in web-based 2D dashboards. Re-
cently, to explore visualizing software development data in VR,

1https://www.bitergia.com

 
 
 
 
 
 
ESEM ’21, October 11–15, 2021, Bari, Italy

Moreno-Lumbreras et al.

Bitergia and the Universidad Rey Juan Carlos have developed Babi-
aXR,2 a toolset for visualizing data in 3D, both on-screen and in
VR devices.

The fact that we can design a visualization of exactly the same
data, with very similar charts, in two different settings (2D charts
in traditional screens, and 3D charts in VR devices) gives us the
chance of trying fair comparisons between both. In particular, we
can design VR scenes with charts similar to the 2D, on-screen
visualizations that Bitergia uses for its customers, and then run
an experiment where subjects use those settings so that we can
compare results.

Hence, in this report we propose a controlled experiment for
the comparative evaluation of two approaches: 2D data visual-
ization vs. VR immersive visualization of data about software
development. The main aim is to test if VR immersion is at least as
effective and efficient as on-screen 2D for the visualization of the
same data. We will design a set of comprehension tasks, formulated
as the answer to a question, and analyze both the correctness of
answer and the time to get an answer.

The field of analysis for our experiment will be pull request
activity. Pull request, as part of modern code review [1, 16], is a
software development activity that has been widely researched
by academia in the last years [8, 9, 18]. It is of major interest to
industry and practitioners as it is very human-intensive and often
the cause of bottlenecks and inefficiencies [15].

2 BACKGROUND
2.1 2D Visualizations
With the help of GrimoireLab,3 Bitergia gathers and enriches
software development data to create a set of pre-existing dashboards.
In particular, there are five panels/dashboards whose main goal is
to show the several aspects of the pull request process.

(1) Overview (see Figure 1a): provides a general overview of
the activity and the authors of each of the pull requests.
People and organizations are the key pieces of information
to display.

(2) Efficiency (see Figure 1b): gives information about the gen-
eral trend of the code review process and its time to merge
pull requests. The main visualizations are the speedometer
that helps understand how good the process has been in the
chosen period of time. And two extra visualizations that pro-
vide the BMI (Backlog Management Index) for pull requests,
a known metric in the maintenance ecosystem [10], and the
lead time.

(3) Timing (see Figure 1c): displays information about the time
to close pull requests. It offers insight to the user about the
evolution of the time to close a pull request over time. This
is intended to visually find bottlenecks in the development
process.

(4) Backlog (see Figure 1d): displays the remaining open pull
requests over time and the newest and oldest of them. This
aims at identifying old pull requests still open with the goal
of cleaning this backlog and focus on the important ones.

(5) Time to Attention: in any development process the time
until a project member answers a pull request is key to have
a proper submitter-reviewer interaction. This panel helps
understand how good the project is when collaborating with
others or within the project.

An example of the described panels in Kibana is shown in Fig-
ure 1. The data represented is not the data that will be used in the
experiment, but the visualizations on them are the ones that will be
used. All the panels represent the data with a set of different charts,
from simple charts showing raw data as tables to other common
charts as simple bars, pies, or line charts.

All these panels are now part of existing production solutions
for several organizations that are using them to analyze their code
review process. The CHAOSS Community or the FINOS Foundation
are a couple of existing and public dashboards4 although these
panels are deployed across all of the customers of the Bitergia
Analytics Platform that use GitHub. This includes those using them
in open source projects, or those using GitHub Enterprise edition
and analyzing internal developments.

These five panels are an example of the out-of-the-box function-
ality provided by the GrimoireLab open source project. However,
customers require certain actions to polish the dashboards and ade-
quate them to their needs. The issue is indeed related to the lack of
space to represent all of the data that interests a stakeholder. The
five panels cover specific use cases that have become standard in the
tool, but each of them needs the other four to offer a complete pic-
ture of the code review process. This requires the data consumer to
move from one to the other, apply and fix filters across dashboards
and other actions to successfully navigate through them.

When designing the panels, the developers had a list of associated

questions (i.e., goals) for each of them. In particular:

(1) Overview: Number of organizations sending pull requests,
status over time of the number of open and closed pull re-
quests, number of people submitting pull requests over time,
distribution of the organizations sending pull requests over
time.

(2) Efficiency: How good is the project dealing with pull requests,
the workload adequacy of the project and its evolution (Back-
log Management Index - Review Efficiency Index), and lead
time (latency between the initiation and completion of a
process).

(3) Timing: Past bottlenecks, time to merge pull requests per

organization.

(4) Backlog: Accumulated time of the backlog over time, backlog

over time, oldest pull requests.

(5) Time to Attention: How fast are developers answering to a

pull request.

On the other hand, and as noted before, there are aspects that
require to combine information in more than one panel. Among
these, we can list the following:

• Organizational performance (number of still open pull re-
quests, time to close a pull requests, time to be closed a pull
request, lead time).

2https://babiaxr.gitlab.io/ (see also section 3.3)
3https://chaoss.github.io/grimoirelab/ (see also section 3.3)

4http://chaoss.biterg.io/

To VR or not to VR: Is virtual reality suitable to understand software development metrics?

ESEM ’21, October 11–15, 2021, Bari, Italy

• Project performance (number of still open pull requests, time
to close a pull requests, time to be closed a pull request, lead
time).

• Relation between the time to answer a pull requests and the

time to be reviewed or the time to review.

• Comparison of organizations contributing to a project by
their pull request review time, existing open backlog (what
they have opened).

2.2 VR Visualizations
The goal of the VR environment is to take advantage of the 3 dimen-
sions that it provides. We will use BabiaXR for the development of
VR visualizations, providing a scene where the visualizations will
be organized.

BabiaXR provides a set of different 3D visualizations, including
some with three axis (e.g., bars, cylinders, bubbles) to represent
depth, height, and width in the same visualization, and cylinder
charts that offer the radius for expressing one more metric (com-
pared to a common bars chart). BabiaXR also provides 3D treemap
visualizations that can be used to represent the code city metaphor.
Figure 2 shows an example of the scenes that can be created with
BabiaXR. The scene represented in the figure is not the scene that
we will use in the experiment, although the general effect will be
similar. In the scene we can see some visualizations that go beyond
what Kibana can provide, taking advantage of the three dimensions.
Thanks to them, we can aggregate two or more Kibana visualiza-
tions in one: for example, we can use the 3D bars map in BabiaXR
aggregating two or more simple Kibana barchart visualizations.

As already reported, the aim of this registered report is to rep-
resent the information displayed in the five panels described in
Section 2.1 in a VR scene, having similar visualizations but taking
advantage of the availability of new geometry metrics (e.g., new
axis, radius, city) for combining or improving the presented in the
on-screen environment.

2.3 Related studies
Wettel et al. [17] proposed one of the first experiments to validate
the 3D visualization based on the city metaphor as a way to compre-
hend some aspects of software systems. Our experiment can have
this type of visualization in the VR immersion, as BabiaXR has
included it in its stack. More recently, Romano et al. [14] conducted
a controlled experiment where they asked participants to perform
program comprehension tasks with the support of the Eclipse In-
tegrated Development Environment with a plugin for gathering
code metrics and identifying bad smells, and a visualization tool
of the city metaphor displayed on a standard computer screen and
in immersive virtual reality. The results show that the participants
using the VR environment were faster than the participants using
the computer screen, but limited to the city metaphor. Our study
will encompass more types of data visualizations. Merino et al. [11]
used the city metaphor visualization to visualize software systems
in three settings: i) a computer screen, ii) an immersive 3D scene
(VR), and iii) a physical 3D printed model. Then, they carried out
an experiment that measured the effectiveness of visualizations
in terms of performance, recollection, and user experience. They
found that participants using the VR scene obtained the highest

recollection, while participants using the 3D printed model were
the fastest ones resolving tasks. Compared to this study, our ex-
periment includes more types of visualization and will encompass
more metrics and aspects about software development projects.

3 EXPERIMENT PLANNING
3.1 Goal
The main objective of the experiment is to analyze if the compre-
hension of software development processes (in particular, modern
code review), via the visualization of their metrics, is better when
presented in VR scenes than in 2D screens.

3.2 Participants and settings
Our experiment will be run with at least 30 subjects from both
academia (students and researchers) and industry. All subjects will
be screened via a questionnaire, to ensure a minimum knowledge
about software development projects (only those passing the ques-
tionnaire will be considered for the experiment). Recruited subjects
will not have any relation with the study or the paper. Before par-
ticipating in the experiment, subjects will fill a demographics form,
which will be used to determine the confounding variables.

Subjects will run the experiment in two settings, their order

being randomly decided for each of them:

On-screen: The setting will be a computer with a web browser
showing a Kibana dashboard, with several panels including all
visualizations needed to answer the questions (as described in Sec-
tion 2.1).

Virtual Reality: The setting will be VR scene (as described in
Section 2.2), in which subjects will immerse using the VR browser
in an Oculus Quest headset. Figure 3 shows a participant during
the VR experiment and the screencast for the supervisor.

In each setting, subjects will be presented with visualizations for
data from a different project (of the two projects that will be used),
and the order in which projects are presented will also be random.
All subjects will be presented with the same questions, but each
question will be presented to each subject randomly only in one of
the two settings, and for one of the two projects.

In both settings, subject’s answers will be aloud, and the screen
or VR scene as presented to the subject will be screencasted. Both
the screencast and subject’s answers will be recorded (and then
transcribed). The setting will be followed by a supervisor, who
provides support if needed, having access to the screencast and
communicating with the subject via voice. If sanitary conditions
recommend isolated work, the supervisor will participate remotely,
following the screencast, and using some conferencing application
to talk with the subject.

We will ensure that all the participants have experience in differ-
ent relevant topics about software development using a question-
naire, reducing the threat that they are not competent enough. We
will also ask for their experience with VR and Kibana to mitigate
the threat that the participant’s experience is not fairly distributed.

3.3 Tools
For the data gathering of the project, we will use the GrimoireLab
platform. GrimoireLab, a free, open source software for software
development analytics [4], is a toolset that includes modules to

ESEM ’21, October 11–15, 2021, Bari, Italy

Moreno-Lumbreras et al.

(a) Overview

(b) Efficiency

(c) Timing

(d) Backlog

Figure 1: GrimoireLab panels in Kibana

Figure 2: Example of a BabiaXR scene

retrieve data from many kinds of software repositories [3], store
it in a database, and then process and analyze it, producing many
different metrics. It also includes visualization modules that can
be used to interact with the data via traditional, on-screen, web
browsers. GrimoireLab is now a project under the umbrella of the
Linux Foundation CHAOSS community.5

Data produced by GrimoireLab can be fed to BabiaXR as well,
so that the same pipeline can be used to visualize data in 2D screens
and in VR devices. BabiaXR is a toolset, developed by the authors,
for 3D data visualization in the browser. It is based on A-Frame,6 an

Figure 3: An example of the VR experiment setup

open web framework to build 3D, augmented reality (i.e., AR), and
VR experiences in the browser. A-Frame extends HTML with new
entities allowing to build 3D scenes as if they were HTML documents,
using techniques common to any front-end web developer. A-Frame
is built on top of Three.js,7 which uses the WebGL API available
in all modern browsers.

5https://chaoss.community
6A-Frame: https://aframe.io

7Three.js: https://threejs.org

To VR or not to VR: Is virtual reality suitable to understand software development metrics?

ESEM ’21, October 11–15, 2021, Bari, Italy

BabiaXR extends A-Frame by providing components to create
visualizations, simplify data retrieval, and manage data (e.g., data
filtering or mapping of fields to visualization features). Scenes built
with BabiaXR can be displayed on-screen, or in VR devices, includ-
ing consumer-grade headsets. Figure 2 shows a sample scene built
with BabiaXR. BabiaXR is open source: Its source code is available
on GitLab8 and it can be installed with npm.9

For the development of the on-screen participants, we will use
Kibana. Kibana is a free and open frontend application that sits
on top of the Elastic Stack, providing search and data visualization
capabilities for data indexed in ElasticSearch. More details about
the panels of Kibana are described in Section 2.1.

3.4 Datasets
The datasets used in the experiment will consist of data from two
software development projects, to be defined. The characteristic
of the projects will be similar for doing a fair comparison and
decrease the threat of having an unbalanced amount of data in one
of the settings. For the data retrieval, we will use GrimoireLab
that provides a complete set of metrics of a software development
project, focusing the experiment on the process analysis (efficiency
of tickets/issues, merge/pull requests, etc.).

3.5 Experimental material
To carry out the experiment we will use common computers for the
on-screen part of the experiment. Since Kibana is a web application
and can be used on any computer with a modern web browser,
participants will be given the possibility to use their own – but in
the event that they do not have one, the experimenter will offer
one. For the virtual reality phase, we will use Oculus Quest 2 VR
glasses. The experimenter will offer the participant these glasses to
carry out the experiment, but if the participant has glasses of the
same model, they can use them. In any case, the scene shown in
VR is a web scene based on webXR standards, so we plan a version
so that any virtual reality device with a modern browser will have
access to it, making the realization of this experiment possible in a
remote setting, if required.

3.6 Tasks
With the help of Bitergia and its experience in analysis of software
development projects, we will define a set of tasks to analyze both
the correctness of the task solutions and the task completion time.
These tasks are yet to be defined; they will be related to a general
view of the pull request activity, to authors, to project efficiency,
among others.

3.7 Hypotheses or research questions
The main research question of our study is:

RQ: “Is comprehension of software development pro-
cesses, via the visualization of their metrics, better when
presented in VR scenes than in 2D screens?”.

This question tests the hypothesis that presenting visualizations
in VR, where available real state is much more abundant (you

8https://gitlab.com/babiaxr/aframe-babia-components
9https://npmjs.org/package/aframe-babia-components

can have visualizations all around you, up and down if needed),
will allow for a better and faster understanding. The hypothesis
is disputable because there are factors that work against it, such
the difficulties that perspective and distance may be cause to the
adequate perception of magnitudes.

For answering the research question, and validating (or not)
the hypothesis, we will focus on specific, measurable aspects of
the answers produced by the subjects: accuracy (as a proxy for
correctness), and time to answer (as a proxy for efficiency). Thus,
we can refine our main RQ in two:

RQ1: “Answers are more accurate in VR than on-screen?”
(alternate: more accurate on-screen)
RQ2: “Answers are obtained faster in VR than on-screen?”
(alternate: faster on-screen)

To ensure that questions to subjects are useful for answering
these two RQs, we will design them so that their answers are nu-
meric.

3.8 Variables
The independent variable that will be input for our experiment
is the setting (on-screen dashboards in VR scene) in which each
answer will be answered. Dependent variables will be the answers
that our subjects produce, and more concretely, how much deviated
they are from the correct answer, and how long it took to produce
them.

In more detail, for each Experiment Question (EQ), the variables

will be:

• Independent variable:

Name : Setting
Description : Setting in which the question was answered

by the subject

Scale : Categorical: “Screen” or“VR”.
Operationalization : Subject answers after interacting with
visualizations in a 2-D screen, or after interacting with
visualizations immerse in virtual reality.

• Dependent variable “time-to-answer”:

Name : TimeToAnswer
Description : Time to answer the question
Scale : Real number
Operationalization : Number of seconds from the ques-

tion is understood to the answer is produced.

• Dependent variable “Error”:

Name : Error
Description : Error in answering the question, with respect

to the right answer.
Scale : Real number
Operationalization : Relative error of the magnitude pro-
duced as answer with respect to the true value of that
magnitude.

• Confounding variable “Experience with Kibana”:

Name : ExperienceKibana
Description : Overall experience with Kibana dashboards
Scale : Categorical: “None”, “small”, “medium”, “high”
Operationalization : Question to the subject, with the op-
erations of the scale, including an indicative number of
hours for each category.

ESEM ’21, October 11–15, 2021, Bari, Italy

Moreno-Lumbreras et al.

• Confounding variable “Experience with VR”:

Name : ExperienceVR
Description : Overall experience with VR devices
Scale : Categorical: “None”, “small”, “medium”, “high”
Operationalization : Question to the subject, with the op-
erations of the scale, including an indicative number of
hours for each category.

• Confounding variable “Experience in software development”:

Name : Experience
Description : Overall experience in software development
Scale : Integer number
Operationalization : Question to the subject about their
experience in software development, with a number of
years of experience as answer.
• Confounding variable “Position”:

Name : Position
Description : Position of the subject.
Scale : Categorical: “practitioner”, “academic”, “student”
Operationalization : Question to the subject about their
position with the three options in the scale as answer.

We will control for confounding variables by splitting the an-
swers according to them, and applying statistical controls to distill
their effects.

3.9 Design
To understand whether VR is well suited to present software development-
related visualizations compared to the traditional 2D on-screen
representation we will conduct a controlled experiment. Therefore,
we will follow the ACM SIGSOFT Empirical Standards [13] for the
experiment design, specifically those aspects that are relevant for
the quantitative method for experiments with humans, to satisfy
all essential attributes and a part of the desirable ones. We will
follow the design of a ”One Factor, Two Level” experiment, being
the factor the independent variable (the setting), and the two levels
the two values it may have (VR or in-screen). Since participants
will be presented randomly with the order for the settings (first
VR or first on-screen), this experiment can be treated and formally
divided into two “One Factor, Two Level” sub-experiments.

3.10 Procedure
The analysis will be completed with a qualitative study, based on
(possibly semi-structured) interviews to a sample of the people who
will go through the experiment, to learn about the details of their
experience.

For preparing the study, we will follow some procedures such as
preparing the settings to perform the experiment, and testing them
in advance, to avoid some mistakes that only the actual testing of
the experiments will show.

3.11 Analysis procedure
For determining the behavior we will use statistical analysis for the
completion time of the tasks and we will measure the significant
differences with the best metric according to the sample size of
the participants. We will also collect feedback about the difficulty
of the tasks, avoiding the deviations between times, and deleting
the outliers. Since the number of participants could be as low as

of 30 participants, estimating the distribution of answers seems
difficult. Therefore, we will use non-parametric tests, which do not
assume specific distributions, to assess if the differences between
answers in both settings are statistically significant, and determine
if the results are meaningful. In principle, we plan to run the Mann-
Whitney U Test [12] and calculate the corresponding Cliff’s Delta
effect size [2].

4 EXECUTION PLAN
Before conducting the experiment, to avoid bias due to the previ-
ous experience with on-screen or VR settings as much as possible,
all subjects will go through a short training procedure, to make
sure they understand how the Kibana-based dashboards, and the
VR immersive scenes work. This is necessary because we expect
many subjects to not have previous exposure to Kibana, nor to VR
immersion.

Upon completion of the training, a demographic form will be
used to control the subject confounding variables. Once the form is
completed, the experiment will begin by showing the subject data
from one of the two projects considered (selected randomly) in a
randomly chosen setting (VR or on-screen). Each subject will be
presented with a number of questions (half of the total number of
questions, selected randomly), in sequence, with each new question
being presented only after the previous one was answered. Then,
each subject will follow the same procedure in the other setting,
with data from the other project.

To end the experiment, the subject will answer a feedback form
which will be analyzed for possible improvements or problems that
the participant has encountered during the experiment.

Once the experiment has been carried out, we will analyze the

possible deviations that have occurred in its development

5 THREATS TO VALIDITY
5.1 Internal validity
Internal validity is related to uncontrolled factors that can influence
the effectiveness. In our case it pertains to:

• Subjects. We ensure that all participants have experience in
different relevant topics about software development projects
using a questionnaire, thus reducing the threat that they were
not competent enough. We will also ask for their experience
in VR and Kibana, so that we can later do the corresponding
analysis to mitigate the threat of the influence of previous
experience with them.

• Tasks. The choice of tasks (questions) could be biased in
favor of one of the settings. We mitigate this threat by devel-
oping scenes that are valid for both VR and on-screen, with
exactly the same tasks, so that the level of difficulty is as
similar as possible. We also will include tasks that put both
modes at a disadvantage: Tasks focused on precision could
be easier on-screen, while tasks focused on locality could be
easier in VR. Not controlled aspects (e.g., the relative size of
the buildings in VR) could have an influence on the results.
• Training. In both settings we will present a basic guide on
how to use the settings for performing the tasks, we will
offer texts about how the tool is used and how the interaction
with the elements works. We also will investigate whether

To VR or not to VR: Is virtual reality suitable to understand software development metrics?

ESEM ’21, October 11–15, 2021, Bari, Italy

a practical tutorial on how to interact with a VR headset
could reduce the experience gap between VR and on-screen,
improving the correctness of the VR tasks.

5.2 External validity
External validity relates to the generalizability of the results of the
experiment. In our case it pertains to:

• Sample Size. The number of participants in the experiment
can be relatively small. A larger sample would be needed
to have more conclusive results. In order to determine if
the results are statistically meaningful, we will run tests
depending on this sample size.

• Subjects. We will mitigate the threat of subject representa-
tiveness by categorizing them, including their professinal
position and the years of experience in the main software
development projects topics, obtaining a balanced mix of
academics and professionals.

• Target Systems. Another threat is represented by the choice
of the target systems. We will ensure that the two systems
presented will have similar characteristics in order to miti-
gate the threat of having unbalanced systems that can inter-
fere with the accuracy (correctness) and efficiency (comple-
tion time) of the responses.

• Experimenter Effect. The experimenters can be the au-
thors of the research, which may influence any subjective
aspect of the experiment. For example, task solutions cannot
be graded correctly. To mitigate this threat, another author
can carry out part of the experiments as a supervisor. Both
experimenters will build a model of the responses based on
previous experiments in the literature. Even if we will try
to mitigate this threat extensively, we cannot exclude all
possible influences on the results of the experiment.

• Time Measurement. To mitigate the threat that task com-
pletion times are not affected by external factors, subjects
will be required to answer via voice in both settings.

6 CONTRIBUTIONS AND IMPLICATIONS
There is little evidence about the convenience of using VR for visu-
alizing and interacting with data, and in particular a relative lack of
evidence comparing on-screen with VR immersion in software en-
gineering. As VR devices become increasingly commonplace, they
will be more used in software engineering, and it will be important
to know as much as possible about how they impact practitioners,
in comparison with traditional on-screen environments.

Our main contribution would be to offer the results of a con-
trolled experiment where 2D and VR are compared in terms of
accuracy and speed of the same, high-level software comprehen-
sion tasks. It will allow to identify affordances, applications, and
challenges of both approaches, but especially for the VR one as it
has been underinvestigated –and is thus less known– by the scien-
tific community. We will also provide practitioners –in particular,
but not limited to, the software analytics company that drives the
development of BabiaXR– with future directions, showing what
works and what requires further development. Finally, all software
used in the experiment is available under a free/open source li-
cense, making it possible for others (researchers and practitioners)

to inspect how we have addressed the problems, to reuse it at their
convenience or to adapt it to their needs.

We envision, given the technological maturity that VR is achiev-
ing, that VR is going to gain momentum in many areas. Software en-
gineering should not be an exception to this. Regardless of whether
the results obtained in our controlled experiment are positive or
negative, we foresee that other researchers can devote their efforts
to further advance this field of knowledge. We will use VR in our
experiment with code reviews and from a more software process
perspective. Future research could focus on other software develop-
ment activities (e.g., modeling, developing, testing, profiling) and
with other perspectives in mind (e.g., source code, diagrams).

ACKNOWLEDGMENTS
We acknowledge the financial support of the Spanish Government
for the projects IND2018/TIC-9669 and RTI-2018-101963-B-I00.

REFERENCES
[1] Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-

lenges of modern code review. In 2013 35th ICSE. IEEE, 712–721.

[2] N. Cliff. 1993. Dominance statistics: Ordinal analyses to answer ordinal questions.

Psychological Bulletin 114 (1993), 494–509.

[3] Santiago Dueñas, Valerio Cosentino, Gregorio Robles, and Jesus M Gonzalez-
Barahona. 2018. Perceval: Software project data at your will. In Proceedings of the
40th International Conference on Software Engineering: Companion Proceeedings.
1–4.

[4] Santiago Dueñas, Valerio Cosentino, Jesus M. Gonzalez-Barahona, Alvaro del
Castillo San Felix, Daniel Izquierdo-Cortazar, Luis Cañas-Díaz, and Alberto Pérez
García-Plaza. 2021. GrimoireLab: A toolset for software development analytics.
PeerJ Computer Science (2021). Accepted, publication pending.

[5] Anthony Elliott, Brian Peiris, and Chris Parnin. 2015. Virtual reality in software
engineering: Affordances, applications, and challenges. In 2015 IEEE/ACM 37th
IEEE International Conference on Software Engineering, Vol. 2. IEEE, 547–550.
[6] Dean Jackson and Jeff Gilbert. 2020. WebGL 2.0 Specification. Khronos Group
Specification. https://www.khronos.org/registry/webgl/specs/latest/2.0/
[7] Brandon Jones and Manish Goregaokar. 2020. WebXR Device API. W3C Working

Draft. https://www.w3.org/TR/webxr/

[8] Oleksii Kononenko, Tresa Rose, Olga Baysal, Michael Godfrey, Dennis Theisen,
and Bart De Water. 2018. Studying pull request merges: a case study of shopify’s
active merchant. In Proceedings of the 40th ICSE SEIP. 124–133.

[9] Chandra Maddila, Chetan Bansal, and Nachiappan Nagappan. 2019. Predicting
pull request completion time: a case study on large scale cloud services. In
Proceedings of the 2019 27th ESEC/FSE. 874–882.

[10] Kennedy Ndenga Malanga, Jean Mehat, Ganchev Ivaylo, and Franklin Wabwoba.
2015. Assessing quality of open source software based on community metrics.
(2015).

[11] Leonel Merino, Johannes Fuchs, Michael Blumenschein, Craig Anslow, Moham-
mad Ghafari, Oscar Nierstrasz, Michael Behrisch, and Daniel A. Keim. 2017. On
the Impact of the Medium in the Effectiveness of 3D Software Visualizations. In
2017 IEEE Working Conference on Software Visualization (VISSOFT). 11–21.
[12] Nadim Nachar. 2008. The Mann-Whitney U: A Test for Assessing Whether Two
Independent Samples Come from the Same Distribution. Tutorials in Quantitative
Methods for Psychology 4 (03 2008). https://doi.org/10.20982/tqmp.04.1.p013
[13] Paul Ralph. 2021. ACM SIGSOFT Empirical Standards Released. SIGSOFT Softw.
Eng. Notes 46, 1 (Feb. 2021), 19. https://doi.org/10.1145/3437479.3437483
[14] Simone Romano, Nicola Capece, Ugo Erra, Giuseppe Scanniello, and Michele
Lanza. 2019. On the use of virtual reality in software visualization: The case of
the city metaphor. Information and Software Technology 114 (2019), 92 – 106.
[15] Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern code review: a case study at google. In Proceedings of
the 40th International Conference on Software Engineering: Software Engineering
in Practice. 181–190.

[16] Patanamon Thongtanunam, Shane McIntosh, Ahmed E Hassan, and Hajimu
Iida. 2017. Review participation in modern code review. Empirical Software
Engineering 22, 2 (2017), 768–817.

[17] R. Wettel, M. Lanza, and R. Robbes. 2011. Software systems as cities: a controlled

experiment. In 2011 33rd ICSE. 551–560.

[18] Yue Yu, Huaimin Wang, Vladimir Filkov, Premkumar Devanbu, and Bogdan
Vasilescu. 2015. Wait for it: Determinants of pull request evaluation latency on
github. In 2015 IEEE/ACM 12th working conference on mining software repositories.
IEEE, 367–371.

