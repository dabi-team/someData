2
2
0
2

n
u
J

3
2

]

V
C
.
s
c
[

1
v
8
7
6
1
1
.
6
0
2
2
:
v
i
X
r
a

BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose
Estimation

Ivan Grishchenko Valentin Bazarevsky Andrei Zanﬁr

Eduard Gabriel Bazavan Mihai Zanﬁr

Richard Yee Karthik Raveendran Matsvei Zhdanovich Matthias Grundmann Cristian Sminchisescu
Google
1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA
{igrishchenko, valik, andreiz, egbazavan, mihaiz, yeer, krav, matvey, grundman, sminchisescu}@google.com

Abstract

We present BlazePose GHUM Holistic, a lightweight
neural network pipeline for 3D human body landmarks and
pose estimation, speciﬁcally tailored to real-time on-device
inference. BlazePose GHUM Holistic enables motion cap-
ture from a single RGB image including avatar control, ﬁt-
ness tracking and AR/VR effects. Our main contributions
include i) a novel method for 3D ground truth data acqui-
sition, ii) updated 3D body tracking with additional hand
landmarks and iii) full body pose estimation from a monoc-
ular image.

1. Introduction

Accurate real-time inference of the human body skeleton
enables a variety of applications ranging from motion cap-
ture to interactive video games. Running on a consumer’s
phone or laptop without relying on dedicated sensors (e.g.
IR on the Kinect [1]) would vastly democratize the technol-
ogy for non-professional users. Over the past decade, there
have been numerous advances in estimating 3D landmarks
on the human body [2] or volumetric representations [14].
The majority of these are either too computational expen-
sive to be run on mobile devices, require a specialized lab
setup (e.g. multi-camera) or lack sufﬁcient detail w.r.t. body
topology (e.g. no ﬁngers). To overcome these limitations,
we created BlazePose GHUM Holistic, a lightweight neu-
ral network pipeline that predicts 3D landmarks and pose
of the human body on-device, including hands, from a sin-
gle monocular image, runs in real-time at 15 FPS on most
modern mobile phones and browsers and is available to de-
velopers and creators via MediaPipe.

Our approach handles three issues that we identiﬁed with
current interactive motion capture solutions. First, we ad-
dress the challenge of acquiring diverse 3D ground truth
of the human body. We introduce a novel approach that is

Figure 1. Overview of our BlazePose GHUM Holistic pipeline.
From a single RGB input image, we predict 2D and 3D landmarks
for body and hands in one feedforward pass. Full 3D pose and
shape information is further inferred via a GHUM [16] lifter.

based on ﬁtting a statistical 3D human model GHUM [16]
to a diverse set of 2D annotations. To further improve ac-
curacy, we propose to use depth ordering annotations as su-
pervision during ﬁtting.

Second, the standard topology for on-device body land-
marks prediction does not include hands and ﬁngers, e.g. 33
landmarks by BlazePose [3] and PoseNet [8]. This impedes
the building of a uniﬁed motion capture system for the entire
body. To overcome this limitation, we use a spatial trans-
former [7] approach to crop high-res hand regions from the
original image seeded by BlazePose’s palm prediction as a
prior. Then we run a retrained hand tracking model [5, 12]
to predict 21 3D hand landmarks for each hand in a single
feed-forward pass.

Finally, we tackle the issue of moving beyond accu-
rate 3D representations of the human body towards high-
level semantic understanding and mapping to enable ex-
pressive use cases like 3D avatars. To this end, we present a
lightweight model that predicts the full body and hand pose
from 3D landmarks represented as joint rotations of a 3D
human model. We employ a statistical 3D human model
called GHUM [16] that additionally acts as a pose prior
constraining the state of predictions to plausible and real-
istic movements.

 
 
 
 
 
 
HUND [18], THUNDR [19]).

Due to the nature of 3D to 2D projection ﬁtting can result
in several realistic 3D body poses for the given 2D annota-
tion (i.e. with the same X and Y but different Z). To min-
imize this ambiguity we asked annotators to provide depth
order between pose skeleton edges where they are certain,
similarly to Ordinal Depth Supervision approach [11]. This
task proved to be easy showing high consistency between
annotators (98% on cross-validation) and helped to reduce
the depth ordering errors during ﬁtting from 25% to 3%.

Model BlazePose GHUM Holistic utilizes a two-step
detector-tracker approach where the tracker operates on a
cropped region-of-interest containing the human within the
original image. Thus the model is trained to predict 3D
body pose in relative coordinates of a metric space with ori-
gin in the subject’s hips center.

Experiments To evaluate the quality of our models
against other well-performing publicly available solutions,
we use yoga domain, as one of the most challenging in
body poses articulations. Each image contains only a single
person located 2-4 meters from the camera. To be consis-
tent with other solutions, we perform evaluation only for
17 keypoints from COCO topology. As shown in Tab. 1,
our approach outperforms that of leading commercial and
academic solutions. We train a variety of different mod-
els (Heavy, Full, Lite) that provide various levels of trade-
off between accuracy and on-device inference speed, see
Tab. 2.

Model

BlazePose Heavy
BlazePose Full
BlazePose Lite
AlphaPose ResNet50
Apple Vision

2D

3D
mAP mae (mm)
36
68.1
39
62.6
45.0
45
N/A
63.4
N/A
32.8

Table 1. Comparison for single person pose tracking methods.

Device

In-Browser1
Pixel 4 CPU 2
Pixel 4 GPU
Desktop 3

BlazePose BlazePose BlazePose
Full, ms Heavy, ms
29
147
22
10

Lite, ms
13
25
8
7

15
40
9
8

Table 2. Inference speed across various devices

Figure 2. Sample results of BlazePose GHUM Holistic. For an
input image our pipeline predicts real-time on-device 2D and 3D
landmarks for body and hands (ﬁrst and second columns) as well
as GHUM [16] pose parameters (third column). Our methods
work for challenging poses, various backgrounds and diverse ap-
pearances.

2. 3D body landmarks

The key challenge to build the 3D part of our pose model
is obtaining realistic, in-the-wild 3D data.
In contrast to
2D, which can be obtained via human annotation, accu-
rate manual 3D annotation is a uniquely challenging task.
It requires either a lab setup [6, 10], specialised hardware
with depth sensors for 3D scans [16] or building a synthetic
dataset [4, 21] which has an inherent domain gap to real
world data. Each of these approaches introduces additional
challenges to preserve a good level of human and environ-
ment diversity as present in real-world pictures.

3D data acquisition Our approach is based on a statisti-
cal 3D human body model GHUM [16], which is built using
a large corpus of human shapes and motions. To obtain 3D
human body pose ground truth, we ﬁt the GHUM model
to our existing 2D pose dataset, which covers various do-
mains (yoga/ﬁtness/dance), surroundings (indoor/outdoor),
devices (mobile/laptop). In doing so, we obtain real world
3D keypoint coordinates in metric space (see ﬁg. 3). Dur-
ing the ﬁtting process the shape and the pose variables of
GHUM were optimized such that the reconstructed model
aligns with the underlying image evidence. This includes
2D keypoint and silhouette semantic segmentation align-
ment as well as shape and pose regularization terms (check

Figure 3. Sample GHUM ﬁtting for an input image. Left: original image, middle: 3D GHUM reconstruction (different viewpoint), right:
blended result projected on top of the original image.

3. Hand landmarks for holistic human pose

Holistic human pose estimation requires accurate track-
ing of hands in addition to the body. To include hand land-
marks in the BlazePose topology, we had to solve two main
issues. First, the BlazePose model’s input resolution of
256x256 is insufﬁcient to capture hand details. Second, we
need to ensure that hands prediction is spatially invariant for
left and right hands to guarantee the same level of accuracy
as leading on-device hand tracking models [12]. Using a
single model would require us to a) increase the input reso-
lution which in turn would slow down model inference and
b) balance the loss for small and big body parts a single
pixel error is more signiﬁcant for ﬁngers, than for example
the hip landmark. Instead, we opted for an alternative ap-
proach that uses a separate hand prediction model [12]that
is inferred on high resolution crops obtained from ROIs pro-
vided by BlazePose (i.e. re-cropping).

Pipeline and re-crop model The four palm landmarks
produced by BlazePose give us a rough estimate of the
hand region. But this is insufﬁcient to use as input for the
subsequent hand landmark model [12] as it was trained on
more accurate crops obtained from all 21 hand landmarks
with only slight transformation augmentations. One solu-
tion would be to train with more aggressive transformation
augmentations but this signiﬁcantly reduces accuracy of the
model due to its limited capacity targeting real-time on-
device inference. To close this gap, we trained a re-crop
model that takes raw crop from BlazePose output and re-
ﬁnes it on a higher resolution to a level acceptable for the
subsequent hand landmark model. Using BlazePose as a
prior for hand locations also helps us to untangle hands and
body parts of different people.

1Google Chrome on MacBook Pro (15-inch 2017)
2Single Core via XNNPACK backend
3Intel i9-10900K. Nvidia GTX 1070 GPU

Experiments We compare the hand landmarks quality of
a standalone hand tracker [12] with our novel re-crop ap-
proach, see Tab. 3. Our pipeline produces a better hand
landmark quality. We postulate this is due to a more accu-
rate crop on the current frame, whereas the approach of [12]
uses a one frame delay.

Pipeline

Tracking pipeline (baseline)
Pipeline without re-crops
Pipeline with re-crops

2D

3D
mnae mae (mm))
20
9.8%
27
11.8%
18
9.7%

Table 3. Hand prediction quality.The mean error per hand (MEH)
is normalized by the hand size.

4. Body pose estimation

To enable expressive use cases like 3D avatars, we must
go beyond inferring 3D coordinates and obtain a high-level
semantic understanding via joint rotations that can be used
to drive rigged characters. To this end, we obtain the 3d
pose and shape GHUM mesh from the set of our previously
3d landmarks inferred directly from images.

Statistical human model. We use the recently introduced
statistical 3d human body model GHUM [16], to represent
the pose and shape of the human body. The model has been
trained end-to-end, in a deep learning framework, using a
large corpus of human shapes and motions. The model has
generative body shape and facial expressions β = (βb, βf )
represented using deep variational auto-encoders and gen-
erative pose θ = (θb, θlh, θrh) for the body, left and right
hands, respectively, represented as normalizing ﬂows [17].
The pelvis translation and rotation are controlled separately,
and represented by a 6d rotation representation [20] r ∈
R6×1 and a translation vector t ∈ R3×1 w.r.t the origin
(0, 0, 0). The mesh consists of of Nv = 10,168 vertices

and Nt = 20,332 triangles. To pose the mesh, we apply
the GHUM network V(θb, βb, r, t) ∈ RNv×3 to obtain the
posed vertices. We omit the facial expressions, as we here
focus on main body, hand poses and shape. We also drop
the b subscript for convenience.

Lifter. The BlazePose GHUM Holistic network outputs
33 body landmarks, and 21 landmarks for each hand, in
a root-centered 3D camera coordinate system. In order to
increase the expressivity of the outputs, without sacriﬁng
performance, we propose a sample-and-train methodology,
based on a novel GHUM Lifter neural network. Our neural
network takes as input the concatenated 3D body and hands
landmarks and outputs GHUM mesh parameters.

MLPMixer. At the core of our lifter lies an MLP-Mixer
architecture [13]. Our mixer takes as input a sequence of S
3D keypoints, each one projected to a desired hidden di-
mension C, with the same projection matrix. The mixer
consists of multiple layers of identical size, and each layer
consists of two MLP blocks. The ﬁrst one is the token-
mixing MLP: it acts on columns of the mixer input and is
shared across all columns. The second one is the channel-
mixing MLP: it acts on rows of the token-mixing MLP and
is shared across all rows. Each MLP block contains two
fully-connected layers and a nonlinearity applied indepen-
dently to each row of its input data tensor.

Full model training. We use an MLPMixer architecture
which takes as input S = 75 tokens (i.e. 2 x 21 hand
3D landmarks and 33 body 3D landmarks) and transforms
them to produce GHUM state parameters r, t, β, θ. Given
generative codes for pose and shape θ, β ∈ N (0; I), r
drawn from the Haar distribution on SO(3), and t uni-
formly sampled from a (−0.1 . . . −0.1) × (−0.1 . . . 0.1) ×
(−0.1 . . . −0.1) meters box, we produce a posed GHUM
sample mesh V(θ, β, r, t). The associated S 3D landmarks
can be retrieved by a simple (ﬁxed) linear regression ma-
trix W ∈ RNv×S, such that X = WV(θ, β, r, t). In our
experiments, we noticed that injecting noise at this point,
i.e. X + N (0; (cid:15)I), supports the more accurate retrieval of
the full mesh given real. We also experimented with Trans-
former [15] or simple MLP architectures. The latter failed
to converge (which was also experienced by [19]), while the
former had a similar performance, but required more mem-
ory and it was harder to deploy on mobile devices.

Experiments.
In order to validate our GHUM lifter archi-
tecture we ran experiments on a held out test set of 10,000
in the wild images with very challenging poses (yoga, ﬁt-
ness, dancing) containing GHUM ﬁts which were curated
for any errors. We compared with various SOTA methods

for 3D pose and shape estimation and observe signiﬁcant
improvements for our method even though it runs one order
of magnitude faster. Results are reported in Tab. 4.

Method
SPIN [9]
HUND [18] (WS)
THUNDR [19] (WS)
BlazePose GHUM Holistic

MPJPE-PA MPJPE
139.5
156
138
121

101
106.6
97.5
78

Table 4. Mean per joint positional error with Procrustes alignment
(MPJPE-PA) and without Procrustes alignment (MPJPE) on a held
out test set containing in the wild images with very complex poses
(yoga, ﬁtness, dancing

5. Applications and conclusion

Figure 4. Open-sourced avatar demo in MediaPipe.

BlazePose GHUM Holistic provides a simple and mean-
ingful representation of the human body that can be used for
multiple applications right out of the box. 3D landmarks en-
able us to move beyond 2D space to a real world coordinate
system. Pose estimation gives us high level interpretation
of 3D landmarks as well as extra hand points to add ﬁne
grain details when needed (e.g. gesture detection).
It en-
ables motion capture for avatar control, repetition counting
and posture correction for ﬁtness and sports, as well as 3D
effects for AR/VR.

To showcase BlazePose GHUM Holistic we created
an open-source avatar demo (see Fig. 4) in MediaPipe
(https://mediapipe.dev). The demo is available
for web browsers and allows to control body and hands of a
standard Mixamo avatar at 15 FPS (MacBook Pro 15-inch
2017).

In the future, we plan on further improving the model
that predicts 3D landmarks as well as capturing reliable fa-
cial expressions to add them to BlazePose GHUM Holistic.

[17] Andrei Zanﬁr, Eduard Gabriel Bazavan, Hongyi Xu, Bill
Freeman, Rahul Sukthankar, and Cristian Sminchisescu.
Weakly supervised 3d human pose and shape reconstruction
with normalizing ﬂows. ECCV, 2020. 3

[18] Andrei Zanﬁr, Eduard Gabriel Bazavan, Mihai Zanﬁr,
William T Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Neural descent for visual 3d human pose and
shape. 2020. 2, 4

[19] Mihai Zanﬁr, Andrei Zanﬁr, Eduard Gabriel Bazavan,
William T. Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Thundr: Transformer-based 3d human reconstruc-
tion with markers. In ICCV, 2021. 2, 4

[20] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. 2018. 3

[21] Tyler Zhu, Per Karlsson, and Christoph Bregler. Simpose:
Effectively learning densepose and surface normals of peo-
ple from simulated data, 2020. 2

References

[1] Azure kinect body tracking joints.

https://docs.microsoft.com/en- us/azure/
kinect-dk/body-joints. 1

[2] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic
3d human reconstruction in-the-wild. In CVPR, 2019. 1
[3] Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveen-
dran, Tyler Zhu, Fan Zhang, and Matthias Grundmann.
Blazepose: On-device real-time body pose tracking, 2020.
1

[4] Eduard Gabriel Bazavan, Andrei Zanﬁr, Mihai Zanﬁr,
William T. Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Hspace: Synthetic parametric humans animated in
complex environments, 2021. 2

[5] Andrey Vakunov Fan Zhang, Valentin Bazarevsky, George
Sung, Chuo-Ling Chang, Matthias Grundmann, and Andrei
Tkachenka. Mediapipe hands: On-device real-time hand
tracking. In CVPR, 2020. 1

[6] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3.6m: Large scale datasets and predic-
tive methods for 3d human sensing in natural environments.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 36(7):1325–1339, jul 2014. 2

[7] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
Koray Kavukcuoglu. Spatial transformer networks, 2015. 1
[8] Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Posenet: A convolutional network for real-time 6-dof camera
relocalization, 2015. 1

[9] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-ﬁtting in the loop. In ICCV, 2019. 4

[10] Ruilong Li, Shan Yang, David A. Ross, and Angjoo
Kanazawa. Learn to dance with aist++: Music conditioned
3d dance generation, 2021. 2

[11] Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
Ordinal depth supervision for 3D human pose estimation. In
Computer Vision and Pattern Recognition (CVPR), 2018. 2
[12] George Sung, Kanstantsin Sokal, Esha Uboweja, Valentin
Bazarevsky, Jonathan Baccash, Eduard Gabriel Bazavan,
Chuo-Ling Chang, and Matthias Grundmann. On-device
real-time hand gesture recognition. 2021. 1, 3

[13] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-
cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,
Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.
Mlp-mixer: An all-mlp architecture for vision. 2021. 4
[14] G¨ul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin
Yumer, Ivan Laptev, and Cordelia Schmid. Bodynet: Volu-
metric inference of 3d human body shapes, 2018. 1

[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 4

[16] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanﬁr, Bill
Freeman, Rahul Sukthankar, and Cristian Sminchisescu.
GHUM & GHUML: Generative 3D human shape and artic-
ulated pose models. CVPR, 2020. 1, 2, 3

