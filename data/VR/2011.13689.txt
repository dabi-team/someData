This work has been submitted to the IEEE for possible
publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

0
2
0
2

v
o
N
7
2

]
I

A
.
s
c
[

1
v
9
8
6
3
1
.
1
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
Automated acquisition of structured, semantic models of manipulation
activities from human VR demonstration

Andrei Haidu and Michael Beetz
{haidu, beetz}@uni-bremen.de

Abstract— In this paper we present a system capable of
collecting and annotating, human performed, robot under-
standable, everyday activities from virtual environments. The
human movements are mapped in the simulated world using
off-the-shelf virtual reality devices with full body, and eye
tracking capabilities. All the interactions in the virtual world
are physically simulated, thus movements and their effects are
closely relatable to the real world. During the activity execu-
tion, a subsymbolic data logger is recording the environment
and the human gaze on a per-frame basis, enabling ofﬂine
scene reproduction and replays. Coupled with the physics
engine, online monitors (symbolic data loggers) are parsing
(using various grammars) and recording events, actions, and
their effects in the simulated world.

I. INTRODUCTION

We are currently seeing fast progress of robotic agents
learning manipulation tasks [1],
in particular through
imitation [2] and reinforcement learning [3]. However, in
many cases the amount of experience needed for learning
exceeds what can provided with reasonable effort [4], in
particular if a single experience corresponds to a complete
manipulation episode. One way of reducing the number of
experiences needed for learning is to increase the informa-
tion content of an episode. This can be achieved by trans-
forming experiences into appropriately structured activity
models and interpreting episodes by applying structural
and teleological, as well as causal and intuitive physics
knowledge to them. While several approaches are starting
to look towards this direction [5] we believe that there
is much higher potential in such activity representations
as human activities are highly structured and have many
regularities [6], [7] that have been investigated thoroughly
in an interdisciplinary research ﬁeld called action science
[8].

In this paper we advance the acquisition of

learn-
ing data from human virtual reality demonstrations by
building on top of our previous work: AMEvA (Auto-
mated Models of Everyday Activities) [9], [10] – a special-
purpose knowledge acquisition, interpretation and pro-
cessing framework, capable of recording and detecting
force-dynamic states of manipulation activities performed
in virtual environments. The main limitations of
the
previous system were the coarser representations of the

The authors are with the Institute for Artiﬁcial Intelligence, University

of Bremen, Germany.

This work was partially funded by Deutsche Forschungsgemeinschaft
(DFG) through the Collaborative Research Center 1320, EASE, and by the
EU H2020 project REFILLS (Project ID: 731590).

Fig. 1. Pick-and-place motion patterns with gaze visualization

human models and their actions. The human model was
consisting of only the head and hands, tracked by the
VR headset and the two controllers. This approximation
implies that we can only get trajectory data of the hands
and the object manipulated by the hand but not full-
pose data of the human body. Also, the model allows the
human to reach locations that are impossible to reach
in reality because other body parts would collide with
the environment, or cannot use various body parts to
manipulate the environment, for example closing a drawer
or a door using an elbow or a foot.

As a result, the contributions of this paper are: (a)
the improved human model representation; (b) a more
realistic interaction with the virtual world; and (c) the
granularity of the action and event recognition/segmen-
tation; For (a), we integrated gaze and full body tracking
systems to provide us in depth access to the human
motions and intentions. The virtual human body obtained
an in-depth semantic representation for its skeletal body
parts using KNOWROB [11], thus commonly shared with
the robot. This was done by extending the upper ontology
with classes, properties and relations of the virtual body
parts. In (b) the primary interaction advancement is a fully
physics-based grasping model, where grasping occurs as a
result of opposing forces and friction applied to an object
through the hand parts. The integrated full body allows,
as previously mentioned, for more natural and complex

Fig. 2.

System overview

interactions, such as closing doors/drawers using other
body parts than the hands (elbows, foot). For (c), the
main contribution is the implementation of more granular
online monitors capable of detecting and segmenting
generic, physics-enabled interactions. We extended the
Flanagan model [12] for pick-and-place being able to
robustly detect reach-to-grasp (prehension) movements
such as reaching, ﬁxation, grasping, sliding, picking-up,
transporting, putting-down.

Figure 1 partially illustrates the aforementioned contri-
butions by depicting a segmented pick-and-place action
into meaningful motion patterns (top) and the gaze of
the human during the same execution (bottom). The
scene is reproduced at the start of the reaching motion
and annotated with colored markers (hand trajectory and
human pose) for each following motion phase. Red for
reaching, yellow for ﬁxation, green for picking-up, teal for
transporting and blue for the putting-down motion.

The remainder of the paper is organized as follows:
Section II gives an overview of the framework, describes
the symbolic virtual environment and the structure of the
episodic memories connected with the robot’s knowledge
base. Section III presents the interaction of the human
with the virtual environment. Section IV introduces the
online activity recognition modules and their heuristics.
Section V introduces example queries for accessing the
experiments data from the hybrid knowledge base. We in-
troduce various related works in section VI, and conclude
in section VII.

II. OVERVIEW

Figure 2 depicts the overview of the system, where a
user is asked to perform a task in a virtual environment.
The environment is connected to an extended KNOWROB
knowledge base, which includes the representation of
all the entities and their properties in the virtual world
(humans, objects,
lights, articulations, partonomies), as
well as the actions and events which can occur during
the task execution. A subsymbolic, and symbolic, logger
is running in the background, recording on a per frame
basis every change in the environment (including human
gaze), where the latter is a collection of modules capable
of detecting and recording actions and events. These are
then indexed and stored in a hybrid knowledge base in

the form of episodic memories.

A. Virtual Environment

The data structure used in the virtual world can be
split into two categories: “materialized” and “abstract”
entities. These can further be categorized as relevant for
the rendering engine, the physics engine, for both, or
for none. The “materialized” entities are the ones that
can be drawn by the rendering engine and simulated
by the physics engine: the human body (skeletal body),
the objects in the environment (rigid bodies) or physics-
based particles such as liquids or soft bodies. “Abstract”
entities, such as lights or visual effects particles (sparks,
ﬂames etc.) are only relevant for the rendering engine,
whereas joint/spring entities are only relevant for the
physics engine. Custom abstract entities such as gaze
pose are irrelevant for both, since they are not simulated
nor rendered. Depending on their types, the entities have
various properties, such as collisions, mass, friction pa-
rameters, textures, etc. for “materialized” entities. Emitted
light color, intensity, shape, etc. for lights. Motion lim-
its (prismatic/linear), damping, stiffness coefﬁcients, etc.
for joints. Another important property is the partonomy
relationships between the entities, for example that the
drawer handle is part of the drawer. For each entity and
property there is a corresponding representation in the
extended ontology of KNOWROB.

B. Episodic Memories

The right side of Figure 2 depicts the structure of the
logged datasets. It is split into a symbolic and subsymbolic
representation. The symbolic part is stored in KNOWROB
and represented using OWL [13], whereas for the subsym-
bolic part we are using MongoDB databases. Examining
the dataset from top to bottom, we ﬁrst have a number
of tasks stored using a KNOWROB instance bundled with a
MongoDB server. A task represents a virtual environment
scenario where users are given a speciﬁc assignment.
Tasks are concepts in the ontology, where each new task
is represented as a unique instance in the knowledge
base and a separate database in MongoDB. Such a task
example would be to: "set the table for dinner", "clean
the table after breakfast", or "put the dirty dishes in
the dishwasher". Each task component is split into two

parts: the metadata, which stores the data at the task
level, and the episodes collection, which are the instances
of every executed tasks. The concept of “episode” is
also represented in the ontology, therefore every episode
instance will receive a unique identiﬁer at instantiation,
thus allowing KNOWROB to classify each occurred event
per episode and to query the knowledge base as a whole.
The symbolic section of the metadata represents the
instantiation of every entity class from the virtual world,
which entails: generating unique identiﬁers using UUID’s
for each entity, thus guaranteeing uniqueness of all the
instances across any previous or future tasks; and collect-
ing all properties of the entities. The subsymbolic section
stores all the binary assets required for the virtual envi-
ronment to re-build the task scenario: meshes, materials,
textures, shaders, lighting builds of the map, etc. These are
stored using MongoDB’s large ﬁles management system
gridfs.

At the symbolic level each episode stores all the events,
actions and their effects that occurred during the task
execution, these are stored into per episode separate OWL
ﬁles. At the subsymbolic level, for each episode a new
MongoDB collection is created, storing the world
states on a per-frame basis and the gaze information.
Since the database is no longer altered after the episode
completion, this allows us to optimize the database for fast
querying using multiple indexes. Every relevant ﬁeld
will be indexed and internally organized as a B-tree
data structure, thus reducing the search time complexity
to O (log n).

III. INTERACTION

In this section we present how the mapping of the hu-
man movements onto the virtual avatar is implemented.
In order to interact with the environment we are using
three types of interactions. First, we have the full body
tracking data applied to the virtual human body as an
animation. Second, a more reﬁned controller is used for
the hands movement, allowing for a more precise ﬁne-
tuning to approximate real world conditions. Finally, we
have a physics-based grasp system capable of switching
between various predeﬁned grasp styles. With the three
interactions styles the user gains full control over its
virtual movements, giving him/her the freedom to move
in the open world as naturally as possible.

A. Full Body Tracking

The full body tracking system is currently a live ani-
mation, meaning it will map every received bone position
from the tracking software to the avatar, neglecting the
environment. Basically, it can apply forces to the envi-
ronment, but it cannot receive any feedback. Thus,
if
willing, the user could cause instabilities in the simulated
world by walking through walls or furniture. Although
the limitations, the user can still take advantage of the
additional limbs to interact with the world, and will force
him/her to use natural movements when doing so.

B. Hand Control

In order to have a more close-to-reality interaction,
we needed to use a different approach for the parts
predominantly used to interact with the world, namely the
hands. A direct mapping would have made the interaction
most reactive, it however would also cause the user to
interact with objects as they would have been weightless.
We therefore implemented an approach to allow a two-
way interaction with the world. When lifting a heavy
object, the object would pull back on the hands, or slip
out. This would compel the user to use strategies to
interact with the environment, as it would in the real
world, such as using two hands for stabilizing objects,
or for lifting heavier ones. For the implementation we
used two 3D PID controllers, one for the translational
motion and one for the rotational one. The controllers
are in a closed loop with the tracking system, from which
they receive the 6D target pose and then apply forces
and torques to the hand to try to move it to the desired
pose. The controller parameters gives us the possibility to
calibrate the hands in order to match close to real world
values in terms of responsiveness and strength.

C. Physics-based Grasping

In order to have plausible action effects during the
task execution we implemented a physics-based grasping
model on top of the simulated hands. We have chosen
the most common grasp styles for everyday object manip-
ulation and classiﬁed them according to Feix et al. [14].
The grasp styles were created using standard 3d animation
tools, where each grasp consists of multiple frames storing
the joint state information of the skeletal hand. The left
part of Figure 3 shows an example of three grasp styles
(top to bottom: pinch, wrap and tripod) and some of their
frames as waypoints. These animations are then stored in
into “physics grasp animation” datasets which are loaded
by the grasp controller. The controller will therefore react
to two user inputs: a discrete one, which changes the grasp
style; and an analog one, which interpolates the button
mapping between the grasp animation frames, resulting
in the target angle for the joints.

Fig. 3. Physics-based grasp animations (left) and contact sensors (right)

To control the bone movements, each joint is equipped
with a driver, which takes as input the interpolated value
from the grasp animation. Torques are then applied to

move and maintain the joint at the given angle. The
drivers operate similar to a PD controller using two
inputs: stiffness and damping. Where stiffness controls the
strength of the drive towards the target joint angle, and
damping towards the target velocity. In other words the
strength of executing the grasp and the strength of main-
taining the grasp pose from any external perturbation.

Fig. 4. Naïve physics reasoning capabilities

In Figure 4 we showcase the real world plausibility
of the physics-based interaction systems. We depict a
timeframe of picking up a ﬁlled plate using a lateral
grasp style with one or two hands. We notice on the top
timeframe during the pick-up motion due to its weight the
plate twists out of the grasp. However, using two hands,
the weight is distributed among two points, thus greatly
reducing the twist force applied by the plate, resulting in
a successful pick-up. Such events can provide robots with
naïve physics understanding of their actions. Having the
grasp styles represented as concepts in the ontology, it
will also provide the robot with a mapping between grasp
styles and objects, or actions.

IV. ONLINE ACTIVITY PARSER

In the following section we present the online activity
and event detection modules running in realtime during
the task execution and symbolically record the various
events occurring in the virtual world.

A. Grasp Detection

As described in the previous subsection III-C the cur-
rent grasping model
is physics-based, therefore, even
though we have the user’s intention to grasp something,
the actual success of the action depends on the sur-
rounding environment. In order to differentiate between
a contact with the hand while grasping and an actual
grasp event, we equip the virtual hands with arrays of
contact sensors (see right side of Figure 4) to detect if the
user has something in the hand while executing the grasp
animation. The contact arrays are grouped into different
sets, in the example image we have green for the ﬁnger
bones (phalanges), red for the thumb, and blue for the
palm (carpal bones). The grasp detector module is only
activated when the user starts the grasp animation. The
module is then keeping track of all the objects in contact

with the sensors in the sets. A grasp is detected when
an object is in contact with sensors from at least two
different sets. Having the sensors covering most of the
hand area, it can can also detect grasping multiple objects
simultaneously.

The module will not differentiate between the grasping
contexts, for example, grasping an object to be trans-
ported, a handle to open a drawer, or grasping an un-
movable object such as the furniture. It will regardlessly
semantically annotate, and record the occurred grasping
event containing: the start and end time of the event,
the hand and the grasped object instances,
including
the grasp style used for performing the action. Using
KNOWROB these events can further be particularized,
using ﬁrst order logic rules to infer grasps related events
such as “grasping onto” or “holding onto” if during the
grasping event the “acted on” instance has a mass larger
than the strength of what the grasping hand can handle.
The grasp detection system is susceptible for different
grammars for detecting grasps, it can handle more than
three sets of sensors array if required. For examples if one
would like to detect lateral ﬁnger grasps (these however
are very uncommon in a kitchen environment), one could
add for each ﬁnger a different set of sensor array.

B. Contact and Supported-by

One of the building blocks for detecting force-dynamic
events are the contact and supported-by events [15]. In our
framework the two are bundled together in one module
which is equipped on every object in the world. Tightly
coupled with the physics engine, the module will sub-
scribes for any collisions occurring on the object. When
a collision is triggered, it is interpreted into a semantic
contact event, and it starts checking for supported-by
event until the contact ﬁnishes. The grammar for checking
for the supported-by event requires that the two objects
in contact have a relative vertical velocity of close to zero.
This way if an object is being transported on a tray will
still be detected as supported-by the tray.

Being tightly coupled with the physics engine,

the
modules are only active when the objects are simulated.
For optimization reasons the physics engine disables all
objects that are in a stable state, when this happens, the
modules are also deactivated. This way the system can
scale to support thousands of objects without effecting
the real-time performance. Once an object is being simu-
lated again, the corresponding module is re-activated and
continues listening for events.

In the left side of Figure 5 we show an example of
detecting contact and supported-by events between two
objects and a supporting surface. The yellow ellipsoid
areas represent contact events, whereas the green ones
supported-by.

C. Extended Pick-and-Place Detection

Building upon the aforementioned modules, we now
introduce the grammar used for detecting and segmenting

Fig. 5. Key frames of the activity parsing grammar

a pick-and-place action into relevant motion patterns.
Figure 5 depicts the most signiﬁcant stages used by the
grammar to detect the reach-to-grasp action (top) and
transport with placing (bottom).

For every pick-and-place action we ﬁrst start detecting
the reaching motion, which is an important motion pat-
tern for any robot control system executing manipulations
tasks. In our model, reaching represents: the motion of the
user’s hand towards an object which will eventually be
grasped. The reaching motion is ﬁnished when the hand
(contact sensors) are in contact with the to-be-grasped
object. The starting point for reaching is hard to detect
because there are no distinct features for such motions.
Our current solution is to approximate this with a region
of interest in front of the hand. In image (a) we can see
the depiction of a reaching motion. We have in the scene
three objects which are supported by a surface. A gripper,
representing the human hand, and the region of interest
in front of the hand. Whenever an object overlaps this
region of interest, it will be marked as a potential object-
to-be-reached, the occurrence timestamp will be saved,
and its relative distance to the hand will be tracked. In
the depicted image the three objects are all overlapping
the region of interest of the hand, where the blue stars
represent the moments when the objects are marked as a
reach-to candidate.

In the next image (b) we showcase two main happen-
ings: ﬁrstly, (•) the start of a potential ﬁxation action,
directly causing the end of the related reaching motion
(still potential). Fixation represents in our system: the
movement of the hand –while in contact with the to-
be-grasped object– to the desired grasp pose, however,
not every reaching motion needs to be preceded by a
ﬁxation. In the image the contact between the hand and
the potential object is shown by the small yellow ellipsoid
area. Secondly, (•) we drawn the trajectory of the hand
from its previous pose until its current one. The trajectory
is split into two colors: a green one, representing the
reaching trajectory, and a gray one which would have been
the theoretical starting point for a reaching motion, if the
previously mentioned rule would not had been broken,

namely: "the motion of the user’s hand towards an object".
At one point the hand was moving away from the objects,
thus the start-to reach time of the tracked objects was
being constantly reset until the hand started moving back
towards the targets again. In the image we can notice the
apple is no longer in the region of interest, however the
milk carton is is. The model is still keeping track of the
objects in the region since it cannot know the intention
of the user, it might not grasp the object.

In image (c) the user grasped the object (grasp contact
points shown in red). During the grasp the region of
interest in no longer required, and the cached potential
objects are removed. The hand trajectory is extended
(shown in blue), representing the hand ﬁxation motion
pattern. After grasping the object there are two possible
motions that can follow: (d) slide or (e) pick-up. We
deﬁne a sliding motion: a voluntary motion of a grasped
object on its supporting surface. The sliding motion in
the example frame ended when the object contact with
its supporting plane ended. This could mean that the
object was either picked-up or directly transported. In the
example image, the object goes directly in the transport
state. The sliding trajectory is depicted in purple. The
other possible case is a pick-up motion, deﬁned as: lifting
a grasped object off its supporting surface. Similarly as
in the reach motion, pick-up does not have any distinct
features to segment it. We know it starts when the contact
with the supporting surface is broken, but cannot tell
when it ends. For example, when picking up an object off
the ﬂoor and placing it on the top cupboard, it is hard to
tell when the pick-up motion becomes a transport motion.
For this we wen with a similar approach to reaching, when
the contact with the supporting surface is broken, a region
of interest is created, and when the grasped object leaves
this area, the pick-up motion is marked as ﬁnished. In the
image the trajectory is depicted in yellow, and the leaving
of the region of interest with a purple star.

The following motion is transport, and it is depicted in
ﬁgure (f) and (h). This is deﬁned in our grammar as: the
motion of a grasped object after a slide or pick-up, until
either released, or a put-down or a sliding motion occurs.

In (g) we can observe a the transport motion followed
by directly by sliding. This happens if the grasped object
is not approaching the supporting surface from above.
If the object is approaching the supporting surface from
above, a put-down motion will occur. We know this ends
when the object is in contact with the supporting surface,
however, we do not know when did it start. For this reason
during the transport motions we cache the movements of
the grasped object with a decay time, we illustrated this
in the images using a green transparent line beneath the
hand trajectory. The approach now becomes, whenever a
put-down motion happens after the transport, a region of
interest area is created and we backtrack the motions of
the grasped object until it stops overlapping this area, we
then mark this as a start point of the put-down action.
We depicted this situation in image (i). Similarly as in
the the reaching case, if during the put-down motion, the
object is moving away from the supporting surface, the
put-down motion is cut short, starting only when there
was a continuous approaching motion towards the target.
This case is depicted in image (j).

V. EXPERIMENTS

In the following section we brieﬂy present a few short-
ened KNOWROB [11] query examples on how speciﬁc data
can be accessed and visualized from the hybrid knowledge
base, the results are visualized in Figure 6.

Fig. 6. Query results

In the ﬁrst example we ask for directly detected events,
without having any speciﬁc reasoning. The query boils
down to searching for a given action type acted on a given
object type. After a result is found, the action timestamps
and the object instance are used to reconstruct the world
at the given time and append the queried markers to it.
The results can be seen in the top left part of the ﬁgure.

?− e n ti ty ( Act ,

[ an , action ,

[ type ,

’ S l i d i n g ’ ] ,

[name, Obj ] ,

[ type ,

’ CerealBox ’ ] ] ] ] ) ,

[ S t a r t , End ] ) ,

[ an , object ,

[ object ,
occurs ( Act ,
show( world ( S t a r t ) ) ,
show( t r a j e c t o r y ( Obj ) ,
show( marker ( Obj ) , S t a r t ) .

[ S t a r t , End ] ) ,

In the following query we take advantage of the parton-
omy representation of the world in order to narrow down
our search for speciﬁc objects. In this case we are querying
for grasping handles belonging for speciﬁc classes. We can
se the results visualized in the top right side of the image.

?− e n ti t y ( Act ,

[ an , action ,

[ type ,

’ Grasping ’ ] ,

[name, Obj ] ,

[ type ,

’ Handle ’ ] ] ] ] ) ,

[ type ,

’ FridgeDoor ’ ] ] ) ,

[ an , object ,

[ an , object ,

[ object ,
e n ti t y ( Door ,
holds ( part_of ( Obj , Door ) ) ,
occurs ( Act ,
[ S t a r t , End ] ) ,
show( world ( S t a r t ) ) ,
show( t r a j e c t o r y ( Obj ) ,
show( marker ( Obj ) , S t a r t ) .

[ S t a r t , End ] ) ,

In the ﬁnal query we are using sub-actions to deﬁne an
upper action type. In our case a pick-and-place action,
which is not directly detected by the system, only their
subparts. Rules are deﬁned on which sub-actions belong
to the type, and their order can be speciﬁed using tem-
poral reasoning [16].

?− e n ti t y ( Act ,

[ an , action ,

[ type ,

’ PickAndPlace ’ ] ,

[ an , object ,

[name, Obj ] ,

[ type ,

’ CerealBox ’ ] ] ] ,

[ an , action ,
[ an , action ,

[ type ,
[ type ,

’ Reaching ’ ] ,
’ Grasping ’ ] ,

. .
. .

] ] ,
] ] ,

[ object ,
[ sub−action ,
[ sub−action ,
[ . . ] ,
[ sub−action ,

[ an , action ,
[ S t a r t , End ] ) ,

occurs ( Act ,
show( world ( S t a r t ) ) ,
show( t r a j e c t o r y ( Obj ) ,

[ S t a r t , End ] ) ,

[ . . ] .

[ type ,

’ S l i d i n g ’ ] ,

. .

] ] ) ,

VI. RELATED WORK

Similar to our approach, in [17] the authors present a
similar system with the scope of collecting automatically
annotated synthetic data form virtual environments, fo-
cusing mainly on visual realism. In comparison the human
interaction in our system is physics based, is capable of
automatically detecting and annotating actions and events
in a robot understandable manner.

In [18] the authors use a dataset of human activities
executed in a virtual reality environment to generate
hypotheses about casual dependencies between actions. A
consolidation of the two system would allow the possibil-
ity to collect and access labeled human activities without
the need of manual annotation.

VII. CONCLUSION AND FUTURE WORK

In this work we presented a framework able to automat-
ically collect fully annotated motion data from virtual en-
vironments. The data is stored in a hybrid knowledge base,
combining symbolic and subsymbolic representation, un-
derstandable and accessible by robots. For the virtual
environment representation we are using Unreal Engine
4 with its underlying physics engine NVIDIA PhysX.

As future research, we are planning to implement
various improvements to the system, such as avoiding
one directional physics on the animated human body,
integrating a reﬁned physics interaction similar to the
hand controllers. As a ﬁrst step would be to use blended
animations, which has the capability to follow the tracked
body animation and to react physical interference. We
would also like to use human gaze patterns in order to
reﬁne the action detection heuristics, especially in cases
where there are no other cues to use. Another objective is
to collect a large scale dataset similar to epic kitchen [19]
and have it available online for robots and researchers via
the OPENEASE[20] platform.

REFERENCES

[1] S. Amarjyoti, “Deep reinforcement learning for robotic manipula-

tion - the state of the art,” CoRR, vol. abs/1701.08878, 2017.

[2] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, Springer Hand-
Springer, 2008, ch. 59. Robot programming by

book of Robotics.
demonstration.

[3] J. Kober, J. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, pp. 1238–1274, 09 2013.

[4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning
hand-eye coordination for robotic grasping with deep learning and
large-scale data collection,” The International Journal of Robotics
Research, vol. 37, no. 4-5, pp. 421–436, 2018.

[5] K. Ramirez-Amaro, M. Beetz, and G. Cheng, “Transferring skills
to humanoid robots by extracting semantic representations from
observations of human activities,” Artiﬁcial Intelligence, vol. 247,
pp. 95 – 118, 2017, special Issue on AI and Robotics.

[6] J. R. Flanagan, M. C. Bowman, and R. S. Johansson, “Control
strategies in object manipulation tasks,” Current Opinion in Neu-
robiology, 2006.

[7] M. F. Land, “Eye movements and the control of actions in everyday
life,” Progress in Retinal and Eye Research, vol. 25, no. 3, pp. 296 –
324, 2006.

[8] A. Herwig, M. Beisert, and W. Prinz, Action science: Foundations of

an emerging discipline. MIT Press, 2013.

[9] A. Haidu and M. Beetz, “Automated models of human everyday ac-
tivity based on game and virtual reality technology,” in International
Conference on Robotics and Automation (ICRA), Montreal, Canada,
2019.

[10] A. Haidu, D. Beßler, A. K. Bozcuoglu, and M. Beetz, “Knowrob-
sim — game engine-enabled knowledge processing for cognition-
enabled robot control,” in International Conference on Intelligent
Robots and Systems (IROS). Madrid, Spain: IEEE, 2018.

[11] M. Beetz, D. Beßler, A. Haidu, M. Pomarlan, A. K. Bozcuoglu, and
G. Bartels, “Knowrob 2.0 – a 2nd generation knowledge processing
framework for cognition-enabled robotic agents,” in International

Conference on Robotics and Automation (ICRA), Brisbane, Australia,
2018.

[12] J. R. Flanagan, M. C. Bowman, and R. S. Johansson, “Control
strategies in object manipulation tasks,” Curr. Opin. Neurobiol.,
vol. 16, no. 6, pp. 650–659, Dec 2006.

[13] I. Horrocks, P. F. Patel-Schneider, and F. V. Harmelen, “From shiq
and rdf to owl: The making of a web ontology language,” Journal
of Web Semantics, vol. 1, p. 2003, 2003.

[14] T. Feix, J. Romero, H.-B. Schmiedmayer, A. Dollar, and D. Kragic,
“The grasp taxonomy of human grasp types,” Human-Machine
Systems, IEEE Transactions on, vol. 46, no. 1, pp. 66–77, 2016.
[15] A. Fern, J. M. Siskind, and R. Givan, “Learning temporal, relational,
force-dynamic event deﬁnitions from video,” in Eighteenth National
Conference on Artiﬁcial Intelligence. USA: American Association for
Artiﬁcial Intelligence, 2002, p. 159–166.
[16] J. F. Allen, “Maintaining knowledge about

temporal
Commun. ACM, vol. 26, no. 11, pp. 832–843, Nov. 1983.

intervals,”

[17] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,
S. Orts-Escolano, and J. Garcia-Rodriguez, “A visually realistic grasp-
ing system for object manipulation and interaction in virtual reality
environments,” Computers & Graphics, vol. 83, pp. 77 – 86, 2019.

[18] K. R.-A. Constantin Uhde, Nicolas Berberich and G. Cheng, “The
robot as scientist: Using mental simulation to test causal hypothe-
ses extracted from human activities in virtual reality.” in Proc. of
International Conference on Intelligent Robots and Systems (IROS),
2020.

[19] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari,
E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray,
“Scaling egocentric vision: The epic-kitchens dataset,” in European
Conference on Computer Vision (ECCV), 2018.

[20] M. Beetz, M. Tenorth, and J. Winkler, “Open-EASE – a knowledge
processing service for robots and robotics/ai researchers,” in IEEE
International Conference on Robotics and Automation (ICRA), Seat-
tle, Washington, USA, 2015, ﬁnalist for the Best Cognitive Robotics
Paper Award.

