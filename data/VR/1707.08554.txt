7
1
0
2

g
u
A
2

]

V
C
.
s
c
[

2
v
4
5
5
8
0
.
7
0
7
1
:
v
i
X
r
a

Interpatient Respiratory Motion Model Transfer for Virtual
Reality Simulations of Liver Punctures

Andre Mastmeyer

Matthias Wilms

Heinz Handels

Univ. of Luebeck
Inst. of Med. Inform.
Ratzeburger Allee 160
23568 Luebeck,
Germany

mastmeyer@imi.uni-
luebeck.de

Univ. of Luebeck
Inst. of Med. Inform.
Ratzeburger Allee 160
23568 Luebeck,
Germany

wilms@imi.uni-
luebeck.de

ABSTRACT

Univ. of Luebeck
Inst. of Med. Inform.
Ratzeburger Allee 160
23568 Luebeck,
Germany

handels@imi.uni-
luebeck.de

Current virtual reality (VR) training simulators of liver punctures often rely on static 3D patient data and
use an unrealistic (sinusoidal) periodic animation of the respiratory movement. Existing methods for
the animation of breathing motion support simple mathematical or patient-speciﬁc, estimated breathing
models. However with personalized breathing models for each new patient, a heavily dose relevant or
expensive 4D data acquisition is mandatory for keyframe-based motion modeling. Given the reference
4D data, ﬁrst a model building stage using linear regression motion ﬁeld modeling takes place. Then
the methodology shown here allows the transfer of existing reference respiratory motion models of a
4D reference patient to a new static 3D patient. This goal is achieved by using non-linear inter-patient
registration to warp one personalized 4D motion ﬁeld model to new 3D patient data. This cost- and
dose-saving new method is shown here visually in a qualitative proof-of-concept study.

Keywords
Virtual Reality, Liver Puncture Training, 4D Motion Models, Inter-patient Registration of Motion Models

1

INTRODUCTION

The virtual training and planning of minimally
invasive surgical interventions with virtual real-
ity simulators provides an intuitive, visuo-haptic
user interface for the risk-sensitive learning and
planning of interventions. The simulation of liver
punctures has been an active research area for
years [For16, For15, Mas14].

Obviously ﬁrst, the stereoscopic visualization of
the anatomy of the virtual patient body is impor-
tant [For12]. Second, the haptic simulation of the
opposing forces through the manual interaction,

Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for proﬁt
or commercial advantage and that copies bear this notice and
the full citation on the ﬁrst page. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires
prior speciﬁc permission and/or a fee.

rendered by haptic input and output devices, with
the patient is key [For13]. Third in recent de-
velopments, the simulation of the appearance and
forces of the patient’s breathing motions is vital
[Mas17, Mas14].

The previously known VR training simulators
usually use time invariant 3D patient models. A
puncture of the spinal canal can be simulated
sufﬁciently plausibly by such models.
In the
thoracic and upper abdominal region, however,
respiratory and cardiac movements are constantly
present. In the diaphragm area at the bottom of
the lungs just above the liver, breathing move-
ment differences in the longitudinal z direction
of up to 5 cm were measured [Sep02]. Now
the necessary data consists
for 4D animation,
of a single 3D CT data set and a mathematical
or personalized animation model. Our aim here
is to incorporate these physiological-functional
movements into realistic modeling in order to

To appear in www.WSCG.eu proceedings/journal 2017
p. 1

 
 
 
 
 
 
Figure 1: Left: Hardware: (1) Main stereo rendering window with successful needle insertion into a
target, (2) ﬂuoroscopic simulation, (3) Ultrasound simulation, (4) haptic device handle. Right: Main
rendering window displaying oblique cut and color-coded patient structures just before needle insertion
into a targeted bile duct (green).

offer the user a more realistic visuo-haptic VR
puncture simulation. This means also to take
into account the intra- and intercycle variability
(hysteresis, variable amplitude during inhalation
/ exhalation).

A major interest and long term goal of virtual
and augmented reality is the planning [Rei06]
and intra-operative navigation assistance [Nic05].
However, in these works breathing motion is not
incorporated or applicability limits by neglecting
breathing motion in terms of minimal tumor size
are given [Nic05]. Published approaches from
other groups [Vil14, Vil11] model only a sinu-
soidal respiratory motion without hysteresis and
amplitude variation. First steps in the direction of
a motion model building framework were taken
by our group [Ehr11]. Accurate simulation of res-
piratory motion depending on surrogate signals is
relevant e.g.
in fractionated radiotherapy. How-
ever, since a patient-speciﬁc 4D volume data set is
required for personalized breathing model build-
ing and its acquisition is associated with a high
radiation dose with 4D-CT (≥ 20-30 mSv (eff.)),
our approach is the transfer of existing 4D breath-
ing models to new 3D patient data. For compar-
ison, the average natural background radiation is
approximately 2.1 mSv (eff.) per year1.

On the other hand, there is no medical indication
to acquire 4D CT data just for training purposes

and model building from 4D MR data to be in-
cluded is unjustiﬁable for cost reasons.

In this paper, we present a feasibility study with
ﬁrst qualitative results for the transfer of an ex-
isting 4D breathing model [Wil14] to static 3D
patient data, in which only a 3D CT covering
chest and upper abdomen at maximum inhalation
is necessary (approximately 2- 13 mSv (eff.))2.

2 RECENT SOLUTION

The existing solution requires a full 4D data
set acquisition for each new patient.
In
[For16, Mas16, Mas13, For14], concepts for
a 3D VR simulator and efﬁcient patient modeling
liver
for the training of different punctures (e.g.
punctures) have already been presented,
see
Fig. 1. A Geomagic Phantom Premium 1.5 High-
Force is used for the manual control and haptic
force feedback of virtual surgical instruments.
Nvidia shutter glasses and a stereoscopic display
provide the plausible rendering of the simulation
scene. This system uses time invariant 3D CT
data sets as a basis for the patient model. In case
of manual interaction with the model, tissue de-
formation due to acting forces of the instruments
are represented by a direct visuo-haptic volume
rendering method.

New developments of VR simulators [For15] al-
low a time-variant 4D-CT data set to be used in
real time for the visualized patient instead of a

1 Intercontinental ﬂight max. 0.11 mSv (eff.)

2 Siemens Somatom Deﬁnition AS

To appear in www.WSCG.eu proceedings/journal 2017
p. 2

static 3D CT data set. The respiratory movement
can be visualized visuo-haptically as a keyframe
model using interpolation or with a ﬂexible linear
regression based breathing model as described be-
low.

3 PROPOSED SOLUTION
The new solution requires only a 3D data set ac-
quisition for each new patient.

3.1 Modeling of Breathing Motion
Realistic, patient-speciﬁc modeling of breathing
motion in [For15] relies on a 4D CT data set cov-
ering one breathing cycle. It consists of Nphases
phase 3D images indexed by j. Furthermore,
a surrogate signal (for example, acquired by
spirometry) to parametrize patient breathing in a
low-dimensional manner is necessary.
We use a measured spirometry signal v(t) [ml]
and its temporal derivative in a composite sur-
rogate signal: (v(t), v(cid:48)(t))T . This allows to de-
scribe different depths of breathing and the dis-
tinction between inhalation and exhalation (respi-
ratory hysteresis). We assume linearity between
signal and motion extracted from the 4D data.
First, we use the ’sliding motion’-preserving ap-
proach from [SR12] for Nphases − 1 intra-patient
inter-phase image registrations to a selected refer-
ence phase jre f :

ϕ pat4D
j

argmin
ϕ

=
(cid:16)

DNSSD

(cid:104)
I pat4D
j

, I pat4D
jre f

◦ ϕ

(cid:105)

+ αS · RS(ϕ)

(cid:17)

,

(1)

j ∈ {1, .., jre f − 1, jre f + 1, .., Nphases},

where a distance measure DNSSD (normalized sum
of voxel-wise squared differences [Thi98]) and a
specialized regularization RS establishes smooth
voxel correspondences except in the pleural cavity
where discontinuity is a wished feature [SR12].
Based on the results, the coefﬁcients apat4D
are es-
timated as vector ﬁelds over the positions x. The
personalized breathing model then can be stated
as a linear multivariate regression [Wil14]:

1..3

ˆϕ pat4D(x,t) =apat4D
1
apat4D
2
apat4D
3

(x) · v(t) +
(x) · v(cid:48)(t) +

(x), x ∈ Ωpat4D.

(2)

jref

Thus, a patient’s breathing state can be repre-
sented by a previously unseen breathing signal:
Any point in time t corresponds to a shifted ref-
erence image I pat4D
◦ ˆϕ pat4D(x,t). Equipped with
a real-time capable rendering technique via ray-
casting with bent rays (see [For15] for full tech-
nical details), the now time variant model-based
animatable CT data I pat4D
can be displayed in a
new variant of the simulator and used for train-
ing. The rays are bent by the breathing motion
model and this conveys the impression of an an-
imated patient body, while being very time efﬁ-
cient (by space-leaping and early ray-termination)
compared to deforming the full 3D data set for
each time point and linear ray-casting afterwards
[For15].

jref

3.2 Transfer of Existing Respiratory
Models to new, static Patient Data

Using the method described so far, personalized
breathing models can be created, whose ﬂexibility
is sufﬁcient to approximate the patients’ breathing
states, which are not seen in the observation phase
of the model formation.

However, the dose-relevant or expensive acquisi-
tion of at least one 4D data set has thus far been
necessary for each patient.

Therefore, here we pursue the idea to transfer a
readily built 4D reference patient breathing model
to new static patient data pat3D and to animate it
in the VR simulator described in Sec. 2.

ref

jref

For this purpose, it is necessary to correct for
the anatomical differences between the reference
patient with the image data I pat4D
and the new
image data I pat3D
patient
based on a similar
breathing phase. This is achieved, for example,
by a hold-breath scan (ref) in the maximum
inhalation state, which corresponds to a certain
phase jref
in a standardized 4D acquisition
protocol. A nonlinear inter-patient registration
ϕ(x) : Ωpat3D → Ωpat4D with minimization of a
relevant image distance D ensures the necessary
compensation [Mas16, Mas13]:

ϕ pat3D→pat4D
(cid:16)

jref

argmin
ϕ

DSSD

=
(cid:104)
I pat3D
ref

(cid:105)

, I pat4D
jref

◦ ϕ

+ αD · RD(ϕ)

(cid:17)

,

(3)

To appear in www.WSCG.eu proceedings/journal 2017
p. 3

where a distance measure DSSD (sum of squared
voxel-wise differences) and a diffusive non-linear
regularization RD establishes smooth inter-patient
voxel correspondences. On both sides, the breath-
ing phase 3D image of maximum inhalation is se-
lected as the reference phase (ref). The distance
measurement can be selected according to the
modality and quality of the image data. The trans-
formation ϕ pat3D→pat4D
, which is determined in
jref
the nonlinear inter-patient registration, can now
be used to warp the intra-patient inter-phase de-
formations of the reference patient ϕ pat4D
as a
plausible estimate ϕ pat3D
( j ∈ {1, . . . , n}; ◦: right
to left):

j

j

=

ϕ pat3D
j
(cid:16)
ϕ pat3D→pat4D

jref

(cid:17)−1

◦ ϕ pat4D
j

◦ ϕ pat3D→pat4D

jref

. (4)

The approach for estimating the respiratory mo-
tion for the new patient can now be applied anal-
ogously to the reference patient (see Sec. 3.1).
With a efﬁcient regression method [Wil14], the
breathing movement of virtual patient models,
which are only based on a comparatively low dose
of acquired 3D-CT data, can be plausibly approx-
imated:

ˆϕ pat3D(x,t) =apat3D
1
apat3D
2
apat3D
3

(x) · v(t) +
(x) · v(cid:48)(t) +

(x), x ∈ Ωpat3D.

(5)

Optionally, simulated surrogate signals v(t) can
be used for the 4D animation of 3D CT data.
Simple alternatives are to use the surrogate sig-
nal of the reference patient or also a (scaled) sig-
nal of the new patient pat3D, which can simply
be recorded with a spirometric measuring device
without new image acquisition.

4 EXPERIMENTS AND RESULTS
We performed a qualitative feasibility study, re-
sults are animated in the 4D VR training simulator
[For15].

For the 4D reference patient, a 4D-CT data set of
the thorax and upper abdomen with 14 respiratory
phases (5122× 462 voxel to 13 mm) and a spirom-
etry signal v(t) were used (Fig. 2). The new pa-
tient is represented only by a static 3D CT data set
(5122× 318 voxel to 13 mm).

All volume image data was reduced to a size of
2563 voxel due to the limited graphics memory
of the GPU used (Nvidia GTX 680 with 3 GB
RAM).

According to Eq. 1 we ﬁrst perform the intra-
patient inter-phase registrations to a chosen ref-
erence phase jre f .

The registrations from Eqs. 1 and 3 use weights
αS = 0.1 and αD = 1 for the regularizers RS and
RD. In both registration processes, the phase with
maximum inhalation is used as the reference res-
piratory phase jre f and for the training of the
breathing model.

The respiratory signal used for model training is
shown in Fig. 2b, gray curve. We show the areas
with plausible breathing simulation and use the
unscaled respiratory signal of pat3D with larger
variance to provoke artifacts (Fig. 2b, blue curve).
The model training according to Eqs. 2 and 5 is
very efﬁcient using matrix computations.

j

We use manual expert segmentations of the
liver and lungs, available for every phase of
the 4D patient, to mainly assess the quality of
the inter-patient registration in Eq. 3. Via the
availabe inter-phase registrations ϕ pat4D
(Eq. 1)
to the 4D reference phase, we ﬁrst warp the
phase segmentation masks accordingly. After
applying the inter-patient registration to pat3D,
we have the segmentation masks of pat4D in
the space of the targeted 3D patient. Now for
this patient, also a manual expert segmentation
is availabe for comparison. Quantitatively, the
DICE coefﬁcients of the transferred segmentation
masks (liver, lungs) can be given to classify the
quality of the registration chain of the reference
respiratory phases (single atlas approach). Quali-
tatively, we present sample images for four time
instants and a movie.

The mean DICE coefﬁcients of the single-atlas
registration of the liver and lung masks to the
new static patient pat3D yield satisfying values of
0.86±0.12 and 0.96±0.09. Note the clearly dif-
ferent scan ranges of the data sets (Fig. 2a). The
animation of the relevant structures is shown as
an example in Fig. 3, using a variable real breath-
ing signal of the target patient pat3D (Fig. 2b).
In the puncture-relevant liver region, the patient’s
breathing states are simulated plausibly for the 4D

To appear in www.WSCG.eu proceedings/journal 2017
p. 4

(a) Field of view difference between reference patient
pat4D (turquoise) and target patient pat3D (yellow).
Figure 2: (a) Field of views and (b) respiratory signals of the patients pat4D (gray dashed) vs. pat3D.

(b) Selected times of the spirometry signal from pat3D (blue).

reference patient (Fig. 3) and, more importantly,
the 3D patient (Figs. 4, 5), to which the motion
model of pat4D was transferred3.

5 DISCUSSION, OUTLOOK AND

CONCLUSION

For interested readers, the basic techiques for 4D
breathing motion models have been introduced in
[Ehr11] by our group. However there, the motion
model is restricted to the inside of the lungs and
by design a mean motion model is built from sev-
eral 4D patients. The mean motion model is ar-
tiﬁcial to some degree, more complex and timely
to build. The method described here for the trans-
fer of retrospectively modeled respiratory motion
of one 4D reference patient to a new 3D patient
data set is less complex and extends to a larger
body area. It already allows the plausible anima-
tion of realistic respiratory movements in a 4D-
VR-training-simulator with visuo-haptic interac-
tion. Of course in the future, we want to build a
mean motion model for the whole body section
including (lower) lungs and the upper abdomen,
too.

In other studies, we found αD = 1 in Eq. 3 robust
(compromise between accuracy and smoothness)

3 Demo movie, click here

for inter-patient registration with large shape vari-
ations [Mas13, Mas16]. In Eq. 1 for intra-patient
inter-phase registration, we use αS = 0.1 to allow
more ﬂexibility for more accuracy as the shape
variation between two phases of the same patient
is much smaller [SR12].

We achieve qualitatively plausible results for the
liver area in this feasibility study.
In the up-
per thorax especially at the rib cage in neighbor-
hood to the dark lungs stronger artifacts can occur
(Fig. 5c). They are due to problems in the inter-
patient registration that is a necessary step for the
transfer of the motion model. The non-linear de-
formation sometimes is prone to misaligned ribs.
The same is true for the lower thorax with perfora-
tion ﬁrst of the liver and then diaphragm (Fig. 4c).
Further optimization have to be carried out as ar-
tifacts can appear on the high contrast lung edge
(diaphragm, ribs) with a small tidal volume. For
liver punctures only, the artifacts of smeared ribs
are minor as can be seen in Fig. 4.

Summing up,
the previous assumption from
Sec. 2 of a dose-relevant or expensive acquisition
of a 4D-CT data set for each patient, can be
mitigated for liver punctures by the presented
transfer of an existing 4D breathing model.

Future work will deal with the better adaptation
and simulation of the breathing signal. Further

To appear in www.WSCG.eu proceedings/journal 2017
p. 5

(a) First time point.

(b) Second time point.

(c) Third time point.

(d) Fourth time point.

Figure 3: Field of view, respiratory signal and coronal views with overlayed motion ﬁeld to the CT data
of the patients pat4D (a-d). The color wheel legend below indicates the direction of the motion ﬁeld.

To appear in www.WSCG.eu proceedings/journal 2017
p. 6

(a) First time point.

(b) Second time point.

(c) Third time point.

(d) Fourth time point.

Figure 4: Coronal views with overlayed motion ﬁeld to the CT data of the patient pat3D (a-d) deformed
with the model of pat4D. The color wheel legend below indicates the direction of the motion ﬁeld.

To appear in www.WSCG.eu proceedings/journal 2017
p. 7

(a) First time point.

(b) Second time point.

(c) Third time point.

(d) Fourth time point.

Figure 5: Upper thorax coronal views of the animated CT data of the patient pat3D (a-d) deformed with
the model of pat4D. Rib artifacts are indicated by the yellow arrow in (c).

topics are the optimization of the inter-patient reg-
istration and the construction of alternatively se-
lectable mean 4D reference breathing models. As
in [For16], the authors plan to perform usability
studies with medical practitioners.

To conclude, the method allows VR needle punc-
ture training in the hepatic area of breathing vir-
tual patients based on a low-risk and cheap 3D
data acquisition for the new patient only. The re-
quirement of a dose-relevant or expensive acqui-
sition of a 4D CT data set for each new patient
can be mitigated by the presented concept. Future

work will include the reduction of artifacts and
building mean reference motion models.

6 ACKNOWLEDGEMENT
Support by grant: DFG HA 2355/11-2.

7 REFERENCES
[Ehr11] Ehrhardt, J., Werner, R., Schmidt-

Richberg, A., Handels, H. Statistical model-
ing of 4D respiratory lung motion using dif-
feomorphic image registration. IEEE Trans-
actions on Medical Imaging, 30(2):251–
265, September 2011.

To appear in www.WSCG.eu proceedings/journal 2017
p. 8

[For12] Fortmeier, D., Mastmeyer, A., Handels,
H. GPU-based visualization of deformable
volumetric soft-tissue for real-time simu-
lation of haptic needle insertion. German
Conference on Medical Image Processing
BVM - 2012: Algorithms - Systems - Ap-
plications. Proceedings from 18.-20. March
2012 in Berlin, pages 117–122, 2012.
[For13] Fortmeier, D., Mastmeyer, A., Handels,
H. Image-based palpation simulation with
soft tissue deformations using chainmail on
the GPU. German Conference on Medi-
cal Image Processing - BVM 2013, pages
140–145, 2013.

[For14] Fortmeier, D., Mastmeyer, A., Handels,
H. An image-based multiproxy palpation al-
gorithm for patient-speciﬁc VR-simulation.
Medicine Meets Virtual Reality 21, MMVR
2014, pages 107–113, 2014.

[For15] Fortmeier, D., Wilms, M., Mastmeyer,
A., Handels, H. Direct visuo-haptic 4D
volume rendering using respiratory motion
models. IEEE Trans Haptics, 8(4):371–383,
2015.

[For16] Fortmeier, D., Mastmeyer, A., Schröder,
J., Handels, H. A virtual reality system for
PTCD simulation using direct visuo-haptic
rendering of partially segmented image data.
IEEE J Biomed Health Inform, 20(1):355–
366, 2016.

[Mas13] Mastmeyer, A., Fortmeier, D., Magh-
soudi, E., Simon, M., Handels, H. Patch-
based label fusion using local conﬁdence-
measures and weak segmentations. Proc.
SPIE Medical Imaging: Image Processing,
pages 86691N–1–11, 2013.

[Mas14] Mastmeyer, A., Hecht, T., Fortmeier,
D., Handels, H. Ray-casting based evalu-
ation framework for haptic force-feedback
during percutaneous transhepatic catheter
drainage punctures.
Int J Comput Assist
Radiol Surg, 9:421–431, 2014.

[Mas16] Mastmeyer, A., Fortmeier, D., Handels,

H. Efﬁcient patient modeling for visuo-
haptic VR simulation using a generic patient
atlas. Comput Methods Programs Biomed,
132:161–175, 2016.

[Mas17] Mastmeyer, A., Fortmeier, D., Handels,

H. Evaluation of direct haptic 4d volume
rendering of partially segmented data for
liver puncture simulation. Nature Scientiﬁc
Reports, 7(1):671, 2017.

[Nic05] Nicolau, S., Pennec, X., Soler, L., Ay-
ache, N. A complete augmented reality
guidance system for liver punctures: First
clinical evaluation. Medical Image Comput-
ing and Computer-Assisted Intervention–
MICCAI 2005, pages 539–547, 2005.
[Rei06] Reitinger, B., Bornik, A., Beichel, R.,
Schmalstieg, D. Liver surgery planning us-
ing virtual reality. IEEE Computer Graphics
and Applications, 26(6):36–47, 2006.
[Sep02] Seppenwoolde, Y., Shirato, H., Ki-
tamura, K., Shimizu, S., Herk, M.van ,
Lebesque, J. V., Miyasaka, K. Precise and
real-time measurement of 3D tumor motion
in lung due to breathing and heartbeat, mea-
sured during radiotherapy. Int J Radiation
Oncololgy, Biology, Physics, 53(4):822–
834, Jul 2002.

[SR12] Schmidt-Richberg, A., Werner, R., Han-
dels, H., Ehrhardt, J. Estimation of slipping
organ motion by registration with direction-
dependent regularization. Medical Image
Analysis, 16(1):150 – 159, 2012.

[Thi98] Thirion, J.-P. Image matching as a dif-
fusion process: an analogy with maxwell’s
demons. Medical Image Analysis, 2(3):243
– 260, 1998.

[Vil11] Villard, P., Boshier, P., Bello, F., Gould,
D. Virtual reality simulation of liver biopsy
with a respiratory component. Liver Biopsy,
InTech, pages 315–334, 2011.

[Vil14] Villard, P., Vidal, F., Cenydd, L., Hol-
brey, R., Pisharody, S., Johnson, S., Bulpitt,
A., John, N., Bello, F., Gould, D. Interven-
tional radiology virtual simulator for liver
biopsy. Int J Comput Assist Radiol Surg,
9(2):255–267, 2014.

[Wil14] Wilms, M., Werner, R., Ehrhardt, J.,

et al. Multivariate regression approaches for
surrogate-based diffeomorphic estimation
of respiratory motion in radiation therapy.
Phys Med Biol, 59:1147–1164, 2014.

To appear in www.WSCG.eu proceedings/journal 2017
p. 9

