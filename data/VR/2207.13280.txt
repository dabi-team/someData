2
2
0
2

g
u
A
4
1

]

O
R
.
s
c
[

2
v
0
8
2
3
1
.
7
0
2
2
:
v
i
X
r
a

On-Device CPU Scheduling for Sense-React Systems

Aditi Partap
aditi712@stanford.edu
Stanford University

Samuel Grayson
grayson5@illinois.edu
University
of Illinois at Urbana-Champaign

Muhammad Huzaifa
huzaifa2@illinois.edu
University
of Illinois at Urbana-Champaign

Saurabh Gupta
saurabhg@illinois.edu
University
of Illinois at Urbana-Champaign

Sarita Adve
sadve@illinois.edu
University
of Illinois at Urbana-Champaign

Kris Hauser
kkhauser@illinois.edu
University
of Illinois at Urbana-Champaign

Brighten Godfrey
pbg@illinois.edu
University
of Illinois at Urbana-Champaign

Radhika Mittal
radhikam@illinois.edu
University
of Illinois at Urbana-Champaign

Abstract

Sense-react systems (e.g. robotics and AR/VR) have to take
highly responsive real-time actions, driven by complex deci-
sions involving a pipeline of sensing, perception, planning,
and reaction tasks. These tasks must be scheduled on resource-
constrained devices such that the performance goals and the
requirements of the application are met. This is a difficult
scheduling problem that requires handling multiple scheduling
dimensions, and variations in resource usage and availability.
In practice, system designers manually tune parameters for
their specific hardware and application, which results in poor
generalization and increases the development burden. In this
work, we highlight the emerging need for scheduling CPU
resources at runtime in sense-react systems. We study three
canonical applications (face tracking, robot navigation, and
VR) to first understand the key scheduling requirements for
such systems. Armed with this understanding, we develop
a scheduling framework, Catan, that dynamically schedules
compute resources across different components of an app so as
to meet the specified application requirements. Through experi-
ments with a prototype implemented on a widely-used robotics
framework (ROS) and an open-source AR/VR platform, we
show the impact of system scheduling on meeting the perfor-
mance goals for the three applications, how Catan is able to
achieve better application performance than hand-tuned config-
urations, and how it dynamically adapts to runtime variations.

1

Introduction

Sense-react systems are becoming ever more pervasive. A
variety of robots now assist us with various mundane, high
precision, or dangerous tasks [6, 10, 11, 35, 39, 45, 49, 70, 81].
AR/VR (Augmented/Virtual Reality) has affected the way
we teach, practice medicine [48, 71], and play games, and
is envisioned to be the next interface for compute [66, 76].
These systems typically feature a pipeline of tasks that involve
continually sensing the environment and processing the sensed

inputs to generate a reaction. Such a pipeline ranges from a
single linear chain of components in the simplest applications
to a multi-chain directed acyclic graph (DAG) in more complex
ones. Most such systems run on devices with limited compute
resources, (e.g. Intel NUC [19, 38], Raspberry Pi [41, 44],
Qualcomm Snapdragon XR2 [1], etc.). This work focuses on
managing the compute (CPU) resources on such platforms.

Sense-react systems allow the environmental inputs to be
sampled and processed at a configurable rate (i.e. the input rate
in sense-react systems can be actively controlled, as opposed
to being driven by external factors). CPU scheduling for a
given sense-react application (app) thus involves tackling two
inter-related aspects — how should the available compute
resources be divided across different app components, and
the rate at which each component should be executed (which
includes the input sampling rate).

The appropriate scheduling decision depends on the
amount of available CPU resources, the compute usage of
each component, and the app’s performance requirements.
The scheduler must take into account dynamic variations in
compute usage (due to input-dependent computation times)
and compute availability (e.g. due to battery constraints). The
performance requirements differ along different components
of a sense-react app, and involve semantic trade-offs, where
the tasks performed by some app components are more critical
than others (e.g. in a navigating robot, components responsible
for avoiding local collisions are more important than those
responsible for global path planning).

As we show in §7, under-utilizing or overloading the
system (e.g. by triggering a component too slowly or too
fast), and mis-allocating resources (e.g. giving a greater share
of resources to a less critical component) impact the app’s
performance (that includes the ability of a robot to track a
moving object or avoid collisions, or achieving the desired
motion-to-photon latency in an AR/VR system).

 
 
 
 
 
 
, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Despite rich literature in system scheduling, past work falls
short of addressing all of the specific challenges of sense-react
systems (e.g the ability and need to configure input rates,
handle heterogeneous tasks with differing requirements, and
tackle variability in compute usage at different time-scales).
Thus compute resource management in such systems remains
a challenge in which app developers are provided little help.
While there are frameworks and systems that assist in app
development (e.g. ROS [30], ILLIXR [13, 66], etc), they leave
scheduling decisions entirely up to the developers. Developers,
therefore, manually fine-tune their systems to come up with
static configurations which generalize poorly across scenarios
(and over time) and increase the burden of app development.
To ease the burden of app development and deployment,
we build a scheduling framework, Catan, which takes an app’s
semantic requirements as initial inputs from the developer,
and dynamically schedules the app components at runtime
as per the (varying) compute usage and availability. It is easier
for the developer to specify the semantic inputs (that are based
on domain expertise, and remain unchanged over time and
across compute platforms), as opposed to directly configuring
the scheduling knobs (that must be adapted based on compute
usage and availability).

We begin by studying three different applications (§3) – face
tracking, robot navigation and exploration, and virtual reality.
Through our case studies, we identify the key design consid-
erations for a sense-react system scheduler (§4), and use the
resulting insights to design Catan (§5). Catan adopts a hierar-
chical approach, making its scheduling decision in two stages –
first determining the spatial allocation of CPU cores across app
components, and then determining the temporal allocation of
CPU slices and the rate of executing different app components.
We develop analytical models and heuristics to translate the
scheduling decisions into light-weight constraint-optimization
problems, that Catan solves periodically at runtime to account
for variability in compute usage and availability. Catan can
be plugged into existing frameworks for robotics and AR/VR
(e.g. ROS and ILLIXR respectively), and can be provided as
an optional service to the apps using the framework (§6).

Through our evaluation on the three aforementioned case-
studies (§7), we show (i) how scheduling decisions impact
application performance, and how Catan can (ii) effectively
navigate the semantic trade-offs in performance goals as
per the available compute resources, (iii) achieve better
performance than the default (hand-tuned) configurations, and
(iv) handle variability in compute usage at different timescales.

2 Background

Overview. We can represent a sense-react application as a
directed acyclic graph (DAG), where each node (or vertex)
represents a computation task and each edge represents the
flow of data between tasks. The source nodes in the DAG (with
no incoming edges) are comprised of various sensors, such
as a camera, LiDAR, inertial measurement unit (IMU), etc.,

that continually capture environmental inputs. The sink nodes
(with no outgoing edges) produce reactions, e.g. actuators
in robotics and display in AR/VR. The in-between nodes are
responsible for processing the input streams, e.g. by running
detection and planning algorithms to determine appropriate
reactions to changes in the environment. In most cases, every
time a node runs, it uses the latest outputs generated by its
predecessor nodes as its inputs (although some apps may have
nodes that buffer multiple inputs and batch process them every
time they run). DAGs for different applications vary in their
complexity. We describe three examples in §3.
Deployment Platforms. In many low cost robots, sensors
and actuators are attached to a single on-board computer that
runs all components in the DAG, often using CPU as the only
compute resource [19, 40, 41]. GPUs and other accelerators
can be attached at the cost of higher expense and battery usage.
In larger robot systems, the nodes may be split across multiple
on-board machines [8, 38], or some nodes may be offloaded
to edge or cloud servers [29, 65, 69, 90].

While VR headsets have historically offloaded the
computation to an attached server [12], the desired solution
is to run AR/VR applications on stand-alone devices (e.g.
Oculus Go [22] and Quest [23]) equipped with an embedded
system [1, 21, 27]. The embedded platforms provide on-board
compute: CPUs, GPUs and accelerators such as DSPs.

In this work, we focus on systems using a single on-board
computer, with CPU as the only contended resource. 1 This cov-
ers a broad range of real-world sense-react systems. Extending
our work to systems with multiple contended resources (CPU,
GPU, memory, and network) is an interesting future direction.
Development Frameworks. It is common to use a software
framework (e.g. [5, 15, 20, 24, 25, 30, 62, 66]) for developing
robotics and AR/VR applications. Robot Operating System
(ROS) [30, 79] is by far the most popular development
framework for robotics, with more than 300K estimated
users [36]. It is widely used for developing research prototypes,
with industry usage also growing rapidly [3, 4, 7, 16, 70].
It allows developers to program each component of an
application individually, and provides communication APIs
among those components. ILLIXR (Illinois Extended Reality
testbed) is a complete end-to-end open source VR system
and research testbed, which provides configurable VR system
configurations with widely used system components and work-
flows [13, 66]. ILLIXR provides an OpenXR [24] interface to
the apps, which abstracts over the XR device, runtime, and OS.
The above (open) frameworks leave resource management
decisions entirely up to the app developers. Developers
manually configure the rate at which different nodes are
triggered. CPU scheduling across nodes is left up to the default
OS policies, unless explicitly configured by the developer. 2

1Our VR case-study makes use of GPUs for frame display, but, unlike the
CPU cores, we assume GPU resources are not contended.
2We do not know what scheduling knobs and policies are used in closed-source
/ proprietary systems, which are heavily engineered nonetheless.

On-Device CPU Scheduling for Sense-React Systems

, ,

Figure 1. DAG representing the face tracking robot app. The colors
green, purple, blue and orange represent sensor nodes, perception
nodes, planning nodes and actuators respectively

Figure 2. DAG representing the robot navigation application.

Related Dataflow Systems. DAG abstraction is common
across other systems, including real-time [53, 60, 83, 84,
93, 98, 99], sensor nodes [74, 88], and distributed stream
processing and dataflow systems [28, 46, 47, 52, 67, 72, 73, 77,
85, 91, 91, 95, 96]. Sense-react systems are distinct from these.
As we show in §4, the computation (execution times) in
sense-react systems exhibits a high degree of variability
over time — an aspect that conventional real-time systems
(based on fixed periodicity) do not handle. In comparison
with traditional sensor nodes, sense-react systems have higher
processing complexity and variability, which emphasizes the
importance of proper CPU scheduling.

Distributed stream-processing systems focus on cluster-
wide resource management to handle an incoming stream
of queries. We instead focus on CPU scheduling within a
single computer on board a robot or a VR device, that runs
a single long-running application. Such a system differs in
its scheduling knobs and requirements (detailed in §4). For
instance, in sense-react systems, the scheduler has greater
control over the app, and can actively control the input load.

To summarize, sense-react systems are more dynamic than
traditional real-time systems, more complex than sensor nodes,
and more controlled than distributed dataflow systems. They
thus open an interesting design space for system scheduling.

3 Our Case Studies

We use the three representative applications, spanning robotics
and AR/VR, as our case studies to inform and evaluate our
scheduler design. We briefly describe these applications below.

3.1 Face Tracking Robot

As our first case study, we consider a face-tracking robot
implemented using ROS [32, 63]. It involves a rotating camera
tasked with tracking a moving object (face). Figure 1 repre-
sents the app DAG, which is a simple linear pipeline of the
following nodes: (i) a camera to capture and preprocess images,
(ii) an object detection node to detect the face in the image, (iii)
a planning node to compute the velocity at which the camera
must rotate to track the face, and (iv) the camera rotation motor.
Performance Goals. The goal of this robot is to always keep
the object in its view, i.e. to not lose track of the object.

3.2 Robot 2D Navigation

For our second case study, we consider a robot navigation
app implemented using ROS [33], where the robot is tasked
with exploring and mapping an unknown area. This is a basic
feature that most autonomous mobile robots need to support
[86, 97]. This is a more complex app, that involves semantic
trade-offs, and higher variations in compute usage over time.
Figure 2 represents the application’s DAG. It uses two
sensors – a laser scanner (to capture the environment seen by
the robot) and an odometer (which reports speed and location
information to all other nodes). The global localization, map-
ping and planning nodes (GL, GM, and GP) are responsible
for planning the robot’s trajectory based on its accumulated
knowledge about the area. The trajectory is planned so as
to move towards unknown areas for exploration. The local
mapping and planning nodes (LM and LP) are responsible for
ensuring that the robot avoids collision with obstacles in its
immediate vicinity when following the global trajectory. We
list the specific function of each processing node below. More
details about relevant algorithms can be found in [82, 86].
(i). The local mapper (LM) uses the laser scans and odometry
to update its knowledge about the robot’s immediate vicinity.
(ii). The global localization node (GL) performs two tasks: (a)
for every scan that it receives, it uses particle filtering (Chapter
8 in [86]) to produce an estimated correction for the robot’s
location (which accounts for potential drifts in odometry
readings), and (b) it then filters the scans, discarding the ones
that carry little new information about the environment. Unlike
other nodes in the app that always use the latest inputs available
from their predecessor nodes, GL buffers the received scans,
and batch processes them every time it runs to ensure that all
relevant information about the environment is captured.
(iii) The global mapper (GM) maintains a global map of all
the areas that the robot has explored so far. It uses occupancy
grid mapping (Chapter 9 in [86]) to update the map based on
newly filtered scans produced by GL.
(iv) The global planner (GP) computes the global trajectory of
the robot based on GM’s global map and the robot’s position
(derived from the latest correction from GL and the correspond-
ing odometry reading) using graph search (Chapter 3 in [82]).
(v) Navigation Command (NC) uses the robot’s position, along
with the current trajectory (i.e. GP’s output) to decide the
direction in which the robot should move.
(vi) The local planner (LP) uses the local cost map (that
contains information about nearby obstacles) and the odometry
information, along with the navigation command from NC,
to output the robot’s velocity to the actuator (the mobile base).
Performance Goals. The foremost goal is to avoid colliding
with obstacles. The secondary goal is to ensure that trajectories
are planned using the most up-to-date information about the
robot’s location and the map, otherwise planned trajectories
will be infeasible, unsafe, and inefficient. Finally, the robot
should be able to explore the area at a sufficiently high rate.

, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Figure 3. DAG representing the VR application in ILLIXR [13, 66]

3.3 Virtual Reality

Our final case study considers a different category of
sense-react systems – virtual reality (VR) implemented in
ILLIXR [13, 66]. The VR system’s task is to provide a smooth
visualization of the virtual world, based on the user’s head
movement in the real world.

Figure 3 represents the various nodes in a VR system. It uses
two sensors: (a) an IMU (inertial measurement unit), which
measures acceleration and angular velocity of the user’s head,
and (b) a Camera. The Visual-Inertial Odometry node (VIO)
uses a localization (SLAM) algorithm to fuse information
from the camera and IMU, and produce an estimate of the
user’s pose (location and orientation). It generates high
accuracy poses but is computationally too expensive to run at
a high frequency. To remedy this, the integrator (Int) integrates
new samples from the IMU to estimate the change in the
pose since the last pose from the VIO. This produces higher
frequency, but less accurate, estimates of the user’s pose.

The Render node uses this pose to generate an image (aka a
‘frame’) of the virtual world from the current perspective of the
user. However, the latency of the rendering process adds a lag
between the user’s movement and the generation and display of
the corresponding frame, called the motion-to-photon latency
(MTP) [92, 94]. To minimize this latency, the Timewarp node
takes the latest rendered frame from Render and the latest pose
estimate from Int, and reprojects the frame to be from the latest
perspective of the user (reprojecting is much faster than render-
ing a new frame). Timewarp submits these reprojected frames
to the GPU driver which displays them on a head-mounted
display at the refresh frequency, based on the display hardware.
The reprojection in the ILLIXR version we use accounts
for the user’s rotational motion, but not translational motion
Therefore, we report two MTP metrics - rotational MTP and
translational MTP, representing the delay in capturing the
user’s rotational and translational motion respectively.3
Performance Goals. To provide a smooth virtual experience,
the primary goal of the scheduler is to ensure that the displayed
frame closely tracks the user’s motion with low delay and the
best available pose.

Our experiment setup for each of the above case studies
uses the apps as implemented in their respective frameworks
(ROS for §3.1 and §3.2, and ILLIXR for §3.3). In order to do
controlled (repeatable) experiments, we use standard physics

3It is difficult to capture the latency of the actual display; we therefore do not
include that in our MTP, similar to [66]. With current display technologies,
this latency is roughly constant and affects all scheduling mechanisms studied
here similarly.

simulators [9, 37] to simulate the sensing and actuation for
the two robotics apps (as is standard practice in robotics [54]) –
the rest of the processing nodes are executed as in a real system.
For VR, ILLIXR supports using offline generated datasets as
well as real-time camera and IMU data. For repeatability, we
choose to use widely used sensor datasets [56]4; the rest of
the (real) ILLIXR system is agnostic to this choice. We detail
the experiment setup for each case study in §7.

4 Design Considerations for CPU Scheduler

We use the above case studies to inform our scheduler design.
In this section we list some of the key design considerations.

Terminology. First, we will clarify terminology. As mentioned
in §2, we use the term node to refer to each vertex of the applica-
tion DAG. We use the term chain to denote a unique path from
a source node (e.g., a sensor) to a sink node (e.g., the actuator or
display) along the DAG. A node may belong to multiple chains.
For example, the face-tracking app (Figure 1) comprises of
a single chain from the Camera to the Motor. On the other
hand, the navigation app (Figure 2) consists of multiple chains
from scan and odometry to the mobile base, that pass through
different sets of intermediate nodes (e.g. {scans → LM → LP
→ base}, {scans → GL → NC → LP → base}, {scans → GL
→ GP → NC → LP → base}, etc). Likewise, the VR DAG
(Figure 3) consists of six different chains from Camera and
IMU to Display, passing through different intermediate nodes.

4.1 Scheduling dimensions

In order to optimize app performance, the CPU scheduler
must co-optimize the following key dimensions.
(i) Spatial Core Allocation. Given a multi-core platform, the
scheduler must determine how the CPU cores are divided
across different components in a sense-react app.
(ii) Temporal CPU Allocation. For a core to which multiple
components are assigned, the scheduler must decide how many
CPU slices must be allocated to each of them, and how often.
(iii) Execution Rate. For each component, the scheduler must
decide the rate at which it is triggered (or executed). This fol-
lows from determining the CPU allocation for that component.
We use the term ‘component’ to loosely refer to the
granularity at which the scheduler makes its decisions – we
discuss this more precisely in §4.2.

4.2 Scheduling granularity

We use the term subchain to refer to a series of DAG nodes
within a chain that run at the same rate in an event-driven
manner (with the output from one node triggering the next).
Two nodes in a chain would belong to different subchains if
there is value to running them at different rates (e.g. if they
belong to a different set of chains). Note that each node may
be in many chains, but can only belong to one subchain.

4We use the sensor dataset to emulate the possible trajectory of a person
wearing the VR device. We used the aerial dataset since there is no available
dataset for human head movement.

On-Device CPU Scheduling for Sense-React Systems

, ,

For example, in the navigation DAG (Figure 2), it makes
semantic sense for LP to output a new velocity only upon every
new input from LM that carries updated knowledge of the
robot’s immediate vicinity (and can typically arrive at a faster
rate than global navigation commands from NC). Therefore,
LP and LM can belong to the same subchain. On the other
hand, it is useful to run GP at a higher rate than GM (which
is computationally more expensive), allowing the global
trajectory to be updated based on updated position estimates.
Since all nodes within a subchain must run at the same rate,
a subchain forms the natural granularity at which we can make
the scheduling decisions listed in §4.1. The scheduler needs to
actively trigger only the first node in each subchain, and each
of the remaining nodes in the subchain can be event-triggered
when the preceding node produces a new output. 5

4.3 Performance metrics

The scheduling decisions (§4.1) impact the performance goals
listed in §3. However, it is difficult for the scheduler to directly
reason about app-level performance metrics (e.g. ability
to avoid collisions or track a moving object). We instead
express different app performance metrics using generalized
DAG-level metrics that the scheduler can directly reason about
and optimize. Our first metric corresponds to how quickly and
frequently new inputs are processed along a given chain in
the DAG, while the second corresponds to the processing rate
of a given node. We describe these metrics below:
Chain Response Time. It is the worst-case time from the
moment a change occurs in the environment, to the moment
a reaction is produced at the sink of the chain. We define
response time for consecutive pairs of inputs (𝑖𝑘−1 and 𝑖𝑘 )
that are fully processed by the chain to produce new outputs
at the sink (𝑜𝑘−1 and 𝑜𝑘 respectively). 6 In the worst case, an
environmental change occurs immediately after the source
of the chain captures a previous input 𝑖𝑘−1 — the system
will not react to the change until the next input 𝑖𝑘 is captured
and processed by the chain to produce the output 𝑜𝑘 . Chain
response time is, therefore, the time difference between when
the sink produces a new output 𝑜𝑘 , and when the source
captures the previous input 𝑖𝑘−1. Intuitively, minimizing
response time (𝑜𝑘 −𝑖𝑘−1) implies simultaneously minimizing
the latency of processing an input along a chain (𝑜𝑘−1 −𝑖𝑘−1)
and maximizing the chain throughput, i.e. minimizing the
period (𝑜𝑘 −𝑜𝑘−1) between two consecutive outputs.

The robot’s ability to track the moving object for the face
tracking app is correlated with the chain’s response time (§7).
Likewise, a navigating robot’s ability to avoid collisions is cor-
related with the response time along the chain ‘scans → LM →

5This has lower latency than asynchronous triggering of consecutive nodes.
6Note that there may be other inputs captured by the chain source between
𝑖𝑘−1 and 𝑖𝑘 that get dropped (or overwritten by newer inputs) along the chain
and do not influence the output at the sink. Likewise, there may be other
outputs between 𝑜𝑘−1 and 𝑜𝑘 that use newer inputs along other chains incident
at the sink, but are based on input 𝑖𝑘−1 from the given chain’s source.

Figure 4. Variation in the CPU usage over time of two components
in the navigation application: GP (left) and GL (right).

LP → base’, which captures how quickly the robot can react
to dynamic obstacles in its immediate vicinity. Minimizing ro-
tational MTP in VR directly translates to minimizing response
time along the ‘IMU → Int → Timewarp → Display’ chain.
Node Throughput. It is the rate at which a given node
processes its inputs to produce new outputs. Ensuring that the
latest position estimate is available for generating the robot’s
trajectory requires high throughput for the GL node (that feeds
into multiple other nodes) in the navigation app.

4.4 Semantic preferences and constraints

Preferences across different metrics. Certain performance
metrics are semantically more important than others. For
example, in order to avoid collisions during robot navigation,
minimizing response times along the local chains must be
given highest priority. Next in line is ensuring high GL through-
put (to avoid faulty navigation commands derived from stale
position information), followed by a sufficiently high through-
put for the remaining nodes to minimize the exploration time.
Similarly, as detailed in §7, the VR system also involves seman-
tic preferences across the response times for different chains.
Constraints on different metrics. An app may require an
upper bound on a chain’s response time or a lower bound
on a node’s throughput, based on semantic requirements. In
addition, a node’s throughput can be upper bounded by the
hardware limits of the sensors and actuators/display.

4.5 Variations in Compute Usage and Availability

Variation in compute usage over time. We use node com-
putation time to capture the amount of CPU consumed by a
node every time it runs. We observe that the computation time
of some nodes may increase over time. For example, as the
navigating robot covers more area, the size of the global map
increases, which increases the time taken by (i) GM to update
and generate the global map, and (ii) GP to plan the robot’s
trajectory based on the map (as shown in Figure 4 (left)).
Input-dependent variability. We observe that the GL node
for robot navigation has a bimodal computation time — for
each laser scan, it either does a quick check and discards it if
there is no new information, or processes the scan to update the
pose correction. For the frames that were fully processed by
GL, we additionally observe occasional spikes in computation
times (as shown in Figure 4 (right)). A closer analysis revealed
that these spikes arise from a semantic optimization called

050100150200250Time since exploration start (s)0.060.080.100.120.140.160.180.20Computation time (s)Navigation: Compute time for Node GP, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Figure 5. High-level workflow of Catan

“loop closure” [14, 86], where on re-visiting a location, the
robot runs an expensive non-linear optimization to jointly
update the map and all robot poses.

The VIO node in the VR app also shows high variability
in compute usage, depending both on the complexity (feature
density) of the user’s surroundings and the user’s motion.
Variation in available compute resources. The amount
of resources available to the application would vary across
different hardware platforms. Even for a given platform, the
amount of available compute resources may change over the
duration of the application run — e.g. as a robot’s battery
starts draining over time, the number or frequency of CPU
cores can be reduced to save power.

The scheduler must update scheduling decisions over time
to account for such variability. Moreover, it must be robust
to situations where a node is unable to finish processing an
input during its allocated CPU time slices.

4.6 Putting it all together

We design Catan, a CPU scheduler for sense-react application,
based on the above considerations. For a given app (repre-
sented as a DAG), Catan makes CPU scheduling decisions
(§4.1) at the granularity of subchains (§4.2), so as to satisfy the
semantic preferences and constraints (§4.4) on chain response
times and node throughputs (§4.3), while handling potential
variations in compute usage and availability (§4.5).

Figure 5 represents the high-level workflow of Catan. It
provides an interface for the app developers to express their
semantic requirements. In particular, the developers specify (i)
the DAG structure (nodes and edges), (ii) grouping of nodes
into subchains, (iii) which metrics must be optimized (i.e. re-
sponse times of which chains and throughput of which nodes)
and the corresponding constraints and weights across these
metrics (to capture semantic trade-offs). 7 Catan periodically
tracks the per-node computation times and compute availabil-
ity, and adaptively determines the schedule (node execution
rates and CPU allocation), so as to best meet the specified
semantic requirements. We detail Catan’s design in §5.

Catan inputs from app developers require significant
domain expertise (in the form of thorough understanding of
the app requirements). Current frameworks, on the other hand,
require app developers to directly configure the low-level
scheduling knobs (node execution rates and system scheduling
policies) — this requires system-level expertise in addition

to domain expertise. More importantly, the inputs required
by Catan are largely independent of the compute availability
and usage — they rely only on app semantics and can remain
unchanged across deployment platforms and over time. The
precise schedule and system-level knobs, on the other hand,
must be dynamically adapted based on varying compute
usage and availability (§7 shows how static configurations
lead to sub-optimal performance). Catan, thus, simplifies the
inputs required from app developers by providing a layer of
abstraction that translates static semantic requirements into
a dynamic low-level system configuration and schedule.

One could simplify the inputs required from app developers
even further by adding hooks to the development framework
to infer the DAG structure, inferring subchain grouping
based on specified performance constraints and weights, and
inferring DAG-level performance requirements from app-level
performance requirements. These extensions are beyond the
scope of this work, and we leave them for future research.

5 Catan Design

We model our scheduling problem as a constrained opti-
mization problem, wherein the objective is a weighted linear
combination of the DAG-level metrics (i.e. chain response
times and reciprocal of node throughputs). Catan schedules
the app subchains, so as to optimize the specified objective
and meet any specified constraints.8

Catan adopts a hierarchical approach by breaking the
scheduling decisions into two stages — the first stage
determines the mapping between subchains and cores (§5.1),
and the second stage makes scheduling decisions at per-core
and per-subchain granularity (§5.2). We begin with outlining
our approach under the assumption that the computation time
at each node and the number of cores stays constant, and
discuss how we handle variability in §5.3.

5.1 Stage I: Core Allocation
For a DAG with 𝑁 subchains on a system with 𝐾 cores,
Catan’s first stage of optimization outputs a boolean matrix of
size 𝑁 ×𝐾, where an element 𝑎𝑖 𝑗 is 1 if subchain 𝑖 is allowed
to execute on core 𝑗. Given the exponential solution space,
we add a constraint for tractability: each subchain either runs
alone on one or more cores or shares a single core with other
subchains (i.e. two or more subchains do not share two or
more cores). We also add a trivial constraint: each subchain
should be assigned to at least one core, and vice versa.

We begin with estimating the period 𝑝𝑖 of the subchain 𝑆𝑖 , as
a function of 𝑎𝑖 𝑗 variables. The period of a subchain captures
its execution rate; subchain 𝑆𝑖 processes a new input every 𝑝𝑖
time units. We consider two cases (based on the above con-
straints), and make simplifying assumptions in each case for
computational tractability – we relax these assumptions when
making finer-grained scheduling decisions in Stage II (§5.2).

7Catan allows the developers to optionally specify a few other semantic inputs
that we discuss in §5.

8We allow the constraints to be soft, i.e. if the scheduler cannot meet them,
it prints a warning, and aims to meet a looser (scaled up) set of constraints.

Continual tracking of node computation times and compute availability.Static semantic inputs(app DAG & requirements)Dynamic schedule(node execution rates & CPU allocation)Long-running sense-react appOn-Device CPU Scheduling for Sense-React Systems

, ,

Metric

Chain 𝐶𝑡 : 𝑆𝑡 1 →𝑆𝑡 2..𝑆𝑡𝑛, with periods 𝑝𝑡 1..𝑝𝑡𝑛
Approximation
𝑝𝑡 1 +Σ𝑛
𝑥=22∗𝑝𝑡𝑥
1/(𝑚𝑎𝑥𝑛
𝑝𝑡𝑥 )
𝑥=1

Chain Latency
Chain Throughput
Chain Response Time

Latency + 1/Throughput

Table 1. Approximate formulae for chain-level metrics. Note that we
use the worst case latency formula, and average throughput (the exact
time gap between two outputs can vary in practice). These simple
approximations allow us to formulate the problem as MILP and GP
in Stages I and II respectively.

(i) If multiple subchains share a single core 𝑥, we assume each
subchain gets an equal share of the CPU core (inspired by
Linux’ fair scheduling [17, 18]). The period of a subchain 𝑆𝑖
that is assigned to core 𝑥 will then be the sum of computation
time of all nodes in 𝑆𝑖 multiplied by the number of subchains
that are sharing the core 𝑥.
(ii) If a subchain 𝑆𝑖 is assigned 𝑘𝑖 cores (𝑘𝑖 ≥ 1), we assume
all nodes in 𝑆𝑖 have the same degree of parallelism (i.e. all
nodes can be parallelized to use 𝑞 cores, where 1 ≤𝑞 ≤ 𝑘𝑖 ) and
their compute scales perfectly with it. We use the analytical
formulation described later in §5.2 to compute the period 𝑝𝑖
based on the computation time of each node in the subchain.
Thus, for a given core allocation 𝑎𝑖 𝑗 , we get approximate pe-
riods 𝑝𝑖 for each subchain 𝑆𝑖 . We next estimate the chain-level
and node-level metrics as a function of 𝑝𝑖 for each subchain
which, in turn, allows us to estimate the final objective function
as a function of 𝑎𝑖 𝑗 . For a subchain 𝑆𝑖 with period 𝑝𝑖 , the
throughput of each node in 𝑆𝑖 is equal to the subchain’s through-
put, i.e. 1/𝑝𝑖 . Table 1 lists how we estimate chain-level metrics,
for a chain 𝐶𝑡 comprising of subchains 𝑆𝑡 1 → 𝑆𝑡 2 → 𝑆𝑡 3..𝑆𝑡𝑛.
The average chain throughput is bottlenecked by its slowest
subchain. We estimate the worst-case chain latency as follows:
at each subchain 𝑆𝑡𝑖 in the chain, a new output 𝑜𝑡 (𝑖−1) from the
preceding subchain (𝑆𝑡 (𝑖−1) ) might have to wait for a whole
period, (i.e. 𝑝𝑡𝑖 ) for 𝑆𝑡𝑖 to finish its current execution (except
for at 𝑆𝑡 1 where there’s no waiting time), and it will take 𝑝𝑡𝑖
time for 𝑆𝑡𝑖 to fully process 𝑜𝑡 (𝑖−1) and produce a new output. 9
We then estimate the chain response time as the sum of the
chain latency and period (reciprocal of throughput).

Using the models above, we formulate a Mixed Integer
Linear Program (MILP) that solves for 𝑎𝑖 𝑗 such that the
specified objective function is optimized. Intuitively, the solver
determines the right trade off of which subchains can share
a single core vs which ones should be allowed to scale up to
multiple cores, based on the specified weights and constraints.

5.2 Stage II: Per Core and Per Subchain Scheduling

Given the subchain to core mappings from our first optimiza-
tion stage, the next stage makes finer-grained scheduling
decisions for each subchain and core. We consider two cases:

9It is a common practice in real-time literature [60] to compute worst case
latency of chains in this manner.

Single subchain on one or more cores. For a subchain 𝑆𝑖
comprising of nodes {𝑛𝑖1,𝑛𝑖2,...𝑛𝑖𝑚 } with 𝑘𝑖 assigned cores, the
scheduler computes the time period 𝑝𝑖 of the source node (𝑛𝑖1)
and the degree of parallelism for a node in the subchain (𝑞𝑖 ),
such that the response time along the subchain is minimized.
The period of the subchain corresponding to a given value

of 𝑞 ∈ [1,𝑘𝑖 ] is given by:

𝑝𝑖 (𝑞) =𝑚𝑎𝑥 (𝑚𝑎𝑥𝑚

𝑗=1 (𝑐𝑞

𝑖 𝑗 ),Σ𝑚

𝑗=1(𝑐𝑞

𝑖 𝑗 )/⌊𝑘𝑖 /𝑞⌋)

(1)

𝑗=1(𝑐𝑞

where 𝑐𝑞
𝑖 𝑗 is 𝑛𝑖 𝑗 ’s computation time with at most 𝑞 cores (only
a subset may be designed to use all 𝑞). Note that, unlike
period computation in Stage 1, we allow different nodes in the
subchain to use different degrees of parallelism in this stage.
The corresponding response time of the subchain, for a
given 𝑞, can be analytically computed as (Σ𝑚
𝑖 𝑗 ) + 𝑝𝑖 (𝑞)).
We iterate over all possible values of 𝑞 ∈ [1,𝑘𝑖 ] and select
the value 𝑞𝑖 (and the corresponding 𝑝𝑖 ) that results in lowest
response time for the subchain. We have proved that this
rate allocation achieves subchain response time within 2×
the optimal, and equal to the optimal if all the nodes in the
subchain can only use a single thread (i.e. 𝑞𝑖 = 1) [2].
Multiple subchains on a single core. For subchains 𝑆1,𝑆2...𝑆𝑛
assigned the same core, the scheduler must determine their
periods and temporal allocation of CPU time across them.
We construct a periodic schedule, and execute 𝑓𝑖 fraction of
subchain 𝑆𝑖 in each period, such that Σ𝑛
𝑖=1(𝑓𝑖 ∗𝑐 (𝑆𝑖 )) equals the
period of the schedule, where 𝑐 (𝑆𝑖 ) is the sum of computation
time for all nodes in 𝑆𝑖 . The execution of each (fractional)
subchain within a period follows a configurable ordering. 10
Each subchain will finish processing one input once every
1/𝑓𝑖 periods, i.e. 𝑝𝑖 =
. We allow 𝑓𝑖 to be larger than
1 for subchains that require optimizing average throughput
(e.g. GL in robot navigation) 11, and constrain 1/𝑓𝑖 to be an
integer for other subchains to allow the scheduler to control
the exact throughputs. We combine the above period formula
with metric estimations given in Table 1 , and formulate
a Geometric Programming problem to compute the 𝑓𝑖 and
𝑝𝑖 variables such that the specified objective function is
optimized under the specified constraints.

Σ𝑛
𝑖=1 (𝑓𝑖𝑐 (𝑠𝑖 ))
𝑓𝑖

5.3 Handling Variations

Coarse Grained Variations. We handle coarse-grained
variations over time by continually recording the computation
time across all nodes and tracking the number of available
cores. We re-compute the stage II scheduling decisions
(§5.2) periodically using the 95%ile computation time for
each node measured over the previous 50 values in our
implementation. We handle multimodal computation times by

10By default, Catan assigns the order based on specified weights, but provides
an option to directly specify it as a semantic input.
11We cannot assume a fixed time period of input processing for nodes that
buffer and batch process inputs, and instead consider their average throughput.
Such nodes can either be auto-identified (given support from framework) or
can be explicitly identified by the developer.

, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

taking the weighted sum of 95%ile computation times across
the different modes. We invoke the more expensive stage I
optimization (§5.1) less frequently.
Fine Grained Variability. In spite of periodically adapting
the scheduling decisions, a node may still exceed its expected
(previously measured) computation time. To handle these
situations, the scheduler implements a priority-based stealing
mechanism, wherein if nodes 𝐴 and 𝐵 share a core, and 𝐵
has lower priority than 𝐴, then in each period of the schedule,
the scheduler allocates 𝐵’s CPU time to 𝐴 if the last output
from 𝐴 was not received at its expected time period. We infer
node priorities from the specified weights, and also provide
the option to explicitly specify these as semantic inputs.

6

Implementation

We implement Catan scheduler on top of Linux, as a ROS node
for robotics and as an ILLIXR plugin for VR, both of which use
the same backend which handles all the scheduling decisions.
Initialization. At initialization time, Catan requires the DAG
structure and the thread ids corresponding to each node, since
they are required to enforce fine grained scheduling. Note
that both of the above inputs can be automatically obtained
by adding hooks to the framework. For robotics, we modify
the ROS communication library [31] to expose the ids of all
the threads it uses under-the-hood to handle communication
between nodes.
Bootstrapping. Catan takes the application DAG and
constraints/weights as input, and bootstraps the core allocation
by assigning equal compute time to all the nodes and running
the Stage - I solver. 12 Based on the output, Catan spawns
a thread 𝑇𝑄 𝑗 for each core with multiple subchains, and a
thread 𝑇 𝑆𝑖 for every subchain assigned to one or more cores, to
handle the per-core and per-subchain scheduling respectively.
It bootstraps the per core fractional schedule by assigning a
fraction of 1 to all subchains. The bootstrapped configuration
lasts only for the first 2s, until actual node computation times
are measured and the solver is invoked. Lastly, it spawns a
dynamic re-optimization thread which periodically updates
the scheduling decisions. All the scheduler threads (𝑇𝑄 𝑗 and
𝑇 𝑆𝑖 ) are assigned the SCHED_FIFO policy with a very high
priority of 4, except for the re-optimization thread which runs
with the SCHED_OTHER policy, so as not to interfere with
the application’s execution.
Runtime. At runtime, each scheduler thread 𝑇 𝑆𝑖 triggers
the first (source) node of the subchain 𝑖 at the analytically
computed time period. The threads 𝑇𝑄 𝑗 enforce the core 𝑗’s
periodic fractional schedule using two mechanisms. First, it
triggers the first node in each subchain 𝑠𝑦 once every period
if 𝑓𝑦 ≥ 1, and once every 1/𝑓𝑦 periods otherwise. Secondly,
in each period of the schedule, it assigns SCHED_FIFO
policy with the lowest priority of 1 to all the node threads, and

12We set same compute times of all nodes at bootstrap, so as to share the CPU
equally among all nodes (the exact value, set to 5ms, does not matter).

iteratively bumps up the SCHED_FIFO priority (to 2) of (all
the threads of) each subchain for the computed fraction of its
time within the period. As described in §5.3, the controller can
enforce priority-based stealing within each period. For threads
that we do not schedule explicitly, (e.g. IMU in ILLIXR which
needs to run at high frequency and has negligible compute),
we assign SCHED_FIFO policy with a fixed high priority (3)
so that they can preempt any node whenever needed 13.

The scheduler takes 0.12ms to pause the execution of one
node, and start executing the next. We add an extra constraint
on each subchain 𝑐 (𝑠𝑖 ) ∗ 𝑓𝑖 ≥ 1 to limit the scheduler’s
intervention (and the ensuing overhead due to that). We also
add 5% slack time in each period, which can be used by the
re-optimization thread or the application threads.14 We scale
the analytical functions for the low level metrics accordingly,
in both the optimization formulations.
Dynamic Re-optimization. We periodically re-solve for the
new optimal scheduling decisions, based on the latest compute
time estimates of all nodes (as discussed in 5.3). We have
implemented both the MILP and GP formulations using the
Mosek Fusion C++ library. It takes our solver 24-26ms to
solve the GP for the navigation application and 5-6ms for
VR. Solving the MILP is more expensive, requiring 60ms for
the navigation DAG for 2 cores. We invoke both the solvers
2s after initialization, and then re-run Stage I every 20s, and
Stage II every 5s. Increasing the time periods can decrease the
overhead of running the solvers. We chose the periods so as to
keep the overhead roughly under 0.05 CPU cores15. For each
execution of the Stage I optimization, if the core allocation
changes, then we kill the existing 𝑇𝑄 𝑗 and 𝑇 𝑆𝑖 threads and
re-initialize them based on the new solution. We kill the
threads for simplicity, and observe negligible overheads
due to this, but the implementation can be updated to send
information to the existing threads instead of killing them.

7 Evaluation

7.1 Face Tracking

Our first study highlights the importance of scheduling
decisions in sense-react systems in the absence of complex
semantic trade-offs and variability.
Scheduler Configuration. As the app comprises of only
a single chain, the scheduling objective is to minimize its
response time. All app nodes belong to the same subchain.
Experiment Setup. We use Gazebo [9] (a standard physics
simulator used for evaluating robotics applications) to
simulate a dummy moving along a square path centered at a
robotic camera that can rotate at a velocity of up to 3.2rad/s

13One could use Linux’ SCHED_DEADLINE scheduler to implement
Catan’s scheduling decisions instead of SCHED_FIFO, but we found it to
be too inflexible for handling variations.
14This is done to account for the fact that Linux does not allow SCHED_FIFO
processes to use more than 95% CPU time [18].
15The process of selecting the solver periods can easily be automated based
on this criteria by tracking the solvers’ compute usage.

On-Device CPU Scheduling for Sense-React Systems

, ,

compute). The black and gray vertical lines indicate the
optimal value assigned by Catan, and the default configuration
respectively. The compute capacity is the bottleneck with one
core, while the slowest node is the bottleneck with two cores.
Response time (shown in the top graphs) is lowest at the
optimal source frequency computed by Catan using the
analytical formulation in §5.2. This is because a lower than
optimal frequency lowers throughput of the chain, while
higher than optimal frequency increases latency (detailed
explanation omitted for brevity).

We draw the following key observations with respect to
the tracking performance (shown in the bottom graphs): (i)
Tracking performance strongly correlates with the response
time, confirming the impact of low-level metrics on app per-
formance. (ii) The system has the best tracking performance
when running at the optimal frequency configured by Catan,
outperforming the default configuration. This shows the
impact of scheduling on app performance. (iii) In general,
it is harder to track a faster moving object. Catan’s optimal
frequency is able to track the object for wider speed ranges.

7.2 Robot Navigation

Scheduler Configuration. We model the objective function as
the weighted sum of response times for the different chains in-
volving each node in the navigation DAG (detailed below), and
the average output period for GL (i.e. the average time interval
at which it processes scans) as a measure of its throughput. We
do not explicitly schedule odometry, and configure Scan → LM
→ LP as a single subchain. All other nodes shown in Figure 2
(i.e. GL, GM, GP, and NC) are treated as individual subchains,
that can run at different rates. The analytical response times for
the local chains (involving the LM and LP) sourced at scans
and at odometry differ only by a constant (odometry’s period)
– we combine them into a single response time metric for our
objective function, assigning it the highest weight of 1.0 (as
they are responsible for avoiding collisions). We use a weight
of 0.5 for GL’s average output period (to ensure freshness of
pose estimates and odometry for path planning). In addition to
these, our objective function includes the response time along
three chains that include the three remaining processing nodes
(GM, GP, NC) sourced at GL– each assigned a small weight
of 0.005 (to ensure that these nodes are not starved as the local
chain and GL are prioritized). 16 We also add the following
application specific constraints into our optimization formula-
tion: (i) GL’s average throughput must be at most 50Hz, based
on the maximum frequency at which the simulator can publish
laser scans. (ii) GP’s throughput must be at most 1Hz.
Experiment Setup. We use Stage [37] to simulate a P3AT
robot [26]. The simulator feeds laser scans and odometry
into the navigation application, which is implemented on
top of ROS [33]. The application feeds the robot’s velocity

16When analytically computing response times along the chains sourced at
GL, the scheduler uses the empirically observed value of the rate at which
it outputs filtered scans (1.5Hz at 75%ile).

Figure 6. Response time (top) and tracking performance for different
object speeds (bottom) as source frequency is varied.

([42, 43]). Gazebo feeds camera inputs to detection and
planning nodes implemented in ROS, which in turn feed the
velocity output back to the simulated camera.

We run the whole setup (i.e. the simulator as well as the
application) on an AWS EC2 instance (m5.4xlarge). Gazebo
runs on 5 reserved cores, and sends camera inputs as fast as
it can to a shim source node, which models the time taken
for post-processing or formatting camera inputs (measured
as 25ms using the usb-cam module [34]) and then publishes
the latest input at the specified frequency. Since the algorithm
to detect the simulated dummy is trivial, we model realistic
timings by augmenting our detection node – every time it pro-
cesses a simulated frame, it also runs a single-cascade HAAR
algorithm [32, 78] on a frame drawn from a real video data-set
[59, 87] (the time taken for this varies between 55-60ms). The
planning node (which takes 1ms) tracks the dummy using the
detection output from the simulated frame. We experiment
with varying source frequency and speed at which the dummy
moves, as well as with different number of cores 𝑘 made
available to the application (that excludes Gazebo).
Baselines. We compare Catan to the default configuration,
which is based on the source frequency hand-tuned by the
application developer (30 Hz) and uses Linux’ default policy
SCHED_OTHER (based on Completely Fair Scheduler) for
all other scheduling decisions. This configuration remains the
same regardless of the number of cores.
Metrics. We evaluate Catan on the chain response time as
well as the robot’s face tracking performance. We measure
the tracking performance as average angular distance (Δangle)
between the camera’s orientation and the position of the
dummy (a lower value implies better performance).
Results. Figure 6 shows the response time and tracking
performance for a range of source frequencies on a system
with one and two cores (to model platforms with limited

on 1 coreon 1 coreon 2 coreson 2 coresBlack vertical lines indicate computed optimal frequency. Gray vertical lines indicate default frequency., ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Figure 7. Map 1 (left) and Map 2 (right) used for navigation
experiments. The blue squares represent the obstacles and the red
square represents the robot. The obstacles in Map1 are dynamic, and
move along the two doorways.

Figure 8. No. of runs with collisions (out of 40) on Map1, with (i)
Default config, (ii) Default with LC, LP at 50Hz, and (iii) with Catan,

Figure 9. Tail odometry staleness at NC on Map2 for Catan vs
Default vs Catan w/o priority stealing (median over 20 runs).

back into Stage. We configure the simulator to publish laser
scans and odometry at 50Hz. We conduct experiments on two
maps: (i) Map 1 (Figure 7(left)) requires the robot to move
around a dynamic obstacle at the entrances of a room (this
models realistic scenarios where collisions with dynamic
obstacles may occur at narrow doorways). This map allows
us to evaluate the ability of the robot to avoid collisions with
dynamic obstacles under different configurations. (ii) Map
2 (Figure 7(right)) requires the robot to explore a larger area.
We do not add any dynamic obstacles to this map, focusing
on other performance metrics. Unless otherwise specified, all
nodes run on a system with one core (it is common for robots
to have single / dual core on-device compute [26]).
Comparison with manually-tuned configuration.
Baselines. We begin with comparing the performance of
Catan with the default configuration (manually-tuned by the
developer of the navigation app). In line with the interface

currently exposed by ROS, the default configuration only spec-
ifies the execution rate of each node [33] (set to 5Hz for LM,
10Hz for LP, 0.2Hz for GM, upto 1Hz for GP, 10Hz for NC),
relying on Linux’ default policies (SCHED_OTHER based
on Completely Fair Scheduler) for other scheduling decisions.
Metrics. We evaluate Catan against the default configuration on
the robot’s performance for both Map1 and Map2. For Map1,
we do 40 experiment runs and compare the number of runs in
which the robot collides with an obstacle. For Map2, we com-
pare the staleness of odometry used for generating navigation
command and the area exploration rate. We capture odometry
staleness by recording the time difference between an odom-
etry reading and when the corresponding pose is used by NC.
For each experiment run, we aggregate the staleness values
collected over time buckets of 5s by computing the 95%ile val-
ues. For each 5s bucket, we report the median of these values
across 20 runs. We define the area exploration rate as the time
taken to explore 90% of the map area, averaged over 20 runs.
We omit the comparison of low-level metrics for brevity, but
Catan is able to achieve better response time for all chains.

Map1 Results. Figure 8 compares the number of experiment
runs (out of a total of 40) in which the robot collides with an
obstacle on Map1. With the default configuration (which runs
the LM and LP at low rates of 5 Hz and 10 Hz), more than
50% runs suffer from collisions. Increasing the rate of LM and
LP nodes to 50 Hz reduces the number of collision to 9. With
a high weight assigned to the response time along the local
chain, Catan is able to run the local chains at an even higher
rate (125 - 170 Hz), reducing the number of collisions to only
3. These results highlight the impact of scheduling decisions
on a robot’s performance.

Map2 Results. Figure 9 shows the odometry staleness
metric. We find that, in general, Catan has lower odometry
staleness than the default policy. When GL’s compute usage
spikes up (especially during loop closures) and as the compute
load increases over time (due to increased CPU usage for GM
and GP), Catan is able to explicitly prioritize GL over GM and
GP due to its periodic adaptation and priority-based stealing.
We isolate the impact of priority-based stealing by disabling it
for GL (results shown with the pink line in Figure 9). Disabling
priority-based stealing increases staleness, as the the scheduler
cannot handle sudden spikes in GL due to loop closure.

The above results show how Catan performs better than
the default configuration with respect to avoiding collisions
and ensuring freshness of odometry readings. Improvements
in these more critical metrics come at the cost of a small
reduction in the area exploration rate – the time taken to
explore 90% of the area increases by 10.9%, as GM is
allocated smaller amount of resources in Catan than in the
default scheme. Thus, Catan achieves the desired trade-offs,
as per the configured weights and priorities.
Adapting to variations in resource usage over time.
We next highlight the importance of updating scheduling
decisions over time, as resource usage changes. We extend the

010203040# Runs with Collisions (in 40 explorations)Default 1 CoreDefault(LC, LP 50Hz)1CoreCatan1 CoreScheduling Policy2393020406080100120140160180200220240Time since exploration start (s)050100150200250300350400450500550600650Odometry Staleness at NC (ms)CatanDefaultCatan w/o priority-stealOn-Device CPU Scheduling for Sense-React Systems

, ,

Figure 10. The output period (as a measure of reciprocal of
throughput) for the YOLO chain across three schemes: Catan, and
two static baselines Static-FullRun and Static-20s on 1 core. We
report the median of average period over 20 runs.

Figure 11. Comparing tail odometry staleness at NC (median over 20
runs) on Map2 across three schemes: Catan, and two static baselines
Static-FullRun and Static-20s on 1 core.

navigation DAG by adding another chain that runs an object
detector on camera images as the robot moves around (this is
representative of robotic tasks, such as ObjectGoal task [50]).
The chain comprises of a pre-processing node (similar to the
one used in §7.1) and a YOLO object detection node [80]17. To
model realistic timings in the simulated environment, we feed
images from a real image dataset [61] to the object detection
node. We augment our objective function for Catan to include
the response time along this chain as another metric with a
very low weight of 0.0005. We run this extended navigation
application on a system with one core.
Baselines. We compare Catan with two baselines that use
a static configuration (that is not updated over time): (i)
Static-FullRun that uses the same optimization problem as
Catan based on the tail computation time for each node over an
entire experiment run on Map2 using a single core. It computes
the schedule once, and does not re-solve it periodically. Note
that since computation times of GM and GP nodes increase
as the run progresses, this scheme uses a schedule derived
from over-estimated computation time towards the beginning
of the run. (ii) Static-20s is similar to Static-Full, but uses
tail computation times of each node over the first 20s of

17While YOLO is usually GPU based, we don’t run it on GPU since we focus
on CPU scheduling. We leave CPU/GPU co-scheduling [89] to future work.

Figure 12. The output period (as a measure of reciprocal of
throughput) for the YOLO chain across three schemes: Catan, and
two static baselines static-FullRun and Static-20s on 2 cores.

the run. This scheme would use a schedule derived from an
under-estimated computation time in the later half of the run.
Metrics. We desire the objects to be detected in real time at a
high rate, but that should not come at the cost of not being able
to navigate well. Hence, we evaluate Catan and the baselines on
multiple metrics – number of collisions in 40 runs on Map1 and,
odometry staleness at NC, area exploration rate and the average
output period for the YOLO chain (as a measure of reciprocal
of its throughput) over 20 runs on Map2. To aggregate YOLO
chain’s throughput data across 20 runs, we first take the average
period (measured as the time gap between two consecutive out-
puts from the YOLO chain) over 10s buckets for each run, and
then plot the median of these average values for each bucket.
Results. Figure 10 shows the average output period for the
YOLO chain over runs on Map2 – lower is better. We find that
Static-FullRun, which overestimates the computation time
of navigation nodes in the first half of the run, assigns a low
rate to the YOLO chain (resulting in high output period in the
first half). Static-20s, which underestimates the computation
of navigation nodes (by looking at the values in the first 20s
of a run), assigns a high rate to YOLO – as we discuss next,
this comes at the cost of worse performance on a more critical
metric. Catan, with its dynamic re-solving assigns a high rate
(low period) to YOLO at the beginning of the run, and then
gradually decreases the rate (increases the period) over time.
Figure 11 shows the odometry staleness at navigation
command across the three schemes run on Map2. While
Static-FullRun and Catan both perform similarly on this
metric, Static-20s exhibits higher staleness towards the second
half of the run. By using under-estimated computation times
for the second half, Static-20s mis-allocates resources, giving
a smaller share of CPU time to GL, GM, and GP, and a more
than necessary amount of CPU resources to the local chain
nodes, NC, and YOLO. This reduces the effective throughput
of GL (in spite of priority-based stealing) and increases
odometry staleness.

All

three schemes performed similarly with respect
to avoiding collisions on Map1, and in terms of the area
exploration rate, and so we omit detailed results.

020406080100120140160180200220240Real Time (s)0123456789101112Yolo Output Period(s)CatanStatic-FullRunStatic-20s020406080100120140160180200220240260Time since exploration start (s)050100150200250300350400450500550600650700750800850Odometry Staleness at NC (ms)CatanStatic-FullRunStatic-20s020406080100120140160180200220240Real Time (s)012345678910Yolo Output Period(s)Catan 2CoresStatic-FullRun 2CoresStatic-20s 2Cores, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Adapting to change in resource availability. Our experi-
ments so far used a single core. We next ran the extended
navigation application (with the YOLO chain) on Map2 on a
system with two cores, to evaluate how well Catan auto-scales.
Baselines. We configured the system to use the same node exe-
cution rates as those computed by the two baselines above (i.e.
Static-FullRun and Static-20s with static schedules derived
from node computation times on Map2 on one core), relying on
default Linux policies to schedule the nodes across two cores.
Metrics. We compare the average output period for the YOLO
chain over time, as defined previously.
Results. Figure 12 plots the average output period for the
YOLO chain over time across Catan and the two baselines on
2 cores. We find that Catan is able to make efficient use of the
extra core – it dynamically allocates the core to just the local
chain in the first half, and the local chain and GP in the second
half. It is, thus, able to run YOLO at a higher rate (and lower
output period) than other static baselines that do not increase
the execution rates.
Key Takeaways. (i) Scheduling decisions impact how well
a robot can navigate an environment while avoiding collisions.
(ii) Catan achieves the desired trade-offs across different met-
rics. (iii) It is important to adapt scheduling decisions over time
and as resource availability changes – Catan is able to do so
by dynamically re-solving the schedule over time. (iv) It is im-
portant to adapt to sudden fine-grained variability in resource
usage – Catan is able to do so via priority based stealing.

7.3 Virtual Reality

Scheduler Configuration. We model the objective function as
a weighted sum of response times for various chains in the VR
DAG. We model IMU → Int as a subchain but do not explicitly
schedule it since it has negligible compute time and executes at
a high frequency. We configure Camera → VIO as a single sub-
chain (since only VIO uses Camera’s output). The remaining
nodes, Timewarp and Render are individual subchains. We use
a weight of 1 and 0.5 for the chains IMU → Int → Timewarp
and IMU → Int → Render → Timewarp respectively (these
chains are more important, corresponding to rotational and
translational motion-to-photon response time respectively).
For all the remaining (4) chains that go through VIO, we use a
weight of 0.005. We also add throughput constraints as per the
display refresh rate (vsync, determined by the display hardware
and driver): (i) Timewarp’s rate is upper and lower-bounded by
vsync. (ii) Render’s rate is upper-bounded by vsync. The order
of the subchains is specified to be Timewarp followed by Cam-
era → VIO, then Render, so that each render output is as close
to the next timewarp execution as possible. We enable priority-
based stealing only for timewarp, i.e. it is not preempted and
can steal CPU time from the other nodes if needed.
Experiment Setup. We use the ILLIXR system for our
VR application [13, 66]. As discussed in § 3.3, we use a
standard dataset for the camera and IMU inputs to ILLIXR
for repeatability. We use Vicon Room 1 Medium from

EuRoC [56], which consists of an 85 second long trajectory,
and provides camera images at 20 Hz and IMU readings at 200
Hz. The Render component renders a custom scene consisting
of a living room with furniture and textured walls. We use a
display with a resolution of 2560x1440, and a refresh rate of
120 Hz (vsync period of 8.33 ms). We run the CPU parts of
our setup on a single core of an Intel i9-10900K CPU clocked
at 5.3 GHz. The GPU parts execute on an NVIDIA GeForce
RTX 3090 with a core clock of 2.1 GHz.
Baselines. We compare Catan against two baselines. The first
baseline configures static rates (Timewarp and Render running
at vsync frequency and Camera running at the frequency
of the image dataset) and relies on Linux default policy
SCHED_OTHER for all other scheduling decisions. The
second baseline extends the first one, by using SCHED_FIFO
for all the nodes and prioritizing the execution of different
components in the following order: IMU and Int (so that they
run at the dataset’s high frequency), followed by TW, then
Render and then Camera and VIO (based on decreasing order
of their chains’ weights).
Metrics. We compare the response time along the chains (A)
IMU → Int → Timewarp (that captures rotational MTP), (B)
IMU → Int → Render → Timewarp (that captures transla-
tional MTP), and (C) Camera → VIO → Int → Timewarp
(that captures how quickly accurate pose-information is
available from VIO). For each scheme, we first average the
metrics within each run over time, and then depict the average
and standard deviation of these values across 20 runs.
Results. Figure 13 compares the two baselines with Catan.
We make the following observations: (i) The priority stealing
mechanism in Catan enables it to execute Timewarp just before
vsync in-spite of variations in computation times of different
nodes, leading to better response time by ≈7%. (ii) Catan’s
temporal CPU allocation to Render ensures that it finishes
execution as close to vsync as possible, leading to ≈25% im-
provement in chain B response time. (iii) Lastly, our system is
able to trigger Camera at a high rate,18 while continually adapt-
ing to varying compute times, without affecting the rest of the
system due to its explicitly planned schedule, leading to ≈11%
improvement in chain C. Overall, Catan is able to make the de-
sired trade-offs across different components in the application.

8 Related Work

System scheduling has a rich literature. We discuss three broad
(and most closely related) categories, highlighting their limi-
tations in addressing the requirements for sense-react systems.
Real-time scheduling. Prior work in real-time scheduling
for DAGs assumes static periodic schedules, and focus on
meeting latency constraints under given (and fixed) time
periods at which a node is triggered [53, 83, 98, 99] (including

18Even though the offline dataset has a fixed camera input rate, sending
triggers for camera at a higher rate reduces delay due to asynchrony between
offline camera inputs and triggers. We expect to see similar (and potentially
larger) benefits with live camera inputs.

On-Device CPU Scheduling for Sense-React Systems

, ,

9 Conclusion

In this work, we build an understanding of the scheduling
requirements and challenges for sense-react systems by
studying three applications spanning robotics and VR. Using
this understanding, we develop a scheduler to manage
on-device CPU resources for sense-react applications. We
show how it is able to handle multiple scheduling dimensions
across heterogeneous app nodes, adapt to variations in
compute resource usage at different timescales, and achieve
the specified semantic trade-offs.

We believe our work to be a first step in a rich problem
domain of sense-react system scheduling, with multiple
interesting questions open for future research. For instance,
Catan currently requires several inputs from the app devel-
opers, e.g. performance constraints and objectives along
different components of an application. While we believe
that specifying such high-level inputs is still significantly
easier than configuring low-level system knobs (as required
today), an important direction for future research includes
automatically inferring these inputs (e.g. via data-driven
approaches) to further ease app development.

As mentioned in §2, extending our scheduler (currently
restricted to managing CPU resources on a single platform) to
handle other resources (e.g. GPU, accelerators, etc), varying
deployment models (e.g. tasks offloaded to edge or cloud
servers) as well as different frameworks (e.g. ROS 2) is
another interesting future direction.

Acknowledgements

This work was supported by Intel, AG NIFA under grant
2021-67021-34418, the Applications Driving Architectures
(ADA) Center, a JUMP Center co-sponsored by SRC and
DARPA, the DARPA DSSOC program, and NSF under grants
2120464 and 2008971.

References

[1] Qualcomm Snapdragon XR2 5G Platform. https://www.qualcomm.

com/products/snapdragon-xr2-5g-platform.

[2] Anonymized link for supplementary proofs. https://drive.google.
com/file/d/1mnblouvU2UMb9fdQzEQszPRWPBUHv9CH/
view?usp=sharing.

[3] AWS Robomaker. https://aws.amazon.com/robomaker/.
[4] Clearpath Robotics. https://clearpathrobotics.com/.
[5] CORBA. https://www.corba.org/.
[6] da Vinci Si Surgical Robot.

https://med.nyu.edu/robotic-

surgery/physicians/what-robotic-surgery.

[7] Fetch Robotics. http://docs.fetchrobotics.com/FetchRobotics.pdf.
http://docs.fetchrobotics.com/
[8] Fetch Robotics Specification.

FetchRobotics.pdf.

[9] Gazebo. http://gazebosim.org/.
[10] Hansen Medical. https://www.aurishealth.com/hansen-medical.
https:
[11] How Drones Are Being Used to Battle Wildfires.

//www.smithsonianmag.com/videos/category/smithsonian-
channel/drones-are-now-being-used-to-battle-wildfires/.

[12] Htc vive pro. https://www.vive.com/us/product/vive-pro/.
[13] ILLIXR (Illinois Extended Reality

testbed) Consortium.

https://illixr.org.

Figure 13. Response time of the chains processing (A) approx.
rotational motion, (B) approx. translational motion, and (C) accurate
motion via VIO. The error bars show one standard deviation above
and below. These results are aggregated over 20 runs.

the few that look at robotics applications [58, 84, 93, 100]).
Davare et al. [60] focus on assigning per-node time periods
in automotive applications but under given node priorities and
core allocations. These works use expensive algorithms, and
do not account for variability in processing times.
Stream-processing systems. Many stream processing engines
make scheduling decisions locally at each node (which are
often suboptimal) [28, 46, 47, 72, 73, 95, 96]. Among those
that make system-wide decisions, most focus on resource al-
location or task scheduling to handle a given (though variable)
input load (e.g. [57, 67, 77, 85]). A few look at input load
shedding to avoid system overload, but do not simultaneously
optimize resource allocation and task ordering (e.g. [52, 91]).
More generally, such mechanisms are designed for a different
application domain and environment (e.g. query processing in
cloud clusters), and do not optimize for the application-level
goals in resource-constrained sense-react systems.
Sensor Nodes. Pixie [74] is an operating system for sensor
nodes, which provides a dataflow programming model with
abstractions for managing resources - but leaves all the
scheduling decisions upto the application. Flask [75] also uses
a dataflow model to define a Domain Specific Language for
sensor network applications, but does not handle variations
in compute usage or availability. While Eon [88] provides a
flexible interface to express how the scheduling parameters
can be adapted at runtime, it requires the developer to specify
how to change each parameter based on different resource
availability. More generally, none of the sensor node systems
deal with complex compute or fine grained variability.
Offline profiling. Some of the data-driven approaches rely
on extensive offline profiling to tune different aspects of the
system configuration (e.g. [51, 55, 64, 68, 101]). However,
such approaches cannot adapt to dynamic variations over time
and across deployment scenarios. Nonetheless, extending
such data-driven approaches to handle the requirements of
sense-react systems is an interesting future direction.

, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

[14] Introduction to mobile robotics - slam. http://ais.informatik.uni-

freiburg.de/teaching/ss12/robotics/slides/12-slam.pdf.

[15] LCM. https://lcm-proj.github.io/.
[16] Lessons learned building a self-driving car on ROS, Cruise Automation.
https://roscon.ros.org/2018/presentations/ROSCon2018_
LessonsLearnedSelfDriving.pdf.

[17] Linux Completely Fair Scheduler. https://www.kernel.org/doc/html/

latest/scheduler/sched-design-CFS.html.

[18] Linux CPU Scheduling - Limiting the CPU usage of

real-
https://man7.org/linux/man-

time and deadline processes.
pages/man7/sched.7.html.
[19] LoCoBot. http://www.locobot.org/.
[20] NVIDIA Isaac SDK. https://developer.nvidia.com/isaac-sdk.
[21] NVIDIA Jetson: Embedded Systems for Next-Generation Au-
tonomous Machines. https://www.nvidia.com/en-in/autonomous-
machines/embedded-systems/.

[22] Oculus Go. https://www.oculus.com/go/.
[23] Oculus Quest 2. https://www.oculus.com/quest-2/.
[24] Openxr. https://www.khronos.org/openxr/.
[25] OROCOS. http://www.orocos.org/.
[26] P3AT Robot.

https://www.generationrobots.com/media/

Pioneer3AT-P3AT-RevA-datasheet.pdf.

[27] Qualcomm Snapdragon 835 Mobile Platform.

https://www.

qualcomm.com/products/snapdragon-835-mobile-platform.

[28] RabbitMQ. https://www.rabbitmq.com/.
[29] Rapyuta Robotics. https://www.rapyuta-robotics.com/.
[30] ROS. https://www.ros.org/.
[31] ROS Communication Library. http://wiki.ros.org/ros_comm.
[32] ROS Face Tracking. http://wiki.ros.org/face_detection_tracking.
[33] ROS Navigation2D Application (Source: https://github.com/

skasperski/navigation_2d). http://wiki.ros.org/nav2d.

[34] ROS Usb cam : Driver for USB cameras. http://wiki.ros.org/usb_cam.
[35] ROSA Knee System. https://www.zimmerbiomet.com/medical-

professionals/knee/product/rosa-knee.html.

[36] ROS:Community Metrics Report.

http://download.ros.org/

downloads/metrics/metrics-report-2018-07.pdf.

[37] Stage Simulator. http://playerstage.sourceforge.net/doc/Stage-

3.2.1/.

[38] Terrasentia : The automated crop monitoring robot.

https:

//blog.plantwise.org/2018/07/17/terrasentia-the-automated-
crop-monitoring-robot/.

[39] Tesla Autopilot. https://www.tesla.com/autopilot.
[40] TurtleBot. https://www.turtlebot.com/.
[41] TurtleBot 3 .

https://emanual.robotis.com/docs/en/platform/

turtlebot3/features/#specifications.

[42] Turtlebot3 Robot Actuators. https://emanual.robotis.com/docs/

en/platform/turtlebot3/specifications/#actuators.

Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of
embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018.
[51] J. Ansel, S. Kamil, K. Veeramachaneni, J. Ragan-Kelley, J. Bosboom,
U. O’Reilly, and S. Amarasinghe. Opentuner: An extensible framework
for program autotuning. In 2014 23rd International Conference on
Parallel Architecture and Compilation Techniques (PACT), pages
303–315, 2014.

[52] Brian Babcock, Mayur Datar, and Rajeev Motwani. Load shedding
for aggregation queries over data streams. In ICDE ’04: Proceedings
of the 20th International Conference on Data Engineering, 2004.
[53] S. Baruah, V. Bonifaci, A. Marchetti-Spaccamela, L. Stougie, and
A. Wiese. A generalized parallel task model for recurrent real-time
processes. In 2012 IEEE 33rd Real-Time Systems Symposium, pages
63–72, 2012.

[54] Raunak P. Bhattacharyya, Derek J. Phillips, Changliu Liu, Jayesh K.
Gupta, Katherine Driggs-Campbell, and Mykel J. Kochenderfer.
Simulating emergent properties of human driving behavior using multi-
agent reward augmented imitation learning. In 2019 International
Conference on Robotics and Automation (ICRA), pages 789–795, 2019.
[55] Muhammad Bilal and Marco Canini. Towards automatic parameter
In Proceedings of the 2017
tuning of stream processing systems.
Symposium on Cloud Computing, SoCC ’17, page 189–200, New York,
NY, USA, 2017. Association for Computing Machinery.

[56] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern
Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart.
The EuRoC micro aerial vehicle datasets. The International Journal
of Robotics Research, 35(10):1157–1163, 2016.

[57] Don Carney, U˘gur Çetintemel, Alex Rasin, Stan Zdonik, Mitch
Cherniack, and Mike Stonebraker. Operator scheduling in a data
stream manager. In Proceedings of the 29th International Conference
on Very Large Data Bases - Volume 29, VLDB ’03, page 838–849.
VLDB Endowment, 2003.

[58] Hyun-Seon Choi, Yecheng Xiang, and Hyoseung Kim. Picas: New
design of priority-driven chain-aware scheduling for ros2. 2021
IEEE 27th Real-Time and Embedded Technology and Applications
Symposium (RTAS), pages 251–263, 2021.

[59] G. G. Chrysos, E. Antonakos, S. Zafeiriou, and P. Snape. Offline de-
formable face tracking in arbitrary videos. In 2015 IEEE International
Conference on Computer Vision Workshop (ICCVW), pages 954–962,
2015.

[60] Abhijit Davare, Qi Zhu, Marco Di Natale, Claudio Pinello, Sri Kanajan,
and Alberto Sangiovanni-Vincentelli. Period optimization for hard
real-time distributed automotive systems. In Proceedings of the 44th
annual Design Automation Conference, pages 278–283, 2007.
[61] Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and
Andrew Zisserman. The pascal visual object classes (voc) challenge.
Int. J. Comput. Vision, 88(2):303–338, June 2010.

[43] Turtlebot3 Robot Actuators Specifications. https://emanual.robotis.

[62] P. Fitzpatrick, E. Ceseracciu, D. Domenichelli, A. Paikan, G. Metta,

com/docs/en/dxl/x/xm430-w210/.

[44] Turtlebot3 Waffle Pi. https://www.robotis.us/turtlebot-3-waffle-pi/.
[45] Waymo. https://waymo.com/.
[46] ZeroMQ. http://zeromq.org/.
[47] Daniel J. Abadi, Yanif Ahmad, Magdalena Balazinska, Ugur
Cetintemel, Mitch Cherniack, Jeong-Hyon Hwang, Wolfgang Lindner,
Anurag S. Maskey, Alexander Rasin, Esther Ryvkina, Nesime Tatbul,
Ying Xing, and Stan Zdonik. The Design of the Borealis Stream
Processing Engine. In CIDR 2005.

[48] R. Aggarwal, T. P. Grantcharov, J. R. Eriksen, D. Blirup, V. B.
Kristiansen, P. Funch-Jensen, and A. Darzi. An evidence-based virtual
reality training program for novice laparoscopic surgeons. Ann Surg,
244(2):310–314, Aug 2006.

[49] Amazon Robotics. https://www.amazonrobotics.com/#/.
[50] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey
Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra

and L. Natale. A middle way for robotics middleware, 2014.

[63] Patrick Goebel. ROS By Example. Lulu, 2013.
[64] Herodotos Herodotou, Fei Dong, and Shivnath Babu. No one (cluster)
size fits all: Automatic cluster sizing for data-intensive analytics. In
Proceedings of the 2nd ACM Symposium on Cloud Computing, SOCC
’11, New York, NY, USA, 2011. Association for Computing Machinery.
[65] Guoqiang Hu, Wee Peng Tay, and Yonggang Wen. Cloud robotics:
architecture, challenges and applications. IEEE network, 2012.
[66] Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang,
Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn
Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V.
Adve. Exploring extended reality with ILLIXR: A new playground
for architecture research. In 2021 IEEE International Symposium on
Workload Characterization (To Appear). A prior version of this work
appears in CoRR abs/2004.04643, March 2021.

On-Device CPU Scheduling for Sense-React Systems

, ,

[67] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis
Fetterly. Dryad: Distributed data-parallel programs from sequential
building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys Euro-
pean Conference on Computer Systems 2007, EuroSys ’07, page 59–72,
New York, NY, USA, 2007. Association for Computing Machinery.

[68] P. Jamshidi and G. Casale. An uncertainty-aware approach to optimal
configuration of stream processing systems. In 2016 IEEE 24th Interna-
tional Symposium on Modeling, Analysis and Simulation of Computer
and Telecommunication Systems (MASCOTS), pages 39–48, 2016.
[69] Ben Kehoe, Sachin Patil, Pieter Abbeel, and Ken Goldberg. A survey
of research on cloud robotics and automation. IEEE Transactions on
automation science and engineering, 2015.
[70] KiwiBot. https://www.kiwicampus.com/.
[71] Stephan Krohn, Johanne Tromp, Eva M Quinque, Julia Belger, Felix
Klotzsche, Sophia Rekers, Paul Chojecki, Jeroen de Mooij, Mert
Akbal, Cade McCall, Arno Villringer, Michael Gaebler, Carsten Finke,
and Angelika Thöne-Otto. Multidimensional evaluation of virtual
reality paradigms in clinical neuropsychology: Application of the
vr-check framework. J Med Internet Res, 22(4):e16724, Apr 2020.
[72] Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli,
Christopher Kellogg, Sailesh Mittal, Jignesh M Patel, Karthik
Ramasamy, and Siddarth Taneja. Twitter heron: Stream processing
at scale. In Proceedings of the 2015 ACM SIGMOD International
Conference on Management of Data, pages 239–250, 2015.

[73] Wei Lin, Zhengping Qian, Junwei Xu, Sen Yang, Jingren Zhou, and
Lidong Zhou. Streamscope: Continuous reliable distributed processing
of big data streams. In 13th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 16). USENIX Association, 2016.

[74] Konrad Lorincz, Bor-rong Chen, Jason Waterman, Geoff Werner-Allen,
and Matt Welsh. Resource aware programming in the pixie os. In
Proceedings of the 6th ACM Conference on Embedded Network Sensor
Systems, SenSys ’08, page 211–224, New York, NY, USA, 2008.
Association for Computing Machinery.

[75] Geoffrey Mainland, Greg Morrisett, and Matt Welsh. Flask: Staged
In Proceedings of
functional programming for sensor networks.
the 13th ACM SIGPLAN International Conference on Functional
Programming, ICFP ’08, page 335–346, New York, NY, USA, 2008.
Association for Computing Machinery.

[76] Morgan McGuire. “exclusive: How nvidia research is reinventing the

display pipeline for the future of vr, part 1. 2017.

[77] Derek G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard,
Paul Barham, and Martín Abadi. Naiad: A timely dataflow system.
In Proceedings of the Twenty-Fourth ACM Symposium on Operating
Systems Principles, SOSP ’13, page 439–455, New York, NY, USA,
2013. Association for Computing Machinery.

[78] Michael Jones Paul Viola. Rapid object detection using a boosted
cascade of simple features. In Proceedings of the 2001 IEEE computer
society conference on computer vision and pattern recognition. CVPR
2001, 2001.

[79] Morgan Quigley, Ken Conley, Brian P Gerkey, Josh Faust, Tully Foote,
Jeremy Leibs, Rob Wheeler, and Andrew Y Ng. Ros: an open-source
robot operating system. 2009.

[80] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You
only look once: Unified, real-time object detection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.

[81] Roomba. https://www.irobot.com/roomba.
[82] Stuart Russell and Peter Norvig. Artificial intelligence: a modern
approach 4th edition. 2020. http://aima.cs.berkeley.edu/index.html.
[83] A. Saifullah, K. Agrawal, C. Lu, and C. Gill. Multi-core real-time
scheduling for generalized parallel task models. In 2011 IEEE 32nd
Real-Time Systems Symposium, pages 217–226, 2011.

[84] Y. Saito, F. Sato, T. Azumi, S. Kato, and N. Nishio. Rosch:real-time
In 2018 IEEE 24th International

scheduling framework for ros.

Conference on Embedded and Real-Time Computing Systems and
Applications (RTCSA), pages 52–58, 2018.

[85] M. J. Sax, M. Castellanos, Q. Chen, and M. Hsu. Performance
optimization for distributed intra-node-parallel streaming systems.
In 2013 IEEE 29th International Conference on Data Engineering
Workshops (ICDEW), pages 62–69, 2013.

[86] Wolfram Burgard Sebastian Thrun and Dieter Fox. Probabilistic Robot-
ics. MIT Press, 2005. https://mitpress.mit.edu/books/probabilistic-
robotics.

[87] J. Shen, S. Zafeiriou, G. G. Chrysos, J. Kossaifi, G. Tzimiropoulos,
and M. Pantic. The first facial landmark tracking in-the-wild challenge:
Benchmark and results. In 2015 IEEE International Conference on
Computer Vision Workshop (ICCVW), pages 1003–1011, 2015.
[88] Jacob Sorber, Alexander Kostadinov, Matthew Garber, Matthew
Brennan, Mark D. Corner, and Emery D. Berger. Eon: A language
and runtime system for perpetual systems. In Proceedings of the 5th
International Conference on Embedded Networked Sensor Systems,
SenSys ’07, page 161–174, New York, NY, USA, 2007. Association
for Computing Machinery.

[89] Yuhei Suzuki, Takuya Azumi, Shinpei Kato, and Nobuhiko Nishio.
Real-time ros extension on transparent cpu/gpu coordination mech-
2018 IEEE 21st International Symposium on Real-Time
anism.
Distributed Computing (ISORC), pages 184–192, 2018.

[90] Ajay Kumar Tanwani, Nitesh Mor, John Kubiatowicz, Joseph E.
Gonzalez, and Ken Goldberg. A fog robotics approach to deep robot
learning: Application to object recognition and grasp planning in
surface decluttering. CoRR, abs/1903.09589, 2019.

[91] Nesime Tatbul, U˘gur Çetintemel, Stan Zdonik, Mitch Cherniack, and
Michael Stonebraker. Load shedding in a data stream manager. In Pro-
ceedings of the 29th International Conference on Very Large Data Bases
- Volume 29, VLDB ’03, page 309–320. VLDB Endowment, 2003.
[92] J.M.P. van Waveren. The asynchronous time warp for virtual reality
on consumer hardware. In Proceedings of the 22nd ACM Conference
on Virtual Reality Software and Technology, pages 37–46. ACM, 2016.
[93] Micaela Verucchi, Mirco Theile, Marco Caccamo, and Marko Bertogna.
Latency-aware generation of single-rate dags from multi-rate task sets.
In 2020 IEEE Real-Time and Embedded Technology and Applications
Symposium (RTAS), pages 226–238. IEEE, 2020.

[94] Daniel Wagner. Motion to Photon Latency in Mobile AR and VR, 2018.
[95] Matt Welsh and David Culler. Adaptive overload control for busy
internet servers. In Proceedings of the 4th conference on USENIX
Symposium on Internet Technologies and Systems, 2003.

[96] Matt Welsh, David Culler, and Eric Brewer. Seda: An architecture
for well-conditioned, scalable internet services. In Proceedings of
the Eighteenth ACM Symposium on Operating Systems Principles,
SOSP ’01, page 230–243, New York, NY, USA, 2001. Association
for Computing Machinery.

A frontier-based approach for autonomous
[97] Brian Yamauchi.
In Proceedings 1997 IEEE International Sympo-
exploration.
sium on Computational Intelligence in Robotics and Automation
CIRA’97.’Towards New Computational Principles for Robotics and
Automation’, pages 146–151. IEEE, 1997.

[98] M. Yang, T. Amert, K. Yang, N. Otterness, J. H. Anderson, F. D.
Smith, and S. Wang. Making openvx really "real time". In 2018 IEEE
Real-Time Systems Symposium (RTSS), pages 80–93, 2018.

[99] Y. Yang, A. Pinto, A. Sangiovanni-Vincentelli, and Q. Zhu. A design
flow for building automation and control systems. In 2010 31st IEEE
Real-Time Systems Symposium, pages 105–116, 2010.

[100] Yuqing Yang and Takuya Azumi. Exploring real-time executor on ros
2. In 2020 IEEE International Conference on Embedded Software and
Systems (ICESS), pages 1–8, 2020.

[101] Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili-
pose, Paramvir Bahl, and Michael J. Freedman. Live video analytics
at scale with approximation and delay-tolerance. In 14th USENIX

, ,

Aditi Partap, Samuel Grayson, Muhammad Huzaifa, Sarita Adve, Brighten Godfrey, Saurabh Gupta, Kris Hauser, and Radhika Mittal

Symposium on Networked Systems Design and Implementation (NSDI

17). USENIX Association, 2017.

