2
2
0
2

n
a
J

2
1

]

O
R
.
s
c
[

1
v
2
4
7
4
0
.
1
0
2
2
:
v
i
X
r
a

nuReality: A VR environment for research of
pedestrian and autonomous vehicle interactions

Paul Schmitt, Nicholas Britten, JiHyun Jeong, Amelia Coﬀey,
Kevin Clark, Shweta Sunil Kothawade, Elena Corina Grigore,
Adam Khaw, Christopher Konopka, Linh Pham, Kim Ryan,
Christopher Schmitt, Aryaman Pandya, Emilio Frazzoli

December 10, 2021

Abstract

We present nuReality, a virtual reality “VR” en-
vironment designed to test the eﬃcacy of vehicu-
lar behaviors to communicate intent during inter-
actions between autonomous vehicles “AVs” and
pedestrians at urban intersections. In this project
we focus on expressive behaviors as a means for
pedestrians to readily recognize the underlying in-
tent of the AV’s movements. VR is an ideal tool
to use to test these situations as it can be im-
mersive and place subjects into these potentially
dangerous scenarios without risk. nuReality pro-
vides a novel and immersive virtual reality en-
vironment that includes numerous visual details
(road and building texturing, parked cars, swaying
tree limbs) as well as auditory details (birds chirp-
ing, cars honking in the distance, people talking).
In these ﬁles we present the nuReality environ-
ment, its 10 unique vehicle behavior scenarios, and
the Unreal Engine and Autodesk Maya source ﬁles
for each scenario. The ﬁles are publicly released
as open source at www.nuReality.org, to support
the academic community studying the critical AV-
pedestrian interaction.

1

Introduction

Motional sees eﬀective communication between
AVs and pedestrians as a key challenge to ac-
ceptance and adoption within society. In order

This research was supported by Motional, Boston,

MA 02210, USA.

Please direct all

correspondence

to nuReal-

ity@motional.com

for a pedestrian to feel safe, the communica-
tion between the pedestrian and AV should be
simple and and easy to understand. Ideally the
AV would clearly and intuitively indicate its
intentions, as in this environment, to stop and
let a pedestrian cross the road. In order to be
deemed intuitive, the communication should
follow patterns that familiar to the pedestrian.
In this way, pedestrians across diﬀerent com-
munities would not need to pause, view, and
learn a new set of signals.

In these ﬁles we present nuReality, a VR
environment designed to address some of the
challenges we encountered with existing sim-
ulation tools to test the eﬃcacy of vehicular
behaviors to communicate intent during inter-
actions between AVs and pedestrians at urban
intersections. nuReality was designed speciﬁ-
cally to recreate a life-like visualization from
the perspective of a pedestrian attempting to
cross the street. Figure 1 shows a panoramic
view of the environment. The following sec-
tions detail the contents of nuReality and how
they can be leveraged to support AV research.

2 Content of the Files

The virtual reality environment is modeled af-
ter an urban 4-way intersection. There is no
clear pedestrian crossing zone (zebra stripes),
no stop signs, no stoplights, or other elements
to indicate that the car lawfully would have
to come to a complete stop. While there is

1

 
 
 
 
 
 
Figure 1: Panorama of the virtual environment from the ﬁrst-person perspective of a pedestrian
about to cross the street as an AV approaches

a ﬂashing yellow light above the intersection
and a yellow pedestrian warning sign can be
seen on the diagonally opposite street corner,
these traﬃc signals are only cautionary and
the pedestrian is uncertain if the vehicle will
stop and allow them to cross the street. Sig-
niﬁcant care was taken in creating a realis-
tic environment, as enhanced “place illusion”
and “plausibility” would elicit higher ﬁdelity of
participant reactions Slater [1]. Numerous vi-
sual details (including but not limited to road
and building texturing, parked cars and sway-
ing tree limbs) as well as auditory details (birds
chirping, cars, people talking) were included
in order to achieve spatial presence within the
virtual environment Wirth [2]. All scenarios
were crafted initially using Autodesk Maya,
brought into Unreal Engine for additional edit-
ing, and then compiled into new applications.
Two vehicle models are present in the ﬁles:
a conventional human driven vehicle and a novel
Autonomous Vehicle without a human opera-
tor. Both vehicles were modeled after a white
2019 Chrysler Paciﬁca, however the AV model
has some notable diﬀerences. The AV model
includes both side-mirror and roof mounted li-
dar and radar sensors and has no visible oc-
cupants. The human driven model includes a
male driver who looks straight ahead and re-
mains motionless during the interaction.

The following brieﬂy describes each of the
scenarios available within the nuReality ﬁles.
• Orientation: No vehicle. Allows the user
the get familiarized to the environment

2

• Practice: Human driver baseline vehi-
cle. Followed a deceleration curve modeled
after human drivers stopping at a stop con-
trolled intersection in a natural setting
• Scene 1: Human driver baseline vehi-
cle. Followed a deceleration curve mod-
eled after human drivers stopping at a stop-
controlled intersection in a natural setting.
See Figure 2

• Scene 2: AV baseline. Followed the same
deceleration curve as the Practice scenario
and Scene 1. The vehicle is equipped with
the AV sensor suite. See Figure 3

• Scene 3: AV with a Light Bar. As the
vehicle is yielding, a light bar mounted at
the top of the windshield uses cyan colored
LEDs to display a sweeping motion from the
center to the edge bar. See Figure 4

• Scene 4: AV with Expressive Deceler-
ation. The behavior follows the animation
principles of exaggeration and ease in/ ease
out to make the deceleration less robotic.
The AV exaggerates its motion by initiat-
ing the braking sequence 10 meters earlier
and achieving a peak deceleration value 10%
greater than the baseline scenario.

• Scene 5: AV Stops Short The AV yields
5 meters farther away from the pedestrian
than in the other conditions, coming to an
early stop. See Figure 5

• Scene 6: AV uses Explicit Sound. Exag-
gerated brake noises and low engine RPM
sounds that increases in volume as the car
approaches closer to the pedestrian were ad-
ded to this scenario

• Scene 7: AV uses Expressive Nose Dive
and Explicit Sound. As the vehicle comes
to a stop the nose is dipped down 1 inch
the tail raises 1 inch before leveling out.
This action is combined with the exagger-
ated sounds from Scene 6

• Scene 8: Human Driver Doesn’t Stop.
Appears to slow as it approaches the inter-
section but continues through without stop-
ping

• Scene 9: AV Doesn’t Stop. Appears to
slow as it approaches the intersection but
continues through without stopping

• Scene 10: AV with Expressive Nose Dive.
As the vehicle comes to a stop the nose is
dipped down 1 inch the tail raises 1 inch
before leveling out

Figure 2: Traditional vehicle with a human in
the driver’s seat

Figure 3: Autonomous vehicle with visible LI-
DAR sensors and no human driver

Figure 4: Autonomous vehicle with cyan LED
lights that move in a sweeping motion

Figure 5: Autonomous vehicle stops one car’s
length farther away from the pedestrian’s
crossing point

3 How nuReality Can Sup-

port AV Research

The nuReality ﬁles are designed as a base to
support experimentation and further research
into expressive robotics, autonomous vehicles,
pedestrian interactions, and related areas. Fur-
ther, they are designed to overcome some of
the challenges of existing simulation tools. First,
nuReality was designed in Virtual Reality over
other 3D alternatives in order to give users a
truly immersive experience. A few advantages
of nuReality over commonly used AV simula-
tion tools are realism and HRI tailoring. Since
our main goal was to foster high-ﬁdelity reac-
tions from trial participants, we worked with
a VR studio to create a life-like environment.
As encounters with AVs become more fre-
quent in people’s everyday lives, challenges of
communication and inferring intention must

3

be solved before driverless vehicles’ beneﬁts
of safety and accessibility can be appreciated.
Trust and understanding in human-AV inter-
actions is a precursor to widespread acceptance
and adoption.

These challenges are bigger than a single
company. The larger research community is
instrumental to uncovering solutions for pedes-
trian interactions with driverless vehicles. That
is why Motional has decided to use a VR plat-
form and make these ﬁles open source. Over
the past few years VR technology and sys-
tems have become more commercially avail-
able. The beneﬁts of using VR technology lie
not only in experimental control, reproducibil-
ity, and ecological validity; but also in acces-
sibility Xueni [3]. While VR has been used in
AV-pedestrian research before Schmidt et al.
[4], we hope that nuReality will act as a cata-
lyst in its adoption as an accessible means of
experimentation.

The contents of nuReality are as described
in section 3, however we leave it up to the
broader research community to build upon that
foundation. Access to the Maya and Unreal
source ﬁles gives researchers the ability to mod-
ify the given scenarios as well as the overar-
ching environment. This opens up numerous
possibilities including but not limited to de-
veloping scenarios with novel expressive be-
haviors, ﬁne tuning the existing scenarios, and
changing the environment to better suit diﬀer-
ent regions of the world.

Figure 6: Example of a participant using the
VR equipment while receiving instructions

4

4 Related Work

Expressive (or Predictable, Legible) Robotics
Dragan [5] is an exciting area of research that
leverages principles from a variety of ﬁelds, ex.
human factors Norman [6] Major and Shah [7]
and animation Coron [8], to generate behav-
iors that enable an observer to quickly and
conﬁdently infer correctly the vehicle’s goal or
to match behaviors to what an observer would
expect from a human driver.

Our work builds on the contributions from
a small yet important community of researchers
working on the challenges around AV commu-
nication. Anca Dragan’s Legible Robotics Dra
[9] and Andrea Thomaz’ Socially Intelligent
Robots Tho [10] have been applied success-
fully to humanoid and mobile robots. Mean-
while, research such as InterACT int [11] and
work at the University of Leeds UL [12] and
Technische ¨Unive-rsit¨at M¨unichen TUM [13],
is advancing our understanding of pedestrians’
behaviors, needs, feelings, and mental ﬂows.
Solutions involving diﬀerent technologies are
also arising in order to ﬁll in this communica-
tions gap. Much research has been performed
on electronic Human Machine Interfaces (e-
HMIs) to enable communication between the
AV and pedestrian Fridman et al. [14]. Many
solutions, such as the Ford/VTTI light bar
Shutko [15] and Volvo 360 light ring Vol [16],
utilize a combination of light patterns to com-
municate the vehicle’s intent. Others have also
experimented with using sounds and simulated
eyes Jag [17].

The scenarios in these ﬁles were designed
speciﬁcally based on the principles formulated
by this sphere of research.

5 Conclusion and Outlook

We present nuReality, a virtual reality (VR)
environment designed to test the eﬃcacy of ex-
pressive behaviors to communicate vehicle in-
tentions during interactions between AVs and
pedestrians at urban intersections. We focus,
in this project, on expressive behaviors as a
means for pedestrians to readily recognize the
underlying intent of an AV’s movements. The

result of these ﬁles is a base oﬀ of which oth-
ers can continue testing and building that will
push research towards developing future gener-
ations of AVs that can eﬀectively communicate
with humans in complex situations. This com-
munication will build trust between humans
and AVs, help humans be more conﬁdent in
their decision making around AV interactions,
and make AV-human interactions safer.

References

[1] M. Slater, “Place illusion and plausibility
can lead to realistic behaviour in immer-
sive virtual environments,” Phil. Trans.
R. Soc., vol. 364, p. 3549–3557, 2009.

[2] W. Wirth, “A process model of the for-
mation of spatial presence experiences,”
Media Psychology, vol. 9, no. 3, pp. 493–
525, 2007.

[3] P. Xueni, “Why and how to use vir-
tual reality to study human social inter-
action: The challenges of exploring a new
research landscape,” British Journal of
Psychology, vol. 109, no. 3, pp. 395–417,
2018.

[4] H. Schmidt, J. Terwilliger, D. Aladawy,
and L. Fridman, “Hacking nonverbal
communication between pedestrians and
vehicles in virtual reality,” CoRR, vol.
abs/1904.01931, 2019.
[Online]. Avail-
able: http://arxiv.org/abs/1904.01931

[5] A. Dragan. (2013) Legibility and pre-
dictability of
[Online].
Available: https://ieeexplore.ieee.org/ab
stract/document/6483603

robot motion.

[6] D. Norman, The Design of Everyday
New York, New York: Basic

Things.
Books, 2013.

[7] L. Major and J. Shah, What To Ex-
pect When You’re Expecting Robots: The
Future of Human-Robot Collaboration.
Cambridge, MA: Basic Books, 2020.

5

[8] T. Coron. (2021) Understand disney’s
12 principles of animation.
[Online].
Available: https://www.creativebloq.c
om/advice/understand-the-12-principles
-of-animation

[9] (2017) Anca dragan. [Online]. Available:
http://people.eecs.berkeley.edu/∼anca/
publications.html

[10] Social intelligent machines lab. [Online].
Available: https://www.cc.gatech.edu/so
cial-machines/publications.html

[11] interact project. [Online]. Available: http
s://www.interact-roadautomation.eu/

[12] University of

leeds driving simulator.
[Online]. Available: https://uolds.leeds.
ac.uk/

[13] (2015) Networked traﬃc:

develop

Tum re-
simu-
searchers
lator.
https:
//www.tum.de/nc/en/about-tum/new
s/press-releases/details/32625/

[Online]. Available:

pedestrian

[14] L. Fridman, B. Mehler,

L. Xia,
Y. Yang, L. Y. Facusse, and B. Reimer,
Crowd-
“To walk or not
sourced assessment of external vehicle-
to-pedestrian displays,” CoRR,
vol.
abs/1707.02698, 2017.
[Online]. Avail-
able: http://arxiv.org/abs/1707.02698

to walk:

[15] J. Shutko.

(2018) Seeing the

light:
Our call for a standard self-driving car
language to communicate intent. [Online].
Available: https://medium.com/self-driv
en/seeing-the-light-our-call-for-a-stand
ard-self-driving-car-language-to-commu
nicate-intent-3f3628cc7b2

[16] Self driving road trip. [Online]. Available:
https://www.volvocars.com/us/cars/co
ncepts/360c?redirect=true

[17] The virtual

[Online].
eyes have
Available: https://www.jaguarlandrover.
com/2018/virtual-eyes-have-it

it.

