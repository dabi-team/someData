2
2
0
2

p
e
S
9
2

]

G
L
.
s
c
[

3
v
9
7
7
3
1
.
4
0
2
2
:
v
i
X
r
a

Formulating Robustness Against Unforeseen Attacks

Sihui Dai
Princeton University
sihuid@princeton.edu

Saeed Mahloujifar
Princeton University
sfar@princeton.edu

Prateek Mittal
Princeton University
pmittal@princeton.edu

Abstract

Existing defenses against adversarial examples such as adversarial training typi-
cally assume that the adversary will conform to a speciﬁc or known threat model,
such as (cid:96)p perturbations within a ﬁxed budget. In this paper, we focus on the
scenario where there is a mismatch in the threat model assumed by the defense
during training, and the actual capabilities of the adversary at test time. We ask the
question: if the learner trains against a speciﬁc “source" threat model, when can
we expect robustness to generalize to a stronger unknown “target" threat model
during test-time? Our key contribution is to formally deﬁne the problem of learning
and generalization with an unforeseen adversary, which helps us reason about the
increase in adversarial risk from the conventional perspective of a known adversary.
Applying our framework, we derive a generalization bound which relates the gen-
eralization gap between source and target threat models to variation of the feature
extractor, which measures the expected maximum difference between extracted fea-
tures across a given threat model. Based on our generalization bound, we propose
variation regularization (VR) which reduces variation of the feature extractor across
the source threat model during training. We empirically demonstrate that using
VR can lead to improved generalization to unforeseen attacks during test-time, and
combining VR with perceptual adversarial training (Laidlaw et al., 2021) achieves
state-of-the-art robustness on unforeseen attacks. Our code is publicly available at
https://github.com/inspire-group/variation-regularization.

1

Introduction

Neural networks have impressive performance on a variety of datasets (LeCun et al., 1998; He
et al., 2015; Krizhevsky et al., 2017; Everingham et al., 2010) but can be fooled by imperceptible
perturbations known as adversarial examples (Szegedy et al., 2014). The conventional paradigm to
mitigate this threat often assumes that the adversary generates these examples using some known
threat model, primarily (cid:96)p balls of speciﬁc radius (Cohen et al., 2019; Zhang et al., 2020b; Madry
et al., 2018), and evaluates the performance of defenses based on this assumption. This assumption,
however, is unrealistic in practice. In general, the learner does not know exactly what perturbations
the adversary will apply during test-time.

To bridge the gap between the setting of robustness studied in current adversarial ML research and
robustness in practice, we study the problem of learning with an unforeseen adversary. In this
problem, the learner has access to adversarial examples from a proxy “source" threat model but wants
to be robust against a more difﬁcult “target" threat model used by the adversary during test-time. We
ask the following questions:

1. When can we expect robustness on the source threat model to generalize to the true unknown

target threat model used by the adversary?

2. How can we design a learning algorithm that reduces the drop in robustness from source

threat model to target threat model?

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
To address the ﬁrst question, we introduce unforeseen adversarial generalizability which provides a
framework for reasoning about what types of learning algorithms produce models that generalize
well to unforeseen attacks. Based on this framework, we derive a generalization bound which relates
the difference in adversarial risk across source and target threat models to a quantity we call variation:
the expected maximum difference between extracted features across a given threat model.

Our bound addresses the second question; it suggests that learning algorithms that bias towards
models with small variation across the source threat model exhibit smaller drop in robustness to
particular unforeseen attacks. Thus, we propose variation regularization (VR) to improve robustness
to unforeseen attacks. We then empirically demonstrate that when combined with adversarial training,
VR improves generalization to unforeseen attacks during test-time across multiple datasets and
architectures. Our contributions are as follows:

We formally deﬁne the problem of learning with an unforeseen adversary with respect to ad-
versarial risk. We make the case that one way of learning with an unforeseen adversary is to ensure
that the gap between the empirical adversarial risk measured on the source adversary and the expected
adversarial risk on the target adversary remains small. To this end, we deﬁne unforeseen adversarial
generalizability which provides a framework for understanding under what conditions we would
expect small generalization gap.

Under our framework for generalizability, we derive a generalization bound for generalization
across threat models. Our bound relates the generalization gap to a quantity we deﬁne as variation,
the expected maximum difference between extracted features across a given threat model. We
demonstrate that under certain conditions, we can decrease this upper bound while only using
information about the source threat model.

Using our bound, we propose a regularization term which we call variation regularization
(VR). We incorporate this regularization term into adversarial training and perceptual adversarial
training (Laidlaw et al., 2021), leading to learning algorithm that we call AT-VR and PAT-VR
respectively. We ﬁnd that VR can lead to improved robustness on unforeseen attacks across datasets
such as CIFAR-10, CIFAR-100, and ImageNette over adversarial training without VR. Additionally,
PAT-VR achieves state-of-the-art (SOTA) robust accuracy on LPIPS-based attacks, improving over
PAT by 21% and SOTA robust accuracy on a union of (cid:96)∞, (cid:96)2, spatially transformed (Xiao et al.,
2018), and recolor attacks (Laidlaw & Feizi, 2019).

2 Related Works

Adversarial examples and defenses Previous studies have shown that neural networks can be fooled
by perturbations known as adversarial examples, which are imperceptible to humans but cause NNs
to predict incorrectly with high conﬁdence (Szegedy et al., 2014). These adversarial examples can be
generated by various threat models including (cid:96)p perturbations, spatial transformations (Xiao et al.,
2018), recoloring (Laidlaw & Feizi, 2019), and broader threat models such as fog and snow distortions
(Kang et al., 2019). While many defenses have been proposed, most defenses provide guarantees for
speciﬁc threat models (Cohen et al., 2019; Zhang et al., 2020b; Croce & Hein, 2020a; Yang et al.,
2020; Zhang et al., 2020a) or use knowledge of the threat model during training (Madry et al., 2018;
Zhang et al., 2019; Wu et al., 2020a; Tramèr & Boneh, 2019; Maini et al., 2020). Adversarial training
is a popular defense framework in which a model is trained using adversarial examples generated
by a particular threat model, such as (cid:96)2 or (cid:96)∞ attacks (Madry et al., 2018; Zhang et al., 2019; Wu
et al., 2020a). Prior works have also extended adversarial training to defend against unions of attack
types such as unions of (cid:96)p-balls (Maini et al., 2020; Tramèr & Boneh, 2019) and against stronger
adversaries more aligned to human perception (Laidlaw et al., 2021).

Bounds for Learning with Adversarial Examples An interesting body of work studies generaliza-
tion bounds for speciﬁc attacks Cullina et al. (2018); Attias et al. (2019); Montasser et al. (2019);
Raghunathan et al. (2019); Chen et al. (2020); Diakonikolas et al. (2019); Yu et al. (2021); Diochnos
et al. (2019). In particular, they study generalization in the setting where the learning algorithm
minimizes the adversarial risk on the training set and hopes to generalize to same adversary during
test-time. Montasser et al. (2021) provide bounds for the problem of generalizing to an unknown
adversary with oracle access to that adversary during training. Our work differs since we study
generalization and provide bounds under the setting in the learner only has access to samples from a
weaker adversary than present at test-time.

2

“Unforeseen" attacks and defenses While several prior works have studied “unforeseen" attacks
(Kang et al., 2019; Stutz et al., 2020; Laidlaw et al., 2021; Jin & Rinard, 2020), these works are
empirical works, and the term “unforeseen attack" has not been formally deﬁned. Kang et al. (2019)
ﬁrst used the term “unforeseen attack" when proposing a set of adversarial threat models including
Snow, Fog, Gabor, and JPEG to evaluate how well defenses can generalize from (cid:96)∞ and (cid:96)2 to broader
threat models. Stutz et al. (2020) and Chen et al. (2022) propose adversarial training based techniques
with a mechanism for abstaining on certain inputs to improve generalization from training on (cid:96)∞ to
stronger attacks including (cid:96)p attacks of larger norm. Other defenses against “unforeseen attacks"
consider them to be attacks that are not used during training, but not necessarily stronger than those
used in training. For instance, Laidlaw et al. (2021) propose using LPIPS (Zhang et al., 2018), a
perceptually aligned image distance metric, to generate adversarial examples during training. They
demonstrate that by training using adversarial examples using this distance metric, they can achieve
robustness against a variety of adversarial threat models including (cid:96)∞, (cid:96)2, recoloring, and spatial
transformation. However, the LPIPS attack is the strongest out of all threat models tested and contains
a large portion of those threat models. To resolve these differences in interpretation of “unforeseen
attack", we provide a formal deﬁnition of learning with an unforeseen adversary.

Domain Generalization A problem related to generalizing to unforeseen attacks is the problem
of domain generalization under covariate shift. In the domain generalization problem, the learner
has access to multiple training distributions and has the goal of generalizing to an unknown test
distribution. (Albuquerque et al., 2019) demonstrate that when the test distribution lies within a
convex hull of the training distributions, learning is feasible. (Ye et al., 2021) propose a theoretical
framework for domain generalization in which they derive a generalization bound in terms of the
variation of features across training distributions. We focus on the problem of generalizing to
unforeseen adversaries and demonstrate that a generalization bound in terms of variation of features
across the training threat model exists.

3 Adversarial Learning with an Unforeseen Adversary

,

Y

D

= (

. We use X to denote the support of

Notations We use
D
X
m iid samples from
we will refer to the defense as a learning algorithm
training data as inputs and outputs the learned classiﬁer ( ˆf =
We use
a function class where

) to denote the data distribution and Dm to denote a dataset formed by
. To match learning theory literature,
, which takes the adversarial threat model and
(S, Dm) where S is the threat model).
denotes

A
is applied over and

to denote the function class that
f
∀

A
h where g

(S, Dm)

, f = g

In this section, we will deﬁne what constitutes an unforeseen attack and the learner’s goal in the
presence of unforeseen attacks. We then introduce unforeseen adversarial generalizability which
provides a framework for reasoning about what types of learning algorithms give models that
generalize well to unforeseen adversaries.

G ◦ H

∈ H

∈ F

∈ F

∈ G

, h

A
.

A

=

F

F

X

◦

.

3.1 Formulating Learning with an Unforeseen Adversary

X . For any input x
}

To formulate adversarial learning with an unforeseen adversary, we begin by deﬁning threat model
and adversarial risk. We will then use these deﬁnitions to explain the goal of the learner in the
presence of an unforeseen adversary.
Deﬁnition 3.1 (Threat Model). The threat model is deﬁned by a neighborhood function N (
0, 1
{
Deﬁnition 3.2 (Expected and Empirical Adversarial Risk). We deﬁne expected adversarial risk for a
model f with respect to a threat model N as LN (f ) = E(x,y)∼D maxx(cid:48)∈N (x) (cid:96)(f (x(cid:48)), y) where (cid:96) is
, but have
a loss function. In practice, we generally do not have access to the true data distribution
m
i=1. We can approximate LN (f ) with the empirical adversarial risk deﬁned
m iid samples
(xi, yi)
{
}
(cid:80)m
as ˆLN (f ) = 1
i=1 maxx(cid:48)
m

X, N (x) contains x.

i∈N (xi) (cid:96)(f (x(cid:48)

) : X
·

i), yi)

→

D

∈

In adversarial learning, the learner’s goal is to ﬁnd a function f
that minimizes LT where T
threat model used by the adversary. We call T the target threat model. We call the threat model that
the learner has access to during training the source threat model. We divide the adversarial learning
problem into 2 cases, learning with a foreseen adversary and learning with an unforeseen adversary.
To distinguish between these 2 cases, we ﬁrst deﬁne the subset operation for threat models.

∈ F

3

Deﬁnition 3.3 (Threat Model Subset and Superset). We call a threat model S a subset of another
threat model T (and T a superset of S) if S(x)
. We denote this as
X
, then we call S a strict subset of T (and
S). If S(x)
S
X
S).
T a strict superset of S) and denote this as S

T (x) almost everywhere in

T (x) almost everywhere in

⊆
T (or T

T (or T

⊆

⊇

⊂

⊂

⊃

Learning with a Foreseen Adversary In learning with a foreseen adversary, the target threat model
S). The learner has access to S and a dataset Dm of
T is a subset of the source threat model S (T
m iid samples from the data distribution
for
(S, Dm) achieves LT (f ) < (cid:15) for some small (cid:15) > 0. The learner cannot compute LT (f ),
which f =
A
ˆLT (f ). This setting of learning with a foreseen adversary represents when
but can compute ˆLS(f )
the adversary is weaker than assumed by the learner and since LS(f )
LT (f ), which means that as
long as the learner can achieve LS(f ) < (cid:15), then they are guaranteed that LT (f ) < (cid:15).

. The learner would like to use a learning algorithm

A

≥

⊆

≥

D

. The learner would like to use a learning algorithm

Learning with an Unforeseen Adversary In learning with an unforeseen adversary, the target threat
S). In this setting, we call T an
model T is a strict superset of the source threat model S (T
unforeseen adversary. The learner has access to S and a dataset Dm of m iid samples from the
data distribution
(S, Dm)
achieves LT (f ) < (cid:15) for some small (cid:15) > 0. This setting of learning with an unforeseen adversary
represents when the adversary is strictly stronger than assumed by the learner. Compared to learning
with a foreseen adversary, this problem is more difﬁcult since LS(f ) may not be reﬂective of LT (f ).
By construction LT (f )
LS(f ), but it is unclear how much larger LT (f ) is. When can we guarantee
that LT (f ) is close to LS(f )? We will address this question in the Section 3.2 when we deﬁne threat
model generalizability and Section 4 when we provide a bound for LT (f )

for which f =

LS(f ).

A

A

≥

⊃

D

−

3.2 Formulating Generalizability with an Unforeseen Adversary

A

A

(S, Dm) achieves small ˆLS(f ) (which can be measured by

that performs well against an unforeseen adversary? One way is to have
) while ensuring that ˆLS(f ) is

How should we deﬁne
f =
close to LT (f ). This leads us to the following deﬁnition for generalization gap.
Deﬁnition 3.4 (Generalization Gap). For threat models S and T , the generalization gap is deﬁned as
LT (f )

ˆLS(f ). We observe that

A

−

LT (f )

−

ˆLS(f ) = LT (f )

−
(cid:123)(cid:122)
threat model generalization gap

(cid:124)

LS(f )
(cid:125)

ˆLS(f )
+ LS(f )
−
(cid:125)
(cid:123)(cid:122)
(cid:124)
sample generalization gap

0, so
We note that in the special case of learning with a foreseen adversary, LT (f )
ˆLS(f ) and bounding the generalization gap be achieved by bounding the
LT (f )
sample generalization gap, which has been studied by prior works (Attias et al., 2019; Raghunathan
et al., 2019; Chen et al., 2020; Yu et al., 2021).

ˆLS(f )

LS(f )

LS(f )

−

≤

≤

−

−

We would like to ensure that the generalization gap is small with high probability. We can achieve
this by ensuring that both the sample generalization gap and threat model generalization gap are
small. This leads us to deﬁne robust sample generalizability and threat model generalizability which
describe conditions necessary for us to expect the respective generalization gaps to be small. We then
combine these generalizability deﬁnitions and deﬁne unforeseen adversarial generalizability which
describes the conditions necessary for a learning algorithm to be able to generalize to unforeseen
attacks.
Deﬁnition 3.5 (Robust Sample Generalizability). A learning algorithm
generalizes across function class
when running

on threat model S where (cid:15) : N
, we have

R+, if for any distribution

on m iid samples Dm from

robustly ((cid:15)(
·

), δ)-sample

→

A

D

F

A

P[LS(

(S, Dm))

D
ˆLS(

A
Deﬁnition 3.5 implies that any learning algorithm that ((cid:15)(
·
our chosen hypothesis class
gap with high probability.

A

≤

F

(S, Dm)) + (cid:15)(m)]

1

δ

−

≥

), δ)-robustly sample generalizes across
with (cid:15)(m) << 1, δ << 1, we can achieve small sample generalization

We now deﬁne generalizability for the threat model generalization gap.
Deﬁnition 3.6 (Threat Model Generalizability). Let S be the source threat model used by the
), δ)-robustly generalizes to target threat model T where
learner. A learning algorithm
·

((cid:15)(

,
·

A

4

N

(cid:15) : T
∪ {∞}
→
with m iid samples from

×

R+

∈
, we have:

and δ

[0, 1] if for any data distribution

and any training dataset Dm

D

D
P[LT (

A

(S, Dm))

LS(

(S, Dm)) + (cid:15)(T, m)]

A

1

δ

−

≥

≤

We note that the Deﬁnition 3.6 considers generalization to a given T , which does not fully account for
the unknown nature of T , since from the learner’s perspective, the learner does not know which threat
model it wants LT to be small for. We address this in the following deﬁnition where we combine
Deﬁnitions 3.5 and 3.6 and deﬁne generalizability to unforeseen adversarial attacks.
Deﬁnition 3.7 (Unforeseen Adversarial Generalizability). A learning algorithm
class
(cid:15) : N
((cid:15)1, δ)-sample generalizes and ((cid:15)2, δ)-robustly generalizes to any threat model T .

on function
), δ)-robustly generalizes to unforeseen threat models where
·
robustly

with source adversary S, ((cid:15)(
,
·
N

if there exists (cid:15)1, (cid:15)2 with (cid:15)1(m) + (cid:15)2(T, m)

(cid:15)(T, m) such that

∪ {∞}

F
×

R+

→

A

A

≤

We remark that in Deﬁnition 3.7, (cid:15) is a function of T , which accounts for differences in difﬁculty
of possible target threat models. Ideally, we would like (cid:15)(T, m) at sufﬁciently large m to be small
across a set of reasonable threat models T (ie. imperceptible perturbations) and expect it to be large
(and possibly inﬁnite) for difﬁcult or unreasonable threat models (ie. unbounded perturbations).

4 A Generalization Bound for Unforeseen Attacks

While prior works have proposed bounds on sample generalization gap (Attias et al., 2019; Raghu-
nathan et al., 2019; Chen et al., 2020; Yu et al., 2021), to the best of our knowledge, prior works
have not provided bounds on threat model generalization gap. In this section, we demonstrate that
we can bound the threat model generalization gap in terms of a quantity we deﬁne as variation, the
expected maximum difference across features learned by the model across the target threat model. We
then show that with the existence of an expansion function, which relates source variation to target
variation, any learning algorithm which with high probability outputs a model with small source
variation can achieve small threat model generalization gap.

4.1 Relating generalization gap to variation

∀

h

∈

, h :

∈ H

where

G ◦ H

F
X →

RK is a top level
, g : Rd
We now consider function classes of the form
=
Rd is a d-dimensional feature extractor. Since the
classiﬁer into K classes and
[1...d] ﬂuctuates a lot across the threat model
top classiﬁer g is ﬁxed for a function f , if h(ˆx)i, i
∈
T (x), then the adversary can manipulate this feature to cause misclassiﬁcation. The relation
ˆx
between features and robustness has been analyzed by prior works such as (Ilyas et al., 2019; Tsipras
et al., 2019; Tramèr & Boneh, 2019). We now demonstrate that we can bound the threat model
generalization gap in terms of a measure of the ﬂuctuation of h across T , which we call variation.
Rd across a threat model N
Deﬁnition 4.1 (Variation). The variation of a feature vector h(
is given by

g
∀

∈ G

→

(h, N ) = E(x,y)∼D max

V

x1,x2∈N (x) ||

−

) :
·
h(x1)

X →
h(x2)

2
||

D

denote the data distribution. Let

Theorem 4.2 (Variation-Based Threat Model Generalization Bound). Let S denote the source threat
model and
is a class of Lipschitz classiﬁers
=
with Lipschitz constant upper bounded by σG. Let the loss function be ρ-Lipschitz. Consider a
δ over the
learning algorithm
A
randomness of Dm,
(ρσG(cid:15)(T, m), δ)-
V
robustly generalizes from S to T .

h. If with probability 1
, then

G ◦ H
(S, Dm) = g

(cid:15)(T, m) where (cid:15) : T

and denote f =

over
F
(h, T )

∪ {∞}

where

◦
R+

→

A

A

×

−

≤

N

F

G

Theorem 4.2 shows we can bound the threat model generalization gap between any source S and
unforeseen adversary T in terms of variation across T . With regards to Deﬁnition 3.6, Theorem
4.2 suggests that any learning algorithm over
that with high probability outputs models with low
variation on the target threat model can generalize well to that target.

F

4.2 Relating source and target variation

Since the learning algorithm
achieves small
which relates the source variation (which can be computed by the learner) to target variation.

that
(h, T ). We address this problem by introducing the notion of an expansion function,

cannot use information from T , it is unclear how to deﬁne such

A

A

V

5

Deﬁnition 4.3 (Expansion Function for Variation (Ye et al., 2021)). A function s : R+
R+
threat model T if the following properties hold:

0
} →
is an expansion function relating variation across source threat model S to target

0, +

∪ {

∪ {

1. s(

∞}
) is monotonically increasing and s(x)
·

2. limx→0+ s(x) = s(0) = 0
3. For all h that can be modeled by function class

x,

x

∀

≥

0

≥

, s(

V

F

(h, S))

≥ V

(h, T )

When an expansion function for variation from S to T exists, then we can bound the threat model
generalization gap in terms of variation on S. This follows from Theorem 4.2 and Deﬁnition 4.3.
Corollary 4.4 (Source Variation-Based Threat Model Generalization Bound). Let S denote the
source threat model and
is a class of
Lipschitz classiﬁers with Lipschitz constant upper bounded by σG. Let the loss function be ρ-Lipschitz.
Let T be any unforeseen threat model for which an expansion function s from S to T exists. Consider
a learning algorithm
δ
F
over the randomness of Dm, s(
(ρσG(cid:15)(T, m), δ)-robustly generalizes from S to T .

(S, Dm) = g
(cid:15)(T, m) where (cid:15) : T

denote the data distribution. Let

h. If with probability 1

and denote f =

−
, then

∪ {∞}

(h, S))

G ◦ H

where

◦
×

over

R+

→

A

A

A

≤

=

D

N

F

V

G

V

Corollary 4.4 allows us to relate generalization across threat models of a model f = g
(h, S))
(h, T ). While this expression is still dependent on the target threat model T (since s is
instead of
(h, S)) without knowledge of T due to the monotonicity of the
dependent on T ), we can reduce s(
expansion function. Thus, provided that an expansion function exists, we can use techniques such as
regularization in order to ensure that our learning algorithm actively chooses models with low source
variation. This result leads to the question: when does the expansion function exist?

h to s(

V

V

◦

4.3 When does the expansion function exist?

We now demonstrate a few cases in which the expansion function exists or does not exist. We begin
by providing basic examples of source threat models S and target threat models T without constraints
on function class.
Proposition 4.5. When S = T , an expansion function s exists and is given by s(x) = x.
Proposition 4.6. Let S =
T . Then, for all feature
extractors h, we have that V (h, S) = 0 while V (h, T ) can be greater than 0. In this case, no
expansion function exists such that s(V (h, S))

, and T be a threat model such that S
}

V (h, T ).

x
{

⊂

≥

While we did not consider a constrained function class in the previous two settings, the choice of
function class can also impact the existence of an expansion function. For instance, in the setting
to only use feature extractors with a constant output, then the
of Proposition 4.6, if we constrain
F
expansion function s(x) = x is valid. We now consider the case where our function class
uses
linear feature extractors and derive expansion functions for (cid:96)p adversaries.
Theorem 4.7 (Linear feature extractors with (cid:96)p threat model (p
corresponding label y
x
(cid:15)2
feature extractor with bounded condition number: h
B <

F
Rn and
∈
ˆx
x
ˆx
q
{
≤
||
| ||
−
U (x). Consider a linear
Rd, σmax(W )

∈ {
. Then, an expansion function exists and is linear.

}
N+, p, q > 0. Deﬁne target threat model T (x) = S(x)

[1...K]. Consider S(x) =

∞
and U (x) =

)). Let inputs x

W x + b
|

∪
Rd×n, b

σmin(W ) ≤

with p, q

∈
p
||

ˆx
{

∪
(cid:15)1

| ||

W

+

≤

−

N

∈

∈

∈

∈

ˆx

}

∞}

Theorem 4.7 demonstrates that in the case of a linear feature extractor a linear expansion function
exists for any data distribution from a source (cid:96)p adversary to a union of (cid:96)p adversaries. This result
suggests that with a function class using linear feature extractors, we can improve generalization to
(cid:96)p balls with larger radii by using a learning algorithm that biases towards models with small
(h, S).
We demonstrate this in Appendix D where we experiment with linear models on Gaussian data. We
also provide visualizations of expansion function for a nonlinear model (ResNet-18) on CIFAR-10 in
Section 5.6.

V

5 Adversarial Training with Variation Regularization

Our generalization bound from Corollary 4.4 suggests that learning algorithms that bias towards small
source variation can improve generalization to other threat models when an expansion function exists.

6

In this section, we propose adversarial training with variation regularization (AT-VR) to improve
generalization to unforeseen adversaries and evaluate the performance of AT-VR on multiple datasets
and model architectures.

5.1 Adversarial training with variation regularization

To integrate variation into AT, we consider the following training objective:

min
f ∈F ,f =g◦h

1
n

n
(cid:88)

i=1

[ max
x(cid:48)∈S(xi)
(cid:124)

(cid:96)(f (x(cid:48)), yi)

+λ max

h(x(cid:48))

h(x(cid:48)(cid:48))

−

x(cid:48),x(cid:48)(cid:48)∈S(xi) ||
(cid:124)

(cid:123)(cid:122)
empirical variation

]

2
||
(cid:125)

(cid:123)(cid:122)
empirical adversarial risk

(cid:125)

≥

where λ
0 is the regularization strength. For the majority of our experiments in the main text, we
use the objective of PGD-AT (Madry et al., 2018) as the approximate empirical adversarial risk. We
note that this can be replaced with other forms of AT such as TRADES (Zhang et al., 2019). We can
approximate empirical variation by using gradient-based methods. For example, when N (x) is a (cid:96)p
ball around x, we compute the variation term by using PGD to simultaneously optimize over x1 and
x2. We discuss methods for computing variation for other source threat models in Appendix E.10.

5.2 Experimental Setup

We investigate the performance of training neural networks with AT-VR on image data for a variety
of datasets, architectures, source threat models, and target threat models. We also combine VR with
perceptual adversarial training (PAT) (Laidlaw et al., 2021), the current state-of-the-art for unforeseen
robustness, which uses a source threat model based on LPIPS (Zhang et al., 2018) metric.

Datasets We train models on CIFAR-10, CIFAR-100, (Krizhevsky et al., 2009) and ImageNette
(Howard). ImageNette is a 10-class subset of ImageNet (Deng et al., 2009).

Model architecture On CIFAR-10, we train ResNet-18 (He et al., 2016), WideResNet(WRN)-28-
10 (Zagoruyko & Komodakis, 2016), and VGG-16 (Simonyan & Zisserman, 2015) architectures.
On ImageNette, we train ResNet-18 (He et al., 2016). For PAT-VR, we use ResNet-50. For all
architectures, we consider the feature extractor h to consist of all layers of the NN and the top
level classiﬁer g to be the identity function. We include experiments for when we consider h to be
composed of all layers before the fully connected layers in Appendix F.

Source threat models Across experiments with AT-VR, we consider 2 different source threat models:
(cid:96)∞ perturbations with radius 8
255 and (cid:96)2 perturbations with radius 0.5. For PAT-VR, we use LPIPS
computed from an AlexNet model (Krizhevsky et al., 2017) trained on CIFAR-10. We provide
additional details about training procedure in Appendix C. We also provide results for additional
source threat models such as StAdv and Recolor in Appendix E.10.

Target threat models We evaluate AT-VR on a variety of target threat models including, (cid:96)p ad-
versaries ((cid:96)∞, (cid:96)2, and (cid:96)1 adversaries), spatially transformed adversary (StAdv) (Xiao et al., 2018),
and Recolor adversary (Laidlaw & Feizi, 2019). For StAdv and Recolor threat models, we use the
original bounds from (Xiao et al., 2018) and (Laidlaw & Feizi, 2019) respectively. For all other threat
models, we specify the bound ((cid:15)) within the ﬁgures in this section. We also provide evaluations on
additional adversaries including Wasserstein, JPEG, elastic, and LPIPS-based attacks in Appendix
E.7 for CIFAR-10 ResNet-18 models.

Baselines We remark that we are studying the setting where the learner has already chosen a source
threat model and during testing the model is evaluated on a strictly larger unknown target. Because
of this, for AT-VR experiments, we use standard PGD-AT (Madry et al., 2018) (AT-VR with λ = 0)
as a baseline. For PAT-VR experiments, we use PAT (PAT-VR with λ = 0) as a baseline. We note
that VR can be combined with other training techniques such as TRADES (Zhang et al., 2019) and
provide results in Appendix E.9.

5.3 Performance of AT-VR across different imperceptible target threat models

We ﬁrst investigate the impact of AT-VR on robust accuracy across different target threat models that
are strictly larger than the source threat model used for training. To enforce this, we evaluate robust
accuracy on a target threat model that is the union of the source with a different threat model. For

7

Union with Source
StAdv

Dataset

Architecture

Source

λ

ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
CIFAR-10
CIFAR-10
ImageNette
ImageNette
ImageNette
ImageNette
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100

VGG-16
VGG-16
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18

(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞

0
1
0
0.5
0
0.7
0
0.1
0
1
0
0.1
0
0.75
0
0.2

Clean
acc
88.49
85.21
82.83
72.91
85.93
72.73
79.67
77.80
88.94
85.22
80.56
78.01
60.92
51.53
54.94
48.97

Source
acc
66.65
67.38
47.47
48.84
49.86
49.94
44.36
45.42
84.99
83.08
49.63
50.80
36.01
38.26
22.74
25.04

(cid:96)∞
(cid:15) = 12
255
6.44
13.43
28.09
33.69
28.73
35.11
26.14
28.41
0.00
9.53
32.38
35.57
3.98
11.47
12.61
16.48

(cid:96)2
(cid:15) = 1
34.72
40.74
24.94
24.38
20.89
22.30
30.82
32.08
79.08
80.43
49.63
50.80
16.90
25.65
14.40
15.82

Re-
color
66.52
67.30
47.47
48.84
49.86
49.94
44.36
45.42
72.15
75.26
49.63
50.80
34.87
36.96
22.71
24.95

Union
all
0.33
11.77
2.48
12.59
1.10
14.72
4.35
6.83
0.00
6.80
25.68
31.82
0.40
3.11
2.42
3.48

0.76
34.40
4.38
18.62
2.28
25.33
7.31
10.57
1.27
18.04
34.27
42.37
1.80
5.12
3.99
4.96

Table 1: Robust accuracy of various models trained at different strengths of VR applied on logits on
various threat models. λ = 0 represents the baseline (standard AT). The “source acc" column reports
the accuracy on the source attack ((cid:96)∞, (cid:15) = 8
255 or (cid:96)2, (cid:15) = 0.5). For each individual threat model, we
evaluate accuracy on a union with the source threat model. The union all column reports the accuracy
on the union across all listed threat models.

(cid:96)∞ and (cid:96)2 attacks, we measure accuracy using AutoAttack (Croce & Hein, 2020a), which reports
the lowest robust accuracy out of 4 different attacks: APGD-CE, APGD-T, FAB-T, and Square. For
(cid:96)∞ and (cid:96)2 threat models, we use radius (cid:15) = 12
255 and (cid:15) = 1 for evaluating unforeseen robustness.
We report clean accuracy, source accuracy (robust accuracy on the source threat model), and robust
accuracy across various targets in Table 1. We present results with additional strengths of VR in
Appendix E.5.

AT-VR improves robust accuracy on broader target threat models. We ﬁnd that overall across
datasets, architecture, and source threat model, using AT-VR improves robust accuracy on unforeseen
targets but trades off clean accuracy. For instance, we ﬁnd that on CIFAR-10, our ResNet-18 model
using VR improves robustness on the union of all attacks from 2.48% to 12.59% for (cid:96)∞ source and
from 0.33% to 11.77% for (cid:96)2. The largest improvement we observe is a 33.64% increase in robust
accuracy for the ResNet-18 CIFAR-10 with (cid:96)2 source model on the StAdv target.

AT-VR maintains accuracy on the source compared to standard AT, but trades off clean accu-
racy. We ﬁnd that AT-VR is able to maintain similar source accuracy in comparson to standard
PGD AT but consistently trades off clean accuracy. For example, for WRN-28-10 on CIFAR-10, we
ﬁnd that source accuracy increases slightly with VR (from 49.86% to 49.94%), but clean accuracy
drops from 85.93% to 72.73%. In Appendix E.5, where we provide results on additional values of
regularization strength (λ), we ﬁnd that increasing λ generally trades off clean accuracy but improves
union accuracy. We hypothesize that this tradeoff occurs because VR enforces the decision boundary
to be smooth, which may prevent the model from ﬁtting certain inputs well.

5.4 State-of-the-art performance with PAT-VR

We now combine variation regularization with PAT. We present results in Table 2.

Source
(cid:15)
0.5
0.5
0.5
1 1
1
1

λ

0
0.05
0.1
0
0.05
0.1

Clean
acc
86.6
86.9
85.1
71.6
72.1
72.5

(cid:96)∞
(cid:15) = 8
255
38.8
34.9
31.4
28.7
29.5
29.4

(cid:96)2
(cid:15) = 1
44.3
40.6
37.1
33.3
34.8
35.1

StAdv

5.8
9.4
44.9
64.5
59.6
61.8

Re-
color
60.8
64.6
80.5
67.5
69.7
70.7

Union

PPGD

LPA

2.1
3.7
24.9
27.8
28.2
28.8

16.2
21.9
48.7
26.6
56.7
56.9

2.2
2.2
29.7
9.8
18.5
30.8

Table 2: Robust accuracy of ResNet-50 models trained using AlexNet-based PAT-VR with (cid:15) = 0.5
and (cid:15) = 1. λ = 0 corresponds to standard PAT. The union column reports the accuracy obtained on
the union of (cid:96)∞, (cid:96)2, StAdv, and Recolor adversaries. The PPGD and LPA columns report robust
accuracy under AlexNet-based PPGD and LPA attacks with (cid:15) = 0.5.

8

PAT-VR achieves state-of-the-art robust accuracy on AlexNet-based LPIPS attacks (PPGD
and LPA). Laidlaw et al. (2021) observed that LPA attacks are the strongest perceptual attacks, and
that standard AlexNet-based PAT with source (cid:15) = 1 can only achieve 9.8% robust accuracy on LPA
attacks with (cid:15) = 0.5. In comparison, we ﬁnd that applying variation regularization can signiﬁcantly
improve over performance on LPIPS attacks. In fact, using variation regularization strength λ = 0.1
while training with (cid:15) = 0.5 can achieve 29.7% robust accuracy on LPA, while training with λ = 0.1
and (cid:15) = 1 improves LPA accuracy to 30.8%.

PAT-VR achieves state-of-the-art union accuracy across (cid:96)∞, (cid:96)2, StAdv, and Recolor attacks.
We observe that as regularization strength λ increases, union accuracy also increases. For source
(cid:15) = 0.5, we ﬁnd that union accuracy increases from 2.1% without variation regularization to 24.9%
with variation regularization at λ = 0.1. For source (cid:15) = 1, we observe a 1% increase in union
accuracy from λ = 0 to λ = 0.1. However, this comes at a trade-off with accuracy on speciﬁc threat
models. For example, when training with (cid:15) = 0.5, we ﬁnd that variation regularization at λ = 0.1
trades off accuracy on (cid:96)∞ and (cid:96)2 sources (7.4% and 7.2% drop in robust accuracy respectively), but
improves robust accuracy on StAdv attacks from 5.8% to 44.9%. Meanwhile, for (cid:15) = 1, we ﬁnd that
at λ = 0.1, variation regularization trades off accuracy on StAdv to improve accuracy across (cid:96)∞, (cid:96)2,
and Recolor threat models.

Unlike AT-VR, PAT-VR maintains clean accuracy in comparison to PAT. We ﬁnd that PAT-VR
generally does not trade off additional clean accuracy in comparison to PAT. In some cases (at source
(cid:15) = 1), increasing variation regularization strength can even improve clean accuracy.

5.5

Inﬂuence of AT-VR on threat model generalization gap across perturbation size

Figure 1: Threat model generalization gap of ResNet-18 models on CIFAR-10 trained using AT-VR
at regularization strength λ measured on target (cid:96)p, p =
adversarial examples with radius
(cid:15). The generalization gap is measured with respect to cross entropy loss. All models are trained
8
255 . We ﬁnd that increasing VR strength decreases the
with source (cid:96)∞ perturbations of radius
generalization gap across (cid:15).

, 2, 1
}

{∞

In Section 5.3, we observed that AT-VR improves robust accuracy on a variety of unforeseen target
threat models at the cost of clean accuracy. This suggests that AT-VR makes the change in adversarial
loss on more difﬁcult threat models increase more gradually. In this section, we experimentally
verify this by plotting the gap between source and target losses (measured via cross entropy) across
different perturbation strengths (cid:15) for (cid:96)p threat models (p
) for ResNet-18 models trained
}
on CIFAR-10. We present results for models using AT-VR with (cid:96)∞ source attacks in Figure 1. For
these experiments, we generate adversarial examples using APGD from AutoAttack (Croce & Hein,
2020b). We also provide corresponding plots for (cid:96)2 source attacks in Appendix E.4.

∈ {∞

, 2, 1

We ﬁnd that AT-VR consistently reduces the gap between source and target losses on (cid:96)p attacks
across different target perturbation strengths (cid:15). We observe that this gap decreases as regularization
increases across target threat models. This suggests that VR can reduce the generalization gap across
threat models, making the loss measured on the source threat model better reﬂect the loss measured
on the target threat model, which matches our results from Corollary 4.4.

5.6 Visualizing the expansion function

The effectiveness of AT-VR suggests that an expansion function exists between across the different
imperceptible threat models tested. In this section, we visualize the expansion function between (cid:96)∞

1Values taken from Laidlaw et al. (2021)

9

0.040.060.08Target(cid:15)024LT−ˆLSTarget‘∞0.51.01.52.0Target(cid:15)024LT−ˆLSTarget‘25101520Target(cid:15)0123LT−ˆLSTarget‘1λ=0λ=0.05λ=0.1λ=0.3λ=0.5and (cid:96)2 source and target pairs for ResNet-18 models on CIFAR-10. We train a total of 15 ResNet-18
models using PGD-AT with and without VR on (cid:96)2 and (cid:96)∞ source threat models. We evaluate variation
on models saved every 10 epochs during training along with the model saved at epoch with best
performance, leading to variation computation on a total of 315 models for each source threat model.

We consider 4 cases: (1) (cid:96)∞ source with (cid:15) =
8
255 to (cid:96)∞ target with (cid:15) = 16
255 , (2) (cid:96)∞ source
with (cid:15) = 8
255 to a target consisting of the union
of the source with an (cid:96)2 threat model with ra-
dius 0.5, (3) (cid:96)2 source with (cid:15) = 0.5 to a target
consisting of the union of the source with an (cid:96)∞
threat model with (cid:15) = 8
255 , and (4) (cid:96)2 source
with (cid:15) = 0.5 to (cid:96)2 target with (cid:15) = 1. In cases
(2) and (3), since the target is the union of (cid:96)p
balls, we approximate the variation of the union
by taking the maximum variation across both (cid:96)p
balls. We plot the measured source vs target vari-
ation along with the minimum linear expansion
function s in Figure 2.

We ﬁnd that in all cases the distribution of source
vs target variation is sublinear, and we can upper
bound this distribution with a linear expansion
function with relatively small slope. Recall our
ﬁnding in Theorem 4.7 that for linear models
there exists a linear expansion function across (cid:96)p
norms. We hypothesize that this property also
appears for ResNet-18 models because neural networks are piecewise linear.

Figure 2: Plots of minimum linear expansion func-
tion s shown in blue computed on 315 adversarially
trained ResNet-18 models. Each grey point repre-
sents variation measured on the source and target
pair. Variation is computed on the logits. The two
columns represent the source adversary ((cid:96)∞ and
(cid:96)2 respectively). The two rows represent the target
adversary ((cid:96)∞ and (cid:96)2 respectively).

6 Discussion, Limitations, and Conclusion

We highlight a limitation in adversarial ML research: the lack of understanding of how robustness
degrades when a mismatch in source and target threat models occurs. Our work takes steps toward
addressing this problem by formulating the problem of learning with an unforeseen adversary and
providing a framework for reasoning about generalization under this setting. With this framework,
we derive a bound for threat model generalization gap in terms of variation and use this bound to
design an algorithm, adversarial training with variation regularization (AT-VR). We highlight several
limitations of our theoretical results: (1) the bounds provided can be quite loose and may not be
good predictors of unforeseen loss, (2) while we show that an expansion function between (cid:96)p balls
exists for linear models, it is unclear if that is the case for neural networks. Additionally, we highlight
several limitations of AT-VR: (1) its success depends on the existence of an expansion function, (2)
VR trades off additional clean accuracy and increases computational complexity of training. Further
research on improving source threat models and the accuracy and efﬁciency of adversarial training
algorithms can improve the performance of AT-VR. Finally, we note that in some applications, such as
defending against website ﬁngerprinting (Rahman et al., 2020) and bypassing facial recognition based
surveillance (Shan et al., 2020), adversarial examples are used for good, so improving robustness
against adversarial examples may consequently hurt these applications.

Acknowledgments and Disclosure of Funding

We would like to thank Tianle Cai, Peter Ramadge, and Vincent Poor for their feedback on this work.
This work was supported in part by the National Science Foundation under grants CNS-1553437 and
CNS-1704105, the ARL’s Army Artiﬁcial Intelligence Innovation Institute (A2I2), the Ofﬁce of Naval
Research Young Investigator Award, the Army Research Ofﬁce Young Investigator Prize, Schmidt
DataX award, and Princeton E-fﬁliates Award. This material is also based upon work supported by
the National Science Foundation Graduate Research Fellowship under Grant No. DGE-2039656.
Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the views of the National Science Foundation.

10

0.02.55.07.5SourceVariation051015TargetVariationSource‘∞Target‘∞s(x)=1.72x0.00.51.0SourceVariation051015TargetVariationSource‘2s(x)=16.12x0.02.55.07.5SourceVariation0.02.55.07.5TargetVariationTarget‘2s(x)=1.00x0.00.51.0SourceVariation0.00.51.0TargetVariations(x)=1.14xReferences

Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T. H., and Mitliagkas, I. Generalizing to unseen

domains via distribution matching. arXiv preprint arXiv:1911.00804, 2019.

Attias, I., Kontorovich, A., and Mansour, Y. Improved generalization bounds for robust learning. In

Algorithmic Learning Theory, pp. 162–183. PMLR, 2019.

Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In 2017 ieee

symposium on security and privacy (sp), pp. 39–57. IEEE, 2017.

Chen, J., Raghuram, J., Choi, J., Wu, X., Liang, Y., and Jha, S. Revisiting adversarial robustness of
classiﬁers with a reject option. In The AAAI-22 Workshop on Adversarial Machine Learning and
Beyond, 2022. URL https://openreview.net/forum?id=UiF3RTES7pU.

Chen, L., Min, Y., Zhang, M., and Karbasi, A. More data can expand the generalization gap between
adversarially robust and standard models. In International Conference on Machine Learning, pp.
1670–1680. PMLR, 2020.

Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized
smoothing. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pp. 1310–1320. PMLR, 2019. URL
http://proceedings.mlr.press/v97/cohen19c.html.

Croce, F. and Hein, M. Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$.
In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020a. URL https://openreview.net/forum?id=rklk_
ySYPB.

Croce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International conference on machine learning, pp. 2206–2216. PMLR,
2020b.

Cullina, D., Bhagoji, A. N., and Mittal, P. Pac-learning in the presence of evasion adversaries. arXiv

preprint arXiv:1806.01471, 2018.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255.
Ieee, 2009.

Diakonikolas, I., Kane, D. M., and Manurangsi, P. Nearly tight bounds for robust proper learning of

halfspaces with a margin. arXiv preprint arXiv:1908.11335, 2019.

Diochnos, D. I., Mahloujifar, S., and Mahmoody, M. Lower bounds for adversarially robust pac

learning. arXiv preprint arXiv:1906.05815, 2019.

Everingham, M., Gool, L. V., Williams, C. K. I., Winn, J. M., and Zisserman, A. The pascal
visual object classes (VOC) challenge. Int. J. Comput. Vis., 88(2):303–338, 2010. doi: 10.1007/
s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.

Glorot, X. and Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks.
In Teh, Y. W. and Titterington, D. M. (eds.), Proceedings of the Thirteenth International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy,
May 13-15, 2010, volume 9 of JMLR Proceedings, pp. 249–256. JMLR.org, 2010. URL http:
//proceedings.mlr.press/v9/glorot10a.html.

He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In 2015 IEEE International Conference on Computer
Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026–1034. IEEE Computer Society,
2015. doi: 10.1109/ICCV.2015.123. URL https://doi.org/10.1109/ICCV.2015.123.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

11

Howard, J. Imagenette. URL https://github.com/fastai/imagenette/.

Ilyas, A., Santurkar, S., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs,

they are features. Advances in neural information processing systems, 32, 2019.

Jin, C. and Rinard, M. Manifold regularization for locally stable deep neural networks. arXiv preprint

arXiv:2003.04286, 2020.

Kang, D., Sun, Y., Hendrycks, D., Brown, T., and Steinhardt, J. Testing robustness against unforeseen

adversaries. arXiv preprint arXiv:1908.08016, 2019.

Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing.

arXiv preprint

arXiv:1803.06373, 2018.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional
neural networks. Commun. ACM, 60(6):84–90, 2017. doi: 10.1145/3065386. URL http:
//doi.acm.org/10.1145/3065386.

Laidlaw, C. and Feizi, S.

Functional adversarial attacks.

In Wallach, H. M., Larochelle,
H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pp. 10408–10418, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
6e923226e43cd6fac7cfe1e13ad000ac-Abstract.html.

Laidlaw, C., Singla, S., and Feizi, S. Perceptual adversarial robustness: Defense against unseen threat
models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=
dFwBosAcJkN.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models
resistant to adversarial attacks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.

Maini, P., Wong, E., and Kolter, J. Z. Adversarial robustness against the union of multiple perturbation
models. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp.
6640–6650. PMLR, 2020. URL http://proceedings.mlr.press/v119/maini20a.html.

Montasser, O., Hanneke, S., and Srebro, N. Vc classes are adversarially robustly learnable, but only

improperly. In Conference on Learning Theory, pp. 2512–2530. PMLR, 2019.

Montasser, O., Hanneke, S., and Srebro, N. Adversarially robust learning with unknown perturbation
sets. In Belkin, M. and Kpotufe, S. (eds.), Conference on Learning Theory, COLT 2021, 15-19
August 2021, Boulder, Colorado, USA, volume 134 of Proceedings of Machine Learning Research,
pp. 3452–3482. PMLR, 2021. URL http://proceedings.mlr.press/v134/montasser21a.
html.

Raghunathan, A., Xie, S. M., Yang, F., Duchi, J. C., and Liang, P. Adversarial training can hurt

generalization. arXiv preprint arXiv:1906.06032, 2019.

Rahman, M. S., Imani, M., Mathews, N., and Wright, M. Mockingbird: Defending against deep-
IEEE Transactions on

learning-based website ﬁngerprinting attacks with adversarial traces.
Information Forensics and Security, 16:1594–1609, 2020.

Shan, S., Wenger, E., Zhang, J., Li, H., Zheng, H., and Zhao, B. Y. Fawkes: Protecting privacy against
unauthorized deep learning models. In 29th USENIX Security Symposium (USENIX Security 20),
pp. 1589–1604, 2020.

12

Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition.
In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1409.1556.

Stutz, D., Hein, M., and Schiele, B. Conﬁdence-calibrated adversarial training: Generalizing to unseen
attacks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp.
9155–9166. PMLR, 2020. URL http://proceedings.mlr.press/v119/stutz20a.html.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R.
Intriguing properties of neural networks. In Bengio, Y. and LeCun, Y. (eds.), 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.

Tramèr, F. and Boneh, D. Adversarial training and robustness for multiple perturbations.

In
Conference on Neural Information Processing Systems (NeurIPS), 2019. URL https://arxiv.
org/abs/1904.13000.

Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with

accuracy. In International Conference on Learning Representations, number 2019, 2019.

Wu, D., Xia, S., and Wang, Y. Adversarial weight perturbation helps robust generalization. In
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https://proceedings.
neurips.cc/paper/2020/hash/1ef91c212e30e14bf125e9374262401f-Abstract.html.

Wu, K., Wang, A., and Yu, Y. Stronger and faster wasserstein adversarial attacks. In International

Conference on Machine Learning, pp. 10377–10387. PMLR, 2020b.

Xiao, C., Zhu, J., Li, B., He, W., Liu, M., and Song, D. Spatially transformed adversarial examples.
In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https:
//openreview.net/forum?id=HyydRMZC-.

Yang, G., Duan, T., Hu, J. E., Salman, H., Razenshteyn, I., and Li, J. Randomized smoothing of all
shapes and sizes. In International Conference on Machine Learning, pp. 10693–10705. PMLR,
2020.

Ye, H., Xie, C., Cai, T., Li, R., Li, Z., and Wang, L. Towards a theoretical framework of out-of-
distribution generalization. CoRR, abs/2106.04496, 2021. URL https://arxiv.org/abs/2106.
04496.

Yu, Y., Yang, Z., Dobriban, E., Steinhardt, J., and Ma, Y. Understanding generalization in adversarial

training via the bias-variance decomposition. arXiv preprint arXiv:2103.09947, 2021.

Zagoruyko, S. and Komodakis, N. Wide residual networks. In Wilson, R. C., Hancock, E. R., and
Smith, W. A. P. (eds.), Proceedings of the British Machine Vision Conference 2016, BMVC 2016,
York, UK, September 19-22, 2016. BMVA Press, 2016. URL http://www.bmva.org/bmvc/
2016/papers/paper087/index.html.

Zhang, D., Ye, M., Gong, C., Zhu, Z., and Liu, Q. Black-box certiﬁcation with randomized smoothing:
A functional optimization based framework. Advances in Neural Information Processing Systems,
33:2316–2326, 2020a.

Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-
off between robustness and accuracy. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7472–7482.
PMLR, 2019. URL http://proceedings.mlr.press/v97/zhang19p.html.

13

Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D. S., and Hsieh, C. Towards
stable and efﬁcient training of veriﬁably robust neural networks. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020b. URL https://openreview.net/forum?id=Skxuk1rFwB.

Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effective-
ness of deep features as a perceptual metric. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 586–
595. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.
00068. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_
Unreasonable_Effectiveness_CVPR_2018_paper.html.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See Section 6 for a discussion of

limitations.

(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
Section 6. In some applications, such as evading website ﬁngerprinting, adversarial
examples are helpful so improving defenses against them reduces their beneﬁt in these
applications.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] All assumptions

are speciﬁed in the theorem statements (See Theorem 4.2 and Theorem 4.7)

(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix B.1,

B.2, B.3

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We provide our
code in the supplemental material

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See Appendix C for training details

(c) Did you report error bars (e.g., with respect to the random seed after running ex-
periments multiple times)? [Yes] We provide error bars for ResNet-18 models on
CIFAR-10 with (cid:96)∞ source in Appendix E.1, but not on other experiments because of
the high computational cost of adversarial training.

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We use publicly
available datasets such as CIFAR-10, CIFAR-100, and ImageNette as well as existing
network architectures. These are cited in Section 5.2

(b) Did you mention the license of the assets? [Yes] See Section 5.2
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

14

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

15

Appendix

Table of Contents

A Discussion of Related Works

B Proofs of Theorems and Analysis of Bounds
.
.
.

.
B.1 Proof of Theorem 4.2 .
.
.
B.2 Impact of choice of source and target
B.3 Proof of Theorem 4.7 .
.
.
B.4 How well can empirical expansion function predict loss on the target threat model
.
.

for neural networks? .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

C Additional Experimental Setup Details

D Experiments for linear models on Gaussian data
.

D.1 Experimental Setup .
.
.
D.2 Visualizing the expansion function for Gaussian data .
.
D.3 Generalization curves .
.
.
D.4 Accuracies over regularization strength .
.
.
D.5 Evaluations on a separate test set

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.
.

E Additional Results for Logit Level AT-VR
.

.

.

.

.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.
E.1 Results on Additional Seeds .
E.2 Expansion function on random features .
.
E.3 Expansion function between (cid:96)p and StAdv threat model
E.4 Additional Results with (cid:96)p Target Threat Models .
.
.
.
.
.
.
.
E.5 Additional strengths of variation regularization .
.
.
.
.
.
.
E.6 Full AutoAttack results on CIFAR-10 .
.
.
.
.
.
E.7 Evaluations on other adversaries .
.
.
.
.
E.8 Comparison to training with all attacks .
.
.
.
E.9 Combining variation regularization with TRADES .
.
.
E.10 Combining variation regularization with other sources .
.
E.11 Computational complexity of AT-VR .

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

F Additional Results for Feature Level AT-VR

F.1 Expansion function for variation on features
F.2 Additional results with (cid:96)p target threat models
F.3 Robust accuracies with feature level AT-VR .

.

.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

A Discussion of Related Works

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.

16

18
18
18
19

21

22

23
23
23
24
24
24

24
25
25
26
28
29
29
30
30
30
31
33

33
33
33
35

ρ(φ,

) = maxy∈Y supe,e(cid:48)∈D ρ(P(φ(Xe)

Comparison to Ye et al. (2021) Ye et al. (2021) derive a generalization bound for the do-
main generalization problem in terms of variation across features. They deﬁne variation as
is a set of training distri-
V
D
R is a function that maps the input to a 1-D feature, ρ is a symmetric dis-
butions, φ :
tance metric for distributions, and Xe denotes inputs from domain e.
In comparison, we ad-
dress the problem of learning with an unforeseen adversary and deﬁne unforeseen adversarial
generalizability. Our generalization bound using variation is an instantiation of our generaliz-
ability framework. Additionally, we deﬁne a different measure of variation for threat models
(h, N ) = E(x,y)∼D maxx1,x2∈N (x) ||
2, which allows us to use it as regularization
h(x2)
V
during training.

y), P(φ(Xe(cid:48))
|
|

y)) where

h(x1),

X →

−

D

||

16

Comparison to Laidlaw et al. (2021) Laidlaw et al. (2021) proposes training with perturbations of
bounded LPIPS (Zhang et al., 2018) since LPIPS metric is a better approximation of human perceptual
distance than (cid:96)p metrics. In their proposed algorithm, perceptual adversarial training (PAT), they
combine standard adversarial training with adversarial examples generated via their LPIPS bounded
attack method. In terms of terminology introduced in our paper, the Laidlaw et al. (2021) improve
the choice of source threat model while using an existing learning algorithm (adversarial training).
Meanwhile, our work takes the perspective of having a ﬁxed source threat model and improving
the learning algorithm. This allows us to combine our approach with various source threat models
including the attacks used by Laidlaw et al. (2021) in PAT (see Appendix E.10).

Comparison to Stutz et al. (2020) and Chen et al. (2022) Stutz et al. (2020) and Chen et al. (2022)
address the problem of unforeseen attacks by adding a reject option in order to reject adversarial
examples generated with a larger perturbation than used during training. These techniques introduce
a modiﬁed adversarial training objective that maximizes accuracy on perturbations within the source
threat model and maximize rejection rate of large perturbations. In comparison, we look at the
problem of improving robustness on larger threat models instead of rejecting adversarial examples
from larger threat models. Our algorithm AT-VR actively tries to ﬁnd a robust model that minimizes
our generalization bound without abstaining on any inputs.

Comparison to Croce & Hein (2020a) Croce & Hein (2020a) prove that certiﬁed robustness against
(cid:96)1 and (cid:96)∞ bounded perturbations implies certiﬁed robustness against attacks generated with any (cid:96)p
ball. The size of this (cid:96)p certiﬁed radius is the radius of the largest (cid:96)p ball that can be contained in
the convex hull of the (cid:96)1 and (cid:96)∞ balls for which the model is certiﬁably robust. In our work, we are
primarily interested in empirical robustness on target threat models that are supersets of the source
threat model used. We demonstrate that by using variation regularization, we can improve robust
performance on unforeseen threat models (including larger (cid:96)p perturbations, StAdv, and Recolor)
even when our learning algorithm optimizes for robust models on a single (cid:96)p ball.

Comparison to other forms of regularization for adversarial robustness Prior works in ad-
versarial training propose regularization techniques enforcing feature consistency to improve the
performance of adversarial training. For example, TRADES adversarial training(Zhang et al.,
2019) uses a regularization term in the objective to reduce trade-off between clean accuracy and
robust accuracy compared with PGD adversarial training. This regularization term takes the form:
λ maxˆx∈S(x) (cid:96)(f (x)f (ˆx)). Our variation regularization differs from TRADES since we regularize
(cid:96)2 distance between extracted features.

Another regularization is adversarial logit pairing (ALP) (Kannan et al., 2018). This regularization is
also (cid:96)2 based; in ALP, an adversarial example is ﬁrst generated via x(cid:48) = maxˆx∈S(x) (cid:96)(f (ˆx), y) and the
(cid:96)2 distance between the logits of this adversarial example and the original image λ
2 is
−
added to the training objective. ALP can be thought of as a technique to make the logits of adversarial
examples close to the logits of clean images. Variation regularization (λ maxx(cid:48),x(cid:48)(cid:48)∈S(x) ||
−
f (x(cid:48)(cid:48))
2) differs from ALP since it encourages the logits of any pair of images (not only adversarial
examples) that lie within the source threat model to have similar features and does not use information
about the label of the image.

||
f (x(cid:48))

f (x(cid:48))
||

f (x)

||

(cid:15)

∈ {±

Jin & Rinard (2020) propose a regularization technique motivated by concepts from manifold
regularization. Their regularization is computed with randomly sampled maximal perturbations
d and applied to standard training. Their regularization includes 2 terms, one which
p
}
p and x + p across
regularizes the hamming distance between the ReLU masks of the network for x
inputs x, and the second which regularizes the squared (cid:96)2 distance between the network output on
2
2). They demonstrate that using both regularization terms
x
−
||
with (cid:15)
[2, 8] leads to robustness on (cid:96)∞, (cid:96)2, and Wasserstein attacks. In comparison, our variation
regularization is motivated from the perspective of generalization across threat models. We use use
smaller values of (cid:15) in conjunction with adversarial training and regularize worst case (cid:96)2 distance
between logits of any pair of examples within the source threat model.

p and x + p (

f (x + p)

f (x

p)

−

−

−

∈

||

17

B Proofs of Theorems and Analysis of Bounds

B.1 Proof of Theorem 4.2

Proof. By deﬁnition of expected adversarial risk, we have that for any f
LS(f ) = E(x,y)∼D( max
x1∈T (x)

(cid:96)(f (x1), y)

max
x2∈S(x)

LT (f )

−

−

∈ F
(cid:96)(f (x2), y))

≤
By ρ-Lipschitzness of (cid:96)

E(x,y)∼D

max
x1∈T (x),x2∈S(x)

((cid:96)(f (x1), y)

E(x,y)∼D

≤

max
x1∈T (x),x2∈S(x)

ρ

f (x1)

||

= ρE(x,y)∼D

max
x1∈T (x),x2∈S(x) ||

g(h(x1))

(cid:96)(f (x2), y))

f (x2)

2
||

g(h(x2))

2
||

−

−

−

By σG Lipschitzness:

ρσGE(x,y)∼D

≤

max
x1∈T (x),x2∈S(x) ||

h(x1)

−

h(x2)

2
||

Since S

T :

⊆

≤

ρσGE(x,y)∼D max

h(x1)

h(x2)

2
||

−

x1,x2∈T (x) ||
= ρσG

(h, T )

V

Note that our learning algorithm
probability 1

A

δ. Combining this with the above bound gives:

over

outputs a classiﬁer with

F

(h, T )

V

≤

(cid:15)(T, m) with

−

P[LT (f )

LS(f )

−

≤

ρσG(cid:15)(T, m)]

1

δ

−

≥

B.2 Impact of choice of source and target

In Theorem 4.2, we demonstrated a bound on threat model generalization gap that does not depend
on source threat model, so this bound does not allow us to understand what types of targets are easier
to generalize given a source threat model. In this section, we will introduce a tighter bound in terms
of Hausdorff distance, which takes both source and target threat model into account.
Deﬁnition B.1 (Directed Hausdorff Distance). Let A, B
metric. The Hausdorff distance from A to B based on d is given by

R be a distance

X and let d : X

→

X

×

⊂

Hd(A, B) = max
x1∈A

min
x2∈B

d(x1, x2)

Intuitively what this measures is, if we were to take every point in A and project it to the nearest point
in set B, what is the maximum distance projection? We can then derive a generalization bound in
terms of Hausdorff distance based on feature space distance.
Theorem B.2 (Threat Model Generalization Bound with Hausdorff Distance). Let S denote the
source threat model and
be a class of
Lipschitz classiﬁers, where the Lipschitz constant is upper bounded by σG. Let (cid:96) be a ρ-Lipschitz
loss function with respect to the 2-norm. Then, for any target threat model T with S
T . Let
2 be the distance between feature extracted by the model. Then,
dh :=

denote the data distribution. Let

G ◦ H

h(x1)

h(x2)

. Let

⊆

=

D

F

G

||

−

||
LT (f )

LS(f )

−

≤

ρσGE(x,y)∼D[Hdh(T (x), S(x))]

Proof. By deﬁnition of expected adversarial risk,
LS(f ) = E(x,y)∼D( max
x1∈T (x)

LT (f )

−

(cid:96)(f (x1), y)

max
x2∈S(x)

−

(cid:96)(f (x2), y))

Note that since we are subtracting the max across S(x), this expression is upper bounded by any
choice of ˆx
S(x). Thus, we can choose ˆx so that (cid:96)(f (ˆx), y) is close to maxx1∈T (x) (cid:96)(f (x1), y.
∈
This gives us:

E(x,y)∼D( max
x1∈T (x)

≤

min
ˆx∈S(x)

(cid:96)(f (x1), y)

−

(cid:96)(f (ˆx), y))

18

By ρ-Lipschitzness of (cid:96)

By σG Lipschitzness:

E(x,y)∼D max
x1∈T (x)

min
x2∈S(x)

ρ

f (x1)

||

≤

= ρE(x,y)∼D max
x1∈T (x)

min
x2∈S(x) ||

g(h(x1))

f (x2)

2
||

g(h(x2))

2
||

−

−

ρσGE(x,y)∼D max
x1∈T (x)

≤

min
x2∈S(x) ||

h(x1)

−

h(x2)

2
||

Since S

T :

⊆

≤

ρσGE(x,y)∼D max
x1∈T (x)

min
x2∈S(x) ||

−
= ρσGE(x,y)∼D[Hdh (T (x), S(x))]

h(x1)

h(x2)

2
||

We note that this bound is tighter than the variation based bound and goes to 0 when S = T . Since
this bound also depends on both S and T , we can also see that the “difﬁculty" of a target T with
respect to a chosen source threat model S can be measured through the directed Hausdorff distance
from T (x) to S(x).

B.3 Proof of Theorem 4.7

Lemma B.3 (Variation Upper Bound for (cid:96)p threat model, p
corresponding label y
(cid:15)max
upper bounded by

[1...K]. Let the adversarial constraint be given by T (x) =
W x + b
|

Let h be a linear feature extractor: h

∪
∞
Rd×n, b

Rd

W

+

∈

∈

∈

∈

}

Rn and
x
p
||

∈
ˆx
≤
−
. Then, variation is
}

| ||

ˆx

{

). Let inputs x

N

(cid:40)

∈ {
2 − 1
1

p(h, T )

V

≤

2(cid:15)maxn
2(cid:15)maxσmax(W )

p σmax(W ) p

2
p = 1, p = 2

≥

Proof.

V

(h, T ) = E(x,y)∼D max

h(x1)

h(x2)

x1,x2∈T (x) ||

2
||

= E(x,y)∼D max

W (x1

x1,x2∈T (x) ||

−
x2)

−

2
||

||
Consider the case where p > 2. Then, by Hölder’s inequality:

x1,x2∈T (x)

≤

E(x,y)∼D max

σmax(W )

x1

x2

2
||

−

(1)

E(x,y)∼D max

x1,x2∈T (x)

n

≤

1

2 − 1

p σmax(W )

x1

||

x2

p
||

−

When p = 1 or p = 2, then from 1, we have:

2(cid:15)maxn

1

2 − 1

p σmax(W )

≤

E(x,y)∼D max

x1,x2∈T (x)

≤

σmax(W )

x1

||

−

x2

p
||

2(cid:15)maxσmax(W )

≤

Lemma B.4 (Variation lower bound for (cid:96)p threat model, p
corresponding label y
(cid:15)max
lower bounded by

. Let h be a linear feature extractor: h
}

[1...K]. Let the adversarial constraint be given by T (x) =
W x + b

∪
∞
Rd×n, b

W
|

∈ {

Rd

+

∈

∈

∈

∈

Rn and
x
p
||

∈
ˆx
≤
. Then, variation is
}

| ||

−

ˆx

{

). Let inputs x

N

2(cid:15)maxσmin(W ) p
2(cid:15)max√
n σmin(W )
where σmin(W ) denotes the smallest singular value of W .

p(h, T )

≥

V

2
≥
p = 1

(cid:40)

19

Proof.

V

(h, T ) = E(x,y)∼D max

x1,x2∈T (x) ||

W (x1

x2)

2
||

−

Then, for p

2:

≥

≥

≥

σmin(W )E(x,y)∼D max

x1,x2∈T (x) ||

σmin(W )E(x,y)∼D max

x1,x2∈T (x) ||

x1

x1

x2

2
||

−

x2

p
||

−

(2)

= 2(cid:15)maxσmin(W )

For p = 1 from 2, we have:

1
√n

≥

σmin(W )E(x,y)∼D max

x1,x2∈T (x) ||

x1

x2

1
||

−

=

2(cid:15)max
√n

σmin(W )

[1...K]. Let S(x) =

Lemma B.5 ((cid:96)p threat models with larger radius, p
label y
ˆx
(cid:15)1
| ||
| ||
Consider a linear feature extractor with bounded condition number: h
Rd, σmax(W )
. Then a valid expansion function is given by:

N
+
and T (x) =

B <

ˆx
{

∈
}

p
||

∞

≤

−

∪

∈

ˆx

x

Rn and corresponding
). Let inputs x
∈
(cid:15)1.
where (cid:15)2
x
(cid:15)2
ˆx
p
≤
||
{
W x + b

≥
Rd×n, b

−
∈ {

}
W
|

∈

∈

σmin(W ) ≤

∞}

sp(z) =






z

√nB (cid:15)2
(cid:15)1
B (cid:15)2
z
(cid:15)1
2 − 1
1
p B (cid:15)2
n
(cid:15)1

z

p = 1
p = 2
p > 2

Proof. By Lemma B.3 and Lemma B.4,
satisﬁes properties of expansion function.

V

(h, T )

s(

V

≤

(h, S)). Additionally, it is clear that sp

Lemma B.6 (Variation upper bound for the union of (cid:96)p and (cid:96)q threat models (p, q
Rn and corresponding label y
inputs x
ˆx
∈
(cid:15)2
x
T2(x) =
ˆx
q
≤
−
||
{
extractor: h
W
W x + b
|

. Deﬁne adversarial constraint T = T1
}
Rd×n, b
∈
∈
(cid:40)

. Let v(p, h, T ) be deﬁned as
}
2 − 1
1

[1...K]. Consider T1(x) =

| ||
∈ {

∈
Rd

ˆx
{

| ||

∪

ˆx

v(p, (cid:15), W ) =

p σmax(W ) p

2(cid:15)n
2(cid:15)σmax(W )

2
p = 1, p = 2

≥

N
)). Let
∞
∪
and
(cid:15)1
x
p
||
T2. Let h be a linear feature

∈
−

+

≤

}

where σmax(W ) denotes the largest singular value of W .

Then variation is upper bounded by:

V(p,(cid:15)1),(q,(cid:15)2)(h, T )

≤

max(v(p, (cid:15)1, W ), v(q, (cid:15)2, W ))

Proof.

V

(h, T ) = E(x,y)∼D max

x1,x2∈T (x) ||

W (x1

x2)

2
||

−

E(x,y)∼D max

x1,x2∈T (x)

σmax(W )

x1

||

x2

2
||

−

≤
T2, maxx1,x2∈T (x) ||

Since T = T1
2 can be upper bounded by the diameter of the
||
hypersphere that contains both T1 and T2. We can compute this diameter by taking the max out of
the diameter of the hypersphere containing T1 and the diameter of the hypersphere containing T2.
This was computed in proof of Lemma B.3 to bound the case of a single (cid:96)p norm. Thus,

x2

x1

−

∪

V(p,(cid:15)1),(q,(cid:15)2)(h, T )
where the expression for v follows from the result of Lemma B.3.

max(v(p, (cid:15)1, W ), v(q, (cid:15)2, W ))

≤

20

∈
. Deﬁne target threat model T = S

Lemma B.7 ((cid:96)p to union of (cid:96)p and (cid:96)q threat model (p, q
corresponding label y
ˆx
(cid:15)2
condition number: h
function is given by:

[1...K]. Consider S(x) =

W x + b
|

ˆx
p
{
||
| ||
Rd, σmax(W )

∪
Rd×n, b

∈
x
−

∈ {

W

∈

∈

σmin(W ) ≤

}

∪
≤

N

+
(cid:15)1

∞
}

) ). Let inputs x
and U (x) =

∈
ˆx

ˆx
{

Rn and
x
q
||

≤
U . Consider a linear feature extractor with bounded
. Then a valid expansion

| ||

−

B <

∞}

q (cid:15)2,(cid:15)1)

sp,q(z) =






1
2
(cid:15)1

√nB max((cid:15)2,(cid:15)1)
(cid:15)1
− 1
√nB max(n
B max((cid:15)2,(cid:15)1)
(cid:15)1
1
− 1
2
B max(n
(cid:15)1
B max((cid:15)2,n
(cid:15)1
− 1
1
2
B max(n

− 1

1
2

q (cid:15)2,n
(cid:15)1

q (cid:15)2,(cid:15)1)

p (cid:15)1)

1
2

− 1

p (cid:15)1)

p = 1, q = 2

p = 1, q > 2
p = 2, q = 1

p = 2, q > 2

p > 2, q

2

≤

p > 2, q > 2

Proof. By Lemma B.6 and Lemma B.4,
satisﬁes properties of expansion function.

V

(h, T )

s(

V

≤

(h, S)). Additionally, it is clear that sp

Proof of Theorem 4.7. Directly follows from Lemma B.5 and Lemma B.7.

B.4 How well can empirical expansion function predict loss on the target threat model for

neural networks?

Using the empirical expansion function slopes from Figures 2 and 8, we compute the expected cross
entropy loss (with softmax) on the target threat model via Corollary 4.4. We provide the predicted
and true target losses in Table 3 for (cid:96)∞, (cid:15) = 8

255 source threat model and 4 for (cid:96)2, (cid:15) = 0.5.

Source Loss
Target
(cid:96)∞, (cid:15) = 16
0.93
255
0.93
(cid:96)2, (cid:15) = 0.5
0.93
StAdv (cid:15) = 0.05
(cid:96)∞, (cid:15) = 16
1.26
255
1.26
(cid:96)2, (cid:15) = 0.5
1.26
StAdv (cid:15) = 0.05
Table 3: Predicted and measured losses on multiple target threat models for ResNet-18 model trained
on CIFAR-10 with (cid:96)∞, (cid:15) = 8/255 source threat model.

Predicted Target loss True Target Loss
2.44
0.93
5.13
1.76
1.27
2.11

Source Variation
4.90
4.90
4.90
0.98
0.98
0.98

12.85
7.86
9.87
3.64
2.64
3.05

Gap
10.41
6.93
4.74
1.88
1.37
0.94

Source Loss
Target
(cid:96)∞, (cid:15) = 8
0.64
255
0.64
(cid:96)2, (cid:15) = 1
0.64
StAdv (cid:15) = 0.05
(cid:96)∞, (cid:15) = 8
0.85
255
0.85
(cid:96)2, (cid:15) = 1
StAdv (cid:15) = 0.05
0.85
Table 4: Predicted and measured losses on multiple target threat models for ResNet-18 model trained
on CIFAR-10 with (cid:96)2, (cid:15) = 0.5 source threat model.

Predicted Target loss True Target Loss
2.65
1.93
12.26
1.77
1.51
2.10

Source Variation
0.78
0.78
0.78
0.20
0.20
0.20

18.52
1.91
16.51
5.38
1.16
4.88

Gap
15.87
0.02
4.25
3.61
0.35
2.78

In general, we ﬁnd that for models with smaller variation (those trained with variation regularization),
the loss estimate using the slope from the expansion function generally improves. In the case where
the target threat model is Linf, we believe that the large gap between predicted and true loss for the
unregularized model stems from the fact that we model the expansion function with a linear model.
From Figure 2, we can see that at larger values of source variation the linear model for expansion
function becomes an increasingly loose upper-bound. Improving the model for expansion function
(ie. using a log function) may reduce this gap.

21

C Additional Experimental Setup Details

Variation Computation Algorithm for AT-VR We provide the algorithm we use to compute
variation regularization for (cid:96)p source adversaries in Algorithm 1.

Algorithm 1: Variation Regularization Computation for (cid:96)p ball
Input
Notations :

denotes the uniform distribution of dimension of the input, (cid:81)
∈ X

, (cid:96)p radius (cid:15), n number of steps, α step size, feature extractor h

:x

(cid:96)p,(cid:15) denotes

U
projection onto (cid:96)p ball of radius (cid:15)
:Variation v

R
∈
(cid:15), (cid:15))) ;
(cid:15), (cid:15))) ;

(
(

U
U

−
−

h(x2)
−
(cid:96)p,(cid:15)(x1 + α
(cid:96)p,(cid:15)(x2 + α

2 ;
||
x1v) ;
∇
x2v) ;
∇

Output
(cid:81)
(cid:81)

←
←

(cid:96)p,(cid:15)(x +
1 x1
2 x2
(cid:96)p,(cid:15)(x +
3 for i = 1...n do
h(x1)
4
(cid:81)
(cid:81)

5

v
x1
x2

← ||
←
←

6
7 end
8 v
← ||
9 return v

h(x1)

h(x2)

2 ;

||

−

// randomly initialize x1
// randomly initialize x1

// compute objective
// single step of PGD to optimize x1
// single step of PGD to optimize x2

// compute final variation

Variation Computation for PAT-VR Laidlaw et al. (2021) propose an algorithm called Fast
Lagrangian Perceptual Attack (Fast-LPA) to generate adversarial examples for training using PAT. We
make a slight modiﬁcation of the Fast-LPA algorithm, replacing the loss in the original optimization
objective with the variation objective, to obtain Fast Lagrange Perceptual Variation (Fast-LPV). The
explicit algorithm for Fast-LPV is provided in Algorithm 2. We use the variation obtained from
Fast-LPV for training with PAT-VR.

Algorithm 2: Fast Lagrangian Perceptual Variation (Fast-LPV)
Input

), LPIPS distance d(

,
·

), input x, label y, bound (cid:15), number of
·

:feature extractor h(
·
steps n
:variation v

(0.1) ;
(0.1) ;

// randomly initialize x1
// randomly initialize x2

←
represents optimization objective

τ (max(0, d(x1, x)

h(x2)

h(x1)

← ||

2
||

−

−

// τ increases exponentially
// obj

(cid:15)) + max(0, d(x2, x)

(cid:15))) ;

−

−

// compute normalized gradient wrt x1

// compute normalized gradient wrt x2
// step size η decays exponentially
// derivative of d in direction of ∆1
// derivative of d in direction of ∆2
// take a step of size η in LPIPS distance
// take a step of size η in LPIPS distance

Output

←
←

x + 0.01
1 x1
x + 0.01
2 x2
3 for i = 1...n do
10i/n ;
4

∗ N
∗ N

τ
obj

5

6

7

8

9

10

11

;

;

∇x1 [obj]
||∇x1 [obj]||2
∇x2 [obj]
||∇x2 [obj]||2
(0.1)i/n ;
∗
d(x1, x1 + 0.1∆1)/0.1 ;
d(x2, x2 + 0.1∆2)/0.1 ;
x1 + (η/m)∆1 ;
x2 + (η/m)∆2 ;

←

∆1

∆2

η
m1
m2
x1
x2

←

←
(cid:15)

←
←
←
←

12
13 end
14 v
15 return v

← ||

h(x1)

h(x2)

2;

||

−

Additional Experimental Setup Details for AT-VR We run all training on a NVIDIA A100 GPU.
For all datasets and architectures, we perform PGD adversarial training (Madry et al., 2018) and add
variation regularization to the objective. For all datasets, train with normalized inputs. On ImageNette,
224. We train models on seed
we normalize using ImageNet statistics and resize all images to 224

×

22

0 for 200 epochs with SGD with initial learning rate of 0.1 and decrease the learning rate by a factor
of 10 at the 100th and 150th epoch. We use 10-step PGD and train with (cid:96)∞ perturbations of radius
255 and (cid:96)2 perturbations with radius 0.5. For (cid:96)∞ perturbations we use step size of α = 2
8
255 while
for (cid:96)2 perturbations we use step size of 0.075. We use the same settings for computing the variation
regularization term. For all models, we evaluate performance at the epoch which achieves the highest
robust accuracy on the test set.

Additional Experimental Setup Details for PAT-VR We build off the ofﬁcial code repository for
PAT and train ResNet-50 models on CIFAR-10 with AlexNet-based LPIPS threat model with bound
(cid:15) = 0.5 and (cid:15) = 1. For computing AlexNet-based LPIPS we use the CIFAR-10 AlexNet pretrained
model provided in the PAT ofﬁcial code repository. We train all models for 100 epochs and evaluate
the model saved at the ﬁnal epoch of training. To match evaluation technique as Laidlaw et al. (2021),
we evaluate with (cid:96)∞ attacks, (cid:96)2 attacks, StAdv, and Recolor with perturbation bounds 8
255 , 1, 0.05,
0.06 respectively. Additionally, we present accuracy measured using AlexNet-based PPGD and LPA
attacks (Laidlaw et al., 2021) with (cid:15) = 0.5.

D Experiments for linear models on Gaussian data

In Section 4.3, we demonstrated that for a linear feature extractor, the expansion function exists, so
decreasing variation across an (cid:96)p source adversarial constraint should improve generalization to other
(cid:96)p constraints. We now verify this experimentally by training a linear model for binary classiﬁcation
on isotropic Gaussian data.

D.1 Experimental Setup

×

Data generation We sample data from 2 isotropic Gaussians with covariance σ2In where In
denotes the n
n identity matrix. For class 0, we sample from a Gaussian with mean θ0 =
(0.25, 0, 0, ..., 0)T , and for class 1, we sample from a Gaussian with mean θ1 = (0.75, 0, 0, ..., 0)T
and clip all samples to range [0, 1] to simulate image data. We sample 1000 points per class. We vary
. Since our generalization bound considers only threat model
σ
}
generalization gap and not sample generalization gap, we evaluate the models using the same data
samples as used during training for the bulk of our experiments, but we provide results on a separate
test set for one setting in Appendix D.5.

0.125, 0.25

25, 100

and n

∈ {

∈ {

}

Model architecture We train a model consisting of a linear feature extractor and linear top level
Rd and g(x) = Ax + b2
classiﬁer: f = g
where A

h where h(x) = W x + b1 where W
R2. For our experiments, we use d

◦
Rd×2, b2

∈

Rn×d, b1
.
5, 25
}

∈
∈ {

∈

∈

Source Threat Models Across experiments we use 2 different source threat models: (cid:96)∞ perturba-
tions with radius 0.01 and (cid:96)2 perturbations with radius 0.01.

Training Details We perform AT-VR with adversarial examples generated using APGD (Croce &
Hein, 2020b) for 200 epochs. We train models using SGD with learning rate of 0.1 and momentum
of 0.9. We use cross entropy loss during both training and evaluation. For variation regularization,
we use 10 steps for optimization and use step size (cid:15)/9 where (cid:15) is the radius of the (cid:96)p ball used during
training/evaluation.

D.2 Visualizing the expansion function for Gaussian data

In Section 4.3, we demonstrated that with a linear feature extractor, for any dataset there exists a linear
expansion function. We now visualize the true expansion function for Gaussian data with a linear
function class across 4 different combinations of input dimension n, Gaussian standard deviation σ,
and feature dimension d. We set our source threat model to be (cid:96)p, p
with perturbation size
of 0.01. We set our target threat model to be (cid:96)p, p
perturbation size of 0.05 and compute
source variation and target variation of 100 randomly sampled h. We sample parameters of h from a
standard normal distribution. We plot the linear expansion function with minimum slope in Figure 3.
We ﬁnd that across all settings we can ﬁnd a linear expansion function that is a good ﬁt. Additionally,
we ﬁnd that the slope of this linear expansion function stays consistent across changes in σ and d.

, 2
}

∈ {∞

∈ {∞

, 2

}

23

We ﬁnd that input dimension n can inﬂuence the expansion function which matches; for example,
the slope of the expansion function for source (cid:96)2 to target (cid:96)∞ increases from
21 to 39.09. This
matches our results in Lemma B.5 and Lemma B.7 where our computed expansion function depends
on n.

∼

Figure 3: Plots of minimum linear expansion function s shown in blue computed on 100 randomly
initialized linear feature extractors across 4 different combinations of input dimension n, Gaussian
standard deviation σ, and feature dimension d. Each grey point represents the variation of a single
model measured across the source and target. Columns represent the source threat model ((cid:96)∞ and (cid:96)2
with (cid:15) = 0.01) while rows represent the target threat model ((cid:96)∞ and (cid:96)2 with (cid:15) = 0.05).

D.3 Generalization curves

We plot the threat model generalization curves for varied settings of input dimension n, feature
extractor output dimension d, and Gaussian standard deviation σ in Figure 4. We ﬁnd that across
all settings, applying variation regularization leads to smaller generalization gap across values of (cid:96)p
radius (cid:15).

D.4 Accuracies over regularization strength

We plot the accuracies corresponding to the n = 25, σ = 0.125 and d = 5 setting in Figure 4 in
Figure 5. We note that while Figure 4 demonstrated that regularization improves decreases the size of
the generalization gap, there is trade-off in accuracy which can be seen at small of (cid:15). However, we
ﬁnd that generally variation regularization improves robust accuracy on unforeseen threat models.

D.5 Evaluations on a separate test set

We now plot generalization gap for the n = 25, σ = 0.125 and d = 5 setting with cross entropy loss
on the target threat model measured on a separate test set of 2000 samples in Figure 6. We ﬁnd that
the trends we observed on the train set are consistent with those observed when evaluating on the
train set shown in Figure 4.

E Additional Results for Logit Level AT-VR

In this section, we present additional results for AT-VR when considering the logits to be the output
of the feature extractor h.

24

0.300.350.40SourceVariation1.501.752.00TargetVariationTarget‘∞Source‘∞s(x)=4.88x0.080.090.10SourceVariation1.501.752.00TargetVariationSource‘2s(x)=20.82x0.300.350.40SourceVariation0.40.5TargetVariationTarget‘2s(x)=1.31x0.070.080.09SourceVariation0.350.400.45TargetVariations(x)=5.02xn=25,σ=0.125,d=50.500.55SourceVariation2.42.6TargetVariationTarget‘∞Source‘∞s(x)=4.87x0.110.120.13SourceVariation2.42.6TargetVariationSource‘2s(x)=20.67x0.500.55SourceVariation0.550.600.65TargetVariationTarget‘2s(x)=1.21x0.110.120.13SourceVariation0.550.600.65TargetVariations(x)=5.00xn=25,σ=0.125,d=250.300.350.40SourceVariation1.52.0TargetVariationTarget‘∞Source‘∞s(x)=4.97x0.070.080.090.10SourceVariation1.52.0TargetVariationSource‘2s(x)=21.69x0.300.350.40SourceVariation0.40.5TargetVariationTarget‘2s(x)=1.32x0.070.080.090.10SourceVariation0.40.5TargetVariations(x)=5.02xn=25,σ=0.25,d=51.11.2SourceVariation5.56.0TargetVariationTarget‘∞Source‘∞s(x)=4.87x0.140.150.16SourceVariation5.05.56.0TargetVariationSource‘2s(x)=39.09x1.11.2SourceVariation1.11.2TargetVariationTarget‘2s(x)=1.00x0.140.150.16SourceVariation0.700.750.80TargetVariations(x)=5.01xn=100,σ=0.125,d=5Figure 4: Threat model generalization gap of linear models on Gaussian data trained with varied
variation regularization strength λ measured on adversarial examples of generated by target (cid:96)p, p =
perturbations with radius (cid:15) at varied input dimension n, feature extractor output dimension d
{∞
and standard deviation σ. The generalization gap is measured with respect to cross entropy loss. All
models are trained with source (cid:96)p, p =

radius of 0.01.

, 2
}

, 2

{∞

}

E.1 Results on Additional Seeds

In Table 5 we present the mean and standard deviations for robust accuracy of CIFAR-10 ResNet-18
models over 3 trials seeded at 0, 1, and 2. We ﬁnd that the improvement observed through variation
regularization on unforeseen target threat models is signiﬁcant for both (cid:96)2 and (cid:96)∞ attacks for ResNet-
18 CIFAR-10 models; for bolded accuracies on target threat modes (with the exception of (cid:96)∞ source
to (cid:96)2 target which we did not report as an improvement in the main text) we ﬁnd that the range of
the error bars do not overlap with the results for standard adversarial training (λ = 0). Additionally,
we ﬁnd that the trade-off with clean accuracy observed when using variation regularization is also
signiﬁcant.

E.2 Expansion function on random features

In Section 5.6, we plotted the expansion function across models trained with adversarial training at
various levels of variation regularization. We now visualize the expansion function for random feature
extractors to investigate to what extent the learning algorithm inﬂuences expansion function. We plot
the source and target variations (corresponding to the same setup as in Section 5.6) of 300 randomly

25

0.0250.0500.075Target(cid:15)0.00.5LT−LSSource‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.000.050.10LT−LSSource‘∞toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)0.00.51.01.5LT−LSSource‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘2toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.1λ=0.5λ=1λ=50.0250.0500.075Target(cid:15)0.00.51.0LT−LSSource‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘∞toTarget‘2n=25,σ=0.125,d=25λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)0.00.51.01.5LT−LSSource‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.00.1LT−LSSource‘2toTarget‘2n=25,σ=0.125,d=25λ=0λ=0.1λ=0.5λ=1λ=50.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.000.020.04LT−LSSource‘∞toTarget‘2n=25,σ=0.25,d=5λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.000.020.04LT−LSSource‘2toTarget‘2n=25,σ=0.25,d=5λ=0λ=0.1λ=0.5λ=1λ=50.0250.0500.075Target(cid:15)0246LT−LSSource‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.00.10.2LT−LSSource‘∞toTarget‘2n=100,σ=0.125,d=5λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)01020LT−LSSource‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.00.20.4LT−LSSource‘2toTarget‘2n=100,σ=0.125,d=5λ=0λ=0.1λ=0.5λ=1λ=5Figure 5: Accuracies of linear models on Gaussian data trained with at varied variation regularization
strength λ measured on adversarial examples of radius (cid:15). All models are trained with radius 0.01.

Figure 6: Cross entropy loss of linear models on Gaussian data trained with regularization strength
λ measured on adversarial examples of radius (cid:15). Cross entropy loss is measured with respect to a
separate test set. All models are trained with source (cid:96)p radius of 0.01.

initialized feature extractors of ResNet-18 on CIFAR-10 in Figure 7. For random initialization, we use
Xavier normal initialization (Glorot & Bengio, 2010) for weights and standard normal initialization
for biases.

Figure 7: Plots of minimum linear expansion function s shown in blue computed on 300 randomly
initialized ResNet-18 models. Each grey point represents the variation measured on the source and
target attack. Variation is computed on the logits. The two columns represent the source adversary
((cid:96)∞ and (cid:96)2 respectively). The two rows represent the target adversary ((cid:96)∞ and (cid:96)2 respectively).

We ﬁnd that with randomly initialized models, we can also ﬁnd a linear expansion function with
small slope. In comparison to expansion functions from adversarially trained models (Figure 7),
we ﬁnd that using randomly initialized models leads to minimum linear expansion function s with
smaller slope for (cid:96)∞ target threat model. This suggests that learning algorithm can have an impact on
expansion function.

E.3 Expansion function between (cid:96)p and StAdv threat model

While we have shown that an expansion function with small slope exists between (cid:96)2 and (cid:96)∞ threat
models, it is unclear whether this also holds for non-(cid:96)p threat models. However, we do observe from

26

0.0250.0500.075Target(cid:15)0.70.80.9Accuracy(%)Source‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.9000.9250.950Accuracy(%)Source‘∞toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)0.60.8Accuracy(%)Source‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.900.95Accuracy(%)Source‘2toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.1λ=0.5λ=1λ=50.0250.0500.075Target(cid:15)0.00.51.0LT−LSSource‘∞toTarget‘∞0.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘∞toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.01λ=0.05λ=0.1λ=0.50.0250.0500.075Target(cid:15)0.00.51.01.5LT−LSSource‘2toTarget‘∞0.0250.0500.075Target(cid:15)0.000.050.100.15LT−LSSource‘2toTarget‘2n=25,σ=0.125,d=5λ=0λ=0.1λ=0.5λ=1λ=50.250.300.350.40SourceVariation0.40.50.6TargetVariationSource‘∞Target‘∞s(x)=1.50x0.02000.02250.02500.0275SourceVariation0.250.300.350.40TargetVariationSource‘2s(x)=13.43x0.300.35SourceVariation0.300.35TargetVariationTarget‘2s(x)=1.00x0.0200.0250.030SourceVariation0.0250.0300.035TargetVariations(x)=1.18xDataset

Source

λ

CIFAR-10
CIFAR-10
CIFAR-10

CIFAR-10
CIFAR-10

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞

Clean

acc

88.29 ± 0.51
87.39 ± 0.36
86.07 ± 0.50
84.54 ± 0.61
84.71 ± 1.32

Source

acc

67.15 ± 0.45
68.75 ± 0.08
68.78 ± 0.17
67.97 ± 0.15
67.39 ± 0.29

(cid:96)∞
(cid:15) = 12
255
6.77 ± 0.31
11.56 ± 0.57
13.13 ± 1.33
14.61 ± 0.59
13.62 ± 0.81

Union with Source
(cid:96)2
(cid:15) = 1

StAdv

35.40 ± 0.65
39.57 ± 1.08
41.52 ± 1.19
42.13 ± 0.59
41.14 ± 1.56

1.21 ± 0.46
10.31 ± 0.41
18.71 ± 2.79
23.24 ± 0.94
33.70 ± 5.27

Re-

color

66.99 ± 0.43
68.60 ± 0.10
68.59 ± 0.12
67.83 ± 0.12
67.29 ± 0.31

Union

all

0.52 ± 0.25
5.93 ± 0.38
8.70 ± 0.50
10.69 ± 0.15
11.58 ± 0.37

0
0.25
0.5

0.75
1

0
0.05

0.1
0.3
0.5

CIFAR-10
CIFAR-10

83.01 ± 0.29
82.70 ± 0.56
81.79 ± 0.29
78.87 ± 0.72
74.24 ± 2.04

47.44 ± 0.08
48.38 ± 0.39
48.65 ± 0.25
49.16 ± 0.12
48.62 ± 0.20

24.67 ± 0.70
25.25 ± 0.97
24.99 ± 0.73
24.95 ± 0.26
24.59 ± 1.32

27.79 ± 0.44
2.16 ± 0.28
29.55 ± 0.43
2.61 ± 0.43
29.89 ± 0.53
3.47 ± 0.50
CIFAR-10
31.89 ± 0.13
8.53 ± 0.70
CIFAR-10
33.07 ± 1.09
13.35 ± 0.83
CIFAR-10
Table 5: Mean and standard deviation across 3 trials for robust accuracy of various models trained
at different strengths of variation regularization applied on logits on various threat models. λ = 0
corresponds to standard adversarial training. Models are trained with either source threat model (cid:96)∞
with radius 8
255 or (cid:96)2 with radius 0.5. The “source acc" column reports the accuracy on the source
attack. For each individual threat model, we evaluate accuracy on a union with the source threat
model. The union all column reports the accuracy obtained on the union across all listed threat
models.

47.44 ± 0.08
48.38± 0.39
48.65 ± 0.25
49.16 ± 0.12
48.62 ± 0.20

4.17 ± 0.26
4.87 ± 0.66
6.33 ± 0.83
12.96 ± 0.81
19.91 ± 1.12

Table 1 that AT-VR with (cid:96)2 and (cid:96)∞ sources leads to signiﬁcant improvements in robust accuracy on
StAdv, which is a non-(cid:96)p threat model, suggesting that an expansion function between these threat
models exists. Using the same models for generating Figure 2, we plot the expansion function from
(cid:96)∞ and (cid:96)2 sources to StAdv ((cid:15) = 0.05) in Figure 8. We observe that for both source threat models a
linear expansion function exists to StAdv.

Figure 8: Expansion function from (cid:96)∞, (cid:15) = 8/255 and (cid:96)2, (cid:15) = 0.5 source threat models to StAdv
((cid:15) = 0.05) target threat model computed over 315 adversarially trained ResNet-18 models. The linear
expansion function with minimum slope is plotted in blue.

We now visualize the expansion function from StAdv ((cid:15) = 0.03) source to (cid:96)∞ ((cid:15) = 8/255), (cid:96)2
((cid:15) = 0.5), and StAdv ((cid:15) = 0.05) target threat models. We present plots in Figure 9. Unlike our plots
of expansion function for (cid:96)2 and (cid:96)∞ source threat models, we ﬁnd that for StAdv a linear expansion
function is not a tight upper bound on the true trend in source vs target variation. A better model for
expansion function would be piecewise linear function with 2 slopes, one for variation values near 0
and one for larger variation values since the slopes at points where source variation is closer to 0 is
much larger than the slopes computed at points further from 0.

Figure 9: Expansion function from StAdv ((cid:15) = 0.03) source threat model to (cid:96)∞ ((cid:15) = 8/255), (cid:96)2
((cid:15) = 0.5), and StAdv ((cid:15) = 0.05) target threat models. Expansion function is computed using 60
ResNet-18 models adversarially trained on CIFAR-10 with adversarial examples generated via StAdv
((cid:15) = 0.03). The linear expansion function with minimum slope is plotted in blue.

27

0.02.55.07.5SourceVariation0510TargetVariation‘∞,(cid:15)=8/255Sources(x)=1.29x0.00.51.0SourceVariation0510TargetVariation‘2,(cid:15)=0.5Sources(x)=14.32x0246SourceVariation050100150TargetVariation‘∞,(cid:15)=8/255Targets(x)=28.61x0246SourceVariation01020TargetVariation‘2,(cid:15)=0.5Targets(x)=3.64x0246SourceVariation051015TargetVariationStAdv,(cid:15)=0.05Targets(x)=2.80xFigure 10: Robust accuracy on the CIFAR-10 test set of ResNet-18 models trained using AT-
VR at varied regularization strength λ measured on adversarial examples of generated by target
perturbations with radius (cid:15). The generalization gap is measured with respect to
(cid:96)p, p =
, 2, 1
}
cross entropy loss. Variation regularization is applied on the logits. All models are trained with
source (cid:96)∞ perturbations of radius 8

{∞

255 .

E.4 Additional Results with (cid:96)p Target Threat Models

In Figure 1 of the main text, we plotted the unforeseen generalization gap and robust accuracies for
ResNet-18 models trained on CIFAR-10 with AT-VR at various perturbation size (cid:15) with (cid:96)∞ source.
We plot the robust accuracy across (cid:96)p threat models for the models trained with standard adversarial
training (λ = 0) and with highest variation regularization strength used (λ = 0.5) in Figure 10. We
ﬁnd that at large values of (cid:96)p perturbation size, the model using variation regularization achieves
higher robust accuracy than the model trained using standard adversarial training. This improvement
is most clear for (cid:96)∞ targets.

We provide corresponding plots of unforeseen generalization gap and robust accuracy on CIFAR-10
on (cid:96)p target threat models for an (cid:96)2 source in Figure 11. Similar to trends with (cid:96)∞ source threat model,
we ﬁnd that increasing the strength of variation regularization decreases the unforeseen generalization
gap. We ﬁnd that with an (cid:96)2 source threat model, the robust accuracy for the model trained with
variation regularization also has consistently higher accuracy across target (cid:96)p threat models compared
to the model trained with standard adversarial training (λ = 0).

Figure 11: Top row: Generalization gap of on the CIFAR-10 test set for ResNet-18 models trained
using AT-VR at varied regularization strength λ measured on adversarial examples of generated by
perturbations with radius (cid:15). Variation regularization is applied on the logits.
target (cid:96)p, p =
The generalization gap is measured with respect to cross entropy loss. All models are trained with
source (cid:96)2 perturbations of radius 0.5. Bottom row: Corresponding robust accuracy of λ = 0 and
λ = 1 models displayed in top row.

, 2, 1
}

{∞

28

0.040.060.08Target(cid:15)02040Accuracy(%)Target‘∞0.51.01.52.0Target(cid:15)0204060Accuracy(%)Target‘25101520Target(cid:15)0204060Accuracy(%)Target‘1λ=0λ=0.50.040.060.08Target(cid:15)51015LT−ˆLSTarget‘∞0.51.01.52.0Target(cid:15)0246LT−ˆLSTarget‘25101520Target(cid:15)0123LT−ˆLSTarget‘1λ=0λ=0.25λ=0.5λ=0.75λ=10.040.060.08Target(cid:15)020Accuracy(%)Target‘∞0.51.01.52.0Target(cid:15)0204060Accuracy(%)Target‘25101520Target(cid:15)0204060Accuracy(%)Target‘1λ=0λ=1E.5 Additional strengths of variation regularization

In Table 6 we present additional results for models trained at AT-VR at different strengths λ of varia-
tion regularization. Overall, we ﬁnd that models using variation regularization improve robustness on
unforeseen attacks. Generally, we ﬁnd that union accuracies are also larger with higher strengths of
variation regularization.

Union with Source
StAdv

Dataset

Architecture

Source

λ

ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
CIFAR-10
ResNet-18
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
ImageNette
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100
CIFAR-100

VGG-16
VGG-16
VGG-16
VGG-16
VGG-16
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞

0
0.25
0.5
0.75
1
0
0.05
0.1
0.3
0.5
0
0.1
0.3
0.5
0.7
0
0.01
0.05
0.1
0.15
0
0.25
0.5
0.75
1
0
0.05
0.1
0.15
0.2
0
0.25
0.5
0.75
1
0
0.05
0.1
0.2
0.3

Clean
acc
88.49
87.49
85.75
84.78
85.21
82.83
83.34
81.94
79.36
72.91
85.93
84.82
83.47
80.43
72.73
79.67
78.12
79.24
77.80
76.19
88.94
86.83
86.80
85.83
85.22
80.56
79.64
78.01
75.62
73.50
60.92
56.20
54.23
51.53
50.85
54.94
53.59
52.32
48.97
46.39

Source
acc
66.65
68.66
68.93
67.86
67.38
47.47
48.04
48.64
49.28
48.84
49.86
50.42
51.19
51.16
49.94
44.36
45.13
44.73
45.42
44.65
84.99
84.28
84.00
83.92
83.08
49.63
50.50
50.80
49.94
49.17
36.01
38.26
38.51
38.26
37.00
22.74
24.24
24.78
25.04
24.82

(cid:96)∞
(cid:15) = 12
255
6.44
11.59
13.42
14.27
13.43
28.09
29.28
29.50
31.94
33.69
28.73
30.10
31.71
33.18
35.11
26.14
27.22
27.07
28.41
27.44
0.00
1.58
4.25
5.81
9.53
32.38
33.27
35.57
36.15
36.28
3.98
7.85
9.70
11.47
10.53
12.61
14.05
15.09
16.48
16.45

(cid:96)2
(cid:15) = 1
34.72
39.06
41.90
41.68
40.74
24.94
24.34
24.15
25.08
24.38
20.89
20.85
20.01
20.76
22.30
30.82
33.34
30.99
32.08
29.65
79.08
80.66
80.23
80.94
80.43
49.63
50.50
50.80
49.94
49.15
16.90
23.43
24.33
25.65
24.89
14.40
14.93
15.57
15.82
14.54

Re-
color
66.52
68.52
68.66
67.75
67.30
47.47
48.04
48.64
49.28
48.84
49.86
50.42
51.19
51.16
49.94
44.36
45.13
44.73
45.42
44.65
72.15
74.17
74.17
74.73
75.26
49.63
50.50
50.80
49.94
49.17
34.87
36.63
36.87
36.96
35.82
22.71
24.19
24.74
24.95
24.75

Union
all
0.33
6.11
9.25
10.58
11.77
2.48
2.29
3.13
8.28
12.59
1.10
3.42
9.81
12.06
14.72
4.35
5.24
4.65
6.83
7.48
0.00
0.94
2.88
4.28
6.80
25.68
28.48
31.82
33.22
34.22
0.40
1.34
2.29
3.11
3.05
2.42
2.65
2.91
3.48
3.07

0.76
10.78
20.13
23.45
34.40
4.38
4.32
6.01
12.75
18.62
2.28
5.54
15.00
21.05
25.33
7.31
8.09
7.94
10.57
11.99
1.27
8.38
12.82
15.57
18.04
34.27
39.01
42.37
43.26
44.00
1.80
2.88
4.03
5.12
5.09
3.99
4.26
4.25
4.96
5.04

Table 6: Robust accuracy of various models trained at different strengths of variation regularization
applied on logits on various threat models. Models are trained with either source threat model (cid:96)∞
with radius 8
255 or (cid:96)2 with radius 0.5. The “source acc" column reports the accuracy on the source
attack. For each individual threat model, we evaluate accuracy on a union with the source threat
model. The union all column reports the accuracy obtained on the union across all listed threat
models.

E.6 Full AutoAttack results on CIFAR-10

In Table 7, we report the full AutoAttack evaluation of the ResNet-18 models trained on CIFAR-10
with AT-VR with highest regularization strength displayed in in Table 13. We ﬁnd that robust accuracy
is consistent across attack types, suggesting that variation regularization is not obfuscating gradients.

29

Source

51.56
48.84
48.84
48.84
68.00
67.39
67.38
67.38

255 )

(cid:96)∞
((cid:15) = 12
37.84
33.72
33.69
33.69
18.23
14.73
13.43
13.43

(cid:96)2
((cid:15) = 1)
30.56
24.93
24.38
24.38
44.16
41.67
40.74
40.74

APGD-CE
APGD-T
FAB-T
Square
APGD-CE
APGD-T
FAB-T
Square

(cid:96)∞ source
λ = 0.5

(cid:96)2 source
λ = 1

Table 7: Full AutoAttack evaluations for the ResNet-18 models trained with AT-VR with variation
regularization strength on (cid:96)∞ ((cid:15) = 8

255 ) and (cid:96)2 ((cid:15) = 0.5) source adversaries.

E.7 Evaluations on other adversaries

In Table 8, we present additional evaluations for ResNet-18 CIFAR-10 models on additional adver-
saries including Wasserstein adversarial attacks, JPEG attacks, elastic attacks, and perceptual attacks.
For Wasserstein adversarial attacks, we use PGD with dual projection (Wu et al., 2020b). We use (cid:96)∞
JPEG, (cid:96)1 JPEG and elastic attacks from (Kang et al., 2019) and AlexNet LPIPS-based attacks (PPGD
and LPA) from (Laidlaw et al., 2021) with perturbation size (cid:15) speciﬁed in Table 8. Overall, we ﬁnd
that using variation regularization with (cid:96)p sources also improves performance on these additional
adversaries. This suggests that an expansion function exists between variation on (cid:96)p sources to more
complicated threat models, including more perceptually-aligned threat models such as the bounded
AlexNet LPIPS distance used by PPGD and LPA attacks.

Union with Source

λ

Source

Wasserstein

0
1
0
0.5

(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞

Clean
acc
88.49
85.21
82.83
72.91

Source
acc
66.65
67.38
47.47
48.84

(cid:15) = 0.007
45.80
48.32
26.03
29.50

LPA
(cid:15) = 0.5
0.49
4.12
0.28
2.18
Table 8: Robust accuracy of ResNet-18 models on CIFAR-10 evaluated on additional adversaries
including Wasserstein adversaries (Wu et al., 2020b), JPEG compression adversaries and elastic
adversary (Kang et al., 2019), and AlexNet LPIPS perceptual adversaries (Laidlaw et al., 2021).
Perturbation size (cid:15) is speciﬁed for each threat model. Accuracies on each target adversary reported
are given with a union computed on the source.

(cid:96)∞ JPEG
(cid:15) = 0.125
48.73
56.99
34.45
36.63

Elastic
(cid:15) = 0.25
13.49
25.67
25.23
29.62

(cid:96)1 JPEG
(cid:15) = 64
6.42
21.75
2.32
7.25

PPGD
(cid:15) = 0.5
3.65
24.21
2.44
4.96

(cid:15) = 0.01
31.75
47.58
16.62
20.89

E.8 Comparison to training with all attacks

In this section, we compare to the MAX adversarial training approach from Tramèr & Boneh (2019)
which trains directly on all target threat models of interest. Since MAX training uses information
about the target threat model for training while our approach does not, the union accuracies achieved
via MAX training should be viewed as an upper bound on performance. For models using MAX
training, we train for a total of 100 epochs and evaluate performance on the model saved at the ﬁnal
epoch. We compare robust accuracies for MAX training to robust accuracies of models trained with
variation regularization from Table 1. For fair comparison, we report robust accuracies of models
trained with variation regularization measured through evaluating on the target threat model (instead
of the union of target with source threat model as in Table 1. We provide results in Table 9. We
note that in general, training with the union of all attacks achieves more balanced accuracies across
threat models. The only exception is with WRN-28-10 for which MAX training achieves lower union
accuracy and StAdv accuracy; however, this may be due to using only 100 epochs of training.

E.9 Combining variation regularization with TRADES

In the main paper, we provided experiments with variation regularization combined with PGD
adversarial training (Madry et al., 2018) on (cid:96)∞ and (cid:96)2 source threat models. We note that variation
regularization is not exclusive to PGD adversarial training and can be applied in conjunction with other
adversarial training based techniques including TRADES (Zhang et al., 2019). We present results

30

λ

StAdv

Source

Dataset

Architecture

MAX
(cid:96)2
(cid:96)∞

ResNet-18
ResNet-18
ResNet-18

CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10 WRN-28-10 MAX
CIFAR-10 WRN-28-10
CIFAR-10
CIFAR-10
ImageNette
ImageNette
ImageNette
CIFAR-100
CIFAR-100
CIFAR-100

Clean
acc
77.81
85.21
72.91
80.98
72.73
67.26
77.80
69.32
85.22
78.01
52.06
51.53
48.97
Table 9: Robust accuracy of various models trained at different strengths of VR applied on logits on
various threat models. Source of MAX represents the accuracies obtained by directly training on all
target threat models. The union all column reports the accuracy on the union across all listed threat
models.

(cid:96)∞
(cid:15) = 12
255
41.76
13.39
33.66
29.78
35.08
29.00
28.38
38.57
9.50
35.54
13.29
11.46
16.51

Union
all
27.23
11.27
10.58
4.98
13.26
26.12
5.38
37.25
5.83
31.03
5.04
2.26
2.59

(cid:96)2
(cid:15) = 1
36.74
40.79
24.34
38.25
22.35
41.48
32.08
64.36
80.43
72.94
19.35
25.64
15.81

Re-
color
63.99
60.07
47.38
57.60
49.01
59.11
62.24
59.34
44.99
64.15
26.42
18.98
24.01

VGG-16
VGG-16
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18

38.90
32.15
16.09
9.83
23.15
42.98
11.96
57.04
16.25
51.75
8.01
4.17
4.70

0
1
0.5
0
0.7
0
0.1
0
1
0.1
0
0.75
0.2

(cid:96)∞
MAX
(cid:96)∞
MAX
(cid:96)2
(cid:96)∞
MAX
(cid:96)2
(cid:96)∞

for TRADES-VR with TRADES hyperparameter β = 6.0 in Table 10. Interestingly, we ﬁnd that
compared to PGD adversarial training results in Table 1, TRADES generally has better performance
on larger (cid:96)∞ and (cid:96)2 threat models (on par with AT-VR with PGD). We ﬁnd that across architectures
and datasets applying variation regularization over TRADES adversarial training generally improves
robustness on unforeseen (cid:96)∞ and StAdv threat models, but trades off clean accuracy, source accuracy,
and unforeseen (cid:96)2 target accuracy. Despite this trade-off, we ﬁnd that TRADES-VR consistently
improves on the accuracy across the union of all threat models in comparison to standard TRADES.

Union with Source
StAdv

Dataset

Architecture

Source

λ

ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
ResNet-18
ImageNette
ResNet-18
ImageNette
ResNet-18
ImageNette
ResNet-18
ImageNette
ResNet-18
CIFAR-100
ResNet-18
CIFAR-100
ResNet-18
CIFAR-100
ResNet-18
CIFAR-100

(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞

0
2
0
0.5
0
1
0
2
0
0.2
0
1
0
0.5

Clean
acc
86.79
79.81
82.67
79.11
84.73
75.99
88.66
83.80
78.32
73.83
58.71
53.44
53.80
51.16

Source
acc
68.99
64.84
49.15
48.98
52.09
50.09
85.55
82.27
50.62
48.79
37.79
37.71
23.02
24.39

(cid:96)∞
(cid:15) = 12
255
13.13
16.04
31.00
32.08
32.74
33.96
0.08
14.37
35.54
36.15
6.73
10.09
13.77
15.93

(cid:96)2
(cid:15) = 1
43.58
42.46
28.04
26.54
24.68
21.76
80.71
80.10
50.62
48.79
21.40
24.55
15.21
14.33

Re-
color
68.85
64.76
49.15
48.98
52.09
50.09
72.74
75.03
50.62
48.79
36.74
36.59
22.99
24.34

Union
all
1.00
14.67
3.64
11.78
2.98
15.08
0.03
11.75
33.32
34.62
1.21
2.40
3.33
3.40

1.89
42.05
5.32
16.21
4.54
25.09
1.94
27.08
44.03
44.97
2.72
4.05
4.94
4.87

Table 10: Robust accuracy of various models trained with TRADES-VR at different strengths of
variation regularization applied on logits on various threat models. Models are trained with either
source threat model (cid:96)∞ with radius 8
255 or (cid:96)2 with radius 0.5. The “source acc" column reports the
accuracy on the source attack. For each individual threat model, we evaluate accuracy on a union
with the source threat model. The union all column reports the accuracy obtained on the union across
all listed threat models.

E.10 Combining variation regularization with other sources

To demonstrate that variation regularization can be applied with source threat models outside of (cid:96)p
balls, we evaluate the performance of AT-VR with other sources including StAdv and Recolor.

E.10.1 Computing variation with StAdv and Recolor sources

In StAdv (Xiao et al., 2018), adversarial examples are generated by optimizing for a per pixel ﬂow
ﬁeld f , where fi corresponds to the displacement vector of the ith pixel of the image. This ﬂow ﬁeld

31

Union with Source
StAdv

λ

Source

0
0.5
1
0
0.5
1

StAdv
StAdv
StAdv
Recolor
Recolor
Recolor

Clean
acc
86.94
83.88
81.24
94.88
94.18
94.13

Source
acc
54.04
60.11
62.78
39.11
71.36
72.92

(cid:96)2
(cid:15) = 0.5
5.63
5.66
9.97
3.07
19.22
20.03

(cid:96)∞
(cid:15) = 4
255
3.57
3.29
5.83
5.38
25.39
26.81
Table 11: Robust accuracy of ResNet-18 models trained using AT-VR with StAdv and Recolor source
threat models with variation regularization applied on logits on various threat models. During training
we use 0.03 and 0.04 as the perturbation bounds for StAdv and Recolor respectively. During testing
we use 0.05 for StAdv and 0.06 for Recolor. The “source acc" column reports the accuracy on the
source attack. For each individual threat model, we evaluate accuracy on a union with the source
threat model. The union all column reports the accuracy obtained on the union across all listed threat
models.

Union
all
0.96
2.39
4.44
0.00
0.03
0.17

Re-
color
3.27
8.60
13.10
21.75
64.39
66.02

12.62
24.64
31.09
0.00
0.06
0.20

is obtained by solving:

arg min
f

(cid:96)adv(x, f ) + τ (cid:96)ﬂow(f )

(3)

where (cid:96)adv is the CW objective (Carlini & Wagner, 2017) and (cid:96)f low is a regularization term minimizing
spatial transformation distance to ensure that the perturbation is imperceptible. τ controls the strength
of this regularization.

We adapt this objective to compute variation, replacing (cid:96)adv with the variation objective. Rather than
solving for a single ﬂow ﬁeld, we solve

(h, StAdvτ ) = max

h(f1(x))

f1,f2 ||

V

h(f2(x))

2
||

−

−

τ ((cid:96)ﬂow(f1) + (cid:96)ﬂow(f2))

Here f1(x) and f2(x) denote the perturbed input after applying f1 and f2 respectively and h denotes
our feature extractor. The second term ensures that both f1 and f2 have small spatial transformation
distance. We solve the optimization problem using PGD.

In Recolor attacks (Laidlaw & Feizi, 2019), the objective function takes the same form as Equation 3
where f is now a color perturbation function. We optimize for adversarial examples for Recolor in
the same way as for StAdv, but incorporate an additional clipping step to ensure that perturbations of
each color are within the speciﬁed bounds.

E.10.2 Experimental setup details for StAdv and Recolor sources

We train ResNet-18 models using AT-VR with StAdv and Recolor sources. For these models, we
train with source perturbation bound of 0.03 for StAdv and 0.04 for Recolor attacks. We use 10
iterations for both StAdv and Recolor during training. We train for a maximum of 100 epochs and
evaluate on the model saved at epoch achieving the highest source accuracy. For evaluation, we use
StAdv perturbation bound of 0.05 and Recolor perturbation bound of 0.06 and use 100 iterations to
generate adversarial examples for both attacks. Additionally, we evaluate on (cid:96)∞ and (cid:96)2 attacks with
radius 4

255 and 0.5 respectively using AutoAttack.

E.10.3 Experimental Results for StAdv and Recolor sources

We report results in Table 11. Similar to trends observed with (cid:96)p sources, we ﬁnd that using variation
regularization improves unforeseen robustness when using StAdv and Recolor sources. For example,
with StAdv source, using variation regularization with λ = 1 increases robust accuracy on unforeseen
(cid:96)2 attacks from 3.57% to 5.83%, and with Recolor source, using variation regularization with λ = 1
increases robust accuracy on unforeseen (cid:96)2 attacks from 3.07% to 20.03%. The largest increase is for
attacks of the same perturbation type but larger radius; for example, for StAdv source ((cid:15) = 0.03) on
unforeseen StAdv target ((cid:15) = 0.05), the robust accuracy increases from 12.62% to 31.09%. Similarily,
for Recolor source ((cid:15) = 0.04) on unforeseen Recolor target ((cid:15) = 0.06), the robust accuracy increases
from 21.75% to 66.02%. Interestingly, we also ﬁnd that using variation regularization with StAdv and
Recolor sources leads to a signiﬁcant increase in source accuracy as well. For StAdv, source accuracy
increases from 54.04% without variation regularization to 62.78% with variation regularization at

32

λ = 1. For Recolor source, this increase is even larger; source accuracy increases from 39.11%
without variation regularization to 72.92% with variation regularization.

Further inspecting the source accuracy of the models trained Recolor source, we ﬁnd that without
variation regularization, the model overﬁts to the 10-iteration attack used during training. When
evaluated with 10-iteration Recolor, the standard adversarial training (λ = 0) model achieves 86.62%
robust accuracy, but when 100 iterations of the attack is used during testing, the model’s accuracy
drops to 39.11%. Interestingly, variation regularization helps prevent adversarial training from
overﬁtting to the 10-iteration attack, causing the resulting source accuracy on the models with
variation regularization to be signiﬁcantly higher.

E.11 Computational complexity of AT-VR

One limitation of AT-VR is that it can be 3x as expensive compared to adversarial training. This
is because the computation for variation also requires gradient based optimization. We note that
this computational expense occurs when we use the same number of iterations of PGD for variation
computation as standard adversarial training. In this section, we study whether we can use fewer
iterations of PGD for generating the adversarial example and computing variation. We present our
results for training ResNet-18 on CIFAR-10 with source threat model (cid:96)2, (cid:15) = 0.5 using AT-VR
and standard AT (λ = 0) in Table 12. We ﬁnd that even with a single iteration, AT-VR is able to
signiﬁcantly improve union accuracy over standard AT. For (cid:96)∞ adversarial training, we ﬁnd that a

Union with Source

λ

0
0
0
1

PGD Clean
iters
1
3
10
1

acc
89.00
88.72
88.49
86.88

Source
acc
66.53
67.58
66.65
67.00

255 )

(cid:96)∞
((cid:15) = 12
5.54
7.07
6.44
11.52

(cid:96)2
((cid:15) = 1)
31.55
35.47
34.72
37.24

StAdv Recolor Union all

0.26
0.55
0.76
38.34

33.43
36.41
66.52
64.44

0.05
0.18
0.33
10.09

Table 12: Robust accuracy of ResNet-18 models trained on CIFAR-10 using AT-VR with a single
PGD iteration (λ = 1, PGD iters=1) in comparison to standard adversarial training (λ = 0) with
various numbers of PGD iterations.

single iteration of training does not work well due to the poor performance of adversarial training
with FGSM.

F Additional Results for Feature Level AT-VR

In the main paper, we provide results for AT-VR with variation regularization applied at the layer of
the logits. In terms of our theory, this would correspond to considering the identity function to be the
top level classiﬁer. In this section, we consider the top level classiﬁer to be all fully connected layers
at the end of the NN architectures used and evaluate variation regularization applied at the input into
the fully connected classiﬁer.

F.1 Expansion function for variation on features

In Figure 12, we plot the minimum linear expansion function computed on 315 adversarially trained
feature extractors (analogous to Figure 2 in the main text). Additionally, we plot the minimum linear
expansion function on 300 randomly initialized feature extractors. For random initialization, we use
Xavier normal initialization (Glorot & Bengio, 2010) for weights and standard normal initialization
for biases. We ﬁnd that we can ﬁnd a linear expansion function with small slope across (cid:96)∞ and (cid:96)2
source and target pairs. In comparison to expansion function for variation computed at the logits, we
ﬁnd that the slope of the expansion function s found is similar.

F.2 Additional results with (cid:96)p target threat models

We present the unforeseen generalization gap for ResNet-18 models on CIFAR-10 trained with source
(cid:96)∞ threat model with radius 8
255 at various strengths of variation regularization and the corresponding

33

Figure 12: Plots of minimum linear expansion function s shown in blue computed on A) 315
adversarially trained feature extractors and B) 300 randomly initialized feature extractors. Each grey
point represents the variation measured on the source and target attack. The two columns represent
the source adversary ((cid:96)∞ and (cid:96)2 respectively). The two rows represent the target adversary ((cid:96)∞ and
(cid:96)2 respectively).

robust accuracy of the model trained with standard AT (λ = 0) and highest variation regularization
(λ = 2) in Figure 13. We ﬁnd at large values of unforeseen (cid:15), the model trained with variation
regularization achieve both smaller unforeseen generalization gap and higher robust accuracy.

Figure 13: Top row: Unforeseen generalization gap of on the CIFAR-10 test set for ResNet-18
models trained using AT-VR at varied regularization strength λ measured on adversarial examples of
perturbations with radius (cid:15). The generalization gap is measured
generated by target (cid:96)p, p =
with respect to cross entropy loss. All models are trained with source (cid:96)∞ perturbations of radius 8
255 .
Bottom row: Corresponding robust accuracy of λ = 0 and λ = 2 models displayed in top row.

, 2, 1
}

{∞

We repeat experiments with ResNet-18 models on CIFAR-10 trained with source (cid:96)2 threat model
with radius of 0.5. We report the measured unforeseen generalization gap to (cid:96)∞, (cid:96)2, and (cid:96)1 target
threat models at different radii (measured via cross entropy loss on adversarial examples generated
with APGD) along with corresponding robust accuracy of the no regularization and maximum
regularization strength models in Figure 14. We ﬁnd that trends observed when the source threat
model was (cid:96)∞ are consistent with the trends for (cid:96)2 source threat model: increasing the strength of
variation regularization decreases the size of the unforeseen generalization gap and increases robust
accuracy across various (cid:96)p target threat models.

34

A)B)0.040.060.08Target(cid:15)024LT−ˆLSTarget‘∞0.51.01.52.0Target(cid:15)024LT−ˆLSTarget‘25101520Target(cid:15)0123LT−ˆLSTarget‘1λ=0λ=0.1λ=0.5λ=1λ=20.040.060.08Target(cid:15)02040Accuracy(%)Target‘∞0.51.01.52.0Target(cid:15)0204060Accuracy(%)Target‘25101520Target(cid:15)0204060Accuracy(%)Target‘1λ=0λ=2Figure 14: Top row: Unforeseen generalization gap of on the CIFAR-10 test set for ResNet-18
models trained using AT-VR at varied regularization strength λ measured on adversarial examples of
generated by target (cid:96)p, p =
perturbations with radius (cid:15). The generalization gap is measured
with respect to cross entropy loss. All models are trained with source (cid:96)2 perturbations of radius 0.5.
Bottom row: Corresponding robust accuracy of λ = 0 and λ = 5 models displayed in top row.

, 2, 1
}

{∞

F.3 Robust accuracies with feature level AT-VR

We repeat experiments corresponding to Table 1 in the main paper for models trained with feature
level AT-VR in Table 13. We observe similar trends with variation regularization applied at the
features instead of logits: variation regularization improves unforeseen accuracy and generally
improves source accuracy, but trades off performance on clean images.

35

0.040.060.08Target(cid:15)51015LT−ˆLSTarget‘∞0.51.01.52.0Target(cid:15)0246LT−ˆLSTarget‘25101520Target(cid:15)0123LT−ˆLSTarget‘1λ=0λ=0.1λ=0.5λ=1λ=50.040.060.08Target(cid:15)0102030Accuracy(%)Target‘∞0.51.01.52.0Target(cid:15)0204060Accuracy(%)Target‘25101520Target(cid:15)0204060Accuracy(%)Target‘1λ=0λ=5Union with Source
StAdv

Recolor

Union all

Dataset

Architecture

Source

λ

Clean
acc
88.49
86.87
84.75
82.83
79.72
75.58
85.93
85.86
84.27
79.67
76.38
72.27
88.94
86.29
83.06
80.56
79.06
78.09
60.92
56.37
52.73
54.94
54.21
49.29

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)∞
(cid:96)∞
(cid:96)∞

0
2.0
5.0
0
1.0
2.0
0
0.5
1
0
0.01
0.05
0
1.0
5.0
0
0.05
0.1
0
1
2
0
0.1
0.5

0.33
ResNet-18
CIFAR-10
ResNet-18
CIFAR-10
6.16
10.83
ResNet-18
CIFAR-10
2.48
ResNet-18
CIFAR-10
7.09
ResNet-18
CIFAR-10
11.44
CIFAR-10
ResNet-18
1.10
CIFAR-10 WRN-28-10
CIFAR-10 WRN-28-10
4.14
7.59
CIFAR-10 WRN-28-10
4.35
CIFAR-10
5.69
CIFAR-10
8.02
CIFAR-10
0.00
ImageNette
1.45
ImageNette
7.75
ImageNette
25.68
ImageNette
28.89
ImageNette
29.30
ImageNette
0.40
CIFAR-100
2.10
CIFAR-100
3.14
CIFAR-100
2.42
CIFAR-100
CIFAR-100
2.54
3.70
CIFAR-100
Table 13: Robust accuracy of various models trained at different strengths of variation regularization
8
on various threat models. Models are trained with either source threat model (cid:96)∞ with radius
255
or (cid:96)2 with radius 0.5. The “source acc" column reports the accuracy on the source attack. For each
individual threat model, we evaluate accuracy on a union with the source threat model. The union all
column reports the accuracy obtained on the union across all listed threat models.

VGG-16
VGG-16
VGG-16
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18

0.76
10.01
29.20
4.38
11.38
16.56
2.28
5.36
9.71
7.31
9.14
12.18
1.27
8.66
22.98
34.27
37.40
38.32
1.80
4.81
7.46
3.99
4.10
5.74

66.52
68.06
66.84
47.47
49.43
48.35
49.86
50.13
51.01
44.36
44.87
42.14
72.15
73.25
74.22
49.63
50.47
50.01
34.87
37.52
35.28
22.71
23.48
24.58

255 )

(cid:96)∞
((cid:15) = 12
6.44
12.66
13.29
28.09
32.57
32.19
28.73
30.04
31.47
26.14
27.35
26.80
0.00
2.55
10.11
32.38
34.06
34.11
3.98
8.65
8.76
12.61
13.61
16.02

(cid:96)2
((cid:15) = 1)
34.72
41.05
40.71
24.94
26.64
26.89
20.89
21.62
22.86
30.82
32.59
32.41
79.08
80.20
78.60
49.63
50.47
50.01
16.90
23.41
22.33
14.40
15.10
15.62

Source
acc
66.65
68.24
66.93
47.47
49.43
48.35
49.86
50.13
51.01
44.36
44.87
42.14
84.99
83.62
80.89
49.63
50.47
50.01
36.01
38.66
36.15
22.74
23.52
24.66

36

