0
2
0
2

p
e
S
7
2

]

R
G
.
s
c
[

2
v
8
6
9
5
0
.
2
0
0
2
:
v
i
X
r
a

Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling

Dongbo Zhang
Beihang University
Beijing, China
zhangdongbo9212@163.com

Hong Qin
Stony Brook University
New York, USA
qin@cs.stonybrook.edu

Xuequan Lu
Deakin University
Australia
xuequan.lu@deakin.edu.au

Ying He
Nanyang Technological University
Singapore
YHe@ntu.edu.sg

ABSTRACT
Point cloud filtering is a fundamental problem in geometry model-
ing and processing. Despite of significant advancement in recent
years, the existing methods still suffer from two issues: 1) they are
either designed without preserving sharp features or less robust in
feature preservation; and 2) they usually have many parameters
and require tedious parameter tuning. In this paper, we propose
a novel deep learning approach that automatically and robustly
filters point clouds by removing noise and preserving their sharp
features. Our point-wise learning architecture consists of an en-
coder and a decoder. The encoder directly takes points (a point and
its neighbors) as input, and learns a latent representation vector
which goes through the decoder to relate the ground-truth posi-
tion with a displacement vector. The trained neural network can
automatically generate a set of clean points from a noisy input.
Extensive experiments show that our approach outperforms the
state-of-the-art deep learning techniques in terms of both visual
quality and quantitative error metrics. The source code and dataset
can be found at https://github.com/dongbo-BUAA-VR/Pointfilter.

CCS CONCEPTS
• Theory of computation → Computational geometry; • Com-
puting methodologies → Shape modeling; Point-based mod-
els;

KEYWORDS
Automatic point cloud filtering, deep learning, autoEncoder, feature-
preserving.

1 INTRODUCTION
As the output of 3D scanning processes, point clouds are widely
used in geometry processing, autonomous driving and robotics.
Many point clouds, especially the ones obtained from consumer-
level depth sensors, are corrupted with noise. Therefore, point
cloud filtering is a necessary preprocessing step for the downstream
applications.

The last two decades have witnessed significant progress in point
cloud filtering. Many elegant algorithms have been proposed. The
LOP (Locally Optimal Projection) methods including LOP [Lipman
et al. 2007], WLOP (weighted LOP) [Huang et al. 2009] and CLOP
(continuous LOP) [Preiner et al. 2014]), are robust to noise and
outliers. RIMLS (robust implicit moving least squares) [Öztireli
et al. 2009] and GPF (GMM-inspired Feature-preserving Point Set

Filtering) [Lu et al. 2018] can preserve sharp features. Nevertheless,
these techniques still suffer from either feature smearing or less
robustness in filtering. For example, the LOP family [Huang et al.
2009; Lipman et al. 2007; Preiner et al. 2014] are not designed for
preserving sharp features, because of their inherent isotropic nature.
RIMLS [Öztireli et al. 2009] and GPF [Lu et al. 2018] depend heavily
on normal filters and are not robust for large noise or non-uniform
sampling. Furthermore, it is not easy for GPF [Lu et al. 2018] to find
a proper radius that balances noise removal and gaps near edges,
and it is also slow due to high computational cost of Expectation-
Maximization (EM) optimization. Finally, they all need careful trial-
and-error parameter tuning which is tedious and time-consuming.
Deep learning has proven highly effective in processing regular
data, such as images and videos [He et al. 2015; Simonyan and
Zisserman 2015]. However, it is non-trivial to apply deep learning
techniques to irregular data, such as point clouds. To our knowl-
edge, point cloud filtering with resorting to deep learning has been
rarely studied so far, such as EC-Net [Yu et al. 2018a], PCN [Rako-
tosaona et al. 2019] and TotalDenoising [Hermosilla et al. 2019].
These data-driven approaches avoid the tedious parameter tuning,
however, they still generate limited results with either smooth-
ing out sharp features or poor generalization, which diminishes
their usages in real-world applications. In this paper, we propose a
novel deep learning approach to overcome the above issues. Mo-
tivated by the successes of AutoEncoder and PointNet [Charles
et al. 2017; Schmidhuber 2015], we design an end-to-end neural
network, called Pointfilter, for point cloud filtering. Our network
is an encoder-decoder based architecture which straightforwardly
takes the raw neighboring points of each noisy point as input, and
regresses a displacement vector to push this noisy point back to its
ground truth position. In designing the loss function, we also take
sharp features into account so that sharp features can be preserved
by this network. Given a noisy point cloud as input, our trained
model can automatically and robustly predict a corresponding clean
point cloud, by removing noise and preserving sharp features. Var-
ious experiments demonstrate that our method achieves better
performance than the state-of-the-art techniques (or comparable
to optimization based methods like RIMLS and GPF which require
decent normals and trial-and-error parameter tuning), in terms of
visual quality and error metrics. Our method is fast and avoids man-
ual parameter tuning. We show the main features of the existing
techniques and our method in Table 1. The main contributions of
this paper are:

 
 
 
 
 
 
Table 1: Comparison of the main characteristics (CHs) between the state-of-the-art point set filtering techniques and our
method. For the normal independence, we only consider the inference stage for learning-based techniques. ✓ and × denote
YES and NO, respectively. ✓∖ indicates an “intermediate” level between YES and NO.

CHs

Methods
LOP [Lipman et al. 2007] &
WLOP [Huang et al. 2009]
CLOP [Preiner et al. 2014]
RIMLS [Öztireli et al. 2009]
GPF [Lu et al. 2018]
EC-Net [Yu et al. 2018a]
PCN [Rakotosaona et al. 2019]
TotalDenoising [Hermosilla et al. 2019]
Ours

normal
independence

noise outliers

Feature
aware

Parameters Speed

✓

✓
×
×
✓
✓
✓
✓

✓

✓
✓
✓
✓
✓
✓
✓

✓∖

✓∖
×
✓∖
×
✓
×
✓∖

×

×
✓
✓
✓
×
×
✓

×

×
×
×
✓
✓
✓
✓

✓∖

✓
✓∖
×
✓
✓∖
✓
✓

• a novel end-to-end neural network that achieves point cloud

filtering by encoder-decoder modeling;

• an effective loss function that takes sharp features into ac-

count.

We will make the source code and dataset publicly available.

2 RELATED WORK
We first review the methods for point cloud filtering, and then
discuss the existing deep learning techniques for point clouds.

Point Cloud Filtering

2.1
Point cloud filtering can be generally classified into two types:
two-step based techniques and projection-based methods.

Two-step based Methods. The two-step based framework con-
sists of at least two steps: normal smoothing and point position
update under the guidance of the filtered normals. To preserve
sharp features, Avron et al. [Avron et al. 2010] and Sun et al. [Sun
et al. 2015] introduced L1 and L0 optimization for point set filter-
ing, respectively. Recently, with sharp feature skeletons, a point
cloud smoothing technique [Zheng et al. 2017] was presented based
on the guided filter and was extended to point clouds via a multi-
normal strategy. Zheng et al. [Zheng et al. 2018] extended the rolling
guidance filter to point set filtering and designed a new point posi-
tion updating strategy to overcome sharp edge shrinkage. Lu et al.
[Lu et al. 2018] proposed a two-step geometry filtering approach
for both meshes and point clouds. Although these two-step meth-
ods achieve promising results in point cloud filtering, they rely
highly on normal estimation which is typically sensitive to heavy
noise and non-uniform sampling. Most point set filtering methods
achieve filtered results through projecting the input point set onto
the underlying point set surface.

Projection-based Methods. One popular category of this type
is moving least squares and its variants [Alexa et al. 2003, 2001;
Amenta and Kil 2004; Fleishman et al. 2005; Levin 1998; Levin 2004;
Öztireli et al. 2009]. The moving least squares (MLS) has been semi-
nally formulated by Levin [Levin 1998; Levin 2004]. Some works
defined moving least squares (MLS) and extremal surfaces [Alexa
et al. 2003, 2001; Amenta and Kil 2004]. Later, two different variants

2

have been presented for projection: statistics-based and robust im-
plicit moving least squares (RIMLS) [Fleishman et al. 2005; Öztireli
et al. 2009]. Although RIMLS can preserve share features to some
extent, it is not easy to find a proper support radius, which makes
it difficult to noisy input. Lange et al. [Lange and Polthier 2005] de-
veloped a method for anisotropic fairing of a point sampled surface
using an anisotropic geometric mean curvature flow. Recently, the
LOP (locally optimal projection) based methods have become in-
creasingly popular. For example, Lipman et al. [Lipman et al. 2007]
proposed the locally optimal projection operator (LOP) which is pa-
rameterization free. Later, Huang et al. [Huang et al. 2009] presented
a weighted LOP (WLOP), which enhances the uniform distribution
of the input points. A kernel LOP has also been proposed to speed
up the computation of LOP [Liao et al. 2013]. More recently, a con-
tinuous LOP (CLOP) has been presented to reformulate the data
term to be a continuous representation of the input point set and
arrives at a fast speed [Preiner et al. 2014]. The LOP-based methods
cannot preserve sharp features/edges, due to lack of consideration
of sharp information. Note that a few projection-based methods
utilize smoothed normals as prior to preserve sharp features, such
as EAR [Huang et al. 2013] and GPF [Lu et al. 2018]. Since these
methods contain a number of parameters, careful trial-and-error
parameter tuning is required to obtain decent results, especially for
complex models.

2.2 Deep Learning on Point Clouds
Point-based Network Architecture. Qi et al. [Charles et al. 2017]
proposed the pioneering network architecture, named PointNet,
which can consume raw points without voxelization or rendering.
In contrast to the conventional CNNs that rely on convolution op-
erators, PointNet adopts the multi-layer perceptrons for feature
extraction, hereby working well for irregular domains. PointNet
is simple and elegant, and provides a unified framework for shape
classification and segmentation. However, PointNet processes the
points individually, and cannot characterize local structures which
are crucial for high-level semantic understanding. To overcome this
limitation, Qi et al. developed an improved version, PointNet++,
which aggregates local structures in a hierarchical way [Qi et al.

Figure 1: Pointfilter architecture: given a noisy patch with N points (generated by the preprocessing step), we use PCA for
alignment and feed the aligned patch into the neural network. Normalized input passes through the shared multi-layer per-
ceptrons (MLPs) to extract local features and then aggregates each point feature by a max pooling layer. The MLPs consist of 5
hidden layers with neuron sizes 64, 128, 256, 512, and 1024, respectively. Following the aggregated features, three fully connected
layers, with neuron sizes 512, 256, 3, are used to regress a displacement vector between the noisy point and the underlying sur-
face. All layers except the last one adopt BatchNorm and ReLU, whereas the last layer only uses the activation function tanh
to constrain the displacement vector space.

2017]. Following PointNet and PointNet++, lots of network archi-
tectures applied on raw point clouds emerged. For instance, based
on dynamic local neighborhood graph structure, Wang et al. [Wang
et al. 2019] designed an EdgeConv block to capture the relationships
both in spatial and feature space. At the same time, an alternative
convolutional framework, SpiderCNN [Xu et al. 2018], was pro-
posed to aggregate neighboring features by a special family of
parameterized weighted functions instead of MLPs. Leveraging the
spatial-locally correlation, a novel block, named X-Conv [Li et al.
2018], is proposed to tackle the degradation of shape information
and variance to point ordering caused by directly applying con-
volution kernels on point clouds. Inspired by the Scale Invariance
Feature Transform [Lowe 2004] (SIFT) which is a robust 2D repre-
sentation, the SIFT-like module [Jiang et al. 2018] was developed to
encode information of different orientations and scales and could be
flexibly incorporated into PointNet-style networks. Besides shape
classification and segmentation tasks, there are few point input
network architectures applied on upsampling [Yifan et al. 2019; Yu
et al. 2018b], local shape properties estimation [Guerrero et al. 2018;
Lu et al. 2020] and so on.

Deep Learning on Point Cloud Filtering. As for point cloud
filtering, Roveri et al. [Roveri et al. 2018] proposed a filtering net-
work, PointProNet, designed for consolidating raw point clouds
corrupted with noise. Benefiting from powerful 2D convolution,
PointProNet transfers 3D point clouds consolidation into 2D height
map filtering. To preserve sharp edges while filtering, Yu et al.
[Yu et al. 2018a] introduced a novel edge-aware network archi-
tecture, called EC-Net, by incorporating a joint loss function. It
works well for models with sharp features, but training EC-Net
requires manually labelling sharp edges. Combining with [Guerrero
et al. 2018], a two-stage network architecture, PointCleanNet (PCN)
[Rakotosaona et al. 2019], was developed for removing outliers
and denoisnig separately. Recently, Hermosilla et al. [Hermosilla
et al. 2019] proposed an unsupervised method to filter noisy point
clouds. By imposing priors, the method can directly train on noisy
data without needing ground truth examples or even noisy pairs.
However, this method cannot preserve share features due to lack

of sharp feature information during the training stage. Duan et al.
[Duan et al. 2019] presented a neural-network-based framework,
named NPD, to smooth 3D noisy point clouds via projecting noisy
points onto the reference planes. Since it handles points individu-
ally, NPD cannot exploit the local information which is important
for projection. Also, the global L2 loss is not effective for feature
preservation.

Figure 2: Pre-processing of Pointfilter: given a noisy point
cloud (left) and a clean point cloud (right), we create a
pair of noisy and clean patches (colored in blue). The pre-
processing results are shown in the middle (clean patch is
colored in yellow).

3 METHOD
3.1 Overview
Given a noisy point cloud, we aim to restore its clean version by our
Pointfilter in a manner of supervised learning. Before introducing
details of our Pointfilter framework, we first formulate a noisy point
cloud as follows:

ˆP = P + N,
(1)
where ˆP = { ˆp1, ... , ˆpn | ˆpi ∈ R3, i = 1 ... n} is an observed point
cloud corrupted with noise, P is the corresponding clean point
cloud (underlying surface) and N is the additive noise. In this work,
we address the filtering problem in a local way, which means the
filtered result of a noisy point only depends on its neighboring
structure. As we know, point cloud filtering is an ill-posed problem

3

and it is difficult to straightforwardly regress the additive noise
for each noisy point like image filtering. As an alternative, we
handle point cloud filtering by projecting each noisy point onto the
underlying surface. More specifically, we treat the additive noise N
as displacement vectors between the noisy point cloud ˆP and the
clean point cloud P, and learn the displacement vector for each noisy
point. To achieve this, we propose an encoder-decoder architecture
network, named Pointfilter, to regress the additive noise N, shown
in Fig. 1. We briefly introduce a pre-processing step for the input
data in Section 3.2, and then show how to model our Pointfilter in
Section 3.3. We finally explain how we train our network in Section
3.4 and how we make inference with the trained network in Section
3.5.

(2)

3.2 Preprocessing
Given a pair of point clouds P and ˆP, the noisy patch ˆPi and its
corresponding ground truth patch Pi are defined as follows
ˆPi = { ˆpj | ∥ ˆpj − ˆpi ∥ < r }, Pi = {pj | ∥pj − ˆpi ∥ < r },
where ˆpi , ˆpj ∈ ˆP, pj ∈ P and r is the patch radius. Once patches are
generated, two issues need to be addressed in point cloud filtering:
(1) how to avoid unnecessary degrees of freedom from observed
space? (2) how to guarantee our Pointfilter is insensitive to certain
geometric transformations (e.g. rigid transformations)? For the first
issue, an immediate remedy is to translate patches into origin and
then scale them into unit length, i.e., ˆPi = ( ˆPi − ˆpi )/r . Similarly, the
ground truth patch Pi does the same thing, i.e., Pi = (Pi − ˆpi )/r . To
be invariant to rigid transformations (e.g., rotations), a few methods
[Charles et al. 2017; Qi et al. 2017] attempted to predict rotation
matrix R ∈ SO(3) via an additive spatial transformer network,
while it has been proven to be fragile to rotations without massive
data augmentation [You et al. 2018]. We align the input patches
by aligning their principle axes of the PCA with the Cartesian
space. Specifically, we first align the last principle axis with the
z-axis, and then align the second principle axis with the x-axis.
The alignment process is illustrated in Fig. 2. To effectively tune
network parameters with batches, the number of points in each
input patch should be the same. In our experiments, we empirically
set the default value | ˆPi | = 500. We pad the origin for patches
with insufficient points (< 500) and do random downsampling for
patches with sufficient points (> 500). We set the patch radius r to
5% of the model’s bounding box diagonal length.

3.3 The Pointfilter Framework
The architecture of our point cloud filtering framework is demon-
strated in Fig. 1. The key idea of our Pointfilter is to project each
noisy point onto the underlying surface according to its neighbor-
ing structure. To achieve this, we design our Pointfilter network
as an encoder-decoder network. Specifically, the encoder consists
of two main parts: (1) feature extractors (i.e., MLPs) that are used
to extract different scales of features; (2) a collector that is used to
aggregate the features (N × 1024) as a latent vector z ∈ R1024. The
encoder module attempts to obtain a compact representation for an
input patch. In the decoder module, a regressor is employed to eval-
uate the displacement vectors with the latent representation vector
z as input. In this work, we adopt the recent PointNet [Charles et al.

2017] as the backbone in our Pointfilter. In practice, the extractors
and collector are realised by the shared MLPs and max pooling
layer, respectively, and the regressor is constructed by three fully
connected layers. Details of our Pointfilter network are shown in
Fig. 1. At the beginning of our Pointfiler, a PCA-induced rotation
matrix R is applied to transform the input patch to a canonical
space. Therefore, at the end of our Pointfiler, an inverse matrix R−1
should be multiplied by the evaluated displacement vector to get
the final displacement vector.

Loss function. To enable the filtered point cloud approximat-
ing the underlying surface while preserving sharp features, the
loss function should be elaborately defined. A simple option for
measuring the filtered point cloud would be the L2 distance, which
has been used in [Rakotosaona et al. 2019]. As shown in Fig. 3,
compared with the L2-based distance (4 (b)) which is sampling de-
pendent, a more general alternative is to project noisy points onto
the underlying surface (4 (c), (d)). Moreover, the L2-based distance
does not specifically consider feature information, since it simply
finds the closest points on the ground truth, regardless of feature
points or non-feature points. It leads to less sharp feature results
(see Fig. 4 (c)). Thus, the loss should be capable of measuring the
projection distance. Inspired by [Kolluri 2008], our projection loss
is defined as

(cid:205)

pj ∈ Pi

La
pr oj

=

|(¯pi − pj ) · nT
pj
(cid:205)
pj ∈ Pi ϕ(∥ ¯pi − pj ∥)

| · ϕ(∥ ¯pi − pj ∥)

,

(3)

where ¯pi is the filtered point of the noisy point ˆpi , and npj is the
ground-truth normal of the point pj , and ϕ(∥ ¯pi − pj ∥) is a Gaussian
function giving larger weights to the points near ¯pi , defined as

ϕ(∥ ¯pi − pj ∥) = exp

−

(cid:32)

∥ ¯pi − pj ∥2
2
σp

(cid:33)

.

(4)

The kernel size σp is defined as σp = 4(cid:112)diaд/m, where diaд is
the length of the diagonal of the bounding box of patch Pi and
m = | ˆPi | [Huang et al. 2009]. Besides approximating the underlying
surface, we also expect the filtered points are distributed uniformly.
To achieve this goal, we adopt a repulsion term that penalizes point
aggregation. Overall, we formulate the loss function as

L = ηLa

pr oj

+ (1 − η) Lr ep , Lr ep = max
pj ∈ Pi

| ¯pi − pj |,

(5)

where η is a trade-off parameter to control the repulsion force in
the filtering process, and we empirically set η = 0.97 in our training
stage.

We found that the projection loss La

pr oj tends to blur sharp fea-
tures (see Fig. 4 (b)). We address this issue by considering normal
similarity in our loss function, in which we introduce a bilateral
mechanism to construct the projection distance formula (Eq. (3)).
Specifically, the function is defined as the normal similarity be-
tween the current point and its neighboring points in the patch.
(cid:33)

(cid:32)

For simplicity, the function is θ (n ¯pi , npj ) = exp

−

1 − nT
npj
¯pi
1 − cos(σn )

[Huang et al. 2013], where n ¯pi is the normal of the filtered point
¯pi and σn is the support angle (default to 15◦). Therefore, our final

4

(a) Noisy input

(b) L2

(c) La

pr o j

(d) Lb

pr o j

Figure 3: Illustrating different loss functions on a toy model which simulates the side view of two planes. (a) We draw the
ground-truth points in green and the noisy points in red, and the underlying surface by dashed purple lines. (b) The L2 loss
function (i.e., closest point here) is sampling dependent since it directly maps noisy points back to the closest sampled points.
(c) The La
pr oj loss would blur sharp edges (dashed curved red lines), though it could project noisy points onto the surface (hollow
circles). (d) Our proposed Lb
pr oj projects each noisy point onto the underlying surface (hollow circles) with considering normal
information, hereby is more effective in preserving sharp features. See the close-up windows for the differences.

projection function is defined as

(cid:205)

pj ∈ Pi

Lb
pr oj

=

|(¯pi − pj ) · nT
pj
(cid:205)

| · ϕ(∥ ¯pi − pj ∥)θ (n ¯pi , npj )

.

(6)

pj ∈ Pi ϕ(∥ ¯pi − pj ∥)θ (n ¯pi , npj )

For efficiency and simplicity, the normal of the filtered point n ¯pi is
assigned by the normal of the ground truth point which is nearest
to the filtered point. It is worth noting that Pointfilter only requires
ground-truth point normals in the training stage.

We chose the encoder-decoder structure because: (1) it is stable
and mature; (2) it can learn complex and compact representations
of a point cloud; (3) the learned latent representations are helpful to
regress the displacement vector according to the input noisy patch.

(a) Noisy

(b) La

pr o j

(c) L2

(d) Lb

pr o j

pr oj ). Lb

Figure 4: Comparison of different loss functions (La
pr oj , L2
and Lb
pr oj can better preserve sharp features than
other loss functions since normal information is considered.
Note that Lr ep is respectively added to three loss functions
for fair comparisons.

3.4 Training
We implementing Pointfilter in PyTorch on a desktop PC with an
Intel Core I7-8750H CPU (2.20 GHz, 16GB memory) and a GeForce
GTX 1060 GPU (6GB memory, CUDA 9.0). We set 50 training epochs
and a mini-batch size of 64. We adopted SGD as our optimizer
and decreased the learning rate from 1e-4 to 1e-8 with increasing
epochs. We also adopted batch-normalization [Ioffe and Szegedy
2015], ReLU [Nair and Hinton 2010] and tanh in Pointfilter.

5

3.5 Network Inference
Given a trained Pointfilter, our approach filters a noisy point cloud
in a point-wise way. Firstly, we build a patch for each noisy point
and transform it to a canonical space as described in Section 3.2.
Secondly, we feed each aligned patch into Pointfilter and obtain a
displacement vector. Finally, we map the displacement vector back
to the underlying space. The inference is formulated as follows:
¯pi = r R−1 f (R( ˆPi − ˆpi )/r ) + ˆpi ,

(7)

where ¯pi and ˆpi are the filtered point and noisy point, respectively.
f represents our Pointfilter. R ∈ SO(3) is the PCA-induced rotation
matrix, and r is the patch radius. To get better filtered results, we
adopt multiple iterations of inference to progressively filter the
noisy point cloud, especially for point clouds corrupted with larger
noise.

4 EXPERIMENTAL RESULTS
4.1 Dataset
As a supervised learning method, we prepare a training dataset
consisting of 22 3D clean models (11 CAD models and 11 non-CAD
models) which are shown in Fig. 5. Each model is generated by
randomly sampling 100k points from its original surface. Given a
clean model, its corresponding noisy models are synthesized by
adding Gaussian noise with the standard deviations of 0.0%, 0.25%,
0.5%, 1%, 1.5% and 2.5% of the clean model’s bounding box diagonal
length. In short, our training dataset contains 132 (22 × 6) models.
Specifically, for each epoch in the training stage, we randomly
sample 8, 000 patches from each model as training samples. Notice
that these models are our final training dataset, and we do not augment
any data on-the-fly during training. Besides, the normal information
for clean models are required for training, as indicated in Eq. (6).

To demonstrate the generalization of the proposed Pointfilter,
our test dataset includes both synthesized noisy models and raw-
scan models, which will be explained in the following experiments
(Section 4.4 and 4.5). For synthesized data, 15 3D clean models and
their corresponding noisy models are synthesized by adding Gauss-
ian noise with the standard deviations of 0.5%, 1%, 1.5% and 2.5% of
the clean model’s bounding box diagonal length. Each clean model
contains 100k points randomly sampled from its original surface.

t
e
S
g
n

i

n
i
a
r
T

)
c
i
t
e
h
t
n
y
S
(

t
e
S

t
s
e
T

Figure 5: Our dataset. Note that 7 scanned models are tested
in Section 4.

In addition, we also test 7 raw scanned point clouds including both
shapes and scenes.

4.2 Compared Techniques
We compare our approach with the state-of-the-art point cloud
filtering techniques, namely WLOP [Huang et al. 2009], CLOP
[Preiner et al. 2014], RIMLS [Öztireli et al. 2009], GPF [Lu et al.
2018], EC-Net [Yu et al. 2018a], PointCleanNet (PCN) [Rakotosaona
et al. 2019] and TotalDenoising (TD) [Hermosilla et al. 2019]. Specif-
ically, RIMLS and GPF are designed to preserve sharp features
by incorporating smooth normals which are achieved by the bi-
lateral smoothing [Huang et al. 2013]. For fair comparisons and
visualization purposes, we (i) tune the main parameters of each
state-of-the-art technique to achieve as good visual results as pos-
sible (EC-Net, PCN and our method have fixed parameters); (ii)
employ the same surface reconstruction parameters for the same
model. Notice that surface reconstruction is straightforwardly ap-
plied to the filtered point sets. As for PCN and TotalDenoising, we
use the source codes released by the authors to train a new model
over our training dataset. Since EC-Net requires manually labelling
polylines of sharp edges for training, we simply utilize the trained
model released by the authors instead. We compare our method
with these methods, in terms of both visual quality and quantity (if
ground truth is available).

4.3 Evaluation Metrics
For the sake of analysing the performance of our Pointfilter quanti-
tatively, the evaluation metrics should be defined. The Pointfilter

6

aims to project noisy points onto its underlying surface. It is in-
tuitive to evaluate the distance errors by averaging the distances
between a point in the ground truth and its closest points in the fil-
tered point cloud [Lu et al. 2018]. The distance error can be defined
as

D(pi ) = 1
M

(cid:213)

∥pi − ¯pj ∥2
2,

(8)

¯pj ∈NN(pi )
where pi is the ground truth point and ¯pj is one of its neighboring
point in the filtered point cloud. M = |NN(pi )| and NN represents
the nearest neighbors. In our paper, we set M to 10. Inspired by [Fan
et al. 2017], we also introduce the Chamfer distance (CD) to evaluate
the error between the filtered point cloud and its corresponding
ground truth (clean point cloud). CD is defined as

C(P, ¯P) = 1
N1

(cid:213)

pi ∈P

min
¯pj ∈¯P

(∥pi − ¯pj ∥2
2 )

+ 1
N2

(cid:213)

pj ∈P

min
¯pi ∈¯P

(∥pj − ¯pi ∥2
2 ),

(9)

where N1 and N2 represent the cardinalities of the clean point
cloud P and the filtered point cloud ¯P, respectively. The CD metric
finds the nearest neighbor in the other set and sums the squared
distances up. It can be viewed as an indicator function which mea-
sures the “similarity" between two point sets. Also, it can be easily
implemented in parallel. Besides the two above metrics, the point-
to-surface (P2F) [Yifan et al. 2019] distance against the ground truth
mesh is also used in our evaluation.

4.4 Visual Comparisons
Point clouds with synthetic noise. The synthetic noise level is
estimated by the diagonal length of the bounding box. For example,
0.5% noise denotes 0.5% of the diagonal length. As shown in Fig.
6, we test four CAD models (Boxunion, Cube, Fandisk and Tetra-
hedron) with 0.5% noise. Compared with the state-of-the-art point
cloud filtering techniques, we observe that results by our Pointfilter
generates visually better results, in terms of noise removal and
features preservation. Note that RIMLS and GPF can also preserve
sharp features to some extent; however, they depend greatly on the
capability of normal filters which become less robust when meeting
large noise. Compared to RIMLS and GPF, we elegantly detour the
normal filtering issue since our framework requires the easily ob-
tained ground-truth normals for training only. As shown in Fig. 10,
RIMLS and GPF produce less desired results when handling 1.0%
noise, for example, obvious gaps in sharp edges (10 (b)) and striking
outliers (10 (c)). By contrast, our Pointfilter is robust in preserving
sharp features. Since the outliers exist in RIMLS and GPF results,
their filtered point clouds involve shrinkage to some extent. Despite
that WLOP and CLOP are good at generating smooth results, they
still fail to retain sharp features. Regarding EC-Net, it generates less
pleasant results, in terms of removing noise. In addition to CAD
models, we also test some non-CAD models corrupted with syn-
thetic noise. As shown in Fig. 7, our proposed Pointfilter can also
output visually decent results while preserving sharp features. For
relatively smooth models shown in Fig. 8, our Pointfilter provides
competitive filtering results, without any trial-and-error parame-
ter tuning. Moreover, since WLOP and CLOP rely on the support

(a) Noisy

(b) RIMLS

(c) GPF

(d) WLOP

(e) CLOP

(f) PCN

(g) TD

(h) Ours

Figure 6: Visual comparison of point clouds with 0.5% noise. Our method preserves sharp features better than the existing
methods.

(a) Noisy

(b) RIMLS

(c) GPF

(d) WLOP

(e) CLOP

(f) PCN

(g) TD

(h) Ours

Figure 7: Visual comparison of point cloud filtering with 0.5% synthetic noise. The overall MSEs (×10−3) for different methods
are shown in the figure.

radius to generate desirable results, the results of their methods
usually have slight shrinking in narrow cylinder area (Fig. 8 (b)
and 8 (c)). Besides Gaussian noise, we also test our Pointfilter on
other types of synthetic noise including Impulsive and Uniform
noise. It should be noted that all compared models in Fig. 14 are
trained under Gaussian noise only. From Fig. 14, we can see that the
results generated by our Pointfilter are better than other methods.
Although TotalDenoising outputs smoothing results, it fails to pre-
serve sharp features due to the non-consideration of sharp features

information. In contrast, our Pointfilter has a better generalization
capability in dealing with different types of noise.
Point clouds with raw noise. We also evaluate our Pointfilter
on raw scanned point clouds corrupted with raw noise. Since the
ground truth models of these raw scanned point sets are not avail-
able, we demonstrate the visual comparisons with other methods,
as suggested by previous techniques [Lu et al. 2018]. Notice that we
do not re-train our Pointfilter for the type of raw noise (except Fig. 19
and Table 4). From Fig. 11, we see that our results are better than
the state-of-the-art techniques. Besides noise removal, Pointfilter is

7

(a) Noisy

(b) RIMLS

(c) GPF

(d) WLOP

(e) CLOP

(f) PCN

(g) TD

(h) Ours

Figure 8: Visual comparison of point clouds filtering with 0.5% synthetic noise. The overall MSEs (×10−3) for different methods
are shown in the figure.

(a) RIMLS

(b) GPF

(c) WLOP

(d) CLOP

(e) PCN

(f) TD

(g) Ours

Figure 9: Quantitative comparison in mean square error (MSE). The overall errors (×10−3) for different methods over four
models are shown in the figure.

capable of retaining sharp features which are marked by yellow box
and black arrows in Fig. 11. Fig. 12 shows a virtually scanned point
cloud model. Compared with other filtering methods, Pointfilter
still produces higher quality results, in terms of preserving sharp
edges. Fig. 13 shows that our approach induces a better enhance-
ment on the surface reconstruction quality, in terms of preserving

sharp features. Fig. 19 shows Pointfilter results on KinectV1 dataset
[Wang et al. 2016]. For fair comparisons, we re-train both PCN and
our Pointfilter according to the training/test set configuration in
this dataset. To further evaluate Pointfilter, we also compare the
re-trained version and the original version trained by the synthetic
noise. As shown in Fig. 19, our re-trained version of Pointfilter (Fig.

8

of the Chamfer distance (CD), mean square error (MSE) and point-
to-surface distance (P2F). As illustrated in Table 2, our method
averagely achieves the lowest errors.

Despite that the results of RIMLS are comparable to ours, it
requires quality normals (we used bilateral smoothing [Huang et al.
2013] for RIMLS) and trial-and-error parameter tuning to obtain
satisfactory results. Moreover, such parameter tuning is tedious,
time-consuming, and especially difficult for users who do not have
any background knowledge. In contrast, our method is automatic
and easy to use, and is generally the most accurate one among all
the compared approaches. We also evaluate our Pointfilter on the
KinectV1 and KinectV2 datasets [Wang et al. 2016]. Table 4 shows
our re-trained Pointfilter produces lower errors (Chamfer distance)
in the smoothing results than the re-trained PCN.
Runtime. Because surface reconstruction is an application over
point clouds, we only calculate the runtime of each point set filter-
ing method. In particular, optimized-based methods (RIMLS, GPF,
WLOP and CLOP) involve multiple steps and require trial-and-error
efforts to tune parameters to produce decent visual results, which
means these methods generally require a much longer “runtime” in
practice. Thus, we only consider learning-based methods (EC-Net,
PCN and Ours), in terms of time consumption in the test stage.
Table 3 summaries the runtime of each learning-based method on
some point clouds using the same configuration. Table 3 sees that
EC-Net is the fastest method among the learning-based methods, as
it is an upsampling method and only requires a few patches evenly
distributed on the noisy input in the test phase. For fair compar-
isons, we only consider the runtime of the noise removal module in
PCN, because extra time consumption would be introduced for the
outliers removal module. In spite of this, PCN is still the slowest
one. Our approach ranks the second in speed, and we suspect that
it is due to the point-wise manner.

Table 3: Runtime performance (in seconds) for three
learning-based methods in the test stage. All examples were
run on the same computer configurations (Section .3.4).

Methods

Models
Cube
Fandisk
Boxunion
Tetrahedron
Horse
Face-Yo
Fertility-tri
Face-Raw
Pyramid-Raw
Nefertiti-Raw

EC-Net

PCN

TD

Ours

27.73
26.92
26.15
28.64
26.91
27.27
27.43
22.63
44.74
27.00

360.64
369.67
365.09
326.11
365.62
362.26
370.21
306.39
618.98
353.44

96.43
96.52
95.28
97.88
98.21
97.68
98.36
85.17
163.91
96.00

62.34
62.45
65.41
63.21
63.55
63.08
63.52
55.71
105.85
62.38

4.6 Robustness
Different levels of noise. From Fig. 20, we can observe that our
Pointfilter achieves superior performance, especially for heavy
noise like 2.5%. As such, our Pointfilter is more robust than other
compared methods in handling increasing levels of noise.

9

(a) Noisy

(b) RIMLS

(c) GPF

(d) Ours

Figure 10: Point clouds with large noise (1.0%).

19 (d)) is better than the retrained PCN, and produces better re-
sults than the originally trained Pointfilter. In addition, we also test
our Pointfilter on scene-level models from the Paris-rue-Madame
Database [Serna et al. 2014] (see Fig. 18). As we can see from these
figures, our Pointfilter is still able to produce better results. More
filtering results for raw noise are demonstrated in Fig. 15.
Point clouds with noticeable outliers. Although Pointfilter is
not particularly designed for removing outliers, we observe it pro-
duces competitive results in point clouds with noticeable outliers.
Following [Lu et al. 2018], we also conduct an experiment by com-
paring our method with the LOP-based methods (WLOP & CLOP)
which use L1-norm and are robust to outliers. This experiment
demonstrates the capability of our Pointfiler in dealing with notice-
able outliers. As Fig. 16 shows, Pointfilter can generate a comparable
result to WLOP and CLOP.

Table 2: Average errors of all filtered point clouds over our
test synthetic models (15 models with 0.5% Gaussian noise),
in terms of Chamfer distance (CD), mean square error (MSE)
and mean point-to-surface distance (P2F).

Metrics

Methods
Noisy
RIMLS
GPF
WLOP
CLOP
PCN
TD
Ours

CD (10−5) MSE (10−3)

P2F (10−3)

3.655
1.102
2.273
1.888
1.421
0.942
1.483
0.833

5.869
4.082
4.791
4.222
4.059
3.981
4.353
3.884

4.058
1.549
2.446
1.726
1.439
1.351
1.878
1.147

4.5 Quantitative Comparisons
Errors. We calculate the above metrics of the methods on some
point clouds. These point sets are achieved by adding synthetic
noise to the ground truth. To depict the distance errors, we calculate
the mean square error (MSE) for each ground-truth point via Eq.
(8) and visualize the results in Fig. 9. We observe that Pointfiler
generates comparable or better results, especially for sharp features.
To comprehensively evaluate our Pointfiler, we also calculate the
overall errors over the 15 synthetic models in the test set, in terms

(b) RIMLS

(d) WLOP

(f) EC-Net

(h) TD

(a) Noisy

(c) GPF

(e) CLOP

(g) PCN

(i) Ours

Figure 11: Results on the raw Face model. We highlight the artifacts using arrows and show close-up views for better visual-
ization.

(a) Noisy

(b) RIMLS

(c) GPF

(d) WLOP

(e) CLOP

(f) EC-Net

(g) PCN

(h) TD

(i) Ours

Figure 12: Filtered results of the raw Pyramid point cloud.

Table 4: Comparison with PCN on KinectV1 and KinectV2
datasets [Wang et al. 2016]. The Chamfer distance (10−5) is
used here.

Methods
Noisy
PCN
Ours
Ours (re-trained)

KinectV1 KinectV2

2.467
2.078
2.356
1.884

3.569
3.188
3.411
3.149

Different numbers of points. We implement experiments over
the test dataset, containing different numbers of points (10K, 30K,
50K and 80K). Fig. 21 obviously shows that the performance of our
Pointfilter is better than other competitors (except the case of 10K
points). We suspect that the relatively sparse neighboring points
of a patch (comparing to training dataset) are insufficient to depict
the local structure, which leads to projecting noisy points onto a
misleading underlying surface and less desired results. This is often
involved in patch based methods, such as PCN and Pointfilter.
Non-uniform point distribution. We also test our Pointfilter
under non-uniform point distribution. We synthesize a model which

10

consists of a curved surface and a plane surface. As analyzed above,
patch-based methods tend to generate less desired results for less
dense points. As shown in Fig. 22, we can see that our Pointfilter
can achieve a better result on the dense part, though it outperforms
other compared methods.

4.7 Ablation Study
With/without the repulsion term. We found that points would
aggregate together without the repulsion term. The repulsion term
is introduced (Eq. (5)) to mitigate this issue. As shown in Fig. 23, our
repulsion term plays a significant role in maintaining a relatively
uniform distribution for points. In addition, we also evaluate the
influence of the trade-off parameter η in Eq. (5). In Fig. 24, we
empirically found that η with 0.97 achieves a superior performance
to other choices, based on the statistics over all models in the test
set. Some selected visual results with different repulsion parameters
are shown in Fig. 24. It is obvious that a small η cannot preserve
share features, especially for corner regions.
Alternative loss. For fairness, we train three loss functions (L2,
pr oj (Eq. (5)) and Lb
La
pr oj (Eq. (6))) separately under the same train-
ing configuration. Table 5 manifests that our Pointfilter with Lb

pr oj

(a) Noisy

(b) RIMLS

(c) GPF

(d) WLOP

(e) CLOP

(f) EC-Net

(g) PCN

(h) TD

(i) Ours

Figure 13: Results on the raw Nefertiti model. The corresponding Poisson reconstructed meshes are shown in the bottom.

(a) Noisy

(b) PCN

(c) TD

(d) Ours

Figure 15: Filtered results of point clouds with raw noise.

(a) Noisy

(b) EC-Net

(c) PCN

(d) TD

(e) Ours

Figure 14: Point cloud filtering for three different types of
synthetic noise (from top to bottom: Gaussian, impulsive,
uniform). See the close-up views for differences. The overall
MSEs (×10−3) for different methods are shown in the figure.

has the best performance. Thus, the proposed loss function is cru-
cial and useful for boosting the performance in filtering noisy point
clouds.
Alternative backbones. In this work, we adopt PointNet [Charles
et al. 2017], which is simple and easy to use, as the backbone in the
encoder part (see Figure. 1). It is necessary to test other advanced
backbones such as DGCNN [Wang et al. 2019] and PointCNN [Li
et al. 2018]. For fair purpose, we only replace corresponding layers
in the encoder and all results are measured by the Chamfer distance.
As shown in Table 6, other backbones do not show the noticeable

(a) Noisy input

(b) WLOP

(c) CLOP

(d) Ours

Figure 16: Pointfilter can produce comparable results to the
LOP-based methods (e.g., WLOP and CLOP) in presence of
noticeable outliers. Big radii are employed here for these
methods.

performance gain over PointNet. For variants with DGCNN and
PointCNN, we suspect that the neighboring information in the
presence of noise may pose negative impact on the two backbones
which depend on finding neighboring information.

11

(a)

(b)

(c)

(d)

Figure 17: Failure cases: excessive noise ((a)(b)) and large
holes ((c)(d)).

Table 5: Average performance of Pointfilter with alternative
loss functions on our test dataset (15 synthetic models). The
Chamfer distances (10−5) under different levels of noise are
evaluated.

Losses
La
pr oj
L2
Lb
pr oj

0.5%
2.434
1.843
1.196

1.0%
4.488
2.371
1.951

1.5%
7.337
5.171
3.044

2.5%
13.766
9.053
6.414

Table 6: Average performance of Pointfilter with alternative
backbones on our test dataset (15 synthetic models). The
Chamfer distances (10−5) under different levels of noise are
evaluated.

Backbones
PointCNN
DGCNN
PointNet

0.5%
2.446
2.441
1.196

1.0%
3.592
3.342
1.951

1.5%
4.839
4.682
3.044

2.5%
8.767
8.423
6.414

5 CONCLUSION
In this paper, we proposed a Pointfilter framework for feature-
preserving point cloud filtering. Our architecture can be easily
trained. Given an input noisy point cloud, our method can auto-
matically infer the involved displacement vectors and further the
filtered point cloud with preserved sharp features. Extensive ex-
periments and comparisons showed that our method outperforms
the state-of-the-art point set filtering techniques (or comparable to
optimization based methods like RIMLS which need quality nor-
mals and trial-and-error parameter tuning), in terms of both visual
quality and evaluation errors. Our approach is automatic and also
achieves impressive performance on test time. Compared to PCN
[Rakotosaona et al. 2019], it should be noted that our Pointfilter
is not designed for large-scale outliers removing, and our Pointfil-
ter and PCN are thus complementary in terms of sharp features
preservation and heavy outliers removal.

Our method involves a few limitations. First, our Pointfilter be-
comes hard to retain the sharp features when handling excessive
noise (see Fig. 17 left). Secondly, our method fails to handle signifi-
cant holes in point clouds (see Fig. 17 right). In future, we would
like to incorporate global shape information into our framework to
help guide point cloud filtering.

12

REFERENCES
M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C.T. Silva. 2003. Computing
and rendering point set surfaces. IEEE Transactions on Visualization and Computer
Graphics 9, 1 (Jan. 2003), 3–15. https://doi.org/10.1109/tvcg.2003.1175093

Marc Alexa, Johannes Behr, Daniel Cohen-Or, Shachar Fleishman, David Levin, and
Claudio T. Silva. 2001. Point Set Surfaces. In Proceedings of the Conference on
Visualization ’01 (VIS ’01). IEEE Computer Society, Washington, DC, USA, 21–28.
http://dl.acm.org/citation.cfm?id=601671.601673

Nina Amenta and Yong Joo Kil. 2004. Defining Point-set Surfaces. ACM Transactions
on Graphics. 23, 3 (Aug. 2004), 264–270. https://doi.org/10.1145/1015706.1015713
Haim Avron, Andrei Sharf, Chen Greif, and Daniel Cohen-Or. 2010. ℓ1-Sparse recon-
struction of sharp point set surfaces. ACM Transactions on Graphics 29, 5 (Oct.
2010), 1–12. https://doi.org/10.1145/1857907.1857911

R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas. 2017. PointNet: Deep
Learning on Point Sets for 3D Classification and Segmentation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.
https://doi.org/10.1109/cvpr.2017.16

Chaojing Duan, Siheng Chen, and Jelena Kovacevic. 2019. 3D Point Cloud Denoising
via Deep Neural Network Based Local Surface Estimation. In ICASSP 2019-2019
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 8553–8557.

Haoqiang Fan, Hao Su, and Leonidas J Guibas. 2017. A point set generation network for
3D object reconstruction from a single image. In Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR). IEEE, 605–613.

Shachar Fleishman, Daniel Cohen-Or, and Cláudio T. Silva. 2005. Robust Moving
Least-squares Fitting with Sharp Features. ACM Transactions on Graphics. 24, 3
(July 2005), 544–552. https://doi.org/10.1145/1073204.1073227

Paul Guerrero, Yanir Kleiman, Maks Ovsjanikov, and Niloy J. Mitra. 2018. PCPNet:
Learning Local Shape Properties from Raw Point Clouds. Computer Graphics Forum
37, 2 (May 2018), 75–85. https://doi.org/10.1111/cgf.13343

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning

for Image Recognition. arXiv preprint arXiv:1512.03385 (2015).

Pedro Hermosilla, Tobias Ritschel, and Timo Ropinski. 2019. Total Denoising: Unsuper-
vised Learning of 3D Point Cloud Cleaning. In Proceedings of the IEEE International
Conference on Computer Vision. 52–60.

Hui Huang, Dan Li, Hao Zhang, Uri Ascher, and Daniel Cohen-Or. 2009. Consolidation
of unorganized point clouds for surface reconstruction. ACM Transactions on
Graphics 28, 5 (Dec. 2009), 1. https://doi.org/10.1145/1618452.1618522

Hui Huang, Shihao Wu, Minglun Gong, Daniel Cohen-Or, Uri Ascher, and Hao (Richard)
Zhang. 2013. Edge-aware point set resampling. ACM Transactions on Graphics 32,
1 (Jan. 2013), 1–12. https://doi.org/10.1145/2421636.2421645

Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep net-
work training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167
(2015).

Mingyang Jiang, Yiran Wu, Tianqi Zhao, Zelin Zhao, and Cewu Lu. 2018. Pointsift: A
sift-like network module for 3D point cloud semantic segmentation. arXiv preprint
arXiv:1807.00652 (2018).

Ravikrishna Kolluri. 2008. Provably good moving least squares. ACM Transactions on

Algorithms 4, 2 (May 2008), 1–25. https://doi.org/10.1145/1361192.1361195

Carsten Lange and Konrad Polthier. 2005. Anisotropic smoothing of point sets. Com-
puter Aided Geometric Design 22, 7 (Oct. 2005), 680–692. https://doi.org/10.1016/j.
cagd.2005.06.010

David Levin. 1998. The approximation power of moving least-squares. Math. Comp.
67, 224 (Oct. 1998), 1517–1532. https://doi.org/10.1090/s0025-5718-98-00974-0
David Levin. 2004. Mesh-Independent Surface Interpolation. Springer Berlin Heidelberg,

37–49. https://doi.org/10.1007/978-3-662-07443-5_3

Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. 2018.
PointCNN: Convolution On X-Transformed Points. (2018). arXiv:cs.CV/1801.07791
Bin Liao, Chunxia Xiao, Liqiang Jin, and Hongbo Fu. 2013. Efficient feature-preserving
local projection operator for geometry reconstruction. Computer-Aided Design 45,
5 (May 2013), 861–874. https://doi.org/10.1016/j.cad.2013.02.003

Yaron Lipman, Daniel Cohen-Or, David Levin, and Hillel Tal-Ezer. 2007.
Parameterization-free projection for geometry reconstruction. ACM Transactions
on Graphics 26, 3 (July 2007), 22. https://doi.org/10.1145/1276377.1276405

David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints.
International Journal of Computer Vision 60, 2 (Nov. 2004), 91–110. https://doi.org/
10.1023/b:visi.0000029664.99615.94

Dening Lu, Xuequan Lu, Yangxing Sun, and Jun Wang. 2020. Deep feature-preserving
normal estimation for point cloud filtering. Computer-Aided Design (2020), 102860.
Xuequan Lu, Scott Schaefer, Jun Luo, Lizhuang Ma, and Ying He. 2018. Low rank matrix
approximation for geometry filtering. arXiv preprint arXiv:1803.06783 (2018).
X. Lu, S. Wu, H. Chen, S. Yeung, W. Chen, and M. Zwicker. 2018. GPF: GMM-Inspired
Feature-Preserving Point Set Filtering.
IEEE Transactions on Visualization and
Computer Graphics 24, 8 (Aug 2018), 2315–2326. https://doi.org/10.1109/TVCG.
2017.2725948

Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted
boltzmann machines. In Proceedings of the 27th international conference on machine

(a) Noisy input

(b) EC-Net

(c) PCN

(d) TD

(e) Ours

Figure 18: Results on two scanned models (top 600K and bottom 1000K) from the Paris-rue-Madame Database [Serna et al.
2014].

Figure 20: Average CD errors (10−5) of filtered point clouds in
the test set (15 synthetic models), in terms of different noise
levels.

cgf.13753

Riccardo Roveri, A. Cengiz Öztireli, Ioana Pandele, and Markus Gross. 2018. Point-
ProNets: Consolidation of Point Clouds with Convolutional Neural Networks. Com-
puter Graphics Forum 37, 2 (May 2018), 87–99. https://doi.org/10.1111/cgf.13344
Jürgen Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural
Networks 61 (Jan. 2015), 85–117. https://doi.org/10.1016/j.neunet.2014.09.003
Andrés Serna, Beatriz Marcotegui, FranÃğois Goulette, and Jean-Emmanuel Deschaud.
2014. Paris-rue-Madame Database - A 3D Mobile Laser Scanner Dataset for Bench-
marking Urban Detection, Segmentation and Classification Methods. In ICPRAM.
K. Simonyan and A. Zisserman. 2015. Very Deep Convolutional Networks for Large-
Scale Image Recognition. In International Conference on Learning Representations.
Yujing Sun, Scott Schaefer, and Wenping Wang. 2015. Denoising point sets via ℓ0
minimization. Computer Aided Geometric Design 35-36 (May 2015), 2–15. https:
//doi.org/10.1016/j.cagd.2015.03.011

Peng-Shuai Wang, Yang Liu, and Xin Tong. 2016. Mesh denoising via cascaded normal

regression. ACM Trans. Graph. 35, 6 (2016), 232–1.

Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and
Justin M. Solomon. 2019. Dynamic Graph CNN for Learning on Point Clouds. ACM
Transactions on Graphics 38, 5 (Oct. 2019), 1–12. https://doi.org/10.1145/3326362
Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. 2018. SpiderCNN: Deep
Learning on Point Sets with Parameterized Convolutional Filters. In Proceedings
of the European Conference on Computer Vision (ECCV). Springer International
Publishing, 90–105. https://doi.org/10.1007/978-3-030-01237-3_6

Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, and Olga Sorkine-Hornung.
2019. Patch-Based Progressive 3D Point Set Upsampling. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.

(a) Noisy

(b) PCN-R

(c) Ours

(d) Ours-R

Figure 19: Visual Comparisons with PCN on the KinectV1
and KinectV2 datasets.

learning (ICML). 807–814.

A. C. Öztireli, G. Guennebaud, and M. Gross. 2009. Feature Preserving Point Set
Surfaces based on Non-Linear Kernel Regression. Computer Graphics Forum 28, 2
(April 2009), 493–501. https://doi.org/10.1111/j.1467-8659.2009.01388.x

Reinhold Preiner, Oliver Mattausch, Murat Arikan, Renato Pajarola, and Michael
Wimmer. 2014. Continuous projection for fast L1 reconstruction. ACM Transactions
on Graphics 33, 4 (July 2014), 1–13. https://doi.org/10.1145/2601097.2601172

Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. 2017. PointNet++: Deep
Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of
the Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran
Associates, Inc., 5099–5108.

Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J. Mitra, and Maks
Ovsjanikov. 2019. PointCleanNet : Learning to Denoise and Remove Outliers from
Dense Point Clouds. Computer Graphics Forum (June 2019). https://doi.org/10.1111/

13

(a) 10K

(b) 30K

(c) 50K

(d) 80K

Figure 21: Average CD errors (10−5) of filtered point clouds in the test set (15 synthetic models), in terms of different numbers
of points.

Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. 2018a.
EC-Net: an Edge-aware Point set Consolidation Network. In Proceedings of the
European Conference on Computer Vision (ECCV). 386–402.

Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. 2018b.
PU-Net: Point Cloud Upsampling Network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/
cvpr.2018.00295

Yinglong Zheng, Guiqing Li, Shihao Wu, Yuxin Liu, and Yuefang Gao. 2017. Guided
point cloud denoising via sharp feature skeletons. The Visual Computer 33, 6-8
(May 2017), 857–867. https://doi.org/10.1007/s00371-017-1391-8

Yinglong Zheng, Guiqing Li, Xuemiao Xu, Shihao Wu, and Yongwei Nie. 2018. Rolling
normal filtering for point clouds. Computer Aided Geometric Design 62 (May 2018),
16–28. https://doi.org/10.1016/j.cagd.2018.03.004

(a) Noisy

(b) PCN

(c) TD

(d) Ours

Figure 22: Non-uniform point cloud.

(a) Input

(b) Without repulsion

(c) With repulsion

Figure 23: Results with and without the repulsion term
(Lr ep ).

Figure 24: Average CD errors of filtered point clouds on the
test set (15 synthetic models), in terms of different trad-off
parameter η in Eq. (5).

Yang You, Yujing Lou, Qi Liu, Lizhuang Ma, Weiming Wang, Yuwing Tai, and Cewu Lu.
2018. PRIN: Pointwise Rotation-Invariant Network. arXiv preprint arXiv:1811.09361
(2018).

14

