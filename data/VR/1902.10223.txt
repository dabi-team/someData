Virtual Environments for Rehabilitation of Postural Control Dysfunction

Zhu Wang*
New York University

Anat Lubetzky†
New York University

Marta Gospodarek‡
New York University

Makan TaghaviDilamani§
New York University

Ken Perlin¶
New York University

9
1
0
2

b
e
F
8

]

C
H
.
s
c
[

1
v
3
2
2
0
1
.
2
0
9
1
:
v
i
X
r
a

ABSTRACT

We developed a novel virtual reality [VR] platform with 3-
dimensional sounds to help improve sensory integration and vi-
suomotor processing for postural control and fall prevention in in-
dividuals with balance problems related to sensory deﬁcits, such as
vestibular dysfunction (disease of the inner ear). The system has
scenes that simulate scenario-based environments. We can adjust the
intensity of the visual and audio stimuli in the virtual scenes by con-
trolling the user interface (UI) settings. A VR headset (HTC Vive
or Oculus Rift) delivers stereo display while providing real-time
position and orientation of the participants’ head. The 3D game-like
scenes make participants feel immersed and gradually exposes them
to situations that may induce dizziness, anxiety or imbalance in their
daily-living.

Index Terms:
Visualization design and evaluation methods

Human-centered computing—Visualization—

1 INTRODUCTION

According to the U.S. Centers for Disease Control and Prevention,
falls are the top public health problem for older Americans. Fall-
related injuries bring one to the emergency room every 11 seconds
and cause one death every 19 minutes. One in four 65+ Americans
experiences at least one fall each year. With that, the direct cost of
the medical attention for fall injuries reached 50 billion US Dollars
in 2015 [3].

Vestibular dysfunction, a disease of the inner ear, is a condition
that leads to imbalance and dizziness and signiﬁcantly increases fall
risk [32] [1]. The National Health and Nutrition Examination Survey
estimated that 35.4 of adults in the United States sought medical
attention for a vestibular dysfunction between 2001-2004. Every
year, 7 million clinic visits are due to dizziness [30] [17]. People
with chronic dizziness and imbalance are two to three times more
likely to fall in comparison to people without chronic dizziness [16],
with one report indicating a 12-fold increase in the fall risk among
individuals with vestibular dysfunction [1]. The increased fall risk
is [1, 16], particularly in complex environments with busy visual and
auditory cues. In addition, once a patient experienced loss of balance
or dizziness in a certain environment, they are likely to develop
anxiety of that situation or avoid it altogether, thus reducing their
participation in the community, especially in urban environments.

To help patients improve their balance function and adapt to vari-
ous sensory stimuli, therapists need to set up complex environments
and simulate the stimuli in patients’ rehabilitation at the clinics. For
that reason, virtual reality (VR) seems to be appropriate in the realm
of rehabilitation [37]. VR can maximize the advantages of com-
puter technology to render all kinds of semi-realistic 3D models and
auditory stimuli that exist in the real world.

*e-mail: zhu.wang@nyu.edu
†e-mail: anat@nyu.edu
‡e-mail: mo1417@nyu.edu
§e-mail: mt3299@nyu.edu
¶e-mail: ken.perlin@gmail.com

Indeed, Physical Therapy has decades of experience of using VR
technology, but the use has increased exponentially over the past
few years. With the release of modern VR systems (such as HTC
Vive and Oculus Rift) since the 2010s, VR technology has become
more affordable, portable and with high quality making it a feasible
rehabilitation solution for clinics. Compared with conventional
balance rehabilitation methods, a VR program can simulate real-life
situation without a need to leave the clinic to the actual site (which
is often not feasible). A patient can then gradually be exposed to
various sensory cues in different contexts with increasing levels
of difﬁculty. All of this is done in a safe and supervised setting,
and the patient can ‘leave’ the environment at any time. All the
procedures and contents are fully controllable and repeatable which
makes large-scale clinical research accessible.

Our collaboration with the clinics gives us an opportunity to build
a seamless transfer between new technologies and balance rehabili-
tation (particularly related to sensory deﬁcits) to improve the quality
of life and reduce falls. Our system was developed and reﬁned based
on patients’ stories and feedback, as well as the needs expressed by
therapists. We learned about the type of environments that make
patients dizzy, off-balance or anxious (e.g., subway, airports etc.)
and the stimuli that enhance those responses (e.g., colors, ﬂow of
people etc.). We also interviewed physical therapists about their
needs that cannot be met by traditional clinical tools. When de-
signing a rehabilitation software, it is crucial to test its usability in
clinics to make sure that clinicians are able to operate the system
without technical assistance. Therefore, when the ﬁrst version of our
system was ready, we began descriptive case studies and a clinical
implementation study. So far, we have done a usability experiment
with 6 physical therapists who treated 17 patients. From those ex-
periments, we gathered user experience when therapists and patients
used the system. These data signiﬁcantly helped us in evolving the
functionalities and system designs.

2 RELATED WORK
Virtual reality rehabilitation has been shown to be more effective
than conventional rehabilitation in regards to physical outcomes for
patients with vestibular dysfunction [21]. It has been suggested
that immersive VR environments offer more playability, realistic
physical ﬁdelity, and increased cognitive load. With training tasks
that mimic the real-world conditions, the transfer from a game-like
training program to daily function should be simpler. Patients who
participated in VR balance training programs reported they prefer
VR over traditional exercises because of more immersive experi-
ence, more enjoyment, less after-activity fatigue, and a perception
of less difﬁculty [21]. For instance, the Computer Assisted Re-
habilitation Environment (CAREN) is a virtual reality system for
balance assessment and rehabilitation. The CAREN has been tested
in a few clinical centers around the US, and demonstrated to be
effective in diverse populations [15, 27]. However, the estimated
cost of installing, running and maintaining the CAREN is over $1
million [15]. Some off-the-shelf augmented reality systems, such as
Wii and Kinect, are accessible to rehabilitation but are not as immer-
sive as Head Mounted Displays (HMDs) and the program cannot
be customized to individual needs or to create a sensory experience
similar to real-life situations [12]. Auditory cues have been used
as real-time biofeedback [7], but are not typically the method of
choice for balance rehabilitation of patients with vestibular dysfunc-

 
 
 
 
 
 
tion due to the various degrees of hearing loss often accompanying
vestibular disorders [29]. In our paradigm, we utilize the combina-
tion of immersive visual and auditory cues to simulate challenges
that cause symptoms to patients in real scenarios. Scenarios are
hard to duplicate or repeat in the clinic. In this way, patients will
have no risk of safety issues to walk through all environments that
may cause them discomfort. In addition, because the environmental
stimuli are fully controllable, measurable and reproducible, virtual
reality interventions can help clinicians to customize stimuli and
target speciﬁc impairments within a functional context [23].

3 CONCEPT AND SIGNIFICANCE

Our portable platform utilizes the HTC Vive or Oculus Rift and
includes dynamic environments that capture focused elements of
the visual experience to assess the effect of various types of visual
cues (e.g., static to dynamic, visual perturbation, lighting, speed of
object, etc.) on balance in a safe and immersive manner and without
applying destabilizing or nauseating stimuli. The platform targets
treatment of multiple domains underlying balance performance in-
cluding visual dependence, somatosensory contribution, reactive vs.
proactive balance, and dual-tasking within a functional context [10].
The system mainly focuses on visual and auditory complexity
within a functional context. The functional contexts include different
environments in which patients can experience sensory overload.
Patients might ﬁnd themselves unsteady, or light-headed when they
are facing a complex environment such as a group of people walking
around, cars passing. The complex situations are most likely to
occur when people are blending into their social lives, especially
when they need to take public transportation such as a subway train
or a ﬂight. A subway station or an airport is where patients easily to
encounter a massive crowd of people. Yet a closed subway station
provides a different sensory experience than a large open airport,
hence the functional context is different. Some of our patients avoid
taking public transportation (i.e., the subway), others avoid ﬂying.
A fast approaching object can easily make patients lose their
balance since people can prepare their body for the situation if they
know what kind of gesture and stabilization they need in advance.
But in our daily life, it is not possible for people to see everything
coming beforehand. For example, it is inevitable to see people
throwing balls in parks, people skating and passing by at a fast
speed, or even birds ﬂying over your head.

Within different environments, the patients deal with sensory
complexity that can be adjusted to their level of symptom, anxiety
or imbalance. We can start from an empty environment and progres-
sively increase the number of moving people, different movement
directions, speed, etc. In addition, a patient can be trained to react
to sudden events, such as a ﬂying ball or an approaching train. This
trains their ability to move outside their base of support as well as
overcome the fear of responding to a surprising visual event. The
task itself depends on patients abilities. Patients can start sitting
or standing with support. They can progress to moving their head,
looking around, walking or other balance tasks as per the therapist’s
judgment and the patient’s symptoms.

To enhance patients’ ability to participate in the community and
prevent people from falling, an ideal therapy would be a simulation
of the same scenarios where the patients can conquer the stress with
no safety issues. Unfortunately, it is not feasible to set up a training
session in a real airport or subway station. However, VR can help
clinicians to build a simulation of the scenarios which make the
patients stressful and anxious without stepping out of clinic rooms.
Instead of taking patients physically to the complex environments,
VR instantly transforms clinic rooms into any complex environment
so that all kinds of precaution and arrangement can ensure the safety
of the patients. The adjustable level of content complexity helps
clinicians to customize proper training sessions for all types of
patients who have different balance disorder situations.

Figure 1: Airport scene with high visual intensity (4/4 walking direction
level, 4/4 walking amount level).

Figure 2: Subway scene with low visual intensity (1/4 walking direction
level, 1/4 walking amount level).

The low cost of our platform components will enable us to reach
out to multiple clinics and patients at fall risk that otherwise have
limited access to such technology. A gap exists in the availability
of ecologically valid but low-cost technology that can be widely
available for assessment, and treatment of patients with balance
problems and fall risk. Our platform offers a unique solution to this
problem.

4 SYSTEM DESIGN

4.1 Virtual Environments

In our system, there are 4 scenes: airport, subway, city, ball&park.
The airport scene has a 3D graphics model of an airport terminal
which simulates a real airport (see Figure 1). 3D models of pedes-
trians walk around at different speeds, and airplanes ﬂy over the
terminal randomly in a range of 50-58 seconds. In addition to the
sounds of footsteps from the people passing by and the sounds of
planes taking off, the scene has the ambient sound which contains
crowd chatter, announcements recorded from a real airport, and
noises of airport machines and luggage.

The subway scene is a 3D model of a real subway station(see
Figure 2). It has the same crowd generation module to create the
people walking in groups on the platform, mezzanine, and staircase.
Subway trains are active on four rails with each rail has a subway
car pass by every 35-50 seconds. Similar to the airport scene, the
sound layer consists of the noise of the station, people chatting,
announcements recorded from a real subway, as well as the sound

Figure 3: City scene with high lighting condition (4/4 brightness)
and medium visual intensity (2/4 walking direction level, 2/4 walking
amount level, 2/4 car amount level).

Figure 4: Ball&park scene contains a ball ﬂying near the player’s head
and with medium lighting condition (2/4 brightness) and high visual
intensity (3/4 walking direction level, 3/4 walking amount level, 3/3 car
amount level).

of footsteps and the trains passing by.

The city scene simulates 7 blocks of a city which contains vehicles
moving around, randomly generated buildings and virtual people on
the street (see Figure 3). When entering the scene, the initial position
is in the middle of a sidewalk, and participants are free to walk and
stand on the sidewalk or street. The sound in the scene illustrates
the footsteps and conversations of pedestrians, cars passing by and
honking, as well as distant sounds of the city, i.e. trafﬁc, construction
noise, and ambulance sirens.

The ball & park scene has a square-shaped park in the middle of a
city and tennis ball machines throwing balls towards participants (see
Figure 4). The participants stand in the middle of the park and need
to avoid the tennis balls from 3 tennis ball machines. The therapist
can choose how many balls will be activated at the same time (from
0 to 3) and from which direction(s). Once the ball machines are
activated, every two balls from each ball machine have a randomized
interval between 2-4 seconds. People walk back and forth on the
sidewalks, and vehicles run around the street blocks. The player can
hear the sound effect of the ball ﬂying near the head, with an addition
to the background sounds of the park: footsteps and conversations
of pedestrians, birds, and wind in the trees, and distant cars passing
by.

Because of the randomized timing, the fast ﬂying ball in the scene
is a sudden and unpredictable event which requires patients to move
outside of their base of support and may elicit anxiety or loss of
balance.

The airport scene, subway scene, and city scene are the simula-
tions of our daily life but with different space types. The airport
scene is a big enclosed space, the subway scene is a conﬁned, en-
closed space and the city scene is an open space. They can help
clinicians to explore the probable effect that the different space types
can make on their patients.

Similar to the various sunlight conditions in a real open-air envi-
ronment, the city and ball&park have four levels of lighting ranging
from 0 to 3. The brightest level simulates the sunlight at noon of
a sunny day, the second level provides a cloudy day or morning
sunlight, the third level represents dim lighting, and the fourth level
represents a dark scene. Clinicians can control the lighting level
when they need to create a lighting variant environment for their
patients to adapt to different lighting conditions.

4.2 User Interface and Content Control

To make the platform easier to use, we made an intuitive user inter-
face with sliders, checkboxes, and buttons for users to choose scenes,
change levels of visual or auditory stimulus, and enable/disable

Figure 5: The main menu of the platform. The user interface is able
to fully control the levels of the visual and auditory stimuli by using the
sliders, checkboxes, and buttons.

graphics effects or contents (see Figure 5 and Figure 6).

The “Scene” slider helps players to choose between the airport,
subway, city, or ball&park scenes. In the airport, subway, and city
scenes, we control the pedestrians’ walking speed by modifying
“Speed”, directions and paths of walking pedestrians by “WalkingDi-
rection”, and quantity in each walking group by “WalkingAmount”.
Choosing 0 speed provides a static scene (no movement), and 1-3
gives them the options of low, medium, and high speed. Additionally,
the speed of the cars in the city scene can be controlled individually
whereas the speed of the airplane in the airport scene, and the speed
of the subway train in the subway scene is ﬁxed.

“SoundLevel” with range 0-2 controls the complexity of the audi-
tory cues in the scenes. The overall amplitude of the sound layer is
self-selected to the highest comfortable level of the user.

“CarAmount” ranging from 0 to 3 controls the number of cars
on the street of the city scene and the ball&park scene. Zero will
generate no cars, and 1-3 are for low, medium and high quantity
accordingly. “Difﬁculty” adjusts the difﬁculty level of the ball&park
scene. In level 0, only one tennis ball machine is activated, and it
shoots a ball every 2 to 4 seconds. Level 1-3 each has one of the
three ball machines activated randomly, and the interval between
two balls ranges from (3 + #level) to (5 + #level) seconds. Level
4 has two of the three ball machines activated randomly, and the
interval between two balls from each ball machine ranges from 3 to
5 seconds.

The checkboxes help users to choose if they want to enable or

Figure 6: UI on the main menu can help users to choose from the
four scenes and control the complexity level of the visual and auditory
contents in each scene.

Figure 7: NavMesh(Navigation Mesh) deﬁnes walkable space by
using polygons. An agent makes a detour when it detects potential
collision during travel to the destination.

disable the details of the secondary graphics contents in the Airport,
city and subway scene. “Light”, “Color” and “Material” checkboxes
respectively control the lighting in the scenes, colors, and materials
on the buildings, ﬂoors, cars, walls, posters, etc.

4.3 Sound Implementation
Previous research indicates that sound can have a signiﬁcant in-
ﬂuence on body balance. Results showed that attributes of sound
like frequency [31] and movement [25] can both increase [26], or
decrease [33] the postural sway depending on the context.

Taking into account that the auditory system has an important
role in maintaining body balance, the implementation of sound in
our system is based on spatial audio, particularly dynamic binau-
ral rendering on headphones. Spatial audio is widely used in VR
applications as it has the advantage of creating a level of realism
not possible to achieve in traditional stereo systems [2, 4]. When
using binaural audio technology, spatial auditory cues are encoded
into a mono audio signal which results in the change of the per-
ceived localization of sound. Using the head-tracking data, audio
is modulated according to the position of the listener’s head. The
technology allows positioning of sound sources in any direction
around the listener, thus enabling the creation of soundscape much
more similar to reality and more immersive. The use of 3D sound is
especially important for this project as the goal of the system is to
create a simulation which combines different sensory cues present
in real-life situations.

Audio assets used in the system are divided into two main groups:
sound objects and ambiances. Sound objects are attached to the
visual objects in the scene and their position is changing accordingly.
These include the sounds of footsteps, trains, announcements, cars,
balls, airplanes, etc. Ambiances are created separately for each scene
and they do not change with the position of the listener and objects.
These include different background sounds, i.e. sounds of the crowd
chatter, distant trains, wind, birds, trafﬁc and general room tone of
each of the spaces. To achieve the high level of realism, most of the
sounds were gathered from original recordings performed in subway
stations and streets. The ambiances were recorded using ambisonics
technology. Ambisonics is a full-sphere surround sound format
which allows capturing the soundscape from every direction [9].
During the playback, the ambisonics sound layer changes depending
on the rotation of the listener’s head which enables the faithful
reproduction of the 360°environment with the real position of the
sound objects.

All of the sounds used in the system are assigned to three different
intensity levels which relate to the increasing complexity of the

soundscape and amount of auditory stimuli played at the same time
from different directions. The lowest level has no sound, the medium
level has a minimum number of the sound effects with moderate
complexity of background sounds, whereas the highest level creates
a bustling auditory environment with a large number of sound cues.
Clinicians can adjust the intensity of the audio layer depending on
the symptoms and abilities of each patient.

4.4 Dynamic Collision Detection
In the early versions, our system did not have a collision detection
mechanism. The walking avatars were not aware of the existence
of other game objects in the scenes so that they could run through
the player or the other walking avatars if their paths got across. This
was a little intimidating for some patients and reduced the level of
perceived realism for others. For a rich visual project aiming at a
certain degree of reality, we desired every avatar to make a detour
and plan a new route when they ﬁnd the player or other avatars on
their way so that it would not collide them.

Unity provides a navigation component called NavMesh (Nav-
igation Mesh, see Figure 7) which deﬁnes the walkable areas by
polygons and produces waypoints for game objects to walk through
the walkable polygons. The NavMesh consists of two modules:
agent and obstacle. The agent module helps to ﬁnd a walkable path
and avoid obstacles, and the obstacle module deﬁnes the objects
needs to be avoided. In our scenes, the walking avatars should be
both agent and obstacle. Because they need to avoid the player and
the other walking avatars, and they need to be avoided by the others
in the meantime. However, the two modules cannot be activated
simultaneously to the same game object according to Unity’s ofﬁcial
document and our experiments. If we activate the two module on
a game object at the same time, the object will try to avoid itself
so it would jitter or walk in circles. To have it work properly, we
still add two modules to the avatars, but alternatively turn on only
one of them at a time. Because the pathﬁnding is only needed when
a collision is coming, we activate the agent and inactivate obstacle
module only when a potential collision is detected; After the new
waypoints are generated, the avatar enables the obstacle and disables
the agent module for the next potential collision detection.

4.5 Hardware Setup
Our system is implemented in C# language using Unity Engine
version 2018.2.0f1(64-bit)(©Unity Technologies, San Francisco,
California). It uses OpenVR as the API to get full compatibility with
all major VR display platforms such as Oculus Rift and HTC Vive.
HTC Vive and Oculus Rift both have a resolution of 1080x1200

UI ControlAirportSubwayCityBall&ParkSpeed(Pedestrian)Pedestrian AmountWalking DirectionSound LevelPlaneTextureColorSpeed(Pedestrian)Pedestrian AmountWalking DirectionSound LevelSubway TrainSpeed(Pedestrian/Car)Pedestrian AmountWalking DirectionSound LevelLightingSpeed(Pedestrian/Car)Pedestrian AmountWalking DirectionSound LevelLightingCar AmountCar AmountDifficulty(Ball)TextureColorTextureColorPlayer Spawn PositionTextureColorPlayer Spawn PositionAgentObstaclesDestinationWalkable SpacePotentialCollisionDetour• “I am able to travel but in busy airports I sit in a wheelchair...
people coming from front and back and all the sounds....”

• “when I walked into the airport lobby and there was a patterned

ﬂoor. I almost fell over.”

5.1.2 Subway
The intervention includes manipulations on the quantity and walking
directions of the virtual pedestrians; choose the initial spawn position
for participants from the platform, the mezzanine, or the stairs;
change the audio intensity level between no sound, low and high
intensity sound. Clinicians can enable or disable passing trains and
pedestrians, the textures and colors on the ﬂoor, pillars, walls, etc.
The subway scene was developed based on patients’ stories, such

as:

• “I don’t feel comfortable standing on the platform, I feel like I

will fall over.”

• “I don’t take the subway, there are too many people and I am
afraid of falling onto the track if someone bumps into me.”

5.1.3 City
Clinicians can manipulate the quantity, speed, walking directions of
the virtual pedestrian; the quantity and speed of cars on the street;
enable/disable cars and pedestrians; the textures and colors of the
buildings and vehicles; 4 levels of lighting condition; audio intensity
level between no sound, low sound, and high intensity sound.

Some feedback from patients:

• “This feels like a mild version of the outside experience.”

• “I feel uneasy with the rapid change of light to dark...it’s like

in the cinema.”

5.1.4 Ball&park
The participant needs to avoid 1 or 2 balls approaching their head in
each round. Clinicians can manipulate the quantity, speed, walking
directions of the virtual pedestrian; the difﬁculty levels of throwing
balls; the quantity and speed of cars on the street; enable/disable
cars; the textures and colors of the buildings and vehicles; 4 levels
of lighting condition; audio intensity level between no sound, low
and high intensity sound.

Some feedback from patients:

• “If I dodged like that in open space, I would feel dizzy, but I

had no problem doing it within the scene!”

• “I think that would be a great therapy for making me move my

head...If you played with where it goes”

Patients completed the Short Feedback Questionnaire (SFQ) [8]
after the ﬁrst exposure to a scene. The SFQ is designed to obtain
information about the subjective responses of the participants to
the VR experience in each scenario, including enjoyment, perceived
difﬁculty of the task, sense of presence and side effects. Five physical
therapists completed the System Usability Scale once per month [38].
The Usability questionnaire includes ten items which provide a
global view of subjective assessment of a system’s usability. The
therapists also had a monthly meeting with the Principal Investigator
to discuss feedback regarding the system. Average usability scores
were 70 when PT used the system ¡ 5 times, dropped to 63 at 5-10
times usage and increased to 73 above 10 times (ideal is 100, 68 is
acceptable). SFQ scores per scene were 16 for the subway, airport,
city and 15 for ball&park (20 is the best score). Enjoyment and
‘feeling inside the task’ were rated above 4 / 5 for all (highest on
the subway), and 3.75 for ball&park. The City scene was the most
commonly used scene. Discomfort and difﬁculty (scored 1-5 where
lower is less discomfort) were around 1 for the airport and park, and
close to 2 for the subway and city. In the majority of the patients,
there was no increase in symptoms from baseline to post VR session.

Figure 8: An experimental setup. One of our case-study participants
is wearing Oculus Rift and avoiding a ﬂying ball in the park scene.

for each eye, a 90 Hz refresh rate, and 110◦ ﬁeld of view [13, 24].
The audio is implemented using Wwise®middleware and Google
Resonance plugin for 3D audio rendering.

P5000

for CPU,

or AMD FX™8350

HTC VIVE minimum speciﬁcations [13] are Intel®Core™i5-
4590
4GB for RAM,
or AMD
NVIDIA®GeForce®GTX 1070/Quadro
Radeon™Vega 56 for GPU, and Windows®8.1 for operation system.
The minimum requirements of Oculus Rift [24] are Intel i3-6100 or
AMD FX4350/Ryzen 3 1200 for CPU, 8GB for RAM, NVIDIA
GTX 1050Ti or AMD Radeon RX 470 for GPU (alternatively,
NVIDIA GTX 960 or AMD Radeon R9 290), and Windows®10 for
OS. The sound playback requires closed-back stereo headphones.
Otherwise, it is possible to use built-in headphones from Oculus
Rift although they provide less isolation from outside environment.
Our lab setup is an Alienware®laptop 15 R3 running Win-
dows®10 with 8GB RAM, Intel i7-7820HK CPU, Nvidia®GTX
1080 Max-Q GPU, and Bose SoundTrue®around-ear headphones II.
During our test, the platform runs on the highest level of graphics
contents can work at over 120 fps with either HTC Vive or Oculus
Rift.

5 EXPERIMENTS

5.1 Usability Experiment

For a new technology to be implemented successfully in clinical
practice, users should be involved in early stages of development and
evaluation of the technology [28,34]. The International Organization
for Standardization (ISO,9241-11) deﬁned the usability of a device
as the extent to which the device can be used by speciﬁc users to
achieve speciﬁc goals [6].

For over 4 months, the experiments enrolled 6 therapists and 17
patients with peripheral or central vestibular dysfunction. 4 of the
patients did not ﬁnish the whole experiment due to medical reasons,
scheduling problems or insurmountable anxiety.

5.1.1 Airport

Clinicians control the walking directions of the virtual pedestrians,
and they can choose the initial spawn position for participants from
the hall, the second ﬂoor, or the stairs. They can enable or disable
the taking-off planes, the posters and the walls, the texture on the
ﬂoor, etc. Clinicians also can change the sound level between no
sound, low sound and high sound depending on the capabilities of
patients.

This scene was developed based on patients’ stories, such as:

clinical trial.

Below are a couple of quotes from the patients participating in

the case studies:

• “I love the mezzanine in the airport because I can face my
biggest fear of transparent heights but I know that it’s virtual
and I’m not going to fall”

• “I was able to carry 3 bags on the subway...this has helped me

tremendously...”

• “I feel better after doing this then when I walked in.”

• “I’m more energized after doing it.”

• “Every time I come, I feel better after.”

In the clinic, our system was found to be feasible, and for the
most part, easy to operate. The patients enjoyed the scenes, and the
physical therapists conﬁrmed that the system added a dimension
to sensory integration rehabilitation that they did not have before.
Symptoms were minimal, particularly on the milder scenes. A
few implementation challenges should be noted. These include:
guarding the patients without blocking the Vive light-houses (if a
therapist blocked the light-house, a system restart might be required
which takes away treatment time), frequent updates of the PC which
may take away the time a therapist has with the patient and then
the therapist may choose not to use the system altogether, patients
stepping on the cable when walking around, and the cable getting
loose. To alleviate these last two limitations, we are now working
on transitioning from the current tethered VR system to a wireless
setup by using the HTC Vive wireless adapter. In September 2017,
HTC announced their wireless solution for VIVE and VIVE pro.
The solution uses Intel’s highspeed cable-free WiGig network to
transmit data between a computer and a VIVE headset. It requires
four additional hardware: PCIe WiGig card, wireless link box, Vive
Wireless Adapter, and a power bank. The wireless link box connects
to the PCIe card, wirelessly pairs with the Vive Wireless Adapter,
and helps to build a fast, low latency wireless network between a
computer and the headset. The power bank provides the necessary
power to the adapter and the headset. The wireless setup demands
the extra weight of the wireless adapter and the power bank, and
the WiGig network is not robust if the wireless adapter is further
than 20ft from the wireless link box, but patients can have a much
better immersive experience when entering the VR world untethered
without the constraints of the cable connecting the headset and
computer.

Our long-term goal is to provide a streamlined, individualized,
user-friendly intervention platform ready to be tested in clinical
trials; that will lead to improved participation restriction and quality
of life following a customized virtual environments training.

REFERENCES

[1] Y. Agrawal, J. P. Carey, C. C. Della Santina, M. C. Schubert, and L. B.
Minor. Disorders of balance and vestibular function in us adults: data
from the national health and nutrition examination survey, 2001-2004.
Archives of internal medicine, 169(10):938–944, 2009.

[2] D. R. Begault and L. J. Trejo. 3-d sound for virtual reality and multi-

media. 2000.

[3] CDC. Falls among older adults: An overview - home and recreational

safety, 2017.

[4] T. Chandrasekera, S.-Y. Yoon, and N. D’Souza. Virtual environments
with soundscapes: a study on immersion and effects of spatial abilities.
Environment and Planning B: Planning and Design, 42(6):1003–1019,
2015.

[5] E. Dannenbaum, G. Chilingaryan, and J. Fung. Visual vertigo analogue
scale: an assessment questionnaire for visual vertigo. Journal of
Vestibular Research, 21(3):153–159, 2011.

Figure 9: Pre-intervention and
post-intervention results of
the
Functional Gait Analysis of the
two participants.

Figure 10: Pre-intervention and
post-intervention results of
the
Activities-Speciﬁc Balance Con-
ﬁdence study of the two partici-
pants.

5.2 Case Studies

In two in-depth case studies, two female patients aged 72, 75 with
unilateral peripheral vestibular hypofunction participated in 8 indi-
vidualized sessions. Each session was 45 minutes long, averaging
about 5-6 scenes per session for 1-4 minutes. Typically, sessions
were performed once every week. The complexity of the environ-
ments and the level of stimulus intensity were gradually increased
depending on the patients’ progress and symptoms. Participants
were asked to walk, turn around, sidestep and turn their head within
the VR environment.

We used a combination of patient-reported outcomes and func-
tional tests of gait and balance to assess patients’ progress. The
Functional Gait Analysis [20] contains 10 walking tasks (e.g., step
over an obstacle, walk and turn, stair climbing) to assess dynamic
balance and postural stability during gait. The maximal score is
30, and a score under 22 is associated with high fall risk in older
adults. The Four Step Square test [36] accesses the ability to step
over objects forward, sideways, and backward as fast as possible.
The Activities-Speciﬁc Balance Conﬁdence is a questionnaire to re-
port one’s level of conﬁdence that he/she will not lose their balance
during various activities of daily living (0 means no conﬁdence in
their balance, 100% is full conﬁdence, a score under 67% relates
to high fall risk [35]). The Visual Vertigo Analog Scale asks the
patient to rate the intensity of their dizziness in nine situations of
visual motions [5]. The State and Trait Anxiety questionnaire (STAI
S / T) [14] evaluates participant’s anxiety levels in their daily living
(Trait) and right now (State). Lower scores are better, and 20 is the
lowest possible score.

From baseline to post-intervention, Functional Gait Analysis
improved from 20 to 26 (patient 1) and 17 to 20 (patient 2) (see
Figure 9). Four Step Square Test has not changed for patient 1
(9 seconds) but improved from 15 to 8 seconds in patient 2. Both
patients improved their Activities-Speciﬁc Balance Conﬁdence (52%
to 70% and 41% to 59%) (see Figure 10), their Visual Vertigo Analog
Scale (284mm to 211mm and 640mm to 234mm) and their State
(S) / Trait (T) Anxiety (from 46(S)/37(T) to 22(S)/27(T) and from
32(S)/25(T) to 21(S)/21(T)).

6 CONTRIBUTIONS AND LIMITATIONS

Virtual reality (VR) programs have been used for many years in bal-
ance rehabilitation, and overall have been shown to be effective for
various vestibular disorders [11]. Recent advances in head-mounted
display (HMD) technology allow for high level of immersion at low
costs and minimal technical requirement [18, 19, 22]. Our Vive app
enables patients to engage in safe yet challenging environments that
are not easily reproducible in traditional rehabilitation. Using this
platform could improve balance and reduce anxiety in patients with
vestibular disorders, especially in hectic environments. This was
observed in our case studies and will be further investigated in a

051015202530Pre-InterventionPost-InterventionParticipant 1Participant 2010%20%30%40%50%60%70%80%Post-InterventionPre-InterventionParticipant 1Participant 2[26] M. Russolo. Sound-evoked postural responses in normal subjects. Acta

oto-laryngologica, 122(1):21–27, 2002.

[27] P. H. Sessoms, K. R. Gottshall, J.-D. Collins, A. E. Markham, K. A.
Service, and S. A. Reini. Improvements in gait speed and weight shift
of persons with traumatic brain injury and vestibular dysfunction using
a virtual reality computer-assisted rehabilitation environment. Military
medicine, 180(suppl 3):143–149, 2015.

[28] S. G. S. Shah, I. Robinson, and S. AlShawi. Developing medical
device technologies from users’ perspectives: a theoretical framework
for involving users in the development process. International journal
of technology assessment in health care, 25(4):514–521, 2009.
[29] K. Sienko, S. Whitney, W. Carender, and C. Wall III. The role of
sensory augmentation for people with vestibular deﬁcits: real-time bal-
ance aid and/or rehabilitation device? Journal of Vestibular Research,
27(1):63–76, 2017.

[30] P. D. Sloane. Dizziness in primary care: results from the national
ambulatory medical care survey. Journal of Family Practice, 29(1):33–
39, 1989.

[31] R. Soames and S. Raper. The inﬂuence of moving auditory ﬁelds
on postural sway behaviour in man. European journal of applied
physiology and occupational physiology, 65(3):241–245, 1992.
[32] J. A. Stevens, P. S. Corso, E. A. Finkelstein, and T. R. Miller. The
costs of fatal and non-fatal falls among older adults. Injury prevention,
12(5):290–295, 2006.

[33] M. N. Stevens, D. L. Barbour, M. P. Gronski, and T. E. Hullar. Auditory
contributions to maintaining balance. Journal of Vestibular Research,
26(5-6):433–438, 2016.

[34] J. Van Hoof, E. J. Wouters, H. R. Marston, B. Vanrumste, and
R. Overdiep. Ambient assisted living and care in the netherlands:
the voice of the user. International Journal of Ambient Computing and
Intelligence (IJACI), 3(4):25–40, 2011.

[35] S. Whitney, M. Hudak, and G. Marchetti. The activities-speciﬁc bal-
ance conﬁdence scale and the dizziness handicap inventory: a compari-
son. Journal of vestibular research, 9(4):253–259, 1999.

[36] S. L. Whitney, G. F. Marchetti, L. O. Morris, and P. J. Sparto. The
reliability and validity of the four square step test for people with
balance deﬁcits secondary to a vestibular disorder. Archives of physical
medicine and rehabilitation, 88(1):99–104, 2007.

[37] S. L. Whitney, P. J. Sparto, J. R. Cook, M. S. Redfern, and J. M.
Furman. Symptoms elicited in persons with vestibular dysfunction
while performing gaze movements in optic ﬂow environments. Journal
of Vestibular Research, 23(1):51–60, 2013.

[38] M. M. Zare, A. Aslani, M. Fakhrahmad, and S. Ezzatzadegan. Devel-
oping an android-based patient decision aid based on ottawa standards
for patients after kidney transplant and its usability evaluation. Studies
in health technology and informatics, 249:61–68, 2018.

[6] E. Din. 9241-11. ergonomic requirements for ofﬁce work with visual
display terminals (vdts)–part 11: Guidance on usability. International
Organization for Standardization, 1998.

[7] M. Dozza, F. B. Horak, and L. Chiari. Auditory biofeedback substitutes
for loss of sensory information in maintaining stance. Experimental
brain research, 178(1):37–48, 2007.

[8] N. Erez, P. L. Weiss, R. Kizony, and D. Rand. Comparing performance
within a virtual supermarket of children with traumatic brain injury
to typically developing children: A pilot study. OTJR: occupation,
participation and health, 33(4):218–227, 2013.

[9] M. A. Gerzon. Periphony: With-height sound reproduction. Journal of

the Audio Engineering Society, 21(1):2–10, 1973.

[10] F. B. Horak, D. M. Wrisley, and J. Frank. The balance evaluation
systems test (bestest) to differentiate balance deﬁcits. Physical therapy,
89(5):484–498, 2009.

[11] M. C. Howard. A meta-analysis and systematic literature review of
virtual reality rehabilitation programs. Computers in Human Behavior,
70:317–327, 2017.

[12] S.-Y. Hsu, T.-Y. Fang, S.-C. Yeh, M.-C. Su, P.-C. Wang, and V. Y.
Wang. Three-dimensional, virtual reality vestibular rehabilitation for
chronic imbalance problem caused by m´eni`eres disease: a pilot study.
Disability and rehabilitation, 39(16):1601–1606, 2017.

[13] Speciﬁcations of headset and recommended minimum computer, 2018.
https://www.vive.com/us/product/vive-virtual-reality-system/.
[14] L. J. Julian. Measures of anxiety: State-trait anxiety inventory (stai),
beck anxiety inventory (bai), and hospital anxiety and depression scale-
anxiety (hads-a). Arthritis care & research, 63(S11):S467–S472, 2011.
[15] A. Kalron, I. Fonkatz, L. Frid, H. Baransi, and A. Achiron. The effect
of balance training on postural control in people with multiple sclerosis
using the caren virtual reality system: a pilot randomized controlled
trial. Journal of neuroengineering and rehabilitation, 13(1):13, 2016.
[16] C. Ko, H. Hoffman, and D. Sklare. Chronic imbalance or dizziness and
falling: Results from the 1994 disability supplement to the national
health interview survey and the second supplement on aging study. In
Annual Meeting of the Association for Research in Otolaryngology,
vol. 6, 2006.

[17] K. Kroenke, R. M. Hoffman, and D. Einstadter. How common are
various causes of dizziness? a critical review. Southern medical journal,
93(2):160–7, 2000.

[18] A. V. Lubetzky, D. Harel, H. Darmanin, and K. Perlin. Assessment via
the oculus of visual weighting and reweighting in young adults. Motor
control, 21(4):468–482, 2017.

[19] A. V. Lubetzky, E. E. Kary, D. Harel, B. Hujsak, and K. Perlin. Fea-
sibility and reliability of a virtual reality oculus platform to measure
sensory integration for postural control in young adults. Physiotherapy
theory and practice, pp. 1–16, 2018.

[20] G. F. Marchetti, C.-C. Lin, A. Alghadir, and S. L. Whitney. Respon-
siveness and minimal detectable change of the dynamic gait index and
functional gait index in persons with balance and vestibular disorders.
Journal of Neurologic Physical Therapy, 38(2):119–124, 2014.
[21] D. Meldrum, S. Herdman, R. Vance, D. Murray, K. Malone, D. Duffy,
A. Glennon, and R. McConn-Walsh. Effectiveness of conventional
versus virtual reality–based balance exercises in vestibular rehabilita-
tion for unilateral peripheral vestibular loss: Results of a randomized
controlled trial. Archives of physical medicine and rehabilitation,
96(7):1319–1328, 2015.

[22] A. Micarelli, A. Viziano, I. Augimeri, D. Micarelli, and M. Alessan-
drini. Three-dimensional head-mounted gaming task procedure maxi-
mizes effects of vestibular rehabilitation in unilateral vestibular hypo-
function: a randomized controlled pilot trial. International Journal of
Rehabilitation Research, 40(4):325–332, 2017.

[23] M. Morel, B. Bideau, J. Lardy, and R. Kulpa. Advantages and limita-
tions of virtual reality for balance assessment and rehabilitation. Neu-
rophysiologie Clinique/Clinical Neurophysiology, 45(4-5):315–326,
2015.

[24] Speciﬁcations of headset and recommended minimum computer, 2018.

https://www.oculus.com/rift.

[25] S. H. Park, K. Lee, T. Lockhart, and S. Kim. Effects of sound on
postural stability during quiet standing. Journal of neuroengineering
and rehabilitation, 8(1):67, 2011.

