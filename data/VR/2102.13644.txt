1
2
0
2

b
e
F
6
2

]

V
C
.
s
c
[

1
v
4
4
6
3
1
.
2
0
1
2
:
v
i
X
r
a

Surgical Visual Domain Adaptation: Results from the MICCAI
2020 SurgVisDom Challenge

Aneeq Zia1, Kiran Bhattacharyya1, Xi Liu1, Ziheng Wang1, Satoshi Kondo2, Emanuele
Colleoni3, Beatrice van Amsterdam3, Razeen Hussain4, Raabid Hussain5, Lena
Maier-Hein6, Danail Stoyanov3, Stefanie Speidel7, and Anthony Jarc1

1Intuitive, Inc.
2Konica Minolta, Inc.
3University College London
4University of Genoa
5University of Burgundy
6German Cancer Research Center (DKFZ)
7National Center for Tumor Diseases (NCT)

Abstract

Surgical data science is revolutionizing minimally invasive surgery by enabling context-aware ap-
plications. However, many challenges exist around surgical data (and health data, more generally)
needed to develop context-aware models. This work - presented as part of the Endoscopic Vision (En-
doVis) challenge at the Medical Image Computing and Computer Assisted Intervention (MICCAI)
2020 conference - seeks to explore the potential for visual domain adaptation in surgery to overcome
data privacy concerns. In particular, we propose to use video from virtual reality (VR) simulations of
surgical exercises in robotic-assisted surgery to develop algorithms to recognize tasks in a clinical-like
setting. We present the performance of the diﬀerent approaches to solve visual domain adaptation
developed by challenge participants. Our analysis shows that the presented models were unable to
learn meaningful motion based features form VR data alone, but did signiﬁcantly better when small
amount of clinical-like data was also made available. Based on these results, we discuss promising
methods and further work to address the problem of visual domain adaptation in surgical data sci-
ence. We also release the challenge dataset publicly at https://www.synapse.org/surgvisdom2020.

1

Introduction

The goal of surgical data science is to improve the quality and eﬃcacy of surgical care through the
collection, analysis, and modeling of data [1, 2, 3]. Some of the key data modalities are images and
videos [1, 4]. Consequently, within the medical imaging and computer assisted interventions domain,
there have been multiple challenges organized to target problems in machine vision [5, 6, 7, 8, 9, 10, 11].
Speciﬁcally, surgical activity detection and workﬂow analysis have a history of approaches and recent
interest [12, 13, 14, 15, 16, 17, 18, 19].

Being able to automatically recognize diﬀerent steps or tasks within a surgical procedure can lead
to development of many exciting context-aware applications, like task-based performance reports, OR
management, and real-time surgeon augmentation [12, 20, 21, 22]. However, the lack of publicly available
large-scale labelled surgical datasets for the community remains an issue [1, 3]. One of the main reasons
for this lack of data is the sensitivities around surgical data and personal health information which makes
it hard to collect and share [23]. Other reasons like diﬃculties in surgical data acquisition and annotation
also cause problems in sharing such data [3].

To address this issue, we proposed a challenge to see whether virtual reality (VR) simulations of
robotic-assisted surgical tasks can be leveraged to learn transferable representations of surgical tasks
In the broader computer vision community, this research problem is framed as
for real-life settings.
visual domain adaptation [24, 25] and already has some preliminary implementations in the medical
domain for image classiﬁcation and segmentation [26, 27, 28]. Speciﬁcally, our aim was to see if relevant
motion features can be learned from videos of VR surgical tasks and then be used to recognize those

1

 
 
 
 
 
 
Figure 1: Sample frames from the dataset for each surgical task within VR and porcine domains.

surgical tasks in clinical-like settings. Our analysis of participants models showed that supplementing a
small proportion of clinical-like data with VR data signiﬁcantly helps the models to learn generalizable
features. We discuss in detail the performance of each method in recognizing diﬀerent surgical tasks and
propose future work to further the ﬁeld of visual domain adaptation in surgery.

2 Surgical Visual Domain Adaptation Challenge

2.1 Overview

This challenge was organized as a sub-challenge 1 under the Endoscopic Vision Challenge 2 at MICCAI
2020 3. The design of the challenge was based on BIAS standards [9] (the full design document can be
found in Appendix A). We divided the challenge into two categories. The ﬁrst category required the
participants to use a mix of VR videos and a small number of clinical-like videos to train their models
and make predictions on clinical-like videos (soft-domain adaptation). Whereas, for the second category,
the participants were required to use only VR data when training their models and were tested on the
same clinical-like videos as before (hard-domain adaptation).

2.2 Data Description

The dataset consists of videos from two domains: virtual reality (VR) and clinical-like (surgical tasks
performed on a porcine model). All videos were captured from either a da Vinci robotic system (Xi or
Si) or the da Vinci simulator.There are a total of 3 surgical tasks from both VR and a porcine model:
dissection (DS), knot-tying (KT), and needle-driving (ND). The subjects performing these tasks ranged
from beginners with no surgical expertise to expert surgeons. Figure 1 shows some sample frames from
the dataset.

The training dataset consists of video clips where subjects performed any one of the 3 tasks. Each
video clip contains only one task with varying duration, i.e. each training video clip has a single task
label (ND, KT, DS). In the training dataset, video clips extracted from both VR skill exercises and a
porcine model were provided. The VR exercises were completed using a da Vinci simulator. In addition
to VR videos, a small set of video clips of analogous surgical tasks performed on a porcine model are
provided. The VR videos were captured at 60 fps with a resolution of 720p (1280×720) from one channel
of the endoscope. The porcine videos were captured at 20 fps with a resolution of 960 × 540 from one
channel of the endoscope. In total, the training set contains 450 clips from VR exercises and 26 clips
from a porcine model as detailed in Figure 2.

The testing dataset consists of 16 video clips from the porcine model only, where the subject performed
at least one of the tasks. Eight video clips in the test set contained more than one surgical task with no

1https://www.synapse.org/surgvisdom2020
2https://endovis.grand-challenge.org/
3https://miccai2020.org/

2

Figure 2: Training and testing data set characteristics used for both categories in the challenge. A) The
total number of video clips per task in VR and clinical-like porcine domains for the training set. B) The
total duration of video per task in VR and clinical-like porcine domain for the training set. C) Total
number of videos in the testing set from clinical-like porcine domain where each task appears. D) Total
duration of task time in the testing set in the porcine domain.

overlap – two tasks did not occur at the same time. Speciﬁcally, KT and ND tasks co-occurred in eight
video clips, and DS tasks occurred in the other eight videos. Any periods of inactive time in the video
or other tasks that did not qualify as one of the 3 surgical tasks used here were not used to evaluate
challenge submissions. The challenge participants had to produce frame-by-frame predictions for each
video clip in the test set.

This dataset is now released publicly and can be downloaded from https://www.synapse.org/

surgvisdom2020 and used for non-commercial purposes.

2.3 Challenge Categories

Two challenge categories are proposed to evaluate participating team’s model. For each category, teams
were not allowed to use any other surgical dataset, public or private, at any step of their training
process. However, they were allowed to use publicly available vision datasets for pre-training models,
like ImageNet [29] and Kinetics [30].

2.3.1 Category 1: Soft domain-adaptation

Challenge participants were asked to use the entire training dataset–both VR and porcine videos–to
train their machine learning models to recognize the 3 surgical tasks. As there are some porcine videos
in the training set (∼10%), we call this the “soft domain-adaptation” problem. The performance of these
models was then evaluated on the test dataset with videos of only surgical tasks on porcine model.

2.3.2 Category 2: Hard domain-adaptation

Challenge participants were asked to train their models on only the VR video clips in the training dataset
and exclude the porcine video clips. Since there are no porcine video clips in the training set, we call

3

Figure 3: Team Parakeet’s proposed spatio-temporal model architecture. VGG16: 2D VGG16 pretrained
on imagenet dataset, Conv: 3D convolutional layer, BN: batch normalization, FCN: fully connected layer,
F: number of features.

this the “hard domain-adaptation” problem. The performance of these models trained only of VR data
was then evaluated on the test dataset with only porcine videos.

3 Participating Teams Methods

3.1 Team Parakeet (University of Genoa and University of Burgundy)

Team Parakeet performed some pre-processing of data before feeding it into their model training. Since
the fps of the VR and porcine videos was signiﬁcantly diﬀerent, the training videos were downsampled to
5 fps. This frame rate was chosen as the videos consisted of moderate movements which do not require
a high refresh rate for determining movements. For memory limitations, the frames were also desampled
to a lower resolution of 224x224x3 pixels. The video frames were input to a 2D-3D convolutional neural
net (CNN) depicted in Figure 3. This team’s proposed architecture takes sequence of 8 frames as input
(8x224x224x3) and outputs the softmax classiﬁcation scores. First 2D VGG16 features (7x7x512) were
extracted for each video frame individually. The VGG network was pretrained using the imagenet
dataset [29]. The VGG16 features for the sequence were passed through a 3DConvNet to integrate
spatial and temporal information. The 3DConvNet consisted of 3x3 convolutional layers followed by
a fully connected layer and was trained from scratch. All convolutional layers had ReLU activations
followed by batch normalization. Categorical-crossentropy was used as the loss function. A 40% dropout
was used before the ﬁnal fully connected softmax layer. The ﬁlter sizes are shown in Figure 3. The
architecture was implemented on a computer with dedicated GPU (NVIDIA GeForce GTX 1080, 8 GB
RAM processor) using Keras and Tensorﬂow libraries. For category 2, only a training dataset (without
validation) was used containing all the VR video frames. Whereas for category 1, some porcine model
videos were included in the training dataset along with VR videos with a training-testing split ratio of
60:40. The training was performed for 500 epochs with a batch size of 32 using Adam optimizer with a
learning rate of 0.0001. During training, a data augmentation strategy was adopted in which the video
stream was divided into ﬁxed size segments. The input frames for the proposed VGG-3DConvNet were
randomly selected from within each segment. Thus, the input to the network was at 1 fps with data
augmentation performed on 5 fps data stream.

3.2 Team SK (Konica Minolta, Inc.)

The second participating team consisted of Satoshi Kondo from Konika Minolta Inc, Japan. The method
proposed by them was based on 3D deep neural network. In order to utilize the image features of surgical

4

Figure 4: A) Pre-processing steps for team SK. Color-variance based segmentation of tools (top) and
line ﬁlter based segmentation for surgical needle (bottom). B) Proposed model for team SK. SlowFast
network (taken from [32]) pre-trained on Kinetics 400.

tools and suture needle, two types of pre-processing were implemented as shown in Fig 4A. The ﬁrst
type of pre-processing involved detection of surgical tools based on image processing. The main colors
of the surgical tools are silver and black where the values in RGB for those colors are very similar.
Therefore, the standard deviation of RGB values for each pixel was calculated and areas where the
standard deviation was below a certain threshold were masked. This threshold was set as 10 (as the
pixel value is expressed in 8 bits). The second type of pre-processing used a line ﬁlter [31] to extract
In the
suture needle. With preprocessing, there are three cases in terms of number input channels.
ﬁrst case, the original image by itself has three input channels. In the second case, the original image
is concatenated with one of the pre-processed images resulting in four input channels. In the ﬁnal case,
the original image is used along with two pre-processed images resulting in ﬁve input channels.

For the proposed model, SlowFast network [32] was used as a base network as shown in Fig 4B. The
base network was pre-trained with Kinetics-400 dataset [30] and a fully connected layer was added at the
end to output three classes. In order to cater for the data imbalance, over-sampling and under-sampling
were performed on porcine and VR training videos, respectively. The training method used by this
team is as follows. An input image is resized to 320 x 240 after the border area is cropped. The frame
rate is sub-sampled to 5 fps and the number of frames for the 3D deep neural network is 32. Lookahead
optimizer [33] is used in training with an initial learning rate of 1e-7 for Category 1 and 1e-8 for Category
2. The learning rate is updated with cosine annealing at each epoch. The mini-batch size is kept as 12
and the experiments are run for 30 epochs using softmax cross entropy loss. Data augmentation like
translation, rotation, resizing and contrast adaptations, are also applied on the ﬂy during the training.
The ﬁnal model selected after experimentation used ﬁve channel inputs for Category 1 and four channel
inputs, the original image and the mask image, for Category 2. At inference, a moving average ﬁlter was

5

Figure 5: Workﬂow of the presented method of Team ECBA. First two segmentation models (RAS-
Net [34]) were trained on manually segmented images (one on virtual reality and the other on porcine
frames respectively). Then, once all the video images were segmented, a 3D ResNet-18 for action recog-
nition was trained to predict on test frames.

used with a length of 4 seconds.

3.3 Team ECBA (University College London)

Team ECBA hypothesized that the domain gap between real and VR images can be reduced by removing
the background of endoscopic views, which was claimed to be the one of the main diﬀerences between
the two domains. They proposed to address the domain adaption for surgical action recognition in two
separated steps, as shown in Figure 5. First, the team removed the background pixels from both VR
and porcine images using a segmentation network trained on manually segmented frames. Then, a 3D
action recognition network was trained on the segmented frames to predict surgical actions on the given
test set. Finally, once the predictions on the test videos were obtained, the team applied a classiﬁcation
ﬁlter on them to avoid rapid class bouncing due to the action changes inside a single video.

3.3.1 Segmentation network

Since no segmentation ground truth was provided for the videos in this challenge, the team manually
segmented 300 frames for each category (VR and Porcine) using VGG image annotator tool. Inside each
category, the team selected the segmentation frames to be balanced in terms of performed action, i.e.
100 frames were selected for each action (Dissection, Needle driving and Knot Tying). For each video
frame, we segmented both tools and needles, leaving all the rest as background. Before manual segmen-
tation, each frame was ﬁrst cropped to remove camera side artifacts and then resized to 384x480. Images
were ﬁnally randomly split into training (270 frames) and validation set (30 frames). The team chose
RASNet [34] as the segmentation models (one for VR and one for porcine frames). This architecture
has the famous U-net [35] structure and uses an Imagenet pre-trained ResNet [36] as feature extractor.
Moreover, in the decoding part, several reﬁned attention blocks are employed along with classic convo-
lutional layers and skip connections to allow the network to properly focus on key regions in the image.
Since the dataset the team used to train the segmentation networks did count only 300 labelled frames
for each category, they augmented the data by randomly ﬂipping them both horizontally and vertically.
The team trained their model using the sum of binary cross entropy and Intersection over Union (IoU)
score as our loss function, that has been shown to be particularly eﬀective for tool segmentation [34, 37].
The team chose Adam [38] as optimizer with learning rate 0.001, a batch size of 4, and trained both
models for 300 epochs, selecting the best as the one that maximized the IoU score on validation data.

3.3.2

3D action recognition network

Once both VR and porcine segmentation models were trained, the team trained the 3D action recognition
network on the segmented frames. For this task, the team selected the architecture proposed in [39]:
here, the authors proposed a 3D ResNet-18 trained on Kinetics dataset for surgical gesture recognition.

6

Although in their work Funke I. et al chose a temporal window of 16 frames recorded at 5 frames per
second (fps), the team doubled this window to 32 to increase the the temporal ﬁeld of view of the network,
thus allowing it to process 6 second of video in one shot. The team trained the action recognition network
using multi-class cross entropy loss with Adam optimizer, a batch size of 32 and a learning rate 0.001.
In order to train the network, the team split the segmented frames into train and validation sets. Since
validation data should be a representative sample of the test set, the team chose two porcine videos
from each action folder as validation videos, leaving all the remaining data (remaining porcine videos
and all VR videos) for training. Again, the team selected the model that obtained the best loss scores
on validation data. Finally, once the predictions on the test set were obtained, a classiﬁcation ﬁlter
was applied to avoid rapid class bouncing, that is particularly present during action changes inside a
single video. For each video frame the ﬁlter applies a temporal window of length W that covers −W/2
past frames and W/2 future frames. The new class is selected as: newCi = maxCount(Ci+t) for
t ∈ [−W/2, W/2], where i is the selected frame, C is the class predicted by the 3D network and newC is
the class after ﬁltering. In words, the ﬁlter selects the most frequent class inside the window and labels
the central time frame with that class. Since surgical actions usually last from 5 second on, the team
selected a temporal window with the size W = 100. Considering that the test videos were recorded at
20 frames per second, the selected temporal window is 5 seconds large.

4 Results

4.1 Evaluation Criteria

For each video clip in the testing set, we computed the weighted average f1-score [40] for the predicted
class labels as provided by the challenge participants. The f1-score is computed in the following way for
each label in the test video clip,

f 1 = 2 ∗

precision ∗ recall
precision + recall

(1)

Then the f1-score for each label present in the video was averaged weighted by support (the number of
true instances for each label). Teams were ranked by this mean f1-score from all videos to determine the
winner and runner-up for each category. However, to test rank stability, we also computed unweighted
average f1-scores, global f1-scores, and balanced accuracy scores for each test video clip and, subsequently,
ranked teams by their mean for these metrics across all test videos. We found no dramatic diﬀerences
in team ranking when using these other metrics.

For the sake of comparison, we have also included predictions from a random model that was equally
likely to predict any of the 3 labels for each frame of video. The predictions from the random model
serve as a baseline for the precision and recall performance of challenge submissions. In all ﬁgures below,
we refer to random model as ‘Rand’.

4.2 Category 1

For Category 1, where the training set included some porcine videos, all teams outperformed the random
model (Rand), as can be seen in the distributions of weighted average f1-scores for each team in Figure
6A. Team SK outperformed ECBA and both teams outperformed Parakeet. However, Parakeet had a
large variance in weighted average f1-scores that sometimes under-performed the random predictions.

In Figure 6B, we can see the precision and recall of each team broken down by task labels averaged
across all videos. All submissions had similarly high precision and recall for the Dissection task when
compared to the random predictions. However, the submissions tended to have higher precision than
recall for the Knot-tying task and higher recall than precision for the Needle driving task. In nearly all
cases, submissions outperformed the precision and recall of the random predictions. Nevertheless, the
recall of the submissions for the Knot-tying label was closest to the baseline provided by the random
predictions, suggesting that this was the most challenging metric for the submissions.

Upon closer inspection of the models’ performances on each test video clip in Figure 6C, we can
see that some test video clips were more diﬃcult than others for all of the model submissions. For
instance, all submissions performed only slightly better than ‘Rand’ for test_video_clip_0013. Similarly,
test_video_clip_0001 and 0006 also had drops in model performance across all submissions. These
videos where the model performances dropped included the Needle-driving and Knot-tying tasks but not
Dissection, mirroring the task-based results seen in Figure 6B.

7

Figure 6: Category 1 Results. A) Distribution of weighted average f1-scores of each team for all videos
in the test set. B) Average recall and precision for each team on each task. C) Weighted-average f1-score
for each team for each video.

On the other hand, there were some test videos where all the teams showed strong performance far
above the ‘Rand’ predictions. For instance, for test_video 0010, 0014, and 0015, all submissions had an
average weighted f1-score of nearly 1.0 suggested that most frames were predicted correctly. The videos
where the models perform dramatically better than random were videos with the Dissection task label.
When ranking the teams by overall performance we found that the mean value of various diﬀerent
metrics produced the same ranking as seen in table with Category 1 rankings. All submissions consistently
outperformed the ‘Rand’ predictions by large margins for all metrics we computed. Team SK consistently
outperformed ECBA but, for some metrics, only by a small margin. For example, the mean balanced
accuracy for Team SK and ECBA were nearly the same value. On the other hand, the unweighted
f1-score emphasized the diﬀerence between each team the most and also had the lowest value for the
random prediction model.

Category 1 rankings (average for test set)

Team
Rand
Parakeet
ECBA
SK

weighted f1-score
0.45
0.60
0.79
0.80

unweighted f1-score
0.207
0.414
0.488
0.604

global f1-score
0.327
0.599
0.742
0.774

balanced accuracy
0.327
0.644
0.776
0.778

8

4.3 Category 2

For category 2, where the training set only had virtual reality videos, the submissions had large variances
in their performance as seen in Figure 7A. Team SK and Parakeet had similar distributions of weighted
average f1-scores across all the videos and much more variance than random predictions.

Figure 7: Category 2 Results. A) Distribution of weighted average f1-scores of each team for all videos
in the test set. B) Average recall and precision for each team on each task. C) Weighted-average f1-score
for each team for each video.

When inspecting the precision and recall of each model grouped by the surgical task label in Figure
7B, we ﬁnd that the submissions don’t consistently outperform the random predictions. However, the
precision and recall of the submission from team Parakeet outperforms the random predictions for the
Dissection and Needle-driving tasks.

The mixed results in precision and recall are likely because submissions tended to perform far better
on some test video clips than others as seen in Figure 7C. The subset of test video clips for which the
submissions outperformed random predictions included only the Dissection task label. However, for
the test videos which included the Knot-tying and Needle driving tasks, the submissions often under-
performed the random predictions, suggesting the occurrence of over-ﬁtting during model training.

These diﬀerences between random predictions and submissions are further summarized in the Cat-
egory 2 rankings table. The mean value of various diﬀerent metrics produced the same rankings with
submissions outperforming the random predictions overall with diﬀerent margins. For instance, the
value of the mean weighted f1-scores only had small diﬀerences between the teams. However, balanced
accuracy and global f1-score showed a much larger gap between team Parakeet and SK.

9

Category 2 rankings (average for test set)

Team
Rand
SK
Parakeet

weighted f1-score
0.45
0.46
0.47

unweighted f1-score
0.207
0.225
0.266

global f1-score
0.327
0.370
0.475

balanced accuracy
0.327
0.369
0.559

5 Discussion

Overall, the teams performed better on the dissection tasks in both categories. For category 1, all teams
performed better than random suggesting that all models were able to learn generalizable patterns in the
dataset. More speciﬁcally, in Category 1, Team SK and Team ECBA performed similarly in metrics but
actually had diﬀerent strengths as seen by their diﬀerent precision and recalls in Figure 6B. This means
that a fusion of these two models can potentially have some added beneﬁt and perform better than the
individual models.

For Category 2, the teams performed worse than they did in Category 1, but still better overall than
random predictions suggesting that there were still some generalizable patterns learned. Team Parakeet
showed stronger performance in Dissection and Needle-driving suggesting that the VR domain provided
similar features to porcine domain for these two tasks.

Comparing diﬀerent methodologies, it seems like the approach taken by Team ECBA and Team SK
of using tool detection and having networks pretrained on Kinetics dataset, helped them in Category
1, overall. However, these techniques did not lead to better performances for Category 2. For team
Parakeet, using VGG network to extract features before learning temporal features seemed to help them
perform reasonably well for dissection and needle-driving in both categories. This suggests that a mixed
method of combining multiple model architectures may be needed to discern diﬀerent surgical tasks from
diﬀerent modalities.

It is important to note that the overall lack of performances could potentially be due to the relatively
small size and the class distribution of the dataset. The per class performances of diﬀerent teams were
almost directly proportional to the number of samples available in the training set for each class. This
shows that even though all teams did less well on knot-tying and needle-driving as compared to dissection,
additional data for these two tasks can potentially lead to much better performances.

Beyond additional data, other modeling approaches could potentially improve results as well. While
all submissions in this challenge leveraged a variant of a 3D CNN architecture, other recent work has
also shown that 2D+1 architectures – 2D CNN layers followed by 1D CNN layers – can perform well on
activity recognition problems [41, 42]. Moreover, the frame-level pre-processing methods used by team
ECBA and SK to segment or emphasize tools is similar to other work which shows that tool detection can
aid in automated surgical phase recognition [43, 44]. Additional video-level pre-processing, like optical
ﬂow calculation, could potentially improve model performances [45, 46, 47, 48].

6 Conclusion

In this paper, we provided results from a challenge that addressed the problem of visual domain adap-
tation in surgical data science. The models submitted by challenge participants showed some promise
in recognizing basic surgical tasks in clinical-like settings when trained on VR surgical data. We found
that supplementing the VR training set with even a very small proportion (∼10%) of clinical-like data
dramatically improves the generalizability of the features learned by the models. We expect one of the
constraints on the performance of the submitted models was the size of the dataset. However, we believe
that VR data and synthetic data is uniquely poised to address this issue in the long-term since the data
can be more easily generated and shared than clinical videos. While the problem of surgical visual do-
main adaptation remains unresolved, we hope that the current work and data encourage further interest
and involvement in the problem of visual domain transfer in surgical data science.

References

[1] Lena Maier-Hein, Swaroop S Vedula, Stefanie Speidel, Nassir Navab, Ron Kikinis, Adrian Park,
Matthias Eisenmann, Hubertus Feussner, Germain Forestier, Stamatia Giannarou, et al. Surgical
data science for next-generation interventions. Nature Biomedical Engineering, 1(9):691–696, 2017.

10

[2] S Swaroop Vedula and Gregory D Hager. Surgical data science: the new knowledge domain. Inno-

vative surgical sciences, 2(3):109–121, 2017.

[3] Lena Maier-Hein, Matthias Eisenmann, Duygu Sarikaya, Keno März, Toby Collins, Anand Malpani,
Johannes Fallert, Hubertus Feussner, Stamatia Giannarou, Pietro Mascagni, et al. Surgical data
science–from concepts to clinical translation. arXiv preprint arXiv:2011.02284, 2020.

[4] Andre Esteva, Katherine Chou, Serena Yeung, Nikhil Naik, Ali Madani, Ali Mottaghi, Yun Liu,
Eric Topol, Jeﬀ Dean, and Richard Socher. Deep learning-enabled medical computer vision. NPJ
Digital Medicine, 4(1):1–9, 2021.

[5] Stefanie Speidel, Lena Maier-Hein, Danail Stoyanov, Hassan Al Hajj, Gwenolé Quellec, Pierre-Henri
Conze, Mathieu Lamard, Béatrice Cochener, Imanol Luengo, Abdolrahim Kadkhodamohammadi,
and et al. Endoscopic vision challenge. Mar 2020.

[6] Olivier Commowick, Audrey Istace, Michael Kain, Baptiste Laurent, Florent Leray, Mathieu Simon,
Sorina Camarasu-Pop, Pascal Girard, Roxana Ameli, Jean-Christophe Ferré, and et al. Miccai 2016
ms lesion segmentation challenge: supplementary results, Jul 2018.

[7] Huazhu Fu, Yanwu Xu, José Ignacio Orlando, Hrvoje Bogunovi, Fei Li, and Xiulan Zhang. 2nd

retinal fundus glaucoma challenge 2020. Mar 2020.

[8] Jiancheng Yang, Xiaoyang Huang, Jiajun Chen, Donglai Wei, Bingbing Ni, Liang Jin, and Ming Li.

Rib fracture detection and classiﬁcation challenge. Mar 2020.

[9] Lena Maier-Hein, Annika Reinke, Michal Kozubek, Anne L Martel, Tal Arbel, Matthias Eisenmann,
Allan Hanbury, Pierre Jannin, Henning Müller, Sinan Onogur, et al. Bias: Transparent reporting
of biomedical image analysis challenges. Medical image analysis, 66:101796, 2020.

[10] Max Allan, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal, Yun-Hsuan Su, Nicola
Rieke, Iro Laina, Niveditha Kalavakonda, Sebastian Bodenstedt, et al. 2017 robotic instrument
segmentation challenge. arXiv preprint arXiv:1902.06426, 2019.

[11] Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi,
Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, et al. 2018
robotic scene segmentation challenge. arXiv preprint arXiv:2001.11190, 2020.

[12] Nicolas Padoy. Machine and deep learning for workﬂow recognition during surgery. Minimally

Invasive Therapy & Allied Technologies, 28(2):82–90, 2019.

[13] Shoji Morita, Hitoshi Tabuchi, Hiroki Masumoto, Tomofusa Yamauchi, and Naotake Kamiura. Real-
time extraction of important surgical phases in cataract surgery videos. Scientiﬁc reports, 9(1):1–8,
2019.

[14] Shuja Khalid, Mitchell Goldenberg, Teodor Grantcharov, Babak Taati, and Frank Rudzicz. Eval-
uation of deep learning models for identifying surgical actions and measuring performance. JAMA
network open, 3(3):e201664–e201664, 2020.

[15] Daichi Kitaguchi, Nobuyoshi Takeshita, Hiroki Matsuzaki, Hiroaki Takano, Yohei Owada, Tsuyoshi
Enomoto, Tatsuya Oda, Hirohisa Miura, Takahiro Yamanashi, Masahiko Watanabe, et al. Real-time
automatic surgical phase recognition in laparoscopic sigmoidectomy using the convolutional neural
network-based deep learning approach. Surgical endoscopy, 34(11):4924–4931, 2020.

[16] Ajay Kumar Tanwani, Pierre Sermanet, Andy Yan, Raghav Anand, Mariano Phielipp, and Ken
Goldberg. Motion2vec: Semi-supervised representation learning from surgical videos. In 2020 IEEE
International Conference on Robotics and Automation (ICRA), pages 2174–2181. IEEE, 2020.

[17] Aidean Sharghi, Helene Haugerud, Daniel Oh, and Omid Mohareri. Automatic operating room
In International Conference on Medical

surgical activity recognition for robot-assisted surgery.
Image Computing and Computer-Assisted Intervention, pages 385–395. Springer, 2020.

[18] Aneeq Zia, Chi Zhang, Xiaobin Xiong, and Anthony M Jarc. Temporal clustering of surgical
activities in robot-assisted surgery. International journal of computer assisted radiology and surgery,
12(7):1171–1178, 2017.

11

[19] Aneeq Zia, Andrew Hung, Irfan Essa, and Anthony Jarc. Surgical activity recognition in robot-
assisted radical prostatectomy using deep learning. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, pages 273–280. Springer, 2018.

[20] Kristen C Brown, Kiran D Bhattacharyya, Sue Kulason, Aneeq Zia, and Anthony Jarc. How to
bring surgery to the next level: Interpretable skills assessment in robotic-assisted surgery. Visceral
Medicine, 36(6):463–470, 2020.

[21] Roger Daglius Dias, Steven J Yule, and Marco A Zenati. Augmented cognition in the operating

room. In Digital Surgery, pages 261–268. Springer, 2021.

[22] Aneeq Zia, Liheng Guo, Linlin Zhou, Irfan Essa, and Anthony Jarc. Novel evaluation of surgical
activity recognition models using task-based eﬃciency metrics. International journal of computer
assisted radiology and surgery, 14(12):2155–2163, 2019.

[23] Lawrence O Gostin, Sam F Halabi, and Kumanan Wilson. Health data and privacy in the digital

era. Jama, 320(3):233–234, 2018.

[24] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135–

153, 2018.

[25] Xingchao Peng, Ben Usman, Neela Kaushik, Dequan Wang, Judy Hoﬀman, and Kate Saenko. Visda:
A synthetic-to-real benchmark for visual domain adaptation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, pages 2021–2026, 2018.

[26] Tobias Ross, David Zimmerer, Anant Vemuri, Fabian Isensee, Manuel Wiesenfarth, Sebastian Bo-
denstedt, Fabian Both, Philip Kessler, Martin Wagner, Beat Müller, et al. Exploiting the potential
of unlabeled endoscopic video data with self-supervised learning. International journal of computer
assisted radiology and surgery, 13(6):925–933, 2018.

[27] Faisal Mahmood, Richard Chen, and Nicholas J Durr. Unsupervised reverse domain adaptation
IEEE transactions on medical imaging,

for synthetic medical images via adversarial training.
37(12):2572–2581, 2018.

[28] Manish Sahu, Ronja Strömsdörfer, Anirban Mukhopadhyay, and Stefan Zachow. Endo-sim2real:
Consistency learning-based domain adaptation for instrument segmentation. In International Con-
ference on Medical Image Computing and Computer-Assisted Intervention, pages 784–794. Springer,
2020.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012.

[30] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017.

[31] Yoshinobu Sato, Shin Nakajima, Hideki Atsumi, Thomas Koller, Guido Gerig, Shigeyuki Yoshida,
and Ron Kikinis. 3d multi-scale line ﬁlter for segmentation and visualization of curvilinear structures
in medical images. In CVRMed-MRCAS’97, pages 213–222. Springer, 1997.

[32] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video
recognition. In Proceedings of the IEEE international conference on computer vision, pages 6202–
6211, 2019.

[33] Michael Zhang, James Lucas, Jimmy Ba, and Geoﬀrey E Hinton. Lookahead optimizer: k steps
In Advances in Neural Information Processing Systems, pages 9597–9608,

forward, 1 step back.
2019.

[34] Zhen-Liang Ni, Gui-Bin Bian, Xiao-Liang Xie, Zeng-Guang Hou, Xiao-Hu Zhou, and Yan-Jie Zhou.
Rasnet: Segmentation for tracking surgical instruments in surgical videos using reﬁned attention
segmentation network. In 2019 41st Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (EMBC), pages 5735–5738. IEEE, 2019.

12

[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-assisted
intervention, pages 234–241. Springer, 2015.

[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778, 2016.

[37] Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net with vgg11 encoder pre-trained on ima-

genet for image segmentation. arXiv preprint arXiv:1801.05746, 2018.

[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[39] Isabel Funke, Sebastian Bodenstedt, Florian Oehme, Felix von Bechtolsheim, Jürgen Weitz, and Ste-
fanie Speidel. Using 3d convolutional neural networks to learn spatiotemporal features for automatic
surgical gesture recognition in video. In International Conference on Medical Image Computing and
Computer-Assisted Intervention, pages 467–475. Springer, 2019.

[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011.

[41] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for
video action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 12046–12055, 2019.

[42] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R Manmatha, Alexander J Smola, and Philipp Krähen-
bühl. Compressed video action recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6026–6035, 2018.

[43] Odysseas Zisimopoulos, Evangello Flouty, Imanol Luengo, Petros Giataganas, Jean Nehme, Andre
Chow, and Danail Stoyanov. Deepphase: surgical phase recognition in cataracts videos.
In In-
ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages
265–272. Springer, 2018.

[44] Manish Sahu, Anirban Mukhopadhyay, Angelika Szengel, and Stefan Zachow. Tool and phase

recognition using contextual cnn features. arXiv preprint arXiv:1610.08854, 2016.

[45] Amin Ullah, Khan Muhammad, Javier Del Ser, Sung Wook Baik, and Victor Hugo C de Albu-
querque. Activity recognition using temporal optical ﬂow convolutional features and multilayer
lstm. IEEE Transactions on Industrial Electronics, 66(12):9692–9702, 2018.

[46] S Santhosh Kumar and Mala John. Human activity recognition using optical ﬂow based feature
set. In 2016 IEEE international Carnahan conference on security technology (ICCST), pages 1–5.
IEEE, 2016.

[47] Ammar Ladjailia, Imed Bouchrika, Hayet Farida Merouani, Nouzha Harrati, and Zohra Mahfouf.
Human activity recognition via optical ﬂow: decomposing activities into basic actions. Neural
Computing and Applications, 32(21):16387–16400, 2020.

[48] Senem Tanberk, Zeynep Hilal Kilimci, Dilek Bilgin Tükel, Mitat Uysal, and Selim Akyokuş. A hybrid
deep model using deep learning and dense optical ﬂow approaches for human activity recognition.
IEEE Access, 8:19799–19809, 2020.

13

A Appendix

The ﬁnal design document for the challenge can be found below.

14

