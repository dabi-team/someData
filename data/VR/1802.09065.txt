Perceptual Quality Assessment of Immersive
Images Considering Peripheral Vision Impact

Peiyao Guo, Qiu Shen, Zhan Ma, David J. Brady, and Yao Wang

1

8
1
0
2

b
e
F
5
2

]

M
M

.
s
c
[

1
v
5
6
0
9
0
.
2
0
8
1
:
v
i
X
r
a

Abstract—Conventional

images/videos are often rendered
within the central vision area of the human visual system (HVS)
with uniform quality. Recent virtual reality (VR) device with
head mounted display (HMD) extends the ﬁeld of view (FoV)
signiﬁcantly to include both central and peripheral vision areas.
image quality sensation among these
It exhibits the unequal
areas because of the non-uniform distribution of photoreceptors
on our retina. We propose to study the sensation impact on
the image subjective quality with respect to the eccentric angle
θ across different vision areas. Often times, image quality is
controlled by the quantization stepsize q and spatial resolution
s, separately and jointly. Therefore, the sensation impact can
be understood by exploring the q and/or s in terms of the θ,
resulting in self-adaptive analytical models that have shown quite
impressive accuracy through independent cross validations. These
models can further be applied to give different quality weights at
different regions, so as to signiﬁcantly reduce the transmission
data size but without subjective quality loss. As demonstrated
in a gigapixel imaging system, we have shown that the image
rendering can be speed up about 10× with the model guided
unequal quality scales, in comparison to the the legacy scheme
with uniform quality scales everywhere.

Index Terms—Peripheral vision,
quantization stepsize, spatial resolution

image subjective quality,

I. INTRODUCTION

We see the world through our eyes, with the binocular ﬁeld
of view (FoV) about 220◦ horizontally [1]. We follow the
convention to represent the range of FoV using the horizontal
viewing range throughout
this work unless we point out
otherwise. Our FoV mainly includes three distinct regions as
shown in Fig. 1(a), i.e., central vision (or macular) area (CVA)
with one-side 9◦ eccentrically, near peripheral area (NPA)
with one-side 30◦ eccentrically and far peripheral area (FPA)
covering the rest region.

Usually, conventional video frames or images are rendered
within a very limited range in our current FoV (i.e., typically
around 18◦ by calculating the distance between the user and
the display screen) on the ﬂat display in the front, such as the
TV panel, mobile screen, etc. Such viewing range overlaps
with the central vision area of our human visual system (HVS).
It is far less than our visible FoV in reality.

Panoramic and immersive images/videos are becoming quite
popular, because of the recent introduction of a variety of
powerful virtual reality (VR) devices, such as the HTC Vive,
Oculus Rift, and the Samsung Gear VR. The basic elements of
these systems are a binocular head mounted display (HMD),
with head tracking hardware as shown in Fig. 1(b). The expe-
rience of viewing 2D or 3D immersive images in this manner
can be stunning, especially given a very wide panoramic
FoV (i.e., binocular 110◦ of HTC Vive HMD that is much

larger than the 18◦ of our central vision), within which users
can navigate and interact with their virtual environment. The
sensation of a vividly virtualized reality can be dramatic
compared to traditional viewing of images on ﬁxed display
screens having very limited FoVs.

A topic of interest in this context is the perceived quality
of immersive VR images. Conventionally, we have mainly
studied the uniform subjective quality of the image and video
they are rendered in the central
by simply assuming that
vision with the highest sensation. We name few of them here,
i.e., the structural similarity (SSIM) [2], the just-noticeable-
distortion (JND) [3], the quality model considering the spatial,
temporal and amplitude resolutions of the compressed video
(Q-STAR) [4], [5], etc. As seen, recent VR devices extend the
viewing range of the displayed content signiﬁcantly (e.g., 110◦
of HTC Vive, or 90◦ of Samsung GearVR). Although it is still
less than our binocular 220◦ FoV, it already includes both the
central and peripheral vision areas. However, to the best of our
knowledge, we have not seen a systematic study discussing
the peripheral vision impact on the overall perceptual quality
of immersive images. Such study would be of great value in
the development of objective quality assessment predictors for
immersive images and videos, for analyzing the perceptual
impact of wireless transmission on immersive images and
videos, and so on.

Our HVS exhibits very different perceptual sensation in
different areas according to the existing research works [6]
on vision and neuroscience. For instance, macular (central
vision) area as aforementioned, which is about 9◦ eccentrically
(i.e., from the center of our retina), often requires ultra high
resolution and high ﬁdelity, while visual perception would
have very sharp degradation in our periphery. This is because
of the highly non-uniform distribution of photoreceptors on
the human retina [7], as shown in Fig. 1(c). Therefore, we
could leverage this biological characteristics to distribute non-
uniform image quality scales in different areas. More specif-
ically, high quality representation of the content is used in
corresponding central vision area, but reduced quality copies
in peripheral areas. Naturally speaking, for a compressed
image, a high quality version normally demands more bits for
representation, but less bits consumption for reduced quality
one [8]. Hence, such a non-uniform quality distribution in
central and peripheral vision would lead to bandwidth con-
sumption reduction, but this requires a quantitative model that
could explicitly express the peripheral vision impact on image
perceptual quality. This model could be used to give appro-
priate compression factors that save the bits consumption, but
without incurring any perceptual degradation.

 
 
 
 
 
 
2

(a)

(b)

(c)

Fig. 1.
Illustration of the panoramic FoV exhibited by the head mounted display as well as the characteristics of the vision areas: (a) central vision area
(CVA), near peripheral area (NPA) and far peripheral area (FPA) of current FoV for a panoramic scene (b) an example of head mounted display for a typical
virtual reality device (c) the distribution of photoreceptors on the human retina and corresponding non-uniform visual perception

Toward this goal, we propose to measure the image quality
degradation in peripheral areas with respect to the degree of
eccentricity (θ) from the center of the retina. Here, we use the
horizontal eccentricity to simplify the illustration of the 2-D
viewing range of a FoV. Note that the quality of a compressed
image is mainly determined by the quantization stepsize q1
and spatial resolution s [4], [5], [10]. Thus the problem can be
rephrased to devise appropriate mathematical forms to describe
the variations of q and s, with respect to the θ, separately and
jointly, referred to as the q-impact, s-impact and joint q-s-
impact, respectively.

We have invited hundreds of subjects with normal vision
to participate the subjective quality assessment for peripheral
vision impact study. Both q and s can be well modeled
using a generalized parametric Gaussian model in terms of
the θ separately. We have further discovered that q-impact is
independent of the s through the joint q-s-impact exploration.
By connecting with the perceptual quality models assuming
the uniform image quality in our previous works [10], [11],
we could ﬁnally reach a closed-form perceptual quality model
for immersive image with the consideration of the peripheral
vision impact.

Parameters are either ﬁxed or can be estimated using the
content features. Proposed models are then utilized to generate
images with non-uniform quality scales (using different q or
s in peripheral area) for independent cross-validations using
another set of data. Note that the cross-validation is performed
to rate the same immersive image but different quality setups,
i.e., one is with the conventional uniform quality while the
other one is with the unequal quality to the content regions in
the central and peripheral areas. It demonstrate the effective
results with both Pearson correlation (PCC) and Spearman
Rank Correlation coefﬁcients (SRCC) larger than 0.92.

We then devise this quality model for fast gigapixel image
retrieval and rendering with the head mounted display, given
that the model driven unequal quality setup could reduce the
data size signiﬁcantly without incurring the perceptual quality
degradation. Our tests have shown that with the unequal
quality setup, image retrieval time is about 10× reduction.
This would be very useful for the gigapixel image navigation
application.

1Images are encoded with the quantization stepsize q via H.264/AVC intra
[9], in which QP represents the quantization parameter.

codec. q = 2
We use QP and q interchangeably throughout this work.

QP−4
6

The reminder parts of this work are organized as follows:
Section II ﬁrst introduces relevant studies in the literature, and
then Section III explains the details regarding how to measure
the peripheral vision impact on an immersive image shown
in HMD, and propose analytical models to quantify the q
and s with respect to θ separately or jointly. The proposed
peripheral vision model is cross-validated in Section IV. The
proposed model is applied to guide the fast retrieval of the
gigapixel images in Section V. Finally, the conclusion is drawn
in Section VI.

II. RELATED WORK

This work greatly appreciates those efforts devoted in the
area of image and video quality assessments (IQA). Thus, we
review several of them that are commonly adopted in practice.
Following the distortion measurement of the analog signal,
mean square error (MSE) or similar peak signal-to-noise ratio
(PSNR) is easily extended to evaluate the distortion of the
image or video, assuming the quality is directly related to
the pixel amplitude degradation (i.e., due to the noise or
compression) and each pixel is equally important.

Our HVS often plays magic when viewing the image or
video by selectively ignoring or emphasizing certain regions
(i.e., masking [12]–[14]). Particularly, users are very sensitive
to the structural distortion instead of perceiving the amplitude
loss of a single pixel. Hence, structural similarity [2] can
be well captured by the mean, variance and co-variance of
the original and distorted images. All the computations are
evaluated in pixel domain as the MSE or PSNR.

National Telecommunications and Information Adminis-
tration (NTIA) has published a generalized video quality
metric (VQM) [15] based on the extensive subjective studies
performed in Video Quality Experts Group (VQEG) Phase II
Full Reference Television tests. It suggested that the distortion
of an image could be represented by the weighted distortions
of several content features (i.e., seven in total),
including
spatial activities, edge distributions, chroma spread, etc. Here,
we need to ﬁrst calculate the features from the original and
distorted image, and then derive the ﬁnal distortion score in
features’ domain.

All aforementioned metrics belong to the full reference
category that requires both the distorted and original image
in many application scenarios, original
sources. However,
sources are not available. Instead, we could utilize some partial

9°30°55°360°Panoramic FoV110°FoV of HVSCVANPAFPAlensDisplayHead Tracking  System110°60°18°(a) Attic

(b) Temple

(c) Ship

(d) Train

(e) Beach

(f) Sculpture

(g) Football

(h) Desert

Fig. 2. Examples of immersive images used for peripheral vision impact study.

3

informations (such as wavelet transform coefﬁcients [16], di-
visive normalization based representation [17], etc.) extracted
from the original image (but with much lower data rate) to
help the distortion evaluation.
Ultimately, we expect

to measure the image quality
blindly [18], [19] without any reference. Most of them are
developed based on the natural scene statistics (NSS) [20]
either in spatial domain [21], [22], or in transform domain
(e.g., DCT [23]), etc. Machine learning [23], [24] can be also
introduced to further the performance. All above non-reference
quality assessment methods are mainly focusing on the im-
age/video with ﬁxed spatial/temporal resolution. Intuitively,
compressed video can be represented as a function of its spatial
resolution (e.g., frame size). temporal resolution (e.g., frame
rate) and amplitude resolution (e.g., quantization induced pixel
amplitude degradation) for its perceptual quality [4], [25] and
bit rate [8]. Together with the content adaptive parameters,
these models can be easily applied to do multi-dimensional
optimization [26].

More recently, we have performed some preliminary studies
to model the quality of immersive images rendered on the
head mounted displays [10], [11] considering the impacts of
the spatial resolution and quantization. However, these works
still follow the traditional methodology that an uniform quality
(i.e., quantization or spatial resolution) is applied for the
entire immersive image without taking into the unequal ﬁdelity
sensation of our HVS between the central and peripheral vision
areas. In the coming paragraphs, we will discuss the perceptual
quality modeling of the immersive image considering the
peripheral vision impact.

III. PERCEPTUAL QUALITY MODELING OF IMMERSIVE
IMAGE CONSIDERING THE PERIPHERAL VISION IMPACT

It is known that human eye has different spatial resolution
distinguishability between central and peripheral vision area
because of the highly non-uniform distribution of photorecep-
tors on the human retina [7], shown in Fig. 1(c). Tyler [27]
has proposed a power function to quantify the density of cones
(e.g., measured by the number of cones per mm2) from 1◦ to
20◦ eccentrically, i.e.,

ρ(θ) = 50000 ·

(cid:19)− 2

3

(cid:18) θ
300

,

θ ∈ [1◦, 20◦],

(1)

while ρ(θ) is linearly decreased until 4000 cones/mm2 with
θ > 20◦. Mathematically, Tyler’s ρ(θ) can be seen as an
approximation of the generalized Gaussian distribution.

Intuitively, users have sensitive perception in the area with
the higher density of cones, but may not tell the difference of
image degradation in the area with less cones. We believe that
the ability to distinguish the image perceptual quality variation

should follow the density distribution of the photoreceptors
ρ(θ) in our retina. As the image quality can be represented
by its signal ﬁdelity (that is often controlled using q or s) [4],
the overall problem is to model the q or s with respect to
the degree of eccentricity θ without noticing the image quality
degradation perceptually. Towards this goal, we have carefully
designed the subjective tests to collect users’ opinion scores,
i.e. mean opinion score (MOS) so as to develop the analytical
models.

A. Subjective Experimental Setup

1) Testing Platform: We choose the HTC Vive system [28]
with its associated HMD to perform the subjective quality
assessments. This is mainly because the HTC Vive platform
offers the state-of-the-art user experience when enjoying the
immersive content. It gives the user the freedom to navigate
and interact with the content inside the virtualized environ-
ment. The HTC Vive HMD provides the binocular 110◦
FoV at 2160 × 1200 spatial resolution refreshed at 90Hz (or
FPS). To accurately control the q and s when performing the
assessments, we have implemented an interactive UI, as shown
in Fig. 3. This control panel is displayed on the screen of
a powerful computer paired with the HTC Vive system for
rendering. A technician will manually adapt the quality factors
to display different image pairs, when subject wear the HMD
and perform the rating process.

2) Test Sequence Pool: Eight immersive images, i.e., Attic,
Temple, Ship, Train, Beach, Sculpture, Football, Desert, from
the SUN360 database [29] downsampled to the spatial resolu-
tion at 4096×2160 are chosen as our test materials, as shown
in Fig. 2. Another twelve images are used for cross-validation
shown in Fig. 8. In total, we have randomly selected twenty
images with their spatial information index (SI) [30] presented

Fig. 3. Screenshot of the interactive control console interface. This interface
is installed in the HTC Vive system connected computer and a technician is
required to operate the quality factor adaptation.

When you notice the difference?CVA_qualityNPA_qualityFPA_quality1023040506070804

In general, we combine the double stimulus [31] and just-
noticeable-distortion (JND) criteria together. We show con-
secutive image pairs during the test session. Each pair is
displayed for about 5 seconds with a 3-seconds pause to record
the subjective JND opinion. There is a 1-minute interval for
subjects to rest their eyes between two different images. For
each pair, one is with the uniform quality at its highest quality
(e.g., one example is q = qmin and s = smax ) which is noted
as the anchor, and the other one is with the unequal quality
settings in central and peripheral areas. Various quality scales
are compared against the anchor to identify the boundary
that our HVS could perceive the quality difference. Since we
divide the FoV into multi-regions, rating comparison process
is performed from the CVA to FPA step-wisely.

Speciﬁcally, anchor image is presented at its native spatial
resolution smax with q = qmin (or QP 22). It is then compared
with images that are processed with various qs (or equivalent
QPs) and ss sequentially in a predeﬁned order. Normally,
we increase q or reduce s step by step to degrade the
image quality, until we ﬁnally perceive the quality degradation
subjectively - this is referred to as the JND moment. We
retrieve the recorded q or s for the image just before the JND
moment, i.e., q = qc or s = sc, as the parameters applied in
CVA. In another words, with q ≤ qc or s ≥ sc, we will not
sense any perceptual difference of the image shown in our
CVA.

Afterwards, we ﬁx the content quality at the CVA of each
test sample using the qc or sc, and degrade the quality of
both the NPA and FPA together until the subject notices the
distortion. We record the corresponding q or s, noted as qpn
or spn , respectively. Then, we ﬁx the quality of the NPA with
the qpn or spn , and continue to degrade the quality of the
FPA separately till subjects feel the difference perceptually.
Similarly, associated qpf or spf are marked.

4) Data Post-Processing: For test #1, we ﬁx the spatial
resolution at smax and adjust q to obtain the qc, qpn , and qpf
that associate with the JND moment. Similarly, we derive the
sc, spn and spf for test #2. As discussed earlier, it would
contain a large amount of combinations for joint q-s-impact
study. To reduce the complexity, we have constrained a few
typical spatial resolutions as shown in Table I. For each spatial
resolution s, we follow the similar procedure as test #1 to

TABLE I
QUALITY CONTROL PARAMETERS USED IN SUBJECTIVE ASSESSMENTS

Test #1

QP
22, 25, 28, 31, 34,
37, 40, 43, 46, 49

Test #2

0

Test #3

22, 25, 28, 31, 34,
37, 40, 43, 46, 49
22, 25, 28, 31, 34,
37, 40, 43, 46, 49
22, 25, 28, 31, 34,
37, 40, 43, 46, 49
22, 25, 28, 31, 34,
37, 40, 43, 46, 49

s

4096×2160

4096×2160, 2880×1620, 2560×1440,
1920×1080 ,1600×900, 1280×720,
720×480, 320×240

4096×2160

3072×1260

2048×1080

1024×540

Fig. 4. The spatial information indices of the test sequences.

in Fig. 4. As seen, these test samples cover a wide range
of the content characteristics, reprensenting typical scenarios
of immersive applications. Besides, each test image contains
meaningful saliency area within user’s FoV as rendered in the
HMD.

To ensure the derived model generally applicable, every im-
age are prepared with multiple versions to cover the sufﬁcient
quality scales, via different combinations of the s and/or q. As
shown in Table I, we have performed three independent tests
to study the separable q-impact, s-impact as well as the joint
q-s-impact. More speciﬁcally, for evaluating the independent
q-impact, we have enforced the spatial resolution at its native
resolution, ie., s = smax = 4096 × 2160, but applied ten
different QPs (or q); Meanwhile, we have adapted s with eight
distinct levels for each raw image (i.e., QP = 0) to study the
s-impact. For joint q-s-impact, we still use ten different QPs,
but with only four distinct spatial resolutions. One reason is to
reduce the size of the test samples for each participants. This
is because the subjects feels uncomfortable and tired after a
very long rating process. Normally, we would like to keep the
test duration for each subject less than 30 minutes [11]. In
total, we recruit 175 students (aging from 18 to 30), including
101 male and 74 female, from different majors in Nanjing
University to participate this assessment. All viewers have
normal vision (or after correction) and color perception. About
90% of viewers are naive with video processing, subjective
assessment or virtual reality.

3) Test Protocol: Subjects who wear the HMD, are asked
to stay steady by focusing on a green cross overlaid in the FoV
center without head and body movement when performing the
tests. This is to avoid viewing noise when the user randomly
shifts their attention in a large area.

As aforementioned, the rendered FoV within the HTC HMD
is 110◦-wide horizontally which includes both central and
peripheral vision areas. Therefore, we divide each FoV into
three main regions, i.e., central vision area (CVA) with one-
side eccentricity θ from 0◦ to 9◦, near peripheral area (NPA)
with θ ∈ (9◦, 30◦] and the rest from 30◦ to 55◦ for the far
peripheral area (FPA). We do not discuss the area outside the
current FoV in this work.

0306090120150SI306090120150SIAtticTempleShipTrainBeachSculptureFootballDesertStudioFishSingerGymDivingManArenaGardenConferenceChurchWeddingHouserecord corresponding qs,c, qs,pn , and qs,pf . Such process in
test #3 presumes the separable effects of the quantization and
spatial resolution similar as our earlier work [5], [11].

It

takes about 3 × 4 × 8 × 8/60 = 26 minutes for
each subject to view a test sequence including all q-induced
quality scales for each immersive image at a particular spatial
resolution s. Each test sequence is assessed by 40-50 viewers
approximately. We collect all of these raw data (i.e., q, s)
and apply the screening method to remove outliers (so as to
reduce the rating noise). More speciﬁcally, we ﬁrst generate
the probability distribution of the q or s at CVA, NPA and
FPA respectively, for each image with the data from all
subjects, and then calculate the mean µ standard deviation
σ. For the j-th image rated by the i-th subject on q-impact,
if |qj,i,c − µqj,c| > 2 × σqj,c, we would exclude this number.
Here, µqj,c and σqj,c are the mean and standard deviation of the
corresponding q at central vision area for i-th image measured
from all subjects. Furthermore, all the rating data of a subject
will be removed if we have found his/her individual rating is
excluded for two or more different image samples. After the
data screening, each test sample at a particular quality scale
has about 35 valid rating, with the mean of these valid data
ﬁnally produced as the effective q or s for each immersive
image at different visual areas (i.e., CVA, NPA and FPA).

B. Analytical Models

As aforementioned, our HVS exhibits quite different sensa-
tion among various areas. Hence, we would like to leverage
such characteristics to implement the unequal quality scales,
i.e., reduced quality at peripheral areas, but without perceptual
degradation. This would reduce the image size, given that
bit rate consumption is typically decreased when reducing
the image quality [2], [5]. Therefore, quantitative models that
expresses the perceptual quality considering the peripheral
vision impact are highly desired. This section details the model
development. More speciﬁcally, we ﬁrst explore how central
and peripheral vision inﬂuence the distinguishability of im-
age’s spatial resolution sor quantization stepsize q separately,
and then extend the study on joint impacts of the quantization
and spatial resolution. Together with our previous work [4],
[10], [11], we ﬁnally provide a closed-form perceptual quality
model to explicitly explain the image quality (in terms of the
MOS) with the consideration of unequal impact in the central
and peripheral vision areas.

1) Separate Impact of Quantization and Spatial Resolution:
To simplify the model development, q and s are ﬁrst normal-
ized using qmin = 8 (or QP 22) and smax = 4096 × 2160, i.e.,
ˆq = qmin/q, and ˆs = s/smax, respectively. When q = 64, the
corresponding ˆq is 0.125; while s = 2048 × 1080, ˆs = 0.25.
Figure 5 and 6 show the collected ˆq and ˆs with respect to the
eccentric angle θ.

We have found that an uniﬁed parametric generalized Gaus-

sian function could explain the ˆq and ˆs very well, i.e.,

1
√
2π
c

× e−

|(b·θ)a|

2c2 + d,

(2)

where a, b, c, d are control parameters derived using the least-
squared criteria by ﬁtting the measured points and hypothe-

5

(a) Attic

(b) Temple

(c) Ship

(d) Train

(e) Beach

(f) Sculpture

(g) Football

(h) Desert

Fig. 5. Normalized quantization versus eccentric angle θ. e represents the
root mean square error (RMSE). Parameters are ﬁxed for all image content.
Discrete points are measured data, while continuous curve is ﬁtted model.

sized analytical model in Eq. (2). This model (2) also coincides
with the density distribution of photoreceptors ρ(θ), where
Laplacian distributed ρ(θ) is a special case of the generalized
Gaussian function.

Fitted parameters for ˆq(θ) and ˆs(θ) are shown in Table II,
respectively. Note that parameters are different for q and s
due to the reason that adapting the q infers the high frequency
information loss of compression while varying the s implies
the loss of some low frequency content.

Parameter a is identical as it reﬂects the decay speed of
the quality perception with respect to the increase of the θ.
This is mainly because it is determined by the density of
the cones of human retina. b is correlated with the quality

 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0190 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0281 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0101 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0561 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0200 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0180 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.0231 -60 -40 -20  0  20 40 60 θ 00.10.20.30.40.5ˆq(θ)e=0.05596

we can still use the ﬁxed c for all images. It is suspected
that current Vive display does not offer sufﬁcient pixel den-
sity (e.g., pixel per inch or PPI) to accurately reﬂect the
compression-induced amplitude variations per pixel. But for
spatial re-sampling, pixels are restored with predeﬁned ﬁlers,
which signiﬁcantly differ from the original pixel in native
4096×2160 resolution.

Parameter d indicates the model limits based on our intu-
ition. For example, we could not perceive any difference when
θ goes to inﬁnity (i.e., the number of the photoreceptor goes
to zero). For q, we could apply the q = qmax = 228 (with
corresponding QP 51 used in H.264/AVC and HEVC [32],
[33]), implying d = 0.03. But the worst image quality that
subjects can distinguish is with QP 48, it’s suitable to set d=
0.05; For s, it is impractical to have s = 0 for rendering, thus
we set d = 0.06 with the least model prediction error.

Furthermore, we introduce how to predict parameter c for
model ˆs(θ). Intuitively, image quality is mainly determined by
its spatial complexity, color distribution, and local orientation.
Through careful examination, we have found that c could be
predicted by the linear combination of the ρcSI, ρµI and ρµγv ,
i.e.,

+ 0.2557, (3)

c = −0.002 · ρcSI + 0.4342 · ρµI + 3.9029 · ρµγv
where ρcSI is the SI of the central vision area of an image.
(This is because we constrain the saliency region in the central
vision.) ρµI
is the averaged intensity of the image in HSI
color space [34]. ρµγv refers to the intensity of the vertical
orientation which is calculated using a 3×3 Gabor ﬁlter [35].
2) Joint Impacts of Quantization and Spatial Resolution:
This section investigates the joint impacts of the quantization
and spatial resolution on the perceptual quality with respect
to the eccentricity θ. Generally speaking, the joint impacts
are not indeed separable. However, motivated by our previous
work [4], [5], [8], we still attempt to enforce the separable
effects for q and s. Towards this purpose, we have performed
the test #3 shown in Table I, where the q-impact is studied
at different spatial resolutions. To reduce the overall rating
duration, we use a few typical spatial resolutions, but still
allow ten distinct qs to cover a variety of quality scales.

We plot the normalized ˆq(θ) at different spatial resolution
s in Fig. 7. It is found that discrete measurements are almost
overlapped for different spatial resolutions. This implies that
a single analytical model may be sufﬁcient to explain the q-
impact at different s. Nevertheless, we directly ﬁt the discrete
ˆq(θ)s using Eq. (2), ﬁrst assuming the independent parameters
at different spatial resolution, and then enforcing the same
parameters for all spatial resolutions, via the least squared
error criteria. Parameters are listed in Table III. As seen, we
could deﬁnitely pursue the least error with s dependent param-
eters, but a model with ﬁxed parameters is easier for future
application deployment. On the other hand, the error is within
reasonable range for ﬁxed parameters setup in comparison to
the s adaptive parameters. Thus, we propose to apply the ﬁxed
parameters for the following discussion. As will be unfolded
in cross-validation, even with the ﬁxed parameters, our model
still offers quite impressive performance for subjective quality
estimation together with our preliminary studies in [11].

(a) Attic

(b) Temple

(c) Ship

(d) Train

(e) Beach

(f) Sculpture

(g) Football

(h) Desert

Fig. 6. Normalized spatial resolution versus eccentric angle θ. e represents
the root mean square error (RMSE). Parameters except c are ﬁxed, while c is
content dependent. Discrete points are measured data, while continuous curve
is ﬁtted model.

degradation factors. As aforementioned, q would introduce
compress-induced high frequency information loss while s
brings the loss of some low frequency content due to the
spatial re-sampling.

Parameter c is generally content-dependent. But for ˆq,

TABLE II
PARAMETERS IN PERIPHERAL VISION MODEL FOR ˆq(θ) OR ˆs(θ)

b
0.08
0.033
x in c(x) are content features extracted from the image.

c
1.38
c(x)

d
0.05
0.06

a
2.2
2.2

ˆq(θ)
ˆs(θ)

 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.55e=0.0714 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.74e=0.0453 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.64e=0.0055 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.78e=0.0661 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.71e=0.0428 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.80e=0.0474 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.76e=0.0532 -60 -40 -20  0  20 40 60 θ00.20.40.60.8ˆs(θ)c=0.69e=0.05137

(a) Attic

(b) Temple

(c) Ship

(d) Train

(e) Beach

(f) Sculpture

(g) Football

(h) Desert

Fig. 7. Normalized ˆq(θ) at different spatial resolutions. Discrete points are measured data; while the curve is ﬁtted model.

3) The Overall Quality Model considering the Joint Impacts
of Quantization and Spatial Resolution : We could easily
derive the appropriate quality control factors q and s with
respect to the θ following the above discussion. However,
in practice, we are often required to provide the subjective
quality quantitatively, i.e., relating the quality control factors to
the perceptual quality (typically represented using the MOS).
Fortunately, our previous attempts in [4], [5], [36] have inves-
tigated the impacts of the spatial resolution, and quantization
on conventional image/video that are rendered with a very
limited FoV. Recently, we have extended this study to the
immersive image displayed using the HMD with a 110◦ [10],
[11] FoV. All of these studies have been conducted for the
image/video with the uniform quality, even for the immersive
image. Moreover, these works suggest that the impacts of the
spatial resolution and quantization on the perceptual quality
are generally separable for practical application.

Combining the models in [11] and Eq. (2), we could ﬁnally

reach a closed-form function at

0.7

Q(ˆs, ˆq) = Qmax ·

1 − e−α·ˆs
1 − e−α
where Qmax is the averaged MOS of the image at smax and
qmin, and parameter α and β(s) can be predicted via features
of the image content itself. As demonstrated in [11], Qmax

1 − e−β(s)·ˆq
1 − e−β(s)

(4)

·

,

TABLE III
PARAMETERS FOR ˆq(θ) AT DIFFERENT SPATIAL RESOLUTION. FITTING
ERROR IS REPRESENTED USING RMSE.

s
4096×2160
3072×1260
2048×1080
1024×540

a
2.2
2.2
2.4
2.4
2.2

b
0.05
0.05
0.06
0.05
0.055

c
1.2
1.3
1.2
1.1
1.1

d
0.05
0.05
0.06
0.08
0.06

e
0.0330
0.02365
0.04789
0.04733
0.04567

ˆq(s, θ)

ˆq(θ)

can be set as a constant, i.e., 86. This also ﬁts our intuition
that users might give similar MOS for the same high ﬁdelity
content, as long as they have corrected visual sensation.

For the scenario that only spatial resolution adapts with
ﬁxed quantization, Q(ˆs, ˆq) is deducted into Q(ˆs). Following
the model in (2), we could assign the different spatial resolu-
tions in central and peripheral vision areas, but still offering the
same perceptual quality as the one using the uniform spatial
resolution everywhere. Intuitively, we use the highest spatial
resolution at the central vision area, but reduced spatial reso-
lution in the periphery. This implies that the perceptual quality
of the immersive image with non-uniform spatial resolution in
the central and peripheral vision areas is determinted by the
quality in the central vision area, i.e.,

Q(ˆs(θ)) = Q(ˆs(θc)) = Qmax ·

0.7
c

1 − e−α·ˆs
1 − e−α ,

(5)

with sc = ˆs(θc) representing the spatial resolution in the
central vision area within the eccentric θc.

Similarly, we could reach at

Q(ˆq(θ)) = Q(ˆq(θc)) = Qmax ·

1 − e−β(smax)·ˆqc
1 − e−β(smax)

,

(6)

with qc = ˆq(θc) representing the quantization stepsize in
the central vision area within the eccentric θc, for the case
that only quantization stepsize q varies at the native spatial
resolution smax.

Furthermore, as revealed in aforementioned study on the
joint impacts of the quantization and spatial resolation, the q-
impact can be ﬁxed for all spatial resolutions. Thus, we would
derive the perceptual quality of the image as

Q(ˆs, ˆq(θ)) = Q(ˆs, ˆq(θc))

= Qmax ·

0.7

1 − e−α·ˆs
1 − e−α

·

1 − e−β(s)·ˆqc
1 − e−β(s)

,

(7)

 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x2160 -60 -40 -20  0  20 40 60 θ00.10.20.30.40.50.6ˆq(θ)s = 1024x540s = 2048x1080s = 3072x1260s = 4096x21608

(a) Studio

(b) Man

(c) Diving

(d) Singer

(e) Gym

(f) Fish

(g) Arena

(h) Garden

(i) Conference

(j) Church

(k) Wedding

(l) House

Fig. 8.

Immersive images used for independent cross-validation that are randomly selected from SUN360 database [29] to cover a variety of scenarios.

where s is the current spatial resolution and qc is the quanti-
zation stepsize in the central vision area.

IV. INDEPENDENT MODEL CROSS-VALIDATION

This section details the cross-validation for our proposed

analytical models.

A. Cross-Validation of Separate Impacts of Quantization and
Spatial Resolution

For individual q-impact and s-impact, we invite another
individual subjects to participate the independent cross-
validation assessments. Six images (image (a)- (f) shown in
Fig. 8) are randomly selected from SUN360 database [29].
Different from the model development where we combine the
double stimulus and JND to ﬁnd out the impacts of the quan-
tization and spatial resolution on the perceptual quality with
respect to the eccentric angle θ, we propose to directly measure
the MOSs for each image pair. One is with the uniform quality
using qc or sc (i.e., for simplicity, we could initially set qc =
8, sc = 4096×2160 or other native resolutions); and the other
one is with non-uniform quality using q(θ) or, s(θ). For non-
uniform quality, the entire image is divided into seven parts to
ensure the smooth quality transition across regions as shown in
Fig. 9. Associated q or s (or corresponding ˆq or ˆs) are derived
through developed model in (2). Note that except those ﬁxed
parameters, content features are extracted from the images to
derive the corresponding c via (3) explicitly.

To let the participants familiarize themselves with the qual-
ity scales from the worst (MOS = 0) to the best (MOS =

10), we prepare the test samples for the Attic and Dessert
image in Fig. 2 with uniform quality at different scales through
multiple qs or ss. We then mix the test image pairs from all test
images shown in Fig. 8, and place them randomly to collect
MOSs. Subject is asked to give its MOS from 0 to 10 for each
displayed sample sequentially. Note that each image sample
repeats three times in total. Thus, the same content is repeated
for six times: three for the uniform quality, and other three for
the copy with non-uniform quality. Intuitively, the MOSs for
each test sample should be very close from a speciﬁc subject.
We enforce the repetition to avoid the random noise.

For each image sample, all raw MOSs from all subjects
are collected and then processed following the same screening
method discussed in Sec. III-A. Finally, the averaged value is
referred to as its MOS. We then plot the MOSs for the samples
with uniform quality versus the MOSs for the samples with
non-uniform quality, of the same image content, in Fig. 10 for
respective q-impact and s-impact.

We analyze the mean MOS of the image with uniform
quality as well as corresponding non-uniform quality. As
the comparison demonstrates, more than ninety percent of
participants cannot distinguish image sample with uniform or
non-uniform quality conﬁguration. Both high PCC and SRCC
in Fig. 10 have shown that the MOSs of non-uniform quality
samples are highly correlated with the MOSs of uniform
quality samples, implying the impressive efﬁciency of our
proposed individual q(θ) and s(θ) to model the peripheral
vision sensitivity impact quantatitively.

Fig. 9. An image with non-uniform quality using various q in central and
peripheral areas via model (2).

Fig. 10.
Illustration of measured MOS for the images with uniform
quality (UQI) versus corresponding ones with non-uniform quality (NUQI)
considering q-impact or s-impact separately.

2345MOS of UQI22.533.544.55MOS of NUQIq-impacts-impactPCC=0.9367SRCC=0.93719

(a)

(b)

(c)

(d)

Fig. 12.
Illustration of a gigapixel streaming system: (a) end-to-end
architectural overview (b) multi-scale structure considering the variations from
both spatial resolution and quantization (c) different quality scales applied
among distinct vision areas of our FoV with HMD (d) uniform quality applied
in our FoV with HMD

rendered in such high pixel resolution,
it could cover an
ultra wide scene and also provide thumbnail scale texture
details simultaneously. For a typical display with 1080p or 4K
resolution in the market, we could zoom in/out to navigate the
gigapixel image to locate the region of interest. To ensure the
smooth navigation, we have proposed a multi-scale pipeline
and devised our models to enable the ultra-low-latency content
retrieval without any loss of the perceptual quality.

A. System Architecture

Figure 12 illustrates the end-to-end pipeline from the image
capturing to ﬁnal display. Such gigapixel image/video can be
captured via a parallel or array camera [38]. One example is
the Mantis 70 from Aqueti Inc. (www.aqueti.com). The Aqueti
Mantis 70 camera is an array of 18 narrow ﬁeld microcameras
each with a 25 mm focal length lens and a 1.6 micron pixel
pitch. Each uses a Sony IMX 274 color CMOS sensor. Sensor
read-out, ISP and data compression is implemented using an
array of NVIDIA Tegra TX1 (http://www.nvidia.com/object/
tegra-x1-processor.html) modules with 2 sensors per TX1. The
sensors are arrayed to cover a 73 horizontal ﬁeld of view and
a 21 degree vertical ﬁeld of view. The instantaneous ﬁeld of
view is 65 microradians and the fully stitched image has a
native resolution of 107 megapixels.

For each 107 megapixel image sampled at RGB color space,
it requires the network bandwidth at (3×107×5)/20 ≈ 642

(a)

(b)

Fig. 11.
NUQI (b) predicted MOS of NUQI versus its measured MOS.

Illustration of (a) measured MOS of UQI versus its corresponding

B. Cross-Validation of Joint Impacts of Quantization and
Spatial Resolution

This section extends the cross-validation to the scenarios
that impacts of the quantization and spatial resolutions are
twelve images
considered jointly. Quality variation of all
(shown in Fig. 8) are assessed by more than ﬁfty subjects.
In addition to the native spatial resolution at smax, we also
prepare another two discrete spatial resolutions at 3584×1890
and 1523×810. At each s, we compress the different regions
with corresponding q(θ) following the (2) (cf. Fig. 9). We then
apply the same procedure as discussed in Sec. IV-A to collect
the MOSs (i.e., for both uniform and non-uniform quality).
Here, subjects rates range from 0 (bad) to 10 (excellent) [31].
All raw MOSs are screened as well to reduce the rating noise.
Figure 11(a) reveals that most of the subjects can not tell
the difference between the uniform and non-uniform quality
copies of the same content, with very high PCC at 0.964 and
SRCC at 0.953.

C. Self-Adaptive MOS Prediction

Aforementioned cross-validations mainly compares the im-
age pairs where one is prepared with conventional uniform
quality and the other one is with the non-uniform quality via
applying different q and/or s among different visual areas. This
section we explore the possibility to predict the MOS through
the models in (5), (6) and (2), and evaluate the correlations
against the collected MOSs via the subjective assessment.

Since we focus our attention within current FoV when
wearing the HMD. Instead of extracting the content features
from the entire immersive image in [11], we propose to
extract the features within current FoV only to derive the
model parameters. With these content adaptive parameters, we
could easily estimate the MOS in a straightforward fashion.
Figure 11(b) illustrates the predicted MOS versus the collected
MOS for those image samples prepared with the unequal
quality scales among central and peripheral vision areas,
resulting in averaged PCC and SRCC more than 0.87.

V. APPLICATION

With the advances in both imaging and display tech-
nology, image resolution gets improved signiﬁcantly, from
conventional megapixel scale (such as 1080p, 4K) to the
gigapixel scale (such as 40K, 64K) [37]. With the image

0246810MOS of UQI0246810MOS of NUQIPCC=0.9645SRCC=0.95260246810predicted MOS of NUQI0246810MOS of NUQIPCC=0.8729SRCC=0.8743WorkerWorkerWorkerWorkerCoordinatorCDN EdgesCDN EdgesCDN EdgesBackbonerawstreamsParallel CameraCDNMultiscale ImageAccelerationUserq(θ)Spatial Resolution ScaleQuantization Scale# 0# 1# 2# N-1# 0# 2#M-1  FPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPAFPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPANPACVACVACVACVAtiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletiletile10

(a)

(b)

(c)

Fig. 13. Gigapixel source images used for performance comparison: (a) Road (trafﬁc intersection) (b) Park (university campus) (c) Airport

Mbps. Here, we assume the compression ratio is 20× of a typ-
ical image coding method (such as the H.264/AVC Intra [32])
and 5 FPS continuous snapshots speed of a mainstream DSLR.
Even for the image sampled at YUV420 space, it still demands
stable 341 Mbps to sustain the high quality image streaming
and rendering over the Internet. This actually imposes the huge
challenge to enable the real-time gigapixel image playback.
Thus, we have developed a multi-scale image acceleration
(MIA) engine that would process each gigapixel image into
multiple quality scales (i.e., via a variety of q and s) according
to our proposed models in (2).

The MIA engine enables the massive parallel processing
so as to produce the each basic processing unit at multiple
quality scales in real-time (or even much faster). Here, the
basic processing unit is often referred to as the “tile”. As
shown in Fig. 12(a), the MIA mainly includes two parts: one
is the coordinator that captures the raw inputs from a parallel
camera, scales the spatial resolutions and distributes the tiles
into multiple parallel workers; and the other one is the worker
focusing on the real-time bitrate transcoding where any high-
quality tile video input is transcoded into various versions with
multiple quantization applied. With various combinations of
the spatial resolution and quantization, we can offer a great
amount of quality (and bitrate [5]) scales to enable the real-
time delivery over the existing network. Bit stream at various
quality scales (or bit rate scales) of the same content are used
to combat the network dynamics, such as congestion, fading,
etc [39].

In other end, users could wear a Samsung S7 powered
GearVR to perform the image navigation in a virtualized
space (such as zoom in/out, 6 degree-of-freedom movement,
etc), shown in Fig. 12(a) as well. Samsung S7 features a
2560×1440 display, covering the content of our current FoV
in the front. Typically, a speciﬁc FoV consists of one or more
tiles (or tile videos). FoV or viewport navigation can be easily
achieved via the tile video adaptation. In practice, we need to
setup an appropriate tile size to fully leverage the peripheral
vision model and carefully balance the trade-off between the
coding efﬁciency and processing parallelism. In this work, we
set the spatial resolution of a tile at 256×144.

B. Performance Comparison

In practice, the landscape overview of the gigapixel image is
ﬁrst displayed in GearVR, where the gigapixel image is scaled
to ﬁt the S7 screen as aforementioned. The images are shown
in Fig. 13. We then can zoom the image to the next higher
spatial resolution step by step, to show the ﬁne details of a
particular region of interest. We can also perform directional

navigation to focus our attention to a new FoV, as presented
in Fig. 14(a).

In default, a gigapixel image is scaled at various spatial
resolutions but with quantization ﬁxed at qmin to ensure
the high-quality rendering. Alternatively, besides the spatial
resolution scales, we have processed the content at each
spatial resolution into multiple quantization scales, shown in
Fig. 12(b). We enforce the tiling for both default and proposed
schemes. For the FoV rendered in HMD, default solution will
have uniform quality everywhere shown in Fig. 12(d), but
our proposed scheme would have the highest quality in CVA,
reduced quality in NPA and further reduced quality in FPA, as
illustration in Fig. 12(c). As demonstrated previously, quality
scales can be achieved by adapting quantizaton q and spation
resoltion s across various vision areas. In the meantime, it
results in the data size reduction because both quality and
bit rate of the image/video content are the function of the q
and s [4], [8]. Given the same access network condition (i.e.,
bandwidth), the smaller data size, the faster image renders. On
average, our scheme could save the image retrieval time about
90.18% shown in Table IV.

It is also noted in Table IV that retrieval time saving is con-
tent dependent. Therefore, we perform the image navigation
and record the saving percentage (∆T ) in Fig. 14(b), with
respect to the size of lossless FoV image (coded with q = 0).
As shown, we could even achieve almost 20x speedup (about
95% reduction of retrieval time) when the FoV content demon-
strates the smooth distribution (with small SI) dominantly (sky
scene highlighted as FoV#1, FoV#2); On the contrary, the
saving is reduced when the content is complex with complicate
texture (grass land highlighted as FoV#4).

VI. CONCLUSION

Non-uniform distribution of photoreceptors on human retina
infers that visual perception on natural image is less sensitive
in peripheral vision area than in central vision area. We have
studied such peripheral vision impact on perceptual quality of
immersive images rendered in virtual reality equipped head
mounted display, and derived closed-form theoretical models
that explicitly describe the control factors (i.e., quantization
stepsize, spatial resolution and their joint effect) of image
quality with respect to the degree of the eccentricity.

Models are well explained by an uniﬁed parametric gen-
eralized Gaussian function where parameters are either ﬁxed
or can be well estimated by the features extracted from the
native content. We randomly select another set of images,
extract their features and form the models to guide the non-
uniform quality assignment in different regions and predict the
subjective quality of the image. Independent cross validation

11

Scene

R = 10Mbps

TABLE IV
AVERAGED RETRIEVAL TIME OF THE IMAGE SHOWN IN CURRENT FOV
WITH/WITHOUT PERIPHERAL VISION MODEL (2).
R1= 5Mbps
3/s
2/s
tm
tori
0.5381
6.3312
Road
0.5513
6.2912
Park
0.9388
8.0340
Airport
average 4
0.6761
6.8855
1 R represents transmission rate.
2 tori represents the loading time of a original gigapixel-scale image.
3 tm represents the loading time of a gigapixel-scale image with non-uniform quality

Saving
time
91.71%
91.50%
89.18%
90.18%

tm/s
0.1345
0.1378
0.2347
0.1690

tori/s
1.5828
1.5728
2.0085
1.7214

tori/s
3.1656
3.1456
4.0170
3.4427

tm/s
0.2691
0.2757
0.4694
0.3381

R = 20Mbps

model applied.

4 In average scene, we calculate the averaged retrieval time among the above all

scenes.

users navigate the content inside an virtualized environment.
Such navigation induced quality impact exploration is also
stuided in our another work [40]. We also would like to
make our data public accessible at http://vision.nju.edu.cn/
immersive video/ and encourage more participants from the
society to work on this avenue.

(a)

ACKNOWLEDGMENT

The authors would like to thank all volunteers for their

contribution in subjective assessments.

REFERENCES

[1] (2017). [Online]. Available: https://en.wikipedia.org/wiki/Field of view
[2] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Trans. Image Processing, vol. 13, no. 4, pp. 600 – 612, April 2004.
[3] C.-H. Chou and Y.-C. Li, “A perceptually tuned subband image coder
based on the measure of just-noticeable-distortion proﬁle,” IEEE Trans.
Circuits and Systems for Video Technology, vol. 5, no. 6, pp. 467 – 476,
August 1995.

[4] Y. Xue, Y.-F. Ou, Z. Ma, and Y. Wang, “Perceptual video quality
assessment on a mobile platform considering both spatial resolution and
quantization artifacts,” in Proc. of PacketVideo, 2010.

[5] Z. Ma, M. Xu, Y.-F. Ou, and Y. Wang, “Modeling Rate and Perceptual
Quality of Video as Functions of Quantization and Frame Rate and Its
Applications,” IEEE Trans. Circuits and Systems for Video Technology,
vol. 22, no. 5, pp. 671 – 682, May 2012.

[6] H. Strasburger, I. Rentschler, and M. Juettner, “Peripheral vision and
pattern recognition: A review,” Journal of Vision, vol. 11, no. 13, pp.
1–82, May 2011.

[7] C. A. Curcio, K. R. Sloan, R. E. Kalina, and A. E. Hendrickson, “Human
photoreceptor topography,” Journal of comparative neurology, vol. 292,
no. 4, pp. 497–523, 1990.

[8] Z. Ma, F. C. A. Fernandes, and Y. Wang, “Analytical rate model
temporal and

for compressed video considering impacts of spatial,
amplitude resolutions,” in Proc. of the IEEE ICME, May 2013.

[9] S. Ma, W. Gao, D. Zhao, and Y. Lu, “A study on the quantization scheme
in h. 264/avc and its application to rate control,” Advances in Multimedia
Information Processing-PCM 2004, pp. 192–199, 2005.

[10] R. Zhou, M. Huang, S. Tan, L. Zhang, D. Chen, J. Wu, T. Yue, X. Cao,
and Z. Ma, “Modeling the impact of spatial resolutions on perceptual
quality of immersive image/video,” in Proc. of the IEEE IC3D, Dec
2016.

[11] M. Huang, Q. Shen, R. Zhou, Z. Ma, X. Cao, and A. C. Bovik,
“Modeling the perceptual quality of immersive images rendered on head
mounted displays,” submitted to IEEE Trans. Image Processing, 2017.
[12] G. E. Legge and J. M. Foley, “Constrast masking in human vision,”
Journal of the Optical Society of America, vol. 70, no. 12, pp. 1458–
1471, 1980.

[13] P. J. Bex, “Sensitivity to spatial distortion in natural scenes,” Journal of

Vision, vol. 10, no. 23, Feb 2010.

(b)

Illustration of the (a) gigapixel image navigation (b) retrieval time
Fig. 14.
reduction (∆T ) distribution with respect to the lossless image size of current
FoV (L)

is conducted to demonstrate that users could not tell the differ-
ence between uniform and non-uniform quality assignments in
peripheral vision. This evident the efﬁciency of our proposed
models.

We further devise our proposed model to apply the non-
uniform quality setup for a real-time gigapixel imaging and
rendering system. Compared with the legacy scenario that
images are exisited with uniform quality, our model guided
non-uniform quality scales among various tile videos could
signiﬁcantly reduce the image size and therefore improve the
rendering throughput about 10×, without noticeable perceptual
quality degradation.

As the future work, we will combine this static spatial vision
study with the temporal variation that is often happened when

FoV #1FoV #2FoV #3FoV #412

[40] S. Xie, Y. Xu, Q. Qian, Q. Shen, Z. Ma, and W. Zhang, “Modeling
the perceptual impact of viewport adaptation for immersive video,” in
submitted to Circuits and Systems (ISCAS), 2018 IEEE International
Symposium on, 2018.

[14] J. A. Redi, P. Gastaldo, I. Heynderickx, and R. Zunino, “Color distri-
bution information for the reduced-reference assessment of perceived
image quality,” IEEE Trans. Circuits and Systems for Video Technology,
vol. 20, no. 12, pp. 1757–1769, Dec. 2010.

[15] M. H. Pinson and S. Wolf, “A new standardized method for objectively
measuring video quality,” IEEE Trans. on Broadcasting, vol. 50, no. 3,
pp. 312 – 322, 2004.

[16] Z. Wang and E. P. Simoncelli, “Reduced-reference image quality assess-
ment using a wavelet-domain natural image statistic model,” in Proc. of
the SPIE Human Vision and Electronic Imaging X, Jan. 2005.

[17] Q. Li and Z. Wang, “Reduced-reference image quality assessment using
divisive normalization-based image representation,” IEEE J. Selected
Topics in Signal Processing, vol. 3, no. 2, pp. 202 – 211, April 2009.
[18] X. Li, “Blind image quality assessment,” in Proc. of the IEEE ICIP,

Sept. 2002.

[19] Z. Wang, H. R. Sheihk, and A. C. Bovik, “No-reference perceptual
quality assessment of jpeg compressed images,” in Proc. of the IEEE
ICIP, Sept. 2002.

[20] H. R. Sheikh, A. C. Bovik, and L. Cormack, “No-reference quality
assessment using natural scene statistics: Jpeg2000,” IEEE Trans. on
Image Processing, vol. 14, no. 11, pp. 1918 – 1927, 2005.

[21] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality
assessment in the spatial domain,” IEEE Trans. on Image Processing,
vol. 21, no. 12, pp. 4695–4708, 2012.

[22] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely
blind” image quality analyzer,” IEEE Signal Process. Lett, vol. 20, no. 3,
pp. 3339–3352, 2013.

[23] M. Saad, A. C. Bovik, and C. Charrier, “Blind image quality assessment:
A natural scene statistics approach in the DCT domain,” IEEE Trans.
on Image Processing, vol. 21, no. 8, pp. 3339–3352, 2012.

[24] A. K. Moorthy and A. C. Bovik, “Blind image quality assessment: From
scene statistics to perceptual quality,” IEEE Trans. on Image Processing,
vol. 20, no. 12, pp. 3350–3364, 2011.

[25] Y.-F. Ou, Z. Ma, T. Liu, and Y. Wang, “Perceptual quality assessment
of video considering both frame rate and quantization artifacts,” IEEE
Trans. Circuits and Systems for Video Technology, vol. 21, no. 3, pp.
286–298, June 2011.

[26] H. Hu, Z. Ma, and Y. Wang, “Optimization of spatial, temporal and
amplitude resolution for rate-constrained video coding and scalable
video adaptation,” in Proc. of the IEEE ICIP, Oct. 2012.

[27] C. W. Tyler, “Analysis of human receptor density,” in Basic and clinical

applications of vision science. Springer, 1997, pp. 63–71.

[28] HTC Vive. [Online]. Available: http://www.vive.com/us/
[29] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, “Recognizing scene
viewpoint using panoramic place representation,” in Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on.
IEEE,
2012, pp. 2695–2702.

[30] H. Yu and S. Winkler, “Image complexity and spatial information,” in
Quality of Multimedia Experience (QoMEX), 2013 Fifth International
Workshop on.

IEEE, 2013, pp. 12–17.

[31] Rec. ITU-R BT.500-11, “Methodology for the subjective assessment of

the quality of television pictures,” 2002.

[32] T. Wiegand, G.-J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview
of the H.264/AVC video coding standard ,” IEEE Trans. Circuits and
Systems for Video Technology, vol. 13, no. 7, pp. 560 – 576, July 2003.
[33] J.-R. Ohm, G.-J. Sullivan, H. Schwarz, T.-K. Tan, and T. Wiegand,
“Comparison of the Coding Efﬁciency of Video Coding Standards—
Including High Efﬁciency Video Coding (HEVC),” IEEE Transactions
on Circuits and Systems for Video Technology, vol. 22, no. 12, pp. 1669–
1684, Dec. 2012.

[34] [Online]. Available: https://en.wikipedia.org/wiki/HSL and HSV
[35] Wikipedia, “Gabor ﬁlter — wikipedia, the free encyclopedia,” 2017.
[Online]. Available: https://en.wikipedia.org/w/index.php?title=Gabor
ﬁlter&oldid=781673347

[36] Y.-F. Ou, Y. Xue, and Y. Wang, “Q-STAR: a perceptual video quality
model considering impact of spatial, temporal, and amplitude resolu-
tions,” IEEE Transactions on Image Processing, vol. 23, no. 26, pp.
2473 – 2486, 2014.

[37] D. J. Brady, M. E. Gehm, R. A. Stack, D. L. Marks, D. S. Kittle, D. R.
Golish, E. M. Vera, and S. D. Feller, “Multiscale gigapixel photography,”
nature, vol. 486, pp. 386–389, June 2012.

[38] D. J. Brady, W. Pang, H. Li, Z. Ma, T. Yue, and X. Cao, “Parallel

cameras,” Optica (invited), 2017.

[39] S. Lederer, C. Mueller, and C. Timmerer, “Dynamic adaptive streaming
over HTTP dataset,” in Proc. ACM Multimedia Systems Conf. (MM-
Sys’12), Feb. 2012, pp. 89–94.

