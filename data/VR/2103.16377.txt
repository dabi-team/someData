1
2
0
2

r
a

M
0
3

]

G
L
.
s
c
[

1
v
7
7
3
6
1
.
3
0
1
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2021

GREEDY-GQ WITH VARIANCE REDUCTION: FINITE-
TIME ANALYSIS AND IMPROVED COMPLEXITY

Shaocong Ma, Ziyi Chen & Yi Zhou
Department of ECE
University of Utah
Salt Lake City, UT 84112
{s.ma,u1276972,yi.zhou}@utah.edu

Shaofeng Zou
Department of EE
University at Buffalo
Buffalo, NY 14260
szou3@buffalo.edu

ABSTRACT

Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal
control. Recently, the ﬁnite-time analysis of Greedy-GQ has been developed un-
der linear function approximation and Markovian sampling, and the algorithm is
shown to achieve an (cid:15)-stationary point with a sample complexity in the order of
O((cid:15)−3). Such a high sample complexity is due to the large variance induced by
the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ
(VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algo-
rithm applies the SVRG-based variance reduction scheme to reduce the stochas-
tic variance of the two time-scale updates. We study the ﬁnite-time convergence
of VR-Greedy-GQ under linear function approximation and Markovian sampling
and show that the algorithm achieves a much smaller bias and variance error than
the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an
improved sample complexity that is in the order of O((cid:15)−2). We further compare
the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL exper-
iments to corroborate our theoretical ﬁndings.

1

INTRODUCTION

In reinforcement learning (RL), an agent interacts with a stochastic environment following a certain
policy and receives some reward, and it aims to learn an optimal policy that yields the maximum ac-
cumulated reward Sutton & Barto (2018). In particular, many RL algorithms have been developed to
learn the optimal control policy, and they have been widely applied to various practical applications
such as ﬁnance, robotics, computer games and recommendation systems Mnih et al. (2015; 2016);
Silver et al. (2016); Kober et al. (2013).

Conventional RL algorithms such as Q-learning Watkins & Dayan (1992) and SARSA Rummery &
Niranjan (1994) have been well studied and their convergence is guaranteed in the tabular setting.
However, it is known that these algorithms may diverge in the popular off-policy setting under lin-
ear function approximation Baird (1995); Gordon (1996). To address this issue, the two time-scale
Greedy-GQ algorithm was developed in Maei et al. (2010) for learning the optimal policy. This
algorithm extends the efﬁcient gradient temporal difference (GTD) algorithms for policy evaluation
Sutton et al. (2009b) to policy optimization. In particular, the asymptotic convergence of Greedy-
GQ to a stationary point has been established in Maei et al. (2010). More recently, Wang & Zou
(2020) studied the ﬁnite-time convergence of Greedy-GQ under linear function approximation and
Markovian sampling, and it is shown that the algorithm achieves an (cid:15)-stationary point of the objec-
tive function with a sample complexity in the order of O((cid:15)−3). Such an undesirable high sample
complexity is caused by the large variance induced by the Markovian samples queried from the
dynamic environment. Therefore, we want to ask the following question.

• Q1: Can we develop a variance reduction scheme for the two time-scale Greedy-GQ algorithm?

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2021

In fact, in the existing literature, many recent work proposed to apply the variance reduction tech-
niques developed in the stochastic optimization literature to reduce the variance of various TD learn-
ing algorithms for policy evaluation, e.g., Du et al. (2017); Peng et al. (2019); Korda & La (2015);
Xu et al. (2020). Some other work applied variance reduction techniques to Q-learning algorithms,
e.g., Wainwright (2019); Jia et al. (2020). Hence, it is much desired to develop a variance-reduced
Greedy-GQ algorithm for optimal control. In particular, as many of the existing variance-reduced
RL algorithms have been shown to achieve an improved sample complexity under variance reduc-
tion, it is natural to ask the following fundamental question.

• Q2: Can variance-reduced Greedy-GQ achieve an improved sample complexity under Markovian

sampling?

In this paper, we provide afﬁrmative answers to these fundamental questions. Speciﬁcally, we
develop a two time-scale variance reduction scheme for the Greedy-GQ algorithm by leveraging
the SVRG scheme Johnson & Zhang (2013). Moreover, under linear function approximation and
Markovian sampling, we prove that the proposed variance-reduced Greedy-GQ algorithm achieves
an (cid:15)-stationary point with an improved sample complexity O((cid:15)−2). We summarize our technical
contributions as follows.

1.1 OUR CONTRIBUTIONS

We develop a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for optimal control in re-
inforcement learning. Speciﬁcally, the algorithm leverages the SVRG variance reduction scheme
Johnson & Zhang (2013) to construct variance-reduced stochastic updates for updating the parame-
ters in both time-scales.

We study the ﬁnite-time convergence of VR-Greedy-GQ under linear function approximation and
Markovian sampling in the off-policy setting. Speciﬁcally, we show that VR-Greedy-GQ achieves
an (cid:15)-stationary point of the objective function J (i.e., (cid:107)∇J(θ)(cid:107)2 ≤ (cid:15)) with a sample complexity
in the order of O((cid:15)−2). Such a complexity result improves that of the original Greedy-GQ by a
signiﬁcant factor of O((cid:15)−1) Wang & Zou (2020). In particular, our analysis shows that the bias
error caused by the Markovian sampling and the variance error of the stochastic updates are in the
order of O(M −1), O(ηθM −1), respectively, where ηθ is the learning rate and M corresponds to the
batch size of the SVRG reference batch update. This shows that the proposed variance reduction
scheme can signiﬁcantly reduce the bias and variance errors of the original Greedy-GQ update (by
a factor of M ) and lead to an improved overall sample complexity.

The analysis logic of VR-Greedy-GQ partly follows that of the conventional SVRG, but requires
substantial new technical developments. Speciﬁcally, we must address the following challenges.
First, VR-Greedy-GQ involves two time-scale variance-reduced updates that are correlated with
each other. Such an extension of the SVRG scheme to the two time-scale updates is novel and re-
quires new technical developments. Speciﬁcally, we need to develop tight variance bounds for the
two time-scale updates under Markovian sampling. Second, unlike the convex objective functions
of the conventional GTD type of algorithms, the objective function of VR-Greedy-GQ is generally
non-convex due to the non-stationary target policy. Hence, we need to develop new techniques to
characterize the per-iteration optimization progress towards a stationary point under nonconvexity.
In particular, to analyze the two time-scale variance reduction updates of the algorithm, we intro-
duce a ‘ﬁne-tuned’ Lyapunov function of the form Rm
t − (cid:101)θ(m)(cid:107)2, where the
parameter ct is ﬁne-tuned to cancel other additional quadratic terms (cid:107)θ(m)
t − (cid:101)θ(m)(cid:107)2 that are implic-
itly involved in the tracking error terms. The design of this special Lyapunov function is critical to
establish the formal convergence of the algorithm. With these technical developments, we are able
to establish an improved ﬁnite-time convergence rate and sample complexity for VR-Greedy-GQ.

t = J(θ(m)

) + ct(cid:107)θ(m)

t

1.2 RELATED WORK

Q-learning and SARSA with function approximation. The asymptotic convergence of Q-learning
and SARSA under linear function approximation were established in Melo et al. (2008); Perkins &
Precup (2003), and their ﬁnite-time analysis were developed in Zou et al. (2019); Chen et al. (2019).
However, these algorithms may diverge in off-policy training Baird (1995). Also, recent works
focused on the Markovian setting. Various analysis techniques have been developed to analyze

2

Published as a conference paper at ICLR 2021

the ﬁnite-time convergence of TD/Q-learning under Markovian samples. Speciﬁcally, Wang et al.
(2020) developed a multi-step Lyapunov analysis for addressing the biasedness of the stochastic
approximation in Q-learning. Srikant & Ying (2019) developed a drift analysis to the linear stochas-
tic approximation problem. Besides the linear function approximation, the ﬁnite-time analysis of
Q-learning under neural network function approximation is developed in Xu & Gu (2019).

GTD algorithms. The GTD2 and TDC algorithms were developed for off-policy TD learning.
Their asymptotic convergence was proved in Sutton et al. (2009a;b); Yu (2017), and their ﬁnite-time
analysis were developed recently in Dalal et al. (2018); Wang et al. (2017); Liu et al. (2015); Gupta
et al. (2019); Xu et al. (2019). The Greedy-GQ algorithm is an extension of these algorithms to
optimal control and involves nonlinear updates.

RL with variance reduction: Variance reduction techniques have been applied to various RL al-
gorithms. In TD learning, Du et al. (2017) reformulate the MSPBE problem as a convex-concave
saddle-point optimization problem and applied SVRG Johnson & Zhang (2013) and SAGA Defazio
et al. (2014) to primal-dual batch gradient algorithm. In Korda & La (2015), the variance-reduced
TD algorithm was introduced for solving the MSPBE problem, and later Xu et al. (2020) provided
a correct non-asymptotic analysis for this algorithm over Markovian samples. Recently, some other
works applied the SVRG , SARAH Nguyen et al. (2017) and SPIDER Fang et al. (2018) variance
reduction techniques to develop variance-reduced Q-learning algorithms, e.g., Wainwright (2019);
Jia et al. (2020). In these works, TD or TDC algorithms are in the form of linear stochastic approx-
imation, and Q-learning has only a single time-scale update. As a comparison, our VR-Greedy-GQ
takes nonlinear two time-scale updates to optimization a nonconvex MSPBE.

2 PRELIMINARIES: POLICY OPTIMIZATION AND GREEDY-GQ

In this section, we review some preliminaries of reinforcement learning and recap the Greedy-GQ
algorithm under linear function approximation.

2.1 POLICY OPTIMIZATION IN REINFORCEMENT LEARNING

In reinforcement learning, an agent takes actions to interact with the environment via a Markov
Decision Process (MDP). Speciﬁcally, an MDP is speciﬁed by the tuple (S, A, P, r, γ), where S and
A respectively correspond to the state and action spaces that include ﬁnite elements, r : S ×A×S →
[0, +∞) denotes a reward function and γ ∈ (0, 1) is the associated reward discount factor.

At any time t, assume that the agent is in the state st ∈ S and takes a certain action at ∈ A
following a stationary policy π, i.e., at ∼ π(·|st). Then, at the subsequent time t + 1, the current
state of the agent transfers to a new state st+1 according to the transition kernel P(·|st, at). At the
same time, the agent receives a reward rt = r(st, at, st+1) from the environment for this action-state
transition. To evaluate the quality of a given policy π, we often use the action-state value function
Qπ : S × A → R that accumulates the discounted rewards as follows:

Qπ(s, a) = Es(cid:48)∼P(·|s,a) [r(s, a, s(cid:48)) + γV π(s(cid:48))] ,

where V π(s) is the state value function deﬁned as V π(s) = E
. In particu-
lar, deﬁne the Bellman operator T π such that T πQ(s, a) = Es(cid:48),a(cid:48)[r(s, a, s(cid:48)) + γQ(s(cid:48), a(cid:48))] for any
Q(s, a), where a(cid:48) ∼ π(·|s(cid:48)). Then, Qπ(s, a) is a ﬁxed point of T π, i.e.,

(cid:104) (cid:80)∞

(cid:105)
t=0 γtrt|s0 = s

T πQπ(s, a) = Qπ(s, a),

(1)
The goal of policy optimization is to learn the optimal policy π∗ that maximizes the expected total
reward E[(cid:80)∞
t=0 γtrt|s0 = s] for any initial state s ∈ S, and this is equivalent to learn the optimal
value function Q∗(s, a) = supπ Qπ(s, a), ∀s, a. In particular, Q∗ is a ﬁxed point of the Bellman
operator T that is deﬁned as T Q(s, a) = Es(cid:48)∼P(·|s,a)[r(s, a, s(cid:48)) + γ maxb∈A Q(s(cid:48), b)].

∀s, a.

2.2 GREEDY-GQ WITH LINEAR FUNCTION APPROXIMATION

The Greedy-GQ algorithm is inspired by the ﬁxed point characterization in eq. (1), and in the tabular
setting it aims to minimize the Bellman error (cid:107)T πQπ − Qπ(cid:107)2
µs,a is induced by

µs,a . Here, (cid:107) · (cid:107)2

3

Published as a conference paper at ICLR 2021

the state-action stationary distribution µs,a (induced by the behavior policy πb), and is deﬁned as
(cid:107)Q(cid:107)2

= E(s,a)∼µs,a [Q(s, a)2].

µs,a

In practice, the state and action spaces may include a large number of elements that makes tabular
approach infeasible. To address this issue, function approximation technique is widely applied. In
this paper, we consider approximating the state-action value function Q(s, a) by a linear function.
Speciﬁcally, consider a set of basis functions {φ(i) : S × A → R, i = 1, 2, . . . , d}, each of which
maps a given state-action pair to a certain value. Deﬁne φs,a = [φ(1)(s, a); ...; φ(d)(s, a)] as the
feature vector for (s, a). Then, under linear function approximation, the value function Q(s, a) is
s,aθ, where θ ∈ Rd denotes the parameter of the linear approxi-
approximated by Qθ(s, a) = φ(cid:62)
mation. Consequently, Greedy-GQ aims to ﬁnd the optimal θ∗ that minimizes the following mean
squared projected Bellman error (MSPBE).

(MSPBE): J(θ) :=

(cid:107)ΠT πθ Qθ − Qθ(cid:107)2

1
2
where µs,a is the stationary distribution induced by the behavior policy πb, Π is a projection operator
that maps an action-value function Q to the space Q spanned by the feature vectors, i.e., ΠQ =
arg minU ∈Q (cid:107)U −Q(cid:107)µs,a . Moreover, the policy πθ is parameterized by θ. In this paper, we consider
the class of Lipschitz and smooth policies (see Assumption 4.2).
Next, we introduce the Greedy-GQ algorithm. Deﬁne V s(cid:48)(θ) = (cid:80)
δs,a,s(cid:48)(θ) = r(s, a, s(cid:48)) + γV s(cid:48)(θ) − φ(cid:62)
the objective function J(θ) in eq. (2) is expressed as

s(cid:48),a(cid:48)θ,
s,aθ and denote (cid:98)φs(θ) = ∇V s(θ). Then, the gradient of

a(cid:48)∈A πθ(a(cid:48)|s(cid:48))φ(cid:62)

(2)

µs,a

,

∇J(θ) = −E[δs,a,s(cid:48)(θ)φs,a] + γE[ (cid:98)φs(cid:48)(θ)φ(cid:62)

s,a]ω∗(θ),

s,a]−1E[δs,a,s(cid:48)(θ)φs,a]. To address the double-sampling issue when esti-
where ω∗(θ) = E[φs,aφ(cid:62)
mating the product of expectations involved in E[ (cid:98)φs(cid:48)(θ)φ(cid:62)
s,a]ω∗(θ), Sutton et al. (2009a) applies a
weight doubling trick and constructs the following two time-scale update rule for the Greedy-GQ
algorithm: for every t = 0, 1, 2, ..., sample (st, at, rt, st+1) using the behavior policy πb and do

(Greedy-GQ):






θt+1 = θt − ηθ

(cid:0) − δt+1(θt)φt + γ(ω(cid:62)
t ωt − δt+1(θt)(cid:1)φt,
(cid:0)φ(cid:62)
ωt+1 = ωt − ηω
πθt+1 = P(φ(cid:62)θt+1).

t φt) (cid:98)φt+1(θt)(cid:1),

(3)

where ηθ, ηω > 0 are the learning rates and we denote δt+1(θ) := δst,at,st+1(θ), φt := φst,at,
(cid:98)φt+1(θt) := (cid:98)φst+1(θt) for simplicity. To elaborate, the ﬁrst two steps correspond to the two time-
scale updates for updating the value function Qθ, whereas the last step is a policy improvement
operation that exploits the updated value function to improve the target policy, e.g., greedy, (cid:15)-greedy,
softmax and mellowmax Asadi & Littman (2017).

The above Greedy-GQ algorithm uses a single Markovian sample to perform the two time-scale
updates in each iteration. Such a stochastic Markovian sampling often induces a large variance that
signiﬁcantly slows down the overall convergence. This motivates us to develop variance reduction
schemes for the two time-scale Greedy-GQ in the next section.

3 GREEDY-GQ WITH VARIANCE REDUCTION

In this section, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm under
Markovian sampling by leveraging the SVRG variance reduction scheme Johnson & Zhang (2013).
To simplify notations, we deﬁne the stochastic updates regarding a sample xt = (st, at, rt, st+1)
used in the Greedy-GQ as follows:

Gxt(θ, ω) := −δt+1(θ)φt + γ(ω(cid:62)φt) (cid:98)φt+1(θ),
Hxt(θ, ω) := (cid:0)φ(cid:62)

t ω − δt+1(θ)(cid:1)φt.

Next, consider a single MDP trajectory {xt}t≥0 obtained by the behavior policy πb.
In partic-
ular, we divide the entire trajectory into multiple batches of samples {Bm}m≥1 so that Bm =

4

Published as a conference paper at ICLR 2021

{x(m−1)M , ..., xmM −1}, and our proposed VR-Greedy-GQ uses one batch of samples in every
epoch. To elaborate, in the m-th epoch, we ﬁrst initialize this epoch with a pair of reference points
θ(m)
0 = (cid:101)θ(m), ω(m)
of
the previous epoch, respectively. Then, we compute a pair of reference batch updates using the
reference points and the batch of samples as follows

0 = (cid:101)ω(m), where (cid:101)θ(m), (cid:101)ω(m) are set to be the output points θ(m−1)

, ω(m−1)
M

M

(cid:101)G(m) =

1
M

mM −1
(cid:88)

k=(m−1)M

Gxk ((cid:101)θ(m), (cid:101)ω(m)),

(cid:101)H (m) =

1
M

mM −1
(cid:88)

k=(m−1)M

Hxk ((cid:101)θ(m), (cid:101)ω(m)).

(4)

, ω(m)
t

In the t-th iteration of the m-th epoch, we ﬁrst query a random sample xξm
from the batch
Bm uniformly with replacement (i.e., sample ξm
from {(m − 1)M, ..., mM − 1} uniformly).
t
Then, we use this sample to compute the stochastic updates Gxξm
at both of the points
(θ(m)
), ((cid:101)θ(m), (cid:101)ω(m)). After that, we use these stochastic updates and the reference batch
t
updates to construct the variance-reduced updates in Algorithm 1 via the SVRG scheme, where for
simplicity we denote the stochastic updates Gxξm
. In particular,
we project the two time-scale updates onto the Euclidean ball with radius R to stabilize the algo-
rithm updates, and we assume that R is large enough to include at least one stationary point of J.
Lastly, we further update the policy via the policy improvement operation P.

respectively as G(m)

, H (m)
t

, Hxξm

, Hxξm

t

t

t

t

t

t

Algorithm 1: Variance-Reduced Greedy-GQ
Input: learning rates ηθ, ηω, batch size M .
Initialize: (cid:101)θ(1) = θ0, (cid:101)ω(1) = ω0, π
for m = 1, 2, . . . do
θ(m)
0 = (cid:101)θ(m), ω(m)
for t = 0, 1, . . . , M − 1 do

(cid:101)θ(1) ← P(φ(cid:62) (cid:101)θ(1)).

(cid:104)

Query a sample from Bm with replacement.
θ(m)
t+1 = ΠR
ω(m)
t+1 = ΠR
t
Policy improvement: πθ(m)

, ω(m)
t
, ω(m)
t
← P(φ(cid:62)θ(m)

θ(m)
t − ηθ
(cid:104)
ω(m)
t − ηω

(cid:0)G(m)
(cid:0)H (m)

(θ(m)
t
(θ(m)
t

t+1).

t

t+1

0 = (cid:101)ω(m). Compute (cid:101)G(m), (cid:101)H (m) according to eq. (4).

) − G(m)
t
) − H (m)

((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)G(m)(cid:1)(cid:105)
.
(˜θ(m), ˜ω(m)) + (cid:101)H (m)(cid:1)(cid:105)

t

.

end
Set (cid:101)θ(m+1) = θ(m)

M , (cid:101)ω(m+1) = ω(m)
M .

end
Output: parameter θ chosen among {θ(m)

}t,m uniformly at random.

t

The above VR-Greedy-GQ algorithm has several advantages and uniqueness. First, it takes incre-
mental updates that use a single Markovian sample per-iteration. This makes the algorithm sample
efﬁcient. Second, VR-Greedy-GQ applies variance reduction to both of the two time-scale updates.
As we show later in the analysis, such a two time-scale variance reduction scheme signiﬁcantly
reduces the variance error of both of the stochastic updates.

We want to further clarify the incrementalism and online property of VR-Greedy-GQ. Our VR-
Greedy-GQ is based on the online-SVRG and can be viewed as an incremental algorithm with regard
to the batches of samples used in the outer-loops, i.e., in every outer-loop the algorithm samples a
new batch of samples and use them to perform variance reduction in the corresponding inner-loops.
Therefore, VR-Greedy-GQ can be viewed as an online batch-incremental algorithm. In general,
there is a trade-off between incrementalism and variance reduction for SVRG-type algorithms: a
larger batch size in the outer-loops enhances the effect of variance reduction, while a smaller batch
size makes the algorithm more incremental.

4 FINITE-TIME ANALYSIS OF VR-GREEDY-GQ

In this section, we analyze the ﬁnite-time convergence rate of VR-Greedy-GQ. We adopt the follow-
ing standard technical assumptions from Wang & Zou (2020); Xu et al. (2020).

5

Published as a conference paper at ICLR 2021

Assumption 4.1 (Feature boundedness). The feature vectors are uniformly bounded, i.e., (cid:107)φs,a(cid:107) ≤
1 for all (s, a) ∈ S × A.
Assumption 4.2 (Policy smoothness). The mapping θ (cid:55)→ πθ is k1-Lipschitz and k2-smooth.

We note that the above class of smooth policies covers a variety of practical policies, including
softmax and mellowmax policies Asadi & Littman (2017); Wang & Zou (2020).
Assumption 4.3 (Problem solvability). The matrix C := E[φs,aφ(cid:62)
Assumption 4.4 (Geometric uniform ergodicity). There exists Λ > 0 and ρ ∈ (0, 1) such that

s,a] is non-singular.

dTV

(cid:0)P(st|s0 = s), µ(cid:1) ≤ Λρt,

sup
s∈S

for any t > 0, where dTV is the total-variation distance.

Based on the above assumptions, we obtain the following ﬁnite-time convergence rate result.
Theorem 4.5 (Finite-time convergence). Let Assumptions 4.1– 4.4 hold and consider the VR-
Greedy-GQ algorithm. Choose learning rates ηθ, ηω and the batch size M that satisfy the conditions
speciﬁed in eqs. (15) to (19). Then, after T epochs, the output of the algorithm satisﬁes

E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2 ≤ O

(cid:16)

1
ηθT M

+

1
T

(cid:0)ηω +

η2
θ
η2
ω

(cid:1) + (cid:0)ηω +

η2
θ
η2
ω

(cid:1)2

+

(cid:17)

,

1
M

where ξ, ζ are random indexes that are sampled from {0, ..., M − 1} and {1, ..., T } uniformly at
random, respectively.

ω + η2

Theorem 4.5 shows that VR-Greedy-GQ asymptotically converges to a neighborhood of a stationary
point at a sublinear rate. In particular, the size of the neighborhood is in the order of O(M −1 +
θ η−4
η4
ω), which can be driven arbitrarily close to zero by choosing a large batch size and
sufﬁciently small learning rates that satisfy the two time-scale condition ηθ/ηω → 0. Moreover, the
convergence error terms implicitly include a bias error O( 1
M ) caused by the Markovian sampling
and a variance error O( ηθ
M ) caused by the stochastic updates, both of which are substantially reduced
by the large batch size M . This shows that the SVRG scheme can effectively reduce the bias and
variance error of the two time-scale stochastic updates.

By further optimizing the choice of hyper-parameters, we obtain the following characterization of
sample complexity of VR-Greedy-GQ.
Corollary 4.6 (Sample complexity). Under the same conditions as those of Theorem 4.5, choose
learning rates so that ηθ = O( 1
), and set T ,M = O((cid:15)−1). Then, the required
sample complexity for achieving E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2 ≤ (cid:15) is in the order of T M = O((cid:15)−2).

M ), ηω = O(η2/3

θ

Such a complexity result is orderwise lower than the complexity O((cid:15)−3) of the original Greedy-GQ
Wang & Zou (2020). Therefore, this demonstrates the advantage of applying variance reduction to
the two time-scale updates of VR-Greedy-GQ. We also note that for online stochastic non-convex
optimization, the sample complexity of the SVRG algorithm is in the order of O((cid:15)−5/3) Li & Li
(2018), which is slightly better than our result. This is reasonable as the SVRG in stochastic opti-
mization is unbiased due to the i.i.d. sampling. In comparison, VR-Greedy-GQ works on a single
MDP trajectory that induces Markovian noise, and the two-timescale updates of the algorithm also
introduces additional tracking error.

5 SKETCH OF THE TECHNICAL PROOF

In this section, we provide an outline of the technical proof of the main Theorem 4.5 and highlight
the main technical contributions. The details of the proof can be found in the appendix.

We note that our proof logic partly follows the that of the conventional SVRG, i.e., exploiting the
objective function smoothness and introducing a Lyapunov function. However, our analysis requires
substantial new developments to address the challenges of off-policy control, two time-scale updates
of VR-Greedy-GQ and correlation of Markovian samples.

6

Published as a conference paper at ICLR 2021

The key step of the proof is to develop a proper Lyapunov function that drives the parameter to a
stationary point along the iterations. In addition, we also need to develop tight bounds for the bias
error, variance error and tracking error. We elaborate the key steps of the proof below.

Step 1: We ﬁrst deﬁne the following Lyapunov function with certain ct > 0 to be determined later.

Rm
t

:= J(θ(m)

t

) + ct(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2.

(5)

To explain the motivation, note that unlike the analysis of variance-reduced TD learning Xu et al.
t − (cid:101)θ(m)(cid:107)2 can be decomposed into (cid:107)θ(m)
(2020) where the term (cid:107)θ(m)
t − θ∗(cid:107)2 + (cid:107)(cid:101)θ(m) − θ∗(cid:107)2 to
get the desired upper bound, here we do not have θ∗ due to the non-convexity of J(θ). Hence, we
need to properly merge this term into the Lyapunov function Rm
t . By leveraging the smoothness of
J(θ) and the algorithm update rule, we obtain the following bound for the Lyapunov function Rm
t
(see eq. (10) in the appendix for the details).
t+1] ≤ E[J(θ(m)

)] + O(cid:0)E[(cid:107)θ(m)

E[Rm

t

+ O(cid:0)E[(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2] + E[(cid:107)ω(m)

t − (cid:101)θ(m)(cid:107)2] + E[(cid:107)∇J(θ(m)
t − ω∗(θ(m)

t

t

)](cid:107)2 + M −1(cid:1)
)(cid:107)2](cid:1).

(6)

In particular, the error term 1
stochastic updates, and the last two terms correspond to tracking errors.

M is due to the noise of Markovian sampling and the variance of the

)] + O(cid:0)E[(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2](cid:1) = Rm

t by choosing a proper ct of Rm

Step 2: To telescope the Lyapunov function over t based on eq. (6), one may want to deﬁne
J(θ(m)
t . However, note that
t
eq. (6) involves the last two tracking error terms, which also implicitly depend on E(cid:107)θ(m)
t − (cid:101)θ(m)(cid:107)2
as we show later in the Step 3. Therefore, we need to carefully deﬁne the ct of Rm
so that after
t
applying the tracking error bounds developed in the Step 3, the right hand side of eq. (6) can yield
t − (cid:101)θ(m)(cid:107)2. It turns out that we need to deﬁne ct via the
an Rm
recursion speciﬁed in eq. (11) in the appendix. We rigorously show that the sequence {ct}t is uni-
formly bounded by a small constant (cid:98)c = 1
8 . Then, plugging these bounds into eq. (6) and summing
over one epoch, we obtain the following bound (see eq. (13) in the appendix for the details).

t without involving the term E(cid:107)θ(m)

ηθ

M −1
(cid:88)

t=0

E[(cid:107)∇J(θ(m)

t

)(cid:107)2] ≤ E[Rm

0 ] − E[Rm

M ] + O(cid:0)ηθ + η2

θ M E[(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2](cid:1)

(cid:16)

ηθ

+ O

M −1
(cid:88)

t=0

E(cid:107)ω(m)

t − ω∗(θ(m)

t

(cid:16)
)(cid:107)2 − ηθ(cid:98)c

ηω +

η2
θ
η2
ω

(cid:17) M −1
(cid:88)

t=0

E[(cid:107)θ(m)

(cid:17)
t − (cid:101)θ(m)(cid:107)2]

.

Step 3: We derive bounds for the tracking error terms (cid:80)M −1
t=0
ω∗((cid:101)θ(m))(cid:107)2 in the above bound in Lemma D.7 and Lemma D.8.

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 and E(cid:107)(cid:101)ω(m) −

Step 4: Lastly, by substituting the tracking error bounds obtained in Step 3 into the bound obtained
in Step 2, the resulting bound does not involve the term (cid:80)M −1
t − (cid:101)θ(m)(cid:107)2. Then, summing
t=0
this bound over the epochs m = 1, ..., T , we obtain the desired ﬁnite-time convergence rate result.

E(cid:107)θ(m)

6 EXPERIMENTS

In this section, we conduct two reinforcement learning experiments, namely, Garnet problem
Archibald et al. (1995) and Frozen Lake game Brockman et al. (2016), to test the performance of
VR-Greedy-GQ in the off-policy setting, and compare it with Greedy-GQ in the Markovian setting.

6.1 GARNET PROBLEM

For the Garnet problem, we refer to Appendix F for the details of the problem setup. In Figure 1
(left), we plot the minimum gradient norm v.s.
the number of pseudo stochastic gradient com-
putations for both algorithms using 40 Garnet MDP trajectories, and each trajectory contains 10k
samples. The upper and lower envelopes of the curves correspond to the 95% and 5% percentiles
of the 40 curves, respectively.
It can be seen that VR-Greedy-GQ outperforms Greedy-GQ and
achieves a signiﬁcantly smaller asymptotic gradient norm.

7

Published as a conference paper at ICLR 2021

Figure 1: Comparison of Greedy-GQ and VR-Greedy-GQ in solving the Garnet problem.

In Figure 1 (middle), we track the estimated variance of the stochastic update for both algorithms
along the iterations. Speciﬁcally, we query 500 Monte Carlo samples per iteration to estimate the
pseudo gradient variance E(cid:107)G(m)
)(cid:107)2. It can be seen from the ﬁgure that
the stochastic updates of VR-Greedy-GQ induce a much smaller variance than Greedy-GQ. This
demonstrates the effectiveness of the two time-scale variance reduction scheme of VR-Greedy-GQ.

) − ∇J(θ(m)

, ω(m)
t

(θ(m)
t

t

t

We further study the asymptotic convergence error of VR-Greedy-GQ under different batch sizes M .
We use the default learning rate setting that is mentioned previously and run 100k iterations for one
Garnet trajectories. We use the mean of the convergence error of the last 10k iterations as an estimate
of the asymptotic convergence error (the training curves are already saturated and ﬂattened). Figure
1 (right) shows the asymptotic convergence error of VR-Greedy-GQ under different batch sizes M .
It can be seen that VR-Greedy-GQ achieves a smaller asymptotic convergence error with a larger
batch size, which matches our theoretical result.

Figure 2: Comparison of MSPBE and reward obtained by Greedy-GQ, VR-Greedy-GQ and PG.

In Figure 2 (Left), we plot the MSPBE J(θ) v.s. number of gradient computations for both Greedy-
GQ and VR-Greedy-GQ, where one can see that VR-Greedy-GQ achieves a much smaller MSPBE
than Greedy-GQ. In Figure 2 (Middle), we plot the estimated expected maximum reward (see Ap-
pendix F for details) v.s. number of gradient computations for Greedy-GQ, VR-Greedy-GQ and
actor-critic, where for actor-critic we set learning rate ηθ = 0.02 for the actor update and ηω = 0.01
for the critic update. One can see that VR-Greedy-GQ achieves a higher reward than the other two
algorithms, demonstrating the high quality of its learned policy. In addition, we also plot the es-
timated expected maximum reward v.s. number of iterations for Greedy-GQ, VR-Greedy-GQ and
policy gradient in Figure 2 (Right). For the policy gradient, we apply the standard off-policy policy
gradient algorithm. For each update, we sample 30 independent trajectories with a ﬁxed length 60
to estimate the expected discounted return. The learning rate of policy gradient is set as ηθ. We note
that each iteration of policy gradient consumes 1800 samples and hence it is very sample inefﬁcient.
Hence we set the x-axis to be number of iterations for a clear presentation (otherwise it becomes
a ﬂat curve). One can see that VR-Greedy-GQ achieves a much higher expected reward than both
Greedy-GQ and policy gradient.

6.2 FROZEN LAKE GAME

We further test these algorithms in solving the more complex frozen lake game. we refer to Ap-
pendix F for the details of the problem setup. Figure 3 shows the comparison between VR-Greedy-

8

Published as a conference paper at ICLR 2021

GQ and Greedy-GQ, and one can make consistent observations with those made in the Garnet ex-
periment. Speciﬁcally, Figure 3 (left) shows that VR-Greedy-GQ achieves a much more stationary
policy than Greedy-GQ. Figure 3 (middle) shows that the stochastic updates of VR-Greedy-GQ
induce a much smaller variance than those of Greedy-GQ. Moreover, Figure 3 (right) veriﬁes our
theoretical result that VR-Greedy achieves a smaller asymptotic convergence error with a larger
batch size.

Figure 3: Comparison of Greedy-GQ and VR-Greedy-GQ in solving the Frozen Lake problem.

We further plot the MSPBE v.s. number of gradient computations for both Greedy-GQ and VR-
Greedy-GQ in Figure 4 (Left), where one can see that VR-Greedy-GQ outperforms Greedy-GQ.
In Figure 2 (Middle), we plot the estimated expected maximum reward v.s. number of gradient
computations for Greedy-GQ, VR-Greedy-GQ and actor-critic, where for actor-critic we set learning
rate ηθ = 0.2 for the actor update and ηω = 0.1 for the critic update. It can be seen that VR-
Greedy-GQ achieves a higher reward than the other two algorithms. In Figure 2 (Right), we plot the
estimated expected maximum reward v.s. number of iterations for Greedy-GQ, VR-Greedy-GQ and
policy gradient. For policy gradient, we use the same parameter settings as before. One can see that
VR-Greedy-GQ achieves a much higher expected reward than both Greedy-GQ and policy gradient.

Figure 4: Comparison of MSPBE and reward obtained by Greedy-GQ, VR-Greedy-GQ and PG.

7 CONCLUSION

In this paper, we develop a variance-reduced two time-scale Greedy-GQ algorithm for optimal con-
trol by leveraging the SVRG variance reduction scheme. Under linear function approximation and
Markovian sampling, we establish the sublinear ﬁnite-time convergence rate of the algorithm to a
stationary point and prove an improved sample complexity bound over that of the original Greedy-
GQ. The RL experiments well demonstrated the effectiveness of the proposed two time-scale vari-
ance reduction scheme. Our algorithm design may inspire new developments of variance reduction
for two time-scale RL algorithms. In the future, we will explore Greedy-GQ with other nonconvex
variance reduction schemes to possibly further improve the sample complexity.

ACKNOWLEDGEMENT

The work of S. Zou was supported by the National Science Foundation under Grant CCF-2007783.

9

Published as a conference paper at ICLR 2021

REFERENCES

TW Archibald, KIM McKinnon, and LC Thomas. On the generation of Markov decision processes.

Journal of the Operational Research Society, 46(3):354–361, 1995.

Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 243–
252. JMLR. org, 2017.

Leemon Baird. Residual algorithms: Reinforcement learning with function approximation.

In

Machine Learning Proceedings 1995, pp. 30–37. Elsevier, 1995.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym, 2016.

Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
mance of Q-learning with linear function approximation: Stability and ﬁnite-time analysis. arXiv
preprint arXiv:1905.11425, 2019.

Gal Dalal, Bal´azs Sz¨or´enyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. Proceedings of
Machine Learning Research, 75:1–35, 2018.

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 1646–1654, 2014.

Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In Proc. International Conference on Machine Learning (ICML),
pp. 1049–1058, 2017.

Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
In Proc. Advances in Neural

optimization via stochastic path-integrated differential estimator.
Information Processing Systems (NeurIPS), pp. 689–699, 2018.

Geoffrey J. Gordon. Chattering in SARSA (λ)-a CMU learning lab internal report. Citeseer, 1996.

Harsh Gupta, R Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate
In Proc. Advances in Neural Information

selection for two time-scale reinforcement learning.
Processing Systems (NeurIPS), pp. 4706–4715, 2019.

Haonan Jia, Xiao Zhang, Jun Xu, Wei Zeng, Hao Jiang, Xiaohui Yan, and Ji-Rong Wen. Variance
reduction for deep q-learning using stochastic recursive gradient. arXiv:2007.12817, 07 2020.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems, pp. 315–323, 2013.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The

International Journal of Robotics Research, 32(11):1238–1274, 2013.

Nathaniel Korda and Prashanth La. On td (0) with function approximation: Concentration bounds
and a centered variant with exponential convergence. In International Conference on Machine
Learning, pp. 626–634, 2015.

Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex

optimization. In Advances in neural information processing systems, pp. 5564–5574, 2018.

Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In Proc. International Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pp. 504–513. Citeseer, 2015.

Hamid Reza Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy
In Proc. International Conference on Machine

learning control with function approximation.
Learning (ICML), 2010.

10

Published as a conference paper at ICLR 2021

Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with
In Proc. International Conference on Machine Learning (ICML), pp.

function approximation.
664–671. ACM, 2008.

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, and G. Ostrovski. Human-level control through deep reinforcement learn-
ing. Nature, 518:529–533, 2015.

V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proc. International Conference on
Machine Learning (ICML), pp. 1928–1937, 2016.

Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak´aˇc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 2613–2621. JMLR. org, 2017.

Zilun Peng, Ahmed Touati, Pascal Vincent, and Doina Precup. SVRG for policy evaluation with

fewer gradient evaluations. arXiv:1906.03704, 2019.

Theodore J Perkins and Doina Precup. A convergent form of approximate policy iteration. In Proc.

Advances in Neural Information Processing Systems (NeurIPS), pp. 1627–1634, 2003.

G. A. Rummery and M. Niranjan. Online Q-learning using connectionist systems. Technical Report,

Cambridge University Engineering Department, September 1994.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of Go with deep neural networks and tree search. nature, 529(7587):484, 2016.

Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation

andtd learning. In Conference on Learning Theory, pp. 2803–2830. PMLR, 2019.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction, Second Edition.

The MIT Press, Cambridge, Massachusetts, 2018.

Richard S Sutton, Hamid R Maei, and Csaba Szepesv´ari. A convergent O(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in neural in-
formation processing systems, pp. 1609–1616, 2009a.

Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesv´ari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learn-
ing with linear function approximation. In Proc. International Conference on Machine Learning
(ICML), pp. 993–1000, 2009b.

Martin Wainwright. Variance-reduced q-learning is minimax optimal. arXiv:1906.04697, 06 2019.

Gang Wang, Bingcong Li, and Georgios B. Giannakis. A multistep lyapunov approach for ﬁnite-

time analysis of biased stochastic approximation. arXiv:1909.04299, 2020.

Yue Wang and Shaofeng Zou. Finite-sample analysis of greedy-gq with linear function approxima-
tion under markovian noise. In Proc. Machine Learning Research, volume 124, pp. 11–20, Aug
2020.

Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Finite sample analysis of the
In Proc. Advances in Neural Information

gtd policy evaluation algorithms in markov setting.
Processing Systems (NeurIPS), pp. 5504–5513, 2017.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A ﬁnite time analysis of two time-scale actor

critic methods. arXiv preprint arXiv:2005.01350, 2020.

Pan Xu and Quanquan Gu. A ﬁnite-time analysis of q-learning with neural network function ap-

proximation. arXiv preprint arXiv:1912.04511, 2019.

11

Published as a conference paper at ICLR 2021

Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale off-policy TD learning: Non-
In Proc. Advances in Neural Information Pro-

asymptotic analysis over Markovian samples.
cessing Systems (NeurIPS), pp. 10633–10643, 2019.

Tengyu Xu, Zhe Wang, Yi Zhou, and Yingbin Liang. Reanalysis of variance reduced temporal
difference learning. In Proc. International Conference on Learning Representations (ICLR), 2020.

Huizhen Yu. On convergence of some gradient-based temporal-differences algorithms for off-policy

learning. arXiv preprint arXiv:1712.09652, 2017.

Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. In Advances in Neural Information Processing Systems, pp. 8665–8675,
2019.

12

Published as a conference paper at ICLR 2021

Appendix

Table of Contents

A Filtration and List of Constants

B Proof of Theorem 4.5

C Proof of Corollary 4.6

D Technical Lemmas

E Other Supporting Lemmas

F Details of Experiments

13

13

20

20

27

29

A FILTRATION AND LIST OF CONSTANTS

Filtration We follow the deﬁnition of ﬁltration in VRTD (Appendix D, Xu et al. (2020)). Recall
that Bm denotes the set of Markovian samples used in the m-th epoch, and we also abuse the notation
here by letting x(m)
be the sample picked in the t-th iteration of the m-th epoch. Then, we deﬁne
the ﬁltration for Markovian samples as follows

t

F1,0 = σ(B0 ∪ σ(˜θ(0), ˜w(0))), F1,1 = σ(F1,0 ∪ σ(x(1)
F2,0 = σ(cid:0)B1 ∪ F1,M ∪ σ(˜θ(1), ˜w(1))(cid:1), F2,1 = σ(F2,0 ∪ σ(x(2)
...
Fm,0 = σ(cid:0)Bm−1 ∪ Fm−1,M ∪ σ(˜θ(m−1), ˜w(m−1))(cid:1), Fm,1 = σ(Fm,0 ∪ σ(x(m)

0 )), . . . , F1,M = σ(F1,M −1 ∪ σ(x(1)

M −1))

0 )), . . . , F2,M = σ(F2,M −1 ∪ σ(x(2)

M −1))

)), . . . ,

0

Fm,M = σ(Fm,M −1 ∪ σ(x(m)

M −1)).

Moreover, we deﬁne Et,m as the conditional expectation with respect to the σ-ﬁeld Ft,m.

List of Constants We summarize all the constants that are used in the proof as follows.

• G = rmax + (1 + γ)R + γ(|A|Rk1 + 1)R.
• H = (2 + γ)R + rmax.
• C1 = (1 + 2Λ ρ
• C2 = H 2(1 + Λ ρ

1−ρ )(G + C∇J )2.

1−ρ ).

(1 + ρΛ

1−ρ ).

• C3 = 8R2
λC
• C4 = 2
λC

(R(2 + γ) + rmax)2(1 + ρΛ

1−ρ ).

B PROOF OF THEOREM 4.5

We ﬁrst deﬁne the following Lyapunov function

Rm
t

:= J(θ(m)

t

) + ct(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2,

13

Published as a conference paper at ICLR 2021

where ct > 0 is to be determined later. Our strategy is to characterize the per-iteration progress of
Rm
t and use Lemma E.10 to bound
the second term of Rm

t . In particular, we use Lemma E.9 to bound the ﬁrst term of Rm

t . Note that Lemma E.9 implies that

J(θ(m)

t+1) ≤ J(θ(m)

t

) − ηθ(cid:104)∇J(θ(m)

t

), g(m)

t − ∇J(θ(m)

t

)(cid:105) − ηθ(cid:107)∇J(θ(m)

t

)(cid:107)2 +

L
2

θ (cid:107)g(m)
η2

t

(cid:107)2.

Let ξ(m)
t

:= (cid:104)∇J(θ(m)

t

), g(m)

t − ∇J(θ(m)

t

)(cid:105). Then, we obtain that

J(θ(m)

t+1) ≤ J(θ(m)

t

) − ηθξ(m)

t − ηθ(cid:107)∇J(θ(m)

t

)(cid:107)2 +

L
2

θ (cid:107)g(m)
η2

t

(cid:107)2.

(7)

Substituting eq. (7) and eq. (30) into the deﬁnition of Rm

t+1, we obtain that

Rm

t+1 := J(θ(m)
t+1) + ct+1(cid:107)θ(m)
) − ηθξ(m)
≤ J(θ(m)
(cid:104)

t+1 − (cid:101)θ(m)(cid:107)2
t − ηθ(cid:107)∇J(θ(m)

t

t

)(cid:107)2 +

L
2

θ (cid:107)g(m)
η2

t

(cid:107)2

t − (cid:101)θ(m)(cid:107)2 − 2ηθζ (m)
t − (cid:101)θ(m)(cid:107)2(cid:3)(cid:105)

t

t

θ (cid:107)g(m)
η2

(cid:107)2 + (cid:107)θ(m)

+ ηθ

(cid:107)∇J(θ(m)

+ ct+1
(cid:2) 1
βt
) + ct+1(ηθβt + 1)(cid:107)θ(m)
= J(θ(m)
(cid:1)(cid:107)∇J(θ(m)

+ (cid:0) − ηθ + ct+1

)(cid:107)2 + βt(cid:107)θ(m)

t

t

t

ηθ
βt

t − (cid:101)θ(m)(cid:107)2
)(cid:107)2 + (cid:0) L
2

θ + ct+1η2
η2
θ

(cid:1)(cid:107)g(m)

t

(cid:107)2

t − 2ct+1ηθζ (m)
Next, we bound the two inner product terms ξ(m)

− ηθξ(m)

.

t

t

and ζ (m)
t

.

Bounding the term ξ(m)

t

:

Recall the variance-reduced stochastic update

ξ(m)
t

:= (cid:104)∇J(θ(m)

t

), g(m)

t − ∇J(θ(m)

t

)(cid:105).

t = G(m)
g(m)

t

(θ(m)
t

, ω(m)
t

) − G(m)

t

((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)G(m).

Then, the term ξ(m)

can be decomposed as

t
t = (cid:104)∇J(θ(m)
ξ(m)
= (cid:104)∇J(θ(m)

t

t

), g(m)
), G(m)
t
), G(m)
t
), −G(m)

t − ∇J(θ(m)
(θ(m)
t
(θ(m)
t
((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)G(m)(cid:105)

)(cid:105)
, ω∗(θ(m)
t
, ω(m)
t

)) − ∇J(θ(m)
t
(θ(m)
) − G(m)
t

t

t

t
+ (cid:104)∇J(θ(m)
+ (cid:104)∇J(θ(m)

t

t

)(cid:105)
, ω∗(θ(m)

t

))(cid:105)

In the last equality, the ﬁrst inner product term is the bias caused by Markovian samples, and by
Lemma D.3 we have that

E(cid:104)∇J(θ(m)
=E(cid:104)∇J(θ(m)

t

), G(m)
(θ(m)
t
t
), G(m)(θ(m)

t

t

, ω∗(θ(m)
, ω∗(θ(m)

t

)) − ∇J(θ(m)
)) − ∇J(θ(m)

t

)(cid:105)

)(cid:105)

t
E(cid:107)∇J(θ(m)

t

)(cid:107)2 + E(cid:107)G(m)(θ(m)

t

, ω∗(θ(m)

t

t
)) − ∇J(θ(m)

t

)(cid:105)(cid:107)2

=

≤

1
4
1
4

E(cid:107)∇J(θ(m)

t

)(cid:107)2 +

C1
M

.

The second inner product term is the bias caused by tracking error, and we further obtain that

(cid:104)∇J(θ(m)
1
4

t
(cid:107)∇J(θ(m)

t

), G(m)
t

(θ(m)
, ω(m)
t
t
)(cid:107)2 + L1(cid:107)ω(m)

≤

t − ω∗(θ(m)

t

)(cid:107)2.

) − G(m)

t

(θ(m)
t

, ω∗(θ(m)

t

))(cid:105)

14

Published as a conference paper at ICLR 2021

The third inner product term is unbiased. Combining all of these bounds, we ﬁnally obtain that

|Eξ(m)
t

| ≤

C1
M

+

1
2

(cid:107)∇J(θ(m)

t

)(cid:107)2 + L1(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2.

(8)

Bounding the term ζ (m)

t

:

ζ (m)
t

:= (cid:104)g(m)

t − ∇J(θ(m)

t

), θ(m)

t − (cid:101)θ(m)(cid:105).

Similar to the previous proof for bounding ξ(m)

t

, we can decompose ζ (m)

t

as

t = (cid:104)θ(m)
ζ (m)
= (cid:104)θ(m)

t − (cid:101)θ(m), g(m)
t − (cid:101)θ(m), G(m)

t

t

)(cid:105)
, ω∗(θ(m)
t
, ω(m)
t

t − ∇J(θ(m)
(θ(m)
t
(θ(m)
t
((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)G(m)(cid:105)

)) − ∇J(θ(m)
t
(θ(m)
) − G(m)
t

t

+ (cid:104)θ(m)
+ (cid:104)θ(m)

t − (cid:101)θ(m), G(m)
t − (cid:101)θ(m), −G(m)

t

t

)(cid:105)
, ω∗(θ(m)

t

))(cid:105)

In the last equality, the ﬁrst inner product term is the bias caused by Markovian samples. We obtain
that

t − (cid:101)θ(m), G(m)
t − (cid:101)θ(m)(cid:107)2 +

E(cid:107)θ(m)

t

, ω∗(θ(m)

)) − ∇J(θ(m)

)(cid:105)

t
(θ(m)
t

t
)) − ∇J(θ(m)
, ω∗(θ(m)

t

t

)(cid:105)(cid:107)2

E(cid:104)θ(m)
1
2
1
2

≤

≤

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

(θ(m)
t
1
E(cid:107)G(m)
t
2
1
2

C1
M

.

The second inner product term is the bias caused by tracking error. We obtain that

t − (cid:101)θ(m), G(m)
(cid:107)θ(m)
t − (cid:101)θ(m)(cid:107)2 +

(cid:104)θ(m)
1
2

(θ(m)
t
L1
2

t

, ω(m)
t
(cid:107)ω(m)

≤

t − ω∗(θ(m)

t

)(cid:107)2.

) − G(m)

t

(θ(m)
t

, ω∗(θ(m)

t

))(cid:105)

The third inner product term is unbiased. Combining all of these bounds, we ﬁnally obtain that

|Eζ (m)
t

| ≤

1
2

C1
M

+ (cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

L1
2

(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2.

(9)

Next, we continue to bound the Lyapunov function. Recall we have shown that

Rm

t+1 ≤ J(θ(m)

t

) + ct+1(ηθβt + 1)(cid:107)θ(m)
(cid:1)(cid:107)∇J(θ(m)

+ (cid:0) − ηθ + ct+1

t

t − (cid:101)θ(m)(cid:107)2
)(cid:107)2 + (cid:0) L
2

ηθ
βt

θ + ct+1η2
η2
θ

(cid:1)(cid:107)g(m)

t

(cid:107)2

− ηθξ(m)

t − 2ct+1ηθζ (m)

t

.

Taking expectation on both sides of the above inequality and applying eq. (8), eq. (9), and Lemma
D.1, we obtain that

E[Rm

t+1] ≤ E(cid:2)J(θ(m)

t

t − (cid:101)θ(m)(cid:107)2(cid:3)
) + ct+1(ηθβt + 1)(cid:107)θ(m)
)(cid:107)2

(cid:1)E(cid:107)∇J(θ(m)

t

+ (cid:0) − ηθ + ct+1

ηθ
βt
(cid:1)(cid:104)

+ (cid:0) L
2

θ + ct+1η2
η2
θ

6L1E(cid:107)ω(m)

t − (cid:101)θ(m)(cid:107)2 +

)(cid:107)2 + 9L1E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

t

t − ω∗(θ(m)
1
M

· 9C1 + 9E(cid:107)∇J(θ(m)

t

)(cid:107)2(cid:105)

(cid:107)∇J(θ(m)

t

)(cid:107)2 + L1(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2(cid:3)

+ ηθ

+ 9L2E(cid:107)θ(m)
(cid:2) C1
1
2
M
C1
(cid:2) 1
M
2

+ 2ct+1ηθ

+

+ (cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

L1
2

15

(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2(cid:3).

(10)

Published as a conference paper at ICLR 2021

We note that the tracking error term (cid:107)ω(m)
t − (cid:101)θ(m)(cid:107)2. Here
t
we use a trick to merge this dependence to the coefﬁcient ct+1. Speciﬁcally, we add and subtract the
same term in the above bound and obtain that

)(cid:107)2 has dependence on (cid:107)θ(m)

t − ω∗(θ(m)

E[Rm

(cid:104)
t+1] ≤ E

J(θ(m)
t

θ + ct+1η2
η2
θ

(cid:1)(cid:3)(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2(cid:105)

(cid:0) L
2

−

1
2

ηθ
βt

) + (cid:2)ct+1(ηθβt + 1 + 2ηθ) + 9L1
(cid:1)(cid:105)
+ 9(cid:0) L
2

θ + ct+1η2
η2
θ

ηθ + ct+1
ηθ + 2ct+1ηθ + 9(cid:0) L
2
(cid:1)E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
(cid:105)
(cid:1) + ηθL1 + ηθL1ct+1
E(cid:107)ω(m)

θ + ct+1η2
η2
θ

θ + ct+1η2
η2
θ

(cid:1)(cid:105) C1
M

θ + ct+1η2
η2
θ

6L1

t

E(cid:107)∇J(θ(m)

)(cid:107)2

(cid:104)

(cid:104)

+

+

(cid:104)

(cid:104)

(cid:104)

+

−

+

+ 9L1

(cid:0) L
2
(cid:0) L
2
(cid:0) L
2
(cid:0) L
2

6L1

6L1

θ + ct+1η2
η2
θ

(cid:1) + ηθL1 + ηθL1ct+1

θ + ct+1η2
η2
θ

(cid:1) + ηθL1 + ηθL1ct+1

(cid:105)

(cid:105)

·

·

t

t − ω∗(θ(m)
)(cid:107)2
(cid:104)
5ηω + (cid:0) 9
12L2
λC
5ηω + (cid:0) 9
λC

(cid:104)
12L2

4
λC
4
λC

+ 2L2
3

(cid:1)9L2

2

+ 2L2
3

(cid:1)9L2

2

(cid:105)

(cid:105)

η2
θ
η2
ω
η2
θ
η2
ω

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2.

Then, we deﬁne Rm
t
recursion.

:= J(θ(m)

t

) + ct(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 with ct being speciﬁed via the following

ct = ct+1(ηθβt + 1 + 2ηθ) + 9L1

(cid:0) L
2

θ + ct+1η2
η2
θ

(cid:1)

(cid:104)
6L1

+

(cid:0) L
2

θ + ct+1η2
η2
θ

(cid:1) + ηθL1 + ηθL1ct+1

(cid:105)

·

(cid:104)

4
λC

12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

(cid:105)
. (11)

η2
θ
η2
ω

Based on this deﬁnition, the previous inequality reduces to

E[Rm

t+1] ≤ E[Rm

E(cid:107)∇J(θ(m)

)(cid:107)2

t

(cid:104)

(cid:1)(cid:105)

−

ηθ
βt

+ 9(cid:0) L
2

θ + ct+1η2
η2
θ

1
ηθ + ct+1
t ] +
2
ηθ + 2ct+1ηθ + 9(cid:0) L
2
(cid:1)E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
(cid:105)
(cid:1) + ηθL1 + ηθL1ct+1
E(cid:107)ω(m)

θ + ct+1η2
η2
θ

θ + ct+1η2
η2
θ

(cid:1)(cid:105) C1
M

θ + ct+1η2
η2
θ

6L1

6L1

θ + ct+1η2
η2
θ

(cid:1) + ηθL1 + ηθL1ct+1

(cid:105)

·

+ 9L1

(cid:104)

+

(cid:104)

(cid:104)

+

−

(cid:0) L
2
(cid:0) L
2
(cid:0) L
2

t

t − ω∗(θ(m)
)(cid:107)2
(cid:104)
5ηω + (cid:0) 9
12L2
λC

4
λC

+ 2L2
3

(cid:1)9L2

2

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2.

(cid:105)

η2
θ
η2
ω
(12)

Assume that ct ≤ (cid:98)c for some universal constant (cid:98)c > 0 (we will formally prove it later). Then, we
sum the above inequality over one epoch and obtain that

(cid:104) 1
2

ηθ − (cid:98)c

ηθ
βt

− 9(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1)(cid:105) M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

≤ E[Rm

0 ] − E[Rm

M ] +

(cid:104)

ηθ + 2(cid:98)cηθ + 9(cid:0) L

θ + (cid:98)cη2
η2
2
(cid:105) M −1
(cid:88)
(cid:1) + ηθL1 + ηθL1(cid:98)c

θ

θ + (cid:98)cη2
η2

θ

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

(cid:1)(cid:105)

C1 + 9L1

(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1)M E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

t=0

θ + (cid:98)cη2
η2

θ

(cid:105)
(cid:1) + ηθL1 + ηθL1(cid:98)c

·

(cid:104)

4
λC

12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

η2
θ
η2
ω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2.

(13)

16

(cid:104)

(cid:104)

+

−

6L1

6L1

(cid:0) L
2

(cid:0) L
2

Published as a conference paper at ICLR 2021

By Lemma D.7, we have that

M −1
(cid:88)

t=0

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

≤

4
λC

(cid:104) 1
ηω

+ M

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:105)(cid:105)

+ 18L2

4ηω

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

(cid:104)(cid:0) 9
λC
(cid:1) η2
θ
η2
ω

+ 2L2
3

4
λC

(cid:0) 9
λC

· 9C1 +

4
λC

ηω · 12C2 + (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

36
λC

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

+

+

+

(cid:104)

12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

η2
θ
η2
ω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2

(C3 + C4).

4
λC
8
λC

For simplicity, we deﬁne D := 6L1
previous inequality and simplifying, we obtain that

2 + (cid:98)c(cid:1) + L1 + L1(cid:98)c. Substituting the above bound into the
(cid:0) L

E(cid:107)∇J(θ(m)

)(cid:107)2

t

(cid:104) 1
2

ηθ − (cid:98)c

ηθ
βt

− 9(cid:0) L
2

≤ ERm

M +

0 − ERm
(cid:34)

(cid:1)(cid:105) M −1
(cid:88)

θ

θ + (cid:98)cη2
η2
ηθ + 2(cid:98)cηθ + 9(cid:0) L

t=0

(cid:104)

+ Dηθ

4
λC

(cid:104) 1
ηω

+ M

(cid:104)(cid:0) 9
λC

+ 2L2
3

2

θ

θ + (cid:98)cη2
η2
η2
θ
η2
ω

(cid:1)9L2

1

(cid:1)(cid:105)

C1 + 9L1

(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1)M E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

+ 18L2

4ηω

(cid:105)(cid:105)

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

+

4
λC

(cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

· 9C1 +

4
λC

ηω · 12C2 + (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

36
λC

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2 +

(cid:35)

8
λC

(C3 + C4).

One can see that the above bound is independent of (cid:80)M −1
E(cid:107)θ(m+1)
t
t=0
we desire. After simpliﬁcation, the above inequality further implies that

− (cid:101)θ(m)(cid:107)2, and this is what

η2
θ + (cid:98)cη2

θ

(cid:1) − D(cid:0) 9
λC

+ 2L2
3

(cid:1) 36
λC

η3
θ
η2
ω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

(cid:104) 1
2
≤E[Rm

ηθ − (cid:98)c

− 9(cid:0) L
2

ηθ
βt
0 ] − E[Rm
M ]
ηθ + 2(cid:98)cηθ + 9(cid:0) L

(cid:104)

+

θ + (cid:98)cη2
η2

θ

(cid:1)(cid:105)

C1 +

8
λC

(C3 + C4)Dηθ + Dηθ

2

(cid:34)

+

9L1

(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1)M + Dηθ

(cid:34)

4
λC

(cid:104) 1
ηω

+ M

(cid:104)(cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

1

(cid:0) 9
λC

+ 2L2
3

(cid:105)(cid:105)

+ 18L2

4ηω

(cid:1) η2
θ
η2
ω

(cid:35)

(cid:104) 4
λC
η2
θ
η2
ω

· 9C1 +

(cid:105)

ηω · 12C2

4
λC

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2.
(14)

Choose optimal learning rates: Here, we provide the omitted proof of our earlier claim made
after eq. (12), that is, the upper bound of {ct} is a small constant. We ﬁrst present the following
fundamental simple lemma, and the proof is omitted.

Lemma B.1. Let {ci}i=0,...,M be a ﬁnite sequence with cM = 0 and satisﬁes the following relation
for certain a > 1:

Then, {ci}i=0,...,M is a deceasing sequence and

ct ≤ a · ct+1 + b.

c0 ≤ ab ·

aM − 1
a − 1

.

17

Published as a conference paper at ICLR 2021

Next, we derive the upper bound (cid:98)c of ct. Set βt = 1 for all t. Then we have that

ct ≤ ct+1(ηθ + 1 + 2ηθ) + 9L1

(cid:0) L
2

θ + ct+1η2
η2
θ

(cid:1)

(cid:104)

+

6L1

(cid:0) L
2

θ + ct+1η2
η2
θ

(cid:1) + ηθL1 + ηθL1ct+1

(cid:105)

·

4
λC

(cid:104)
12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

(cid:105)

η2
θ
η2
ω

:= a · ct+1 + b

where

and

a = 1 + (3 + 16L1)ηθ

b =

15
2

L1Lη2

θ + L1ηθ ·

4
λC

(cid:104)
12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

(cid:105)

.

η2
θ
η2
ω

Note that here we require

and

Moreover, let

4
λC

(cid:104)
12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

(cid:105)

η2
θ
η2
ω

≤ 1

max{ηω, ηθ} ≤ 1.

(3 + 16L1)ηθ ≤

1
M

.

Based on the above conditions, we obtain that

(15)

(16)

(17)

c0 ≤

≤

(cid:104) 15
2
(cid:104) 15
2

L1Lηθ +

L1Lηθ +

4L1
λC
4L1
λC

Lastly, we choose

(cid:104)
12L2

(cid:104)
12L2

5ηω + (cid:0) 9
λC
5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

+ 2L2
3

(cid:1)9L2

2

(cid:105)(cid:105)

(cid:105)(cid:105)

·

·

η2
θ
η2
ω
η2
θ
η2
ω

4 + 16L1
3 + 16L1
4 + 16L1
3 + 16L1

(cid:104)
(1 + (3 + 16L1)ηθ)M − 1

(cid:105)

·

· (e − 1).

(cid:104) 15
2

L1Lηθ +

(cid:104)

12L2

4L1
λC

5ηω + (cid:0) 9
λC
8 . Since {ct}t is decreasing, we obtain that (cid:98)c = 1

+ 2L2
3

(cid:1)9L2

(cid:105)(cid:105)

η2
θ
η2
ω

2

·

Therefore c0 ≤ 1
(cid:98)c = 1
the following, and we choose an appropriate (ηθ, ηω) such that the coefﬁcient is greater than 1

8 . Now, substituting βt = 1 and
)(cid:107)2 in eq. (14), the coefﬁcient reduces to

8 into the coefﬁcient of the term (cid:80)M −1

E(cid:107)∇J(θ(m)

t=0

t

4 ηθ.

4 + 16L1
3 + 16L1

· (e − 1) ≤

1
8

.

(18)

3
8

ηθ − 9(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1) − D(cid:0) 9
λC

+ 2L2
3

(cid:1) 36
λC

η3
θ
η2
ω

≥

1
4

ηθ.

(19)

Deriving the ﬁnal bound: Exploiting the above conditions on the learning rates, eq. (14) further
implies that

1
4

ηθ

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

≤E[J((cid:101)θ(m))] − E[J((cid:101)θ(m+1))]
ηθ + 2(cid:98)cηθ + 9(cid:0) L

θ + (cid:98)cη2
η2

+

(cid:104)

θ

2

(cid:1)(cid:105)

C1 +

8
λC

(C3 + C4)Dηθ + Dηθ

(cid:34)

+

9L1

(cid:0) L
2

θ + (cid:98)cη2
η2

θ

(cid:1)M + Dηθ

(cid:34)

4
λC

(cid:104) 1
ηω

+ M

(cid:104)(cid:0) 9
λC

+ 2L2
3

18

(cid:104) 4
λC
η2
θ
η2
ω

(cid:1)9L2

1

(cid:105)(cid:105)

+ 18L2

4ηω

(cid:0) 9
λC

+ 2L2
3

· 9C1 +

(cid:105)

ηω · 12C2

4
λC

(cid:1) η2
θ
η2
ω

(cid:35)

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2.
(20)

Published as a conference paper at ICLR 2021

On the other hand, by Lemma D.8 we have that

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 ≤ (1 −
4
λC

+

1
λCηω)mM E(cid:107)(cid:101)ω(0) − ω∗((cid:101)θ(0))(cid:107)2
2
2
(cid:0)C3 + C4
λC

H 2ηω +

(cid:1) 1
M

4
λC

+

(cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
η2
ω

.

Substituting the above bound into eq. (20) and summing over m, we obtain that

1
4

ηθ

1
T M

T
(cid:88)

M −1
(cid:88)

m=1

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

(cid:110)(cid:104)

ηθ + 2(cid:98)cηθ + 9(cid:0) L

θ + (cid:98)cη2
η2

θ

(cid:1)(cid:105)

C1 +

8
λC

(C3 + C4)Dηθ + Dηθ

(cid:104) 4
λC

(cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

· 9C1 +

(cid:105)(cid:111)

ηω · 12C2

4
λC

2
(cid:1)M

θ + (cid:98)cη2
η2

θ

≤

E[J((cid:101)θ(0))]

+

1
T M
1
M
1
T M

+

(cid:104)

9L1

(cid:0) L
2
(cid:104) 1
ηω

(cid:104) 4
λC
(cid:0) L
2
(cid:0)C3 + C4

+ Dηθ

+

(cid:104)
9L1

1
M
(cid:104) 4
λC

·

+ M

(cid:104)(cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:105)(cid:105)(cid:105)(cid:105)

+ 18L2

4ηω

· E(cid:107)(cid:101)ω(0) − ω∗((cid:101)θ(0))(cid:107)2 ·

1
1 − (1 − 1
2 λCηω)M

θ

θ + (cid:98)cη2
η2
(cid:1) 1
M

+ M

(cid:1)M + Dηθ

(cid:104) 4
λC

+

4
λC

H 2ηω +

(cid:104) 1
ηω
2
(cid:0)2L2
λC

(cid:104)(cid:0) 9
λC
9
λC

3G2 +

G2(cid:1) η2
θ
η2
ω

(cid:105)(cid:105)

.

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:105)(cid:105)(cid:105)

+ 18L2

4ηω

Rearranging the above inequality, we obtain the following ﬁnal bound, where ξ, ζ are random in-
dexes that are sampled from {0, ..., M − 1} and {1, ..., T } uniformly at random, respectively.

E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2

ηθ + (cid:98)cηθ

(cid:1)(cid:105)

C1 +

8
λC

(C3 + C4)D + D

(cid:104) 4
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

· 9C1 +

(cid:105)(cid:111)

ηω · 12C2

4
λC

(cid:35)(cid:35)

(cid:105)(cid:105)

+ 18L2

4ηω

(cid:0) 9
λC
η2
θ
η2
ω

1

+ M

(cid:104)(cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

≤

1
ηθT M
4
M

+

(cid:110)(cid:104)

+

1
T M

· 4E[J((cid:101)θ(0))]
1 + 2(cid:98)c + 9(cid:0) L
(cid:0) L
2

9L1

(cid:34)

2

ηθ + (cid:98)cηθ

· E(cid:107)(cid:101)ω(0) − ω∗((cid:101)θ(0))(cid:107)2 ·
(cid:34)

(cid:0) L
2

ηθ + (cid:98)cηθ

(cid:1) + D

+

9L1

(cid:34)

(cid:1)M + D

(cid:104) 1
ηω

4
λC
1
1 − (1 − 1
2 λCηω)M
(cid:34)
(cid:104)(cid:0) 9
λC

4
λC

ηωM

(cid:104) 1

+

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:35)

(cid:105)(cid:105)

+ 18L2

4ηω

·

(cid:104) 4
λC

(cid:0)C3 + C4

(cid:1) 1
M

+

4
λC

H 2ηω +

2
λC

(cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
η2
ω

(cid:105)

(cid:35)
.

1

ηθT M ). The second term is of order O( 1

Next, we simplify the above inequality into an asymptotic form. Note that the ﬁrst term is in the order
of O(
)),
and the last term is the product of a term of order O( η2
θ
η2
ω
+ηω), which leads to the overall order O(( η2
O( 1
orders together, we obtain the following asymptotic convergence rate result.

ηωT M + 1
ηωM ) and another term of order
M ). Combining these asymptotic

M ). The third term is of order O(

+ ηω + 1
+ηω)2 + 1

T (ηω+ η2

M + η2

θ
η2
ω

θ
η2
ω

θ
η2
ω

1

E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2 = O

(cid:16)

1
ηθT M

+ (cid:0) η2
θ
η2
ω

+ ηω

(cid:1)2

+

1
M

+

1
T

(cid:0)ηω +

(cid:1)(cid:17)

.

η2
θ
η2
ω

19

Published as a conference paper at ICLR 2021

C PROOF OF COROLLARY 4.6

Regarding the convergence rate result of Theorem 4.5, we choose the optimized learning rates such
that ηθ = O(η3/2

ω ), and we obtain that

E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2 = O

(cid:16)

1
ηθT M

+ η2

ω +

1
M

+

(cid:17)

.

ηω
T

Then, we set ηθ = O( 1
M 2/3 ). Under
this learning rate setting, the learning rate conditions in eq. (15), eq. (16), eq. (18), eq. (19) are
all satisﬁed for a sufﬁciently large constant-level M . Then, the overall convergence rate further
becomes

M ) such that eq. (17) is satisﬁed, and moreover ηω = O(

1

(cid:16) 1
T
By choosing T, M = O((cid:15)−1), we conclude that
E(cid:107)∇J(θ(ζ)

ξ )(cid:107)2 ≤ (cid:15) is in the order of T M = O((cid:15)−2).

ξ )(cid:107)2 = O

E(cid:107)∇J(θ(ζ)

+

(cid:17)
.

1
M

(21)

the sample complexity for achieving

D TECHNICAL LEMMAS

In this section, we present all the technical lemmas that are used in the proof of the main theorem.

t

t

(cid:107)2:

(cid:107)2 and E(cid:107)h(m)

Bounding E(cid:107)g(m)
Lemma D.1. Under the same assumptions as those of Theorem 4.5, the square norm of the one-step
update of θ(m)
E(cid:107)g(m)
t

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 + 9L2

t − (cid:101)θ(m)(cid:107)2

)(cid:107)2 + 9L2
1

E(cid:107)θ(m)

2

in Algorithm 1 is bounded as
t − ω∗(θ(m)
9C1 + 9E(cid:107)∇J(θ(m)

t
E(cid:107)ω(m)
(cid:107)2 ≤ 6L2
1
1
M

+

t

t

)(cid:107)2

where the constant C1 is speciﬁed in Lemma D.3.

Proof. For convenience, deﬁne

and

T (m)
t

:= G(m)
t

(θ(m)
t

, ω(m)
t

) − G(m)

t

((cid:101)θ(m), (cid:101)ω(m)),

S (m)
t

:= G(m)
t

(θ(m)
t

, ω∗(θ(m)

t

)) − G(m)

t

((cid:101)θ(m), ω∗((cid:101)θ(m))).

Then, we obtain that

(cid:107)g(m)
t

(cid:107)2 = (cid:107)G(m)
t
= (cid:107)T (m)
t
− S (m)
≤ 3(cid:107)T (m)

, ω(m)
t

) − G(m)

(θ(m)
t
+ (cid:101)G(m) − (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m))) + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))

((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)G(m)(cid:107)2

t

t + S (m)
t
− S (m)
t

(cid:107)2
(cid:107)2 + 3(cid:107) (cid:101)G(m) − (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2

t
+ 3(cid:107)S (m)
1(cid:107)ω(m)
+ 3(cid:107)S (m)

≤ 6L2

t + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2
t − ω∗(θ(m)
t + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2,

)(cid:107)2 + 9L2

t

1(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

where (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m))) is obtained by substituting the arguments (cid:101)θ(m), ω∗((cid:101)θ(m)) into the def-
inition in eq. (4). Moreover, we have that

t + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2

(cid:107)S (m)
= (cid:107)S (m)
≤ 3(cid:107)S (m)

t + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m))) − ∇J(θ(m)
t − Em,tS (m)

(cid:107)2 + 3(cid:107) (cid:101)G(m)(θ(m)

t
, ω∗(θ(m)

t

t

t

) + ∇J(θ(m)

)(cid:107)2

t

)) − ∇J(θ(m)

t

)(cid:107)2

+ 3(cid:107)∇J(θ(m)

t

)(cid:107)2,

20

Published as a conference paper at ICLR 2021

which further implies that

t + (cid:101)G(m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2

E(cid:107)S (m)
≤ 3E(cid:107)S (m)

t
E(cid:107)θ(m)

(cid:107)2 + 3E(cid:107) (cid:101)G(m)(θ(m)

t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

)(cid:107)2 + 3E(cid:107)∇J(θ(m)

t

)(cid:107)2

≤ 3L2
2

t − (cid:101)θ(m)(cid:107)2 +

1
M

· 3C1 + 3E(cid:107)∇J(θ(m)

t

)(cid:107)2.

Combining all the above bounds, we ﬁnally obtain that

E(cid:107)g(m)
t

(cid:107)2 ≤ 6L2
1

E(cid:107)ω(m)

t − ω∗(θ(m)

t

+ 9L2
2

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

)(cid:107)2 + 9L2
1
1
M

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
)(cid:107)2.

· 9C1 + 9E(cid:107)∇J(θ(m)

t

Lemma D.2. Under the same assumptions as those of Theorem 4.5, we have that

(cid:107)h(m)
t

(cid:107)2 ≤ 6L2

4(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 + 9L2

4(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 + 6L2

5(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

6C2
M

.

Proof. For convenience, deﬁne

V (m)
t

:= H (m)
t

(θ(m)
t

, ω(m)
t

) − H (m)

t

((cid:101)θ(m), (cid:101)ω(m)),

and

U (m)
t

:= H (m)
t

(θ(m)
t

, ω∗(θ(m)

t

)) − H (m)

t

((cid:101)θ(m), ω∗((cid:101)θ(m))).

Then, we obtain that

(cid:107)h(m)
t

t

(cid:107)2 = (cid:107)H (m)
= (cid:107)V (m)
t
≤ 3(cid:107)V (m)

, ω(m)
t

) − H (m)

(θ(m)
((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)H (m)(cid:107)2
t
+ (cid:101)H (m) − (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m))) + (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m))) − U (m)
− U (m)
t

(cid:107)2 + 3(cid:107) (cid:101)H (m) − (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2

t

t

t

t
+ 3(cid:107)U (m)
4(cid:107)ω(m)
+ 3(cid:107)U (m)

≤ 6L2

+ (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2

t − ω∗(θ(m)

)(cid:107)2 + 9L2
+ (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2.

t

t

4(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

+ U (m)
t

(cid:107)2

Moreover, note that

(cid:107)U (m)
t

+ (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2 ≤ 2(cid:107)U (m)
5(cid:107)θ(m)

≤ 2L2

t

(cid:107)2 + 2(cid:107) (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2
2C2
M

t − (cid:101)θ(m)(cid:107)2 +

.

Combining the above bounds, we ﬁnally obtain that

(cid:107)h(m)
t

(cid:107)2 ≤ 6L2

4(cid:107)ω(m)
t − ω∗(θ(m)
5(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

t

)(cid:107)2 + 9L2
6C2
M

.

+ 6L2

4(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

Bounding pseudo-gradient variance:

Lemma D.3. Under the same assumptions as those of Theorem 4.5, we have that

E(cid:107) (cid:101)G(m)(θ(m)

t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

)(cid:107)2 ≤

C1
M

.

21

Published as a conference paper at ICLR 2021

Proof. Note that the variance can be expanded as

E(cid:107) (cid:101)G(m)(θ(m)

t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

)(cid:107)2

=

1
M 2

M −1
(cid:88)

E(cid:2)

(cid:107)G(m)
s

(θ(m)
t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

)(cid:107)2

+

(cid:88)

s=0
(cid:104)G(m)
i

i(cid:54)=j

(θ(m)
t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

), G(m)
j

(θ(m)
t

, ω∗(θ(m)

t

)) − ∇J(θ(m)

t

)(cid:105)(cid:3)

≤

(cid:2)

1
M 2

M −1
(cid:88)

(G + C∇J )2 +

s=0

(cid:88)

i(cid:54)=j

Λρ|i−j|(G + C∇J )2(cid:3)

≤

1
M

(1 + 2Λ

ρ
1 − ρ

)(G + C∇J )2.

Then, we deﬁne the constant C1 := (1 + 2Λ ρ

1−ρ )(G + C∇J )2.

Lemma D.4. Under the same assumptions as those of Theorem 4.5, we have that

E(cid:107) (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2 ≤

C2
M

.

Proof. Note that this second moment term can be expanded as

E(cid:107) (cid:101)H (m)((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)2 =

1
M 2

M −1
(cid:88)

i=0

E(cid:107)H (m)
i

((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:107)

+

1
M 2

(cid:88)

i(cid:54)=j

E(cid:104)H (m)
i

((cid:101)θ(m), ω∗((cid:101)θ(m))), H (m)

j

((cid:101)θ(m), ω∗((cid:101)θ(m)))(cid:105)

≤

H 2
M

+

≤ H 2(1 + Λ

(cid:88)

ρ|i−j|

1
M 2 H 2Λ
ρ
1 − ρ

i(cid:54)=j
1
M

)

.

Lastly, we deﬁne the constant C2 := H 2(1 + Λ ρ

1−ρ ).

Bounding Markovian Noise:

Lemma D.5. Let the same assumptions as those of Theorem 4.5 hold and deﬁne

ς (m)
t

Then, it holds that

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:0)φ(m)

t

(φ(m)
t

)(cid:62) − C(cid:1)(cid:0)ω(m)

t − ω∗(θ(m)

t

)(cid:1)(cid:105).

E[ς (m)
t

] ≤

1
8

λC(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 +

C3
M

,

where C3 = 8R2
λC

(1 + ρΛ

1−ρ ).

Proof. By deﬁnition of ς (m)

t

, we obtain that

E(cid:104)ω(m)
=E(cid:104)ω(m)

t

t − ω∗(θ(m)
t − ω∗(θ(m)
λC
4

E(cid:107)ω(m)

t

≤

1
2

·

t − ω∗(θ(m)

t

), (cid:0)φ(m)
t
), Em,t−1

(φ(m)
t
(cid:0)φ(m)

)(cid:62) − C(cid:1)(cid:0)ω(m)
(φ(m)
t

t

t − ω∗(θ(m)

t

)(cid:1)(cid:105)
t − ω∗(θ(m)

t

)(cid:62) − C(cid:1)(cid:0)ω(m)

)(cid:1)(cid:105)

)(cid:107)2 +

1
2

·

4R2
λCM 2

E(cid:107)

M −1
(cid:88)

i=0

(cid:0)φ(m)

i

(φ(m)
i

)(cid:62) − C(cid:1)(cid:107)2.

22

Published as a conference paper at ICLR 2021

For the last term, note that

E(cid:107)

M −1
(cid:88)

i=0

(cid:0)φ(m)

i

(φ(m)
i

)(cid:62) − C(cid:1)(cid:107)2

= E

M −1
(cid:88)

i=0

(cid:107)φ(m)
i

(φ(m)
i

)(cid:62) − C(cid:107)2 + E (cid:88)

(cid:104)φ(m)
i

(φ(m)
i

)(cid:62) − C, φ(m)

j

(φ(m)
j

)(cid:62) − C(cid:105)

i(cid:54)=j

≤ 4M + 4

Λρ|i−j|

(cid:88)

i(cid:54)=j

≤ 4M + 4M

ρΛ
1 − ρ

.

Combining the above bounds, we ﬁnally obtain that

E(cid:104)ω(m)
λC
8

≤

t − ω∗(θ(m)

t

), (cid:0)φ(m)

t

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 +

(φ(m)
t
8R2
λC

)(cid:62) − C(cid:1)(cid:0)ω(m)
1
M

ρΛ
1 − ρ

(1 +

)

.

t − ω∗(θ(m)

t

)(cid:1)(cid:105)

We deﬁne C3 := 8R2
λC

(1 + ρΛ

1−ρ ).

Lemma D.6. Let the same assumptions as those of Theorem 4.5 hold and deﬁne

κ(m)
t

Then, we obtain that

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:2)(φ(m)

t

)(cid:62)ω∗(θ(m)

t

) − δ(m)

t+1 (θ(m)

t

)(cid:3)φ(m)

t

(cid:105).

Eκ(m)

t ≤

1
8

λC(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 +

C4
M

.

Proof. Similar to the proof of Lemma D.6, we have that

t − ω∗(θ(m)
E(cid:104)ω(m)
1
λC
E(cid:107)ω(m)
4
2

·

t

≤

t − ω∗(θ(m)

t

)(cid:107)2

), (cid:2)(φ(m)

t

)(cid:62)ω∗(θ(m)

t

) − δ(m)

t+1 (θ(m)

t

)(cid:3)φ(m)

t

(cid:105)

+

1
2

·

4
λCM 2

E(cid:107)

M −1
(cid:88)

i=0

(cid:0)(cid:2)(φ(m)

i

)(cid:62)ω∗(θ(m)

t

) − δ(m)

i+1 (θ(m)

t

)(cid:3)φ(m)

i

(cid:1)(cid:107)2.

For the last term, we can bound it as

E(cid:107)

M −1
(cid:88)

i=0

(cid:0)(cid:2)(φ(m)

i

)(cid:62)ω∗(θ(m)

t

) − δ(m)

i+1 (θ(m)

t

)(cid:3)φ(m)

i

(cid:1)(cid:107)2

≤(R(2 + γ) + rmax)2M + (R(2 + γ) + rmax)2 ρΛ
1 − ρ

M.

Combining all the above bounds, we ﬁnally obtain that

t − ω∗(θ(m)
E(cid:107)ω(m)

E(cid:104)ω(m)
λC
8

t − ω∗(θ(m)

t

t

≤

), (cid:2)(φ(m)

t

)(cid:62)ω∗(θ(m)

t

) − δ(m)

t+1 (θ(m)

t

)(cid:3)φ(m)

t

)(cid:107)2 +

2
λC

(R(2 + γ) + rmax)2(1 +

(cid:105)
ρΛ
1 − ρ

)

1
M

.

We then deﬁne C4 := 2
λC

(R(2 + γ) + rmax)2(1 + ρΛ

1−ρ ).

Bounding Tracking Error:

23

Published as a conference paper at ICLR 2021

Lemma D.7. Under the same assumptions as those of Theorem 4.5, the tracking error can be
bounded as

M −1
(cid:88)

t=0

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

≤

4
λC

(cid:104) 1
ηω

+ M

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:105)(cid:105)

+ 18L2

4ηω

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

· 9C1 +

4
λC

ηω · 12C2 + (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

36
λC

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

(cid:104)(cid:0) 9
λC
(cid:1) η2
θ
η2
ω

+ 2L2
3

4
λC

(cid:0) 9
λC

+

+

+

(cid:104)

12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

η2
θ
η2
ω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2

(C3 + C4).

4
λC
8
λC

Proof. Recall the one-step update at ω(m)
t+1 :

ω(m)
t+1 = ΠR

(cid:0)ω(m)

t − ηωh(m)

t

(cid:1).

Then, we obtain the following upper bound of the tracking error (cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2,

(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤ (cid:107)ω(m)
≤ (cid:107)ω(m)

t − ω∗(θ(m)
t − ω∗(θ(m)

t

t

t + ω∗(θ(m)

t

) − ηωh(m)
)(cid:107)2 − 2ηω(cid:104)ω(m)
), ω∗(θ(m)

t − ω∗(θ(m)
t
) − ω∗(θ(m)
t+1)(cid:105)
t+1)(cid:107)2.

+ 2(cid:104)ω(m)
+ 2η2

t − ω∗(θ(m)
ω(cid:107)h(m)

t
) − ω∗(θ(m)
(cid:107)2 + 2(cid:107)ω∗(θ(m)

t

t

t

) − ω∗(θ(m)
), h(m)
t

t+1)(cid:107)2
(cid:105)

Substituting the bound of Lemma D.2 into the above bound, we obtain that

)(cid:107)2 − 2ηω(cid:104)ω(m)
)(cid:107)2

t − ω∗(θ(m)

t

), h(m)
t

(cid:105)

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 + 9L2
1

(cid:107)ω(m)
≤ (cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2
t − ω∗(θ(m)

t

+ λCηω(cid:107)ω(m)
+ (cid:0) 9
λC

+ 2L2
3

t

t − ω∗(θ(m)
(cid:104)

(cid:1) η2
θ
ηω
t − (cid:101)θ(m)(cid:107)2 +

6L2
1

+ 9L2
2

E(cid:107)θ(m)

+ 2η2
ω

(cid:104)

6L2
4

E(cid:107)ω(m)

1
M
t − ω∗(θ(m)

t

· 9C1 + 9E(cid:107)∇J(θ(m)

t

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
)(cid:107)2(cid:105)

)(cid:107)2 + 9L2
4

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 + 6L2

5

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 +

(cid:105)
.

6C2
M

Taking expectation on both sides of the above inequality and simplifying, we obtain that

+ 2L2
3

(cid:1)6L2

1

η2
θ
ηω

(cid:17)

+ 12L2

4η2
ω

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

≤

t+1 − ω∗(θ(m)
E(cid:107)ω(m)
t+1)(cid:107)2
(cid:16)
1 − λCηω + (cid:0) 9
λC
(cid:1)9L2

+ 2L2
3

+

1

(cid:104)(cid:0) 9
λC
+ (cid:0) 9
λC

η2
θ
ηω
9C1
M

+ 18L2

4η2
ω

(cid:105)
E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
(cid:1) η2
θ
ηω

+ (cid:0) 9
λC

+ 2L2
3

t − (cid:101)θ(m)(cid:107)2

+ 2L2
3

·

(cid:1) η2
θ
ηω
ω + (cid:0) 9
λC
t − 2ηωEς (m)

+ 2L2
3

t

(cid:104)

+

12L2

5η2

− 2ηωEκ(m)

12C2
M
(cid:105)
E(cid:107)θ(m)

+ η2

ω ·

(cid:1)9L2

2

η2
θ
ηω

9E(cid:107)∇J(θ(m)

t

,

24

)(cid:107)2

(22)

Published as a conference paper at ICLR 2021

where

and

ς (m)
t

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:0)φ(m)

t

(φ(m)
t

)(cid:62) − C(cid:1)(cid:0)ω(m)

t − ω∗(θ(m)

t

)(cid:1)(cid:105),

κ(m)
t

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:2)(φ(m)

t

)(cid:62)ω∗(θ(m)

t

) − δ(m)

t+1 (θ(m)

t

)(cid:3)φ(m)

t

(cid:105).

Applying Lemma D.5, and Lemma D.6 to (22), we obtain that

E(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤

t − ω∗(θ(m)

t

)(cid:107)2

(cid:16)

+

1 −

1
2
(cid:104)(cid:0) 9
λC
+ (cid:0) 9
λC

(cid:104)

+

12L2

5η2

λCηω + (cid:0) 9
λC
(cid:1)9L2

+ 2L2
3

1

+ 2L2
3

·

(cid:1) η2
θ
ηω
ω + (cid:0) 9
λC
1
M

+ 2ηω(C3 + C4)

+ 2L2
3

(cid:1)6L2

1

η2
θ
ηω
9C1
M

+ 18L2

4η2
ω

+ η2

ω ·

(cid:17)

4η2
ω

+ 12L2

E(cid:107)ω(m)

η2
θ
ηω
(cid:105)
E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2
(cid:1) η2
θ
ηω

+ (cid:0) 9
λC

+ 2L2
3

12C2
M
(cid:105)
E(cid:107)θ(m)

η2
θ
ηω

t − (cid:101)θ(m)(cid:107)2

+ 2L2
3

(cid:1)9L2

2

.

9E(cid:107)∇J(θ(m)

)(cid:107)2

t

(23)

)(cid:107)2

Telescoping the above inequality over one epoch, we obtain that

(cid:16) 1
2

≤ E(cid:107)ω(m)

+ 2L2
3

λCηω − (cid:0) 9
λC
M − ω∗(θ(m)
(cid:104)(cid:0) 9
λC

+ 2L2
3

+ M

M )(cid:107)2
(cid:1)9L2

1

(cid:1)6L2

1

η2
θ
ηω

− 12L2

4η2
ω

(cid:17) M −1
(cid:88)

t=0

E(cid:107)ω(m)

t − ω∗(θ(m)

t

(cid:105)

+ 18L2

4η2
ω

η2
θ
ηω

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

+ (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
ηω

· 9C1 + η2

ω · 12C2 + (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
ηω

9

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

(cid:104)

+

12L2

5η2

ω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

η2
θ
ηω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2

+ 2ηω(C3 + C4).

Choosing an appropriate (ηθ, ηω) such that

1
2

λCηω − (cid:0) 9
λC

+ 2L2
3

(cid:1)6L2

1

η2
θ
ηω

and we ﬁnally obtain that

M −1
(cid:88)

t=0

E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

− 12L2

4η2

ω ≥

1
4

λCηω,

(24)

≤

4
λC

(cid:104) 1
ηω

+ M

+ 2L2
3

(cid:1)9L2

1

η2
θ
η2
ω

(cid:105)(cid:105)

+ 18L2

4ηω

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

· 9C1 +

4
λC

ηω · 12C2 + (cid:0) 9
λC

+ 2L2
3

(cid:1) η2
θ
η2
ω

36
λC

M −1
(cid:88)

t=0

E(cid:107)∇J(θ(m)

t

)(cid:107)2

(cid:104)(cid:0) 9
λC
(cid:1) η2
θ
η2
ω

+ 2L2
3

4
λC

(cid:0) 9
λC

+

+

+

(cid:104)

12L2

5ηω + (cid:0) 9
λC

+ 2L2
3

(cid:1)9L2

2

η2
θ
η2
ω

(cid:105) M −1
(cid:88)

t=0

E(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2

(C3 + C4).

4
λC
8
λC

25

Published as a conference paper at ICLR 2021

Lemma D.8. Under the same assumptions as those of Theorem 4.5, the tracking error can be
bounded as

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 ≤ (1 −
4
λC

+

1
λCηω)mM E(cid:107)(cid:101)ω(0) − ω∗((cid:101)θ(0))(cid:107)2
2
2
(cid:0)C3 + C4
λC

H 2ηω +

(cid:1) 1
M

4
λC

+

(cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
η2
ω

.

Proof. Recall the one-step update at ω(m)
t+1 :

ω(m)
t+1 = ΠR

(cid:0)ω(m)

t − ηωh(m)

t

(cid:1).

Then, we obtain the following upper bound of the tracking error (cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2.

(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤ (cid:107)ω(m)
≤ (cid:107)ω(m)

t − ω∗(θ(m)
t − ω∗(θ(m)

t

t

) − ω∗(θ(m)
), h(m)
t

t+1)(cid:107)2
(cid:105)

t + ω∗(θ(m)

t

) − ηωh(m)
)(cid:107)2 − 2ηω(cid:104)ω(m)
), ω∗(θ(m)

t − ω∗(θ(m)
t
) − ω∗(θ(m)
t+1)(cid:105)
t+1)(cid:107)2.

t
) − ω∗(θ(m)

+ 2(cid:104)ω(m)
+ 2η2

t − ω∗(θ(m)
ωH 2 + 2(cid:107)ω∗(θ(m)

t

t

Then above inequality can be further bounded as

(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤ (cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 − 2ηω(cid:104)ω(m)
)(cid:107)2

t − ω∗(θ(m)

t

), h(m)
t

(cid:105)

+ λCηω(cid:107)ω(m)
+ (cid:0) 9
λC
ωH 2.
+ 2η2
Taking conditional expectation on both sides of the above inequality, we obtain that

t − ω∗(θ(m)

(cid:1) η2
θ
ηω

+ 2L2
3

G2

t

Em,0(cid:107)ω(m)
≤ Em,0(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2
t − ω∗(θ(m)

t

− 2ηωEm,0(cid:104)ω(m)

t − ω∗(θ(m)

+ λCηω(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2 − 2ηωEm,0(cid:104)ω(m)
), −H (m)
t
9η2
θ
λCηω

t − ω∗(θ(m)
((cid:101)θ(m), (cid:101)ω(m)) + (cid:101)H (m)(cid:105)
G2

)(cid:107)2 +

t

t

), H (m)
t

(θ(m)
t

, ω(m)
t

)(cid:105)

+ 2H 2η2
= Em,0(cid:107)ω(m)

3G2η2
ω + 2L2
θ
t − ω∗(θ(m)

t

)(cid:107)2 − 2ηωEm,0(cid:104)ω(m)

t − ω∗(θ(m)

t

+ λCηω(cid:107)ω(m)

t

t − ω∗(θ(m)
)(cid:107)2
G2(cid:1) η2
9
θ
ηω
λC

3G2 +

ω + (cid:0)2L2

+ 2H 2η2

), H (m)
t

(θ(m)
t

, ω(m)
t

)(cid:105)

(25)

To further bound the inequality above, we ﬁrst consider the following explicit form of the pseudo-
gradient term:

H (m)
t

(θ, ω) = (cid:2)(φ(m)

)(cid:62)ω − δ(m)

t
= φ(m)
t
= (cid:0)φ(m)

t

t

t+1 (θ)(cid:3)φ(m)
)(cid:62)(cid:0)ω − ω∗(θ)(cid:1) + (cid:2)(φ(m)
)(cid:62) − C(cid:1)(cid:0)ω − ω∗(θ)(cid:1) + C(cid:0)ω − ω∗(θ)(cid:1)

)(cid:62)ω∗(θ(m)

t

t

(φ(m)
t
(φ(m)
t
)(cid:62)ω∗(θ) − δ(m)

t+1 (θ)(cid:3)φ(m)

t

+ (cid:2)(φ(m)

t

) − δ(m)

t+1 (θ)(cid:3)φ(m)

t

.

(26)

By Assumption 4.3, we have

−2ηωEm,0(cid:104)ω(m)

t − ω∗(θ(m)

t

), C(cid:0)ω(m)

t − ω∗(θ(m)

t

26

)(cid:1)(cid:105) ≤ −2ηλC(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2.

(27)

Published as a conference paper at ICLR 2021

Substituting eq. (27) and eq. (26) into eq. (25) yields that

E(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤ (1 − λCηω)E(cid:107)ω(m)

)(cid:107)2 − 2ηωEκ(m)

t − 2ηωEς (m)

t

+ 2H 2η2

ω + (cid:0)2L2

t

t − ω∗(θ(m)
9
λC

3G2 +

G2(cid:1) η2
θ
ηω

,

(28)

where

and

ς (m)
t

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:0)φ(m)

t

(φ(m)
t

)(cid:62) − C(cid:1)(cid:0)ω(m)

t − ω∗(θ(m)

t

)(cid:1)(cid:105),

κ(m)
t

:= (cid:104)ω(m)

t − ω∗(θ(m)

t

), (cid:2)(φ(m)

t

)(cid:62)ω∗(θ(m)

t

) − δ(m)

t+1 (θ(m)

t

)(cid:3)φ(m)

t

(cid:105).

Applying Lemma D.5, and Lemma D.6 to the above inequality, we obtain that

E(cid:107)ω(m)

t+1 − ω∗(θ(m)

t+1)(cid:107)2 ≤ (1 −

λCηω)E(cid:107)ω(m)

t − ω∗(θ(m)

t

)(cid:107)2

1
2
(cid:0)C3 + C4

(cid:1) 1
M

+ 2ηω

Telescoping the above inequality over one epoch, we obtain that

+ 2H 2η2

ω + (cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
ηω

.

E(cid:107)ω(m)

M − ω∗(θ(m)

M )(cid:107)2 ≤ (1 −

λCηω)M E(cid:107)ω(m)

1
2
(cid:0)C3 + C4

+ 2ηω

0 − ω∗(θ(m)
0
1 − (1 − 1

)(cid:107)2

2 λCηω)M

1
2 λCηω

(cid:1) 1
M

·

+ 2H 2η2

ω ·

1 − (1 − 1

2 λCηω)M

1
2 λCηω

+ (cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
ηω

·

1 − (1 − 1

2 λCηω)M

1
2 λCηω

By deﬁnition, (cid:101)ω(m) = ω(m)
chosen as the reference parameter, ω(m)

M and (cid:101)θ(m) = θ(m)

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 ≤ (1 −

1
2
(cid:0)C3 + C4

+ 2ηω

M , and the initial parameter for the current inner loop is

0 = (cid:101)ω(m) and θ(m)
λCηω)M E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2

0 = (cid:101)θ(m). Then we have

(cid:1) 1
M

·

1 − (1 − 1

2 λCηω)M

1
2 λCηω

+ 2H 2η2

ω ·

1 − (1 − 1

2 λCηω)M

1
2 λCηω
Then, we unroll the inequality above and yield that

+ (cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
ηω

·

1 − (1 − 1

2 λCηω)M

1
2 λCηω

E(cid:107)(cid:101)ω(m) − ω∗((cid:101)θ(m))(cid:107)2 ≤ (1 −
4
λC

+

1
λCηω)mM E(cid:107)(cid:101)ω(0) − ω∗((cid:101)θ(0))(cid:107)2
2
2
(cid:0)C3 + C4
λC

H 2ηω +

(cid:1) 1
M

4
λC

+

(cid:0)2L2

3G2 +

9
λC

G2(cid:1) η2
θ
η2
ω

.

E OTHER SUPPORTING LEMMAS

Constant Bounds:
Lemma E.1. Within the set {θ : (cid:107)θ(cid:107) ≤ R}, there exists a constant C∇J such that

(cid:107)∇J(θ)(cid:107) ≤ C∇J .

sup
θ

(29)

Proof. By Lemma E.9, ∇J(θ) is smooth. Hence, by the compactness of {θ : (cid:107)θ(cid:107) ≤ R}, we
conclude that (cid:107)∇J(θ)(cid:107) is bounded by a certain constant C∇J .

27

.

.

Published as a conference paper at ICLR 2021

Lemma E.2. Let G := rmax + (1 + γ)R + γ(|A|Rk1 + 1)R be a constant unrelated to m and t.
Then (cid:107)G(m)

(cid:107) ≤ G for all m and t.

t

Proof. By its deﬁnition, we obtain that

(cid:107)G(m)
t

(cid:107) = (cid:107)(cid:0) − δt+1(θt)φt + γ(ω(cid:62)

t φt) (cid:98)φt+1(θt)(cid:1)(cid:107)

≤ (cid:107)(cid:0) − δt+1(θt)(cid:107)(cid:107)φt(cid:107) + γ(cid:107)(ω(cid:62)
≤ rmax + (1 + γ)R + γ(|A|Rk1 + 1)R.

t φt)(cid:107)(cid:107) (cid:98)φt+1(θt)(cid:1)(cid:107)

Lemma E.3. Let H = (2 + γ)R + rmax be a constant unrelated to m and t. Then (cid:107)H (m)
for all m and t.

t

(cid:107) ≤ H

Proof. The result follows from the deﬁnition:

(cid:107)H (m)
t

(cid:107) = (cid:107)(cid:2)φT

t ωt − δt+1(θt)(cid:3)φt(cid:107)

≤ (2 + γ)R + rmax.

Lipschitz Continuity:
Lemma E.4. The mapping ω (cid:55)→ G(m)

t+1(θ, ω) is L1-Lipschitz in ω for all θ.

Proof. See Lemma 3 of Wang & Zou (2020).

Lemma E.5. The mapping θ (cid:55)→ G(m)

t+1(θ, ω∗(θ)) is L2-Lipschitz in θ.

Proof. See Lemma 3 of Wang & Zou (2020).

Lemma E.6. The mapping ω∗(·) is L3-Lipschitz.

Proof. See eq.(56) of Wang & Zou (2020).

Lemma E.7. The mapping ω (cid:55)→ H (m)

t+1 (θ, ω) is L4-Lipschitz.

Proof. It follows that

(cid:107)H (m)

t+1 (θ, ω2) − H (m)

(cid:3)T

ω1

(cid:1)φ(m)

t − (cid:0)δ(m)

t+1 (θ) − (cid:2)φ(m)

t

(cid:3)T

ω2

(cid:1)φ(m)

t

(cid:107)

t+1 (θ, ω2)(cid:107) = (cid:107)(cid:0)δ(m)
≤ (cid:107)φ(m)
t
≤ (cid:107)ω1 − ω2(cid:107).

t+1 (θ) − (cid:2)φ(m)
(cid:107)2(cid:107)ω1 − ω2(cid:107)

t

Hence, L4 = 1.
Lemma E.8. The mapping θ (cid:55)→ H (m)

t+1 (θ, ω∗(θ)) is L5-Lipschitz.

Proof. By deﬁnition, we have

t+1 (θ1, ω∗(θ1)) − H (m)
t+1 (θ1) − (cid:2)φ(m)
t+1 (θ1) − δ(m)

(cid:107)H (m)
= (cid:107)(cid:0)δ(m)
≤ (cid:107)δ(m)
≤ (cid:0)(γ|A|k1R + 1) + 1 + L3

t+1 (θ2, ω∗(θ2))(cid:107)
t − (cid:0)δ(m)
ω∗(θ1)(cid:1)φ(m)
(cid:3)T
t+1 (θ2)(cid:107) + (cid:107)ω∗(θ1) − ω∗(θ2)(cid:107)

(cid:1)(cid:107)θ1 − θ2(cid:107).

t

t+1 (θ2) − (cid:2)φ(m)

t

(cid:3)T

ω∗(θ2)(cid:1)φ(m)

t

(cid:107)

Hence, L5 = (γ|A|k1R + 1) + 1 + L3.

Bounding Lyapunov function:

28

Published as a conference paper at ICLR 2021

Lemma E.9 (L-smoothness of J). For any θ1 and θ2, it holds that

|J(θ1) − J(θ2) − (cid:104)∇J(θ2), θ1 − θ2(cid:105)| ≤

L
2

(cid:107)θ1 − θ2(cid:107)2.

Proof. See Lemma 2 of Wang & Zou (2020).

Lemma E.10. It holds that

(cid:107)θ(m)

t+1 − (cid:101)θ(m)(cid:107)2 = η2

θ (cid:107)g(m)
t
(cid:104) 1
βt

+ ηθ

(cid:107)2 + (cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 − 2ηθζ (m)

t

(cid:107)∇J(θ(m)

t

)(cid:107)2 + βt(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2(cid:105)

,

(30)

where ζ (m)

t

:= (cid:104)g(m)

t − ∇J(θ(m)

t

), θ(m)

t − (cid:101)θ(m)(cid:105).

Proof. Note that

(cid:107)θ(m)

t+1 − (cid:101)θ(m)(cid:107)2 = (cid:107)θ(m)

(cid:107)2 + (cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2 + 2(cid:104)θ(m)

, θ(m)

t − (cid:101)θ(m)(cid:105)

t

t − (cid:101)θ(m)(cid:107)2 − 2ηθ(cid:104)g(m)
t − (cid:101)θ(m)(cid:107)2 − 2ηθ(cid:104)g(m)
), θ(m)
t − (cid:101)θ(m)(cid:107)2 − 2ηθ(cid:104)g(m)

t − (cid:101)θ(m)(cid:105)

t
t − (cid:101)θ(m)(cid:105)

t+1 − θ(m)
, θ(m)
t − ∇J(θ(m)

t

t − ∇J(θ(m)

t

), θ(m)

t − (cid:101)θ(m)(cid:105)

), θ(m)

t − (cid:101)θ(m)(cid:105)

= η2

= η2

= η2

(cid:107)2 + (cid:107)θ(m)
(cid:107)2 + (cid:107)θ(m)

t

t

t

t+1 − θ(m)
θ (cid:107)g(m)
θ (cid:107)g(m)
− 2ηθ(cid:104)∇J(θ(m)
θ (cid:107)g(m)
t
(cid:104) 1
βt

+ ηθ

t
(cid:107)2 + (cid:107)θ(m)

(cid:107)∇J(θ(m)

t

)(cid:107)2 + βt(cid:107)θ(m)

t − (cid:101)θ(m)(cid:107)2(cid:105)

.

F DETAILS OF EXPERIMENTS

Garnet problem: The Garnet problem Archibald et al. (1995) is speciﬁed as G(nS , nA, b, d), where
nS and nA denote the cardinality of the state and action spaces, respectively, b is referred to as the
branching factor–the number of states that have strictly positive probability to be visited after an
action is taken, and d denotes the dimension of the features. In our experiment, we set nS = 5,
nA = 3, b = 2, d = 4 and generate the features Φ ∈ RnS ×d via the uniform distribution on [0, 1].
We then normalize its rows to have unit norm. Then, we randomly generate a state-action transition
kernel P ∈ RnS ×nA×nS via the uniform distribution on [0, 1] (with proper normalization). We set
the behavior policy as the uniform policy, i.e., πb(a|s) = n−1
A for any s and a. The discount factor
is set to be γ = 0.95. As the transition kernel and the features are known, we compute (cid:107)∇J(θ)(cid:107)2
to evaluate the performance of all the algorithms. We set the default learning rates as ηθ = 0.02
and ηω = 0.01 for both VR-Greedy-GQ and Greedy-GQ algorithm. For VR-Greedy-GQ, we set the
default batch size as M = 3000.

Frozen Lake: We generate a Gaussian feature matrix with dimension 8 to linearly approximate the
value function and we aim to evaluate a target policy based on a behavior policy. The target policy
is generated via the uniform distribution on [0, 1] with proper normalization and the behavior policy
is the uniform policy. We set the learning rates as ηθ = 0.2 and ηω = 0.1 for both algorithms and
set the batch size as M = 3000 for the VR-Greedy-GQ. We run 200k iterations for each of the 10
trajectories.

Estimated maximum Reward: In the experiments, we compute the maximum reward as follows:
When the policy parameter θt is updated to θt+1, we estimate the corresponding reward by sam-
pling a Markov decision process {s1, a1, s2, a2, . . . , sN , aN , sN +1} using πθ. Then we estimate the
expected reward using

ˆrt =

1
N

N
(cid:88)

i=1

r(si, ai, si+1).

29

Published as a conference paper at ICLR 2021

Under the ergodicity assumption, this average reward will tend to the expected reward with respected
the stationary distribution induced by πθ (Wu et al. (2020)). Then the maximum reward is deﬁned
as the maximum estimated expected reward along the training trajectory; that is,

In the experiments, we set N = 100 when estimating the expected reward.

Maximum Reward = max

t

ˆrt.

30

