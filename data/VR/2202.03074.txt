2
2
0
2

b
e
F
8

]

V
C
.
s
c
[

2
v
4
7
0
3
0
.
2
0
2
2
:
v
i
X
r
a

Imposing Temporal Consistency on Deep Monocular Body Shape and
Pose Estimation

A. Zimmer1,2, A. Hilsmann1, W. Morgenstern1, and P. Eisert1,3

1Fraunhofer HHI
2TU Berlin
3HU Berlin

Abstract

Accurate and temporally consistent modeling of human
bodies is essential for a wide range of applications, in-
cluding character animation, understanding human so-
cial behavior and AR/VR interfaces. Capturing human
motion accurately from a monocular image sequence is
still challenging and the modeling quality is strongly
inﬂuenced by the temporal consistency of the captured
body motion. Our work presents an elegant solution
for the integration of temporal constraints in the ﬁtting
process. This does not only increase temporal consis-
tency but also robustness during the optimization. In
detail, we derive parameters of a sequence of body mod-
els, representing shape and motion of a person, includ-
ing jaw poses, facial expressions, and ﬁnger poses. We
optimize these parameters over the complete image se-
quence, ﬁtting one consistent body shape while impos-
ing temporal consistency on the body motion, assuming
linear body joint trajectories over a short time. Our ap-
proach enables the derivation of realistic 3D body mod-
els from image sequences, including facial expression
and articulated hands.
In extensive experiments, we
show that our approach results in accurately estimated
body shape and motion, also for challenging movements
and poses. Further, we apply it to the special appli-
cation of sign language analysis, where accurate and
temporal consistent motion modelling is essential, and
show that the approach is well-suited for this kind of
application.

1

Introduction

Estimation of human 3D body motion and shape from
monocular image sequences and videos is a challeng-
ing task in the ﬁeld of computer vision and graphics.
There are many approaches addressing this problem,
but most of them estimate motion and shape on a
frame by frame basis without addressing temporal con-
sistency, and thus suﬀer from artifacts and inaccurate

results. There is e.g. SMPL-X [17], which uses a para-
metric human shape model that is ﬁtted to the individ-
ual frames, without explicitly accounting for temporal
consistency, and which is prone to inconsistent shape
and pose, leading to wiggling and jittering in the tem-
poral sequence. These methods ignore the information
of temporal vicinity present in a given image sequence
or video. However, this information should be used to
improve body pose and shape estimation.

The approach presented in this paper accurately esti-
mates human body motion and shape from monocular
image sequences, and adds temporal consistency con-
straints for more stable solutions. It ﬁnds a consistent
body shape for the entire image sequence and imposes
temporal consistency on the body pose estimates from
image to image. Our approach includes the estima-
tion of facial expression and ﬁnger poses, which makes
it suitable for highly detailed body motion estimation,
such as sign language capture.

Possible applications of body shape and pose estima-
tion lie in body motion tracking, character animation,
understanding human social behavior and AR/VR in-
terfaces in favor of the entertainment industry, robotics,
sports analytics, medical applications and many more.
Another possible application of our approach is the
generation of realistic sign language using avatars.
Currently, not all media provide additional sign lan-
guage. Therefore, human sign language interpreters are
needed. Automatically generated avatars, which are
able to realistically and human-like emulate detailed
sign language, would enable barrier-free access to all
acoustically supported media. A current state of the art
approach for generating sign language motion is the em-
ployment of a deep neural network to learn from large
amounts of sign language training data. With the re-
sults of this paper, such training data can be acquired in
large quantities by accurately estimating sign language
body motion from videos or image sequences.

We evaluated our body pose and shape estimation
approach on image sequences and videos with sim-

1

 
 
 
 
 
 
ple and complex body poses and motion,
including
sign language image sequences. Additionally, we used
state of the art motion capture sequences with ground
truth data for quantitative evaluation. The exper-
iments showed that through incorporating temporal
consistency on body shape and motion, our approach
yields more accurate and robust results than SMPLify-
X. Moreover, we showed that our approach not only
avoids jittering artifacts and inaccuracies, but also helps
to produce plausible and realistic poses and smooth
body motion.

2 State of the Art

Fitting body model parameters to a monocular image
sequence can be performed for every image individually,
or with the remaining images of the sequence as context.

2.1 Fitting a Body Model to Images

Body model ﬁtting is done on the basis of monocular
camera systems, systems with multiple cameras, with
depth sensors, or other conﬁgurations. The challenging
task of ﬁtting a body model to a single image without
further depth information is discussed in the following.
In order to capture whole body motions, a detailed
representation of the human body is needed, such as
meshes or parametric body models. Parameters of the
body model can be ﬁtted using keypoints of the image,
like joint locations. SMPLify-X [17] ﬁnds body shape
and pose with articulated hands and facial expression
for people pictured on a single RGB image. With Open-
Pose [3], 2D joint locations of the imaged people are es-
timated, to which the parameters of the camera and the
body model SMPL-X [17] are ﬁtted. After an initial-
ization phase for the camera parameters, the ﬁtting is
realized by projecting the model joints onto the image
plane according to current camera properties and by
minimizing the distance between projected joints and
detected keypoints. At the same time, the deviation
from the neutral state is penalized with pose and shape
priors. An interpenetration penalty is applied to sup-
press body parts intersecting each other. SMPLify-X’s
pose prior VPoser is realized as a variational autoen-
coder, which translates the body pose parameters into
a normally distributed latent space. It is trained for the
body model SMPL-X on a database of roughly one mil-
lion recordings of common and uncommon body poses.
SMPLify-X is the enhancement of SMPLify [1] from the
body model SMPL [14] to SMPL-X, which in contrast
to SMPL includes articulated hands and facial expres-
sion. The research group of SMPLify-X thereafter pub-
lished ExPose [4], a body shape and pose estimation
approach which achieves similar results to SMPLify-
X, but is 200 times as fast [4]. SMPL-X parameters

are regressed directly, similarly learned as in Human
Mesh Recovery (HMR) [10], on a new SMPL-X dataset.
The parameters for hands and facial expression are re-
gressed separately by a network using the respective
image parts in high resolution. Finally, the two param-
eter sets are merged and ﬁne-tuned.

Monocular full body capture in real-time is enabled
by [29], which uses a neural network architecture con-
sisting of residual, convolutional and fully connected
modules and employs a combination of 2D and 3D key-
point positions and cropped parts of the input image
for the face and hands as intermediate representation
of the body. Inter-part correlations of hands and high-
level body features are used to ﬁnetune hand and ﬁnger
keypoint detection. Additionally to facial expression
estimation, the cropped image is warped and projected
onto the SMPL+H body model [19]. Alternatively, a
feature pyramid with diﬀerent image scales can be cre-
ated from the input image to derive the shape of the
depicted human. This is done by [26], employing an
encoder which takes the image and produces a feature
pyramid, supervised by pixel-wise image segmentation
into body parts and background, on the highest reso-
lution feature. Then, there is a feedback loop over the
feature pyramid, inside a regression network. There,
model parameters of the body model SMPL [14] are
estimated to mesh-image-alignment from features of in-
creasing resolution. Due to the usage of SMPL as un-
derlying body model, no facial expression or ﬁnger poses
are estimated.

Two prevailing methods for the stated problem are
HMR [10] and Unite the People [13], which both do not
include articulated hands or facial expression. HMR
learns a mapping from image pixels directly to SMPL
parameters, from 2D keypoint annotated images and a
large database of 3D human motion and shape, with-
out paired data. It minimizes the reprojection loss of
keypoints and uses a discriminator network. In Unite
the People, a database of paired data is generated with
SMPLify [1] to train a direct regression method.

There are approaches for specialized problems, such
as jointly identifying a person and the object they are
interacting with [27], or specifying on low-resolution
images [25]. Camera systems with alternative captur-
ing types, like polarized 2D images [30], images with a
depth channel [31], or images from an event-based cam-
era [24] can be used to improve diﬀerent aspects of body
model parameter ﬁtting based on individual images.

2.2 Fitting a Body Model to Image Se-

quences

Analyzing an image sequence as a whole yields informa-
tion about the motion, rather than the poses individ-
ually and thus about the temporal connection between

2

the estimated poses. The Video Inference for Human
Body Pose and Shape Estimation (VIBE) approach [11]
is based on using a motion discriminator for performing
body shape and pose estimation on videos or image se-
quences. Because SMPL [14] is used as the underlying
body model, it does not include articulated hands or
facial expression. VIBE exploits the large-scale motion
capture dataset AMASS [15] and in-the-wild, 2D key-
point annotated videos to train a motion discriminator
model and estimate SMPL parameters. The motion dis-
criminator is trained like a generative adversarial net-
work, with two separate neural networks.

Similar to our approach, Monocular Total Capture
(MTC) [23] captures body shape and pose, including
expressive hands and face, of a person depicted in a
monocular image sequence, represented by a 3D de-
formable mesh body model and uses one shared shape
vector for the whole image sequence. For every im-
age, the joint conﬁdence maps and 3D orientation of
body parts are estimated with a Convolutional Neural
Network (CNN), which are described as 3D Part Orien-
tation Fields (POFs). The POFs are used to optimize
the parameters of the body model ADAM [9], employ-
ing the pose and shape prior from the CMU Graphics
Lab Motion Capture Database [8]. For temporal con-
sistency, a texture-based tracking method is utilized,
using optical ﬂow. It reﬁnes the body model parame-
ters based on the previous image, its parameters, and
the parameters of the current image.

An approach concentrating on articulated hands on
monocular videos is FrankMocap [20]. The poses and
shapes of the hands are estimated separately from the
body and are merged afterwards, as in ExPose. Body
pose and hands are both modelled with SMPL-X, thus
FrankMocap includes expressive face and articulated
hands. Switching between methods to iteratively es-
timate shape and pose in an ongoing loop is an alterna-
tive approach for the given problem. SPIN [12] switches
between the deep regression network of HMR and an it-
erative optimization routine, using SMPLify [1], to im-
prove the accuracy of body shape and motion estima-
tion. Due to choosing SMPL as the body model, SPIN
does not include expressive face or articulated hands.

For temporally consistent capturing of the human
body pose and shape including clothes and texture,
[2] uses a cascaded network architecture of image en-
coders and Multi-Layer Perceptrons (MLPs). First, a
Temporal Voxel Regression Network (TVRN) produces
temporally consistent voxel occupancy grids for every
frame. Then, this is used by a geometry prediction net-
work (H3DN) to reﬁne the surface 3D reconstruction,
by acquiring a multi-scale shape encoding. Afterwards,
a texture prediction network (H3DTexN) is used to pre-
dict a color value for each surface vertex. Using a modi-
ﬁed residual neural network (ResNet), pixel-wise image

features are acquired, which are processed in an MLP
decoder for every frame but with shared parameters,
again acquiring a multi-scale shape encoding. This pro-
duces a textured 3D surface that captures ﬁnger pose
and facial expression, but not in great detail.

To capture humans in challenging poses and motion,
ChallenCap [7] uses a learning-and-optimization frame-
work, which learns motion characteristics and is trained
on “a new challenging human motion dataset with both
unsynchronized marker-based and light-weight multi-
image references” [7]. First, a noisy skeletal motion
map is acquired, this is then processed in their tempo-
ral encoder-decoder and generation network HybridNet
and their motion discriminator, which is followed by a
robust motion optimization phase, which uses 2D key-
point and silhouette information from the video frames.
This approach needs a 3D body scanned template mesh
of the actor as input, besides the video frames, and does
not capture facial expression or ﬁnger poses.

Pure human pose estimation from image sequences
or videos, outputting only a skeleton of joints instead
of estimating the person’s body shape, is another chal-
lenging research area. A recent overview of deep learn-
ing based approaches to this task is provided by [28].
Almost 300 publications on 2D and 3D body pose es-
timation, related datasets and evaluation metrics are
In addition, re-
analyzed, discussed and compared.
maining challenges such as occlusion, computational
eﬃciency, and insuﬃcient training data are identiﬁed.
One of those publications is [18], which performs pure
3D human pose estimation and uses a fully convolu-
tional model, which consists of dilated temporal convo-
lutions over 2D keypoints to capture long-term informa-
tion. They propose to use this for applications where
labeled data is scarce to enable semi-supervised learning
on unlabeled video using a reprojection loss, similar to
this paper. [22] performs skeleton-based action recogni-
tion and adopts the possible dependencies of physically
connected and disconnected joints and the hierarchical
structure in the skeleton to better model the spatial
and temporal features of human actions.
It employs
a spatial-temporal module which contains motif-based
graph convolutional networks (GCNs), which work on
graphs having human body joints as nodes and bones
as edges, and which contain a variable temporal dense
block to encode short-, mid and long-range local infor-
mation. In the last stages of the network, a non-local
neural network module for global temporal features is
used, with the aid of an attention mechanism.

2.3 This Work’s Contribution

This work presents a method for temporally consistent
human body shape and motion estimation by ﬁtting
SMPL-X parameters to a monocular image sequence.

3

It estimates 2D joint location keypoints bottom-up and
then ﬁts SMPL-X parameters to the keypoints top-
down in an optimization framework. A consistent shape
is found by estimating one shared shape parameter vec-
tor for the whole image sequence. Following, the body
motion is estimated accurately by imposition of tempo-
ral consistency. For this purpose, we minimize objec-
tive functions, which consist of a reprojection loss term,
a number of pose and shape priors and loss terms for
temporally consistent body shape and motion.

Most similar to our approach are VIBE [11] and MTC
[23]. However, VIBE does not include expressive face
and articulated hands and MTC does not focus on
shape ﬁtting and uses a gender non-speciﬁc body model,
in contrast to our work. Therefore, our approach sur-
passes the other two in capturing people naturally and
accurately, including detailed ﬁnger movements, emo-
tional countenance, expressive gestures and a diverse
In compari-
spectrum of gender-speciﬁc body shape.
son to SMPLify-X, our approach exploits the context of
temporally neighboring frames to provide body models
which are temporally consistent in shape and motion.
This enables the derivation of realistic 3D body mod-
els from image sequences, including facial expression
and articulated hands. We present extensive experi-
ments and apply our approach to the challenging task
of sign language motion capture.

3 Temporally Consistent Shape

and Pose Fitting

Our approach takes an image sequence of a single per-
son in motion as input and generates a sequence of body
models with a consistent shape and smooth motion of
the imaged person, including facial expression and ar-
ticulated hands.

As shown in the method overview in Fig. 1, body
model parameters are ﬁtted to the image sequence in
three phases.
In phase 1, the image sequence is pro-
cessed consecutively with OpenPose [3] to obtain pre-
dicted joint locations as keypoints at image pixel posi-
tions per image. These keypoints are fed into SMPLify-
X [17] to calculate an initialization for the desired tem-
porally consistent body shape and motion (Sec. 3.1). In
phase 2, a consistent shape for the image sequence is de-
rived from the initialized body models by optimization
(Sec. 3.2). Finally, in phase 3, the SMPL-X param-
eters are optimized along the sequence for temporally
consistent body motion (Sec. 3.3).

We use SMPL-X [17] as underlying body model, be-
cause it is particularly well suited for the capturing of
expressive motion since it includes facial expression and
articulated hands. SMPL-X uses the following param-
eters: human body shape, general pose, ﬁnger poses,

jaw pose, facial expression, and global rotation of the
model. Finally, the camera center is also a parameter
of interest for our approach.

In all three phases of our optimization framework,
the respective objective function includes the objective
function from SMPLify-X, denoted by the loss term Eoi ,
which is further described in Sec. 3.1. In phase 2, ad-
ditionally, the consistent shape ﬁtting loss Eβ(cid:48) is opti-
mized, while phase 3 also includes the temporal consis-
tency loss ETi, introduced in Sec. 3.2 and 3.3 respec-
tively.

3.1 Phase 1: Initialization

We use conventional SMPLify-X results for the given
image sequence as initialization for phases 2 and 3 of
the temporally consistent shape and pose ﬁtting. We
generate keypoints for the images using OpenPose and
feed them to SMPLify-X for the estimation of an initial
body model and camera parameters. The optimization
of SMPLify-X’s objective function for each image of a
sequence I can be achieved by optimizing the following
objective function (Eq. 1).

E1(B, Θ, Ψ, λ) =

1
|I|

(cid:88)

i∈I

Eoi

(1)

The set of images used in the current optimization step
is denoted as I. From here on, all capitalized Greek
letters are matrices of dimension d × |I|, containing the
respective body model parameter vector (with dimen-
sion d) for every image in the sequence. For example B
is the matrix (β1, ..., β|I|), where βi is the shape param-
eter vector of the body model belonging to image i. Θ
is the matrix of body pose parameters for every image,
including ﬁnger pose Θh, jaw pose Θf and general body
pose Θb, while the parameters for facial expression are
described by Ψ. Finally, λ describes weights used from
SMPLify-X that steer the inﬂuence of each term inside
Eoi.

SMPLify-X’s objective function, here described with
slightly diﬀerent notation by the loss term Eoi for im-
age i, implements priors for jaw pose, hand poses, body
shape, and facial expression, denoted by Eθfi
,
, Eθhi
Eβi and Eψi respectively, which are all deﬁned as (cid:96)2-
norms. A reprojection loss, further explained below, is
used as data term EJi for image i. As body pose prior
Eθbi
, SMPLify-X’s prior VPoser is used, a deep learning
based method discussed in Sec. 2. A prior against over-
bending of knees and elbows Eαi is deployed, penaliz-
ing the deviation from 0 of those body pose parameters
exponentially. The prior ECi avoids collisions between
body parts by ﬁnding colliding faces of the mesh em-
ploying Bounding Volume Hierarchies [21] and penaliz-

4

Figure 1: Method overview with three phases, including interim results as the green blocks and optimization frameworks
as the red blocks. Latter all contain a reprojection error to OpenPose keypoints and assorted priors, consist of 5 stages
with diﬀerent weights, while phase 1 optimizes on individual frames and phase 2 and 3 on sets of frames.

As described in [17], this loss minimizes the distance
between the 2D keypoints Jest and the 2D projections
of the corresponding 3D joints Rθ(J(β))j. Rθ(·) de-
scribes a function that transforms the joints along the
kinematic chain according to the pose θ. ρ denotes a ro-
bust error function to account for noisy detections and
γj are per-joint weights for annealed optimization.

In SMPLify-X’s approach, the ﬁrst step is the initial-
ization of the depth of the camera based on the keypoint
locations for the shoulders and hips estimated for the
given image. Then, an iterative ﬁtting begins, consist-
ing of ﬁve stages in which the camera and SMPL-X
model parameters are ﬁtted to the given keypoints for
the image. In every stage, the presented objective func-
tion is optimized.

SMPLify-X’s weights λ (given in Tab. 1) are conﬁg-
ured in such a way that at the beginning of the ﬁve
stages, primarily body model parameters are strongly
regulated with priors to achieve realistic human mo-
tion. As the stages progress, priors for jaw pose and
inhibiting overbending of knees and elbows gain weight
in relation to the rest. Eventually, the weighting focus
shifts to minimizing the reprojection loss and to details
such as hand poses, facial expression, and collision pre-
vention between body parts.

For optimization of the objective function, SMPLify-
X uses the limited-memory BFGS (L-BFGS) algorithm
[16]. All body model parameters, the camera center
and the global rotation undergo optimization in this
phase. The result of this phase are temporally incon-
sistent shape and pose estimations from SMPLify-X,
which are used as initialization for the next two phases.

Figure 2:
Input im-
Illustration of reprojection loss:
age with OpenPose keypoints marked in red (left), SMPL-
X model in initial rest-pose (center) and pose and shape
adapted SMPL-X model according to the OpenPose key-
points (right).

ing these by the depth of intrusion.

Eoi (B, Θ, Ψ, λ) = λJ EJi + λΘb Eθbi

+ λΘf Eθfi

+ λΘhEθhi

+ λαEαi + λBEβi + λΨEψi + λCECi

(2)

The diﬀerent λs weigh the ﬁve optimization stages
SMPLify-X undergoes, which are explained below. Fur-
ther details on these priors are discussed in SMPLify-X
[17].

The reprojection loss (example in Fig. 2) inside the
data term EJi is computed such that for each SMPL-
X body joint projected to the image plane, a penalty
value obtained from the distance to the corresponding
OpenPose keypoint is summed up.

More precisely, the data term containing the repro-

jection loss is:

EJi(B,Θ, K, Jest) =
(cid:88)

γjωjρ(ΠK(Rθi(J(βi))j) − Jest,j)

joint j

(3)

5

Phase 1Phase 2Phase 3SMPLify-Xbody model iInitialization per framesubset of frames1 shared shape vectorShared shapeInitializationShape Fittingall framesInitializationShared shapeall framesTemporally consistent body modelsfor the whole sequenceconsistent body models for all framesTemporal Consistent Pose Fitting(with constant shape vector)frame iframe jframe iStage
λΘb body pose prior
λΘh hand pose prior
λB shape prior
λΨ expression prior
λΘf jaw pose prior 1
λΘf jaw pose prior 2
λΘf jaw pose prior 3
λα bending prior
λC collision loss
λJ hand joints
λJ face joints
λJ rest of joints

1
404
404
100
100
63.6
201
201
35.8
0
0
0
1

2
404
404
50
50
63.6
201
201
35.8
0
0
0
1

3
57.4
57.4
10
10
24
75.7
75.8
13.5
0
0
0
1

4
4.8
4.8
5
5
6.9
21.9
21.9
3.9
0.1
0.1
0
1

5
4.8
4.8
5
5
6.9
21.9
21.9
3.9
1
2
2
1

Table 1: Weights λ used by SMPLify-X and the following
phases for the diﬀerent priors and subterms in Eq. 2.

3.2 Phase 2: Shape Fitting on Subset of

Images

Our goal is to estimate one consistent body shape pa-
rameter vector for the whole image sequence. As the
optimization of a full sequence is computationally too
expensive, we optimize over a subset Isub ⊂ I. The
shape parameters are derived solely from the estimated
keypoints from OpenPose and their proportional rela-
tions.

This phase is similar to the ﬁrst phase, despite opti-
mizing one shared shape over a subset of images instead
of individual ones over all images. This is achieved by
optimizing objective function E2:

1 (Sec. 3.1). As initialization for this phase, Msub is
updated to the set ˆMsub by changing the shape param-
eter matrix B to [β(cid:48), ... , β(cid:48)] for this phase, so that
the shared shape parameter vector is used as estimated
shape for every image of the sequence. Then, β(cid:48) is ini-
tialized with the mean of the shape parameters of the
body models in Msub.

Optimization of the objective function of this phase
(Eq. 4) is performed as described in phase 1, using the
L-BFGS algorithm. The shared shape parameter vector
β(cid:48), all body model parameters except the independent
shape parameter vectors, the camera center and the
global rotation undergo optimization here.
It under-
goes ﬁve stages of optimization, with the same weights
λ as in phase 1. The weight wβ(cid:48) for the shared shape
prior Eβ(cid:48) is set to the values of the shape prior weight
λB from phase 1 (Sec. 3.1).

The subset Isub is sampled randomly from I. Alter-
native choices for Isub are possible, but taking random
images was found to be suﬃcient, because in the case
of inaccurate keypoints in a sub-sequence of the image
sequence the random sample most likely also contains
keypoints for diﬀerent parts of the image sequence. An-
other possibility is to choose the |Isub| images with the
highest average keypoint conﬁdence score. To obtain
complete body shape information, such a set of images
could alternatively be chosen such that no part of the
body remains hidden in the course of the images.

The result of the shared shape parameter β(cid:48) is saved
and is set constant for phase 3 which estimates a smooth
motion over time (Sec. 3.3).

E2(B, Θ, Ψ, λ) = Eβ(cid:48) +

1
|Isub|

(cid:88)

Eoi

i∈Isub

(4)

3.3 Phase 3: Temporal Consistency on

all Images

As the SMPLify-X loss term Eoi and the variables are
the same as explained for objective function E1 of phase
1 (Eq. 1) and β(cid:48) describes the shared shape parameter
vector for the whole image sequence, a shape prior Eβ(cid:48)
for the consistent shape is employed, which regularizes
the shared shape parameter vector to a neutral shape
and is realized as a weighted (cid:96)2-norm:

Eβ(cid:48) (β(cid:48)) = wβ(cid:48) ·

n
(cid:88)

i=1

x2
i

(5)

with β(cid:48) = [x1, ..., xn](cid:62) and wβ(cid:48) being the shape prior
weight for the shared shape parameter vector.

As initialization for this phase’s optimization step,
the results from phase 1 (Sec. 3.1) is used, but the indi-
vidual shape vectors per image is replaced by the shared
shape parameter vector β(cid:48) as estimated shape for every
image of the sequence. This shared shape vector is ini-
tialized with the mean of the neglected shape vectors.
More formally put: Let Msub be the set of body mod-
els for the images in Isub, which were found in phase

The goal of this phase is to estimate camera and
body pose parameters for the entire input image se-
quence, which describe temporally consistent body mo-
tion, while the shape parameters are set to the consis-
tent shape β(cid:48) for every image i, which was found in
phase 2 (Sec. 3.2), and are not considered for optimiza-
tion anymore. Therefore, the objective function E3 of
this phase includes a temporal consistency loss ETi, the
SMPLify-X loss term Eoi and the variables as explained
for objective function E1 of phase 1 (Eq. 1):

E3(B, Θ, Ψ, λ) =

1
|I|

(cid:88)

i∈I

(cid:0)Eoi + ETi

(cid:1)

(6)

For smooth body motion all body parts follow a
steady individual path over the course of the image se-
quence. Therefore, the underlying assumption used in
this phase is that the 3D position of every body part
is close to the mean position of that body part of the
temporally neighboring frames. Fig. 3 illustrates this
approach, where the mean position of the elbow joint

6

in the neighboring frames is identical to the position in
frame i, hence the temporal consistency loss is 0 for this
very frame and joint.

3D joint positions of the current shaped and posed body
model are set against the mean 3D position of the joints
of the frames in the temporal neighborhood.

All body models are initialized with the pose esti-
mates of phase 1 (Sec. 3.1) while the shape parameter
vectors are set to the consistent shape β(cid:48) found in phase
2 (Sec. 3.2). The weights λ inside Eoi for the ﬁve stages
of optimization are set as described in phase 1 (Sec. 3.1).
The body model parameters for pose, the global rota-
tion and the camera center for each image i undergo
optimization.

Instead of averaging the joint positions over time for a
measure of temporal consistency, other approaches are
possible, like averaging the joint angles or averaging the
positions over more than two frames.

Optimizing the objective function of this phase
(Eq. 6) in the ﬁve-stage manner from phase 1 and 2 for
the entire potentially long image sequence at once, is
computationally complex for common optimizers. To
eﬃciently approximate this, a Sliding Window Algo-
rithm is introduced, which is described in the following.

Optimization with the Sliding Window Algo-
rithm: The presented objective function of phase 3
(Eq. 6) ensures temporal consistency of body poses for
a complete image sequence. The increasing number of
parameters undergoing optimization for longer image
sequences leads to a drastic increase in computational
complexity for common optimizers, which increases the
runtime. Therefore, this section presents a method ap-
proximating the optimum, using a sliding window ap-
proach. Instead of optimizing the body model param-
eters of all images in a given image sequence at once,
it optimizes the body model parameters of all subsets
of consecutive images of a given window size n, start-
ing at the ﬁrst n images, iterating through the image
sequence.

In each iteration the presented ﬁve-stage optimiza-
tion is performed on these n images, optimizing their
body model parameters, minimizing the objective func-
tion E3 (Eq. 6). Since the temporal consistency term
of E3 for a frame uses the mean joint position of the
temporally neighboring frames, the calculation of this
term for the last frame in each window would also use
the frame following the window and the associated body
model. However, because this body model is not opti-
mized until the next iteration, only the rough initializa-
tion from phase 1 would be used for this optimization.
Therefore, the temporal consistency term is not calcu-
lated for the last frame in the window, because this
would lead to more inaccurate results. An illustration
is given in Fig. 4.

The result for image i of the present iteration is ﬁnal if
this was the last iteration in which the image was inside
the sliding window. Otherwise it functions as a good

Figure 3: Temporal consistency calculation (Eq. 7) for
frame i, for the elbow joint marked in orange and the tem-
poral mean position of the joint in red.

The term ETi describes the temporal consistency loss
for the body model corresponding to image i, by cal-
culating the distance between 3D joint positions of the
body model of the current frame and of the mean of the
temporal predecessor, current and successor frames.

ETi(β(cid:48), Θ) =



(cid:80)

wT

jointj



0

with

mi,j =
(cid:16)
1
3

Rθi−1

(cid:16)

(cid:0)J(β(cid:48))(cid:1)

Rθi

j − mi,j

(cid:17)2

if 2 (cid:54) i (cid:54) |I| − 1,

else,

(cid:0)J(β(cid:48))(cid:1)

j + Rθi

(cid:0)J(β(cid:48))(cid:1)

j + Rθi+1

(cid:0)J(β(cid:48))(cid:1)

(7)

(cid:17)

j
(8)

The ﬁrst case of Eq. 7 deﬁnes the temporal consistency
loss term for image frame 2 to the second last frame
of image sequence I. The second case ensures that, for
the ﬁrst (resp. last) frame of I the temporal consistency
loss-value is 0, which is because this frame does not have
a predecessor (resp. successor) frame. Following the no-
tation of [17], Rθi(J(β(cid:48)))j is the 3D position of joint j
of the posed and shaped body model belonging to im-
age i, according pose parameter vector θi and shared
shape parameter vector β(cid:48). Therefore, mi,j is the mean
position of joint j over the neighboring images of im-
age i. To tune to what extent temporal consistency is
imposed onto the body model sequence, the temporal
consistency weight wT can be used. ETi partially re-
sembles the data term EJi from SMPLify-X, where the
2D joint positions of the current shaped and posed body
model are set against the 2D keypoints, while here, the

7

called subjects from here on.

4.1 Setting

Architecture: All following computations were per-
formed on a system with two 20 core Xeon CPUs,
128GB RAM and four NVIDIA RTX 2080 GPUs (12GB
VRAM each). OpenPose was executed in parallel on all
four GPUs, while SMPLify-X and the algorithms devel-
oped in our work were not, which enabled processing
four image sequences at the same time.

Datasets: To evaluate our approach,

its perfor-
mance was analyzed and compared on databases from
two sources. The ﬁrst database is our own image se-
quence, consisting of 722 images, from here on called
own-data. In this, the subject uses sign language and
is captured from the front.

The second database consists of videos from MoVi
[6], which were processed to image sequences. MoVi is
a large marker-based motion capture dataset on videos
with actors wearing natural or minimal clothing and
performing various body motions. The body model and
camera parameters found by MoVi were converted to
SMPL+H [19] and additional soft-tissue deformations
parameters by AMASS [15]. Therefore this database is
well suited for quantitative evaluation in this paper.

From MoVi, we chose six subjects for evaluation and
the corresponding body models from AMASS are used
as ground truth, meaning these models are assumed to
be a good representation of the real subjects and their
poses and serve as reference for the evaluation phase.
The three subjects 17, 1 and 32 have average body vol-
ume, while subjects 41 and 16 have higher and sub-
ject 45 has lower body volume than average.
In the
videos of the motion capture dataset, all subjects are
depicted from a slight side view and perform the same
21 body motions. Two of these motions were selected
for further quantitative and qualitative evaluation in
our work. Hand waving was chosen as a simple motion
for body shape and pose estimation, which is from here
on referred to as Simple Motion, and sitting down cross-
legged and standing up again as a diﬃcult one, which is
from here on called Complex Motion. The MoVi video
sequences were processed to image sequences with the
original framerate of 25 images per second, which re-
sults in an individual number of frames for every sub-
ject and motion, given in Table 2. In some instances
for these subjects, MoVi starts the motion capture pro-
cess slightly later inside the video or stops it slightly
earlier than our work started and ended the image se-
quences, thereby some frames at the end or beginning
of the image sequence have no corresponding ground
truth result.

All following experiments use the parameter settings

given in Table 3, unless otherwise speciﬁed.

Figure 4: Procedure of the Sliding Window Algorithm over
image sequence of 9 frames (marked in gray) with window
size 4. The frames underlined in blue denote the frames
inside the sliding window, which currently get optimized,
while the orange underlined frames are not optimized (body
model parameters set to constant) but inﬂuence the opti-
mization, because the temporal loss term (Eq. 7) is calcu-
lated for all frames in the window except the last.

initialization for the upcoming iteration. A smaller win-
dow size leads to a less accurate approximation of the
objective function (Eq. 6). The temporally consistent
motion enforced in this phase 3, together with the uni-
ﬁed shape in phase 2, lead to a temporally consistent
body shape and motion sequence by optimizing the ob-
jective function E3 (Eq. 6) with the presented Sliding
Window Algorithm. The performance of the presented
approach is analyzed and compared to SMPLify-X in
the following section by qualitative and quantitative
evaluation.

4 Evaluation

In the following, it is analyzed if the application of tem-
porally consistent shape and pose ﬁtting to an image
sequence as described in Sec. 3 produces an accurately
ﬁtted body model per image and smooth estimated mo-
tion, including expressive face and hands.

This is evaluated qualitatively by projecting the es-
timated body models onto the images according to the
estimated camera parameters. These projections are
compared to the original images and to the projec-
tions of the body models which were found by ap-
plying SMPLify-X [17] for every image. Quantitative
evaluation includes body shape evaluation by compar-
ing volumes of the estimated body models to volumes
of ground truth meshes, which were captured using a
marker-based motion capture system. To furthermore
measure the performance of pose estimation, the mean
vertex-to-surface error to the ground truth meshes is
analyzed.

People shown in image sequences, whose body pose
and shape estimates are evaluated in this section, are

8

Motion sequence

Simple Motion
subject 1
subject 16
subject 17
subject 32
subject 41
subject 45
Complex Motion
subject 1
subject 16
subject 17
subject 32
subject 41
subject 45

Total number
of frames

Number of
frames with
ground truth

50
75
75
100
100
75

225
200
175
250
219
177

50
75
75
85
97
62

222
182
162
247
219
175

Table 2: The number of frames for every image sequence
taken from the complete MoVi video of a subject. In some
instances ground truth data was not available for frames
from the beginning or end of a sequence.

Setting
temporal consistency weight
shape sample size
window size

Value
100
15
7

Table 3: Common parameter settings of subsequent exper-
iments.

(a) Original frames

(b) SMPLify-X results

(c) Our results

4.2 Shape Fitting Evaluation

(d) VIBE results

In this section, the shape ﬁtting results of our approach
are evaluated and compared to SMPLify-X [17] and
VIBE [11].

4.2.1 Qualitative Analysis

Fig. 5, compares our approach to SMPLify-X as well as
VIBE for a sequence which captures complex hand mo-
tion during sign language. While our approach achieves
a consistent body shape, shape and pose estimated by
SMPLify-X changes signiﬁcantly, resulting in an inac-
curate body shape. For the sake of completeness, we
also show VIBE results here, but note that the compar-
ison lacks as VIBE dose not include hands as well as
expressive face. Hence, as expected, VIBE is not able
to capture the complex hand motions depicted in the
example frames.

Similarly for both Simple and Complex Motion, as
they were deﬁned in Sec. 4.1, our work derives more
accurate estimates than SMPLify-X. This becomes ap-
parent e.g.
in Fig. 6, which shows subject 1 in the
MoVi dataset performing Complex Motion, the ground
truth body model, which was captured with AMASS

Figure 5: Shape and pose estimation results for an image
sequence of own-data with sign language. SMPLify-X cap-
tures inconsistent body shape, severely disﬁgured arm poses
and wobbling legs, while our work estimates the shape and
motion stable and close to reality. VIBE is temporally stable
but is not able to capture the complex hand motion.

and the body model estimates from our work and from
SMPLify-X. It can be seen that SMPLify-X estimates a
body shape and camera center resulting in a distorted
body model being over 2m high in chrouching pose and
more than 2.5 times as high as the ground truth model,
while our work’s method captures the body shape and
pose accurately. Moreover, in Fig. 6, one can see that
the distance to the camera also remains consistent in
time in our approach. This can be attributed to the
forced shape consistency together with the keypoint re-
projection.

Empirically, we found that for random sampling, a
sample size |Isub| of 15 is suﬃcient to ﬁnd an accu-
rately ﬁtted shape, without being aﬀected too much by
inaccurate keypoints. For increasingly complex motion

9

Figure 6: The body model estimation meshes for the orig-
inal frame on the left are shown on the right. The left body
model shows the ground truth model, the central model
shows the nearly accurate model estimate by our work’s
approach, while SMPLify-X’s estimate can be seen on the
right, having a completely distorted body shape.

with respect to occlusion, speed of motion, and so on, a
higher sample size is recommended and for faster opti-
mization a lower one. An alternative sampling strategy
is to choose the |Isub| images with the highest aver-
age keypoint conﬁdence score, which improves results
compared to random sampling with the same sample
size. The improvement slope with higher sampling size
of this strategy also decreases, because the newly added
images have lower conﬁdence scores than the ones al-
ready in use, and are thus less helpful in ﬁtting the body
shape.

Our approach estimates one consistent and accurate
shape for the whole image sequence, while SMPLify-
X ﬁnds unrealistic shape changes within one motion
sequence.

4.2.2 Volume Analysis

The estimated shape is evaluated quantitatively by vol-
ume to volume comparison and with the mean volume
error for an image sequence between the resulting body
models of the method and the ground truth. The mean
volume error for an image sequence is deﬁned as the
average of the per frame absolute diﬀerence between
the volume of the estimated body model and the corre-
sponding ground truth AMASS model.

The measured mean volume errors for our method
and SMPLify-X are shown in Fig. 7 for the Simple Mo-
tion sequences of the MoVi dataset.
It can be seen
that our method achieves a lower mean volume error
than SMPLify-X for Simple Motion for all subjects.
For Complex Motion, which is presented in Fig. 8,
our work’s approach produces consistent low mean vol-
ume errors, while SMPLify-X has extreme outliers, thus
leading to severe deviations in estimated body shape.
The data shows that our work achieves drastically bet-
ter shape estimates than SMPLify-X.

In Fig. 12, the volume distribution over all frames
of four subjects performing Complex Motion compared
between our work, SMPLify-X, and the ground truth,

Figure 7: Mean volume error for Simple Motion, subjects
from MoVi.

Figure 8: Mean volume error for Complex Motion, subjects
from MoVi.

is depicted. SMPLify-X produces body models with
big deviations from ground truth and with the most
inaccurate volumes in the ﬁrst third and towards the
end of the image sequence. One frame corresponding
to the ﬁrst peak for Complex Motion of subject 45 is
shown in Fig. 9 and one frame corresponding to the sec-
ond peak in Fig. 6. Analysis of these frames with high
SMPLify-X mean volume error reveals that this is the
moment where the subjects put one hand on the ﬂoor
to sit down. The other peak in the volume results for
Complex Motion occurs at the moment where, while
standing up, the last hand loses contact to the ground.
Fig. 12 shows that our approach is in turn able to ac-
curately and stably ﬁt the body shape throughout the
sequence, unaﬀected by these complex body poses. The
reasoning behind some ﬁgures not having data for a few
frames at the beginning or end of an image sequence,

10

was given in Sec. 4.1.

Because our approach calculates one shape for a
whole image sequence and only allows changes of the
pose for the individual frames, it is guaranteed that the
shape is consistent over time and still adapted to the
subject. In contrast to this, SMPLify-X calculates an
individual body shape for each frame, which is inde-
pendent of predecessor and successor frames and thus
varies drastically from frame to frame in some instances,
which became evident in this section’s results.

4.3 Temporal Consistency Evaluation

To evaluate the temporal consistency performance of
our work, compared to SMPLify-X, quantitative and
qualitative measures are applied in the following.

4.3.1 Qualitative Analysis

The introduction of temporal consistency improves re-
sults for image sequences containing subjects perform-
ing common and uncommon poses. Fig. 9 shows a frame
of the Complex Motion image sequence where a woman
sits down on the ﬂoor (left) and the estimated body
model from SMPLify-X (center) and our work’s ap-
proach (right).
Introducing temporal consistency the
derived body model by our work captures the unusual
body pose much better than SMPLify-X, which pro-
duces a body model with a distorted body pose. The
body model in Fig. 9b is displayed darker than the one
by our work, because SMPLify-X positioned the camera
center and artiﬁcial light source inaccurately far away
from the body model.

(a) Original frame

(b) SMPLify-X

(c) Our approach

Figure 9: Subject 45 from MoVi performing Complex Mo-
tion. SMPLify-X (b) is not able to capture the correct pose,
but our work (c) estimates a nearly accurate body pose.

The video V1 in the Electronic Supplementary Ma-
terial, shows the whole own-data sequence as well
as results produced with our approach compared to
SMPLify-X as well as VIBE. The ﬁrst half of the video
shows general hand movements and the second half

11

shows sign language motion. Overall, our approach
is able to prevent the jittering of motion and shape
changes generated by SMPLify-X. Furthermore, at the
beginning, the hand movement estimates look simi-
larly accurate; detailed results can be seen in Fig. 5.
The results show that our approach prohibits extremely
changing arm poses found in SMPLify-X, which do
not describe the depicted motion, up to having both
arms behind the head as well as wobbling of the legs
noticeable for SMPLify-X. For complex hand motions
of sign language, the impact of implementing tempo-
ral and shape consistency becomes very clear, since
our approach is able to produce consistent results, but
SMPLify-X fails to consistently produce accurate re-
sults.

The poses of hands and ﬁngers are estimated rather
accurately by both SMPLify-X and the presented ap-
proach, which also becomes apparent in Fig. 5 and the
referred video. In some cases the image based estima-
tion captures hand and ﬁnger poses more accurately
than the marker-based motion capture ground truth.
Fig. 6 shows this for the right hand of the subject. This
is due to the subjects having had several markers on the
hands, but not on the ﬁngers. In contrast to SMPLify-
X and our aproach, VIBE is not able to capture the
complex hand motion.

Performance evaluation of facial expression and jaw
pose estimation poses problems, because the face usu-
ally only takes up a small part of the image and without
high resolution it holds insuﬃcient information for ac-
curate keypoint and following body model estimation.
Nonetheless, qualitative performance evaluation of jaw
pose estimation revealed that neither SMPLify-X nor
our work is able to reliably capture jaw pose correctly.
In Fig. 5 the closed jaw and facial expression was cap-
tured correctly, but often subjects with closed jaw are
estimated by both approaches with an slightly open
mouth, as can be seen intermittently in the second half
of the referred video. Thus, both approaches capture
hand and ﬁnger pose accurately, but are not able to
reliably capture the jaw pose as well.

The choice of the window size is a trade-oﬀ: An in-
creasing window size leads to more accurate results, but
also to a higher runtime, so it has to be chosen bal-
ancing those two goals. Nevertheless, as motion is not
strongly correlated after a larger number of frames, ex-
periments showed that even small window sizes of 7 or
9 are suﬃcient in order to produce stable and robust
results. Fig. 10 shows example frames of Subject 16
from MoVi performing the complex motion of standing
up from a cross-legged position. While a window size of
3 is not suﬃcient in order to approximate the objective
function, a window size of 7 yields stable and robust
results. Larger window sizes, however did not lead to a
signiﬁcant increase of quality in our experiments.

Besides image and window size, the runtime depends
on the complexity of the body motion. Hence, reported
runtimes are only example numbers for orientation. On
a PC as described in Sec. 4.1, the mean runtime was
0.154h per completed window, for images with size 720
x 960 pixels and window size of 7. For larger images
of size 1920 x 1080, and window size 7, the mean run-
time per window was 0.232h. Furthermore, runtime
increases with more complex depicted body motion.

Imposition of temporal consistency could be seen to
enable improvements compared to SMPLify-X for usual
and unusual body poses and to reduce the wobbling of
estimated shape and motion.

4.3.2 Vertex-to-surface Error

For evaluation of the accuracy of the estimated body
motion by SMPLify-X and our work’s approach, the
vertex-to-surface error to the ground truth meshes is
compared. This error metric is computed for one body
model to another with the iterative closest point al-
gorithm, which translates and rotates the ﬁrst mesh
in such a way, that the distance between the meshes
is minimized. This distance is calculated AABB-Tree
based for every vertex of the ﬁrst mesh to the closest
point on the faces of the second mesh [5]. The average
of all these distances is used as a measure of similarity
between these body models.

The vertex-to-surface error results for our work and
SMPLify-X to the ground truth meshes can be seen in
Fig. 11a for Simple and in Fig. 11b for Complex Mo-
tion. In 11 out of the 12 sequences our work’s approach
produces a lower vertex-to-surface error than SMPLify-
X.

For further analysis of the vertex-to-surface error re-
sults, the mean and standard deviation for both ap-
proaches are given in Table 4. Mean and standard de-
viation of the vertex-to-surface error for both motions
are signiﬁcantly lower for our work’s method than for
SMPLify-X. The estimation of the Simple Motion se-
quences undergoes improvement, but those of the Com-
plex Motion sequences are especially strongly reﬁned.
This shows a strong and comprehensive improvement
in pose estimation by introducing shape and temporal
consistency.

The results of the evaluation section are discussed in

the following.

5 Discussion

In this work, temporal and shape consistency was
imposed on body shape and motion estimation from
monocular image sequences. This was achieved in a
three-phased optimization framework, based on the ap-
proach presented in SMPLify-X [17]. Quantitative and

Simple Motion
our work
SMPLify-X

mean
2.59 cm
3.36 cm

standard deviation
0.81 cm
0.91 cm

Complex Motion
our work
SMPLify-X

mean
5.04 cm
7.83 cm

standard deviation
2.52 cm
6.62 cm

Table 4: Mean and standard deviation of the vertex-to-
surface error for both methods and motions.

qualitative analysis of body model estimation revealed
that our approach improves results in comparison to
SMPLify-X. By imposing temporal and shape consis-
tency, the body shape is derived more accurately and
the body pose estimation is improved signiﬁcantly.

We showed that the body shape of a depicted sub-
ject can be estimated accurately with the optimization
in the shape ﬁtting phase (Sec. 3.2). Quantitative eval-
uation results showed that our approach brings more
accurate shape estimates than SMPLify-X, not only for
simple, but also for complex body motion and poses.
However, a deviation of over 30dm3 was detected for
subject 41 (see Fig. 7), who was dressed completely in
black. To address this, further work could research de-
riving the body shape from the body silhouette, which
could be found using image segmentation.

Evaluation of the imposition of temporal consistency
on body motion, in addition to shape consistency, re-
vealed that our approach enables accurate derivation of
complex body poses and partially self-occluded bodies.
It was able to resolve the wobbling of shape and motion
which was seen for SMPLify-X’s results. Instead, our
approach resulted in smooth body motion, which was
also quantitatively shown to be closer to the ground
truth meshes.

In addition, we discovered that imposing shape and
temporal consistency together leads to a temporally
consistent behaviour of the distance between body
model and camera. The resulting eﬀect is a consistent,
realistic distance between the model and the camera for
our approach, whereas SMPLify-X generates inconsis-
tent distances to the camera, including noticeable out-
liers.

6 Conclusion

We applied our approach for accurate estimation of
body shape and motion to challenging tasks, such as
estimation of intricate body motion or sign language
capture, where correct timing and smooth motion are
extremely important for both expressiveness as well as
acceptance. Based on quantitative and qualitative anal-
ysis, it can be concluded that imposing shape and tem-

12

poral consistency signiﬁcantly improves the accuracy of
estimated body models. Therefore, it is e.g. suitable for
the task of generating training data for realistic sign lan-
guage reproduction from real image sequences or videos.
This enables sign language support for arbitrary videos
using artiﬁcial avatars, allowing a more inclusive ac-
cess to many forms of media. Accurate body shape
and motion estimation can furthermore be used in a
variety of other contexts, like understanding of human
social behavior or body motion tracking for the enter-
tainment industry, robotics, sports analytics, medical
applications and many more.

As could be seen in some qualitative and quantita-
tive evaluation, SMPLify-X is able to derive an accu-
rate body shape and pose from images in many cases
but shows inaccuracies for some scenarios and a gen-
eral jittering of body motion, since it processes every
frame without the rest of the image sequence as con-
text. These inaccuracies and jittering were dissolved to
a great extent by the approach presented in this paper.
The introduction of temporal and shape consistency to
body pose estimation thus enables capturing motion re-
alistically steady and smooth.

7 Acknowledgements

This work has partly been funded by the European
Union’s Horizon 2020 research and innovation pro-
gramme under agreement No 952147 (Invictus) as well
as the German Federal Ministry of Education and Re-
search (BMBF) through the Research Program MoDL
under Contract no. 01 IS 20044.

The employed data set MoVi is available under [6],
the data set own-data and the code produced for this
paper are not publicly available. There are no com-
peting interests and no further acknowledgements re-
garding this publication. All authors have contributed
suﬃciently to the scientiﬁc work.

References

[1] F. Bogo, A. Kanazawa, C. Lassner, P. V. Gehler,
J. Romero, and M. J. Black. Keep it SMPL: au-
tomatic estimation of 3d human pose and shape
from a single image. In B. Leibe, J. Matas, N. Sebe,
and M. Welling, editors, Computer Vision - ECCV
2016 - 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceed-
ings, Part V, volume 9909 of Lecture Notes in
Computer Science, pages 561–578. Springer, 2016.

[2] A. Caliskan, A. Mustafa, and A. Hilton. Temporal
consistency loss for high resolution textured and
clothed 3dhuman reconstruction from monocular
video. CoRR, abs/2104.09259, 2021.

[3] Z. Cao, G. Hidalgo, T. Simon, S. Wei, and
Y. Sheikh. Openpose: Realtime multi-person 2d
pose estimation using part aﬃnity ﬁelds. CoRR,
abs/1812.08008, 2018.

[4] V. Choutas, G. Pavlakos, T. Bolkart, D. Tzionas,
and M. J. Black. Monocular expressive body re-
gression through body-driven attention. CoRR,
abs/2008.09062, 2020.

[5] M. Fournier, J. Dischler, and D. Bechmann. 3d
distance transform adaptive ﬁltering for smooth-
ing and denoising triangle meshes. In Y. T. Lee,
S. M. H. Shamsuddin, D. Gutierrez, and N. M.
Suaib, editors, Proceedings of the 4th International
Conference on Computer Graphics and Interac-
tive Techniques in Australasia and Southeast Asia
2006, Kuala Lumpur, Malaysia, November 29 -
December 2, 2006, pages 407–416. ACM, 2006.

[6] S. Ghorbani, K. Mahdaviani, A. Thaler, K. P.
K¨ording, D. J. Cook, G. Blohm, and N. F. Troje.
Movi: A large multipurpose motion and video
dataset. CoRR, abs/2003.01888, 2020.

[7] Y. He, A. Pang, X. Chen, H. Liang, M. Wu, Y. Ma,
and L. Xu. Challencap: Monocular 3d capture
of challenging human performances using multi-
modal references. CoRR, abs/2103.06747, 2021.

[8] J. K. Hodgins.

capture database.
resources.php.

lab motion
Cmu graphics
http://mocap.cs.cmu.edu/

[9] H. Joo, T. Simon, and Y. Sheikh. Total cap-
ture: A 3d deformation model for tracking faces,
hands, and bodies. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR
2018, Salt Lake City, UT, USA, June 18-22, 2018,
pages 8320–8329. IEEE Computer Society, 2018.

[10] A. Kanazawa, M. J. Black, D. W. Jacobs, and
J. Malik. End-to-end recovery of human shape and
pose. In 2018 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2018, Salt
Lake City, UT, USA, June 18-22, 2018, pages
7122–7131. IEEE Computer Society, 2018.

[11] M. Kocabas, N. Athanasiou, and M. J. Black.
VIBE: video inference for human body pose and
shape estimation. CoRR, abs/1912.05656, 2019.

[12] N. Kolotouros, G. Pavlakos, M. J. Black, and
K. Daniilidis.
Learning to reconstruct 3d hu-
man pose and shape via model-ﬁtting in the
loop.
In 2019 IEEE/CVF International Confer-
ence on Computer Vision, ICCV 2019, Seoul, Ko-
rea (South), October 27 - November 2, 2019, pages
2252–2261. IEEE, 2019.

[13] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J.
Black, and P. V. Gehler. Unite the people: Closing
the loop between 3d and 2d human representations.

13

Applications of Artiﬁcial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Edu-
cational Advances in Artiﬁcial Intelligence, EAAI
2019, Honolulu, Hawaii, USA, January 27 - Febru-
ary 1, 2019, pages 8989–8996. AAAI Press, 2019.

[23] D. Xiang, H. Joo, and Y. Sheikh. Monocular total
capture: Posing face, body, and hands in the wild.
In IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2019, Long Beach, CA,
USA, June 16-20, 2019, pages 10965–10974. Com-
puter Vision Foundation / IEEE, 2019.

[24] L. Xu, W. Xu, V. Golyanik, M. Habermann,
L. Fang, and C. Theobalt. Eventcap: Monocu-
lar 3d capture of high-speed human motions using
an event camera. CoRR, abs/1908.11505, 2019.

[25] X. Xu, H. Chen, F. Moreno-Noguer, L. A. Jeni,
and F. D. la Torre. 3d human pose, shape and tex-
ture from low-resolution images and videos. CoRR,
abs/2103.06498, 2021.

[26] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu,
L. Wang, and Z. Sun. 3d human pose and shape re-
gression with pyramidal mesh alignment feedback
loop. CoRR, abs/2103.16507, 2021.

[27] J. Y. Zhang, S. Pepose, H. Joo, D. Ramanan,
J. Malik, and A. Kanazawa. Perceiving 3d human-
object spatial arrangements from a single image in
the wild. CoRR, abs/2007.15649, 2020.

[28] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen,
R. Liu, J. Shen, N. Kehtarnavaz, and M. Shah.
Deep learning-based human pose estimation: A
survey. CoRR, abs/2012.13392, 2020.

[29] Y. Zhou, M. Habermann, I. Habibie, A. Tewari,
C. Theobalt, and F. Xu. Monocular real-time full
body capture with inter-part correlations. CoRR,
abs/2012.06087, 2020.

[30] S. Zou, X. Zuo, Y. Qian, S. Wang, C. Xu, M. Gong,
and L. Cheng.
3d human shape reconstruction
from a polarization image. CoRR, abs/2007.09268,
2020.

[31] X. Zuo, S. Wang, J. Zheng, W. Yu, M. Gong,
R. Yang, and L. Cheng. Sparsefusion: Dynamic
human avatar modeling from sparse RGBD images.
CoRR, abs/2006.03630, 2020.

In 2017 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017, Honolulu, HI,
USA, July 21-26, 2017, pages 4704–4713. IEEE
Computer Society, 2017.

[14] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll,
and M. J. Black. SMPL: a skinned multi-person
linear model. ACM Trans. Graph., 34(6):248:1–
248:16, 2015.

[15] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-
Moll, and M. J. Black. AMASS: archive of motion
capture as surface shapes.
In 2019 IEEE/CVF
International Conference on Computer Vision,
ICCV 2019, Seoul, Korea (South), October 27 -
November 2, 2019, pages 5441–5450. IEEE, 2019.
[16] J. Nocedal and S. J. Wright. Nonlinear equations.

In Nonlinear Equations. Springer, 2006.

[17] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart,
A. A. A. Osman, D. Tzionas, and M. J. Black.
Expressive body capture: 3d hands,
face, and
body from a single image. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019,
pages 10975–10985. Computer Vision Foundation
/ IEEE, 2019.

[18] D. Pavllo, C. Feichtenhofer, D. Grangier, and
M. Auli. 3d human pose estimation in video with
temporal convolutions and semi-supervised train-
ing. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach,
CA, USA, June 16-20, 2019, pages 7753–7762.
Computer Vision Foundation / IEEE, 2019.
[19] J. Romero, D. Tzionas, and M. J. Black. Embodied
hands: modeling and capturing hands and bodies
together. ACM Trans. Graph., 36(6):245:1–245:17,
2017.

[20] Y. Rong, T. Shiratori, and H. Joo. Frankmo-
cap: Fast monocular 3d hand and body motion
capture by regression and integration. CoRR,
abs/2008.08324, 2020.

[21] M. Teschner, S. Kimmerle, B. Heidelberger,
G. Zachmann, L. Raghupathi, A. Fuhrmann,
M. Cani, F. Faure, N. Magnenat-Thalmann,
W. Straßer, and P. Volino. Collision detection for
deformable objects.
In C. Schlick and W. Pur-
gathofer, editors, 25th Annual Conference of the
European Association for Computer Graphics, Eu-
rographics 2004 - State of the Art Reports, Greno-
ble, France, August 30 - September 3, 2004. Euro-
graphics Association, 2004.

[22] Y. Wen, L. Gao, H. Fu, F. Zhang, and S. Xia.
Graph cnns with motif and variable temporal block
for skeleton-based action recognition.
In The
Thirty-Third AAAI Conference on Artiﬁcial Intel-
ligence, AAAI 2019, The Thirty-First Innovative

14

(a) Original frames

(b) Pose estimation results for window size 3

(a) The mean vertex-to-surface error for Simple Motion

(c) Pose estimation results for window size 7

Figure 10: Subject 16 from MoVi performing Complex
Motion (standing up from a cross-legged position) for win-
dow size 3 and 7. While a small window size of 3 does
not suﬃciently approximate the objective function in 6 and
leads to instable results, a slightly larger window size of 7
leads to temporally stable and robust results.

(b) The mean vertex-to-surface error for Complex Motion

Figure 11: The mean vertex-to-surface error for all six
subjects and both analyzed motions. Overall, our work’s
method achieves a lower mean vertex-to-surface error than
SMPLify-X.

15

(a) Volumes of subject 1

(b) Volumes of subject 17

(c) Volumes of subject 32

(d) Volumes of subject 41

Figure 12: Chronological sequences of body volume in
body model estimation of Complex Motion. SMPLify-X re-
sults (red) have drastic deviations while our work’s results
(blue) are close to the ground truth (yellow).

16

