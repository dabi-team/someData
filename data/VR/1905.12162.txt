VolumetricCaptureofHumanswithaSingleRGBDCameraviaSemi-ParametricLearningRohitPandey,AnastasiaTkach,ShuoranYang,PavelPidlypenskyi,JonathanTaylor,RicardoMartin-Brualla,AndreaTagliasacchi,GeorgePapandreou,PhilipDavidson,CemKeskin,ShahramIzadi,SeanFanelloGoogleInc.AbstractVolumetric(4D)performancecaptureisfundamentalforAR/VRcontentgeneration.Whereaspreviousworkin4Dperformancecapturehasshownimpressiveresultsinstudiosettings,thetechnologyisstillfarfrombeingaccessibletoatypicalconsumerwho,atbest,mightownasingleRGBDsensor.Thus,inthiswork,weproposeamethodtosynthe-sizefreeviewpointrenderingsusingasingleRGBDcamera.Thekeyinsightistoleveragepreviouslyseen“calibration”imagesofagivenusertoextrapolatewhatshouldberen-deredinanovelviewpointfromthedataavailableinthesensor.Giventhesepastobservationsfrommultipleview-points,andthecurrentRGBDimagefromaﬁxedview,weproposeanend-to-endframeworkthatfusesboththesedatasourcestogeneratenovelrenderingsoftheperformer.Wedemonstratethatthemethodcanproducehighﬁdelityim-ages,andhandleextremechangesinsubjectposeandcam-eraviewpoints.Wealsoshowthatthesystemgeneralizestoperformersnotseeninthetrainingdata.Werunexhaustiveexperimentsdemonstratingtheeffectivenessoftheproposedsemi-parametricmodel(i.e.calibrationimagesavailabletotheneuralnetwork)comparedtootherstateoftheartma-chinelearnedsolutions.Further,wecomparethemethodwithmoretraditionalpipelinesthatemploymulti-viewcap-ture.Weshowthatourframeworkisabletoachievecom-pellingresults,withsubstantiallylessinfrastructurethanpreviouslyrequired.1.IntroductionTheriseofVirtualandAugmentedRealityhasincreasedthedemandforhighquality3Dcontenttocreatecompellinguserexperienceswheretherealandvirtualworldseam-lesslyblendtogether.Objectscanningtechniquesareal-readyavailableformobiledevices[30],andtheyarealreadyintegratedwithinARexperiences[20].However,neithertheindustrialnortheresearchcommunityhaveyetbeenFigure1.Weproposeanovelformulationtosynthesizevolumet-ricrenderingsofhumanfromarbitraryviewpoints.Oursystemcombinespreviouslyseenobservationsoftheuser(calibrationim-ages)withthecurrentRGBDimage.Givenanarbitrarycamerapositionwecangenerateimagesoftheperformerhandlingdiffer-entuserposesandgeneralizingtounseensubjects.abletodevisepracticalsolutionstogeneratehighqualityvolumetricrenderingsofhumans.Atthecostofreducedphoto-realism,theindustryiscur-rentlyovercomingtheissuebyleveraging“cartoon-like”virtualavatars.Ontheotherendofthespectrum,complexcapturerigs[7,39,3]canbeusedtogenerateveryhighqualityvolumetricreconstructions.Someofthesemeth-ods[8,18]arewellestablished,andlieatthefoundationofspecialeffectsinmanyHollywoodproductions.De-spitetheirsuccess,thesesystemsrelyonhigh-end,costlyinfrastructuretoprocessthehighvolumeofdatathattheycapture.Therequiredcomputationaltimeofseveralmin-utesperframemakethemunsuitableforreal-timeapplica-tions.Anotherwaytocapturehumansistoextendreal-timenon-rigidfusionpipelines[35,23,44,45,22]tomulti-viewcapturesetups[12,36,11].However,theresultsstillsuf-ferfromdistortedgeometry,poortexturingandinaccuratelighting,makingitdifﬁculttoreachthelevelofqualityre-quiredinAR/VRapplications[36].Moreover,thesemeth-odsrelyonmulti-viewcapturerigsthatrequireseveral(≈4-8)calibratedRGBDsensors.arXiv:1905.12162v1  [cs.CV]  29 May 2019Conversely,ourgoalistomakethevolumetriccapturetech-nologyaccessiblethroughconsumerlevelhardware.Thus,inthispaper,wefocusontheproblemofsynthesizingvol-umetricrenderingsofhumans.Ourgoalistodevelopamethodthatleveragesrecentadvancesinmachinelearn-ingtogenerate4Dvideosusingaslittleinfrastructureaspossible–asingleRGBDsensor.Weshowhowasemi-parametricmodel,wherethenetworkisprovidedwithcal-ibrationimages,canbeusedtorenderanimageofanovelviewpointbyleveragingthecalibrationimagestoextrapo-latethepartialdatathesensorcanprovide.Combinedwithafullyparametricmodel,thisproducesthedesiredrender-ingfromanarbitrarycameraviewpoint;seeFig.1.Insummary,ourcontributionisanewformulationofvolu-metriccaptureofhumansthatemploysasingleRGBDsen-sor,andthatleveragesmachinelearningforimagerender-ing.Crucially,ourpipelinedoesnotrequirecomplexinfras-tructuretypicallyrequiredby4Dvideocapturesetups.Weperformexhaustivecomparisonswithmachinelearned,aswellastraditionalstate-of-the-artcapturesolutions,showinghowtheproposedsystemgeneratescompellingre-sultswithminimalinfrastructurerequirements.2.RelatedworkCapturinghumansin3Disanactiveresearchtopicinthecomputervision,graphics,andmachinelearningcommuni-ties.Wecategorizerelatedworkintothreemainareasthatarerepresentativeofthedifferenttrendsintheliterature:image-basedrendering,volumetriccapture,andmachinelearningsolutions.Imagebasedrendering.Despitetheirsuccess,mostofmethodsinthisclassdonotinferafull3Dmodel,butcannonethelessgeneraterenderingsfromnovelviewpoints.Furthermore,theunderlying3Dgeometryistypicallyaproxy,whichmeanstheycannotbeusedincombinationwithAR/VRwhereaccurate,metricreconstructionscanen-ableadditionalcapabilities.Forexample,[9,21],createimpressiverenderingsofhumansandobjects,butwithlim-itedviewpointvariation.Modernextensions[1,41]pro-duce360◦panoramas,butwithaﬁxedcameraposition.ThemethodofZitnicketal.[50]infersanunderlyinggeomet-ricmodelbypredictingproxydepthmaps,butwithasmall30◦coverage,andtherenderingheavilydegradeswhentheinterpolatedviewisfarfromtheoriginal.Extensionstothesemethods[14,4,47]haveattemptedtocircumventtheseproblemsbyintroducinganopticalﬂowstagewarpingtheﬁnalrenderingsamongdifferentviews,butwithlimitedsuccess.Volumetriccapture.Commercialvolumetricreconstruc-tionpipelinesemploycapturestudiosetupstoreachthehighestlevelofaccuracy[7,39,12,11,36].Forin-stance,thesystemusedin[7,39],employsmorethan100IR/RGBcameras,whichtheyusetoaccuratelyestimatedepth,andthenreconstruct3Dgeometry[27].Non-rigidmeshalignmentandfurtherprocessingisthenperformedtoobtainatemporallyconsistentatlasfortexturing.Roughly28minutesperframearerequiredtoobtaintheﬁnal3Dmesh.Currently,thisisthestate-of-the-artsystem,andisemployedinmanyAR/VRproductions.Othermethods[51,35,12,11,36,13],furtherpushthistechnologybyus-inghighlycustomized,highspeedRGBDsensors.Highframeratecameras[16,15,46]canalsohelpmakethenon-rigidtrackingproblemmoretractable,andcompellingvol-umetriccapturecanbeobtainedwithjust8customRGBDsensorsratherthanhundreds[28].Howeverthesemethodsstillsufferfrombothgeometricandtextureaberrations,asdemonstratedbyDouetal.[11]andDuetal.[13].Machinelearningtechniques.Theproblemofgenerat-ingimagesofanobjectfromnovelviewpointscanalsobecastfromamachinelearning,asopposedtographics,stand-point.Forinstance,Dosovitskiyetal.[10]generatesre-renderingsofchairsfromdifferentviewpoints,butthequal-ityoftherenderingislow,andtheoperationisspecializedtodiscreteshapeclasses.Morerecentworks[25,38,49]trytolearnthe2D-3Dmappingbyemployingsomenotionof3Dgeometry,ortoencodemultiview-stereoconstraintsdirectlyinthenetworkarchitecture[17].Aswefocusonhumans,ourresearchismorecloselyrelatedtoworksthatattempttosynthesize2Dimagesofhumans[48,2,43,32,31,34,5].Thesefocusongeneratingpeopleinunseenposes,butusu-allyfromaﬁxedcameraviewpoint(typicallyfrontal)andscale(notmetricallyaccurate).Thecoarse-to-ﬁneGANsof[48]synthesizesimagesthatarestillrelativelyblurry.Maetal.[31]detectsposeintheinput,whichhelpstodisentan-gleappearancefrompose,resultinginimprovedsharpness.Evenmorecomplexvariants[32,43]thatattempttodis-entangleposefromappearance,andforegroundfromback-ground,stillsufferfrommultipleartifacts,especiallyinoc-cludedregions.AdenseUVmapcanalsobeusedasaproxytore-renderthetargetfromanovelviewpoint[34],buthigh-frequencydetailsarestillnoteffectivelycaptured.OfparticularrelevanceistheworkbyBalakrishnanetal.[2],wherethroughtheidentiﬁcationandtransformationofbodypartsresultsinmuchsharperimagesbeinggenerated.Nonetheless,notehowthisworksolelyfocusesonfrontalviewpoints.Ourapproach.Indirectcontrast,ourgoalistorenderasubjectinunseenposesandarbitraryviewpoints,mimick-ingthebehaviorofvolumetriccapturesystems.Thetaskathandismuchmorechallengingbecauseitrequiresdis-entanglingpose,texture,backgroundandviewpointsimul-taneously.ThisobjectivehasbeenpartiallyachievedbyMartin-Bruallaetal.[33]bycombiningthebeneﬁtsofge-ometricalpipelines[11]tothoseofconvolutionalarchitec-tures[42].However,theirworkstillnecessitatesacompletemeshbeingreconstructedfrommultipleviewpoints.Incon-trast,ourgoalistoachievethesamelevelofphoto-realismfromasingleRGBDinput.Totacklethis,weresorttoasemi-parametricapproach[40],whereacalibrationphaseisusedtoacquireframesoftheusersappearancefromafewdifferentviewpoints.Thesecalibrationimagesarethenmergedtogetherwiththethecurrentviewoftheuserinanend-to-endfashion.Weshowthatthesemi-parametricap-proachisthekeytogeneratinghighquality,2Drenderingsofpeopleinarbitraryposesandcameraviewpoints.3.ProposedFrameworkAsillustratedinFigure1,ourmethodreceivesasinput:1)anRGBDimagefromasingleviewpoint,2)anovelcam-eraposewithrespecttothecurrentviewand3)acollectionofafewcalibrationimagesobservingtheuserinvariousposesandviewpoints.Asoutput,itgeneratesarenderedimageoftheuserasobservedfromthenewviewpoint.OurproposedframeworkisvisualizedinFigure2,andincludesthefourcorecomponentsoutlinedbelow.Re-rendering&PoseDetector:fromtheRGBDimage¯Icapturedfromacamera¯v,were-renderthecoloreddepthmapfromthenewcameraviewpointvtogenerateanimageIcloud,aswellasitsapproximatenormalmapN.Noteweonlyre-rendertheforegroundoftheimage,byemployingafastbackgroundsubtractionmethodbasedondepthandRGBasdescribedin[15].Wealsoestimatetheposeκoftheuser,i.e.keypoints,inthecoordinateframeofv,aswellasascalarconﬁdencec,measuringthedivergencebetweenthecameraviewpoints:Icloud,κ,N,c=R(¯I,¯v,v).(1)CalibrationImageSelector:fromthecollectionofcali-brationRGBDimagesandposes{¯Incalib,¯κncalib},weselectonethatbestresemblesthetargetposeκintheviewpointv:¯Icalib,¯κcalib=S({¯Incalib,¯κncalib},κ).(2)CalibrationImageWarper:giventheselectedcalibra-tionimage¯Icalibandtheuser’spose¯κcalib,aneuralnetworkWwithlearnableparametersωwarpsthisimageintothedesiredposeκ,whilesimultaneouslyproducingthesilhou-ettemaskI•warpofthesubjectinthenewpose:Iwarp,I•warp=Wω(¯Icalib,¯κcalib,κ).(3)NeuralBlender:ﬁnally,weblendtheinformationcap-turedbythetraditionalre-renderingin(1)tothewarpedcalibrationimage(3)toproduceourﬁnalimageIout:Iout=Bβ(Icloud,Iwarp,I•warp,N,c).(4)Notethatwhile(1)and(2)arenotlearnable,theyextractquantitiesthatexpressthegeometricstructureoftheprob-lem.Conversely,bothwarper(3)and(4)aredifferentiableandtrainedend-to-endwherethelossistheweightedsumbetweenwarperLwarperandblenderLblenderlosses.Theweightsωwarperandωblenderarechosentoensuresimilarcon-tributionsbetweenthetwo.Wenowdescribeeachcompo-nentindetails,motivatingthedesignchoiceswetook.3.1.Re-rendering&PoseDetectorWeassumethatcameraintrinsicparameters(opticalcen-teroandfocallengthf)areknownandthusthefunctionΠ−1(p,z|o,f):R37→R3mapsa2Dpixelp=(x,y)withassociateddepthztoa3Dpointinthelocalcameracoordinateframe.Rendering→Icloud.ViathefunctionΠ−1,weﬁrstcon-vertthedepthchannelof¯IintoapointcloudofsizeMinmatrixformas¯P∈R4×M.WethenrotateandtranslatethispointcloudintothenovelviewpointcoordinateframeasP=T¯P,whereT∈R4×4isthehomogeneoustransforma-tionrepresentingtherelativetransformationbetween¯vandv.WerenderPtoa2DimageIcloudinOpenGLbysplattingeachpointwitha3×3kerneltoreducere-samplingarti-facts.Notethatwheninputandnovelcameraviewpointsareclose,i.e.¯v∼v,thenIout∼Icloud,whilewhen¯v(cid:28)vthenIcloudwouldmostlycontainunusableinformation.Posedetection→κ.Wealsoinfertheposeoftheuserbycomputing2Dkeypoints¯κ2D=Kγ(¯I)usingthemethodofPapandreetal.[37]whereKisapre-trainedfeed-forwardnetwork.Wethenlift2Dkeypointstotheir3Dcounter-parts¯κbyemployingthedepthchannelof¯Iand,asbefore,transformtheminthecameracoordinateframevasκ.Weextrapolatemissingkeypointswhenpossiblerelyingontherigidityofthelimbs,torso,face,otherwisewesimplydis-cardtheframe.Finally,inordertofeedthekeypointsκtothenetworksin(3)and(4)followingthestrategyin[2]:weencodeeachpointinanimagechannel(foratotalof17channels)asaGaussiancenteredaroundthepointwithaﬁxedvariance.Wetriedotherrepresentations,suchastheoneusedin[43],butfoundthattheselectedoneleadtomorestabletraining.Conﬁdenceandnormalmap→c,N.Inorderfor(4)todeterminewhetherapixelinimageIcloudcontainsap-propriateinformationforrenderingfromviewpointvweprovidetwosourcesofinformation:anormalmapandaconﬁdencescore.ThenormalmapN,processedinawayanalogoustoIcloud,canbeusedtodecidewhetherapixelin¯Ihasbeenwellobservedfromtheinputmeasurement¯v(e.g.thenetworkshouldlearntodiscardmeasurementstakenatlow-grazingangles).Conversely,therelationshipbetween¯vandv,encodedbyc,canbeusedtoinferwhetherFigure2.Proposedframework–WetakeininputthecurrentRGBDimage,anovelviewpointandacollectionofimagesacquiredinacalibrationstage,whichdepicttheusersindifferentposesobservedfromseveralviewpoints.TheRe-rendering&pose-detectorprojectsthetextureusingdepthinformationandre-projectbacktotheﬁnalviewpoint,togetherwiththetargetpose.Wealsocomputeaconﬁdencescoreofthecurrentobservationswithrespecttothenovelviewpoint.ThisscoreisencodedinthenormalmapNandtheconﬁdencec.TheCalibrationImageSelectorpickstheclosestimage(intermsofviewpoint)fromapreviouslyrecordedcalibrationbank.TheCalibrationImageWarpertriestoaligntheselectedcalibrationimagewiththecurrentpose,italsoproducesasilhouettemask.TheNeuralBlendercombinestheinformationfromthewarpedRGBimage,alignedcalibrationimage,silhouetteimageandviewpointconﬁdencetorecovertheﬁnal,highlydetailedRGBimage.anovelviewpointisback-facing(i.e.c<0)orfront-facingit(i.e.c>0).Wecomputethisquantityasthedotproductbetweenthecamerasviewvectors:c=[0,0,1]·rz/krzk,where¯visalwaysassumedtobetheoriginandrzisthethirdcolumnoftherotationmatrixforthenovelcameraviewpointv.Anexampleofinputandoutputofthismod-ulecanbeobservedinFigure2,toprow.3.2.CalibrationImageSelectorInapre-processingstage,wecollectasetofcalibrationim-ages{¯Incalib}fromtheuserwithassociatedposes{¯κncalib}.Forexample,onecouldasktheusertorotateinfrontofthecamerabeforethesystemstarts;anexampleofcalibrationsetisvisualizedinthesecondrowofFigure2.Whileitisunreasonabletoexpectthiscollectiontocontaintheuserinthedesiredpose,andobservedexactlyfromtheviewpointv,itisassumedthecalibrationsetwillcontainenoughin-formationtoextrapolatetheappearanceoftheuserfromthenovelviewpointv.Therefore,inthisstageweselectarea-sonableimagefromthecalibrationsetthat,whenwarpedby(3)willprovidesufﬁcientinformationto(4)toproducetheﬁnaloutput.Wecomputeascoreforallthecalibrationimages,andthecalibrationimagewiththehighestscoreisselected.Afewexamplesoftheselectionprocessareshowninthesupplementarymaterial.Ourselectionscoreiscom-posedofthreeterms:Sn=ωheadSnhead+ωtorsoSntorso+ωsimSnsim(5)Fromthecurrent3Dkeypointsκ,wecomputea3Dunitvectorrepresentingtheforwardlookingdirectionoftheuser’shead.Thevectoriscomputedbycreatingalocalco-ordinatesystemfromthekeypointsoftheeyesandnose.Analogously,wecompute3Dunitvectors{dncalib}fromthecalibrationimageskeypoints{¯κncalib}.TheheadscoreisthensimplythedotproductSnhead=d·dncalib,andasimilarprocessisadoptedforSntorso,wherethecoordinatesystemiscreatedfromtheleft/rightshoulderandthelefthipkey-points.Thesetwoscoresarealreadysufﬁcienttoaccuratelyselectacalibrationimagefromthedesirednovelviewpoint,howevertheydonottakeintoaccounttheconﬁgurationofthelimbs.Thereforeweintroduceathirdterm,Snsim,thatcomputesasimilarityscorebetweenthekeypoints¯κncalibinthecalibrationimagestothoseinthetargetposeκ.Tosim-Figure3.TheCalibrationWarpertakesasinputtheselectedcal-ibrationtheselectedcalibrationimage¯Icalibandpose¯κcalibandalignsittothetargetposeκ.ItalsoproducesaforegroundmaskI•warp.Forvisualizationpurposesmultiplechannelsarecollapsedintoasingleimage.Seetextfordetails.plifythenotation,werefertoˆκandˆκncalibastheimage-space2Dcoordinatesofkeypointsinhomogeneouscoordinates.Wecancomputeasimilaritytransformation(rotation,trans-lation,scale)Tn∈R3×3thatalignsthetwosets.Notethatatleast2pointsareneededtoestimateour4DOFtrans-formation(oneforrotation,twofortranslation,andoneforscale),thereforewegrouparmkeypoints(elbow,wrist)andlegkeypoints(knee,foot)together.Forinstance,forallthekeypointsbelongingtotheleftarmgroup(LA)wecalcu-late:argminTLAnXLAkˆκLA−TLAnˆκn,LAcalibk2(6)Wethendeﬁnethesimilarityscoreas:SLA=exp(−σkˆκ−TLAnˆκn,LAcalibk)(7)TheﬁnalSnsimisthesumofthescoresforthe4limbs(in-dexedbyj).Theweightsωjaretunedtogivemoreimpor-tancetoheadandtorsodirections,whichdeﬁnethedesiredtargetviewpoint.Thecalibrationimage¯Icalibwiththere-spectivepose¯κcalibwiththehighestscore¯Sisreturnedfromthisstage.Allthedetailsregardingthechosenparameterscanbefoundinthesupplementarymaterial.3.3.CalibrationWarperTheselectedcalibrationimage¯Icalibshouldhaveasimilarviewpointtov,butthepose¯κcalibcouldstillbedifferentfromthedesiredκ,asthecalibrationsetissmall.There-fore,wewarp¯IcalibtoobtainanimageIwarp,aswellasitssilhouetteI•warp.ThearchitecturewedesignedisinspiredbyBalakrishnanetal.[2],whichusesU-NETmodules[42];seeFigure3foranoverview.Thecalibrationpose¯κcalibtensor(17channels,oneperkey-point)andcalibrationimage¯IcalibgothroughaU-NETmodulethatproducesasoutputpartmasks{I•part,p}plusabackgroundmaskI•bg.Thesemasksselectwhichregionsofthebodyshouldbewarpedaccordingtoasimilaritytrans-formation.Similarlyto[2],thewarpingtransformationsarenotlearned,butcomputedvia(6)onkeypointgroupsofatleasttwo2Dpoints;wehave10groupsofkeypoints(seesupplementarymaterialfordetails).Thewarpedtex-ture¯Iwarp,phas3RGBchannelsforeachkeypointsgroupp(30channelsintotal).However,incontrastto[2],wedonotusethemasksjusttoselectpixelstobewarped,butalsowarpthebodypartmasksthemselvestothetargetposeκ.Wethentakethemaximumacrossallthechannelsandsupervisethesynthesisoftheresultingwarpedsilhouette¯I•partwarp.Wenoticedthatthisiscrucialtoavoidoverﬁt-ting,andtoteachthenetworktotransferthetexturefromthecalibrationimagetothetargetviewandkeepinghighfrequencydetails.Wealsodifferfrom[2]inthatwedonotsynthesizethebackground,asweareonlyinterestedintheperformer,butwedoadditionallypredictabackgroundmaskI•bg.Finally,the10channelsencodingtheper-parttexture¯Iwarp,pandthewarpedsilhouettemask¯I•partwarpgothroughanotherU-NETmodulethatmergestheper-parttexturesandreﬁnestheﬁnalforegroundmask.Pleaseseeadditionaldetailsinthesupplementarymaterial.TheCalibrationWarperistrainingminimizingmultiplelosses:Lwarp=wWrecLWrec+wWfgLWfg+wWbgLWbg++wWfgrefLWfgref+wWGANLWGAN,(8)wherealltheweightswW∗areempiricallychosensuchthatallthelossesareapproximatelyinthesamedynamicrange.WarpreconstructionlossLWrec.Ourperceptualrecon-structionlossLWrec=kVGG(Iwarp)−VGG(Igt)k2measuresthedifferenceinVGGfeature-spacebetweenthepredictedimageIwarp,andthecorrespondinggroundtruthimageIgt.Giventhenatureofcalibrationimages,Iwarpmaylackhighfrequencydetailssuchasfacialexpressions.Therefore,wecomputethelossselectingfeaturesfromconv2uptoconv5layersoftheVGGnetwork.WarpbackgroundlossLWbg.Inordertoremovetheback-groundcomponentof[2],wehavealossLWbg=kI•bg−I•bg,gtk1betweenthepredictedmaskI•bgandthegroundtruthmaskI•bg,gt=1−I•gt.Weconsideredotherlosses(e.g.lo-gistic)buttheyallproducedverysimilarresults.WarpforegroundlossLWfg.Eachpartmaskiswarpedintotargetposeκbythecorrespondingsimilaritytransforma-tion.Wethenmergeallthechannelswithamax-poolingop-erator,andretrieveaforegroundmask¯I•partwarp,overwhichweimposeourlossLWfg=k¯I•partwarp−I•gtk1.Thislossiscrucialtopushthenetworktowardslearningtransformationratherthanmemorizingthesolution(i.e.overﬁtting).WarpforegroundreﬁnementlossLWfgref.ThewarpedpartmasksI•part,pmaynotmatchthesilhouettepreciselyduetotheassumptionofsimilaritytransformationamongthebodyparts,thereforewealsoreﬁnethemaskproducingaﬁnalbinaryimageI•warp.ThisistrainedbyminimizingthelossLWfgref=kI•warp−I•gtk1.WarpGANlossLWGAN.WeﬁnallyaddaGANcomponentthathelpshallucinatingrealistichighfrequencydetailsasshownin[2].Followingtheoriginalpaper[19]wefoundmorestableresultswhenusedthefollowingGANcompo-nent:LWGAN=−log(D(I•warp)),wherethediscriminatorDconsistsof5convlayerswith256ﬁlters,withmaxpool-inglayerstodownsamplethefeaturemaps.Finallyweadd2fullyconnectedlayerswith256featuresandasigmoidactivationtoproducethediscriminatorlabel.3.4.NeuralBlenderThere-renderedimageIcloudcanbeenhancedbythecon-tentinthewarpedcalibrationIwarpviaaneuralblendingoperationconsistingofanotherU-NETmodule:pleaseseethesupplementarymaterialformoredetailsregardingthearchitecture.Bydesign,thismoduleshouldalwaysfavordetailsfromIcloudifthenovelcameraviewvisclosetotheoriginal¯v,whileitshouldleveragethetextureinIwarpforback-facingviews.Toguidethenetworktowardsthis,wepassasinputthenormalmapN,andtheconﬁdencec,whichispassedasanextrachanneltoeachpixel.Theseadditionalchannelscontainalltheinformationneededtodisambiguatefrontalfrombackviews.ThemaskI•warpactsasanadditionalfeaturetoguidethenetworktowardsun-derstandingwhereitshouldhallucinateimagecontentnotvisibleinthere-renderedimageIcloud.Theneuralblenderissupervisedbythefollowingloss:Lblender=wBrecLBrec+wBGANLBGAN(9)BlenderreconstructionlossLBrec.ThereconstructionlosscomputesthedifferencebetweentheﬁnalimageoutputIoutandthetargetviewIgt.ThislossisdeﬁnedLBrec=kVGG(Iout)−VGG(Igt)k2+w‘1kIout−Igtk1.Asmall(w‘1=0.01)photometric(‘1)lossisneededtoensurefastercolorconvergence.BlenderGANlossLBGAN.Thislossfollowsthesamede-signoftheonedescribedforthecalibrationwarpernetwork.4.EvaluationWenowevaluateourmethodandcomparewithrepresenta-tivestate-of-the-artalgorithms.Wethenperformanabla-tionstudyonthemaincomponentsofthesystem.AlltheresultshereareshownontestsequencesnotusedduringFigure4.ExamplesofinputRGBDandgroundtruthnovelviewswithassociatedmasks.Notethatinourdatasetwehaveaccessto8novelviewsforeachinputframe.training;additionalexhaustiveevaluationscanbefoundinthesupplementarymaterial.4.1.TrainingDataCollectionThetrainingprocedurerequiresinputviewsfromanRGBDsensorandmultiplegroundtruthtargetviews.Recentmulti-viewdatasetsofhumans,suchasHuman3.6M[24],onlyprovides4RGBviewsandasinglelow-resolutiondepth(TOF)sensor,whichisinsufﬁcientforthetaskathand;thereforewecollectedourowndatasetwith20subjects.Similarlyto[33],weusedamulti-camerasetupwith8highresolutionRGBviewscoupledwithacustomactivedepthsensor[46].Allthecamerasweresynchronizedatat30Hzbyanexternaltrigger.TherawRGBresolutionis4000×3000,whereasthedepthresolutionis1280×1024.Duetomemorylimitationsduringthetraining,wedown-sampledalsotheRGBimagesto1280×1024pixels.Eachperformerwasfreetoperformanyarbitrarymovementinthecapturespace(e.g.walking,jogging,dancing,etc.)whilesimultaneouslyperformingfacialmovementsandex-pressions.Foreachsubjectwerecorded10sequencesof500frames.Foreachparticipantinthetrainingset,weleft2sequencesoutduringtraining.Onesequenceisusedascal-ibration,wherewerandomlypick10framesateachtrain-ingiterationascalibrationimages.Thesecondsequenceisusedastesttoevaluatetheperformanceofaseenactorbutunseenactions.Finally,weleft5subjectsoutfromthetrainingdatasetstoassesstheperformancesofthealgorithmonunseenpeople.Silhouettemasksgeneration.AsdescribedinSec.3.3andSec.3.4,ourtrainingprocedurereliesongroundtruthfore-groundandbackgroundmasks(I•gtandI•bg,gt=1−I•gt).Thus,weusethestate-of-the-artbodysemanticsegmenta-Figure5.Comparisonswithstateoftheartmethods.Noticehowtheproposedframeworkfavorablycompareswithtraditionalvolumetriccapturerigsthatusemany(8)camerasfrommultipleviewpoints.Noticethatduetoitsreal-timenature,Motion2Fusion[11]canaffordonlylowresolution(1280×1024)RGBimagesforthetexturingphase,whereasFVV[7]acceptsasinput4000×3000images.tionalgorithmbyChenetal.[6]togeneratethesemasksI•gtwhicharethenreﬁnedbyapairwiseCRF[29]toimprovethesegmentationboundaries.Wedonotexplicitmakeuseofthesemanticinformationextractedbythisalgorithmsuchasin[33],leavingthisforfuturework.Notethatattesttime,thesegmentationisnotrequiredinput,butnonethe-lesswepredictasilhouetteasabyproductastoremovethedependencyonthebackgroundstructure.ExamplesofourtrainingdatacanbeobservedinFigure4.Nomanualanno-tationisrequiredhencedatacollectionisfullyautomatic.4.2.ComparisonwithStateoftheArtWenowcomparethemethodwithrepresentativestateoftheartapproaches:weselectedalgorithmsforcompari-sonrepresentativeofthedifferentstrategiestheyuse.TheveryrecentmethodbyBalakrishnanetal.[2]wasse-lectedasastateoftheartmachinelearningbasedapproachduetoitshighqualityresults.Wealsore-implementedtraditionalcapturerigsolutionssuchasFVV[7]andMotion2Fusion[11].FinallywecomparewithLookin-Good[33],ahybridpipelinethatcombinesgeometricpipelineswithdeepnetworks.Notice,thatthesesystemsusealltheavailableviews(8camerasinourdataset)asin-put,whereasourframeworkreliesonasingleRGBDview.QualitativeResults.WeshowqualitativeresultsonFig-ure5.Noticehowouralgorithm,usingonlyasingleRGBDFigure6.Resultsofthevariousstageofthepipeline.Noticehoweachstageofthesystemcontributestoachievetheﬁnalhighqual-ityresults,provingtheeffectivnessofourdesignchoices.Finally,thankstothesemi-parametricmodel,thealgorithmgeneralizeswellacrossunseensubjects.input,outperformsthemethodofBalakrishnanetal.[2]:wesynthesizesharperresultsandalsohandleviewpointandscalechangescorrectly.Additionally,theproposedframeworkgeneratescompellingresults,oftencompara-bletomultiviewmethodssuchasLookinGood[33],Mo-ProposedIcloud¯IcalibIwarpBalakrishnanetal.[2]LookinGood[33]M2F[11]FVV[7]1view1view1view1view1view8views8views8views‘1Loss17.4027.2720.0218.7018.0138.8033.727.39PSNR28.4322.3521.1027.3222.9329.9328.2132.60MS-SSIM0.920.840.870.910.860.920.960.96VGGLoss12.5021.2021.4113.9620.1610.655.346.51Table1.Quantitativeevaluationsontestsequences.WecomputedmultiplemetricssuchasPhotometricError(‘1loss),PSNR,MS-SSIMandPerceptualLoss.WecomparedthemethodwiththeoutputoftherenderingstageIcloud,theoutputofthecalibrationselector¯IcalibandtheoutputofthecalibrationwarperIwarp.WealsoshowhowourmethodoutperformsonmultiplemetricsthestateoftheartmethodofBalakrishnaetal.[2].WealsofavorablycomparewithfullcapturerigsolutionssuchasMotion2Fusion[11],FVV[7]andtheLookinGoodsystem[33].Figure7.Comparisonoftheproposedsystemwiththefullypara-metricmodel.Noticehowthesemi-parametricpartiscrucialtogetthehighestlevelofquality.tion2Fusion[11]orFVV[7].QuantitativeComparisons.Toquantitativelyassessandcomparethemethodwiththestateoftheart,wecomputedmultiplemetricsusingtheavailablegroundtruthimages.TheresultsareshowninTable1.Oursystemclearlyout-performsthemultiplebaselinesandcomparesfavorablytostateoftheartvolumetriccapturesystemsthatusemultipleinputviews.4.3.AblationStudyWenowquantitativelyandqualitativelyanalyzeeacheachstageofthepipeline.InFigure6noticehoweachstageofthepipelinecontributestoachievetheﬁnalhighqualityresult.Thisprovesthateachcomponentwascarefullyde-signedandneeded.Noticealsohowwecanalsogeneralizetounseensubjectsthankstothesemi-parametricapproachweproposed.TheseexcellentresultsarealsoconﬁrmedinthequantitativeevaluationwereportedinTable1:notehowtheoutputofthefullsystemconsistentlyoutperformstheonefromthere-rendering(Icloud),thecalibrationimagese-lector(¯Icalib),andthecalibrationimagewarper(Iwarp).Wereferthereadertothesupplementarymaterialformorede-tailedexamples.Comparisonwithfullyparametricmodel.Inthisexper-imentweremovedthesemi-parametricpartofourframe-Figure8.Predictionsforviewpointsnotinthetrainingset.Themethodcorrectlyinfersviewswherenogroundtruthisavailable.work,i.e.thecalibrationselectorandthecalibrationwarper,andtraintheneuralblenderontheoutputofthere-renderer(i.e.afullyparametricmodel).Thisissimilartotheap-proachpresentedin[33],appliedtoasingleRGBDimage.WeshowtheresultsinFigure7:noticehowtheproposedsemi-parametricmodeliscrucialtoproperlyhandlelargeviewpointchanges.Viewpointgeneralization.WeﬁnallyshowinFigure8qualitativeexamplesforviewpointsnotpresentinthetrain-ingset.Noticehowweareabletorobustlyhandlethosecases.Pleaseseesupplementarymaterialsformoreexam-ples.5.ConclusionsWeproposedanovelformulationtotackletheproblemofvolumetriccaptureofhumanswithmachinelearning.Ourpipelineelegantlycombinestraditionalgeometrytosemi-parametriclearning.Weexhaustivelytestedtheframeworkandcompareditwithmultiplestateoftheartmethods,showingunprecedentedresultsforasingleRGBDcamerasystem.Currently,ourmainlimitationsareduetosparsekeypoints,whichweplantoaddressbyaddingadditionaldiscriminativepriorssuchasin[26].Infuturework,wewillalsoinvestigateperformingendtoendtrainingoftheentirepipeline,includingthecalibrationkeyframeselectionandwarping.References[1]R.Anderson,D.Gallup,J.T.Barron,J.Kontkanen,N.Snavely,C.Hern´andez,S.Agarwal,andS.M.Seitz.Jump:virtualrealityvideo.ACMTOG,2016.2[2]G.Balakrishnan,A.Zhao,A.V.Dalca,F.Durand,andJ.V.Guttag.Synthesizingimagesofhumansinunseenposes.CVPR,2018.2,3,5,6,7,8[3]J.Carranza,C.Theobalt,M.A.Magnor,andH.-P.Seidel.Free-viewpointvideoofhumanactors.SIGGRAPH,2003.1[4]D.Casas,M.Volino,J.Collomosse,andA.Hilton.4DVideoTexturesforInteractiveCharacterAppearance.EU-ROGRAPHICS,2014.2[5]C.Chan,S.Ginosar,T.Zhou,andA.A.Efros.Everybodydancenow.CoRR,2018.2[6]L.-C.Chen,Y.Zhu,G.Papandreou,F.Schroff,andH.Adam.Encoder-decoderwithatrousseparableconvolutionforse-manticimagesegmentation.CoRR,abs/1802.02611,2018.7[7]A.Collet,M.Chuang,P.Sweeney,D.Gillett,D.Evseev,D.Calabrese,H.Hoppe,A.Kirk,andS.Sullivan.High-qualitystreamablefree-viewpointvideo.ACMTOG,2015.1,2,7,8[8]P.Debevec,T.Hawkins,C.Tchou,H.-P.Duiker,W.Sarokin,andM.Sagar.Acquiringthereﬂectanceﬁeldofahumanface.InSIGGRAPH,2000.1[9]P.E.Debevec,C.J.Taylor,andJ.Malik.Modelingandrenderingarchitecturefromphotographs:Ahybridgeometryandimage-basedapproach.InSIGGRAPH,1996.2[10]A.Dosovitskiy,J.T.Springenberg,M.Tatarchenko,andT.Brox.Learningtogeneratechairswithconvolutionalnet-works.CVPR,2015.2[11]M.Dou,P.Davidson,S.R.Fanello,S.Khamis,A.Kowdle,C.Rhemann,V.Tankovich,andS.Izadi.Motion2fusion:Real-timevolumetricperformancecapture.SIGGRAPHAsia,2017.1,2,3,7,8[12]M.Dou,S.Khamis,Y.Degtyarev,P.Davidson,S.R.Fanello,A.Kowdle,S.O.Escolano,C.Rhemann,D.Kim,J.Taylor,P.Kohli,V.Tankovich,andS.Izadi.Fusion4d:Real-timeperformancecaptureofchallengingscenes.SIGGRAPH,2016.1,2[13]R.Du,M.Chuang,W.Chang,H.Hoppe,andA.Varshney.Montage4D:Real-timeSeamlessFusionandStylizationofMultiviewVideoTextures.JournalofComputerGraphicsTechniques,8(1),January2019.2[14]M.Eisemann,B.D.Decker,M.Magnor,P.Bekaert,E.D.Aguiar,N.Ahmed,C.Theobalt,andA.Sellent.Floatingtextures.ComputerGraphicsForum,2008.2[15]S.R.Fanello,J.Valentin,A.Kowdle,C.Rhemann,V.Tankovich,C.Ciliberto,P.Davidson,andS.Izadi.Lowcomputeandfullyparallelcomputervisionwithhashmatch.InICCV,2017.2,3[16]S.R.Fanello,J.Valentin,C.Rhemann,A.Kowdle,V.Tankovich,P.Davidson,andS.Izadi.Ultrastereo:Efﬁ-cientlearning-basedmatchingforactivestereosystems.InCVPR,2017.2[17]J.Flynn,I.Neulander,J.Philbin,andN.Snavely.Deepstereo:Learningtopredictnewviewsfromtheworld’sim-agery.InCVPR,2016.2[18]G.FyffeandP.Debevec.Single-shotreﬂectancemeasure-mentfrompolarizedcolorgradientillumination.InIEEEInternationalConferenceonComputationalPhotography,2015.1[19]I.J.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,andY.Bengio.Gen-erativeadversarialnets.InNIPS,2014.6[20]Google.Arcore-googledevelopersdocumentation,2018.1[21]S.J.Gortler,R.Grzeszczuk,R.Szeliski,andM.F.Cohen.Thelumigraph.InSIGGRAPH,1996.2[22]K.Guo,J.Taylor,S.Fanello,A.Tagliasacchi,M.Dou,P.Davidson,A.Kowdle,andS.Izadi.Twinfusion:Highframeratenon-rigidfusionthroughfastcorrespondencetracking.In3DV,2018.1[23]M.Innmann,M.Zollh¨ofer,M.Nießner,C.Theobalt,andM.Stamminger.VolumeDeform:Real-timeVolumetricNon-rigidReconstruction.InECCV,2016.1[24]C.Ionescu,D.Papava,V.Olaru,andC.Sminchisescu.Hu-man3.6m:Largescaledatasetsandpredictivemethodsfor3dhumansensinginnaturalenvironments.IEEEPAMI,2014.6[25]D.Ji,J.Kwon,M.McFarland,andS.Savarese.Deepviewmorphing.CoRR,2017.2[26]H.Joo,T.Simon,andY.Sheikh.Totalcapture:A3ddefor-mationmodelfortrackingfaces,hands,andbodies.CVPR,2018.8[27]M.KazhdanandH.Hoppe.Screenedpoissonsurfacerecon-struction.ACMTOG,2013.2[28]A.Kowdle,C.Rhemann,S.Fanello,A.Tagliasacchi,J.Tay-lor,P.Davidson,M.Dou,K.Guo,C.Keskin,S.Khamis,D.Kim,D.Tang,V.Tankovich,J.Valentin,andS.Izadi.Theneed4speedinreal-timedensevisualtracking.SIGGRAPHAsia,2018.2[29]P.Kr¨ahenb¨uhlandV.Koltun.Efﬁcientinferenceinfullyconnectedcrfswithgaussianedgepotentials.InNIPS,2011.7[30]L.Labs.3Dscannerapp,2018.https://www.3dscannerapp.com/.1[31]L.Ma,X.Jia,Q.Sun,B.Schiele,T.Tuytelaars,andL.VanGool.Poseguidedpersonimagegeneration.InNIPS,2017.2[32]L.Ma,Q.Sun,S.Georgoulis,L.V.Gool,B.Schiele,andM.Fritz.Disentangledpersonimagegeneration.CVPR,2018.2[33]R.Martin-Brualla,R.Pandey,S.Yang,P.Pidlypenskyi,J.Taylor,J.Valentin,S.Khamis,P.Davidson,A.Tkach,P.Lincoln,A.Kowdle,C.Rhemann,D.B.Goldman,C.Ke-skin,S.Seitz,S.Izadi,andS.Fanello.Lookingood:Enhanc-ingperformancecapturewithreal-timeneuralre-rendering.InSIGGRAPHAsia,2018.2,6,7,8[34]N.Neverova,R.A.G¨uler,andI.Kokkinos.Denseposetrans-fer.ECCV,2018.2[35]R.A.Newcombe,D.Fox,andS.M.Seitz.Dynamicfusion:Reconstructionandtrackingofnon-rigidscenesinreal-time.InCVPR,June2015.1,2[36]S.Orts-Escolano,C.Rhemann,S.Fanello,W.Chang,A.Kowdle,Y.Degtyarev,D.Kim,P.L.Davidson,S.Khamis,M.Dou,V.Tankovich,C.Loop,Q.Cai,P.A.Chou,S.Mennicken,J.Valentin,V.Pradeep,S.Wang,S.B.Kang,P.Kohli,Y.Lutchyn,C.Keskin,andS.Izadi.Holo-portation:Virtual3dteleportationinreal-time.InUIST,2016.1,2[37]G.Papandreou,T.Zhu,N.Kanazawa,A.Toshev,J.Tomp-son,C.Bregler,andK.P.Murphy.Towardsaccuratemulti-personposeestimationinthewild.CVPR,2017.3[38]E.Park,J.Yang,E.Yumer,D.Ceylan,andA.C.Berg.Transformation-groundedimagegenerationnetworkfornovel3dviewsynthesis.InCVPR,2017.2[39]F.Prada,M.Kazhdan,M.Chuang,A.Collet,andH.Hoppe.Spatiotemporalatlasparameterizationforevolvingmeshes.ACMTOG,2017.1,2[40]X.Qi,Q.Chen,J.Jia,andV.Koltun.Semi-parametricimagesynthesis.CoRR,2018.3[41]C.Richardt,Y.Pritch,H.Zimmer,andA.Sorkine-Hornung.Megastereo:Constructinghigh-resolutionstereopanoramas.InCVPR,2013.2[42]O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolu-tionalnetworksforbiomedicalimagesegmentation.MIC-CAI,2015.3,5[43]C.Si,W.Wang,L.Wang,andT.Tan.Multistageadversar-iallossesforpose-basedhumanimagesynthesis.InCVPR,2018.2,3[44]M.Slavcheva,M.Baust,D.Cremers,andS.Ilic.Killingfu-sion:Non-rigid3dreconstructionwithoutcorrespondences.InCVPR,2017.1[45]M.Slavcheva,M.Baust,andS.Ilic.Sobolevfusion:3dre-constructionofscenesundergoingfreenon-rigidmotion.InCVPR,2018.1[46]V.Tankovich,M.Schoenberg,S.R.Fanello,A.Kowdle,C.Rhemann,M.Dzitsiuk,M.Schmidt,J.Valentin,andS.Izadi.Sos:Stereomatchingino(1)withslantedsupportwindows.IROS,2018.2,6[47]M.Volino,D.Casas,J.Collomosse,andA.Hilton.Optimalrepresentationofmultipleviewvideo.InBMVC,2014.2[48]B.Zhao,X.Wu,Z.Cheng,H.Liu,andJ.Feng.Multi-viewimagegenerationfromasingle-view.CoRR,2017.2[49]T.Zhou,S.Tulsiani,W.Sun,J.Malik,andA.A.Efros.Viewsynthesisbyappearanceﬂow.CoRR,2016.2[50]C.L.Zitnick,S.B.Kang,M.Uyttendaele,S.Winder,andR.Szeliski.High-qualityvideoviewinterpolationusingalayeredrepresentation.ACMTOG,2004.2[51]M.Zollh¨ofer,M.Nießner,S.Izadi,C.Rehmann,C.Zach,M.Fisher,C.Wu,A.Fitzgibbon,C.Loop,C.Theobalt,andM.Stamminger.Real-timenon-rigidreconstructionusinganrgb-dcamera.ACMTOG,2014.2VolumetricCaptureofHumanswithaSingleRGBDCameraviaSemi-ParametricLearningSupplementaryMaterialRohitPandey,AnastasiaTkach,ShuoranYang,PavelPidlypenskyi,JonathanTaylor,RicardoMartin-Brualla,AndreaTagliasacchi,GeorgePapandreou,PhilipDavidson,CemKeskin,ShahramIzadi,SeanFanelloGoogleInc.Inthissupplementarymaterial,weprovideadditionalinfor-mationregardingourmethod’simplementation,morede-tailsandmoreablationstudiesonimportantcomponentsoftheproposedframework.1.FrameworkDetailsWedetailherethechoicesofvariousparameterstoaidinreproducingresults.1.1.CalibrationImageSelector-ParamsInFigure1weshowsomeexampleoutputsofthecalibra-tionselectormodule.Notehowthemoduleselectsthecal-ibrationimagethatmostcloselymatchestheviewpointthepersonisseenfrom,basedonthetargetpose.ForEq.5weempiricallyselectωhead=5,ωtorso=3,andωsim=1soastoweightheheadandtorsocomponentsofthescorehigh-est,thenfactorinthetransformationscoresofthelimbs.1.2.CalibrationImageWarperKeypointgrouping:Wedetect17keypointsandgroupthemintointo10bodyparts.Thebodypartsconsistof1)head2)body3)leftupperarm4)rightupperarm5)leftlowerarm6)rightlowerarm7)leftupperleg8)rightupperleg9)leftlowerleg,and10)rightlowerleg.Theheadkeypointscon-sistofthenose,left/righteyes,andleft/rightears.Thebodykeypointsconsistoftheleft/rightshoulder,andleft/righthipkeypoints.Eachlimbconsistsoftwokeypoints;theshoul-derandelbowfortheupperarms,theelbowandwristforthelowerarms,thehipandkneefortheupperlegs,andthekneeandankleforthelowerlegs.Networkarchitecture:OurU-Netarchitectureconsistsof5encoderblocksfollowedby5decoderblocks.Eachencoderblockdownsamplestheinputbyafactorof2andconsistsof2convolutionallayers;theﬁrstwithakernelsizeof3andstride1andthesecondwithakernelsizeof4andstride2.EachdecoderblockconsistsofabilinearupsamplinglayerFigure1.Examplesofselectedcalibrationimages.Theclosestviewpointtothetargetisselected.thatupsamplestheinputbyafactorof2,followedbyacon-volutionallayerwithkernelsize3andstride1.Theencoderblocksuse64ﬁltersfortheﬁrstblockfollowedby128ﬁl-tersfortheremaining4blocks.Thedecoderblocksuse128ﬁltersfortheﬁrst4convolutionallayersand32ﬁltersfortheﬁnalblock.Additionally,weaddskipconnectionsfromtheencodertothedecoderintheformofconcatena-tionoffeaturemapsofmatchingsize.LeakyReLuacti-vations(withalpha=0.2)areusedforallconvolutionallayers.arXiv:1905.12162v1  [cs.CV]  29 May 2019Figure2.TheﬁnalNeuralBlendermoduletakestheoutputRGBandmaskofthecalibrationwarpermodule,aswellasthewarpedRGB,normals,andviewpointconﬁdencefromthere-renderingmodule,andlearnshowtoblendthemintotheﬁnaloutputRGB.Thecalibrationimagewarpermoduleaddsaﬁnalconvo-lutionallayerwith4channelstoproducetheRGBoutputIwarp,andthemaskI•warp.TanhactivationisusedfortheRGBandsigmoidforthemaskprediction.1.3.NeuralBlenderTheneuralblendermoduleisshowninFigure2.Forsim-plicitywere-usethesameU-Netarchitecturedescribedinthecalibrationimagewarpermodulesectionabove.How-ever,thelastconvolutionallayernowoutputsonly3chan-nelsfortheﬁnalblendedRGB.1.4.TrainingDetailsOurnetworksareimplementedinTensorﬂowandtrainedinparallelon16NVIDIAV100GPUseachwith16GBofmemory.WeusetheAdamoptimizer[1]withalearn-ingrateof1−5forthegeneratorand1−6forthediscrimi-nator.Weperformlightdataaugmentationduringtrainingwithrandomcroppinginasizerangeof0.85to1.timesoftheinputimagesize.Additionally,weaddstandard‘2lossregularization(withweight1−5)totheweightsofthenetwork.Wefoundthatourdataaugmentationandregu-larization,coupledwiththevariationsintroducedviausingallpossiblecombinationsofsourceandtargetcamerasinourtrainingset,weresufﬁcienttopreventover-ﬁttingandmakeournetworkgeneralizetounseenposes,viewpointsandpeople.2.RunningtimeofthesystemTheproposedarchitecturehasanend-to-endruntimeof104.3msonaTitanVGPU.Notethat,thisistheunopti-mizedruntimeanddoesnottakeadvantageofﬂoat16in-ferenceortensorcoresonthevoltaarchitectures.Weleaveachievingrealtimeinferenceusingtheproposedarchitec-tureforfuturework.Figure3.Examplesofthewarpedforegroundmaskandreﬁnedforegroundmask,comparedtothegroundtruthmask.Figure4.Effectofthenumberofcalibrationimagesinthepoolontheoutputofthecalibrationimagewarpermodule.3.AdditionalEvaluationInFigure3weshowsomeexamplesofthepredictedwarpedpartmasksandreﬁnedmaskscomparedtothegroundtruthforegroundmask.Notethatthewarpedpartmaskislimitedintheaccuracyofthesilhouetteitcanproduceduetotheassumptionof2Dsimilaritytransformationbetweenbodyparts,however,thepredictedreﬁnedmaskisabletoover-cometheselimitationsandproduceamuchcleanersilhou-ette.InFigure4weshowtheeffectofaddingmoreimagestothecalibrationimagepoolontheoutputofthecalibrationwarp-ingmodule.Allcalibrationimagesarechosenatrandomfromaheldoutsequenceoftheuser.Noticehowthequalityoftheoutputimprovesasthenumberofcalibrationimagesincreasesfrom6to210.Thisisduetothehigherprobabil-Figure5.Additionalresultsshowingvariousstagesofthepipeline.Figure6.Additionalresultsshowingtheviewpointgeneralizationoftheproposedmethod.ityofﬁndingacalibrationimagewithmatchingviewpointasweincreasethesizeoftheimagepool.InFigure5wepresentadditionalresultsshowingtheout-putofvariousstagesoftheproposedpipelineonseenandunseensubjects.InFigure6wepresentadditionalresultsshowingtheabilityoftheproposedmethodtogeneralizetoviewpointsnotinthetrainingdata.4.LimitationsandFutureWorkOneofthelimitationsoftheproposedapproachisthatthecalibrationwarperproducesblurryresultswhentheview-pointoftheselectedcalibrationimageisfarfromthetargetviewpoint.WenoticethisinresultswhenthecalibrationimagepoolissmallasshowninFigure4.Alargercali-brationpoolorapredeﬁnedcalibrationsequencewheretheuserturnsaroundthecameracanhelpalleviatethisissue.Anotherlimitationofthesystemisthatitstrugglestopro-ducereasonableoutputswherekeypointsarenotpresent,forexamplehands.Wehypothesizethataddingadditionalﬁngerkeypointslikeﬁngersandmorefacialkeypointscanhelpboththecalibrationselectionmodule,aswellasthehallucinationmodules,toproducebetterresults.Finally,thesystemshowssometemporalﬂickeringascanbeseeninthesupplementaryvideo.Thisisespeciallyevidentwhentheselectedcalibrationimagechanges,andlikelycanbeal-leviatedviatemporalarchitectureslikeRNNsortemporalcoherencylosses.References[1]D.P.KingmaandJ.Ba.Adam:Amethodforstochasticopti-mization.CoRR,2014.2