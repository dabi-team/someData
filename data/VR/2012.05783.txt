0
2
0
2

c
e
D
0
1

]

G
L
.
s
c
[

1
v
3
8
7
5
0
.
2
1
0
2
:
v
i
X
r
a

Cahier du GERAD G-2020-52

Stochastic Damped L-BFGS with Controlled Norm of the Hessian
Approximation

Sanae Lotﬁ
Tiphaine Bonniot de Ruisselet
Dominique Orban
Andrea Lodi
Polytechnique Montr´eal, Canada

SANAE.LOTFI@POLYMTL.CA
TIPHAINE.BONNIOT@GMAIL.COM
DOMINIQUE.ORBAN@POLYMTL.CA
ANDREA.LODI@POLYMTL.CA

Abstract
We propose a new stochastic variance-reduced damped L-BFGS algorithm, where we leverage es-
timates of bounds on the largest and smallest eigenvalues of the Hessian approximation to balance
its quality and conditioning. Our algorithm, VARCHEN, draws from previous work that proposed
a novel stochastic damped L-BFGS algorithm called SdLBFGS. We establish almost sure conver-
gence to a stationary point and a complexity bound. We empirically demonstrate that VARCHEN
is more robust than SdLBFGS-VR and SVRG on a modiﬁed DavidNet problem—a highly noncon-
vex and ill-conditioned problem that arises in the context of deep learning, and their performance
is comparable on a logistic regression problem and a nonconvex support-vector machine problem.

1. Introduction and Related Work
We consider unconstrained stochastic minimization problems of f : Rn → R where

f (x) = Eξ[F (x, ξ)] (online)

or

f (x) =

1
N

N
(cid:88)

i=1

fi(x) (ﬁnite sum),

(1)

where ξ ∈ Rd denotes a random variable, F : Rn × Rd → R is continuously differentiable and
possibly nonconvex, fi is the loss corresponding to the i-th element of our dataset and N is the size
of the dataset. The algorithm developed below applies to both online and ﬁnite-sum problems.
Stochastic Gradient Descent (SGD) [5, 30] and its variants [12, 18, 25, 28, 32], including variance-
reduced algorithms [13, 17, 26, 34], are widely used to solve (1) in machine learning. However, they
might not be well-suited for highly nonconvex and ill-conditioned problems [6], which are more
effectively treated using (approximate) second-order information. Second-order algorithms are well
studied in the deterministic case [1, 9–11] but there are many areas to explore in the stochastic
context that go beyond existing works [4, 7, 15, 24, 31]. Among these areas, the use of damping in
L-BFGS is an interesting research direction to be leveraged in the stochastic case. Wang et al. [33]
proposed a stochastic damped L-BFGS (SdLBFGS) algorithm and proved almost sure convergence
to a stationary point. However, damping does not prevent the inverse Hessian approximation Hk
from being ill-conditioned [8]. The convergence of SdLBFGS may be heavily affected if the Hessian
approximation becomes nearly singular during the iterations. In order to remedy this issue, Chen
et al. [8] proposed to combine SdLBFGS with regularized BFGS [23]. Our approach differs.

1

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

 
 
 
 
 
 
STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Our contributions:

• Less restrictive assumptions: we force Hk to be uniformly bounded and positive deﬁnite
by requiring the stochastic gradient to be Lipschitz continuous, which is a less restrictive
assumption than those of Wang et al. [33], who require the stochastic function to be twice
differentiable with respect to the parameter vector, and its Hessian to be bounded for all
parameter and random sampling vectors.

• A new damped L-BFGS: we propose a new version of stochastic damped L-BFGS that main-
tains estimates of the smallest and largest eigenvalues of Hk. A solution is proposed when
ill-conditioning is detected that preserves almost sure convergence to a stationary point.

• Choice of the initial inverse Hessian approximation: we propose a new formula for the initial

inverse Hessian approximation to make the algorithm more robust to ill-conditioning.

Notation For a symmetric matrix A, we use A (cid:31) 0 to indicate that A is positive deﬁnite, and
λmin(A) and λmax(A) to denote its smallest and largest eigenvalue, respectively. If B is also sym-
metric, B (cid:22) A means that A − B is positive semideﬁnite. The identity matrix of appropriate size is
denoted I. Finally, Eξ[.] is the expectation over random variable ξ.

2. Formulation of our method

We assume that at iteration k, we can obtain a stochastic approximation

g(xk, ξk) =

1
mk

mk(cid:88)

i=1

∇fξk,i

(xk)

(2)

of ∇f (xk), where ξk denotes the subset of samples taken from a given set of realizations of ξ.
The Hessian approximation constructed at iteration k and its inverse are denoted by Bk and Hk,
respectively, such that Hk = B−1
k

and Bk (cid:31) 0. Iterates are updated according to

xk+1 = xk + αkdk, where dk = −Hkg(xk, ξk)

and αk > 0 is the step size.

(3)

The stochastic BFGS method [31] computes an updated approximation Hk+1 according to

Hk+1 = VkHkV (cid:62)

k + ρksks(cid:62)

k , where

Vk = I − ρksky(cid:62)
k

and ρk = 1/s(cid:62)

k yk,

(4)

which ensures that the secant equation Bk+1sk = yk is satisﬁed, where

sk = xk+1 − xk,

and

yk = g(xk+1, ξk) − g(xk, ξk).

(5)

If Hk (cid:31) 0 and the curvature condition s(cid:62)
Because storing Hk and performing matrix-vector products is costly for large-scale problems, we
use the limited-memory version of BFGS (L-BFGS) [22, 27], in which Hk only depends on the
most recent p iterations and an initial H 0
k (cid:31) 0. The parameter p is the memory of L-BFGS. The
inverse Hessian update can be written as

k yk > 0 holds, then Hk+1 (cid:31) 0 [see, for instance, 14].

Hk = (V (cid:62)

k−1 . . . V (cid:62)

k (Vk−p . . . Vk−1)+

k−p)H 0
k−1 . . . V (cid:62)

ρk−p(V (cid:62)

k−p+1)sk−ps(cid:62)

k−p(Vk−p+1 . . . Vk−1) + · · · + ρk−1sk−1s(cid:62)

k−1.

(6)

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

When αk in (3) is not computed using a Wolfe line search [35, 36], there is no guarantee that the
curvature condition holds. A common strategy is to simply skip the update. By contrast, Powell [29]
proposed damping, which consists in updating Hk using a modiﬁed yk, denoted by ˆyk, to beneﬁt
from information discovered at iteration k while ensuring sufﬁcient positive deﬁniteness. We use

ˆyk := θkyk + (1 − θk)B0

k+1sk,

(7)

which is inspired by [33], and differs from the original proposal of Powell [29], where

θk = 1 if s(cid:62)

k yk ≥ ηs(cid:62)

k B0

k+1sk, and (1 − η)

s(cid:62)
k B0
k+1sk
k+1sk − s(cid:62)
k B0
s(cid:62)

k yk

otherwise,

(8)

with η ∈ (0, 1) and B0

k+1 := (H 0

k+1)−1. The choice (7) ensures that the curvature condition

k ˆyk ≥ ηs(cid:62)
s(cid:62)

k B0

k+1sk ≥ ηλmin(B0

k+1)(cid:107)sk(cid:107)2 > 0,

(9)

is always satisﬁed since H 0
Vi and ρi replaced with ˆVi = I − ˆρis(cid:62)

i ˆyi and ˆρi = 1/s(cid:62)

i ˆyi.

k+1 (cid:31) 0. We obtain the damped L-BFGS update, which is (6) with each

3. A new stochastic damped L-BFGS with controlled Hessian norm

Our working assumption is
Assumption 1 There is κlow ∈ R such that f (x) ≥ κlow for all x ∈ Rn, f is C1 over Rn, and there
is L > 0 such that for all x, y ∈ Rn, (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L (cid:107)x − y(cid:107).

We begin by deriving bounds on the smallest and largest eigenvalues of Hk+1 as functions of bounds
on those of Hk. Proofs can be found in Appendix A.

Lemma 1 Let s and y ∈ Rn such that s(cid:62)y ≥ γ(cid:107)s(cid:107)2 with γ > 0, and such that (cid:107)y(cid:107) ≤ Ly(cid:107)s(cid:107), with
Ly > 0. Let A = µV V (cid:62) + ρss(cid:62), where ρ = 1/s(cid:62)y, µ > 0, and V = I − ρsy(cid:62). Then,

0 < min

(cid:33)

(cid:32)

1
Ly

,

µ
γ L2
1 + µ

y

≤ λmin(A) ≤ λmax(A) ≤

(cid:32)

+ max

0,

1
γ

µ

γ2 L2

y −

(cid:33)

.

µ
γ L2
1 + µ

y

To use Lemma 1 to obtain bounds on the eigenvalues of Hk+1, we make the following assumption:
Assumption 2 There is Lg > 0 such that for all x, y ∈ Rn, (cid:107)g(x, ξ) − g(y, ξ)(cid:107) ≤ Lg (cid:107)x − y(cid:107).

Assumption 2 is required to prove convergence and convergence rates for most recent stochastic
quasi-Newton methods [37]. It is less restrictive than requiring f (x, ξ) to be twice differentiable
with respect to x, and the Hessian ∇2
The next theorem shows that the eigenvalues of Hk+1 are bounded and bounded away from zero.

xxf (x, ξ) to be bounded for any x, ξ, as in [33].

Theorem 2 Let Assumptions 1 and 2 hold. Let H 0
by applying p times the damped BFGS update formula with inexact gradient to H 0
easily computable constants λk+1 and Λk+1 that depend on Lg and H 0
λmin(Hk+1) ≤ λmax(Hk+1) ≤ Λk+1.

If Hk+1 is obtained
k+1, there exist
k+1 such that 0 < λk+1 ≤

k+1 (cid:31) 0 and p > 0.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

The precise form of λk+1 and Λk+1 is given in Appendix A.
A common choice for H 0
k+1 is H 0
k yk, is the scaling parameter.
This choice ensures that the search direction is well scaled, which promotes large steps. To keep
H 0

k+1 from becoming nearly singular or non positive deﬁnite, we deﬁne

k+1I where γk+1 = y(cid:62)

k+1 = γ−1

k yk/s(cid:62)

(cid:16)

H 0

k+1 =

max(γ

k+1

, min(γk+1, γk+1))

(cid:17)

I,

(10)

k+1

< γk+1 can be constants or iteration dependent.

where 0 < γ
The Hessian-gradient product used to compute the search direction dk = −Hkg(xk, ξk) can be ob-
tained cheaply by exploiting a recursive algorithm [27], as described in Algorithm 2 in Appendix B.
Motivated by the success of recent methods combining variance reduction with stochastic L-BFGS
[15, 24, 33], we apply an SVRG-like type of variance reduction [17] to the update. Not only would
this accelerate the convergence, since we can choose a constant step size, but it also improves the
quality of the curvature approximation.
We summarize our complete algorithm, VAriance-Reduced stochastic damped L-BFGS with Con-
trolled HEssian Norm (VARCHEN), as Algorithm 1.

Algorithm 1 Variance-Reduced Stochastic Damped L-BFGS with Controlled Hessian Norm
1: Choose x0 ∈ Rn, step size sequence {αk > 0}k≥0, batch size sequence {mk > 0}k≥0, eigen-
value limits λmax > λmin > 0, memory parameter p, total number of epochs Nepochs, and
sequences {γ
< γk < λmax, for
every k ≥ 0. Set k = 0 and H0 = I.

> 0}k≥0 and {γk+1 > 0}k≥0, such that 0 < λmin < γ

k

k

2: for t = 1, . . . , Nepochs do
3:

Deﬁne xt
while M < N do

k = xk and compute the full gradient ∇f (xt

k). Set M = 0.

Sample batch ξk of size mk ≤ N − M and compute g(xk, ξk) and g(xt
Deﬁne ˜g(xk, ξk) = g(xk, ξk) − g(xt
k, ξk) + ∇f (xt
Estimate Λk and λk in Theorem 2. If Λk > λmax or λk < λmin, delete si, yi and ˆyi for
i = k − p + 1, . . . , k − 2.
Compute dk = −Hk˜g(xk, ξk).
Deﬁne xk+1 = xk + αkdk, and compute sk, yk as in (5), and ˆyk as in (7).
Increment k by one and update M ← M + mk.

k, ξk).

k).

4:

5:

6:

7:

8:

9:

10:

In step 7 of Algorithm 1, we compute an estimate of the upper and lower bounds on λmax(Hk) and
λmin(Hk), respectively. The only unknown quantity in the expressions of Λk and λk in Theorem 2
is Lg, which we estimate as Lg ≈ Lg,k := (cid:107)yk(cid:107)/(cid:107)sk(cid:107). When the estimates are not within the limits
[λmin, λmax], we delete si, yi and ˆyi, i ∈ {k −p+1, . . . , k −2} from storage, such that Hkg(xk, ξk)
is computed using the most recent pair (sk−1, ˆyk−1) only and dk = −Hkg(xk, ξk). Finally, a full
gradient is computed once in every epoch in step 3. The term g(xt
k) can be seen as the
bias in the gradient estimation g(xk, ξk), and it is used here to correct the gradient approximation in
step 6.

k, ξk) − ∇f (xt

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

4. Convergence and Complexity Analysis

We show that Algorithm 1 satisﬁes the assumptions of the convergence analysis and iteration com-
plexity of Wang et al. [33] for stochastic quasi-Newton methods. We make an additional assumption
used by Wang et al. [33] to establish global convergence.
Assumption 3 For all k, ξk is independent of {x1, . . . , xk}, Eξk
exists σ > 0 such that Eξk
Our ﬁrst result follows from Wang et al. [33, Theorem 2.6], whose remaining assumptions are
satisﬁed as a consequence of (2), Theorem 2, the mechanism of Algorithm 1, (4) and our choice of
αk below.

[(cid:107)g(xk, ξk) − ∇f (xk)(cid:107)2] ≤ σ2.

[g(xk, ξk)] = ∇f (xk), and there

Theorem 3 Assume mk = m for all k, that Assumptions 1 to 3 hold for {xk} generated by
Algorithm 1, and that αk := c/(k +1) where 0 < c ≤ λmin/(Lλmax). Then, lim inf (cid:107)∇f (xk)(cid:107) = 0
with probability 1. Moreover, there is Mf > 0 such that E[f (xk)] ≤ Mf for all k. If we additionally
[(cid:107)g(xk, ξk)(cid:107)2] ≤ Mg, then lim (cid:107)∇f (xk)(cid:107) = 0 with
assume that there exists Mg > 0 such that Eξk
probability 1.

Our next result follows in the same way from Wang et al. [33, Theorem 2.8].
Theorem 4 Under the assumptions of Theorem 3, if αk = λmin/(Lλ2
β ∈ ( 1

2 , 1), then, for any (cid:15) ∈ (0, 1), after at most T = O((cid:15)−1/(1−β)) iterations, we achieve

max)k−β for all k > 0, with

1
T

T
(cid:88)

k=1

E

(cid:104)

(cid:107)∇f (xk)(cid:107)2(cid:105)

≤ (cid:15).

(11)

5. Experimental results

We compare VARCHEN to SdLBFGS-VR [33] and to SVRG [17] for solving a multi-class classi-
ﬁcation problem. We train a modiﬁed version1 of the deep neural network model DavidNet2 pro-
posed by David C. Page, on CIFAR-10 [19] for 20 epochs. Note that we also used VARCHEN and
SdLBFGS-VR to solve a logistic regression problem using the MNIST dataset [20] and a noncon-
vex support-vector machine problem with a sigmoid loss function using the RCV1 dataset [21]. The
performance of both algorithms are on par on those problems because, in contrast with DavidNet
on CIFAR-10, they are not highly nonconvex or ill conditioned.
Figure 1 shows that VARCHEN outperforms SdLBFGS-VR for the training loss minimization task,
and both outperform SVRG. VARCHEN has an edge over SdLBFGS-VR in terms of the validation
accuracy, and both outperform SVRG. More importantly, the performance of VARCHEN is more
consistent than that of SdLBFGS-VR, displaying a smoother, less oscillatory behaviour. To further
investigate this observation, we plot the evolution of Λk and λk as shown in Figure 2. We see that
the estimate of the lower bound on the smallest eigenvalue is smaller for SdLBFGS-VR compared
to VARCHEN. We also notice that the estimate of the upper bound of the largest eigenvalue of Hk
takes even more extreme values for SdLBFGS-VR compared to VARCHEN. The extreme values λk
and Λk reﬂect an ill-conditioning problem encountered when using SdLBFGS-VR and we believe
that it explains the extreme oscillations in the performance of SdLBFGS-VR.

1. FastResNet Hyperparameters tuning with Ax on CIFAR10
2. https://myrtle.ai/learn/how-to-train-your-resnet-4-architecture/

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Figure 1: Evolution of the training loss (left) and the validation accuracy (right) for training a modiﬁed
DavidNet on CIFAR-10.

Figure 2: Evolution of the lower bound on the smallest eigenvalue λk (left) and the upper bound on the largest
eigenvalue Λk (right) on a base 10 logarithmic scale for training a modiﬁed DavidNet on CIFAR-10.

6. Conclusion

We used the stochastic damped L-BFGS algorithm in a nonconvex setting, where there are no guar-
antees that Hk remains well-conditioned and numerically nonsingular throughout. We introduced a
new stochastic damped L-BFGS algorithm that monitors the quality of Hk during the optimization
by maintaining bounds on its largest and smallest eigenvalues. Our work is the ﬁrst to address the
Hessian singularity problem by approximating and leveraging such bounds. Moreover, we proposed
a new initial inverse Hessian approximation that results in a smoother, less oscillatory training loss
and validation accuracy evolution. Additionally, we used variance reduction in order to improve
the quality of the curvature approximation and accelerate convergence. Our algorithm converges
almost-surely to a stationary point and numerical experiments have shown that it is more robust to
ill-conditioned problems and more suitable to the highly nonconvex context of deep learning than
SdLBFGS-VR. We consider this work to be a ﬁrst step towards the use of bounds estimates to con-
trol the quality of the Hessian approximation in approximate second-order algorithms. Future work
should aim to improve the quality of these bounds and explore another form of variance reduction
that consists of adaptive sampling [2, 3, 16].

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

                ( S R F K V             7 U D L Q  O R V V 9 $ 5 & + ( 1 6 G / % ) * 6  9 5 6 9 5 *                ( S R F K V           9 D O L G D W L R Q  D F F X U D F \ 9 $ 5 & + ( 1  6 G / % ) * 6  9 5 6 9 5 *                ( S R F K V        / R Z H U  E R X Q G   O R J     V F D O H  9 $ 5 & + ( 1 6 G / % ) * 6  9 5                ( S R F K V        8 S S H U  E R X Q G   O R J     V F D O H  9 $ 5 & + ( 1 6 G / % ) * 6  9 5STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

References

[1] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):

251–276, 1998.

[2] Raghu Bollapragada and Stefan M Wild. Adaptive sampling quasi-Newton methods for

derivative-free stochastic optimization. arXiv preprint arXiv:1910.13516, 2019.

[3] Raghu Bollapragada, Dheevatsa Mudigere, Jorge Nocedal, Hao-Jun Michael Shi, and Ping
Tak Peter Tang. A progressive batching l-bfgs method for machine learning. arXiv preprint
arXiv:1802.05374, 2018.

[4] Antoine Bordes, L´eon Bottou, and Patrick Gallinari. SGD-QN: Careful quasi-Newton stochas-
tic gradient descent. Journal of Machine Learning Research, 10(Jul):1737–1754, 2009.

[5] L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTAT’2010, pages 177–186. Springer, 2010.

[6] L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale ma-

chine learning. Siam Review, 60(2):223–311, 2018.

[7] Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-
Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–
1031, 2016.

[8] Huiming Chen, Ho-Chun Wu, Shing-Chow Chan, and Wong-Hing Lam. A stochastic quasi-
Newton method for large-scale nonconvex optimization with applications. IEEE Transactions
on Neural Networks and Learning Systems, 2019.

[9] Ron S Dembo, Stanley C Eisenstat, and Trond Steihaug.
Journal on Numerical analysis, 19(2):400–408, 1982.

Inexact Newton methods. SIAM

[10] John E Dennis and Jorge J Mor´e. A characterization of superlinear convergence and its appli-
cation to quasi-Newton methods. Mathematics of computation, 28(126):549–560, 1974.

[11] John E Dennis Jr and Robert B Schnabel. Numerical methods for unconstrained optimization

and nonlinear equations, volume 16. SIAM, Philadelphia, PA, 1996.

[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. Journal of machine learning research, 12(7), 2011.

[13] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. In Advances in Neu-
ral Information Processing Systems, pages 689–699, 2018.

[14] Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):

317–322, 1970.

[15] Robert Gower, Donald Goldfarb, and Peter Richt´arik. Stochastic block BFGS: Squeezing more
curvature out of data. In International Conference on Machine Learning, pages 1869–1878,
2016.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

[16] Afrooz Jalilzadeh, Angelia Nedi´c, Uday V Shanbhag, and Farzad Youseﬁan. A variable
sample-size stochastic quasi-Newton method for smooth and nonsmooth stochastic convex
optimization. In 2018 IEEE Conference on Decision and Control (CDC), pages 4097–4102.
IEEE, 2018.

[17] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in neural information processing systems, pages 315–323, 2013.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations, 2015.

[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,

University of Toronto, 2009.

[20] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[21] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection
for text categorization research. Journal of machine learning research, 5(Apr):361–397, 2004.

[22] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale opti-

mization. Mathematical programming, 45(1-3):503–528, 1989.

[23] Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE

Transactions on Signal Processing, 62(23):6089–6104, 2014.

[24] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-

BFGS algorithm. In Artiﬁcial Intelligence and Statistics, pages 249–258, 2016.

[25] Yurii E Nesterov. A method for solving the convex programming problem with convergence

rate O(1/k2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983.

[26] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak´aˇc. Sarah: A novel method for ma-
chine learning problems using stochastic recursive gradient. arXiv preprint arXiv:1703.00102,
2017.

[27] Jorge Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of com-

putation, 35(151):773–782, 1980.

[28] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964.

[29] Michael JD Powell. Algorithms for nonlinear constraints that use Lagrangian functions. Math-

ematical programming, 14(1):224–248, 1978.

[30] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of math-

ematical statistics, pages 400–407, 1951.

[31] Nicol N Schraudolph, Jin Yu, and Simon G¨unter. A stochastic quasi-Newton method for online

convex optimization. In Artiﬁcial intelligence and statistics, pages 436–443, 2007.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

[32] Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average
of its recent magnitude. coursera: Neural networks for machine learning. COURSERA Neural
Networks Mach. Learn, 2012.

[33] Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods
for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017.

[34] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems,
pages 2406–2416, 2019.

[35] Philip Wolfe. Convergence conditions for ascent methods. SIAM Review, 11(2):226–235,

1969.

[36] Philip Wolfe. Convergence conditions for ascent methods II: Some corrections. SIAM Review,

13(2):185–188, 1971.

[37] Farzad Youseﬁan, Angelia Nedi´c, and Uday V Shanbhag. A smoothing stochastic quasi-
Newton method for non-Lipschitzian stochastic optimization problems. In 2017 Winter Simu-
lation Conference (WSC), pages 2291–2302. IEEE, 2017.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Appendix A. Proofs

Proof of Lemma 1. First notice that

γ(cid:107)s(cid:107)2 ≤ s(cid:62)y ≤ (cid:107)s(cid:107)(cid:107)y(cid:107),

and thus (cid:107)s(cid:107) ≤

1
γ

(cid:107)y(cid:107).

Therefore,

1
(cid:107)s(cid:107)(cid:107)y(cid:107)

≤ ρ ≤

1
γ

1
(cid:107)s(cid:107)2 .

(12)

Since A is a real symmetric matrix, the spectral theorem states that its eigenvalues are real and it can
be diagonalized by an orthogonal matrix. That means that we can ﬁnd n orthogonal eigenvectors
and n eigenvalues counted with multiplicity.
Consider ﬁrst the special case where s and y are collinear, i.e. there exists θ > 0 such that y = θs.
Any vector such that u ∈ s⊥, where s⊥ = {x ∈ Rn : x(cid:62)s = 0}, is an eigenvector of A associated
with the eigenvalue µ of multiplicity n − 1. Moreover, s(cid:62)y = θ(cid:107)s(cid:107)2 = (cid:107)s(cid:107)(cid:107)y(cid:107), ρ = 1/(θ(cid:107)s(cid:107)2)
and we have

(cid:20)

(cid:16)

I − ρθss(cid:62)(cid:17)2

µ

+ ρss(cid:62)

(cid:21)

(cid:20)

s =

As =

(cid:16)

1 − ρθ(cid:107)s(cid:107)2(cid:17)2

µ

+ ρ(cid:107)s(cid:107)2

(cid:21)

s = ρ(cid:107)s(cid:107)2s.

Let us call λ = ρ(cid:107)s(cid:107)2, the eigenvalue associated with eigenvector s. From (12) and (cid:107)y(cid:107) ≤ Ly(cid:107)s(cid:107),
we deduce that

1
Ly

≤ λ ≤

1
γ

.

Suppose now that s and y are linearly independent. Any u such that u(cid:62)s = 0 = u(cid:62)y satisﬁes
Au = µu. This provides us with a (n − 2)-dimensional eigenspace S, associated to the eigenvalue
µ of multiplicity n − 2. Note that

As = ρ(cid:107)s(cid:107)2 (1 + µρ(cid:107)y(cid:107)2)s − ρ(cid:107)s(cid:107)2µy,
Ay = s.

Thus neither s nor y is an eigenvector of A. Now consider an eigenvalue λ associated with an
eigenvector u, such that u ∈ S⊥. Since s and y are linearly-independent, we can search for u of the
form u = s + βy with β > 0. The condition Au = λu yields

ρ(cid:107)s(cid:107)2 (1 + µρ(cid:107)y(cid:107)2) + β = λ,
−ρ(cid:107)s(cid:107)2µ = λβ.

We eliminate β = λ − ρ(cid:107)s(cid:107)2 (1 + µρ(cid:107)y(cid:107)2) and obtain

p(λ) = 0,

where

p(λ) = λ2 − λρ(cid:107)s(cid:107)2 (1 + µρ(cid:107)y(cid:107)2) + ρ(cid:107)s(cid:107)2µ.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

The roots of p must be the two remaining eigenvalues λ1 ≤ λ2 that we are looking for. In order to
establish the lower bound, we need a lower bound on λ1 whereas to establish the upper bound, we
need an upper bound on λ2.
On the one hand, let l be the tangent to the graph of p at λ = 0, deﬁned by

l(λ) = p(0) + p(cid:48)(0)λ = µρ(cid:107)s(cid:107)2 − λρ(cid:107)s(cid:107)2 (cid:16)

1 + µρ(cid:107)y(cid:107)2(cid:17)

.

Its unique root is

¯λ =

µ
1 + µρ(cid:107)y(cid:107)2 .

From (12) and since (cid:107)y(cid:107) ≤ Ly(cid:107)s(cid:107), we deduce that

¯λ ≥

µ
1 + µ
γ

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2

≥

µ
γ L2
1 + µ

y

.

Since p is convex, it remains above its tangent, and ¯λ ≤ λ1.
Finally,

λmin(A) ≥ min

(cid:32)

1
Ly

,

µ
γ L2
1 + µ

y

(cid:33)

> 0.

This establishes the lower bound.
On the other hand, the discriminant ∆ = ρ2(cid:107)s(cid:107)4(1 + µρ(cid:107)y(cid:107)2)2 − 4ρ(cid:107)s(cid:107)2µ must be nonnegative
since A is real symmetric, and its eigenvalues are real. We have

ρ(cid:107)s(cid:107)2(1 + µρ(cid:107)y(cid:107)2) +

(cid:113)

ρ2(cid:107)s(cid:107)4(1 + µρ(cid:107)y(cid:107)2)2 − 4ρ(cid:107)s(cid:107)2µ

2

.

λ2 =

For any positive a and b such that a2 − b > 0, we have

a2 − b ≤ a − b

2a . Thus,

(cid:112)

From (12), we deduce that

λ2 ≤ ρ(cid:107)s(cid:107)2(1 + µρ(cid:107)y(cid:107)2) −

µ
1 + µρ(cid:107)y(cid:107)2 .

λ2 ≤

1
γ

+

µ
γ2

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2 −

µ
1 + µ
γ

.

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2

And since (cid:107)y(cid:107) ≤ Ly(cid:107)s(cid:107), it follows

λ2 ≤

1
γ

+

µ

γ2 L2

y −

µ
γ L2
1 + µ

y

.

Finally,

λmax(A) ≤ max

(cid:32)

1
γ

,

1
γ

+

which establishes the upper bound.

µ

γ2 L2

y −

(cid:33)

,

µ
γ L2
1 + µ

y

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Proof of Theorem 2. Consider one damped BFGS update using s and y deﬁned in (5) and ˆyk
deﬁned in (7), i.e, p = 1,

Hk+1 = ˆVkH 0
Let 0 < µ1 := λmin(H 0

k + ˆρksks(cid:62)
ˆV (cid:62)
k+1
k+1) ≤ µ2 := λmax(H 0

k , where

k+1). We have

ˆρk = 1/s(cid:62)

k ˆyk,

ˆVk = I − ˆρksk ˆy(cid:62)
k .

λmin(µ1

ˆVk

k + ˆρksks(cid:62)
ˆV (cid:62)

k ) ≤ λmin(Hk+1) ≤ λmax(Hk+1) ≤ λmax(µ2

ˆVk

k + ˆρksks(cid:62)
ˆV (cid:62)
k ).

(13)

Let us show that we can apply Lemma 1 to

A1 := µ1

ˆVk

k + ˆρksks(cid:62)
ˆV (cid:62)
k

and A2 := µ2

ˆVk

k + ˆρksks(cid:62)
ˆV (cid:62)
k .

From (9), we obtain

k ˆyk ≥ ηλmin(B0
s(cid:62)

k+1)(cid:107)sk(cid:107)2 =

η
λmax(H 0

k+1)

(cid:107)sk(cid:107)2 =

η
µ2

(cid:107)sk(cid:107)2.

Assumption 2 yields

(cid:107)ˆyk(cid:107) = (cid:107)θkyk + (1 − θk)B0

k+1sk(cid:107) ≤ (cid:107)yk(cid:107) + (cid:107)B0

k+1sk(cid:107) ≤ (Lg + 1/µ1)(cid:107)sk(cid:107).

Therefore, we can ﬁrst apply Lemma 1 with sk, ˆyk, γ = η/µ2 > 0, Ly = Lg + 1/µ1 > 0 and
µ = µ1 > 0 for A1, and apply it again with µ = µ2 > 0 for A2. Let L1 := Lg + 1/µ1. Lemma 1
and (13) yield

λmin(Hk+1) ≥ min

(cid:32)

1
L1

,

λmax(Hk+1) ≤

µ2
η

+ max

(cid:33)

1

µ1
η L2
1 + µ1µ2

µ3
η2 L2
2
1 −

0,

> 0,



 .

µ2
1 + µ2
η L2

1

2

Now, consider the case where p > 1 and let

H (h+1)
k+1

:= ˆVk−hH (h)

k+1

k−h + ˆρk−hsk−hs(cid:62)
ˆV (cid:62)

k−h,

0 ≤ h ≤ p − 1,

where

H (p)

k+1 := Hk+1,

ˆρk−h = 1/s(cid:62)

k−h ˆyk−h,

ˆVk = I − ˆρk−hsk−h ˆy(cid:62)

k−h.

Similarly to the case p = 1, we may write

λmin(H (h+1)
λmax(H (h+1)

k+1 ) ≥ λmin(µ(h)
k+1 ) ≤ λmax(µ(h)

2

1

ˆVk−h
ˆVk−h

k−h + ˆρk−hsk−hs(cid:62)
ˆV (cid:62)
k−h + ˆρk−hsk−hs(cid:62)
ˆV (cid:62)

k−h),

k−h),

:= λmin(H (h)
µ(h)
1
2 := λmax(H (h)
µ(h)

k+1),
k+1).

Assume by recurrence that 0 < µ(h)

1 ≤ µ(h)

2 . We show that we can apply Lemma 1 to

A(h)
1

:= µ(h)
1

ˆVk−h

ˆV (cid:62)
k−h + ˆρk−hsk−hs(cid:62)

k−h

and A(h)
2

:= µ(h)
2

ˆVk−h

ˆV (cid:62)
k−h + ˆρk−hsk−hs(cid:62)

k−h.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

From (9), we have

k−h ˆyk−h ≥ ηλmin(B0
s(cid:62)

k−h+1)(cid:107)sk−h(cid:107)2 =

η
λmax(H 0
k−h+1)

(cid:107)sk−h(cid:107)2.

Using Assumption 2,

(cid:107)ˆyk−h(cid:107) = (cid:107)θk−hyk−h + (1 − θk−h)B0

k−h+1sk−h(cid:107) ≤ Lg(cid:107)sk−h(cid:107) + (cid:107)B0

k−h+1sk−h(cid:107),

so that

(cid:107)ˆyk−h(cid:107) ≤ (Lg +

1
λmin(H 0
k−h+1)

)(cid:107)sk−h(cid:107).

We ﬁrst apply Lemma 1 with s = sk−h, y = ˆyk−h, γ = η/λmax(H 0
1/λmin(H 0
for A(h)

k−h+1) > 0 and µ = µ(h)
2 . Let Lk−h+1 := Lg + 1/λmin(H 0


k−h+1) and γk−h+1 = η/λmax(H 0

1 > 0 for A(h)

k−h+1) > 0, Ly = Lg +
2 > 0
k−h+1). Then we have

1 , and apply it a second time with µ = µ(h)

λmin(H (h+1)

k+1 ) ≥ min




1
Lk−h+1

,

and

λmin(H (h)
k+1)
k+1)
(Lk−h+1)2

λmin(H (h)
γk−h+1

1 +




 ,

(14)

λmax(H (h+1)

k+1 ) ≤

1
γk−h+1

+ max




0,

λmax(H (h)
k+1)
γ2
k−h+1

L2

k−h+1 −

λmax(H (h)
k+1)
λmax(H (h)
k+1)
L2
γk−h+1

k−h+1

1 +




 .

(15)

It is clear that we can obtain the lower bound on λmin(Hk+1) recursively using (14). Obtaining the
upper bound on λmax(Hk+1) using (15) is trickier. However, we notice that inequality (15) implies

λmax(H (h+1)

k+1 ) ≤

1
γk−h+1

+ max




0,

λmax(H (h)
k+1)
γ2
k−h+1

L2

k−h+1 −

λmin(H (h)
k+1)
λmax(H (h)
k+1)
L2
γk−h+1

k−h+1

1 +




 .

This upper bound is less tight but it allows us to bound λmax(Hk+1) recursively.

Appendix B. Algorithms

The Two-loop recursion algorithm for evaluating the Hessian-gradient product is given by Algo-
rithm 2.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Algorithm 2 Two-loop recursion algorithm for Hessian-gradient product computation
Require: Current iterate xk, g(xk, ξk), ˆρi = 1/s(cid:62)
1: Deﬁne g = g(xk, ξk)
2: for i = k − 1, k − 2, . . . , k − p do
3:

i ˆyi, si, ˆyi for i ∈ {k − p, . . . , k − 1}.

Compute νi = ˆρis(cid:62)
i g
Compute g = g − νi ˆyi

k g using (10).

4:
5: Compute q = H 0
6: for i = k − p, k − p + 1, . . . , k − 1 do
7:

Compute µ = ˆρi ˆy(cid:62)
i q
Compute q = q + (νi − µ)si

8:
9: Return q = Hk g(xk, ξk)

Appendix C. Experimental Setting

CIFAR-10 is a dataset that contains 60, 000 colour images with labels in 10 classes (airplane, au-
tomobile, bird, cat, deer, dog, frog, horse, ship, and truck), each containing 6, 000 images. We use
50, 000 images for the training task and 10, 000 images for the validation task.
Details of the experiments:

• We apply our Hessian norm control using the bound on the maximum and the minimum

eigenvalues of Hk+1, where the latter is equivalent to controlling (cid:107)Bk+1(cid:107);

• In the deﬁnition of H 0

k+1 in (10), we choose γ

and γk+1 constant;

k+1

• The numerical values for all algorithms are the ones that yielded the best results among all

sets of values that we experimented with.

Numerical values:

• For all algorithms: we train the network for 20 epochs and use a batch size of 256 samples;

• For SVRG, we choose a step size equal to 0.001;

• For both SdLBFGS-VR and Algorithm 1, the memory parameter p = 10, the minimal scaling

parameter γ

k+1

= 0.1 for all k, the constant step size αk = 0.1 and η = 0.25;

• For Algorithm 1, we use a maximal scaling parameter γk+1 = 105 for all k, a lower bound

limit λmin = 10−5 and an upper bound limit λmax = 105.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

