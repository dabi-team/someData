0
2
0
2

c
e
D
0
1

]

G
L
.
s
c
[

1
v
3
8
7
5
0
.
2
1
0
2
:
v
i
X
r
a

Cahier du GERAD G-2020-52

Stochastic Damped L-BFGS with Controlled Norm of the Hessian
Approximation

Sanae Lotï¬
Tiphaine Bonniot de Ruisselet
Dominique Orban
Andrea Lodi
Polytechnique MontrÂ´eal, Canada

SANAE.LOTFI@POLYMTL.CA
TIPHAINE.BONNIOT@GMAIL.COM
DOMINIQUE.ORBAN@POLYMTL.CA
ANDREA.LODI@POLYMTL.CA

Abstract
We propose a new stochastic variance-reduced damped L-BFGS algorithm, where we leverage es-
timates of bounds on the largest and smallest eigenvalues of the Hessian approximation to balance
its quality and conditioning. Our algorithm, VARCHEN, draws from previous work that proposed
a novel stochastic damped L-BFGS algorithm called SdLBFGS. We establish almost sure conver-
gence to a stationary point and a complexity bound. We empirically demonstrate that VARCHEN
is more robust than SdLBFGS-VR and SVRG on a modiï¬ed DavidNet problemâ€”a highly noncon-
vex and ill-conditioned problem that arises in the context of deep learning, and their performance
is comparable on a logistic regression problem and a nonconvex support-vector machine problem.

1. Introduction and Related Work
We consider unconstrained stochastic minimization problems of f : Rn â†’ R where

f (x) = EÎ¾[F (x, Î¾)] (online)

or

f (x) =

1
N

N
(cid:88)

i=1

fi(x) (ï¬nite sum),

(1)

where Î¾ âˆˆ Rd denotes a random variable, F : Rn Ã— Rd â†’ R is continuously differentiable and
possibly nonconvex, fi is the loss corresponding to the i-th element of our dataset and N is the size
of the dataset. The algorithm developed below applies to both online and ï¬nite-sum problems.
Stochastic Gradient Descent (SGD) [5, 30] and its variants [12, 18, 25, 28, 32], including variance-
reduced algorithms [13, 17, 26, 34], are widely used to solve (1) in machine learning. However, they
might not be well-suited for highly nonconvex and ill-conditioned problems [6], which are more
effectively treated using (approximate) second-order information. Second-order algorithms are well
studied in the deterministic case [1, 9â€“11] but there are many areas to explore in the stochastic
context that go beyond existing works [4, 7, 15, 24, 31]. Among these areas, the use of damping in
L-BFGS is an interesting research direction to be leveraged in the stochastic case. Wang et al. [33]
proposed a stochastic damped L-BFGS (SdLBFGS) algorithm and proved almost sure convergence
to a stationary point. However, damping does not prevent the inverse Hessian approximation Hk
from being ill-conditioned [8]. The convergence of SdLBFGS may be heavily affected if the Hessian
approximation becomes nearly singular during the iterations. In order to remedy this issue, Chen
et al. [8] proposed to combine SdLBFGS with regularized BFGS [23]. Our approach differs.

1

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

 
 
 
 
 
 
STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Our contributions:

â€¢ Less restrictive assumptions: we force Hk to be uniformly bounded and positive deï¬nite
by requiring the stochastic gradient to be Lipschitz continuous, which is a less restrictive
assumption than those of Wang et al. [33], who require the stochastic function to be twice
differentiable with respect to the parameter vector, and its Hessian to be bounded for all
parameter and random sampling vectors.

â€¢ A new damped L-BFGS: we propose a new version of stochastic damped L-BFGS that main-
tains estimates of the smallest and largest eigenvalues of Hk. A solution is proposed when
ill-conditioning is detected that preserves almost sure convergence to a stationary point.

â€¢ Choice of the initial inverse Hessian approximation: we propose a new formula for the initial

inverse Hessian approximation to make the algorithm more robust to ill-conditioning.

Notation For a symmetric matrix A, we use A (cid:31) 0 to indicate that A is positive deï¬nite, and
Î»min(A) and Î»max(A) to denote its smallest and largest eigenvalue, respectively. If B is also sym-
metric, B (cid:22) A means that A âˆ’ B is positive semideï¬nite. The identity matrix of appropriate size is
denoted I. Finally, EÎ¾[.] is the expectation over random variable Î¾.

2. Formulation of our method

We assume that at iteration k, we can obtain a stochastic approximation

g(xk, Î¾k) =

1
mk

mk(cid:88)

i=1

âˆ‡fÎ¾k,i

(xk)

(2)

of âˆ‡f (xk), where Î¾k denotes the subset of samples taken from a given set of realizations of Î¾.
The Hessian approximation constructed at iteration k and its inverse are denoted by Bk and Hk,
respectively, such that Hk = Bâˆ’1
k

and Bk (cid:31) 0. Iterates are updated according to

xk+1 = xk + Î±kdk, where dk = âˆ’Hkg(xk, Î¾k)

and Î±k > 0 is the step size.

(3)

The stochastic BFGS method [31] computes an updated approximation Hk+1 according to

Hk+1 = VkHkV (cid:62)

k + Ïksks(cid:62)

k , where

Vk = I âˆ’ Ïksky(cid:62)
k

and Ïk = 1/s(cid:62)

k yk,

(4)

which ensures that the secant equation Bk+1sk = yk is satisï¬ed, where

sk = xk+1 âˆ’ xk,

and

yk = g(xk+1, Î¾k) âˆ’ g(xk, Î¾k).

(5)

If Hk (cid:31) 0 and the curvature condition s(cid:62)
Because storing Hk and performing matrix-vector products is costly for large-scale problems, we
use the limited-memory version of BFGS (L-BFGS) [22, 27], in which Hk only depends on the
most recent p iterations and an initial H 0
k (cid:31) 0. The parameter p is the memory of L-BFGS. The
inverse Hessian update can be written as

k yk > 0 holds, then Hk+1 (cid:31) 0 [see, for instance, 14].

Hk = (V (cid:62)

kâˆ’1 . . . V (cid:62)

k (Vkâˆ’p . . . Vkâˆ’1)+

kâˆ’p)H 0
kâˆ’1 . . . V (cid:62)

Ïkâˆ’p(V (cid:62)

kâˆ’p+1)skâˆ’ps(cid:62)

kâˆ’p(Vkâˆ’p+1 . . . Vkâˆ’1) + Â· Â· Â· + Ïkâˆ’1skâˆ’1s(cid:62)

kâˆ’1.

(6)

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

When Î±k in (3) is not computed using a Wolfe line search [35, 36], there is no guarantee that the
curvature condition holds. A common strategy is to simply skip the update. By contrast, Powell [29]
proposed damping, which consists in updating Hk using a modiï¬ed yk, denoted by Ë†yk, to beneï¬t
from information discovered at iteration k while ensuring sufï¬cient positive deï¬niteness. We use

Ë†yk := Î¸kyk + (1 âˆ’ Î¸k)B0

k+1sk,

(7)

which is inspired by [33], and differs from the original proposal of Powell [29], where

Î¸k = 1 if s(cid:62)

k yk â‰¥ Î·s(cid:62)

k B0

k+1sk, and (1 âˆ’ Î·)

s(cid:62)
k B0
k+1sk
k+1sk âˆ’ s(cid:62)
k B0
s(cid:62)

k yk

otherwise,

(8)

with Î· âˆˆ (0, 1) and B0

k+1 := (H 0

k+1)âˆ’1. The choice (7) ensures that the curvature condition

k Ë†yk â‰¥ Î·s(cid:62)
s(cid:62)

k B0

k+1sk â‰¥ Î·Î»min(B0

k+1)(cid:107)sk(cid:107)2 > 0,

(9)

is always satisï¬ed since H 0
Vi and Ïi replaced with Ë†Vi = I âˆ’ Ë†Ïis(cid:62)

i Ë†yi and Ë†Ïi = 1/s(cid:62)

i Ë†yi.

k+1 (cid:31) 0. We obtain the damped L-BFGS update, which is (6) with each

3. A new stochastic damped L-BFGS with controlled Hessian norm

Our working assumption is
Assumption 1 There is Îºlow âˆˆ R such that f (x) â‰¥ Îºlow for all x âˆˆ Rn, f is C1 over Rn, and there
is L > 0 such that for all x, y âˆˆ Rn, (cid:107)âˆ‡f (x) âˆ’ âˆ‡f (y)(cid:107) â‰¤ L (cid:107)x âˆ’ y(cid:107).

We begin by deriving bounds on the smallest and largest eigenvalues of Hk+1 as functions of bounds
on those of Hk. Proofs can be found in Appendix A.

Lemma 1 Let s and y âˆˆ Rn such that s(cid:62)y â‰¥ Î³(cid:107)s(cid:107)2 with Î³ > 0, and such that (cid:107)y(cid:107) â‰¤ Ly(cid:107)s(cid:107), with
Ly > 0. Let A = ÂµV V (cid:62) + Ïss(cid:62), where Ï = 1/s(cid:62)y, Âµ > 0, and V = I âˆ’ Ïsy(cid:62). Then,

0 < min

(cid:33)

(cid:32)

1
Ly

,

Âµ
Î³ L2
1 + Âµ

y

â‰¤ Î»min(A) â‰¤ Î»max(A) â‰¤

(cid:32)

+ max

0,

1
Î³

Âµ

Î³2 L2

y âˆ’

(cid:33)

.

Âµ
Î³ L2
1 + Âµ

y

To use Lemma 1 to obtain bounds on the eigenvalues of Hk+1, we make the following assumption:
Assumption 2 There is Lg > 0 such that for all x, y âˆˆ Rn, (cid:107)g(x, Î¾) âˆ’ g(y, Î¾)(cid:107) â‰¤ Lg (cid:107)x âˆ’ y(cid:107).

Assumption 2 is required to prove convergence and convergence rates for most recent stochastic
quasi-Newton methods [37]. It is less restrictive than requiring f (x, Î¾) to be twice differentiable
with respect to x, and the Hessian âˆ‡2
The next theorem shows that the eigenvalues of Hk+1 are bounded and bounded away from zero.

xxf (x, Î¾) to be bounded for any x, Î¾, as in [33].

Theorem 2 Let Assumptions 1 and 2 hold. Let H 0
by applying p times the damped BFGS update formula with inexact gradient to H 0
easily computable constants Î»k+1 and Î›k+1 that depend on Lg and H 0
Î»min(Hk+1) â‰¤ Î»max(Hk+1) â‰¤ Î›k+1.

If Hk+1 is obtained
k+1, there exist
k+1 such that 0 < Î»k+1 â‰¤

k+1 (cid:31) 0 and p > 0.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

The precise form of Î»k+1 and Î›k+1 is given in Appendix A.
A common choice for H 0
k+1 is H 0
k yk, is the scaling parameter.
This choice ensures that the search direction is well scaled, which promotes large steps. To keep
H 0

k+1 from becoming nearly singular or non positive deï¬nite, we deï¬ne

k+1I where Î³k+1 = y(cid:62)

k+1 = Î³âˆ’1

k yk/s(cid:62)

(cid:16)

H 0

k+1 =

max(Î³

k+1

, min(Î³k+1, Î³k+1))

(cid:17)

I,

(10)

k+1

< Î³k+1 can be constants or iteration dependent.

where 0 < Î³
The Hessian-gradient product used to compute the search direction dk = âˆ’Hkg(xk, Î¾k) can be ob-
tained cheaply by exploiting a recursive algorithm [27], as described in Algorithm 2 in Appendix B.
Motivated by the success of recent methods combining variance reduction with stochastic L-BFGS
[15, 24, 33], we apply an SVRG-like type of variance reduction [17] to the update. Not only would
this accelerate the convergence, since we can choose a constant step size, but it also improves the
quality of the curvature approximation.
We summarize our complete algorithm, VAriance-Reduced stochastic damped L-BFGS with Con-
trolled HEssian Norm (VARCHEN), as Algorithm 1.

Algorithm 1 Variance-Reduced Stochastic Damped L-BFGS with Controlled Hessian Norm
1: Choose x0 âˆˆ Rn, step size sequence {Î±k > 0}kâ‰¥0, batch size sequence {mk > 0}kâ‰¥0, eigen-
value limits Î»max > Î»min > 0, memory parameter p, total number of epochs Nepochs, and
sequences {Î³
< Î³k < Î»max, for
every k â‰¥ 0. Set k = 0 and H0 = I.

> 0}kâ‰¥0 and {Î³k+1 > 0}kâ‰¥0, such that 0 < Î»min < Î³

k

k

2: for t = 1, . . . , Nepochs do
3:

Deï¬ne xt
while M < N do

k = xk and compute the full gradient âˆ‡f (xt

k). Set M = 0.

Sample batch Î¾k of size mk â‰¤ N âˆ’ M and compute g(xk, Î¾k) and g(xt
Deï¬ne Ëœg(xk, Î¾k) = g(xk, Î¾k) âˆ’ g(xt
k, Î¾k) + âˆ‡f (xt
Estimate Î›k and Î»k in Theorem 2. If Î›k > Î»max or Î»k < Î»min, delete si, yi and Ë†yi for
i = k âˆ’ p + 1, . . . , k âˆ’ 2.
Compute dk = âˆ’HkËœg(xk, Î¾k).
Deï¬ne xk+1 = xk + Î±kdk, and compute sk, yk as in (5), and Ë†yk as in (7).
Increment k by one and update M â† M + mk.

k, Î¾k).

k).

4:

5:

6:

7:

8:

9:

10:

In step 7 of Algorithm 1, we compute an estimate of the upper and lower bounds on Î»max(Hk) and
Î»min(Hk), respectively. The only unknown quantity in the expressions of Î›k and Î»k in Theorem 2
is Lg, which we estimate as Lg â‰ˆ Lg,k := (cid:107)yk(cid:107)/(cid:107)sk(cid:107). When the estimates are not within the limits
[Î»min, Î»max], we delete si, yi and Ë†yi, i âˆˆ {k âˆ’p+1, . . . , k âˆ’2} from storage, such that Hkg(xk, Î¾k)
is computed using the most recent pair (skâˆ’1, Ë†ykâˆ’1) only and dk = âˆ’Hkg(xk, Î¾k). Finally, a full
gradient is computed once in every epoch in step 3. The term g(xt
k) can be seen as the
bias in the gradient estimation g(xk, Î¾k), and it is used here to correct the gradient approximation in
step 6.

k, Î¾k) âˆ’ âˆ‡f (xt

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

4. Convergence and Complexity Analysis

We show that Algorithm 1 satisï¬es the assumptions of the convergence analysis and iteration com-
plexity of Wang et al. [33] for stochastic quasi-Newton methods. We make an additional assumption
used by Wang et al. [33] to establish global convergence.
Assumption 3 For all k, Î¾k is independent of {x1, . . . , xk}, EÎ¾k
exists Ïƒ > 0 such that EÎ¾k
Our ï¬rst result follows from Wang et al. [33, Theorem 2.6], whose remaining assumptions are
satisï¬ed as a consequence of (2), Theorem 2, the mechanism of Algorithm 1, (4) and our choice of
Î±k below.

[(cid:107)g(xk, Î¾k) âˆ’ âˆ‡f (xk)(cid:107)2] â‰¤ Ïƒ2.

[g(xk, Î¾k)] = âˆ‡f (xk), and there

Theorem 3 Assume mk = m for all k, that Assumptions 1 to 3 hold for {xk} generated by
Algorithm 1, and that Î±k := c/(k +1) where 0 < c â‰¤ Î»min/(LÎ»max). Then, lim inf (cid:107)âˆ‡f (xk)(cid:107) = 0
with probability 1. Moreover, there is Mf > 0 such that E[f (xk)] â‰¤ Mf for all k. If we additionally
[(cid:107)g(xk, Î¾k)(cid:107)2] â‰¤ Mg, then lim (cid:107)âˆ‡f (xk)(cid:107) = 0 with
assume that there exists Mg > 0 such that EÎ¾k
probability 1.

Our next result follows in the same way from Wang et al. [33, Theorem 2.8].
Theorem 4 Under the assumptions of Theorem 3, if Î±k = Î»min/(LÎ»2
Î² âˆˆ ( 1

2 , 1), then, for any (cid:15) âˆˆ (0, 1), after at most T = O((cid:15)âˆ’1/(1âˆ’Î²)) iterations, we achieve

max)kâˆ’Î² for all k > 0, with

1
T

T
(cid:88)

k=1

E

(cid:104)

(cid:107)âˆ‡f (xk)(cid:107)2(cid:105)

â‰¤ (cid:15).

(11)

5. Experimental results

We compare VARCHEN to SdLBFGS-VR [33] and to SVRG [17] for solving a multi-class classi-
ï¬cation problem. We train a modiï¬ed version1 of the deep neural network model DavidNet2 pro-
posed by David C. Page, on CIFAR-10 [19] for 20 epochs. Note that we also used VARCHEN and
SdLBFGS-VR to solve a logistic regression problem using the MNIST dataset [20] and a noncon-
vex support-vector machine problem with a sigmoid loss function using the RCV1 dataset [21]. The
performance of both algorithms are on par on those problems because, in contrast with DavidNet
on CIFAR-10, they are not highly nonconvex or ill conditioned.
Figure 1 shows that VARCHEN outperforms SdLBFGS-VR for the training loss minimization task,
and both outperform SVRG. VARCHEN has an edge over SdLBFGS-VR in terms of the validation
accuracy, and both outperform SVRG. More importantly, the performance of VARCHEN is more
consistent than that of SdLBFGS-VR, displaying a smoother, less oscillatory behaviour. To further
investigate this observation, we plot the evolution of Î›k and Î»k as shown in Figure 2. We see that
the estimate of the lower bound on the smallest eigenvalue is smaller for SdLBFGS-VR compared
to VARCHEN. We also notice that the estimate of the upper bound of the largest eigenvalue of Hk
takes even more extreme values for SdLBFGS-VR compared to VARCHEN. The extreme values Î»k
and Î›k reï¬‚ect an ill-conditioning problem encountered when using SdLBFGS-VR and we believe
that it explains the extreme oscillations in the performance of SdLBFGS-VR.

1. FastResNet Hyperparameters tuning with Ax on CIFAR10
2. https://myrtle.ai/learn/how-to-train-your-resnet-4-architecture/

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Figure 1: Evolution of the training loss (left) and the validation accuracy (right) for training a modiï¬ed
DavidNet on CIFAR-10.

Figure 2: Evolution of the lower bound on the smallest eigenvalue Î»k (left) and the upper bound on the largest
eigenvalue Î›k (right) on a base 10 logarithmic scale for training a modiï¬ed DavidNet on CIFAR-10.

6. Conclusion

We used the stochastic damped L-BFGS algorithm in a nonconvex setting, where there are no guar-
antees that Hk remains well-conditioned and numerically nonsingular throughout. We introduced a
new stochastic damped L-BFGS algorithm that monitors the quality of Hk during the optimization
by maintaining bounds on its largest and smallest eigenvalues. Our work is the ï¬rst to address the
Hessian singularity problem by approximating and leveraging such bounds. Moreover, we proposed
a new initial inverse Hessian approximation that results in a smoother, less oscillatory training loss
and validation accuracy evolution. Additionally, we used variance reduction in order to improve
the quality of the curvature approximation and accelerate convergence. Our algorithm converges
almost-surely to a stationary point and numerical experiments have shown that it is more robust to
ill-conditioned problems and more suitable to the highly nonconvex context of deep learning than
SdLBFGS-VR. We consider this work to be a ï¬rst step towards the use of bounds estimates to con-
trol the quality of the Hessian approximation in approximate second-order algorithms. Future work
should aim to improve the quality of these bounds and explore another form of variance reduction
that consists of adaptive sampling [2, 3, 16].

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

                ( S R F K V             7 U D L Q  O R V V 9 $ 5 & + ( 1 6 G / % ) * 6  9 5 6 9 5 *                ( S R F K V           9 D O L G D W L R Q  D F F X U D F \ 9 $ 5 & + ( 1  6 G / % ) * 6  9 5 6 9 5 *                ( S R F K V        / R Z H U  E R X Q G   O R J     V F D O H  9 $ 5 & + ( 1 6 G / % ) * 6  9 5                ( S R F K V        8 S S H U  E R X Q G   O R J     V F D O H  9 $ 5 & + ( 1 6 G / % ) * 6  9 5STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

References

[1] Shun-Ichi Amari. Natural gradient works efï¬ciently in learning. Neural computation, 10(2):

251â€“276, 1998.

[2] Raghu Bollapragada and Stefan M Wild. Adaptive sampling quasi-Newton methods for

derivative-free stochastic optimization. arXiv preprint arXiv:1910.13516, 2019.

[3] Raghu Bollapragada, Dheevatsa Mudigere, Jorge Nocedal, Hao-Jun Michael Shi, and Ping
Tak Peter Tang. A progressive batching l-bfgs method for machine learning. arXiv preprint
arXiv:1802.05374, 2018.

[4] Antoine Bordes, LÂ´eon Bottou, and Patrick Gallinari. SGD-QN: Careful quasi-Newton stochas-
tic gradient descent. Journal of Machine Learning Research, 10(Jul):1737â€“1754, 2009.

[5] LÂ´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTATâ€™2010, pages 177â€“186. Springer, 2010.

[6] LÂ´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale ma-

chine learning. Siam Review, 60(2):223â€“311, 2018.

[7] Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-
Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008â€“
1031, 2016.

[8] Huiming Chen, Ho-Chun Wu, Shing-Chow Chan, and Wong-Hing Lam. A stochastic quasi-
Newton method for large-scale nonconvex optimization with applications. IEEE Transactions
on Neural Networks and Learning Systems, 2019.

[9] Ron S Dembo, Stanley C Eisenstat, and Trond Steihaug.
Journal on Numerical analysis, 19(2):400â€“408, 1982.

Inexact Newton methods. SIAM

[10] John E Dennis and Jorge J MorÂ´e. A characterization of superlinear convergence and its appli-
cation to quasi-Newton methods. Mathematics of computation, 28(126):549â€“560, 1974.

[11] John E Dennis Jr and Robert B Schnabel. Numerical methods for unconstrained optimization

and nonlinear equations, volume 16. SIAM, Philadelphia, PA, 1996.

[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. Journal of machine learning research, 12(7), 2011.

[13] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. In Advances in Neu-
ral Information Processing Systems, pages 689â€“699, 2018.

[14] Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):

317â€“322, 1970.

[15] Robert Gower, Donald Goldfarb, and Peter RichtÂ´arik. Stochastic block BFGS: Squeezing more
curvature out of data. In International Conference on Machine Learning, pages 1869â€“1878,
2016.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

[16] Afrooz Jalilzadeh, Angelia NediÂ´c, Uday V Shanbhag, and Farzad Youseï¬an. A variable
sample-size stochastic quasi-Newton method for smooth and nonsmooth stochastic convex
optimization. In 2018 IEEE Conference on Decision and Control (CDC), pages 4097â€“4102.
IEEE, 2018.

[17] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in neural information processing systems, pages 315â€“323, 2013.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations, 2015.

[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,

University of Toronto, 2009.

[20] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[21] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection
for text categorization research. Journal of machine learning research, 5(Apr):361â€“397, 2004.

[22] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale opti-

mization. Mathematical programming, 45(1-3):503â€“528, 1989.

[23] Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE

Transactions on Signal Processing, 62(23):6089â€“6104, 2014.

[24] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-

BFGS algorithm. In Artiï¬cial Intelligence and Statistics, pages 249â€“258, 2016.

[25] Yurii E Nesterov. A method for solving the convex programming problem with convergence

rate O(1/k2). In Dokl. akad. nauk Sssr, volume 269, pages 543â€“547, 1983.

[26] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin TakÂ´aË‡c. Sarah: A novel method for ma-
chine learning problems using stochastic recursive gradient. arXiv preprint arXiv:1703.00102,
2017.

[27] Jorge Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of com-

putation, 35(151):773â€“782, 1980.

[28] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics, 4(5):1â€“17, 1964.

[29] Michael JD Powell. Algorithms for nonlinear constraints that use Lagrangian functions. Math-

ematical programming, 14(1):224â€“248, 1978.

[30] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of math-

ematical statistics, pages 400â€“407, 1951.

[31] Nicol N Schraudolph, Jin Yu, and Simon GÂ¨unter. A stochastic quasi-Newton method for online

convex optimization. In Artiï¬cial intelligence and statistics, pages 436â€“443, 2007.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

[32] Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average
of its recent magnitude. coursera: Neural networks for machine learning. COURSERA Neural
Networks Mach. Learn, 2012.

[33] Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods
for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927â€“956, 2017.

[34] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems,
pages 2406â€“2416, 2019.

[35] Philip Wolfe. Convergence conditions for ascent methods. SIAM Review, 11(2):226â€“235,

1969.

[36] Philip Wolfe. Convergence conditions for ascent methods II: Some corrections. SIAM Review,

13(2):185â€“188, 1971.

[37] Farzad Youseï¬an, Angelia NediÂ´c, and Uday V Shanbhag. A smoothing stochastic quasi-
Newton method for non-Lipschitzian stochastic optimization problems. In 2017 Winter Simu-
lation Conference (WSC), pages 2291â€“2302. IEEE, 2017.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Appendix A. Proofs

Proof of Lemma 1. First notice that

Î³(cid:107)s(cid:107)2 â‰¤ s(cid:62)y â‰¤ (cid:107)s(cid:107)(cid:107)y(cid:107),

and thus (cid:107)s(cid:107) â‰¤

1
Î³

(cid:107)y(cid:107).

Therefore,

1
(cid:107)s(cid:107)(cid:107)y(cid:107)

â‰¤ Ï â‰¤

1
Î³

1
(cid:107)s(cid:107)2 .

(12)

Since A is a real symmetric matrix, the spectral theorem states that its eigenvalues are real and it can
be diagonalized by an orthogonal matrix. That means that we can ï¬nd n orthogonal eigenvectors
and n eigenvalues counted with multiplicity.
Consider ï¬rst the special case where s and y are collinear, i.e. there exists Î¸ > 0 such that y = Î¸s.
Any vector such that u âˆˆ sâŠ¥, where sâŠ¥ = {x âˆˆ Rn : x(cid:62)s = 0}, is an eigenvector of A associated
with the eigenvalue Âµ of multiplicity n âˆ’ 1. Moreover, s(cid:62)y = Î¸(cid:107)s(cid:107)2 = (cid:107)s(cid:107)(cid:107)y(cid:107), Ï = 1/(Î¸(cid:107)s(cid:107)2)
and we have

(cid:20)

(cid:16)

I âˆ’ ÏÎ¸ss(cid:62)(cid:17)2

Âµ

+ Ïss(cid:62)

(cid:21)

(cid:20)

s =

As =

(cid:16)

1 âˆ’ ÏÎ¸(cid:107)s(cid:107)2(cid:17)2

Âµ

+ Ï(cid:107)s(cid:107)2

(cid:21)

s = Ï(cid:107)s(cid:107)2s.

Let us call Î» = Ï(cid:107)s(cid:107)2, the eigenvalue associated with eigenvector s. From (12) and (cid:107)y(cid:107) â‰¤ Ly(cid:107)s(cid:107),
we deduce that

1
Ly

â‰¤ Î» â‰¤

1
Î³

.

Suppose now that s and y are linearly independent. Any u such that u(cid:62)s = 0 = u(cid:62)y satisï¬es
Au = Âµu. This provides us with a (n âˆ’ 2)-dimensional eigenspace S, associated to the eigenvalue
Âµ of multiplicity n âˆ’ 2. Note that

As = Ï(cid:107)s(cid:107)2 (1 + ÂµÏ(cid:107)y(cid:107)2)s âˆ’ Ï(cid:107)s(cid:107)2Âµy,
Ay = s.

Thus neither s nor y is an eigenvector of A. Now consider an eigenvalue Î» associated with an
eigenvector u, such that u âˆˆ SâŠ¥. Since s and y are linearly-independent, we can search for u of the
form u = s + Î²y with Î² > 0. The condition Au = Î»u yields

Ï(cid:107)s(cid:107)2 (1 + ÂµÏ(cid:107)y(cid:107)2) + Î² = Î»,
âˆ’Ï(cid:107)s(cid:107)2Âµ = Î»Î².

We eliminate Î² = Î» âˆ’ Ï(cid:107)s(cid:107)2 (1 + ÂµÏ(cid:107)y(cid:107)2) and obtain

p(Î») = 0,

where

p(Î») = Î»2 âˆ’ Î»Ï(cid:107)s(cid:107)2 (1 + ÂµÏ(cid:107)y(cid:107)2) + Ï(cid:107)s(cid:107)2Âµ.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

The roots of p must be the two remaining eigenvalues Î»1 â‰¤ Î»2 that we are looking for. In order to
establish the lower bound, we need a lower bound on Î»1 whereas to establish the upper bound, we
need an upper bound on Î»2.
On the one hand, let l be the tangent to the graph of p at Î» = 0, deï¬ned by

l(Î») = p(0) + p(cid:48)(0)Î» = ÂµÏ(cid:107)s(cid:107)2 âˆ’ Î»Ï(cid:107)s(cid:107)2 (cid:16)

1 + ÂµÏ(cid:107)y(cid:107)2(cid:17)

.

Its unique root is

Â¯Î» =

Âµ
1 + ÂµÏ(cid:107)y(cid:107)2 .

From (12) and since (cid:107)y(cid:107) â‰¤ Ly(cid:107)s(cid:107), we deduce that

Â¯Î» â‰¥

Âµ
1 + Âµ
Î³

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2

â‰¥

Âµ
Î³ L2
1 + Âµ

y

.

Since p is convex, it remains above its tangent, and Â¯Î» â‰¤ Î»1.
Finally,

Î»min(A) â‰¥ min

(cid:32)

1
Ly

,

Âµ
Î³ L2
1 + Âµ

y

(cid:33)

> 0.

This establishes the lower bound.
On the other hand, the discriminant âˆ† = Ï2(cid:107)s(cid:107)4(1 + ÂµÏ(cid:107)y(cid:107)2)2 âˆ’ 4Ï(cid:107)s(cid:107)2Âµ must be nonnegative
since A is real symmetric, and its eigenvalues are real. We have

Ï(cid:107)s(cid:107)2(1 + ÂµÏ(cid:107)y(cid:107)2) +

(cid:113)

Ï2(cid:107)s(cid:107)4(1 + ÂµÏ(cid:107)y(cid:107)2)2 âˆ’ 4Ï(cid:107)s(cid:107)2Âµ

2

.

Î»2 =

For any positive a and b such that a2 âˆ’ b > 0, we have

a2 âˆ’ b â‰¤ a âˆ’ b

2a . Thus,

(cid:112)

From (12), we deduce that

Î»2 â‰¤ Ï(cid:107)s(cid:107)2(1 + ÂµÏ(cid:107)y(cid:107)2) âˆ’

Âµ
1 + ÂµÏ(cid:107)y(cid:107)2 .

Î»2 â‰¤

1
Î³

+

Âµ
Î³2

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2 âˆ’

Âµ
1 + Âµ
Î³

.

(cid:107)y(cid:107)2
(cid:107)s(cid:107)2

And since (cid:107)y(cid:107) â‰¤ Ly(cid:107)s(cid:107), it follows

Î»2 â‰¤

1
Î³

+

Âµ

Î³2 L2

y âˆ’

Âµ
Î³ L2
1 + Âµ

y

.

Finally,

Î»max(A) â‰¤ max

(cid:32)

1
Î³

,

1
Î³

+

which establishes the upper bound.

Âµ

Î³2 L2

y âˆ’

(cid:33)

,

Âµ
Î³ L2
1 + Âµ

y

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Proof of Theorem 2. Consider one damped BFGS update using s and y deï¬ned in (5) and Ë†yk
deï¬ned in (7), i.e, p = 1,

Hk+1 = Ë†VkH 0
Let 0 < Âµ1 := Î»min(H 0

k + Ë†Ïksks(cid:62)
Ë†V (cid:62)
k+1
k+1) â‰¤ Âµ2 := Î»max(H 0

k , where

k+1). We have

Ë†Ïk = 1/s(cid:62)

k Ë†yk,

Ë†Vk = I âˆ’ Ë†Ïksk Ë†y(cid:62)
k .

Î»min(Âµ1

Ë†Vk

k + Ë†Ïksks(cid:62)
Ë†V (cid:62)

k ) â‰¤ Î»min(Hk+1) â‰¤ Î»max(Hk+1) â‰¤ Î»max(Âµ2

Ë†Vk

k + Ë†Ïksks(cid:62)
Ë†V (cid:62)
k ).

(13)

Let us show that we can apply Lemma 1 to

A1 := Âµ1

Ë†Vk

k + Ë†Ïksks(cid:62)
Ë†V (cid:62)
k

and A2 := Âµ2

Ë†Vk

k + Ë†Ïksks(cid:62)
Ë†V (cid:62)
k .

From (9), we obtain

k Ë†yk â‰¥ Î·Î»min(B0
s(cid:62)

k+1)(cid:107)sk(cid:107)2 =

Î·
Î»max(H 0

k+1)

(cid:107)sk(cid:107)2 =

Î·
Âµ2

(cid:107)sk(cid:107)2.

Assumption 2 yields

(cid:107)Ë†yk(cid:107) = (cid:107)Î¸kyk + (1 âˆ’ Î¸k)B0

k+1sk(cid:107) â‰¤ (cid:107)yk(cid:107) + (cid:107)B0

k+1sk(cid:107) â‰¤ (Lg + 1/Âµ1)(cid:107)sk(cid:107).

Therefore, we can ï¬rst apply Lemma 1 with sk, Ë†yk, Î³ = Î·/Âµ2 > 0, Ly = Lg + 1/Âµ1 > 0 and
Âµ = Âµ1 > 0 for A1, and apply it again with Âµ = Âµ2 > 0 for A2. Let L1 := Lg + 1/Âµ1. Lemma 1
and (13) yield

Î»min(Hk+1) â‰¥ min

(cid:32)

1
L1

,

Î»max(Hk+1) â‰¤

Âµ2
Î·

+ max

(cid:33)

1

Âµ1
Î· L2
1 + Âµ1Âµ2
ï£«
Âµ3
Î·2 L2
2
1 âˆ’

ï£­0,

> 0,

ï£¶

ï£¸ .

Âµ2
1 + Âµ2
Î· L2

1

2

Now, consider the case where p > 1 and let

H (h+1)
k+1

:= Ë†Vkâˆ’hH (h)

k+1

kâˆ’h + Ë†Ïkâˆ’hskâˆ’hs(cid:62)
Ë†V (cid:62)

kâˆ’h,

0 â‰¤ h â‰¤ p âˆ’ 1,

where

H (p)

k+1 := Hk+1,

Ë†Ïkâˆ’h = 1/s(cid:62)

kâˆ’h Ë†ykâˆ’h,

Ë†Vk = I âˆ’ Ë†Ïkâˆ’hskâˆ’h Ë†y(cid:62)

kâˆ’h.

Similarly to the case p = 1, we may write

Î»min(H (h+1)
Î»max(H (h+1)

k+1 ) â‰¥ Î»min(Âµ(h)
k+1 ) â‰¤ Î»max(Âµ(h)

2

1

Ë†Vkâˆ’h
Ë†Vkâˆ’h

kâˆ’h + Ë†Ïkâˆ’hskâˆ’hs(cid:62)
Ë†V (cid:62)
kâˆ’h + Ë†Ïkâˆ’hskâˆ’hs(cid:62)
Ë†V (cid:62)

kâˆ’h),

kâˆ’h),

:= Î»min(H (h)
Âµ(h)
1
2 := Î»max(H (h)
Âµ(h)

k+1),
k+1).

Assume by recurrence that 0 < Âµ(h)

1 â‰¤ Âµ(h)

2 . We show that we can apply Lemma 1 to

A(h)
1

:= Âµ(h)
1

Ë†Vkâˆ’h

Ë†V (cid:62)
kâˆ’h + Ë†Ïkâˆ’hskâˆ’hs(cid:62)

kâˆ’h

and A(h)
2

:= Âµ(h)
2

Ë†Vkâˆ’h

Ë†V (cid:62)
kâˆ’h + Ë†Ïkâˆ’hskâˆ’hs(cid:62)

kâˆ’h.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

From (9), we have

kâˆ’h Ë†ykâˆ’h â‰¥ Î·Î»min(B0
s(cid:62)

kâˆ’h+1)(cid:107)skâˆ’h(cid:107)2 =

Î·
Î»max(H 0
kâˆ’h+1)

(cid:107)skâˆ’h(cid:107)2.

Using Assumption 2,

(cid:107)Ë†ykâˆ’h(cid:107) = (cid:107)Î¸kâˆ’hykâˆ’h + (1 âˆ’ Î¸kâˆ’h)B0

kâˆ’h+1skâˆ’h(cid:107) â‰¤ Lg(cid:107)skâˆ’h(cid:107) + (cid:107)B0

kâˆ’h+1skâˆ’h(cid:107),

so that

(cid:107)Ë†ykâˆ’h(cid:107) â‰¤ (Lg +

1
Î»min(H 0
kâˆ’h+1)

)(cid:107)skâˆ’h(cid:107).

We ï¬rst apply Lemma 1 with s = skâˆ’h, y = Ë†ykâˆ’h, Î³ = Î·/Î»max(H 0
1/Î»min(H 0
for A(h)

kâˆ’h+1) > 0 and Âµ = Âµ(h)
2 . Let Lkâˆ’h+1 := Lg + 1/Î»min(H 0
ï£«

kâˆ’h+1) and Î³kâˆ’h+1 = Î·/Î»max(H 0

1 > 0 for A(h)

kâˆ’h+1) > 0, Ly = Lg +
2 > 0
kâˆ’h+1). Then we have

1 , and apply it a second time with Âµ = Âµ(h)

Î»min(H (h+1)

k+1 ) â‰¥ min

ï£¬
ï£­

1
Lkâˆ’h+1

,

and

Î»min(H (h)
k+1)
k+1)
(Lkâˆ’h+1)2

Î»min(H (h)
Î³kâˆ’h+1

1 +

ï£¶

ï£·
ï£¸ ,

(14)

Î»max(H (h+1)

k+1 ) â‰¤

1
Î³kâˆ’h+1

+ max

ï£«

ï£¬
ï£­0,

Î»max(H (h)
k+1)
Î³2
kâˆ’h+1

L2

kâˆ’h+1 âˆ’

Î»max(H (h)
k+1)
Î»max(H (h)
k+1)
L2
Î³kâˆ’h+1

kâˆ’h+1

1 +

ï£¶

ï£·
ï£¸ .

(15)

It is clear that we can obtain the lower bound on Î»min(Hk+1) recursively using (14). Obtaining the
upper bound on Î»max(Hk+1) using (15) is trickier. However, we notice that inequality (15) implies

Î»max(H (h+1)

k+1 ) â‰¤

1
Î³kâˆ’h+1

+ max

ï£«

ï£¬
ï£­0,

Î»max(H (h)
k+1)
Î³2
kâˆ’h+1

L2

kâˆ’h+1 âˆ’

Î»min(H (h)
k+1)
Î»max(H (h)
k+1)
L2
Î³kâˆ’h+1

kâˆ’h+1

1 +

ï£¶

ï£·
ï£¸ .

This upper bound is less tight but it allows us to bound Î»max(Hk+1) recursively.

Appendix B. Algorithms

The Two-loop recursion algorithm for evaluating the Hessian-gradient product is given by Algo-
rithm 2.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

STOCHASTIC DAMPED L-BFGS WITH CONTROLLED NORM OF THE HESSIAN APPROXIMATION

Algorithm 2 Two-loop recursion algorithm for Hessian-gradient product computation
Require: Current iterate xk, g(xk, Î¾k), Ë†Ïi = 1/s(cid:62)
1: Deï¬ne g = g(xk, Î¾k)
2: for i = k âˆ’ 1, k âˆ’ 2, . . . , k âˆ’ p do
3:

i Ë†yi, si, Ë†yi for i âˆˆ {k âˆ’ p, . . . , k âˆ’ 1}.

Compute Î½i = Ë†Ïis(cid:62)
i g
Compute g = g âˆ’ Î½i Ë†yi

k g using (10).

4:
5: Compute q = H 0
6: for i = k âˆ’ p, k âˆ’ p + 1, . . . , k âˆ’ 1 do
7:

Compute Âµ = Ë†Ïi Ë†y(cid:62)
i q
Compute q = q + (Î½i âˆ’ Âµ)si

8:
9: Return q = Hk g(xk, Î¾k)

Appendix C. Experimental Setting

CIFAR-10 is a dataset that contains 60, 000 colour images with labels in 10 classes (airplane, au-
tomobile, bird, cat, deer, dog, frog, horse, ship, and truck), each containing 6, 000 images. We use
50, 000 images for the training task and 10, 000 images for the validation task.
Details of the experiments:

â€¢ We apply our Hessian norm control using the bound on the maximum and the minimum

eigenvalues of Hk+1, where the latter is equivalent to controlling (cid:107)Bk+1(cid:107);

â€¢ In the deï¬nition of H 0

k+1 in (10), we choose Î³

and Î³k+1 constant;

k+1

â€¢ The numerical values for all algorithms are the ones that yielded the best results among all

sets of values that we experimented with.

Numerical values:

â€¢ For all algorithms: we train the network for 20 epochs and use a batch size of 256 samples;

â€¢ For SVRG, we choose a step size equal to 0.001;

â€¢ For both SdLBFGS-VR and Algorithm 1, the memory parameter p = 10, the minimal scaling

parameter Î³

k+1

= 0.1 for all k, the constant step size Î±k = 0.1 and Î· = 0.25;

â€¢ For Algorithm 1, we use a maximal scaling parameter Î³k+1 = 105 for all k, a lower bound

limit Î»min = 10âˆ’5 and an upper bound limit Î»max = 105.

Cahier du GERAD G-2020-52

Commit 983c2f1 by Dominique Orban on 2020-11-21 18:45:30 -0500

