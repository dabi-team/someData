Learning-based Prediction and Uplink
Retransmission for Wireless Virtual Reality (VR)
Network

Xiaonan Liu, Xinyu Li, Student Member, IEEE, Yansha Deng, Member, IEEE

1

0
2
0
2

c
e
D
6
1

]
P
S
.
s
s
e
e
[

1
v
5
2
7
2
1
.
2
1
0
2
:
v
i
X
r
a

Abstract—Wireless Virtual Reality (VR) users are able to
enjoy immersive experience from anywhere at anytime. However,
providing full spherical VR video with high quality under limited
VR interaction latency is challenging. If the viewpoint of the VR
user can be predicted in advance, only the required viewpoint is
needed to be rendered and delivered, which can reduce the VR
interaction latency. Therefore, in this paper, we use ofﬂine and
online learning algorithms to predict viewpoint of the VR user
using real VR dataset. For the ofﬂine learning algorithm, the
trained learning model is directly used to predict the viewpoint
of VR users in continuous time slots. While for the online
learning algorithm, based on the VR user’s actual viewpoint
delivered through uplink transmission, we compare it with the
predicted viewpoint and update the parameters of the online
learning algorithm to further improve the prediction accuracy. To
guarantee the reliability of the uplink transmission, we integrate
the Proactive retransmission scheme into our proposed online
learning algorithm. Simulation results show that our proposed
online learning algorithm for uplink wireless VR network with
the proactive retransmission scheme only exhibits about 5%
prediction error.

Index Terms—Viewpoint prediction, uplink retransmission,

ofﬂine and online learning, virtual reality (VR).

I. INTRODUCTION

Since 2015, virtual reality (VR) has become increasingly
popular, and the interactions between VR users and their
world are being revolutionized with the development of VR
technology [1], [2]. This vision has driven the commercial re-
lease of various VR hardware devices, including head-mounted
displays (HMDs) such as HTC Vive [3] and Facebook Oculus
Rift [4]. One of the main disadvantages of the wired HMDs
is the constrained mobility of VR users, which severely
affects the experience of VR users. To address this issue,
wireless connected HMDs can be used to provide immersive
experience from anywhere at anytime. However, one of the
main challenges is to provide seamless and spherical VR video
with high quality under limited VR interaction latency via
ﬂuctuated wireless channels [5].

Meanwhile, when the VR user enjoys the VR video frame,
it mainly focus on a certain direction at any given time slot.
Based on the viewing direction, the corresponding portion of
the image, deﬁned by the Field of View (FoV) [6], needs to
be rendered and delivered. The FoV determines the extent of
the virtual environment that can be viewed. The center of the

X. Liu, X. Li and Y. Deng are with the Department of Engineering, King’s
College London, London, WC2R 2LS, U.K. (e-mail:{xiaonan.liu, xinyu.1.li,
yansha.deng}@kcl.ac.uk). (Corresponding author: Yansha Deng).

FoV that the VR user is watching is called Viewpoint [7]. If
the viewpoint of the VR user is able to be well predicted,
only its corresponding FoV part of the VR video is required
to be rendered and delivered in advance, rather than rendering
and transmitting the whole spherical video, which can further
reduce the VR interaction latency [8].

There are growing research interests in the wireless VR
system. The authors in [9] and [10] proposed an echo state
network (ESN) in wireless VR transmission to maximize the
quality of service (QoS) and success transmission probability
of VR users, respectively. While in [11], the joint caching
and computing optimization problem of VR video frames
was formulated to minimize the average required transmission
rate to reduce communication bandwidth. Nevertheless, the
authors in [9]–[11] mainly focused on the resource allocation
in the wireless VR system, and assumed that the tracking
information, such as the VR users’ head motion was sent to
the small-cell base station (SBS) through uplink transmission
without transmission error.

The viewpoint prediction problem in wireless VR system
has been studied in [7], [12]–[16]. The authors in [12]
considered viewpoint prediction via constant angular velocity
and constant acceleration after every 20 ms. In [13],
the
authors proposed a double exponential smoothing method to
predict users’ head position and rotation after every 50 ms. In
[14], the authors considered Brownian Motion to simulate the
eye movement and used Recurrent Neural Network (RNN)
to predict viewpoint preference in continuous time slots.
However, the methods in [12] and [14] were not data-driven,
and prediction results obtained in [12] and [13] would be
unaligned with the viewpoint preference of VR users, which
may not ﬁt for real-time VR video transmission. In [7], [15]
and [16], the authors only used ofﬂine Linear Regression (LR)
[17] and Neural Network (NN) [18] to predict the viewpoint
of VR users in continuous time slots with real VR dataset,
and assumed that all the viewpoint requests are available at
the SBS, which is not possible without 100% reliable uplink
transmission. Based on such predictions in [7], [15] and [16],
the authors minimized the multicast bandwidth consumption
by sending the predicted part of the spherical VR video.

Because of the random nature of the head motion of VR
users, viewpoint prediction based on delayed uplink viewpoint
transmission may be prone to error, and only using trained LR
and NN cannot guarantee the highest prediction accuracy and
capture the complex dynamics viewpoint preference over time,
which may further degrade the quality of experience (QoE) [5]

 
 
 
 
 
 
of VR users. To address this issue, a RNN based on the state-
of-the-art Long Short-Term Memory (LSTM) [19] or Gated
Recurrent Units (GRU) [20] architecture can be designed to
predict the viewpoint of the VR user. However, the trained LR,
NN, LSTM and GRU learning models, namely, ofﬂine learning
models, cannot adapt to the dynamic changing environment,
and has poor adaptibility to viewpoint prediction of new VR
users.

In contrast to the ofﬂine learning algorithm, online learning
model is updated with each FoV request received, and can
automatically adjust the model itself according to the change
of the received data. In [14], through transmitting the FoV
request to the mobile edge computing (MEC)-enabled SBS
in the wireless VR network, the MEC was able to accurately
predict the required FoV of the VR user over time, render
and deliver the FoV in advance, which could decrease the
VR interaction latency and improve the QoE of the VR user.
Therefore, the online learning algorithm has the potential to
learn and update the best predictor for future FoV preference
at each time slot, and can be updated instantly once the FoV
request of new VR user is received [21]–[23].

To update the hyper-parameters in an online fashion, the
VR users need to transmit its actual viewpoint to the SBS
through uplink transmission, and the SBS will compare the
actual viewpoint with the predicted viewpoint to reduce the
loss between them. In the wireless VR network, due to the
unstable wireless channels and the interference from other VR
users, it is possible that the uplink transmission between the
VR user and the SBS fails. Nevertheless, in the aforementioned
wireless VR systems [7], [9]–[16], the authors did not consider
the potential uplink transmission failure caused by the wireless
ﬂuctuation to the online training, and the potential uplink
transmission enhancement for better online training. To deal
with this issue, the Proactive retransmission scheme [24]–
[27] is ﬁrst proposed for the uplink viewpoint transmission
to achieve ultra-reliable low-latency communication (URLLC)
requirement for this type of small data transmission.

Motivated by above, in this paper, we develop ofﬂine and
online learning algorithms for a wireless uplink VR system
under proactive retransmission scheme to efﬁciently maximize
the viewpoint prediction accuracy of VR users. The main
contributions can be summarized as follows:

• Based on the historical and current viewpoint of the VR
user in the real VR dataset, we develop ofﬂine and online
learning algorithms to predict the viewpoint of the VR
user in continuous time slots, in order to capture the
dynamical viewpoint preference of VR users over time.
• There are 16 VR videos in the real VR dataset, each VR
video has its own property, and we ﬁrst learn separate
learning model for each VR video to predict the view-
point of its corresponding VR users. When the number
of VR videos increases, one learning model for each VR
video may occupy much more computation resource and
memory of the SBS. Therefore, to evaluate the generality
of the viewpoint prediction model, we further propose
one learning model for all VR videos. To ensure that
the FoV request of each VR user in the VR dataset
has the opportunity to be tested and avoid any biased

2

performance, we use K Cross Validation to train the
learning model.

• According to [7], the viewpoint of each VR user has
strong short-term auto-correlation, which means that the
viewpoint can be well predicted based on the historical
viewpoint of each VR user. For the ofﬂine learning
algorithms, we train the n-order Linear Regression (LR),
Neural Network (NN), and Recurrent Neural Network
(RNN) based on the state-of-the-art Long-short Term
Memory (LSTM)/Gated Recurrent Unit (GRU) architec-
ture to predict the viewpoint of the VR user over time.
However, the ofﬂine learning model cannot adapt to the
dynamic changing environment when new VR users exist.
• In the online learning algorithms, we take into account
the effect of the failure during the uplink transmission
on viewpoint prediction. The uplink transmission may
fail because of the unstable wireless channels and in-
terference, which can result in incomplete training data,
and may decrease the prediction accuracy of the learning
model. To guarantee the reliability of the uplink transmis-
sion, we introduce the proactive retransmission scheme to
the uplink VR viewpoint transmission during the online
learning. Interestingly, our results shown that the online
GRU algorithm for uplink wireless VR network with
the proposed retransmission scheme can achieve 95%
prediction accuracy.

The rest of this paper is organized as follows. The VR
data description and analysis are proposed in Section II.
The system model and problem formulation are presented in
Section III. Learning algorithms for viewpoint prediction is
proposed in Section IV. The simulation results and conclusions
are described in Section V and Section VI, respectively.

Fig. 1. VR user viewing direction.

II. VR DATA DESCRIPTION AND ANALYSIS

The VR dataset obtained from [28] includes 16 clips of VR
videos with 153 VR users, and 969 data samples of the motion
in three dimensions, pitch, yaw, and roll, namely, X, Y and Z

YXZYawPitchRoll3

Fig. 2. VR video screenshot.

TABLE I
VR VIDEO INFORMATION

Video Number
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)

VR Video Scene
Airplane Flight
Surﬁng
Basketball Game
Basketball Flying
Roller Coaster1
Boxing
Dancing Girl
The Underwater World
Flying Kite
Football Team
Giant Dinosaur
Grand Canyon
Roller Coaster2
Skiing
Soccer
Survivorman

Resolution
4k
4k
4k
4k
4k
4k
4k
4k
4k
4k
1080p
2k
4k
4k
4k
4k

Bitrate
13.04 Mbps
23.2 Mbps
9.2 Mbps
5.42 Mbps
31.85 Mbps
4.4 Mbps
6.58 Mbps
23.47 Mbps
10.98 Mbps
8.24 Mbps
1.35 Mbps
5.12 Mbps
22.87 Mbps
21.08 Mbps
12.76 Mbps
29.1 Mbps

viewing angles, which are shown in Fig. 1. Each dimension is
presented by an angle (−180◦ to 180◦), and each data includes
the X, Y and Z viewing angles of each VR user at each time
slot.

A. VR Video Description

The scene of the VR video is shown in Fig. 2. From Fig.
2, we can observe that the VR videos can be divided into
three categories: 1) 7 of them are sports content, including
Surﬁng, Basketball, Boxing, Football, Skiing, and Soccer; 2) 2
of them are Landscapes content, including Grand Canyon, and
Survivorman; and 3) 5 of them are Entertainment, including
Airplane ﬂight, Underwater game, Roller coaster, Dancing girl,
Flying Kite, and Glant Dinosaur.

The 16 VR video clips are downloaded from YouTube. The
duration of each VR video is 30 seconds and each VR video
is divided into 300 equal parts, which means that each sample

Fig. 3. The detailed number of VR users watching each VR video.

point of the VR video lasts for 0.1 second. Among these VR
videos, 14 of them are 4K resolution, one of them is 2K
resolution, and one of them is 1080P. The detailed attributes
of each VR video are shown in Table I.

In the experimental data, 153 VR users watched these VR
videos, where 35 of them enjoyed all 16 VR video clips, and
118 of them enjoyed 3 to 5 randomly selected VR video clips.
The detailed number of VR users watching each VR video is
shown in Fig. 3. We can obtain that each VR video is watched
by an average of 60 VR users, with a minimum of 46, and
a maximum of 84. Meanwhile, the age distribution of all VR
users is shown in Fig. 4. From Fig. 4, we can see that more
than half of VR users are between 20 and 30. In addition,
for all the VR users, 38% VR users are female, and 34% VR
users wear glasses.

                        9 5  9 L G H R  , Q G H [                  1 X P E H U  R I  9 5  8 V H U V                                4

Fig. 4. Age distribution of all VR users.

Fig. 6. Y angle distribution of all VR users.

Fig. 5. X angle distribution of all VR users.

B. Viewpoint Distribution

In the VR dataset, most VR users have similar viewpoint
when enjoying the same video. We plot the viewpoint of
all VR users for the 16 VR videos in X, Y and Z angles,
respectively, which are shown in Fig. 5, 6 and 7, respectively.
The X-axis and Y-axis of the viewpoint distribution ﬁgure
are VR video playout time and degree of angles at each time
slot, respectively. From Fig. 5, 6 and 7, we can know that the
viewpoint range of X, Y and Z angles are (-50◦, 50◦), (-150◦,
150◦) and (-50◦, 50◦), respectively.

III. SYSTEM MODEL AND PROBLEM FORMULATION

We consider a wireless VR system, where a small-cell base
station (SBS) is connected to the core network through a ﬁber
link to serve K VR VR users via wireless links as shown in
Fig. 8. The SBS is equipped with M antennas and each VR
user is equipped with a single antenna, respectively.

A. Uplink Transmission Model

At the (t−1)th time slot, the SBS will predict the viewpoint
t = ( ˆX k
ˆV k
t ) of the kth VR user for the tth time
slot. Then, at the tth time slot, the VR user will transmit its

t , ˆZ k

t , ˆY k

Fig. 7. Z angle distribution of all VR users.

Fig. 8. Wireless VR network.

t , Y k
actual viewpoint V k
t ) to the SBS via uplink
transmission. The uplink transmission signal from the kth VR
user to the SBS at the tth time slot can be denoted as

t = (X k

t , Z k

yup
k,t = uH

k,thk,txup

k,t +

KVR
(cid:88)

i=1,i(cid:54)=k

k,thi,txup
uH

i,t + nup
k,t,

(1)

where hk,t ∈ C1×M is the uncorrelated Rayleigh fading
channel vector between the kth VR user and the SBS at the
tth time slot, M is the number of antennas equipped at the
SBS, and α is the large-scale fading coefﬁcient. uk,t ∈ C1×M
is the beamforming vector at the SBS, which can be denoted
as uk,t = hk,t
k,t ∈ CM ×1 is the transmit message
of the kth VR user at the tth time slot. (cid:80)K
k,thi,txup

(cid:107)hk,t(cid:107) [29]. xup

i=1,i(cid:54)=k uH

i,t

                                             SBSCloud NetworkCloud NetworkOptical FiberControl CentreVR UserWireless Linkis the interference from other VR users at the tth time slot. In
addition, nup
k,t ∼ CN (0, σ2
kIM ) is the additive white Gaussian
noise at the SBS at the tth time slot.

Furthermore, at the tth time slot, the data rate between the

5

kth VR user and the SBS can be written as
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

KVR
(cid:80)
i=1,i(cid:54)=k

k,t = log2

k,thk,t|2

Rup

|uH

|uH

I +

k,thi,t|2 + σ2

kIM

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(2)

k,t ≥ Rup

To guarantee the successful uplink transmission, the uplink
transmission rate should larger than a threshold Rup
th , namely,
Rup
th . However, it is possible that the uplink trans-
mission rate of the kth VR user is smaller than the threshold
because of the interference or the poor channel state informa-
tion.

To guarantee the reliability of uplink transmission, we
consider the proactive retransmission scheme. If the actual
viewpoint from the VR user is successfully transmitted via
the uplink transmisson, the SBS will send an ACK feedback,
otherwise, it will send a NACK feedback.

According to the proactive scheme, the kth VR user will re-
peat the uplink transmission in consecution transmission time
intervals (TTIs) with a maximum number of Kre repetitions,
but can receive the feedback after each repetition. The kth VR
user is allowed to stop repetitions once receiving the positive
feedback (ACK). We assume that the processing time of the
received viewpoint and feedback time at the SBS are one
transmission time interval (TTI), respectively. For example,
when Kre = 8, as shown in Fig. 9, we can observe that the
kth VR user is able to receive the 1st feedback in 4TTIs after
the 1st repetition, which means that the minimum round trip
time is 4TTIs. Nevertheless, if the kth VR user cannot obtain
the ACK at the ﬁrst round trip time, it will continue waiting
for the ACK until (Kre + 3)TTIs. However, if the kth VR
user cannot obtain the ACK during the initial transmission, it
needs to continue repetitions until either it receives ACK from
the SBS, or the latency is larger than the uplink transmission
latency threshold. If the 1st successful uplink transmission
of the kth VR user occurs in the lth repetition during the
ﬁrst round trip, the uplink latency of the kth VR user can be
computed as

Tk,l = (l + 3)TTIs,

(3)

Furthermore, the latency after m round trips for the Proactive
scheme with a maximum Kre repetitions can be derived as

T m
k,l = (m − 1)Tk,Kre + Tk,l

(4)

= [(m − 1)(Kre + 3) + (l + 3)]TTIs,

where (m − 1)Tk,Kre means that the uplink transmissions in
former (m−1) round trips are not successful, and Tk,l denotes
the successful uplink retransmission in the ﬁnal mth round trip
given in (3).

Fig. 9. Proactive retransmission scheme.

predicting the viewpoint of the VR user is equal to predicting
the X, Y and Z angles. We consider a sliding window to
predict the viewpoint of the VR user over time, which is shown
in Fig. 10. According to Fig. 10, the future viewpoint of the
VR user is predicted based on the current and past rotation
status. We assume that the pitch, yaw and roll angles of the VR
user at the tth time slot are Xt, Yt, and Zt, respectively. Fur-
thermore, we use Xt:(t+d) = (Xt, Xt+1, ..., Xt+d), Yt:(t+d) =
(Yt, Yt+1, ..., Yt+d) and Zt:(t+d) = (Zt, Zt+1, ..., Zt+d) to
denote the continuous viewpoints in X, Y and Z angles from
the tth time slot to the (t + d)th time slot.

Fig. 10. Sliding window.

To predict the future viewpoint (Xt+d, Yt+d, Zt+d) at the
tth time slot, we use previous viewpoint X(t−Tw):t, Y(t−Tw):t
and Z(t−Tw):t, where Tw is the size of the sliding window.
Then, the predicted viewpoint at the (t + d)th time slot can
be presented as

ˆXt+d = fx,t+d(X(t−Tw):t),

ˆYt+d = fy,t+d(Y(t−Tw):t),

and ˆZt+d = fz,t+d(Z(t−Tw):t),

(5)

(6)

(7)

B. Viewpoint Prediction Methods

where fx,t+d(.), fy,t+d(.), and fz,t+d(.) are prediction func-
tion.

When the VR user enjoys the VR video frame, the viewpoint
has three degrees of freedom (pitch, yaw, and roll) and is de-
termined by the rotation angles in X, Y and Z axis. Therefore,

To predict the viewpoint of the VR user accurately, we
consider two learning algorithms, namely, ofﬂine learning and
online learning.

VR UserSBSATimeTimeT1P1U1P2P1ActualViewpointUplink Latency ThresholdInitial Transmission1st TxRetransmission2nd TxT2T3T4U2U3U4P3P4T1T2T3T4U1T5U5P5F1F2F3F4F5F1ReceivedViewpointPredictedViewpointP6P7P8F6F7F8T6T7T8U6U7U8P2PredictionModelInput DataOutput DataActual TimelinePrediction TimelinePrediction WindowSliding Windowt1tn-Tw……tn…tn…tn+d1) Ofﬂine Learning:

In the ofﬂine learning algorithms,
we propose three methods, which are trained n-order Lin-
ear Regression (LR), Neural Network (NN), and Recurrent
Neural Network (RNN) based on Long-short Term Mem-
ory(LSTM)/Gated Recurrent Unit (GRU) architecture to pre-
dict the viewpoint of VR users. Through dividing the VR
dataset into training and testing dataset, the VR user data in the
training dataset are used to train the models for three ofﬂine
methods, where the trained models are used to predict the
viewpoint of the VR user directly.

2) Online Learning:

In the online learning algorithms,
we still use n-order LR, NN and LSTM/GRU algorithms.
Meanwhile, we use Mean Square Error (MSE) [30] as cost
function in the training step to update the parameters in the
online learning model, and predict the viewpoint of new VR
users. The MSE of the VR users at the tth time slot can be
presented as

MSEt =

1
K VR

KVR
(cid:88)

( ˆV k

t − V k

t )2.

(8)

k=1
At the tth time slot, the SBS will predict the viewpoint of the
VR user for the (t + 1)th time slot. At the (t + 1)th time slot,
the VR user will transmit the actual viewpoint to the SBS via
uplink transmission. Through comparing it with the predicted
viewpoint, the SBS will further update the trained learning
model to improve the prediction accuracy.

C. Rendering and Downlink Transmission Model

When the future viewpoint of the VR user is predicted via
ofﬂine or online learning algorithms, the SBS will render the
predicted viewpoint and transmit it to the VR user through
downlink transmission in advance. Therefore, the VR inter-
action latency can be reduced [14]. In this paper, we mainly
focus on prediction and uplink retransmission in the wireless
VR system, which can be easily integrated into the rendering
and downlink transmission in our previous work [14].

D. Problem Formulation

For the viewpoint prediction, we use ofﬂine and online
learning algorithms to minimize the average prediction error
of VR users, the optimization problem can be formulated as

min

1
T tot
i

(cid:101)Ni

T tot
i(cid:88)

(cid:101)Ni(cid:88)

t=1

k=1

( ˆV k

t − V k

t )2,

(9)

where (cid:101)Ni is the number of the VR users watching the ith VR
video, and T tot

is the total time slots of the ith VR video.

i

IV. LEARNING ALGORITHMS FOR VIEWPOINT
PREDICTION
In the ofﬂine learning, we directly use the trained n-order
LR, NN and LSTM/GRU network to predict the viewpoint of
the VR user in continuous time slots. However, for the online
learning, the VR user will deliver the actual viewpoint to the
SBS via uplink transmission in real-time to further update the
models in the NN and LSTM/GRU learning algorithms and the

6

input of the sliding window, which can improve the prediction
accuracy. If the actual viewpoint at a speciﬁc time slot has not
been successfully delivered to the SBS, the learning algorithms
will predict the viewpoint in the next time slot based on the
models trained in the previous time slots, and the input of the
sliding window at the current time slot is set to be null.

A. Ofﬂine Learning Algorithm

According to Fig. 3, VR dataset contains dozens of VR
users enjoy each VR video, and there are 16 VR videos
and 969 VR user samples. To train the learning model, we
split the data samples of the VR dataset into the training and
the testing datasets. The training dataset is used to train the
learning model, and the testing dataset is used to validate it
on data it has never seen before. The classic approach is to
do a simple 80%-20% [31], which means that we randomly
select 80% data samples of the dataset to construct training
dataset, while the remaining 20% data samples of the dataset
are the testing dataset. However, with a simple 80-20 split,
there is a possibility of high bias if we have limited data.
More importantly, we will miss some important information
about the data samples which are not used for training, which
is able to get good or bad performance only due to chance.
To ensure that each data sample from the original dataset has
the chance of appearing in the training and testing dataset, we
use K Cross Validation [32].

Through using the K Cross Validation to train the learning
models in the VR dataset, each VR user sample has the
opportunity of being tested. We split the VR dataset into
Kcross datasets: one dataset is used for validation, and the
remaining (Kcross − 1) datasets are merged into a training
dataset for prediction learning model evaluation [33]. In our
VR dataset, there are 16 different VR videos. According to
Fig 5, 6 and 7, the viewpoint distribution of VR users in each
VR video are different. Therefore, for each VR video, we can
use its corresponding VR user samples to train a viewpoint
prediction learning model. However, if the number of the VR
videos increases, training one model for one VR video may
cost much more energy and occupy much more computation
resource and memory of the SBS. Therefore,
in order to
evaluate the generality of the trained models, we propose two
viewpoint prediction learning models, namely, one for single
VR video, and the other for all VR videos. The detailed K
Cross Validation for the proposed two viewpoint prediction
schemes are introduced as follows:

(a) One Model for One VR Video: For each VR video,
there are dozens of VR user samples, and we assume that the
number of the VR user samples of the kth VR video is (cid:101)Nk.
We split these dozens of VR user samples into Kcross datasets,
where the number of VR user samples in each sub dataset is
(cid:101)Nk/Kcross, and randomly select (Kcross − 1) sub datasets to
train the learning model and one sub dataset to test the trained
learning model. Through Kcross times training and testing, we
can obtain the average prediction error of the K-folder cross
validation.

(b) One Model for All VR Videos: For all 16 VR videos
and 969 VR user samples, we split these VR user samples

according to the index of VR video. Therefore, there are
16 VR sub datasets. At each training round of the K cross
validation, 16/Kcross VR sub datasets are used for testing,
and the remaining (16 − 16/Kcross) VR sub datasets are
merged into a training VR sub dataset to train the viewpoint
prediction model. For example, in Fig. 11, we consider 4 cross
validation, namely, Kcross = 4. At each training iteration, 12
VR sub datasets will be randomly selected to train the learning
model, and the remaining 4 VR sub dataset will be used to
test the trained learning model. After 4 training iterations, we
can calculate the average prediction error of these 4 trained
learning models.

Fig. 11. 4 cross validation for training learning models.

B. Online Learning Algorithm

In the online learning algorithms, the learning model will
ﬁrst be trained via the training dataset through the K Cross
Validation described in Section IV-A. Then, for the VR user
samples in the testing dataset, at each time slot, each VR user
will update its actual viewpoint to the SBS through uplink
transmission. The online learning algorithms are introduced
in detail as follows.

1) n-order Linear Regression: n-order LR algorithm uses
the least square function to model the nolinear relationship
between the input sliding window and the predicted viewpoint.
It is able to ﬁt the nonlinear relationship between the input and
output, and can be written as

ˆV = WLRgH + bLR,

1 , wLR

2 , ..., wLR

(10)
where WLR = [wLR
n ] and bLR are parameters of
the n-order LR model. In (10), g = [ˆg, ˆg2, ..., ˆgn] is the input of
the n-order LR, where ˆg = (X(t−Tw):t, Y(t−Tw):t, Z(t−Tw):t)
is the vector which includes the X, Y and Z viewing angles
in Tw time slots. ˆVt+1 = ( ˆXt+1, ˆYt+1, ˆZt+1) is the predicted
viewing angles for the (t + 1)th time slot. The loss function
of the n-order LR can be calculated as

LLR

t =

1
K VR

KVR
(cid:88)

(V k

t − ˆV k

t )2.

(11)

k=1
Through gradient descent method [34], the parameters θLR =
{WLR, bLR} can be updated as

t+1 = θLR
θLR

t − ∆LLR
where ∆LLR(.) is the gradient of the loss function. The
proposed Proactive retransmission scheme integrated into the
online n-order LR is illustrated in Algorithm 1.

(θLR
t

(12)

),

t

7

Fig. 12. Proposed multi-layer NN architecture.

1

, ..., θNN

2) Neural Network: In the L-layer NN shown in Fig. 12,
we assume that ΘNN = {θNN
, θNN
L } contains L sets
2
of parameters, and the parameters at the lth (1 ≤ l ≤ L)
layer can be denoted as θNN
l }, where W NN
and bNN
are the neurons’ weights and bias vector at the lth
layer. A feedforward NN with L layers describes a mapping
f NN(rNN, θNN), where rNN is the input vector. We can obtain
the output of the NN through L iterative processing steps, and
the output of the lth layer in NN can be written as

l = {W NN

, bNN

l

l

l

l = f NN
rNN
l
where f NN
l−1; θNN
(rNN
l
the lth NN layer.

l

(rNN

l−1; θNN

l

), l = 1, 2, ..., L,

(13)

) is the mapping function calculated by

At

the tth time slot, we input

the historical viewpoint
of the VR user and obtain the predicted viewpoint via the
feedforward function in the L-layer NN. Then, we use the
MSE criterion among the predicted viewpoint and the actual
viewpoint of the (t + 1)th time slot to compute the loss of the
NN, which can be denoted as

LNN

t,l (cid:107)2,

t,l (θNN

t,l ) = (cid:107)φNN

t,l − ˆφNN
where φNN
is the desired output of the lth layer in NN,
t,l
ˆφNN
is the dependence of the NN’s output to the lth layer’s
t,l
parameters. To minimize the loss function, we adopt
the
backpropagation method based on stochastic gradient descent
(SGD) [35]. The parameters at the lth layer can be updated as

(14)

t+1,l = θNN
θNN

t,l − λNN∆LNN

t.l (θNN

t,l ),

(15)

where λNN ∈ (0, 1] denotes the learning rate of the NN and
∆LNN(.) is the gradient of the loss function. The proposed
Proactive retransmission scheme integrated into the online NN
is presented in Algorithm 1.

the VR user

3) Long-short Term Memory/Gated Recurrent Unit: To
capture the dynamics in viewpoint of
for
the (t + 1)th time slot, we use not only the most re-
cent observation Ot = {O1
t =
t , Y k
{(X k
t )} is the actual viewpoint of the kth VR user
the tth time slot, but also the previous observations
at
Ht = {Ot−To+1, ..., Ot−2, Ot−1}, where To is the size of
the memory window. In order to recognize the viewpoint

t }, where Ok

t , ..., OK

t , Z k

t , O2

V1V2V3V4V13V14V15V16V13V14V15V16V9V10V11V12V9V10V11V12V5V6V7V8V5V6V7V8V1V2V3V4V13V14V15V16V13V14V15V16V9V10V11V12V9V10V11V12V5V6V7V8V5V6V7V8V1V2V3V4V9V10V11V12V9V10V11V12V13V14V15V16V13V14V15V16V5V6V7V8V5V6V7V8V1V2V3V4V5V6V7V8V5V6V7V8V13V14V15V16V13V14V15V16V9V10V11V12V9V10V11V12V5V6V7V8V1V2V3V4V1V2V3V4V13V14V15V16V13V14V15V16V9V10V11V12V9V10V11V12Original DatasetIteration 1Iteration 2Iteration 3Iteration 4Training DataTesting Data......……......Input measured viewpointOutput predicted viewpointInput LayerOutput LayerHidden Layer(Xt, Yt, Zt)(Xt-1, Yt-1, Zt-1)(Xt-Tw+1, Yt-Tw+1, Zt-Tw+1)(Xt-Tw, Yt-Tw, Zt-Tw)(Xt+1, Yt+1, Zt+1)8

Algorithm 1 The Proactive retransmission scheme integrated
into Online Learning Algorithms with n-order LR, NN and
LSTM/GRU

1: Initialize the order n of LR, parameters θLR or θNN or

θRNN , and sliding window size Tw.

2: Use K Cross Validation to train the parameters of the n-

order LR, NN and RNN learning model.

3: for t = 1,...,T do
4:

Get historical viewpoint from the (t−Tw)th time slot to
the (t−1)th time slot from the updated sliding window.
Use the updated online n-order LR, NN, LSTM/GRU
to predict the viewpoint of the VR user for the tth time
slot.
The VR user transmits its actual viewpoint of the tth
time slot via uplink transmission with the Proactive
retransmission scheme.
if the uplink transmission is successful then
or θRNN
t

Update parameters θLR
of the n-
order LR, NN and RNN learning model via (12), (15)
and (16).
Update the sliding window with the actual required
viewpoint of the tth time slot.

or θNN
t

t

5:

6:

7:
8:

9:

else

10:
11:

t

12:

or θNN

t−1 → θNN

t−1 → θLR
θLR
t
Update the sliding window with null of the tth time
slot.
end if
13:
14: end for

t−1 → θRNN

or θRNN

.

t

“without Proactive Retransmission”, respectively. Meanwhile,
in the Genie-aided scheme, the online learning model is trained
with the correct actual viewpoint of each VR user at each time
slot, which is the upper bound of the online learning algorithm
with proactive retransmission scheme and cannot be reached
in the practical wireless VR system.

Fig. 13. Proposed LSTM/GRU architecture (left) with its unfolding structure
(right).

in continuous time slots, we leverage a RNN model with
parameters θRNN = {W RNN, bRNN}, where W RNN and
bRNN are the neurons’ weights and bias vector of the RNN.
The RNN is capable of capturing time correlation of the
viewpoint of the VR user, which can help learn the time-
varying viewpoint for better prediction accuracy.

contains multiple

The LSTM/GRU layer

standard
LSTM/GRU units and receives the current and historical
observations [Ot−To+1, ..., Ot−1, Ot] at the tth time slot and
is connected to an output layer with a Relu non-linearity
activation function, which is shown in Fig. 13. The Relu
layer outputs the predicted viewpoint of the VR user for the
(t + 1)th time slot. To update the model parameter θRNN, we
ﬁrst use MSE to calculate the loss function, and then use the
standard SGD via BackPropagation Through Time (BPTT)
[36]. At the (t + 1)th time slot, θRNN can be updated as

t+1,l = θRNN
θRNN

t,l − λRNN∆LRNN

t,l

(θRNN
t,l

),

(16)

∈ (0, 1]

where λRNN
RNN, ∆LRNN
LRNN
t,l
can be computed by the MSE as

learning rate of

the
) is the gradient of the loss function
(θRNN
)
t,l

) to train parameters of the RNN. LRNN

t,l
(θRNN
t,l

(θRNN
t,l

the

is

t,l

LRNN
t,l

(θRNN
t,l

) = (cid:107)φRNN

t,l − ˆφRNN

t,l

(cid:107)2,

(17)

t,l

where φRNN
is the desired output of the lth layer in RNN,
ˆφRNN
is the dependence of the RNN’s output to the lth layer’s
t,l
parameters. The proposed Proactive retransmission scheme
integrated into the online RNN is presented in Algorithm 1.

V. SIMULATION RESULTS

In this section, we examine the effectiveness of our proposed
ofﬂine and online learning algorithms on the upink viewpoint
prediction of VR users under the Proactive retransmission
scheme. We set the size of the sliding window as 10, and
the size of the prediction window as 1. For the n-order LR,
we consider n = 15. For the NN, we use the fully-connected
NN with two hidden layers, where the ﬁrst and second layers
have 12 and 10 neurons, respectively. For the RNN, it has one
hidden layer with 12 units. The learning rate for learning al-
gorithm is 0.001. For the uplink transmission, we set M = 30,
α = 3, TTI = 0.125 ms, Rup
th = 2 MB/s, σ2 = −110 dBm,
and Kre = 8. Consider a limited square area whose side
length is 100 meters. For simplicity, we use “w/ Proac” and
“w/o Proac” to represent “with Proactive Retransmission” and

A. VR Dataset Processing

We ﬁrst save all the VR user samples in a MATLAB ﬁle.
Then, we use Python 3.6 to delete the useless rows and
columns, and import the VR user data into training and testing
datasets. According to [7], the motion of the VR user has
strong short-term auto-correlations in all three dimensions.
Due to the fact that auto-correlations are much stronger than
the correlation between these three dimensions, the angles in
each direction can be trained independently and separately.
According to Fig. 5, 6 and 7, we can obtain that the range
of Y angle distribution is much larger than that of X and Z.
Therefore, for simplicity, we use ofﬂine and online learning
algorithms to predict Y angle of VR users in this section,
however, our algorithms can also be used for the prediction of
X and Z angles.

B. Viewpoint Prediction

The simulation results of our proposed two viewpoint
prediction learning models, namely, one training model for

LSTM/GRULSTM/GRULSTM/GRUOt-T0+1Ot-1Ot[Ot-T0+1,…,Ot-1 ,Ot ]ReluPredicted ViewpointUnfoldingLSTM/GRUReluPredicted Viewpointsingle VR video, and one training model for all VR videos,
are introduced as follows:

(a) One Training Model for One VR Video: In this
scheme, for each VR video, we use the VR user samples in the
training datasets to train the ofﬂine and online learning models
to predict the Y angle of VR users in the testing datasets, and
average the prediction error of all VR videos.

9

Fig. 16. Average prediction error of different number of VR users in the
training dataset to train the learning model via ofﬂine/online 15-order LR,
NN, LSTM and GRU with Proactive retransmission scheme.

is not signiﬁcantly affected by changing the size of sliding
window due to their capability to adapt
to the viewpoint
preference. When the size of the sliding window is 10, it can
obtain the best performance.

Fig. 16 plots the average prediction error of different number
of VR users in the training dataset to train the learning model
via ofﬂine/online 15-order LR, NN, LSTM and GRU for up-
link VR viewpoint transmission with proactive retransmission
scheme. For the ofﬂine learning algorithms, we observe that
the average prediction error becomes smaller with increasing
number of VR users. With increasing number of VR users,
the ofﬂine learning algorithms can be trained to adapt to the
viewpoint of the VR users much more accurately. It is also
seen that the performance of the LSTM/GRU is better than
that of the NN. This is because the LSTM/GRU is able to
capture the correlation of the viewpoint in continuous time
slots. In addition, it can be seen that the average prediction
error of 15-order LR algorithm is much higher than that of
ofﬂine/online NN, LSTM and GRU. It is because the learning
structure of the LR algorithm is simplier than that of NN,
LSTM and GRU, and its ability to be ﬁt for the nonlinear
VR data is worse than that of the NN, LSTM and GRU. In
addition, LR algorithm may get overﬁt with so many VR users
training the LR model.

Meanwhile, for the proactive retransmission scheme inte-
grated into the online learning algorithm in Fig. 16, it is
interesting to note that its average prediction error is much
smaller than that of ofﬂine learning algorithms and changes
slightly with the increasing number of VR users. This is due
to that through updating the parameters in the trained learning
model, the online learning algorithm is able to adapt to the
viewpoint preference of new VR users over time. Thus, the
prediction accuracy can be improved.

Fig. 17 plots the average prediction error for various number
of VR users in the training dataset to train the learning model
via online 15-order LR, NN, LSTM and GRU for uplink VR
viewpoint transmission with/without proactive retransmission

Fig. 14. Loss of ofﬂine NN, LSTM and GRU algorithms of each epoch.

Fig. 14 plots the loss of ofﬂine NN, LSTM and GRU
algorithms of each epoch. It is seen that the performance of
ofﬂine GRU algorithm outperforms that of LSTM and NN.
This is because the structure of the LSTM is more complex
than that of GRU, so that the parameters of the GRU can be
trained faster and easier to be modiﬁed [37].

Fig. 15. Average prediction error of ofﬂine/online learning algorithms via
different size of sliding window with Proactive retransmission scheme.

Fig. 15 shows the average prediction error of ofﬂine/online
learning algorithms via different size of sliding window for
uplink VR viewpoint transmission with proactive retransmis-
sion scheme. It is noted that the average prediction error of the
ofﬂine/online 15-order LR, NN, LSTM and GRU algorithms

            ( S R F K           0 6 ( 1 1 / 6 7 0 * 5 8                6 O L G L Q J  : L Q G R Z  6 L ] H              $ Y H U D J H  3 U H G L F W L R Q  ( U U R U     2 I I O L Q H  1 1 2 I I O L Q H  / 6 7 0 2 I I O L Q H  * 5 8 2 Q O L Q H  1 1   Z   3 U R D F 2 Q O L Q H  / 6 7 0   Z   3 U R D F 2 Q O L Q H  * 5 8   Z   3 U R D F 2 Q O L Q H     2 U G H U  / 5   Z   3 U R D F                1 X P E H U  R I  7 U D L Q L Q J  8 V H U V            $ Y H U D J H  3 U H G L F W L R Q  ( U U R U     2 I I O L Q H  1 1 2 I I O L Q H  / 6 7 0 2 I I O L Q H  * 5 8 2 I I O L Q H     2 U G H U  / 5 2 Q O L Q H  1 1   Z   3 U R D F 2 Q O L Q H  / 6 7 0   Z   3 U R D F 2 Q O L Q H  * 5 8   Z   3 U R D F 2 Q O L Q H     2 U G H U  / 5   Z   3 U R D F10

Fig. 17. Average prediction error of different number of VR users in the
training dataset to train the learning model via online 15-order LR, NN, LSTM
and GRU with/without Proactive retransmission scheme.

Fig. 18. Average prediction error of ofﬂine/online 15-order LR, NN and GRU
with Proactive retransmission scheme in continuous time slots.

scheme. We can observe that the performance of the proactive
retransmission scheme integrated into the online learning
algorithm is better than that without proactive retransmission
scheme and is close to the performance of the Genie-aided
scheme. In the uplink viewpoint transmission without proac-
tive retransmission, each VR user only transmits its actual
viewpoint to the SBS once even this transmission fails. This
transmission failure is usually because of the unstable channel
state and the interference from other VR users. To cope
with this, the proactive retransmission scheme is applied here
to improve the success transmission of uplink transmission
[38], and the online learning algorithms are capable of better
capturing historical trends of viewpoint preference of the VR
user, which can further improve the prediction accuracy. While
in the Genie-aided scheme, the uplink transmission at each
time slot for each VR user is assumed to be successful.

(b) One Training Model for All VR Videos: In this model,
for all 16 VR videos, we consider 4 Cross Validation shown in
Fig. 11 and use the VR user samples in the training datasets to
train ofﬂine/online 15-order LR, NN and GRU learning models
to predict Y angle of VR users in the testing datasets.

Fig. 18 plots average prediction error of ofﬂine/online 15-
order LR, NN and GRU integrated with proactive retrans-
mission scheme over continuous time slots. For the ofﬂine
learning algorithms, it can be seen that the performance of the
GRU is a bit better than that of the NN. Meanwhile, it can be
observed that at the beginning 30 time slots, the performance
of the 15-order LR is better than that of NN and GRU. It is
because according to Fig. 6, at the beginning, when the VR
user watches the VR video, its viewpoint mainly focus on
the zero point and the 15-order LR ﬁts well at the beginning.
However, after 50 time slots, the performance of GRU is much
better than that of 15-order LR. This is due to that after 50 time
slots, the viewpoint of the VR user will change substantially as
shown in Fig. 6, and the GRU is able to capture the correlation
of the viewpoint of the VR user over continuous time slots.

Furthermore, it is also noted that at the beginning, there are

large ﬂuctuations in the performance of the proactive retrans-
mission scheme integrated into the online learning algorithms.
It is because the parameters in the online learning algorithms
should be modiﬁed to capture the viewpoint preference of the
VR user. In addition, when the viewpoint of the VR user
changes over time, the online learning algorithms need to
further update their parameters to be ﬁt for the viewpoint
changing of the VR users. Therefore, there are small ﬂuc-
tuations in the performance of the online learning algorithms.

Fig. 19. Average prediction error of online GRU algorithms with/without
Proactive retransmission scheme in continuous time slots.

Fig. 19 plots the average prediction error of online GRU
algorithms of uplink viewpoint
transmission with/without
proactive retransmission scheme in continuous time slots. We
can obtain that the performance of the proactive retransmission
scheme with the online GRU algorithm is still better than
that of the scheme without proactive retransmission scheme.
Meanwhile, it can be seen that the performance of the Genie-
aided online GRU algorithm slightly outperforms that of
the online GRU algorithm with the proactive retransmission

                1 X P E H U  R I  7 U D L Q L Q J  8 V H U V          $ Y H U D J H  3 U H G L F W L R Q  ( U U R U     2 Q O L Q H  1 1   Z  R  3 U R D F 2 Q O L Q H  1 1   Z   3 U R D F 2 Q O L Q H  1 1   * H Q L H  D L G H G 2 Q O L Q H  / 6 7 0   Z  R  3 U R D F 2 Q O L Q H  / 6 7 0   Z   3 U R D F 2 Q O L Q H  / 6 7 0   * H Q L H  D L G H G 2 Q O L Q H  * 5 8   Z  R  3 U R D F 2 Q O L Q H  * 5 8   Z   3 U R D F 2 Q O L Q H  * 5 8   * H Q L H  D L G H G 2 Q O L Q H     2 U G H U  / 5   Z  R  3 U R D F 2 Q O L Q H     2 U G H U  / 5   Z   3 U R D F 2 Q O L Q H     2 U G H U  / 5   * H Q L H  D L G H G                7 L P H  6 O R W         $ Y H U D J H  3 U H G L F W L R Q  ( U U R U     2 I I O L Q H  1 1 2 I I O L Q H  * 5 8 2 I I O L Q H     2 U G H U  / 5 2 Q O L Q H  1 1   Z   3 U R D F 2 Q O L Q H  * 5 8   Z   3 U R D F 2 Q O L Q H     2 U G H U  / 5   Z   3 U R D F                7 L P H  6 O R W         $ Y H U D J H  3 U H G L F W L R Q  ( U U R U     2 Q O L Q H  * 5 8   Z  R  3 U R D F 2 Q O L Q H  * 5 8   Z   3 U R D F 2 Q O L Q H  * 5 8   * H Q L H  D L G H G11

[16] Y. Bao, H. Wu, A. A. Ramli, B. Wang, and X. Liu, “Viewing 360 degree
videos: Motion prediction and bandwidth optimization,” in Proc. IEEE
ICNP, pp. 1161 – 1170, Nov. 2016.

[17] T. Hastie, R. Tibshirani, and J. Friedman, “The Elements of Statistical
Learning: Data Mining, Inference, and Prediction,” Springer, 2009.
[18] M. A. Nielsen, “Neural Networks and Deep Learning,” Determination

Press, 2014.

[19] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735 – 1780, Nov. 1997.

[20] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical eval-
uation of gated recurrent neural networks on sequence modeling,”
arXiv:1412.3555, 2014.

[21] S. C. H. Hoi, D. Sahoo, J. Lu, and P. Zhao, “Online learning: A

comprehensive survey,” arXiv:1802.02871, 2018.

[22] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M. L. Shyu,
S. C. Chen, and S. S. Lyengar, “A survey on deep learning: Algorithms,
techniques, and applications,” ACM Comput. Surveys., vol. 52, p. 1–92,
Sep. 2018.

[23] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and
wireless networking: A survey,” IEEE Commun. Surveys Tuts., vol. 21,
no. 3, pp. 2224 – 2287, 3rd Quart. 2019.

[24] “Study on scenarios and requirements for next generation access tech-

nologies (release 15),” 3GPP, TS 38.913 v.15.2.0, Jun. 2018.

[25] M. Series, “IMT vision-framework and overall objectives of the future
development of IMT for 2020 and beyond,” Recommendation ITU, pp.
2083 – 2095, Sep. 2015.

[26] H. Zhang, N. Liu, X. Chu, K. Long, A. Aghvami, and V. C. M.
Leung, “Network slicing based 5g and future mobile networks: Mobility,
resource management, and challenges,” IEEE Commun. Mag., vol. 55,
no. 8, pp. 138 – 145, Aug. 2017.

[27] “Discussion on HARQ support for urllc,” R1-1612246, 3GPP TR-RAN1

87, Nov. 2016.

[28] www.dropbox.com/sh/78ff9djp3v2nv8x/AAACwzDFYwYJzIMrTs8jgM09a.
[29] Y. Deng, M. Elkashlan, N. Yang, P. L. Yeoh, and R. K. Mallik, “Impact
of primary network on secondary network with generalized selection
combining,” IEEE Trans. Veh. Technol., vol. 64, no. 7, pp. 3280 – 3285,
Jul. 2015.

[30] A. Botchkarev, “A new typology design of performance metrics to
measure errors in machine learning regression algorithms,” Interdiscipl.
J. Inf., Knowl. Manage., vol. 14, p. 45–76, Jan. 2019.

[31] S. Ayoubi, N. Limam, M. A. Salahuddin, N. Shahriar, R. Boutaba, F. E.
Solano, and O. M. Caicedo, “Machine learning for cognitive network
management,” IEEE Commun. Mag., vol. 56, no. 1, pp. 158 – 165, Jan.
2018.

[32] J. D. Rodriguez, A. Perez, and J. A. Lozano, “Sensitivity analysis of k-
fold cross validation in prediction error estimation,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 32, no. 3, p. 569–575, Mar. 2010.

[33] S. Raschka, “Model evaluation, model selection, and algorithm selection

in machine learning,” https://arxiv.org/abs/1811.12808, 2018.

[34] S. Ruder, “An overview of gradient descent optimization algorithms,”

arXiv:1609.04747, 2016.

[35] L. Bottou, “Large-scale machine learning with stochastic gradient de-

scent,” Proc. 19th Int. Conf. Comput. Statist., p. 177–186, Sep. 2010.

[36] P. J. Werbos, “Backpropagation through time: what it does and how to
do it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550 – 1560, Oct.
1990.

[37] D. Britz, A. Goldie, M. T. Luong, and Q. Le, “Massive exploration of
neural machine translation architectures,” arxiv:1703.03906, 2017.
[38] Y. Liu, Y. Deng, M. Elkashlan, A. Nallanathan, and G. K. Karagiannidis,
“Analyzing grant-free access for URLLC service,” arxiv:2002.07842,
2020.

scheme, while their gap is small.

VI. CONCLUSIONS

In this paper, ofﬂine and online learning algorithms for
uplink wireless VR network with proactive retransmission
scheme were developed to predict viewpoint of wireless VR
users with real VR dataset. Speciﬁcally, for the ofﬂine learning
algorithm, K Cross Validation was used to train ofﬂine n-
order LR, NN and LSTM/GRU learning algorithms for each
VR video and all VR videos. The trained ofﬂine learning
algorithms were used to directly predict
the viewpoint of
the VR user. In the online learning algorithms, the online
n-order LR, NN and LSTM/GRU algorithms would update
their parameters according to the actual viewpoints delivered
from the new VR users through uplink transmission, which
could further improve the prediction accuracy. Meanwhile, a
proactive retransmission scheme was introduced to the online
learning algorithms to guarantee the reliability of uplink trans-
mission. Simulation results shown that our proposed online
GRU algorithm with the proactive retransmission scheme can
achieve the highest prediction accuracy. Meanwhile, the single
training model for each VR video, and for all VR videos
achieved similar prediction accuracy.

REFERENCES

[1] “Virtual reality and augmented reality device sales to hit 99 million
devices in 2021,” https://www.capacitymedia.com/articles/3755961/VR-
and-AR-device-shipments-to-hit-99m-by-2021., 2017.

[2] “The reality of VR/AR growth,” https://techcrunch.com/2017/01/11/the-

reality-of-vrar-growth/., Jan, 2017.

[3] HTC, “Htc vive,” Available: https://www.vive.com/.
[4] Facebook, “Oculus rift,” Available: https://www.oculus.com/.
[5] F. Hu, Y. Deng, W. Saad, M. Bennis, and A. H. Aghvami, “Cellular-
connected wireless virtual reality: Requirements, challenges, and solu-
tions,” IEEE Commun. Mag., vol. 58, no. 5, pp. 105 – 111, May 2020.
[6] Wikipedia, “Field of view,” available at https://en.wikipedia.org/wiki/

Field of view.

[7] Y. Bao, H. Wu, T. Zhang, A. A. Ramli, and X. Liu, “Shooting a moving
target: Motion-prediction-based transmission for 360-degree videos,” in
Proc. IEEE Int. Conf. Big Data, pp. 1161 – 1170, 2017.

[8] “3rd generation partnership project; Technical speciﬁcation group ser-
vices and system aspects; Extended reality (XR) in 5G,” 3GPP TR
26.928, Feb. 2020.

[9] M. Chen, W. Saad, and C. Yin, “Virtual reality over wireless networks:
Quality-of-service model and learning-based resource management,”
IEEE Trans. Wireless Comm., vol. 66, no. 11, pp. 5621 – 5635, Nov.
2018.

[10] M. Chen, W. Saad, C. Yin, and M. Debbah, “Data correlation-aware
resource management in wireless virtual reality (VR): An echo state
transfer learning approach,” IEEE Trans. Comm., vol. 67, no. 6, pp.
4267 – 4280, Jun. 2019.

[11] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Communications, caching, and
computing for mobile virtual reality: Modeling and tradeoff,” IEEE
Trans. Comm., vol. 67, no. 11, pp. 7573 – 7586, Nov. 2019.

[12] S. M. LaValle, A. Yershova, M. Katsev, and M. Antonov, “Head tracking
for the oculus rift,” in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp.
187 – 194, May/Jun. 2014.

[13] J. J. LaViola, “Double exponential smoothing: an alternative to kalman
ﬁlter-based predictive tracking,” in Proc. Workshop on Virtual Environ-
ments, pp. 199 – 206, 2003.

[14] X. Liu and Y. Deng, “Learning-based prediction, rendering and as-
sociation optimization for MEC-enabled wireless virtual reality (VR)
network,” arxiv:2005.08332, 2020.

[15] Y. Bao, T. Zhang, A. Pande, H. Wu, and X. Liu, “Motion prediction
based multicast for 360-degree video transmissions,” in Proc. IEEE
SECON, pp. 1 – 9, Jun. 2017.

