DIREG3D: DIrectly REGress 3D Hands from Multiple Cameras

Ashar Ali

Upal Mahbub

Gokce Dane

Gerhard Reitmayr

Qualcomm Technologies, Inc., San Diego, CA, USA.
asharali, umahbub, gokced, gerhardr@qti.qualcomm.com

Abstract

In this paper, we present DIREG3D, a holistic frame-
work for 3D Hand Tracking. The proposed framework
is capable of utilizing camera intrinsic parameters, 3D
geometry, intermediate 2D cues, and visual information
to regress parameters for accurately representing a Hand
Mesh model. Our experiments show that information like
the size of the 2D hand, its distance from the optical center,
and radial distortion is useful for deriving highly reliable
3D poses in camera space from just monocular informa-
tion. Furthermore, we extend these results to a multi-view
camera setup by fusing features from different viewpoints.

Keywords— 3D Hand tracking, Multi-view, 3D Hand Mesh

Estimation, Convolutional Mesh Decoders.

1. Introduction

Real-time 3D hand tracking from cameras has significant ap-
plications in the fields of augmented, virtual and mixed reality [9],
human-machine interaction, advanced features in automotive and
healthcare industry, consumer electronics and vision-based tele-
operation of robots. Published solutions for 3D hand modeling
use many different types and number of sensors such as IR based
sensors, depth sensors [15], monochrome or RGB, monocular or
multi-view, and multi-modal sensor systems. Yet, in order to build
a reliable end-to-end system, major challenges are far from being
solved. For example, some solutions require explicit overlap of
two or more cameras’ fields of view (FOVs) [4]. In other solution
the power consumption of the chosen sensor modality [14] is too
large for low power solutions.

Our framework addresses these issues with a novel flexible de-
sign to achieve the best 3D modeling performance for both single
and multiple view systems with a low-power on-device implemen-
tation. Following are the three major contributions of the proposed
approach:

• The proposed method is a lightweight framework for directly
regressing 3D keypoints, hand mesh, and/or parametric rep-
resentation of a hand from an input image and a set of meta-
information.

• The method incorporates high-level feature fusion to take ad-
vantage of multiple views of the same hand in a multi-camera
system.

• The novel architecture of the proposed system also enables
effectively combining the advantages of parametric and non-
parametric estimations using a unique loss function.

Hence, we propose a strong and efficient method to directly regress
3D coordinates and mesh parameters from 2D visual cues. Fig-
ure 1 shows the inference pipeline in more detail. The proposed
system can also take crucial meta-information from each camera
as input such as the 2D size and location of a hand in a frame,
radial distortion of the lens, and 3D rotation between the original
optical axis of the lens and center of the bounding box. We also
demonstrate the performance gain for expanding into multi-view
systems by high-level feature fusion from different views. In ad-
dition, we introduce an interesting mechanism to combine the ef-
fectiveness of parametric and non-parametric models, along with a
unique loss function setup that robustly prepares us for the differ-
ent variations in data domains, like ground-truth mesh definitions
and lighting conditions.

2. Literature Review

In terms of problem formulation, most recent works on 3D
hand modeling lean towards estimating 3D keypoints in a root-
relative camera frame, centered at a hand’s wrist in most cases
[1, 8, 7, 11]. Very few approaches tackle the problem of getting
the keypoints in the world frame [4], but they require either more
than one sensor or the dataset is carefully tailored that it only
has image crops with the optical axis passing through the hand
center [13]. In the existing literature, the solutions for 3D hand
pose estimation vary widely depending on different factors such
as types and number of sensors (RGB/grayscale, presence of lens
distortion, availability of depth/infrared/IMU sensors), position-
ing of sensors (on the head-mounted device, wrist, hand, body or
fingers), power/bandwidth/memory constraints (cloud server vs.
edge device, dedicated processor vs. GPU, real-time vs. offline),
the FOV coverage (single FOV vs. overlapping FOV), availabil-
ity of ground-truth data (semi-supervised learning for limited 3D
annotation), and even due to performance goals for user experi-
ence (high frame rate, low keypoint jitter). Another set of ideas
makes use of Generative Adversarial Networks to learn more and
more realistic poses and hand motions from available 3D mocap

1

(a) Monocular flow

(b) Stereo inference

Figure 1: (a) Monocular Case. While training, we jointly optimize for meshes, 3D and derived 2D keypoints. For inference,
only the CNN forward pass provides hand parameters and keypoints directly. (b) Inference in monocular and stereo cases.

and depth sensors [6].

3D pose estimation solutions can be categorized broadly as
model-free and model-based. Model-free methods for 3D pose
estimation are usually an extension of 2D hand pose estimation by
predicting an extra heatmap for normalized depth or distance. To
provide outputs in real-world/camera space, [13] uses additional
biomechanical constraints and an additional branch of fully con-
nected layers to predict real depth. Model-based methods rely on
a predefined mesh model, which could be either parametric, like
MANO [12] or non-parametric, like Convolutional Mesh Autoen-
coder (CoMA) [8] and SpiralConv [7]. Most approaches are still
confined to estimating root-relative outputs. Recently, some re-
searchers investigated joint optimization for 2D and 3D hand pose.
For example, [10] demonstrated that an intermediate 2D cue (like
a heatmap or lixel) can boost performance when used as a guiding
mechanism to estimate 3D keypoints and meshes. However, in an-
other work, Boukhayma et. al [1] showed that using 2D heatmap
guidance is good only for some specific datasets and not as much
for others. For the stereo setups, hand pose estimation has been
directly inspired by multi-view human pose estimation methods.
Some approaches directly rely on basic triangulation as a prior fol-
lowed by classic inverse kinematics to fit a 3D hand model across
2D predictions from different views [4]. Others rely heavily on
the fusion of features in a 2D map space [5]. A very interest-
ing set of ideas also incline towards obtaining rotation-invariant
canonical features [11]. Even though no present work makes use
of any meta-information for 3D hand pose estimation, in [3], the
authors used canonical camera information to improve depth esti-
mates from monocular RGB input. In our proposed framework,
instead of feeding a hand-tailored canonical representation, we
pass extrinsic and intrinsic camera parameters directly as meta-
information.

3. Hand Tracking System Setup

In this work, our goal is to estimate the 3D keypoints and 3D
hand mesh in real-world coordinates from 2D input images. We
are assuming that an efficient hand detector is available which pro-
vides us both the class information and 2D bounding box for all

hands in each camera frame. In our particular system setup, we
have two grayscale cameras with fisheye distortion placed spa-
tially apart but with overlapping FOVs on a head-mounted device
(HMD). We assume that the cameras’ intrinsic and extrinsic pa-
rameters are known. For this particular experiment, we confine
ourselves to estimating only the ego-centric hand poses. There-
fore, after a detector provides the bounding boxes, we can crop
the region of interest (i.e. pixel representation of the 2D hands)
to use as input to our system. Also, we have the bounding box
information and camera parameters readily available to be used as
metadata. The output of our system provides 3D keypoints and
3D Hand Mesh for the two ego-centric hands in real-world co-
ordinates. Additionally, the system returns a compact parametric
definition of the 3D hand.

4. DIREG3D Methodology

Our proposed DIREG3D framework operates in two differ-
ent modes namely, monocular and stereo/multi-view. While the
monocular mode is the default setup, depending on the availabil-
ity of the same hand in multiple cameras, the system can eas-
ily expand to operate in the stereo or multi-view mode without
any redundant computation. Detail about the modes and meta-
information are further described in the following subsections.

The monocular mode of operation is illustrated in Fig. 1a. The
inputs to the system in monocular mode are the 128 × 128 hand
image crop and 28-D normalized metadata vector. We use a CNN
backbone with residual connections to extract a high-level fea-
ture from the cropped hand image, while the meta-information
is passed through a separate fully connected (FC) branch. Intu-
itively, this step coalesces the most meaningful information from
all different data modalities. After that, the fully connected lay-
ers learn to jointly optimize for multiple objectives which guide
them to coherently understand 3D-2D relationships for the given
lens properties. The two features are concatenated and go through
another set of FC layers. These branch out to multiple heads, in-
cluding MANO parameters, input for mesh convolutional decoder,
and independent keypoint predictions. The mesh parameters feed
into a trainable MANO model, therefore the full setup is end-to-

Figure 2: Good (left) and Bad (right) Estimate examples - Inference on synthetic data mimicking HMD capture.

Figure 3: Good (left) and Bad (right) Estimate examples - Inference on Real Data with only 2D finetuning.

end trainable without any pretraining required. We made a choice
to use a non-parametric model as a training aid, because of the
underlying flexibility it has to fit the training samples without any
mathematical constraints like blending weights, and skinning or
forward kinematic chains.

4.1. Feeding Meta-Information

Most fundamental tasks in computer vision rely heavily on the
sensor information only, whereas relatively lesser exploration has
been done in using some other characteristics already available as
metadata. Most works that claim to infer 3D information such
as depth or 3D pose from monocular input provide normalized or
root-relative outputs. This is because the fundamental formulation
of this problem is under-determined. We propose that some care-
ful exploration into how these lenses distort the 3D information
for 2D projection has the potential to directly give us 3D inference
without training for any normalized notations. We feed the fol-
lowing information from our 2D detection boxes to our 3D pose
network as meta-information.

• 1-D L2 Distance of bounding box center from optical axis.
• 4-D Bounding Box corner coordinates.
• 1-D Scale Ratio of 2D box size to pose network input size.
• 9-D Rotation between the optical axis and hand box center.
• 9-D Modified intrinsic matrix based on the bounding box.
• 4-D Radial (barrel) distortion of the Fisheye Lens.

The 28-D feature vector is normalized between ranges on -1
to 1, after applying the min-max normalization based on statistics
from training data for the 28 individual values. All of these fea-
tures combined are fused with very high-level image features and
the network is jointly optimized to predict the 3D keypoints and
meshes. Intuitively, the meta-information enables the neural net-
work model to predict real-world frame 3D points more accurately

and helps to generalize well when trained with data from multiple
camera types.

4.2. Stereo/Multi-View Mode

Having stereo / multi-view cameras benefits this use case in
more than one way. Firstly, it provides more information about
the same hand from a different but known viewpoint; secondly, it
enables a wider field of view for the application as a whole. Our
pipeline for leveraging stereo view to further refine the quality of
the hand pose is shown in Fig. 1b. Along with concatenating the
features from two views, we also make sure that the network is
aware of the extrinsic relationship between the two cameras. But,
instead of passing the original camera extrinsic parameters, we
first calculate the rotation and translation between the two virtual
cameras, such that each virtual camera is looking at the center of
detected bounding boxes from the two views.

4.3. Losses

Our loss function ensures that the model is jointly optimizing
for a variety of crucial objectives, including bone length, bone an-
gles, meshes, and confidence of keypoint predictions in the 3D
domain. The 3D losses comprise a bone-length loss, a bone angle
loss, L1 losses on keypoints/mesh, keypoint variance estimates,
and a regularization term for the MANO mesh parameters. On top
of that, we also define 2D estimation losses on projected keypoints.
For cases with stereo overlap, we introduce an additional loss
on 2D projections of the final 3D keypoint estimates across both
the views. This imbibes knowledge about the stereo geometry
within the learnable framework, and operates equivalently to in-
verse kinematics (IK)-based fitting methods during the training
process.

5. Results and Discussion

Since capturing and annotating the datasets for real 3D hands
is time-consuming and expensive, we created relevant synthetic
data for this task. The synthetic data exactly mimicked the camera
configurations of Snapdragon® XR2 HMD Reference Design [2].
Multiple synthetic hand models and realistic backgrounds were it-
erated to create a total of ≈ 150k stereo images for the fisheye
cameras, a pair which enables a full 180 degree field of view. Ta-
ble 1 provides the Mean Keypoint Location Error (MKPE) on the
in-house synthetic dataset. This is the mean of distances in mm
between the ground truth 3D location and predicted 3D location
of all the 21 keypoints in world space. We notice that for monocu-
lar only cases in synthetic data, we are getting (≈ 12 mm) MKPE
(on ≈ 13k image). Whenever stereo overlap is available(≈ 4k
images), the performance improves further (≈ 11 mm).

Method
DIREG3D-Mono
DIREG3D-Stereo

Mean KPE (mm) AUC (0-50 mm)

12.37
11.39

0.755
0.774

Table 1: Results on the in-house synthetic dataset

The final 8-bit quantized model runs on the HMD device in
under 9 ms for four crops combined. Four is the max number of
crops required in a single forward pass, a case where both ego
hands are visible in left and right cameras. With a lightweight
hand detector to run on stereo frames together, the full solution is
expected to run at 60 fps on the HMD[2].

Some qualitative examples as shown in Fig. 2 demonstrate that
our approach works well in most cases with and without stereo
overlap. And this is still without any prior information about the
hand/temporal feedback. Also, with very limited finetuning on
real data with just 2D annotations, we can see in Fig. 3 that it
still provides very reasonable estimates. As of now, the method
still fails for some difficult cases like overlapping hands and self-
occlusion with one’s own hand. We believe that these can also
be improved upon by using some temporal information and mesh
intersection losses, which is currently a work-in-progress.

6. Conclusion and Future Directions

We present a simple yet efficient framework to estimate hand
poses and full mesh from both single and multiple camera setups.
By design, the proposed method can also flexibly accommodate
lenses with different characteristics. We are currently working to-
wards expanding the method to incorporate temporal feedback.
Our future research goals also include robustly handling more
complex scenarios such as extreme poses, self-occlusion, a wide
variation of lighting, overlapped hands, and hands with wearables
like rings, gloves, watches, and tattoos.

References

[1] A. Boukhayma, R. d. Bem, and P. H. Torr. 3d hand shape
In Proceedings of the
and pose from images in the wild.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 10843–10852, 2019. 1, 2

[2] Q. dev network. Qualcomm Snapdragon® XR2 HMD refer-
ence design. https://developer.qualcomm.com/
hardware/snapdragon-xr2-hmd-reference-
design. 4

[3] J. M. Facil, B. Ummenhofer, H. Zhou, L. Montesano,
T. Brox, and J. Civera. CAM-Convs: Camera-Aware Multi-
In The IEEE
Scale Convolutions for Single-View Depth.
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019. 2

[4] S. Han, B. Liu, R. Cabezas, C. D. Twigg, P. Zhang,
J. Petkau, T.-H. Yu, C.-J. Tai, M. Akbay, Z. Wang, A. Nitzan,
G. Dong, Y. Ye, L. Tao, C. Wan, and R. Wang. Megatrack:
Monochrome egocentric articulated hand-tracking for virtual
reality. ACM Trans. Graph., 39(4), July 2020. 1, 2

[5] Y. He, R. Yan, K. Fragkiadaki, and S.-I. Yu. Epipolar
transformers. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 7779–
7788, 2020. 2

[6] M. Kocabas, N. Athanasiou, and M. J. Black. Vibe: Video
inference for human body pose and shape estimation. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2020. 2

[7] D. Kulon, R. A. Guler, I. Kokkinos, M. M. Bronstein, and
S. Zafeiriou. Weakly-supervised mesh-convolutional hand
reconstruction in the wild. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020. 1, 2

[8] D. Kulon, H. Wang, R. A. G¨uler, M. M. Bronstein, and
S. Zafeiriou. Single image 3d hand reconstruction with mesh
convolutions. In Proceedings of the British Machine Vision
Conference (BMVC), 2019. 1, 2

[9] A. Masurovsky, P. Chojecki, D. Runde, M. Lafci, D. Prze-
wozny, and M. Gaebler. Controller-free hand tracking for
grab-and-place tasks in immersive virtual reality: Design el-
ements and their empirical study. Multimodal Technologies
and Interaction, 4(4), 2020. 1

[10] G. Moon and K. M. Lee. I2l-meshnet: Image-to-lixel pre-
diction network for accurate 3d human pose and mesh esti-
mation from a single rgb image. In European Conference on
Computer Vision (ECCV), 2020. 2

[11] E. Remelli, S. Han, S. Honari, P. Fua, and R. Wang.
Lightweight multi-view 3d pose estimation through camera-
In IEEE/CVF Conference on
disentangled representation.
Computer Vision and Pattern Recognition (CVPR), June
2020. 1, 2

[12] J. Romero, D. Tzionas, and M. J. Black. Embodied hands:
Modeling and capturing hands and bodies together. ACM
Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6),
Nov. 2017. 2

[13] A. Spurr, U. Iqbal, P. Molchanov, O. Hilliges, and J. Kautz.
Weakly supervised 3d hand pose estimation via biomechani-
cal constraints. In European Conference on Computer Vision
(ECCV), 2020. 1, 2

[14] K. L. Sungpill Choi, Jinsu Lee and H.-J. Yoo. A 9.02mw cnn-
stereo-based real-time 3d hand-gesture recognition processor
for smart mobile devices. International Solid-State Circuits
Conference (ISSCC), 2018. 1

[15] J. Taylor et al. Efficient and precise interactive hand track-
ing through joint, continuous optimization of pose and cor-
respondences. ACM Transactions on Graphics (TOG) - Pro-
ceedings of ACM SIGGRAPH 2016, 35, July 2016. 1

