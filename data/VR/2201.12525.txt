2
2
0
2

n
a
J

9
2

]

V
C
.
s
c
[

1
v
5
2
5
2
1
.
1
0
2
2
:
v
i
X
r
a

1

Spherical Convolution empowered FoV
Prediction in 360-degree Video Multicast with
Limited FoV Feedback
Jie Li1, Ling Han1, Cong Zhang1, Qiyue Li2, Zhi Liu3 (cid:66)

1School of Computer and Information, Hefei University of Technology, Hefei, China
2School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China
3 The University of Electro-Communications,Japan
lijie@hfut.edu.cn,hanling@mail.hfut.edu.cn,
Email:

zhangcong@mail.hfut.edu.cn, liqiyue@mail.ustc.edu.cn, liu@ieee.org

that

Abstract—Field of view (FoV) prediction is critical in
360-degree video multicast, which is a key component of
the emerging Virtual Reality (VR) and Augmented Reality
(AR) applications. Most of the current prediction methods
combining saliency detection and FoV information neither
take into account
the distortion of projected 360-
degree videos can invalidate the weight sharing of tra-
ditional convolutional networks, nor do they adequately
consider the diﬃculty of obtaining complete multi-user FoV
information, which degrades the prediction performance.
This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction
framework combining salient features extracted from 360-
degree video with limited FoV feedback information. A
spherical convolution neural network (CNN) is used in-
stead of a traditional two-dimensional CNN to eliminate
the problem of weight sharing failure caused by video
projection distortion. Speciﬁcally, salient spatial-temporal
features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback
FoV information is represented as a time-series model based
on a spherical convolution-empowered gated recurrent unit
network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental
results show that the performance of the proposed method
is better than other prediction methods.

Index Terms—360-degree video, video multicast, FoV pre-

diction, saliency detection, spherical convolution

I. Introduction

Immersive multimedia,

including 360-degree video
and virtual/augmented reality (VR/AR) video, has re-
cently become more and more popular as demand
for interactive applications increases [1], [2]. 360-degree
video, also known as spherical video, is a novel type
of video that can take advantage of the head-mounted
displays (HMDs) to provide an extraordinary immersive
by showing the user the entire 360-degree spherical
scene with his virtual position as the centre of the sphere
[3]. However, there is an enormous gap between the
bandwidth capacity of traditional wireless technologies

(cid:66)

Corresponding author

and the bandwidth requirements of 360-degree video
streaming.

−

Due to the limitations of the HMD, the user can
see only approximately 12%
15% of the entire video,
which is often referred to as the ﬁeld of view (FoV) [4].
Therefore, transmitting 360-degree video content in its
entirety, as is the strategy of YouTube [2], results in a
large waste of bandwidth and computing resources. To
provide a good experience for users, adaptive streaming
techniques are proposed to optimize the delivery. Tile-
based adaptive streaming is one of the most popular
methods for 360-degree video transmission [5]. It divides
360-degree video frames into multiple tiles in space and
encodes each tile into multiple representations at dif-
ferent bitrates and quality levels. Then the transmission
is optimized by adaptively controlling the video bitrate
according to users’ FoV and network conditions [6], [7].
In addition, adaptive multicast over cellular networks,
as proposed in [8], is considered a more promising way
of transmitting 360-degree video. It can be implemented
to support large-scale live sessions to maximize the
use of wireless resources while optimizing the quality
of experience for users interacting with complex [9] ,
bandwidth-hungry 360-degree video. However, in some
multicast applications (e.g., VR cinema), the bandwidth
is much smaller than that of
of the uplink channel
the downlink channel due to the asymmetry of the
cellular network [10]. This means that not all users can
be supported to upload their FoV information in real
time, which could otherwise lead to a feedback explosion
problem [11]. Hence, only a few users can be selected
to feedback FoV information. Therefore, for 360-degree
video multicast system,
it is of great importance to
perform FoV prediction with only a few FoV feedback
streams.

Currently, FoV prediction methods can be divided
into two categories: trajectory-based and content-based.
The trajectory-based approach predicts future FoVs from
historical head movement trajectories of one user (single-
user) or other users (cross-user). The single-user methods

 
 
 
 
 
 
predict future FoV based on the historical FoVs of users
[12], and cross-user approaches assume that diﬀerent
users have similar viewing behavior for the same video
and predict the future FoV of the target user based
on the historical FoVs of other users [13]. However,
considering only head movements of users for FoV pre-
diction is inaccurate. Therefore, other studies [14]–[16]
have combined historical view trajectories with video
object tracking to predict future FoV. For example, [14]
proposes a deep learning-based FoV prediction scheme,
HOP, which jointly exploits the viewer’s historical FoV
trajectory and target tracking through a long short-term
memory (LSTM) network.

Content-based methods usually extract a salient fea-
ture heatmap from video content, which can be consid-
ered as the most attractive part in a video frame. Because
360-degree videos are diﬃcult to store and process in
the spherical domain, they are usually projected onto a
two-dimensional (2D) plane using equal rectangular pro-
jection (ERP), cylindrical projection (CYL), or truncated
pyramid projection (TPP), which leads to distortion.
Such distortion of spatial variation invalidates weight
sharing in convolutional neural networks (CNNs) and
makes traditional deep learning-based video saliency de-
tection methods inapplicable to 360-degree videos [17].
One way to eliminate the distortion eﬀect is to change
the projection method. [18] proposes to project the image
in cubemap (CMP) format, where the six faces can
be considered as six virtual perspective camera image
planes and be processed with conventional CNNs. How-
ever, this approach does not eliminate distortion, but
only minimizes its eﬀect. Moreover, the face boundaries
introduce additional discontinuities that may require
subsequent processing to combine the six image planes
outputs. Another approach is to change the convolution
method. A CNN model with a kernel stretched to ﬁt
the shape of the patches on the ERP format is proposed
in [19], this can avoid the distortion problem to some
extent. However, the shape of the ﬁlter in their scheme
depends on the longitude of the patch on the sphere,
leading to non-sharing of the convolution kernel param-
eters and making the computational and storage costs
much higher.

In addition, approaches have been proposed to com-
bine salient features with users’ historical FoV infor-
mation to predict future FoVs. Such as in [20], one
user’s history information is considered, which intro-
duces uncertainty that may aﬀect the prediction accuracy
of FoV, especially in multicast scenarios. This is the case
because the single user’s attention may shift for a short
time during the viewing procedure, resulting in histori-
cal trajectories that may not exactly match with video
saliency. However, it is possible to use FoV feedback
from a limited number of users combined with 360-
degree video saliency detection to make more accurate
FoV predictions in multiuser scenarios.

To this

this paper proposes a spherical
convolution-based FoV prediction method, SPVP360, in

end,

2

360-degree video multicast scenarios with limited FoV
feedback, combining 360-degree video saliency as the
main feature and several users’ FoVs as supplementary
features. Speciﬁcally, a SPherical Convolutional Neural
Network (SPCNN) is used instead of the traditional
2D CNN for saliency detection and FoV prediction to
eliminate projection error. The ﬁrst step is to construct
the salient feature extraction module and the convolu-
tional block attention module (CBAM) based on SPCNN
to perform saliency detection. Then a SPherical Convo-
lutional Gated Recursive Unit (SP-ConvGRU) is used,
which is a time-series model of FoV information from
the FoV feedback from a small number of users. Finally,
salient spatial-temporal video features and historical FoV
information are combined for accurate FoV prediction.
Experimental results show that the performance of the
proposed method is better than other prediction meth-
ods.

In summary, the contribution of this study is threefold:
1) Presenting the special FoV prediction problem in a
360-degree video multicast scenario and designing a
prediction method with video saliency and a limited
number of real users’ FoV information.

2) Using SPCNN to eliminate the projection distortion
of 360-degree video, and proposing a saliency de-
tection model based on SPCNN to extract spatial-
temporal features from 360-degree video and in-
troduce an attention mechanism in the network to
improve the performance.

3) Performing exhaustive experiments to show that the
proposed method achieves better results than other
methods on publicly available 360-degree video
datasets.

The remainder of this paper is organized as follows.
Section II describes related work on 360-degree video
and FoV prediction. Section III speciﬁcally introduces the
proposed algorithm. Section IV presents the experimen-
tal setup, and performance evaluation results. Finally, the
conclusions of this paper are given in Section V.

II. Related Work

This section presents a brief introduction to 360-degree
video and its projection, and introduces work related
to 360-degree video saliency detection and viewpoint
prediction.

A. 360-Degree Video and Projection

A 360-degree video can be recorded by a single
omnidirectional camera (e.g., a Samsung Gear 360) or
aggregated and stitched using multiple cameras with
separate 2D videos. To facilitate storage and processing
of 360-degree video from a spherical domain, it is usually
projected onto a 2D plane [21]. Current projection meth-
ods for 360-degree video include ERP, CYL, CMP, and
TPP. For instance, the CYL projection maps the latitude
and longitude lines as constant-spaced horizontal and

vertical
lines respectively to form a cylinder, which
results in blanking of the north and south poles. The
ERP projection directly uses the latitude and longitude
on the sphere as the horizontal and vertical coordinates
respectively on the original frame, which results in
greater distortion in the polar region of the sphere, as
shown in Fig. 1. The pixels at the north and south poles
are stretched to be extremely long, and the closer they
are to the poles, the more severe the distortion. The
CMP projection maps a spherical video to an external
cube. The upper and lower faces of the cube correspond
to the polar regions, and the four faces in the middle
correspond to the equatorial region. At present, the most
popular projection formats in panoramic video are ERP
and CMP.

Fig. 1. Projection of a spherical image onto a 2D plane in ERP format
causes distortion.

In addition, the HMD senses the user’s viewing di-
rection to reproject the 360-degree video in 2D format
onto the sphere during 360-degree video playback. Users
can enjoy spherical video content by wearing an HMD.
However, due to the limitations of the HMD, the user
can only see a portion of the entire video. This unique
property of 360-degree video viewing provides an op-
portunity to reduce bandwidth consumption. Ideally,
if the future FoV is known in advance, only the cor-
responding part of the video needs to be transmitted
instead of the whole 360-degree video, which greatly
reduces bandwidth consumption. Therefore, FoV pre-
diction is of great importance as a key technology for
FoV-adaptive streaming. FoV prediction is based on the
fact that users tend to focus on interesting (salient)
features of the scene that attract their attention. Saliency
detection can reveal these features. In addition, users’
FoV has a temporal meaningful association. Overall, the
combination of video saliency detection and historical
head motion trajectories (real-time) of users can be used
to predict users’ FoV in the near future. Generally, FoV
prediction algorithms can be divided into two categories:
trajectory-based [12]–[16], [22]–[27] and content-based
[17]–[20], [28]–[39].

B. Trajectory-Based Prediction Methods

Trajectory-based approaches [12]–[16], [22]–[24], [40]–
[42] predict future viewing direction from one user’s
(single-user) or other users’ (cross-user) historical head
movement trajectories. [12], [40]–[42] proposed to use

3

historical head movement data to predict future FoV.
However, they only consider the user’s historical view-
point trajectory and ignore the relationship between
the video target and the future FoV, resulting in poor
prediction accuracy. Therefore, some studies [14], [15],
[25], [43] have combined historical trajectories with video
target tracking. For instance, [43] developed a new real-
time 360-degree video streaming FoV prediction method
based on video content-based motion tracking and dy-
namic user interest modeling. [25] proposed a fast and
eﬃcient online FoV prediction model, PARIMA, to pre-
dict future FoVs by using the user’s past FoVs and the
trajectories of the main objects as a proxy for the video
content. However, there are signiﬁcant diﬀerences in user
trajectories in 360-degree videos. Diﬀerent users may
exhibit diﬀerent viewing behavior patterns even for the
same 360-degree video, and therefore it is insuﬃcient to
only use historical head motion trajectories to predict the
FoV for all users.

C. Content-Based Prediction Methods

Content-based prediction approaches

can extract
salient features in 360-degree videos using saliency de-
tection to perform FoV prediction. Saliency detection
analyzes the content of video frames to ﬁnd the areas
that are most likely to attract users. Although deep
in traditional
CNNs have been extremely successful
video saliency prediction, there has been little research
on 360-degree video saliency prediction. Most existing
methods [44]–[46] directly apply techniques designed for
traditional videos to 360-degree videos, but traditional
deep learning-based video saliency detection methods
are not applicable to 360-degree videos, because 360-
degree videos projected to the 2D plane in ERP format
are distorted. This distortion from spatial variation inval-
idates the weight sharing of traditional CNNs, resulting
in poor saliency detection. For instance, [44] proposed a
video saliency model that adds an attention mechanism
to the CNN-LSTM network structure to achieve fast
end-to-end saliency learning. The CNN-LSTM is one
of the latest saliency prediction methods with the best
performance for 2D videos, but it is not very eﬀective
for saliency detection of 360-degree videos. Also, there
are methods [47], [48] in which the combination of
video information and audio information is proposed for
saliency detection.

To eliminate the impact caused by distortion, one strat-
egy [18], [32], [33], [49] is to convert the 360-degree video
into multiple perspective views and process them with
traditional CNN on each perspective view. However, this
approach does not eliminate distortion, but only mini-
mizes its impact. For example, [33] proposed a saliency
prediction network for 360-degree videos, taking video
frames and optical ﬂows in CMP format as input, and
then performing saliency prediction of these features by
decoder and bidirectional convolutional LSTM. Because
these views are processed separately, the face boundaries

Prejection FoVSevere distortion in the polar region  Little distortion in the equatorial regionintroduce an additional boundary problem that may
require subsequent processing to merge the face out-
puts, which can aﬀect saliency prediction performance.
Another strategy is to counteract the eﬀect of distortion
by changing the convolution method, as in [19], [36],
[50]. For example, [36] proposed a new framework,
SphereNet, which adjusts the sampling grid position
of the convolution ﬁlter according to the geometry of
the spherical image representation and wraps the ﬁlter
around the sphere to avoid the eﬀect of image distortion.
In addition, content-based prediction methods can also
use the user’s own past heatmap sequences with salient
features for FoV prediction [17], [20], [28]–[31], [34], [35],
[37]–[39]. As an illustration, [20] proposed a multi-CNN
network model that takes video frames in CMP format
as input to perform corresponding saliency detection on
each face separately, obtains a comprehensive saliency
map using the CNN network, and then predicts the
next point of view by combining the viewers’ previous
viewing trajectories. However, these methods only con-
sider the historical information of a single user, which
introduces uncertainty and may aﬀect the accuracy of
FoV prediction. For example, a FoV prediction method
was proposed in [51]. Unlike the existing methods, this
method uses the saliency of the video and the user head
tracking trajectory as the inputs for prediction, and the
saliency of the video is extracted using 3DCNN, which
reduces the computational requirement. Additionally,
[38] proposed a FoV prediction model for 360-degree
video that uses a spherical harmonic function to extract
features in diﬀerent frequency bands and diﬀerent ori-
entations to estimate saliency, and introduces visual un-
certainty and balancing mechanisms in FoV prediction.

III. SPVP360 architecture

This

section presents an SPCNN-based method,
SPVP360, which is a multi-source FoV prediction method
combining salient features extracted from 360-degree
videos with limited FoV feedback information. The ﬁrst
step is to outline the structure of the proposed network,
followed by explaining the details of SPVP360 and each
component. Finally, the loss function is introduced to
train the network.

A. Overview

The architecture of the proposed network is shown
in Fig. 2. Spherical convolution is used instead of tra-
ditional 2D convolution to extract salient features and
FoV features in 360-degree video to improve predic-
tion accuracy. The proposed network consists of two
modules: the salient feature extraction module and the
FoV prediction module. The feature extraction module
takes video frames as input and extracts spatial and
temporal features. The FoV prediction module takes
FoV information from a small number of users as input
and extracts FoV features. Accurate predictions can then
be made based on video saliency and FoV features.

4

Unlike traditional methods, the proposed network with
SPCNN to eliminate the projection distortion eﬀect uses
minority FoV information combined with salient features
to predict the most attractive FoV information and is
suitable for 360-degree video multicast scenarios.

B. Spherical Convolution on an ERP Panorama

Because 360-degree video is diﬃcult to store and
process in the spherical domain, it is usually projected
onto a 2D plane. Commonly used projection methods,
such as ERP and CYL, cause some degree of distortion.
If a 2D convolution-based neural network model is used
to extract saliency in 360-degree video, the distortion of
the projected image will result in low feature extraction
accuracy. The reason for this is that traditional 2D CNN
has only translational invariance, but no rotational in-
variance. Moreover, the distortion of spatial variation
caused by projection invalidates the sharing of transla-
tion weights in the 2D convolutional network, as shown
in Fig. 3. To solve this problem, a spherical convolution
[52] is introduced, in which a spherical kernel is deﬁned.
This kernel can be rotated and convolved with the spher-
ical image patch. As shown in Fig. 4, the parameters of
the spherical crown kernel are the same everywhere on
the sphere, which implies that the kernel is shared.

Spherical convolution is deﬁned on a spherical domain
rather than the 2D image domain. It is an operation on
the spherical images f and the kernel ﬁlter g on the
sphere manifold S2. To simplify the notation, the image
f and the ﬁlter g can be modeled as a function on the
RK, where K is the number of channels and
sphere S2
R3 with norm 1 [53].
S2 is deﬁned as the set of points x
The shift operation on the 2D plane corresponds to the
rotation operation on the spherical domain. Assuming
that R is a rotation matrix, a rotation can be performed
using the matrix-vector product Rx. Obviously, the pixel
value of x should remain unchanged before and after
rotation, that is:

→

∈

[LR g](x) = g(R−

1x)

(1)

where LR is the rotation operator of the function g.

The proposed spherical convolution is deﬁned anal-
ogously to the 2D version. Its essence is to multiply
image and the
and sum the pixels of the spherical
corresponding part of the constantly rotated convolution
kernel.

g] =< LR g, f >=

[ f

∗

K

(cid:90) s2

(cid:88)k=1

g(R−

1x) f (x)dx

(2)

∗

where

denotes the spherical convolution operator.

For an ERP format projected 360-degree video, the
rotation matrix R can be parameterized by spherical

5

Fig. 2. SPCNN-based FoV prediction network combining video saliency and limited FoV feedback.

Fig. 3. Using standard convolution (e.g., with a 3
3 kernel) on a
panorama in ERP format suﬀers from distortion of sampling locations
(red) close to the poles, resulting in a failure of parameter sharing.

×

coordinates λ
∈
be expressed as:

[0, 2π] and ψ

∈

[0, π]. Then Eq. 2 can

(cid:34)

K

g](λ, ψ) =

f

λ(cid:48), ψ(cid:48)

g

λ(cid:48)

λ, ψ(cid:48)

ψ

sin λ(cid:48)dλ(cid:48)dψ(cid:48)

−

−

(cid:88)k=0
(3)
where f
is denoted as a point on f and
is the rotated convolution kernel. sin λ(cid:48)
λ(cid:48)
g
is the compensation for projection distortion. In addi-

λ(cid:48), ψ(cid:48)
ψ

λ, ψ(cid:48)
(cid:0)

−

−

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)
(cid:1)

[ f

∗

(cid:0)

Fig. 4. Parameter sharing. The ﬁrst row shows the region of the original
spherical crown kernel and the region of the spherical crown kernel
in the projected panorama. The second row shows the image after
spherical convolution with the same latitude parameters shared and
with diﬀerent latitude parameters that are not shared.

singleframeχtconsecutiveframes(χt,χt+τ)FewuserfeedbackFoVsXNt−1XNt−kXnt−1Xnt−kN∑n=1Xnt−1N∑n=1Xnt−kPoolingSoftmax+ChannelattentionmoduleSpatialattentionmodule××S-convGRUσσtanh∗∗1−∗+HtRtIt˜HtSPVP360SaliencyFeatureExtractionmodule(ST-SPCNN)Spatialfeatureextractionmodule(S-SPCNN)Temporalfeatureextractionmodule(T-SPCNN)FoVpredictionmodule2SP-ConvGRUThepredictedFoVPtatmomenttFSt,i=8FTt,i=8FSTtF′tPstPvt1632641282563842561283264128128256256512256+384CBAM+512IM1+SP-ConvGRU2SP-ConvGRU2SP-ConvGRU1SP-ConvGRU11281IMInferencemodule++SphericalConvolutionlayerMaxpoolinglayerUnsamplinglayerSaliencyfeaturesFoVfeaturesConcatenatingWeightedsummationH2t−kH2t−2H1t−2H1t−kH1t−1H2t−1 Standard kernelthe sampling locations (red)Spherical crown kernel360-degree videoSpherical crown kernel region in the projected panoramaRegion of spherical crown kernel centred at the north poleSpherical crown kernel  in the projected panoramaProjectionRepresenting the spherical crown kernelImage patch at the same latitude have the same weight, image patch at different latitudes have different weights.Image patchConvolved patchConvolutionOutputtion, the deﬁnition has the parameter sharing property,
instead of compensation for projection distortion as in
traditional 2D convolution.

C. Salient Feature Extraction Module (ST-SPCNN)

The mechanism of the human visual system suggests
that only small regions receive the most visual attention
at high resolution, whereas other peripheral regions
receive negligible attention at low resolution [54]. Fur-
thermore, human attention is more likely to be drawn
to the moving parts in a video frame. Considering both
spatial and temporal features of 360-degree video, two
sets of feature extraction models are designed: the Spa-
tial SPherical Convolutional Neural Network (S-SPCNN)
and the Temporal SPherical Convolutional Neural Net-
work (T-SPCNN). The S-SPCNN takes one video frame
as input to extract spatial features Fs, and T-SPCNN
takes a stack of two consecutive video frames as input to
extract temporal features (i.e., motion features) Ft. CBAM
can then be embedded in the network to regulate its
spatial-temporal features.

×

S-SPCNN consists of a contraction path and an expan-
sion path, with the feature representation Fs
i at the output
of layer i. The contraction path resembles a traditional
convolutional network, with ﬁve layers of spherical con-
volution and a convolution kernel of size 3
3. Each
layer of spherical convolution is followed by the action
of the Rectiﬁed Linear Unit (ReLU) activation layer and a
maximum pooling layer of size 2
2 with a step size of 2.
The pooling layer is removed in the ﬁfth layer to obtain
7 pixels. The
a feature map with an output size of 14
expansion path consists of three deconvolutional layers,
each of which includes an unpooling layer, a spherical
convolution layer of size 3
3, and a ReLU function. In
×
addition, a batch normalization (BN) layer is added to
each spherical convolutional layer to avoid overﬁtting.
The ﬁnal space specialization obtained is the FS

×

×

t,i=8.

Because moving parts in a video frame are more likely
to attract a user’s attention, T-SPCNN is proposed to
extract motion features (i.e., temporal features) of video
frames. The stacked optical ﬂow between consecutive
frames (χt, χt+τ) is used as input to T-SPCNN. The struc-
ture of T-SPCNN resembles that of S-SPCNN, except
that the ﬁrst six convolutional layers of the T-SPCNN
compressed path are spherical convolutions with a step
size of 2 and no pooling layers, and the unfolding
path consists of two upper convolutional layers. Each
layer undergoes BN and ReLU function actions after
spherical convolution. To take full advantage of multi-
scale information with diﬀerent receptive domains, the
input to the unfolding path is a concatenation of output
features from diﬀerent layers. For example, the input to
the ﬁrst layer of the unfolded path is the concatenation of
the output features of the previous layer and the output
features of the fourth layer of the contracted path.

The ﬁnal output time feature is FT

t,i=8
are then fed into the inference module to generate spatial

t,i=8. FS

t,i=8 and FT

6

feature saliency map and temporal feature saliency map,
respectively. Then they are concatenated to generate
spatial-temporal features FST
112,
which can be expressed as:

t of total size 384

56

×

×

t = (FS
FST

t,i=8; FT

t,i=8)

(4)

t

Attention not only tells people where to focus, but
also improves the representation of interest. It can help
extract important features and suppress unnecessary
features. A CBAM resembling that described in [55] is
added after the spatial-temporal features FST
to improve
saliency detection accuracy. The CBAM consists of a
channel attention module (CAM) and a spatial attention
module (SAM), arranged in channel-space order. The
CAM uses the channel relationships between features to
generate a channel attention graph. To eﬃciently com-
pute channel attention Mc(FST
t ), the spatial information
is ﬁrst aggregated by averaging pooling and maximum
pooling operations. Then FST
Max are fed into the
multilayer perceptron (MLP). The output feature vectors
are ﬁnally combined using the summation of elements
to generate the channel attention function Mc(FST
t ) with
1. Brieﬂy, the channel attention is
a total size of 64
×
calculated as follows:

Avg and FST

×

1

Mc(FST

t ) = σ(MLP(AvgPool(FST

t )) + MLP(MaxPool(FST
Max)))

Avg)) + MLP(McFST

t )))

= σ(MLP(FST

(5)
where σ denotes the sigmoid function and FST
Avg and FST
Max
represent the average pooling and maximum pooling
features, respectively.

The SAM uses the spatial relationships between fea-
tures to generate spatial attention. To compute spatial
attention, two pooling operations are ﬁrst performed on
the spatial features to generate average pooling features
and maximum pooling features. These are then concate-
nated, and a convolutional layer is used to generate the
c). Because the output of channel
spatial attention Ms(M(cid:48)
attention is the input of spatial attention, the spatial
attention can be obtained as:

M(cid:48)

c = Mc(FST
t )

FST
t

(cid:12)

Ms(M(cid:48)

c) = σ( f 7
×
= σ( f 7

7([AvgPool(M(cid:48)
7([(M(cid:48)
Avg); (M(cid:48)
×

c); MaxPool(M(cid:48)
Max)]))

c)]))

(6)

(7)

Avg and M(cid:48)

where M(cid:48)
Max are the average and maximum
pooling features after aggregating channel information,
respectively.
denotes element-wise multiplication. In
this way, the channel attention values are propagated
along the spatial dimension. The total size of Ms(M(cid:48)
c) is
64

×
The overall attention in a channel-space order process

112.

56

(cid:12)

×

can be summarized as:

Fcbam
t

c)
= Ms(M(cid:48)

M(cid:48)
c

(cid:12)

(8)

By concatenating Fst and the output of CBAM Fst
t can be obtained:

the ﬁnal spatial-temporal features F(cid:48)

cbam,

t = (FST
F(cid:48)
t

; Fcbam
t

)

(9)

An inference module A f is constructed to generate the
saliency mapping, which models the saliency of video
frames based on the spatial-temporal features F(cid:48) of the
video frames. The module is a SPCNN structure consist-
ing of a two-layer spherical convolution. Mathematically,
Ps can be computed as:

t = A f (FST
Ps
t

; Fcbam
t

)

(10)

Fig. 5. Salient feature generated by the proposed extraction module.
Red circles are moving parts in the video.

Figure 5 presents results from the salient feature ex-
traction module. Clearly, the T-SPCNN focuses only on
the region with moving persons in the middle of the
video frame, ignoring other salient regions, whereas the
S-SPCNN has an overly large range of saliency regions
and contains insigniﬁcant regions as well. In contrast, the
fused spatial-temporal feature map from the proposed
model is more accurate.

D. FoV Prediction Module

The model proposed in this paper combines salient
spatial-temporal features of video with FoV feedback
information from a small number of users to build a
multi-source time-series prediction model. When users
watch a video, their FoVs are not only attracted by the
content of the current video frame, but also causally
associated with the content of multiple frames in the
previous sequence, resulting in long-distance sequencing
between the FoVs of multiple frames. Therefore, the FoV
information is represented from the feedback from a
small number of users as a time-series model based on
SP-ConvGRU, and the salient spatial-temporal features
of video are combined for FoV prediction.

During multicast transmission„ a few selected users’
lines of sight can be collected from the HMD. On a spher-
ical image, they are typically represented in the Eulerian
coordinate system as (α, β, γ), where α is the cross-roll
angle, β is the pitch angle, and γ is the yaw angle. First,
these are translated into longitude and latitude:

1
λ = sin−

β

α2 + β2 + γ2

1/2





(cid:0)
ψ = tan−

1(

γ/α)

(cid:1)

−





(11)

(12)

7

where λ and ψ denote the longitude and latitude, re-
spectively.

The user’s line of sight on the ERP panorama can then

be calculated as:

u = (λ/2π + 0.5)

v = (ψ/π + 0.5)

U

V

×

×

(13)

(14)

where U and V are the width and height of the image,
respectively. Then the FoV heatmap can be considered
as a grayscale image with a 2D Gaussian distribution
intensity value inside the FoV and a value of 0 outside
the FoV, which can be considered as a rectangular area
centered on the line of sight1.

SP-ConvGRU introduces spherical convolution, which
has powerful spatial information preservation and mod-
eling capability for temporal features, to extend tradi-
tional GRU. The element-wise multiplication
is re-
placed by a spherical convolution operator in the input-
to-state and state-to-state transformations of GRU. Tak-
ing the ﬁrst layer of SP-ConvGRU as an example, a single
SP-ConvGRU at time step t can be written as follows:

(cid:12)

(16)

(15)

1, Xt])
−

1, Xt])
[Ht
−
[Ht
[Rt ∗
Ht
−

It = σ (Wz ∗
Rt = σ (Wr ∗
˜Ht = tanh (Wo ∗
It)
Ht = (1
∗
where σ and tanh are the activation functions of the
sigmoid and the hyperbolic tangent, respectively; It and
Rt are the reset and update gates for frame t; and Wz,
Wr, and Wo denote the kernel weight parameters of each
convolutional layer. ˜Ht represents a memory cell, and Ht
represents the output of the hidden layer.

Ht
1 + It ∗

1, Xt])
−
˜Ht

(18)

(17)

−

A two-layer SP-ConvGRU (2SP-ConvGRU) network is
then constructed to predict the FoV. The numbers of
feature maps for the 2SP-ConvGRU layers are both 64.
3. 2SP-
Each layer has a convolution kernel size of 3
×
ConvGRU takes a sequence of FoVs, XN
, XN
2, XN
1,
t
t
−
−
−
as input and takes advantage of the correlation of FoV
information through the hidden layer. XN
k is obtained by
t
−
N
n=1 Xn
k represents the FoV of a single user n
t
−
k. The hidden state in the second layer of
at moment t
(cid:80)
2SP-ConvGRU, Ht, is then fed into the inference module
to estimate the FoV Pv at time t.

k, and Xn
−

· · ·

t
−

k,

t

Pv

t = A f (Ht)

(19)

Finally, a fusion method based on the magnitude
of global-regional disparity is used to combine the es-
timated FoV Pv and the video saliency Ps. This fu-
sion strategy approximates the human visual perception
mechanism by mutual suppression of similar proximity
features, which can reduce the saliency weights of the
uniform distribution in salient regions and highlight

1The size of the FoV depends on the speciﬁc HDM equipment. For

HTC Vive, this is approximately 110

90 degrees.

×

T-SPCNNS-SPCNNTwo consecutive frames of 360-degree videoST-SPCNNlocal salient peaks. The speciﬁc steps are as follows.
First, Pv and Ps are normalized to [0, 1]. After this, global
maxima Mv and Ms can be obtained. Next, the image is
region-segmented, the extremum of each small region
is calculated, and the average of all regional extrema is
taken to obtain mv and ms. Finally, the estimated FoV Pv
and the video saliency Ps are linearly weighted together
to obtain P.

Pt = Ps

t ∗

(Ms −

ms)2 + Pv
t ∗

(Mv −

mv)2

(20)

By comparing the global maximum M with the aver-
age mi of the corresponding local maxima, the signiﬁcant
features in the signiﬁcance map become more obvious
and should be given more weight when the diﬀerence is
large. On the contrary, when the signiﬁcant features in
the signiﬁcance map are more uniform, they should be
given less weight to suppress them when the diﬀerence
is small.

Note that the proposed method can predict the user’s
FoV at moment t + κ by combining the saliency maps
extracted from video frames at t + κ and FoV feedback
at t, where κ denotes the time interval used for future
prediction. In practice, κ can be considered as the uplink
transmission delay for FoV feedback.

E. Loss Deﬁnition

Saliency detection is usually evaluated using diﬀerent
metrics to capture diﬀerent quality factors. Commonly
used metrics such as mean square error (MSE) and inter-
section over union (IoU) are designed for spatially uni-
formly discretized conventional images. Because a pro-
jected 360-degree image is non-uniformly spatially dis-
cretized, the squared error of each pixel in the panorama
is weighted by its stereo angle on the sphere so that the
panorama is homogeneous after discretization.

When training SPVP360, the length of the input FoV
feedback sequence is set to R frames. The parameters of
the saliency detection model are ﬁxed during training to
extract saliency maps for the corresponding video frames
and fuse them with the FoV maps. The SPVP360 loss
function can then be deﬁned as the average MSE over R
frames:

DSPVP360 =

1
R

R
(cid:88)r
∈

Dloss(P, G)

(21)

Dloss(P, G) =

1
ΛΨ

ωλ,ψ(Pλ,ψ −

Gλ,ψ)2

(22)

Ψ
ψ
Λ (cid:88)
(cid:88)λ
∈
∈
where P and G indicate the output of the SPVP360 model
and the ground truth, respectively. Pλ,ψ and Gλ,ψ denote
the image point on the map at latitude and longitude
λ, ψ, respectively. ωλ,ψ is the weight of each point, which
is proportional to its solid angle θλ,ψ, deﬁned as ωλ,ψ =
θλ,ψ/4π (4π is the solid angle of the unit ball).

8

IV. Experiments
This section describes the experimental setup and re-
sults of the proposed approach. First, the network setup
is presented in the experiments, and then the proposed
method is compared with other methods.

A. Experimental Setup

TABLE I
Experimental parameter settings.

SPVP360

Initial learning rate
Batch size
Optimizer
Momentum
Weight decay

0.1
25
SGD
0.9
5
10−

∗

1

Dataset and Analysis: To train and test the proposed
model, we use two datasets. In the ﬁrst dataset [56], 27
volunteers are asked to randomly watch videos with
an HTC Vive HDM at a ﬁxed starting position. This
provides a benchmark for predicting the saliency of 104
panoramic 360-degree videos. We use 80 video clips for
training and 24 video clips for testing. We also evaluate
our model on another publicly available dataset [28],
where the dataset contains 208 panoramic videos, each
ranging from 20 seconds to 60 seconds length, with
at least 31 users’ viewing tracks for each video. The
public datasets we use show a large diversity in con-
tent, including indoor scenes, outdoor activities, music
performances, sports games, short ﬁlms, etc. We classify
the videos into low and high motion scenes based on
the motion patterns in the videos. High sports scenes
include roller skating 1, roller skating 2, basketball,
etc. Low sports scenes videos include indoor scenes,
outdoor scenes, etc. We number the high motion scene
video (named H-video1) and the low motion scene video
(named L-video1) separately.

In addition, to gain insight into the head movement
of the user while watching the 360-degree video, we
introduce an additional set of measures to investigate the
change in FoV in successive frames. For demonstration
purposes, we measure the head movement frequency
by calculating the average latitude/longitude diﬀerence
between two consecutive frames in the whole video and
set a threshold range for longitude and latitude [16]. If
the mean longitude is greater than 0.65◦, it is deﬁned as
a higher frequency of head movement in the horizontal
direction (denoted as More). If the mean longitude is
between 0.3◦ and 0.65◦, the head movement frequency
is deﬁned as medium frequency head movement in the
horizontal direction (denoted as Middle). If the mean
longitude is less than 0.3◦, the head movement frequency
is deﬁned as lower frequency head movement in the
horizontal direction (denoted as Less). The deﬁnition of
mean latitude in vertical direction is the same as that
of mean longitude, the thresholds of which are set as
0.1◦, 0.3◦, i.e., if the mean latitude is between 0.1◦ and

0.3◦, the vertical head movement frequency is deﬁned
as medium frequency of head movement (denoted as
Middle). In addition, if the diﬀerence of head movement
frequency in horizontal or vertical direction is less than
or equal to one level, the head movement frequency is set
to the higher level of the two. If the diﬀerence between
the head movement in horizontal or vertical direction is
more than one level, the head movement frequency is
set to Middle.

Implementation details: The proposed model has
been implemented in Pytorch, and the experimental code
is available to the public at GitHub2. The training setup
is shown in Table I. All the experiments are performed
on Nvidia Tesla V100 GPU or Nvidia Titan 2060i GPU.
The sequence length used to train 2SP-ConvGRU is 3. To
investigate the eﬀect of time interval, four sets of inter-
vals κ =
are used for prediction in
these experiments. This means that the estimated FoV at
time t + κ is obtained based on FoV feedback at time t
and the saliency at time t + κ.

0.03s, 0.5s, 1s, 1.5s, 2s

{

}

FoV prediction evaluation metrics: To evaluate the
performance of the proposed FoV prediction method, we
use three metrics: accuracy, precision, and recall [51]. We
divide each frame into a number of small regions, each
of which is called a block. Where a block pixel value
of all zeros indicates a region that is not viewed by the
user, and a block pixel value of not all zeros indicates
a region that is viewed by the user. In calculating the
accuracy, we use the Jaccard index. In particular, the
number of blocks correctly predicted to be viewed is the
number of intersection of the predicted heat map and
the blocks viewed in the ground truth. Accuracy is the
ratio of the number of blocks correctly predicted to be
viewed to the number of merges between the predicted
heatmap and the ground truth of the viewed blocks.
Precision is a calculation of the ratio of the number of
tiles to be viewed for a correct prediction to the number
of tiles viewed in the predicted heat map. The higher the
precision, the fewer tiles are incorrectly predicted. Recall
is a calculation of the ratio of the number of tiles correctly
predicted to be viewed to the number of tiles viewed
in the actual heat map. The higher the recall value, the
fewer tiles are incorrectly predicted not to be viewed.

Saliency detection evaluation metrics: To evaluate the
performance of the proposed saliency detection module,
ST-SPCNN,
its output is compared with the ground
truth and with other competitors. Three metrics are used:
Normalized Scanpath Saliency (NSS), Linear Correlation
Coeﬃcient (CC), and Area Under the Curve (AUC),
as proposed in [57]. The location-based NSS metric
calculates the average distance between the normalized
predicted FoV and the ground truth eye gaze position,
which reﬂects the prediction accuracy. The distribution-
based CC metric represents a linear correlation between
the predicted FoV and the ground truth. AUC gauges
the variance between the estimated FoV and the human-

2https://github.com/hankebo/SPVP360

9

annotated ground truth by calculating the true positive
(TP) and false positive (FP) rates.

Other competitors: The proposed method is compared
with ﬁve competitors, PanoSalNet [17], SalGAN360 [18],
DeepVS [45], GazeGAN [46] and Flare [41]. PanoSal-
Net includes a CNN-based panoramic saliency detection
model and an LSTM-based head movement prediction
model for 360-degree videos that uses transfer learning
on a traditional saliency model. SalGAN360 is a saliency
detection model based on generative adversarial net-
works, which rotates the panorama in the training set
through multiple horizontal and vertical angles. DeepVS
is an advanced, high-performance video saliency detec-
tion method for conventional 2D video. GazeGAN is a
saliency model for 2D video-based generative adversar-
ial networks that achieves high performance on multiple
datasets.

Because SalGAN360 and DeepVS are just saliency
detection models, during the FoV prediction evalua-
tion, ablation experiments are performed to combine
SalGAN360 and DeepVS separately with the proposed
2SP-ConvGRU. The resulting models are denoted as
SalGAN360+ and DeepVS+. The results of the proposed
ST-SPCNN saliency detection module are also illus-
trated and compared with those of SalGAN360, DeepVS,
and the saliency detection part of PanoSalNet (called
PanoSalNet-). The prediction results of 2SP-ConvGRU
(named Viewport) are also compared with SPVP360. In
addition, we add Flare as an experimental comparison,
which is a simple method to predict future FoV using
historical head motion data through machine learning.

B. Experimental Results

The Results of head movement frequency analysis:
The frequency of user’s head movements in diﬀerent
types of videos is analyzed in the experiments. In Fig. 6
the high-video and low-video represent the distribution
of head movement frequencies of all users for that
particular single video, respectively. The ave-high and
ave-low represent the average distribution of user head
movement frequencies for all high motion type videos
and low motion type videos, respectively. The ave-all
represents the average distribution of head movement
frequencies of users for all videos. From the Fig 6(a),
we can see that most users’ head movement frequency
is at Middle. In addition, users tend to have more
signiﬁcant head movement in horizontal direction and
less frequent head movement in vertical direction, partly
because prolonged head down and head up can make
users feel uncomfortable.

FoV prediction results: During the experiments, dif-
ferent time intervals and diﬀerent numbers of users
sending FoV feedbacks are set for FoV prediction; the re-
sults are shown in Fig. 7-9. The relationship between the
number of feedback users (i.e., users sending FoV) and
the accuracy and latency as well as the computational
eﬀort is also studied. The delay time indicates the time

10

(a)

(b)

(c)

Fig. 7. The performance comparison for FoV prediction in dataset [56].

prediction methods combining saliency detection and
users’ feedback FoV have higher-accuracy results than
the prediction methods considering only video saliency.
Moreover, the accuracy becomes better as the number of
feedback users increases. Furthermore, Viewport outper-
forms even DeepVS and SalGAN360 as the number of
user FoVs increases.

In order to further study how the number of feedback
users aﬀect the FoV prediction, diﬀerent number of
feedback users are selected and the prediction interval is
set to κ0 = 0.03 on the Nvidia Titan 2060i GPU for pre-

(a)

(b)

Fig. 6. The distribution of head movement frequency.

required for one prediction and the computational eﬀort
is determined using FLOPs. In addition, the eﬀect of
diﬀerent head movement frequencies on FoV prediction
is also investigated.

As illustrated in Fig. 7, ﬁve users’ FoV feedback is
randomly selected, and the prediction interval κ = 0.03s
is used for prediction. The ﬁrst four sets of results are
averages of FoV predictions for four individual videos,
respectively. The last set of data is the average of FoV
predictions for all 360-degree videos, with a total of 10
videos. From the experimental results, we can see that
our prediction method outperforms other methods in
terms of accuracy, precision and recall. Although the
prediction performance of our model degrades in some
videos, our method still performs better than the com-
parison methods. In addition, we set the conﬁdence level
0.088 for
to 95% with a conﬁdence interval of 0.8589
accuracy, 0.8856
0.0652
for recall.

0.0715 for precision, and 0.8933

±

±

±

The next step is to investigate the eﬀect of the number
of users sending FoV on the prediction results. Diﬀerent
numbers of feedback users are selected, and κ0 = 0.03s
is used for prediction. When there is no feedback user,
only video saliency information is used for FoV pre-
diction, and the results are shown in Fig. 8. The FoV

high-video1high-video2low-video1low-video2ave-highave-lowave-all0102030405060708090100frequency of head movement of all users(%)Longitude more middle lesshigh-video1high-video2low-video1low-video2ave-highave-lowave-all0102030405060708090100Latitude  frequency of head movement of all users(%) more middle lessH-video1H-video2L-video1L-video2ave-all0.00.20.40.60.81.0  accuracy SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport FlareH-video1H-video2L-video1L-video2ave-all0.00.20.40.60.81.0   precision SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport FlareH-video1H-video2L-video1L-video2ave-all0.00.20.40.60.81.0  recall SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport FlareTABLE II
The impact of the number of users sending FoV.

The number of users

Accuracy

Latency(s)

Computational requirement (GFLOPs)

2

5

8

11

0.8463

0.8589

0.8709

0.8941

0.2732

0.3101

0.3428

0.3874

5.2817

7.3298

9.3779

11.426

11

diction. The relationship between diﬀerent numbers of
feedback users, prediction accuracy, latency, and compu-
tational requirement is shown in Table II. From this table,
we can see that the accuracy, latency, and computational
requirement increase with the increase of the number
of feedback users. Higher accuracy, lower latency and
lower computational requirement indicate better per-
formance of the prediction method. When more than
10 users are selected for prediction, the computational
power of the Nvidia Titan 2060i GPU is capped and
latency is greatly increased. Note that considering that
latency is important for prediction, we cannot choose too
many users and must make a trade-oﬀ between system
requirements and performance.

TABLE III
The time required for prediction on different devices.

The GPU type

SP-STCNN(s)

SP-ConvGRU(s)

Nvidia Tesla V100

Nvidia Titan 2060i

0.1253

0.2528

0.0235

0.0573

We have also tested it on Nvidia Tesla V100 GPU
and Nvidia Titan 2060i GPU, respectively. The results
are shown in Table III. The prediction times are 0.1488s
and 0.3101s on Nvidia Tesla V10 GPU and Nvidia Ti-
tan 2060i GPU, respectively. Note that this is obtained
without code optimization and we believe this is fast
enough for a seamless video experience. In addition, the
performance of the mobile GPU is similar to that of the
Nvidia Titan 2060i GPU [58], which proves the feasibility
of the proposed solution in mobile devices.

TABLE IV
The effects of different head movement frequency on prediction
performance.

Type

More

Middle

Less

Accuracy

Precision

0.8545

0.8589

0.8601

0.8781

0.8856

0.8912

Recall

0.8872

0.8933

0.9001

We next investigate the eﬀect of users with diﬀerent
head movement frequencies on the prediction results. We
select ﬁve users from each of the diﬀerent head motion
frequency types (i.e., More, Middle, Less) for FoV pre-
diction. The prediction interval is set to κ0 = 0.03s. The
performance of our prediction method for diﬀerent head
movement types is shown in Table IV. We can see that
there is little diﬀerence in the prediction performance of

our method in these three cases, although our method
performs only slightly better when the head motion
frequency is low, which demonstrates the feasibility of
the proposed scheme in the presence of diﬀerent user
head movements.

Next, the eﬀect of time interval on FoV prediction
is evaluated. Five feedback users’ FoVs are selected;
the results are shown in Fig. 9. We can see that the
prediction accuracy of our method decreases slightly as
the time interval increases, however, the performance of
our method is still better than the comparison methods,
demonstrating the eﬀectiveness of the proposed scheme.
In addition, the results of diﬀerent prediction methods
are visualized, as shown in Fig. 10. It is apparent that
SalGAN360+ and PanoSalNet- incorrectly treat almost
all equatorial regions as salient regions, whereas DeepVS
ignores many salient regions. ST-SPCNN extracts a
saliency map that is close to the ground truth. How-
ever, this does not mean that the ST-SPCNN saliency
regions are error-free. With the addition of Viewport
in SPVP360, SalGAN360+, and PanoSalNet, the regions
that should not have been considered signiﬁcant are
corrected to some extent. The signiﬁcant regions ignored
by DeepVS+ are also corrected to some extent by adding
FoV. However, the proposed model’s FoV prediction
map is still closer to the ground truth than the others. In
summary, both qualitative and quantitative results show
that the prediction forecast from the proposed model is
the most eﬀective.

Saliency detection results: The performance of the
proposed saliency detection model is also evaluated. The
results of ST-SPCNN are compared with PanoSalNet-,
SalGAN360, DeepVS and GazeGAN. The NSS, CC, and
AUC results are shown in Fig. 11. The ﬁrst four sets
of results are averaged over the saliency maps of four
individual videos, respectively. The middle two sets of
results are averaged over the saliency maps of the high-
motion videos and the low-motion videos, respectively.
The last set of data is averaged over the saliency map
of all the 360-degree videos, representing a total of ten
videos. Higher values of NSS, CC, and AUC imply more
accurate saliency detection. From the Fig 11(a)-11(c)-
11(e), we can see that the proposed saliency detection
method signiﬁcantly outperforms the other methods on
three evaluation metrics, which proves the eﬀectiveness
of the proposed 360-degree video saliency prediction
model. The proposed ST-SPCNN eliminates the eﬀect
of distortion by convolution, which greatly improves its
performance. Although SalGAN360 proposes changes to

12

(a)

(b)

(c)

(a)

(b)

(c)

Fig. 8. The eﬀect of the number of users sending FoV on the prediction
performance.

Fig. 9.

Impact of κ on prediction accuracy, precision and recall.

the projection to minimize the eﬀects of distortion, its
performance is still limited. This may have been the
case because each individual face output has its own
unique saliency region, and combining the individual
face outputs produces many more insigniﬁcant regions
in the saliency detection results. PanoSalNet- is designed
for 360-degree videos, DeepVS is designed for 2D videos
ﬁxed in a single FoV, and GazeGAN considers data
enhancement strategies to enable better performance of
saliency detection in 2D videos. However, they do not
consider the eﬀect of video distortion, resulting in poor
performance of saliency detection in 360 videos.

We have also evaluated our model on dataset2 [28]
and selected eight videos for testing. In Fig 11(b)-11(d)-
11(f), the ﬁrst four sets of results are averaged over
the saliency maps of the individual videos, respectively.
The middle two sets of results are averaged over the
saliency maps of the high motion videos and the low
motion videos, respectively, and the last set of data is
averaged over the saliency maps of all 360 degree videos.
Our proposed saliency detection method continues to
outperform the other four methods on all three met-
rics, which proves that the proposed 360-degree video
saliency prediction model is eﬀective.

0246810120.00.20.40.60.81.0   accuracyNumber of users SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare0246810120.00.10.20.30.40.50.60.70.80.91.0  precisionNumber of users SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare0246810120.00.10.20.30.40.50.60.70.80.91.0   recallNumber of users SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare0.030.511.520.00.20.40.60.81.0   accuracyTime interval (s) SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare0.030.511.520.00.20.40.60.81.0  precisionTime interval (s) SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare0.030.511.520.00.20.40.60.81.0  recallTime interval (s) SPVP360 PanoSalNet SalGAN360+ DeepVS+ ST-SPCNN PanoSalNet- SalGAN360 DeepVS Viewport Flare 13

Fig. 10. Saliency maps of ﬁve randomly selected videos generated by the proposed method and by seven other methods, as well as the ground
truth.

We next evaluate the contribution of the diﬀerent
components of the model in the saliency detection, and
the ablation experiments are performed using the ﬁrst
dataset [56]. For this purpose, we have constructed four
diﬀerent variants, including: (1) S-SPCNN; (2) T-SPCNN;
(3) ST-SPCNN- (Indicates no attention mechanism); and
(4) ST-SPCNN. And we also compare the variants set in
our experiments with the existing method ST-CNN [45].
The impact of diﬀerent components. We compare the
performance of the spherical convolution-based network
with existing networks. S-SPCNN and S-CNN denote
video spatial feature extraction networks. T-SPCNN and
T-CNN denote video temporal feature extraction net-

TABLE V
The performance of different network components.

Components

S-SPCNN

S-CNN

T-SPCNN

T-CNN

ST-SPCNN-

ST-CNN

NSS

0.6301

0.5819

0.782

0.617

0.9653

0.8487

CC

0.4058

0.3837

0.4384

0.4137

0.5587

0.4634

AUC

0.5471

0.5173

0.5762

0.5335

0.6069

0.5589

works. ST-SPCNN and ST-CNN denote spatial-temporal
feature extraction networks. S-SPCNN, T-SPCNN and

DeepVS+SalGAN360+InputSPVP360GazeGANPanoSalNetDeepVSPanoSalNet-SalGAN360ST-SPCNNGround truth 14

(a) The NSS results in dataset1.

(b) The NSS results in dataset2.

(c) The CC results in dataset1.

(d) The CC results in dataset2.

Fig. 11. The performance comparison for saliency detection.

(e) The AUC results in dataset1.

(f) The AUC results in dataset2.

ST-SPCNN- denote the spherical convolution-based net-
works. From Table V, ﬁrstly, the performance of the
combined spatial-temporal feature network outperforms
that of the spatial feature extraction network alone and
the temporal feature extraction network alone, indicating
that considering both temporal and spatial features of
the video can yield better results. Secondly, the per-
formance of the spherical convolution-based network
is higher than the existing methods. This is because
the distortion of the 2D plane of the 360-degree video
projection causes the parameter sharing of the 2D con-
volution to fail, which makes the network detection less
eﬀective. This also indicates that spherical convolution

can eliminate the eﬀect of distortion and thus improve
the performance of the saliency detection model.

TABLE VI
The impact of different CBAM.

CBAM

no CBAM

with 2d-CBAM

with SP-CBAM

NSS

0.9653

1.0271

1.1736

CC

0.5587

0.5765

0.6234

AUC

0.6069

0.6278

0.6619

The impact of CBAM. We have also tested the per-
formance of the attention module with diﬀerent con-
volutions. Table VI shows the possible conﬁgurations,

H-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.20.40.60.81.01.2  NSS ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANH-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.20.40.60.81.01.2  NSS ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANH-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.10.20.30.40.50.60.7  CC ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANH-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.20.40.60.8  CC ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANH-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.10.20.30.40.50.60.7  AUC ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANH-video1H-video2L-video1L-video2ave-highave-lowave-all0.00.10.20.30.40.50.60.7  AUC ST-SPCNN PanoSalNet- SalGAN360 DeepVS GazeGANwhere the ﬁrst one indicates the inclusion of no attention
mechanism, the second one indicates the inclusion of an
attention mechanism based on 2D convolution, and the
last one indicates the inclusion of an attention mecha-
nism based on spherical convolution. Clearly, the results
in the table indicate that adding the attention module
based on spherical convolution yields better saliency
detection results.

V. Conclusions
This paper has proposed a spherical convolution-
based method, SPVP360, which is a multi-source FoV
prediction method combining salient features extracted
from 360-degree video with limited FoV feedback in-
formation. A spherical convolutional network was used
instead of a traditional 2D convolutional network to
eliminate the problem of weight sharing failure in
the convolutional network caused by video projection
distortion. Speciﬁcally, salient spatial-temporal features
were extracted through a spherical convolution-based
saliency detection model, ST-SPCNN, a small amount
of user-feedback FoV information was represented as a
time-series model based on the spherical convolution-
based GRU network, and ﬁnally the extracted salient
video features were combined to predict future user
FoVs. The approach taken by the spherical convolution-
based saliency detection model (ST-SPCNN) is to ﬁrst
extract spatial-temporal features using S-SPCNN and T-
SPCNN, and then to actively enhance spatial-temporal
features by the attention mechanism and map them into
the saliency map. The proposed method is compared
with several FoV prediction methods on two 360-degree
video dataset. The experimental results show that the
performance of the proposed method is better than other
prediction methods.

VI. Acknowledgments
This work is supported in part by grants from
the National Natural Science Foundation of China
(52077049), Anhui Provincial Natural Science Founda-
tion (2008085UD04), and Fundamental Research Funds
for the Central Universities (PA2020GDJQ0027).

References
[1] Z. Liu, Q. Li, X. Chen, C. Wu, S. Ishihara, J. Li, and Y. Ji, “Point
cloud video streaming: Challenges and solutions,” IEEE Network,
vol. 35, no. 5, pp. 202–209, 2021.

[2] C.-L. Fan, W.-C. Lo, Y.-T. Pai, and C.-H. Hsu, “A survey on 360
video streaming: Acquisition, transmission, and display,” ACM
Computing Surveys (CSUR), vol. 52, no. 4, pp. 1–36, 2019.

[3] A. Xu, X. Chen, Y. Liu, and Y. Wang, “A ﬂexible viewport-adaptive
processing mechanism for real-time vr video transmission,” in
2019 IEEE International Conference on Multimedia Expo Workshops
(ICMEW), 2019, pp. 336–341.

[4] J. Zhang, Y. Zhong, Y. Han, D. Li, C. Yu, and J. Mo, “A 360 ◦video
adaptive streaming scheme based on multiple video qualities,” in
2020 IEEE/ACM 13th International Conference on Utility and Cloud
IEEE, 2020, pp. 402–407.
Computing (UCC).

[5] J. Li, R. Feng, W. Sun, Z. Liu, and Q. Li, “Qoe-driven coupled
uplink and downlink rate adaptation for 360-degree video live
streaming,” IEEE Communications Letters, vol. 24, no. 4, pp. 863–
867, 2020.

15

[6] C. Guo, Y. Cui, and Z. Liu, “Optimal multicast of tiled 360 vr
video,” IEEE Wireless Communications Letters, vol. 8, no. 1, pp.
145–148, 2018.

[7] C. Guo, L. Zhao, Y. Cui, Z. Liu, and D. W. K. Ng, “Power-eﬃcient
wireless streaming of multi-quality tiled 360 vr video in mimo-
ofdma systems,” IEEE Transactions on Wireless Communications,
vol. 20, no. 8, pp. 5408–5422, 2021.

[8] O. Eltobgy, O. Arafa, and M. Hefeeda, “Mobile streaming of
live 360-degree videos,” IEEE Transactions on Multimedia, vol. 22,
no. 12, pp. 3139–3152, 2020.

[9] G. Zhai and X. Min, “Perceptual image quality assessment: a
survey,” Science China Information Sciences, vol. 63, no. 11, pp. 211–
301, 2020.

[10] K. Lee, G. Guerrero, S. Cha, Y. Kim, and S. Cho, “Vr theater,
a virtual reality based multi-screen movie theater simulator for
verifying multi-screen content and environment,” in SMPTE 2017
Annual Technical Conference and Exhibition. SMPTE, 2017, pp. 1–13.
[11] M. Rumney et al., LTE and the evolution to 4G wireless: Design and

measurement challenges.

John Wiley & Sons, 2013.

[12] Y. Bao, H. Wu, T. Zhang, A. A. Ramli, and X. Liu, “Shooting
a moving target: Motion-prediction-based transmission for 360-
degree videos,” in 2016 IEEE International Conference on Big Data
(Big Data).

IEEE, 2016, pp. 1161–1170.

[13] A. T. Nasrabadi, A. Samiei, and R. Prakash, “Viewport prediction
for 360 ◦videos: A clustering approach,” in Proceedings of the
30th ACM Workshop on Network and Operating Systems Support for
Digital Audio and Video, ser. NOSSDAV ’20. New York, NY, USA:
Association for Computing Machinery, 2020, p. 34–39.

[14] J. Tang, Y. Huo, S. Yang, and J. Jiang, “A viewport prediction
framework for panoramic videos,” in 2020 International Joint
Conference on Neural Networks (IJCNN).

IEEE, 2020, pp. 1–8.

[15] S. Petrangeli, G. Simon, and V. Swaminathan, “Trajectory-based
viewport prediction for 360-degree virtual reality videos,” in 2018
IEEE International Conference on Artiﬁcial Intelligence and Virtual
Reality (AIVR), 2018, pp. 157–160.

[16] J. Chen, X. Luo, M. Hu, D. Wu, and Y. Zhou, “Sparkle: User-
aware viewport prediction in 360-degree video streaming,” IEEE
Transactions on Multimedia, vol. 23, pp. 3853–3866, 2021.

[17] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique:
Detecting 360-degree video saliency in head-mounted display for
head movement prediction,” in Proceedings of the 26th ACM Inter-
national Conference on Multimedia, ser. MM ’18. New York, NY,
USA: Association for Computing Machinery, 2018, p. 1190–1198.
[18] F.-Y. Chao, L. Zhang, W. Hamidouche, and O. Deforges, “Sal-
gan360: Visual saliency prediction on 360 degree images with
generative adversarial networks,” in 2018 IEEE International Con-
ference on Multimedia Expo Workshops (ICMEW).
IEEE, 2018, pp.
01–04.

[19] Y.-C. Su and K. Grauman, “Learning spherical convolution for fast
features from 360 ◦imagery,” in Proceedings of the 31st International
Conference on Neural Information Processing Systems, ser. NIPS’17.
Curran Associates, Inc., 2017, p. 529–539.

[20] X. Li, S. Wang, C. Zhu, L. Song, R. Xie, and W. Zhang, “View-
port prediction for panoramic video with multi-cnn,” in 2019
IEEE International Symposium on Broadband Multimedia Systems and
IEEE, 2019, pp. 1–6.
Broadcasting (BMSB).

[21] M. Yu, H. Lakshman, and B. Girod, “A framework to evaluate
omnidirectional video coding schemes,” in 2015 IEEE International
Symposium on Mixed and Augmented Reality. IEEE, 2015, pp. 31–36.
[22] Q. Yang, J. Zou, K. Tang, C. Li, and H. Xiong, “Single and
sequential viewports prediction for 360-degree video streaming,”
in 2019 IEEE International Symposium on Circuits and Systems
(ISCAS), 2019, pp. 1–5.

[23] J. Vielhaben, H. Camalan, W. Samek, and M. Wenzel, “Viewport
forecasting in 360 ◦virtual reality videos with machine learning,”
in 2019 IEEE International Conference on Artiﬁcial Intelligence and
Virtual Reality (AIVR).

IEEE, 2019, pp. 74–747.

[24] M. Jamali, S. Coulombe, A. Vakili, and C. Vazquez, “Lstm-
based viewpoint prediction for multi-quality tiled video coding
in virtual reality streaming,” in 2020 IEEE International Symposium
on Circuits and Systems (ISCAS).

IEEE, 2020, pp. 1–5.

[25] L. Chopra, S. Chakraborty, A. Mondal, and S. Chakraborty,
“Parima: Viewport adaptive 360-degree video streaming,” in Pro-
ceedings of the Web Conference 2021, ser. WWW ’21. New York, NY,
USA: Association for Computing Machinery, 2021, p. 2379–2391.
[26] L. Sun, Y. Mao, T. Zong, Y. Liu, and Y. Wang, “Flocking-based live
streaming of 360-degree video,” in Proceedings of the 11th ACM

16

[46] Z. Che, A. Borji, G. Zhai, X. Min, G. Guo, and P. Le Callet, “How
is gaze inﬂuenced by image transformations? dataset and model,”
IEEE Transactions on Image Processing, vol. 29, pp. 2287–2300, 2019.
[47] X. Min, G. Zhai, K. Gu, and X. Yang, “Fixation prediction through
multimodal analysis,” ACM Trans. Multimedia Comput. Commun.
Appl., vol. 13, no. 1, oct 2016.

[48] D. Zhu, D. Zhao, X. Min, T. Han, Q. Zhou, S. Yu, Y. Chen, G. Zhai,
and X. Yang, “Lavs: A lightweight audio-visual saliency predic-
tion model,” in 2021 IEEE International Conference on Multimedia
and Expo (ICME).

IEEE, 2021, pp. 1–6.
[49] M. Eder and J.-M. Frahm, “Convolutions on spherical images,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops, 2019, pp. 1–5.

[50] C. H. Vo, J.-C. Chiang, D. H. Le, T. T. Nguyen, and T. V. Pham,
“Saliency prediction for 360-degree video,” in 2020 5th Interna-
tional Conference on Green Technology and Sustainable Development
(GTSD), 2020, pp. 442–448.

[51] S. Park, A. Bhattacharya, Z. Yang, S. R. Das, and D. Samaras,
“Mosaic : Advancing user quality of experience in 360-degree
video streaming with machine learning,” IEEE Transactions on
Network and Service Management, vol. PP, no. 99, pp. 1000–1015,
2021.

[52] J. R. Driscoll and D. M. Healy, “Computing fourier transforms and
convolutions on the 2-sphere,” Advances in applied mathematics,
vol. 15, no. 2, pp. 202–250, 1994.

[53] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, “Spherical
CNNs,” in International Conference on Learning Representations
(ICLR), 2018.

[54] W. Lin and C.-C. J. Kuo, “Perceptual visual quality metrics: A
survey,” Journal of visual communication and image representation,
vol. 22, no. 4, pp. 297–312, 2011.

[55] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional
block attention module,” in Proceedings of the European Conference
on Computer Vision (ECCV), September 2018.

[56] Z. Zhang, Y. Xu, J. Yu, and S. Gao, “Saliency detection in 360
◦videos,” in Proceedings of the European Conference on Computer
Vision (ECCV), September 2018, pp. 488–503.

[57] Z. Bylinskii, T. Judd, A. Oliva, A. Torralba, and F. Durand, “What
do diﬀerent evaluation metrics tell us about saliency models?”
IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 41, no. 3, pp. 740–757, 2019.

[58] Q. Liang, P. Shenoy, and D. Irwin, “Ai on the edge: Rethinking ai-
based iot applications using specialized edge architectures,” 2020.

Multimedia Systems Conference, ser. MMSys ’20. New York, NY,
USA: Association for Computing Machinery, 2020, p. 26–37.
[27] Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Sama-
ras, and M. Hoai, “Predicting goal-directed human attention using
inverse reinforcement learning,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2020, pp. 193–
202.

[28] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and S. Gao, “Gaze
prediction in dynamic 360 ◦immersive videos,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2018, pp.
5333–5342.

[29] M. Qiao, M. Xu, Z. Wang, and A. Borji, “Viewport-dependent
saliency prediction in 360 ◦video,” IEEE Transactions on Multime-
dia, vol. 23, pp. 748–760, 2021.

[30] X. Feng, Y. Liu, and S. Wei, “Livedeep: Online viewport prediction
for live virtual reality streaming using lifelong deep learning,” in
2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR).
IEEE, 2020, pp. 800–808.

[31] X. Chen, A. T. Z. Kasgari, and W. Saad, “Deep learning for
content-based personalized viewport prediction of 360-degree vr
videos,” IEEE Networking Letters, vol. 2, no. 2, pp. 81–84, 2020.

[32] B. Dedhia, J.-C. Chiang, and Y.-F. Char, “Saliency prediction
for omnidirectional images considering optimization on sphere
domain,” in ICASSP 2019 - 2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2019, pp.
2142–2146.

[33] Y. Zhang, F. Dai, Y. Ma, H. Li, Q. Zhao, and Y. Zhang, “Saliency
prediction network for 360◦ videos,” IEEE Journal of Selected Topics
in Signal Processing, vol. 14, no. 1, pp. 27–37, 2020.

[34] P. Zhao, Y. Zhang, K. Bian, H. Tuo, and L. Song, “Laddernet:
Knowledge transfer based viewpoint prediction in 360 ◦video,”
in ICASSP 2019-2019 IEEE International Conference on Acoustics,
IEEE, 2019, pp. 1657–1661.
Speech and Signal Processing (ICASSP).
[35] C.-L. Fan, S.-C. Yen, C.-Y. Huang, and C.-H. Hsu, “Optimizing
ﬁxation prediction using recurrent neural networks for 360◦ video
streaming in head-mounted virtual reality,” IEEE Transactions on
Multimedia, vol. 22, no. 3, pp. 744–759, 2020.

[36] B. Coors, A. P. Condurache, and A. Geiger, “Spherenet: Learning
spherical representations for detection and classiﬁcation in om-
nidirectional images,” in Proceedings of the European Conference on
Computer Vision (ECCV), 2018, pp. 518–533.

[37] Y. Zhu, G. Zhai, and X. Min, “The prediction of head and
eye movement for 360 degree images,” Signal Processing: Image
Communication, vol. 69, pp. 15–25, 2018.

[38] Y. Zhu, G. Zhai, X. Min, and J. Zhou, “The prediction of saliency
map for head and eye movements in 360 degree images,” IEEE
Transactions on Multimedia, vol. 22, no. 9, pp. 2331–2344, 2020.
[39] ——, “Learning a deep agent to predict head movement in 360-
degree images,” ACM Trans. Multimedia Comput. Commun. Appl.,
vol. 16, no. 4, Dec. 2020.

[40] S. Petrangeli, V. Swaminathan, M. Hosseini, and F. De Turck, “An
http/2-based adaptive streaming framework for 360 ◦virtual real-
ity videos,” in Proceedings of the 25th ACM International Conference
on Multimedia, ser. MM ’17. New York, NY, USA: Association for
Computing Machinery, 2017, p. 306–314.

[41] F. Qian, B. Han, Q. Xiao, and V. Gopalakrishnan, “Flare: Prac-
tical viewport-adaptive 360-degree video streaming for mobile
devices,” in Proceedings of the 24th Annual International Conference
on Mobile Computing and Networking, ser. MobiCom ’18. New
York, NY, USA: Association for Computing Machinery, 2018, p.
99–114.

[42] P. K. Yadav and W. T. Ooi, “Tile rate allocation for 360-degree tiled
adaptive video streaming,” in Proceedings of the 28th ACM Inter-
national Conference on Multimedia, ser. MM ’20. New York, NY,
USA: Association for Computing Machinery, 2020, p. 3724–3733.
[43] X. Feng, V. Swaminathan, and S. Wei, “Viewport prediction for
live 360-degree mobile video streaming using user-content hybrid
motion tracking,” Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, vol. 3, no. 2, pp. 1–22, 2019.
[44] W. Wang, J. Shen, F. Guo, M.-M. Cheng, and A. Borji, “Revisiting
video saliency: A large-scale benchmark and a new model,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018, pp. 4894–4903.

[45] L. Jiang, M. Xu, T. Liu, M. Qiao, and Z. Wang, “Deepvs: A deep
learning based video saliency prediction approach,” in Proceedings
of the European Conference on Computer Vision (ECCV), September
2018, pp. 602–617.

