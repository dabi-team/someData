1
2
0
2

r
p
A
0
3

]

V
C
.
s
c
[

1
v
7
3
8
4
1
.
4
0
1
2
:
v
i
X
r
a

RobustFusion: Robust Volumetric Performance
Reconstruction under Human-object Interactions
from Monocular RGBD Stream

Zhuo Su∗, Lan Xu∗, Dawei Zhong∗, Zhong Li, Fan Deng, Shuxue Quan and Lu FANG§

1

Abstract—High-quality 4D reconstruction of human performance with complex interactions to various objects is essential in real-world
scenarios, which enables numerous immersive VR/AR applications. However, recent advances still fail to provide reliable performance
reconstruction, suffering from challenging interaction patterns and severe occlusions, especially for the monocular setting. To ﬁll this
gap, in this paper, we propose RobustFusion, a robust volumetric performance reconstruction system for human-object interaction
scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex
interaction patterns and severe occlusions. We propose a semantic-aware scene decoupling scheme to model the occlusions explicitly,
with a segmentation reﬁnement and robust object tracking to prevent disentanglement uncertainty and maintain temporal consistency.
We further introduce a robust performance capture scheme with the aid of various data-driven cues, which not only enables
re-initialization ability, but also models the complex human-object interaction patterns in a data-driven manner. To this end, we
introduce a spatial relation prior to prevent implausible intersections, as well as data-driven interaction cues to maintain natural
motions, especially for those regions under severe human-object occlusions. We also adopt an adaptive fusion scheme for temporally
coherent human-object reconstruction with occlusion analysis and human parsing cue. Extensive experiments demonstrate the
effectiveness of our approach to achieve high-quality 4D human performance reconstruction under complex human-object interactions
whilst still maintaining the lightweight monocular setting.

Index Terms—4D Reconstruction, Performance Reconstruction, Robust, Human-object Interaction, RGBD Camera

(cid:70)

1 INTRODUCTION

T HE rise of virtual reality and augmented reality (VR and AR)

to present information in an innovative and immersive way
has increased the demand for human-centric 4D (3D spatial plus
1D temporal) content generation, with various applications from
entertainment to commerce, from gaming to education, from mili-
tary to art. Further, reconstruct the 4D models of human activities
under human-object interactions both robustly and conveniently
remains unsolved, which suffers from challenging interaction
patterns and severe occlusions. It evolves as a cutting-edge yet bot-
tleneck technique and has recently attracted substantive attention
of both the computer vision and computer graphics communities.
Early model-based methods [1], [2], [3], [4], [5] suffer from
pre-scanned templates or inefﬁcient run-time performance, which
are unacceptable for daily interactive application. Recent volumet-
ric approaches have eliminated the reliance on the templates and
increased both the effectiveness and efﬁciency with modern GPUs.
The high-end solutions [6], [7], [8], [9], [10] achieve realistic
human-object reconstruction using multi-view studio setup which
provides sufﬁcient view observation to solve the challenging

•
•

•

•

∗ : Equal Contribution
Z. Su, D. Zhong and L. Fang are with Dept. of Electronic Engineering
at Tsinghua University, and Beijing National Research Center for Infor-
mation Science and Technology. Z. Su is also with Dept. of Automation,
Tsinghua University. Lan Xu is with School of Information Science and
Technology, Shanghaitech University. Fan Deng, Zhong Li and Shuxue
Quan are with OPPO US Research Center.
This work is supported in part by Natural Science Foundation of China
(NSFC) under contract No. 61860206003 and 62088102,
in part by
OPPO.
§ Correspondence Author: E-mail: fanglu@tsinghua.edu.cn

(a) RGBD stream (Input)

(b) 4D reconstruction (Output)

Fig. 1. Illustration of the system and results of our RobustFusion

interaction and occlusion ambiguity. However, their complex and
expensive multi-view studio setting leads to the high restriction of
the wide daily applications. Differently, the monocular volumetric
approaches adopt the handiest commercial RGBD camera and
temporal fusion pipeline. Early general solutions [11], [12], [13],
[14] handle general dynamic scenes without disentangling human
and objects, suffering from careful and orchestrated motions. Re-
cent solutions [15], [16], [17] embed the human parametric models
into the fusion pipeline to handle more complex motions. Within
this category, our conference version RobustFusion [18] (denoted
as RobustFusion(Conf.)) further enables more robust monocular
capture using various data-driven visual cues such as motion [19],
[20], geometry [21], [22] or semantic segmentation [23]. It gets
rid of the self-scanning constraint for monocular capture with
re-initialization ability, where the captured performer does not

 
 
 
 
 
 
need to turn around carefully to obtain complete reconstruction.
However, these monocular approaches with human priors neglect
to model the mutual inﬂuence between human and object, leading
to limited reconstruction under the challenging interaction scenar-
ios. On the other hand, various researchers [24], [25], [26], [27],
[28], [29], [30], [31] reconstruct the 4D relations between humans
and the objects or the environments. However, they only recover
the naked human bodies or heavily rely on speciﬁc pre-scanned
object and scene templates to optimize the spatial arrangement.
Researchers pay less attention to strengthen the template-less
volumetric performance capture by utilizing the rich human-object
interaction priors, especially for the monocular setting.

In this paper, we attack the above challenges and propose
RobustFusion, a robust human-object volumetric performance
capture system combined with various data-driven visual and
interaction cues using only a single RGBD sensor (with optional
multi-view setup). As illustrated in Fig. 1, our approach solves
the challenging ambiguity and severe occlusions under complex
human-object interactions, achieving robust volumetric perfor-
mance reconstruction, which outperforms the baselines favorably
without using any pre-scanned templates.

Combining data-driven cues for robust volumetric reconstruc-
tion under challenging human-object interactions is non-trivial, let
alone maintaining the lightweight property and fast running per-
formance under the monocular setting. To encode the interaction
pattern and alleviate the occlusion ambiguity, our key idea is to uti-
lize the data-driven interaction cues for human motions prior under
occlusions, as well as the rich visual cues including scene semantic
segmentation, body part parsing estimation, implicit occupancy
learning, and human pose and shape detection. More speciﬁcally,
we ﬁrst embrace the scene semantic cue for scene decoupling
to model the challenging occlusions explicitly for human-object
interactions. To prevent the disentanglement uncertainty, we reﬁne
the human-object segmentation through robust object tracking in
an iterative manner, which utilizes previous reconstruction results
for temporal consistency. We also adopt a human initialization
in the ﬁrst frame similar to RobustFusion(Conf.), which utilizes
the human parsing and implicit occupancy learning to generate a
complete and ﬁne-detailed initial model and non-rigid motion for
the human. Such initialization eliminates the tedious self-scanning
constraint for more robust human-object performance capture.
Then, we propose a robust human performance capture scheme
with the aid of various data-driven cues. Besides the original
strategy with the human pose, shape, and parsing priors to enable
re-initialization ability similar to RobustFusion(Conf.), we further
model the interaction patterns for the challenging human-object
occlusions in a data-driven manner. To this end, we introduce
a novel spatial relation prior to prevent physically implausible
intersections, as well as the interaction poses prior based on Gauss
Mixture Model (GMM) and the temporal interaction prior based
on LSTM predictor to maintain natural motions, especially for
those regions under severe occlusions. Finally, we adopt an adap-
tive fusion scheme to obtain temporally coherent reconstruction
results. With both the human-object occlusion analysis and human
parsing cue, the fusion weights are adaptively adjusted to avoid
deteriorated fusion caused by tracking errors and occlusions. To
summarize, our main contributions include:

• We propose a robust volumetric performance reconstruc-
tion approach for challenging human-object interaction
scenarios using only a single RGBD camera, which em-

2

braces data-driven visual and interaction cues to achieve
signiﬁcant superiority to existing state-of-the-art methods.
• We introduce a novel scene decoupling scheme under the
volumetric capture framework for explicit disentanglement
of human-object interactions, with the aid of robust object
tracking and semantic reﬁnement.

• We propose a novel and robust human-object performance
capture scheme with various data-driven interaction cues,
which can handle challenging human motions with com-
plex interaction patterns and severe occlusions.

2 RELATED WORK

This section presents an overview of research works related to
the proposed RobustFusion system. We roughly divide the related
methods into human volumetric capture, object-related reconstruc-
tion, and data-driven human and scene visual cues.
Human Volumetric Capture. In recent years, free-form dynamic
reconstruction methods combine the volumetric fusion [32] and
the non-rigid tracking based on embedded deformation [33]. The
multi-view solutions [7], [8] rely on complex studio and are
difﬁcult to be deployed for daily usage. Although [10] achieves
an unstructured multi-view setup but still not as convenient as the
single-view setting. In contrast, [11] utilizes only one common
single RGBD camera and achieves real-time dynamic reconstruc-
tion. Then, the solution [12] adds the SIFT features to improve
the accuracy of motion reconstruction, while Guo et al. [34]
utilize shading information to improve the non-rigid registration
and estimate the surface appearance, and Slavcheva et al. [35],
[36] add more constraints to support topology changes. Yu et
al. [15], [16], [37] further take human articulated skeleton prior
into account to increase tracking robustness, while HybridFu-
sion [38] utilizes extra IMU sensors for more reliable motion
tracking and Xu et al. [39] model the mutual gains between cap-
ture view selection and reconstruction. Besides, POSEFusion [17]
combines both implicit inference network and temporal volumetric
fusion in a keyframe selection scheme and can capture more
dynamic details in invisible regions, and some methods [40],
[41] combine the neural rendering techniques. Above methods
still suffer from careful and orchestrated motions, especially for
a tedious self-scanning process where the performer needs to turn
around carefully to obtain complete reconstruction, and Robust-
Fusion(Conf.) [18] liberates this constraint by introducing implicit
occupancy method [21]. However, these methods either cannot
handle modeling human-object interactions (e.g., [10], [15], [16],
[17], [18], [38]) or cannot robustly handle the fast human non-
rigid motions (e.g., [2], [13]). Comparably, our approach is more
robust for capturing challenging motions under human-object
interaction scenarios with the re-initialization ability and enables
the simultaneous reconstruction of both human and object without
the self-scanning constraint.
Object-related Reconstruction. As for object reconstruction, the
method [42] utilizes structure-from-motion to recover complete
3D models of articulated objects and analyses its joint movement
from RGB images. [43] introduces color information to point
cloud registration, which makes the tracking and reconstruction
of rigid objects more robust from RGBD data. [44] uses an
object-aware guidance approach for autonomous scene scanning
and reconstruction. Apart from rigid objects, the reconstruction of
non-rigid objects explored in work [11] mentioned above tracks
the dynamically growing node points and then fuses to a canonical

3

Fig. 2. The pipeline of RobustFusion. Assuming monocular RGBD input, our approach consists of a human initialization stage only at the triggered
frame (Sec. 4.2), a scene decoupling stage that includes mask reﬁnement and object tracking (Sec. 4.3), a robust human tracking stage(Sec. 4.4)
and volumertic fusion stage (Sec. 4.5) to generate live 4D results.

model. Besides, [14] reconstruct dynamic objects and static indoor
environment at the same time. Note that the dynamic motions that
[11] and [14] capture are very limited. In addition to the traditional
reconstruction methods for objects, recovering the 3D shape of an
object from single or multiple RGB images using deep neural
networks has attracted increasing attention in the past few years.
[45] proposes a learned continuous Signed Distance Function
representation of the object. [46] encodes both the geometry and
appearance of the objects and represents objects as continuous
functions that map world coordinates to a feature representation
of local properties. Besides, [47] uses a well-designed encoder-
decoder to generate a coarse 3D volume from each input image,
and then a multi-scale context-aware fusion module is introduced
to adaptively select high-quality reconstructions for different parts
from all coarse 3D volumes to obtain a fused 3D volume, in
which the network is also widely used to handle human-object
interaction. Moreover, [48], [49] propose a 4D human-object
interaction model to detect human-object geometric relation and
the interaction events. Recently, the work [26] learns the spatial
arrangements of humans and objects with pose estimation in a 3D
scene from a single RGB image. The methods [24], [27], [29] try
to generate the plausible human model(s) in existing 3D scenes,
and the work [28] utilizes wearable sensors to estimate the human
pose and location in a 3D scene. However, they are limited to the
naked human body or the pre-scanned models. For tiny objects, the
work [31] also provides a dataset of whole-body human grasping
of objects, and the method [50] proposes an expressive represen-
tation for human grasp modeling. By investigating and exploiting
these object-related reconstruction methods, we propose our object
tracking and reconstruction scheme in the meantime of our robust
human volumetric capture.
Data-driven Human and Scene Visual Cues. For data-driven
human modeling problems, early human modeling methods [51],
[52] take advantage of data-driven machine learning strategies to
convert the capture problem into regression or pose classiﬁcation
problems. Recently, data-driven techniques have attracted more
and more interest due to the rise of deep learning and RGB-based
human modeling approaches bloom. First, the methods [19], [53],
[54] estimate human 2D or 3D skeletal pose, and human para-
metric models [55], [56] with human pose and shape parameters
provide a good sparse representation for human models, based
on which some recent work [20], [25], [57], [58], [59] also learn

the human shape. Note that the use of Gauss Mixture Model from
[59] provides us an idea of utilizing a data-driven pose distribution
cue for human-object interaction scenarios. Besides, LSTM-based
temporal pose prediction is also taken into account. Second, there
are many approaches that directly estimate the human geometry
from RGB images, such as the parametric representation [60],
[61], implicit representation [21], [62], [63], [64] and volume
representation [22], [65]. However, such predicted geometry lacks
ﬁne details, which is important for immersive human modeling.
Even if
[62] achieves ﬁner geometry details, its learned pose
is also as inaccurate as the above methods. Besides, some per-
formance capture methods [66], [67], [68], [69] based on RGB
videos leverage the above learnable pose detection [19], [54] or its
own pose regression network to improve the accuracy of human
motion capture, but these methods have to rely on pre-scanned
template models. As for scene visual cues, scene segmentation
methods [70], [71] fetch the semantic information of the whole
scene including the people and objects in it, and human parsing
methods [23], [72], [73] also propose to fetch the semantic
information of the human model. These data-driven methods
yield colossal potential for human performance reconstruction. We
explore building a robust human and object volumetric capture
algorithm on top of these priors and then achieve signiﬁcant
superiority to previous methods.

3 OVERVIEW

RobustFusion achieves both human and objects volumetric capture
under a uniﬁed framework in a model-speciﬁc way, which can
perform human-object interaction reconstructions and maintain
robust ability to handle challenging human motions. As illustrated
in Fig. 2, our approach takes an RGBD video from Kinect v2 (or
Kinect Azure) as input and generates 4D meshes, achieving more
robust results than previous methods considerably. In our volumet-
ric capture framework, we utilize TSDF [32] volume for geometry
reconstruction, just like in [10], [16]. A brief introduction of our
technical components is provided as follows:
Human initialization. First, for model
initialization, we fol-
low [18] to generate a high-quality watertight human model with
ﬁne geometric details at the beginning, in which we combine
the implicit occupancy regression network with the traditional
non-rigid fusion pipeline using only the front-view RGBD input.

Second, we further utilize the complete model to initialize both the
human motions and the visual priors before the tracking stage. We
adopt a hybrid motion representation, including the newly sampled
ED-graph and embedded SMPL. Besides, various human pose and
parsing priors based on the front-view input are associated with the
initialized model.
Scene Decoupling. To reconstruct the dynamic scene, we ﬁrst
apply a semantic segmentation network to obtain the foreground
masks, including both human and labeled objects. Note that human
masks can also be obtained from the Kinect SDK, and as for the
unlabeled objects in the network, we extract its masks by utilizing
background separation. The segmented masks are too coarse to
be applied for tracking. Thus, with the help of the reconstructed
results, we can project 3D models to current 2D image and use a
iterative strategy to reﬁne the masks.
Object Tracking. After scene decoupling, we track the rigid
motions of the objects by solving an optimization problem under
the Iterative Closest Point (ICP) framework by taking account of
color consistency, geometry consistency, and spatial relationship
between the human and objects. The correct decoupling results
provided by mask reﬁnement enable accurate object tracking, and
the correct tracked object models provide a good reference for
scene decoupling in turn.
Robust Human Tracking. The core of our pipeline is to solve
the hybrid motion parameters from the canonical frame to the
current camera view. We propose a robust human tracking scheme
which utilizes reliable interaction and visual data-driven priors to
optimize both the skeletal and surface motions in an iterative ﬂip-
ﬂop manner. Observed that human poses have particular patterns
in the interaction with objects, we train a GMM model and LSTM
predictor to exploit the spatial and temporal prior information in
the optimization. Moreover, our scheme can handle challenging
motions with the re-initialization ability.
Object-aware Reconstruction. We fuse the masked depth stream
into the canonical TSDF volume after motion tracing to provide
temporal-coherent results for the human and objects separately.
The human model is fused based on the non-rigid motion ﬁeld,
and the object model is fused based on the estimated rigid transfor-
mation. Based on various visual priors and object-aware occlusion
ratios, we adaptively adjust the fusion weight to avoid deteriorated
fusion caused by tracking errors and occlusions. Finally, dynamic
atlas [10] and per-vertex color fusion are adopted to obtain 4D
textured reconstruction results.

4 TECHNICAL DETAILS
4.1 Problem Representation

Motion tracking is a core problem in our performance capture
system. To robustly estimate both human and object motions,
we decouple and track them separately with the data-driven cues.
This subsection brieﬂy overviews these motion representations and
deﬁnes the mathematical notations in our tracking framework.

The motion of rigid objects is formulated by the rigid trans-
formations T = {Ti, i ∈ N } in SE(3) space, where N is
the number of objects. As for human motions, we adopt the
efﬁcient and robust double-layer surface representation for motion
representation [16], which combines the embedded deformation
(ED) and the linear human model SMPL [56]. Since we can get
a complete human model after model initialization (Sec.4.2), we
modify the SMPL-sampled ED-graph by the ED-graph sampled on
the complete model. We utilize SMPL to represent our skeleton

4

motions. SMPL is a linear body model with N = 6890 vertices
and K = 24 joints. Before posing, the body model ¯T deforms into
the morphed model T (β, θ) with the shape parameters β and pose
parameters θ as T (β, θ) = ¯T + Bs(β) + Bp(θ), where Bs(β)
and Bp(θ) are the shape blendshapes and pose blendshapes
respectively. T (¯v; β, θ) denotes the morphed 3D position for
any vertex ¯v ∈ ¯T. The posed SMPL is further formulated as
the blend skinning function: W (T (β, θ), J(β), θ, W), in terms
of the body T (β, θ), pose parameters θ, joint locations J(β)
and the skinning weights W. Speciﬁcally, for any 3D vertex
vc, the linear blend skinning (LBS) operation with the SMPL
skeleton motions is formulated as ˆvc = G(vc, θ)vc, where
G(vc, θ) = (cid:80)
wi,vc Gi is the posed rigid transformation of
i∈B
vc, B is index set of bones, Gi = (cid:81)
ˆξk) is the rigid
k∈Ki
transformation of i-th bone referencing the parent bones whose
ˆξk) is the
indices are Ki in the backward kinematic chain, exp(θk
exponential map of the twist associated with k-th bone, and wi,vc
is the skinning weight associated with i-th bone and point vc. If
vc is on SMPL model, wi,vc is pre-deﬁned in W. If vc is on
the fused surface, wi,vc is given by the weighted average of its
knn-nodes.

exp(θk

Non-rigid motions of the human is represented by a embed-
ded deformation node-graph G = {dqj, xj}, consisting of the
dual quaternions {dqj} and the corresponding ED nodes {xj}.
SE3(dqj) denotes the rigid transformation in SE(3) space.
Then for any 3D vertex vc in the canonical volume, the ED
warping operation is formulated as follows:

˜vc = ED(vc; G) = SE3(

(cid:88)

i∈N (vc)

w(xi, vc)dqi)vc,

(1)

2/(2r2

where N (vc) is a set of node neighbors of vc, and w(xi, vc) =
exp(−(cid:107)vc − xi(cid:107)2
k)) is the inﬂuence weight of the i-th node
xi to vc. The inﬂuence radius rk is set as 0.075m for all the ED
nodes. Similarly, ˜nvc = ED(nvc; G) denotes the warped normal
of vc using the ED motion ﬁeld G.

4.2 Human Initialization

Due to complex non-rigid human motions, the good initial human
model and motion is critical for us to worry-free focus on human-
object interactions. Fortunately, [18] provides us an available
robust human performance capture baseline as initialization.
Model initialization. To eliminate the orchestrated self-scanning
constraint and the consequent fragile tracking of monocular cap-
ture, we propose a model initialization scheme using only the
front-view RGBD input. As illustrated in Fig. 3, to generate
high-ﬁdelity geometry details, we ﬁrst utilize the traditional ED-
based non-rigid fusion method [11], [39] to fuse the depth stream
into live partial TSDF volume. Once the average accumulated
TSDF weight in the front-view voxels reaches a threshold (32
in our setting), a modiﬁed RGBD-based PIFu [21] network [18] is
triggered to generate a watertight mesh. Then, to align the partial
TSDF and the complete mesh, we jointly optimize the unique
human shape β0 and pose θ0, as well as the ED motion ﬁeld G0
from the TSDF volume to the complete mesh as follows:

Ecomp(G0, β0, θ0) =λvdEvdata + λmdEmdata + λbindEbind
+ λpriorEprior.

(2)

5

Fig. 3. Human model and motion initialization pipeline. Assuming the front-view RGBD input, both a partial TSDF volume and a complete mesh
are generated, followed by the alignment and blending operations to obtain a complete human model with ﬁne geometry details, based on which
motion is initialized by re-sampling the ED-node-graph and semantic binding.

The volumetric data term Evdata measures the misalignment error
between the SMPL and the reconstructed geometry in the partial
TSDF volume:

Evdata(β0, θ0) = (cid:80)
¯v∈¯T

ψ(D(W (T (¯v; β0, θ0); β0, θ0)),

(3)

where D(·) takes a point in the canonical volume and returns
the bilinear interpolated TSDF, and ψ(·) is the robust Geman-
McClure penalty function. The mutual data term Emdata further
measures the ﬁtting from both the TSDF volume and the SMPL
model to the complete mesh, which is formulated as the sum of
point-to-plane distances:

Emdata = (cid:80)
(¯v,u)∈C
(cid:80)

ψ(nT

u (W (T (¯v; β0, θ0)) − u))+

ψ(nT

u (˜vc − u)),

(4)

(˜vc,u)∈P

where C and P are the correspondence pair sets found via closest
searching; u is a corresponding 3D vertex on the complete mesh
and nu is its normal. Note that the pose prior term Eprior from
[59] penalizes the unnatural poses while the binding term Ebind
from [16] constrains both the non-rigid and skeletal motions to be
consistent. We solve the resulting energy Ecomp under the ICP
framework, where the non-linear least-squares problem is solved
using Levenberg-Marquardt (LM) method with a custom-designed
Preconditioned Conjugate Gradient (PCG) solver on GPU [7],
[34]. Finally, to seamlessly blend both the partial volume and
the complete mesh in the TSDF domain, we update the voxel
as follows:

D(v) ←

D(v)W(v) + d(v)w(v)
W(v) + w(v)

,

W(v) ← min(W(v) + w(v), wmax),

where D(v) and W(v) denote its TSDF value and accumulated
weight, respectively, and wmax is set as 32 to prevent over-
smoothness of geometry during volumetric fusion in Sec. 4.5 and
the corresponding projective SDF value d(v) and the updating
weight w(v) are as follows:

d(v) = (u − ˜v)sgn(nT

u (u − ˜v)), w(v) = 1/(1 + N(v)),

(6)

Here, For any 3D voxel v, ˜v denotes its warped position after
applying the ED motion ﬁeld; N(v) denotes the number of non-
empty neighboring voxels of v in the partial volume which
indicates the reliability of the fused geometry, and sgn(·) is the
sign function to distinguish positive and negative SDF.
Motion Initialization. The complete model after model initializa-
tion provides a reliable initialization for both the human motion
and the utilized visual priors. As described in Sec. 4.1, before
the tracking stage, we ﬁrst re-sample the sparse ED nodes {xi}
on the mesh to form a non-rigid motion ﬁeld, denoted as G, and
then we rig the mesh with the pose θ0 from its embedded SMPL
model in model initialization and transfer the SMPL skinning
weights to the ED nodes {xi}. For any 3D point vc in the
capture volume, let ˜vc and ˆvc denote the warped positions after
the embedded deformation and skeletal motion, respectively. Note
that the skinning weights of vc for the skeletal motion are given
by the weighted average of the skinning weights of its knn-nodes.
To initialize the pose prior, we apply OpenPose [19] on the RGBD
image to obtain the 2D and lifted 3D joint positions, denoted as
P2D
, respectively, with a detection conﬁdence Cl.Then,
l
we ﬁnd the closest vertex from the watertight mesh to P3D
,
denoted as Jl, which is the associated marker position for the
l-th joint. To utilize the semantic visual prior, we apply the light-
weight human parsing method [23] to the triggered RGB image to
obtain a human parsing image L. Then, we project each ED node
xi into L to obtain its initial semantic label li. After the motion
initialization, inspired by [66], [67], we propose to optimize the
motion parameters G and θ in an iterative ﬂip-ﬂop manner to
fully utilize the rich motion prior information of the visual cues to
capture challenging motions.

and P3D

l

l

(5)

4.3 Scene Decoupling and Object Tracking

Accurate scene decoupling is the premise of robust motion capture
that makes full use of the object-speciﬁc priors. Otherwise, the
wrong segmentation reduces tracking accuracy, and segmentation
noise will be fused in the models. However, the semantic segmen-
tation network unavoidably has noise in human-object junction
and occlusion, which can not be handled only by the input data.
Therefore, we take advantage of our reconstructed human and ob-
ject models to iteratively reﬁne the segmentation masks to prevent

6

o = Mo + M p

the projected object and human mask based on the reconstructed
models. Due to temporal continuity, the current object mask is
similar to the previous frame. The current object mask for tracking
is reﬁned as M r
o . Then we remove the human
occlusion by comparing the depth (Algorithm 1 Line.5), where
function dep(·) returns the depth value for the mask. Based on the
reﬁned object mask M r
o , the transformation between current frame
and previous frame To is solved by optimization ICP (·) and the
projected object mask M p
o is updated. Then we get the reﬁned
human mask M r
h by comparing the depth (Algorithm 1 Line.7).
Function track(·) returns the update human model based on the
reﬁned mask M r
h. Then the projected human mask is updated by
the tracked human model (Algorithm 1 Line.8). Moreover, we also
utilize an iteration framework to raise the reﬁnement accuracy. We
demonstrate the results of the mask reﬁnement pipeline in Fig. 4.
With the mask reﬁnement, we successfully obtain the correct
masks.
Object Tracking. To robustly track the objects, we optimize the
rigid motions (T = {Ti}, i ∈ N ) of the corresponding object
point clouds under ICP iteration framework as follows:

Eobject(T) = λcolorEcolor + λgeoEgeo + λsp oEsp o.

(7)

The color term Ecolor is achieved by the colored point cloud reg-
istration [43], which encourages the color consistency as follows:

Ecolor =

(cid:88)

(cid:88)

(Cp(f (Tiq)) − C(q))2,

(8)

i∈N

(p,q)∈R

where N is the number of objects, R is the correspondence pair
sets found via closest searching and p, q are the closest points of
frame t and frame t − 1. Function C(·) returns color of the point
q while Cp(·) is a pre-computed function continuously deﬁned
on the tangent plane of p and f (·) is the projection function that
projects a 3D point to the tangent plane. The geometry term Egeo
encourages the geometry consistency as follows:
(cid:88)

(cid:88)

(nT

p (p − Tiq))2,

(9)

Egeo =

(a)

(b)

(c)

(d)

(e)

Fig. 4. The results of the proposed mask reﬁnement. (a,e) are the
segmentation mask before and after reﬁnement. We sum the original
object mask (top of (b)) and projection mask of the previous frame
(bottom of (b)) to get the combined mask (top of (c)), then subtract the
projection of the human body and remove noise based on point cloud
continuity to get a denoised mask (bottom of (c)). Based on the denoised
mask, we use Colored-ICP to get the transformation (top of (d)) then use
the estimated transformation to get the updated projection mask (bottom
of (d)). The detailed iteration procedure is explained in Algorithm. 1.

o , M r
h
o = π(Ro)
h = Mh

Algorithm 1 Mask reﬁnement
Input: Mo,Mh
Output: M r
1: M p
2: M p
3: for i = 0, i < 3, i + + do
4: M r
5: M r
6:
7: M r
8: M p
9: return M r

o − (dep(M p
o ), M p
h = Mh − (dep(M p
h = π(track(M r
h))

o = Mo + M p
o
o = M r
To = ICP (M r

o , M r
h

o ))

h ) < dep(M r
o = π(To ∗ Ro)
o ) < dep(Mh))

a

semantic

disentanglement uncertainty and maintain temporal consistency.
As illustrated in Fig. 2, the proposed mask reﬁnement based on
initial semantic segmentation provides accurate segmentation of
both human and object. Then we can decouple the dynamic scene
between rigid object motions and non-rigid human motions to
track and reconstruct them. In this subsection, we summarize the
mask reﬁnement and the object tracking as follows.
Mask Reﬁnement. We utilize
segmentation
method [70] and the human segmentation from Kinect SDK to get
the human and object masks. For operation efﬁciency, we execute
the segmentation network every ﬁve frames. At the same time,
Kinect SDK provides the human mask for the entire sequence,
and the object masks are provided by background separation for
the remaining frames. With the human and object masks, we use
the masked point cloud for motion tracking and reconstruction.
However, wrong segmentation often occurs in the place where
people and objects connect. Directly using the coarse segmentation
mask leads the unstable tracking and erroneous reconstruction.
Therefore, we propose a reﬁnement strategy to obtain the accurate
object and human masks, illustrated in Algorithm 1. Given the
coarse object mask Mo and human mask Mh provided by the
segmentation network and Kinect SDK, we aim to get the reﬁned
object mask M r
h. Here, Ro is the recon-
structed object model of the previous frame. Function π(·) projects
the reconstructed model to get the projected mask. M p
h is

o and human mask M r

o and M p

i∈N

(p,q)∈R

where np is the normal of the point p. To generate a healthy
spatial relation without implausible interpenetration between hu-
man and objects, we introduce an interpenetration term Esp o as
follows:

Esp o =

(cid:88)

(cid:88)

ψ(τ D(Tip))+

i∈N

p∈Oi
(cid:88)

(cid:88)

i,j(i(cid:54)=j)∈N

p∈Oi

ψ(τ Doj(Tip)).

(10)

where Oi is the i-th object, p is point of Oi, D(·) is the same
as in Eqn. 3, Doj(·) takes a point in the live TSDF volume of
j-th object and returns the bilinear interpolated TSDF value, and
ψ(·) is the robust Geman-McClure penalty function, and τ is the
indicator function which equals to 1 only if the visited TSDF value
is positive (inside the corresponding volume).

4.4 Robust Human Tracking

As illustrated in Fig. 5, we propose a novel performance cap-
ture scheme to track challenging human motions under complex
human-object interaction scenarios robustly, in which we intro-
duce a spatial relation prior to prevent implausible interactions,
data-driven interaction cues to maintain natural motions, espe-
cially for those regions under severe human-object occlusions, as

7

Fig. 5. The pipeline of our robust performance capture scheme. Assuming the masked RGBD input, We track the object based on the space relation
cue. Then, both skeletal and non-rigid motions are optimized with the associated human-object interaction and data-driven visual cues. Finally, an
object-aware adaptive volumetric fusion scheme is adopted to generated 4D models.

well as the human pose, shape and parsing priors to enable re-
initialization ability. We ﬁrst optimize the motion ﬁelds described
in Sec. 4.1 including both the skeletal pose and surface-sampled
ED node-graph in a ﬂip-ﬂop iteration manner.
Skeleton Tracking. During each ICP iteration, we ﬁrst optimize
the skeletal pose θ of the human model, which is formulated as
follows:

Esmot(θ) =λsdEsdata + λposeEpose + λpriorEprior+
λtempEtemp + λinterEinter.

(11)

Here, since human motions have particular patterns in the
interactions with objects, we introduce a human-object interaction
term Einter to Eqn. 11, which includes the spacial relation prior,
data-driven interaction pose prior, and motion prediction prior to
keep natural motions and alleviate the impacts of severe object-
human occlusions, formulated as follows:

Einter =λgmmEgmm + λlstmElstm + λsp h1Esp h,

(12)

where Egmm, Elstm and Esp h are energies of interaction pose
prior term, motion prediction prior term and interpenetration term
respectively. The interaction pose and motion prior terms come
from the experiential data-driven cues, representing the single-
frame and temporal priors. At the same time, the interpenetration
term represents the spatial prior of human-object interaction. The
interaction pose prior term resembles the prior term Eprior from
[59]. It is based on a Gauss Mixture Model (16 Gaussians) ﬁtted
to approximately 200000 human-object interaction temporal poses
and formulated as follows:

Egmm = −log((cid:80)
j

wjN (θ; µj, δj)),

(13)

where wj, µj and δj are the mixture weight, the mean, and the
variance of j-th Gaussian model, respectively. Moreover, we train
an LSTM predictor to predict the current pose in terms of the poses
of the previous nine frames and formulate the motion prediction
prior term Elstm as follows:

Elstm = ψ(θ − L(θt−9, θt−8, ..., θt−1)),

(14)

where ψ(·) is the robust Geman-McClure penalty function;
θi, (i = t − 9, t − 8, ..., t − 1) are the skeleton poses of
the previous 9 frames; L(·) is the LSTM prediction fuction.
The interpenetration term Esp h prevents unphysical intersections
from human to objects in space dimension:

Esp h =

(cid:88)

v∈T

ψ(τ Do(W (T (v; β, θ); θ)),

(15)

where Do(·) takes a point in the live TSDF volume of the object
and returns the bilinear interpolated TSDF value, and ψ(·) is
the robust Geman-McClure penalty function, τ is the indicator
function which equals to 1 only if the TSDF value for the object
of the vertex v on SMPL is positive (inside the object volume).

Besides, we also introduce the pose term Epose in [18] and
update its pose detectors to better encourage the skeleton to
match the detections obtained by CNN from the RGB image,
including the 2D position P2D
and the
pose parameters θd from OpenPose [19] and EFT [58] (HMR [20]
in [18]):

, lifted 3D position P3D

l

l

Epose =ψ(ΦT (θ − θd)) +

NJ(cid:88)

l=1

(cid:107)ˆJl − P3D

l (cid:107)2

2),

φ(l)((cid:107)π(ˆJl) − P2D

l (cid:107)2

2+

(16)

where ψ(·) is the robust Geman-McClure penalty function; ˆJl
is the warped associated 3D position and π(·) is the projection
operator. The indicator φ(l) equals to 1 if the conﬁdence Cl for the
l-th joint is larger than 0.5, while Φ is the vectorized representation
of {φ(l)}.

Finally, among the other terms, Esdata measures the point-
to-plane misalignment error between the warped geometry in the
TSDF volume and the depth input:

Esdata =

(cid:88)

ψ(nT

u (ˆvc − u)),

(17)

(vc,u)∈P

where P is the corresponding set found via a projective searching;
u is a sampled point on the depth map while vc is the closet
vertex on the fused surface; the temporal term Etemp encourages

coherent deformations by constraining the skeletal motion to be
consistent with the previous ED motion:

Etemp =

(cid:88)

(cid:107)ˆxi − ˜xi(cid:107)2
2,

(18)

xi
where ˜xi is the warped ED node using non-rigid motion from
previous iteration; and the prior term Eprior from [59] penalizes
the unnatural poses.
Surface Tracking. To capture realistic non-rigid deformation
deﬁned by ED-node graph G, on top of the skeleton tracking
result, we solve the surface tracking energy as follows:

Eemot(G) =λedEedata + λsp h2Esp h + λregEreg+

λtempEtemp.

(19)

Here the dense data term Eedata jointly measures the dense
point-to-plane misalignment and the sparse landmark-based pro-
jected error:

Eedata =

(cid:88)

ψ(nT

u (˜vc − u)) +

(vc,u)∈P

NJ(cid:88)

l=1

φ(l)(cid:107)π(˜Jl) − P2D

l (cid:107)2
2,

(20)
where ˜Jl is the warped associated 3D joint of the l-th joint in the
fused surface. The interpenetration term Esp h2 is as follows:
(cid:88)

Esp h2 =

ψ(τ Do(˜v)),

(21)

v∈T
note that the interpenetration term is associated with a smaller
weight : sp h2 = sp h1/10. The regularity term Ereg from [16]
produces locally as-rigid-as-possible (ARAP) motions to prevent
over-ﬁtting to depth inputs. Besides, the ˆxi after the skeletal
motion in the temporal term Etemp as formulated above is ﬁxed
during current optimization.

Both the pose and non-rigid optimizations in Eqn. 11 and
Eqn. 19 are solved using LM method with the same PCG solver
on GPU [7], [34]. Once the conﬁdence Cl reaches 0.9 and the
projective error (cid:107)π(˜Jl) − P2D
l (cid:107)2
2 is larger than 5.0 for the l-th
joint, the associated 3D position Jl on the fused surface is updated
via the same closest searching strategy of the initialization stage.
When there is no human detected in the image, our whole pipeline
will be suspended until the number of detected joints reaches a
threshold (10 in our setting).

4.5 Object-aware Reconstruction
After the above optimization, we separately fuse the masked
depth into the respective canonical TSDF volume of the human
and objects with occlusion analysis and human semantic cue to
temporally update the geometric details. Note that each voxel in
canonical space is updated using Eqn. 5, while updating weight
w(v) is different between human and objects.

For human reconstruction, we ﬁrst discard the voxels which
are collided or warped into invalid input. Then, to avoid deterio-
rated fusion caused by challenging motion, an effective adaptive
fusion strategy as shown in Fig. 5 is proposed to model semantic
motion tracking behavior. To this end, we apply the human parsing
method [23] to the current RGB image to obtain a human parsing
image L. For each ED node xi, recall that li is its associated
semantic label during initialization while L(π(˜xi) is current cor-
responding projected label. Then, for any voxel v, we formulate
its updating weight w(v) as follows:
−(cid:107)ΦT (θ∗ − θd)(cid:107)2
2
2π

ϕ(li, L(π(˜xi)))
|N (vc)|

w(v) = exp(

(cid:88)

)

,

i∈N (vc)

(22)

8

where θ∗ is the optimized pose; N (vc) is the collection of the
knn-nodes of v; ϕ(·, ·) denote an indicator which equals to 1
only if the two input labels are the same. Note that such a robust
weighting strategy measures the tracking performance based on
the human pose and semantic priors. Then, w(v) is set to be zero
if it is less than a truncated threshold (0.2 in our setting), to control
the minimal integration and further avoid deteriorated fusion of
severe tracking failures. Mocc = (dep(M p
o )) is the
mask of human occluded by object. Function S(·) returns the pixel
number of mask. When the severe object-human occlusion occurs
as S(Mocc)/S(M p
h ) is bigger than a threshold (0.3 in our setting),
we also set w(v) to zero in case deteriorated fusion caused by false
segmentation results and tracking errors caused by occlusions.

h ) > dep(M p

As for object reconstruction, Rin is the root mean square error
(RMSE) of all inlier correspondences in the ICP framework. TSDF
fusion is performed every ﬁve frames based on Eqn. 5 only if
Rin is less than a certain value (0.003 in our setting), in which
the updating weight w(v) is formulated as w(v) = 0.0048
Rin+0.0024 .
Again, similar to human volumetric fusion, once the human
occludes an object, we stop the TSDF fusion for the whole object.
Finally, the dynamic atlas scheme [10] and per-vertex color fusion
are adopted to obtain 4D textured reconstruction results for human
and objects, respectively.

5 EXPERIMENTAL RESULTS
In this section, we ﬁrst report the performance and the main pa-
rameters of our RobustFusion system. Then we compare with the
state-of-the-art methods and evaluate the technical contributions of
our RobustFusion both qualitatively and quantitatively on a variety
of challenging scenarios in Sec. 5.2 and Sec. 5.3, respectively.
Fig. 6 demonstrates the results of RobustFusion, where both
the challenging motions with human-object interactions and the
ﬁne geometry and texture details are faithfully captured. Our
approach can even faithfully reconstruct the interaction scenarios
with multiple performers and various objects (see the last row of
Fig. 6). Please also kindly refer to the supplemental video for the
sequential 4D reconstruction results.

5.1 Performance

We run our experiments on a PC with an NVIDIA GeForce GTX
TITAN Xp GPU and an Intel Core i7-7700K CPU. Our human
initialization takes 15 s, and the following robust performance
capture pipeline runs at an average of 135 ms per frame, where
the visual priors collecting takes 97 ms,
the robust human-
object tracking takes around 21 ms with 4 ICP iteration and
17 ms on average for all the remaining computations. Note that
the semantic segmentation network and volumetric fusion for
objects are executed every ﬁve frames. In all experiments, we
use the following empirically determined parameters: λvd = 1.0,
λmd = 2.0, λbind = 1.0, λprior = 0.01, λcolor = 0.1,
λgeo = 0.9, λsp o = 1.0, λsd = 4.0, λpose = 2.0, λtemp = 1.0,
λinter = 1.0, λgmm = 0.02, λlstm = 0.1, λsp h1 = 2.0,
λsp h2 = 0.2, λed = 4.0 and λreg = 5.0. For the ED model, we
use the four nearest node neighbors for ED warping and the eight
nearest node neighbors to construct the ED graph. For the TSDF
voxel, the size is set as 4 mm in each dimension.

5.2 Comparison

For throughout comparison, we compare our RobustFusion against
the state-of-the-art methods in this subsection, including Double-

9

Fig. 6. 4D human and object reconstructed results of the proposed RobustFusion system, and the interacted objects include a sofa, a cart, two
cartons, a piece of luggage, a chair, and a toy.

10

(a)

(b)

(c)

(d)

(e)

Fig. 7. Qualitative comparison.(a) is the reference RGBD images. (b-d) are the geometry results of DoubleFusion [16], UnstructuredFusion [10] and
RobustFusion(Conf.) [18], respectively. (e) is the geometry/texture results of our Proposed RobustFusion.

TABLE 1
Average projective numerical errors (mm) of our captured sequences
for the concerned methods: DoubleFusion [16], UnstructuredFusion
[10], RobustFusion(Conf.) [18] and our methods, where the
corresponding sequences can refer to the supplementary video.

Human-object Interactions

with luggage & chair

with two cartons

dragging things

rotating a chair

with a luggage

[16]

29.66

16.56

9.11

17.45

18.14

with luggage & carton (1)

14.35

with luggage & carton (2)

18.48

with a cart (girl)

with cart & carton (1)

with cart & carton (2)

with a sofa

with a cart (boy)

with backpack & toy

17.75

17.83

12.93

11.64

23.14

34.34

[10]

28.34

11.38

8.56

15.10

13.24

10.57

13.82

12.47

13.27

7.91

7.66

18.96

32.12

[18]

22.32

8.37

6.45

10.34

9.42

8.51

10.72

9.49

10.34

5.48

7.11

15.55

28.34

Ours

17.74

7.61

4.96

8.61

7.26

7.87

8.50

8.87

8.80

5.05

6.60

15.04

23.37

Fusion [16], UnstructuredFusion [10], HybridFusion [38], Robust-
Fusion(Conf.) [18] and POSEFusion [17] both qualitatively and
quantitatively.
Qualitative Comparison. These state-of-the-art methods are re-
stricted to human reconstruction without modeling human-object
interactions, and UnstructuredFusion [10] is a multi-view method.
For a fair comparison of dynamic reconstruction at the scenes
with objects, we test the above state-of-the-art methods on the
same reﬁned segmentation results of the human in our setting
and modify UnstructuredFusion [10] into the monocular setting
by removing their online calibration stage.

The qualitative comparison of our approach against Double-

Fusion [16], single-view UnstructuredFusion [10] and RobustFu-
sion(Conf.) [18] is as shown in Fig. 7. Both DoubleFusion [16] and
UnstructuredFusion [10] suffer from the fast human motions and
the severe occlusions due to the human-object interactions. More-
over, without a complete model due to the lack of orchestrated
self-circling motions, they tend to integrate erroneous surfaces at
the newly fused region. With the aid of various visual priors, Ro-
bustFusion(Conf.) [18] is more robust to the fast motions but still
suffers from severe occlusions, leading to wrong tracking results
in the limb regions. In contrast, beneﬁt from our robust human
tracking scheme based on data-driven interaction and visual cues,
our approach achieves signiﬁcantly more robust tracking results,
especially for challenging occluded and fast motions. Besides, we
compare against the latest volumetric method POSEFusion [17]
in Fig. 8, which combines implicit inference network with a key-
frame selection strategy to capture details in invisible regions. As
shown in Fig. 8, our approach can achieve more accurate human
tracking and visually pleasant reconstruction results with the
aid of human-object interaction cues. Nevertheless, our approach
can faithfully reconstruct both the humans and objects in the
interaction scenarios, which is unseen in the previous monocular
fusion approaches.

Quantitative Comparison. For quantitative comparison, we ﬁrst
utilize the average projective numerical metric. Speciﬁcally, we
render the reconstructed result to a depth map in the camera view
and compute its MAE (Mean Absolute Error) by taking the depth
input as the reference only in the intersection between the rendered
surface and the human depth. Note that even without ground
truth reconstruction, this MAE metric encodes the reconstruction
error for the non-rigid motion capture process of each method,
providing a reliable quantitative comparison. We only compute
MAE in the human regions for a fair comparison since previous
methods cannot reconstruct objects. Tab. 1 demonstrates the MAE
of different sequences in our experiments, in which our method
leads to considerably less error, i.e., 10.02 mm average MAE,
compared with 18.57 mm of DoubleFusion [16], 14.88 mm of Un-

11

(a)

(b)

(c)

(d)

Fig. 9. Qualitative comparison. (a-d) are the geometry results of Dou-
bleFusion [16], UnstructuredFusion [10], POSEFusion [17] and our Pro-
posed RobustFusion, respectively. The color-coded maps in bottom row
indicate the projective errors.

(a)

(b)

(c)

(d)

Fig. 8. Qualitative comparison. The ﬁrst row is the reference RGB
images. The second to ﬁfth row are the geometry results of Double-
Fusion [16], UnstructuredFusion [10] and POSEFusion [17] and our
method, respectively.

structuredFusion [10] and 11.73 mm of RobustFusion(Conf.) [18].
Moreover, Fig. 9 demonstrates that our method achieves high-
quality reconstruction results with less accumulated artifacts, us-
ing the corresponding sequence “Human-object interactions with
a sofa” in Tab. 1. Note that our MAE for this sequence is 6.60 mm,
compared favorably with 17.24 mm for the reconstructed results
provided by POSEFusion [17]. These quantitative comparisons
above reveal the effectiveness of our method for more robust and
accurate human motion tracking and reconstruction.

To illustrate our robustness for human-speciﬁc motions, we
further compare against HybridFusion [38], which uses extra
body-worn IMU sensors. We utilize the challenging sequence with
ground truth from [38] and remove their orchestrated self-scanning
process for our methods. Even though this sequence does not
include human-object interaction scenarios, such an experiment
further illustrates that our approach with data-driven interaction
cues can handle challenging human-only motions. The quantitative
comparison in terms of the per-frame error in Fig. 10 (e) and the
average errors among the whole sequence in Tab.2 demonstrate
that both our approach and our preliminary version [18] achieve

(e)

Fig. 10. Quantitative comparison. (a-d) are the reconstruction ge-
ometry results of DoubleFusion [16], HybridFusion [38], RobustFu-
sion(Conf.) [18] and our method. (e) is the error curves.

a signiﬁcantly better result than DoubleFusion and even compa-
rable performance against HybridFusion. Note that HybridFusion
still relies on the self-scanning stage for sensor calibration and
suffers from missing geometry caused by the body-worn IMUs as
shown in Fig. 10 (a), while our approach eliminates such tedious
self-scanning and achieves complete and plausible reconstruction
results.

TABLE 2
Average errors on the entire sequence compared to the ground truth
observation from the Vicon system, for these three methods:
DoubleFusion [16], HybridFusion [38], RobustFusion(Conf.) [18] and
our method, respectively.

[16]

[38]

[18]

Ours

average error

0.1904 m

0.0417 m

0.0553 m

0.0546 m

5.3 Evaluation

In this subsection, we evaluate each technical contribution of
our RobustFusion separately. Speciﬁcally, we evaluate the human
initialization, mask reﬁnement, object
tracking, robust human
tracking, and object-aware adaptive fusion, respectively. More-
over, we also evaluate our extension capability by experiments
in multi-person and multi-camera scenarios.

(a)

(b)

(c)

Fig. 11. Evaluation of human initialization. (a) is the 3D human model in
two views. (b) is the ED node-graph that formulates the surface motions.
(c) is the following tracking results that overlay on reference color image
based on the corresponding 3D surface geometry and the ED node-
graph.The results from the ﬁrst row to the third are the results without
human initialization, with initialization only using skeleton optimization
and SMPL-based node-graph, and with our entire initialization process,
respectively.

Human Initialization. For completeness of evaluation, we ﬁrst
evaluate the human initialization scheme on a sequence without a
carefully designed self-scanning process organized as model com-
pletion and initialization in performance capture stages in [18]. As
shown in Fig. 11, without model initialization, only partial initial
geometry with SMPL-based ED node-graph leads to inferior track-
ing and erroneous reconstruction results. This exactly explains
that why DoubleFusion [16] and UnstructuredFusion [10] fail
without careful self-scanning process. To evaluate our alignment
during model initialization and demonstrate the superiority of
our modiﬁed motion representation over original representation in
previous methods [10], [16], the skeletal pose is optimized during
alignment optimization, and only SMPL-based double-layer ED-
graph is adopted for motion tracking, where the results are still
imperfect. In contrast, our approach with both model and motion

initialization successfully obtains a watertight and ﬁne-detailed
human mesh and enables more robust motion tracking.

12

(a)

(b)

(c)

(d)

(e)

Fig. 12. Evaluation of the mask reﬁnement. (a) Reference color images.
(b) The human masks without reﬁnement. (c) The reconstruction results
without reﬁnement. (d) The human masks with reﬁnement. (e) The
reconstruction results with reﬁnement.

Mask Reﬁnement and Object Tracking. Here we evaluate the
proposed mask reﬁnement in Fig. 12. Since the toy is unlabeled
in the network [70], we extract its masks by utilizing background
subtracting. Therefore, the segmentation of objects is also depen-
dent on human segmentation results. The original human segmen-
tation mask in Fig. 12 (b) is inaccurate, especially when human-
object interaction occurs, leading to misaligned object masks due
to the overlaying of the object on the performer. Then, directly
using such coarse segmentation results leads to unstable tracking
and erroneous object reconstructions as shown in Fig. 12 (c). In
contrast, with our mask reﬁnement scheme, the object is separated
from the human segmentation result correctly (Fig. 12 (d)). As
a result, our approach achieves more robust human and object
tracking results in Fig. 12 (e), which illustrates the effectiveness
of our layer-wise strategy and mask reﬁnement scheme.

Besides, Fig. 13 further demonstrates the robustness of our
mask reﬁnement for object mask segmentation and rigid tracking.
Although sofa/chair is labeled in the network [70], it occasionally
fails to extract the masks as shown in the second row of Fig. 13.
With the reﬁned masks and object tracking optimization in the
fourth row of Fig. 13, we can achieve more accurate object track-
ing. The corresponding quantitative comparison in Fig. 14 further
demonstrates that our method achieves the highest accuracy, where
the MAE for the entire object sequence is 35.10 mm and 11.11 mm
for our method w/o and with mask reﬁnement, respectively.
Robust Human Tracking. Our robust human tracking is based on
human-object spatial relation analysis using various data-driven
cues. Here, we evaluate them one by one. First, as shown in
Fig. 15, we evaluate our human-object spatial relation cue – the
interpenetration term for human motion optimization. Note that
we eliminate the interpenetration term by setting λsp h1 = 0 and
λsp h2 = 0 in Fig. 15 (b), where the human model erroneously
inserts into the cart. Differently, our full pipeline provides an es-
sential spatial constraint for human motions estimation, especially
in occlusion cases where no direct observation is available like
the leg in Fig. 15. Beneﬁt from our interpenetration term, we
successfully avoid the interpenetration between human and cart
models as demonstrated in Fig. 15 (c).

13

(a)

(b)

(c)

Fig. 15. Evaluation of the interpenetration term. (a) Reference color
images. (b) The results without interpenetration term. (c) The results
with interpenetration term.

these challenging cases. Besides, as shown in the second row of
Fig. 16, with the aid of pose detection and scene segmentation, the
system can screen the disappears and re-occur of the person with
reconstructing object and achieve the recovery from the failing
track.

Furthermore, the evaluation of empirical data-driven terms,
including an interaction pose prior and a temporal motion pre-
diction prior, is provided in Fig. 17. We eliminate the data-
driven interaction terms by setting λlstm = 0 and λgmm = 0.
Then, due to the severe occlusions between the performer and the
sofa, this variation generates unnatural motion estimation for the
occluded legs as shown in the different rendered views in Fig. 17
(b). With the aid of the empirical constraint, our approach can
generate more plausible and reasonable results, as shown in the
corresponding sub-ﬁgures (c) of Fig. 17. In this comparison, the
pose estimation of the unobserved region is up to the continuity of
observed joints in the kinematic chain, similar to the unconstrained
optimization that may lead to severe deviation from the real
situation. Differently, the human skeletal pose estimation with
an empirical constraint can be well deducted by the historical
experience, including both interaction pose distribution prior based
on Gauss Mixture Model and temporal motion prediction based on
LSTM.
Object-aware Adaptive Fusion. To evaluate our object-aware
adaptive fusion scheme based on the occlusion relation and
semantic errors, we compare our full volumetric fusion pipeline
and the method variation without the object-aware adaptive fusion
strategy. The comparison in Fig. 18 (a) demonstrates that our full
pipeline can effectively avoid the severe accumulated error for
those regions with high-speed motions, such as jumping over the
chair. Besides, as shown in the second row of Fig. 18 (b), the
occlusion by object leads to erroneous surface generation (e.g.,
when the performer pulls the cart and walks behind the object).
In contrast, the geometry results in the bottom row of Fig. 18 (b)
demonstrate that our object-aware adaptive fusion can successfully
model occluded scenarios and avoid deteriorated fusion.

Fig. 13. Evaluation of the mask reﬁnement (for objects). The ﬁrst row
demonstrates the input color images. The second and the fourth row
are the original masks from the network [70] and our reﬁned masks,
respectively. The third and the last row overlaying on the color images
are the tracking results using the object masks before and after mask
reﬁnement, respectively.

Fig. 14. Evaluation of the mask reﬁnement (for objects). The color-coded
maps indicate the projective error maps, in which the two rows are
corresponding to the results in Fig. 13.

Then, we evaluate the data-driven visual cues – the pose
term in human motion optimization. Similar to the preliminary
method [18], we compare to the variation of our pipeline without
pose prior in two scenarios where fast motion or disappear-
reoccurred case happens. The ﬁrst row of Fig. 16 demonstrates that
our variation without pose term suffers from severe accumulated
error, especially for the limb region with faster motions and
more depth noise from the commercial sensor. Our full pipeline
relieves this problem and help achieve superior tracking results for

14

(a) Fast motion scenarios

(a)

(b)

(c)

Fig. 17. Evaluation of the prior constraints. (a) Reference color images.
(b) The results without prior constraints (front view and side view re-
spectively). (c) The results with prior constraints (front view and side
view respectively).

(b) Disappear and reoccur scenarios

Fig. 16. Evaluation of pose term. The top row of (a) and (b) are results
of our method variation without pose term. The bottom row of (a) and
(b) are results of our approach with pose term in optimization.

Expansion for Multi-person scenarios. Here we show our ca-
pability for extending to multi-person capture. As demonstrated
in the last row of Fig. 6, with semantic segmentation labels of
different subjects of the whole scene and our mask reﬁnement
process, we can also enable multi-person reconstruction. Note that
to capture such a larger scene with two persons, we deploy a two-
camera system using the online calibration from [10]. We believe
that it is an essential step for the reconstruction of more general
dynamic scenes.

5.4 Limitation

As a trial for robust monocular volumetric performance capture
under human-object
interactions, we have demonstrated com-
pelling 4D reconstruction results. Nonetheless, our approach is
subject to some limitations.

Similar to the previous methods [10], [16], [17], [18], our
method cannot reconstruct the extremely ﬁne details of the per-
former, such as the ﬁngers, the subtle expression, and shaggy
hair, due to the limited resolution and inherent noise of the depth
input. It is promising to adopt data-driven techniques to further
generate visually pleasant synthetic geometry details in those
model-speciﬁc regions. Besides, the reconstruction of loose and
wide cloth such as a long skirt with high-speed motions remains
to track such large free-form
challenging since it

is difﬁcult

(a)

(b)

Fig. 18. Evaluation of the object-aware adaptive fusion. (a) A sequence
with high-speed motions. (b) A sequence with occlusions. The ﬁrst to
the third row are reference color images, the geometry results without
object-aware adaptive fusion, and the geometry results with object-
aware adaptive fusion.

non-rigid deformation beyond human skeletal motions. It is also
challenging for human initialization in Sec. 4.2. A better human
model regression algorithm during model initialization or utilizing
a pre-scanned detailed template will help remove this limitation.
Furthermore, we cannot handle surface splitting topology changes
like clothes removal, which we plan to address by incorporating
the key-volume update technique [7]. As common for learning
methods, the utilized scene semantic segmentation, human pars-
ing, and pose estimation fail for extreme scenarios not seen in
training, such as severe and extensive (self-)occlusions under
extreme side-view observation. However, our mask reﬁnement
strategy turns to obtain accurate masks, and data-driven cues of
motion prediction and pose prior help us to relieve the occlusion
problem with re-initialization ability. As for more general interac-
tions, our current system still cannot handle tiny objects which can
be played with ﬁngers or non-rigid objects like dolls or papers,
which restricts the wide practical applications of our approach.

The limitation of tiny objects is also due to the limited image
resolution and quality of the available commercial RGBD sensors.
We plan to combine those the task-speciﬁc approach such as [31],
[50] for ﬁne-grained interaction modeling and extend our method
to non-rigid objects by modifying our non-graph sampling and
updating strategy. Besides, our current pipeline has tried to handle
multi-person scenarios with inter-person interactions at a certain
level by using more RGBD sensors as input. It is a promising
and challenging direction to deal with more general inter-person
interactions such as dancing, wrestling, and hugging, even using
the same monocular RGBD input.

6 CONCLUSION
We have presented RobustFusion, a robust volumetric perfor-
mance reconstruction approach for complex human-object in-
teractions and challenging human motions using only a single
RGBD sensor. It combines various data-driven visual and inter-
action cues for robust human-object 4D reconstruction whilst still
maintaining light-weight computation and monocular setup. Our
scene decoupling scheme with segmentation reﬁnement and robust
object tracking enables explicit human-object disentanglement and
temporal-consistent modeling, while our human initialization gets
rid of the tedious self-scanning constraint. Our robust human per-
formance capture with various visual and interaction cues models
complex interaction patterns in a data-driven manner and enables
natural motion reconstruction under challenging human-object
occlusions, with unique re-initialization ability. Our object-aware
adaptive fusion scheme successfully avoids deteriorated fusion
and obtains temporally coherent human-object reconstruction with
the aid of occlusion analysis and human parsing cue. Extensive
experimental results demonstrate the effectiveness and robustness
of our approach for compelling performance capture in various
challenging scenarios with human-object interactions. We believe
that it is a critical step to enable robust and lightweight dynamic
scene reconstruction under human-object interactions, with many
potential applications in VR/AR, entertainment, human behavior
analysis, and immersive telepresence.

REFERENCES

[1] A. Tevs, A. Berner, M. Wand, I. Ihrke, M. Bokeloh, J. Kerber, and H.-
P. Seidel, “Animation cartography—intrinsic reconstruction of shape and
motion,” ACM Transactions on Graphics (TOG), vol. 31, no. 2, pp. 1–15,
2012.

[2] N. J. Mitra, S. Fl¨ory, M. Ovsjanikov, N. Gelfand, L. J. Guibas, and
H. Pottmann, “Dynamic geometry registration,” in Symposium on geom-
etry processing. Citeseer, 2007, pp. 173–182.

[3] H. Li, B. Adams, L. J. Guibas, and M. Pauly, “Robust single-view
geometry and motion reconstruction,” ACM Trans. Graph. (Proc. of
SIGGRAPH Asia), vol. 28, no. 5, p. 175, 2009.

[4] H. Li, L. Luo, D. Vlasic, P. Peers, J. Popovi´c, M. Pauly, and
S. Rusinkiewicz, “Temporally coherent completion of dynamic shapes,”
in ACM Trans. Graph., vol. 31, no. 1, Feb. 2012.
J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon, “The vitruvian
manifold: Inferring dense correspondences for one-shot human pose
estimation,” in 2012 IEEE Conference on Computer Vision and Pattern
Recognition, 2012, pp. 103–110.

[5]

[6] A. Collet, M. Chuang, P. Sweeney, D. Gillett, D. Evseev, D. Calabrese,
H. Hoppe, A. Kirk, and S. Sullivan, “High-quality streamable free-
viewpoint video,” ACM Transactions on Graphics (TOG), vol. 34, no. 4,
p. 69, 2015.

[7] M. Dou, S. Khamis, Y. Degtyarev, P. Davidson, S. Fanello, A. Kowdle,
S. O. Escolano, C. Rhemann, D. Kim, J. Taylor, P. Kohli, V. Tankovich,
and S. Izadi, “Fusion4D: Real-time Performance Capture of Challenging
Scenes,” in ACM SIGGRAPH Conference on Computer Graphics and
Interactive Techniques, 2016.

15

[8] M. Dou, P. Davidson, S. R. Fanello, S. Khamis, A. Kowdle, C. Rhemann,
V. Tankovich, and S. Izadi, “Motion2fusion: Real-time volumetric per-
formance capture,” ACM Trans. Graph., vol. 36, no. 6, pp. 246:1–246:16,
Nov. 2017.

[9] H. Joo, T. Simon, and Y. Sheikh, “Total capture: A 3d deformation
model for tracking faces, hands, and bodies,” in The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.

[10] L. Xu, Z. Su, L. Han, T. Yu, Y. Liu, and L. FANG, “Unstructuredfusion:
Realtime 4d geometry and texture reconstruction using commercialrgbd
cameras,” IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, pp. 1–1, 2019.

[11] R. A. Newcombe, D. Fox, and S. M. Seitz, “Dynamicfusion: Reconstruc-
tion and tracking of non-rigid scenes in real-time,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2015, pp.
343–352.

[12] M. Innmann, M. Zollh¨ofer, M. Nießner, C. Theobalt, and M. Stamminger,
“Volumedeform: Real-time volumetric non-rigid reconstruction,” in Eu-
ropean Conference on Computer Vision. Springer, 2016, pp. 362–379.
[13] M. Slavcheva, M. Baust, D. Cremers, and S. Ilic, “KillingFusion: Non-
rigid 3D Reconstruction without Correspondences,” in IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.

[14] H. Zhang and F. Xu, “Mixedfusion: Real-time reconstruction of an indoor
scene with dynamic objects,” IEEE Transactions on Visualization and
Computer Graphics, vol. 24, no. 12, pp. 3137–3146, 2018.

[15] T. Yu, K. Guo, F. Xu, Y. Dong, Z. Su, J. Zhao, J. Li, Q. Dai, and Y. Liu,
“Bodyfusion: Real-time capture of human motion and surface geometry
using a single depth camera,” in The IEEE International Conference on
Computer Vision (ICCV). ACM, October 2017.

[16] T. Yu, Z. Zheng, K. Guo, J. Zhao, Q. Dai, H. Li, G. Pons-Moll, and
Y. Liu, “Doublefusion: Real-time capture of human performances with
inner body shapes from a single depth sensor,” Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 2019.

[17] Z. Li, T. Yu, Z. Zheng, K. Guo, and Y. Liu, “Posefusion: Pose-guided
selective fusion for single-view human volumetric capture,” 2021.
[18] Z. Su, L. Xu, Z. Zheng, T. Yu, Y. Liu, and L. Fang, “Robustfusion:
Human volumetric capture with data-driven visual cues using a rgbd
camera,” in Computer Vision – ECCV 2020, A. Vedaldi, H. Bischof,
T. Brox, and J.-M. Frahm, Eds. Cham: Springer International Publishing,
2020, pp. 246–264.

[19] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
pose estimation using part afﬁnity ﬁelds,” in Computer Vision and Pattern
Recognition (CVPR), 2017.

[20] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, “End-to-end
recovery of human shape and pose,” in Computer Vision and Pattern
Regognition (CVPR), 2018.

[21] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li,
“Pifu: Pixel-aligned implicit function for high-resolution clothed human
digitization,” in The IEEE International Conference on Computer Vision
(ICCV), October 2019.

[22] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d human re-
construction from a single image,” in The IEEE International Conference
on Computer Vision (ICCV), October 2019.

[23] T. Zhu and D. Oved, “Bodypix github repository,” https://github.com/

tensorﬂow/tfjs-models/tree/master/body-pix, 2019.

[24] Y. Zhang, M. Hassan, H. Neumann, M. J. Black, and S. Tang, “Generat-
ing 3d people in scenes without people,” in Computer Vision and Pattern
Recognition (CVPR), Jun. 2020, pp. 6194–6204.

[25] T. Zhang, B. Huang, and Y. Wang, “Object-occluded human shape
and pose estimation from a single color image,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 7376–7385.

[26] J. Y. Zhang, S. Pepose, H. Joo, D. Ramanan, J. Malik, and A. Kanazawa,
“Perceiving 3d human-object spatial arrangements from a single image
in the wild,” in European Conference on Computer Vision (ECCV), 2020.
[27] S. Zhang, Y. Zhang, Q. Ma, M. J. Black, and S. Tang, “PLACE: Proximity
learning of articulation and contact in 3D environments,” in International
Conference on 3D Vision (3DV), Nov. 2020.

[28] V. Guzov, A. Mir, T. Sattler, and G. Pons-Moll, “Human poseitioning
system (hps): 3d human pose estimation and self-localization in large
scenes from body-mounted sensors,” in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

IEEE, jun 2021.

[29] M. Hassan, P. Ghosh, J. Tesch, D. Tzionas, and M. J. Black, “Popu-
lating 3D scenes by learning human-scene interaction,” in Proceedings
IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR),
Jun. 2021.

[30] P. Patel, C.-H. P. Huang, J. Tesch, D. T. Hoffmann, S. Tripathi, and
M. J. Black, “AGORA: Avatars in geography optimized for regression

analysis,” in Proceedings IEEE/CVF Conf. on Computer Vision and
Pattern Recognition (CVPR), Jun. 2021.

[31] O. Taheri, N. Ghorbani, M. J. Black, and D. Tzionas, “Grab: A dataset
of whole-body human grasping of objects,” in Computer Vision – ECCV
2020, vol. LNCS 12355. Cham: Springer International Publishing,
Aug. 2020, pp. 581–600. [Online]. Available: https://grab.is.tue.mpg.de

[32] B. Curless and M. Levoy, “A volumetric method for building complex
models from range images,” in Proceedings of
the 23rd Annual
Conference on Computer Graphics and Interactive Techniques, ser.
SIGGRAPH ’96. New York, NY, USA: ACM, 1996, pp. 303–312.
[Online]. Available: http://doi.acm.org/10.1145/237170.237269

[33] R. W. Sumner, J. Schmid, and M. Pauly, “Embedded deformation for
shape manipulation,” ACM Transactions on Graphics (TOG), vol. 26,
no. 3, p. 80, 2007.

[34] K. Guo, F. Xu, T. Yu, X. Liu, Q. Dai, and Y. Liu, “Real-time geometry,
albedo and motion reconstruction using a single rgbd camera,” ACM
Transactions on Graphics (TOG), 2017.

[35] M. Slavcheva, M. Baust, and S. Ilic, “SobolevFusion: 3D Reconstruction
of Scenes Undergoing Free Non-rigid Motion,” in IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.

[36] ——, “Variational Level Set Evolution for Non-rigid 3D Reconstruction
from a Single Depth Camera,” in IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 2020.

[37] T. Yu, J. Zhao, Y. Huang, Y. Li, and Y. Liu, “Towards robust and accurate
single-view fast human motion capture,” IEEE Access, vol. 7, pp. 85 548–
85 559, 2019.

[38] Z. Zheng, T. Yu, H. Li, K. Guo, Q. Dai, L. Fang, and Y. Liu, “Hybrid-
fusion: Real-time performance capture using a single depth sensor and
sparse imus,” in European Conference on Computer Vision (ECCV), Sept
2018.

[39] L. Xu, W. Cheng, K. Guo, L. Han, Y. Liu, and L. Fang, “Flyfusion:
Realtime dynamic scene reconstruction using a ﬂying depth camera,”
IEEE Transactions on Visualization and Computer Graphics, pp. 1–1,
2019.

[40] R. Pandey, A. Tkach, S. Yang, P. Pidlypenskyi, J. Taylor, R. Martin-
Brualla, A. Tagliasacchi, G. Papandreou, P. Davidson, C. Keskin, S. Izadi,
and S. Fanello, “Volumetric capture of humans with a single rgbd camera
via semi-parametric learning,” in The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.

[41] R. Martin-Brualla, R. Pandey, S. Yang, P. Pidlypenskyi, J. Taylor,
J. Valentin, S. Khamis, P. Davidson, A. Tkach, P. Lincoln, and et al.,
“Lookingood: Enhancing performance capture with real-time neural re-
rendering,” ACM Trans. Graph., vol. 37, no. 6, Dec. 2018.

[42] X. Huang, I. Walker, and S. Birchﬁeld, “Occlusion-aware reconstruction
and manipulation of 3d articulated objects,” in 2012 IEEE International
Conference on Robotics and Automation.
IEEE, 2012, pp. 1365–1371.
[43] J. Park, Q.-Y. Zhou, and V. Koltun, “Colored point cloud registration
the IEEE international conference on

revisited,” in Proceedings of
computer vision, 2017, pp. 143–152.

[44] L. Liu, X. Xia, H. Sun, Q. Shen, J. Xu, B. Chen, H. Huang, and
K. Xu, “Object-aware guidance for autonomous scene reconstruction,”
ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 1–12, 2018.

[45] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove,
“Deepsdf: Learning continuous signed distance functions for shape
representation,” in The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.

[46] V. Sitzmann, M. Zollh¨ofer, and G. Wetzstein, “Scene representation
networks: Continuous 3d-structure-aware neural scene representations,”
arXiv preprint arXiv:1906.01618, 2019.

[47] H. Xie, H. Yao, S. Zhang, S. Zhou, and W. Sun, “Pix2vox++: multi-
scale context-aware 3d object reconstruction from single and multiple
images,” International Journal of Computer Vision, vol. 128, no. 12, pp.
2919–2935, 2020.

[48] P. Wei, Y. Zhao, N. Zheng, and S.-C. Zhu, “Modeling 4d human-object
interactions for event and object recognition,” in Proceedings of the IEEE
International Conference on Computer Vision, 2013, pp. 3272–3279.
[49] ——, “Modeling 4d human-object interactions for joint event segmenta-
tion, recognition, and object localization,” IEEE transactions on pattern
analysis and machine intelligence, vol. 39, no. 6, pp. 1165–1179, 2016.
[50] K. Karunratanakul, J. Yang, Y. Zhang, M. Black, K. Muandet, and
S. Tang, “Grasping ﬁeld: Learning implicit representations for human
grasps,” in International Conference on 3D Vision (3DV), Nov. 2020.
[51] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
A. Kipman, and A. Blake, “Real-time Human Pose Recognition in Parts
from Single Depth Images,” in Proc. of CVPR, 2011.

[52] V. Ganapathi, C. Plagemann, D. Koller, and S. Thrun, “Real time motion

capture using a single time-of-ﬂight camera,” in Proc. of CVPR, 2010.

16

[53] G. Rogez and C. Schmid, “Mocap Guided Data Augmentation for 3D
Pose Estimation in the Wild,” in Neural Information Processing Systems
(NIPS), 2016.

[54] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shaﬁei, H.-
P. Seidel, W. Xu, D. Casas, and C. Theobalt, “Vnect: Real-time 3d
human pose estimation with a single rgb camera,” ACM Transactions
on Graphics (TOG), vol. 36, no. 4, 2017.

[55] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and
J. Davis, “Scape: Shape completion and animation of people,” in ACM
SIGGRAPH 2005 Papers, ser. SIGGRAPH ’05. New York, NY, USA:
Association for Computing Machinery, 2005, p. 408–416. [Online].
Available: https://doi.org/10.1145/1186822.1073207

[56] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black,
“Smpl: A skinned multi-person linear model,” ACM Trans. Graph.,
vol. 34, no. 6, pp. 248:1–248:16, Oct. 2015.

[57] O. Kovalenko, V. Golyanik, J. Malik, A. Elhayek, and D. Stricker,
“Structure from Articulated Motion: Accurate and Stable Monocular 3D
Reconstruction without Training Data,” Sensors, vol. 19, no. 20, 2019.

[58] H. Joo, N. Neverova, and A. Vedaldi, “Exemplar ﬁne-tuning for 3d
human pose ﬁtting towards in-the-wild 3d human pose estimation,” arXiv
preprint arXiv:2004.03686, 2020.

[59] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black,
“Keep it smpl: Automatic estimation of 3d human pose and shape from
a single image,” in Computer Vision – ECCV 2016, B. Leibe, J. Matas,
N. Sebe, and M. Welling, Eds. Cham: Springer International Publishing,
2016, pp. 561–578.

[60] T. Alldieck, G. Pons-Moll, C. Theobalt, and M. Magnor, “Tex2shape:
Detailed full human body geometry from a single image,” in IEEE
International Conference on Computer Vision (ICCV), 2019.

[61] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and G. Pons-
Moll, “Learning to reconstruct people in clothing from a single RGB
camera,” in IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), jun 2019.

[62] S. Saito, T. Simon, J. Saragih, and H. Joo, “Pifuhd: Multi-level pixel-
aligned implicit function for high-resolution 3d human digitization,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, June 2020.

[63] Z. Huang, Y. Xu, C. Lassner, H. Li, and T. Tung, “Arch: Animatable
reconstruction of clothed humans,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
3093–3102.

[64] Z. Zheng, T. Yu, Y. Liu, and Q. Dai, “Pamir: Parametric model-
conditioned implicit representation for image-based human reconstruc-
tion,” arXiv preprint arXiv:2007.03858, 2020.

[65] G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev, and
C. Schmid, “BodyNet: Volumetric inference of 3D human body shapes,”
in ECCV, 2018.

[66] W. Xu, A. Chatterjee, M. Zollh¨ofer, H. Rhodin, D. Mehta, H.-P. Seidel,
and C. Theobalt, “Monoperfcap: Human performance capture from
monocular video,” ACM Transactions on Graphics (TOG), vol. 37, no. 2,
pp. 27:1–27:15, 2018.

[67] M. Habermann, W. Xu, M. Zollh¨ofer, G. Pons-Moll, and C. Theobalt,
“Livecap: Real-time human performance capture from monocular video,”
ACM Transactions on Graphics (TOG), vol. 38, no. 2, pp. 14:1–14:17,
2019.

[68] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, and C. Theobalt,
“Eventcap: Monocular 3d capture of high-speed human motions using
an event camera,” in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

IEEE, jun 2020.

[69] M. Habermann, W. Xu, M. Zollhoefer, G. Pons-Moll, and C. Theobalt,
“Deepcap: Monocular human performance capture using weak supervi-
sion,” in IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

IEEE, jun 2020.

[70] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing

network,” in CVPR, 2017.

[71] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 2961–2969.

[72] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin, “Look into person:
Self-supervised structure-sensitive learning and a new benchmark for
human parsing,” in The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.

[73] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, and L. Lin, “Instance-level
human parsing via part grouping network,” in The European Conference
on Computer Vision (ECCV), September 2018.

