SVRG meets AdaGrad: Painless Variance Reduction

1
2
0
2

v
o
N
3

]

G
L
.
s
c
[

2
v
5
4
6
9
0
.
2
0
1
2
:
v
i
X
r
a

Benjamin Dubois-Taine(cid:63)1

Sharan Vaswani(cid:63)2

Reza Babanezhad3

Mark Schmidt4

Simon Lacoste-Julien5

1 Université Paris-Saclay

2 Amii, University of Alberta

3 SAIT AI lab, Montreal

4 University of British Columbia

5 Mila, Université de Montréal

Abstract

Variance reduction (VR) methods for ﬁnite-
sum minimization typically require the knowl-
edge of problem-dependent constants that are
often unknown and diﬃcult to estimate. To
address this, we use ideas from adaptive gra-
dient methods to propose AdaSVRG, which
is a more-robust variant of SVRG, a com-
mon VR method. AdaSVRG uses AdaGrad
in the inner loop of SVRG, making it ro-
bust to the choice of step-size. When min-
imizing a sum of n smooth convex functions,
we prove that a variant of AdaSVRG requires
˜O(n + 1/(cid:15)) gradient evaluations to achieve
an O((cid:15))-suboptimality, matching the typical
rate, but without needing to know problem-
dependent constants. Next, we leverage the
properties of AdaGrad to propose a heuristic
that adaptively determines the length of each
inner-loop in AdaSVRG. Via experiments on
synthetic and real-world datasets, we validate
the robustness and eﬀectiveness of AdaSVRG,
demonstrating its superior performance over
standard and other “tune-free” VR methods.

1

Introduction

Variance reduction (VR) methods (Schmidt et al., 2017;
Konečn`y and Richtárik, 2013; Mairal, 2013; Shalev-
Shwartz and Zhang, 2013; Johnson and Zhang, 2013;
Mahdavi and Jin, 2013; Konečn`y and Richtárik, 2013;
Defazio et al., 2014; Nguyen et al., 2017) have proven

(cid:63)Equal contribution.
Correspondence
vaswani.sharan@gmail.com

to benj.duboistaine@gmail.com and

to be an important class of algorithms for stochas-
tic optimization. These methods take advantage of the
ﬁnite-sum structure prevalent in machine learning prob-
lems, and have improved convergence over stochastic
gradient descent (SGD) and its variants (see (Gower
et al., 2020) for a recent survey). For example, when
minimizing a ﬁnite sum of n strongly-convex, smooth
functions with condition number κ, these methods typ-
ically require O ((κ + n) log(1/(cid:15))) gradient evaluations
to obtain an (cid:15)-error. This improves upon the complex-
ity of full-batch gradient descent (GD) that requires
O (κn log(1/(cid:15))) gradient evaluations, and SGD that
has an O(κ/(cid:15)) complexity. Moreover, there have been
numerous VR methods that employ Nesterov acceler-
ation (Allen-Zhu, 2017; Lan et al., 2019; Song et al.,
2020) and can achieve even faster rates.

In order to guarantee convergence, VR methods re-
quire an easier-to-tune constant step-size, whereas SGD
needs a decreasing step-size schedule. Consequently,
VR methods are commonly used in practice, especially
when training convex models such as logistic regres-
sion or conditional Markov random ﬁelds (Schmidt
et al., 2015). However, all the above-mentioned VR
methods require knowledge of the smoothness of the
underlying function in order to set the step-size. The
smoothness constant is often unknown and diﬃcult to
estimate in practice. Although we can obtain global
upper-bounds on it for simple problems such as least
squares regression, these bounds are usually too loose
to be practically useful and result in sub-optimal per-
formance. Consequently, implementing VR methods
requires a computationally expensive search over a
range of step-sizes. Furthermore, a constant step-size
does not adapt to the function’s local smoothness and
may lead to poor empirical performance.

Consequently, there have been a number of works that
try to adapt the step-size in VR methods. Schmidt
et al. (2017) and Mairal (2013) employ stochastic line-
search procedures to set the step-size in VR algorithms.

 
 
 
 
 
 
SVRG meets AdaGrad

While they show promising empirical results using line-
searches, these procedures have no theoretical conver-
gence guarantees. Recent works (Tan et al., 2016; Li
et al., 2020) propose to use the Barzilai-Borwein (BB)
step-size (Barzilai and Borwein, 1988) in conjunction
with two common VR algorithms - stochastic vari-
ance reduced gradient (SVRG) (Johnson and Zhang,
2013) and the stochastic recursive gradient algorithm
(SARAH) (Nguyen et al., 2017). Both Tan et al. (2016)
and Li et al. (2020) can automatically set the step-size
without requiring the knowledge of problem-dependent
constants. However, in order to prove theoretical guar-
antees for strongly-convex functions, these techniques
require the knowledge of both the smoothness and strong-
convexity parameters. In fact, their guarantees require
using a small O(1/κ2) step-size, a highly-suboptimal
choice in practice. Consequently, there is a gap in the
theory and practice of adaptive VR methods. To ad-
dress this, we make the following contributions.

1.1 Background and Contributions

SVRG meets AdaGrad: In Section 3 we use Ada-
Grad (Duchi et al., 2011; Levy et al., 2018), an adap-
tive gradient method, with stochastic variance reduc-
tion techniques. We focus on SVRG (Johnson and
Zhang, 2013) and propose to use AdaGrad within its
inner-loop. We analyze the convergence of the result-
ing AdaSVRG algorithm for minimizing convex func-
tions (without strong-convexity). Using O(n) inner-
loops for every outer-loop (a typical setting used in
practice (Babanezhad Harikandeh et al., 2015; Sebbouh
et al., 2019)), and any bounded step-size, we prove that
AdaSVRG achieves an (cid:15)-error (for (cid:15) = O(1/n)) with
O(n/(cid:15)) gradient evaluations (Theorem 1). This rate
matches that of SVRG with a constant step-size and
O(n) inner-loops (Reddi et al., 2016, Corollary 10).
However, unlike Reddi et al. (2016), our result does
not require knowledge of the smoothness constant in
order to set the step-size. We note that other previous
work (Cutkosky and Orabona, 2019; Liu et al., 2020)
consider adaptive methods with variance reduction for
non-convex minimization; however their algorithms still
require knowledge of problem-dependent parameters.

Multi-stage AdaSVRG: We propose a multi-stage
variant of AdaSVRG where each stage involves running
AdaSVRG for a ﬁxed number of inner and outer-loops.
In particular, multi-stage AdaSVRG maintains a ﬁxed-
size outer-loop and doubles the length of the inner-
loop across stages. We prove that it requires O((n +
1/(cid:15)) log(1/(cid:15))) gradient evaluations to reach an O((cid:15))
error (Theorem 2). This improves upon the complexity
of decreasing step-size SVRG that requires O(n+
n/(cid:15))
gradient evaluations (Reddi et al., 2016, Corollary 9);
and matches the rate of SARAH (Nguyen et al., 2017).

√

AdaSVRG with adaptive termination: Instead
of using a complex multi-stage procedure, we prove
that AdaSVRG can also achieve the improved O((n +
1/(cid:15)) log(1/(cid:15))) gradient evaluation complexity by adap-
tively terminating its inner-loop (Section 4). However,
the adaptive termination requires the knowledge of
problem-dependent constants, limiting its practical use.

To address this, we use the favourable properties of
AdaGrad to design a practical heuristic for adaptively
terminating the inner-loop. Our technique for adap-
tive termination is related to heuristics (Pﬂug, 1983;
Yaida, 2018; Lang et al., 2019; Pesme et al., 2020) that
detect stalling for constant step-size SGD, and may
be of independent interest. First, we show that when
minimizing smooth convex losses, AdaGrad has a two-
phase behaviour - a ﬁrst “deterministic phase” where
the step-size remains approximately constant followed
by a second “stochastic” phase where the step-size de-
creases at an O(1/
t) rate (Theorem 4). We show
that it is empirically possible to eﬃciently detect this
phase transition and aim to terminate the AdaSVRG
inner-loop when AdaGrad enters the stochastic phase.

√

Practical considerations and experimental eval-
uation: In Section 5, we describe some of the prac-
tical considerations for implementing AdaSVRG and
the adaptive termination heuristic. We use standard
real-world datasets to empirically verify the robustness
and eﬀectiveness of AdaSVRG. Across datasets, we
demonstrate that AdaSVRG consistently outperforms
variants of SVRG, SARAH and methods based on the
BB step-size (Tan et al., 2016; Li et al., 2020).

Adaptivity to over-parameterization: Defazio and
Bottou (2019) demonstrated the ineﬀectiveness of
SVRG when training large over-parameterized mod-
els such as deep neural networks. We argue that this
ineﬀectiveness can be partially explained by the in-
terpolation property satisﬁed by over-parameterized
models (Schmidt and Le Roux, 2013; Ma et al., 2018;
Vaswani et al., 2019a). In the interpolation setting, SGD
obtains an O(1/(cid:15)) gradient complexity when minimiz-
ing smooth convex functions (Vaswani et al., 2019a),
thus out-performing typical VR methods. However, in-
terpolation is rarely exactly satisﬁed in practice, and
using SGD can result in oscillations around the solution.
On the other hand, although VR methods have a slower
convergence, they do not oscillate, regardless of interpo-
lation. In Appendix B, we use AdaGrad to exploit the
(approximate) interpolation property, and employ the
above heuristic to adaptively switch to AdaSVRG, thus
avoiding oscillatory behaviour. We design synthetic
problems controlling the extent of interpolation and
show that the hybrid AdaGrad-AdaSVRG algorithm
can match or outperform both stochastic gradient and
VR methods, thus achieving the best of both worlds.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

2 Problem setup

(cid:80)n

We consider the minimization of an objective f :
Rd → R with a ﬁnite-sum structure, minw∈X f (w) =
1
i=1 fi(w) where X is a convex compact set of di-
n
ameter D, meaning supx,y∈X (cid:107)x − y(cid:107) ≤ D. Problems
with this structure are prevalent in machine learning.
For example, in supervised learning, n represents the
number of training examples, and fi is the loss function
when classifying or regressing to training example i.
Throughout this paper, we assume f and each fi are dif-
ferentiable. We assume that f is convex, implying that
there exists a solution w∗ ∈ X that minimizes it, and
deﬁne f ∗ := f (w∗). Interestingly we do not need each
fi to be convex. We further assume that each function
fi in the ﬁnite-sum is Li-smooth, implying that f is
Lmax-smooth, where Lmax = maxi Li. We include the
formal deﬁnitions of these properties in Appendix A.

We focus on the SVRG algorithm (Johnson and Zhang,
2013) since it is more memory eﬃcient than alternatives
like SAG (Schmidt et al., 2017) or SAGA (Defazio et al.,
2014). SVRG has a nested inner-outer loop structure. In
every outer-loop k, it computes the full gradient ∇f (wk)
at a snapshot point wk. An outer-loop k consists of mk
inner-loops indexed by t = 1, 2, . . . mk and the inner-
loop iterate x1 is initialized to wk. In outer-loop k and
inner-loop t, SVRG samples an example it (typically
uniformly at random) and takes a step in the direction
of the variance-reduced gradient gt using a constant
step-size η. This update can be expressed as:

gt = ∇fit(xt) − ∇fit(wk) + ∇f (wk)

xt+1 = ΠX [xt − η gt] ,

(1)

where ΠX denotes the Euclidean projection onto the set
X. The variance-reduced gradient is unbiased, meaning
that Eit[gt|xt] = f (xt). At the end of the inner-loop,
the next snapshot point is typically set to either the
last or averaged iterate in the inner-loop.

SVRG requires the knowledge of both the strong-
convexity and smoothness constants in order to set
the step-size and the number of inner-loops. These
requirements were relaxed in Hofmann et al. (2015);
Kovalev et al. (2020); Gower et al. (2020) that only
require knowledge of the smoothness.

In order to set the step-size for SVRG without requiring
knowledge of the smoothness, line-search techniques
are an attractive option. Such techniques are a com-
mon approach to automatically set the step-size for
(stochastic) gradient descent (Armijo, 1966; Vaswani
et al., 2019b). However, we show that an intuitive
Armijo-like line-search to set the SVRG step-size is not
guaranteed to converge to the solution. Speciﬁcally, we
prove the following proposition in Appendix K.

Proposition 1. If in each inner-loop t of SVRG, ηt
is set as the largest step-size satisfying the condition:
ηt ≤ ηmax and

f (xt − ηtgt) ≤ f (xt) − cηt (cid:107)gt(cid:107)2 where

(c > 0),

then for any c > 0, ηmax > 0, there exists a 1-
dimensional convex smooth function such that if |xt −
w∗| ≤ min{ 1
c , 1}, then |xt+1 − w∗| ≥ |xt − w∗|, imply-
ing that the update moves the iterate away from the
solution when it is close to it, preventing convergence.

In the next section, we suggest a novel approach using
AdaGrad (Duchi et al., 2011) to propose AdaSVRG, a
provably-convergent VR method that is more robust
to the choice of step-size. To justify our decision to use
AdaGrad, we note that in general, there are (roughly)
three common ways of designing methods that do not
require knowledge of problem-dependent constants: (i)
BB step-size, but it still requires knowledge of Lmax to
guarantee convergence in the VR setting (Tan et al.,
2016; Li et al., 2020), (ii) Line-search methods that can
fail to converge in the VR setting (Proposition 1), (iii)
Adaptive gradient methods such as AdaGrad.

.
3 Adaptive SVRG

Algorithm 1 AdaSVRG with ﬁxed-sized inner-loop
Input: w0 (initial point), K (outer-loops), m (inner-
loops)
for k ← 0 to K − 1 do

Compute full gradient ∇f (wk)
ηk ← Compute step-size
Initialize: x1 = wk and G0 = 0.
for t ← 1 to m do

Sample it ∼ Uniform{1, 2, . . . n}
gt = ∇fit(xt) − ∇fit(wk) + ∇f (wk)
Gt = Gt−1 + (cid:107)gt(cid:107)2
At = G1/2
t
xt+1 = ΠX,At

(cid:0)xt − ηkA−1

t gt

(cid:1)

end
wk+1 = 1
m

(cid:80)m

t=1 xt

end
Return ¯wK = 1
K

(cid:80)K

k=1 wk

Like SVRG, AdaSVRG has a nested inner-outer loop
structure and relies on computing the full gradient in
every outer-loop. However, it uses AdaGrad in the inner-
loop, using the variance reduced gradient gt to update
the preconditioner At in the inner-loop t. AdaSVRG
computes the step-size ηk in every outer-loop (see Sec-
tion 5 for details) and uses a preconditioned variance-
reduced gradient step to update the inner-loop iterates:

xt+1 = ΠX,At

(cid:0)xt − ηkA−1

t gt

(cid:1) .

SVRG meets AdaGrad

Algorithm 2 Multi-stage AdaSVRG
Input: ¯w0 (initial point), K (outer-loops), (cid:15) (target
error)
Initialize I = log(1/(cid:15))
for i ← 1 to I do
mi = 2i+1
¯wi = Algorithm 1( ¯wi−1, K, mi)

end
Return ¯wI

Here, ΠX,A (·) is the projection onto set X with respect
to the norm induced by a symmetric positive deﬁnite
matrix A (such projections are common to adaptive
gradient methods (Duchi et al., 2011; Levy et al., 2018;
Reddi et al., 2018)). AdaSVRG then sets the next
snapshot wk+1 to be the average of the inner-loop
iterates. Throughout the main paper, we will only focus
on the scalar variant (Ward et al., 2019) of AdaGrad
(see Algorithm 1 for the pseudo-code). We defer the
general diagonal and matrix variants (see Appendix C
for the pseudo-code) and their corresponding theory to
the Appendix.

We now analyze the convergence of AdaSVRG. We
start with the analysis of a single outer-loop, and prove
the following lemma in Appendix D

Lemma 1 (AdaSVRG with single outer-loop). Assume
(i) convexity of f , (ii) Lmax-smoothness of fi and (iii)
bounded feasible set with diameter D. Deﬁning ρ :=
(cid:0) D2
Lmax, for any outer loop k of AdaSVRG,
ηk

+ 2ηk

(cid:1)√

with (a) inner-loop length mk and (b) step-size ηk,

E[f (wk+1) − f ∗] ≤

ρ2
mk

+

ρ(cid:112)E[f (wk) − f ∗]
√
mk

The proof of the above lemma leverages the theoretical
results of AdaGrad (Duchi et al., 2011; Levy et al.,
2018). Speciﬁcally, the standard AdaGrad analysis
bounds the “noise” term by the variance in the stochas-
tic gradients. On the other hand, we use the properties
of the variance reduced gradient in order to upper-
bound the noise in terms of the function suboptimality.

√

Lemma 1 shows that a single outer-loop of AdaSVRG
converges to the minimizer as O(1/
m), where m is
the number of inner-loops. This implies that in order
to obtain an (cid:15)-error, a single outer-loop of AdaSVRG
requires O(n + 1/(cid:15)2) gradient evaluations. This result
holds for any bounded step-size and requires setting
m = O(1/(cid:15)2). This “single outer-loop convergence”
property of AdaSVRG is unlike SVRG or any of its
variants; running only a single-loop of SVRG is in-
eﬀective, as it stops making progress at some point,
resulting in the iterates oscillating in a neighbourhood
of the solution. The favourable behaviour of AdaSVRG
is similar to SARAH, but unlike SARAH, the above

result does not require computing a recursive gradient
or knowing the smoothness constant.

Next, we consider the convergence of AdaSVRG with
a ﬁxed-size inner-loop and multiple outer-loops. In
the following theorems, we assume that we have a
bounded range of step-sizes implying that for all k,
ηk ∈ [ηmin, ηmax]. For brevity, similar to Lemma 1, we
(cid:1)√
deﬁne ρ := (cid:0) D2
ηmin
Theorem 1 (AdaSVRG with ﬁxed-size inner-loop).
Under the same assumptions as Lemma 1, AdaSVRG
with (a) step-sizes ηk ∈ [ηmin, ηmax], (b) inner-loop size
mk = n for all k, results in the following convergence
rate after K ≤ n iterations.

+ 2ηmax

Lmax.

E[f ( ¯wK) − f ∗] ≤

√

ρ2(1 +

5) + ρ(cid:112)2 (f (w0) − f ∗)

K

where ¯wK = 1
K

(cid:80)K

k=1 wk.

The proof (refer to Appendix F) recursively uses the
result of Lemma 1 for K outer-loops.

The above result requires a ﬁxed inner-loop size
mk = n, a setting typically used in practice (Ba-
banezhad Harikandeh et al., 2015; Gower et al., 2020).
Notice that the above result holds only when K ≤ n.
Since K is the number of outer-loops, it is typically
much smaller than n, the number of functions in the
ﬁnite sum, justifying the theorem’s K ≤ n require-
ment. Moreover, in the sense of generalization error,
it is not necessary to optimize below an O(1/n) accu-
racy (Boucheron et al., 2005; Sridharan et al., 2008).

Theorem 1 implies that AdaSVRG can reach an (cid:15)-error
(for (cid:15) = Ω(1/n)) using O(n/(cid:15)) gradient evaluations.
This result matches the complexity of constant step-size
SVRG (with mk = n) of (Reddi et al., 2016, Corollary
10) but without requiring the knowledge of the smooth-
ness constant. However, unlike SVRG and SARAH, the
convergence rate depends on the diameter D rather
than (cid:107)w0 − w∗(cid:107), the initial distance to the solution.
This dependence arises due to the use of AdaGrad in
the inner-loop, and is necessary for adaptive gradient
methods. Speciﬁcally, Cutkosky and Boahen (2017)
prove that any adaptive (to problem-dependent con-
stants) method will necessarily incur such a dependence
on the diameter. Hence, such a diameter dependence
can be considered to be the “cost” of the lack of knowl-
edge of problem-dependent constants.

Since the above result only holds for (cid:15) = Ω(1/n), we pro-
pose a multi-stage variant (Algorithm 2) of AdaSVRG
that requires O((n + 1/(cid:15)) log(1/(cid:15))) gradient evaluations
to attain an O((cid:15))-error for any (cid:15). To reach a target
suboptimality of (cid:15), we consider I = log(1/(cid:15)) stages. For
each stage i, Algorithm 2 uses a ﬁxed number of outer-
loops K and inner-loops mi with stage i is initialized

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

to the output of the (i − 1)-th stage. In Appendix G,
we prove the following rate for multi-stage AdaSVRG.

Theorem 2 (Multi-stage AdaSVRG). Under the same
assumptions as Theorem 1, multi-stage AdaSVRG with
I = log(1/(cid:15)) stages, K ≥ 3 outer-loops and mi = 2i+1
(cid:15) + 1
inner-loops at stage i, requires O(n log 1
(cid:15) ) gradient
5)(cid:1) (cid:15)-sub-optimality.
evaluations to reach a (cid:0)ρ2(1 +

√

We see that multi-stage AdaSVRG matches the con-
vergence rate of SARAH (upto constants), but does
so without requiring the knowledge of the smoothness
constant to set the step-size. Observe that the number
of inner-loops increases with the stage i.e. mi = 2i+1.
The intuition behind this is that the convergence of
AdaGrad (used in the k-th inner-loop of AdaSVRG)
is slowed down by a “noise” term proportional to
f (wk) − f ∗ (see Lemma 1). When this “noise” term
is large in the earlier stages of multi-stage AdaSVRG,
the inner-loops have to be short in order to maintain
the overall O(1/(cid:15)) convergence. However, as the stages
progress and the suboptimality decreases, the “noise”
term becomes smaller, and the algorithm can use longer
inner-loops, which reduces the number of full gradient
computations, resulting in the desired convergence rate.

Thus far, we have focused on using AdaSVRG with
ﬁxed-size inner-loops. Next, we consider variants that
can adaptively determine the inner-loop size.

4 Adaptive termination of inner-loop

Recall that the convergence of a single outer-loop
k of AdaSVRG (Lemma 1) is slowed down by
the (cid:112)(f (wk)−f ∗)/mk term. Similar to the multi-stage
variant, the suboptimality f (wk) − f ∗ decreases as
AdaSVRG progresses. This allows the use of longer
inner-loops as k increases, resulting in fewer full-
gradient evaluations. We instantiate this idea by setting
mk = O (1/(f (wk)−f ∗)). Since this choice requires the
knowledge of f (wk) − f ∗, we alternatively consider us-
ing mk = O(1/(cid:15)), where (cid:15) is the desired sub-optimality.
We prove the following theorem in Appendix H.

Theorem 3 (AdaSVRG with adaptive-sized in-
ner-loops). Under the same assumptions as Lemma 1,
AdaSVRG with (a) step-sizes ηk ∈ [ηmin, ηmax], (b1)
inner-loop size mk = 4ρ2
for all k or (b2) inner-loop
size mk =
f (wk)−f ∗ for outer-loop k, results in the
following convergence rate,

4ρ2

(cid:15)

E[f (wK) − f ∗] ≤ (3/4)K[f (w0) − f ∗].

The above result implies a linear convergence in the
number of outer-loops, but each outer-loop requires
O(1/(cid:15)) inner-loops. Hence, Theorem 3 implies that
AdaSVRG with adaptive-sized inner-loops requires

O ((n + 1/(cid:15)) log(1/(cid:15))) gradient evaluations to reach an
(cid:15)-error. This improves upon the rate of SVRG and
matches the convergence rate of SARAH that also re-
quires inner-loops of length O(1/(cid:15)). Compared to The-
orem 1 that has an average iterate convergence (for
¯wK), Theorem 3 has the desired convergence for the last
outer-loop iterate wK and also holds for any bounded
sequence of step-sizes. However, unlike Theorem 1, this
result (with either setting of mk) requires the knowl-
edge of problem-dependent constants in ρ.

To address this issue, we design a heuristic for adaptive
termination in the next sections. We start by describing
the two phase behaviour of AdaGrad and subsequently
utilize it for adaptive termination in AdaSVRG.

4.1 Two phase behaviour of AdaGrad

Diagnostic tests (Pﬂug, 1983; Yaida, 2018; Lang et al.,
2019; Pesme et al., 2020) study the behaviour of the
SGD dynamics to automatically control its step-size.
Similarly, designing the adaptive termination test re-
quires characterizing the behaviour of AdaGrad used
in the inner loop of AdaSVRG.

We ﬁrst investigate the dynamics of constant step-size
AdaGrad in the stochastic setting. Speciﬁcally, we moni-
tor the evolution of (cid:107)Gt(cid:107)∗ = (cid:112)Tr(Gt) across iterations.
Ei(cid:107)∇fi(x) − ∇f (x)(cid:107)2 as a uni-
We deﬁne σ2 := supx∈X
form upper-bound on the variance in the stochastic
gradients for all iterates. We prove the following the-
orem showing that there exists an iteration T0 when
the evolution of (cid:107)Gt(cid:107)∗ undergoes a phase transition.

Theorem 4 (Phase Transition in AdaGrad Dynam-
ics). Under the same assumptions as Lemma 1 and
(iv) σ2-bounded stochastic gradient variance and deﬁn-
ing T0 = ρ2Lmax
, for constant step-size AdaGrad we
have E(cid:107)Gt(cid:107)∗ = O(1) for t ≤ T0, and E(cid:107)Gt(cid:107)∗ =
O(

t − T0) for t ≥ T0.

√

σ2

Theorem 4 (proved in Appendix I) indicates that the
norm (cid:107)Gt(cid:107)∗ is bounded by a constant for all t ≤ T0,
implying that its rate of growth is slower than log(t).
This implies that the step-size of AdaGrad is approx-
imately constant (similar to gradient descent in the
full-batch setting) in this ﬁrst phase until iteration T0.
Indeed, if σ = 0, T0 = ∞ and AdaGrad is always in
this deterministic phase. This result generalizes (Qian
and Qian, 2019, Theorem 3.1) that analyzes the diag-
onal variant of AdaGrad in the deterministic setting.
After iteration T0, the noise σ2 starts to dominate, and
AdaGrad transitions into the stochastic phase where
(cid:107)Gt(cid:107)∗ grows as O(
t). In this phase, the step-size de-
√
creases as O(1/
t), resulting in slower convergence
to the minimizer. AdaGrad thus results in an overall
T (cid:1) rate (Levy et al., 2018), where the ﬁrst
O (cid:0)1/T + σ2/

√

√

SVRG meets AdaGrad

term corresponds to the deterministic phase and the
second to the stochastic phase.

Using the fact that for AdaGrad in the inner-loop of
AdaSVRG, the term f (wk) − f ∗ behaves as the “noise”
similar to σ2, we prove Corollary 2 that shows that
the same phase transition as Theorem 4 happens at
iteration T0 := ρ2Lmax
f (wk)−f ∗ . By detecting this phase tran-
sition, AdaSVRG can terminate its inner-loop after T0
iterations. This is exactly the behaviour we want to
replicate in order to avoid the slowdown due to the
(cid:112)f (wk) − f ∗ term and is similar to the ideal adap-
tive termination required by Theorem 3. Putting these
results together, we conclude that if AdaSVRG termi-
nates each inner-loop according to a diagnostic test that
can exactly detect the phase transition in the growth
of (cid:107)GT (cid:107)∗, the resulting AdaSVRG variant will have an
O((n + 1
(cid:15) )) gradient complexity.
Since the exact detection of this phase transition is not
possible, we design a heuristic to detect it without re-
quiring the knowledge of problem-dependent constants.

(cid:15) ) log( 1

4.2 Heuristic for adaptive termination

Algorithm 3 AdaSVRG with adaptive termination
test
Input: w0 (initial point), K (outer-loops), θ (adaptive
termination parameter), M (maximum inner-loops)
for k ← 0 to K − 1 do

Compute full gradient ∇f (wk)
ηk ← Compute step-size
Initialize AdaGrad: x1 = wk and G0 = 0.
for t ← 1 to M do

Sample it ∼ Uniform{1, 2, . . . n}
gt = ∇fit(xt) − ∇fit(wk) + ∇f (wk)
Gt = Gt−1 + (cid:107)gt(cid:107)2
At = G1/2
if t mod 2 == 0 and t ≥ n then
∗−(cid:107)Gt/2(cid:107)2
R = (cid:107)Gt(cid:107)2
(cid:107)Gt/2(cid:107)2
∗
if R ≥ θ then

t

∗

Terminate inner loop

end

end
xt+1 = ΠX,At

(cid:0)xt − ηkA−1

t gt

(cid:1)

end
wk+1 = 1
mk

(cid:80)mk

t=1 xt

end
Return wK

Similar to tests used to detect stalling for SGD (Pﬂug,
1983; Pesme et al., 2020), the proposed diagnostic test
has a burn-in phase of n/2 inner-loop iterations that
allows the initial AdaGrad dynamics to stabilize. After
this burn-in phase, for every even iteration, we com-

∗

pute the ratio R = (cid:107)Gt(cid:107)2
∗−(cid:107)Gt/2(cid:107)2
. Given a threshold
(cid:107)Gt/2(cid:107)2
∗
hyper-parameter θ, the test terminates the inner-loop
when R ≥ θ. In the ﬁrst deterministic phase, since the
growth of (cid:107)Gt(cid:107)2
∗ is slow, (cid:107)G2t(cid:107)2
∗ ≈ (cid:107)Gt(cid:107)2
∗ and R ≈ 0.
In the stochastic phase, (cid:107)Gt(cid:107)2
∗ = O(t), and R ≈ 1,
justifying that the test can distinguish between the
two phases. AdaSVRG with this test is fully speciﬁed
in Algorithm 3. Experimentally, we use θ = 0.5 to give
an early indication of the phase transition.1

5 Experiments

We ﬁrst describe the practical considerations for imple-
menting AdaSVRG and then evaluate its performance
on real and synthetic datasets. We do not use projec-
tions in our experiments as these problems have an
unconstrained w∗ with ﬁnite norm (we thus assume D
is big enough to include it), and that we empirically
observed that our iterates always stayed bounded, thus
not requiring any projection.2

Implementing AdaSVRG: Though our theoretical
results hold for any bounded sequence of step-sizes,
its choice aﬀects the practical performance of Ada-
Grad (Vaswani et al., 2020) (and hence AdaSVRG).
Theoretically, the optimal step-size minimizing the
bound in Lemma 1 is given by η∗ = D√
. Since we
2
do not have access to D, we use the following heuristic
to set the step-size for each outer-loop of AdaSVRG.
In outer-loop k, we approximate D by (cid:107)wk − w∗(cid:107), that
can be bounded using the co-coercivity of smooth con-
vex functions as (cid:107)wk − w∗(cid:107) ≥ 1/Lmax(cid:107)∇f (wk)(cid:107) (Nes-
terov, 2004, Thm. 2.1.5 (2.1.8)). We have access to
∇f (wk) for the current outer-loop, and store the value
of ∇f (wk−1) in order to approximate the smooth-
ness constant. Speciﬁcally, by co-coercivity, Lmax ≥
Lk := (cid:107)∇f (wk)−∇f (wk−1)(cid:107)
. Putting these together, ηk =
3. Although a similar heuristic could be
√
used to estimate Lmax for SVRG or SARAH, the re-
sulting step-size is larger than 1/Lmax implying that
it would not have any theoretical guarantee, while
our results hold for any bounded sequence of step-
sizes. Although Algorithm 1 requires setting wk+1 to
be the average of the inner-loop iterates, we use the
last-iterate and set wk = xmk , as this is a more common
choice (Johnson and Zhang, 2013; Tan et al., 2016) and

(cid:107)∇f (wk)(cid:107)
2 maxi=0,...,k Li

(cid:107)wk−wk−1(cid:107)

1We note that SARAH (Nguyen et al., 2017) also sug-
gests a heuristic for adaptively terminating the inner-loop.
However, their test is not backed by any theoretical insight.
2We note in passing that the literature for unconstrained
stochastic optimization often explicitly assumes that the
iterates stay bounded (Ahn et al., 2020; Bollapragada et al.,
2019; Babanezhad Harikandeh et al., 2015).

3For k = 0, we compute the full gradient at a random

point w−1 and approximate L0 in the same way.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

(a) Logistic loss

Figure 1: Comparison of AdaSVRG against SVRG variants, SVRG-BB and SARAH with batch-size = 64 for
logistic loss (top 2 rows) and Huber loss (bottom 2 rows). For both losses, we compare AdaSVRG against the
best-tuned variants, and show the sensitivity to step-size (we limit the gradient norm to a maximum value of 10).

(b) Huber loss

results in better empirical performance. We compare
two variants of AdaSVRG, with (i) ﬁxed-size inner-
loop Algorithm 1 and (ii) adaptive termination Algo-
rithm 3. We handle a general batch-size b, and set
m = n/b for Algorithm 1. This is a common practical
choice (Babanezhad Harikandeh et al., 2015; Gower
et al., 2020; Kovalev et al., 2020). For Algorithm 3, the
burn-in phase consists of n/2b iterations and M = 10n/b.

Evaluating AdaSVRG: In order to assess the eﬀec-

tiveness of AdaSVRG, we experiment with binary clas-
siﬁcation on standard LIBSVM datasets (Chang and
Lin, 2011). In particular, we consider (cid:96)2-regularized
problems (with regularization set to 1/n) with three
losses - logistic loss, the squared loss or the Huber loss.
For each experiment we plot the median and standard
deviation across 5 independent runs. In the main paper,
we show the results for four of the datasets and relegate
the results for the three others to Appendix L. Simi-

SVRG meets AdaGrad

Figure 2: Comparison of AdaSVRG against SVRG variants, SVRG-BB and SARAH with batch-size = 64 for
squared loss We compare AdaSVRG against the best-tuned variants, and show the sensitivity to step-size (we
limit the gradient norm to a maximum value of 10). In cases where SVRG-BB diverged, we remove these curves.

(a) Squared loss

larly, we consider batch-sizes in the range [1, 8, 64, 128],
but only show the results for b = 64 in the main paper.

compare

the AdaSVRG variants
2013),

against
We
loopless-
SVRG (Johnson and Zhang,
SVRG (Kovalev et al., 2020), SARAH (Nguyen
et al., 2017), and SVRG-BB (Tan et al., 2016), the
only other tune-free VR method.4 Since each of these
methods requires a step-size, we search over the
grid [10−3, 10−2, 10−1, 1, 10, 100], and select the best
step-size for each algorithm and each experiment.
As is common, we set m = n/b for each of these
methods. We note that though the theoretical results
of SVRG-BB require a small O(1/κ2) step-size and
O(κ2) inner-loops, Tan et al. (2016) recommends
setting m = O(n) in practice. Since AdaGrad results
in the slower O(1/(cid:15)2) rate (Levy et al., 2018; Vaswani
et al., 2020) compared to the O(n + 1
(cid:15) ) rate of VR
methods, we do not include it in the main paper. We
demonstrate the poor performance of AdaGrad on two
example datasets in Fig. 4 in Appendix L.

We plot the gradient norm of the training objective
(for the best step-size) against the number of gradient
evaluations normalized by the number of examples.
We show the results for the logistic loss (Fig. 1(a)),
Huber loss (Fig. 1(b)), and squared loss (Fig. 2(a)).
Our results show that (i) both variants of AdaSVRG

4We do not compare against SAG (Schmidt et al., 2017)

because of its large memory footprint.

(without any step-size tuning) are competitive with the
other best-tuned VR methods, often out-performing
them or matching their performance; (ii) SVRG-BB
often has an oscillatory behavior, even for the best
step-size; and (iii) the performance of AdaSVRG with
adaptive termination (that has superior theoretical
complexity) is competitive with that of the practically
useful O(n) ﬁxed inner-loop setting.

In order to evaluate the eﬀect of the step-size on a
method’s performance, we plot the gradient norm after
50 outer-loops vs step-size for each of the compet-
ing methods. For the AdaSVRG variants, we set the
step-size according to the heuristic described earlier.
For the logistic loss (Fig. 1(a)), Huber loss (Fig. 1(b))
and squared loss (Fig. 2(a)), we observe that (i) the
performance of typical VR methods heavily depends
on the choice of the step-size; (ii) the step-size cor-
responding to the minimum loss is diﬀerent for each
method, loss and dataset; and (iii) AdaSVRG with the
step-size heuristic results in competitive performance.
Additional experiments in Appendix L conﬁrm that
the good performance of AdaSVRG is consistent across
losses, batch-sizes and datasets.

6 Discussion

Although there have been numerous papers on VR
methods in the past ten years, all of the provably conver-
gent methods require knowledge of problem-dependent

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

constants such as L. On the other hand, there has
been substantial progress in designing adaptive gra-
dient methods that have eﬀectively replaced SGD for
training ML models. Unfortunately, this progress has
not been leveraged for developing better VR methods.
Our work is the ﬁrst to marry these lines of litera-
ture by designing AdaSVRG, that achieves a gradient
complexity comparable to typical VR methods, but
without needing to know the objective’s smoothness
constant. Our results illustrate that it is possible to de-
sign principled techniques that can “painlessly” reduce
the variance, achieving good theoretical and practical
performance. We believe that our paper will help open
up an exciting research direction. In the future, we aim
to extend our theory to the strongly-convex setting.

7 Acknowledgments

We would like to thank Raghu Bollapragada for help-
ful discussions. This research was partially supported
by the Canada CIFAR AI Chair Program, a Google
Focused Research award and an IVADO postdoctoral
scholarship. Simon Lacoste-Julien is a CIFAR Associate
Fellow in the Learning in Machines & Brains program.

References

Ahn, K., Yun, C., and Sra, S. (2020). SGD with shuf-
ﬂing: optimal rates without component convexity
and large epoch requirements. In Neural Informa-
tion Processing Systems 2020, NeurIPS 2020.

Allen-Zhu, Z. (2017). Katyusha: The ﬁrst direct acceler-
ation of stochastic gradient methods. In Proceedings
of the 49th Annual ACM SIGACT Symposium on
Theory of Computing, STOC.

Armijo, L. (1966). Minimization of functions having
lipschitz continuous ﬁrst partial derivatives. Paciﬁc
Journal of mathematics, 16(1):1–3.

Babanezhad Harikandeh, R., Ahmed, M. O., Virani, A.,
Schmidt, M., Konečn`y, J., and Sallinen, S. (2015).
Stop wasting my gradients: Practical SVRG. Ad-
vances in Neural Information Processing Systems,
28:2251–2259.

Barzilai, J. and Borwein, J. M. (1988). Two-point step
size gradient methods. IMA journal of numerical
analysis, 8(1):141–148.

Belkin, M., Rakhlin, A., and Tsybakov, A. B. (2019).
Does data interpolation contradict statistical opti-
mality? In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pages 1611–1619.
PMLR.

Bollapragada, R., Byrd, R. H., and Nocedal, J. (2019).
Exact and inexact subsampled newton methods for

optimization. IMA Journal of Numerical Analysis,
39(2):545–578.

Boucheron, S., Bousquet, O., and Lugosi, G. (2005).
Theory of classiﬁcation: A survey of some recent
advances. ESAIM: probability and statistics, 9:323–
375.

Chang, C.-C. and Lin, C.-J. (2011). LIBSVM: A li-
brary for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(3):1–
27. Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.

Cutkosky, A. and Boahen, K. (2017). Online convex
optimization with unconstrained domains and losses.
arXiv preprint arXiv:1703.02622.

Cutkosky, A. and Orabona, F. (2019). Momentum-
based variance reduction in non-convex SGD. arXiv
preprint arXiv:1905.10018.

Defazio, A., Bach, F., and Lacoste-Julien, S. (2014).
SAGA: A fast incremental gradient method with sup-
port for non-strongly convex composite objectives. In
Advances in Neural Information Processing Systems,
NeurIPS.

Defazio, A. and Bottou, L. (2019). On the ineﬀec-
tiveness of variance reduced optimization for deep
learning. In Advances in Neural Information Pro-
cessing Systems, NeurIPS.

Duchi, J. C., Hazan, E., and Singer, Y. (2011). Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Gower, R. M., Schmidt, M., Bach, F., and Richtarik,
P. (2020). Variance-reduced methods for machine
learning. Proceedings of the IEEE, 108(11):1968–
1983.

Hofmann, T., Lucchi, A., Lacoste-Julien, S., and
McWilliams, B. (2015). Variance reduced stochastic
gradient descent with neighbors. Advances in Neural
Information Processing Systems, 28:2305–2313.

Johnson, R. and Zhang, T. (2013). Accelerating stochas-
tic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing
Systems, NeurIPS.

Konečn`y, J. and Richtárik, P.

Semi-
stochastic gradient descent methods. arXiv preprint
arXiv:1312.1666.

(2013).

Kovalev, D., Horváth, S., and Richtárik, P. (2020).
Don’t jump through hoops and remove those loops:
SVRG and Katyusha are better without the outer
loop. In Algorithmic Learning Theory, pages 451–467.
PMLR.

SVRG meets AdaGrad

Lan, G., Li, Z., and Zhou, Y. (2019). A uniﬁed variance-
reduced accelerated gradient method for convex op-
timization. In Advances in Neural Information Pro-
cessing Systems, pages 10462–10472.

Pesme, S., Dieuleveut, A., and Flammarion, N.
(2020). On convergence-diagnostic based step sizes
for stochastic gradient descent.
arXiv preprint
arXiv:2007.00534.

Lang, H., Xiao, L., and Zhang, P. (2019). Using statis-
tics to automate stochastic optimization.
In Ad-
vances in Neural Information Processing Systems,
pages 9540–9550.

Levy, K. Y., Yurtsever, A., and Cevher, V. (2018).
Online adaptive methods, universality and accelera-
tion. In Advances in Neural Information Processing
Systems, NeurIPS.

Li, B., Wang, L., and Giannakis, G. B. (2020). Al-
most tune-free variance reduction. In International
Conference on Machine Learning, pages 5969–5978.
PMLR.

Liang, T., Rakhlin, A., et al. (2020). Just interpolate:
Kernel “ridgeless” regression can generalize. Annals
of Statistics, 48(3):1329–1347.

Liu, M., Zhang, W., Orabona, F., and Yang, T. (2020).
Adam+: A stochastic method with adaptive variance
reduction. arXiv preprint arXiv:2011.11985.

Loizou, N., Vaswani, S., Laradji, I., and Lacoste-Julien,
S. (2020). Stochastic Polyak step-size for SGD: An
adaptive learning rate for fast convergence. arXiv
preprint:2002.10542.

Ma, S., Bassily, R., and Belkin, M. (2018). The power
of interpolation: Understanding the eﬀectiveness of
SGD in modern over-parametrized learning. In Pro-
ceedings of the 35th International Conference on Ma-
chine Learning, ICML.

Mahdavi, M.

and Jin, R.

Mixed-
Grad: An O(1/T) convergence rate algorithm for
stochastic smooth optimization.
arXiv preprint
arXiv:1307.7192.

(2013).

Mairal, J. (2013). Optimization with ﬁrst-order sur-
In International Conference on

rogate functions.
Machine Learning, pages 783–791.

Meng, S. Y., Vaswani, S., Laradji, I., Schmidt, M.,
and Lacoste-Julien, S. (2020). Fast and furious con-
vergence: Stochastic second order methods under
interpolation. In The 23nd International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS.

Nesterov, Y. (2004). Introductory lectures on convex
optimization: A basic course. Springer Science &
Business Media.

Nguyen, L. M., Liu, J., Scheinberg, K., and Takáč,
M. (2017). SARAH: a novel method for machine
learning problems using stochastic recursive gradient.
In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 2613–2621.

Pﬂug, G. C. (1983). On the determination of the step

size in stochastic quasigradient methods.

Qian, Q. and Qian, X. (2019). The implicit bias
arXiv preprint

of adagrad on separable data.
arXiv:1906.03559.

Reddi, S. J., Hefny, A., Sra, S., Poczos, B., and Smola,
A. (2016). Stochastic variance reduction for non-
convex optimization. In International conference on
machine learning, pages 314–323.

Reddi, S. J., Kale, S., and Kumar, S. (2018). On the
convergence of Adam and Beyond. In International
Conference on Learning Representations.

Schmidt, M., Babanezhad, R., Ahmed, M., Defazio,
A., Clifton, A., and Sarkar, A. (2015). Non-uniform
stochastic average gradient method for training condi-
tional random ﬁelds. In Proceedings of the Eighteenth
International Conference on Artiﬁcial Intelligence
and Statistics, AISTATS.

Schmidt, M. and Le Roux, N. (2013). Fast convergence
of stochastic gradient descent under a strong growth
condition. arXiv preprint:1308.6370.

Schmidt, M., Le Roux, N., and Bach, F. (2017). Mini-
mizing ﬁnite sums with the stochastic average gradi-
ent. Mathematical Programming, 162(1-2):83–112.

Sebbouh, O., Gazagnadou, N., Jelassi, S., Bach, F., and
Gower, R. (2019). Towards closing the gap between
the theory and practice of SVRG. In Advances in
Neural Information Processing Systems, pages 648–
658.

Shalev-Shwartz, S. and Zhang, T. (2013). Stochastic
dual coordinate ascent methods for regularized loss
minimization. Journal of Machine Learning Research,
14(Feb):567–599.

Song, C., Jiang, Y., and Ma, Y. (2020). Variance
reduction via accelerated dual averaging for ﬁnite-
sum optimization. Advances in Neural Information
Processing Systems, 33.

Sridharan, K., Shalev-Shwartz, S., and Srebro, N.
(2008). Fast rates for regularized objectives. Advances
in neural information processing systems, 21:1545–
1552.

Tan, C., Ma, S., Dai, Y.-H., and Qian, Y. (2016).
Barzilai-Borwein step size for stochastic gradient
descent. arXiv preprint arXiv:1605.04131.

Vaswani, S., Bach, F., and Schmidt, M. (2019a).
Fast and faster convergence of SGD for over-
parameterized models and an accelerated perceptron.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS.

Vaswani, S., Kunstner, F., Laradji, I., Meng, S. Y.,
Schmidt, M., and Lacoste-Julien, S. (2020). Adap-
tive gradient methods converge faster with over-
parameterization (and you can do a line-search).
arXiv preprint arXiv:2006.06835.

Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M.,
Gidel, G., and Lacoste-Julien, S. (2019b). Painless
stochastic gradient: Interpolation, line-search, and
convergence rates. In Advances in Neural Informa-
tion Processing Systems, NeurIPS.

Ward, R., Wu, X., and Bottou, L. (2019). AdaGrad
stepsizes: Sharp convergence over nonconvex land-
scapes, from any initialization. In Proceedings of the
36th International Conference on Machine Learning,
ICML.

Yaida, S. (2018). Fluctuation-dissipation relations
arXiv preprint

for stochastic gradient descent.
arXiv:1810.00004.

Zhang, C., Bengio, S., Hardt, M., Recht, B., and
Vinyals, O. (2017). Understanding deep learning re-
quires rethinking generalization. In 5th International
Conference on Learning Representations, ICLR.

SVRG meets AdaGrad

Supplementary material

Organization of the Appendix

A Deﬁnitions

B Heuristic for adaptivity to over-parameterization

D Proof of Lemma 1

E Main Proposition

F Proof of Theorem 1

G Proof of Theorem 2

H Proof of Theorem 3

I Proof of Theorem 4

J Helper Lemmas

K Counter-example for line-search for SVRG

L Additional Experiments

A Deﬁnitions

Our main assumptions are that each individual function fi is diﬀerentiable and Li-smooth, meaning that for all v
and w,

fi(v) ≤ fi(w) + (cid:104)∇fi(w), v − w(cid:105) +

Li
2

(cid:107)v − w(cid:107)2 ,

(Individual Smoothness)

which also implies that f Lmax-smooth, where Lmax is the maximum smoothness constant of the individual
functions. We also assume that f is convex, meaning that for all v and w,

f (v) ≥ f (w) − (cid:104)∇f (w), w − v(cid:105).

(Convexity)

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

B Heuristic for adaptivity to over-parameterization

Algorithm 4 Hybrid AdaGrad-AdaSVRG
Input: x1 (initial point), T (iteration budget), M (maximum inner loops), θ (adaptive termination parameter)
Phase 1: AdaGrad
G0 ← 0
for t ← 1 to T do

Sample it ∼ Uniform{1, 2, . . . n}
ηt ← Compute step-size
Gt = Gt−1 + (cid:107)gt(cid:107)2
At = G1/2
if t mod 2 == 0 and t ≥ 2n then
R = (cid:107)Gt(cid:107)2
∗−(cid:107)Gt/2(cid:107)2
(cid:107)Gt/2(cid:107)2
∗
if R ≥ θ then

t

∗

Terminate and switch to AdaSVRG

end

end
xt+1 = ΠX,At

(cid:0)xt − ηkA−1

t gt

(cid:1)

end
Phase 2: AdaSVRG
wK = Algorithm 3(xt, (cid:98) T −t
Return wK

n (cid:99), θ, M )

In this section, we reason that the poor empirical performance of SVRG when training over-parameterized
models (Defazio and Bottou, 2019) can be partially explained by the interpolation property (Schmidt and
Le Roux, 2013; Ma et al., 2018; Vaswani et al., 2019a) satisﬁed by these models (Zhang et al., 2017). In particular,
we focus on smooth convex losses, but assume that the model is capable of completely ﬁtting the training data,
and that w∗ lies in the interior of X. For example, these properties are simultaneously satisﬁed when minimizing
the squared hinge-loss for linear classiﬁcation on separable data or unregularized kernel regression (Belkin et al.,
2019; Liang et al., 2020) with (cid:107)w∗(cid:107) ≤ 1.

Formally, the interpolation condition means that the gradient of each fi in the ﬁnite-sum converges to zero at
an optimum. Additionally, we assume that each function fi has ﬁnite minimum f ∗
i . If the overall objective f is
minimized at w∗, ∇f (w∗) = 0, then for all fi we have ∇fi(w∗) = 0. Since the interpolation property is rarely
exactly satisﬁed in practice, we allow for a weaker version that uses ζ 2 := Ei[f ∗ − f ∗
i ] ∈ [0, ∞) (Loizou et al.,
2020; Vaswani et al., 2020) to measure the extent of the violation of interpolation. If ζ 2 = 0, interpolation is
exactly satisﬁed.

When ζ 2 = 0, both constant step-size SGD and AdaGrad have a gradient complexity of O(1/(cid:15)) in the smooth
convex setting (Schmidt and Le Roux, 2013; Vaswani et al., 2019a, 2020). In contrast, typical VR methods have
an ˜O(n + 1
(cid:15) ) complexity. For example, both SVRG and AdaSVRG require computing the full gradient in every
outer-loop, and will thus unavoidably suﬀer an Ω(n) cost. For large n, typical VR methods will thus be necessarily
slower than SGD when training models that can exactly interpolate the data. This provides a partial explanation
for the ineﬀectiveness of VR methods when training over-parameterized models. When ζ 2 > 0, AdaGrad has an
O(1/(cid:15) + ζ/(cid:15)2) rate (Vaswani et al., 2020). Here ζ, the violation of interpolation plays the role of noise and slows
down the convergence to an O(1/(cid:15)2) rate. On the other hand, AdaSVRG results in an ˜O(n + 1/(cid:15)) rate, regardless
of ζ.

Following the reasoning in Section 4, if an algorithm can detect the slower convergence of AdaGrad and switch
from AdaGrad to AdaSVRG, it can attain a faster convergence rate. It is straightforward to show that AdaGrad
has a a similar phase transition as Theorem 4 when interpolation is only approximately satisﬁed. This enables
the use of the test in Section 4 to terminate AdaGrad and switch to AdaSVRG, resulting in the hybrid algorithm
described in Algorithm 4. If the diagnostic test can detect the phase transition accurately, Algorithm 4 will attain
an O(1/(cid:15)) convergence when interpolation is exactly satisﬁed (no switching in this case). When interpolation is
only approximately satisﬁed, it will result in an O(1/(cid:15)) convergence for (cid:15) ≥ ζ (corresponding to the AdaGrad rate

SVRG meets AdaGrad

in the deterministic phase) and will attain an O(1/ζ 2 + ((n + 1/(cid:15)) log(ζ/(cid:15))) convergence thereafter (corresponding
to the AdaSVRG rate). This implies that Algorithm 4 can indeed obtain the best of both worlds between AdaGrad
and AdaSVRG.

Figure 3: Comparison of AdaGrad, AdaSVRG and Algorithm 4 (denoted "Hybrid" in the plots) with logistic loss
and batch-size 64 on datasets with diﬀerent fraction of mislabeled data-points. Interpolation is exactly satisﬁed
for the left-most plot.

Evaluating Algorithm 4: We use synthetic experiments to demonstrate the eﬀect of interpolation on the
convergence of stochastic and VR methods. Following the protocol in (Meng et al., 2020), we generate a linearly
separable dataset with n = 104 data points of dimension d = 200 and train a linear model with a convex loss.
This setup ensures that interpolation is satisﬁed, but allows to eliminate other confounding factors such as
non-convexity and other implementation details. In order to smoothly violate interpolation, we show results with
a mislabel fraction of points in the grid [0, 0.1, 0.2].

We use AdaGrad as a representative (fully) stochastic method, and to eliminate possible confounding because
of its step-size, we set it using the stochastic line-search procedure (Vaswani et al., 2020). We compare the
performance of AdaGrad, SVRG, AdaSVRG and the hybrid AdaGrad-AdaSVRG (Algorithm 4) each with a
budget of 50 epochs (passes over the data). For SVRG, as before, we choose the best step-size via a grid-search.
For AdaSVRG, we use the ﬁxed-size inner-loop variant and the step-size heuristic described earlier. In order to
evaluate the quality of the “switching” metric in Algorithm 4, we compare against a hybrid method referred to as
“Optimal Manual Switching” in the plots. This method runs a grid-search over switching points - after epoch
{1, 2, . . . , 50} and chooses the point that results in the minimum loss after 50 epochs.

In Fig. 3, we plot the results for the logistic loss using a batch-size of 64 (refer to Appendix L for other losses
and batch-sizes). We observe that (i) when interpolation is exactly satisﬁed (no mislabeling), AdaGrad results in
superior performance over SVRG and AdaSVRG, conﬁrming the theory in Appendix B. In this case, both the
optimal manual switching and Algorithm 4 do not switch; (ii) when interpolation is not exactly satisﬁed (with
10%, 20% mislabeling), the AdaGrad progress slows down to a stall in a neighbourhood of the solution, whereas
both SVRG and AdaSVRG converge to the solution; (iii) in both cases, Algorithm 4 detects the slowdown in
AdaGrad and switches to AdaSVRG, resulting in competitive performance with the optimal manual switching.
For all three datasets, Algorithm 4 matches or out-performs the better of AdaGrad and AdaSVRG, showing that
it can achieve the best-of-both-worlds.

In Appendix L, we evaluate the performance of these methods for binary classiﬁcation with kernel mappings
on the mushrooms and ijcnn datasets. This experimental setup can also simultaneously ensure convexity and
interpolation (Vaswani et al., 2019b), and we observe the same favourable behaviour of the proposed hybrid
algorithm.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

C Algorithm in general case

We restate Algorithm 1 to handle the full matrix and diagonal variants. The only diﬀerence is in the initialization
and update of Gt.

Algorithm 5 AdaSVRG with ﬁxed-sized inner-loop
Input: w0 (initial point), K (outer-loops), m (inner-loops)
for k ← 0 to K − 1 do

Compute full gradient ∇f (wk)
ηk ← Compute step-size

Initialize: x1 = wk and G0 =

for t ← 1 to m do

(cid:40)
0
δI

(scalar variant)
(full matrix and diagonal variants)

.

Sample it ∼ Uniform{1, 2, . . . n}
gt = ∇fit(xt) − ∇fit(wk) + ∇f (wk)
(cid:107)gt(cid:107)2
diag (cid:0)gtg(cid:62)
gtg(cid:62)
t

Gt = Gt−1 +






(cid:1)

t

(scalar variant)
(diagonal variant)
(full matrix variant)

At = G1/2
t
xt+1 = ΠX,At

(cid:0)xt − ηkA−1

t gt

(cid:1)

end
wk+1 = 1
m

(cid:80)m

t=1 xt

end
Return ¯wK = 1
K

(cid:80)K

k=1 wk

D Proof of Lemma 1

We restate Lemma 1 to handle the three variants of AdaSVRG.

Lemma 2 (AdaSVRG with single outer-loop). Assuming (i) convexity of f , (ii) Lmax-smoothness of fi and
(iii) bounded feasible set with diameter D. For the scalar variant, deﬁning ρ := (cid:0) D2
Lmax, for any
ηk
outer loop k of AdaSVRG, with (a) inner-loop length mk and (b) step-size ηk,
ρ(cid:112)E[f (wk) − f ∗]
√
mk

E[f (wk+1) − f ∗] ≤

ρ2
mk

+ 2ηk

(cid:1)√

+

For the full matrix and diagonal variants, setting ρ(cid:48) :=

(cid:16) D2
ηk

+ 2ηk

(cid:17) √

dLmax,

E[f (wk+1) − f ∗] ≤

(ρ(cid:48))2 + (cid:112)dδ/4Lmax
mk

+

ρ(cid:48)(cid:112)E[f (wk) − f ∗]
√
mk

Proof. For any of the three variants, we have, for any outer loop iteration k and any inner loop iteration t,

(cid:107)xt+1 − w∗(cid:107)2
At

t gk) − PX,At(w∗)(cid:13)
2
(cid:13)
At

(cid:13)PX,At(xt − ηkA−1
(cid:13)xt − ηkA−1

= (cid:13)
≤ (cid:13)
= (cid:107)xt − w∗(cid:107)2
At

t gk − w∗(cid:13)
2
(cid:13)
At

− 2ηk(cid:104)gt, xt − w∗(cid:105) + η2

k (cid:107)gt(cid:107)2

A−1
t

(2)

(3)

(4)

where the inequality follows from Reddi et al. (2018, Lemma 4) Dividing by ηk, rearranging and summing over all

SVRG meets AdaGrad

inner loop iterations at stage k gives
mk(cid:88)

mk(cid:88)

(cid:104)gt, xt − w∗(cid:105) ≤

2

(cid:107)xt − w∗(cid:107)2

+

mk(cid:88)

t=1

−

At−1
ηk

ηk (cid:107)gt(cid:107)2

A−1
t

At
ηk

t=1

≤

=

t=1
D2
ηk
(cid:18) D2
ηk

Tr(Amk ) + 2ηkTr(Amk )

(cid:19)

+ 2ηk

Tr(Amk )

By Lemma 4, we have that Tr(Amk ) ≤
the full matrix and diagonal variants. Therefore we set

(cid:113)(cid:80)mk

t=1 (cid:107)gt(cid:107)2 in the scalar case, and Tr(Amk ) ≤

and

a(cid:48) =

(cid:40) 1
2
1
2

(cid:0) D2
ηk
(cid:0) D2
ηk

(cid:1)
(cid:1)√

+ 2ηk
+ 2ηk

(scalar variant)

d (full matrix and diagonal variants)

b =

(cid:40)
0
dδ

(scalar variant)
(full matrix and diagonal variants)

Going back to the above inequality and taking expectation we get

mk(cid:88)

(cid:104)∇f (xt), xt − w∗(cid:105) ≤ a(cid:48) E





(cid:118)
(cid:117)
(cid:117)
(cid:116)

mk(cid:88)


(cid:107)gt(cid:107)2 + b


t=1

t=1

Using convexity of f yields

mk(cid:88)

t=1

(cid:20)
E[f (xt) − f ∗] ≤ a(cid:48)E

(cid:118)
(cid:117)
(cid:117)
(cid:116)

mk(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)E(cid:2)

≤ a(cid:48)

t=1

mk(cid:88)

t=1

(cid:21)

(cid:107)gt(cid:107)2 + b

(cid:107)gt(cid:107)2 + b(cid:3)

= a(cid:48)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

mk(cid:88)

t=1

E[(cid:107)gt(cid:107)2] + b

(5)

(Lemma 3 and Lemma 4)

(6)

√

(cid:113)(cid:80)mk
d

t=1 (cid:107)gt(cid:107)2 + dδ in

(7)

(8)

(9)

(10)

where the second inequality comes from Jensen’s inequality applied to the (concave) square root function. Now,
from Kovalev et al. (2020),

E[(cid:107)gt(cid:107)2] ≤ 4LmaxE[f (wk) − f ∗] + 4LmaxE[f (xt) − f ∗]

Going back to the previous equation, squaring and setting τ = a(cid:48)

√

4Lmax we get

(cid:33)2

E[f (xt) − f ∗]

≤ τ 2

(cid:32) mk(cid:88)

t=1

(cid:32) mk(cid:88)

t=1

E[f (xt) − f ∗] + mkE[f (wk) − f ∗] +

(cid:33)

b
4Lmax

(11)

(12)

Using Lemma 5,
mk(cid:88)

E[f (xt) − f ∗] ≤ τ 2 + τ

t=1

(cid:114)

mkE[f (wk) − f ∗] +

b
4Lmax

≤ τ 2 + τ

(cid:114) b

4Lmax

+ τ (cid:112)mkE[f (wk) − f ∗]

(13)

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

Finally, using Jensen’s inequality we get

E [f (wk+1) − f ∗] = E

(cid:34)
f

(cid:32)

mk(cid:88)

t=1

1
mk
(cid:113) b

4Lmax

(cid:33)

(cid:35)

xt

− f ∗

+

τ (cid:112)E[f (wk) − f ∗]
√
mk

(14)

(15)

τ 2 + τ

≤

mk

which concludes the proof by noticing that by deﬁnition τ = ρ in the scalar case and τ = ρ(cid:48) in the full matrix and
diagonal cases.

E Main Proposition

SVRG meets AdaGrad

We ﬁrst state the main proposition for the three variants of AdaSVRG, which we later use for proving theorems.
Proposition 2. Assuming (i) convexity of f (ii) Lmax-smoothness of f (iii) bounded feasible set (iv) ηk ∈
[ηmin, ηmax] (v) mk = m for all k, then for the scalar variant,

E[f ( ¯wK) − f ∗] ≤

√

5)

ρ2(1 +
m

+

ρ(cid:112)2 (f (w0) − f ∗)
mK

√

and for the full matrix and diagonal variants,

E[f ( ¯wK) − f ∗] ≤

(ρ(cid:48))2(1 +

√

5) + 2ρ(cid:48)(cid:112)dδ/Lmax

m

+

ρ(cid:48)(cid:112)2 (f (w0) − f ∗)
mK

√

where ¯wK = 1
K

(cid:80)K

k=1 wk, ρ =

(cid:16) D2
ηmin

(cid:17) √

+ 2ηmax

Lmax and ρ(cid:48) =

(cid:16) D2
ηmin

+ 2ηmax

(cid:17) √

dLmax.

Proof. As in the previous proof we deﬁne

and

b =

(cid:40)
0
dδ

τ =

(cid:40)

ρ
ρ(cid:48)

(scalar variant)
(full matrix and diagonal variants)

(scalar variant)
(full matrix and diagonal variants)

Using the result from Lemma 1 and letting ∆k := E[f (wk) − f ∗], we have,

∆k+1 ≤

τ 2 + τ

(cid:113) b

4Lmax

m

√
∆k√
m

+ τ

(16)

Squaring gives





∆2

k+1 ≤

τ 2 + τ

(cid:113) b

4Lmax

m

τ

√
∆k√
m

+

2




≤ 2

(cid:16)

τ 2 + τ

(cid:113) b

(cid:17)2

4Lmax

m2

+ 2

τ 2
m

∆k ≤ 4

τ 2
m2 + 4

τ 2

b
4Lmax

m2 + 2

τ 2
m

∆k

(17)

which we can rewrite as

Since ∆2

k+1 − 2 τ 2

∆2

k+1 − 2

τ 2
m
m )2 − τ 4
m ∆k+1 = (∆k+1 − τ 2
τ 2
m

(∆k+1 −

m2 , we get
τ 4
m2 + 2

)2 ≤ 5

∆k+1 ≤ 4

τ 4
m2 + 2

τ 2
m

(∆k − ∆k+1) + 4

τ 2b
Lmaxm2

τ 2
m

(∆k − ∆k+1) + 4

τ 2b
Lmaxm2

Summing this gives

K−1
(cid:88)

(cid:18)

k=0

∆k+1 −

(cid:19)2

τ 2
m

(cid:18)

5τ 4 + 4

(cid:18)

5τ 4 + 4

≤

≤

τ 2b
Lmax

τ 2b
Lmax

(cid:19) K

m2 + 2

(cid:19) K

m2 + 2

τ 2
m

τ 2
m

K−1
(cid:88)

(∆k − ∆k+1)

k=0

∆0

Using Jensen’s inequality on the (concave) square root function gives

1
K

K−1
(cid:88)

(∆k+1 −

k=0

τ 2
m

) ≤

1
K

K−1
(cid:88)

k=0

(cid:115)(cid:18)

∆k+1 −

(cid:19)2

≤

τ 2
m

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
K

K−1
(cid:88)

(cid:18)

k=0

∆k+1 −

(cid:19)2

τ 2
m

(18)

(19)

(20)

(21)

(22)

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

going back to the previous inequality this gives

1
K

K−1
(cid:88)

k=0

(∆k+1 −

τ 2
m

) ≤

(cid:115)(cid:18)

5τ 4 + 4

τ 2b
Lmax

√

τ 2

≤

5 + 2τ (cid:112)b/Lmax

m

mK

(cid:19) 1
m2 + 2τ 2 ∆0
√
2∆0√
τ
mK

+

which we can rewrite

1
K

K−1
(cid:88)

k=0

∆k+1 ≤

√

τ 2(1 +

5) + 2τ (cid:112)b/Lmax

m

√
2∆0√
τ
mK

+

Setting ¯wK = 1
K

(cid:80)K−1

k=0 wk and using Jensen’s inequality on the convex function f , we get
√

τ 2(1 +

5) + 2τ (cid:112)b/Lmax

m

√
2∆0√
τ
mK

+

E[f ( ¯wK) − f ∗] ≤

which concludes the proof.

F Proof of Theorem 1

(23)

(24)

(25)

(26)

For the remainder of the appendix we deﬁne ρ(cid:48) := (cid:0) D2
ηmin
all three variants of AdaSVRG.

(cid:1)√

+ 2ηmax

dLmax. We restate and prove Theorem 1 for

Theorem 5 (AdaSVRG with ﬁxed-size inner-loop). Under the same assumptions as Lemma 1, AdaSVRG
with (a) step-sizes ηk ∈ [ηmin, ηmax], (b) inner-loop size mk = n for all k, results in the following convergence
rate after K ≤ n iterations. For the scalar variant,

E[f ( ¯wK) − f ∗] ≤

√

ρ2(1 +

5) + ρ(cid:112)2 (f (w0) − f ∗)

K

and for the full matrix and diagonal variants,

E[f ( ¯wK) − f ∗] ≤

√

(ρ(cid:48))2(1 +

5) + 2ρ(cid:48)(cid:112)dδ/Lmax + ρ(cid:48)(cid:112)2 (f (w0) − f ∗)

K

,

where ¯wK = 1
K

(cid:80)K

k=1 wk.

Proof. We have 1

m = 1

n ≤ 1

K by the assumption. Using the result of Proposition 2 for the scalar variant we have
√
2∆0√
ρ
mK
√
2∆0
K

ρ2(1 +
m
ρ2(1 +
K

E[f ( ¯wK) − f ∗] ≤

(28)

(27)

5)

5)

√

√

+

+

≤

ρ

Using the result of Proposition 2 for the full matrix and diagonal variants we have

E[f ( ¯wK) − f ∗] ≤

(ρ(cid:48))2(1 +

(ρ(cid:48))2(1 +

≤

√

√

5) + 2ρ(cid:48)(cid:112)dδ/Lmax

m

5) + 2ρ(cid:48)(cid:112)dδ/Lmax

K

+

+

√
ρ(cid:48)
√

2∆0
mK
√
2∆0
K

ρ(cid:48)

(29)

(30)

Corollary 1. Under the assumptions of Theorem 1,
reach (cid:15)-accuracy is O (cid:0) n
√
(cid:15)
(ρ(cid:48))2(1+

(cid:1) when (cid:15) ≥
2(f (w0)−f ∗)

dδ/Lmax+ρ(cid:48)

5)+2ρ(cid:48)

ρ2(1+

5)+ρ

√

√

√

√

n

2(f (w0)−f ∗)

the computational complexity of AdaSVRG to

for the scalar variant and when (cid:15) ≥

n

for the full matrix and diagonal variants.

Proof. We deal with the scalar variant ﬁrst. Let c = ρ2(1 +
to reach (cid:15)-accuracy we require

√

5) + ρ(cid:112)2 (f (w0) − f ∗). By the previous theorem,

SVRG meets AdaGrad

c
K

≤ (cid:15)

(31)

(cid:1) outer loops to reach (cid:15)-accuracy. For mk = n, 3n gradients are computed in each outer

We thus require O (cid:0) 1
loop, thus the computational complexity is indeed O (cid:0) n
The condition (cid:15) ≥ c

n follows from the assumption that K ≤ n.

(cid:1).

(cid:15)

(cid:15)

√

5) + 2ρ(cid:48)(cid:112)dδ/Lmax +

The proof for the full matrix and diagonal variants is similar by taking c = (ρ(cid:48))2(1 +
ρ(cid:48)(cid:112)2 (f (w0) − f ∗).

G Proof of Theorem 2

Theorem 2 (Multi-stage AdaSVRG). Under the same assumptions as Theorem 1, multi-stage
AdaSVRG with I = log(1/(cid:15)) stages, K ≥ 3 outer-loops and mi = 2i+1 inner-loops at stage i, requires
O(n log 1
With the same assumptions as above, the full matrix and diagonal variants of multi-stage AdaSVRG
require O(n log 1
(cid:15)-sub-optimality

(cid:15) ) gradient evaluations to reach a (cid:0)ρ2(1 +
(cid:16)

5)(cid:1) (cid:15)-sub-optimality.

5) + ρ(cid:48)(cid:112)dδ/Lmax

(cid:15) ) gradient evaluations to reach a

(ρ(cid:48))2(1 +

(cid:15) + 1

(cid:15) + 1

√

√

(cid:17)

Proof. We deal with the scalar variant ﬁrst. Let ∆i := E[f ( ¯wi) − f ∗] and c := ρ2(1 +
and mi = 2i+1, as in the theorem statement. We claim that for all i, we have ∆i ≤ c
For i = 0, we have

5). Suppose that K ≥ 3
2i . We prove this by induction.

√

√

5)

c
20 = c = ρ2(1 +
√

= Lmax(1 +

5)

(cid:18) D2
η

(cid:19)2

+ 2η

The quantity D2

η + 2η reaches a minimum for η∗ = D√

2

. Therefore we can write

c
20 = Lmax(1 +
= Lmax(1 +

√

√

(cid:18)√

5)

2D + 2

(cid:19)2

D
√
2

5)8D2

√

√

√

≥ (1 +

≥ (1 +

= (1 +
≥ ∆0

5)8Lmax (cid:107) ¯w0 − w∗(cid:107)2
5)4 (f ( ¯w0) − f ∗)
5)4∆0

(32)

(33)

(34)

(35)

(36)

(smoothness)

(37)

(38)

Now suppose that ∆i−1 ≤ c

2i−1 for some i ≥ 1. Using the upper-bound analysis of AdaSVRG in Proposition 2 we

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

get

c
mi +
c
2i+1 +
c
2i+1 +
c
2i+1 +

ρ(cid:112)2∆i−1
√
miK
ρ(cid:112)2∆i−1
√
2i+1K

ρ(cid:112)2 c
2i−1
√
2i+1K
(cid:32)

c
2i+1

ρ
√
c

∆i ≤

=

≤

=

≤

(cid:33)

2i+1
2i−2K

(cid:114)

ρ

(cid:114) 8
K

c
2i+1 +

c
2i+1

=

c
2i+1 +

c
2i+1

(cid:113)

ρ2(1 +

√

5)

(cid:115)

8
√

5)K

(1 +

Since K ≥ 3, one can check that

8
√

(1+

5)K

≤ 1 and thus

∆i ≤

c
2i+1 +

c
2i+1 =

c
2i

(39)

(40)

(induction hypothesis)

(41)

(42)

(43)

(44)

which concludes the induction step.
At time step I = log 1
assume that K = γ for some constant γ ≥ 3, the gradient complexity is given by

(cid:15) , we thus have ∆I ≤ c

2I = c(cid:15). All that is left is to compute the gradient complexity. If we

I
(cid:88)

i=1

K(n + mi) =

I
(cid:88)

i=1

γ(n + 2i+1)

= γn log

≤ γn log

(cid:18)

(cid:19)

I
(cid:88)

2i+1

+ γ

(cid:18) 1
(cid:15)
(cid:18) 1
(cid:15)
1
(cid:15)

(cid:19)

+ 4γ

i=1
1
(cid:15)
(cid:19)

1
(cid:15)

= O

(n +

) log

which concludes the proof for the scalar variant.
Now we look at the full matrix and diagonal variants. Let’s take c = (ρ(cid:48))2(1 +
K ≥ 3 and mi = 2i+1, as in the theorem statement. Again, we claim that for all i, we have ∆i ≤ c
this by induction. For i = 0, we have

5) + 2ρ(cid:48)(cid:112)dδ/Lmax. Suppose that
2i . We prove

√

c
20 = c = (ρ(cid:48))2(1 +
√

≥ dLmax(1 +

5)

√

5) + 2ρ(cid:48)(cid:112)dδ/Lmax

(cid:18) D2
η

(cid:19)2

+ 2η

The quantity D2

η + 2η reaches a minimum for η∗ = D√

2

. Therefore we can write

c
20 ≥ dLmax(1 +
= dLmax(1 +

√

√

(cid:18)√

5)

2D + 2

(cid:19)2

D
√
2

5)8D2

√

√

√

5)8Lmax (cid:107) ¯w0 − w∗(cid:107)2
5)4 (f ( ¯w0) − f ∗)
5)4∆0

≥ d(1 +

≥ d(1 +

= d(1 +
≥ ∆0

(45)

(46)

(47)

(48)

(49)

(smoothness)

(50)

(51)

SVRG meets AdaGrad

The induction step is exactly the same as in the scalar case.

H Proof of Theorem 3

We restate and prove Theorem 3 for the three variants of AdaSVRG.

Theorem 6 (AdaSVRG with adaptive-sized inner-loops). Under the same assumptions as Lemma 1, AdaSVRG
with (a) step-sizes ηk ∈ [ηmin, ηmax], (b1) inner-loop size mk = ν
for outer-loop k, results in the following convergence rate,

(cid:15) for all k or (b2) inner-loop size mk =

ν
f (wk)−f ∗

E[f (wK) − f ∗] ≤ (3/4)K[f (w0) − f ∗].

where ν = 4ρ2 for the scalar variant and ν =

(cid:32)

(cid:113)

2ρ(cid:48)+

16(ρ(cid:48))2+12ρ(cid:48)

√

dδ/4Lmax

(cid:33)2

3

for the full matrix and diagonal

variants.

Proof. Let us deﬁne

and

α =

(cid:40) 1
2
1
2

(cid:0) D2
ηmin
(cid:0) D2
ηmin

+ 2ηmax
+ 2ηmax

(cid:1)
(cid:1)√

(scalar variant)

d (full matrix and diagonal variants)

b =

(cid:40)
0
dδ

(scalar variant)
(full matrix and diagonal variants)

Similar to the proof of Lemma 1, for a inner-loop k with mk iterations and α := 1
2

(cid:0) D2
ηmin

+ 2ηmax

(cid:1) we can show

(cid:35)

f (xt) − f ∗

≤ α

(cid:34) mk(cid:88)

E

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

mk(cid:88)

t=1

E

(cid:104)

(cid:107)gt(cid:107)2(cid:105)

+ b ≤ α

(cid:118)
(cid:117)
(cid:117)
(cid:116)4Lmax

mk(cid:88)

t=1

(cid:35)

E [f (xt) − f ∗] + 4Lmax

mk(cid:88)

t=1

E [f (wk) − f ∗]
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:15)k

+b (52)

(cid:118)
(cid:117)
(cid:117)
(cid:116)4LmaxE

≤ α

(cid:34) mk(cid:88)

t=1

f (xt) − f ∗

+ 4Lmaxmk(cid:15)k + b

Using Lemma 5 we get

If we set mk = C
(cid:15)k

,

(cid:34) mk(cid:88)

E

t=1

(cid:34) mk(cid:88)

E

t=1

(cid:35)

f (xt) − f ∗

≤ 4Lmaxα2 +

(cid:112)

4Lmaxα2mk(cid:15)k + α2b

(cid:35)

ft(xt) − f ∗

≤ 4Lmaxα2 +

(cid:112)

4Lmaxα2C + α2b

≤ 4Lmaxα2 +

(cid:112)

4Lmaxα2C +

√

α2b.

Deﬁne a :=
wk+1,

√

(cid:18)

4Lmaxα2 and γ := (cid:112)b/4Lmax, by dividing both sides of (55) by mk and using the deﬁnition of

E [f (wk+1) − f ∗] ≤

√

a2 + a(

C + γ)

mk

(cid:32)

=

√

a2 + a(

(cid:33)

C + γ)

C

E [f (wk) − f ∗]

(57)

√

2a+

4a2+12(a2+aγ)

(cid:19)2

Setting C =
, we get (cid:15)k+1 ≤ 3/4(cid:15)k. However, the above proof requires knowing (cid:15)k. Instead,
let us assume a target error of (cid:15), implying that we want to have E[f (wK) − f ∗] ≤ (cid:15). Going back to (54) and

3

(53)

(54)

(55)

(56)

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

setting mk = C/(cid:15), we obtain,

E [f (wk+1) − f ∗] ≤

a2 + a((cid:112)C(cid:15)k/(cid:15) + γ)
mk

=

(cid:32)

a2 + a((cid:112)C(cid:15)k/(cid:15) + γ)
C

(cid:33)

(cid:15)

(58)

a2(cid:15)k + a(cid:112)C(cid:15)k/(cid:15)
C
C + aγ

a2 + a

√

(cid:32)

≤

=

C

√

(cid:15)k(cid:15) + aγ(cid:15)k

(cid:33)

(cid:32)

(cid:15)k =

√

a2 + a

C + aγ

C

(Assuming that (cid:15) ≤ (cid:15)k for all k ≤ K.)
(cid:33)

E [f (wk) − f ∗]

(59)

With C =

(cid:18)

√

2a+

4a2+12(a2+aγ)

(cid:19)2

3

, we get linear convergence to (cid:15)-suboptimality. Based on the above we require

K = O(log(1/(cid:15))) outer-loops. However in each outer-loop we need O(n + 1
total computation complexity is of O((n + 1
The proof is done by noticing that in the scalar variant, γ = 0 and a = ρ so that

(cid:15) ) log( 1

(cid:15) )).

(cid:15) ) gradient evaluations. All in all, our

C =

(cid:32)

2ρ + (cid:112)4ρ2 + 12ρ2
3

(cid:33)2

= 4ρ2

and in the full matrix and diagonal variants, a = ρ(cid:48) and γ = (cid:112)dδ/4Lmax so that





C =

(cid:113)

2ρ(cid:48) +

16(ρ(cid:48))2 + 12ρ(cid:48)(cid:112)dδ/4Lmax

3

2




SVRG meets AdaGrad

I Proof of Theorem 4

We restate and prove Theorem 4 for the three variants of AdaGrad.

Theorem 7 (Phase Transition in AdaGrad Dynamics). Under the same assumptions as Lemma 1 and (iv)
σ2-bounded stochastic gradient variance and deﬁning T0 = ρ2Lmax
, for constant step-size AdaGrad we have
σ2
E(cid:107)Gt(cid:107)∗ = O(1) for t ≤ T0, and E(cid:107)Gt(cid:107)∗ = O(
t − T0) for t ≥ T0.
The same result holds for the full matrix and diagonal variants of constant step-size AdaGrad for T0 =
(cid:16)

√

√

(cid:17)2

√

ρ(cid:48)√

Lmax+

2ρ(cid:48)

dδLmax+dδ

σ2

Proof. We start with the scalar variant. Consider the general AdaGrad update

xt+1 = ΠX,At

(cid:0)xt − ηA−1

t ∇fit(xt)(cid:1)

The same we did in the proof of Theorem 1, we can bound suboptimality as

(cid:107)xt+1 − w∗(cid:107)2
At

≤ (cid:107)xt − w∗(cid:107)2
At

− 2η (cid:104)xt − w∗, ∇fit(xt)(cid:105) + η2(cid:107)∇fit(xt)(cid:107)2

A−1
t

By re-arranging, dividing by η and summing for T iteration we have

T
(cid:88)

t=1

(cid:104)xt − w∗, ∇fit(xt)(cid:105) ≤

1
2η

T
(cid:88)

(cid:107)xt − w∗(cid:107)2

At−At−1

+

t=1
(cid:18) D2
η

≤

1
2

(cid:19)

+ 2η

Tr(AT )

η
2

T
(cid:88)

t=1

(cid:107)∇fit(xt)(cid:107)2

A−1
t

Deﬁne

and

α =

(cid:40) 1
2
1
2

(cid:0) D2
(cid:0) D2

η + 2η(cid:1)
η + 2η(cid:1)√

(scalar variant)

d (full matrix and diagonal variants)

b =

(cid:40)
0
dδ

(scalar variant)
(full matrix and diagonal variants)

(60)

(61)

(Lemma 1)

(62)

We then have Tr(AT ) ≤ α
expectation and using the upper-bound we get

(cid:113)(cid:80)T

t=1 (cid:107)∇fit(xt)(cid:107)2 + b by Lemma 3 and Lemma 4. Going back to Eq. (62) and taking

(cid:104)xt − w∗, ∇f (xt)(cid:105)

≤ αE

(cid:35)

(cid:34) T

(cid:88)

E

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)





T
(cid:88)



(cid:107)∇fit(xt)(cid:107)2 + b



t=1

Using convexity of f and Jensen’s inequality on the (concave) square root function, we have

E(cid:107)∇fit(xt)(cid:107)2 + b

(cid:35)

f (xt) − f ∗

≤ α

(cid:34) T

(cid:88)

E

t=1

= α

= α

≤ α

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t=1

T
(cid:88)

t=1

E [(cid:107)∇fit(xt) − ∇f (xt)(cid:107)2 + 2(cid:104)∇fi1(xt) − ∇f (xt), ∇f (xt)(cid:105) + (cid:107)∇f (xt)(cid:107)2] + b (65)

E(cid:107)∇fit(xt) − ∇f (xt)(cid:107)2 + E(cid:107)∇f (xt)(cid:107)2 + b

(since E[∇fit(xt) − ∇f (xt)] = 0)

(E(cid:107)∇fit(xt) − ∇f (xt)(cid:107)2 + 2LmaxE (f (xt) − f ∗)) + b

(66)

(63)

(64)

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

where we used smoothness in the last inequality. Now, if σ = 0, namely ∇f (xt) = ∇fit(xt), we have

(cid:35)

f (xt) − f ∗

≤ α

(cid:34) T

(cid:88)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)2Lmax

T
(cid:88)

t=1

(f (xt) − f ∗) + b

T
(cid:88)

(f (xt) − f ∗) ≤ α22Lmax +

√

α2b

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(f (xt) − f ∗) ≤ α

(cid:112)

2Lmax +

√

αb1/4

Using Lemma 5 we get

so that

Now,

(cid:107)GT (cid:107)∗ =

(cid:118)
(cid:117)
(cid:117)
(cid:116)Tr

(cid:32) T

(cid:88)

t=1

∇f (xt)T ∇f (xt) + δ

=

(cid:33)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:107)∇f (xt)(cid:107)2 + b

(67)

(68)

(69)

(70)

Thus we have

(cid:107)GT (cid:107)∗ =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:107)∇f (xt)(cid:107)2 + b ≤

√

b +

(cid:118)
(cid:117)
(cid:117)
(cid:116)2Lmax

T
(cid:88)

t=1

(f (xt) − f ∗) ≤

√

b +

(cid:112)

2αLmaxb1/4 + 2αLmax

(71)

where we used smoothness for the inequality. This shows that (cid:107)GT (cid:107)∗ is a bounded series in the deterministic case.
Now, if σ (cid:54)= 0, going back to Eq. (66) we have

T
(cid:88)

t=1

E [f (xt) − f ∗] ≤ α

(cid:118)
(cid:117)
(cid:117)
(cid:116)T σ2 + 2Lmax

T
(cid:88)

t=1

(E (f (xt) − f ∗)) + b

Using Lemma 5 we get

T
(cid:88)

t=1

E [f (xt) − f ∗] ≤ 2α2Lmax +

We then have

E(cid:107)GT (cid:107)2

∗ = E

(cid:34) T

(cid:88)

t=1

(cid:107)∇fit(xt)(cid:107)2 + b

T α2σ2 + α2b ≤ 2α2Lmax +

√

T α2σ2 + α

√

b

(cid:112)

(cid:35)

(72)

(73)

(74)

=

T
(cid:88)

t=1

E(cid:107)∇fit(xt) − ∇f (xt)(cid:107)2 +

T
(cid:88)

t=1

E(cid:107)∇f (xt)(cid:107)2 + b

(same as Eq. (65))

≤ T σ2 + 2Lmax

T
(cid:88)

t=1

E [f (xt) − f ∗] + b

≤ T σ2 + 4α2L2

max + 2Lmax

T α2σ2 + 2Lmaxα

√

T + 2αLmax)2 + 2Lmaxα

b + b

√

√

√

b + b

from which we get

≤ (σ

E(cid:107)GT (cid:107)∗ = E(cid:112)(cid:107)GT (cid:107)2
≤ (cid:112)E(cid:107)GT (cid:107)2
√

(cid:113)

∗

∗

≤

(σ
√

T + 2αLmax)2 + 2Lmaxα
(cid:113)
√

≤ σ

T + 2αLmax +

2Lmaxα

b + b

√

b + b

(smoothness)

(by Eq. (73))

(75)

(76)

(Jensen’s inequality)

(by Eq. (75))

(77)

This implies that for T ≤

(cid:16)

√

2αLmax+

√

(cid:17)2

b+b

2Lmaxα
σ2

, we have

SVRG meets AdaGrad

E(cid:107)GT (cid:107)∗ ≤ 4αLmax + 2

(cid:113)

2Lmaxα

√

b + b

and for T ≥

(cid:16)

√

2αLmax+

√

2Lmaxα
σ2

(cid:17)2

b+b

,

The proof is done by noticing that

E(cid:107)GT (cid:107)∗ = O(

√

T )

α =

(cid:40) ρ√
Lmax
ρ(cid:48)
Lmax

√

(scalar case)

(full matrix and diagonal cases)

(78)

(79)

Corollary 2. Under the same assumptions as Lemma 1, for outer-loop k of AdaSVRG with constant step-size
ηk, there exists T0 =

C

f (wk)−f ∗ such that,

E(cid:107)Gt(cid:107)∗ =

(cid:40)

O(1),
√
O(

t − T0),

for t ≤ T0
for t ≥ T0.

Proof. Using Theorem 7 with σ2 = f (wk) − f ∗ gives us the result.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

J Helper Lemmas

We make use of the following helper lemmas from (Vaswani et al., 2020), proved here for completeness.

Lemma 3. For any of the full matrix, diagonal and scalar versions, we have

m
(cid:88)

t=1

(cid:107)xt − w∗(cid:107)2

At−At−1

≤ D2Tr(Am)

Proof. For any of the three versions, we have by construction that At is non-decreasing, i.e. At − At−1 (cid:23) 0 (for
the scalar version, we consider At as a matrix of dimension 1 for simplicity). We can then use the bounded feasible
set assumption to get

(cid:80)m

t=1 (cid:107)xt − w∗(cid:107)2

At−At−1

t=1 λmax(At − At−1) (cid:107)xt − w∗(cid:107)2

≤ (cid:80)m
≤ D2(cid:80)T

t=1 λmax(At − At−1).

We then upper-bound λmax by the trace and use the linearity of the trace to telescope the sum,

≤ D2 (cid:80)m
t=1 Tr(At − At−1) = D2 (cid:80)m
= D2(Tr(Am) − Tr(A0)) ≤ D2Tr(Am)

t=1 Tr(At) − Tr(At−1),

Lemma 4. For any of the full matrix, diagonal and scalar versions, we have

Moreover, for the scalar version we have

m
(cid:88)

t=1

(cid:107)gt(cid:107)2

A−1
t

≤ 2Tr(Am)

Tr(Am) ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

m
(cid:88)

t=1

(cid:107)gt(cid:107)2

and for the full matrix and diagonal version we have

Tr(Am) ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)d

m
(cid:88)

t=1

(cid:107)gt(cid:107)2 + d2δ

Proof. We prove this by induction. Start with m = 1.

For the full matrix version, A1 = (δI + g1g(cid:62)

1 )1/2 and we have

(cid:107)g1(cid:107)2

A−1
1

1 A−1
= g(cid:62)
= Tr(cid:0)A−1

1 g1 = Tr(cid:0)g(cid:62)
1 (A2

1 g1g(cid:62)
1
1 − δI)(cid:1) = Tr(A1) − Tr(cid:0)δA−1

(cid:1) = Tr(cid:0)A−1

1 A−1

1 g1

1

(cid:1)

(cid:1) ≤ Tr(A1)

For the diagonal version A1 = (δI + diag(g1g(cid:62)

1 ))1/2 we have
t g1 = Tr(cid:0)g(cid:62)

(cid:107)g1(cid:107)2

A−1
1

= g(cid:62)

1 A−1

1 A−1

1 g1

(cid:1) = Tr(cid:0)A−1

1 g1g(cid:62)
1

(cid:1)

(80)

Since A−1
Thus we get

1

is diagonal, the diagonal elements of A−1

1 g1g(cid:62)

1 are the same as the diagonal elements of A−1

1 diag(g1g(cid:62)

1 ).

(cid:107)g1(cid:107)2

A−1
1

= Tr(cid:0)A−1

1 diag(g1g(cid:62)

1 )(cid:1) = Tr(cid:0)A−1

1

(cid:0)A2

1 − δI(cid:1)(cid:1) = Tr(A1) − δTr(cid:0)A−1

1

(cid:1) ≤ Tr(A1)

For the scalar version A1 = (cid:0)g(cid:62)

1 g1

(cid:1)1/2 and we have

SVRG meets AdaGrad

(cid:107)g1(cid:107)2

A−1
1

= A−1

1 (cid:107)g1(cid:107)2 = A−1

1 g(cid:62)

1 g1 = A−1

1 A2

1 = A1 = Tr(A1)

Induction step: Suppose now that it holds for m − 1, i.e. (cid:80)m−1
also holds for m.
For the full matrix version we have

t=1 (cid:107)gt(cid:107)2

A−1
t

≤ 2Tr(Am−1). We will show that it

(cid:80)m

t=1 (cid:107)gt(cid:107)2

A−1
t

≤ 2Tr(Am−1) + (cid:107)gm(cid:107)2

A−1
m

(cid:16)

= 2Tr

(A2

m − gmg(cid:62)

m)1/2(cid:17)

+ Tr(cid:0)A−1

m gmg(cid:62)
m

(cid:1)

(Induction hypothesis)

(AdaGrad update)

We then use the fact that for any X (cid:23) Y (cid:23) 0, we have (Duchi et al., 2011, Lemma 8)
(X − Y )1/2(cid:17)

X 1/2(cid:17)

X −1/2Y

≤ 2Tr

+ Tr

2Tr

(cid:16)

(cid:16)

(cid:16)

(cid:17)

.

As X = A2

m (cid:23) Y = gmg(cid:62)
For the diagonal version we have

m (cid:23) 0, we can use the above inequality and the induction holds for m.

m
(cid:88)

t=1

(cid:107)gt(cid:107)2

A−1
t

≤ 2Tr(Am−1) + (cid:107)gm(cid:107)2

A−1
m

= 2Tr

(cid:16)(cid:0)A2

m − diag(gmg(cid:62)
m

(cid:1)1/2(cid:17)

+ Tr(cid:0)A−1

m gmg(cid:62)
m

(cid:1)

(Induction hypothesis)

(AdaGrad update)

As before, since A−1
elements A−1

m diag(gmg(cid:62)

m). Thus we get

m is diagonal, we have that the diagonal elements of A−1

m gmg(cid:62)

m are the same as the diagonal

m
(cid:88)

t=1

(cid:107)gt(cid:107)2

A−1
t

≤ 2Tr

(cid:16)(cid:0)A2

m − diag(gmg(cid:62)
m

(cid:1)1/2(cid:17)

+ Tr(cid:0)A−1

m diag(gmg(cid:62)

m)(cid:1)

We can then again apply the result from Duchi et al. (2011, Lemma 8) with X = A2
and we obtain the desired result.

m (cid:23) Y = diag(gmg(cid:62)

m) (cid:23) 0,

For the scalar version, since A−1

m is a scalar we have

m
(cid:88)

t=1

(cid:107)gt(cid:107)2

A−1
t

≤ 2Tr(Am−1) + (cid:107)gm(cid:107)2

A−1
m

= 2Tr

(cid:16)(cid:0)A2

m − g(cid:62)

mgm

(cid:1)1/2(cid:17)

(Induction hypothesis)

(cid:1)

(AdaGrad update)

+ Tr(cid:0)A−1

m g(cid:62)

mgm

We can then again apply the result from Duchi et al. (2011, Lemma 8) with X = A2
obtain the desired result.

m ≥ Y = g(cid:62)

mgm ≥ 0, and we

Bound on the trace: For the trace bound, recall that Am = G1/2

m . For the scalar version we have

Tr(Am) = Tr

(cid:16)

G

(cid:17)

1/2
m

= G

1/2
m =

(cid:113)(cid:80)m

t=1 g(cid:62)

t gt =

(cid:113)(cid:80)m

t=1 (cid:107)gt(cid:107)2

For the diagonal and full matrix variants, we use Jensen’s inequality to get

Tr(Am) = Tr

(cid:16)

(cid:17)

=

G1/2
m

d
(cid:88)

(cid:113)

λj(Gm) = d

(cid:18) 1
d

(cid:19)

λj(Gm)

d
(cid:88)

(cid:113)

j=1

j=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

≤ d

1
d

d
(cid:88)

j=1

λj(Gm) =

√

d(cid:112)Tr(Gm).

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

there λj(Gm) denotes the j-th eigenvalue of Gm.
For the full matrix version, we have

(cid:112)Tr(Gm) =

(cid:113)

Tr(cid:0)(cid:80)m

t=1 gtg(cid:62)

t + δI(cid:1) =

For the diagonal version, we have

(cid:112)Tr(Gm) =

(cid:113)

Tr(cid:0)(cid:80)m

t=1 diag(gtg(cid:62)

t ) + δI(cid:1) =

(cid:113)(cid:80)m

t=1 Tr(cid:0)gtg(cid:62)

t

(cid:1) + dδ =

(cid:113)(cid:80)m

t=1 (cid:107)gt(cid:107)2 + dδ

(cid:113)(cid:80)m

t=1 Tr(cid:0)diag(gtg(cid:62)

t )(cid:1) + dδ =

(cid:113)(cid:80)m

t=1 (cid:107)gt(cid:107)2 + dδ

which concludes the proof.

Lemma 5. If x2 ≤ a(x + b) for a ≥ 0 and b ≥ 0,

x ≤

1
2

(

(cid:112)

a2 + 4ab + a) ≤ a +

√

ab

Proof. The starting point is the quadratic inequality x2 − ax − ab ≤ 0. Letting r1 ≤ r2 be the roots of the
quadratic, the inequality holds if x ∈ [r1, r2]. The upper bound is then given by using

a + b ≤

a +

√

√

√

b

√

a +

a2 + 4ab
2

≤

√

a +

a2 +
2

√

4ab

r2 =

= a +

√

ab.

K Counter-example for line-search for SVRG

Algorithm 6 SVRG-Inner-Armijo
initialization;
Input :w0 ∈ Rn, ηmax > 0, c > 0
for k = 0, 1, . . . do
xk
0 = wk
for t = 0, . . . , m − 1 do

t ) − ∇fit(wk) + ∇f (wk)

it ∼ U [n] (pick it uniformly from {1, . . . , n})
gt = ∇fit(xk
Find ηt ≤ ηmax maximal such that
t ) − cηt (cid:107)gt(cid:107)2
fit(xk
t+1 ← xk
xk

t − ηtgt) ≤ fit(xk
t − ηtgt

end
wk+1 = xk
m

end

Proposition 3. For any c > 0, ηmax > 0, there exists a 1-dimensional function f whose minimizer is x∗ = 0, and
t+1| ≥ |xk
for which the following holds: If at any point of Algorithm 6, we have |xk
t |.

t | ∈ (cid:0)0, min{ 1

c , 1}(cid:1), then |xk

Proof. Deﬁne the following function

f (x) =

1
2

(cid:0)f1(x) + f2(x)(cid:1) =

1
2

(cid:0)a(x − 1)2 + a(x + 1)2(cid:1) = a(cid:0)x2 + 1(cid:1)

(81)

where a > 0 is a constant that will be determined later. We then have the following

f (cid:48)(x) = 2ax
f (cid:48)
1(x) = 2a(x − 1)
f (cid:48)
2(x) = 2a(x + 1)

The minimizer of f is 0, while the minimizers of f1 and f2 are 1 and -1, respectively. This symmetry will make
the algorithm fail.

SVRG meets AdaGrad

Now, as stated by the assumption, let |xk
Case 1: it = 1. Then we have

t | ∈ (cid:0)0, min{ 1

c , 1}(cid:1). WLOG assume xk

t > 0, the other case is symmetric.

gt = f (cid:48)

1(xk
= 2a(xk
= 2axk

t ) − f (cid:48)
1(wk) + f (cid:48)(wk)
t − 1) − 2a(wk − 1) + 2awk
t > 0

Observe that gt > 0. Since xk
direction −gt from xk
Thus in that case xk
Case 2: it = 2. Then we have

t < 1 and the function f1 is strictly decreasing in the interval (−∞, 1], moving in the
t can only increase the function value. Thus the Armijo line search will fail and yield ηt = 0.
t+1 = xk
t .

gt = f (cid:48)

2(xk
= 2a(xk
= 2axk

2(wk) + f (cid:48)(wk)
t ) − f (cid:48)
t + 1) − 2a(wk + 1) + 2awk
t > 0

The Armijo line search then reads

which we can rewrite as

f2(xk

t − 2aηtxk

t ) ≤ f2(xk

t ) − cηt(2axk

t )2

a(cid:0)xk
⇒a(xk

t + 1 − 2aηtxk
t
t + 1)2 − 4a2ηt(xk

≤ a(xk
t + 1)xk

t + 1)2 − 4ca2ηt(xk
t a3(xk
t + 4η2

t )2
t )2 ≤ a(xk

(cid:1)2

t + 1)2 − 4ca2ηt(xk

t )2

Simplifying this gives

which simpliﬁes even further to

ηta(xk

t )2 ≤ (xk

t + 1)xk

t − c(xk

t )2

Therefore, the Armijo line-search will return a step-size such that

ηt ≤

− c

1 + 1
xk
t
a

ηt ≥ min

(cid:26) 1 + 1
xk
t
a

− c

(cid:27)

, ηmax

(82)

Now, recall that by assumption we have xk

t < 1/c. Then 1/xk

t − c > 0, which implies that

Now is the time to choose a. Indeed, if a is such that 1/a ≤ ηmax, we then have by Eq. (82) that

− c

1 + 1
xk
t
a

≥

1
a

We then have

ηt ≥

1
a

t+1 = xk
xk
t − ηtgt
t − 2aηtxk
= xk
t
= (1 − 2aηt)xk
t
t = −xk
≤ (1 − 2)xk
t

where the inequality comes from ηt ≥ 1/a and the fact that xk

t ≥ 0. Thus we indeed have |xk

t+1| ≥ |xk
t |.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

L Additional Experiments

L.1 Poor performance of AdaGrad compared to variance reduction methods

Figure 4: Poor performance of best-tuned step-size (after grid-search) AdaGrad compared to SVRG and AdaSVRG
on logistic regression problems with batch size 64 on two example datasets

L.2 Additional experiments with batch-size = 64

(a) Comparison against best-tuned variants

Figure 5: Comparison of AdaSVRG against SVRG variants, SVRG-BB and SARAH for logistic loss and batch-size
= 64. For the sensitivity to step-size plot to be readable, we limit the gradient norm to a maximum value of 10.

(b) Sensitivity to step-size

0255075100125150(Gradient evaluations) / n103102101Gradient Norm (log)a1aAdaGradSVRGAdaSVRG0255075100125150(Gradient evaluations) / n1011109107105103rcv1AdaGradSVRGAdaSVRGSVRG meets AdaGrad

(a) Comparison against best-tuned variants

Figure 6: Comparison of AdaSVRG against SVRG variants, SVRG-BB and SARAH for Huber loss and batch-size
= 64. For the sensitivity to step-size plot to be readable, we limit the gradient norm to a maximum value of 10.

(b) Sensitivity to step-size

(a) Comparison against best-tuned variants

(b) Sensitivity to step-size

Figure 7: Comparison of AdaSVRG against SVRG variants, SVRG-BB and SARAH for squared loss and batch-size
= 64. For the sensitivity to step-size plot to be readable, we limit the gradient norm to a maximum value of 10.
In some cases, SVRG-BB diverged, and we remove the curves accordingly so as not to clutter the plots.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

L.3 Studying the eﬀect of the batch-size on the performance of AdaSVRG

(a) Batch-size = 128

(b) Batch-size = 8

Figure 8: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for logistic loss
for diﬀerent batch-sizes.

(c) Batch-size = 1

SVRG meets AdaGrad

(a) Batch-size = 128

(b) Batch-size = 8

Figure 9: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for logistic loss
for diﬀerent batch-sizes.

(c) Batch-size = 1

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

(a) Batch-size = 128

(b) Batch-size = 8

Figure 10: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for Huber
loss for diﬀerent batch-sizes.

(c) Batch-size = 1

SVRG meets AdaGrad

(a) Batch-size = 128

(b) Batch-size = 8

Figure 11: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for Huber
loss for diﬀerent batch-sizes.

(c) Batch-size = 1

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

(a) Batch-size = 128

(b) Batch-size = 8

(c) Batch-size = 1

Figure 12: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for squared
loss for diﬀerent batch-sizes. In some cases, SVRG-BB diverged, and we remove the curves accordingly so as not
to clutter the plots.

SVRG meets AdaGrad

(a) Batch-size = 128

(b) Batch-size = 8

(c) Batch-size = 1

Figure 13: Comparison of AdaSVRG against best-tuned variants of SVRG, SVRG-BB and SARAH for squared
loss for diﬀerent batch-sizes. In some cases, SVRG-BB diverged, and we remove the curves accordingly so as not
to clutter the plots.

Dubois-Taine, Vaswani, Babanezhad, Schmidt, Lacoste-Julien

L.4 Additional interpolation experiments

(a) Batch-size = 128

(b) Batch-size = 64

(c) Batch-size = 8

(d) Batch-size = 1

Figure 15: Comparison of best-tuned SGD, best-tuned SVRG, AdaGrad, AdaSVRG and Algorithm 4 with squared
hinge loss on datasets with diﬀerent fraction of mislabeled data-points for diﬀerent batch-sizes. Interpolation is
exactly satisﬁed for the left-most plots.

SVRG meets AdaGrad

(a) Batch-size = 128

(b) Batch-size = 8

(c) Batch-size = 1

Figure 14: Comparison of best-tuned SGD, best-tuned SVRG, AdaGrad, AdaSVRG and Algorithm 4 with logisitic
loss on datasets with diﬀerent fraction of mislabeled data-points for diﬀerent batch-sizes. Interpolation is exactly
satisﬁed for the left-most plots.

