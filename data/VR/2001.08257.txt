Using Virtual Reality in Electrostatics Instruction:

The Impact of Training

C. D. Porter,∗ J. R. H. Smith, E. M. Stagar, A. Simmons, M. Nieberding, and C. M. Orban

Department of Physics, The Ohio State University, Columbus, OH 43210

J. Brown†

Department of Engineering Education,

The Ohio State University, Columbus, OH 43210

A. Ayers‡

Advanced Computing Center for the Arts and Design,

The Ohio State University, Columbus, OH 43210

(Dated: July 2, 2020)

Abstract

Recent years have seen a resurgence of interest in using Virtual Reality (VR) technology to

beneﬁt instruction, especially in physics and related subjects. As VR devices improve and become

more widely available, there remains a number of unanswered questions regarding the impact of

VR on student learning and how best to use this technology in the classroom. On the topic of

electrostatics, for example, a large, controlled, randomized study performed by Smith et al. 20171,

found that VR-based instruction had an overall negligible impact on student learning compared to

videos or images. However, they did ﬁnd a strong trend for students who reported frequent video

game play to learn better from VR than other media. One possible interpretation of this result is

that extended videogame play provides a kind of “training” that enables a student to learn more

comfortably in the virtual environment. In the present work we consider if a VR training activity

that is unrelated to electrostatics can help prepare students to learn electrostatics from subsequent

VR instruction. We ﬁnd that preliminary VR training leads to a small but statistically signiﬁcant

improvement in student performance on our electrostatics assessment. We also ﬁnd that student

reported game play is still correlated with higher scores on this metric.

0
2
0
2

l
u
J

1

]
h
p
-
d
e
.
s
c
i
s
y
h
p
[

4
v
7
5
2
8
0
.
1
0
0
2
:
v
i
X
r
a

1

 
 
 
 
 
 
I.

INTRODUCTION

Many topics in physics are inherently three-dimensional (3D), but are usually taught using

two-dimensional media such as whiteboards and computer screens. Stereoscopic virtual

reality (VR) allows students to view 3D scenes with depth perception, which should be

advantageous for teaching certain content in physics and other STEM disciplines. Eﬀorts to

develop stereoscopic VR visualizations for physics began in the mid-1990s2–4 and continue

to the present day (e.g.5–10 and references therein) as the technology improves.

The growth in VR11 content for physics should be followed by detailed studies of the

impact of these visualization methods on student learning. The studies that have been per-

formed in physics and related STEM ﬁelds report varying degrees of success12–22, including

many cases in which stereoscopic visualization techniques did not prove to be pedagogically

more valuable than more conventional visualization methods. In this paper we will present

data from a new study in a large introductory electromagnetism class at Ohio State Uni-

versity that will address these questions. A particularly aﬀordable way to provide students

with a reasonably high-quality VR experience that we emphasize in this paper is so-called

Google Cardboard23 in which a typical smartphone is placed in a cardboard or plastic head-

set which may only cost a few dollars. This reduced cost is important because it means

each student can potentially have their own VR headset, so that VR can become a regular

part of instruction. The reduced cost allowed us to perform a large study using a set of six

aﬀordably priced smartphones.

Prior studies investigating the eﬀectiveness of VR in physics and astronomy9,12–22,24 have

yielded mixed results. Although students given VR interventions often report being more

engaged with the material, and physical immersion in VR has been shown to increase spatial

awareness in search tasks25, the advantage of VR over other media in achieving gains in

speciﬁc learning outcomes is still unclear. Unfortunately, because of the prohibitive cost of

conventional VR headsets, many of these prior studies have limited sample sizes and in some

cases VR treatment was not compared to a control group.

There have been a few large studies with careful controls. Madden et al.21 considers a

VR intervention for an astronomy course on the topic of the moon phases. That study had

172 participants across three treatment groups (VR, computer, and “hands-on”/control),

and found no statistically signiﬁcant diﬀerence in learning gains between treatment groups.

2

A fuller description of their study appears in22.

Other large studies by Smith et al.1 and Porter et al.26, which include several authors of

this paper, did not ﬁnd statistically signiﬁcant diﬀerences in pre-post test gains for VR com-

pared to other media on topics of electrostatics and magnetostatics. The studies involved,

respectively, 301 and 289 participants from college-level introductory physics classes. Also

of note is a study by Greenwald et al.27 where 20 college students completed activities re-

lating to electrostatics and answered questions. Students who completed the activities in a

VR headset and interacting with the virtual environment did not outperform students who

completed essentially the same activities by drawing on a tablet (i.e. a 2D medium).

If one looks outside the physics content areas just described, there do exist large studies

in which statistically signiﬁcant eﬀects of stereoscopic VR compared to other media have

been detected. A notable example from college-level mathematics is Porter and Snapp28.

A recently published meta-study by Merchant et al.29 considered dozens of K12 VR stud-

ies and found (among other conclusions) that VR content overall tends to be eﬀective in

producing learning gains. However, the goal of Merchant et al.29 was not to weigh the use-

fulness of stereoscopic VR versus more traditional media, and the meta-study considered

non-stereoscopic virtual worlds accessed through conventional desktop and laptop comput-

ers to be VR. So while the paper is very interesting and thorough, its relevance is in many

ways oblique to the work that we will describe here.

In Smith et al.1, although VR did not prove to be more eﬀective for students in general,

it was found that students who reported frequent video game play (a.k.a. “gamers”) and

were given the VR treatment had much higher gains than any other group (non-gamers, or

gamers who received electrostatics instruction from video renderings or images). Porter et

al. found a similar trend for “gamer” students to signiﬁcantly outperform non-gamer peers

on magnetostatics assessments after viewing magnetostatics content, although, interestingly,

the VR treatment did not help the gamer students more than other media as Smith et al.

found. Both Smith et al. and Porter et al. interpret these results to imply that gamers may

have a “familiarity” with visuospatial rotations to the point where they are less likely to be

cognitively overwhelmed by and better able to learn from inherently 3D content. Smith et

al.1 and Porter et al.28 both conclude with questions asking if “repeated exposure” to VR or

“training” of students with VR can help non-gamers learn as eﬀectively as gamers do from

VR content.

3

Building oﬀ of this line of inquiry, in the present study we consider if VR training activities

on topics unrelated to electrostatics can improve students’ ability to learn eﬀectively from

VR instruction on electrostatics. These activities are described in the next section. The

inclusion of virtual training prior to engaging in this experience has been utilized across

other virtual environments, such as the landing room developed by Johnston et al.30 This

space forms a familiar environment for users to learn necessary interactions and adjust to

a new visual format before diving into an abstract view of a cell membrane. While other

examples of this training exist in other studies, few have demonstrated its impact on overall

performance.

Due to a lack of independently-validated assessments for electrostatics with a high fraction

of 3D questions, we developed a suite of questions as a preliminary survey of 3D electrostatics

(see the Methods section). This is only brieﬂy summarized there because of page constraints.

The reliability of this survey is discussed below, along with student performance.

II. METHODS

The subjects of this work were students in the second semester of an introductory calculus-

based physics course at a large Midwestern university, oﬀered in autumn. This course was

being oﬀered “oﬀ-sequence” meaning that students who begin physics in their ﬁrst semester

would have taken this course in spring. Students were oﬀered the equivalent of one homework

assignment’s course credit for coming to our lab and participating in either the research

study, or an alternative assignment of roughly equivalent length. Of 281 initial respondents,

279 agreed to participate in research.

As students entered the testing area, they were randomly assigned to one of two treatment

types: VR with preliminary training, and VR with no initial training. The assessments

were identical for all students, regardless of treatment type, except for a few questions

posed during the preliminary training, which were unrelated to electrostatics. The students’

average overall performance in physics was fairly constant between treatment types, as

determined by post-hoc analysis of students’ ﬁnal scores as a percentage of points in their

physics course

(Training: (84.5 ± 0.9)%, No Training: (84.2 ± 0.9)%, p > 0.8). There was almost no

variation in the percentage of students reporting their sex as female in the two treatment

4

types (Training: 20%, No Training: 20%). Although gender identity would be a better

descriptor of participants, gender identity is not available.

A. Treatments

VR visualizations were created as Android smartphone applications. The apps were

written using Unity, a cross-platform game engine developed by Unity Technologies31, and

the Google VR SDK for Unity. Smartphones were placed in plastic goggles which have lenses

to focus the near point of the eye, and a divider to split the ﬁeld of view. The smartphone

displays an app in a split-screen mode so that 3D phenomena are shown on the right half of

the phone to the right eye from an angle slightly to the right, and the equivalent is shown to

the left eye from an angle slightly to the left. This creates a stereoscopic 3D virtual reality

experience giving the impression of depth perception.

Preliminary training: Students in the preliminary training group viewed scenes that were

unrelated to electrostatics. In the ﬁrst scene, students were shown a 3D model of a house,

and were asked to rotate the house, view it from all angles, and count the number of windows.

In the second scene, students were shown a 3D model of a single-propeller airplane, and were
asked three questions related to angular momentum such as the initial direction of (cid:126)L, and
the change in (cid:126)L if certain maneuvers are performed. Screenshots from these training scenes

are shown in Fig. 1.

Only the preliminary training group was given these initial scenes. Students took an

average of 4 minutes on all training scenes combined. All students took a pretest on a 2D

computer screen with 13 multiple choice questions on electrostatics. Students then moved

on to the electrostatics VR instruction which consisted of several visualizations of electric

ﬁelds, and a few in-VR questions to promote engagement. This instruction is described in

greater detail below. Students were then given the posttest (on a computer) consisting of

11 multiple choice questions, followed by 10 in-VR posttest questions. A schematic of the

testing protocol is shown in Fig. 2.

VR Instruction: The VR scenes on electrostatics shown to both treatment groups involved

visualization of electric ﬁelds due to charge distributions including an electric dipole, a long,

charged rod, and a large, charged plate. The electric ﬁeld was represented as an array

of vectors as opposed to using the density of continuous ﬁeld lines. The application was

5

FIG. 1. Screen shots from two preliminary training scenes. Students were asked to rotate the house

in (a) and count the panes of glass. They were then asked to view the rotation of the propeller on

the plane in (b) and determine the direction of the angular momentum using the right-hand rule.

then built as an Android application package (APK ﬁle), and installed on two Nexus 5X

smartphones. The app splits the phone’s screen into two halves, one for each eye. Each

phone is then placed in a cardboard or plastic viewer. The students can then view the

electric systems in stereoscopic 3D. The app utilizes the smartphone sensors so that when

the students turn their heads, the system being displayed on the screen rotates, allowing

students to see it from any orientation. Students were shown 7 instructional scenes and were

told to look around and study the magnetic ﬁeld vectors from many angles before moving

on. Students were also asked a series of 3 questions within the VR simulation to ensure

that students were engaging with the content. Students controlled the rate at which the

visualizations progressed.

6

FIG. 2. The sequence of treatment and test questions applied to the two groups: those who received

preliminary VR training, and those who did not.

B. Assessment

Discussions with experienced instructors were used to determine which aspects of electric

ﬁelds are commonly prioritized in their learning goals for this course. The study team then

designed a set of problems on this content that are highly three-dimensional in nature, and

are therefore most likely to be aided by stereoscopic 3D treatments. These problems fall into
two broad categories: (1) determining the direction of (cid:126)E at locations that are not simply

co-planar with a distribution of charge, and (2) understanding features of the vector ﬁeld as

a whole, such as divergence.

The study team wrote 13 pretest questions and 11 posttest questions to address the

above topics. A complete description of the assessment is beyond the scope of this paper.

But because these questions have not been independently validated by other groups, we

provide in the present work additional statistics on the reliability of this assessment, and

some example assessment items in Fig. 3.

To avoid confounding factors related to medium, reliability analysis was performed only

the questions posed using a 2D computer screen. Treating these questions as a preliminary

scale for three-dimensional understanding of electric ﬁelds, a reliability analysis in SPSS

reveals a Cronbach’s alpha of 0.91 (or 0.83 for the pretest only, 0.82 for the posttest only).

A factor analysis in SPSS revealed a single factor with an eigenvalue greater than 1 (4.8),

7

FIG. 3. Two example items from the assessment. Item (a) was posed using a 2D computer screen,

whereas Item (b) was posed in VR, such that students could rotate the charge arrangement and

clearly see that the point P was directly above the positive charge.

and this factor explained 43% of the variance. From these data we conclude that although

the assessment still needs to be independently validated and could be improved in many

ways, it does appear to be internally consistent and statistically well-behaved.

We note that the questions used on this assessment are not identical to questions used

in Smith et al., although there is some overlap. Some questions from Smith et al. were

altered to allow for partial credit if students get some Cartesian components of the electric

ﬁeld direction correct, but not all. For example, students answering the item shown in Fig.

3 would have received 1/3 of a point for each component answered correctly. Questions

were also added that included arrangements of three or four charged particles, such that the

8

particles and point at which the electric ﬁeld direction is to be determined did not lie in

any 2D plane (as in Fig. 3(a)). These diﬀerences, coupled with the fact that students from

Smith et al. were from an “on-sequence” course, mean that these studies cannot be directly

compared, quantitatively.

C. Gaming

One additional diﬀerence between the present work and Smith et al.

is that in Smith

et al. students were asked about gaming frequency, but were not asked about the type of

game they primarily play. In this work, students were asked how frequently they currently

play videogames, and were then asked whether the games are primarily 2D, 3D, or both.

Common examples of each were given (such as Candy Crush for 2D, and Minecraft for

3D). The intent was to classify students based on their responses as either “3D gamers”

or “Not 3D gamers”, since the expectation was that 3D gaming would be key. However, a

post-hoc analysis showed no signiﬁcant diﬀerences in our results when students are classiﬁed

solely on frequency of gaming, with no consideration of 3-dimensionality. This brings into

question whether familiarity with 3D visuospatial rotation in an electronic context is truly

an explanation of the interaction eﬀect between gaming and gains in Smith et al. and Porter

et al. This is discussed further in the Discussion and Conclusions section. This independence

of the 3D nature of the games played leads us to classify the groups simply as “gamers”

and “non-gamers”, with “gamers” being those students who reported playing once per week

or more. The present work contains no further comparison between 3D gamers and other

types of gamers.

III. RESULTS AND DISCUSSION

We ﬁnd that the pre-trained group did have higher gains than the untrained group (see

Fig. 4). The diﬀerence in gains is statistically signiﬁcant with p = 0.014, and an eﬀect size

of d = 0.24. This result is clearly driven by an increase in scores by the trained group’s

score and a pre-to-post decrease in the untrained group’s score. This is consistent with

trained students either learning from the intervention, or improving slightly due to the

retesting eﬀect. But it is also consistent with the untrained students being overwhelmed by

9

or confused by the intervention, causing lower posttest results than pretest results. Here,

pretest questions were all posed on a 2D computer screen. But posttest questions posed in

diﬀerent formats have been grouped together. Some posttest questions were asked entirely

on a 2D computer screen and others were posed in VR with answers recorded on a separate

computer. These categories of question are compared further below. Questions posed in VR

during the electrostatics instruction have not been counted as either pre or post; they are

discussed further below. In all cases, answers were recorded using a 2D computer screen,

even if they were posed in VR. These various formats are discussed further below.

FIG. 4. Average scores on pretest and postests for the group that received preliminary training,

and the group that did not.

Because this work was initially motivated by the correlation between gains from VR

treatments and gaming experience, it is worth breaking down these scores by gaming expe-

rience. Fig. 5 shows the gains from the two treatment groups broken down by prior gaming

experience.

That trained gamers show positive gains and that trained non-gamers show yet higher

gains ﬁts the hypothesis that training can compensate for a lack of familiarity with virtual

10

5560657075PrePostAverage Student Score (%)Pre-trained (N = 142)No training (N = 137)FIG. 5. Average post-pre gains for the group given preliminary training, and the group that did

not, separated by those reporting high gaming experience (gamer) and low gaming experience

(non-gamer).

environments and visuospatial rotations. However, we were surprised to see that gamers

who did not receive preliminary training performed worse than any other group, having

negative gains. Untrained non-gamers, for example, had small but positive gains. In light

of these inconsistent results, it is important to note that there are no statistically signiﬁcant

diﬀerences between the performance of gamers and non-gamers in their physics course overall

(gamers: 84.0% ± 0.8%, non-gamers: 85.2% ± 1.0%, p = 0.35). The inconsistent interaction

eﬀect between training and gaming casts some doubt over the the simple hypothesis that

prior video game play provides important advantages to students for which training can at

least partially compensate. It is unclear whether the hypothesis may yet be true, since this

inconsistent interaction eﬀect of gaming and training is not statistically signiﬁcant (p = 0.41,

repeated measures analysis of variance, with treatment and gaming score as between-subjects

factors).

To obtain some insight into this paradoxical result, we considered that questions were

11

-6-4-202468Gamer (N = 179)Non-gamer (N = 100)Average Gains (%)Pre-trained (N = 142)No training (N = 137)asked in two formats: some questions were posed in VR while others were posed on a

conventional computer monitor (i.e. “in 2D”). These questions can be categorized into four

groups: 1) pretest questions asked entirely on a computer monitor, 2) questions posed in VR

during electrostatics instruction, 3) posttest questions asked on a computer monitor, and 4)

posttest questions posed in VR. Fig. 6 shows student scores on each of these question sets,

arranged in chronological order, and split into trained and untrained groups.

FIG. 6. Average scores at diﬀerent time points on questions posed in diﬀerent media, separated

by treatment type.

Pretest scores for the two groups (which were asked in 2D) are consistent within standard

error. Likewise, the non-VR posttest scores are consistent between the two groups and the

overall result is that the non-VR posttest scores are slightly lower. It is unlikely that the

posttest questions are signiﬁcantly more diﬃcult than the pretest questions, because the two

question sets are identical up to occasional swapping of positive and negative charges, or

rotation from one high-symmetry point to an analogous high-symmetry point. This being

the case, the similarity between the “pre” and non-VR “post” test does seem to imply

that the VR intervention had a negligible impact on student learning, regardless of whether

12

4045505560657075808590PreMid VRPostPost in-VRAverage Score (%)Trained (N =142)Untrained (N = 137)students were in the trained or untrained group, and contrary to our expectations.

Perhaps the most interesting feature of Fig. 6 is the large and statistically signiﬁcant

(d = 0.22, p = 0.02) diﬀerence between the two treatment groups’ scores on the questions

posed in VR at the midpoint, during electrostatics instruction in VR. The group that received

preliminary training in VR performed approximately as well on the questions posed in VR as

they did in the pretest (which was in 2D), whereas those who received no initial acclimation

to VR had scores about 16% lower at this mid-point. So in this sense there was a net beneﬁt

for students who received the initial acclimation with VR, but the beneﬁt was limited to

questions posed in VR.

The additional 7-10% uptick in scores at the ﬁnal set of posttest questions posed in VR

shown in Fig. 6 must be viewed with some skepticism, as we cannot verify that the VR-based

questions were of comparable diﬃculty to the other non-VR question sets. The change in

medium, for example, is necessarily accompanied by slightly diﬀerent posing of questions

and style. Conceivably, this uptick could be indicative of continued improvement in student

understanding of the material and engagement with VR, but such a claim would require

signiﬁcant additional experimental evidence.

Given that this inquiry was prompted by an apparent connection of the subject matter

with student gaming history, we further break down these four question sets by student

gaming history. This is shown in Fig. 7.

The dashed lines show the scores by the untrained group, and the solid lines show those

of the trained group. Here we see that the drop for “Mid VR” questions is seen in both

untrained gamers and untrained non-gamers. We also see that, overall, gamers score higher

on the pretest than non-gamers. Interestingly, by the second set of VR questions, which is

the last set of questions that students complete, there is no statistically signiﬁcant diﬀerence

between these groups either by gaming or by training.

It bears mentioning that the non-gamer group has proportionally more women than the

gamer group. As is shown in Fig. 8, this means that equalization between self-reported

gamers and non-gamers by the last question set also corresponds to equalization between

males and females.

13

FIG. 7. Average scores at diﬀerent time points on questions posed in diﬀerent media, separated

by treatment type and gaming experience.

A. Student feedback

Upon completion of all questions related to electrostatics, but prior to being asked de-

mographics questions, students were asked to rate the VR intervention in three ways. They

were asked 1) “How helpful was this intervention?”, 2) “How enjoyable was this interven-

tion?”, and 3) “How likely are you to recommend this intervention to a friend?”. Students

were asked to respond using a 5-point scale ranging from -2 (“Highly unhelpful”, “Highly

unenjoyable”, “Highly unlikely”, respectively) to +2 (“Highly enjoyable”, etc.) with 0 being

neutral. In all cases, average scores were positive, very close to +1 (“Helpful”, “Enjoyable”,

“Likely”). Figure 9 shows that both groups (trained and untrained) found the VR instruc-

tion equally helpful and equally enjoyable. Although there was a slight diﬀerence in how

likely the two groups were to recommend the VR to a friend, the diﬀerence is not signiﬁcant

after a post-hoc (Bonferroni) correction.

We ﬁnd analogous results when splitting student responses according to sex and according

14

30405060708090PreMid VRPostPost in-VRAverage Score (%)Trained Gamer (N = 88)Trained Non-gamer (N = 54)Untrained Gamer (N = 91)Untrained Non-gamer (N = 46)FIG. 8. Average scores at diﬀerent time points on questions posed in diﬀerent media, separated

by sex. Three students included in other ﬁgures are excluded here due to identiﬁability concerns

and/or response completeness.

to gaming history (not shown). There are no diﬀerences in means that are signiﬁcant after a

post-hoc correction, and there are also no large diﬀerences in the distributions of responses.

These questions were also asked of students in the study by Smith et al.1 which explored

student learning comparing VR to other treatment types including videos and still images.

Students signiﬁcantly preferred VR over other treatment types.

IV. DISCUSSION AND CONCLUSION

Smith et al.1 found that students who reported frequent video game play seem better able

to learn from VR-based instruction on electrostatics than “non-gamers”. As discussed in

that study, these “gamer” students who received VR instruction signiﬁcantly outperformed

students who received equivalent instruction from videos or still images. In designing the

present study our hypothesis was that preliminary training in VR on a topic unrelated to

15

40455055606570758085PreMid VRPostPost in-VRAverage Scores (%)Male (N = 219)Female (N = 56)FIG. 9. Average feedback on VR intervention separated by treatment group. There were no

signiﬁcant diﬀerences between treatment groups in perceived helpfulness of the intervention, its

enjoyability, nor in how likely students would be to recommend it to a friend. Three students

included in other ﬁgures are excluded here due to response completeness.

electrostatics would help even non-gamer students perform as well on electrostatics problems

as gamer students.

Figure 4 shows that there is indeed an interaction eﬀect between preliminary training

and subsequent gains. It should be noted, however, that the gains averaged over all groups

(trained and untrained) are not statistically diﬀerent from zero.

If comfort with visuospatial rotations in an electronic context (either through training or

gaming history) were all that were required to learn eﬀectively from VR, then one would

expect there to be a signiﬁcant dependence on the type of game typically played (2D or

3D). This was not the case. It is possible that frequent gaming of any kind increases the

likelihood of some minimum exposure to 3D gaming, even if more time is spent on 2D gaming

than on 3D gaming. Greater clarity could be achieved through a simple modiﬁcation of the

gaming experience question, such as asking students to move a slider to indicate the fraction

of gaming that is 2D and the fraction that is 3D. Another possibility is that exposure to

gaming of any kind increases student self-eﬃcacy in the space of electronic visualization.

Self-eﬃcacy, pioneered by Bandura32, is often strongly correlated with perseverance in a

16

-2-1.5-1-0.500.511.52HelpfulRecommendEnjoyableAverage Rating (-2 to 2)Trained (N = 141)Untrained (N = 135)discipline or activity. Although this work shows that training can improve gains from VR

instruction, the mechanism for this is still uncertain.

Note that in Fig. 5, students who did report frequent video game play but who did not

receive the training improved the least, with an overall worse score on the posttest than on

the pretest. This would appear inconsistent with any of the explanations above related to

self-eﬃcacy, or accumulated comfort with electronic visuospatial rotations, but the story is

clearer in Fig. 7. There, one can see that the initial average score of untrained gamers was

as high (within error bars) as any score achieved by any other group at any time point, such

that the small negative gains may be attributable to ceiling eﬀects. Other results on Fig. 5

were unsurprising and generally the training helped students to perform overall better on

posttests.

Figure 6 shows that the primary eﬀect of VR training was to increase the trained group’s

scores on in-VR questions during instruction. The training, however, did not improve scores

on the 2D, computer-based posttest questions. Also, trained and untrained groups achieve

nearly the same score on the posttest in-VR questions. One possible explanation of this is

that when electrostatic questions are posed in VR to students who have never used VR, the

experience is overwhelming and they do poorly. But they do better later on when similar

questions are asked in VR the second time, such that the midpoint VR experience serves

like a preliminary training for the initially untrained students. In other words, all students

perform better in VR after one or more exposures to questions in VR. The VR instruction

appears to have had no eﬀect on the computer-based post test. This indicates either that

the in-VR training is ineﬀective, or that learning in the VR context is not transferring to

the 2D context, or both.

The breakdown of the data in Fig. 7 also shows that gaming experience correlates with

average scores on these electrostatics assessments, including a pretest that was given before

receiving any electrostatics instruction. Fig. 7 shows that gamers scored around 13% higher

on all question types, except the ﬁnal post in-VR questions, making this a much larger eﬀect

than any due to the VR treatment. This bolsters the possibility that the Fig. 5 data showing

untrained gamer students performing the worst in terms of gains, might be related to ceiling

eﬀects.

The original intent of this study was to compare overall gains between the two treatment

groups, and not to compare groups’ performances on individual questions or questions in

17

one medium compared to another. It is therefore entirely possible that the observed uptick

in performance on the ’post in-VR’ questions is simply due to variation in question diﬃculty.

It seems unlikely, though, that males could score 75% on the pretest, and 77% on the post

in-VR questions if their diﬃculty levels were very diﬀerent. The apparent equalization of

male and female scores on the post in-VR questions thus strongly warrants additional study.

Although we ﬁnd evidence that VR training is beneﬁcial for student acclimation to VR-

based instruction, overall, we conclude that the VR-based instruction in this study has

essentially no eﬀect on student understanding overall. This is especially apparent if the ﬁnal

goal is for students to answer inherently 3D questions on 2D media like paper exams and

computer screens. In this sense, our results agree with other large studies like Smith et al.1

(on electrostatics), Madden et al.21,22 (on moon phases), Porter et al.26 (on magnetostatics),

and Brown et al.33 (engineering) that VR-based instruction is not more eﬀective than other

media at teaching inherently 3D topics. Since interest in VR-based instruction in physics is

unlikely to subside, one takeaway from our study (borne out particularly in Fig. 7) is that

VR training does seem to positively aﬀect all students’ ability to learn in a VR environment.

This is true even for students who are not gamers, who, in our data, are more likely to be

women.

The custom-designed assessment for electrostatics is adequately reliable for the purposes

of this preliminary study, it has not been independently validated, and it could be improved

in future work. A validated assessment would be especially useful for interpreting puz-

zling results like the diﬀerences we found between VR-based assessments of electrostatics

knowledge versus assessments that were done on a typical computer. Having a validated in-

strument for electrostatics would clearly be a great beneﬁt to the VR education community

in much the same way that the Purdue Spatial Rotations test34 has been for other studies

(e.g. Brown et al.33).

18

ACKNOWLEDGMENTS

The development of VR visualizations has been supported by the OSU STEAM Factory,

OSU’s Marion campus, and the Oﬃce for Distance Education and eLearning (ODEE).

∗ porter.284@osu.edu; permanent address: 191 W. 19th Ave, Columbus, OH, USA

† brown.4972@osu.edu

‡ ayers.224@osu.edu

1 Joseph R. Smith, Amber Byrum, Timothy M. McCormick, Nick Young, Chris Orban, and

Chris D. Porter. A controlled study of stereoscopic virtual reality in freshman electrostatics.

In Physics Education Research Conference 2017, PER Conference, pages 376–379, Cincinnati,

OH, July 26-27 2017.

2 Marilyn C. Salzman, R. Bowen Loftin, Chris Dede, and Deirdre McGlynn. Sciencespace: Lessons

for designing immersive virtual realities. In Conference Companion on Human Factors in Com-

puting Systems, CHI ’96, pages 89–90, New York, NY, USA, 1996. ACM.

3 Christopher J. Dede, Marilyn C. Salzman, and R. Bowen Loftin. The development of a virtual

world for learning newtonian mechanics. In Selected Papers from the First International Con-

ference on Hypermedia, Multimedia, and Virtual Reality: Models, Systems, and Applications,

MHVR ’94, pages 87–106, London, UK, UK, 1996. Springer-Verlag.

4 Alan B. Craig, William R. Sherman, and Jeﬀrey D. Will. Chapter 6 - education applications.

In Alan B. CraigWilliam R. ShermanJeﬀrey D. Will, editor, Developing Virtual Reality Appli-

cations, pages 189 – 237. Morgan Kaufmann, Boston, 2009.

5 J. Pirker, I. Lesjak, and C. Guetl. Maroon vr: A room-scale physics laboratory experience. In

2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT), pages

482–484, July 2017.

6 W. Hu, J. Jiang, and L. Shi. Electromagnetism experiment simulation based on game engine. In

2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),

pages 781–785, June 2018.

7 X. Liu, Y. Liu, and Y. Wang. Real time 3d magnetic ﬁeld visualization based on augmented

reality. In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pages 1052–

19

1053, March 2019.

8 J. Franklin and Andrew Ryder. Electromagnetic ﬁeld visualization in virtual reality. American

Journal of Physics, 87(2):153–157, 2019.

9 Scott W Greenwald, Wiley Corning, Gavin McDowell, Pattie Maes, and John Belcher. Elec-

trovr: An electrostatic playground for collaborative, simulation-based exploratory learning in

immersive virtual reality. Proceedings of the International Conference on Computer Supported

Collaborative Learning, 2019.

10 Physics Education Research Conference: Session presider: Jared Canright. Teaching 3d physical

concepts using virtual and augmented reality. https://www.compadre.org/per/perc/2019/

Detail.cfm?ID=7712, 2019.

11 In this article we do not say much about Augmented Reality (AR). While AR technology

is maturing rapidly and holds great potential for classroom impact, it remains true that AR

technology that provides stereoscopic depth perception is still very expensive and not within

access of most instructors.

12 D. Demaree, S. Stonebraker, W. Zhao, and L. Bao. Virtual reality in introductory physics labo-

ratories. In J. Marx, P. Heron, and S. Franklin, editors, 2004 Physics Education Research Con-

ference, volume 790 of American Institute of Physics Conference Series, pages 93–96, September

2005.

13 Peter E. Johnson, Jeﬀrey D. Will, and Christopher R. Graunke. Virtual reality for 3D visual-

ization in a statics course. Proceedings of the 2005 American Society for Engineering Education

Annual Conference & Exposition, 2005.

14 Jeﬀrey Will and Eric Johnson. Scientiﬁc Visualization For Undergraduate Education. Proceed-

ings of the 2004 American Society for Engineering Education Annual Conference & Exposition,

2004.

15 S. Zhou, J. Han, N. Pelz, X. Wang, L. Peng, H. Xiao, and L. Bao. Inquiry style interactive

virtual experiments: a case on circular motion. European Journal of Physics, 32:1597–1606,

November 2011.

16 Roxana Moreno and Richard E. Mayer. Personalized Messages That Promote Science Learning

in Virtual Environments. Journal of Educational Psychology, 96(1):165–173, 2004.

17 William Winn and William Bricken. Designing virtual worlds for use in mathematics education:

The example of experimental algebra. Educ. Technol., XXXII(12):12–19, December 1992.

20

18 Hannes Kaufmann, Dieter Schmalstieg, and Michael Wagner. Construct3d: A virtual reality

application for mathematics and geometry education. EDUCATION AND INFORMATION

TECHNOLOGIES, 5:263–276, 2000.

19 Hannes Kaufmann and Dieter Schmalstieg. Mathematics and geometry education with collab-

orative augmented reality. Computers & Graphics, 27(3):339 – 345, 2003.

20 Jorge Trindade, Carlos Fiolhais, and Leandro Almeida. Science learning in virtual environments:

a descriptive study. British Journal of Educational Technology, 33(4):471–488, September 2002.

21 Jack H. Madden, Andrea Stevenson Won, Jonathon P. Schuldt, Byungdoo Kim, Swati Pandita,

Yilu Sun, T. J. Stone, and Natasha Holmes. Virtual reality as a teaching tool for moon phases

and beyond.

In Physics Education Research Conference 2018, Washington, DC, August 1-2

2018.

22 J. Madden, S. Pandita, J. P. Schuldt, B. Kim, A. S. Won, and N. G. Holmes. Ready student

one: Exploring the predictors of student learning in virtual reality. https://arxiv.org/abs/

1910.10939, 2019.

23 Google cardboard website.

https://vr.google.com/cardboard/.

[Online. accessed 28-

October-2015].

24 P. Blanco, G. Windmiller, W. Welsh, and S. Hauze. Lessons Learned from Teaching Astronomy

with Virtual Reality. In G. Schultz, J. Barnes, and L. Shore, editors, Advancing Astronomy for

All: ASP 2018, volume 524 of Astronomical Society of the Paciﬁc Conference Series, page 159,

November 2019.

25 Randy Pausch, Dennis Proﬃtt, and George Williams. Quantifying immersion in virtual re-

ality.

In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive

Techniques, SIGGRAPH 97, page 1318, USA, 1997. ACM Press/Addison-Wesley Publishing

Co.

26 Chris D. Porter, Jonathon R. Brown, Joseph R Smith, Erik M. Stagar, Amber Simmons, Megan

Nieberding, Abigail Ayers, and Christopher Orban. A controlled study of virtual reality in ﬁrst-

year magnetostatics. In Physics Education Research Conference 2019, PER Conference, Provo,

UT, July 24-25 2019.

27 Scott W. Greenwald, Wiley Corning, Markus Funk, and Pattie Maes. Comparing learning in

virtual reality with learning on a 2d screen using electrostatics activities. Journal of Universal

Computer Science, 24(2):220–245, feb 2018. http://www.jucs.org/jucs_24_2/comparing_

21

learning_in_virtual.

28 Chris D. Porter and Bart Snapp. Impacts of smartphone based virtual reality projects in calculus

3. submitted to J. Res. Math. Ed., 2019.

29 Zahira Merchant, Ernest T. Goetz, Lauren Cifuentes, Wendy Keeney-Kennicutt, and Trina J.

Davis. Eﬀectiveness of virtual reality-based instruction on students’ learning outcomes in k-12

and higher education: A meta-analysis. Computers & Education, 70:29 – 40, 2014.

30 Angus P.R. Johnston, James Rae, Nicholas Ariotti, Benjamin Bailey, Andrew Lilja, Robyn

Webb, Charles Ferguson, Sheryl Maher, Thomas P. Davis, Richard I. Webb, John McGhee, and

Robert G. Parton. Journey to the centre of the cell: Virtual reality immersion into scientiﬁc

data. Traﬃc, 19(2):105–110, 2018.

31 Unity website. http://www.unity3d.com. [Online. accessed 28-October-2016].

32 Bandura A. Self-eﬃcacy: Toward a unifying theory of behavioral change. Psychological Review,

84(191), 1977.

33 Jonathan R. Brown, Irina Kuznetcova, Ethan Kirk Andersen, Nick H Abbott, Deborah M.

Grzybowski, and Christopher D. Porter. Implementing classroom-scale virtual reality into a

freshman engineering visuospatial skills course. In 2019 FYEE Conference, Penn State Univer-

sity , Pennsylvania, July 2019. ASEE Conferences. https://peer.asee.org/33701.

34 Roland B. Guay. Purdue spatial visualization test - visualization of rotations. Purdue Research

Foundation, 1977.

22

