2
2
0
2

n
a
J

7

]

G
L
.
s
c
[

3
v
5
4
8
7
0
.
2
0
1
2
:
v
i
X
r
a

MARINA: Faster Non-Convex Distributed Learning with Compression

Eduard Gorbunov 1 2 3 Konstantin Burlachenko 3 Zhize Li 3 Peter Richt´arik 3

Abstract

We develop and analyze MARINA: a new com-
munication efﬁcient method for non-convex dis-
tributed learning over heterogeneous datasets. MA-
RINA employs a novel communication compres-
sion strategy based on the compression of gradi-
ent differences that is reminiscent of but different
from the strategy employed in the DIANA method
of Mishchenko et al. (2019). Unlike virtually
all competing distributed ﬁrst-order methods, in-
cluding DIANA, ours is based on a carefully de-
signed biased gradient estimator, which is the
key to its superior theoretical and practical perfor-
mance. The communication complexity bounds
we prove for MARINA are evidently better than
those of all previous ﬁrst-order methods. Further,
we develop and analyze two variants of MARINA:
VR-MARINA and PP-MARINA. The ﬁrst method
is designed for the case when the local loss func-
tions owned by clients are either of a ﬁnite sum
or of an expectation form, and the second method
allows for a partial participation of clients – a
feature important in federated learning. All our
methods are superior to previous state-of-the-art
methods in terms of oracle/communication com-
plexity. Finally, we provide a convergence anal-
ysis of all methods for problems satisfying the
Polyak-Łojasiewicz condition.

1. Introduction

Non-convex optimization problems appear in various appli-
cations of machine learning, such as training deep neural
networks (Goodfellow et al., 2016) and matrix completion
and recovery (Ma et al., 2018; Bhojanapalli et al., 2016). Be-
cause of their practical importance, these problems gained
much attention in recent years, which led to a rapid develop-

1Moscow Institute of Physics and Technology, Moscow, Rus-
sia 2Yandex, Moscow, Russia 3King Abdullah University of Sci-
ence and Technology, Thuwal, Saudi Arabia. Correspondence
to: Eduard Gorbunov <eduard.gorbunov@phystech.edu>, Peter
Richt´arik <peter.richtarik@kaust.edu.sa>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

ment of new efﬁcient methods for non-convex optimization
problems (Danilova et al., 2020), and especially the training
of deep learning models (Sun, 2019).

Training deep neural networks is notoriously computation-
ally challenging and time-consuming. In the quest to im-
prove the generalization performance of modern deep learn-
ing models, practitioners resort to using increasingly larger
datasets in the training process, and to support such work-
loads, it is imperative to use advanced parallel and dis-
tributed hardware, systems, and algorithms. Distributed
computing is often necessitated by the desire to train models
from data naturally distributed across several edge devices,
as is the case in federated learning (Koneˇcn´y et al., 2016;
McMahan et al., 2017). However, even when this is not the
case, distributed methods are often very efﬁcient at reduc-
ing the training time (Goyal et al., 2017; You et al., 2020).
Due to these and other reasons, distributed optimization has
gained immense popularity in recent years.

However, distributed methods almost invariably suffer from
the so-called communication bottleneck: the communica-
tion cost of information necessary for the workers to jointly
solve the problem at hand is often very high, and depend-
ing on the particular compute architecture, workload, and
algorithm used, it can be orders of magnitude higher than
the computation cost. A popular technique for resolving
this issue is communication compression (Seide et al., 2014;
Koneˇcn´y et al., 2016; Suresh et al., 2017), which is based on
applying a lossy transformation/compression to the models,
gradients, or tensors to be sent over the network to save
on communication. Since applying a lossy compression
generally decreases the utility of the exchanged messages,
such an approach will typically lead to an increase in the
number of communications, and the overall usefulness of
this technique manifests itself in situations where the com-
munication savings are larger compared to the increased
need for the number of communication rounds (Horv´ath
et al., 2019).

The optimization and machine learning communities have
exerted considerable effort in recent years to design dis-
tributed methods supporting compressed communication.
From many methods proposed, we emphasize VR-DIANA
(Horv´ath et al., 2019), FedCOMGATE (Haddadpour et al.,
2020), and FedSTEPH (Das et al., 2020) because these pa-

 
 
 
 
 
 
MARINA: Faster Non-Convex Distributed Learning with Compression

pers contain the state-of-the-art results in the setup when
the local loss functions can be arbitrary heterogeneous.

1.1. Contributions

We propose several new distributed optimization methods
supporting compressed communication, speciﬁcally focus-
ing on smooth but nonconvex problems of the form

(cid:26)

min
x∈Rd

f (x) = 1
n

(cid:27)

fi(x)

,

n
(cid:80)
i=1

(1)

where n workers/devices/clients/peers are connected in a
centralized way with a parameter-server, and client i has an
access to the local loss function fi only. We establish strong
complexity rates for them and show that they are better than
previous state-of-the-art results.

• MARINA. The main contribution of our paper is a new
distributed method supporting communication compression
called MARINA (Alg 1). In this algorithm, workers apply an
unbiased compression operator to the gradient differences
at each iteration with some probability and send them to
the server that performs aggregation by averaging. Unlike
all known methods operating with unbiased compression
operators, this procedure leads to a biased gradient estima-
tor. We prove convergence guarantees for MARINA, which
are strictly better than previous state-of-the-art methods
(see Table 1). For example, MARINA’s rate O( 1+ω/
) is
ε2
O(
ω) times better than that of the state-of-the-art method
DIANA (Mishchenko et al., 2019), where ω is the variance
parameter associated with the deployed compressor. For
example, in the case of the Rand1 sparsiﬁcation compressor,
we have ω = d − 1, and hence we get an improvement
by the factor O(
d). Since the number d of features can
be truly very large when training modern models, this is a
substantial improvement that can even amount to several
orders of magnitude.

√

√

√

n

• Variance Reduction on Nodes. We generalize MARINA
to VR-MARINA, which can handle the situation when the
local functions fi have either a ﬁnite-sum (each fi is an
average of m functions) or an expectation form, and when it
is more efﬁcient to rely on local stochastic gradients rather
than on local gradients. When compared with MARINA, VR-
MARINA additionally performs local variance reduction on
all nodes, progressively removing the variance coming from
the stochastic approximation, leading to a better oracle com-
plexity than previous state-of-the-art results (see Table 1).
When no compression is used (i.e., ω = 0), the rate of VR-
√
m√
nε2 ), while the rate of the state-of-the-art
MARINA is O(
method VR-DIANA is O( m2/3
ε2 ). This is an improvement
nm1/6). When much compression is
by the factor O(
applied, and ω is large, our method is faster by the factor
O( m2/3+ω
m1/2+ω1/2 ). In the special case, when there is just a sin-

√

gle node (n = 1), and no compression is used, VR-MARINA
reduces to the PAGE method of Li et al. (2020); this is an
optimal ﬁrst-order algorithm for smooth non-convex ﬁnite-
sum/online optimization problems.

• Partial Participation. We develop a modiﬁcation of MA-
RINA allowing for partial participation of the clients, which
is a feature critical in federated learning. The resulting
method, PP-MARINA, has superior communication com-
plexity to the existing methods developed for this settings
(see Table 1).

• Convergence Under the Polyak-Łojasiewicz Condi-
tion. We analyze all proposed methods for problems sat-
isfying the Polyak-Łojasiewicz condition (Polyak, 1963;
Łojasiewicz, 1963). Again, the obtained results are strictly
better than previous ones (see Table 2). Statements and
proofs of all these results are in the Appendix.

• Simple Analysis. The simplicity and ﬂexibility of our
analysis offer several extensions. For example, one can
easily generalize our analysis to the case of different quan-
tization operators and different batch sizes used by clients.
Moreover, one can combine the ideas of VR-MARINA and
PP-MARINA and obtain a single distributed algorithm with
compressed communications, variance reduction on nodes,
and clients’ sampling. We did not do this to keep the expo-
sition simpler.

1.2. Related Work

Non-Convex Optimization. Since ﬁnding a global mini-
mum of a non-convex function is, in general, an NP-hard
problem (Murty & Kabadi, 1987), many researchers in non-
convex optimization focus on relaxed goals such as ﬁnding
an ε-stationary point. The theory of stochastic ﬁrst-order
methods for ﬁnding ε-stationary points is well-developed: it
contains lower bounds for expectation minimization without
smoothness of stochastic realizations (Arjevani et al., 2019)
and for ﬁnite-sum/expectation minimization (Fang et al.,
2018; Li et al., 2020) as well as optimal methods matching
the lower bounds (see (Danilova et al., 2020; Li et al., 2020)
for the overview). Recently, distributed variants of such
methods were proposed (Sun et al., 2020; Sharma et al.,
2019; Khanduri et al., 2020).

Compressed Communications. Works on distributed
methods supporting communication compression can be
roughly split into two large groups: the ﬁrst group focuses
on methods using unbiased compression operators (which
refer to as quantizations in this paper), such as RandK, and
the second one studies methods using biased compressors
such as TopK. One can ﬁnd a detailed summary of the most
popular compression operators in (Safaryan et al., 2020;
Beznosikov et al., 2020).

Unbiased Compression. In this line of work, the ﬁrst con-

MARINA: Faster Non-Convex Distributed Learning with Compression

Table 1: Summary of the state-of-the-art results for ﬁnding an ε-stationary point for the problem (1), i.e., such a point ˆx that
E (cid:2)(cid:107)∇f (ˆx)(cid:107)2(cid:3) ≤ ε2. Dependences on the numerical constants, “quality” of the starting point, and smoothness constants are omitted in
the complexity bounds. Abbreviations: “PP” = partial participation; “Communication complexity” = the number of communications
rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of (stochastic) ﬁrst-order oracle calls needed to ﬁnd
an ε-stationary point. Notation: ω = the quantization parameter (see Def. 1.1); n = the number of nodes; m = the size of the local
dataset; r = (expected) number of clients sampled at each iteration; b(cid:48) = the batchsize for VR-MARINA at the iterations with compressed
communication. To simplify the bounds, we assume that the expected density ζQ of the quantization operator Q (see Def. 1.1) satisﬁes
ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and (cid:96)2-quantization, see (Beznosikov et al., 2020)). We notice that (Haddadpour et al.,
2020) and (Das et al., 2020) contain also better rates under different assumptions on clients’ similarity.

Citation

Communication Complexity

Oracle Complexity

√

1+(1+ω)
ε2

ω/n

√

1+(1+ω)
ε2

ω/n

Setup

(1)

(1)+(5)

(1)+(6)

PP, (1)

Method

DIANA

FedCOMGATE (1)
FedSTEPH, r = n

MARINA (Alg. 1)

DIANA

VR-DIANA

(Mishchenko et al., 2019)
(Horv´ath et al., 2019)
(Li & Richt´arik, 2020)
(Haddadpour et al., 2020)

(Das et al., 2020)

Thm. 2.1 & Cor. 2.1 (NEW)

(Li & Richt´arik, 2020)

(Horv´ath et al., 2019)

VR-MARINA (Alg. 2), b(cid:48) = 1(2)

Thm. 3.1 & Cor. 3.1 (NEW)

DIANA (3)

FedCOMGATE (3)
VR-MARINA (Alg. 2), b(cid:48) = 1
(cid:16) 1
nε2

VR-MARINA (Alg. 2), b(cid:48) = Θ

(cid:17)

(Mishchenko et al., 2019)
(Li & Richt´arik, 2020)
(Haddadpour et al., 2020)

Thm. 3.2 & Cor. 3.2 (NEW)

Thm. 3.2 & Cor. 3.2 (NEW)

FedSTEPH

(Das et al., 2020)

PP-MARINA (Alg. 4)

Thm. 4.1 & Cor. 4.1 (NEW)

√

1+ω
ε2
1+ω/n
ε4
1+ω/
ε2
ω/n
(cid:17)√

√

n

√

1+ω
nε4
1+ω/n
ε4
1+ω/
ε2
ω/n
(cid:17)√

√

n

+ 1+ω
nε4
1+ω/n

1+(1+ω)
ε2
(cid:16)
m2/3+ω
ε2
√
(1+ω)m
ε2

ω,

(cid:110)

(cid:111)

/

√

1+max

n

√

+ 1+ω
nε4
1+ω/n

1+(1+ω)
ε2
(cid:16)
m2/3+ω
ε2
√
(1+ω)m
ε2

ω,

(cid:110)

(cid:111)

/

√

1+max

n

√

1+(1+ω)
ε2

ω/n

+ 1+ω
nε4

1+(1+ω)
ε2

ω/n

+ 1+ω
nε4

1+ω
ε2
√
n

√

n

1+ω
nε3

ω + 1+ω/
+
ε2
√
ω + 1+ω/
ε2
rε4 + (1+ω)(n−r)
r(n−1)ε4
1+(1+ω)
n/r
ε2

1+ω/n

√

1+ω
nε4
√
n

ω + 1+ω/
+
ε2
nε2 + 1+ω/
ω
nε4

√

√

1+ω
nε3
n

1+ω/n

rε4 + (1+ω)(n−r)
r(n−1)ε4
1+(1+ω)
n/r
ε2

√

(cid:80)n

(cid:104)(cid:13)
(cid:13) 1
n

i=1 Q(xj )(cid:13)
(cid:13)

(1) The results for FedCOMGATE are derived under assumption that for all vectors x1, . . . , xn ∈ Rd the quantization operator Q satisﬁes
≤ G for some constant G ≥ 0. In fact, this assumption does not hold for classical quantization
E
operators like RandK and (cid:96)2-quantization on Rd. The counterexample: n = 2 and x1 = −x2 = (t, t, . . . , t)(cid:62) with arbitrary large t > 0.
(2) One can even further improve the communication complexity by increasing b(cid:48).
(3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.

(cid:13)Q (cid:0) 1

2 − (cid:13)

i=1 xj

(cid:1)(cid:13)
(cid:13)

(cid:80)n

2(cid:105)

n

vergence result in the non-convex case was obtained by
Alistarh et al. (2017) for QSGD, under assumptions that
the local loss functions are the same for all workers, and
the stochastic gradient has uniformly bounded second mo-
ment. After that, Mishchenko et al. (2019) proposed DIANA
(and its momentum version) and proved its convergence rate
for non-convex problems without any assumption on the
boundedness of the second moment of the stochastic gradi-
ent, but under the assumption that the dissimilarity between
local loss functions is bounded. This restriction was later
eliminated by Horv´ath et al. (2019) for the variance reduced
version of DIANA called VR-DIANA, and the analysis was ex-
tended to a large class of unbiased compressors. Finally, the
results for QSGD and DIANA were recently generalized and
tightened by Li & Richt´arik (2020) in a unifying framework
that included many other methods as well.

SGD) for non-convex problems was obtained by Karim-
ireddy et al. (2019) for homogeneous problems under the
assumption that the second moment of the stochastic gradi-
ent is uniformly bounded. The last assumption was recently
removed from the analysis of EC-SGD by Stich & Karim-
ireddy (2020); Beznosikov et al. (2020), while the ﬁrst re-
sults without the homogeneity assumption were obtained
by Koloskova et al. (2020a) for Choco-SGD, but still under
the assumption that the second moment of the stochastic
gradient is uniformly bounded. This issue was resolved by
Beznosikov et al. (2020). In general, the current understand-
ing of optimization methods with biased compressors is far
from complete: even in the strongly convex case, the ﬁrst
linearly converging (Gorbunov et al., 2020) and accelerated
(Qian et al., 2020) error-compensated stochastic methods
were proposed just recently.

Biased Compression. Biased compression operators are
less “optimization-friendly” than unbiased ones. Indeed,
one can construct a simple convex quadratic problem for
which distributed SGD with Top1 compression diverges ex-
ponentially fast (Beznosikov et al., 2020). However, this
issue can be resolved using error compensation (Seide et al.,
2014). The ﬁrst analysis of error-compensated SGD (EC-

Other Approaches. Besides communication compression,
there are also different techniques aiming to reduce the
overall communication cost of distributed methods. The
most popular ones are based on decentralized communi-
cations and multiple local steps between communication
rounds, where the second technique is very popular in feder-
ated learning (Koneˇcn´y et al., 2016; Kairouz et al., 2019).

MARINA: Faster Non-Convex Distributed Learning with Compression

Table 2: Summary of the state-of-the-art results for ﬁnding an ε-solution for the problem (1) satifying Polyak-Łojasiewicz condition
(see As. 2.1), i.e., such a point ˆx that E [f (ˆx) − f (x∗)] ≤ ε. Dependences on the numerical constants and log(1/ε) factors are omitted
and all smoothness constanst are denoted by L in the complexity bounds. Abbreviations: “PP” = partial participation; “Communication
complexity” = the number of communications rounds needed to ﬁnd an ε-stationary point; “Oracle complexity” = the number of
(stochastic) ﬁrst-order oracle calls needed to ﬁnd an ε-stationary point. Notation: ω = the quantization parameter (see Def. 1.1); n = the
number of nodes; m = the size of the local dataset; r = (expected) number of clients sampled at each iteration; b(cid:48) = the batchsize for
VR-MARINA at the iterations with compressed communication. To simplify the bounds, we assume that the expected density ζQ of the
quantization operator Q (see Def. 1.1) satisﬁes ω + 1 = Θ(d/ζQ) (e.g., this holds for RandK and (cid:96)2-quantization, see (Beznosikov et al.,
2020)). We notice that (Haddadpour et al., 2020) and (Das et al., 2020) contain also better rates under different assumptions on clients’
similarity.

Setup

(1)

Method

Citation

DIANA
FedCOMGATE (1)
MARINA (Alg. 1)

(Li & Richt´arik, 2020)

(Haddadpour et al., 2020)

Thm. 2.2 & Cor. C.2 (NEW)

DIANA

(Li & Richt´arik, 2020)

(1)+(5)

VR-DIANA

(Li & Richt´arik, 2020)

VR-MARINA (Alg. 2), b(cid:48) = 1(2)

Thm. D.2 & Cor. D.2 (NEW)

DIANA (3)

FedCOMGATE (3)

(Mishchenko et al., 2019)
(Li & Richt´arik, 2020)
(Haddadpour et al., 2020)

VR-MARINA (Alg. 2), b(cid:48) = 1

Thm. D.4 & Cor. D.4 (NEW)

VR-MARINA (Alg. 2), b(cid:48) = Θ

(cid:17)

(cid:16) 1
nµε

Thm. D.4 & Cor. D.4 (NEW)

FedSTEPH (4)

(Das et al., 2020)

(1)+(6)

PP, (1)

Communication Complexity

L(1+(1+ω)

ω/n)

√

Oracle Complexity

√

L(1+(1+ω)

ω/n)

L(1+max

ω,

(1+ω)m

/

n)

L(1+max

ω,

(1+ω)m

(cid:111)

√

(cid:111)

√

/

n)

µ
L(1+ω)
µ

√

n)

ω + L(1+ω/
µ

√

L(1+(1+ω)

L

µ
+ L(1+ω)
nµ
(cid:17)√
(cid:16)
m2/3+ω
µ
√

(cid:110)

ω/n)
+
(cid:16) L
µ + 1

ε

(cid:17)

1+ω/n

√

1+(1+ω)
ε2

µ

ω/n

+ 1+ω
nε4

ω + m+

+

L(1+ω)
µ

ω + 1
nµε +
(cid:18)

+ L
µ

1 + ω√
n

+

(cid:19)

(cid:113) ω

n2µε
n)

√

ω + L(1+ω/
µ
(cid:17)3/2

(cid:16) L
µ

√

µ
L(1+ω)
nµε

√

n)

ω + L(1+ω/
µ

√

L(1+(1+ω)

L

µ
+ L(1+ω)
nµ
(cid:17)√
(cid:16)
m2/3+ω
µ
√

(cid:110)

ω/n)
+
(cid:16) L
µ + 1

ε

(cid:17)

1+ω/n

√

µ

ω/n

1+(1+ω)
ε2

+ 1+ω
nε4

ω + m+

+

L(1+ω)
nµε

ω + 1
nµε +
(cid:18)

+ L
µ

1 + ω√
n

+

ω

nµε + L(1+ω/
nµ2ε
(cid:16) L
µ

(cid:17)3/2

(cid:19)

(cid:113) ω

n2µε
n)

√

√

n/r)

PP-MARINA (Alg. 4)

Thm. E.2 & Cor. E.2 (NEW)

(ω+1)n
r

+ L(1+(1+ω)

µ

n/r)

(ω+1)n
r

+ L(1+(1+ω)

µ

(cid:80)n

2 − (cid:13)

(cid:104)(cid:13)
(cid:13) 1
n

(cid:13)Q (cid:0) 1

i=1 Q(xj )(cid:13)
(cid:13)

for FedCOMGATE are derived under assumption that

(1) The results
E
RandK and (cid:96)2-quantization on Rd. The counterexample: n = 2 and x1 = −x2 = (t, t, . . . , t)(cid:62) with arbitrary large t > 0.
(2) One can even further improve the communication complexity by increasing b(cid:48).
(3) No assumptions on the smoothness of the stochastic realizations fξ(x) are used.
(4) The rate is derived under assumption that r = Ω((1 + ω)(cid:112)L/µ log(1/ε)).

for all vectors x1, . . . , xn

i=1 xj

(cid:1)(cid:13)
(cid:13)

(cid:80)n

2(cid:105)

n

Rd the quantization operator Q satisﬁes
≤ G for some constant G ≥ 0. In fact, this assumption does not hold for classical quantization operators like

∈

One can ﬁnd the state-of-the-art distributed optimization
methods using these techniques and their combinations in
(Lian et al., 2017; Karimireddy et al., 2020; Li et al., 2019;
Koloskova et al., 2020b). Moreover, there exist results
based on the combinations of communication compression
with either decentralized communication, e.g., Choco-SGD
(Koloskova et al., 2020a), or local updates, e.g., Qsparse-
Local-SGD (Basu et al., 2019), FedCOMGATE (Haddadpour
et al., 2020), FedSTEPH (Das et al., 2020), where in (Basu
et al., 2019) the convergence rates were derived under an as-
sumption that the stochastic gradient has uniformly bounded
second moment and the results for Choco-SGD, FedCOM-
GATE, FedSTEPH were described either earlier in the text,
or in Table 1.

1.3. Preliminaries

We will rely on two key assumptions thrughout the text.
Assumption 1.1 (Uniform lower bound). There exists f∗ ∈
R such that f (x) ≥ f∗ for all x ∈ Rd.
Assumption 1.2 (L-smoothness). We assume that fi is Li-

smooth for all i ∈ [n] = {1, 2, . . . , n} meaning that the
following inequality holds ∀x, y ∈ Rd, ∀i ∈ [n]:

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ Li(cid:107)x − y(cid:107).

(2)

This assumption implies that f is Lf -smooth with L2
L2 = 1
n

i=1 L2
i .

(cid:80)n

f ≤

Finally, we describe a large class of unbiased compression
operators satisfying a certain variance bound, which we will
refer to, in this paper, by the name quantization.
Deﬁnition 1.1 (Quantization). We say that a stochastic map-
ping Q : Rd → Rd is a quantization operator/quantization
if there exists ω > 0 such that for any x ∈ Rd , we have
E [Q(x)] = x, E (cid:2)(cid:107)Q(x) − x(cid:107)2(cid:3) ≤ ω(cid:107)x(cid:107)2.

(3)

For the given quantization operator Q(x), we deﬁne the
the expected density as ζQ = supx∈Rd E [(cid:107)Q(x)(cid:107)0] , where
(cid:107)y(cid:107)0 is the number of non-zero components of y ∈ Rd.

Notice that the expected density is well-deﬁned for any
quantization operator since (cid:107)Q(x)(cid:107)0 ≤ d.

MARINA: Faster Non-Convex Distributed Learning with Compression

2. MARINA

In this section, we describe the main algorithm of this work:
MARINA (see Algorithm 1). At each iteration of MARINA,
each worker i either sends to the server the dense vector
∇fi(xk+1) with probability p, or it sends the quantized
gradient difference Q (cid:0)∇fi(xk+1) − ∇fi(xk))(cid:1) with prob-
ability 1−p. In the ﬁrst situation, the server just averages the
vectors received from workers and gets gk+1 = ∇f (xk+1),
whereas in the second case, the server averages the quan-
tized differences from all workers and then adds the result
to gk to get gk+1. Moreover, if Q is identity quantization,
i.e., Q(x) = x, then MARINA reduces to Gradient Descent
(GD).

Algorithm 1 MARINA
1: Input: starting point x0, stepsize γ, probability p ∈

(0, 1], number of iterations K

2: Initialize g0 = ∇f (x0)
3: for k = 0, 1, . . . , K − 1 do
Sample ck ∼ Be(p)
4:
Broadcast gk to all workers
5:
for i = 1, . . . , n in parallel do
6:
7:
8:

xk+1 = xk − γgk
Set gk+1
= ∇fi(xk+1) if ck = 1, and gk+1
i
gk + Q (cid:0)∇fi(xk+1) − ∇fi(xk))(cid:1) otherwise

i

=

(cid:80)n

i=1 gk+1

i

end for
gk+1 = 1
n

9:
10:
11: end for
12: Return:
{xk}K−1
k=0

ˆxK chosen uniformly at random from

However, for non-trivial quantizations, we have E[gk+1 |
xk+1] (cid:54)= ∇f (xk+1) unlike all other distributed methods
using exclusively unbiased compressors we know of. That
is, gk+1 is a biased stochastic estimator of ∇f (xk+1). How-
ever, MARINA is an example of a rare phenomenon in
stochastic optimization when the bias of the stochastic gra-
dient helps to achieve better complexity.

2.1. Convergence Results for Generally Non-Convex

Problems

We start with the following result.

Theorem 2.1. Let Assumptions 1.1 and 1.2 be satisﬁed.
Then, after

K = O

(cid:18)

(cid:18)

1 +

(cid:113) (1−p)ω
pn

∆0L
ε2

(cid:19)(cid:19)

iterations with ∆0 = f (x0) − f∗, L2 = 1
n
the stepsize γ ≤ L−1 (cid:16)
(cid:17)−1
duces point ˆxK for which E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2.

1 + (cid:112)(1−p)ω/(pn)

(cid:80)n

i=1 L2
i and
, MARINA pro-

One can ﬁnd the full statement of the theorem together with
its proof in Section C.1 of the Appendix.

The following corollary provides the bounds on the number
of iterations/communication rounds and estimates the total
communication cost needed to achieve an ε-stationary point
in expectation. Moreover, for simplicity, throughout the
paper we assume that the communication cost is propor-
tional to the number of non-zero components of transmitted
vectors from workers to the server.

Corollary 2.1. Let the assumptions of Theorem 2.1 hold
and p = ζQ/d. If γ ≤ L−1 (cid:16)
MARINA requires

1 + (cid:112)ω(d−ζQ)/(nζQ)

, then

(cid:17)−1

(cid:18)

O

∆0L
ε2

(cid:18)

(cid:114)

1 +

(cid:16) d
ζQ

ω
n

(cid:17)(cid:19)(cid:19)

− 1

iterations/communication rounds in order to achieve
E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communica-
tion cost per worker is O(d + ζQK).

Let us clarify the obtained result. First of all, if ω = 0
(no quantization), then ζQ = 0 and the rate coincides with
the rate of Gradient Descent (GD). Since GD is optimal
among ﬁrst-order methods in terms of reducing the norm
of the gradient (Carmon et al., 2019), the dependence on
ε in our bound cannot be improved in general. Next, if n
is large enough, i.e., n ≥ ω(d/ζQ − 1), then1 the iteration
complexity of MARINA (method with compressed commu-
nications) and GD (method with dense communications)
coincide. This means that in this regime, MARINA is able
to reach a provably better communication complexity than
GD!

2.2. Convergence Results Under Polyak-Łojasiewicz

condition

In this section, we provide a complexity bounds for MARINA
under the Polyak-Łojasiewicz (PŁ) condition.

Assumption 2.1 (PŁ condition). Function f satisﬁes
Polyak-Łojasiewicz (PŁ) condition with parameter µ, i.e.,

(cid:107)∇f (x)(cid:107)2 ≥ 2µ (f (x) − f (x∗)) .

(4)

holds for x∗ = arg minx∈Rd f (x) and for all x ∈ Rd.

Under this and previously introduced assumptions, we de-
rive the following result.

Theorem 2.2. Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed.
Then, after

(cid:18)

(cid:26)

K = O

max

(cid:18)

1 +

(cid:113) (1−p)ω
pn

(cid:19)(cid:27)

log ∆0
ε

(cid:19)

1

p , L

µ

1For (cid:96)2-quantization this requirement is satisﬁed when n ≥ d.

MARINA: Faster Non-Convex Distributed Learning with Compression

iterations
L2
=
L−1 (cid:16)

min

(cid:26)

with ∆0
=
(cid:80)n
1
i=1 L2
and
i
n
1 + (cid:112)2(1−p)ω/(pn)

f (x0) − f (x∗),
stepsize
≤
(cid:27)

γ

, p(2µ)−1

, MARINA

the

(cid:17)−1

produces a point xK for which E[f (xK) − f (x∗)] ≤ ε.

One can ﬁnd the full statement of the theorem together with
its proof in Section C.2 of the Appendix.

3. Variance Reduction

Throughout this section, we assume that the local loss on
each node has either a ﬁnite-sum form (ﬁnite sum case),

E

n = 1, then MARINA reduces to the optimal method PAGE
(Li et al., 2020).

In this part, we will rely on the following average smooth-
ness assumption.
Assumption 3.1 (Average L-smoothness). For all k ≥ 0
and i ∈ [n] the minibatch stochastic gradients difference
(∇fij(xk+1) − ∇fij(xk)) computed on
(cid:101)∆k
i | xk, xk+1(cid:105)
(cid:104)
(cid:101)∆k
(cid:21)

the i-th worker satisﬁes E

i = 1
b(cid:48)

i and

= ∆k

j∈I (cid:48)

(cid:80)

i,k

| xk, xk+1

≤ L2

i

b(cid:48) (cid:107)xk+1 − xk(cid:107)2

(7)

(cid:20)(cid:13)
(cid:13)
(cid:13) (cid:101)∆k

i − ∆k
i

(cid:13)
2
(cid:13)
(cid:13)

fi(x) = 1
m

m
(cid:80)
j=1

fij(x),

or an expectation form (online case),

fi(x) = Eξi∼Di[fξi(x)].

3.1. Finite Sum Case

(5)

(6)

with some Li ≥ 0, where ∆k

i = ∇fi(xk+1) − ∇fi(xk).

This assumption is satisﬁed in many standard minibatch
regimes. In particular, if I (cid:48)
i,k = {1, . . . , m}, then Li = 0,
i,k consists of b(cid:48) i.i.d. samples from the uniform
and if I (cid:48)
distributions on {1, . . . , m} and fij are Lij-smooth, then
Li ≤ maxj∈[m] Lij.

In this section, we generalize MARINA to problems of the
form (1)+(5), obtaining VR-MARINA (see Algorithm 2). At

Algorithm 2 VR-MARINA: ﬁnite sum case
1: Input: starting point x0, stepsize γ, minibatch size b(cid:48),

probability p ∈ (0, 1], number of iterations K

2: Initialize g0 = ∇f (x0)
3: for k = 0, 1, . . . , K − 1 do
Sample ck ∼ Be(p)
4:
Broadcast gk to all workers
5:
for i = 1, . . . , n in parallel do
6:
7:
8:

xk+1 = xk − γgk
Set gk+1
= ∇fi(xk+1) if ck = 1, and gk+1
i
(cid:16) 1
gk + Q
j∈I (cid:48)
b(cid:48)
otherwise, where I (cid:48)
the minibatch, |I (cid:48)

=
(cid:17)
(∇fij(xk+1) − ∇fij(xk))
i,k is the set of the indices in

(cid:80)

i,k

i,k| = b(cid:48)

i

(cid:80)n

i=1 gk+1

i

end for
gk+1 = 1
n

9:
10:
11: end for
12: Return:
{xk}K−1
k=0

ˆxK chosen uniformly at random from

each iteration of VR-MARINA, devices are to compute the
full gradients ∇fi(xk+1) and send them to the server with
probability p. Typically, p ≤ 1/m and m is large, meaning
that workers compute full gradients rarely (once per ≥ m
iterations in expectation). At other iterations, workers com-
pute minibatch stochastic gradients evaluated at the current
and previous points, compress them using an unbiased com-
pression operator, i.e., quantization/quantization operator,
and send the resulting vectors gk+1
i −gk to the server. More-
over, if Q is the identity quantization, i.e., Q(x) = x, and

Under this and the previously introduced assumptions, we
derive the following result.
Theorem 3.1. Consider the ﬁnite sum case (1)+(5). Let
Assumptions 1.1, 1.2 and 3.1 be satisﬁed. Then, after
(cid:17)(cid:19)(cid:19)

(cid:114)

(cid:18)

(cid:18)

(cid:16)

∆0
ε2

L +

1−p
pn

ωL2 + (1+ω)L2

b(cid:48)

K = O

iterations with ∆0 = f (x0) − f∗, L2 = 1
n
L2
i=1 L2
stepsize
the
=
i
(cid:113)(cid:0)ωL2 + (1+ω)L2/b(cid:48)(cid:1) (1−p)/(pn)
(cid:17)−1
(cid:16)

L +

(cid:80)n

and

1
n

, VR-MARINA

(cid:80)n

i=1 L2
i ,
≤
γ

produces such a point ˆxK that E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2.

One can ﬁnd the full statement of the theorem together with
its proof in Section D.1.1 of the Appendix.
Corollary 3.1. Let the assumptions of Theorem 3.1 hold
and p = min {ζQ/d, b(cid:48)/(m+b(cid:48))}, where b(cid:48) ≤ m.
If γ ≤
(cid:18)
(cid:113)(cid:0)ωL2 + (1+ω)L2/b(cid:48)(cid:1) max{d/ζQ−1,m/b(cid:48)}/n

(cid:19)−1

then

L +

VR-MARINA requires

(cid:32)

(cid:32)

(cid:32)

O

∆0
ε2

L

1 +

(cid:113) ω max{d/ζQ−1,m/b(cid:48)}

(cid:33)

n

(cid:113) (1+ω) max{d/ζQ−1,m/b(cid:48)}
nb(cid:48)

+L

(cid:33)(cid:33)

and O (m + b(cid:48)K)
iterations/communication
stochastic oracle calls per node in expectation in order
to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total
communication cost per worker is O(d + ζQK).

rounds

First of all, when workers quatize differences of the full
gradients, then I (cid:48)
i,k = {1, . . . , m} for all i ∈ [n] and k ≥ 0,

MARINA: Faster Non-Convex Distributed Learning with Compression

implying L = 0. In this case, the complexity bounds for VR-
MARINA recover the ones for MARINA. Next, when ω = 0
(no quantization) and n = 1, our bounds for iteration and
oracle complexities for VR-MARINA recover the bounds for
PAGE (Li & Richt´arik, 2020), which is optimal for ﬁnite-
sum smooth non-convex optimization. This observation
implies that the dependence on ε and m in the complexity
bounds for VR-MARINA cannot be improved in the class
of ﬁrst-order stochastic methods. Next, we notice that up
to the differences in smoothness constants, the iteration
and oracle complexities for VR-MARINA beneﬁt from the
number of workers n. Finally, as Table 1 shows, the rates
for VR-MARINA are strictly better than ones for the previous
state-of-the-art method VR-DIANA (Horv´ath et al., 2019).

We provide the convergence results for VR-MARINA in the
ﬁnite-sum case under the Polyak-Łojasiewicz condition,
together with complete proofs, in Section D.1.2 of the Ap-
pendix.

3.2. Online Case

(cid:80)

In this section, we focus on problems of type (1)+(6).
For
this type of problems, we consider a slightly
modiﬁed version of VR-MARINA. That is, we replace
line 8 in Algorithm 2 with the following update rule:
gk+1
=
i
gk + Q
other-
j∈I (cid:48)
wise, where Ii,k, I (cid:48)
i,k are the sets of the indices in the mini-
batches, |Ii,k| = b, |I (cid:48)
ij is independently
sampled from Di for i ∈ [n], j ∈ [m] (see Algorithm 3 in
the Appendix).

(xk+1) if ck = 1, and gk+1

i,k| = b(cid:48), and ξk

= 1
b
(cid:16) 1
b(cid:48)

(xk+1) − ∇fξk

j∈Ii,k
(cid:80)

(∇fξk

(xk))

∇fξk

(cid:17)

i,k

ij

ij

ij

i

Before we provide our convergence results in this setup, we
reformulate Assumption 3.1 for the online case.

Assumption 3.2 (Average L-smoothness). For all k ≥ 0
and i ∈ [n] the minibatch stochastic gradients difference
(cid:101)∆k
(xk)) computed on

(xk+1) − ∇fξk

(∇fξk

i = 1
b(cid:48)

j∈I (cid:48)

(cid:80)

i,k

ij

ij

the i-th worker satisﬁes E

i | xk, xk+1(cid:105)
(cid:104)
(cid:101)∆k

= ∆k

i and

(cid:20)(cid:13)
(cid:13)
(cid:13) (cid:101)∆k

i − ∆k
i

(cid:13)
2
(cid:13)
(cid:13)

E

(cid:21)

| xk, xk+1

≤ L2

i

b(cid:48) (cid:107)xk+1 − xk(cid:107)2

(8)

with some Li ≥ 0, where ∆k

i = ∇fi(xk+1) − ∇fi(xk).

Moreover, we assume that the variance of the stochastic
gradients on all nodes is uniformly upper bounded.

Assumption 3.3. We assume that for all i ∈ [n] there exists
such constant σi ∈ [0, +∞) that for all x ∈ Rd

Under these and previously introduced assumptions, we
derive the following result.
Theorem 3.2. Consider the online case (1)+(6). Let As-
sumptions 1.1, 1.2, 3.2 and 3.3 be satisﬁed. Then, after

K = O

(cid:18)

(cid:18)

∆0
ε2

L +

(cid:114)

(cid:16)

1−p
pn

ωL2 + (1+ω)L2

b(cid:48)

(cid:17)(cid:19)(cid:19)

=

=

(cid:80)n

iterations with ∆0
1
n

i=1 L2
i , L2
(cid:16)

f (x0) − f∗, L2
(cid:80)n
the
(cid:113)(cid:0)ωL2 + (1+ω)L2/b(cid:48)(cid:1) (1−p)/(pn)

γ ≤
b = Θ (cid:0)σ2/(nε2)(cid:1) , σ2 = 1
produces a point ˆxK for which E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2.

, and
i , VR-MARINA

=
stepsize

i=1 L2
i ,

i=1 σ2

(cid:17)−1

L +

(cid:80)n

1
n

n

3.2.

One can ﬁnd the full statement of the theorem, together with
its proof, in Section D.2.1 of the Appendix.
Theo-
Let
Corollary
rem 3.2 hold and choose p = min {ζQ/d, b(cid:48)/(b+b(cid:48))},
where b(cid:48) ≤ b,
If γ ≤
(cid:18)
(cid:19)−1

b = Θ (cid:0)σ2/(nε2)(cid:1).

(cid:113)(cid:0)ωL2 + (1+ω)L2/b(cid:48)(cid:1) max{d/ζQ−1,b/b(cid:48)}/n

assumptions

L +

the

of

,

then VR-MARINA requires
(cid:32)

(cid:32)

(cid:32)

(cid:114)

O

∆0
ε2

L

1 +

ω
n max

(cid:110) d
ζQ

− 1, σ2
nb(cid:48)ε2

(cid:33)

(cid:111)

(cid:114)

+L

(1+ω)
nb(cid:48) max

(cid:110) d
ζQ

− 1, σ2
nb(cid:48)ε2

(cid:33)(cid:33)

(cid:111)

iterations/communication rounds and O(ζQK + σ2/(nε2))
stochastic oracle calls per node in expectation to achieve
E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communica-
tion cost per worker is O(d + ζQK).

Similarly to the ﬁnite-sum case, when ω = 0 (no quan-
tization) and n = 1, our bounds for iteration and oracle
complexities for VR-MARINA recover the bounds for PAGE
(Li & Richt´arik, 2020), which is optimal for online smooth
non-convex optimization as well. That is, the dependence
on ε in the complexity bound for VR-MARINA cannot be
improved in the class of ﬁrst-order stochastic methods. As
previously, up to the differences in smoothness constants,
the iteration and oracle complexities for VR-MARINA beneﬁt
from an increase in the number of workers n.

We provide the convergence results for VR-MARINA in
the online case under the Polyak-Łojasiewicz condition,
together with complete proofs, in Section D.2.2 of the Ap-
pendix.

Eξi∼Di [∇fξi(x)] = ∇fi(x),

Eξi∼Di

(cid:107)∇fξi(x) − ∇fi(x)(cid:107)2(cid:105)
(cid:104)

≤ σ2
i .

4. Partial Participation

(9)

(10)

Finally, we propose another modiﬁcation of MARINA. In
particular, we prove an option for partial participation of

MARINA: Faster Non-Convex Distributed Learning with Compression

Figure 1: Comparison of MARINA with DIANA, and of VR-MARINA with VR-DIANA, on binary classiﬁcation problem
involving non-convex loss (11) with LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 in the Appendix.
Stepsizes for the methods are chosen according to the theory and the batchsizes for VR-MARINA and VR-DIANA are ∼ m/100.
In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

the clients - a feature important in federated learning. The
resulting method is called PP-MARINA (see Algorithm 4
in the Appendix). At each iteration of PP-MARINA, the
server receives the quantized gradient differences from r
clients with probability 1 − p, and aggregates full gradi-
ents from all clients with probability p, i.e., PP-MARINA
coincides with MARINA up to the following difference:
gk+1
if ck = 1,
i
= gk + Q (cid:0)∇fi(xk+1) − ∇fi(xk))(cid:1), gk+1 =
and gk+1
i
(cid:80)
gk+1
1
k is the set of r i.i.d. sam-
ik
r
ples from the uniform distribution over {1, . . . , n}. That is,
if the probability p is chosen to be small enough, then with
high probability the server receives only quantized vectors
from a subset of clients at each iteration.

= ∇fi(xk+1), gk+1 = 1
n

otherwise, where I (cid:48)

i=1 gk+1

ik∈I (cid:48)
k

(cid:80)n

i

Below, we provide a convergence result for PP-MARINA for
smooth non-convex problems.
Theorem 4.1. Let Assumptions 1.1 and 1.2 be satisﬁed.
Then, after

K = O

(cid:18)

(cid:18)

1 +

(cid:113) (1−p)(1+ω)
pr

∆0L
ε2

(cid:19)(cid:19)

iterations with ∆0 = f (x0) − f∗, L2 = 1
n
stepsize γ ≤ L−1 (cid:16)
1 + (cid:112)(1−p)(1+ω)/(pr)
produces a point ˆxK for which E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2.

i=1 L2
i and the
, PP-MARINA

(cid:80)n
(cid:17)−1

One can ﬁnd the full statement of the theorem together with
its proof in Section E.1 of the appendix.
Corollary 4.1. Let the assumptions of Theorem 4.1 hold
If γ ≤
and choose p = ζQr/(dn), where r ≤ n.

1 + (cid:112)(1+ω)(dn−ζQr)/(b(cid:48)ζQr)

(cid:17)−1

L−1 (cid:16)
requires

, then PP-MARINA

(cid:18)

O

∆0L
ε2

(cid:18)

(cid:114)

1 +

1+ω
r

(cid:17)(cid:19)(cid:19)

(cid:16) dn

ζQr − 1

iterations/communication
achieve
E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total com-
munication cost is O (dn + ζQrK).

rounds

to

When r = n, i.e., all clients participate in communication
with the server at each iteration, the rate for PP-MARINA
recovers the rate for MARINA under the assumption that
(1 + ω)(d/ζQ − 1) = O(ω(d/ζQ − 1)), which holds for
a wide class of quantization operators, e.g., for identical
quantization, RandK, and (cid:96)p-quantization. In general, the
derived complexity is strictly better than previous state-of-
the-art one (see Table 1).

We provide the convergence results for PP-MARINA under
the Polyak-Łojasiewicz condition, together with complete
proofs, in Section E.2 of the Appendix.

5. Numerical Experiments

5.1. Binary Classiﬁcation with Non-Convex Loss

We conduct several numerical experiments2 on binary clas-
siﬁcation problem involving non-convex loss (Zhao et al.,
2010) (used for two-layer neural networks) with LibSVM

2Our code is available at https://github.com/

burlachenkok/marina.

02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-310-210-1jjrf(xk)jj2mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-310-210-1jjrf(xk)jj2w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0100200300400500600# of Epochs10-310-210-1jjrf(xk)jj2mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-310-210-1jjrf(xk)jj2mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)MARINA: Faster Non-Convex Distributed Learning with Compression

data (Chang & Lin, 2011) to justify the theoretical claims of
the paper. That is, we consider the following optimization
problem:

(cid:40)

min
x∈Rd

f (x) =

1
N

N
(cid:88)

t=1

(cid:41)

(cid:96)(a(cid:62)

t x, yi)

,

(11)

where {at} ∈ Rd, yi ∈ {−1, 1} for all t = 1, . . . , N , and
the function (cid:96) : Rd → R is deﬁned as

(cid:18)

(cid:96)(b, c) =

1 −

1
1 + exp(−bc)

(cid:19)2

.

The distributed environment is simulated in Python 3.8 us-
ing MPI4PY and other standard libraries. Additional details
about the experimental setup together with extra experi-
ments are deferred to Section A of the Appendix.

In our experiments, we compare MARINA with the full-batch
version of DIANA, and then VR-MARINA with VR-DIANA.
We exclude FedCOMGATE and FedPATH from this compar-
ison since they have signiﬁcantly worse oracle complexities
(see Table 1). The results are presented in Fig. 1. As our the-
ory predicts, the ﬁrst row shows the superiority of MARINA
to DIANA both in terms of iteration/communication complex-
ity and the total number of transmitted bits to achieve the
given accuracy. Next, to study the oracle complexity as well,
we consider non-full-batched methods – VR-MARINA and
VR-DIANA – since they have better oracle complexity than
the full-batched methods in the ﬁnite-sum case. Again, the
results presented in the second row justify that VR-MARINA
outperforms VR-DIANA in terms of oracle complexity and
the total number of transmitted bits to achieve the given
accuracy.

5.2. Image Classiﬁcation

We also compared the performance of VR-MARINA and VR-
DIANA on the training ResNet-18 (He et al., 2016) at
CIFAR100 (Krizhevsky et al., 2009) dataset. Formally, the
optimization problem is

(cid:40)

min
x∈Rd

f (x) =

1
N

N
(cid:88)

i=1

(cid:41)

(cid:96)(p(f (ai, x)), yi)

,

(12)

where {(ai, yi)}N
i=1 encode images and labels from
CIFAR100 dataset, f (ai, x) is the output of ResNet-18
on image ai with weights x, p is softmax function, and (cid:96)(·, ·)
is cross-entropy loss. The code is wrtitten in Python 3.9
using PyTorch 1.7, and the distributed environment is
simulated.

The results are presented in Fig. 2. Again, VR-MARINA
converges signiﬁcantly faster than VR-DIANA both in terms
of the oracle complexity and the total number of transmitted
bits to achieve the given accuracy. See other details and
observations in Section A of the Appendix.

Figure 2: Comparison of VR-MARINA with VR-DIANA on
training ResNet-18 at CIFAR100 dataset. Number of
workers equals 5. Stepsizes for the methods were tuned
and the batchsizes are ∼ m/50. In all cases, we used the
RandK sparsiﬁcation operator, the approximate values of K
are given in the legends (d is dimension of the problem).

Acknowledgements

The work of Peter Richt´arik, Eduard Gorbunov, Konstantin
Burlachenko and Zhize Li was supported by KAUST Base-
line Research Fund. The paper was written while E. Gor-
bunov was a research intern at KAUST. The work of E. Gor-
bunov in Sections 1, 2, and C was also partially supported
by the Ministry of Science and Higher Education of the Rus-
sian Federation (Goszadaniye) 075-00337-20-03, project
No. 0714-2020-0005, and in Sections 3, 4, D, E – by
RFBR, project number 19-31-51001. We thank Konstantin
Mishchenko (KAUST) for a suggestion related to the exper-
iments, Elena Bazanova (MIPT) for the suggestions about
improving the text, and Slavom´ır Hanzely (KAUST), Egor
Shulgin (KAUST), and Alexander Tyurin (KAUST) for spot-
ting the typos.

0500100015002000250030003500Communication Rounds5×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)0500100015002000250030003500Communication Rounds10-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)0.00.20.40.60.81.01.21.4#bits/n1e115×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)0.00.20.40.60.81.01.21.4#bits/n1e1110-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)050100150200250Epochs5×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)050100150200250Epochs10-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)MARINA: Faster Non-Convex Distributed Learning with Compression

References

Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic,
M. QSGD: Communication-efﬁcient SGD via gradient
quantization and encoding. In Advances in Neural Infor-
mation Processing Systems, pp. 1709–1720, 2017.

Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J.,
Srebro, N., and Woodworth, B. Lower bounds for
arXiv preprint
non-convex stochastic optimization.
arXiv:1912.02365, 2019.

Basu, D., Data, D., Karakus, C., and Diggavi, S. Qsparse-
local-SGD: Distributed SGD with quantization, sparsiﬁ-
cation and local computations. In Advances in Neural
Information Processing Systems, pp. 14668–14679, 2019.

Beznosikov, A., Horv´ath, S., Richt´arik, P., and Safaryan, M.
On biased compression for distributed learning. arXiv
preprint arXiv:2002.12410, 2020.

Bhojanapalli, S., Kyrillidis, A., and Sanghavi, S. Drop-
ping convexity for faster semi-deﬁnite optimization. In
Conference on Learning Theory, pp. 530–582. PMLR,
2016.

Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. Lower
bounds for ﬁnding stationary points i. Mathematical
Programming, pp. 1–50, 2019.

Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support
vector machines. ACM transactions on intelligent systems
and technology (TIST), 2(3):1–27, 2011.

Danilova, M., Dvurechensky, P., Gasnikov, A., Gorbunov,
E., Guminov, S., Kamzolov, D., and Shibaev, I. Recent
theoretical advances in non-convex optimization. arXiv
preprint arXiv:2012.06188, 2020.

Das, R., Hashemi, A., Sanghavi, S., and Dhillon, I. S. Im-
proved convergence rates for non-convex federated learn-
ing with compression. arXiv preprint arXiv:2012.04061,
2020.

Fang, C., Li, C., Lin, Z., and Zhang, T. Near-optimal
non-convex optimization via stochastic path integrated
differential estimator. Advances in Neural Information
Processing Systems, 31:689, 2018.

Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
Deep learning, volume 1. MIT press Cambridge, 2016.

Gorbunov, E., Kovalev, D., Makarenko, D., and Richt´arik, P.
Linearly converging error compensated sgd. Advances in
Neural Information Processing Systems, 33, 2020.

Goyal, P., Doll´ar, P., Girshick, R., Noordhuis, P.,
Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and
He, K. Accurate, large minibatch SGD: training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Haddadpour, F., Kamani, M. M., Mokhtari, A., and Mah-
davi, M. Federated learning with compression: Uni-
arXiv preprint
ﬁed analysis and sharp guarantees.
arXiv:2007.01154, 2020.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Horv´ath, S., Ho, C.-Y., ˇLudov´ıt Horv´ath, Sahu, A. N.,
Canini, M., and Richt´arik, P. Natural compression for dis-
tributed deep learning. arXiv preprint arXiv:1905.10988,
2019.

Horv´ath, S., Kovalev, D., Mishchenko, K., Stich, S., and
Richt´arik, P. Stochastic distributed learning with gradi-
ent quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019.

Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis,
M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode,
G., Cummings, R., et al. Advances and open problems
in federated learning. arXiv preprint arXiv:1912.04977,
2019.

Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. Er-
ror feedback ﬁxes signSGD and other gradient compres-
sion schemes. In International Conference on Machine
Learning, pp. 3252–3261, 2019.

Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S.,
and Suresh, A. T. Scaffold: Stochastic controlled averag-
ing for federated learning. In International Conference
on Machine Learning, pp. 5132–5143. PMLR, 2020.

Khanduri, P., Sharma, P., Kaﬂe, S., Bulusu, S., Rajawat, K.,
and Varshney, P. K. Distributed stochastic non-convex op-
timization: Momentum-based variance reduction. arXiv
preprint arXiv:2005.00224, 2020.

Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. De-
centralized deep learning with arbitrary communication
compression. ICLR, pp. arXiv:1907.09356, 2020a. URL
https://arxiv.org/abs/1907.09356.

Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich,
S. A uniﬁed theory of decentralized sgd with changing
topology and local updates. In International Conference
on Machine Learning, pp. 5381–5393. PMLR, 2020b.

Koneˇcn´y, J., McMahan, H. B., Yu, F. X., Richt´arik, P.,
Suresh, A. T., and Bacon, D. Federated learning: Strate-
gies for improving communication efﬁciency. arXiv
preprint arXiv:1610.05492, 2016.

Koneˇcn´y, J., McMahan, H. B., Yu, F., Richt´arik, P., Suresh,
A. T., and Bacon, D. Federated learning: strategies for

MARINA: Faster Non-Convex Distributed Learning with Compression

Safaryan, M., Shulgin, E., and Richt´arik, P. Uncertainty
principle for communication compression in distributed
and federated learning and the search for an optimal com-
pressor. arXiv preprint arXiv:2002.08958, 2020.

Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochas-
tic gradient descent and its application to data-parallel
distributed training of speech dnns. In Fifteenth Annual
Conference of the International Speech Communication
Association, 2014.

Sharma, P., Kaﬂe, S., Khanduri, P., Bulusu, S., Rajawat,
K., and Varshney, P. K.
Parallel restarted spider–
communication efﬁcient distributed nonconvex optimiza-
tion with optimal computation complexity. arXiv preprint
arXiv:1912.06036, 2019.

Stich, S. U. and Karimireddy, S. P. The error-feedback
framework: Better rates for sgd with delayed gradients
and compressed updates. Journal of Machine Learning
Research, 21:1–36, 2020.

Sun, H., Lu, S., and Hong, M. Improving the sample and
communication complexity for decentralized non-convex
optimization: Joint gradient estimation and tracking. In
International Conference on Machine Learning, pp. 9217–
9228. PMLR, 2020.

Sun, R. Optimization for deep learning: theory and algo-

rithms. arXiv preprint arXiv:1912.08957, 2019.

Suresh, A. T., Yu, F. X., Kumar, S., and McMahan, H. B.
Distributed mean estimation with limited communication.
In Proceedings of the 34th International Conference on
Machine Learning, 2017.

You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli,
S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J.
Large batch optimization for deep learning: Training bert
in 76 minutes. In International Conference on Learning
Representations, 2020. URL https://openreview.
net/forum?id=Syx4wnEtvH.

Zhao, L., Mammadov, M., and Yearwood, J. From convex
to nonconvex: a loss function analysis for binary classiﬁ-
cation. In 2010 IEEE International Conference on Data
Mining Workshops, pp. 1281–1288. IEEE, 2010.

improving communication efﬁciency. In NIPS Private
Multi-Party Machine Learning Workshop, 2016.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers

of features from tiny images. 2009.

Li, X., Yang, W., Wang, S., and Zhang, Z. Communica-
tion efﬁcient decentralized training with multiple local
updates. arXiv preprint arXiv:1910.09126, 5, 2019.

Li, Z. and Richt´arik, P. A uniﬁed analysis of stochastic
gradient methods for nonconvex federated optimization.
arXiv preprint arXiv:2006.07013, 2020.

Li, Z., Bao, H., Zhang, X., and Richt´arik, P. Page: A sim-
ple and optimal probabilistic gradient estimator for non-
convex optimization. arXiv preprint arXiv:2008.10898,
2020.

Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and
Liu, J. Can decentralized algorithms outperform central-
ized algorithms? a case study for decentralized parallel
stochastic gradient descent. In Advances in Neural Infor-
mation Processing Systems, pp. 5330–5340, 2017.

Łojasiewicz, S. A topological property of real analytic
subsets. Coll. du CNRS, Les ´equations aux d´eriv´ees par-
tielles, 117:87–89, 1963.

Ma, C., Wang, K., Chi, Y., and Chen, Y. Implicit regulariza-
tion in nonconvex statistical estimation: Gradient descent
converges linearly for phase retrieval and matrix comple-
tion. In International Conference on Machine Learning,
pp. 3345–3354. PMLR, 2018.

McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and
Ag¨uera y Arcas, B. Communication-efﬁcient learning
of deep networks from decentralized data. In Proceed-
ings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2017.

Mishchenko, K., Gorbunov, E., Tak´aˇc, M., and Richt´arik,
P. Distributed learning with compressed gradient differ-
ences. arXiv preprint arXiv:1901.09269, 2019.

Murty, K. and Kabadi, S. Some np-complete problems
in quadratic and nonlinear programming. Mathematical
programming, 39(2):117–129, 1987.

Polyak, B. T. Gradient methods for the minimisation of func-
tionals. USSR Computational Mathematics and Mathe-
matical Physics, 3(4):864–878, 1963.

Qian, X., Richt´arik, P., and Zhang, T. Error compen-
sated distributed sgd can be accelerated. arXiv preprint
arXiv:2010.00091, 2020.

MARINA: Faster Non-Convex Distributed Learning with Compression

Appendix

A. Extra Experiments

A.1. Binary Classiﬁcation with Non-Convex Loss

A.1.1. SETUP

In Section 5.1, we present the behavior of MARINA, VR-MARINA, DIANA, and VR-DIANA on the binary classiﬁcation
problem involving non-convex loss (Zhao et al., 2010). The datasets were taken from LibSVM (Chang & Lin, 2011) and
split into ﬁve equal parts among ﬁve clients (we excluded N − 5 · (cid:98)N/5(cid:99) last datapoints from each dataset), see the summary
in Table 3.

Table 3: Summary of the datasets and splitting of the data among clients (Figure 1).

Dataset
mushrooms
w8a
phishing
a9a

n N (# of datapoints)
5
5
5
5

8 120
49 745
11 055
32 560

d (# of features)
112
300
69
124

The code was written in Python 3.8 using MPI4PY to emulate the distributed environment and then was executed on a
machine with 48 cores, each is Intel(R) Xeon(R) Gold 6246 CPU 3.30GHz.

A.1.2. EXTRA EXPERIMENTS

In this section, we provide additional numerical results on the comparison of MARINA, VR-MARINA, DIANA, and VR-DIANA
on the problem (11). Since one of the main goals of our experiments is to justify the theoretical ﬁndings of the paper, in the
experiments, we used the stepsizes from the corresponding theoretical results for the methods (for DIANA and VR-DIANA the
stepsizes were chosen according to (Horv´ath et al., 2019; Li & Richt´arik, 2020)). Next, to compute the stochastic gradients,
we use batchsizes = max{1, m/100} for VR-MARINA and VR-DIANA.

The results for the full-batched methods are reported in Figure 3, and the comparison of VR-MARINA and VR-DIANA is given
in Figure 4. Clearly, in both cases, MARINA and VR-MARINA show faster convergence than the previous state-of-the-art
methods, DIANA and VR-DIANA, for distributed non-convex optimization with compression in terms of (cid:107)∇f (xk)(cid:107)2 and
f (xk) decrease w.r.t. the number of communication rounds, oracle calls per node and the total number of transferred bits
from workers to the master.

We also tested MARINA and DIANA on mushrooms dataset with a bigger number of workers (n = 20). The results are
reported in Figure 5. Similarly to the previous numerical tests, MARINA shows its superiority to DIANA with n = 20 as well.

A.2. Image Classiﬁcation

A.2.1. SETUP

In Section 5.2, we demonstrate the performance of VR-MARINA and VR-DIANA on training ResNet-18 at CIFAR100
dataset. ResNet-18 has d = 11 689 512 parameters to train and CIFAR100 contains N = 50 000 colored images. The
dataset is split into 5 parts among 5 workers in such a way that the ﬁrst four workers get 10 112 samples and the ﬁfth one get
9 552 samples. The code was written in Python 3.9 using PYTORCH 1.7 and then was executed on a machine with NVIDIA
GPU Geforce RTX 2080 Ti with 11 GByte onboard global GPU memory.

In all experiments, we use batchsize = 256 on each worker and tune the stepsizes for each method separately. That is, for each
method and for each choice of K for RandK operator we run the method with stepsize γ ∈ {10−6, 0.1, 0.2, 0.5, 1.0, 5.0} to
ﬁnd the interval containing the best stepsize. Next, the obtained interal is split into ∼ 10 equal parts and the method is run
with corresponding stepsizes. Other parameters of the methods are chosen according to the theory. The summary of used

MARINA: Faster Non-Convex Distributed Learning with Compression

Figure 3: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (11) with
LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 (n = 5). Stepsizes for the methods are chosen
according to the theory. In all cases, we used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

parameters is given in Table 4.

Table 4: Summary of the parameters used in the experiments presented in Fig. 2 and Fig. 6. Stepsizes were tuned, batchsize
= 256 on each worker, other parameters were picked according to the theory, except the last line, where p for VR-MARINA
without compression was picked as for VR-MARINA with RandK, K = 100 000 compression operator.

Method

VR-MARINA
VR-MARINA
VR-MARINA
VR-DIANA
VR-DIANA
VR-DIANA
VR-MARINA
VR-DIANA
VR-MARINA

RandK, K =
100 000
500 000
1 000 000
100 000
500 000
1 000 000
11 689 512 (K = d)
11 689 512 (K = d)
11 689 512 (K = d)

γ
0.95
0.95
0.95
0.15
0.35
0.35
3.5
2.5
3.5

p
0.008554
0.024691
0.024691
0.025316
0.025316
0.025316
0.024691
0.025316
0.008554

02500500075001000012500150001750020000Communication Rounds10-210-1f(xk)mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-1f(xk)w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-16×10-22×10-1f(xk)phishingMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-11.2×10-11.4×10-11.6×10-11.8×10-12×10-12.2×10-12.4×10-12.6×10-1f(xk)a9aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-510-410-3jjrf(xk)jj2phishingMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-410-310-210-1jjrf(xk)jj2a9aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-210-1f(xk)mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-1f(xk)w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-16×10-22×10-1f(xk)phishingMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-11.2×10-11.4×10-11.6×10-11.8×10-12×10-12.2×10-12.4×10-12.6×10-1f(xk)a9aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-310-210-1jjrf(xk)jj2mushroomsMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-310-210-1jjrf(xk)jj2w8aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-510-410-3jjrf(xk)jj2phishingMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-410-310-210-1jjrf(xk)jj2a9aMARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)MARINA: Faster Non-Convex Distributed Learning with Compression

Figure 4: Comparison of VR-MARINA with VR-DIANA on binary classiﬁcation problem involving non-convex loss (11) with
LibSVM data (Chang & Lin, 2011). Parameter n is chosen as per Tbl. 3 (n = 5). Stepsizes for the methods are chosen
according to the theory and the batchsizes are ∼ m/100. In all cases, we used the RandK sparsiﬁcation operator with K
∈ {1, 5, 10}.

A.2.2. EXTRA EXPERIMENTS

Results presented in Fig. 2 show the superiority of VR-MARINA to VR-DIANA in training ResNet-18 at CIFAR100. To
emphasize the effect of compression we also run VR-MARINA and VR-DIANA without compression, see the results in Fig. 6.
First of all, one con notice that the methods do beneﬁt from compression: VR-MARINA and VR-DIANA with compression
converge much faster than their non-comressed versions in terms of the total number of transmitted bits to achieve given

02500500075001000012500150001750020000Communication Rounds10-210-1f(xk)mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-13×10-24×10-26×10-22×10-1f(xk)w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-1f(xk)phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds1.2×10-11.4×10-11.6×10-11.8×10-12×10-12.2×10-12.4×10-12.6×10-1f(xk)a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-510-410-310-2jjrf(xk)jj2phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-810-610-410-2jjrf(xk)jj2a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-210-1f(xk)mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02000000400000060000008000000#bits/n10-13×10-24×10-26×10-22×10-1f(xk)w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-1f(xk)phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n1.2×10-11.4×10-11.6×10-11.8×10-12×10-12.2×10-12.4×10-12.6×10-1f(xk)a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-310-210-1jjrf(xk)jj2mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)02000000400000060000008000000#bits/n10-310-210-1jjrf(xk)jj2w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-510-410-310-2jjrf(xk)jj2phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)01000000200000030000004000000500000060000007000000#bits/n10-810-610-410-2jjrf(xk)jj2a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-210-1f(xk)mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-13×10-24×10-26×10-22×10-1f(xk)w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-1f(xk)phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs1.2×10-11.4×10-11.6×10-11.8×10-12×10-12.2×10-12.4×10-12.6×10-1f(xk)a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-310-210-1jjrf(xk)jj2mushroomsVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-310-210-1jjrf(xk)jj2w8aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-510-410-310-2jjrf(xk)jj2phishingVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)0100200300400500600# of Epochs10-810-610-410-2jjrf(xk)jj2a9aVR-MARINA (K=1)VR-MARINA (K=5)VR-MARINA (K=10)VR-DIANA (K=1)VR-DIANA (K=5)VR-DIANA (K=10)MARINA: Faster Non-Convex Distributed Learning with Compression

Figure 5: Comparison of MARINA with DIANA on binary classiﬁcation problem involving non-convex loss (11) with
mushrooms dataset and n = 20 workers. Stepsizes for the methods are chosen according to the theory. In all cases, we
used the RandK sparsiﬁcation operator with K ∈ {1, 5, 10}.

accuracy.

Moreover, as Fig. 2 shows, VR-MARINA with K = 100 000 converges faster than VR-MARINA with larger K in terms of
the epochs. That is, the method with more aggresive compression requires less oracle calls to achieve the same accuracy.
The reason of such an unusual behavior is the choice of p: when K = 100 000 the theoretical choice of p is much smaller
than for K = 500 000 and K = 1 000 000. Therefore, in VR-MARINA with K = 100 000, the workers compute the full
gradients more rarely than in the case of larger K. As the result, it turns out, that the total number of oracle calls needed to
achieve given accuracy also smaller for K = 100 000 than for larger K. Moreover, we see this phenomenon even without
applying compression: VR-MARINA without compression and with p as in the experiment with VR-MARINA with K = 100
000 converges faster than VR-MARINA without compression and with theoretical choice of p, which is the same as in the
case when K = 500 000, 1 000 000, see Table 4.

02500500075001000012500150001750020000Communication Rounds10-210-1f(xk)mushrooms,n=20MARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)02500500075001000012500150001750020000Communication Rounds10-310-210-1jjrf(xk)jj2mushrooms,n=20MARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-210-1f(xk)mushrooms,n=20MARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)0.00.20.40.60.81.01.2#bits/n1e710-310-210-1jjrf(xk)jj2mushrooms,n=20MARINA (K=1)MARINA (K=5)MARINA (K=10)DIANA (K=1)DIANA (K=5)DIANA (K=10)MARINA: Faster Non-Convex Distributed Learning with Compression

Figure 6: Comparison of VR-MARINA with VR-DIANA on training ResNet-18 at CIFAR100 dataset. Number of workers
equals 5. Stepsizes for the methods were tuned and the batchsizes are ∼ m/50. We used the RandK sparsiﬁcation operator,
the approximate values of K are given in the legends (d is dimension of the problem). We also show the performance of
VR-MARINA and VR-DIANA without compression.

0500100015002000250030003500Communication Rounds5×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)0.00.20.40.60.81.01.2#bits/n1e125×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)050100150200250Epochs5×1006×1007×100f(x)Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)0500100015002000250030003500Communication Rounds10-610-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)0.00.20.40.60.81.01.2#bits/n1e1210-610-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)050100150200250Epochs10-610-510-410-310-2jjrf(x)jj2Training ResNet-18 @ CIFAR100VR-MARINA (K¼0.009d)VR-MARINA (K¼0.043d)VR-MARINA (K¼0.086d)VR-DIANA (K¼0.009d)VR-DIANA (K¼0.043d)VR-DIANA (K¼0.086d)VR-MARINA (no compr.)VR-MARINA (no compr.p¼0:009)VR-DIANA (no compr.)MARINA: Faster Non-Convex Distributed Learning with Compression

B. Basic Facts and Auxiliary Results

B.1. Useful Properties of Expectations

Variance decomposition. For a random vector ξ ∈ Rd and any deterministic vector x ∈ Rd, the variance can be
decomposed as

(cid:107)ξ − Eξ(cid:107)2(cid:105)
(cid:104)

E

= E (cid:2)(cid:107)ξ − x(cid:107)2(cid:3) − (cid:107)Eξ − x(cid:107)2

Tower property of mathematical expectation. For random variables ξ, η ∈ Rd, we have

under an assumption that all expectations in the expression above are well-deﬁned.

E [ξ] = E [E [ξ | η]]

B.2. One Lemma

In this section, we formulate a lemma from (Li et al., 2020), which holds in our settings as well. We omit the proof of this
lemmas since it is identical to the one from (Li et al., 2020).
Lemma B.1 (Lemma 2 from (Li et al., 2020)). Assume that function f is L-smooth and xk+1 = xk − γgk. Then

f (xk+1) ≤ f (xk) −

γ
2

(cid:107)∇f (xk)(cid:107)2 −

(cid:18) 1
2γ

−

(cid:19)

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2

(cid:107)gk − ∇f (xk)(cid:107)2.

(15)

(13)

(14)

MARINA: Faster Non-Convex Distributed Learning with Compression

C. Missing Proofs for MARINA

C.1. Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 2.1 together with the proof of this result.

Theorem C.1 (Theorem 2.1). Let Assumptions 1.1 and 1.2 be satisﬁed and

γ ≤

(cid:18)

L

1 +

1
(cid:113) (1−p)ω
pn

(cid:19) ,

where L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of MARINA we have

(cid:104)(cid:13)
(cid:13)∇f (ˆxK)(cid:13)
(cid:13)

2(cid:105)

E

≤

2∆0
γK

,

where ˆxK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

K = O

(cid:32)

∆0L
ε2

(cid:32)

(cid:115)

1 +

(1 − p)ω
pn

(cid:33)(cid:33)

(16)

(17)

(18)

iterations MARINA produces such a point ˆxK that E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2. Moreover, under an assumption that the
communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server,
we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

∆0L
ε2

(cid:32)

(cid:115)

1 +

(cid:33)

(1 − p)ω
pn

(cid:33)

(pd + (1 − p)ζQ)

,

(19)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 2.1. The scheme of the proof is similar to the proof of Theorem 1 from (Li et al., 2020). From Lemma B.1,
we have

E[f (xk+1)] ≤ E[f (xk)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

(20)

Next, we need to derive an upper bound for E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3). By deﬁnition of gk+1, we have

gk+1 =






∇f (xk+1)
n
(cid:80)
i=1

gk + 1
n

Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1) with probability 1 − p.

with probability p,

Using this, variance decomposition (13) and tower property (14), we derive:

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(14)=

(1 − p)E

(14),(13)
=

(1 − p)E





(cid:13)
(cid:13)
gk +
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1) − ∇f (xk+1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2







(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

(cid:13)
(cid:13)
Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1) − ∇f (xk+1) + ∇f (xk)
(cid:13)
(cid:13)
(cid:13)

2



+(1 − p)E

i=1
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

MARINA: Faster Non-Convex Distributed Learning with Compression

Since Q (cid:0)∇f1(xk+1) − ∇f1(xk)(cid:1) , . . . , Q (cid:0)∇fn(xk+1) − ∇fn(xk)(cid:1) are independent random vectors for ﬁxed xk and
xk+1 we have

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) = (1 − p)E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

(cid:0)Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1) − ∇fi(xk+1) + ∇fi(xk)(cid:1)

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+(1 − p)E

i=1
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

=

(3)
≤

1 − p
n2

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1) − ∇fi(xk+1) + ∇fi(xk)(cid:13)
(cid:13)

2(cid:105)

E

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

(1 − p)ω
n2

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)∇fi(xk+1) − ∇fi(xk)(cid:13)
(cid:13)

2(cid:105)

E

+ (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

Using L-smoothness (2) of fi together with the tower property (14), we obtain

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(1 − p)ω
n2

n
(cid:88)

i=1

i E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E
L2

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

=

(1 − p)ωL2
n

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

(21)

Next, we introduce a new notation: Φk = f (xk) − f∗ + γ
establish the following inequality:

2p (cid:107)gk − ∇f (xk)(cid:107)2. Using this and inequalities (20) and (21), we

E [Φk+1] ≤ E

(cid:20)
f (xk) − f∗ −

γ
2

γ
2p

E

(cid:20) (1 − p)ωL2
n

+

(cid:107)∇f (xk)(cid:107)2 −

(cid:19)

(cid:18) 1
2γ
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2
2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

−

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

= E [Φk] −

(16)
≤ E [Φk] −

γ
2
γ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(cid:18) γ(1 − p)ωL2
2pn

−

1
2γ

+

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ,

(22)

where in the last inequality, we use γ(1−p)ωL2
2γ + L
k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

− 1

2pn

2 ≤ 0 following from (16). Summing up inequalities (22) for

1
K

K−1
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ≤

2
γK

K−1
(cid:88)

k=0

(E[Φk] − E[Φk+1]) =

2 (E[Φ0] − E[ΦK])
γK

=

2∆0
γK

,

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of ˆxK, we obtain (17) that
implies (18) and (19).

Corollary C.1 (Corollary 2.1). Let the assumptions of Theorem 2.1 hold and p = ζQ
the quantization (see Def. 1.1). If

d , where ζQ is the expected density of

then MARINA requires

γ ≤

(cid:18)

(cid:114)

L

1 +

1
(cid:16) d
ζQ

ω
n

(cid:17)(cid:19) ,

− 1

K = O

(cid:32)

∆0L
ε2

(cid:32)

(cid:115)

1 +

ω
n

(cid:18) d
ζQ

(cid:19)(cid:33)(cid:33)

− 1

MARINA: Faster Non-Convex Distributed Learning with Compression

iterations/communication rounds to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost per worker is

(cid:32)

O

d +

∆0L
ε2

(cid:32)

(cid:114)

ζQ +

ωζQ
n

(cid:33)(cid:33)

(d − ζQ)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof of Corollary 2.1. The choice of p = ζQ

d implies

1 − p
p

=

d
ζQ

− 1,

(cid:18)

pd + (1 − p)ζQ ≤ ζQ +

1 −

(cid:19)

ζQ
d

· ζQ ≤ 2ζQ.

Plugging these relations in (16), (18), and (19), we get that if

then MARINA requires

γ ≤

(cid:18)

(cid:114)

L

1 +

1
(cid:16) d
ζQ

ω
n

(cid:17)(cid:19) ,

− 1

K = O

= O

(cid:32)

(cid:32)

∆0L
ε2

∆0L
ε2

(cid:32)

(cid:115)

1 +

(cid:32)

(cid:115)

1 +

(cid:33)(cid:33)

(1 − p)ω
pn

(cid:19)(cid:33)(cid:33)

ω
n

(cid:18) d
ζQ

− 1

iterations/communication rounds in order to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost per
worker is

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

(cid:32)

= O

d +

∆0L
ε2

∆0L
ε2

(cid:32)

(cid:115)

1 +

(cid:33)

(1 − p)ω
pn

(cid:33)

(pd + (1 − p)ζQ)

(cid:32)

(cid:114)

ζQ +

ωζQ
n

(cid:33)(cid:33)

(d − ζQ)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

C.2. Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide the full statement of Theorem 2.2 together with the proof of this result.

Theorem C.2 (Theorem 2.2). Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed and

γ ≤ min




(cid:18)

1 +



L

1
(cid:113) 2(1−p)ω
pn

(cid:19) ,

p
2µ






,

where L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of MARINA we have

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ (1 − γµ)K∆0,

(23)

(24)

MARINA: Faster Non-Convex Distributed Learning with Compression

where ∆0 = f (x0) − f (x∗). That is, after

(cid:32)

(cid:40)

K = O

max

(cid:32)

(cid:115)

1 +

(1 − p)ω
pn

(cid:33)(cid:41)

(cid:33)

log

∆0
ε

1
p

,

L
µ

(25)

iterations MARINA produces such a point xK that E[f (xK) − f (x∗)] ≤ ε. Moreover, under an assumption that the
communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server,
we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O

d + max

(cid:32)

(cid:40)

1
p

,

L
µ

(cid:32)

(cid:115)

1 +

(1 − p)ω
pn

(cid:33)(cid:41)

(pd + (1 − p)ζQ) log

(cid:33)

,

∆0
ε

(26)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 2.2. The proof is very similar to the proof of Theorem 2.1. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(4)

≤ (1 − γµ)E (cid:2)f (xk) − f (x∗)(cid:3) −

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

Using the same arguments as in the proof of (21), we obtain

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(1 − p)ωL2
n

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

Putting all together, we derive that the sequence Φk = f (xk) − f (x∗) + γ

p (cid:107)gk − ∇f (xk)(cid:107)2 satisﬁes

E [Φk+1] ≤ E

(cid:20)

(1 − γµ)(f (xk) − f (x∗)) −

+

γ
p

E

(cid:20) (1 − p)ωL2
n

(cid:20)
(1 − γµ)(f (xk) − f (x∗)) +

= E

(cid:18) γ
2

+

γ
p

(1 − p)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

(cid:19)

(cid:18) 1
2γ
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

L
2

−

(cid:107)xk+1 − xk(cid:107)2 +

γ
2
2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:19)

2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:13)

(cid:18) γ(1 − p)ωL2
pn

+

−

1
2γ

+

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

(23)
≤ (1 − γµ)E[Φk],

where in the last inequality, we use γ(1−p)ωL2
− 1
the recurrence and using g0 = ∇f (x0), we obtain

pn

2γ + L

2 ≤ 0 and γ

2 + γ

p (1 − p) ≤ (1 − γµ) γ

p following from (23). Unrolling

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ E[ΦK] ≤ (1 − γµ)KΦ0 = (1 − γµ)K(f (x0) − f (x∗))

that implies (25) and (26).

Corollary C.2. Let the assumptions of Theorem 2.2 hold and p = ζQ
(see Def. 1.1). If

d , where ζQ is the expected density of the quantization





γ ≤ min

1

(cid:18)

(cid:114)

L

1 +

(cid:16) d
ζQ

2ω
n

− 1

p
2µ

(cid:17)(cid:19) ,





,

MARINA: Faster Non-Convex Distributed Learning with Compression

then MARINA requires

(cid:32)

(cid:40)

K = O

max

d
ζQ

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

(cid:18) d
ζQ

(cid:19)(cid:33)(cid:41)

− 1

(cid:33)

log

∆0
ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per worker
is

(cid:32)

(cid:40)

(cid:32)

L
µ

(cid:114)

ωζQ
n

O

d + max

d,

ζQ +

(d − ζQ)

log

(cid:33)(cid:41)

(cid:33)

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof. The choice of p = ζQ

d implies

1 − p
p

=

d
ζQ

− 1,

(cid:18)

pd + (1 − p)ζQ ≤ ζQ +

1 −

(cid:19)

ζQ
d

· ζQ ≤ 2ζQ.

Plugging these relations in (23), (25), and (26), we get that if

γ ≤ min




1

(cid:18)

(cid:114)

1 +



L

(cid:16) d
ζQ

2ω
n

− 1

p
2µ

(cid:17)(cid:19) ,






,

then MARINA requires

(cid:32)

(cid:40)

K = O

max

(cid:32)

(cid:115)

1 +

1
p

,

L
µ

(1 − p)ω
pn

(cid:33)(cid:41)

(cid:32)

(cid:40)

= O

max

d
ζQ

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

(cid:18) d
ζQ

(cid:33)

log

∆0
ε
(cid:19)(cid:33)(cid:41)

− 1

log

(cid:33)

∆0
ε

iterations/communication rounds in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost per
worker is

d + K(pd + (1 − p)ζQ) = O

d + max

(cid:32)

(cid:32)

(cid:40)

(cid:40)

(cid:32)

(cid:115)

1 +

1
p

,

L
µ

(1 − p)ω
pn

(cid:33)(cid:41)

(pd + (1 − p)ζQ) log

(cid:33)

∆0
ε

= O

d + max

d,

(cid:32)

(cid:114)

ζQ +

L
µ

ωζQ
n

(cid:33)(cid:41)

(d − ζQ)

log

(cid:33)

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

D. Missing Proofs for VR-MARINA

D.1. Finite Sum Case

D.1.1. GENERALLY NON-CONVEX PROBLEMS

In this section, we provide the full statement of Theorem 3.1 together with the proof of this result.

Theorem D.1 (Theorem 3.1). Consider the ﬁnite sum case (1)+(5). Let Assumptions 1.1, 1.2 and 3.1 be satisﬁed and

γ ≤

1

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

where L2 = 1
n

(cid:80)n

i=1 L2

i and L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of VR-MARINA we have

(cid:104)(cid:13)
(cid:13)∇f (ˆxK)(cid:13)
(cid:13)

2(cid:105)

E

≤

2∆0
γK

,

where ˆxK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

K = O

(cid:32)

∆0
ε2

(cid:32)

(cid:115)

L +

1 − p
pn

(cid:18)

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)(cid:33)

(27)

(28)

(29)

iterations VR-MARINA produces such a point ˆxK that E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total number of stochastic
oracle calls per node equals

m + K(pm + 2(1 − p)b(cid:48)) = O

m +

(cid:32)

(cid:32)

(cid:115)

L +

(cid:18)

1 − p
pn

∆0
ε2

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:33)

(pm + (1 − p)b(cid:48))

.

(30)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of
transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

(cid:32)

(cid:115)

L +

(cid:18)

1 − p
pn

∆0
ε2

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:33)

(pd + (1 − p)ζQ)

,

(31)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 3.1. The proof of this theorem is a generalization of the proof of Theorem 2.1. From Lemma B.1, we
have

E[f (xk+1)] ≤ E[f (xk)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

(32)

Next, we need to derive an upper bound for E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3). Since gk+1 = 1

n

representation of gk+1:

n
(cid:80)
i=1

gk+1
i

, we get the following

gk+1 =






∇f (xk+1)
n
(cid:80)
i=1

gk + 1
n

(cid:32)

Q

1
b(cid:48)

(cid:80)
j∈I (cid:48)

i,k

(∇fij(xk+1) − ∇fij(xk))

with probability 1 − p.

with probability p,

(cid:33)

MARINA: Faster Non-Convex Distributed Learning with Compression

Using this, variance decomposition (13) and tower property (14), we derive:

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(14)=

(1 − p)E

(14),(13)
=

(1 − p)E






(cid:13)
(cid:13)
(cid:13)
gk +
(cid:13)
(cid:13)
(cid:13)



Q



1
b(cid:48)

1
n

n
(cid:88)

i=1

(cid:88)

j∈I (cid:48)

i,k

(∇fij(xk+1) − ∇fij(xk))

(cid:13)

(cid:13)
(cid:13)
 − ∇f (xk+1)
(cid:13)
(cid:13)
(cid:13)

2









(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)



Q



1
b(cid:48)

j∈I (cid:48)

i=1
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

i,k

2(cid:105)

.

+(1 − p)E

(cid:88)

(∇fij(xk+1) − ∇fij(xk))


 − ∇f (xk+1) + ∇f (xk)

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Next, we use the notation: (cid:101)∆k

i = 1
b(cid:48)

(∇fij(xk+1) − ∇fij(xk)) and ∆k

i = ∇fi(xk+1) − ∇fi(xk). These vectors

i for all i ∈ [n]. Moreover, Q( (cid:101)∆k

1), . . . , Q( (cid:101)∆k

n) are independent random vectors for ﬁxed

(cid:80)
j∈I (cid:48)

i,k

(cid:104)
i | xk, xk+1(cid:105)
(cid:101)∆k
satisfy E
xk and xk+1. These observations imply

= ∆k

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

=

(1 − p)E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

(cid:16)

i=1

Q( (cid:101)∆k

i ) − ∆k
i

(cid:17)

2
 + (1 − p)E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

(cid:20)(cid:13)
(cid:13)Q( (cid:101)∆k
(cid:13)

i ) − (cid:101)∆k

i + (cid:101)∆k

i − ∆k
i

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

=

(14),(13)
=

(14),(3)
=

(14),(13)
=

1 − p
n2

1 − p
n2

n
(cid:88)

i=1
n
(cid:88)

i=1

E

(cid:18)

E

+(1 − p)E

1 − p
n2

1 − p
n2

n
(cid:88)

i=1
n
(cid:88)

i=1

+ E

(cid:20)(cid:13)
(cid:13) (cid:101)∆k
(cid:13)

i − ∆k
i

2(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

i ) − (cid:101)∆k
i

(cid:20)(cid:13)
(cid:13)Q( (cid:101)∆k
(cid:13)
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:20)(cid:13)
(cid:20)(cid:13)
2(cid:21)
(cid:13) (cid:101)∆k
(cid:13) (cid:101)∆k
(cid:13)
(cid:13)

+ E

(cid:13)
(cid:13)
(cid:13)

2(cid:105)

i

(cid:18)

ωE

(cid:18)

ωE

(cid:104)(cid:13)
(cid:13)∆k
i

2(cid:105)

(cid:13)
(cid:13)

+ (1 + ω)E

(cid:20)(cid:13)
(cid:13) (cid:101)∆k
(cid:13)

i − ∆k
i

2(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

i − ∆k
i

2(cid:21)(cid:19)

(cid:13)
(cid:13)
(cid:13)

+ (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

Using L-smoothness (2) and average L-smoothness (7) of fi together with the tower property (14), we get

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

1 − p
n2

n
(cid:88)

(cid:18)

i=1

ωL2

(cid:19)

2(cid:105)

i +

(1 + ω)L2
i
b(cid:48)
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(1 + ω)L2
b(cid:48)

(cid:19)

+(1 − p)E

=

1 − p
n

(cid:18)

ωL2 +

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

(33)

Next, we introduce new notation: Φk = f (xk) − f∗ + γ

2p (cid:107)gk − ∇f (xk)(cid:107)2. Using this and inequalities (32) and (33), we

MARINA: Faster Non-Convex Distributed Learning with Compression

establish the following inequality:

E [Φk+1] ≤ E

(cid:20)
f (xk) − f∗ −

γ
2p

E

(cid:20) 1 − p
n

+

γ
2
(cid:18)

(cid:107)∇f (xk)(cid:107)2 −

ωL2 +

(1 + ω)L2
b(cid:48)
(cid:18) γ(1 − p)
2pn

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

= E [Φk] −

(27)
≤ E [Φk] −

γ
2
γ
2

(cid:18) 1
2γ
(cid:19)

(cid:19)

−

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2
2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)
(cid:13)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

(cid:18)

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)

−

1
2γ

+

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ,

(34)

where in the last inequality, we use γ(1−p)
− 1
2pn
(34) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

ωL2 + (1+ω)L2

b(cid:48)

2γ + L

(cid:16)

(cid:17)

2 ≤ 0 following from (27). Summing up inequalities

1
K

K−1
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ≤

2
γK

K−1
(cid:88)

k=0

(E[Φk] − E[Φk+1]) =

2 (E[Φ0] − E[ΦK])
γK

=

2∆0
γK

,

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of ˆxK, we obtain (28) that
implies (29), (30), and (31).

Remark D.1 (About batchsizes dissimilarity). We notice that our analysis can be easily extended to handle the version of VR-
MARINA with different batchsizes b(cid:48)
n on different workers, i.e., when |I (cid:48)
(∇fij(xk+1)−
∇fij(xk)). In this case, the statement of Theorem 3.1 remains the same with the small modiﬁciation: instead of L2
b(cid:48)
complexity bounds will have 1
n

i and (cid:101)∆k

i,k| = b(cid:48)

1, . . . , b(cid:48)

i = 1
b(cid:48)
i

(cid:80)n

the

j∈I (cid:48)

(cid:80)

i=1

i,k

.

L2
i
b(cid:48)
i

Corollary D.1 (Corollary 3.1). Let the assumptions of Theorem 3.1 hold and p = min
is the expected density of the quantization (see Def. 1.1). If

(cid:110) ζQ
d ,

b(cid:48)
m+b(cid:48)

(cid:111)

, where b(cid:48) ≤ m and ζQ

γ ≤

(cid:114)

L +

1

max{d/ζQ−1,m/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

then VR-MARINA requires

(cid:32)

O

∆0
ε2

(cid:32)

(cid:32)

(cid:114)

L

1 +

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+ L

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:33)(cid:33)

iterations/communication rounds,

(cid:32)

O

m +

∆0
ε2

(cid:32)

(cid:32)

(cid:114)

L

b(cid:48) +

ω max {(d/ζQ − 1)(b(cid:48))2, mb(cid:48)}
n

(cid:33)

(cid:114)

+ L

(1 + ω) max {(d/ζQ − 1)b(cid:48), m}
n

(cid:33)(cid:33)

stochastic oracle calls per node in expectation in order to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communica-
tion cost per worker is

(cid:32)

O

d +

∆0ζQ
ε2

(cid:32)

(cid:32)

(cid:114)

L

1 +

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+ L

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:33)(cid:33)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

Proof of Corollary 3.1. The choice of p = min

(cid:110) ζQ
d ,

b(cid:48)
m+b(cid:48)

(cid:111)

implies

1 − p
p

= max

pm + (1 − p)b(cid:48) ≤

pd + (1 − p)ζQ ≤

− 1,

(cid:26) d
ζQ
2mb(cid:48)
m + b(cid:48) ≤ 2b(cid:48),
ζQ
d

· d +

(cid:18)

1 −

(cid:27)

,

m
b(cid:48)

(cid:19)

ζQ
d
√

√

1

γ ≤

(cid:114)

L +

max{d/ζQ−1,m/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

· ζQ ≤ 2ζQ.

√

a +

b, we get that if

Plugging these relations in (27), (29), (30) and (31) and using

a + b ≤

then VR-MARINA requires

K = O

= O

= O

(cid:32)

(cid:32)

(cid:32)

∆0
ε2

∆0
ε2

∆0
ε2

(cid:32)

(cid:115)

L +

(cid:18)

1 − p
pn

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)(cid:33)

(cid:32)

L +

(cid:114)

L2 ω max {d/ζQ − 1, m/b(cid:48)}
n

+ L2 (1 + ω) max {d/ζQ − 1, m/b(cid:48)}

nb(cid:48)

(cid:33)(cid:33)

(cid:32)

(cid:32)

(cid:114)

L

1 +

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+ L

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

iterations/communication rounds and

m + K(pm + 2(1 − p)b(cid:48)) = O

m +

(cid:32)

(cid:32)

= O

m +

(cid:32)

(cid:115)

L +

(cid:18)

1 − p
pn

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:32)

(cid:32)

(cid:114)

L

1 +

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

∆0
ε2

∆0
ε2

(cid:33)(cid:33)

(cid:33)

(pm + (1 − p)b(cid:48))

(cid:114)

+L

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:33)

(cid:33)

b(cid:48)

(cid:32)

= O

m +

(cid:32)

(cid:32)

(cid:114)

L

b(cid:48) +

∆0
ε2

ω max {(d/ζQ − 1)(b(cid:48))2, mb(cid:48)}
n

(cid:33)

(cid:114)

+L

(1 + ω) max {(d/ζQ − 1)b(cid:48), m}
n

(cid:33)(cid:33)

stochastic oracle calls per node in expectation in order to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communica-
tion cost per worker is

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

∆0
ε2

(cid:18)

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:33)

(pd + (1 − p)ζQ)

(cid:32)

(cid:115)

L +

(cid:32)

(cid:32)

1 − p
pn
(cid:114)

L

1 +

(cid:32)

= O

d +

∆0ζQ
ε2

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

(cid:114)

+L

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:33)(cid:33)

MARINA: Faster Non-Convex Distributed Learning with Compression

D.1.2. CONVERGENCE RESULTS UNDER POLYAK-ŁOJASIEWICZ CONDITION

In this section, we provide an analysis of VR-MARINA under the Polyak-Łojasiewicz condition in the ﬁnite sum case.

Theorem D.2. Consider the ﬁnite sum case (1)+(5). Let Assumptions 1.1, 1.2, 3.1 and 2.1 be satisﬁed and

γ ≤ min






1

(cid:114)

L +

(cid:16)

2(1−p)
pn

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






,

where L2 = 1
n

(cid:80)n

i=1 L2

i and L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of VR-MARINA, we have

where ∆0 = f (x0) − f (x∗). That is, after

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ (1 − γµ)K∆0,

K = O







max






1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ







log

∆0
ε

(cid:17)






(35)

(36)

(37)

iterations VR-MARINA produces such a point xK that E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε, and the expected total number of stochastic
oracle calls per node equals

m + K(pm + 2(1 − p)b(cid:48)) = O



m + max










1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

(cid:17)






(pm + (1 − p)b(cid:48)) log







∆0
ε

.

(38)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of
transmitted vectors from workers to the server we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O







d + max






1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

(cid:17)






(pd + (1 − p)ζQ) log







,

∆0
ε

(39)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof. The proof is very similar to the proof of Theorem 3.1. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(4)

≤ (1 − γµ)E (cid:2)f (xk) − f (x∗)(cid:3) −

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

Using the same arguments as in the proof of (33), we obtain

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(cid:18)

1 − p
n

ωL2 +

(cid:19)

(1 + ω)L2
b(cid:48)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

MARINA: Faster Non-Convex Distributed Learning with Compression

Putting all together we derive that the sequence Φk = f (xk) − f (x∗) + γ

p (cid:107)gk − ∇f (xk)(cid:107)2 satisﬁes

E [Φk+1] ≤ E

(cid:20)
(1 − γµ)(f (xk) − f (x∗)) −

(cid:18) 1
2γ
(1 + ω)L2
b(cid:48)

(cid:19)

−

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

(cid:19)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

(cid:18) γ
2
(1 + ω)L2
b(cid:48)

+

(cid:19)

γ
p

−

(cid:19)

2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:13)
(cid:19)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

(1 − p)

1
2γ

+

L
2

+

γ
p

E

(cid:20) 1 − p
n

(cid:18)

ωL2 +

(cid:20)

= E

(1 − γµ)(f (xk) − f (x∗)) +

(cid:18) γ(1 − p)
pn

+

(cid:18)

ωL2 +

(35)
≤ (1 − γµ)E[Φk],
(cid:16)

where in the last inequality we use γ(1−p)
2γ + L
from (35). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

ωL2 + (1+ω)L2

− 1

pn

b(cid:48)

(cid:17)

2 ≤ 0 and γ

2 + γ

p (1 − p) ≤ (1 − γµ) γ

p following

E (cid:2)f (xk+1) − f (x∗)(cid:3) ≤ E[Φk+1] ≤ (1 − γµ)k+1Φ0 = (1 − γµ)k+1(f (x0) − f (x∗))

that implies (37), (38), and (39).

Corollary D.2. Let the assumptions of Theorem D.2 hold and p = min
density of the quantization (see Def. 1.1). If

(cid:110) ζQ
d ,

b(cid:48)
m+b(cid:48)

(cid:111)

, where b(cid:48) ≤ m and ζQ is the expected

γ ≤ min






1

(cid:114)

L +

2 max{d/ζQ−1,m/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






,

then VR-MARINA requires

(cid:32)

(cid:40)

O

max

(cid:32)

(cid:114)

1 +

1
p

,

L
µ

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:41)

(cid:33)

log

∆0
ε

iterations/communication rounds,

(cid:32)

O

m + max

(cid:40)

b(cid:48)
p

,

L
µ

(cid:32)

(cid:114)

b(cid:48) +

ω max {(d/ζQ − 1)(b(cid:48))2, mb(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {(d/ζQ − 1)b(cid:48), m}
n

(cid:41)

(cid:33)

log

∆0
ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication
cost per worker is

(cid:32)

O

d + ζQ max

(cid:40)

1
p

,

L
µ

(cid:32)

(cid:114)

1 +

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:41)

(cid:33)

log

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof. The choice of p = min

(cid:110) ζQ
d ,

b(cid:48)
m+b(cid:48)

(cid:111)

implies

1 − p
p

= max

− 1,

(cid:26) d
ζQ
2mb(cid:48)
m + b(cid:48) ≤ 2b(cid:48),
ζQ
d

· d +

(cid:18)

1 −

(cid:27)

,

m
b(cid:48)

(cid:19)

ζQ
d

· ζQ ≤ 2ζQ.

pm + (1 − p)b(cid:48) ≤

pd + (1 − p)ζQ ≤

MARINA: Faster Non-Convex Distributed Learning with Compression

Plugging these relations in (35), (37), (38) and (39) and using

a + b ≤

√

√

a +

b, we get that if

√

1

(cid:114)

L +

2 max{d/ζQ−1,m/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






,

γ ≤ min






then VR-MARINA requires

K = O



max







= O

max

(cid:32)

= O

max









(cid:40)

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ







log

∆0
ε

(cid:17)






(cid:113)

L +

L2 ω max{d/ζQ−1,m/b(cid:48)}
+ L2 (1+ω) max{d/ζQ−1,m/b(cid:48)}
µ

nb(cid:48)

n










∆0
ε

log

1
p

,

1
p

,

(cid:32)

(cid:114)

1 +

,

L
µ

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:41)

(cid:33)

log

∆0
ε

1
p

iterations/communication rounds and



m + K(pm + 2(1 − p)b(cid:48)) = O

m + max





(cid:32)

= O

m + max

(cid:32)

(cid:114)

1 +

,

L
µ

1
p

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

1
p

,

(cid:17)






(pm + (1 − p)b(cid:48)) log







∆0
ε





(cid:40)

(cid:33)

ω max {d/ζQ − 1, m/b(cid:48)}
n
(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:41)

b(cid:48) log

(cid:33)

∆0
ε

(cid:32)

= O

m + max

(cid:40)

b(cid:48)
p

,

L
µ

(cid:32)

(cid:114)

b(cid:48) +

ω max {(d/ζQ − 1)(b(cid:48))2, mb(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {(d/ζQ − 1)b(cid:48), m}
n

(cid:41)

(cid:33)

log

∆0
ε

stochastic oracle calls per node in expectation in order to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communi-
cation cost per worker is

d + K(pd + (1 − p)ζQ) = O

d + max







(cid:32)






1
p

,

(cid:40)

= O

d + ζQ max

(cid:114)

L +

1−p
pn

(cid:16)
ωL2 + (1+ω)L2

b(cid:48)

(cid:17)

µ






(pd + (1 − p)ζQ) log







∆0
ε

(cid:32)

(cid:114)

1 +

1
p

,

L
µ

ω max {d/ζQ − 1, m/b(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, m/b(cid:48)}
nb(cid:48)

(cid:41)

(cid:33)

log

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

D.2. Online Case

MARINA: Faster Non-Convex Distributed Learning with Compression

Algorithm 3 VR-MARINA: online case
1: Input: starting point x0, stepsize γ, minibatch sizes b, b(cid:48) < b, probability p ∈ (0, 1], number of iterations K
2: Initialize g0 = 1
nb

(x0), where Ii,0 is the set of the indices in the minibatch, |Ii,0| = b, and ξ0

∇fξ0

(cid:80)n

(cid:80)

i=1

j∈Ii,0

ij

ij is

independently sampled from Di for i ∈ [n], j ∈ [m]

3: for k = 0, 1, . . . , K − 1 do
Sample ck ∼ Be(p)
4:
Broadcast gk to all workers
5:
for i = 1, . . . , n in parallel do
6:
7:

xk+1 = xk − γgk
(cid:80)

8:

Set gk+1

i =




1
b
gk + Q

j∈Ii,k
(cid:16) 1
b(cid:48)

∇fξk
(cid:80)

ij

(xk+1),

(∇fξk
indices in the minibatches, |Ii,k| = b, |I (cid:48)

j∈I (cid:48)



i,k

ij

(xk+1) − ∇fξk
i,k| = b(cid:48), and ξk

ij

(cid:17)

(xk))

if ck = 1,

,

if ck = 0,

where Ii,k, I (cid:48)

i,k are the sets of the

ij is independently sampled from Di for i ∈ [n], j ∈ [m]

end for
gk+1 = 1
n

9:
10:
11: end for
12: Return: ˆxK chosen uniformly at random from {xk}K−1
k=0

i=1 gk+1

(cid:80)n

i

D.2.1. GENERALLY NON-CONVEX PROBLEMS

In this section, we provide the full statement of Theorem 3.2 together with the proof of this result.
Theorem D.3 (Theorem 3.2). Consider the online case (1)+(6). Let Assumptions 1.1, 1.2, 3.2 and 3.3 be satisﬁed and

γ ≤

1

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

where L2 = 1
n

(cid:80)n

i=1 L2

i and L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of VR-MARINA, we have
(cid:18)

(cid:19)

(cid:104)(cid:13)
(cid:13)∇f (ˆxK)(cid:13)
(cid:13)

2(cid:105)

E

≤

2∆0
γK

+

σ2
nb

1 +

,

1
pK

where ˆxK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after
(cid:32)

(cid:115)

(cid:32)

(cid:19)(cid:33)(cid:33)

K = O

1
p

+

∆0
ε2

L +

(cid:18)

1 − p
pn

ωL2 +

(1 + ω)L2
b(cid:48)

(40)

(41)

(42)

iterations with b = Θ( σ2
number of stochastic oracle calls per node equals

nε2 ) VR-MARINA produces such a point ˆxK that E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total

b+K(pb+2(1−p)b(cid:48)) = O

(cid:32)

(cid:32)

σ2
nε2 +

1
p

+

∆0
ε2

(cid:32)

(cid:115)

L +

(cid:18)

ωL2 +

1 − p
pn

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)(cid:33) (cid:18)

p

σ2
nε2 + (1 − p)b(cid:48)

(cid:19)(cid:33)

. (43)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of
transmitted vectors from workers to the server we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

(cid:32)

(cid:32)

(cid:115)

L +

1
p

+

∆0
ε2

(cid:18)

ωL2 +

1 − p
pn

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)(cid:33)

(cid:33)

(pd + (1 − p)ζQ)

,

(44)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 3.2. The proof follows the same steps as the proof of Theorem 3.1. From Lemma B.1, we have
(cid:18) 1
2γ

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

E[f (xk+1)] ≤ E[f (xk)] −

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

L
2

γ
2

γ
2

−

(cid:19)

(45)

MARINA: Faster Non-Convex Distributed Learning with Compression

Next, we need to derive an upper bound for E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3). Since gk+1 = 1

n

n
(cid:80)
i=1

gk+1
i

, we get the following

representation of gk+1:

gk+1 =





1
nb

n
(cid:80)
i=1

gk + 1
n

(cid:80)
j∈Ii,k
n
(cid:80)
i=1

∇fξk
(cid:32)

ij

Q

1
b(cid:48)

(xk+1)

with probability p,

(∇fξk

ij

(xk+1) − ∇fξk

ij

(xk))

with probability 1 − p.

(cid:33)

(cid:80)
j∈I (cid:48)

i,k

Using this, variance decomposition (13), tower property (14), and independence of ξk

ij for i ∈ [n], j ∈ Ii,k, we derive:

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(14)=

(1 − p)E



Q



1
b(cid:48)

1
n

n
(cid:88)

i=1

(cid:88)

j∈I (cid:48)

i,k

(∇fij(xk+1) − ∇fij(xk))

(cid:13)

(cid:13)
(cid:13)
 − ∇f (xk+1)
(cid:13)
(cid:13)
(cid:13)

2









(cid:13)
(cid:13)
(cid:13)
gk +
(cid:13)
(cid:13)
(cid:13)


n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

p
n2b2 E




(cid:88)

(cid:16)

∇fξk

ij

(xk+1) − ∇f (xk+1)

(cid:17)

i=1

j∈Ii,k

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(14),(13)
=

(1 − p)E






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



Q



1
b(cid:48)

1
n

n
(cid:88)

i=1

(cid:88)

j∈I (cid:48)

i,k

(∇fij(xk+1) − ∇fij(xk))


 − ∇f (xk+1) + ∇f (xk)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2




+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+

p
n2b2

n
(cid:88)

(cid:88)

i=1

j∈Ii,k

(cid:20)(cid:13)
(cid:13)
(cid:13)∇fξk

ij

E

(cid:13)
(xk+1) − ∇f (xk+1)
(cid:13)
(cid:13)

2(cid:21)

(14),(10)
=

(1 − p)E






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



Q



1
b(cid:48)

1
n

n
(cid:88)

i=1

(cid:88)

j∈I (cid:48)

i,k

(∇fij(xk+1) − ∇fij(xk))


 − ∇f (xk+1) + ∇f (xk)

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+

pσ2
nb

,

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where σ2 = 1
n

(cid:80)n

i=1 σ2

i . Applying the same arguments as in the proof of inequality (33), we obtain
(cid:18)

(cid:19)

1 − p
n

ωL2 +

(1 + ω)L2
b(cid:48)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

Next, we introduce new notation: Φk = f (xk) − f∗ + γ
establish the following inequality:

2p (cid:107)gk − ∇f (xk)(cid:107)2. Using this and inequalities (45) and (46), we

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+

pσ2
nb

.

(46)

E [Φk+1] ≤ E

(cid:20)
f (xk) − f∗ −

γ
2p

E

(cid:20) 1 − p
n

+

γ
2
(cid:18)

ωL2 +

(cid:107)∇f (xk)(cid:107)2 −

(cid:18) 1
2γ
(cid:19)

(cid:19)

−

L
2

(cid:107)xk+1 − xk(cid:107)2 +

γ
2
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
2
(cid:13)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

+

(cid:21)

pσ2
nb

= E [Φk] −

(40)
≤ E [Φk] −

γ
2
γ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(1 + ω)L2
b(cid:48)
(cid:18) γ(1 − p)
2pn

,

γσ2
2nb
ωL2 + (1+ω)L2

(cid:16)

(cid:17)

(cid:18)

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)

−

1
2γ

+

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γσ2
2nb

(47)

where in the last inequality, we use γ(1−p)
− 1
2pn
(47) for k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

b(cid:48)

2γ + L

2 ≤ 0 following from (40). Summing up inequalities

1
K

K−1
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ≤

2
γK

K−1
(cid:88)

k=0

(E[Φk] − E[Φk+1]) +

σ2
nb

=

2 (E[Φ0] − E[ΦK])
γK

+

σ2
nb

≤

2∆0
γK

+

σ2
nb

(cid:18)

1 +

(cid:19)

,

1
pK

MARINA: Faster Non-Convex Distributed Learning with Compression

since g0 = 1
nb
obtain (41) that implies (42), (43), and (44).

∇fξ0

j∈Ii,0

i=1

ij

(cid:80)n

(cid:80)

(x0) and ΦK ≥ 0. Finally, using the tower property (14) and the deﬁnition of ˆxK, we

Remark D.2 (About batchsizes dissimilarity). Similarly to the ﬁnite sum case, our analysis can be easily extended to handle
the version of VR-MARINA with different batchsizes b1, . . . , bn and b(cid:48)
n on different workers, i.e., when |Ii,k| = bi,
i for i ∈ [n]. In this case, the statement of Theorem 3.2 remains the same with the small modiﬁciation: instead of L2
|I (cid:48)
b(cid:48)
the complexity bounds will have 1
= Θ(ε2).
n

, and instead of the requirement b = Θ

it will have 1
n2

i,k| = b(cid:48)

1, . . . , b(cid:48)

(cid:16) σ2
nε

(cid:80)n

(cid:80)n

i=1

i=1

(cid:17)

σ2
i
bi

L2
i
b(cid:48)
i

Corollary D.3 (Corollary 3.2). Let the assumptions of Theorem 3.2 hold and p = min
b = Θ (cid:0)σ2/(nε2)(cid:1) and ζQ is the expected density of the quantization (see Def. 1.1). If

(cid:110) ζQ
d ,

b(cid:48)
b+b(cid:48)

(cid:111)

, where b(cid:48) ≤ b,

γ ≤

(cid:114)

L +

1

max{d/ζQ−1,b/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

then VR-MARINA requires

(cid:32)

O

max

(cid:27)

(cid:26) d
ζQ

,

σ2
nb(cid:48)ε2

+

∆0
ε2

(cid:32)

(cid:32)

(cid:115)

L

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)

(cid:115)

+ L

(1 + ω)

nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)(cid:33)

iterations/communication rounds and

(cid:32)

O

max

(cid:27)

(cid:26) db(cid:48)
ζQ

,

σ2
nε2

+

∆0Lb(cid:48)

ε2 +

∆0L
ε2

(cid:115)

ωb(cid:48)
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

(cid:27)

b(cid:48),

σ2
nε2

+

∆0L
ε2

(cid:115)

1 + ω
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nε2

(cid:27)(cid:33)

stochastic oracle calls per node in expectation to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost
per worker is

(cid:32)

(cid:26)

O

max

d,

(cid:27)

σ2ζQ
nb(cid:48)ε2

+

∆0ζQ
ε2

(cid:32)

(cid:32)

(cid:115)

L

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)

(cid:115)

+ L

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)(cid:33)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof of Corollary 3.1. The choice of p = min

(cid:110) ζQ
d ,

b(cid:48)
b+b(cid:48)

(cid:111)

implies

1 − p
p

pb + (1 − p)b(cid:48) ≤

pd + (1 − p)ζQ ≤

= max

− 1,

(cid:26) d
ζQ
2bb(cid:48)
b + b(cid:48) ≤ 2b(cid:48),
(cid:18)
ζQ
d

· d +

1 −

(cid:27)

,

b
b(cid:48)

(cid:19)

ζQ
d

· ζQ ≤ 2ζQ.

Plugging these relations in (40), (42), (43) and (44) and using

a + b ≤

√

√

a +

b, we get that if

√

1

γ ≤

(cid:114)

L +

max{d/ζQ−1,b/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

,

(cid:17)

MARINA: Faster Non-Convex Distributed Learning with Compression

then VR-MARINA requires

1
p

+

∆0
ε2

(cid:32)

(cid:115)

L +

(cid:18)

1 − p
pn

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)(cid:33)

K = O

(cid:32)

(cid:32)

= O

max

(cid:32)

= O

max

(cid:27)

(cid:27)

(cid:26) d
ζQ

(cid:26) d
ζQ

,

,

σ2
nb(cid:48)ε2

σ2
nb(cid:48)ε2

+

+

∆0
ε2

∆0
ε2

(cid:32)

L +

(cid:114)

L2 ω max {d/ζQ − 1, b/b(cid:48)}
n

+ L2 (1 + ω) max {d/ζQ − 1, b/b(cid:48)}

nb(cid:48)

(cid:33)(cid:33)

(cid:32)

(cid:32)

(cid:115)

L

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)

(cid:115)

+L

(1 + ω)

nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)(cid:33)

iterations/communication rounds and

b + K(pb + 2(1 − p)b(cid:48)) = O

b +

(cid:32)

(1 − p)b(cid:48)
p

+

∆0
ε2

(cid:32)

= O

max

(cid:27)

, b

+

(cid:26) db(cid:48)
ζQ

∆0
ε2

(cid:32)

(cid:115)

L +

(cid:32)

(cid:32)

1 − p
pn
(cid:114)

L

1 +

(cid:18)

ωL2 +

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:33)

(pb + (1 − p)b(cid:48))

ω max {d/ζQ − 1, b/b(cid:48)}
n

(cid:33)

(cid:114)

+L

(1 + ω) max {d/ζQ − 1, b/b(cid:48)}
nb(cid:48)

(cid:33)

(cid:33)

b(cid:48)

(cid:32)

= O

max

(cid:27)

(cid:26) db(cid:48)
ζQ

,

σ2
nε2

+

∆0
ε2

(cid:32)

(cid:32)

(cid:115)

L

b(cid:48) +

ωb(cid:48)
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nε2

(cid:27)(cid:33)

(cid:115)

+L

1 + ω
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nε2

(cid:27)(cid:33)(cid:33)

stochastic oracle calls per node in expectation to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost
per worker is

d + K(pd + (1 − p)ζQ) = O

d +

(cid:32)

(1 − p)ζQ
p

(cid:32)

(cid:26)

= O

max

d,

σ2ζQ
nb(cid:48)ε2

+

(cid:27)

(cid:32)

(cid:115)

L +

∆0
ε2

(cid:18)

ωL2 +

1 − p
pn

(1 + ω)L2
b(cid:48)

(cid:19)(cid:33)

(cid:33)

(pd + (1 − p)ζQ)

+

∆0ζQ
ε2

(cid:32)

(cid:32)

(cid:115)

L

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)

(cid:115)

+L

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)ε2

(cid:27)(cid:33)(cid:33)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

D.2.2. CONVERGENCE RESULTS UNDER POLYAK-ŁOJASIEWICZ CONDITION

In this section, we provide an analysis of VR-MARINA under Polyak-Łojasiewicz condition in the online case.

Theorem D.4. Consider the ﬁnite sum case (1)+(6). Let Assumptions 1.1, 1.2, 3.2, 2.1 and 3.3 be satisﬁed and

γ ≤ min






1

(cid:114)

L +

(cid:16)

2(1−p)
pn

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






,

(48)

MARINA: Faster Non-Convex Distributed Learning with Compression

where L2 = 1
n

(cid:80)n

i=1 L2

i and L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of VR-MARINA, we have

where ∆0 = f (x0) − f (x∗). That is, after

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ (1 − γµ)K∆0 +

σ2
nbµ

,

K = O







max






1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ







log

∆0
ε

(cid:17)






(49)

(50)

(cid:16) σ2
iterations with b = Θ
nµε
number of stochastic oracle calls per node equals

(cid:17)

VR-MARINA produces such a point xK that E (cid:2)f (xK) − f (x∗)(cid:3) ≤ ε, and the expected total

b + K(pb + 2(1 − p)b(cid:48)) = O







m + max






1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

(cid:17)






(pb + (1 − p)b(cid:48)) log







.

∆0
ε

(51)

Moreover, under an assumption that the communication cost is proportional to the number of non-zero components of
transmitted vectors from workers to the server, we have that the expected total communication cost per worker equals

d + K(pd + (1 − p)ζQ) = O







d + max






1
p

,

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

(cid:17)






(pd + (1 − p)ζQ) log







,

∆0
ε

(52)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof. The proof is very similar to the proof of Theorem 3.2. From Lemma B.1 and PŁ condition, we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(4)

≤ (1 − γµ)E (cid:2)f (xk) − f (x∗)(cid:3) −

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

Using the same arguments as in the proof of (46), we obtain

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(cid:18)

1 − p
n

ωL2 +

(cid:19)

(1 + ω)L2
b(cid:48)
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

+(1 − p)E

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

2(cid:105)

+

pσ2
nb

.

(53)

Putting all together, we derive that the sequence Φk = f (xk) − f (x∗) + γ

E [Φk+1] ≤ E

= E

(cid:20)
(1 − γµ)(f (xk) − f (x∗)) −

+

γ
p

E

(cid:20) 1 − p
n

(cid:18)

ωL2 +

(cid:20)
(1 − γµ)(f (xk) − f (x∗)) +

(cid:18) γ(1 − p)
pn

+

(cid:18)

ωL2 +

(35)
≤ (1 − γµ)E[Φk] +

γσ2
nb

,

−

(cid:19)

p (cid:107)gk − ∇f (xk)(cid:107)2 satisﬁes
γ
2
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

(cid:107)xk+1 − xk(cid:107)2 +

L
2

(cid:19)

(cid:13)gk − ∇f (xk)(cid:13)
2
(cid:13)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

+

(cid:21)

pσ2
nb

+

(cid:19)

γ
p

−

(1 − p)

1
2γ

+

L
2

(cid:19)

2(cid:21)
(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:19)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

γσ2
nb

(cid:18) 1
2γ
(1 + ω)L2
b(cid:48)

(cid:18) γ
2
(1 + ω)L2
b(cid:48)

MARINA: Faster Non-Convex Distributed Learning with Compression

where in the last inequality we use γ(1−p)
2γ + L
from (48). Unrolling the recurrence and using g0 = ∇f (x0), we obtain

ωL2 + (1+ω)L2

− 1

pn

b(cid:48)

(cid:16)

(cid:17)

2 ≤ 0 and γ

2 + γ

p (1 − p) ≤ (1 − γµ) γ

p following

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ E[ΦK] ≤ (1 − γµ)KΦ0 +

γσ2
nb

K−1
(cid:88)

k=0

(1 − γµ)k

≤ (1 − γµ)K(f (x0) − f (x∗)) +

≤ (1 − γµ)K(f (x0) − f (x∗)) +

γσ2
nb

∞
(cid:88)

(1 − γµ)k

k=0

σ2
nbµ

.

Together with b = Θ

(cid:17)

(cid:16) σ2
nµε

it implies (50), (51), and (52).

Corollary D.4. Let the assumptions of Theorem D.4 hold and p = min
density of the quantization (see Def. 1.1). If

(cid:110) ζQ
d ,

b(cid:48)
b+b(cid:48)

(cid:111)

, where b(cid:48) ≤ b and ζQ is the expected

γ ≤ min






1

(cid:114)

L +

2 max{d/ζQ−1,b/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






and

then VR-MARINA requires

b = Θ

(cid:18) σ2
nµε

(cid:19)

,

σ2 =

1
n

n
(cid:88)

i=1

σ2
i ,

(cid:32)

(cid:40)

O

max

d
ζQ

,

σ2
nb(cid:48)µε

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:33)

(cid:115)

+

L
µ

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:41)

(cid:33)

log

∆0
ε

iterations/communication rounds,

(cid:32)

(cid:40)

O

max

b(cid:48)d
ζQ

,

σ2
nµε

,

L
µ

(cid:32)

(cid:115)

b(cid:48) +

ωb(cid:48)
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nµε

(cid:27)(cid:33)

(cid:114)

+ L
µ

1+ω
n max

(cid:110)(cid:16) d
ζQ

(cid:17)

− 1

b(cid:48), σ2
nµε

(cid:41)

(cid:111)

(cid:33)

log ∆0
ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication
cost per worker is

(cid:32)

(cid:40)

O

ζQ max

d
ζQ

,

σ2
nb(cid:48)µε

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µ

(cid:27)(cid:33)

(cid:115)

+

L
µ

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µ

(cid:27)(cid:41)

(cid:33)

log

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof. The choice of p = min

(cid:110) ζQ
d ,

b(cid:48)
b+b(cid:48)

(cid:111)

implies

1 − p
p

= max

− 1,

(cid:26) d
ζQ
2bb(cid:48)
b + b(cid:48) ≤ 2b(cid:48),
(cid:18)
ζQ
d

· d +

1 −

(cid:27)

,

b
b(cid:48)

(cid:19)

ζQ
d

· ζQ ≤ 2ζQ.

pb + (1 − p)b(cid:48) ≤

pd + (1 − p)ζQ ≤

MARINA: Faster Non-Convex Distributed Learning with Compression

Plugging these relations in (48), (50), (51) and (52) and using

a + b ≤

√

√

a +

b, we get that if

√

1

(cid:114)

L +

2 max{d/ζQ−1,b/b(cid:48)}
n

(cid:16)

ωL2 + (1+ω)L2

b(cid:48)

p
2µ

,

(cid:17)






,

γ ≤ min






then VR-MARINA requires

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

1
p

,







log

∆0
ε

(cid:17)






(cid:113)

L +

d
ζQ

,

b
b(cid:48) ,

+ L2 (1+ω) max{d/ζQ−1,b/b(cid:48)}
L2 ω max{d/ζQ−1,b/b(cid:48)}
µ

nb(cid:48)

n










∆0
ε

log

K = O



max







= O

max

(cid:32)

= O

max









(cid:40)

,

σ2
nb(cid:48)µε

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:33)

(cid:115)

+

L
µ

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:41)

(cid:33)

log

∆0
ε

d
ζQ

iterations/communication rounds and

b + K(pb + 2(1 − p)b(cid:48)) = O







(cid:32)

b + max




(cid:40)

= O

b + max

(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

1
p

,

(cid:17)






(pb + (1 − p)b(cid:48)) log







∆0
ε

d
ζQ

(cid:32)

(cid:114)

1 +

,

b
b(cid:48) ,

L
µ

ω max {d/ζQ − 1, b/b(cid:48)}
n

(cid:33)

(cid:114)

+

L
µ

(1 + ω) max {d/ζQ − 1, b/b(cid:48)}
nb(cid:48)

(cid:41)

b(cid:48) log

(cid:33)

∆0
ε

(cid:32)

(cid:40)

= O

max

b(cid:48)d
ζQ

,

σ2
nµε

,

L
µ

(cid:32)

(cid:115)

b(cid:48) +

ωb(cid:48)
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nµε

(cid:27)(cid:33)

(cid:115)

+

L
µ

1 + ω
n

max

(cid:26)(cid:18) d
ζQ

(cid:19)

− 1

b(cid:48),

σ2
nµε

(cid:27)(cid:41)

(cid:33)

log

∆0
ε

stochastic oracle calls per node in expectation to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication
cost per worker is

d + K(pd + (1 − p)ζQ) = O

d + max










(cid:114)

(cid:16)

1−p
pn

L +

ωL2 + (1+ω)L2

b(cid:48)

µ

1
p

,

(cid:17)






(pd + (1 − p)ζQ) log







∆0
ε

(cid:32)

= O

ζQ max


(cid:40)

d
ζQ

,

σ2
nb(cid:48)µε

,

L
µ

(cid:32)

(cid:115)

1 +

ω
n

max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:33)

(cid:115)

+

L
µ

1 + ω
nb(cid:48) max

(cid:26) d
ζQ

− 1,

σ2
nb(cid:48)µε

(cid:27)(cid:41)

(cid:33)

log

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

MARINA: Faster Non-Convex Distributed Learning with Compression

E. Missing Proofs for PP-MARINA

Algorithm 4 PP-MARINA
1: Input: starting point x0, stepsize γ, probability p ∈ (0, 1], number of iterations K, clients-batchsize r ≤ n
2: Initialize g0 = ∇f (x0)
3: for k = 0, 1, . . . , K − 1 do
Sample ck ∼ Be(p)
4:
Choose I (cid:48)
5:
{1, . . . , n} otherwise
Broadcast gk to all workers
for i = 1, . . . , n in parallel do

k = {1, . . . , n} if ck = 1, and choose I (cid:48)

k as the set of r i.i.d. samples from the uniform distribution over

6:
7:
8:

9:

10:

11:

xk+1 = xk − γgk

(cid:40)

Set gk+1

i =

∇fi(xk+1)
gk + Q (cid:0)∇fi(xk+1) − ∇fi(xk)(cid:1)

if ck = 1,
if ck = 0.

end for

Set gk+1 =






∇f (xk+1)
(cid:80)
gk + 1
r
ik∈I (cid:48)
k

Q (cid:0)∇fik (xk+1) − ∇fik (xk)(cid:1)

if ck = 1,
if ck = 0.

12: end for
13: Return: ˆxK chosen uniformly at random from {xk}K−1
k=0

E.1. Generally Non-Convex Problems

In this section, we provide the full statement of Theorem 4.1 together with the proof of this result.

Theorem E.1 (Theorem 4.1). Let Assumptions 1.1 and 1.2 be satisﬁed and

γ ≤

1

(cid:18)

L

1 +

(cid:113) (1−p)(1+ω)
pr

(cid:19) ,

where L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of PP-MARINA, we have

(cid:104)(cid:13)
(cid:13)∇f (ˆxK)(cid:13)
(cid:13)

2(cid:105)

E

≤

2∆0
γK

,

where ˆxK is chosen uniformly at random from x0, . . . , xK−1 and ∆0 = f (x0) − f∗. That is, after

K = O

(cid:32)

∆0L
ε2

(cid:32)

(cid:115)

1 +

(1 − p)(1 + ω)
pr

(cid:33)(cid:33)

(54)

(55)

(56)

iterations PP-MARINA produces such a point ˆxK that E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2. Moreover, under an assumption that the
communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server,
we have that the expected total communication cost (for all workers) equals

dn + K(pdn + (1 − p)ζQr) = O

dn +

(cid:32)

(cid:32)

(cid:115)

1 +

∆0L
ε2

(1 − p)(1 + ω)
pr

(cid:33)

(pdn + (1 − p)ζQr)

,

(57)

(cid:33)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof of Theorem 4.1. The proof is very similar to the proof of Theorem 3.1. From Lemma B.1, we have

E[f (xk+1)] ≤ E[f (xk)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

(58)

MARINA: Faster Non-Convex Distributed Learning with Compression

Next, we need to derive an upper bound for E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3). By deﬁnition of gk+1, we have

gk+1 =






∇f (xk+1)
(cid:80)
gk + 1
r
ik∈I (cid:48)
k

Q (cid:0)∇fik (xk+1) − ∇fik (xk)(cid:1) with probability 1 − p.

with probability p,

Using this, variance decomposition (13) and tower property (14), we derive:

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

(14)=

(1 − p)E

(14),(13)
=

(1 − p)E






(cid:13)
(cid:13)
(cid:13)
gk +
(cid:13)
(cid:13)
(cid:13)

1
r

(cid:88)

ik∈I (cid:48)
k

(cid:13)
(cid:13)
(cid:13)
Q (cid:0)∇fik (xk+1) − ∇fik (xk)(cid:1) − ∇f (xk+1)
(cid:13)
(cid:13)
(cid:13)

2









(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
r

(cid:88)

(cid:13)
(cid:13)
(cid:13)
Q (cid:0)∇fik (xk+1) − ∇fik (xk)(cid:1) − ∇f (xk+1) + ∇f (xk)
(cid:13)
(cid:13)
(cid:13)

2




ik∈I (cid:48)
k
(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

+(1 − p)E

Next, we use the notation: ∆k
E (cid:2)∆k
ik
xk+1. These observations imply

| xk, xk+1(cid:3) = ∆k for all ik ∈ I (cid:48)

i = ∇fi(xk+1) − ∇fi(xk) for i ∈ [n] and ∆k = ∇f (xk+1) − ∇f (xk). These vectors satisfy
k are independent random vectors for ﬁxed xk and

k. Moreover, Q(∆k
ik

) for ik ∈ I (cid:48)

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3)

=

(1 − p)E






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
r

(cid:88)

ik∈I (cid:48)
k

(cid:0)Q(∆k
ik

) − ∆k(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2


 + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

=

(14),(13)
=

1 − p
r2 E





(cid:88)

ik∈I (cid:48)
k

(cid:13)
(cid:13)Q(∆k
ik

) − ∆k
ik

+ ∆k
ik

− ∆k(cid:13)
2
(cid:13)

 + (1 − p)E



(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

1 − p
rn

n
(cid:88)

(cid:16)

i=1

(cid:104)(cid:13)
(cid:13)Q(∆k

i ) − ∆k
i

2(cid:105)

(cid:13)
(cid:13)

E

+ E

+(1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)∆k

i − ∆k(cid:13)
(cid:13)

2(cid:105)(cid:17)

(14),(3)
=

1 − p
rn

n
(cid:88)

(cid:16)

i=1

ωE

(cid:104)(cid:13)
(cid:13)∆k
i

2(cid:105)

(cid:13)
(cid:13)

+ E

(cid:104)(cid:13)
(cid:13)∆k

i − ∆k(cid:13)
(cid:13)

2(cid:105)(cid:17)

+ (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

(14),(13)
=

(1 − p)(1 + ω)
rn

n
(cid:88)

i=1

(cid:104)(cid:13)
(cid:13)∆k
i

2(cid:105)

(cid:13)
(cid:13)

E

+ (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

Using L-smoothness (2) of fi together with the tower property (14), we get

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(1 − p)(1 + ω)
nr

n
(cid:88)

i=1

i E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E
L2

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

=

(1 − p)(1 + ω)L2
r

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

(59)

Next, we introduce new notation: Φk = f (xk) − f∗ + γ

2p (cid:107)gk − ∇f (xk)(cid:107)2. Using this and inequalities (58) and (59), we

MARINA: Faster Non-Convex Distributed Learning with Compression

establish the following inequality:

E [Φk+1] ≤ E

(cid:20)
f (xk) − f∗ −

(cid:107)∇f (xk)(cid:107)2 −

γ
2
(cid:20) (1 − p)(1 + ω)L2
r

(cid:19)

−

(cid:18) 1
2γ

L
2
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

+

γ
2p

E

(cid:107)xk+1 − xk(cid:107)2 +

2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

γ
2

= E [Φk] −

(54)
≤ E [Φk] −

γ
2
γ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(cid:18) γ(1 − p)(1 + ω)L2
2pr

−

1
2γ

+

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ,

(60)

where in the last inequality we use γ(1−p)(1+ω)L2
k = 0, 1, . . . , K − 1 and rearranging the terms, we derive

− 1

2pr

2γ + L

2 ≤ 0 following from (54). Summing up inequalities (34) for

1
K

K−1
(cid:88)

k=0

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ≤

2
γK

K−1
(cid:88)

k=0

(E[Φk] − E[Φk+1]) =

2 (E[Φ0] − E[ΦK])
γK

=

2∆0
γK

,

since g0 = ∇f (x0) and Φk+1 ≥ 0. Finally, using the tower property (14) and the deﬁnition of ˆxK, we obtain (55) that
implies (56) and (57).

Corollary E.1 (Corollary 4.1). Let the assumptions of Theorem 4.1 hold and p = ζQr
density of the quantization (see Def. 1.1). If

dn , where r ≤ n and ζQ is the expected

then PP-MARINA requires

γ ≤

1

(cid:18)

(cid:114)

L

1 +

1+ω
r

(cid:16) dn

ζQr − 1

(cid:17)(cid:19) ,

K = O

(cid:32)

∆0L
ε2

(cid:32)

(cid:115)

1 +

1 + ω
r

(cid:18) dn
ζQr

(cid:19)(cid:33)(cid:33)

− 1

iterations/communication rounds to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost is

(cid:18)

O

dn +

(cid:16)

∆0L
ε2

ζQr + (cid:112)(1 + ω)ζQ (dn − ζQr)

(cid:17)(cid:19)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof of Corollary 4.1. The choice of p = ζQr

dn implies
1 − p
p

=

dn
ζQr

− 1,

(cid:18)

pdn + (1 − p)ζQr ≤ ζQr +

1 −

Plugging these relations in (54), (56), and (57), we get that if

(cid:19)

ζQr
dn

· ζQr ≤ 2ζQr.

then PP-MARINA requires

γ ≤

1

(cid:18)

(cid:114)

L

1 +

1+ω
r

(cid:16) dn

ζQr − 1

(cid:17)(cid:19) ,

K = O

= O

(cid:32)

(cid:32)

∆0L
ε2

∆0L
ε2

(cid:32)

(cid:115)

1 +

(cid:32)

(cid:115)

1 +

(cid:33)(cid:33)

(1 − p)(1 + ω)
pr

1 + ω
r

(cid:18) dn
ζQr

(cid:19)(cid:33)(cid:33)

− 1

MARINA: Faster Non-Convex Distributed Learning with Compression

iterations/communication rounds in order to achieve E[(cid:107)∇f (ˆxK)(cid:107)2] ≤ ε2, and the expected total communication cost is

dn + K(pdn + (1 − p)ζQr) = O

dn +

(cid:32)

(cid:32)

(cid:115)

1 +

∆0L
ε2

(1 − p)(1 + ω)
pr

(cid:33)

(pdn + (1 − p)ζQr)

(cid:33)

(cid:18)

= O

dn +

(cid:16)

∆0L
ε2

ζQr + (cid:112)(1 + ω)ζQ (dn − ζQr)

(cid:17)(cid:19)

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

E.2. Convergence Results Under Polyak-Łojasiewicz condition

In this section, we provide an analysis of PP-MARINA under Polyak-Łojasiewicz condition.

Theorem E.2. Let Assumptions 1.1, 1.2 and 2.1 be satisﬁed and

γ ≤ min




1

(cid:18)

1 +



L

(cid:113) 2(1−p)(1+ω)
pr

(cid:19) ,

p
2µ






,

where L2 = 1
n

(cid:80)n

i=1 L2

i . Then after K iterations of PP-MARINA, we have

where ∆0 = f (x0) − f (x∗). That is, after

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ (1 − γµ)K∆0,

(cid:32)

(cid:40)

K = O

max

(cid:32)

(cid:115)

1 +

(1 − p)(1 + ω)
pr

(cid:33)(cid:41)

(cid:33)

log

∆0
ε

1
p

,

L
µ

(61)

(62)

(63)

iterations PP-MARINA produces such a point xK that E[f (xK) − f (x∗)] ≤ ε. Moreover, under an assumption that the
communication cost is proportional to the number of non-zero components of transmitted vectors from workers to the server,
we have that the expected total communication cost (for all workers) equals

dn + K(pdn + (1 − p)ζQr) = O

dn + max

(cid:32)

(cid:40)

1
p

,

L
µ

(cid:32)

(cid:115)

1 +

(1 − p)(1 + ω)
pr

(cid:33)(cid:41)

(pdn + (1 − p)ζQr) log

(cid:33)

∆0
ε

, (64)

where ζQ is the expected density of the quantization (see Def. 1.1).

Proof. The proof is very similar to the proof of Theorem 4.1. From Lemma B.1 and PŁ condition we have

E[f (xk+1) − f (x∗)] ≤ E[f (xk) − f (x∗)] −

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) −

γ
2

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

+

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3)

(4)

≤ (1 − γµ)E (cid:2)f (xk) − f (x∗)(cid:3) −

(cid:18) 1
2γ

−

(cid:19)

L
2

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) +

γ
2

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) .

Using the same arguments as in the proof of (59), we obtain

E (cid:2)(cid:107)gk+1 − ∇f (xk+1)(cid:107)2(cid:3) ≤

(1 − p)(1 + ω)L2
r

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) + (1 − p)E

(cid:104)(cid:13)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)

2(cid:105)

.

MARINA: Faster Non-Convex Distributed Learning with Compression

Putting all together, we derive that the sequence Φk = f (xk) − f (x∗) + γ

p (cid:107)gk − ∇f (xk)(cid:107)2 satisﬁes

E [Φk+1] ≤ E

(cid:20)

(1 − γµ)(f (xk) − f (x∗)) −

(cid:107)xk+1 − xk(cid:107)2 +

(cid:107)gk − ∇f (xk)(cid:107)2

(cid:21)

(cid:19)

−

(cid:18) 1
2γ

L
2
(cid:107)xk+1 − xk(cid:107)2 + (1 − p) (cid:13)

γ
2
2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
2(cid:21)
(cid:13)gk − ∇f (xk)(cid:13)
(cid:13)
(cid:13)

+

γ
p

E

(cid:20) (1 − p)(1 + ω)L2
r

(cid:20)
(1 − γµ)(f (xk) − f (x∗)) +

= E

(cid:18) γ
2

(cid:18) γ(1 − p)(1 + ω)L2
pr

+

−

1
2γ

+

L
2

(61)
≤ (1 − γµ)E[Φk],

+

γ
p
(cid:19)

(cid:19)

(1 − p)

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

where in the last inequality we use γ(1−p)(1+ω)L2
2γ + L
Unrolling the recurrence and using g0 = ∇f (x0), we obtain

− 1

pr

2 ≤ 0 and γ

2 + γ

p (1 − p) ≤ (1 − γµ) γ

p following from (61).

E (cid:2)f (xK) − f (x∗)(cid:3) ≤ E[ΦK] ≤ (1 − γµ)KΦ0 = (1 − γµ)K(f (x0) − f (x∗))

that implies (63) and (64).

Corollary E.2. Let the assumptions of Theorem E.2 hold and p = ζQr
quantization (see Def. 1.1). If

dn , where r ≤ n and ζQ is the expected density of the

γ ≤ min

then PP-MARINA requires




1

(cid:18)

(cid:114)

1 +



L

2(1+ω)
r

(cid:17)(cid:19) ,

(cid:16) dn

ζQr − 1






,

p
2µ

(cid:32)

(cid:40)

K = O

max

dn
ζQr

L
µ

(cid:32)

(cid:115)

1 +

1 + ω
r

(cid:18) dn
ζQr

− 1

(cid:19)(cid:33)(cid:41)

(cid:33)

log

∆0
ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

(cid:18)

(cid:26)

O

dn + max

dn,

(cid:16)

L
µ

ζQr + (cid:112)(1 + ω)ζQ (dn − ζQr)

(cid:17)(cid:27)

log

(cid:19)

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

Proof. The choice of p = ζQr

dn implies

1 − p
p

=

dn
ζQr

− 1,

(cid:18)

pdn + (1 − p)ζQr ≤ ζQr +

1 −

(cid:19)

ζQr
dn

· ζQr ≤ 2ζQr.

Plugging these relations in (61), (63), and (64), we get that if

γ ≤ min





1

(cid:18)

(cid:114)

L

1 +

2(1+ω)
r





,

p
2µ

(cid:17)(cid:19) ,

(cid:16) dn

ζQr − 1

MARINA: Faster Non-Convex Distributed Learning with Compression

then PP-MARINA requires

(cid:32)

(cid:40)

K = O

max

(cid:32)

(cid:115)

1 +

(1 − p)(1 + ω)
pr

(cid:33)(cid:41)

(cid:33)

log

∆0
ε

1
p

,

L
µ

(cid:32)

(cid:40)

= O

max

dn
ζQr

L
µ

(cid:32)

(cid:115)

1 +

1 + ω
r

(cid:18) dn
ζQr

− 1

(cid:19)(cid:33)(cid:41)

(cid:33)

log

∆0
ε

iterations/communication rounds to achieve E[f (xK) − f (x∗)] ≤ ε, and the expected total communication cost is

dn + K(pdn + (1 − p)ζQr) = O

dn + max

(cid:32)

(cid:18)

(cid:40)

(cid:26)

(cid:32)

(cid:115)

1 +

1
p

,

L
µ

(1 − p)(1 + ω)
pr

(cid:33)(cid:41)

(pdn + (1 − p)ζQr) log

(cid:33)

∆0
ε

= O

dn + max

dn,

(cid:16)

L
µ

ζQr + (cid:112)(1 + ω)ζQ (dn − ζQr)

(cid:17)(cid:27)

log

(cid:19)

∆0
ε

under an assumption that the communication cost is proportional to the number of non-zero components of transmitted
vectors from workers to the server.

