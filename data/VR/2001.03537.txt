0
2
0
2

n
a
J

8

]

C
D
.
s
c
[

1
v
7
3
5
3
0
.
1
0
0
2
:
v
i
X
r
a

OO-VR: NUMA Friendly Object-Oriented VR Rendering
Framework For Future NUMA-Based Multi-GPU Systems

Chenhao Xie
chenhao.xie@pnnl.gov
Pacific Northwest National Lab (PNNL) and
University of Houston

Xin Fu
xfu8@central.uh.edu
ECMOS Lab, ECE Department,
University of Houston

Mingsong Chen
mschen@sei.ecnu.edu.cn
School of Computer Science and Software Engineering,
East China Normal University

Shuaiwen Leon Song
Shuaiwen.Song@pnnl.gov
Pacific Northwest National Lab (PNNL) and
The University of Sydney

ABSTRACT
With the strong computation capability, NUMA-based multi-GPU
system is a promising candidate to provide sustainable and scalable
performance for Virtual Reality (VR) applications and deliver the
excellent user experience. However, the entire multi-GPU system
is viewed as a single GPU under the single programming model
which greatly ignores the data locality among VR rendering tasks
during the workload distribution, leading to tremendous remote
memory accesses among GPU models (GPMs). The limited inter-
GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major
obstacle when executing VR applications in the multi-GPU system.
By conducting comprehensive characterizations on different kinds
of parallel rendering frameworks, we observe that distributing
the rendering object along with its required data per GPM can
reduce the inter-GPM memory accesses. However, this object-level
rendering still faces two major challenges in NUMA-based multi-
GPU system: (1) the large data locality between the left and right
views of the same object and the data sharing among different
objects and (2) the unbalanced workloads induced by the software-
level distribution and composition mechanisms.

To tackle these challenges, we propose object-oriented VR ren-
dering framework (OO-VR) that conducts the software and hard-
ware co-optimization to provide a NUMA friendly solution for VR
multi-view rendering in NUMA-based multi-GPU systems. We first
propose an object-oriented VR programming model to exploit the
data sharing between two views of the same object and group
objects into batches based on their texture sharing levels. Then,
we design an object aware runtime batch distribution engine and
distributed hardware composition unit to achieve the balanced
workloads among GPMs and further improve the performance of
VR rendering. Finally, evaluations on our VR featured simulator
show that OO-VR provides 1.58x overall performance improvement

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ISCA ’19, June 22–26, 2019, Phoenix, AZ, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6669-4/19/06. . . $15.00
https://doi.org/10.1145/3307650.3322247

and 76% inter-GPM memory traffic reduction over the state-of-
the-art multi-GPU systems. In addition, OO-VR provides NUMA
friendly performance scalability for the future larger multi-GPU
scenarios with ever increasing asymmetric bandwidth between
local and remote memory.

ACM Reference Format:
Chenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song. 2019. OO-
VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future
NUMA-Based Multi-GPU Systems . In The 46th Annual International Sympo-
sium on Computer Architecture (ISCA ’19), June 22–26, 2019, Phoenix, AZ, USA.
ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3307650.3322247

1 INTRODUCTION
With the vast improvements in graphics technology, Virtual Real-
ity (VR) is becoming a potential popular product for major high-
tech companies such as Facebook [30], Google [16] and NVIDIA
[29]. Different from normal PC or mobile graphics applications, VR
promises a fully immersive experience to users by directly display-
ing images in front of users’ eyes. Due to the dramatic experience
revolution VR brings to users, the global VR market is expected
to grow exponentially and generate $30 billion annual revenue by
2022 [24, 34].

Despite the growing market penetration, achieving true immer-
sion for VR applications still faces severe performance challenges
[20]. First, the display image must have a high pixel density as well
as a broad field of views which requires a high display resolution.
Meanwhile, the high-resolution image must be delivered at an ex-
tremely short latency so that users can preserve the continuous
illusion of reality. However, the state-of-the-art graphics hardware
– the Graphics Processing Units (GPUs) in particular – cannot meet
these strict performance requirements [20]. Historically, GPUs gain
performance improvements through integrating more transistors
and scaling up the chip size, but these optimizations on single-GPU
system can barely satisfy VR users due to the limited performance
boost [33]. Multi-GPU system with much stronger computation ca-
pability is a promising candidate to provide sustainable and scalable
performance for VR applications [21, 33].

In recent years, the major GPU verdors combine multiple small
GPU models (e.g., GPMs) to build a future multi-GPU system under
a single programming model to provide scalable computing re-
sources. They employ high speed inter-GPU links such as NVLINK
[28] and AMD Crossfire[3] to achieve fast data transmit among

 
 
 
 
 
 
GPMs. The memory system and address mapping in this multi-GPU
system are designed as a Non-Uniform Memory Access (NUMA)
architecture to achieve 4x storage capacity over single-GPU sys-
tem. The NUMA-based multi-GPU system employs shared memory
space to avoid data duplication and synchronization overheads
across the distributed memories [5]. In this study, we target the fu-
ture multi-GPU system because it serves the VR applications more
energy-efficiently than distributed multi-GPU system that employs
separated memory space, and is becoming a good candidate for fu-
ture mobile VR applications. Since the entire system is viewed as a
single GPU under the single programming model, the VR rendering
workloads are sequentially launched and distributed to different
GPMs without specific scheduling. Applying this naive single pro-
gramming model greatly hurts the data locality among rendering
workloads and incurs huge inter-GPM memory accesses, which
significant constrain the performance of multi-GPU system for VR
applications due to the bandwidth asymmetry between the local
DRAM and the inter-GPM links. There have been many studies
[5, 21, 25, 43] to improve the performance of NUMA-based multi-
GPU system by minimizing the remote accesses. However, these
solutions are still based on single programming model without
considering the special data redundancy in VR rendering, hence,
they cannot efficiently solve the performance bottleneck for VR
applications.

Aiming to reduce the inter-GPM memory accesses, a straightfor-
ward method is employing parallel rendering frameworks [7, 13,
14, 19] to split the rendering tasks into multiple parallel sub-tasks
under specific software policy before assigning to the multi-GPU
system. Since these frameworks are originally designed for dis-
tributed multi-GPU system, a knowledge gap still exists on how
to leverage parallel rendering programming model to efficiently
execute VR applications in NUMA-based multi-GPU system. To
bridge this gap, we first investigate three different parallel rendering
frameworks (i.e. frame-level, tile-level and object-level). By con-
ducting comprehensive experiments on our VR featured simulator,
we find that the object-level rendering framework that distributes
the rendering object along with its required data per GPM can con-
vert some remote accesses to local memory accesses. However, this
object-level rendering still faces two major challenges in NUMA-
based multi-GPU system: (1) a large number of inter-GPM memory
accesses because it fails to capture the data locality between left
and right view of the same object as well as the data sharing among
different objects; (2) the serious workload unbalance among GPMs
due to the inefficient software-level distribution and composition
mechanisms.

To overcome these challenges, we propose object-oriented VR
rendering framework (OO-VR) that reduces the inter-GPM memory
traffic by exploiting the data locality among objects. Our OOVR
framework conducts the software and hardware co-optimizations
to provide a NUMA friendly solution for VR multi-view rendering.
First, we propose an object-oriented VR programming model that
provides a simple software interface for VR applications to exploit
the data sharing between the left and right views of the same ob-
ject. The proposed programming model also automatically groups
objects into batches based on their data sharing levels. Then, to
combat the limitation of software-level solutions on workload dis-
tribution and composition, we design a object aware runtime batch

Chenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

Figure 1: Rendering the VR world into left and right views.
(Frame is captured from The LAB[31])

distribution engine in hardware level to balance the rendering work-
loads among GPMs. We predict the execution time for each batch
so that we can pre-allocate the required data of each batch to the
local memory to hide long data copy latency. We further design the
distributed composition unit in hardware level to fully utilize the
rendering output units across all GPMs for best pixel throughput.
To summarize, the paper makes following contributions:

• We investigate the performance of future NUMA-based multi-
GPU systems for VR applications, and find that the inter-
GPM memory accesses are the major performance bottle-
neck.

• We conduct comprehensive characterizations on major paral-
lel rendering frameworks, and observe that the data locality
among rendering objects can help to significantly reduce the
inter-GPM memory accesses but the state-of-the-art frame-
works and multi-GPU systems fail to capture this interesting
feature.

• We propose a software and hardware co-designed Object-
Oriented VR (OO-VR) rendering framework that leverages
the data locality feature to convert the remote inter-GPM
memory accesses to local memory accesses.

• We further build a VR featured simulator to evaluate our pro-
posed design by rendering VR enabled real-world games with
different resolutions. The results show that OO-VR achieves
1.58x performance improvement and 76% inter-GPM mem-
ory traffic reduction over the state-of-the-art multi-GPU
system. With its nature of NUMA friendly, OO-VR exhibits
strong performance scalability and potentially benefits the
future larger multi-GPU scenarios with ever increasing asym-
metric bandwidth between local and remote memory.

2 BACKGROUND AND MOTIVATION
2.1 Multi-View Rendering in Virtual Reality
In contrast to other traditional graphics applications, the state-of-
the-art VR applications employ Head-Mounted-Display (HMD), or
VR helmet, to directly present visuals to users’ eyes. To display 3D
objects in VR, a pair of frames (i.e., stereoscopic frames) are gener-
ated for both left and right eyes by projecting the scene onto two

Left viewRight viewOO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

Figure 2: Multi-view VR rendering process with simultaneous multi-projection (SMP) enabled: (a) the output of each rendering
step; (b) The overview of the 4-step rendering process; and (c) GPU architecture for SMP-enabled VR rendering. Modern GPUs
employ unified shader model that processes all shaders in programmable Streaming Multiprocessors (SMs). They also feature
a new fixed function unit inside each Polymorph Engine (PME) to support SMP.

2D plate images. This process is referred as stereo rendering in
computer graphics. Figure 1 shows an example of such VR projec-
tion. The green and yellow boxes represent the rendering process
for left and right views, respectively, creating two display images
for the HMD. Stereo rendering requires two concurrent rendering
process for the two eyes’ views, resulting in doubled amount of
workload for the VR pipeline. Due to the observation that some
objects in the scene (e.g. the robot in Figure 1) are shared by two
eyes, mainstream graphics engines such as NVIDIA and UNITY
employ simultaneous multi-projection (SMP) to generate the left
and right frames simultaneously through single rendering process
[8, 9, 27, 35]. This significantly reduces workload redundancy and
achieves substantial performance gain.

Based on the conventional three-step rendering process (i.e., Ge-
ometry Process, Rasterization and Fragment Process) defined by
modern graphics application programming interface (API) [1, 2],
VR rendering inserts multi-projection process after the geometry
process and prior to the Rasterization, shown in Figure 2(a). Thus,
when SMP is enabled, VR rendering process is composed of four
steps, detailed in Figure 2(b). Basically, VR rendering begins from
reading the application-issued vertex from GPU memory. During
the geometry process 1 , the vertex shader calculates the 3D coordi-
nates of the vertex and assembles them into primitives (i.e. triangles
in Figure 2(a)-(1)). After that, the generated triangles pass through
the geometry-related shaders which perform clipping, face culling
and tessellation to generate extra triangles and remove non-visible
triangles. Then, the SMP step 2 is responsible for generating mul-
tiple projections of a single geometry stream. In other words, GPU
executes geometry process only once but produces two positions
for each triangle (Figure 2(a)-(2)). These triangles are then streamed

into the rasterization stage 3 to generate fragments (Figure 2(a)-
(3)), each of which is equivalent to a pixel in a 2D image. Finally,
the fragment process 4 generates pixels by calculating the corre-
sponding fragment attributes to determine their colors and texture
(Figure 2(a)-(4)). The output pixels will be written into the frame
buffer in GPU memory for displaying.

2.2 SMP Featured GPU Architectures
Traditionally, GPUs are designed as the special-purpose graph-
ics processors for performing modern rendering tasks. Figure 2(c)
shows a SMP supported GPU architecture which models the re-
cent NVIDIA Pascal GPUs[27]. It consists of several programmable
streaming multiprocessors (SMs) 1 , some fixed function units such
as the GigaThread Engine 2 , Raster Engine 3 , Polymorph En-
gine (PME) 4 , and Render Output Units (ROPs) 5 . Each SM 1 is
composed of a unified texture/L1 cache (TX/L1 $), several texture
units (TXU) and hundreds of shader cores that execute a variety
of graphics shaders (e.g., the functions in both geometry and frag-
ment process). The GigaThread Engine 2 distributes the rendering
workloads among PMEs if there are adequate computing resources.
The raster engine 3 is a hardware accelerator for rasterization pro-
cess. Each PME 3 conducts input assembler, vertex fetching, and
attribute setup. To support multi-view rendering, NVIDIA Pascal
architecture integrates an SMP engine into each PME. The SMP en-
gine is capable of processing geometry for two different viewports
which are the projection centers for the left and right views. In other
words, it duplicates the geometry process from left to right views
through changing the projection centers instead of executing the
geometry process twice. Finally, the Render Output Units (ROPs)
5 perform anti-aliasing, pixel compression and color output. As

(3) Rasterization                      (1) Geometry Process                      (2) Multi-Projection                      (4) Fragment Process                      Input AssemblerVertex ShaderTessellation ShaderGeometry ShaderSimultaneousMulti-ProjectionRasterizationAttribute SetupPixel ShaderColor Output(b) Multi-View Rendering PipelinePMERaster EngineSMPL2 CacheMemory ControllersOff-chip Memory(c) GPU Architecture That Supports Simultaneous Multi-Projection (a) Graphical IllustrationXBARROP…GigaThreadEnginePMESMPPMESMP…SMTXUInst. $……TX/L1 $SMTXUInst. $……TX/L1 $SMTXUInst. $……TX/L1 $…ROPROP123412345Table 1: Differences Between PC Gaming and VR

Display
Field of View (FoV)

Gaming PC
2D LCD panel
24-30" diagonal

Number of Pixel
Frame latency

2-4 Mpixels
16-33 ms

Stereo VR
Stereo HMD
120◦ horizontally
135◦ vertically
58.32x2 Mpixels
5-10 ms

Figure 3: The Overview of the multi-GPU architecture. Dis-
tributed rendering tasks for the same object causes signifi-
cant remote memory access and data duplication.

Figure 2(c) illustrates, all the SMs and ROPs share a L2 cache and
read/write data to the off-chip memory through the memory con-
trollers, each of which is paired to one memory channel. Prior to
rendering, GPU memory contents such as framebuffer and texture
data are pre-allocated in GPU’s off-chip memory. Programmers can
manually manage the memory allocation using the graphics APIs
such as OpenGL and Direct3D[1, 2].

Although recent generations of GPUs have shown capability to
deliver good gaming experiences and also gradually evolved to sup-
port SMP, it is still difficult for them to satisfy the extremely high
demands on rendering throughput from immersive VR applications.
The human vision system has both wide field of view (FoV) and
incredibly high resolution when perceiving the surrounding world;
the requirement for enabling an immersive VR experience is much
more stringent than that for PC gaming. Table 1 lists the major
differences between PC gaming and stereo VR [20]. As it demon-
strates, stereo VR requires GPU to deliver 116 (58.32×2) Mpixels
within 5 ms. Missing the rendering deadline will cause frame drop
which significantly damages VR quality. Although the VR vendors
today employ frame re-projection technologies such as Asynchro-
nous Time Warp (ATW)[15, 36] to artificially fill in dropped frames,
they cannot fundamentally solve the problem of rendering dead-
line missing due to little consideration on users’ perception and
interaction. Thus, improving the overall rendering efficiency is still
the highest design priority for modern VR-oriented GPUs [6].

2.3 NUMA-Based Multi-GPU System and Its

Performance Bottleneck

In recent years, major GPU vendors such as NVIDIA have proposed
to integrate multiple easy-to-manufacture GPU chips at package
level (i.e., multi-chip design)[5] or at system level[25, 43] using high

Chenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

bandwidth interconnection technologies such as Grand-Reference
Signaling (GRS)[32] or NVLinks[28], in order to address future chip
density saturation issue. Figure 3 shows the overview of the multi-
GPU architecture which consists of four GPU models (i.e., GPMs).
In terms of compute capability, each GPM is configured to resem-
ble the latest NVIDIA GPU architecture (e.g., Figure 2(c)). Inside
each GPM, SMs are connected to the GPM local memory hierarchy
including a local memory-side L2 cache and off-chip DRAM, via an
XBAR. In the overall multi-chip design (MCM-GPU), XBARs are
interconnected through high speed links such as NVLinks to sup-
port the communication among different GPMs. This multi-GPU
system generally acts as a large single GPU; its memory system and
address mapping are designed as a Non-Uniform Memory Access
(NUMA) architecture. This design also reduces the programming
complexity (e.g., unified programming model similar to CUDA) for
GPU developers.

Future Multi-GPU System Bottleneck for VR Workloads.
As previous works [5, 25] have indicated, data movement among
GPMs will become the major obstacle for the continued perfor-
mance scaling in these future NUMA-based multi-GPU systems.
This situation is further exacerbated when executing VR applica-
tions caused by the large data sharing among GPMs. Due to the
nature of view redundancy in VR applications, the left and right
views may include the same object (e.g., the rabbit in Figure 3)
which require the same texture data. However, to effectively utilize
the computing resources from all the GPMs in such multi-GPM
platforms, the rendering tasks for left and right views will be dis-
tributed to different groups or islands of GPMs in a more balanced
fashion; each view will then be further broken into smaller pieces
and distributed to the individual GPMs of that group. This naive
strategy could greatly hurt data locality in the SMP model. For
example, if the basic texture data used to describe the rabbit in
Figure 3 is stored in the local memory of GPM_0, other GPMs need
to issue remote memory accesses to acquire this data. Due to the
asymmetrical bandwidth between the local DRAM (e.g., 1TB/s) and
inter-GPM NVLink (e.g., 64GB/s), the remote memory access will
likely become one of the major performance bottlenecks in such
multi-GPU system design. More sophisticated parallel rendering
frameworks such as OpenGL Multipipe SDK [7], Chromium [19]
and Equalizer[13, 14], are designed for distributed environment
where they separate memory space and the memory data need
to be duplicated in each memory which greatly limits the storage
capacity on our NUMA-based multi-GPU systems. Thus, employ-
ing them on our architecture requires further investigation and
characterization. We will show this study in Section 4.

Figure 4 presents the performance of a 4-GPM multi-GPU sys-
tem as the bandwidth of inter-GPM links is decreased from 1TB/s
to 32GB/s (refer to Section 3 for experimental methodology). We
can observe that the rendering performance is significantly lim-
ited by the bandwidth. On average, applying 128GB/s, 64GB/s and
32GB/s inter-GPM bandwidth results in 22%, 42% and 65% perfor-
mance degradation compared to the baseline 1TB/s bandwidth,
respectively. Although improving the inter-GPM bandwidth is a
straightforward method to tackle the problem, it has proven diffi-
cult to achieve due to additional silicon cost and power overhead
[5].

DRAMGPM 0SMs  +  L1$XBARL2$DRAMGPM 3SMs  +  L1$XBARL2$DRAMGPM 1SMs  +  L1$XBARL2$DRAMGPM 2SMs  +  L1$XBARL2$64GB/s NVLink1 TB/sLTLBRTRBOO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

Figure 5: The original rendering frame (left) and results with
SMP enabled (right).

Table 3: BENCHMARKS

Figure 4: Normalized performance sensitivity to inter-GPM
link bandwidth for a 4-GPM Multi-GPU system.

Abbr. Names
Doom 3
DM3

Library
OpenGL[2]

HL2

Half-Life 2

DirectX[1]

Resolution
1600x1200
1280x1024
640x480
1600x1200
1280x1024
640x480
1280x1024

#Draw
191

328

1267

DirectX

NFS

UT3

WE

Need For
Speed
Unreal
Tournament 3
Wolfenstein

DirectX

1280x1024

876

DirectX

640x480

1697

Table 2: BASELINE CONFIGURATION

GPU frequency
Number of GPMs
Number of SMs
SM configuration

Texture filtering configuration
Raster Engine
Number of ROP
L2 cache
Inter-GPU interconnect

Local DRAM bandwidth

1GHz
4
32, 8 per GPMs
64 shader core per SM
128KB Unified L1 cache
4 texture unit
16x Anisotropic filtering
16x16 tiled rasterization
32, 8 per GPMs
4MB in total, 16-ways
64GB/s NVLink
uni-directional
1TB/s

This motivates us to provide software-hardware co-design strate-
gies to enable "true" immersive VR experience for future users via
significantly reducing the inter-GPM traffic and alleviating the
performance bottleneck of executing VR workloads on future multi-
GPU platforms. We believe this is the first attempt to co-design at
system architecture level for eventually realizing future planet-scale
VR.

3 EXPERIMENTAL METHODOLOGY
We investigate the performance impact of multi-GPU system for vir-
tual reality by extending ATTILA-sim [10], a cycle-level rasterization-
based GPU simulator which covers a wide spectrum of graphics
features on modern GPUs. The model of ATTILA-sim is designed
upon boxes (a module of functional pipeline) and signals (simulat-
ing the interconnect of different components). Because the current
ATTILA-sim models an AMD TeraScale2 architecture [17], it is
difficult to configure it using the same amount of SMs as NVIDIA
Pascal-like architectures [27]. To fairly evaluate the design impact,
we accordingly scale down other parameters such as the number
of ROPs and L2 cache. Similar strategies have been used to study
modern graphics architectures in previous works[40–42]. The GPM
memory system consists of two level cache hierarchy and a local
DRAM. The L1 cache is private to each SM while the L2 cache are
shared by all the SMs. Table 2 shows the simulation parameters
applied in our baseline multi-GPU system.

In order to support multi-view VR rendering, we implement
the SMP engine in ATTILA-sim based on the state-of-the-art SMP
technology [8, 9] which re-projects the triangles in left view to right

using updated viewport. Figure 5 shows the rendering example of
Half-Life 2 after enabling SMP in ATTILA-sim. Our SMP engine
first gathers the X coordinate of the display frame which is from -W
to +W, where W is a coordinate offset parameter. Then, it duplicates
each triangle generated from the geometry process. After that, the
SMP engine shifts the viewport of the rendering object by half of
W, left or right depending on the eye. The SMP engine can also
re-project the triangle based on user-defined viewports for left and
right views. Finally, we modify the triangle clipping to prevent the
spill over into the opposite eye. We validated the implementation of
the SMP engine in ATTILA-sim by comparing the triangle number,
fragment number and performance improvement with that from
executing VR benchmarks on the state-of-the-art GPUs (e.g. Sponza
and San Mangle in NVIDIA VRWork [29]) on NVIDIA GTX 1080 Ti).
Specifically, we observe that the added SMP rendering on ATTILA-
sim can provide a 27% speed up over the sequential rendering on
two views.

We also model the inter-GPU interconnect as high bandwidth
point-to-point NVLinks with 64GB/s bandwidth (one direction).
We assume each GPM has 6 ports and each pair of ports is used
to connect two GPMs, indicating that the intercommunication be-
tween two GPMs will not be interfered by other GPMs. Based on
the sensitivity study shown in Figure 4, we configure the inter-GPM
link bandwidth as 64GB/s bandwidth. Following the common GPU
design, each ROP in our simulation outputs 4 pixels per cycle to
the framebuffer. To further alleviate the remote memory access
latency on the NUMA-based baseline architecture, we employ the
state-of-the-art First-Tough (FT) page placement policy and remote
cache scheme [5] to create a fair baseline evaluation environment.
Table 3 lists the set of graphics benchmarks employed to evaluate
our design. This set includes five well-known 3D games, covering
different rendering libraries and 3D engines. We also list the original
rendering resolution and the number of draw commands for these
benchmarks. Two benchmarks (Doom3 and Half-Life 2) from the ta-
ble are rendered with a range of resolutions (1600×1200, 1280×1024,

00.20.40.60.81Performance Slowdown1TB/s256GB/s128GB/s64GB/s32GB/sSMP+W-W0Original Rendering Result Left ViewRight View+3/2W-3/2W00Chenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

(a) AFR

(b) Tile-level SFR (V)

(c) Tile-level SFR (H)

(d) Object-level SFR

Figure 6: Three types of parallel rendering schemes for parallel VR applied on future NUMA-based Multi-GPU systems.

Figure 7: Normalized performance improvement (left) and
single frame latency (right) across different benchmarks.

640×480) while for other games we adopt 1280x1024 resolution if it
is available and supported by the simulator. In order to feature these
PC games as VR applications, we modify the ATTILA Common
Driver Layer (ACDL) to enable the multi-view rendering. In our
experiments, we let all the workloads run to completion to generate
the accurate frames on the simulator and gather the average frame
latency for each game.

4 CHARACTERIZING PARALLEL

RENDERING SCHEMES ON FUTURE
NUMA-BASED MULTI-GPU SYSTEMS

Aiming to reduce the NUMA-induced bottlenecks, a straightfor-
ward method is to employ parallel rendering schemes in VR appli-
cations to distribute a domain of graphics workloads on a targeted
computing resource. While some parallel rendering frameworks
such as Equalizer [13, 14] and OpenGL Multiple SDK [7] have been
used in many cluster-based PC games, the NUMA-based multi-GPU
systems face some different challenges when performing parallel
rendering. In this section, we perform a detailed analysis using three
state-of-the-art parallel rendering schemes (including frame-level,
tile-level and object-level parallel rendering) for VR application
running on such future NUMA-based multi-GPU architectures, to
further understand the design challenges.

4.1 Alternate Frame Rendering (AFR)
Alternate Frame Rendering (AFR), also known as frame-level par-
allel rendering, executes one rendering process on each GPU in
a multi-GPU environment. As Figure 6a demonstrates, AFR dis-
tributes a sequence of rendering frames along with the required
data across different GPMs. AFR is often considered to be a better
fit for distributed memory environment since the separate memory
spaces make the concurrent rendering of different frames easier

to implement[14]. To separate our NUMA memory system into
unique memory spaces, we leverage the software-level segmented
memory allocation to reserve distributed memory segments for
each frame. We also employ a simple task scheduler to map the
rendering workloads of a frame to a specific GPM. The benefit of
this design is to eliminate the inter-GPM commutation.

Figure 7 shows the performance improvement and single frame
latency affected by AFR scheme. The results are normalized to
the baseline NUMA-based multi-GPU setup (with 64GB/s NVLink)
where the entire system is viewed as a single GPU under the pro-
gramming model and rendering workloads are directly launched
to this system without specific parallel rendering scheduling. On
average, AFR improves the performance (i.e., overall frame rate)
by 1.67X comparing to the baseline setup. AFR not only eliminates
the performance degradation of low bandwidth inter-GPU links,
but also increases the rendering throughput by leveraging the SMP
feature of the GPM. However, Figure 7(right) also suggests that
AFR increases the single frame latency by 59% as a frame is pro-
cessed by only one GPU. This increased single-frame latency may
cause significant motion anomalies, including judder, lagging and
sickness in VR system [42, 44] because it highly impacts whether
the corresponding display on VR head-gear device can be in sync
with the actual motion occurrence. Additionally, we observe that
AFR near-linearly increases the memory bandwidth and capac-
ity requirement according to the pre-allocate memory space for
each frame. This decreases the maximum system memory capacity
which directly limits the rendering resolution, texture details and
perceived quality for different VR applications.

4.2 Tile-Level Split Frame Rendering (SFR)
In contrast to AFR, split frame rendering (SFR) tends to reduce
the single-frame latency by splitting a single frame into smaller
rendering workloads and each GPM only responses to one group of
workloads. Figure 6b shows a tile-level SFR which splits the render-
ing frame into several pixel tiles in the screen space and distributes
these sets of pixels across different GPMs. This basic method is
widely used in cluster-based PC gaming because it requires very
low software effort[37]. To employ tile-level SFR, we simply lever-
age the sort-first algorithm to define the tile-window size before
the rendering tasks is processed in GPMs. Although this design can
effectively reduce single-frame latency, its vertical pixel stripping
[37] does distribute left and right views into different GPMs, ig-
noring the redundancy of the two views. Thus, to enable the SMP

3210GPM0GPM1GPM2GPM33210T0T1T2T3T0T1T2T3+GPM0GPM1GPM2GPM3+GPM0GPM1GPM2GPM3T0T1T2T3T0T1T2T3Object 4Object 0…Object 5Object 1…Object 6Object 2…Object 7Object 3…O0O1O2O3O4+GPM0GPM1GPM2GPM3X01234DM3-640DM3-1280DM3-1600HL2-640HL2-1280HL2-1600NFSUT3WEAvg.Overall Performance00.511.522.5DM3-640DM3-1280DM3-1600HL2-640HL2-1280HL2-1600NFSUT3WEAvg.Single Frame LatencyOO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

Figure 8: Normalized performance after enabling SFR across
different benchmarks.

Figure 10: The best-to-worst performance ratio among
GPMs in object-level SFR across different workloads.

Figure 8 and 9 illustrate the performance (i.e., the overall frame
rate) impact and inter-GPM memory traffic for different SFR sce-
narios. The results are normalized to the baseline setup. We have
the following observations:

(i) The tile-level SFR schemes only slightly improve the rendering
performance over the baseline case, e.g., on average 28% and 3%
for Tile-level (V) and Tile-level (H), respectively. This is because
although processing a small set of pixels via tile-level SFR can
improve the data locality within one GPU, the tile-level SFR schemes
increase the inter-GPM memory traffic by an average of 50% for
the vertical culling (V) and 44% for horizontal culling (H) due to
the object overlapping across the tiles. While the horizontal culling
(H) fails to capture the data sharing for large objects (e.g., the
bridge on the right side of Figure 6c), vertical culling (V) ignores the
redundancy between the left and right view. Since when applying
SMP-based VR rendering the GPMs do not render the left and right
views simultaneously, the large texture data have to be moved
frequently across the GPMs.

(ii) The object-level SFR outperforms tile-level SFR schemes and
achieves an average of 60%, 32% and 57% performance improvement
over the baseline, tile-level (V) and tile-level (H), respectively. The
speedups are mainly from the inter-GPMs traffic reduction, indi-
cated by Figure 9. By placing the required data in the local DRAM
for the rendered objects, Object-level SFR reduces approximately
40% of inter-GPMs traffic compared to the baseline. However, the
state-of-the-art object-level SFR can not fully address the NUMA-
induced performance bottlenecks for VR execution, because it still
executes the objects from the left and right views separately. In
other word, it ignores the multi-view redundancy in VR applications
which limits its rendering efficiency.

(iii) Additionally, we also observe that the object-level SFR is
challenged by low load balance and high composition overhead. Fig-
ure 10 shows the ratio between the best and the worst performance
among different GPMs under the Round-Robin object scheduling
policy. Since each object has a variety of graphical properties (e.g.,
the total amount of triangles, the level of details, the viewport win-
dow size, etc), the processing time is typically different for each
object. If one GPM is assigned more complex objects than the oth-
ers, it will take more time to complete the rendering tasks. Since
the overall performance of Multi-GPU system is determined by
the worst-case GPM processing, low load balance will significantly
degrade the overall performance. Meanwhile, the high composition
overhead (i.e., assembling all the rendering outputs from different

Figure 9: Total inter-GPM memory traffic across different
benchmarks.

under this tile-level SFR, an alternative is to employ a horizontal
culling method, shown in Figure 6c. It groups the left and right
views as a large pixel tile so that the rendering workloads in the
left view can by re-projected into the right via the SMP engine to
reduce redundancy geometry processing and improve data sharing.

4.3 Object-Level Split Frame Rendering
Distributing objects among processing units represents a specific
type of split rendering frame (SFR). Figure 6d shows an example
of object-level SFR which is often referred as sort-last rendering
[13]. In contrast to the traditional vertical and horizontal tile-level
SFR, the distribution under object-level SFR begins after the GPU
starts the rendering process. During object-level SFR, a root node
is selected (e.g., GPM0 in this example) to distribute the render-
ing objects to other working units (e.g., GPM1, GPM2 and GPM3).
Once a worker completes the assembled object, the output color
in its local DRAM is sent to the root node to composite the final
frame. In this study, we first profile the entire rendering process
to get the total number of rendering objects, and then issue them
to different GPMs in a round-robin fashion. Note that only one
object is executed in each GPM at a time for better data locality.
Although this object distribution can also occur during rendering
process (e.g., between rasterization and fragment processing [21]),
it typical requires to insert additional inter-GPM synchronization
which may cause increasing inter-GPM traffic and performance
degradation. Thus, we only distribute the objects at the beginning
of the rendering pipeline for our experiments.

00.511.522.5Normalized PerformanceTile-Level (V)Tile-Level (H)Object-Level00.20.40.60.811.21.41.61.82Normalized Inter-GPM TrafficTile-Level (V)Tile-Level (H)Object-Level00.511.522.5Best-to-Worst RatioGPMs into a frame) also contributes to the unbalanced execution
time. As we mentioned previously, only the root node is employed
to distribute and composite rendering tasks in the current object-
level SFR. In this case, extra workloads will be issued to the root
node while the ROP units of the other GPMs can not be fully utilized
during this color output stage, causing bad composition scalabil-
ity [7]. Therefore, we aim to propose software-hardware support
to efficiently handle these challenges facing the state-of-the-art
object-level SFR in a NUMA-based multi-GPU environment.

5 OBJECT-ORIENTED VR RENDERING

F-RAMEWORK

In order to address the performance issues of the object-level SFR
applied on future NUMA-based multi-GPU systems, we propose
the object-oriented VR rendering framework (OO-VR). The basic
design diagram is shown in Figure 11. It consists of several novel
components. First, we propose an object-oriented VR programming
model at the software layer to support multi-view rendering for
the object-level SFR. It also provides an interface to connect the
VR applications to the underlying multi-GPU platform. Second,
we propose an object-aware runtime distribution engine at the
hardware layer to balance the rendering workloads across GPMs. In
OO-VR, this distribution engine predicts the rendering time for each
object before it is distributed. It replaces the master-slave structure
among GPMs so that the actual distribution is only determined by
the rendering process. Finally, we design a distributed hardware
composition unit to utilize all the ROPs of the GPMs to assemble
and update the final frame output from the framebuffer. Due to
the NUMA feature, the framebuffer is distributed across all the
GPMs instead of only one DRAM partition, so that it can provide
4x output bandwidth of the baseline scenario. We detail each of
these components as follows.

5.1 Object-Oriented VR Programming Model
The Object-Oriented VR Programming Model extends the conven-
tional object-level SFR as we introduced in Section 4 and uses a
similar software structure as today’s Equalizer[13, 14] and OpenGL
Multipipe SDK (MPK)[7]. Figure 12 uses an simplified working
flow diagram to explain our programming model. In this study, we
propose two major components that drive our OO-VR program-
ming model: Object-Oriented Application (OO_Application) to drive
the VR multi-view rendering for each object, and Object-Oriented
Middleware (OO_Middleware) to reorder objects and regroup the
ones that share similar features as a large batch which acts as the
smallest scheduling units on the NUMA-based multi-GPU system.
The OO_Application provides a software interface (dark blue
box) for developers to merge the left and right views of same object
as a single rendering tasks. The OO_Application is designed by
extending the conventional object-level SFR. For each object, we
replace the original viewport which is set during the rendering
initialization with two new parameters – viewportL and viewportR,
each of which points to one view of the object. In order to en-
able rendering multi-views at the same time, we apply the built-in
openGL extension GL_OV R_multiview2 to set two viewports ID
for a single object. After that, each SMP engine integrated in a GPM

Chenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

Figure 11: Our proposed object-oriented VR rendering
framework (OO-VR).

automatically renders the left and right views to its own position-
ing using the same texture data. We also design an auto-model
to extend the conventional object-level SFR to enable multi-view
rendering through generating two fixed viewports for each object
via shifting the original viewport along the X coordinate. In this
case, only one rendering process needs to be setup for each object.
In constrast to the single-path stereo rendering enabled in modern
VR SDKs [13, 29], our OO_Application does not decompose the left
and right views during the rendering initialization so that it still
follows the execution model of the object-level SFR.

OO_Middleware is the software-hardware bridge to connect
the OO_Application and multi-GPU system. It is automatically
executed during the application initialization stage to issue a group
of the objects to the rendering queue of the multi-GPU system.
In the conventional object-level SFR, the objects are scheduled in
a master-slave fashion following the programmer-defined order.
However, different objects that may share some common texture
data are not rendered on the same GPM. As Figure 12 illustrates,
both "pillar1" and "pillar2" share the common "stone" texture. If they
are rendered on different GPMs, the "stone" texture may need to be
reallocated, increasing remote GPM access. In OO-VR, we leverage
OO_Middleware to group objects based on their texture sharing
level (TSL) to exploit the data locality across different objects.

To implement this, OO_Middeware first picks an object from the
head of the queue as the root. It then finds the next independent
object of the root and computes the TSL between the two using
Equation (1).

T S L =

T
(cid:213)

t

(Pr (t ) · Pn (t )) /

T
(cid:213)

t

Pr (t )

(1)

Where t is the shared texture data between the two objects, Pr (t)
and Pn (t) represent the percentages of t among all the required
textures for the root and the target object.

Object-Oriented Programming ModelRuntime Batch Distribution EngineRendering Time PredictorObject-Oriented VR ApplicationGPM0GPM1GPM2GPM3Distributed Hardware CompositionROP0ROP1ROP2ROP3FrameBufferObject-Oriented MiddlewareSoftware Layer:Hardware Layer:OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

Figure 12: Simplified working flow of an Object-Oriented ap-
plication, middleware and Multi-GPU rendering pipeline.

TSL represents how many texture data will be shared if we group
the target object with the root. If TSL is greater than 0.5, we group
them together as a batch and this batch then becomes the new
root which consists all textures from the previous iterator and the
target object. After this grouping, the OO_Middleware removes the
target object from the queue and continues to search for the next
object until the total number of triangles within the batch is higher
than 4096, or all the objects in the queue have been selected. The
triangle number limitation is used to prevent load imbalance from
an inflated batch.

After this step, this batch is marked as ready and issued to a GPM
in the system for rendering. Finally, the OO_Middleware repeats
this grouping process for all the objects in the frame until there is no
object in the queue. Note that for the objects that have dependency
on any of the objects in a batch, we directly merge them to the
batch and increase the triangle limitation so that they can follow
the programmer-defined rendering order.

5.2 Object-Aware Runtime Distribution Engine
After receiving the batches from the OO_Middleware, the Multi-
GPU system needs to distribute them across different GPMs for
multi-view rendering. For workload balancing, we propose an
object-aware runtime distribution engine at the hardware layer
instead of using the software-based distribution method based on
master-slave execution used in the conventional object-level SFR.
Comparing to the software-level solution which needs to split the
tasks before assigning to the multi-GPU system, the hardware en-
gine provides efficient runtime workload distribution by collecting
rendering information. Figure 13 illustrates the architecture design
of the proposed distribution engine. The new hardware architec-
ture is implemented as a micro-controller for the multi-GPU system
which responses for predicting the rendering time for each batch,
allocating an object to the earliest available GPM, and pre-allocating
memory data using the Pre-allocation Units (PAs) in each GPM.

Figure 13: The architecture design of the object-aware run-
time distribution engine.

Recall the discussion in Section 3 for our evaluation baseline,
we employ the first-touch memory mapping policy (FT) [5] to
allocate the required data in the local DRAM. Although FT can help
reduce inter-GPM memory access, it can also cause performance
degradation if the required data is not ready during the rendering
process. As a result, we consider to pre-allocate data before objects
are being distributed across GPMs. In this case, OO-VR needs to be
aware of the runtime information of each GPM to determine which
GPM is likely to become idle the earliest.

In order to obtain this information, we need to predict approxi-
mately how long the current batch will be completed. Equation (2)
shows a basic way to estimate the rendering time of the current
task X , introduced by [39]:

t (X ) = RT (дx , cx , HW , ST )
Where дx , cx is the geometry and texture property of the object X ,
HW is the hardware information, and ST is the current rendering
step (i.e., geometry process, multi-view projection, rasterazition or
fragment process) of the object X .

(2)

While a complex equation can increase the estimation accuracy,
it also requires more comprehensive data and increases hardware
design complexity and computing overhead. Because the objective
of our prediction model is to identify the earliest available GPM
instead of accurately predicting each batch’s rendering time, we
propose a simple linear memorization-based model to estimate the
rendering time as Equation (3):

t (X ) = c0 · #trianдlex = c1 · #tvx + c2 · #pixelx
Where #trianдlex , #tvx and #pixelx represent the triangle counts,
the number of transformed vertexes and the number of rendered
pixels of the current batch, respectively. c0, c1 and c2 represent the
triangle, vertex and pixel rate of the GPM.

(3)

After building this estimation model, we split the prediction
process into two phases: total rendering time estimation and elapsed
rendering time prediction. We setup two counters to record the
total rendering time and the elapsed rendering time for each GPM.
First, we leverage #trianдlex (which can be directly acquired from

OO_ApplicationInitializeOO_MiddlewareBegin FrameObject0 “pillar1”ViewportL*ViewportRTexture[stone]…Object1 “flag”ViewportL*ViewportRTexture[cloth]…ObjectN“pillar2”ViewportL*ViewportRTexture[stone]…End FrameExitGPU Setup Batch1Object0“pillar1”ObjectN“pillar2”…Find NextFull? Or End? Group StartEnd DrawAll?……Multi-GPUsConfig.Object Distribution EngineMulti-View RenderingEnd?Frame ReadyNoYesYesYesNoNo#objects#batchesNext BatchRuntime Batch Distribution EngineGPM0GPM1GPM2GPM3RenderingTimePredictorXObject-Oriented Programming Model Runtime InformationBatch QueueCountersObject PropertiesPA UnitPA UnitPA UnitPA UnitChenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

a single GPM limits the pixel rate which impacts the overall ren-
dering performance. Since the NUMA-based multi-GPU system
can be considered as a large single GPU, we consider to distribute
the composition tasks across all the GPMs which is currently not
supported due to the lack of relevant communication mechanism
and hardware.

For example, shown in Figure 14, we first split the entire FB into 4
partitions using the screen-space coordinate of the final frame. Here
we employ the same memory mapping policy as the vertical Tile-
level SFR (V). Based on this, we propose the distributed hardware
composition unit (DHC) to determine which part of FrameBuffer is
used to store what color outputs of the final frame. This design is
based on the observation that the color outputs of the final frames
only incur a small number of memory access compared to the main
rendering phase so that the small amount of remote communication
for this phase will not become a performance bottleneck for NUMA-
based multi-GPU systems. This is also why vertical culling shown
in Figure 14 can perform well as the last stage of VR rendering (i.e.,
after the object-aware runtime distribution for the main rendering
phase) since the inter-GPM bandwidth can be effectively utilized
by the distributed hardware composition.

5.4 Overhead Analysis
The major hardware components added into the existing multi-
GPU system is the object-aware runtime distribution engine, which
consists of a rendering time predictor, GPM counters and a batch
queue. For the baseline Multi-GPU architecture that we modeled
for this work (Table 2), we allocate 64 bits for each counter and
16 bits for each batch ID to store the predicted rendering time.
Additionally, to predict the total and elapsed rendering time, twelve
32-bits registers are used to track the triangle counts, the number
of transformed vertexes and the number of the rendered pixels for
the current batches. In total, we only require 960 bits for storage
and several small logic units. We use McPAT[22] to evaluate the
area and energy overhead of the added storage and logic units for
the distribution engine. The area overhead is 0.59 mm2 under 24nm
technology which is 0.18% to modern GPUs (e.g., GTX1080). The
power overhead is 0.3W which is 0.16% of TDP to GTX1080.

6 EVALUATION
We model the object-oriented VR rendering framework (OO-VR) by
extending AITTILA-sim [10]. To get the object graphical properties
(e.g., viewports, number of triangles and texture data), we profile the
rendering-traces from our real-game benchmarks as shown in Table
3. Then in ATTILA-sim, we implement the OOVR programming
model in its GPUDriver, and the object distribution engine in its
command processor, and the distributed hardware composition
during the color writing procedure. To evaluate the effectiveness
of our proposed OO-VR design, we compare it with several design
scenarios: (i) Baseline - the baseline multi-GPU system with single
programming model (Section 2); (ii) 1TB/s-BW - the baseline system
with 1 TB/s inter-GPU link bandwidth; (iii) Object-level - the Object-
level SFR which distributes objects among GPMs (Section 4); (iv)
Frame-level - the AFR which renders entire frame within each GPM;
and (v) OO_APP - the proposed object-oriented programming model
(Section 3). We provide results and detailed analysis of our proposed

Figure 14: The distributed FrameBuffer for hardware com-
position.

the OO_Application) to predict the total rendering time. During
rendering, the distribution engine tracks #tvx and #pixelx from
GPMs to calculate the elapsed rendering time. If the #tvx or #pixelx
increases by 1, the elapsed counter increases by c1 or c2, respectively.
At the end, by comparing the distance between the two counters
from each GPM, we can predict which GPM will become available
first.

At the beginning of the rendering, the distribution engine uses
the first 8 batches to initialize c0, c1 and c2. The first 8 batches will be
distributed across GPMs under the Round-Robin object scheduling
policy and baseline FT memory mapping scheme is also applied to
allocate the rendering data. After GPMs complete this round of 8
batches, the total rendering time will be sent back to the distribution
engine to calculate c0, c1 and c2. Then, starting from the 9th batch,
the rendering time predictor is enabled to find the earliest available
GPM. After that, the PA Unit pre-allocates the required data to
the selected GPMs, and the rendering time predictor updates the
predicted total rendering time by increasing the triangle counts.
Note that we limit the maximum size of the batch queue to 4 objects
to reduce the memory space requirement. Multiple batches could
be distributed onto one GPM at the same time. In this case, a PA
Unit sequentially fetches the data based on the order of the batch
ID.

We further observe that even though distribution engine can
effectively balance the rendering tasks, it is possible that some
large objects may still become the performance bottleneck if all
the other batches have been completed. To fully utilize the com-
puting and memory resources of these idle GPMs, we employ a
simple fine-grained task mapping mechanism to fairly distribute
the rest of the processing units (e.g. triangles in geometry process
and fragments in fragment process) to idle GPMs based on their
IDs. Meanwhile, the PA units duplicate the required data to the
corresponding unused DRAM to eliminate inter-GPMs access for
these left-over fine-grained tasks.

5.3 Distributed Hardware Composition Unit
In the conventional object-level SFR, the entire FrameBuffer (FB)
is mapped in the DRAM of the master node, and all the rendering
outputs will then be transmitted to the master node for the final
composition. Although the color outputs can be executed asyn-
chronously with the shader process, a small amount of ROPs in

GPM0DHCFBGPM1DHCFBGPM3DHCFBGPM2DHCFBFB0FB1FB2FB3OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

OO-VR on inter-GPM memory traffic. Both Baseline and 1TB/s-
BW have the same inter-GPM memory traffic, and Frame-Level is
processing each frame in one GPM and has near-zero inter-GPM
traffic. Moreover, the memory traffic reduction is mainly cause by
our software-level design, the inter-GPM traffic is the same under
the impact of OO_APP and OO-VR. Therefore, Fig.16 only shows
the results for Baseline, Object-Level and OO-VR, and we mainly in-
vestigate these three techniques in the following subsections. From
the figure, we observe OO-VR can save 76% and 36% inter-GPM
memory accesses comparing to the Baseline and Object-level SFR,
respectively. This is because OO-VR allocates the required rendering
data to the local DRAM of GPMs. The majority inter-GPM memory
accesses are contributed by the distributed hardware composition,
command transmit and Z-test during fragment process. We ob-
serve that the delay caused by these inter-GPM memory accesses
can be fully hidden by executing thousands of threads simultane-
ously in numerous shader cores. In addition, the data transfer via
the inter-GPM links also leads to higher power dissipation (e.g.
10pj/bit for board or 250pj/bit for nodes based on different inte-
gration technologies[5]). By reducing inter-GPM memory traffic,
OO-VR also achieves significant energy and cost saving.

6.3 Sensitivity To Inter-GPM Link Bandwidth
Inter-GPU link bandwidth is one of the most important factors in
multi-GPU systems. Previous works [5, 25] have shown that increas-
ing the bandwidth of inter-processor link is difficult and requires
high fabric cost. To understand how inter-GPM link bandwidth
impacts the design choice, we examine the performance gain of
OO-VR under a variety of choices on link bandwidth. Fig.17 shows
the speedup under different link bandwidth when applying Base-
line, Object-level SFR and our proposed OO-VR. In this figure, we
normalize the performance to the Baseline with 64GB/s inter-GPM
link. We observe that the inter-GPU link bandwidth highly affects
the Baseline and Object-level SFR design scenarios. This is because
these two designs cannot capture the data locality within the GPM
to minimize the inter-GPU memory accesses during rendering. The
large amount of shared data across GPMs significantly stalls the
rendering performance. In the contrast, OO-VR fairly distributes
the rendering workloads into different GPMs and convert numer-
ous remote data to local data. By doing this, it fully utilizes the
high-speed local memory bandwidth and is insensitive to the band-
width of inter-GPM link even the inter-GPM memory accesses are
not entirely eliminated. As the local memory bandwidth scales in
future GPU design (e.g. High-Bandwidth Memory (HBM)[11]), the
performance of the future multi-GPU scenario is more likely to be
constrained by inter-GPU memory. In this case, we consider the
OO-VR can potentially benefit the future multi-GPU scenario by
reducing inter-GPM memory traffic.

6.4 Scalability of OO-VR
Fig.18 shows the average speedup of the Baseline, Object-level
SFR and OO-VR as the number of GPMs increases. The results are
normalized to single-GPU system. As the figure shows, the Baseline
and Object-level SFR suffer limited performance scalability due to
the NUMA bottleneck. With 8 GPMs, the Baseline and Object-level
SFR only improve the overall performance by 2.08x and 3.47x on

Figure 15: Normalized speedup of overall VR rendering for
single frame under different design scenarios.

design on performance, inter-GPU memory traffic, sensitivity study
for inter-GPM link bandwidth and the performance scalability over
the number of GPMs.

6.1 Effectiveness On Performance
Fig.15 shows the performance results with respect to single frame
latency under the five design scenarios. We gather the entire ren-
dering cycles from the beginning to the end for each frame and
normalized the performance speedup to baseline case. We show
the performance speedup for single frame because it is critical to
avoid motion sickness for VR. From the figure, we have several
observations.

First, without hardware modifications, the OO_APP improves
the performance about 99%, 39% an 28% on average comparing to
the Baseline, Object-level SFR and 1TB/s-BW, respectively. It com-
bines the two views of the same object and enable the multi-view
rendering to share the texture data. In addition, by grouping ob-
jects into large batches, it further increases the data locality within
one GPM to reduce the inter-GPM memory traffic. However, it
still suffers serious workload unbalance. For instance, object-level
SFR slightly outperforms OO_APP when executing DM3-1280 and
DM3-1600. This is because some batches within these two bench-
marks require much longer rendering time than other batches, the
software scheduling policy alone in OO_APP can not balance the
execution time across GPMs without runtime information. Second,
we observe that on average, OO-VR outperforms Baseline, Object-
level SFR and OO_APP by 1.58x, 99% and 59%, respectively. With
the software and hardware co-design, OO-VR distributes batches
based on the predicted rendering time and provides better work-
load balance than OO_APP. It also increases the pixel rate by fully
utilizing the ROPs of all GPMs.

We also observe that OO-VR could achieve similar performance
as Frame-level parallelism which is considered to provide ideal
performance on overall rendering cycles for all frames (as shown
in Fig.7(left)). However, in terms of the single frame latency, Frame-
level parallelism suffers 40% slowdown while OO-VR could signifi-
cantly improve the performance.

6.2 Effectiveness On Inter-GPU Memory Traffic
Reducing inter-GPM memory traffic is another important criteria
to justify the effectiveness of OO-VR. Fig.16 shows the impact of

0.20.611.41.82.22.633.43.8Normalized SpeedupObject-LevelFrame-Level1TB/s-BWOO_APPOOVRChenhao Xie, Xin Fu, Mingsong Chen, and Shuaiwen Leon Song

Figure 16: Normalized inter-GPM memory
traffic under different design scenarios.

Figure 17: Normalized speedup of
the proposed OO-VR, object-level
SFR and baseline under different
inter-GPM link bandwidth.

Figure 18: Normalized speedup of
the proposed OO-VR, object-level
SFR and baseline over single GPU
under different number of GPMs.

average over the single GPU processing. On the other hand, the
OO-VR provides scalable performance improvement by distributing
independent rendering tasks to each GPM. Hence, with 4 and 8
GPMs, it achieves 3.64x and 6.27x speedup over the single GPU
processing, respectively.

the multi-GPU system to enhance the pixel throughput. Meanwhile,
many architecture approaches [4, 40, 41] have been proposed to
reduce the memory traffic during rendering. Our work focuses on
multi-view VR rendering in multi-GPU system which is orthogonal
to these architecture technologies.

7 RELATED WORK
Architecture Approach For NUMA Based Multi-GPU System.
There have been many works [5, 21, 25] improving the performance
for NUMA based multi-GPU system. Some of them[5, 25, 43] in-
troduce architectural optimizations to reduce the inter-GPM mem-
ory traffic for GPGPU application while Kim et al.[21] redistribute
primitives to each GPM to improve the scalability on performance.
However, none of them discusses the data sharing feature of VR
application. Our approach exploits the data locality during VR
rendering to reduce the inter-GPM memory traffic and achieves
scalable performance for multi-GPU system.

Parallel Rendering. Currently, PC clusters are broadly used
to render high-interactive graphics applications. To drive the clus-
ter, software-level parallel rendering frameworks such as OpenGL
Multipipe SDK [7], Chromium [19], Equalizer[13, 14] have been
developed. They provides the application programming interface
(API) to develop parallel graphics applications for a wide rand of
platforms. Their works tend to split the rendering tasks during ap-
plication development under different configurations. In our work,
we propose a software and hardware co-designed object-oriented
VR rendering framework for the parallel rendering in NUMA based
multi-GPU system.

Performance Improvement For Rendering. In order to bal-
ance the rendering workloads among multiple GPUs, some studies
[12, 18, 26] propose a software-level solution that employs CPU to
predict the execution time before rendering to adaptively determine
the workload size. However, such software-level method requires a
long time to acquire the hardware runtime information from GPUs
which causes performance overhead. Our hardware-level scheduler
could quickly collect the runtime information and conduct the real-
time object distribution to balance the workloads. There are also
some works [23, 38] designing NUMA aware algorithms for fast
image composition. Instead of implementing a composition kernel
in software, we resort to a hardware-level solution that leverages
hardware components to distribute the composition tasks across

8 CONCLUSIONS
In modern NUMA-based multi-GPU system, the low bandwidth
of inter-GPM links significantly limits the performance due to the
intensive remote data accesses during the multi-view VR rendering.
In this paper, we propose object-oriented VR rendering framework
(OO-VR) that converts the remote inter-GPM memory accesses to
local memory accesses by exploiting the data locality among objects.
First, we characterize the impact of several parallel rendering frame-
works on performance and memory traffic in the NUMA-based
multi-GPU systems. We observe the high data sharing among some
rendering objects but the state-of-the-art rendering framework and
multi-GPU system cannot capture this interesting feature to re-
duce the inter-GPM traffic. Then, we propose an object-oriented VR
programming model to combine the two views of the same object
and group objects into large batches based on their texture sharing
levels. Finally, we design a object aware runtime batch distribution
engine and distributed hardware composition unit to achieve the
balanced workloads among GPUs and further improve the perfor-
mance of VR rendering. We evaluate the proposed design using VR
featured simulator. The results show that OO-VR improves the over-
all performance by 1.58x on average and saves inter-GPM memory
traffic by 76% over the baseline case. In addition, our sensitivity
study proves that OO-VR can potentially benefit the future larger
multi-GPU scenario with ever increasing asymmetric bandwidth
between local and remote memory.

ACKNOWLEDGMENT
This research is supported by U.S. DOE Office of Science, Office
of Advanced Scientific Computing Research, under the CENATE
project (award No. 466150), The Pacific Northwest National Lab-
oratory is operated by Battelle for the U.S. Department of Energy
under contract DEAC05-76RL01830. This research is partially sup-
ported by National Science Foundation grants CCF-1619243, CCF-
1537085(CAREER), CCF-1537062.

00.20.40.60.81Normalized Inter-GPM TrafficBaselineObject-LevelOOVR00.511.522.5332GB/s64GB/s128GB/s256GB/sSpeedupInter-GPU BandwidthBaselineObject-levelOOVR012345671248SpeedupNumber of GPUsBaselineObject-levelOOVROO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems

[25] Ugljesa Milic, Oreste Villa, Evgeny Bolotin, Akhil Arunkumar, Eiman Ebrahimi,
Aamer Jaleel, Alex Ramirez, and David Nellans. 2017. Beyond the socket: NUMA-
aware GPUs. In Proceedings of the 50th Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO). ACM, 123–135.

[26] Brendan Moloney, Daniel Weiskopf, Torsten Möller, and Magnus Strengert. 2007.
Scalable Sort-first Parallel Direct Volume Rendering with Dynamic Load Balanc-
ing. (2007), 45–52. https://doi.org/10.2312/EGPGV/EGPGV07/045-052

[27] Nvidia. 2016. GeForce GTX 1080 Whitepaper. https://international.download.
nvidia.com/geforce-com/international/pdfs/GeForce_GTX_1080_Whitepaper_
FINAL.pdf

[28] Nvidia. 2016. NVIDIA VR Sli. https://developer.nvidia.com/vrworks/graphics/

vrsli

[29] Nvidia. 2018. Nvidia VRworks. https://developer.nvidia.com/vrworks
[30] Oculus. 2018. Oculus VR Products. https://www.oculus.com/
[31] Alanah Pearce and Mitch Dyer. 2016.

THE LAB: VALVE’s free and
https://www.ign.com/articles/2016/03/24/

fun VR mini-game collection.
the-lab-valves-free-and-fun-vr-mini-game-collection

[32] John W Poulton, William J Dally, Xi Chen, John G Eyles, Stephen G Tell, John M
Wilson, and C Thomas Gray. 2013. A 0.54 pJ/b 20 Gb/s Ground-Referenced
Single-Ended Short-Reach Serial Link in 28 nm CMOS for Advanced Packaging
Applications. (2013).

[33] Bruno Raffin, Luciano Soares, Tao Ni, Robert Ball, Greg S Schmidt, Mark A
Livingston, Oliver G Staadt, and Richard May. 2006. Pc clusters for virtual reality.
In Virtual Reality Conference, 2006. IEEE, 215–222.

[34] Grand View Research. 2017. Virtual Reality (VR) Market Analysis By Device,
By Technology, By Component, By Application (Aerospace Defense, Commer-
cial, Consumer Electronics, Industrial, Medical), By Region, And Segment Fore-
casts, 2018 - 2025.
https://www.grandviewresearch.com/industry-analysis/
virtual-reality-vr-market

[35] Unity. 2018. Unity User Manual-Single Pass Stereo Rendering. https://docs.

unity3d.com/Manual/SinglePassStereoRendering.html

[36] JMP Van Waveren. 2016. The asynchronous time warp for virtual reality on
consumer hardware. In Proceedings of the 22nd ACM Conference on Virtual Reality
Software and Technology. ACM, 37–46.

[37] Alex Vlachos. 2016. Advanced VR rendering performance. In Game Developer

Conference.

[38] Pan Wang, Zhiquan Cheng, Ralph Martin, Huahai Liu, Xun Cai, and Sikun Li.
2013. NUMA-aware image compositing on multi-GPU platform. The Visual
Computer 29, 6-8 (2013), 639–649.

[39] Michael Wimmer and Peter Wonka. 2003. Rendering Time Estimation for Real-
time Rendering. In Proceedings of the 14th Eurographics Workshop on Rendering
(EGRW ’03). Eurographics Association, Aire-la-Ville, Switzerland, Switzerland,
118–129. http://dl.acm.org/citation.cfm?id=882404.882422

[40] Chenhao Xie, Xin Fu, and Shuaiwen Song. 2018. Perception-Oriented 3D Ren-
dering Approximation for Modern Graphics Processors. In IEEE International
Symposium on High Performance Computer Architecture, HPCA 2018, Vienna,
Austria, February 24-28, 2018. 362–374.

[41] Chenhao Xie, Shuaiwen Leon Song, Jing Wang, Weigong Zhang, and Xin Fu.
2017. Processing-in-Memory Enabled Graphics Processors for 3D Rendering. In
2017 IEEE International Symposium on High Performance Computer Architecture
(HPCA). https://doi.org/10.1109/HPCA.2017.37

[42] Chenhao Xie, Xingyao Zhang, Ang Li, Xin Fu, and Shuaiwen Leon Song. 2019.
PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World
With Customized Memory Cube. In 2019 IEEE International Symposium on High
Performance Computer Architecture (HPCA).

[43] Vinson Young, Aamer Jaleel, Evgeny Bolotin, Eiman Ebrahimi, David Nellans,
and Oreste Villa. 2018. Combining HW/SW Mechanisms to Improve NUMA
Performance of Multi-GPU Systems. In 51st Annual IEEE/ACM International
Symposium on Microarchitecture (MICRO).

[44] D. Zhang and Y. Luo. 2012. Single-trial ERPs elicited by visual stimuli at two
contrast levels: Analysis of ongoing EEG and latency/amplitude jitters. In 2012
IEEE Symposium on Robotics and Applications (ISRA). 85–88. https://doi.org/10.
1109/ISRA.2012.6219126

REFERENCES
[1] 2017. Direct3D. https://msdn.microsoft.com/en-us/library/windows/desktop/

bb219837(v=vs.85).aspx

[2] 2017. OpenGL. https://www.opengl.org/about/
[3] AMD. 2017. AMD crossfire. https://www.amd.com/en/technologies/crossfire
[4] Jose-Maria Arnau, Joan-Manuel Parcerisa, and Polychronis Xekalakis. 2014. Elim-
inating Redundant Fragment Shader Executions on a Mobile GPU via Hard-
ware Memoization. In Proceeding of the 41st Annual International Symposium on
Computer Architecuture (ISCA ’14). IEEE Press, Piscataway, NJ, USA, 529–540.
http://dl.acm.org/citation.cfm?id=2665671.2665748

[5] Akhil Arunkumar, Evgeny Bolotin, Benjamin Cho, Ugljesa Milic, Eiman Ebrahimi,
Oreste Villa, Aamer Jaleel, Carole-Jean Wu, and David Nellans. 2017. MCM-GPU:
Multi-chip-module GPUs for continued performance scalability. ISCA 2017 45, 2
(2017), 320–332.

[6] Dean Beeler and Anuj Gosalia. 2016. Asynchronous Time Warp On Oculus Rift.
https://developer.oculus.com/blog/asynchronous-timewarp-on-oculus-rift/
[7] Praveen Bhaniramka, P. C. D. Robert, and S. Eilemann. 2005. OpenGL multipipe
SDK: a toolkit for scalable parallel rendering. In VIS 05. IEEE Visualization, 2005.
119–126. https://doi.org/10.1109/VISUAL.2005.1532786

[8] Hsin-Jung Chen, Feng-Hsiang Lo, Fu-Chiang Jan, and Sheng-Dong Wu. 2010.
Real-time multi-view rendering architecture for autostereoscopic displays. In
Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium
on. IEEE, 1165–1168.

[9] François De Sorbier, Vincent Nozick, and Hideo Saito. 2010. GPU-Based multi-
view rendering. In Computer Games, Multimedia and Allied Technology. 7–13.

[10] V. M. del Barrio, C. Gonzalez, J. Roca, A. Fernandez, and Espasa E. 2006. ATTILA: a
cycle-level execution-driven simulator for modern GPU architectures. In ISPASS.
[11] Advanced Micro Devices(AMD). 2015. High-Bandwidth Memory (HBM). https:

//www.amd.com/Documents/High-Bandwidth-Memory-HBM.pdf

[12] Stefan Eilemann, Ahmet Bilgili, Marwan Abdellah, Juan Hernando, Maxim
Makhinya, Renato Pajarola, and Felix Schürmann. 2012. Parallel rendering on
hybrid multi-gpu clusters. In Eurographics Symposium on Parallel Graphics and
Visualization. The Eurographics Association.

[13] Stefan Eilemann, Maxim Makhinya, and Renato Pajarola. 2009. Equalizer: A
scalable parallel rendering framework. IEEE transactions on visualization and
computer graphics 15, 3 (2009), 436–452.

[14] Stefan Eilemann, David Steiner, and Renato Pajarola. 2018. Equalizer 2.0-
Convergence of a Parallel Rendering Framework. arXiv preprint arXiv:1802.08022
(2018).

[15] Daniel Evangelakos and Michael Mara. 2016. Extended TimeWarp Latency
Compensation for Virtual Reality. In Proceedings of the 20th ACM SIGGRAPH
Symposium on Interactive 3D Graphics and Games (I3D ’16). ACM, New York, NY,
USA, 193–194. https://doi.org/10.1145/2856400.2876015

[16] Google. 2018. Google VR. https://vr.google.com/
[17] Mike Houston. 2008. Anatomy of AMD TeraScale Graphics Engine. In SIGGRAPH.
[18] Chang Hui, Lei Xiaoyong, and Dai Shuling. 2009. A dynamic load balancing
algorithm for sort-first rendering clusters. In Computer Science and Information
Technology, 2009. ICCSIT 2009. 2nd IEEE International Conference on. IEEE, 515–
519.

[19] Greg Humphreys, Mike Houston, Ren Ng, Randall Frank, Sean Ahern, Peter D
Kirchner, and James T Klosowski. 2002. Chromium: a stream-processing frame-
work for interactive rendering on clusters. ACM transactions on graphics (TOG)
21, 3 (2002), 693–702.

[20] David Kanter. 2015. Graphics processing requirements for enabling immersive

VR. In AMD White Paper.

[21] Youngsok Kim, Jae-Eon Jo, Hanhwi Jang, Minsoo Rhu, Hanjun Kim, and Jangwoo
Kim. 2017. GPUpd: A Fast and Scalable multi-GPU Architecture Using Cooper-
ative Projection and Distribution. In Proceedings of the 50th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO-50 ’17). ACM, New York,
NY, USA, 574–586. https://doi.org/10.1145/3123939.3123968

[22] Sheng Li, Jung Ho Ahn, Richard D. Strong, Jay B. Brockman, Dean M. Tullsen,
and Norman P. Jouppi. 2009. McPAT: An Integrated Power, Area, and Timing
Modeling Framework for Multicore and Manycore Architectures. In Proceedings
of the 42Nd Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO). New York, NY, USA.

[23] M. Makhinya, S. Eilemann, and R. Pajarola. 2010. Fast Compositing for Cluster-
parallel Rendering. In Proceedings of the 10th Eurographics Conference on Parallel
Graphics and Visualization (EG PGV’10). Eurographics Association, Aire-la-Ville,
Switzerland, Switzerland, 111–120. https://doi.org/10.2312/EGPGV/EGPGV10/
111-120

[24] Markets and Markets. 2018.

Virtual Reality Market by Offering (Hard-
ware and Software), Technology, Device Type (Head-Mounted Display,
Gesture-Tracking Device), Application (Consumer, Commercial, Enter-
prise, Healthcare, Aerospace Defense) and Geography - Global Fore-
https://www.marketsandmarkets.com/Market-Reports/
cast to 2024.
reality-applications-market-458.html

