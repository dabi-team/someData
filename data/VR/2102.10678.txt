Towards Immersive Virtual Reality Simulations of Bionic Vision

Justin Kasowski
University of California,
Santa Barbara, CA, USA
justin_kasowski@ucsb.edu

Nathan Wu
University of California,
Santa Barbara, CA, USA
yangwu@ucsb.edu

Michael Beyeler
University of California,
Santa Barbara, CA, USA
mbeyeler@ucsb.edu

1
2
0
2

b
e
F
1
2

]

C
H
.
s
c
[

1
v
8
7
6
0
1
.
2
0
1
2
:
v
i
X
r
a

Figure 1: Virtual and real patients for bionic vision. Top row: Retinal prosthesis patient. A microelectrode array is implanted
in the eye to stimulate the retina (left). Light captured by an external camera is transformed into electrical pulses delivered to
the retina to evoke visual percepts (middle), which a patient uses to walk towards a door (right). Bottom row: Virtual patient.
Anatomical data is used to place a simulated implant on a simulated retina (left). Visual input from a virtual reality (VR) device
is used to generate realistic predictions of simulated prosthetic vision (SPV, middle), which a virtual patient uses to walk to a
simulated door in VR (inner-right), or a door in the real world captured by the head-mounted display’s camera (outer-right).
Edges stand out due to the specific preprocessing methods used, but a variety of methods can be tested. Behavioral performance
can then be compared between real prosthesis patients, SPV of the real world, and SPV of the virtual world.

ABSTRACT
Bionic vision is a rapidly advancing field aimed at developing visual
neuroprostheses (‘bionic eyes’) to restore useful vision to people
who are blind. However, a major outstanding challenge is predict-
ing what people ‘see’ when they use their devices. The limited field
of view of current devices necessitates head movements to scan
the scene, which is difficult to simulate on a computer screen. In
addition, many computational models of bionic vision lack biolog-
ical realism. To address these challenges, we propose to embed
biologically realistic models of simulated prosthetic vision (SPV) in
immersive virtual reality (VR) so that sighted subjects can act as
‘virtual patients’ in real-world tasks.

CCS CONCEPTS
• Human-centered computing → Accessibility technologies;
Virtual reality; HCI design and evaluation methods.

KEYWORDS
retinal implant, visually impaired, virtual reality, immersion, simu-
lated prosthetic vision, vision augmentation, virtual patient

ACM Reference Format:
Justin Kasowski, Nathan Wu, and Michael Beyeler. 2021. Towards Immersive
Virtual Reality Simulations of Bionic Vision. In Augmented Humans ’21.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/1122445.1122456

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AHs ’21, February 22–24, 2021, Online
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Retinal degenerative diseases cause profound visual impairment
in more than 10 million people worldwide, and retinal prostheses
(‘bionic eyes’) are being developed to restore vision to these individ-
uals. Analogous to a cochlear implant, these devices convert video
from a head-mounted camera into electrical pulses used to stimu-
late retinal neurons, which the brain interprets as visual percepts
(‘phosphenes’; Fig. 1, top row). Current devices have been shown to
enable basic orientation & mobility tasks [1], but a growing body of
evidence suggests that the vision restored by these devices differs
substantially from normal sight [4, 10].

 
 
 
 
 
 
AHs ’21, February 22–24, 2021, Online

Kasowski, et al.

A major outstanding challenge is predicting what people ‘see’
when they use their devices. Studies of simulated prosthetic vi-
sion (SPV) often simplify phosphenes into small independent light
sources [5, 8, 14] even though recent evidence suggests phosphenes
vary drastically across subjects and electrodes [3, 10]. Another
challenge is addressing the narrow field of view (FOV) found in
most devices (but see [11]). This requires patients to scan the envi-
ronment with head movements while trying to piece together the
information [10], but many previous SPV studies are performed
on computer monitors. While some studies attempt to address
this [5, 8, 14], most fail to account for phosphene distortions. It is
therefore unclear how the findings of common SPV studies would
translate to real retinal prosthesis patients.

2 PROTOTYPE
To address these challenges, we embedded a biologically realistic
model of SPV [2, 3] in immersive virtual reality (VR) using the
Unity development platform, allowing sighted subjects to act as
virtual patients in real-world tasks (see Fig. 1, bottom row). In this
setup, the visual input about to be rendered to an HTC VIVE head-
mounted display (HMD) mimics the external camera of a retinal
implant. This input can come from the HMD’s camera or can be
simulated in a virtual environment. A combination of compute and
fragment shaders is used to simulate how this input is likely per-
ceived by a real patient. Unlike previous models, our work is based
on open-source code described in [4], which generates a realistic
prediction of SPV that matches the field of view and distortions of
real devices. This allows sighted subjects to ‘see’ through the eyes
of a retinal prosthesis patient, taking into account their head and (in
future work) eye movements as they explore an immersive virtual
environment. Future SPV models can be plugged in and applied to
new prostheses once they become available.

3 RESEARCH GOALS
3.1 Provide realistic estimates of current bionic

eye technologies

The prevailing approach to SPV is to assume that activating a grid
of electrodes leads to the percept of a grid of luminous dots, the
brightness of which scales linearly with stimulus amplitude [5, 8,
14]. By ignoring percept distortions [3, 10], performance predictions
of such studies can be highly misleading. In contrast, our work is
constrained by neuroanatomical and psychophysical data.

In addition, current devices are typically evaluated on simple
behavioral tasks, such as letter/object recognition [7, 9], following
a line painted on the ground, or finding a door in an empty room
[13]. Even simple letter recognition tasks require head movements
to scan the scene, which is best emulated in an immersive VR
environment (note that FOVbionic eye << FOVHMD).

3.2 Assess the potential of advanced

stimulation strategies

With the limited number of pixels found in current devices (e.g.,
Argus II: 6 × 10 electrodes), it is impossible to accurately represent
a scene without preprocessing. Rather than aiming to restore “nat-
ural vision”, there is potential merit in borrowing computer vision

Figure 2: Different image processing techniques for simu-
lated prosthetic vision (SPV). A) Original Image. B) SPV after
segmenting important objects. C) Edge enhancement of the
original image. D) SPV of the edge-enhanced image.

algorithms as preprocessing techniques to maximize the usefulness
of bionic vision. Edge enhancement and contrast maximization are
already routinely used in current retinal implants, and more ad-
vanced techniques based on object segmentation or visual saliency
might further improve visual performance [12]. For example, using
an object detection algorithm for cars could help prostheses users
at a crosswalk (Fig. 2), while facial recognition or text magnification
would be useful in other scenarios.

Although current bionic eyes have been implanted in over 500
patients worldwide, experimentation with improved stimulation
protocols remains challenging and expensive. Virtual patients can
offer an affordable alternative for designing high-throughput ex-
periments that can test theoretical predictions, the best of which
can then be validated in real prosthesis patients.

3.3 Guide the prototyping of future devices
The insights gained through virtual patients may help drive the
changes for future devices. Obvious improvements could be realized
by testing different electrode layouts and stimulation frequencies.
Some of these factors have been modeled before in isolation, but
these models often predicted higher visual acuity than what was
found in clinical trials [5, 6, 8, 14]. Therefore, using an established
and psychophysically validated computational model of bionic vi-
sion may prove invaluable to generating realistic predictions of
visual prosthetic performance.

4 CONCLUSION
The present work constitutes a first essential step towards immer-
sive VR simulations of bionic vision. The proposed system has the
potential to 1) further our understanding of the qualitative experi-
ence associated with different bionic eye technologies, 2) provide
realistic expectations of bionic eye performance for patients, doc-
tors, manufacturers, and regulatory bodies, and 3) accelerate the
prototyping of new devices.

Towards Immersive Virtual Reality Simulations of Bionic Vision

AHs ’21, February 22–24, 2021, Online

REFERENCES
[1] Lauren N. Ayton, Nick Barnes, Gislin Dagnelie, Takashi Fujikado, Georges Goetz,
Ralf Hornig, Bryan W. Jones, Mahiul M. K. Muqit, Daniel L. Rathbun, Katarina
Stingl, James D. Weiland, and Matthew A. Petoe. 2020. An update on retinal
prostheses. Clinical Neurophysiology 131, 6 (June 2020), 1383–1398. https:
//doi.org/10.1016/j.clinph.2019.11.029

[2] Michael Beyeler, Geoffrey M. Boynton, Ione Fine, and Ariel Rokem. 2017.
pulse2percept: A Python-based simulation framework for bionic vision. preprint.
Neuroscience. https://doi.org/10.1101/148015

[3] Michael Beyeler, Devyani Nanduri, James D. Weiland, Ariel Rokem, Geoffrey M.
Boynton, and Ione Fine. 2019. A model of ganglion axon pathways accounts for
percepts elicited by retinal implants. Scientific Reports 9, 1 (June 2019), 9199. https:
//doi.org/10.1038/s41598-019-45416-4 Number: 1 Publisher: Nature Publishing
Group.

[4] M. Beyeler, A. Rokem, G. M. Boynton, and I. Fine. 2017. Learning to see again: bio-
logical constraints on cortical plasticity and the implications for sight restoration
technologies. J Neural Eng 14, 5 (June 2017), 051003. https://doi.org/10.1088/1741-
2552/aa795e

[5] Spencer C. Chen, Gregg J. Suaning, John W. Morley, and Nigel H. Lovell. 2009.
Simulating prosthetic vision: I. Visual models of phosphenes. Vision Research 49,
12 (June 2009), 1493–1506. https://doi.org/10.1016/j.visres.2009.02.003

[6] Derrick L. Cheng, Paul B. Greenberg, and David A. Borton. 2017. Advances in
Retinal Prosthetic Research: A Systematic Review of Engineering and Clinical
Characteristics of Current Prosthetic Initiatives. Current Eye Research 42, 3 (March
2017), 334–347. https://doi.org/10.1080/02713683.2016.1270326 Publisher: Taylor
& Francis _eprint: https://doi.org/10.1080/02713683.2016.1270326.

[7] Lyndon da Cruz, Brian F. Coley, Jessy Dorn, Francesco Merlini, Eugene Filley,
Punita Christopher, Fred K. Chen, Varalakshmi Wuyyuru, Jose Sahel, Paulo
Stanga, Mark Humayun, Robert J. Greenberg, Gislin Dagnelie, and for the Argus
II Study Group. 2013. The Argus II epiretinal prosthesis system allows letter
and word reading and long-term function in patients with profound vision loss.
British Journal of Ophthalmology 97, 5 (May 2013), 632–636. https://doi.org/10.
1136/bjophthalmol-2012-301525 Publisher: BMJ Publishing Group Ltd Section:
Clinical science.

[8] Gregoire Denis, Christophe Jouffrais, Corinne Mailhes, and Marc J.-M. Mace. 2014.
Simulated prosthetic vision: improving text accessibility with retinal prostheses.

Annual International Conference of the IEEE Engineering in Medicine and Biology
Society. IEEE Engineering in Medicine and Biology Society. Annual International
Conference 2014 (2014), 1719–1722. https://doi.org/10.1109/EMBC.2014.6943939
[9] Thomas L. Edwards, Charles L. Cottriall, Kanmin Xue, Matthew P. Simunovic,
James D. Ramsden, Eberhart Zrenner, and Robert E. MacLaren. 2018. Assessment
of the Electronic Retinal Implant Alpha AMS in Restoring Vision to Blind Patients
with End-Stage Retinitis Pigmentosa. Ophthalmology 125, 3 (March 2018), 432–
443. https://doi.org/10.1016/j.ophtha.2017.09.019

[10] Cordelia Erickson-Davis and Helma Korzybska. 2020. What do blind people “see”
with retinal prostheses? Observations and qualitative reports of epiretinal implant
users.
https://doi.org/10.1101/2020.02.03.932905
Publisher: Cold Spring Harbor Laboratory Section: New Results.

, 2020.02.03.932905 pages.

[11] Laura Ferlauto, Marta Jole Ildelfonsa Airaghi Leccardi, Naïg Aurelia Ludmilla
Chenais, Samuel Charles Antoine Gilliéron, Paola Vagni, Michele Bevilacqua,
Thomas J. Wolfensberger, Kevin Sivula, and Diego Ghezzi. 2018. Design and
validation of a foldable and photovoltaic wide-field epiretinal prosthesis. Nature
Communications 9, 1 (March 2018), 992. https://doi.org/10.1038/s41467-018-
03386-7 Number: 1 Publisher: Nature Publishing Group.

[12] Nicole Han, Sudhanshu Srivastava, Aiwen Xu, Devi Klein, and Michael
Beyeler. 2021. Deep Learning–Based Scene Simplification for Bionic Vision.
arXiv:2102.00297 [cs.CV]

[13] Mark S. Humayun, Jessy D. Dorn, Ashish K. Ahuja, Avi Caspi, Eugene Filley, Gislin
Dagnelie, Joël Salzmann, Arturo Santos, Jacque Duncan, Lyndon daCruz, Saddek
Mohand-Said, Dean Eliott, Matthew J. McMahon, and Robert J. Greenberg. 2009.
Preliminary 6 month results from the Argus II epiretinal prosthesis feasibility
study. Conference proceedings: ... Annual International Conference of the IEEE
Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and
Biology Society. Annual Conference 2009 (2009), 4566–4568. https://doi.org/10.
1109/IEMBS.2009.5332695

[14] Marc P. Zapf, Paul B. Matteucci, Nigel H. Lovell, Steven Zheng, and Gregg J.
Suaning. 2014. Towards photorealistic and immersive virtual-reality envi-
ronments for simulated prosthetic vision: integrating recent breakthroughs
in consumer hardware and software. Annual International Conference of the
IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine
and Biology Society. Annual International Conference 2014 (2014), 2597–2600.
https://doi.org/10.1109/EMBC.2014.6944154

