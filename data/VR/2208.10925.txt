Vox-Surf: Voxel-based Implicit Surface
Representation

Hai Li∗, Xingrui Yang∗, Hongjia Zhai, Yuqian Liu, Hujun Bao, Guofeng Zhang†

1

2
2
0
2

g
u
A
1
2

]

V
C
.
s
c
[

1
v
5
2
9
0
1
.
8
0
2
2
:
v
i
X
r
a

Fig. 1. Demonstration of progressive surface reconstruction using Vox-Surf. We apply voxel pruning and splitting periodically during
training until only voxels near the surface remains (left three images) and get ﬁner bounding voxels, geometry surface, and rendered
texture image (right three images).

Abstract—Virtual content creation and interaction play an important role in modern 3D applications such as AR and VR. Recovering
detailed 3D models from real scenes can signiﬁcantly expand the scope of its applications and has been studied for decades in the
computer vision and computer graphics community. The relatively mature reconstruction methods are mainly based on multi-view stereo,
which heavily relies on intermediate representations, such as dense point clouds triangulated from matching features. Due to the lack of
global constraints, such representations are easily affected by the environment, resulting in surface noise or missing, reducing the
reconstruction quality. Moreover, such methods are often difﬁcult to render realistic images through multi-view texture mapping. Recently
proposed neural implicit surface reconstruction methods directly utilize raw images as supervision for end-to-end training, effectively
eliminating the information loss caused by indirect methods. However, they store the entire dense space in the neural network
parameters, making it unsuitable for large-scale scenes and some interactive applications without surface extraction. To alleviate this
problem, we propose Vox-Surf, a voxel-based implicit surface representation. Our Vox-Surf divides the space into ﬁnite bounded voxels.
Each voxel stores geometry and appearance information in its corner vertices. Vox-Surf is suitable for almost any scenario thanks to
sparsity inherited from voxel representation and can be easily trained from multiple view images. We leverage the progressive training
procedure to extract important voxels gradually for further optimization so that only valid voxels are preserved, which greatly reduces the
number of sampling points and increases rendering speed. The ﬁne voxels can also be considered as the bounding volume for collision
detection. The experiments show that Vox-Surf representation can learn delicate surface details and accurate color with less memory and
faster rendering speed than other methods. We also show that Vox-Surf can be more practical in scene editing and AR applications.

Index Terms—Surface reconstruction, Implicit representation, Scene editing.

(cid:70)

1 INTRODUCTION

V irtual content creation and interaction are important parts

of 3D applications. Usually, virtual content needs to be
created by professional designers, which is complicated and time-
consuming. Therefore, reconstructing accurate surfaces from real

• Hai Li, Hongjia Zhai, Hujun Bao, Guofeng Zhang are with the State Key

Lab of CAD&CG, Zhejiang University, Hangzhou, China.
E-mails: {garyli, zhj1999, baohujun, zhangguofeng}@zju.edu.cn
Xingrui Yang was with Visual Information Laboratory, University of Bristol,
Bristol, United Kingdom.
E-mail: x.yang@bristol.ac.uk
Yuqian Liu was with Autonomous Driving Group, SenseTime, China.
E-mail: liuyuqian@senseauto.com

•

•

*. Equal contribution
†. Corresponding author

scenes is a critical technique for virtual content generation, which
is also an essential research topic in computer vision and computer
graphics.

Before the age of deep learning, surface reconstruction from
images was dominated by the multi-view stereo pipeline [1], [2],
which highly depends on feature detection and matching. Although
these methods are relatively mature in academia and industry, the
indirect reconstruction process creates a gap between the input
images and the ﬁnal reconstruction. This gap will lead to a loss
of information and pose a challenge in reconstructing complex
scenes. For example, in the presence of weak textures, repetitive
features, or brightness inconsistency, it would be difﬁcult to match
the exact features, leading to the triangulation of wrong 3D points
and, eventually, reconstruction errors. Additionally, the ﬁnal mesh

 
 
 
 
 
 
and corresponding texture are generated separately. The discrete
triangular meshes and texture patches often fail to render realistic
images.

Most recently, there has been a trend of using neural networks as
scene representations. Works like [3], [4] have shown that implicit
surface representations such as signed distance ﬁelds (SDF) or
occupancy ﬁelds can be directly learned and stored inside a multi-
layer perceptron (MLP). These networks can learn a continuous
scene representation from discrete 3D point samples. Based on
this discovery, DVR [5] and IDR [6] extend this representation to
image-based surface reconstruction tasks. However, these methods
only deﬁne appearance on the surface and are prone to defects in
under-observed areas.

With the advent of NeRF-based [7] methods, there has been
a considerable improvement in the novel view synthesis task.
NeRF and its following methods leverage volume rendering to
learn a radiance ﬁeld to represent a dense space. However, this
representation does not explicitly reconstruct the surface. Thus,
methods like NeuS [8], UNISURF [9] and VolSDF [10] propose to
combine the implicit surface representation and radiance ﬁeld to
achieve surface reconstruction through volume rendering. These
methods can directly use posed images for end-to-end training
without additional representations, which minimizes information
loss and reach a higher accuracy than traditional methods.

However, these methods aim to reconstruct the entire space with
a single network, making large-scale reconstructions infeasible due
to the limited capacity of the network. Apart from that, the implicit
nature of the scene makes it difﬁcult to manipulate, such as scene
splitting and editing. To circumvent this limitation, inspired by [11],
[12], we adopt a hybrid architecture that consists of an explicit
voxel representation with the implicit surface representation. This
architecture combines the beneﬁt of an explicit representation,
which allows the scene to be manipulated, and the representational
power of the implicit network.

Fig. 2. Demonstration of difference between traditional voxel-based
implicit surface, neural implicit surface and voxel-based neural implicit
surface representations.

We propose Vox-Surf, a neural implicit surface rendering
framework that combines voxel-based representation with image-
based surface reconstruction for efﬁcient surface reconstruction
and realistic rendering. As shown in Figure 2, compared to the
traditional voxel-based implicit surface [13], which stores explicit
SDF, weight and color in tiny voxels, our method only stores
trainable embeddings vector in voxels’ vertices, which contain
knowledge of the local geometry and appearance. Compared to
dense neural implicit surface methods, we divide the target scene
into multiple bounded voxels, thereby reducing sampling and
learning process of empty areas. All in all, our method can represent
ﬁner surfaces with larger voxels and a lightweight network.
To further increase the reconstruction accuracy and maintain the

2

memory consumption, we leverage the progressive voxel pruning
and splitting strategy ( Figure 1) together with a surface-aware
sampling strategy to decrease the voxel size and sample only the
valid points during training. The ﬁne voxels can act as bounding
volumes for collision detection in AR or VR interactions, which is
more efﬁcient than other representations.

To summarize, our main contributions are as follows:

• We proposed Vox-Surf, a voxel-based neural implicit
surface representation which can learn end-to-end from
multi-view posed images.

• We propose to use progressive training and a surface-
aware sampling strategy to maintain memory efﬁciency and
improve the reconstruction quality. Our proposed method
performs well in three different datasets with variable scales
through extensive experiments.

• We show that our Vox-Surf representation enables scene
editing and composition, which is infeasible for other
implicit surface representations. This property paved the
way for potential AR/VR applications.

2 RELATED WORK
2.1 Surface reconstruction methods

Surface reconstruction from posed images can be achieved with
traditional multi-view stereo methods [2]. A dense depth map is
estimated for each input image by exploiting the photo-consistency
property across nearby frames [14]. Different scene representations
are used depending on the actual applications. Some of the popular
choices are point clouds [15], [16], voxels [17], [18], level-sets [19],
and triangular meshes [20], [21], [22]. Voxels and level-sets (e.g.,
signed distance functions) are suitable methods for representing
ﬁne details, whereas meshes are more compact and simple for
rendering and other 3D tasks such as intersection tests.

2.2 Implicit scene representation

Recent works on implicit neural representations paved the way for
learning-based scene representations [3], [4], [23]. Compared to
traditional methods, they can represent a continuous scene with
signiﬁcantly smaller memory footprints and can be used to generate
consistent novel views. They leverage the capacity of multi-layer
perceptron (MLP) to learn a mapping function that can be used
to encode an entire scene. However, the representational power of
a single MLP is limited and does not scale well to large scenes.
Therefore many works rely on scaling the input data to a smaller
area, usually a unit sphere or unit cube. A much more efﬁcient
choice is to encode the scene into local blocks. Inspired by [12], we
adopted a sparse voxel structure that saves computational resources
by only reconstructing occupied spaces. The voxels can also be
further subdivided to reconstruct ﬁner details.

2.3 Volume rendering methods

Volume rendering based on neural scene representations is made
popular by NeRF [7], which renders images by α-composing
density and colors along with the camera rays. However, NeRF
and many other similar works assume the surface to be rendered
from a density ﬁeld, which is suitable for handling transparent
scenes but unnecessary if one only cares about representing explicit
surfaces. Recent surface rendering methods follow the idea that
rendered colors should be directly generated from surface points,

3

Fig. 3. Proposed Pipeline. We ﬁrst divide the scene into multiple voxels and assign embeddings to voxel vertices. Then we calculate the voxel-ray
intersections and sampling points along the ray. Based on the position of sampling points, we trilinear interpolate the nearest voxel embeddings to get
the embedding for each point and feed it into the geometry extractor to get SDF values and feature vectors. Then we concatenate the feature vector
with ray direction and embedding and feed it into the appearance extractor to get the color for each sampling point.

UNISURF [9] proposes to learn an implicit surface by iteratively
performing root-ﬁnding along with each viewing ray. The color
is then determined from features extracted from the surface. A
similar idea appears in [8]. They propose an unbiased weighting
function directly conditioned on the estimated SDF values along
with the camera ray. These methods are most related to our work
in that we also aim to reconstruct an explicit surface representation.
As we demonstrate in our experiments, adding explicit voxel
representations allows the above network to scale to large scenes,
both indoor and outside.

3 METHOD
Given a set of posed images I = {I1, ..., In} of a scene with 6-DoF
transformation Ti ∈ SE(3), our goal is to reconstruct the colored
surface with ﬁnite sparse voxels implicitly. We ﬁrst divide the
bounded scene with a set of coarse voxels and use neural networks
to model the geometry and appearance information of the scene.
By minimizing the difference between the rendered images and the
input images, we can optimize the weights of the neural networks.
In order to improve the rendering efﬁciency and model ﬁne-
grained geometry information, we prune those voxels that do not
contain surface structure and continuously subdivide the voxels that
contain surface structure for learning better geometry information.
Additionally, we propose a surface-aware sampling strategy to
maximize the possibility of valid sampling while maintaining the
memory footprint. The whole pipeline is shown in Figure 3, and we
will describe the details of surface representation in the following
sections.

3.1 Geometry and appearance representations
The scene is divided by a set of coarse voxels V = {V1, ...,Vk}
and each voxel has eight corner vertices which contain the
encoded geometry and appearance information. This information is
represented by a ﬁxed-length optimizable embedding e ∈ RLe . So,
for any 3d point p ∈ R3 within one of the voxels Vi, we can obtain
its embedding via retrieval function Γ : R3 → RD, which maps point
p to a Le-length embedding vector e. This function is implemented
by trilinear interpolation, which interpolates embeddings from eight
voxel corners based on their coordinates.

In Vox-Surf, We leverage the Multi-layer Perceptrons (MLP)
network to represent the two extractors: geometry extractor Fσ and
appearance extractor Fc. The geometry extractor Fσ (e) : RLe →

R1+L f maps an embedding vector e of a spatial position p ∈ R3
to its signed distance value, σ ∈ R, which is the shortest distance
from a point p to a surface and a geometry feature vector f ∈ RL f .
The signed distance σ also indicates whether p is inside or outside
of the surface S . Thus, the surface S of the scene can be easily
extracted by Equation 1.

S = { p ∈ R3 | Fσ (Γ(p))[0] = 0 },

(1)

where operation [0] means taking the ﬁrst value from Fσ , which in
our case is signed distance σ of position p.

After obtaining the surface of the scene, we can calculate the
normal vector n ∈ R3 by Equation 2, which is computed using
double backpropagation trick [5] and implemented by automatic
differentiation provided in Pytorch.

n =

dS
dp

=

dFσ (Γ(p))[0]
dp

(2)

The geometry feature vector f = Fσ (e)[1 :] ∈ RL f contains both
the structure and appearance information. We then concatenate the
geometry feature vector f , ray direction d and point embedding
vector e as the input of appearance extractor Fc to obtain the color
c at position p.

In practice, we adopt the positional encoding trick proposed in
Nerf [7] on both embedding vector e and view direction d before
feeding into the network.

3.2 Voxel initialization

Like all other neural implicit based methods [7], [8], [9], we
require multiple-view images with poses as input and also know
the approximate extent of the scene. These prerequisites can be
easily obtained from some existing SLAM [24], [25] or SFM
[1] methods. Unlike methods that use an implicit function to
reconstruct the entire space, the proposed Vox-Surf is much more
ﬂexible by only reconstructing occupied space and updating each
voxel individually.

We ﬁrst divide the bounding area into a set of uniformly
spaced voxels V . The initial voxel size is chosen to contain the
potential surface reconstruction. Since we use the embedding as
network input instead of 3d coordinates (x,y,z), our representation
is coordinate agnostic. Therefore our method does not require the
scene to be re-scaled or re-centerd as many works do. Based on
the initial voxels, we then build a sparse hierarchical octree [26]

4

mVi
∑i mVi

step size. The sampling probability for each voxel alongside the
ray is
. We then apply the inverse Cumulative Distribution
Function (CDF) sampling strategy to sample the points for all rays
in parallel and deﬁne the interval range [tl,tr] for each sample point
p = o + t ∗ d, where t is the midpoint of the interval. This step is
similar to that used in [12].

3.3.2 Surafce aware sampling
In order to optimize the extractor network Fσ , Fc and voxel vertices
embeddings e from scratch, we need to sample sufﬁciently dense
points for training, which could cost a lot of memory. However, in
most surface reconstruction cases, the surface is usually assumed
to be opaque, which means the point on the ray that intersects the
surface contributes most to the appearance of the ray. Based on this
assumption, existing dense space methods either require to ﬁnd
the exact intersection point by iteratively applying a root-ﬁnding
algorithm along the ray to choose the sample region [9], or leverage
the importance sampling strategy to insert extra points in certain
areas based on uniform samples [8].In our Vox-Surf representation,
we can take advantage of the above two methods simultaneously,
but it is more efﬁcient and does not add extra points.

Since only voxels adjacent to the surface are preserved (See
subsection 3.5), we only need to focus on the voxel, which contains
the surface intersecting with visible rays. Therefore we propose the
surface-aware sampling strategy shown in Figure 4. The procedure
can be roughly divided into three steps: (1) Uniform Voxel
Sampling: We uniformly sample points p on the rays inside voxels.
(2) Cross-Surface Voxel Searching: We leverage the geometry
extractor Fsigma(Γ(p)) to compute the signed distance value for
each sampled point. The surface intersection is found by looking
for two points pi and pi+1 where the SDF value changes from
positive(outside) to negative(inside) along the viewing ray. We
then mark voxels that contain these points as important voxels.
(3) Surface-Aware Voxel Resampling: We increase the sampling
probability inside important voxels, then normalize the sampling
probability along the ray following [7]. Given the selected important
voxels, we resample the rays based on re-computed probability but
keep the total point number ﬁxed. In practice, we further divide this
strategy into full surface-aware resampling and ﬁrst surface-aware
resampling according to whether only the ﬁrst important voxel is
used. We use the former to optimize all possible surfaces when the
shape is unstable and use the latter to reconstruct ﬁne details on
stable shapes.

3.4 Volume rendering

NeRF-based methods [7] render the color via volume rendering
[27] which accumulate the point color along the ray r(t) = o + t · d
according to the volume density α with following equation.

C(o, d) =

(cid:90) +∞

0

T (t)α(r(t))c(r(t), d)dt,

(3)

where T (t) = exp(− (cid:82) t
0 α(u)du) is accumulated transmittance along
the ray r(t) from 0 to t, which means the probability of the ray
arriving position ro + trd without hitting any other particle. The
essential problem here is how to transform the SDF values into
density weights T (t)α(t) so that we can weigh the contribution
of different points along the ray based on their distance to the
actual surface. We adopt the weight function proposed in NeuS
[8] for rendering, which transforms the SDF to density using The

Fig. 4. Demonstration of Surface-Aware Sampling. We ﬁrst uniform
sample points and calculate the SDF value for each sampling point.
Then we ﬁnd the cross surface region and mark them as important
voxels. Next, we recompute the sampling probability and re-sample the
points. According to the number of important voxels, we further divide
this strategy into full surface-aware sampling and ﬁrst surface-aware
sampling.

and take all voxels as leaf nodes. This structure accelerates the
ray intersection test, which we will describe later. Each vertex
is assigned a Le-dimensional trainable embedding with a random
initial value. For two adjacent voxels, they share 4 vertices and
corresponding embeddings.

3.3 Voxel-based Point Sampling

Point sampling is an essential step for implicit scene optimization.
To fully leverage the advantage of voxels, we proposed the voxel-
based sampling strategy, which is faster and more memory efﬁcient.
We will describe the process of ray generation and the proposed
points sampling strategy below.

3.3.1 Ray generation

For each image, we generate a cluster of 3d rays, and each ray
can be denoted in the form of r = (o, d), where o is the camera
center and d is the direction vector from o to each image pixel
inside world space. To avoid redundant sampling, we apply the axis-
aligned bounding box intersection (AABB) test for each ray with
the octree of voxels. This procedure gives us the intersect voxel set
for each ray r and the distance of ray inside each voxel mVi . We
cull out rays that do not intersect any voxels. For the remaining
rays, we ﬁrst deﬁne the number of sampling points concerning the
total distance inside all intersected voxels ∑i mVi with a pre-deﬁned

5

Fig. 5. Qualitative results on DTU dataset. We show the surface reconstruction results with six objects from different view directions. From left to right:
reference image, mesh model from the front view with voxels after third time splitting for better visualization, mesh model from left view, mesh model
from right view.

S-density. The S-density function φs(σ ) is a unimodal function of
signed distance σ of point p, where

φs(σ ) =

se−s·σ
(1 + e−s·σ )2

(4)

is the derivative of the Sigmoid function Φs(σ ) = (1 + e−s·σ )−1, s
is a scale parameter which controls the shape of the distribution.
The value of the point closer to the surface S is bigger than the
weight of far points.

In our proposed voxel representation, the voxels are indepen-
dent of each other due to the interpolation mechanism, which
means that the sampling spacing in each voxel cannot exceed the
boundary. Thus, to prevent cross-voxel accumulation situation, i.e.
ti and t j are cross the voxel boundary. Based on the new interval,
we cut off the interval near voxel boundaries and recalculate the ti
and t j.

Based on the S-density φs(σ ), the opaque density function ρ

3.5 Progressive training

can be derived with the following equation,

ρ(t) = max(

− Φs
dt (σ )
Φs(σ )

, 0)

Thus,
exp(− (cid:82) ti+1

ti

volume

the
ρ(t)dt). In a discrete version,

density

function

(5)

α(t) = 1 −

In order to reduce memory pressure and improve surface accuracy,
we only need to preserve and optimize the voxels that contain the
surface. Thus we adopt the pruning and splitting strategy similar
to other voxel-based methods [12] but more suitable for surface
representation.

α(ti) = ReLU(

Φs(Fσ (Γ(r(ti)))[0]) − Φs(Fσ (Γ(r(ti+1)))[0])
Φs(Fσ (Γ(r(ti)))[0])

),

(6)

where ReLU() is the Rectiﬁed Linear Unit [28].

The discrete accumulated transmittance is shown as the follow-

ing:

T (ti) =

i−1
∏
j=1

(1 − α(t j))

(7)

3.5.1 Voxel pruning

Since each voxel contains a continuous signed distance ﬁeld, we
can process all voxels in parallel. We ﬁrst uniformly sample enough
3d points inside each voxel. Then we compute the SDF values
using the geometry extractor Fσ . To decide whether to retain or
prune the voxel, we deﬁned a distance threshold τ, formally in
Equation 9.

So, with the Np sampled points in subsection 3.3 {pi =
o + ti · d|i = 1, ..., Np,ti < ti+1} along the ray, we can obtain the
approximate color by

C(r) =

Np
∑
i=1

T (ti)αi(ti)ci

(8)

Ki = |Fσ (Γ(p))[0] | < τ if ∃p inside Vi,Vi ∈ V

(9)

Here K ∈ {0, 1} is a ﬂag, which means whether the voxel is
reserved. We then prune the corresponding leaf node voxels from
the octree according to the K.

3.5.2 Voxel splitting

Representing a scene inside a set of coarse voxels is not sufﬁcient.
The coarse-level voxel can not recover ﬁne structure for a large
space. In order to represent more details, we periodically split
the existing voxels into eight sub-voxels and insert them into the
existing octree. The initial embeddings of newly generated voxel
vertices are computed using embedding retrieval function Γ, and
then these embeddings are optimized independently of their parent
nodes.

Pruning and splitting allow us to obtain sparse and important
voxels for ﬁner optimization and boost the representation power of
each voxel.

3.6 Losses

We leverage the following loss functions to optimize the embed-
dings, geometry and appearance extractor networks. For each ray,
we ﬁrst compute the weighted sum color C(r) from Equation 8,
and take the L1 loss between then ground truth color ˆC(r) and the
rendered color C(r).

Lcolor = ∑

(cid:107) ˆC(r) −C(r)(cid:107)1

r

(10)

To constrain the regulate ﬁeld, we also add the eikonal loss
term [29] on sampled points. This term plays an important role in
the initialization of the shape at the early stage.

Leikonal = ∑

((cid:107)∇Fσ (Γ(pi))[0](cid:107)2 − 1)2

i

(11)

Moreover, we uniformly sample additional points inside every
observed voxel
to further regulate the signed distance ﬁeld
throughout the voxel. This strategy works especially well for under-
observed voxels, often in large-scale scenes like street view.

Depth loss Depth sensors are also becoming common in
everyday life, such as ToF and lidars. These sensors can provide
coarse depth information in a certain range. This information
of depth is particularly useful in surface reconstruction. Recent
density-based methods [30], [31] leverage depth as strong geometry
supervision and obtained better results. However, the different
formulations cannot directly apply the same loss function in our
SDF-based representation. The main difﬁculty of relating rendered
depth to SDF is that the SDF values are uncertain without the
knowledge of the accurate surface. Therefore, we propose an
occupancy-based depth loss using Equation 12.

6

Fig. 6. Detail comparison with Vox-Surf (left) and NeuS [8](right). The
results show that our method keeps more details and less noise.

Fig. 7. Surface accuracy at different iterations of Scene 24. The red circle
indicates the surface splitting.

even when the ray intersects with multiple surfaces, this loss still
works well given enough observation.

Lnear = ∑

t

(cid:107)Fσ (Γ(r(t)))[0](cid:107)2

(15)

For points between the near range ˆt − δt ≤ t ≤ ˆt + δt, we directly
constrain the SDF value to 0. The value δt highly depends on the
conﬁdence of the given depth.

Finally, the total depth loss is the combination of the above

occ(t) = Sigmoid(−scale · Fσ (Γ(r(t)))[0])

(12)

three losses:

Ldepth = Loutside + Lnear + Linside

(16)

This continuous occupancy function acts like a scaled truncated
signed distance function whose gradient only peaks near the surface.
Thus, we split the ray r(t) with depth supervision into three
intervals with different corresponding losses:

Loutside = ∑

(cid:107)1 − occ(t)(cid:107)2

t

(13)

For points in front of the given depth t < ˆt − δt, δt is a small noise
tolerate depth range. we always assume these points are outside
the surface.

Linside = ∑

(cid:107)occ(t)(cid:107)2

t

(14)

For points behind the given depth t > ˆt + δt, we always assume
these points are inside the surface. In experiments, we found that

4 EXPERIMENTS
We conduct our experiments on three types of datasets: DTU [32]
(small objects), ScanNet [33] (indoor scenes) and Kitti360 [34]
(outdoor scenes). In this section, we will describe the detailed
training settings and evaluation results.

4.1 Results on DTU dataset
The DTU dataset contains multi-view images with ﬁxed camera
parameters at 1200 × 1600. This data set consists of 124 different
scenes with different shapes and appearances, We use 15 scenes
from the DTU dataset for training and evaluation, same as those
used in IDR [6] and NeuS [8] for a fair comparison.

TABLE 1
Reconstruction results on DTU

7

Scene ID

COLMAP

DVR

IDR

NeuS

PSNR

Chamfer

PSNR

Chamfer

PSNR

Chamfer

PSNR*

Chamfer

PSNR

Ours
Chamfer

24
37
40
55
63
65
69
83
97
105
106
110
114
118
122

Mean

20.28
15.5
20.71
20.76
20.57
14.54
21.89
23.20
18.48
21.30
22.33
18.25
20.28
25.39
25.29

20.58

0.81
2.05
0.73
1.22
1.79
1.58
1.02
3.05
1.40
2.05
1.00
1.32
0.49
0.78
1.17

1.36

16.23
13.93
18.15
17.14
17.84
17.23
16.33
18.10
16.61
18.39
17.39
14.43
17.08
19.08
21.03

17.26

4.10
4.54
4.24
2.61
4.34
2.81
2.53
2.93
3.03
3.24
2.51
4.80
3.09
1.63
1.58

3.20

23.29
21.36
24.39
22.96
23.22
23.94
20.34
21.87
22.95
22.71
22.81
21.26
25.35
23.54
27.98

23.30

1.63
1.87
0.63
0.48
1.04
0.79
0.77
1.33
1.16
0.76
0.67
0.90
0.42
0.51
0.53

1.54

26.70
23.72
26.54
25.62
31.22
32.83
29.20
32.83
27.12
32.41
32.18
28.83
28.38
35.00
34.97

29.94

0.83
0.98
0.56
0.37
1.13
0.59
0.60
1.45
0.95
0.78
0.52
1.43
0.36
0.45
0.45

0.77

24.98
23.17
25.32
22.89
30.12
31.62
27.88
31.62
26.67
30.58
30.58
27.69
27.17
33.01
33.68

28.87

0.72
1.15
0.51
0.35
1.09
0.58
0.59
1.35
0.91
0.77
0.46
1.09
0.35
0.42
0.43

0.72

Voxels

33271
22172
22232
8755
35678
30958
12737
17185
23754
10422
20483
20249
28364
13726
16789

4.1.2 Evaluation results
We show the qualitative reconstruction results of the DTU dataset
in Figure 5 from three different views. The results show that our
proposed method can reconstruct an accurate surface of the complex
scene with different geometry information. Then we compare the
surface detail with Neus [8] in Figure 6. From the highlighted
area, we can see that our surface preserves more details with
less noise. The overall quantitative results show in Table 1. The
best and the second best results are shown in bold and underline
respectively. We show the image error (PSNR) and reconstruction
error (Chamfer) with COLMAP [1], [2], DVR [5], IDR [6] and
NeuS [8]. The results show that our method outperforms the SOTA
methods in reconstruction quality but is slightly lower in image
quality. The reason comes from two aspects. One is that we use a
lightweight network to increase the rendering speed, while NeuS
uses a larger network to ﬁt the color information in each view
direction. The second is that NeuS sacriﬁces part of the surface
accuracy to ﬁt the inconsistency between viewing angles. We
further compare the memory footprint and rendering speed. As
shown in Table 2, our network is much smaller than the previous
method, and the rendering speed is much faster. Here we use a
single NVIDIA 3090 card to compute the speed of rending a batch
of rays and a 200 × 150 image. Our methods are about 5 times
faster than NeuS with the same batch size. Since we do not use
surface normal in the color network, we can further optimize the
network by removing the gradient computation and making our
method 3 times faster. Furthermore, since our method consumes
less memory than NeuS, we can further increase the batch size
to achieve a rendering speed of about 0.05 seconds per image,
which is enough for real-time applications. Additionally, we show
the Chamfer distance at different iterations in Figure 7. As the
number of voxel splitting increases, the gain to surface accuracy
gets smaller, while the voxel block will increase rapidly. Therefore,
we need to make some trade-offs according to the actual needs.

Failure case and solution: We found that if the scene contains
delicate structure, some surfaces may be may be lost during training.
This problem is uaually caused by the voxel pruning in the early
stage when delicate structures are not well learned, as shown in

Fig. 8. Surface missing caused by voxel pruning in early stage (left two
images). This can be alleviated by increasing the sampling density in the
early stage (right image).

4.1.1 Implementation details

We use the data provided in IDR [6] where the object is constrained
in the unit sphere. We ﬁrst generate the initial voxels and
corresponding octree inside a unit cube with a voxel size of
0.8. We set the initial max voxel hit number to 20 since the
scene is small and the uniform ray sampling step size to 0.03.
We apply the voxel pruning every 50,000 epochs and splitting at
20,000, 50,000, 100,000, 20,000, 300,000 iterations, respectively,
the pruning threshold τ = 0.01. Before the second splitting, we
use only uniform voxel sampling to ﬁnd the rough shape. Then
we use the full surface-aware voxel resampling strategy until the
fourth splitting. After the fourth splitting, the shape is stable, so
we change the strategy to ﬁrst surface-aware voxel resampling to
continually reﬁne the ﬁne details. The voxel embedding length
Le is 16, and the geometry extractor Fσ is a 4-layer MLP with
128 hidden units of each layer, whereas the appearance extractor
Fc is a 4-layer MLP with 128 hidden units each. Before feeding
into the extractors, we apply the same positional encoding trick
proposed in NeRF [7] with 4 frequencies on voxel embeddings e
and 8 frequencies on ray directions d. The learning rate is 0.001
for all objects.

8

(a) ground truth

(b) COLMAP

(c) TSDF fusion

(d) Ours

Fig. 9. Qualitative results on ScanNet. From left to right: ground truth mesh rendered in normal maps, reconstructed surface from COLMAP, TSDF
fusion and Vox-Surf.

(a) ground truth image

(b) rendered image

(c) ground truth depth

(d) rendered depth

Fig. 10. Novel view renderings of Vox-Surf on ScanNet. From left to right: ground truth image, rendered image, the ground truth depth map, and
rendered depth.

Figure 8. To alleviate this problem, an efﬁcient solution is to
increase the sampling density at an early stage to improve the
probability of sampling in the delicate part.

TABLE 2
Performance results on DTU

IDR

NeuS

Ours

Ours(optim)

Network Parameters
Render Time (batch of rays)
Render Time (image)

2.91M 1.41M 0.36M
0.03s
0.14s
0.35s
1.65s

-
1.08s

0.36M
0.01s (0.02s)
0.12s (0.05s)

4.2 Results on ScanNet dataset

ScanNet [33] is a large indoor RGB-D dataset containing more than
1600 room-scale sequences. ScanNet is a challenging dataset with
many images contaminated with severe artefacts such as motion
blur, etc. Also, their poses were estimated from BundleFusion [35]
instead of by accurate motion capture devices, which also poses a
challenge to scene reconstruction methods.

4.2.1 Implementation details
Besides posed RGB images, ScanNet also provides depth maps
which can be exploited to build initial voxel maps and further
supervise the network’s training, as described in subsection 3.6. We
ﬁrst back-project all depth observations into 3D points to generate
initial voxels. We then voxelized these points using an initial voxel
size of 0.4. We set the max hit voxel to 10 and the ray sampling
step size to 0.01. Since RGB-D sensors are only accurate within
a certain distance, we limit the maximum depth range to 5.0 to
reduce noisy samples. We also progressively split and prune the
voxels twice throughout training, making the smallest voxel size
0.1. Please note that we do not explicitly scale the scene before
training as other works do [8], [9].

We also observed that many frames in ScanNet have wrong
poses. Inspired by [36], we implement a per-frame pose com-
pensation module to solve this issue. Each frame has an initial
pose which ScanNet provides. They also have a corrective pose
initialized as the identity matrix and optimizable during training.

4.2.2 Evaluation results
We compared our method with COLMAP and a traditional TSDF
fusion method [13] on the subject of surface reconstruction of
RGB-D sequences. For a fair comparison, all methods use the

9

(a) Scan points

(b) COLMAP

(c) TSDF fusion

(d) Ours

Fig. 11. Qualitative results on Kitti360. From left to right: (a) coarse point cloud from depth sensors, (b) reconstructed surface from COLMAP, (c)
reconstructed surface from TSDF fusion, (d) reconstructed surface from Vox-Surf. The blue wireframe in Vox-Surf is the used voxels.

TABLE 3
Reconstruction results on ScanNet

Scan ID

0002
0005
0707
0782

TSDF fusion

Ours

Chamfer↓

F-Score↑

Chamfer↓

F-Score↑

0.055
0.291
0.046
0.453

0.946
0.758
0.894
0.504

0.056
0.080
0.061
0.378

0.950
0.884
0.976
0.623

(a) ground truth image

(b) rendered image

Fig. 12. Novel view renderings of Vox-Surf on Kitti360.

same set of images. The qualitative results are shown in Figure 9.
As can be seen, COLMAP fails to reconstruct surfaces with poor
geometric features. While TSDF fusion shows sharper edges in the
close range, our method can reconstruct more complete surfaces
and ﬁll in holes that can not be observed by depth sensors, such as
reﬂective materials. We also show better denoising properties than
TSDF fusion, as displayed by smoother surfaces at the far range,
such as the walls, etc. We also show some rendering examples
in Figure 10.

We also evaluate our system quantitatively on the ScanNet
dataset. We compare against other reconstruction techniques based
on neural implicit rendering and a traditional TSDF fusion method.
We are mainly interested in two metrics: the Chamfer distance and
F-Score [37]. We sample 20k points from both the reconstructed
and ground truth mesh for all metrics. The F-Score is calculated
based on a distance threshold of 0.05m. This threshold is chosen
to align with other works.

4.3 Results on Kitti360 dataset

Kitti360 is a large-scale dataset with rich sensory information and
full annotations, containing several long driving distance sequences.
It also provides dense semantic and instance annotations for 3D
point clouds and 2D images.

4.3.1 Implementation details
Since Kitti360 only contains large unbounded street scenarios,
directly voxelize the whole scene is infeasible. Instead, we take
two strategies to reduce the memory overhead: ﬁrstly, we split
the sequence into small segments and learn these sequences
individually. Secondly, we initialize the voxels based on the coarse
points provided in the dataset. We then voxelize the points inside
each segment with an initial size of 1.0. We set the max hit voxel to
30 and the uniform ray sampling step size to 0.002. We do not apply
pruning and splitting during training to save GPU memory. We also
downscale the image to 94 × 352. Additionally, we mask out the
sky in each image using the semantic information provided by the
dataset. We also apply depth loss on the projected coarse points.
The voxel embedding length is 16, and the geometry extractor is
composed of 4-layers MLP network with 256 hidden units, whereas
the appearance extractor is a 4-layer MLP with 256 hidden units.
We apply the positional encoding with 4 frequencies on voxel
embeddings and 6 frequencies on view directions.

As can be seen from Table 3, we compare favourably with
traditional TSDF fusion on mesh reconstruction in terms of mesh
completion (reﬂected as the F-Score) and accuracy (as shown by
the Chamfer distance).

4.3.2 Evaluation results
We compare the qualitative surface reconstruction result with
COLMAP [1], [2] and TSDF fusion [13] (Figure 11). The
qualitative results show that our Vox-Surf can recover a clean

10

Fig. 13. Example of object editing application. The leftmost column is the original voxels and their rendering result. We apply local scaling, detaching
and duplication on selected voxels, and the real texture image is displayed on the bottom row, respectively.

Fig. 14. Physical simulation with Vox-Surf. The collision process is simulated by voxels (top row), and then the texture image is rendered by the
proposed method (bottom row).

(a) An RGB image

(b) Reconstructed 3D structure

(c) AR effect on multiple images

Fig. 15. An example of AR application. We ﬁrst reconstruct the scene from multi-view posed images with coarse depth using our Vox-Surf as shown in
(b). With the pose of each view, we can achieve the effect of virtual and real fusion with occlusion handling and rigid body constraint, as shown in (c).

11

the scene, such as materials, lighting, etc., which is a practical but
challenging problem that can support more applications such as
relighting and texture editing. Secondly, our method currently relies
on prior knowledge of the scene, such as camera pose, bounding
space, etc. Reducing the dependence on these priors and applying
them to complex scenarios is a more practical problem. Thirdly,
the trained network is scene-speciﬁc and cannot be generalized to
other scenes without re-training. Although works like [38], [39]
tried to train a generalizable decoder network, it is still a long way
from being practical. We will conduct in-depth research on these
problems in our future work and explore how to integrate them
with practical applications better.

7 CONCLUSIONS
We propose a novel voxel-based implicit surface representation
named, Vox-Surf, and a progressive training strategy with an
effective surface-aware sampling strategy to let our representation
learn articulate surface from multiple view images. Our method
takes voxel as an individual rendering unit, which is suitable for
3D editing and interaction applications like AR and VR.

REFERENCES

[1]

[2]

J. L. Sch¨onberger and J. Frahm, “Structure-from-motion revisited,” in
IEEE Conference on Computer Vision and Pattern Recognition, CVPR,
2016, pp. 4104–4113.
J. L. Sch¨onberger, E. Zheng, J. Frahm, and M. Pollefeys, “Pixelwise view
selection for unstructured multi-view stereo,” in European Conference on
Computer Vision, ECCV, vol. 9907, 2016, pp. 501–518.

[3] L. M. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
“Occupancy networks: Learning 3d reconstruction in function space,” in
IEEE Conference on Computer Vision and Pattern Recognition, CVPR,
2019, pp. 4460–4470.
J. J. Park, P. Florence, J. Straub, R. A. Newcombe, and S. Lovegrove,
“Deepsdf: Learning continuous signed distance functions for shape
representation,” in IEEE Conference on Computer Vision and Pattern
Recognition, CVPR, 2019, pp. 165–174.

[4]

[5] M. Niemeyer, L. M. Mescheder, M. Oechsle, and A. Geiger, “Differen-
tiable volumetric rendering: Learning implicit 3D representations without
3D supervision,” in IEEE Conference on Computer Vision and Pattern
Recognition, CVPR, 2020, pp. 3501–3512.

[6] L. Yariv, Y. Kasten, D. Moran, M. Galun, M. Atzmon, R. Basri, and
Y. Lipman, “Multiview neural surface reconstruction by disentangling
geometry and appearance,” in Advances in Neural Information Processing
Systems, NeurIPS, 2020.

[7] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
and R. Ng, “Nerf: Representing scenes as neural radiance ﬁelds for view
synthesis,” in European Conference on Computer Vision, ECCV, vol.
12346, 2020, pp. 405–421.

[8] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, “Neus:
Learning neural implicit surfaces by volume rendering for multi-view
reconstruction,” in Advances in Neural Information Processing Systems,
NeurIPS, 2021, pp. 27 171–27 183.

[9] M. Oechsle, S. Peng, and A. Geiger, “UNISURF: unifying neural implicit
surfaces and radiance ﬁelds for multi-view reconstruction,” in IEEE/CVF
International Conference on Computer Vision, ICCV, 2021, pp. 5569–
5579.

[10] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, “Volume rendering of neural
implicit surfaces,” in Advances in Neural Information Processing Systems,
NeurIPS, 2021, pp. 4805–4815.

[11] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. T. Loop, D. Nowrouzezahrai,
A. Jacobson, M. McGuire, and S. Fidler, “Neural geometric level of
detail: Real-time rendering with implicit 3D shapes,” in IEEE Conference
on Computer Vision and Pattern Recognition, CVPR, 2021, pp. 11 358–
11 367.

[12] L. Liu, J. Gu, K. Z. Lin, T. Chua, and C. Theobalt, “Neural sparse voxel
ﬁelds,” in Advances in Neural Information Processing Systems, NeurIPS,
2020.

[13] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger, “Real-time 3D
reconstruction at scale using voxel hashing,” ACM Trans. Graph., vol. 32,
no. 6, pp. 169:1–169:11, 2013.

Fig. 16. Example of multiple object composition.

and complete surface and preserve the details. We also show
that our representation can learn the precise color on the surface
in Figure 12. The problems with COLMAP and TSDF fusion are
mainly caused by the changes of light in outdoor scenes and the
inaccuracy of pose. The expression of Vox-Surf can effectively
alleviate these problems through implicit constraints.

5 APPLICATIONS
The proposed Vox-Surf representation is more conducive to editing
and combining objects and scenes. Since each voxel contains the
part of the surface and its appearance, we can edit the surface by
manipulating the explicit voxels. As shown in Figure 13, the ﬁrst
column is the original voxels and their rendering result, based on
these voxels. We apply scaling, translation and duplication to the
selected voxels, and the real texture image are displayed on the
bottom line respectively. This is especially useful for interactive
3D editing. Similarly, we can render the occlusion effect between
objects by combining the voxels of multiple objects as shown
in Figure 16. Our method can also support the simultaneous
training of multiple objects by placing multiple objects in one
scene. With the gradual reﬁnement of the voxels, we can separate
the voxels corresponding to each object, thereby obtaining multiple
individual objects that share the common extraction network.
We also demonstrate the use of Vox-Surf in physics simulations
in Figure 14. We can treat the voxels as bounding volumes for
3D objects, which is more effective than other collision detection
methods. By simulating the external and internal collisions of the
generated voxels, we can also obtain a realistic image sequence of
objects colliding, which have various uses for visual effects.

This procedure can also be applied to the interaction of
virtual objects in AR environment (Figure 15). With the pre-build
scenes and objects in Vox-Surf representation, we can realize the
combination of scenes and objects with rigid body constraints.
Similarly, it can also be used for grasping or other interactions of
virtual objects.

6 LIMITATIONS AND FUTURE WORKS
Our Vox-Surf representation still has some limitations. Firstly, our
method cannot disentangle the intrinsic and extrinsic properties of

[39] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron,
R. Martin-Brualla, N. Snavely, and T. A. Funkhouser, “Ibrnet: Learning
multi-view image-based rendering,” in IEEE Conference on Computer
Vision and Pattern Recognition, CVPR, 2021, pp. 4690–4699.

12

[14] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski, “A
comparison and evaluation of multi-view stereo reconstruction algorithms,”
in IEEE Conference on Computer Vision and Pattern Recognition, CVPR,
2006, pp. 519–528.

[15] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for
3D object reconstruction from a single image,” in IEEE Conference on
Computer Vision and Pattern Recognition, CVPR, 2017, pp. 2463–2471.
[16] C. Lin, C. Kong, and S. Lucey, “Learning efﬁcient point cloud gener-
ation for dense 3D object reconstruction,” in Conference on Artiﬁcial
Intelligence, AAAI, 2018, pp. 7114–7121.

[17] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, “3D-R2N2: A
uniﬁed approach for single and multi-view 3D object reconstruction,” in
European Conference on Computer Vision, ECCV, vol. 9912, 2016, pp.
628–644.

[18] H. Xie, H. Yao, X. Sun, S. Zhou, and S. Zhang, “Pix2Vox: Context-aware
3D reconstruction from single and multi-view images,” in IEEE/CVF
International Conference on Computer Vision, ICCV, 2019, pp. 2690–
2698.

[19] M. M. Kazhdan, M. Bolitho, and H. Hoppe, “Poisson surface reconstruc-
tion,” in Eurographics Symposium on Geometry Processing, vol. 256,
2006, pp. 61–70.

[20] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y. Jiang, “Pixel2mesh:
Generating 3D mesh models from single RGB images,” in European
Conference on Computer Vision, ECCV, vol. 11215, 2018, pp. 55–71.

[21] H. Kato, Y. Ushiku, and T. Harada, “Neural 3D mesh renderer,” in IEEE
Conference on Computer Vision and Pattern Recognition, CVPR, 2018,
pp. 3907–3916.

[22] H. Li, W. Ye, G. Zhang, S. Zhang, and H. Bao, “Saliency guided subdivi-
sion for single-view mesh reconstruction,” in International Conference on
3D Vision, 3DV, 2020, pp. 1098–1107.

[23] V. Sitzmann, M. Zollh¨ofer, and G. Wetzstein, “Scene representation
networks: Continuous 3D-structure-aware neural scene representations,”
in Advances in Neural Information Processing Systems, NeurIPS, 2019,
pp. 1119–1130.

[24] R. Mur-Artal, J. M. M. Montiel, and J. D. Tard´os, “ORB-SLAM: A
versatile and accurate monocular SLAM system,” IEEE Trans. Robotics,
vol. 31, no. 5, pp. 1147–1163, 2015.

[25] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 611–625, 2018.
[26] S. Laine and T. Karras, “Efﬁcient sparse voxel octrees,” IEEE Trans. Vis.

Comput. Graph., vol. 17, no. 8, pp. 1048–1059, 2011.

[27] N. L. Max, “Optical models for direct volume rendering,” IEEE Trans.

Vis. Comput. Graph., vol. 1, no. 2, pp. 99–108, 1995.

[28] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
boltzmann machines,” in International Conference on Machine Learning,
ICML, 2010, pp. 807–814.

[29] A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman, “Implicit
geometric regularization for learning shapes,” in International Conference
on Machine Learning, ICML, vol. 119, 2020, pp. 3789–3799.

[30] K. Deng, A. Liu, J. Zhu, and D. Ramanan, “Depth-supervised nerf: Fewer
views and faster training for free,” CoRR, vol. abs/2107.02791, 2021.
[31] K. Rematas, A. Liu, P. P. Srinivasan, J. T. Barron, A. Tagliasacchi,
T. A. Funkhouser, and V. Ferrari, “Urban radiance ﬁelds,” CoRR, vol.
abs/2111.14643, 2021.

[32] R. R. Jensen, A. L. Dahl, G. Vogiatzis, E. Tola, and H. Aanæs, “Large
scale multi-view stereopsis evaluation,” in IEEE Conference on Computer
Vision and Pattern Recognition, CVPR, 2014, pp. 406–413.

[33] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and
M. Nießner, “Scannet: Richly-annotated 3D reconstructions of indoor
scenes,” in IEEE Conference on Computer Vision and Pattern Recognition,
CVPR, 2017, pp. 2432–2443.

[34] Y. Liao, J. Xie, and A. Geiger, “KITTI-360: A novel dataset and
benchmarks for urban scene understanding in 2D and 3D,” CoRR, vol.
abs/2109.13410, 2021.

[35] A. Dai, M. Nießner, M. Zollh¨ofer, S. Izadi, and C. Theobalt, “Bundle-
fusion: Real-time globally consistent 3D reconstruction using on-the-ﬂy
surface reintegration,” ACM Trans. Graph., vol. 36, no. 3, pp. 24:1–24:18,
2017.

[36] D. Azinovic, R. Martin-Brualla, D. B. Goldman, M. Nießner, and J. Thies,
“Neural RGB-D surface reconstruction,” CoRR, vol. abs/2104.04532, 2021.
[37] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox,
“What do single-view 3D reconstruction networks learn?” in IEEE
Conference on Computer Vision and Pattern Recognition, CVPR, 2019,
pp. 3405–3414.

[38] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural radiance
ﬁelds from one or few images,” in IEEE Conference on Computer Vision
and Pattern Recognition, CVPR, 2021, pp. 4578–4587.

