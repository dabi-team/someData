A Novel Hand Gesture Detection and Recognition system
based on ensemble-based Convolutional Neural Network

Abir Sen · Tapas Kumar Mishra · Ratnakar Dash

2
2
0
2

b
e
F
5
2

]

V
C
.
s
c
[

1
v
9
1
5
2
1
.
2
0
2
2
:
v
i
X
r
a

Abstract Nowadays, hand gesture recognition has be-
come an alternative for human-machine interaction. It
has covered a large area of applications like 3D game
technology, sign language interpreting, VR (virtual re-
ality) environment, and robotics. But detection of the
hand portion has become a challenging task in com-
puter vision and pattern recognition communities. Deep
learning algorithm like convolutional neural network
(CNN) architecture has become a very popular choice
for classiﬁcation tasks, but CNN architectures suﬀer
from some problems like high variance during predic-
tion, overﬁtting problem and also prediction errors. To
overcome these problems, an ensemble of CNN-based
approaches is presented in this paper. Firstly, the ges-
ture portion is detected by using the background sepa-
ration method based on binary thresholding. After that,
the contour portion is extracted, and the hand region
is segmented. Then, the images have been resized and
fed into three individual CNN models to train them
in parallel. In the last part, the output scores of CNN
models are averaged to construct an optimal ensemble
model for the ﬁnal prediction. Two publicly available
datasets (labeled as Dataset-1 and Dataset-2) contain-
ing infrared images and one self-constructed dataset
have been used to validate the proposed system. Exper-

Abir Sen
Department of Computer Science and Engineering, National
Institute of Technology, Rourkela 769008, India
E-mail: tarusince92@gmail.com; 518cs1007@nitrkl.ac.in

Tapas Kumar Mishra
Department of Computer Science and Engineering, National
Institute of Technology, Rourkela 769008, India
E-mail: mishrat@nitrkl.ac.in

Ratnakar Dash
Department of Computer Science and Engineering, National
Institute of Technology, Rourkela 769008, India
E-mail: ratnakar@nitrkl.ac.in

imental results are compared with the existing state-of-
the-art approaches, and it is observed that our proposed
ensemble model outperforms other existing proposed
methods.

Keywords Deep Learning · Hand Gesture Recogni-
tion · Hand Detection · Contour Extraction · Gesture
Segmentation · Ensemble Learning

1 Introduction

From the past few years, gesture recognition has boosted
human-computer interaction (HCI) to facilitate inter-
action between human beings and computer devices.
Nowadays, virtual reality is appearing in people’s daily
life, and undoubtedly it will be the mainstream of human-
machine interaction in the future [1]. The principal rea-
son for this is the low-cost availability of high-performing
sensors. Microsoft Kinect V2 provides infrared, depth
image, RGB color, and skeleton information from the
captured scenes. Intel Realsense camera provides infor-
mation about depth, RGB and, skeleton data, whereas
a Leap Motion controller device can capture the in-
frared and skeleton data. Most of the works have been
performed by using the Leap Motion controller to im-
prove human-machine interaction. For example, in [2],
the authors have proposed a method for controlling the
robot arm using the Leap Motion controller. Most re-
searchers perform hand gesture recognition in real-time
by analyzing hand-skeleton information provided by the
Leap Motion device. All works along these lines are two-
staged: the ﬁrst stage is to compute feature descriptors
then use any classiﬁer to recognize the hand gestures.
In [3], it is shown that skeleton-based nodes are used as
feature descriptors, and then they are fed into Hidden
Conditional Neural Field (HCNF) classiﬁer for classify-
ing hand gestures. In [4], the authors have proposed a

 
 
 
 
 
 
2

Abir Sen et al.

new approach to recognize gestures for infrared images
collected by using Leap Motion device, which has three
stages: (1) the system builds image descriptor without
any segmentation phase; (2) the resultant descriptor is
dimensionally reduced by using compressive sensing al-
gorithm; and (3) the reduced feature vectors are fed into
support vector machine (SVM) classiﬁer for ﬁnal classi-
ﬁcation. In [5], the authors have proposed the American
sign language recognition system with Leap Motion de-
vice, where k-nearest neighbor and SVM algorithms are
used to classify 26 English Alphabet letters. In [6], they
have used the Gabor feature extractor and SVM clas-
siﬁer for performing gesture recognition tasks after the
segmentation phase.
In most cases, the stages of the hand gesture recognition
system are bounded to gesture detection and classiﬁca-
tion. But most of the approaches need to extract the
features before classifying them. With the rapid growth
in deep learning and computer vision, the researchers
have chosen the CNN model for both feature extraction
and classiﬁcation and have achieved better classiﬁcation
results.
In [7], a hand gesture recognition system is proposed
based on canny edge detection for data pre-processing
and CNN architecture to extract the features automat-
ically. In [8], they have proposed a continuous hand ges-
ture recognition system which is having two modules (1)
ﬁrst is the segmentation phase to segment the gestures;
(2) the second is a recognition module to recognize the
gestures using CNN. In [9], they have applied CNN ar-
chitecture to classify hand gestures as feature extractors
and then to optimize the classiﬁcation process, and they
have used SVM to improve the performance. In [10],
they have proposed a hand gesture recognition system
based on CNN, and to solve the overﬁtting problem,
deep convolutional generative adversarial network (DC-
GAN) is applied. They have trained on fewer samples
and achieved better results. In [11], they have detected
the hand region portion from the whole image. After
segmentation of the ﬁngertips from the gesture images,
they are fed into CNN classiﬁer to classify the gesture
images, and their proposed methodology had achieved
96.2% of recognition rate. In [12], they have presented
a CNN model-based hand gesture recognition system
for unmanned aerial vehicles (UAV) ﬂight controls af-
ter collecting skeletal data by using the Leap motion
controller, and their proposed work has achieved a ges-
ture recognition rate of 97%.
But there are some limitations of using CNN archi-
tectures during classifying gestures due to high vari-
ance during predictions, which leads to overﬁtting in
the model. So there is an eﬃcient way to reduce the
variance, is to train multiple CNN models and then av-

eraging the outputs from those models. This method
is called as the ensemble learning method, which can
eﬀectively minimize variance problem and can produce
some good classiﬁcation results.
In this paper, we have presented a hand gesture recog-
nition based on ensemble-based CNN model. The whole
proposed work has three stages: (1) hand detection by
background separation; (2) contour portion extraction
and hand region segmentation; (3) classiﬁcation of the
hand gesture images by training three individual CNN
architectures in parallel and then averaging the output
scores of these models to build ﬁnal ensemble model.
The remainder of this paper is organized as follows.
The details of our proposed methodology are described
in Section 2. The experimental details, dataset descrip-
tion, experimental results, and comparison with other
schemes are described in Section 3. Finally conclusion
part is presented in Section 4.

2 Method overview

Our proposed method contains some important stages
like gesture detection, binary thresholding, contour se-
lection, hand portion extraction, resizing, fed the re-
sized images into individual CNN models, evaluate the
performance of three custom CNN architectures (GoogLeNet-
like, AlexNet-like, VGGNet-like), then build an opti-
mal ensemble model for classifying gesture images. The
block diagram of our proposed framework is illustrated
in Fig. 4. All the phases are discussed in this section.

2.1 Image pre-processing

Image pre-processing is a signiﬁcant phase to facili-
tate the model. This phase contains essential stages like
gesture detection by background subtraction, binary
thresholding, contour region extraction, hand portion
segmentation, applying the median ﬁlter, and resizing
of images before being fed into the CNN classiﬁer. In
Fig. 3, the whole pre-processing phase of generating
resized images is shown. The diﬀerent pre-processing
steps are discussed below.

2.1.1 Gesture detection by background subtraction

In image pre-processing, detection is a phase that iden-
tiﬁes where the object is in the image and the bound-
aries. In our work, the main objective of gesture detec-
tion is to remove the unwanted background portion to
extract the gesture portion. To detect the hand region,
we have applied binary thresholding technique to seg-
ment between the background and foreground region,

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

3

2.1.3 Resizing

Large images require more memory space and time than
smaller images to be trained in the deep learning model.
So in our work, after the gesture detection, contour ex-
traction, and hand portion extraction phase, the images
are resized into an aspect ratio of (64× 64) before being
fed into CNN models for both extracting features and
classiﬁcation.

2.2 Ensemble model for classifying gestures

We have designed an optimal ensemble based CNN model
by averaging the output scores from three CNN archi-
tectures. We have constructed our self-designed CNN
models based on GoogLeNet [15], VGGNet [16], and
AlexNet [17] architectures. As we know that the CNN
architectures like GoogLeNet, VGGNet provide power-
ful performance by taking lots of parameters for com-
puting compared to other CNN architectures. In this
section, we will discuss some self-designed CNN archi-
tectures: AlexNet-like, VGGNet-like, and GoogLeNet-
like and then ensemble model.

AlexNet-like: Classifying images from datasets like
CIFAR10 and CIFAR100, is a simple task in the deep
learning era. However, when there is the availability of
a large dataset like ImageNet [18] consisting of millions
of images, it will become a very challenging task. To
train millions of labeled images, AlexNet [17], a pow-
erful CNN architecture, was proposed to extract the
features by running the model on CUDA GPUs. It con-
sists of eight layers, ﬁve are convolutional, and 3 are
fully connected layers and have max-pooling layers fol-
lowed by the Relu activation function.
In our experiment, we have optimized AlexNet architec-
ture into self-designed CNN architecture called AlexNet-
like by reducing the input image size to (64 × 64) after
the pre-processing phase. We have optimized the inter-
nal settings of the models, such as the number of ﬁlters,
depth of architectures, which enhances the model per-
formance by reducing the number of parameters. The
model performance has been increased after reducing
the number of parameters. The proposed CNN archi-
tecture is graphically illustrated in Fig. 5 and the model
performance is shown in Table 3

VGGNet-like: VGGNet architecture [16] is a very pow-
erful architecture that increases the depth of networks
by adding convolution layers of ﬁlter size (3 × 3), max-
pooling layers, batch-normalization layers, and dense
layers and improves the model accuracy by adding more
convolutional and dropout layers and it has achieved

Fig. 1: Real-time hand detection.

which makes the hand portion white in color and the
background portion black in color. An example of real-
time hand detection has been presented in In Fig. 1.

2.1.2 Contour portion and Hand region extraction

Contour region represents the boundary or outline of an
object which is present in an image. The contour por-
tion, having the largest area, is assumed to be the hand
region. The center of hand region (palm center) is es-
timated by using distance transform [13], [14] method.
According to this transform method, the pixel point
in the contour region portion with having maximum
value is considered as palm center. The palm radius
is calculated by calculating the distance between the
palm center and the point outside the contour region
portion. In this way, the hand region is extracted from
the arm portion. Fig. 2 describes the whole procedure
about contour portion selection and hand region seg-
mentation. After extraction of the hand region portion,
they are resized before fed into the CNN classiﬁer for
training.

Fig. 2: Hand region extraction process (A) Con-
tour region enclosed by bounded box, where red
point represents palm center, (B) extraction of
hand region portion, (C) resizing of images be-
fore fed into CNN classiﬁer.

4

Abir Sen et al.

Fig. 3: The whole pre-processing phase of generating hand region extraction before fed into CNN
architecture (A) Gesture detection by background separation, (B) Transform to gray scale, (C)
Binary thresholding, (D) Conrour region selection, (E) Hand portion extraction, (F) Resizing of
images before applying median ﬁlter for noise removal.

Fig. 4: Block diagram of our proposed framework for gesture classiﬁcation.

Fig. 5: AlexNet-like architecture (One of the 3 self-constructed CNN architectures) used in our
proposed technique.

92.7% top-5 test accuracy in the ILSVRC-2012 clas-
siﬁcation challenge. This architecture can be used as
transfer learning for large-scale datasets.
Similarly, in our experiment, like AlexNet-like architec-
ture, we have proposed self-designed VGGNet-like ar-

chitecture, optimized by decreasing the depth of the
network. After optimization, the number of parame-
ters has been decreased, which makes the model easily
trainable. The detailed conﬁguration of our proposed
VGGNet-like architecture for Dataset-1 and the num-

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

5

Fig. 6: GoogLeNet-like architecture structure.

ber of trainable parameters at each layer are shown in
Table 1. The model performance is shown in Table 3.
In Table 3, we have shown the total number of param-
eters of VGG16 architecture on Dataset-1, Dataset-2,
and self-constructed dataset.

GoogLeNet-like: GoogLeNet [15] provides new inspi-
ration to develop high-capability deep learning archi-
tecture. It is a very powerful CNN architecture based
on the inception module. In case of inception module,
the architecture is restricted by only (1 × 1), (3 × 3),
(5 × 5) ﬁlters. In this architecture, (1 × 1) convolution
ﬁlter is used before (3 × 3), (5 × 5) convolutional ﬁlters.
After that we have concatenated all (1 × 1), (3 × 3),
(5 × 5) convolutional ﬁlters to perform convolution on
the output coming from the previous layer. Inception
module also contains one max-pooling layer, then the
outputs of all three ﬁlters are concatenated (known as
ﬁlter concatenation) and passed into the next layer as
input.
In our experiment, we have built our custom CNN ar-
chitecture called GoogLeNet-like architecture inspired
by GoogLeNet. We have established an Inception ar-
chitecture while optimizing the number of ﬁlters and
layers. Hence, the model becomes easily trainable by
reducing the number of parameters. The structure of
this architecture is shown in Fig. 6 and the model per-
formance is shown in Table 3.

Ensemble model: Ensemble learning is a popular ma-
chine learning technique that combines diﬀerent learn-
ing models to reduce prediction errors and enhances
model accuracy [19], [20]. It is well known for being
accurate and more robust than any individual CNN
model. In our experiment, we have used the parallel
ensemble model as it has two types of methods, i.e.,
sequential and parallel. The technique uses three inde-
pendent models in parallel, and then the model per-
formance is improved by averaging the output scores

of these three heterogeneous models. Then this average
score is used to predict the ﬁnal gesture label. The steps
followed to build the ensemble model are listed in Algo-
rithm 1. Fig. 7 shows the ﬂow diagram, how the output
scores from three individual models (GoogLeNet-like,
VGGNet-like, and AlexNet-like) are averaged, and to
build ﬁnal ensemble model for the classiﬁcation task.

Algorithm 1 Ensembling method

INPUT: Training data I=(xi, yi), where i← 1 to x, where
x is number of training samples.
C is the number of classiﬁers.
OUTPUT: Ensemble classiﬁer E.
step 1: Learn the base classiﬁers.
for k ← 1 to C do

Build the independent classiﬁers based on I.

end for
step 2: Do the averaging of output scores from C number
of models to learn the ﬁnal classiﬁer E.
step 3: Utilize the classiﬁer E for classiﬁcation and ﬁnal
prediction.

Fig. 7: Ensemble model.

6

Abir Sen et al.

Table 1: Detail conﬁguration of our proposed VGGNet-Like architecture for Dataset-1.

No of ﬁlters Kernel size

—
64
—
64
—
—
128
—
128
—
—
256
—
256
—
256
—
—
512
—
512
—
512
—
—
—
—
—
—
—

—
3 × 3 × 64
—
3 × 3 × 64
—
2 × 2
3 × 3 × 128
—
3 × 3 × 128
—
2 × 2
3 × 3 × 256
—
3 × 3 × 256
—
3 × 3 × 256
—
2 × 2
3 × 3 × 512
—
3 × 3 × 512
—
3 × 3 × 512
—
2 × 2
—
—
—
—
—

Stride
—
1
—
1
—
2
1
—
1
—
2
1
—
1
—
1
—
2
1
—
1
—
1
—
2
—
—
—
—
—

Output shape
64 × 64 × 1
64 × 64 × 64
64 × 64 × 64
64 × 64 × 64
64 × 64 × 64
32 × 32 × 64
32 × 32 × 128
32 × 32 × 128
32 × 32 × 128
32 × 32 × 128
16 × 16 × 128
16 × 16 × 256
16 × 16 × 256
16 × 16 × 256
16 × 16 × 256
16 × 16 × 256
16 × 16 × 256
8 × 8 × 256
8 × 8 × 512
8 × 8 × 512
8 × 8 × 512
8 × 8 × 512
8 × 8 × 512
8 × 8 × 512
4 × 4 × 512
8198
512
512
10
—
Total parameters

No of parameters
0
640
256
36,928
256
0
73,856
512
147,584
512
0
295,168
1024
590,080
1024
590,080
1024
0
1,180,160
2048
2,359,808
2048
2,359,808
2048
0
0
4,194,816
262,656
5130
0
12,107,466

layer type
Input image
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Max Pooling
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Max Pooling
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Max Pooling
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Convolution 2D
Batch Normalization
Max Pooling
Flatten
Dense
Dense
Dense
Output

3 Experiment

3.1 Datasets

Two publicly available gesture image datasets, hand
gesture dataset [4] (labeled as Dataset-1), multi-modal
hand gesture dataset [23] (labeled as Dataset-2), and
one self-constructed dataset have been used to evalu-
ate our proposed model. Dataset-1 contains 20,000 im-
ages of 10 distinct gestures (Palm, L, Fist, Fist move,
Thumb, Index, Ok, Palm move, Close, and Palm down)
performed by ten diﬀerent people (ﬁve men and ﬁve
women). A total number of 200 images are recorded per
gesture, So each person, there will be 200 × 10 = 2, 000
images. Here each infrared image has a resolution of
640 × 240, in Fig. 8, we have shown some sample im-
ages from Dataset-1.

Fig. 8: Hand gesture data samples.

Dataset-2 contains 16 diﬀerent gestures (Close, L,
Palm down, Fist move, Five, Four, Hang, Heavy, In-
dex, Ok, Palm, Palm move, Palm Up, Three, Two, and
Up) performed by 25 people (17 men and eight women),
where we have chosen only static gesture images after

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

7

considering each static gesture has one kind of hand
pose. Here all the gesture images have a non-identical
resolution.
In order to validate the proposed method, one self-
constructed dataset containing 15,000 binary images
captured by webcam has been used. This dataset con-
tains 14 diﬀerent gestures (Close, Fist, Five, Four, Hang,
Heavy, Index, L, Ok, Palm move, Three, Thumb, two,
Palm) performed by three people (two men and one
woman). Some self-constructed sample images are shown
in Fig. 9.

Fig. 9: Self-constructed dataset samples.

3.2 Evaluation protocol

In our work, we have randomly split datasets into 80%
for training and 20% for testing. Further, the training
set is split into 75% for training and 25% for valida-
tion. Now in the dataset, 60% is training images, 20%
is validation, and the remaining 20% is testing images.
The training image sets are used to ﬁt the model and
to study model parameters; validation image sets are
used to estimate prediction errors during model selec-
tion, and testing images are used to evaluate the per-
formance of the proposed ensemble model.

3.3 Experimental details

For training our model, in the pre-processing phase,
we perform binary thresholding to detect the gesture
portion after removing the background section. After
thresholding, we have extracted the contour regions in
the resulting image, the region with the largest area
represents the hand portion. Then median ﬁlter [21] has
been applied on the hand contour region to remove the
noise. After that, the images are resized into a ﬁxed as-
pect ratio of 64×64 and augmented before fed into three
individual models (GoogLeNet-like, VGGNet-like, and
AlexNet-like) for training. The augmentation includes
rotations, zooming, horizontal and vertical-shift, and

ﬂipping during training of the model. While training
each model, we have used softmax activation function,
categorical cross entropy as loss function and Adam op-
timizer [22] with an initial learning rate of 0.0001 and
default values of β1 =0.9, β2=0.999. We have trained
the training samples for 30 epochs with batch-size 160.
Then the output scores of three CNN models are av-
eraged to provide the ﬁnal gesture label. Here we have
used the model averaging ensemble method by using the
“Average” class in Keras deep learning framework. The
optimal values of parameters and hyper-parameters of
three self-designed CNN models are listed in Table 2
and the model diagram is shown in Fig. 4. All of our
experiments have been performed on a system with In-
tel Core i7-10750H CPU, 16GB RAM having NVIDIA
GeForce GTX 1650 GPU, and the whole program of our
proposed model is developed under Keras deep learning
framework.

Table 2: Summary of hyper-parameter settings.

Parameter
Input size
Epochs
Batch size
Learning rate
Pooling
Dropout
Optimizer
Activation function
Error Function

Value
64 × 64
30
160
0.0001
2 × 2
0.2
Adam
ReLU
Categorical cross entropy

3.4 Transfer learning method

Transfer learning is one of the most eﬀective ways to
classify large-scale datasets. It is considered to be fast
and more straightforward than build a CNN architec-
ture from scratch. In our work, we have considered some
well-known pre-trained CNN architectures like VGG16
[16], AlexNet [17], and GoogLeNet [15] and then applied
ﬁne-tuning on them on the considered three datasets
by using transfer learning. For this, we have consid-
ered the input shape of (64 × 64 × 1) and the ﬁnal
FC (fully connected) layer of each CNN architecture
have been replaced with a new FC (fully connected)
layer containing n nodes, where n represents the num-
ber of classes of gesture images in Dataset-1, Dataset-2,
and self-constructed dataset, respectively. In Table 3,
these transfer learning based models are referred to as
VGG16-TL, AlexNet-TL, and GoogLeNet-TL, respec-
tively. In transfer learning, the CNN architectures are

8

Abir Sen et al.

ﬁne-tuned by using Adam optimizer with learning rate
0.0001 and default values of β1=0.9, β2=0.999. Here we
have trained the samples for 30 epochs with batch-size
160.

3.5 Results

In this section, we have discussed the results from our
every experiment over Dataset-1, Dataset-2, and self-
constructed dataset by using our proposed model and
then compared our model with several suitable methods
in terms of confusion matrix, gesture recognition rate
(%), and accuracy. In our experiment, each column of
the confusion matrix represents the actual gesture la-
bel, and the corresponding row represents the predicted
gesture label.

Hand gesture classiﬁcation accuracy is deﬁned as:

Accuracy = Number of correctly classiﬁed gesture images

Total number of testing samples

× 100

Table 3 shows the testing results obtained by using
diﬀerent CNN architectures over Dataset-1, Dataset-
2, and self-constructed dataset. Each CNN model has
been trained using diﬀerent input size of (32 × 32),
(64 × 64) and (128 × 128). It is observed that the input
image size of (64 × 64) gives better classiﬁcation results
compared to other input image size.
The transfer learning results by using three pre-trained
CNN models over three datasets have been listed in
Table 3. It is observed that our proposed self-designed
CNN architectures VGGNet-like, AlexNet-like, and
GoogLeNet-like architectures require less number of pa-
rameters and have faster training time (in sec) com-
pared to transfer learning based models such as VGG16-
TL, AlexNet-TL, and GoogLeNet-TL.
It is found in Table 3 that our proposed ensemble model
outperforms other CNN architectures and gives promis-
ing results in terms of gesture recognition accuracy(%).
Fig. 10 shows the model accuracy and losses for individ-
ual models (i.e., AlexNet-like, GoogLeNet-like, VGGNet-
like) and ensemble model respectively on Dataset-2 with
respect to the number of epochs. In Fig. 10(D), it is
observed that the ensemble model has achieved an im-
proved gesture recognition performance compared to in-
dividual CNN models.

The analysis of the confusion matrix, shown in Ta-
bles 4, 5 and 6, are carried out by using the proposed
ensemble model. The analysis indicates some gestures,
which produce misclassiﬁcation errors. For instance,
“Palm-move” and “Palm” in Dataset-1 both are quite
similar from viewing positions leading to a misclassiﬁca-
tion rate of 0.2%. As an example, shown in Table 4, that
“Palm move” is misclassiﬁed as “Palm” at a recognition

rate of 0.2%. The reason for this misclassiﬁcation may
occur due to the data augmentation technique making
two gestures look similar. For another example shown
in Table 5, that “Fist move” is misclassiﬁed as “Heavy”
and “Up” at a recognition rate of 0.5% and 1%. A sim-
ilar situation also happens for gestures “Five”, “Palm”,
“Three”, “Two” and also “Up”. Similarly, in Table 6,
it is observed that gesture “Thumb” is misclassiﬁed as
“Five” and “Four” at a recognition rate of 0.3% and
1.1%. Some misclassiﬁed testing samples from Dataset-
1, Dataset-2, and our self-constructed dataset are illus-
trated in Fig. 11.

3.6 Comparison with other schemes

To show the eﬀectiveness our proposed model, we have
compared our model with other existing schemes shown
by [4] and [23] tabulated in Tables 7 and 8. It is ob-
served in Table 7, that our proposed work has achieved
a better gesture recognition rate (in terms of accuracy
results) than the technique proposed by Mantec´on T
[4] over Dataset-1. In Table 8, it is observed that the
proposed work by [4] used a combination of feature de-
scriptor and SVM to perform classiﬁcation of gestures
at a recognition rate of 99.0% and is evaluated on leap
motion sensor captured infrared images containing ten
gesture classes.

The null hypothesis, in this case, is: the model is
not statistically signiﬁcant. To check the statistical rele-
vance in our experiment, we have performed one sample
t-test by using IBM SPSS statistical analysis software.
To get the value of t, the following formula has been
used:

t = (X−µ)

SD√
n

where, X mean of the samples
µ the test value
SD sample standard deviation
n sample size

To calculate the value of X, we have split Dataset-1
into ten parts and then evaluated the testing accuracy
for each part, followed by calculating the mean (consid-
ered as sample mean) of these accuracy values with the
ensemble model 2.2.
As shown in Table 9, the sample mean value (X) is
99.821, and the standard deviation (SD) is 0.3088. The
number of samples (sample size) is 10, and the test
value (µ) is 99. The results of the one-sample test are
shown in Table 10, where the test statistic of the one-
sample t-test is denoted as t. Here t = 8.405, t is calcu-
lated by using the above mentioned formula.
The results show that p value < 0.001, where p value is

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

9

Fig. 10: Epoch-wise plots of accuracy and loss for individual models: Graphs A to D show the
accuracy (training and validation) plots of the custom CNN models (i.e. AlexNet-like, GoogLeNet-
like, VGGNet-like) and ensemble model, respectively. E to H show the loss occurs in case of each
individual model and ensemble model for Dataset-2.

10

Abir Sen et al.

Table 3: Gesture recognition performance comparison results among diﬀerent CNN models on
Dataset-1, Dataset-2 and self-constructed dataset.

Model Structure

Basic CNN
VGG16-TL
AlexNet-TL
GoogLeNet-TL
VGGNet-like
AlexNet-like
GoogLeNet-like
Ensemble model
(proposed)

Parameters

Dataset-1
(containing 10 gesture classes)
Training
Time (s)
147
325
220
280
273
150
185

Accuracy
(%)
94.72
99.65
99.14
99.60
99.65
99.36
99.50

198,474
39,928,522
28,810,690
7,223,540
12,107,466
2,464,842
5,670,392

Parameters

Dataset-2
(containing 16 gesture classes)
Training
Time (s)
635
1120
840
900
1,020
660
780

Accuracy
(%)
82.79
95.65
94.00
93.64
95.94
94.21
93.38

237,136
39,953,104
28,816,696
7,236,146
12,110,544
2,466,384
5,670,698

Parameters

Self-constructed dataset
(containing 14 gesture classes)
Training
Time (s)
143
280
150
180
240
148
190

Accuracy
(%)
83.10
99.30
98.43
97.60
99.53
98.53
97.20

236,110
39,944,910
28,814,694
7,231,944
12,109,518
2,465,870
5,670,596

20,242,700

215

99.80

20,247,626

790

96.50

20,245,984

195

99.70

Table 4: Confusion Matrix for gesture recognition rate (%) using proposed ensemble model on
Dataset-1.

Predicted
label

Palm
L
Fist
Fist move
Thumb
Index
Ok
Palm move
Close
Palm down

Palm
100.0
0
0
0
0
0
0
0
0
0

L
0
100.0
0
0
0
0
0
0
0
0

Fist
0
0
100.0
0
0
0
0
0
0
0

Actual label

Fist move Thumb

0
0
0
100.0
0
0
0
0
0
0

0
0
0
0
100.0
0
0
0
0
0

Index
0
0
0
0
0
100.0
0
0
0
0

OK
0
0
0
0
0
0
98.6
1.4
0
0

Palm move
0.2
0
0
0
0
0
0
99.8
0
0

Close
0
0
0
0
0
0
0
0
100.0
0

Palm down
0
0
0
0
0
0
0
0
0
100.0

*Bold value represents the maximum recognition rate(%) in each column

Fig. 11: Visualization of some falsely classi-
ﬁed images in Dataset-1, Dataset-2 and self-
constructed dataset.

Fig. 12: Performance of the proposed ensemble
model on some testing samples from Dataset-1,
Dataset-2 and our custom dataset.

used in hypothesis testing, to determine whether there
is evidence to reject the null hypothesis by using calcu-
lated probability.

If p < α where, α (signiﬁcance level) = 0.05, then
null hypothesis is rejected. In Table 10, it is observed
that p-value is less than α, where α = 0.05. So null

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

11

Table 5: Confusion Matrix for gesture recognition rate (%) using proposed ensemble model on
Dataset-2.

Actual label

Five

Four Hang Heavy

Index

Predicted label

Close
L
Palm
down
Fist
move
Five
Four
Hang
Heavy
Index
Ok
Palm
Palm
move
Palm
Up
Three
Two
Up

Close

L

99.6
0

0
99.5

Palm
down
0
0.1

Fist
move
0.1
0

0
0

0

0

0
0.1
0
0
0.1
0.1
0.1

0

0

0
0
0

0

0

0
0.2
0
0
0.2
0
0

0

0

0.1
0
0

96.6

0.1

2.0

0.1

0
0
0
0
0.2
0
1.4

0.6

0

0
0
1.0

98.3

0

0
0
0
0.5
0
0
0

0

0

0
0
1.0

96.6
0.7
0
0
0
0.5
0.7

0

0

0
0.1
0

0
0

0

0

0
99.9
0
0
0
0
0

0.1

0

0
0
0

0
0.2

0.2

0

0
0
99.5
0.1
0
0
0

0

0

0
0
0

0
0

0

0

0
0
0
99.8
0.1
0
0

0

0.1

0
0
0

0
0.4

0

0.1

0
0
0
0
99.3
0
0

0

0

0
0.2
0

Three Two

Up

Ok

1.6
0

0

0

0.1
0
0
0
0
96.0
0

2.3

0

0
0
0

Palm

0
0

1.3

0

0.1
0
0
0
0
0
97.1

1.2

0.3

0
0
0

Palm
move
0
0

Palm
Up
0
0

0

0

0
0
0
0
0
0
2.0

96.0

0

0.1

0.3
0
0
0
0
0
1.0

2.8

2.0

94.8

0
0.1

0

0

0
4.1
0
0.1
0
0.2
0

0

0

0
0.1

0

0

0
0
0.2
0
0
0
0

0

0

0
0
0

0
0
1.0

95.2
0.3
0

1.1
98.6
0

0
0

1.3

0.3

0
0
0
0
0
0
0

0.2

3.0

0
0
95.2

*Bold value represents the maximum recognition rate(%) in each column

Table 6: Confusion Matrix for gesture recognition rate (%) using proposed ensemble model on
self-constructed dataset.

Close

Fist

Five

Four

Hang Heavy

Index

L

Ok

Actual label

Predicted label

0
99.6
0.4
0
0
0
0
0
0
0
0
0
0
0
*Bold value represents the maximum recognition rate(%) in each column

Close
Fist
Five
Four
Hang
Heavy
Index
L
Ok
Palm move
Three
Thumb
Two
Palm

100.0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
100.0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
100.0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
100.0
0
0
0
0
0
0
0

0
0
98.9
1.1
0
0
0
0
0
0
0
0
0
0

0
0
2.1
97.9
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
100.0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
100.0
0
0
0
0
0

Palm
move
0
0.1
0
0
0
0
0
0
0
99.9
0
0
0
0

Three Thumb Two

Palm

0
0
0
0
0
0
0
0
0
0
100.0
0
0
0

0
0
0.3
1.1
0
0
0
0
0
0
0
98.6
0
0

0
0.4
0
0
0
0
0.4
0
0
0.4
0
0
98.8
0

0
0
0
0
0
0
0
0.4
0
0
0
1.1
0
98.5

Table 7: Gesture recognition performance comparison with other existing schemes on Dataset-1 in
terms of accuracy results.

Gesture Label

Palm
L
Fist
Fist move
Thumb
Index
Ok
Palm move
Close
Palm Down
Mean

Proposed(Background separation + hand portion extraction +
CNN + Ensemble method)
1
1
1
1
1
1
0.990
0.990
1
1
0.998

[4](Feature descriptor
+ SVM)
1
0.990
0.990
1
0.990
1
0.990
1
1
1
0.990

hypothesis is rejected, and we can state that there is
a statistically signiﬁcant diﬀerence in the mean of the
accuracy values.

The proposed work by [23] (segmentation + (HOG
+ LBP) +SVM) has reported recognition rate of re-
spectively 96.0% after validating on publicly available

12

Abir Sen et al.

Table 8: Gesture recognition performance comparison with other existing proposed methods.

Method

Feature descriptor + SVM [4]

Segmentation + (HOG+ LBP)
+SVM [23]

Proposed

Used datasets
Leap motion sensor
captured infrared images
containing 10 gesture classes
Multimodal Leap motion
dataset (publicly available)
containing16 diﬀerent hand poses
Leap motion sensor
captured infrared images
containing 10 gesture classes
Multimodal Leap motion
dataset (publicly available)
containing 16 diﬀerent gesture classes
Self-constructed dataset
where binary images collected by the
camera of computer,
containing 14 diﬀerent gesture classes

Recognition rate

99.0%

96.0%

99.8%

96.4%

99.7%

Table 9: One sample statistics.

One sample statistics

N Mean
10

99.8210

Std deviation
0.3088

Std Error Mean
0.0976

Table 10: One sample test.

One sample test
Test value=99
df One-sided p Two-sided p Mean diﬀerence
<0.001
9

Signiﬁcance

0.82099

<0.001

t
8.405

95% Conﬁdence interval of the diﬀerence
Lower
0.60003

Upper
1.04196

multimodal Leap motion dataset. So our proposed work
(background separation + hand region extraction +
CNN + ensemble method) has achieved a promising
average gesture recognition rate of 99.8%, 96.4%, and
99.7% respectively after validating on Dataset-1 (leap
motion sensor captured infrared images), Dataset-2 (mul-
timodal Leap motion dataset), and self-constructed dataset
compared to other existing schemes.

3.7 Our model performance on real-time video

To verify the eﬀectiveness of our proposed model, we
have deployed our trained model on the webcam-based
video to perform real-time gesture detection and clas-
siﬁcation of gestures. This proposed model was imple-
mented by using the system of having Intel Core i7-
10750H CPU, 16GB RAM having NVIDIA GeForce
GTX 1650 GPU. This work consists of six stages:
(1) Firstly, the method uses motion-based background
separation technique [24] in the video frame sequence to
detect the hand portion after separating the unwanted
background portion.

(2) To diﬀerentiate between foreground and background,
we have applied binary thresholding, making the de-
tected hand portion visible, whereas other unwanted
parts remain black.
(3) After that, we have extracted the contour region
from the detected image. The contour portion with hav-
ing the largest area is considered to be the hand portion.
(4) After hand portion segmentation, the median ﬁlter
has been used to remove noise (shadow problem due to
low light).
(5) Next, the images are resized into a ﬁxed aspect ra-
tio, and then they are fed into three individual CNN
models for training.
(6) In the last part, the output scores of the CNN mod-
els are averaged to build an ensemble model for clas-
sifying gesture images. The ﬂow diagram of our real-
time gesture recognition system is shown in Fig. 13 and
the gesture recognition performance of our proposed
method on real-time video is shown in Fig. 14. It is
observed that our proposed ensemble model has been
achieved a reasonable recognition rate in the presence
of the low-light environment. The average recognition

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

13

speed of our experiment has reached 20 fps while per-
forming real-time gesture recognition by using the en-
semble model.

3.8 Real-time gesture recognition performance analysis

Analysis of hand gesture recognition performance in the
real-time scenario has been shown in Table 11. In this
work, to show the average gesture classiﬁcation time,
we have taken three samples of each gesture, denoted
as ‘Gesture sample 1’, ‘Gesture sample 2’ and ‘Gesture
sample 3’ for 5 seconds. Then we have calculated the
delay between video capture and gesture classiﬁcation
results by the following formula:

Time delay = (time taken for video capture − time
taken for gesture classiﬁcation)

After calculating sample-wise time delay, we have cal-
culated the average classiﬁcation time for each gesture
(in milliseconds). Average gesture classiﬁcation time is
calculated by the formula:

Recognition time = Average recognition time for each gesture

Total number of gesture classes

For instance, three samples of gesture “Ok” have been
taken for 5 seconds. The time delay between video cap-
ture and classiﬁcation results for three gesture samples
is 0.106 ms, 0.098 ms, and 0.091 ms, respectively. So
the average gesture recognition time for gesture “Ok”
is 0.099 ms. Similarly, for every gesture, we have shown
that the processing time for average gesture classiﬁ-
cation is 0.117 ms by using the formula as mentioned
above.

3.9 Discussions

From our experiment, we have ensured that custom-
CNN models can produce good results by hyper-parameter
optimization, model structure, and using dropout in
dense layers. But CNN architectures suﬀer from high
variance problem due to high dependence on training
data and having overﬁtting problem: this leads to in-
crease in bias and reduction in generalization. We have
resolved this issue by training three self-designed CNN
models (i.e., GoogLeNet-like, VGGNet-like, and AlexNet-
like) in parallel and then averaging the output scores
from CNN models to construct the ensemble model for
classiﬁcation tasks.
Table 3 shows that our ensemble model has achieved
a higher classiﬁcation accuracy (99.80% on Dataset-1,
96.50% on Dataset-2, and 99.76% on the self-constructed
dataset) than other individual CNN models. Tables 7
and 8 also show the superiority of our proposed method

Fig. 13: Flow chart of real-time gesture recog-
nition system.

14

Abir Sen et al.

Table 11: Real-time gesture recognition performance analysis along with average gesture classiﬁ-
cation duration.

Average time delay between video capture
and
classiﬁcation results in milliseconds (ms)
(duration for each sample collection is 5 sec)

Gesture label Gesture sample 1 Gesture sanple 2 Gesture sample 3

Ok
Two
Five
Close
Fist
Hang
Heavy
L
Index
Three
Four
Palm move
Thumb
Palm

0.106
0.104
0.147
0.085
0.125
0.085
0.094
0.123
0.131
0.115
0.103
0.140
0.096
0.131

0.098
0.165
0.136
0.078
0.122
0.092
0.101
0.113
0.135
0.120
0.113
0.107
0.087
0.128

Average gesture classiﬁcation time (in ms)

0.091
0.130
0.112
0.091
0.120
0.101
0.114
0.207
0.177
0.130
0.107
0.097
0.091
0.187

Average classiﬁcation time for each gesture
(in ms)
0.099
0.133
0.131
0.084
0.122
0.093
0.103
0.147
0.147
0.122
0.107
0.114
0.091
0.148
0.117

Fig. 14: Gesture recognition performance of our proposed ensemble model on webcam video.

by comparing the results with the state-of-the-art ap-
proaches for the classiﬁcation of the gesture images.
To show the superiority of our proposed approach, we
have deployed our trained ensemble model on the webcam-
based video to perform a real-time gesture recognition
system and has achieved a good recognition accuracy.
The average gesture recognition speed has reached 20
fps. The average recognition speed is slightly dropped
through our ensemble method compared to individual
CNN models because it is built by combining the output
layers from three CNN models. But we have eﬀectively

improved the accuracy and stability of our proposed
gesture recognition system. Recognizing gestures in a
very complicated background is still a challenging task
during the segment of the gesture portion. In the future,
we will improve our model to tackle hand movements in
a very complex environment. Furthermore, our method
can be extended by creating several combinations of
models and selecting the best model combination to
construct the ensemble model for the current task.

A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network

15

4 Conclusion

In this paper, an eﬃcient gesture recognition system has
been presented based on ensemble-based CNN. We have
employed a set of stages to process the gesture image
and segment the hand region before fed into three CNN
classiﬁers for training in parallel. In the last part, the
output scores of these three CNN models are averaged
to build our ensemble model for ﬁnal prediction. Two
publicly available datasets (Dataset-1 and Dataset-2)
and one self-constructed dataset have been used to val-
idate the proposed approach. The results of our exper-
iments exhibited the superiority of the ensemble-based
CNN approach over existing schemes. Our proposed
method has achieved 99.80%, 96.50%, and 99.76% accu-
racy on Dataset-1, Dataset-2, and our custom dataset,
respectively. The average recognition speed has reached
20 fps, and the average gesture classiﬁcation takes 0.117
ms, which meets the requirements of an eﬃcient real-
time gesture recognition system. Our proposed work
can be beneﬁcial to build HCI (human-computer in-
teraction) based system, gesture-controlled home au-
tomation system, sign language translation (really help-
ful for people with hearing or speech impairment), and
controlling robots. In our future work, we will extend
our proposed framework to build a multi-modal fusion-
based application by combining other modalities like
speech, eye-gaze tracking, and face recognition.

References

1. Chen Z.h, Kim JT, Liang J, Zhang J, Yuan YB (2014)
Real-time hand gesture recognition using ﬁnger segmenta-
tion. The Scientiﬁc World Journal, vol 2014

2. Pititeeraphab Y, Choitkunnan P, Thongpance N, Kul-
lathum K, Pintavirooj C (2016) Robot-arm control system
using LEAP motion controller. 2016 International Confer-
ence on Biomedical Engineering (BME-HUST), pp 109-112
3. Wei Lu, Zheng Tong, Jinghui Chu (2016) Dynamic hand
gesture recognition with leap motion controller. IEEE Sig-
nal Processing Letters 23(9):1188-1192

4. Mantec´on T, del Blanco CR, Jaureguizar F, Garc´ıa N
(2016) Hand gesture recognition using infrared imagery pro-
vided by leap motion controller. International Conference
on Advanced Concepts for Intelligent Vision Systems, pp
47-57

5. Ching-Hua Chuan, Eric Regina, Caroline Guardino (2014)
American Sign Language Recognition Using Leap Motion
Sensor. 13th International Conference on Machine Learning
and Applications, pp 541-544

6. Deng-Yuan Huang, Wu-Chih Hub, Sung-Hsiang Chang
(2011) Gabor ﬁlter-based hand-pose angle estimation for
hand gesture recognition under varying illumination. Ex-
pert Systems with Applications 38(5):6031-6042

7. Xing Yingxin, Li Jinghua, Wang Lichun, Kong Dehui
(2016) A robust hand gesture recognition method via con-
volutional neural network. 6th International Conference on
Digital Home (ICDH), pp 64-67

8. Huogen Wang, Pichao Wang, Zhanjie Song, Wanqing Li
(2017) Large-scale multimodal gesture segmentation and
recognition based on convolutional neural networks. Pro-
ceedings of the IEEE International Conference on Com-
puter Vision Workshops, pp 3138-3146

9. Li G, Tang H, Sun Y, Kong J, Jiang et al (2019) Hand
gesture recognition based on convolution neural network.
Cluster Computing 22(2):2719-2729

10. Fang W, Ding Y, Zhang F, Sheng J (2019) Gesture recog-
nition based on CNN and DCGAN for calculation and text
output. IEEE Access, vol 7, pp 28230-28237

11. Neethu, PS and Suguna, R and Sathish, Divya (2020)
An eﬃcient method for human hand gesture detection and
recognition using deep learning convolutional neural net-
works. Soft Computing,pp 1-10

12. Hu, Bin and Wang, Jiacun (2020) Deep learning based
hand gesture recognition and UAV ﬂight controls. Interna-
tional Journal of Automation and Computing, Springer, vol
17(1), pp 17-29

13. Chen, Zhi-hua and Kim, Jung-Tae and Liang, Jianning
and Zhang, Jing and Yuan, Yu-Bo (2014) Real-time hand
gesture recognition using ﬁnger segmentation. The Scien-
tiﬁc World Journal, Hindawi

14. Xu, Pei (2017) A real-time hand gesture recognition
and human-computer interaction system. arXiv preprint
arXiv:1704.07296

15. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S et al (2015)
Going deeper with convolutions. Proceedings of the IEEE
conference on computer vision and pattern recognition, pp
1-9

16. Simonyan K, Zisserman A (2014) Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556

17. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet
classiﬁcation with deep convolutional neural networks. Ad-
vances in neural information processing systems, pp 1097-
1105

18. Deng J, Dong W, Socher R, Li LJ et al (2009) Imagenet:
A large-scale hierarchical image database. 2009 IEEE con-
ference on computer vision and pattern recognition, pp 248-
255

19. Rajaraman S, Jaeger S, Antani SK (2019)Performance
evaluation of deep neural ensembles toward malaria parasite
detection in thin-blood smear images. PeerJ 7:e6977

20. Polikar, Robi (2012) Ensemble learning. Springer, pp 1-

34

21. Gupta, Gajanand (2011) Algorithm for image process-
ing using improved median ﬁlter and comparison of mean,
median and improved median ﬁlter.International Journal
of Soft Computing and Engineering (IJSCE), vol 1(5), pp
304-311

22. Kingma, Diederik P and Ba, Jimmy (2014) Adam:
A method for stochastic optimization. arXiv preprint
arXiv:1412.6980

23. Mantec´on T, del Blanco CR, Jaureguizar F, Garc´ıa N
(2019) A real-time gesture recognition system using near-
infrared imagery. PloS one 14(10)

24. Rakibe, Rupali S and Patil, Bharati D (2013) Back-
ground subtraction algorithm based human motion detec-
tion. International Journal of scientiﬁc and research publi-
cations (Citeseer), vol 3(5), pp 2250-3153

