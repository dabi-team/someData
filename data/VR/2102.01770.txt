To appear in IEEE Transactions on Visualization and Computer Graphics

A privacy-preserving approach to streaming eye-tracking data

Brendan David-John, Student Member, IEEE, Diane Hosfelt,
Kevin Butler Senior Member, IEEE and Eakta Jain, Member, IEEE

1
2
0
2

r
a

M
9
1

]

C
H
.
s
c
[

2
v
0
7
7
1
0
.
2
0
1
2
:
v
i
X
r
a

Fig. 1: Top: The Gatekeeper model protects identity by delivering relevant data at different levels directly through the API, while
withholding raw gaze samples that contain biometric features. This approach cannot be used directly with applications that require
raw gaze samples. Bottom: In scenarios where a Gatekeeper API cannot be implemented, we instead apply a privacy mechanism to
raw gaze samples to serve applications that use gaze samples or event data directly.

Abstract— Eye-tracking technology is being increasingly integrated into mixed reality devices. Although critical applications are
being enabled, there are signiﬁcant possibilities for violating user privacy expectations. We show that there is an appreciable risk of
unique user identiﬁcation even under natural viewing conditions in virtual reality. This identiﬁcation would allow an app to connect a
user’s personal ID with their work ID without needing their consent, for example. To mitigate such risks we propose a framework that
incorporates gatekeeping via the design of the application programming interface and via software-implemented privacy mechanisms.
Our results indicate that these mechanisms can reduce the rate of identiﬁcation from as much as 85% to as low as 30%. The impact of
introducing these mechanisms is less than 1.5◦ error in gaze position for gaze prediction. Gaze data streams can thus be made private
while still allowing for gaze prediction, for example, during foveated rendering. Our approach is the ﬁrst to support privacy-by-design in
the ﬂow of eye-tracking data within mixed reality use cases.

Index Terms—Privacy, Eye Tracking, Eye Movements, Biometrics

• Brendan David-John is a PhD student at the University of Florida.

E-mail: brendanjohn@uﬂ.edu.

• Diane Hosfelt was a privacy and security researcher at Mozilla at the time

of writing E-mail: dianehosfelt@gmail.com

• Dr. Kevin Butler is an Associate Professor at the University of Florida.

E-mail: butler@uﬂ.edu

• Dr. Eakta Jain is an Assistant Professor at the University of Florida.

E-mail: ejain@cise.uﬂ.edu

1 INTRODUCTION

As eye trackers are integrated into mixed reality hardware, data gath-
ered from a user’s eyes ﬂows from the mixed reality platform to the
applications (apps) that use this data. This data is a critical enabler
for a number of mixed reality use cases: streaming optimization [63],
foveated rendering [10, 66, 67, 79], redirected walking [49, 50, 54, 99],
gaze-based interfaces [34, 84, 107], education [81], and social interac-
tion [26, 61, 64, 70, 74]. The eye-tracking data also contains a variety of
information about the user which are not necessarily needed by each
application. For example, eye movements identify attributes such as
gender, bio-markers for various health conditions, and identity. As a

©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse
of any copyrighted component of this work in other works. DOI: 10.1109/TVCG.2021.3067787

1

Eye ImageEye Position, e.g., (x, y, t)Camera360◦Video Safety TrainingEye Tracking PlatformEmbedded Gaze estimationRedirected WalkingEvent DetectionEye Position w/ label, e.g., (x, y, t, F/S/SP)Gatekeeper (Browser, API)Event Sample AggregatorAOI Metrics, e.g., Dwell TimeEvent Data, e.g., (Fix. #, dur, x̅, y̅, t)AOI Metric ComputationGaze-based InterfaceGatekeeper ModelCollaborative TrainingVisualization, AOI AnalysisUser IdentificationFoveated RenderingGaze PredictionUser Identification (x’, y’, t)Eye ImageEye Position, e.g., (x, y, t)CameraEye Tracking PlatformEmbedded Gaze estimationPrivacy MechanismEvent DetectionClassificationUser Identification Saliency Map GenerationTraining Saliency ModelUser Identification Real-time Event DetectionPrivacy MechanismEye Position w/ label, e.g., (x, y, t, F/S/SP)(x’, y’, t, F/S/SP)Standalone Privacy Mechanism 
 
 
 
 
 
result, how this data is handled, and to whom, has privacy and security
implications.

The problem of applications receiving data and passing it along to
colluding apps or parent companies erodes public trust in technology,
and cannot be “regulated away”. It has received public attention in the
context of similar personal devices, such as smartphones. Recently,
The Weather Channel took location data it mined from users’ foot
trafﬁc at different businesses, and sold it to hedge funds to inform their
investments before quarterly income statements were released.1. Even
with regulation, imagine that the weather app collecting location data
colludes with an advertising application that belongs to the same parent
company. The user will then be served personalized ads based on her
location: such as car ads appearing after a visit to the car dealership
for an oil change. Now imagine that the parent company also knows
which cars she glanced at while waiting, or that she actually spent most
of the time looking at the motorcycle parked out front relative to the
other vehicles.

This problem becomes even more severe when we recognize that
mixed reality headsets are going to have as much enterprise use as
personal use. A user might log in at work to do their job-related
training with their known real-world identity, but attend labor union
meetings as User X to avoid negative repercussions.2,3 The agent that
connects these two identities has the power to “out” the user to her
work organization.

In this paper, we have investigated the threat of biometric identiﬁ-
cation of a user from their eye movements when they are being eye
tracked within immersive virtual reality environments. For several
mixed reality use cases, raw eye-tracking data does not need to be
passed along to the application. As shown in Figure 1, a Gatekeeper
that resides between the eye tracking platform and applications can
alleviate this threat by encapsulating raw data within an application
programming interface (API). We have proposed a design for such an
API in Section 4.

This philosophy of serving data on a “need-to-know basis” is effec-
tive in preventing data from being used for deviant purposes instead
of their originally intended purpose. However, there remain certain
applications that rely on access to raw gaze data. In this case, we have
proposed privacy mechanisms to erase identifying signatures from the
raw gaze data before it is passed on to the application. We have evalu-
ated how the proposed privacy mechanisms impact utility, i.e., what the
application needs gaze data to do. Finally, we have investigated how
the proposed privacy mechanisms impact applications that need access
to eye events, i.e., eye-tracking data labeled as ﬁxations, saccades, or
smooth pursuits.

Our work is part of a broader thrust in the eye tracking and virtual
reality communities on characterizing risks related to unregulated mas-
sive scale user eye tracking, and developing technological mitigations
for these risks. For risks associated with an adversary gaining access
to the eye image itself, we direct readers to the privacy mechanisms
presented in [22, 44]. For a differential privacy perspective, we direct
readers to [45, 59, 97]. For a differential privacy perspective on the
identiﬁcation of users by colluding apps, we direct readers to the de-
tailed analysis in [15,97], with the caveat that the utility task considered
in this body of work is gaze-based document type classiﬁcation. In
contrast, we focus on utility tasks that are speciﬁc to mixed reality. Our
goal is to provide a foundation for future researchers and developers to
organize their thinking around the risks created by the ﬂow of behav-
ioral data in mixed reality, and the proactive rather than reactive design
of mitigation strategies.

2 EYE-TRACKING APPLICATIONS IN MIXED REALITY

We can expect eye tracking to run as a service within a mixed reality
device, analogous to the way that location services run on phones today.

1https://www.nytimes.com/interactive/2019/12/19/opinion/

location-tracking-cell-phone.html

2https://tcf.org/content/report/virtual-labor-organizing/
3https://www.foley.com/en/insights/publications/2015/09/

be-careful-what-you-say-during-a-union-organizing

Eye tracking is a speciﬁc case of more general behavioral tracking
services in mixed reality, including head, hand, and body tracking.
Mixed reality platforms such as Microsoft and Facebook will collect
raw data from the native sensors, process it to perform noise removal
and event detection, and pass the processed data up the software stack.
Because a rich, self-sustaining mixed reality ecosystem will rely on
independent content developers, a mixed reality web browser, akin
to a conventional web browser, will provide the software interface
to access a wide array of content for consumers. In this section, we
highlight critical eye-tracking applications for mixed reality that use
aggregate-level, individual-level, and sample-level gaze data.

2.1 Aggregate-level eye-tracking applications

Aggregate gaze data is collected from many viewers to drive applica-
tions such as highlighting salient regions using heatmaps [28, 82, 95],
and learning perceptual-based streaming optimizations for 360◦ con-
tent [63, 101]. These applications typically rely on a data collection
process conducted in research lab environments for a sample of viewers.
Viewer data is then used to train machine-learning models or evaluate
the most effective streaming methodology within the dataset. Results
from the dataset are then released in aggregate form to inform the de-
ployment of such methods on consumer devices. This provides utility
to the consumer without creating privacy risks, however training data
for machine-learning models may pose a risk to privacy [25], as well
as publicly-released datasets that include the raw gaze data used to
generate aggregate representations [1, 40, 41, 57, 102].

2.2 Event-level eye-tracking applications

Eye movement behavior captured by eye-tracking events, such as ﬁx-
ations, saccades, and smooth pursuit, contribute to gaze-based inter-
faces [34, 77], evaluating training scenarios [19, 30, 43], and identifying
neurodegenerative diseases [75] and ASD [18]. Detecting eye-tracking
events enables improved techniques for redirected walking [49, 50, 54],
a critical application for VR that expands the usable space of virtual
environment within a conﬁned physical environment. The most com-
mon method to quantify an individual’s gaze behavior is to mark Areas
of Interest (AOIs) within content and measure how gaze interacts with
this region. Typical metrics for these regions depend on ﬁxation and
saccade events only, recording dwell times, the number of ﬁxations or
glances, and ﬁxation order [55, 76]. Event data also poses a privacy
risk, as it reveal the viewer’s intent and preferences based on how gaze
interactions with different stimuli content.

2.3 Sample-level eye-tracking applications

Multiple key mixed reality applications depend on individual gaze
samples from an eye-tracker of a sampling rate of at least 60Hz. This
includes foveated rendering [10, 66, 67, 79], which is expected to have
the biggest impact on deploying immersive VR experiences on low-
power and mobile devices. This application relies on gaze samples
to determine where the foveal region of the user currently is, and to
predict where it will land during an eye movement to ensure that the
user does not perceive rendering artifacts [3]. Similarly, gaze prediction
models are trained that predict future gaze points while viewing 360◦
imagery and 3D rendered content [40, 41].

Another key set of applications that require sample-level data are
gaze guidance techniques [88, 89]. Gaze guidance takes advantage
of sensitivity to motion in the periphery to present a ﬂicker in lumi-
nance that will attract the user’s eyes, using eye tracking to remove
the ﬂicker before the user can ﬁxate upon the region and perceive the
cue [8, 38]. This technique enables manipulation of visual attention,
and ultimately user behavior. For example, gaze guidance in 2D en-
vironments has been shown to improve spatial information recall [7],
improve training of novices to identify abnormalities in mammogram
images [96], and improve retrieval task performance in real-world envi-
ronments [12]. Gaze guidance has also been used to enhance redirected
walking techniques in VR by evoking involuntary eye movements, and
taking advantage of saccadic suppression [99]. Guiding gaze through
saccades and manipulating the user allows for use of a 6.4m×6.4m vir-
tual space within a 3.5m×3.5m physical space, signiﬁcantly improving

2

To appear in IEEE Transactions on Visualization and Computer Graphics

Table 1: State-of-the-art gaze-based biometric methods. Key: RBF = Radial Basis Function Network, RDF = Random Decision Forests, STAT =
Statistical test, SVM = Support Vector Machine.

Method
Schroder et al. [93]
Schroder et al. [93]
George&Routray [35]
Lohr et al. [60]
Lohr et al. [60]
Eberz et al. [31]
Rigas et al. [86]
Monaco [68]

Features
Fixation, Saccade
Fixation, Saccade
Fixation, Saccade
Fixation, Saccade
Fixation, Saccade
Fixations, Binocular Pupil

Classiﬁer
RBF
RDF
RBF
STAT
RBF
SVM

Fixations, Saccades, Density maps Multi-score fusion

Gaze Velocity/Acceleration

STAT

Dataset
BioEye 2015, MIT data set
BioEye 2015, MIT data set
BioEye 2015
VREM-R1, SBA-ST
VREM-R1, SBA-ST
[31]
[86]
EMVIC 2014

Results
IR: 94.1%, 86.76%
IR: 90.9%, 94.67%
IR:93.5%
EER: 9.98%, 2.04%
EER: 14.37%, 5.12%
EER: 1.88%
EER: 5.8%, IR: 88.6%
IR: 39.6%

upon the usable area within VR experiences. This application requires
an eye tracker sampling rate of 250Hz or more, and requires sample-
level data to know precisely when gaze moves towards the periphery
cue. Providing sample-level data with high accuracy at this frequency
poses a serious risk to user privacy in the form of gaze-based biometric
features that can then be extracted from these gaze positions.

3 RELATED WORK
Human eyes reﬂect their physical attributes. For example, algorithms
can estimate the ages of users by monitoring the change in the gaze
patterns as they age [73, 106], their gender based on the temporal
differences in gaze patterns while viewing faces [92], and their race
from the racial classiﬁcation of faces they tend to look at [9].

Beyond physical attributes, gaze allows rich insights into psycho-
logical attributes, such as neurological [56] and behavioral disor-
ders [27, 72, 80]. The eyes can also reveal whether an individual suffers
from an affective disorder—anxious individuals’ gaze is characterized
by vigilance for threat during free viewing, while depressed individuals’
gaze is characterized by reduced maintenance of gaze on positive stim-
uli [5]. Eye tracking has also been used to investigate gaze behavior in
individuals on the autism spectrum, ﬁnding that they generally tend to
ﬁxate less on faces and facial features [13, 23].

Pupillometry, when combined with scene metadata could allow al-
gorithms to infer user sexual orientation, as shown in clinical studies
measuring genital responses, offering a less invasive way to infer in-
dividual’s preferences [85]. In addition to allowing sexual orientation
inferences, pupillometry can reveal insight into women’s hormonal cy-
cles using similar methodology [52]. Pupil size also reveals the user’s
cognitive load [29] as well as emotional arousal, as shown in studies
with images [17, 53] and videos [83]. Interestingly, pupil response
seems to be modulated by subconscious processing, changing when the
mind wanders [100].

Body mass index (BMI) status appears to inﬂuence gaze parame-
ters that are not under conscious control, allowing BMI estimation
when presenting individuals with images of foods of differing caloric
content [37]. These risks involve knowledge of both eye position and
stimuli, whereas user identiﬁcation can be applied to raw eye move-
ments without knowledge of what the stimuli was.

3.1 State-of-the-art in user identiﬁcation based on eye

movements

Gaze patterns can be used to identify individuals as they contain unique
signatures that are not under a user’s voluntary control [47, 48]. The
Eye Movement Veriﬁcation and Identiﬁcation Competitions in 2012
and 2014 challenged researchers to develop algorithms that identiﬁed
users based on their eye movements when they followed a jumping
dot (2012) and when they looked at images of human faces (2014). The
best models’ accuracy ranged from 58% to 98% for the jumping dot
stimuli, and nearly 40% accuracy compared to a 3% random guess
probability for viewing faces.

Based on recent surveys on eye movements biometrics [33, 87] as
well as our own literature search, we identiﬁed algorithms that have
been shown to successfully identify individual users from their eye
movements in Table 1. These algorithms have been applied to existing
gaze-biometric challenge datasets, as well as the natural viewing of

image stimuli in 2D (MIT data set). The method with the best biometric
performance produces an Equal Error Rate of 1.88% using pupil-based
features [31], however the majority of consumer applications in mixed-
reality do not require pupil diameter. Thus, we selected to implement
the RBF approach proposed by George and Routray [35], as it relies
only on ﬁxation and saccade events. This method also produces impres-
sive results with VR eye-tracking data [60] and natural viewing of 2D
images [93].

3.2 State-of-the-art in eye-tracking security and privacy
In recent years privacy concerns related to eye-tracking applications
has grown signiﬁcantly [16, 42, 44, 51, 58, 98]. In response, researchers
have developed methods to enhance privacy of aggregate features, like
saliency heatmaps [59] and event statistics [15, 32, 97]. These methods
have been shown to reduce performance in classiﬁcation of gender
and identity, however the methods operate only on aggregate gaze data
after it has been collected and processed. Recent work by Li et al. has
applied formal privacy guarantees to raw streams of gaze designed to
obfuscate viewer’s gaze relative to AOIs within stimuli over time [58].
The ability to protect biometric identity was was evaluated empirically
on the 360 em dataset [1], reducing identiﬁcation to chance rate. Our
work develops a threat model based on the streaming of gaze samples
and the privacy risk related to biometric identiﬁcation within an XR
ecosystem.

4 DESIGNING AN API FOR GAZE PRIVACY
The typical architecture and data ﬂow in an eye-tracking platform is
shown in Figure 1. Existing eye trackers process user data in three
stages: eye image capture, which images the user’s eye, eye position
estimation, which infers the point of regard from the eye image, and
event detection, which classiﬁes each point of regard as belonging
to a ﬁxation, saccade, blink, etc. When eye trackers were specialty
equipment, all this data was made available to the application. These
applications were typically research data gathering software. The
major difference now is that the applications will have a proﬁt-based
business model. This model will naturally create incentives to share
user gaze data and make inferences by combining data across devices
for advertising revenue, for example. We have identiﬁed privacy risks
created by this ecosystem in Section 3. In this section, we deﬁne our
threat model and propose the design of an application programming
interface (API) which adopts a privacy-preserving approach to passing
gaze data to downstream applications.
Threat Model We assume that the components comprising the eye-
tracking platform and API are trusted, i.e., the integrity of the hardware
and software could be attested through mechanisms such as secure
boot [4] and integrity measurement [90], and we assume that the op-
erating system is protected, e.g., through SELinux mandatory access
controls [69]. The adversary is capable of examining all data transmit-
ted to the eye-tracking applications, and seeks to use this information
to re-identify the user. An adversarial application has the capability to
collude with other applications by sharing information through either
overt or covert channels [65] in order to re-identify users.

Our privacy-preserving solution is focused on preventing biometric
identiﬁcation of users from their gaze data. First, the eye is imaged by a
camera, producing an eye image that is provided to the platform, which

3

processes the image into position coordinates. The platform provides
this eye position to trusted applications like the browser, which then
pass the eye position on to browser apps that perform tasks such as AOI
analysis for performance in training scenarios, saccade detection for
redirected walking, and smooth pursuits for gaze-based interaction.
Nai¨ve API Design The simplest way to provide a gaze API would be
to pass along the raw gaze data to applications. At any point in time,
the application would be able to request getGazePosition(). From
this, the application would be able to compute ﬁxations, saccades, and
dwell time; in particular, an AOI application would be able to compute
ﬁxations in an AOI, time to ﬁrst saccade into the AOI, and dwell time
in the AOI.

Providing raw gaze data also allows for computation of the velocity
of eye movements, and other features that are commonly used for
identity classiﬁcation tasks [33, 35, 93]. Allowing for raw gaze access
in an untrusted context, such as the web, allows arbitrary apps the
ability to re-identify users.

4.1 Enabling AOI Metrics

However, we can modify the gaze API to be privacy-preserving by
acting as a Gatekeeper. Privacy vulnerabilities are caused by the design
assumption that the application is benign, and the data is used only for
the purpose for which it is collected. As discussed previously, appli-
cations need not be benign, and connecting user data across devices
will allow for richer inferences to be made about that user. This threat
motivates our proposed Gatekeeper design. An added beneﬁt of our
proposed design is that the Gatekeeper model provides desired metrics
directly to applications, instead of requiring applications to process
streamed user gaze data and calculate the metrics themselves.

Advertisers and other AOI applications are interested in the number
of ﬁxations and the dwell time of a ﬁxation in a predetermined AOI.
Under the Gatekeeper framework, instead of passing along raw gaze
positions, an API allows requests for this information. For example,
a getFixations method takes a rectangular area and returns a list of
ﬁxations that had occurred in that area, and a getDwellTime method
takes as input a ﬁxation and returns in milliseconds the dwell time of the
ﬁxation. Additionally, we provide a getSaccades method that would
return a list of saccades into the AOI. Saccades are a strong classiﬁer
feature for identity, when raw gaze points are included, however we
mitigate this risk by providing only lower dimensional summary data.
It is important to note that this API is designed speciﬁcally to pro-
vide AOI metrics and summary data of eye movement events. The
API does not scale to address applications such as platform foveated
rendering, which requires raw gaze samples for utility. The Gatekeeper
model does support streaming optimizations based on the current gaze
position within a discrete set of tiles [20, 78], by providing only infor-
mation about which tile they are currently attending too. This type
of optimization is critical for low-power devices to ensure high visual
quality while preserving precious network resources.

4.2 Enabling Real-time Event Data

In some situations, such as gaze-based interfaces and redirected walk-
ing, applications will need to be notiﬁed when a new ﬁxation or saccade
occurs, instead of querying for all ﬁxations or saccades.

In this scenario, we can use an EventListener model instead of a
query-based model. When a new event occurs, the EventListener
will be notiﬁed and given the event data, (x, y, t) and a boolean indi-
cating if it is a ﬁxation, saccade, or smooth pursuit. More complex eye
movements are difﬁcult to detect in real-time with the sampling rate of
mixed reality eye-tracking devices, and typically are not implemented
in real-time applications.

Our typical model for streaming event data is to send an event when
the eye movement has concluded. For example, in a gaze-based inter-
face the application needs to be notiﬁed that a smooth pursuit occurred,
and where it landed. In applications such as redirected walking it is
critical to know when a saccade begins, to take advantage of saccadic
blindness [49,50,54,99]. In this case, one mode of the EventListener
will be to indicate when a saccade event has started and ﬁnished, as
opposed to only when the saccade has ﬁnished.

Table 2: Privacy mechanism variable deﬁnitions.

Variable Description

x
y
t
e
X
G
X (cid:48)
K
L
M
N
δx
δy

Horizontal gaze position
Vertical gaze position
Timestamp
Event label: Fix. (F), Sacc. (S), Smooth Pursuit (SP)
Input time series of gaze samples
Number of gaze positions in time series
Output privacy-enhanced time series
Temporal downsample factor relative to sampling rate
Spatial downsample factor relative to 3840×2160
Number of rows in equirectangular projection
Number of columns in equirectangular projection
Horizontal step size: 360
N
Vertical step size: 180
M

4.3 Enabling Privacy-enhanced Sample Data

Most applications will be able to function with the aforementioned API
designs; however, two key mixed reality applications that will require
sample-level data are foveated rendering and subtle gaze guidance.

Foveated rendering is critical for performance on next generation
wearable VR headsets. In an ideal situation, platforms will use GPU-
based foveated rendering—where gaze information is sent to the graph-
ics driver, informing it to do fewer calculations for the parts of the
screen that are away from the center of view. This requires cooperation
with the graphics hardware driver for optimal performance. Experi-
ments on native platforms show up to a 2.71 times speed up in frames
per second [66]. This will not be possible in all cases, so platforms
and browsers will also need to leverage software-based foveated ren-
dering and streaming optimization [71]. In this scenario, gaze samples
are transmitted directly to the content or webpage, which then knows
where it should render objects in more detail. However, this exposes
the raw gaze data to the application and allows the content to perform
further processing on the raw gaze information, whether that is user
identiﬁcation or inferring sensitive characteristics.

In these scenarios the eye-tracking platform must stream sample-
level data, and it is impossible to simply abstract data using a privacy-
preserving API. Therefore, we propose the use of a privacy mechanism
to manipulate gaze samples as they are streamed to increase privacy.

5 METHODOLOGY

In this section, we propose, implement, and evaluate three privacy
mechanisms with the goal of mitigating the threats identiﬁed in Sec-
tion 4. Our goal is to reduce the accuracy of user identiﬁcation based
on features derived from common eye events, such as ﬁxations and
saccades. We consider the following privacy mechanisms: addition of
Gaussian noise to raw gaze data, temporal downsampling, and spatial
downsampling.

We implement these mechanisms and evaluate them against the base-
line identiﬁcation rate when raw gaze data is passed to the application
as is. For each of the privacy mechanisms, we also evaluate the utility
of the data that is passed downstream.

5.1 Privacy Mechanism Deﬁnitions

We deﬁne the data received by the privacy mechanism to be a time
series where each tuple is comprised of horizontal and vertical gaze
positions (x, y), a time stamp t, and the event label assigned to the sam-
ple e: X = {(x1, y1,t1, e1), (x2, y2,t2, e2), ..., (xG, yG,tG, eG)}, a set of
G gaze positions. This data is processed via a privacy mechanism and
the processed output as a time series X (cid:48), with additional variables de-
ﬁned in Table 2. The following three privacy mechanisms are explored
in this paper.

Additive Gaussian Noise

Noise is sampled from a Gaus-
sian distribution of zero mean and standard deviation σ de-
ﬁned in visual degree is added to the gaze positions. Noise

4

To appear in IEEE Transactions on Visualization and Computer Graphics

Table 3: Dataset characteristics.

Dataset
ET-DK2 (ours)
VR-Saliency [95]
VR-EyeTracking [102]
360 em [1]
DGaze [40]

Participants
18
130
43
13
43

# Stimuli Avg. # Stimuli

50
23
208
14
5

50
8
148
14
2

Stimuli Duration
25s
30s
20s-70s
38s-85s
180s-350s

Stimuli Type
360◦ Images
360◦ Images
360◦ Videos
360◦ Videos
3D Rendered Scene

Task
Free Viewing
Free Viewing
Free Viewing
Free Viewing
Free Viewing

is independently sampled for horizontal and vertical gaze posi-
tions as X (cid:48) = {(x1 + N(0, σ ), y1 + N(0, σ ),t1, e1), (x2 + N(0, σ ), y2 +
N(0, σ ),t2, e2), ..., (xG + N(0, σ ), yG + N(0, σ ),tG, eG)}.

Temporal Downsampling Temporal downsampling reduces the
temporal resolution of the eye-tracking data stream. Downsampling
is implemented by streaming the data at a frequency of the original
sampling rate divided by a scaling parameter K. The output time
series is deﬁned as X (cid:48) = {(x(K·p)+1, y(K·p)+1,t(K·p)+1, e(K·p)+1), ...}
for all
For example, with a scaling pa-
the private gaze positions are deﬁned as X (cid:48) =
rameter of two,
{(x1, y1,t1, e1), (x3, y3,t3, e3), (x5, y5,t5, e5), ...}, retaining only every
For a scaling parameter of three, X (cid:48) =
other gaze sample.
{(x1, y1,t1, e1), (x4, y4,t4, e4), (x7, y7,t7, e7), ...}.

integers p ∈ [0, G

K ].

Spatial Downsampling Spatial downsampling reduces the resolu-
tion of eye-tracking data down to a discrete set of horizontal and vertical
gaze positions. Intuitively, the scene is divided into a grid and each
gaze sample is approximated by the grid cell that it lies within. Spa-
tial downsampling is performed by deﬁning a target equirectangular
domain spanning 180◦ vertically and 360◦ horizontally with M rows
and N columns. For smaller values of M and N there are less possible
positions, and thus reduced spatial resolution. Raw gaze positions
(x ∈ [0, 360◦), y ∈ [0, 180◦),t) are transformed by ﬁrst computing the
horizontal step size δy = 180
N . Downsam-
pled gaze positions are then computed as ((cid:98) x
(cid:99) · δy,t), where
δx
(cid:98)·(cid:99) represents the ﬂoor function that rounds down to the nearest integer.
For the results presented in this paper, we parameterize spatial
downsampling as a factor L relative to an equirectangular domain
of M = 2160 and N = 3840, mapping to a domain of M = 2160
and
L
N = 3840
L . For example, an input downsampling factor of L equals two
will result in M = 1080 and N = 1920, a factor of L equals three will
result in a resolution of M = 720 and N = 1280, and so on.

M and vertical step size δx = 360

(cid:99) · δx, (cid:98) y
δy

5.2 Datasets

In order to evaluate the privacy mechanisms on how effectively they
prevented an adversary from re-identifying the user, we selected ﬁve
existing datasets of VR eye-tracking data. Table 3 presents character-
istics of each dataset included in analysis. Datasets were selected to
have diversity in the number of participants, the number of stimuli pre-
sented, and the task being performed. Four of the datasets are publicly
available, while ET-DK2 consists of data previously collected by the
authors.4

5.2.1 ET-DK2
The ET-DK2 dataset consists of twenty participants viewing ﬁfty 360◦
images using an Oculus-DK2 HMD with integrated SMI 60Hz binocu-
lar eye tracker. Data was collected under an IRB approved protocol in
December 2017 for the purpose of generating saliency maps from gaze
data. Two participants were not included in analysis, as one participant
got motion sickness, and the data collection software did not log data
from all 50 images for one participant. The remaining 18 individuals
were made up of ﬁve females and thirteen males with an average age of
32, and an age range of 23 to 52 years. Each participant viewed images
from the Salient360! [82] dataset in random order. Participants were
seated in a swivel chair so they could rotate and explore each 360◦
scene while eye and head movements were recorded.

4The dataset will be released publicly when the manuscript is published

5

All participants performed a 9-point calibration at the beginning of
the experiment, and eye-tracking accuracy was validated to less than
2◦ visual angle before image viewing. Each 360◦ image was shown
for 25 seconds, following the Salient360! [82] protocol. In contrast
to their protocol, we varied the starting orientation of the participant
within the 360◦ image across eight orientations instead of being held
constant. Halfway through the experiment participants were given a
ﬁve minute break, after which the eye tracker was re-calibrated before
viewing the rest of the images. The entire data collection process took
approximately 40 minutes, including informed consent and a post-study
demographics survey.

5.2.2 VR-Saliency

The VR-Saliency [95] dataset includes gaze data collected from par-
ticipants viewing 360◦ images on a 2D display, in VR while seated in
a swivel chair, and in VR while standing. We analyze only the seated
VR condition, as it is the only VR condition with raw data available
at 120Hz for all stimuli. Free-viewing data was collected in a similar
manner to ET-DK2 for the purpose of saliency map generation, however
only eight 360◦ images were viewed by each participant.

5.2.3 VR-EyeTracking

The VR-EyeTracking [102] dataset includes gaze data collected at
100Hz from participants viewing 360◦ videos. The dataset application
is to train a deep network model for predicting gaze within dynamic
VR environments. The video stimuli did not have a ﬁxed duration, as in
ET-DK2 and VR-Saliency, however participants viewed many videos
and took many breaks to avoid motion sickness.

5.2.4 360 em

The 360 em [1] dataset includes gaze data collected at 120Hz from
participants viewing 360◦ videos. Fourteen of the stimuli consisted of
typical 360◦ videos from YouTube, while one stimuli was created by
the authors to elicit speciﬁc eye and head movements. The dataset ap-
plication is to train and evaluate event detection algorithms, classifying
ﬁxation, saccade, smooth pursuit, and OKN events in VR viewing data.
For our analysis we only consider the fourteen stimuli downloaded
from YouTube.

5.2.5 DGaze

The DGaze [40] dataset includes gaze data collected at 100Hz from
participants that explore and navigate various 3D rendered scenes.
Within each environment multiple animals dynamically move around,
attracting visual attention of the participant. Gaze data is used to train
and evaluate the DGaze model for gaze prediction. DGaze can predict
gaze position given head orientation, or predict the next gaze position
given the current gaze position. Gaze prediction by DGaze has been
demonstrated in the context of foveated rendering, and can help account
for latency in the eye-tracking and rendering pipeline [3, 40, 79].

5.3 Metrics

For each dataset metrics are computed to identify privacy risks, and
evaluate the impact of privacy mechanisms on application utility. Utility
measures depend on the application of eye-tracking within the datasets,
ranging from AOI analysis to gaze prediction. We deﬁne a utility metric
for each dataset depending on the type of stimuli and application.

5.3.1 Privacy

In our context, privacy refers to how effectively the mechanism prevents
an adversary from identifying an individual. Identiﬁcation is deﬁned
as a classiﬁcation task: an algorithm matches the input to the database
and return the closest match. If the algorithm matches the input to
the ground truth identity, then the comparison is counted as a True
Positive, otherwise it is considered a False Negative. The Identiﬁcation
Rate (IR), is the total number of True Positive classiﬁcations divided
by the total number of comparisons [47, 48, 93]. A high IR indicates
accurate classiﬁcation of identity, and therefore, low privacy.

5.3.2 Utility

Predicting future gaze position from eye-tracking data is a critical area
of research that has yet to be solved [40, 41]. Using the DGaze dataset
we evaluate the ability to predict ground truth gaze position 100 ms into
the future when gaze data output from a privacy mechanism is used
as the testing data, and as both the training and testing data. Utility is
measured as angular gaze prediction error for each input gaze sample,
with lower values indicating higher accuracy.

The most common form of eye-tracking analysis is performed us-
ing static AOIs deﬁned within image content [55, 76]. AOI analysis
is used to study gaze behavior during social interaction [11], while
viewing websites [103], and to evaluate content placement in 3D envi-
ronments [2], among many other applications. A key AOI metric that
is robust to ﬁxation detection parameters is dwell time [76]. Dwell
time measures how long a viewer’s gaze fell within an AOI, and allows
for comparison between which AOIs attracted the most attention. We
evaluate the loss in utility between ground truth and gaze data out-
put by a privacy mechanism by computing the Root Mean Squared
Error (RMSE) between AOI dwell times. AOI utility is measured for
the ET-DK2 dataset, as two rectangular AOIs are marked within each
image that correspond with a salient object, such as people or natural
landmarks, to measure individual viewing behavior within the scene.
Eye-tracking data is also used to generate saliency maps, which
represent a probability distribution over visual content that highlights
regions most likely to be looked at by a viewer [55]. Saliency maps
generated from aggregate eye-tracking data from many viewers and
are used to train and evaluate deep learning models for saliency and
scanpath prediction [6, 24]. Saliency metrics are computed for both
360◦ images (VR-Saliency), and 360◦ video (VR-EyeTracking and
360 em). We compute KL-Divergence [55] to measure the impact on
aggregate-level gaze measures and saliency modeling.

5.4 Implementation Details: Biometric Re-identiﬁcation

We deﬁne two classiﬁers for biometric identiﬁcation using a Radial
Basis Function (RBF) network [35, 60], with one network to classify
ﬁxation events and one to classify saccade events. This method is anal-
ogous to a traditional neural network with an input layer representing
a feature vector (cid:126)x ∈ Rp containing p ﬁxation or saccade features from
a single event, one hidden layer consisting of m nodes, and an output
layer containing c class scores, one for each unique individual in the
dataset. The output class scores are used to measure which individual
the input feature vector is most similar to. Thus, larger scores indicate
a higher probability of the ﬁxation or saccade event being from that
class, or individual. Each node in the hidden layer is deﬁned by an
activation function φi((cid:126)x) and a set of real-valued activation weights wi,c,
where i ∈ [1, 2, . . . , m] and j ∈ [1, 2, . . . ,C]. The similarity score for a
given class c in the output layer is computed as a weighted sum of all
activation functions in the hidden layer,

Scorec((cid:126)x) =

m
∑
i=1

wi,c · φi((cid:126)x).

(1)

The activation function of each hidden node takes the form of a
Gaussian distribution centered around a prototype vector (cid:126)µi with spread
coefﬁcient βi. The function is deﬁned as

φi((cid:126)x) = e−βi||(cid:126)x−(cid:126)µi||2

,

(2)

6

Fig. 2: Evaluation procedure for the gaze-based biometric classiﬁer.

with shape coefﬁcient βi and prototype feature vector (cid:126)µi deﬁned prior
to training the network. Thus, an RBF network must be constructed
in two stages by ﬁrst deﬁning the prototypes and then optimizing the
activation weights.

First, k-means clustering is applied to a training set of n feature
vectors to determine k representative feature vectors per individual [35,
60]. Through this process βi and (cid:126)µi are deﬁned for each of the m = k · c
hidden nodes. The activation function φi((cid:126)x) is then deﬁned using the
cluster centroid as (cid:126)µi, and βi as 1
2σ , where σ is the average distance
between all points in the cluster and the centroid (cid:126)µi.

Second, the activation weights wi,c are learned from the same set
of training data used to deﬁne the activation functions. Weights are
trained using only ﬁxation or saccade features from the training set.
Training can be implemented using gradient descent [94], or by the
Moore–Penrose inverse when setting up the network as a linear sys-
tem [35]. The latter method is implemented in this work by deﬁning
the RBF network using an activation output matrix An×m, where rows
consist of the n training feature vectors input to the m previously de-
ﬁned activation functions, weight matrix Wm×c comprised of activation
weights wi,c, and an output matrix Yn×c generated as a one-hot encod-
ing of the ground truth identity labels. Using matrix multiplication the
following system deﬁnes the RBF Network A ·W = Y .

The weight matrix W is then learned by computing W = A∗ ·Y , where
A∗ is the Moore-Penrose inverse of A computed using MATLAB’s
pinv implementation. Class score predictions ˆY are then generated
for the testing data ˆA by computing ˆA · W = ˆY . Every sample in the
testing set is then classiﬁed as the class label with the maximum score.
To classify a stream of events the class scores from all events are
ﬁrst summed together, and then the class with the maximum value
returned. Scores from the ﬁxation RBF and saccade RBF are combined
by averaging the score between the two and summing them together
for equal contribution to the ﬁnal classiﬁcation.

5.5 Evaluation Protocol
The evaluation protocol for the RBF-based biometric, illustrated in
Figure 2, is derived from [93], where a stream of gaze data collected
from multiple participants viewing numerous static images is used for
training and testing the identity classiﬁcation. The size of the training
and testing sets are deﬁned by the number of stimuli from which gaze
data is used. For example, with a training/testing split of 50%/50% gaze
data from a half of the dataset selected at random is used for training
and the other half for testing. Fixation and saccade events data from
all C participants are aggregated from the training stimuli and are then
used to train the ﬁxation and saccade RBF networks for classifying
identity, as described in Section 5.4. Fixation and saccade events from
the testing set are input to the trained RBF networks to classify the
identity of each participant. Each participant is present in both the
training set and the testing set. Identiﬁcation rate is then computed as
the number of correct matches divided by the number of comparisons.

6 RESULTS
In this section we will compute privacy and utility metrics to evaluate
the proposed privacy mechanisms from Section 5.1 for each dataset

Privacy:Identification RateTraining Dataset:Fix./Sacc. FeaturesM StimuliDatasetCSubjectsM + N = # StimuliTrain Fix. & Sacc. RBFN: 32 hidden nodes per subjectFixations/Saccades from gaze data in training setSM+NS1I1IN…I1IM+N………N Test Stimuli per Subject IDS7SM+N…S1SM+NTesting Dataset:Fix./Sacc. FeaturesN StimuliIncorrect MatchCorrect MatchRBFN Model:Classify Fix./Sacc. Event stream as Subject IDTo appear in IEEE Transactions on Visualization and Computer Graphics

recommended, as it had the least observed impact on identiﬁcation rate
and event detection is degraded at sampling rates less than 120Hz [104].

6.2 Utility Evaluation

The utility of eye-tracking data depends on the context of the appli-
cation, thus we evaluate the impact of our privacy mechanisms at
three different scales: sample-level gaze points, individual-level gaze
behavior, and aggregate-level gaze behavior over many individuals.
First, we evaluate sample-level utility by computing gaze prediction
error using the DGaze neural network architecture, then, individual-
level utility by computing dwell time for AOIs deﬁned in the ET-DK2
dataset, and ﬁnally, we compute aggregate-level utility measures for
generating saliency heatmaps of 360◦ images and video by computing
KL-Divergence for the VR-Saliency, VR-EyeTracking, and 360 em
datasets. Tables 4, 5, and 6 present the impact of privacy mechanisms
on utility based on the parameter that provided the largest decrease in
identiﬁcation rate.
Gaze Prediction Evaluating gaze prediction accuracy involved conﬁg-
uring the DGaze neural network to predict gaze position 100ms into
the future, which as a baseline produces an average gaze prediction
error of 4.30◦. Gaze prediction error was as high as 9.50◦ for the Gaus-
sian mechanism, more than double the baseline gaze prediction error
reported in [40]. Next, we evaluated performance by re-training the
DGaze model from scratch and applying privacy mechanisms to both
training and testing data dataset. This resulted in much lower prediction
errors, with results as low as 5.44◦ (Table 4), which are comparable to
the 4.30◦ reported in [40].

Introducing the privacy mechanism to both training and testing
data implies that raw gaze data is not shared with any party during
model training and deployment. Our experiments indicate that it is still
possible to learn a reasonable gaze prediction model without access to
the raw gaze data. Withholding raw gaze data from the training dataset
is desirable, as it removes the need to safeguard additional data and
alleviates the risk of membership inference attacks [25]. We expect
future gaze prediction models will improve in performance, and in turn
decrease the absolute gaze prediction error when using gaze data output
from the privacy mechanisms.
AOI Analysis The impact of privacy mechanisms on area of inter-
est (AOI) analysis is measured as the Root Mean Squared Error (RMSE)
between AOI metrics. There are several popular AOI metrics, suitable
for different analyses, such as number of visits to an AOI [103], time
to ﬁrst ﬁxation, and number of visits to an AOI [43]. For an overview
of AOI analysis, see the discussion by Le Meur and Baccino [55]. For
an investigation into privacy mechanisms, we select Dwell Time as
a representative AOI metric. Dwell time is the amount of time spent
by a user on an AOI, computed as the sum of the durations of all the
ﬁxations inside that AOI. The key logical operation is checking whether
a ﬁxation location falls within the bounding box that demarcates the
AOI, which is the typical ﬁrst step in all AOI metrics.

If the ﬁxation location is perturbed, such as with the privacy mecha-
nisms proposed above, then we can anticipate an error being introduced
in the dwell time computation. We report the RMSE computed be-
tween AOI Dwell Time for each individual on the original dataset and
after privacy mechanisms are applied, averaged across all stimuli in the
dataset.

RMSE in dwell time computation for additive Gaussian noise and
temporal downsampling is below 40ms (Tables 4 and 5), which is in-
signiﬁcant for the practical application of AOI metrics, as a ﬁxation
itself typically lasts 200ms [91, 105]. However, for spatial downsam-
pling, an RMSE of 247ms is introduced, which is greater than the
length of one visual ﬁxation. While being a few ﬁxations off on average
may not have a large effect on AOI applications such as evidence-based
user experience design, it may be noticeable in scenarios with multiple
small AOIs close together, such as ﬁguring out which car the user spent
longest looking at on a virtual visit to a car dealership.
Saliency Map Generation Saliency maps represent a spatial probabil-
ity distribution of attention over an image or video. Maps are generated
by aggregating ﬁxations from eye-tracking data of multiple observers
to highlight regions that attract the most attention in the stimulus [46].

Fig. 3: Mean and standard deviations of identiﬁcation rates across
datasets of 360◦ images (ET-DK2, VR-Saliency), 360◦ videos (VR-
EyeTracking, 360 em), and 3D rendered scenes (DGaze). Lines for
each dataset indicate a baseline of random guessing for the given
number of subjects.

listed in Table 3. In Section 6.1, we ﬁrst compute identiﬁcation rate
using the RBF biometric for each dataset without modiﬁcation, to
establish a baseline privacy risk. Then, we compute identiﬁcation rate
for the privacy mechanisms for different parameter values and discuss
observed effects. Last, in Section 6.2 we explore the privacy achieved
by each mechanism, and the measured impact on eye-tracking utility.

6.1 Gaze-based Biometric

We evaluate the RBF biometric by splitting gaze data from stimuli
viewed by each participant into training and testing sets as described
in Section 5.5. For each dataset we evaluate a 75%/25%, 50%/50%,
and 25%/75% training/test split, except for DGaze as each participant
only saw two stimuli. Identiﬁcation rate is computed over ten runs with
random stimuli selected as part of the training and test set, to account
for variance in stimuli content.

Figure 3 presents the mean and standard deviation of identiﬁcation
rates for each dataset, along with a baseline rates corresponding to
random guessing. For all datasets, identiﬁcation rate were highest when
there was more training data than testing data, i.e., a 75%/25% split.
ET-DK2 produced the highest identiﬁcation rate with 85% on average,
where participants viewed 50 static 360◦ images. VR-Saliency used a
similar protocol with 130 participants, however only eight images were
shown to each individual on average. A lower identiﬁcation rate of 9%
was observed in this dataset, compared to a baseline guess rate of 0.77%.
Further analysis comparing identiﬁcation rates for ET-DK2 using only
eight stimuli, and VR-Saliency with eighteen random subjects closed
the gap, producing identiﬁcation rates of 47% and 22% respectively.
Identiﬁcation rates for the VR-EyeTracking and 360 em datasets are
lower on average than the ET-DK2 dataset, reporting rates of 33% and
47%. We observed that DGaze produced an identiﬁcation rate of 2.7%,
showing only slight improvement over a baseline rate of 2.3%. This
dataset differs in that participants moved through two 3D rendered
virtual scenes using a controller for teleportation for several minutes at
a time, instead of viewing many 360◦ scenes from a ﬁxed viewpoint.

In summary, we observe that using more data for training and view-
ing many different stimuli produces higher identiﬁcation rates. Thus,
it will become easier and easier to re-identify an individual as a large
volume of gaze data is collected in a variety of contexts. Identiﬁcation
rates are as high as 85% depending on the circumstances, highlighting
the need to enforce privacy in future mixed reality applications.

Figure 4 presents the mean and standard deviations achieved when
privacy mechanisms are applied to each dataset. A training/testing split
of 75%/25% is used to generate these results. We observe that Gaussian
noise achieves the most privacy, reducing the identiﬁcation rate of ET-
DK2 from 85% to 30% on average. Temporal downsampling is not

7

25%/75%50%/50%75%/25%Training/Testing Split020406080100Identification Rate (%)ET-DK2VR-SaliencyVR-EyeTracking360_emDGazeFig. 4: Mean and standard deviation of identiﬁcation rate for each privacy mechanism with different internal parameters. Gaussian noise generates
the lowest observed identiﬁcation rates across all datasets, while temporal downsampling has the least impact.

Table 4: This table illustrates the impact of introducing the Gaussian Noise privacy mechanism on the identiﬁcation rate as well as on three use
cases. The reported numbers are for σ = 10◦. The second column shows how the identiﬁcation rate falls after the privacy mechanism is applied.
The fourth column reports an error metric that is relevant to that use case.

Mechanism
Gaussian
Noise
Gaussian
Noise
Gaussian
Noise

Identif. Rate

Utility

3% → 2%

Gaze Prediction

Impact on Utility
Avg. Prediction Error Difference = 1.14◦

Dataset

DGaze (Re-trained)

85% → 30%

AOI Analysis

Dwell Time RMSE = 0.0359s

ET-DK2 (360◦ images)

33% → 9% Generate Saliency Map

KL-Divergence = 0.0367

VR-EyeTracking (360◦ videos)

Table 5: This table illustrates the impact of introducing the Temporal Downsample privacy mechanism on the identiﬁcation rate as well as on
three use cases. The reported numbers are for K = 3. The second column shows how the identiﬁcation rate falls after the privacy mechanism is
applied. The fourth column reports an error metric that is relevant to that use case.

Mechanism
Temporal
Downsample
Temporal
Downsample
Temporal
Downsample

Identif. Rate

Utility

3% → 3%

Gaze Prediction

Impact on Utility
Avg. Prediction Error Difference = 0.22◦

85% → 79%

AOI Analysis

Dwell Time RMSE = 0.006s

Dataset

DGaze (Not Re-trained)

ET-DK2 (360◦ images)

9% → 7%

Generate Saliency Map

KL-Divergence = 0.0019

VR-Saliency (360◦ images)

Table 6: The lowest achievable identiﬁcation rate (IR) for the Spatial Downsample was at L = 64, and the corresponding impact on utility are
reported below. The arrow indicates the IR before and after the privacy mechanism is applied.

Mechanism
Spatial
Downsample
Spatial
Downsample
Spatial
Downsample

Identif. Rate

Utility

3% → 2%

Gaze Prediction

Impact on Utility
Avg. Prediction Error Difference = 0.51◦

Dataset

DGaze (Re-trained)

85% → 48%

AOI Analysis

Dwell Time RMSE = 0.2473s

47% → 29% Generate Saliency Map

KL-Divergence = 0.1293

ET-DK2 (360◦ images)

360 em (360◦ videos)

Saliency maps are used directly for gaze prediction [24] and to opti-
mize streaming [63, 101] or rendering [62]. We compute error as the
KL-Divergence between a saliency map generated from the original
gaze data and the saliency map generated by gaze data after the privacy
mechanisms have been applied. KL-Divergence measures the rela-
tive entropy between the two saliency maps and is commonly used in
loss functions to train deep saliency prediction models and to evaluate
learned models [21, 24, 39, 55].

The spatial errors introduced by the privacy mechanism may cause
regions highlighted by the saliency map to shift or spread out, lead-
ing to larger KL-Divergence values. A recent survey revealed the
best performing model in predicting human ﬁxations produced a KL-
Divergence of 0.48 for the MIT300 dataset, with baseline models pro-
ducing values of 1.24 or higher [14]. We observed that spatial downsam-
pling produces the largest KL-Divergence on average of 0.1293, while

Gaussian and temporal downsampling mechanisms produces much
smaller values of 0.0367 and 0.0019 respectively. Spatial downsam-
pling introduced errors that are approximately a fourth of the existing
gap in ﬁxation prediction. Errors of this magnitude will cause saliency
maps generated from spatially downsampled gaze data to deviate from
ground truth, and negatively impact performance of models that use the
maps for training.

7 CONCLUSIONS AND FUTURE WORK

As eye-tracking technology is built into mixed reality devices, they
open up possibilities for violating user privacy. In this paper, we have
examined a speciﬁc threat to user privacy: unique user identiﬁcation
based on their eye movement data. This identiﬁcation would enable
colluding applications to connect a user logged in “anonymously” with
their work ID, for example.

8

Gaussian12345678910 (°)0102030405060708090100Identification Rate (%)ET-DK2VR-SaliencyVR-EyeTracking360_emDGazeTemporal Downsample23Scale Factor0102030405060708090100Identification Rate (%)ET-DK2VR-SaliencyVR-EyeTracking360_emDGazeSpatial Downsampling1248163264Scale Factor0102030405060708090100Identification Rate (%)ET-DK2VR-SaliencyVR-EyeTracking360_emDGazeTo appear in IEEE Transactions on Visualization and Computer Graphics

We ﬁrst determine biometric identiﬁcation rates across ﬁve datasets
of eye movements in immersive environments. We show that identiﬁca-
tion rates can reach as high as 85% depending on the type of stimulus
used to elicit the eye movements, and the amount of eye movement data
collected in total. Our highest identiﬁcation rates were achieved when
viewing many 360◦ images with short duration (ET-DK2), with all
datasets having an identiﬁcation rate higher than chance except DGaze.
We hypothesize this is the result of the DGaze dataset providing view-
ers only two scenes to explore, containing sparse environments with
animals that they can follow around by using teleporting to navigate. In
the context of saliency Borji [14] describes the role that stimuli plays
in eye movements elicited by viewers, suggesting that datasets from
more diverse stimuli is needed to improve generalized performance of
saliency prediction models. In the context of privacy, this suggests that
the presence of biometric features within gaze data collected in envi-
ronments differs for photorealistic, static, and dynamic stimuli. Given
enough eye movement data collected from the right stimuli, there is an
appreciable risk for identiﬁcation.

We propose a Gatekeeper model to alleviate biometric authentication
by apps that need AOI metrics or event speciﬁc data for their utility.
This model provides API calls that return desired metrics and summary
information of ﬁxation and saccades to applications without providing
streams of raw gaze data, which sufﬁces for certain classes of mixed
reality use cases. However, in the case of use cases such as foveated
rendering, streaming gaze data is required. We propose that in this
case, privacy mechanisms be applied to the raw data stream to reduce
identiﬁcation rate, while maintaining the utility needed for the given
application. We evaluated three privacy mechanisms: additive Gaussian
noise, temporal downsampling, and spatial downsampling. Our best
results used additive Gaussian noise to reduce an identiﬁcation rate
of 85% to 30% while supporting AOI analysis, gaze prediction, and
saliency map generation.
Implications Imagine the scenario described earlier of a worker that
anonymously attends labor union meetings as User X. The eye-tracking
data collected during a VR union meeting attended by User X is exposed
through a database breach or collusion with the employer, who then
discovers a match between User X and their real identity at a rate greater
than chance. Even though they were not the only worker to attend this
meeting, biometric data suggested they were the most likely employee
to have attended, turning User X into a scapegoat for the entire group.
The individual may then have their reputation tarnished in retaliation
by their employer. Our investigations are a ﬁrst step towards protecting
such a user. Though the proposed mechanisms lower identiﬁcation
rates, they do not eliminate the possibility of weak identiﬁcation. More
work is needed to create and evaluate mechanisms that allow users,
organizations, and platforms to trust eye tracking, and more broadly,
behavioral tracking, within mixed reality use cases.
Limitations Our threat model assumes a trusted platform. In cases
where the platform itself cannot be trusted, there is a need for user-
implementable solutions, similar in spirit to the user-implementable
optical defocus in [42]. Our characterization of the proposed privacy
mechanisms is based on one biometric authentication approach (RBFN).
As newer methods are developed, we will likely need new privacy
mechanisms that can applied as a software patch for the mixed reality
headset. This work also considers each privacy mechanism individually.
We expect there will be greater gains in terms of privacy when applying
a combination of different privacy mechanisms.
Future Work In addition to exploring combinations of privacy mech-
anisms, future work might draw inspiration from research in location
privacy, and investigate adapting location k-anonymity schemes for
gaze [36]. It would also be interesting to characterize stimuli as be-
ing dangerous from the perspective of biometric signatures, akin to
“click-bait”. More broadly, while our work considers the user privacy,
future work might also consider security from a platform’s perspective.
Consider the case of an attacker injecting gaze positions to fool an AOI
metric into thinking that an AOI has been glanced at (for monetization
of advertisements). One potential solution to this problem is direct
anonymous attestation in a trusted platform module (TPM) to assure
gaze consumers that there have been no injections.

ACKNOWLEDGMENTS

Authors acknowledge funding from the National Science Founda-
tion (Awards FWHTF-2026540, CNS-1815883, and CNS-1562485),
the National Science Foundation GRFP (Awards DGE-1315138 and
DGE-1842473), and the Air Force Ofﬁce of Scientiﬁc Research (Award
FA9550-19-1-0169).

REFERENCES

[1] I. Agtzidis, M. Startsev, and M. Dorr. A ground-truth data set and a
classiﬁcation algorithm for eye movements in 360-degree videos. arXiv
preprint arXiv:1903.06474, 2019.

[2] R. Alghofaili, M. S. Solah, and H. Huang. Optimizing visual element
placement via visual attention analysis. In 2019 IEEE Conference on
Virtual Reality and 3D User Interfaces (VR), pp. 464–473. IEEE, 2019.
[3] E. Arabadzhiyska, O. T. Tursun, K. Myszkowski, H.-P. Seidel, and
P. Didyk. Saccade landing position prediction for gaze-contingent ren-
dering. ACM Transactions on Graphics (TOG), 36(4):1–12, 2017.
[4] W. A. Arbaugh, D. J. Farber, and J. M. Smith. A secure and reliable
bootstrap architecture. In Proceedings of the 1997 IEEE Symposium on
Security and Privacy, pp. 65–71, 1997.

[5] T. Armstrong and B. O. Olatunji. Eye tracking of attention in the affective
disorders: A meta-analytic review and synthesis. Clinical psychology
review, 32(8):704–723, 2012.

[6] M. Assens, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor. Path-
gan: visual scanpath prediction with generative adversarial networks. In
Proceedings of the European Conference on Computer Vision (ECCV),
pp. 0–0, 2018.

[7] R. Bailey, A. McNamara, A. Costello, S. Sridharan, and C. Grimm. Im-
pact of subtle gaze direction on short-term spatial information recall. In
Proceedings of the Symposium on Eye Tracking Research and Applica-
tions, pp. 67–74, 2012.

[8] R. Bailey, A. McNamara, N. Sudarsanam, and C. Grimm. Subtle gaze
direction. ACM Transactions on Graphics (TOG), 28(4):1–14, 2009.
[9] Y. Bar-Haim, T. Ziv, D. Lamy, and R. M. Hodes. Nature and nurture in
own-race face processing. Psychological science, 17(2):159–163, 2006.
[10] B. Bastani, E. Turner, C. Vieri, H. Jiang, B. Funt, and N. Balram.
Foveated pipeline for AR/VR head-mounted displays. Information Dis-
play, 33(6):14–35, 2017.

[11] J. K. Bennett, S. Sridharan, B. John, and R. Bailey. Looking at faces:
autonomous perspective invariant facial gaze analysis. In Proceedings of
the ACM Symposium on Applied Perception, pp. 105–112. ACM, 2016.
[12] T. Booth, S. Sridharan, A. McNamara, C. Grimm, and R. Bailey. Guiding
attention in controlled real-world environments. In Proceedings of the
ACM Symposium on Applied Perception, pp. 75–82, 2013.

[13] Z. Boraston and S.-J. Blakemore. The application of eye-tracking technol-
ogy in the study of autism. The Journal of physiology, 581(3):893–898,
2007.

[14] A. Borji. Saliency prediction in the deep learning era: Successes and lim-
itations. IEEE transactions on pattern analysis and machine intelligence,
2019.

[15] E. Bozkir, O. G¨unl¨u, W. Fuhl, R. F. Schaefer, and E. Kasneci. Differential
privacy for eye tracking with temporal correlations. arXiv preprint
arXiv:2002.08972, 2020.

[16] E. Bozkir, A. B. ¨Unal, M. Akg¨un, E. Kasneci, and N. Pfeifer. Privacy
preserving gaze estimation using synthetic images via a randomized
encoding based framework. In Proceedings of the Symposium on Eye
Tracking Research and Applications, pp. 1–5, 2020.

[17] M. Bradley. Natural selective attention: orienting and emotion. Psy-

chophysiology, 46(1):1–11, 2009.

[18] J. Bradshaw, F. Shic, A. N. Holden, E. J. Horowitz, A. C. Barrett, T. C.
German, and T. W. Vernon. The use of eye tracking as a biomarker of
treatment outcome in a pilot randomized clinical trial for young children
with autism. Autism Research, 12(5):779–793, 2019.

[19] A. Burova, J. M¨akel¨a, J. Hakulinen, T. Keskinen, H. Heinonen, S. Sil-
tanen, and M. Turunen. Utilizing vr and gaze tracking to develop ar
solutions for industrial maintenance. In Proceedings of the 2020 CHI
Conference on Human Factors in Computing Systems, pp. 1–13, 2020.

[20] J. Chakareski, R. Aksu, X. Corbillon, G. Simon, and V. Swaminathan.
Viewport-driven rate-distortion optimized 360º video streaming. In 2018
IEEE International Conference on Communications (ICC), pp. 1–7. IEEE,
2018.

9

[21] F.-Y. Chao, L. Zhang, W. Hamidouche, and O. Deforges. Salgan360:
Visual saliency prediction on 360 degree images with generative adver-
sarial networks. In 2018 IEEE International Conference on Multimedia
& Expo Workshops (ICMEW), pp. 01–04. IEEE, 2018.

[22] A. K. Chaudhary and J. B. Pelz. Privacy-preserving eye videos using
rubber sheet model. In ACM Symposium on Eye Tracking Research &
Applications, pp. 1–5, 2020.

[23] K. Chawarska and F. Shic. Looking but not seeing: Atypical visual
scanning and recognition of faces in 2 and 4-year-old children with
autism spectrum disorder. Journal of autism and developmental disorders,
39(12):1663, 2009.

[24] D. Chen, C. Qing, X. Xu, and H. Zhu. Salbinet360: Saliency prediction
on 360° images with local-global bifurcated deep network. In 2020 IEEE
Conference on Virtual Reality and 3D User Interfaces (VR), pp. 92–100.
IEEE, 2020.

[25] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang.
When machine unlearning jeopardizes privacy. arXiv, pp. arXiv–2005,
2020.

[26] S. Cho, S.-w. Kim, J. Lee, J. Ahn, and J. Han. Effects of volumetric
capture avatars on social presence in immersive virtual environments. In
2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),
pp. 26–34. IEEE, 2020.

[27] K. M. Dalton, B. M. Nacewicz, T. Johnstone, H. S. Schaefer, M. A.
Gernsbacher, H. H. Goldsmith, A. L. Alexander, and R. J. Davidson.
Gaze ﬁxation and the neural circuitry of face processing in autism. Nature
neuroscience, 8(4):519–526, 2005.

[28] E. J. David, J. Guti´errez, A. Coutrot, M. P. Da Silva, and P. L. Callet. A
dataset of head and eye movements for 360 videos. In Proceedings of
the 9th ACM Multimedia Systems Conference, pp. 432–437. ACM, 2018.
[29] A. T. Duchowski, K. Krejtz, I. Krejtz, C. Biele, A. Niedzielska, P. Kiefer,
M. Raubal, and I. Giannopoulos. The index of pupillary activity: Mea-
suring cognitive load vis-`a-vis task difﬁculty with pupil oscillation. In
Proceedings of the 2018 CHI Conference on Human Factors in Comput-
ing Systems, pp. 1–13, 2018.

[30] A. T. Duchowski, V. Shivashankaraiah, T. Rawls, A. K. Gramopadhye,
B. J. Melloy, and B. Kanki. Binocular eye tracking in virtual reality for
inspection training. In Proceedings of the Symposium on Eye Tracking
Research & Applications, pp. 89–96, 2000.

[31] S. Eberz, G. Lovisotto, K. B. Rasmussen, V. Lenders, and I. Martinovic.
28 blinks later: Tackling practical challenges of eye movement biometrics.
In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, pp. 1187–1199, 2019.

[32] W. Fuhl. Reinforcement learning for the manipulation of eye tracking

data. arXiv preprint arXiv:2002.06806, 2020.

[33] C. Galdi, M. Nappi, D. Riccio, and H. Wechsler. Eye movement analysis
for human authentication: a critical survey. Pattern Recognition Letters,
84:272–283, 2016.

[34] C. Gebhardt, B. Hecox, B. van Opheusden, D. Wigdor, J. Hillis,
O. Hilliges, and H. Benko. Learning cooperative personalized poli-
cies from gaze data. In Proceedings of the 32nd Annual ACM Symposium
on User Interface Software and Technology, pp. 197–208, 2019.
[35] A. George and A. Routray. A score level fusion method for eye movement

biometrics. Pattern Recognition Letters, 82:207–215, 2016.

[36] A. Gkoulalas-Divanis, P. Kalnis, and V. S. Verykios. Providing k-
anonymity in location based services. ACM SIGKDD explorations
newsletter, 12(1):3–10, 2010.

[37] R. Graham, A. Hoover, N. A. Ceballos, and O. Komogortsev. Body mass
index moderates gaze orienting biases and pupil diameter to high and
low calorie food images. Appetite, 56(3):577–586, 2011.

[38] S. Grogorick, M. Stengel, E. Eisemann, and M. Magnor. Subtle gaze
guidance for immersive environments. In Proceedings of the ACM Sym-
posium on Applied Perception, pp. 1–7, 2017.

[39] J. Guti´errez, E. J. David, A. Coutrot, M. P. Da Silva, and P. Le Callet.
Introducing un salient360! benchmark: A platform for evaluating visual
attention models for 360 contents. In 2018 Tenth International Confer-
ence on Quality of Multimedia Experience (QoMEX), pp. 1–3. IEEE,
2018.

[40] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, and D. Manocha. Dgaze:
Cnn-based gaze prediction in dynamic scenes. IEEE transactions on
visualization and computer graphics, 26(5):1902–1911, 2020.

[41] Z. Hu, C. Zhang, S. Li, G. Wang, and D. Manocha. SGaze: A data-
driven eye-head coordination model for realtime gaze prediction. IEEE
Transactions on Visualization and Computer Graphics, 25(5):2002–2010,

2019.

[42] B. John, S. Jorg, S. Koppal, and E. Jain. The security-utility trade-off
for iris authentication and eye animation for social virtual avatars. IEEE
transactions on visualization and computer graphics, 2020.

[43] B. John, S. Kalyanaraman, and E. Jain. Look out! a design framework
for safety training systems a case study on omnidirectional cinemagraphs.
In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces
Abstracts and Workshops (VRW), pp. 147–153. IEEE, 2020.

[44] B. John, S. Koppal, and E. Jain. EyeVEIL: degrading iris authentication
in eye tracking headsets. In ACM Symposium on Eye Tracking Research
& Applications, p. 37. ACM, 2019.

[45] B. John, A. Liu, L. Xia, S. Koppal, and E. Jain. Let it snow: Adding pixel
noise to protect the user’s identity. In Proceedings of the Symposium on
Eye Tracking Research and Applications, pp. 1–3, 2020.

[46] B. John, P. Raiturkar, O. Le Meur, and E. Jain. A benchmark of four
methods for generating 360◦ saliency maps from eye tracking data. In-
ternational Journal of Semantic Computing, 13(03):329–341, 2019.
[47] P. Kasprowski and K. Harezlak. The second eye movements veriﬁcation
and identiﬁcation competition. In IEEE International Joint Conference
on Biometrics, pp. 1–6. IEEE.

[48] P. Kasprowski, O. V. Komogortsev, and A. Karpov. First eye movement
veriﬁcation and identiﬁcation competition at btas 2012. In 2012 IEEE
ﬁfth international conference on biometrics: theory, applications and
systems (BTAS), pp. 195–202. IEEE, 2012.

[49] M. Keyvanara and R. Allison. Transsaccadic awareness of scene trans-
formations in a 3d virtual environment. In ACM Symposium on Applied
Perception 2019, pp. 1–9, 2019.

[50] M. Keyvanara and R. Allison. Effect of a constant camera rotation
on the visibility of transsaccadic camera shifts. In Proceedings of the
Symposium on Eye Tracking Research and Applications, pp. 1–8, 2020.
[51] J. L. Kr¨oger, O. H.-M. Lutz, and F. M¨uller. What does your gaze re-
veal about you? on the privacy implications of eye tracking. In IFIP
International Summer School on Privacy and Identity Management, pp.
226–241. Springer, 2019.

[52] B. Laeng and L. Falkenberg. Women’s pupillary responses to sexually
signiﬁcant others during the hormonal cycle. Hormones and behavior,
52(4):520–530, 2007.

[53] P. Lang, M. Greenwald, M. M. Bradley, and A. O. Hamm. Looking at
pictures: affective, facial, visceral, and behavioral reactions. Psychophys-
iology, 30(3):261–73, 1993.

[54] E. Langbehn, F. Steinicke, M. Lappe, G. F. Welch, and G. Bruder. In the
blink of an eye: Leveraging blink-induced suppression for imperceptible
position and orientation redirection in virtual reality. ACM Transactions
on Graphics (TOG), 37(4):66, 2018.

[55] O. Le Meur and T. Baccino. Methods for comparing scanpaths and
saliency maps: strengths and weaknesses. Behavior research methods,
45(1):251–266, 2013.

[56] R. J. Leigh and D. S. Zee. The neurology of eye movements. Oxford

University Press, USA, 2015.

[57] C. Li, M. Xu, X. Du, and Z. Wang. Bridge the gap between vqa and
human behavior on omnidirectional video: A large-scale dataset and
a deep learning model. In Proceedings of the 26th ACM international
conference on Multimedia, pp. 932–940, 2018.

[58] J. Li, A. R. Chowdhury, K. Fawaz, and Y. Kim. Kalεido: Real-time
privacy control for eye-tracking systems. In 29th {USENIX} Security
Symposium ({USENIX} Security 20), 2020.

[59] A. Liu, L. Xia, A. Duchowski, R. Bailey, K. Holmqvist, and E. Jain.
Differential privacy for eye-tracking data. In ACM Symposium on Eye
Tracking Research & Applications, p. 28. ACM, 2019.

[60] D. J. Lohr, S. Aziz, and O. Komogortsev. Eye movement biometrics
using a new dataset collected in virtual reality. In Proceedings of the
Symposium on Eye Tracking Research and Applications, pp. 1–3, 2020.
[61] S. Lombardi, J. Saragih, T. Simon, and Y. Sheikh. Deep appearance
models for face rendering. ACM Transactions on Graphics (TOG),
37(4):68, 2018.

[62] P. Longhurst, K. Debattista, and A. Chalmers. A gpu based saliency map
for high-ﬁdelity selective rendering. In Proceedings of the 4th interna-
tional conference on Computer graphics, virtual reality, visualisation
and interaction in Africa, pp. 21–29, 2006.

[63] P. Lungaro, R. Sj¨oberg, A. J. F. Valero, A. Mittal, and K. Tollmar. Gaze-
aware streaming solutions for the next generation of mobile VR expe-
riences. IEEE Transactions on Visualization and Computer Graphics,
24(4):1535–1544, 2018.

10

To appear in IEEE Transactions on Visualization and Computer Graphics

[64] A. MacQuarrie and A. Steed. Perception of volumetric characters’ eye-
gaze direction in head-mounted displays. In Proceedings of 2019 IEEE
Virtual Reality (VR), vol. 2019. IEEE, 2019.

[65] C. Marforio, H. Ritzdorf, A. Francillon, and S. Capkun. Analysis of the
communication between colluding applications on modern smartphones.
In Proceedings of the 28th Annual Computer Security Applications Con-
ference, pp. 51–60, 2012.

[66] X. Meng, R. Du, and A. Varshney. Eye-dominance-guided foveated
rendering. IEEE Transactions on Visualization and Computer Graphics,
26(5):1972–1980, 2020.

[67] X. Meng, R. Du, M. Zwicker, and A. Varshney. Kernel foveated render-
ing. Proceedings of the ACM on Computer Graphics and Interactive
Techniques, 1(1):1–20, 2018.

[68] J. V. Monaco. Classiﬁcation and authentication of one-dimensional behav-
ioral biometrics. In IEEE International Joint Conference on Biometrics,
pp. 1–8. IEEE, 2014.

[69] J. Morris, S. Smalley, and G. Kroah-Hartman. Linux security modules:
General security support for the linux kernel. In Proceedinsg of the 2002
USENIX Security Symposium, 2002.

[70] C. Mousas, A. Koilias, D. Anastasiou, B. Hekabdar, and C.-N. Anag-
nostopoulos. Effects of self-avatar and gaze on avoidance movement
behavior. In 2019 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR), pp. 726–734. IEEE, 2019.

[71] J. H. Mueller, P. Voglreiter, M. Dokter, T. Neff, M. Makar, M. Steinberger,
and D. Schmalstieg. Shading atlas streaming. In SIGGRAPH Asia 2018
Technical Papers, p. 199. ACM, 2018.

[72] P. Mundy. A review of joint attention and social-cognitive brain systems
in typical development and autism spectrum disorder. European Journal
of Neuroscience, 47(6):497–514, 2018.

[73] D. Munoz, J. Broughton, J. Goldring, and I. Armstrong. Age-related
performance of human subjects on saccadic eye movement tasks. Experi-
mental brain research, 121(4):391–400, 1998.

[74] M. Murcia-L´opez, T. Collingwoode-Williams, W. Steptoe, R. Schwartz,
T. J. Loving, and M. Slater. Evaluating virtual reality experiences through
participant choices. In 2020 IEEE Conference on Virtual Reality and 3D
User Interfaces (VR), pp. 747–755. IEEE, 2020.

[75] J. Orlosky, Y. Itoh, M. Ranchet, K. Kiyokawa, J. Morgan, and H. Devos.
Emulation of physician tasks in eye-tracked virtual reality for remote
diagnosis of neurodegenerative disease. IEEE Transactions on Visualiza-
tion and Computer Graphics, 23(4):1302–1311, 2017.

[76] J. L. Orquin, N. J. Ashby, and A. D. Clarke. Areas of interest as a
signal detection problem in behavioral eye-tracking research. Journal of
Behavioral Decision Making, 29(2-3):103–115, 2016.

[77] Y. S. Pai, B. I. Outram, B. Tag, M. Isogai, D. Ochi, and K. Kunze.
Gazesphere: Navigating 360-degree-video environments in VR using
head rotation and eye gaze. In ACM SIGGRAPH 2017 Posters, p. 23.
ACM, 2017.

[78] G. Papaioannou and I. Koutsopoulos. Tile-based caching optimization
In Proceedings of the Twentieth ACM International
for 360 videos.
Symposium on Mobile Ad Hoc Networking and Computing, pp. 171–180,
2019.

[79] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Lue-
bke, and A. Lefohn. Towards foveated rendering for gaze-tracked virtual
reality. ACM Transactions on Graphics (TOG), 35(6):179, 2016.
[80] K. A. Pelphrey, J. P. Morris, and G. McCarthy. Neural basis of eye gaze

processing deﬁcits in autism. Brain, 128(5):1038–1048, 2005.

[81] Y. Rahman, S. M. Asish, N. P. Fisher, E. C. Bruce, A. K. Kulshreshth, and
C. W. Borst. Exploring eye gaze visualization techniques for identifying
distracted students in educational vr. In 2020 IEEE Conference on Virtual
Reality and 3D User Interfaces (VR), pp. 868–877. IEEE, 2020.
[82] Y. Rai, J. Guti´errez, and P. Le Callet. A dataset of head and eye move-
ments for 360 degree images. In Proceedings of the 8th ACM on Multi-
media Systems Conference, pp. 205–210. ACM, 2017.

[83] P. Raiturkar, A. Kleinsmith, A. Keil, A. Banerjee, and E. Jain. Decoupling
light reﬂex from pupillary dilation to measure emotional arousal in videos.
In Proceedings of the ACM Symposium on Applied Perception, pp. 89–96,
2016.

[84] V. Rajanna and J. P. Hansen. Gaze typing in virtual reality: impact of
keyboard design, selection method, and motion. In Proceedings of the
Symposium on Eye Tracking Research and Applications, p. 15. ACM,
2018.

of eyes and genitals. Biological Psychology, 104:56–64, 2015.

[86] I. Rigas, E. Abdulin, and O. Komogortsev. Towards a multi-source fusion
approach for eye movement-driven recognition. Information Fusion,
32:13–25, 2016.

[87] I. Rigas and O. V. Komogortsev. Current research in eye movement
biometrics: An analysis based on bioeye 2015 competition. Image and
Vision Computing, 58:129–141, 2017.

[88] S. Rothe, F. Althammer, and M. Khamis. Gazerecall: Using gaze di-
rection to increase recall of details in cinematic virtual reality. In Pro-
ceedings of the 17th International Conference on Mobile and Ubiquitous
Multimedia, pp. 115–119, 2018.

[89] S. Rothe, D. Buschek, and H. Hußmann. Guidance in cinematic virtual
reality-taxonomy, research status and challenges. Multimodal Technolo-
gies and Interaction, 3(1):19, 2019.

[90] R. Sailer, X. Zhang, T. Jaeger, and L. Van Doorn. Design and Implementa-
tion of a TCG-based Integrity Measurement Architecture. In Proceedings
of the 2004 USENIX Security Symposium, 2004.

[91] D. D. Salvucci and J. H. Goldberg. Identifying ﬁxations and saccades
In Proceedings of the Symposium on Eye

in eye-tracking protocols.
Tracking Research & Applications, pp. 71–78, 2000.

[92] N. Sammaknejad, H. Pouretemad, C. Eslahchi, A. Salahirad, and
A. Alinejad. Gender classiﬁcation based on eye movements: A process-
ing effect during passive face viewing. Advances in cognitive psychology,
13(3):232, 2017.

[93] C. Schr¨oder, S. M. K. Al Zaidawi, M. H. Prinzler, S. Maneth, and
G. Zachmann. Robustness of eye movement biometrics against varying
stimuli and varying trajectory length. In Proceedings of the 2020 CHI
Conference on Human Factors in Computing Systems, pp. 1–7, 2020.

[94] F. Schwenker, H. A. Kestler, and G. Palm. Three learning phases for
radial-basis-function networks. Neural networks, 14(4-5):439–458, 2001.
[95] V. Sitzmann, A. Serrano, A. Pavel, M. Agrawala, D. Gutierrez, B. Masia,
and G. Wetzstein. Saliency in VR: How do people explore virtual envi-
ronments? IEEE Transactions on Visualization and Computer Graphics,
24(4):1633–1642, 2018.

[96] S. Sridharan, R. Bailey, A. McNamara, and C. Grimm. Subtle gaze
manipulation for improved mammography training. In Proceedings of
the Symposium on Eye Tracking Research and Applications, pp. 75–82,
2012.

[97] J. Steil, I. Hagestedt, M. X. Huang, and A. Bulling. Privacy-aware eye
tracking using differential privacy. In ACM Symposium on Eye Tracking
Research & Applications. ACM, 2019.

[98] J. Steil, M. Koelle, W. Heuten, S. Boll, and A. Bulling. Privaceye:
privacy-preserving head-mounted eye tracking using egocentric scene
image and eye movement features. In ACM Symposium on Eye Tracking
Research & Applications, p. 26. ACM, 2019.

[99] Q. Sun, A. Patney, L.-Y. Wei, O. Shapira, J. Lu, P. Asente, S. Zhu,
M. Mcguire, D. Luebke, and A. Kaufman. Towards virtual reality inﬁnite
walking: dynamic saccadic redirection. ACM Transactions on Graphics
(TOG), 37(4):67, 2018.

[100] S. Uzzaman and S. Joordens. The eyes know what you are thinking: eye
movements as an objective measure of mind wandering. Consciousness
and cognition, 20(4):1882–1886, 2011.

[101] M. Xu, C. Li, S. Zhang, and P. Le Callet. State-of-the-art in 360
video/image processing: Perception, assessment and compression. IEEE
Journal of Selected Topics in Signal Processing, 14(1):5–26, 2020.
[102] Y. Xu, Y. Dong, J. Wu, Z. Sun, Z. Shi, J. Yu, and S. Gao. Gaze prediction
in dynamic 360◦ immersive videos. In Proceedings of IEEE CVPR 2018,
pp. 5333–5342, 2018.

[103] C. Yangandul, S. Paryani, M. Le, and E. Jain. How many words is a
picture worth? attention allocation on thumbnails versus title text regions.
In ACM Symposium on Eye Tracking Research & Applications, pp. 1–5,
2018.

[104] R. Zemblys and O. Komogortsev. Developing photo-sensor oculography
(PS-OG) system for virtual reality headsets. In ACM Symposium on Eye
Tracking Research & Applications, p. 83. ACM, 2018.

[105] R. Zemblys, D. C. Niehorster, O. Komogortsev, and K. Holmqvist. Using
machine learning to detect events in eye-tracking data. Behavior research
methods, 50(1):160–181, 2018.

[106] A. T. Zhang and B. O. Le Meur. How old do you look? inferring your
age from your gaze. In 2018 25th IEEE International Conference on
Image Processing (ICIP), pp. 2660–2664. IEEE, 2018.

[85] G. Rieger, B. M. Cash, S. M. Merrill, J. Jones-Rounds, S. M. Dhar-
mavaram, and R. C. Savin-Williams. Sexual arousal: The correspondence

[107] G. Zhang and J. P. Hansen. Accessible control of telepresence robots
based on eye tracking. In ACM Symposium on Eye Tracking Research &

11

Applications, p. 50. ACM, 2019.

12

