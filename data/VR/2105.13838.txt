1
2
0
2

y
a
M
8
2

]

O
R
.
s
c
[

1
v
8
3
8
3
1
.
5
0
1
2
:
v
i
X
r
a

It’s your turn! - A collaborative human-robot pick-and-place
scenario in a virtual industrial setting
Christine Busch
Martina Mara
Kathrin Meyer
LIT Robopsychology Lab,
Johannes Kepler University
Linz, Austria
ﬁrstname.lastname@jku.at

Brigitte Krenn
Tim Reinboth
Stephanie Gross
Austrian Research Institute for
Artiﬁcial Intelligence
Vienna, Austria
ﬁrstname.lastname@ofai.at

Michael Heiml
Thomas Layer-Wagner
Polycular e.U.
Hallein, Austria

ﬁrstname.lastname@polycular.com

ABSTRACT
In human-robot collaborative interaction scenarios, nonverbal
communication plays an important role. Both, signals sent by
a human collaborator need to be identiﬁed and interpreted by
the robotic system, and the signals sent by the robot need to
be identiﬁed and interpreted by the human. In this paper, we
focus on the latter. We implemented on an industrial robot in
a VR environment nonverbal behavior signalling the user that
it is now their turn to proceed with a pick-and-place task. The
signals were presented in four different test conditions: no sig-
nal, robot arm gesture, light signal, combination of robot arm
gesture and light signal. Test conditions were presented in two
rounds to the participants. The qualitative analysis was con-
ducted with focus on (i) potential signals in human behaviour
indicating why some participants immediately took over from
the robot whereas others needed more time to explore, (ii) hu-
man reactions after the nonverbal signal of the robot, and (iii)
whether participants showed different behaviours in the differ-
ent test conditions. We could not identify potential signals why
some participants were immediately successful and others not.
There was a bandwidth of behaviors after the robot stopped
working, e.g. participants rearranged the objects, looked at
the robot or the object, or gestured the robot to proceed. We
found evidence that robot deictic gestures are helpful for the
human to correctly interpret what to do next. Moreover, there
was a strong tendency that humans interpreted the light signal
projected on the robot’s gripper as a request to give the object
in focus to the robot. Whereas a robot’s pointing gesture at
the object was a strong trigger for the humans to look at the
object.

Author Keywords
human-cobot interaction; pick-and-place actions; non-verbal
signalling; virtual reality
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
HRI’21, Workshop "Exploring Applications for Autonomous Non-Verbal
Human-Robot Interactions" March 8, 2021
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-2138-9.
DOI: https://doi.org/10.1145/3313831.XXXXXXX

INTRODUCTION AND BACKGROUND WORK
As collaborative robots (cobots) become more prevalent in
industries, investigating the interactions between humans and
cobots becomes relevant, especially as there is strong evidence,
that humans respond socially to computers [7]. In this context,
the investigation of robots’ nonverbal communicative signals is
of particular importance to foster user acceptance and enhance
user experience as well as to facilitate successful joint task
completion.

In this paper, we qualitatively investigate human reactions to
a "it’s your turn" signal from an industrial robot in virtual
reality. In this human-robot-collaboration experiment, a UR10
robot is signalling to a human during a pick-and-place task
that it is now their turn to put small boxes from a conveyor belt
in a cardboard box. In our research, we focus on nonverbal
interaction, due to the unreliability of speech in task descrip-
tions. In situated tasks, verbal descriptions of objects and
actions are often imprecise and partially incorrect [5, 4]. Al-
though not problematic for humans, this raises a challenge for
automatic processing in robots. In addition, problems of auto-
matic speech recognition systems in typically noisy industrial
working environments speak against verbal interactions.

In general, nonverbal communication is the "unspoken dia-
logue" that creates shared meaning in social interactions [2].
There are several categorizations of nonverbal communica-
tion in distinct, although socially interrelated modes, e.g. in
kinesics, proxemics, haptics, chronemics, vocalics, and presen-
tation [6]. In the present paper, we focus on kinesics, a highly
articulated form of nonverbal communication, speciﬁcally arm
gestures. Saunderson et al. [13] categorize kinesics-based
robotics research into (i) arm gestures, (ii) body and head
movements, (iii) eye gaze, and (iv) facial expressions.

El Zaatari et al. [3] mention four types of interaction in in-
dustrial human-cobot collaboration: independent – a worker
and a cobot operate on separate work pieces; simultaneous – a
worker and a cobot operate on separate processes on the same
work piece at the same time; sequential – a worker and a cobot
perform sequential manufacturing processes on the same work
piece; supportive – a worker and a cobot work towards the
same process on the same work piece interactively. The signal

 
 
 
 
 
 
"it’s your turn" is potentially relevant for both sequential and
supportive collaboration.

Studies have shown that robot arm gestures are inﬂuential in
improving the response time in interactive tasks. In a coopera-
tion task, Riek et al. [12] investigated how humans might react
to three robot arm gestures: beckoning, giving, shaking hands.
The gestures were displayed by BERTI, a humanoid robot,
manipulating the gesture style (abruptly or smoothly) and the
gesture orientation (front or side). They were presented to the
participants via videos, who then had to pick an action based
on what the participant thought the robot is asking him or her
to do. Results showed that people react to abrupt gestures
more quickly than smooth ones and front-oriented gestures
more quickly than side oriented gestures.

Quintero et al. [11] investigated pointing gestures in human-
human and human-robot interaction while collaboratively per-
forming manipulation tasks. Feedback via predeﬁned "yes"
and "no" conﬁrmation gestures allowed the robot (a seven-
degrees-of-freedom robot arm) or the human to verify that the
right object was selected. Their results showed 28% misinter-
pretations of robot pointing gestures by humans, 2% misinter-
pretations of human pointing gestures by the robotic system
and 10% misinterpretations in human-human interaction.

Sheikholeslami et al. [14] explored the efﬁcacy of nonverbal
communication capabilities of a robotic manipulator in a col-
laborative car door assembly task. First, the authors observed
the type of hand conﬁgurations that humans use to nonver-
bally instruct another person. Then they examined how well
human gestures with frequently used hand conﬁgurations are
understood by the recipients. Subsequently they implemented
the most human-recognized human hand conﬁgurations on a
seven-degrees-of-freedom robotic manipulator to investigate
the efﬁcacy of human-inspired hand poses on a three-ﬁngered
gripper. In two video-based online surveys, human and robot
gestures were presented to participants. Out of the 14 se-
lected and implemented gestures, recognition rate of human
gestures was greater than 90%. 10 implemented gestures were
relatively well recognized (recognition rate > 60%) by the
participants. However, four gestures yielded lower recognition
rates (left, right, swap, conﬁrm), possibly due to the angle
of the arm relative to the assembly object, the complexity of
the task or physical limitations of the robot gripper such as
lacking a thumb.

We implemented our experimental setting in virtual reality
(VR), as it is a seminal technology offering several advantages
not only for human-robot-interaction research [8, 10] but also
for training courses and education [9, 1]. For trainings, the
safety aspect, the high degree of immersion and the thus result-
ing effectivity of trainings for low cost are emphasized by [9]
and [1]. Oyekan et al. [8] argue that due to high velocity and
massive force of industrial robots, VR is an effective surround-
ing to investigate human reactions towards predictable and
unpredictable robotic movements, and to develop strategies
and solutions based on these reactions. Peters et al. [10] em-
phasize the advantages of less expensive and time consuming
HRI experiments in VR.

In the presented paper, we qualitatively investigated human
reactive behaviours to a cobot displaying arm (beckoning,
pointing) and light gestures as turn-giving signals.

In general, the idea of focusing on sequences where the human
does not take the parcel and put it into the box immediately
after the robot stopped working, is that there may be more to
learn about participants’ behaviours during the task from in-
stances in which participants were not immediately successful,
rather than from those in which participants quickly succeeded
in the task. The eye-gaze, motion capture and video recordings
each provide data about what participants did, as they, presum-
ably, tried to ﬁgure out what was going on, when the robotic
arm stopped working. Whereas immediate success provides
little data about how participants approached the situation,
the recordings of all other trials provide rich data about what
participants did in response to the robot’s inaction. However,
even in a relatively constrained environment such as the one
reported her, human behaviour remains difﬁcult to interpret.
Even more elusive is why a particular human acted a certain
way, meaning what exactly motivated the action and what that
action was intended to achieve. It is instructive to bear in
mind, that this way of thinking about human action deals with
voluntary action in particular, while involuntary behaviours
present further challenges of their own. That is why, rather
than trying to infer intentions and chains of thought into the
minds of the participants, we ﬁrst turned our attention to what
visibly happened.

VR GAME SCENARIO
The game plays in a virtual rubber duck factory. Human and
robot stand in front of each other separated by a conveyor belt.
The robot is a digital twin, a 1:1 replica of the CHIMERA1
mobile manipulator comprising a UR10 collaborative robot
arm with a two-ﬁnger gripper mounted on a MiR100 mobile
platform, having the same capabilities and looks as in real
life. The conveyor belt transports parcels containing rubber
ducks which need to be taken from the belt and put into a
cardboard box which is positioned next to the robot. The
task of the human is to watch the scene and help the robot
when needed. Apart from her/his face to face position with
the robot, the human has a 360 degree view of the factory hall,
seeing the individual parcels coming from one side and the
cardboard boxes ﬁlled with the duck parcels being transported
away by a second conveyor belt. See Figure 1 for a view on
the scenario from the human’s perspective. Equipped with an
optical hand tracking module on the VR headset, the human
is able to take the parcel from the conveyor belt and to put it
into the cardboard box next to the robot. S/he is also able to
reorganize the parcels in the box.

The typical game scenario is that the robot is working and the
human is watching, i.e., the robot takes parcel per parcel from
the conveyor belt and puts them into the box. At some point,
the robot stops working and it is the human’s turn to take
action. The goal of the experimental setup is to test whether
the human understands that it is now her/his turn and that s/he
should do the robot’s work. Apart from the situation where

1https://www.joanneum.at/en/robotics/infrastructure/
mobile-manipulation

control condition). The rationale for choosing this setup was
to assess potential carry-over effects between game rounds.

Procedure & Task
Once they had been welcomed, participants signed their agree-
ment to the experimental procedure and ﬁlled out a question-
naire. Next, they received an introduction to the VR game
CoBot Studio as well as to the VR equipment by a research
assistant. Afterwards, participants put on the VR equipment
which was then calibrated. If the participants indicated that
they felt comfortable, an interactive tutorial sequence was
started which helped participants to get accustomed to the VR
environment. The packaging task took place as one of three
different mini games within the CoBot Studio VR game that
the participants subsequently played in random order. When
starting the packaging task mini game, participants received
an introduction and were informed about their task by a disem-
bodied instructor. To account for as much variance in behavior
as possible to the received signal cue, task information was
kept short: The participants were only informed that they
should watch the robot during its work and assist if needed.

After participants indicated that they were ready to start, the
ﬁrst mini game round started and proceeded as previously
outlined. In each game round, the participant had 60 seconds to
grab the rubber duck parcel after the robot indicated her/him to
take over. If the ﬁrst 60 seconds were exceeded (= failure), the
participant received an information that the task had not been
completed correctly. The game round was then terminated. If
the participant grabbed the rubber duck parcel within the 60
second countdown, s/he was given another 60 seconds to put
the parcel in the shipping carton (= success) before the game
round was terminated. After completing two game rounds,
participants received an outro and continued with a different
mini game. Upon ﬁnishing the CoBot Studio VR game, they
ﬁlled out another questionnaire, were thanked and rewarded
with a miniature rubber duck.

SAMPLE
A total of n = 133 participants were recruited at an Austrian
University and during a research exhibition. However, due to
technical problems, the video ﬁles of nine participants were
either not utilizable at all or defective regarding one of the
game rounds. Hence, the analyses for round 1 and 2 were
conducted on a data set of 124 participants respectively. Re-
garding their gender and age, the participant groups slightly
differed in the two game rounds. In round 1, 55 (44%) were
female, 68 (55%) were male and one person selected the op-
tion “other”. The mean age of participants in round 1 was
M = 31.18 (SD = 9.90), with a range from 14 to 65 years.
In round 2, 54 (44%) were female, 69 (56%) were male and
one person selected the option “other”. Their mean age was
M = 31.14 years (SD = 9.91), again ranging from 14 to 65
years. The participant groups did not vary regarding their
prior experience in working with industrial robots and their
experience with VR headsets. 21 (17%) stated that they had
previously worked with an industrial robot, while 100 (81%)
had not previously worked with an industrial robot and 3 (2%)
stated that they were unsure. Moreover, 35 (28%) had never

Figure 1. Screenshot: Person’s view on the packaging hall.

the robot arm stops with its gripper above the parcel (control
condition), two types of signals were explored: (i) the robot
arm produces human-like pointing – ﬁrst at the human, then
at the parcel. The signal is meant to indicate that it is now
the human’s turn to pick up the parcel from the conveyor belt
and to put it into the cardboard box; (ii) a light signal, i.e., a
projection of downwards running arrows on the gripper. For
more details on the experimental setup and the conditions
tested see the following section.

EXPERIMENTAL SETUP
To explore the interpretability of different motion- and light-
based signals, we conducted a mixed design experiment in
which participants collaborated on a packaging task with a
mobile sensitive manipulator in a virtual reality mini game.
This mini game is part of the research-based VR game CoBot
Studio2, taking place in a virtual rubber duck factory.

Manipulated Variables
We manipulated one between-subjects factor (“it’s your turn”
signal variant presented by the robot) and one within-subjects
factor (number of rubber ducks presented per round). The
signal variant was manipulated in the following four levels:

• Variant 1 (V1, control group): gripper stops above parcel

with no further signaling.

• Variant 2 (V2): arm gestures, i.e., the robot arm points at
the person, then at the parcel containing the rubber duck.

• Variant 3 (V3): gripper stops above parcel and demonstrates

light signals – blue arrows running down the gripper.

• Variant 4 (V4): combination of robot arm gestures and light

signal.

Participants collaborated with the robot in two game rounds
which were completed in randomized order and varied regard-
ing the number of rubber duck parcels being packaged by the
robot before the participant was supposed to take over. The
robot took either three or two rubber duck parcels from a con-
veyor belt and put them in a cardboard box, before signaling
the participants to continue (or before simply stopping in the

2For more
lit-robopsychology-lab/research/projects/cobot-studio/

https://www.jku.at/en/

information,

see

used a VR headset, 50 (40%) had once used a VR headset, and
39 (32%) were regularly using VR headsets.

QUALITATIVE STUDY
For each participant and experimental variant (V1-V4) a video
was recorded in VR covering the following 8 perspectives
(see Figure 5 from left to right): top – a bird’s eye view on
robot and user; side – view from the left side; overshoulder –
from over the head of the user; top-angular – from behind the
robot, low to high; side-angular – from the left side, elevated;
perspective – from the left side, diagonal and elevated; point of
view user – ﬁrst person perspective human; point of view robot
– from behind the robot at human head level. The yellow dot
represents the head of the user, the red line indicates the eye
gaze coming from the eye tracker built into the VR headset,
and the red circle indicates the focus point of the human gaze.
The black device is the control stick the user holds in her/his
left or right hand. The user’s respective other hand is only
visible in the "point of view user" video whenever the hand is
in the user’s ﬁeld of vision. This way, we can also see when
the human grabs a parcel and what s/he does with it.

In the qualitative analysis, we were interested in (i) potential
signals in the human behaviour that could function as indi-
cators for distinguishing immediately successful users from
those who needed more time or failed to stand in for the robot
and put the parcel into the cardboard box; (ii) human reac-
tions after the robot had stopped working; (iii) whether the
participants showed different behaviours in reaction to the test
conditions V1 to V4. As regards (i), we could not ﬁnd any
indicators in the users’ overt behaviour that would give hint
on who would more or less quickly grasp the task.

In fact, people were quite good in grasping what kind of help
the robot needed in all conditions. See Figure 2 showing that
over 90% of the participants in conditions V1-V3 were already
successful during the ﬁrst round and 100% were successful
in the second round. The data also show a tendency that
deictic gesturing of the robot arm supports humans to grasp
that they should put the parcel in the box (94% success rate in
V2 round 1). Interestingly, participants in condition V4 were
less successful (86% success in round 1, and 91% in round 2,
which are both worse than the results in conditions V1-V3).
As it seems, the combination of deictic gestures of the robot
arm with the particular light signals, arrows running down the
gripper, rather impairs than supports human understanding
of the robot’s intention, especially when already in doubt.
For those, however, who were immediately successful, the
combined signal in V4 seems to be a useful indicator, as two
times more participants were immediately successful in V4
round 1 as compared to the ﬁrst rounds in conditions V1-V3.
See Figure 3 for illustration. Overall, only a small number of
participants was immediately successful in ﬁrst rounds, 20%
of the participants in conditions V1-V3, 40% in condition V4.
Whereas it were 80% or more in round 2 of V1-V3. Here, V4
negatively stands out with only 71% of the participants being
immediately successful in round 2.

Human behaviours after the robot had stopped working
For the analysis of the behaviours the humans showed after the
robot had stopped working, we concentrated on videos from

Condition

V1
V2
V3
V4

Round 1
#s
31
31
20
30

#is
6
7
4
12

#
34
33
22
35

Round 2
#s
35
33
22
31

#is
28
28
18
22

#
35
33
22
34

Table 1. Number of participants per round and test condition; # to-
tal number of persons in condition and round; #s number of persons
successful per condition and round; #is number of persons immediately
successful per condition and round.

Figure 2. Comparison of successes rates (%) for the 4 study conditions
V1-V4. We distinguish overall success within the 1st and 2nd round.
Overall success is deﬁned as follows: after the robot had stopped work-
ing, the participant grabs the parcel and puts it into the cardboard box,
any time before the time out.

participants who were not immediately successful in their ﬁrst
round per condition, and found the following behaviours in
different frequencies:

• Human looks at robot and waits (A)
• Human tries to give the parcel to the robot (B)
• Human rearranges the parcel (C)
• Human tries to move the robot arm (D)
• Human looks into the cardboard box before acting (E)
• Human looks back and forth between robot and parcel (F)
• Human searches for a signal in the environment (G)
• Human looks at parcel arriving in front of the robot (H)
• Human generally tries to intervene and take parcels off the
conveyor belt already before the robot stops working (I)

Figure 3. Comparison of successes rates (%) between the 4 study con-
ditions V1-V4. We distinguish immediate success within the 1st and 2nd
round. Immediate success is deﬁned as follows: immediately after the
robot had stopped working, the participant grabs the parcel and puts it
into the cardboard box.

Behaviour

F
E
N
B
H
G
L
A
Persons in
Condition

Conditions
V1 V2 V3 V4
21
21
12
16
9
14
14
13
19
13
7
10
8
11
3
9

14
5
7
15
12
11
4
4

20
4
4
18
22
0
0
1

25

24

16

23

Table 2. Behaviours and occurrence frequencies per condition; only
those trials were considered where the human was not immediately suc-
cessful in the respective ﬁrst rounds.

Figure 4. Frequent behaviours after robot had stopped working per ﬁrst
rounds of conditions V1-V4.

• Human already grabs the parcel before the robot stops (J)
• Human gaze follows the parcel as it approaches on the

conveyor belt (K)

• Human makes gestures as if to say to the robot "go on

working" (L)

• Human tries previously learned system command gestures
(thumbs up, palms up) which are for instance used to signal
that the person is ready (M)

• Human rearranges the parcels in the cardboard box (N)

The occurrence frequency of these signals differs greatly. In
the following, we will discuss those with comparatively high
occurrence in at least one of the conditions V1-V4. See Ta-
ble 2 for the frequency counts and Figure 4 for a graphical
representation of the differences between the conditions.

From Figure 4, we see that there are some behaviours the
participants show more often than others. The most frequent
behaviour in all conditions was "the human looks back and
forth between robot and parcel" (F), raging from 83% in V2 to
91% in V4. Moreover 94% of the persons in V3, 75% in V2,
and still 61% in V4 and 52% in V1 try to give the parcel to the
robot (B). Thus it seems that the light signal only (V3) was
taken as a strong indicator for the humans to give the parcel
to the robot. 92% in V2, 83% in V4 and 75% in V3 look at
the parcel at the time it arrives in front of the robot (H), which
may be an indicator that speciﬁc signals such as deixis and
or light as compared to no signal draw the human attention
to the object in focus. 69% in V3 search for a signal in the
environment, which may be due to a learning effect carrying
over from other game sequences. In V1 and V4, more than
half of the humans (64% V1, 52% V4) look into the cardboard
box before they set an action (E). This was much less the case
in V2 (17%) and V3 (31%). 56% in V1 rearrange the parcels
in the cardboard box (N). Which is only done by 17% in V2.
Overall, behaviours G, L, A, E and N were not at all or only
rarely shown in V2, which we take as an indicator that robot
deictic gestures are helpful.

objects. The signals were varied in four conditions: no signal
(control group), robot arm gesture, light signal, combination
of robot arm gesture and light signal.

Between 20% and 40% of participants had immediate suc-
cess in interpreting the signal of the robot in the ﬁrst round.
Between 70% and 85% were immediately successful in the
second round. In general, more than 85% of the participants
were successful in the ﬁrst round and 90% up to 100% in
the respective second round. While the conditions with no
signal, a robot arm gesture and a light gesture were relatively
comparable, the fourth condition combining robot arm gesture
and light signal stands out. While in the ﬁrst round 40% of
participants were relatively successful (instead of 20% in the
other rounds), in the second round it was only 71% (instead
of 80% or more in the other conditions). In order to gain
more insights in this phenomenon, future research is needed.
Another research question requiring further investigation is
why some participants were immediately successful, while
others needed more time or even failed. Different behaviours
were exhibited if participants were not immediately successful,
such as rearranging the objects, looking at the robot or the ob-
ject, gesturing the robot to continue, trying to move the robot
arm etc. We found evidence for the relevance of robot deictic
gestures as source of information for the human collaboration
partner what to do next. Moreover, we found evidence for a
light signal projected on the gripper to be interpreted by the
human as the robot’s request to hand over the object in focus.
However, this should be explored in further experiments and
via interviews following the interaction, to gain insights in par-
ticipants interpretations and expectations of industrial robots
and their signals in speciﬁc application scenarios.

ACKNOWLEDGMENTS
This work was supported by the Austrian Research Promotion
Agency (FFG), Ideen Lab 4.0 project CoBot Studio (872590),
and the Vienna Science and Technology Fund (WWTF),
project "Human tutoring of robots in industry" (NXT19-005).

REFERENCES

CONCLUSION AND FUTURE WORK
We presented a qualitative study where an industrial cobot in
VR conducted a pick-and-place task and then signalled its hu-
man collaboration partner that it is their turn to pick-and-place

[1] Andrzej Burghardt, Dariusz Szybicki, Piotr Gierlak,

Krzysztof Kurc, Paulina Pietru´s, and Rafał Cygan. 2020.
Programming of industrial robots using virtual reality
and digital twins. Applied Sciences 10, 2 (2020), 486.

Figure 5. Screenshot from the recorded VR video, showing various views on an ongoing robot-user interaction; "point of view user" is the primary
view for our analyses as it shows the human ﬁrst person view.

[2] Judee K Burgoon, Laura K Guerrero, and Valerie

Manusov. 2016. Nonverbal communication. Routledge.

operator training using virtual reality interfaces.
Computers in Industry 109 (2019), 114–120.

[3] Shirine El Zaatari, Mohamed Marei, Weidong Li, and

Zahid Usman. 2019. Cobot programming for
collaborative industrial tasks: An overview. Robotics
and Autonomous Systems 116 (2019), 162–180.

[4] Stephanie Gross, Brigitte Krenn, and Matthias Scheutz.

2016. Multi-modal referring expressions in
human-human task descriptions and their implications
for human-robot interaction. Interaction Studies 17, 2
(2016), 180–210.

[5] Stephanie Gross, Brigitte Krenn, and Matthias Scheutz.
2017. The reliability of non-verbal cues for situated
reference resolution and their interplay with language:
implications for human robot interaction. In Proceedings
of the 19th ACM International Conference on
Multimodal Interaction. 189–196.

[6] Richard G Jones. 2013. Communication in the real

world: An introduction to communication studies. Flat
World Knowledge.

[7] Clifford Nass and Youngme Moon. 2000. Machines and
mindlessness: Social responses to computers. Journal of
social issues 56, 1 (2000), 81–103.

[8] John O Oyekan, Windo Hutabarat, Ashutosh Tiwari,

Raphael Grech, Min H Aung, Maria P Mariani, Laura
López-Dávalos, Timothé Ricaud, Sumit Singh, and
Charlène Dupuis. 2019. The effectiveness of virtual
environments in developing collaborative strategies
between industrial robots and humans. Robotics and
Computer-Integrated Manufacturing 55 (2019), 41–54.

[9] Luis Pérez, Eduardo Diez, Rubén Usamentiaga, and
Daniel F García. 2019. Industrial robot control and

[10] Christopher Peters, Fangkai Yang, Himangshu Saikia,
Chengjie Li, and Gabriel Skantze. 2018. Towards the
use of mixed reality for HRI design via virtual robots. In
1st International Workshop on Virtual, Augmented, and
Mixed Reality for HRI (VAM-HRI), Cambridge, UK,
March 23, 2020.

[11] Camilo Perez Quintero, Romeo Tatsambon, Mona

Gridseth, and Martin Jägersand. 2015. Visual pointing
gestures for bi-directional human robot interaction in a
pick-and-place task. In 2015 24th IEEE International
Symposium on Robot and Human Interactive
Communication (RO-MAN). IEEE, 349–354.

[12] Laurel D Riek, Tal-Chen Rabinowitch, Paul Bremner,

Anthony G Pipe, Mike Fraser, and Peter Robinson. 2010.
Cooperative gestures: Effective signaling for humanoid
robots. In 2010 5th ACM/IEEE International Conference
on Human-Robot Interaction (HRI). IEEE, 61–68.

[13] Shane Saunderson and Goldie Nejat. 2019. How robots

inﬂuence humans: A survey of nonverbal
communication in social human–robot interaction.
International Journal of Social Robotics 11, 4 (2019),
575–608.

[14] Sara Sheikholeslami, AJung Moon, and Elizabeth A
Croft. 2017. Cooperative gestures for industry:
Exploring the efﬁcacy of robot hand conﬁgurations in
expression of instructional gestures for human–robot
interaction. The International Journal of Robotics
Research 36, 5-7 (2017), 699–720.

