Perceptual Quality Assessment of Omnidirectional
Images as Moving Camera Videos

Xiangjie Sui, Kede Ma, Member, IEEE, Yiru Yao, and Yuming Fang, Senior Member, IEEE

1
2
0
2

n
a
J

5

]

V

I
.
s
s
e
e
[

2
v
7
4
5
0
1
.
5
0
0
2
:
v
i
X
r
a

Fig. 1: Illustration of how humans explore static omnidirectional panoramas. Each user may have different viewing behaviors (i.e.,
scanpaths) under different viewing conditions, giving rise to different video representations of the same 360° image with varying
perceived quality. We consider two types of viewing conditions - the starting point and the exploration time. Speciﬁcally, the starting
point provides the longitude and the latitude, where the initial viewport can be extracted. A gaze scanpath is generated, when each
user is freely exploring the virtual environment within the exploration time. A video sequence that contains only global motion can
then be obtained by sampling, along each user’s scanpath, a number of viewports from the omnidirectional panorama. We compute
the perceived quality of the omnidirectional image by comparing it to its reference using existing video quality measures.

Abstract—Omnidirectional images (also referred to as static 360° panoramas) impose viewing conditions much different from those
of regular 2D images. How do humans perceive image distortions in immersive virtual reality (VR) environments is an important
problem which receives less attention. We argue that, apart from the distorted panorama itself, two types of VR viewing conditions
are crucial in determining the viewing behaviors of users and the perceived quality of the panorama: the starting point and the
exploration time. We ﬁrst carry out a psychophysical experiment to investigate the interplay among the VR viewing conditions, the
user viewing behaviors, and the perceived quality of 360° images. Then, we provide a thorough analysis of the collected human data,
leading to several interesting ﬁndings. Moreover, we propose a computational framework for objective quality assessment of 360°
images, embodying viewing conditions and behaviors in a delightful way. Speciﬁcally, we ﬁrst transform an omnidirectional image to
several video representations using different user viewing behaviors under different viewing conditions. We then leverage advanced
2D full-reference video quality models to compute the perceived quality. We construct a set of speciﬁc quality measures within the
proposed framework, and demonstrate their promises on three VR quality databases.

Index Terms—Omnidirectional images, perceptual quality assessment, virtual reality

1 INTRODUCTION

Virtual reality (VR) photography is the art of capturing or creating
a complete natural scene as a single omnidirectional image [1], also
known as a static 360° panorama. The viewing experience enabled
by omnidirectional images is substantially different from traditional
multimedia data, as humans are allowed to freely explore immersive
virtual environments (see Fig. 1). Therefore, understanding how hu-
mans perceive visual distortions of omnidirectional images emerges
as a new research direction due to its importance to panoramic image
acquisition, compression, storage, transmission, and reproduction [2].
Objective quality assessment of omnidirectional images is often
performed in 2D projected planes by leveraging existing 2D image
quality assessment (IQA) models (see Fig. 2). However, different map
projections come with different problems. For example, equirectangular
projection generates severe shape distortions near the poles, whereas

• Xiangjie Sui, Yiru Yao, and Yuming Fang are with the School of Information
Management, Jiangxi University of Finance and Economics, Nanchang
330032, Jiangxi, China (e-mail: suixiangjie2017@163.com,
2848444870@qq.com, fa0001ng@e.ntu.edu.sg).

• Kede Ma is with the Department of Computer Science, City University of
Hong Kong, Kowloon, Hong Kong (e-mail: kede.ma@cityu.edu.hk).

cube map projection has an oversampling rate of up to 190% compared
to the sphere [3]. It follows that distortions measured in the 2D plane
may not correspond to distortions observed in the sphere. To alleviate
the projection mismatch, several objective IQA models [4, 5] make
local quality measurements in the plane, and pool them using spherical
areas as weightings. A better implementation of this idea is to compute
quality estimates uniformly over the sphere [6, 7].

In 2D IQA, user viewing behavior can be well controlled in a lab-
oratory environment, and is often assumed similar without explicit
modeling. However, this assumption does not hold in omnidirectional
IQA. Equipped with a head-mounted display (HMD), humans are able
to use both signiﬁcant head and gaze movements to explore viewports
of interest in the scene. Recently, Sitzmann et al. [8] found that un-
der different viewing conditions, agreement among gaze scanpaths of
subjects is not high. To the best of our knowledge, no existing work
gives a complete treatment of viewing behavior when evaluating the
perceived quality of omnidirectional images.

In this paper, we argue that there are at least two types of VR
viewing conditions – the starting point and the exploration time – that
signiﬁcantly affect viewing behavior, and subsequently determine the
perceived VR quality. The viewing behavior, represented by the so-
called scanpath, is a 2D gaze trajectory over the sphere [14]. The
starting point provides the longitude and the latitude, at which the

………Different starting points and scanpathsVisual qualityDifferent video sequences Starting point 3Starting point 2Starting point 1PoorFairGood Scanpath 3 Scanpath 2 Scanpath 1 
 
 
 
 
 
Database

Upenik et al. [9]

Projection
ERP
CMP

# of images

6/54

Duan et al. [10]

ERP

16/320

Resolution
3,000×1,500 (ERP)
2,250×1,500 (CMP)
11,332×5,666
to 13,320×6,660

30

20

Sun et al. [11]

ERP

16/528

4,096×2,048

N/A

Huang et al. [12]

ERP

12/144

4,096×2,160

Chen et al. [13]

ERP

15/450

4,096×2,048

20

20

Exploration time HM/EM data

Distortion type

Public availability

HM

JPEG compression, Projection

N/A

JPEG compression, JPEG2000 compression,
Gaussian blur, Gaussian noise
JPEG compression,
H.264 compression, H.265 compression

Upon request

N/A

HM and EM

N/A

N/A

Downsampling, JPEG compression

HM and EM

Gaussian noise, Gaussian blur,
Downsampling, Stitching distortion,
VP9 compression, H.265 compression

https://vision.nju.edu.
cn/20/87/c29466a467
079/page.htm
http://live.ece.utexas.
edu/research/VR3D/
index.html
https://github.com/
xiangjieSui/img2video

Ours

ERP

36/72

7,680×3,840

5/15

HM and EM

Stitching distortion, H.265 compression

Table 1: Summary of VR IQA databases. ERP and CMP stand for the equirectangular projection and the cube map projection, respectively. The
data in the “# of images” column is in the form of “# of reference images/# of distorted images.”

Fig. 2: Illustration of different representations of an omnidirectional
image. Equirectangular projection is commonly used to obtain a 2D
plane for storage, while rectilinear projection is adopted to extract a
viewport for visual consumption at a time instant.

initial viewport is centered (see Fig. 2). The exploration time records
how long it takes for a user to ﬁnish exploring an omnidirectional
image.

We ﬁrst conduct a psychophysical experiment to study the inter-
play among the VR viewing conditions, the user viewing behaviors,
and the perceived quality of panoramic images. Thorough analysis
of the collected human data validates that viewing conditions have an
important impact on the perceived quality of omnidirectional images.
Furthermore, we propose a computational framework for objective
quality assessment of distorted panoramas, incorporating viewing con-
ditions and behaviors. Speciﬁcally, we represent a panorama by moving
camera videos, where we sample, along different users’ scanpaths, se-
quences of rectilinear projections of viewports. The resulting videos
contain only global motion [15], as if they were captured by a moving
camera where the moving patterns are determined by user viewing be-
haviors. Instead of learning omnidirectional IQA models from scratch,
the novel video representations allow us to directly adapt existing video
quality assessment (VQA) tools to this immersive application. We
construct several quality models within the proposed computational
framework by ﬁrst predicting frame-level quality using existing 2D
IQA models [16] and then pooling the quality estimates temporally [17].
Extensive experiments on the proposed database and two publicly avail-
able VR databases [10, 13] demonstrate the promise of our framework
for objective quality assessment of 360° images.

2 RELATED WORK

In this section, we ﬁrst introduce previous psychophysical studies on
the perceptual quality of omnidirectional images. We then brieﬂy
describe 2D IQA/VQA methods that will serve as building blocks in
the proposed computational framework. Last, we review IQA models
that are speciﬁcally designed for 360° images.

2.1 Subjective Quality Assessment of Panoramic Images

Since the human eye is the ultimate receiver of 360° images, the most
trustworthy way of evaluating visual quality is through psychophysical
experiments. Upenik et al. [9] constructed one of the ﬁrst VR IQA

databases to study the impact of compression and projection on the
visual quality of panoramas. The absolute category rating was adopted
to collect the mean opinion score (MOS) of each image, where a higher
MOS means better perceived quality. Additionally, head movement
(HM) data was recorded for visual saliency analysis. Duan et al. [10]
built a high-resolution VR IQA database with four distortion types.
Apart from the HM data, eye movement (EM) data was recorded for
human behavior analysis. They reported that the majority of 2D IQA
methods are insufﬁcient to provide accurate quality predictions. Sun et
al. [11] proposed so far the largest VR IQA database, consisting of 528
impaired omnidirectional images produced from 16 references. Huang
et al. [12] studied the joint effect of spatial resolution and JPEG com-
pression on the perceived quality of 360° images. Recently, Chen et
al. [13] conducted a subjective quality assessment of stereoscopic om-
nidirectional panoramas. The detailed information of these databases is
summarized in Table 1.

Although several databases included user behavior statistics (i.e.,
HM/EM data), no timestamp information was found. As a consequence,
it is difﬁcult to recover user viewing conditions and behaviors (e.g.,
the starting point and the scanpath), which are indispensable in omni-
directional IQA. Besides, most previous subjective experiments were
carried out on visual materials with global uniform distortions. Little
investigation is dedicated to local non-uniform distortions, which may
inﬂuence user viewing behavior in a substantially different way.

2.2 Full-Reference Quality Assessment of 2D Images and

Videos

Full-reference IQA and VQA involve developing computational models
that are capable of automatically predicting the perceptual quality of
images and videos, by comparing to their pristine references. Most full-
reference IQA/VQA models are designed for 2D images and videos,
among which the mean squared error (MSE) and its derivative peak
signal-to-noise ratio (PSNR) are the most widely used. MSE calculates
the square differences of pixels between the original and distorted
images, and is shown to be poorly correlated with human perception
of image quality. Later methods tried to model aspects of the human
visual system (HVS) or treated it as a “black box” with some holistic
assumptions, with the structural similarity (SSIM) index [18] being
the most successful. Recently, there has been a surge of interest in
leveraging hierarchical representations of deep neural networks (DNNs)
for the design of IQA metrics. Johnson et al. [19] used the MSE
computed on convolution responses of pre-trained DNNs to guide the
optimization of image super-resolution algorithms. Zhang et al. [20]
demonstrated the perceptual relevance of deep features pre-trained from
a wide range of vision tasks. Ding et al. [21] developed an IQA metric
with explicit tolerance to visually similar textures.

Compared with IQA, objective quality assessment of videos is more
challenging due to the complex interactions between spatial and tem-
poral distortions. A simple and computationally efﬁcient solution is
to compute frame-level quality scores by IQA methods, followed by

projection2D Plane SphereRectilinearViewportEquirectangularprojectionFig. 3: Reference images in the proposed database.

temporal pooling. Another type of VQA methods attempted to directly
extract spatiotemporal features for quality prediction. Zeng et al. [22]
proposed a spatiotemporal SSIM index by treating video signals as 3D
volume data. Kim et al. [23] developed a DNN-based full-reference
VQA method by incorporating spatiotemporal human visual perception.
Xu et al. [24] presented a spatiotemporal feature learning framework,
where a DNN with 3D convolution kernels was used to learn spatiotem-
poral distortion thresholds.

Fig. 4: Examples of stitching distortions. From left to right, the distor-
tion parameters are set to 0.5, 0.75, and 1, respectively.

2.3 Objective Quality Assessment of Panoramic Images

Nearly all quality measures for panoramas adapted existing 2D IQA
methods to three formats - 2D plane, sphere, and viewport. Meth-
ods [4, 5] in the 2D plane tried to compensate for the non-uniform
sampling due to sphere-to-plane projection. Take the equirectangular
projection as an example. The local quality measure is weighted by
cos(θ ), where θ is the corresponding latitude of the pixel/patch in the
spherical domain. In [25], Craster parabolic projection was employed
to guarantee uniform sampling density. However, map projections
are likely to cause geometric deformations (see Fig. 2). The second
type of methods such as S-PSNR [6] and S-SSIM [7] computed local
quality estimates uniformly over the sphere. Yu et al. [6] proposed two
variants of S-PSNR by deriving importance weightings from statistical
distributions of the HM/EM data. The third type of methods [26–28]
focused on extracting viewports that are highly likely to be explored by
viewers.

The above methods [4–7, 25–28] were meaningful attempts to omni-
directional IQA. However, most of them were built on top of traditional
2D IQA models such as PSNR and SSIM, ignoring years of improve-
ments in this ﬁeld, where more robust and accurate models are available.
Previous viewport-based methods only made partial use of the HM/EM
data, and did not give a temporal treatment of extracted viewports,
making quality assessment ineffective.

3 SUBJECTIVE QUALITY ASSESSMENT OF 360° IMAGES

In this section, we conduct a psychophysical experiment to study the
interplay among the VR viewing conditions, the user viewing behaviors,
and the perceived quality of omnidirectional images. We ﬁrst describe
the construction of the proposed database, followed by the design of
subjective testing and the analysis of human data.

3.1 Database Construction

Reference Images The proposed database1 contains 36 pristine-
quality images, 24 of which are captured by us using an insta360
Pro 2 camera, and the remaining 12 images are downloaded from the
Internet2 (carrying a Creative Commons license). All images have a
resolution of 7, 860 × 3, 840, whose thumbnails are shown in Fig. 3.

Distorted Images The driving goal of the subjective test is to
investigate how viewing conditions and behaviors affect the perceived
quality of omnidirectional images. To this end, two types of distortions
are included in this database - stitching distortion and H.265 compres-
sion. Stitching artifacts occur when images of different views are not

1https://github.com/xiangjieSui/img2video
2https://www.insta360.com/cn/product/insta360-pro/

#download-sample

(a)

(b)

(a)

(b)

Fig. 6: Consistency of user viewing behaviors under different starting
points. The exploration time is ﬁxed to 5 seconds. (a) Initial viewport
that contains a passerby, attracts human visual attention, leading to a
higher PLCC of 0.935. (b) Initial viewport that exhibits symmetrical
image structures with no eye-catching event, results in a much lower
PLCC of 0.187.

Fig. 5: The design of our psychophysical experiment. (a) Overall
experimental procedure, which consists of one training session, two
test sessions with a 10-minute break in between. Each test session
contains 18 distorted omnidirectional images. (b) Phase I: the ﬁrst
5 seconds of viewing and Phase II: the last 10 seconds of viewing,
separated by a voice prompt. The subjects need to give two scores
to indicate their viewing experience in Phase I, and both Phase I and
Phase II.

properly aligned when stitching. In our database, the popular software
toolbox Nuke3 is employed to generate stitching distortions. We ﬁrst
import the raw images (i.e., ﬁsheye images in the format of .dng) into
Nuke, and adjust the distortion parameter of one lens to control the
level of radial distortions. We choose the stitching distortion parameters
from {0.5, 0.75, 1} with higher values implying severer distortions (see
Fig. 4). We use the FFmpeg libx265 encoder to generate compression
artifacts by setting the quantization parameters to {38, 44, 50} with
higher values indicating increased distortions. We apply stitching dis-
tortions to the 24 images captured by us, and compress the 12 images
downloaded from the Internet. In summary, the proposed database
consists of 72 images, half of which are distorted.

Viewing Conditions We consider two viewing conditions: the
starting point and the exploration time. Two starting points are chosen
for each image: one for a salient region (e.g., due to the localized
stitching distortion) and the other on the opposite side of the former
(e.g., where the stitching distortion is not visible within the initial
viewport). Similarly, two time periods, i.e., 5 and 15 seconds, are set
for each image. Totally, there are 2 × 2 = 4 viewing conditions.

3.2 Psychophysical Experiment Design

We employ the single stimulus continuous quality evaluation method de-
scribed in the ITU-R BT 500.13 recommendation [29] to gather human
data. Subjects are asked to rate the quality of an omnidirectional image
on a continuous scale of [1, 5], labeled by ﬁve quality levels (“bad”,
“poor”, “fair”, “good”, and “excellent”). The images are displayed in
a random order using an HTC Vive VR HMD, which provides a ﬁeld
of view (FoV) of 110°. EM and HM data are collected by a built-in
Tobii Pro eye tracking system with a ﬁxed sampling frequency of 20
Hz. Image playback is supported by a high-performance server with an
AMD Ryzen 9 3950X 16-Core CPU, a 128 GB RAM, and an NVIDIA
GeForce RTX 2080 Ti GPU. The user interface is built by the Unity
Game Engine.

We invite 22 subjects to participate in the psychophysical study.
They are divided into two groups, and are asked to view 360° images
from two different sets of starting points. We conduct two test sessions
with a 10-minute break in between to minimize the effect of fatigue and
discomfort (see Fig. 5 (a)). A training session is included to familiarize
the subjects with the rating procedure and to exclude those who feel
discomfort exploring VR environments. We also design a rating strategy
to collect quality scores with different exploration periods (see Fig. 5

3https://www.foundry.com/products/nuke

(a)

(b)

Fig. 7: (a) Farthest longitudinal distance to the starting point averaged
across viewers for 5 and 15 seconds of exploration. (b) Consistency
between scanpaths from different users in terms of PLCC.

(b)). A voice prompt is played when the subjects have viewed a 360°
image for 5 seconds to remind them of giving a quality score based
on their viewing experience so far. When the subjects ﬁnish viewing
the image within 15 seconds, they need to give another quality score
according to their overall viewing experience. It is worth noting that
each image is viewed only once by one subject to ensure that user data
is collected without prior knowledge of the scene. To sum up, in the
proposed database, each image is associated with ﬁve tags, including
the starting point, the exploration time, the distortion type, the distortion
level, and the MOS.

3.3 Psychophysical Data Analysis
3.3.1 Do Viewing Conditions Affect Viewing Behaviors?
We arrange the HM data gathered from each user in a matrix, where the
row dimension records the latitude and the longitude, and the column
dimension records the time instance. For each image, we compute
the average Pearson linear correlation coefﬁcient (PLCC) of the HM
matrices across every pair of users under the same viewing condition,
as an indication of user behavior consistency.

How Does the Starting Point Affect Viewing Behaviors? It is
natural to infer that the starting point has an important inﬂuence on the
gaze scanpath. For example, salient targets in the initial viewport may
attract attention, leading to more consistent gaze scanpaths at least for
a short time period (see Fig. 6 (a)). If the image structures of the initial
viewport are symmetric, the gaze scanpaths tend to be random, as the
users have equal probability to start exploring the scene from left or
right (see Fig. 6 (b)).

How Does the Exploration Time Affect Viewing Behaviors?
The exploration time directly affects how many viewports can be ob-
served by the users. Fig. 7 (a) shows the farthest longitudinal distances
to the starting point averaged across users for 5 and 15 seconds of
exploration. When a 360° image is displayed, the subjects usually
take some time to adapt to the new scene, and 5 seconds may not be
sufﬁcient for them to fully explore the scene. In contrast, most users

102030Image index0100200300Longitudinal distance in degree5s15s0102030Image index00.51PLCC5s15s(a) 5 seconds, stitching

(b) 15 seconds, stitching

(c) 5 seconds, compression

(d) 15 seconds, compression

(e) Starting point I, stitching

(f) Starting point II, stitching

(g) Starting point I, compression

(h) Starting point II, compression

Fig. 8: MOSs of 360° images in the proposed database under different viewing conditions.

Source of variation
Starting point
Exploration time
Distortion type
Starting point ×
Exploration time
Starting point ×
Distortion type
Exploration time ×
Distortion type
Starting point ×
Exploration time ×
Distortion type
Residual
Total

SS
2.76
0
3.49

19.06

0.08

0

9.45

39.11
88.31

d. f .
1
1
1

MS
2.76
0
3.49

F
9.60
0.01
12.14

p
≈ 0
0.91
≈ 0

1

1

1

1

19.10

66.28

≈ 0

0.08

0.29

0.59

0

0.01

0.90

9.45

32.86

≈ 0

0.29

136
143

Table 2: The results of multi-factorial ANOVA test. SS: sum of squares.
d. f .: degrees of freedom. MS: mean square. F: F value. p: p-value
for the null hypothesis.

are able to ﬁnish viewing the image within 15 seconds. Fig. 7 shows
the PLCC results for different exploration time, where we observe that
the correlation between scanpaths from different viewers decreases
substantially over time.

3.3.2 Do Viewing Conditions Affect Perceived Quality?

Assuming a reasonable gaze speed, a short exploration time means
fewer observed viewports, which may highlight the starting point in the
quality assessment process. By contrast, with a long exploration time,
the viewers are more likely to be inﬂuenced by viewports close to the
end of viewing due to the recency effect [30]. These hypotheses have
been validated in our psychophysical experiment. We show the MOSs
of the images under different viewing conditions in Fig. 8, and have
two important ﬁndings:

• Both the starting point and the exploration time have a noticeable
impact on the perceived quality of 360° images with localized
distortions (i.e., stitching distortions). However, they seem to
have little effect on H.265 compressed images with global uniform
distortions. Therefore, we have identiﬁed the distortion type as
a determining factor on how the viewers respond to different

viewing conditions.

• The recency effect is clearly observed when the users explore
locally distorted omnidirectional images (see Figs. 8 (e) and
(f)). From Starting Point I where stitching distortions appear in
initial viewports, the users usually give low quality scores after
5 seconds of viewing (i.e., in Phase I). However, if the viewers
are allowed to explore the panoramic scene for 15 seconds, the
quality ratings are considerably higher. On the contrary, from
Starting Point II, where stitching distortions are at the opposite
side of the initial viewports, the users would probably see such
localized distortions in Phase II. As a result, they tend to give
high and low quality scores for 5 and 15 seconds of viewing,
respectively.

To test the signiﬁcance of the starting point, the exploration time,
and the distortion type on inﬂuencing the perceived quality of omni-
directional images, we apply the multi-factorial analysis of variance
(ANOVA) [31], which considers all factors at once. The results are
summarized in Table 2, from which we identify two signiﬁcant individ-
ual effects: the starting point and the distortion type (whose p-values
are below the threshold of 0.05). The exploration time cannot alone
explain the variability in perceived quality (with p-value = 0.91). A
statistically signiﬁcant interplay effect is also identiﬁed, implying that
the perceived quality depends on the combination of the starting point
and the exploration time. These results show that the viewing condi-
tions have a signiﬁcant impact on the perceived quality. Nevertheless,
it is reasonable to assume that the perceived quality depends on the
combination of the starting point, the exploration time, and the distor-
tion type, despite that the corresponding F value is not the largest in
Table 2.

4 OBJECTIVE QUALITY ASSESSMENT OF 360° IMAGES

In this section, we describe a general computational framework for
omnidirectional IQA, where user viewing conditions and behaviors are
incorporated naturally by treating omnidirectional images as moving
camera videos, as shown in Fig. 9.

4.1 Input Data

The inputs to our computational model consist of 1) a pair of reference
and distorted panoramas, 2) a starting point and an exploration time as

5101520Image index12345MOSStarting point IStarting point II5101520Image index12345MOS24681012Image index12345MOS24681012Image index12345MOS5101520Image index12345MOS5s15s5101520Image index12345MOS24681012Image index12345MOS24681012Image index12345MOSFig. 9: Proposed computational framework for omnidirectional IQA. The processing of the reference panorama has been omitted for simplicity.

two types of viewing conditions, and 3) scanpaths from different users
as one type of viewing behavior:

• The reference and distorted panoramas, X and Y , are usually
in the form of 2D equirectangular projections, which are to be
transformed to spherical representations for viewport extraction.

• The starting point, P0 = (φ0, θ0), speciﬁes the longitude and the
latitude, at which the initial viewport is centered for a viewer to
start exploring the virtual scene.

• The exploration time, T , records how long it takes for a viewer

to explore the distorted panorama.

• The scanpath, P(t) : R (cid:55)→ R2, describes a 2D gaze trajectory
when exploring the visual ﬁeld [14]. It takes a time instant t ∈
[0, T ] as input, and produces a 2D spherical coordinate (φ , θ ),
where P(0) = (0, 0). The viewport at a speciﬁc time instant t can
be extracted at P(t) + P0.

4.2 Omnidirectional Image-to-Video Conversion

Given the viewing behavior of a user, we convert a panorama into a
video sequence, which contains only global motion as if the underly-
ing static scene were captured by a moving camera. This is achieved
by sampling a sequence of rectilinear projections of viewports of the
panorama along the scanpath [32], with a predeﬁned sampling rate.
Speciﬁcally, given the current sample point P(t) + P0 as the center,
we ﬁrst set the FoV to [−π/6, π/6] along both longitude and latitude
directions, inspired by the theory of near peripheral vision [33]. This
speciﬁes 3D Cartesian coordinates of the square viewport, which are
assumed to be perpendicular to the Z-axis for convenience. The cor-
responding pixel values can be retrieved by projecting 3D Cartesian
points onto the unit sphere and then onto the 2D plane. Bilinear inter-
polation is used as the optional resampling ﬁlter. We then choose a
sampling rate

R =

1
s1

× Ret,

(1)

where Ret is the maximum sampling rate constrained by the eye tracker
and s1 ≥ 1 is a stride parameter. The resulting moving camera video
has a total of N = R × T frames.

4.3 Omnidirectional Image Quality Prediction

Generally, any existing VQA model could be adopted at this stage to
evaluate the perceived quality of 360° images. Here we follow a two-
stage approach: frame-level quality estimation followed by temporal
pooling. For the i-th viewer, where i ∈ {1, 2 . . . , M}, we denote the j-th
frames of the reference and distorted videos by Xi j and Yi j, respectively.
The frame-level quality can then be computed by

Qi j = D (cid:0)Xi j,Yi j

(cid:1) ,

(2)

where D denotes a full-reference IQA model. The global quality Qi
as perceived by the i-th user can be computed by fusing frame-level
quality scores:

Qi = F(Qi1, . . . , QiN ),

(3)

where F is a temporal pooling strategy that may model aspects of the
memory effect of the human brain. Similar as computing the MOS, we
average quality estimates across all viewers to obtain the ﬁnal quality
score of the distorted panorama:

Q =

1
M

M
∑
i=1

Qi.

(4)

4.4 Speciﬁc Omnidirectional IQA Models
We construct several speciﬁc examples of omnidirectional IQA mea-
sures within the proposed computational framework. First, we need
to specify 2D IQA models for computing frame-level quality. The
main selection criterion is that the model should correlate well with
human perception of image quality, in terms of benchmarking as well
as optimizing image processing algorithms. In this paper, we select ﬁve
full-reference image quality models:

• PSNR, the Peak Signal-to-Noise Ratio, is built on top of the
MSE by incorporating the maximum power of a signal. Arguably
PSNR (or MSE) is the most widely used IQA measure, and enjoys
a number of desirable properties for optimization purposes.

• SSIM [18], the Structural SIMilarity index, assumes that the HVS
is highly adapted to extract local image structures of the visual
ﬁeld. Thus, a measure of structural information loss may provide
a good approximation to perceived quality degradation. Over
the years, SSIM and its multi-scale extension [34] have been re-
garded as standard “perceptual” metrics to guide the optimization
of methods for image denoising [35], image compression [36],
image synthesis [37], and video coding [38].

• VIF [39], the Visual Information Fidelity measure, offers an
information theoretical perspective of IQA, and uses the mutual
information [40] to quantify the amount of information preserved
in the distorted image. Its industrial implementation - VMAF [41]
has been successfully applied to adjust the parameter settings in
video engineering.

• NLPD [42], the Normalized Laplacian Pyramid Distance, is based
on a multi-scale nonlinear representation that models the opera-
tions in early stages of the HVS. NLPD has been used to optimize
tone mapping algorithms, where the input image has a much
higher dynamic range than that of the output image [42].

• DISTS [21], the Deep Image Structure and Texture Similarity met-
ric, uses a DNN to construct an injective and perceptual transform,
and makes SSIM-motivated quality measurements in the trans-
form domain. DISTS is robust to texture substitution and mild
geometric transformation. In a recent comparison of IQA models

ScanpathsamplingDistorted panoramaVideo sequence2D-IQAmethodExploration timeScanpathStarting pointTemporal poolingPredictedqualityFrame-level quality...P0

T

P(t)

Value
{(− π

2 , 0), (0, 0), ( π

2 , 0), (π, 0)}

15 seconds




φ =

π

−vt, 0 ≤ t ≤ T
4
2 + v(t − T
− π
2 − v(t − 3T

4 ), T
4 ), 3T

4 < t ≤ 3T
4 ,
4 < t ≤ T

θ = 0

Fig. 10: Illustration of the default scanpath. φ0 is the longitude of the
starting point.

Table 3: Default viewing conditions and behaviors for panoramic image-
to-video conversion. (φ , θ ) are the longitude and the latitude, and v is
the gaze velocity.

for optimization of image processing systems [43], DISTS out-
performs ten competing models in blind image deblurring, single
image super-resolution, and lossy image compression.

We adopt the temporal hysteresis model [44] as the default pool-
ing strategy. Speciﬁcally, to mimic users’ intolerance to poor quality
events and reluctant reaction to quality improvement events, a memory
component is deﬁned at each video frame:

Qm

j =

(cid:40)Q1
min

(cid:110)

max{1, j−K}, . . . , Qm
Qm

j−2, Qm

j−1

(cid:111)

if j = 1

otherwise

(5)

where we omit the user index i in the subscript to make the notation
uncluttered. K is a parameter related to the duration of memory [44].

The temporal hysteresis pooling also accounts for the fact that hu-
mans react sharply to quality degradation events by deﬁning a current
quality component at each video frame:

Qc

j =

min{ j+K,N}
∑
k= j

wkQs
k,

and

{Qs

k} = sort

(cid:16)
{Qk}min{ j+K,N}
k= j

(cid:17)

,

(6)

(7)

j, . . . , Qs

where sort() sorts {Q j, . . . , Qmin{ j+K,N}} in ascending order, resulting
in {Qs
min{ j+K,N}}. w is a normalized weighting vector speciﬁed
by the descending half of a Gaussian function. The adjusted time-
varying quality score of Yj is computed by linearly combining the
memory and current components:

Qa

j = αQm

j + (1 − α)Qc
j,

(8)

where α is a parameter to trade off the two terms. The global quality is
obtained by averaging the quality scores of all frames:

Q =

1
N

N
∑
j=1

Qa
j .

(9)

may be obtained by taking the empirical expectation over several prefer-
able types of viewing conditions and behaviors. Speciﬁcally, we sample
four different starting points evenly spaced along the equator. Consid-
ering a reasonable gaze speed of 24°/s [8], we set a ﬁxed exploration
time T to 15 seconds [9–13]. To keep the computational complexity
manageable, we design a single scanpath by taking into account the fact
that the front equator regions are viewed more frequently than other
parts. Speciﬁcally, the user ﬁrst browses the panorama from the starting
point (φ0, 0), then gradually moves the gaze counterclockwise along
the equator to (φ0 − π/2, 0) for viewing the left part of the 360° image.
Next, the user begins to explore the right part of the scene by moving
the gaze clockwise from (φ0 − π/2, 0) to (φ0 + π/2, 0). Finally, the
user returns to the starting point (φ0, 0) and ﬁnishes the browsing (see
Fig. 10). Note that we constrain the gaze movements along the equator
by clamping the latitude to θ = 0. The detailed speciﬁcations of the
default viewing conditions and behaviors are summarized in Table 3.

Given a pair of reference and distorted panoramas, we ﬁrst downsam-
ple them to reduce the computational complexity as suggested in [18].
The implementations of the ﬁve full-reference IQA models are obtained
from the respective authors. The three parameters in the temporal hys-
teresis model, including the memory duration K = 20, the normalized
Gaussian weighting function w with standard deviation (2K − 1)/12,
and the linear factor α = 0.8, are set according to [44].

We use three subject-rated VR datasets - the proposed database
in Section 3, the OIQA database in [10], and the LIVE 3D VR IQA
database (LIVE) in [13]. The OIQA database contains 320 distorted
panoramas, generated from 16 reference panoramas with four distortion
types at ﬁve distortion levels, including JPEG compression (JPEG),
JPEG2000 compression (JP2K), Gaussian noise (GN), and Gaussian
blur (GB). The LIVE database includes 15 reference stereoscopic omni-
directional panoramas. Six distortion types with ﬁve levels are applied
to produce 450 distorted images, including GN, GB, downsampling
(DS), stitching distortion (ST), VP9 compression, and H.265 compres-
sion.

We use two evaluation metrics to quantify the quality prediction
performance, including PLCC and Spearman’s rank-order correlation
coefﬁcient (SRCC). A better quality model achieves higher PLCC and
SRCC values. As suggested in [45], we map model predictions to
human quality ratings through a four-parameter logistic function before
calculating PLCC:

f (Q) = (β1 − β2)

1
− Q−β3
|β4|

1 + e

+ β2,

(10)

where {βi}4

i=1 are the parameters to be ﬁtted.

5 EXPERIMENTS
In this section, we ﬁrst describe the implementation details of the
proposed computational framework for omnidirectional IQA. Next, we
introduce the evaluation procedures, and compare our methods with
state-of-the-art quality measures, followed by a statistical signiﬁcance
test. Last, we conduct comprehensive ablation studies to analyze the
sensitivity of individual components.

5.1 Implementation Details and Evaluation Protocols
The proposed computational framework requires user viewing condi-
tions and behaviors to transform static panoramas to moving camera
videos. When such information is not available, the overall quality score

5.2 Main Results
We add an “O-” to the ﬁve 2D IQA methods listed in Section 4.4
as a preﬁx to name the proposed quality models (e.g., PSNR to O-
PSNR). We include S-PSNR [6], S-SSIM [5] , WS-PSNR [4], and
CPP-PSNR [25] as representative omnidirectional IQA models for
comparison. We also directly apply 2D IQA models to equirectangular
projections as baselines. To measure relative perceptual gains when
incorporating viewing conditions and behaviors, we further create a
set of viewport-based methods by extracting viewports uniformly dis-
tributed over the sphere for quality computation using the same 2D IQA
models, as suggested in [28]. Similarly, we add a “V-” in front to name
viewport-based methods. For all models, we compute quality values

WS-PNSR
CPP-PNSR
PSNR
V-PSNR
O-PSNR

PLCC
0.151
S-PSNR
S-SSIM 0.149
0.153
0.129
0.165
0.148
0.583
SSIM 0.148
V-SSIM 0.149
O-SSIM 0.468
0.111
0.151
0.605
0.012
0.069
0.479
0.079
0.055
0.489

VIF
V-VIF
O-VIF
NLPD
V-NLPD
O-NLPD
DISTS
V-DISTS
O-DISTS

ST

Proposed database
H.265

Overall

JPEG

JP2K

OIQA database
GB

GN

Overall

SRCC
-0.113
0.055
-0.116
-0.054
-0.114
-0.049
0.516
0.057
0.044
0.495
0.057
0.046
0.555
-0.009
-0.017
0.534
0.025
0.069
0.518

PLCC
0.931
0.922
0.931
0.930
0.924
0.928
0.933
0.910
0.930
0.923
0.920
0.923
0.893
0.907
0.895
0.898
0.867
0.900
0.916

SRCC
0.890
0.932
0.893
0.906
0.893
0.893
0.911
0.932
0.916
0.881
0.872
0.861
0.843
0.870
0.892
0.857
0.861
0.910
0.903

PLCC
0.225
0.018
0.215
0.215
0.231
0.241
0.597
0.036
0.038
0.579
0.356
0.493
0.617
0.244
0.244
0.311
0.450
0.512
0.660

SRCC
-0.103
-0.031
-0.104
-0.079
-0.102
-0.077
0.467
-0.030
-0.038
0.435
0.331
0.342
0.496
-0.063
-0.065
0.472
0.299
0.402
0.613

PLCC
0.890
0.922
0.890
0.891
0.891
0.905
0.905
0.910
0.924
0.938
0.916
0.929
0.937
0.925
0.964
0.972
0.863
0.942
0.955

SRCC
0.847
0.903
0.847
0.849
0.848
0.898
0.891
0.893
0.905
0.922
0.900
0.915
0.923
0.945
0.954
0.958
0.915
0.937
0.942

PLCC
0.886
0.930
0.886
0.885
0.891
0.897
0.901
0.924
0.932
0.941
0.955
0.960
0.969
0.919
0.954
0.964
0.939
0.961
0.971

SRCC
0.887
0.931
0.886
0.885
0.893
0.896
0.901
0.926
0.931
0.939
0.956
0.962
0.968
0.947
0.954
0.962
0.952
0.959
0.969

PLCC
0.784
0.869
0.785
0.767
0.759
0.835
0.884
0.849
0.891
0.918
0.960
0.957
0.965
0.849
0.933
0.942
0.959
0.965
0.973

SRCC
0.780
0.870
0.781
0.764
0.754
0.831
0.886
0.845
0.891
0.921
0.958
0.954
0.965
0.893
0.933
0.945
0.956
0.957
0.969

PLCC
0.915
0.955
0.915
0.914
0.925
0.913
0.914
0.951
0.942
0.942
0.950
0.947
0.947
0.952
0.970
0.974
0.951
0.963
0.966

SRCC
0.881
0.941
0.881
0.878
0.895
0.884
0.881
0.937
0.929
0.930
0.921
0.916
0.917
0.947
0.957
0.963
0.944
0.949
0.952

PLCC
0.763
0.828
0.764
0.757
0.744
0.795
0.797
0.809
0.850
0.866
0.871
0.883
0.889
0.854
0.911
0.912
0.837
0.883
0.882

SRCC
0.751
0.823
0.751
0.747
0.733
0.779
0.780
0.802
0.844
0.862
0.862
0.873
0.880
0.844
0.907
0.907
0.830
0.875
0.875

Table 4: Performance comparison of omnidirectional IQA methods on the proposed and OIQA databases. The best results are highlighted in bold.

GB

GN

ST

VP9

H.265

DS

Overall

PLCC
0.887
0.902
0.887
0.885
0.877
0.901
0.916
0.891
0.911
0.920
0.943
0.944
0.951
0.923
0.937
0.940
0.954
0.954
0.958

SRCC
0.758
0.810
0.758
0.750
0.730
0.792
0.841
0.781
0.830
0.863
0.893
0.899
0.914
0.860
0.887
0.895
0.921
0.917
0.926

PLCC
0.897
0.920
0.897
0.898
0.896
0.897
0.893
0.913
0.925
0.924
0.927
0.924
0.917
0.923
0.935
0.931
0.921
0.903
0.900

SRCC
0.863
0.905
0.863
0.879
0.857
0.861
0.848
0.898
0.911
0.911
0.922
0.919
0.905
0.913
0.930
0.920
0.910
0.890
0.888

PLCC
0.657
0.634
0.656
0.635
0.623
0.689
0.704
0.551
0.643
0.652
0.691
0.754
0.740
0.659
0.707
0.721
0.664
0.764
0.708

SRCC
0.638
0.612
0.634
0.615
0.618
0.684
0.693
0.542
0.622
0.640
0.694
0.754
0.754
0.646
0.701
0.719
0.649
0.758
0.703

PLCC
0.608
0.784
0.607
0.596
0.572
0.662
0.744
0.770
0.809
0.822
0.839
0.862
0.877
0.878
0.872
0.866
0.738
0.765
0.802

SRCC
0.594
0.756
0.595
0.582
0.561
0.649
0.722
0.743
0.785
0.785
0.819
0.841
0.873
0.861
0.849
0.845
0.708
0.749
0.781

PLCC
0.806
0.874
0.806
0.800
0.780
0.836
0.884
0.848
0.899
0.927
0.913
0.926
0.941
0.930
0.945
0.946
0.854
0.869
0.895

SRCC
0.797
0.874
0.797
0.787
0.766
0.829
0.888
0.844
0.903
0.930
0.918
0.930
0.946
0.931
0.951
0.952
0.855
0.872
0.890

PLCC
0.865
0.853
0.865
0.866
0.863
0.877
0.882
0.860
0.870
0.892
0.904
0.904
0.904
0.881
0.886
0.894
0.920
0.947
0.950

SRCC
0.780
0.778
0.779
0.776
0.786
0.802
0.812
0.759
0.804
0.825
0.806
0.812
0.814
0.790
0.805
0.813
0.852
0.920
0.938

PLCC
0.727
0.746
0.727
0.725
0.715
0.753
0.758
0.729
0.768
0.769
0.841
0.845
0.790
0.653
0.705
0.735
0.749
0.814
0.850

SRCC
0.649
0.722
0.649
0.646
0.642
0.689
0.672
0.704
0.749
0.716
0.833
0.839
0.753
0.621
0.671
0.701
0.748
0.813
0.851

S-PSNR
S-SSIM
WS-PSNR
CPP-PSNR
PSNR
V-PSNR
O-PSNR
SSIM
V-SSIM
O-SSIM
VIF
V-VIF
O-VIF
NLPD
V-NLPD
O-NLPD
DISTS
V-DISTS
O-DISTS

Table 5: PLCC and SRCC results of OIQA methods on the LIVE database.

(a) Proposed database

(b) OIQA

(c) LIVE

Fig. 11: Statistical signiﬁcance diagram based on quality prediction residuals using F-test. A black block means the row model performs
signiﬁcantly better than the column model, a white block means the opposite, and a gray block indicates the signiﬁcant difference between the
row and column models is not observed.

PSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSPSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSPSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSPSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSPSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSPSNRSSIMVIFNLPDDISTSS-PSNRWS-PSNRCPP-PSNRS-SSIMV-PSNRV-SSIMV-VIFV-NLPDV-DISTSO-PSNRO-SSIMO-VIFO-NLPDO-DISTSon panoramas of the same downsampled resolution. The quality score
of a stereoscopic image is computed by averaging quality estimates of
both views. We list the quantitative results on the proposed database
and the OIQA database in Table 4, and the LIVE database in Table 5,
from which we make several interesting observations.

First, it is quite surprising that recent 2D IQA models directly applied
to equirectangular projections outperform existing omnidirectional IQA
models. For example, the performance of S-PSNR, S-SSIM, WS-PSNR,
and CPP-PSNR is worse than that of VIF, NLPD, and DISTS on the
OIQA database [10]. This suggests that instead of sticking to standard
quality measures - PSNR and SSIM, we may transfer recent advances
in the domain of 2D IQA to VR applications. Second, current omnidi-
rectional IQA methods, projection-based methods, and viewport-based
methods fail to capture the localized stitching distortions in the pro-
posed database. For example, the SRCC values of PSNR, S-PSNR, and
V-PSNR are even negative (see Table 4). When the stitching distortions
are synthesized globally as in the LIVE database, the overall results
get back to a reasonable level (see Table 5). Finally, the proposed com-
putational framework achieves signiﬁcant performance improvements
compared to both projection-based and viewport-based methods, espe-
cially on the proposed database. O-NLPD and O-DISTS also achieve
the best performance on the OIQA database and the LIVE database,
respectively. A noticeable exception is O-VIF, which underperforms
VIF and V-VIF on the LIVE database. This may be because VIF tends
to over-penalize the stitching distortions, which have large differences
in pixel values but look more acceptable compared to other distortion
types. The temporal hysteresis model in O-VIF tends to amplify such
penalties, resulting in a signiﬁcant performance drop.

To ascertain that the improvement of the proposed computational
framework is statistically signiﬁcant, we carry out a statistical sig-
niﬁcance test by following the approach introduced in [46]. First, a
nonlinear function is applied to map objective quality scores to subjec-
tive scores. We observe that the prediction residuals all have zero mean,
and thus the model with a lower variance is generally considered better.
We conduct a hypothesis testing using the F-statistic, i.e., the ratio of
variances. The null hypothesis is that the prediction residuals of one
quality model come from the same distribution, and are statistically
indistinguishable (with 95% conﬁdence) from the residuals of another
model. After comparing every possible pairs of objective models, the

O-PSNR

O-SSIM

O-DISTS

PLCC
SRCC
PLCC
SRCC
PLCC
SRCC

AM
0.565
0.470
0.573
0.417
0.681
0.626

HM
0.628
0.449
0.594
0.396
0.677
0.620

GW
0.767
0.737
0.470
0.525
0.843
0.844

MM
0.538
0.453
0.577
0.429
0.694
0.648

PS
0.586
0.307
0.524
0.527
0.682
0.584

TH
0.597
0.467
0.579
0.435
0.660
0.613

Table 6: Performance comparison of our models with different
temporal pooling strategies on the proposed database. AM: arithmetic
mean. HM: harmonic mean. GW: ascending half of Gaussian
weighting. MM: Minkowski mean. PS: percentile scoring. The default
pooling is highlighted in bold.

O-PSNR

O-SSIM

O-DISTS

Input size
1,920 × 3,840
960 × 1,920
480 × 960
1,920 × 3,840
960 × 1,920
480 × 960
1,920 × 3,840
960 × 1,920
480 × 960

Viewport size
640 × 640
320 × 320
160 × 160
640 × 640
320 × 320
160 × 160
640 × 640
320 × 320
160 × 160

PLCC
0.573
0.597
0.603
0.601
0.579
0.506
0.647
0.660
0.667

SRCC
0.454
0.467
0.470
0.399
0.435
0.351
0.602
0.613
0.624

Table 7: Performance comparison of our models with different input
resolutions on the proposed database. The default size determined
by [18] is highlighted in bold.

O-PSNR

O-SSIM

O-DISTS

s1
4
2
1
4
2
1
4
2
1

R
5
10
20
5
10
20
5
10
20

PLCC
0.599
0.597
0.597
0.574
0.574
0.579
0.661
0.660
0.660

SRCC
0.466
0.466
0.467
0.379
0.379
0.435
0.613
0.613
0.613

Table 8: Performance comparison of our models with different sampling
rates on the proposed database. The sampling rate supported by the
HMD is highlighted in bold.

O-PSNR

O-SSIM

O-DISTS

Scanpath pattern
Default scanpath
Default scanpath with nonzero latitudes
Counterclockwise rotation
Default scanpath
Default scanpath with nonzero latitudes
Counterclockwise rotation
Default scanpath
Default scanpath with nonzero latitudes
Counterclockwise rotation

PLCC
0.797
0.797
0.797
0.866
0.866
0.865
0.882
0.882
0.883

SRCC
0.780
0.780
0.780
0.862
0.861
0.861
0.875
0.875
0.875

Table 9: Performance comparison of our models with different scan-
path patterns on the OIQA database, where users’ scanpaths are not
available.

results are summarized in Fig. 11, where a black block means the row
model performs signiﬁcantly better than the column model, a white
block means the opposite, and a gray block indicates the signiﬁcant
difference between the row and column models is not observed. From
the ﬁgure, we conclude that quality models within the proposed compu-
tational framework are statistically better than the competing methods
in most cases.

5.3 Ablation Experiments

In this subsection, we conduct a series of ablation experiments to
analyze the impact of temporal pooling strategies, input resolutions,
sampling rates, and scanpath patterns within the proposed computa-
tional framework. Here we only consider three 2D IQA measures as
base models: PSNR, SSIM, and DISTS.

Choice of Temporal Pooling Strategy In addition to the tem-
poral hysteresis pooling [44], we test another ﬁve strategies, includ-
ing arithmetic mean, harmonic mean, the ascending half of Gaussian
weighting [44], Minkowski mean (using (cid:96)2-norm) and percentile scor-
ing (using 10%) [47]. From Table 6, we ﬁnd that temporal pooling
makes a noticeable difference on the proposed database. For example,
when DISTS [21] is the base model, switching the default hysteresis
pooling to Gaussian weighting signiﬁcantly boosts the performance,
better accounting for the recency effect. This veriﬁes our omnidirec-
tional image-to-video conversion as a natural way of incorporating
viewing conditions and behaviors into the quality assessment process.

Choice of Input Resolution The resolution of the input 360° im-
ages determines the effective viewing distance and the sizes of viewport.
Table 7 shows the results on the proposed database, where we observe
that model performance is generally better as the input resolution re-
duces. In our implementation, we employ automatic downsampling as
suggested in [18] to keep the shorter side of the panorama in the range
of 512 and 1024. Accordingly, the size of the square viewport is in the
range of 170 × 170 and 341 × 341. As such, we strike a good balance
between signal ﬁdelity and computational complexity.

Choice of Sampling Rate In the proposed computational frame-
work, we generate video sequences by sampling viewports along users’

scanpaths at a certain rate. Thus, it is natural to ask: what is the opti-
mal sampling rate in terms of prediction accuracy and computational
complexity? We test our models with different sampling rates by ad-
justing the stride parameter s1 in Eq. (1), and list the results on the
proposed database in Table 8. We ﬁnd that they are robust to variation
of sampling rates. In our default setting, a constant sampling rate is
assumed to extract viewports that are uniformly distributed along the
scanpath. However, in practice, humans tend to alternate between two
modes: attention and re-orientation [8]. The attention mode is activated
when viewers have paused on interesting parts of the scene, while the
re-orientation mode begins when the human eye moves to new salient
regions. In the future, we may take advantage of this viewing behavior
for computational complexity reduction.

Choice of Scanpath Pattern To investigate the impact of scan-
paths on the prediction accuracy, we test two additional trajectories
within the proposed computational framework: 1) default scanpath with
nonzero latitudes by adding a random Brownian motion to the current
latitude and 2) counterclockwise rotation along the equator for 2π. The
results on the OIQA database are listed in Table 9, from which we
can see that no scanpath pattern seems to be signiﬁcantly better than
the others. The reason may be that most test panoramas are distorted
globally in the OIQA database, resulting in relatively uniform quality.

6 CONCLUSION AND DISCUSSION

In this paper, we have taken steps towards perceptual quality assessment
of omnidirectional images. We conducted a psychophysical experiment
to study how the viewing conditions (i.e., the starting point and the ex-
ploration time) affect user viewing behaviors and the perceived quality
of 360° images. We then introduced a computational framework to
design objective omnidirectional IQA models, which incorporates the
viewing conditions and behaviors into the quality prediction process.
The key idea is to map panoramas to moving camera videos by ex-
tracting the sequences of viewports along the scanpaths. Experimental
results on three VR IQA databases demonstrated the promise of the
proposed framework, where we successfully transferred the advances
in 2D IQA to VR applications.

Our framework suggests a natural extension to personalized omnidi-
rectional IQA, which may be more suitable in VR applications as user
viewing behaviors tend to vary based on their own personal experiences
and preferences. This can be easily achieved by exploiting behavior
statistics of a single user, instead of averaging across several users.

Although the proposed methods have offered signiﬁcant perceptual
gains compared to existing models, they are somewhat limited at han-
dling localized stitching distortions (as shown in Table 4). Although
our omnidirectional image to video conversion may detect the stitching
distortions as long as they are seen by viewers, this does not necessarily
mean that existing objective quality models can quantify them in a
proper way due to the idiosyncratic visual appearances (see Fig. 4). We
believe that current and future models that take better account for local
distortions will have great potential in boosting the performance within
the proposed computational framework.

The current work focuses on omnidirectional image quality with a
ﬁxed display constraint. However, what and how to display a panoramic
image may have an impact on its perceived quality. For example,
Zhang et al. [48] suggested to resample the omnidirectional image to
the optimal resolution before coding, as a way of matching the reso-
lutions between the HMD and the image. Jabar et al. [49] found the
perceived quality of 360° images has a dependency on the FoV of the
extracted viewport, and the optimal FOV for viewing panoramas is
110°. All of these suggest to incorporate the display information as an
additional viewing condition into the proposed computational frame-
work. Moreover, it would be interesting to build computational models
to predict the overall quality-of-experience of users when exploring
360° images, including VR discomfort and sickness [50].

ACKNOWLEDGMENTS

This work was supported in part by the National Key R&D Program of
China under Grant 2018AAA0100601, the National Natural Science

Foundation of China under Grants 62071407 and 61822109, the Fok
Ying Tung Education Foundation under Grant 161061, the Jiangxi
Natural Science Foundation of China under Grant 20202ACB202007,
and the CityU APRC Grant (9610487).

REFERENCES

[1] “VR photography,” 2020 (accessed Apr. 10, 2020). [Online]. Available:

https://en.wikipedia.org/wiki/VR photography

[2] M. Xu, C. Li, S. Zhang, and P. L. Callet, “State-of-the-art in 360°
video/image processing: Perception, assessment and compression,” IEEE
Journal of Selected Topics in Signal Processing, vol. 14, no. 1, pp. 5–26,
Jan. 2020.

[3] King-To Ng, Shing-Chow Chan, and Heung-Yeung Shum, “Data compres-
sion and transmission aspects of panoramic videos,” IEEE Transactions
on Circuits and Systems for Video Technology, vol. 15, no. 1, pp. 82–95,
Jan. 2005.

[4] Y. Sun, A. Lu, and L. Yu, “Weighted-to-spherically-uniform quality evalu-
ation for omnidirectional video,” IEEE Signal Processing Letters, vol. 24,
no. 9, pp. 1408–1412, Sep. 2017.

[5] F. Lopes, J. Ascenso, A. Rodrigues, and M. P. Queluz, “Subjective and
objective quality assessment of omnidirectional video,” in Applications of
Digital Image Processing XLI, vol. 10752, International Society for Optics
and Photonics. SPIE, Sep. 2018, pp. 249–265.

[6] M. Yu, H. Lakshman, and B. Girod, “A framework to evaluate omnidirec-
tional video coding schemes,” in IEEE International Symposium on Mixed
and Augmented Reality, 2015, pp. 31–36.

[7] S. Chen, Y. Zhang, Y. Li, Z. Chen, and Z. Wang, “Spherical structural
similarity index for objective omnidirectional video quality assessment,”
in IEEE International Conference on Multimedia and Expo, 2018, pp. 1–6.
[8] V. Sitzmann, A. Serrano, A. Pavel, M. Agrawala, D. Gutierrez, B. Masia,
and G. Wetzstein, “Saliency in VR: How do people explore virtual envi-
ronments?” IEEE Transactions on Visualization and Computer Graphics,
vol. 24, no. 4, pp. 1633–1642, Apr. 2018.

[9] E. Upenik, M. ˇReˇr´abek, and T. Ebrahimi, “Testbed for subjective evalu-
ation of omnidirectional visual content,” in Picture Coding Symposium,
2016, pp. 1–5.

[10] H. Duan, G. Zhai, X. Min, Y. Zhu, Y. Fang, and X. Yang, “Perceptual
quality assessment of omnidirectional images,” in IEEE International
Symposium on Circuits and Systems, 2018, pp. 1–5.

[11] W. Sun, K. Gu, S. Ma, W. Zhu, N. Liu, and G. Zhai, “A large-scale
compressed 360-degree spherical image database: From subjective quality
evaluation to objective model comparison,” in IEEE 20th International
Workshop on Multimedia Signal Processing, 2018, pp. 1–6.

[12] M. Huang, Q. Shen, Z. Ma, A. C. Bovik, P. Gupta, R. Zhou, and X. Cao,
“Modeling the perceptual quality of immersive images rendered on head
mounted displays: Resolution and compression,” IEEE Transactions on
Image Processing, vol. 27, no. 12, pp. 6039–6050, Dec. 2018.

[13] M. Chen, Y. Jin, T. Goodall, X. Yu, and A. C. Bovik, “Study of 3D
virtual reality picture quality,” IEEE Journal of Selected Topics in Signal
Processing, vol. 14, no. 1, pp. 89–102, Jan. 2020.

[14] D. Noton and L. Stark, “Scanpaths in saccadic eye movements while
viewing and recognizing patterns,” Vision Research, vol. 11, no. 9, pp. 929
– 942, Sep. 1971.

[15] F. Dufaux and J. Konrad, “Efﬁcient, robust, and fast global motion estima-
tion for video coding,” IEEE Transactions on Image Processing, vol. 9,
no. 3, pp. 497–501, Mar. 2000.

[16] Z. Wang and A. C. Bovik, Modern Image Quality Assessment.

San

Rafael, CA, USA: Morgan Claypool Publishers, 2006.

[17] Z. Tu, C.-J. Chen, L.-H. Chen, N. Birkbeck, B. Adsumilli, and A. C.
Bovik, “A comparative evaluation of temporal pooling methods for blind
video quality assessment,” CoRR, vol. abs/2002.10651, 2020. [Online].
Available: https://arxiv.org/abs/2002.10651

[18] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
assessment: From error visibility to structural similarity,” IEEE Transac-
tions on Image Processing, vol. 13, no. 4, pp. 600–612, Apr. 2004.
[19] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
style transfer and super-resolution,” in European Conference on Computer
Vision, 2016, pp. 694–711.

[20] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unrea-
sonable effectiveness of deep features as a perceptual metric,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp. 586–
595.

[21] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, “Image quality assessment:
Unifying structure and texture similarity,” CoRR, vol. abs/2004.07728,
2020. [Online]. Available: https://arxiv.org/abs/2004.07728

[22] K. Zeng and Z. Wang, “3D-SSIM for video quality assessment,” in IEEE
International Conference on Image Processing, 2012, pp. 621–624.
[23] W. Kim, J. Kim, S. Ahn, J. Kim, and S. Lee, “Deep video quality assessor:
From spatio-temporal visual sensitivity to a convolutional neural aggre-
gation network,” in European Conference on Computer Vision, 2018, pp.
219–234.

[24] M. Xu, J. Chen, H. Wang, S. Liu, G. Li, and Z. Bai, “C3DVQA:
Full-reference video quality assessment with 3D convolutional neural
network,” CoRR, vol. abs/1910.13646, 2019. [Online]. Available:
http://arxiv.org/abs/1910.13646

[25] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical
panoramic video,” in Optics and Photonics for Information Processing
X, vol. 9970, International Society for Optics and Photonics. SPIE, Sep.
2016, pp. 57 – 65.

[26] G. Luz, J. Ascenso, C. Brites, and F. Pereira, “Saliency-driven omnidi-
rectional imaging adaptive coding: Modeling and assessment,” in IEEE
International Workshop on Multimedia Signal Processing, 2017, pp. 1–6.
[27] M. Xu, C. Li, Z. Chen, Z. Wang, and Z. Guan, “Assessing visual quality
of omnidirectional videos,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 29, no. 12, pp. 3516–3530, Dec. 2019.
[28] J. Xu, Z. Luo, W. Zhou, W. Zhang, and Z. Chen, “Quality assessment of
stereoscopic 360-degree images from multi-viewports,” in Picture Coding
Symposium, 2019, pp. 1–5.

[29] B. Series, “Methodology for the subjective assessment of the quality of
television pictures,” Recommendation ITU-R BT, pp. 500–13, 2012.
[30] D. S. Hands and S. E. Avons, “Recency and duration neglect in subjective
assessment of television picture quality,” Applied Cognitive Psychology,
vol. 15, no. 6, pp. 639–657, Nov. 2001.

[31] B. G. Tabachnick and L. S. Fidell, Using multivariate statistics. 5th ed,

Pearson Education, 2007.

[32] Y. Ye, E. Alshina, and J. Boyce, “JVET-G1003: Algorithm description of
projection format conversion and video quality metrics in 360lib version
4,” Joint Video Exploration Team, Turin, Italy, Rep. JVET-G1003, Tech.
Rep., Jul. 2017.

[33] J. Besharse and D. Bok, The Retina And Its Disorders. Academic Press,

2011.

[34] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural sim-
ilarity for image quality assessment,” in The Thrity-Seventh Asilomar
Conference on Signals, Systems Computers, vol. 2, 2003, pp. 1398–1402.
[35] S. S. Channappayya, A. C. Bovik, C. Caramanis, and R. W. Heath, “SSIM-
optimal linear image restoration,” in IEEE International Conference on
Acoustics, Speech and Signal Processing, 2008, pp. 765–768.

[36] J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Variational
image compression with a scale hyperprior,” CoRR, vol. abs/1802.01436,
2018. [Online]. Available: http://arxiv.org/abs/1802.01436

[37] J. Snell, K. Ridgeway, R. Liao, B. D. Roads, M. C. Mozer, and R. S. Zemel,
“Learning to generate images with perceptual similarity metrics,” in IEEE
International Conference on Image Processing, 2017, pp. 4277–4281.
[38] S. Wang, A. Rehman, Z. Wang, S. Ma, and W. Gao, “SSIM-motivated rate-
distortion optimization for video coding,” IEEE Transactions on Circuits
and Systems for Video Technology, vol. 22, no. 4, pp. 516–529, Apr. 2012.
[39] H. R. Sheikh, A. C. Bovik, and G. de Veciana, “An information ﬁdelity
criterion for image quality assessment using natural scene statistics,” IEEE
Transactions on Image Processing, vol. 14, no. 12, pp. 2117–2128, Dec.
2005.

[40] ——, “An information ﬁdelity criterion for image quality assessment using
natural scene statistics,” IEEE Transactions on Image Processing, vol. 14,
no. 12, pp. 2117–2128, Dec. 2005.

[41] Netﬂix,

[Online]. Available:

“Toward a practical perceptual video quality metric,”
https://medium.com/netﬂix-techblog/

2016.
toward-a-practical-perceptual-video-quality-metric-653f208b9652
[42] V. Laparra, A. Berardino, J. Ball´e, and E. P. Simoncelli, “Perceptually
optimized image rendering,” Journal of the Optical Society of America A,
vol. 34, no. 9, p. 1511, Sep. 2017.

[43] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, “Comparison
of
image processing
systems,” CoRR, vol. abs/2005.01338, 2020. [Online]. Available:
https://arxiv.org/abs/2005.01338

image quality models for optimization of

[44] K. Seshadrinathan and A. C. Bovik, “Temporal hysteresis model of time
varying subjective video quality,” in IEEE International Conference on

Acoustics, Speech and Signal Processing, 2011, pp. 1153–1156.

[45] VQGE, “Final report from the video quality experts group on the
validation of objective models of video quality assessment,” 2000.
[Online]. Available: http://www.vqeg.org

[46] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, “A statistical evaluation of
recent full reference image quality assessment algorithms,” IEEE Transac-
tions on Image Processing, vol. 15, no. 11, pp. 3440–3451, Nov. 2006.

[47] A. K. Moorthy and A. C. Bovik, “Visual importance pooling for image
quality assessment,” IEEE Journal of Selected Topics in Signal Processing,
vol. 3, no. 2, pp. 193–201, Apr. 2009.

[48] Y. Zhang, Y. Wang, F. Liu, Z. Liu, Y. Li, D. Yang, and Z. Chen, “Subjective
panoramic video quality assessment database for coding applications,”
IEEE Transactions on Broadcasting, vol. 64, no. 2, pp. 461–473, 2018.

[49] F. Jabar, J. Ascenso, and M. P. Queluz, “Field-Of-View Effect on the
perceived quality of omnidirectional images,” in IEEE International Con-
ference on Multimedia Expo Workshops, 2020, pp. 1–6.

[50] H. G. Kim, H. Lim, S. Lee, and Y. M. Ro, “VRSA Net: VR sickness
assessment considering exceptional motion for 360° VR video,” IEEE
Transactions on Image Processing, vol. 28, no. 4, pp. 1646–1660, Apr.
2019.

Xiangjie Sui received the B.E. degree from the Jiangxi
University of Finance and Economics, Nanchang, China,
in 2018. He is currently pursuing the M.A.Sc. degree
with the School of Information Management, Jiangxi
University of Finance and Economics, Nanchang, China.
His research interests include visual quality assessment,
and VR image/video processing.

Kede Ma (S’13–M’18) received the B.E. degree
from the University of Science and Technology of
China, Hefei, China, in 2012, and the M.S. and Ph.D.
degrees in electrical and computer engineering from the
University of Waterloo, Waterloo, ON, Canada, in 2014
and 2017, respectively. He was a Research Associate
with the Howard Hughes Medical Institute and New
York University, New York, NY, USA, in 2018. He is
currently an Assistant Professor with the Department
of Computer Science, City University of Hong Kong. His research interests
include perceptual image processing, computational vision, and computational
photography.

Yiru Yao received the B.E. degree from the Jiangxi
University of Finance and Economics, Nanchang, China,
in 2020. She is currently pursuing the M.A.Sc. degree
with the School of Information Management, Jiangxi
University of Finance and Economics, Nanchang, China.
Her research interests include visual quality assessment,
and VR image/video processing.

Yuming Fang (S’13–SM’17) received the B.E. degree
from Sichuan University, Chengdu, China, the M.S. de-
gree from the Beijing University of Technology, Beijing,
China, and the Ph.D. degree from Nanyang Technolog-
ical University, Singapore. He is currently a Professor
with the School of Information Management, Jiangxi
University of Finance and Economics, Nanchang, China.
His research interests include visual attention modeling,
visual quality assessment, computer vision, and 3D im-
age/video processing. He serves as an Associate Editor
for IEEE ACCESS. He serves on the Editorial Board of Signal Processing:
Image Communication.

