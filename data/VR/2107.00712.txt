Passing a Non-verbal Turing Test:
Evaluating Gesture Animations Generated from Speech

Manuel Rebol*
Graz University of Technology

Christian G ¨utl†
Graz University of Technology

Krzysztof Pietroszek‡
American University

1
2
0
2

g
u
A
1
2

]

V
C
.
s
c
[

2
v
2
1
7
0
0
.
7
0
1
2
:
v
i
X
r
a

Figure 1: Gesture generation for professor Jonathan G. (left: original video frame, middle: ground truth, right: generated gesture
animation) and television show host John O. (left: original video frame, middle: ground truth, right: generated gesture animation)

ABSTRACT
People communicate using both speech and non-verbal signals such
as gestures, facial expression, or body pose. Non-verbal signals
impact the meaning of the spoken utterance in an abundance of
ways. An absence of non-verbal signals impoverishes the process
of communication. Yet, when users are represented as avatars, it is
difﬁcult to translate non-verbal signals along with the speech into the
virtual world without specialized motion-capture hardware. In this
paper, we propose a novel, data-driven technique for generating ges-
tures directly from speech. Our approach is based on the application
of Generative Adversarial Neural Networks (GANs) to model the
correlation rather than causation between speech and gestures. This
approach approximates neuroscience ﬁndings on how non-verbal
communication and speech are correlated. We create a large dataset
that consists of speech and corresponding gestures in a 3D human
pose format from which our model learns the speaker-speciﬁc cor-
relation. We evaluate the proposed technique in a user study that is
inspired by the Turing test. For the study, we animate the generated
gestures on a virtual character. We ﬁnd that users are not able to
distinguish between the generated and the recorded gestures. More-
over, users are able to identify our synthesized gestures as related
or not related to a given utterance. Code and videos are available at
https://github.com/mrebol/Gestures-From-Speech

Keywords: Gesture Animation, GAN, 3D Human Pose Estimation,
Human Body Language, VR.

Index Terms: Computer systems organization—Neural networks
Computing methodologies—Activity recognition and understanding
Computing methodologies—Animation

1 INTRODUCTION
Non-verbal communication is an essential component of the com-
munication process, as it conveys a large portion of the message’s
meaning. Thus, equipping digital representations of humans with
non-verbal communication skills ﬁnds many applications in human-
computer systems, from telehealth to entertainment to education.

*e-mail: rebol@student.tugraz.at
†e-mail: c.guetl@tugraz.at
‡e-mail: pietrosz@american.edu

Despite ongoing research in human-machine communication, the
interaction with virtual humans does not feel similar to talking to a
person. We tackle the problem of unrealistic human-agent commu-
nication in virtual reality. We aim for synthesizing and animating
natural non-verbal communication in the form of hand and arm ges-
tures. Our objective is to improve the interaction between humans
and machines. Whenever people communicate, body language plays
an important role. Hand and arm gestures are performed to support
spoken language during communication and to convey meaning
effectively. Moreover, speech and gestures reﬂect the personality
and the intention of a person which is an important factor in virtual
online communication. Gestures are idiosyncratic and contain in-
formation such as personal feelings, empathy, aggressivity, as well
as authority and authenticity of a human [26, 31]. The meaning of
spoken language highly depends on body language because it has
the power to signal irony and even reverse the sense of words pro-
nounced. Therefore, the relationship between speech and gestures is
of great importance when animating virtual humans.

In this paper, we propose a novel, data-driven approach to gener-
ating meaningful gestures. We utilize the large amount of speaker
video data available to train a model such that it learns the relation-
ship by observing samples of speakers. We adopt the state-of-the-art
human pose estimation algorithms [5, 39, 53] to extract the 2D pose
from video and to project it into 3D space. Using our extraction
framework, we create a 3D speech-gesture human pose dataset
with two educational speakers and two comedians. Our resulting
dataset is signiﬁcantly larger than previous datasets [12, 46] that are
recorded using motion capture. Moreover, our approach of creating
3D speech-gesture data can be applied to any speaker with exiting
video material. In contrast to the dataset created by [16], our dataset
consists of human pose data in 3D space. We design a Genera-
tive Adversarial Network (GAN) [17] framework and apply deep
learning to train a model that generates speaker-speciﬁc gestures
from speech. We train our model on different speakers from our 3D
speech-gesture dataset. Compared to the GAN framework of [16],
our architecture predicts 3D pose data. Moreover, we enforce con-
stant bone length during the training of our GAN model to generate
anatomically plausible gestures. Once we have generated gestures,
we post-process the human pose data further such that we animate
smooth and plausible gestures on avatars in a virtual environment.
To achieve that, we apply skeletal constraints and inverse kinematic
computations. We design a user study to evaluate the qualitative
results of our trained gesture model. In contrast to other recent
approaches [1, 13, 16] which perform the user study directly on the

 
 
 
 
 
 
gesture output in human pose format, we animate the generated
gestures on an avatar in a virtual environment. Hence, instead of
comparing abstract skeletal representations of gestures, our study
setting is natural and closely related to watching real humans com-
municate. Our user study is set up in a format that is inspired by the
Turing test. We compare the gestures extracted from the original
video to our generated gestures side-by-side and ask the participants
of our study to identify the “real” gestures extracted from the video
source. In a second experiment, we compare our generated gestures
against gestures from an uncorrelated speech of the same speaker.
We show examples of speakers from two different domains and their
original and generated gestures animated on a virtual character in
Fig. 1.

We discuss related work in the area of gesture synthesizing in
the next section. Then, we describe our approach, consisting of the
3D gesture dataset creation, the gesture-speech translation with the
GAN, and the animation on a virtual character, in detail. We present
the evaluation of our method that was done by conducting a user
study in Sect. 4.

2 PRIOR WORK

Over the last three decades, many techniques were developed to
generate gestures and body posture for a virtual human [32–34].
Typically, the animation generation is divided into two stages. First,
the gesture and body pose to be generated is identiﬁed by analyzing
a speech or text of the utterance. In the second stage, the gesture and
body pose is generated and applied to the virtual human.

2.1 Communication Theory

Previously developed techniques often do not apply the communi-
cation theories and ignore the neurophysiological basis of gestures.
Our insight is supported by the Gestural Theory [21] according to
which non-verbal communication is not secondary to the speech’s se-
mantics or prosodics. Rather, speech and gestures are co-related and
emerge together from the communicative intention of the thoughts
forming in the speaker’s mind. The Gestural Theory also poses that
gesturing is not used only as a conscious way to illustrate speech,
but rather is an integral part of the thought process that generates the
speech [35]. To further support that claim, various studies provided
evidence that gestures are most effective when they are complemen-
tary rather than redundant with the semantics of the speech [44].
Other theories of communication, such as McNeill’s Growth Point
hypothesis also state that gestures and language emerge in a shared
process from a communicative intent [36]. In fact, new cognitive
models explore the distribution of communicative content across
output modalities rather than assuming speech-to-gesture causal-
ity [28].

An important related dimension of virtual human non-verbal com-
munication that is not addressed by previous animation synthesis
techniques is the viewer’s perspective. From the perspective of
the viewer, the process of understanding a gesture performed by
others (e.g. virtual human) is a combination of interpretations of
auditory and visual signals rather than being a purely visual phe-
nomenon. This is because humans are simultaneously verbally and
visually-oriented [45]. The human brain expects to process visible
body movement when we hear spoken words. If the speaker is not
visible, humans recreate the missing visuals in their imagination.
If the speaker’s body language does not meet the expectation or,
worse, clashes with the semantics of the utterance, the process of
communication is negatively affected.

Research in the neurology of speech suggests that the brain’s
auditory processing of speech is based on the gestural production of
sounds and the original vocal energy as referenced from the body,
rather than being based on the acoustical attributes of the sound
itself [10]. This is because our brains ﬁrst evolved to understand
visible gesture and then re-adapted for vocal “gestures”, to ﬁnally

learn to process spoken words. Based on these observations, we
conclude that the traditional approach to the generation of gestures
from speech does not take into account communication theories and
neuroscience ﬁndings. This disconnect was previously acknowl-
edged by Chiu et al.
, who wrote: “It is believed that in human
communication, the brain is co-planning the gesture and the utter-
ance, so approaches that do not use future information about the
planned utterance may be unlikely to match the sophistication of
human gesture-speech coordination.” [8].

2.2 Gesture Generation

An example of successful generation of beat gestures was presented
by Levine et al. [32], who generated co-verbal gestures from speech
by extracting prosodic (tone and intonation) variations from the
speech. The approach was unsuccessful in temporally matching the
expressiveness of the speech with iconic or metaphoric gestures,
as the information required to do so is not present in the prosodic
properties of the speech. This concern was addressed by combining
prosody with parsing of the spoken text [34]. Text can be ana-
lyzed for emotional content and rhetorical style, providing a rich
– although imposed by text semantics and thus lacking subtleties
of subtext interpretation – basis for iconic and metaphoric gesture
synthesis. Finally, text-driven rule-based approaches [33] create a
sequence of gestures, but the rules must be coded manually. As a
result, the gesture animations produced using rule-based systems
tend to be time-consuming, repetitive, and lacking in variation.

Gesture synthesis, once a gesture to be generated is identiﬁed,
often uses kinematic procedural techniques [18]. As an alterna-
tive, Kopp and Wachsmuth [29] presented a neurophysiologically-
based approach to drive the trajectory of gesturing arm motions.
Physics-based simulation has also been implemented, achieving bet-
ter visual ﬁdelity than kinematic-based approaches [49]. Procedural
techniques enable full control of the motion, allowing to map the
temporal dimension of the gesture to the temporal dimension of the
speech. Unfortunately, they usually result in unnatural-looking hand
movement and thus cause the uncanny valley effect [42].

To mitigate the uncanny valley effect, techniques based on motion
capture data have seen increased use. The use of motion graphs [30]
allows concatenated segments of motion captures to create a se-
quence, such as in [38, 40]. Chi et al. [7] use the effort and shape
components of Laban Movement Analysis to provide an expres-
sive parameterization of motion. Hartmann et al. [19] use tension,
continuity, and bias splines to control arm trajectories and provide
expressive control through parameters for activation, spatial and
temporal extent, ﬂuidity, and repetition. Pietroszek et al. proposed
synthesizing motion by comparing rough input with a large number
of high-quality motion templates in real-time using a compute shader
implementation of Dynamic Time Warping [40]. Project SAIBA that
combines the efforts of several research groups developed “behavior
realizers” [48]. The project is based on animation engines capable of
realizing commands written in the Behavior Markup Language [48].
These systems emphasize control and use a combination of proce-
dural techniques and motion captures, but they do not allow the
generation of gestures in real-time from speech.

Machine learning data-driven approaches were also applied to
predict the gestures that may be speciﬁc to a given person’s gestur-
ing style [25]. Recent work proposed applying deep learning to the
mapping from text and prosody to gesture [8]. Prior work [29] also
addressed modeling individual gesture styles through analyzing the
relationship between extracted utterance information and a person’s
gesture. While earlier works based on this approach have focused
on addressing the mapping relation between linguistic features and
gestures [27], recent work [28] has also addressed how to use acous-
tic features to help gesture determination. Chiu et al. [8] proposed a
data-driven model that builds upon prior work [3] by combining the
advantages of deep neural networks for mapping complex relation-

ships with an undirected second-order linear chain for modeling the
temporal coordination of speech and gestures.

Recurrent neural networks (RNNs) such as long short-term mem-
ory (LSTM) [23] or gated recurrent units (GRU) [9] have been at
the forefront of human motion prediction. However, such deep
neural networks based models are primarily deterministic. There
have been attempts to modify RNN encoder-decoder frameworks
to work as a combination of deterministic and probabilistic human
motion prediction models [14]. In recent advances, Jain et al. [24]
have introduced several structures and frameworks with deep RNNs
to produce state-of-the-art results in human motion prediction. In
contrast to RNN approaches, [4] proposed a sparse autoencoder
model to predict human motions in an unsupervised manner without
recurrent units. Even though these models are good at human motion
prediction, applying them to the generation of novel human actions
has been a challenge.

Most recent approaches focus on the non-deterministic relation-
ship between speech and gestures. To overcome the problem of
predicting the mean gesture, Alexanderson et al. [1] introduce a
probabilistic approach. They adapt the MoGlow framework [20]
which implements LSTM cells to generate gestures with realistic
motion. Ferstl et al. [13] use a GAN model in their non-deterministic
approach. They train the adversary to generate realistic sub-features
such as plausible dynamics and smooth motion. In contrast, Ginosar
et al. [16] implement a single adversary which ensures realistic
motion. Compared to previous approaches, they use in-the-wild
YouTube video data to train their speaker models. It allows them
to generate models for speakers from different genres, i.e. show
business, education and religion. Furthermore, their dataset is much
larger than previous datasets. For a given set of speakers, they pro-
cess more than 20 hours of data. This allows the models to learn
from a variety of different input gestures. One drawback of the work
of Ginosar et al. compared cannot be used in applications that re-
quire virtual humans. Although previous authors [1, 13, 16] evaluate
the results comprehensively, they conduct the human study using
gestures in the skeletal human pose format which is different from
observing humans in reality. Thus, for the evaluation of our genera-
tive approach, we animate the gestures on virtual characters which
are visually more closely related to real human communication.

3 OUR APPROACH

We model the correlation between body language and speech without
imposing a causality between them. We consider learning models
that correlate the input and output rather than create a causal rela-
tionship between them. As a result, rather than considering discrimi-
native models that model the conditional probability p(y|x), which
can be interpreted as a causal relationship from x to y, we focus
only on generative models that model the joint probability distribu-
tion p(x, y), which can be interpreted as a co-relational relationship
between x and y.

Translating our theoretical approach into the computation, we
design a GAN architecture that predicts gestures given input speech.
The goal of the network is to learn the co-relation, not causality,
in a <speech, gesture> pair. Once trained, the GAN generates
the gesture sequence given the speech sequence. Our non-verbal
communication animation model is therefore data-driven. The GAN
takes speech as input and predicts the gesture animation as output.
Our method is divided into three core components. We illustrate
the structural overview in Fig. 2. The gestures in human pose format
are provided by our ﬁrst component 3D Human Pose Estimation.
We estimate the human pose in each frame of input video sequences.
Once the 2D human pose is estimated, it is projected into 3D space
and handed over to the GAN. The gesture generation GAN rep-
resents the main component. Our GAN takes two inputs during
training, the raw audio of the speech and the gestures in human pose
format. We extract the raw audio from our video dataset. The third

component animates the generated gestures on a virtual character.
This component computes the rotation angles of body parts given
pose data and enforces physically plausible animations.

3.1 3D Human Pose Estimation

The amount and the quality of the training data determine the pre-
dictive power of our model. In the multimodal task of gesture gener-
ation, it is of utmost importance to have a large dataset that contains
a wide variety of gestures. Unfortunately, existing motion capture
datasets of gestures do not sufﬁce, as they are neither large enough
nor sufﬁciently labeled. Instead of relying on existing, insufﬁcient
datasets, we generate the training data from another readily-available
large source of data: 2D videos of people performing lectures and
speeches. Recent advancements in motion capture estimation from
2D video [6, 11, 52] allow us to process many online videos of lec-
tures and speeches and extract 3D body poses and gestures from
them.

We extract the gestures from video data in the 3D human pose
format. This efﬁcient representation of gestures allows us to extract
only the information required for gesture modeling. By extracting
the human pose, we ignore image information such as background,
color, and shape, which is irrelevant to our task. We take the fol-
lowing precautions to ensure high quality of data, both before and
after extracting body language from the video. In the process, we
ensure that the extracted gestures are still temporally synchronized
with audio. Before extracting motion from a video, we eliminate
those fragments of the videos where the face, arms, or hands of the
speaker are not entirely visible, e.g. because it is being partially
occluded by an object.

We approach the gesture extraction in two steps.

In the ﬁrst
step, we extract the 2D Human Pose from the raw video using the
OpenPose framework [5, 43]. In the second step, we project the 2D
pose into 3D space for the large body parts and hands separately.
For the 3D body pose estimation, we use the model implemented by
Pavllo et al. [39]. For the 3D hand pose estimation, we implement
a model similar to Zimmerman et al. [53]. We create a continuous
heatmap by applying Gaussian kernels at the estimated OpenPose
2D keypoint positions. We also add temporal smoothing to improve
video processing.

As a result of the above process, we created a large dataset of
motion captures that correspond to the gestures used by humans
when speaking. Throughout the process, we preserved the time
synchronization between the audio recording of the speech and the
extracted motion corresponding to the speech. The extracted motion
capture sequences serve as the speech time series that is labeled by
the gesture time series during the supervised training of our network.

3.2 Gesture Generation - Generative Adversarial Net-

work

We implement the Generative Adversarial Neural Network (GAN)
[2,17,37,51] framework that allows us to model the multimodal task
of predicting gestures from speech. Inside the GAN framework, we
locate the gesture generator G and the motion discriminator D.

Gesture Generator The objective of the gesture generator is
twofold. First, the generator is trained to predict gestures close to
the ground truth gestures extracted from the input video. Second,
the main objective of the generator inside the GAN framework is to
fool the discriminator. Consequently, the predicted gestures should
have realistic motion.

We implement the UNet architecture introduced by [41] for our
generator. The UNet models the relationship between the gestures
encoded in human pose format and speech. Inside the UNet, the
skip connections forward low-level prosodic features extracted from
the input audio. These features are necessary to predict smaller
beat gestures. The UNet bottleneck extracts high-level features that

Figure 2: Gesture pipeline. Our gesture generation pipeline takes videos of speakers as an input (top left) and produces animated gestures (top
right). In between, three major components exist: 3D Human Pose Estimation (blue), Gesture Generation with a Generative Adversarial Network
(red) and Pose Animation (green).

contain information about long input sequences. This is used to
predict the posture of the speaker.

We introduce the following loss function to train the parameters

of our UNet model:

LGen(G) = E

(s,p)[||p − G(s)||1] +
λbone E

(s)[||B(G(st )) − B(G(st−1))||1],

(1)

where vector s refers to the input speech and vector p refers to the
pseudo ground truth body keypoints. The function B returns the bone
lengths in 3D space by computing the euclidean distance between
pairs of keypoints at consecutive time steps t and t − 1.

The ﬁrst term in Equation 1 ensures that model learns to predict
pose positions G(s) that are similar to the ground truth pose positions
p extracted from the video. The second term Equation 1 ensures that
the predicted bone lengths stay constant over time. This term forces
the model to learn about the anatomical constraint of constant bone
length in the human skeleton. The hyperparameter λbone ∈ (0, 1) is
used to weight the importance of constant bone length term with
respect to the other terms within our objective function deﬁned in
Equation 4. During training, we accept a higher loss from the ﬁrst
term to avoid regressing to the mean pose. The loss of the second
term remains low.

Motion Discriminator The communication theory suggests that
the same utterance can lead to different gestures. Hence, the regres-
sion loss between generated gestures and pseudo ground truth does
not model the relation between speech and gestures as a whole. The
problem is the regression towards the mean pose. For example, if the
input data includes the same utterance twice where one time the pose
moves into the direct opposite direction of the other time, the model
learns to predict no motion. When considering the whole training
dataset, this will result in predictions with less motion compared to

the training dataset. To avoid this phenomenon, our discriminator
inside the Gesture GAN architecture ensures that the motion of the
generated gestures is similar to the motion extracted from video.

The discriminator receives a motion sequence, either predicted
by the generator or obtained from the input video, as input. The
motion is computed by taking the difference between consecutive
pose vectors. We express this by introducing the function M

M(v) = vt − vt−1 ,

(2)

which computes the motion between two consecutive pose vectors
vt and vt−1.

The objective of the discriminator is to detect if a given gesture
sequence is real or generated, while the generator tries to fool the
discriminator. Consequently, we have created a two-player optimiza-
tion game. As a result, the generator tries to predict gestures similar
to the real gestures in terms of the motion. The discriminator tries
to identify the predicted gestures as such, even as they get more
realistic during training.

The complete GAN loss function including the discriminator D

is deﬁned as

LGAN(G, D) = E

(p)[log D(M(p))] + E

(s)[log(1 − D(M(G(s))))] .
(3)
The objective of the discriminator is to maximize this function.
Consequently, the term loss is only true concerning the generator.
The discriminator is trained to output D(·) → 1 if input motion is
real and D(·) → 0 if the input motion is generated.

GAN Objective We train the parameters of our model by com-
bining the loss functions shown in Equation 1 and Equation 3. The
ﬁnal objective function is deﬁned as

min
G

max
D

LGAN(G, D) + LGen(G).

(4)

UNet GestureGeneratorMotionDiscriminatorMel SpectogramReal orGenerated?Real 3D Gestures  . . .Generated 3D Gestures. . .Speaker Video . . .Gesture Generation - Generative Adversarial NetworkAudioFrames2D HumanPose Estimation3D BodyPose Estimation3D HandPose Estimation3D HumanPoseAnimated Gestures. . .PlausibilityConstraints2D Pose3D Human Pose EstimationPose AnimationJoint-to-RotationComputationThe generator G has the objective to minimize this function whereas
the discriminator D aims to maximize LGAN.

3.3 Pose Animation

Our Gesture GAN model produces 3D Human Pose sequences which
we animate on virtual humans using rotation angles between bones
and inverse kinematic computations. By connecting the keypoints
predicted by our Gesture GAN, we create a skeletal representation of
the human pose. We use this skeletal representation of the gestures
to animate an avatar in a virtual environment.

One challenge when transferring the predicted 3D Human Pose
into a 3D animation is the missing information about anatomical
details and ambiguities. Since we animate the gestures on a virtual
human of different size and shape compared to the original speaker,
we omit the information about the bone length of the generated
skeleton. Instead, we only consider the rotation between the bones.
When viewing the angles between two bones in the Euler angle
representation, we obtain the pitch and yaw angles from the skeleton.
However, we do not have any information about the roll angle. To
tackle this problem, we approximate the roll rotation of different
body parts using inverse kinematics.

Besides recovering the missing information about the roll angle,
we also have to deal with the second problem introduced when con-
verting the human pose representation to a virtual human animation,
which is implausible motion. Although our Gesture GAN is trained
to predict motion which is similar to real human motion, there exists
no constraint which particularly enforces anatomically plausible
motion. Hence, the predicted motion in some cases appears to be
very artiﬁcial, especially when animated on a virtual human avatar.
This problem is very distracting for the viewer of the animation
because anatomically implausible motion is quickly noticed. To
overcome this issue, we introduce motion constraints on the ﬁngers.
The motion constraints enforce that the ﬁngers can only be bent
in anatomically possible angles and directions. In addition to the
motion constraints, we apply motion smoothing.

3.4 Training the Network

For the purpose of training our GAN, we generate a gesture dataset of
over one hundred hours extracted from videos of four speakers from
two domains. Speciﬁcally, we generated 72 hours of motion capture
data for television show hosts John O. and Ellen D. as well as 64
hours of motion capture data for professor Jonathan G. and Shelly K.
We encode the gestures in the efﬁcient human pose format. By using
this format, we ignore irrelevant information such as background
and the shape of different body parts of the speaker.

We feed the extracted gestures in 3D human pose format to the
discriminator inside our GAN architecture. The input to our GAN
generator is the Mel spectrogram of the audio directly extracted from
the training videos of each speaker. We sample the audio data at a
rate of 16,000 samples per second. In order to provide the generator
with the temporal context, the input interval is four seconds long,
which was found to be the best length empirically. We evaluate
the predictions of our Gesture GAN on our validation set using the
Percent of Correct Keypoints (PCK) [50] metric with proximity
radius α = 0.2. The quantitative results are shown in Table 1. We
observe that John O. and Shelly K. who are in sitting position and
therefore show less upper body movement achieve a higher PCK.
In contrast, the PCK is lower for speakers that are standing (Ellen
D.) and walking around (Jonathan G.). We select the speakers John
O. because of the high PCK and Jonathan G. because we want to
examine both sitting and standing speakers for further experiments
in our user study.

4 EXPERIMENTAL EVALUATION

In order to validate the perceptual quality of the synthesised anima-
tions, we performed a user study designed to answer the following

Category

Speaker

PCK ↑

TV Show

Education

Ellen D.
John O.
Shelly K.
Jonathan G.

31.0
60.3
40.6
23.6

Table 1: Quantitative evaluation of gestures. We evaluate the four
speakers from the two categories TV Show and Education on our
validation datasets. For comparison we use the Percent of Correct
Keypoints (PCK) metric.

Figure 3: Virtual scene. The scene shows the two virtual characters
performing non-verbal communication next to each other. We use this
setting for our side-by-side comparisons.

questions:

1. Can users detect difference between the synthesized gesture
and the ground truth (original) gesture for a given speech
fragment?

2. Are synthesized co-verbal gestures perceived by users as cor-

related with a given speech fragment?

To answer the above questions, we designed two experimental tasks.
In the ﬁrst task, we present users with a series of side by side
videos of two identical avatars performing different gestures for the
same, 12-second long speech fragment that can be heard by the users.
One of the videos shows the ground through gestures, as extracted
from the video recording of the speaker and applied to the avatar.
The other video shows the gestures synthesized by our system from
the same speech fragment. The left-right position of the videos is
randomized. The user is asked to decide which sequence of gestures
is the ground truth gesture for the speech fragment.

In the second task, we also present the user with a series of videos
of two identical avatars performing different gestures for 12-second
long speech fragment.
In this tasks gesture sequences for both
avatars are synthesized. However, one of the gestures sequence,
either right or left, is synthesized from the speech fragment heard
by the participant, while the other avatar is performing gestures
synthesized for a different, randomly selected speech fragment from
the same speaker. The user is asked to decide which sequence

Figure 4: User study. The participants in our study were asked to
decide which animated gestures they perceived as more natural. The
gestures were animated on two identically looking avatars, referred to
as A and B.

of gestures is the ground truth gesture for the presented speech
fragment.

In both tasks, the user can hear the speech and see the avatar
performing the gestures, but the face of the gesturing avatar is hidden.
We consider face expression and lips synchronization a confounding
factor in our study, resulting from the uncanny valley effect. Our
experimental design is inspired by the original Turing test [47].
Our system “passed” our test, if after a limited number of attempts,
the human judges are unable to establish a statistically signiﬁcant
difference between the sequence of gestures generated by our system
and the real gestural sequence.

4.1 Implementation and Apparatus
For the visualization of our generated gestures, we animate the UMA
2 Multipurpose Avatars using the Unity 3D game engine version
2019.3. Both animations of the side-by-side comparison run on two
identical avatars, see Fig. 3. The only difference between the avatars
are the gestures that they perform. The avatars are placed against a
neutral background, with a light source in front of them. The avatars
are positioned at a distance of 1.5 meters next to each other, such
that they do not occlude each other while gesturing. Both avatars
face the camera at the same 180° angle. The camera is positioned
three meters in front of the avatars. The camera angle and shot size is
adjusted such that it records the upper body of both avatars without
showing the face.

While the avatars are performing the gestures in 3D space, the
virtual camera records the animation in 2D at the resolution of
1000 × 360 at 30 frames per second. We record the avatars from
the hip upwards to only capture the information relevant for gesture
evaluation and to eliminate the face expression as a confounding
factor. An example of our setting is shown in Figure 4.

4.2 Null Hypotheses
For the ﬁrst task, the null hypothesis is that the proposed gesture
synthesis is perceptually indistinguishable from the ground truth.
For the second task, the null hypothesis is that the speech fragment
is not correlated with the synthesised gesture.

4.3 Participants
For our study, we recruited 100 participants on Amazon Mechanical
Turk (Mturk) platform. Each participant received a compensation
of $3. The requirement was that the participants are ﬂuent in En-
glish. All participants ﬁnished the study. We did not collect any
demographic data, or any identiﬁable data or information about par-
ticipants’ prior computer-related experience – all collected data was
anonymous at all time.

4.4 Procedure
The participants were ﬁrst presented with the description of the
study and asked to conﬁrm their willingness to participate. Then, the
participants were presented with three repetitions of task 1 and task

Figure 5: Quantitative results. The four bars refer to the four differ-
ent tasks the participants completed during the user study for the
speakers John O. and Jonathan G. The height of the bar represents
the rate at which the participants selected the gestures predicted by
our model. The vertical intervals at the top of each bar indicate the
standard deviation across the participants.

2 as a training. Next, the participants were presented with a series of
twenty 12-second long videos for task 1 followed by a series of ten
12-seconds long videos for task 2. In each task, half of the videos
were generated for the speech of John O., while the other half were
generated from the speech of professor Jonathan G.

For each task, the participants were asked to select the video that
feels as a more natural representation of gestures for the presented
speech fragment. Participants were forced to select one option,
before they could proceed to the next video. Also, we ensured
that it was not possible to select the answer without watching the
whole video. Additionally, it was not possible to watch the same
video twice. Once completed, the participants were presented with a
“thank you” page.

4.5 Results

We illustrate the quantitative results for both speakers in Fig. 5. We
separate the two tasks for each speaker.

For task 1, determining which gestures are generated and which
are ground truth, the participants mistook synthesized gestures as
ground truth 52.5% of the time on average (σ 2 = 1.0%). The differ-
ence between selecting ground truth and synthesized gestures were
not signiﬁcant. The results for individual speakers were not signiﬁ-
cant either, with participants selecting synthesized gesture over the
ground truth, in average, 51.2% of the time for John O. (σ 2 = 1.4%),
and 53.8% of the time for professor Jonathan G. (σ 2 = 0.6%).

For task 2, determining which gesture was generated for a given
speech fragment, the participants chose the gesture sequence that
was indeed generated for the given speech fragment in average
65% of the time (σ 2 = 3.55%). Interestingly, the participants were
signiﬁcantly more likely to point at a correct gesture sequence in
case of gestures generated for John O., selecting it correctly 71.2%
of the time, while correctly identifying the gesture sequence, on
average, 58.8% of the time for gestures generated for professor
Jonathan G. In both cases considered individually, the experiment
showed signiﬁcant difference between the gestures generated for
the heard fragment of speech and gestures generated for some other
fragment of speech from the same speaker.

Figure 6: Qualitative results. We compare the predicted gestures animated on a virtual character (top) and the original video (bottom) of speaker
Jonathan G. Overall, the motion and the beat gestures are very similar. Whenever Jonathan G. lifts his hand, the same motion is predicted by
our model. In column two and three we show that sometimes only one or the other hand is lifted in the prediction. In column four, we show that
metaphoric gesture “intersection” is not predicted by our model.

4.6 Discussion

Based on the above results, we conclude that our system generates
realistic body language, since the participants were unable to deter-
mine whether the gesture animations were generated or captured. It
is important to note that our study does not attempt to establish that
our gesture synthesis creates gesture sequences that are perceptually
superior to the ground truth. Instead, we attempt to establish that,
despite a relatively large and diverse sample, the participants are
unable to reject the null hypothesis for task 1, while at the same time
being able to reject the null hypothesis for task 2. In other words,
the closer our system is to ground truth, the more difﬁcult it is to
reject the null hypothesis for task 1.

Rejecting the null hypothesis in task 1 would mean that our system
generates gestures that are perceptually inferior as compared to the
ground truth. Not rejecting the hypothesis, however, does not mean
we have proven the equivalence between synthesized and extracted
gestures – it is not possible to do so. However, not rejecting the null
hypothesis shows that a difference, if any, between the synthesized
gestures and ground truth gestures must be small.

The inability to reject the null hypothesis for task 1 could be a
result of either the difference between the two groups being too
small for our sample size to be signiﬁcant or it could be a result
of the participants not paying attention to the study and selecting
right or left videos at random. However, the rejection of the null
hypothesis for task 2 serves as a validation for the claim that our gen-
erated gestures are indistinguishable from the real gestures, because
ﬁnding signiﬁcant difference in task 2 suggests, that the participants
were indeed paying attention to the presented videos and were not
selecting answers A or B at random.

We also took a closer look at each speciﬁc user’s decision in judg-
ing the generated vs ground truth gesture animations. The analysis
revealed that participants were fooled by our gesture prediction in
those gesture sequences, in which the ground truth does not include
an iconic gestures. Whenever the speaker performs an iconic gesture,

subjects were able to identify the ground truth. This is an interest-
ing result, since it suggests that the fact that our system does not
model the speech semantics and therefore does not explicitly gener-
ate iconic gestures might provide our users with a winning strategy:
searching for iconic gestures in the speech and identifying those. It
remains an open question whether or not sufﬁciently large training
dataset would result in the system encoding correlation between
semantics of speech and iconic gestures. We give an insight into the
qualitative results of speaker Jonathan G. in Fig. 6.

Another qualitative observation is the difference in the correct
recognition of gestures generated for a given speech fragment be-
tween John O. and Jonathan G. gestures. We observe that ground
truth gestures of John O., a comedian, are more pronounced and
exaggerated. Possibly, the gestures performed by John O., an experi-
enced actor, may be better correlated with his speech than are the
gestures of professor Jonathan G. If that is true, it may be possible
to create a model that actually generates better gestures for a given
speaker than the speaker’s own gestures.

One aspect that needs to be considered when interpreting our
results is that we animated real and generated gestures on virtual
humans which cannot be directly compared to a real human com-
munication which is more complex. In real human communication,
the human brain uses biological motion detection when watching
living beings [15]. Biological and nonbiological motion is detected
differently as shown by Hiris [22]. As a consequence, watching
gestures of a speaker on video and animated gestures on avatars are
judged differently by humans.

5 CONCLUSION

Generation of nonverbal communication, including gestures, is a
missing cornerstone in creating virtual humans that communicate
as efﬁciently as humans. Human-agent communication is used
broadly in online learning environments, virtual assistance, and
communication tools.

In this work, we predict the gestures from speech and animate
them on virtual characters in a 3D environment. We implemented
a novel 3D human pose estimation pipeline, which allows us to
train speaker-speciﬁc gesture generation models from in-the-wild
video data. Utilizing large-scale video datasets allows our model
to learn from a variety of different gestures. We implement the
non-deterministic relationship between speech and gestures using a
GAN model.

The results of our user study show that the gestures predicted
by our model are natural and correspond to speech. 51.2% and
53.8% of participants selected that our generated gestures seem
more natural than the original gestures for speakers John O. and
Jonathan G., respectively. Furthermore, when comparing speech
correlated and uncorrelated gestures from the same speaker, partici-
pants identiﬁed the speech correlated gestured correctly in 71.2% of
times for speaker John O. and 58.8% for speaker Jonathan G. The
results of both experiments indicate that our method can be applied
successfully to speakers from different genres.

In future work, we will develop a model that synthesizes gesture
animations given a speech stream of an arbitrary speaker. Therefore,
data from multiple speakers needs to be combined such that a uni-
versal speech-gesture model can learn about how gesturing styles
depend on the voice and prosody of humans.

ACKNOWLEDGMENTS

We want to thank Graz University of Technology and American
University for maintaining a successful collaboration and support-
ing research stays abroad. This collaboration was endorsed by the
Marshall Plan Foundation.

REFERENCES

[1] S. Alexanderson, G. Henter, T. Kucherenko, and J. Beskow. Style-
controllable speech-driven gesture synthesis using normalising ﬂows.
Computer Graphics Forum, pp. 487–496, 2020.

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adver-
sarial networks. In International Conference on Machine Learning, pp.
214–223, 2017.

[3] T. Artieres et al. Neural conditional random ﬁelds. In Proceedings of
the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, pp. 177–184, 2010.

[4] J. B¨utepage, M. J. Black, D. Kragic, and H. Kjellstr¨om. Deep represen-
tation learning for human motion prediction and classiﬁcation. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), p.
2017. IEEE, 2017.

[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh.
Openpose: Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2019.

[6] C.-H. Chen and D. Ramanan. 3d human pose estimation= 2d pose
estimation+ matching. arXiv preprint arXiv:1612.06524, 2016.
[7] D. Chi, M. Costa, L. Zhao, and N. Badler. The emote model for
effort and shape. In Proceedings of the 27th annual conference on
Computer graphics and interactive techniques, pp. 173–182. ACM
Press/Addison-Wesley Publishing Co., 2000.

[8] C.-C. Chiu, L.-P. Morency, and S. Marsella. Predicting co-verbal
gestures: a deep and temporal modeling approach. In International
Conference on Intelligent Virtual Agents, pp. 152–166. Springer, 2015.
[9] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On the
properties of neural machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259, 2014.

[10] T. W. Deacon. The symbolic species: The co-evolution of language

and the brain. WW Norton & Company, 1998.

[11] Y. Du, Y. Wong, Y. Liu, F. Han, Y. Gui, Z. Wang, M. Kankanhalli, and
W. Geng. Marker-less 3d human motion capture with monocular image
sequence and height-maps. In European Conference on Computer
Vision, pp. 20–36. Springer, 2016.

[12] Y. Ferstl and R. McDonnell. Investigating the use of recurrent motion
modelling for speech gesture generation. In International Conference
on Intelligent Virtual Agents, p. 93–98, 2018.

[13] Y. Ferstl, M. Neff, and R. McDonnell. Multi-objective adversarial

gesture generation. In Motion, Interaction and Games, 2019.

[14] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik. Recurrent network
models for human dynamics. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4346–4354, 2015.

[15] S. Gilaie-Dotan, R. Kanai, B. Bahrami, G. Rees, and A. P. Saygin.
Neuroanatomical correlates of biological motion detection. Neuropsy-
chologia, 51(3):457 – 463, 2013.

[16] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik. Learn-
ing individual styles of conversational gesture. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE, June 2019.
[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances in neural information processing systems, pp. 2672–2680,
2014.

[18] B. Hartmann, M. Mancini, and C. Pelachaud. Implementing expressive
gesture synthesis for embodied conversational agents. In International
Gesture Workshop, pp. 188–199. Springer, 2005.

[19] B. Hartmann, M. Mancini, and C. Pelachaud. Implementing expressive
gesture synthesis for embodied conversational agents. In International
Gesture Workshop, pp. 188–199. Springer, 2005.

[20] G. E. Henter, S. Alexanderson, and J. Beskow. MoGlow: Probabilistic
and controllable motion synthesis using normalising ﬂows. ArXiv,
2019.

[21] G. W. Hewes, R. Andrew, L. Carini, H. Choe, R. A. Gardner, A. Kort-
landt, G. S. Krantz, G. McBride, F. Nottebohm, J. Pfeiffer, et al. Primate
communication and the gestural origin of language [and comments and
reply]. Current Anthropology, 14(1/2):5–24, 1973.

[22] E. Hiris. Detection of biological and nonbiological motion. Journal of

Vision, 7(12):4–4, 2007.

[23] S. Hochreiter and J. Schmidhuber. Lstm can solve hard long time lag
problems. In Advances in neural information processing systems, pp.
473–479, 1997.

[24] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-rnn:
Deep learning on spatio-temporal graphs. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 5308–
5317, 2016.

[25] M. Kipp. Gesture generation by imitation: From human behavior to

computer character animation. Universal-Publishers, 2005.

[26] M. Kipp and J. Martin. Gesture and emotion: Can basic gestural form
features discriminate emotions? In 2009 3rd International Conference
on Affective Computing and Intelligent Interaction and Workshops, pp.
1–8, 2009.

[27] S. Kita. The temporal relationship between gesture and speech: A
study of japanese-english bilinguals. MS, Department of Psychology,
University of Chicago, 90:91–94, 1990.

[28] S. Kopp, K. Bergmann, and S. Kahl. A spreading-activation model of
the semantic coordination of speech and gesture. In Proceedings of the
35th Annual Meeting of the Cognitive Science Society (CogSci 2013),
2013.

[29] S. Kopp and I. Wachsmuth. Synthesizing multimodal utterances for con-
versational agents. Computer animation and virtual worlds, 15(1):39–
52, 2004.

[30] L. Kovar, M. Gleicher, and F. Pighin. Motion graphs. In ACM SIG-

GRAPH 2008 classes, p. 51. ACM, 2008.

[31] R. Krauss, Y. Chen, and P. Chawla. Nonverbal behavior and nonver-
bal communication: What do conversational hand gestures tell us?
Advances in Experimental Social Psychology, 28:389–450, 1996.
[32] S. Levine, C. Theobalt, and V. Koltun. Real-time prosody-driven
synthesis of body language. In ACM Transactions on Graphics (TOG),
vol. 28, p. 172. ACM, 2009.

[33] M. Lhommet and S. C. Marsella. Gesture with meaning. In Interna-
tional Workshop on Intelligent Virtual Agents, pp. 303–312. Springer,
2013.

[34] S. Marsella, Y. Xu, M. Lhommet, A. Feng, S. Scherer, and A. Shapiro.
Virtual character performance from speech. In Proceedings of the 12th
ACM SIGGRAPH/Eurographics Symposium on Computer Animation,

pp. 25–35. ACM, 2013.

[35] D. McNeill. Hand and mind: What gestures reveal about thought.

University of Chicago press, 1992.

[36] D. McNeill. Gesture and thought. University of Chicago press, 2008.
[37] M. Mirza and S. Osindero. Conditional generative adversarial nets.

arXiv preprint arXiv:1411.1784, 2014.

[38] M. M¨uller, T. R¨oder, and M. Clausen. Efﬁcient content-based retrieval
In ACM Transactions on Graphics (ToG),

of motion capture data.
vol. 24, pp. 677–685. ACM, 2005.

[39] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli. 3d human pose
estimation in video with temporal convolutions and semi-supervised
training. In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2019.

[40] K. Pietroszek, P. Pham, S. Rose, L. Tahai, I. Humer, and C. Eckhardt.
Real-time avatar animation synthesis from coarse motion input. In
Proceedings of the 23rd ACM Symposium on Virtual Reality Software
and Technology, p. 71. ACM, 2017.

[41] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional net-
works for biomedical image segmentation. In International Conference
on Medical Image Computing and Computer-Assisted Intervention
(MICCAI), 2015.

[42] J. Seyama and R. S. Nagayama. The uncanny valley: Effect of realism
on the impression of artiﬁcial human faces. Presence: Teleoperators
and virtual environments, 16(4):337–351, 2007.

[43] T. Simon, H. Joo, I. Matthews, and Y. Sheikh. Hand keypoint detection
in single images using multiview bootstrapping. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.

[44] M. A. Singer and S. Goldin-Meadow. Children learn when their
teacher’s gestures and speech differ. Psychological Science, 16(2):85–
89, 2005.

[45] J. Streeck and M. L. Knapp. The interaction of visual and verbal fea-
tures in human communication. Advances in nonverbal communication,
10:3–23, 1992.

[46] K. Takeuchi, S. Kubota, K. Suzuki, D. Hasegawa, and H. Sakuta.
Creating a gesture-speech dataset for speech-based automatic gesture
generation. In HCI International 2017 - Posters Extended Abstracts,
pp. 198–202, 2017.

[47] A. M. Turing. Computing machinery and intelligence (1950). The
Essential Turing: The Ideas that Gave Birth to the Computer Age. Ed.
B. Jack Copeland. Oxford: Oxford UP, pp. 433–64, 2004.

[48] H. Vilhj´almsson, N. Cantelmo, J. Cassell, N. E. Chafai, M. Kipp,
S. Kopp, M. Mancini, S. Marsella, A. N. Marshall, C. Pelachaud, et al.
The behavior markup language: Recent developments and challenges.
In International Workshop on Intelligent Virtual Agents, pp. 99–111.
Springer, 2007.

[49] H. Welbergen, D. Reidsma, Z. M. Ruttkay, and J. Zwiers. Elckerlyc-
a bml realizer for continuous, multimodal interaction with a virtual
human. Journal on Multimodal User Interfaces, 3(4):271–284, 2010.
[50] Y. Yang and D. Ramanan. Articulated human detection with ﬂexible
mixtures of parts. IEEE Transactions on Pattern Analysis and Machine
Intelligence, pp. 2878–90, 2013.

[51] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas.
Stackgan: Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. arXiv preprint, 2017.

[52] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis.
Sparseness meets deepness: 3d human pose estimation from monocular
video. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4966–4975, 2016.

[53] C. Zimmermann and T. Brox. Learning to estimate 3d hand pose from
single rgb images. In IEEE International Conference on Computer
Vision (ICCV), 2017.

