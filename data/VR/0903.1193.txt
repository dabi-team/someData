9
0
0
2

n
u
J

9
1

]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[

3
v
3
9
1
1
.
3
0
9
0
:
v
i
X
r
a

Reality as simplicity

Giulio Ruﬃni

Starlab Barcelona, C. de l’Observatori, s.n., 08035 Barcelona, Spain

Abstract

The aim of this paper is to study the relevance of simplicity and its formal
representation as Kolmogorov or algorithmic complexity in the cognitive sci-
ences. The discussion is based on two premises: 1) all human experience is
generated in the brain, 2) the brain has only access to information. Taken
together, these two premises lead us to conclude that all the elements of
what we call ‘reality’ are derived mental constructs based on information
and compression, i.e., algorithmic models derived from the search for sim-
plicity in data. Naturally, these premises apply to humans in real or virtual
environments as well as robots or other cognitive systems. Based on this, it
is further hypothesized that there is a hierarchy of processing levels where
simplicity and compression play a major role. As applications, I illustrate
ﬁrst the relevance of compression and simplicity in fundamental neuroscience
with an analysis of the Mismatch Negativity paradigm. Then I discuss the
applicability to Presence research, which studies how to produce real-feeling
experiences in mediated interaction, and use Bayesian modeling to deﬁne in
a formal way diﬀerent aspects of the illusion of Presence. The idea is put
forth that given alternative models (interpretations) for a given mediated
interaction, a brain will select the simplest one it can construct weighted by
prior models. In the ﬁnal section the universality of these ideas and appli-
cations in robotics, machine learning, biology and education is discussed. I
emphasize that there is a common conceptual thread based on the idea of
simplicity, which suggests a common study approach.

Key words: Simplicity, Neuroscience, Kolmogorov Complexity, Presence

Preprint submitted to Brain Research Bulletin

April 24, 2022

 
 
 
 
 
 
1. Introduction

Reality is merely an illusion, albeit a very persistent one.
Everything should be made as simple as possible, but not simpler.
A. Einstein

In this paper I discuss a general paradigm to study cognition based on
ideas from algorithmic information and probability theory.
It is a brain-
centric, subjective paradigm. While following a long tradition in theories
about modeling and simplicity, its novelty lies perhaps in its application
in the cognitive sciences, both natural and artiﬁcial, providing a common
framework that brings together many scientiﬁc endeavors, from neuroscience
to physics to machine learning. In this paradigm, the central concepts are
information and brains—or, more generally, cognitive systems. As we will
show, putting the brain in center stage will lead us to the idea that brains
seek to model the incoming ﬂux of information—and to simplicity as a driving
principle.

In order to illustrate the general applicability of thinking about simplic-
ity, we can start with its application in the philosophy of science. Loosely
speaking, science endeavors in observing phenomena by carefully planned
experimental work, extracting the rules or regularities in observed data, and
using them to make predictions which can help us falsify and thus improve
them. The goal of science is to develop simplifying models with compressive
and predictive power [1]. A mystery that has baﬄed scientists for centuries is
why simple models, which by deﬁnition are good compressors, are also good
at predicting the future.

A perhaps more intriguing statement is that science is essentially what
brains do: cognition is ﬁrst and foremost model building. When a scientist—
which we could call a professional thinker—or a cognitive system develops a
model of what is observed, a tool for data compression is produced. A good
model can be used to literally compress the collected information: we can just
compute the diﬀerence between data and model outputs (e.g., using Huﬀman
coding). A good model will give produce an easy-to-compress diﬀerence ﬁle
with mostly 0’s.

But is this all a model is, a compression engine? In a general sense,
a model is a mathematical construct that is designed to correspond to a
prototype, which may be physical , biological, social, psychological or even
some other mathematical model [2]. As such, the purpose of a model is to
1) predict some aspects of the future, 2) inﬂuence further experimentation

2

or observation, 3) foster conceptual progress and understanding, 4) assist
the axiomatization of the prototyped situation, and 5) foster mathematics
and the art of making mathematical modes [3, p. 78]. The ﬁrst aspect, in
particular, is somewhat mysterious. Why can we predict the future at all?

What does this have to do with the elusive concept we call ‘reality’ ? If
‘reality’ belongs to the realm of cognitive systems, then it must be ﬁrmly an-
chored on the concept of information, which is all cognitive systems, natural
or artiﬁcial, have access to. In other words, reality is an information-based
construct of our brains—a model—and one we could certainly call an illu-
sion, too. And since, as we will argue, models are to represent information
in simple terms, the notion of reality arises, ultimately, from the search for
simplicity. In some sense ‘simplicity’ is equivalent to ‘reality’.

Information has now taken the central role in many human activities, and
most clearly in science. Formally, it is a well deﬁned concept describing to
the number of yes/no questions required to specify a physical (or abstract)
state and it owes its existence to the mathematical concept of the bit, the
essence of dichotomy and an invention of Leibniz. As our brains deal with
information from the external universe, a data stream of huge bandwidth is
compressed into manageable concepts such as ‘mass’ or ‘matter’, ‘person’ or
‘justice’. How does this happen? In existential terms, the initial conclusion a
blank brain would reach from is this: there is information. In philosophical
terms, materialism is replaced by ‘informationalism’ or ‘modelism’, bringing
us closer to Platonic idealism, or reality as mathematics.

While the points discussed so far may be intuitive, we need a rigorous
deﬁnition of simplicity, and we need to explain why simplicity is important to
brains. Fortunately, a precise deﬁnition of simplicity, or rather its antonym,
complexity, exists: it is provided by algorithmic information theory, further
described below.

These ideas follow a rather long thread which originate perhaps with
Plato and were then more clearly articulated by Leibniz in his Discourse
on Metaphysics (1686)—as pointed out by Chaitin [1]. The importance of
simple explanations is also commonly associated with Occam, who in the
14th century stated “Pluralitas non est ponenda sine neccesitate”, which we
may translate loosely as “models should be no more complex than suﬃcient
to explain the data”. This is also known as ‘Occam’s razor’. In more recent
times, the idea that explanations should be as simple as possible—but no
more—was also exposed and practiced by Einstein, to whom is also attributed

3

the statement that reality is just a persistent illusion1. More recently, the
relevance of the idea of simplicity in the cognitive sciences and its relation to
Kolmogorov Complexity has been pointed out [4, 5]. The present paper aims
to continue in this direction by shifting further the focus from the search
for simplicity in the physical universe to the subjective realm of the human
brain and model building in cognitive systems.

In the following sections, the brain-in-the-universe model is analyzed, and
the meaning of computation in this context brieﬂy discussed. This is followed
by a review of the various deﬁnitions for simplicity and a discussion of why
simplicity is an important concept in the context of neuroscience and Pres-
ence. In the last section of this paper I provide an overview of the relation of
these ideas to several other ﬁelds—including physics, biology, machine learn-
ing, robotics and education—and suggest that a common research framework
may be applicable.

2. Cognition from information

Consider then a universe and a brain in it, and the following two premises:
1) all human experience is generated in the brain, 2) the brain has only ac-
cess to information. If we think of this brain as a modeling tool exchanging
information with the rest of the universe it is part of (the ‘environment’),
then it follows that all questions about ‘reality’ should be framed in an in-
formation theoretic framework with neuroscience at the center—an assertion
that aﬀects both neuroscience and physics. By ‘environment’ here I mean
that which the brain exchanges information with, in the form of sensory in-
puts and the outputs associated to what we call ‘actions’.
In truth, this
brain can only state that there is information and information ﬂow—see Fig-
ure 1. Information is the currency the brain uses to give states an identity
and infer things about them. In our information-theoretic approach we could
deﬁne a brain as a a semi-isolated physical/computational system capable of
controlling some of its couplings/information interfaces with the rest of the
universe. This control of bi-directional information ﬂow seems to be a key
requirement for a cognitive system.

1Indeed, the discovery of the theory of relativity—which profoundly separated our daily
experience of space and time from their physical description—-and quantum mechanics
have eroded signiﬁcantly the seemingly rock solid concept we had of ‘reality’, bringing us
closer to Plato’s cave.

4

Figure 1: The brain creates the model of reality through information exchange (in and
out) with the ‘outside”. In this case we show the full universe sub-divided into a brain
interacting with the rest of the universe by bi-directional information exchange, which
physically occurs through sensors and eﬀectors.

If all brains have access to is information, we can also think of brains as
‘information processing machines’—computers. Brains work hard to process
the incoming information ﬂow gathered by our senses and make decisions
(actions). Brains send commands to eﬀectors and harvest information from
sensors. Moreover, since the information gathered from our senses depends
on how we choose to extract it from the outside world (through the passive
and active aspects of sensing and modality choices), our model of reality must
include a model of ourselves—of our ‘bodies’ and internal ‘algorithms’. We
must have a description of our bodies and of our acquired prior knowledge.
These self-models are what we commonly call body representation and self-
awareness.

I will posit here that as a modeling tool the brain is organized in a hier-
archical way. Sensors extract features and those are fused in diﬀerent stages.
The resulting higher-level features are again compressed, and so on. More-
over, high level outputs may aﬀect low level compressors as well. Sensing is
active, and it is known that high-level processes can modulate perception.
Hierarchical modeling and processing has been discussed before, see e.g., [6],
and here it will appear naturally in the application contexts described in

5

InformationThe universe001010101010101010101010100101010111110101010101010100010101010100101010101010101010011001110101001010010101101010101010101010101010010101010101010101010101010101010101010101010101010100101010101010101001010101010101010101010101010010101011111010101010101010001010101010010101010101010101001100111010100101001010110101010101010101010101001010101010101010101010101010101010101010101010 section 6.

From an information-theoretic perspective, the result of a brain’s inter-
action with its environment is to change both states, and this is mediated
Information theory provides the concep-
by an exchange of information.
tual framework for understanding the relation of brains with the universe.
Some of the brain-environment information exchange is mediated by ‘con-
trolled information membranes’ (sensors and eﬀectors), but some it is not
and can be dangerous. In an eventual physical-informational theory yet to
be developed—physicist John Wheeler’s It from bit [7]—one would say that
the information conveyed by a bullet as it traverses a brain will change its
state suﬃciently to destroy its function as a modeling tool.

Given the above two premises, we can ask how brains infer what is
‘out there’. To survive–to maintain homeostasis—brains build models (al-
gorithms) to function eﬀectively and to predict the future (i.e., future infor-
mation streams). Thus, the ﬁrst important observation is that ‘reality’ is a
construct of our brains—a model. Reality is the algorithm brains build to
sustain existence (and dodge information bullets). Examples of the models
built by brains and that are successful at compressing information include:
space, time, charge, the dimension of space, mass, energy, atoms, quarks,
tigers, people.... At this stage we can only point out that having access to a
good model of reality, in terms of its compressive, operative and predictive
power, may be important for survival. I discuss in more depth the relevance
of simplicity in the next sections.

An important aspect in coupling modeling with function in cognitive sys-
tems is the concept of ‘pain’ (or its antonym ‘pleasure’), which provides,
perhaps again in a hierarchical manner, the objective function to optimize
in reality model building. Pain can be related to physiological concepts such
as homeostasis. Modeling would appear to be necessary for homeostasis and
therefore life.

To provide this information-centric model a modern incarnation and il-
lustrate the connection of modeling with function, consider the mapping of
both environment and a into a computer program. In principle, a computer
(a quantum one, to be precise) could simulate reality in as much detail as
needed. In a practical implementation, we could consider the human as a pro-
gram module with some interface with the other program (the surrounding
universe). The program ‘human’ could for example instantiate a command
to move its eyes. Some underlying routines would control eye gaze auto-
matically as a function of head orientation through a vestibular submodule.

6

Such routines would implicitly contain a model of the humans’ body and
environment physics (e.g., the law of gravity).

Let us clarify here that when we have only limited access to all the vari-
ables of a problem, the correct natural formalism is to work with so-called
probabilities. Probabilities subsume and extend the concept of classical de-
terministic models. E.g., our classical notion of model is assigned to the
expectation value associated to the probability function. This is actually
mandatory in quantum mechanics, where models can only provide probabil-
ity amplitudes. I emphasize here the subjective stance in this paper. With
Jaynes [8], we refer to probabilities as a mental mathematical construct that
represents in an unbiased way our knowledge of reality: a model.

3. What is computation?

Since modeling and compression require a notion of computation, let us
brieﬂy analyze this apparently well-deﬁned notion. A simple mechanistic
model of a universe computing its future is a closed system of billiard balls.
Billiard ball computers have been proposed for the theoretical analysis of un-
conventional conservative computing [9]. A subset of balls can be arbitrarily
identiﬁed as the Turing state register and the rest as the tape, with newtonian
mechanics as the transition function. The model may seem overly simplistic,
but it contains the most fundamental element in the theory of computation:
dynamics. In an information theory framework, physical states are mapped
into information and dynamics to computation.

Similarly, the ‘real’ universe evolves and in some sense computes its future
[10]—and bodies and brains appear to be part of it. However, this model
already presents a puzzle. What deﬁnes the boundary between ‘me’ and the
‘rest of the universe’ ? From a fundamental physical viewpoint, it is not sim-
ple to deﬁne it. That is, while boundaries may be deﬁned in arbitrary ways,
there does not appear to be a natural one. As an interesting option, mutual
algorithmic information has been proposed as a means to deﬁne the natural
physical boundaries of an organism [11]. A complete theory should be able
account for this divide or do away with it. Is there a guiding principle to sep-
arate ‘inference machines’ from the rest of the universe? Is the complement
of an inference machine also an inference machine?

A possible formalization of the problem may be the description of the
brain-universe relationship by two coupled Turing machines. The concept
of a coupled (‘beyond-Turing’) Turing machine has been described before

7

Figure 2: The interaction of brain with the environment can be described by the symmetric
coupling of two Turing machines.

[12]. A coupled Turing machine is one which can accept external inputs
while operating, and which may also possess output channels. What we are
proposing here is a speciﬁc symmetric construct involving two such machines:
one in which the output of one is the input of the other—see Figure 2.
Alternatively, the theory of inference machines may be used [13] (further
discussed below) and extended to consider the coupled case.

It is important here to emphasize that computation is a dynamical phe-
nomenon that requires recursion and a concept of time. Yet time may not
be a fundamental physical concept. It may itself be thought of as a derived
concept—another model (the ‘great simpliﬁer’ in the words of physicist Ju-
lian Barbour [14]). Note that our billiard ball ‘universe-computer’ can be
described in a timeless way by the constants of motion (or, more directly, by
the Hamiltonian of the system and the initial conditions). Can we construct
a new information theory that does not rely on the concept of recursivity
and hence time? It should be clear that in some sense computation does not
add anything to the information of the initial state: everything is already
in the program plus data which is available at time zero. Both time and
computation are probably themselves models.

8

100101011010101011011100101010004. Simplicity

As has been set forth, what we call reality is an interpreted abstraction—
a mental construct—based on the observed regularities in multiple sensory
input streams and in our own interactions (output streams). Hence, the no-
tion of reality is a model, with rules, regularities or invariants as its building
blocks. We can easily show that physical concepts such as mass and space-
time, or mathematical notions such as set and even number are models we
use to simplify sensory information streams, typically in the form of invari-
ants. Compression is closely related to model or representation building,
identiﬁcation of invariants, memory and prediction. The notion of repetition
or recursivity is in this context the fundamental building block—algorithms
require recursion.

In physical terms, simplicity, compression and modeling are all related
to the identiﬁcation of invariances in data streams. This is easily seen in
the case of time-invariances: once dynamical laws are found, the evolution of
the system can be described through its constants of the motion, and these
reﬂect time-invariants. In fact, solving the equations of motion is equivalent
to ﬁnding all the constants of motion. For example, in physical systems with
time-translation invariance the quantity called energy is a constant of the
motion, a conserved quantity. If invariants are found, e.g., empirically, the
representation of the dynamics can be compressed.

A general relation of conserved quantities to symmetries in the physi-
cal world was formalized by Emmy N¨oether [15], who showed that to each
symmetry there corresponds a conservation law (see e.g., [16, 17]). In our
brain-centric context, symmetries refer to transformations of a multivariate
input/output stream at a given moment of time (e.g., how a rotation of pro-
pioceptive space aﬀects vision and sound), or to more general transformation
involving both space and time (Lorentz transformations). In essence, the ex-
istence of invariant quantities (conserved quantities) and symmetries that can
be constructed from observed data points to non-randomness, and therefore
to the possibility of compression and simplicity. In terms of a Lagrangian
description of dynamics, if there is a continuous symmetry then there is a
missing reference to a coordinate which is therefore conserved.

In the following section I describe diﬀerent approaches to quantify sim-
plicity and their relation. Using standard modern language, complexity will
be deﬁned.

9

4.1. Simplicity from algorithmic information theory

Compression and therefore simplicity were ﬁrst successfully formalized
by the notion of algorithmic complexity or Kolmogorov complexity (KC2),
a mathematical concept co-discovered during the second half of the 20th
century by Solomonoﬀ, Kolmogorov and Chaitin and which provides a well-
established albeit formal cornerstone to address the question of compression
in brains—both natural or artiﬁcial. We recall its deﬁnition: the Kolmogorov
complexity of a data set is the length of the shortest program capable of gen-
erating it. More precisely, let U be a universal computer (a Turing machine),
and let p be a program (see e.g., [18, 19]). Then the Kolmogorov or algorith-
mic complexity of a string x with respect to U is deﬁned by

KU (x) = min

p: U (p)=x

l(p),

(1)

the minimum length over all programs that print the string x and then halt.
Although a technical point, the restriction to programs that halt is important,
for no program is then the concatenation of other programs. An important
fact is that this is a meaningful deﬁnition: although the precise length of the
minimizing program depends on the programming language used, it does so
only up to a constant. That is, if U is a universal computer, then for any
computer A we can easily show that KU (x) < KA(x) + c. The constant c is
the length of the program for U to emulate A.

Unfortunately, G¨odel’s incompleteness theorem, or its equivalent, Tur-
ing’s halting theorem, implies we cannot compute in general the KC of an
arbitrary string: it is impossible to test all possible algorithms smaller than
the size of the string to compress, since we have no assurance of that will ever
halt [20]. However, if we change the rules of the game and state that there is
a ﬁnite computation time, compressibility becomes a practical question. A
theory of ‘practical’ complexity can tell us what the expected length of the
shortest programs can be if computational resources are ﬁnite (in time and
space), or if there are constraints in natural compressing mechanisms.

A further connection to simplicity using the concept of KC was devel-
oped by Solomonoﬀ [19] with an emphasis on statistics and prediction. The
fundamental quantity here is the algorithmic or universal (un-normalized)

2Kolmogorov complexity is also known as ‘algorithmic information’, ‘algorithmic en-
tropy’, ‘Kolmogorov-Chaitin complexity’, ‘descriptional complexity’, ‘shortest program
length’ and ‘algorithmic randomness’.

10

probability PU (x) of a string x. This is the probability that a given string x
could be generated by a random program. An important result is that this
is given by

PU (x) =

(cid:88)

2−l(p).

(2)

p: U (p)=x

Here the earlier remark that concatenations of programs cannot be new pro-
grams is crucial. This result says that short programs contribute more to
the probability of observing a given data string. A further important result
connecting this probability to KC is that

PU (x) ≈ 2−KU (x),

(3)

that is, the probability of a given string to be produced by a random program
is dominated by its Kolmogorov complexity. What this result says is that
the probability of observing a string is actually dominated by the shortest
program capable of generating it. More precisely, suppose that we are given a
string x (a measurement). Then we can ask what is the relative probability of
observing a future string y, i.e., the total string xy (concatenation implied),
or the alternative xz. This will be given in this framework by

(cid:88)

2−l(p)

PU (xy)
PU (xz)

=

p: U (p)=xy
(cid:88)

2−l(p)

p: U (p)=xz

≈ 2−[KU (xy)−KU (xz)].

(4)

An important aspect in this approach is that predictions are made considering
all the appropriate explanations with their proper weights, and not only the
most likely one.

The precept that short explanations are in some sense more likely is
also the essence of the Minimum Description Length (MDL) and the Mini-
mum Message Length (MML) approaches to statistical inference [19, 21, 22].
Among possible explanations (programs) for data (an observed data string),
those which are shortest are more likely.

Two points need to be clariﬁed to highlight the connection of MDL with
KC. The ﬁrst is that in general we can think of the shortest program as
the sum of two sub-programs: one that encodes the regularities in the data,
and the other which gives the error or remainder. This is rather intuitive.
If models do not ﬁt the data well, they will have to keep an explicit, un-
compressed tally of the error as part of the program. Thus, the concept of

11

misﬁtting appears here in a natural way, providing the connection to Bayesian
estimation—further discussed below. The error term in this framework pro-
vides a solid deﬁnition for the concept of ‘noise’, which is simply that part
of the data we have no power to model with the information we have ac-
cess to (if we have prior information it should be packed with the new one).
For example, we could think of the concept of ‘sphere’ as arising from MDL
analysis of all haptic experiences, as a sort of average shape—the simplest
archetype for experienced objects.

Thus,

in ideal MDL shortness is measured in algorithmic complexity
terms, and this approach is essentially equivalent to KC minimization. Model
plus error separation happens in a very natural manner in the context of com-
pression. That is, if a set of data is to be compressed and we have no other
information, KC or ideal MDL provide the recipe of shortest program length,
which gives, as a byproduct, a recipe for separating something to be called
“abstract model” from “noise”. Similarly, in Bayesian terms code length of
the model and code length of model plus data together correspond to prior
probability and marginal likelihood respectively in the Bayesian framework
[23].

The second is that in practice we know that KC is in general uncom-
putable. For this reason, in MML a particular compression scheme based on
Shannon information theory using eﬃcient codes is used. This method states
that given some data and an eﬃcient “code” to represent models and data,
the best model is that which minimizes the sum of the length (in bits) of
the model plus the length of the data once encoded by the model (which we
can think of as the error or noise we cannot model in Bayesian terms). This
can be shown to be equivalent to selecting the highest Bayesian posterior
probability, which we discuss in the next section. The use of eﬃcient coding,
based on entropy, as opposed to algorithmic coding, based on complexity,
implies that this approach is practical, since in practice MML can be com-
puted. Moreover, it can also be shown that in a statistical average sense,
entropy is a good proxy for complexity [18].

It can also be stated that the only diﬀerence between practical MML and
its universal version in ideal MDL is that the second one uses a universal
Turing machine, whereas the former does not in the interest of practical use.

4.2. Statistical inference: Bayes and Occam’s razor

Jaynes’s formalization of probability theory [8] is closely related to the
philosophy of this paper, since it too takes a subjective stance. Jaynes dis-

12

cusses how a cognitive system (a human brain or a robot) would seek to
formalize logic and its extension, probability theory, in order to optimize
the search for models. In particular, Jaynes deﬁnes probability theory as a
natural extension of logic, targeting the problem of optimal inference from
models: reasoning about propositions and their plausibility, which are tools
for model building. We recall here the conceptual kernel of Bayes theory is
the relation for the ‘probability’

p(a, b) = p(a|b) p(b) = p(b|a) p(a)

(5)

where a and b may describe for example data and models.

The relation of Occam’s razor to Bayesian theory is discussed for example
in [8, 23, 24]. Given two models, b1 and b2, their relative probability given
some data a is

p(b1|a)
p(b2|a)

=

p(b1) p(a|b1)
p(b2) p(a|b2)

.

(6)

Without access to any prior information, we may say that the two models
have equal priors, and conclude

p(b1|a)
p(b2|a)

∼

p(a|b1)
p(a|b2)

.

(7)

In Bayesian theory the quantity p(a|b), with a referring to data and b to
a hypothesis or model is called the evidence.
It is a measure of how well
the data is ﬁt by the model. Without prior information about the model
probability distribution, the probability for a model is thus proportional to
the evidence. The evidence gives the probability to observe data if the model
is true. It is proportional to the goodness of ﬁt but inversely proportional to
the model available search space—the model volume. For example, a theory
with two parameters will provide a higher evidence than a three parameter
model if goodness of ﬁt is similar. In essence, but rather loosely, goodness
of ﬁt being equal and with a uniform prior, simpler models will yield greater
probability for the data, because they live in a smaller space.

The use of uniform priors was already proposed by Laplace. Laplace
proposed a related concept called the ‘principle of indiﬀerence’. This prin-
ciple states that given a set of possible scenarios about which we have no
information, the only logical thing to do which will represent correctly our
knowledge is to assign to each equal probabilities. More precisely, suppose

13

that there are n > 1 mutually exclusive and collectively exhaustive possi-
bilities. The principle states that if the n possibilities are indistinguishable
except for their names, then each possibility should be assigned a probability
equal to 1/n. Assigning equal probabilities to a set of scenarios amounts to
providing the simplest model of the phenomena. In this case the model is
a probability function, and a standard measure of probability of is entropy.
This principle was formalized by Jaynes as the so-called Maximum Entropy
Principle [8, 25].

We end this section with a word of caution. Despite statements to the
contrary, the above arguments do not prove the validity of Occam’s razor
or simplicity as a principle for inductive inference. As pointed out in [26]
and [27], the no free lunch theorems for optimization [28] imply that unless
some assumptions are made about the space of possible algorithms and the
space of data, Occam’s razor does not follow. For example, the connection of
Bayesianism and Minimal Description Length can be justiﬁed if the universal
prior of Solomonoﬀ applies, but not in general [22]. This is a very important
point, and implies that we need to search for explanations for the importance
of simplicity in the realm of biology and neuroscience, and not in mathematics
alone. We provide some ideas in section 5.

4.3. Inference machines

For completeness we brieﬂy mention a new approach to complexity. An
absolute measure of complexity using the concept of (strong) inference ma-
chines has recently been proposed [13].
Inference machines theory (IMT)
is a mathematical formalization of physical devices performing observation,
prediction or recollection tasks—what we call here cognitive systems—and
they generalize Turing machines. An important aspect in the theory is that
an inference machine is itself part of the universe it is trying to infer. This
recent formalization has already delivered very interesting results, including
the fact that for any inference device there exist functions it cannot infer,
and that there can only be one strong inference device.

A natural deﬁnition of complexity is provided in this formalism in terms
of the size (in some sense) of the input subspace of universes allowed by
the ‘program’ for the inference device to produce the required output data
In eﬀect,
stream: the bigger this subspace, the simpler the program is.
the setup of the inference device constrains the types of universes in which
the computation takes place. A simple program is less constraining. What is
especially interesting about this theory is that there is a notion of an absolute

14

complexity (deﬁned by the unique strong inference device), as opposed to the
standard computational theory, where complexity is to some extent relative.
Further work is needed to understand the implications of IMT to the subject
of this paper.

5. Why is simplicity important?

As we have seen, simplicity is clearly useful in compression, but its appli-
cability to inference is not obvious from ﬁrst principles. If the universe and
our world are really simple, as posited by Leibniz and others, then simple
models should be sought. As discussed above, simple models can be said to
be statistically more likely in some sense: if ‘god’ is a monkey typing random
programs on a computer (i.e., if Solmonoﬀ’s universal prior applies), then
simple programs are more likely explanations or causes of observed data and
simplicity will not only be practical, but also better at prediction. Unfor-
tunately, it has not been possible to justify this assumption yet. However,
simplicity oﬀers advantages even in the absence of a simple universe. We can
consider diﬀerent aspects of this question.

Firstly, I note that brains, as inference machines, need to survive in their
environment. We should therefore analyze evolution and natural selection as
the conceptual basis for the discussion.

Cognitive systems carry out three main diﬀerent tasks, which we can
classify as recalling, observing and acting, and predicting or extrapolating—
corresponding to past, present and future. In all three of these tasks—also
discussed by [13] in the context of IMT—we can ﬁnd value in simplicity.

Past: memory is deﬁnitely an important dimension in the discussion.
Simplicity is valuable as it allows to store information more economically,
using less memory resources—although at the expense of computation. If a
simple model is found to compress the data, it will become easier to store
and eventually recall. Good models can account for large data streams, and
therefore provide a deeper view into the past. Moreover, as explained by
Jaynes, simple models represent knowledge in a fair way. Suppose axioms
A, B and C ‘explain’ (decompress to) the facts. We could also add another
axiom, call it D, if it does not conﬂict with the facts. But adding it is a
disservice to our representation of knowledge, because it is adding unfounded
aspects to the model, adding information that is unwarranted and potentially
biased—an invention.

15

Present: simplicity is advantageous in deducing, planning, acting or de-
ciding, including in observation, experimentation and decision—i.e., dealing
with the present. Simple models lead to simple deductions, with less clutter
from bogus tenets leading to ‘model noise’ (such as those that may derive
from axiom D above). Simple models are easier to run, requiring less memory
resources and memory access. Experiments provide the information we want
with less clutter from spurious variables to be controlled if they are easy to
execute. I emphasize that the act of experimenting is crucial for brains, as
it is through experimentation that the world is explored. When our hands
reach out to touch an object, we are carrying a multi-modal experiment to
build, based on earlier models, an improved model or representation of the
object. The existence of an underlying simple models allows for the design
of simple experiments. Likewise, a simple model should be easier to use for
decision making. Given the No Free Lunch theorem, using simpler models
appears to oﬀer only advantages, even if the universe is not simple. That is,
if there are no generally superior algorithms, using the simplest available is
a practical strategy.

We may also add here two obvious but important observations. First,
in the act of observation brains access only partial information from the
environment. This gives rise to the notion of noise, for example. And it
also entails the notion of ‘coarse graining’, which may be a key feature of
living beings. Coarse graining means that special combinations of data are
observed (e.g., macroscopic averages). For example, we can easily measure
the center of mass of a planet (or at least, much easier than the position of
each atom). It also happens that we can model and predict special coarse
grained quantities, while it would be impossible to predict every detail of the
environment. As an example of this we refer the reader to [29], where it is
shown that it is possible to coarse grain irreducible (incompressible) cellular
automata in order to make them compressible. The Kolmogorov complexity
of an incompressible stream is basically the length of the data stream. Incom-
pressibility here means that in order to model what will happen in the future,
there is no other approach than brute force computation of every single step,
there are no regularities to exploit or shortcuts to take. Secondly, the act
of observation perturbs the universe, because information is injected into it.
Observation or sensing are active and disturb that which we observe. Both
of these statements are true in classical physics but take on an extreme form
in quantum physics. These obvious points lie at the core of the foundations
of physics, a point we come back to below.

16

Future: from the prism of evolution and biology, prediction or extrap-
olation are certainly an important element for survival, and simplicity is
advantageous for prediction tasks as well. As we have discussed, simple
models are less biased [8, 25], representing in truthful way what we know
from the available data and using the least resources. In this paper I take
the view that, given the No Free Lunch theorem, simple models are the most
practical—in principle as good as any other for prediction purposes. Simple
programs are easier to use for prediction, requiring less in terms of memory
resources to store. Perhaps this is the true evolutionary origin of simplicity,
as the original simple organisms counted on few resources and could only con-
struct simple models. Simplicity through recursivity allows the replacement
of memory space for time. This logic may also apply to DNA. Evolution
would seem to lead to a layers of simplifying processors. Finally, short pro-
grams are also easier to debug and correct, which is certainly important for
biological processors such as DNA.

The relevance of simplicity for extrapolation is also at the core of the
question of why mathematics and simple theories turn out to be the right
ones in science, and especially in physics—a mystery that has perplexed
many thinkers, including Einstein and Feynman. A possible answer is that
inference machines will by nature—including limited resources—always seek
and ﬁnd some simplicity, even in random data. Let us recall here that any
desired sequence can be found in a truly random number. An inference
machine exposed to a lucky (compressible) portion of the stream will deduce
all sorts of things and be fooled by the passing mirage. This is the complexity,
or rather simplicity, version of the Anthropic principle (see e.g., [30] for a
discussion of Anthropic theories). The “inferotropic principle” would state
that without simplicity there would not be infererence machines. An related
explanation may be provided by coarse-graining. Perhaps the ﬁrst task of
inference machines is coarse-graining for compressibility: choosing correctly
how to observe and interact with the world may be the key to simplicity.
This would give an added dimension to the statement that brains construct
reality.

6. Simplicity and cognitive science

6.1. Fundamental neuroscience

Diﬀerent experimental scenarios could explore the hypothesis that the
brain is a compression machine. One especially interesting approach is to

17

study how compression and model building are handled at diﬀerent hierarchy
levels.

As is well known, there are mechanisms in the brain that are well adapted
to modeling patterns and detecting change. Some of the related experimental
work involves studying the response of the brain to diﬀerent input sequences.
It is possible today to provide stimuli with accurate timing to the brain
(visual, auditory, haptic) and then measure the response as seen using scalp
electrophysiology. For a brief review of compression in visual processing, see
[4]. In audition, it is typically necessary to average the response over several
hundreds or thousands of such events in order to ﬁlter out the noise and
observe what is called an Event Related Response (ERP).

In Mismatch Negativity experiments (MMN), auditive patterns are pre-
sented to the subject and the response of the brain is measured using an
EEG or MEG recording system [31, 32]. The MMN component is thought
to result from the comparison process between the stimulus and the neural
representation of the auditory past maintained in sensory memory [33]. This
could be a simple recording of the recent past, or an abstracted model (‘ab-
stract invariance’ in the language of [34]). MMN is a low level automatic
response generated by infrequence pattern breaks, not requiring attention
and present even during sleep.
It has been used extensively to study the
perceptual organization of sound as well as memory. It has been shown that
MMN mechanisms can detect regularities and use them for prediction. The
appearance of a MMN wave indicates that the system has made a wrong
prediction, or, equivalently, that the current model needs updating. There-
fore, MMN can be used to study how the central auditory system encodes
auditive time series near the low end of the processing chain.

MMN experiments in eﬀect measure the change in response when the in-
ferred patterns are broken. The phenomenon of MMN basically illustrates the
fact that the brain, at some very primitive level, is able to extract patterns—
to compress information. Although algorithmic complexity theory has not
been applied to this problem, it seems a very interesting prospect. We can
for example measure how many iterations are needed before the pattern is
assimilated as a function of algorithmic complexity, and we can try to un-
derstand the type of pattern that the brain’s mechanisms involved are best
at locking into.

In more detail, when a deviant tone is presented, the ERP diﬀers from
the standard response. This illustrates the fact that the brain is acting like a
real time modeling engine. What makes this especially interesting is the fact

18

that the level of response to the deviant may be related to the complexity of
the sequence. In the experiments described in [33], the following repeating
(non-random) sequence was tested:

S1: ABABAB - ABABAB - BABABA - BABABA - ABABAB - ABABAB ...

where A and B stand for two tones and the dashes for silence periods. Each
group of letters is called a ‘train’. If the silence periods were suﬃciently short,
a MMN event occurred at each tone repetition (marked in bold). On the other
hand, for long silence periods (inter-train interval > 240 ms), there was no
such MMN response. This shows that the relevant brain’s auditory subsystem
treats each train independently when they are suﬃciently separated in time,
and as a continuous stream otherwise. Let us analyze this in more detail.
We can code this sequence as

S1 = R0(R3(AB) + Rn(−) + R3(BA) + Rn(−))

using a simple coding language where Rn stands for ‘repeat n times’, with
n = 0 referring to inﬁnite repetition. In the case of short silence periods, the
stream can be rather eﬃciently approximated using a very simple ﬁrst order
rule, namely

Sn+1 = N OT (Sn)

which we can loosely call ‘alternate’ symbols. Then, the continuos sequence

S1: ABABABABABABBABABABABABAABABABABABAB ...

results in a MMN coding of

MMN[S1]: 00000000000010000000000001000000000000 ...

This coding scheme results in some errors (represented here by 1s) occurring
at regular intervals, which are however in principle easy to code by a subse-
quent compressing layer. So MMN acts as a low-level compressing or coding
system. A hierarchy of such coding systems could detect and encode regu-
larities at diﬀerent time scales. If there is a hierarchical complexity scheme
in the data, an appropriate pattern detector can be constructed from simpler
ones by concatenation of simple pattern detectors—but other possibilities
probably exist. The existence of such structure in the natural data would
explain the fact that simplicity is preferred, and why low level simplicity

19

couples to more automatic detection systems. This process may also be the
essence of music.

What happens if the inter-train intervals (ITIs) become suﬃciently long?
Clearly, the simple alternating rule cannot account for long-scale regularities
(recall that the sequence in this particular experiment was fully predictable).
In [33] it was shown that no MMN response is elicited with long ITIs, indi-
cating that either a) the coding scheme captured all the complexity (perfect
prediction), or b) the MMN mechanism was partially or totally inactive (the
rule too complex to learn in its entirety). In order to test the ﬁrst hypothesis
some tests using pattern breaking at the larger scale (e.g., repeating 3 times
one of the trains randomly) could be carried out (but unfortunately they
were not in [33]). However, the results in [33] using another sequence (see
below), suggest that such larger scale rules are not encoded. Experiments
in with ITIs and non-random series included inserting repetitions at other
places, e.g.,

S2: ABABAB - - AABABA - - BABABA - - BBABAB ...

again in predictable manner. What was observed is that the inferred pattern
by the auditory system was again that of alternation. With long ITIs, this
rule generated no response in case S1, but did in case S2 and similar others.
Moreover, the results also hinted at the existence of higher level encodings or
‘logical consequences’ of the simple alternating rule. For example, MMN was
also observed to occur when the consequence Sn+2 = Sn is violated (experi-
mentally, in S2 after onset of the third tone). A rather logical interpretation
here is an intended abstraction in the coding scheme, some sort of relativity
principle. The abstract rule of alternation could be seen to be ‘coordinate
independent’, e.g., indiﬀerent to how it is started, ABABAB ∼ BABABA.
This would be a stronger form of simplicity encoding. In fact, this would be
the best compressing method if the system is handling trains as independent
events, or, e.g., if the alternation of trains ABABAB and BABABA were
truly random.

Although the study in [33] was not explicitly designed for complexity anal-
ysis, some could easily be devised using random violation of rules. Consider
the patterns

Z1= ABABAB - - ABABAB - - ABABAB ...
Z2= AABABA - - AABABA - - AABABA ...
Z3= BAABAB - - BAABAB - - BAABAB ...

20

These sequences have been ordered according to complexity. E.g., using
Unix’s compress (which uses LZW data compression) compresses most the
ﬁrst, then the second, then the third sequences. The ﬁrst sequence is eﬃ-
ciently described by the AB subunit and the silent space (common to all).
To describe the second one, we need A twice and then BA twice plus the
space. The last sequence requires also two subsequences, BA and AB, the
ﬁrst one repeated once, the second twice, and Z1= R0(R3(AB) + Rn(−)), Z2
= R0(R2(A) + R2(BA) + Rn(−)) and Z3= R0(R1(BA) + R2(AB) + Rn(−)).
We can see how the length of the programs grows.

Targeted MMN experiments could thus shed light on the role and strate-
gies for compression and simplicity in memory and prediction. We would
expect that complex patterns might elicit a weaker or delayed MMN re-
sponse (in agreement with [33] and other experiments, e.g., [35]) and take
longer to learn (for example, [36] discusses how rapidly simple but abstract
rules are learned). Or we might ﬁnd that MMN related mechanisms can
code only very simple rules, leaving algorithmic integration to higher level
processes which are more diﬃcult to observe using EEG. Finally, the MMN
paradigm, which is used aurally and apparently engaging low level pattern
detection systems, could be expanded to other modalities (somatosensory,
visual, etc.) using emerging virtual reality (VR) technologies, as well as in
the complexity scale to explore model building in diﬀerent human cognitive
sub-systems.

6.2. Presence

The idea of model simplicity can be used in the context of the ﬁeld called
Presence. In line with the theme of this paper, Presence can be said to study
how the human brain constructs the model of reality (including body and
self) through full (immersive) or partial (mixed) digital interaction. As such
it is part of a wider family of disciplines studying how cognitive systems
build models of their environment and interact with it, i.e., cognitive sci-
ences. In Presence, the technological goal is to place a brain in a controlled,
convincing and interactive information bath, so that subjects feel and act
as if the designed experience were real [37]. Presence provides the natural
scientiﬁc approach to study the issues discussed in this paper at diﬀerent
perceptual and cognitive levels. The designed information bath can be fully
digital (immersive), or mixed, containing both real and digital elements.

Let us consider now the following thought experiment. A person is placed
in an enviroment in which computers prepare and control all sensorial inputs

21

(via sophisticated audio and video 3D displays, haptics, olfaction, vestibular
stimulation, etc.) and in which the person’s brain commands are intercepted
at the level of, say, peripheral nerves or even using brain-computer interfaces.
If this experiment is carried out successfully, the person will feel fully im-
mersed in the virtual environment and, in measurable terms [37, 38, 39, 40],
act as what he or she is experiencing were real. In this experiment, it suﬃces
to describe the universe in terms of bits, because the environment is re-
ally created by program in a computer managing sensorial inputs and brain
outputs—see Figure 1. We can identify three main elements in this setup:
1) a human or animal agent (a brain), 2) bi-directional human-machine in-
terfaces, 3) a machine agent. All these are necessary to create the subject-
environment loop. This ideal experiment represents the central paradigm in
the ﬁeld. The reader familiar with the movie Matrix will easily recognize the
concept of immersion, capture and replacement of eﬀerent and aﬀerent data
streams, which in the movie are implemented in an invasive manner.

Reality, according to this paper and illustrated by the above thought
experiment, is a model arising from the coupling of brains and bits. As
has been argued, among all the possible models that can account for our
observations, all other things being equal the simplest ones are ranked more
probable by the brain. Following the logic laid out in this essay, I contend
that in order to achieve ‘more’ Presence, simplicity in the explanation of
what is happening both at low and high cognitive levels is a key aspect.
Here simplicity will include a consideration of departure from models built
from prior experience, which could be inherently complexity-increasing. We
state this as the simplicity hypothesis and as a virtual reality design principle:
given alternate models (interpretations) for a given mediated experience, a
brain will select the simplest one it can construct that agrees with the data
and with prior models. Furthermore, the simpler the model, the more real
the experience will feel.

Three important aspects need to be emphasized here. The ﬁrst is that
models must take into account all the available information, including past
data encoded as a priori model distributions. This is agreement with the al-
gorithmic complexity or MDL description of modeling. Agreeing with prior
models just means that in the search for simple models we will be treating
all available information in a democratic way. The second is that in or-
der to tie these concepts with modern Presence ideas, we need to postulate
the existence of a hierarchy of models, or what is the same, a hierarchical
compression scheme going from low level perceptual mechanisms (sensorial

22

spatio-temporal binding [39]) to higher cognitive modeling (an extension of
binding concepts to higher processing stages). In addition, diﬀerent mod-
eling hierarchies will have access to diﬀerent amounts of working memory.
Third, the principle of simplicity needs to be applied with care, since we do
not mean to say that the brain attains by any means the KC limit, but only
a suﬃcient approximation of practical value.

Finally, if the brain’s goal is to construct models for survival, and if
pain—as related to homeostasis—provides the objective function to optimize
through model building, then a rather direct road to Presence is through
the stimulation of pain receptors—nociception.
I come back to this point
below, but note here that human skin is rich in pain receptors associated to
mechanical pressure or deformation, chemistry and temperature.

Simplicity and sensorimotor consistency with models

The relevance of modeling and simplicity in Presence is already implicit in
work describing the importance of so-called ‘sensorimotor contingencies’ and
the binding problem. This refers to the process by which the brain eﬀortlessly
uses spatially and temporally segregated activity in neuronal ensembles to
form a uniﬁed perception [38, 39] and to the importance of ‘coherence’ of
motor outputs and multimodal sensorial streams. It should be clear at this
point that sensorimotor contingencies refer to the consistency of inputs and
outputs with models, and hence, to the idea of simplicity. They could be
better called, in the spirit of this paper, sensorimotor consistencies.

As an example, let us consider the so-called ‘rubber arm’ experiment
In the experiment, the real forearm is hidden, and a fake one
[41, 42].
is displayed and stroked. While the fake arm is stroked, the real one is
simultaneously stimulated but hidden from view. Given the sensorial visual
and haptic evidence, the experimental subject could hypothesize:

• Model 1: That is my hand being stroked – This is a simple plausible
theory (which we may call ‘body illusion’) that may account well for
what is presently happening to the subject, but which disregards the
recent past as coded in high-level memory (“I am in an experiment”).
Higher-level cognition will not accept this model, but low-level models
in the brain have no access to long-term memory and complex pro-
cessing, so they are easily fooled by the experimental setup. The ex-
perimental evidence available to the subject can be accounted for by
this simple model and the subject will feel at some level that the fake

23

hand is real, something which can be inferred through physiological
measurements of galvanic skin response, for example. Of course, the
better rendition of a fake arm we can provide, the less noise the subject
will need to deal with to accept the illusion. Moreover, if the subject
were to suﬀer from some severe form of amnesia aﬀecting short-term
memory, this explanation might become more plausible at all levels and
even become ‘reality’ after a suﬃciently long time.

• Model 2: There is a complex set-up in place to fool me into believing
that this is my hand – This is a more complex model, but in fact it is the
simplest explanation available to high-level processing that is consistent
with all the modeling hierarchies and takes into account all the available
data to the subject (e.g., from birth). However, this model, despite
being the simplest and most truthful taking into consideration all the
data, is not available to all processing subsystems—where it is not the
simplest. Low-level modeling will still be swayed by the illusion.

As can be seen, a cross-hierarchy modeling conﬂict is taking place.
If a
hammer is suddenly driven into the fake arm, the subject will often pull
away. This is not surprising, since such reﬂexes are controlled at low level.
Thus, we cannot say that the ﬁrst or the second models of reality are in
control. Both are active in some sense.

Another similar example is provided by the so-called ‘Pinocchio’ illusion
(see, e.g., [43]). In a variant, a blindfolded subject is made to stroke a third
party nose while his/hers is simultaneously stroked by the experimenter.
The coherence of inputs (haptic inputs through nose, hand and propiocep-
tion) supports the “I have a long nose” proprioceptive low level body model.
Again, a higher level explanation involving the cortex and an experimen-
tal conspiracy (“there is a set-up to fool me”) is more complex and simply
unavailable to low level modeling systems.

To summarize, we conjecture here that the feeling of Presence, as mea-
sured by subjective or objective ways, is increased if the induced sensorial
input (the input data stream), has a low complexity, i.e., it can be modelled
in a simple manner by the subject’s brain compressing subsystems. Physi-
cal consistency in the inputs, in the sense of there being a simplifying low
level model available to match the data, is an important element to enhance
Presence, as it connects with low level, small memory capacity modeling
mechanisms. As we progress higher in the modeling hierarchy, Bayesian

24

prior expectations (memory play an important aspect: explanations with a
better match with past experiences are in a sense inherently simpler.

There can be conﬂicts across modeling hierarchies, and in some sense
there is no unique model of reality, but several competing ones. We can-
not for the moment state which levels in the modeling hierarchy are favored
when in disagreement, but it would seem to follow that low-level modeling
mechanisms will associate to more primitive responses—and viceversa. We
can also conjecture that low-level (perceptual) modes and high-level (cogni-
tive) models will reinforce the illusion if they are both in agreement. Finally,
the stimulation of nociceptors should stimulate (motivate) model building
signiﬁcantly, and may thus enhance Presence.

For simplicity, in what follows we will refer to only two modeling hierar-
chies, ‘low’ and ‘high’, but in reality we can expect that there are many such
processing layers.

PI and Psi

The conceptual framework presented here is compatible with the recent
proposal for concepts such as ‘Place Illusion’ (PI or Π) and ‘Plausiblity’
(Psi or Ψ) to establish a theory of Presence [37]. Both of these ﬁnd their
roots in the theme of model building, and in particular on the consistency
of interaction data streams with low level (PI) and high level (Psi) simple
models.

PI refers to a qualia: the strong illusion of being in a place in spite of the
sure knowledge that you are not there. It is the space-analog of the body illu-
sions (rubber-arm, long-nose) that we have discussed above. Virtual reality
(VR) systems—e.g., using immersive head-mounted displays—are typically
quite successful at generating this illusion. From the point of view of VR de-
sign, we could also add the requirement that the perceived place correspond
to the intended one by the VR designer. To achieve PI, the input/output
streams must respect the structure of low level models that code things like
“if I initiate motor commands to rotate my head in such a way, the visual
ﬁeld shall rotate in such a manner”. Such low level sensorimotor models are
probably rather rigid. For example, based on a model of reality (body, space
and physics), the vestibular system exerts direct inﬂuence on eye muscles
in order to sustain stability of images in the retina (and hence simplify the
resulting data stream). Such models encode body models and “routine ex-
istential physics”, including Newtonian physics, geometric optics, etc., but,
e.g., not general relativity or quantum mechanics, which are beyond normal

25

experience.

PI can be generated by providing this natural consistency between stimuli
and actions, i.e., input and output streams. In order to produce it, immersion
in an appropriate virtual reality environment is necessary. As explained in
[37], PI relates to how the world is perceived (perception). The associated
modeling layers are probably hard-coded genetically to a great extent (as
opposed to learned). Similarly, we could extend this concept deal with how
the body is perceived (BI or ‘body illusion) or more generally, to the physical
world (PHI or ‘physical illusion)

On the other hand, Plausibility (Psi) refers to consistency of the data
stream with prior learned models. That is, it is associated to higher-level
modeling tasks of the brain. As deﬁned in [37], Psi is the illusion that what
is apparently happening is really happening (even though you know for sure
that it is not). Thus, it represents a step upward in the modeling hierarchy
(but not to the top). From the point of experience design, we could also
add that what is intended to appear to be happening is the actual perceived
illusion. That is, what is experienced is actually the model of reality M which
the VR designer is trying to convey. We discuss this point further below.

The ultimate level in this context may be called ‘Reality Illusion’ (RI)
and would occur if the input stream is convincing at all modeling levels,
even the highest ones. Today, the only way to achieve this would be to place
somebody in a real but very controlled environment. This is the realm of con
artists, scams, and related human aﬀairs. It may also happen during dreams,
perhaps because some of the model checking mechanisms are turned oﬀ. I
discuss below an ‘almost real’ scenario.

It should be apparent that VR technology will become a very powerful
tool to study human modeling hierarchies.
I provide now a more math-
ematical description of these ideas using Bayesian theory and algorithmic
complexity. For a related application of Bayesian theory to neural coding
and computation see, e.g., [44].

First let us state what a ‘reality model’ or hypothesis is here. A model M
is a program running in a cognitive system (or in a cognitive system)—a ﬁnite
binary string—that can compress and predict the information associated to
a set of events. To be precise, let us deﬁne the set of events E as the
input/output information stored in the short term working memory of the
subject—another ﬁnite binary string. Event streams contain information
about input sensorial streams, output eﬀector streams, and other working
memory data related to processing. After a time span ∆t, an event data

26

stream E is stored in the subject’s memory. Diﬀerent modeling layers will
have diﬀerent memory and natural timescales.

A Bayesian formulation for Presence is now provided. Consider the prob-

ability of a model M co-existing with a set of events E:

P (E, M ) = P (E|M ) · P (M ).

(8)

The reality model M is, e.g., a model for the self, body, the environment and
agency in the environment (the ‘universe’). What this equation says is that
the plausibility of a model M together with a designed set of events E (the
sensorial input plus eﬀector output stream, to be precise) is proportional
to the conditional probability of the events given a reality model M (the
evidence) times the probability of the designed reality model (the prior, which
derives from older data). Here we can formally deﬁne two concepts associated
to Bayesian terminology, the evidence and the prior: the consistency of new
data with the model or evidence ∼ P (Events|M ); the possibility in relation
with previous models or prior ∼ P (M ).

Let M be alternative model of reality intended to be conveyed by a de-
signed VR system. This may be an model targeting some or all levels of
processing. E.g., it may include physical aspects, agency, and target diﬀer-
ent modeling levels in the hierarchy. The goal of a good Presence system
is to convince the user of the ‘reality’ of the experience thorough the joint
generation of a set of events (inputs, outputs) E. I note the importance of
designed interaction—both inputs to and outputs from the cognitive system
conform the set of events and are important.

As we will see, there is no fundamental diﬀerence between PI and Psi
from the point of view of this mathematical formulation—they both measure
a probability. However, PI will be a function of the evidence for the desired
model low-level aspects as well as its prior probability—which will be rather
peaked and not very plastic.

I deﬁne next the formal version Place Illusion associated to an intended
model of reality using the Bayesian inference formalism.
I will use Greek
letters to denote PI (Π) and Psi (Ψ) for short and to remind us of the
quantitative and formal shift in their meaning.

Deﬁnition 1. The M -Place Illusion of a model M associated to a set of
events E (as generated by interaction of the subject with a virtual reality
system with underlying model M ), ΠM , is deﬁned to be
ΠM ≡ P (M l, E) = P (E|M l) P (M l),

(9)

27

where M l refers to the reality model low level (e.g., perceptual) aspects.

Since the subject is responsible for a subset of E, the model of reality must in-
clude him or her as well (e.g., include a body representation). For hardwired
low level models, the prior will dominate. In order to produce a convincing
Place or Body Illusion, the generated events must be reasonably consistent
with the intended reality, but this reality must not depart much from the
usual one. It is probably very diﬃcult to teach the nervous system to ignore
latency problems in vision and touch, or vision-vestibular inconsistencies, for
example. It may be possible to believe we have a long nose, but harder to
feel we have three eyes. The value of ΠM would correspond to the question
“Is the intended low level model real?”, but this question will probably not
be asked by the subject if he or she are not aware of what is happening or
of the intended model.

Similarly, the model-associated Plausibility ΨM is proportional to ΠM , to

the evidence for higher order models, and to their prior:

Deﬁnition 2. The M -Plausibility of a model M given a set of events E,
ΨM , is deﬁned to be

ΨM ≡ P (M, E) = P (M l, E) · P (M h, E)

= ΠM · P (E|M h) · P (M h)

(10)

where we have assumed independence of low and high order models.

Presumably, hardwired models are not very aﬀected much by data, but higher
level models are plastic and adaptive. In the prior P (M h), ‘past-experience’
modeling is accounted that will disfavor potential models that depart greatly
from those already adopted. It is for this reason that even in a high realism
scenario (digital or not) the subject will not readily accept fully a new reality.
For example, if we suddenly started seeing ﬂying cows, it would take us some
time to accept this as part of reality.

The value of ΨM corresponds to the question “Is the intended model
real?”, but again this question may not be asked by a subject unaware of
what is happening in the VR experience or of the intended model. Also note
that there is no intrinsic diﬀerence between past and present evidence. Past
data produces evidence for models, which then become priors for subsequent
estimation in a process called sequential estimation [24].

What happens if the M model is poorly designed by the VR engineer, or
there are, e.g., timing issues in the system’s displays? Then, the intended

28

illusion M will not take place. This could be due to a bad evidence and/or
prior (i.e., a low ΨM score), or to the existence of a higher plausibility score
with an alternative model, M (cid:48). That is, even if the score for M is not high,
the subject may still feel that the experience is real. Perhaps he or she will
favor a diﬀerent reality model than the intended one. In order to account for
this let us deﬁne now the generic Plausibility (i.e., not model-tied) of a set
of events E.

Deﬁnition 3. The Plausibility of a set of events E, Ψ = Ψ(E), is deﬁned
to be

(cid:88)

Ψ =

P (M, E) = P (E)

(11)

M
where the sum is over all models M the subject can construct.

This would correspond to the question “Is this really happening?”. Perhaps
no single model can account well for the data, but there are many potential
models out there. For the purpose of model building, it may be suﬃcient
to retain Mmax ≡ maxM P (M, E)— “What is the most plausible model I
can construct that ﬁts this experience”? However, instead of keeping the
most probable solution, our brains may work with the P (M, E) distribution
function, in a purer Bayesian spirit. We postulated this occurrence when
there is a disagreement between low and high level models (in the rubber-
arm experiment). Model independent PI could be deﬁned in a similar way.
We can write the ideal MDL analog of Equation 8 in terms of program

length as

l(M, E) = l(E|M ) + l(M )

and we would conjecture that

Ψ =

(cid:88)

M

2−l(M,E) ≈ 2−KM,E

(12)

(13)

where KM,E refers to the total algorithmic length describing the data, which
can be decomposed into a sub-program M and error. If this result were to
apply it would mean that the universal prior is actually used by the cognitive
system in question, but this is something to address experimentally. As was
argued above, this may well be the case because simplicity is a practical
strategy for cognitive systems, even if the universe is not simple.

We have stated that low level modeling systems have little memory allo-
cation, so they can only detect simple patterns. It could be for this reason

29

that PI is an independent phenomenon from Psi. That is, a high Ψ(E) neces-
sitates a high Π(E), but not vice-versa. This hierarchical scheme discussed
above is imposed most likely by evolution, since presumably the low level
pattern detection mechanisms arose ﬁrst, in simpler organisms. But this is
another experimental question: to what extent can Ψ impact Π? This pos-
sibility is not included in the simple formalism above, which could be easily
extended.

We illustrate some of the above ideas with another thought experiment.
Let ARE (‘Almost Real’) be a real—as opposed to virtual—environment with
underlying model MW . The environment is designed to fool the subject into
believing they have entered a time machine and travelled in time and space
to the Far West, and more speciﬁcally into a saloon scene. The subject enters
the room dressed like a cowboy and discovers a space laid out in full detail,
with real people in elaborate costumes. The environment provides a realistic
(non-mediated) input/output stream to the subject with a high-level model
of being in the Far West (in time and space). This will yield a high ΠMW
(very high place illusion) as there is no illusion in the physical sense—the
scenario is real and all the ‘sensorimotor contingencies’ will be met perfectly.
Therefore ΠMW = P (E|M l
W ) = 1. But the subject will not readily
believe that the intended situation is real: the Model-Plausibility of MW ,
ΨMW = ΠMW · P (E|M h
W ) will be greatly penalized by older data
(i.e., the prior) despite the high ΠMW value. It will improve as the days or
months go by, of course, if the experiment is somehow allowed to continue.
The subject, upon entering the room will correctly maintain that “I am in
a real room with real people in disguise, I am not really in the Far West
in the 1800’s”. The Plausibility of the events Ψ(E) ( “Is this happening?”)
will be high, because there is a well known model that ﬁts the data and
has a reasonable prior (“Somebody is trying to fool me”), albeit not the one
intended by the illusion designer.

W ) · P (M h

W )P (M l

We close this section with a note on diseases of Presence. Mental disor-
ders such as schizophrenia, which is marked by dysfunctional impairments in
the perception or expression of reality, could perhaps be studied as model-
ing pathologies, given the prevalence of cognitive deﬁcits associated to this
disease [45].

30

7. Other applications and future work

I provide next an overview of the relevance of simplicity paradigms to

other areas, all of which with connections to neuroscience research.

7.1. Robotics

The theory of reality construction ﬁnds practical applications in robotics.
Indeed, if we understand someday how natural brains model the world, we
can apply the knowledge to the constructions of robots. And vice-versa, as
by trying to build artiﬁcial cognitive systems we will probably develop new
tools to study the brain. We highlight some work with connections to the
ideas presented here.

The use of self-modeling is an active area of research in robotics.
In
[46] the cognitive agents were robots programmed to update models of their
own bodies (and, extrapolating, their environment) from past experience
and in this way improve locomotive performance. In particular, this robot’s
learning cycle included the design of specially directed experiments to select
among competing models. This was achieved through actuation-sensation
interaction with the environment. As discussed above, this parallels the
scientiﬁc method, where collected data from directed observations suggest
models which are then tested via new experiments.

In [47, 48] a mathematical model based on the search from simplicity is
provided in the context of robotics. The ideas proposed stem from work in
[49], where it is argued, much in line with the present work, that the basis
of experience arises from the construction of laws relating motor action and
sensorial reaction, or more technically, the laws of sensorimotor dependen-
cies. The idea is that such models (which we identify here directly with the
concept of ‘reality’) are constructed by exploration and measurement. The
point made is that both motor and sensorial data streams live in very high
dimensional spaces. Simplicity must be sought, if nothing else to make con-
trol manageable. The authors explain that the existence of invariances in
the relation

s = ϕ(B(C), u),

(14)

where u is the state of the environment, C is an output motor command which
conﬁgures the body position B, and s is the sensorial input stream—reveals
the structure of an underlying group of transformations. To avoid compli-
cations the motor command uniquely identiﬁes the body position without

31

reference to past motions. The authors use the language of diﬀerential ge-
ometry based on the fact that the searched-for models must be invariant with
respect to sensorimotor coordinate transformations. Diﬀerent types of invari-
ances can be explored. For example, invariance with respect to simultaneous
motor and environment changes can be used to deduce the dimensionality
of ‘external space’ (we use quotes to highlight again that space is itself a
model). We note that the notion of space is directly tied to the type of sen-
sors available. E.g., some ‘real’ dimensions of space may not be accessible at
all to the cognitive system in question. The dimensions of the invariant space
may be deduced by the dimensionality of the sensorial subspaces spanned by
motor or environment changes if the sensorial apparatus is blind to them—
i.e., if there are invariances. This work underscores the practical importance
of model-building from sensorial information, interaction and searching for
simplicity.

7.2. Fundamental physics

In physics, information is taking an ever more fundamental role, and
since the days in which the physicist John Wheeler posited “It from bit?”
it has become clear that information and the bit could become the modern
equivalents of the greek atom and the 20th century quantum. An interesting
and modern attempt at reducing physics to information theory is provided
in [50], where it is argued that the concept of Fisher information is the key
to uniﬁcation of classical and quantum physics, and a fundamental physical
concept. More recently, a principle called Information Causality which states
that the transmission of m classical bits can cause an information gain of at
most m bits is being studied as a candidate precursor of quantum theory [51],
thus giving information and information conservation a fundamental status
in physics. In [5] I discuss in more detail the importance of simplicity and of
a brain-centric, information-based approach to the foundations of physics—
but see also [52] for an interesting attempt to frame physics in the context
of computation.

7.3. Biology

In the modeling hierarchy, if brains are modeling systems in short time
scales, DNA models the environment over long time scales, and these models
are manifested (‘run’) in phenotypes. Organisms can be conceptualized in
information-theoretic terms, and evolution as a form of computation. Or-
ganisms interact with the environment, and they aim to survive. We can

32

Figure 3: An organism creates the model of reality through information exchange (in
and out) with the ‘outside”. In this case we show the full universe sub-divided into an
organism interacting with the rest of the universe by bi-directional information exchange,
which physically occurs through the organism’s information membrane. The organism’s
DNA encodes the model of reality, a model which evolves over long time scales.

state then that organisms encode models of their environment in their bod-
ies, and ultimately in their DNA. Therefore, DNA can be thought of as a
compressed form of the elements in the environment the organism will have
to cope with. In the evolutionary long time scales, DNA is the model that
encodes those aspects of reality (the environment) needed for survival, which
we may call homeostasis with some abuse of language. Figure 3 provides an
illustration of this conceptualization. Evolution and natural selection thus
enter the discourse naturally.
If we can deﬁne coherently the concept of
agents or inference machines at the organism level, it can be argued that
those that best compress the information bath will be better prepared to
survive. Compression and modeling are closely tied to prediction and hence
survival.

We can think of evolution as another step in the hierarchical ladder of
computation, but one with a long time scale. E.g., the structure of our bodies
reﬂect a model encoded in our genes that has evolved (as is being computed)
over eons. In this fashion we can extend the idea of cognition from the brain

33

InformationThe universe0010101010101010101010101001010101111101010101010101000101010101001010101010101010100110011101010010100101011010101010101010101010100101010101010101010101010101010101010101010101010101001010101010101010010101010101010101010101010100101010111110101010101010100010101010100101010101010101(acting at short time scales) to the organism level (acting on long time scales
through DNA and with evolution). The discussion above, focusing on the
brain interacting with information, can thus be naturally extended to the
organism level.

In relation to biology and the origins of the brain, we can state that life
and, in particular, nervous systems evolved to ﬁnd methods to simplify and
encapsulate the apparent complexity of the universe, the context being the
usual one: permanence of the ﬁttest (natural selection). Thus, biological
evolution is Nature’s algorithm for compression and modeling, a theme to be
explored in research and in practical applications.

In mode of summary, I would like to propose an information-based deﬁni-
tion of life spanning all time scales: a living being, or entity or agent, can be
deﬁned to be a replicating program that successfully encodes and runs a (par-
tial) model of reality, thus increasing its chances of survival as a replicating
program. Evolution is to be thought here of as a form of computation, the
means to obtain perduring entities. The results in [13] seem to indicate that
there is place for only one strong inference machine in the universe, which
could be thought of as the cusp of evolution. Modeling and computation
apply, in this deﬁnition, at short and long time scales, from DNA to brains.
Evolution is only needed if ‘perdurance’ is under stress. If an organism
ﬁnds the solution to perdure without biological evolution (or, at least, natural
bio-evolution), aging and mortality are no longer a must. The stream of
reproduction will stop, but the pattern will remain.

Experimentally, we can try to ﬁnd evidence for hierarchical modeling and
simplicity at the physiological level. The drive for modeling and simplicity
through evolution and natural selection, it has been argued, generates hi-
erarchical modeling systems, which at the organic level we may call control
systems. The recent observations of scale-free and multiscale phenomena in
physiological signals may be taken as evidence for these ideas, because behind
scale free or mutiscale phenomena there are hierarchical generating systems.
A natural parallel to the MMN discussion above would be to study simple
modeling (control) physiological systems. Simple organisms can provide a
very good starting point for such research. Recently, it has been shown that
bacteria are also capable of modeling and predicting environmental change
[53], by encoding them in their gene regulatory networks, thus showing that
environmental anticipation (modeling) is an adaptive trait even at this bio-
logical level.

34

7.4. Mathematics and Machine Learning

The relevance of simplicity to cognitive systems may explain the power
of mathematics. Mathematics can be deﬁned as the science of pattern and
deductive structure [3, p. 108]. Mathematics provides the tools for meta-
compression: mathematics is the discipline per excellence that draws conclu-
sions logically implied by a set of axioms. Where mathematics is successful
in compressing, a modeling landscape is reduced to a ﬁnite set of axioms
together with equations for ‘logico-dynamics’. Today we know that any such
system will leave gaps and that completeness and consistency are in gen-
eral incompatible (G¨odel, Turing [20])—how the axioms are chosen is very
important.

Further work is needed to understand the role of algorithmic complexity
(KC) in machine learning. Although it has been proven that it is in general
impossible to compute the KC of a string due to the halting problem (or,
what amounts to be the same, Godel incompleteness), there may still be
something to be proven regarding the performance less demanding tasks.
Can we say anything about KC if a limit is imposed on computation time?
A very exciting new trend is emerging in machine learning, the so-called
automatization of science [54, 55].
In this groundbreaking work, we ﬁnd
machine learning and simplicity principles coming together for the search for
models, but a theory for model building is not provided. Much work remains
to be done to optimize the search for models relying on simplicity Bayesian
criteria. A natural question in this endeavor is how to choose models that
ﬁt the data: how do we trade-oﬀ error and model complexity (the work
parsimony is normally employed in this context)? With compression as the
guiding theme the answer is immediately provided by KC or MDL, which ﬁx
this tradeoﬀ and inherently deﬁne noise—as discussed at length above.

7.5. Education

During development, the most important skill to acquire is arguably the
capacity to model or compress information. This is especially true in the
21st century, given the agile access we now have to plentiful data thanks
to information technologies. But how do we actually construct high-level
abstract models? Human beings derive all sorts of models of reality from
their interaction with their environment. When a child asks “why?” he or
she is asking for a derivation of a fact from previous knowledge, or perhaps
for a new axiom for their model building. This is a fundamental cognitive
activity, yet it is poorly understood. How does the process of modeling

35

actually take place? Given the fact that we have today a new generation of
powerful technologies to control (in the laboratory) the interaction of brains
with especially designed environments we can ask how can we use them
to explore the transfer of interaction experiences to more formal knowledge
representations.

Can we conceive means for cross-hierarchical, cross-modal transfer of
modeling skills, e.g., from music to mathematics? Music is already an in-
teresting case: an enjoyable musical piece is a fulﬁlling exercise in non-trivial
model building (which could explain why diﬀerent people like diﬀerent types
of music—diﬀerent Bayesian priors). What is the relation between low level
(e.g., reﬂex) models to higher knowledge? For example, [56] discusses the
relationship of the propioceptive system with our physical modelling capa-
bilities, and hypothesizes that the neural correlates of physics and math-
ematics did not evolve de novo, but are rooted in our ‘subconscious’ body
sense—proprioception. Bodily manipulations such as juggling, suggest a well
synchronized physical interaction as if the juggler were a physics expert. The
juggler uses ‘embodied knowledge’ to interact with the environment. Is this
transfer transferred to higher cognitive levels somehow?

8. Conclusions

In this paper, following earlier work described in [5], a neuro-centric, sub-
jective approach to cognition based on information theory has been proposed,
with the underlying idea that information is the most fundamental physical
and cognitive concept. The discussion is intended to apply to any cognitive
system, simple or complex, natural or artiﬁcial (e.g., robots).

I have argued that evolution and natural selection lead to compressing or
modeling systems, including auto-modeling. The reason is that the construc-
tion of models from data is advantageous for survival. Modeling is equivalent
to compression or the search for simplicity. In this sense, reality, the con-
struction of models from information, is equivalent to simplicity.

I have then reviewed the concept of simplicity and provided mathematical
descriptions for this concept. The search for simplicity, modeling, the search
for symmetries, conserved quantities and compression have all been shown to
be closely related. Simplicity can be described by Kolmogorov or algorithmic
complexity, but also from other angles, such as or the principle of indiﬀerence,
Bayes’ theory or minimum description length. A subjective interpretation of

36

probability theory is necessary for it to describe how we represent knowledge
and make decisions.

Although simplicity cannot been proven to be the best strategy for ex-
trapolation and prediction without further assumptions about the universe,
it appears to be a practical strategy for compression and unbiased repre-
sentation of knowledge, allowing for economic storage and manipulation of
information from interaction with the environment. Simplicity is advanta-
geous in deducing, planning, observing, learning, deciding and debugging.
It is also advantageous in predicting, if even in a purely practical, resource
conscious, way. The NFL theorem implies that, unless we make assumptions
about data, we cannot a priori say which algorithm or classiﬁer will perform
best. Simplicity is a practical recipe.

From this algorithmic framework, a hierarchy of compression systems at
diﬀerent levels has been postulated, from those in genes to those in brains,
from low level pattern recognition to complex endeavors including self-models
and science. Diﬀerent approaches can be used to study these sub-systems
and their inter-relation.

I have demonstrated the use this paradigm with applications in neuro-
science and in current theories of Presence, and shown that the use of model-
ing and simplicity as a unifying theme can provide the framework for planning
further experimental and theoretical work in these areas. In particular, for-
mal deﬁnitions for Plausibility and Place Illusion have been provided using
statistical inference and algorithmic information theory concepts. I have also
proposed that the MMN paradigm, which is used aurally for the study of low
level pattern detection in brains, could be expanded to other modalities (so-
matosensory, visual, etc.) using VR, as well as in the complexity scale to
explore model building.

Finally, the idea of simplicity as a common unifying thread has been
emphasized, which suggests that a joint inter-disciplinary study approach in-
volving branches of science such as mathematics, machine learning, informa-
tion theory, neuroscience, physics, computer science, robotics and Presence
could be mutually advantageous.

Acknowledgements

This work has greatly beneﬁtted from discussions with many people. Spe-
cial thanks to Carles Grau, Ed Rietman, Walter Van de Velde, Mel Slater,

37

David Wolpert, David L. Dowe, Gregory Chaitin and Miriam Reiner, Ju-
lian Barbour for their ideas, inspiration and useful discussions. This work
partly supported by the PEACH Coordination Action (33909) of the Future
and Emerging Technologies (FET) programme within the Sixth Framework
Programme for Research of the European Commission.

References

[1] G. J. Chaitin, Leibniz, Information, Math and Physics,

in: Wissen
und Glauben / Knowledge and Belief, Akten des 26. Internationalen
Wittgenstein-Symposiums 2003, 2004, pp. 277–286.

[2] R. Aris, Mathematical modelling techniques, Pitman, 1978.

[3] P. J. Davis, R. Hersch, The mathematical experience, Mariner Books,

1981.

[4] N. Chater, P. Vitanyi, Simplicity: a unifying principle in cognitive sci-

ence?, Trends in Cognitive Sciences 7 (1) (2003) 19–22.

[5] G. Ruﬃni, Information, complexity, brains and reality (“Kolmogorov

Manifesto”), http://arxiv.org/pdf/0704.1147v1.

[6] K. Friston, Learning and inference in the brain, Neural Networks 16

(2003) 1325–1352.

[7] C. W. Misner, K. S. Thorne, W. H. Zurek, John Wheeler, relativity and

quantum information, Physics Today April.

[8] E. Jaynes, Probability theory - the logic of science, Cambridge, 2003.

[9] E. Fredkin, T. Toﬀoli, Conservative logic, Int. J. Theor. Phys. 21 (1982)

219–253.

[10] S. Lloyd, The computational capacity of the universe, Phys.Rev.Lett.

88.

[11] G. J. Chaitin, Perspectives in biological complexity, IUBS Press, 1991,

Ch. Algorihmic information & evolution, pp. 51–60.

[12] B. J. Copeland, R. Sylvan, Beyond the universal Turing machine, Jour-

nal of Philosophy.

38

[13] D. Wolpert, The physical limits of inference, Physica D: Nonlinear Phe-

nomena 237 (9) (2008) 1257–1281.

[14] J. Barbour, The end of time, Oxford University Press, 1999.

[15] E. Noether, Invariante variationsprobleme, Nachr. d. K¨onig. Gesellsch.

d. Wiss. zuG¨ottingen, Math-phys. Klasse (1918) 235–257.

[16] H. Goldstein, Classical Mechanics, second edition Edition, Reading, MA:

Addison-Wesley Publishing, 1980.

[17] W. L. C. Falter, Symmetries in Physics, Springer Verlag, 1996.

[18] T. M. Cover, J. A. Thomas, Elements of information theory, John Wiley

& sons, 1991.

[19] M. Li, P. Vitanyi, An Introduction to Kolmogorov Complexity and Its

Applications, Spriger Verlag, 2008.

[20] G. J. Chaitin, Nature’s imagination, Oxford U. Press, 1995, Ch. Ran-
domness in arithmetic and the decline and fall of reductionism in pure
mathematics, pp. 27–44.

[21] C. S. Wallace, D. L. Dowe, Minimum message length and Kolmogorov

Complexity, The Computer Journal 42 (4).

[22] P. M. B. Vit´anyi, M. Li, Minimum description length induction,
Bayesianism, and Kolmogorov Complexity, IEEE Transactions on In-
formation Theory 46 (2) (2000) 446–464.

[23] D. J. C. Mackay, Information theory, inference and learning algorithms,

Cambridge U. Press, 2003.

[24] M. E. Tipping, Advanced lectures on machine learning, Springer, 2004,
Ch. Bayesian inference: an introduction to principles and practice in
machine learning, pp. 41–62.

[25] E. Jaynes, Information theory and statistical mechanics, Physical Re-

view 106 (4) (1957) 620–630.

[26] D. Wolpert, On the Bayesian “Occam factors” argument for Occam’s

razor, MIT Press, Cambridge, MA., 1995.

39

[27] P. Domingos, The role of Occam’s razor in knowledge discovery, Data

Mining and Knowledge Discovery 3 (1999) 409–425.

[28] D. H. Wolpert, W. G. Macready, No free lunch theorems for optimiza-
tion, IEEE Transactions on Evolutionary Computation 1 (1) (1997) 67–
82.

[29] N. Israeli, N. Goldenfeld, Computational irreducibility and the pre-
dictability of complex physical systems, Phys. Rev. Lett. 7 (92).

[30] L. Susskind, The Cosmic Landscape, Little, Brown and Company, 2005.

[31] R. N¨a¨at¨anen, et al., Early selective-attention on evoked potential rein-

terpreted, Acta Psychol Amst 42 (313–329).

[32] C. Grau, C. Escera, E. Yago, M. Polo, Mismatch negativity and audi-
tory sensory memory evaluation: a new faster paradigm, NeuroReport
9 (1998) 2451–2456.

[33] M. Atienza, J. L. Cantero, C. Grau, C. Gomez, E. Dominguez-Marin,
C. Escera, Eﬀects of temporal encoding on auditory object formation: a
mismatch negativity study, Cognitive Brain Research 16 (2003) 359–371.

[34] T. W. Picton, et al., Mismatch Negativity: Diﬀerent water in the same

river, Audiology and Neuro-Otology 5 (2000) 111–139.

[35] S. Kanoh, R. Futami, N. Hoshimiya, Sequential grouping of tone se-
quence as reﬂected by the mismatch negativity, Biol. Cybern. 91 (2004)
388–395.

[36] A. Benidixen, E. Schr¨oger, Memory trace formation for abstract au-
ditory features and its consequences in diﬀerent attentional contexts,
Biological Psychology 78 (2008) 231–241.

[37] M. Slater, Place illusion and plausibility can lead to realistic behaviour
in immersive virtual environments, To appear in Phil. Trans. of the
Royal Society (B).

[38] M. V. Sanchez-Vives, M. Slater, From presence to consciousness through

virtual reality, Nature Reviews Neuroscience 6.

40

[39] M. A. Harvey, M. V. Sanchez-Vives, The binding problem in presence

research, Presence 14 (5) (2005) 616–621.

[40] M. Slater, et al., Understanding and realizing Presence in the Presenccia

project, IEEE Computer Graphics and Applications.

[41] M. Botvinick, J. Cohen, Rubber hands ‘feel’ touch that eyes see, Nature

391 (1998) 756.

[42] W. IJsselsteijn, et al., Is this my hand before me? the rubber hand
illusion in reality, virtual reality and mixed reality, in: M. Slater (Ed.),
Proceedings of the 8th international workshop on Presence, Presence
2005, 2005.

[43] J. R. Lackner, Some proprioceptive inﬂuences on the perceptual repre-
sentation of body shape and orientation, Brain 111 (1988) 281–297.

[44] D. C. Knill, A. Pouget, The Bayesian brain: the role of uncertainty in
neural coding and computation, Trends in Neuroscience 27 (12) (2004)
712–719.

[45] R. Lewis, Should cognitive deﬁcit be a diagnostic criterion for

schizophrenia?, J Psychiatry Neurosci 29 (2) (2004) 102–113.

[46] J. Bongard, V. Zykov, H. Lipson, Resilient machines through continuous

self- modeling, Science 314 (2006) 1118.

[47] D. Philipona, J. O’Regan, J. P. Nadal, Is there something out there?: In-
ferring space from sensorimotor dependencies, Neural Comput. 15 (2003)
2029–2049.

[48] D. Philipona, J. O’Regan, J. Nadal, O. Coenen, Perception of the struc-
ture of the physical world using unknown multimodal sensors and ef-
fector, in: Advances in Neural Information Processing Systems, MIT
Press, 2004, pp. 945–952.

[49] J. K. O’Regan, A. No¨e, A sensorimotor account of vision and visual
consciousness, Behavioral and Brain Sciences 24 (2001) 939–1031.

[50] B. R. Frieden, Physics from Fisher information, Cambridge U. Press,

2004.

41

[51] M. Pawlowski, et al., A new physical principle: Information Causality,

arXiv: 0905.2292v1.

[52] B. Whitworth, The physical world as virtual reality, Tech. rep., Centre
for Discrete Mathematics and Theoretical Computer Science (2007).

[53] A. Mitchell, et al, Adaptive prediction of environmental changes by mi-

croorganisms, Nature.

[54] M. Schmidt, et al., Distilling free-form natural laws from experimental

data, Science 324 (2009) 81–84.

[55] R. D. King, et al., The automation of science, Science 324 (2009) 85–89.

[56] V. Smetacek, F. Mechsner, Making sense, Nature 432 (2004) 21.

42

