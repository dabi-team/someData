5
1
0
2

l
u
J

1
3

]

G
L
.
s
c
[

1
v
8
8
7
8
0
.
7
0
5
1
:
v
i
X
r
a

Fast Stochastic Algorithms for SVD and PCA:
Convergence Properties and Convexity

Ohad Shamir
Weizmann Institute of Science
ohad.shamir@weizmann.ac.il

Abstract

We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast compu-
tation of leading singular vectors. We prove several new results, including a formal analysis of a block
version of the algorithm, and convergence from random initialization. We also make a few observa-
tions of independent interest, such as how pre-initializing with just a single exact power iteration can
signiï¬cantly improve the runtime of stochastic methods, and what are the convexity and non-convexity
properties of the underlying optimization problem.

1 Introduction

We consider the problem of recovering the top k left singular vectors of a d Ã— n matrix X = (x1, . . . , xn),
where k (cid:28) d. This is equivalent to recovering the top k eigenvectors of XX (cid:62), or equivalently, solving the
optimization problem

min
W âˆˆRdÃ—k:W (cid:62)W =I

âˆ’W (cid:62)

(cid:33)

(cid:32)

1
n

n
(cid:88)

i=1

xix(cid:62)
i

W.

(1)

This is one of the most fundamental matrix computation problems, and has numerous uses (such as low-rank
matrix approximation and principal component analysis).

For large-scale matrices X, where exact eigendecomposition is infeasible, standard deterministic ap-
proaches are based on power iterations or variants thereof (e.g.
the Lanczos method) [8]. Alternatively,
one can exploit the structure of Eq. (1) and apply stochastic iterative algorithms, where in each iteration
we update a current d Ã— k matrix W based on one or more randomly-drawn columns xi of X. Such algo-
rithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.g.
[2, 4, 3, 10, 6]. Another stochastic approach is based on random projections, e.g. [9, 20].

Unfortunately, each of these algorithms suffer from a different disadvantage: The deterministic algo-
rithms are accurate (runtime logarithmic in the required accuracy (cid:15), under an eigengap condition), but re-
quire a full pass over the matrix for each iteration, and in the worst-case many such passes would be required
(polynomial in the eigengap). On the other hand, each iteration of the stochastic algorithms is cheap, and
their number is independent of the size of the matrix, but on the ï¬‚ip side, their noisy stochastic nature means
they are not suitable for obtaining a high-accuracy solution (the runtime scales polynomially with (cid:15)).

Recently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq. (1), which has a â€œbest-of-
both-worldsâ€ property: The algorithm is based on cheap stochastic iterations, yet the algorithmâ€™s runtime is
logarithmic in the required accuracy (cid:15). More precisely, for the case k = 1, xi of bounded norm, and when

1

 
 
 
 
 
 
there is an eigengap of Î» between the ï¬rst and second leading eigenvalues of the covariance matrix 1
the required runtime was shown to be on the order of

n XX (cid:62),

(cid:18)

d

n +

(cid:19)

1
Î»2

log

(cid:19)

.

(cid:18) 1
(cid:15)

(2)

The algorithm is therefore suitable for obtaining high accuracy solutions (the dependence on (cid:15) is logarith-
mic), but essentially at the cost of only O(log(1/(cid:15))) passes over the data. The algorithm is based on a recent
variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems
([13]), although the optimization problem in Eq. (1) is inherently non-convex. See Section 3 for a more
detailed description of this algorithm, and [19] for more discussions as well as empirical results.

The results and analysis in [19] left several issues open. For example, it is not clear if the quadratic
dependence on 1/Î» in Eq. (2) is necessary, since it is worse than the linear (or better) dependence that
can be obtained with the deterministic algorithms mentioned earlier, as well as analogous results that can
be obtained with similar techniques for convex optimization problems (where Î» is the strong convexity
parameter). Also, the analysis was only shown for the case k = 1, whereas often in practice, we may want
to recover k > 1 singular vectors simultaneously. Although [19] proposed a variant of the algorithm for that
case, and studied it empirically, no analysis was provided. Finally, the convergence guarantee assumed that
the algorithm is initialized from a point closer to the optimum than what is attained with standard random
initialization. Although one can use some other, existing stochastic algorithm to do this â€œwarm-startâ€, no
end-to-end analysis of the algorithm, starting from random initialization, was provided.

In this paper, we study these and related questions, and make the following contributions:

â€¢ We propose a variant of VR-PCA to handle the k > 1 case, and formally analyze its convergence
(Section 3). The extension to k > 1 is non-trivial, and requires tracking the evolution of the subspace
spanned by the current solution at each iteration.

â€¢ In Section 4, we study the convergence of VR-PCA starting from a random initialization. And show
that with a slightly smarter initialization â€“ essentially, random initialization followed by a single power
iteration â€“ the convergence results can be substantially improved.
In fact, a similar initialization
scheme should assist in the convergence of other stochastic algorithms for this problem, as long as a
single power iteration can be performed.

â€¢ In Section 5, we study whether functions similar to Eq. (1) have hidden convexity properties, which
would allow applying existing convex optimization tools as-is, and improve the required runtime. For
the k = 1 case, we show that this is in fact true: Close enough to the optimum, and on a suitably-
designed convex set, such a function is indeed Î»-strongly convex. Unfortunately, the distance from
the optimum has to be O(Î»), and this precludes a better runtime in most practical regimes. However,
it still indicates that a better runtime and dependence on Î» should be possible.

2 Some Preliminaries and Notation

We consider a d Ã— n matrix X composed of n columns (x1, . . . , xn), and let

A =

1
n

XX (cid:62) =

1
n

n
(cid:88)

i=1

xix(cid:62)
i .

Thus, Eq. (1) is equivalent to ï¬nding the k leading eigenvectors of A.

2

We generally use bold-face letters to denote vectors, and capital letters to denote matrices. We let Tr(Â·)
denote the trace of a matrix, (cid:107) Â· (cid:107)F to denote the Frobenius norm, and (cid:107) Â· (cid:107)sp to denote the spectral norm.
A symmetric d Ã— d matrix B is positive semideï¬nite, if inf zâˆˆRd z(cid:62)Bz â‰¥ 0. A is positive deï¬nite if the
inequality is strict. Following standard notation, we write B (cid:23) 0 to denote that A is positive semideï¬nite,
and B (cid:23) C if B âˆ’ C (cid:23) 0. B (cid:31) 0 means that B is positive deï¬nite.

A twice-differentiable function F on a subset of Rd is convex, if its Hessian is alway positive semidef-
inite. If it is always positive deï¬nite, and (cid:31) Î»I for some Î» > 0, we say that the function is Î»-strongly
convex. If the Hessian is always â‰º sI for some s â‰¥ 0, then the function is s-smooth.

3 The VR-PCA Algorithm and a Block Version

We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its general-
ization for k > 1.

Algorithm 1 VR-PCA: Vector version (k = 1)
1: Parameters: Step size Î·, epoch length m
2: Input: Data matrix X = (x1, . . . , xn); Initial unit vector Ëœw0
3: for s = 1, 2, . . . do
Ëœu = 1
i=1 xi
4:
n
5: w0 = Ëœwsâˆ’1
6:

for t = 1, 2, . . . , m do

i Ëœwsâˆ’1

(cid:0)x(cid:62)

(cid:80)n

(cid:1)

Pick it âˆˆ {1, . . . , n} uniformly at random
w(cid:48)
Ëœwsâˆ’1
wt = 1
(cid:107)w(cid:48)

t = wtâˆ’1 + Î· (cid:0)xit
t(cid:107) w(cid:48)

wtâˆ’1 âˆ’ x(cid:62)
it

(cid:0)x(cid:62)
it

t

(cid:1) + Ëœu(cid:1)

7:

8:

9:

10:

end for
Ëœws = wm

11:
12: end for

The basic idea of the algorithm is to perform stochastic updates using randomly-sampled columns xi of
the matrix, but interlace them with occasional exact power iterations, and use that to gradually reduce the
variance of the stochastic updates. Speciï¬cally, the algorithm is split into epochs s = 1, 2, . . ., where in
each epoch we do a single exact power iteration with respect to the matrix A (by computing Ëœu), and then
perform m stochastic updates, which can be re-written as

w(cid:48)

t = (I + Î·A)wtâˆ’1 + Î·

(cid:16)

xitx(cid:62)

it âˆ’ A

(cid:17)

(wtâˆ’1 âˆ’ Ëœwsâˆ’1) , wt =

1
(cid:107)w(cid:48)
t(cid:107)

wt,

The ï¬rst term is essentially a power iteration (with a ï¬nite step size Î·), whereas the second term is zero-
mean, and with variance dominated by (cid:107)wtâˆ’1 âˆ’ Ëœwsâˆ’1(cid:107)2. As the algorithm progresses, wtâˆ’1 and Ëœwsâˆ’1
both converge toward the same optimal point, hence (cid:107)wtâˆ’1 âˆ’ Ëœwsâˆ’1(cid:107)2 shrinks, eventually leading to an
exponential convergence rate.

To handle the k > 1 case (where more than one eigenvector should be recovered), one simple technique
is deï¬‚ation, where we recover the leading eigenvectors v1, v2, . . . , vk one-by-one, each time using the
k = 1 algorithm. However, a disadvantage of this approach is that it requires a positive eigengap between
all top k eigenvalues, otherwise the algorithm is not guaranteed to converge. Thus, an algorithm which
simultaneously recovers all k leading eigenvectors is preferable.

3

We will study a block version of Algorithm 1, presented as Algorithm 2. It is mostly a straightfor-
ward generalization (similar to how power iterations are generalized to orthogonal iterations), where the
d-dimensional vectors wtâˆ’1, Ëœwsâˆ’1, u are replaced by d Ã— k matrices Wtâˆ’1, ËœWsâˆ’1, ËœU , and normalization is
replaced by orthogonalization1. Indeed, Algorithm 1 is equivalent to Algorithm 2 when k = 1. The main
twist in Algorithm 2 is that instead of using ËœWsâˆ’1, ËœU as-is, we perform a unitary transformation (via the
k Ã— k orthogonal matrix Btâˆ’1) which maximally aligns them with Wtâˆ’1. Note that Btâˆ’1 is a k Ã— k matrix,
and since k is assumed to be small, this does not introduce signiï¬cant computational overhead.

Algorithm 2 VR-PCA: Block version

Parameters: Rank k, Step size Î·, epoch length m
Input: Data matrix X = (x1, . . . , xn); Initial d Ã— k matrix ËœW0 with orthonormal
columns
for s = 1, 2, . . . do
i=1 xi

ËœWsâˆ’1

x(cid:62)
i

(cid:80)n

(cid:16)

(cid:17)

ËœU = 1
n
W0 = ËœWsâˆ’1
for t = 1, 2, . . . , m do

Btâˆ’1 = V U (cid:62), where U SV (cid:62) is an SVD decomposition of W (cid:62)
tâˆ’1
(cid:66) Equivalent to Btâˆ’1 = arg minB(cid:62)B=I (cid:107)Wtâˆ’1 âˆ’ ËœWsâˆ’1B(cid:107)2
F
Pick it âˆˆ {1, . . . , n} uniformly at random
x(cid:62)
W (cid:48)
xit
it
(cid:17)âˆ’1/2

t = Wtâˆ’1 + Î·

Wtâˆ’1 âˆ’ x(cid:62)
it

ËœWsâˆ’1Btâˆ’1

+ ËœU Btâˆ’1

(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

Wt = W (cid:48)
t

W (cid:48)(cid:62)

t W (cid:48)
t

ËœWsâˆ’1

end for
ËœWs = Wm

end for

We now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of

Algorithm 1 given in [19]:

Theorem 1. Deï¬ne the d Ã— d matrix A as 1
composed of the eigenvectors corresponding to the largest k eigenvalues. Suppose that

n XX (cid:62) = 1

i=1 xix(cid:62)

n

i , and let Vk denote the d Ã— k matrix

(cid:80)n

â€¢ maxi (cid:107)xi(cid:107)2 â‰¤ r for some r > 0.

â€¢ A has eigenvalues s1 > s2 â‰¥ . . . â‰¥ sd, where sk âˆ’ sk+1 = Î» for some Î» > 0.

â€¢ k âˆ’ (cid:107)V (cid:62)
k

ËœW0(cid:107)2

F â‰¤ 1
2 .

Let Î´, (cid:15) âˆˆ (0, 1) be ï¬xed. If we run the algorithm with any epoch length parameter m and step size Î·, such
that

Î· â‰¤

cÎ´2
r2 Î» , m â‰¥

c(cid:48) log(2/Î´)
Î·Î»

,

kmÎ·2r2 + rk

(cid:112)

mÎ·2 log(2/Î´) â‰¤ c(cid:48)(cid:48)

(3)

1The normalization Wt = W (cid:48)
t

(cid:17)âˆ’1/2

ensures that Wt has orthonormal columns. We note that in our analysis, Î· is

chosen sufï¬ciently small so that W

t is always invertible, hence the operation is well-deï¬ned.

(cid:48)(cid:62)
t W (cid:48)
t

(cid:16)

W
(cid:48)(cid:62)
t W (cid:48)

4

(where c, c(cid:48), c(cid:48)(cid:48) designate certain positive numerical constants), and for T =
probability at least 1 âˆ’ (cid:100)log2(1/(cid:15))(cid:101)Î´, it holds that

(cid:108) log(1/(cid:15))
log(2/Î´)

(cid:109)

epochs, then with

k âˆ’ (cid:107)V (cid:62)
k

ËœWT (cid:107)2

F â‰¤ (cid:15).

k W (cid:107)2

For any orthogonal W , k âˆ’ (cid:107)V (cid:62)

F lies between 0 and k, and equals 0 when the column spaces of
Vk and W are the same (i.e., when W spans the k leading singular vectors). According to the theorem,
taking appropriate2 Î· = Î˜(Î»/(kr)2), and m = Î˜((rk/Î»)2), the algorithm converges with high probability
to a high-accuracy approximation of Vk. Moreover, the runtime of each epoch of the algorithm equals
O(mdk2 + dnk). Overall, we get the following corollary:

Corollary 1. Under the conditions of Theorem 1, there exists an algorithm returning ËœWT such that k âˆ’
(cid:107)V (cid:62)
k

F â‰¤ (cid:15) with arbitrary constant accuracy, in runtime O

(cid:17)
Î»2 ) log(1/(cid:15))

dk(n + r2k3

ËœWT (cid:107)2

(cid:16)

.

ËœWt(cid:107)2

This runtime bound is the same3 as that of [19] for k = 1.
The proof of Theorem 1 appears in Subsection 6.1, and relies on a careful tracking of the evolution
of the potential function k âˆ’ (cid:107)V (cid:62)
F . An important challenge compared to the k = 1 case is that the
k
matrices Wtâˆ’1 and ËœWsâˆ’1 do not necessarily become closer over time, so the variance-reduction intuition
discussed earlier no longer applies. However, the column space of Wtâˆ’1 and ËœWsâˆ’1 do become closer, and
this is utilized by introducing the transformation matrix Btâˆ’1. We note that although Btâˆ’1 appears essential
for our analysis, it isnâ€™t clear that using it is necessary in practice: In [19], the suggested block algorithm
was Algorithm 2 with Btâˆ’1 = I, which seemed to work well in experiments. In any case, using this matrix
doesnâ€™t affect the overall runtime beyond constants, since the additional runtime of computing and using
this matrix (O(dk2)) is the same as the other computations performed at each iteration.

A limitation of the theorem above is the assumption that the initial point ËœW0 is such that kâˆ’(cid:107)V (cid:62)
k

F â‰¤
1
2 . This is a non-trivial assumption, since if we initialize the algorithm from a random d Ã— O(1) orthogo-
nal matrix ËœW0, then with overwhelming probability, (cid:107)V (cid:62)
F = O(1/d). However, experimentally the
k
algorithm seems to work well even with random initialization [19]. Moreover, if we are interested in a
theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm
for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a ËœW0. The idea is that ËœW0 is
only required to approximate Vk up to constant accuracy, so purely stochastic algorithms (which are good in
obtaining a low-accuracy solution) are quite suitable. In the next section, we further delve into these issues,
and show that in our setting such algorithms in fact can be substantially improved.

ËœW0(cid:107)2

ËœW0(cid:107)2

4 Warm-Start and the Power of a Power Iteration

In this section, we study the runtime required to compute a starting point satisfying the conditions of Theo-
rem 1, starting from a random initialization. Combined with Theorem 1, this gives us an end-to-end analysis
of the runtime required to ï¬nd an (cid:15)-accurate solution, starting from a random point. For simplicity, we will
only discuss the case k = 1, i.e. where our goal is to compute the single leading eigenvector v1, although

2Speciï¬cally, we can take m = c(cid:48) log(2/Î´)/Î·Î» and Î· = aÎ´2/r2Î», where a is sufï¬ciently small to ensure that the ï¬rst and third

condition in Eq. (3) holds. It can be veriï¬ed that itâ€™s enough to take a = min

(cid:26)

c,

c(cid:48)(cid:48)
4Î´2ck log(2/Î´) ,

1
4Î´2c

(cid:16)

c(cid:48)(cid:48)
k log(2/Î´)

(cid:17)2(cid:27)

.

3[19] showed that itâ€™s possible to further improve the runtime for sparse X, replacing d by the average column sparsity ds. This
is done by maintaining parameters in an implicit form, but itâ€™s not clear how to implement a similar trick in the block version, where
k > 1.

5

our observations can be generalized to k > 1. In the k = 1 case, Theorem 1 kicks in once we ï¬nd a vector
w satisfying (cid:104)v1, w(cid:105)2 â‰¥ 1
2 .

As mentioned previously, one way to get such a w is to run a purely stochastic algorithm, which com-
putes the leading eigenvector of a covariance matrix E[xx(cid:62)] given a stream of i.i.d. samples x. We can
easily use such an algorithm in our setting, by sampling columns from our matrix X = (x1, . . . , xn) uni-
formly at random, and feed to such a stochastic optimization algorithm, guaranteed to approximate the
leading eigenvector of 1
n

i=1 xix(cid:62)
i .

(cid:80)n

To the best of our knowledge, the existing iteration complexity guarantees for such algorithms (assuming
the norm constraint r â‰¤ 1 for simplicity) scale at least4 as d/Î»2. Since the runtime of each iteration is O(d),
we get an overall runtime of O((d/Î»)2).

The dependence on d in the iteration bound stems from the fact that with a random initial unit vector
w0, we have (cid:104)v1, w0(cid:105)2 â‰ˆ 1
d . Thus, we begin with a vector almost orthogonal to the leading eigenvector v1
(depending on d). In a purely stochastic setting, where only noisy information is available, this necessitates
conservative updates at ï¬rst, and in all the analyses we are aware of, the number of iterations appear to
necessarily scale at least linearly with d.

However, it turns out that in our setting, with a ï¬nite matrix X, we can perform a smarter initialization:
Sample w from the standard Gaussian distribution on Rd, perform a single power iteration w.r.t. the covari-
ance matrix A = 1
n XX (cid:62), i.e. w0 = Aw/(cid:107)Aw(cid:107), and initialize from w0. For such a procedure, we have the
following simple observation:
Lemma 1. For w0 as above, it holds for any Î´ that with probability at least 1 âˆ’ 1

d âˆ’ Î´,

(cid:104)v1, w0(cid:105)2 â‰¥

Î´2
12 log(d) nrank(A)

,

where nrank(A) = (cid:107)A(cid:107)2
F
(cid:107)A(cid:107)2
sp

is the numerical rank of A.

The numerical rank (see e.g. [18]) is a relaxation of the standard notion of rank: For any d Ã— d matrix
A, nrank(A) is at most the rank of A (which in turn is at most d). However, it will be small even if A
is just close to being low-rank. In many if not most machine learning applications, we are interested in
matrices which tend to be approximately low-rank, in which case nrank(A) is much smaller than d or even
a constant. Therefore, by a single power iteration, we get an initial point w0 for which (cid:104)v1, w0(cid:105)2 is on the
order of 1/nrank(A), which can be much larger than the 1/d given by a random initialization, and is never
substantially worse.

Proof of Lemma 1. Let s1 â‰¥ s2 â‰¥ . . . â‰¥ sd â‰¥ 0 be the d eigenvalues of A, with eigenvectors v1, . . . , vd.
We have

(cid:104)v1, w0(cid:105)2 =

(cid:104)v1, Aw(cid:105)2
(cid:107)Aw(cid:107)2 =

(s1(cid:104)v1, w(cid:105))2

(cid:16)(cid:80)d

i=1 sivi(cid:104)vi, w(cid:105)

(cid:17)2 =

(cid:80)d

s2
1(cid:104)v1, w(cid:105)2
i=1 s2

i (cid:104)vi, w(cid:105)2

.

Since w is distributed according to a standard Gaussian distribution, which is rotationally symmetric, we
can assume without loss of generality that v1, . . . , vd correspond to the standard basis vectors e1, . . . , ed,
in which case the above reduces to

s2
1w2
1
i w2
i=1 s2
i

(cid:80)d

â‰¥

s2
1
i=1 s2
i

(cid:80)d

w2
1
maxi w2
i

,

4For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired

output. The guarantee of [4] scale as d2/Î»2, and the guarantee of [10] scales as d/Î»3 in our setting.

6

where w1, . . . , wd are independent and scalar random variables with a standard Gaussian distribution.

First, we note that s2

nius norm of A. Therefore,

1 equals (cid:107)A(cid:107)2
s2
1
i s2
i

=

(cid:80)

sp, the spectral norm of A, whereas (cid:80)d
(cid:107)A(cid:107)2
sp
nrank(A) , and we get overall that
(cid:107)A(cid:107)2
F

=

1

i=1 s2

i equals (cid:107)A(cid:107)2

F , the Frobe-

(cid:104)v1, w0(cid:105)2 â‰¥

1
nrank(A)

w2
1
maxi w2
i

.

(4)

We consider the random quantity w2

1/ maxi w2

i , and independently bound the deviation probability of

the numerator and denominator. First, for any t â‰¥ 0 we have

Pr(w2

1 â‰¤ t) = Pr(w1 âˆˆ [âˆ’

âˆš

âˆš

t,

t]) =

âˆš

t

(cid:90)

âˆš

t

z=âˆ’

(cid:114) 1
2Ï€

(cid:18)

exp

âˆ’

(cid:19)

z2
2

â‰¤

(cid:114) 1
2Ï€

âˆš

âˆ— 2

t =

(cid:114) 2
Ï€

t .

(5)

Second, by combining two standard Gaussian concentration results (namely, that if W = max{|w1|, . . . , |wd|},
then 0 â‰¤ E[W ] â‰¤ 2(cid:112)2 log(d), and by the Cirelson-Ibragimov-Sudakov inequality, Pr(W âˆ’ E[W ] > t) â‰¤
exp(âˆ’t2/2)), we get that

and therefore

Pr(max

i

Pr(max

i

|wi| > 2(cid:112)2 log(d) + t) â‰¤ exp(âˆ’t2/2),

i > (2(cid:112)2 log(d) + t)2) â‰¤ exp(âˆ’t/2).
w2

(6)

Combining Eq. (5) and Eq. (6), with a union bound, we get that for any t1, t2 â‰¥ 0, it holds with probability
at least 1 âˆ’

(cid:113) 2

Ï€ t1 âˆ’ exp(âˆ’t2

2/2) that

w2
1
maxi w2
i

â‰¥

t1
(2(cid:112)2 log(d) + t2)2

.

To slightly simplify this for readability, we take t2 = (cid:112)2 log(d), and substitute Î´ =
that with probability at least 1 âˆ’ Î´ âˆ’ 1/d,

(cid:113) 2

Ï€ t1. This implies

w2
1
maxi w2
i

â‰¥

Ï€
2 Î´2
18 log(d)

>

Î´2
12 log(d)

.

Plugging back into Eq. (4), the result follows.

This result can be plugged into the existing analyses of purely stochastic PCA/SVD algorithms, and
can often improve the dependence on the d factor in the iteration complexity bounds to a dependence on
the numerical rank of A. We again emphasize that this is applicable in a situation where we can actually
perform a power iteration, and not in a purely stochastic setting where we only have access to an i.i.d. data
stream (nevertheless, it would be interesting to explore whether this idea can be utilized in such a streaming
setting as well).

To give a concrete example of this, we provide a convergence analysis of the VR-PCA algorithm (Algo-
rithm 1), starting from an arbitrary initial point, bounding the total number of stochastic iterations required
by the algorithm in order to produce a point satisfying the conditions of Theorem 1 (from which point the
analysis of Theorem 1 takes over). Combined with Theorem 1, this analysis also justiï¬es that VR-PCA
indeed converges starting from a random initialization.

7

Theorem 2. Using the notation of Theorem 1 (where Î» is the eigengap, v1 is the leading eigenvector, and
r = maxi (cid:107)xi(cid:107)2), and for any Î´ âˆˆ (0, 1
2 ), suppose we run Algorithm 1 with some initial unit-norm vector
Ëœw0 such that

(cid:104)v1, Ëœw0(cid:105)2 â‰¥ Î¶ > 0,

and a step size Î· satisfying

Î· â‰¤

cÎ´2Î»Î¶ 3
r2 log2(2/Î´)

(7)

(for some universal constant c). Then with probability at least 1 âˆ’ Î´, after

T =

(cid:22) c(cid:48) log(2/Î´)
Î·Î»Î¶

(cid:23)

stochastic iterations (lines 6 âˆ’ 10 in the pseudocode, where c(cid:48) is again a universal constant), we get a point
wT satisfying 1 âˆ’ (cid:104)v1, wT (cid:105)2 â‰¤ 1
2 . Moreover, if Î· is chosen on the same order as the upper bound in Eq. (7),
then

T = Î˜

(cid:18) r2 log3(2/Î´)
Î´2Î»2Î¶ 4

(cid:19)

.

Note that the analysis does not depend on the choice of the epoch size m, and does not use the special
structure of VR-PCA (in fact, the technique we use is applicable to any algorithm which takes stochastic
gradient steps to solve this type of problem5). The proof of the theorem appears in Section 6.2.

Considering Î´, r as a constants, we get that the runtime required by VR-PCA to ï¬nd a point w such that
1 âˆ’ (cid:104)v1, wT (cid:105)2 â‰¤ 1
2 is O(d/Î»2Î¶ 4) where Î¶ is a lower bound on (cid:104)v1, Ëœw0(cid:105)2. As discussed earlier, if Ëœw0 is
a result of random initialization followed by a power iteration (requiring O(nd) time), and the covariance
matrix A has small numerical rank, then Î¶ = (cid:104)v1, Ëœw0(cid:105)2 = Ëœâ„¦(1/ log(d)), and the runtime is

(cid:18)

O

nd +

(cid:19)
d
Î»2 log4(d)

(cid:32)

(cid:32)

= O

d

n +

(cid:18) log2(d)
Î»

(cid:19)2(cid:33)(cid:33)

.

By Corollary 1, the runtime required by VR-PCA from that point to get an (cid:15)-accurate solution is

(cid:18)

(cid:18)

O

d

n +

(cid:19)

1
Î»2

log

(cid:19)(cid:19)

,

(cid:18) 1
(cid:15)

so the sum of the two expressions (which is d (cid:0)n + 1
Î»2
required by the algorithm.

(cid:1) up to log-factors), represents the total runtime

Finally, we note that this bound holds under the reasonable assumption that the numeric rank of A
If this assumption doesnâ€™t hold, Î¶ can be as large as d, and the resulting bound will have
is constant.
a worse polynomial dependence on d. We suspect that this is due to a looseness in the dependence on
Î¶ = (cid:104)v1, Ëœw0(cid:105)2 in Theorem 2, since better dependencies can be obtained, at least for slightly different
algorithmic approaches (e.g. [4, 10, 6]). We leave a sharpening of the bound w.r.t. Î¶ as an open problem.

5Although there exist previous analyses of such algorithms in the literature, they unfortunately do not quite apply to our algo-

rithm, for various technical reasons.

8

5 Convexity and Non-Convexity of the Rayleigh Quotient

As mentioned in the introduction, an intriguing open question is whether the d (cid:0)n + 1
(cid:1) runtime
Î»2
guarantees from the previous sections can be further improved. Although a linear dependence on d, n seems
unavoidable, this is not the case for the quadratic dependence on 1/Î». Indeed, when using deterministic
methods such as power iterations or the Lanczos method, the dependence on Î» in the runtime is only 1/Î» or
even (cid:112)1/Î» [15]. In the world of convex optimization from which our algorithmic techniques are derived,
the analog of Î» is the strong convexity parameter of the function, and again, it is possible to get a dependence
of 1/Î», or even (cid:112)1/Î» with accelerated schemes (see e.g. [13, 16, 7] in the context of the variance-reduction
technique we use). Is it possible to get such a dependence for our problem as well?

(cid:1) log (cid:0) 1
(cid:15)

Another question is whether the non-convex problem that we are tackling (Eq. (1)) is really that non-
convex. Clearly, it has a nice structure (since we can solve the problem in polynomial time), but perhaps it
actually has hidden convexity properties, at least close enough to the optimal points? We note that Eq. (1)
can be â€œtriviallyâ€ convexiï¬ed, by re-casting it as an equivalent semideï¬nite program [5]. However, that
would require optimization over d Ã— d matrices, leading to poor runtime and memory requirements. The
question here is whether we have any convexity with respect to the original optimization problem over
â€œthinâ€ d Ã— k matrices.

In fact, the two questions of improved runtime and convexity are closely related: If we can show that the
optimization problem is convex in some domain containing an optimal point, then we may be able to use
fast stochastic algorithms designed for convex optimization problems, inheriting their good guarantees.

To discuss these questions, we will focus on the k = 1 case for simplicity (i.e., our goal is to ï¬nd a
(cid:80)n
i ), and study potential convexity properties

n XX (cid:62) = 1

i=1 xix(cid:62)

n

leading eigenvector of the matrix A = 1
of the negative Rayleigh quotient,

FA(w) = âˆ’

w(cid:62)Aw
(cid:107)w(cid:107)2 =

1
n

n
(cid:88)

(cid:18)

âˆ’

i=1

(cid:104)w, xi(cid:105)2
(cid:107)w(cid:107)2

(cid:19)

.

Note that for k = 1, this function coincides with Eq. (1) on the unit Euclidean sphere, and with the same
optimal points, but has the nice property of being deï¬ned on the entire Euclidean space (thus, at least its
domain is convex).

At a ï¬rst glance, such functions FA appear to potentially be convex at some bounded distance from an

optimum, as illustrated for instance in the case where A =

(see Figure 1). Unfortunately, it turns

(cid:19)

(cid:18) 1 0
0 0

out that the ï¬gure is misleading, and in fact the function is not convex almost everywhere:

Theorem 3. For the matrix A above, the Hessian of FA is not positive semideï¬nite for all but a measure-zero
set.
Proof. The leading eigenvector of A is v1 = (1, 0), and FA(w) = âˆ’ w2
at some w equals

. The Hessian of this function

1
1+w2
2

w2

2
1 + w2

2)3

(w2

(cid:18) w2

2(3w2
âˆ’2w1w2(w2

1 âˆ’ w2
2)
1 âˆ’ w2
2)

âˆ’2w1w2(w2
1(w2

1 âˆ’ w2
2)
1 âˆ’ 3w2
2)

w2

(cid:19)

.

9

Figure 1: The function (w1, w2) (cid:55)â†’ âˆ’ w2
,
1
1+w2
2
corresponding to FA(w) where A = (1 0 ; 0 0).
It is invariant to re-scaling of w, and attains a
minimum at (a, 0) for any a (cid:54)= 0.

w2

The determinant of this 2 Ã— 2 matrix equals

Figure 2: Illustration of the construction of the
convex set on which FA is strongly convex and
smooth. v1 is the leading eigenvector of A, and a
minimum of FA (as well as any re-scaling of v1).
w0 is a nearby unit vector, and we consider the
intersection of a hyperplane orthogonal to w0,
and an Euclidean ball centered at w0.

(w2

=

=

(cid:0)w2

4
1 + w2
2)6
4w2
1w2
2
1 + w2
2)6
1w2
4w2
2
1 + w2
2)6

(w2

(w2

1w2

2(3w2

1 âˆ’ w2

2)(w2

1 âˆ’ 3w2

2) âˆ’ 4w2

1w2

2(w2

1 âˆ’ w2

2)2(cid:1)

(cid:0)(3w2

1 âˆ’ w2

2)(w2

1 âˆ’ 3w2

2) âˆ’ 4(w2

1 âˆ’ w2

2)2(cid:1)

(cid:0)âˆ’(w2

1 + w2

2)2(cid:1) = âˆ’

1w2
4w2
2
1 + w2

2)4 ,

(w2

which is always non-positive, and strictly negative for w for which w1w2 (cid:54)= 0 (which holds for all but a
measure-zero set of Rd). Since the determinant of a positive semideï¬nite matrix is always non-negative, this
implies that the Hessian isnâ€™t positive semideï¬nite for any such w.

The theorem implies that we indeed cannot use convex optimization tools as-is on the function FA, even
if weâ€™re close to an optimum. However, the non-convexity was shown for FA as a function over the entire
Euclidean space, so the result does not preclude the possibility of having convexity on a more constrained,
lower-dimensional set. In fact, this is what we are going to do next: We will show that if we are given some
point w0 close enough to an optimum, then we can explicitly construct a simple convex set, such that

â€¢ The set includes an optimal point of FA.

â€¢ The function FA is O(1)-smooth and Î»-strongly convex in that set.

10

ğ’—1ğ’˜0ğ»ğ’˜0Unit Euclidean sphereğµğ’˜0(ğ‘Ÿ)Optimal pointsThis means that we can potentially use a two-stage approach: First, we use some existing algorithm (such
as VR-PCA) to ï¬nd w0, and then switch to a convex optimization algorithm designed to handle functions
with a ï¬nite sum structure (such as FA). Since the runtime of such algorithms scale better than VR-PCA, in
terms of the dependence on Î», we can hope for an overall runtime improvement.

Unfortunately, this has a catch: To make it work, we need to have w0 very close to the optimum â€“ in
fact, we require (cid:107)v1 âˆ’ w0(cid:107) â‰¤ O(Î»), and we show (in Theorem 5) that such a dependence on the eigengap Î»
cannot be avoided (perhaps up to a small polynomial factor). The issue is that the runtime to get such a w0,
using stochastic-based approaches we are aware of, would scale at least quadratically with 1/Î», but getting
dependence better than quadratic was our problem to begin with. For example, the runtime guarantee using
VR-PCA to get such a point w0 (even if we start from a good point as speciï¬ed in Theorem 1) is on the
order of

(cid:18)

d

n +

(cid:19)

1
Î»2

log

(cid:19)

,

(cid:18) 1
Î»

whereas the best known guarantees on getting an (cid:15)-optimal solution for Î»-strongly convex and smooth
functions (see [1]) is on the order of

(cid:18)

d

n +

(cid:19)

(cid:114) n
Î»

log

(cid:19)

.

(cid:18) 1
(cid:15)

Therefore, the total runtime we can hope for would be on the order of

(cid:18)(cid:18)

d

n +

(cid:19)

1
Î»2

log

(cid:18) 1
Î»

(cid:19)

(cid:18)

+

n +

(cid:19)

(cid:114) n
Î»

log

(cid:19)(cid:19)

.

(cid:18) 1
(cid:15)

(8)

In comparison, the runtime guarantee of using just VR-PCA to get an (cid:15)-accurate solution is on the order of

(cid:18)

d

n +

(cid:19)

1
Î»2

log

(cid:19)

.

(cid:18) 1
(cid:15)

(9)

Unfortunately, Eq. (9) is the same as Eq. (8) up to log-factors, and the difference is not signiï¬cant unless
the required accuracy (cid:15) is extremely small (exponentially small in n, 1/Î»). Therefore, our construction is
mostly of theoretical interest. However, it still shows that asymptotically, as (cid:15) â†’ 0, it is indeed possible
to have runtime scaling better than Eq. (9). This might hint that designing practical algorithms, with better
runtime guarantees for our problem, may indeed be possible.

To explain our construction, we need to consider two convex sets: Given a unit vector w0, deï¬ne the

hyperplane tangent to w0,

Hw0 = {w : (cid:104)w, w0(cid:105) = 1}

as well as a Euclidean ball of radius r centered at w0:

Bw0(r) = {w : (cid:107)w âˆ’ w0(cid:107) â‰¤ r}

The convex set we use, given such a w0, is simply the intersection of the two, Hw0 âˆ© Bw0(r), where r is a
sufï¬ciently small number (see Figure 2).

The following theorem shows that if w0 is O(Î»)-close to an optimal point (a leading eigenvector v1 of
A), and we choose the radius of Bw0(r) appropriately, then Hw0 âˆ© Bw0(r) contains an optimal point, and
the function FA is indeed Î»-strongly convex and smooth on that set. For simplicity, we will assume that A
is scaled to have spectral norm of 1, but the result can be easily generalized.

11

Theorem 4. For any positive semideï¬nite A with spectral norm 1, eigengap Î» and a leading eigenvector
v1, and any unit vector w0 such that (cid:107)w0 âˆ’ v1(cid:107) â‰¤ Î»
44 , the function FA(w) is 20-smooth and Î»-strongly
convex on the convex set Hw0 âˆ© Bw0

(cid:1), which contains a global optimum of FA.

(cid:0) Î»
22

The proof of the theorem appears in Subsection 6.3. Finally, we show below that a polynomial depen-
dence on the eigengap Î» is unavoidable, in the sense that the convexity property is lost if w0 is signiï¬cantly
further away from v1.
Theorem 5. For any Î», (cid:15) âˆˆ (cid:0)0, 1
(cid:1), there exists a positive semideï¬nite matrix A with spectral norm 1,
2
eigengap Î», and leading eigenvector v1, as well as a unit vector w0 for which (cid:107)v1 âˆ’ w0(cid:107) â‰¤ (cid:112)2(1 + (cid:15))Î»),
such that FA is not convex in any neighborhood of w0 on Hw0.

Proof. Let

for which v1 = (1, 0, 0), and take

ï£«

ï£­

A =

1
0
0
0 1 âˆ’ Î» 0
0
0
0

ï£¶

ï£¸ ,

(cid:112)

w0 = (

1 âˆ’ p2, 0, p),

where p = (cid:112)(1 + (cid:15))Î» (which ensures (cid:107)v1âˆ’w0(cid:107)2 = (cid:112)2p2 = (cid:112)2(1 + (cid:15))Î»). Consider the ray {((cid:112)1 âˆ’ p2, t, p) :
t â‰¥ 0}, and note that it starts from w0 and lies in Hw0. The function FA along that ray (considering it as a
function of t) is of the form

âˆ’

(1 âˆ’ p2) + (1 âˆ’ Î»)t2
(1 âˆ’ p2) + t2 + p2 = âˆ’

1 âˆ’ p2 + (1 âˆ’ Î»)t2
1 + t2

.

The second derivative with respect to t equals

âˆ’2

(3t2 âˆ’ 1)(Î» âˆ’ p2)
(t2 + 1)3

= 2

(3t2 âˆ’ 1)(cid:15)Î»
(t2 + 1)3 ,

where we plugged in the deï¬nition of p. This is a negative quantity for any t < 1âˆš
. Therefore, the function
3
FA is strictly concave (and not convex) along the ray we have deï¬ned and close enough to w0, and therefore
isnâ€™t convex in any neighborhood of w0 on Hw0.

6 Proofs

6.1 Proof of Theorem 1

Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case,
it is more intricate and requires several new technical tools. To streamline the presentation of the proof, we
begin with proving a series of auxiliary lemmas in Subsection 6.1.1, and then move to the main proof in
Subsection 6.1. The main proof itself is divided into several steps, each constituting one or more lemmas.

Throughout the proof, we use the well-known facts that for all matrices B, C, D of suitable dimensions,
Tr(B + C) = Tr(B) + Tr(C), Tr(BC) = Tr(CB), Tr(BCD) = Tr(DBC), and Tr(B(cid:62)B) = (cid:107)B(cid:107)2
F .
Moreover, since Tr is a linear operation, E[Tr(B)] = E[Tr(B)] for a random matrix B.

12

6.1.1 Auxiliary Lemmas

Lemma 2. For any B, C, D (cid:23) 0, it holds that Tr(BC) â‰¥ Tr(B(C âˆ’ D)) and Tr(BC) â‰¥ Tr((B âˆ’ D)C).

Proof. It is enough to prove that for any positive semideï¬nite matrices E, G, it holds that Tr(EG) â‰¥ 0. The
lemma follows by taking either E = B, G = D (in which case, Tr(BC) = Tr(B(C âˆ’ D)) + Tr(BD) â‰¥
Tr(B(Câˆ’D))), or E = D, G = C (in which case, Tr(BC) = Tr((Bâˆ’D)C)+Tr(DC) â‰¥ Tr((Bâˆ’D)C)).
Any positive semideï¬nite matrix M can be written as the product M 1/2M 1/2 for some symmetric matrix

M 1/2 (known as the matrix square root of M ). Therefore,

Tr(EG) = Tr(E1/2E1/2G1/2G1/2) = Tr(G1/2E1/2E1/2G1/2)

= Tr((E1/2G1/2)(cid:62)(E1/2G1/2)) = (cid:107)E1/2G1/2(cid:107)2

F â‰¥ 0.

Lemma 3. If B (cid:23) 0 and C (cid:31) 0, then

where I is the identity matrix.

Tr(BCâˆ’1) â‰¥ Tr(B(2I âˆ’ C)),

Proof. We begin by proving the one-dimensional case, where B, C are scalars b â‰¥ 0, c > 0. The inequality
then becomes bcâˆ’1 â‰¥ b(2 âˆ’ c), which is equivalent to 1 â‰¥ c(2 âˆ’ c), or upon rearranging, (c âˆ’ 1)2 â‰¥ 0,
which trivially holds.

Turning to the general case, we note that by Lemma 2, it is enough to prove that Câˆ’1 âˆ’ (2I âˆ’ C) (cid:23) 0.
To prove this, we make a couple of observations. The positive deï¬nite matrix C (like any positive deï¬nite
matrix) has a singular value decomposition which can be written as U SU (cid:62), where U is an orthogonal matrix,
and S is a diagonal matrix with positive entries. Its inverse is U Sâˆ’1U (cid:62), and 2I âˆ’ C = 2I âˆ’ U SU (cid:62) =
U (2I âˆ’ S)U (cid:62). Therefore,

Câˆ’1 âˆ’ (2I âˆ’ C) = U Sâˆ’1U (cid:62) âˆ’ U (2I âˆ’ S)U (cid:62) = U (Sâˆ’1 âˆ’ (2I âˆ’ S))U (cid:62).

To show this matrix is positive semideï¬nite, it is enough to show that each diagonal entry of Sâˆ’1 âˆ’ (2I âˆ’ S)
is non-negative. But this reduces to the one-dimensional result we already proved, when b = 1 and c > 0 is
any diagonal entry in S. Therefore, Câˆ’1 âˆ’ (2I âˆ’ C) (cid:23) 0, from which the result follows.

Lemma 4. For any matrices B, C,

and

Tr(BC) â‰¤ (cid:107)B(cid:107)F (cid:107)C(cid:107)F

(cid:107)BC(cid:107)F â‰¤ (cid:107)B(cid:107)sp(cid:107)C(cid:107)F .

Proof. The ï¬rst inequality is immediate from Cauchy-Shwartz. As to the second inequality, letting ci denote
the i-th column of C, and (cid:107) Â· (cid:107)2 the Euclidean norm for vectors,

(cid:107)BC(cid:107)F =

(cid:115)(cid:88)

i

(cid:107)Bci(cid:107)2

2 â‰¤

(cid:115)(cid:88)

i

((cid:107)B(cid:107)sp(cid:107)ci(cid:107)2)2 = (cid:107)B(cid:107)sp

(cid:115)(cid:88)

i

(cid:107)ci(cid:107)2

2 = (cid:107)B(cid:107)sp(cid:107)C(cid:107)F .

13

Lemma 5. Let B1, B2, Z1, Z2 be k Ã— k square matrices, where B1, B2 are ï¬xed and Z1, Z2 are stochastic
and zero-mean (i.e.
their expectation is the all-zeros matrix). Furthermore, suppose that for some ï¬xed
Î±, Î³, Î´ > 0, it holds with probability 1 that

â€¢ For all Î½ âˆˆ [0, 1], B2 + Î½Z2 (cid:23) Î´I.

â€¢ max{(cid:107)Z1(cid:107)F , (cid:107)Z2(cid:107)F } â‰¤ Î±.

â€¢ (cid:107)B1 + Î·Z1(cid:107)sp â‰¤ Î³.

Then

E (cid:2)Tr (cid:0)(B1 + Z1)(B2 + Z2)âˆ’1(cid:1)(cid:3) â‰¥ Tr(B1Bâˆ’1

2 ) âˆ’

Î±2(1 + Î³/Î´)
Î´2

.

Proof. Deï¬ne the function

f (Î½) = Tr (cid:0)(B1 + Î½Z1)(B2 + Î½Z2)âˆ’1(cid:1) ,

Î½ âˆˆ [0, 1].

Since B2 + Î½Z2 is positive deï¬nite, it is always invertible, hence f (Î½) is indeed well-deï¬ned. Moreover, it
can be differentiated with respect to Î½, and we have

f (cid:48)(Î½) = Tr (cid:0)Z1(B2 + Î½Z2)âˆ’1 âˆ’ (B1 + Î½Z1)(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1(cid:1) .

Again differentiating with respect to Î½, we have

f (cid:48)(cid:48)(Î½) = Tr

(cid:16)

âˆ’ 2Z1(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1
+ 2(B1 + Î½Z1)(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1(cid:17)
(cid:16)(cid:16)

(cid:17)

âˆ’ Z1 + (B1 + Î½Z1)(B2 + Î½Z2)âˆ’1Z2

(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1(cid:17)

.

= 2 Tr

Using Lemma 4 and the triangle inequality, this is at most

2(cid:107) âˆ’ Z1 + (B1 + Î½Z1)(B2 + Î½Z2)âˆ’1Z2(cid:107)F (cid:107)(B2 + Î½Z2)âˆ’1Z2(B2 + Î½Z2)âˆ’1(cid:107)F
â‰¤ 2 (cid:0)(cid:107)Z1(cid:107)F + (cid:107)(B1 + Î½Z1)(B2 + Î½Z2)âˆ’1Z2(cid:107)F

(cid:1) (cid:107)(B2 + Î½Z2)âˆ’1(cid:107)2
(cid:17)

sp(cid:107)Z2(cid:107)F
(cid:107)(B2 + Î½Z2)âˆ’1(cid:107)2

sp(cid:107)Z2(cid:107)F

(cid:16)

â‰¤ 2

(cid:107)Z1(cid:107)F + (cid:107)B1 + Î½Z1(cid:107)sp(cid:107) (B2 + Î½Z2)âˆ’1 (cid:107)sp(cid:107)Z2(cid:107)F
(cid:18)

1
Î´

(cid:19) 1

Î´2 Î± =

2Î±2(1 + Î³/Î´)
Î´2

.

â‰¤ 2

Î± + Î³

Î±

Applying a Taylor expansion to f (Â·) around Î½ = 0, with a Lagrangian remainder term, and substituting the
values for f (cid:48)(Î½), f (cid:48)(cid:48)(Î½), we can lower bound f (1) as follows:

f (1) â‰¥ f (0) + f (cid:48)(0) âˆ— (1 âˆ’ 0) âˆ’

|f (cid:48)(cid:48)(Î½)| âˆ— (1 âˆ’ 0)2

1
2
(cid:1) + Tr (cid:0)Z1Bâˆ’1
2 âˆ’ B1Bâˆ’1

max
Î½

= Tr (cid:0)B1Bâˆ’1

2

2 Z2Bâˆ’1
2

(cid:1) âˆ’

Î±2(1 + Î³/Î´)
Î´2

.

Taking expectation over Z1, Z2, and recalling they are zero-mean, we get that

E[f (1)] â‰¥ Tr (cid:0)B1Bâˆ’1

2

(cid:1) âˆ’

Î±2(1 + Î³/Î´)
Î´2

.

Since E[f (1)] = E (cid:2)Tr (cid:0)(B1 + Z1)(B2 + Z2)âˆ’1(cid:1)(cid:3), the result in the lemma follows.

14

Lemma 6. Let U1, . . . , Uk and R1, R2 be positive semideï¬nite matrices, such that R2 âˆ’ R1 (cid:23) 0, and deï¬ne
the function

f (x1 . . . xk) = Tr

ï£­

ï£«

(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33) (cid:32) k

(cid:88)

i=1

xiUi + R2

(cid:33)âˆ’1ï£¶
ï£¸ .

over all (x1 . . . xk) âˆˆ [Î±, Î²]d for some Î² â‰¥ Î± â‰¥ 0. Then min(x1...xk)âˆˆ[Î±,Î²]d f (x) = f (Î±, . . . , Î±).
Proof. Taking a partial derivative of f with respect to some xj, we have

âˆ‚
âˆ‚xj

f (x)

ï£«

= Tr

ï£­Uj

(cid:33)âˆ’1

xiUi + R2

âˆ’

(cid:32) k

(cid:88)

i=1

(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33) (cid:32) k

(cid:88)

i=1

(cid:33)âˆ’1

xiUi + R2

Uj

(cid:33)âˆ’1ï£¶
ï£¸

xiUi + R2

(cid:32) k

(cid:88)

i=1

ï£«

ï£«

= Tr

ï£­

ï£­I âˆ’

(cid:32) k

(cid:88)

i=1

= Tr

(cid:32)(cid:32) k

(cid:88)

i=1

ï£«

ï£­

ï£«

= Tr

ï£­(R2 âˆ’ R1)

(cid:33) (cid:32) k

(cid:88)

Ui + R1

(cid:33)âˆ’1ï£¶

xiUi + R2

ï£¸ Uj

(cid:33)âˆ’1ï£¶
ï£¸

xiUi + R2

(cid:32) k

(cid:88)

i=1

(cid:33)

xiUi + R2

âˆ’

i=1

(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33)(cid:33) (cid:32) k

(cid:88)

i=1

(cid:33)âˆ’1

xiUi + R2

Uj

(cid:33)âˆ’1ï£¶
ï£¸

xiUi + R2

(cid:32) k

(cid:88)

i=1

(cid:33)âˆ’1

xiUi + R2

Uj

(cid:32) k

(cid:88)

i=1

(cid:32) k

(cid:88)

i=1

(cid:33)âˆ’1ï£¶
ï£¸ .

xiUi + R2

By the lemmaâ€™s assumptions, each matrix in the product above is positive semideï¬nite, hence the product
f (x) â‰¥ 0, which implies that the
is positive semideï¬nite, and the trace is non-negative. Therefore,
function is minimized when each xj takes its smallest possible value, i.e. Î±.

âˆ‚
âˆ‚xj

Lemma 7. Let B be a k Ã— k matrix with minimal singular value Î´. Then

1 âˆ’

(cid:107)B(cid:62)B(cid:107)2
F
(cid:107)B(cid:107)2
F

(cid:26)

â‰¥ max

1 âˆ’ (cid:107)B(cid:107)2

F ,

(cid:0)k âˆ’ (cid:107)B(cid:107)2
F

(cid:1)

(cid:27)

.

Î´2
k

Proof. We have

1 âˆ’

(cid:107)B(cid:62)B(cid:107)2
F
(cid:107)B(cid:107)2
F
so it remains to prove 1 âˆ’ (cid:107)B(cid:62)B(cid:107)2
(cid:0)k âˆ’ (cid:107)B(cid:107)2
â‰¥ Î´2
F
(cid:107)B(cid:107)2
k
F
of B. The singular values of B(cid:62)B are Ïƒ2
1, . . . , Ïƒ2
norm of its vector of singular values. Therefore, the lemma is equivalent to requiring

F (cid:107)B(cid:107)2
F
(cid:107)B(cid:107)2
F
(cid:1). Let Ïƒ1, . . . , Ïƒk denote the vector of singular values
k, and the Frobenius norm of a matrix equals the Euclidean

= 1 âˆ’ (cid:107)B(cid:107)2
F ,

â‰¥ 1 âˆ’

(cid:107)B(cid:107)2

F

1 âˆ’

(cid:80)k

(cid:80)k

i=1 Ïƒ4
i
i=1 Ïƒ2
i

â‰¥

Î´2
k

(cid:32)

k âˆ’

(cid:33)

Ïƒ2
i

,

k
(cid:88)

i=1

assuming Ïƒi âˆˆ [Î´, 1] for all i. This holds since
(cid:80)

(cid:80)

1 âˆ’

=

(cid:80)
(cid:80)

i Ïƒ4
i
i Ïƒ2
i

i âˆ’ (cid:80)
i Ïƒ2
(cid:80)
i Ïƒ2
i

i Ïƒ4
i

=

i Ïƒ2
i
(cid:80)

(cid:0)1 âˆ’ Ïƒ2
i
i Ïƒ2
i

(cid:1)

â‰¥

Î´2 (cid:80)
i

(cid:1)

(cid:0)1 âˆ’ Ïƒ2
i
k

=

Î´2
k

(cid:32)

k âˆ’

(cid:33)

Ïƒ2
i

.

(cid:88)

i

15

Lemma 8. For any d Ã— k matrices C, D with orthonormal columns, let

DC = arg

min
DB : (DB)(cid:62)(DB)=I

(cid:107)C âˆ’ DB(cid:107)2
F

be the nearest orthonormal-columns matrix to C in the column space of D (where B is a k Ã—k matrix). Then
the matrix B minimizing the above equals B = V U (cid:62), where C(cid:62)D = U SV (cid:62) is the SVD decomposition of
C(cid:62)D, and it holds that

(cid:107)C âˆ’ DC(cid:107)2

F â‰¤ 2(k âˆ’ (cid:107)C(cid:62)D(cid:107)2

F ).

Proof. Since D has orthonormal columns, we have D(cid:62)D = I, so the deï¬nition of B is equivalent to

B = arg min

B : B(cid:62)B=I

(cid:107)C âˆ’ DB(cid:107)2
F .

This is the orthogonal Procrustes problem (see e.g. [8]), and the solution is easily shown to be B = V U (cid:62)
where U SV (cid:62) is the SVD decomposition of C(cid:62)D. In this case, and using the fact that (cid:107)C(cid:107)2
F = k
(as C, D have orthonormal columns), we have that (cid:107)C âˆ’ DC(cid:107)2

F = (cid:107)D(cid:107)2

F equals

(cid:107)C âˆ’ DB(cid:107)2

F = (cid:107)C(cid:107)2

F + (cid:107)D(cid:107)2

F âˆ’ 2 Tr(C(cid:62)DB) = 2

(cid:16)

(cid:17)
k âˆ’ Tr(U SV (cid:62)(V U (cid:62)))

(cid:16)

(cid:17)
k âˆ’ Tr(U SU (cid:62))

.

= 2

Since the trace function is similarity-invariant, this equals 2k âˆ’ Tr(S). Let s1 . . . , sk be the diagonal ele-
ments of S, and note that they can be at most 1 (since they are the singular values of C(cid:62)D, and both C and
D have orthonormal columns). Recalling that the Frobenius norm equals the Euclidean norm of the singular
values, we can therefore upper bound the above as follows:

(cid:16)

(cid:17)
k âˆ’ Tr(U SU (cid:62))

2

= 2 (k âˆ’ Tr(S)) = 2

k âˆ’

(cid:32)

k
(cid:88)

i=1

(cid:33)

(cid:32)

si

â‰¤ 2

k âˆ’

(cid:33)

k
(cid:88)

i=1

s2
i

(cid:16)

= 2

k âˆ’ (cid:107)C(cid:62)D(cid:107)2
F

(cid:17)

.

Lemma 9. Let Wt, W (cid:48)
Vk with orthonormal columns, it holds that

t be as deï¬ned in Algorithm 2, where we assume Î· < 1

3 . Then for any d Ã— k matrix

(cid:12)
(cid:12)(cid:107)V (cid:62)
(cid:12)

k Wt(cid:107)2

F âˆ’ (cid:107)V (cid:62)

k Wtâˆ’1(cid:107)2
F

(cid:12)
(cid:12)
(cid:12) â‰¤

12kÎ·
1 âˆ’ 3Î·

.

Proof. Letting st, stâˆ’1 denote the vectors of singular values of V (cid:62)
k Wtâˆ’1, and noting that they
are both in [0, 1]k (as Vk, Wtâˆ’1, Wt all have orthonormal columns), the left hand side of the inequality in the
lemma statement equals

k Wt and V (cid:62)

|(cid:107)st(cid:107)2 âˆ’ (cid:107)stâˆ’1(cid:107)2| = ((cid:107)st(cid:107)2 + (cid:107)stâˆ’1(cid:107)2) | (cid:107)st(cid:107)2 âˆ’ (cid:107)stâˆ’1(cid:107)2 | â‰¤ 2

âˆš

k(cid:107)st âˆ’ stâˆ’1(cid:107)2 â‰¤ 2k(cid:107)st âˆ’ stâˆ’1(cid:107)âˆ,

where (cid:107) Â· (cid:107)âˆ is the inï¬nity norm. By Weylâ€™s matrix perturbation theorem6 [12], this is upper bounded by

2k(cid:107)V (cid:62)

k Wt âˆ’ V (cid:62)

k Wtâˆ’1(cid:107)sp â‰¤ 2k(cid:107)Vk(cid:107)sp(cid:107)Wt âˆ’ Wtâˆ’1(cid:107)sp â‰¤ 2k(cid:107)Wt âˆ’ Wtâˆ’1(cid:107)sp.

(10)

6Using its version for singular values, which implies that the singular values of matrices B and B + E are different by at most

(cid:107)E(cid:107)sp.

16

Recalling the relationship between Wt and Wtâˆ’1 from Algorithm 2, we have that

W (cid:48)

t = Wtâˆ’1 + Î·N,

where

(cid:107)N (cid:107)sp â‰¤ (cid:107)xitx(cid:62)

it Wtâˆ’1(cid:107)sp + (cid:107)xitx(cid:62)
it

ËœWsâˆ’1Btâˆ’1(cid:107)sp + (cid:107)

1
n

n
(cid:88)

i=1

xix(cid:62)
i

ËœWsâˆ’1Btâˆ’1(cid:107)sp â‰¤ 3,

as Wtâˆ’1, ËœWsâˆ’1, Btâˆ’1 all have orthonormal columns, and xitx(cid:62)
i have spectral norm at
most 1. Therefore, W (cid:48)
t equals Wtâˆ’1, up to a matrix perturbation of spectral norm at most 3Î·. Again by
Weylâ€™s theorem, this implies that the k non-zero singular values of the d Ã— k matrix W (cid:48)
t are different from
those of Wtâˆ’1 (which has orthonormal columns) by at most 3Î·, and hence all lie in [1 âˆ’ 3Î·, 1 + 3Î·]. As a

i=1 xix(cid:62)

it and 1
n

(cid:80)n

result, the singular values of

(cid:16)

W (cid:48)(cid:62)

t W (cid:48)
t

(cid:17)âˆ’1/2

all lie in

(cid:104) 1
1+3Î· ,

1
1âˆ’3Î·

(cid:105)
. Collecting these observations, we have

(cid:107)Wt âˆ’ Wtâˆ’1(cid:107)sp = (cid:107)(Wtâˆ’1 + Î·N )

â‰¤ (cid:107)Wtâˆ’1

(cid:18)(cid:16)

W

(cid:48)(cid:62)
tâˆ’1W (cid:48)

tâˆ’1

(cid:17)âˆ’1/2

âˆ’ I

(cid:16)

W

(cid:19)

(cid:48)(cid:62)
tâˆ’1W (cid:48)
(cid:16)

+ Î·N

(cid:17)âˆ’1/2

âˆ’ Wtâˆ’1(cid:107)sp

tâˆ’1

W

(cid:48)(cid:62)
tâˆ’1W (cid:48)

tâˆ’1

(cid:17)âˆ’1/2

(cid:107)sp

(cid:16)

W

â‰¤ (cid:107)

(cid:48)(cid:62)
tâˆ’1W (cid:48)

(cid:17)âˆ’1/2

âˆ’ I(cid:107)sp + Î·(cid:107)N (cid:107)sp(cid:107)

(cid:16)

W

(cid:48)(cid:62)
tâˆ’1W (cid:48)

tâˆ’1

(cid:17)âˆ’1/2

(cid:107)sp

â‰¤

3Î·
1 âˆ’ 3Î·

+

tâˆ’1
3Î·
1 âˆ’ 3Î·

=

6Î·
1 âˆ’ 3Î·

.

Plugging back to Eq. (10), the result follows.

6.1.2 Main Proof

To simplify the technical derivations, note that the algorithm remains the same if we divide each xi by
r,
and multiply Î· by r. Since maxi (cid:107)xi(cid:107)2 â‰¤ r, this corresponds to running the algorithm with step-size Î·r
rather than Î·, on a re-scaled dataset of points with squared norm at most 1, and with an eigengap of Î»/r
instead of Î». Therefore, we can simply analyze the algorithm assuming that maxi (cid:107)xi(cid:107)2 â‰¤ 1, and in the end
plug in Î»/r instead of Î», and Î·r instead of Î·, to get a result which holds for data with squared norm at most
r.

âˆš

Part I: Establishing a Stochastic Recurrence Relation

We begin by focusing on a single iteration t of the algorithm, and analyze how (cid:107)V (cid:62)
F (which measures
the similarity between the column spaces of Vk and Wt) evolves during that iteration. The key result we
need is Lemma 10 below, which is specialized for our algorithm in Lemma 11.

k Wt(cid:107)2

Lemma 10. Let A be a d Ã— d symmetric matrix with all eigenvalues s1 â‰¥ s2 â‰¥ . . . â‰¥ sd in [0, 1], and
suppose that sk âˆ’ sk+1 â‰¥ Î» for some Î» > 0.

Let N be a d Ã— k zero-mean random matrix such that (cid:107)N (cid:107)F â‰¤ ÏƒF

1, and deï¬ne

(cid:32)

rN = 46 (ÏƒF

N )2

1 +

(cid:18) 1
4

8
3

Ïƒsp
N + 2

17

N and (cid:107)N (cid:107)sp â‰¤ Ïƒsp
(cid:19)2(cid:33)

N with probability

Let W be a d Ã— k matrix with orthonormal columns, and deï¬ne

W (cid:48) = (I + Î·A)W + Î·N , W (cid:48)(cid:48) = W (cid:48)(W

(cid:48)(cid:62)W (cid:48))âˆ’1/2,

for some Î· âˆˆ

(cid:104)

0,

1
4 max{1,ÏƒF

N }

(cid:105)

.

k W (cid:48)(cid:48)(cid:107)2
F
F â‰¥ k âˆ’ 1

2 , then

â€¢ If (cid:107)V (cid:62)

k W (cid:107)2

If Vk = [v1, v2 . . . , vk] is the d Ã— k matrix of Aâ€™s ï¬rst k eigenvectors, then the following holds:
â€¢ E (cid:2)1 âˆ’ (cid:107)V (cid:62)

(cid:1) (cid:0)1 âˆ’ (cid:107)V (cid:62)

(cid:3) â‰¤ (cid:0)1 âˆ’ 4

(cid:1) + Î·2rN

5 Î·Î»(cid:107)V (cid:62)

k W (cid:107)2
F

k W (cid:107)2
F

(cid:104)

EN

k âˆ’ (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:16)

(cid:105)

â‰¤

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17) (cid:18)

1 âˆ’

(cid:19)

Î·Î»

1
10

+ Î·2rN .

Proof. Using the fact that Tr(BCD) = Tr(CDB) for any matrices B, C, D, we have

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

= E

= E

= E

(cid:104)

Tr
(cid:20)

Tr

(cid:20)

Tr

By deï¬nition of W (cid:48), we have

(cid:48)(cid:48)(cid:17)(cid:105)

(cid:16)

W
(cid:18)(cid:16)

(cid:48)(cid:48)(cid:62)VkV (cid:62)
k W
(cid:48)(cid:62)W (cid:48)(cid:17)âˆ’1/2

W

(cid:18)(cid:16)

(cid:48)(cid:62)VkV (cid:62)

k W (cid:48)(cid:17) (cid:16)
W

W

W

(cid:48)(cid:62)VkV (cid:62)

W

k W (cid:48) (cid:16)
(cid:48)(cid:62)W (cid:48)(cid:17)âˆ’1(cid:19)(cid:21)

(cid:48)(cid:62)W (cid:48)(cid:17)âˆ’1/2(cid:19)(cid:21)

.

(11)

W

where we deï¬ne

Also, we have

where

(cid:48)(cid:62)VkV (cid:62)

k W (cid:48) = ((I + Î·A)W + Î·N )(cid:62) VkV (cid:62)

k ((I + Î·A)W + Î·N )

= B1 + Z1,

B1 = W (cid:62)(I + Î·A)VkV (cid:62)
Z1 = Î·N (cid:62)VkV (cid:62)

k (I + Î·A)W + Î·2N (cid:62)VkV (cid:62)
k (I + Î·A)W + Î·W (cid:62)(I + Î·A)VkV (cid:62)

k N
k N.

W

(cid:48)(cid:62)W (cid:48) = ((I + Î·A)W + Î·N )(cid:62) ((I + Î·A)W + Î·N )

= B2 + Z2,

B2 = W (cid:62)(I + Î·A)(I + Î·A)W + Î·2N (cid:62)N
Z2 = Î·N (cid:62)(I + Î·A)W + Î·W (cid:62)(I + Î·A)N.
With these deï¬nitions, we can rewrite Eq. (11) as E (cid:2)Tr((B1 + Z1)(B2 + Z2)âˆ’1)(cid:3). We now wish to remove
Z1, Z2, by applying Lemma 5. To do so, we check the lemmaâ€™s conditions:

â€¢ Z1, Z2 are zero mean: This holds since they are linear in N , and N is assumed to be zero-mean.

18

â€¢ B2 + Î½Z2 (cid:23) 3

8 I for all Î½ âˆˆ [0, 1]: Recalling the deï¬nition of B2, Z2, and the facts that A (cid:23) 0,
N (cid:62)N (cid:23) 0 (by construction), and W (cid:62)W = I, we have that B2 (cid:23) I. Moreover, the spectral norm of
Z2 is at most

2Î·(cid:107)N (cid:62)(I + Î·A)W (cid:107)sp â‰¤ 2Î·(cid:107)N (cid:107)sp(cid:107)I + Î·A(cid:107)sp(cid:107)W (cid:107)sp â‰¤ 2Î·Ïƒsp

N (1 + Î·) â‰¤ 2Î·ÏƒF

N (1 + Î·),

which by the assumption on Î· is at most 2 1
4
of B2 + Î½Z2 is at least 1 âˆ’ Î½(5/8) â‰¥ 3/8.

(cid:0)1 + 1
4

(cid:1) = 5

8 . This implies that the smallest singular value

â€¢ max{(cid:107)Z1(cid:107)F , (cid:107)Z2(cid:107)F } â‰¤ 5

2 Î·ÏƒF

these two matrices is at most

N : By deï¬nition of Z1, Z2, and using Lemma 4, the Frobenius norm of

2Î·(cid:107)N (cid:107)F (cid:107)(I + Î·A)(cid:107)sp(cid:107)W (cid:107)sp â‰¤ 2Î·ÏƒF

N (1 + Î·),

which by the assumption on Î· is at most 2Î·ÏƒF
N

(cid:0)1 + 1
4

(cid:1) = 5

2 Î·ÏƒF
N .

â€¢ (cid:107)B1 + Î·Z1(cid:107)sp â‰¤ (cid:0) 1

4 Ïƒsp

N + 2(cid:1)2

: Using the deï¬nition of B1, Z1 and the assumption Î· â‰¤ 1
4 ,

(cid:107)B1 + Î·Z1(cid:107)sp â‰¤ (cid:107)B1(cid:107)sp + Î·(cid:107)Z1(cid:107)sp
â‰¤ (1 + Î·)2 + Î·2(Ïƒsp

N (1 + Î·)

N )2 + 2Î·Ïƒsp
5
8

Ïƒsp
N

N )2 +

(Ïƒsp

â‰¤

<

(cid:18) 5
4
(cid:18) 1
4

(cid:19)2

+

1
16

Ïƒsp
N + 2

(cid:19)2

.

Applying Lemma 5 and plugging back to Eq. (11), we get

(cid:104)

E

(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ E (cid:2)Tr((B1 + Z1)(B2 + Z2)âˆ’1)(cid:3)

â‰¥ Tr (cid:0)B1Bâˆ’1

2

(cid:1) âˆ’

(cid:32)

(Î·ÏƒF

N )2

1 +

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

Ïƒsp
N + 2

400
9

(12)

We now turn to lower bound Tr (cid:0)B1Bâˆ’1
let

2

(cid:1), by ï¬rst re-writing B1, B2 in a different form. For i = 1, . . . , d,

Ui = W (cid:62)viv(cid:62)

i W,

where vi is the eigenvector of A corresponding to the eigenvalue si. Note that each Ui is positive semideï¬-
nite, and (cid:80)d

i=1 Ui = W (cid:62)W = I. We have

B1 = W (cid:62)(I + Î·A)VkV (cid:62)

k (I + Î·A)W + Î·2N (cid:62)VkV (cid:62)

k N

= W (cid:62) ((I + Î·A)Vk) ((I + Î·A)Vk)(cid:62) W + Î·2N (cid:62)VkV (cid:62)

k N

k
(cid:88)

(1 + Î·si)2W (cid:62)viv(cid:62)

i W + Î·2N (cid:62)VkV (cid:62)

k N

i=1
k
(cid:88)

(1 + Î·si)2Ui + Î·2N (cid:62)VkV (cid:62)

k N.

=

=

i=1

19

(13)

Similarly,

B2 = W (cid:62)(I + Î·A)(I + Î·A)W + Î·2N (cid:62)N

=

=

d
(cid:88)

i=1
d
(cid:88)

i=1

(1 + Î·si)2W (cid:62)viv(cid:62)

i W + Î·2N (cid:62)N

(1 + Î·si)2Ui + Î·2N (cid:62)N.

(14)

Plugging Eq. (13) and Eq. (14) back into Eq. (12), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ Tr

ï£­

ï£«

(cid:32) k

(cid:88)

(1 + Î·s1)2Ui + Î·2N (cid:62)VkV (cid:62)

k N

i=1

âˆ’

400
9

(cid:32)

(Î·ÏƒF

N )2

1 +

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

Ïƒsp
N + 2

(1 + Î·si)2Ui + Î·2N (cid:62)N

(cid:33)âˆ’1ï£¶
ï£¸

(cid:33) (cid:32) d

(cid:88)

i=1

(15)

Recalling that s1 â‰¥ s2 â‰¥ . . . â‰¥ sk and letting Î± = (1 + Î·sk)2, Î² = (1 + Î·s1)2, the trace term can be

lower bounded by

ï£«

min
x1,...,xkâˆˆ[Î±,Î²]

Tr

ï£­

(cid:32) k

(cid:88)

i=1

xiUi + Î·2N (cid:62)VkV (cid:62)

k N

(cid:33) (cid:32) k

(cid:88)

d
(cid:88)

xiUi +

(1 + Î·si)2Ui + Î·2N (cid:62)N

(cid:33)âˆ’1ï£¶
ï£¸ .

i=1

i=k+1

Applying Lemma 6 (noting that as required by the lemma, (cid:80)d
(cid:80)d

i=k+1(1 + Î·si)2Ui + Î·2N (cid:62) (cid:0)I âˆ’ VkV (cid:62)
ï£«

k

(cid:33) (cid:32)

(cid:32)

Tr

ï£­

(1 + Î·sk)2

Ui + Î·2N (cid:62)VkV (cid:62)

k N

(1 + Î·sk)2

k
(cid:88)

i=1

Using Lemma 2, this can be lower bounded by

i=k+1(1+Î·si)2Ui+Î·2N (cid:62)N âˆ’Î·2N (cid:62)VkV (cid:62)

k N =

(cid:1) N (cid:23) 0), we can lower bound the above by

d
(cid:88)

Ui +

(1 + Î·si)2Ui + Î·2N (cid:62)N

(cid:33)âˆ’1ï£¶
ï£¸ .

i=k+1

k
(cid:88)

i=1

ï£«

(cid:32)

Tr

ï£­

(1 + Î·sk)2

(cid:33) (cid:32)

(1 + Î·sk)2

k
(cid:88)

i=1

Ui

k
(cid:88)

i=1

d
(cid:88)

Ui +

(1 + Î·si)2Ui + Î·2N (cid:62)N

(cid:33)âˆ’1ï£¶
ï£¸

i=k+1

ï£«

(cid:32) k

(cid:88)

= Tr

ï£­

(cid:33) (cid:32) k

(cid:88)

Ui

Ui +

d
(cid:88)

i=1

i=1

i=k+1

Applying Lemma 3, this is at least

(cid:19)2

(cid:18) 1 + Î·si
1 + Î·sk

Ui +

(cid:18) Î·

(cid:19)2

1 + Î·sk

(cid:33)âˆ’1ï£¶
ï£¸

N (cid:62)N

(cid:32)(cid:32) k

(cid:88)

Tr

i=1

(cid:33) (cid:32)

Ui

2I âˆ’

k
(cid:88)

i=1

Ui âˆ’

d
(cid:88)

i=k+1

(cid:19)2

(cid:18) 1 + Î·si
1 + Î·sk

Ui âˆ’

(cid:18) Î·

(cid:19)2

1 + Î·sk

(cid:33)(cid:33)

N (cid:62)N

.

20

Recalling that I = (cid:80)d

i=1 Ui = (cid:80)k
(cid:33) (cid:32) k

i=1 Ui + (cid:80)d
(cid:32)

(cid:88)

Ui +

d
(cid:88)

(cid:32)(cid:32) k

(cid:88)

Tr

Ui

i=1

i=1

i=k+1

i=k+1 Ui, this can be simpliï¬ed to

2 âˆ’

(cid:19)2(cid:33)

(cid:18) 1 + Î·si
1 + Î·sk

Ui âˆ’

(cid:18) Î·

(cid:19)2

1 + Î·sk

(cid:33)(cid:33)

N (cid:62)N

.

(16)

Since Ui (cid:23) 0, then using Lemma 3, we can lower bound the expression above by shrinking each of the
(cid:18)

terms. In particular, since si â‰¤ sk âˆ’ Î» for each i â‰¥ k + 1,

2 âˆ’

(cid:16) 1+Î·si
1+Î·sk

(cid:17)2(cid:19)

2 âˆ’

(cid:19)2

(cid:18) 1 + Î·si
1 + Î·sk

â‰¥ 2 âˆ’

1 + Î·si
1 + Î·sk

â‰¥ 2 âˆ’

1 + Î·(sk âˆ’ Î»)
1 + Î·sk

= 1 +

Î·Î»
1 + Î·sk

,

which by the assumption that Î· â‰¤ 1/4 and sk â‰¤ s1 â‰¤ 1, is at least 1+ 4
and recalling that (cid:80)d

i=1 Ui = I, we get the lower bound

5 Î·Î». Plugging this back into Eq. (16),

(cid:32)(cid:32) k

(cid:88)

Tr

Ui

(cid:33) (cid:32) k

(cid:88)

Ui +

d
(cid:88)

(cid:18)

1 +

(cid:19)

Î·Î»

Ui âˆ’

4
5

(cid:18) Î·

(cid:19)2

1 + Î·sk

N (cid:62)N

(cid:33)(cid:33)

i=1
(cid:32)(cid:32) k

(cid:88)

i=1

i=1
(cid:33) (cid:32)

Ui

I +

i=k+1
(cid:32)

Î·Î»

I âˆ’

4
5

= Tr

Again using Lemma 2, this is at least

(cid:33)

Ui

âˆ’

k
(cid:88)

i=1

(cid:18) Î·

(cid:19)2

1 + Î·sk

(cid:33)(cid:33)

N (cid:62)N

.

(cid:32)(cid:32) k

(cid:88)

Tr

(cid:33) (cid:32)

Ui

I +

(cid:33)(cid:33)(cid:33)

(cid:32)

Î·Î»

I âˆ’

4
5

k
(cid:88)

i=1

Ui

âˆ’

(cid:18) Î·

(cid:19)2

1 + Î·sk

i=1
(cid:32)(cid:32) k

(cid:88)

i=1
(cid:32)(cid:32) k

(cid:88)

i=1

â‰¥ Tr

â‰¥ Tr

(cid:33) (cid:32)

Ui

I +

(cid:33) (cid:32)

Ui

I +

(cid:33)(cid:33)(cid:33)

(cid:33)(cid:33)(cid:33)

(cid:32)

Î·Î»

I âˆ’

(cid:32)

Î·Î»

I âˆ’

4
5

4
5

k
(cid:88)

i=1
k
(cid:88)

i=1

Ui

Ui

âˆ’

(cid:18) Î·

1 + Î·sk

âˆ’ Î·2 (cid:0)ÏƒF
N

(cid:1)2

.

Tr

(cid:19)2

(cid:32)(cid:32) k

(cid:88)

(cid:33)

(cid:33)

Ui

N (cid:62)N

i=1

(cid:16)

Tr

N (cid:62)N

(cid:17)

Recall that this is a lower bound on the trace term in Eq. (15). Plugging it back and slightly simplifying, we
get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ Tr

(cid:32)(cid:32) k

(cid:88)

(cid:33) (cid:32)

Ui

I +

Î·Î»

I âˆ’

(cid:32)

(cid:33)(cid:33)(cid:33)

âˆ’ Î·2rN ,

4
5

k
(cid:88)

i=1

Ui

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

Ïƒsp
N + 2

where

i=1

(cid:32)

rN = 46 (ÏƒF

N )2

1 +

21

The trace term above can be re-written (using the deï¬nition of Ui and the fact that Tr(B(cid:62)B) = (cid:107)B(cid:107)2

F ) as

(cid:32)(cid:32)

Tr

W (cid:62)

(cid:18)

=

1 +

(cid:18)

=

1 +

4
5
4
5

k
(cid:88)

i=1
(cid:19)

(cid:19)

Î·Î»

= (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:33) (cid:32)

viv(cid:62)

i W

I +

(cid:32)

Î·Î»

I âˆ’ W (cid:62)

4
5

k
(cid:88)

i=1

viv(cid:62)

i W

(cid:33)(cid:33)(cid:33)

Î·Î»

Tr

(cid:16)

W (cid:62)VkV (cid:62)

k W

(cid:17)

âˆ’

4
5

(cid:16)(cid:16)

Î·Î» Tr

W (cid:62)VkV (cid:62)

k W

(cid:17) (cid:16)

W (cid:62)VkV (cid:62)

k W

(cid:17)(cid:17)

4
5

(cid:107)V (cid:62)

F âˆ’

k W (cid:107)2
(cid:18)

(cid:18)

1 +

Î·Î»

1 âˆ’

4
5

Î·Î»(cid:107)W (cid:62)VkV (cid:62)

k W (cid:107)2
F

k W (cid:107)2
F

(cid:107)W (cid:62)VkV (cid:62)
k W (cid:107)2
(cid:107)V (cid:62)
F

(cid:19)(cid:19)

.

Applying Lemma 7, and letting Î´ denote the minimal singular value of V (cid:62)

k W , this is lower bounded by

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

4
5

Î·Î» max

(cid:26)

1 âˆ’ (cid:107)V (cid:62)

k W (cid:107)2

F ,

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:27)(cid:19)

.

(cid:16)

Î´2
k

Overall, we get that

(cid:104)

E

(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

4
5

(cid:26)

Î·Î» max

We now consider two options:

1 âˆ’ (cid:107)V (cid:62)

k W (cid:107)2

F ,

(cid:16)

Î´2
k

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:27)(cid:19)

âˆ’ Î·2rN .

(17)

â€¢ Taking the ï¬rst argument of the max term in Eq. (17), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

(cid:16)

Î·Î»

4
5

1 âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:19)

âˆ’ Î·2rN .

Subtracting 1 from both sides and simplifying, we get

E

(cid:104)
1 âˆ’ (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

(cid:18)

â‰¤

1 âˆ’

4
5

Î·Î»(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:19) (cid:16)

1 âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)

+ Î·2rN .

â€¢ Suppose that (cid:107)V (cid:62)

k W (cid:107)2

F â‰¥ k âˆ’ 1

2 . Taking the second argument of the max term in Eq. (17), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

â‰¥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

(cid:16)

4Î·Î»Î´2
5k

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:19)

âˆ’ Î·2rN .

Subtracting both sides from k, , we get

E

(cid:104)
k âˆ’ (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

(cid:16)

(cid:16)

(cid:16)

â‰¤

=

â‰¤

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:16)

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)

+ Î·2rN

4Î·Î»Î´2
5k
4Î·Î»Î´2
5k

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:19)

+ Î·2rN

(cid:19)(cid:19)

(cid:18)

k âˆ’

1
2

+ Î·2rN

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

4Î·Î»Î´2
5k

(cid:17)

âˆ’
(cid:17) (cid:18)

1 âˆ’

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17) (cid:18)

1 âˆ’

22

Since k â‰¥ 1, we can lower bound the (cid:0)k âˆ’ 1
2
k W satisfy k âˆ’ (cid:80)k
implies that the singular values Ïƒ1, . . . , Ïƒk of V (cid:62)
(as Vk, W have orthonormal columns), so no Ïƒi can be less than 1
2 and Î´ â‰¥ 1
the lower bounds k âˆ’ 1
2 into the above, we get
(cid:17) (cid:18)
(cid:105)

(cid:1) term by k

2 â‰¥ k

(cid:16)

(cid:104)
k âˆ’ (cid:107)V (cid:62)

E

k W (cid:48)(cid:48)(cid:107)2
F

â‰¤

k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

2 . Moreover, the condition k âˆ’ (cid:107)V (cid:62)

k W (cid:107)2

F â‰¤ 1
2
2 . But each Ïƒi is in [0, 1]
2 . Plugging

2 . This implies that Î´ â‰¥ 1

i=1 Ïƒ2

i â‰¤ 1

1 âˆ’

1
10

(cid:19)

Î·Î»

+ Î·2rN .

Lemma 11. Let A, Wt be as deï¬ned in Algorithm 2, and suppose that Î· âˆˆ
holds for some positive numerical constants c1, c2, c3:

(cid:104)
0,

(cid:105)

1
âˆš
23

k

. Then the following

â€¢ E (cid:2)1 âˆ’ (cid:107)V (cid:62)

(cid:3) â‰¤ (cid:0)1 âˆ’ c1Î·Î»(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:1) (cid:0)1 âˆ’ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:1) + c2kÎ·2

k W (cid:48)(cid:48)(cid:107)2
F
F â‰¥ k âˆ’ 1

k Wt+1(cid:107)2
F

2 , then
(cid:105)

â‰¤

â€¢ If (cid:107)V (cid:62)

k Wt(cid:107)2
(cid:104)
k âˆ’ (cid:107)V (cid:62)

E

(cid:16)

k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2
F

(cid:17)

(1 âˆ’ c1Î· (Î» âˆ’ c2Î·)) + c3Î·2(k âˆ’ (cid:107)V (cid:62)
k

ËœWsâˆ’1(cid:107)2

F ).

In the above, the expectation is over the random draw of the index it, conditioned on Wt and ËœWsâˆ’1.
Proof. To apply Lemma 10, we need to compute upper bounds ÏƒF
norms of N , which in our case equals (xitx(cid:62)
it
Wt, ËœWsâˆ’1, Bt have orthonormal columns, the spectral norm of N is at most

N and Ïƒsp
âˆ’ A)(Wt âˆ’ ËœWsâˆ’1Bt). Since (cid:107)A(cid:107)sp, (cid:107)xitx(cid:62)
it

N on the Frobenius and spectral
(cid:107)sp â‰¤ 1, and

(cid:107)(xitx(cid:62)

it âˆ’ A)(Wt âˆ’ ËœWsâˆ’1Bt)(cid:107)sp â‰¤

(cid:16)

(cid:107)xitx(cid:62)

it (cid:107)sp + (cid:107)A(cid:107)sp

(cid:17) (cid:16)

(cid:107)Wt(cid:107)sp + (cid:107) ËœWsâˆ’1(cid:107)sp(cid:107)Bt(cid:107)sp

(cid:17)

â‰¤ 4,

so we may take Ïƒsp

N = 4. As to the Frobenius norm, using Lemma 4 and a similar calculation, we have

To upper bound this, deï¬ne

(cid:107)N (cid:107)2

F â‰¤ 4(cid:107)Wt âˆ’ ËœWsâˆ’1Bt(cid:107)2
F .

VWt = arg

min
VkB:(VkB)(cid:62)(VkB)=I

(cid:107)Wt âˆ’ VkB(cid:107)2
F

to be the nearest orthonormal-columns matrix to Wt in the column space of Vk, and
(cid:107)VWt âˆ’ ËœWsâˆ’1B(cid:107)2
F

ËœWV = arg

min
ËœWsâˆ’1B:( ËœWsâˆ’1B)(cid:62)( ËœWsâˆ’1B)=I

to be the nearest orthonormal-columns matrix to VWt in the column space of ËœWsâˆ’1. Also, recall that by
deï¬nition,

ËœWsâˆ’1Bt = arg

(cid:107)Wt âˆ’ ËœWsâˆ’1B(cid:107)2
F

min
ËœWsâˆ’1B:( ËœWsâˆ’1B)(cid:62)( ËœWsâˆ’1B)=I
is the nearest orthonormal-columns matrix to Wt in the column space of ËœWsâˆ’1. Therefore, we must have
(cid:107)Wt âˆ’ ËœWsâˆ’1Bt(cid:107)2
F . Using this and Lemma 8, we have
F â‰¤ (cid:107)Wt âˆ’ ËœWV (cid:107)2
F
= (cid:107)(Wt âˆ’ VWt) âˆ’ ( ËœWV âˆ’ VWt)(cid:107)2
F
â‰¤ 2(cid:107)Wt âˆ’ VWt(cid:107)2
k âˆ’ (cid:107)V (cid:62)

F â‰¤ (cid:107)Wt âˆ’ ËœWV (cid:107)2
(cid:107)Wt âˆ’ ËœWsâˆ’1Bt(cid:107)2

F + 2(cid:107) ËœWV âˆ’ VWt(cid:107)2
F
(cid:16)
k âˆ’ (cid:107)V (cid:62)
Wt

ËœWsâˆ’1(cid:107)2
F

k Wt(cid:107)2
F

= 4

+ 4

(cid:17)

(cid:16)

(cid:17)

.

23

By deï¬nition of VWt, we have VWt = VkB where B(cid:62)B = B(cid:62)V (cid:62)
is an orthogonal k Ã— k matrix, and (cid:107)V (cid:62)
Wt
ËœWsâˆ’1(cid:107)2
4(k âˆ’ (cid:107)V (cid:62)
k Wt(cid:107)2
upper bounded by

k VkB = (VkB)(cid:62)(VkB) = I. Therefore B
ËœWsâˆ’1(cid:107)2
F = (cid:107)V (cid:62)
F , so the above equals
k
F ). Overall, we get that the squared Frobenius norm of N can be

F ) + 4(k âˆ’ (cid:107)V (cid:62)
k

F = (cid:107)B(cid:62)V (cid:62)
k

ËœWsâˆ’1(cid:107)2

ËœWsâˆ’1(cid:107)2

(ÏƒF

N )2 = 16

(k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k âˆ’ (cid:107)V (cid:62)
k

(cid:16)

ËœWsâˆ’1(cid:107)2
F )

(cid:17)

.

Plugging Ïƒsp

N and (ÏƒF

N )2 into the rN as deï¬ned in Lemma 10, and picking any Î· âˆˆ [0,

1
âˆš
23

k

] (which satisï¬es

âˆš

the condition in Lemma 10 that Î· âˆˆ

, since 4 max{1, ÏƒF

n } â‰¤ 4 max{1,

16 âˆ— 2k} <

(cid:104)

0,

1
4 max{1,ÏƒF

N }

(cid:105)

âˆš

23

k), we get

(cid:16)

rN = 736

(k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k âˆ’ (cid:107)V (cid:62)
k

(cid:17)
ËœWsâˆ’1(cid:107)2
F )

(cid:32)

1 +

(cid:18) 1
4

8
3

4 + 2

(cid:19)2(cid:33)

(cid:16)

â‰¤ 18400

(k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k âˆ’ (cid:107)V (cid:62)
k

(cid:17)
ËœWsâˆ’1(cid:107)2
F )

.

This implies that rN â‰¤ 36800k always, which by application of Lemma 10, gives the ï¬rst part of our lemma.
As to the second part, assuming (cid:107)V (cid:62)

k Wt(cid:107)2

F â‰¥ k âˆ’ 1

E

(cid:104)
k âˆ’ (cid:107)V (cid:62)

k Wt+1(cid:107)2
F

(cid:16)

(cid:105)

â‰¤

(cid:16)

=

k Wt(cid:107)2
k âˆ’ (cid:107)V (cid:62)
F
+ 18400 Î·2 (cid:16)

k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2
F

2 and applying Lemma 10, we get that
(cid:19)

(cid:17) (cid:18)

1 âˆ’

(k âˆ’ (cid:107)V (cid:62)
(cid:17) (cid:18)

1 âˆ’ Î·

F ) + (k âˆ’ (cid:107)V (cid:62)
k
(cid:19)(cid:19)

Î» âˆ’ 18400Î·

(cid:17)

ËœWsâˆ’1(cid:107)2
F )

Î·Î»

1
10
k Wt(cid:107)2
(cid:18) 1
10
ËœWsâˆ’1(cid:107)2

+ 18400 Î·2(k âˆ’ (cid:107)V (cid:62)
k

F ).

This corresponds to the lemma statement.

Part II: Solving the Recurrence Relation for a Single Epoch
Since we focus on a single epoch, we drop the subscript from ËœWsâˆ’1 and denote it simply as ËœW .
Suppose that Î· = Î±Î», where Î± is a sufï¬ciently small constant to be chosen later. Also, let

bt = k âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2

F and Ëœb = k âˆ’ (cid:107)V (cid:62)
k

ËœW (cid:107)2
F .

Then Lemma 11 tells us that if Î± is a sufï¬ciently small constant, bt â‰¤ 1

2 , then

E [bt+1|Wt] â‰¤ (cid:0)1 âˆ’ cÎ±Î»2(cid:1) bt + c(cid:48)Î±2Î»2Ëœb

(18)

for some numerical constants c, c(cid:48).

Lemma 12. Let B be the event that bt â‰¤ 1
constants c1, c2, c3, if Î± â‰¤ c1, then

2 for all t = 0, 1, 2, . . . , m. Then for certain positive numerical

E[bm|B] â‰¤ (cid:0)(cid:0)1 âˆ’ c2Î±Î»2(cid:1)m

+ c3Î±(cid:1) Ëœb,

where the expectation is over the randomness in the current epoch.

24

Proof. Recall that bt is a deterministic function of the random variable Wt, which depends in turn on Wtâˆ’1
and the random instance chosen at round t. We assume that W0 (and hence Ëœb) are ï¬xed, and consider how
bt evolves as a function of t. Using Eq. (18), we have

E[bt+1|Wt, B] = E

(cid:20)

bt+1|Wt, bt+1 â‰¤

(cid:21)

1
2

â‰¤ E[bt+1|Wt] â‰¤ (cid:0)1 âˆ’ cÎ±Î»2(cid:1) bt + c(cid:48)Î±2Î»2Ëœb.

Note that the ï¬rst equality holds, since conditioned on Wt, bt+1 is independent of b1, . . . , bt, so the event B
is equivalent to just requiring bt+1 â‰¤ 1/2.

Taking expectation over Wt (conditioned on B), we get that

E[bt+1|B] â‰¤ E

(cid:12)
(cid:104)(cid:0)1 âˆ’ cÎ±Î»2(cid:1) bt + c(cid:48)Î±2Î»2Ëœb
(cid:12)
(cid:12)B
= (cid:0)1 âˆ’ cÎ±Î»2(cid:1) E [bt|B] + c(cid:48)Î±2Î»2Ëœb.

(cid:105)

Unwinding the recursion, and using that b0 = Ëœb, we therefore get that

E[bm|B] â‰¤ (cid:0)1 âˆ’ cÎ±Î»2(cid:1)m Ëœb + c(cid:48)Î±2Î»2Ëœb

â‰¤ (cid:0)1 âˆ’ cÎ±Î»2(cid:1)m Ëœb + c(cid:48)Î±2Î»2Ëœb

= (cid:0)1 âˆ’ cÎ±Î»2(cid:1)m Ëœb + c(cid:48)Î±2Î»2Ëœb

(cid:18)

=

(cid:0)1 âˆ’ cÎ±Î»2(cid:1)m

+

(cid:19)

c(cid:48)
c

Î±

Ëœb.

mâˆ’1
(cid:88)

(cid:0)1 âˆ’ cÎ±Î»2(cid:1)i

i=0
âˆ
(cid:88)

(cid:0)1 âˆ’ cÎ±Î»2(cid:1)i

i=0
1
cÎ±Î»2

as required.

We now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:

Lemma 13. The following holds for certain positive numerical constants c1, c2, c3: If Î± â‰¤ c1, then for any
Î² âˆˆ (0, 1) and m, if

Ëœb + c2kmÎ±2Î»2 + c3k

(cid:112)

mÎ±2Î»2 log(1/Î²) â‰¤

then it holds with probability at least 1 âˆ’ Î² that

1
2

,

(19)

bt â‰¤ Ëœb + c2kmÎ±2Î»2 + c3k

(cid:112)

mÎ±2Î»2 log(1/Î²) â‰¤

1
2

for all t = 0, 1, 2, . . . , m.
Proof. To prove the lemma, we analyze the stochastic process b0(= Ëœb), b1, b2, . . . , bm, and use a concentra-
tion of measure argument. First, we collect the following facts:

â€¢ Ëœb = b0 â‰¤ 1

2 : This directly follows from the assumption stated in the lemma.

â€¢ As long as bt â‰¤ 1

2 , E [bt+1|Wt] â‰¤ bt + c2Î±2Î»2Ëœb for some constant c2: Supposing Î± is sufï¬ciently

small, then by Eq. (18),

E [bt+1|Wt] â‰¤ (cid:0)1 âˆ’ cÎ±Î»2(cid:1) bt + c(cid:48)Î±2Î»2Ëœb â‰¤ bt + c(cid:48)Î±2Î»2Ëœb.

25

â€¢ |bt+1 âˆ’ bt| is bounded by c(cid:48)

3kÎ±Î» for some constant c(cid:48)
most some sufï¬ciently small constant c1 (e.g. Î± â‰¤ 1

3: Applying Lemma 9, and assuming that Î± is at
12 , so Î· = Î±Î» â‰¤ 1

12 ),

|bt+1 âˆ’ bt| =

(cid:12)
(cid:12)(cid:107)V (cid:62)
(cid:12)

k Wt+1(cid:107)2

F âˆ’ (cid:107)V (cid:62)

k Wt(cid:107)2
F

(cid:12)
(cid:12)
(cid:12) â‰¤

12kÎ·
1 âˆ’ 3Î·

â‰¤

12kÎ±Î»
3/4

= 16kÎ±Î».

Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows
that with probability at least 1âˆ’Î², it holds simultaneously for all t = 1, . . . , m (and for t = 0 by assumption)
that

bt â‰¤ Ëœb + c2mÎ±2Î»2Ëœb + c3k

(cid:112)

mÎ±2Î»2 log(1/Î²)

for some constants c2, c3, as long as the expression above is less than 1
2 , then we get that bt â‰¤ 1
1
the lemma.

2 . If the expression is indeed less than
2 for all t. Upper bounding Ëœb by k and slightly simplifying, we get the statement in

Combining Lemma 12 and Lemma 13, and using Markovâ€™s inequality, we get the following corollary:

Lemma 14. Let conï¬dence parameters Î², Î³ âˆˆ (0, 1) be ï¬xed. Suppose that m, Î± are chosen such that
Î± â‰¤ c1 and

Ëœb + c2kmÎ±2Î»2 + c3k

(cid:112)

mÎ±2Î»2 log(1/Î²) â‰¤

1
2

,

where c1, c2, c3 are certain positive numerical constants. Then with probability at least 1 âˆ’ (Î² + Î³), it holds
that

for some positive numerical constants c, c(cid:48).

bm â‰¤

(cid:0)(cid:0)1 âˆ’ cÎ±Î»2(cid:1)m

+ c(cid:48)Î±(cid:1) Ëœb.

1
Î³

Part III: Analyzing the Entire Algorithmâ€™s Run

Given the analysis in Lemma 14 for a single epoch, we are now ready to prove our theorem. Let

By assumption, at the beginning of the ï¬rst epoch, we have Ëœb0 = k âˆ’ (cid:107)V (cid:62)
k
Lemma 14, for any Î², Î³ âˆˆ (cid:0)0, 1
(cid:1), if we pick any
2

Ëœbs = k âˆ’ (cid:107)V (cid:62)
k

ËœWs(cid:107)2
F .

ËœW0(cid:107)2

F â‰¤ 1

2 . Therefore, by

(cid:26)

Î± â‰¤ min

c1,

(cid:27)

1
2c(cid:48) Î³2

and m â‰¥

3 log(1/Î³)
cÎ±Î»2

such that

1
2

+c2kmÎ±2Î»2 +c3k

then we get with probability at least 1 âˆ’ (Î² + Î³) that

bm â‰¤

1
Î³

(cid:18)
(cid:0)1 âˆ’ cÎ±Î»2(cid:1) 3 log(1/Î³)

cÎ±Î»2 +

(cid:19)

Ëœb0

Î³2

1
2

(cid:112)

mÎ±2Î»2 log(1/Î²) â‰¤

,

1
2
(20)

Using the inequality (1 âˆ’ (1/x))ax â‰¤ exp(âˆ’a), which holds for any x > 1 and any a, and taking x =
1/(cÎ±Î»2) and a = 3 log(1/Î³), we can upper bound the above by

(cid:18)

(cid:18)

exp

âˆ’3 log

1
Î³

=

(cid:18)

1
Î³

Î³3 +

1
2

(cid:18) 1
Î³
(cid:19)

Î³2

Ëœb0 â‰¤ Î³Ëœb0.

(cid:19)(cid:19)

+

(cid:19)

Ëœb0

Î³2

1
2

26

Since bm equals the starting point Ëœb1 for the next epoch, we get that Ëœb1 â‰¤ Î³Ëœb0 â‰¤ Î³ 1
2 . Again applying
Lemma 14, and performing the same calculation we have that with probability at least 1 âˆ’ (Î² + Î³) over the
next epoch, Ëœb2 â‰¤ Î³Ëœb1 â‰¤ Î³2Ëœb0. Repeatedly applying Lemma 14 and using a union bound, we get that after
T epochs, with probability at least 1 âˆ’ T (Î² + Î³),

k âˆ’ (cid:107)V (cid:62)
k

ËœWT (cid:107)2

F = ËœbT â‰¤ Î³T Ëœb0 < Î³T .

Therefore, for any desired accuracy parameter (cid:15), we simply need to use T =

k âˆ’ (cid:107)V (cid:62)
k

ËœWs(cid:107)2

F â‰¤ (cid:15) with probability at least 1 âˆ’ T (Î² + Î³) = 1 âˆ’

(cid:108) log(1/(cid:15))
log(1/Î³)

(cid:109)

(Î² + Î³).

(cid:108) log(1/(cid:15))
log(1/Î³)

(cid:109)

epochs, and get

Using a conï¬dence parameter Î´, we pick Î² = Î³ = Î´

2 , which ensures that the accuracy bound above

holds with probability at least

1 âˆ’

(cid:25)

(cid:24) log(1/(cid:15))
log(2/Î´)

Î´ â‰¥ 1 âˆ’

(cid:25)

(cid:24) log(1/(cid:15))
log(2)

Î´ = 1 âˆ’

(cid:24)
log2

(cid:18) 1
(cid:15)

(cid:19)(cid:25)

Î´.

Substituting this choice of Î², Î³ into Eq. (20), and recalling that the step size Î· equals Î±Î», we get that
k âˆ’ (cid:107)V (cid:62)
k

F â‰¤ (cid:15) with probability at least 1 âˆ’ (cid:100)log2(1/(cid:15))(cid:101)Î´, provided that

ËœWT (cid:107)2

Î· â‰¤ cÎ´2Î» , m â‰¥

c(cid:48) log(2/Î´)
Î·Î»

,

kmÎ·2 + k

(cid:112)

mÎ·2 log(2/Î´) â‰¤ c(cid:48)(cid:48)

for suitable positive constants c, c(cid:48), c(cid:48)(cid:48).

To get the theorem statement, recall that the analysis we performed pertains to data whose squared norm
is bounded by 1. By the reduction discussed at the beginning of the proof, we can apply it to data with
squared norm at most r, by replacing Î» with Î»/r, and Î· with Î·r, leading to the condition

Î· â‰¤

cÎ´2
r2 Î» , m â‰¥

c(cid:48) log(2/Î´)
Î·Î»

,

kmÎ·2r2 + rk

(cid:112)

mÎ·2 log(2/Î´) â‰¤ c(cid:48)(cid:48)

and establishing the theorem.

6.2 Proof of Theorem 2

The proof relies mainly on the techniques and lemmas of Section 6.1, used to prove Theorem 1. As done in
Section 6.1, we will assume without loss of generality that r = maxi (cid:107)xi(cid:107)2 is at most 1, and then transform
the bound to a bound for general r (see the discussion at the beginning of Subsection 6.1.2)

First, we extract the following result, which is essentially the ï¬rst part of Lemma 11 (for k = 1):

Lemma 15. Let A, wt be as deï¬ned in Algorithm 1, and suppose that Î· âˆˆ (cid:2)0, 1

23

(cid:3). Then

Eit

(cid:2)1 âˆ’ (cid:104)v1, wt+1(cid:105)2(cid:12)

(cid:12)wt, Ëœwsâˆ’1

(cid:3) â‰¤ (cid:0)1 âˆ’ cÎ·Î»(cid:104)v1, wt(cid:105)2(cid:1) (cid:0)1 âˆ’ (cid:104)v1, wt(cid:105)2(cid:1) + c(cid:48)Î·2,

for some positive numerical constants c, c(cid:48).

Note that this bound holds regardless of what is Ëœwsâˆ’1, and in particular holds across different epochs
of Algorithm 1. Therefore, it is enough to show that starting from some initial point w0, after sufï¬ciently
many stochastic updates as speciï¬ed in line 6-10 of the algorithm (or in terms of the analysis, sufï¬ciently
many applications of Lemma 15), we end up with a point wT for which 1 âˆ’ (cid:104)v1, wT (cid:105) â‰¤ 1
2 , as required.

27

Note that to simplify the notation, we will use here a single running index w0, w1, w2, . . . , wT (whereas in
the algorithm we restarted the indexing after every epoch).

The proof is based on martingale arguments, quite similar to the ones in Subsection 6.1.2 but with slight

changes. First, we let

bt = 1 âˆ’ (cid:104)v1, wt(cid:105)2
to simplify notation. We note that b0 = 1 âˆ’ (cid:104)v1, w0(cid:105)2 is assumed ï¬xed, whereas b1, b2, . . . are random
variables based on the sampling process. Lemma 11 tells us that if Î· is sufï¬ciently small, and bt â‰¤ 1 âˆ’ Î¾ for
some Î¾ âˆˆ (0, 1), then

E [bt+1|bt] â‰¤ (1 âˆ’ cÎ·Î»Î¾) bt + c(cid:48)Î·2.

(21)

for some numerical constants c, c(cid:48).

Lemma 16. Let B be the event that bt â‰¤ 1 âˆ’ Î¾ for all t = 0, 1, . . . , T . Then for certain positive numerical
constants c1, c2, c3, if Î· â‰¤ c1Î», then

E[bT |B] â‰¤

(cid:18)

(1 âˆ’ c2Î·Î»Î¾)T + c3

(cid:19)

.

Î·
Î»Î¾

Proof. Using Eq. (21), we have for any bt satisfying event B that

E[bt+1|bt, B] = E [bt+1|bt, bt+1 â‰¤ 1 âˆ’ Î¾] â‰¤ E[bt+1|bt] â‰¤ (1 âˆ’ cÎ·Î»Î¾) bt + c(cid:48)Î·2.

Taking expectation over bt (conditioned on B), we get that

E[bt+1|B] â‰¤ E (cid:2)(1 âˆ’ cÎ·Î»Î¾) bt + c(cid:48)Î·2(cid:12)

(cid:12)B(cid:3)
= (1 âˆ’ cÎ·Î»Î¾) E [bt|B] + c(cid:48)Î·2.

Unwinding the recursion, we get

E[bT |B] â‰¤ (1 âˆ’ cÎ·Î»Î¾)T b0 + c(cid:48)Î·2

T âˆ’1
(cid:88)

i=0

(1 âˆ’ cÎ·Î»Î¾)i

â‰¤ (1 âˆ’ cÎ·Î»Î¾)T + c(cid:48)Î·2

âˆ
(cid:88)

i=0

(1 âˆ’ cÎ·Î»Î¾)i

= (1 âˆ’ cÎ·Î»Î¾)T + c(cid:48)Î·2 1
cÎ·Î»Î¾

â‰¤ (1 âˆ’ cÎ·Î»Î¾)T +

c(cid:48)
c

Î·
Î»Î¾

.

We now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:

Lemma 17. The following holds for certain positive numerical constants c1, c2, c3: If Î· â‰¤ c1Î», then for any
Î² âˆˆ (0, 1), if

T Î·2 log(1/Î²) â‰¤ 1 âˆ’ Î¾,

(22)

b0 + c2T Î·2 + c3
then it holds with probability at least 1 âˆ’ Î² that

(cid:112)

bt â‰¤ b0 + c2T Î·2 + c3

(cid:112)

T Î·2 log(1/Î²) â‰¤ 1 âˆ’ Î¾

for all t = 0, 1, . . . , T .

28

Proof. To prove the lemma, we analyze the stochastic process b1, b2, . . . , bT , and use a concentration of
measure argument. First, we collect the following facts:

â€¢ b0 â‰¤ 1 âˆ’ Î¾: This directly follows from the assumption stated in the lemma.

â€¢ E [bt+1|bt] â‰¤ bt + c(cid:48)Î·2 for some constant c(cid:48): By Eq. (21),

E [bt+1|Wt] â‰¤ (1 âˆ’ cÎ·Î»Î¾) bt + c(cid:48)Î·2 â‰¤ bt + c(cid:48)Î·2.

â€¢ |bt+1 âˆ’bt| is bounded by cÎ· for some constant c: Applying Lemma 9 for the case k = 1, and assuming

Î· â‰¤ 1/12,

|bt+1 âˆ’ bt| = (cid:12)

(cid:12)(cid:104)v1, wt+1(cid:105)2 âˆ’ (cid:104)v, wt(cid:105)2(cid:12)

(cid:12) â‰¤

12Î·
1 âˆ’ 3Î·

â‰¤

12Î·
3/4

= 16Î·.

Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows
that with probability at least 1 âˆ’ Î², it holds simultaneously for all t = 0, 1, . . . , T that

bt â‰¤ b0 + c2T Î·2 + c3

(cid:112)

T Î·2 log(1/Î²)

for some constants c2, c3. If the expression is indeed less than 1 âˆ’ Î¾, then we get that bt â‰¤ 1 âˆ’ Î¾ for all t,
from which the lemma follows.

Combining Lemma 16 and Lemma 17, and using Markovâ€™s inequality, we get the following corollary:

Lemma 18. Let conï¬dence parameters Î², Î³ âˆˆ (0, 1) be ï¬xed. Then for some positive numerical constants
c1, c2, c3, c, c(cid:48), if Î· â‰¤ c1Î» and

b0 + c2T Î·2 + c3

(cid:112)

T Î·2 log(1/Î²) â‰¤ 1 âˆ’ Î¾,

then with probability at least 1 âˆ’ (Î² + Î³), it holds that

bT â‰¤

(cid:18)

1
Î³

(1 âˆ’ cÎ·Î»Î¾)T + c(cid:48) Î·
Î»Î¾

(cid:19)

.

We are now ready to prove our theorem. By Lemma 18, for any Î², Î³ âˆˆ (cid:0)0, 1
2

(cid:1) and any

(cid:26)

Î· â‰¤ min

c1,

such that

(cid:27)

Î»Î¾

1
2c(cid:48) Î³2
b0 + c2T Î·2 + c3

and T â‰¥

3 log(1/Î³)
cÎ·Î»Î¾

(cid:112)

T Î·2 log(1/Î²) â‰¤ 1 âˆ’ Î¾,

(23)

we get with probability at least 1 âˆ’ (Î² + Î³) that

bT â‰¤

1
Î³

(cid:18)

(1 âˆ’ cÎ·Î»Î¾)

3 log(1/Î³)

cÎ·Î»Î¾ +

(cid:19)

.

Î³2

1
2

Using the inequality (1 âˆ’ (1/x))ax â‰¤ exp(âˆ’a), which holds for any x > 1 and any a, and taking x =
1/(cÎ·Î»Î¾) and a = 3 log(1/Î³), we can upper bound the above by

(cid:18)

(cid:18)

exp

âˆ’3 log

(cid:19)(cid:19)

(cid:18) 1
Î³

(cid:19)

+

Î³2

1
2

=

1
Î³

(cid:18)

Î³3 +

(cid:19)

,

Î³2

1
2

1
Î³

29

and since we assume Î³ < 1
bT â‰¤ 1

2 , and therefore 1 âˆ’ (cid:104)v1, wT (cid:105)2 â‰¤ 1

2 , this is at most 1

2 as required.

2 . Overall, we got that with probability at least 1 âˆ’ Î² âˆ’ Î³,

It remains to show that the parameter choices in Eq. (23) can indeed be satisï¬ed. First, we ï¬x Î¾ = 1
2 Î¶
(where we recall that 0 < Î¶ â‰¤ (cid:104)v1, w0(cid:105)2), which trivially ensures that b0 = 1 âˆ’ (cid:104)v1, w0(cid:105)2 is at most 1 âˆ’ 2Î¾.
Moreover, suppose we pick Î² = Î³ in (0, exp(âˆ’1)), and Î·, T so that

Î· â‰¤

câˆ—Î³2Î»Î¾3
log2(1/Î³)

, T =

(cid:22) 3 log(1/Î³)
c(cid:48)
âˆ—Î·Î»Î¾

(cid:23)

,

(24)

where câˆ—, c(cid:48)
âˆ— are sufï¬ciently small constants so that the bounds on Î·, T in Eq. (23) are satisï¬ed. This implies
that the third bound in Eq. (23) is also satisï¬ed, since by plugging in the values / bounds of T and Î·, and
using the assumptions Î³ = Î² â‰¤ exp(âˆ’1) and Î¾ â‰¤ 1, we have

b0 + c2T Î·2 + c3

(cid:112)

T Î·2 log(1/Î³)

â‰¤ 1 âˆ’ 2Î¾ + c2

â‰¤ 1 âˆ’ 2Î¾ + c2

3 log(1/Î³)
c(cid:48)
âˆ—Î»Î¾

Î· + c3

(cid:115)

3 log(1/Î³)
c(cid:48)
âˆ—Î»Î¾

3câˆ—Î³2Î¾2
c(cid:48)
âˆ— log(1/Î³)

+ c3

(cid:115)

3câˆ—Î³2Î¾2
c(cid:48)
âˆ—

â‰¤ 1 âˆ’ 2Î¾ +

(cid:32)

3c2câˆ—
c(cid:48)
âˆ—

+ c3

(cid:115)

(cid:33)

Î¾,

3câˆ—
c(cid:48)
âˆ—

Î· log(1/Î³)

which is less than 1 âˆ’ Î¾ if we pick câˆ— sufï¬ciently small compared to c(cid:48)
âˆ—.

To summarize, we get that for any Î³ âˆˆ (0, exp(âˆ’1)), by picking Î· as in Eq. (24), we have that after
T iterations (where T is speciï¬ed in Eq. (24)), with probability at least 1 âˆ’ 2Î³, we get wT such that
1 âˆ’ (cid:104)v1, wT (cid:105) â‰¤ 1

2 . Substituting Î´ = 2Î³ and Î¶ = 2Î¾, we get that if

and Î· satisï¬es

(cid:104)v1, Ëœw0(cid:105)2 â‰¥ Î¶ > 0,

Î· â‰¤

c1Î´2Î»Î¶ 3
log2(2/Î´)

(for some universal constant c1), then with probability at least 1 âˆ’ Î´, after

T =

(cid:22) c2 log(2/Î´)
Î·Î»Î¶

(cid:23)

.

stochastic iterations, we get a satisfactory point wT .

As discussed at the beginning of the proof, this analysis is valid assuming r = maxi (cid:107)xi(cid:107)2 â‰¤ 1. By the
reduction discussed at the beginning of Subsection 6.1.2, we can get an analysis for any r by substituting
Î» â†’ Î»/r and Î· â†’ Î·r. This means that we should pick Î· satisfying

Î·r â‰¤

c1Î´2(Î»/r)Î¶ 3
log2(2/Î´)

â‡’ Î· â‰¤

c1Î´2Î»Î¶ 3
r2 log2(2/Î´)

,

and getting the required point after

T =

(cid:22) c2 log(2/Î´)
(Î·r)(Î»/r)Î¶

(cid:23)

=

(cid:22) c2 log(2/Î´)
Î·Î»Î¶

(cid:23)

iterations.

30

6.3 Proof of Theorem 4

For simplicity of notation, we drop the A subscript from FA, and refer simply to F .

We ï¬rst prove the following two auxiliary lemmas:

Lemma 19. If A is a symmetric matrix, then the gradient of the function F (w) = âˆ’ w(cid:62)Aw
equals

(cid:107)w(cid:107)2 at some w

2

âˆ’

(cid:107)w(cid:107)2 (F (w)I + A) w,

and its Hessian equals

4
(cid:107)w(cid:107)2 ww(cid:62)
where BâŠ¥ = B + B(cid:62) (i.e., a matrix B plus its transpose).

1
(cid:107)w(cid:107)2

I âˆ’

âˆ’

(cid:18)(cid:18)

(cid:19) (cid:18)

(cid:19)(cid:19)âŠ¥

F (w)I + A

,

Proof. By the product and chain rules (using the fact that
the gradient of F (w) = âˆ’ 1

(cid:0)w(cid:62)Aw(cid:1) equals

(cid:107)w(cid:107)2

1

(cid:107)w(cid:107)2 is a composition of w (cid:55)â†’ (cid:107)w(cid:107)2 and z (cid:55)â†’ 1
z ),

(cid:16)

w

2
(cid:107)w(cid:107)4

(cid:17)

w(cid:62)Aw

âˆ’ (Aw)

2
(cid:107)w(cid:107)2 ,

(25)

giving the gradient bound in the lemma statement after a few simpliï¬cations.

Differentiating the vector-valued Eq. (25) with respect to w (using the product and chain rules, and the

fact that

1

(cid:107)w(cid:107)4 is a composition of w (cid:55)â†’ (cid:107)w(cid:107)2, z (cid:55)â†’ z2, and z (cid:55)â†’ 1

z ), we get that the Hessian of F equals

2
(cid:107)w(cid:107)8 âˆ— 2(cid:107)w(cid:107)2 âˆ— 2w

(cid:19)(cid:62) (cid:16)

(cid:17)

w(cid:62)Aw

+ w

2
(cid:107)w4(cid:107)

(2Aw)(cid:62)

I

2
(cid:107)w(cid:107)4 (w(cid:62)Aw) + w
2

(cid:18)

âˆ’

(cid:18)

2

(cid:19)(cid:62)

âˆ’ A

âˆ’

(cid:107)w(cid:107)2 âˆ’ (Aw)
2F (w)
(cid:107)w(cid:107)2 I +
(cid:18)
1
(cid:107)w(cid:107)2

(cid:107)w(cid:107)4 âˆ— 2w
8F (w)
(cid:107)w(cid:107)4 ww(cid:62) +
8F (w)
(cid:107)w(cid:107)2 ww(cid:62) âˆ’

2F (w)I âˆ’

= âˆ’

= âˆ’

4
(cid:107)w(cid:107)4 ww(cid:62)A âˆ’

2

4
(cid:107)w(cid:107)4 Aww(cid:62)

(cid:107)w(cid:107)2 A +
4
(cid:107)w(cid:107)2 Aww(cid:62)

(cid:19)

,

4
(cid:107)w(cid:107)2 ww(cid:62)A + 2A âˆ’

which can be veriï¬ed to equal the expression in the lemma statement (using the fact that A, ww(cid:62) and I are
all symmetric matrices, hence equal their transpose).

2 (which implies (cid:104)w0, v1(cid:105) > 0).
1 be the intersection of the ray {av1 : a â‰¥ 0} with the hyperplane Hw0 = {w : (cid:104)w, w0(cid:105) = 1}. Then

Lemma 20. Let w0, v1 be two unit vectors such that (cid:107)w0 âˆ’ v1(cid:107) â‰¤ (cid:15) < 1
Let v(cid:48)
(cid:107)v(cid:48)

1 âˆ’ w0(cid:107) â‰¤ 5

4 (cid:15).

Proof. See Figure 2 in the main text for a graphical illustration.

Letting v(cid:48)

1 = av, a must satisfy (cid:104)av1, w0(cid:105) = 1. Since v1, w0 are unit vectors, this implies

a =

1
(cid:104)v1, w0(cid:105)

=

2
2 âˆ’ (cid:107)v1 âˆ’ w0(cid:107)2 ,

31

and since (cid:107)v1 âˆ’ w0(cid:107) â‰¤ (cid:15), this means that

(cid:20)

a âˆˆ

1,

(cid:21)

.

2
2 âˆ’ (cid:15)2

Therefore,

(cid:107)v(cid:48)

1 âˆ’ w0(cid:107) â‰¤ (cid:107)v1 âˆ’ w0(cid:107) + (cid:107)v(cid:48)

1 âˆ’ v1(cid:107) â‰¤ (cid:15) + (cid:107)av1 âˆ’ v1(cid:107) â‰¤ (cid:15) + |a âˆ’ 1| â‰¤ (cid:15) +

2

2 âˆ’ (cid:15)2 âˆ’ 1 = (cid:15) +

(cid:15)2
2 + (cid:15)2 ,

and since (cid:15) < 1

2 , this is at most 5

4 (cid:15).

We now turn to prove the theorem. Let âˆ‡2(w) denote the Hessian at some point w. To show smoothness
and strong convexity as stated in the theorem, it is enough to ï¬x some unit w0 which is (cid:15)-close to the leading
eigenvector v1 (where (cid:15) is assumed to be sufï¬ciently small), and show that for any point w on Hw0 which
is O((cid:15)) close to w0, and any direction g along Hw0 (i.e. any unit g such that (cid:104)g, w0(cid:105) = 0), it holds that
g(cid:62)âˆ‡2(w)g âˆˆ [Î», 20]. This implies that the second derivative in an O((cid:15)) neighborhood of w0 on Hw0 is
always in [Î», 20], hence the function is both Î»-strongly convex in that neighborhood.

More formally, letting (cid:15) âˆˆ (0, 1) be a small parameter to be chosen later, consider any w0 such that

any w such that

and any g such that

(cid:107)w0(cid:107) = 1 , (cid:107)w0 âˆ’ v1(cid:107) â‰¤ (cid:15),

(cid:104)w âˆ’ w0, w0(cid:105) = 0 , (cid:107)w âˆ’ w0(cid:107) â‰¤ 2(cid:15),

(cid:107)g(cid:107) = 1 , (cid:104)g, w0(cid:105) = 0.
Our goal is to show that for an appropriate (cid:15), we have g(cid:62)âˆ‡2(w)g âˆˆ [Î», 20]. Moreover, by Lemma 20, the
neighborhood set Hw0 âˆ© Bw0(2(cid:15)) would also contain a point av1 for some a, which is a global optimum of
F due to its scale-invariance. This would establish the theorem.

The easier part is to show the upper bound on g(cid:62)âˆ‡2(w)g. Since g is a unit vector, it is enough to bound

the spectral norm of âˆ‡2(w), which equals
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
(cid:107)w(cid:107)2

I âˆ’

(cid:18)(cid:18)

F (w)I + A

F (w)I + A

(cid:19)(cid:19)âŠ¥(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

(cid:107)F (w)I + A(cid:107)sp

(cid:19) (cid:18)

(cid:19) (cid:18)

4
(cid:107)w(cid:107)2 ww(cid:62)
4
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
4
(cid:13)
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
(cid:13)sp
(cid:13)
4
(cid:13)
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
(cid:13)

(cid:18)

I âˆ’

I âˆ’

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:32)

â‰¤

â‰¤

â‰¤

2
(cid:107)w(cid:107)2

2
(cid:107)w(cid:107)2

2
(cid:107)w(cid:107)2

(cid:107)I(cid:107)sp +

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

((cid:107)F (w)I(cid:107)sp + (cid:107)A(cid:107)sp) .

Since the spectral norm of A is 1, and (cid:107)w(cid:107)2 â‰¥ 1 (as w lies on a hyperplane Hw0 tangent to a unit vector
w0), it is easy to verify that this is at most 2(1 + 4)(1 + 1) = 20 as required.
We now turn to lower bound g(cid:62)âˆ‡2(w)g, which by Lemma 19 equals

âˆ’

1
(cid:107)w(cid:107)2 g(cid:62)

(cid:18)(cid:18)

I âˆ’

4
(cid:107)w(cid:107)2 ww(cid:62)

(cid:19) (cid:18)

(cid:19)(cid:19)âŠ¥

F (w)I + A

g.

32

Since g(cid:62)BâŠ¥g = g(cid:62)Bg + g(cid:62)B(cid:62)g = 2g(cid:62)Bg, the above equals

âˆ’

2
(cid:107)w(cid:107)2 g(cid:62)

(cid:18)

I âˆ’

4
(cid:107)w(cid:107)2 ww(cid:62)

(cid:19) (cid:18)

(cid:19)

F (w)I + A

g.

(26)

Using the fact that w = w0 + (w âˆ’ w0), and (cid:104)g, w0(cid:105) = 0, we get that (cid:104)g, w(cid:105) = (cid:104)g, w âˆ’ w0(cid:105). Moreover,
since A is positive semideï¬nite and has spectral norm of 1, F (w) = âˆ’ w(cid:62)Aw
(cid:107)w(cid:107)2 âˆˆ [âˆ’1, 0]. Expanding Eq. (26)
and plugging these in, we get

âˆ’

=

â‰¥

2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2

(cid:18)

F (w)g(cid:62)

(cid:18)

I âˆ’

(cid:18)

(cid:18)

âˆ’F (w)(cid:107)g(cid:107)2 +

âˆ’F (w)(cid:107)g(cid:107)2 âˆ’

(cid:19)

(cid:18)

I âˆ’

g + g(cid:62)

4
(cid:107)w(cid:107)2 ww(cid:62)
4F (w)
(cid:107)w(cid:107)2 (cid:104)g, w âˆ’ w0(cid:105)2 âˆ’ g(cid:62)Ag +
4
(cid:107)w(cid:107)2 (cid:107)g(cid:107)2(cid:107)w âˆ’ w0(cid:107)2 âˆ’ g(cid:62)Ag âˆ’

(cid:19)

(cid:19)

Ag

4
(cid:107)w(cid:107)2 ww(cid:62)
4
(cid:107)w(cid:107)2 (cid:104)g, w âˆ’ w0(cid:105)w(cid:62)Ag

(cid:19)

4

(cid:19)
(cid:107)w(cid:107)2 (cid:107)g(cid:107)(cid:107)w âˆ’ w0(cid:107)(cid:107)w(cid:107)(cid:107)A(cid:107)sp(cid:107)g(cid:107)

.

Since (cid:107)g(cid:107) = 1, (cid:107)A(cid:107)sp = 1, (cid:107)w âˆ’ w0(cid:107) â‰¤ 2(cid:15), and (cid:107)w(cid:107)2 = (cid:107)w0(cid:107)2 + (cid:107)w âˆ’ w0(cid:107)2 is between 1 and 1 + 4(cid:15)2,
this is at least

(cid:16)

(cid:112)

2
(cid:107)w(cid:107)2

(âˆ’F (w)) âˆ’ 16(cid:15)2 âˆ’ g(cid:62)Ag âˆ’ 8(cid:15)

1 + 4(cid:15)2(cid:17)(cid:17)
(27)
Let us now analyze âˆ’F (w) and g(cid:62)Ag more carefully. The idea will be to show that since we are close
to the optimum, âˆ’F (w) is very close to 1, and g (which is orthogonal to the near-optimal w0) is such that
g(cid:62)Ag is strictly smaller than 1. This would give us a positive lower bound on Eq. (27).

âˆ’F (w) âˆ’ g(cid:62)Ag âˆ’ 8(cid:15)

1 + 4(cid:15)2(cid:17)

2
(cid:107)w(cid:107)2

2(cid:15) +

(cid:112)

=

(cid:16)

(cid:16)

.

â€¢ By the triangle inequality and the assumptions (cid:107)w0 âˆ’ v1(cid:107) â‰¤ (cid:15), (cid:107)w âˆ’ w0(cid:107) â‰¤ 2(cid:15), we have (cid:107)w âˆ’ v1(cid:107) â‰¤
3(cid:15). Also, we claim that F (Â·) is 4-Lipschitz outside the unit Euclidean ball (since the gradient of F at
any point with norm â‰¥ 1, according to Lemma 19, has norm at most 4). Therefore, |F (w) + 1| =
|F (w) âˆ’ F (v1)| â‰¤ 4(cid:107)w âˆ’ v1(cid:107) â‰¤ 12(cid:15), so overall,

F (w) â‰¤ âˆ’1 + 12(cid:15).

(28)

â€¢ Since (cid:104)w0, g(cid:105) = 0, and (cid:107)w0 âˆ’ v1(cid:107) â‰¤ (cid:15), it follows that

|(cid:104)v1, g(cid:105)| â‰¤ |(cid:104)v1 âˆ’ w0, g(cid:105)| + |(cid:104)w0, g(cid:105)| â‰¤ (cid:107)v1 âˆ’ w0(cid:107)(cid:107)g(cid:107) + 0 â‰¤ (cid:15).

Letting v1, . . . , vd and 1 = s1 > s2 â‰¥ .. â‰¥ sd â‰¥ 0 be the eigenvectors and eigenvalues of A in
decreasing order (and recalling that s2 â‰¤ s1 âˆ’ Î» = 1 âˆ’ Î» for some eigengap Î» > 0), we get

g(cid:62)Ag =

d
(cid:88)

i=1

si(cid:104)vi, g(cid:105)2 â‰¤ (cid:104)v1, g(cid:105)2 + (1 âˆ’ Î»)

d
(cid:88)

i=1

(cid:104)vi, g(cid:105)2

= (cid:104)v1, g(cid:105)2 + (1 âˆ’ Î»)(1 âˆ’ (cid:104)v1, g(cid:105)2) = Î»(cid:104)v1, g(cid:105)2 + (1 âˆ’ Î»)
â‰¤ Î»(cid:15)2 + (1 âˆ’ Î») = 1 âˆ’ (1 âˆ’ (cid:15)2)Î».

(29)

33

Plugging Eq. (28) and Eq. (29) back into Eq. (27), we get a lower bound of

1 âˆ’ 12(cid:15) âˆ’ (cid:0)1 âˆ’ (1 âˆ’ (cid:15)2)Î»(cid:1) âˆ’ 8(cid:15)

(cid:16)

2(cid:15) +

(cid:112)

1 + 4(cid:15)2(cid:17)(cid:17)

(cid:16)

2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2

=

(cid:16)

(1 âˆ’ (cid:15)2)Î» âˆ’ 8(cid:15)

(cid:16)

1.5 + 2(cid:15) +

(cid:112)

1 + 4(cid:15)2(cid:17)(cid:17)

ï£«
ï£­1 âˆ’ (cid:15)2 âˆ’

=

2
(cid:107)w(cid:107)2

(cid:16)

8(cid:15)

1.5 + 2(cid:15) +

âˆš

1 + 4(cid:15)2(cid:17)

ï£¶

Î»

ï£¸ Î».

Using the fact that

âˆš

1 + z2 â‰¤ 1 + z, this can be loosely lower bounded by

2
(cid:107)w(cid:107)2

(cid:18)

1 âˆ’ (cid:15) âˆ’

8(cid:15) (2.5 + 4(cid:15))
Î»

(cid:19)

Î».

Recalling that (cid:107)w(cid:107)2 = (cid:107)w0(cid:107)2 + (cid:107)w âˆ’ w0(cid:107)2 is at most 1 + 4(cid:15)2, and picking (cid:15) sufï¬ciently small compared to
Î», (say (cid:15) = Î»/44), we get that the above is at least Î», which implies the required strong convexity condition.
To summarize, by picking (cid:15) = Î»/44, we have shown that the function F (w) is Î»-strongly convex and
20-smooth in a neighborhood of size 2(cid:15) = Î»
22 around w0 on the hyperplane Hw0, provided that (cid:107)w0 âˆ’v1(cid:107) â‰¤
(cid:15) = Î»
44 . By Lemma 20, we are guaranteed that this neighborhood contains v1 up to some rescaling (which
is immaterial for our scale-invariant function F ), hence by optimizing F in that neighborhood, we will get
a globally optimal solution.

Acknowledgments

This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel
Science Foundation grant 425/13.

References

[1] A. Agarwal and L. Bottou. A lower bound for the optimization of ï¬nite sums. In ICML, 2015.

[2] R. Arora, A. Cotter, K. Livescu, and N. Srebro. Stochastic optimization for PCA and PLS. In 2012

50th Annual Allerton Conference on Communication, Control, and Computing, 2012.

[3] R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In NIPS, 2013.

[4] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental PCA. In NIPS,

2013.

[5] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[6] C. De Sa, K. Olukotun, and C. RÂ´e. Global convergence of stochastic gradient descent for some non-

convex matrix problems. In ICML, 2015.

[7] R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster

stochastic algorithms for empirical risk minimization.

[8] G. H Golub and C. Van Loan. Matrix computations, volume 3. John Hopkins University Press, 2012.

34

[9] N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: Probabilistic algorithms

for constructing approximate matrix decompositions. SIAM review, 53(2):217â€“288, 2011.

[10] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In NIPS, 2014.

[11] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

statistical association, 58(301):13â€“30, 1963.

[12] R. Horn and C. Johnson. Matrix analysis. Cambridge university press, 2012.

[13] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In NIPS, 2013.

[14] T.P. Krasulina. The method of stochastic approximation for the determination of the least eigenvalue of
a symmetrical matrix. USSR Computational Mathematics and Mathematical Physics, 9(6):189â€“195,
1969.

[15] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and lanczos al-
gorithms with a random start. SIAM journal on matrix analysis and applications, 13(4):1094â€“1122,
1992.

[16] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In NIPS, 2014.

[17] E. Oja. Simpliï¬ed neuron model as a principal component analyzer. Journal of mathematical biology,

15(3):267â€“273, 1982.

[18] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric func-

tional analysis. Journal of the ACM (JACM), 54(4):21, 2007.

[19] O. Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In ICML,

2015.

[20] D. Woodruff. Sketching as a tool for numerical linear algebra. Theoretical Computer Science, 10(1-

2):1â€“157, 2014.

35

