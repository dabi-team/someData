5
1
0
2

l
u
J

1
3

]

G
L
.
s
c
[

1
v
8
8
7
8
0
.
7
0
5
1
:
v
i
X
r
a

Fast Stochastic Algorithms for SVD and PCA:
Convergence Properties and Convexity

Ohad Shamir
Weizmann Institute of Science
ohad.shamir@weizmann.ac.il

Abstract

We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast compu-
tation of leading singular vectors. We prove several new results, including a formal analysis of a block
version of the algorithm, and convergence from random initialization. We also make a few observa-
tions of independent interest, such as how pre-initializing with just a single exact power iteration can
signiﬁcantly improve the runtime of stochastic methods, and what are the convexity and non-convexity
properties of the underlying optimization problem.

1 Introduction

We consider the problem of recovering the top k left singular vectors of a d × n matrix X = (x1, . . . , xn),
where k (cid:28) d. This is equivalent to recovering the top k eigenvectors of XX (cid:62), or equivalently, solving the
optimization problem

min
W ∈Rd×k:W (cid:62)W =I

−W (cid:62)

(cid:33)

(cid:32)

1
n

n
(cid:88)

i=1

xix(cid:62)
i

W.

(1)

This is one of the most fundamental matrix computation problems, and has numerous uses (such as low-rank
matrix approximation and principal component analysis).

For large-scale matrices X, where exact eigendecomposition is infeasible, standard deterministic ap-
proaches are based on power iterations or variants thereof (e.g.
the Lanczos method) [8]. Alternatively,
one can exploit the structure of Eq. (1) and apply stochastic iterative algorithms, where in each iteration
we update a current d × k matrix W based on one or more randomly-drawn columns xi of X. Such algo-
rithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.g.
[2, 4, 3, 10, 6]. Another stochastic approach is based on random projections, e.g. [9, 20].

Unfortunately, each of these algorithms suffer from a different disadvantage: The deterministic algo-
rithms are accurate (runtime logarithmic in the required accuracy (cid:15), under an eigengap condition), but re-
quire a full pass over the matrix for each iteration, and in the worst-case many such passes would be required
(polynomial in the eigengap). On the other hand, each iteration of the stochastic algorithms is cheap, and
their number is independent of the size of the matrix, but on the ﬂip side, their noisy stochastic nature means
they are not suitable for obtaining a high-accuracy solution (the runtime scales polynomially with (cid:15)).

Recently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq. (1), which has a “best-of-
both-worlds” property: The algorithm is based on cheap stochastic iterations, yet the algorithm’s runtime is
logarithmic in the required accuracy (cid:15). More precisely, for the case k = 1, xi of bounded norm, and when

1

 
 
 
 
 
 
there is an eigengap of λ between the ﬁrst and second leading eigenvalues of the covariance matrix 1
the required runtime was shown to be on the order of

n XX (cid:62),

(cid:18)

d

n +

(cid:19)

1
λ2

log

(cid:19)

.

(cid:18) 1
(cid:15)

(2)

The algorithm is therefore suitable for obtaining high accuracy solutions (the dependence on (cid:15) is logarith-
mic), but essentially at the cost of only O(log(1/(cid:15))) passes over the data. The algorithm is based on a recent
variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems
([13]), although the optimization problem in Eq. (1) is inherently non-convex. See Section 3 for a more
detailed description of this algorithm, and [19] for more discussions as well as empirical results.

The results and analysis in [19] left several issues open. For example, it is not clear if the quadratic
dependence on 1/λ in Eq. (2) is necessary, since it is worse than the linear (or better) dependence that
can be obtained with the deterministic algorithms mentioned earlier, as well as analogous results that can
be obtained with similar techniques for convex optimization problems (where λ is the strong convexity
parameter). Also, the analysis was only shown for the case k = 1, whereas often in practice, we may want
to recover k > 1 singular vectors simultaneously. Although [19] proposed a variant of the algorithm for that
case, and studied it empirically, no analysis was provided. Finally, the convergence guarantee assumed that
the algorithm is initialized from a point closer to the optimum than what is attained with standard random
initialization. Although one can use some other, existing stochastic algorithm to do this “warm-start”, no
end-to-end analysis of the algorithm, starting from random initialization, was provided.

In this paper, we study these and related questions, and make the following contributions:

• We propose a variant of VR-PCA to handle the k > 1 case, and formally analyze its convergence
(Section 3). The extension to k > 1 is non-trivial, and requires tracking the evolution of the subspace
spanned by the current solution at each iteration.

• In Section 4, we study the convergence of VR-PCA starting from a random initialization. And show
that with a slightly smarter initialization – essentially, random initialization followed by a single power
iteration – the convergence results can be substantially improved.
In fact, a similar initialization
scheme should assist in the convergence of other stochastic algorithms for this problem, as long as a
single power iteration can be performed.

• In Section 5, we study whether functions similar to Eq. (1) have hidden convexity properties, which
would allow applying existing convex optimization tools as-is, and improve the required runtime. For
the k = 1 case, we show that this is in fact true: Close enough to the optimum, and on a suitably-
designed convex set, such a function is indeed λ-strongly convex. Unfortunately, the distance from
the optimum has to be O(λ), and this precludes a better runtime in most practical regimes. However,
it still indicates that a better runtime and dependence on λ should be possible.

2 Some Preliminaries and Notation

We consider a d × n matrix X composed of n columns (x1, . . . , xn), and let

A =

1
n

XX (cid:62) =

1
n

n
(cid:88)

i=1

xix(cid:62)
i .

Thus, Eq. (1) is equivalent to ﬁnding the k leading eigenvectors of A.

2

We generally use bold-face letters to denote vectors, and capital letters to denote matrices. We let Tr(·)
denote the trace of a matrix, (cid:107) · (cid:107)F to denote the Frobenius norm, and (cid:107) · (cid:107)sp to denote the spectral norm.
A symmetric d × d matrix B is positive semideﬁnite, if inf z∈Rd z(cid:62)Bz ≥ 0. A is positive deﬁnite if the
inequality is strict. Following standard notation, we write B (cid:23) 0 to denote that A is positive semideﬁnite,
and B (cid:23) C if B − C (cid:23) 0. B (cid:31) 0 means that B is positive deﬁnite.

A twice-differentiable function F on a subset of Rd is convex, if its Hessian is alway positive semidef-
inite. If it is always positive deﬁnite, and (cid:31) λI for some λ > 0, we say that the function is λ-strongly
convex. If the Hessian is always ≺ sI for some s ≥ 0, then the function is s-smooth.

3 The VR-PCA Algorithm and a Block Version

We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its general-
ization for k > 1.

Algorithm 1 VR-PCA: Vector version (k = 1)
1: Parameters: Step size η, epoch length m
2: Input: Data matrix X = (x1, . . . , xn); Initial unit vector ˜w0
3: for s = 1, 2, . . . do
˜u = 1
i=1 xi
4:
n
5: w0 = ˜ws−1
6:

for t = 1, 2, . . . , m do

i ˜ws−1

(cid:0)x(cid:62)

(cid:80)n

(cid:1)

Pick it ∈ {1, . . . , n} uniformly at random
w(cid:48)
˜ws−1
wt = 1
(cid:107)w(cid:48)

t = wt−1 + η (cid:0)xit
t(cid:107) w(cid:48)

wt−1 − x(cid:62)
it

(cid:0)x(cid:62)
it

t

(cid:1) + ˜u(cid:1)

7:

8:

9:

10:

end for
˜ws = wm

11:
12: end for

The basic idea of the algorithm is to perform stochastic updates using randomly-sampled columns xi of
the matrix, but interlace them with occasional exact power iterations, and use that to gradually reduce the
variance of the stochastic updates. Speciﬁcally, the algorithm is split into epochs s = 1, 2, . . ., where in
each epoch we do a single exact power iteration with respect to the matrix A (by computing ˜u), and then
perform m stochastic updates, which can be re-written as

w(cid:48)

t = (I + ηA)wt−1 + η

(cid:16)

xitx(cid:62)

it − A

(cid:17)

(wt−1 − ˜ws−1) , wt =

1
(cid:107)w(cid:48)
t(cid:107)

wt,

The ﬁrst term is essentially a power iteration (with a ﬁnite step size η), whereas the second term is zero-
mean, and with variance dominated by (cid:107)wt−1 − ˜ws−1(cid:107)2. As the algorithm progresses, wt−1 and ˜ws−1
both converge toward the same optimal point, hence (cid:107)wt−1 − ˜ws−1(cid:107)2 shrinks, eventually leading to an
exponential convergence rate.

To handle the k > 1 case (where more than one eigenvector should be recovered), one simple technique
is deﬂation, where we recover the leading eigenvectors v1, v2, . . . , vk one-by-one, each time using the
k = 1 algorithm. However, a disadvantage of this approach is that it requires a positive eigengap between
all top k eigenvalues, otherwise the algorithm is not guaranteed to converge. Thus, an algorithm which
simultaneously recovers all k leading eigenvectors is preferable.

3

We will study a block version of Algorithm 1, presented as Algorithm 2. It is mostly a straightfor-
ward generalization (similar to how power iterations are generalized to orthogonal iterations), where the
d-dimensional vectors wt−1, ˜ws−1, u are replaced by d × k matrices Wt−1, ˜Ws−1, ˜U , and normalization is
replaced by orthogonalization1. Indeed, Algorithm 1 is equivalent to Algorithm 2 when k = 1. The main
twist in Algorithm 2 is that instead of using ˜Ws−1, ˜U as-is, we perform a unitary transformation (via the
k × k orthogonal matrix Bt−1) which maximally aligns them with Wt−1. Note that Bt−1 is a k × k matrix,
and since k is assumed to be small, this does not introduce signiﬁcant computational overhead.

Algorithm 2 VR-PCA: Block version

Parameters: Rank k, Step size η, epoch length m
Input: Data matrix X = (x1, . . . , xn); Initial d × k matrix ˜W0 with orthonormal
columns
for s = 1, 2, . . . do
i=1 xi

˜Ws−1

x(cid:62)
i

(cid:80)n

(cid:16)

(cid:17)

˜U = 1
n
W0 = ˜Ws−1
for t = 1, 2, . . . , m do

Bt−1 = V U (cid:62), where U SV (cid:62) is an SVD decomposition of W (cid:62)
t−1
(cid:66) Equivalent to Bt−1 = arg minB(cid:62)B=I (cid:107)Wt−1 − ˜Ws−1B(cid:107)2
F
Pick it ∈ {1, . . . , n} uniformly at random
x(cid:62)
W (cid:48)
xit
it
(cid:17)−1/2

t = Wt−1 + η

Wt−1 − x(cid:62)
it

˜Ws−1Bt−1

+ ˜U Bt−1

(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

Wt = W (cid:48)
t

W (cid:48)(cid:62)

t W (cid:48)
t

˜Ws−1

end for
˜Ws = Wm

end for

We now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of

Algorithm 1 given in [19]:

Theorem 1. Deﬁne the d × d matrix A as 1
composed of the eigenvectors corresponding to the largest k eigenvalues. Suppose that

n XX (cid:62) = 1

i=1 xix(cid:62)

n

i , and let Vk denote the d × k matrix

(cid:80)n

• maxi (cid:107)xi(cid:107)2 ≤ r for some r > 0.

• A has eigenvalues s1 > s2 ≥ . . . ≥ sd, where sk − sk+1 = λ for some λ > 0.

• k − (cid:107)V (cid:62)
k

˜W0(cid:107)2

F ≤ 1
2 .

Let δ, (cid:15) ∈ (0, 1) be ﬁxed. If we run the algorithm with any epoch length parameter m and step size η, such
that

η ≤

cδ2
r2 λ , m ≥

c(cid:48) log(2/δ)
ηλ

,

kmη2r2 + rk

(cid:112)

mη2 log(2/δ) ≤ c(cid:48)(cid:48)

(3)

1The normalization Wt = W (cid:48)
t

(cid:17)−1/2

ensures that Wt has orthonormal columns. We note that in our analysis, η is

chosen sufﬁciently small so that W

t is always invertible, hence the operation is well-deﬁned.

(cid:48)(cid:62)
t W (cid:48)
t

(cid:16)

W
(cid:48)(cid:62)
t W (cid:48)

4

(where c, c(cid:48), c(cid:48)(cid:48) designate certain positive numerical constants), and for T =
probability at least 1 − (cid:100)log2(1/(cid:15))(cid:101)δ, it holds that

(cid:108) log(1/(cid:15))
log(2/δ)

(cid:109)

epochs, then with

k − (cid:107)V (cid:62)
k

˜WT (cid:107)2

F ≤ (cid:15).

k W (cid:107)2

For any orthogonal W , k − (cid:107)V (cid:62)

F lies between 0 and k, and equals 0 when the column spaces of
Vk and W are the same (i.e., when W spans the k leading singular vectors). According to the theorem,
taking appropriate2 η = Θ(λ/(kr)2), and m = Θ((rk/λ)2), the algorithm converges with high probability
to a high-accuracy approximation of Vk. Moreover, the runtime of each epoch of the algorithm equals
O(mdk2 + dnk). Overall, we get the following corollary:

Corollary 1. Under the conditions of Theorem 1, there exists an algorithm returning ˜WT such that k −
(cid:107)V (cid:62)
k

F ≤ (cid:15) with arbitrary constant accuracy, in runtime O

(cid:17)
λ2 ) log(1/(cid:15))

dk(n + r2k3

˜WT (cid:107)2

(cid:16)

.

˜Wt(cid:107)2

This runtime bound is the same3 as that of [19] for k = 1.
The proof of Theorem 1 appears in Subsection 6.1, and relies on a careful tracking of the evolution
of the potential function k − (cid:107)V (cid:62)
F . An important challenge compared to the k = 1 case is that the
k
matrices Wt−1 and ˜Ws−1 do not necessarily become closer over time, so the variance-reduction intuition
discussed earlier no longer applies. However, the column space of Wt−1 and ˜Ws−1 do become closer, and
this is utilized by introducing the transformation matrix Bt−1. We note that although Bt−1 appears essential
for our analysis, it isn’t clear that using it is necessary in practice: In [19], the suggested block algorithm
was Algorithm 2 with Bt−1 = I, which seemed to work well in experiments. In any case, using this matrix
doesn’t affect the overall runtime beyond constants, since the additional runtime of computing and using
this matrix (O(dk2)) is the same as the other computations performed at each iteration.

A limitation of the theorem above is the assumption that the initial point ˜W0 is such that k−(cid:107)V (cid:62)
k

F ≤
1
2 . This is a non-trivial assumption, since if we initialize the algorithm from a random d × O(1) orthogo-
nal matrix ˜W0, then with overwhelming probability, (cid:107)V (cid:62)
F = O(1/d). However, experimentally the
k
algorithm seems to work well even with random initialization [19]. Moreover, if we are interested in a
theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm
for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a ˜W0. The idea is that ˜W0 is
only required to approximate Vk up to constant accuracy, so purely stochastic algorithms (which are good in
obtaining a low-accuracy solution) are quite suitable. In the next section, we further delve into these issues,
and show that in our setting such algorithms in fact can be substantially improved.

˜W0(cid:107)2

˜W0(cid:107)2

4 Warm-Start and the Power of a Power Iteration

In this section, we study the runtime required to compute a starting point satisfying the conditions of Theo-
rem 1, starting from a random initialization. Combined with Theorem 1, this gives us an end-to-end analysis
of the runtime required to ﬁnd an (cid:15)-accurate solution, starting from a random point. For simplicity, we will
only discuss the case k = 1, i.e. where our goal is to compute the single leading eigenvector v1, although

2Speciﬁcally, we can take m = c(cid:48) log(2/δ)/ηλ and η = aδ2/r2λ, where a is sufﬁciently small to ensure that the ﬁrst and third

condition in Eq. (3) holds. It can be veriﬁed that it’s enough to take a = min

(cid:26)

c,

c(cid:48)(cid:48)
4δ2ck log(2/δ) ,

1
4δ2c

(cid:16)

c(cid:48)(cid:48)
k log(2/δ)

(cid:17)2(cid:27)

.

3[19] showed that it’s possible to further improve the runtime for sparse X, replacing d by the average column sparsity ds. This
is done by maintaining parameters in an implicit form, but it’s not clear how to implement a similar trick in the block version, where
k > 1.

5

our observations can be generalized to k > 1. In the k = 1 case, Theorem 1 kicks in once we ﬁnd a vector
w satisfying (cid:104)v1, w(cid:105)2 ≥ 1
2 .

As mentioned previously, one way to get such a w is to run a purely stochastic algorithm, which com-
putes the leading eigenvector of a covariance matrix E[xx(cid:62)] given a stream of i.i.d. samples x. We can
easily use such an algorithm in our setting, by sampling columns from our matrix X = (x1, . . . , xn) uni-
formly at random, and feed to such a stochastic optimization algorithm, guaranteed to approximate the
leading eigenvector of 1
n

i=1 xix(cid:62)
i .

(cid:80)n

To the best of our knowledge, the existing iteration complexity guarantees for such algorithms (assuming
the norm constraint r ≤ 1 for simplicity) scale at least4 as d/λ2. Since the runtime of each iteration is O(d),
we get an overall runtime of O((d/λ)2).

The dependence on d in the iteration bound stems from the fact that with a random initial unit vector
w0, we have (cid:104)v1, w0(cid:105)2 ≈ 1
d . Thus, we begin with a vector almost orthogonal to the leading eigenvector v1
(depending on d). In a purely stochastic setting, where only noisy information is available, this necessitates
conservative updates at ﬁrst, and in all the analyses we are aware of, the number of iterations appear to
necessarily scale at least linearly with d.

However, it turns out that in our setting, with a ﬁnite matrix X, we can perform a smarter initialization:
Sample w from the standard Gaussian distribution on Rd, perform a single power iteration w.r.t. the covari-
ance matrix A = 1
n XX (cid:62), i.e. w0 = Aw/(cid:107)Aw(cid:107), and initialize from w0. For such a procedure, we have the
following simple observation:
Lemma 1. For w0 as above, it holds for any δ that with probability at least 1 − 1

d − δ,

(cid:104)v1, w0(cid:105)2 ≥

δ2
12 log(d) nrank(A)

,

where nrank(A) = (cid:107)A(cid:107)2
F
(cid:107)A(cid:107)2
sp

is the numerical rank of A.

The numerical rank (see e.g. [18]) is a relaxation of the standard notion of rank: For any d × d matrix
A, nrank(A) is at most the rank of A (which in turn is at most d). However, it will be small even if A
is just close to being low-rank. In many if not most machine learning applications, we are interested in
matrices which tend to be approximately low-rank, in which case nrank(A) is much smaller than d or even
a constant. Therefore, by a single power iteration, we get an initial point w0 for which (cid:104)v1, w0(cid:105)2 is on the
order of 1/nrank(A), which can be much larger than the 1/d given by a random initialization, and is never
substantially worse.

Proof of Lemma 1. Let s1 ≥ s2 ≥ . . . ≥ sd ≥ 0 be the d eigenvalues of A, with eigenvectors v1, . . . , vd.
We have

(cid:104)v1, w0(cid:105)2 =

(cid:104)v1, Aw(cid:105)2
(cid:107)Aw(cid:107)2 =

(s1(cid:104)v1, w(cid:105))2

(cid:16)(cid:80)d

i=1 sivi(cid:104)vi, w(cid:105)

(cid:17)2 =

(cid:80)d

s2
1(cid:104)v1, w(cid:105)2
i=1 s2

i (cid:104)vi, w(cid:105)2

.

Since w is distributed according to a standard Gaussian distribution, which is rotationally symmetric, we
can assume without loss of generality that v1, . . . , vd correspond to the standard basis vectors e1, . . . , ed,
in which case the above reduces to

s2
1w2
1
i w2
i=1 s2
i

(cid:80)d

≥

s2
1
i=1 s2
i

(cid:80)d

w2
1
maxi w2
i

,

4For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired

output. The guarantee of [4] scale as d2/λ2, and the guarantee of [10] scales as d/λ3 in our setting.

6

where w1, . . . , wd are independent and scalar random variables with a standard Gaussian distribution.

First, we note that s2

nius norm of A. Therefore,

1 equals (cid:107)A(cid:107)2
s2
1
i s2
i

=

(cid:80)

sp, the spectral norm of A, whereas (cid:80)d
(cid:107)A(cid:107)2
sp
nrank(A) , and we get overall that
(cid:107)A(cid:107)2
F

=

1

i=1 s2

i equals (cid:107)A(cid:107)2

F , the Frobe-

(cid:104)v1, w0(cid:105)2 ≥

1
nrank(A)

w2
1
maxi w2
i

.

(4)

We consider the random quantity w2

1/ maxi w2

i , and independently bound the deviation probability of

the numerator and denominator. First, for any t ≥ 0 we have

Pr(w2

1 ≤ t) = Pr(w1 ∈ [−

√

√

t,

t]) =

√

t

(cid:90)

√

t

z=−

(cid:114) 1
2π

(cid:18)

exp

−

(cid:19)

z2
2

≤

(cid:114) 1
2π

√

∗ 2

t =

(cid:114) 2
π

t .

(5)

Second, by combining two standard Gaussian concentration results (namely, that if W = max{|w1|, . . . , |wd|},
then 0 ≤ E[W ] ≤ 2(cid:112)2 log(d), and by the Cirelson-Ibragimov-Sudakov inequality, Pr(W − E[W ] > t) ≤
exp(−t2/2)), we get that

and therefore

Pr(max

i

Pr(max

i

|wi| > 2(cid:112)2 log(d) + t) ≤ exp(−t2/2),

i > (2(cid:112)2 log(d) + t)2) ≤ exp(−t/2).
w2

(6)

Combining Eq. (5) and Eq. (6), with a union bound, we get that for any t1, t2 ≥ 0, it holds with probability
at least 1 −

(cid:113) 2

π t1 − exp(−t2

2/2) that

w2
1
maxi w2
i

≥

t1
(2(cid:112)2 log(d) + t2)2

.

To slightly simplify this for readability, we take t2 = (cid:112)2 log(d), and substitute δ =
that with probability at least 1 − δ − 1/d,

(cid:113) 2

π t1. This implies

w2
1
maxi w2
i

≥

π
2 δ2
18 log(d)

>

δ2
12 log(d)

.

Plugging back into Eq. (4), the result follows.

This result can be plugged into the existing analyses of purely stochastic PCA/SVD algorithms, and
can often improve the dependence on the d factor in the iteration complexity bounds to a dependence on
the numerical rank of A. We again emphasize that this is applicable in a situation where we can actually
perform a power iteration, and not in a purely stochastic setting where we only have access to an i.i.d. data
stream (nevertheless, it would be interesting to explore whether this idea can be utilized in such a streaming
setting as well).

To give a concrete example of this, we provide a convergence analysis of the VR-PCA algorithm (Algo-
rithm 1), starting from an arbitrary initial point, bounding the total number of stochastic iterations required
by the algorithm in order to produce a point satisfying the conditions of Theorem 1 (from which point the
analysis of Theorem 1 takes over). Combined with Theorem 1, this analysis also justiﬁes that VR-PCA
indeed converges starting from a random initialization.

7

Theorem 2. Using the notation of Theorem 1 (where λ is the eigengap, v1 is the leading eigenvector, and
r = maxi (cid:107)xi(cid:107)2), and for any δ ∈ (0, 1
2 ), suppose we run Algorithm 1 with some initial unit-norm vector
˜w0 such that

(cid:104)v1, ˜w0(cid:105)2 ≥ ζ > 0,

and a step size η satisfying

η ≤

cδ2λζ 3
r2 log2(2/δ)

(7)

(for some universal constant c). Then with probability at least 1 − δ, after

T =

(cid:22) c(cid:48) log(2/δ)
ηλζ

(cid:23)

stochastic iterations (lines 6 − 10 in the pseudocode, where c(cid:48) is again a universal constant), we get a point
wT satisfying 1 − (cid:104)v1, wT (cid:105)2 ≤ 1
2 . Moreover, if η is chosen on the same order as the upper bound in Eq. (7),
then

T = Θ

(cid:18) r2 log3(2/δ)
δ2λ2ζ 4

(cid:19)

.

Note that the analysis does not depend on the choice of the epoch size m, and does not use the special
structure of VR-PCA (in fact, the technique we use is applicable to any algorithm which takes stochastic
gradient steps to solve this type of problem5). The proof of the theorem appears in Section 6.2.

Considering δ, r as a constants, we get that the runtime required by VR-PCA to ﬁnd a point w such that
1 − (cid:104)v1, wT (cid:105)2 ≤ 1
2 is O(d/λ2ζ 4) where ζ is a lower bound on (cid:104)v1, ˜w0(cid:105)2. As discussed earlier, if ˜w0 is
a result of random initialization followed by a power iteration (requiring O(nd) time), and the covariance
matrix A has small numerical rank, then ζ = (cid:104)v1, ˜w0(cid:105)2 = ˜Ω(1/ log(d)), and the runtime is

(cid:18)

O

nd +

(cid:19)
d
λ2 log4(d)

(cid:32)

(cid:32)

= O

d

n +

(cid:18) log2(d)
λ

(cid:19)2(cid:33)(cid:33)

.

By Corollary 1, the runtime required by VR-PCA from that point to get an (cid:15)-accurate solution is

(cid:18)

(cid:18)

O

d

n +

(cid:19)

1
λ2

log

(cid:19)(cid:19)

,

(cid:18) 1
(cid:15)

so the sum of the two expressions (which is d (cid:0)n + 1
λ2
required by the algorithm.

(cid:1) up to log-factors), represents the total runtime

Finally, we note that this bound holds under the reasonable assumption that the numeric rank of A
If this assumption doesn’t hold, ζ can be as large as d, and the resulting bound will have
is constant.
a worse polynomial dependence on d. We suspect that this is due to a looseness in the dependence on
ζ = (cid:104)v1, ˜w0(cid:105)2 in Theorem 2, since better dependencies can be obtained, at least for slightly different
algorithmic approaches (e.g. [4, 10, 6]). We leave a sharpening of the bound w.r.t. ζ as an open problem.

5Although there exist previous analyses of such algorithms in the literature, they unfortunately do not quite apply to our algo-

rithm, for various technical reasons.

8

5 Convexity and Non-Convexity of the Rayleigh Quotient

As mentioned in the introduction, an intriguing open question is whether the d (cid:0)n + 1
(cid:1) runtime
λ2
guarantees from the previous sections can be further improved. Although a linear dependence on d, n seems
unavoidable, this is not the case for the quadratic dependence on 1/λ. Indeed, when using deterministic
methods such as power iterations or the Lanczos method, the dependence on λ in the runtime is only 1/λ or
even (cid:112)1/λ [15]. In the world of convex optimization from which our algorithmic techniques are derived,
the analog of λ is the strong convexity parameter of the function, and again, it is possible to get a dependence
of 1/λ, or even (cid:112)1/λ with accelerated schemes (see e.g. [13, 16, 7] in the context of the variance-reduction
technique we use). Is it possible to get such a dependence for our problem as well?

(cid:1) log (cid:0) 1
(cid:15)

Another question is whether the non-convex problem that we are tackling (Eq. (1)) is really that non-
convex. Clearly, it has a nice structure (since we can solve the problem in polynomial time), but perhaps it
actually has hidden convexity properties, at least close enough to the optimal points? We note that Eq. (1)
can be “trivially” convexiﬁed, by re-casting it as an equivalent semideﬁnite program [5]. However, that
would require optimization over d × d matrices, leading to poor runtime and memory requirements. The
question here is whether we have any convexity with respect to the original optimization problem over
“thin” d × k matrices.

In fact, the two questions of improved runtime and convexity are closely related: If we can show that the
optimization problem is convex in some domain containing an optimal point, then we may be able to use
fast stochastic algorithms designed for convex optimization problems, inheriting their good guarantees.

To discuss these questions, we will focus on the k = 1 case for simplicity (i.e., our goal is to ﬁnd a
(cid:80)n
i ), and study potential convexity properties

n XX (cid:62) = 1

i=1 xix(cid:62)

n

leading eigenvector of the matrix A = 1
of the negative Rayleigh quotient,

FA(w) = −

w(cid:62)Aw
(cid:107)w(cid:107)2 =

1
n

n
(cid:88)

(cid:18)

−

i=1

(cid:104)w, xi(cid:105)2
(cid:107)w(cid:107)2

(cid:19)

.

Note that for k = 1, this function coincides with Eq. (1) on the unit Euclidean sphere, and with the same
optimal points, but has the nice property of being deﬁned on the entire Euclidean space (thus, at least its
domain is convex).

At a ﬁrst glance, such functions FA appear to potentially be convex at some bounded distance from an

optimum, as illustrated for instance in the case where A =

(see Figure 1). Unfortunately, it turns

(cid:19)

(cid:18) 1 0
0 0

out that the ﬁgure is misleading, and in fact the function is not convex almost everywhere:

Theorem 3. For the matrix A above, the Hessian of FA is not positive semideﬁnite for all but a measure-zero
set.
Proof. The leading eigenvector of A is v1 = (1, 0), and FA(w) = − w2
at some w equals

. The Hessian of this function

1
1+w2
2

w2

2
1 + w2

2)3

(w2

(cid:18) w2

2(3w2
−2w1w2(w2

1 − w2
2)
1 − w2
2)

−2w1w2(w2
1(w2

1 − w2
2)
1 − 3w2
2)

w2

(cid:19)

.

9

Figure 1: The function (w1, w2) (cid:55)→ − w2
,
1
1+w2
2
corresponding to FA(w) where A = (1 0 ; 0 0).
It is invariant to re-scaling of w, and attains a
minimum at (a, 0) for any a (cid:54)= 0.

w2

The determinant of this 2 × 2 matrix equals

Figure 2: Illustration of the construction of the
convex set on which FA is strongly convex and
smooth. v1 is the leading eigenvector of A, and a
minimum of FA (as well as any re-scaling of v1).
w0 is a nearby unit vector, and we consider the
intersection of a hyperplane orthogonal to w0,
and an Euclidean ball centered at w0.

(w2

=

=

(cid:0)w2

4
1 + w2
2)6
4w2
1w2
2
1 + w2
2)6
1w2
4w2
2
1 + w2
2)6

(w2

(w2

1w2

2(3w2

1 − w2

2)(w2

1 − 3w2

2) − 4w2

1w2

2(w2

1 − w2

2)2(cid:1)

(cid:0)(3w2

1 − w2

2)(w2

1 − 3w2

2) − 4(w2

1 − w2

2)2(cid:1)

(cid:0)−(w2

1 + w2

2)2(cid:1) = −

1w2
4w2
2
1 + w2

2)4 ,

(w2

which is always non-positive, and strictly negative for w for which w1w2 (cid:54)= 0 (which holds for all but a
measure-zero set of Rd). Since the determinant of a positive semideﬁnite matrix is always non-negative, this
implies that the Hessian isn’t positive semideﬁnite for any such w.

The theorem implies that we indeed cannot use convex optimization tools as-is on the function FA, even
if we’re close to an optimum. However, the non-convexity was shown for FA as a function over the entire
Euclidean space, so the result does not preclude the possibility of having convexity on a more constrained,
lower-dimensional set. In fact, this is what we are going to do next: We will show that if we are given some
point w0 close enough to an optimum, then we can explicitly construct a simple convex set, such that

• The set includes an optimal point of FA.

• The function FA is O(1)-smooth and λ-strongly convex in that set.

10

𝒗1𝒘0𝐻𝒘0Unit Euclidean sphere𝐵𝒘0(𝑟)Optimal pointsThis means that we can potentially use a two-stage approach: First, we use some existing algorithm (such
as VR-PCA) to ﬁnd w0, and then switch to a convex optimization algorithm designed to handle functions
with a ﬁnite sum structure (such as FA). Since the runtime of such algorithms scale better than VR-PCA, in
terms of the dependence on λ, we can hope for an overall runtime improvement.

Unfortunately, this has a catch: To make it work, we need to have w0 very close to the optimum – in
fact, we require (cid:107)v1 − w0(cid:107) ≤ O(λ), and we show (in Theorem 5) that such a dependence on the eigengap λ
cannot be avoided (perhaps up to a small polynomial factor). The issue is that the runtime to get such a w0,
using stochastic-based approaches we are aware of, would scale at least quadratically with 1/λ, but getting
dependence better than quadratic was our problem to begin with. For example, the runtime guarantee using
VR-PCA to get such a point w0 (even if we start from a good point as speciﬁed in Theorem 1) is on the
order of

(cid:18)

d

n +

(cid:19)

1
λ2

log

(cid:19)

,

(cid:18) 1
λ

whereas the best known guarantees on getting an (cid:15)-optimal solution for λ-strongly convex and smooth
functions (see [1]) is on the order of

(cid:18)

d

n +

(cid:19)

(cid:114) n
λ

log

(cid:19)

.

(cid:18) 1
(cid:15)

Therefore, the total runtime we can hope for would be on the order of

(cid:18)(cid:18)

d

n +

(cid:19)

1
λ2

log

(cid:18) 1
λ

(cid:19)

(cid:18)

+

n +

(cid:19)

(cid:114) n
λ

log

(cid:19)(cid:19)

.

(cid:18) 1
(cid:15)

(8)

In comparison, the runtime guarantee of using just VR-PCA to get an (cid:15)-accurate solution is on the order of

(cid:18)

d

n +

(cid:19)

1
λ2

log

(cid:19)

.

(cid:18) 1
(cid:15)

(9)

Unfortunately, Eq. (9) is the same as Eq. (8) up to log-factors, and the difference is not signiﬁcant unless
the required accuracy (cid:15) is extremely small (exponentially small in n, 1/λ). Therefore, our construction is
mostly of theoretical interest. However, it still shows that asymptotically, as (cid:15) → 0, it is indeed possible
to have runtime scaling better than Eq. (9). This might hint that designing practical algorithms, with better
runtime guarantees for our problem, may indeed be possible.

To explain our construction, we need to consider two convex sets: Given a unit vector w0, deﬁne the

hyperplane tangent to w0,

Hw0 = {w : (cid:104)w, w0(cid:105) = 1}

as well as a Euclidean ball of radius r centered at w0:

Bw0(r) = {w : (cid:107)w − w0(cid:107) ≤ r}

The convex set we use, given such a w0, is simply the intersection of the two, Hw0 ∩ Bw0(r), where r is a
sufﬁciently small number (see Figure 2).

The following theorem shows that if w0 is O(λ)-close to an optimal point (a leading eigenvector v1 of
A), and we choose the radius of Bw0(r) appropriately, then Hw0 ∩ Bw0(r) contains an optimal point, and
the function FA is indeed λ-strongly convex and smooth on that set. For simplicity, we will assume that A
is scaled to have spectral norm of 1, but the result can be easily generalized.

11

Theorem 4. For any positive semideﬁnite A with spectral norm 1, eigengap λ and a leading eigenvector
v1, and any unit vector w0 such that (cid:107)w0 − v1(cid:107) ≤ λ
44 , the function FA(w) is 20-smooth and λ-strongly
convex on the convex set Hw0 ∩ Bw0

(cid:1), which contains a global optimum of FA.

(cid:0) λ
22

The proof of the theorem appears in Subsection 6.3. Finally, we show below that a polynomial depen-
dence on the eigengap λ is unavoidable, in the sense that the convexity property is lost if w0 is signiﬁcantly
further away from v1.
Theorem 5. For any λ, (cid:15) ∈ (cid:0)0, 1
(cid:1), there exists a positive semideﬁnite matrix A with spectral norm 1,
2
eigengap λ, and leading eigenvector v1, as well as a unit vector w0 for which (cid:107)v1 − w0(cid:107) ≤ (cid:112)2(1 + (cid:15))λ),
such that FA is not convex in any neighborhood of w0 on Hw0.

Proof. Let

for which v1 = (1, 0, 0), and take





A =

1
0
0
0 1 − λ 0
0
0
0



 ,

(cid:112)

w0 = (

1 − p2, 0, p),

where p = (cid:112)(1 + (cid:15))λ (which ensures (cid:107)v1−w0(cid:107)2 = (cid:112)2p2 = (cid:112)2(1 + (cid:15))λ). Consider the ray {((cid:112)1 − p2, t, p) :
t ≥ 0}, and note that it starts from w0 and lies in Hw0. The function FA along that ray (considering it as a
function of t) is of the form

−

(1 − p2) + (1 − λ)t2
(1 − p2) + t2 + p2 = −

1 − p2 + (1 − λ)t2
1 + t2

.

The second derivative with respect to t equals

−2

(3t2 − 1)(λ − p2)
(t2 + 1)3

= 2

(3t2 − 1)(cid:15)λ
(t2 + 1)3 ,

where we plugged in the deﬁnition of p. This is a negative quantity for any t < 1√
. Therefore, the function
3
FA is strictly concave (and not convex) along the ray we have deﬁned and close enough to w0, and therefore
isn’t convex in any neighborhood of w0 on Hw0.

6 Proofs

6.1 Proof of Theorem 1

Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case,
it is more intricate and requires several new technical tools. To streamline the presentation of the proof, we
begin with proving a series of auxiliary lemmas in Subsection 6.1.1, and then move to the main proof in
Subsection 6.1. The main proof itself is divided into several steps, each constituting one or more lemmas.

Throughout the proof, we use the well-known facts that for all matrices B, C, D of suitable dimensions,
Tr(B + C) = Tr(B) + Tr(C), Tr(BC) = Tr(CB), Tr(BCD) = Tr(DBC), and Tr(B(cid:62)B) = (cid:107)B(cid:107)2
F .
Moreover, since Tr is a linear operation, E[Tr(B)] = E[Tr(B)] for a random matrix B.

12

6.1.1 Auxiliary Lemmas

Lemma 2. For any B, C, D (cid:23) 0, it holds that Tr(BC) ≥ Tr(B(C − D)) and Tr(BC) ≥ Tr((B − D)C).

Proof. It is enough to prove that for any positive semideﬁnite matrices E, G, it holds that Tr(EG) ≥ 0. The
lemma follows by taking either E = B, G = D (in which case, Tr(BC) = Tr(B(C − D)) + Tr(BD) ≥
Tr(B(C−D))), or E = D, G = C (in which case, Tr(BC) = Tr((B−D)C)+Tr(DC) ≥ Tr((B−D)C)).
Any positive semideﬁnite matrix M can be written as the product M 1/2M 1/2 for some symmetric matrix

M 1/2 (known as the matrix square root of M ). Therefore,

Tr(EG) = Tr(E1/2E1/2G1/2G1/2) = Tr(G1/2E1/2E1/2G1/2)

= Tr((E1/2G1/2)(cid:62)(E1/2G1/2)) = (cid:107)E1/2G1/2(cid:107)2

F ≥ 0.

Lemma 3. If B (cid:23) 0 and C (cid:31) 0, then

where I is the identity matrix.

Tr(BC−1) ≥ Tr(B(2I − C)),

Proof. We begin by proving the one-dimensional case, where B, C are scalars b ≥ 0, c > 0. The inequality
then becomes bc−1 ≥ b(2 − c), which is equivalent to 1 ≥ c(2 − c), or upon rearranging, (c − 1)2 ≥ 0,
which trivially holds.

Turning to the general case, we note that by Lemma 2, it is enough to prove that C−1 − (2I − C) (cid:23) 0.
To prove this, we make a couple of observations. The positive deﬁnite matrix C (like any positive deﬁnite
matrix) has a singular value decomposition which can be written as U SU (cid:62), where U is an orthogonal matrix,
and S is a diagonal matrix with positive entries. Its inverse is U S−1U (cid:62), and 2I − C = 2I − U SU (cid:62) =
U (2I − S)U (cid:62). Therefore,

C−1 − (2I − C) = U S−1U (cid:62) − U (2I − S)U (cid:62) = U (S−1 − (2I − S))U (cid:62).

To show this matrix is positive semideﬁnite, it is enough to show that each diagonal entry of S−1 − (2I − S)
is non-negative. But this reduces to the one-dimensional result we already proved, when b = 1 and c > 0 is
any diagonal entry in S. Therefore, C−1 − (2I − C) (cid:23) 0, from which the result follows.

Lemma 4. For any matrices B, C,

and

Tr(BC) ≤ (cid:107)B(cid:107)F (cid:107)C(cid:107)F

(cid:107)BC(cid:107)F ≤ (cid:107)B(cid:107)sp(cid:107)C(cid:107)F .

Proof. The ﬁrst inequality is immediate from Cauchy-Shwartz. As to the second inequality, letting ci denote
the i-th column of C, and (cid:107) · (cid:107)2 the Euclidean norm for vectors,

(cid:107)BC(cid:107)F =

(cid:115)(cid:88)

i

(cid:107)Bci(cid:107)2

2 ≤

(cid:115)(cid:88)

i

((cid:107)B(cid:107)sp(cid:107)ci(cid:107)2)2 = (cid:107)B(cid:107)sp

(cid:115)(cid:88)

i

(cid:107)ci(cid:107)2

2 = (cid:107)B(cid:107)sp(cid:107)C(cid:107)F .

13

Lemma 5. Let B1, B2, Z1, Z2 be k × k square matrices, where B1, B2 are ﬁxed and Z1, Z2 are stochastic
and zero-mean (i.e.
their expectation is the all-zeros matrix). Furthermore, suppose that for some ﬁxed
α, γ, δ > 0, it holds with probability 1 that

• For all ν ∈ [0, 1], B2 + νZ2 (cid:23) δI.

• max{(cid:107)Z1(cid:107)F , (cid:107)Z2(cid:107)F } ≤ α.

• (cid:107)B1 + ηZ1(cid:107)sp ≤ γ.

Then

E (cid:2)Tr (cid:0)(B1 + Z1)(B2 + Z2)−1(cid:1)(cid:3) ≥ Tr(B1B−1

2 ) −

α2(1 + γ/δ)
δ2

.

Proof. Deﬁne the function

f (ν) = Tr (cid:0)(B1 + νZ1)(B2 + νZ2)−1(cid:1) ,

ν ∈ [0, 1].

Since B2 + νZ2 is positive deﬁnite, it is always invertible, hence f (ν) is indeed well-deﬁned. Moreover, it
can be differentiated with respect to ν, and we have

f (cid:48)(ν) = Tr (cid:0)Z1(B2 + νZ2)−1 − (B1 + νZ1)(B2 + νZ2)−1Z2(B2 + νZ2)−1(cid:1) .

Again differentiating with respect to ν, we have

f (cid:48)(cid:48)(ν) = Tr

(cid:16)

− 2Z1(B2 + νZ2)−1Z2(B2 + νZ2)−1
+ 2(B1 + νZ1)(B2 + νZ2)−1Z2(B2 + νZ2)−1Z2(B2 + νZ2)−1(cid:17)
(cid:16)(cid:16)

(cid:17)

− Z1 + (B1 + νZ1)(B2 + νZ2)−1Z2

(B2 + νZ2)−1Z2(B2 + νZ2)−1(cid:17)

.

= 2 Tr

Using Lemma 4 and the triangle inequality, this is at most

2(cid:107) − Z1 + (B1 + νZ1)(B2 + νZ2)−1Z2(cid:107)F (cid:107)(B2 + νZ2)−1Z2(B2 + νZ2)−1(cid:107)F
≤ 2 (cid:0)(cid:107)Z1(cid:107)F + (cid:107)(B1 + νZ1)(B2 + νZ2)−1Z2(cid:107)F

(cid:1) (cid:107)(B2 + νZ2)−1(cid:107)2
(cid:17)

sp(cid:107)Z2(cid:107)F
(cid:107)(B2 + νZ2)−1(cid:107)2

sp(cid:107)Z2(cid:107)F

(cid:16)

≤ 2

(cid:107)Z1(cid:107)F + (cid:107)B1 + νZ1(cid:107)sp(cid:107) (B2 + νZ2)−1 (cid:107)sp(cid:107)Z2(cid:107)F
(cid:18)

1
δ

(cid:19) 1

δ2 α =

2α2(1 + γ/δ)
δ2

.

≤ 2

α + γ

α

Applying a Taylor expansion to f (·) around ν = 0, with a Lagrangian remainder term, and substituting the
values for f (cid:48)(ν), f (cid:48)(cid:48)(ν), we can lower bound f (1) as follows:

f (1) ≥ f (0) + f (cid:48)(0) ∗ (1 − 0) −

|f (cid:48)(cid:48)(ν)| ∗ (1 − 0)2

1
2
(cid:1) + Tr (cid:0)Z1B−1
2 − B1B−1

max
ν

= Tr (cid:0)B1B−1

2

2 Z2B−1
2

(cid:1) −

α2(1 + γ/δ)
δ2

.

Taking expectation over Z1, Z2, and recalling they are zero-mean, we get that

E[f (1)] ≥ Tr (cid:0)B1B−1

2

(cid:1) −

α2(1 + γ/δ)
δ2

.

Since E[f (1)] = E (cid:2)Tr (cid:0)(B1 + Z1)(B2 + Z2)−1(cid:1)(cid:3), the result in the lemma follows.

14

Lemma 6. Let U1, . . . , Uk and R1, R2 be positive semideﬁnite matrices, such that R2 − R1 (cid:23) 0, and deﬁne
the function

f (x1 . . . xk) = Tr





(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33) (cid:32) k

(cid:88)

i=1

xiUi + R2

(cid:33)−1
 .

over all (x1 . . . xk) ∈ [α, β]d for some β ≥ α ≥ 0. Then min(x1...xk)∈[α,β]d f (x) = f (α, . . . , α).
Proof. Taking a partial derivative of f with respect to some xj, we have

∂
∂xj

f (x)



= Tr

Uj

(cid:33)−1

xiUi + R2

−

(cid:32) k

(cid:88)

i=1

(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33) (cid:32) k

(cid:88)

i=1

(cid:33)−1

xiUi + R2

Uj

(cid:33)−1


xiUi + R2

(cid:32) k

(cid:88)

i=1





= Tr



I −

(cid:32) k

(cid:88)

i=1

= Tr

(cid:32)(cid:32) k

(cid:88)

i=1







= Tr

(R2 − R1)

(cid:33) (cid:32) k

(cid:88)

Ui + R1

(cid:33)−1

xiUi + R2

 Uj

(cid:33)−1


xiUi + R2

(cid:32) k

(cid:88)

i=1

(cid:33)

xiUi + R2

−

i=1

(cid:32) k

(cid:88)

i=1

xiUi + R1

(cid:33)(cid:33) (cid:32) k

(cid:88)

i=1

(cid:33)−1

xiUi + R2

Uj

(cid:33)−1


xiUi + R2

(cid:32) k

(cid:88)

i=1

(cid:33)−1

xiUi + R2

Uj

(cid:32) k

(cid:88)

i=1

(cid:32) k

(cid:88)

i=1

(cid:33)−1
 .

xiUi + R2

By the lemma’s assumptions, each matrix in the product above is positive semideﬁnite, hence the product
f (x) ≥ 0, which implies that the
is positive semideﬁnite, and the trace is non-negative. Therefore,
function is minimized when each xj takes its smallest possible value, i.e. α.

∂
∂xj

Lemma 7. Let B be a k × k matrix with minimal singular value δ. Then

1 −

(cid:107)B(cid:62)B(cid:107)2
F
(cid:107)B(cid:107)2
F

(cid:26)

≥ max

1 − (cid:107)B(cid:107)2

F ,

(cid:0)k − (cid:107)B(cid:107)2
F

(cid:1)

(cid:27)

.

δ2
k

Proof. We have

1 −

(cid:107)B(cid:62)B(cid:107)2
F
(cid:107)B(cid:107)2
F
so it remains to prove 1 − (cid:107)B(cid:62)B(cid:107)2
(cid:0)k − (cid:107)B(cid:107)2
≥ δ2
F
(cid:107)B(cid:107)2
k
F
of B. The singular values of B(cid:62)B are σ2
1, . . . , σ2
norm of its vector of singular values. Therefore, the lemma is equivalent to requiring

F (cid:107)B(cid:107)2
F
(cid:107)B(cid:107)2
F
(cid:1). Let σ1, . . . , σk denote the vector of singular values
k, and the Frobenius norm of a matrix equals the Euclidean

= 1 − (cid:107)B(cid:107)2
F ,

≥ 1 −

(cid:107)B(cid:107)2

F

1 −

(cid:80)k

(cid:80)k

i=1 σ4
i
i=1 σ2
i

≥

δ2
k

(cid:32)

k −

(cid:33)

σ2
i

,

k
(cid:88)

i=1

assuming σi ∈ [δ, 1] for all i. This holds since
(cid:80)

(cid:80)

1 −

=

(cid:80)
(cid:80)

i σ4
i
i σ2
i

i − (cid:80)
i σ2
(cid:80)
i σ2
i

i σ4
i

=

i σ2
i
(cid:80)

(cid:0)1 − σ2
i
i σ2
i

(cid:1)

≥

δ2 (cid:80)
i

(cid:1)

(cid:0)1 − σ2
i
k

=

δ2
k

(cid:32)

k −

(cid:33)

σ2
i

.

(cid:88)

i

15

Lemma 8. For any d × k matrices C, D with orthonormal columns, let

DC = arg

min
DB : (DB)(cid:62)(DB)=I

(cid:107)C − DB(cid:107)2
F

be the nearest orthonormal-columns matrix to C in the column space of D (where B is a k ×k matrix). Then
the matrix B minimizing the above equals B = V U (cid:62), where C(cid:62)D = U SV (cid:62) is the SVD decomposition of
C(cid:62)D, and it holds that

(cid:107)C − DC(cid:107)2

F ≤ 2(k − (cid:107)C(cid:62)D(cid:107)2

F ).

Proof. Since D has orthonormal columns, we have D(cid:62)D = I, so the deﬁnition of B is equivalent to

B = arg min

B : B(cid:62)B=I

(cid:107)C − DB(cid:107)2
F .

This is the orthogonal Procrustes problem (see e.g. [8]), and the solution is easily shown to be B = V U (cid:62)
where U SV (cid:62) is the SVD decomposition of C(cid:62)D. In this case, and using the fact that (cid:107)C(cid:107)2
F = k
(as C, D have orthonormal columns), we have that (cid:107)C − DC(cid:107)2

F = (cid:107)D(cid:107)2

F equals

(cid:107)C − DB(cid:107)2

F = (cid:107)C(cid:107)2

F + (cid:107)D(cid:107)2

F − 2 Tr(C(cid:62)DB) = 2

(cid:16)

(cid:17)
k − Tr(U SV (cid:62)(V U (cid:62)))

(cid:16)

(cid:17)
k − Tr(U SU (cid:62))

.

= 2

Since the trace function is similarity-invariant, this equals 2k − Tr(S). Let s1 . . . , sk be the diagonal ele-
ments of S, and note that they can be at most 1 (since they are the singular values of C(cid:62)D, and both C and
D have orthonormal columns). Recalling that the Frobenius norm equals the Euclidean norm of the singular
values, we can therefore upper bound the above as follows:

(cid:16)

(cid:17)
k − Tr(U SU (cid:62))

2

= 2 (k − Tr(S)) = 2

k −

(cid:32)

k
(cid:88)

i=1

(cid:33)

(cid:32)

si

≤ 2

k −

(cid:33)

k
(cid:88)

i=1

s2
i

(cid:16)

= 2

k − (cid:107)C(cid:62)D(cid:107)2
F

(cid:17)

.

Lemma 9. Let Wt, W (cid:48)
Vk with orthonormal columns, it holds that

t be as deﬁned in Algorithm 2, where we assume η < 1

3 . Then for any d × k matrix

(cid:12)
(cid:12)(cid:107)V (cid:62)
(cid:12)

k Wt(cid:107)2

F − (cid:107)V (cid:62)

k Wt−1(cid:107)2
F

(cid:12)
(cid:12)
(cid:12) ≤

12kη
1 − 3η

.

Proof. Letting st, st−1 denote the vectors of singular values of V (cid:62)
k Wt−1, and noting that they
are both in [0, 1]k (as Vk, Wt−1, Wt all have orthonormal columns), the left hand side of the inequality in the
lemma statement equals

k Wt and V (cid:62)

|(cid:107)st(cid:107)2 − (cid:107)st−1(cid:107)2| = ((cid:107)st(cid:107)2 + (cid:107)st−1(cid:107)2) | (cid:107)st(cid:107)2 − (cid:107)st−1(cid:107)2 | ≤ 2

√

k(cid:107)st − st−1(cid:107)2 ≤ 2k(cid:107)st − st−1(cid:107)∞,

where (cid:107) · (cid:107)∞ is the inﬁnity norm. By Weyl’s matrix perturbation theorem6 [12], this is upper bounded by

2k(cid:107)V (cid:62)

k Wt − V (cid:62)

k Wt−1(cid:107)sp ≤ 2k(cid:107)Vk(cid:107)sp(cid:107)Wt − Wt−1(cid:107)sp ≤ 2k(cid:107)Wt − Wt−1(cid:107)sp.

(10)

6Using its version for singular values, which implies that the singular values of matrices B and B + E are different by at most

(cid:107)E(cid:107)sp.

16

Recalling the relationship between Wt and Wt−1 from Algorithm 2, we have that

W (cid:48)

t = Wt−1 + ηN,

where

(cid:107)N (cid:107)sp ≤ (cid:107)xitx(cid:62)

it Wt−1(cid:107)sp + (cid:107)xitx(cid:62)
it

˜Ws−1Bt−1(cid:107)sp + (cid:107)

1
n

n
(cid:88)

i=1

xix(cid:62)
i

˜Ws−1Bt−1(cid:107)sp ≤ 3,

as Wt−1, ˜Ws−1, Bt−1 all have orthonormal columns, and xitx(cid:62)
i have spectral norm at
most 1. Therefore, W (cid:48)
t equals Wt−1, up to a matrix perturbation of spectral norm at most 3η. Again by
Weyl’s theorem, this implies that the k non-zero singular values of the d × k matrix W (cid:48)
t are different from
those of Wt−1 (which has orthonormal columns) by at most 3η, and hence all lie in [1 − 3η, 1 + 3η]. As a

i=1 xix(cid:62)

it and 1
n

(cid:80)n

result, the singular values of

(cid:16)

W (cid:48)(cid:62)

t W (cid:48)
t

(cid:17)−1/2

all lie in

(cid:104) 1
1+3η ,

1
1−3η

(cid:105)
. Collecting these observations, we have

(cid:107)Wt − Wt−1(cid:107)sp = (cid:107)(Wt−1 + ηN )

≤ (cid:107)Wt−1

(cid:18)(cid:16)

W

(cid:48)(cid:62)
t−1W (cid:48)

t−1

(cid:17)−1/2

− I

(cid:16)

W

(cid:19)

(cid:48)(cid:62)
t−1W (cid:48)
(cid:16)

+ ηN

(cid:17)−1/2

− Wt−1(cid:107)sp

t−1

W

(cid:48)(cid:62)
t−1W (cid:48)

t−1

(cid:17)−1/2

(cid:107)sp

(cid:16)

W

≤ (cid:107)

(cid:48)(cid:62)
t−1W (cid:48)

(cid:17)−1/2

− I(cid:107)sp + η(cid:107)N (cid:107)sp(cid:107)

(cid:16)

W

(cid:48)(cid:62)
t−1W (cid:48)

t−1

(cid:17)−1/2

(cid:107)sp

≤

3η
1 − 3η

+

t−1
3η
1 − 3η

=

6η
1 − 3η

.

Plugging back to Eq. (10), the result follows.

6.1.2 Main Proof

To simplify the technical derivations, note that the algorithm remains the same if we divide each xi by
r,
and multiply η by r. Since maxi (cid:107)xi(cid:107)2 ≤ r, this corresponds to running the algorithm with step-size ηr
rather than η, on a re-scaled dataset of points with squared norm at most 1, and with an eigengap of λ/r
instead of λ. Therefore, we can simply analyze the algorithm assuming that maxi (cid:107)xi(cid:107)2 ≤ 1, and in the end
plug in λ/r instead of λ, and ηr instead of η, to get a result which holds for data with squared norm at most
r.

√

Part I: Establishing a Stochastic Recurrence Relation

We begin by focusing on a single iteration t of the algorithm, and analyze how (cid:107)V (cid:62)
F (which measures
the similarity between the column spaces of Vk and Wt) evolves during that iteration. The key result we
need is Lemma 10 below, which is specialized for our algorithm in Lemma 11.

k Wt(cid:107)2

Lemma 10. Let A be a d × d symmetric matrix with all eigenvalues s1 ≥ s2 ≥ . . . ≥ sd in [0, 1], and
suppose that sk − sk+1 ≥ λ for some λ > 0.

Let N be a d × k zero-mean random matrix such that (cid:107)N (cid:107)F ≤ σF

1, and deﬁne

(cid:32)

rN = 46 (σF

N )2

1 +

(cid:18) 1
4

8
3

σsp
N + 2

17

N and (cid:107)N (cid:107)sp ≤ σsp
(cid:19)2(cid:33)

N with probability

Let W be a d × k matrix with orthonormal columns, and deﬁne

W (cid:48) = (I + ηA)W + ηN , W (cid:48)(cid:48) = W (cid:48)(W

(cid:48)(cid:62)W (cid:48))−1/2,

for some η ∈

(cid:104)

0,

1
4 max{1,σF

N }

(cid:105)

.

k W (cid:48)(cid:48)(cid:107)2
F
F ≥ k − 1

2 , then

• If (cid:107)V (cid:62)

k W (cid:107)2

If Vk = [v1, v2 . . . , vk] is the d × k matrix of A’s ﬁrst k eigenvectors, then the following holds:
• E (cid:2)1 − (cid:107)V (cid:62)

(cid:1) (cid:0)1 − (cid:107)V (cid:62)

(cid:3) ≤ (cid:0)1 − 4

(cid:1) + η2rN

5 ηλ(cid:107)V (cid:62)

k W (cid:107)2
F

k W (cid:107)2
F

(cid:104)

EN

k − (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:16)

(cid:105)

≤

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17) (cid:18)

1 −

(cid:19)

ηλ

1
10

+ η2rN .

Proof. Using the fact that Tr(BCD) = Tr(CDB) for any matrices B, C, D, we have

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

= E

= E

= E

(cid:104)

Tr
(cid:20)

Tr

(cid:20)

Tr

By deﬁnition of W (cid:48), we have

(cid:48)(cid:48)(cid:17)(cid:105)

(cid:16)

W
(cid:18)(cid:16)

(cid:48)(cid:48)(cid:62)VkV (cid:62)
k W
(cid:48)(cid:62)W (cid:48)(cid:17)−1/2

W

(cid:18)(cid:16)

(cid:48)(cid:62)VkV (cid:62)

k W (cid:48)(cid:17) (cid:16)
W

W

W

(cid:48)(cid:62)VkV (cid:62)

W

k W (cid:48) (cid:16)
(cid:48)(cid:62)W (cid:48)(cid:17)−1(cid:19)(cid:21)

(cid:48)(cid:62)W (cid:48)(cid:17)−1/2(cid:19)(cid:21)

.

(11)

W

where we deﬁne

Also, we have

where

(cid:48)(cid:62)VkV (cid:62)

k W (cid:48) = ((I + ηA)W + ηN )(cid:62) VkV (cid:62)

k ((I + ηA)W + ηN )

= B1 + Z1,

B1 = W (cid:62)(I + ηA)VkV (cid:62)
Z1 = ηN (cid:62)VkV (cid:62)

k (I + ηA)W + η2N (cid:62)VkV (cid:62)
k (I + ηA)W + ηW (cid:62)(I + ηA)VkV (cid:62)

k N
k N.

W

(cid:48)(cid:62)W (cid:48) = ((I + ηA)W + ηN )(cid:62) ((I + ηA)W + ηN )

= B2 + Z2,

B2 = W (cid:62)(I + ηA)(I + ηA)W + η2N (cid:62)N
Z2 = ηN (cid:62)(I + ηA)W + ηW (cid:62)(I + ηA)N.
With these deﬁnitions, we can rewrite Eq. (11) as E (cid:2)Tr((B1 + Z1)(B2 + Z2)−1)(cid:3). We now wish to remove
Z1, Z2, by applying Lemma 5. To do so, we check the lemma’s conditions:

• Z1, Z2 are zero mean: This holds since they are linear in N , and N is assumed to be zero-mean.

18

• B2 + νZ2 (cid:23) 3

8 I for all ν ∈ [0, 1]: Recalling the deﬁnition of B2, Z2, and the facts that A (cid:23) 0,
N (cid:62)N (cid:23) 0 (by construction), and W (cid:62)W = I, we have that B2 (cid:23) I. Moreover, the spectral norm of
Z2 is at most

2η(cid:107)N (cid:62)(I + ηA)W (cid:107)sp ≤ 2η(cid:107)N (cid:107)sp(cid:107)I + ηA(cid:107)sp(cid:107)W (cid:107)sp ≤ 2ησsp

N (1 + η) ≤ 2ησF

N (1 + η),

which by the assumption on η is at most 2 1
4
of B2 + νZ2 is at least 1 − ν(5/8) ≥ 3/8.

(cid:0)1 + 1
4

(cid:1) = 5

8 . This implies that the smallest singular value

• max{(cid:107)Z1(cid:107)F , (cid:107)Z2(cid:107)F } ≤ 5

2 ησF

these two matrices is at most

N : By deﬁnition of Z1, Z2, and using Lemma 4, the Frobenius norm of

2η(cid:107)N (cid:107)F (cid:107)(I + ηA)(cid:107)sp(cid:107)W (cid:107)sp ≤ 2ησF

N (1 + η),

which by the assumption on η is at most 2ησF
N

(cid:0)1 + 1
4

(cid:1) = 5

2 ησF
N .

• (cid:107)B1 + ηZ1(cid:107)sp ≤ (cid:0) 1

4 σsp

N + 2(cid:1)2

: Using the deﬁnition of B1, Z1 and the assumption η ≤ 1
4 ,

(cid:107)B1 + ηZ1(cid:107)sp ≤ (cid:107)B1(cid:107)sp + η(cid:107)Z1(cid:107)sp
≤ (1 + η)2 + η2(σsp

N (1 + η)

N )2 + 2ησsp
5
8

σsp
N

N )2 +

(σsp

≤

<

(cid:18) 5
4
(cid:18) 1
4

(cid:19)2

+

1
16

σsp
N + 2

(cid:19)2

.

Applying Lemma 5 and plugging back to Eq. (11), we get

(cid:104)

E

(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ E (cid:2)Tr((B1 + Z1)(B2 + Z2)−1)(cid:3)

≥ Tr (cid:0)B1B−1

2

(cid:1) −

(cid:32)

(ησF

N )2

1 +

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

σsp
N + 2

400
9

(12)

We now turn to lower bound Tr (cid:0)B1B−1
let

2

(cid:1), by ﬁrst re-writing B1, B2 in a different form. For i = 1, . . . , d,

Ui = W (cid:62)viv(cid:62)

i W,

where vi is the eigenvector of A corresponding to the eigenvalue si. Note that each Ui is positive semideﬁ-
nite, and (cid:80)d

i=1 Ui = W (cid:62)W = I. We have

B1 = W (cid:62)(I + ηA)VkV (cid:62)

k (I + ηA)W + η2N (cid:62)VkV (cid:62)

k N

= W (cid:62) ((I + ηA)Vk) ((I + ηA)Vk)(cid:62) W + η2N (cid:62)VkV (cid:62)

k N

k
(cid:88)

(1 + ηsi)2W (cid:62)viv(cid:62)

i W + η2N (cid:62)VkV (cid:62)

k N

i=1
k
(cid:88)

(1 + ηsi)2Ui + η2N (cid:62)VkV (cid:62)

k N.

=

=

i=1

19

(13)

Similarly,

B2 = W (cid:62)(I + ηA)(I + ηA)W + η2N (cid:62)N

=

=

d
(cid:88)

i=1
d
(cid:88)

i=1

(1 + ηsi)2W (cid:62)viv(cid:62)

i W + η2N (cid:62)N

(1 + ηsi)2Ui + η2N (cid:62)N.

(14)

Plugging Eq. (13) and Eq. (14) back into Eq. (12), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ Tr





(cid:32) k

(cid:88)

(1 + ηs1)2Ui + η2N (cid:62)VkV (cid:62)

k N

i=1

−

400
9

(cid:32)

(ησF

N )2

1 +

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

σsp
N + 2

(1 + ηsi)2Ui + η2N (cid:62)N

(cid:33)−1


(cid:33) (cid:32) d

(cid:88)

i=1

(15)

Recalling that s1 ≥ s2 ≥ . . . ≥ sk and letting α = (1 + ηsk)2, β = (1 + ηs1)2, the trace term can be

lower bounded by



min
x1,...,xk∈[α,β]

Tr



(cid:32) k

(cid:88)

i=1

xiUi + η2N (cid:62)VkV (cid:62)

k N

(cid:33) (cid:32) k

(cid:88)

d
(cid:88)

xiUi +

(1 + ηsi)2Ui + η2N (cid:62)N

(cid:33)−1
 .

i=1

i=k+1

Applying Lemma 6 (noting that as required by the lemma, (cid:80)d
(cid:80)d

i=k+1(1 + ηsi)2Ui + η2N (cid:62) (cid:0)I − VkV (cid:62)


k

(cid:33) (cid:32)

(cid:32)

Tr



(1 + ηsk)2

Ui + η2N (cid:62)VkV (cid:62)

k N

(1 + ηsk)2

k
(cid:88)

i=1

Using Lemma 2, this can be lower bounded by

i=k+1(1+ηsi)2Ui+η2N (cid:62)N −η2N (cid:62)VkV (cid:62)

k N =

(cid:1) N (cid:23) 0), we can lower bound the above by

d
(cid:88)

Ui +

(1 + ηsi)2Ui + η2N (cid:62)N

(cid:33)−1
 .

i=k+1

k
(cid:88)

i=1



(cid:32)

Tr



(1 + ηsk)2

(cid:33) (cid:32)

(1 + ηsk)2

k
(cid:88)

i=1

Ui

k
(cid:88)

i=1

d
(cid:88)

Ui +

(1 + ηsi)2Ui + η2N (cid:62)N

(cid:33)−1


i=k+1



(cid:32) k

(cid:88)

= Tr



(cid:33) (cid:32) k

(cid:88)

Ui

Ui +

d
(cid:88)

i=1

i=1

i=k+1

Applying Lemma 3, this is at least

(cid:19)2

(cid:18) 1 + ηsi
1 + ηsk

Ui +

(cid:18) η

(cid:19)2

1 + ηsk

(cid:33)−1


N (cid:62)N

(cid:32)(cid:32) k

(cid:88)

Tr

i=1

(cid:33) (cid:32)

Ui

2I −

k
(cid:88)

i=1

Ui −

d
(cid:88)

i=k+1

(cid:19)2

(cid:18) 1 + ηsi
1 + ηsk

Ui −

(cid:18) η

(cid:19)2

1 + ηsk

(cid:33)(cid:33)

N (cid:62)N

.

20

Recalling that I = (cid:80)d

i=1 Ui = (cid:80)k
(cid:33) (cid:32) k

i=1 Ui + (cid:80)d
(cid:32)

(cid:88)

Ui +

d
(cid:88)

(cid:32)(cid:32) k

(cid:88)

Tr

Ui

i=1

i=1

i=k+1

i=k+1 Ui, this can be simpliﬁed to

2 −

(cid:19)2(cid:33)

(cid:18) 1 + ηsi
1 + ηsk

Ui −

(cid:18) η

(cid:19)2

1 + ηsk

(cid:33)(cid:33)

N (cid:62)N

.

(16)

Since Ui (cid:23) 0, then using Lemma 3, we can lower bound the expression above by shrinking each of the
(cid:18)

terms. In particular, since si ≤ sk − λ for each i ≥ k + 1,

2 −

(cid:16) 1+ηsi
1+ηsk

(cid:17)2(cid:19)

2 −

(cid:19)2

(cid:18) 1 + ηsi
1 + ηsk

≥ 2 −

1 + ηsi
1 + ηsk

≥ 2 −

1 + η(sk − λ)
1 + ηsk

= 1 +

ηλ
1 + ηsk

,

which by the assumption that η ≤ 1/4 and sk ≤ s1 ≤ 1, is at least 1+ 4
and recalling that (cid:80)d

i=1 Ui = I, we get the lower bound

5 ηλ. Plugging this back into Eq. (16),

(cid:32)(cid:32) k

(cid:88)

Tr

Ui

(cid:33) (cid:32) k

(cid:88)

Ui +

d
(cid:88)

(cid:18)

1 +

(cid:19)

ηλ

Ui −

4
5

(cid:18) η

(cid:19)2

1 + ηsk

N (cid:62)N

(cid:33)(cid:33)

i=1
(cid:32)(cid:32) k

(cid:88)

i=1

i=1
(cid:33) (cid:32)

Ui

I +

i=k+1
(cid:32)

ηλ

I −

4
5

= Tr

Again using Lemma 2, this is at least

(cid:33)

Ui

−

k
(cid:88)

i=1

(cid:18) η

(cid:19)2

1 + ηsk

(cid:33)(cid:33)

N (cid:62)N

.

(cid:32)(cid:32) k

(cid:88)

Tr

(cid:33) (cid:32)

Ui

I +

(cid:33)(cid:33)(cid:33)

(cid:32)

ηλ

I −

4
5

k
(cid:88)

i=1

Ui

−

(cid:18) η

(cid:19)2

1 + ηsk

i=1
(cid:32)(cid:32) k

(cid:88)

i=1
(cid:32)(cid:32) k

(cid:88)

i=1

≥ Tr

≥ Tr

(cid:33) (cid:32)

Ui

I +

(cid:33) (cid:32)

Ui

I +

(cid:33)(cid:33)(cid:33)

(cid:33)(cid:33)(cid:33)

(cid:32)

ηλ

I −

(cid:32)

ηλ

I −

4
5

4
5

k
(cid:88)

i=1
k
(cid:88)

i=1

Ui

Ui

−

(cid:18) η

1 + ηsk

− η2 (cid:0)σF
N

(cid:1)2

.

Tr

(cid:19)2

(cid:32)(cid:32) k

(cid:88)

(cid:33)

(cid:33)

Ui

N (cid:62)N

i=1

(cid:16)

Tr

N (cid:62)N

(cid:17)

Recall that this is a lower bound on the trace term in Eq. (15). Plugging it back and slightly simplifying, we
get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ Tr

(cid:32)(cid:32) k

(cid:88)

(cid:33) (cid:32)

Ui

I +

ηλ

I −

(cid:32)

(cid:33)(cid:33)(cid:33)

− η2rN ,

4
5

k
(cid:88)

i=1

Ui

(cid:18) 1
4

8
3

(cid:19)2(cid:33)

.

σsp
N + 2

where

i=1

(cid:32)

rN = 46 (σF

N )2

1 +

21

The trace term above can be re-written (using the deﬁnition of Ui and the fact that Tr(B(cid:62)B) = (cid:107)B(cid:107)2

F ) as

(cid:32)(cid:32)

Tr

W (cid:62)

(cid:18)

=

1 +

(cid:18)

=

1 +

4
5
4
5

k
(cid:88)

i=1
(cid:19)

(cid:19)

ηλ

= (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:33) (cid:32)

viv(cid:62)

i W

I +

(cid:32)

ηλ

I − W (cid:62)

4
5

k
(cid:88)

i=1

viv(cid:62)

i W

(cid:33)(cid:33)(cid:33)

ηλ

Tr

(cid:16)

W (cid:62)VkV (cid:62)

k W

(cid:17)

−

4
5

(cid:16)(cid:16)

ηλ Tr

W (cid:62)VkV (cid:62)

k W

(cid:17) (cid:16)

W (cid:62)VkV (cid:62)

k W

(cid:17)(cid:17)

4
5

(cid:107)V (cid:62)

F −

k W (cid:107)2
(cid:18)

(cid:18)

1 +

ηλ

1 −

4
5

ηλ(cid:107)W (cid:62)VkV (cid:62)

k W (cid:107)2
F

k W (cid:107)2
F

(cid:107)W (cid:62)VkV (cid:62)
k W (cid:107)2
(cid:107)V (cid:62)
F

(cid:19)(cid:19)

.

Applying Lemma 7, and letting δ denote the minimal singular value of V (cid:62)

k W , this is lower bounded by

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

4
5

ηλ max

(cid:26)

1 − (cid:107)V (cid:62)

k W (cid:107)2

F ,

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:27)(cid:19)

.

(cid:16)

δ2
k

Overall, we get that

(cid:104)

E

(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

4
5

(cid:26)

ηλ max

We now consider two options:

1 − (cid:107)V (cid:62)

k W (cid:107)2

F ,

(cid:16)

δ2
k

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:27)(cid:19)

− η2rN .

(17)

• Taking the ﬁrst argument of the max term in Eq. (17), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

(cid:16)

ηλ

4
5

1 − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:19)

− η2rN .

Subtracting 1 from both sides and simplifying, we get

E

(cid:104)
1 − (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

(cid:18)

≤

1 −

4
5

ηλ(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:19) (cid:16)

1 − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)

+ η2rN .

• Suppose that (cid:107)V (cid:62)

k W (cid:107)2

F ≥ k − 1

2 . Taking the second argument of the max term in Eq. (17), we get

E

(cid:104)
(cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

≥ (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:18)

1 +

(cid:16)

4ηλδ2
5k

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)(cid:19)

− η2rN .

Subtracting both sides from k, , we get

E

(cid:104)
k − (cid:107)V (cid:62)

k W (cid:48)(cid:48)(cid:107)2
F

(cid:105)

(cid:16)

(cid:16)

(cid:16)

≤

=

≤

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:16)

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17)

+ η2rN

4ηλδ2
5k
4ηλδ2
5k

(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:19)

+ η2rN

(cid:19)(cid:19)

(cid:18)

k −

1
2

+ η2rN

k − (cid:107)V (cid:62)

k W (cid:107)2
F

k − (cid:107)V (cid:62)

k W (cid:107)2
F

4ηλδ2
5k

(cid:17)

−
(cid:17) (cid:18)

1 −

k − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:17) (cid:18)

1 −

22

Since k ≥ 1, we can lower bound the (cid:0)k − 1
2
k W satisfy k − (cid:80)k
implies that the singular values σ1, . . . , σk of V (cid:62)
(as Vk, W have orthonormal columns), so no σi can be less than 1
2 and δ ≥ 1
the lower bounds k − 1
2 into the above, we get
(cid:17) (cid:18)
(cid:105)

(cid:1) term by k

2 ≥ k

(cid:16)

(cid:104)
k − (cid:107)V (cid:62)

E

k W (cid:48)(cid:48)(cid:107)2
F

≤

k − (cid:107)V (cid:62)

k W (cid:107)2
F

2 . Moreover, the condition k − (cid:107)V (cid:62)

k W (cid:107)2

F ≤ 1
2
2 . But each σi is in [0, 1]
2 . Plugging

2 . This implies that δ ≥ 1

i=1 σ2

i ≤ 1

1 −

1
10

(cid:19)

ηλ

+ η2rN .

Lemma 11. Let A, Wt be as deﬁned in Algorithm 2, and suppose that η ∈
holds for some positive numerical constants c1, c2, c3:

(cid:104)
0,

(cid:105)

1
√
23

k

. Then the following

• E (cid:2)1 − (cid:107)V (cid:62)

(cid:3) ≤ (cid:0)1 − c1ηλ(cid:107)V (cid:62)

k W (cid:107)2
F

(cid:1) (cid:0)1 − (cid:107)V (cid:62)

k W (cid:107)2
F

(cid:1) + c2kη2

k W (cid:48)(cid:48)(cid:107)2
F
F ≥ k − 1

k Wt+1(cid:107)2
F

2 , then
(cid:105)

≤

• If (cid:107)V (cid:62)

k Wt(cid:107)2
(cid:104)
k − (cid:107)V (cid:62)

E

(cid:16)

k − (cid:107)V (cid:62)

k Wt(cid:107)2
F

(cid:17)

(1 − c1η (λ − c2η)) + c3η2(k − (cid:107)V (cid:62)
k

˜Ws−1(cid:107)2

F ).

In the above, the expectation is over the random draw of the index it, conditioned on Wt and ˜Ws−1.
Proof. To apply Lemma 10, we need to compute upper bounds σF
norms of N , which in our case equals (xitx(cid:62)
it
Wt, ˜Ws−1, Bt have orthonormal columns, the spectral norm of N is at most

N and σsp
− A)(Wt − ˜Ws−1Bt). Since (cid:107)A(cid:107)sp, (cid:107)xitx(cid:62)
it

N on the Frobenius and spectral
(cid:107)sp ≤ 1, and

(cid:107)(xitx(cid:62)

it − A)(Wt − ˜Ws−1Bt)(cid:107)sp ≤

(cid:16)

(cid:107)xitx(cid:62)

it (cid:107)sp + (cid:107)A(cid:107)sp

(cid:17) (cid:16)

(cid:107)Wt(cid:107)sp + (cid:107) ˜Ws−1(cid:107)sp(cid:107)Bt(cid:107)sp

(cid:17)

≤ 4,

so we may take σsp

N = 4. As to the Frobenius norm, using Lemma 4 and a similar calculation, we have

To upper bound this, deﬁne

(cid:107)N (cid:107)2

F ≤ 4(cid:107)Wt − ˜Ws−1Bt(cid:107)2
F .

VWt = arg

min
VkB:(VkB)(cid:62)(VkB)=I

(cid:107)Wt − VkB(cid:107)2
F

to be the nearest orthonormal-columns matrix to Wt in the column space of Vk, and
(cid:107)VWt − ˜Ws−1B(cid:107)2
F

˜WV = arg

min
˜Ws−1B:( ˜Ws−1B)(cid:62)( ˜Ws−1B)=I

to be the nearest orthonormal-columns matrix to VWt in the column space of ˜Ws−1. Also, recall that by
deﬁnition,

˜Ws−1Bt = arg

(cid:107)Wt − ˜Ws−1B(cid:107)2
F

min
˜Ws−1B:( ˜Ws−1B)(cid:62)( ˜Ws−1B)=I
is the nearest orthonormal-columns matrix to Wt in the column space of ˜Ws−1. Therefore, we must have
(cid:107)Wt − ˜Ws−1Bt(cid:107)2
F . Using this and Lemma 8, we have
F ≤ (cid:107)Wt − ˜WV (cid:107)2
F
= (cid:107)(Wt − VWt) − ( ˜WV − VWt)(cid:107)2
F
≤ 2(cid:107)Wt − VWt(cid:107)2
k − (cid:107)V (cid:62)

F ≤ (cid:107)Wt − ˜WV (cid:107)2
(cid:107)Wt − ˜Ws−1Bt(cid:107)2

F + 2(cid:107) ˜WV − VWt(cid:107)2
F
(cid:16)
k − (cid:107)V (cid:62)
Wt

˜Ws−1(cid:107)2
F

k Wt(cid:107)2
F

= 4

+ 4

(cid:17)

(cid:16)

(cid:17)

.

23

By deﬁnition of VWt, we have VWt = VkB where B(cid:62)B = B(cid:62)V (cid:62)
is an orthogonal k × k matrix, and (cid:107)V (cid:62)
Wt
˜Ws−1(cid:107)2
4(k − (cid:107)V (cid:62)
k Wt(cid:107)2
upper bounded by

k VkB = (VkB)(cid:62)(VkB) = I. Therefore B
˜Ws−1(cid:107)2
F = (cid:107)V (cid:62)
F , so the above equals
k
F ). Overall, we get that the squared Frobenius norm of N can be

F ) + 4(k − (cid:107)V (cid:62)
k

F = (cid:107)B(cid:62)V (cid:62)
k

˜Ws−1(cid:107)2

˜Ws−1(cid:107)2

(σF

N )2 = 16

(k − (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k − (cid:107)V (cid:62)
k

(cid:16)

˜Ws−1(cid:107)2
F )

(cid:17)

.

Plugging σsp

N and (σF

N )2 into the rN as deﬁned in Lemma 10, and picking any η ∈ [0,

1
√
23

k

] (which satisﬁes

√

the condition in Lemma 10 that η ∈

, since 4 max{1, σF

n } ≤ 4 max{1,

16 ∗ 2k} <

(cid:104)

0,

1
4 max{1,σF

N }

(cid:105)

√

23

k), we get

(cid:16)

rN = 736

(k − (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k − (cid:107)V (cid:62)
k

(cid:17)
˜Ws−1(cid:107)2
F )

(cid:32)

1 +

(cid:18) 1
4

8
3

4 + 2

(cid:19)2(cid:33)

(cid:16)

≤ 18400

(k − (cid:107)V (cid:62)

k Wt(cid:107)2

F ) + (k − (cid:107)V (cid:62)
k

(cid:17)
˜Ws−1(cid:107)2
F )

.

This implies that rN ≤ 36800k always, which by application of Lemma 10, gives the ﬁrst part of our lemma.
As to the second part, assuming (cid:107)V (cid:62)

k Wt(cid:107)2

F ≥ k − 1

E

(cid:104)
k − (cid:107)V (cid:62)

k Wt+1(cid:107)2
F

(cid:16)

(cid:105)

≤

(cid:16)

=

k Wt(cid:107)2
k − (cid:107)V (cid:62)
F
+ 18400 η2 (cid:16)

k − (cid:107)V (cid:62)

k Wt(cid:107)2
F

2 and applying Lemma 10, we get that
(cid:19)

(cid:17) (cid:18)

1 −

(k − (cid:107)V (cid:62)
(cid:17) (cid:18)

1 − η

F ) + (k − (cid:107)V (cid:62)
k
(cid:19)(cid:19)

λ − 18400η

(cid:17)

˜Ws−1(cid:107)2
F )

ηλ

1
10
k Wt(cid:107)2
(cid:18) 1
10
˜Ws−1(cid:107)2

+ 18400 η2(k − (cid:107)V (cid:62)
k

F ).

This corresponds to the lemma statement.

Part II: Solving the Recurrence Relation for a Single Epoch
Since we focus on a single epoch, we drop the subscript from ˜Ws−1 and denote it simply as ˜W .
Suppose that η = αλ, where α is a sufﬁciently small constant to be chosen later. Also, let

bt = k − (cid:107)V (cid:62)

k Wt(cid:107)2

F and ˜b = k − (cid:107)V (cid:62)
k

˜W (cid:107)2
F .

Then Lemma 11 tells us that if α is a sufﬁciently small constant, bt ≤ 1

2 , then

E [bt+1|Wt] ≤ (cid:0)1 − cαλ2(cid:1) bt + c(cid:48)α2λ2˜b

(18)

for some numerical constants c, c(cid:48).

Lemma 12. Let B be the event that bt ≤ 1
constants c1, c2, c3, if α ≤ c1, then

2 for all t = 0, 1, 2, . . . , m. Then for certain positive numerical

E[bm|B] ≤ (cid:0)(cid:0)1 − c2αλ2(cid:1)m

+ c3α(cid:1) ˜b,

where the expectation is over the randomness in the current epoch.

24

Proof. Recall that bt is a deterministic function of the random variable Wt, which depends in turn on Wt−1
and the random instance chosen at round t. We assume that W0 (and hence ˜b) are ﬁxed, and consider how
bt evolves as a function of t. Using Eq. (18), we have

E[bt+1|Wt, B] = E

(cid:20)

bt+1|Wt, bt+1 ≤

(cid:21)

1
2

≤ E[bt+1|Wt] ≤ (cid:0)1 − cαλ2(cid:1) bt + c(cid:48)α2λ2˜b.

Note that the ﬁrst equality holds, since conditioned on Wt, bt+1 is independent of b1, . . . , bt, so the event B
is equivalent to just requiring bt+1 ≤ 1/2.

Taking expectation over Wt (conditioned on B), we get that

E[bt+1|B] ≤ E

(cid:12)
(cid:104)(cid:0)1 − cαλ2(cid:1) bt + c(cid:48)α2λ2˜b
(cid:12)
(cid:12)B
= (cid:0)1 − cαλ2(cid:1) E [bt|B] + c(cid:48)α2λ2˜b.

(cid:105)

Unwinding the recursion, and using that b0 = ˜b, we therefore get that

E[bm|B] ≤ (cid:0)1 − cαλ2(cid:1)m ˜b + c(cid:48)α2λ2˜b

≤ (cid:0)1 − cαλ2(cid:1)m ˜b + c(cid:48)α2λ2˜b

= (cid:0)1 − cαλ2(cid:1)m ˜b + c(cid:48)α2λ2˜b

(cid:18)

=

(cid:0)1 − cαλ2(cid:1)m

+

(cid:19)

c(cid:48)
c

α

˜b.

m−1
(cid:88)

(cid:0)1 − cαλ2(cid:1)i

i=0
∞
(cid:88)

(cid:0)1 − cαλ2(cid:1)i

i=0
1
cαλ2

as required.

We now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:

Lemma 13. The following holds for certain positive numerical constants c1, c2, c3: If α ≤ c1, then for any
β ∈ (0, 1) and m, if

˜b + c2kmα2λ2 + c3k

(cid:112)

mα2λ2 log(1/β) ≤

then it holds with probability at least 1 − β that

1
2

,

(19)

bt ≤ ˜b + c2kmα2λ2 + c3k

(cid:112)

mα2λ2 log(1/β) ≤

1
2

for all t = 0, 1, 2, . . . , m.
Proof. To prove the lemma, we analyze the stochastic process b0(= ˜b), b1, b2, . . . , bm, and use a concentra-
tion of measure argument. First, we collect the following facts:

• ˜b = b0 ≤ 1

2 : This directly follows from the assumption stated in the lemma.

• As long as bt ≤ 1

2 , E [bt+1|Wt] ≤ bt + c2α2λ2˜b for some constant c2: Supposing α is sufﬁciently

small, then by Eq. (18),

E [bt+1|Wt] ≤ (cid:0)1 − cαλ2(cid:1) bt + c(cid:48)α2λ2˜b ≤ bt + c(cid:48)α2λ2˜b.

25

• |bt+1 − bt| is bounded by c(cid:48)

3kαλ for some constant c(cid:48)
most some sufﬁciently small constant c1 (e.g. α ≤ 1

3: Applying Lemma 9, and assuming that α is at
12 , so η = αλ ≤ 1

12 ),

|bt+1 − bt| =

(cid:12)
(cid:12)(cid:107)V (cid:62)
(cid:12)

k Wt+1(cid:107)2

F − (cid:107)V (cid:62)

k Wt(cid:107)2
F

(cid:12)
(cid:12)
(cid:12) ≤

12kη
1 − 3η

≤

12kαλ
3/4

= 16kαλ.

Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows
that with probability at least 1−β, it holds simultaneously for all t = 1, . . . , m (and for t = 0 by assumption)
that

bt ≤ ˜b + c2mα2λ2˜b + c3k

(cid:112)

mα2λ2 log(1/β)

for some constants c2, c3, as long as the expression above is less than 1
2 , then we get that bt ≤ 1
1
the lemma.

2 . If the expression is indeed less than
2 for all t. Upper bounding ˜b by k and slightly simplifying, we get the statement in

Combining Lemma 12 and Lemma 13, and using Markov’s inequality, we get the following corollary:

Lemma 14. Let conﬁdence parameters β, γ ∈ (0, 1) be ﬁxed. Suppose that m, α are chosen such that
α ≤ c1 and

˜b + c2kmα2λ2 + c3k

(cid:112)

mα2λ2 log(1/β) ≤

1
2

,

where c1, c2, c3 are certain positive numerical constants. Then with probability at least 1 − (β + γ), it holds
that

for some positive numerical constants c, c(cid:48).

bm ≤

(cid:0)(cid:0)1 − cαλ2(cid:1)m

+ c(cid:48)α(cid:1) ˜b.

1
γ

Part III: Analyzing the Entire Algorithm’s Run

Given the analysis in Lemma 14 for a single epoch, we are now ready to prove our theorem. Let

By assumption, at the beginning of the ﬁrst epoch, we have ˜b0 = k − (cid:107)V (cid:62)
k
Lemma 14, for any β, γ ∈ (cid:0)0, 1
(cid:1), if we pick any
2

˜bs = k − (cid:107)V (cid:62)
k

˜Ws(cid:107)2
F .

˜W0(cid:107)2

F ≤ 1

2 . Therefore, by

(cid:26)

α ≤ min

c1,

(cid:27)

1
2c(cid:48) γ2

and m ≥

3 log(1/γ)
cαλ2

such that

1
2

+c2kmα2λ2 +c3k

then we get with probability at least 1 − (β + γ) that

bm ≤

1
γ

(cid:18)
(cid:0)1 − cαλ2(cid:1) 3 log(1/γ)

cαλ2 +

(cid:19)

˜b0

γ2

1
2

(cid:112)

mα2λ2 log(1/β) ≤

,

1
2
(20)

Using the inequality (1 − (1/x))ax ≤ exp(−a), which holds for any x > 1 and any a, and taking x =
1/(cαλ2) and a = 3 log(1/γ), we can upper bound the above by

(cid:18)

(cid:18)

exp

−3 log

1
γ

=

(cid:18)

1
γ

γ3 +

1
2

(cid:18) 1
γ
(cid:19)

γ2

˜b0 ≤ γ˜b0.

(cid:19)(cid:19)

+

(cid:19)

˜b0

γ2

1
2

26

Since bm equals the starting point ˜b1 for the next epoch, we get that ˜b1 ≤ γ˜b0 ≤ γ 1
2 . Again applying
Lemma 14, and performing the same calculation we have that with probability at least 1 − (β + γ) over the
next epoch, ˜b2 ≤ γ˜b1 ≤ γ2˜b0. Repeatedly applying Lemma 14 and using a union bound, we get that after
T epochs, with probability at least 1 − T (β + γ),

k − (cid:107)V (cid:62)
k

˜WT (cid:107)2

F = ˜bT ≤ γT ˜b0 < γT .

Therefore, for any desired accuracy parameter (cid:15), we simply need to use T =

k − (cid:107)V (cid:62)
k

˜Ws(cid:107)2

F ≤ (cid:15) with probability at least 1 − T (β + γ) = 1 −

(cid:108) log(1/(cid:15))
log(1/γ)

(cid:109)

(β + γ).

(cid:108) log(1/(cid:15))
log(1/γ)

(cid:109)

epochs, and get

Using a conﬁdence parameter δ, we pick β = γ = δ

2 , which ensures that the accuracy bound above

holds with probability at least

1 −

(cid:25)

(cid:24) log(1/(cid:15))
log(2/δ)

δ ≥ 1 −

(cid:25)

(cid:24) log(1/(cid:15))
log(2)

δ = 1 −

(cid:24)
log2

(cid:18) 1
(cid:15)

(cid:19)(cid:25)

δ.

Substituting this choice of β, γ into Eq. (20), and recalling that the step size η equals αλ, we get that
k − (cid:107)V (cid:62)
k

F ≤ (cid:15) with probability at least 1 − (cid:100)log2(1/(cid:15))(cid:101)δ, provided that

˜WT (cid:107)2

η ≤ cδ2λ , m ≥

c(cid:48) log(2/δ)
ηλ

,

kmη2 + k

(cid:112)

mη2 log(2/δ) ≤ c(cid:48)(cid:48)

for suitable positive constants c, c(cid:48), c(cid:48)(cid:48).

To get the theorem statement, recall that the analysis we performed pertains to data whose squared norm
is bounded by 1. By the reduction discussed at the beginning of the proof, we can apply it to data with
squared norm at most r, by replacing λ with λ/r, and η with ηr, leading to the condition

η ≤

cδ2
r2 λ , m ≥

c(cid:48) log(2/δ)
ηλ

,

kmη2r2 + rk

(cid:112)

mη2 log(2/δ) ≤ c(cid:48)(cid:48)

and establishing the theorem.

6.2 Proof of Theorem 2

The proof relies mainly on the techniques and lemmas of Section 6.1, used to prove Theorem 1. As done in
Section 6.1, we will assume without loss of generality that r = maxi (cid:107)xi(cid:107)2 is at most 1, and then transform
the bound to a bound for general r (see the discussion at the beginning of Subsection 6.1.2)

First, we extract the following result, which is essentially the ﬁrst part of Lemma 11 (for k = 1):

Lemma 15. Let A, wt be as deﬁned in Algorithm 1, and suppose that η ∈ (cid:2)0, 1

23

(cid:3). Then

Eit

(cid:2)1 − (cid:104)v1, wt+1(cid:105)2(cid:12)

(cid:12)wt, ˜ws−1

(cid:3) ≤ (cid:0)1 − cηλ(cid:104)v1, wt(cid:105)2(cid:1) (cid:0)1 − (cid:104)v1, wt(cid:105)2(cid:1) + c(cid:48)η2,

for some positive numerical constants c, c(cid:48).

Note that this bound holds regardless of what is ˜ws−1, and in particular holds across different epochs
of Algorithm 1. Therefore, it is enough to show that starting from some initial point w0, after sufﬁciently
many stochastic updates as speciﬁed in line 6-10 of the algorithm (or in terms of the analysis, sufﬁciently
many applications of Lemma 15), we end up with a point wT for which 1 − (cid:104)v1, wT (cid:105) ≤ 1
2 , as required.

27

Note that to simplify the notation, we will use here a single running index w0, w1, w2, . . . , wT (whereas in
the algorithm we restarted the indexing after every epoch).

The proof is based on martingale arguments, quite similar to the ones in Subsection 6.1.2 but with slight

changes. First, we let

bt = 1 − (cid:104)v1, wt(cid:105)2
to simplify notation. We note that b0 = 1 − (cid:104)v1, w0(cid:105)2 is assumed ﬁxed, whereas b1, b2, . . . are random
variables based on the sampling process. Lemma 11 tells us that if η is sufﬁciently small, and bt ≤ 1 − ξ for
some ξ ∈ (0, 1), then

E [bt+1|bt] ≤ (1 − cηλξ) bt + c(cid:48)η2.

(21)

for some numerical constants c, c(cid:48).

Lemma 16. Let B be the event that bt ≤ 1 − ξ for all t = 0, 1, . . . , T . Then for certain positive numerical
constants c1, c2, c3, if η ≤ c1λ, then

E[bT |B] ≤

(cid:18)

(1 − c2ηλξ)T + c3

(cid:19)

.

η
λξ

Proof. Using Eq. (21), we have for any bt satisfying event B that

E[bt+1|bt, B] = E [bt+1|bt, bt+1 ≤ 1 − ξ] ≤ E[bt+1|bt] ≤ (1 − cηλξ) bt + c(cid:48)η2.

Taking expectation over bt (conditioned on B), we get that

E[bt+1|B] ≤ E (cid:2)(1 − cηλξ) bt + c(cid:48)η2(cid:12)

(cid:12)B(cid:3)
= (1 − cηλξ) E [bt|B] + c(cid:48)η2.

Unwinding the recursion, we get

E[bT |B] ≤ (1 − cηλξ)T b0 + c(cid:48)η2

T −1
(cid:88)

i=0

(1 − cηλξ)i

≤ (1 − cηλξ)T + c(cid:48)η2

∞
(cid:88)

i=0

(1 − cηλξ)i

= (1 − cηλξ)T + c(cid:48)η2 1
cηλξ

≤ (1 − cηλξ)T +

c(cid:48)
c

η
λξ

.

We now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:

Lemma 17. The following holds for certain positive numerical constants c1, c2, c3: If η ≤ c1λ, then for any
β ∈ (0, 1), if

T η2 log(1/β) ≤ 1 − ξ,

(22)

b0 + c2T η2 + c3
then it holds with probability at least 1 − β that

(cid:112)

bt ≤ b0 + c2T η2 + c3

(cid:112)

T η2 log(1/β) ≤ 1 − ξ

for all t = 0, 1, . . . , T .

28

Proof. To prove the lemma, we analyze the stochastic process b1, b2, . . . , bT , and use a concentration of
measure argument. First, we collect the following facts:

• b0 ≤ 1 − ξ: This directly follows from the assumption stated in the lemma.

• E [bt+1|bt] ≤ bt + c(cid:48)η2 for some constant c(cid:48): By Eq. (21),

E [bt+1|Wt] ≤ (1 − cηλξ) bt + c(cid:48)η2 ≤ bt + c(cid:48)η2.

• |bt+1 −bt| is bounded by cη for some constant c: Applying Lemma 9 for the case k = 1, and assuming

η ≤ 1/12,

|bt+1 − bt| = (cid:12)

(cid:12)(cid:104)v1, wt+1(cid:105)2 − (cid:104)v, wt(cid:105)2(cid:12)

(cid:12) ≤

12η
1 − 3η

≤

12η
3/4

= 16η.

Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows
that with probability at least 1 − β, it holds simultaneously for all t = 0, 1, . . . , T that

bt ≤ b0 + c2T η2 + c3

(cid:112)

T η2 log(1/β)

for some constants c2, c3. If the expression is indeed less than 1 − ξ, then we get that bt ≤ 1 − ξ for all t,
from which the lemma follows.

Combining Lemma 16 and Lemma 17, and using Markov’s inequality, we get the following corollary:

Lemma 18. Let conﬁdence parameters β, γ ∈ (0, 1) be ﬁxed. Then for some positive numerical constants
c1, c2, c3, c, c(cid:48), if η ≤ c1λ and

b0 + c2T η2 + c3

(cid:112)

T η2 log(1/β) ≤ 1 − ξ,

then with probability at least 1 − (β + γ), it holds that

bT ≤

(cid:18)

1
γ

(1 − cηλξ)T + c(cid:48) η
λξ

(cid:19)

.

We are now ready to prove our theorem. By Lemma 18, for any β, γ ∈ (cid:0)0, 1
2

(cid:1) and any

(cid:26)

η ≤ min

c1,

such that

(cid:27)

λξ

1
2c(cid:48) γ2
b0 + c2T η2 + c3

and T ≥

3 log(1/γ)
cηλξ

(cid:112)

T η2 log(1/β) ≤ 1 − ξ,

(23)

we get with probability at least 1 − (β + γ) that

bT ≤

1
γ

(cid:18)

(1 − cηλξ)

3 log(1/γ)

cηλξ +

(cid:19)

.

γ2

1
2

Using the inequality (1 − (1/x))ax ≤ exp(−a), which holds for any x > 1 and any a, and taking x =
1/(cηλξ) and a = 3 log(1/γ), we can upper bound the above by

(cid:18)

(cid:18)

exp

−3 log

(cid:19)(cid:19)

(cid:18) 1
γ

(cid:19)

+

γ2

1
2

=

1
γ

(cid:18)

γ3 +

(cid:19)

,

γ2

1
2

1
γ

29

and since we assume γ < 1
bT ≤ 1

2 , and therefore 1 − (cid:104)v1, wT (cid:105)2 ≤ 1

2 , this is at most 1

2 as required.

2 . Overall, we got that with probability at least 1 − β − γ,

It remains to show that the parameter choices in Eq. (23) can indeed be satisﬁed. First, we ﬁx ξ = 1
2 ζ
(where we recall that 0 < ζ ≤ (cid:104)v1, w0(cid:105)2), which trivially ensures that b0 = 1 − (cid:104)v1, w0(cid:105)2 is at most 1 − 2ξ.
Moreover, suppose we pick β = γ in (0, exp(−1)), and η, T so that

η ≤

c∗γ2λξ3
log2(1/γ)

, T =

(cid:22) 3 log(1/γ)
c(cid:48)
∗ηλξ

(cid:23)

,

(24)

where c∗, c(cid:48)
∗ are sufﬁciently small constants so that the bounds on η, T in Eq. (23) are satisﬁed. This implies
that the third bound in Eq. (23) is also satisﬁed, since by plugging in the values / bounds of T and η, and
using the assumptions γ = β ≤ exp(−1) and ξ ≤ 1, we have

b0 + c2T η2 + c3

(cid:112)

T η2 log(1/γ)

≤ 1 − 2ξ + c2

≤ 1 − 2ξ + c2

3 log(1/γ)
c(cid:48)
∗λξ

η + c3

(cid:115)

3 log(1/γ)
c(cid:48)
∗λξ

3c∗γ2ξ2
c(cid:48)
∗ log(1/γ)

+ c3

(cid:115)

3c∗γ2ξ2
c(cid:48)
∗

≤ 1 − 2ξ +

(cid:32)

3c2c∗
c(cid:48)
∗

+ c3

(cid:115)

(cid:33)

ξ,

3c∗
c(cid:48)
∗

η log(1/γ)

which is less than 1 − ξ if we pick c∗ sufﬁciently small compared to c(cid:48)
∗.

To summarize, we get that for any γ ∈ (0, exp(−1)), by picking η as in Eq. (24), we have that after
T iterations (where T is speciﬁed in Eq. (24)), with probability at least 1 − 2γ, we get wT such that
1 − (cid:104)v1, wT (cid:105) ≤ 1

2 . Substituting δ = 2γ and ζ = 2ξ, we get that if

and η satisﬁes

(cid:104)v1, ˜w0(cid:105)2 ≥ ζ > 0,

η ≤

c1δ2λζ 3
log2(2/δ)

(for some universal constant c1), then with probability at least 1 − δ, after

T =

(cid:22) c2 log(2/δ)
ηλζ

(cid:23)

.

stochastic iterations, we get a satisfactory point wT .

As discussed at the beginning of the proof, this analysis is valid assuming r = maxi (cid:107)xi(cid:107)2 ≤ 1. By the
reduction discussed at the beginning of Subsection 6.1.2, we can get an analysis for any r by substituting
λ → λ/r and η → ηr. This means that we should pick η satisfying

ηr ≤

c1δ2(λ/r)ζ 3
log2(2/δ)

⇒ η ≤

c1δ2λζ 3
r2 log2(2/δ)

,

and getting the required point after

T =

(cid:22) c2 log(2/δ)
(ηr)(λ/r)ζ

(cid:23)

=

(cid:22) c2 log(2/δ)
ηλζ

(cid:23)

iterations.

30

6.3 Proof of Theorem 4

For simplicity of notation, we drop the A subscript from FA, and refer simply to F .

We ﬁrst prove the following two auxiliary lemmas:

Lemma 19. If A is a symmetric matrix, then the gradient of the function F (w) = − w(cid:62)Aw
equals

(cid:107)w(cid:107)2 at some w

2

−

(cid:107)w(cid:107)2 (F (w)I + A) w,

and its Hessian equals

4
(cid:107)w(cid:107)2 ww(cid:62)
where B⊥ = B + B(cid:62) (i.e., a matrix B plus its transpose).

1
(cid:107)w(cid:107)2

I −

−

(cid:18)(cid:18)

(cid:19) (cid:18)

(cid:19)(cid:19)⊥

F (w)I + A

,

Proof. By the product and chain rules (using the fact that
the gradient of F (w) = − 1

(cid:0)w(cid:62)Aw(cid:1) equals

(cid:107)w(cid:107)2

1

(cid:107)w(cid:107)2 is a composition of w (cid:55)→ (cid:107)w(cid:107)2 and z (cid:55)→ 1
z ),

(cid:16)

w

2
(cid:107)w(cid:107)4

(cid:17)

w(cid:62)Aw

− (Aw)

2
(cid:107)w(cid:107)2 ,

(25)

giving the gradient bound in the lemma statement after a few simpliﬁcations.

Differentiating the vector-valued Eq. (25) with respect to w (using the product and chain rules, and the

fact that

1

(cid:107)w(cid:107)4 is a composition of w (cid:55)→ (cid:107)w(cid:107)2, z (cid:55)→ z2, and z (cid:55)→ 1

z ), we get that the Hessian of F equals

2
(cid:107)w(cid:107)8 ∗ 2(cid:107)w(cid:107)2 ∗ 2w

(cid:19)(cid:62) (cid:16)

(cid:17)

w(cid:62)Aw

+ w

2
(cid:107)w4(cid:107)

(2Aw)(cid:62)

I

2
(cid:107)w(cid:107)4 (w(cid:62)Aw) + w
2

(cid:18)

−

(cid:18)

2

(cid:19)(cid:62)

− A

−

(cid:107)w(cid:107)2 − (Aw)
2F (w)
(cid:107)w(cid:107)2 I +
(cid:18)
1
(cid:107)w(cid:107)2

(cid:107)w(cid:107)4 ∗ 2w
8F (w)
(cid:107)w(cid:107)4 ww(cid:62) +
8F (w)
(cid:107)w(cid:107)2 ww(cid:62) −

2F (w)I −

= −

= −

4
(cid:107)w(cid:107)4 ww(cid:62)A −

2

4
(cid:107)w(cid:107)4 Aww(cid:62)

(cid:107)w(cid:107)2 A +
4
(cid:107)w(cid:107)2 Aww(cid:62)

(cid:19)

,

4
(cid:107)w(cid:107)2 ww(cid:62)A + 2A −

which can be veriﬁed to equal the expression in the lemma statement (using the fact that A, ww(cid:62) and I are
all symmetric matrices, hence equal their transpose).

2 (which implies (cid:104)w0, v1(cid:105) > 0).
1 be the intersection of the ray {av1 : a ≥ 0} with the hyperplane Hw0 = {w : (cid:104)w, w0(cid:105) = 1}. Then

Lemma 20. Let w0, v1 be two unit vectors such that (cid:107)w0 − v1(cid:107) ≤ (cid:15) < 1
Let v(cid:48)
(cid:107)v(cid:48)

1 − w0(cid:107) ≤ 5

4 (cid:15).

Proof. See Figure 2 in the main text for a graphical illustration.

Letting v(cid:48)

1 = av, a must satisfy (cid:104)av1, w0(cid:105) = 1. Since v1, w0 are unit vectors, this implies

a =

1
(cid:104)v1, w0(cid:105)

=

2
2 − (cid:107)v1 − w0(cid:107)2 ,

31

and since (cid:107)v1 − w0(cid:107) ≤ (cid:15), this means that

(cid:20)

a ∈

1,

(cid:21)

.

2
2 − (cid:15)2

Therefore,

(cid:107)v(cid:48)

1 − w0(cid:107) ≤ (cid:107)v1 − w0(cid:107) + (cid:107)v(cid:48)

1 − v1(cid:107) ≤ (cid:15) + (cid:107)av1 − v1(cid:107) ≤ (cid:15) + |a − 1| ≤ (cid:15) +

2

2 − (cid:15)2 − 1 = (cid:15) +

(cid:15)2
2 + (cid:15)2 ,

and since (cid:15) < 1

2 , this is at most 5

4 (cid:15).

We now turn to prove the theorem. Let ∇2(w) denote the Hessian at some point w. To show smoothness
and strong convexity as stated in the theorem, it is enough to ﬁx some unit w0 which is (cid:15)-close to the leading
eigenvector v1 (where (cid:15) is assumed to be sufﬁciently small), and show that for any point w on Hw0 which
is O((cid:15)) close to w0, and any direction g along Hw0 (i.e. any unit g such that (cid:104)g, w0(cid:105) = 0), it holds that
g(cid:62)∇2(w)g ∈ [λ, 20]. This implies that the second derivative in an O((cid:15)) neighborhood of w0 on Hw0 is
always in [λ, 20], hence the function is both λ-strongly convex in that neighborhood.

More formally, letting (cid:15) ∈ (0, 1) be a small parameter to be chosen later, consider any w0 such that

any w such that

and any g such that

(cid:107)w0(cid:107) = 1 , (cid:107)w0 − v1(cid:107) ≤ (cid:15),

(cid:104)w − w0, w0(cid:105) = 0 , (cid:107)w − w0(cid:107) ≤ 2(cid:15),

(cid:107)g(cid:107) = 1 , (cid:104)g, w0(cid:105) = 0.
Our goal is to show that for an appropriate (cid:15), we have g(cid:62)∇2(w)g ∈ [λ, 20]. Moreover, by Lemma 20, the
neighborhood set Hw0 ∩ Bw0(2(cid:15)) would also contain a point av1 for some a, which is a global optimum of
F due to its scale-invariance. This would establish the theorem.

The easier part is to show the upper bound on g(cid:62)∇2(w)g. Since g is a unit vector, it is enough to bound

the spectral norm of ∇2(w), which equals
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
(cid:107)w(cid:107)2

I −

(cid:18)(cid:18)

F (w)I + A

F (w)I + A

(cid:19)(cid:19)⊥(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

(cid:107)F (w)I + A(cid:107)sp

(cid:19) (cid:18)

(cid:19) (cid:18)

4
(cid:107)w(cid:107)2 ww(cid:62)
4
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
4
(cid:13)
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
(cid:13)sp
(cid:13)
4
(cid:13)
(cid:107)w(cid:107)2 ww(cid:62)
(cid:13)
(cid:13)

(cid:18)

I −

I −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:32)

≤

≤

≤

2
(cid:107)w(cid:107)2

2
(cid:107)w(cid:107)2

2
(cid:107)w(cid:107)2

(cid:107)I(cid:107)sp +

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)sp

((cid:107)F (w)I(cid:107)sp + (cid:107)A(cid:107)sp) .

Since the spectral norm of A is 1, and (cid:107)w(cid:107)2 ≥ 1 (as w lies on a hyperplane Hw0 tangent to a unit vector
w0), it is easy to verify that this is at most 2(1 + 4)(1 + 1) = 20 as required.
We now turn to lower bound g(cid:62)∇2(w)g, which by Lemma 19 equals

−

1
(cid:107)w(cid:107)2 g(cid:62)

(cid:18)(cid:18)

I −

4
(cid:107)w(cid:107)2 ww(cid:62)

(cid:19) (cid:18)

(cid:19)(cid:19)⊥

F (w)I + A

g.

32

Since g(cid:62)B⊥g = g(cid:62)Bg + g(cid:62)B(cid:62)g = 2g(cid:62)Bg, the above equals

−

2
(cid:107)w(cid:107)2 g(cid:62)

(cid:18)

I −

4
(cid:107)w(cid:107)2 ww(cid:62)

(cid:19) (cid:18)

(cid:19)

F (w)I + A

g.

(26)

Using the fact that w = w0 + (w − w0), and (cid:104)g, w0(cid:105) = 0, we get that (cid:104)g, w(cid:105) = (cid:104)g, w − w0(cid:105). Moreover,
since A is positive semideﬁnite and has spectral norm of 1, F (w) = − w(cid:62)Aw
(cid:107)w(cid:107)2 ∈ [−1, 0]. Expanding Eq. (26)
and plugging these in, we get

−

=

≥

2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2

(cid:18)

F (w)g(cid:62)

(cid:18)

I −

(cid:18)

(cid:18)

−F (w)(cid:107)g(cid:107)2 +

−F (w)(cid:107)g(cid:107)2 −

(cid:19)

(cid:18)

I −

g + g(cid:62)

4
(cid:107)w(cid:107)2 ww(cid:62)
4F (w)
(cid:107)w(cid:107)2 (cid:104)g, w − w0(cid:105)2 − g(cid:62)Ag +
4
(cid:107)w(cid:107)2 (cid:107)g(cid:107)2(cid:107)w − w0(cid:107)2 − g(cid:62)Ag −

(cid:19)

(cid:19)

Ag

4
(cid:107)w(cid:107)2 ww(cid:62)
4
(cid:107)w(cid:107)2 (cid:104)g, w − w0(cid:105)w(cid:62)Ag

(cid:19)

4

(cid:19)
(cid:107)w(cid:107)2 (cid:107)g(cid:107)(cid:107)w − w0(cid:107)(cid:107)w(cid:107)(cid:107)A(cid:107)sp(cid:107)g(cid:107)

.

Since (cid:107)g(cid:107) = 1, (cid:107)A(cid:107)sp = 1, (cid:107)w − w0(cid:107) ≤ 2(cid:15), and (cid:107)w(cid:107)2 = (cid:107)w0(cid:107)2 + (cid:107)w − w0(cid:107)2 is between 1 and 1 + 4(cid:15)2,
this is at least

(cid:16)

(cid:112)

2
(cid:107)w(cid:107)2

(−F (w)) − 16(cid:15)2 − g(cid:62)Ag − 8(cid:15)

1 + 4(cid:15)2(cid:17)(cid:17)
(27)
Let us now analyze −F (w) and g(cid:62)Ag more carefully. The idea will be to show that since we are close
to the optimum, −F (w) is very close to 1, and g (which is orthogonal to the near-optimal w0) is such that
g(cid:62)Ag is strictly smaller than 1. This would give us a positive lower bound on Eq. (27).

−F (w) − g(cid:62)Ag − 8(cid:15)

1 + 4(cid:15)2(cid:17)

2
(cid:107)w(cid:107)2

2(cid:15) +

(cid:112)

=

(cid:16)

(cid:16)

.

• By the triangle inequality and the assumptions (cid:107)w0 − v1(cid:107) ≤ (cid:15), (cid:107)w − w0(cid:107) ≤ 2(cid:15), we have (cid:107)w − v1(cid:107) ≤
3(cid:15). Also, we claim that F (·) is 4-Lipschitz outside the unit Euclidean ball (since the gradient of F at
any point with norm ≥ 1, according to Lemma 19, has norm at most 4). Therefore, |F (w) + 1| =
|F (w) − F (v1)| ≤ 4(cid:107)w − v1(cid:107) ≤ 12(cid:15), so overall,

F (w) ≤ −1 + 12(cid:15).

(28)

• Since (cid:104)w0, g(cid:105) = 0, and (cid:107)w0 − v1(cid:107) ≤ (cid:15), it follows that

|(cid:104)v1, g(cid:105)| ≤ |(cid:104)v1 − w0, g(cid:105)| + |(cid:104)w0, g(cid:105)| ≤ (cid:107)v1 − w0(cid:107)(cid:107)g(cid:107) + 0 ≤ (cid:15).

Letting v1, . . . , vd and 1 = s1 > s2 ≥ .. ≥ sd ≥ 0 be the eigenvectors and eigenvalues of A in
decreasing order (and recalling that s2 ≤ s1 − λ = 1 − λ for some eigengap λ > 0), we get

g(cid:62)Ag =

d
(cid:88)

i=1

si(cid:104)vi, g(cid:105)2 ≤ (cid:104)v1, g(cid:105)2 + (1 − λ)

d
(cid:88)

i=1

(cid:104)vi, g(cid:105)2

= (cid:104)v1, g(cid:105)2 + (1 − λ)(1 − (cid:104)v1, g(cid:105)2) = λ(cid:104)v1, g(cid:105)2 + (1 − λ)
≤ λ(cid:15)2 + (1 − λ) = 1 − (1 − (cid:15)2)λ.

(29)

33

Plugging Eq. (28) and Eq. (29) back into Eq. (27), we get a lower bound of

1 − 12(cid:15) − (cid:0)1 − (1 − (cid:15)2)λ(cid:1) − 8(cid:15)

(cid:16)

2(cid:15) +

(cid:112)

1 + 4(cid:15)2(cid:17)(cid:17)

(cid:16)

2
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)2

=

(cid:16)

(1 − (cid:15)2)λ − 8(cid:15)

(cid:16)

1.5 + 2(cid:15) +

(cid:112)

1 + 4(cid:15)2(cid:17)(cid:17)


1 − (cid:15)2 −

=

2
(cid:107)w(cid:107)2

(cid:16)

8(cid:15)

1.5 + 2(cid:15) +

√

1 + 4(cid:15)2(cid:17)



λ

 λ.

Using the fact that

√

1 + z2 ≤ 1 + z, this can be loosely lower bounded by

2
(cid:107)w(cid:107)2

(cid:18)

1 − (cid:15) −

8(cid:15) (2.5 + 4(cid:15))
λ

(cid:19)

λ.

Recalling that (cid:107)w(cid:107)2 = (cid:107)w0(cid:107)2 + (cid:107)w − w0(cid:107)2 is at most 1 + 4(cid:15)2, and picking (cid:15) sufﬁciently small compared to
λ, (say (cid:15) = λ/44), we get that the above is at least λ, which implies the required strong convexity condition.
To summarize, by picking (cid:15) = λ/44, we have shown that the function F (w) is λ-strongly convex and
20-smooth in a neighborhood of size 2(cid:15) = λ
22 around w0 on the hyperplane Hw0, provided that (cid:107)w0 −v1(cid:107) ≤
(cid:15) = λ
44 . By Lemma 20, we are guaranteed that this neighborhood contains v1 up to some rescaling (which
is immaterial for our scale-invariant function F ), hence by optimizing F in that neighborhood, we will get
a globally optimal solution.

Acknowledgments

This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel
Science Foundation grant 425/13.

References

[1] A. Agarwal and L. Bottou. A lower bound for the optimization of ﬁnite sums. In ICML, 2015.

[2] R. Arora, A. Cotter, K. Livescu, and N. Srebro. Stochastic optimization for PCA and PLS. In 2012

50th Annual Allerton Conference on Communication, Control, and Computing, 2012.

[3] R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In NIPS, 2013.

[4] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental PCA. In NIPS,

2013.

[5] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

[6] C. De Sa, K. Olukotun, and C. R´e. Global convergence of stochastic gradient descent for some non-

convex matrix problems. In ICML, 2015.

[7] R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster

stochastic algorithms for empirical risk minimization.

[8] G. H Golub and C. Van Loan. Matrix computations, volume 3. John Hopkins University Press, 2012.

34

[9] N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: Probabilistic algorithms

for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

[10] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In NIPS, 2014.

[11] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

statistical association, 58(301):13–30, 1963.

[12] R. Horn and C. Johnson. Matrix analysis. Cambridge university press, 2012.

[13] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In NIPS, 2013.

[14] T.P. Krasulina. The method of stochastic approximation for the determination of the least eigenvalue of
a symmetrical matrix. USSR Computational Mathematics and Mathematical Physics, 9(6):189–195,
1969.

[15] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and lanczos al-
gorithms with a random start. SIAM journal on matrix analysis and applications, 13(4):1094–1122,
1992.

[16] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In NIPS, 2014.

[17] E. Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical biology,

15(3):267–273, 1982.

[18] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric func-

tional analysis. Journal of the ACM (JACM), 54(4):21, 2007.

[19] O. Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In ICML,

2015.

[20] D. Woodruff. Sketching as a tool for numerical linear algebra. Theoretical Computer Science, 10(1-

2):1–157, 2014.

35

