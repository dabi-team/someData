EgoRenderer: Rendering Human Avatars from Egocentric Camera Images

Tao Hu1*, Kripasindhu Sarkar2, Lingjie Liu2, Matthias Zwicker1, Christian Theobalt2
1Department of Computer Science, University of Maryland, College Park
2Max Plank Institute for Informatics, Saarland Informatics Campus

1
2
0
2

v
o
N
4
2

]

V
C
.
s
c
[

1
v
5
8
6
2
1
.
1
1
1
2
:
v
i
X
r
a

Abstract

We present EgoRenderer, a system for rendering full-
body neural avatars of a person captured by a wearable,
egocentric ﬁsheye camera that is mounted on a cap or a
VR headset. Our system renders photorealistic novel views
of the actor and her motion from arbitrary virtual camera
locations. Rendering full-body avatars from such egocen-
tric images come with unique challenges due to the top-
down view and large distortions. We tackle these challenges
by decomposing the rendering process into several steps,
including texture synthesis, pose construction, and neural
image translation. For texture synthesis, we propose Ego-
DPNet, a neural network that infers dense correspondences
between the input ﬁsheye images and an underlying para-
metric body model, and to extract textures from egocentric
inputs. In addition, to encode dynamic appearances, our
approach also learns an implicit texture stack that captures
detailed appearance variation across poses and viewpoints.
For correct pose generation, we ﬁrst estimate body pose
from the egocentric view using a parametric model. We then
synthesize an external free-viewpoint pose image by project-
ing the parametric model to the user-speciﬁed target view-
point. We next combine the target pose image and the tex-
tures into a combined feature image, which is transformed
into the output color image using a neural image translation
network. Experimental evaluations show that EgoRenderer
is capable of generating realistic free-viewpoint avatars
of a person wearing an egocentric camera. Comparisons
to several baselines demonstrate the advantages of our
approach. Project page: https://vcai.mpi-inf.
mpg.de/projects/EgoRenderer/.

1. Introduction

The goal of this work is to render full-body avatars with
realistic appearance and motion of a person wearing an ego-
centric ﬁsheye camera from arbitrary external camera view-

*Work partly conducted during TH’s internship at MPI-INF

Figure 1: Based on a wearable ﬁsheye camera setup (a), we
propose EgoRenderer, which is trained for a single person
and can produce full-body avatars of the person from new
viewpoints and in new poses (c)(d) by taking as input an
egocentric image (b) captured by the ﬁsheye camera.

points (Figure 1). Such egocentric capture and rendering
enables new applications in sport performance analysis or
health care. Real-time free-viewpoint rendering of self-
embodied avatars is also important in virtual reality (VR)
and augmented reality (AR) applications, notably telepres-
ence. A key advantage of our approach is that it uses a
lightweight and compact sensor that could be mounted to
glasses, headsets or caps, and that it is fully mobile. There-
fore, actors can freely roam and are not limited to stay in
conﬁned spaces visible to external multi-camera setups.

We approach free-viewpoint neural avatar rendering
from our egocentric view by a combination of new solu-
tions for egocentric pose estimation, appearance transfer,
and free-viewpoint neural rendering; each of these need
to be tailored to the challenging egocentric top-down ﬁsh-
eye perspective with strong distortions and self-occlusions.
Most established pose estimation methods employ external
outside-in camera views [29, 30, 39, 8, 10] and are not di-
rectly applicable to our setting.

Some recent approaches are designed to estimate 3D
skeletal pose from head-mounted ﬁsheye cameras [57, 51].
However, our setting requires a denser pixel wise estimation
of egocentric pose and shape, as dense correspondences are
prerequisite to transfer the texture appearance of a person

 
 
 
 
 
 
from egocentric to external views (Figure 3). Similarly, re-
cent neural rendering-based pose transfer methods enable
creation of highly realistic animation videos of humans un-
der user-speciﬁed target motion [5, 2, 21, 54]. However,
all of these are tailored to external oustside-in views, such
that target motions already need to be speciﬁed as skeletal
pose or template mesh sequences from the extrinsic cam-
era view. We face the additional challenge of transferring
appearance and pose from the starkly distorted egocentric
view to the external view. To enable highly realistic appear-
ance and pose transfer of the actor wearing the camera to an
arbitrary external view, even in more general scene condi-
tions, EgoRenderer decomposes the rendering pipeline into
texture synthesis, pose construction, and neural image
translation, as shown in Figure 4.

Texture Synthesis. In contrast to most aforementioned
image-based pose transfer methods for outside-in views [5,
40], EgoRenderer explicitly builds an estimation of surface
textures of a person on top of a parametric body model.
Speciﬁcally, we extract explicit (color) textures from ego-
centric images and learn implicit textures from a multi-view
dataset in a training phase. We then combine them to form
our full texture representation of the person. Compared to
static color texture maps, the learned implicit textures bet-
ter capture detailed appearance variation across poses and
viewpoints. To extract (partial) textures for visible body
parts from egocentric images, we create a large synthetic
dataset (see Figure 2) and train an Ego-DPNet network tai-
lored to our setup to infer the dense correspondences be-
tween the input egocentric images and an underlying para-
metric body model, as shown in Figure 3.

Pose Construction. Different from earlier neural hu-
man rendering methods that expect target poses as input,
irrespective of where these targets are from [40], EgoRen-
derer works end-to-end. We have to exactly reproduce pose
and appearance seen in an egocentric image from any ex-
ternal viewpoint. We support neural rendering of the target
view by projecting the 3D parametric model from egocen-
tric camera space to the target viewpoint, enabling us to also
transfer the partially visible texture appearance.

Neural Image Translation. Pose construction enables
us to render the 3D model in the desired external view us-
ing both implicit textures and color textures. We transform
these images into the ﬁnal color image by means of a neu-
ral image translation network. Experiments show that this
implicit to explicit rendering approach outperforms direct
pose-to-image translation.

Our qualitative and quantitative evaluations demonstrate
that our EgoRenderer system generalizes better to novel
viewpoints and poses than baseline methods on our test set.
To summarize, our contributions are as follows:
1) A large synthetic ground truth training dataset of top-
down ﬁsheye images and an Ego-DPNet network tailored to

Figure 2: Examples from our synthetically rendered ﬁsheye
training dataset (top-right: ground truth DensePose). Our
dataset features a large variety of poses, human body ap-
pearance, and realistic backgrounds.

Figure 3: DensePose [10] performs poorly on images cap-
tured by our setup, sometimes failing to detect the human
(second row). Our DensePose predictions are on the right.

our ﬁsheye camera setup to predict dense correspondence to
a parametric body model from egocentric images.
2) An end-to-end EgoRenderer system that takes single ego-
centric images as input and generates full-body avatar ren-
derings from external user-deﬁned viewpoints.

2. Related Work

Our approach is closely related to many sub-ﬁelds of vi-
sual computing, and below we discuss a small subset of
these connections.
Neural Rendering. Neural rendering is a class of deep im-
age and video generation approaches that combines gen-
erative machine learning techniques with physical knowl-
edge from computer graphics to obtain controllable outputs.
Many neural rendering methods [45, 46, 31, 18, 49] learn
implicit representations of scene instead of explicitly mod-
eling geometry, such as DeepVoxels [45], SRNs [46], NeRF
[31] and NSVF [18]. However, only a few neural scene
representations handle dynamic scenes [13, 24]. Our neu-
ral rendering approach is inspired by the aforementioned
scene speciﬁc methods. To handle the dynamic nature of
the scene, we learn a person speciﬁc implicit texture map
on a parametric model of humans. This learned implicit tex-
ture stack, along with the pose dependent appearance from
the egocentric camera, is used by a high ﬁdelity generator
to produce realistic renderings of the person.

Figure 4: Pipeline overview. Given an egocentric image Ie of a person and a user-deﬁned viewpoint v, EgoRenderer synthe-
sizes the full body avatar Ie→t from viewpoint v. EgoRenderer is custom tailored to our cap-mounted ﬁsheye camera setup,
and decomposes the rendering process into texture synthesis, pose construction and neural image translation. For texture
synthesis, given an input image Ie of a person in egocentric camera space, we ﬁrst predict the dense correspondences (Pe)
between the input image Ie and an underlying parametric mesh model using Ego-DPNet, which allows a partial UV Texture-
map Te to be extracted for the body regions visible in the image. We also learn an implicit texture stack (Tm) during training,
and concatenate Te and Tm as the global texture representation Tg. For pose construction, given a user-deﬁned viewpoint v,
we synthesize a target pose image Pt by projecting the parametric model from egocentric space to the target viewpoint. In the
Feature Rendering step, the parametric body mesh is textured with the global texture stack Tg to produce intermediate feature
images Re→t . An image-translation network RenderNet converts the feature images to the ﬁnal realistic image Ie→t .

Pose Transfer and Human Re-enactment. Pose trans-
fer, ﬁrst introduced by [26], refers to the problem of re-
rendering a person from a different viewpoint and pose from
the appearance of a single image. Most approaches for-
mulate this problem as an image-to-image mapping prob-
lem, i.e. given a reference image of a target person, map-
ping the body pose in the form of renderings of a skele-
ton [5, 41, 35, 16, 61], dense mesh [20, 54, 19, 37, 33, 9] or
joint position heatmaps [26, 2, 27], to real images. To better
map the appearance of the reference to the generated im-
age, some methods [19, 37] ﬁrst map the person appearance
in screen space to UV space and feed the rendering of the
person in the target pose with the UV texture map into an
image translation network. However, these methods gener-
ally works in external camera setup with regular viewpoints.
We propose a new method that extracts appearance from the
top-down egocentric ﬁsheye camera using a novel network,
and combines it with a learned person speciﬁc neural tex-
ture for a high ﬁdelity generation.
Egocentric Systems. Because of their mobility and ﬂex-
ibility, egocentric systems have made signiﬁcant progress
in recent years. Applications of egocentric systems can be
divided into face, gesture, and full body. [7, 6, 50, 23, 4,
48, 17] study face estimation with a head-mounted cam-
era. [43, 47, 28, 3, 32, 34, 44] perform gesture and activity

recognition by using a head-mounted or chest-worn cam-
era. In the case of full body, most of the methods either opt
for an inside-out conﬁguration [38, 11, 58, 59], or employ
ﬁsheye cameras for a large ﬁeld of view [57, 36]. Rhodin
et al. [36] propose the ﬁrst full-body capture method with a
helmet-mounted stereo ﬁsheye camera pair. Xu et al. [57]
and Tome et al. [51] use a much more compact and ﬂexible
monocular setting for full body pose estimation. However,
our setting learns a denser pixel wise estimation of egocen-
tric pose and shape (see Figure 3).

3. Approach

Our EgoRenderer system is built on a single cap-
mounted ﬁsheye camera (similar to [57]) that captures ego-
centric images of persons at run time. Given an egocentric
image Ie of a person, we render a full body avatar of the
person from a user-deﬁned viewpoint, which we call ex-
ternal free-viewpoint space in this paper. Our system de-
composes the rendering process into texture synthesis, pose
construction, and neural image translation, as shown in Fig-
ure 4. First, for texture synthesis, we build a global tex-
ture stack Tg that combines explicit textures Te from ego-
centric images and an implicit texture stack Tm, which is
learned in a training phase, as the texture representations of

the person. Our system represents body pose and shape as a
parametric mesh [25]. For pose construction, given a user-
deﬁned target viewpoint, we synthesize a target pose image
by projecting the parametric model from egocentric camera
space to the target viewpoint. The third step renders the 3D
model with the global texture stack and generates implicit
feature images Re→t of the person, which are then trans-
formed to the ﬁnal rendered color image Re→t by a neural
image-translation network RenderNet in the fourth step.

3.1. Input and Output

Our input is an egocentric image Ie of a person and a
target viewpoint v, and the output is a photorealistic image
Ie→t of the person from the target viewpoint. In our training
stage, we take pairs of images (Ie, It ) of the same person
as input. In this paper, we call Ie images from egocentric
space, and It images from external free-viewpoint space.

3.2. Texture Synthesis

Extracting Partial UV Texture Maps from Egocentric
Space. The pixels of the input egocentric images are
transformed to UV texture space through dense corre-
spondences between the input images and an underlying
SMPL [25] model. DensePose [10] (pre-trained on the
COCO-DensePose dataset), can predict 24 body segments
and their part speciﬁc UV coordinates on images captured
with regular cameras and mostly from chest high view-
points. However, DensePose [10] fails on the egocentric
images in our setup, as shown in Figure 3.

Capturing a large amount of annotated DensePose data
for egocentric data is a mammoth task that would require
large amounts of human labor. To solve this problem,
we rendered a large, synthetic egocentric ﬁsheye training
dataset that enables training of a deep neural network to
predict dense correspondences on our hardware setup. Our
dataset contains 178,800 synthetic images with ground truth
UV coordinate maps, and features various poses, body ap-
pearances and backgrounds, as shown in Figure 2. With
this synthetic dataset, we train an Ego-DPNet ( f ) network
to predict the DensePose Pe, that is, Pe = f (Ie). More details
of the dataset can be found in the supplementary material.

Ego-DPNet was built on the DensePose-RCNN architec-
ture, and we trained it in multi-stage using transfer learning.
Different from the original DensePose-RCNN, which takes
sparse annotated ground truth points and relies on a inpaint-
ing teacher network to interpolate dense ground truth, our
Ego-DPNet directly takes synthetic images as dense ground
truth without the teacher network. In addition, another dif-
ference is that the input image size of Ego-DPNet is ﬁxed
in training and testing due to our speciﬁc ﬁsheye camera
setup. The performance of Ego-DPNet is shown in Figure
3. With the DensePose prediction Pe and the input image Ie,
we extract the partial UV texture map Te by an UV Texture

Figure 5: The impact of learning on the implicit texture
stack Tm. Left: initial state; right: ﬁnal state.

Extraction module u, that is, Te = u(Ie, Pe).
Learning Implicit Textures in Training. Besides textures
from egocentric space, we also learn a d-dimensional im-
plicit texture stack (Tm) from training images to capture
detailed appearance variation across poses and viewpoints.
Note that we initialize Tm with an explicit texture map that
is the average texture of the training images. The initial and
ﬁnal state of Tm during training is shown in Figure 5. We
provide our experiments with 3 channels in Tm.
Global Texture Stack. We concatenate explicit textures Te
and implicit textures Tm into the global texture stack Tg =
[Te, Tm] to represent dynamic textures of the person. In our
paper, Tg has 6 channels, 3 from Te and 3 from Tm.

3.3. Pose Construction

Given an egocentric image Ie and a target user-deﬁned
viewpoint v, the second step is to synthesize the target pose
of the person under the viewpoint, which is represented by
a DensePose image Pt in Figure 4. First, a pose estimation
module (Mo2Cap2 [57] in our experiments) is used to ex-
tract the 3D joint pose of the person, which is used to drive
a 3D SMPL model by Inverse Kinematics. The target pose
is then rendered by projecting the 3D model to viewpoint v.

3.4. Intermediate Feature Image Rendering

Using the global texture stack Tg = [Te, Tm] and the tar-
get pose image Pt , we use a Feature Rendering operation
r to produce a 6-dimensional feature image Re→t , that is,
Re→t = r(Te, Tm, Pt ). The operation r is implemented by dif-
ferentiable, bilinear sampling in our experiments.

3.5. Neural Image Translation

In the ﬁnal step, the feature image Re→t is translated to a
realistic image Ie→t using a translation network g, which we
call RenderNet, Ie→t = g(Re→t ). RenderNet is built on the
Pix2PixHD [55] architecture. The discriminator for adver-
sarial training of RenderNet also uses the multiscale design
of Pix2PixHD, and we use a three scale discriminator net-
work for the adversarial training.

3.6. Training Details and Loss Functions

We train the EgoRenderer in two stages. We ﬁrst train
the Ego-DPNet on our synthetic dataset, and then train Ren-

derNet on real data. Note that RenderNet is person spciﬁc.
Ego-DPNet. For better generalization on real world im-
agery, we train Ego-DPNet in multiple stages using transfer
learning. It was ﬁrst pre-trained on the COCO-DensePose
dataset [10] to learn good low-level features from real im-
ages with usual camera optics. Then we ﬁne tune it on our
synthetic dataset to predict DensePose on egocentric im-
ages.
RenderNet. In training our system takes pairs of egocentric
inputs and ground truth images (Ie, It ) of the same person as
input. The output of RenderNet can be expressed as

Ie→t = g ◦ r(u(Ie, f (Ie)), Tm, Pt ),

where all operations g, r, f , u are differentiable. For
speed, we pre-compute Ego-Pose Pe = f (Ie) and Pt , and
directly read them as input in training. We optimize the pa-
rameters of RenderNet g and the implicit texture stack Tm.
Loss Functions. We apply the combination of the following
loss functions to train RenderNet:

• Perceptual Loss. We use a perceptual loss based on
the VGG Network [12], which measures the difference be-
tween the activations on different layers of the pretrained
VGG network [42] applied on the generated image Ie→t and
ground truth target image It ,

Lp = ∑

1
N j

(cid:12)p j (Ie→t ) − p j (It )(cid:12)
(cid:12)
(cid:12) ,

where p j is the activation and N j the number of elements

of the j-th layer in the pretrained VGG network.

• Adversarial Loss. We use a multiscale discriminator
D of Pix2PixHD [55] to leverage an adversarial loss Ladv in
our system. D is conditioned on both the generated image
and rendered feature image.

• Face Identity Loss. We use a pre-trained network to
ensure that RenderNet and the implicit texture stack pre-
serve the face identity on the cropped face of the generated
and ground truth image,

L f ace =| N f ace (Ie→t ) − N f ace (It ) |,
where, N f ace is the pretrained SphereFaceNet [22].

The ﬁnal loss is then

LG = λpLp + λ f aceL f ace + λGANLadv.

The networks are trained using the Adam optimizer [14]
with an initial learning rate of 2 × 10−4, β1 = 0.5. The loss
weights are set empirically to λGAN = 1, λp = 10, λ f ace = 5.
Note that the initial learning rate of Tm is 2 × 10−3, 10 times
of the learning rate of RenderNet.

4. Experiments

Datasets. Since there are no public datasets suitable for
our project, we captured 4 datasets by ourselves, and the
rendered human avatars in our study are shown in Figure 6.

Figure 6: Renderings produced by our methods (for all sub-
jects H1, H2, H3, H4 in our study). All renderings are pro-
duced from new viewpoints and poses unseen in training.

We refer to them as H1, H2, H3 and H4. H1 and H2 were
captured in outdoor scenes, while H3 and H4 were captured
in a studio. We have 11 multi-view cameras for the H1 and
H2 datasets, and 8 for H3 and H4, and use the training/test
split of 80%/20% to train and evaluate our methods.

4.1. Baselines

We denote our method as Im-Tex, and compare Im-Tex
with 5 other systems, Pix2PixHD [55], Fea-Net [37], and
three variants, Ex-Tex, Only-Ego, and Only-MV. All these
baseline methods have similar architectures as ours, and
we provide them with the same input and loss functions
in training, so all these methods are directly comparable.
1) Pix2PixHD. We used Pix2PixHD’s code with minimal
modiﬁcations that lead to better performance. Pix2PixHD
directly translates a target pose Pt to a generated image Ie→t .
2) Fea-Net. We refer to the human re-rendering method
[37] as Fea-Net (short for FeatureNet), which is the state
of the art in human re-enactment and novel view render-
ing when this project was developed. Different from our
method, which maintains a global implicit texture stack,
Fea-Net uses a network to extract 16-dimensional feature
maps from egocentric images that serve as implicit textures.
Though Fea-Net was originally trained on multiple identi-
ties to do garment and motion transfer, it can also be directly
applied on our person speciﬁc task. 3) Ex-Tex. As a variant
of our implicit texture method, Ex-Tex works with an ex-
plicit and static texture stack. Compared with Im-Tex, the
texture stack (Tm) in Figure 4 is not updated during train-
ing. Hence we call it an explicit texture stack as shown in
Figure 5-left. 4) Only-Ego. We also consider a variant that
does not have the texture stack Tm, which means Tg = Te,
and in feature rendering, we only sample textures from ego-
centric images. We call this method Only-Ego. 5) Only-
MV. In contrast to Only-Ego, another variant is Only-MV,
which does not take textures from egocentric images, but
only from training images It . In this case, Tg = Tm. Only-
MV can be seen as an extension of Deferred Neural Ren-
dering [49].

Figure 7: Comparisons of the six methods. All avatars are generated from novel viewpoints and poses unseen in training.

Methods

Im-Tex
Pix2PixHD
Ex-Tex
Only-Ego
Only-MV
Fea-Net

H1

SSIM↑
7.529
7.395
7.431
7.437
7.543
7.444

LPIPS↓
1.623
1.713
1.691
1.769
1.738
1.695

H2
SSIM LPIPS
1.617
6.840
1.713
6.798
1.683
6.778
1.704
6.782
1.626
6.828
1.687
6.823

H3
SSIM LPIPS
1.569
6.469
1.640
6.342
1.676
6.326
1.660
6.361
1.585
6.360
1.630
6.350

H4
SSIM LPIPS
1.571
7.535
1.615
7.428
1.629
7.435
7.586
1.578
1.587
7.505
1.616
7.449

Mean relative improvements(%)
RIPSNR ↑
RILPIPS ↑
RISSIM ↑
7.562
1.268
.748
.404
2.427
-
.555
3.630
0.089
.467
-
0.240
-
4.263
1.190
.912
3.600
0.487

Table 1: Quantitative results (multiplied by 10) of single-video training on different datasets, and relative improvements.

4.2. Evaluations on Test Datasets

We consider two regimes: training on single- or multi-
camera video sequences. The evaluation is done on the
hold-out cameras and hold-out parts of the sequence. That
is, there is no overlap between the training and test sets in
terms of the camera or body poses. For fair comparisons
among different methods, we use DensePose results com-
puted on the target frame as input in training and evaluation
like other human rendering papers [40, 5].

Metrics. The following three metrics are used for compar-
ison, the Structural Similarity Index (SSIM) [56], and the
Learned Perceptual Image Patch Similarity (LPIPS) [60],
and peak signal-to-noise ratio (PSNR). The recent LPIPS
claims to capture human judgment better than existing hand
designed metrics, whereas PSNR or SSIM is often inac-
curate in capturing the visual perception and would differ
starkly with slight changes in texture/wrinkles [60].

Single video experiments. We ﬁrst evaluate our system in

a single video case. We use single-camera videos from one
of the cameras in our rig, and evaluate the 6 methods on the
hold-out cameras and hold-out parts of the sequence. The
qualitative results are shown in Figure 7. It can be observed
that our results show better realism and preserve more de-
tails, and the quantitative results are provided in Table 1. We
also calculate the mean relative improvement (RI) of each
approach (x) over the worst one (y, indicated by -) on metric
m for all datasets: RIm(x, y) = |m(y) − m(x)|/m(y), where
m(x) is the result of x on metric m. RI is shown in percent.
Our Im-Tex outperforms the others on 8 out of 11 metrics,
and we achieve the best LPIPS scores on all the datasets.
In general, our system is capable of learning avatars from
monocular videos, and can be applied in everyday activi-
ties for which setting a studio with multi-view cameras to
capture training data would be highly challenging.

Multi-video comparisons. We also conduct multi-video
experiments where each method was trained on multiple
videos from different viewpoints in Table 2. We train the

Figure 8: Comparisons to Textured Neural Avatars [40]. Our method (left) produces realistic time- and pose-dependent
appearance detail, such as pose-dependent wrinkles in clothing, or the shifting of the shirt. In contrast, Textured Neural
Avatars cannot produce the same level of realism, since a static texture with ﬁxed clothing wrinkles is warped into new poses.

Figure 9: Renderings between single- and multi-video train-
ing. From left to right: egocentric image and estimated
DensePose, single-video result, multi-video result, ground
truth. With multi-video training, more details are restored,
such as the right hand.

H1

SSIM LPIPS
1.174
8.134
ImT
1.218
8.038
P2P
1.258
7.993
ExT
1.228
Ego
8.038
MV 8.174
1.217
1.220
8.022
FNet

PSNR
2.104
2.077
2.059
2.075
2.140
2.095

H4

SSIM LPIPS
1.246
7.893
1.301
7.823
1.374
7.716
1.287
7.825
1.343
7.770
1.372
7.744

PSNR
1.780
1.753
1.722
1.763
1.732
1.736

Table 2: Quantitative comparisons of multi-video training
on H1 outdoor and H4 indoor datasets. SSIM and LPIPS
are multiplied by 10, PSNR by 0.1. The methods are listed
as follows: Im-Tex, Pix2PixHD, Ex-Tex, Only-Ego, Only-
MV, Fea-Net.

6 methods with 9 multi-view cameras for H1, and 4 multi-
view cameras for H4. Each method performs better when
more video sequences are added in training, and especially
our method can reconstruct more details, as shown in Figure
9. More comparisons on multi-video training are provided
in Figure 13 and the supplementary material.

4.3. Comparisons to Textured Neural Avatar (TNA)

We compare our method against static texture based
method TNA in Figure 8, where ours can produce time- and
pose-dependent appearance details, whereas TNA cannot.

Figure 10: Compared with Only-MV, the use of egocen-
tric texture of our Im-Tex leads to improvements visible in
faces, neck and collar. We even restore the tiny clothes but-
tons visible in the Ego-Image.

Figure 11: In applications, given an egocentric camera im-
age (left), our methods can generate plausible avatars from
different viewpoints, even with high elevation degree (the
last four) unseen in training. We show the same example
frame as Figure 4.

4.4. Ablation Study

We study the advantages of the learned neural texture
over other human rendering methods introduced in Sec-
tion 4.1, including Pix2PixHD [55], Fea-Net [37], Ex-Tex,
Only-Ego, and Only-MV. Note that the main difference
among these methods is the texture synthesis module. We
get the following conclusions by comparing different meth-
ods according to the quantitative and qualitative results in
Table 1, Figure 7-13 and our supplementary material. We

Figure 12: Comparisons with Only-MV on occluded parts.

target poses by Feature Rendering and formulates the ren-
dering as an easier transformation from implicit textures to
avatars. Second, ours leveraging the Ego-Texture as input
outperforms Only-MV, as shown in Figure 7, 10, where ours
shows better realism (e.g. faces and clothes) and even re-
constructs the tiny buttons visible in the egocentric images
in Figure 10. We also see that ours outperforms Only-MV
even on occluded parts (e.g. the human head in row 1 of Fig-
ure 7, the back of human in the second row of Figure 13, and
Figure 12), where the RGB textures from the Ego-Texture
are not directly used. However, the additional supervision
contained in the Ego-Texture makes Render-Net learn better
correlation between the occluded part and the visible texture
regions, and possibly helps learn implicit shape information
useful for more accurate renderings.
Discussion: egocentric setup. Why not ﬁrst transform the
ﬁsheye images to normal images, then perform traditional
human synthesis? Since we are using a ﬁsheye camera with
a 180◦ ﬁeld of view (FOV), it is not possible to project (in
other words ‘undistort’) our image to a rectilinear image.
Although the cropped ﬁsheye image (by reducing the FOV)
can be undistorted, we lose the main advantage of using the
ﬁsheye image by doing so – full body capturing under a
wide range of motions, including fully extended arms, etc.
This is also the reason many existing egocentric methods
process the ﬁsheye images directly [57, 52].

Figure 13: Renderings of different views in applications.

4.5. Applications

use a > b to denote that method a outperforms method b.

1) Texture synthesis methods (e.g. Im-Tex, Only-MV)
outperform the direct pose-to-image translation method
(Pix2PixHD), implied by Only-MV > Pix2PixHD, where
the only difference is that Only-MV has an implicit texture
stack Tm. 2) Textures from egocentric images help improve
the rendering quality, implied by Im-Tex > Only-MV. 3)
The implicit texture stack Tm matters, implied by Im-Tex
> Only-Ego. 4) Implicit texture stacks capture detailed ap-
pearance variation across poses and viewpoints better than
explicit color texture stacks, implied by Im-Tex > Ex-
Tex. 5) For our human speciﬁc task, maintaining a globally
shared texture stack works better than extracting temporary
features from egocentric images for each frame, implied by
Im-Tex > Fea-Net [37]. Different from Im-Tex, Fea-Net
has a special FeatureNet to extract high dimensional fea-
tures from egocentric inputs. Though Fea-Net has almost
twice as many parameters as ours, its performance is not as
good as ours on this person speciﬁc task.
Why does the proposed method outperform the others?
First, we attribute this improvement to the explicit tex-
ture synthesis. Compared with Pix2PixHD, which learns
implicit correlations between the target poses and RGB
avatars, our method explicitly utilizes the semantics of the

In real applications, as introduced in Figure 4 and Sec-
tion 3.3, EgoRenderer uses Mo2Cap2 [57] to extract 3D
poses and synthesize target poses. The synthesized target
poses and comparisons of the six methods are provided in
Figure 13 for the example frame in Figure 4. Our method
generates higher quality avatars than the other methods. In
Figure 11, we show more viewpoints. It can be observed
that EgoRenderer synthesizes reasonable avatars from novel
viewpoints even with high elevation degrees. In addition,
EgoRenderer can work in both local (human-centered) and
global coordinate systems with camera tracking. See more
details in the supplementary material.

5. Summary and Discussion

We proposed the EgoRenderer

to synthesize free-
viewpoint avatars from a single egocentric ﬁsheye camera,
and our system is an important step towards practical daily
full-body reconstruction. We see our approach as the ba-
sis for many exciting applications in various areas, such
as performance analysis, and human reconstructions in AR
and VR. However, our system suffers from certain limita-
tions mainly caused by the inaccuracies of Mo2Cap2 [57]:
1) Mo2Cap2 can estimate 15 joints (e.g., neck, shoulders,
elbows, wrists, hips, knees, ankles and toes), whereas 24
joints are required to drive a SMPL model, which may cause

inaccuracies in inverse kinematics and unnatural motions.
2) Mo2Cap2 per-frame predictions exhibit some temporal
instability at run time, which causes temporal jittering in
renderings. We believe these issues can be solved by opti-
mizing the pose estimation module.
Acknowledgements. MZ was supported by NSF#1813583.
This work was partially funded by the ERC Consolidator
Grant 4DRepLy (770784)

References

[1] CMU mocap dataset. http://mocap.cs.cmu.edu/,

2008. 12

[2] Kﬁr Aberman, M. Shi, Jing Liao, Dani Lischinski, B. Chen,
and D. Cohen-Or. Deep video-based performance cloning.
Computer Graphics Forum, 38, 2019. 2, 3

[3] Congqi Cao, Yifan Zhang, Yi Wu, Hanqing Lu, and Jian
Cheng. Egocentric gesture recognition using recurrent 3d
convolutional neural networks with spatiotemporal trans-
former modules. In IEEE International Conference on Com-
puter Vision, pages 3783–3791. IEEE Computer Society,
2017. 3

[4] Y. Cha, T. Price, Z. Wei, X. Lu, N. Rewkowski, R. Chabra,
Z. Qin, H. Kim, Z. Su, Y. Liu, A. Ilie, A. State, Z. Xu,
J. Frahm, and H. Fuchs. Towards fully mobile 3d face,
body, and environment capture using only head-worn cam-
IEEE Transactions on Visualization and Computer
eras.
Graphics, 24(11):2993–3004, 2018. 3

[5] C. Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A.
Efros. Everybody dance now. 2019 IEEE/CVF International
Conference on Computer Vision (ICCV), pages 5932–5941,
2019. 2, 3, 6

[6] Mohamed Elgharib, Mallikarjun BR, Ayush Tewari,
Hyeongwoo Kim, Wentao Liu, Hans-Peter Seidel, and Chris-
tian Theobalt. Egoface: Egocentric face performance capture
and videorealistic reenactment, 2019. 3
[7] Mohamed Elgharib, Mohit Mendiratta,

Justus Thies,
Matthias Nießner, Hans-Peter Seidel, Ayush Tewari,
Vladislav Golyanik, and Christian Theobalt. Egocentric
videoconferencing. ACM Transactions on Graphics, 39(6),
Dec 2020. 3

[8] A. Elhayek, E. de Aguiar, A. Jain, J. Tompson, L. Pishchulin,
M. Andriluka, C. Bregler, B. Schiele, and C. Theobalt. Ef-
ﬁcient convnet-based marker-less motion capture in general
scenes with a low number of cameras. In 2015 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 3810–3818, 2015. 1

[9] A. K. Grigor’ev, Artem Sevastopolsky, Alexander Vakhitov,
and Victor S. Lempitsky. Coordinate-based texture inpaint-
ing for pose-guided human image generation. Computer Vi-
sion and Pattern Recognition (CVPR), pages 12127–12136,
2019. 3

[10] R. A. Güler, N. Neverova, and I. Kokkinos. Dense-
In 2018
pose: Dense human pose estimation in the wild.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7297–7306, 2018. 1, 2, 4, 5

[11] Hao Jiang and Kristen Grauman. Seeing invisible poses: Es-
timating 3d body pose from egocentric video. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3501–3509. IEEE Computer Society, 2017. 3

[12] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. vol-
ume 9906, pages 694–711, 10 2016. 5

[13] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng
Xu, Justus Thies, Matthias Nießner, Patrick Pérez, Christian
Richardt, Michael Zollöfer, and Christian Theobalt. Deep
video portraits. ACM Transactions on Graphics (TOG), 37,
2018. 2

Pointrend:

[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2015. 5
[15] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross B.
Girshick.
Image segmentation as rendering.
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 9796–9805, 2020. 12
[16] Bernhard Kratzwald, Zhiwu Huang, Danda Pani Paudel, and
Luc Van Gool. Towards an understanding of our world by
GANing videos in the wild. arXiv:1711.11453, 2017. 3
[17] Hao Li, Laura C. Trutoiu, Kyle Olszewski, Lingyu Wei, Tris-
tan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang
Facial performance sensing head-mounted display.
Ma.
ACM Trans. Graph., 34(4):47:1–47:9, 2015. 3

[18] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua,
ArXiv,

and C. Theobalt. Neural sparse voxel ﬁelds.
abs/2007.11571, 2020. 2

[19] Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zoll-
höfer, Florian Bernard, Hyeongwoo Kim, Wenping Wang,
and Christian Theobalt. Neural human video rendering by
learning dynamic textures and rendering-to-video transla-
IEEE Transactions on Visualization and Computer
tion.
Graphics, PP:1–1, 05 2020. 3

[20] Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo
Kim, Florian Bernard, Marc Habermann, Wenping Wang,
and Christian Theobalt. Neural rendering and reenactment of
human actor videos. ACM Transactions on Graphics (TOG),
2019. 3

[21] Lingjie Liu, Weipeng Xu, M. Zollhöfer, H. Kim, F. Bernard,
Marc Habermann, W. Wang, and C. Theobalt. Neural an-
imation and reenactment of human actor videos. ArXiv,
abs/1809.03658, 2018. 2

[22] Weiyang Liu, Y. Wen, Zhiding Yu, Ming Li, B. Raj, and Le
Song. Sphereface: Deep hypersphere embedding for face
recognition. 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 6738–6746, 2017. 5
[23] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser
Sheikh. Deep appearance models for face rendering. ACM
Trans. Graph., 37(4), July 2018. 3

[24] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-
umes: Learning dynamic renderable volumes from images.
ACM Trans. Graph. (SIGGRAPH), 38(4), 2019. 2

[25] M. Loper, Naureen Mahmood, J. Romero, Gerard Pons-
Moll, and Michael J. Black. Smpl: a skinned multi-person
linear model. ACM Trans. Graph., 34:248:1–248:16, 2015.
4, 12

[26] Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuyte-
laars, and Luc Van Gool. Pose guided person image genera-
tion. In Advances in Neural Information Processing Systems,
pages 405–415, 2017. 3

[27] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc van
Gool, Bernt Schiele, and Mario Fritz. Disentangled person
image generation. Computer Vision and Pattern Recognition
(CVPR), 2018. 3

[28] Minghuang Ma, Haoqi Fan, and Kris M. Kitani. Going
deeper into ﬁrst-person activity recognition. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1894–1903. IEEE Computer Society, 2016. 3

[29] Dushyant Mehta, H. Rhodin, D. Casas, P. Fua, Oleksandr
Sotnychenko, Weipeng Xu, and C. Theobalt. Monocular 3d
human pose estimation in the wild using improved cnn super-
vision. 2017 International Conference on 3D Vision (3DV),
pages 506–516, 2017. 1

[30] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
H. Rhodin, Mohammad Shaﬁei, H. Seidel, Weipeng Xu, D.
Casas, and C. Theobalt. Vnect: real-time 3d human pose
estimation with a single rgb camera. ACM Trans. Graph.,
36:44:1–44:14, 2017. 1

[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV, 2020. 2

[32] Franziska Mueller, Dushyant Mehta, Oleksandr Sotny-
chenko, Srinath Sridhar, Dan Casas, and Christian Theobalt.
Real-time hand tracking under occlusion from an egocentric
rgb-d sensor. In Proceedings of International Conference on
Computer Vision, October 2017. 3

[33] Natalia Neverova, Riza Alp Güler, and Iasonas Kokkinos.
Dense pose transfer. European Conference on Computer Vi-
sion (ECCV), 2018. 3

[34] Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, and
Tatsuya Harada. Recognizing activities of daily living with a
wrist-mounted camera. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 3103–3111. IEEE Com-
puter Society, 2016. 3

[35] Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and
Francesc Moreno-Noguer. Unsupervised person image syn-
thesis in arbitrary poses. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018. 3
[36] Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafut-
dinov, Mohammad Shaﬁei, Hans-Peter Seidel, Bernt Schiele,
and Christian Theobalt. Egocap: egocentric marker-less mo-
tion capture with two ﬁsheye cameras. ACM Trans. Graph.,
35(6):162:1–162:11, 2016. 3

[37] Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu,
Vladislav Golyanik, and Christian Theobalt. Neural re-
rendering of humans from a single image. In European Con-
ference on Computer Vision (ECCV), 2020. 3, 5, 7, 8, 12
[38] Takaaki Shiratori, Hyun Soo Park, Leonid Sigal, Yaser
Sheikh, and Jessica K. Hodgins. Motion capture from body-
mounted cameras. ACM Trans. Graph., 30(4):31, 2011. 3

pose recognition in parts from single depth images. CVPR
2011, pages 1297–1304, 2011. 1

[40] Aliaksandra Shysheya, E. Zakharov, Kara-Ali Aliev, R.
Bashirov, Egor Burkov, K. Iskakov, Aleksei Ivakhnenko,
Yury Malkov, I. Pasechnik, D. Ulyanov, Alexander Vakhitov,
and V. Lempitsky. Textured neural avatars. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2382–2392, 2019. 2, 6, 7

[41] Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere,
and Nicu Sebe. Deformable GANs for pose-based human
image generation. In CVPR 2018, 2018. 3

[42] K. Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2015. 5

[43] S. Singh, C. Arora, and C. V. Jawahar. First person action
In IEEE Con-
recognition using deep learned descriptors.
ference on Computer Vision and Pattern Recognition, pages
2620–2628, 2016. 3

[44] Suriya Singh, Chetan Arora, and C. V. Jawahar. Trajectory
aligned features for ﬁrst person action recognition. Pattern
Recognit., 62:45–55, 2017. 3

[45] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Nießner, Gordon Wetzstein, and Michael Zollhöfer. Deep-
voxels: Learning persistent 3d feature embeddings. In Com-
puter Vision and Pattern Recognition (CVPR), 2019. 2
[46] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wet-
Scene representation networks: Continuous 3d-
zstein.
In Advances
structure-aware neural scene representations.
in Neural Information Processing Systems (NeurIPS), 2019.
2

[47] Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, and
Christian Theobalt. Fast and robust hand tracking using
detection-guided optimization. In Proceedings of Computer
Vision and Pattern Recognition (CVPR), June 2015. 3
[48] Yusuke Sugano and Andreas Bulling. Self-calibrating head-
mounted eye trackers using egocentric visual saliency. In Ce-
line Latulipe, Bjoern Hartmann, and Tovi Grossman, editors,
Proceedings of the 28th Annual ACM Symposium on User In-
terface Software & Technology, pages 363–372. ACM, 2015.
3

[49] Justus Thies, Michael Zollhöfer, and Matthias Nießner. De-
image synthesis using neural tex-
ferred neural rendering:
tures. ACM Transactions on Graphics (TOG), 38, 2019. 2,
5

[50] Justus Thies, Michael Zollöfer, Marc Stamminger, Christian
Theobalt, and Matthias Nießner. FaceVR: Real-Time Facial
Reenactment and Eye Gaze Control in Virtual Reality. arXiv
preprint arXiv:1610.03151, 2016. 3

[51] Denis Tomè, Patrick Peluse, L. Agapito, and H. Badino. xr-
egopose: Egocentric 3d human pose from an hmd camera.
2019 IEEE/CVF International Conference on Computer Vi-
sion (ICCV), pages 7727–7737, 2019. 1, 3

[52] Denis Tomè, Patrick Peluse, Lourdes Agapito, and Hernán
Badino. xr-egopose: Egocentric 3d human pose from an
HMD camera. In IEEE International Conference on Com-
puter Vision, pages 7727–7737. IEEE, 2019. 8

[39] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finoc-
chio, A. Blake, Mat Cook, and R. Moore. Real-time human

[53] G. Varol, J. Romero, X. Martin, Naureen Mahmood,
Michael J. Black, I. Laptev, and C. Schmid. Learning from

synthetic humans. 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 4627–4635,
2017. 12

[54] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
In Advances in Neural Information Pro-
video synthesis.
cessing Systems (NeurIPS), 2018. 2, 3

[55] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
pages 8798–8807, 06 2018. 4, 5, 7, 12

[56] Zhou Wang, A. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural
similarity. IEEE Transactions on Image Processing, 13:600–
612, 2004. 6

[57] Weipeng Xu, Avishek Chatterjee, Michael Zollhöfer, Helge
Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian
Theobalt. Mo2cap2: Real-time mobile 3d motion capture
with a cap-mounted ﬁsheye camera. IEEE transactions on
visualization and computer graphics, PP, 03 2018. 1, 3, 4, 8
[58] Ye Yuan and Kris Kitani. 3d ego-pose estimation via imita-
tion learning. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 735–750, 2018. 3

[59] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast-
ing as real-time PD control. In IEEE International Confer-
ence on Computer Vision, pages 10081–10091. IEEE, 2019.
3

[60] Richard Zhang, Phillip Isola, Alexei A. Efros, E. Shechtman,
and O. Wang. The unreasonable effectiveness of deep fea-
tures as a perceptual metric. 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 586–595,
2018. 6

[61] Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei
Wang, and Xiang Bai. Progressive pose attention transfer for
person image generation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2347–2356, 2019. 3

Appendices

A. Dataset

More details of the proposed synthetic dataset. To render our synthetic dataset, we animated characters using the SMPL
model [25] with around 3000 different motions sampled from the CMU MoCap [1] dataset. More than 600 body textures
were randomly chosen from the texture set provided by the SURREAL [53] dataset. In total, we rendered 178,800 images
for training.
Dataset Pre-processing After we captured datasets, we ﬁrst synchronized the egocentric ﬁsheye camera and multi-view
cameras to register them in time, and used the pre-trained PointRend [15] for foreground segmentation.

B. More Experimental Results

A. Quantitative Comparisons

Single-video comparisons. We also provide the additional L1 distances (Table 3) of each method on single-video datasets.

Im-Tex
0.994
1.191
1.448
0.894

Pix2PixHD [55]
0.994
1.199
1.522
0.906

H1
H2
H3
H4

Ex-Tex Only-Ego Only-MV Fea-Net [37]
0.950
1.215
1.562
0.944

0.995
1.233
1.484
0.911

0.999
1.224
1.516
0.905

0.998
1.175
1.511
0.926

Table 3: L1 distances of single-video training on different datasets. Numbers are multiplied by 10

Multi-video comparisons. The L1 distances of multi-video experiments for indoor (H1) and outdoor scenes (H4) are pro-
vided in Table 4, where each method was trained on multiple videos from different viewpoints, 9 multi-view cameras for H1,
and 4 multi-view cameras for H4.

Im-Tex
0.645
0.767

Pix2PixHD [55]
0.650
0.791

H1
H2

Ex-Tex Only-Ego Only-MV Fea-Net [37]
0.688
0.836

0.641
0.834

0.646
0.817

0.651
0.790

Table 4: L1 distances of multi-video training on H1 outdoor and H4 indoor datasets. Numbers are multiplied by 10

B. Local and Global Coordinate System

Our system, EgoRenderer can work in both local (human-centered coordinate) and global coordinate systems (Figure
14). For local system (left), we assume each user-speciﬁc viewpoint is relative to the human. For global system (right),
we synthesize global target poses by integrating local poses estimated by Mo2Cap2 and global tracking by external devices.
More results can be found in the video demo.

(a) Renderings at timestamp 1.

(b) Renderings at another timestamp.

Figure 14: EgoRenderer can work in both local (left) and global coordinate systems (right).

