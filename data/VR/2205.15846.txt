SaccadeNet: Towards Real-time Saccade Prediction for Virtual
Reality Inﬁnite Walking

Yashas Joshi∗, Charalambos Poullis†
Immersive and Creative Technologies lab
Concordia University

2
2
0
2

y
a
M
1
3

]

R
G
.
s
c
[

1
v
6
4
8
5
1
.
5
0
2
2
:
v
i
X
r
a

Figure 1: Right: A deep-learning powered redirected walking technique that predicts the onset duration of Saccades
during head rotations to mask virtual environment manipulations and redirect the users. Mid: average eye gaze positions
for two users from our preliminary experiment. Left: Screen capture from a custom-designed VR exploration game that
required participants to walk at least 38m straight in the virtual environment from within a 3.5 x 3.5 m2 of physical
tracked space.
Abstract

Introduction

1

Modern Redirected Walking (RDW) techniques signif-
icantly outperform classical solutions. Nevertheless,
they are often limited by their heavy reliance on eye-
tracking hardware embedded within the VR headset to
reveal redirection opportunities.

We propose a novel RDW technique that lever-
ages the temporary blindness induced due to saccades
for redirection. However, unlike the state-of-the-art,
our approach does not impose additional eye-tracking
hardware requirements. Instead, SaccadeNet, a deep
neural network, is trained on head rotation data to
predict saccades in real-time during an apparent head
rotation. Rigid transformations are then applied to
the virtual environment for redirection during the on-
set duration of these saccades.

We present three user studies. The relationship be-
tween head and gaze directions is conﬁrmed in the ﬁrst
user study, followed by the training data collection in
our second user study. Then, after some ﬁne-tuning ex-
periments, the performance of our RDW technique is
evaluated in a third user study. Finally, we present the
results demonstrating the eﬃcacy of our approach. It
allowed users to walk up a straight virtual distance of at
least 38 meters from within a 3.5x3.5m2 of the physical
tracked space. Moreover, our system unlocks saccadic
redirection on widely used consumer-grade hardware
without eye-tracking.

∗yashasjoshi1996@gmail.com
†charalambos@poullis.org

Over recent years, there have been notable advance-
ments in Virtual Reality (VR) devices due to the ad-
vent of GPUs and low-cost displays. As a result, many
interaction-based VR applications surged to gain main-
stream consumer and industrial attention. People can
now explore virtual environments (VEs) from the com-
fort of their living rooms or oﬃces. However, navigat-
ing virtual environments that are spatially larger than
the available physical-tracked space (PTS) remains an
open research problem. The most common locomotion
techniques currently available rely on pointing devices
or walking in place. These techniques are unnatural
and can negatively impact the sense of presence and
immersion since they fail to provide the required iner-
tial force feedback necessary to furnish a sensation of
moving into space.

Redirected walking (RDW), circa 2001 (Razzaque,
Kohn, and Mary C. Whitton 2001), oﬀered a more
natural approach to locomotion. In theory, users can
navigate an inﬁnite virtual space using RDW while re-
maining within the boundaries of the available PTS.
Since its inception, researchers have proposed many
hardware and software-based techniques.

Hardware-based techniques, e.g.

omnidirectional
treadmills (Nagamori, Wakabayashi, and Ito 2005),
and virtusphere (Fernandes, Raja, and Eyre 2003), re-
quire expensive specialized equipment that allows the
user to walk in place. The main drawback of these
techniques is that the inertial force feedback provided
is not equivalent to natural walking (Christensen et al.
2000), therefore causing vection.

1

 
 
 
 
 
 
Software-based techniques, although cost-eﬀective,
augment the visual content presented to the user. A
state-of-the-art technique following a software-based
approach is (Sun, Patney, et al. 2018). This technique
leverages the natural phenomenon of change blindness
induced due to rapid eye movements (saccades) and eye
blinks. Observable artifacts are introduced in the orig-
inal content -as light ﬂashes in 2D or light-orbs in 3D-
and serve as visual stimuli to trigger change blindness.
Highly eﬃcient eye-trackers embedded in the VR head-
set continuously track users’ eyes in real-time. When
a saccade or a blink is detected, the system applies
subtle rotations to warp the entire VE while guiding
the users away from any potential collision with phys-
ical objects, like furniture and walls, detected using a
Kinect. Although being apparent, these rotations are
imperceptible due to change blindness. However, the
main drawback of this technique is that it changes how
the user interacts with the virtual environment, deviat-
ing from the content creator’s intention - for example,
navigating around a landmark or playing a game while
being distracted by artiﬁcially induced visual stimuli.
Following a similar approach, in (Joshi and Poullis
2020), an eye-tracking device is embedded in the head-
set and tracks the user’s gaze in real-time while ex-
ploiting the psychological phenomenon of inattentional
blindness. The system divides the user’s virtual ﬁeld-
of-view (FoV) into zones using foveated rendering
and continuously updates the peripheral zone, leading
to subtle, imperceptible changes due to inattentional
blindness. Furthermore, during the temporary blind-
ness triggered due to natural suppressions such as sac-
cades or blinks, the system updates the foveal zone.
Hence the entire frame buﬀer is updated in the direc-
tion determined by the RDW algorithm.

The integrated eye trackers required for saccadic
redirection and foveated rendering are notably expen-
sive for consumer use. Therefore, this paper investi-
gates the use of deep neural networks for predicting sac-
cades during the users’ head rotations. We present ex-
periments that reaﬃrm the relationship between head
and eye rotations and show that users predominantly
ﬁxate around the center of the FoV while mainly us-
ing head rotations to change their focus. A deep neu-
ral network, SaccadeNet, is trained to predict saccades
during these head rotations. The VE is adjusted during
this phase to redirect the user towards the center of the
PTS. Furthermore, as determined by Panero and Zel-
nik (Panero and Zelnik 1979), the horizontal FoV for a
human eye is only 124◦. Therefore, to avoid vestibulo-
ocular reﬂexive (VOR) eye movements (R et al. 2020),
we must only predict saccades for the head rotations
exceeding 124◦ in order to distort the FoV and induce
temporary blindness. Thus, we ﬁlter the head rota-
tions with velocities exceeding 150◦/sec, i.e., more than
124◦/sec, and ignore the rest as a limitation of our
method. Therefore, this redirection method is eﬀec-
tive only during speciﬁc tasks that elicit repeated head
rotations. On the other hand, this method of predict-
ing saccades can have multiple applications, including
foveated rendering and RDW. Finally, we present the

evaluation results showing the real-time performance
of SaccadeNet. The VE updates were imperceptible to
the users, and the system allowed a nearly linear walk
in VR while being conﬁned within a 3.5 × 3.5m2 of
PTS.

2 Background & Related Work

Among many interactions, locomotion remains an un-
solved impediment to achieving an ideal immersive VR
experience. Commonplace techniques like teleporta-
tion and ﬂying serve their purpose while relying heavily
on external controllers and gamepads. They also often
induce simulator sickness and break the presence due
to their synthetic feel. On the other hand, an increased
sense of presence (Usoh et al. 1999) and reduced simu-
lator sickness (LaViola Jr. 2000) make natural walking
the most favored form of locomotion by the majority
of developers. Furthermore, since it is the most natu-
ral form of navigation for humans, it also increases the
spatial understanding of VEs due to its intuitive nature
(T. Peck, H. Fuchs, and M. Whitton 2011; R. A. Rud-
dle, Volkova, and Bulthoﬀ 2011; R. Ruddle and Lessels
2009).

This section presents a brief overview of the research
most relevant to our proposed method.
It is catego-
rized in terms of (i) redirected walking, (ii) hardware
and software-based solutions, and (iii) natural visual
suppressions and state-of-the-art techniques.

2.1 Redirected walking

Despite the advantages, locomotion by natural walk-
ing poses a signiﬁcant challenge. Limited availability
of the PTS constrains users within a ﬁnite boundary,
while the virtual space can theoretically be bound-
less (Poullis and You 2009). To tackle this dilemma,
a solely software-based technique, redirected walking,
was introduced circa 2000 (Razzaque, Kohn, and Mary
C. Whitton 2001).

Visual sense often dominates upon contradicting
with vestibular or proprioceptive senses (Dichgans and
Brandt 1978). Redirected walking leverages this ef-
fect and manipulates the users’ virtual FoV such that
their physical and virtual motion diﬀer (Nilsson et al.
2018; S. E. et al. 2012). For example, by asking users to
walk along predeﬁned curved virtual paths, researchers
could inject subtle rotational gains that trick users
into taking curved physical paths with a shorter radius
(Langbehn, Lubos, et al. 2017). These minor discrep-
ancies in the curvatures of physical and virtual paths
suﬃciently convince users of exploring a comparatively
bigger VE than the available PTS (L. E. and F. 2018).
The VE rotation is estimated using two main pa-
rameters: target directions in VE and PTS. Among
many methods of predicting the target direction in the
VE, (Zank and Kun 2015) is one of the approaches that
use users’ past walking direction, while (Steinicke et al.
2008) uses head rotations, and (Zank and Kun 2016)
the gaze direction. However, algorithms like steer-

2

to-center, steer-to-orbit, and steer-to-multiple-targets
(Razzaque, Kohn, and Mary C Whitton 2005) are used
to determine the target direction in PTS. As the names
suggest, steer-to-center algorithms guide users toward
the center of the PTS, steer-to-orbit guides them on a
ﬁxed orbit around the center of the PTS, and steer-to-
multiple-targets guide them towards one of the closest
predeﬁned waypoints in the PTS. Experiments per-
formed by Hodgson et al.
(Hodgson and Bachmann
2013) clearly showed a performance advantage of using
steer-to-center over other algorithms in vast open vir-
tual spaces. Alignment-based Redirection Controllers
(ARC) (N. L. Williams, Bera, and Manocha 2021) is
another such algorithm that also avoids obstacles in
the physical space by maintaining a similar distance
with another virtual object.

2.2 Hardware and software-based solu-

tions

After Razzaque (Razzaque, Kohn, and Mary C. Whit-
ton 2001) initially paved the way towards solving the
enigma of realistic navigation in VR, many researchers
followed by either making the technique more eﬃcient
or taking a completely diﬀerent hardware-based ap-
proach.
Hardware-based approaches: Sliding across a fric-
tionless surface (Iwata and Fujii 1996) and walking
while suspended freely in the air (Walther-Franks et
al. 2013) are prime examples of hardware-based ap-
proaches that focus on stationary equipment.
In
techniques like omnidirectional treadmill
contrast,
(Souman et al. 2010; Darken, Cockayen, and Carmein
1997;
Iwata 1999; Huang 2003; Nagamori, Wak-
abayashi, and Ito 2005) and walking in a giant Ham-
ster Ball (Medina, Fruland, and Weghorst 2008; Fer-
nandes, Raja, and Eyre 2003) focus on moving equip-
ment to provide a comparatively realistic experience.
Some methods even consolidate external physical props
(Cheng et al. 2015), or manipulate the entire PTS
(Suma, S. Clark, et al. 2011; Suma, Lipps, et al. 2012).
Although they are plausible solutions to locomotion,
they either fail to provide an inertial force-feedback
necessary to furnish the sensation of self-motion (Chris-
tensen et al. 2000), or have many external dependen-
cies.
Software-based approaches: These techniques
solely rely on digital manipulations for redirection.
For example, some scale the user’s head rotations
and translations in real-time (Razzaque, Kohn, and
Mary C. Whitton 2001; Azmandian, Grechkin, and
Rosenberg 2017), while others rely on producing self-
overlapping virtual spaces by partially or fully wrap-
ping the entire VE (Sun, Wei, and Kaufman 2016;
Dong et al. 2017), or objects in it (Matsumoto et al.
2016).

techniques

More recently,

like (Langbehn and
Steinicke 2018) and (Sun, Patney, et al. 2018) rely on
single subtle VE rotations during a visual suppression,
i.e., blink/saccade. Although these subtle techniques
are imperceptible and thus preferable in most cases,

overt techniques are sometimes favored due to safety
or practical limitations. Techniques like freeze-back-
up, freeze-turn, and 2:1 turn are some standard ap-
proaches proposed by Williams et al. in (B. Williams
et al. 2007). Freeze-back-up allows users to step back
upon hitting the PTS boundary with a frozen FoV,
freeze-turn allows users to turn by 180◦ with a frozen
FoV, and 2:1 turn allows users to make half a turn
(180◦) in any direction while concurrently making a
complete turn (360◦) of the FoV in the opposite di-
In each case, users face the same direction
rection.
as before in VE with walkable physical space in front.
However, the 2:1 turn is exceptionally preferred as it
reduces the contradictions between visual and vestibu-
lar systems. Moreover, these are generally used in hy-
brid systems (Joshi and Poullis 2020) as a reset mech-
anism when subtle redirection techniques fail. A re-
set mechanism is mainly employed as a last resort for
the safety of the users and equipment. Additionally,
open-source platforms like OpenRDW (Li et al. 2021),
and RDW Toolkit (Azmandian, Grechkin, Bolas, et al.
2016) provide benchmarks and a variety of redirection
algorithms for evaluation.

2.3 Natural Visual Suppression

Humans face temporary blindness from time to time
(Ronald A. Rensink, O’Regan, and J. J. Clark 1997;
Ronald A. Rensink 2002) due to the actions known as
visual suppressions (Volkmann 1986). Two of the most
frequent visual suppressions are blinks and saccades.
Blinks are generally essential to maintaining eye func-
tions by spreading tears and removing irritants from
their surface. They also modulate cognitive processes
such as attentional allocation (T et al. 2012), and are
linked to dopaminergic pathway activation in the hu-
man striatum, with clinical conditions in which this
pathway activation is increased (e.g., Schizophrenia)
or decreased (e.g., Parkinson’s) showing concordant
changes in blink frequency. On the other hand, Sac-
cades are a necessary eye behavior that allows move-
ment of the relatively small fovea to visual points of in-
terest. They are the ballistic eye movements to change
focus from one object to another (Volkmann 1986).

With speeds reaching up to 900◦/s (Bahill, M. R.
Clark, and Stark 1975),
the temporary blindness
caused before, during, and after a saccade can last
for 20 to 200 ms (Burr, Morrone, and Ross 1994).
Tracking some of these fast saccades requires a high
refresh rate and high accuracy eye-tracking system.
On the contrary, blinks are scarce and more gradual
in comparison. The temporary blindness induced due
to a blink can typically last for about 100 to 400 ms
(Ramot 2008). Users fail to notice any change intro-
duced in the scene during this temporary blindness.
This phenomenon is commonly known as change blind-
ness (O’Regan et al. 2000). However, humans make
many joint head-eye movements, including not just sac-
cades and blinks, but smooth pursuits and reﬂexive,
compensatory eye movements such as vestibulo-ocular
reﬂexive (VOR) eye movements (R et al. 2020). Pri-

3

marily, these oculomotor behaviors do not have simi-
lar dynamics. For example, smooth pursuit and VOR
movements have lower velocities and accelerations than
saccades. The purpose of VOR is to keep an image or
object of interest stable on the retina and fovea as the
head moves. While visual perception can change dur-
ing saccades and render temporary blindness, it varies
diﬀerently for diﬀerent eye movements.

A method proposed by Langbehn et al. (Langbehn
and Steinicke 2018) leverages the change blindness in-
duced during naturally occurring blinks to rotate the
entire VE. In contrast, Sun et al. (Sun, Patney, et al.
2018) leverages the same phenomenon, but with the
trigger being saccades instead of blinks. However, the
latter relied on simulating artiﬁcial saccades by ﬂash-
ing orbs in both image and object space, distracting
the users from the task at hand. Following these ap-
proaches, Joshi et al.
(Joshi and Poullis 2020) pro-
posed a technique that combines the eﬀects of change
blindness with inattentional blindness. The FoV was
divided into three zones: peripheral, foveal, and tran-
sitional, and rendered using dynamic foveated render-
ing. Based on their importance, the zones are updated
one at a time, slowly replacing the entire frame buﬀer
without noticing. Finally, they update the foveal zone
using the temporary blindness caused due to naturally
occurring saccades.

In this paper, we leverage change blindness induced
due to saccades and develop a deep learning model for
predicting them only during a head rotation. The en-
tire frame buﬀer is refreshed in a single shot, and hence
the user is redirected.

3 Head-Eye Relationship

Humans adjust their gaze continuously by applying si-
multaneous alterations to our head and eye rotations.
The ﬁrst user study examines this relationship between
head and eye directions under the assumption that the
users are preoccupied with a cognitive task that elicits
repeated head rotations in VR.

3.1 Application and Procedure

We developed an immersive VR experience to examine
the relationship between head and eye rotations that
portrayed an open sky environment to eliminate direc-
tional cues. A particular task was designed to ensure
numerous simultaneous head rotations during the ex-
periments. Participants were instructed to locate tiny,
stationary targets and eliminate them using a semi-
automatic ﬁrearm. These targets were spawned at a
distance of 10 meters from the user. Since saccades
tend to be purely eye-driven rather than a joint head-
eye movement when the object or image of interest is
within 18 deg of ﬁxation point (J 1999; Y et al. 2015),
it was also ensured that these targets were separated
by at least 20◦ from each other. Furthermore, multiple
targets in random directions were shown to trigger fre-
quent gaze shifts, resulting in simultaneous head and

eye rotations. Upon elimination, the targets were pro-
grammed to respawn in another random direction.

The VR viewport was assumed as a 1x1 grid, and
the gaze data was recorded for each participant to gen-
erate a heatmap for the entire task duration. The av-
erage gaze position was determined by the bounding
boxes on each heatmap and plotting a minimum en-
closing ellipse. Each test was divided into ﬁve repeated
steps; a target retrieval task for a one-minute interval,
followed by a one-minute break. Therefore, each par-
ticipant performed the task for ﬁve minutes with ﬁve
breaks. Finally, an elimination score was kept to en-
courage more target hits in a given time frame.

3.2 Participants and Pre-Test Ques-

tionnaire

The study involved 12 participants, two female, with
an average age of 24.54 years and a standard devia-
tion of 4.38.
In addition, each participant ﬁlled out
a pre-test and a post-test questionnaire to measure
the demographics and simulator sickness levels using
Kennedy’s simulator sickness questionnaire (Kennedy
et al. 1993), respectively. The reported median for
their VR device experience was four with normal or
corrected-to-normal vision, and their experience using
an eye-tracking device was three. This data was gath-
ered using a 5-point Likert scale, with one least and
ﬁve being most familiar.

3.3 Equipment and Safety

Every experiment during this research was performed
on a workstation with an Intel(R) Core(TM) i9 - 9900K
CPU @3.60GHz and an NVIDIA RTX 2080 Ti GPU.
HTC Vive Pro Eye was used as the primary VR headset
with an integrated Tobii Eye Tracker, each operating
at a frame rate of 120 Hz with eye-tracking accuracy
of 0.5◦ − 1.1◦. Furthermore, an eye calibration for each
participant was performed using the VIVE Pro Eye
Setup tool before each experiment in every user study
to avoid any noisy gaze estimations.

Additionally, due to the ongoing coronavirus pan-
demic, the procedures for all the experiments during
this research were scrutinized and approved by the
institution’s Environmental Health and Safety board
(EHS) and the Ethics Research (ER) board. Neverthe-
less, participants were informed of the risks and their
right to quit.

3.4 Analysis - Average Foveal Region

Figure 2 shows the heatmaps for two random par-
ticipants. Diﬀerent colors indicate the average time
spent by the users ﬁxating on any particular region of
the viewport. This time increases as the colors shift
from navy blue to dark red. Furthermore, bound-
ing boxes are marked to quantify the region of aver-
age gaze. Gartner and Schoenherr’s smallest enclosing
ellipse algorithm (G¨artner and Sch¨onherr 1997) was

4

Table 2: Overview of SSQ responses for User study #2.

Scores
Nausea(N)
5.45
Oculomotor(O) 10.83
14.91
Disorient.(D)
11.49
Total
Score(TS)

Mean Median

0
3.79
6.96
7.48

SD Min Max
19.08
0
7.21
60.64
0
17.77
41.76
0
17.66
48.62
0
14.99

Figure 2: Heatmaps and bounding boxes drawn on the
FoV of two participants from the ﬁrst user study. Time
spent ﬁxating on the viewport increases from navy blue to
dark red.

study shows that it can redirect users successfully in
VR without any noticeable visual artifacts.

5 Model, Data Acquisition &

Table 1: Overview of SSQ responses for User study #1.

Learning

Scores
Nausea(N)
7.95
Oculomotor(O) 9.475
16.24
Disorient.(D)
12.16
Total
Score(TS)

Mean Median

0
7.58
0
5.61

SD Min Max
57.24
0
16.19
37.9
0
11.71
83.52
0
26.43
63.58
0
18.19

0.1442

0.282 + (y−0.451)2

then applied to determine the smallest ellipse that en-
closes every bounding box. This ellipse is given by
(x−0.491)2
= 1 with (0.491, 0.451) being
its center. Since we initially considered the viewport
as a 1 x 1 plane,
it is evident that almost all the
users mainly focused around its center, i.e. (0.5, 0.5).
Therefore, it is safe to assume that the users perform a
saccade-like action during an apparent and rapid head
rotation. Moreover, since the human’s horizontal FoV
spans roughly 124◦ (Panero and Zelnik 1979), we hy-
pothesize that when the users rotate their heads in VR
with a velocity of more than 150◦/sec, i.e., more than
124◦/sec, the FoV is distorted enough to cause tempo-
rary blindness.
Simulator Sickness Questionnaire. We used the
formulas provided in (Kennedy et al. 1993) to calcu-
late the Total Severity (TS) and its corresponding sub-
scales such as Nausea (N), Oculomotor (O), and Dis-
orientation (D). A majority (83.34%) of participants
reported no signiﬁcant signs of simulator sickness with
an expected highest average score for disorientation
(16.24) due to the vestibular disturbances caused by
repeated head rotations. An overview of these SSQ
responses is shown in Table 1.

4 Technical overview

To further strengthen the hypothesis formulated in our
ﬁrst study, we developed and trained a deep neural net-
work, SaccadeNet, that predicts saccades solely based
on head rotations. Once a saccade is predicted, we ad-
just the VE according to the Redirection algorithm and
redirect the users during the onset duration of these
predicted saccades. Indeed, like high-end eye-trackers,
SaccadeNet performs in real-time, and our ﬁnal user

5

This section presents SaccadeNet, a deep neural net-
work that predicts saccades only during tracked head
rotations in real-time. It was developed using Pytorch
framework and comprised four 1D convolutional layers,
a layer to ﬂatten features, and four fully connected lay-
ers. The width remains constant (10) throughout the
layers as each convolutional layer was speciﬁed with a
kernel size of 3, and a padding and stride of 1. Since
our data is a time series, the input window size of nine
was chosen with consecutive samples selected from the
dataset. Therefore, the ﬁrst layer consisted of nine in-
put channels and 16 output channels. It was followed
by (16, 32), (32, 64), and (64, 128) input and output
channels (layers) for the second, third, and fourth lay-
ers, respectively. The output is ﬂattened to produce
a 128x10 input for the ﬁrst fully connected layer with
1024 output channels, followed by (1024, 512), (512,
256), (256, 128), and (128, 1) fully connected layers.

The convolutional layers and the ﬁrst three fully con-
nected layers employed a Leaky Rectiﬁed Linear Unit
(Leaky ReLU) activation function in the forward pass
to circumvent potential vanishing gradients problems.
While the fourth fully connected layer employed ReLU
activation to produce entirely positive values for binary
classiﬁcation in the ﬁnal layer with Sigmoid activation
to obtain probabilities between 0 and 1. Adam opti-
mizer was used with a binary cross-entropy (BCE) loss
function and 0.001 learning rate for backpropagation.
Finally, the model was trained over ten epochs with a
batch size of 128.

5.1 Data Acquisition

An application similar to the one in preliminary study
was designed to collect data for training SaccadeNet.
With the primary task being the same, the tests were
divided into three timed trials to reduce the potential
of simulator sickness. These trials were timed for ﬁve,
ten, and ﬁfteen minutes, respectively. A short break
followed each trial as per the participants’ needs. Fur-
thermore, user interactions in this user study are con-
sistent with the ﬁrst user study, and participants are
under the same experimental constraints. At the end
of the experiment, Kennedy’s sickness simulator ques-

tionnaire (Kennedy et al. 1993) was ﬁlled to quantify
the comfort level during this immersive experience.

A total of 14 participants were recruited for data
collection with an average age of 25.86 years and a
standard deviation of 4.37. To avoid redundancy in
data, each participant recruited for this phase diﬀered
from those who participated in the ﬁrst study. With
a normal or corrected-to-normal vision, the reported
median for their experiences using a VR device and an
eye-tracking device were four and three, respectively
on a 5-point Likert scale, with one being least familiar
and ﬁve being most.
Simulator Sickness Questionnaire (SSQ) Major-
ity (71.43%) of participants reported no signiﬁcant
signs of simulator sickness with an expected highest
average score for disorientation. Table 2 shows the re-
sults of the SSQ responses after data acquisition.

5.2 Learning & Inference

Input features. We trained SaccadeNet on time se-
ries data comprised of historical head rotations. Sev-
eral hand-crafted features were initially computed from
head rotations. After conﬁrming the eﬀect of each
of those features on saccade detection, the follow-
ing were eventually ﬁltered for training SaccadeNet.
Speciﬁcally, we categorized the head’s historical ﬁxa-
tion direction, its angular velocity, and the accelera-
tion between three successive frames as the most per-
tinent information for training SaccadeNet. Assuming
ft−2, ft−1andft denote the last three frames; we ﬁltered
the following features:

1. ht−2: y component from the head rotation at ft−2
2. ht−1: y component from the head rotation at ft−1
3. ht: y component from the head rotation at ft
4. ∆Dy: Change in direction from ft−1 to ft
5. V2: Angular Velocity from ft−2 to ft−1
6. V1: Angular Velocity from ft−1 to ft
7. ∆V : Change in Angular Velocity from V2 to V1
8. A2: Angular Acceleration from ft−2 to ft−1
9. A1: Angular Acceleration from ft−1 to ft
10. ∆A: Change in Angular Acceleration from A2 to

A1

As humans are least sensitive to horizontal changes,
and we only rotate the virtual environment horizon-
tally for redirection, each of these features were mea-
sured on the yaw/y/UP axis. Additionally, each fea-
ture was recorded independently in world space, i.e.
eye-in-world and head-in-world velocities.

These ten features, along with the ground truth of
saccadic events, completed a data point saved at each
frame. One million nine thousand six hundred and
sixty-seven of these data points were saved in total. As
determined by Sun et al. (Sun, Patney, et al. 2018), a
ballistic eye movement with an angular velocity of more
than 180◦/s was classiﬁed as a saccade. Therefore, the
ground truth was set to 1 in every frame for the onset
duration of a saccadic event, and 0 for the rest. Fur-
thermore, since our primary shortcoming is to predict

saccades only during an apparent head rotation, any
saccadic event during a head rotation velocity of less
than 150◦/sec was disregarded, and the ground truth
was artiﬁcially set to 0 for that frame. Therefore, the
model never predicts a saccade when the head rotation
velocity is less than 150◦/sec. This threshold is from
our hypothesis (section 3.4) and addresses the prob-
lem of VOR eye movements. Finally, we normalize the
dataset, and using a window size of nine, train Sac-
cadeNet for binary classiﬁcation with the optimization
details explained in Section 5.
Training, validation, and testing. The dataset was
divided into three subsets, i.e., training set, validation
set, and test set, with an 80:10:10 split ratio. After
each epoch, the model was evaluated with the vali-
dation set, while the ﬁnal predictive performance was
evaluated using a test set. The average precision during
validation was 89.91% and the mean accuracy 93.41%.,
and the precision on the test set was 88.72% and the
accuracy 93.51%.

Furthermore, the model was validated using the Area
Under the Curve of the Receiver Operating Charac-
teristics curve (AUC ROC). Figure 3 shows the ROC
graph plotted for various thresholds against True Pos-
itive Rate (TPR) and False Positive Rate (FPR). The
dotted line indicates the worst-case scenario, i.e., com-
pletely random predictions (AUC = 0.5), and the or-
ange curve shows the ROC. AUC for the ROC curve
was higher than the worst-case and closer to 1, i.e.,
0.966.

Figure 3: ROC is plotted against FPR on the x-axis and
TRP on the y-axis.

6 User study

For the ﬁnal experiment, we designed an application to
evaluate our event-based redirected walking system as
a whole. Whenever SaccadeNet predicts a saccade, we
adjust the VE and exploit the natural phenomenon of
change blindness to hide the redirection, thus ensuring
a smooth and distraction-free immersive experience.

6.1 Application and Procedure

According to a study published by Simons et al. (Si-
mons and R. Rensink 2005), change blindness is the
inability of observers to notice massive changes in plain

6

sight, which can be attributed to the lack of attention.
This phenomenon is commonplace in VR applications
where the users are typically engaged in cognitive tasks
such as training simulations and games. Therefore, we
designed a ﬁrst-person treasure hunt game on a myste-
rious island occupied by dragons and swamp crawlers
to examine our redirected walking system.
Application & Task. The main objective in this
immersive game is for the user to collect three crys-
tals from the ruins of an ancient abandoned arena. To
achieve this, the participants have to walk from their
initial spawn position to several predeﬁned locations in
VR marked by glowing crystals. At the beginning of
the experience, a quick tutorial explains various inter-
actions and the ﬁnal objective. Upon completing the
tutorial, the ﬁrst crystal and directions to the second
crystal are revealed. Each of these crystals unlocks a
new magical power. The staﬀ’s power to throw light-
ning bolts is unlocked with the ﬁrst crystal. 1

The eﬀect of change blindness induced due to sac-
cades is further strengthened by introducing a cognitive
workload in the form of distractors (T. C. Peck, Mary
C. Whitton, and Henry Fuchs 2008) when walking to-
wards the second destination. Tiny dragons spawned
far away ﬂew with an additional audio cue to a ran-
dom position in a predeﬁned orbital pattern around
the character and were separated by atleast 20◦. Fig-
ure 1 (left) shows a screen capture of a participant’s
perspective during the experiments. Participants were
directed to zap these dragons using their newly gained
lightning bolt power triggered by a button on the Vive
controller.

The experience depicted a cognitive task that caused
repeated head rotations to increase the probability of
predicting a saccade, leading to more redirections. The
shortest straight distance to the second destination
(38m) was multiple magnitudes larger than the longest
possible straight distance in the PTS (4.95m). The
cyan-colored box in Figure 1 (right) indicates the PTS,
and the grey and yellow lines indicate the physical and
virtual paths taken by the user during the ﬁnal exper-
iments, respectively.
Redirection algorithm. A steer-to-center redirec-
tion algorithm was implemented with a 2:1 turn reset
mechanism in place.
Procedure. The study conducted two diﬀerent exper-
iments, with the proposed redirected walking technique
being the independent variable tested. The ﬁrst ex-
periment involved participants going through the ear-
lier mentioned course of events with an experimental
condition, i.e., both redirection and reset mechanism
enabled. The second experiment repeated the same
course of actions with a controlled condition, i.e., only
reset mechanism enabled. At the end of each experi-
ment, relevant information variables such as the num-
ber of resets, total distance traveled, and total time
taken was gathered for later analysis. Additionally,
both the experiments were separated by a short break.

1A gameplay video and several screen captures are included

in the supplemental material.

6.2 Participants

While designing the experiment, we performed a power
analysis with an eﬀect size of 0.7 and determined the
sample size to be 20. The power of the experiment
was 0.956. We recruited 32 participants, with a mean
age of 27.38 years and a standard deviation of 3.74.
Twelve of them participated in the ﬁne-tuning exper-
iments and the other 20, 8 female, participated in the
ﬁnal evaluation. The reported medians for their ex-
periences using any VR headset and an eye-tracking
device were three and two, respectively, with normal
or corrected-to-normal vision. A 5-point Likert scale
was utilized to gather this information, with one being
the least and ﬁve being the most familiar.

Before the experiment started, participants were in-
formed about the reset mechanism and their objectives
in the game. They were also instructed to walk at an
average pace and be engaged with the task. Moreover,
subjective feedback at the end of the experiment asked
the question ”Did you notice any visual disparity or
shift in the virtual environment?”.

6.3 Analysis

A detailed analysis of the results obtained from the ﬁ-
nal study is presented in this section. The ﬁrst part
presents the quantitative results of evaluating the pre-
diction model, SaccadeNet. In the latter part, we show
quantitative results of the performance of the proposed
redirected walking technique in its entirety.

6.3.1 SaccadeNet - Quantitative Performance

Evaluation

SaccadeNet performed well within real-time limits with
an average accuracy of 94.75%, recall of 99.99%, and
sensitivity of 94.68%. The model’s F1-score for real-
time data was 0.72. Furthermore, a saccade typically
spans over several frames. An analysis of the results
showed that the training dataset was highly imbal-
anced towards the negative training examples, i.e., no
saccades, which leads to a higher number of false pos-
itives since the true positives are orders of magnitude
less. Thus, although the model can predict a sac-
cade correctly, it often mispredicts its duration by a
few frames before and after the actual saccade. This
causes more false positives. More importantly, it raises
the question as to at which point in time one should
apply the redirection for it to be imperceptible; at the
ﬁrst positive prediction or after X consecutive positive
predictions, and if the latter, what is the best X. We
perform four ﬁne-tuning experiments involving a to-
tal of 12 participants (2+3+3+4) to address this ques-
tion. These ﬁne-tuning experiments compensate for
the 56.52% precision obtained from the ﬁnal study.
Fine-tuning.
In the ﬁrst experiment, we applied
redirection at the ﬁrst positive prediction. Since the
VE was rotated just before the actual saccade began,
the two participants (2/2) reported that they noticed
the redirection. In the second and third experiments,
the VE was rotated only if two and three consecutive

7

frames made positive predictions, respectively. All par-
ticipants (3/3) in the second and two (2/3) in the third
experiment reported noticing the angular shift, while
the other participant (1/3) in the third experiment re-
ported a distraction-free experience. In the fourth ex-
periment, redirection was only applied if positive pre-
dictions were made over four consecutive frames. All
participants (4/4) reported a distraction-free experi-
ence and could not perceive any angular shift.

Given the outcomes of these experiments, we use a
window size of four consecutive positive predictions be-
fore applying the redirection in the ﬁnal user study for
evaluation.

6.3.2 Performance Evaluation of the Proposed

Redirected Walking Technique

We performed a statistical analysis to evaluate the pro-
posed RDW technique. A one-way analysis of vari-
ance (ANOVA) was performed between groups, with
repeated measures and α = 0.05. The analysis sta-
tistically diﬀerentiated the impacts of using and not
using redirection on our dependent variables, such as
the number of resets, total distance traveled, and to-
tal time taken. The eﬀect sizes for all the variables
were determined using partial eta squared (η2
p) val-
ues from the analysis. Partial eta squared values re-
i.e., toggling
ported that our independent variable,
redirection, accounted for almost 86.5% of the ob-
served variance in the number of resets.
In compar-
ison, it only contributed about 0.6% and 0.0% to the
variances observed in total distance traveled and total
time taken, respectively. The analysis also reported
a statistically signiﬁcant diﬀerence between the num-
ber of resets (F (1, 62) = 396.094, p < 0.001), with
and without using our proposed redirection technique.
However, a statistically insigniﬁcant eﬀect of toggling
redirection was observed on the total distance trav-
elled (F (1, 62) = 0.366, p > 0.05) and time taken
(F (1, 62) = 0.003, p > 0.05). Means for dependent
variables w.r.t. the independent are plotted in ﬁgure 4
with error bars at a conﬁdence interval of 95%.

Furthermore, as determined by (Sun, Patney, et al.
2018), the proposed system applied an average abso-
lute gain of 12.59◦ per redirection to the VE, at about
0.55 redirections per second. Thus, 1375.1◦ of aver-
age absolute gain was introduced to each participant’s
FoV in total with a standard deviation of 432.131◦.
Moreover, each participant covered a straight distance
of at least 38 meters in the VE by walking within the
3.5x3.5m2 PTS. Figure 1 (right) shows the path of a
participant during the ﬁnal study. Additionally, as the
lower precision was compensated by only redirecting
after four consecutive positive predictions, none of the
participants noticed any disparity or distraction in the
VE.

Alas, most of our participants (71.87%) reported no
signiﬁcant signs of simulator sickness. The mean score
for disorientation peaked among the other subscales,
similar to our previous studies. However, due to our
continuous eﬀorts of reducing this score, we can see it

Table 3: An overview of the SSQ responses.

Mean Median
Scores
10.14
Nausea(N)
Oculomotor(O)
9.95
Disorientation(D)13.05
12.39
Total
Score(TS)

9.54
3.79
13.92
9.35

SD Min Max
38.16
0
12.1
60.64
0
15.21
55.68
0
16.18
56.1
0
14.55

dropping from our ﬁrst study in Section 3.4 (16.24) to
the Data Collection in Section 5.1 (14.91), and ﬁnally,
this ﬁnal user study (13.05). Table 3 shows an overview
of the SSQ responses for the ﬁnal user study. Overall,
every participant had a smooth experience, with one
stating, “I felt like walking straight inside the game
but I was actually walking in circles to my surprise.
It’s a really good experience and I didn’t notice any
distractions.”

7 Discussion

Our motivation behind this technique is to perform sac-
cade prediction and saccadic redirection on commodity
hardware and eliminate the need for specialized hard-
ware, e.g., eye trackers, which is currently the case
for state-of-the-art (Sun, Patney, et al. 2018). How-
ever, like all RDW techniques, it only works in ap-
plications with a moderate cognitive workload. The
saccades used to mask the VE rotations are predicted
using the head rotation data in real-time. Therefore,
the technique is speciﬁcally eﬀective when the users
are preoccupied with a task that elicits repeated head
rotations such as battleground training simulations or
game-like scenarios similar to the one used in our ﬁ-
nal user study. On the contrary, in other situations
like indoor explorations and calmer experiences, this
technique will fail to perform. Additionally, since the
saccades tend to be purely eye-driven rather than a
joint head-eye movement when the object of interest
is within 18◦ of ﬁxation point, the technique will per-
form best when the targets are separated by 20◦. Fur-
thermore, this method of predicting saccades can have
multiple applications, including foveated rendering and
redirected walking.

7.1 Evaluation

The eﬃcacy of our system is examined in the ﬁnal user
study. We demonstrated that SaccadeNet eliminates
the need for high-end eye trackers for redirected walk-
ing, and qualitatively and quantitatively evaluated its
eﬀectiveness. Results showed that the users could ex-
plore long straight virtual distances of at least 38 me-
ters by naturally walking within a room-scale physical
tracked space of 3.5x3.5m2. Figure 1 (right) shows an
example of a long, straight, virtual walk with an en-
tirely circular physical path. During data collection,
even though artiﬁcially saving the ground truth to 0
for the frames when the head rotation velocity is less

8

Figure 4: Marginal means reported by one-way ANOVA analysis of (a) Distance Travelled, (b) Number of Resets in
PTS, and (c) Total Time Taken. Conﬁdence Interval = 95%

than 150◦/sec resulted in an imbalanced dataset and
the danger of overﬁtting, the predictions are compen-
sated in our ﬁne-tuning experiments during the ﬁnal
evaluation. Fine-tuning the hyper-parameters led us
to only apply redirection if four consecutive positive
predictions were made. Resulting in each of the 20
participants completing the task without any distrac-
tion, despite the model’s 56.52% precision. Upon ask-
ing, ”Did you notice any visual disparity or shift in the
virtual environment during the entire experience?” in a
post-test questionnaire, one participant stated, ”I did
not notice any visual disparity or shift. The experience
was smooth.”

7.2 Limitations

7.2.1 Seemingly Forced Head-Rotations

There could be extreme scenarios with repeated left-
right head rotations depending on the location of the
enemies. Similarly, there could be no head rotations
when the new target location lies within the users’
FoV. However, the enemies’ placement is random, and
we do not force the users to look in any particular
direction. Additionally, since the scope of our current
technique only includes training simulations and hyper-
active game-like experiences, we do not concern with
natural head rotations. Nonetheless, we plan to incor-
porate it in the future.

7.2.2 Slow User Movement

Similar to the image and object space subtle gaze di-
rectors used by Sun et al. (Sun, Patney, et al. 2018), if
the user is walking at a slower pace, there is little any
RDW technique can do without introducing distractors
and stimulating the required user actions. Moreover,
the task designed for the ﬁnal user study acts as a
moderate cognitive workload. It also helps us further
strengthen the eﬀect of change blindness by introduc-
ing an attention deﬁcit.

7.2.3 Comparison

We did not compare directly with other RDW tech-
niques because the comparison would be unfair with-

out embedded eye-trackers. Instead, quantitative anal-
ysis with resets-only baseline provides a more accurate
measure of eﬀectiveness and allows for more informa-
tive comparisons with other techniques.

7.3 Advantages

The options for VR headsets with integrated eye-
trackers are currently minimal. They are also con-
siderably more expensive than any regular VR head-
set, limiting their use for mainly research or indus-
trial purposes. Apart from the positional tracking, our
technique does not require additional hardware, i.e.,
eye-trackers or Kinect. It gives us many essential ad-
vantages such as computing requirements, accessibility,
and hardware cost, while distinguishing our approach
to redirection from the other state-of-the-art works re-
ported in the literature review section (Sun, Patney, et
al. 2018; Langbehn and Steinicke 2018; Langbehn, Lu-
bos, et al. 2017; Joshi and Poullis 2020). Moreover,
since positional tracking in mobile VR has recently
been enabled through SDKs like Google AR Core and
ARKit, optimizing the model for cell phone processors
can also potentially unlock the possibility of saccade
prediction on readily available mobile VR.

8 Conclusion and Future Work

This work presents a novel event-based redirection
technique powered by deep learning. SaccadeNet, a
CNN-based model, was trained to predict the change
blindness induced due to saccades during a head ro-
tation. Our technique exploits these predicted visual
suppressions and repeatedly applies subtle rotations to
the VE. These rotations, however subtle, are enough
for the users to subconsciously change their physical
walking direction while perceiving a straight motion in
VR. Three user studies were conducted for (i) reaﬃrm-
ing the relationship between head and eye directions,
(ii) training data acquisition for SaccadeNet, and (iii)
evaluating the proposed predictor with the redirected
walking technique and demonstrating its eﬀectiveness
in VR applications with moderate cognitive workload.
The studies also conﬁrmed that the proposed method

9

could successfully handle long straight walks, allowing
users to freely roam in large scale virtual spaces by
walking in applications such as hyper-active immersive
games or training simulations.

Future Work. Despite overcoming the eye-tracking
hardware requirement for saccadic redirection, many
potential avenues for improvement still exist. Firstly,
since we only classify saccades during head rotations,
we plan to address the exciting problem of predicting
saccades in normal viewing conditions. It will also be
interesting to explore the correlations of saccades with
diﬀerent scenes and stimulations in the FoV. For ex-
ample, prediction of saccades while the users attend to
various audio-visual stimulations like explosions, ﬂash-
ing lights, or spatial sounds embedded directly into the
scene without distracting or changing the 3D content.
Additionally, there is also room for improvement in
our dataset. We predict saccades solely based on his-
torical head rotation data. Therefore, we plan to add
saliency maps and depth maps from the virtual scenes
to train robust gaze forecasting models. Furthermore,
correlations between a user’s torso and gaze have also
been examined with some positive outcomes (L. and
H. 2019). We plan to exploit these correlations and
train a new model with additional features from hand,
foot, and torso movements. Lastly, the scope of redi-
rection in our technique is limited to only VE rotations;
we would also like to explore translational gains in the
future.

References

Azmandian, Mahdi, Timofey Grechkin, Mark Bolas, et
al. (2016). “The redirected walking toolkit: a uni-
ﬁed development platform for exploring large vir-
tual environments”. In: 2016 IEEE 2nd Workshop
on Everyday Virtual Reality (WEVR), pp. 9–14. doi:
10.1109/WEVR.2016.7859537 (cit. on p. 3).

Azmandian, Mahdi, Timofey Grechkin, and Evan
Suma Rosenberg (2017). “An evaluation of strate-
gies for two-user redirected walking in shared phys-
ical spaces”. In: 2017 IEEE Virtual Reality (VR).
IEEE, pp. 91–98. doi: 10.1109/VR.2017.7892235
(cit. on p. 3).

Bahill, A. Terry, Michael R. Clark, and Lawrence Stark
(1975). “The main sequence, a tool for studying hu-
man eye movements”. In: Mathematical biosciences
24.3-4, pp. 191–204 (cit. on p. 3).

Burr, David C., M. Concetta Morrone, and John Ross
(1994). “Selective suppression of the magnocellular
visual pathway during saccadic eye movements”. In:
Nature 371.6497, pp. 511–513 (cit. on p. 3).

Cheng, Lung-Pan et al. (2015). “Turkdeck: Physical
virtual reality based on people”. In: Proceedings of
the 28th Annual ACM Symposium on User Interface
Software & Technology. ACM, pp. 417–426 (cit. on
p. 3).

Christensen, Robert R et al. (2000). “Inertial-force
feedback for the treadport locomotion interface”. In:

Presence: Teleoperators & Virtual Environments 9.1,
pp. 1–14 (cit. on pp. 1, 3).

Darken, R. P., W.R. Cockayen, and D Carmein (1997).
“The omni-directional treadmill: A locomotion de-
vice for virtual worlds”. In: Proceedings of the 10th
Annual ACM Symposium on User Interface Software
and Technology. ACM, New York, pp. 213–221 (cit.
on p. 3).

Dichgans, Johannes and Thomas Brandt (1978). “Vi-
sual Vestibular Interaction: Eﬀects on Self-Motion
Perception and Postural Control”. In: Perception.
Handbook of Sensory Physiology, pp. 755–804 (cit.
on p. 2).

Dong, Zhi-Chao et al. (2017). “Smooth assembled map-
pings for large-scale real walking”. In: ACM Transac-
tions on Graphics (TOG) 36.6, p. 211 (cit. on p. 3).
E., Langbehn and Steinicke F. (2018). “Redirected
Walking in Virtual Reality”. In: Encyclopedia of
Computer Graphics and Games. doi: 10.1007/978-
3-319-08234-9_253-1 (cit. on p. 2).

E., Suma et al. (2012). “A Taxonomy for Deploying
Redirection Techniques in Immersive Virtual Envi-
ronments”. In: Proceedings of the 2012 IEEE Vir-
tual Reality. VR ’12. USA: IEEE Computer Society,
pp. 43–46. isbn: 9781467312479. doi: 10.1109/VR.
2012.6180877. url: https://doi.org/10.1109/
VR.2012.6180877 (cit. on p. 2).

Fernandes, K. J., V. Raja, and J. Eyre (2003). “Cy-
bersphere: The fully immersive spherical projection
system”. In: Communications of the ACM 46.9,
pp. 141–146 (cit. on pp. 1, 3).

G¨artner, Bernd and Sven Sch¨onherr (1997). Smallest
Enclosing Ellipses – Fast and Exact (cit. on p. 4).
Hodgson, Eric and Eric Bachmann (2013). “Comparing
four approaches to generalized redirected walking:
Simulation and live user data”. In: IEEE transac-
tions on visualization and computer graphics 19 (4),
pp. 634–643 (cit. on p. 3).

Huang, J. Y. (2003). “An omnidirectional stroll-based
virtual reality interface and its application on over-
head crane training”. In: IEEE Transactions on Mul-
timedia 5.1, pp. 39–51 (cit. on p. 3).

Iwata, H. (1999). “Walking about virtual environ-
ments on an inﬁnite ﬂoor”. In: IEEE Virtual Reality,
pp. 286–293 (cit. on p. 3).

Iwata, H. and T. Fujii (1996). “Virtual perambulator:
a novel interface device for locomotion in virtual en-
vironment”. In: Proceedings of the IEEE 1996 Vir-
tual Reality Annual International Symposium. IEEE,
pp. 60–65 (cit. on p. 3).

J, Stahl (1999). “Amplitude of human head movements
associated with horizontal saccades”. In: Exp Brain
Res. 126 (1), pp. 41–54 (cit. on p. 4).

Joshi, Yashas and Charalambos Poullis (2020). “Inat-
tentional Blindness for Redirected Walking Using
Dynamic Foveated Rendering”. In: IEEE Access 8,
pp. 39013–39024. doi: 10 . 1109 / ACCESS . 2020 .
2975032 (cit. on pp. 2–4, 9).

Kennedy, Robert S. et al. (1993). “Simulator Sickness
Questionnaire: An enhanced method for quantifying
simulator sickness”. In: The International Journal

10

of Aviation Psychology 3 (3), pp. 203–220 (cit. on
pp. 4–6).

L., Sidenmark and Gellersen H. (2019). “Eye head and
torso coordination during gaze shifts in virtual re-
ality”. In: ACM Transactions on Computer-Human
Interaction (TOCHI) 20 (1), pp. 1–40 (cit. on p. 10).
Langbehn, Eike, Paul Lubos, et al. (2017). “Applica-
tion of redirected walking in room-scale vr. In Vir-
tual Reality”. In: IEEE Virtual Reality (cit. on pp. 2,
9).

Langbehn, Eike and Frank Steinicke (2018). “In the
blink of an eye - Leveraging blink-induced suppres-
sion for imperceptible position and orientation redi-
rection in virtual reality”. In: ACM Transactions on
Graphics 37 (4), pp. 1–11 (cit. on pp. 3, 4, 9).

LaViola Jr., Joseph J. (2000). “A discussion of cyber-
sickness in virtual environments”. In: ACM SIGCHI
32.1, pp. 47–56 (cit. on p. 2).

Li, Yi-Jun et al. (2021). “OpenRDW: A Redirected
Walking Library and Benchmark with Multi-User,
Learning-based Functionalities and State-of-the-art
Algorithms”. In: 2021 IEEE International Sympo-
sium on Mixed and Augmented Reality (ISMAR),
pp. 21–30. doi: 10.1109/ISMAR52148.2021.00016
(cit. on p. 3).

Matsumoto, Keigo et al. (2016). “Unlimited Corridor:
Redirected Walking Techniques Using Visuo Haptic
Interaction”. In: SIGGRAPH ’16. New York, NY,
USA: Association for Computing Machinery. isbn:
9781450343725. doi: 10 . 1145 / 2929464 . 2929482
(cit. on p. 3).

Medina, Eliana, Ruth Fruland, and Suzanne Weghorst
(2008). “Virtusphere: Walking in a human size
VR “hamster ball””. In: Proceedings of the Human
Factors and Ergonomics Society Annual Meeting.
Vol. 52. 27. SAGE Publications Sage CA: Los An-
geles, CA, pp. 2102–2106 (cit. on p. 3).

Nagamori, A., K. Wakabayashi, and M. Ito (2005).
“The ball array treadmill: A locomotion interface for
virtual worlds”. In: IEEE Virtual Reality, pp. 3–6
(cit. on pp. 1, 3).

Nilsson, Niels Christian et al. (2018). “15 Years of Re-
search on Redirected Walking in Immersive Virtual
Environments”. In: IEEE Computer Graphics and
Applications 38.2, pp. 44–56. doi: 10 . 1109 / MCG .
2018.111125628 (cit. on p. 2).

O’Regan, J. Kevin et al. (2000). “Picture Changes
during Blinks: Looking Without Seeing and See-
ing Without Looking”. In: Visual cognition 7 (1–3),
pp. 191–211 (cit. on p. 3).

Panero, Julius and Martin Zelnik (1979). “HU-
MAN DIMENSION AND INTERIOR SPACE:
A SOURCEBOOK OF DESIGN REFERENCE
STANDARDS”. In: (cit. on pp. 2, 5).

Peck, T.C., H. Fuchs, and M.C. Whitton (2011). “An
evaluation of navigational ability comparing redi-
rected free exploration with distractors to walking-
in-place and joystick locomotion interfaces”. In:
IEEE, pp. 56–62 (cit. on p. 2).

Peck, Tabitha C., Mary C. Whitton, and Henry Fuchs
(2008). “Evaluation of Reorientation Techniques for

Walking in Large Virtual Environments”. In: 2008
IEEE Virtual Reality Conference, pp. 121–127. doi:
10.1109/VR.2008.4480761 (cit. on p. 7).

Poullis, Charalambos and Suya You (2009). “Auto-
matic creation of massive virtual cities”. In: 2009
IEEE Virtual Reality Conference. IEEE, pp. 199–
202 (cit. on p. 2).

R, Kothari et al. (2020). “Gaze-in-wild: A dataset for
studying eye and head coordination in everyday ac-
tivities”. In: Sci Rep 10 (2539) (cit. on pp. 2, 3).
Ramot, Daniel (2008). Average duration of a single
blink. https://bionumbers.hms.harvard.edu (last ac-
cessed 27th May 2022) (cit. on p. 3).

Razzaque, Sharif, Zachariah Kohn, and Mary C Whit-
ton (2005). Redirected walking. Citeseer (cit. on p. 3).
– (2001). “Redirected walking”. In: Proceedings of Eu-

rographics 9, pp. 105–106 (cit. on pp. 1–3).

Rensink, Ronald A. (2002). “Change detection”. In:
Annual review of psychology 53, pp. 245–277 (cit. on
p. 3).

Rensink, Ronald A., J. Kevin O’Regan, and James J.
Clark (1997). “To See or Not to See:The Need for
Attention to Perceive Changes in Scenes”. In: Psy-
chological science 8 (5), pp. 368–373 (cit. on p. 3).
Ruddle, R. A., E. P. Volkova, and H. H. Bulthoﬀ
(2011). “Walking improves your cognitive map in en-
vironments that are large-scale and large in extent”.
In: ACM Trans. on Computer-Human Interaction 18
(2), 10:1–10:22 (cit. on p. 2).

Ruddle, R.A. and S. Lessels (2009). “Walking Inter-
face to Navigate Virtual Environments”. In: ACM
Trans. on Computer-Human Interaction 16 (1), 5:1–
5:18 (cit. on p. 2).

Simons, DJ and RA Rensink (2005). “Change blind-
ness: past, present, and future”. In: Trends in cogni-
tive science 9.1, pp. 16–20. doi: 10.1016/j.tics.
2004.11.006 (cit. on p. 6).

Souman, Jan L et al. (2010). “Making virtual walking
real: Perceptual evaluation of a new treadmill con-
trol algorithm”. In: ACM Transactions on Applied
Perception (TAP) 7.2, p. 11 (cit. on p. 3).

Steinicke, Frank et al. (2008). “Taxonomy and Imple-
mentation of Redirection Techniques for Ubiquitous
Passive Haptic Feedback”. In: IEEE, pp. 217–223
(cit. on p. 2).

Suma, Evan A, Seth Clark, et al. (2011). “Leveraging
change blindness for redirection in virtual environ-
ments”. In: 2011 IEEE Virtual Reality Conference.
IEEE, pp. 159–166 (cit. on p. 3).

Suma, Evan A, Zachary Lipps, et al. (2012). “Impos-
sible spaces: Maximizing natural walking in virtual
environments with self-overlapping architecture”. In:
IEEE Transactions on Visualization and Computer
Graphics 18.4, pp. 555–564 (cit. on p. 3).

Sun, Qi, Anjul Patney, et al. (2018). “Towards vir-
tual reality inﬁnite walking: dynamic saccadic redi-
rection”. In: ACM Transactions on Graphics (TOG)
37.4, p. 67 (cit. on pp. 2–4, 6, 8, 9).

Sun, Qi, Li-Yi Wei, and Arie Kaufman (2016). “Map-
ping virtual and physical reality”. In: ACM Trans-
actions on Graphics (TOG) 35.4, p. 64 (cit. on p. 3).

11

T, Nakano et al. (2012). “Blink-related momentary ac-
tivation of the default mode network while viewing
videos”. In: Proceedings of the National Academy of
Sciences 110 (2), pp. 702–706 (cit. on p. 3).

Usoh, Martin et al. (1999). “Walking ¿ walking-in-place
¿ ﬂying, in virtual environments”. In: ACM, pp. 359–
364 (cit. on p. 2).

Volkmann, Frances C. (1986). “Human visual suppres-
sions”. In: Vision Research 26 (9), pp. 1401–1416
(cit. on p. 3).

Walther-Franks, Benjamin et al. (2013). “Suspended
Walking: A Physical Locomotion Interface for Vir-
tual Reality”. In: International Conference on En-
tertainment Computing. Springer Berlin Heidelberg,
pp. 185–188 (cit. on p. 3).

Williams, Betsy et al. (2007). “Exploring large virtual
environments with an HMD when physical space is
limited”. In: ACM, pp. 41–48 (cit. on p. 3).

Williams, Niall L., Aniket Bera, and Dinesh Manocha
(2021). “ARC: Alignment-based Redirection Con-
troller for Redirected Walking in Complex Environ-
ments”. In: IEEE Transactions on Visualization and
Computer Graphics 27.5, pp. 2535–2544. doi: 10 .
1109/TVCG.2021.3067781 (cit. on p. 3).

Y, Fang et al. (2015). “Eye-Head Coordination for Vi-
sual Cognitive Processing”. In: PLOS ONE 10 (3)
(cit. on p. 4).

Zank, Markus and Andreas Kun (2015). “Using lo-
comotion models for estimating walking targets in
immersive virtual environments”. In: IEEE (cit. on
p. 2).

– (2016). “Eye tracking for locomotion prediction in

redirected walking”. In: IEEE (cit. on p. 2).

12

