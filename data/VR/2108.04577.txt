1
2
0
2

g
u
A
0
1

]
I

N
.
s
c
[

1
v
7
7
5
4
0
.
8
0
1
2
:
v
i
X
r
a

This paper has been submitted to IEEE Access. Copyright may change without notice.
Please cite it as: M. Lecci, M. Drago, A. Zanella, M. Zorzi, "An Open Framework for Analyzing and Modeling XR Network
Trafﬁc," Submitted to IEEE Access, Pre-print available on arXiv.

Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.

Digital Object Identiﬁer

An Open Framework for Analyzing and
Modeling XR Network Trafﬁc

MATTIA LECCI, Graduate Student Member, IEEE, MATTEO DRAGO, Graduate Student Member,
IEEE, ANDREA ZANELLA, Senior Member, IEEE, and MICHELE ZORZI, Fellow, IEEE
Department of Information Engineering, University of Padova, Italy (e-mails: {leccimat, dragomat, zanella, zorzi}@dei.unipd.it)

Corresponding author: Mattia Lecci (e-mail: leccimat@dei.unipd.it).

This work was partially supported by the National Institute of Standards and Technology (NIST) through Award No. 60NANB19D122.
The identiﬁcation of any commercial product or trade name does not imply endorsement or recommendation by NIST, nor is it intended to
imply that the materials or equipment identiﬁed are necessarily the best available for the purpose. Mattia Lecci’s activities were also
supported by Fondazione CaRiPaRo under the grant “Dottorati di Ricerca 2018.” A preliminary version of this paper proposing a simpler
trafﬁc model with fewer trafﬁc traces was presented in [1].

ABSTRACT Thanks to recent advancements in the technology, eXtended Reality (XR) applications are
gaining a lot of momentum, and they will surely become increasingly popular in the next decade. These
new applications, however, require a step forward also in terms of models to simulate and analyze this type
of trafﬁc sources in modern communication networks, in order to guarantee to the users state of the art
performance and Quality of Experience (QoE).
Recognizing this need, in this work, we present a novel open-source trafﬁc model, which researchers can
use as a starting point both for improvements of the model itself and for the design of optimized algorithms
for the transmission of these peculiar data ﬂows. Along with the mathematical model and the code, we
also share with the community the traces that we gathered for our study, collected from freely available
applications such as Minecraft VR, Google Earth VR, and Virus Popper. Finally, we propose a roadmap for
the construction of an end-to-end framework that ﬁlls this gap in the current state of the art.

INDEX TERMS Trafﬁc modeling, trafﬁc analysis, network simulations, virtual reality applications

I. INTRODUCTION
After several years of innovations, the technology is ﬁ-
nally ready for applications such as Virtual Reality (VR),
Augmented Reality (AR) and Mixed Reality (MR) to go
mainstream (in the following we will use the term eXtended
Reality (XR) as a general expression to consider all these
distinct interaction modes). According to some estimates, by
2025 there will be over 200 million people using XR for
immersive gaming experience and 95 million enjoying live
events in this novel way [2]. This immediately translates
in an increasing sale of devices and headsets dedicated to
experience this new type of contents, with an estimated
shipment of these devices in the order of tens of millions in
the coming decade, generating billions in revenue for all the
ﬁelds in which this technology will be deployed [3]–[5].

Although it all started from the entertainment and video
gaming arenas, where players could immerse in a virtual 3D
world, now we can see XR applied in various ﬁelds, such
as building or landscape design, real estate, marketing, and

healthcare, opening to the possibility of learning new con-
cepts and training employees for difﬁcult situations in a com-
pletely different way [3], [6]–[10]. Automotive companies,
for example, are using VR to cut the time that leads to the
physical model of a new product from weeks to days [5]. The
peculiarity of this new class of contents, in fact, besides the
wide range of use cases, is that the end user does not passively
receive the information but, depending on the interaction
with the virtual environment in which he/she is immersed,
the trafﬁc ﬂow towards the content provider can change
accordingly. From a sales point of view, instead, VR can give
customers realistic experiences with products, allowing them
to easily consider different options and conﬁgurations [3].

In this paper, we will focus on examples related to the
video gaming world (even though equivalent conclusions
can be drawn for different XR applications), where the user
interacts with the application using a keyboard or joypad, and
the results of such actions are immediately seen on the PC or
TV screen. Through Head Mounted Devices (HMDs), when
playing with videogames supporting VR, users can also react
by moving their heads and, for instance, depending on where

VOLUME x, xxxx

1

 
 
 
 
 
 
M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

the head is pointing, the application streams distinct portions
of the environment [11].

Even though traditionally gaming software ran on devices
which needed to respect several hardware constraints to
generate high-quality images, now the paradigm is shifting
towards a cloud approach [12], [13]. This can be extended
to all other use cases besides gaming, and for this reason,
we can refer to this new paradigm as Cloud XR. By moving
the computing and graphical processing units into the cloud,
in fact, less powerful devices can be used to fully exploit
this new technology. This would beneﬁt not only in terms
of the actual cost of an HMD (which still plays a huge role
in promoting the adoption by new users) but also in the
ﬁnal Quality of Experience (QoE). Having all the computing
resources self-contained in the device would mean not only a
higher weight and volume, but also concerns in terms of heat
and battery life [4], [9].

This shift towards cloud infrastructures requires the opti-
mization of current communication systems, to fully support
distributed XR services. To this end, we need accurate mod-
els of the applications that generate these data ﬂows and, to
the best of our knowledge, no previous work has addressed
this problem so far.

In this work we try to ﬁll this gap by proposing a trafﬁc
model that emulates an XR application, while also sketching
a roadmap to guide researchers in the development of more
precise models, using ours as a baseline.

To further understand what are the steps that most inﬂu-
ence the XR performance, it is useful to describe a common
end-to-end XR architecture [2], [9]. First, we can start from
the collection and processing of sensory and tracking infor-
mation, delegated to an ad hoc device. Then, this information
is sent to an XR server to compose the viewport, i.e., what
is actually shown to the user. This process includes 2D/3D
media encoding and the generation of additional metadata
(including the scene description). The device’s presentation
engine at the client side, after receiving and decoding the
information stream, generates the images to display. These
images are derived from the decoded signals, rendering meta-
data, and other information, if applicable. Finally, video and
audio tracks associated with the current pose are generated
by synchronizing and spatially aligning the rendered media.
These steps need to be accomplished with minimal delay to
guarantee adequate QoE.

In fact, the motion-to-photon latency, i.e., the time from an
action (e.g., a head movement) to the update of what is shown
on the display must be below 20 ms to avoid the so called
cybersickness, associated to disorientation and dizziness [2],
[14]–[18]. Following this physiological constraint, several
industry players pose the network requirements, in terms of
latency, in the range of 5–10 ms [2], [4], [9], [14], [19].
Also in terms of the gaming QoE, it has been demonstrated
that for ﬁrst-person shooters, racing games, and team soccer
matches, application latency directly impacts the results of
competitive e-sports and, if not properly addressed, would
lead to abandoning the game [12]. This translates into strin-

gent constraints both in Downlink (DL) and in Uplink (UL),
considering that not only the content must be streamed as
soon as it is required, but also the user movements need
to be promptly notiﬁed to the server. For this reason, the
software that collects each movement input must consider
all 6 Degrees of Freedom (6DoF), tracking both translations
and rotations in the three perpendicular axes (based on the
VR device, some may consider only rotational motion, i.e.,
3 Degrees of Freedom (3DoF)). To take immersive mobile
experience to the next level, many improvements will be
required in head, body, and even gaze tracking [7].

It is also important to distinguish between processing
latency, associated with computation and rendering, and net-
work latency. Rendering complex gaming images can be
quite demanding, and the delay introduced by these oper-
ations can be larger than that caused by network services,
which further motivates the need to ofﬂoad these functions to
proper cloud infrastructures [12].

Besides delay-related issues, an additional problem con-
sists in the bursty nature of the XR trafﬁc, meaning that
the throughput measured over short time windows could be
much higher than its average value [9], which can be the case
for an application that periodically generates collections of
packets to refresh the viewport. Another aspect impacting the
throughput is that, in order for the technology to be as close
as possible to human vision, we will need a higher spatial
and temporal resolution of the content presented to the user
than currently possible (i.e., 3D 360° 8196×4096 resolution
at 90 Hz and beyond display refresh rate) [7], [9], [14].

The core technology that is expected to guarantee the
satisfaction of all these requirements, by paving the way for
an optimized distribution of processing capabilities, is 5G.
Many players have already invested in 5G for the rising of
XR, for operations at both sub-6 GHz and mmWave [7], [8],
[20], [21].

Nonetheless, even though some efforts have also been
devoted by standard bodies to the redaction of technical
reports [14], [19], at the present time researchers are limited
by the lack of precise trafﬁc models representing the stream
to/from an XR server. Having these models would allow the
research community to design telecommunication solutions
that could reduce the delay contribution related to the net-
work, while also considering all the processing steps. For this
reason, we propose a generative model for XR trafﬁc sources,
obtained from real application traces, and we also delineate
a roadmap of the necessary steps to further improve it with
additional features able to cope with the aforementioned
problems, i.e., motion-to-photon latency, burstiness, capacity.
While in Sec. II we summarize the current state of the art
in the XR arena, Sec. III is devoted to the description of
the acquisition setup that we used to collect about 70 GB
of data for a total of more than 4 hours of traced trafﬁc
time using different VR applications, both from the hardware
and from the software point of view. We will also describe
each of these applications and illustrate how we analyzed
the dataset. The model obtained from this analysis will be

2

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

presented in Sec. IV, and its end-to-end validation, along with
some example use cases, are discussed in Sec. V. Finally, in
Sec. VI we propose a roadmap to extend our baseline model
with additional increasingly complex features, and Sec. VII
concludes the paper.

II. STATE OF THE ART
A seminal conceptual model that describes the human and
technical elements creating the participatory environments of
virtual reality systems was proposed in [22], dating back to
1994. This demonstrates that the interest in the deﬁnition of
common models for the study of this framework started even
before the technology was ready, or even invented.

Despite the research interest in this ﬁeld, to the best of our
knowledge little work has been done on the creation of gener-
ative trafﬁc models in XR contexts, while the focus was put
on different aspects of the technology. In particular, a huge
effort has been devoted to the creation and validation of prac-
tical systems that use immersive technology to interact with
the world in different ways. An example can be found in [23],
which describes a system for the interactive analysis of
large datasets with time-dependent data, realized on a multi-
processor parallel machine in order to guarantee a smooth
user experience. Instead, in [24] the authors developed a
proof-of-concept system, combining Oculus Rift HMD and
the Phantom Premium 1.5 High Force haptic device with the
goal of demonstrating the feasibility of combining HMD and
haptics in one system. Also, XR solutions have been tested
for purposes of architectural design [25] and for providing
virtual performance instructions and feedback on users that
want to play a real piano [26].

From a more technical perspective, a complete overview
of the latest developments on immersive and 360◦ video
streaming can be found in [11], where the author aims at
providing a complete overview on four of the most important
challenges in this ﬁeld, namely: omnidirectional video cod-
ing and compression, subjective and objective QoE and the
factors that can affect it, saliency measurement and Field-of-
View (FoV) prediction, and adaptive streaming of immersive
360◦ videos. As stressed in [11], ﬁnding a proper way to
measure the user’s QoE may be difﬁcult. This is especially
important with respect to the design of telecommunication
infrastructures able to optimize the experience of the user,
and to guarantee constant and stable service quality.

For this reason, a lot of effort has been devoted to cre-
ating network solutions for the maximization of the quality
of the delivered content. In [27], for example, the authors
proposed a scheme for uplink delivery of tile-based VR video
over cellular networks. In particular, they formulate a re-
source allocation problem as a frequency and time-dependent
non-deterministic polynomial NP-hard problem, and propose
three distinct algorithms to solve it. Instead, in [28] the
authors consider a QoE-driven transmission of VR 360◦
contents in a multi-user massive MIMO wireless network.
Speciﬁcally, in this scenario multiple users in the cell are
requesting the same content, and the goal is to optimize the

reception of such information through a stable scheme for
the transmission of the viewport tiles. In this work, they also
try to allocate the power in order to guarantee a consistent
delivery rate for each stream.

The impact of latency on the overall experience of the
user has been mentioned in Sec. I, along with the importance
of tracking the movements of the user in applications with
strict delay requirements. The authors of [29] used a real
VR head-tracking dataset to maximize the quality of the
delivered video chunk under low-latency constraints. In that
case, a deep recurrent neural network was designed for the
prediction of the users’ FoV (allowing to cluster those with
overlapping FoV) while information on the future content
and the users’ locations was used as input of a proactive
physical-layer multicast transmission scheme.

A key solution to the latency problem would be to rely on
the capabilities of 5G and edge cloud, exploiting what has
been referred to as Cloud XR in Sec. I. Indeed, in [30] the
authors demonstrated that 5G and edge cloud are necessary to
sustain the requirements of applications such as VR gaming.
All these solutions, however, lack a model capable of
generating data ﬂows that can easily be associated with a
real XR application. The approach of [27] consisted in using
240 frames of each 8K 360◦ uncompressed video sequence
available from [31]. In that case the author applied the
HVEC Kvazaar encoding procedure, setting the frame rate
to 25 Frames Per Second (FPS) and the Group of Pictures
(GoP) size to 8, and using a constant tiling scheme, ideal for
the purpose of their work. Despite the high level of details
implemented in such a model, the use of a trace-based ﬂow
is limiting per se, considering also the limited portion of the
video that they selected. Having an ofﬂine encoding strategy
is another limit, that in our framework has been overcome by
integrating the rendering server in the processing pipeline.

Also in [28], the simulation setup from the point of view
of the VR architecture was deﬁned in order to highlight the
features of the algorithms proposed by the authors, and the
nature of the trafﬁc ﬂow (e.g., average frame size, inter and
intra-frames correlation, inter-frame interval, etc.) was not
taken into account.

Regarding the problem of tracking the movements of the
users, in [29] the authors fed the recurrent neural network
with the 3DoF traces from [32], tracking the pose of 50
different users watching a catalog of 10 HD 360◦ videos from
YouTube (60 seconds long, 4K resolution, 30 FPS, FoV of
100◦ × 100◦). Having a generative model that creates such a
dataset based on statistical studies on a collection of different
traces would have greatly aided the training of the neural
network used in [29]. Also, ﬁnding a dataset that represents
well the problem that we want to solve is usually not feasible,
and this may further limit the research outcomes.

As a consequence, our goal is to provide the community
with a tool for the automatic generation of such traces. A
preliminary version of this work was proposed in [1], and
here we extend it with the acquisition of longer and more
heterogeneous traces, that now include realistic interaction

VOLUME x, xxxx

3

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

with several VR applications. This extension also allowed a
more detailed and thorough validation of the model. Besides
making both the model and the traces public, we also propose
a possible roadmap for making the framework as complete
and detailed as possible, highlighting the most important
contributions that would beneﬁt researchers aiming at the
design of ad hoc network protocol optimizations for this new
type of trafﬁc sources.

III. VR TRAFFIC: ACQUISITION AND ANALYSIS
In this section, we describe our basic trafﬁc modeling work.
Speciﬁcally, in Sec. III-A we describe our acquisition setup
and the VR applications that we acquired, then in Sec. III-B
we analyze the raw trafﬁc traces, and the different streams
composing them, both in terms of content and in terms of
statistics.

A. ACQUISITION SETUP
For the rendering server, we used a desktop PC equipped with
an Intel Core i7 processor, 32 GB of RAM, and an NVIDIA
GeForce RTX 2080 Ti graphics card. For the headset, instead,
we used an iPhone XS enclosed in a VR cardboard, which
allows a realistic interaction with the applications. The two
nodes were connected via Wi-Fi to improve the user’s free-
dom of movement, at the cost of a slightly less stable channel
and of possible interference from other surrounding devices.
VR applications were thus run on the rendering server and
streamed to the headset using the application RiftCat 2.0 (on
the server), and VRidge 2.7.7 (on the phone).1 This setup
allows the user to play VR games on the SteamVR platform
for up to a maximum of 10 minutes continuously, enough
to obtain trafﬁc traces to be analyzed (note that this limit
is given by the free version of VRidge, and is absent in
the premium version). Many settings can be tuned in this
application, such as the display resolution, the frame rate
(either 30 or 60 FPS), the target data rate (i.e., the data rate
the application will try to consistently stream to the client,
which can be set from 1 to 50 Mbps), the video encoder
(NVIDIA NVENC was used), and the video compression
standard (H.264 was chosen), among other advanced settings.
As opposed to [1], we acquired traces while realistically
interacting with available VR applications using mouse, key-
board, and head movements. Our setup only allowed us to
interact with 3DoF, i.e., the user was seated and only head
rotations were sensed. To simplify the analysis of the trafﬁc
stream, audio was not activated.

For this purpose, we selected three popular VR applica-

tions targeting different types of interactions. Speciﬁcally:

• Minecraft: an extremely popular game, with the mod
Vivecraft enabling room-scale or seated VR experi-
ences. The user can explore the virtual environment
by walking or swimming, and interact with the virtual
world by cutting trees, digging holes, crafting tools, etc.

1riftcat.com/vridge

4

• Virus Popper: during this fast-paced educational game,
many cartoony-looking viruses swarm a virtual room,
and the user has to attack them with cleaning tools for
survival.

• Google Earth VR - Tour: the VR version of Google
Earth, allowing a user to explore the world with satellite
imagery, 3D terrain of the entire globe, and 3D buildings
in hundreds of cities around the world. The SteamVR
application also enables tours, teleporting the user all
around the world every few seconds.

• Google Earth VR - Cities: in this case, a more interactive
experience is yielded, allowing the user to fully explore
cities or landmarks for as long as they want.

Please note that Google Earth VR was used in two different
ways, thus allowing us to analyze two different versions of
the same application.

To capture streamed packets, we ran Wireshark, a popular
open-source packet analyzer, on the rendering server. The
trafﬁc analysis was performed at 30 and 60 FPS for target
data rates of {10, 20, 30, 40, 50} Mbps and for all 4 appli-
cations with a resolution of 1920×1080, for a total of over
70 GB of PCAP traces and 4 hours of analyzed VR trafﬁc.
Our dataset containing the processed VR trafﬁc traces can be
found within our software and can be easily reused, as later
described in Sec. V.

B. TRAFFIC ANALYSIS
As described in [1], we were able to partially reverse engineer
both the DL and UL streams, and together with the help of
RiftCat’s developers, we are now able to reliably process the
raw trafﬁc traces. We found that UDP sockets over IPv4 are
used and both UL and DL streams contain several types of
packets. Speciﬁcally, the UL stream contains packets such
as synchronization, video frame reception information, and
frequent small head-tracking information packets, whereas
the DL stream contains synchronization, acknowledgment,
and video frame packet bursts.

To improve the stream quality, the RiftCat team developed
a custom version of the ENet protocol2, a relatively thin,
simple and robust network communication layer on top of
UDP, which offers reliable, in-order packet delivery.

In Fig. 1 we show a visual representation of a slice of
bidirectional VR streaming. The plot shows the main data
streams in both DL and UL, giving an idea of how this
transmission works.

Most of the trafﬁc is concentrated in DL and is made up of
packet bursts encoding video frames. Video frame fragments
were consistently found to be 1320 B long in all acquired
traces, with a data size (the UDP payload) of 1278 B. The
last packet of the burst also has the same size as the others,
suggesting that padding has been used in order to simplify
the protocol, although this biases the frame size distribution
to be discrete.

2Available: https://github.com/nxrighthere/ENet-CSharp

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

Video Frame (DL)

Frame Feedback (DL)

Frame Feedback (UL)

Head Tracking (UL)

1,500

1,000

500

]

B

[

e
z
i
s

t
e
k
c
a
P

0

0

1,500

1,000

500

0

10.6

10.8

11

11.2

10

20

30

40

50

60

70

80

90

100

Time [ms]

FIGURE 1: Portion of trafﬁc trace from Virus Popper (50 Mbps, 30 FPS). For this trace, 130–140 individual fragments make
up a video frame burst.

Virus Popper

Google Earth VR - Tour

Google Earth VR - Cities

Minecraft

Expected

30 FPS

60 FPS

]
s
p
b
M

[

e
t
a
r

d
e
r
u
s
a
e

M

60

40

20

0

]
s
p
b
k
[

e
t
a
r

o
e
d
i
v
-
n
o
n
L
D

.
g
v
A

40

30

20

10

0

]
s
p
b
k
[

e
t
a
r
L
U

.
g
v
A

140

120

100

80

10

20

30

40

50

20

40

60

20

40

60

Target rate [Mbps]

Measured rate [Mbps]

Measured rate [Mbps]

(a) Measured downlink data rate.

(b) Average non-video DL rate.

(c) Average UL rate.

]

B
k
[

e
z
i
s

e
m
a
r
f

o
e
d
i
v

.
g
v
A

200

100

0

]
s

m

[

I
F
I

.
g
v
A

35

30

25

20

15

0

20

40

60

Measured rate [Mbps]

20

40

60

Measured rate [Mbps]

(d) Average video frame size.

(e) Video Inter-Frame Inter-arrival (IFI)
time.

FIGURE 2: Results from acquired VR trafﬁc traces.

The second most noticeable trafﬁc stream is the UL head
tracking information, which the headset acquires and sends
to the rendering server to update the viewport to be rendered.
The head tracking payload was identiﬁed to be either 192 B
or 97 B long, sometimes changing over the course of a single
trafﬁc trace, although the reason why different packet sizes

were found is unclear.

Finally, smaller packets in both UL and DL, with payloads
of respectively 21 B and 10 B, were identiﬁed to contain
feedback on the reception of video frames, which is probably
used in the streaming protocol to decide whether or not to
retransmit some frames.

VOLUME x, xxxx

5

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

F
D
P

F
D
C

0.04

0.02

0

20

40

60

80

100

120

10

15

20

Frame Size [kB]

IFI [ms]

(a) Video frame size distribution.

(b) IFI distribution.

FIGURE 3: Video frame distributions for Virus Popper (30 Mbps, 60 FPS).

1

0.5

F
D
P

0

25

By reverse-engineering the bits composing the UDP pay-
load of video frames, it was possible to identify a recurring
set of bits suggesting a 31 B APP-layer header and allowing
us to identify some key ﬁelds, such as (i) the frame sequence
number, (ii) the number of fragments composing the frame,
(iii) the fragment sequence number, (iv) the total frame size,
and (v) a checksum. This information allowed us to reliably
process and aggregate video frames.

Given the settings of the streaming application (i.e., frame
rate and target data rate), it is clear that a Constant Bit
Rate (CBR) video encoding is performed in the background.
In Fig. 2a we show the performance of the video encoder,
almost always exceeding the target rate although by only
5–10%. A simple explanation of this behavior might be
the underestimation of header sizes in the computations of
the CBR encoder, such as the header of the custom ENet
protocol. Notably, both frame rates behave similarly across
all four applications, with stable performance.

Figs. 2b and 2c show the low overhead due to non-
video DL and UL transmissions (including head tracking),
respectively. Speciﬁcally, non-video DL trafﬁc only accounts
for 3–5 kbps while UL trafﬁc for about 135–150 kbps, with
60 FPS traces consistently showing higher rates with respect
to 30 FPS ones, probably due to the doubled amount of
feedback. Only two out of our forty traces show different
rates, possibly due to some imperfection in the streaming. In
any case, these trafﬁc ﬂows are much lower than the target
rates and appear constant, irrespective of the data rate or
the application. This consideration lead us to the decision
of ignoring them, focusing only on modeling the DL video
trafﬁc.

Considering R the target data rate and F the application
frame rate, the average video frame size is expected to be
close to the ideal S = R/F , as shown in Fig. 2d. Note
that the x-axis reports the measured data rate rather than the
target data rate, i.e., the average data rate estimated from the
acquired traces, which differs slightly from the target rate, as
shown in Fig. 2a.

Furthermore, Fig. 2e shows that the average Inter-Frame

Inter-arrival (IFI) time perfectly matches the expected 1/F ,
equal to 33.¯3 ms for 30 FPS traces and 16.¯6 ms for 60 FPS
traces.

Moving to the analysis of the Probability Density Func-
tions (PDFs), it is important to know that in a collection
of packets associated to a video source, we can usually
distinguish Intra-coded frames (I-frames) (sometimes called
called keyframes), Predictive-coded frames (P-frames), and
Bipredictive-coded frames (B-frames). While I-frames are
compressed similarly to simple static pictures, P-frames ex-
ploit the temporal correlation of successive frames to reduce
the compressed frame size. B-frames, instead, can exploit
the information from both previous and subsequent frames,
further improving the compression efﬁciency at the cost of
non-real-time transmission. All the details associated with
these compression techniques are regulated by standards like
H.264 [33].

Interestingly, a single clear peak is visible in Fig. 3a,
suggesting that different frames do not fall into different cat-
egories. The RiftCat team conﬁrmed that no clear distinction
among frame types is to be expected, since H.264 Periodic
Intra Refresh is used when NVENC is selected as the en-
coder. This means that instead of large keyframes, the image
can be effectively refreshed over many frames using columns
of intra-blocks that move across the video from one side to
the other. In this way, each frame can be capped to roughly
the same size, which is extremely convenient to maintain a
CBR stream.

As already mentioned, VRidge simpliﬁes the transmission
by discretizing some units. Fig. 3a shows a clear staircase
Cumulative Distribution Function (CDF) for the video frame
size, suggesting that video frames have been padded as the
underlying distribution is indeed discrete, with a distance
between consecutive stairs of 1278 B, i.e., the UDP payload
of packets containing fragments of video frames.

Similarly, the IFI time also appears to be discretized with a
millisecond precision around the mean 1
F , as seen in Fig. 3b,
although some noise due to, e.g., variable rendering and en-
coding time, wireless channel condition, transmission queue

6

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

10 Mbps

20 Mbps

30 Mbps

40 Mbps

50 Mbps

30 FPS

60 FPS

t
s
e
t

v
o
n
r
i

m
S
-
v
o
r
o
g
o
m
l
o
K

0.6

0.4

0.2

0

N akaga mi
Logistic
Laplace
Cauchy
G aussian
W eibull M ax E.V.
t-distribution

2
χ

Exponential
Rayleigh

Pareto
Beta
G aussian
W eibull M in E.V.
Truncated
Truncated

t
s
e
t

v
o
n
r
i

m
S
-
v
o
r
o
g
o
m
l
o
K

1

0.8

0.6

0.4

0.2

0

Rice

N akaga mi
Cauchy
G aussian

Logistic
Laplace
t-distribution
G aussian
Truncated

2
χ

Beta

Exponential
Rayleigh
Pareto
Rice
W eibull M in E.V.
W eibull M ax E.V.
Truncated

Fitting distribution

(a) Video frame size ﬁt quality.

Fitting distribution

(b) IFI ﬁt quality.

FIGURE 4: Video frame ﬁt qualities for the Google Earth VR - Cities application. Fit quality is measured using the KS test
(lower is better). Box plots show median (red), 1st and 3rd quartiles (box), minimum and maximum (whiskers) of the KS test
with a given distribution, while markers show the exact values for the different traces.

state, transmission times, just to mention a few, smooths the
CDF.

IV. TRAFFIC MODEL

Following the analysis of Sec. III-B, in this section we will
describe the proposed model for VR trafﬁc based on the
collected VR trafﬁc traces.

The analysis presented in the previous section reveals
that both packet sizes and IFI times appear to be discrete
in the collected data traces. However, such granularity is
likely due to speciﬁc design choices of the communication
protocols used by the considered applications, rather than
being a native characteristic of the XR services. Therefore,
we believe it is more suitable to use continuous random
variables to model the size of the data blocks generated by the
XR application and the time between them. By doing so, we
free our model from the speciﬁc constraints of this streaming
application, with no loss of generality (as the discrete case
can always be obtained from the continuous one), and in fact
making it easier to accommodate other (non-discrete) cases
in our framework if needed.

A. DISTRIBUTION FITTING

Given the extremely large number of samples per trace (200–
600 s at 30 or 60 FPS), common quality of ﬁt statistical
tests yield poor performance due to the discretized distribu-
tions. Intuitively, while the PDF of discrete and continuous
distributions takes completely different forms, the CDF of a
discretized distribution is simply a staircased version of the
related continuous distribution. In that case, the goodness of
ﬁt can be tested by comparing the CDFs, for example using

the Kolmogorov-Smirnov (KS) test [34], deﬁned as:

KS = sup

x

|Fe(x) − Ft(x)| ,

(1)

where supx is the supremum of the set of distances, Fe(x)
is the empirical CDF of the acquired data, and Ft(x) is the
CDF of the target distribution. The KS test will thus be used
to score the quality of ﬁt, where values closer to zero indicate
a better parameter estimation.

To ﬁt and evaluate the best probability distributions for
our data, we used the popular SciPy library [35]. We tested
15 of the most common continuous univariate distributions
available in the scipy.stats package, evaluating their
performance on both frame size and IFI on our trafﬁc traces.
Note that the SciPy library performs a maximum likelihood
estimation of the parameters of the distribution, including
location and scale, which SciPy adds to all continuous
distributions by transforming the random variable X into
(X − loc)/scale. Given the exceptional accordance between
expected values and computed averages (see Figs. 2d and 2e)
and considering the proposed generative model (described in
Sec. IV-B), we ﬁxed the location parameter to the expected
value (i.e., R/F for the frame size and 1/F for the IFI), ﬁt-
ting only the scale and the remaining parameters. A selection
of distributions is shown in Fig. 4.

We found that the Student’s t and Logistic distributions,
closely followed by the Laplace, Gaussian, and Cauchy
distributions, were the best ﬁtting ones in almost all traces
for both frame size and IFI. Fig. 5 shows how similar the
ﬁtted distributions actually are. Although the Student’s t
distribution performs slightly better than the Logistic one
in the slight majority of the collected traces, in our case
the Logistic distribution was the best choice. In fact, the

VOLUME x, xxxx

7

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

Data

t-distribution

Logistic

Laplace

F
D
C

1

0.8

0.6

0.4

0.2

0

F
D
C

1

0.8

0.6

0.4

0.2

0

KS: 0.0476
KS: 0.0483
KS: 0.068

KS: 0.0732
KS: 0.0748
KS: 0.0862

20

40

60

80

100

120

10

15

20

25

Data Size [kB]

IFI [ms]

(a) Video frame size distribution comparison.

(b) IFI distribution comparison.

FIGURE 5: Comparison of the three best ﬁtting distributions for Virus Popper (30 Mbps, 60 FPS). The KS test is also shown,
where lower values indicate a better ﬁt.

third parameter of the Student’s t distribution is only able to
yield minuscule improvements over the Logistic distribution,
which only needs two parameters. Furthermore, if custom
simulators need to manually implement the desired random
stream, the Student’s t distribution is very hard to repro-
duce [36], while the Logistic distribution requires a simple
transformation. This is the case when common libraries for
random number generation cannot be used, such as in our
implementation described in Sec. V.

As a reference, we use SciPy’s deﬁnition of a logistic

distribution, with PDF in its standardized form as follows:

TABLE 1: Parameters of the proposed generative model.
Each VR application is characterized by ﬁve parameters: two
for the frame size dispersion DF S = αxβ, one for the 60 FPS
IFI dispersion DIF I = γ, two for the 30 FPS IFI dispersion
DIF I = δx(cid:15).

α

β

γ

δ

(cid:15)

Virus Popper
Minecraft
GE VR Tour
GE VR Cities

0.1784
0.1857
0.2554
0.2597

-0.2403
-0.1872
-0.2031
-0.2539

0.03721
0.07133
0.03468
0.03457

0.01433
0.02419
0.01056
0.008953

0.1764
0.2267
0.2756
0.3119

f (x) =

e−x
(1 + e−x)2 .

(2)

frame rates, and modeling and testing different values would
require new data for the corresponding frame rate.

To shift or scale the distribution, the location and scale
parameters are used as previously described.

After carefully studying the acquired trafﬁc traces, we
propose to generalize the scale parameters for both video
frame size and IFI time with a power law, namely:

B. GENERATIVE MODEL
Now that we characterized and ﬁtted the statistical distribu-
tions of the 40 acquired traces, we want to deﬁne a generative
model which would allow a user to synthesize XR trafﬁc at
will, be it for analysis or simulation purposes. As already
discussed in Sec. VI-A, in this paper we propose a simple
generative model, that only attempts to capture the statisti-
cal distributions of video frame size and Inter-Frame Inter-
arrivals (IFIs), leaving higher-order statistical descriptions
for future work.

We deﬁne the dispersion as the ratio of the scale over the
location parameter, attempting to ﬁnd a common value for
both frame rates, since absolute values are likely to differ by a
constant factor (see Figs. 2d and 2e). While data aggregation
is doable for frame sizes (as shown, for example, in Fig. 6a),
data for IFI did not allow us to do so. As shown in Fig. 6b,
in fact, data for 30 and 60 FPS behaves differently, making it
impossible for us to create a single model for this parameter.
This implies that our model will only be able to generalize
over data rates, whereas 30 and 60 FPS are the only supported

y = axb.

(3)

Furthermore, as Fig. 6b suggests, the 60 FPS IFI ﬁts for all
applications resulted in |b| < 10−4, suggesting a constant
behavior, irrespective of the data rate. In that case, we thus
assumed a constant ﬁt (a corner case of power law with b =
0) by computing the average value across all tested target data
rates.

As can clearly be observed from the collected data, the pro-
posed model has been extracted from acquisitions between
about 10 and 50 Mbps, thus using it beyond these limits is
not advisable since no data in our possession can validate the
quality of the synthetic traces.

We let different applications have separate models, obtain-
ing a data set of 10 traces per application (half at 30 FPS,
half at 60 FPS). The parameters for all applications can be
found in Table 1 and the generative algorithm is summarized
in Algorithm 1.

8

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

n
o
i
s
r
e
p
s
i
d
e
z
i
s

e
m
a
r
f

o
e
d
i
v

d
e
t
t
i
F

0.16

0.14

0.12

0.1

0.08

30 FPS

60 FPS

Fit

60 FPS

n
o
i
s
r
e
p
s
i
d

I
F
I
d
e
t
t
i
F

0.04

0.03

0.02

30 FPS

0

20

40

60

0

20

40

60

Measured rate [Mbps]

(a) Video frame size model.

Measured rate [Mbps]

(b) IFI model.

FIGURE 6: Generalization models for the Google Earth VR - Cities application. Individual points show the scale parameter of
the Logistic model ﬁtted on the acquired data, while the dashed red lines attempt to generalize the model to intermediate target
data rates.

Algorithm 1 Generative model for XR trafﬁc

Require: AppName, FrameRate, DataRate
1: FsAvg = DataRate / FrameRate
2: IﬁAvg = 1 / FrameRate
3: α, β, γ, δ, (cid:15) = GetParameters (AppName) {see Table 1}
4: FsDispersion = α · DataRateβ
5: FsScale = FsDispersion · FsAvg

IﬁDispersion = δ · DataRate(cid:15)

6: if FrameRate == 60 then
IﬁDispersion = γ
7:
8: else if FrameRate == 30 then
9:
10: else
11:
12: end if
13: IﬁScale = IﬁDispersion · IﬁAvg

Error: only 30 and 60 FPS supported

V. SIMULATION RESULTS
To further test
the validity of the proposed model, we
implemented it on top of Network Simulator 3 (ns-3), a
popular open-source full-stack simulation software, and has
been made publicly available together with the processed
VR trafﬁc traces in CSV format.3 Further details on the
implementation of this trafﬁc model on ns-3 can be found
in [1].

To test our model, we set up simulation campaigns where
multiple users equipped with HMDs communicate with a
central Wi-Fi Access Point (AP), using a wireless connec-
tion based on the IEEE 802.11ac standard. The central AP
also acts as rendering server, generating one VR stream for
each receiving Station (STA) of the scenario. Transmissions
randomly start within the ﬁrst second of simulation, avoiding
that different streams start at the same time.

We show results for trafﬁc streams imported directly from
the acquired traces as well as for our model. Since a sin-
gle trace is available for each parameter combination (i.e.,
application, frame rate, data rate), for a ﬁxed parameter

3Available in the ns-3 app store: https://apps.nsnam.org/app/bursty-app/

TABLE 2: List of simulation parameters

Parameter

Value

Parameter

Duration
Distance
Mobility
IP
Socket

60 s
1 m
Fixed
v4
UDP

RTS/CTS
MCS
Channel Width
Guard Interval Duration
Fragment Size

Value

Disabled
VHT MCS 9
160 MHz
400 ns
1472 B

combination, the trafﬁc ﬂows will all come from the same
trace, although different 60 s windows are sampled to further
decouple different users. Instead, simulations running our
proposed model have been repeated twice: one with the target
data rate submitted to VRidge when acquiring the corre-
sponding trace, and one with the empirical data rate measured
directly from the acquired traces (the two rates differ slightly,
as can be seen from Fig. 2a). This information can also be
found directly into the metadata of the acquired traces, made
available in CSV format together with our model.

A. MODEL VALIDATION
Exhaustive simulation campaigns have been run for all four
applications and ﬁve data rates at both 30 and 60 FPS,
each repeated 10 times to obtain solid average statistics.
Conﬁdence intervals are not shown as they are extremely
tight. Additional simulation parameters are shown in Table 2.
In the following section, plots will show burst-level rather
than fragment-level metrics, which in case of a video stream
are much more informative and bring a more realistic per-
spective on the quality perceived by the user. In fact, in this
case we are more interested in the performance regarding full
video frames rather than single packets, and thus all packets
from a burst will have to be collected before the HMD will
be able to process and show the frame to the user.

To validate our proposed model, we simulate a scenario as
similar as possible to our acquisition setup, where a rendering

VOLUME x, xxxx

9

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

Model (target rate)

Model (empirical rate)

Trace

30 FPS

60 FPS

]
s
p
b
M

[

t
u
p
h
g
u
o
r
h
t

.
g
v
A

60

40

20

0

]
s
m

[

y
a
l
e
d

e
m
a
r
f

.
g
v
A

3

2

1

0

]
s
m

[

y
a
l
e
d

e
m
a
r
f

.
c
r
e
p

h
t
5
9

4

3

2

1

0

10

20

30

40

50

10

20

30

40

50

10

20

30

40

50

Target data rate [Mbps]

Target data rate [Mbps]

Target data rate [Mbps]

(a) Average throughput.

(b) Average frame delay.

(c) 95th percentile of frame delay.

FIGURE 7: Simulation results for a single user streaming the Google Earth VR - Cities application over a Wi-Fi link. The
statistics refer to fully received frames rather than to single fragments.

server transmits the VR stream to a single user. Note that the
Wi-Fi connection is able to withstand hundreds of megabits-
per-second, thus a single user transmitting up to 50 Mbps
is largely underutilizing the channel, allowing us to obtain
unbiased results with respect to the limits of the channel
capacity. We simulated all 40 combinations of parameters (4
applications, 5 data rates, 2 frame rates), although we only
show results for the 10 related to the Google Earth VR - Cities
application in Fig. 7.

In Fig. 7a we show the average throughput obtained by
the 3 simulation campaigns in the 10 parameter sets. Clearly,
both 30 and 60 FPS runs obtain similar results, since this met-
ric disregards the frame rate. In fact, both models targeting
the nominal rate (shown in dots and circular markers) are
perfectly superimposed on the main diagonal. Simulations
using the original trafﬁc traces, instead, tend to have a slightly
higher throughput (solid line with cross markers), as was ex-
pected by looking at Fig. 2a. Since data rate, frame size, and,
conversely, latency are correlated, we matched our model’s
data rate with the empirical one, as shown by the dashed line
with square markers. As the ﬂexibility of our model allows us
to choose an arbitrary target rate, we can see a perfect match
in the computed average throughput.

In Fig. 7b, instead, we show the average frame delay
measured from the Application (APP) layer of the AP to
the APP layer of the STA. Processing, encoding/decoding
and other technical delays must be added to obtain the full
motion-to-photon latency, and thus the network delay should
remain below 5–10 ms, as mentioned in Sec. I. The most
noticeable difference with respect to the previous ﬁgure is
that the two frame rates are clearly separated. This is because
our reference application, described in Sec. III-B, allows
us to choose a target data rate, trying to maintain a CBR
transmission during the whole duration of the stream. This
translates into frame sizes which depend directly on the
frame rate, following the formula S = R/F as described
in Sec. III-B. Since the channel capacity for these simula-
tions is kept constant, doubling the frame rate halves the
frame sizes which, in turn, halves the average video frame

delay. As expected, the model using the target rate slightly
underestimates the average frame delay, which depends on
the real application throughput, always slightly lower than
the one empirically computed from the trafﬁc traces. Instead,
similarly to the average throughput, setting the model to the
trace’s empirical rate yields an almost perfect match with
the VR traces we acquired. Finally, notice that the average
frame delay always remains below 3 ms, far below the bound
suggested by the industry experts [2], [4], [9], [14], [19].

To complete this analysis, in Fig. 7c we report the 95th
percentile delay performance of our simulations. This metric
is important as it gives an idea of the worst-case performance
of the network. In fact, only ensuring average performance
is not enough to obtain a smooth and appreciable user
experience, since frequent stutters in the streamed video
might easily ruin the interactivity of the application and even
disorient the user. Ensuring that the 95th percentile of the
delay is within acceptable bounds allows for a more ﬂuid and
overall better experience. In the case under analysis, it can be
easily seen that both models using target and empirical rate
slightly underestimate the frame delay of the acquired traces.
It is likely that the ﬁtted Logistic distribution is not able to
fully grasp the minute details of the trafﬁc trace, making our
model unable to match the real trafﬁc.

Note that, while these results are bound to the speciﬁ-
cations of the network under analysis (e.g., MCS, channel
width, guard interval duration, fragment size, presence of
RTS/CTS, Wi-Fi standard, mobility, environment) the frame-
work that we proposed is general. This suggests that it can
be used to study a variety of more or less complex scenarios
and network architectures with different sets of parameters,
assessing how they affect the end-to-end performance.

To conclude, it appears that our model is indeed able
to reliably predict average statistics, while it could still be
improved to better mimic slightly more advanced and speciﬁc
features. These reﬁnements will be pursued in our future
work.

10

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

Model (target rate)

Model (empirical rate)

Trace

30 FPS

60 FPS

]
s
p
b
M

[

t
u
p
h
g
u
o
r
h
t

.
g
v
A

400

300

200

100

0

]
s
m

[

y
a
l
e
d

e
m
a
r
f

.
g
v
A

20

15

10

5

0

]
s
m

[

y
a
l
e
d

e
m
a
r
f

.
c
r
e
p

h
t
5
9

20

15

10

5

0

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

8

1

2

3

4

5

6

7

8

# Users

# Users

# Users

(a) Average throughput.

(b) Average frame delay.

(c) 95th percentile of frame delay.

FIGURE 8: Simulation results for multiple users streaming the Google Earth VR - Cities application over a Wi-Fi link. The
statistics refer to fully received frames rather than to single fragments.

B. USE CASE EXAMPLE
Finally, we propose a simple example use case for our VR
trafﬁc generator. We consider a VR arena setting, where
multiple users are attached directly to a single AP streaming
wirelessly. We assume that each user requests a 50 Mbps
stream and observe how many STAs can be supported by an
arena with an analogous setup.

As expected, we notice again from Fig. 8 that our model
needs to be calibrated against the empirical rate of the ac-
quired trace to yield reliable results. In fact, from Fig. 8a we
can see that the average throughput of the calibrated model
perfectly matches the throughput of the trafﬁc trace up to at
least 8 users, where the network is able to support more than
400 Mbps.

In Fig. 8b it is possible to see an unstable network con-
dition, when 8 users are trying to stream simultaneously. It
appears that the slightly higher throughput required by the
trace and the empirical rate model with respect to the target
rate model is enough to push the network to its limit, resulting
in a sudden increase of the average frame delay, at both
30 and 60 FPS. Focusing on the 30 FPS simulations, the plot
shows that up to 6 users can be supported within the 5 ms
bound, while 7 users slightly exceed this limit, and ﬁnally
8 users make the network unstable and are thus pushed over
the 10 ms limit for both the trace and the model using the
empirical rate. It is important to notice that the more unstable
the network, the worse the prediction accuracy of our model.
This is probably due to the simpliﬁcations that we introduced,
such as the Logistic distribution and the uncorrelated samples
for both the IFI time and frame size stochastic processes.
Similarly, at 30 FPS, up to 7 users can be supported, but an
additional user makes the system highly unstable and with
poor prediction performance from our model.

Finally, in Fig. 8c we show the results for the 95th per-
centile of the delay. Similarly to the average delay, this metric
also shows the instability of the network for 8 users with
much worse performance. Focusing on 30 FPS, the system
is able to keep the delay below the 5 ms bound only when no
more than 2 users are present, whereas up to 5-6 users can

be served if a 10 ms is still deemed acceptable. Instead, at
60 FPS up to 6 users can be served while keeping the network
delay within 5 ms, while the 10 ms limit is only surpassed
when the network becomes unstable with 8 users.

These counterintuitive conclusions come from the fact that
the application ﬁxes a data rate, not a quality of experience.
This means that doubling the frame rate results in halving
the frame size, thus reducing the perceived image quality of
the streamed application, which turns into an almost halved
delay. Fixing a constant bit rate thus results in higher frame
rates yielding lower latencies, at the cost of a lower image
quality.

In general, there is good accordance between the results
predicted by the calibrated model and the trafﬁc traces, while
the uncalibrated model often shows overly optimistic results.
When the trafﬁc in the network increases too much and the
network becomes unstable, the three simulations diverge sig-
niﬁcantly, making our synthetic traces less reliable, although
this is a corner case that might be of lesser interest.

VI. XR TRAFFIC MODELING ROADMAP
Starting from the model described in the previous sections,
in the following we propose an end-to-end framework to
evaluate network solutions, tailored for XR applications. The
goal is to list and detail the tasks required for the construction
of such a framework, in order to encourage researchers in this
ﬁeld to advance with their work the state of the art, using our
baseline as a valid starting point.

While Sec. VI-A is devoted to highlighting our contribu-
tions, in Secs. VI-B, VI-C and VI-E we set down each addi-
tional task, describing how they can lead to the optimization
of network protocols.

A. EXPLOITING FIRST-ORDER STATISTICS
The model proposed in this paper, despite its basic function-
alities, represents a solid foundation on top of which future
works can iterate to develop more sophisticated strategies. In
particular, we designed an open-source, highly customizable
setup (described in Sec. III) to acquire trafﬁc traces by

VOLUME x, xxxx

11

snifﬁng the packets traveling on the local network where the
experiments were conducted.

At this stage, packets are generated following ﬁrst-order
statistics, sampling the size and inter-frame interval from the
distribution ﬁtted on the collected data (see Sec. III-B). As
a consequence, with this model we can emulate the creation
of application frames that replicate the strategy implemented
by the rendering server used in our experiments. While this
model is already useful for some applications, it lends itself to
several interesting extensions, which capture other important
features of the statistics of XR trafﬁc. As an example, in the
rest of this section we discuss the importance of studying the
correlation between different packets and of understanding
how the movements of the user impact the generated trafﬁc
as two key areas of future improvement for our model.

B. INTRODUCING TEMPORAL CORRELATION

More advanced studies can be carried out to improve the
model with additional features. One important aspect to
elaborate on is the correlation among subsequent frames, or
even among a speciﬁc group of frames.

As mentioned in Sec. III-B, when compressing a video
stream both intra-frame and inter-frame compression tech-
niques could be exploited, and this inﬂuences not only the
structure of the packets since the type of compression greatly
inﬂuences the frame size, but also the strategy to inject them
into the network. It is also possible that some manufactur-
ers use advanced coding techniques such as Periodic Intra
Refresh, as was explained in Sec. III-B for the streaming
application used for our analysis, or more advanced standards
such as H.265 [37] using different compression techniques.
In that case, the importance of temporal correlation might
decrease, although further analysis should be carried out to
ensure this.

It should be clear, by now, that the availability of a model
capable of generalizing how such frame sequences are cre-
ated, independently from the technical setup, is important,
and the fact that each manufacturer may use its own policy
represents an additional challenge. In addition, having a
model that integrates and generalizes the temporal correlation
among frames would allow researchers to elaborate strategies
to guarantee a certain level of latency and throughput, for
example by giving different priorities and scheduling options
to different types of packets.

For applications with constant delay requirements and high
values of FPS, a solution could be to buffer (at the device side
or at the rendering server) speciﬁc packets associated with
keyframes, in order to improve the encoding process. This
would require stable network performance and an application
capable of communicating directly with the network, e.g.,
exploiting cross-layer solutions, to be aware of any change of
the link quality that would trigger speciﬁc countermeasures
or improvements, if applicable.

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

C. INTRODUCING HEAD TRACKING
A further improvement of the model should exploit the in-
formation on movement tracking, in particular related to the
head, for all 6DoF. In this case, snifﬁng the packets traveling
through the network might not be enough, and we thus need
to gather information from different sensors (e.g., gyroscope,
accelerometer and compass), that could be integrated into the
device used to interact with the virtual world.

With respect to VRidge, the software that we used to make
our phone acting as a VR headset and our PC as a rendering
server, the developers provide an API for this purpose.4
By connecting to the head tracking endpoint, the software
provides positional, rotational, or combined data, and even
the possibility of modifying phone tracking data in real time
before it is used for the rendering step.

This is important because, by aligning the motion trace
with the trafﬁc generated by the application, it can be de-
termined whether there is a correlation between a certain
movement of the user and the corresponding drop in the
reception of packets, or other network-related events. For
example, knowing the direction of the physical movement
of the user might help mmWave wireless systems (such
as 802.11ad/ay) keep beam alignment between the AP and
the user device, thus limiting the risk of abrupt connection
interruptions if the line of sight is lost.

It has to be highlighted that this approach could beneﬁt
every communication infrastructures that can be used to
deliver XR content, as user tracking data can be exploited
at different layers of the protocol stack.

D. FULL TRAFFIC EMULATOR
The last step to further increase the ﬁdelity (but also the
complexity) of the trafﬁc model is to fully characterize and
emulate all the different information sub-ﬂows and how they
interact with each other. For example, as shown in Fig. 1
and explained in Sec. III-B, the VR stream comprises both
DL and UL messages containing information such as video
frames, head tracking information, and feedback.

A full-blown emulator would send all this information to
and from the user, reacting accordingly whenever a packet
is lost or corrupted, or when communication delays are
present. This level of detail requires a much more in-depth
analysis of the transmission protocol of a real XR application,
understanding all the consequences of erratic and unexpected
behaviors of the network.

Such a precise model would be extremely useful when
running large simulation campaigns as it would give the most
accurate and reliable results. However, the amount of work
required to analyze and reproduce a realistic behavior would
be extremely high.

E. QOE-CENTRIC XR
As highlighted thoroughly in the previous paragraphs, the
ﬁnal goal of all these approaches is to guarantee high-level

4https://github.com/RiftCat/vridge-api

12

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

performance to the ﬁnal user. In particular, in the XR domain,
we tend to measure the performance in terms of overall
satisfaction of the customers, referred to as QoE, and, to
the best of our knowledge, there is no standardized way to
evaluate these metrics.

In our case, besides the quality of the shown image, also
the latency of the communication between the HMD and the
rendering server can make a difference (especially if the latter
is in the cloud), considering that cybersickness has a huge
impact on the user experience. For this reason, researchers
should be encouraged to design algorithms that guarantee
stable and constant performance, taking into account that the
trafﬁc in the network varies depending on the application and
user activity.

Moreover, since in a common scenario we have differ-
ent users, there may be a need to support different trafﬁc
categories at the same time in the same network. This re-
quires a system able to fairly distribute resources among the
ﬂows, where learning algorithms could be implemented to
orchestrate every operation, either from a network or from an
application perspective.

Given a certain condition of the user, or other available
information, the algorithm could predict the QoE trend and
act accordingly in case of an anticipated performance drop.
At this point on the roadmap, the network design should
focus on the user, trying to guarantee a stable experience
also when Variable Bit Rate (VBR) ﬂows are considered. In
fact, in a CBR ﬂow (much easier to handle from a network
point of view), the perceived image quality can be affected in
case of a scene with a large amount of action and details. In
this case, it may be difﬁcult to ﬁt everything at a ﬁxed rate
and, as a consequence, the user experiences a downgrade in
terms of quality. This further highlights the need for novel
solutions, able to tackle these problems by trading off system
complexity and QoE.

VII. CONCLUSIONS
In this paper we described the current state of the art regard-
ing the telecommunication aspects needed to support high-
quality XR streaming, mainly focusing on the challenges
needed to obtain faithful trafﬁc models that the community
could use to test protocols and optimize networks.

We then proceeded to acquire over 4 hours of VR trafﬁc,
study in detail this type of trafﬁc, and propose a model to
generate synthetic trafﬁc traces, while also making freely
available to the community both our implementation and the
VR dataset.

Finally, we show some results on the predictive power
of our model, while also acknowledging its weak points.
Furthermore, we provided an example use case where mul-
tiple users coexist in the same network, naively sharing radio
resources up to its collapse. Further work could better study
effective scheduling strategies for XR trafﬁc streams, possi-
bly coexisting with other applications in the same network
while also ensuring robustness in case of ﬂuctuating channel
quality. Also, the model could be tested and validated for

higher values of FPS, by collecting and analyzing additional
traces at 90 FPS or higher.

With this work, we hope to pave the way for the research
community to start working towards the optimization and
support of this speciﬁc type of trafﬁc, given the extreme in-
terest from the main standard bodies and the most prominent
telecommunication industries.

ACKNOWLEDGMENT
We want to thank the RiftCat team for patiently answering all
the disclosable technical questions we asked, allowing us to
improve our work.

REFERENCES
[1] M. Lecci, A. Zanella, and M. Zorzi, “An ns-3 Implementation of a Bursty
Trafﬁc Framework for Virtual Reality Sources,” in ACM Workshop on ns-3
(WNS3), Jun. 2021.

[2] Huawei Technologies Co., “Empowering consumer-focused immersive
VR and AR experiences with mobile broadband,” White Paper,
2016. [Online]. Available: https://www.huawei.com/en/industry-insights/
outlook/mobile-broadband/insights-reports/vr-and-ar

[3] Oculus Business, “Virtual Reality – Set
Mainstream,” White paper, Sep. 2020.
//go.facebookinc.com/security-whitepaper.html

to Enter

the Business
[Online]. Available: https:

[4] Huawei

Technologies

Appli-
Avail-
https://carrier.huawei.com/~/media/CNBGV2/download/bws2021/

cation
able:
ar-insight-and-application-practice-white-paper-en.pdf

Practice,” White

and
[Online].

Insight

Paper,

2021.

“AR

Co.,

[5] PriceWaterhouseCoopers, “Seeing is believing – How virtual reality and
augmented reality are transforming business and the economy,” Report,
https://www.pwc.com/gx/en/technology/
2019.
publications/assets/how-virtual-reality-and-augmented-reality.pdf

[Online]. Available:

[6] ZTE, “5G Cloud XR Application,” White paper, 2019.

[Online].
https://www.mobile360series.com/wp-content/uploads/2019/

Available:
09/zte-white-paper.pdf

[7] Qualcomm Technologies, “The Mobile Future of eXtended Reality (XR),”
Presentation, Nov. 2020. [Online]. Available: https://www.qualcomm.
com/media/documents/ﬁles/the-mobile-future-of-extended-reality-xr.pdf
[8] Ericsson, “How 5G and Edge Computing can enhance virtual reality,”
Blog Post, Apr. 2020. [Online]. Available: https://www.ericsson.com/en/
blog/2020/4/how-5g-and-edge-computing-can-enhance-virtual-reality
[9] 5G Americas, “5G Services Innovation,” White paper, Nov. 2019.
https://www.5gamericas.org/wp-content/uploads/

[Online]. Available:
2019/11/5G-Services-Innovation-FINAL-1.pdf

[10] Deloitte, “Real

learning in a virtual world,” Blog Post, Aug. 2018.
[Online]. Available: https://www2.deloitte.com/us/en/insights/industry/
technology/how-vr-training-learning-can-improve-outcomes.html
[11] F. Chiariotti, “A survey on 360-degree video: Coding, quality of experience

and streaming,” submitted to Elsevier Computer Communications, 2021.

[12] Nokia, “Cloud gaming and 5G – Realizing the opportunity,” White Paper,
2020. [Online]. Available: https://onestore.nokia.com/asset/207843
[13] Huawei, “Preparing for a Cloud AR/VR Future,” White Paper, 2017.
[Online]. Available: https://www-ﬁle.huawei.com/-/media/corporate/pdf/
x-lab/cloud_vr_ar_white_paper_en.pdf?la=en

[14] 3GPP, “Extended Reality (XR) in 5G,” Technical Report (TR) 26.928, Dec.

2020.

[15] L. J. Hettinger and G. E. Riccio, “Visually induced motion sickness in
virtual environments,” Presence: Teleoperators and Virtual Environments,
vol. 1, no. 3, pp. 306–310, Jan. 1992.

[16] E. L. Groen and J. E. Bos, “Simulator sickness depends on frequency of
the simulator motion mismatch: An observation,” Presence: Teleoperators
and Virtual Environments, vol. 17, no. 6, pp. 584–593, Dec. 2008.
[17] Sebastian von Mammen, Andreas Knote, and Sarah Edenhofer, “Cyber
sick but still having fun,” in ACM Conference on Virtual Reality Software
and Technology (VRST), Munich, Germany, Nov. 2016.

[18] H. G. Kim, W. J. Baddar, H.-t. Lim, H. Jeong, and Y. M. Ro, “Measurement
of exceptional motion in VR video contents for VR sickness assessment
using deep convolutional autoencoder,” in ACM Symposium on Virtual
Reality Software and Technology (VRST), Gothenburg, Sweden, Nov.
2017.

VOLUME x, xxxx

13

[19] ITU, “Requirements for mobile edge computing enabled content delivery
networks,” Report, Nov. 2019. [Online]. Available: ITU-TF.743.10
[20] Orange, “XR: 5G extends the boundaries of reality,” Blog Post, Jul. 2020.
[Online]. Available: https://hellofuture.orange.com/en/xr-5g-extends-the-
boundaries-of-reality/

[21] Samsung Research, “Samsung 6G Vision,” White Paper, Jul. 2020.
https://news.samsung.com/global/samsungs-6g-

[Online]. Available:
white-paper-lays-out-the-companys-vision-for-the-next-generation-of-
communications-technology

[22] J. Latta and D. Oberg, “A conceptual virtual reality model,” IEEE Com-
puter Graphics and Applications, vol. 14, no. 1, pp. 23–29, Jan. 1994.
[23] B. Hentschel, M. Wolter, and T. Kuhlen, “Virtual reality-based multi-view
visualization of time-dependent simulation data,” in IEEE Virtual Reality
Conference, Mar. 2009, pp. 253–254.

[24] E. Saad, W. R. J. Funnell, P. G. Kry, and N. M. Ventura, “A virtual-reality
system for interacting with three-dimensional models using a haptic device
and a head-mounted display,” in IEEE Life Sciences Conference (LSC),
Oct. 2018, pp. 191–194.

[25] O. Ergün, ¸S. Akin, ˙I. G. Dino, and E. Surer, “Architectural design in virtual
reality and mixed reality environments: A comparative analysis,” in IEEE
Conference on Virtual Reality and 3D User Interfaces (VR), Mar. 2019,
pp. 914–915.

[26] R. Guo, J. Cui, W. Zhao, S. Li, and A. Hao, “Hand-by-Hand Mentor: An
AR based Training System for Piano Performance,” in IEEE Conference
on Virtual Reality and 3D User Interfaces Abstracts and Workshops
(VRW), Mar. 2021, pp. 436–437.

[27] J. Yang, J. Luo, D. Meng, and J.-N. Hwang, “QoE-Driven Resource
Allocation Optimized for Uplink Delivery of Delay-Sensitive VR Video
Over Cellular Network,” IEEE Access, vol. 7, pp. 60 672–60 683, May
2019.

[28] L. Teng, G. Zhai, Y. Wu, X. Min, W. Zhang, Z. Ding, and C. Xiao, “QoE
Driven VR 360◦ Video Massive MIMO Transmission,” IEEE Transactions
on Wireless Communications, Jul. 2021, early Access.

[29] C. Perfecto, M. S. Elbamby, J. D. Ser, and M. Bennis, “Taming the Latency
in Multi-User VR 360◦: A QoE-Aware Deep Learning-Aided Multicast
Framework,” IEEE Transactions on Communications, vol. 68, no. 4, pp.
2491–2508, Apr. 2020.

[30] B. Krogfoss, J. Duran, P. Perez, and J. Bouwen, “Quantifying the Value of
5G and Edge Cloud on QoE for AR/VR,” in 12th International Conference
on Quality of Multimedia Experience (QoMEX), Athlone, Ireland, May
2020.

[31] X. Liu, Y. Huang, L. Song, R. Xie, and X. Yang, “The SJTU UHD
360◦ Immersive Video Sequence Dataset,” in International Conference on
Virtual Reality and Visualization (ICVRV), May 2017, pp. 400–401.
[32] W.-C. Lo, C.-L. Fan, J. Lee, C.-Y. Huang, K.-T. Chen, and C.-H.
Hsu, 360◦ Video Viewing Dataset in Head-Mounted Virtual Reality.
New York, NY, USA: Association for Computing Machinery, 2017, pp.
211–216. [Online]. Available: https://doi.org/10.1145/3083187.3083219

[33] ITU-T Telecommunication Standardization Sector of ITU, “H.264 : Ad-
vanced video coding for generic audiovisual services,” Tech. Rep., Jun.
2019.

[34] I. M. Chakravarti, R. G. Laha, and J. Roy, Handbook of Methods of Applied

Statistics.

John Wiley and Sons, 1967, vol. 1.

[35] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,
D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J.
van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J.
Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y. Feng, E. W.
Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen,
E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa,
P. van Mulbregt, and SciPy 1.0 Contributors, “SciPy 1.0: Fundamental
Algorithms for Scientiﬁc Computing in Python,” Nature Methods, vol. 17,
pp. 261–272, Mar. 2020.

[36] W. Shaw, “Sampling Student’s T distribution – Use of the inverse cumula-
tive distribution function,” Journal of Computational Finance, vol. 9, no. 4,
pp. 37–73, Jan. 2006.

[37] ITU-T Telecommunication Standardization Sector of ITU, “H.265 : High

efﬁciency video coding,” Tech. Rep., Nov. 2019.

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

MATTIA LECCI
(Graduate Student Member,
IEEE) received the B.Sc. degree (Hons.) in infor-
mation engineering and the M.Sc. degree (Hons.)
in telecommunication engineering from the Uni-
versity of Padova, Italy, in 2016 and 2018, respec-
tively, where he is currently pursuing the Ph.D.
degree in information engineering.

He was a Guest Researcher with the National
Institute for Standards and Technology (NIST)
in 2018. His main research activities are channel
modeling for the mmWave frequency band, MAC scheduling for WiGig
technologies, applied machine learning for communications, virtual reality
trafﬁc modeling, and open-source software development.

MATTEO DRAGO (Graduate Student Member,
IEEE) received his B.Sc. (2016) and M.Sc. (2019)
in Telecommunication Engineering from the Uni-
versity of Padova, Italy. Since October 2019, he
has been a Ph.D. Student at the University of
Padova. He visited Nokia Bell Labs, Dublin, in
2018, working on QoS provisioning in 60 GHz
networks. His research interests are in the study
of the next generation of vehicular networks oper-
ating at millimeter-wave.

ANDREA ZANELLA (Senior Member, IEEE) re-
ceived the Laurea degree in computer engineering
from the University of Padova, Italy, in 1998,
and the Ph.D. degree in 2001. In 2000, he spent
nine months with Prof. Mario Gerla’s research
team at the University of California, Los Angeles
(UCLA). He is currently a Full Professor with
the Department of Information Engineering (DEI),
University of Padova. He is one of the coordina-
tors of the SIGnals and NETworking (SIGNET)
research lab. His long-established research activities are in the ﬁelds of
protocol design, optimization, and performance evaluation of wired and
wireless networks. He has been serving as a Technical Area Editor for
the IEEE INTERNET OF THINGS JOURNAL and an Associate Editor
for the IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS
AND NETWORKING, the IEEE COMMUNICATIONS SURVEYS AND
TUTORIALS, and Digital Communications and Networks

14

VOLUME x, xxxx

M. Lecci et al.: An Open Framework for Analyzing and Modeling XR Network Trafﬁc

MICHELE ZORZI (Fellow, IEEE) received the
Laurea and Ph.D. degrees in electrical engineering
from the University of Padova, Italy, in 1990 and
1994, respectively.

From 1992 to 1993, he was on leave at the
University of California at San Diego (UCSD). In
1993, he joined the Dipartimento di Elettronica e
Informazione, Politecnico di Milano, Italy. After
spending three years with the Center for Wireless
Communications, UCSD, in 1998 he joined the
School of Engineering, University of Ferrara, Italy, where he became a
Professor in 2000. Since November 2003, he has been a Faculty Member
with the Department of Information Engineering, University of Padova.
His current research interests include performance evaluation in mobile
communications systems, the Internet of Things, cognitive communications
and networking, 5G mmWave cellular systems, vehicular networks, and
underwater communications and networks.

Dr. Zorzi received several awards from the IEEE Communications Soci-
ety, including the Best Tutorial Paper Award in 2008 and 2019, the Education
Award in 2016, the Stephen O. Rice Best Paper Award in 2018, and the
Joseph LoCicero Award for Exemplary Service to Publications in 2020. He
was the Editor-in-Chief of the IEEE Wireless Communications Magazine
from 2003 to 2005, the IEEE TRANSACTIONS ON COMMUNICATIONS
from 2008 to 2011, and the IEEE TRANSACTIONS ON COGNITIVE
COMMUNICATIONS AND NETWORKING from 2014 to 2018. He has
served the IEEE Communications Society as a Member-at-Large of the
Board of Governors from 2009 to 2011 and from 2021 to 2023, as the
Director of Education from 2014 to 2015, and as the Director of Journals
from 2020 to 2021.

VOLUME x, xxxx

15

