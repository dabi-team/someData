FoV Privacy-aware VR Streaming

Xing Wei and Chenyang Yang
School of Electronics and Information Engineering, Beihang University, Beijing 100191, China
Email: {weixing, cyyang}@buaa.edu.cn

1
2
0
2

t
c
O
0
2

]

M
M

.
s
c
[

1
v
7
1
4
0
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Proactive tile-based virtual reality (VR) video
streaming can use the trace of FoV and eye movement to predict
future requested tiles, then renders and delivers the predicted
tiles before playback. The quality of experience (QoE) depends
on the combined effect of tile prediction and consumed resources.
Recently, it has been found that with the FoV and eye movement
data collected for a user, one can infer the identity and preference
of the user. Existing works investigate the privacy protection for
eye movement, but never address how to protect the privacy
in terms of FoV and how the privacy protection affects the
QoE. In this paper, we strive to characterize and satisfy the
FoV privacy requirement. We consider “trading resources for
privacy”. We ﬁrst add camouﬂaged tile requests around the
real FoV and deﬁne spatial degree of privacy (SDoP) as a
normalized number of camouﬂaged tile requests. By consuming
more resources to ensure SDoP, the real FoVs can be hidden.
Then, we proceed to analyze the impacts of SDoP on the QoE by
jointly optimizing the durations for prediction, computing, and
transmission that maximizes the QoE given arbitrary predictor,
conﬁgured resources, and SDoP. We ﬁnd that a larger SDoP
requires more resources but degrades the performance of tile
prediction. Simulation with state-of-the-art predictors on a real
dataset veriﬁes the analysis and shows that a user requiring a
larger SDoP can be served with better QoE.

Index Terms—privacy-aware VR, proactive VR, FoV privacy

protection, spatial degree of privacy, VR federated learning

I. INTRODUCTION
As the main type of wireless virtual reality (VR) services,
360◦ video streaming consumes large amount of computing
and communication resources to avoid playback stalls and
black holes that degrade the quality of experience (QoE).
However, humans can only see a small portion of the full
panoramic view (e.g., 110◦ × 110◦) at arbitrary time, which is
called the ﬁeld of view (FoV). To improve QoE with limited
resources, proactive VR video streaming is proposed [1],
which divides a full panoramic view segment into small tiles
in spatial domain. Before playback of the segment, the tiles
overlapped with FoV are ﬁrst predicted using the observed
trace of FoV and eye movement, then these tiles are rendered
and delivered to the user. The QoE depends on how many
correctly predicted tiles can be rendered and transmitted, i.e.,
the combined effect of tile prediction and consumed resources.
Harnessing the trace of FoV and eye movement is the key for
prediction.

While proactive VR video streaming is being intensively
investigated, most of the existing works neglect the willingness
of users. Are users willing to share their traces of FoV and
eye movement when watching 360◦ videos?

Recent works show that both types of data can be used to
dig personal information. With less than ﬁve minutes’ FoV
trace, the identity of 95% users among all the 511 users
are correctly identiﬁed by a random forest algorithm in [2].
With the eye movement trace, a statistical approach proposed
in [3] can achieve an equal error rate of 2.04% to identify

users. Furthermore, the FoV and eye movement data of a
user collected for proactive VR video streaming can reveal
the intent and preference of the user [4]. Undoubtedly, not all
users are willing to share these data.

Existing works have only considered to protect the privacy
of eye movement either by adding noise [4], [5] or down-
sampling the real gaze position [4]. However, the trace of
FoV plays a more important role for tile requests predic-
tion. For example, the state-of-the-art prediction accuracy is
achieved on a real dataset [6] only with the trace of FoV
[7]. When considering FoV privacy protection in proactive VR
video streaming, several fundamental questions arise: How to
characterize and satisfy the FoV privacy requirement? What
is the impact of privacy requirements on proactive tile-based
VR video streaming?

In this paper, we strive to answer these questions. Our

contributions are summarized as follows.

• We consider “trading resources for privacy”. Speciﬁcally,
to blur the real FoV, camouﬂaged tile requests are added
around the FoV. Then, the overall tile requests become
a mixture of the tile requests overlapped with the FoV
of a user and the camouﬂaged tile requests. We deﬁne
spatial degree of privacy (SDoP) as a normalized num-
ber of camouﬂaged tile requests to reﬂect the privacy
requirement. To satisfy a user with a larger SDoP re-
quirement, more extra tiles are rendered and transmitted
beyond those really requested, which makes inferring the
real FoV more difﬁcult at the cost of consuming more
resources.

• To show the impact of privacy requirement, we optimize
the durations for prediction, communication, and com-
puting under arbitrarily given predictor, communication
and computing resources to maximize the QoE. From
the optimal solution, we ﬁnd that longer total duration for
communication and computing is required for a user with
large value of SDoP, which degrades the performance for
predicting tile requests. Simulation with the state-of-the-
art predictors on a real dataset shows that the increase
of consumed resources for a user with a higher SDoP
is larger than the degradation of prediction performance,
hence the QoE can be improved.

II. SYSTEM MODEL
Consider a tile-based VR video streaming system with a
multi-access edge computing (MEC) server co-located with
a base station (BS) that serves K users. The MEC server
accesses a VR video library by local caching or high-speed
backhaul, thus the delay from Internet to the MEC server can
be neglected. The server also equips with powerful computing
units for rendering, training a tile predictor and making the

 
 
 
 
 
 
prediction. Each user requests VR videos from the library
according to their own interests. Hence, the requests at the
MEC server can be regarded as multiple individual requests.
In the sequel, we consider arbitrary one request for a video
from a user.

Each VR video consists of L segments in temporal domain,
and each segment consists of M tiles in spatial domain. The
playback duration of each tile equals to the playback duration
of a segment, denoted by Tseg [1], [8]. During the playback
of a segment, the user turns around freely to view FoVs. Tiles
overlapped with FoVs are the requested tiles.

Each user equips with an head mounted display (HMD),
which can measure the trace of viewpoint (centre point of the
FoV) and log the tile requests, upload the viewpoint trace or
the tile requests to the MEC server, and pre-buffer segments.
The HMD can also equipped with a light-weighted computing
unit for training a predictor and making the prediction.

A. Proactive VR Video Streaming Procedure

Proactive VR video streaming requires to predict the tile
requests, either in direct or indirect manner. For direct predic-
tion, the future tile requests are predicted from the observed
tile requests [9]. For indirect prediction, the future viewpoints
are ﬁrst predicted from the observed viewpoint trace [1] and
then mapped to the predicted tile requests [7], [10].

If a predictor needs to be trained in advance, the whole
proactive procedure contains two stages: (1) ofﬂine predictor
training, (2) online tile predicting, rendering and delivering.
For some simple predictors with no need for training as those
used in [1], [7], stage (1) can be omitted.

Predictor training can be operated at the MEC server or
the HMD. When training at the MEC server (i.e., centralized
training), the real viewpoint traces or tile requests of multiple
users should be uploaded. When training at each HMD (i.e.,
federated training), the real viewpoint traces or tile requests
can be stored at each HMD and only the model parameters of
predictor are uploaded to the MEC server.

The procedure in the online streaming stage is shown in
Fig. 1. When a user requests a VR video, the MEC server ﬁrst
renders and transmits the initial (l0−1)th segments in a passive
streaming mode [11]. After an initial delay, the ﬁrst segment
begins to play at the time instant tb, which is the start time
of an observation window. Then, proactive streaming begins,
where subsequent segments are predicted with the well-trained
predictor, and the predicted tiles are computed and transmitted.
In the sequel, we take online proactive streaming for the l0th
segment as an example for elaboration.

After the viewpoint trace or the tile requests in the obser-
vation window with duration tobw is collected, the tiles to be
played in a prediction window with duration Tpdw = Tseg can
be predicted at the MEC server or a HMD. If predicting view-
points at the HMD, then the HMD can map the viewpoints to
the tile requests and upload the tile requests, or it can leave
the mapping to the MEC server and upload the viewpoints.

Based on the predicted tiles, the MEC server determines
the tiles to be streamed, renders the tiles with duration tcpt,
and transmits the tiles with duration tcom. To avoid stalling,
the tiles in the l0th segment should be rendered and deliv-
ered before the start time of playback of the l0th segment,

i.e., the time instant te. The duration starting from tb and
terminating at te is the online proactive streaming time for
a segment, denoted as Tps. We can observe from the ﬁgure
that Tps = (l0 − 1)Tseg, where in the example l0 = 3 and
hence Tps = 2Tseg. The durations for observation, computing,
and transmitting should satisfy tobw + tcpt + tcom = Tps. The
total duration for communication and computing is denoted as
tcc (cid:44) tcom + tcpt.

Fig. 1: Streaming the ﬁrst four segments of a VR video,
indirect prediction. tb is the start time of the observation
window, te is the start time of playback of the l0th segment.

B. Computing and Transmission Model

The number of bits that can be rendered per second,
referred to as the computing rate, is Ccpt,k (cid:44) Fcpt
(in bit/s),
K·µr
where Fcpt is the conﬁgured resource at the MEC server for
rendering (in ﬂoating-point operations per second, FLOPS), µr
is the required ﬂoating-point operations (FLOPs) for rendering
one bit of FoV (in FLOPs/bit) [10].

(cid:17)

k|2

k)H wi

k |˜hi
kd−α
σ2

The BS serves K single-antenna users using zero-forcing
beamforming with Nt antennas. The instantaneous data
the ith time slot for the kth user is C i
com,k =
rate at
(cid:16)
1 + pi
(cid:44)
k is the equivalent channel gain, pi

where B is the bandwidth, ˜hi
B log2
k
k and wi
(hi
k are
respectively the transmit power and beamforming vector for
k ∈ CNt are respectively the distance
the kth user, dk and hi
and the small scale channel vector from the BS to the kth
user, α is the path-loss exponent, σ2 is the noise power, and
(·)H denotes conjugate transpose.

We consider indoor users as in the literature, where the
distances of users, dk, usually change slightly [6], [8] and
hence are assumed ﬁxed. Due to the head movement and
the variation of the environment, small-scale channels are
time-varying, which are assumed as remaining constant in
each time slot with duration ∆T and changing indepen-
dently with identical distribution among time slots. With
the proactive transmission, the rendered tiles in a segment
should be transmitted with duration tcom. The number of
bits transmitted with tcom can be expressed as C com,k · tcom,
where C com,k (cid:44) 1
com,k · ∆T is the time average
Ns
transmission rate, and Ns is the number of time slots in tcom.
Since future channels are unknown at the time instant tb, we
use ensemble-average rate Eh{Ccom,k} [12] to approximate
the time-average rate C com,k, where Eh{·} is the expectation

i=1 C i

(cid:80)Ns

Viewpoint tracePlayback PassiveProactiveInitial delayt1RenderingTransmitting 2343344Prediction and tile selectionTile selectioncpttcomtpdwsegTTobwtccTpsTbtetover h, which is accurate when Ns or Nt/K is large [10].
By allocating transmit power among users to compensate the
path loss, the ensemble-average transmission rate for each user
is equal. For notional simplicity, we use Ccom and Ccpt to
replace Eh{Ccom,k} and Ccpt,k.

We use the ratio of tiles in a segment that can be rendered
and transmitted with assigned transmission and computing
rates and corresponding durations to measure the commu-
nication and computing (CC) capability of the system for
streaming the tiles, which is,

Ccc(tcom, tcpt) (cid:44) min

(cid:26) Ccomtcom
scom

,

Ccpttcpt
scpt

(cid:27)(cid:30)

, M

M (1)

(cid:111)

(cid:110) Ccomtcom
scom

, Ccpttcpt
scpt

, M
where min
is the number of tiles that
can be computed and transmitted, scom = pxw · pxh · b · rf ·
Tseg/γc [13] is the number of bits in each tile for transmission,
scpt = pxw · pxh · b · rf · Tseg is the number of bits in a tile for
rendering, pxw and pxh are the pixels in wide and high of a
tile, b is the number of bits per pixel relevant to color depth
[13], rf is the frame rate, and γc is the compression ratio.

To reﬂect the system capability of streaming tiles in unit

time, we further deﬁne the resources rate as

Rcc (cid:44) Ccc(tcom, tcpt)

tcc

(2)

C. Performance Metric of Tile Prediction

Average segment degree of overlap (average-DoO) has been
used to measure the prediction performance for a VR video
[10]. It indicates the average overlap of the predicted tiles and
the real requested tiles among all the proactively streamed
segments, which is deﬁned as

D(tobw) (cid:44)

1
L − l0 + 1

L
(cid:88)

l=l0

qT
l · el(tobw)
(cid:107)ql(cid:107)1

∈ [0, 100%]

(3)

where ql (cid:44) [ql,1, ..., ql,M ]T denotes the real tile requests
the lth segment with ql,m ∈ {0, 1}, el(tobw) (cid:44)
for
[el,1(tobw), ..., el,M (tobw)]T denotes the predicted tile requests
for the segment with el,m(tobw) ∈ {0, 1}, and (·)T denotes
transpose. When the mth tile in the lth segment
is truly
requested, ql,m = 1, otherwise ql,m = 0. When the tile is
predicted to be requested, el,m(tobw) = 1, otherwise it is zero.
We consider (cid:107)el (tobw) (cid:107)1 = Nfov, where Nfov is the number
of tiles in a FoV and (cid:107) · (cid:107)1 denotes the (cid:96)1 norm of a vector.
A larger value of average-DoO indicates a better prediction.
As veriﬁed in [10], a predictor can be more accurate with
a longer observation window. Therefore, average-DoO is a
monotonically increasing function of tobw.

D. Metric of Quality of Experience

For proactive tile-based VR video streaming, the QoE can
be measured by the percentage of the correctly delivered tiles
among all the really requested tiles [10], i.e.,

QoE =

1
L − l0 + 1

L
(cid:88)

l=l0

(ql)T · sl
(cid:107)ql(cid:107)1

(4)

where sl (cid:44) [sl,1, ..., sl,M ]T with sl,m ∈ {0, 1}. When the mth
tile in the lth segment is determined by the MEC server to be
streamed, sl,m = 1, otherwise sl,m = 0.

From (1) and (3), the QoE can be expressed as

QoE = Q (D (tobw) , Ccc(tcom, tcpt)) ∈ [0, 100%]

which depends on the average-DoO and the CC capability.
When the value of the QoE is 100%, all the really requested
tiles are proactively computed and delivered before playback.
When Ccc(tcom, tcpt) is larger, more tiles can be rendered and
transmitted, then more really requested tiles can be delivered.
When D (tobw) is improved, more streamed tiles are the really
requested tiles. This indicates that the QoE monotonically in-
creases with the average-DoO and CC capability, respectively.

III. FOV PRIVACY-PROTECTING PROACTIVE STREAMING

In the proactive VR Video streaming procedure, the FoV
may be leaked out during predictor training and online pre-
diction. According to whether the tile requests are predicted
directly or indirectly, whether a predictor requires training, and
where the training and predicting are respectively executed, we
provide twelve cases in Table I.

We can ﬁnd that although federated training or simple
predictors with no need for training can avoid the FoV leakage
during the predictor training stage, the real FoV can still
be inferred at the MEC server during the online prediction,
because either the real or predicted effective data for inferring
(viewpoints and tile requests) is uploaded. In this section, we
provide a method to avoid the FoV leakage, deﬁne a metric
to reﬂect the privacy requirement for FoV, and discuss how to
satisfy the requirement.

A. Conceal Real FoVs with Extra Tiles

The FoV leakage comes from the uploaded viewpoints or
tile requests. To protect the FoVs, the HMD should conceal
the real data. If this is achieved by adding noise as in [4],
[5], then the tiles recovered from the noisy viewpoints cannot
include all the really-requested tiles and black holes occur.
Alternatively, we can remove some real tile requests or add
some camouﬂaged tile requests. If some really-requested tiles
are not rendered and transmitted, again, black holes occur.
Therefore, we add camouﬂaged tile requests. The overall tiles
to be rendered and delivered are composed of the predicted
tile requests from a user (with the number as Nfov) and the
camouﬂaged tile requests (with the number denoted as Ncf).
As shown in Fig. 2a, without the camouﬂaged tile requests,
when the MEC server receives the real tile requests with No.
24-27, 34-37, it can immediately obtain the content in the real
FoV. As shown in Fig. 2b, if the camouﬂaged tile requests
with No. 13-18, 23, 28, 33, 38, 43-48 are added, the overall
tiles to be streamed are with No. 13-18, 23-28, 33-38, 43-48
and the MEC server only sees the white borderline region. As
the number of camouﬂaged tile requests increases, it becomes
more and more difﬁcult for the server to correctly infer the
real FoV. In Fig. 2c, some inferred FoVs might even have no
intersection with the real FoV.

With camouﬂaged tile requests, we summarize the FoV
protection in the twelve cases in Table II. In most of cases,
the FoV can be protected, except several indirect prediction
cases where the real viewpoints are uploaded, which should
not be used in practice.

TABLE I: FoV leakage in proactive VR video streaming procedure

No.

Direct/Indirect
prediction

1
2
3
4
5
6
7
8
9
10
11
12

Direct

Indirect

Where to train and predict

Training at MEC, predicting at MEC
Training at MEC, predicting at HMD
Training at HMD, predicting at HMD
Training at HMD, predicting at MEC
No need for training, predicting at MEC
No need for training, predicting at HMD
Training at MEC, predicting at MEC
Training at MEC, predicting at HMD
Training at HMD, predicting at HMD
Training at HMD, predicting at MEC
No need for training, predicting at MEC
No need for training, predicting at HMD

∗When prediction is accurate.

Predictor training
HMD upload data FoV leakage
Real tile requests
Real tile requests
Model parameters
Model parameters

(cid:88)
(cid:88)
×
×

Real viewpoints
Real viewpoints
Model parameters
Model parameters

(cid:88)
(cid:88)
×
×

Tile prediction

HMD upload data
Real tile requests
Predicted tile requests
Predicted tile requests
Real tile requests
Real tile requests
Predicted tile requests
Real viewpoints
Predicted viewpoints or tile requests
Predicted viewpoints or tile requests
Real viewpoints
Real viewpoints
Predicted viewpoints or tile requests

FoV leakage
(cid:88)
(cid:88)∗
(cid:88)∗
(cid:88)
(cid:88)
(cid:88)∗
(cid:88)
(cid:88)∗
(cid:88)∗
(cid:88)
(cid:88)
(cid:88)∗

TABLE II: FoV protection with camouﬂaged tile requests

Predictor training

HMD upload data
Real and camouﬂaged tile requests
Real and camouﬂaged tile requests
Model parameters
Model parameters

Real viewpoints
Real viewpoints
Model parameters
Model parameters

No.

1
2
3
4
5
6
7
8
9
10
11
12

FoV protection

(cid:88)

×
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Tile prediction

HMD upload data
Real and camouﬂaged tile requests
Predicted and camouﬂaged tile requests
Predicted and camouﬂaged tile requests
Real and camouﬂaged tile requests
Real and camouﬂaged tile requests
Predicted and camouﬂaged tile requests
Real viewpoints
Predicted and camouﬂaged tile requests#
Predicted and camouﬂaged tile requests#
Real viewpoints
Real viewpoints
Predicted and camouﬂaged tile requests#

#The HMD uploads the predicted tile requests rather than viewpoints to protect FoVs.

FoV protection

(cid:88)

×
(cid:88)
(cid:88)
×
×
(cid:88)

B. Spatial Degree of Privacy

To reﬂect the privacy requirement of a user for protecting
the real FoV during proactive VR streaming procedure, we
deﬁne a metric of spatial degree of privacy, which is the
normalized number of camouﬂaged tile requests, i.e.,

ρs (cid:44)

Ncf
M − Nfov

∈ [0, 100%]

(5)

When ρs = 0, the user has no privacy requirement, Ncf = 0,
i.e., only the real or predicted tile requests are uploaded to the
MEC server. When ρs = 100%, the user has most stringent
privacy requirement, Ncf + Nfov = M , i.e., the MEC server
sees the whole panoramic region and cannot infer the real FoV.
When a user demands for a VR video, ρs is determined by
the user. Given the required SDoP, the overall number of tiles
in a segment to be rendered and transmitted is

Np(ρs) = Nfov + Ncf = Nfov + (cid:100)ρs(M − Nfov)(cid:101)

(6)

where (cid:100)·(cid:101) is the ceiling function.

C. Satisfy the SDoP Requirement

If the CC capability is not sufﬁcient for rendering and
transmitting the overall tiles of a user with required SDoP,
i.e., Ccc(tcom, tcpt) · M < Np(ρs), then the MEC server
will still stream the predicted tiles to save resources, i.e.,
Np(ρs) = Nfov. As a consequence, the FoV will be leaked
out. This suggests that the CC capability should be larger for
a user with larger value of ρs. To gain useful insight, we
assume that the CC capability can exactly satisfy the required
SDoP, i.e., Ccc(tcom, tcpt) · M = Np(ρs).

IV. IMPACTS OF SDOP ON THE QOE
In this section, we investigate the impact of SDoP re-
quirement on the QoE. To this end, we ﬁrst analyze the

impact of SDoP on the duration for communication and
computing and average-DoO, respectively, from the jointly
optimized durations of observation window, communication,
and computing to maximize the QoE. Then, we discuss the
overall impact of SDoP on the QoE.
A. Joint Optimization of the Durations

Given computing rate Ccpt, transmission rate Ccom and
arbitrary SDoP ρs, we can ﬁnd the optimal durations for ob-
servation window, communication and computing to maximize
the QoE from the following problem

P0 :

max
tobw,tcpt,tcom

Q (D (tobw) , Ccc(tcom, tcpt))

(7a)

s.t. Ccc(tcom, tcpt) =

Np(ρs)
M

tobw + tcpt + tcom = Tps

(7b)

(7c)

As derived in the appendix of [14], the solution of P0 is,

t∗
obw =

t∗
com =

(cid:22) (cid:18)

Tps −

(cid:18) scom
Ccom

+

scomNp(ρs)
Ccom

t∗
cpt =

(cid:19)

scpt
Ccpt
scptNp(ρs)
Ccpt

· Np(ρs)

(cid:19)(cid:30)

(cid:23)

τ

(8a)

(8b)

where τ (in seconds) is the sampling interval in the obser-
vation window. The optimal duration for communication and
computing t∗

cc can be obtained from (8b) as

t∗
cc =

(cid:18) scom
Ccom

+

scpt
Ccpt

(cid:19)

· Np(ρs)

(9)

By substituting (8b) into (2), the maximized resources rate is

R∗

cc = 1

(cid:30)(cid:18) scomM
Ccom

(cid:19)

+

scptM
Ccpt

(10)

(cid:16) scomM
Ccom

+ scptM
where
Ccpt
render and transmit all tiles in a segment.

(cid:17)

is the required optimal duration to

watching 10 VR videos are used for training and testing
predictors.1 We randomly split the total traces into training
and testing sets with ratio 8:2.

We use two indirect tile predictors, deep-position-only and
trivial-motion predictors, which achieve the state-of-the-art
accuracy for the dataset, according to evaluation in [7]. These
predictors represent two types of predictors: The former needs
to be trained while the latter is with no need for training. The
deep-position-only predictor employs a sequence-to-sequence
LSTM-based architecture, which uses the time series of past
viewpoint positions as input to predict the time series of future
positions [7]. The predictor does not consider the time required
for computing and communication as well as the SDoP. To
reserve time for computing and communication, we tailor the
predictor by setting the duration between the end of the obser-
vation window and the beginning of the prediction windows
as t∗
cc, and setting the durations of observation and prediction
windows as t∗
obw and Tseg, respectively. To protect the FoV, we
consider training and predicting at the HMD, which is case
9 in Table II. In predictor training, we consider a classical
federated learning algorithm, FederatedAveraging [19].
The settings of the federated learning are as follows. In each
round, we select all of K = 30 users to update the model
parameters of the predictor. The number of local epochs for
each user is El = 50, the number of communication rounds
is R = 10. Hence, every trace in the training set is used
50 × 10 = 500 times, which is consistent with the centralized
training [7]. The weighting coefﬁcient of the kth user on the
model parameter is ck = nk
, where Ntrain = 300×0.8 = 240
Ntrain
is the total number of traces in the training set, nk is the
number of video traces of the kth user in the training set.
Due to the random division of training and test sets, nk varies
from 6 to 10. We refer to the predictor as tailored federated
position-only predictor. Other details and hyper-parameters of
the tailored predictor are the same as the deep-position-only
predictor [7]. The trivial-motion predictor simply uses the last
position in the observation window as the predicted time series
of future positions [7]. We consider predicting at the HMD,
which is case 12 in Table II.

The maximized resources rate R∗

cc depends on the con-
ﬁgured communication and computing resources as well as
the number of users. For example, when K = 4, Nt = 8,
P = 24 dBm, B = 150 MHz, and dk = 5 m, the ensemble-
average transmission rate for a user is Ccom = 2.85 Gbps
[10]. When Nvidia RTX 8000 GPU is used for rendering
VR videos for four users, the computing rate for a user is
Ccpt = 2.2 Gbps [10]. Then, the maximized resources rate is
R∗
= 0.6. To reﬂect the variation
of conﬁgured resources, we set R∗

(cid:46)(cid:16) scomM
Ccom

+ scptM
Ccpt

cc = 1

cc ∈ [0.6, 2].

(cid:17)

(a) No privacy requirement (ρs = 0), Nfov = 4 × 2 = 8.

(b) Low privacy requirement (ρs = 31%), Ncf = 6 × 4 − Nfov = 16.

(c) High privacy requirement (ρs = 65%), Ncf = 7 × 6 − 8 = 34.
Fig. 2: Real FoV, camouﬂaged tile requests, and inferred FoVs
with different ρs in a segment, M = 60.

B. Impact of SDoP

cct∗

com, t∗

By substituting (9) and (10) into (2), the CC capability can
cpt) = R∗
be rewritten as Ccc(t∗
cc. Further considering
(9) and (10), we can ﬁnd that with a large value of ρs, CC
capability can be increased by using longer total duration
for communication and computing t∗
cc. Then, the duration of
obw = Tps − t∗
observation window t∗
cc will be reduced, which
can also be veriﬁed from (8a). The reduction of t∗
obw degrades
the average-DoO.

A large value of SDoP needs a higher CC capability, which
however degrades the average-DoO. The overall impact on the
QoE depends on if the QoE is dominated by the increase of
CC capability or the reduction of the average-DoO.

V. TRACE-DRIVEN SIMULATION RESULTS

In this section, we show the overall impact of SDoP on QoE

via trace-driven simulation results.

The settings of VR videos are listed in Table III. To gain
useful insight, we assume that all users have identical SDoP
requirement for all videos, ranging from 0 to 100%. After
training the tailored federated position-only predictor, D(t∗
obw)
of the two predictors and Ccc(t∗
cpt) can be obtained. The
QoE of the two predictors is ﬁrst calculated from (4), and then
averaged over the test set. The details of simulation procedure

com, t∗

First, we consider the tile prediction on a real dataset [6],
where 300 traces of head movement positions from 30 users

1According to the analysis in [7], [18], the traces of the ﬁrst 20 users in
the dataset have mistakes, thus we only use the traces of the other 30 users.

WidthPlayback durationHeightReal FoVsegT    1  2  3  4  5  6  7  8  9  10          11  12  13  14  15  16  17  18  19  20    21  22  23  24  25  26  27  28  29  30    31  32  33  34  35  36  37  38  39  40    41  42  43  44  45  46  47  48  49  50    51  52  53  54  55  56  57  58  59  60          1  2  3  4  5  6  7  8  9  10         10 20 30 40 50 60               WidthCamouflaged  tile requestsPlayback durationHeightsegT    1  2  3  4  5  6  7  8  9  10          11  12  13  14  15  16  17  18  19  20    21  22  23  24  25  26  27  28  29  30    31  32  33  34  35  36  37  38  39  40    41  42  43  44  45  46  47  48  49  50    51  52  53  54  55  56  57  58  59  60          1  2  3  4  5  6  7  8  9  10         10 20 30 40 50 60               Inferred FoVsWidthPlayback durationHeightsegT    1  2  3  4  5  6  7  8  9  10    11  12  13  14  15  16  17  18  19  20    21  22  23  24  25  26  27  28  29  30          31  32  33  34  35  36  37  38  39  40          41  42  43  44  45  46  47  48  49  50    51  52  53  54  55  56  57  58  59  60     1  2  3  4  5  6  7  8  9  10         10 20  30 40 50 60Resolution
Number of tiles M
Pixels in wide of a tile pxw
Compression ratio γc
Number of bits in a tile for transmission scom
Size of FoV

TABLE III: Settings of VR videos
3840×2160 pixels [15]
10 rows × 20 columns = 200 [6]
3840/20 = 192
2.41 [16]
5.9 Mbits
100◦ × 100◦ circles [7], [17]

Bits per pixel
Frame rate rf
Pixels in height of a tile pxh
Playback duration of a segment Tseg
Number of bits in a tile for rendering scpt
Number of tiles in a FoV Nfov

12 [13]
30 FPS [15]
2160/10 = 216
1 s [15]
14.2 Mbits
33 [6]

can be found in Procedure 1 of [14].

Fig. 3: Average QoE v.s. resource rates and SDoP.

In Fig. 3, we show the average QoE achieved by tailored
federated position-only predictor versus the assigned resources
rate and SDoP. The results using trivial-motion predictors are
similar, hence are not provided for conciseness. We can ob-
serve that no matter how much the resources rate is assigned,
which predictor is employed, the average QoE can always be
improved with the increase of ρs. Besides, either larger value
of ρs or higher resources rate can support better QoE. For
example, consider the point “P”. To achieve QoE = 94%, one
can increase ρs by 0.2 or increase R∗

cc by 1.4.

To explain why better QoE can be achieved with larger
SDoP and visualize how much resources is traded for SDoP,
we consider an example case in Fig. 4. It is shown that a lager
value of SDoP needs higher CC capability Ccc(t∗
cpt)
but degrades average-DoO D(t∗
obw), as expected. Since the
degradation of average-DoO is smaller (from 76% to 61%),
than the increase of the CC capability (from 0.18 to 1), the
QoE is larger for a user with higher SDoP requirement.

com, t∗

Fig. 4: Average QoE, CC capability, and average-DoO v.s.
SDoP, R∗

cc = 0.6.

VI. CONCLUSION
In this paper, we deﬁned spatial degree of privacy to reﬂect
the requirement for protecting the FoV of a user and investi-
gated the impact of SDoP on proactive VR video streaming.

When the conﬁgured resources for a user can satisfy the
SDoP requirement of the user, we found from the optimized
durations that a large value of SDoP degrades the performance
of predicting tile requests due to requiring more resources.
Simulation with the state-of-the-art predictors on a real dataset
validated the analysis, and showed that by providing more
resources to a user requiring larger SDoP, the QoE of the
user can also be improved although the prediction performance
degrades.

REFERENCES

[1] F. Qian, L. Ji, B. Han, and V. Gopalakrishnan, “Optimizing 360 video
delivery over cellular networks,” ACM SIGCOMM Workshop, 2015.
[2] M. R. Miller, F. Herrera, H. Jun, J. A. Landay, and J. N. Bailenson,
“Personal identiﬁability of user tracking data during observation of 360-
degree VR video,” Scientiﬁc Reports, vol. 10, no. 1, pp. 1–10, 2020.
[3] D. J. Lohr, S. Aziz, and O. Komogortsev, “Eye movement biometrics
using a new dataset collected in virtual reality,” ACM Symposium on
Eye Tracking Research and Applications, 2020.

[4] B. David-John, D. Hosfelt, K. Butler, and E. Jain, “A privacy-preserving
approach to streaming eye-tracking data,” IEEE Trans. Visual. Comput.
Graphics, vol. 27, no. 5, pp. 2555–2565, 2021.

[5] J. Li, A. R. Chowdhury, K. Fawaz, and Y. Kim, “Kal(cid:15)ido: Real-
time privacy control for eye-tracking systems,” 30th USENIX Security
Symposium (USENIX Security 21), Aug. 2021.

[6] W.-C. Lo, C.-L. Fan, J. Lee, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu,
“360◦ video viewing dataset in head-mounted virtual reality,” ACM
MMSys, 2017.

[7] M. F. Romero Rondon, L. Sassatelli, R. Aparicio-Pardo, and F. Pre-
cioso, “Track: A new method from a re-examination of deep ar-
chitectures for head motion prediction in 360-degree videos,” IEEE
Trans. Pattern Anal. Machine Intell., early access, 2021, source code:
https://gitlab.com/miguelfromeror/head-motion-prediction/tree/master/.
[8] C.-L. Fan, W.-C. Lo, Y.-T. Pai, and C.-H. Hsu, “A survey on 360◦ video
streaming: Acquisition, transmission, and display,” ACM Comput. Surv.,
vol. 52, no. 4, Aug. 2019.

[9] C. Fan, S. Yen, C. Huang, and C. Hsu, “Optimizing ﬁxation prediction
using recurrent neural networks for 360◦ video streaming in head-
mounted virtual reality,” IEEE Trans. Multimedia, vol. 22, no. 3, pp.
744–759, March 2020.

[10] X. Wei, C. Yang, and S. Han, “Prediction, communication, and com-
puting duration optimization for VR video streaming,” IEEE Trans.
Commun., vol. 69, no. 3, pp. 1947–1959, 2021.

[11] 3GPP, “Extended reality (XR) in 5G,” 2020, 3GPP TR 26.928 version

16.0.0 release 16.

[12] D. Bethanabhotla, G. Caire, and M. J. Neely, “Adaptive video streaming
for wireless networks with multiple users and helpers,” IEEE Trans.
Commun., vol. 63, no. 1, pp. 268–285, 2015.

[13] iLab, “Cloud VR network solution whitepaper,” Huawei Technologies
CO., LTD., Tech. Rep., 2018. [Online]. Available: https://www.huawei.
com/minisite/pdf/ilab/cloud vr network solution white paper en.pdf

[14] X. Wei

and C. Yang,

“Spatial privacy-aware VR streaming,”

arXiv:2104.14170, 2021.

[15] A. Mahzari, A. T. Nasrabadi, A. Samiei, and R. Prakash, “FoV-aware
edge caching for adaptive 360◦ video streaming,” ACM MM, 2018.
[16] M. Zhou, W. Gao, M. Jiang, and H. Yu, “HEVC lossless coding and
improvements,” IEEE Trans. Circuits Syst. Video Technol., vol. 22,
no. 12, pp. 1839–1843, 2012.

[17] C.-L. Fan, J. Lee, W.-C. Lo, C.-Y. Huang, K.-T. Chen, and C.-H. Hsu,
“Fixation prediction for 360◦ video streaming in head-mounted virtual
reality,” ACM NOSSDAV, 2017.

[18] M. F. R. Rond´on, L. Sassatelli, R. Aparicio-Pardo, and F. Precioso, “A
uniﬁed evaluation framework for head motion prediction methods in
360° videos,” ACM MMsys, 2020.

[19] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” PMLR AISTATS, 2017.

020%40%60%80%100%00.20.40.60.8160%70%80%90%100%