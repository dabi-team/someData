1

Cooperative Multigroup Broadcast 360° Video
Delivery Network: A Hierarchical Federated Deep
Reinforcement Learning Approach

Fenghe Hu, Student Member,

IEEE, Yansha Deng, Member,

IEEE,

A. Hamid Aghvami, Fellow,

IEEE,

1
2
0
2

v
o
N
5

]
P
S
.
s
s
e
e
[

3
v
7
4
3
1
1
.
0
1
0
2
:
v
i
X
r
a

Abstract—With the stringent requirement of receiving video
from the unmanned aerial vehicle (UAV) from anywhere in
the stadium of sports events and the signiﬁcant-high per-cell
throughput for video transmission to virtual reality (VR) users,
a promising solution is a cell-free multi-group broadcast (CF-MB)
network with cooperative reception and broadcast access-points
(AP). To explore the beneﬁt of broadcasting user-correlated
decode-dependent video resources to spatially correlated VR
users, the network should dynamically schedule the video and
cluster APs into virtual cells for a different group of VR users
with overlapped video requests. By decomposing the problem
into scheduling and association sub-problems, we ﬁrst introduce
the conventional non-learning-based scheduling and association
algorithms, and a centralized deep reinforcement learning (DRL)
association approach based on the rainbow agent with a con-
volutional neural network (CNN) to generate decisions from
observation. To reduce its complexity, we then decompose the
association problem into multiple sub-problems, resulting in
a networked-distributed Partially Observable Markov decision
process (ND-POMDP). To solve it, we propose a multi-agent
deep DRL algorithm. To jointly solve the coupled association and
scheduling problems, we further develop a hierarchical federated
DRL algorithm with scheduler as meta-controller, and association
as the controller. Our simulation results show that our CF-
MB network can effectively handle real-time video transmission
from UAVs to VR users. Our proposed learning architecture
is effective and scalable for a high-dimensional cooperative
association problem with increasing APs and VR users. Also,
our proposed algorithms outperform non-learning based methods
with signiﬁcant performance improvement.

I. INTRODUCTION

Unmanned aerial vehicle (UAV) systems bring fast and
easy accessibility of aerial video capture into our daily life.
Although the existing WiFi or Long-Term-Evolution (LTE)
technologies can support low-resolution video transmission for
ﬂight control, they are not suitable for applications, where
many audiences need simultaneous streaming high-resolution
videos from UAVs for enhancing viewing experience with
virtual reality contents in large sports events. A typical solution
is the True View Technology for large sports events introduced
by Intel [1]. With the help of a large camera array distributed
around the stadium and on UAVs, the overall scenes can be
rebuilt in real-time with a huge amount of viewpoints. The cap-
tured video from diverse angles is processed into a volumetric
video set, which contains real-time content for virtual reality

F. Hu, Y. Deng, and A. H. Aghvami are with King’s College London, UK
(E-mail:fenghe.hu, yansha.deng, hamid.aghvami@kcl.ac.uk)(Corresponding
author: Yansha Deng).

(VR) video resources from different viewpoints. This enhances
the audiences’ viewing experience by immersing audiences
in their selected environment with head-mounted displays
(HMD). To realise the full vision of event enhancing VR
service, a wireless network is needed to receive, process and
transmit the captured 360° VR video from multiple UAVs to
massive VR users. However, as shown by Qualcomm [2], [3],
the overall capacity requirement for such service from network
to VR users can reach 22T bps/km2 level, which can’t be
satisﬁed with existing wireless technologies. Also, this service
requires seamless real-time responses to VR users’ viewpoint
selections, and the newly generated video frames should be
successfully transmitted and decoded without noticeable jitter
or delay [2], [4].

Existing researches on VR video transmission mainly focus
on reducing the transmission delay via caching and wireless
resource allocation for pre-stored video resources [5]–[9]. In
[5]–[7], the authors designed a caching algorithm to reduce
the transmission delay of VR video resources from the UAVs
or cloud server to the VR users with the support of the
edge server. By periodically re-arranging the video resource
held at the edge server, the requested video resource can be
directly transmitted to the VR users from the edge server
without fetching from the UAVs in real-time to save the overall
delay. In [8], the authors optimized the resource allocation
for VR video transmission under the consideration of data
correlation. With the human factor in the loop, the authors
[9] extended [8] by integrating the prediction of VR users’
motions prediction into allocation algorithm and reduce the
overall delay of the video resource transmission. However, [4],
[10] assumed pre-stored independent VR video resource in the
form of chunk or image without considering video increment
decoding schemes. In [6], a scheduling algorithm was applied
to manage the processing and transmission of correlated tasks
in VR. However, their models are not for real-time VR video
capture and transmission.

To satisfy the critical requirement of transmitting real-time
VR video from UAVs to a large number of VR users, the
broadcasting technique is shown to be a promising solution
[10], especially for the scenario with highly correlated re-
quests. By discretizing the video resources into smaller units,
namely,
the correlated tiles can be broadcasted
to all VR users requesting these tiles. This can largely re-
duce the bandwidth requirement. However, the performance
of the broadcast system in a large area network is heavily

tiles [11],

 
 
 
 
 
 
limited by inter-cell interference, especially for cell-edge VR
users. To cope with this challenge, one possible solution is
cooperative transmission, which has been proposed in [12]–
[14]. To facilitate a wide range of cooperation among a large
number of distributed access points (AP), the authors in [13]
proposed a cell-free (CF) multi-input-multi-output (MIMO)
network by connecting APs to a central server via high-
speed backhaul links. This concept is further extended to a
user-centric CF-MIMO network, where the APs are clustered
into different groups that can serve multiple groups of users
simultaneously [14]. However, the association problem in such
a network is complex to solve due to the exponential increase
of the complexity with the number of cooperative APs [14].
Besides, the environment information is high-dimensional with
a large number of VR users and cooperative APs. Luckily,
deep reinforcement learning (DRL) has been shown useful in
solving high dimensional communication problems in complex
environments [8], [10], [15], but its scalability is still an issue
for multiple-agent large-scale networks.

Motivated by the above,

in this paper, a CF broadcast
network is proposed to jointly stream the VR video resources
from UAVs and broadcast to the target VR user groups with
spatial and content correlation. In this network, there are two
challenging problems to solve in real-time: 1) a scheduler
to arrange the transmission and re-transmission of VR video
resources; 2) an association algorithm to dynamically re-group
APs to connect UAVs with each VR user group and reduce
interference. Importantly, the scheduling and association stages
occur sequentially. More specially, the scheduler ﬁrst decides
the tiles to be transmitted. The optimal association then group
APs based on UAV and VR users’ positions to avoid high inter-
cell interference. This calls for a joint design of scheduling and
association algorithms. Our contributions are summarized as
follows:

• We ﬁrst propose a decode-forward (DF) CF-MB network
for VR video resource transmission with UAV-APs uplink
from UAV camera to APs group, and APs-VR downlink
from APs group to users. We also deﬁne our VR video
resource via tiles, and QoE metric via the viewpoint-
peak-signal-noise-ratio (V-PSNR) based on the number
of successfully decoded tiles at
the VR users’ sides.
Then, we formulate our optimization problem as a semi-
Markov-decision-process (semi-MDP).

• We then highlight the limitation of conventional cen-
tralized learning algorithms where the complexity of
the problem increases exponentially with an increasing
number of participating APs. To cope with this challenge,
we ﬁrst formulate the association part of the optimization
problem as a networked partially observable Markov-
decision-process (ND-POMDP) via mean-ﬁeld theorem.
In this way, we decompose the association problem into
multiple subproblems, which are networked coordinated.
We then propose a distributed multi-agent DRL approach
with the help of federated to stabilize and accelerate the
learning. Our results highlight that our distributed algo-
rithm can efﬁciently solve the association optimization
problem with decent scalability. The existing learning

2

algorithms from existing works fail to capture the scala-
bility problem and are only capable to deal with several
access-points (APs).

• To jointly optimise the interplay between the scheduling
and the association, we propose a hierarchical DRL
architecture with a centralized scheduler and distributed
association to jointly optimize the V-PSNR. Our results
show that the hierarchy learning structure can effectively
handle the complex optimization problem with sequential
decisions.

The remainder of this paper is organized as follows. Section
II illustrates the communication model and video decoding
model. In Section III, we deﬁne our optimization target by
deﬁning viewpoint peak signal-noise ratio (V-PSNR) as the
QoE metric. We propose conventional methods for scheduling
and association separately. In Section IV, we ﬁrst propose
our centralized DRL algorithm for the association problem.
We introduce ND-POMDP problem, which is then solved in
a federated multi-agent association setting. Then, in Section
V, we apply the hierarchical
learning method to capture
both scheduling and association sub-problems. The numerical
results are presented in Section VI. Finally, we conclude the
paper in Section VII.

II. SYSTEM MODEL

As illustrated in Fig. 1a and Fig. 2, we consider a cell-
free multi-group broadcast (CF-MB) network for 360° video
transmission in a large sports event. This CF-MB network is
composed of 1) a set of APs B, which are located in the grid;
2) a central server, which connects all APs through backhaul
links; 3) a set of randomly located camera UAVs
optical
U, where each UAV provides the video resource from their
orientation; and 4) a set of VR V users, whose locations follow
Poisson cluster process (PCP) with |U| clusters [16]. We
consider the distribution of VR users follows PCP because the
user density is highly correlated in the hot-spot area in large
events or large-scale networks [17], such as stores and certain
booths. It is also noted that the VR users’ video requests can
correlate to their locations in our considered scenario. Thus,
PCP is more realistic for a user-centric correlated scenario than
other uniform distributions.

Each VR user requests its video resource from a UAV based
on their ﬁeld-of-view (FOV) selection. As shown in Fig.1b,
VR users ﬁrst select their interested FoV provided by multiple
UAVs. Then, as shown in Fig.1c, VR users request the tiles
from 360° video provided by the selected UAV based on their
FOV selection. Also, as shown in Fig. 1c, the overlapped
FoV results in the correlation among VR users’ requests [10].
We assume that VR users in each PCP cluster request video
resources from the same UAV but different tiles, while the
clusters can be overlapped or disjointed. All nodes are located
inside the serving area of the plane R2, and remain spatially
static in each group-of-picture (GOP) once deployed. The
video resource is captured by UAVs, processed by the central
server, and transmitted to VR users via the CF-MB network
on request. In short, the CF-MB network acts as a decoded-
forward (DF) relay, which receives the video from the UAV

3

(a) Event scenario with viewpoints captured by multiple UAVs

(b) Tiled-based video mapping from 360° to 2D

(c) User correlation in tiles

Figure 1: Illustration of scenario, tiled-based video model and corresponding VR user correlation.

and broadcasts the processed video to target the VR user
group based on VR users’ requests. However, the resource
requests are small packets in tens of bytes level, whereas the
video data’s size is usually in GB level. Due to the signiﬁcant
different trafﬁc characteristics of video data and VR requests,
we focus on the UAV-APs uplink from UAVs to the APs in
the CF-MB network, and the APs-VR downlink from the APs
in the CF-MB network to VR users for video resource in this
paper.

A. Transmission Channel Model

To capture the different channel characteristics between
APs, UAVs and VR user groups, we consider different channel
models for the UAV-APs uplink and the APs-VR downlink,
respectively. The UAV-APs uplink from UAV to APs and
APs-UAV downlink from APs to VR user group occupy
BUL and BDL bandwidth, respectively. We also assume that
a perfect channel state information (CSI) is available at the
APs. We assume that both channels follow the block fading
assumption, where the channel remains constant on a time-
frequency coherence block [18].

1) UAV-APs Uplink: The UAV-APs uplink between a UAV
and a AP group forms a virtual single-input-multi-output
(SIMO) system, where multiple APs are associated to enhance
the signal reception quality. Considering potential line-of-sight
(LoS) and non-line-of-sight (NLoS) for low altitude ﬂying
drones, we adopt free-space path loss and Rayleigh fading
to model the UAV-APs uplink path loss model as

hu,b =

(cid:40)

c

( 4πdu,bf UL
( 4πdu,bf UL

c

c

c

P u,b
)αUL ηLoSβu,b,
LoS
NLoS = 1 − P u,b
)αUL ηNLoSβu,b, P u,b

LoS

,

(1)

du,b

π sin−1( hu,b

where θu,b = 180
) is the elevation angle of the
drone, hu,b represents the height of ﬂying drone, du,b denotes
the distance between the bth AP and the uth UAV [19], f UL
is the uplink channel center frequency, ηLoS and ηNLoS are the
excessive path loss coefﬁcients in LoS and NLoS cases, c is
the light speed, and αUL is the path loss exponent. In (1), we
adopt the LoS probability of the UAV-APs uplink as [20]

c

where θu,b = 180
UAV.

π × arcsin( hu
du,b

), hu is the ﬂight height of

Based on (1) and (2), the combined channel between the

bth AP and the uth UAV can be expressed as
4πdu,bf UL
c

LoSηLoS + P u,b

hu,b = [P u,b

NLoSηNLoS](

c

)αULβu,b,

(3)

where P u,b

LoS is given in (2).

2) AP-VR Uplink: We consider Rayleigh fading for multi-
input-single-output (MISO) transmission between each AP
group and VR user [13]. The channel between the bth AP
and vth VR user is represented as

hb,v = d−αDL

b,v βb,v,

(4)

where db,v represents the distance between the bth AP and
the vth VR user, αDL represents the AP-VR uplink path loss
exponent, and βb,v denotes the Rayleigh small-scale fading.

B. Tile Transmission Data Rate

From the perspective of the CF-MB network, the network is
operating in frequency-division-duplex (FDD) mode with the
transmission of UAV-APs uplink and APs-VR downlink at the
same time over different frequency bands . We assume that all
UAV and VR users are equipped with one antenna. Each AP
is equipped with two antennas where one antenna for UAV-
APs uplink, one for APs-VR downlink. The tile transmission
model can be seen as a DF relay system, where UAV-APs
uplink is SIMO transmission and APs-VR downlink is MISO
transmission.

1) Data Rate of UAV-APs Uplink: For the SIMO trans-
mission of UAV-APs uplink from single UAV to multiple
cooperative APs, we adopt
the maximum-ratio combining
(MRC) technique to realise the multiple reception gain. The
received signal γu∗,b from scheduled the u∗th UAV to the
bth AP within associated APs group Bu∗
at time t can be
t
expressed as

yu∗,b = hu∗,bsu∗

+

U
(cid:88)

hu(cid:48),bsu(cid:48)

+ n0

,

(5)

P u,b

LoS =

1
1 + 11.95 exp(−0.14[θu,b − 11.95])

,

(2)

(cid:124) (cid:123)(cid:122) (cid:125)
Desired signal

u(cid:48)∈Ut\u∗
(cid:124)
(cid:123)(cid:122)
Interference from Other UAVs’

(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)
Noise

γu∗,Bu

t

=

(cid:88)

b∈Bu
t

wbyu∗,b,

(6)

γBu

t ,v∗ =

where hu,b denotes the channel vector from the uth UAV
to the bth AP, Ut
is the current scheduled UAV, hu(cid:48),b is
the interference channel from other interfering UAVs, su is
the signal transmitted by the uth UAV, N0 ∼ CN (0, IN )
represents the Gaussian white noise. Then, the signal after
MRC can be expressed as

t

||F, b ∈ Bu

u∗,b/||hu∗,Bu

where wb is a general weighted MRC scheme with weight
wb = hH
t , || · ||F represents Frobenius
norm, and hu∗,Bu
t |×1 channel
vector from target the u∗th UAV to a corresponding APs group
Bu

t [21].
Thus, the received SINR for tile upload from the uth UAV

= [hu∗,b0, ..., hu∗,b|Bu

t | ] is a |Bu

t

to accesspoint group Bk

t at time t can be expressed as

γu∗,Bu

t

=

pu|wbhu,b|2

(cid:80)
b∈Bu
t

(cid:80)
b∈Bu
t

U
(cid:80)
u(cid:48)∈U \u

pu(cid:48)|wbhu(cid:48),b|2 + (cid:80)
b∈Bu
t

|wb|2σ2

.

(7)

Due to the ﬂat-fading in each broadcast slot, the received
(t) during resource block at the group

data capacity Du∗,Bu∗
of APs Bu∗
t

t

from the u∗th UAV is given by

Du∗,Bu∗

t

= TbBUL log2(1 + γu∗,Bu∗

t

).

(8)

2) Data Rate of the APs-VR Uplink: In APs-VR uplink,
the APs form virtual-cells to jointly broadcast the tiles to
corresponding VR user groups and enhance the broadcasting
quality. As shown in Fig. 2, the cooperative APs can enhance
the signal quality in receiving from the VR users, but the
inter-cluster interference limits the overall performance. To
realise the gain of jointly broadcasting and to improve the
worst VR user’s performance, we adopt linear sum maximum
precoding [22]. With perfect channel state information (CSI),
the precoding matrix in the bth AP can be given by

V k
t(cid:88)

wb = αb

hH
b,v
||hb,v||2 , b ∈ Bk
t ,

v
where αb is the normalize factor to ensure ||wb||2

F = 1.

(9)

user (v∗ ∈ V u

Based on (9), the signal received at the selected v∗th VR
t ) from the bth AP can be expressed as
B
(cid:88)

(cid:88)

yBu

t ,v∗ =

hb,v∗ wbsb +

hb(cid:48),v∗ wb(cid:48)sb(cid:48) + nv∗

b∈Bu
t

b(cid:48)∈B\Bu
t

(cid:88)

=

hb,v∗

V u
t(cid:88)

αb

hH
b,v
||hb,v||2 sb
(cid:125)

b∈Bu
t
(cid:124)

+

v
(cid:123)(cid:122)
Desired Signal
Bn
t(cid:88)

B
(cid:88)

V n
t(cid:88)

αb(cid:48)

hb(cid:48),v∗

t ∈B\Bu
t

Bn
(cid:124)

b(cid:48)

v(cid:48)
(cid:123)(cid:122)
Inter-group Interference

b(cid:48),v(cid:48)

hH
||hb(cid:48),v(cid:48)||2 sb(cid:48)
(cid:125)

+ nv∗

,

(cid:124)(cid:123)(cid:122)(cid:125)
Noise

(10)
where wb denotes the precoding matrix for the bth AP in group

t at time t, and hb,v is the path loss for the channel between

Bu
the bth AP and the vth VR user at time t.

4

Based on (10), the SINR from the bth AP in APs group Bu
t
t at time t can be expressed

to v∗th VR user in user group V u
as

pb|hb,v∗ wb|2

(cid:80)
b∈Bu
t

B
(cid:80)
b(cid:48)∈B\Bu
t

pb(cid:48)|hb(cid:48),v∗ wb(cid:48)|2 + σ2

.

(11)

Under given SINR, the received data DBu

t ,v in one broadcast
to v∗th VR user can be
slot Tb from the APs group Bu
t
calculated by the minimum ergodic rate within the broadcast
group

DBu

t ,v = TbBDL

c

log2(1 + γBu

t ,v∗ ).

(12)

C. Tiled-based Video Resource Model

Tile-based VR video transmission can effectively support
the broadcasting of video resources [11]. It splits the captured
video resource into small tiles, which can be decoded individ-
ually. By exploring the nature of video codec, the tiled-based
video transmission is introduced for VR video transmission,
where the tiles in the same location can be decoded individ-
ually [11]. As shown in Fig. 1a, each UAV records a 360°
video stream with on-broad camera, which is converted and
transmitted in 2D video format via Equirectangular projection.
As shown in Fig. 1b, we deﬁne that each tile contains for
30° × 30° square part in 180° × 360° video, which is full-
view from a certain viewpoint [23]. Thus, each UAV provides
6 × 12 tiles, which is shown in Fig. 1b. The size of one tile
is deﬁned as µMT bits, where µ is the compression rate. The
compression rate is decided by tile type, which is explained
tile set as J , which is
later. We also denote the overall
provided by the set of UAVs U.

As shown in Fig. 2, a set of new tiles from newly captured
video frames are generated every Tf time, i.e. at frame rate
1/Tf. In our scenario, we assume that all video frames from
different UAVs are captured and encoded in the same frame
rate.

D. Tiles Requests, Receiving and Decoding Model

t , v ∈ V at time t (|J v

To describe the content request in each VR user via tiles,
we highlight that the ﬁeld-of-view (FoV) of human is deﬁned
as 150° × 210° [11]. Thus, vth VR user requests 5 × 7 tiles,
denoted by J v
t | = 5 × 7 = 35). The
actual number of tiles can vary based on different positions of
viewpoints in 360° space which requires tiles follow the rule
of 3D-2D projection [24] (as shown in Fig. 1c), and viewpoints
are randomly generated. In this way, the group of VR users
who requests the same tile j, can be served via the broadcast
channel at the same time. We denote this group of VR users as
Vj. The highly correlated tile requests in our scenario highlight
the potential beneﬁt of broadcasting overlapping tiles.

To transmit the requested tiles to corresponding VR users,
the tiles are delivered via the aforementioned DF network
transmission, since the success of tile transmission will only
occur when both the UAV-APs uplink and APs-VR downlink

success. The successful transmission of jth tile can be written
as the combination of successful transmission in UAV-APs
uplink and APs-VR downlink as

1[Du,v ≥ µMT] = 1[Du,Bu

≥ µMT] ∧ 1[DBu

t

t ,v ≥ µMT],

(13)
where µMT is the size of tile to be transmitted, 1[x] = 1
as x is true, 1[x] = 0, otherwise. ∧ is logical and operation.
1[x] ∧ 1[x] = 1 as x and y is true, 1[x] ∧ 1[x] = 0.

After receiving the tiles,

the tiles need to be decoded
dependently with frame decoding scheme, as shown in Fig.
2, since frames are encoded incrementally within group-of-
pictures (GOP) to reduce the overall data rate. For the low-
latency video encoding scheme, we only consider two typical
kinds of the frame inside one GOP — intra-coded frame (I
Frame), and predicted-coded frame (P frame). The I frame
can be decoded individually, whereas the P frame requires
the same location’s frame or tile in previous time instance
to decode [25]. With such a dependent encoding scheme, the
overall channel capacity required for video transmission can
be saved. Thus, one tile can be successfully decoded only
when the previous tiles are successfully decoded, whose set is
denoted as Jv

t in the vth VR user at time t.

E. Network Transmission Procedure

We show the transmission procedure for tiles from UAV
to VR users in this subsection. Recap that in our considered
CF-MB network, the network performs as a DF relay system
to support the tile j transmission from uth UAV to VR user
group Vj via APs group Bu
t . For time-frequency resources,
we adopt a time-division duplex, which is assumed in many
massive MIMO works [26].

As shown in Fig. 2, each frame with a duration of Tf is
divided into Tf/Tr re-scheduling slots, where Tr is the length
of each re-scheduling slot. Each re-scheduling slot is further
divided into Tr/Tb broadcast slots, where Tb is the length for
each broadcast slot. As shown in Fig. 2, there are 5 stages in
each re-scheduling slot of the network transmission procedure,
which are scheduling stage, association stage, UAV-APs uplink
transmission stage, processing stage, and APs-VR downlink
transmission stage. In the scheduling stage, the network ﬁrst
decides the priority of tiles in each UAV based on VR users’
requests. Then, the (cid:98)Tr/Tb(cid:99) tiles with highest priority in each
UAV is picked for transmission within Tr.

In the association stage, each AP selects one UAV and
corresponding VR users to serve inside each broadcast slot
Tr, since the single-antenna UAV is capable of transmitting
one tile at the same time. We denoted the target VR users
whose requested jth tile is transmitted at time t forms the
VR user group Vj (j ∈ Jt). Then, the APs select the same
UAV are clustered as a virtual cell Bu
t to jointly serve the same
UAV and corresponding group of VR users, i.e. both UAV-APs
uplink and APs-VR downlink.

Then, in the UAV-APs uplink stage, the uth UAV transmits
the scheduled tile j to it associated AP group Bu
t , which jointly
receives the signal. In the processing stage, the tile is processed
at the central server, whose delay is considered as a constant
value and ignored in our analysis. In the broadcast stage, the

5

APs in virtual cell Bu
t jointly broadcast the tile to the VR user
group requesting tile j, i.e. Vj. The UAV-APs uplink stage,
processing stage, and APs-VR downlink stage repeated until
the end of Tr with the same scheduling priority.

F. Quality-of-experience Metric for VR Users

Generally, for video-based VR service, the literature deﬁnes
the QoE as the break-in-presence (BIP), which describes the
event when users stop responding to the virtual environment
(the video frame is not delivered upon a certain threshold
or the resource requirement is not satisﬁed) [4]. However,
when it comes to practical VR applications,
the QoE is
deﬁned by the information loss in the video of current scenes.
Sometimes, failing to transmit one of the tiles on time may
not signiﬁcantly inﬂuence the performance, as the difference
between this failure tile and the previous tile is negligible.
Thus, the BIP, which simply classiﬁes the transmission of each
frame into success and failure cases, lacks realistic meaning,
detailed resolution and accuracy. Thus, we borrow the idea
of Peak Signal-to-Noise (PSNR), which measures the spatial
information difference between desired and received video.
We then create our QoE matrix via viewport-PSNR (V-PSNR)
inspired by the idea from PSNR and BIP.

We calculate the information differently based on the
amount of successfully received and decoded tiles in each
time slot with PSNR function [27] and decide each tile is
successfully received and decoded via BIP function [4]. By
doing so, we can quantify the decoded QoE with PSNR value
inside the vth VR user ﬁeld-of-view at time t using V-PSNR
as

|J v
t |

,

(14)

V-PSNRv

t = 10 log10

|J v

j∈J v
t

1[j ∈ Jv
t ]

t | − (cid:80)
is the desired tile set, and Jv
where J v
t represents the success-
t
ful decoded tiles at time t in the vth VR user (Jv
t ). The
V-PSNR value gives 20 log10 |J v
t | if all the tiles requested
by the vth VR user are transmitted successfully. 1[j ∈ Jv
t ]
denotes the BIP function of single tile j at time t. The BIP
of tile j depends on the successful transmission of its and its
dependent tiles:



t ⊆ J v

1[Du,v ≥ µMT],
1[Du,v ≥ µMT] ∧

t < Tf,
t ≥ Tf

1[j ∈ Jv

t ] =

1[j(cid:48) ∈ Jv
t ]
(cid:124)
(cid:123)(cid:122)
(cid:125)
Dependent tile received

,



(15)
where 1[Du,v ≥ µMT] is given in (13), j and j(cid:48) are dependent
tiles, j is required to be decoded with j(cid:48) incrementally, i.e. j
depends j(cid:48) to decode. In each GOP, when t < Tf, the tile is
from I frame, which can be decoded independently.

III. PROBLEM FORMULATION AND CONVENTIONAL
METHODS

In this section, we deﬁned and decomposed our optimization
problem into scheduling and association sub-problems. We
then introduce the conventional methods for each sub-problem.

6

Figure 2: Communication stages and video tiles decoding relationship for considering system.

A. Problem Formulation

We aim to design an algorithm for the CF-MB network
which supports the tile transmission from UAVs to VR users
and enhance the QoE of VR users by dynamically adjusting
the scheduling and association decisions. The system can be
seen as a Markov decision process (MDP), as the system state
can be fully characterised via a state s without correlation
with historical decisions in each time slot. Knowing that the
small-scale fading is independent of historical information. We
denote the set of the state as S. Each state s contains VR
users’ request, VR users’ decoding sequence, UAVs’ position,
VR users’ V-PSNR, UAV-APs uplink’s, APs-UAV downlink’s
channel information, and etc. The system transfers to a new
state in the next
time slot based on the scheduling and
association decisions with the probability transfer function.
However, simultaneously adjusting both decisions is complex
[28], especially when these two decisions are made in dif-
ferent time scales — scheduling priority is updated every re-
scheduling slot Tr, and association decision is updated every
broadcast slot Tb. To deal with this, in our proposed tile
transmission procedure, we highlight that the scheduling and
association procedures are executed successively. Scheduling
is the primitive action of association. Then, it is appropriate
to deﬁne the original problem as a semi-MDP with scheduler
as Markov options of the association MDP, and solve it via a
hierarchy architecture [29].

We ﬁrst deﬁne our considered problem as a semi-Markov
Decision Process (semi-MDP) [29]. The decision is made
via policies from both scheduling and assocation. With the
scheduling policy as πs and the association policy as πa, the
scheduling priority and associated virtual cells {Bu
} (u ∈ U)
tb
are decided based on current state in sequence. Here, πs is
a weighted mapping from the current state to the priority
of tile transmission. The Ttr/Ttb
tiles with highest priority
is allocated to be transmitted, whose set is denoted as Jtr.
Here, πa is the distribution mapping from the current environ-
ment state and selected scheduling decisions to the selection
of each UAV and corresponding VR user group. Here, the
scheduling priority is considered as the primitive actions,
which is followed by association decisions that persist through
Tr. Then, the system state Stb+1 in (tb + 1)-th broadcast slot
transfers from the system state Stb based on the probability

transfer function. The transfer function and expected reward
are deﬁned via each state, scheduling policy, and association
policy, such that it forms semi-Markov decision process (semi-
MDP) problem, and the scheduling is deﬁned as the option on
the association MDP.

The Markov option (scheduling) is deﬁned by a tuple of
initial state set I ⊆ S, terminal condition t = nTr, n ∈ N,
and policy π : S → As [29]. Here, we consider a constant
re-schedule time. Thus, the initial state I contains the state
when t = nTr, n ∈ N. The scheduling policy maps the state
to the scheduling priority. Note that scheduling needs to run
at equal or slower time-step than association, i.e. Tr ≥ Ta.

With the deﬁnition of semi-MDP, it is possible to separate
the original problem into sub-problems: scheduling and associ-
ation. They can be jointly optimized by a multi-layer hierarchy
structure with scheduling as meta-controller [30]. First, we
write our optimization target as maximizing the accumulative
V-PSNR gain over broadcast slots in TGOP via ﬁnding the
optimal πs and πa.
TGOP(cid:88)

(cid:88)

(cid:88)

E[

max
πs,πa

∆V-PSNRv
tb

|Jtb ∼ πs, Vj ∼ πs],

tb=0

j∈Jtb

v∈Vj
(cid:124)

(cid:123)(cid:122)
V-PSNR Gain for transmitted tile j
(cid:125)
(cid:123)(cid:122)
V-PSNR Gain in Tb for scheduled tile set Jtb

(cid:125)

(cid:124)

− V-PSNRv

(16)
where the V-PSNR gain is denoted as ∆V-PSNRv
=
tb
V-PSNRv
tb−1. The scheduling sub-problem acts
tb
as a meta-controller to optimize the cumulative intrinsic V-
PSNR gain with certain πa in Tr time-scale:
TGOP(cid:88)

(cid:88)

(cid:88)

∆V-PSNRv
tr

|πa].

(17)

E[

max
πs

tr=0

v∈Vj

j∈Jtr
(cid:124)

(cid:123)(cid:122)
V-PSNR gain within Tr

(cid:125)

The association sub-problem maximizes the cumulative extrin-
sic V-PSNR gain with a given Jtb in Tb time-scale

E[

max
πa

TGOP(cid:88)

tb=0

(cid:88)

(cid:88)

∆V-PSNRv
tb

].

(18)

v∈Vj

j∈Jt
(cid:124)

(cid:123)(cid:122)
V-PSNR gain within Tb

(cid:125)

From (17) and (18), we can observe that the scheduling and

association problems are directly coupled, which need to be
jointly optimized.

IV. REINFORCEMENT LEARNING APPROACH FOR
ASSOCIATION

7

B. Conventional Approaches

In this section, we introduce conventional scheduling
and association approaches for each sub-problem, namely,
popularity-based proportional fair (P-PF) scheduling, cell-
based (CB), and cell-free (CF) associations, respectively.

1) Popularity-based Scheduling: According to (15) and
(17), the potential V-PSNR gain for transmitting the tile j is
jointly determined by the number of VR users in the group Vj,
and the transmission successful rate in current and previous
broadcast slots. From (14) and (15), we know that the V-
PSNR gain in each broadcast slot Tb tightly correlates to the
number of VR users who request the tile j, i.e. |Vj|. Thus, the
more VR users request the tile j, the more V-PSNR gain via
transmitting the tile j. This instantly results in a popularity-
based scheduling algorithm, where tiles with higher popularity
are transmitted in each Tb. Remind that, the scheduling action
directly decides which tile to transmit in each broadcast slot
for each UAV, which in turn decides and the corresponding
VR user group Vj.

Additionally, to take decoding state and fairly serve all VR
users, we borrow the idea of proportional fair (PF) scheduler
that has been widely used in existing cellular network [31]. By
adding the previous tiles’ decoding state in denominator, the
resulting P-PF scheduling method determines the prioritization
of tile j at time tr as

P-PFj =

(cid:80)
v∈Vj

1[j ∈ J v
tr

, j /∈ Jv
tr

]

tr−1
(cid:80)
(cid:48)=0
tr

1[jt(cid:48)

r

∈ Jv
tr

]

,

(19)

r denotes the tile at time t(cid:48)

where Jv
tr denotes the successfully decoded tiles in the vth VR
user at time t, and jt(cid:48)
r that is required
by tile j’s decoding, the value of numerator is 1 if current tile
is required by vth VR user, the value of denominator is the
sum of previous successfully received tiles, which is required
by j to decode.

2) Cell-based and Cell-free Association: We adopt two
conventional network schemes to handle the association prob-
lem, which are cell-based (CB) and cell-free MIMO (CF)
associations: 1) In CB network, each AP is an individual cell,
where each AP makes its decision based on its observation
independently cooperation. Speciﬁcally, each AP is associated
with the largest VR user group Vj, which has its corresponding
vth UAV and jth tile (j ∈ J v) inside its observation.
This scheme may bring high inter-cell interference and poor
cell-edge performance; and 2) In the CF network, all APs
cooperatively receive one tile in every UAV-APs uplink stage
and broadcast one tile in the broadcast stage. In another word,
all APs are grouped in one virtual cell. In this scheme, the tile
with the highest priority among all tiles in all UAVs is selected
to be transmitted. This scheme provides high channel capacity
for transmitting UAV and corresponding VR users, resulting in
inefﬁcient time resource usage with geometry correlated VR
users, i.e. Du,v (cid:29) µMT.

With separated sequential scheduling and association sub-
problems, we ﬁrst design an intelligent association algorithm
working with the conventional P-PF scheduling method to
showcase the beneﬁts of adjusting the association dynamically.
the
By employing the deterministic scheduling algorithm,
original problem now degrades as a common MDP. This eases
our analysis.

In our considered scenario,

the geometry-correlated VR
users’ requests provide another degree-of-freedom in system
design. The association algorithm should spatially reuse the
frequency resource by dynamically grouping APs into virtual
cells, which improves the resource utilization and efﬁciency of
the system. The conventional approaches are simple and easy
to deploy, but their performances drop in certain scenarios
due to the lack of adjustment based on the environment. It
calls for an intelligent algorithm, which is capable of adjust-
ing association policy for the complex and high-dimensional
environment with hundreds of VR users. Among different
intelligent algorithms, reinforcement learning is shown to be
useful in solving communication problems, which are model-
free and shown to be useful in addressing POMDP problems
with a complex environment [28], [32].

To solve the association problem with reinforcement learn-
ing algorithms, we notice that the state information can’t be
fully observed. Both the future channel state information and
the precise information of the users’ positions are unavailable.
In this way, the problem has to be updated as a partially
observable MDP (POMDP). We then complete the deﬁnition
of our considered POMDP with the following deﬁnitions::

• The observation o (o ∈ O) only contains all nodes’
position and VR users’ tile request without including
the UAV and VR users’ channel state, due to that the
channels are largely dominated by the large-scale fading
under APs’ cooperative reception and transmission in CF-
MB network [13].

• The action a (a ∈ A) for the association is a one-hop
mapping from each AP to the tuple of serving UAV, tile
j, and corresponding VR user group Vj. As each AP has
|U| actions to choose, the size of action space of A can
be calculated as |A| = |U||B|, and the action at time t is
denoted as At.

• The reward Rt (Rt = r, r ∈ R) is the V-PSNR gain at

time t, designed as

(cid:88)

Rt =

∆V-PSNRv
t ,

(20)

where ∆V-PSNRv

v∈V
t = V-PSNRv

t − V-PSNRv

t−1.

A. Centralized Deep Reinforcement Learning

To solve our proposed POMDP problem with a reinforce-
ment learning algorithm, we start from a very basic centralized
approach to proving the effectiveness and set a baseline for the
learning approaches. Remind that in our considered CF-MB
network, the existence of the central server naturally facilities
the centralized approach, where a centralized agent is placed

8

layers: advantage and value stream, respectively. The advan-
tage stream measures how good the action will be compared to
the averaged V-PSNR. The value stream gives the expectation
of V-PSNR value from the current state. The output from both
streams is then aggregated as the policy [33].

Figure 3: Network Structure of Distributed Association Agent.

B. Rainbow Algorithm

Figure 4: The grid-based observation generated from environment state.

at the central server to make joint association decisions for
all APs dynamically and maximize the long-term V-PSNR.
Considering that the state space is too large and impossible
to be captured via a conventional table-based reinforcement
learning approach. The deep neural networks are introduced,
which effectively encode and represent the observation as a
low dimensional hidden vector by discovering the similarity
between them. With a small size hidden vector, the following
neural layers can effectively ﬁt the target value with small
network size.

1) Grid-based Observation and Neural Network Layers:
To further reduce the complexity of observation, we manually
degraded the input dimension via a grid-based observation,
which is a fuzzy representation of the state. For each pixel-
like grid, the geometry-correlated observed information inside
is summed and presented. As shown in Fig. 4, for each UAV,
we have three grid-maps, which correspond to the position of
UAVs, APs, and the VR user group requesting the currently
scheduled tiles (Vj, j ∈ Jt), respectively. The value in each
UAV and APs grid map is 1 if the node exists in that grid.
For the VR user group grid-map, the value in each grid is
the summation number of tile requests from the VR users in
that grid, which is normalized into the range of (0, 1] over the
maximum number of tiles’ requests in each grid. For example,
two VR users from 1th UAV locate in the same grid and
request 2 and 3 tiles from scheduled tiles Jt, the maximum
number of tile requests in grids is 8. Then, the normalized
value in that grid is 0.625.
To capture the spatial

information in grid observations
among UAV, AP, and VR users, we introduce convolutional
layers to encode the observation into a low dimensional vector.
The beneﬁt of applying convolutional layers in communication
problems has been shown by previous researches [15]. The
convolutional layers can easily learn to estimate the potential
signal and interference, as the convolutional operation matches
the signal and interference calculation formula. As shown in
Fig. 3, we design ﬁve layers of convolutional layers and one
linear layer to encode the observation into a hidden vector,
which is then processed by a duelling network. The duelling
network contains two streams composed of two noisy linear

However, the conventional neural network in reinforcement
learning is designed for static reward from the environment
[34]. Due to the random nature of the wireless environment
and the absence of a channel state,
the reward varies in
distribution form. The distributional DRL approach allows
the algorithm to adapt
to this case, which improves the
performance [33]. With distributional DRL, the value func-
tion’s distribution, which is denoted as z(s, o, a), is directly
estimated in distribution form. The value estimation d gen-
erated by the neural network for each action is a discrete
mapping from the actual value distribution z(s, o, a) to Natom
distributive value supports. We denote the distribution mapping
as d = (z, pθ(s, o, a)), with probability mass pi
θ(s, o, a) on ith
support. Then, the action with the highest expectation of the
estimated distribution is selected. The network parameters θ
are updated and optimized by minimizing the Kullbeck-Leibler
divergence between the estimated distribution (estimated by
neural network with parameter θ) and target distribution dt at
time t as [35]

n−1
(cid:88)

DKL(

k=0

Rt+k+1 + z, pθ(St+n, Ot+n, a(cid:48))||dt),

(21)

which measures the difference between forward-view n-step
distribution target and current distribution estimation dt at
time t, the a(cid:48) (a(cid:48) ∈ A(St+n)) is the action selected by the
policy and estimated distribution from neural network with
parameter θ and at time t + n. The loss is minimized with
categorical algorithm and gradient descent [36, Algorithm.
1]. We select the algorithm which provides the distributional
estimation capability as well as other stability improvement
tricks, such as double Q-learning, which is called rainbow
Algorithm. 1.

C. Networked-Markov Decision Process and Distributed
Multi-Agent Algorithm

It is important to note that the performance of a central-
ized learning approach is largely limited by the dimension
explosion problem caused by increasing serving area and
the number of participating APs, i.e. the action space grows
exponentially with the number of APs |A| = |U||B|. Besides,
the transmission, concatenation, and processing of large size
observation at the central server cause heavy backhaul over-
head.

To address this issue, we apply distributed reinforcement
learning, where each agent makes its own decision individu-
ally. We then further divide the association optimization target
Eq. (18) spatially and solves it via a homogeneous multi-
agent setting based on the mean-ﬁeld theorem [37], [38]. First,
the wireless signal fades with the increase of communication

Algorithm 1: Rainbow DRL based APs association.
input : An environment Env.

1 Initiate network parameters.
2 Initiate environment Env, state S0 and observation O0.
3 repeat
4

if Game end then

5

6

7

8

9

10

11

12

13

14

15

Obtain S0 from revising environment
Reset(Env) and set t = 0
if t can be divided by Tr/Tb then

t based on action At

Obtain network observation Ot and scheduled
tile set Jt for current time period from
Scheduling(St)
Select an action At greedily:
E[dt]
At = arg maxa∈A(St)
APs forms virtual cells Bu
Tile is transmitted from uth UAV to corresponding
VR users set V u via APs group Bu
t .
Env generates new state St+1
Calculate reward Rt for all VR users
Push tuple (Ot, At, Rt) to experience replay
Steps time period index t ← t + 1
Train the network parameters by minimising loss
deﬁned in (21) with a batch of memories
(Ot(cid:48), At(cid:48), Rt(cid:48), Ot(cid:48)+1) in experience replay
Perform a gradient descent for neural network

16
17 until Converge

distance, especially for our considered small APs. The far-side
UAVs and APs have limited impacts on the signal gain or
interference of the current AP’s surrounding area. Second, the
reward function is geometrically separable, and each AP can
obtain a precise part that is correlated to it in global reward.
Third, each AP always has a limited amount of neighbours,
which is far smaller than the overall number of agents. Third,
the number of correlated AP is small compared to the overall
AP number. Thus, as shown in Fig. 3, it is possible to let each
agent only capture the observation from surrounding areas
without losing any useful information, since the surrounding
area contains all information that correlates to the current AP.
The surrounding areas of different APs are partly overlapped
(cell-edge area). As such, the set of AP forms a network and
each AP only cares about itself and its neighbours.

With the above characteristics, we can formulate our as-
sociation problem as a networked decentralized partially ob-
servable Markov decision processes (ND-POMDP) problem
[39], which is a factored version of Decentralized-POMDP
problem with mean-ﬁeld theorem [37]. The local observation
set Ob now contains the local observation information sur-
rounding b-th agent. The joint action space can be denoted
as A = (cid:81)
b∈Bb Ab, where Ab is the set of local action
space of the bth AP. The reward for bth AP is denoted as
t (s, ab, a−b) = (cid:80)
v∈V b ∆V-PSNRv
Rb
t , where V b is the VR
users in bth AP’s observation range.

Then, the Bellman equation for b-th agent with state-action

function qb(s, ab) can be written as:
(cid:88)

(cid:88)

∆V-PSNRv
t

qb(s, ab) =

v∈Vj

j∈Jt
+ Es(cid:48)∈S [

(cid:88)

πa(a(cid:48)

b|s(cid:48), (a−b))qb(s(cid:48), a(cid:48)

b)].

9

(22)

a(cid:48)
b∈Ab

where the π−b
a present the joint policy of bth agent’s neighbors,
s(cid:48) is the state at t + 1, a−b presents bth agent’s neighbors’
action. As such, the size of the problem is largely reduced and
can be solved distributively. Each agent improves the V-PSNR
from surrounded VR users which also improve the overall V-
PSNR value. The DN-POMDP can be solved by common
reinforcement learning approaches and proved to converge
[40], as the environment is stationary with known neighbours’
policy. The algorithm is also shown to be converged with
averaged neighbours policy [37].

However, from Eq. (22), we can see that the optimization
target still depends on bth AP’s neighbours’ policy, which is
not controllable for the current AP. In some cases, sharing
actions among agents is undesirable due to latency or privacy
reasons. In our case, we consider the agent without
the
neighbours’ information. The environment is a non-stationary
environment from the perspective of any individual AP. Here,
we employ federated learning and Boltzmann policy to reduce
the variance of the learning. Federated learning has been
shown useful
in improving cooperative performance [39].
In our considered network, the optimization problem for all
agents can be seen as identical with a similar environment and
reward. Federated learning can improve the learning speed and
reduce the variance caused by unknown neighbours’ policy
[41]. Although the convergence of federated learning multi-
agent algorithm with neighbours’ actions is similar to param-
eter sharing of multi-agent learning and shown by many works
[42], but there is no strict proof of that without neighbours’
actions and only shown to be useful in practice. We apply
FL via federated average (FedAvg) algorithm which performs
averaging every Tfederated time intervals. The second trick we
used is Boltzmann policy. Greedy action selection is widely
used in reinforcement learning algorithm. However, in multi-
agent setting, the action with maximum value usually requires
other agents’ cooperation. This usually does not hold while all
agents selecting their action greedily [37]. Thus, the greedy
action selection ignores the need of potential cooperation
actions from neighbors, which can easily fail to converge.
Thus, we adopt
the Boltzmann policy to capture actions
with relatively small return, but potentially beneﬁt the overall
environment via effective cooperation. The Boltzmann policy
for bth AP in state s can be formulated as [37]

πb
a (ab|s, (a−b)) =

(cid:0) (cid:80)

exp (−βqb(s, ab))

exp (−βqb(s, ab))(cid:1) ,

(23)

ab∈Ab
where β is the temperature for Boltzmann policy, qb(s, ab) is
the estimation output of the network at ab action. To solve
the optimization problem for each separated sub-problem,
we again adopt a rainbow agent for the same reason as the
centralized learning approach. The algorithm is presented in
Algorithm. 2.

Algorithm 2: Hierarchical DRL based joint scheduling
and association.
input : An environment Env

1 Initiate scheduling network and association network

parameters.

2 Initiate environment Env, state S0, and scheduling

observation Os
0.

3 repeat
4

if Game end then

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

Reset Env and t = 0, obtain new S0, Os
0

t−N , As

t(cid:48)=t−N Rt(cid:48)) to

if t can be divided by Tr/Tb then
t−N , (cid:80)t
Store tuple (Os
scheduling experience replay
Calculate priority of scheduling As
Select Tr/Tb tiles with largest priority for each
UAV Jt = arg max(As
Train and update scheduling network’s
parameters with memories in experience
replay

t = E[ds
t]

t, Tr/Tb)

t from state St, scheduled tiles Jt and

for b ∈ B do
Obtain Ob
past state St−1
Select an action Ab
APs forms virtual cells Bu
their actions
Tiles are transmitted from uth UAV to
corresponding VR users V u via APs group Bu
t
for b ∈ B do

t for UAVs based on

t with (23)

t for VR users in bth AP’s

Calculate reward Rb
surrounding area
t , Ab
Push tuple (Ob
replay
Train and update association network’s
parameters following the same procedure as
Algorithm. 1.

t ) to bth AP’s experience

t, Rb

Step Env and generates St+1.
if t can be divided by Tfederated then
Perform FedAvg among APs B

23 until Converge

V. HIERARCHICAL LEARNING WITH LEARNING-BASED
SCHEDULER

After solving the association problem, we try to include
scheduling in an intelligent algorithm and investigate the
beneﬁt of the joint design of the scheduling and association.
Recap that the scheduling and association process is executed
successively, we deﬁne the joint problem as a semi-ND-
POMDP with scheduler as options of the association process’s
ND-POMDP. We then try to solve the problem with a hierar-
chical reinforcement learning algorithm.

The idea of hierarchical reinforcement learning is updating
two networks, one for scheduling policy and another for asso-
ciation policy. We update the scheduling policy with a ﬁxed
association policy, verse visa. Note that both scheduling and
association network shares the same environment reward in
different time scale. This guarantees monotonic improvement
for the environment V-PSNR. In the transmission procedures,

10

each AP ﬁrstly observe the tile requests and UAVs’, APs’,
and VR users’ positions from the environment. The agent of
the scheduler at a central server then generates its observation
based on the returned information from APs. The Jt tiles are
then scheduled for transmission during the next re-scheduling
slot. The agents constantly adjust their association decisions
based on their policy, and the AP capture the V-PSNR gain
to improve the scheduling and association policy. Recap that
the scheduling option part contains initial state, policy and
scheduling priority action set which is denoted by I, πs, As,
respectively. As such, following the optimization target in Eq.
(17), the Bellmann equation for scheduling part with option-
value function qs(s, as) can be written as:

t+N
(cid:88)

(cid:88)

(cid:88)

qs(s, as) =

∆V-PSNRv
k

+ Es(cid:48)∈S [

k=t
(cid:88)

j∈Jk

v∈Vj
s|s)qs(s(cid:48), a(cid:48)

πs(a(cid:48)

s)|Jk ∼ arg max ass(cid:48) ∈ I],

a(cid:48)
s ∈As

(24)
where option-value function qs(s, as) presents the expected
future reward, N is the number of system time period before
re-scheduling, as is the scheduling priority and the action of
scheduling part. Similar to the algorithm in Section IV, the
Bellman equation of bth AP’s association with scheduled tiles
Jt at time t can be rewritten as

qb(s, ab; Jt) =

(cid:88)

(cid:88)

∆V-PSNRv
t

v∈Vj

j∈Jt
+ Es(cid:48)∈S [

(cid:88)

πa(a(cid:48)

b|s(cid:48), (a−b))qb(s(cid:48), a(cid:48)

b; Jt+1)].

a(cid:48)
b∈Ab

(25)
The association policy maximizes the intrinsic reward, which
is the V-PSNR gain in each broadcast slot. The scheduling
policy maximizes the extrinsic reward, which is the potential
V-PSNR gain for transmitting scheduled tiles with a certain
association policy.

To capture the complex environment for scheduling and
association, as shown in Fig. 5, we follow the same association
optimization algorithm as the distributed DRL approach in
Section IV. For the scheduling part, we also employ the CNN-
based rainbow algorithm with a smaller reception ﬁeld than
the association part for the same reason. The tile requests are
naturally clustered as VR users always request continue tiles
inside their FoV. Scheduling the nearby tiles can potentially
beneﬁt
the V-PSNR by fully completing the tile request
from part of VR users ﬁrst. Due to the random nature of
the communication system, the V-PSNR gain of transmitting
groups of tiles is in distribution form.

With hundreds of tiles and corresponding requests,

the
observation of the scheduling part
is composited by the
popularity of tiles in 6 × 12 grid where the popularity locates
in the same position as they are in 360° view. The popularity
of VR users in different locations is then concatenated into a
joint popularity map. Apart from popularity, we also consider
the popularity of re-transmission tiles in the same formula.
Thus, each AP observes 6 × 12 × 2 tiles’ popularity for
each UAV from its surrounding state. The overall observation

11

Figure 5: Learning Architecture of Hierarchical Reinforcement Learning

Figure 6: Network Structure of Scheduler Agent in Center Server

is generated by concatenating each AP’s observation. Thus,
taking one example, the serving area is separated into 3 × 3
squares. Thus, there are 3 × 12 grid in the horizontal axis and
3 × 6 × 2 in the vertical axis for each UAV’s tiles’ request.
Then, as shown in Fig. 6, we apply a similar network structure
as the agent in Section IV. The |Jt| tiles with the highest
weight are scheduled and transmitted. The network makes an
association decision and transmits tiles. Then, the network is
updated with returned V-PSNR gain. The algorithm for the
hierarchical learning approach is represented in Algorithm.
2.

VI. SIMULATION RESULT

In this section, we examine the QoE of tile streaming
from UAV to VR users in our proposed CF-MB network
within a squared serving area. The parameter of our simulation
and learning system is given in Table. I1. In the following,
we present the V-PSNR performance for our proposed three
learning algorithms in Section VI-A and Section VI-B.

In the simulation, we set

the number of VR users as
the VR users are distributed following PCP,

|V| = 120,

1The authors acknowledge the use of the research computing facility at

King’s College London, Rosalind (https://rosalind.kcl.ac.uk).

whose cluster radius is set as rc = 20m, the number of
UAVs is |U| = 4. We set the number of AP as |B| = 9,
which are located in a 3 × 3 grid with 30 m gap inside
the serving area which is 80 m×80 m square. Each AP can
observe 60m × 60m squared area surrounding itself. The time
period of learning algorithms contains 10Tb, which means the
scheduling and association policy is updated after broadcasting
10 tiles. Note that for a centralized algorithm, due to the large
action space of our environment setting (|A| = 49), we can’t
train this oversize model with commercial computers. Thus,
we reduce the size of action space by only 2 UAV and half
broadcast slots of the current setting. We plot the performance
of a centralized algorithm just to show the effectiveness of
centralized learning in this scenario. Note that it does not
present the actual performance of centralized learning with
the full-size environment.

To ease the presentation of V-PSNR, we normalize the
resulting V-PSNR value into [0, 5] (5 frames in each GOP).
Note that,
the DRL algorithm is well-known for its lack
of reliability. Average performance is not sufﬁcient to de-
scribe the performance of the algorithm. To show the risk
of our algorithm [43], we use a standard derivative (SD)
error bar to show the performance. We present +std, aver-
age performance, and -std, V-PSNR value over 105 random
GOPs with independently generated UAV and VR users. For
each algorithm setting, we train 6 × 104 epochs and pick
the best model during training to plot
the result. In the
following, we use ”Centralized(Reduced)”, ”Distributed DRL
w/ FL”, and ”Hierarchical w/ FL” to denote the centralized
DRL association algorithm with P-PF scheduler, federated
distributed DRL algorithm with P-PF scheduler, a hierarchical
algorithm with federated distributed DRL and learning-based
scheduler algorithm, respectively. To show the effectiveness
of FL, we compare two more algorithms: Distributed DRL

Channel parameters
AP-VR link path-loss exponent (AP-VR) αDL
VR center frequency
Accesspoint grid length
User density
Accesspoint EIRP
Accesspoint transmission bandwidth
Noise power θ2
Video parameter
Frame rate
Group of picture
Pixel per degree
Video compression rate
Frame size ratio (P/I)
User ﬁeld-of-view
Tile size
Number of re-schedule between frames Tf
Broadcast slots between re-schedule Tr

Channel parameters
UAV-AP link path-loss exponent (UAV-AP) αUL
UAV center frequency
Drone hovering height
Excessive NLoS Attenuation
UAV EIRP
UAV transmission bandwidth
Number of UAV
Learning parameters
Temperature (β)
Learning rate
Dropout rate
Batch size
Atoms (Association)

Setting
4
5.5 GHz
30 m
100
48 dBm
5 MHz
−91 dBm
Setting
90 Hz
5 (IPPPP)
60
150
0.7
210° × 150° Atoms (Scheduler)
30° × 30°
28
10

Noisy layer std
Discount γ
Multi-step learning

12

Setting
2
4.5 GHz
30 m
20 dB
48 dBm
5 MHz
2
Setting
100
6.25 × 10−5
0.2
32
21
11
0.5
1
3

Table I: Environment and Learning Parameters.

dynamic policies as part of the environment. Reinforcement
learning does not have a convergence guarantee for a non-
stationary environment. Federated learning can reduce the
variance of the environment [41]. In the Hierarchical w/ FL
approach, the agent of scheduler acts as a meta-controller, who
helps the distributed DRL agents to cooperate. Fig. 8 plots
the overall V-PSNR values of different learning algorithms.
we observe that the average V-PSNR of algorithms follows:
Hierarchical w/ FL ≈ Distributed DRL w/ FL > CB ≈ CF >
Hierarchical w/o FL > Distributed DRL w/o FL.

In Fig. 9, we show the generated hidden vectors of our
proposed neural network by visualising the output of CNN
and the ﬁnal policy. The ﬁgure is generated with 104 randomly
generated environment examples. With generated vectors from
the output of CNN layers, we apply a technique developed
for the visualization of high-dimensional data called “t-SNE”
to calculate the distance between vectors. Then, the principle
composition analyses (PCA) is performed on the vectors to
reduce the dimension to 2D space and visualise them in Fig. 9.
Each point is coloured according to the association decisions
[44]. In this way, the point cloud presents how the neural
network recognises the environment and makes decisions.

In Fig. 9, we also randomly present four pairs of ob-
servations together with their most similar observations by
picking the nearest one according to the result of the t-SNE
algorithm. Each observation is observed by the AP in the
center, which is represented as a grid-map. In each grid-map,
the colours in grids represent the position of VR users and their
corresponding UAV. The position of APs is also marked and
coloured by its association decision from current observation.
To ease the reading of these ﬁgures, we number the 9 APs in
our simulation with numbers from 1−9 based on their relative
positions. In (a), the grid-maps are observed by 2nd AP. In left
grid-map, we can see that 2nd and 5th AP is jointly associated
to serve 2nd UAV. In right grid-map, 2nd, 5th APs jointly serve
0th UAV. In (b), the 4th APs forms virtual cells with 7th and
8th APs cooperatively to serve 0th UAV in the left grid-map.
In right grid-map, 4th and 1st APs jointly serve 3th UAVs.
In (c), the 1st and 2nd APs jointly serve the surrounding VR

(a)

(b)

Figure 7: Convergence curves for our proposed algorithms.

Figure 8: V-PSNR Performance of proposing algorithms.

without FL and Hierarchical FL without FL. For simplicity,
we use ”Distributed DRL w/o FL” and ”Hierarchical FL w/o
FL”, respectively.

A. Overall Convergence and Policy Visualization

Fig. 7 plots the overall V-PSNR versus the training epochs.
In Fig. 7a, we observe that the Centralized(Reduced), Dis-
tributed DRL w/ FL and Hierarchical w/ FL converge fast
within 10,000 epochs. Because the FL method combines
the knowledge among APs. In Fig. 7b, we observe that the
Distributed DRL w/o FL fails to converge, whereas the Hier-
archical w/o FL approach does converge but slower. Because
the Distributed DRL w/o FL approach treats other agents with

13

Figure 9: t-SNE embedding of the representations with the learned policy.

user groups, which request tiles from 3rd UAV. In (d), the left
observation from 8th AP shows that it fails to cooperate with
9th AP to serve 2nd UAV and corresponding VR user group.
This highlights the fact that the value of the actions in each
agent is jointly decided by its and its neighbours’ actions in
the multi-agent system. The right observation has shown that
6th, 8th, and 9th APs jointly serve 0th UAV, whereas the 4th
and 7th APs jointly serve 2th UAV.

serve one UAV and corresponding VR user group in the CF
association. The CF-MB network provides uniform services
in this case, which lacks geographical awareness and won’t
work in a large-scale network. We observe that the V-PSNR
of the CB association algorithm drops dramatically with the
increasing cluster radius. The reason is that the increasing
cluster radius can lead to more overlap clusters, high inter-
cell interference and poor cell-edge performance.

B. Quality-of-experience Analysis

In this subsection, we plot

the V-PSNR value using
including Central-
VR users of three learning algorithms,
ized(Reduced), Distributed DRL w/ FL, and Hierarchical w/
FL, together with two conventional algorithms (CB, CF) in
different scenarios. We show the generalization and effective-
ness of our proposing algorithms.

Fig. 10 plots the V-PSNR value versus the number of VR
users. We observe that all algorithms’ V-PSNR stay nearly
unchanged with increasing numbers of VR users in CF-MB
network. This matches our expectation for CF-MB network,
where the UAV-APs cooperative reception enhance the re-
ceived signal from the UAV and the APs-VR broadcasting is
not sensitive to the number of receiving VR users. It is worth
mentioning that we only train a single model using random
VR users and obtain similar results with different numbers of
VR users setting.

Fig. 11 plots the V-PSNR value versus the VR users’ cluster
radius. We observe that the V-PSNR value of our proposed
distributed algorithms, including Distributed DRL w FL and
Hierarchical w FL, drop slightly with the increasing cluster
radius, but outperform other algorithms (CF, CB). Because
the reuse of frequency resources spatially improves the per-
formance. The V-PSNR value of CF association algorithm
keeps the same for different cluster radius, as all APs jointly

Fig. 12 plots the V-PSNR versus the number of broadcast
slots, which also reveals the slot utilization of our proposed
algorithms. Remind that in our considered environment, 4
UAV holds 288 tiles in total. If we set large Tb and fewer
broadcast slots, then two tiles should be fully transmitted
successfully within one broadcast slot (160 slots). If we set
small Tb (more broadcast slots), each tile can occupy one
broadcast slot individually (320 slots). We observe that the V-
PSNR of CF association method increases with the number
of broadcast slots, as no interference in each slot. The V-
PSNR value of CB approaches decrease with the increase of
broadcast slots with lower per-slot utilization. By dynamically
arranging the association policy, our proposed algorithm can
always achieve the maximum slot utility among different
approaches.

Fig. 13 plot the V-PSNR value of different algorithms versus
the increasing number of UAVs. We observe that the V-PSNR
of CF and CB decreases with the increasing number of UAVs
due to the lack of resources. We can see that the learning-
based algorithms still outperform conventional methods. It
achieves high utilization for each broadcast slot with increased
UAV number in both average and standard derivation of
V-PSNR. It should be noted that
the training complexity
of learning algorithms increases linearly with the increasing
number of UAV in our network design, which becomes the
most important factor limiting the scalability of our algorithm.

14

Figure 10: V-PSNR of our proposing algorithms with different number of VR users.

Figure 12: V-PSNR of our proposing algorithms with different broadcast slots in each
frame duration.

Figure 11: V-PSNR of our proposing algorithms with different VR users’ cluster radius.

Figure 13: V-PSNR of our proposing algorithms with different number of UAVs.

VII. CONCLUSION
In this paper, we introduced a cell-free multi-group broad-
cast network for real-time VR video transmission from UAVs
to VR users for experience enhancement in a sports event.
To optimise the quality-of-experience of VR users with de-
pendent decoded video resources and correlated VR users, we
highlighted the importance of scheduling video tiles and the
dynamical association of APs. We have also shown that a joint
design is needed for correlated and sequential scheduling and
association procedures. To explore the learning-based dynamic
association algorithm, we propose a centralized and multi-
agent deep reinforcement learning algorithm, which captures
the environment via convolutional
layers. To jointly solve
the coupled association and scheduling algorithm, we further
developed a hierarchical algorithm with scheduler as meta-
controller and association algorithm as the controller. Our
results demonstrated that both distributed APs and hierarchical
with federated learning algorithms can effectively handle a
large number of APs and VR users and outperform the
centralized algorithm and non-learning-based approach with
decent scalability.

REFERENCES

[1] Intel,

“Intel® True

2020.
[Online]. Available: https://www.intel.co.uk/content/www/uk/en/sports/
technology/true-view.html

Sports,”

View

Intel

in

-

[2] F. Hu, Y. Deng, W. Saad, M. Bennis, and A. H. Aghvami, “Cellular-
Connected Wireless Virtual Reality: Requirements, Challenges, and
Solutions,” IEEE Commun. Mag., vol. 58, pp. 105–111, May 2020.
[3] Qualcomm Technologies. Inc., “VR and AR Pushing Connectivity
Limits,” Qualcomm., Tech. Rep., 2018. [Online]. Available: https:
//www.qualcomm.com/invention/extended-reality/virtual-reality

[4] Z. Chen, E. Bjornson, and E. G. Larsson, “Dynamic Resource Allocation
in Co-Located and Cell-Free Massive MIMO,” in IEEE Trans. Green
Commun. Netw., vol. 4, no. 1.
Institute of Electrical and Electronics
Engineers Inc., Mar. 2020, pp. 209–220.

[5] M. Chen, W. Saad, and C. Yin, “Deep Learning for 360° Content
Transmission in UAV-Enabled Virtual Reality,” in 53rd ICC 2019,
Shanghai (China), May 2019, pp. 1–6.

[6] X. Yang, Z. Chen, K. Li, Y. Sun, N. Liu, W. Xie, and Y. Zhao,
“Communication-constrained mobile edge computing systems for wire-
less virtual reality: Scheduling and tradeoff,” IEEE Access, vol. 6, pp.
16 665–16 677, Mar. 2018.

[7] S. Sukhmani, M. Sadeghi, M. Erol-Kantarci, and A. E. Saddik, “Edge
Caching and Computing in 5G for Mobile AR/VR and Tactile Internet,”
IEEE Multimed., pp. 1–1, Nov. 2018.

[8] M. Chen, W. Saad, C. Yin, and M. Debbah, “Data Correlation-Aware
Resource Management in Wireless Virtual Reality (VR): An Echo State
Transfer Learning Approach,” IEEE Trans. Commun., Feb. 2019.
[9] X. Hou, J. Zhang, M. Budagavi, and S. Dey, “Head and body motion
prediction to enable mobile vr experiences with low latency,” in 38th
GLOBECOM 2019, Waikoloa, Dec. 2019, pp. 1–7.

[10] C. Perfecto, M. S. Elbamby, J. D. Ser, and M. Bennis, “Taming the
Latency in Multi-user VR 360: A QoE-aware Deep Learning-aided
Multicast Framework,” IEEE Trans. Commun., vol. 68, no. 4, Apr.
2020. [Online]. Available: http://arxiv.org/abs/1811.07388

[11] 3GPP, “3GPP TS 26.247 Progressive Download and Dynamic Adaptive

Streaming over HTTP (3GP-DASH),” 2018.

[12] X. Ge, L. Pan, Q. Li, G. Mao, and S. Tu, “Multipath Cooperative
Communications Networks for Augmented and Virtual Reality Trans-
mission,” IEEE Trans. Multimed., vol. 19, no. 10, pp. 2345–2358, Jul.
2017.

[13] H. Q. Ngo, A. Ashikhmin, H. Yang, E. G. Larsson, and T. L. Marzetta,
“Cell-Free Massive MIMO Versus Small Cells,” IEEE Trans. Wirel.
Commun., vol. 16, no. 3, pp. 1834–1850, Jan. 2017.

[14] S. Buzzi and C. D’Andrea, “Cell-free massive MIMO: User-centric
approach,” IEEE Wireless Commun. Lett., vol. 6, no. 6, pp. 706–709,
Dec. 2017.

[15] W. Cui, K. Shen, and W. Yu, “Spatial Deep Learning for Wireless

Scheduling,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1248–
1261, Jun. 2019.

theory,” arXiv preprint arXiv:2003.04451, 2020. [Online]. Available:
http://arxiv.org/abs/2003.04451

15

[39] N. Ranjit, V. Pradeep, T. Milind, and Y. Makoto, “Networked
distributed POMDPs: a synthesis of distributed constraint optimization
and POMDPs,” in AAAI’05, Pittsburgh, Pennsylvania, USA, 2005, pp.
133–139. [Online]. Available: https://www.aaai.org/Papers/AAAI/2005/
AAAI05-022.pdf

[40] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-
Agent Actor-Critic for Mixed Cooperative-Competitive Environments,”
Jun. 2017.

[41] Y. S. Nasir and D. Guo, “Multi-Agent Deep Reinforcement Learning
for Dynamic Power Allocation in Wireless Networks,” IEEE Journal
on Selected Areas in Communications, vol. 37, no. 10, pp. 2239–2250,
Aug. 2019.

[42] Y. Al-Eryani, M. Akrout, and E. Hossain, “Multiple Access in Cell-
Free Networks: Outage Performance, Dynamic Clustering, and Deep
Reinforcement Learning-Based Design,” IEEE Journal on Selected
Areas in Communications, vol. 39, no. 4, pp. 1028–1042, Aug. 2021.

[43] S. C. Y. Chan, S. Fishman, J. Canny, A. Korattikara, and S. Guadarrama,
“Measuring the Reliability of Reinforcement Learning Algorithms,”
arXiv preprint arXiv:1912.05663, Dec. 2019.
[Online]. Available:
http://arxiv.org/abs/1912.05663

[44] V. Mnih, K. Kavukcuoglu, D. Silver,

“Human-level
Control Through Deep Reinforcement Learning,” Nature, vol. 518,
no. 7540, pp. 529–533, Feb. 2015.
[Online]. Available: https:
//www.nature.com/articles/nature14236

and etc,

[16] C. Saha, M. Afshang, and H. S. Dhillon, “Poisson cluster process:
Bridging the gap between PPP and 3GPP HetNet models,” in 2017 ITA.
San Diego: IEEE, Feb. 2017, pp. 1–9.

[17] B. Shang and L. Liu, “Machine Learning Meets Point Process: Spatial
Spectrum Sensing in User-Centric Networks,” IEEE Wireless Commu-
nications Letters, vol. 9, no. 1, pp. 34–37, Sep. 2020.

[18] A. Karimi, K. I. Pedersen, N. H. Mahmood, J. Steiner, and P. Mogensen,
“5G Centralized Multi-Cell Scheduling for URLLC: Algorithms and
System-Level Performance,” IEEE Access, vol. 6, pp. 72 253–72 262,
Nov. 2018.

[19] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Mobile Unmanned
Aerial Vehicles (UAVs) for Energy-Efﬁcient Internet of Things Commu-
nications,” IEEE Trans. Wirel. Commun., vol. 1, no. 11, pp. 7574–7589,
Nov. 2017.

[20] ——, “Unmanned Aerial Vehicle with Underlaid Device-to-Device
Communications: Performance and Tradeoffs,” IEEE Trans. Wirel. Com-
mun., vol. 15, no. 6, pp. 3949–3963, Feb. 2016.

[21] N. Jindal, J. G. Andrews, and S. Weber, “Multi-antenna Communication
in Ad Hoc Networks: Achieving MIMO Gains with SIMO Transmis-
sion,” IEEE Trans. Commun., vol. 59, no. 2, pp. 529–540, Dec. 2011.
[22] J. Joung, H. D. Nguyen, P. H. Tan, and S. Sun, “Multicast linear
precoding for MIMO-OFDM systems,” IEEE Commun. Lett., vol. 19,
no. 6, pp. 993–996, Apr. 2015.

[23] M. Zink, R. Sitaraman, and K. Nahrstedt, “Scalable 360° Video
Stream Delivery: Challenges, Solutions, and Opportunities,” Proc.
IEEE, vol. 107, no. 4, pp. 639–650, Apr. 2019. [Online]. Available:
https://ieeexplore.ieee.org/document/8643410/

[24] A. Xu, X. Chen, Y. Liu, and Y. Wang, “A Flexible Viewport-Adaptive
Processing Mechanism for Real-Time VR Video Transmission,” in 2019
IEEE Int. Conf. Multimed. Expo Work., Shanghai (China), Aug. 2019,
pp. 336–341.

[25] ITU, “H.265: High efﬁciency video coding,” 2020. [Online]. Available:

https://www.itu.int/rec/T-REC-H.265

[26] J. Flordelis, F. Rusek, F. Tufvesson, E. G. Larsson, and O. Edfors, “Mas-
sive MIMO Performance—TDD versus FDD: What Do Measurements
Say?” IEEE Trans. Wirel. Commun., vol. 17, no. 4, pp. 2247–2261, Feb.
2018.

[27] C. Li, M. Xu, L. Jiang, S. Zhang, and X. Tao, “Viewport Proposal CNN
for 360° Video Quality Assessment,” in IEEE Conf. Comput. Vis. Pattern
Recognit., Long Beach, CA, Jun. 2019.

[28] H. Peng and X. Shen, “Multi-Agent Reinforcement Learning Based Re-
source Management in MEC- and UAV-Assisted Vehicular Networks,”
IEEE Journal on Selected Areas in Communications, vol. 39, no. 1, pp.
131–141, Nov. 2021.

[29] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and semi-
MDPs: A framework for temporal abstraction in reinforcement learning,”
Artiﬁcial Intelligence, vol. 112, no. 1, pp. 181–211, Dec. 1999.
[30] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, “Hierar-
chical Deep Reinforcement Learning: Integrating Temporal Abstraction
and Intrinsic Motivation,” in Advances in Neural Information Processing
Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, Eds. Curran Associates, Inc., 2016, pp. 3675–3683.
[31] R. Margolies, A. Sridharan, V. Aggarwal, R. Jana, N. K. Shankara-
narayanan, V. A. Vaishampayan, and G. Zussman, “Exploiting Mobility
in Proportional Fair Cellular Scheduling: Measurements and Algo-
rithms,” IEEE/ACM Trans. Netw., vol. 24, no. 1, pp. 355–367, Feb. 2016.
[32] H. Shiri, J. Park, and M. Bennis, “Communication-Efﬁcient Massive
UAV Online Path Control: Federated Learning Meets Mean-Field Game
Theory,” Mar. 2020.

[33] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski,
W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow:
Combining Improvements in Deep Reinforcement Learning,” in 32nd
AAAI 2018. AAAI press, Oct. 2018, pp. 3215–3222.

[34] T. Jaakkola, S. P. Singh, and M. I. Jordan, “Reinforcement learning
algorithm for partially observable Markov decision problems,” in Adv.
Neural Inf. Process. Syst., 1995, pp. 345–352.

[35] M. G. Bellemare, W. Dabney, and R. Munos, “A Distributional Perspec-

tive on Reinforcement Learning,” Jul. 2017.

[36] F. Meire, G. A. Mohammad, P. Bilal, and etc, “Noisy networks for

exploration,” in ICLR 2018, Vancouver (Canada), 2018.

[37] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean Field

Multi-Agent Reinforcement Learning,” Feb. 2018.

[38] H. Shiri, J. Park, and M. Bennis, “Communication-efﬁcient massive
UAV online path control: Federated learning meets mean-ﬁeld game

