2
2
0
2

g
u
A
7
2

]

C
O
.
h
t
a
m

[

3
v
3
7
3
4
0
.
9
0
0
2
:
v
i
X
r
a

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration for
Strongly Convex Decentralized Optimization

Huan Li 1 Zhouchen Lin 2 Yongchun Fang 1

Abstract

(cid:15) ) stochastic gradient evaluations and O((κb + κc) log 1

We study stochastic decentralized optimization for the problem of training machine learning models with large-
scale distributed data. We extend the widely used EXTRA and DIGing methods with variance reduction (VR),
and propose two methods: VR-EXTRA and VR-DIGing. The proposed VR-EXTRA requires the time of
O((κs + n) log 1
(cid:15) ) communication rounds to reach
precision (cid:15), which are the best complexities among the non-accelerated gradient-type methods, where κs and
κb are the stochastic condition number and batch condition number for strongly convex and smooth problems,
respectively, κc is the condition number of the communication network, and n is the sample size on each distributed
node. The proposed VR-DIGing has a little higher communication cost of O((κb + κ2
(cid:15) ). Our stochastic
gradient computation complexities are the same as the ones of single-machine VR methods, such as SAG, SAGA,
and SVRG, and our communication complexities keep the same as those of EXTRA and DIGing, respectively. To
further speed up the convergence, we also propose the accelerated VR-EXTRA and VR-DIGing with both the
optimal O((
(cid:15) ) communication
(cid:15) ) stochastic gradient computation complexity and O(
complexity. Our stochastic gradient computation complexity is also the same as the ones of single-machine
accelerated VR methods, such as Katyusha, and our communication complexity keeps the same as those of
accelerated full batch decentralized methods, such as MSDA. To the best of our knowledge, our accelerated
methods are the ﬁrst to achieve both the optimal stochastic gradient computation complexity and communication
complexity in the class of gradient-type methods.

nκs + n) log 1

κbκc log 1

c) log 1

√

√

1. Introduction

Emerging machine learning applications involve huge amounts of data samples, and the data are often distributed across
multiple machines for storage and computational reasons. In this paper, we consider the following distributed convex
optimization problem with m nodes, and each node has n local training samples:

min
x∈Rp

m
(cid:88)

i=1

f(i)(x), where

f(i)(x) =

1
n

n
(cid:88)

j=1

f(i),j(x),

(1)

where the local component function f(i),j represents the jth sample of node i, and it is not accessible by any other node
in the communication network. The network is abstracted as a connected and undirected graph G = (V, E), where
V = {1, 2, ..., m} is the set of nodes, and E ⊂ V × V is the set of edges. Nodes i and j can send information to each other
if and only if (i, j) ∈ E. The goal of the networked nodes is to cooperatively solve problem (1) via local computation
and communication, that is, each node i makes its decision only based on the local computations on f(i), for example, the
gradient, and the local information received from its neighbors in the network.

When the local data size n is large, the cost of computing the full batch gradient ∇f(i) at each iteration is expensive. To
address the issue of large-scale distributed data, stochastic decentralized algorithms are often used to solve problem (1),

1Institute of Robotics and Automatic Information Systems, College of Artiﬁcial Intelligence, Nankai University, Tianjin, China.
2Key Laboratory of Machine Perception, School of Artiﬁcial Intelligence, Peking University, Beijing, China. Institute for Artiﬁcial
Intelligence, Peking University, Beijing, China. Peng Cheng Laboratory, Shenzhen, China.
. Correspondence to: Huan Li and Zhouchen Lin <lihuanss@nankai.edu.cn, zlin@pku.edu.cn>.

 
 
 
 
 
 
Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

where each node only randomly samples one component gradient at each iteration (extendable to the mini-batch settings
with more than one randomly selected component). Most decentralized algorithms alternate between computations and
communications. Thus to compare the performance of such methods, two measures are used: the number of communication
rounds and the number of stochastic gradient evaluations, where one communication round allows each node to send
information to their neighbors, for example, O(1) vectors of size p, and one stochastic gradient evaluation refers to
computing the randomly sampled ∇f(i),j for all i ∈ V in parallel (Kovalev et al., 2020b).

Although stochastic decentralized optimization has been a hot topic in recent years, and several algorithms have been
proposed, to the best of our knowledge, in the class of algorithms not relying on the expensive dual gradient evaluations,
there is no algorithm optimal in both the number of communication rounds and the number of stochastic gradient evaluations
(Kovalev et al., 2020b), where “optimal” means matching the corresponding lower bounds. In this paper, we extend two
widely used decentralized algorithms of EXTRA (Shi et al., 2015) and DIGing (Nedi´c et al., 2017; Qu & Li, 2018), which
have sparked a lot of interest in the distributed optimization community, to stochastic decentralized optimization by combin-
ing them with the powerful variance reduction technique. Furthermore, we propose two accelerated stochastic decentralized
algorithms, which are optimal in the above two measures of communications and stochastic gradient computations.

1.1. Notations and Assumptions

Denote x(i) ∈ Rp to be the local variable for node i. To simplify the algorithm description in a compact form, we introduce
the aggregate objective function f (x) with its aggregate variable x and aggregate gradient ∇f (x) as

x =




 ,






xT
(1)
...
xT

(m)

f (x) =

m
(cid:88)

i=1

f(i)(x(i)), ∇f (x) =






∇f(1)(x(1))T
...
∇f(m)(x(m))T




 .

(2)

Denote x∗ to be the optimal solution of problem (1), and let x∗ = 1(x∗)T , where 1 is the column vector of m ones. Denote
I as the identity matrix, and N(i) as the neighborhood of node i. Denote Ker(U ) = {x ∈ Rm|U x = 0} as the kernel space
of matrix U ∈ Rm×m, and Span(U ) = {y ∈ Rm|y = U x, ∀x ∈ Rm} as the linear span of all the columns of U . For
matrices, we denote (cid:107) · (cid:107) as the Frobenius norm for simplicity without ambiguity, since it is the only matrix norm we use in
this paper. The notation A (cid:23) B means A − B is positive semideﬁnite.

We make the following assumptions for the functions in (1).

Assumption 1 Each f(i)(x) is L(i)-smooth and µ-strongly convex. Each f(i),j(x) is L(i),j-smooth and convex.

We say a function g(x) is L-smooth if its gradient satisﬁes (cid:107)∇g(y) − ∇g(x)(cid:107) ≤ L(cid:107)y − x(cid:107). Motivated by (Hendrikx et al.,
2021; 2020), we deﬁne several notations as follows:

Lf = max

i

L(i), L(i) =

1
n

n
(cid:88)

j=1

L(i),j, Lf = max

i

L(i),

κs =

Lf
µ

,

κb =

Lf
µ

.

Then f (x) is also µ-strongly convex and Lf -smooth. It always holds that L(i) ≤ L(i) ≤ nL(i)

1, which further gives

Lf ≤ Lf ≤ nLf

and κb ≤ κs ≤ nκb.

(3)

(4)

We follow (Hendrikx et al., 2021) to call κb the batch condition number, and κs the stochastic condition number, which
are classical quantities in the analysis of batch optimization methods and ﬁnite-sum optimization methods, respectively.
Generally, we have κs (cid:28) nκb, see (Allen-Zhu, 2018) for the example and analysis.

In decentralized optimization, communication is often represented as a matrix multiplication with a weight matrix W ∈
Rm×m. We make the following assumptions for this weight matrix associated to the network2.

Assumption 2

1See footnote 14 in (Allen-Zhu, 2018) for the analysis.
2The weights can be assigned heuristically or optimized given the ﬁxed graph structure (Boyd et al., 2004).

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

1. Wi,j (cid:54)= 0 if and only if agents i and j are neighbors or i = j. Otherwise, Wi,j = 0.

2. W = W T , I (cid:23) W (cid:23) ωI, and W 1 = 1.

√
2
2 for DIGing. We can relax ω to be any small positive constant for DIGing3, and ﬁx it
We let ω = 0 for EXTRA, and ω =
√
2
2 to simplify the analysis. For EXTRA, we can also relax the condition to I (cid:23) W (cid:23) (−1 + δ)I for any small positive
to
constant δ4. Part 2 of Assumption 2 implies that the eigenvalues of W lie in [ω, 1], and its largest one σ1(W ) equals 1.
Moreover, if the network is connected, we have σ2(W ) < 1, where σ2(W ) means the second largest eigenvalue. We often
use

κc =

1
1 − σ2(W )

(5)

as the condition number of the communication network, which upper bounds the ratio between the largest eigenvalue and
the smallest non-zero eigenvalue of (I − W ), which is a gossip matrix (Scaman et al., 2017).

As will be introduced in the next section, we often use κb and κc to describe the number of communication rounds, and κs
for the number of stochastic gradient evaluations in stochastic decentralized optimization.

1.2. Literature Review

In this section, we give a brief review for the decentralized and stochastic methods, as well as their combination. Table 1
sums up the complexities of the representative ones.

1.2.1. FULL BATCH DECENTRALIZED ALGORITHMS

Distributed optimization has gained signiﬁcant attention for a long time (Bertsekas, 1983; Tsitsiklis et al., 1986). The
modern distributed gradient descent (DGD) was proposed in (Nedi´c & Ozdaglar, 2009) for the general network topology,
and was further extended in (Nedi´c, 2011; Ram et al., 2010; Yuan et al., 2016). These algorithms are usually slow due
to the diminishing step-size, and suffer from the sublinear convergence even for strongly convex and smooth objectives.
To avoid the diminishing step-size and speed up the convergence, several methods relying on tracking the differences of
gradients have been proposed. Typical examples include EXTRA (Shi et al., 2015), DIGing (Nedi´c et al., 2017; Qu & Li,
2018), NIDS (Li et al., 2019), and other similar algorithms (Xu et al., 2015; Xin et al., 2018). Especially, EXTRA (Li & Lin,
2020) and NIDS (Li et al., 2019) have the O((κb + κc) log 1
(cid:15) ) complexity both in communications and full batch gradient
evaluations to solve problem (1) to reach precision (cid:15), which is the best among the non-accelerated algorithms. DIGing has a
slight higher complexity of O((κb + κ2
(cid:15) ) (Alghunaim et al., 2021). Another typical class of distributed algorithms is
based on the Lagrangian function, and they work with the Fenchel dual. Examples include the dual ascent (Terelius et al.,
2011; Scaman et al., 2017; Uribe et al., 2020), ADMM (Iutzeler et al., 2016; Makhdoumi & Ozdaglar, 2017; Aybat et al.,
2018), and the primal-dual method (Lan et al., 2020; Scaman et al., 2018; Hong et al., 2017; Jakoveti´c, 2019). However, the
dual-based methods often need to compute the gradient of the Fenchel conjugate of the local functions, called dual gradient
in the sequel, which is expensive.

c) log 1

Nesterov’s acceleration technique is an efﬁcient approach to speed up the convergence of ﬁrst-order methods, and it has also
been successfully applied to decentralized optimization. Typical examples include the distributed Nesterov gradient with
consensus (Jakoveti´c et al., 2014), the distributed Nesterov gradient descent (Qu & Li, 2020), the multi-step dual accelerated
method (MSDA) (Scaman et al., 2017; 2019), accelerated penalty method (Li et al., 2020b), accelerated EXTRA (Li &
Lin, 2020), and the accelerated proximal alternating predictor-corrector method (APAPC) (Kovalev et al., 2020b). Some
of these methods have suboptimal computation complexity, and Chebyshev acceleration (CA) (Arioli & Scott, 2014) is a
(cid:15) ) lower
powerful technique to further reduce the computation cost. Scaman et al. (2017; 2019) proved the Ω(
bound on the number of communication rounds and the Ω(
(cid:15) ) lower bound on the number of full batch gradient
evaluations, which means that any ﬁrst-order full batch decentralized methods cannot be faster than these bounds. The
MSDA and APAPC methods with CA achieve these lower bounds.

κbκc log 1

κb log 1

√

√

O( 1

3In this case, condition (15) is relaxed to (cid:107)V x(cid:107)2 ≤ (1 − ω2)(cid:107)x(cid:107)2. By the similar proofs of Theorem 1, we can obtain the
ω2 (κb + κ2
4In this case, the complexity of EXTRA becomes O( 1

(cid:15) ) complexity for DIGing.

c) log 1

δ (κb + κc) log 1

(cid:15) ).

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

1.2.2. STOCHASTIC ALGORITHMS ON A SINGLE MACHINE

Stochastic gradient descent (SGD) has been the workhorse in machine learning. However, since the variance of the
noisy gradient will not go to zero, SGD often suffers from the slow sublinear convergence. Variance reduction (VR) was
designed to reduce the negative effect of the noise, which can improve the stochastic gradient computation complexity to
(cid:15) ). On the other hand, full batch methods, such as gradient descent, require O(κb log 1
O((κs + n) log 1
(cid:15) ) iterations, and
thus O(nκb log 1
(cid:15) ) individual gradient evaluations for ﬁnite-sum problems with n samples, which may be much larger than
O((n + κs) log 1
(cid:15) ) when κs (cid:28) nκb. Representative examples of VR methods include SAG (Schmidt et al., 2017), SAGA
(Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014). We can further accelerate the VR methods
to the O((
(cid:15) ) stochastic gradient computation complexity by Nesterov’s acceleration technique. Examples
include Katyusha (Allen-Zhu, 2018) and its extensions in (Zhou et al., 2019; Kovalev et al., 2020a). Other accelerated
stochastic algorithms can be found in (Lan & Zhou, 2018; Lin et al., 2018; Fercoq & Richt´arik, 2015; Lin et al., 2015). Lan
& Zhou (2018) proved the Ω((
(cid:15) ) lower bound for strongly convex and smooth stochastic optimization, and
Katyusha achieves this lower bound.

nκs + n) log 1

nκs + n) log 1

√

√

1.2.3. STOCHASTIC DECENTRALIZED ALGORITHMS

√

κbκc log 1

To address the issue of large-scale distributed data, Chen & Sayed (2012) and Ram et al. (2010) extended the DGD method
to the distributed stochastic gradient descent (DSGD). To further improve the convergence of stochastic decentralized
algorithms, Pu & Nedi´c (2021) combined DSGD with gradient tracking, Mokhtari & Ribeiro (2016) combined EXTRA
with SAGA, and proposed the decentralized double stochastic averaging gradient algorithm, Xin et al. (2020b) combined
gradient tracking with the VR technique, and two algorithms are proposed, namely, GT-SAGA and GT-SVRG. Li et al.
(2020a) generalized the approximate Newton-type method called DANE with gradient tracking and variance reduction.
See (Xin et al., 2020a) for a detailed review for the non-accelerated stochastic decentralized algorithms. Hendrikx et al.
(2021) proposed an accelerated decentralized stochastic algorithm called ADFS for problems with ﬁnite-sum structures,
which achieves the optimal O(
(cid:15) ) communication complexity. However, ADFS is a dual-based method, and it
needs to compute the dual gradient at each iteration, which is expensive. Recently, Hendrikx et al. (2020) further proposed a
(cid:15) ) communication
dual-free decentralized method with variance reduction, called DVR, which achieves the O(κb
complexity and the O((κs + n) log 1
(cid:15) ) stochastic gradient computation complexity. These complexities can be further
improved to (cid:101)O(
(cid:15) ) by the Catalyst acceleration (Lin et al., 2018), respectively,
κbκc
where (cid:101)O hides the poly-logarithmic factor, which is at least O(log κb)5. We see that DVR-Catalyst achieves the optimal
stochastic gradient computation complexity up to log factor. However, its communication cost is increased by a factor
) compared with ADFS, which is always much larger than 1 in machine learning applications6, and it is of the
O(
(cid:15) ) stochastic gradient computation
O(
(cid:15) ) communication lower bounds. The study on acceleration for the general stochastic problems
and the Ω(
without ﬁnite-sum structures can be found in (Dvinskikh & Gasnikov, 2021), (Gorbunov et al., 2019), and (Fallah et al.,
2019). See the recent review (Gorbunov et al., 2022) for the accelerated stochastic decentralized algorithms.

n) order in the worst case. Hendrikx et al. (2021) proved the Ω((

nκs + n) log 1

nκs + n) log 1

(cid:113) nκb
κs
√

(cid:15) ) and (cid:101)O((

κbκc log 1

(cid:113) nκb
κs

κc log 1

log 1

√

√

√

√

√

1.3. Contributions

Although both the decentralized methods and stochastic methods have been well studied, their combination still has much
work to do. For example, as far as we know, there is no gradient-type stochastic decentralized method achieving both the
state-of-the-art communication and stochastic gradient computation complexities (either accelerated or non-accelerated)
of the decentralized methods and stochastic methods simultaneously. In this paper we aim to address this issue. Our
contributions include:

1. We extend the widely used EXTRA and DIGing methods to deal with large-scale distributed data by combining
them with the powerful VR technique. We prove the O((κs + n) log 1
(cid:15) ) stochastic gradient computation complexity
and the O((κb + κc) log 1
(cid:15) ) communication complexity for VR-EXTRA, which are the best complexities among the
non-accelerated stochastic decentralized methods as far as we know. The stochastic gradient computation complexity

5See Proposition 17 in (Lin et al., 2018) and Corollary 7 in (Li & Lin, 2020).
6As discussed in Section 1.2.2 for the comparison between the VR methods and gradient descent, stochastic methods have no advantage

when κs ≈ nκb. We often assume κs (cid:28) nκb.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Table 1. Comparisons of various state-of-the-art decentralized and stochastic algorithms. See (3) and (5) for the deﬁnitions of κb, κs, and
κc. (cid:101)O hides the poly-logarithmic factors. The complexities of Acc-VR-EXTRA and Acc-VR-DIGing hold under some conditions to
restrict the size of κc. See part 1 of Remarks 3 and 4. Acc-VR-EXTRA-CA and Acc-VR-DIGing-CA remove these restrictions.

Methods

stochastic gradient
computation complexity

communication
complexity

dual
gradient
based ?

EXTRA
(Shi et al., 2015)
(Li & Lin, 2020)
DIGing
(Nedi´c et al., 2017)
(Alghunaim et al., 2021)
MSDA+CA
(Scaman et al., 2017)
APAPC+CA
(Kovalev et al., 2020b)

VR methods
(Schmidt et al., 2017)
(Defazio et al., 2014)
(Johnson & Zhang, 2013)
Katyusha
(Allen-Zhu, 2018)

Full batch decentralized algorithms

O (cid:0)n (κb + κc) log 1

(cid:15)

(cid:1)

O (cid:0)(κb + κc) log 1

(cid:15)

(cid:1)

O (cid:0)n (cid:0)κb + κ2

c

(cid:1) log 1

(cid:15)

(cid:1)

O (cid:0)(cid:0)κb + κ2

c

(cid:1) log 1

(cid:15)

(cid:1)

(cid:1)

√

O (cid:0)n

κb log 1
(cid:15)

O (cid:0)√
O (cid:0)√
Stochastic algorithms on a single machine

κb log 1
(cid:15)

O (cid:0)n

√

(cid:1)

κbκc log 1
(cid:15)

κbκc log 1
(cid:15)

(cid:1)

(cid:1)

O (cid:0)(κs + n) log 1

(cid:15)

(cid:1)

O (cid:0)(

√

nκs + n) log 1
(cid:15)

(cid:1)

\

\

GT-SAGA
(Xin et al., 2020b)
GT-SVRG
(Xin et al., 2020b)
ADFS
(Hendrikx et al., 2021)
DVR+CA
(Hendrikx et al., 2020)
DVR+Catalyst
(Hendrikx et al., 2020)
Lower bounds
(Hendrikx et al., 2021)

VR-EXTRA

VR-DIGing

Acc-VR-EXTRA

Acc-VR-DIGing

Acc-VR-EXTRA+CA

Acc-VR-DIGing+CA

Stochastic decentralized algorithms
O (cid:0)(cid:0)κ2

O (cid:0)(cid:0)κ2

(cid:15)

(cid:1)

c + n(cid:1) log 1
sκ2
c log κs + n(cid:1) log 1
sκ2
√
(cid:1)

nκs + n) log 1
(cid:15)

(cid:15)

O (cid:0)(cid:0)κ2

O (cid:0)(

(cid:1)

c + n(cid:1) log 1
sκ2
c log κs + n(cid:1) log 1

(cid:15)

(cid:15)

(cid:1) O (cid:0)(cid:0)κ2

(cid:15)

(cid:1)

√

O (cid:0)(κs + n) log 1
(cid:101)O (cid:0)(
Ω (cid:0)(

nκs + n) log 1
(cid:15)

nκs + n) log 1
(cid:15)

√

(cid:101)O

(cid:1)

(cid:1)

Our results for stochastic decentralized optimization

sκ2
O (cid:0)√
O (cid:0)κb
(cid:16)√

κbκc
Ω (cid:0)√

κbκc log 1
(cid:15)
√

κc log 1
(cid:15)
(cid:113) nκb
κs

κbκc log 1
(cid:15)

(cid:17)

log 1
(cid:15)

O (cid:0)(κb + κc) log 1

(cid:1)

(cid:1) log 1

(cid:15)

(cid:1)

O (cid:0)(cid:0)κb + κ2
O (cid:0)√

c

κbκc log 1
(cid:15)
√

κb log 1
(cid:15)

κbκc log 1
(cid:15)

κbκc log 1
(cid:15)

O (cid:0)κc
O (cid:0)√
O (cid:0)√

(cid:1)

(cid:1)

(cid:1)

(cid:15)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

O (cid:0)(κs + n) log 1

(cid:1)

(cid:15)

O (cid:0)(κs + n) log 1

(cid:15)

(cid:1)

O (cid:0)(

O (cid:0)(

O (cid:0)(

O (cid:0)(

√

nκs + n) log 1
(cid:15)

√

nκs + n) log 1
(cid:15)

√

nκs + n) log 1
(cid:15)

√

nκs + n) log 1
(cid:15)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

no

no

yes

no

no

no

no

no

yes

no

no

\

no

no

no

no

no

no

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

is the same as the single-machine VR methods, while the communication complexity is the same as the full batch
EXTRA. For VR-DIGing, we establish the O((κs + n) log 1
(cid:15) ) stochastic gradient computation complexity and the
O((κb + κ2
(cid:15) ) communication complexity. The latter one is a little worse than that of VR-EXTRA on the
dependence of κc. Due to the parallelism across m nodes, running VR-EXTRA and VR-DIGing with mn samples is
as fast as running the single-machine VR methods with n samples.

c) log 1

√

√

√

nκs + n) log 1

2. To further speed up the convergence, we combine EXTRA and DIGing with the accelerated VR technique. The
(cid:15) ) stochastic gradient computation complexity
proposed Acc-VR-EXTRA achieves the optimal O((
κbκc log 1
(cid:15) ) communication complexity under some mild conditions to restrict the size of κc. The
and the optimal O(
proposed Acc-VR-DIGing has the optimal O((
(cid:15) ) stochastic gradient computation complexity and the
O(κc
(cid:15) ) communication complexity with a little worse dependence on κc. The two methods are implemented
in a single loop, and thus they are practical. We further combine Acc-VR-EXTRA and Acc-VR-DIGing with the
Chebyshev acceleration to remove the restrictions on the size of κc, and improve the communication complexity of
Acc-VR-DIGing to be optimal. Our complexities do not hide any poly-logarithmic factor. To the best of our knowledge,
our methods are the ﬁrst to exactly achieve both the optimal stochastic gradient computation complexity and the
communication complexity in the class of gradient-type methods.

nκs + n) log 1

κb log 1

√

√

κc log 1

Table 1 summarizes the complexity comparisons to the state-of-the-art stochastic decentralized methods. Our VR-EXTRA
has the same stochastic gradient computation complexity as DVR-CA, but our communication cost is lower than theirs
when κc ≤ O(κ2
b). On the other hand, by combining with Chebyshev acceleration, our VR-EXTRA and VR-DIGing can
also obtain the O(κb
(cid:15) ) communication complexity. For the accelerated methods, our Acc-VR-EXTRA-CA and
Acc-VR-DIGing-CA outperform DVR-Catalyst on the stochastic gradient computation complexity at least by the poly-
(cid:17)
logarithmic factor O(log κb), and our communication cost is also lower than that of DVR-Catalyst by the factor O
.
On the other hand, DVR and its Catalyst acceleration require O(np) memory at each node, while our methods only need
O(p) memory7. Although ADFS has the same complexities as our Acc-VR-EXTRA-CA and Acc-VR-DIGing-CA, our
methods are gradient-type methods, while theirs requires to compute the dual gradient at each iteration, which is much more
expensive.

(cid:16)(cid:113) nκb
κs

2. Non-accelerated Variance Reduced EXTRA and DIGing

We ﬁrst review the classical EXTRA and DIGing methods in Section 2.1. Then we develop the variance reduced EXTRA
and DIGing in Sections 2.2 and 2.3. At last, we discuss the complexities of the proposed methods in Section 2.4.

2.1. Review of EXTRA and DIGing

A traditional way to analyze the decentralized optimization model is to write problem (1) in the following equivalent manner:

min
x(1),··· ,x(m)

m
(cid:88)

i=1

f(i)(x(i)),

s.t. x(1) = x(2) = · · · = x(m).

Following (Alghunaim et al., 2021) and using the notations in (2), we further reformulate the above problem as the following
linearly constrained problem:

where the symmetric matrices U ∈ Rm×m and V ∈ Rm×m satisfy

min
x

f (x) +

1
2α

(cid:107)V x(cid:107)2,

s.t. U x = 0,

U x = 0 ⇔ x(1) = · · · = x(m)

and V x = 0 ⇔ x(1) = · · · = x(m).

(6)

(7)

where 1
2α (cid:107)V x(cid:107)2 can be regarded as the augmented term in the augmented Lagrange method (Bertsekas, 1982), which may
speed up the convergence than the methods based on the pure Lagrangian function. Introducing the following augmented
Lagrangian function

L(x, λ) = f (x) +

(cid:107)V x(cid:107)2 +

(cid:104)U x, λ(cid:105) ,

1
2α

1
α

7This is similar to the memory cost comparison between SAG/SAGA and SVRG.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

we can apply the basic gradient method with a step-size α in the Gauss−Seidel-like order to compute the saddle point of
problem (6), which leads to the following iterations (Alghunaim et al., 2021; Nedi´c et al., 2017):

xk+1 = xk − (cid:0)α∇f (xk) + U λk + V 2xk(cid:1) ,
λk+1 = λk + U xk+1.

(8)

Iteration (8) is a uniﬁed algorithmic framework, and different choices of U and V give different methods (Alghunaim et al.,

2021). Speciﬁcally, when we choose U =
et al., 2015), which consists of the following iterations:

and V =

(cid:113) I−W
2

(cid:113) I−W
2

, (8) reduces to the famous EXTRA algorithm (Shi

xk+1 = (I + W )xk −

I + W
2

xk−1 − α (cid:0)∇f (xk) − ∇f (xk−1)(cid:1) .

When we choose U = I − W and V =
iterations:

√

I − W 2, (8) reduces to the DIGing (Nedi´c et al., 2017) method with the following

sk+1 = W sk + ∇f (xk) − ∇f (xk−1),
xk+1 = W xk − αsk+1.

Both EXTRA and DIGing rely on tracking the differences of gradients at each iteration.

2.2. Development of VR-EXTRA and VR-DIGing

(i)) + ∇f(i)(wk

(i)) = ∇f(i),j(xk

(i)) − ∇f(i),j(wk

(i)) in (8) by its VR estimator (cid:101)∇f(i)(xk

Now, we come to extend EXTRA and DIGing with the variance reduction technique proposed in SVRG (Johnson & Zhang,
2013). Speciﬁcally, SVRG maintains a snapshot vector wk
(i) after several SGD iterations, and keeps an iterative estimator
(cid:101)∇f(i)(xk
(i)) of the full batch gradient for some randomly selected j. When
extending EXTRA and DIGing to stochastic decentralized optimization, a straightforward idea is to replace the local gradient
∇f(i)(xk
(i)). However, in this way the resultant algorithm needs the same number of
stochastic gradient evaluations and communication rounds to precision (cid:15). As summarized in Table 1, our goal is to provide
computation and communication complexities matching those of SVRG and EXTRA/DIGing, respectively, which are not
equal. To address this issue, we use the mini-batch VR technique, that is, select b independent samples with replacement as
a mini-batch S(i), and use this mini-batch to update the VR estimator. By carefully choosing the mini-batch size b, we can
balance the communication and stochastic gradient computation costs. Moreover, to simplify the algorithm development
and analysis, we adopt the loopless SVRG proposed in (Kovalev et al., 2020a). Combining the above ideas, we have the
following VR variant of (8) described in a distributed way:

∇k

(i) =

1
b

(cid:88)

j∈Sk

(i)

(cid:16)

1
np(i),j

∇f(i),j(xk

(i)) − ∇f(i),j(wk

(i))

(cid:17)

+ ∇f(i)(wk

(i)),

∀i,

xk+1
(i) = xk

(i) −


α∇k

(i) +

(cid:88)

j∈N(i)

Uijλk

(j) +

(cid:88)

j∈N(i)



(V 2)ijxk
(j)

 ,

∀i,

λk+1
(i) = λk

(i) +

(cid:88)

j∈N(i)

Uijxk+1
(j) ,

∀i,

(cid:40)

wk+1

(i) =

(i) with probability b
xk
n ,
(i) with probability 1 − b
wk
n ,

∀i,

(9a)

(9b)

(9c)

(9d)

where the mini-batch VR estimator update rule (9a) is motivated by (Allen-Zhu, 2018), in which each sample j on node
i is selected with probability p(i),j = L(i),j
. The probabilistic update of the snapshot vector in (9d) is motivated by
(cid:80)n
(Kovalev et al., 2020a), in which we update the full batch gradient ∇f(i)(wk+1
(i); otherwise, we use the old
one. Steps (9b) and (9c) come from (8), but replacing the local gradients by their VR estimators. In steps (9a) and (9d), each
node selects Sk

independent of the other nodes.

(i) ) if wk+1

(i) = xk

j=1 L(i),j

(i) and computes wk+1

(i)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

At last, we write (9a)-(9d) in the EXTRA/DIGing style. Similar to (2), we denote

∇k =






(∇k
(1))T
...
(m))T

(∇k






to simplify the algorithm description. From steps (9b) and (9c), we have

xk+1 = (2I − U 2 − V 2)xk − (I − V 2)xk−1 − α (cid:0)∇k − ∇k−1(cid:1)

in the compact form. Plugging U =

(cid:113) I−W
2

and V =

(cid:113) I−W
2

into (11), we have

(10)

(11)

xk+1 = (I + W )xk −

I + W
2

xk−1 − α (cid:0)∇k − ∇k−1(cid:1) ,

which is the VR variant of EXTRA, called VR-EXTRA. Plugging U = I − W and V =

√

I − W 2 into (11), we have

xk+1 = 2W xk − W 2xk−1 − α (cid:0)∇k − ∇k−1(cid:1) ,

which is further equivalent to the following method, called VR-DIGing,

sk+1 = W sk + ∇k − ∇k−1,
xk+1 = W xk − αsk+1.

We see that VR-EXTRA and VR-DIGing are quite similar to the original EXTRA and DIGing. The only difference is
that we replace the local gradients by their VR estimators. Thus the implementation is as simple as that of the original
EXTRA and DIGing. We give the speciﬁc descriptions of VR-EXTRA and VR-DIGing in Algorithm 1 in a distributed way,
including the parameter settings. To discuss EXTRA and DIGing in a uniﬁed framework, we denote

κ = 2κc for EXTRA

and

κ = κ2

c for DIGing.

(12)

See Lemma 1 for the reason. We will use κ frequently in this paper when we do not distinguish EXTRA and DIGing, and
the readers can use (12) to get the speciﬁc properties of EXTRA and DIGing, respectively.

2.3. Extension to Large κ

The particular choice of the mini-batch size b in Algorithm 1 may be smaller than 1 when κ is large, which makes the
algorithm meaningless. We discuss EXTRA and DIGing in a uniﬁed way in this section, so we use κ in this section, which
is deﬁned by (12). In fact, b ≥ 1 if and only if κ ≤ max{κs, n}, see the proof of Theorem 1 in Section 4. In this section we
consider the case of κ > max{κs, n}.
Intuitively speaking, when κ is very large such that κb + κ ≥ κs + n, to reach the desired O((κb + κ) log 1
(cid:15) ) communication
complexity and the O((κs + n) log 1
(cid:15) ) stochastic gradient computation complexity, as summarized in Table 1, we should
perform less than 1 stochastic gradient evaluation in average at each iteration. This observation motivates us to introduce
some zero samples, that is to say, let f(i),n+1 = · · · = f(i),n(cid:48) = 0 for all i, and consider problem

min
x∈Rp

m
(cid:88)

i=1

f (cid:48)
(i)(x), where

f (cid:48)
(i)(x) =

1
n(cid:48)

n(cid:48)
(cid:88)

j=1

f(i),j(x).

(13)

The zero samples do not spend time to compute the stochastic gradient. We see that problems (13) and (1) are equivalent. To
use Algorithm 1 to solve problem (13), we denote

L(i),j =

nµn(cid:48) − nL(i)
n(cid:48) − n

for all n < j ≤ n(cid:48),

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

(i) = 0, compute x1

(i) and w1

(i) for all i by (9b) and (9d), respectively. Let α =
(i)) for

c for DIGing. Let s1

(i) = ∇f(i)(w0

Initialize: x0

Algorithm 1 VR-EXTRA and VR-DIGing
(i) = w0

(i) = xint, λ0
max{Lf ,κµ} ) and b = max{Lf ,nµ}

1

max{Lf ,κµ} , where κ = 2κc for EXTRA, and κ = κ2

O(
DIGing.
Let distribution D(i) be to output j ∈ [1, n] with probability p(i),j = L(i),j
nL(i)
for k = 1, 2, ... do

.

(i) ← b independent samples from D(i) with replacement, ∀i,

Step 1: Sk
Step 2: Compute ∇k
Step 3: For EXTRA, compute xk+1

(i) by (9a), ∀i,

(i) by


xk

(i) +

xk+1
(i) =



Wijxk
(j)

 −


xk−1

(i) +

1
2

(cid:88)

j∈N(i)

(cid:88)

j∈N(i)



Wijxk−1
(j)

 − α

(cid:16)

∇k

(i) − ∇k−1
(i)

(cid:17)

, ∀i,

For DIGing, compute xk+1

(i) by

sk+1
(i) =

(cid:88)

j∈N(i)

Wijsk

(j) + ∇k

(i) − ∇k−1
(i) ,

Step 4: Compute wk+1

(i) by (9d), ∀i.

end for

and let each sample be selected with probability

(cid:80)n(cid:48)
and select the zero samples with probability 1 − L(i)
Deﬁne the following notations:

L(i),j
j=1 L(i),j

xk+1
(i) =

(cid:88)

j∈N(i)

Wijxk

(j) − αsk+1
(i) ,

∀i,

. Then we select the samples in [1, n] with probability L(i)
µn(cid:48) ,

µn(cid:48) . It can be seen that f (cid:48)

(i)(x) is nL(i)

n(cid:48)

-smooth and nµ

n(cid:48) -strongly convex.

n(cid:48) = κ, µ(cid:48) =

nµ
n(cid:48) , L(cid:48)

f = max

i

nL(i)
n(cid:48) =

nLf
n(cid:48)

, L

(cid:48)
(i) =

(cid:80)n(cid:48)

j=1 L(i),j
n(cid:48)

(cid:48)
f = max
, L

i

L

(cid:48)
(i).

(14)

f ,κµ(cid:48)} ) = O( 1
We can easily check α = O(
Then we can use Algorithm 1 to solve problem (13).

1
max{L(cid:48)

nµ ), and b =

max{L(cid:48)
max{L(cid:48)

f ,n(cid:48)µ(cid:48)}
f ,κµ(cid:48)} = 1. See the proof of Theorem 2 in Section 4.

2.4. Complexities

We prove the convergence of VR-EXTRA and VR-DIGing in a uniﬁed framework. From Assumption 2, we have the
following easy-to-identify lemma, where the third inequality in (15) can be proved similarly to Lemma 4 in (Li et al., 2020b).

Lemma 1 Suppose that Assumption 2 holds with ω = 0 for EXTRA. Let U = V =

(cid:113) I−W
2

. Then we have

(cid:107)U x(cid:107)2 ≤ (cid:107)V x(cid:107)2,

(cid:107)V x(cid:107)2 ≤

1
2

(cid:107)x(cid:107)2,

and

(cid:107)U λ(cid:107)2 ≥

1
κ

(cid:107)λ(cid:107)2, ∀λ ∈ Span(U ),

(15)

2

where κ =
Then (15) also holds with κ =

1−σ2(W ) = 2κc. Suppose that Assumption 2 holds with ω =
(1−σ2(W ))2 = κ2
c.

1

√
2
2 for DIGing. Let U = I −W and V =

√

I − W 2.

Denote the following set of random variables:

Sk = ∪m
i=1

Sk
(i),

ξk = {S0, w1, S1, w2 · · · , Sk−1, wk}.

The next theorem gives the communication complexity and stochastic gradient computation complexity of algorithm
(9a)-(9d) in a uniﬁed way.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Theorem 1 Suppose that Assumption 1 holds, and U and V satisfy (7) and (15). Let α =

28 max{Lf ,κµ} and λ0 = 0.

1

1. If κ ≤ max{κs, n}, let b = max{Lf ,nµ}
tion rounds and O((κs + n) log 1

max{Lf ,κµ} . Then algorithm (9a)-(9d) requires the time of O((κb + κ) log 1

(cid:15) ) communica-

(cid:15) ) stochastic gradient evaluations to ﬁnd xk such that Eξk [(cid:107)xk − x∗(cid:107)2] ≤ (cid:15).

2. If κ ≥ max{κs, n}, let b = 1. Then algorithm (9a)-(9d) requires the time of O((κb + κ) log 1

and O((κb + κ) log 1

(cid:15) ) stochastic gradient evaluations to ﬁnd xk such that Eξk

(cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (cid:15).

(cid:15) ) communication rounds

Remark 1 Let’s explain the time of one communication rounds and one stochastic gradient evaluation. At each iteration,
algorithm (9a)-(9d) performs one round of communication, that is, each node i receives information xk
(j) from its
neighbors for all j ∈ N(i). Then each node i selects Sk
(i) with b stochastic gradient evaluations.
∇f(i)(wk+1
(i) ) is updated with probability b/n, and each time with n stochastic gradient evaluations. So each node computes
b stochastic gradients in average at each iteration. Since the computation is performed in parallel across all the nodes, we
say that each iteration requires the time of one communication round and b stochastic gradient evaluations in average.

(i) randomly and computes ∇k

(j) and λk

From Theorem 1, we see that when κ ≥ max{κs, n}, the stochastic gradient computation cost increases to O((κb +κ) log 1
(cid:15) ).
We can use the zero-sample strategy described in Section 2.3 to reduce the computation cost to O((κs + n) log 1
(cid:15) ), as
described in the following theorem.

Theorem 2 Suppose that Assumption 1 and conditions (7) and (15) hold. Assume κ > max{κs, n}. Applying Algorithm 1
to solve problem (13), it requires the time of O((κb + κ) log 1
(cid:15) ) stochastic
gradient evaluations to ﬁnd an (cid:15)-precision solution of problem (1) such that Eξk [(cid:107)xk − x∗(cid:107)2] ≤ (cid:15).

(cid:15) ) communication rounds and O((κs + n) log 1

We see that by introducing the zero samples with carefully designed n(cid:48) and L(i),j for n < j ≤ n(cid:48), the complexities in
Theorem 2 keep the same as those in part one of Theorem 1.

For the particular VR-EXTRA and VR-DIGing methods, we have the following complexities accordingly, where we replace
κ in Theorems 1 and 2 by 2κc and κ2

c, respectively.

Corollary 1 Suppose that Assumptions 1 and 2 hold with ω = 0. Use the zero-sample strategy if 2κc ≥ max{κs, n}. Then
(cid:15) ) communication rounds and O((κs + n) log 1
the VR-EXTRA method in Algorithm 1 requires the time of O((κb + κc) log 1
(cid:15) )
stochastic gradient evaluations to ﬁnd xk such that Eξk [(cid:107)xk − x∗(cid:107)2] ≤ (cid:15).

Corollary 2 Suppose that Assumptions 1 and 2 hold with ω =
c) log 1
the VR-DIGing method in Algorithm 1 requires the time of O((κb + κ2
stochastic gradient evaluations to ﬁnd xk such that Eξk [(cid:107)xk − x∗(cid:107)2] ≤ (cid:15).

2

2 . Use the zero-sample strategy if κ2

c ≥ max{κs, n}. Then
(cid:15) ) communication rounds and O((κs + n) log 1
(cid:15) )

√

Remark 2

1. The communication complexity of VR-DIGing has a worse dependence on κc than that of VR-EXTRA. This is because
in problem (6), while DIGing uses U = I − W . From Lemma 1, we see that different choice

EXTRA uses U =
of U gives different order of κc.

(cid:113) I−W
2

2. From Table 1, we see that EXTRA and VR-EXTRA have the same communication complexity, and DIGing and VR-
DIGing also have the same communication complexity. Thus extending EXTRA and DIGing to stochastic decentralized
optimization does not need to pay a price of more communication cost theoretically.

3. When κ ≤ max{κs, n}, running VR-EXTRA and VR-DIGing with mn samples needs the time of O((κs + n) log 1
(cid:15) )
stochastic gradient evaluations by parallelism, which is the same as that of running the single-machine VR methods with
n samples when we ignore the communication time. On the other hand, when we run the single-machine VR methods
with mn samples, the required time increases to O((κs + mn) log 1
(cid:15) ). Thus the linear speedup is achieved when n
is larger than κs. The situation of κ > max{κs, n} is more complicated because at each iteration, some machines
would be computing gradients, while others would be idle if the zero-sample is chosen. Parallelism is destroyed and
the actual running time would be larger than the time of O((κs + n) log 1

(cid:15) ) stochastic gradient evaluations.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Algorithm 2 Acc-VR-EXTRA and Acc-VR-DIGing

Initialize: x0

(i) = z0

(i) = w0

(i), λ0
2Lf b , where κ = 2κc for EXTRA, and κ = κ2

(i) = 0 for all i, α = O( 1
Lf

θ2 = Lf

c for DIGing.
√

(cid:113) I−W
2

for EXTRA, and U = I − W and V =

Let U = V =
Let distribution D(i) be to output j ∈ [1, n] with probability p(i),j = L(i),j
nL(i)
for k = 0, 1, 2, ... do

.

I − W 2 for DIGing.

Sk
(i) ← b independent samples from D(i) with replacement, ∀i,
Perform steps (16a)-(16f), ∀i,

), b = max{ max{
max{

nLf /µ,n}
κLf /µ,κ}

√
√

, Lf
Lf

}, θ1 = min{ 1
2

(cid:113) κµ
Lf

, 1
2 },

end for

4. Both in theory and in practice, we can choose a larger mini-batch size b than the particular choice given in Algorithm
1, at the expense of a higher stochastic gradient computation complexity than O((κs + n) log 1
(cid:15) ). However, the
communication complexity remains unchanged. See the proof of Theorem 1. Denote τ to be the ratio between
the practical running time of performing one communication round and one stochastic gradient computation. If
κs + n ≤ τ (κb + κ), that is, communications dominate the total running time, we can increase the mini-batch size to
max{Lf ,nµ}
max{Lf ,κµ}

τ (κb+κ)
κs+n = τ , which does not increase the total running time of O(τ (κb + κ) log 1

(cid:15) ).

3. Accelerated Variance Reduced EXTRA and DIGing

In this section, we develop the accelerated VR-EXTRA and VR-DIGing methods. In algorithm (9a)-(9d), we combine (8)
with the loopless SVRG to get the non-accelerated methods. To develop the accelerated methods, a straightforward idea
is to combine (8) with the loopless Katyusha proposed in (Kovalev et al., 2020a), which leads to the following algorithm
(16a)-(16f). We give the parameter settings in Algorithm 2. We will not write (16a)-(16f) in the EXTRA/DIGing style since
the resultant methods are complex, and they are not very similar to the original EXTRA and DIGing besides the feature of
tracking the differences of gradients.

yk
(i) = θ1zk

∇k

(i) =

1
b

(i) + θ2wk
(i) + (1 − θ1 − θ2)xk
1
(cid:88)
np(i),j

∇f(i),j(yk

(cid:16)

(i),

∀i,

j∈Sk

(i)

(i)) − ∇f(i),j(wk

+ ∇f(i)(wk

(i)),

∀i,

(cid:17)
(i))

zk+1
(i) =

1
1+ µα
θ1





µα
θ1

(i) +zk
yk

(i) −


α∇k

(i) +

1
θ1

(cid:88)

j∈N(i)

Uijλk

(j) +θ1

(cid:88)

j∈N(i)





(V 2)ijzk
(j)



 , ∀i,

λk+1
(i) = λk

(i) + θ1

(cid:88)

Uijzk+1
(j) ,

∀i,

xk+1
(i) = yk
(i) + θ1
(cid:40)

wk+1

(i) =

j∈N(i)
(cid:16)

(cid:17)

,

(i)

∀i,

zk+1
(i) − zk
(i) with probability b
xk
n ,
(i) with probability 1 − b
wk
n ,

∀i.

(16a)

(16b)

(16c)

(16d)

(16e)

(16f)

In the above algorithm, steps (16a) and (16e) are the Nesterov’s acceleration steps, which are motivated by (Allen-Zhu,

2018; Kovalev et al., 2020a). Steps (16c) and (16d) involve the operation of U x, which is uncomputable for U =
in
EXTRA in the distributed environment. Introducing the auxiliary variable (cid:101)λk = U λk and multiplying both sides of (16d) by
U leads to

(cid:113) I−W
2

zk+1 =

1
1 + µα
θ1

(cid:18) µα
θ1

yk + zk −

1
θ1

(cid:101)λk+1 = (cid:101)λk + θ1U 2zk+1,

(cid:16)

α∇k + (cid:101)λk + θ1V 2zk(cid:17)(cid:19)

,

(17)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

in the compact form. From the deﬁnitions of U = V =
gossip-style communications. For DIGing, we do not need such auxiliary variables.

, we only need to compute W z, which corresponds to the

(cid:113) I−W
2

3.1. Complexities

Theorem 3 gives the complexities of algorithm (16a)-(16f) in a uniﬁed way, and Corollaries 3 and 4 provide the complexities
for the particular Acc-VR-EXTRA and Acc-VR-DIGing methods, respectively.

Theorem 3 Suppose that Assumption 1 holds, and U and V satisfy (7) and (15). Let θ1 = min{ 1
2

α = 1

10Lf

, λ0 = 0, and b = max{ max{
max{

nLf /µ,n}
κLf /µ,κ}

, Lf
Lf

}.

√
√

(cid:113) κµ
Lf

2 }, θ2 = Lf
, 1

2Lf b ,

1. If κ ≤ nLf
Lf

√
, such that b = max{
√
max{
√
communication rounds and O((
x∗(cid:107)2(cid:3) ≤ (cid:15).

nLf /µ,n}
κLf /µ,κ}
nκs + n) log 1

, then algorithm (16a)-(16f) requires the time of O((κ +

(cid:15) ) stochastic gradient evaluations to ﬁnd zk such that Eξk

√

κbκ) log 1
(cid:15) )
(cid:2)(cid:107)zk −

2. If κ ≥ nLf
Lf
rounds and O( Lf
Lf

, such that b = Lf
Lf
κbκ) log 1

(κ +

√

, then algorithm (16a)-(16f) requires the time of O((κ +

√

κbκ) log 1

(cid:15) ) communication

(cid:15) ) stochastic gradient evaluations to ﬁnd zk such that Eξk

(cid:2)(cid:107)zk − x∗(cid:107)2(cid:3) ≤ (cid:15).

Remark 3

1. As introduced in Section 1.1, we have κb ≤ κs ≤ nκb, and we always assume κs (cid:28) nκb in the analysis of
stochastic algorithms. Thus we can expect nLf
to be large for large-scale data. On the other hand, κc depends on the
Lf
network scale and connectivity. For example, κc = O(1) for the commonly used Erd˝os−R´enyi random graph, and
κc = O(m log m) for the geometric graph. In the worst case, for example, the linear graph or cycle graph, we have
κc = O(m2) (Nedi´c et al., 2018). Thus we can also expect κ to be not very large when the number of distributed
nodes is limited and the network is well connected. So we can expect that the assumption κ ≤ nLf
always holds for
Lf
large-scale distributed data, for example, thousands of nodes and each node with millions of data.

2. Similar to VR-EXTRA and VR-DIGing, both in theory and in practice, we can choose a larger mini-batch size b than
the particular choice given in Algorithm 2, at the expense of higher stochastic gradient computation complexities
than the ones given in Theorem 3. However, the O((
(cid:15) ) communication complexity remains unchanged.
See the proof of Theorem 3. We take part one of Theorem 3 as an example. Recall the deﬁnition of τ in Remark
√
κbκ, κ}, that is, communications dominate the total running time, we can
2(4).
nκs, n} ≤ τ max{
√
√
nLf /µ,n}
increase the mini-batch size to max{
τ max{
κbκ,κ}
√
nκs,n} = τ , which does not increase the total running time of
√
max{
κLf /µ,κ}
max{

κbκ + κ) log 1

If max{

√

√

√

O(τ (

κbκ + κ) log 1

(cid:15) ).

Corollary 3 Suppose that Assumptions 1 and 2 hold with ω = 0. Under the parameter settings in Theorem 3 with κ = 2κc,
if 2κc ≤ nLf
κbκc log 1
(cid:15) )
Lf
(cid:15) ) stochastic gradient evaluations to ﬁnd zk such that Eξk [(cid:107)zk − x∗(cid:107)2] ≤ (cid:15).
communication rounds and O((

and κc ≤ κb, the Acc-VR-EXTRA algorithm requires the time of O((κc +

nκs + n) log 1

κbκc) log 1

(cid:15) ) = O(

√

√

√

Corollary 4 Suppose that Assumptions 1 and 2 hold with ω =
if κ2

c ≤ κb, the Acc-VR-DIGing algorithm requires the time of O((κ2

2 . Under the parameter settings in Theorem 3 with κ = κ2
c,
√
κb log 1
(cid:15) )
(cid:15) ) stochastic gradient evaluations to ﬁnd zk such that Eξk [(cid:107)zk − x∗(cid:107)2] ≤ (cid:15).

(cid:15) ) = O(κc

communication rounds and O((

nκs + n) log 1

c ≤ nLf
Lf

κb) log 1

c + κc

and κ2

√

√

√

2

Remark 4

1. From Table 1, we see that the communication complexity and stochastic gradient computation complexity of Acc-
VR-EXTRA are both optimal under the restrictions of 2κc ≤ nLf
and κc ≤ κb. Similarly, the stochastic gradient
Lf
computation complexity of Acc-VR-DIGing is also optimal. However, its communication complexity is worse than the
corresponding lower bound by the O(

√

κc) factor.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

2. Running Acc-VR-EXTRA and Acc-VR-DIGing with mn samples needs the time of O((

(cid:15) ) stochastic
gradient evaluations, which is the same as that of running the single-machine Katyusha with n samples when we ignore
the communication time. On the other hand, when we run the single-machine Katyusha with mn samples, the required
time increases to O((
(cid:15) ). Since acceleration takes effect only when κs (cid:29) mn, the parallelism
speeds up Katyusha by the

m factor. On the other hand, when κs ≤ n, the linear speedup is achieved.

mnκs + mn) log 1

√

√

nκs + n) log 1

√

At last, we compare VR-EXTRA and VR-DIGing with Acc-VR-EXTRA and Acc-VR-DIGing. Both the accelerated
methods and non-accelerated methods have their own advantages.

1. The accelerated methods need less stochastic gradient computation evaluations than the non-accelerated methods when
κs > n and κ ≤ nLf
. In this case, acceleration takes effect. Otherwise, the accelerated methods have no advantage
Lf
over non-accelerated methods on the computation cost. Moreover, the non-accelerated methods have the superiority of
simple implementation.

2. The accelerated methods need less communication rounds than their non-accelerated counterparts when κ ≤ κb. When
dealing with large-scale distributed data in machine learning, we may expect that computation often dominates the total
running time. Otherwise, the full batch accelerated decentralized methods such as APAPC (Kovalev et al., 2020b) may
be a better choice.

3.2. Chebyshev Acceleration

In this section we remove the restrictions on the size of κc in Corollaries 3 and 4, which come from matrix U , as shown in
(15). To make κ small, our goal is to construct a new matrix ˆU by U such that Ker( ˆU ) = Span(1) and (cid:107) ˆU λ(cid:107)2 ≥ 1
c (cid:107)λ(cid:107)2 for
all λ ∈ Span( ˆU ), where c is a much smaller constant than κ. Moreover, the construction procedure should not take more
κc) time. Then we only need to replace U and V by ˆU and some matrix ˆV in algorithm (16a)-(16f), where ˆU and
than O(
ˆV should satisfy the relations in (15). We follow (Scaman et al., 2017) to use Chebyshev acceleration to construct ˆU , which
is a common acceleration scheme to minimize c.

√

3.2.1. REVIEW OF CHEBYSHEV ACCELERATION

We ﬁrst give a brief description of Chebyshev acceleration, which was ﬁrst used to accelerate distributed optimization in
(Scaman et al., 2017). We ﬁrst introduce the Chebyshev polynomials deﬁned as T0(x) = 1, T1(x) = x, and Tk+1(x) =
2xTk(x) − Tk−1(x) for all k ≥ 1. Given a positive semideﬁnite symmetric matrix L ∈ Rm×m such that Ker(L) = Span(1),
denote λ1(L) ≥ λ2(L) ≥ · · · ≥ λm−1(L) > λm(L) = 0 as the eigenvalues of L. Following the notations in (Scaman et al.,
2017), we deﬁne γ(L) = λm−1(L)
λ1(L)+λm−1(L) . Then c3L has the spectrum in
λ1(L)
[1 − c−1
2 ]. For any polynomial pt(x) of degree at most t, Theorem 6.1 in (Auzinger & Melenk, 2017) tells us that
the solution of the following problem

1−γ(L) , and c3 =

, c2 = 1+γ(L)

2 , 1 + c−1

, c1 =

√
√

γ(L)

γ(L)

1−

1+

2

min
pt:pt(0)=0

x∈[1−c−1

max
2 ,1+c−1
2 ]

|pt(x) − 1|

is

Tt(c2(1 − x))
Tt(c2)
Moreover, from Corollary 6.1 in (Auzinger & Melenk, 2017), we have

Pt(x) = 1 −

.

min
pt:pt(0)=0

x∈[1−c−1

max
2 ,1+c−1
2 ]

|pt(x) − 1| =

2ct
1
1 + c2t
1

,

(cid:104)
1 − 2ct
1
1+c2t
1

, 1 + 2ct
1
1+c2t
1

(cid:105)

. For the particular choice of t = 3√

γ(L)

, it can be

which means that the spectrum of Pt(c3L) lies in

(cid:16) (cid:16)

√

(cid:17)1/

γ(L) (cid:17)3

1 ≤

1 − (cid:112)γ(L)

checked that ct
e3+e−3 ≤ 0.1. Thus the spectrum
of Pt(c3L) is a subinterval of [0.9, 1.1]. On the other hand, it can be checked that Pt(c3L) is a gossip matrix satisfying
Ker(Pt(c3L)) = Span(1) (Scaman et al., 2017). In practice, we can compute the operation Pt(c3L)x by the following
procedure (Scaman et al., 2017):

≤ e−3, which further leads to 2ct
1
1+c2t
1

≤

2

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Input: x,
Initialize: a0 = 1, a1 = c2, z0 = x, z1 = c2(I − c3L)x,
for s = 1, 2, ..., t − 1 do
as+1 = 2c2as − as−1,
zs+1 = 2c2(I − c3L)zs − zs−1.

end for
Output: Pt(c3L)x = x − zt
at .

3.2.2. REMOVE THE RESTRICTIONS ON THE SIZE OF κc

(cid:113) I−W
2

(cid:113) 1

, deﬁne ˆU = ˆV =

2 (cid:107)x(cid:107)2 and (cid:107) ˆU λ(cid:107)2 ≥ 0.9

For the particular choice of U = V =
λ1(U 2)+λn−1(U 2) . Then we
have (cid:107) ˆV x(cid:107)2 ≤ 1
κc).
=
So ˆU and ˆV satisfy the relations in (15), and replacing U and V by ˆU and V does not destroy the proof of Theorem 3. In
the algorithm implementation, we only need to replace the operations U 2z and V 2z in (17) by 1
2.2 Pt(c3U 2)z. Moreover,
replacing κ by the constant 3 in Theorem 3, we can expect that the assumptions on κ always hold. Since we need O(
κc)
time to construct ˆU at each iteration, so the communication complexity remains (cid:0)√

3 (cid:107)λ(cid:107)2 for all λ ∈ Span( ˆU ) with t =

2.2 Pt(c3U 2), where c3 =

2.2 (cid:107)λ(cid:107)2 ≥ 1

1−σ2(W )

3√

= O(

3√

γ(U 2)

(cid:1).

√

√

2

κbκc log 1
(cid:15)

Corollary 5 Suppose that Assumptions 1 and 2 hold with ω = 0. Under the parameter settings in Theorem 3 with
κ = 3, Acc-VR-EXTRA with Chebyshev acceleration (CA) requires the time of (cid:0)√
(cid:1) communication rounds and
O((

(cid:15) ) stochastic gradient evaluations to ﬁnd zk such that Eξk [(cid:107)zk − x∗(cid:107)2] ≤ (cid:15).

nκs + n) log 1

κbκc log 1
(cid:15)

√

√

√

2

√

λ1(U )+λn−1(U ) . Then we have (cid:107) ˆV x(cid:107)2 ≤ 1

For the particular choice of U = I − W and V =
where c3 =

I − W 2, deﬁne ˆU = 2−
2 (cid:107)x(cid:107)2 and (cid:107) ˆU λ(cid:107)2 ≥ 1
κc). Similar to the above analysis for the Acc-VR-EXTRA-CA method, we have the following complexity corollary.
κc)

O(
Note that since we replace κ by the constant 20 in Theorem 3, and use the fact that the construction of ˆU needs O(
time at each iteration, we can reduce the communication cost from O(κc

2.2 Pt(c3U ), ˆW = I − ˆU , and ˆV =
20 (cid:107)λ(cid:107)2 for all λ ∈ Span( ˆU ) with t = 3√

I − ˆW 2,
=

κbκc log 1

κb log 1

γ(U )

√

√

√

2

(cid:15) ) to O(

(cid:15) ).

(cid:112)

Corollary 6 Suppose that Assumptions 1 and 2 hold with ω =
Acc-VR-DIGing-CA requires the time of O(
gradient evaluations to ﬁnd zk such that Eξk [(cid:107)zk − x∗(cid:107)2] ≤ (cid:15).

κbκc log 1

√

√
2
2 . Under the parameter settings of Theorem 3 with κ = 20,
√
(cid:15) ) stochastic

(cid:15) ) communication rounds and O((

nκs + n) log 1

Remark 5

1. From Corollaries 5 and 6, we see that the restrictions on κc in Corollaries 3 and 4 have been removed, and the

communication complexity of Acc-VR-DIGing has been improved to be optimal by Chebyshev acceleration.

2. We can also combine Chebyshev acceleration with the non-accelerated VR-EXTRA and VR-DIGing, and give the
(cid:1) communication complexity,

(cid:1) stochastic gradient computation complexity and the O (cid:0)(cid:0)κb

O (cid:0)(κs + n) log 1
which are the same as those of DVR (Hendrikx et al., 2020).

(cid:1) log 1

κc

√

(cid:15)

(cid:15)

4. Proof of Theorems

We prove Theorems 1 and 3 in this section. We ﬁrst introduce some useful properties. For L-smooth and convex function
f (x), we have (Nesterov, 2004)

f (y) − f (x) − (cid:104)∇f (x), y − x(cid:105) ≥

1
2L

(cid:107)∇f (y) − ∇f (x)(cid:107)2.

(18)

Recall that x∗ is the optimal solution of problem (1), then x∗ is also the optimal solution of the following linearly constrained
convex problem

where U satisﬁes (7). Furthermore, there exists λ∗ ∈ Span(U ) such that

min
x

f (x),

s.t. U x = 0,

∇f (x∗) +

1
α

U λ∗ = 0.

(19)

(20)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

The existence of λ∗ is proved in (Shi et al., 2015, Lemma 3.1). U x∗ = 0 and (20) are the Karush-Kuhn-Tucker (KKT)
optimality conditions of problem (19).

4.1. Non-accelerated VR-EXTRA and VR-DIGing

We ﬁrst give a classical property of the VR technique (Johnson & Zhang, 2013; Xiao & Zhang, 2014).

Lemma 2 Suppose that Assumption 1 and condition (7) hold. Then for algorithm (9a)-(9d), we have

ESk

(cid:2)(cid:107)∇k − ∇f (xk)(cid:107)2(cid:3) ≤

(cid:18)

4Lf
b

f (xk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U xk(cid:11)

1
α

+

4Lf
b

(cid:18)

f (wk) − f (x∗) +

(cid:10)λ∗, U wk(cid:11)

(cid:19)

.

1
α

(21)

Proof 1 From the proof of Lemma D.2 in (Allen-Zhu, 2018), we have

ESk

(i)

(cid:2)(cid:107)∇k

(i) − ∇f(i)(xk

(i))(cid:107)2(cid:3) ≤

1
b

Ej∼D(i)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

1
np(i),j

∇f(i),j(xk

(i)) − ∇f(i),j(wk

(i))

2(cid:35)

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

where D(i) is deﬁned in Algorithm 1. Using identity (cid:107)a + b(cid:107)2 ≤ 2(cid:107)b(cid:107)2 + 2(cid:107)b(cid:107)2 and (18), from the proof of Lemma D.2 in
(Allen-Zhu, 2018) and recalling that p(i),j = L(i),j
nL(i)

, we have

(i))(cid:107)2(cid:3)

ESk

(i)

≤

2
b

(cid:2)(cid:107)∇k

Ej∼D(i)

(cid:16)

(i) − ∇f(i)(xk
(cid:34)(cid:13)
1
(cid:13)
(cid:13)
np(i),j
(cid:13)
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
np(i),j

Ej∼D(i)

+

2
b

∇f(i),j(xk

(i)) − ∇f(i),j(x∗)

(cid:16)

∇f(i),j(wk

(i)) − ∇f(i),j(x∗)

2(cid:35)

2(cid:35)

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(i) − x∗(cid:69)(cid:17)

≤

(cid:16)

4L(i)
b
4L(i)
b

+

f(i)(xk

(i)) − f(i)(x∗) −

(cid:68)
∇f(i)(x∗), xk

(cid:16)

f(i)(wk

(i)) − f(i)(x∗) −

(cid:68)
∇f(i)(x∗), wk

(i) − x∗(cid:69)(cid:17)

.

From the convexity of f(i)(x), the deﬁnitions in (2), (3), and (10), and the fact that Sk
for all i and j, we have

(i) and Sk

(j) are selected independently

ESk

(cid:2)(cid:107)∇k − ∇f (xk)(cid:107)2(cid:3) ≤

4Lf
b

(cid:0)f (xk) − f (x∗) − (cid:10)∇f (x∗), xk − x∗(cid:11)(cid:1)

+

4Lf
b

(cid:0)f (wk) − f (x∗) − (cid:10)∇f (x∗), wk − x∗(cid:11)(cid:1) .

From the optimality condition in (20) and U x∗ = 0, we have the conclusion.

The following property is also useful in the analysis of mini-batch VR methods.

Lemma 3 For algorithm (9a)-(9d), we have

ESk

(cid:2)∇k(cid:3) = ∇f (xk).

(22)

Proof 2 From the deﬁnition of ∇k
we have

(i) in (9a), and the fact that the elements in Sk

(i) are selected independently with replacement,

ESk

(i)

(cid:2)∇k

(i)

(cid:3) =Ej∼D(i)

(cid:20)

(cid:16)

1
np(i),j

∇f(i),j(xk

(i)) − ∇f(i),j(wk

(i))

(cid:17)

+ ∇f(i)(wk

(cid:21)
(i))

= ∇f(i)(xk

(i)).

Using the deﬁnitions in (2) and (10), we have the conclusion.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

The next lemma describes a progress in one iteration of (9a)-(9d).

Lemma 4 Suppose that Assumption 1 and conditions (7) and (15) hold. Then for algorithm (9a)-(9d), we have

(cid:20)
f (xk+1) − f (x∗) +

ESk

(cid:21)
(cid:10)λ∗, U xk+1(cid:11)

1
α

≤

(cid:18) 1
2α
1
2α
1
2τ

+

+

−

(cid:19)

µ
2

(cid:107)xk − x∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)xk+1 − x∗(cid:107)2(cid:3)

(cid:0)(cid:107)λk − λ∗(cid:107)2 − ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)(cid:1)

ESk

(cid:2)(cid:107)∇f (xk) − ∇k(cid:107)2(cid:3) −

1
2α

(cid:107)V xk(cid:107)2 −

(cid:18) 1
4α

−

τ + Lm
2

(cid:19)

ESk

(cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3),

for some τ > 0 and Lm = max{Lf , κµ}.

Proof 3 From the Lf -smoothness of f (x) and the deﬁnition of Lm, we have

f (xk+1) ≤f (xk) + (cid:10)∇f (xk), xk+1 − xk(cid:11) +

Lm
2

(cid:107)xk+1 − xk(cid:107)2

=f (xk) + (cid:10)∇f (xk) − ∇k, xk+1 − xk(cid:11) + (cid:10)∇k, xk+1 − xk(cid:11) +

Lm
2

(cid:107)xk+1 − xk(cid:107)2

a
≤f (xk) +

τ + Lm
2
+ (cid:10)∇k, xk+1 − x∗(cid:11) + (cid:10)∇k, x∗ − xk(cid:11) ,

(cid:107)∇f (xk) − ∇k(cid:107)2 +

1
2τ

(cid:107)xk+1 − xk(cid:107)2

where we use Young’s inequality in

a
≤. Since

∇k =

1
α

(xk − xk+1) −

1
α

U λk −

1
α

V 2xk,

λk+1 = λk + U xk+1,

from (9b) and (9c), we have

(cid:10)∇k, xk+1 − x∗(cid:11)

1
α
1
α
1
α

b=

c=

=

d
≤

1
α
1
α

−

1
2α

+

−

1
2α

(cid:10)xk − xk+1, xk+1 − x∗(cid:11) −

(cid:10)xk − xk+1, xk+1 − x∗(cid:11) −

(cid:10)λk, U xk+1(cid:11) −

(cid:10)V xk, V xk+1(cid:11)

1
α

(cid:10)λ∗, U xk+1(cid:11)

(cid:10)λk − λ∗, λk+1 − λk(cid:11) −

1
α
(cid:0)(cid:107)xk − x∗(cid:107)2 − (cid:107)xk+1 − x∗(cid:107)2 − (cid:107)xk+1 − xk(cid:107)2(cid:1)

(cid:10)V xk, V xk+1(cid:11)

(cid:0)(cid:107)λk − λ∗(cid:107)2 − (cid:107)λk+1 − λ∗(cid:107)2 + (cid:107)λk+1 − λk(cid:107)2(cid:1)

1
2α
1
2α
(cid:0)(cid:107)xk − x∗(cid:107)2 − (cid:107)xk+1 − x∗(cid:107)2(cid:1) +

(cid:0)(cid:107)V xk(cid:107)2 + (cid:107)V xk+1(cid:107)2 − (cid:107)V xk − V xk+1(cid:107)2(cid:1) −

−

1
2α

(cid:107)V xk(cid:107)2 −

1
4α

(cid:107)xk+1 − xk(cid:107)2 −

(cid:10)λ∗, U xk+1(cid:11)

1
α

(cid:0)(cid:107)λk − λ∗(cid:107)2 − (cid:107)λk+1 − λ∗(cid:107)2(cid:1)

1
2α
1
(cid:10)λ∗, U xk+1(cid:11) ,
α

(23)

(24)

(25)

(26)

(27)

where we use U x∗ = 0, V x∗ = 0, and the symmetry of U and V in b=, (26) in c=, (cid:107)λk+1 − λk(cid:107)2 = (cid:107)U xk+1(cid:107)2 ≤ (cid:107)V xk+1(cid:107)2
and (cid:107)V (xk+1 − xk)(cid:107)2 ≤ 1

d
≤. On the other hand, from (22) and the strong convexity of f (x), we have

2 (cid:107)xk+1 − xk(cid:107)2 in

ESk

(cid:2)(cid:10)∇k, x∗ − xk(cid:11)(cid:3) = (cid:10)∇f (xk), x∗ − xk(cid:11) ≤ f (x∗) − f (xk) −

µ
2

(cid:107)xk − x∗(cid:107)2.

(28)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Plugging (27) and (28) into (24), we have

ESk

(cid:2)f (xk+1)(cid:3)
µ
2

≤ f (x∗) −

(cid:107)xk − x∗(cid:107)2 +

(cid:0)(cid:107)xk − x∗(cid:107)2 − ESk

+

−

1
2α
1
2α

(cid:107)V xk(cid:107)2 −

(cid:18) 1
4α

−

τ + Lm
2

(cid:2)(cid:107)∇f (xk) − ∇k(cid:107)2(cid:3)

ESk

1
2τ
(cid:2)(cid:107)xk+1 − x∗(cid:107)2(cid:3)(cid:1) +
(cid:19)

1
2α
(cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3) −

ESk

(cid:0)(cid:107)λk − λ∗(cid:107)2 − ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)(cid:1)

ESk

(cid:2)(cid:10)λ∗, U xk+1(cid:11)(cid:3) .

1
α

Rearranging the terms, we have the conclusion.

To prove the linear convergence, we should make the constant before (cid:107)λk − λ∗(cid:107)2 in (23) smaller than that before
(cid:107)λk+1 − λ∗(cid:107)2, which is established in the next lemma.

Lemma 5 Suppose that Assumption 1 and conditions (7) and (15) hold. Let α = 1
(9a)-(9d), we have

28Lm

and λ0 = 0. Then for algorithm

(cid:20)
f (xk+1) − f (x∗) +

ESk

1
2

(cid:21)
(cid:10)λ∗, U xk+1(cid:11)

1
α

−

(cid:18) 1
2α
(cid:18) 1
2α

+

µ
2

−

(cid:19)

(cid:107)xk − x∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)xk+1 − x∗(cid:107)2(cid:3)

(cid:19)

1 − ν
4κLmα2

(cid:107)λk − λ∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

(cid:18)

+

Lf
6Lmb

f (xk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U xk(cid:11)

1
α

+

Lf
6Lmb

(cid:18)

f (wk) − f (x∗) +

(cid:10)λ∗, U wk(cid:11)

(cid:19)

,

1
α

≤

with ν = 3140
3141 .

Proof 4 From the optimality condition in (20) and the smoothness property in (18), we have

f (xk+1) − f (x∗) +

1
α

(cid:10)λ∗, U xk+1(cid:11) = f (xk+1) − f (x∗) − (cid:10)∇f (x∗), xk+1 − x∗(cid:11)

(cid:107)∇f (xk+1) − ∇f (x∗)(cid:107)2 =

2Lmα2 (cid:107)α∇f (xk+1) + U λ∗(cid:107)2
(cid:13)xk+1 −xk +U (λk −λ∗)+V 2xk +α∇k −α∇f (xk)+α∇f (xk)−α∇f (xk+1)(cid:13)
(cid:13)
2
(cid:13)

1

1
2Lm
1
2Lmα2
1 − ν
2Lmα2 (cid:107)U (λk − λ∗)(cid:107)2 −

≥

a=

b
≥

c
≥

1
2Lmα2

(cid:18) 1
ν

(cid:19)

− 1

(cid:13)
(cid:13)xk+1 − xk + V 2xk

1 − ν
2κLmα2 (cid:107)λk − λ∗(cid:107)2 −

−

2
Lm

(cid:18) 1
ν

(cid:19)

− 1

(cid:18) 2
Lmα2 + 2Lm

+α∇k − α∇f (xk) + α∇f (xk) − α∇f (xk+1)(cid:13)
2
(cid:13)
(cid:19) (cid:18) 1
ν
(cid:18) 1
ν

(cid:107)∇k − ∇f (xk)(cid:107)2 −

(cid:107)xk+1 − xk(cid:107)2

(cid:107)V xk(cid:107)2,

2
Lmα2

− 1

− 1

(cid:19)

(cid:19)

(29)

(30)

where we use (25) in a=, (cid:107)a−b(cid:107)2 ≥ (1−ν)(cid:107)a(cid:107)2−( 1
i=1 (cid:107)ai(cid:107)2,
c
the Lf -smoothness of f (x), and (cid:107)V 2xk(cid:107)2 ≤ (cid:107)V xk(cid:107)2 in
≥, where the requirement of λk − λ∗ ∈ Span(U ) in (15) holds
since λ0 ∈ Span(U ), the update in (9c), and λ∗ ∈ Span(U ). Dividing both sides of (30) by 2 and plugging it into (23), we

i=1 ai(cid:107)2 ≤ n (cid:80)n

b
≥ for some 0 < ν < 1, (15), (cid:107) (cid:80)n

ν −1)(cid:107)b(cid:107)2 in

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

have

(cid:20)
f (xk+1) − f (x∗) +

ESk

1
2

(cid:21)
(cid:10)λ∗, U xk+1(cid:11)

1
α

(cid:19)

(cid:107)xk − x∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)xk+1 − x∗(cid:107)2(cid:3)

≤

−

+

(cid:18) 1
2α
(cid:18) 1
2α
(cid:18) 1
2τ
(cid:18) 1
4α

+

−

µ
2

−

+

−

1 − ν
4κLmα2
(cid:18) 1
1
Lm
ν
τ + Lm
2

(cid:19)

(cid:107)λk − λ∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

(cid:19)(cid:19)

− 1

ESk

(cid:2)(cid:107)∇f (xk) − ∇k(cid:107)2(cid:3) −

−

(cid:18) 1
Lmα2 + Lm

(cid:19) (cid:18) 1
ν

(cid:19)(cid:19)

− 1

ESk

(cid:19)(cid:19)

− 1

(cid:107)V xk(cid:107)2

−

(cid:18) 1
2α

(cid:18) 1
ν

1
Lmα2
(cid:2)(cid:107)xk − xk+1(cid:107)2(cid:3).

Letting τ = 12.5Lm, ν = 3140
ν − 1) ≤ 1
( 1
and 1

3141 , and α = 1
24Lm

Lmα2 ( 1
, and using (21), we have the conclusion.

, such that 1

2τ + 1
Lm

2α − 1

28Lm

ν − 1) ≥ 0, 1

4α − τ +Lm

2 − (

Now, we are ready to prove Theorem 1.

Proof 5 From step (9d), we have

1

Lmα2 + Lm)( 1

ν − 1) ≥ 0,

(cid:104)
f(i)(wk+1

(cid:68)
∇f(i)(x∗), wk+1

(i) ) −
(cid:68)
∇f(i)(x∗), xk

(i)) −

(i) − x∗(cid:69)(cid:17)

(i) − x∗(cid:69)(cid:105)
(cid:18)

f(i)(xk

+

1 −

E

wk+1
(i)
b
n

=

(cid:16)

(cid:19) (cid:16)

b
n

f(i)(wk

(i)) −

(cid:68)
∇f(i)(x∗), wk

(i) − x∗(cid:69)(cid:17)

.

From the deﬁnitions in (2), the optimality condition in (20), and the fact that each wk+1
(i)
node, we further have

is computed independently at each

(cid:20)
f (wk+1) − f (x∗) +

(cid:10)λ∗, U wk+1(cid:11)

(cid:21)

1
α

Ewk+1
(cid:18)

=

b
n

f (xk) − f (x∗) +

(cid:10)λ∗, U xk(cid:11)

(cid:19)

(cid:18)

+

1 −

1
α

b
n

(cid:19) (cid:18)

f (wk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U wk(cid:11)

.

1
α

(31)

Multiplying both sides of (31) by n
the easy-to-identity equation n
Lf
Lmb ≤ 1, we have

b ( 1

10n − Lf

b ( 1
2 − b

2 − b
10n − Lf

6Lmb ) and adding it to (29), taking expectation with respect to ξk, from
10n ) under the condition

6Lmb )(1 − b

10n − Lf

6Lmb ≤ n

n ) + Lf

2 − b

b ( 1

6Lmb )(1 − b

1
2

Eξk+1

(cid:20)
f (xk+1) − f (x∗) +

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

1
α

Eξk+1

(cid:20)
f (wk+1) − f (x∗) +

(cid:21)
(cid:10)λ∗, U wk+1(cid:11)

1
α

n
b
1
2α

+

+
(cid:18) 1
2

(cid:18) 1
2

−

Eξk+1

−

b
10n

≤

(cid:19)

−

b
Lf
10n
6Lmb
(cid:2)(cid:107)xk+1 − x∗(cid:107)2(cid:3) +
(cid:19)

1
2α

(cid:20)
f (xk) − f (x∗) +

Eξk

(cid:21)

(cid:10)λ∗, U xk(cid:11)

Eξk+1

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

+

+

(cid:18) 1
2

n
b
(cid:18) 1
2α

−

−

b
10n
(cid:19)
µ
2

−

Eξk

(cid:19) (cid:18)

1 −

Lf
6Lmb
(cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) +

b
10n
(cid:18) 1
2α

Eξk

(cid:20)

f (wk) − f (x∗) +

(cid:21)

(cid:10)λ∗, U wk(cid:11)

1
α

(cid:19)

−

1 − ν
4κLmα2

Eξk

(cid:2)(cid:107)λk − λ∗(cid:107)2(cid:3)

1
α
(cid:19)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

a
≤

(cid:26) 1
2

Eξk

(cid:20)
f (xk) − f (x∗) +

−

−

b
10n

Lf
6Lmb
(cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) +

(cid:18) 1
2

+

+

n
b
1
2α

Eξk
(cid:26)

1
α
(cid:19)

(cid:21)
(cid:10)λ∗, U xk(cid:11)

(cid:20)
f (wk) − f (x∗) +

Eξk

(cid:21)
(cid:10)λ∗, U wk(cid:11)

1
α

(cid:2)(cid:107)λk − λ∗(cid:107)2(cid:3)

(cid:27)

1
2α

Eξk

× max

1 −

b
5n

, 1 −

b
10n

, 1 − αµ, 1 −

(cid:27)

,

1 − ν
2κLmα

28Lm

α (cid:104)λ∗, U x(cid:105) ≥ f (x∗) + 1
, we know the algorithm needs O(( n

α (cid:104)λ∗, U x∗(cid:105) for any x, and 1
where we use the fact f (x) + 1
(cid:15) ) b= O(( n
b + Lm
µ + κ) log 1
setting of α = 1
(cid:15) ) iterations to ﬁnd
(cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (cid:15), where we use Lm = max{Lf , κµ} in b=. Recall that each iteration requires the
xk such that Eξk
time of one communication round and b stochastic gradient evaluations in average, see Remark 1. So the communication
complexity is O(( n
(cid:15) ) =
O((n + bLm
µ ) log 1

(cid:15) ), and the stochastic gradient computation complexity is O((n + bLf

10n − Lf
µ + κ) log 1

2 − b
b + Lf

b + Lf
(cid:15) ).

µ + bκ) log 1

µ + κ) log 1

6Lmb > 0 in

a
≤. From the

µ , n}, we have κµ ≤ max{Lf , nµ} and b = max{Lf ,nµ}

Case 1. If κ ≤ max{ Lf
Lf ≤ Lf ≤ max{Lf , nµ}. We also have b = max{Lf ,nµ}
and Lf ≥ µ. This veriﬁes that the setting of b is meaningful. On the other hand, since b = max{Lf ,nµ}
nµ = max{ Lf

max{Lf ,κµ} ≥ max{Lf ,nµ}

max{Lf ,κµ} ≤ max{Lf ,nµ}

Lf
max{Lf ,nµ}
and the stochastic gradient computation complexity is O(( Lf

µ , κ}. Then the communication complexity is O(( Lf
µ = max{Lf ,nµ}
(cid:15) ), where we use bLm

µ + n) log 1

nLm
max{Lf ,nµ}

≤ 1 and n

max{Lf ,nµ}

≤ nLm

b =

Lm

Lf

µ

, we have Lf

Lmb =
µ + κ) log 1
(cid:15) ),
µ + n.

≤ Lf

≤ n, where we use Lf ≤ nLf given in (4)

= 1, where we use

Lm

, similar to the above analysis, we also have Lf

If we choose b ≥ max{Lf ,nµ}
µ , κ}. So the
communication complexity remains unchanged, but the stochastic gradient computation complexity increases. This veriﬁes
Remark 2(4). Specially, if we let b = max{Lf ,nµ}
τ (κb+κ)
κs+n , we have bLm
max{Lf ,κµ}
Lmb = Lf
µ + n + κ) log 1

Case 2. If κ ≥ max{ Lf
gradient computation complexity are both O(( Lf

κµ ≤ 1. The communication complexity and stochastic

µ , n}, letting b = 1, we have Lf

≤ Lf
(cid:15) ) = O(( Lf

(cid:15) ), where we use κ ≥ n.

Lmb ≤ 1 and n

µ = τ (κb + κ).

b ≤ max{ Lf

µ + κ) log 1

Lm

Now, we prove Theorem 2, which reduces the stochastic gradient computation complexity from O(( Lf
O(( Lf

(cid:15) ) in the case of κ ≥ max{ Lf

µ , n} by the zero-sample strategy.

µ + n) log 1

µ + κ) log 1

(cid:15) ) to

(cid:48)
Proof 6 From the deﬁnitions in (14), it can be easily checked that L
(i) =
nL(i)+nµn(cid:48)−nL(i)
nµ
n(cid:48)
nLf
κ ,nµ}

(cid:48)
(cid:48)
(i) = nµ, and b =
= nµ, L
f = L

f ,n(cid:48)µ(cid:48)}
f ,κµ(cid:48)} =

max{L(cid:48)
max{L(cid:48)

max{

(cid:80)n(cid:48)

j=1 L(i),j
n(cid:48)
= nµ

(cid:80)n

j=1 L(i),j +(cid:80)n(cid:48)

j=n+1 L(i),j

=

=
nµ = 1, where we use κ ≥ κs ≥ Lf
µ .

n(cid:48)

Replacing n, Lf , µ, and Lf in the proof of Theorem 1 by n(cid:48), L(cid:48)
L(cid:48)
Lmb = nµ
Lm = max{L(cid:48)
f
(cid:15) ) iterations to ﬁnd xk such that Eξk
O(κ log 1
gradient computation complexity are both O(κ log 1

(cid:48)
f , µ(cid:48), and L
f given in (14), respectively, we know
L(cid:48)
(cid:15) ) = O((κ + Lf
µ(cid:48) + κ) log 1
f
(cid:15) ) =
b +
(cid:2)(cid:107)xk − x∗(cid:107)2(cid:3) ≤ (cid:15). So the communication complexity and the stochastic
=

(cid:15) ). Since we select the samples in (cid:2)1, n(cid:3) with probability

nµ = 1. So the algorithm needs O(( n(cid:48)

f , κµ(cid:48)} = nµ and

µ ) log 1

(cid:80)n(cid:48)

(cid:80)n

j=1 L(i),j
j=1 L(i),j

nLf

µκ κ log 1

nµκ = Lf
is O( Lf
µ log 1
takes O(n 1
(cid:15) ) = O(n log 1
κ κ log 1
computation complexity is O(( Lf

µκ , and the zero samples do not spend the computation time, so the valid number of stochastic gradient evaluations
κ , which
(cid:15) ) valid stochastic gradient evaluations in total. So the ﬁnal valid stochastic gradient
µ + n) log 1

(cid:15) ). On the other hand, we compute the full batch gradient with probability b

(cid:15) ) = O( Lf

n(cid:48) = 1

(cid:15) ).

At last, we explain that the zero samples do not destroy the proof of Theorem 1. For the zero sample f(i),j(x) = 0, we have
∇f(i),j(x) = 0. So it also satisﬁes the convexity and L(i),j-smooth property (18) even for positive L(i),j. We can check

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

that (21) and (22) also hold. In the proofs of Lemmas 4 and 5, we use the smoothness and strong convexity of f (cid:48)
explained in Section 2.3, which also hold.

(i)(x), as

4.2. Accelerated VR-EXTRA and VR-DIGing

From Lemma D.2 in (Allen-Zhu, 2018) and similar to Lemma 2, we have

ESk

(cid:2)(cid:107)∇k − ∇f (yk)(cid:107)2(cid:3) ≤

2Lf
b

(cid:0)f (wk) − f (yk) − (cid:10)∇f (yk), wk − yk(cid:11)(cid:1) .

Similar to (22), we also have

ESk

(cid:2)∇k(cid:3) = ∇f (yk).

(32)

(33)

The following lemma is the counterpart of Lemma 4, which gives a progress in one iteration of procedure (16a)-(16f).

Lemma 6 Suppose that Assumption 1 and conditions (7) and (15) hold. Let θ1 + θ2 ≤ 1. Then for algorithm (16a)-(16f),
we have

(cid:20)
f (xk+1) − f (x∗) +

ESk

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

1
α

≤ (1 − θ1 − θ2)

(cid:18)

f (xk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U xk(cid:11)

1
α

(cid:18)

f (wk) − f (x∗) +

+ θ2

(cid:19)

(cid:10)λ∗, U wk(cid:11)

1
α

(cid:18) Lf
τ b

(cid:19)

− θ2

(cid:0)f (wk) − f (yk) − (cid:10)∇f (yk), wk − yk(cid:11)(cid:1)

(cid:19)

+

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

+

+

+

−

θ2
1
2α
1
2α
θ2
1
2α

(cid:18) θ2
1
2α
1
2α

(cid:107)zk − x∗(cid:107)2 −

(cid:107)λk − λ∗(cid:107)2 −
(cid:18) θ2
1
4α

(cid:107)V zk(cid:107)2 −

−

τ θ2

1 + Lf θ2
1
2

(cid:19)

ESk

(cid:2)(cid:107)zk+1 − zk(cid:107)2(cid:3) −

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − yk(cid:107)2(cid:3),

for some τ > 0.

Proof 7 From the Lf -smoothness of f (x), similar to (24), we have

f (xk+1) ≤f (yk) + (cid:10)∇f (yk), xk+1 − yk(cid:11) +

(cid:107)xk+1 − yk(cid:107)2

≤f (yk) +

a=f (yk) +

(cid:107)∇f (yk) − ∇k(cid:107)2 +

(cid:107)∇f (yk) − ∇k(cid:107)2 +

1
2τ
1
2τ

(cid:10)∇k, zk+1 − z∗(cid:11) + θ1

(cid:10)∇k, z∗ − zk(cid:11) .

Lf
2
τ + Lf
2
1 + Lf θ2
τ θ2
1
2

(cid:107)zk+1 − zk(cid:107)2

(cid:107)xk+1 − yk(cid:107)2 + (cid:10)∇k, xk+1 − yk(cid:11)

+ θ1

where we use

in a=, which comes from (16e). Since

xk+1 − yk = θ1(zk+1 − zk)

∇k =

θ1
α

(zk − zk+1) + µ(yk − zk+1) −

1
α

U λk −

θ1
α

V 2zk,

λk+1 = λk + θ1U zk+1,

(34)

(35)

(36)

(37)

(38)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

from (16c) and (16d), similar to (27), we have

θ1

(cid:10)∇k, zk+1 − x∗(cid:11)
θ2
1
α

=

(cid:10)zk − zk+1, zk+1 − x∗(cid:11) + µθ1

(cid:10)yk − zk+1, zk+1 − x∗(cid:11)

−

θ2
1
α

−

θ2
1
2α

+

+

−

b=

=

c
≤

(cid:10)λk, U zk+1(cid:11) −

θ1
α
(cid:10)zk − zk+1, zk+1 − x∗(cid:11) + µθ1

θ2
1
α

(cid:10)V zk, V zk+1(cid:11)

(cid:10)yk − zk+1, zk+1 − x∗(cid:11)

(cid:10)λ∗, U zk+1(cid:11) −

θ2
θ1
1
α
α
(cid:0)(cid:107)zk − x∗(cid:107)2 − (cid:107)zk+1 − x∗(cid:107)2 − (cid:107)zk+1 − zk(cid:107)2(cid:1)

(cid:10)λk − λ∗, λk+1 − λk(cid:11) −

1
α

(cid:10)V zk, V zk+1(cid:11)

(cid:0)(cid:107)V zk(cid:107)2 + (cid:107)V zk+1(cid:107)2 − (cid:107)V zk+1 − V zk(cid:107)2(cid:1) −

(cid:10)λ∗, U zk+1(cid:11)

θ1
α

(cid:0)(cid:107)yk − x∗(cid:107)2 − (cid:107)zk+1 − x∗(cid:107)2 − (cid:107)zk+1 − yk(cid:107)2(cid:1)

(cid:0)(cid:107)λk − λ∗(cid:107)2 − (cid:107)λk+1 − λ∗(cid:107)2 + (cid:107)λk+1 − λk(cid:107)2(cid:1)

µθ1
2
1
2α
θ2
1
2α
(cid:0)(cid:107)zk − x∗(cid:107)2 − (cid:107)zk+1 − x∗(cid:107)2(cid:1) +

µθ1
2
(cid:0)(cid:107)λk − λ∗(cid:107)2 − (cid:107)λk+1 − λ∗(cid:107)2(cid:1) −

(cid:107)zk+1 − yk(cid:107)2 −

(cid:10)λ∗, U zk+1(cid:11) ,

θ1
α

θ2
1
2α

+

−

1
2α
µθ1
2

(cid:0)(cid:107)yk − x∗(cid:107)2 − (cid:107)zk+1 − x∗(cid:107)2(cid:1)

θ2
1
2α

(cid:107)V zk(cid:107)2 −

θ2
1
4α

(cid:107)zk+1 − zk(cid:107)2

where we use (38) in b=, (cid:107)λk+1 − λk(cid:107)2 = (cid:107)θ1U zk+1(cid:107)2 ≤ θ2
the other hand, from (33), we have

1(cid:107)V zk+1(cid:107)2 and (cid:107)V (zk+1 − zk)(cid:107)2 ≤ 1

2 (cid:107)zk+1 − zk(cid:107)2 in

(39)

c
≤. On

(cid:10)∇f (yk), x∗ − zk(cid:11)

(cid:2)(cid:10)∇k, x∗ − zk(cid:11)(cid:3) = θ1

θ1ESk
d= (cid:10)∇f (yk), θ1x∗ + θ2wk + (1 − θ1 − θ2)xk − yk(cid:11)
= θ1
e
≤ θ1

(cid:10)∇f (yk), x∗ − yk(cid:11) + (1 − θ1 − θ2) (cid:10)∇f (yk), xk − yk(cid:11) + θ2
(cid:16)
+(1−θ1−θ2)(f (xk)−f (yk))+θ2
f (x∗)−f (yk)−

(cid:107)yk−x∗(cid:107)2(cid:17)

µ
2

(cid:10)∇f (yk), wk − yk(cid:11)
(cid:10)∇f (yk), wk−yk(cid:11)

= θ1f (x∗)+(1−θ1 −θ2)f (xk)−(1−θ2)f (yk)−

µθ1
2

(cid:107)yk −x∗(cid:107)2 +θ2

(cid:10)∇f (yk), wk −yk(cid:11) ,

(40)

where we use (16a) in d=, and the strong convexity of f (x) in e=. Plugging (39) and (40) into (35), and using (32), we have

ESk

(cid:2)f (xk+1)(cid:3)

≤ θ1f (x∗) + (1 − θ1 − θ2)f (xk) + θ2f (yk) −

µθ1
2
(cid:0)f (wk) − f (yk) − (cid:10)∇f (yk), wk − yk(cid:11)(cid:1)

(cid:107)yk − x∗(cid:107)2 + θ2

(cid:10)∇f (yk), wk − yk(cid:11)

(cid:0)(cid:107)zk − x∗(cid:107)2 − ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)(cid:1) +

(cid:0)(cid:107)λk − λ∗(cid:107)2 − ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)(cid:1) −

µθ1
2
θ1
α

ESk

(cid:2)(cid:10)λ∗, U zk+1(cid:11)(cid:3)

(cid:0)(cid:107)yk − x∗(cid:107)2 − ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)(cid:1)

(cid:107)V zk(cid:107)2 −

(cid:18) θ2
1
4α

−

τ θ2

1 + Lf θ2
1
2

(cid:19)

ESk

(cid:2)(cid:107)zk+1 − zk(cid:107)2(cid:3) −

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − yk(cid:107)2(cid:3)

+

+

+

−

Lf
τ b
θ2
1
2α
1
2α
θ2
1
2α

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

f
= θ1f (x∗) + (1 − θ1 − θ2)f (xk) + θ2f (wk)

(cid:18) Lf
τ b

(cid:19)

− θ2

(cid:0)f (wk) − f (yk) − (cid:10)∇f (yk), wk − yk(cid:11)(cid:1)

(cid:107)zk − x∗(cid:107)2 −

(cid:18) θ2
1
2α
(cid:0)(cid:107)λk − λ∗(cid:107)2 − ESk

(cid:19)

+

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)(cid:1)

ESk

(cid:2)(cid:10)λ∗, U xk+1 − θ2U wk − (1 − θ1 − θ2)U xk(cid:11)(cid:3)
1 + Lf θ2
1
2

(cid:18) θ2
1
4α

ESk

τ θ2

−

(cid:19)

(cid:107)V zk(cid:107)2 −

(cid:2)(cid:107)zk+1 − zk(cid:107)2(cid:3) −

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − yk(cid:107)2(cid:3),

+

+

+

−

−

θ2
1
2α
1
2α
1
α
θ2
1
2α

where we use (16a) and (16e) in

f
=. Rearranging the terms, we have the conclusion.

Similar to Lemma 5, we establish the smaller constant before (cid:107)λk − λ∗(cid:107)2 than that before (cid:107)λk+1 − λ∗(cid:107)2 in the next lemma.

Lemma 7 Suppose that Assumption 1 and conditions (7) and (15) hold. Choose b such that θ2 = Lf
α = 1

, and λ0 = 0. Then for algorithm (16a)-(16f), we have

2Lf b ≤ 1

2 . Let θ1 ≤ 1
2 ,

10Lf

(cid:18)

1 −

θ1
2

(cid:19)

(cid:20)
f (xk+1) − f (x∗) +

ESk

≤ (1 − θ1 − θ2)

(cid:18)

f (xk) − f (x∗) +

1
α
1
α

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

(cid:19)

(cid:10)λ∗, U xk(cid:11)

(cid:18)

+ θ2

f (wk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U wk(cid:11)

1
α

θ2
1
2α
(cid:18) 1
2α

(cid:107)zk − x∗(cid:107)2 −

(cid:18) θ2
1
2α

+

µθ1
2

(cid:19)

ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)

(cid:19)

−

(1 − ν)θ1
4κLf α2

(cid:107)λk − λ∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3),

+

+

with ν = 127
128 .

(41)

Proof 8 From (18) and (20), similar to (30), we have

f (xk+1) − f (x∗) +

1
α

(cid:10)λ∗, U xk+1(cid:11) ≥

2Lf α2 (cid:107)α∇f (xk+1) + U λ∗(cid:107)2
(cid:13)
(cid:13)αµ(zk+1 − yk) + θ1(zk+1 − zk) + U (λk − λ∗) + θ1V 2zk

1

a=

1
2Lf α2

+α∇k − α∇f (yk) + α∇f (yk) − α∇f (xk+1)(cid:13)
2
(cid:13)

≥

1 − ν
2Lf α2 (cid:107)U (λk − λ∗)(cid:107)2 −

1
2Lf α2

(cid:19)

− 1

(cid:18) 1
ν

(cid:13)
(cid:13)αµ(zk+1 − yk) + θ1(zk+1 − zk)

+θ1V 2zk + α∇k − α∇f (yk) + α∇f (yk) − α∇f (xk+1)(cid:13)
2
(cid:13)

b
≥

1 − ν
2κLf α2 (cid:107)λk − λ∗(cid:107)2 −

5µ2
2Lf

(cid:18) 1
ν

(cid:19)

− 1

(cid:107)zk+1 − yk(cid:107)2 −

−

5
2Lf

(cid:18) 1
ν

(cid:19)

− 1

(cid:107)∇k − ∇f (yk)(cid:107)2 −

(cid:18) 5θ2
1
2Lf α2 +

5Lf θ2
1
2

5θ2
1
2Lf α2
(cid:19) (cid:18) 1
ν

(cid:19)

− 1

(cid:18) 1
ν

(cid:107)V zk(cid:107)2

(cid:19)

− 1

(cid:107)zk+1 − zk(cid:107)2,

(42)

where we use (37) in a=, (15), the Lf -smoothness of f (x), (36), and (cid:107)V 2zk(cid:107)2 ≤ (cid:107)V zk(cid:107)2 in

b
≥. Multiplying both sides of

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

(42) by θ1
(cid:18)

2 and plugging it into (34), using (32), we have

1 −

(cid:19)

(cid:20)
f (xk+1) − f (x∗) +

ESk

θ1
2

≤ (1 − θ1 − θ2)

(cid:18)

f (xk) − f (x∗) +

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

(cid:19)

(cid:10)λ∗, U xk(cid:11)

1
α
1
α

(cid:18)

f (wk) − f (x∗) +

+ θ2

(cid:18) Lf
τ b

+

5Lf θ1
2bLf

(cid:107)zk − x∗(cid:107)2 −

− 1

(cid:18) 1
ν
(cid:18) θ2
1
2α

+

+

+

−

−

θ2
1
2α
(cid:18) 1
2α
(cid:18) θ2
1
2α
(cid:18) θ2
1
4α

−

−

−

(1 − ν)θ1
4κLf α2
5θ3
1
4Lf α2
τ θ2

(cid:18) 1
ν
1 + Lf θ2
1
2

1
α
(cid:19)

(cid:19)

(cid:10)λ∗, U wk(cid:11)

(cid:19)

− θ2

(cid:0)f (wk) − f (yk) − (cid:10)∇f (yk), wk − yk(cid:11)(cid:1)

(cid:19)

+

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)

(cid:19)

(cid:107)λk − λ∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

(cid:19)(cid:19)

− 1

(cid:107)V zk(cid:107)2 −

−

(cid:18) 5θ3
1
4Lf α2 +

−

(cid:18) µθ1
2
(cid:19) (cid:18) 1
5Lf θ3
1
ν
4

5µ2θ1
4Lf

(cid:19)(cid:19)

− 1

(cid:18) 1
ν

ESk

(cid:2)(cid:107)zk+1 − yk(cid:107)2(cid:3)

(cid:19)(cid:19)

− 1

ESk

(cid:2)(cid:107)zk+1 − zk(cid:107)2(cid:3).

2 , θ2 = Lf
Letting θ1 ≤ 1
ν − 1) ≥ 0, and θ2
2 − 5µ2θ1
µθ1
( 1

2Lf b , τ = 3Lf , ν = 127
4α − τ θ2
1 +Lf θ2
2

128 , and α = 1
10Lf
− ( 5θ3

4Lf α2 + 5Lf θ3

such that Lf
)( 1

4Lf

4

1

1

1

1

τ b + 5Lf θ1

2Lf b ( 1

ν −1)−θ2 ≤ 0, θ2

2α − 5θ3

4Lf α2 ( 1

1

1

ν −1) ≥ 0,

ν − 1) ≥ 0, we have the conclusion.

Now, we are ready to prove Theorem 3.

√
√

Proof 9 Let b ≥ max{ max{
max{

nLf /µ,n}
κLf /µ,κ}

, Lf
Lf

}, then we know θ2 = Lf

have max{ max{
max{

nLf /µ,n}
κLf /µ,κ}

, Lf
Lf

} ≤ max{max{

√
√

(cid:113) nLf

µ , n}

(cid:113) µ
κLf

2Lf b ≤ 1
, Lf
Lf

} = max{

2 and b ≥ 1, where we use Lf ≥ Lf . We also

(cid:113) nLf
κLf

(cid:113) n2µ
κLf

,

, Lf
Lf

}

a
≤ n, where

a
≤ uses

κ ≥ 1 and µ ≤ Lf ≤ Lf ≤ nLf given in (4). This veriﬁes that the setting of b is meaningful.

Multiplying both sides of (31) by

(cid:18)

1 −

(cid:19)

θ1
2

ESk

b

20κ

θ2
n − θ1
(cid:20)
f (xk+1) − f (x∗) +

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

1
α

and adding it to (41), we have

(cid:20)
f (wk+1) − f (x∗) +

Ewk+1

(cid:21)
(cid:10)λ∗, U wk+1(cid:11)

1
α

θ2
n − θ1

b

20κ

+

(cid:32)

≤

1 − θ1 − θ2 +

(cid:32)

(cid:18)

+

θ2 +

1 −

(cid:19)

b
n

(cid:107)zk − x∗(cid:107)2 −

b
n

b

20κ

θ2
n − θ1
θ2
n − θ1
(cid:18) θ2
1
2α

b

+

20κ

(cid:33) (cid:18)

f (xk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U xk(cid:11)

1
α

(cid:33) (cid:18)

f (wk) − f (x∗) +

(cid:19)

(cid:10)λ∗, U wk(cid:11)

1
α

(cid:19)

µθ1
2

ESk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3)

(cid:19)

−

(1 − ν)θ1
4κLf α2

(cid:107)λk − λ∗(cid:107)2 −

1
2α

ESk

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3).

+

+

θ2
1
2α
(cid:18) 1
2α

We can easily check that

1 − θ1 − θ2 +

= 1 − θ1 +

20κ

b

b
n

θ2
n − θ1
nθ1θ2
20bκ
1 − nθ1
20bκ

= 1 − θ1 − θ2 +

b
≤ 1 − θ1 +

θ1
39

= 1 −

θ2
1 − nθ1
20bκ
38
39

θ1,

(43)

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

and

(cid:18)

θ2 +

1 −

(cid:19)

b
n

θ2
n − θ1

b

20κ

= θ2 +

(cid:18)

=

1 −

b
≤ in the following two cases.

where we check
(cid:113) nLf
κLf

max{

(cid:113) n2µ
κLf

,

−

(cid:19)

b
n

θ2
n − θ1

b

20κ

(cid:18)

+

1 −

(cid:19)

θ1
20κ

θ2
n − θ1

b

20κ

(cid:18) θ1
20κ
(cid:19)

θ1
20κ

θ2
n − θ1

b

20κ

,

In the ﬁrst case, if κ ≤ Lf

, Lf
Lf

}. So we have nθ2
20bκ

c= nLf

40Lf b2κ ≤ nLf

40Lf κ

κLf
nLf

= 1

40 and nθ1

µ , we have θ1 = 1
≤ n
20bκ = n
40κ

(cid:113) κµ
Lf

40bκ

2

(cid:113) κµ
Lf
(cid:113) κLf
n2µ

and b ≥

(cid:113) κµ
Lf

=

1

40 , where c= uses the setting of θ2.
κ , n
b ≥ max{

(cid:113) nLf
µ

} ≥ n

κ , Lf

1

Lf

So we get

In the second case, if κ ≥ Lf

κ . So we have nθ2

40bκ ≤ 1

40 and nθ1

20bκ = n

40bκ ≤ 1

µ , we have θ1 = 1
40 , where

2 and
d
≤ uses θ2 ≤ 1
2

b
≤.
d
≤ n

20bκ
b
≤.

derived at the beginning of this proof. So we also get

Taking expectation with respect to ξk on both sides of (43) and rearranging the terms, we have

(cid:18)

1 −

(cid:19)

θ1
2

Eξk+1

(cid:20)
f (xk+1) − f (x∗) +

(cid:10)λ∗, U xk+1(cid:11)

(cid:21)

1
α

(cid:20)
f (wk+1) − f (x∗) +

Eξk+1

(cid:10)λ∗, U wk+1(cid:11)

(cid:21)

1
α

(cid:19)

µθ1
2
(cid:19)

θ1

Eξk

(cid:2)(cid:107)zk+1 − x∗(cid:107)2(cid:3) +

Eξk+1
(cid:20)
f (xk) − f (x∗) +

Eξk+1
(cid:21)
(cid:10)λ∗, U xk(cid:11)

1
2α

(cid:2)(cid:107)λk+1 − λ∗(cid:107)2(cid:3)

(cid:18)

1 −

(cid:19)

θ1
20κ

Eξk

(cid:20)
f (wk) − f (x∗) +

(cid:21)
(cid:10)λ∗, U wk(cid:11)

1
α

(cid:2)(cid:107)zk − x∗(cid:107)2(cid:3) +

(cid:18) 1
2α

−

(1 − ν)θ1
4κLf α2

(cid:19)

Eξk

(cid:2)(cid:107)λk − λ∗(cid:107)2(cid:3)

≤

+

+

(cid:18)

+

+

b

θ2
n − θ1
(cid:18) θ2
1
2α

+

20κ

1 −

38
39
θ2
n − θ1
θ2
1
2α

Eξk

b

20κ

(cid:26)(cid:18)

f
≤

1 −

Eξk

Eξk

(cid:19)

b

θ1
2
θ2
n − θ1
(cid:18) θ2
1
2α
(cid:40)

20κ

+

+

+

µθ1
2
1 − 38
39 θ1
1 − θ1
2

× max

(cid:20)
f (xk) − f (x∗) +

(cid:21)

(cid:10)λ∗, U xk(cid:11)

(cid:20)
f (wk) − f (x∗) +

(cid:21)
(cid:10)λ∗, U wk(cid:11)

(cid:19)

Eξk

(cid:2)(cid:107)zk − x∗(cid:107)2(cid:3) +

(cid:2)(cid:107)λk − λ∗(cid:107)2(cid:3)

(cid:27)

1
2α

Eξk

, 1 −

θ1
20κ

,

1
1 + µα
θ1

, 1 −

(1 − ν)θ1
2κLf α

(cid:41)

,

1
α

1
α
1
α

f
≤ uses the fact that f (x) + 1

where
such that b

n ≥ θ1
20κ .

α (cid:104)λ∗, U x(cid:105) ≥ f (x∗) + 1

α (cid:104)λ∗, U x∗(cid:105) for any x, and nθ1

20bκ ≤ 1

40 in the above analysis

From the settings of θ1 and α and κ ≥ 1, we can easily check 1− 38
1− θ1
2
≤ θ1, and 1 − (1−ν)θ1
) due to µα = µ
1 − µα
10Lf
2θ1
(cid:2)(cid:107)zk − x∗(cid:107)2(cid:3) ≤ (cid:15).

(cid:113) µ
= O(1 − µ
Lf θ1
Lf
+ Lf θ1
(cid:15) ) iterations to ﬁnd zk such that Eξk
µ ) log 1

≤ 1
10

O(( κ
θ1

39 θ1

≤ 1 − 18

39 θ1 ≤ 1 − 18
2κLf α = O(1 − θ1

39

κ = O(1 − θ1
θ1

κ ),

1
1+ µα
θ1

≤

κ ). Thus the algorithm needs

We ﬁrst consider the communication complexity.

1. If κ ≤ Lf

µ , we have θ1 = 1

2

(cid:113) κµ
Lf

and O(( κ
θ1

+ Lf θ1

µ ) log 1

(cid:15) ) = O(

(cid:113) κLf

µ log 1

(cid:15) ). So the communication complexity is

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

(cid:113) κLf

O(

µ log 1

(cid:15) ).

2. If κ ≥ Lf

µ , we have θ1 = 1

2 and O(( κ
θ1

complexity is O(κ log 1

(cid:15) ).

+ Lf θ1

µ ) log 1

(cid:15) ) = O((κ + Lf

µ ) log 1

(cid:15) ) = O(κ log 1

(cid:15) ). So the communication

So the algorithm needs the time of O(max{
that Eξk
Next, we consider the stochastic gradient computation complexity.

(cid:2)(cid:107)zk − x∗(cid:107)2(cid:3) ≤ (cid:15).

µ , κ} log 1

(cid:113) κLf

(cid:15) ) communication rounds to ﬁnd an (cid:15)-optimal solution zk such

1. If max{

(cid:113) nLf

µ , n} Lf

(cid:113) κLf

≥ max{

µ , κ} such that b = max{

max{

nLf /µ,n}
κLf /µ,κ}

√
√

plexity is O(b max{

µ , κ} log 1

(cid:15) ) = O(max{

µ , n} log 1

(cid:15) ).

(cid:113) nLf

Lf
(cid:113) κLf

, the stochastic gradient computation com-

, the stochastic gradient computation complexity is

2. If max{

O(b max{

(cid:113) nLf

µ , n} Lf

(cid:113) κLf

Lf
µ , κ} log 1

µ , κ} such that b = Lf
(cid:15) ).
max{

µ , κ} log 1

(cid:113) κLf

Lf

(cid:113) κLf

≤ max{

Lf

(cid:15) ) = O( Lf
√
√

nLf /µ,n}
κLf /µ,κ}

3. If we choose b > max{ max{
max{

, Lf
Lf

}, the stochastic gradient computation complexity is higher than the above

ones. But the communication complexity remains unchanged. This veriﬁes Remark 3(2).

At last, we discuss the condition max{

(cid:113) nLf

µ , n} Lf

Lf

≥ max{

(cid:113) κLf

µ , κ}. We know that φ(κ) ≡ max{
(cid:40) (cid:113) κLf
µ ,
κ,

if 0 ≤ κ ≤ Lf
µ ,
if κ ≥ Lf
µ ,

(cid:113) κLf

µ , κ}

and

to κ such that φ(κ) =

is a piece-wise increasing function with respect

(cid:40)

φ(κ)

≤ Lf
µ ,
≥ Lf
µ ,

if φ(κ) =

(cid:113) κLf
µ ,

if φ(κ) = κ.

1. If n ≥ Lf

µ , we have max{

µ , n} Lf

Lf

= nLf
Lf

≥ Lf

µ . So the condition max{

(cid:113) nLf

(cid:113) nLf

µ , n} Lf

Lf

≥ max{

(cid:113) κLf

µ , κ} is

equivalent to nLf
Lf

≥ κ.

(cid:113) nLf

µ , n} Lf

Lf

(cid:114)

=

nL2
f
µLf

≤ Lf

µ . So max{

(cid:113) nLf

µ , n} Lf

Lf

≥ max{

(cid:113) κLf

µ , κ} is equivalent to

2. If n ≤ Lf

µ , we have max{

(cid:114)

(cid:113) κLf

nL2
f
µLf

≥

µ , that is, nLf
Lf
5. Numerical Experiments

≥ κ.

Consider the following decentralized regularized logistic regression problem:

min
x∈Rp

m
(cid:88)

i=1

f(i)(x), where

f(i)(x) =

µ
2

(cid:107)x(cid:107)2 +

1
n

n
(cid:88)

j=1

(cid:16)

log

1 + exp(−y(i),jAT

(cid:17)
(i),jx)

,

where the pairs (A(i),j, y(i),j) ∈ Rp × {1, −1} are taken from the RCV1 dataset8 with p = 47236, m = 49, and n = 500.
Denote A(i) = [A(i),1, A(i),2, · · · , A(i),n] ∈ Rp×n as the data matrix on the ith node. For this special problem and dataset,
we observe Lf = maxi
4 + µ, respectively. We test the
performance of the proposed algorithms on different ratios between κs and n. Speciﬁcally, we test on µ = 5 × 10−5,
µ = 5 × 10−6, and µ = 5 × 10−7, which correspond to κs = Lf
µ ≈ 5 × 103, κs ≈ 5 × 104, and κs ≈ 5 × 105, respectively.
Note that n = 500. We also observe nκb
κs

4n + µ ≈ 0.016 + µ and Lf = maxi

4n + µ = 1

≈ 31.9.

(cid:107)A(i)(cid:107)2
F

(cid:107)A(i)(cid:107)2
2

8https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

We test the performance on two kinds of networks: the Erd˝os−R´enyi random graph and the two-dimensional grid graph,
m
where each pair of nodes has a connection with the ratio of 0.2 for the ﬁrst graph, and m nodes are placed in the
grid and each node is connected with its neighbors around it for the second graph. Theoretically, κc = O(1) for the ﬁrst
graph, and κc = O(m log m) for the second graph. Practically, we observe κc = 4.62 and κc = 19.9 for the two graphs,
respectively. We set the weight matrix as W = M −λminI
for both graphs, where M is the Metropolis weight matrix (Boyd
1−λmin
if (i, j) ∈ E,
if i = j,
if (i, j) /∈ E and i (cid:54)= j.

, d(i) is the degree of node i, and λmin < 0 is the

1
max{d(i),d(j)}
1 − (cid:80)
0

et al., 2004) with Mij =

m ×

l∈N(i)




Mil



√

√

smallest negative eigenvalue of M .

for Acc-VR-EXTRA, Acc-VR-EXTRA-CA, Acc-VR-DIGing, and Acc-VR-DIGing-CA, α = 3
Lf

We compare the proposed VR-EXTRA, Acc-VR-EXTRA, Acc-VR-EXTRA-CA, VR-DIGing, Acc-VR-DIGing, and Acc-
VR-DIGing-CA with EXTRA (Shi et al., 2015), DIGing (Nedi´c et al., 2017), GT-SVRG (Xin et al., 2020a), APAPC (Kovalev
et al., 2020b), and accelerated DVR (Acc-DVR) (Hendrikx et al., 2020). For the case of µ = 5 × 10−6, we choose the best
step-sizes α = 1
for
Lf
VR-EXTRA, VR-DIGing, and GT-SVRG, and α = 7
for EXTRA and DIGing. The other parameters are chosen according
Lf
to the theories. We set the parameters of APAPC according to Theorem 2 in (Kovalev et al., 2020b). For Acc-DVR, we
n
follow the suggestions in the experimental section in (Hendrikx et al., 2020) to set the number of inner iterations to
1−pcomm
(one pass over the local dataset), and the batch Lipschitz constant as Lf = 0.01Lf , which leads to better performance of
4n + µ. We follow Algorithm 2 and Theorem 5 in (Hendrikx et al.,
Acc-DVR than the theoretical setting of Lf = maxi
2020) to choose other parameters of Acc-DVR, that is, α = 2
n − µ,
pcomm = (1 + n+κβ
β+µ , and κβ
)−1, pij = 1−pcomm
9. For the case of µ = 5 × 10−7, we choose the
s
b κc
same parameters as above. For the case of µ = 5 × 10−5, we set the step-sizes α = 2
Lf
GT-SVRG, and α = 3
Lf
< 2κc and nκb
nκb
< κ2
κs
κs
according to Remark 3(1).

for VR-EXTRA, VR-DIGing, and
for EXTRA and DIGing, and other parameters are chosen the same as above. Specially, since
c for the grid graph, we set the mini-batch size b = Lf
Lf

for Acc-VR-EXTRA and Acc-VR-DIGing

α(1+1/(4nµ)) }, β = Lf

, η = min{pcomm(β + µ),

Lf κc
b = Lf

s = Lf

(cid:107)A(i)(cid:107)2
2

, κβ

β+µ

κβ

pij

n

Figures 1 and 2 plot the results on the Erd˝os−R´enyi random graph and grid graph, respectively, where f ∗ is approximated
by the minimum value of the objective function over all iterations of all the compared algorithms. We have the following
observations:

1. VR-EXTRA and VR-DIGing need less gradient computations than EXTRA and DIGing to reach the same precision for
all cases of µ. This veriﬁes the efﬁciency of variance reduction in decentralized optimization to reduce the computation
cost.

2. When considering the communication cost, VR-EXTRA and VR-DIGing perform worse than EXTRA and DIGing in
practice, although they have the same communication complexities theoretically. This is reasonable since EXTRA
and DIGing go through all the data at each communication round, while VR-EXTRA and VR-DIGing only use a
mini-batch. We observe the mini-batch size of b = Lf
Lf

≈ 16 in our experiment.

3. Acc-VR-EXTRA and Acc-VR-DIGing perform better than VR-EXTRA and VR-DIGing on both computations and
communications when κs is much larger than n. For example, κs = 1000n in the top plots and κs = 100n in the
middle plots. Otherwise, as seen in the bottom plots with κs = 10n, Acc-VR-EXTRA and Acc-VR-DIGing perform
quite similarly to VR-EXTRA and VR-DIGing, especially on the computations. This veriﬁes that acceleration only
takes effect to reduce the computation cost when κs (cid:29) n.

4. When considering the communication cost, Acc-VR-EXTRA performs similarly to the optimal full batch method of

APAPC. This observation matches the theory that the two methods have the same communication complexity.

5. Acc-VR-EXTRA-CA and Acc-VR-DIGing-CA do not perform well in practice, although they are theoretically optimal.
Thus Acc-VR-EXTRA-CA and Acc-VR-DIGing-CA are more interesting in theory, but they are not suggested in
practice.

9We do not ﬁnd the parameters in APAPC and Acc-DVR in the role as step-sizes in the form of O( 1
Lf

), thus we set the parameters

according to their theories directly.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Figure 1. Comparisons on Erd˝os−R´enyi random graph with µ = 5 × 10−7 (top), µ = 5 × 10−6 (middle), and µ = 5 × 10−5 (bottom).

0500100015002000# of full batch gradient computations10-1510-1010-5100EXTRADIGingAcc-DVRAPAPCVR-DIGingVR-EXTRAGT-SVRGAcc-VR-DIGing-CAAcc-VR-EXTRA-CAAcc-VR-EXTRAAcc-VR-DIGing020004000600080001000012000# of communication rounds10-1510-1010-5100Acc-DVRVR-DIGingVR-EXTRAGT-SVRGDIGingEXTRAAcc-VR-DIGing-CAAcc-VR-EXTRA-CAAcc-VR-DIGingAPAPCAcc-VR-EXTRA0100200300400500600700# of full batch gradient computations10-1510-1010-5100EXTRADIGingAPAPCAcc-DVRVR-DIGingVR-EXTRAGT-SVRGAcc-VR-DIGing-CAAcc-VR-EXTRA-CAAcc-VR-EXTRAAcc-VR-DIGing05001000150020002500300035004000# of communication rounds10-1510-1010-5100Acc-DVRVR-DIGingVR-EXTRAGT-SVRGDIGingEXTRAAcc-VR-DIGing-CAAcc-VR-EXTRA-CAAcc-VR-DIGingAPAPCAcc-VR-EXTRA050100150200# of full batch gradient computations10-1510-1010-5100EXTRADIGingAPAPCAcc-DVRAcc-VR-DIGing-CAAcc-VR-EXTRA-CAVR-DIGingVR-EXTRAGT-SVRGAcc-VR-DIGingAcc-VR-EXTRA0200400600800100012001400# of communication rounds10-1510-1010-5100Acc-DVRVR-DIGingVR-EXTRAAcc-VR-DIGing-CAAcc-VR-EXTRA-CAGT-SVRGAcc-VR-DIGingEXTRADIGingAPAPCAcc-VR-EXTRAVariance Reduced EXTRA and DIGing and Their Optimal Acceleration

Figure 2. Comparisons on grid graph with µ = 5 × 10−7 (top), µ = 5 × 10−6 (middle), and µ = 5 × 10−5 (bottom).

2004006008001000120014001600# of full batch gradient computations10-1510-1010-5100EXTRADIGingAcc-DVRAPAPCVR-DIGingVR-EXTRAGT-SVRGAcc-VR-EXTRA-CAAcc-VR-DIGing-CAAcc-VR-EXTRAAcc-VR-DIGing200040006000800010000120001400016000# of communication rounds10-1510-1010-5100Acc-DVRAcc-VR-EXTRA-CAAcc-VR-DIGing-CAVR-DIGingVR-EXTRAGT-SVRGDIGingEXTRAAPAPCAcc-VR-EXTRAAcc-VR-DIGing0100200300400500# of full batch gradient computations10-1510-1010-5100APAPCEXTRADIGingAcc-DVRVR-EXTRAGT-SVRGVR-DIGingAcc-VR-EXTRA-CAAcc-VR-DIGing-CAAcc-VR-DIGingAcc-VR-EXTRA010002000300040005000# of communication rounds10-1510-1010-5100Acc-DVRAcc-VR-DIGing-CAAcc-VR-EXTRA-CAVR-DIGingVR-EXTRAGT-SVRGAPAPCDIGingEXTRAAcc-VR-DIGingAcc-VR-EXTRA050100150200# of full batch gradient computations10-1510-1010-5100APAPCEXTRADIGingAcc-VR-EXTRA-CAAcc-VR-DIGing-CAAcc-DVRVR-DIGingVR-EXTRAAcc-VR-DIGingGT-SVRGAcc-VR-EXTRA050010001500# of communication rounds10-1510-1010-5100Acc-DVRAcc-VR-EXTRA-CAAcc-VR-DIGing-CAVR-DIGingAcc-VR-DIGingGT-SVRGVR-EXTRAAPAPCAcc-VR-EXTRADIGingEXTRAVariance Reduced EXTRA and DIGing and Their Optimal Acceleration

6. Conclusion and Future Research

This paper extends the widely used EXTRA and DIGing methods with variance reduction, and four VR-based stochastic
decentralized algorithms are proposed. The proposed VR-EXTRA has the O((κs + n) log 1
(cid:15) ) stochastic gradient compu-
tation complexity and the O((κb + κc) log 1
(cid:15) ) communication complexity. The proposed VR-DIGing has a little worse
communication complexity of O((κb + κ2
(cid:15) ). Our stochastic gradient computation complexities keep the same as the
single-machine VR methods such as SVRG, and our communication complexities are the same as those of EXTRA and
nκs +n) log 1
(cid:15) )
DIGing, respectively. The proposed accelerated VR-EXTRA and VR-DIGing achieve both the optimal O((
√
stochastic gradient computation complexity and O(
(cid:15) ) communication complexity. They are also the same as the
ones of single-machine accelerated VR methods such as Katyusha, and the accelerated full batch decentralized methods
such as MSDA, respectively.

κbκc log 1

c) log 1

√

DIGing, called gradient tracking in other literatures, is a fundamental decentralized algorithm in the distributed optimization
(cid:15) ), which is worse than the O((κb + κc) log 1
community. However, its state-of-the-art complexity is O((κb + κ2
(cid:15) )
(cid:15) ) to O((κb + κc) log 1
one of EXTRA. An open problem is that can we improve it from O((κb + κ2
(cid:15) )? Recently,
c log 1
Koloskova et al. (2021) gave some potential directions, where the complexity is improved from O(κ2
bκ2
(cid:15) ) (Qu &
Li, 2018) to O(κbκc log 1
c has been reduced to κc. Currently, it is unclear whether the
complexity can be further improved to O((κb + κc) log 1

(cid:15) ). That is, the dependence on κ2
(cid:15) ).

c) log 1

c) log 1

In our Algorithms 1 and 2, we set the parameters dependent on κc, Lf , and Lf , where κc needs the global knowledge of the
network, while Lf and Lf needs to know the parameters of the other nodes. It is important to design practical algorithms
only dependent on the local parameters, such as the Lipschitz constant L(i) and strong-convexity constant µ(i), while still
keep the optimal complexities. Another open problem is whether the reformulation (6) can be extended to directed graphs,
where U and V cannot simply take the ones in this paper. Other interesting extensions include compression, asynchrony,
and so on.

References

Alghunaim, S. A., Ryu, E. K., Yuan, K., and H.Sayed, A. Decentralized proximal gradient algorithms with linear covnergence

rates. IEEE Transactions on Automatic Control, 66(6):2787–2794, 2021.

Allen-Zhu, Z. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. Journal of Machine Learning Research,

18(221):1–51, 2018.

Arioli, M. and Scott, J. Chebyshev acceleration of iterative reﬁnement. Numerical Algorithms, 66(3):591–608, 2014.

Auzinger, W. and Melenk, J. M. Iterative solution of large linear systems. Lecture notes, TU Wien, 2017.

Aybat, N. S., Wang, Z., Lin, T., and Ma, S. Distributed linearized alternating direction method of multipliers for composite

convex consensus optimization. IEEE Transactions on Automatic Control, 63(1):5–20, 2018.

Bertsekas, D. P. Constrained Optimization and Lagrange Multiplier Methods. Athena Scientiﬁc, Belmont, Massachusetts,

1982.

Bertsekas, D. P. Distributed asynchromous computation of ﬁxed points. Mathmatical Programming, 27:107–120, 1983.

Boyd, S., Diaconis, P., and Xiao, L. Fastest mixing markov chain on a graph. SIAM Review, 46(4):667–689, 2004.

Chen, J. and Sayed, A. H. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE

Transactions on Signal Processing, 60(8):4289–4305, 2012.

Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: A fast incremental gradient method with support for non-strongly
convex composite objectives. In Advances in Neural Information Processing Systems (NIPS), pp. 1646–1654, 2014.

Dvinskikh, D. and Gasnikov, A. Decentralized and parallelized primal and dual accelerated methods for stochastic convex

programming problems. Journal of Inverse and Ill-posed Problems, 29(3):385–405, 2021.

Fallah, A., G¨urb¨uzbalaban, M., Ozdaglar, A., Simsekli, U., and Zhu, L. Robust distributed accelerated stochastic gradient

methods for multi-agent networks. preprint arXiv:1910.08701, 2019.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Fercoq, O. and Richt´arik, P. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):

1997–2023, 2015.

Gorbunov, E., Dvinskikh, D., and Gasnikov, A. Optimal decentralized distributed algorithms for stochastic convex

optimization. preprint arXiv:1911.07363, 2019.

Gorbunov, E., Rogozin, A., Beznosikov, A., Dvinskikh, D., and Gasnikov, A. Recent theoretical advances in decentralized
distributed convex optimization. In High-Dimensional Optimization and Probability, pp. 253–325. Springer, 2022.

Hendrikx, H., Bach, F., and Massouli´e, L. Dual-free stochastic decentralized optimization with variance reduction. In

Advances in Neural Information Processing Systems (NeurIPS), pp. 19455–19466, 2020.

Hendrikx, H., Bach, F., and Massouli´e, L. An optimal algorithm for decentralized ﬁnite-sum optimization. SIAM Journal on

Optimization, 31(4):2753–2783, 2021.

Hong, M., Hajinezhad, D., and Zhao, M.-M. Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex
optimization and learning over networks. In International Conference on Machine Learning (ICML), pp. 1529–1538,
2017.

Iutzeler, F., Bianchi, P., Ciblat, P., and Hachem, W. Explicit convergence rate of a distributed alternating direction method of

multipliers. IEEE Transactions on Automatic Control, 61(4):892–904, 2016.

Jakoveti´c, D. A uniﬁcation and generatliztion of exact distributed ﬁrst order methods. IEEE Transactions on Signal and

Information Processing over Networks, 5(1):31–46, 2019.

Jakoveti´c, D., Xavier, J., and Moura, J. M. F. Fast distributed gradient methods. IEEE Transactions on Automatic Control,

59(5):1131–1146, 2014.

Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in

Neural Information Processing Systems (NIPS), pp. 315–323, 2013.

Koloskova, A., Lin, T., and Stich, S. U. An improved analysis of gradient tracking for decentralized machine learning. In

Advances in Neural Information Processing Systems (NeurIPS), pp. 11422–11435, 2021.

Kovalev, D., Horv´ath, S., and Richt´arik, P. Don’t jump through hoops and remove those loops: SVRG and Katyusha are
better without the outer loop. In International Conference on Algorithmic Learning Theory (ALT), pp. 451–467, 2020a.

Kovalev, D., Salim, A., and Richt´arik, P. Optimal and practical algorithms for smooth and strongly convex decentralized

optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 18342–18352, 2020b.

Lan, G. and Zhou, Y. An optimal randomized incremental gradient method. Mathematical Programming, 171:167–215,

2018.

Lan, G., Lee, S., and Zhou, Y. Communication-efﬁcient algorithms for decentralized and stochastic optimization. Mathe-

matical Programming, 180:237–284, 2020.

Li, B., Cen, S., Chen, Y., and Chi, Y. Communication-efﬁcient distributed optimization in networks with gradient tracking

and variance reduction. Journal of Machine Learning Research, 21(180):1–51, 2020a.

Li, H. and Lin, Z. Revisiting EXTRA for smooth distributed optimization. SIAM Journal on Optimization, 30(3):1795–1821,

2020.

Li, H., Fang, C., Yin, W., and Lin, Z. Decentralized accelerated gradient methods with increasing penalty parameters. IEEE

transactions on Signal Processing, 68:4855–4870, 2020b.

Li, Z., Shi, W., and Yan, M. A decentralized proximal-gradient method with network independent step-sizes and separated

convergence rates. IEEE Transactions on Signal Processing, 67(17):4494–4506, 2019.

Lin, H., Mairal, J., and Harchaoui, Z. Catalyst acceleration for ﬁrst-order convex optimization: from theory to practice.

Journal of Machine Learning Research, 18(212):1–54, 2018.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Lin, Q., Lu, Z., and Xiao, L. An accelerated randomized proximal coordinate gradient method and its application to

regularized empirical risk minimization. SIAM Journal on Optimization, 25(4):2244–2273, 2015.

Makhdoumi, A. and Ozdaglar, A. Convergence rate of distributed ADMM over networks. IEEE Transactions on Automatic

Control, 62(10):5082–5095, 2017.

Mokhtari, A. and Ribeiro, A. DSA: Decenrtalized double stochastic averaging gradient algorithm. Journal of Machine

Learning Research, 17(61):1–35, 2016.

Nedi´c, A. Asynchronous broadcast-based convex optimization over a network. IEEE Transactions on Automatic Control, 56

(6):1337–1351, 2011.

Nedi´c, A. and Ozdaglar, A. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic

Control, 54(1):48–61, 2009.

Nedi´c, A., Olshevsky, A., and Shi, W. Achieving geometric convergence for distributed optimization over time-varying

graphs. SIAM Journal on Optimization, 27(4):2597–2633, 2017.

Nedi´c, A., Olshevsky, A., and Rabbat, M. G. Network topology and communication-computation tradeoffs in decentralized

optimization. Proceedings of the IEEE, 106(5):953–976, 2018.

Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic, Boston, 2004.

Pu, S. and Nedi´c, A. Distributed stochastic gradient tracking methods. Mathematical Programming, 187:409–457, 2021.

Qu, G. and Li, N. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network

Systems, 5(3):1245–1260, 2018.

Qu, G. and Li, N. Accelerated distributed Nesterov gradient descent. IEEE Transactions on Automatic Control, 65(6):

2566–2581, 2020.

Ram, S. S., Nedi´c, A., and Veeravalli, V. V. Distributed stochastic subgradient projection algorithms for convex optimization.

Journal of Optimization Theory and Applications, 147:516–545, 2010.

Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massouli´e, L. Optimal algorithms for smooth and strongly convex
distributed optimization in networks. In International Conference on Machine Learning (ICML), pp. 3027–3036, 2017.

Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massouli´e, L. Optimal algorithms for non-smooth distributed optimization

in networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 2740–2749, 2018.

Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massouli´e, L. Optimal convergence rates for convex distributed

optimization in networks. Journal of Machine Learning Research, 20(159):1–31, 2019.

Schmidt, M., Le Roux, N., and Bach, F. Minimizing ﬁnite sums with the stochastic average gradient. Mathematical

Programming, 162:83–112, 2017.

Shi, W., Ling, Q., Wu, G., and Yin, W. EXTRA: An exact ﬁrst-order algorithm for decentralized consensus optimization.

SIAM Journal on Optimization, 25(2):944–966, 2015.

Terelius, H., Topcu, U., and Murray, R. M. Decentralized multi-agent optimization via dual decomposition. IFAC proceedings

volumes, 44(1):11245–11251, 2011.

Tsitsiklis, J. N., Bertsekas, D. P., and Athans, M. Distributed asynchronous deterministic and stochastic gradient optimization

algorithms. IEEE Transaction on Automatic Control, 31(9):803–812, 1986.

Uribe, C. A., Lee, S., Gasnikov, A., and Nedi´c, A. A dual approach for optimal algorithms in distributed optimization over

networks. In Information Theory and Applications Workshop (ITA), pp. 1–37, 2020.

Xiao, L. and Zhang, T. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on

Optimization, 24(4):2057–2075, 2014.

Variance Reduced EXTRA and DIGing and Their Optimal Acceleration

Xin, R., Khan, U. A., and Kar, S. A linear algorithm for optimization over directed graphs with geometric convergence.

IEEE Control Systems Letters, 2(3):315–320, 2018.

Xin, R., Kar, S., and Khan, U. A. Decentralized stochastic optimization and machine learning: A uniﬁed variance-reduction
framework for robust performance and fast convergence. IEEE Signal Processing Magazine, 37(3):102–113, 2020a.

Xin, R., Khan, U. A., and Kar, S. Variance-reduced decentralized stochastic optimization with accelerated convergence.

IEEE Transactions on Signal Processing, 68:6255–6271, 2020b.

Xu, J., Zhu, S., Soh, Y. C., and Xie, L. Augmented distributed gradient methods for multi-agent optimization under

uncoordinated constant stepsizes. In IEEE Conference on Decision and Control (CDC), pp. 2055–2060, 2015.

Yuan, K., Ling, Q., and Yin, W. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26

(3):1835–1854, 2016.

Zhou, K., Shang, F., and Cheng, J. A simple stochastic variance reduced algorithm with fast convergence rates.

In

International Conference on Machine Learning (ICML), pp. 5975–5984, 2019.

