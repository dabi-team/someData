Low-Latency Immersive 6D Televisualization with Spherical Rendering

Max Schwarz∗ and Sven Behnke

1
2
0
2

p
e
S
3
2

]

O
R
.
s
c
[

1
v
3
7
3
1
1
.
9
0
1
2
:
v
i
X
r
a

Abstract— We present a method for real-time stereo scene
capture and remote VR visualization that allows a human
operator to freely move their head and thus intuitively control
their perspective during teleoperation. The stereo camera is
mounted on a 6D robotic arm, which follows the operator’s
head pose. Existing VR teleoperation systems either induce high
latencies on head movements, leading to motion sickness, or
use scene reconstruction methods to allow re-rendering of the
scene from different perspectives, which cannot handle dynamic
scenes effectively. Instead, we present a decoupled approach
which renders captured camera images as spheres, assuming
constant distance. This allows very fast re-rendering on head
pose changes while keeping the resulting temporary distortions
during head translations small. We present qualitative exam-
ples, quantitative results in the form of lab experiments and a
small user study, showing that our method outperforms other
visualization methods.

I. INTRODUCTION
There are many ways to control robotic systems, from
remote control over teleoperation to full autonomy. Tele-
operation is highly relevant and valuable to the robotic
research community, ﬁrst of all because it allows to ad-
dress tasks that are impossible to solve using state-of-the-
art autonomous control methods—the human intellect and
creativity in solving problems and reacting to unforeseen
events is unmatched. Furthermore, teleoperation can generate
training examples for improving autonomy in the future.
There are also immediate beneﬁts for the economy and daily
life: Teleoperation can allow humans to work remotely in
potentially dangerous environments, which is highly desir-
able in situations like the current COVID-19 pandemic. This
interest is embodied in new robotics competitions like the
ANA Avatar XPRIZE Challenge1 and a multitude of other
past and ongoing competitions [1], [2].

One important, if not the most important feature of a
teleoperation system is its capability to display the remote
environment to the human operator in a convincing, immer-
sive way. Maintaining full situational awareness is necessary
to both induce security and conﬁdence for the operator as
well as to solve tasks efﬁciently and robustly.

We present a real-time scene capture and visualization sys-
tem for teleoperated robots, which gives the human operator
wearing a head-mounted display full 6D control of the ﬁrst-
person camera perspective. In summary, our contributions
include:

1) A robotic head assembly equipped with a 180° 4K
stereo camera pair, capable of 6D movement with low
latency;

∗Both authors are with the Autonomous Intelligent Systems group of

University of Bonn, Germany; schwarz@ais.uni-bonn.de

1https://www.xprize.org/prizes/avatar

Cameras

Robot with 6D Head

e
s
o
P
d
a
e
H

150 ms

2
×
4
K
V
d
e
o

i

Spherical Rendering

e
s
o
P
d
a
e
H

11 ms

E
y
e
V
e
w
s

i

Stereoscopic VR System

Fig. 1. Our low-latency televisualization approach. The 2×4K video stream
from a stereo pair of wide-angle cameras is rendered using a sphere model
to the operator’s VR headset. On a fast timescale, operator movements are
handled without much latency by re-rendering from the new pose. The
operator’s pose is also streamed to the robotic system, which moves the
cameras accordingly in 6D, although on a slower timescale.

2) a combination of calibration steps for camera intrinsics
and extrinsics, hand-eye, and VR calibration; and
3) an efﬁcient rendering method that decouples the VR
view from the latency-afﬂicted robot head pose, pro-
viding a smooth low-latency VR experience at essen-
tially unlimited frame rate.

II. RELATED WORK

Viewing a stereo camera video feed is one of the estab-
lished methods for remote immersion, providing live imagery
with intuitive depth perception.

Martins et al. [3] mount a stereo camera on a ﬁeld robot
developed for rescue applications. While the camera is ﬁxed
to the robot, its track-driven body can bend to adjust the
camera pitch angle. The operator wears a head-mounted
display, whose tracking pose is used to control the pitch
angle and the yaw angle of the entire robot. Roll motions
are simulated by rotating the camera images in 2D. The
authors report that the high latency on yaw changes was
this method
confusing for operators. We also note that

 
 
 
 
 
 
Fig. 2. Range of motion supported by our setup. We show the left eye view of the HMD, cropped to the central part for clarity. Note that the human
operator can easily change perspective and observe parts of the scene that were previously hidden.

will not accurately represent head orientations due to the
decomposition into Euler angles, which are then applied at
different locations. In contrast, our method supports full 6D
head movement (see Fig. 2) and reacts to pose changes with
almost zero latency.

Zhu et al. [4] study whether head or gaze tracking is more
suitable for camera orientation control. However, they do
not use a HMD but a 2D monitor for displaying camera
images. Both head and gaze control modes operate in a
relative fashion, moving the camera in the indicated direction
until the head/eye returns to its neutral position. The authors
conclude that
the gaze-based control method feels more
natural. In contrast, our system uses the absolute head pose
to directly control the camera pose, leading to very natural
and intuitive control of the camera.

This technique can also be seen in the work of Agarwal
et al. [5], who control a robot’s head pose directly from a
human head pose measured with a Microsoft Kinect 2. Their
robot head is only capable of 2D movement, pitch and yaw,
so the human pose is mapped with a neural network to the
lower-dimensional space. The robot head is not equipped
with cameras, however.

Very similarly, in one of the earliest works we could ﬁnd,
Heuring and Murray [6] control a stereo camera pair mounted
on a pan/tilt unit with sufﬁciently low latency to follow
human head motions precisely.

Conversely, Lipton et al. [7] combine stereo capture with
VR to enable teleoperation, but mount their camera in a ﬁxed
location on the robot, providing only limited ﬁeld of view.
When the robot or its head cannot be moved fast enough,
latencies are introduced into a real-time teleoperation system.
One particularly effective approach for removing latencies is
to display not the live camera feed, but a reconstructed 3D
scene, in which the operator can move freely without much
latency. We explored this technique in our own work, where
we rendered reconstructed 3D laser maps of the environment
to the user [8] and displayed textured meshes generated
from a live RGB-D feed [9]. In both cases, we used a VR
headset which allowed the user free movement inside the
reconstructed data with low latency. Later on, we improved
on these techniques in Stotko et al. [10] by displaying a
high-ﬁdelity 3D mesh of the environment generated from an
automatically moving RGB-D camera mounted on the end-
effector of a mobile manipulator.

Visualizing reconstructed scenes has the disadvantage that
although the system reacts to observer pose changes with

almost no latency (requiring just a rendering step), changes in
the scene itself typically only show up after longer delays—
if the scene representation supports dynamic content at all.
This makes these approaches completely unsuited for many
telepresence situations, e.g. when interacting with a per-
son or performing manipulation. Furthermore, reconstruction
methods typically rely on depth, either measured by depth
cameras or inferred (SfM). Many materials and surfaces
encountered in everyday scenes result in unreliable depth
and may violate assumptions made by the reconstruction
method (e.g. transparent objects, reﬂections), resulting in
wrong transformations or missing scene parts.

RGB-D-based visualization solutions as the one by Whit-
ney et al. [11], or Sun et al. [12], who display live RGB-D
data as point clouds for teleoperation of a Baxter robot, can
cope with dynamic scenes but still suffer from measurement
problems and the inherent sparsity—the operator can often
look through objects.

Displaying a live stereo camera feed as done in our method
neatly sidesteps these issues, since the 3D reconstruction
happens inside the operator’s visual cortex. We address
the resulting higher observer pose latency using a smart
rendering method discussed in the next section.

III. METHOD

A. Robotic Platform & VR Equipment

Our robot’s head is mounted on a UFACTORY xArm 6,
providing full 6D control of the head (see Figs. 1 and 4).
The robotic arm is capable of moving a 5 kg payload, which
is more than enough to position a pair of cameras and a
small color display for telepresence (not used in this work).
Furthermore, the arm is very slim, which results in a large
workspace while being unobtrusive. Finally, it is capable of
fairly high speeds (180 °/s per joint, roughly 1 m/s at the
end-effector), thus being able to match dynamic human head
movements.

We chose two Basler a2A3840-45ucBAS cameras for the
stereo pair, which offer 4K video streaming at 45 Hz. The
cameras are paired with C-Mount wide-angle lenses, which
provide more than 180° ﬁeld of view in horizontal direction.
We also experimented with Logitech BRIO webcams with
wide-angle converters, which offer auto-focus but can only
provide 30 Hz at 4K, resulting in visible stutters with moving
objects. The Basler cameras are conﬁgured with a ﬁxed
exposure time (8 ms) to reduce motion blur to a minimum.

Thead(θ)

Tcam

Tmount

Tarm(θ)

Tmark

Fig. 4.
Frames involved in the calibration procedure. Transformations
colored yellow are computed using forward kinematics from measured joint
angles while blue transformations are optimized. The auxiliary transform
Tmarker is optimized, but not used in the calibrated system. Note: The camera
FoV is shown for illustration purposes and is not to scale.

left and right camera image frame as follows:

pL = fµ(T −1
pR = fγ(T L

cam · T −1
R · T −1

head(θ) · Tmount · Tarm(θ) · Tmark),
cam · T −1

head(θ) · Tmount · Tarm(θ) · Tmark),

(1)

(2)

where Tarm(θ) and Thead(θ) are the poses of the arm and head
ﬂanges relative to their bases for the current joint conﬁgura-
tion θ, and f is the double-sphere projection function with
parameters µ and γ found above.

We collect samples with known 2D pixel coordinates
ˆpL and ˆpR extracted using the ArUco marker detector of
the OpenCV library. During sample collection,
the head
continuously moves in a predeﬁned sinusoidal pattern while
the robot’s arm is moved manually using teach mode. Sample
collection takes about 5 min for an engineer.

Finally, we minimize the squared error function

E =

N
(cid:88)

i=1

||pL − ˆpL||2

2 + ||pR − ˆpR||2
2

(3)

over all N samples using the Ceres solver, yielding optimal
transforms Tcam and Tmount.

C. Head Control

The remaining question is how to transfer the head pose,
measured in the VR space, to the robot side. To initialize the
mapping, we keep the robot head in predeﬁned nominal pose
T robot
nom , looking straight ahead. We ask the human operator to
do the same and record the resulting head pose in VR space
T VR
nom, taking care to remove any remaining undesirable pitch
and roll component.

During operation, the head pose T robot
head

is computed from

the tracking pose T VR

head in straightforward fashion:

head = T robot
T robot

nom · (T VR

nom)−1 · T VR
head,

(4)

directly mapping the operator’s head movement to the robot’s
head.

The head pose is smoothed by a recursive low-pass ﬁlter
with a cut-off frequency of 100 Hz to remove very high
frequency components. It is also monitored for larger jumps,

Fig. 3. Raw image from one of the wide-angle cameras. The horizontal
ﬁeld of view is higher than 180°.

The robot head is mounted on a torso equipped with two

Franka Emika Panda arms and anthropomorphic hands.

The operator wears an HTC Vive Pro Eye head-mounted
display, which offers 1440×1600 pixels per eye with an
update rate of 90 Hz and 110° diagonal ﬁeld of view. While
other HMDs with higher resolution and/or FoV exist, this
headset offers eye tracking which will be beneﬁcial for
building telepresence solutions in future work. As can be
seen, the camera FoV is much larger than the HMD FoV,
which ensures that the visible FoV stays inside the camera
FoV even if the robot head should be lagging behind the
operator’s movement.

The VR operator station is driven by a standard PC
(Intel i9-9900K 3.6 GHz, NVidia RTX 2080), connected via
Gigabit Ethernet to the robot computer (same speciﬁcations).

B. Calibration

Before the system can be used, multiple transforms and
parameters need to be calibrated. We devised a principled
approach starting at camera intrinsic calibration over hand-
eye calibration on the robot side, to VR calibration on the
operator side.

Since the cameras have very high FoV with signiﬁcant
ﬁsh-eye distortion (see Fig. 3), we use the Double-Sphere
camera model [13] to describe the intrinsics. Its parameter
sets µ and γ for the left and right cameras, respectively, can
be calibrated easily using the kalibr software package [14],
just requiring an Aprilgrid calibration target. The kalibr
software also estimates the extrinsic transformation between
the two cameras T L
R .

As shown in Fig. 4, there are further extrinsic transforma-
tions to be calibrated, especially if one wants to accurately
perform manipulation with a robotic arm. We follow a classic
hand-eye calibration approach with an ArUco marker on the
robot’s wrist, with the goal of estimating Tcam, but also the
transformation Tmount between the mounting of the head arm
and the robot arm, and Tmark, the pose of the marker on the
wrist.

We can compute the 2D pixel positions pL and pR in the

r

T C
V

r

(a) Pure rotation

(b) Rotation + Translation

Fig. 5. Spherical rendering example in 2D. We show only one camera C
of the stereo pair, the other is processed analogously. The robot camera is
shown in white with its very wide FoV. The corresponding VR camera V ,
which renders the view shown in the HMD, is shown in green. The camera
image is projected onto the sphere with radius r, and then back into the
VR camera. Pure rotations (a) result in no distortion, while translations (b)
will distort the image if the objects are not exactly at the assumed depth.

]
◦
[

γ

r
o
r
r
E

r
a
l
u
g
n
A

5

0

−5

−10

0

0.5

1

1.5

2

2.5

3

Using this design, head movements are immediately re-
acted to. Pure rotations of the head will have no perceptible
latency, since the wide-angle cameras will already have
captured the area the operator will look at. Translational
movements work as if everything is at a constant distance,
yielding distortions wherever this assumption does not hold
(see Fig. 5). The amount of distortion can be quantiﬁed by
the angular projection error γ and depends on the distance d
to the object and the head translation ∆x (here perpendicular
to the view ray):

γ = arctan

(cid:19)

(cid:18) d
∆x

− arctan

(cid:18) r
∆x

(cid:19)
.

(5)

π

Note that γ quickly saturates for large d against the value
2 − arctan( r
∆x ) (see Fig. 6). For d < r, the distortion can
become signiﬁcant. In conclusion, it is important to choose
r close to a typical minimum object distance. Of course, as
soon as the robot head catches up and moves into the correct
pose, any distortion is corrected.

Real object distance d [m]

E. Latency Improvements

Fig. 6. Angular error introduced by translation of the operator head by
∆x = 10 cm depending on the distance to the object. Since the spherical
reprojection assumes a distance of 1 m, we see no error at that distance.
The red dashed line shows the error upper bound for large distances d.

in which case we switch to a slower initialization mode that
moves to the target pose with limited velocity.

D. Spherical Rendering

If operator and robot head poses matched exactly, the
camera images could be rendered immediately to the HMD
by projecting each pixel in HMD image space to the cam-
era images and looking up the corresponding color value.
However, when the poses are not identical, as is usually the
case, we have a 6D transform T V
C between the virtual VR
camera V (representing the operator’s eye) and actual camera
position C. Traversing this transform requires 3D positions,
but we do not have depth information.

Hence, we must make assumptions which will not impact
the result quality much, since the transform T V
C will be quite
small. The simplest assumption is one of constant depth
for the entire scene. Since we are dealing with wide-angle
cameras, it is natural to assume constant distance, leading to
spherical geometry around the camera. With this assumption,
we can raycast from the VR camera V , ﬁnd the intersection
with the sphere, and ﬁnd the corresponding pixel in the
camera image (see Fig. 5).

In practice, the rendering part is implemented in OpenGL
using the Magnum 3D Engine2. We use the standard ras-
terization pipeline by rendering a sphere of radius r = 1 m
at the camera location. A fragment shader then computes
the pixel location (in C) of each visible point of the sphere
and performs a texture lookup with bilinear interpolation to
retrieve the color value.

2https://magnum.graphics

We found that we needed to take special care at nearly
all parts of the pipeline to ensure low-latency operation with
the required time precision.

Starting at the cameras, we wrote a custom driver which
uses the built-in timestamping feature of the cameras to
obtain high-precision timing information in order to correlate
images with head-arm transforms. Furthermore, it is nec-
essary to compress the image stream for transmission over
Ethernet, since the raw stream bandwidth exceeds 350 MB/s.
For easy interoperability with standard ROS packages, we
chose an MJPEG compression, which is done efﬁciently
on the GPU [15]. The resulting stream has a bandwidth
of 30 MB/s, which is acceptable in our setup. If even less
bandwidth is required, hardware-accelerated H.264 encoding
could be a solution.

Furthermore, we developed a custom driver for the xArm,
which provides joint position measurements at a high rate.
This required modiﬁcations to the arm ﬁrmware. Our modi-
ﬁed version can control and report joint positions at 100 Hz.
Its measurements are interpolated and synchronized with the
timestamped camera images using the ROS tf stack.

Finally,

standard
the VR does
image transport ROS
cannot
decompress the two MJPEG streams with 4K resolution at
45 Hz. Instead, we use the GPUJPEG decoder [15].

not
use
package, which

the

All in all, these examples highlight that many off-the-
shelf products are not designed for the required low-latency
operation and manual adaptation requires considerable effort.

IV. EXPERIMENTS

A. Quantitative Experiments

In a ﬁrst experiment, we measured the total image latency
from acquisition start inside the camera to display in the
HMD. For this, we used the timestamping feature of the
camera, with the camera time synchronized to the robot
computer time. Both computers are synchronized using the

t1

t2

vop
vrob

]
s
/
m

[

y
t
i
c
o
l
e
V

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

Time [s]

Fig. 7.
Head movement latency experiment. Shown are the Cartesian
velocities of the operator’s head vop and the robot’s head vrob during a fast
(0.5 m/s) head movement. Vertical lines have been added to ease analyzing
latencies (start latency t1 ≈ 130 ms, ﬁnish latency t2 ≈ 100 ms).

Initial
view

Adapted
view

∆s
vop

]

m
c
[

n
o
i
t
a
i
v
e
D

8

6

4

2

0

20

10

0

]
s
/
m
c
[

y
t
i
c
o
l
e
V

Fig. 9. Operator view (cropped) during the user study task. The peg has
square cross section of 4×4 cm. The hole in the white plastic object is
0.3 mm larger. In the bottom situation, the user repositioned their head to
better view the interfacing parts.

TABLE I

USER STUDY SUCCESS RATES AND TIMINGS

Visual mode

Success

Completion time [s]

5

10

15

Time [s]

Fig. 8. Head tracking performance. We show the distance ∆s of the VR
eye camera V from the robot camera C for the left eye stream. The operator
was performing highly dynamic head movements. As expected, the position
deviation is highly correlated with operator head velocity vop.

a) Full 6D
b) 3D orientation
c) Fixed perspective

7/7
7/7
6/7

Mean

StdDev

71.0
111.7
158.3

50.6
87.5
39.6

chrony NTP system, so that timestamps are comparable.
Finally, the VR renderer compares the system time against
the camera timestamp at rendering each frame. On average,
the images are shown with 40 ms latency. A considerable
fraction of this time is consumed by the camera exposure
time (8 ms) and USB transfer time (around 10 ms).

Since the VR renderer achieves the maximum HMD
update rate of 90 Hz without difﬁculty, rotational head move-
ments have the same latency as in local VR applications,
which is not noticeable—according to publicly available
information, the Lighthouse VR tracking system operates
with 250 Hz update rate3.

To analyze the total latency of head control, we plotted
Cartesian velocities of the raw target head pose (as recorded
by the VR tracking system) and the measured head pose
during a fast head motion (see Fig. 7). The latency varies
along the trajectory, but the head comes to a stop roughly
200 ms after the operator’s head stopped moving. We note
that this is a maximum bound of the latency; slower head
motions result in lower latencies.

Furthermore, we analyzed the absolute deviation between
the HMD eye pose and the camera pose, i.e. the error that is
introduced during the spherical rendering step (see Fig. 8).
We note that even during fast motions, only minor distortions
were noticeable to the operator.

B. User Study

We conducted a small user study in order to measure
the immersiveness and efﬁcacy of our pipeline. Due to the
ongoing COVID-19 pandemic, we were limited to immediate
colleagues as subjects, which severely constrained the scope
of our study to seven participants.

3https://en.wikipedia.org/wiki/HTC_Vive

Participants were asked to teleoperate the robot, perform-
ing a rather difﬁcult peg-in-hole task involving grasping two
objects, and inserting one into the other with tight tolerances
(see Fig. 9). The task places high demands on the visual
system since a) even small misalignments would prevent
success and b) the peg object was quite hard to hold and
slippage could only be detected visually. For controlling the
two robot arms and hands, the participants used a custom
force-feedback exoskeleton system which is not the focus of
this work. The participants performed the task three times,
with different visual modes:

a) Full 6D: Entire pipeline with full 6D movement.
b) 3D orientation: The operator only controls the ori-
entation of the head (similar to [3], [6]). The spherical ren-
dering system is used, but the camera spheres are translated
to the HMD eye position. Notably, this variant can still react
to orientation changes with low latency by re-rendering.

c) Fixed perspective: The robot head orientation is
ﬁxed to a 45° downward pitch, focusing on the table in front
of the robot. The camera images are rendered directly to the
HMD, i.e. the HMD pose has no effect. This is similar to
devices sold as First-Person-View (FPV) glasses, which are
commonly used in drone racing.

The order of the three trials was randomized to mitigate
effects from learning during the trials. Participants could
attempt the task as often as they liked (with an assisting
human resetting the objects on the table if moved out of
reach). When a maximum time of 5 min was reached without
success, the trial was marked as failed.

Although the task itself was not the primary goal of the
user study, effects of the visual mode on success rates and
average completion time can be observed (Table I). The full
6D mode never led to a failure and yielded the lowest average

Did you maintain situational awareness?

Could you clearly see the objects?

Could you judge depth correctly?

better

6D
3D
Fixed

Was the VR experience comfortable for your eyes?

Did you ﬁnd and recognize the objects?

Was it easy to grasp the objects?

Was it easy to ﬁt the objects together?

Fig. 10.
outliers (marked with •) as well as the average value (marked with ×), for each aspect as recorded in our questionnaire.

Statistical results of our user questionnaire. We show the median, lower and upper quartile (includes interquartile range), lower and upper fence,

1

2

3

4

5

6

7

completion time, although the standard deviation was quite
high—mostly due to external effects such as the robot arms
requiring a reset after too much force was applied, but the
skill level also varied highly from participant to participant.
After the three trials, we asked the participants questions
for each of the visualization modes, with answer possibilities
from the 1-7 Likert scale. From the results reported in
Fig. 10, it is immediately visible that the ﬁxed perspective
leads to signiﬁcantly lower user acceptance. The
variant
users reported that this was mainly due to inability to pan
the camera and instead having to move the workpieces to
a better observable pose. Furthermore, the head translation
seems to have no impact on grasping the objects, but impacts
insertion—where it is beneﬁcial to be able to look slightly
from the side to see the involved edges (compare Fig. 9).

V. DISCUSSION & CONCLUSION

We have demonstrated a method for 3D remote visual-
ization that does not rely on geometry reconstruction and
is thus suited for highly dynamic scenes, but is still able
to deliver low-latency response to 6D head movements. The
user study shows that allowing full 6D head movement leads
to faster task completion and better user acceptance when
compared with ﬁxed-mount and pan-tilt systems, which are
still common in teleoperation.

There are also some points of the system that can be
improved. The work space of the arm used to move the head
is—in its present mounting position—limited and cannot
match the full motion range of sitting humans moving their
torso and neck. For this, it would be interesting to investigate
a more biologically inspired joint conﬁguration.

While our method has been evaluated only in a seated
conﬁguration,
it should be applicable with a completely
mobile robot and operator as well. This can be investigated
in future research.

Our work also does not focus on the transmission of data
to remote sites. It is clear that for transmission over the
Internet, bandwidth requirements would have to be lowered,
e.g. through the use of more efﬁcient video compression
codecs. The system should, however, be able to cope with
typical latencies found on Internet connections (up to 200 ms
for connections halfway around the world).

REFERENCES

[1]

E. Krotkov, D. Hackett, L. Jackel, M. Perschbacher, J. Pippine, J.
Strauss, G. Pratt, and C. Orlowski, “The DARPA Robotics Challenge
ﬁnals: Results and perspectives,” Journal of Field Robotics, vol. 34,
no. 2, pp. 229–240, 2017.

[2] H. Kitano, S. Tadokoro, I. Noda, H. Matsubara, T. Takahashi, A.
Shinjou, and S. Shimada, “RoboCup Rescue: Search and rescue in
large-scale disasters as a domain for autonomous agents research,”
in Int. Conf. on Systems, Man, and Cybernetics (SMC), vol. 6, 1999.
[3] H. Martins, I. Oakley, and R. Ventura, “Design and evaluation of
a head-mounted display for immersive 3D teleoperation of ﬁeld
robots,” Robotica, vol. 33, no. 10, p. 2166, 2015.

[5]

[6]

[7]

[8]

[4] D. Zhu, T. Gedeon, and K. Taylor, “Head or gaze? Controlling remote
camera for hands-busy tasks in teleoperation: A comparison,” in
Conf. of the Computer-Human Interaction SIG of Australia, 2010.
P. Agarwal, S. Al Moubayed, A. Alspach, J. Kim, E. J. Carter,
J. F. Lehman, and K. Yamane, “Imitating human movement with
teleoperated robotic head,” in International Symposium on Robot and
Human Interactive Communication (RO-MAN), 2016, pp. 630–637.
J. J. Heuring and D. W. Murray, “Visual head tracking and slaving
for visual telepresence,” in International Conference on Robotics and
Automation (ICRA), vol. 4, 1996, pp. 2908–2914.
J. I. Lipton, A. J. Fay, and D. Rus, “Baxter’s homunculus: Virtual
reality spaces for teleoperation in manufacturing,” IEEE Robotics
and Automation Letters, vol. 3, no. 1, pp. 179–186, 2017.
T. Rodehutskors, M. Schwarz, and S. Behnke, “Intuitive bimanual
telemanipulation under communication restrictions by immersive 3D
visualization and motion tracking,” in International Conference on
Humanoid Robots (Humanoids), 2015, pp. 276–283.
T. Klamt, M. Schwarz, C. Lenz, L. Baccelliere, D. Buongiorno,
T. Cichon, A. DiGuardo, D. Droeschel, M. Gabardi, M. Kamedula,
N. Kashiri, A. Laurenzi, D. Leonardis, L. Muratore, D. Pavlichenko,
A. S. Periyasamy, D. Rodriguez, M. Solazzi, A. Frisoli, M. Gust-
mann, J. Roßmann, U. S¨uss, N. G. Tsagarakis, and S. Behnke,
“Remote mobile manipulation with the Centauro robot: Full-body
telepresence and autonomous operator assistance,” Journal of Field
Robotics, vol. 37, no. 5, pp. 889–919, 2020.
P. Stotko, S. Krumpen, M. Schwarz, C. Lenz, S. Behnke, R. Klein,
and M. Weinmann, “A VR system for immersive teleoperation and
live exploration with a mobile robot,” in International Conference
on Intelligent Robots and Systems (IROS), 2019.

[9]

[10]

[11] D. Whitney, E. Rosen, E. Phillips, G. Konidaris, and S. Tellex,
“Comparing robot grasping teleoperation across desktop and virtual
reality with ROS reality,” in Robotics Research, 2020.

[12] D. Sun, A. Kiselev, Q. Liao, T. Stoyanov, and A. Loutﬁ, “A new
mixed-reality-based teleoperation system for telepresence and ma-
neuverability enhancement,” IEEE Transactions on Human-Machine
Systems, vol. 50, no. 1, pp. 55–67, 2020.

[14]

[13] V. Usenko, N. Demmel, and D. Cremers, “The double sphere camera
model,” in Int. Conf. on 3D Vision (3DV), 2018, pp. 552–560.
J. Rehder, J. Nikolic, T. Schneider, T. Hinzmann, and R. Siegwart,
“Extending kalibr: Calibrating the extrinsics of multiple IMUs and
of individual axes,” in Int. Conf. on Rob. and Aut. (ICRA), 2016.
P. Holub, J. Matela, M. Pulec, and M. ˇSrom, “UltraGrid: Low-latency
high-quality video transmissions on commodity hardware,” in 20th
ACM International Conference on Multimedia, 2012, pp. 1457–1460.

[15]

