Reinforcement Learning Driven Adaptive VR
Streaming with Optical Flow Based QoE

Wei Quan, Yuxuan Pan, Bin Xiang, Lin Zhang
School of Information and Communication Engineering
Beijing University of Posts and Telecommunications, Beijing, China
{louisqw, panyx, bingo6, zhanglin}@bupt.edu.cn

0
2
0
2

r
a

M
7
1

]

M
M

.
s
c
[

1
v
3
8
5
7
0
.
3
0
0
2
:
v
i
X
r
a

Abstract—With the merit of containing full panoramic content
in one camera, Virtual Reality (VR) and 360◦ videos have
attracted more and more attention in the ﬁeld of industrial
cloud manufacturing and training. Industrial Internet of Things
(IoT), where many VR terminals needed to be online at the
same time, can hardly guarantee VR’s bandwidth requirement.
However, by making use of users’ quality of experience (QoE)
awareness factors, including the relative moving speed and depth
difference between the viewpoint and other content, bandwidth
consumption can be reduced. In this paper, we propose OFB-
VR (Optical Flow Based VR), an interactive method of VR
streaming that can make use of VR users’ QoE awareness to
ease the bandwidth pressure. The Just-Noticeable Difference
through Optical Flow Estimation (JND-OFE) is explored to
quantify users’ awareness of quality distortion in 360◦ videos.
Accordingly, a novel 360◦ videos QoE metric based on PSNR
and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-
OF, OFB-VR proposes a versatile-size tiling scheme to lessen
the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive
BitRate(ABR). For evaluation, we take two prior VR streaming
schemes, Pano and Plato, as baselines. Vast evaluations show
that our system can increase the mean PSNR-OF score by 9.5-
15.8% while maintaining the same rebuffer ratio compared with
Pano and Plato in a ﬂuctuate LTE bandwidth dataset. Evaluation
results show that OFB-VR is a promising prototype for actual
interactive industrial VR. A prototype of OFB-VR can be found
in https://github.com/buptexplorers/OFB-VR.

Index Terms—VR, interactive 360◦ video, adaptive, optical ﬂow

I. INTRODUCTION

VR technologies have grown more and more prosperous
and mature recently. According to statistics, there have been
42.9 million VR users in the US(over 13% of its population),
which is predicted to grow up to 57.1 million in 2022 [1].
Under the overwhelming trend, content providers, including
Youtube, Netﬂix, iQiYi, have begun to provide VR content.

As a superior form of regular videos, VR plays an irre-
placeable role that the regular one cannot ﬁt. Transmitting
rich visual information by a single stream, it could reconstruct
the captured scenario and make viewers feel involved and
immersive. Without a doubt, entertainment
is a vital and
primary application scenario for VR. Except for entertaining,
VR can be extremely useful in industrial areas, such as mon-
itoring robot tasks or VR training [2]. Cloud manufacturing
and remote control can be more accurate as well. Moreover,
panoramic supervising is a promising substitute for the regular

one, for its omnidirectional view, ample detail, and high
resolution.

Unfortunately, augmented bandwidth consumption can crip-
ple the availability of a 360◦ video system. However, Tile-
based streaming schemes [3]–[5] are feasible to tackle the
immense bandwidth consumption. The scheme ﬁrstly cut the
original video into several equal-length segments, which are
referred to as chunks, then split each chunk spatially into
several minor squares, which are referred to as tiles. When
streaming,
the client can selectively decrease some tiles’
quality to reduce bandwidth consumption. However, tile-based
VR streaming schemes have some intrinsic limitations. Firstly,
there is a trade-off between QoE robustness and QoE mean
score. A ﬁne-grained tiling scheme can reduce unviewed
area efﬁciently to improve QoE of the viewport area, but
can be more fragile to viewpoint prediction error. Corse-
grained schemes are just the opposite. Secondly, users perceive
panoramic videos in a way different from the regular one. VR
users can tolerate quality distortion to some extent. In fact,
the more signiﬁcant the relative velocity or depth differences
between the focused object and other areas are, the less likely
the users notice the quality drop is.

Optical ﬂow [6] is a reliable tool to estimate accurate pixel-
wise motion between two continuous frames in videos. Optical
ﬂow estimation assumes that each pixel has ﬁxed luminance
and only makes small displacement, so ideally, each pixel in
the previous frame can match with the one in the succeeding
frame. With this method, we can explore video content related
factors with precise pixel-wise motion information.

In recent years, applying RL to serve as adaptive actors
in tile-based 360◦ videos’ streaming has become a hot topic
[7]. A typical RL system consists of three parts, the action,
the environment state, and the reward. Action depends on
the current policy. After an action is made, actor will get a
delayed reward to tell if the current action is well enough.
During training, the algorithm optimizes policy parameters to
get the cumulative reward as high as possible. Due to the merit
of recalling historical decisions’ reward, RL can be adopted
to make optimal tile quality decisions in dramatic network
variation.

This paper proposes OFB-VR, an optical ﬂow based VR

streaming system:

• We use Convolution Neural Network(CNN) based optical
ﬂow estimation to extract the relative speed and velocity

 
 
 
 
 
 
Fig. 1. Flowchart of OFB-VR

depth of each pixel accurately, to quantify the users’
awareness of quality distortion. Then we integrate those
quantiﬁed factors into PSNR to form a new QoE metric
named PSNR-OF.

• In contrast with the ﬁxed-size tiling scheme, We propose
a versatile-size tiling scheme by grouping tiles with
similar PSNR-OF efﬁciency together. It is proved that
our scheme can help save bandwidth and increase QoE.
• We design an asynchronous advantage actor-critic(A3C)
based RL network to serve as our ABR actor. The
network is designed to beneﬁt from past information to
get the highest PSNR-OF score.

This paper is organized as follows: The related work about
tile-based VR streaming methods are summarized in Section
II. Section III shows the system overview and detailed design
about the proposed system. In Section IV, experiment setups
and the parameters of our RL ABR are given, then we compare
the performance of OFB-VR with two baselines, Plato and
Pano. Finally, the article ends with some conclusions.

II. RELATED WORK

Comparing to traditional video which encodes a view from a
single perspective, 360◦ videos encode a full, omnidirectional
view. Streaming the entire view of 360◦ videos leads to a
considerable waste of the network bandwidth, as users utilize
only a small portion, which contains their viewport, of the full
image. Researchers have proposed some notable algorithms to
reduce the bandwidth requirement.

A promising idea is to spatially split 360◦ videos into
tiles, and encode each tile in multiple bitrates. Users can
adaptively fetch tiles of different quality according to their
network conditions. A few research, like [8], choose to explore
the cooperate chance between several VR users that can
minimize the bandwidth consumption. Most still choose to
dedicate to single user streaming. Extending MPEG-DASH
SRD to the 3D space, Mohammad Hosseini [3] showcases a
dynamic view-aware adaptation technique to tackle the high
bandwidth demands of streaming 360◦ videos to wireless VR
headsets. Chao Zhou et al. [4] select tiles by solving a set of
integer linear programs independently on clusters of collected
user views. Yu Guan et al. [5] build a new quality model

that leverages three 360◦ video-speciﬁc factors. Based on the
model a variable-sized tiling scheme is proposed to strike a
balance between the perceived quality and video encoding
efﬁciency. In [7], Xiaolan Jiang uses RL to take advantage
of historical viewport and bandwidth information, to make
optimal adaptive decisions of tile quality.

III. SYSTEM DESIGN

In this section, we ﬁrstly present the system architecture of
OFB-VR. Secondly, we introduce each part in detail, including
the optical ﬂow based 360◦ specialized QoE metric PSNR-OF,
the versatile-size tiling scheme that can cooperate with it, and
the architecture of the ABR RL network.

A. System Model

The workﬂow of OFB-VR is depicted in ﬁgure 1. The
system can be divided into two parts, the edge server part and
the client part. At the edge server part, ﬁrstly, original videos
uploaded from capture devices to the edge computation server
are cut into chunks, and each chunk is split into basic tiles.
Secondly, we introduce a CNN-based optical ﬂow estimation
network to extract pixel-wise motions. Then, the PSNR-OF
calculation module takes users’ viewpoint motion, each tile’s
size in each quality level, and optical ﬂow estimation result
as input, to compute each basic tile’s PSNR-OF efﬁciency.
Finally, according to the PSNR-OF efﬁciency scores, adjacent
basic tiles with similar scores are merged and encoded into
the ﬁnal version to be stored in the edge content distribution
server. In the client part, the ABR RL network ﬁrstly acquires
the viewpoint of the user and computes the corresponding
PSNR-OF scores of each tile. Then make the optimal tiles’
quality choice according to its trained policy, aiming at
maximizing the weighted sum of target parameters. Detailed
descriptions of each part can be found in the following
subsections.

B. Optical Flow Estimation Based 360◦ QoE Awared Quality
Metric

1) Optical Flow Based Quantiﬁcation of Users’ Awareness
of Quality Distortion: When watching a 360◦ video, the user’s
attention can be attracted by a moving target and move rapidly

PSNR-F ModelEdge ComputingServerReinforce Learning NetworkOriginalVideoOptical Flow Based Motion and Depth EstimationMotionBasic TilingTilesGroupingEdge DistributingServerEncoderUserViewportCompute Velocity and Depth Mapsto follow it. Thanks to this unique and crucial characteristic,
background distortion in a 360◦ video can escape from the
user’s awareness under some speciﬁc condition. In this paper,
we take relative viewpoint velocity, and depth difference
between the attached object and background as our quantify
factor, to seize chances that can transfer parts of the video in
low quality without being noticed.

Relative viewpoint velocity: It’s a simple fact that humans
can’t perceive a moving target as clear and distinct as a static
one. When watching regular videos, it’s impossible to attain
the user’s viewpoint information. As a result, the encoder
can merely assume that users’ attention distributed evenly
and encode each pixel equally. However, VR has the inherent
advantage of knowing users’ viewpoint at any time. Lowering
the quality of parts, whose motion is dramatically different
from the user’s viewpoint motion, is a feasible and promis-
ing way to save bandwidth consumption without noticeable
quality decrease. For example, if the user’s attention is strictly
attached to a moving object, the object would seem stationary,
yet the background would seem moving backward. Under that
circumstance, blur and compression in the relative moving part
are hard to notice.

Relative depth difference: Similar to the relative view-
point, depth differences between the focused area and other
parts of the video can also cripple humans’ sensitivity about
quality decline. For example, if the user pays more attention
to a close object, namely the foreground object, background
elements can maintain QoE while decreasing bitrate.

Quantiﬁcation of Users’ Quality Loss Awareness: Above
mentioned two factors are subjective and ambiguous. With
the notion and empirical ﬁtting function of JND, researchers
can quantify the impacts of those factors. However, it remains
challenging to precisely extract depth information and pixel-
wise motion for its inherent complexity in the algorithmic
aspect. In this paper, we introduce real-time optical ﬂow
estimation in JND estimation, named JND-OFE.

JND-OFE is deﬁned as the minimal changes in pixel values
that can be noticed by viewers. JND-OFE formula can quantify
the threshold of quality drop that users can’t notice. In this
paper, we carry over the empirical ﬁtting function from [9],
then unify the unit and range of our parameters to assure its
validation.

JND-OFE of Relative viewpoint velocity, named SJND:

SJNDi,j(∆v) = 2.047 × ∆v0.634 + 8

JND-OFE of Relative depth difference, named DJND:

DJNDi,j(∆d) =

(cid:26) 9 × ∆d + 12
29 × ∆d − 8

∆d < 1
∆d ≥ 1

(1)

(2)

Inspired by Pano [5], we take the impact of these two factors of
JND as independent. So the joint JND-OFE can be expressed
as: JND-OFEi,j(∆v, ∆d) = SJNDi,j(∆v) × DJNDi,j(∆d)

To leverage the panoramic quality-determining factors, we
design a motion and depth extract module based on optical
ﬂow estimation. With the optical ﬂow map (cid:126)F and velocity of
viewpoint (cid:126)v, all it takes is to calculate(cid:12)
(cid:12) to get the

(cid:12) (cid:126)Fi,j − (cid:126)v(cid:12)

relative viewpoint velocity map ∆V . As for depth estimation,
it’s a little complicated. Although the velocity map isn’t equiv-
alent to the depth map, it’s possible to distinguish background
elements with foreground elements. Videos have at least 24
frames per second(FPS), which guarantees that the time gap
between two consequent frames can’t be longer than 41.6ms.
In such a short period, objects at distant, no matter moving or
static, seem static. Based on this, according to (cid:126)F , a mask of
background can be concluded. Then, normalize all elements
and set the value of background elements to 0 to get velocity
depth base D. After that, take the difference map ∆D between
D and (cid:126)v as the output of relative velocity depth. The last step
is to respectively calculate SJND and DJND and produce the
joint JND result.

2) PSNR-OF, A QoE Metric concerning users’ awareness
of quality distortion: OFB-VR proposes a new QoE metric
speciﬁes on 360◦ videos. The new metric is based on Peak
Signal-to-Noise Ratio (PSNR) and the proposed quality loss
awareness factors. With real-time optical ﬂow estimation, we
can accurately acquire users’ tolerance upper bound of quality
distortion, the JND-OFE score. We insert JND-OFE in PSNR’s
MSE calculation. When calculating the difference between an
original pixel and its encoded version, results lower than the
JND-OFE threshold are ignorable. The new metric is referred
to as Peak Signal-to-Noise Ratio through Optical Flow (PSNR-
OF), which can ignore the impact of quality distortions in
different moving direction or depth layer. In other words,
PSNR-OF can evaluate panoramic videos in a way that accord
to intuitive human feelings. PSNR-OF can be presented as
follow:

PSNR-OF(l) = 20 × log10

2N − 1
(cid:112)MSE(l)

(cid:80)row
i

(cid:80)col
j

MSE(l) =

(cid:2)((cid:12)

(cid:12)Ii,j(l) − ˆIi,j(l)(cid:12)
col × row

(cid:12)) × ki,j(l)(cid:3)2

ki,j(l) =

sgn((cid:12)

(cid:12)Ii,j(l) − ˆIi,j(l)(cid:12)
(cid:12) − JND-OFEi,j) + 1
2

(3)

(4)

(5)

Generally, for uint8 data, N = 8. In the formula. m and n
respectively denote rows and columns of the original image.
Ii,j stand for the pixel value at (i, j) of the original image,
and ˆIi,j(l) for that of image encoded at quality level l.

C. Versatile-size Tiling Scheme

After calculating the pixel-wise PSNR-OF, it’s sufﬁcient to
carry on to perform the versatile-size tiling scheme. Firstly,
ﬁne-grained split the original chunk into basic tiles with a
12 × 24 grid and compute mean PSNR-OF in each tile as
tile-wise PSNR-OF. Secondly, compute PSNR-OF efﬁciency
E of each tile, which stands for average PSNR-OF increase
per quality level. PSNR-OF efﬁciency can be denoted as:

Ei,j =

PSNR-OFi,j(lhigh) − PSNR-OFi,j(llow)
lhigh − llow

(6)

Finally, with the tile-wise efﬁciency E, we can group similar
tiles. The overall goal is to group 288 basic tiles into ﬁxed

number K, say 50, of coarse-grained tiles while minimizing
PSNR-OF efﬁciency’s total variance. In general, our algorithm
uses the rectangle of the whole frame as a beginning. It itera-
tively chooses the best cut among all possibilities, horizontally
or vertically. It repeats the same procedure to cut two tiles
until there are K tiles. The detail implementation is described
in algorithm 1.

Algorithm 1 Versatile-size Tiling Algorithm

T iling(1, 12, 1, 24)

function T iling(x1, x2, y1, y2)

cutDirection, cutP osition, availableSteps ←

GetBestCut(x1, x2, y1, y2)

if availableSteps == 0 then

return

end if
if cutP osition == horizontal then
T iling(x1, cutP osition, y1, y2)
T iling(cutP osition + 1, x2, y1, y2)

else

T iling(x1, x2, y1, cutP osition)
T iling(x1, x2, cutP osition + 1, y2)

end if
end function

function GetBestCut(x1, x2, y1, y2)

Calculate best cutP osition and cutDirection that can

minimize PSNR-OF variance;
Renew availableSteps;
return cutDirection, cutP osition, availableSteps

end function

D. Reinforce Learning ABR network

In this paper, we implement the state-of-the-art actor-critic
method, A3C, as our training algorithm. The A3C algorithm
uses two networks, an actor and a critic. The actor takes action
πθ(s) according to current state s to maximize the cumulative
reward. The critic evaluates actions from the actor and helps
it to update its policy network parameters θ.

Training a network to decide the tile-wise quality level is
tough because of the enormous action space. To be more
speciﬁc,
if each tile has 6 possible versions of different
quality, and the video is split into 12 columns and 6 rows,
namely 72 tiles, the size of its action space would be 572. So
we propose a simpliﬁed alternative, all tiles are sorted into
three types: Areas that near the viewpoint, which are referred
to as core areas; Areas that surround the core area, which
are referred to as surrounding areas; Areas that users can’t
see, which are referred to as outside areas. Our network in
charge of giving the overall bandwidth upper bound of each
area. Tile-wise quality selection tasks are left to the local
dynamic programming (DP) algorithm. Accordingly, our state
st, namely network input, is deﬁned as:

st =

(cid:16) (cid:126)Pt, (cid:126)τt, (cid:126)cot, (cid:126)sut, (cid:126)outt, buft, ratiot, accout

(cid:17)

(7)

Where (cid:126)Pt, (cid:126)τt, (cid:126)cot, (cid:126)sut, (cid:126)outt respectively stand for PSNR-F,
chunk download time, core area total bitrate, surround area
total bitrate, outside area total bitrate. Each of them is a
m dimensional vector that accordingly records its historical
data. m intends the referred historical data’s period length.
buft is the available buffer length, ratet is the actual mean
bitrate while downloading the last chunk, ratiot is the ratio
of basic tiles that predicted to be outside but present in the
real viewport. In our network, there are 5 identical 1D-CNN
with 128 ﬁlters to receive inputs, each of size 3 with stride
1, and other 3 full connected NN with 128 neurons to receive
buft, ratet, accout. A full-connected layer with N neurons is
responded to receiving outputs from these layers, where N is
the size of action space, namely the overall possible quantity of
QP choice of each tile. The critic network shares a similar NN
structure with the actor network except for the output layer.
The critic network only has one neuron as its output.

We deﬁne a total reward function to reﬂect the value of
current action, concerning above mentioned PSNR-OF as our
main reward factor. Rebuffer ratio, a typical video streaming
QoE metric as our main penalty factor. Besides, we take
outside area ratio into account as a penalty factor as well,
since the outside area is meant to be transferred in relatively
low quality or even not to be used at all. The reward function
can be denoted as:

reward =

N
(cid:88)

n=1

Pn − α

N
(cid:88)

n=1

Rtn − β

N
(cid:88)

n=1

ration

−

N −1
(cid:88)

n=1

|Pn+1 − Pn|

(8)

For a video with N chunks, Pn is the uniﬁed mean PSNR-
OF score of the n rd chunck. Rtn stands for the rebuffering
time of the n rd chunk while downloading. ration is the
outside area ratio of the n rd chunk. We use this reward
function as the metric to evaluate the actor network’s decision.
training method. It uses the
gradient to help update policy parameters. Given the policy
network parameters θ, the gradient can be denoted as:

A3C is a policy gradient

∇θR(θ) = Eπθ [∇θlogπθ (s, a) Aπθ (s, a)]

(9)

Aπθ (s, t) is the advantage function:

Aπθ (s, t) = rt+γrt+1+...+γn−1rt+n−1+γnV πθ (s(cid:48))−V πθ (s)
(10)
where V πθ (s) is the output of the critic network in state
s and policy parameters θ. s(cid:48) is the new state after taking
action πθ(s). With the gradient function, the actor network’s
parameters θ can be updated with learning rate α:

θ = θ + α∇θlogπθ(s, a)Aπθ (s, t)

(11)

IV. EXPERIEMTS AND EVALUATION

In this section, we ﬁrstly provide set up of our videos,
viewpoint, and bandwidth datasets, and parameters of our
RL network. Then we evaluate OFB-VR’s performance with
both user study and viewpoint-based simulation. User study

aims to examine whether PSNR-OF can ﬁt intuitive human
feelings and form a trace database for followed up simulations.
After that, we perform a viewpoint-based simulation in ideal
and stable bandwidth conditions to quantitatively evaluate the
improvement of our method on bandwidth saving and QoE
enhancing. Finally, we train our RL network with practical
bandwidth datasets to evaluate the overall performance and
compare it with baseline schemes.

A. Experiment Setup

video and viewpoint dataset: Our video database consists
of 20 360 videos that add up to over 60 minutes. Each video’s
resolution is 2880*1440, and FPS is 30.

Each videos is encoded into 5 quality levels, QP = 20, 25,
30, 35, 40, of one-second chunks with the x264 encoder for
subsequent procedures. Our 10 volunteers are asked to watch
those videos and record their viewpoint trace respectively, as
our viewpoint datasets.

ABR Reinforce Learning network parameters and
dataset: In the state input part, we set dimension of input
vectors m = 8, to recall past 8 seconds’ experience. Possible
quality level choices number to 6, including qp(20, 25, 30, 35,
40) and a blank level. As a result, the number of action space
is 63, since each area has 6 possible choices.In the reward
function, we set α = 33, β = 1.During training, we set the
discount factor γ = 0.99. The learning rates for the actor and
the critic are both set to 0.0001.

We use a dataset from [10]. The dataset contains avail-
able bandwidth in real 4G/LTE networks within the city of
Ghent, Belgium, from 2015-12-16 to 2016-02-04. It includes
bandwidth tracks recorded from different transportation, car,
train, tram, bus, bicycle and foot. We adopt this dataset mainly
for its drastic variation in bandwidth, which can test
the
robustness of our trained network. The throughput grows up
to 95Mbps at some cases, and drop to severed limited when
occasionally switching to 3G. However, the mean throughput
of the dataset is up to 35Mbps, which is so adequate for
single video streaming that can even streaming videos from
our datasets in full size. So we scale down the dataset to
two lower mean throughput version. The higher one has a
mean throughput of 5Mbps, and the lower one has a mean
throughput of 2Mbps. There are 40 logs in total, and each has a
relatively limited duratio ranging from 166 to 758 sec. During
traning we iterately chose a random track in the bandwidth
dataset to overcome the limitation.

Baseline setup We take two recently proposed DASH based
VR streaming scheme, Plato and Pano, as baselines. Both of
them are viewport-driven as well. Pano is a variable-size tiling
scheme with traditional JND and DP ABR. We reconstruct its
DP ABR part, to make sure it’s compatible with the proposed
RL ABR system. Plato is a typical ﬁxed-size tiling scheme.
Considering that it doesn’t make use of the user’s quality
awareness, to make it comparable, we reconstruct it with the
PSNR-OF metric. However, despite the introduction of JND-
OFE, its JND-OFE factor is always 1, according to equation
5. So, in fact, PSNR-OF in Plato degrade to regular PSNR.

B. Validation of Optical Flow Based JND

Fig. 2. Speed Estimation Error

Fig. 3. CDF of Speed Estimation Error

In OFB-VR, the accuracy of JND estimation strictly de-
pends on the accuracy of motion ﬁeld estimation. As a
result, it is sufﬁcient to evaluate JND estimation results by
analyzing the result of the speed estimation result. Speciﬁcally,
in our scheme, it is relative speed derived from optical ﬂow
estimation, while in traditional JND, like Pano, it is derived
from CNN-based object detection. Because CNN-based object
detection’s inherent limitation in generalization, it can mis-
judge or even can’t recognize some untrained target at all. As
a result, Pano chooses to use the relative speed lower bound
in recent history as a rough substitute of a precise pixel-wise
relative viewpoint. As for Plato, to make it comparable, we
unify the QoE metric with PSNR-OF. However, since it doesn’t
introduce any QoE concerned notion, its JND-OFE value is
always set to 0, namely, it is absent in the comparison of
relative value accuracy.

In ﬁgure 2, though optical ﬂow estimation error can lead
to variant errors of relative speed calculation, our method still
outperforms the conservative prediction made by speed lower
bound. The improvement is evident in the speed estimation
error’s CDF, ﬁgure 3. No doubt, the more accurate the JND
estimation is, the better the performance in bandwidth saving
or QoE it can get.

C. Improvement on Bandwidth Consumption and QoE

Fig. 4. Overall QoE Improvement

Fig. 5. Overall Bandwidth Saving

In this part, we make an simulation on three VR streaming
scheme with stable bandwidth, including OFB-VR, Pano, and
Plato. We run this simulation in order to analysis theoreti-
cal joint improvement of proposed versatile-size scheme and
PSNR-OF. To ensure fair compression, We unify the adaptive
logic and viewpoint prediction method. The result is concluded
in the same video and trace database.

050100150200250300350400450time / frame050100150200250rotate speed / degsgroundtruthPanoOFB-VR012345error ratio / %00.20.40.60.81CDFEmpirical CDFPanoOFB-VR1000150020002500300035004000450050005500Bandwidth Consumption / kbps4045505560657075PSNR-OF / dBfixed tiling with traditional JND(Plato)variable tiling with traditional JND(Pano)variable tiling with flow JND(OFB-VR)6264.367.771.874.270.15060.466.75055606570PSNR-OF/dB0100020003000400050006000Bandwidthrequirement/kbpsOFB-VRPanoPlato162017302510180019202900210023403500256029004200324039305200V. CONCLUSION

By leveraging relative velocity and depth difference as QoE
awareness quantify factors, OFB-VR extends the traditional
video QoE metric, PSNR, to seize the user’s awareness of
QoE loss. With the help of PSNR-OF, the versatile-size tiling
scheme is proposed to take the place of the ﬁxed-size tiling
scheme by grouping tiles with similar PSNR-OF efﬁciency
together. Base on this, an ABR RL network is designed
to adaptively make optimal choices of each tile’s quality
for streaming. The performance of the proposed system is
evaluated on a real LTE bandwidth database. The experiment
result proves the validation of the proposed optical-ﬂow-based
JND model, and quantitively evaluate the improvement of our
method over two baselines. It turns out that our method is
useful to save bandwidth resources, especially when providing
users with high-quality panoramic content.

REFERENCES

[1] V. Petrock.
2019.
and-augmented-reality-users-2019

(2019, Mar.) Virtual and augmented reality users
[Online]. Available:https://www.emarketer.com/content/virtual-

[2] L. A. Hormaza, W. M. Mohammed, B. R. Ferrer, R. Bejarano, and
J. L. Martinez Lastra, “On-line training and monitoring of robot tasks
through virtual reality,” in 2019 IEEE 17th International Conference on
Industrial Informatics (INDIN), vol. 1, July 2019, pp. 841–846.

[3] M. Hosseini, “View-aware tile-based adaptations in 360 virtual reality

video streaming,” in 2017 IEEE Virtual Reality (VR), 2017.

[4] C. Zhou, M. Xiao, and Y. Liu, “Clustile: Toward minimizing bandwidth
in 360-degree video streaming,” in IEEE INFOCOM 2018 - IEEE
Conference on Computer Communications, April 2018, pp. 962–970.
[5] Y. Guan, C. Zheng, X. Zhang, Z. Guo, and J. Jiang, “Pano: Optimizing
360 video streaming with a better understanding of quality perception,”
in Proceedings of the ACM Special Interest Group on Data Communi-
cation, 2019, pp. 394–407.

[6] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“Flownet 2.0: Evolution of optical ﬂow estimation with deep networks,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 2462–2470.

[7] X. Jiang, Y.-H. Chiang, Y. Zhao, and Y. Ji, “Plato: Learning-based adap-
tive streaming of 360-degree videos,” in 2018 IEEE 43rd Conference on
Local Computer Networks (LCN).

IEEE, 2018, pp. 393–400.

[8] X. Zhang, X. Hu, L. Zhong, S. Shirmohammadi, and L. Zhang, “Coop-
erative tile-based 360 panoramic streaming in heterogeneous networks
using scalable video coding,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 30, no. 1, pp. 217–231, Jan 2020.

[9] Y. Zhao, L. Yu, Z. Chen, and C. Zhu, “Video quality assessment based
on measuring perceptual noise from spatial and temporal perspectives,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 21, no. 12, pp. 1890–1902, 2011.

[10] J. van der Hooft, S. Petrangeli, T. Wauters, R. Huysegems, P. R. Alface,
T. Bostoen, and F. De Turck, “Http/2-based adaptive streaming of hevc
video over 4g/lte networks,” IEEE Communications Letters, vol. 20,
no. 11, pp. 2177–2180, Nov 2016.

As depicted in ﬁgure 4, ORB-VR can increase PSNR-OF
at most by 4 dB in comparison with Pano, or 7.4 dB in
comparison with Plato, while no extra bandwidth is required.
It’s evident that the ﬁxed-tiling scheme severely harms Plato’s
performance.it
is competitive only when the bandwidth is
adequate to stream each tile in high quality. According to
ﬁgure 5, to maintain fundamental PSNR-OF score, OFB-VR
and Pano need a similar bandwidth, because JND-OFE can
only make a constrained contribution with limited bandwidth.
However, to present the user with medium or high quality,
say 70 PSNR-OF scores, OFB-VR can save up to 17.6%
bandwidth in comparison with Pano. while comparing with
Plato, to achieve the same PSNR-OF score, OFB-VR only
needs about 60% bandwidth.

D. Performance of the ABR RL network

Fig. 6. ABR Reinforce learning net-
work performance

Fig. 7. Sample of bandwidth datasets

After the independent test of the QoE evaluation part, we
integrate them into the proposed ABR RL module, to test the
overall performance of the proposed system. In this part, since
we use the practical LTE bandwidth dataset to take the place
of ideal and ﬁxed bandwidth, we jointly take the rebuffer rate
and PSNR-OF as our evaluate metric. To carry on the test, for
each scheme, we randomly feed one video’s PSNR-OF and
viewpoint from our video dataset to the RL network. Then
iteratively repeat the procedure until all video is tested.

Figure 7 is the practical bandwidth track we used. The two
tracks are derived from the original dataset with different scale
down proportions. The lower one only has 5Mbps drastically
changed bandwidth, while the higher one has an adequate
bandwidth of 10Mbps, but it ﬂuctuates signiﬁcantly as well.
The performances of the ABR RL network with three
difference schemes are respectively given in ﬁgure 6. The
ﬁgure presents the PSNR-OF score and rebuffer rate jointly.
Each point reﬂects the mean PSNR-OF and mean rebuffer
rate during one video’s processing. The closer the distance
between the right-bottom and the point, the better the general
performance the scheme has. Mean PSNR-OF scores of three
schemes are 71.09, 64.90, and 61.4, respectively. In other
words, our system can increase the mean PSNR-OF score by
9.5-15.8% while maintaining the same rebuffer ratio compared
with Pano and Plato in a ﬂuctuate LTE bandwidth dataset. It
is obvious that the proposed OFB-VR outperforms Pano in
PSNR-OF while maintaining the rebuffer ratio at the same
level.

556065707580PSNR-OF/dB00.020.040.060.080.10.120.140.160.180.2Rebuffer Rate/%OFB-VR with high bandwidthOFB-VR with low bandwidthPanowith high bandwidthPanowith low bandwidthPlato with high bandwidthPlato with low bandwidth0200040006000800010000120001400016000Time/s0510152025303540Badwidth/Mbpshigh bandwidthlow bandwidth