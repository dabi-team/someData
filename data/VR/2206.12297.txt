SAQAM: Spatial Audio Quality Assessment Metric

Pranay Manocha1∗, Anurag Kumar,2 Buye Xu,2 Anjali Menon,2 Israel D. Gebru,2 Vamsi K. Ithapu,2 Paul Calamia2

1Department of Computer Science, Princeton University, Princeton, NJ, USA
2Meta Reality Labs Research, Redmond, WA, USA
pmanocha@cs.princeton.edu, {anuragkr90, xub, aimenon, idgebru, ithapu, pcalamia}@fb.com

2
2
0
2

n
u
J

4
2

]
S
A
.
s
s
e
e
[

1
v
7
9
2
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

Audio quality assessment is critical for assessing the perceptual
realism of sounds. However, the time and expense of obtaining
“gold standard” human judgments limit the availability of
such data. For AR&VR, good perceived sound quality and
localizability of sources are among the key elements to ensure
complete immersion of the user. Our work introduces SAQAM
which uses a multi-task learning framework to assess listening
quality (LQ) and spatialization quality (SQ) between any given
pair of binaural signals without using any subjective data. We
model LQ by training on a simulated dataset of triplet human
judgments, and SQ by utilizing activation-level distances from
networks trained for direction of arrival (DOA) estimation. We
show that SAQAM correlates well with human responses across
four diverse datasets. Since it is a deep network, the metric is
differentiable, making it suitable as a loss function for other
tasks. For example, simply replacing an existing loss with our
metric yields improvement in a speech-enhancement network.
Index Terms: spatial audio quality, listening quality, spatializa-
tion metric, binaural audio, speech enhancement

1. Introduction

Audio quality plays a fundamental role in many applications
affecting the quality of listening experiences like AR&VR. To
create an immersive AR&VR experience, both graphics and
spatial audio need to be of high quality and synchronized to the
user’s head movements in real time. In addition, audio sources
must be accurately positioned such that they are localized at the
targeted location. It is well-known that a mismatch between the
stored auditory schemata of the expected listening scene and the
perceived schemata of a simulation yields a signiﬁcant decrease
in presence and immersion in these multi-sensory systems [1].
Therefore, sound-quality evaluation tests are critical since they
provide the necessary user feedback that drives improvements in
these technologies. Listening tests in AR&VR via user studies
demand more time and effort than conventional audio tasks as
they require assessment across many attributes [2] (e.g. listening
quality, spatial localization etc.). Thus, automatic (objective)
SQA methods are more practical.

Recent research has focused on learning a spatial audio
metric from known binaural auditory cues like Interaural Level
Differences (ILD), Interaural Time Differences (ITD) and Cross-
Correlation (IACC) between signals entering the left and the
right ear [3–6]. However, these methods suffer from various
general drawbacks. First, these methods are designed to only
assess spatialization quality (SQ) that ensures how accurately
the test sources are positioned as compared to the reference, but
cannot assess effects of audio ﬁdelity degradation’s, also referred
to as listening quality (LQ). It includes undesired sounds and
distortions that add artifacts to the audio signal and cause a loss
in immersiveness. Second, these methods are full-reference and

∗Work done during internship at Meta.

always require a clean signal for comparison (that share the same
‘content’ - matched reference), and thus cannot be applied in
scenarios where the paired clean reference is unavailable. Third,
they work well under anechoic conditions but are inaccurate
under reverberant conditions. Fourth, they assume that the test
signal is created in relation to the reference, meaning that the
two signals are time-sligned and of equal length which may
not be valid. Finally, none of the metrics are differentiable,
and thus cannot be directly leveraged as a training objective
in downstream tasks. Addressing a few concerns, researchers
have proposed AMBIQUAL [7] that compares multi-channel
ambisonic recordings across LQ and SQ. However, it does not
account for head listening direction, and the inﬂuence of Head
Related Transfer Functions (HRTFs) in the resulting recording.
Moreover, it also requires access to a clean, matched reference,
and is also non-differentiable.

Manocha et al. [8] proposed a full-reference deep perceptual
spatialization metric (DPLM) that evaluates the similarity of
binaural presentation in terms of localization under realistic,
echoic conditions. DPLM computes deep-feature distances
between the full-feature activation stacks of direction-of-arrival
(DOA) models to assess SQ and correlates well with human
perceptual judgments. Moreover, the metric is differentiable and
does not demand access to a clean reference or time-aligned
signals. However, it does not assess LQ.

In this paper, we introduce a novel metric that addresses
some of the above concerns. We propose SAQAM that evaluates
the LQ and SQ between any pair of binaural signals. The
inspiration for the approach comes from human’s ability to
do the same. Given two completely random audio (“non-
matched”) recordings, it is highly likely that a human would
be able to compare them across quality attributes irrespective
of the audio content [9]. We ﬁrst design a LQ assessment
model that assesses the audio ﬁdelity degradations, and is
trained on a carefully simulated dataset of perturbations that
mimic realistic environments. Next,
these recordings are
presented to the model as triplets, and the model is trained
using triplet metric learning [10, 11], by carefully selecting
triplets to have different audio content(s) which ensures content
invariance. Next,
to assess SQ, we build on top of the
DPLM [8] approach that addressed the perceptual characteristics
of localizing sounds. We combine these two tasks using
a multi-task learning framework [12] to leverage the useful
information contained in related tasks to improve generalization
performance on both tasks. We show that, even in the absence of
explicit perceptual training, these distances correlate well with
human perceptual judgments (both via objective and subjective
evaluations). We also show that the resulting metric generalizes
well even for distinct (yet related) tasks such as audio codecs,
and binaural reproduction from mono or multi-channel signals.
Finally, we show that adding SAQAM to the loss function yields
improvements to the existing state-of-the-art model for binaural
speech enhancement.

 
 
 
 
 
 
Figure 1: SAQAM framework: Our framework takes two
inputs x1 and x2, and estimates a rating for each attribute -
spatialization quality (SQ), listening quality (LQ) as well as
overall quality that combines all above attributes (OVRL). Note
that all distances are deep-feature distances.

2. The SAQAM Framework

Our framework SAQAM is designed to assess the quality of a
given pair of binaural signals across various attributes. In this
paper, we consider two such attributes: listening quality (LQ)
and spatialization quality (SQ), but the framework can be eas-
ily extended to include more such attributes. Additionally, the
SAQAM framework can also estimate a combined, overall at-
tribute (OVRL) that implicitly combines all individual attributes.
Refer to Fig 1. Given two binaural signals (x1 and x2), our goal
is to compute distance functions D1(x1, x2), D2(x1, x2) and
D3(x1, x2) that characterize LQ, SQ, and OVRL between the
signals. The distance function is designed to be non-negative
and monotonic, thereby making it a pseudo-metric (we do not
impose triangle or associative properties).

2.1. Multi-task learning

Multi-task learning (MTL) [12] has been beneﬁcial to many
speech applications [13, 14]. MTL aims to leverage useful
information contained in multiple related tasks to help improve
the generalization performance on all tasks. In our case, we
consider LQ and SQ prediction as the two tasks. MTL makes the
model easier to use - in the sense that if only the spatialization
results are needed or if only the listening quality is required, the
appropriate task-head can be used while discarding the others.
Moreover, since MTL encourages learning shared information,
the common shared layers of the model can be used in cases
where an overall estimate of quality (OVRL) is required rather
than individual estimates of LQ and SQ.
LQ head: is trained using a triplet comparison dataset where
we simulate human judgments on a simple question: “Is A or
B closer to reference C?”. We algorithmically modify clean
recordings under various real-life degradations commonly found
in audio processing tasks including compression, additive noise,
speech distortions and binaural recorded noises across sources.
SQ head:
is re-purposed from training a sound source-
localization model that predicts the direction-of-arrival (DOA)
of a given sound source, similar to DPLM [8]. We train DOA
models with carefully designed input perturbations as data aug-
mentations that mimic realistic environments.

2.2. Architecture

Refer to Fig 2. The architecture of both tasks consists of 3
components: feature-extraction block, temporal aggregation
block, and task speciﬁc heads for LQ and SQ. In the MTL
framework, the ﬁrst two blocks are shared between tasks.

For the feature-extraction block, we used an Inception [15]
styled architecture. The 6-block Inception network consists of
1x1, 3x3 and 5x5 ﬁlters, leading to 3x3 max-pooling, and a
1x2 maxpool along the frequency dimension. For the temporal
aggregation block, we use Temporal Convolutional Networks
(TCNs) consisting of 4 temporal blocks., with each block
consisting of 2 convolutional layers with a kernel size of 1x3. We
use weight normalization [16] along with dilated convolutions
to increase the effective receptive ﬁeld of our model.

Figure 2: SAQAM architecture: consists of feature extraction
(Inception [15]), temporal aggregation (TCN), and multi-task
learning heads, one each for LQ and SQ.

For the LQ task, we use 2 conv.

layers to transform
the embedding size to 64 per frame. These are then mean-
pooled in the temporal dimension, the resulting embedding
representing the LQ of the signal. For the SQ task, we divide
azimuth (∈ (−180◦, 180◦)) into 50 equally spaced bins, and
the elevation (∈ (−90◦, 90◦)) into 25 equal bins. The resulting
embedding from the aggregation stage is then mapped to a one-
hot vector, representing the location of the sound source. Note
that both task-heads output a framewise estimate.

2.3. Loss Functions

We use different losses to train the LQ and SQ heads. For LQ,
we use deep metric learning with a triplet-loss as the basis to
train the LQ head. Formally, we deﬁne the triplets as a set
T = {ti}N
n} with xa
the anchor sample, xp the positive sample, and xn the negative
sample. Then, we deﬁne the triplet loss as:

i=1 where each triplet ti = {xi

a, xi

p, xi

L(t) = max{0, D1(xa, xp) − D1(xa, xn) + δ}.

(1)

Here δ is the margin value to prevent trivial solutions, and
D1(x1, x2) is the deep-feature distance [17] between the full-
feature activation stacks between the two audio inputs.

For the SQ metric, we use the same classiﬁcation formula-
tion as Manocha et al. [8]. However, instead of the conventional
cross-entropy loss, we use the earth mover’s distance (EMD) as a
loss function for training. It focuses on inter-class relationships,
whereas plain cross-entropy loss ignores any relationship be-
tween classes. This is especially useful if the classes are ordered
since that leads to a closed form solution, which is:

EMD2(ˆp, p) =

N
(cid:88)

( ˆPn − Pn)2

(2)

n=1

where Pn is the n-th element of the cumulative density function
of p, and ˆp is the predicted output after softmax at the ﬁnal layer.
We follow a normal label distribution around the correct class,
and use a discrete soft label distribution compared to one-hot:

pn =





0.4 n=v
0.2 n=v±1
0.1 n=v±2
0

otherwise

(3)

where v is the index corresponding to the ground truth localiza-
tion. Note that we also enforce continuity between the ﬁrst and
the last bins for the azimuth space.

2.4. Usage

Given a test input x1, and a reference signal x2, the LQ
score is obtained from the LQ task head of the network by
computing the deep-feature distances across activation maps
D1(x1, x2). Similarly, the SQ score is obtained from the SQ
task head of the network by computing the deep-feature distances
across activation maps D2(x1, x2). Finally, the OVRL score
is obtained from from the shared body (temporal aggregation
block) of the network by computing the deep-feature distances
across activation maps D3(x1, x2). Note that the reference
signal does not need to contain the same speech (or content -
hence ‘non-matching’) as the test signal.

Listening Quality Task HeadFeature ExtractionTemporalAggregationSpatialization Task Headx1x2Listening Quality D1(x1,x2)(LQ)Spatialization Quality D2(x1,x2) (SQ)Overall Quality D3(x1,x2)(OVRL)1x11x11x1PoolBNBNBN3x35x51x1BNCONCATENATEPool (stride=4)1x3WN1x3WN1x31x3WN1x3WNFeature Extraction (X 6)TCN (X 4)Task HeadsLQSQ1x3WN1x3WN1x3WN1x3WN3. Experimental Setup

3.1. Datasets and training

Speech recordings from the TIMIT [18] dataset are used as
the source for anechoic recordings (Dc). We used a pool
of 11 publicly available Binaural Room Impulse Response
(BRIR) databases including ADREAM [19], AIR 1 4 [20],
BRAS [21], Huddersﬁeld [22], Ilmenau [23], IoSR [24], Old-
enburg IE BTE [25], Rostock [26], TU Berlin [27], and Sal-
ford [28]. The resulting pool contained approximately 125k
BRIR pairs from 36 different rooms (RT60 ranging from 0.08
to 0.97 sec). All single channel additive noises are sampled
randomly from the DNS Challenge [29].

For the perturbation set, we consider all degradations
commonly found in various audio processing tasks including
additive noise, speech distortions (e.g. clipping and frequency
masking, frequency resampling, pitch shifting), compression
(e.g., mu-law and MP3), and recorded binaural sounds [30, 31].
We also use the data collected using the binaural multichannel
wiener ﬁlter (MWF) [32] algorithm, and ﬁnd that adding datasets
and perturbations with subtle differences increases the robustness
of our model to small differences.

p, xi

a, xi

To obtain the simulated triplet set (ti = {xi

n}), we
ﬁrst sample three clean recordings (sa, sp, sn) from Dc and
binauralize. Next, a perturbation is sampled from the above men-
tioned perturbation set, and applied to (sa, sp, sn) at three dif-
ferent levels uniformly sampled from a range (-20dB to +30dB)
to create (xa, xp, xn). It is ensured that the perturbation lev-
els of xa and xp are closer than those for xa and xn to satisfy
our triplet formulation. Note here that the inputs to be model
(xa, xp, xn) are created by sampling different clean recordings,
but adding the same noise at different levels. This encourages
the model to be invariant to recording content. These ﬁndings
can be treated as a pilot study for future work on a dataset of
subjective quality preferences.

We use an adaptive margin (δ) strategy that starts out with
a low value (δ = 0.5), and increases gradually (δ = 1.50) to
ensure robust training [33]. We use 3-sec excerpts for training.
For both heads, phase and magnitude spectrogram are extracted
from the 2 channels using a 512-point DFT with a hamming
window of length 512, and 50% overlap. We use a learning rate
of 10−4 with a batch size of 64 for training. As part of online
data augmentation to make the model invariant to small delay,
we randomly add a 0.25s silence to the audio at the beginning or
the end to provide shift-invariance property to the model. For the
localization model, we follow the same training methodology as
in Manocha et al. [8].

3.2. Baselines

For LQ, since there does not exist any metric that assesses audio
ﬁdelity degradations of binaural signals, we compare it with
a single channel quality metric like PESQ [34]. For SQ, we
compare our approach to the DPLM approach of Manocha
et al. [8], and the BAMQ approach of Flessner et al. [4] that
estimates the binaural cues at frame-level and combines them
using a set of learned weights to output an overall quality
between two recordings.

Figure 3: SAQAM with angular distance: This shows SAQAM’s
variation with angular distance for three ﬁxed reference posi-
tions compared across non-matched content

4. Results

4.1. Objective evaluations

Objectively, we compare various metrics on (i) robustness
to content variations; (ii) clustering in learned space; and
(iii) monotonic behavior with increasing perturbation levels.
Refer to Table 1 for the results. Note that none of the above-
mentioned baselines accepts non-matched inputs, except DPLM,
which only assesses SQ.
Robust to content variations To evaluate robustness to non-
matched recordings, we create a test dataset of two groups:
ﬁrst group consists of pairs of recordings with the same quality
attributes (LQ and SQ) but different speech content; the other
group consists of recordings with different quality attributes and
speech content. We calculate the common area between these
normalized distributions. Our SAQAM metric has the lowest
common area, suggesting that it is agnostic to speech content.
Clustered Embed. Space: To better understand the learnt
representations, we evaluate the retrieval performance of our
model using Precision@K to measure the quality of the top K
items in the ranked list. This is done for both LQ and SQ task
heads. We divide both attributes into 10 different perturbation-
level groups, with each group consisting of 100 recordings with
the same perturbation and level, but non-matched content. We
take randomly selected queries and calculate the number of
correct class instances in the top K retrievals. We report the
mean precision@K (MPk) over all test queries which shows
high precision retrievals that suggests that the embeddings indeed
capture these quality attributes.
Monotonicity To show our metric’s monotonicity with increas-
ing angular distance and noise, we create a test dataset having
non-matched audio recordings at increasing perturbation levels.
To check for monotonicity in SQ, we evaluate SAQAM between
a ﬁxed reference, and a moving test source for three different
source positions compared across recordings of non-matched
content (see Fig 3). Manocha et al. [8] showed the trend for
recordings having matched content, and here we see that the
general trend is similar. To check for monotonicity in LQ, we
calculate the Spearman rank order correlation (SC) between the
distance from the metric and the perturbation levels. The abso-
lute correlation is high suggesting that the score from the metric
decreases with increasing noise level.

Name

Attribute CommonArea↓ M P10 ↑ M P25 ↑ Monotonicity↑

4.2. Subjective evaluations

Proposed

DPLM

SQ
LQ
SQ

0.19
0.23
0.25

0.92
0.87
0.87

0.89
0.79
0.82

0.94
0.92
0.93

Table 1: Objective evaluations. Sec 4.1 describes common area,
mean precision, and monotonicity (using Spearman correlations)
across simulated datasets. ↑ or ↓ is better.

We use previously published diverse third-party studies to verify
that our trained metric correlates well with subjective ratings
related to their task. We compute the correlation between the
model’s predicted distance with the publicly available subjective

Azimuth Angles (from -90 to +90 deg)Metric’s Distance16014012010080604020-75-50-250255075Az = -45 degAz = 0 degAz = 45 degType

Name

P1

P1’

P2

P3

P4

Speech Castanets Guitar

Speech Castanets Music

Speech

Pink Noise Guitar

Pink Noise Vocals Castanets Glocken

EM AM

Conven.

Proposed

PESQ
SQ
OVRL
DPLM
BAMQ

0.09
0.78
0.86
0.69
0.42
Table 2: Subjective evaluation (1): Compared across overall ratings. Models include: Conventional model (e.g., PESQ), our proposed
models (SQ and OVRL models), Baseline models including DPLM and BAMQ. Spearman Correlation (SC). ↑ is better.

-0.02
0.53
0.66
0.53
-0.02

-0.05
0.47
0.47
0.47
-0.05

0.01
0.52
0.64
0.45
-0.17

-0.04
0.69
0.70
0.67
0.23

-0.20
0.23
0.40
0.22
0.65

0.11
0.60
0.89
0.61
0.36

0.08
0.82
0.88
0.83
0.18

0.13
0.49
0.77
0.42
0.11

0.08
0.26
0.69
0.06
0.08

0.07
0.88
1.0
0.83
0.52

0.17
1.0
0.94
0.94
0.77

0.12
0.94
0.94
0.94
0.09

0.01
0.96
1.0
0.94
0.03

0.23
0.95
1.0
0.94
0.83

Baseline

Type

Name

P3 (SQ)

P3 (LQ)

PESQ↑

STOI↑

L2↓ M.STFT↓

Si-SDR↑

Speech

Pink Noise Guitar

Speech

Pink Noise Guitar

Noisy

1.15

70.9

0.058

0.32

-1.51

Conven.

Proposed

Baseline

PESQ
SQ
LQ
DPLM
BAMQ

0.09
0.76
-
0.71
0.42

-0.20
0.74
-
0.42
0.21

0.08
0.40
-
0.36
0.08

0.23
-
0.43
-
0.02

0.29
-
0.45
-
0.01

0.18
-
0.28
-
0.08

Table 3: Subjective evaluation (2): Compared across individual
attribute ratings. Models include: Conventional model (e.g.,
PESQ), our proposed models (SQ and LQ models), and DPLM
and BAMQ, as baseline. Spearman Correlation (SC). ↑ is better.

ratings, using SC. These scores are evaluated per condition
where we average scores for each condition. We choose four
distinct classes of available datasets for our analysis: i) (P1 and
P1') Bilateral Ambisonics [35]; ii) (P2) Spherical Microphone
Array [36]; iii) (P3) Headphone Equalization [37]; and iv)
(P4) Bitrate Compressed Ambisonics [38]. All datasets have
an overall rating of quality, but P3 additionally has individual
ratings for SQ and LQ. For more information on the datasets,
please refer to Manocha et al. [8].

Results are displayed in Tables 2 and 3. Next, we summarize

with a few observations:
• In Table 2 when compared across overall listening subjective
ratings, our OVRL metric scores higher correlations than the
SQ metric across all datasets suggesting that OVRL implicitly
learns useful features combining SQ and LQ attributes,
showing the usefulness of MTL.

• Our SQ metric obtains higher correlations than DPLM also
suggesting the usefulness of our MTL framework, as well as
our new EMD objective.

• Our LQ metric obtains higher correlations than PESQ which
suggests that it considers all channels to predict similarity,
rather than evaluating each channel individually like PESQ.
• Correlations across P3 are low but have improved slightly
compared to DPLM. This suggests that our model fails to
capture subtle differences, some of which are close to JNDs.
• Overall, our proposed metric SAQAM achieves the best
performance, and shows higher generalizability across a wide
variety of attributes, and perturbations.

4.3. Ablations

We perform ablation studies to better understand the inﬂuence
of different components of our metric. We show the advantange
of our MTL framework, and also show the inﬂuence of different
input representations to our model.
Multi-task learning In this ablation, we assess the signiﬁcance
of the multi-task formulation that predicts LQ and SQ together.
Our results show that training to predict both together results in
higher performance than training the two models individually;

Type

Name

P3 (SQ)

P3 (LQ)

Speech

Pink Noise Guitar

Speech

Pink Noise Guitar

Individual M.

Proposed

SQ
LQ
SQ
LQ

0.75
-
0.76
-

0.61
-
0.74
-

0.21
-
0.40
-

-
0.40
-
0.43

-
0.36
-
0.45

-
0.26
-
0.28

Table 4: Ablations: Models include: Individual trained models
for Spatialization and Listening Quality, as well as our Multi-
task framework models. Spearman Correlation (SC). ↑ is better.

LogMSE
Scratch SAQAM
Finetune SAQAM

83.6
85.88
86.50
Table 5: Evaluation of enhancement models using a held out
test set with objective measures.

0.013
0.011
0.008

9.15
10.31
10.9

1.65
1.75
1.83

0.18
0.17
0.13

often the improvement is of the order of 20 to 30% (see table 4).
Different input feature representations To evaluate how the
metric correlates with different types of input representations, we
train another set of models using either magnitude spectrum or
phase spectrum, and compare performance with SAQAM which
is based on both magnitude and phase spectrum features. We
observe that phase features are better correlated with subjective
ratings than magnitude features. We also observe that SAQAM
scores the highest correlations, suggesting that it makes use of
both sets of features.

4.4. Binaural speech enhancement

To further demonstrate the effectiveness of our metric, we design
a binaural speech enhancement model that is based on a popular
U-Net type convolutional neural network [39]. The input to the
model is the real and imaginary components of the STFT of the
binaural signal, and the outputs are the complex ratio masks.

The baseline enhancement model is trained using Log
Mean Square Error (LogMSE) between the estimated and target
real and imaginary STFT components for both channels. We
supplement SAQAM in two different ways: (i) Scratch SAQAM
- where we train using LogMSE and SAQAM from scratch; and
(ii) Finetune SAQAM - where we ﬁrst pretrain on LogMSE and
ﬁnetune on SAQAM.

For evaluation, we randomly select 2000 unseen audio
recordings from an unseen dataset [40, 41]. Following prior
work, we evaluate the quality of enhanced binaural recordings
using a variety of objective measures: i) PESQ (from 0.5 to 4.5);
(ii) Short Time Objective Intelligibility (STOI) (from 0 to 100);
(iii) L2 distance on the waveform; (iv) Scale invariant signal
to distortion ratio (Si-SDR); and v) Multi-resolution STFT at
various FFT, hop, and window sizes, all evaluated over each
channel separately, and then averaged to get one value.

As we see in Table 5, both of our models perform better
than the baseline. We observe that our Finetune SAQAM model
scores the best objective scores. This highlights the usefulness of
using SAQAM in audio similarity tasks, especially in identifying
and eliminating minor human perceptible artifacts that are not
captured by traditional losses.

5. Conclusions and future work

We present SAQAM, a general purpose, differentiable, non-
matching, non-clean-reference based objective metric to assess
listening quality and spatial localization differences between
any two binaural signals. We show the utility of MTL and
deep-feature distances that guide the model to correlate well
with human subjective ratings without any perceptual training or
calibration. In the future, we would like to include more spatial
quality attributes like perception of distance and coloration.

6. References
[1] S. Werner and F. Klein, “Inﬂuence of context dependent quality
parameters on the perception of externalization and direction
of an auditory event,” in AES Conference: 55th International
Conference: Spatial Audio. AES, 2014.

[2] A. Lindau, “Spatial audio quality inventory (SAQI). test manual.”

[3] S. K¨ampf, J. Liebetrau, S. Schneider et al., “Standardization of
PEAQ-MC: Extension of ITU-R bs. 1387-1 to multichannel audio,”
in AES Conference: 40th International Conference: Spatial Audio:
Sense the Sound of Space. AES, 2010.

[4] J.-H. Fleßner, R. Huber, and S. D. Ewert, “Assessment and
prediction of binaural aspects of audio quality,” Journal of the
AES, vol. 65, no. 11, pp. 929–942, 2017.

[5] J.-H. Seo, S. B. Chon, K.-M. Sung et al., “Perceptual objective
quality evaluation method for high quality multichannel audio
codecs,” Journal of the AES, vol. 61, no. 7/8, pp. 535–545.

[6] M. Takanen and G. Lorho, “A binaural auditory model for the
evaluation of reproduced stereophonic sound,” in AES Conference:
45th International Conference: Applications of Time-Frequency
Processing in Audio. AES, 2012.

[7] M. Narbutt, J. Skoglund, A. Allen et al., “AMBIQUAL: Towards
a quality metric for headphone rendered compressed ambisonic
spatial audio,” Applied Sciences, vol. 10, no. 9, 2020.

[8] P. Manocha, A. Kumar, B. Xu et al., “DPLM: A deep perceptual
IEEE, 2021, pp.

spatial-audio localization metric,” in WASPAA.
6–10.

[9] P. Manocha, B. Xu, and A. Kumar, “NORESQA: A framework
for speech quality assessment using non-matching references,”
NeurIPS, vol. 34, 2021.

[10] J. Wang, Y. Song, T. Leung et al., “Learning ﬁne-grained
image similarity with deep ranking,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2014, pp.
1386–1393.

[11] E. Hoffer and N. Ailon, “Deep metric learning using triplet
network,” in International workshop on similarity-based pattern
recognition. Springer, 2015, pp. 84–92.

[12] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1,

pp. 41–75, 1997.

[13] S.-W. Fu, Y. Tsao, and X. Lu, “SNR-Aware convolutional neural
network modeling for speech enhancement.” in Interspeech, 2016,
pp. 3768–3772.

[14] D. Chen and B. K.-W. Mak, “Multitask learning of deep neural
networks for low-resource speech recognition,” IEEE/ACM TASLP,
vol. 23, no. 7, pp. 1172–1183, 2015.

[15] C. Szegedy, W. Liu, Y. Jia et al., “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2015.

[16] T. Salimans and D. P. Kingma, “Weight normalization: A simple
reparameterization to accelerate training of deep neural networks,”
Advances in neural information processing systems, vol. 29, 2016.

[17] P. Manocha, A. Finkelstein, R. Zhang et al., “A differentiable
perceptual audio metric learned from just noticeable differences,”
Interspeech, 2020.

[18] J. S. Garofolo, L. F. Lamel, W. M. Fisher et al., “DARPA TIMIT
acoustic-phonetic continous speech corpus cd-rom. nist speech
disc 1-1.1,” NASA STI/Recon technical report n, vol. 93, p. 27403,
1993.

[19] F. Winter, H. Wierstorf, A. Podlubne et al., “Database of binaural
room impulse responses of an apartment-like environment,” in AES
Convention 140. AES, 2016.

[20] M. Jeub, M. Schafer, and P. Vary, “A binaural room impulse re-
sponse database for the evaluation of dereverberation algorithms,”
in 2009 16th International Conference on Digital Signal Process-
ing.

IEEE, 2009, pp. 1–5.

[21] L. Asp¨ock, F. Brinkmann, D. Ackermann et al., “BRAS-

Benchmark for room acoustical simulation,” 2020.

[22] B. I. Bacila and H. Lee, “360° binaural room impulse response
(BRIR) database for 6DOF spatial perception research,” in AES
Convention 146. AES, 2019.

[23] C. Mittag, M. B¨ohme,

“Dataset of
KEMAR-BRIRs measured at
several positions and head
orientations in a real room,” Dec. 2016. [Online]. Available:
https://doi.org/10.5281/zenodo.206860

and S. Werner,

[24] J. Francombe, “Iosr listening room multichannel BRIR dataset.”

[25] H. Kayser, S. D. Ewert, J. Anem¨uller et al., “Database of
multichannel in-ear and behind-the-ear head-related and binaural
room impulse responses,” EURASIP, vol. 2009, pp. 1–10, 2009.

[26] V. Erbes, M. Geier, S. Weinzierl et al., “Database of single-channel
and binaural room impulse responses of a 64-channel loudspeaker
array,” in AES Convention 138. AES, 2015.

[27] H. Wierstorf, M. Geier, and S. Spors, “A free database of head
related impulse response measurements in the horizontal plane
with multiple distances,” in AES Convention 130. AES, 2011.

[28] D. Satongar, Y. W. Lam, and C. Pike, “Measurement and analysis
of a spatially sampled binaural room impulse response dataset,” in
21st International Congress on Sound and Vibration, 2014.

[29] C. K. Reddy, E. Beyrami, H. Dubey et al., “The interspeech 2020
deep noise suppression challenge: Datasets, subjective speech
quality and testing framework,” arXiv arXiv:2001.08662, 2020.

[30] P. Foster, S. Sigtia, S. Krstulovic et al., “Chime: A dataset for
sound source recognition in a domestic environment,” in IEEE
WASPAA, 2015.

[31] A. Weisser, J. M. Buchholz, C. Oreinos et al., “The ambisonic
recordings of typical environments (ARTE) database,” Acta Acus-
tica United With Acustica, vol. 105, no. 4, 2019.

[32] B. Cornelis, S. Doclo, T. Van dan Bogaert et al., “Theoretical
analysis of binaural multimicrophone noise reduction techniques,”
IEEE TASLP, vol. 18, no. 2, pp. 342–355, 2009.

[33] B. Harwood, V. Kumar BG, G. Carneiro et al., “Smart mining for

deep metric learning,” in CVPR, 2017, pp. 2821–2829.

[34] A. W. Rix, J. G. Beerends, M. P. Hollier et al., “Perceptual
evaluation of speech quality (PESQ)-a new method for speech
quality assessment of telephone networks and codecs,” in 2001
IEEE ICASSP, vol. 2.

IEEE, 2001, pp. 749–752.

[35] Z. Ben-Hur, D. L. Alon, R. Mehra et al., “Binaural reproduction
based on bilateral ambisonics and ear-aligned hrtfs,” IEEE/ACM
TASLP, vol. 29, pp. 901–913, 2021.

[36] T. L¨ubeck, H. Helmholz, J. M. Arend et al., “Perceptual evaluation
of mitigation approaches of impairments due to spatial undersam-
pling in binaural rendering of spherical microphone array data,”
Journal of the AES, vol. 68, no. 6, pp. 428–440, 2020.

[37] I. Engel, D. L. Alon, P. W. Robinson et al., “The effect of
generic headphone compensation on binaural renderings,” in AES
Conference: 2019 AES International Conference on Immersive
and Interactive Audio. AES, 2019.

[38] T. Rudzki, I. Gomez-Lanzaco, P. Hening et al., “Perceptual evalua-
tion of bitrate compressed ambisonic scenes in loudspeaker based
reproduction,” in AES International Conference on Immersive and
Interactive Audio, 2019.

[39] K. Tan and D. Wang, “Learning complex spectral mapping
with gated convolutional recurrent networks for monaural speech
enhancement,” IEEE/ACM TASLP, vol. 28, pp. 380–390, 2019.

[40] V. Panayotov, G. Chen, D. Povey et al., “Librispeech: an asr corpus
IEEE, 2015,

based on public domain audio books,” in ICASSP.
pp. 5206–5210.

[41] V. R. Algazi, R. O. Duda, D. M. Thompson et al., “The CIPIC
hrtf database,” in Proceedings of the 2001 IEEE Workshop on the
Applications of Signal Processing to Audio and Acoustics (Cat. No.
01TH8575).

IEEE, 2001, pp. 99–102.

