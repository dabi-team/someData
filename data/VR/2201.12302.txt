2
2
0
2

n
a
J

8
2

]

C
O
.
h
t
a
m

[

1
v
2
0
3
2
1
.
1
0
2
2
:
v
i
X
r
a

Adaptive Accelerated (Extra-)Gradient Methods with Variance
Reduction

Zijian Liu∗

Ta Duy Nguyen†

Alina Ene‡

Huy L. Nguy˜ên§

Abstract

In this paper, we study the ﬁnite-sum convex optimization problem focusing on the general convex case.
Recently, the study of variance reduced (VR) methods and their accelerated variants has made exciting progress.
However, the step size used in the existing VR algorithms typically depends on the smoothness parameter, which
is often unknown and requires tuning in practice. To address this problem, we propose two novel adaptive VR
algorithms: Adaptive Variance Reduced Accelerated Extra-Gradient (AdaVRAE) and Adaptive Variance Reduced
Accelerated Gradient (AdaVRAG). Our algorithms do not require knowledge of the smoothness parameter.
(cid:19)

(cid:19)

(cid:18)

(cid:18)

AdaVRAE uses O

n log log n +

gradient evaluations and AdaVRAG uses O

n log log n +

(cid:113) nβ log β
(cid:15)

(cid:113) nβ
(cid:15)

gradient evaluations to attain an O((cid:15))-suboptimal solution, where n is the number of functions in the ﬁnite sum
and β is the smoothness parameter. This result matches the best-known convergence rate of non-adaptive VR
methods and it improves upon the convergence of the state of the art adaptive VR method, AdaSVRG. We
demonstrate the superior performance of our algorithms compared with previous methods in experiments on
real-world datasets.

1

Introduction

In this paper, we consider the ﬁnite-sum optimization problem in the form of

(cid:40)

1
n

n
(cid:88)

i=1

min
x∈X

(cid:41)

fi(x) + h(x)

(1)

where each function fi is convex and β-smooth, h is convex and potentially nonsmooth but admitting an eﬃcient
proximal operator, and X ⊆ Rd is a closed convex set. Additionally, we further assume that X is compact when
β is unknown. Problem (1) has found a wide range of applications in machine learning, typically in empirical risk
minimization problems, and has been extensively studied in the past few years.

Among existing approaches to solve this problem, variance reduced (VR) methods [18, 10, 37, 36] have recently
shown signiﬁcant improvement over the classic stochastic gradient methods such as stochastic gradient descent
(SGD) and its variants. For example, in strongly convex problems, VR methods such as [1, 22, 29] can achieve the
(cid:1) to attain an O((cid:15))-suboptimal solution, where κ is
optimal number of gradient evaluations of O (cid:0)(n +
nκ) log 1
(cid:15)
the condition number, which improves over full-batch gradient descent (O (cid:0)nκ log 1
(cid:1)) and Nesterov’s accelerated
(cid:1)). For general convex problems, the current state-of-the-art VR methods,
gradient descent [33, 34] (O (cid:0)n

√

√

(cid:15)

κ log 1
(cid:15)

(cid:18)

n log log n +

(cid:113) nβ
(cid:15)

(cid:19)

gradient evaluations, which

namely VRADA [39] can ﬁnd an O((cid:15))-suboptimal solution using O

nearly-matches the lower bound of Ω

(cid:18)

n +

(cid:113) nβ
(cid:15)

(cid:19)

[42].

However, most of existing VR gradient methods have the same limitation as classic gradient methods; that is,
they require the prior knowledge of the smoothness parameter in order to set the step size. Lacking this information,
one may have to carefully perform hyper-parameter tuning to avoid the situation that the algorithm divergences
or converges too slowly due to too large or too small step size. This limitation of gradient methods motivates the
development of methods that aim to adapt to unknown problem structures. A notable line of work starting with the

∗Equal contribution. Department of Computer Science, Boston University, zl3067@bu.edu.
†Equal contribution. Department of Computer Science, Boston University, taduy@bu.edu.
‡Department of Computer Science, Boston University, aene@bu.edu
§Khoury College of Computer and Information Science, Northeastern University, hu.nguyen@northeastern.edu.

1

 
 
 
 
 
 
Table 1: Our results and comparison with prior works.

Algorithm

SVRG [18]

SVRG++ [4]

Katyusha [1]

VARAG [22]

VRADA [39]

AdaSVRG [12]

General convex

Adaptive

-
O (cid:0)n log β
(cid:18)

O

n log β

(cid:15) +

(cid:1)

(cid:15) + β

(cid:15)
(cid:113) nβ
(cid:15)

(cid:19)

(cid:18)

O

n min (cid:8)log β

(cid:15) , log n(cid:9) +

(cid:19)

(cid:113) nβ
(cid:15)

(cid:18)

O

n min (cid:8)log log β

(cid:15) , log log n(cid:9) +

(cid:19)

(cid:113) nβ
(cid:15)

O (cid:0) nβ

(cid:15)

(cid:1) (ﬁxed sized inner loop, only if (cid:15) = Ω( β

n ))

O (cid:0)n log β

(cid:15) + β

(cid:15)

(cid:1) (multi-stage)

AdaVRAE (unknown β) (This Paper)

AdaVRAE (known β) (This Paper)

AdaVRAG (unknown β) (This Paper) O

(cid:18)

(cid:18)

O

O

n min (cid:8)log log β

(cid:15) , log log n(cid:9) +

n min (cid:8)log log β

(cid:15) , log log n(cid:9) +

(cid:18)

n min (cid:8)log log β log β

(cid:15)

, log log n(cid:9) +

AdaVRAG (known β) (This Paper)

(cid:18)

O

n min (cid:8)log log β

(cid:19)

(cid:19)

(cid:113) nβ
(cid:15)

(cid:113) nβ
(cid:15)

(cid:19)

(cid:113) nβ log β
(cid:15)

(cid:19)

(cid:113) nβ
(cid:15)

Lower Bound [42]

Ω

n +

(cid:15) , log log n(cid:9) +
(cid:19)

(cid:113) nβ
(cid:15)

(cid:18)

No

No

No

No

No

Yes

Yes

No

Yes

No

-

inﬂuential AdaGrad algorithm has designed a family of gradient descent based methods that set the step size based
on the gradients or iterates observed in previous iterations [31, 13, 21, 25, 26, 6, 8, 20, 19, 15, 5, 14]. Remarkably,
these works have shown that, in the setting where we have access to the exact full gradient in each iteration, it is
possible to match the convergence rates of both unaccelerated and accelerated gradient descent methods without
any prior knowledge of the smoothness parameter. These methods have also been analyzed in the stochastic setting
under a bounded variance assumption, and they achieve a convergence rate that is comparable to that of SGD.

Given the theoretical and practical success of adaptive methods, it is natural to ask whether one can design
VR methods that achieve state of the art convergence guarantees without any prior knowledge of the smoothness
parameter. The recent work of [12] gives the ﬁrst adaptive VR method — AdaSVRG — with the gradient complexity
n log β
of O
accelerated.

. AdaSVRG builds on the AdaGrad [13] and SVRG algorithms [18], both of which are not

(cid:15) + β

(cid:16)

(cid:17)

(cid:15)

Our contributions: In this work, we take this line of work further and design the ﬁrst accelerated VR methods
that do not require any prior knowledge of the smoothness parameter. Our algorithms, Adaptive Variance Reduced
Accelerated Extra-Gradient (AdaVRAE) and Adaptive Variance Reduced Accelerated Gradient (AdaVRAG), only

(cid:19)

(cid:113) nβ
(cid:15)

(cid:18)

(cid:19)

(cid:113) nβ log β
(cid:15)

and O

n log log n +

n log log n +

use O
gradient evaluations respectively to attain an O((cid:15))-
suboptimal solution when β is unknown, both of which signiﬁcantly improve the convergence rate of AdaSVRG.
Table 1 compares our algorithms and prior VR methods and Section 2 discusses our algorithmic approaches and
techniques. The convergence rate of AdaVRAE matches up to constant factors the best-known convergence rate of
non-adaptive VR methods [39, 19]. Both of our algorithms follow a diﬀerent approach from these methods that is
based on extra-gradient and mirror descent, instead of dual averaging.

We demonstrate the eﬃciency of our algorithms in practice on multiple real-world datasets. We show that
AdaVRAG and AdaVRAE are competitive with existing standard and adaptive VR methods while having the
advantage of not requiring hyperparameter tuning, and in many cases AdaVRAG outperforms these benchmarks.

2

(cid:18)

1.1 Related work

Variance reduced gradient methods: Variance reduction technique [36, 37, 38, 30, 18, 10] has been proposed
to improve the convergence rate of stochastic gradient descent algorithms in the ﬁnite sum problem and has since
become widely-used in many successful algorithms. Notable improvements can be seen in strongly convex optimiza-
tion problems where earliest algorithms such as SVRG [18] or SAGA [10] obtain O (cid:0)(n + κ) log 1
(cid:1) convergence rate
of plain SGD, with the latter requiring an additional assumption on the σ2-boundedness of
compared with O

(cid:17)

(cid:15)

the variance term, i.e., Ei
≤ σ2. However, these non-accelerated methods do not achieve the
optimal convergence rate. Recent works such as [29, 1, 22] focus on designing accelerated methods and successfully
match the optimal lower bound for strongly convex optimization of Ω (cid:0)(n +

(cid:1) given by [23].

√

(cid:107)∇fi(x) − ∇f (x)(cid:107)2(cid:105)
(cid:104)

nκ) log 1
(cid:15)

(cid:16) σ2κ
β(cid:15)

In non-strongly convex problems, however, existing works do not yet match the lower bound of Ω

(cid:18)

n +

(cid:113) βn
(cid:15)

(cid:19)

shown in [42]. The best eﬀort so far can be found in the line of accelerated methods started by [1] and followed
by [2, 22, 28] that rely on incorporating the checkpoint in each update. AdaVRAG follows the same idea but
oﬀers simpler update and more eﬃcient choice of coeﬃcients that results in a better convergence rate, equivalent
to VRADA [39]. By comparison, while VRADA is a dual-averaging scheme, AdaVRAG is a mirror descent method
and AdaVRAE is an extra-gradient algorithm.

In a diﬀerent line of research [3, 16, 43], variance reduction has been applied to non-convex optimization to ﬁnd

critical points with much better convergence rate.

Adaptive methods with variance reduction: There has been extensive research on adaptive methods
[13, 21, 35, 41, 11] in the setting where we compute a full gradient in each iteration. However, there are only few
works combining adaptive methods with VR techniques in the ﬁnite sum setup. Most relevant for our work is
AdaSVRG [12]. This algorithm is built upon SVRG which as mentioned earlier is a non-accelerated method and
has a slower convergence rate. AdaSVRG uses the gradient norm to update the step size, similar to [13] and the
step is reset in every epoch, which could lead to step sizes that are too large in later stages. In contrast, both
AdaVRAG and AdaVRAE are accelerated VR methods and use a cumulative step size. AdaVRAG uses the iterate
log β factor by
movement to update the step size, as in [6, 15]. AdaVRAE improves the convergence rate by a
using the gradient diﬀerence similarly to [32, 19, 14].

√

A diﬀerent line of work considers VR methods that set the step size using stochastic line search [37, 30] or
Barzilai-Borwein step size [40, 27]. The former methods do not have theoretical guarantees, and the latter methods
require knowledge of the smoothness parameter in order to obtain theoretical bounds.

Recent works design variance-reduced methods for non-convex optimization. STORM [9] and STORM+[24]
design an adaptive step size, though the former still requires the smoothness parameter in the step size. Super-Adam
[17] also requires their parameters to satisfy some inequality involving the smoothness parameter like STORM.

1.2 Notation and problem setup
Let [n] denote the set {1, 2, · · · , n}. For simplicity, we only consider the Euclidean norm (cid:107)·(cid:107) := (cid:107)·(cid:107)2
be extended to (cid:107)x(cid:107)A :=
x(cid:62)Ax for any A (cid:31) 0 with almost no change). x+ represents max {x, 0}.

√

We are interested in solving the following problem

(Our work can

min
x∈X

{F (x) = f (x) + h(x)}

(cid:80)n

i=1 fi(x) and for i ∈ [n], fi : Rd → R and h : X → R are convex functions with a closed convex
where f (x) := 1
n
set X ⊆ Rd. Let x∗ = arg minx∈X F (x). We say a function G is β-smooth if (cid:107)∇G(x) − ∇G(y)(cid:107) ≤ β (cid:107)x − y(cid:107) for all
2 (cid:107)y − x(cid:107)2. In this paper we always assume that
x, y ∈ Rd. Equivalently, we have G(y) ≤ G(x) + (cid:104)∇G(x), y − x(cid:105) + β
each fi is β-smooth, which implies that f is also β-smooth. We assume that we can eﬃciently solve optimization
2 (cid:107)x − v(cid:107)2(cid:17)
where γ ≥ 0 and v ∈ Rd. When the smoothness parameter
problems of the form arg minx∈X
β is unknown, we additionally assume that X is compact with diameter D, i.e., supx,y∈X (cid:107)x − y(cid:107) ≤ D.

γh(x) + 1

(cid:16)

2 Our algorithms and convergence guarantees

In this section, we describe our algorithms and state their convergence guarantees. Our algorithm AdaVRAE
shown in Algorithm 1 is a novel accelerated scheme that uses past extra-gradient update steps in the inner loop
and novel averaging to achieve acceleration. AdaVRAE adaptively sets the step sizes based on the stochastic

3

Algorithm 1 AdaVRAE
Input: initial point u(0), domain diameter D.
Parameters: {a(s)},{Ts}, A(0)
T0
x(1)
0 = z(1)
Initialize γ(1)
for s = 1 to S:
0 = A(s−1)
A(s)
Ts−1
for t = 1 to Ts:

0 = u(0), compute ∇f (u(0))

0 = γ, where γ is any small constant

> 0, η > 0.

(cid:0)a(s)(cid:1)2

− Ts

(cid:26)

a(s) (cid:68)
g(s)
x(s)
t−1, x
t = arg minx∈X
t−1 + a(s) + (cid:0)a(s)(cid:1)2
t = A(s)
(cid:16)
A(s)
t−1 + a(s)x(s)
t−1x(s)

(cid:69)

t + (cid:0)a(s)(cid:1)2

u(s−1)(cid:17)

+ a(s)h(x) +

γ(s)
t−1
2

(cid:13)
(cid:13)x − z(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

t ∼ Uniform ([n])
(x(s)

t ) − ∇fi(s)

t

(u(s−1)) + ∇f (u(s−1))

Let A(s)
x(s)
t = 1
A(s)
t
if t (cid:54)= Ts:
Pick i(s)
g(s)
t = ∇fi(s)
else:
t = ∇f (x(s)
g(s)
t )
(cid:114)
(cid:16)
γ(s)
t−1

γ(s)
t = 1
η

η2

t

z(s)
t = arg minz∈X
= x(s)
Ts

u(s) = x(s+1)

0
return u(S)

(cid:26)

(cid:17)2

(cid:13)
2
(cid:13)
(cid:13)

t − g(s)

+ (cid:0)a(s)(cid:1)2 (cid:13)
(cid:13)g(s)
(cid:13)
(cid:69)
a(s) (cid:68)
+ a(s)h(z) +
, z(s+1)
0

g(s)
t
= z(s)
Ts

, g(s+1)
0

= g(s)
Ts

t−1

, z

γ(s)
t−1
2
, γ(s+1)
0

(cid:13)
(cid:13)z − z(s)
(cid:13)
t−1
= γ(s)
Ts

(cid:13)
2
(cid:13)
(cid:13)

+

t −γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)z − x(s)
(cid:13)

t

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

Algorithm 2 AdaVRAG
Input: initial point u(0), domain diameter D.
Parameters: {a(s)}, a(s) ∈ (0, 1), {q(s)}, {Ts}, η > 0.
x(1)
0 = u(0)
Initialize γ(1)
for s = 1 to S:
0 = a(s)x(s)
x(s)
for t = 1 to Ts:

0 = γ, where γ is any small constant

0 + (1 − a(s))u(s−1), compute ∇f (u(s−1))

t−1) − ∇fi(s)
(cid:26)(cid:68)

t

(u(s−1)) + ∇f (u(s−1))
(cid:69)

g(s)
t

, x

+ h(x) +

γ(s)
t−1q(s)
2

(cid:13)
(cid:13)x − x(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

t ∼ Uniform ([n])
(x(s)

Pick i(s)
g(s)
t = ∇fi(s)
x(s)
t = arg minx∈X
t = a(s)x(s)
x(s)

t

Option I: γ(s)

t = γ(s)

t−1

Option II: γ(s)

t = γ(s)

t + (1 − a(s))u(s−1)
(cid:114)
(cid:13)
(cid:13)x(s)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

t−1

1 +
(cid:13)
(cid:13)x(s)
(cid:13)

t −x(s)
η2
t −x(s)
η2
= x(s)
Ts

(cid:13)
(cid:13)
(cid:13)

t−1

2

, γ(s+1)
0

t−1 +
, x(s+1)
0

u(s) = 1
Ts
return u(S)

(cid:80)Ts

t=1 x(s)

t

= γ(s)
Ts

4

gradient diﬀerence. Our choice of step sizes is a novel adaptation to the VR setting of the step sizes used by the
works [32, 20, 19, 14] in the batch/full-gradient setting. Our algorithm builds on the work [14], which provides an
unaccelerated past extra-gradient algorithm in the batch/full-gradient setting.

Theorem 2.1 states the parameter choices and the convergence guarantee for AdaVRAE, and we give its proof
in Section A in the appendix. The convergence rate of AdaVRAE matches up to constant factors the rate of the
state of the art non-adaptive VR methods [19, 39]. The initial step size γ(1)
can be set to any small constant γ,
which in practice we choose γ = 0.01. Similarly to AdaGrad, setting η = Θ(D) gives us the optimal dependence
of the convergence rate in the domain diameter. For simplicity, we state the convergence in Theorem 2.1 and 2.2
when η = Θ(D). We refer the reader to Theorems A.1 and B.1 in the appendix for the precise choice of parameters
as well as the full dependence of the convergence rate on arbitrary choices of γ and η. In both Theorem 2.1 and
2.2, we measure convergence using the number of individual gradient evaluations ∇fi, assuming that the exact
computation of ∇f takes n gradient evaluations.

0

Theorem 2.1. (Convergence of AdaVRAE) Deﬁne s0 = (cid:100)log2 log2 4n(cid:101), c = 3
Algorithm 1 as follows:

2 . Suppose we set the parameters of

a(s) =

(cid:40)

(4n)−0.5s
s−s0−1+c
2c

1 ≤ s ≤ s0
s0 < s

,

Ts = n,
5
4

=

.

A(0)
T0

Suppose that X is a compact convex set with diameter D and we set η = Θ(D). The number of individual gradient
evaluations to achieve a solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for Algorithm 1 is

#grads =




O(cid:0)n log log V1

(cid:1)

(cid:15)

(cid:18)

O



n log log n +

(cid:19)

(cid:113) nV1
(cid:15)

if (cid:15) ≥ V1
n
if (cid:15) < V1
n

where V1 = O (cid:0)F (u(0)) − F (x∗) + (γ + β) D2(cid:1).

Our algorithm AdaVRAG is shown in Algorithm 2. Compared with AdaVRAE, AdaVRAG has a worse de-
pendence on the smoothness parameter β but it performs only one projection onto X in each inner iteration.
Additionally, as we discuss in more detail below, it uses adaptive step sizes based on the iterate movement.

AdaVRAG follows a similar framework to existing VR methods such as VARAG [22] and VRADA [39]. Similarly
to VRADA, the algorithm achieves acceleration at the epoch level, where an epoch is an iteration of the outer loop.
The iterations in an epoch update the main iterates via mirror descent with novel choices of step sizes and coeﬃcients.
The stochastic gradient is computed at a point that is a convex combination between the current iterate and the
checkpoint; the coeﬃcients of this combination remain ﬁxed throughout the epoch. The step sizes are adaptively
set based on the iterate movement.

The structure of the inner iterations of our algorithm diﬀers from both VARAG and VRADA in several notable
aspects. VARAG also uses mirror descent to update the main iterates and it computes the stochastic gradient at
suitable combinations of the iterates and the checkpoint. AdaVARAG uses a diﬀerent averaging of the iterates to
compute the snapshots. Moreover, it uses a very diﬀerent and simpler choice for the coeﬃcient used to combine
the main iterates and the checkpoint in order to obtain the points at which the stochastic gradients are evaluated.
In VARAG, this coeﬃcient is set to a constant (namely, 1/2) in the initial iterations, whereas in AdaVRAG, it
starts from a small number and is increased gradually. This choice is critical for improving the ﬁrst term in the
convergence from O(n log n) to O(n log log n). In a similar manner, VRADA attains the same convergence by a
new choice of coeﬃcient. However, this is achieved via a very diﬀerent approach based on dual-averaging.

The step sizes used by AdaVRAG have two components: the step γ(s)

that is updated based on the iterate
movement and the per-epoch coeﬃcient q(s) to achieve acceleration at the epoch level. Our analysis is ﬂexible and
allows the use of several approaches for updating the steps γ(s)
. One approach, shown as option I in Algorithm 2, is
based on the multiplicative update rule of AdaGrad+ [15] which generalizes the AdaGrad update to the constrained
setting. We also propose a diﬀerent variant, shown as option II, that updates the steps in an additive manner. Our
analysis shows a similar convergence guarantee for both options, with the main diﬀerence being in the dependence
β log β, whereas option II has a worse dependence of β.
on the smoothness: option I incurs a dependence of
Option II achieved improved performance in our experiments.

√

t

t

5

Theorem 2.2 states the parameter choices and the convergence guarantee for AdaVRAG, and we give its proof
in Section B in the appendix. Analogously to AdaVRAE, the initial step size γ can be set to any small constant.

Theorem 2.2. (Convergence of AdaVRAG) Deﬁne s0 = (cid:100)log2 log2 4n(cid:101), c = 3+
of Algorithm 2 as follows:

√

4

33

. Suppose we set the parameters

a(s) =

q(s) =

(cid:40)

1 − (4n)−0.5s

c
s−s0+2c

1
(1−a(s))a(s)
8(2−a(s))a(s)
3(1−a(s))






1 ≤ s ≤ s0
s0 < s

,

1 ≤ s ≤ s0

,

s0 < s

Ts = n.

Suppose that X is a compact convex set with diameter D and we set η = Θ(D). Additionally, we assume that
2η2 > D2 if Option I is used for setting the step size. The number of individual gradient evaluations to achieve a
solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for Algorithm 2 is

#grads =




O (cid:0)n log log V2

(cid:1)

(cid:15)

(cid:18)

O



n log log n +

(cid:19)

(cid:113) nV2
(cid:15)

(cid:15) ≥ V2
n
(cid:15) < V2
n

,

where

(cid:40)

V2 =

(cid:16)

F (u(0)) − F (x∗) +

O
O (cid:0)F (u(0)) − F (x∗) + (cid:0)γ + β2(cid:1) D2(cid:1)

γ + β log

(cid:16) β
γ

(cid:17)(cid:17)

D2(cid:17)

for Option I

for Option II

.

(cid:16)

Comparison to AdaSVRG: As noted in the introduction, the state of the art adaptive VR method is the
AdaSVRG algorithm [12], which is a non-accelerated method. Both of our algorithms achieve a faster convergence
using diﬀerent approaches and step sizes. AdaSVRG resets the step sizes in each epoch, whereas our algorithms
use a cumulative update approach for the step sizes. In our experimental evaluation, the resetting of the step sizes
led to slower convergence. AdaSVRG (multi-stage variant) uses varying epoch lengths similarly to SVRG++ [4],
whereas our algorithms use epoch lengths that are set to n. Using an epoch of length n allows for implementing
the random sampling via a random permutation of [n] and is the preferred approach in practice.

Both our algorithms and AdaSVRG require that the domain X has bounded diameter. This is a restriction that is
shared by almost all existing adaptive methods. Recent work [5, 14] in the batch/full-gradient setting have proposed
unaccelerated methods that are suitable for unbounded domains, at a loss of additional factors in the convergence.
All of the existing accelerated methods require that the domain is bounded, even in the batch/full-gradient setting.
We note that our analysis holds for arbitrary compact domains, whereas the analysis of AdaSVRG only applies to
domains that contain the global optimum. Similarly to AdaGrad, both our algorithms and AdaSVRG can be used
in the unconstrained setting under the promise that the iterates do not move too far from the optimum.

Non-adaptive variants of our algorithms: In the setting where the smoothness parameter is known, we
can set the step sizes of our algorithms based on the smoothness, as shown in Algorithms 3 and 4 (Sections C
and D in the appendix). Both algorithms match the convergence rates of the state of the art VR methods [19, 39]
using diﬀerent algorithmic approaches based on mirror descent and extra-gradient instead of dual-averaging. We
experimentally compare the non-adaptive algorithms to existing methods in Section E of the appendix.

2.1 Analysis outline

We outline some of the key steps in the analysis of AdaVRAE. For the purpose of simplicity, we assume h = 0 and
η = D. By building on the standard analysis of the stochastic regret for extra-gradient methods, we obtain the
following result for the progress of one iteration:

(cid:20)(cid:18)

E

≤ E

A(s)

t −

(cid:34)

γ(s)
t−1
2

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

f (x(s)

(cid:17)
t ) − f (x∗)

(cid:16)

f (x(s)

t−1) − f (x∗)

(cid:17)(cid:21)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

− A(s)
t−1
2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

6

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:69)(cid:21)

∇f (x(s)

t

t ), u(s−1) − x(s)
2(cid:35)
(cid:13)
(cid:13)
(cid:13)

t−1

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

(cid:34)

+ E

t−1

t − γ(s)
γ(s)
2
a(s)(cid:17)2 (cid:68)

(cid:20)(cid:16)

+ E

+ E

− E

(cid:124)

(cid:34) (cid:0)a(s)(cid:1)2
2γ(s)
t
A(s)
t−1
2β

(cid:34)

t ) − ∇f (x(s)

t−1)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
(cid:123)(cid:122)
gain

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

(cid:125)

(2)

In comparison to the standard analysis, the coeﬃcient for the checkpoint appears in the coeﬃcient of f (x(s)
which becomes

t )−f (x∗),
, making the sum not telescope immediately. To resolve this,

instead of the usual A(s)

t − (cid:0)a(s)(cid:1)2(cid:17)
A(s)

(cid:16)

t

we ﬁrst turn our attention to the analysis of the stochastic gradient diﬀerence

(cid:13)
(cid:13)g(s)
(cid:13)
t − g(s)

t − g(s)
(cid:13)
2
(cid:13)
(cid:13)

t−1

(cid:13)
2
(cid:13)
(cid:13)

. The key idea is to split

(cid:18)

(cid:19)

t−1

into

(cid:13)
2
(cid:13)
(cid:13)

1
2γ(s)
t

− 1
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

(a(s))2
t−1
2γ(s)
t
For the ﬁrst term, we build on the techniques from prior work in the batch/full-gradient setting [14]. For the second
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
. The gradient diﬀerence loss term is cancelled by the gain term in (2), and thus we

term, we use Young’s inequality to write E

, and bound each term in turn.

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

(cid:0)a(s)(cid:1)2 (cid:13)
(cid:13)g(s)
(cid:13)

t−1) − g(s)

(cid:20)(cid:13)
(cid:13)g(s)
(cid:13)

t ) − g(s)

t − g(s)

t − g(s)

(a(s))2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

≤ E

(cid:20)
4

+ 4

(cid:13)
(cid:13)
(cid:13)

2(cid:21)

2(cid:21)

2(cid:21)

t−1

t−1

t−1

+

+

t

(cid:20)
2

E

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

(cid:13)
(cid:13)
(cid:13)

can focus on the ﬁrst two variance terms. We apply the usual variance reduction technique put forward by [22] (see
Lemma A.2) to bound the two variance terms, as follows:

E

(cid:20)(cid:13)
(cid:13)g(s)
(cid:13)

t − ∇f (x(s)
t )

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤ E

(cid:16)

(cid:104)

2β

f (u(s−1)) − f (x(s)

t ) −

(cid:68)
∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)(cid:17)(cid:105)

.

Thus we obtain an upper bound on (a(s))2
. This is the reason
for setting the coeﬃcient for the checkpoint to (cid:0)a(s)(cid:1)2, so that the LHS of (2) can become the usual telescoping
sum A(s)
. Using the convexity of f , we obtain the following key result
for the progress of each epoch:

in terms of (cid:0)a(s)(cid:1)2 (cid:16)

f (u(s−1)) − f (x(s)
t )

(cid:17)
t−1) − f (x∗)

(cid:17)
t ) − f (x∗)

t − g(s)

− A(s)
t−1

f (x(s)

f (x(s)

(cid:13)
(cid:13)g(s)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

t−1

16β

(cid:16)

(cid:17)

(cid:16)

t

(cid:104)

E

A(s)
Ts
(cid:34)

≤ E

+ E

+ E

+ E

γ(s)
0
2
(cid:34) Ts(cid:88)

t=1
(cid:16)

(cid:20)
Ts
(cid:34) Ts(cid:88)

t=1

(cid:17)(cid:105)

f (x(s)

0 ) − f (x∗)
2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:16)

f (x(s)
Ts

) − f (x∗)

(cid:17)

(cid:13)
(cid:13)z(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:16)

−

− A(s)
0
γ(s)
Ts
2

(cid:13)
(cid:13)z(s)
(cid:13)
2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

Ts

(cid:13)
(cid:13)x(s)
(cid:13)

t−1

t − γ(s)
γ(s)
2
a(s)(cid:17)2 (cid:16)

f (u(s−1)) − f (x∗)

(cid:17)(cid:21)

(cid:32)

1
2γ(s)
t

−

1
16β

(cid:33)

(cid:16)

a(s)(cid:17)2 (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

Intuitively, we want to have another telescoping sum when summing up the above inequality across all epochs
s. To do so, we can set the starting points of the next epoch to be the ending points of the previous one, i.e.,
(cid:0)a(s)(cid:1)2 (cid:0)f (u(s−1)) − f (x∗)(cid:1) appears on
, z(s)
x(s)
. However, an extra term Ts
Ts
Ts
(cid:0)a(s)(cid:1)2 so that we can
the RHS. We need to reset the new starting coeﬃcient in the new epoch A(s)
0
telescope the LHS.

= u(s), γ(s)
Ts

to A(s−1)
Ts−1

= γ(s+1)
0

= x(s+1)
0

= z(s+1)
0

− Ts

To bound the term (cid:80)S

s=1

(cid:80)Ts

t=1

t −γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

and g(s)
Ts

= g(s+1)
0

, we can consider the doubly indexed sequences

(cid:18)

− 1
1
16β
2γ(s)
t
(cid:17)
(cid:16)

γ(s)
t

(cid:19)

(cid:0)a(s)(cid:1)2 (cid:13)
(cid:13)g(s)
(cid:13)
(cid:17)
(cid:16)

and

g(s)
t

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

, since γ(s)
Ts

= γ(s+1)
0

as two singly indexed sequences

7

(a) Logistic loss

(b) Squared loss

Figure 1: a1a

(c) Huber loss

(a) Logistic loss

(b) Squared loss

(c) Huber loss

Figure 2: mushrooms

(γk) and (gk) and the coeﬃcient a(s) to be another sequence (ak). Then we can employ the following two inequalities:

D2
2

(γK − γ0) −

1
48β

K
(cid:88)

k (cid:107)gk − gk−1(cid:107)2 ≤ 12βD2
a2

K
(cid:88)

k=1

(cid:18) 1
2γk

−

k=1
(cid:19)

1
24β

k (cid:107)gk − gk−1(cid:107)2 ≤ 12βD2
a2

0 = A(s−1)
Ts−1

Finally, we need to choose the parameters a(s) so that the conditions needed for our analysis are satisﬁed and A(s)
Ts
is suﬃciently large, so that we attain a fast convergence. We have to choose a(s) such that (cid:0)a(s)(cid:1)2
for all
s, t ≥ 1 and that A(s)
≥ 0. The main idea is to divide the epochs into two phases: in the ﬁrst
phase, A(s)
= Ω(n2). The
Ts
nearly-optimal choice of a(s) in the ﬁrst phase is (4n)−0.5s , stopping at s = s0 = (cid:100)log2 log2 4n(cid:101), while in the second
phase, we have to be more conservative and choose a(s) = s−s0+ 1
. With this we can obtain the convergence rate
(cid:111)
of O

quickly rises to Ω(n) and in the second phase, to achieve the optimal

rate, A(s)
Ts

≤ 4A(s)
t−1

(cid:113) nβ
(cid:15)

log log β

(cid:0)a(s)(cid:1)2

n min

− Ts

+

(cid:19)

(cid:18)

(cid:110)

3

.

2

(cid:15) , log log n

(cid:113) nβ
(cid:15)

3 Experiments

In this section we demonstrate the performances of AdaVRAG and AdaVRAE in comparison with the existing
standard and adaptive VR methods. We use the experimental setup and the code base of [12]1.

Datasets and loss functions: We experiment with binary classiﬁcation on four standard LIBSVM datasets:
a1a, mushrooms, w8a and phishing [7]. For each dataset, we show the results for three diﬀerent objective functions:
logistic, squared and huber loss. Following the setting in [12] we add a (cid:96)2-regularization term to the loss function,
with regularization set to 1/n.

1Their code can be found at https://github.com/bpauld/AdaSVRG

8

(a) Logistic loss

(b) Squared loss

Figure 3: w8a

(c) Huber loss

(a) Logistic loss

(b) Squared loss

(c) Huber loss

Figure 4: phishing

Constraint: In all experiments, we evaluate the algorithms under a ball constraint. That is, the domain of each
problem in our experiment is a ball of radius R = 100 around the initial point, which means for every algorithm, in
the update step, we need to do a projection onto this ball.

Algorithms and hyperparameter selection: We compare AdaVRAE and AdaVRAG with the common
VR algorithms: SVRG [18], SVRG++[4], VARAG [22], VRADA [39], and AdaSVRG [12] (in the experiment the
multi-stage variant performs worse than the ﬁxed-sized inner loop variant, and we omit it from the plots). Among
these, only AdaSVRG is an adaptive VR method, which does not require parameter tuning. For the non-adaptive
methods we chose the step size (or equivalently, the inverse of the smoothness parameter (1/β) for VRADA) via
hyperparameter search over {0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100}. For each experiment, we used the choice that led
to the best performance, and we report the parameters used in Table 2. The adaptive methods — AdaSVRG,
AdaVRAE, AdaVRAG — do not require any hyperparameter tuning and we set their parameters as prescribed by
the theoretical analysis. For AdaSVRG, we used η = D/
2R as recommended in the original paper. For
2 =
AdaVRAE and AdaVRAG, we used γ = 0.01 and η = D/2 = R.

√

√

Implementation and initialization: For all algorithms, in the inner loop, we use a random permutation to
select a function. We also ﬁx the batch size to 1 in all cases to match the theoretical setting. We initialize u(0) to
be a random point in [0, 10]d where each dimension is uniformly chosen in [0, 10]. Each experiment is repeated ﬁve
times with diﬀerent initial point, which is kept the same across all algorithms.

Results: The results are shown in Figures 1, 2, 3, 4. For each experiment, we plot the mean value and 95%
conﬁdence interval of the training objective against the number of gradient evaluations normalized by the number
of examples.

Discussion: We observe that, in all experiments, AdaVRAG consistently performs competitively with all
methods and generally have the best performances. The non-accelerated methods in general converge more slowly
compared with accelerated methods, especially in the later epochs. In some cases, VARAG suﬀers from a slow
convergence rate in the ﬁrst phase. This is possibly due to the fact that it sets to 1/2 the coeﬃcient for the
checkpoint in the ﬁrst phase. VRADA sometimes exhibits similar behavior but to a lesser extent. In AdaVRAG
and AdaVRAE, the coeﬃcient for the checkpoint is set to be small in the beginning and gradually increased over time
when the quality of the checkpoint is improved. The other adaptive method, AdaSVRG, exhibits slow convergence
in many cases. One reason might be that AdaSVRG resets the step size in every epoch and, in later epochs, the

9

step size may be too large for the algorithm to converge. In contrast, AdaVRAG and AdaVRAE use cumulative
step sizes.

References

[1] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The Journal of

Machine Learning Research, 18(1):8194–8244, 2017.

[2] Zeyuan Allen-Zhu. Katyusha x: Practical momentum method for stochastic sum-of-nonconvex optimization.

arXiv preprint arXiv:1802.03866, 2018.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International

conference on machine learning, pages 699–707. PMLR, 2016.

[4] Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In

International conference on machine learning, pages 1080–1089. PMLR, 2016.

[5] Kimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos. Adaptive extra-gradient methods
for min-max optimization and games. In International Conference on Learning Representations (ICLR), 2021.

[6] Francis Bach and Kﬁr Y Levy. A universal algorithm for variational inequalities adaptive to smoothness and

noise. In Conference on Learning Theory, pages 164–194. PMLR, 2019.

[7] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions on

intelligent systems and technology (TIST), 2(3):1–27, 2011.

[8] Ashok Cutkosky. Anytime online-to-batch, optimism and acceleration. In International Conference of Machine
Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pages 1446–1454. PMLR, 2019.

[9] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. Advances

in neural information processing systems, 32, 2019.

[10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information processing systems,
pages 1646–1654, 2014.

[11] Timothy Dozat. Incorporating nesterov momentum into adam. 2016.

[12] Benjamin Dubois-Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Simon Lacoste-Julien. Svrg

meets adagrad: Painless variance reduction. arXiv preprint arXiv:2102.09645, 2021.

[13] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of machine learning research, 12(7), 2011.

[14] Alina Ene and Huy L Nguyen. Adaptive and universal algorithms for variational inequalities with optimal

convergence s. arXiv preprint arXiv:2010.07799, 2021.

[15] Alina Ene, Huy L Nguyen, and Adrian Vladu. Adaptive gradient methods for constrained convex optimization
and variational inequalities. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages
7314–7321, 2021.

[16] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization

via stochastic path integrated diﬀerential estimator. arXiv preprint arXiv:1807.01695, 2018.

[17] Feihu Huang, Junyi Li, and Heng Huang. Super-adam: Faster and universal framework of adaptive gradients.

arXiv preprint arXiv:2106.08208, 2021.

[18] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

Advances in neural information processing systems, 26:315–323, 2013.

[19] Pooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesvári. A simpler approach to accelerated optimiza-
tion: iterative averaging meets optimism. In International Conference on Machine Learning, pages 4984–4993.
PMLR, 2020.

10

[20] Ali Kavis, Kﬁr Y. Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm with opti-
mal guarantees for constrained optimization. In Advances in Neural Information Processing Systems (NeurIPS),
pages 6257–6266, 2019.

[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv preprint

arXiv:1412.6980, 2014.

[22] Guanghui Lan, Zhize Li, and Yi Zhou. A uniﬁed variance-reduced accelerated gradient method for convex

optimization. arXiv preprint arXiv:1905.12412, 2019.

[23] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic optimization. SIAM

Journal on Optimization, 28(4):2753–2782, 2018.

[24] Kﬁr Levy, Ali Kavis, and Volkan Cevher. Storm+: Fully adaptive sgd with recursive momentum for nonconvex

optimization. Advances in Neural Information Processing Systems, 34, 2021.

[25] Kﬁr Y. Levy. Online to oﬄine conversions, universality and adaptive minibatch sizes. In Advances in Neural

Information Processing Systems (NeurIPS), pages 1613–1622, 2017.

[26] Kﬁr Y Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and acceleration. In

Advances in Neural Information Processing Systems (NeurIPS), pages 6500–6509, 2018.

[27] Bingcong Li, Lingda Wang, and Georgios B Giannakis. Almost tune-free variance reduction. In International

Conference on Machine Learning, pages 5969–5978. PMLR, 2020.

[28] Zhize Li. Anita: An optimal

loopless accelerated variance-reduced gradient method.

arXiv preprint

arXiv:2103.11333, 2021.

[29] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization. arXiv

preprint arXiv:1506.02186, 2015.

[30] Julien Mairal. Optimization with ﬁrst-order surrogate functions.

In International Conference on Machine

Learning, pages 783–791. PMLR, 2013.

[31] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization.

In Conference on Learning Theory (COLT), pages 244–256. Omnipress, 2010.

[32] Mehryar Mohri and Scott Yang. Accelerating online convex optimization via adaptive prediction. In Artiﬁcial

Intelligence and Statistics (AISTATS), pages 848–856, 2016.

[33] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ

2). In Doklady an ussr, volume 269, pages 543–547, 1983.

[34] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science &

Business Media, 2003.

[35] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International

Conference on Learning Representations, 2018.

[36] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential conver-

gence rate for ﬁnite training sets. arXiv preprint arXiv:1202.6258, 2012.

[37] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming, 162(1-2):83–112, 2017.

[38] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss mini-

mization. Journal of Machine Learning Research, 14(2), 2013.

[39] Chaobing Song, Yong Jiang, and Yi Ma. Variance reduction via accelerated dual averaging for ﬁnite-sum

optimization. Advances in Neural Information Processing Systems, 33, 2020.

[40] Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic gradient

descent. Advances in Neural Information Processing Systems, 29:685–693, 2016.

11

[41] Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of

its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.

[42] Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. Advances

in neural information processing systems, 29:3639–3647, 2016.

[43] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex optimization.

arXiv preprint arXiv:1806.07811, 2018.

A Analysis of algorithm 1

In this section, we analyze Algorithm 1 and prove the following convergence guarantee:

Theorem A.1 (Convergence of AdaVRAE). Deﬁne s0 = (cid:100)log2 log2 4n(cid:101), c = 3

2 . If we choose parameters as follows

a(s) =

(cid:40)

(4n)−0.5s
s−s0−1+c
2c

1 ≤ s ≤ s0
s0 < s

,

Ts = n,
5
4

=

.

A(0)
T0

Assuming X is a compact convex set with diameter D, the number of individual gradient evaluations to achieve a
solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for Algorithm 1 is

#grads =






(cid:1)

O(cid:0)n log log V
O

n log log n +

(cid:16)

(cid:15)

(cid:17)

(cid:113) V n
(cid:15)

if (cid:15) ≥ V
n
if (cid:15) < V
n

where V = 5
2

(cid:0)F (u(0)) − F (x∗)(cid:1) + γ (cid:13)

(cid:13)u(0) − x∗(cid:13)
2
(cid:13)

+

16β(D4+2η4)
η2

.

To start with, we state and prove the following variance reduction lemma commonly used in accelerated methods:

Lemma A.2. (Variance Reduction) Let i ∼ Uniform([n]) and g = ∇fi(x) − ∇fi(u) + ∇f (u) be an estimate of the
gradient of f at x. We have

(cid:104)

(cid:107)g − ∇f (x)(cid:107)2(cid:105)

Ei

≤ 2β (f (u) − f (x) − (cid:104)∇f (x), u − x(cid:105)) .

Proof. By the deﬁnition of g,

(cid:107)g − ∇f (x)(cid:107)2(cid:105)
(cid:104)

Ei

= Ei

(cid:104)

(cid:107)∇fi(x) − ∇fi(u) + ∇f (u) − ∇f (x)(cid:107)2(cid:105)
(cid:107)∇fi(u) − ∇fi(x)(cid:107)2(cid:105)
(cid:104)

(a)
≤ Ei

(b)
≤ Ei [2β (fi(u) − fi(x) − (cid:104)∇fi(x), u − x(cid:105))]
(c)
= 2β (f (u) − f (x) − (cid:104)∇f (x), u − x(cid:105)) ,

(cid:107)X − E [X](cid:107)2(cid:105)
where (a) is because Ei [∇fi(u) − ∇fi(x)] = ∇f (u) − ∇f (x) and E
convexity and β-smoothness of fi, (c) is by i ∼ Uniform([n]) and the deﬁnition of f .

(cid:104)

(cid:104)

≤ E

(cid:107)X(cid:107)2(cid:105)

, (b) is by the

A.1 Single iteration progress

We ﬁrst analyze the progress in function value made in a single iteration of an epoch. The analysis follows the
standard method as in [14]; however, we need to pay attention to the extra term for the checkpoint that appears in
the convex combination for x(s)

. We start oﬀ by the following observation

t

12

Lemma A.3. For any s ≥ 1 and t ∈ [Ts],

t − x(s)
x(s)

t−1 =

(cid:16)

a(s)
A(s)
t−1

t − x(s)
x(s)

t

(cid:17)

+

(cid:16)

(cid:0)a(s)(cid:1)2
A(s)
t−1

u(s−1) − x(s)

t

(cid:17)

.

Proof. We note that the deﬁnition x(s)

t = 1
A(s)
t

(cid:16)

A(s)

t−1x(s)

t−1 + a(s)x(s)

t + (cid:0)a(s)(cid:1)2

u(s−1)(cid:17)

implies

A(s)

t x(s)

t = A(s)

t−1x(s)

t−1 + a(s)x(s)
t +
(cid:16)

(a)
⇔ A(s)

t−1(x(s)

t − x(s)

t−1) = a(s)(x(s)

t − x(s)

t ) +

a(s)(cid:17)2

(u(s−1) − x(s)
t )

a(s)(cid:17)2
(cid:16)

u(s−1)

⇔ x(s)

t − x(s)

t−1 =

(cid:16)

a(s)
A(s)
t−1

t − x(s)
x(s)

t

(cid:17)

+

(cid:0)a(s)(cid:1)2
A(s)
t−1

(cid:16)
u(s−1) − x(s)

t

(cid:17)

,

where (a) is by A(s)

t = A(s)

t−1 + a(s) + (cid:0)a(s)(cid:1)2.

Next, we bound the function progress in a single epoch via the stochastic regret. Note that, this lemma is
somewhat weaker than we would desire, due to the appearance the coeﬃcient of the checkpoint, making the LHS
not immediately telescope. We will account for this factor later in the analysis.

Lemma A.4. For all epochs s ≥ 1 and all iterations t ∈ [Ts]

(cid:20)(cid:18)

E

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

(cid:17)(cid:21)

F (x(s)

t−1) − F (x∗)


≤E


a(s) (cid:68)



(cid:124)

g(s)
t

t − x∗(cid:69)
, x(s)
(cid:125)
(cid:123)(cid:122)
stochastic regret

(cid:16)

+

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)





(cid:34)

A(s)
t−1
2β

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:18)

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

− A(s)
t−1

− E

+ E

Proof. Using the observation in Lemma A.3, we have

(cid:16)
h(x(s)

t−1) − h(x∗)

(cid:17)(cid:21)

.

t ) − F (x(s)
t ) − f (x(s)

t−1)
t−1) + h(x(s)

F (x(s)
=f (x(s)
(cid:68)
(a)
∇f (x(s)
≤

t ), x(s)

t − x(s)

t−1

−

t ) − h(x(s)
t−1)
(cid:13)
(cid:69)
1
(cid:13)∇f (x(s)
(cid:13)
2β
(cid:0)a(s)(cid:1)2
A(s)
t−1
+ h(x(s)

+

(cid:69)

t

(cid:13)
2
(cid:13)
(cid:13)

(b)
=

a(s)
At−1

−

1
2β

(cid:68)
∇f (x(s)

t ), x(s)

t − x(s)

(cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

t ) − h(x(s)

t−1)

t ) − ∇f (x(s)

(cid:13)
2
(cid:13)
t−1)
(cid:13)

+ h(x(s)

t ) − h(x(s)

t−1)

where (a) is due to the smoothness of f and (b) comes from Lemma A.3. By the convexity of f , we also have

F (x(s)
=f (x(s)
(cid:68)
∇f (x(s)

t ) − F (x∗)
t ) − f (x∗) + h(x(s)
t − x∗(cid:69)

t ), x(s)

≤

t ) − h(x∗)
+ h(x(s)

t ) − h(x∗)

We combine the two inequalities and obtain

(cid:16)

A(s)
t−1

F (x(s)

t ) − F (x(s)

(cid:17)
t−1)

+ a(s) (cid:16)

F (x(s)

t ) − F (x∗)

(cid:17)

13

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)

≤a(s) (cid:68)

∇f (x(s)

(cid:16)

+

t ), x(s)

a(s)(cid:17)2 (cid:68)
t − x∗(cid:69)
(cid:13)
(cid:13)
2
t ) − ∇f (x(s)
(cid:13)∇f (x(s)
(cid:13)
(cid:13)
t−1)
(cid:13)
+ a(s) (cid:16)
(cid:17)
(cid:16)
t ) − h(x(s)
h(x(s)
t−1)
a(s)(cid:17)2 (cid:68)
(cid:16)
t − x∗(cid:69)

, x(s)

+

∇f (x(s)

−

At−1
2β
+ A(s)
t−1
=a(s) (cid:68)

g(s)
t

+ a(s) (cid:68)
(cid:16)

+ A(s)
t−1

∇f (x(s)

t ) − g(s)

t

h(x(s)

t ) − h(x(s)

, x(s)

t − x∗(cid:69)
−
+ a(s) (cid:16)
(cid:17)
t−1)

h(x(s)

(cid:17)
t ) − h(x∗)

(cid:69)

t

t ), u(s−1) − x(s)
A(s)
(cid:13)
t ) − ∇f (x(s)
(cid:13)∇f (x(s)
t−1
(cid:13)
2β
(cid:17)
h(x(s)
t ) − h(x∗)

.

(cid:13)
2
(cid:13)
t−1)
(cid:13)

Note that we can rearrange the terms

A(s)
t−1
(cid:18)

(cid:16)

F (x(s)

t−1)

t ) − F (x(s)
a(s)(cid:17)2(cid:19) (cid:16)

(cid:16)

A(s)

t −

(cid:16)

A(s)
t−1
(cid:18)

A(s)

t −

h(x(s)

t ) − h(x(s)
a(s)(cid:17)2(cid:19) (cid:16)
(cid:16)

=

=

(cid:17)

+ a(s) (cid:16)

F (x(s)

t ) − F (x∗)

(cid:17)

F (x(s)

(cid:17)
t ) − F (x∗)
(cid:17)
+ a(s) (cid:16)
t−1)

h(x(s)

(cid:17)
t ) − h(x∗)

− A(s)
t−1

(cid:16)

F (x(s)

(cid:17)
t−1) − F (x∗)

,

h(x(s)

t ) − h(x∗)

(cid:17)

− A(s)
t−1

(cid:16)

h(x(s)

(cid:17)
t−1) − h(x∗)

.

Thus we obtain

(cid:18)

(cid:16)

A(s)

t −

a(s)(cid:17)2(cid:19) (cid:16)
t − x∗(cid:69)

, x(s)

F (x(s)

t ) − F (x∗)
a(s)(cid:17)2 (cid:68)

(cid:16)

∇f (x(s)

+

≤a(s) (cid:68)

g(s)
t

(cid:17)

− A(s)
t−1

(cid:16)

F (x(s)

(cid:17)
t−1) − F (x∗)

, x(s)

t − x∗(cid:69)

−

t ) − ∇f (x(s)

(cid:13)
2
(cid:13)
t−1)
(cid:13)

(cid:69)

t

t ), u(s−1) − x(s)
A(s)
t−1
2β

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
(cid:16)

+ a(s) (cid:68)
(cid:18)

+

A(s)

t −

∇f (x(s)

t

t ) − g(s)
a(s)(cid:17)2(cid:19) (cid:16)

(cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

− A(s)
t−1

h(x(s)

(cid:17)
t−1) − h(x∗)

.

(3)

Observe that for t < Ts
a(s) (cid:68)
(cid:104)

E

∇f (x(s)

t ) − g(s)

t

, x(s)

t − x∗(cid:69)(cid:105)

(cid:104)

= E

E

i(s)
t

= 0.

a(s) (cid:68)
(cid:104)

∇f (x(s)

t ) − g(s)

t

, x(s)

t − x∗(cid:69)(cid:105)(cid:105)

and for t = Ts,we have ∇f (x(s)
both sides of (3), we get

t ) = g(s)

t

(cid:104)

thus E

a(s) (cid:68)

∇f (x(s)

t ) − g(s)

t

, x(s)

t − x∗(cid:69)(cid:105)

= 0. By taking expectations w.r.t.

(cid:20)(cid:18)

E

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

F (x(s)

g(s)
t

, x(s)

t − x∗(cid:69)

+

(cid:16)

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:17)(cid:21)

t−1) − F (x∗)
(cid:69)(cid:21)

A(s)
t−1
2β

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:18)

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

− A(s)
t−1

(cid:16)
h(x(s)

t−1) − h(x∗)

(cid:17)(cid:21)

.

≤E

(cid:20)
a(s) (cid:68)
(cid:34)

− E

+ E

To analyze the stochastic regret, we split the inner product as follows

14

(cid:68)
g(s)
t

, x(s)

t − x∗(cid:69)

(cid:68)
g(s)
t

t − x∗(cid:69)
, z(s)

+

=

(cid:68)
t − g(s)
g(s)

t−1, x(s)

t − z(s)

t

(cid:69)

(cid:68)

+

t−1, x(s)
g(s)

t − z(s)

t

(cid:69)

.

For each term we give a bound as stated in Lemma A.5.

Lemma A.5. For any s ≥ 1 all iterations t ∈ [Ts], we have

a(s) (cid:68)

t−1, x(s)
g(s)

t − z(s)

t

(cid:69)

≤

t−1 − z(s)

γ(s)
(cid:13)
(cid:13)z(s)
t−1
(cid:13)
2
+ a(s) (cid:16)

h(z(s)
t

t

−

(cid:13)
2
(cid:13)
(cid:13)
) − h(x(s)
t )

γ(s)
t−1
2
(cid:17)

.

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x(s)

t

(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

a(s) (cid:68)

g(s)
t

t − x∗(cid:69)
, z(s)

≤

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − γ(s)
γ(s)
t − x∗(cid:13)
2
(cid:13)
(cid:13)
2
γ(s)
(cid:13)
(cid:13)
2
(cid:13)z(s)
t−1
(cid:13)
(cid:13)
−
(cid:13)
2
+ a(s) (cid:16)
h(x∗) − h(z(s)

t − z(s)

t−1

−

(cid:17)

)

t

.

+

γ(s)
t−1
2
t − γ(s)
γ(s)
2

t−1 − x∗(cid:13)
(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)x(s)
(cid:13)

t−1

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

a(s) (cid:68)

t − g(s)
g(s)

t−1, x(s)

t − z(s)

t

(cid:69)

≤

(cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

γ(s)
t
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

.

Proof. Since x(s)
we have

t = arg minx∈X

(cid:26)

a(s) (cid:68)

(cid:69)

g(s)
t−1, x

+ a(s)h(x) +

γ(s)
t−1
2

(cid:13)
(cid:13)x − z(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

, by the optimality condition of x(s)

t

,

(cid:68)
a(s)g(s)

t−1 + a(s)h(cid:48)(x(s)

t ) + γ(s)

t−1

(cid:16)

t − z(s)
x(s)

t−1

(cid:17)

, x(s)

t − z(s)

t

(cid:69)

≤ 0,

where h(cid:48)(x(s)

t ) is a subgradient of h at x(s)

t

. We rearrange the above inequality and obtain

t−1, x(s)
g(s)

t − z(s)

t

(cid:69)

(cid:68)

≤ γ(s)
t−1

t − z(s)
x(s)

t−1, z(s)

t − x(s)

t

(cid:69)

+ a(s) (cid:68)

h(cid:48)(x(s)

t ), z(s)

t ) ∈ ∂h(x(s)
a(s) (cid:68)

(cid:68)
t − z(s)
x(s)

t−1, z(s)

(a)
≤ γ(s)
t−1
γ(s)
(cid:13)
(cid:13)z(s)
t−1
(cid:13)
2
+ a(s) (cid:16)
h(z(s)
t

(b)
=

t − x(s)
t
γ(s)
t−1
2
(cid:17)

,

−

(cid:13)
2
(cid:13)
(cid:13)
) − h(x(s)
t )

t−1 − z(s)

t

(cid:69)

t − x(s)
(cid:17)

t

(cid:69)

+ a(s) (cid:16)

h(z(s)
t

) − h(x(s)
t )

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x(s)

t

(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

where (a) follows from the convexity of h and the fact that h(cid:48)(x(s)
.
(cid:104)a, b(cid:105) = 1
2

(cid:107)a + b(cid:107)2 − (cid:107)a(cid:107)2 − (cid:107)b(cid:107)2(cid:17)
Using the optimality condition of z(s)

, we have

(cid:16)

t

t ) ∈ ∂h(x(s)

t ), and (b) is due to the identity

(cid:68)
a(s)g(s)

t + a(s)h(cid:48)(z(s)

t

(cid:16)

) + γ(s)
t−1

t − z(s)
z(s)

t−1

(cid:17)

(cid:16)

+

t − γ(s)
γ(s)

t−1

(cid:17) (cid:16)

t − x(s)
z(s)

t

(cid:17)

t − x∗(cid:69)
, z(s)

≤ 0

where h(cid:48)(z(s)

t

t

) ∈ ∂h(z(s)
a(s) (cid:68)

) is a subgradient of h at z(s)
t − x∗(cid:69)
, z(s)

(cid:68)
t − z(s)
z(s)

≤ γ(s)
t−1

g(s)
t

t

. We rearrange the above inequality and obtain

(cid:69)

(cid:16)

+

t − γ(s)
γ(s)

t−1

(cid:17) (cid:68)

t − x(s)
z(s)

t

, x∗ − z(s)

t

(cid:69)

+ a(s) (cid:68)

h(cid:48)(z(s)
t

t−1, x∗ − z(s)
t
(cid:69)
), x∗ − z(s)

t

(c)
≤ γ(s)
t−1

(cid:68)
t − z(s)
z(s)

t−1, x∗ − z(s)

t

(cid:69)

(cid:16)

+

t − γ(s)
γ(s)

t−1

15

(cid:17) (cid:68)

t − x(s)
z(s)

t

, x∗ − z(s)

t

(cid:69)

(d)
=

=

t

)

(cid:17)

−

t−1

h(x∗) − h(z(s)

+ a(s) (cid:16)
γ(s)
(cid:20)(cid:13)
(cid:13)
t−1 − x∗(cid:13)
2
(cid:13)z(s)
(cid:13)z(s)
t−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
t − γ(s)
γ(s)
(cid:20)(cid:13)
t − x∗(cid:13)
2
(cid:13)x(s)
(cid:13)
(cid:13)
+
(cid:13)
2
(cid:17)
+ a(s) (cid:16)
h(x∗) − h(z(s)
t − γ(s)
γ(s)
t − x∗(cid:13)
2
(cid:13)
(cid:13)
2
γ(s)
(cid:13)
(cid:13)
2
(cid:13)z(s)
t−1
(cid:13)
(cid:13)
−
(cid:13)
2
+ a(s) (cid:16)
h(x) − h(z(s)

t − z(s)

(cid:13)
(cid:13)x(s)
(cid:13)

t−1

t−1

−

(cid:17)

)

)

,

t

t

t − z(s)

t−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
t − x∗(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
t − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t − z(s)

t

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

−

+

γ(s)
t−1
2
t − γ(s)
γ(s)
2

t−1 − x∗(cid:13)
(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)x(s)
(cid:13)

t−1

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

where (c) follows from the convexity of h and the fact that h(cid:48)(z(s)
(cid:107)a + b(cid:107)2 − (cid:107)a(cid:107)2 − (cid:107)b(cid:107)2(cid:17)
.
(cid:104)a, b(cid:105) = 1
2
For the third inequality, we have

(cid:16)

t

) ∈ ∂h(z(s)

t

), and (d) is due to the identity

a(s) (cid:68)

t − g(s)
g(s)

t−1, x(s)

t − z(s)

t

(cid:69) (e)

t − z(s)

t

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

t − g(s)

t−1

≤ a(s) (cid:13)
(cid:13)g(s)
(cid:13)
(cid:0)a(s)(cid:1)2
(cid:13)
(cid:13)g(s)
(cid:13)
2γ(s)
t

(f )
≤

t − g(s)

t−1

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

+

γ(s)
t
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − z(s)

t

(cid:13)
2
(cid:13)
(cid:13)

.

where (e) is by the Cauchy–Schwarz inequality, (f ) is by Young’s inequality.

With above results, we obtain the descent lemma for one iteration. A key idea to remove (cid:0)a(s)(cid:1)2 from the
(cid:13)
(cid:13)g(s)
(cid:13)

is to split the term (a(s))2
2γ(s)
t

(cid:18) (a(s))2
2γ(s)
t

(cid:19) (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t − g(s)

(a(s))2
16β

F (x(s)

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

into

t−1

t−1

+

−

(cid:16)

coeﬃcient of

(a(s))2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:17)
t ) − F (x∗)
(cid:13)
2
(cid:13)
(cid:13)

and apply the VR lemma for the second term.

Lemma A.6. For all epochs s ≥ 1 and all iterations t ∈ [Ts], we have

(cid:20)(cid:18)

E

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

F (x(s)

t−1) − F (x∗)

(cid:17)(cid:21)

(cid:34)

≤E

γ(s)
t−1
2
(cid:20)(cid:16)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:69)(cid:21)

+ E

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

+

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:16)

(cid:16)

(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34)(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
a(s)(cid:17)2 (cid:16)

(cid:20)(cid:16)

+ E

+ E

+ E

+ E

f (u(s−1)) − f (x(s)

t ) −

(cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:35)

(cid:69)(cid:17)

f (u(s−1)) − f (x(s)

t−1) −

(cid:68)
∇f (x(s)

t−1), u(s−1) − x(s)

t−1

(cid:35)

(cid:69)(cid:17)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

h(u(s−1)) − h(x(s)
t )

(cid:17)(cid:21)

.

Proof. By Lemma A.5, we can bound a(s) (cid:68)

g(s)
t

, x(s)

t − x∗(cid:69)

as follows

a(s) (cid:68)

g(s)
t

, x(s)

t − x∗(cid:69)

≤

γ(s)
t−1
2

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

16

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x(s)

t

(cid:13)
2
(cid:13)
(cid:13)

+ a(s) (cid:16)

h(x∗) − h(x(s)
t )

(cid:17)

+

≤

γ(s)
t−1
2
+ a(s) (cid:16)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

h(x∗) − h(x(s)
t )

(cid:17)

+

(cid:13)
(cid:13)g(s)
(cid:13)

(cid:0)a(s)(cid:1)2
2γ(s)
t
t − x∗(cid:13)
(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
(cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:13)
(cid:13)g(s)
(cid:13)

+

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

.

t−1

t − γ(s)
γ(s)
2
(cid:13)
2
(cid:13)
(cid:13)

t−1

.

t − g(s)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

Combining the above result with Lemma A.4, we know

(cid:20)(cid:18)

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

t ) − F (x∗)

(cid:17)

(cid:16)

− A(s)
t−1

F (x(s)

t−1) − F (x∗)

(cid:17)(cid:21)

E

(cid:34)

≤E

γ(s)
t−1
2
(cid:20)(cid:16)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:69)(cid:21)

+ E

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

+

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

Note that

+ E

+ E

=

(a)
≤

=

(cid:34) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:20)(cid:18)

A(s)

t −

t−1

(cid:13)
(cid:13)g(s)
t − g(s)
(cid:13)
a(s)(cid:17)2(cid:19) (cid:16)

(cid:16)

(cid:13)
2
(cid:13)
(cid:13)

−

A(s)
t−1
2β

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

h(x(s)

t ) − h(x∗)

(cid:17)

(cid:16)

− A(s)
t−1

h(x(s)

t−1) − h(x∗)

(cid:17)

+ a(s) (cid:16)

h(x∗) − h(x(s)
t )

(cid:17)(cid:21)

.

(4)

(cid:18)

A(s)

t −

(cid:16)

(cid:18)

(cid:16)

A(s)

t −
a(s)(cid:17)2 (cid:16)

(cid:16)

−

(cid:18)

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)
a(s)(cid:17)2(cid:19) (cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

− A(s)
t−1

(cid:16)

h(x(s)

(cid:17)
t−1) − h(x∗)

+ a(s) (cid:16)

h(x∗) − h(x(s)
t )

(cid:17)

h(x(s)

(cid:17)
t ) − h(x∗)

(cid:16)

a(s)(cid:17)2 (cid:16)

+

h(u(s−1)) − h(x∗)

(cid:17)

(cid:17)

− A(s)
t−1

(cid:16)

h(x(s)

(cid:17)
t−1) − h(x∗)

− a(s) (cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

h(u(s−1)) − h(x∗)
a(s)(cid:17)2(cid:19) (cid:16)

h(x(s)

(cid:17)
t ) − h(x∗)

(cid:16)

a(s)(cid:17)2 (cid:16)

+

h(u(s−1)) − h(x∗)

(cid:17)

t (h(x(s)

− A(s)
(cid:16)
a(s)(cid:17)2 (cid:16)

t ) − h(x∗))
h(u(s−1)) − h(x(s)
t )

(cid:17)

,

(5)

where (a) is by the convexity of h and A(s)

t = A(s)

t−1 + a(s) + (cid:0)a(s)(cid:1)2. Plugging in (5) into (4), we know

(cid:20)(cid:18)

E

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

t ) − F (x∗)

(cid:17)

(cid:16)

− A(s)
t−1

F (x(s)

t−1) − F (x∗)

(cid:17)(cid:21)

(cid:34)

≤E

γ(s)
t−1
2
(cid:20)(cid:16)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:69)(cid:21)

+ E

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

+

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:34) (cid:0)a(s)(cid:1)2
2γ(s)
t
a(s)(cid:17)2 (cid:16)

(cid:20)(cid:16)

+ E

+ E

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

−

A(s)
t−1
2β

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

h(u(s−1)) − h(x(s)
t )

(cid:17)(cid:21)

.

Now for E

(cid:20)(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
(cid:13)
(cid:13)

2(cid:21)

, when 1 < t < Ts, we have

E

(cid:20)(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

(cid:20)
4

≤ E

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − g(s)

t

(cid:13)
2
(cid:13)
(cid:13)

+ 4

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t−1) − g(s)

t−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

17

(cid:20)
2

+ E

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
(cid:16)

t ) − ∇f (x(s)

2(cid:21)

(cid:13)
(cid:13)
t−1)
(cid:13)

(cid:104)

(b)
≤ E

8β
(cid:104)

8β
(cid:20)
2

+ E

+ E

f (u(s−1)) − f (x(s)
(cid:16)

f (u(s−1)) − f (x(s)

t ) −

(cid:68)
∇f (x(s)
(cid:68)
∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)(cid:17)(cid:105)

t−1), u(s−1) − x(s)

t−1

(cid:69)(cid:17)(cid:105)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1) −
2(cid:21)
(cid:13)
(cid:13)
t−1)
(cid:13)

,

(6)

where (b) is by Lemma A.2 for all 1 < t < Ts. When t = 1, note that both
f (x(s)

are zero by our deﬁnition x(s)

(cid:68)
∇f (x(s)

t−1), u(s−1) − x(s)

t−1)−

t−1

(cid:69)

, which means the above inequality is still true. When t = Ts, note that

g(s)
0
f (x(s)
holds in this case. Now we conlclude the above inequality is right for t ∈ [Ts].

t ), u(s−1) − x(s)

(cid:68)
∇f (x(s)

t ) −

(cid:69)

t

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
0 = u(s−1) and ∇f (x(s)
(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t−1) − g(s)
0 ) = ∇f (x(s−1)
Ts−1
(cid:13)
2
(cid:13)
(cid:13)

t ) − g(s)

(cid:13)
2
(cid:13)
(cid:13)

t−1

t

and f (u(s−1)) −
) = g(s−1)
Ts−1

=

= 0 and f (u(s−1)) −

is always non-negative due to the convexity of f . So the above inequality also

into

(cid:18) (a(s))2
2γ(s)
t

(a(s))2
16β

−

(cid:19) (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(a(s))2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

and applying (6)

Splitting (a(s))2
2γ(s)
t
t − g(s)

(cid:13)
(cid:13)g(s)
(cid:13)

(cid:13)
t − g(s)
(cid:13)g(s)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

t−1

16β

, we have

t−1

(cid:13)
2
(cid:13)
(cid:13)

to (a(s))2

(cid:20)(cid:18)

E

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

F (x(s)

t−1) − F (x∗)

(cid:17)(cid:21)

(cid:34)

≤E

γ(s)
t−1
2
(cid:20)(cid:16)

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:69)(cid:21)

+ E

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

+

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:16)

(cid:16)

(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34)(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
a(s)(cid:17)2 (cid:16)

(cid:20)(cid:16)

+ E

+ E

+ E

+ E

f (u(s−1)) − f (x(s)

t ) −

(cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:35)

(cid:69)(cid:17)

f (u(s−1)) − f (x(s)

t−1) −

(cid:68)
∇f (x(s)

t−1), u(s−1) − x(s)

t−1

(cid:35)

(cid:69)(cid:17)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

t−1)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

h(u(s−1)) − h(x(s)
t )

(cid:17)(cid:21)

.

A.2 Single epoch progress and ﬁnal output

Even though Lemma A.6 looks somewhat more convoluted, when we sum up over all iterations in one epoch, many
terms are canceled out nicely and we obtain the following lemma that states the progress of the function value in
one epoch. The trick is to set the value for each term at the end of one epoch equal to its value in the next one,
. Due to the accumulation of the term (cid:0)F (u(s−1)) − F (x∗)(cid:1) throughout the epoch, we
with an exception for A(s−1)
Ts−1
(cid:0)a(s)(cid:1)2.
0 = A(s−1)
will set A(s)
− Ts
Ts−1

Lemma A.7. For all epochs s ≥ 1, if

(cid:16)

a(s)(cid:17)2

≤ 4A(s)

t−1, ∀t ∈ [Ts] .

We have

(cid:104)

E

A(s)
Ts

(cid:16)

F (u(s)) − F (x∗)

(cid:17)

(cid:16)

− A(s−1)
Ts−1

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

18

(cid:34)

≤E

γ(s)
0
2
(cid:34) Ts(cid:88)

(cid:13)
0 − x∗(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

t=1

−

−

γ(s+1)
0
2
(cid:0)a(s)(cid:1)2
16β

+ E

0

(cid:13)
(cid:13)z(s+1)
(cid:13)
(cid:33)

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

Proof. Using Lemma A.6, we know

Ts(cid:88)

E

(cid:20)(cid:18)

A(s)

t −

(cid:16)

a(s)(cid:17)2(cid:19) (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

F (x(s)

t−1) − F (x∗)

(cid:17)(cid:21)

t=1

Ts(cid:88)

t=1

≤

(cid:34)

E

γ(s)
t−1
2

(cid:13)
(cid:13)z(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t
2

(cid:13)
(cid:13)z(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

Ts(cid:88)

E

(cid:20)(cid:16)

+

a(s)(cid:17)2 (cid:68)

∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:69)

(cid:16)

a(s)(cid:17)2 (cid:16)

+

h(u(s−1)) − h(x(s)
t )

(cid:17)(cid:21)

t=1

Ts(cid:88)

t=1

Ts(cid:88)

t=1

Ts(cid:88)

E

E

E

+

+

+

(cid:16)

(cid:16)

(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:34)(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
0 − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

(a)
= E

t=1
(cid:34)
γ(s)
0
2
(cid:34)Ts−1
(cid:88)

+ E

+ E

+ E

t=1

(cid:16)

(cid:34) (cid:0)a(s)(cid:1)2
2
(cid:32) (cid:0)a(s)(cid:1)2
(cid:34) Ts(cid:88)
2γ(s)
t
0 − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

t=1

(cid:34)

(b)
≤E

γ(s)
0
2
(cid:34) Ts(cid:88)

+ E

t=1

(cid:34) Ts(cid:88)

t=1

+ E

γ(s)
0
2
(cid:34) Ts(cid:88)

+ E

(cid:34)

(c)
= E

−

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
0 − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

f (u(s−1)) − f (x(s)

t ) −

(cid:68)
∇f (x(s)

t ), u(s−1) − x(s)

t

(cid:35)

(cid:69)(cid:17)

f (u(s−1)) − f (x(s)

t−1) −

(cid:68)
∇f (x(s)

t−1), u(s−1) − x(s)

t−1

(cid:35)

(cid:69)(cid:17)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

−

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

−

γ(s+1)
0
2

(cid:13)
(cid:13)z(s+1)
(cid:13)

0

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

(cid:16)

a(s)(cid:17)2 (cid:16)

f (u(s−1)) − f (x(s)
t )

(cid:17)

(cid:16)

a(s)(cid:17)2 (cid:16)

+

h(u(s−1)) − h(x(s)
t )

(cid:17)

(cid:35)

f (u(s−1)) − f (x(s)
Ts

) +

(cid:68)
∇f (x(s)
Ts

), u(s−1) − x(s)
Ts

(cid:35)

(cid:69)(cid:17)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

−

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

−

γ(s+1)
0
2

(cid:13)
(cid:13)z(s+1)
(cid:13)

0

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:16)

a(s)(cid:17)2 (cid:18)

f (u(s−1)) − f (x(s)

t ) +

a(s)(cid:17)2 (cid:16)
(cid:16)

h(u(s−1)) − h(x(s)
t )

(cid:33)

(cid:0)a(s)(cid:1)2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

−

γ(s+1)
0
2

(cid:13)
(cid:13)z(s+1)
(cid:13)

0

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

(cid:16)

a(s)(cid:17)2 (cid:16)

F (u(s−1)) − F (x(s)
t )

(cid:35)

(cid:17)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:33)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)
(cid:17)(cid:19)(cid:35)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)
2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

t=1

(cid:34) Ts(cid:88)

t=1

+ E

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

19

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

.

(7)

where (a) is due to z(s+1)

0

= z(s)
Ts

, γ(s+1)
0

= γ(s)
Ts

, x(s)

0 = u(s−1), (b) is by the convexity of f

(cid:68)
∇f (x(s)
Ts

), u(s−1) − x(s)
Ts

(cid:69)

≤ f (u(s−1)) − f (x(s)
Ts

),

(c) is by the deﬁnition of F = f + h. By adding (cid:80)Ts
t=1

(cid:0)a(s)(cid:1)2 (cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

to both sides of 7. we obtain

(cid:34) Ts(cid:88)

E

A(s)
t

(cid:16)

F (x(s)

(cid:17)
t ) − F (x∗)

− A(s)
t−1

(cid:16)

F (x(s)

(cid:17)
t−1) − F (x∗)

(cid:35)

(cid:34)

≤E

t=1
γ(s)
0
2
(cid:20)
Ts
(cid:34) Ts(cid:88)

+ E

+ E

t=1

Note that

(cid:13)
(cid:13)z(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s+1)
0
2

(cid:16)

a(s)(cid:17)2 (cid:16)

F (u(s−1)) − F (x∗)

− x∗(cid:13)
2
(cid:13)
(cid:13)

0

(cid:13)
(cid:13)z(s+1)
(cid:13)
(cid:17)(cid:21)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:33)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

(cid:34) Ts(cid:88)

E

(cid:16)

A(s)
t

F (x(s)

t ) − F (x∗)

(cid:17)

− A(s)
t−1

(cid:16)

F (x(s)

(cid:17)
t−1) − F (x∗)

(cid:35)

t=1
(cid:104)
A(s)
Ts
(cid:104)
A(s)
Ts

(cid:16)

(cid:16)

=E

(d)
= E

F (x(s)
Ts

) − F (x∗)

(cid:17)

F (u(s)) − F (x∗)

(cid:17)

− A(s)
0

− A(s)
0

0 ) − F (x∗)

(cid:17)(cid:105)

(cid:16)

F (x(s)
(cid:16)

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

,

where (d) is due to the deﬁnition u(s) = x(s)
Ts

and x(s)

(cid:20)
A(s)
Ts

E

(cid:16)

F (u(s)) − F (x∗)

(cid:17)

(cid:18)

A(s)

0 + Ts

−

0 = u(s−1). Finally we have
a(s)(cid:17)2(cid:19) (cid:16)

F (u(s−1)) − F (x∗)

(cid:16)

(cid:17)(cid:21)

(cid:34)

≤E

γ(s)
0
2
(cid:34) Ts(cid:88)

(cid:13)
0 − x∗(cid:13)
2
(cid:13)z(s)
(cid:13)
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

t=1

−

−

γ(s+1)
0
2
(cid:0)a(s)(cid:1)2
16β

+ E

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

0

(cid:13)
(cid:13)z(s+1)
(cid:13)
(cid:33)

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

t−1

t=1

Ts(cid:88)

t − γ(s)
γ(s)
2
(cid:32) (cid:0)a(s)(cid:1)2
8β

−

A(s)
t−1
2β

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:33)

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t ) − ∇f (x(s)

2(cid:35)
(cid:13)
(cid:13)
t−1)
(cid:13)

Combining the fact A(s)

0 = A(s−1)
Ts−1

− Ts

(cid:0)a(s)(cid:1)2 and our condition (cid:0)a(s)(cid:1)2

≤ 4A(s)
t−1

, we get the desired result.

The telescoping sum on the LSH allows us to obtain the guarantee for the ﬁnal output u(S).

Lemma A.8. For all S ≥ 1, assume we have
a(s)(cid:17)2

(cid:16)

≤ 4A(s)

t−1, ∀t ∈ [Ts] , ∀s ∈ [S] .

.

.

Then

(cid:104)

E

(cid:16)

F (u(S)) − F (x∗)

(cid:17)(cid:105)

A(S)
TS
(cid:16)

≤A(0)
T0

F (u(0)) − F (x∗)

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

(cid:17)

+

γ
2

2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)
t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

Proof. Note that our assumptions satisfy the requirements for Lemma A.7, by Applying Lemma A.7 and make the
telescoping sum from s = 1 to S, we obtain
(cid:104)

(cid:17)(cid:105)

(cid:16)

E

A(S)
TS

F (u(S)) − F (x∗)

20

(cid:16)

≤A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

+ E

(cid:34)

γ(s)
0
2

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)z(s)
(cid:13)

−

0 − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

+

0

(cid:13)
(cid:13)z(S+1)
(cid:13)
(cid:33)

γ(S+1)
0
2
(cid:0)a(s)(cid:1)2
16β

−

2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:16)

≤A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

+

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

(cid:16)

=A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

+

0 − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

+

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:13)
(cid:13)

γ(1)
(cid:13)
(cid:13)z(s)
0
(cid:13)
2
t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:13)u(0) − x∗(cid:13)
t − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

γ
2

+

2
(cid:13)
(cid:13)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

where we use γ(1)

0 = γ and z(1)

0 = u(0).

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

,

A.3 Bound for the residual term

We turn to bound the term

S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

This follows the standard analysis used to bound the residual term in adaptive methods. We ﬁrst admit Lemma

A.10 to give the ﬁnal bound for this term.

Lemma A.9. If X is a compact convex set with diameter D, we have

S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

Proof. It follows that

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

≤

8β (cid:0)D4 + 2η4(cid:1)
η2

S
(cid:88)

Ts(cid:88)

s=1

t=1

S
(cid:88)

Ts(cid:88)

t − γ(s)
γ(s)
2

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − γ(s)
γ(s)
2

t−1

D2 +

s=1
γ(s)
Ts

t=1
− γ(1)
0
2

D2 +

S
(cid:88)

s=1

Ts(cid:88)

t=1

+

t − x∗(cid:13)
2
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t
(cid:0)a(s)(cid:1)2
16β
(cid:0)a(s)(cid:1)2
16β

−

−

(cid:33)

(a)
≤

(b)
=

−

(cid:33)

(cid:33)

(cid:0)a(s)(cid:1)2
16β

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

γ(s)
Ts

=

− γ(1)
0
2

D2 −

D4
16β (D4 + 2η4)

a(s)(cid:17)2 (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

S
(cid:88)

Ts(cid:88)

(cid:16)

t=1

s=1
(cid:33)

η4
8β (D4 + 2η4)

(cid:16)

a(s)(cid:17)2 (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

S
(cid:88)

Ts(cid:88)

(cid:32)

+

s=1

t=1

1
2γ(s)
t

(c)
≤

8β (cid:0)D4 + 2η4(cid:1)
η2

−

where (a) is by γ(s)

t ≥ γ(s)

t−1

and

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
(cid:13) ≤ D, (b) is by noticing γ(s+1)
(cid:13)

0

= γ(s)
Ts

, (c) is by Lemma A.10.

Lemma A.10. Under our update rule of γ(s)

t

, we have

21

(cid:16)

D2
2

γ(S)
TS

− γ(1)
0

(cid:17)

−

D4
16β (D4 + 2η4)

S
(cid:88)

Ts(cid:88)

(cid:16)

s=1

t=1

a(s)(cid:17)2 (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

≤

4β (cid:0)D4 + 2η4(cid:1)
η2

S
(cid:88)

Ts(cid:88)

(cid:32)

s=1

t=1

1
2γ(s)
t

−

η4
8β (D4 + 2η4)

(cid:33)

(cid:16)

a(s)(cid:17)2 (cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

≤

4β (cid:0)D4 + 2η4(cid:1)
η2

0 , g(1)
. For k ≥ 1, assume that g(s)

1 , . . . , g(1)
T1

Proof. For simplicity, g(1)
as (γk)k≥0
can write γk = 1
η

η2γ2

k−1 + a2

(cid:113)

t

= g(2)

1 , . . . , g(2)
T2

1 , . . . , γ(2)
0 , g(2)
T2
is the element that correspond to gk, and let ak = a(s). Then we

, . . . as (gk)k≥0

1 , . . . , γ(1)
T1

0 , γ(1)

0 , γ(2)

and γ(1)

= γ(2)

, . . .

η2γ2

0 + (cid:80)k
t=1 a2
For 1). Using

t (cid:107)gt − gt−1(cid:107)2 and hence γk = 1

√

√

a + b ≤

a +

k (cid:107)gk − gk−1(cid:107)2. By writing η2γ2
(cid:113)
t=1 a2
(cid:113)(cid:80)k

η
b we have γk ≤ γ0 + 1
η

η2γ0 + (cid:80)k

k = η2γ2
t (cid:107)gt − gt−1(cid:107)2.
t=1 a2

√

t (cid:107)gt − gt−1(cid:107)2. Therefore

k−1 + a2

k (cid:107)gk − gk−1(cid:107)2 we obtain η2γ2

k =

D2
2

(γk − γ0) −

D4
16β (D4 + 2η4)

k
(cid:88)

t=1

t (cid:107)gt − gt−1(cid:107)2
a2

t (cid:107)gt − gt−1(cid:107)2 −
a2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

k
(cid:88)

D2
2η
4β (cid:0)D4 + 2η4(cid:1)
η2

t=1

≤

(a)
≤

D4
16β (D4 + 2η4)

k
(cid:88)

t=1

t (cid:107)gt − gt−1(cid:107)2
a2

where for (a) we use ax − bx2 ≤ a2
4b

.

For 2). Let τ be the last index such that γτ ≤
t (cid:107)gt − gt−1(cid:107)2 ≤ 0 for all k. Assume τ > 0
a2

η4
8β(D4+2η4)

(cid:16) 1
2γt

−

(cid:17)

t=1

(cid:80)k

4β(D4+2η4)
η4

or τ = −1 if γ0 >

4β(D4+2η4)
η4

. If τ ≤ 0 we have

(cid:18) 1
2γt

−

η4
8β (D4 + 2η4)

(cid:19)

t (cid:107)gt − gt−1(cid:107)2
a2

k
(cid:88)

t=1
τ
(cid:88)

t=1

1
2γt
τ
(cid:88)

≤

=η2

=η2

(b)
≤η2

t=1
τ
(cid:88)

t=1
τ
(cid:88)

t=1

t (cid:107)gt − gt−1(cid:107)2
a2

t − γ2
γ2
2γt

t−1

(γt − γt−1) (γt + γt−1)
2γt

(γt − γt−1)

≤η2γτ
(c)
≤

4β (cid:0)D4 + 2η4(cid:1)
η2

where (b) is due to γt−1 ≤ γt, (c) is by the deﬁnition of τ .

Finally we give an explicit choice for the parameters to satisfy all conditions and give the ﬁnal necessary bound.

A.4 Parameter choice and bound

The following lemma states the bound for the coeﬃcients.

22

Lemma A.11. Under the choice of parameters in Theorem A.1, ∀s ≥ 1, we have

and

(cid:16)

a(s)(cid:17)2

< 4A(s)
0

A(s)
Ts

≥

(cid:40)

n(4n)−0.5s
n
4c (s − s0)2

1 ≤ s ≤ s0
s0 < s

Proof. As a reminder, we choose the parameters as follows, where c = 3
2

and s = s0 = (cid:100)log2 log2 4n(cid:101)

a(s) =

(cid:40)

(4n)−0.5s
s−s0−1+c
2c

1 ≤ s ≤ s0
s0 < s

,

Ts = n,
5
4

=

.

A(0)
T0

The idea in this choice is that we divide the time into two phases in which the convergence behaves diﬀerently. In
the ﬁrst phase, A(s)
quickly gets to Ω(n) and we can set the coeﬃcients for the checkpoint relatively small. In the
Ts
(cid:113) nβ
= Ω(n2). In this phase, we need to be more conservative and
second phase, to achieve the optimal
(cid:15)
set the coeﬃcients for the checkpoint large. We analyze the two phases separately.

rate, A(s)
Ts

First we show by induction that for 1 ≤ s ≤ s0,

A(s)

0 = 1 + n

s−2
(cid:88)

(4n)−0.5k

,

A(s)
Ts

= 1 + n

k=0
s
(cid:88)

(4n)−0.5k

.

k=0

(8)

(9)

Indeed, we have

A(1)

0 = A(0)
T0

− T1

A(1)
T1

= A(1)

0 + T1

5
4

(cid:16)

a(1)(cid:17)2 (a)
=
(cid:18)
(cid:16)

a(1) +

− n(4n)−1 =

−

5
4

1
4
= 1 + n (cid:0)(4n)−0.5 + (4n)−1(cid:1) ,

= 1,

a(1)(cid:17)2(cid:19) (b)

where (a) and (b) are both by plugging in a(1) = (4n)−0.5 and T1 = n. Supposed that 8 and 9 hold for all k ≤ s < s0.
For k = s + 1 ≤ s0, we have

A(s+1)
0

− Ts+1

= A(s)
Ts
(cid:32)

(c)
=

s
(cid:88)

k=0

a(s+1)(cid:17)2
(cid:16)
(cid:33)

1 + n

(4n)−0.5k

− n(4n)−0.5s

= 1 + n

s−1
(cid:88)

(4n)−0.5k

,

k=0

A(s+1)
Ts+1

= A(s+1)
0

+ Ts+1

(cid:18)

a(s+1) +

(cid:16)

a(s+1)(cid:17)2(cid:19)

(cid:32)

1 + n

s−1
(cid:88)

(4n)−0.5k

(cid:33)

(d)
=

(cid:16)

+ n

(4n)−0.5s+1

+ (4n)−0.5s (cid:17)

k=0

= 1 + n

s+1
(cid:88)

(4n)−0.5k

,

k=0

23

(a(s))2
4

and

(10)

(11)

where (c) is by plugging a(s+1) = (4n)−0.5s+1, Ts+1 = n and the assumption on A(s)
Ts
(4n)−0.5s+1 and Ts+1 = n. Now the induction is completed. From this we can see that A(s)
A(s)
Ts

> n(4n)−0.5s.
Next, for s > s0, we show by induction that

0 ≥ 1 >

, (d) is by plugging a(s+1) =

A(s)

0 >

A(s)
Ts

>

n
2
n
2

+

+

n
4c
n
4c

(s − s0 − 2 + 2c)(s − s0 − 1) −

(s − s0 − 1 + 2c)(s − s0).

n
4c2 (s − s0 − 1 + c)2,

Indeed we have A(s0)
Ts0

= 1 + n (cid:80)s0

k=0(4n)−0.5k

> n(4n)−0.5s0 ≥ n(4n)−0.5log2 log2 4n

= n

2 . Hence

− Ts0+1

(cid:16)

a(s0+1)(cid:17)2

A(s0+1)
0

A(s0+1)
Ts0+1

(e)
≥

= A(s0)
Ts0
n
n
2
4
= A(s0+1)
0

−

,

+ Ts0+1

(cid:18)(cid:16)

a(s0+1)(cid:17)

+

(cid:16)

a(s0+1)(cid:17)2(cid:19)

(f )
≥

>

n
2
n
2

+ n

(cid:18) 1
2

+

(cid:19)

1
4

+

n
2

,

where (e) and (f ) are both by a(s0+1) = 1
2
k = s + 1 we have

, Ts0+1 = n. Supposed that 10 and 11 hold for all s0 < k ≤ s. For

A(s+1)
0

= A(s)
Ts

− Ts+1

(cid:16)

a(s+1)(cid:17)2

A(s+1)
Ts+1

+

(g)
>

n
n
2
4c
n
n
2
4c
= A(s+1)
0

=

+

(h)
>

= A(s)
Ts
n
2
n
2

=

+

+

n
4c
n
4c

(s − s0 − 1 + 2c)(s − s0) − n

(s − s0 − 1 + 2c)(s − s0) −
a(s+1)(cid:17)2(cid:19)

a(s+1) +

+ Ts+1

(cid:18)

(cid:16)

(cid:19)2

(cid:18) s − s0 + c
2c
n
4c2 (s − s0 + c)2,

+ Ts+1a(s+1)

(s − s0 − 1 + 2c)(s − s0) +

n
2c

(s − s0 + c)

(s − s0 + 2c)(s − s0 + 1),

where (g) and (h) are both due to Ts+1 = n, a(s+1) = s−s0+c
completed. We can see that if c = 3
2

. we have

2c

and the assumption on A(s)
Ts

. Now the induction is

A(s)

0 > n

= n

= n

= n

+

+

(cid:18) 1
2
(cid:18) 1
2
(cid:18) 1
2

(s − s0 − 1 + c)2
4c
(s − s0 − 1 + c)2
12c
(s − s0 − 1 + c)2
16c2
(cid:18) (s − s0 − 1 + c)2
16c2

+

+

(cid:18)

1 −

(cid:19)

1
c

−

(cid:19)

s − s0 − 1 + c2
4c
(cid:19)

−

+

s − s0 − 1 + c2
4c
(s − s0 − 1 + c)2
24c

−

s − s0 − 1 + c2
4c

(cid:19)

(s − s0 − 1)2 − 3(s − s0 − 1) + (c2 − 6c2 + 12c)
24c

(cid:19)

>

(s − s0 − 1 + c)2
16c2

=

(cid:0)a(s)(cid:1)2
4

and A(s)
Ts

> n

4c (s − s0)2.

24

A.5 Putting all together

We are now ready to put everything together and complete the proof of Theorem A.1.

Proof. (Theorem A.1) From Lemma A.11, we know(cid:0)a(s)(cid:1)2
t ∈ [Ts]

< 4A(s)
0

for any s ≥ 1, which implies for any s ≥ 1,

a(s)(cid:17)2
Combining our parameters, we can ﬁnd the requirements for Lemma A.8 are satisﬁed, which will give us

< 4A(s)

t−1.

(cid:16)

(cid:104)

E

A(S)
TS

(cid:16)

F (u(S)) − F (x∗)

(cid:17)(cid:105)

(cid:16)

≤ A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

+

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

By using Lemma A.9, we know

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)
γ
2
t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)x(s)
(cid:13)

2
(cid:13)
(cid:13)
(cid:32) (cid:0)a(s)(cid:1)2
2γ(s)
t

(cid:33)

(cid:0)a(s)(cid:1)2
16β

−

(cid:13)
(cid:13)g(s)
(cid:13)

t − g(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

E

(cid:104)
A(S)
TS

(cid:16)

F (u(S)) − F (x∗)

(cid:17)(cid:105)

(cid:16)

≤ A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

(a)
≤

(cid:16)

5
4

F (u(0)) − F (x∗)

(cid:17)

+

+

γ
2

⇒ E

(cid:104)

(cid:105)
F (u(S)) − F (x∗)

≤

(b)
≤

V
2A(S)
TS
(cid:40) 2V

(4n)1−0.5S
2cV
n(S−s0)2

1 ≤ S ≤ s0

s0 < S

where (a) is by plugging in A(0)
T0

= 5
4

, (b) is by A.11.

2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

γ
2
(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)
(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+

8β (cid:0)D4 + 2η4(cid:1)
η2

+

8β (cid:0)D4 + 2η4(cid:1)
η2

• If (cid:15) ≥ V
n

, we choose S = (cid:6)log2 log2

4V
(cid:15)

(cid:7) ≤ (cid:100)log2 log2 4n(cid:101) = s0, so we have
(cid:105)
(cid:104)

F (u(S)) − F (x∗)

≤

E

2V
(4n)1−0.5S
2V
(cid:1) 1−0.5S
(cid:15)
(cid:1) −0.5S

(cid:0) 4V
(cid:15)

2 (cid:0) 4V

(cid:15)

(c)
≤

=

where (c) is by n ≥ V
. Note that the
(cid:15)
ﬁnal full gradient computation in the last epoch is not needed, therefore the number of individual gradient
evaluations is

(cid:15) = 1
2

(cid:15)

(cid:15)

(cid:15)

, (d) is by (cid:0) 4V

= (cid:0) 4V

≥ (cid:0) 4V

(cid:1) −0.5S

(cid:1) −0.5log2 log2

4V

(cid:1) −0.5(cid:100)log2 log2

4V

(cid:15) (cid:101)

(d)
≤ (cid:15),

#grads = n +

S−1
(cid:88)

s=1

(2(Ts − 1) + n) + 2(TS − 1)

< 3nS
(cid:24)
log2 log2

= 3n

(cid:18)

= O

n log log

(cid:25)

.

4V
(cid:15)
(cid:19)

V
(cid:15)

• If (cid:15) < V
n

, we choose S = s0 +

(cid:108)(cid:113) 2cV
n(cid:15)

(cid:109)

≥ s0 + 1, so we have

(cid:104)

(cid:105)
F (u(S)) − F (x∗)

E

≤

2cV
n(S − s0)2

25

2cV
(cid:16)(cid:108)(cid:113) 2cV
n(cid:15)

(cid:109)(cid:17)2

2cV
(cid:16)(cid:113) 2cV
n(cid:15)

(cid:17)2

=

≤

n

n

= (cid:15).

The number of individual gradient evaluations is

#grads = n +

S−1
(cid:88)

s=1

(2(Ts − 1) + n) + 2(TS − 1)

< 3nS
= 3ns0 + 3n(S − s0)

(cid:38)(cid:114)

= 3n (cid:100)log2 log2 4n(cid:101) + 3n

(cid:32)

= O

n log log n +

(cid:114)

nV (z)
(cid:15)

(cid:39)

2cV
n(cid:15)
(cid:33)

.

B Analysis of algorithm 2

In this section, we analyze Algorithm 2 and prove the following convergence guarantee:

√
Theorem B.1. (Convergence of AdaVRAG) Deﬁne s0 = (cid:100)log2 log2 4n(cid:101), c = 3+
of Algorithm 2 as follows:

4

33

. Suppose we set the parameters

a(s) =

q(s) =

(cid:40)

1 − (4n)−0.5s

c
s−s0+2c

1
(1−a(s))a(s)
8(2−a(s))a(s)
3(1−a(s))






1 ≤ s ≤ s0
s0 < s

,

1 ≤ s ≤ s0

,

s0 < s

Ts = n.

Suppose that X is a compact convex set with diameter D and we set η = Θ(D). Addtionally, we assume that
2η2 > D2 if Option I is used for setting the step size. The number of individual gradient evaluations to achieve a
solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for Algorithm 2 is

#grads =






(cid:1)

O (cid:0)n log log V
O

n log log n +

(cid:16)

(cid:15)

(cid:17)

(cid:113) nV
(cid:15)

(cid:15) ≥ V
n
(cid:15) < V
n

,

where

V =






1

2 (F (u(0)) − F (x∗)) + γ (cid:13)
2 (F (u(0)) − F (x∗)) + γ (cid:13)

1

(cid:13)u(0) − x∗(cid:13)
2
(cid:13)

(cid:13)u(0) − x∗(cid:13)
2
(cid:13)

(cid:104)
β −

+

(cid:16)
1 − D2
2η2

(cid:17)

γ

(cid:105)+ (cid:32)

D2 + 2(η2 + D2) log

(cid:33)

2η2β
2η2−D2
γ

+ η2 (cid:16) D2

η2 + β − γ

(cid:17)+ (cid:16) 2D2

η2 + β − γ

(cid:17)

for Option I

.

for Option II

B.1 Single epoch progress and ﬁnal output

We ﬁrst analyze the progress in function value made in a single iteration of an epoch. The analysis is done in a
standard way by combining the smoothness and convexity of f , the convexity of h and the optimality condition of
x(s)
t

.

26

Lemma B.2. For all epochs s ≥ 1 and all iterations t ∈ [Ts], we have

E

(cid:104)

F (x(s)

(cid:105)
t ) − F (x∗)

≤ E

(cid:104)(cid:16)

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

1 − a(s)(cid:17) (cid:16)
(cid:34)
γ(s)
t−1q(s)a(s)
2

+ E

+ E

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:34)(cid:32)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

Proof. We have

E

(a)
≤ E

t ) − f (x(s)

(cid:105)
t−1)

(cid:104)

f (x(s)
(cid:20)(cid:68)

∇f (x(s)

t−1), x(s)

t − x(s)

t−1

(cid:69)

+

β
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:68)

g(s)
t

=E

, x(s)

t − x(s)

t−1

(cid:69)

+

(cid:68)
∇f (x(s)

t−1) − g(s)

t

, x(s)

t − x(s)

t−1

(cid:69)

+

β
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

,

where (a) is due to f being β-smooth. Using Cauchy–Schwarz inequality and Young’s inequality (ab ≤ λ
with λ > 0) we have

2 a2 + 1

2λ b2

(cid:69)

t

t−1

(cid:68)
∇f (x(s)

t − x(s)
, x(s)
t−1) − g(s)
(cid:13)
(cid:13)
(cid:13)
(cid:13)x(s)
t−1) − g(s)
(cid:13)∇f (x(s)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 − a(s)
(cid:13)
(cid:13)∇f (x(s)
(cid:13)
2β

t − x(s)
(cid:13)
2
(cid:13)
(cid:13)

t−1) − g(s)

+

t

t

t−1

(cid:13)
(cid:13)
(cid:13)

β
2(1 − a(s))

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

,

≤

≤

also note that

t − x(s)
x(s)

t−1 =

(cid:16)

a(s)x(s)

t + (1 − a(s))u(s−1)(cid:17)

−

(cid:16)

a(s)x(s)

t−1 + (1 − a(s))u(s−1)(cid:17)

= a(s) (cid:16)

t − x(s)
x(s)

t−1

(cid:17)

.

Hence, we obtain

t ) − f (x(s)

(cid:105)
t−1)

E

≤E

(cid:104)

f (x(s)
(cid:34)

(cid:68)
g(s)
t

, a(s) (cid:16)

t − x(s)
x(s)

t−1

(cid:17)(cid:69)

+

(cid:104)(cid:68)

(b)
≤E

+ E

g(s)
t
(cid:34)

, a(s) (cid:16)
t − x(s)
x(s)
β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

t−1

=E

(cid:104)(cid:68)

, a(s) (cid:16)

g(s)
t

t − x∗(cid:17)(cid:69)
x(s)

(cid:17)(cid:69)

+

1 − a(s)
2β
1 − a(s)(cid:17) (cid:16)
(cid:16)
2(cid:35)
(cid:13)
(cid:13)
(cid:13)
, a(s) (cid:16)

t−1

t − x(s)

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:68)
g(s)
t

+

+ E

(cid:104)(cid:16)

1 − a(s)(cid:17) (cid:16)

f (u(s−1)) − f (x(s)

t−1)

(cid:17)(cid:105)

+ E

(c)
= E

(cid:104)(cid:68)

g(s)
t

, a(s) (cid:16)

t − x∗(cid:17)(cid:69)
x(s)

+

(cid:68)
∇f (x(s)

+ E

(cid:104)(cid:16)

1 − a(s)(cid:17) (cid:16)

f (u(s−1)) − f (x(s)

t−1)

t−1), a(s) (cid:16)
(cid:34)

(cid:17)(cid:105)

+ E

(d)
= E

(cid:104)(cid:68)

g(s)
t

, a(s) (cid:16)

t − x∗(cid:17)(cid:69)
x(s)

+

(cid:68)
∇f (x(s)

t−1), a(s) (cid:16)

(cid:13)
(cid:13)∇f (x(s)
(cid:13)

t−1) − g(s)

t

(cid:13)
2
(cid:13)
(cid:13)

+

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

f (u(s−1)) − f (x(s)

t−1) − (cid:104)∇f (x(s)

t−1), u(s−1) − x(s)

t−1(cid:105)

(cid:17)(cid:105)

(cid:17)(cid:69)

(cid:68)
∇f (x(s)

t−1),

(cid:16)

−

1 − a(s)(cid:17) (cid:16)

x∗ − x(s)
t−1
(cid:34)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)
(cid:16)
(cid:17)

(cid:13)
(cid:13)x(s)
(cid:13)
1 − a(s)(cid:17) (cid:16)

x∗ − x(s)
t−1

−

t − x(s)

t−1

u(s−1) − x(s)
t−1

(cid:17)(cid:69)(cid:105)

(cid:17)(cid:69)(cid:105)

u(s−1) − x(s)
t−1
2(cid:35)
(cid:13)
(cid:13)
(cid:13)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)
(cid:17)(cid:69)

(cid:16)

x∗ − x(s)
t−1

(cid:13)
(cid:13)x(s)
(cid:13)
1 − a(s)(cid:17) (cid:16)

+

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

f (u(s−1)) − f (x(s)

t−1)

(cid:17)(cid:105)

27

+ E

(cid:34)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

(cid:34)

(e)
≤ E

(cid:68)
g(s)
t

, a(s) (cid:16)

t − x∗(cid:17)(cid:69)
x(s)

+

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

(cid:16)

1 − a(s)(cid:17)

+

f (u(s−1)) + a(s)f (x∗) − f (x(s)

t−1)

,

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:105)

where (b) is by Lemma A.2, (c) is because of

(cid:104)(cid:68)

E

, a(s) (cid:16)

g(s)
t

x∗ − x(s)
t−1

(cid:17)(cid:69)(cid:105)

= E

(cid:104)(cid:68)

∇f (x(s)

t−1), a(s) (cid:16)

x∗ − x(s)
t−1

(12)

(cid:17)(cid:69)(cid:105)

,

(d) is by x(s)

t−1 = a(s)x(s)

t−1 + (cid:0)1 − a(s)(cid:1) u(s−1), (e) is due to the convexity of f which implies
(cid:17)(cid:69)

(cid:68)
∇f (x(s)

t−1), a(s) (cid:16)

x∗ − x(s)
t−1

≤ a(s) (cid:16)

f (x∗) − f (x(s)

(cid:17)
t−1)

.

By adding E

(cid:104)

f (x(s)

(cid:105)
t−1) − f (x∗)

to both sides of (12), we obtain

(cid:105)
t ) − f (x∗)

E

≤E

(cid:104)

f (x(s)
(cid:34)

(cid:68)
g(s)
t

, a(s) (cid:16)

t − x∗(cid:17)(cid:69)
x(s)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

+

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

+

1 − a(s)(cid:17) (cid:16)
(cid:16)

f (u(s−1)) − f (x∗)

(cid:35)

(cid:17)

.

(13)

Next, we upper bound the inner product

(cid:68)
g(s)
t

, a(s) (cid:16)

t − x∗(cid:17)(cid:69)
x(s)

. By the optimality condition of x(s)

t

, we have

(cid:68)

t + h(cid:48)(x(s)
g(s)

t ) + γ(s)

t−1q(s) (cid:16)

t − x(s)
x(s)

t−1

(cid:17)

, x(s)

t − x∗(cid:69)

≤ 0,

where h(cid:48)(x(s)

t ) is a subgradient of h at x(s)

t

. We rearrange the above inequality and obtain

t ) ∈ ∂h(x(s)
a(s) (cid:68)
≤a(s) (cid:68)

g(s)
t

, x(s)

t − x∗(cid:69)
t ) + γ(s)

h(cid:48)(x(s)

(f )

≤ a(s) (cid:16)

h(x∗) − h(x(s)
t )

t−1q(s) (cid:16)
(cid:17)

(g)

= a(s) (cid:16)

h(x∗) − h(x(s)
t )

(cid:17)

+

t − x(s)
x(s)

t−1

(cid:17)

, x∗ − x(s)

t

(cid:69)

+ a(s)γ(s)

t−1q(s) (cid:68)
a(s)γ(s)
(cid:18)(cid:13)
t−1q(s)
(cid:13)x(s)
(cid:13)
2

t − x(s)
x(s)
t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

t−1, x∗ − x(s)

t

(cid:69)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x(s)

t

2(cid:19)

(cid:13)
(cid:13)
(cid:13)

,

(14)

where (f ) follows from the convexity of h and the fact that h(cid:48)(x(s)
.
(cid:104)a, b(cid:105) = 1
2

(cid:107)a + b(cid:107)2 − (cid:107)a(cid:107)2 − (cid:107)b(cid:107)2(cid:17)
We plug in (14) into (13), and obtain

(cid:16)

t ) ∈ ∂h(x(s)

t ), and (g) is due to the identity

E

≤E

(cid:105)
t ) − f (x∗)

(cid:104)

f (x(s)
1 − a(s)(cid:17) (cid:16)
(cid:104)(cid:16)
(cid:34)

γ(s)
t−1q(s)a(s)
2

+ E

1 − a(s)(cid:17) (cid:16)
(cid:34)

γ(s)
t−1q(s)a(s)
2

(cid:104)(cid:16)

(h)
= E

+ E

(cid:104)(cid:16)

(i)
≤E

f (u(s−1)) − f (x∗)

(cid:17)

+ a(s) (cid:16)

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
(cid:13)
(cid:13)

F (u(s−1)) − F (x∗)

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:17)(cid:105)

+

2(cid:19)

h(x∗) − h(x(s)
t )
(cid:32)
β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)
1 − a(s)(cid:17)
(cid:16)
β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

2(cid:19)

+

+ h(x∗) − a(s)h(x(s)
t ) −
(cid:32)

−

γ(s)
t−1q(s)a(s)
2

(cid:105)
h(u(s−1))

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

(cid:17)

(cid:17)

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

+ h(x∗) − h(x(s)
t )

(cid:105)

28

(cid:34)

+ E

γ(s)
t−1q(s)a(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:32)

+

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

where (h) is by the deﬁnition of F = f + h, and (i) is by the convexity of h which implies

h(x(s)

t ) = h

(cid:16)

a(s)x(s)

t + (1 − a(s))u(s−1)(cid:17)

≤ a(s)h(x(s)

t ) + (1 − a(s))h(u(s−1)).

Now we move the term E

(cid:104)
(cid:105)
h(x∗) − h(x(s)
t )

to the LHS, and obtain

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

,

E

≤E

(cid:105)
t ) − F (x∗)

(cid:104)

F (x(s)
(cid:34)

(cid:16)

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

(cid:17)

+

(cid:34)(cid:32)

+ E

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

γ(s)
t−1q(s)a(s)
2
(cid:33)

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)
2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

−

By Lemma B.2, if 1
Ts

(cid:80)Ts

t=1 x(s)

t

is deﬁned as a new chekpoint like what we do in Algorithm 2, the following

guarantee for the function value progress in one epoch comes up immediately by the convexity of F .

Lemma B.3. For all epochs s ≥ 1, we have

(cid:104)

E

F (u(s)) − F (x∗)

(cid:105)

≤ E

(cid:104)(cid:16)

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

1 − a(s)(cid:17) (cid:16)
(cid:34)

Ts(cid:88)

t=1

Ts(cid:88)

t=1

+ E

1
Ts

γ(s)
t−1q(s)a(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

+ E

(cid:34)

1
Ts

Proof. We have

(cid:104)

(cid:105)
F (u(s)) − F (x∗)
(cid:34)

F (x(s)

t ) − F (x∗)

(cid:35)

(cid:17)

E

(a)
≤ E

(b)
≤E

Ts(cid:88)

(cid:16)

t=1

1
Ts
1 − a(s)(cid:17) (cid:16)
(cid:34)

(cid:104)(cid:16)

+ E

1
Ts

+ E

(cid:34)

1
Ts

Ts(cid:88)

t=1

Ts(cid:88)

t=1

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

γ(s)
t−1q(s)a(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

,

where (a) is by the convexity of F and the deﬁnition of u(s) = 1
Ts

(cid:80)Ts

t=1 x(s)

t

, and (b) is by Lemma B.2.

Lemma A.8 is a quite general result without any assumptions on any parameters. To ensure that we can make
the telescoping sum over the function value part, and also to simplify the term besides the function value part, we
need some speciﬁc conditions on our parameters to be satisﬁed, which is stated in Lemma B.4. With these extra
conditions, we can ﬁnally ﬁnd the following guarantee for the function value gap of the ﬁnal output u(S).

Lemma B.4. For all S ≥ 1, if the parameters satisfy

(cid:0)2 − a(s)(cid:1) a(s)
1 − a(s)

≤ q(s), ∀s ∈ [S]

29

and

then we have

(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤

Ts
q(s)a(s)

, ∀s ∈ [S − 1] .

(cid:21)
(F (u(S)) − F (x∗))

(F (u(0)) − F (x∗))

≤

(cid:20)

E

TS
q(S)a(S)
(1 − a(1))T1
q(1)a(1)
(cid:34) S
(cid:88)

+ E

Ts(cid:88)

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

Proof. If (2−a(s))a(s)

1−a(s) ≤ q(s) for any s ∈ [S], by using Lemma B.3, we know

E

≤E

(cid:105)

(cid:104)
F (u(s)) − F (x∗)
1 − a(s)(cid:17) (cid:16)
(cid:104)(cid:16)
(cid:34)

+ E

1
Ts

+ E

(cid:34)

1
Ts

Ts(cid:88)

t=1

Ts(cid:88)

t=1

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

γ(s)
t−1q(s)a(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

≤E

(cid:104)
(1 − a(s))(F (u(s−1)) − F (x∗))
(cid:32) Ts(cid:88)

(cid:34)

(cid:105)

+ E

q(s)a(s)
Ts

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:33)(cid:35)
(cid:13)
(cid:13)
(cid:13)

Now multiply both sides by

Ts
q(s)a(s)

, we have

(cid:21)
(F (u(s−1)) − F (x∗))

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

If (1−a(s+1))Ts+1

q(s+1)a(s+1) ≤ Ts

is satisﬁed for any s ∈ [S − 1], we can make the telescoping sum from s = 1 to S to get

(F (u(s)) − F (x∗))

(cid:21)

E

≤E

(cid:20)

Ts
q(s)a(s)
(cid:20) (1 − a(s))Ts
q(s)a(s)
(cid:34) Ts(cid:88)
γ(s)
t−1
2

t=1

+ E

q(s)a(s)
(cid:20)

E

TS
q(S)a(S)
(1 − a(1))T1
q(1)a(1)
(cid:34) S
(cid:88)

+ E

Ts(cid:88)

s=1

t=1

(cid:21)
(F (u(S)) − F (x∗))

≤

(F (u(0)) − F (x∗))

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

B.2 Bound for the residual term

By the analysis in the previous subsection, we get an upper bound for the function value gap of u(S) involving
F (u(0)) − F (x∗) and

(cid:34) S
(cid:88)

Ts(cid:88)

E

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

30

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

(15)

In this subsection we will show how to bound 15 under the compact assumption of X . Before giving the detailed
analysis of the two diﬀerent update options, we ﬁrst state the following lemma to simplify 15.

Lemma B.5. If γ(s)

t−1 for any s ∈ [S], t ∈ [Ts] and X is a compact convex set with diameter D, then we have

t ≥ γ(s)
(cid:34) S
(cid:88)

E

Ts(cid:88)

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

≤

γ
2

Proof. It follows that

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

S
(cid:88)

Ts(cid:88)

s=1

t=1

S
(cid:88)

Ts(cid:88)

s=1

t=1

S
(cid:88)

Ts(cid:88)

t=1
(cid:32)

=

(a)
≤

=

(b)
=

=

≤

s=1

S
(cid:88)

s=1

S
(cid:88)

s=1
γ(1)
0
2

γ(1)
0
2

γ(s)
t−1
2

γ(s)
t−1
2

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

γ(s)
t
2

γ(s)
t
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

t − γ(s)
γ(s)
2

t−1

t − γ(s)
γ(s)
2

t−1

D2 +

γ(s)
0
2

(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
Ts
2

(cid:13)
(cid:13)x(s)
(cid:13)

Ts

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(cid:32)

γ(s)
0
2

(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s+1)
0
2

(cid:13)
(cid:13)x(s+1)
(cid:13)

0

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

Ts(cid:88)

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(1)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(1)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

+

γ(S+1)
0
2

S
(cid:88)

Ts(cid:88)

s=1

(cid:13)
(cid:13)x(S+1)
(cid:13)

0

− x∗(cid:13)
2
(cid:13)
(cid:13)

+

t − γ(s)
γ(s)
2

t−1

D2 +

S
(cid:88)

Ts(cid:88)

s=1
t=1
β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

(c)
=

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+

S
(cid:88)

s=1

Ts(cid:88)

t=1

where (a) is due to γ(s)
t ≥ γ(s)
(c) is by the deﬁnition of x(1)

t−1

and

(cid:13)
(cid:13)x(s)
(cid:13)
0 = u(0) and γ(1)

t−1

D2 +

t=1
t − γ(s)
γ(s)
2
t − x∗(cid:13)
(cid:13) ≤ D, (b) follows from the deﬁnition of x(s+1)
(cid:13)

β − γ(s)
t−1
2

t − x(s)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

t−1

0

,

0 = γ. Now Taking expectations with both sides yields what we want.

= x(s)
Ts

and γ(s+1)
0

= γ(s)
Ts

,

With the above result, we can show the bound of 15 under Option I and Option II respectively. There are two
(cid:111)

key common parts in our analysis, the ﬁrst one is to notice that we can reduce the doubly indexed sequence

(cid:110)

x(s)
t

and

(cid:111)

(cid:110)

γ(s)
t

into two singly indexed sequences, which are much easier to bound. The second technique is to deﬁne

a hitting time τ to upper bound γ(s)

t

. Read our proof for the details.

Lemma B.6. For Option I, if X is a compact convex set with diameter D and 2η2 > D2, we have

(cid:34) S
(cid:88)

Ts(cid:88)

E

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

≤

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+

(cid:20) β
2

−

(cid:18) 1
2

−

(cid:19)

γ

D2
4η2

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:21)+ 

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

D2 + 2(η2 + D2) log

2η2β
2η2−D2
γ



 .

Proof. For Option I, by the deﬁnition of γ(s)

t

, we have

t ≥ γ(s)
γ(s)

t−1, ∀s ∈ [S] , t ∈ [Ts] .

31

By requiring that X is a compact convex set with diameter D, we can apply Lemma B.5 and obtain

(cid:34) S
(cid:88)

Ts(cid:88)

E

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

≤

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

(16)

Note that the last element x(s)
Ts

(resp. γ(s)
Ts
the (s + 1)-th epoch, which means we can consider the doubly indexed sequences {x(s)
indexed sequences {x(cid:48)

) in the s-th epoch is just the start element x(s+1)

0 = γ} with the reformulated update rule as follows

t, t ≥ 0} and {γ(cid:48)

t } and {γ(s)

t, t ≥ 0, γ(cid:48)

0

(resp. γ(s+1)
) in
0
t } as two singly

(cid:115)

t = γ(cid:48)
γ(cid:48)

t−1

1 +

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)
η2

t−1

(cid:13)
2
(cid:13)

.

Besides, by deﬁning T (cid:48) = (cid:80)S

s=1 Ts, we have

S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

=

T (cid:48)
(cid:88)

t=1

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

.

Note that we require 2η2 > D2, so if γ ≥ 2η2β

2η2−D2 ⇔ β

2 −

the reformulated update rule, we have

(cid:16) 1
2 − D2

4η2

(cid:17)

γ ≤ 0 ⇒ β

2 −

(cid:16) 1
2 − D2

4η2

(cid:17)

t−1 ≤ 0, by using
γ(cid:48)

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(γ(cid:48)

t)2 − (γ(cid:48)
t + γ(cid:48)
2(γ(cid:48)

t−1)2
t−1)

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(γ(cid:48)
2η2(γ(cid:48)

t−1)2D2
t + γ(cid:48)

t−1)

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

+

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:18) γ(cid:48)

t−1

4η2 D2 +
(cid:18) 1
2

−

−

(cid:20) β
2

(cid:19)

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:19)

D2
4η2

(cid:21)

γ(cid:48)
t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1

=

=

(a)
≤

=

≤0,

where (a) is by γ(cid:48)

t ≥ γ(cid:48)

t−1

. Now we assume γ < 2η2β

2η2−D2 , deﬁne

(cid:26)

τ = max

t ∈ [T (cid:48)], γ(cid:48)

t−1 <

2η2β
2η2 − D2

(cid:27)

.

By our assumption on γ, we know τ ≥ 1, Combining the reformulated update rule, we have

T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1
τ
(cid:88)

t=1

≤

≤

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:20) β
2
(cid:20) β
2

−

−

(cid:18) 1
2
(cid:18) 1
2

−

−

(cid:19)

(cid:19)

D2
4η2

D2
4η2

(cid:21)

(cid:21)

γ(cid:48)
t−1

γ(cid:48)
t−1

(cid:13)
2
(cid:13)

(cid:13)
2
(cid:13)

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

32

(b)
≤

(c)
≤

(d)
=

(e)
≤

(f )
≤

=

(g)
≤

(cid:20) β
2

(cid:20) β
2

(cid:20) β
2

(cid:20) β
2

(cid:20) β
2
(cid:20) β
2

(cid:20) β
2

−

−

−

−

−

−

−

(cid:18) 1
2

(cid:18) 1
2

(cid:18) 1
2

(cid:18) 1
2

(cid:18) 1
2
(cid:18) 1
2

(cid:18) 1
2

−

−

−

−

−

−

−

D2
4η2

D2
4η2

D2
4η2

D2
4η2

D2
4η2

D2
4η2

D2
4η2

(cid:21) τ

(cid:88)

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:19)

(cid:19)

γ

γ

t=1
(cid:21) (cid:32)

D2 +

(cid:19)

(cid:21) (cid:32)

γ

D2 +

τ −1
(cid:88)

t=1

τ −1
(cid:88)

t=1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:33)

(cid:13)
2
(cid:13)

(cid:33)

η2 (γ(cid:48)

t)2 − (γ(cid:48)
(γ(cid:48)
t−1)2

t−1)2

(cid:19)

(cid:21) (cid:32)

γ

(cid:19)

(cid:21) (cid:32)

γ

γ

(cid:19)

D2 + (cid:0)η2 + D2(cid:1)

τ −1
(cid:88)

t=1

(γ(cid:48)

t)2 − (γ(cid:48)
(γ(cid:48)
t)2

t−1)2

(cid:33)

D2 + 2 (cid:0)η2 + D2(cid:1)

τ −1
(cid:88)

t=1

log

(cid:21) (cid:18)

D2 + 2 (cid:0)η2 + D2(cid:1) log

(cid:19)

(cid:21)

γ


D2 + 2 (cid:0)η2 + D2(cid:1) log

(cid:33)

γ(cid:48)
t
γ(cid:48)
t−1
(cid:19)

γ(cid:48)
τ −1
γ
2η2β
2η2−D2
γ



 ,

where (b) is by γ(cid:48)

t−1 ≥ γ, (c) is by (cid:13)
(cid:13)x(cid:48)

τ − x(cid:48)

τ −1

(cid:13)
(cid:13) ≤ D, (d) is by the reformulated update rule, (e) is due to

(cid:115)

t = γ(cid:48)
γ(cid:48)

t−1

1 +

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)
η2

t−1

(cid:13)
2
(cid:13)

(cid:115)

≤ γ(cid:48)

t−1

1 +

D2
η2 ,

(f ) is by the inequality 1 − 1

x2 ≤ log x2 = 2 log x, (g) is by the deﬁnition of τ .

Combining two cases of γ, we obtain the bound

S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

=

≤

T (cid:48)
(cid:88)

t=1
(cid:20) β
2

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

−

(cid:18) 1
2

−

(cid:19)

γ

D2
4η2

β − γ(cid:48)
2
(cid:21)+ 

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

D2 + 2 (cid:0)η2 + D2(cid:1) log

2η2β
2η2−D2
γ


 . (17)

By plugging in (17) into (16), we have

(cid:34) S
(cid:88)

Ts(cid:88)

E

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

≤

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+

(cid:20) β
2

−

(cid:18) 1
2

−

(cid:19)

γ

D2
4η2

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:21)+ 

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

D2 + 2 (cid:0)η2 + D2(cid:1) log

2η2β
2η2−D2
γ



 .

Lemma B.7. For Option II, if X is a compact set with diameter D, we have

(cid:34) S
(cid:88)

Ts(cid:88)

E

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

t=1

s=1
(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

≤

γ
2

(cid:18) D2

+

η2
2

η2 + β − γ

η2 + β − γ

(cid:19)

.

t − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

γ(s)
t−1
2
(cid:19)+ (cid:18) 2D2

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

Proof. For Option II, by the deﬁnition of γ(s)

t

, we have

γ(s)
t ≥ γ(s)

t−1, ∀s ∈ [S] , t ∈ [Ts] .

33

By requiring that X is a compact convex set with diameter D, we can apply Lemma B.5 and obtain

(cid:34) S
(cid:88)

Ts(cid:88)

E

s=1

t=1

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x∗(cid:13)
2
(cid:13)
(cid:13)

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

≤

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

+ E

(cid:34) S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

.

(18)

Note that the last element x(s)
Ts

(resp. γ(s)
Ts
in the (s + 1)-th epoch, which means we can consider the doubly indexed sequences {x(s)
indexed sequences {x(cid:48)

) in the s-th epoch is just the starting element x(s+1)
t } and {γ(s)

0 = γ} with the reformulated update rule as follows

t, t ≥ 0} and {γ(cid:48)

t, t ≥ 0, γ(cid:48)

0

(resp. γ(s+1)
)
t } as two singly

0

t = γ(cid:48)
γ(cid:48)

t−1 +

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)
η2

t−1

(cid:13)
2
(cid:13)

.

Besides, by deﬁning T (cid:48) = (cid:80)S

s=1 Ts, we have

S
(cid:88)

Ts(cid:88)

s=1

t=1

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

=

T (cid:48)
(cid:88)

t=1

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

.

If γ ≥ D2

η2 + β ⇔ D2

2η2 + β−γ

2 ≤ 0 ⇒ D2

2η2 +

β−γ(cid:48)
2

t−1

≤ 0, by using the reformulated update rule, we have

T (cid:48)
(cid:88)

t=1

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

Now we assume γ < D2

η2 + β. Deﬁne

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:18) D2

2η2 +

β − γ(cid:48)
2

t−1

(cid:19)

(cid:13)
2
(cid:13)

=

T (cid:48)
(cid:88)

t=1

≤ 0.

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

By our assumption on γ, we know τ ≥ 1. Combining the reformulated update rule, we have

(cid:26)

τ = max

t ∈ [T (cid:48)], γ(cid:48)

t−1 <

D2
η2 + β

(cid:27)

.

T (cid:48)
(cid:88)

t=1
T (cid:48)
(cid:88)

t=1
τ
(cid:88)

t=1
τ
(cid:88)

t=1
τ
(cid:88)

=

≤

(a)
≤

(b)
=

=

(c)
=

(d)
≤

t − γ(cid:48)
γ(cid:48)
2

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:18) D2

2η2 +

β − γ(cid:48)
2

t−1

(cid:18) D2

2η2 +

β − γ(cid:48)
2

t−1

(cid:19)

(cid:19)

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

(cid:13)
2
(cid:13)

(cid:19)

(cid:19)

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

η2 (cid:0)γ(cid:48)

t − γ(cid:48)

t−1

(cid:1)

η2 (γ(cid:48)

τ − γ)

(cid:18) D2

2η2 +

β − γ
2

β − γ
2
(cid:19)

(cid:18) D2

2η2 +

t=1
(cid:18) D2

2η2 +

β − γ
2

(cid:18) D2

2η2 +

(cid:18) D2

2η2 +

β − γ
2

β − γ
2

(cid:19)

(cid:32)

η2

γ(cid:48)
τ −1 +

(cid:13)
(cid:13)x(cid:48)

τ −1

(cid:13)
2
(cid:13)

(cid:33)

− γ

τ − x(cid:48)
η2
(cid:19)

(cid:19)

(cid:18)
2

η2

D2
η2 + β − γ

34

=

η2
2

(cid:18) D2

η2 + β − γ

(cid:19) (cid:18) 2D2

η2 + β − γ

(cid:19)

,

where (a) is by the fact γ(cid:48)
(cid:13)
(cid:13)
(cid:13)x(cid:48)
(cid:13) ≤ D.

τ − x(cid:48)
Combining two cases of γ, we obtain the bound

τ −1

t−1 ≥ γ, (b) and (c) are by the reformulated update rule, (d) is by the deﬁnition of τ and

S
(cid:88)

Ts(cid:88)

t − γ(s)
γ(s)
2

t−1

D2 +

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

(cid:13)
2
(cid:13)
(cid:13)

t−1

D2 +

β − γ(cid:48)
2

t−1

(cid:13)
(cid:13)x(cid:48)

t − x(cid:48)

t−1

(cid:13)
2
(cid:13)

t=1
t − γ(cid:48)
γ(cid:48)
2

s=1
T (cid:48)
(cid:88)

t=1
η2
2

=

≤

(cid:18) D2

η2 + β − γ

(cid:19)+ (cid:18) 2D2

η2 + β − γ

(cid:19)

.

(19)

By plugging in (19) into (18), we have

(cid:34) S
(cid:88)

Ts(cid:88)

E

γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

+

β − γ(s)
t−1
2

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

t=1

s=1
(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

≤

γ
2

(cid:18) D2

+

η2
2

η2 + β − γ

η2 + β − γ

(cid:19)

.

t − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

γ(s)
t−1
2
(cid:19)+ (cid:18) 2D2

B.3 Parameter bound

Combining the previous two parts analysis on the function value gap and the residual term, we already can see the
bound for F (u(S)) − F (x∗). However, we need to make sure that our choice stated in Theorem B.1 indeed satisﬁes
the conditions used in previous lemmas, besides, we also need to give the bounds for our choice explicitly. The
following two lemmas can help us to do this.

Lemma B.8. Under the choice of parameters in Theorem B.1, ∀s ≥ 1, we have the following facts

a(s0) ≤

1
2

,

(cid:0)2 − a(s)(cid:1) a(s)
1 − a(s)
(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤ q(s),

≤

Ts
q(s)a(s)

.

Proof. Under the choice of parameters in Theorem B.1, the ﬁrst inequality follows that

a(s0) = 1 − (4n)−0.5s0

≤ 1 − (4n)−0.5log2 log2 4n

=

1
2

.

For the second inequality, note that

(cid:0)2 − a(s)(cid:1) a(s)
(cid:0)1 − a(s)(cid:1) q(s)

=

(cid:40)(cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2

3
8

1 ≤ s ≤ s0
s0 < s

.

By noticing (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2

1−a(s) ≤ q(s) becomes true immediately.
For the third inequality, note that we have Ts ≡ n, we only need to prove for any s ≥ 1, there is

≤ a(s) ≤ 1, the inequality (2−a(s))a(s)

We consider the following three cases:

1 − a(s+1)
q(s+1)a(s+1)

≤

1
q(s)a(s)

.

35

• For 1 ≤ s ≤ s0 − 1, note that (cid:0)1 − a(s+1)(cid:1)2

= (4n)−0.5s

= 1 − a(s), q(s) =

1
(1−a(s))a(s)

. We know

1 − a(s+1)
q(s+1)a(s+1)

= (1 − a(s+1))2

= 1 − a(s)
1
q(s)a(s)

=

.

• For s = s0, note that a(s0+1) = c

1+2c = 9−

√

8

33

, q(s0+1) = 8(2−a(s0+1))a(s0+1)

3(1−a(s0+1))

we have

1 − a(s0+1)
q(s0+1)a(s0+1)

=

=

3(1 − a(s0+1))2
8(2 − a(s0+1)) (cid:0)a(s0+1)(cid:1)2
1
2

(a)
≤ 1 − a(s0)

(b)
=

1
q(s0)a(s0)

,

where (a) is by a(s0) ≤ 1
2

, (b) is by q(s0) =

1
(1−a(s0))a(s0)

.

• For s ≥ s0 + 1, note that q(s) = 8(2−a(s))a(s)
3(1−a(s))

, by plugging in q(s), we only need to show

(1 − a(s+1))2
(a(s+1))2(2 − a(s+1))

≤

1 − a(s)
(a(s))2(2 − a(s))

.

Plug in a(s) =

c
s−s0+2c

, the above inequality is equivalent to

(2(s − s0) + 3c)(s − s0 + 1 + 2c)(s − s0 + 1 + c)2 ≤ (2(s − s0) + 2 + 3c)(s − s0 + c)(s − s0 + 2c)2.

Let y = s − s0 ≥ 1, we need to show

(2y + 3c)(y + 1 + 2c)(y + 1 + c)2 ≤ (2y + 2 + 3c)(y + c)(y + 2c)2

is true for y ≥ 1. People can check when c = 3+

√

4

33

, the above inequality is right for y ≥ 1.

Lemma B.9. Under the choice of parameters in Theorem B.1, ∀s ≥ 1, we have the following bounds

(1 − a(1))T1
q(1)a(1)

=

1
4

and

(cid:40)

q(s)a(s)
Ts

≤

4
(4n)1−0.5s
2(5+

33)c2
3n(s−s0+2c)2

√

1 ≤ s ≤ s0

s0 < s

.

Proof. Note that a(1) = 1 − 1
√
2

n

, T1 = n, q(1) =

1
(1−a(1))a(1)

, plugging in these values, we obtain

(1 − a(1))T1
q(1)a(1)

= (1 − a(1))2T1

=

1
4

36

• For 1 ≤ s ≤ s0, note that q(s) =

1
(1−a(s))a(s)

in our choice, so we know

q(s)a(s)
Ts

=

(a)
=

1
Ts(1 − a(s))
4
(4n)1−0.5s

where (a) is by plugging in Ts = n and a(s) = 1 − (4n) −0.5s .

• For s > s0, note that q(s) = 8(2−a(s))a(s)
3(1−a(s))

we have

q(s)a(s)
Ts

=

(b)
=

(c)
≤

8(2 − a(s))(a(s))2
3Ts(1 − a(s))
8(2 − a(s))(a(s))2
3n(1 − a(s))
√
33)c2
2(5 +
3n(s − s0 + 2c)2 ,

where (b) is by plugging inTs = n, (c) is by noticing 2−a(s)
a(s) =

.

c
s−s0+2c

1−a(s) ≤ 2−a(s0+1)

1−a(s0+1) = 5+

4

33

for s > s0, and plug in

√

B.4 Putting all together

We are now ready to put everything together and complete the proof of Theorem B.1.

Proof. (Theorem B.1) By Lemma B.8, ∀s ≥ 1, we have

(cid:0)2 − a(s)(cid:1) a(s)
1 − a(s)
(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤ q(s),

≤

Ts
q(s)a(s)

.

Hence all the conditions for Lemma B.4 are satisﬁed. Besides, we assume X is a compact convex set with diameter
D, which satisﬁes the requirements for Lemma B.7 and B.6.

1. For Option I, by Lemma B.4 and B.6

(cid:20)

E

TS
q(S)a(S)

(cid:21)
(F (u(S)) − F (x∗))

≤

(1 − a(1))T1
q(1)a(1)

(F (u(0)) − F (x∗)) +
(cid:21)+ 

(cid:19)

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

γ
2

γ

D2 + 2(η2 + D2) log

+

(cid:20) β
2

−

(cid:18) 1
2

−

D2
4η2

2η2β
2η2−D2
γ



 .

2. For Option II, by Lemma B.4 and B.7

(cid:20)

E

TS
q(S)a(S)

(cid:21)
(F (u(S)) − F (x∗))

≤

(1 − a(1))T1
q(1)a(1)
(cid:18) D2
η2
2

+

(F (u(0)) − F (x∗)) +

γ
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

η2 + β − γ

(cid:19)+ (cid:18) 2D2

η2 + β − γ

(cid:19)

.

Plugging in the bound (1−a(1))T1

q(1)a(1) = 1

4

from Lemma B.9, we have

(cid:20)

E

TS
q(S)a(S)

(F (u(S)) − F (x∗))

(cid:21)

≤

V
2

37

⇒ E

(cid:104)

(cid:105)
F (u(S)) − F (x∗)

≤

(a)
≤

q(S)a(S)V
2TS






2V
(4n)1−0.5S
√
(5+
3n(S−s0+2c)2

33)c2V

1 ≤ S ≤ s0

s0 < S

,

where (a) is by the bound for q(S)a(S)

TS

from Lemma B.9.

• If (cid:15) ≥ V
n

, we choose S = (cid:6)log2 log2

4V
(cid:15)

(cid:7) ≤ (cid:100)log2 log2 4n(cid:101) = s0, so we have

(cid:104)

E

F (u(S)) − F (x∗)

(cid:105)

≤

(b)
≤

=

2V
(4n)1−0.5S
2V
(cid:1) 1−0.5S
(cid:15)
(cid:1) −0.5S

(cid:0) 4V
(cid:15)

2 (cid:0) 4V

(cid:15)

where (b) is by n ≥ V
(cid:15)
individual gradient evaluations is

, (c) is by (cid:0) 4V

(cid:15)

(c)
≤ (cid:15),

(cid:1) −0.5S

= (cid:0) 4V

(cid:15)

(cid:1) −0.5(cid:100)log2 log2

4V

(cid:15) (cid:101)

≥ (cid:0) 4V

(cid:15)

(cid:1) −0.5log2 log2

4V

(cid:15) = 1
2

. The number of

#grads = nS +

S
(cid:88)

s=1

2Ts

= 3nS
(cid:24)

= 3n

(cid:18)

log2 log2

= O

n log log

(cid:25)

.

4V
(cid:15)
(cid:19)

V
(cid:15)

• If (cid:15) < V
n

, we choose S = s0 +

(cid:24)

c

(cid:18)(cid:113) (5+

√

33)V

3n(cid:15)

(cid:19)(cid:25)

(cid:24)

≥ s0 +

c

(cid:18)(cid:113)

5+

− 15
8

√

33

3 − 15

8

(cid:19)(cid:25)

= s0 + 1, so we have

(cid:104)

E

F (u(S)) − F (x∗)

(cid:105)

≤

√

(5 +

33)c2V

3n(S − s0 + 2c)2

√

(5 +
(cid:18)(cid:113) (5+

33)c2V
√

33)V

3n(cid:15)

− 15
8

(cid:19)(cid:25)

(cid:19)2

+ 2c

=

≤

≤

(cid:18)

(cid:24)

3n

s0 +

c
√

(5 +
(cid:113) (5+

33)c2V
√

33)V

3n(cid:15)

(cid:18)
c

(cid:19)2

+ c
8

√

(5 +
(cid:18)
c

(cid:113) (5+

33)c2V
√

33)V

3n(cid:15)

(cid:19)2

3n

3n

The number of individual gradient evaluations is

= (cid:15).

S
(cid:88)

s=1

2Ts

#grads = nS +

= 3nS

38

Algorithm 3 VRAE
Input: initial point u(0), smoothness parameter β.
Parameters: {a(s)}, {Ts}, A(0)
T0
x(1)
0 = z(1)
for s = 1 to S:
A(s)
0 = A(s−1)
Ts−1
for t = 1 to Ts:

> 0
0 = u(0), compute ∇f (u(0))

(cid:0)a(s)(cid:1)2

− Ts

(cid:26)

a(s) (cid:68)
x(s)
g(s)
t = arg minx∈X
t−1, x
t−1 + a(s) + (cid:0)a(s)(cid:1)2
t = A(s)
(cid:16)
A(s)
t−1x(s)
t−1 + a(s)x(s)

(cid:69)

t + (cid:0)a(s)(cid:1)2

u(s−1)(cid:17)

+ a(s)h(x) + 4β

(cid:13)
(cid:13)x − z(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

t ∼ Uniform ([n])
(x(s)

t ) − ∇fi(s)

t

(u(s−1)) + ∇f (u(s−1))

Let A(s)
x(s)
t = 1
A(s)
t
if t (cid:54)= Ts:
Pick i(s)
g(s)
t = ∇fi(s)
else:
t = ∇f (x(s)
g(s)
t )
z(s)
t = arg minz∈X
= x(s)
Ts

t

u(s) = x(s+1)

0
return u(S)

(cid:26)

a(s) (cid:68)
, z(s+1)
0

, z

g(s)
t
= z(s)
Ts

(cid:69)

+ a(s)h(z) + 4β

(cid:13)
(cid:13)z − z(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

, g(s+1)
0

= g(s)
Ts

= 3ns0 + 3n(S − s0)





(cid:115)

= 3n (cid:100)log2 log2 4n(cid:101) + 3n

(cid:32)

(cid:114)

= O

n log log n +

c



(cid:33)



.

nV
(cid:15)

√

33)V

(5 +

3n(cid:15)

−

15
8











C AdaVRAE for known β

In this section, we give a non-adaptive version of our algorithm AdaVRAE. The algorithm is shown in Algorithm 3.
The only change is in the step size: we set γ(s)
t = 8β for all epochs s and iterations t. The analysis readily extends
to show the following convergence guarantee:

Theorem C.1. Let s0 = (cid:100)log2 log2 4n(cid:101), c = 3

2 . If we choose parameters as follows

a(s) =

(cid:40)

(4n)−0.5s
s−s0−1+c
2c

1 ≤ s ≤ s0
s0 < s

,

Ts = n,
5
4

=

.

A(0)
T0

The number of gradient evaluations to achieve a solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for Algorithm 3 is

#grads =






(cid:1)

O(cid:0)n log log V
O

n log log n +

(cid:16)

(cid:15)

(cid:17)

(cid:113) V n
(cid:15)

if (cid:15) ≥ V
n
if (cid:15) < V
n

where V = 5
2

(cid:0)F (u(0)) − F (x∗)(cid:1) + 8β (cid:13)

(cid:13)u(0) − x∗(cid:13)
2
(cid:13)

.

39

Algorithm 4 VRAG
Input: initial point u(0), smoothness parameter β
Parameters: {a(s)} where a(s) ∈ (0, 1), {Ts}
x(1)
0 = u(0)
for s = 1 to S:
x(s)
0 = a(s)x(s)
for t = 1 to Ts:

0 + (1 − a(s))u(s−1), calculate ∇f (u(s−1))

t−1) − ∇fi(s)
(cid:26)(cid:68)

t

(u(s−1)) + ∇f (u(s−1))
(cid:69)

g(s)
t

, x

+ h(x) +

β(2−a(s))a(s)
2(1−a(s))

(cid:13)
(cid:13)x − x(s)
(cid:13)

t−1

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

t ∼ Uniform ([n])
(x(s)

t

Pick i(s)
g(s)
t = ∇fi(s)
x(s)
t = arg minx∈X
x(s)
t = a(s)x(s)
u(s) = 1
Ts
return u(S)

t=1 x(s)

(cid:80)Ts

t

t + (1 − a(s))u(s−1)
, x(s+1)
= x(s)
0
Ts

Proof. Note that Algorithm 3 is essentially the same as Algorithm 1 by choosing γ(s)
Hence the requirements for Lemma A.8 still hold. So we can obtain

t ≡ 8β with no other changes.

(cid:104)

E

A(S)
TS

(cid:16)

F (u(S)) − F (x∗)

(cid:17)(cid:105)

(cid:16)

≤ A(0)
T0

F (u(0)) − F (x∗)

(cid:17)

+ 4β

Then by the similar proof in Theorem A.1, we get the desired result.

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

D AdaVRAG for known β

In this section, we give a non-adaptive version of our algorithm AdaVRAG. The algorithm is shown in Algorithm
4. VRAG admits the following convergence guarantee:

√
Theorem D.1. (Convergence of VRAG) Deﬁne s0 = (cid:100)log2 log2 4n(cid:101), c = 3+
follows

4

33

. If we choose the parameters as

a(s) =

(cid:40)

1 − (4n)−0.5s

c
s−s0+2c

1 ≤ s ≤ s0
s0 < s

,

Ts = n.

The number of individual gradient evaluations to achieve a solution u(S) such that E (cid:2)F (u(S)) − F (x∗)(cid:3) ≤ (cid:15) for
Algorithm 4 is

#grads =






(cid:1)

O (cid:0)n log log V
O

n log log n +

(cid:16)

(cid:15)

(cid:17)

(cid:113) nV
(cid:15)

(cid:15) ≥ V
n
(cid:15) < V
n

,

where

V =

1
2

(F (u(0)) − F (x∗)) + β

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

Before giving the proof of Theorem C.1, we state some intuition on our parameter choice. Note that by deﬁning

the following two auxiliary sequences

q(s) =

γ(s)
t−1 =




1
(1−a(s))a(s)
(2−a(s))a(s)

1−a(s)
β (cid:0)2 − a(s)(cid:1) a(s)
(1 − a(s))q(s)

1 ≤ s ≤ s0

,

s0 < s

, ∀t ∈ [Ts] ,

the update rule of x(s)
Algorithm 2. Since γ(s)
t−1

t

in every epoch in Algorithm 4 is equivalent to the update rule of x(s)
in every epoch in
is a constant in the corresponding epoch now, we will use γ(s) without the subscript to

t

40

simplify the notation. The above argument means that we can apply Lemma B.3 directly to obtain the following
lemma.

Lemma D.2. For all epochs s ≥ 1, we have

(cid:104)

(cid:105)
F (u(s)) − F (x∗)

E

≤ E

(cid:20)(cid:16)

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

(cid:17)

+

γ(s)q(s)a(s)
2Ts

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s+1)
(cid:13)

0

2(cid:19)(cid:21)

− x∗(cid:13)
(cid:13)
(cid:13)

.

Proof. By applying Lemma B.3, we know

E

≤E

(cid:105)
(cid:104)
F (u(s)) − F (x∗)
1 − a(s)(cid:17) (cid:16)
(cid:104)(cid:16)
(cid:34)

+ E

1
Ts

+ E

(cid:34)

1
Ts

Ts(cid:88)

t=1

Ts(cid:88)

t=1

F (u(s−1)) − F (x∗)

(cid:17)(cid:105)

γ(s)
t−1q(s)a(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

2(cid:19)(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

(cid:32)

β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2 (cid:0)1 − a(s)(cid:1)

−

γ(s)
t−1q(s)a(s)
2

(cid:33)

(cid:13)
(cid:13)x(s)
(cid:13)

t − x(s)

t−1

2(cid:35)
(cid:13)
(cid:13)
(cid:13)

(cid:34)

(cid:16)

(a)
= E

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

(cid:20)(cid:16)

(cid:20)(cid:16)

=E

(b)
=E

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

1 − a(s)(cid:17) (cid:16)

F (u(s−1)) − F (x∗)

(cid:17)

+

(cid:17)

(cid:17)

+

+

γ(s)q(s)a(s)
2Ts

γ(s)q(s)a(s)
2Ts
γ(s)q(s)a(s)
2Ts

Ts(cid:88)

(cid:13)
(cid:13)x(s)
(cid:13)

t−1 − x∗(cid:13)
2
(cid:13)
(cid:13)

2(cid:35)
t − x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s)
(cid:13)

−

t=1
(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)
(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)
0 − x∗(cid:13)
2
(cid:13)
(cid:13)

2(cid:19)(cid:21)

−

−

Ts

(cid:13)
(cid:13)x(s)
(cid:13)
(cid:13)
(cid:13)x(s+1)
(cid:13)

− x∗(cid:13)
(cid:13)
(cid:13)
− x∗(cid:13)
(cid:13)
(cid:13)

0

2(cid:19)(cid:21)

,

where (a) is by γ(s)

t−1q(s) =

β(2−a(s))a(s)
1−a(s)

and γ(s) = γ(s)

t−1, ∀t ∈ [Ts], (b) is by x(s+1)

0

= x(s)
Ts

.

Now if we still multiply both sides by

, we need to ensure that γ(s) can help us to make a telescoping
sum. However, this is not always true. So we need some diﬀerent conditions as stated in the following lemma to
obtain a bound for the function value gap of u(S). The new bound for the function value gap of u(S) for Algorithm
4 is as follows.

Ts
q(s)a(s)

Lemma D.3. If ∀s (cid:54)= s0, we have

a(s+1) ≤ a(s),

(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤

Ts
q(s)a(s)

.

Additionally, for s0, assume we have

(cid:0)1 − a(s0+1)(cid:1)2
(cid:0)2 − a(s0+1)(cid:1) (cid:0)a(s0+1)(cid:1)2 ≤

Ts0+1

(1 − a(s0))Ts0
(cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2 .

Then for S ≤ s0,

(cid:20)

E

(cid:16)

TS
q(S)a(S)

F (u(S)) − F (x∗)

(cid:17)(cid:21)

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

(cid:16)

≤

F (u(0)) − F (x∗)

(cid:17)

+

β
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

For S > s0,

E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

TS

q(S)a(S)

(cid:16)

F (u(S)) − F (x∗)

(cid:17)

(cid:35)

≤

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

+

β
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

Proof. By applying Lemma D.2 and multiply both sides by

Ts

q(s)a(s) ,we have

(cid:20)

E

(cid:16)

Ts
q(s)a(s)

F (u(s)) − F (x∗)

(cid:17)(cid:21)

≤ E

(cid:34) (cid:0)1 − a(s)(cid:1) Ts
q(s)a(s)

(cid:16)

F (u(s−1)) − F (x∗)

(cid:17)

+

γ(s)
2

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s+1)
(cid:13)

0

2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

.

41

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

S
(cid:88)

+

F (u(0)) − F (x∗)

(cid:17)

+ E

s=1
(cid:34) S
(cid:88)

s=1

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

γ(s)
2
β (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2
2

−

(cid:13)
(cid:13)x(s+1)
(cid:13)

0

2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:18)(cid:13)
(cid:13)x(s)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)x(s+1)
(cid:13)

0

2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:17)

F (u(0)) − F (x∗)
+
(cid:104)(cid:0)2 − a(s+1)(cid:1) (cid:0)a(s+1)(cid:1)2
2

β (cid:0)2 − a(1)(cid:1) (cid:0)a(1)(cid:1)2
2
− (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2(cid:105)

(cid:13)
(cid:13)x(1)
(cid:13)

0 − x∗(cid:13)
2
(cid:13)
(cid:13)

(cid:18)(cid:13)
(cid:13)x(s+1)
(cid:13)

0

2(cid:19)

− x∗(cid:13)
(cid:13)
(cid:13)





For S ≤ s0
(cid:20)

F (u(S)) − F (x∗)

(cid:17)(cid:21)

≤E

E

(cid:16)

TS
q(S)a(S)
(cid:34) (cid:0)1 − a(1)(cid:1) T1
q(1)a(1)
(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)
(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)


(cid:16)

(cid:16)

+ E



S−1
(cid:88)

β

s=1

(a)
=

=

(b)
≤

− E

(cid:34)

β (cid:0)2 − a(S)(cid:1) (cid:0)a(S)(cid:1)2
2

(cid:18)(cid:13)
(cid:13)x(S+1)
(cid:13)

0

2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)
(cid:34)

− E

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

+

β (cid:0)2 − a(S)(cid:1) (cid:0)a(S)(cid:1)2
2

(cid:18)(cid:13)
(cid:13)x(S+1)
(cid:13)

0

2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)
β
2
2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

,

where (a) is by the deﬁnition of γ(s) when s ≤ s0, (b) is by (cid:0)2 − a(1)(cid:1) (cid:0)a(1)(cid:1)2
that our assumption a(s+1) ≤ a(s) ⇒ (cid:0)2 − a(s+1)(cid:1) (cid:0)a(s+1)(cid:1)2

≤ (cid:0)2 − a(s)(cid:1) (cid:0)a(s)(cid:1)2.

≤ 1 and x(1)

0 = u(0), additionally, note

For S > s0, we can also make the telescoping sum from s = s0 + 1 to S by a similar argument to get
(cid:34) (cid:0)1 − a(s0+1)(cid:1) Ts0+1
q(s0+1)a(s0+1)

F (u(s0)) − F (x∗)

F (u(S)) − F (x∗)

TS
q(S)a(S)

(cid:13)
(cid:13)x(s0+1)
(cid:13)

2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

≤ E

(cid:17)(cid:21)

β
2

+

E

(cid:17)

(cid:16)

(cid:16)

(cid:20)

0

.

Multiplying both sides by (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2, we have

E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

TS

(cid:16)

q(S)a(S)

F (u(S)) − F (x∗)

(cid:35)

(cid:17)

≤E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2 (cid:0)1 − a(s0+1)(cid:1) Ts0+1
q(s0+1)a(s0+1)

(cid:16)

F (u(s0)) − F (x∗)

+

(cid:17)

(c)
= E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2 (cid:0)1 − a(s0+1)(cid:1)2

(2 − a(s0+1)) (cid:0)a(s0+1)(cid:1)2

Ts0+1

(cid:16)

F (u(s0)) − F (x∗)

(cid:17)

+

β (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2
2
β (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2
2

(cid:13)
(cid:13)x(s0+1)
(cid:13)

0

2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)x(s0+1)
(cid:13)

0

2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

,

where (c) is by the deﬁnition q(s0+1) =

(2−a(s0+1))a(s0+1)
1−a(s0+1)

. Note that by our assumption

(cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2 (cid:0)1 − a(s0+1)(cid:1)2

Ts0+1

(2 − a(s0+1)) (cid:0)a(s0+1)(cid:1)2

≤ (1 − a(s0))Ts0

=

Ts0
q(s0)a(s0)

,

so we know

E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

TS

(cid:16)

q(S)a(S)

F (u(S)) − F (x∗)

(cid:35)

(cid:17)

42

(cid:34)

≤E

(cid:16)

Ts0
q(s0)a(s0)

F (u(s0)) − F (x∗)

(cid:17)

β (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2
2

+

(cid:13)
(cid:13)x(s0+1)
(cid:13)

0

2(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

.

Now combining

(cid:20)

E

(cid:16)

Ts0
q(s0)a(s0)

F (u(s0)) − F (x∗)

(cid:17)(cid:21)

≤

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)
(cid:34)

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

+

β
2

− E

β (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2
2

(cid:18)(cid:13)
(cid:13)x(s0+1)
(cid:13)

0

2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)
2(cid:19)(cid:35)
− x∗(cid:13)
(cid:13)
(cid:13)

,

we have

E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

TS

(cid:16)

q(S)a(S)

F (u(S)) − F (x∗)

(cid:17)

(cid:35)

≤

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

+

β
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

Using the above new lemma w.r.t. the function value gap of u(S), we ﬁnally can give the proof of Theorem D.1.
Proof. (Theorem D.1) Note that by our choice a(s+1) ≤ a(s) is true for any s (cid:54)= s0. Besides, our parameters (cid:8)a(s)(cid:9)
and (cid:8)q(s)(cid:9) are totally the same as the choice in Theorem B.1 when s ≤ s0. Hence we know

is still true for s ≤ s0 − 1. For s ≥ s0 + 1, note that our new (cid:8)q(s)(cid:9) are only diﬀerent from the choice in Theorem
B.1 by a constant, which implies

(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤

Ts
q(s)a(s)

also holds for s ≥ s0 + 1. Besides, we can show

(1 − a(s+1))Ts+1
q(s+1)a(s+1)

≤

Ts
q(s)a(s)

(cid:0)1 − a(s0+1)(cid:1)2
(cid:0)2 − a(s0+1)(cid:1) (cid:0)a(s0+1)(cid:1)2 ≤

Ts0+1

(1 − a(s0))Ts0
(cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

is true by plugging in the value of a(s0+1) = c
D.3 are satisﬁed, then we know for S ≤ s0,

1+2c

and noticing that a(s0) ≤ 1
2

. Hence all the conditions for Lemma

(cid:20)

E

(cid:16)

TS
q(S)a(S)

F (u(S)) − F (x∗)

(cid:17)(cid:21)

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

(cid:16)

≤

F (u(0)) − F (x∗)

(cid:17)

+

β
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

For S > s0,

E

(cid:34) (cid:0)2 − a(s0)(cid:1) (cid:0)a(s0)(cid:1)2

TS

(cid:16)

q(S)a(S)

F (u(S)) − F (x∗)

(cid:17)

(cid:35)

≤

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

(cid:16)

F (u(0)) − F (x∗)

(cid:17)

+

β
2

(cid:13)
(cid:13)

(cid:13)u(0) − x∗(cid:13)

2
(cid:13)
(cid:13)

.

By noticing

and

a(s0) = 1 − (4n)−0.5s0

≥ 1 − (4n)−0.5(log2 log2 4n)+1

(cid:16)

2 − a(s0)(cid:17) (cid:16)

a(s0)(cid:17)2

≥

⇒

= 1 −

1
√
2
√
2

2 −
4

(cid:0)1 − a(1)(cid:1) T1
q(1)a(1)

=

1
4

,

combining the fact that our new (cid:8)q(s)(cid:9) for S > s0 have the same order of the choice in Theorem B.1. Following a
similar proof, we can arrive the desired result.

43

E Hyperparameter choices and additional results

Table 2 reports the hyperparameter choices used in the experiments. VRAG and VRAE are the non-adaptive
versions our algorithms (Algorithms 3 and 4). We set their step sizes via a hyperparameter search as described in
Section 3. Figures 5, 6, 7, 8 give the experimental evaluation of our non-adaptive algorithms.

Table 2: Hyperparameters used in the experiments

Dataset

Loss

SVRG SVRG++ VARAG VRADA VRAG VRAE

a1a

mushrooms

w8a

phishing

logistic

squared

huber

logistic

squared

huber

logistic

squared

huber

0.5

0.01

0.05

0.5

0.01

0.05

0.1

0.01

0.01

logistic

50

squared

0.05

huber

0.5

0.5

0.05

0.1

1

0.01

0.1

1

0.01

0.1

100

0.5

1

1

0.05

0.1

1

0.05

0.1

1

0.01

0.1

100

1

1

1

0.1

0.5

1

0.1

0.1

100

100

100

100

1

5

1

0.1

0.1

1

0.05

0.1

1

0.05

0.1

100

1

5

1

0.05

0.1

1

0.01

0.05

5

0.05

0.5

100

1

5

44

(a) Logistic loss

(b) Squared loss

Figure 5: a1a

(c) Huber loss

(a) Logistic loss

(b) Squared loss

(c) Huber loss

Figure 6: mushrooms

(a) Logistic loss

(b) Squared loss

Figure 7: w8a

(c) Huber loss

(a) Logistic loss

(b) Squared loss

(c) Huber loss

Figure 8: phishing

45

