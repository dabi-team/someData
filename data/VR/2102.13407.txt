1
2
0
2

r
p
A
9

]

C
H
.
s
c
[

3
v
7
0
4
3
1
.
2
0
1
2
:
v
i
X
r
a

The Virtual Emotion Loop: Towards
Emotion-Driven Product Design via Virtual
Reality

Davide Andreoletti, Luca Luceri, Achille Peternier, Tiziano Leidi, Silvia Giordano

University of Applied Sciences and Arts of Southern Switzerland (SUPSI)

April 12, 2021

Abstract

Emotions play a signiﬁcant role in the interaction between products
and users. However, it is still not very well understood how users’ emo-
tions can be incorporated in product design. We argue that this gap
is due to a lack of a methodological and technological framework for
an eﬀective investigation of the elicitation conditions of emotions and
of the corresponding emotional response of the users. Indeed, emotions
are either disregarded or not systematically considered in current design
approaches. For example, the eﬀectiveness of emotion elicitation condi-
tions embodied into the product is generally validated by assessing users’
emotional response through ineﬀective means (e.g., surveys and inter-
views).
In our view, emotion-driven design should encompass a thor-
ough assessment of users’ emotional reactions in relation to certain elic-
itation conditions. In this paper, we argue that Virtual Reality (VR) is
the most suitable mean to perform this investigation, and we propose a
novel methodological framework, referred to as the Virtual-Reality-Based
Emotion-Elicitation-and-Recognition loop (VEE-loop), that can be ex-
ploited to realize it. Speciﬁcally, the VEE-loop consists in a continuous
monitoring of users’ emotions, which are then provided to product design-
ers as an implicit users’ feedback. This information is used to dynamically
change the content of VR environment, and the process is iterated until
the desired aﬀective state is solicited. The main applications that we envi-
sion are in product design (i.e., to early validate the eﬀectiveness of some
emotion elicitation conditions) and digital product delivery, e.g., distance
learning can be adapted in real-time according to the user’s emotional
status.

1

Introduction

The capability of a product (being it tangible or not, e.g., a service) to engage
its users at the emotional level is considered by many the main factor behind

1

 
 
 
 
 
 
[2] claim that up to 95%
its success [1]. In this respect, the authors of Ref.
of our buying decisions are not driven by rational arguments. Despite this, the
fulﬁllment of functional requirements is generally the only objective in product
design, with emotional aspects being often underestimated, if not even totally
disregarded [3]. While this contradiction is commonly interpreted as a phe-
nomenon of cultural inertia [3] (argument that we also support), in this paper
we give the following additional interpretation: there is no methodological and
technological framework that helps designers to systematically experiment emo-
tion elicitation conditions, as well as to assess the consequent users’ emotions.
Emotion-driven design is the set of processes and methods used for devel-
oping products with the speciﬁc aim of evoking certain emotional responses.
Following this paradigm, designers consider the ﬁnal users’ emotions since the
very early stages of development, and not, as often done, by leaving them as an
afterthought. Existing approaches aimed to foster emotion-driven design (e.g.,
[1]) are based on the idea that designers should familiarize with all the aspects of
emotions, ranging from their deﬁnition and elicitation conditions (e.g., in terms
of products’ sensory characteristics) and manifestations on people. Following
a similar line of reasoning, in this paper we argue that emotion-driven design
should be characterized by i) a systematic experimentation of various emotion
elicitation conditions (e.g., the sensory qualities of the product and context of
its usage) and by ii) the reliable measurement of users’ emotional response,
i.e., emotion recognition. Then, we also argue that emotional elicitation and
emotion recognition should be iterated in a continuous loop until the emotion
elicitation conditions can evoke the emotion intended by the designer.

Following the scheme envisioned in this paper, a designer makes her stylistic
choice with the intention to evoke some emotional response (i.e., emotion elic-
itation phase); users’ emotions are qualitatively measured (i.e., emotion recog-
nition phase); the designer observes the emotions actually perceived by users
and gains relevant insights to adjust her choices accordingly (i.e., loop phase).
We are aware that the emotional reaction might be signiﬁcantly aﬀected by the
number of iterations. Just as an example, boredom might be evoked more fre-
quently after having done several experiments, regardless of the current emotion
elicitation conditions. We discuss this and similar issues in Section 6. Please
note that the proposed scheme can be implemented only in the design phase
(e.g., to validate the hypothesis that some feature triggers a speciﬁc emotional
reaction) or, whenever possible, in the delivery phase as well (e.g., by dynami-
cally adapting the characteristic of a service in real-time). While the proposed
scheme can, in principle, be implemented using the most varied approaches, we
believe that Virtual Reality (VR) provides the perfect controlled environment
to turn our vision into a practical design instrument. Therefore, we refer to the
proposed framework as the VEE-loop, i.e., the Virtual-Reality-Based Emotion-
Elicitation-and-Recognition loop.

We identify several factors that, in our view, make VR the most suitable
technology to implement the proposed scheme. First, among all the existing
digital technologies, VR is the one that guarantees the most tangible experience
across diﬀerent domains. In fact, VR allows users to feel a sense of presence that

2

makes emotion elicitation conditions quite similar to a real scenario [4], with the
remarkable advantage of also enabling a ﬂexible modiﬁcation of the virtual scene
experienced by the user, i.e., the Virtual Environment (VE). In addition, VR
allows gathering a set of users-related data from which their emotions can be
inferred (e.g., bio-feedback and behaviors). Note that a number of bio-feedback
signals can be gathered using the Head Mounted Displays (HMDs) used by VR
system, as well as with other external devices (e.g., wearables).

The VEE-loop has the potential to beneﬁt a high number of application
areas. For example, it can be used as a tool to perform a validation of the ca-
pability of a product to trigger speciﬁc emotions, before its actual production.
Indeed, designers could obtain an emotional feedback from potential customers
and tune the design accordingly. Given that this feedback is obtained before the
actual product development, the risk of designing unsuccessful products is signif-
icantly reduced. In this respect, the immersion level provided by VR guarantees
a higher ﬁdelity of this emotional reaction with respect to other methods, while
its ﬂexibility allows testing a high number of products’ characteristics. The
VEE-loop can also improve services in which the knowledge of users’ aﬀective
states is highly beneﬁcial, but unavailable for some reason (e.g., due to physi-
cal distancing measures imposed to handle the Covid-19 pandemic). In remote
learning, for example, the emotional states of students can be monitored, and
the virtual lecture dynamically changed (e.g., to induce calm in students or to
draw their attention) [5].

This paper is structured as follows. In Section 2 we elaborate on the im-
portance of emotions in user-product interaction, we present the characteristics
of emotion-driven product design and we motivate the use of VR as enabling
technology to implement it. Then, in Section 3 we describe the VEE-loop in
detail, while in Section 4 we show how the VEE-loop advances existing ap-
proaches. Section 5 is devoted to the presentation of the impact and of the
application areas of the VEE-loop. Finally, in Section 6 we discuss opportu-
nities and challenges derived from the use of the VEE-loop, also adding some
concluding remarks.

2 Towards VR-based Emotion-Driven Product

Design

This Section starts by elaborating on the importance of emotions in users-
product interaction. Then, it introduces the paradigm of emotion-driven prod-
uct design, highlighting its main characteristics. Finally, we motivate the use of
VR as an enabling technology towards the realization of this paradigm.

2.1 Emotions in users-product interaction

Emotions are present in almost all human experiences, including the interac-
tion between users and products. Based on the ﬁndings of previous works, the

3

authors of Ref. [1] identify three main products’ characteristics that evoke emo-
tions on their users, namely appearance, functionality and symbolic meaning. As
for the appearance, it is acknowledged that sensory qualities (e.g., shape and
color) are associated with diﬀerent emotional experiences. For instance, warm
colors are generally chosen to increase the arousal levels of evoked emotions [6].
In general, the functionalities of a product elicit positive emotions if they fulﬁll
the needs of the users, and negative otherwise. For instance, a product that
improves a situation that is perceived as frustrating and limiting (e.g., by en-
abling to gain space in small environments) is likely to evoke positive emotions.
On the contrary, a product that is cumbersome and reduces the available space
likely leads to frustration and annoyance. Then, the symbolic meaning of a
product refers to its connection with a broader scheme of beliefs and values. In
relation to the symbolic meaning of a product, the appraisal theory [7] states
that emotions are triggered by the foretaste that users have when they evaluate
the role of the product in their lives. For instance, a treadmill can evoke positive
emotions in those who see it as a mean to get ﬁtter, but negative ones in those
worried by strain. Another example is the symbolic value given by the aﬃnity
of a product with a certain idea (e.g., a ﬂag that represents a certain political
view).

The importance of the symbolic meaning of products in evoking emotions is
well expressed in the famous quote stated by Simon Sinek in one of the most
viewed TED talk ever1: people don’t buy what you do, they buy why you do it
[8]. Indeed, the meaning that a person ascribes to a product is strongly cor-
related with her inner values, and their aﬃnity with a company’s mission and
concerns. In relation to this, the law of concern formulated in [9] aﬃrms that
every emotion hides a personal concern and a disposition to prefer particular
states of the world. This fact has led the authors of Ref. [10] to describe emo-
tions as gateways to what people really care for, and entry points to uncover
their underlying concerns. Along similar lines, the authors of Ref.
[3] argue
that understanding users at the emotional level allows having a deeper compre-
hension of their values, which is crucial to produce radical product innovations,
while the sole understanding of users’ functional needs yields only superﬁcial
and slight product modiﬁcations. Moreover, products capable to emotionally
engage their users foster creative and innovative thinking [10], and beneﬁt well-
being [11]. Therefore, the capability of understanding and engaging users at the
emotional level is crucial to design products that are appreciated and guarantee
loyalty of customers in the long term (in this respect, note also that a clear
and tight connection between a product and a speciﬁc emotion reinforces brand
identiﬁcation [3]).

2.2 Characteristics of emotion-driven product design

Given the major role of emotions in the relation between users and products,
emotion-driven design, i.e, the realization of products with the deliberate

1https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action?

language=en

4

intention to evoke speciﬁc emotions [10], is rapidly becoming an important re-
search area. In particular, several frameworks have been recently proposed to
help designers in the creation of products with emotional intentions. These
frameworks (e.g., the Emotion-Driven Innovation paradigm [1]) share the idea
that designers should be supported in the acquisition and practical exploita-
tion of a solid emotion knowledge, which is deﬁned in Ref.
[10] as the explicit
understanding of the physical manifestations of emotions and of their eliciting
conditions. At the moment, these frameworks are still not very diﬀused [11];
arguably, this limited diﬀusion is mainly due to the lack of a technological layer
to facilitate the study of emotion elicitation and recognition. In this paper, we
claim that VR represents the most powerful medium to invert this tendency
and consolidate the practice of emotion-driven design. In the following, we ex-
press our view on the characteristics that a framework for emotion-driven design
should have, both concerning the study of the conditions of emotion elicitation
and the recognition of emotions from their manifestation. Then, in subsection
2.3, we provide arguments that support the idea of using VR as the basis to
implement such a framework.

2.2.1 Emotion Recognition

The capability to disambiguate between diﬀerent emotions (e.g., to understand
the diﬀerence between frustration and annoyance) has been deﬁned in Ref. [10]
as emotional granularity and is regarded in Ref.
[12] as a core advantage for
the realization of emotion-driven products. Indeed, it is essential that designers
understand the nuances of emotions beyond the simple positive versus negative
distinction [11] (in this respect, just think that consumers may experience 25
diﬀerent positive emotions when interacting with a product [10]).

The most straightforward approach to understand users’ emotions is by di-
rect communication. However, people are generally not aware of their emotions,
nor they can properly verbalize and communicate them. Hence, traditional in-
vestigation tools (e.g., surveys and interviews) cannot eﬀectively capture users’
emotions. Moreover, tools based on self-reports require users to interrupt their
activity, which in turn may hinder the validity of their records [10]. Therefore,
emotion recognition techniques, i.e, the qualitative measurement of emotions
from their manifestations, seems to be the most viable alternative.

Aﬀective computing refers to a set of technologies and strategies developed
to automatize emotion recognition, generally exploiting machine learning al-
gorithms that infer the emotions a person most likely perceives from her bio-
feedback (e.g., facial expression, blood pressure, movements, etc.). The fact that
only bio-feedback are considered is, in our view, a severe limitation of the tradi-
tional approach. For instance, behaviors of users, which are actually part of the
manifestations of emotions [11], are currently disregarded. Indeed, emotions are
complex phenomena that are better understood if studied holistically [11]. In
user-product interaction, this holistic investigation would require, for example,
the correlation of the sensory and symbolic characteristics of the product with
the bio-feedback of the users, as well as with her behaviors (e.g., which action

5

users take after using a product) and with the context in which the product is
used [10]. In particular, the possibility to correlate users’ emotions with contex-
tual factors has been considered in Ref. [11] as a way to better understand their
concerns and inner values. Therefore, we believe that a framework for emotion-
driven design should allow probing users’ emotions considering as many aspects
of their manifestations as possible (e.g., bio-feedback, context, behaviour, etc.).

2.2.2 Emotion Elicitation

The task of identifying the conditions that elicit certain emotions is inherently
challenging.
Indeed, while it is acknowledged that certain products’ charac-
teristics induce similar emotional reactions on the majority of their users [6],
emotions are generally subjective experiences. In other words, the relation be-
tween certain types of stimuli and emotions is neither deterministic nor constant,
as it can change over time even for the same person [10]. In addition, elicitation
conditions are extremely complex, as they depend on the interaction of various
factors, such as product’s characteristics and context of usage.

In light of this, we believe that a framework for emotion-driven design should
favour the validation of emotion elicitation conditions both on a single-user and
on a large-scale basis. As for the former, this framework would help design-
ers to understand a customer at the emotional level, and in turn uncover her
latent concerns and desires [3]. In addition, this framework would enable the
design of products performed cooperatively by designers and customers (note
that product co-design is considered more attractive than designer-only and
customer-only design [3]). As for the latter, the framework should allow to eas-
ily validate the eﬀectiveness of diﬀerent combinations of elicitation conditions
(e.g., sensory qualities of the product and environment in which the product is
used) in evoking certain emotions.

2.2.3 Loop of Emotion Recognition and Elicitation

Finally, our envisioned framework for emotion-driven product design is based
on a tight connection between emotion recognition and emotion elicitation. In
fact, users’ emotions should be continuously tracked and provided to designers
as a feedback of the suitability of the chosen elicitation conditions. Elicitation
conditions can then be modiﬁed accordingly, in a more informed manner. This
operation can then be repeated until the expected emotional reaction is achieved,
which gives rise to the Emotion-Recognition-Emotion-Elicitation Loop.
This scheme can be applied, for instance, to validate the eﬀectiveness of ex-
perimental emotion elicitation conditions, as well as to better understand the
factors that triggered some particular emotions.
In the following subsection,
we provide arguments that support our choice of VR as the most suitable can-
didate technology to implement this scheme, which we then refer to as the
VR-based Emotion-Recognition-Emotion-Elicitation Loop, or simply
the VEE-loop. A representation of the VEE-loop is depitected in Fig. 1.

6

Figure 1: High-Level Representation of the VEE loop

2.3 VR as enabling technology

For various reasons, VR is the most natural, direct, and suitable technology to
implement the framework described so far. First, pure VR (i.e., a user interact-
ing with entirely synthetic, computer-generated VEs) allows creating completely
modiﬁable, dynamic experiences. Unlike augmented and mixed reality, which is
limited and linked to the surrounding physical elements, pure VR can be eas-
ily distributed online, experienced everywhere, replayed at will, and its content
regularly updated. The immersion provided by VR also ampliﬁes emotional
reactions [13, 14, 4], which helps both in the emotion recognition and elicitation
phases compared to other less eﬀective means to put a user in a given simulated
situation. In addition, the retention rate of learning and training provided via
VR is increased when compared to more conventional media [15]. The recent
revival of VR also provides a much lower entry-point to the technology, which is
now considered a commodity, oﬀ-the-shelf option no longer limited to research
laboratories or professional contexts. Thanks to this evolution, which also sig-
niﬁcantly increased the quality of modern VR compared to the state of the art
of just few years ago, a signiﬁcantly larger user-base can now be targeted by
VR-enabled solutions.

Then, modern VR equipment already embeds sensors that are critical for
inferring the user’s emotional state (and its evolution) during the virtual expe-
rience. Since body tracking is a central requirement of VR, most of the recent
HMDs are capable of tracking user’s head and hands position in real-time and
at high frequency, while some models include eye tracking as well. These sen-
sors can be used for the proper positioning of the user within the VE (e.g.,
to update the viewpoint and stereoscopic rendering parameters), as well as to

7

VR UserVirtual EnvironmentEMOTION RECOGNITIONdetectionof user’semotionbasedon the implicitfeedback collectedfrom VREMOTION ELICITATIONdynamicadaptationof VRaccordingto i) the currentdetectedemotionand ii) the target emotionprecisely determine what the user is currently looking at, but also to derive a
series of additional metrics such as heartbeat and respiratory rate [16]. Next-
generation HMDs will directly embed dedicated sensors for monitoring such
states (like the HP Reverb G2 Omnicept).

This constant source of information can be used to acquire data that pre-
viously required to dress the user with a cumbersome set of devices and to
prepare the environment for diﬀerent levels of motion tracking (from a simple
Microsoft Kinect to professional-grade systems such as the Vicon). Most of
these capabilities are now integrated into one single device that provides all
the ingredients for building an emotion recognition and elicitation system under
wearable and aﬀordable constraints. Nevertheless, HMDs can still be coupled
with additional monitoring devices to increase the amount, types, and accu-
racy of users’ bio-feedback signals for this task (e.g., by combining the full-body
tracking provided by the Microsoft Kinect with the head and hands positions
returned by the headset). Moreover, tools have been recently proposed to enable
HMDs tracking the movements of the face2. In addition, VR enables simulating
the context in which a user acts, and analyzing her behaviours in relation to
this. Let us note that, since emotions are observable from behaviors as well,
this property of VR has the potential to revolutionize the research in emotion
recognition and to increase the eﬀectiveness of this task.

3 The VEE-Loop

The VEE-loop is the realization, by means of VR technologies, of the Emotion-
Recognition-Emotion-Elicitation loop described in subsection 2.2.3. More specif-
ically, the VEE-loop is implemented by continuously monitoring the aﬀective
states of users and by adapting the VE accordingly. A modiﬁcation of the VE
is performed, for instance, to induce a transition from the current to the de-
sired aﬀective state (e.g., from fear to calm), or to evaluate the eﬀectiveness of
some emotion elicitation conditions. The VEE-loop is composed of a module
for Emotion Recognition (ER module) and one for Emotion Elicitation (EE
module). A detailed representation of the VEE-loop architecture is depicted in
Fig. 2. From this ﬁgure, it is possible to observe that the ER module infers the
emotion most likely perceived by the user by elaborating information such user’s
bio-feedback and the interaction between user and VE (e.g., her behaviour or
the attention she pays to a particular virtual object). The emotion detected by
the ER module and the emotion that the designer aims to evoke are then given
in input to the EE module, which is responsible for dynamically changing the
VE. We further articulate the ER and EE components in the next subsections.

3.1 Emotion Recognition Module

The ER module is responsible for inferring, from a set of multi-modal signals,
the emotion that the user most likely perceives. We identify two main categories

2https://uploadvr.com/htc-facial-tracker-quest-index/

8

Figure 2: Architecture of the proposed VEE-Loop

of these data: i) users’ bio-feedback signals and ii) user-VE interactions. As far
as user’s bio-feedback are concerned (e.g., movements, vital parameters, etc.),
we note that their acquisition can be performed directly with the HMD (e.g.,
by tracking head and eye movement), as well as with other supporting tools
that do not hinder VR experience (e.g., wearable devices). Then, we also argue
that users’ emotions are strongly correlated with her interaction with the VE,
e.g., fascination is observable from the level of attention and the time spent
using some object [11]. To our knowledge, however, the problem of modeling
the interaction between user and VE has never been considered in relation to
the task of emotion recognition. In particular, as users’ behaviours are integral
aspects of their emotional status, it is essential to model the behavior of the
users within the VE (e.g., which actions are taken, which types of objects are
used, etc.).

For the processing of bio-feedback and contextual data, we envision an ar-

chitecture composed of the following 4 layers:

• Feature Extraction: a set of suitable features has to be deﬁned to cap-
ture the properties of users’ bio-feedback and contextual data that are
beneﬁcial for the emotion recognition task. A quite established litera-
ture can help in the deﬁnition of features to represent a high number of
bio-feedback, both handcrafted and automatically learned [17] (e.g., ac-
celeration of joints for body’s movements and spectrogram for voices, or
learned with a deep learning approach). The deﬁnition of features to rep-

9

resent contextual data (e.g., interaction of the user with the VE), instead,
requires a more pioneering attitude. We argue that existing features used
to model the level of attention and engagement can help to deﬁne some
new more emotion-oriented features.

• Fusion: this layer is meant to combine data, features and algorithms to
maximally exploit the information contained in the users’ data, in order to
increase the generalization of the ER module; a research challenge here is
to combine data of diﬀerent domains (e.g., voice, heart rate and interaction
with the VE) that have radically diﬀerent properties, such as acquisition
frequency and temporal dynamic.

• Segmentation: please note that, during the use of VR, users’ emotions may
change over time. This layer performs data segmentation, i.e., it segments
the stream into portions of signals that are coherent with respect to the
particular emotion they carry. This is a remarkable diﬀerence with respect
to existing studies on emotion recognition, which generally assume that
observed data is associated with a single emotional content.

• Emotion Classiﬁcation: ﬁnally, this layer infers the emotion that the ob-
tained segmented data most likely carries. Note that emotions can be
represented either using a set of classes (e.g., joy, fear, etc.), or using a
dimensional approach (e.g., arousal, valence, dominance [18]). Based on
the type of representation that is chosen, this layer performs a supervised
learning task, either a classiﬁcation or a regression.

3.2 Emotion Elicitation Module

The Emotion Elicitation (EE) module outputs a modiﬁed VE based on the
following input: i) a representation of the current VE, ii) the emotion detected
by the ER module and the iii) the emotion that designer aims to evoke. Firstly,
the EE module computes a measure of distance between the emotion intended
by the designer and that recognized by the ER module. Then, based on this
distance, the properties of the current VE are appropriately modiﬁed (e.g., the
colour of a given virtual object is changed).

The main research questions that are still pending here are how to measure
the distance between emotions, and how to modify the VE accordingly. We
note that, in order to compute this distance, emotions should be better de-
scribed using a dimensional representation (e.g., in the valance/arousal plane),
which allows quantifying the diﬀerence among them. Then, the diﬃculty in
modifying a content to elicit emotions is a well-known problem, in particular
when advanced interactive media, such as VR, are considered [4]. In our view,
the ﬁrst step to tackle this problem is the deﬁnition of a representation of the
VE that includes, for instance, positions, semantic (i.e., functional role) and the
sensory qualities (e.g., shape and size) of the most salient virtual objects. The
second step is the deﬁnition of a model that, as a function of the detected emo-
tion, its distance with the target emotion, and the representation of the current

10

VE, returns an indication on how the VE should be modiﬁed. For instance, if
target emotion is joy, but the one detected by the ER module is sadness, the
colors of objects should be tuned to be more warmed. In our view, this task
is still very complex to be automatized, and would require the manual tuning
of VE’s characteristics. However, in case the VEE-loop became a tool of com-
mon use, it would function also as a tool for data collection. Speciﬁcally, the
tuning choices of designers, along with the emotional reactions of users, might
be collected and then used to train automatic systems (e.g., machine learning
algorithms).

4 Advancing the State of the Art

Early prototypes of the VEE-loop are present in the literature. For instance,
in [19] an architecture to perform users’ emotion-driven generation of a VE
is proposed and validated in the context of mental health treatment. Such
architecture is designed to detect users’ emotions from the analysis of multiple
types of bio-feedback (similarly to our ER module) and, accordingly, to generate
a VE to stabilize them, e.g., to induce calm (similarly to our EE module). Whilst
this existing approach is quite similar, in principle, to our idea of VEE-loop, it
has the main drawback of not considering complex models of interaction between
users and VE that, in our view, are essential to realize an instrument suitable for
emotion-driven product design (for instance, in Ref. [19] the generated VE is a
simple maze). Another work that investigates the use of VR as a tool to perform
ER and EE can be found in Ref. [20], where ER is performed using a very simple
machine learning algorithm that works on users’ electroencephalograms, while
EE is implemented using static VEs.
Instead, in order to be a suitable tool
for emotion-driven product design, the VEE-loop will consider a vast array of
heterogeneous bio-feedback, complex models of interaction between users and
the VE, dynamic VEs and more advanced machine learning algorithms.

Most of the research on ER is done on single-mode and standalone data (see
the recent survey [21]), which carry acted and exaggerated emotions. Instead,
the proposed framework allows considering streams of multi-mode data (which
introduce the challenge of identifying the onset and end of emotions) and ex-
ploiting the sense of presence typical of VR experience to induce (and then,
recognize) more spontaneous emotions [22].

Refs.

[1] and [11] describe methodological frameworks that designer can
follow to develop products with emotional intentions. However, as far as we
know, we are the ﬁrst to propose a technological solution that can help in the
design of emotion-driven products. We also remark that the proposed VEE-
loop can also be used to dynamically modify virtual products based on users’
emotions (e.g., a service of remote schooling). In this respect, similar previous
work (that, however, do not make use of VR technologies) are [23, 24, 25, 26] and
[6], which propose systems for emotion-driven recommendation and advertising,
respectively. Similarly, a digital system that adapts the characteristics of a
service based on users’ emotions is proposed in [27]. Then, Ref. [28] proposes a

11

gaming framework that changes the characteristics of the game based on users’
emotions. Finally, Ref. [29] describes the realization of a smart oﬃce in which
sensory features (e.g., light in the oﬃce) and tasks assigned to users are changed
to regulate their emotions.

5

Impact and Application

In this Section, we describe the main practical applications of the VEE-loop, as
well as its potential impact across several areas.

5.1 Areas of application

The VEE-loop is a versatile tool that opens the doors to a wide spectrum
of applications.
In particular, we identify the following three main areas of
applications: 1) product design, 2) virtual service delivery and 3) research in
emotion recognition and elicitation.

In product design, the VEE-loop can be used to validate the capability of
a product to fulﬁll its emotional requirements (i.e., to check the consistency
between intended and perceived emotions) before the actual and expensive tan-
gible production.
In fact, by exploiting the sense of presence given by VR,
the emotions experienced by the users are guaranteed to be as much similar as
possible to the real ones. Hence, users can try the virtual counterparts of the
products under development, and provide the designers with implicit feedback
about the goodness of their functional and stylistic choices. Note also that,
being the VEE-loop a digital tool, this validation can easily involve a higher
number of users with respect to traditional on site experiments, therefore in-
creasing their validity. Beside improving the product, the received feedback also
helps designers to better study their customers at the emotional level and to
understand which factors reinforce brand identiﬁcation [30, 31].

Moreover, services that are delivered using digital channels can beneﬁt from
the use of the VEE-loop. We refer, in particular, to services in the education
ﬁeld, where having the information on users’ emotional states is highly beneﬁcial
[5], but unavailable for some reasons (e.g., in remote schooling during the Covid-
19 pandemic), or in human-machine interaction, where the required equipment
is too much expensive or dangerous (e.g., in the training of practitioners in
industry). In this type of services, which are delivered in real-time, the VEE-
loop is implemented to adapt the VE to the current emotional status of the
users, e.g., to calm them down when they are anxious. This can be done by
modifying the sensory qualities of the virtual objects (as also done in product
design), as well as by adapting the learning tasks to enhance users’ experience.
Moreover, the VEE-loop can facilitate the transition towards an increasingly-
digitized society, where a number of services can be modiﬁed according to users’
emotions. In theatrical exhibitions, for instance, the VEE-loop can be exploited
to better understand the relation between emotions and acting.

12

Finally, the VEE-loop has the potential to advance the state of the art on
the growing ﬁelds of emotion recognition and elicitation. As for the former,
the VEE-loop can help to enhance current models for emotion recognition, e.g.,
by including the context embodied in the VE and users’ behaviours. As for
the latter, the VEE-loop can be exploited in many diﬀerent research areas to
better study the eﬀectiveness of elicitation conditions (e.g., in marketing, interior
design, etc.).

5.2 Potential Impact

In light of the numerous potential applications described before, our proposed
solution can potentially beneﬁt various dimensions of our society, as detailed in
the following.

Economical Impact The VEE loop ﬁnds applications in a countless number
of industrial sectors, while providing potential economical advantages both in
the production and in the marketing phases. For example, it can be used by
experts in advertising to understand what reinforces unique brand association,
or by designers to evaluate users’ emotional response to the characteristics of a
product before its tangible development. This allows designers to take more in-
formed decisions, therefore, reducing the risks (and associated costs) of creating
unsuccessful products and services.

Social Impact The VEE loop can help to deliver more empathetic services
using VR, therefore bringing a high social impact across many diﬀerent areas
(e.g., remote schooling). Potential applications can also target the treatment of
pathologies characterized by disorders on the emotional sphere [19] and collec-
tive training in emergency situations.

Environmental Impact The VEE loop integrates emotional aspects into
services delivered remotely, therefore, increasing their adoption. This has the
potential of improving remote working and practices, thus, limiting unnecessary
travels, and, in turn, reducing the emissions produced by means of transport.

Research Impact Our vision contributes to the research on ER and EE,
and provides a tool that researchers can readily use in the studies relative to
these ﬁelds. The VEE loop is a novel and timely solution that can be a poten-
tial cornerstone in many diﬀerent projects (from the research-oriented to the
more applicative ones), therefore, enabling transversal collaborations between
academy and industry.

Cultural Impact The VEE loop enables avant-garde cultural events deliv-
ered with VR. For instance, stylistic choices of a cultural event (e.g., in the-
atrical representations) can be modiﬁed according to the emotional response of

13

the audience (even if attending remotely) in real-time and in an economically-
sustainable manner. This asset can ﬁnd application in several cultural scenarios,
e.g., theatre, virtual city trip and museum virtual tours.

6 Discussion

In this paper, we have identiﬁed the iteration of emotion elicitation and recog-
nition phases as a desirable property of an emotion-driven design strategy.
We have then provided arguments to support the idea of using VR to realize
this iteration, that we have referred to as the Virtual-Reality-Based Emotion-
Elicitation-and-Recognition loop (VEE-loop). In brief, VR allows creating vir-
tual yet very realistic environments that designers can ﬂexibly modify to induce
speciﬁc emotional reactions. Indeed, VR inherits all the beneﬁts of digital tech-
nologies (e.g., ﬂexible and controlled content modiﬁcation), without sacriﬁcing
the realism of the experience. All these aspects render the VEE-loop a promis-
ing methodological and technological framework that can beneﬁt various areas,
such as product design, virtual service delivery and, more in general, the re-
search in emotion recognition and elicitation. However, several issues still need
to be properly tackled before its adoption. In the following, we summarize the
main characteristics of the VEE-loop, and we discuss corresponding opportuni-
ties and challenges.

Our most important claim is that the VEE-loop can help designers devel-
oping products capable to induce some speciﬁc emotion on their users. The
ﬂexible content adaptation and sense of immersion guaranteed by VR are just
a couple of reasons that support this claim. In fact, VR allows experimenting
a large number of virtual products’ characteristics, which can evoke emotions
similarly to their real counterpart. However, the right emotion elicitation con-
ditions are likely to be found after a number of iterations of the VEE-loop. The
main problem of this iterative approach is that the emotions that users perceive
are not only inﬂuenced by the current virtual content, but also by the number
of iteration itself (e.g., stress and boredom might arise after a long session of
experiments). This issue needs to be correctly tackled to not hinder the validity
of the performed experiments. When the VEE-loop is used in the design phase,
a possible approach consists in limiting the duration of the experiment to a
certain amount of time, and to validate the eﬀectiveness of emotion elicitation
conditions through statistical analysis (e.g., which emotions have been perceived
by the majority of the users involved in the experiments). As for experiments
done to understand the emotional reaction of a speciﬁc user, instead, a possible
strategy is to alternate VEs that carry an emotional content with VEs that are
emotionally neutral, so to bring the user back to her normal conditions.

More generally, VR allows simulating the context in which a product is used,
e.g., the surrounding environment and the after-use experience. In light of this,
the VEE-loop becomes even a more powerful design instrument, as designers
are free to experiment a higher number of emotion elicitation conditions, and
emotion recognition exploits a richer set of observations, e.g., both bio-feedback

14

signals and users’ behaviour. Two open research challenges here are i) how
to deﬁne suitable experiments to gain relevant insights from users’ behaviours,
which are an essential aspect of their emotions (e.g., how users interact with the
VE, which elements they observe, etc.) and ii) how to automatize the dynamic
modiﬁcation of the VE.

Then, the high level of immersion guaranteed by VR lead users to perceive
more spontaneous emotions that, for this reason, are a more valuable feedback
for the designers, but also more diﬃcult to identify. Indeed, most of the research
on emotion recognition is based on the analysis of emotions that are voluntar-
ily exaggerated and that, for this reason, are also easier to recognize. Finally,
emotions must be estimated from a stream of signals and not, as generally done
in previous work, from data that are assumed to carry a single emotion. There-
fore, a segmentation process is required to identify the instants of transitions
between two diﬀerent aﬀective states, before their actual classiﬁcation. To our
knowledge, the problem of segmentation is extensively considered in the action
recognition task, but quite unexplored in the emotion recognition one.

To conclude, the VEE-loop represents a promising methodological and tech-
nological framework that can be exploited to eﬀectively design emotion-driven
products, provided that several issues (both at the technological and method-
ological sides) are correctly handled.

References

[1] T. Alaniz and S. Biazzo, “Emotional design: the development of a pro-
cess to envision emotion-centric new product ideas,” Procedia Computer
Science, vol. 158, pp. 474–484, 2019.

[2] G. Zaltman, “The subconscious mind of the consumer (and how to
reach it),” Harvard Business School. Working Knowledge. Obtenido de
http://hbswk. hbs. edu/item/3246. html, 2003.

[3] C. Wrigley and K. Straker, Aﬀected: Emotionally engaging customers in

the digital age. John Wiley & Sons, 2019.

[4] G. Riva, F. Mantovani, C. S. Capideville, A. Preziosa, F. Morganti, D. Vil-
lani, A. Gaggioli, C. Botella, and M. Alca˜niz, “Aﬀective interactions using
virtual reality: the link between presence and emotions,” CyberPsychology
& Behavior, vol. 10, no. 1, pp. 45–56, 2007.

[5] A. Ortigosa, J. M. Mart´ın, and R. M. Carro, “Sentiment analysis in face-
book and its application to e-learning,” Computers in human behavior,
vol. 31, pp. 527–541, 2014.

[6] Y. Liu, O. Sourina, and M. R. Haﬁyyandi, “Eeg-based emotion-adaptive
advertising,” in 2013 Humaine Association Conference on Aﬀective Com-
puting and Intelligent Interaction.

IEEE, 2013, pp. 843–848.

15

[7] P. Desmet, “Designing emotions,” 2002.

[8] S. Sinek, Start with why: How great leaders inspire everyone to take action.

Penguin, 2009.

[9] N. H. Frijda et al., The emotions. Cambridge University Press, 1986.

[10] P. M. Desmet, S. F. Fokkinga, D. Ozkaramanli, and J. Yoon, “Emotion-
Elsevier, 2016, pp.

driven product design,” in Emotion Measurement.
405–426.

[11] C. Kim, J. Yoon, P. Desmet, and A. Pohlmeyer, “Designing for positive

emotions: Issues and emerging research directions,” 2021.

[12] J. Yoon, A. E. Pohlmeyer, and P. Desmet, “When’feeling good’is not good
enough: Seven key opportunities for emotional granularity in product de-
velopment,” International Journal of Design, vol. 10, no. 3, pp. 1–15, 2016.

[13] J. Diemer, G. W. Alpers, H. M. Peperkorn, Y. Shiban, and A. M¨uhlberger,
a
“The impact of perception and presence on emotional reactions:
review of research in virtual reality,” Frontiers in Psychology, vol. 6,
p. 26, 2015. [Online]. Available: https://www.frontiersin.org/article/10.
3389/fpsyg.2015.00026

[14] J. Mar´ın-Morales, J. L. Higuera-Trujillo, A. Greco, J. Guixeres,
C. Llinares, C. Gentili, E. P. Scilingo, M. Alca˜niz, and G. Valenza,
immersive-virtual emotional experience: Analysis of psycho-
“Real vs.
in a free exploration of an art museum,”
physiological patterns
[Online]. Available:
PLOS ONE, vol. 14, no. 10, pp. 1–24, 10 2019.
https://doi.org/10.1371/journal.pone.0223881

[15] S. K. Babu, S. Krishna, U. R., and R. R. Bhavani, “Virtual reality learning
environments for vocational education: A comparison study with conven-
tional instructional media on knowledge retention.” in 2018 IEEE 18th In-
ternational Conference on Advanced Learning Technologies (ICALT), 2018,
pp. 385–389.

[16] C. Floris, S. Solbiati, F. Landreani, G. Damato, B. Lenzi, V. Megale, and
E. G. Caiani, “Feasibility of heart rate and respiratory rate estimation by
inertial sensors embedded in a virtual reality headset,” Sensors, vol. 20,
no. 24, 2020. [Online]. Available: https://www.mdpi.com/1424-8220/20/
24/7168

[17] M. Buccoli, M. Zanoni, A. Sarti, S. Tubaro, and D. Andreoletti, “Unsuper-
vised feature learning for music structural analysis,” in 2016 24th European
Signal Processing Conference (EUSIPCO).

IEEE, 2016, pp. 993–997.

[18] A. Mehrabian, “Pleasure-arousal-dominance: A general framework for de-
scribing and measuring individual diﬀerences in temperament,” Current
Psychology, vol. 14, no. 4, pp. 261–292, 1996.

16

[19] S. B. i Badia, L. V. Quintero, M. S. Cameir˜ao, A. Chirico, S. Triberti,
P. Cipresso, and A. Gaggioli, “Toward emotionally adaptive virtual reality
for mental health applications,” IEEE journal of biomedical and health
informatics, vol. 23, no. 5, pp. 1877–1887, 2018.

[20] J. Mar´ın Morales, “Modelling human emotions using immersive virtual re-
ality, physiological signals and behavioural responses,” Ph.D. dissertation,
2020.

[21] A. Saxena, A. Khanna, and D. Gupta, “Emotion recognition and detection
methods: A comprehensive survey,” Journal of Artiﬁcial Intelligence and
Systems, vol. 2, no. 1, pp. 53–79, 2020.

[22] S. Susindar, M. Sadeghi, L. Huntington, A. Singer, and T. K. Ferris, “The
feeling is real: Emotion elicitation in virtual reality,” in Proceedings of the
Human Factors and Ergonomics Society Annual Meeting, vol. 63, no. 1.
SAGE Publications Sage CA: Los Angeles, CA, 2019, pp. 252–256.

[23] M. Polignano, F. Narducci, M. de Gemmis, and G. Semeraro, “Towards
emotion-aware recommender systems: an aﬀective coherence model based
on emotion-driven behaviors,” Expert Systems with Applications, vol. 170,
p. 114382, 2021.

[24] M. B. Mariappan, M. Suk, and B. Prabhakaran, “Facefetch: A user emotion
driven multimedia content recommendation system based on facial expres-
sion recognition,” in 2012 IEEE International Symposium on Multimedia.
IEEE, 2012, pp. 84–87.

[25] N. Sindhu, S. Jerritta, and R. Anjali, “Emotion driven mood enhancing
multimedia recommendation system using physiological signal,” in IOP
Conference Series: Materials Science and Engineering, vol. 1070, no. 1.
IOP Publishing, 2021, p. 012070.

[26] M. Rumiantcev and O. Khriyenko, “Emotion based music recommendation
system,” in Proceedings of Conference of Open Innovations Association
FRUCT. Fruct Oy, 2020.

[27] N. Condori-Fernandez, “Happyness:

an emotion-aware qos assurance
framework for enhancing user experience,” in 2017 IEEE/ACM 39th In-
ternational Conference on Software Engineering Companion (ICSE-C).
IEEE, 2017, pp. 235–237.

[28] M. S. Hossain, G. Muhammad, B. Song, M. M. Hassan, A. Alelaiwi, and
A. Alamri, “Audio–visual emotion-aware cloud gaming framework,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 25, no. 12,
pp. 2105–2118, 2015.

[29] S. Munoz, O. Araque, J. F. S´anchez-Rada, and C. A. Iglesias, “An emo-
tion aware task automation architecture based on semantic technologies for
smart oﬃces,” Sensors, vol. 18, no. 5, p. 1499, 2018.

17

[30] V. De Luca, “Emotions-based interactions: Design challenges for increasing

well-being,” 2016.

[31] ——, “Oltre l’interfaccia: emozioni e design dell’interazione per il be-

nessere,” MD Journal, vol. 1, no. 1, pp. 106–119, 2016.

18

