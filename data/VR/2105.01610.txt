Reliving the Dataset: Combining the Visualization
of Road Users’ Interactions with Scenario
Reconstruction in Virtual Reality

Lars T¨ottel∗, Maximilian Zipﬂ, Daniel Bogdoll, Marc Ren´e Zofka, and J. Marius Z¨ollner
Technical Cognitive Systems, FZI Research Center for Information Technology
Karlsruhe, Germany
Email: {toettel, zipﬂ, bogdoll, zofka, zoellner}@fzi.de

1
2
0
2

p
e
S
0
3

]

R
G
.
s
c
[

3
v
0
1
6
1
0
.
5
0
1
2
:
v
i
X
r
a

Abstract—One core challenge in the development of automated
vehicles is their capability to deal with a multitude of complex
trafﬁc scenarios with many, hard to predict trafﬁc participants.
As part of the iterative development process, it is necessary to
detect critical scenarios and generate knowledge from them to
improve the highly automated driving (HAD) function. In order
to tackle this challenge, numerous datasets have been released in
the past years, which act as the basis for the development and
testing of such algorithms. Nevertheless, the remaining challenges
are to ﬁnd relevant scenes, such as safety-critical corner cases,
in these datasets and to understand them completely.

Therefore, this paper presents a methodology to process and
analyze naturalistic motion datasets in two ways: on the one
hand, our approach maps scenes of the datasets to a generic
semantic scene graph which allows for a high-level and objective
analysis. Here, arbitrary criticality measures, e.g. TTC, RSS
or SFF, can be set to automatically detect critical scenarios
between trafﬁc participants. On the other hand, the scenarios are
recreated in a realistic virtual reality (VR) environment, which
allows for a subjective close-up analysis from multiple, interactive
perspectives.

Index Terms—Automated Driving, Data Analysis, Datasets,

Temporal Data, Virtual Reality

I. INTRODUCTION
Naturalistic motion datasets allow for the analysis of be-
havior and interactions of trafﬁc participants in a multitude
of scenarios1 and geographical areas. The data is mostly
collected from infrastructure, drones or ego vehicles and
sometimes combined with underlying high deﬁnition (HD)
maps [2]. A general issue in automated driving related datasets
is the heavy-tailed distribution of unusual, critical or surprising
events. This makes knowledge discovery especially hard, since
the detection and handling of such situations is key for the
robustness of an autonomous system [3]. Critical situations
are particularly relevant, as in such cases the safety of road
users may be at risk.

Besides ﬁnding critical situations such as corner-cases2
within a dataset, a methodology is needed for the visualization

This publication was written in the framework of European Union’s
Horizon 2020 Research and Innovation Programme under grant agreement no
815001, project Drive2theFuture (Needs, wants and behavior of ”Drivers”
and automated vehicles users today and into the future) and also partially
supported by the Intel Collaborative Research Institute for Safe Automated
Vehicles (ICRI-SAVe).

1We follow the deﬁnitions of scene and scenario by Ulbrich et al. [1]
2We follow the conceptualization of corner-cases by Heidecker et al. [4]

Fig. 1: A critical scene that was ﬁrst
identiﬁed from a
naturalistic motion dataset and later reconstructed in our sim-
ulation environment. The reconstruction allows for scenario
exploration in virtual reality, where the user has full control
over the time dimension and can relive the scenario from all
possible points of view.

of the scene considering all trafﬁc participants as well as their
interactions in an effective way. Thus, this paper proposes
an approach to utilize a three-dimensional, spatio-temporal
visualization based on the concept of Space Time Cubes [5],
[6] for application in microscopic road trafﬁc scenarios [7].
As a result, our approach enables and simpliﬁes visual relation
analysis of interacting trafﬁc participants at street level.

At the same time, we implement an interface to a high-
ﬁdelity simulator in order to recreate scenes from datasets
in a realistic environment. Using virtual reality (VR), we
enable users to immerse into the scene from any perspective,
including pre-deﬁned options such as the position of a driver of
any vehicle or pedestrian included in the dataset. As a result,
this integration allows for another close-up level of dataset
analysis. In order to demonstrate this analysis approach, we
also give an application example of a critical scene of a chosen
dataset. To the best of the authors’ knowledge, our approach
is the ﬁrst of its kind which makes use of VR in order to
reconstruct automated driving related datasets for the purpose
of close-up scenario analysis from multiple perspectives.

This paper is structured as follows: In Sec. II, we give an
overview of the related work in the ﬁeld of automated driving
related datasets and their analysis. In Sec. III, we introduce our

 
 
 
 
 
 
concept for the analysis of road trafﬁc datasets using various
criticality measures. In addition, we propose an approach to
visualize the analysis’ results within a virtual reality environ-
ment. Afterwards, we show the realization within a simulation
framework in Sec. IV, followed by an evaluation by applying
our approach to an exemplary dataset in Sec. V. Lastly, we
give a summary and outlook in Sec. VI.

II. RELATED WORK

A. Automated Driving Related Datasets

Automated driving related datasets are becoming more
numerous,
larger, diverse and have signiﬁcantly more an-
notated frames [8]. While the popular KITTI dataset from
2012 consists of 22 scenes with 15.000 annotated frames, the
more recent nuScenes and Waymo Open datasets consist of
thousands of scenes with hundreds of thousands of annotated
frames. The crowdsourced BDD100K dataset even consists of
100.000 scenes [9]–[12].

B. Dataset Analysis

Two core challenges need to be addressed - how to detect
data points, such as corner-cases, that are relevant for inspec-
tion, which are clearly the minority of the points and how to
visualize complex data in a way humans can comprehend. A
pipeline for the detection of relevant data in combination with
a VR based visualization has the potential of addressing this
issue [13]. The analysis of trafﬁc-related datasets is split into
two categories. The average occurrence of accidents or other
risks is described on the macroscopic layer, while the risk of
individual scenes are described on the microscopic layer [7].

C. Criticality Measures

Since we are interested in the detection of relevant scenes,
the applied measures correspond to microscopic risk analy-
sis. There are many classes of criticality measures, such as
Time-To-X or sampling methods, with a variety of concrete
implementations and combinations [7]. While these criticality
measures are mainly utilized to identify critical situations,
so called safety scores are designed to ensure the safety of
vehicle states [14]. Both are well suited for a post-processing
based analysis of datasets. These scores are based on safety
policies, such as the Mobileye Responsibility-Sensitive Safety
(RSS) [15], Nvidia Safety Force Field (SFF) [16] or TUM legal
safety frameworks [17]. The various measures and scores vary
in their ability to detect critical scenarios [7], and thus can be
dynamically exchanged or combined for analysis within our
proposed framework.

data and has been proven to be more successful and satisfying,
compared to classical visualization methods [20], [21]. In the
speciﬁc context of trafﬁc scenarios and automated vehicles,
virtual reality can be used for stated preference experiments
[22], which have been known to suffer from a lack of realism.
It can also be used for pedestrian behavior studies, especially
regarding risky situations [23], [24]. While some methods
are able to import real road networks and generate virtual
environments based on them, they lack the ability to utilize
real recordings of vehicle trajectories [22]. To the best of our
knowledge, there is only one VR application that fully imports
automated driving related datasets for the speciﬁc purpose of
improved data-labeling [25]. However, VR has not been used
in order to explore motion datasets in a high-ﬁdelity simulation
environment.

III. CONCEPT

A. Analysis Framework

The analysis framework presented here is based upon
naturalistic motion datasets. These mostly consist of object
lists that describe trafﬁc scenarios recorded over discrete
timestamps. In addition, many of the datasets also provide high
deﬁnition maps describing the road topology of the considered
location.

Our proposed framework for visual analysis consists of two
core components: an objective analysis (OA) module for the
detection of critical scenes and a subjective analysis (SA)
module for scenario recreation and knowledge discovery in
VR. The resulting processing pipeline for naturalistic motion
datasets is shown in Fig. 2.

Naturalistic
motion dataset

Semantic
scene graph

Criticality analysis

Objective Analysis:
Detection of Critical Scenes

Abstract
visualization

Scenario recreation
in a high-ﬁdelity
simulator

Knowledge
discovery in VR

Subjective Analysis:
Scenario Exploration in VR

D. Dataset VR Exploration

Virtual reality has a long history with many specialized
forms and applications, starting in the 1960s [18]. More
recently, mature and low cost virtual reality solutions have
attracted the interest of the research community, leading to a
growing number of VR-based applications [19]. The visual-
ization and analysis of high complexity datasets in VR can
provide users with new insights and perspectives about the

Fig. 2: Overview of the framework for visual analysis. Using
naturalistic motion datasets, we apply both an objective and a
subjective analysis. The objective analysis module is used to
ﬁnd critical scenes within the dataset using different metrics
and provides abstract visualisation options for scenes and their
surrounding scenarios. The subjective analysis module allows
for a reconstruction and an in-depth investigation of these
scenes in a VR environment.

For the objective analysis, we introduce a module that
assesses trafﬁc scenes by means of criticality measures and
safety models to detect critical situations. Input data con-
taining road users’ trajectories is processed and abstracted.
We implement an agnostic database interface which can deal
with different kinds of input and datasets. Then, for each
scene (i.e. discrete timestamp), a semantic scene graph is
created, which is used to apply a criticality analysis based on
various common measures. Finally, an abstract visualization
of road users provides a comprehensive overview of the
interactions between all trafﬁc participants. The visualization
consists of a 3D visualization of the scene graph’s relations and
the spatio-temporal visualization regarding trafﬁc participants’
trajectories.

For the subjective analysis, based on a transfer of the data
into the virtual reality environment, particularly critical situa-
tions can be analyzed interactively and additional conclusions
can be drawn. For example, a vehicle can occlude the driver’s
ﬁeld of view, which might not have been obvious from an
abstract, top-down perspective of the scene. Regarding this
aspect, we make use of our existing co-simulation approach
[26] which is based on the Robot Operating System (ROS)
[27].

In the following sections, we introduce and describe all
components of our OA and SA modules which are shown
in Fig. 2.

B. Semantic Scene Graph

As the ﬁrst component of the OA module, our state descrip-
tion [28] is designed to be particularly efﬁcient in determining
the criticality of individual road users in relation to other road
users. This generic abstraction allows it to apply almost freely
exchangeable severity measures.

Objects of the given motion dataset are mapped to the lanes
of the underlying road network. This leads to a reduction of
the Cartesian space onto pseudo Frenet coordinates. On basis
of the road topology given by the HD map, trafﬁc participants
can be brought into mutual relation.

Relation types are being divided into three classiﬁcations:
Vehicles traveling in the same lane (longitudinal relation),
lanes (lateral relation), and
vehicles traveling in adjacent
vehicles travelling in intersecting lanes (intersection relation).
This state of a scene can then be described using a directed
graph, where road users are described by nodes and their re-
lationships by edges. A visualization of the graph can be seen
in Fig. 3a. For more information regarding the composition of
the semantic scene graph, the reader is referred to [28].

C. Criticality Measures

To enable safe automated driving, there is a need for robust
perception and planning modules to handle a wide variety
of situations. The development and training of such methods
needs to incorporate corner case situations, which deviate from
regular trafﬁc scenes and are hard to ﬁnd in large datasets.
The most challenging corner cases for automated vehicles,
according to [4], are within the scene and scenario level. To

detect relevant scenes within the scene graph, it is necessary to
analyze the criticality of relations between trafﬁc participants.
Detected scenes can be used as a baseline for further evaluation
of critical scenarios.

As the second component of the OA module, we present
three different measures which are applicable within our
[29], Responsibility-
approach: Time-To-Collision (TTC)
Sensitive Safety (RSS) [15] and Safety Force Field (SFF) [16].
Our framework allows for the integration and combination of
various measures and scores, since they vary in their ability to
detect critical scenarios [7]. Therefore, this work focuses on
the demonstration of analysis and visualization capabilities,
instead of performing a full evaluation of the analysis results.
1) Time-To-Collision: Time-to-Collision (TTC) is one of
the most commonly used criticality measures, which describes
the time between two objects until they would collide with
each other if they were to continue moving based on their
current state [30]. In our case, we apply the constant velocity-
TTCA,B between two objects (A, B), which is deﬁned as
follows:

TTCA,B =

dAB
vB − vA

(1)

where dAB is the gap between object A and object B along
the lane and vA,B describes their current velocity, respectively.
TTC is usually applied for vehicles on the same road.
However, the euclidean distance of two road users, who are
not driving behind each other on the same lane, is mostly not
sufﬁcient to calculate the criticality through TTC. Therefore, a
potential intersection point pi is created by predicting the road
users according to their current driving direction. Then, pi is
used as a point in space for the calculation of the extension
TTCint to T T C:

TTCint =

dB,pi
vB

(2)

Here, vB is the velocity of the vehicle that arrives second at
the intersection point pi and the distance between the second
arriving object B and pi is denoted as dB,pi

.

Equation 2 only applies when the condition,
vehicles arrive at pi at the same time frame, is met.

that both

Due to the discontinuity of the TTC function, the TTC value
of an automated evaluation is often inappropriate if both ve-
locities of the considered vehicle are identical (vA − vB = 0).
Therefore, the inverse Time-to-Collision TTC−1 was presented
in [31]. The higher its value, the higher is the potential risk
to cause an accident. TTC−1 therefore provides a direct and
continuous function for describing the collision risk. Due to
this advantage, we rather apply the inverse Time-to-Collision
over its basic form.

2) Responsibility-Sensitive Safety: RSS [15] is a rigorous
mathematical model that formalizes an interpretation of the
law. RSS uses ﬁve common sense rules as a foundation for its
formulization. In order to implement these, RSS establishes
minimum safe distances and proper responses for different

circumstances. Similar to TTC, RSS also calculates the safety
distances based on the states of two road users. One special
feature of the RSS model is that it calculates response times
and acceleration values for individual road users. In our
approach, these vehicle parameters or person related values
are assumed to be constant and identical for all road users
of a class (Car, Bike, ...), since they are not included in the
examined datasets.

In our implementation, the RSS model does not provide a
continuous function as a result, but rather represents a binary
state. As soon as a safety margin is fallen short of, the trafﬁc
constellation is classiﬁed as critical.

3) Safety Force Field: SFF [16] is a safety-related theory to
prevent collisions by obstacle avoidance, if applied. To utilize
the SFF framework for the analysis of datasets, the safety
potential measure, which determines if actors are in a safe
state, is used.

In order to compute the safety potential ρA,B between
two actors A and B, their safety procedure trajectories are
checked for collisions and evaluated, see Eq. (3). Such a safety
procedure can be modelled as a hard stop. The safety potential
between A, B is 0, if no space-time collision is being detected.
It is strictly positive in all other cases, based on the individual
duration of the safety procedures tAstop, tBstop and the time
until they collide ct:

ρA,B = (cid:13)
(cid:0)tAstop − ct, tBstop − ct
(cid:13)

(cid:1)(cid:13)
(cid:13)p

(3)

While the complete SFF framework utilizes the measure and
concept of it to avoid unsafe states, even for non-visible actors,
the safety potential measure is sufﬁcient for the evaluation of
scene criticality.

Whereas a full implementation of the before mentioned RSS
framework can be found publicly as free software [32], SFF
is part of the commercial NVIDIA DriveWorks framework
[33]. Therefore, we have implemented the safety potential
calculation ourselves as an exemplary case when both road
users are driving on the same lane and thus have a longitudinal
relation (see Sec. III-B). The computation of the safety poten-
tial, including the trajectories of the safety procedure [16] and
the collision checks [34], is fully implemented in the Frenet
space. This allows us to implicitly utilize map-based lanes for
the safety procedure.

D. Abstract Scene and Scenario Visualization

As the third and ﬁnal component of the OA module, we have
implemented two abstract visualization functions for detected
relevant scenes. First, we present the Semantic Scene Graph
Visualization, which displays the scene graph on top of the
scene itself. This provides a fast overview of the relations
between the road users. Second, we show our Spatio-Temporal
Scenario Visualization, which allows for fast insights into the
surrounding scenario. Both types of visualization are shown
in Fig. 3.

1) Semantic Scene Graph Visualization: The computed crit-
icality between two trafﬁc participants A and B are integrated
into the visualization using a colored sphere between A, B and
additional edges to connect both trafﬁc vehicles to the sphere.
Here, darker spheres correspond to more critical relations,
whereas lighter spheres represent less critical relations. Such
a critical situation is shown in Fig. 3a.

2) Spatio-Temporal Scenario Visualization: For a spatio-
temporal visualization for road trafﬁc scenarios, the z coordi-
nate as well as the roll and pitch angles of trafﬁc participants
are neglected and only their two-dimensional (x, y) position
and yaw angle φ are used. Instead, the z-axis is used for the
time dimension.

The visualization is done as follows. First, we visualize the
road network on the spatial plane (x, y, z = 0). In order to
better evaluate the spatial position of individual road users, the
spatio-temporal trajectory is also projected onto the spatial
plane (x, y, z = 0). Equal spatial positions of the spatio-
temporal trajectory and the projected trajectory are connected
by vertical lines. Inserting them every n timesteps enables the
viewer to get a better comprehension of the trafﬁc participant’s
velocity: larger distances between two consecutive vertical
lines represent higher velocities. For that reason, when a
trafﬁc participant only moves vertically upwards in the three-
dimensional space, an accumulation of vertical lines at that
point can easily be interpreted as a stop point as shown in
Fig. 3b.

E. Scenario Recreation in a High-Fidelity Simulator

In order to visualize a dataset in a virtual reality environ-
ment, the complete scenario needs to be modelled. As the ﬁrst
part of the SA module, a methodology for its description is
needed. First introduced in the Pegasus research project [35],
Scholte et al. [36] reﬁned the 6-layer model for the description
of road trafﬁc scenarios including the environment. Here, layer
1 represents the road network and layer 2 represents trafﬁc
infrastructure, such as trafﬁc signs or trees next to the street.
Layer 3 deﬁnes all temporary modiﬁcations of the ﬁrst two
layers, e.g. construction sites on the street. Level 4 describes
all dynamic objects, level 5 describes the environment in terms
of weather, lighting and other conditions. The ﬁnal layer 6 is
used for digital information, such as trafﬁc light states or V2X
information.

Given a dataset, our approach to apply the above mentioned
model is shown in Fig. 4. Here, multiple conditions must be
fulﬁlled. For layer 1, 2 and 3, the dataset has to provide a high-
deﬁnition (HD) map of the road network including relevant
infrastructure in combination with a reference point in geodetic
coordinates (latitude, longitude).

Layer 4 concerns all trafﬁc participants captured in the
dataset. They must have at least a two-dimensional position
and orientation for every timestep as well as a distinct classi-
ﬁcation, e.g. Car or Pedestrian. Consequently, the object type
together with a position and orientation can be passed to the
VR simulation in order to move a 3D model of the object in
the simulated environment.

(a) 3D visualisation of a scene graph.

(b) Spatio-temporal visualization of a scenario.

Fig. 3: Abstract visualizations from the OA module. In (a), the scene graph described in III-B is being visualized. Trafﬁc
participants are displayed as colored boxes. Relations between them, built up by the scene graph, are displayed as white
lines and colored spheres, which represent their criticality status. Darker spheres represent more critical relations. In (b), the
spatio-temporal visualization of a scenario is shown. This enables a quick understanding of the scenario.

widely used in automated driving research. CARLA provides
assets for vehicles, cyclists and pedestrians, which we use to
visualize all agents included in the input datasets.

Furthermore, CARLA is based on Unreal Engine [39] which
allows us to make use of its VR capabilities. We implement
a generic VR view in CARLA, where users can immerse into
the simulation from the viewpoints of the inserted actors, i.e.
as pedestrians or drivers.

In addition, using CARLA and Unreal Engine not only
allows us to render the scene in a high-ﬁdelity 3D envi-
ronment and inﬂuence environmental conditions such as the
weather, but also enables the simulation of sensor data which
is not included in the dataset. For this, CARLA allows for
the attachment of different sensors, e.g. cameras, radars or
LIDARs, to any actor. Consequently, our approach also enables
data augmentation for the training of machine learning based
approaches, as it also can be used for sensor data generation.
Furthermore, CARLA provides different weather conditions,
enabling the exploration of scenarios from the dataset under
different conditions.

IV. REALIZATION

A. Objective Analysis Module

We realized the OA module, as shown in Fig. 6, using
ROS. Input from a naturalistic motion dataset is processed via
a dataset-agnostic interface and converted to respective ROS
types in order to allow handling different types of datasets. By
using ROS, input data can also come directly from intelligent
infrastructure, for example as implemented in the Test Area
Autonomous Driving Baden-W¨urttemberg (TAF-BW) [40].

Based on this, the semantic scene graph is constructed and
the criticality analysis is performed. The criticality analysis
is done by continuously evaluating and integrating different
criticality measures for the currently chosen timestamp. We
have implemented the Semantic Scene Graph and Spatio-
Temporal Scenario visualizations using RViz. For the semantic

Fig. 4: Workﬂow for the recreation of scenarios from datasets
in a 3D simulation environment. Using the dataset and data
from OpenStreetMap, both the static and dynamic environment
can be reconstructed.

Weather or lighting described by layer 5 is almost never
covered in naturalistic motion datasets, therefore, these effects
can be controlled and varied completely by the VR simulation.
If trafﬁc light state information is available from the dataset,
the data can be used for layer 6 and therefore be fed into the
respective simulated trafﬁc lights, otherwise we neglect the
information and turn off all trafﬁc lights.
An example of a dataset providing most of the required ele-
ments mentioned above, i.e. a high-deﬁnition map in Lanelet
[37] format, a geodetic reference point, time-referenced object
trajectories as well as trafﬁc light information in the form
of Signal Phase and Timing (SPaT) is the recently published
TAF-BW dataset [28].

F. Knowledge Discovery in Virtual Reality

As the second and ﬁnal part of the SA module, we immerse
into the reconstructed trafﬁc scenarios using a virtual reality
environment as shown in Fig. 8. Here, we make use of the
open-source simulator CARLA [38] in version 0.9.11 which is

DatasetSnapshot at timestep t3D environment List of traffic participants at time t3D assets with position and orientationInsertionAssignment Processing of Traffic ParticipantsConversion to metric coordinate systemCreation of 3D assetsGeneration of 3D environmentIncluded HD mapOpenStreetMapExtraction of infrastrucutureExtraction of road networkscene graph visualization, we buffer all timestamps from the
input data and allow the user to interactively change the
currently chosen value in order to gain full control over
the time dimension. A visualization of the semantic scene
graph on top of the scene is done by means of colored
spheres connected to two corresponding trafﬁc participants:
darker spheres symbolize more critical interactions, whereas
lighter ones correspond to less critical interactions. The used
criticality measure can be chosen.

Moreover, in order to not overload the visualization in the
case of many trafﬁc participants, we also add a threshhold
parameter for the criticality measure to limit the number of
spheres and edges included in the visualization. As a result, it
is possible to loop through the input data and only visualize
identiﬁed critical interactions, whereas all others can be ig-
nored. Thereby, it is possible to precisely ﬁnd and investigate
critical timestamps. Evaluation results, such as the outputs of
the applied measures and the corresponding timestamps, are
saved for later reuse.

For the investigation of the entire scenario, i.e. of more
than one timestamp, our spatio-temporal visualization is ap-
plied. This visualization includes past trajectories of all trafﬁc
participants, and eventual spatial conﬂicts can be resolved by
using the time as the z-axis (see Fig. 3b).

In summary, using our visual analysis framework, users can
inspect all timestamps included in a dataset and inspect critical
scenes using abstract, three-dimensional visualizations.

B. Subjective Analysis Module

For the subjective analysis consisting of the recreation of
scenarios and the immersion in virtual reality, we make use
of HD maps and create 3D worlds within Unreal Engine.
Common open-source HD map data formats are OpenDRIVE
[41] or Lanelet [37], which can in turn be parsed in order to
create a model of the road. If no map is provided, a geodetic
reference point or background knowledge about the location
of the dataset can be used to create the road model from
OpenStreetMap data. However, this data is not as accurate
as HD maps and might not cover all information.

Afterwards, additional elements such as buildings or veg-
etation are added to the 3D world either manually or auto-
matically by again parsing and converting data from Open-
StreetMap. The 3D world is then integrated into CARLA, and
trafﬁc participants are now inserted and moved by exchanging
ROS messages. Here, we make use of our ROS-based co-
simulation architecture [26]: CARLA is integrated as a visu-
alization model, and an additional module is used to integrate
trafﬁc participants from the given dataset into the 3D world at
any timestamp.

For knowledge discovery in VR, we make use of our VR
hardware setup introduced in [26] and shown in Fig. 5. We
utilize a HTC VIVE VR headset and four SteamVR base
stations, and data is transmitted wirelessly to the headset.
The setup includes both a walking area for the immersion
as a pedestrian or external observer and a driver’s seat for the
immersion as a driver.

Fig. 5: The virtual reality setup in our laboratory. The setup
provides a walking area in order to move within the recreated
trafﬁc constellation.

Moreover, CARLA’s architecture allows us to setup multi-
ple, distributed computers for the VR environment: we run the
simulation server including rendering on a computer running
Windows 10 since many VR headset only support Windows,
and we run the dataset processing and interaction analysis on
another computer running Linux.

V. EVALUATION

For the demonstration of our methodology, we chose the
following approach: ﬁrst, we analyze a given dataset using
the criticality measures described in Sec. III-C. Second, we
visualize one of the most critical timestamps we found. Third
and last, we recreate the same setting in the VR environment
in order to obtain an even deeper insight into the scene and
the surrounding scenario.

The dataset we analyze using the criticality measures from
Sec. III-C is the Test Area Autonomous Driving Baden-
W¨urttemberg dataset [28]. The results of our analysis using
inverse TTC, RSS and SFF are depicted in Fig. 6. Here, in all
three diagrams, the x-axis shows the timestamps included in
the dataset. The computed value of the respective measure is
plotted on the y-axis, using the maximum value of all trafﬁc
participants for the respective timestamp. Both the inverse
TTC and the SFF model describe more critical scenes with
higher criticality values. Since the RSS model provides a
binary output, each trafﬁc scene in which any trafﬁc participant
falls below a safe-distance threshold is marked as critical and
displayed as a high.

While TTC and SFF classify the criticality of scenes dif-
ferently, signiﬁcant overlaps can be seen in multiple times-
tamps and time intervals. However, there are also intervals
that are only critical in one of the two measures, e.g. in
t ∈ [27000 ms, 43000 ms], there are two very critical scenes
according to SFF, but they are much less critical according

is 0.154 seconds and at the second peak at t2 = 119200 ms,
the TTC is 0.164 seconds. Since the second peak is also
identiﬁed by the SFF measure, we chose t2 = 119200 ms
for our demonstration, which is the scene shown in Fig. 1.

To visually analyze the critical scene at t2 = 119200 ms,
the objective analysis module is utilized. In Fig. 3a, a close
up of the critical scene is shown. Relations built up by the
scene graph are displayed as white lines connecting two trafﬁc
participants by means of a colored sphere (see Sec. III-B).
When a relation becomes more critical, the sphere turns from
yellow to red (see relation between vehicle id = 41 and
vehicle id = 42). The abstract top-down visualization in Fig.
8a shows that the vehicles are driving in parallel and that they
are very close to each other.

Finally, we recreate the scenario in our simulation environ-
ment and immerse into the situation using a VR head-mounted
display. Snapshots of the critical scene at t2 = 119200 ms
are shown in Fig. 8. Now, in the VR environment, the user is
consecutively seated at the driver’s seat of the two critically
interacting vehicles, where he can look around and therefore is
able to gain more subjective insights into the trafﬁc scenario.
In addition, the user also has full control of going backwards
and forwards in time.

In our example, we conclude that the critical scene resulted
from two vehicles driving in parallel and very close to each
other. The effect was enforced due to the fact that the pre-
ceding vehicle was not driving in parallel to its own lane, but
rather turned slightly to the left lane. One possible explanation
of this critical situation could be that the red vehicle with
track id = 42 was in a blind spot and could not be seen
by the driver of the vehicle with track id = 41. However,
the vehicles’ positions and orientations might also have been
affected by sensor noise.

In summary, the objective analysis revealed critical scenes
as input for the visual analysis. The abstract visualization
allows the user to generate knowledge about the scene and
the criticality relations between the road users. The ﬁnal
reconstruction of the scene in simulation and VR enables a
more subjective and deeper exploration of the scene from
different perspectives. In particular, driver perspectives of the
vehicles with high-criticality relations provide a high level of
insight and are available as presets in the SA module.

VI. CONCLUSION AND OUTLOOK

Within the objective analysis (OA) module, we have pro-
posed a combination of the analysis of interactive driving sce-
narios from naturalistic motion datasets, using various critical-
ity measures, with two types of visualization approaches. We
applied multiple criticality measures in order to speciﬁcally
select scenes within the dataset which are worth investigating.
On the one hand, we used abstract visualization methods in
order to inspect the relations and criticality between interacting
trafﬁc participants as the last part of the OA module. On the
other hand, we also reconstructed the scene in a virtual reality
environment in order to inspect the recorded trafﬁc situation
from the perspective of drivers and consequently to obtain

Fig. 6: Criticality analysis of all timestamps of the TAF-BW
dataset using the inverse TTC, RSS and SFF models. The red
box highlights an exemplary critical interval.

Fig. 7: Close up analysis of the highlighted interval (see Fig.
6). The criticality status (inverse TTC, RSS, SFF) of each
individual trafﬁc participant visible in this interval is plotted.
We identify t2 = 119200 ms as the most critical timestamp
according to TTC, which is highlighted by the red line.

to TTC. Concurrently, RSS also yields peeks for all of these
timestamps.

We chose to use the maximum value of the inverse TTC
measure for our demonstration, and it can be seen that it is
within t1 = 117400 ms and t2 = 119200 ms, highlighted by
a red box. Therefore, we investigate this time interval in more
detail in Fig. 7.

Here, the courses of inverse TTC, RSS and SFF for all
involved trafﬁc participants are depicted. All measures iden-
tify the vehicles with track id = 42 (orange lines) and
with track id = 41 (blue lines) as the ones interacting
critically in the time interval. In the ﬁrst peak of inverse
TTC at t1 = 117400 ms, the computed Time-To-Collision

(a) Top-down perspective of the abtract scene.

(b) Top-down perspective of the reconstructed scene.

(c) Front view of immersive driver of vehicle with track id = 42.

(d) Side view of immersive driver of vehicle with track id = 41.

Fig. 8: The reconstructed scene from the TAF-BW dataset at t1 = 119200 ms. In (a), an abstract top-down perspective of
the scenario visualized in RViz is shown. In (b), the same scene is shown after it was reconstructed within our simulation
environment. Here, the two critically interacting vehicles are highlighted by a red circle: the vehicle with track id = 42 (Tesla
Model 3, red vehicle) moves in parallel and very close to the vehicle with track id = 41 (Dodge Charger, black vehicle). In
(c) and (d), different ﬁelds of vision from the driver’s seats of both vehicles are shown. The immersion in VR allows the user
to look around and to closely investigate the trafﬁc constellation.

subjective insights into the scene as ﬁnal part of the subjective
analysis (SA) module.

The resulting approach can be applied to common natu-
ralistic motion datasets such as INTERACTION [42], TAF-
BW [28] or inD [43]. Furthermore, it yields a comprehensive
visualization of the interactions between trafﬁc participants
using arbitrary measures such as TTC, RSS and SFF, making
it easier to ﬁnd corner-cases, such as critical situations, and
understand the roles of the respective trafﬁc participants in
the scene or scenario. Due to the modular structure of our ap-
proach, some components can also be utilized for the analysis
of online data streams. This might be the case for processed
sensor data recorded by an automated vehicle, but also for data
coming from intelligent infrastructure. Such infrastructure is
available, for example, in the Test Area Autonomous Driving
Baden-W¨urttemberg [40].

In future studies, we will apply the proposed approach in
scenarios including both automated vehicles, non-automated
vehicles and vulnerable road users. Since our implementations

of TTC, RSS and SFF are only examples to demonstrate the
functionality, a proper combination of analysis measures might
be considered in the future. Consequently, building upon the
knowledge discovered from our OA and SA modules, we plan
to generate a realistic, virtual dataset containing such trafﬁc
scenarios with focus on vulnerable road users. This includes,
amongst others, situations where pedestrians disobey trafﬁc
rules and/or trafﬁc lights.

REFERENCES

[1] S. Ulbrich, T. Menzel, A. Reschka, F. Schuldt, and M. Maurer, “Deﬁning
and Substantiating the Terms Scene, Situation, and Scenario for Auto-
mated Driving,” IEEE Conference on Intelligent Transportation Systems,
Proceedings, ITSC, vol. 2015-October, pp. 982–988, 2015.

[2] Y. Huang and Y. Chen, “Autonomous driving with deep learning: A

survey of state-of-art technologies,” 2020.

[3] P. Koopman, “The Heavy Tail Safety Ceiling,” Automated and
Connected Vehicle Systems Testing Symposium, pp. 1–2, 2018. [Online].
Available: http://users.ece.cmu.edu/$\sim$koopman

[4] F. Heidecker, J. Breitenstein, K. R¨osch, J. L¨ohdeﬁnk, M. Bieshaar,
C. Stiller, T. Fingscheidt, and B. Sick, “An application-driven concep-
tualization of corner cases for perception in highly automated driving,”
2021.

[5] T. H¨agerstrand, “What about people in regional science?” in Papers of

the Regional Science Association, vol. 24, 1970.

[6] M.-j. Kraak, “The Space-Time Cube Revisited from a Geovisualization
Perspective,” 21st International Cartographic Conference (ICC), no.
August, pp. 10–16, 2003.

[7] P. M. Junietz, “Microscopic and macroscopic risk metrics for the safety
validation of automated driving,” TU Darmstadt, Darmstadt, 2019.
[Online]. Available: http://tuprints.ulb.tu-darmstadt.de/9282/

[8] C. ´E. N. Laﬂamme, P. Gigu`ere, and F. Pomerleau, “Driving datasets

literature review,” arXiv, 2019.

[9] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the KITTI vision benchmark suite,” Proceedings of the IEEE
Computer Society Conference on Computer Vision and Pattern Recog-
nition, pp. 3354–3361, 2012.

[10] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam,
H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi,
Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, “Scalability in perception
for autonomous driving: Waymo open dataset,” Proceedings of
the
IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pp. 2443–2451, 2020.

[11] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Kr-
ishnan, Y. Pan, G. Baldan, and O. Beijbom, “Nuscenes: A multimodal
dataset for autonomous driving,” Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, no.
March, pp. 11 618–11 628, 2020.

[12] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and
T. Darrell, “BDD100K: A Diverse Driving Dataset for Heterogeneous
Multitask Learning,” Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition, pp. 2633–2642,
2020.

[13] A. Van Dam, D. H. Laidlaw, and R. M. Simpson, “Experiments in
immersive virtual reality for scientiﬁc visualization,” Computers and
Graphics (Pergamon), vol. 26, no. 4, pp. 535–555, 2002.

[14] H. Zhao, Y. Zhang, P. Meng, H. Shi, L. E. Li, T. Lou, and J. Zhao,
“Safety Score: A Quantitative Approach to Guiding Safety-Aware Au-
tonomous Vehicle Computing System Design,” IEEE Intelligent Vehicles
Symposium, Proceedings, no. Iv, pp. 1479–1485, 2020.

[16] D. Nist´er, H.-L.

[15] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “On a Formal Model
of Safe and Scalable Self-driving Cars,” arXiv, pp. 1–37, 2017.
and Y. Wang,
2019.

“The
J. Ng,
NVIDIA White
Safety
[On-
line]. Available: https://www.nvidia.com/content/dam/en-zz/Solutions/
self-driving-cars/safety-force-ﬁeld/the-safety-force-ﬁeld.pdf

Field,”

Paper,

Force

Lee,

[17] C. Pek, S. Manzinger, M. Koschi, and M. Althoff, “Using online
veriﬁcation to prevent autonomous vehicles from causing accidents,”
Nature Machine Intelligence, vol. 2, no. 9, pp. 518–528, 2020. [Online].
Available: http://dx.doi.org/10.1038/s42256-020-0225-y

[18] M. Okechukwu and F. Udoka, “Understanding Virtual Reality Technol-
ogy: Advances and Applications,” Advances in Computer Science and
Engineering, no. June 2015, 2011.

[19] P. Cipresso, I. A. C. Giglioli, M. A. Raya, and G. Riva, “The past,
present, and future of virtual and augmented reality research: A network
and cluster analysis of the literature,” Frontiers in Psychology, vol. 9,
no. NOV, pp. 1–20, 2018.

[20] P. Millais, S. L. Jones, and R. Kelly, “Exploring data in virtual reality:
Comparisons with 2d data visualizations,” Conference on Human Fac-
tors in Computing Systems - Proceedings, vol. 2018-April, pp. 5–10,
2018.

[21] N. Reski and A. Alissandrakis, “Open data exploration in virtual
technology,” Virtual Reality,
reality: a comparative study of input
vol. 24, no. 1, pp. 1–22, 2020. [Online]. Available: https://doi.org/10.
1007/s10055-019-00378-w

[22] B. Farooq, E. Cherchi, and A. Sobhani, “Virtual Immersive reality
for stated preference travel behaviour experiments: A case study of
autonomous Vehicles on Urban Roads,” arXiv, 2018.

[23] H. Luo, T. Yang, S. Kwon, M. Zuo, W. Li, and I. Choi,
“Using virtual
reality to identify and modify risky pedestrian
behaviors amongst Chinese children,” Trafﬁc Injury Prevention,
vol. 21, no. 1, pp. 108–113, 2020.
[Online]. Available: https:
//doi.org/10.1080/15389588.2019.1694667

[24] A. Meir, T. Oron-Gilad, and Y. Parmet, “Are child-pedestrians able to
identify hazardous trafﬁc situations? Measuring their abilities in a virtual
reality environment,” Safety Science, vol. 80, pp. 33–40, dec 2015.

[25] F. Wirth, J. Quchl, J. Ota, and C. Stiller, “PointAtMe: Efﬁcient 3D point
cloud labeling in virtual reality,” IEEE Intelligent Vehicles Symposium,
Proceedings, vol. 2019-June, no. Iv, pp. 1693–1698, 2019.

[26] M. R. Zofka, L. T¨ottel, M. Zipﬂ, M. Heinrich, T. Fleck, P. Schulz,
and J. M. Z¨ollner, “Pushing ros towards the dark side: A ros-based co-
simulation architecture for mixed-reality test systems for autonomous
vehicles,” in 2020 IEEE International Conference on Multisensor Fusion
and Integration for Intelligent Systems (MFI), 2020, pp. 204–211.
[27] M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating system,”
in ICRA Workshop on Open Source Software, 2009.

[28] M. Zipﬂ, T. Fleck, M. R. Zofka, and J. M. Z¨ollner, “From trafﬁc
sensor data to semantic trafﬁc descriptions: The test area autonomous
driving baden-w¨urttemberg dataset (taf-bw dataset),” in 2020 IEEE 23rd
International Conference on Intelligent Transportation Systems (ITSC).
IEEE, 2020, pp. 1–7.

[29] “ISO 22839:2013: Intelligent

transport systems — forward vehicle
collision mitigation systems — operation, performance, and veri-
ﬁcation requirements,” Sep 2018. [Online]. Available: https://www.iso.
org/standard/45339.html

[30] L. Zheng, K. Ismail, and X. Meng, “Trafﬁc conﬂict techniques for road
safety analysis: Open questions and some insights,” Canadian Journal
of Civil Engineering, vol. 41, no. 7, pp. 633–641, 2014.

[31] V. E. Balas and M. M. Balas, “Driver assisting by inverse time to
collision,” 2006 World Automation Congress, WAC’06, no. June 2014,
pp. 1–7, 2006.

[32] Intel, “C++ Library for Responsibility Sensitive Safety,” https://github.

com/intel/ad-rss-lib, 2021.

[33] Nvidia, “DriveWorks SDK Reference - Safety Force Field,” https://docs.
nvidia.com/drive/driveworks-3.0/safetyforceﬁeld mainsection.html,
2020.

[34] W. Xu, W. Yao, H. Zhao, and H. Zha, “A vehicle model for micro-
trafﬁc simulation in dynamic urban scenarios,” Proceedings - IEEE
International Conference on Robotics and Automation, pp. 2267–2274,
2011.

and B. Bender
scenario

[35] L. Dixon, N. Clancy, B. M. Miller, S. Hoegberg, M. M.
and
Lewis,
knowledge-based
Santa
Monica, CA, USA, RR-1776-NYCEDC, 2018. [Online], accessed:
2021-02-19.
[Online]. Available: https://www.pegasusprojekt.de/ﬁles/
tmpl/Pegasus-Abschlussveranstaltung/05 Scenario Description and
Knowledge-Based Scenario Generation.pdf

““scenario
et
generation,” RAND Corp.,

description

al.,

[36] M. Scholtes, L. Westhofen, L. R. Turner, K. Lotto, M. Schuldes,
H. Weber, N. Wagener, C. Neurohr, M. Bollmann, F. K¨ortke, J. Hiller,
M. Hoss, J. Bock, and L. Eckstein, “6-layer model for a structured
description and categorization of urban trafﬁc and environment,” 2021.
[37] P. Bender, J. Ziegler, and C. Stiller, “Lanelets: Efﬁcient map represen-
tation for autonomous driving,” IEEE Intelligent Vehicles Symposium,
Proceedings, no. Iv, pp. 420–425, 2014.

[38] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proceedings of the 1st
Annual Conference on Robot Learning, 2017, pp. 1–16.

[39] “Unreal engine,” https://www.unrealengine.com/en-US/, accessed: 2021-

02-24.

[40] T. Fleck, K. Daaboul, M. Weber, P. Sch¨orner, M. Wehmer, J. Doll, S. Orf,
N. Sußmann, C. Hubschneider, M. R. Zofka, F. Kuhnt, R. Kohlhaas,
I. Baumgart, R. Z¨ollner, and J. M. Z¨ollner, “Towards large scale urban
trafﬁc reference data: Smart infrastructure in the test area autonomous
driving baden-w¨urttemberg,” in IAS, 2018.

[41] M. Dupuis, M. Strobl, and H. Grezlikowski, “Opendrive 2010 and
beyond–status and future of the de facto standard for the description of
road networks,” in Proc. of the Driving Simulation Conference Europe,
2010, pp. 231–242.

[42] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann,
J. K¨ummerle, H. K¨onigshof, C. Stiller, A. de La Fortelle, and
M. Tomizuka, “Interaction dataset: An international, adversarial and
cooperative motion dataset in interactive driving scenarios with semantic
maps,” arXiv:1910.03088 [cs, eess], Sep. 2019.

[43] J. Bock, R. Krajewski, T. Moers, S. Runde, L. Vater, and L. Eckstein,
“The ind dataset: A drone dataset of naturalistic road user trajectories
at german intersections,” 2019.

