Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear
Latency in Distributed Virtual Reality Systems

Armin Becher∗, Jens Angerer†, Thomas Grauschopf(cid:63)

∗ Technische Hochschule Ingolstadt
Esplanade 10
85049 Ingolstadt
armin.becher@thi.de

† AUDI AG
Auto-Union-Straße 1
85045 Ingolstadt
jens.angerer@audi.de

(cid:63) Technische Hochschule Ingolstadt
Esplanade 10
85049 Ingolstadt
thomas.grauschopf@thi.de

Abstract: Distributed Virtual Reality systems enable globally dispersed users to interact with
each other in a shared virtual environment. In such systems, different types of latencies occur. For
a good VR experience, they need to be controlled. The time delay between the user’s head motion
and the corresponding display output of the VR system might lead to adverse effects such as a
reduced sense of presence or motion sickness. Additionally, high network latency among world-
wide locations makes collaboration between users more difﬁcult and leads to misunderstandings.
To evaluate the performance and optimize dispersed VR solutions it is therefore important to mea-
sure those delays. In this work, a novel, easy to set up, and inexpensive method to measure local
and remote system latency will be described. The measuring setup consists of a microcontroller,
a microphone, a piezo buzzer, a photosensor, and a potentiometer. With these components, it is
possible to measure motion-to-photon and mouth-to-ear latency of various VR systems. By using
GPS-receivers for timecode-synchronization it is also possible to obtain the end-to-end delays be-
tween different worldwide locations. The described system was used to measure local and remote
latencies of two HMD based distributed VR systems.

Keywords: end-to-end latency; motion-to-photon latency, latency distributed VR; GPS synchro-
nization; measure latency; Virtual Reality

1

Introduction

A simple prototype of a distributed Virtual Reality (VR) for remote collaboration in vehicle design
was already built in 1997 [LD97]. With new head-mounted displays (HMDs) like the HTC Vive
and Oculus CV1, high-quality VR became affordable for many users worldwide.

Although current distributed VR systems are signiﬁcantly better than the prototype from 1997,
there are still many problems to be solved before those systems can replace face-to-face contact.
One of the challenges with Virtual Reality in general, and distributed VR in particular, are latencies.
Normand et al. stated, ”[...] further study is required regarding the performance of the network
communication, in particular regarding tolerance over the network’s latency or the minimal ratio
quality/latency required.” [NST+12]. Other studies have shown that delays in VR systems have a
big impact on user experience and need to be considered when designing distributed VR environ-
ments [Bro99, MRWB03, SBHM15, VGB]. Therefore, methods to measure and control latencies

8
1
0
2

p
e
S
7
1

]

R
G
.
s
c
[

1
v
0
2
3
6
0
.
9
0
8
1
:
v
i
X
r
a

 
 
 
 
 
 
are required.

The time delay between movement and the corresponding display output is sometimes referred
to as motion-to-photon latency [RNDM14]. Locally this delay is perceived by a user if he or she
makes a head movement and the corresponding display output is delayed by several milliseconds.
If two or more users share the same virtual environment, remote latencies occur between different
worldwide VR locations.

Another type of delay in dispersed VR systems is audio latency. In this paper, audio latencies
refer to the time delay between the generation of a sound signal until it is received at the local or re-
mote output. Therefore, audio latency will further be referred to as mouth-to-ear latencies [Tim03].
If users speak to each other inside a shared virtual environment, their voices need to be recorded
and transferred over a network. With high mouth-to-ear latency, it is more difﬁcult to communicate
with each other naturally. For instance, if the audio delay between the communication partners is
high, a user might start talking while another is already speaking.

The described measurement method allows scientists to further investigate the sources and
effects of latencies in distributed VR systems. It can also help manufacturers and developers to
evaluate VR hardware and applications.

In the next section of this paper, a quick overview on previous research work will be given.
Afterwards, the novel measurement system will be presented. In the method description section,
further technical details of the system will be provided. Then, the latency measurement results for
two distributed VR systems will be shown. Afterwards, the accuracy of the described system gets
examined. In the last section, a conclusion is given and plans for the future will be outlined.

2 Previous work

2.1 Local motion-to-photon latency

In 2014, S. Friston and A. Steed characterized and compared three different approaches to measure
motion-to-photon latency in VR systems [FS14]. They named them Di Luca’s Method, Sine Fitting
Method and Automated Frame Counting Method.

For the Sine Fitting Method a video camera was used to measure local delays in a VR system.
A 3D object was placed in the virtual scene and conﬁgured to follow the captured movements of
a tracking system. A tracked object in the real world was attached to a pendulum and moved in
a sinusoidal pattern. Both, the VR display showing the virtual object moving along the screen
and the tracked object in the real world were captured with a video camera. Due to the harmonic
oscillation of both objects a sine wave was ﬁtted to the motion of each object. The resulting
phase-shift between both waves was considered to be the motion-to-photon latency of the VR
system [Ste].

In Di Luca’s Method instead of a video camera, two light sensing photodiodes were used to
measure latency. One sensor was attached to the tracked object which was moved back and forth in
front of a linear gradient from black to white. The other diode was attached to the VR display panel.
A virtual gradient following the movement of the tracked object in the real world was displayed on

the VR monitor. The captured luminance values of both photodiodes were recorded with the sound
card of a PC. After the measurement, both resulting waveforms were cross-correlated to determine
the motion-to-photon latency of the VR system [Di 10].

To measure motion-to-photon delay when using Frame Counting, a high-speed camera is re-
quired. Like the Sine-Fitting Method a video camera is used to record the VR display and a
tracked object in the real world simultaneously. A virtual 3D object following the movements of
the tracked target is added to the VR scene. After capturing the high frame rate footage the time
offset between the virtual and the tracked object can be determined. For example, if the real object
reaches the peak position of the pendulum swing, the video frames until the virtual object gets to
the corresponding position in the virtual world can be counted. Because manually counting frames
is time-consuming and error-prone, an automatic Frame Counting application has been developed
by S. Friston and A. Steed [FS14]. Instead of manually analyzing the video footage, MATLAB
scripts were used to compute end-to-end latency automatically.

Another measurement setup described by Raaen et al. was designed to measure the motion-
to-photon delay of HMDs and smartphones [RK15]. They used two light sensors, a laser pen, and
an oscilloscope to detect the delay of a VR system. One diode was attached to the VR display.
The laser pen was mounted on the HMD or smartphone and pointing towards another diode next
to the VR device. Both diodes were connected to an oscilloscope. If the VR system was rotated
in one direction, the VR display switched from white to black, and the laser pen did not hit the
photosensor anymore. The latency was determined by measuring the time difference until both
diodes switched from bright to dark.

2.2 Remote motion-to-photon latency

To measure the delay between different dispersed virtual environments, timecode synchronization
is needed. Purdy et al. described a hardware system to measure latencies in distributed ﬂight
simulators [PBJ+98]. With the described system, it was possible to determine when a pilot in one
simulator perceived virtual aircraft movements of another simulator.

In 2009, Roberts et al. introduced a method to measure the end-to-end latency of distributed
collaborative environments by using two video cameras [RDM+09]. With an initial frame lock
(genlock) the timecodes of both cameras were synchronized before actually taking a measure-
ment, after which Manual Frame Counting was used to determine the end-to-end latency between
different locations.

2.3 Mouth-to-ear latency

In most distributed virtual environments users communicate with each other using microphones
and output devices such as headphones or speakers. If noise canceling headphones are used, it is
necessary to feedback the user’s voice because otherwise he or she would not hear his or her voice
while speaking. In that case, measuring the local mouth-to-ear latency is also important because
hearing the own voice with a high delay can be distracting.

Agastya et al.

investigated the mouth-to-ear latency in VoIP clients and described a simple
method to measure those delays [AMK09]. First, they generated an input signal. This signal was
split into two identical signals which were fed into a laptop and the caller machine’s mic-in jack.
The output of the callee machine’s headphone jack was also fed into the laptop. Later a signal was
generated, and the offset between generation and headphone output was compared to calculate the
mouth-to-ear latency.

3 Overview

In this paper, an open-source system to measure
local and remote motion-to-photon latency will
be presented. Compared to existing measurement
techniques, the described method is specially de-
signed for distributed VR systems.
It will help
to efﬁciently evaluate different setups regarding
motion-to-photon and mouth-to-ear latencies.

The game-engine Unity 3D is used to simulate
the virtual world. The source code of the project
can be downloaded from GitHub 1.

3.1 Motion-to-photon latency

Motion and the corresponding display output of a
VR system are captured by a potentiometer and a
photosensor which are both connected to a micro-
controller. An object tracked by the VR system
is mounted on the potentiometer and automatically
rotated back and forth. The motion is controlled
by a servo motor connected to the microcontroller.
By monitoring the horizontal rotation with the po-
tentiometer, it is possible to determine the rotary
position of the real object at every interval.

To get the delay between the rotation of the real
object and the corresponding display output, the
latest angle captured by the motion-tracking sys-
tem is presented on the VR display. The displayed
value is converted into a brightness code so that it
can be read with the microcontroller. By attaching
the measurement unit’s photosensor to the display,
it is possible to capture this brightness code and

1https://github.com/vr-thi/VRLate

Figure 1: Latency Measurement setup

NetworkHMD with external trackingPotiVR SimulationMicrocontrollerGPS receiverPhotosensorServoPotiMicrocontrollerGPS receiverServoVR SimulationMicBuzzerHeadsetHMD with external trackingPhotosensorMicBuzzerHeadsettherefore the angle which was used by the VR system to render the current frame. The perceived
motion-to-photon latency is determined by comparing both signals with each other using cross-
correlation.

3.2 Mouth-to-ear latency

To determine the mouth-to-ear latency with the described measurement setup, a low-cost piezo
buzzer and a microphone are used. Right before starting the measurement the microcontroller
activates the buzzer for a short period. At every interval, the microcontroller checks if the attached
microphone detects any sound. Then, the mouth-to-ear latency can be determined by counting the
intervals between the beginning of the measurement and the received sound-impulse.

3.3 Remote latency

If the delay between multiple VR systems needs to be measured, synchronization is required. In the
presented method, GPS receivers are used to synchronize multiple microcontrollers and measure
delays in distributed VR systems. Figure 1 shows how the described technique can be used to
measure local latency and delays between two remote VR locations.

3.4 Comparison to previous work

Compared to camera-based methods [Ste, FS14, RDM+09], the use of photodiodes and a poten-
tiometer makes it easy to evaluate latencies of HMD systems. For all measurement methods using
a video camera, it is necessary to remove the lenses from the HMD to capture the output of the
VR display. If the motion-to-photon latency of the HMD itself should be measured it is required
to attach the whole HMD to a pendulum. To capture the display, the camera needs to be attached
to the pendulum as well. With the measurement system described in this work, there is no need to
remove the lenses, and the end-to-end delay of a system can be determined quickly. This will help
hardware and software developers to continuously evaluate changes to the VR system concerning
end-to-end latencies.

With the proposed GPS clock synchronization method, it is possible to accurately measure re-
mote latencies in distributed VR systems using cheap and available hardware components. This ap-
proach has advantages compared to the camera synchronization mechanism described by Roberts
et al. [RDM+09]. First of all, it is not necessary to synchronize both measurement devices by con-
necting them to a shared location. Another beneﬁt of the GPS synchronization technique is that
it does not suffer from noticeable time offsets. Although Roberts et al. did not report any drifts,
it is not clear for how long two cameras can run synchronized without recalibration. Regarding
synchronization accuracy and granularity, the proposed method also surpasses camera-based meth-
ods. While the timecode of external microcontrollers can be precisely synchronized to differ less
than 1 ms from each other, Roberts et al. reported that the camera synchronization method has a
synchronization granularity of one frame which leads to measurement accuracy of approximately
20 ms [RDM+09].

(a) Brightness code

(b) Photosensor

Figure 2: Measure display output

The described method combines the use of photodiodes and a potentiometer to determine the
motion-to-photon latencies in VR systems and is technically comparable to the method described
by Di Luca [Di 10]. Additionally, the mouth-to-ear latency is captured with a similar setup as
described by Agastya et al. [AMK09]. The use of external measurement hardware and GPS syn-
chronization makes it possible to quickly and accurately measure remote end-to-end latencies in
distributed VR systems. The hardware and software tool is provided as an open-source project and
allows others to use and enhance it to their speciﬁc needs.

4 Method Description

4.1 Rotation platform

For the measurement setup, a potentiometer is used to continuously trace the rotation angle of
the tracked object (e.g., an HMD). A servo connected to the microcontroller controls the platform
rotary motion. With this setup, steady back-and-forth movements of the tracked object are possible.

4.2 Photosensor

The photosensor is used to monitor display outputs and captures the rotary angle of the tracking
system encoded in a brightness code.

During measurement, right before the rendering process, the VR system outputs the last cap-
tured horizontal rotation angle translated into a brightness code. To get a resolution of 4096 dif-
ferent values, four areas on the display are used to output a number in the octal numeral system.
By capturing each area with a light-sensitive sensor connected to the microcontroller, it is possible
to get the rotation angle output of the VR system at every interval. Figure 2a shows the displayed
brightness code of the VR system. The octal numeral system is used because the analog signal of
the photosensors is noisy and a bit of headroom between the different brightness levels is needed.
The whole photosensor consists of four TSL250R photodiodes which are very similar to those
used in Di Luca’s Method [Di 10]. According to the datasheet, the sensors have an output rise-

time of about 260 µs [TAO01]. Figure 2b shows the used photosensor. With the HMDs used in
this work, it is not possible to directly attach the photosensor to the VR display without the need
to disassemble the hardware. HMDs use optical lenses in front of the display system to expand
the ﬁeld of view. Without any light path correction, it is not possible to get a sharp image of the
displayed frame on a ﬂat surface.

To read the brightness code of the VR system on HMDs, an additional optical lens is necessary.
For the measurement setup, convex lenses built into the low-cost VR glasses Google Cardboard
where directly attached to the lenses of the HMD to correct the light path of the VR display.
Attaching the photosensor a few millimeters away from the correction lens makes it possible to
read the displayed brightness code.

4.3 Audio Latency Detector

To detect the mouth-to-ear latency of VR systems, a combination of a piezo buzzer and a micro-
phone is used.

By attaching the buzzer to an analog output pin with pulse-width modulation (PWM) capability
of the microcontroller, a square wave signal is generated to get a pure tone. For the described setup
an audio frequency of 4 kHz is used since at around this frequency the sound pressure output of the
used piezo buzzer is at its maximum. The buzzer is attached to the microphone of the VR system.
To detect the generated tone at the speakers of the VR system, a cheap sound detection model is
used. The SparkFun Sound Detector2 consists of a simple electret microphone, a voltage feedback
ampliﬁer and a voltage comparator. When the sound pressure at the detection module reaches
a speciﬁc threshold, the digital output pin of the module is set to high. One digital pin of the
microcontroller is used as input pin and waits for a rising signal of the sound detection module.
When a high signal is detected, the elapsed intervals since activating the buzzer are summed up
and represent the measured mouth-to-ear delay.

4.4 Microcontroller

Teensy 3.2 microcontrollers are used as central units of the described measuring system. With
the built-in 16 Mhz crystal continuous measurements with a stable frequency are possible. Since
the crystal clock accuracy is unspeciﬁed, a worst-case quality for quartz crystals of ±100 ppm is
assumed. With the assumed worst-case accuracy, it is possible to take synchronized measurements
with a maximum time deviation of ±0.5 ms over a timespan of 5 s.

The photosensors and the potentiometer are connected to the analog inputs of the microcon-
troller. The built-in analog-to-digital converter (ADC) of the Teensy 3.2 is used to capture all sensor
inputs and store them in the microcontroller’s Random Access Memory (RAM). All analog-to-
digital conversions of the connected sensors take less than 1 ms.

With the described system, measurements are taken with a frequency of 1000 Hz. This fre-
quency was chosen because of the built-in OLED displays of current HMDs. Those displays show

2https://github.com/sparkfun/Sound_Detector

a new frame for about 1-2 ms until the display turns black again. With intervals of 1 ms, it is
possible to measure the display output at every frame.

Theoretically, even higher frequencies are possible, but for the described VR scenarios 1 ms
intervals proved to be fast enough. Capturing with higher frequencies would also require additional
RAM space.

4.5 Synchronization

To measure the delay between different locations, synchronized timecodes are needed.

For the described setup two u-blox NEO-6M modules continuously transmit GPS data to the
connected microcontrollers. This data contain a UTC timestamp which is used for synchronization.
Besides the serial connection for the GPS data, the u-blox NEO-6M has another pin which outputs
a very accurate time pulse signal that rises every full second with a root-mean-square accuracy of
30 µs [U-B11]. A UTC timestamp can be transmitted to the controllers for a synchronized start of
measurement. All globally-distributed systems will wait until then and start taking measurements
simultaneously. Compared to other timecode synchronization techniques, GPS receivers are a lot
more precise. One of the drawbacks though is that the GPS antenna needs to be placed outdoors
or close to a window to work. For the proposed system a long wire was used to connect the
measurement system with the signal of the GPS module.

4.6 Data evaluation

After measurement, the raw data captured by the microcontrollers are transmitted to the VR simu-
lation computer. Using Unity 3D scripts, the photosensor data of all four photosensors are decoded
into the corresponding decimal number representing the rotation output of the VR system. The
recorded potentiometer and photosensor data are cross-correlated with each other. The lag with
the maximum correlation coefﬁcient represents the motion-to-photon latency of the VR system.

Figure 3 shows an example of the measured motion-to-photon latency of the Oculus Rift CV1.
The blue line is the captured rotation of the potentiometer and the red line the corresponding
display output. For example, in ﬁgure 3 the maximum correlation coefﬁcient and therefore the
estimated motion-to-photon latency is at 8 ms.

The microcontroller computes the mouth-to-ear latency with the attached microphone. Count-
ing the intervals (1000 Hz) between the start of measurement and sound detection is the local or
remote mouth-to-ear latency of the VR system in milliseconds.

5 Measure Latency

In the following sections, the measured motion-to-photon and mouth-to-ear latency of a simple
distributed VR system will be described.

Figure 3: Motion-to-photon latency of the Oculus Rift CV 1

5.1 Setup

Two HMDs were used for latency measurements: the HTC Vive and the Oculus Rift CV1. Both VR
devices were used with the respective external tracking system and got attached to computers with
slightly different hardware speciﬁcations. To better distinguish between both conﬁgurations they
will further be referred to as OCR (PC with Oculus Rift CV 1) and VIVE system (PC with HTC
Vive). The hardware speciﬁcations of both systems are listed in table 1 and 2.

Table 1: VIVE PC speciﬁcations

Table 2: OCR PC speciﬁcations

CPU
RAM
OS
Graphics Card GeForce GTX 970

Intel Core i7-6700
16 GB
Windows 10 - 64 bit

CPU
RAM
OS
Graphics Card GeForce GTX 970

Intel Xeon E5-1620
8 GB
Windows 10 - 64 bit

The VR simulation was running in Unity 3D version 5.5.2f1. The displayed scene was empty
except for the default Unity Skybox and a tracked virtual camera. V-Sync was activated on both
VR systems.

0100020003000400050000.200.240.28PotentiometerTime (ms)Rotation0100020003000400050000.30.50.7PhotodiodeTime (ms)Rotation−100−500501000.00.40.8Lagcross−correlationCross−Correlation(photo,poti)Latency:  8  msFor audio communication between both VR stations the audio chat tool Mumble version 1.2.19
was used. Mumble requires the server process Murmur. The server was running on the OCR PC
(see table 2). Philips SHP1900 headphones got plugged into the audio jack of the HTC Vive. As
audio input device the microphone of the Vive was used. For the Oculus Rift system the built-in
microphone and headphones of the HMD were used.

Both VR systems were directly connected with each other via a CAT6 network cable. With
the built-in network card, a maximum transfer rate of up to 1 Gbit/s is possible. The round-trip
time between both stations was less than 1 ms (average result of 10 ICMP echo requests). To
synchronize the rotation of both VR systems the Unity High-Level API (HLAPI) was used. In the
Unity Network Transform component the maximal possible send rate of 29 updates per second was
chosen3.

5.2 Results and Discussion

Measurements were taken 10 times on each setup. Rotation data was recorded over a timespan of
5 s. The results are summarized in table 3 and 4.

Table 3: Measured motion-to-photon latencies

Table 4: Measured mouth-to-ear latencies

Avg

Min Max

OCR
5.8 ms
VIVE
5.1 ms
O → V 49 ms
V → O 27.8 ms

3 ms
1 ms
42 ms
24 ms

9 ms
10 ms
55 ms
33 ms

SD

0.5
2.7
3.8
2.4

Avg

Min

Max

OCR
100 ms
VIVE
112.4 ms
O → V 144.1 ms
V → O 168.3 ms

93 ms
112 ms
132 ms
157 ms

106 ms
113 ms
151 ms
175 ms

SD

3.7
0.5
5.6
5.4

The measured local motion-to-photon latency of both VR systems was very low. There was
no signiﬁcant difference between both systems. Both results are similar to the motion-to-photon
latency of the Oculus Rift DK2 measured by Raaen and Kjellmo in 2015 [RK15]. However, this
is only true if V-Sync was deactivated for the Oculus Rift DK2. When V-Sync was active, Raaen
and Kjellmo measured an average latency of 41 ms for the Oculus Rift DK2 [RK15]. The results
of this paper indicate that the newer generation of HMDs reaches a low motion-to-photon latency
even if V-Sync is activated.

The measurement results in ﬁgure 3 suggest that extrapolation is used to achieve such low
latencies. Comparing the turning points of the potentiometer and those of the display output in
ﬁgure 3 shows that the recorded platform motion is ﬂatter at the extremes than the movements
recorded at the display. This indicates that the tracked rotation gets extrapolated and leads to
overshooting.

The remote latency between both VR systems was high considering the fact that both stations
were only a few meters apart and directly connected to each other. One reason for that might be
that the network send rate of both systems was set to 29 updates per second.

3https://answers.unity.com/questions/1431826/network-transform-send-rate-send-interval.

html

The latency difference between both directions was about 20 ms. This was unexpected since
the measured local latency for both systems was more or less the same (see table 3). Maybe the
difference is caused by differences in the used VR SDKs (Oculus for the Oculus Rift CV1 and
OpenVR for the HTC Vive). The tracking data of the HTC Vive might have been extrapolated
before they were sent over the network and therefore the measured delays from the Vive to the
OCR is lower. However, this is only a vague assumption that requires further investigations.

To record the local mouth-to-ear latency of a VR system, the microphone was directly rooted
to the loudspeaker using the standard Windows volume mixer. The local measured audio delay
differed only slightly between the measurements.

The latency results for the HTC Vive system are about 10 % higher than those of the Oculus
Rift. Interestingly, the local mouth-to-ear delays are quite stable for the HTC Vive system, whereas
the latency results for the OCR differed from each other up to 13 ms.

6 Evaluation

In the following sections, the accuracy of the described method will be evaluated. Like in the
previous section, all measurements were performed ten times.

6.1 Motion-to-photon latency

To determine whether the described method measures the expected motion-to-photon latency of
a VR system, a frame delay mechanism was implemented in Unity. The brightness codes are
not immediately rendered, but stored in a queue. The length of the queue determines how many
frames the display output should be delayed. Before the rendering process, the current brightness
code is added to the queue, and the oldest value in the queue gets displayed. The HTC Vive was
used to evaluate the accuracy of the motion-to-photon latency measurement setup. The HTC Vive
uses a 90 Hz display, and therefore a delay of one frame should increase the end-to-end delay by
approximately 11.11 ms.

Table 5: Delayed motion-to-photon latencies

Avg

Min

Max

SD

Baseline
One frame delay
Five frames delay
Ten frames delay

5.1 ms
18.1 ms
63.7 ms
120.3 ms

2.6 ms
9 ms
3 ms
5.2 ms
25 ms
10 ms
56 ms
4.1 ms
68 ms
114 ms 124 ms 2.9 ms

Table 5 summarizes the results. The results show that the measurement system can detect the
artiﬁcially added latency. Based on a system delay of 5.1 ms, the expected delay for one, ﬁve and
ten frames is 16.2 ms, 60.7 ms, and 116.2 ms. The measured values in table 5 differ by a maximum
of 4.1 ms from the expected system delays.

6.2 Mouth-to-ear latency

To evaluate the accuracy of the described mouth-to-ear latency measurement method, the used
buzzer was directly attached to the microphone of the measurement system. With this conﬁgu-
ration, the audio delay is expected to be close to zero milliseconds. The time delay between the
generation and detection of the sound signal was measured with the microcontroller. Between the
tone output and the measured signal passed 401.6 ± 0.5µs. An average delay of less than half a
millisecond and the very low standard deviation of 0.5µs show that the mouth-to-ear delay can be
measured very precisely with the described system.

6.3 Clock synchronization

With the following method, the GPS synchronization mechanism described in this work was eval-
uated. Both measurement systems were placed right next to each other, and two digital ports of
both microcontrollers were connected with each other via a short cable. After the devices were
synchronized to GPS time, both got a command to start measuring in the next minute. One mi-
crocontroller logged the start of measurement in microseconds and waited for a hardware interrupt
on the input pin connected to the other measurement system. As soon as the other microcontroller
started to measure, the input pin of the ﬁrst microcontroller was set to high. Within the hardware
interrupt routine of the ﬁrst microcontroller, another timestamp was acquired. The difference be-
tween both timestamps was as low as 15.2 ± 1.7µs. This shows that GPS clock synchronization
works as expected.

7 Conclusion and future work

The presented measurement setup makes it easy to evaluate the motion-to-photon and mouth-to-ear
latency of different distributed VR systems.

Measuring latency of current VR hardware showed some interesting results. It seems that a
very low local motion-to-photon latency can be achievable with current HMDs even if V-Sync
is activated. For the Oculus Rift DK2 Raaen et al. concluded that they ”[...]
found no way
of satisfying both the delay requirement and the requirement to use vertical synchronization at
once.” [RK15]. This statement seems to be no longer valid for current VR headsets like the Oculus
Rift CV1 and HTC Vive.

The measurement system is easy to set up and a feasible tool to evaluate distributed VR sys-
tems. It is open source, cheap and easy to build. It will help scientists and developers to better
understand and improve distributed VR systems regarding local and remote latency.

In the future, we plan to enhance the measurement system further. The external hardware can,
for example, be extended to determine local and remote haptic delays in VR systems, as well as
binocular audio latencies as described by Stitt et al. [SHMK16].

References

[AMK09] Chitra Agastya, Dan Mechanic, and Neha Kothari. Mouth-to-ear latency in popular

voip clients. Columbia University, New York, 2009.

[Bro99]

F. P. Brooks. What’s real about virtual reality? IEEE Computer Graphics and Appli-
cations, 19(6):16–27, 1999.

[Di 10]

[FS14]

[LD97]

Massimiliano Di Luca. New method to measure end-to-end delay of virtual reality.
Presence: Teleoperators and Virtual Environments, 19(6):569–584, 2010.

Sebastian Friston and Anthony Steed. Measuring latency in virtual environments.
IEEE Transactions on Visualization and Computer Graphics, 20(4):616–625, 2014.

V. D. Lehner and T. A. DeFanti. Distributed virtual reality: Supporting remote collab-
oration in vehicle design. IEEE Computer Graphics and Applications, 17(2):13–17,
1997.

[MRWB03] M. Meehan, S. Razzaque, M. C. Whitton, and F. P. Brooks. Effect of latency on
presence in stressful virtual environments. In IEEE Virtual Reality 2003, pages 141–
148, 22-26 March 2003.

[NST+12]

[PBJ+98]

Jean-Marie Normand, Bernhard Spanlang, Franco Tecchia, Marcello Carrozzino,
David Swapp, and Mel Slater. Full body acting rehearsal in a networked virtual
environment — a case study. Presence: Teleoperators and Virtual Environments,
21(2):229–243, 2012.

S. Purdy, D. Barnhart, R. Johnston, R. Wuerfel, and R. Ewart. Latency measure-
ments obtained from the simulation network analysis project.
In Azzedine Bouk-
erche and Paul Francis Reynolds, editors, Proceedings / 2nd International Workshop
on Distributed Interactive Simulation and Real-Time Applications, pages 71–81, Los
Alamitos, Calif., 1998. IEEE Computer Society.

[RDM+09] David Roberts, Toby Duckworth, Carl Moore, Robin Wolff, and John O’Hare. Com-
paring the end to end latency of an immersive collaborative environment and a video
conference. In 13th IEEE/ACM International Symposium on Distributed Simulation
and Real Time Applications, pages 89–94, 2009.

[RK15]

In
Kjetil Raaen and Ivar Kjellmo. Measuring latency in virtual reality systems.
Konstantinos Chorianopoulos, Monica Divitini, Jannicke Baalsrud Hauge, Letizia
Jaccheri, and Rainer Malaka, editors, Entertainment computing - ICEC 2015, volume
9353 of Lecture Notes in Computer Science, pages 457–462. Springer, Cham and
Heidelberg and New York and Dordrecht and London, 2015.

[RNDM14] Parth Rajesh Desai, Pooja Nikhil Desai, Komal Deepak Ajmera, and Khushbu Mehta.
A review paper on oculus rift-a virtual reality headset. International Journal of En-
gineering Trends and Technology, 13(4):175–179, 2014.

[SBHM15] Matthew E. St. Pierre, Salil Banerjee, Adam W. Hoover, and Eric R. Muth. The effects
of 0.2hz varying latency with 20–100ms varying amplitude on simulator sickness in
a helmet mounted display. Displays, 36:1–8, 2015.

[SHMK16] Peter Stitt, Etienne Hendrickx, Jean-Christophe Messonnier, and Brian Katz. The

role of head tracking in binaural rendering, 2016.

[Ste]

Anthony Steed. A simple method for estimating the latency of interactive, real-time
graphics simulations.
In Steven Feiner, Daniel Thalmann, Pascal Guitton, Bernd
Fr¨ohlich, Ernst Kruijff, and Martin Hachet, editors, the 2008 ACM symposium, page
123.

[TAO01]

TAOS Inc. Datasheet - tsl250r, tsl251r, tsl252r: light-to-voltage optical sensors, 2001.

[Tim03]

One-Way Transmission Time. Itu-t recommendation g. 114. ITU-T May, 2003.

[U-B11]

U-Blox AG. Neo-6: u-blox 6 gps modules data sheet, 2011.

[VGB]

Ivan Vaghi, Chris Greenhalgh, and Steve Benford. Coping with inconsistency due
to network delays in collaborative virtual environments. In Don Brutzman, Heedong
Ko, and Mel Slater, editors, the ACM symposium, pages 42–49.

