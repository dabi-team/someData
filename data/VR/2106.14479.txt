Distributed stochastic gradient tracking algorithm
with variance reduction for non-convex optimization

Xia Jiang, Xianlin Zeng, Member, IEEE, Jian Sun, Member, IEEE, and Jie Chen, Fellow, IEEE,

1

1
2
0
2

l
u
J

2
2

]

C
O
.
h
t
a
m

[

2
v
9
7
4
4
1
.
6
0
1
2
:
v
i
X
r
a

Abstract—This paper proposes a distributed stochastic algo-
rithm with variance reduction for general smooth non-convex
ﬁnite-sum optimization, which has wide applications in signal
processing and machine learning communities. In distributed
setting, large number of samples are allocated to multiple agents
in the network. Each agent computes local stochastic gradient and
communicates with its neighbors to seek for the global optimum.
In this paper, we develop a modiﬁed variance reduction technique
to deal with the variance introduced by stochastic gradients.
Combining gradient tracking and variance reduction techniques,
this paper proposes a distributed stochastic algorithm, GT-
VR, to solve large-scale non-convex ﬁnite-sum optimization over
multi-agent networks. A complete and rigorous proof shows
that the GT-VR algorithm converges to ﬁrst-order stationary
points with O( 1
k ) convergence rate. In addition, we provide
the complexity analysis of the proposed algorithm. Compared
with some existing ﬁrst-order methods, the proposed algorithm
has a lower O(P M (cid:15)−1) gradient complexity under some mild
condition. By comparing state-of-the-art algorithms and GT-
VR in experimental simulations, we verify the efﬁciency of the
proposed algorithm.

Index Terms—distributed algorithm, variance reduction, non-
convex ﬁnite-sum optimization, stochastic gradient, complexity
analysis

I. INTRODUCTION

W ITH the development of big data, distributed ﬁnite-

sum optimization has received extensive attention from
researchers in signal processing, control and machine learning
communities [1]–[6]. In distributed ﬁnite-sum optimization,
large-scale signal information or training samples are allocated
to different nodes, and each node updates variable by local data
and obtains global optimal estimate through communication
with neighbors. When the dataset is large or is naturally lo-
cated in a decentralized setting or contains private information,

This work was supported in part by the National Natural Science Foun-
dation of China (Nos. 61720106011, 62088101, U20B2073, 61925303), the
National Key Research and Development Program of China under Grant
2018YFB1700100 and Beijing Institute of Technology Research Fund Pro-
gram for Young Scholars. (Corresponding author: Xianlin Zeng.)

X. Jiang (jiangxia@bit.edu.cn) and J. Sun (sunjian@bit.edu.cn) are with
Key Laboratory of Intelligent Control and Decision of Complex Systems,
School of Automation, Beijing Institute of Technology, Beijing, 100081,
China, and also with the Beijing Institute of Technology Chongqing Innovation
Center, Chongqing 401120, China

X. Zeng (xianlin.zeng@bit.edu.cn) is with Key Laboratory of Intelligent
Control and Decision of Complex Systems, School of Automation, Beijing
Institute of Technology, Beijing, 100081, China

J. Chen (chenjie@bit.edu.cn) is with Beijing Advanced Innovation Center
for Intelligent Robots and Systems (Beijing Institute of Technology), Key Lab-
oratory of Biomimetic Robots and Systems (Beijing Institute of Technology),
Ministry of Education, Beijing, 100081, China, and also with the School of
Electronic and Information Engineering, Tongji University, Shanghai, 200082,
China

it is infeasible to transmit the dataset over networks and handle
it at a centralized node [4], [7]–[10]. In addition, for functions
with large-size local data, computing the local gradient by
the entire local dataset becomes practically difﬁcult. Hence,
methods that sample small batches of local dataset to compute
stochastic gradients are favored. Due to the above reasons, dis-
tributed stochastic ﬁrst-order algorithms are preferable as they
own a low computation complexity without the calculation of
Hessian matrix and are easy to analyze.

Distributed stochastic gradient algorithm is a combination of
average consensus steps between neighbors and local stochas-
tic gradients, which has been popular in machine learning
tasks [11]–[13]. With the similar design idea, considerable
works have been studied for more complex optimization prob-
lems and various multi-agent networks in recent years, e.g.,
distributed stochastic gradient projection algorithms [14], dis-
tributed stochastic mirror descent [15], distributed stochastic
primal-dual algorithm over random networks with imperfect
communications [16] and stochastic gradient-push over time-
varying directed graphs [17]. However, the performance of
distributed stochastic gradient algorithm is generally limited
by two components. One is the local variance introduced
by stochastic gradient at each agent and the other is the
heterogeneous datasets between different agents. To handle the
local variance, many variance reduction techniques have been
proposed to reduce storage space and computation complexity,
such as SAGA [18], SVRG [19], SARAH [20] and Asyn-VR
[21]. Distributed variance-reduced stochastic gradient methods
have also been developed for smooth and strongly-convex op-
timization in recent years [22]–[24]. To achieve robustness to
heterogeneous environments, some works develop distributed
bias-correction techniques such as gradient tracking [25], [26],
EXTRA [27], and primal-dual principles [28], [29]. Integrating
variance reduction and bias-correction techniques, efﬁcient
distributed algorithms with linear convergence rate arise for
strongly-convex ﬁnite-sum optimization [22]. However, the
applicability of these distributed methods for non-convex
optimization remains unclear.

Large-scale non-convex optimization has wide applications
including logistic regression with non-convex regularization
and neural networks training. When the cost functions are
non-convex, the design and theoretical analysis of efﬁcient
algorithms become difﬁcult due to the lack of good properties
of convexity. Very recent works have proposed distributed
variance-reduced methods for non-convex ﬁnite-sum prob-
lems. [30] has proposed D-GET for decentralized non-convex
ﬁnite-sum minimization, which considers local SARAH-type
variance-reduced technique and gradient tracking. However,

 
 
 
 
 
 
as is pointed by [31], D-GET does not have a network-
independent gradient complexity. GT-SARAH proposed in
[31] has achieved a near-optimal total gradient computation
complexity at the cost of twice communication rounds of the
D-GET algorithm. [32] has proposed a GT-SAGA algorithm
by combining the SAGA and gradient tracking techniques.
However, SAGA needs additional storage space compared
with SVRG. Inspired by SVRG technique, we propose a
distributed stochastic ﬁrst-order algorithm with low network-
independent gradient complexity for large-scale ﬁnite-sum
non-convex optimization in this paper.

The contributions of this paper are summarized as follow-

ing.
(1) For general smooth non-convex optimization, we propose
a novel distributed stochastic iterative algorithm, GT-VR,
by combining gradient tracking and variance reduction
techniques. The variance reduction in the proposed algo-
rithm is a modiﬁed version of SVRG technique [19] and
makes use of Bernoulli distribution to reduce the local
variance introduced by stochastic gradient.
(2) By linear matrix inequality, we prove that

the pro-
posed GT-VR algorithm converges to ﬁrst-order station-
ary points with O( 1
k ) convergence rate. To the best of our
knowledge, for general smooth non-convex optimization,
it is the ﬁrst work to provide a sublinear convergence
rate without steady-state error for distributed stochastic
algorithms designed by gradient tracking and variance
reduction. In addition, we provide the range of constant
step-sizes and the probability range of the Bernoulli
distribution.

(3) Compared with some newest algorithms, the proposed
algorithm has a lower gradient complexity. To be spe-
ciﬁc, compared with distributed algorithms DSGT [33],
D2 [34], DSGD [35], whose gradient complexity is
O(ν2(cid:15)−2), the proposed algorithm has a lower network-
independent gradient complexity O(P M (cid:15)−1) under some
mild condition. Comparative experimental results of these
algorithms and GT-VR also verify the efﬁciency of the
proposed algorithm.

The remainder of the paper is organized as follows. Mathe-
matical notations and some stochastic properties are given in
section II. The problem description and distributed stochastic
algorithm are provided in section III. The convergence prop-
erties of proposed methods are provided in section IV and
are analyzed theoretically in section V. The efﬁciency of the
distributed algorithms are veriﬁed by simulations in Section
VI and the conclusion is made in section VII.

II. NOTATIONS AND PRELIMINARIES

A. Mathematical notations

We denote R as the set of real numbers, R+ as the set of
positive real numbers, Z+ as the set of positive integers, Rn
as the set of n-dimensional real column vectors, respectively.
All vectors in the paper are column vectors, unless otherwise
noted. 1n denotes an n × 1 vector with all elements of 1, 0d
denotes a d × 1 vector with all elements of 0 and Id denotes a
d × d identity matrix. The notation ⊗ denotes the Kronecker

2

product and max{· · · } denotes the maximum element in the
set {· · · }. For a real vector v, (cid:107)v(cid:107) is the Euclidean norm.
For a differentiable function f (x), its gradient is represented
by ∇f (x). In the following paper, subscript i refers to this
being the local variables of the ith agent, e.g., xi means local
variable x of agent i.

We ﬁx a rich enough probability space (Ω, F, P), where all
random variables in discussion are properly deﬁned and E[·]
denotes the expectation operator with respect to the probability
measure P. Let A be an event in F with nonzero probability
and X be a random variable. The conditional expectation of
X given A is denoted by E[X|A]. For an event A ∈ F, its
indicator function is denoted as IA. We use σ(·) to denote the
σ − algebra generated by the random variables and/or sets in
its argument. For a matrix A, d(A) denotes its spectral radius.

B. Stochastic Theory

For conditional expectation, there is one basic property,
which is useful in the subsequent analysis and is stated as
following.

Proposition II.1. Let X, Y and Xi(1 ≤ i ≤ n) be random
variables, and E|X| < ∞, E|Xi| < ∞(1 ≤ i ≤ n) .
(1) E[(cid:80)n

i=1 αiE[Xi|Y ] (a.s.), where αi is

i=1 αiXi|Y ] = (cid:80)n

a constant.

(2) If X and Y are mutual independent, then E[X|Y ] = EX.
(3) E[E[X|Y ]] = EX.

Bernoulli distribution: The Bernoulli(P ) distribution is
the discrete probability distribution of a random variable
which takes the value 1 with probability P and the value 0
with probability 1 − P . If X is a random variable with the
Bernoulli(P ) distribution, then

Pr(X = 1) = P, Pr(X = 0) = 1 − P.

(1)

III. PROBLEM DESCRIPTION AND DISTRIBUTED SOLVER
DESIGN

In this paper, we aim to solve the following distributed
ﬁnite-sum optimization problem over a multi-agent network,

min
x∈Rd

f (x) (cid:44) 1
n

n
(cid:88)

i=1

fi(x), fi(x) =

1
mi

mi(cid:88)

j=1

fi,j(x),

(2)

: Rd → R is the local differentiable objective
where fi
function of agent i, further decomposed as the average of
mi component costs {fi,j}mi
j=1, n is the number of agents,
mi is the number of local samples, and (cid:80)n
i=1 mi = M is
the total number of samples in the network. The multi-agent
network containing n agents is denoted by G(V, E, W ), where
V = {1, · · · , n}, E = V × V and W is the adjacent matrix
associated with G. In distributed setting, each agent handles
local information and communicates with its neighbors over
the network G to solve (2) cooperatively.

Remark III.1. This formulation of optimization problem (2)
is widely adopted in empirical risk minimization, where each
local cost fi can be considered as an empirical risk computed
over a ﬁnite number of mi local data samples and lies at the

3

heart of many modern machine learning problems [36], [37].
Compared with the parameter-server type machine learning
system with a fusion center [38], [39], distributed optimization
problem (2) can preserve data privacy, improve the computa-
tion efﬁciency and enhance network robustness. Furthermore,
in many emerging applications such as collaborative ﬁltering,
federated learning, distributed beamforming and dictionary
learning, the data is naturally collected in a distributed setting,
and it is impossible to transfer the distributed data to a cen-
tral location [30]. Therefore, decentralized computation has
sparked considerable interest in both academia and industry.

Next, we design a distributed stochastic algorithm for the
general smooth non-convex optimization (2). There are two
challenges in the distributed stochastic algorithm design. One
challenge is the slow convergence due to the variance of
stochastic gradients by asymptotically estimating the local full
gradient ∇fi, based on randomly selected samples from the
local dataset of agent i. The other is the difference between
i.e., ∇fi(x∗) (cid:54)= 0d,
local and global objective functions,
∀i ∈ {1, · · · , n}, holds for the global optimum x∗. This issue
may be handled by the popular gradient tracking technique
that introduces a local gradient estimator to track the global
gradient.

By combining the distributed gradient tracking [40] with
a variance reduction technique, we propose a ﬁrst-order GT-
VR algorithm. The complete implementation of GT-VR is
summarized in Algorithm 1. The local gradient estimator vk
i
is updated by

i}t≤k−1

in GT-VR, τ k
i

estimator vk
is updated by (3). The only main difference
i
is that
is updated following the Bernoulli
distribution, while in SVRG, it is updated periodically. De-
note F k as the history of the dynamical system deﬁned by
σ({st
i, lt
i={1,··· ,n}). Note that in SVRG, each local gradient
estimator vk
is an unbiased estimator of the local gradient
i
i ) given F k [19], whereas, it does not hold in our
∇fi(xk
proposed algorithm GT-VR. This modiﬁcation is vital for the
convergence of proposed algorithm for the smooth non-convex
optimization.

Remark III.3. Compared with distributed deterministic opti-
mization, which needs to compute the entire local gradient
∇fi at each iteration,
the proposed distributed stochastic
ﬁrst-order algorithm using sampled batch data to compute
is more suitable for the training and
stochastic gradient
processing of large-scale data.

Compared with GT-SAGA [32], the proposed algorithm does
not need to store the value of gradient and saves more storage
space. Compared with the two-timescale hybrid algorithm GT-
SARAH [31], the proposed algorithm is one single-timescale
randomized gradient algorithm. In addition, at each iteration,
there are only two communication rounds with neighbors at
each agent. However, GT-SARAH has a near-optimal gradient
computational complexity, which is better than the proposed
algorithm.

IV. CONVERGENCE RESULT

In this section, we provide the convergence analysis of the

vk
i = ∇fi,sk

i

(xk

i ) − ∇fi,sk

i

(τ k

i ) + ∇fi(τ k

i ).

(3)

proposed algorithm GT-VR with some mild assumptions.

In addition, in GT-VR, we introduce the gradient tracking
technique to achieve the global gradient tracking in distributed
optimization.

Algorithm 1 GT-VR updating at each agent i
i = x1

i ; η; {wir}n

r=1; y1

i = v1

i = ∇fi(x1

i ).

i ; τ 1

1: Initialize: x1
2: for k = 1, 2, · · · do
3:

Update the local estimate of the solution:

xk+1
i =

n
(cid:88)

r=1

wir(xk

r − ηyk

r );

4:

5:
6:
7:

i

i

= 1, τ k+1

at random from the Bernoulli(P ) dis-
= xk+1
, and otherwise,
i

Select lk+1
tribution. If lk+1
τ k+1
= τ k
i .
i
Select sk+1
uniformly at random from {1, · · · , mi}.
Update the local stochastic gradient estimator by (3);
Update the local gradient tracker:

i

i

yk+1
i =

n
(cid:88)

r=1

8: end for

wir(yk

r + vk+1

r − vk

r );

Remark III.2. The variance reduction technique taken in GT-
VR is a modiﬁcation of the well-known SVRG technique. In
both techniques, the entire local full gradient ∇fi(xk
i ) needs to
be computed with a certain probability and the local gradient

Assumption IV.1.

(1) Each cost function fi,j is uniformly

L-smooth, i.e., for some L > 0,

(cid:107)∇fi,j(x) − ∇fi,j(y)(cid:107) ≤ L(cid:107)x − y(cid:107), ∀x, y ∈ Rd.

(2) The adjacent matrix of G, W , is a doubly stochastic
matrix, where Wii > 0 for all i ∈ V, and Wij > 0 if
(i, j) ∈ E for i, j ∈ V.

(3) The family {lk

i , sk
i

: i ∈ V, k ≥ 1} of random variables

in the proposed algorithm is independent.

Assumption IV.1 (1) and (2) are common assumptions
in distributed optimization. (1) guarantees that local batch
objective functions {fi}n
i=1 and the global objective function
f are L-smooth. The adjacent matrix satisfying (2) holds for
the family of undirected graphs and weight-balanced directed
graphs. In addition, Assumption IV.1 (2) guarantees that the
radius of the network ρ satisﬁes

ρ (cid:44) sup
(cid:107)x=1(cid:107)

(cid:107)W (x − n−11n1T
(cid:107)x − n−11n1T

n x)(cid:107)
n x(cid:107)

< 1.

(4)

Assumption IV.1 (3) is standard in the design of stochastic
algorithm and is practical in application.

For the convenience of analysis, we deﬁne several auxiliary

quantities as following:

xk =






xk
1
...
xk
n


 , yk =



 , vk =







yk
1
...
yk
n




 ,






vk
1
...
vk
n

τ k =


 , ∇f (xk) =







τ k
1
...
τ k
n






∇f1(xk
1)
...
∇fn(xk
n)




 ,

(cid:80)n

¯xk = 1
i , ¯yk = 1
n
n
1n ⊗ ¯xk ∈ Rnd and ¯vk = 1n ⊗ ¯vk ∈ Rnd.

i , ¯vk = 1
n

i=1 xk

i=1 yk

(cid:80)n

(cid:80)n

i=1 vk

i , ¯xk =

Then, the proposed algorithm GT-VR satisﬁes
xk+1 = (W ⊗ Id)(cid:0)xk − ηyk(cid:1),
yk+1 = (W ⊗ Id)(cid:0)yk + vk+1 − vk(cid:1),

and

¯yk = ¯vk,

¯xk+1 = ¯xk − η ¯yk,

(5)

(6)

(7)

where the doubly stochastic property of W is used to derive
(7).

Deﬁne ¯η = min

(cid:110)

(cid:113) 1−( 4

9 P )ρ2

3 + 8
2T

(1−3ρ2)

(cid:0)16ρ2L2+(32ρ2L2+2)(1−P ) ρ+1
3 + 16

(cid:111)
, where T (cid:44) 16L2 + (cid:0) 8

(cid:1)5L
3 (1 + 1

ρ

ρ )(1 −
9 + 16(1 − P )(1 + ρ +

, 1
6L ,

P )(cid:1)L2 + (32 + 32P )L2ρ2 + (cid:0) 16

)(cid:1)L2ε3 ∈ R+ and ε3 is a positive number.

2(ρ+1)
9ρ
Then, the convergence result of the proposed algorithm GT-

VR is covered in the following theorem.

(1+ 1

Theorem IV.1. Suppose Assumption IV.1 hold. Let ρ2 < 1
3 ,
3ρ2
9 +1+ρ < P < 1, 0 < η < ¯η and f ∗ (cid:44)
1 −
ρ ) 2
inf x∈Rd f (x) > −∞.
Then f (¯xk) converges,
1
k
∇f (¯xs)(cid:107)2 ≤ O( 1

E(cid:107)∇f (¯xs)(cid:107)2 ≤ O( 1
(cid:80)k
k ) and 1

s=1
E(cid:107)xs − ¯xs(cid:107)2 ≤ O( 1

k ),
E(cid:107)ys −

(cid:80)k

(cid:80)k

s=1

s=1

1
k

k

k ).

Remark IV.1. Theorem IV.1 implies that the proposed GT-
VR algorithm converges to ﬁrst-order stationary points with
O( 1
k ) convergence rate. Compared with GT-HSGD [41], which
converges sublinearly at a rate of O( 1
k ) up to a steady-state
error, the proposed GT-VR has no steady-state error.

Then, we present the complexity of GT-VR in the following

sense.

Deﬁnition IV.1. The algorithm GT-VR is said to achieve an
(cid:15)-accurate stationary point of f in k iterations if

1
k

k
(cid:88)

s=1

E[(cid:107)∇f (¯xs)(cid:107)2] ≤ (cid:15).

(8)

Based on the results in Theorem IV.1, the iteration com-
plexity, gradient computation complexity and communication
complexity of GT-VR are established in the following corol-
lary.

Corollary IV.1. Suppose Assumption IV.1 hold. Let ρ2 < 1
3 ,
1 −

3ρ2
9 +1+ρ < P < 1 and
ρ ) 2

(1+ 1

0 < η ≤ ˜η,
where ˜η = min (cid:8)¯η, 1−3ρ2
(cid:9). Then,
3ρ2L
(1) GT-VR achieves an (cid:15)-accurate stationary point of f with

(9)

O(η−2(cid:15)−1) iterations.

(2) GT-VR achieves an (cid:15)-accurate stationary point of f in
O(P M η−2(cid:15)−1) gradient computations across all agents.

4

(3) GT-VR achieves an (cid:15)-accurate stationary point of f with
O((cid:80)n
i=1 Niη−2(cid:15)−1) communication rounds across all
agents, where Ni denotes the number of neighbors of
agent i.

Remark IV.2. In large-scale sample case, the gradient com-
plexity is O(P M η−2(cid:15)−1). If η = 1
6L , the gradient complexity
is O(P M (cid:15)−1), which is network-independent. If the accuracy
(cid:15) is chosen small enough, this gradient complexity is lower
than the gradient complexity O(ν2(cid:15)−2) of DSGT [33], D2
[34] and DSGD [35], where ν is the bounded variance of
stochastic gradient within each agent.

V. THEORETICAL ANALYSIS

In this section, we present the proofs for Theorem IV.1 and
Corollary IV.1. The analysis framework is general and may
be applied to other distributed algorithms based on variance
reduction and gradient tracking. Recall that F k is the history of
the dynamical system, deﬁned by F k (cid:44) σ({st
i={1,··· ,n}).
The convergence analysis is roughly divided into three steps.
(1) The ﬁrst step is to prove the boundness of E[(cid:107)xk − ¯xk(cid:107)2],
E[(cid:107)τ k − ¯xk(cid:107)2] and E[(cid:107)yk − ¯vk(cid:107)2] by constructing a lin-
ear matrix inequality. (2) The second step is to prove the
boundness of (cid:80)k
E[(cid:107)xs − ¯xs(cid:107)2], (cid:80)k
E[(cid:107)τ s − ¯xs(cid:107)2] and
(cid:80)k
E[(cid:107)ys − ¯vs(cid:107)2] by recursion. (3) Finally, we prove the
boundness of (cid:80)k
E[(cid:107)∇f (¯xs)(cid:107)2] and the convergence of
f (¯xk).

i}t≤k−1

i, lt

s=1

s=1

s=1

s=1

At ﬁrst, we provide a standard result for the adjacent matrix
W satisfying Assumption IV.1, which will be frequently used
in the subsequent analysis.

Lemma V.1.
x1, · · · , xn ∈ Rd, we have

[40] Suppose Assumption IV.1 hold. For any

(cid:107)(W ⊗ Id)(x − ¯x)(cid:107) ≤ ρ(cid:107)x − ¯x(cid:107),

where ρ is the radius of the underlying network.

In the following proposition, we present some useful prop-
local gra-
i ) and ∇fi(¯xk), which will be used to bound

erties of local stochastic gradient estimator vk
i ,
dient ∇fi(xk
E[(cid:107)τ k − ¯xk(cid:107)2] and to prove Theorem IV.1.

Proposition V.1. Suppose Assumption IV.1 holds. Then,

E[(cid:107)vk − ∇f (xk)(cid:107)2]

≤2L2E[(cid:107)xk − ¯xk(cid:107)2] + 2L2E[(cid:107)τ k − ¯xk(cid:107)2],

(10)

n
(cid:88)

(vk

i − ∇fi(¯xk))(cid:107)2]

i=1

(6L2E[(cid:107)xk

i − ¯xk(cid:107)2] + 4L2E[(cid:107)τ k

i − ¯xk(cid:107)2])

E[(cid:107)

1
n

≤

1
n

n
(cid:88)

i=1

and

1
2

E[(cid:107)¯vk(cid:107)2] ≤ E[(cid:107)∇f (¯xk)(cid:107)2]

+

2L2
n

n
(cid:88)

(3E[(cid:107)xk

i − ¯xk(cid:107)2] + 2E[(cid:107)τ k

i − ¯xk(cid:107)2]).

i=1
Proof. See Appendix VIII-A.

(11)

(12)

With Proposition V.1, in the following lemma, we establish
bounds on E[(cid:107)xk − ¯xk(cid:107)2], E[(cid:107)τ k − ¯xk(cid:107)2] and E[(cid:107)yk − ¯vk(cid:107)2],
respectively.

Proposition V.2. Suppose Assumption IV.1 hold. We have the
following inequalities:

(a) E[(cid:107)xk+1 − ¯xk+1(cid:107)2]

≤2ρ2E[(cid:107)xk − ¯xk(cid:107)2] + 2ρ2η2E[(cid:107)yk − ¯vk(cid:107)2],

(13)

(b) E[(cid:107)¯xk+1 − τ k+1(cid:107)2]

)12L2(cid:1)E[(cid:107)¯xk − xk(cid:107)2]

≤(cid:0)2ρ2P + (1 − P )(η2 +

η
β
+ 2ρ2η2P E[(cid:107)yk − ¯vk(cid:107)2]
+ (1−P )(cid:0)(η2 +
η
β

+ (η2 +

η
β

)8L2 + (1 + ηβ)(cid:1)E[(cid:107)¯xk − τ k(cid:107)2]

)2n(1 − P )E[(cid:107)∇f (¯xk)(cid:107)2],

(14)

(c) E[(cid:107)yk+1 − ¯vk+1(cid:107)2]

≤(2ρ2 + 48η2L2ρ4 + 32P η2L2ρ4)E[(cid:107)yk − ¯vk(cid:107)2]

+ 16ρ2L2(cid:0)1 + 6η2L2 + (2 + 2P )ρ2

+ 12L2(1 − P )(η2 +

η
β

)(cid:1)E[(cid:107)xk − ¯xk(cid:107)2]
η
β

+ 16ρ2L2(cid:0)4η2L2 +(1−P )[1 + ηβ + (η2 +

)8L2](cid:1)

E[(cid:107)τ k − ¯xk(cid:107)2]

+ 16ρ2L2n(cid:0)η2 + 2(1 − P )(η2 +

)(cid:1)E[(cid:107)∇f (¯xk)(cid:107)2],

η
β

(15)
where β ∈ R+, η is the step-size and ρ is the radius of the
underlying network.

Proof. See Appendix VIII-B.

s=1

s=1

E[(cid:107)τ s − ¯xs(cid:107)2] and (cid:80)k

With (13)-(15) in Proposition V.2 bounded in terms of
linear combinations of their past values, we will establish a
linear system of inequalities to bound (cid:80)k
E[(cid:107)xs − ¯xs(cid:107)2],
(cid:80)k
E[(cid:107)ys − ¯vs(cid:107)2] by recursion.
s=1
Before that, we introduce one lemma about the spectral radius
of a non-negative matrix, whose proof is in [42, Theorem
8.3.2]
Lemma V.2. Let A ∈ Rd×d be non-negative and x ∈ Rd be
positive. If Ax ≤ Θx for Θ > 0, then the spectral radius of
A satisﬁes d(A) ≤ Θ.

Proposition V.3. Suppose Assumption IV.1 hold. Suppose
3ρ2
ρ2 < 1
9 +1+ρ < P < 1 and
ρ ) 2
(cid:115)

3 , 1 −

(1+ 1

0 < η < min

(cid:110) 1
6L

,

1 − ( 4

3 + 8
2T

9 P )ρ2

(cid:111)
,

where T (cid:44) C2+C3ε3

ρ2

∈ R+ and ε3 ∈ R+. We have

5

where R0 = E(cid:107)y1 − ¯v1(cid:107)2 + E(cid:107)x1 − ¯x1(cid:107)2 + E(cid:107)τ 1 − ¯x1(cid:107)2, C4
and C (cid:48)(cid:48)
Proof. Let β = ρ


4 are deﬁned after (17).
η and ηL ≤ 1
6 . With Lemma V.2, we have

C2 C3
C1
2ρ2η2
2ρ2
0
2 C (cid:48)(cid:48)
2ρ2η2P C (cid:48)(cid:48)
1


E(cid:107)∇f (¯xk)(cid:107)2, (17)

C4
0
C (cid:48)(cid:48)
4

uk+1 ≤

uk +








(cid:124)

(cid:123)(cid:122)
C

(cid:125)





where uk (cid:44)


, C1 = 2ρ2 + 12+8P

E(cid:107)yk − ¯vk(cid:107)2
E(cid:107)xk − ¯xk(cid:107)2
E(cid:107)τ k − ¯xk(cid:107)2
16ρ2L2 + 8ρ+16(ρ+1)(1−P )
(1−P )(cid:1)ρ2L2, C (cid:48)(cid:48)
(cid:0)1+ 9ρ2+11ρ+2
16
9
1 = (1 − P )(cid:0) 2
9 (1 + 1
C (cid:48)(cid:48)
32n(1 − P )ρ2η2L2(1 + 1

ρ2L2 + (32 + 32P )L2ρ4, C3 =
3 (1−P )(1+ 1
ρ ),
ρ ) + 1 + ρ(cid:1) and C4 = 16nρ2η2L2 +
4 = 2η2n(1 − P )(1 + 1
ρ ) and C (cid:48)(cid:48)
ρ )
By induction of (17), it leads to

2 = 2ρ2P + 1

ρ4, C2 =

3ρ

ρ

9

uk+1 ≤ C ku1 +

k−1
(cid:88)

s=0

C s





C4
0
C (cid:48)(cid:48)
4


 E(cid:107)∇f (¯xk−s)(cid:107)2

(18)

then C k
If the spectral radius of C satisﬁes d(C) < 1,
converges to zero at the linear rate O(d(C)k) [42]. Hence,
we next prove d(C) < 1 by Lemma V.2 so as to prove the
3ρ2
result in this proposition. Because 1 −
9 +1+ρ < P < 1,
(1+ 1
ρ ) 2
3 (1−P )(1+ 1
ρ )
we have ε3 (cid:44)
ρ )+1+ρ(cid:1) > 0. Deﬁne a
9 (1+ 1
positive vector ε = [ 1
2η2 , 1, ε3]T . Then, with 0 < η <
(cid:113) 1−( 4
(cid:111)
and ρ2 < 1
the non-negative
3 ,

3ρ2P + 1
3ρ2−(1−P )(cid:0) 2

min
matrix C satisﬁes Cε ≤ 3ρ2ε, i.e.

3 + 8
2T

9 P )ρ2

(cid:110) 1

6L ,

12 + 8P
9

ρ4)

1
2η2 + C2 + C3ε3 ≤

3ρ2
2η2 ,

(2ρ2 +
2ρ2η2 1

2η2 + 2ρ2 = 3ρ2,
1
2η2 + (2ρ2P +
(1 +

2ρ2η2P
+ (1 − P )(cid:0) 2
9

1
3

(1 − P )(1 +

1
ρ

))

1
ρ
Therefore, by Lemma V.2, we obtain d(C) ≤ 3ρ2.

) + 1 + ρ(cid:1)ε3 = 3ρ2ε3.

By (18), we obtain the following bound

(cid:107)uk+1(cid:107) ≤ d(C)k(cid:107)u1(cid:107)+

k−1
(cid:88)

s=0

d(C)s

(cid:13)
(cid:13)
(cid:13)
(cid:13)

and consequently,





C4
0
C (cid:48)(cid:48)
4





(cid:13)
(cid:13)
E(cid:107)∇f (¯xk−s)(cid:107)2,
(cid:13)
(cid:13)

E(cid:107)yk+1 −¯vk+1(cid:107)2,E(cid:107)xk+1 −¯xk+1(cid:107)2,E(cid:107)τ k+1 −¯xk+1(cid:107)2(cid:111)
(cid:110)

max

≤(3ρ2)k(E(cid:107)y1 − ¯v1(cid:107)2 + E(cid:107)x1 − ¯x1(cid:107)2 + E(cid:107)τ 1 − ¯x1(cid:107)2)

k−1
(cid:88)

(3ρ2)s(C4 + C (cid:48)(cid:48)

4 )E(cid:107)∇f (¯xk−s)(cid:107)2,

(20)

(19a)

(19b)

(19c)

k
(cid:88)

max{

s=1

E(cid:107)ys − ¯vs(cid:107)2,

k
(cid:88)

s=1

E(cid:107)xs − ¯xs(cid:107)2,

k
(cid:88)

s=1

E(cid:107)τ s − ¯xs(cid:107)2}

+

s=0

≤

3ρ2R0
1 − 3ρ2 +

C4 + C (cid:48)(cid:48)
4
1 − 3ρ2

k
(cid:88)

s=1

E(cid:107)∇f (¯xs)(cid:107)2,

(16)

where we used the fact that max{a, b} ≤
for any a ≥ 0 and b ≥ 0.

√

a2 + b2 ≤ a + b

6

(cid:17)

In addition, due to 3ρ2 < 1, we have

Then, by (11) and (12), we have

k
(cid:88)

l−1
(cid:88)

(3ρ2)sal−s =

k
(cid:88)

l
(cid:88)

(3ρ2)l−sas

s=1

k
(cid:88)

as

(3ρ2)l−s

l=1

s=0

=

≤

l=1
k
(cid:88)

s=1

1
1 − 3ρ2

l=s
k
(cid:88)

s=1

as,

(21)

for any non-negative sequence (as)s∈Z+. Then, by summing
(20) over iterations and (21), we obtain

(cid:110) k
(cid:88)

max

E(cid:107)ys − ¯vs(cid:107)2,

k
(cid:88)

E(cid:107)xs − ¯xs(cid:107)2,

E(cid:107)τ s − ¯xs(cid:107)2(cid:111)

k
(cid:88)

s=1

s=1
3ρ2R0
1 − 3ρ2 +

≤

C4 + C (cid:48)(cid:48)
4
1 − 3ρ2

s=1
k
(cid:88)

s=1

E(cid:107)∇f (¯xs)(cid:107)2,

(22)

where R0 = E(cid:107)y1 − ¯v1(cid:107)2 + E(cid:107)x1 − ¯x1(cid:107)2 + E(cid:107)τ 1 − ¯x1(cid:107)2.

In next proposition, we show the evolution of the cost

function.

Proposition V.4. Suppose Assumption IV.1 hold. If the step-
size satisﬁes η ≤ 1

6L ,

E[f (¯xk+1)|F k+1]
η
2

≤f (¯xk) −

(cid:107)∇f (¯xk)(cid:107)2

(cid:16)

(cid:107)∇f (¯xk)(cid:107)2 +

2L2
n
(cid:0)6L2(cid:107)xk − ¯xk(cid:107)2 + 4L2(cid:107)τ k − ¯xk(cid:107)2(cid:1)

(3(cid:107)xk − ¯xk(cid:107)2 + 2(cid:107)τ k − ¯xk(cid:107)2)

+ η2L
η
2n

+

− η2L)(cid:107)∇f (¯xk)(cid:107)2 +

3ηL2 + 6η2L3
n

(cid:107)xk − ¯xk(cid:107)2

(cid:107)τ k − ¯xk(cid:107)2.

(24)

η
=f (¯xk) − (
2
4η2L3 + 2ηL2
n

+

Substitute ηL ≤ 1
expectation on both sides of (24), we obtain (23).

6 to the above inequality and take the total

The following lemma will be used to establish the conver-

gence of GT-VR.

[43] Let (Ω, F, P) be a probability space and
Lemma V.3.
(Ft)t∈Z+ be a ﬁltration. Let U t, ξt and ζ t be nonnegative Ft
measurable random variables for t ∈ Z+ such that

E[U t+1|Ft] ≤ U t + ξt − ζ t,

∀t = 1, 2, · · · .
Then U t converges to a random variable and (cid:80)∞
almost surely on the event {(cid:80)∞

t=1 ξt < +∞}.

(25)
t=1 ζ t < +∞

Now, we are ready to provide the proof of Theorem IV.1.

Proof of Theorem IV.1:

Proof. By proposition V.4 and (22), we have

Ef (¯xk+1)
≤Ef (¯xk) −

η
3

E(cid:107)∇f (¯xk)(cid:107)2

0 ≤f (¯x1) − f ∗ −

+

2L
3n

E(cid:107)xk − ¯xk(cid:107)2 +

4L
9n

E(cid:107)τ k − ¯xk(cid:107)2.

(23)

+ (

2L
3n

+

4L
9n

)(

k
(cid:88)

E(cid:107)∇f (¯xs)(cid:107)2

η
3

s=1
3ρ2R0
1 − 3ρ2 +

C4 + C (cid:48)(cid:48)
4
1 − 3ρ2

Proof. By ¯xk+1 = ¯xk − η¯vk and the L-smoothness of the
function f , we have

f (¯xk+1) ≤f (¯xk) − η(cid:104)∇f (¯xk), ¯vk(cid:105) +

(cid:107)¯vk(cid:107)2

η2L
2
η2L
2

=f (¯xk) − η(cid:107)∇f (¯xk)(cid:107)2 +

(cid:107)¯vk(cid:107)2

(cid:42)

− η

∇f (¯xk),

1
n

n
(cid:88)

(vk

i − ∇fi(¯xk))

i=1

≤f (¯xk) −

+

η
2

(cid:107)

1
n

η
2
n
(cid:88)

i=1

(cid:107)∇f (¯xk)(cid:107)2 +

η2L
2

(cid:107)¯vk(cid:107)2

(vk

i − ∇fi(¯xk))(cid:107)2

Taking the conditional expectation with respect to F k+1,

≤f (¯xk) −

E[f (¯xk+1)|F k+1]
η
2
n
(cid:88)

+

η
2

(cid:107)

1
n

i=1

(cid:107)∇f (¯xk)(cid:107)2 +

η2L
2

(cid:107)¯vk(cid:107)2

(vk

i − ∇fi(¯xk))(cid:107)2

k
(cid:88)

E(cid:107)∇f (¯xs)(cid:107)2)

s=1
k
(cid:88)

s=1

E(cid:107)∇f (¯xs)(cid:107)2

η
3

−

10L
9n

C4 + C (cid:48)(cid:48)
4
1 − 3ρ2 )
(cid:123)(cid:122)
(cid:125)
C

=f (¯x1) − f ∗ − (

(cid:124)

+

10L
9n

3ρ2R0
1 − 3ρ2

(cid:43)

=f (¯x1) − f ∗ − C

k
(cid:88)

s=1

E(cid:107)∇f (¯xs)(cid:107)2 +

10L
9n

3ρ2R0
1 − 3ρ2 .

(26)

Since

0 < η <

(1 − 3ρ2)

(cid:0)16ρ2L2 + (32ρ2L2 + 2)(1 − P ) ρ+1

ρ

,

(cid:1)5L

(27)

then,
η
3

C =

−

10L
9

1
1−3ρ2

(cid:16)

16ρ2η2L2 + 32(1−P )ρ2η2L2(1 +

+ 2η2(1 − P )(1 +

(cid:18)

=

η
3

1−

10L
3

1
1−3ρ2

(cid:17)
)

1
ρ
(cid:16)

16ρ2ηL2 + 32(1−P )ρ2ηL2(1 +

1
ρ

)

1
ρ

)

+ 2η(1 − P )(1 +

(cid:17)(cid:19)

1
ρ

)

>0.

It follows from (26) that

1
k

k
(cid:88)

s=1

E(cid:107)∇f (¯xs)(cid:107)2 ≤

1
kC

(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) +

10L
9n

3ρ2R0
1 − 3ρ2

(cid:3),

(28)

which implies that 1
k
Theorem IV.1.

(cid:80)k

s=1

E(cid:107)∇f (¯xs)(cid:107)2 ≤ O( 1

k ) holds in

Now, by (22) and (28), we see that

k
(cid:88)

s=1

max

(cid:110) 1
k
(cid:18) R0
1
1−3ρ2 +
k
(cid:18) C4 + C (cid:48)(cid:48)
1
4
(1−3ρ2)C
k

≤

=

E(cid:107)xs − ¯xs(cid:107)2,

(cid:16) 1
C

C4 + C (cid:48)(cid:48)
4
1−3ρ2
(cid:0)f (¯x1)−f ∗(cid:1)+

1
k

k
(cid:88)

s=1

(cid:107)τ s − ¯xs(cid:107)2(cid:111)

(cid:0)f (¯x1) − f ∗(cid:1) +

10L
9nC

(cid:17)(cid:19)

3ρ2R0
1−3ρ2
(cid:17) 3ρ2R0
1−3ρ2

(cid:19)

,

(cid:16) C4 +C (cid:48)(cid:48)
4
1−3ρ2

10L
9nC

+ 1

which implies that 1
k
IV.1 holds.

(cid:80)k

s=1

E(cid:107)xs − ¯xs(cid:107)2 ≤ O( 1

k ) in Theorem

In addition, by (11), we have

1
k

2
k

2
k

1
k

≤

≤

≤

k
(cid:88)

E(cid:107)ys − ∇f (¯xs)(cid:107)2

s=1
k
(cid:88)

(E(cid:107)ys − ¯vs(cid:107)2 + E(cid:107)¯vs − ∇f (¯xs)(cid:107)2)

s=1
k
(cid:88)

(E(cid:107)ys − ¯vs(cid:107)2 +6L2E(cid:107)xs −¯xs(cid:107)2 +4L2E(cid:107)τ s − ¯xs(cid:107)2)

s=1

(2 + 20L2)

(cid:16) C4 + C (cid:48)(cid:48)
4
(1 − 3ρ2)C

(cid:0)f (¯x1) − f ∗(cid:1)

+

(cid:16) C4 + C (cid:48)(cid:48)
4
1 − 3ρ2

10L
9nC

+ 1

(cid:17) 3ρ2R0
1 − 3ρ2

(cid:17)

,

(cid:80)k

which implies that 1
k ) in
k
Theorem IV.1 holds. Finally, by Proposition V.4 and Lemma
V.3, we see that f (¯xk) converges and (cid:80)∞
k=1 E[(cid:107)∇f (¯xk)(cid:107)2] <
+∞.

E(cid:107)ys − ∇f (¯xs)(cid:107)2 ≤ O( 1

s=1

With the convergence result

in Theorem IV.1 and the
deﬁnition of (cid:15)-accurate stationary point in Deﬁnition IV.1, we
present the analysis of Corollary IV.1 in the following.

Proof of Corollary IV.1:

Proof. By deﬁnition IV.1 and (28), it is clear that to ﬁnd an
(cid:15)-accurate stationary point, it is sufﬁcient to ﬁnd the iterations
k such that

1
kC
(cid:16)

(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) +

10L
9n

3ρ2R0
1 − 3ρ2

(cid:3) ≤ (cid:15)

(29)

1
1−3ρ2

(cid:0)16ρ2ηL2 + 32(1 − P )ρ2ηL2(1 +
. If the step-size η satisﬁes (9), C ≥ η
9

1−3ρ2 hold. Thus, when the iteration k satisﬁes
10
9n

(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) +

(cid:3) ≤ k,

R0
η

(30)

1

where C = η
1 − 10L
3
3
ρ )(cid:1)(cid:17)
ρ )+2η(1−P )(1+ 1
ηL ≥ 3ρ2
and 1
9
η(cid:15)

then, the inequality (29) holds. Therefore, the iteration com-
plexity of GT-VR is O( 1
η(cid:15)

(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) + R0

(cid:3)).

nη

7

i=1(P mi + 2). Since (cid:80)n

For the gradient computations, since there are P mi + 2
gradient computations at each iteration of agent i, the number
of gradient computations across all agents is the iteration com-
plexity multiplied by (cid:80)n
i=1 mi = M ,
(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) +
the computational complexity is O( P M +n
R0
nη

(cid:3)).
At each iteration, each agent i communicate twice with its
neighbors. Let Ni denote the number of neighbors of agent
i. Then, the communication complexity across all agents is
(cid:3)) by multiplying the iteration
i=1 Ni
O(
η(cid:15)
complexity by (cid:80)n

(cid:2)(cid:0)f (¯x1) − f ∗(cid:1) + R0
i=1 Ni.

(cid:80)n

nη

η(cid:15)

VI. SIMULATION

To verify the efﬁcacy of the proposed algorithm, we con-
sider the classical binary classiﬁcation problem, which is to
ﬁnd one optimal predictor x ∈ Rd on a popular logistic regres-
sion learning model. We compare the proposed algorithm with
recently proposed algorithms GT-SAGA [32], GT-SARAH
[31] and D-GET [30]. The learning model is to optimize the
following problem

min
x∈Rd

f (x) (cid:44) 1
n

n
(cid:88)

i=1

fi(x),

fi(x) =

1
mi

mi(cid:88)

1

1 + exp(lija(cid:48)

ijx)

j=1
where aij ∈ Rd, lij ∈ {−1, 1} and {aij, lij}mi
set of training samples of agent i.

+ λ1(cid:107)x(cid:107)2
2,

(31)

j=1 denotes the

TABLE I
REAL DATA FOR BLACK-BOX BINARY CLASSIFICATION

datasets
a9a
w8a
covtype.binary

#samples
32561
64700
581012

#features
123
300
54

#classes
2
2
2

In this experiment, we use the publicly available real
datasets1, which are summarized in Table I. All algorithms
are applied over a ten-agent undirected connected network
with a doubly stochastic adjacent matrix W to solve (31).
Meanwhile, λ1 (cid:44) 5 × 10−4. For the proposed GT-VR
algorithm, the probability P is taken as 0.3 and the step-
size is taken as 0.1. Note that the ranges of step-size and
possibility provided in Theorem IV.1 are rigorous theoretical
results. In practice, we can adjust
them according to the
convergence performance. For comparison, the local variables
in all algorithms are initialized as zero and all the algorithms
take same step-sizes. The simulation codes are provided at
https://github.com/managerjiang/GT VR Simulation.

Deﬁne D(¯x) = (cid:80)10

j=1 wij(xi − xj). The trajectory
of D(¯x) converging to zero implies that the variable estimates
of different agents achieve consensus. For different datasets,
the trajectories of cost function f and D(¯x) are shown in
Fig. 1. We observe that for all datasets, the algorithms all
have good consensus performance. For the trajectories of cost

i=1 x(cid:48)
i

(cid:80)10

1a9a, w8a

and

covtype.binary

are

from the

website

www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/.

8

proposed algorithm presents better convergence performance
than state-of-the-art algorithms.

VIII. APPENDIX

A. Proof of Proposition V.1

For

Proof. (a)
i=1σ(lk
σ(∪n
property of the conditional expectation, we have

(cid:44)
i ), F k) and clearly F k ⊆ Ak. By the tower

convenience, we

deﬁne Ak

E[(cid:107)vk

i − ∇fi(xk

i )(cid:107)2|F k] = E[E[(cid:107)vk

i − ∇fi(xk

i )(cid:107)2|Ak]|F k].
(32)

For E[(cid:107)vk

i − ∇fi(xk

i )(cid:107)2|Ak], we have

i )−(∇fi(xk
(τ k
i )(cid:107)2|Ak]
(τ k

i )−∇fi(τ k

i ))(cid:107)2|Ak]

i − ∇fi(xk
(xk
(xk

i )(cid:107)2|Ak]
i )−∇fi,sk
i ) − ∇fi,sk

i

E[(cid:107)vk
=E[(cid:107)∇fi,sk
≤E[(cid:107)∇fi,sk
mi(cid:88)

i

i

i

=

1
mi

j=1
≤2L2(cid:107)xk

(cid:107)∇fi,j(xk

i )−∇fi,j(¯xk) + ∇fi,j(¯xk)−∇fi,j(τ k

i )(cid:107)2

i − ¯xk(cid:107)2 + 2L2(cid:107)τ k

i − ¯xk(cid:107)2

(33)

where the ﬁrst inequality is from the standard conditional
variance decomposition

i |Ak](cid:107)2|Ak]

E[(cid:107)ak
=E[(cid:107)ak
≤E[(cid:107)ak

i − E[ak
i (cid:107)2|Ak] − (cid:107)E[ak
i (cid:107)2|Ak],

i |Ak](cid:107)2

with ak
i = ∇fi,sk
inequality to (32),

i

(xk

i ) − ∇fi,sk

i

(τ k

i ). Substitute the above

i − ∇fi(xk

i )(cid:107)2|F k]

E[(cid:107)vk
≤2L2E[(cid:107)xk

i − ¯xk(cid:107)2|F k] + 2L2E[(cid:107)τ k

i − ¯xk(cid:107)2F k].

(34)

The proof follows by summing (34) over i and taking the total
expectation on both sides.

(b) With the result in (a), E[(cid:107) 1
n

(cid:80)n

i=1(vk

i −∇fi(¯xk))(cid:107)2|F k]

satisﬁes

E[(cid:107)

1
n

n
(cid:88)

i=1

(vk

i − ∇fi(¯xk))(cid:107)2|F k]

n
(cid:88)

E[(cid:107)vk

i − ∇fi(¯xk)(cid:107)2|F k]

≤

1
n

i=1
n
(cid:88)

≤

(34)
≤

i=1

(2E[(cid:107)vk

1
n
+ 2E[(cid:107)∇fi(xk
1
n

n
(cid:88)

i=1

i − ∇fi(xk

i )(cid:107)2|F k]

i ) − ∇fi(¯xk)(cid:107)2|F k])

(cid:0)2(2L2E[(cid:107)xk

i − ¯xk(cid:107)2|F k] + 2L2E[(cid:107)τ k

i − ¯xk(cid:107)2|F k])

+ 2L2E[(cid:107)xk

i − ¯xk(cid:107)2|F k](cid:1)

=

1
n

n
(cid:88)

(6L2E[(cid:107)xk

i − ¯xk(cid:107)2|F k] + 4L2E[(cid:107)τ k

i − ¯xk(cid:107)2|F k]).

i=1

(35)

The proof follows by taking the total expectation on both sides
of (35).

(a) a9a dataset

(b) w8a dataset

(c) covtype.binary dataset

Fig. 1. The convergence behaviors of GT-VR, GT-SAGA, GT-HSGD, GT-
SARAH, D-GET over the a9a and w8a datasets

function, the algorithm GT-VR decays faster than the state-
of-the-art algorithms D-GET, GT-SAGA and GT-SARAH,
especially for the w8a dataset, demonstrating the excellent
iteration complexity of GT-VR.

VII. CONCLUSION

Focusing on distributed non-convex stochastic optimization,
this paper has developed a novel variance-reduced distributed
stochastic ﬁrst-order algorithm over undirected and weight-
balanced directed graphs by combining gradient tracking and
variance reduction. The variance reduction technique makes
use of Bernoulli distribution to handle the variance by stochas-
tic gradients. The proposed algorithm converges with O( 1
k )
rate and has lower iteration complexity compared with some
existing excellent algorithms. By comparative simulations, the

050010001500iterations00.511.522.533.544.510-3GT-VRGT-SAGAGT-SARAHD-GET050010001500iterations0.250.30.350.40.450.5GT-VRGT-SAGAGT-SARAHD-GET050010001500iterations00.0020.0040.0060.0080.010.0120.014GT-VRGT-SAGAGT-SARAHD-GET050010001500iterations0.20.250.30.350.40.450.5GT-VRGT-SAGAGT-SARAHD-GET050010001500iterations00.20.40.60.811.210-3GT-VRGT-SAGAGT-SARAHD-GET050010001500iterations0.40.410.420.430.440.450.460.470.480.490.5GT-VRGT-SAGAGT-SARAHD-GET(c) By Young’s inequality and the result in (b), E[(cid:107)¯vk(cid:107)2]

By (37) and (39),

9

satisﬁes

E[(cid:107)¯vk(cid:107)2] ≤2E[(cid:107)∇f (¯xk)(cid:107)2] + 2E[(cid:107)

1
n

n
(cid:88)

i=1

(vk

i − ∇fi(¯xk))(cid:107)2]

≤2E[(cid:107)∇f (¯xk)(cid:107)2] +

+ 4L2E[(cid:107)τ k

i=1
i − ¯xk(cid:107)2]),

n
(cid:88)

(6L2E[(cid:107)xk

i − ¯xk(cid:107)2]

2
n

where the last inequality follows from (11).

B. Proof of Proposition V.2
Proof. (a) Recall that ¯xk = 1n ⊗ ¯xk. By (5) and (7),

(cid:107)xk+1 − ¯xk+1(cid:107)2

=(cid:107)(W ⊗ Id)[xk − ¯xk − η(yk − ¯vk)](cid:107)2
≤2ρ2(cid:107)xk − ¯xk(cid:107)2 + 2ρ2η2(cid:107)yk − ¯vk(cid:107)2.

(36)

We take the total expectation on both sides of (36) to obtain

E[(cid:107)xk+1 − ¯xk+1(cid:107)2]

≤2ρ2E[(cid:107)xk − ¯xk(cid:107)2] + 2ρ2η2E[(cid:107)yk − ¯vk(cid:107)2].

(37)

(b) Recall the Bernoulli distribution in GT-VR that

E[(cid:107)¯xk+1 − τ k+1(cid:107)2]

≤P (cid:0)2ρ2E(cid:107)¯xk − xk(cid:107)2 + 2ρ2η2E(cid:107)yk − ¯vk(cid:107)2(cid:1)

)(cid:0)2nE(cid:107)∇f (¯xk)(cid:107)2 + 12L2E(cid:107)xk − ¯xk(cid:107)2

(cid:16)

η
β

(η2 +

+ (1−P )
+ 8L2E(cid:107)τ k − ¯xk(cid:107)2(cid:1) + (1 + ηβ)E(cid:107)¯xk − τ k(cid:107)2(cid:17)
η
)12L2(cid:1)E(cid:107)¯xk − xk(cid:107)2
β

=(cid:0)2ρ2P + (1 − P )(η2 +

+ 2ρ2η2P E(cid:107)yk − ¯vk(cid:107)2 + (1 − P )(cid:0)(η2 +

)8L2

η
β

)2n(1−P )E(cid:107)∇f (¯xk)(cid:107)2.

+ (1+ηβ)(cid:1)E(cid:107)¯xk −τ k(cid:107)2 +(η2 +

η
β

(c) By (6), we have

yk+1 − ¯vk+1

=(W ⊗ Id)(yk − ¯vk + vk+1 − vk − ¯vk+1 + ¯vk).

(40)

For the term vk+1 − vk − ¯vk+1 + ¯vk in (40), it satisﬁes

E[I

i =1}|F k+1] = P and E[I
{lk+1

{lk+1
i

(cid:54)=1}|F k+1] = 1 − P.

(cid:107)vk+1 − vk − ¯vk+1 + ¯vk(cid:107)2

Then,

=(cid:107)vk+1 −vk(cid:107)2 +(cid:107)¯vk+1 −¯vk(cid:107)2 −2

n
(cid:88)

(cid:104)vk+1

i −vk

i , ¯vk+1 −¯vk(cid:105)

i

E[(cid:107)¯xk+1 − τ k+1

(cid:107)2|F k+1]
=E(cid:2)(cid:107)¯xk+1 − (cid:0)I
i =1}xk+1
{lk+1
=E[(cid:107)¯xk+1(cid:107)2|F k+1]

i + I

{lk+1
i

(cid:54)=1}τ k

i

(cid:1)(cid:107)2|F k+1(cid:3)

=(cid:107)vk+1 − vk(cid:107)2 − (cid:107)¯vk+1 − ¯vk(cid:107)2
≤(cid:107)vk+1 − vk(cid:107)2.

i=1

(41)

(cid:54)=1}τ k

i + I
+ E[(cid:107)I
i =1}xk+1
{lk+1
{lk+1
i
− 2E(cid:2)(cid:104)¯xk+1, I
i + I
i =1}xk+1
{lk+1
{lk+1
i
=(cid:107)¯xk+1(cid:107)2 + P (cid:107)xk+1
(cid:107)2 + (1 − P )(cid:107)τ k
i
− 2(cid:104)¯xk+1, P xk+1
i + (1 − P )τ k
i (cid:105)
(cid:107)2 + (1 − P )(cid:107)¯xk+1 − τ k

i (cid:107)2|F k+1]
(cid:54)=1}τ k
i (cid:107)2

=P (cid:107)¯xk+1 − xk+1

i (cid:107)2.

i

i (cid:105)|F k+1(cid:3)

Hence, it follows from Lemma V.1, (40) and (41) that

(cid:107)yk+1 − ¯vk+1(cid:107)2

≤2ρ2((cid:107)yk − ¯vk(cid:107)2 + (cid:107)vk+1 − vk(cid:107)2).

(38)

Summing over i and taking the total expectation on both sides,

E[(cid:107)¯xk+1 − τ k+1(cid:107)2]

=P E[(cid:107)¯xk+1 − xk+1(cid:107)2] + (1 − P )E[(cid:107)¯xk+1 − τ k(cid:107)2].

For the second term,

E[(cid:107)¯xk+1 − τ k(cid:107)2]

=E[(cid:107)¯xk+1 − ¯xk + ¯xk − τ k(cid:107)2]
=E[η2n(cid:107)¯vk(cid:107)2 + (cid:107)¯xk − τ k(cid:107)2 − 2η(cid:104)¯vk, ¯xk − τ k(cid:105)]
≤E[η2n(cid:107)¯vk(cid:107)2 + (cid:107)¯xk − τ k(cid:107)2 +

n(cid:107)¯vk(cid:107)2 + ηβ(cid:107)¯xk − τ k(cid:107)2]

η
β

=(η2 +

≤(η2 +

)nE[(cid:107)¯vk(cid:107)2] + (1 + ηβ)E[(cid:107)¯xk − τ k(cid:107)2]

η
β
η
β
8L2
E(cid:107)τ k − ¯xk(cid:107)2(cid:1) + (1 + ηβ)E[(cid:107)¯xk − τ k(cid:107)2],
n

)n(cid:0)2E(cid:107)∇f (¯xk)(cid:107)2 +

E(cid:107)xk − ¯xk(cid:107)2

12L2
n

+

where the last inequality holds by Proposition V.1 (c).

By taking the total expectation on both sides,

E(cid:107)yk+1 −¯vk+1(cid:107)2 ≤ 2ρ2E(cid:107)yk −¯vk(cid:107)2 +2ρ2E(cid:107)vk+1 −vk(cid:107)2.

The second term E(cid:107)vk+1 − vk(cid:107)2 in the above inequality
satisﬁes

E(cid:107)vk+1 − vk(cid:107)2

≤2E[(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2]

+ 2E[(cid:107)vk+1 − vk − ∇f (xk+1) + ∇f (xk)(cid:107)2]

≤2L2E(cid:107)xk+1 −xk(cid:107)2 +2E[(cid:107)vk+1 −vk −∇f (xk+1)+∇f (xk)(cid:107)2]
≤2L2E(cid:107)xk+1 − xk(cid:107)2 + 4E(cid:107)vk+1 − ∇f (xk+1)(cid:107)2

+ 4E(cid:107)vk − ∇f (xk)(cid:107)2

(39)

≤2L2E(cid:107)xk+1 − xk(cid:107)2 + 8L2E(cid:107)xk+1 − ¯xk+1(cid:107)2

+ 8L2E(cid:107)τ k+1 − ¯xk+1(cid:107)2
+ 8L2E(cid:107)xk − ¯xk(cid:107)2 + 8L2E(cid:107)τ k − ¯xk(cid:107)2,

(42)

where the last equality is from the result in Proposition V.1
(a). Then, for the ﬁrst term (cid:107)xk+1 − xk(cid:107)2 in (42), it satisﬁes

Hence, E(cid:107)yk+1 − ¯vk+1(cid:107)2 satisﬁes

E(cid:107)yk+1 − ¯vk+1(cid:107)2

10

≤2ρ2E(cid:107)yk − ¯vk(cid:107)2 + 2ρ2E(cid:107)vk+1 − vk(cid:107)2
≤2ρ2E(cid:107)yk − ¯vk(cid:107)2 + 2ρ2(cid:2)2L2E(cid:107)xk+1 − xk(cid:107)2

+ 8L2E(cid:107)xk+1 − ¯xk+1(cid:107)2 + 8L2E(cid:107)τ k+1 − ¯xk+1(cid:107)2
+ 8L2E(cid:107)xk − ¯xk(cid:107)2 + 8L2E(cid:107)τ k − ¯xk(cid:107)2(cid:3)

(43)

≤ 2ρ2E(cid:107)yk − ¯vk(cid:107)2 +2ρ2(cid:104)

2L2(cid:16)(cid:0)4+ 24η2L2(cid:1)E(cid:107)xk − ¯xk(cid:107)2

(cid:107)xk+1 − xk(cid:107)2

=(cid:107)(W ⊗ Id − Id)xk − η(W ⊗ Id)yk(cid:107)2
=(cid:107)(W ⊗ Id − Id)(cid:0)xk − ¯xk(cid:1) − η(W ⊗ Id)(cid:0)yk − ¯vk(cid:1)

− η1n ⊗ ¯vk(cid:107)2

≤2(cid:107)(W ⊗ Id − Id)(cid:0)xk − ¯xk(cid:1) − η(W ⊗ Id)(cid:0)yk − ¯vk(cid:1)(cid:107)2

+ 2η2n(cid:107)¯vk(cid:107)2

≤4(cid:107)xk − ¯xk(cid:107)2 + 4η2ρ2(cid:107)yk − ¯vk(cid:107)2 + 2η2n(cid:107)¯vk(cid:107)2.

+ 8L2E(cid:107)xk+1 − ¯xk+1(cid:107)2

+ 4η2ρ2E(cid:107)yk − ¯vk(cid:107)2 + 4η2nE(cid:107)∇f (¯xk)(cid:107)2
+16η2L2E(cid:107)τ k − ¯xk(cid:107)2(cid:17)
+ 8L2E(cid:107)τ k+1 − ¯xk+1(cid:107)2
+ 8L2E(cid:107)xk − ¯xk(cid:107)2 + 8L2E(cid:107)τ k − ¯xk(cid:107)2(cid:105)
(cid:34)
2L2(cid:16)(cid:0)4+ 24η2L2(cid:1)E(cid:107)xk − ¯xk(cid:107)2
≤ 2ρ2E(cid:107)yk − ¯vk(cid:107)2 +2ρ2

(13),(14)

+ 4η2ρ2E(cid:107)yk − ¯vk(cid:107)2 + 4η2nE(cid:107)∇f (¯xk)(cid:107)2
+16η2L2E(cid:107)τ k − ¯xk(cid:107)2(cid:17)

(cid:26)(cid:104)

+ 8L2

2ρ2E(cid:107)xk − ¯xk(cid:107)2 + 2ρ2η2E(cid:107)yk − ¯vk(cid:107)2(cid:105)

(cid:104)

+

(1 − P )(cid:0)(η2 +

)8L2 + (1 + ηβ)(cid:1)E(cid:107)τ k − ¯xk(cid:107)2

η
β

)2n(1−P )E(cid:107)∇f (¯xk)(cid:107)2 +2ρ2η2P E(cid:107)yk −¯vk(cid:107)2

+(η2 +

η
β
+ (cid:0)(1 − P )(η2 +

)12L2 + 2ρ2P (cid:1)E(cid:107)xk − ¯xk(cid:107)2(cid:105)(cid:27)

η
β

(cid:35)

Taking the total expectation, we have

+ 8L2E(cid:107)xk − ¯xk(cid:107)2 + 8L2E(cid:107)τ k − ¯xk(cid:107)2

≤(2ρ2 + 48η2L2ρ4 + 32P η2L2ρ4)E(cid:107)yk − ¯vk(cid:107)2

+ 16ρ2L2(cid:0)1 + 6η2L2 + (2 + 2P )ρ2

+ 12L2(1 − P )(η2 +

η
β

)(cid:1)E[(cid:107)xk − ¯xk(cid:107)2]
η
β

+ 16ρ2L2(cid:0)4η2L2 + (1 − P )[1 + ηβ + (η2 +

)8L2](cid:1)

E(cid:107)xk+1 − xk(cid:107)2

(12)
≤ 4E(cid:107)xk − ¯xk(cid:107)2 + 4η2ρ2E(cid:107)yk − ¯vk(cid:107)2

(cid:16)

+ 2η2n

2E(cid:107)∇f (¯xk)(cid:107)2 +

+ 4L2E(cid:107)τ k

i − ¯xk(cid:107)2)

n
(cid:88)

i=1

(6L2E(cid:107)xk

i − ¯xk(cid:107)2

2
n
(cid:17)

≤4E(cid:107)xk − ¯xk(cid:107)2 + 4η2ρ2E(cid:107)yk − ¯vk(cid:107)2

+ 4η2nE(cid:107)∇f (¯xk)(cid:107)2 + 24η2L2E(cid:107)xk − ¯xk(cid:107)2
+ 16η2L2E(cid:107)τ k − ¯xk(cid:107)2

=(cid:0)4 + 24η2L2(cid:1)E(cid:107)xk − ¯xk(cid:107)2 + 4η2ρ2E(cid:107)yk − ¯vk(cid:107)2
+ 4η2nE(cid:107)∇f (¯xk)(cid:107)2 + 16η2L2E(cid:107)τ k − ¯xk(cid:107)2.

(43)

E[(cid:107)τ k − ¯xk(cid:107)2]

+ 16ρ2L2n(cid:0)η2 + 2(1 − P )(η2 +

)(cid:1)E[(cid:107)∇f (¯xk)(cid:107)2].

η
β

Now, with Assumption IV.1, we have proved all inequalities
in Proposition V.2.

REFERENCES

[1] S. Patterson, Y. C. Eldar, and I. Keidar, “Distributed compressed sensing
for static and time-varying networks,” IEEE Transactions on Signal
Processing, vol. 62, no. 19, pp. 4931–4946, 2014.

[2] X. Wang, Y. Hong, and H. Ji, “Distributed optimization for a class
of nonlinear multiagent systems with disturbance rejection,” IEEE
Transactions on Cybernetics, vol. 46, no. 7, pp. 1655–1666, 2016.
[3] Y. D. X. S. Xiong Menghui, Zhang Baoyong, “Distributed quantized
mirror descent for strongly convex optimization over time-varying
directed graph,” SCIENCE CHINA Information Sciences, pp. 1–1, 2021.
[4] B. Gu, A. Xu, Z. Huo, C. Deng, and H. Huang, “Privacy-preserving
asynchronous vertical federated learning algorithms for multiparty col-
laborative learning,” IEEE Transactions on Neural Networks and Learn-
ing Systems, pp. 1–13, 2021.

11

[26] A. Nedi´c, A. Olshevsky, and W. Shi, “Achieving geometric convergence
for distributed optimization over time-varying graphs,” SIAM Journal
on Optimization, vol. 27, no. 4, pp. 2597–2633, 2017.
[Online].
Available: https://doi.org/10.1137/16M1084316

[27] W. Shi, Q. Ling, G. Wu, and W. Yin, “EXTRA: an exact ﬁrst-order
algorithm for decentralized consensus optimization,” SIAM Journal on
Optimization, vol. 25, no. 2, pp. 944–966, 2015. [Online]. Available:
https://doi.org/10.1137/14096668X

[28] N. S. Aybat, Z. Wang, T. Lin, and S. Ma, “Distributed linearized alter-
nating direction method of multipliers for composite convex consensus
optimization,” IEEE Transactions on Automatic Control, vol. 63, no. 1,
pp. 5–20, 2018.

[29] Q. Ling, W. Shi, G. Wu, and A. Ribeiro, “DLM: Decentralized linearized
alternating direction method of multipliers,” IEEE Transactions on
Signal Processing, vol. 63, pp. 4051–4064, 2015.

[30] H. Sun, S. Lu, and M. Hong, “Improving the sample and communication
complexity for decentralized non-convex optimization: A joint gradient
estimation and tracking approach,” 2019.

[31] R. Xin, U. A. Khan, and S. Kar, “A near-optimal stochastic gradient

method for decentralized non-convex ﬁnite-sum optimization,” 2020.

[32] ——, “A fast randomized incremental gradient method for decentralized

non-convex optimization,” 2020.

[33] ——, “An improved convergence analysis for decentralized online
stochastic non-convex optimization,” IEEE Transactions on Signal Pro-
cessing, vol. 69, pp. 1842–1858, 2021.

[34] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, “d2: Decentralized
training over decentralized data,” in Proceedings of the 35th Interna-
tional Conference on Machine Learning, ser. Proceedings of Machine
Learning Research, vol. 80. PMLR, 10–15 Jul 2018, pp. 4848–4856.
[35] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can
decentralized algorithms outperform centralized algorithms? a case study
for decentralized parallel stochastic gradient descent,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, ser. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.,
2017, p. 5336–5346.

[36] T. Yu, X.-W. Liu, Y.-H. Dai, and J. Sun, “A minibatch proximal
stochastic recursive gradient algorithm using a trust-region-like scheme
and barzilai-borwein stepsizes,” IEEE Transactions on Neural Networks
and Learning Systems, pp. 1–12, 2020.

[37] X.-B. Jin, X.-Y. Zhang, K. Huang, and G.-G. Geng, “Stochastic conju-
gate gradient algorithm with variance reduction,” IEEE Transactions on
Neural Networks and Learning Systems, vol. 30, no. 5, pp. 1360–1369,
2019.

[38] M. Mohammadi Amiri and D. G¨und¨uz, “Machine learning at

the
wireless edge: Distributed stochastic gradient descent over-the-air,” IEEE
Transactions on Signal Processing, vol. 68, pp. 2155–2169, 2020.
[39] Y. Duan, N. Wang, and J. Wu, “Minimizing training time of distributed
machine learning by reducing data communication,” IEEE Transactions
on Network Science and Engineering, vol. 8, no. 2, pp. 1802–1814,
2021.

[40] Y. Tang, J. Zhang, and N. Li, “Distributed zero-order algorithms for
nonconvex multiagent optimization,” IEEE Transactions on Control of
Network Systems, vol. 8, no. 1, pp. 269–281, 2021.

[41] R. Xin, U. A. Khan, and S. Kar, “A hybrid variance-reduced method for

decentralized stochastic non-convex optimization,” 2021.

[42] R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge University

Press, 1990.
[43] H. Robbins

and D. Siegmund,

theorem for
non negative almost supermartingales and some applications,” in
Optimizing Methods in Statistics. Academic Press, 1971, pp. 233–257.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
B9780126045505500158

“A convergence

[5] R. Xin, S. Pu, A. Nedi´c, and U. A. Khan, “A general framework for
decentralized optimization with ﬁrst-order methods,” Proceedings of the
IEEE, vol. 108, no. 11, pp. 1869–1889, 2020.

[6] R. Tutunov, H. Bou-Ammar, and A. Jadbabaie, “Distributed Newton
method for large-scale consensus optimization,” IEEE Transactions on
Automatic Control, vol. 64, no. 10, pp. 3983–3994, 2019.

[7] Y. Lou, L. Yu, S. Wang, and P. Yi, “Privacy preservation in distributed
subgradient optimization algorithms,” IEEE Transactions on Cybernet-
ics, vol. 48, no. 7, pp. 2154–2165, 2018.

[8] S. Mao, Y. Tang, Z. Dong, K. Meng, Z. Y. Dong, and F. Qian, “A privacy
preserving distributed optimization algorithm for economic dispatch
over time-varying directed networks,” IEEE Transactions on Industrial
Informatics, vol. 17, no. 3, pp. 1689–1701, 2021.

[9] Z. Deng and Y. Hong, “Multi-agent optimization design for autonomous
lagrangian systems,” Unmanned Systems, vol. 04, no. 01, pp. 5–13,
2016. [Online]. Available: https://doi.org/10.1142/S230138501640001X
[10] G. C. W. R. X. Y. Wenwu YU, Jinde CAO, “Special focus on distributed
cooperative analysis, control and optimization in networks,” SCIENCE
CHINA Information Sciences, vol. 60, no. 11, pp. 1–1, 2017.

[11] Z. Li, B. Liu, and Z. Ding, “Consensus-based cooperative algorithms
for training over distributed data sets using stochastic gradients,” IEEE
Transactions on Neural Networks and Learning Systems, pp. 1–11, 2021.
[12] W. Li, Z. Wu, T. Chen, L. Li, and Q. Ling, “Communication-censored
distributed stochastic gradient descent,” IEEE Transactions on Neural
Networks and Learning Systems, pp. 1–13, 2021.

[13] D. Yuan, D. W. C. Ho, and S. Xu, “Stochastic strongly convex op-
timization via distributed epoch stochastic gradient algorithm,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 32, no. 6,
pp. 2344–2357, 2021.

[14] S. SundharRam, A. Nedic, and V. V. Veeravalli, “Distributed stochastic
subgradient projection algorithms for convex optimization,” Journal of
Optimization Theory & Applications, vol. 147, no. 3, pp. 516–545, 2010.
[15] Z. Yu, D. W. C. Ho, and D. Yuan, “Distributed randomized gradient-free
mirror descent algorithm for constrained optimization,” IEEE Transac-
tions on Automatic Control, pp. 1–1, 2021.

[16] J. Lei, H.-F. Chen, and H.-T. Fang, “Asymptotic properties of primal-
dual algorithm for distributed stochastic optimization over random
networks with imperfect communications,” SIAM Journal on Control
and Optimization, vol. 56, no. 3, pp. 2159–2188, 2018. [Online].
Available: https://doi.org/10.1137/16M1086133

[17] A. Nedic and A. Olshevsky, “Distributed optimization over time-varying
directed graphs,” IEEE Transactions on Automatic Control, vol. 60,
no. 3, pp. 601–615, 2013.

[18] A. Defazio, F. Bach, and S. Lacoste-Julien, “SAGA: a fast incremental
gradient method with support for non-strongly convex composite objec-
tives,” in Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 1, ser. NIPS’14. Cambridge,
MA, USA: MIT Press, 2014, p. 1646–1654.

[19] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent using
predictive variance reduction,” in Proceedings of the 26th International
Conference on Neural Information Processing Systems - Volume 1, ser.
NIPS’13.
Red Hook, NY, USA: Curran Associates Inc., 2013, p.
315–323.

[20] L. M. Nguyen, J. Liu, K. Scheinberg, and M. Tak´aˇc, “SARAH: a
novel method for machine learning problems using stochastic recursive
the 34th International Conference on
gradient,” in Proceedings of
Machine Learning - Volume 70, ser. ICML’17.
JMLR.org, 2017, p.
2613–2621.

[21] J. Lei and U. V. Shanbhag, “Asynchronous variance-reduced block
schemes for composite non-convex stochastic optimization: block-
speciﬁc steplengths and adapted batch-sizes,” Optimization Methods
and Software, vol. 0, no. 0, pp. 1–31, 2020. [Online]. Available:
https://doi.org/10.1080/10556788.2020.1746963

[22] R. Xin, U. A. Khan, and S. Kar, “Variance-reduced decentralized
stochastic optimization with accelerated convergence,” IEEE Trans.
Signal Process., vol. 68, pp. 6255–6271, 2020. [Online]. Available:
https://doi.org/10.1109/TSP.2020.3031071

[23] J. Lei, P. Yi, J. Chen, and Y. Hong, “A communication-efﬁcient linearly
convergent algorithm with variance reduction for distributed stochastic
optimization,” in 2020 European Control Conference (ECC), 2020, pp.
1250–1255.

[24] ——, “Linearly convergent algorithm with variance reduction for dis-

tributed stochastic optimization,” 2020.
[25] P. D. Lorenzo and G. Scutari, “NEXT:

optimization,” IEEE Trans. Signal
vol. 2, no. 2, pp. 120–136, 2016.
//doi.org/10.1109/TSIPN.2016.2524588

in-network nonconvex
Inf. Process. over Networks,
[Online]. Available: https:

