2
2
0
2

l
u
J

5
2

]

R
G
.
s
c
[

2
v
8
8
9
0
1
.
3
0
2
2
:
v
i
X
r
a

Less Is More: Eﬃcient Networked VR Trans-
formation Handling Using Geometric Al-
gebra

Manos Kamarianakis, Ilias Chrysovergis, Nick Lydatakis, Mike
Kentros and George Papagiannakis

Abstract. As shared, collaborative, networked, virtual environments
become increasingly popular, various challenges arise regarding the ef-
ﬁcient transmission of model and scene transformation data over the
network. As user immersion and real-time interactions heavily depend
on VR stream synchronization, transmitting the entire data set does not
seem a suitable approach, especially for sessions involving a large number
of users. Session recording is another momentum-gaining feature of VR
applications that also faces the same challenge. The selection of a suitable
data format can reduce the occupied volume, while it may also allow
eﬀective replication of the VR session and optimized post-processing for
analytics and deep-learning algorithms.

In this work, we propose two algorithms that can be applied in the
context of a networked multiplayer VR session, to eﬃciently transmit
the displacement and orientation data from the users’ hand-based VR
HMDs. Moreover, we present a novel method for eﬀective VR recording
of the data exchanged in such a session. Our algorithms, based on the use
of dual-quaternions and multivectors, impact the network consumption
rate and are highly eﬀective in scenarios involving multiple users. By
sending less data over the network and interpolating the in-between
frames locally, we manage to obtain better visual results than current
state-of-the-art methods. Lastly, we prove that, for recording purposes,
storing less data and interpolating them on-demand yields a data set
quantitatively close to the original one.

Mathematics Subject Classiﬁcation (2010). Primary 68U05.
Keywords. Interpolation, Keyframe Generation, VR Recorder, Geomet-
ric Algebra.

The authors are aﬃliated with the University of Crete, Greece and the ORamaVR com-
pany[26]. This is an extended version of work originally presented in the CGI 2021 conference,
on the ENGAGE workshop [16]. Corresponding Author: kamarianakis@uoc.gr.

 
 
 
 
 
 
2 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

(a)

(b)

(c)

Figure 1. Catching a tool in a VR collaborative scenario. (a)
A user throws a tool (in our case a medical drill) at another.
(b) The object’s keyframes, sent by the user that threw it,
are interpolated using multivector LERP (see Section 3.4) on
the receiver’s VR engine. (c) The receiver manages to catch
the tool, as a result of the eﬀective frame generation that is
visualized in his/her HMD.

1. Introduction

The rise of the 5G networks and their augmented capabilities, along with the
increasing need for remote working and collaboration, due to the extended
pandemic, have inﬂuenced the research initiatives in many scientiﬁc areas.
Virtual Reality (VR) is an area drastically aﬀected by these facts. Rapid
technological advancements of the past decades, both on hardware and software
level, have enabled VR experts to deliver powerful visualization algorithms,
optimized to provide an immersive user experience. VR applications that
exploit modern GPU capabilities enable the simulation of high-ﬁdelity content
of a great variety. Driven by modern needs and expectations, collaborative,
shared virtual environments (CVEs) have been, upon their inception, a highly
active research topic [3, 24, 30, 33]. One of the main research objectives regards
the immersion of the users that are present and interact within the same
virtual environment. Such a context is common and dictated by modern era
industries that require frequent and eﬃcient remote communication between
people. As safety has become a ﬁrst priority due to the pandemic, VR has
been proven to be a solution that natively addresses many of the challenges
posed, while some others require extra eﬀort to be eﬀectively resolved.

One such interesting problem that arises within the context of a VR CVE,
is the provisioning of users with the same high quality of experience (QoE) in
both single- and multi-player VR sessions. As modern Head-mounted Displays
(HMDs) are able to support intensive resource-demanding VR applications,
we can deliver high-ﬁdelity and immersive content in single user scenarios,
even when the user interacts in real-time with the virtual environment. Such
experience cannot be natively and/or eﬃciently replicated in the context
of multi-user scenarios, even by modern game engines. The basic challenge
that VR application designers have to overcome regards the exchange of
information between the user movements and their actions in real-time. If this

Eﬃcient Networked VR Transformation Handling Using GA

3

information is not eﬃciently transmitted, the virtual avatars of the users will
not be or appear to be in sync. In such cases, this unnatural-feeling caused by
the delay between the users actual movements/actions and their visualization,
drastically reduces the immersion and, in certain cases, may even make the
application incapable to serve its cause, e.g., in VR training applications.

Another interesting problem that experts try to handle deals with session
recording. Nowadays, session recording and playback of a single or multi-
user VR session has become of increasing importance for the functionality
and eﬀectiveness of certain applications. This is especially signiﬁcant for
applications related to training as replaying users actions can serve as an
additional and powerful educational tool. As users are allowed to watch and
study all actions of a virtual scene, they can identify and learn from their
mistakes.

Recording (and replaying) a VR session is not a task that is natively
undertaken by modern game engines and therefore most VR applications
do not include such a feature by default. Researchers try to determine the
optimal way to record and store all events that happen in a VR session
where a single or multiple users exist and interact. Their goal is to explore
the proper methods and structures of data that must be employed so that
they can achieve a real-time logging where the required data storage remains
manageable. Ultimately, such recordings are used to enable replaying of a VR
session, a feature that many VR applications still lack today.

Current bibliography contains numerous examples of how the VR record
and replay features can enhance the functionality of a VR application, espe-
cially the ones related to training, by mainly measuring the performance of
users. In [1,14], the authors present randomized controlled trials on the eﬃcacy
of VR simulation for medical skills training. In both these works, the VR
cohort demonstrates greater improvement in each speciﬁc score category and
signiﬁcantly higher satisfaction, compared to the control group. Grantcharov
et al. [9] demonstrate in their randomized clinical trial that surgeons who
received VR training performed laparoscopic cholecystecomy faster than the
control group, showing great improvement in error and economy of movement
scores. Ihemedu-Steinke et al. [23] measured the concentration, involvement
and enjoyment of users on a virtual automated driving simulator and pro-
vided a comparison of those between a VR and a conventional display. The
experiment showed a statistically signiﬁcant result, that in VR all measured
variables had higher values. Southgate [35] proposed the use of screen capture
video to understand the learning outcomes in virtual reality experiences. Pay-
ing attention to attitudinal change, higher order thinking and metacognition,
it was shown that this was a viable method for gaining deeper insights into
immersive learning. Lahanas et al. [22] presented a novel augmented reality
simulator for skills assessment in minimal invasive surgery. Their simulator
allows training of surgeons and assessment of their skills, while the completion
time, the path length and two speciﬁc types of errors are evaluated.

4 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

In [21], analysis of the motion of virtual reality users is presented. The
authors use the motion captured in virtual environments and perform the
analysis in the same environment. A visual analysis system is developed that
allows immersive visualization of human motion data. This method enables
the examination of behaviours and can ﬁnd useful patterns and outliers in
sessions.

In [25], kinematic analysis of experienced and novice surgeons during
robotic teleoperation was performed. The same transformation data with
our methods were used in order to assess the skills of the users. However,
robotic kits are not capable of fast prototyping of realistic environments and
also event data need to be captured from a video camera. Also, replay is
only available through cameras and has the drawbacks that are explained in
Section 4.1.

Sharma et al. [34] presented a framework for video-based objective
structured assessment of technical skills. In this framework, motion feature
extraction is performed to detect the spatio-temporal interest points (STIPs).
After that, the motion classes are learned using k-means clustering and the
STIPs are classiﬁed into those classes. Finally, motion class counts, data-
driven time windows and sequential motion texture features are computed.
An automated video-based assessment tool for surgical skills was proposed
in [39]. In this work, a four-step process is developed in order to evaluate
the training outcomes in medical schools. These steps are motion class time
series generation, feature modelling, feature selection, and classiﬁcation. In
[38], accelerometer data are used in conjuction with video, providing features
from multi-modal data. This fusion of video and acceleration features can
improve performance for skill assessment. The video data are processed using
techniques similar to the two previous papers, while accelerometer data
processing is performed by aligning those data with the timeseries computed
by the video data. Afterwards, these two kinds of data are combined in order
to extract and select useful features and skills assessments are classiﬁed using
a nearest neighbour classiﬁer.

It becomes apparent from these works and the current research initia-
tives that the use of VR simulations with the capability of VR recording
and replaying, will enhance the learning outcomes of the users, since this
functionality enables the replay of sessions and, also, provides data to analyze,
with automated tools, the performance of the users. The latter empowers
serious game developers to create assessment tools for large-scale real-time
evaluations without the need of human labor.
Our Contribution. In this work, we propose novel methods to address the
two aforementioned problems; A)eﬀective handling of data exchange in multi-
user VR sessions and B) eﬃcient recording of user actions in VR sessions,
especially for training applications. For both problems, we deploy mathe-
matical tools from geometric algebra (GA) and various (sub)algebras. Our
methods rely on the fact that all basic geometric primitives and their trans-
formations used in VR, such as points, planes, lines, translations, rotations

Eﬃcient Networked VR Transformation Handling Using GA

5

and dilations (uniform scalings), can be uniformly represented as multivectors,
i.e., elements of a suitable geometric algebra such as 3D Projective (3D PGA)
or 3D Conformal Geometric Algebra (3D CGA). Furthermore, we provide
an alternative method that utilizes dual quaternions as the representation
form of the position and rotation of the users, both for transmission and
storing purposes. Our two methods are compared in detail with the current
state of the art (SoA), that deploys vectors and quaternions to manage posi-
tional and rotational data respectively. The comparison for both problems
addressed is accomplished in networks of variable bandwidth capacity, rating
from unrestricted networks to ones that are heavily limited. In all cases, we
demonstrate that our methods perform equally or outperform SoA, with
increased beneﬁt from our methods as the network quality deteriorates (see
Section 5). Regarding the data exchange problem, handled in Section 3, we
provide convincing results in a modern game engine and a VR collaborative
training scenario (see the video presentation of this work [17] and Figure 2).
Our methods concerning VR recording are described in Section 4, where
we present evidence that using multivectors and (dual) quaternions as an
alternative to the SoA representation forms yields similar results, especially
when data interpolation is required.
Why use Geometric Algebra? Algebras such as 3D PGA and 3D CGA1
are showing rapid adaptation to VR implementations due to their ability to
represent the commonly used vectors, quaternions and dual-quaternions na-
tively as multivectors. In fact, quaternions and dual-quaternions are contained
as a sub-algebra in both these algebras [11]. Therefore, they incorporate all
beneﬁts of quaternions and dual-quaternions representations such as artifact
minimization in interpolated frames[19]. Furthermore, geometric algebras
enable powerful geometric predicates and modules [18], providing, if used
with caution, performance which is on par with the current state-of-the-art
frameworks[27]. In the past decades, these algebras have proven to be able to
solve a variety of problems in various ﬁelds, involving inverse kinematics [13]
and physics [5]. In conclusion, this work constitutes yet another step towards
an eﬀective all-in-one geometric algebra framework for handling VR data
streams, with performance that is on par or exceeds current SoA frameworks.

2. Basic Mathematical Notation

In the following sections, we provide basic notation and formulas for (dual)
quaternions, dual numbers and multivectors representing motors, for the
sake of completeness. Similar or more complicated formulas can be obtained
via [19, 11, 7] or any other modern book involving dual quaternions and

1Of course, instead of 3D PGA-CGA one could also employ a, geometric or not, algebra that
would allow the representation of motors in a form suitable for interpolation. Determining
if such an algebra, non-isomorphic to a sub-algebra of 3D PGA or 3D CGA, exists was not
done in the context of this work.

6 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Figure 2. Images taken from a modern VR training appli-
cation that incorporates our proposed interpolation methods
for all rigid object transformations as well as hand and avatar
movements. It is recommended to see the video presentation
of this work [17], to better understand the signiﬁcance of
these ﬁgures.

multivectors. We strongly encourage the reader to visit https://bivector.
net for more information on these beautiful algebras.

2.1. Quaternions
The typical form of a quaternion q is

q := qw + qxiii + qyjjj + qzkkk,
(1)
where qw, qx, qy, qz ∈ R and the imaginary units iii, jjj, kkk satisfy the well known
identities iii2 = jjj2 = kkk2 = iiijjjkkk = −1. The quaternion q can also be viewed as a
4-tuple (qw, qx, qy, qz). If qw equals 0, the q is called a pure quaternion.

Addition of the quaternion q to another quaternion

p := pw + pxiii + pyjjj + pzkkk,

(2)

is done componentwise, i.e.,

q + p := (qw + pw) + (qx + px)iii + (qy + py)jjj + (qz + pz)kkk.

(3)

Multiplication of the two quaternions is carried out as follows:

qp :=(qwpw − qxpx − qypy − qzpz) + (qwpx + pwqx + qypz − qzpy)iii

(4)

+ (qwpy + pwqy + qzpx − qxpz)jjj + (qwpz + pwqz + qxpy − qxpy)kkk.

by applying the associative property, applying the basic identities and gather-
ing terms. Note that multiplication is generally not commutative.

The conjugate of q is the quaternion

q∗ := qw − qxiii − qyjjj − qzkkk,

and it is easy to verify that

(pq)∗ = q∗p∗.

(5)

(6)

Eﬃcient Networked VR Transformation Handling Using GA

7

The norm of q is deﬁned as

|q| :=

√

qq∗ =

(cid:113)

and its inverse, assuming |q| (cid:54)= 0, is

w + q2
q2

x + q2

y + q2
z ,

(7)

q∗
|q|2 .
If |q| = 1, q is called a unit quaternion, and it can be written in the form

q−1 :=

(8)

q = cos

θ
2

+ sin

θ
2

(uxiii + uyjjj + uzkkk),

(9)

where uuu := (ux, uy, uz) is a unit vector and θ ∈ [0, π]. Such quaternions
encapsulate the rotation of a 3D point (px, py, pz) by angle θ around an axis
going through the origin in the direction of uuu. Indeed, if we apply the sandwich
quaternionic product to the pure quaternion p = (0, px, py, pz) we will obtain
the pure quaternion

where the (p(cid:48)
aforementioned geometric transformation.

x, p(cid:48)

y, p(cid:48)

p(cid:48) := qpq∗ = (0, p(cid:48)

y, p(cid:48)
(10)
z) is the image of the point (px, py, pz) if we applied the

x, p(cid:48)

z),

2.2. Dual Numbers
A dual number d is deﬁned to be

d := a + (cid:15)b,
(11)
where a, b are elements of some ﬁeld (usually R) and (cid:15) is a dual unit, i.e., it
holds that (cid:15)2 = 0. The elements a and b are referred to as the real and dual
part of d.

Let di := ai + (cid:15)bi, for i ∈ {1, 2} be dual numbers. The addition of these

dual numbers is performed pairwise, i.e.,

d1 + d2 := (a1 + a2) + (cid:15)(b1 + b2),

(12)

and their multiplication is done by applying the associative rule and gathering
terms

(13)
d1d2 := (a1 + (cid:15)b1)(a2 + (cid:15)b2) = a1a2 + (cid:15)(a1b2 + a2b1),
taking into consideration that (cid:15)2 = 0. Note that the multiplication on the right
hand side of the equations above depend on the ﬁeld where the coeﬃcients
a1, b1, a2 and b2 belong.

The multiplication identity is 1 + (cid:15)0 and the multiplicative inverse of

d = a + (cid:15)b, when a (cid:54)= 0, is

d−1 = a−1(1 − (cid:15)ba−1).

(14)

If a = 0, then d = (cid:15)b has no inverse, which dictates that dual numbers form a
ring and not a ﬁeld.

The conjugate of a dual number d = a + (cid:15)b is the dual number

¯d = a − (cid:15)b.

(15)

8 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

A really interesting and useful property of dual numbers is that, the
function of a dual number d = a + (cid:15)b can be obtained by considering the
Taylor expansion at a, i.e.,

f (a + (cid:15)b) = f (a) + (cid:15)bf (cid:48)(a) +

1
2

(cid:15)2b2f (cid:48)(cid:48)(a) + · · · = f (a) + (cid:15)bf (cid:48)(a).

(16)

2.3. Dual Quaternions
A dual quaternion is a dual number D = p + (cid:15)q, where p, q are quaternions.
Regarding dual quaternions2, the addition, multiplication and inverse are
determined using the forms used in the dual numbers case, taking into
consideration that (cid:15) commutes with the imaginary units iii, jjj and kkk. If p =
(pw, px, py, pz) and q = (qw, qx, qy, qz), then D can also be seen as the vector
(pw, px, py, pz, qw, qx, qy, qz).

There are two main conjugates of a dual quaternion D = p + (cid:15)q which

are obtained via

• the conjugation of the quaternions parts

D∗ = p∗ + (cid:15)q∗,

• the conjugation of the dual number form

¯D = p − (cid:15)q.

We can easily verify that D∗ = (cid:0) ¯D(cid:1)∗

.
If a, b are quaternions, we denote

(cid:104)a, b(cid:105) :=

1
2

(ab∗ + a∗b).

The norm of D = p + (cid:15)q is then equal to

√

|D| :=

DD∗ = (cid:112)|p|2 + 2(cid:15)(cid:104)p, q(cid:105) = |p| + (cid:15)

(cid:104)p, q(cid:105)
|p|2 .
√

(17)

(18)

(19)

(20)

The last equality is obtained by applying 16, for f (x) =
dual quaternions, it holds that |D1D2| = |D1||D2|.

x. If D1, D2 are

A dual quaternion D = p + (cid:15)q that satisﬁes |D| = 1, or equivalently
|p| = 1 and (cid:104)p, q(cid:105) = 0, is called unit dual quaternion. Due to these two
linear restrictions, the set of unit dual quaternions, seen as vectors, form a
6-dimensional manifold embedded in the 8-dimensional Euclidean space. Also
note that if Di, for i ∈ {1, 2}, are unit dual quaternions then D−1
i and
the product D1D2 is also a unit dual quaternion.

i = D∗

A remarkable theorem ([19, Lemma 12 in p12]) is the following.

2As bi-quaternions gain more momentum, we note here for clarity that they are distinct
from dual-quaternions. Indeed, dual quaternions are dual numbers where each coeﬃcient is
a quaternion whereas, the bi-quaternions are quaternions where each coeﬃcient is a complex
number. A basic diﬀerence is that, that apart from the common basic elements {1, i, j, k},
the additional element I of bi-quaternions satisﬁes I 2 = −1, whereas the additional element
(cid:15) of the dual-quaternions satisfy (cid:15)2 = 0.

Eﬃcient Networked VR Transformation Handling Using GA

9

Lemma 1. Every rigid transformation can be represented by a unit dual
quaternion, and conversely, every unit dual quaternion represents a rigid
transformation.

The proof of the lemma is based on the fact that a unit dual quaternion
D can be written in the form D = p + (cid:15)q, where p is a unit quaternion
and q = 1
2 (t1iii + t2jjj + t3kkk), for some vector t := (t1, t2, t3). In this case, D
encapsulates the rotation “stored” within p followed by a translation by t.
Indeed, if take the dual quaternion v = (1, 0, 0, 0, 0, v1, v2, v3) that corresponds
to a point v and apply the sandwich multiplication by D, we will obtain

v(cid:48) = DvD∗ = (1, 0, 0, 0, 0, v(cid:48)

1, v(cid:48)

2, v(cid:48)
3)

(21)

where (v(cid:48)
rotation encapsulated in p and then translating by t.

3) is the image of the point (v1, v2, v3) after applying the

2, v(cid:48)

1, v(cid:48)

2.4. Motors in Geometric Algebra
In this section, we provide a brief overview of the 3D PGA or CGA multivectors
that arise in our methods. The multivectors studied represent motors, i.e., they
are of the form M = T R, where T and R are the multivectors encapsulating
a translation and a rotation, respectively. Below, we demonstrate how we
can obtain T and R from a given M , and how we can transmute them into
the corresponding vector and unit quaternion form that correspond to the
same geometric transformations. The later forms are needed by modern game
engines to apply the transformation stored in M to any game object.

• 3D PGA: In this algebra, the multivector

T := 1 − 0.5e0(t1e1 + t2e2 + t3e3),

represents the translation by (t1, t2, t3) and the multivector

R := a + be12 + ce13 + de23,

encapsulates the same rotation with the unit quaternion

q := a − diii + cjjj − bkkk.

In this algebra, it also holds that

e0e0 = 0,

(22)

(23)

(24)

(25)

and therefore, if we evaluate the quantity e0M , we will obtain the
multivector

e0M ≡ e0T R ≡ (e0T )R ≡ e0R.

This multivector will be of the form

e0R = ae0 + be012 + ce013 + de023,

hence we can deduce that

R = a + be12 + ce13 + de23,

(26)

(27)

(28)

which corresponds to the quaternion shown in 24. The inverse of R is
the multivector

R−1 = a − be12 − ce13 − de23.

(29)

10 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Since M and R are known, we may evaluate T , as it equals
T = M R−1 = M (a − be12 − ce13 − de23) = 1 + xe01 + ye02 + ze03,

(30)

and conclude that it corresponds to a translation by (−2x, −2y, −2z).

• 3D CGA: In this algebra, the multivector

T = 1 − 0.5(t1e1 + t2e2 + t3e3)(e4 + e5)

(31)

is the multivector that corresponds to a translation by (t1, t2, t3). Identi-
cal to 3D PGA, the multivector

R = a + be12 + ce13 + de23,

encapsulates the same rotation with the unit quaternion

q := a − diii + cjjj − bkkk.

Since M = T R, it follows from the form of T that

M = T R = R + m,

(32)

(33)

(34)

where m is a multivector that necessarily contains basis elements con-
taining e4 and e5 (or their geometric product) that cannot be cancelled
out. Therefore, we can obtain R by keeping the terms of M that contain
only the basis vectors {1, e1, e2, e3, e12, e23, e13}. Having determined R
and given M is known, we evaluate R−1 using 29 and determine T using

T = M R−1.

(35)

Assuming T is normalized (otherwise we normalize it), we may extract the
corresponding translation vector (t1, t2, t3) from the quantity T (e5 − e4),
as it holds that

T (e5 − e4) = t1e1 + t2e2 + t3e3.

(36)

The conversion of R to quaternion is identical with the case of 3D PGA
above.

3. Data Transmission in Multi-User Collaborative VR

Sessions

In this section, we consider the transmission of a user’s data to the rest of the
users, in the context of a multi-user VR collaborative session. The information
that is necessarily relayed over the network involves the users interactions
through the hand-based HMD controllers such as displacement data (e.g.,
translation and rotation of the controller) within speciﬁc time intervals and
button-press events. Since the VR rendering engine is typically diﬀerent for
each user, it is crucial to eﬃciently relay the data of one user to the rest of the
users’ VR engines, in a synchronized manner. If done correctly [36], all users
will be synchronized and actions between users, such as exchanging objects or
even playing tennis, can be accurately performed. An ineﬃcient transmission
of this information will cause each user’s virtual world to depict the rest of

Eﬃcient Networked VR Transformation Handling Using GA

11

the users with increased latency and therefore, collaboration will no longer be
possible or feel natural.

To better understand the source of this problem, let us initially consider
the pipeline of the data from a user to the rest. When the user moves
the hand-based controllers of his HMD, the hardware initially detects the
movement type and logs it, in various time intervals based on the user’s or
developer’s preferences. This logged movement, that is either a translation
and/or a rotation, is constantly transcoded into a suitable format and relayed
to the VR application and rendered as a corresponding action, e.g., hand
movement, object transformation or some action. The controller’s data format
to be transmitted to the rendering engine aﬀects the overall performance and
quality of experience (QoE) and poses challenges that must be addressed.
These challenges involve keeping the latency between the movement of the
controller and its respective visualization in the HMD below a certain threshold
that will not break the user’s immersion. Furthermore, the information must
be relayed eﬃciently such that a continuous movement of the controller results
in a smooth jitter-less outcome in the VR environment. Such challenges heavily
depend on the implementation details regarding the communication channel
that handles the way that position and rotation of the controller is relayed,
as well as the choice of a suitable interpolation technique. The displacement
data are transmitted at discrete time intervals, approximately 20-40 times
per second. To maintain a high frame-per-second scenery in the VR, multiple
in-between frames must be created on-the-ﬂy by the appropriate tweening
algorithm. An eﬃcient algorithm will allow the generation of natural ﬂow
frames while requiring fewer intermediate keyframes. Such algorithms will help
reduce a) bandwidth usage between the HMD and the rendering engine and
b) CPU-strain, resulting in lower energy consumption as well as lower latency
issues in bandwidth-restricted networks. Moreover, HMDs with controllers
of limited frequency will still be able to deliver the same results as more
expensive HMDs.

3.1. State of the Art
The current SoA methods regarding the format used to transmit the dis-
placement data mainly involves the use of 3D vectors for translation and
quaternions for rotation data. Modern game engines such as Unity3D and
Unreal Engine, have these representation forms already built within their
frameworks. The dominance of these forms is based on the fact that they re-
quire very few ﬂoating point numbers to be represented (3 and 4 respectively)
and their ability to support fast and eﬃcient interpolations. Speciﬁcally, 3D
vectors are usually linearly interpolated, where as the SLERP method is usu-
ally used for quaternion blending. In some engines, such as Unity3D, rotations
are sometimes provided in terms of Euler angles, but for interpolation needs,
they are internally transformed to their quaternion equivalents.

Based on the current SoA forms, the controllers of a VR HMD log their
current position v = (vx, vy, vz) at each time step with respect to a point they
consider as the origin. Their rotation is also stored, as a unit quaternion q =

12 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

(qw, qx, qy, qz) = qw +qxiii+qyjjj +qzkkk. The use of unit quaternions revolutionized
graphics as it provided a convenient, minimal way to represent rotations, while
avoiding known problems (e.g., gimbal lock) of other representation forms
such as Euler angles [19]. The ways to change between unit quaternions and
other forms representing the same rotation, such as rotation matrices and
Euler angles, are summarized in [4].

The interpolation of the 3D vectors containing the positional data is done
linearly, i.e., given v and w vectors we may generate the intermediate vectors
(1 − a)v + aw, for as many a ∈ [0, 1] as needed. Given the unit quaternions q
and r the intermediate quaternions are evaluated using the SLERP blending,
i.e., we evaluate q(q−1r)a, for as many a ∈ [0, 1] as needed, like before. If
these intermediate quaternions are applied to a point p, the image of p, as a
goes from 0 to 1, has a uniform angular velocity around a ﬁxed rotation axis,
which results in a smooth rotation of objects and animated models.

3.2. Room for Improvements
Graphics courses worldwide mention quaternions as the next evolution step
of Euler angles; a step that simpliﬁed things and amended interpolation
problems without adding too much overhead in the process. Despite it being
widespread, the combined use of vectors and quaternions does not come
without limitations.

A drawback that often arises lies in the fact that the simultaneous linear
interpolation of the vectors with the SLERP interpolation of the quaternions
applied to rigid objects does not always yield smooth, natural looking results
in VR. This is empirically observed on various objects, depending on the
movement the user expects to see when moving the controllers. Such artifacts
usually require the developer’s intervention to be amended, usually by de-
manding more intermediate displacements from the controller to be sent, i.e.,
by introducing more non-interpolated keyframes. This results mainly in the
increase of bandwidth required as more information must be sent back and
forth between the rendering engine and the input device, causing a hindrance
in the networking layer. Multiplayer VR applications, that heavily rely on the
input of multiple users on the same rendering engine for multiple objects, are
inﬂuenced even more, when such a need arises. Furthermore, the problem is
intensiﬁed if the rendering application resides on a cloud or edge node; such
scenarios are becoming increasingly more common as they are accelerated
by the advancements of 5G networks and the relative functionalities they
provide.

3.3. Proposed Method Based on Dual Quaternions
In the past few years, graphics specialists have shown that dual quaternions
can be a viable alternative and improvement over quaternions, as they allow us
to unify the translation and rotation data into a single entity. Dual quaternions
are created by quaternions if dual numbers are used instead of real numbers
as coeﬃcients, i.e., they are of the form d := A + (cid:15)B, where A and B are
ordinary quaternions and (cid:15) is the dual unit, an element that commutes with

Eﬃcient Networked VR Transformation Handling Using GA

13

every element and satisﬁes (cid:15)2 = 0 [20]. A subset of these entities, called unit
dual quaternions, are indeed isomorphic to the transformation of a rigid body.
A clear advantage of using dual quaternions is the fact that we only need one
framework to maintain and that applying the encapsulated information to
a single point requires a simple sandwich operator. Moreover, the rotation
stored in the unit dual quaternion A + (cid:15)B can be easily extracted as the
quaternion r := A is the unit quaternion that amounts to the same rotation.
Furthermore, if B(cid:63) denotes the conjugate quaternion of B, then t := 2AB(cid:63) is
a pure quaternion whose coeﬃcients form the translation vector [20].

Taking advantage of the above, we propose the replacement of the
current state-of-the-art sequence (see Figure 3, Top) with the following (see
Figure 3, Middle). The displacement data of an object is again represented
as a vector and a quaternion; in this way, only a total of 7 ﬂoat values (3
and 4 respectively) need to be transmitted. The VR engine combines them
in a dual quaternion [20] and interpolates with the previous state of the
object, also stored as a dual quaternion. Depending on the engine’s and the
user’s preferences, a number of in-between frames are generated via SLERP
interpolation [19] of the original and ﬁnal data. For each dual-quaternion
received or generated, we decompose it to a vector and a quaternion and
apply them to the object. This step is necessary to take advantage of the
built-in optimized mechanisms and GPU implementations of the VR engine.

A major advantage of the proposed method is that we can obtain similar
results with the state-of-the-art method by sending less keyframes per second.
As an empirical law, we may send 20 displacement data per second with
our method to obtain the same quality of generated frames as if we had
sent 30 data per second with the current state-of-the-art method. This 33%
reduction of required data applies for each user of the VR application, greatly
lowering the bandwidth required as more users join. As an example, if n users
participate, the total displacement data required for our method would be
560n bytes per second (20 messages per second X 7 ﬂoats per message X 4
bytes per double, assuming a classic implementation) as opposed to 840n bytes
per second (30 messages per second X 7 ﬂoats per message X 4 bytes per ﬂoat)
with the default method. The numbers of updates per second mentioned above
relate to the case of unrestricted-bandwidth network; for the respective results
regarding constrained networks see Section 5 and Table 1. This method is
incorporated in the MAGES SDK [31] for cooperative VR medical operations,
publicly available to be tested for free, as the default transmission method,
which is indicative of the performance boost it provides.

The drawbacks of this method is the need to constantly transform dual-
quaternions to vector and rotation data after every interpolation step but
this performance overhead is tolerable as the extraction of the displacement
data is accomplished in a straight-forward way. Also, performing SLERP on a
dual quaternion (proposed method) instead of a quaternion (state-of-the-art
method) demands more operations per step. The trade-oﬀs between the two

14 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Figure 3. Algorithm layout of the diﬀerent interpolation
engines used to generate intermediate frames.

methods seem to favor our method, especially in the case of collaborative VR
applications.

3.4. Proposed Method Based on Multivectors
The proposed method described in Section 3.3 was based on the use of dual
quaternions and the fact that interpolating them (using SLERP) produced
smooth intermediate frames. In this section, we go one step further and suggest
the use of multivectors instead of dual-quaternions (see Figure 3,Bottom).
This transition can be done in a straight-forward way if we use multivectors
of 3D Conformal (see [11]) or 3D Projective Algebra (see [7] and its updated
Chapter 11 in [6]). The interpolation of the resulting multivectors can be
accomplished via LERP [10]; if M1 and M2 correspond to two consecutive
displacement data, then we can generate the in-between multivectors

(1 − a)M1 + aM2,
for as many a ∈ [0, 1] as needed (and normalize them if needed). Notice that
since we are only applying these displacements to rigid bodies, we may use

(37)

Eﬃcient Networked VR Transformation Handling Using GA

15

Figure 4. A triangular object is interpolated via multivec-
tors. A motor including both a translation and a rotation
is applied to the triangle via its mass center. Between the
extreme positions of the object, we generate 20 intermediate
frames using LERP (yellow) and SLERP (green) interpola-
tion of the multivector. Only minimal diﬀerences are spotted
between the two outcomes.

LERP instead of SLERP (see Figure 4). For every (normalized) multivector
M received or interpolated, we may now extract the translation vector and
rotation quaternion, as shown in Section 2.4. Every multivector received or
generated has to be decomposed to a vector and a quaternion in order to be
applied to the object, as modern VR Engines natively support only the latter
two formats.

The advantage of such a method lies on the fact that we can use LERP
blending of multivectors instead of SLERP. This saves as a lot of time and
CPU-strain; SLERP interpolation requires the evaluation of a multivector’s
logarithm, which requires a lot of complex operations [8]. Notice that, LERP
is eﬃcient in our case since only rigid objects displacements are transfered
via the network; if we wanted to animate skinned models via multivectors
it is known that only SLERP can produce jitter-less intermediate frames
[19]. Another gain of this proposed method is the ability to incorporate it
in an all-in-one GA framework, that will use only multivectors to represent
model, deformation and animation data. Such a framework is able to deliver
eﬃcient results and embeds powerful modules [28, 18, 27]. In such frameworks,
decomposition of multivectors to vectors and quaternions will be redundant,
as we can apply the displacement to the object’s multivector form via a simple
sandwich operation.

The trade-oﬀs of such an implementation are based on the fact that
modern VR engines do not natively support multivectors and therefore pro-
duction ready modules, with basic functions implemented, are almost non-
existent. An exception is the Klein C++ module for 3D PGA, found in
www.jeremyong.com/klein; for 3D CGA no such module is available the
moment this paper is written. This makes it diﬃcult for GA non-experts to
adopt and implement such methods. Furthermore, multivectors require 16 (3D
PGA) or 32 (3D CGA) ﬂoat values to be represented and using unoptimized,
usually CPU and not GPU-based, modules to handle them may result in slow

16 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

rendering. Optimized modules, such as GAALOP [12], can take advantage of
the fact that very few of the multivector coordinates are indeed non-zero, as
the multivectors involved are always motors, i.e., represent translations and/or
rotations, and therefore have speciﬁc form. Since the full multivector algebra
is not needed for this application, various other approaches exist to achieve
performance optimizations (e.g., exploiting the even and odd sub-algebras of
motors), even when starting from a formulation of the algorithm as a naive
full multivector formula.

4. Recording and Replaying in VR

Figure 5. VR Record and Replay functionality. The point
of view of the user replaying a recorded session. The user is
able to watch the session from any perspective, as well as
pause the replay in order to continue the session on his own.

4.1. Towards An Eﬃcient VR Recorder
Based on current bibliography, accurate recording of a VR session can be
achieved via two methods, mainly diﬀering in what they aim to log. The ﬁrst
method aims to record all users inputs whereas, the second method focuses
on the eﬀects that these inputs have on the virtual scene.

As VR advances, the realism of the environments expands as well; trivial
interactions such as ripping a plastic case, grabbing a tool or removing the
cap from a bottle, have turned to mandatory tasks that greatly add up to the
immersive experience. This exponential content expansion is hindering the

Eﬃcient Networked VR Transformation Handling Using GA

17

development of the latter logging method. Therefore, our research revolves
around a VR logger that records raw users inputs, as we believe that such
an approach suits better to the current growth direction of virtual reality
environments.

Opposed to popular recording techniques, we do not take into consid-
eration video recording from cameras that are located in diﬀerent places
within the VR, whether these are static or, for various reasons, in motion.
Our decision is based on the fact that such recordings can not allow replaying
the session from any other perspective.

Another major disadvantage of such recordings lies on the inability
of the user to pause the recording and resume “playing” the session at a
certain point. This is caused by the lack of storing values and data that are
deeply connected with the current state of the game at that time, but rather
superﬁcial snapshots of the VR scene. Without the ability of resume playing
at a custom state, players cannot eﬀectively improve their performance in a
speciﬁc task, unless replaying the whole session.

Raw video recording of the scene also introduces another major drawback.
The position of objects and the trigger of various events from the VR is no
longer easy to identify and keep track of, without extra eﬀort. Even the
use of signal processing algorithms and classiﬁcation techniques, such as the
ones described in [34, 39, 38], will inevitably introduce errors and hinder the
monitoring process of such data.

On the other hand, recording the original input data of the users will
allow keeping track of the actual raw transformations of objects and events in
the scene. This will sequentially enable methods for assessing the performance
of users and understanding what the users are actually doing. Eventually, we
should be able to create intelligent agents via imitation learning from experts
that are able to complete a session “successfully”, and support the users to
eﬀectively perform similar tasks accordingly.

Moreover, by avoiding collecting data of high dimensionality, such as
the entire state of the virtual world and instead storing low-dimensional
information such as the users inputs and triggers, we can obtain valuable
analytics even by using simple processing algorithms. One should also notice
that the data we choose to record can easily generate the scene by applying
the worlds mechanics, while the reverse process would demand sophisticated
computer vision algorithms. Lastly, using our recording method, users have
the ability to act simultaneously with various recorded interactions and events,
a functionality that can be used creatively to increase the pedagogical beneﬁts
of various simulations.

4.2. Implementation Details
In our proposed recording method, the displacement of all users in the op-
eration is captured, along with their interactions with virtual objects. The
voice of each player is recorded individually, including incoming voice in cases
of multi-player sessions. Additionally, the actions performed as well as the
complete traversal path of the scenegraph [40] are captured. In VR, the objects

18 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

which the user interacts with might come into contact or interact with other
objects, changing the latter’s location or status. Therefore, the transforms of
all subsequently aﬀected objects are also recorded.

Four scripts are responsible for recording all the necessary data mentioned
above; the InteractionRecorder, the PropagateRecording, the GetAudioSamples
and the RecordingWriter scripts. The InteractionRecorder tracks each user’s
head and hands transforms, while the PropagateRecording records the status
changes of every object that users interact with. All sound related recordings
are received by the GetAudioSamples script. Finally, the RecordingWriter is
responsible for writing all recorded interactions, sounds and events from
previous three scripts into respective ﬁles. Note that an instance of each script
is created for every user of a multi-player session.

We denote the ﬁles used to store the transforms of the head, hands and
moved objects by Transform X , whereas the ﬁles that store all the events
(messages) occurring in the session are denoted by Messages X ; in both cases,
X is either Camera, Left Hand, or Right Hand. Finally, the RecordingInfo ﬁle
stores information about the recording, such as the duration of the recording,
and is available only for the owner of the session, i.e., the (ﬁrst) user that
initiated the session.

Figure 6. The associated scripts along with the generated
ﬁles, in the context of recording a VR session of N players.

Below we present a list of all the data that our proposed VR Recorder
stores. The data types used are either 3D Vectors (for transformation data),
Events (for various Scene events and functionalities) or ﬂoat (for the time).

Eﬃcient Networked VR Transformation Handling Using GA

19

• Left/Right Hand Translation (3D Vectors): Consists of 6 ﬂoats
(3 for each hand) describing the position of each hand in the virtual
environment

• Left/Right Hand Rotation (3D Vectors): Consists of 6 ﬂoats (3
for each hand) describing the rotation of each hand in the virtual envi-
ronment

• Camera Translation (3D Vector): Consists of 3 ﬂoats describing the

position of the user’s HMD in the VR environment

• Camera Rotation (3D Vector): Consists of 3 ﬂoats describing the

rotation of the user’s head in the VR environment

• Left/Right Hand Start Interaction (Event): The user has started
interacting with an object. Consists of the name of the interacted object
(string).

• Left/Right Hand End Interaction (Event): The user has ended
interacting with an object. Consists of the name of the interacted object
(string), and the amount of time the user was interacting with the object
(ﬂoat).

• 3D Objects Translation and Rotation (3D Vectors): Consists of
6 ﬂoats describing the position and rotation of an object, with which a
hand has interacted.

• Press Button (Event): Some 3D objects provide an extra functionality
while holding them, such as pressing the button for activating a drill.
• Scenegraph Traverse (Event): Information about traversing the

scenegraph (e.g. go to next/previous action/stage/lesson).

• Time (ﬂoat): The time that has passed since the start of the session,

in seconds.
Figure 6 depicts the VR Recorder pipeline, showing the diﬀerent scripts
that are involved as well as the ﬁles that are created. During replay, all “replay”
avatars and interacted objects are recreated and referenced based on these
recorded ﬁles. The replay functionality is handled by the Replay script.

4.3. The Basic Algorithms
In this section, the algorithms behind the four basic scripts involved in VRRR
are presented.

RecordingWriter (see Algorithm 1) is responsible for writing all informa-
tion regarding the position and the rotation of the logged objects, as well as
messages of events relevant to them. The RecordingWriter is implemented using
the singleton pattern, i.e., only one script exists on each scene. Algorithm 1
provides the implementation of the Recording Writer script. In this algorithm,
the InteractionRecorder script is added in the head and hands of all players
when recording is started. The tracking objects, i.e., heads and hands, have
two corresponding ﬁles where their transformations and messages relevant
to them are stored. The WriteToFile public function is called by the Interac-
tionRecorder and PropagateRecording scripts in order to write the information
mentioned above. Finally, the EndRecording function closes all ﬁles when the
session is ﬁnished.

20 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Algorithm 1 Recording Writer

1: Start():
2: for each player do
3:

4:
5:
6:
7: end for

end for

Add Interaction Recorder Script in head and hands (tracking objects)
for each tracking object do

Create ﬁles for storing transforms and messages

8: WriteToFile(tracking object, content, player):
9: Check whether content contains a message or a transform
10: if content contains message then
11: Write content in messages ﬁle for the tracking object of player
12: else if content contains transform then
13: Write content in transforms ﬁle for the tracking object of player
14: end if

15: EndRecording():
16: Close Transform and Message Files for each player

InteractionRecorder (see Algorithm 2), is responsible for capturing the
transformations on each frame of the tracking object that this script is attached
to, as well as all the events that are relevant. The RecordingWriter passes
the ID of the player this game object is attached to, in order to send the
corresponding information to the associated ﬁle. Afterwards, the type of the
tracking object is found and, according to that, speciﬁc listeners are initialized.
These listeners are responsible for sending messages to the RecordingWriter
when speciﬁc events have been performed. Such kinds of events are interactions
of hands with other objects or tools, or changes in the state of the scene, which
are stored in the head’s messages ﬁle. The Update function is responsible
for sending the transformation of the tracking object to the RecordingWriter,
and, also, check if an event has occured in order to write it in the relevant
ﬁle. Finally, the OnBeginInteraction and OnEndInteraction functions add or
remove the PropagateRecording script to the game object the tracking object
has started or ﬁnished interacting with, and, also, send a relevant message to
the RecordingWriter.

PropagateRecording (see Algorithm 3) sends the transformations of all
the game objects that the hands have interacted with to the RecordingWriter.
At ﬁrst, information about the player and the game object is stored. Then, a
listener is added to that object, if it is a tool. This listener is responsible for
adding the PropagateRecording script to the objects the tool has interacted
with. Finally the Update function destroys the script, if the object has stopped
moving, and, also, writes the transformation data using the player’s ID and
the object’s name as parameters in order for the RecordingWriter to ﬁnd the
associated ﬁle.

Eﬃcient Networked VR Transformation Handling Using GA

21

Algorithm 2 Interaction Recorder

1: Player ← the ID of the player this gameobject refers to

Tracking Object ← Head

2: Start():
3: if this gameobject is Hand then
Tracking Object ← Hand
4:
5: else if this gameobject is Head then
6:
7: end if
8: if Tracking Object is Hand then
9:
10:
11: else if Tracking Object is Head then
12:
13: end if

Add listener OnBeginInteraction()
Add listener OnEndInteraction()

Initialize Event Messages Functions

14: Update():
15: Content ← Transform of this gameobject
16: RecordingWriter.WriteToFile(Tracking Object, Content, Player)
17: if Event has been executed then
18:
19:
20: end if

Content ← Event Message
RecordingWriter.WriteToFile(Tracking Object, Content, Player)

21: OnBeginInteraction(Interacted GameObject):
22: Add PropagateRecording script on Interacted Gameobject
23: Content ← Start Interaction Message
24: RecordingWriter.WriteToFile(Tracking Object, Content, Player)

25: OnEndInteraction(Interacted GameObject):
26: Remove PropagateRecording script from Interacted GameObject
27: Content ← End Interaction Message
28: RecordingWriter.WriteToFile(Tracking Object, Content, Player)

Finally, Replay (see Algorithm 4) is utilized when a user chooses to watch
a replay of a recorded session. This script is also a singleton, and all the other
scripts access it by referring to it’s instance.

At start, all event actions are coupled with the associated messages
which are either predeﬁned, or have been added by the developer for a speciﬁc
application. Thus, when Replay reads a message in the messages ﬁles, it knows
which event to execute. For example, when the Start Interaction message is
read, the algorithm calls the event that is triggered when an interaction of a
hand with a game object has begun. This event executes the same commands
that are performed when users are in play mode. Therefore, the eﬀect of
the message is the same with having the avatar of a user perform a speciﬁc

22 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Algorithm 3 Propagate Recording

1: Player ← the ID of the player this gameobject refers to
2: Tracking Object ← this Gameobject

3: Start():
4: if Gameobject is Tool then
5:
6: end if

Add listener OnToolBeginInteraction on Interacted Gameobject

7: OnToolBeginInteraction(Interacted Gameobject):
8: Add PropagateRecording script on Interacted Gameobject

Destroy this script

9: Update():
10: if Gameobject is not moving then
11:
12: end if
13: Content ← Transform of this Gameobject
14: RecordingWriter.WriteToFile(Tracking Object, Content, Player)

action. After the initialization of the event actions, the algorithm reads the
information of the recording, such as if it is single-player or multiplayer.
Afterwards, the wait time of each player is calculated, in order to synchronize
all the players’ avatars, because the users may have entered the session in
diﬀerent times. Also, this wait time is used for synchronizing the graphics
with the sounds captured from the players’ microphones. At last, the Start
function opens all the transformation and messages ﬁles, in order to read their
content on each frame on the Update function.

On each frame, the following commands are executed for each player.
Firstly, waiting or skipping the graphics update is executed. This wait or skip
ensures that the graphics are synchronized with the sounds of each player’s
avatar. After that, the script reads the next line of the transformation and
messages ﬁles and check if the sound and graphics are de-synchronized. Finally,
a check whether an event has to be executed is performed. If there is an event
for this frame, the message is executed. Otherwise, the transformation changes
are applied to the hands, heads and game objects.

4.4. VR Recording and Replay as part of a modern game engine

The VR Recording and Replay (VRRR) functionality introduced in this work
is already implemented in the MAGES SDK [31, 29], developed by ORamaVR
and available for public testing on ORamaVR’s website [26]. A poster version
of this work was recently accepted in SIGGRAPH [15]; a video summarizing
this work can be found in https://youtu.be/_aoEAOzlyPg. MAGES is a
novel VR authoring SDK, scoping in accelerating the creation process of
surgical training and the assessment of virtual scenarios. It is built on top of

Eﬃcient Networked VR Transformation Handling Using GA

23

Algorithm 4 Replay

1: Start():
2: Initialize all Event Actions
3: Get recording information
4: for each Player do
5:
6:
7:
8:
9:

if Session is Multi-Player then
Get Player’s wait time

end if
Delay ← Player’s wait time
Start Player’s Sound File with Delay
Open Player’s Transformation and Messages ﬁles

10:
11: end for

12: Update():
13: for each Player do
14:
15:
16:
17:
18:
19:

if Player Graphics have to wait then

Wait graphics for Player

end if
if Player Graphics have to move forward then

Skip graphics updates for Player

end if
Read Transform Files
Check if Sound and Graphics are Desynchronized
Read Message Files
if Event has to be executed then

Execute Message

else if No Event to Execute then

20:
21:
22:
23:
24:
25:

Apply Transforms on Hands, Head and Gameobjects

26:
27:
28: end for

end if

both the Unity3D and Unreal game engines and is composed of the following
basic pillars:

• Multiplayer: collaborative networking layer that utilizes GA interpolation

for bandwidth optimization.

• Assessment: real-time performer assessment both with supervised ma-

chine learning and predeﬁned rule-based analytics.

• Deformations: GA-enabled deformable cutting and tearing, as well as

conﬁgurable soft body simulations.

• Curriculum: Tools for deﬁning an educational curriculum enriched with
visual guidance, gamiﬁed elements and objectives to enhance transfer of
knowledge and skills.

• Prototyped surgical techniques: Implementation of commonly used surgi-
cal techniques that can be customized in order to populate new content
in a rapid manner.

24 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

The VRRR functionality extends the second and fourth pillars of the MAGES
SDK, by enabling a) experts to record their sessions, b) novices to learn how
to correctly perform an operation by watching the expert’s recording and
reviewing their own sessions, and c) evaluators to assess the learning outcomes
of the apprentices by evaluating the users via the use of VR Replay (see
Figure 5).

Via VRRR, we may record and replay a VR medical operation, in both
single player and multi player modes. These recordings can be synchronized
with the cloud and also be replayed on any device regardless of the original
hardware they were recorded on. This functionality is not just a video recording
of the in-game view, but rather a full recreation of the operation as it happened
when it was recorded. When replaying, the users are free to move around
the operation room and watch from any angle they like. The VR Recorder
allows to log the user’s sessions within a virtual environment in the form of
positions, rotations and interactions, resulting in improved accuracy without
compromising generalization. Additionally, the accurate data recorded enable
the playback feature. This feature can be a applied for creating high ﬁdelity
VR replays that guide the trainee through his/her tasks.

5. Our Results

5.1. Metrics Regarding Transmission
The methods proposed in Section 3 were implemented in Unity3D and applied
to a VR collaborative training scenario. Figure 1 illustrates an example of
extreme hand-based interpolation in collaborative, networked virtual environ-
ments. In order to properly understand the signiﬁcance of this ﬁgure, it is
advised to watch the paper’s presentation found in [17], where we demonstrate
the eﬀectiveness of our methods compared to the current state of the art.
Speciﬁcally, we compare the three methods under diﬀerent input rates per
second, i.e., the keyframes sent per second to the VR rendering engine. The
input rates tested are 5,10,15 and 20 frames per second (fps), where the last
option is an optimal value to avoid CPU/GPU strain in collaborative VR
scenarios. These rates are indicative values of the maximum possible fps that
would be sent in a network whose bandwidth rates from very-limited (5 fps)
to unrestricted (more than 20 fps). In lower fps, our methods yield jitter-less
interpolated frames compared to the state-of-the-art method, which would
require 30 fps to replicate similar output. As mentioned before, this reduction
of required data that must be transfered per second by 33%-58% (depending
on the network quality, see Table 1) is multiplied by every active user, increas-
ing the impact and the eﬀectiveness of our methods in bandwidth-restricted
environments.

The workﬂows of the two methods, compared with the current state
of the art, are summarized in Figure 3. In Figure 7 we demonstrate the
interpolation of the same object, at speciﬁc time intervals, for all methods;
the intermediate frames feel natural for both methods proposed.

Eﬃcient Networked VR Transformation Handling Using GA

25

Table 1. Summary of the metrics of our methods (Ours)
versus the state-of-the-art methods (SoA). The ﬁrst column
describes the possible network quality which correlates to
the maximum number of updates per second that can be
performed. The second column contains the update rate
required to obtain the same QoE under the speciﬁc network
quality limitations. The third column contains the comparison
of the bandwidth and the running time diﬀerence by our
algorithms compared with the SoA algorithm, when using
the respective update rates of the second column.

Network Quality How to Achieve Best QoE Metrics on Our Methods

Excellent

Good

Mediocre

Poor

SoA: 30 updates/sec
Ours: 20 updates/sec
SoA: 20 updates/sec
Ours: 10 updates/sec
SoA: 15 updates/sec
Ours: 7 updates/sec
SoA: 12 updates/sec
Ours: 5 updates/sec

33% less bandwidth
16.5% lower running time
50% less bandwidth
16.5% lower running time
53% less bandwidth
16.5% lower running time
58% less bandwidth
16.5% lower running time

In Table 1, it is demonstrated that, under various network restrictions,
both proposed methods required less data (in terms of updates per sec) to
be transmitted via the network to achieve the same QoE. This decrease in
data transfer leads to a lower energy consumption of the HMDs by 10% (on
average, preliminary result) and therefore enhances the overall mobility of
the devices relying on batteries. To measure this increase in battery life, we
used various untethered VR HMDs (HTC Vive Focus 3, Meta Quest 2, Pico
neo 3) where we run similar VR sessions with multiple (20+) users. In each
session, we changed the transmission method and the updates interval rate
as shown in Table 1, in order to maintain the same QoE. For each network
quality level, we noted and compared the battery drain diﬀerence of the same
HMD, after a ﬁxed period of time, between the SoA method and our methods
with the respective update setting. For example, in the “Excellent” tier, we
compared the battery drain, after 20 minutes, of the same HMDs, running
the SoA method with 30 updates/sec against HMDs running the proposed
methods with 20 updates/sec.

Our methods provide a performance boost, decrease the required time
to perform the same operation, with fewer keyframes but the same number
of total generated frames, by 16.5% (on average). The running times were
produced in a PC with a 3,1 GHz 16-Core Intel Core i9 processor, with 32
GBs of DDR4 memory. The same percentage of performance boost is expected
in less powerful CPUs; in this case, the overall impact, in terms of absolute
running time, will be even more signiﬁcant.

26 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Figure 7. Diﬀerent interpolation algorithms yield diﬀer-
ent, yet jitter-less, intermediate frames. (Top): State of the
art: Vector and quaternion separate interpolation. (Middle):
Dual-quaternion based interpolation algorithm. (Bottom):
Multivector based interpolation algorithm.

5.2. Metrics Regarding VR Recording
An examination of the performance of the VR Recorder was performed, and
the impact on the computed frames per second (FPS) was assessed. For
the evaluation, the frame rate (minimum, maximum and average) in two
simulations was recorded, one with the VR Recorder disabled and one with
the functionality enabled; the results are reported in table 4. In order to
provide accurate results, similar events were triggered. From the obtained
results, one can discern that only a minor, unnoticeable by the human eye,
drop in FPS occurred by enabling the VR Recorder, proof that the overall
user experience was not aﬀected negatively.

Table 2. Mean relative errors of interpolated translations
for 1-frame and 2-frame interpolation, using Euclidean norm.

Algebra
Dual Quaternions
Linear Algebra
3D CGA

1-frame error (%)
0.02
0.006
0.006

2-frame error(%)
0.009
0.009
0.009

Since FPS is aﬀected by constantly storing the translational and rota-
tional data to ﬁles, we can further reduce the FPS load caused by the VR
recorder by “skipping” data of several frames and instead generate them via
interpolation, if later required. Such an approach will inevitably lead to severe

Eﬃcient Networked VR Transformation Handling Using GA

27

Table 3. Mean relative errors of interpolated rotations for
1-frame and 2-frame interpolation, using Euclidean norm.

Algebra
Dual Quaternions
Linear Algebra
3D CGA

1-frame error (%)
0.02
0.01
0.01

2-frame error(%)
0.02
0.02
0.02

Table 4. Measuring the FPS burden of a VR application due
to the Recorder feature. Evaluation of the average, minimum
and maximum FPS for a simulation with the functionality
disabled (Column 2) or enabled (Column 3).These results
were obtained by running simulations in a PC with an i7-
11375H CPU, 16 GB of RAM and an RTX 3060 GPU.

Metric

Session without
Session with
VR Recording VR Recording

Diﬀerence

Average FPS
Minimum FPS
Maximum FPS

89.56
76.56
93.29

85.13
68.78
92.57

4.43
7.78
0.72

reduction of the size of the stored data, depending on how many we skip. For
example, if we choose to not record data on every other frame, we will have
50% lower size, whereas if we choose to keep data once per three frames we
will need only 1/3 of the original storage space.

Of course, as we record less frames, we are susceptible to interpolation
problems and artifacts may arise. Dealing with the interpolation of the stored
data, in case of replaying, can be accomplished in the same way we dealt with
the interpolation of the data transmitted over the network, in Section 3. As
proven in that section, diﬀerent representation forms for the same, stored,
displacement data require diﬀerent pipelines to be interpolated (see Figure 3).
Apparently, the results regarding the quality of the interpolated data depend-
ing on their form, presented in Section 5 and [17] also apply for the stored
data.

To further demonstrate that using multivectors and/or dual quaternions
to store positional and rotational data instead of the classic SoA forms, we
performed two additional experiments. In both experiments, we recorded a
VR session and stored the data both in SoA form, i.e, vectors for translation
and Euler angles for rotation, as well as in multivector and dual-quaternion
forms. In the ﬁrst experiment (A), we deleted the data of every other frame
whereas, in the second experiment (B), we kept the data of one every three
frames. In this way, we emulated the data that we would obtain if indeed we
recorded one every two or three frames respectively.

Depending on the representation form, we used the methods described in
Sections 3.4 and 3.3 to generate the “missing” data via interpolation (a = 1/2

28 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

for experiment A and a = 1/3 and 2/3 for experiment B). The data created
in this way were then compared to the originally recorded data by performing
the following steps. For every pair of data that we wanted to compare, we
extracted the encapsulated translation vector (x, y, z) and the rotation vector
containing the Euler angles (θx, θy, θz). Then, for each of these two vectors,
we determined the relative error (using the Euclidean norm) between the
original and the interpolated one.

Our ﬁndings are presented in Figure 8 for the ﬁrst experiment, and in
Figure 9 for the second one. In each experiment, we present the relative errors
for the translational and rotational data separately. The results show that
most displacement data have relative errors of less than 0.4% and 0.6% for
interpolating one frame and two frames, respectively. In Tables 2 and 3, we
present the mean relative errors that regard the translational and rotational
data respectively, for both experiments. These quantitative ﬁndings, along
with the qualitative results of Section 5 and [17], prove that alternative,
GA-based representation forms for displacement data are suitable for data
recording as well data transmission over networks for multi-player VR sessions.
Of course, one could try similar experiments and keep, e.g., the data
of one every n frames, for some random n. As n grows larger, we emulate
the transmission of less “actual” data and the creation of more “artiﬁcial”
interpolated frames. Inevitably, the results obtained, although they would
remain quite similar in all methods, they would not qualitatively (e.g., when
visualized) correspond to the actual movement of the object on the scene.
For example, collisions or pass-through of objects in the scene would start
to happen, or de-synchronization of objects that were supposed to move in a
synchronous way could arise in such situations. The maximum n at which such
phenomenons start to appear depends strictly on the nature of VR-session
and the “pace” of the recorded data, i.e., if user data corresponds to slow
smooth movements, we can safely omit recording more data and still obtain
close-to-the-original interpolated frames.

6. Conclusions and Future Work

This work suggests the use of two alternative representation forms, namely
dual-quaternions and multivectors, to encapsulate the displacement informa-
tion of the users in the context of a shared, collaborative virtual environment.
These forms, based on dual-quaternions and multivectors of 3D Projective
or Conformal Geometric Algebra, require speciﬁc interpolation techniques
that we present here. Crucially, we provide the way to transmute between
these forms and the classic SoA representation, i.e., vectors and quaternions
for translation and rotation. The beneﬁts of using the proposed forms is
demonstrated for two major VR functionalities; data transmission over the
network and data recording.

Regarding data transmission over the network, we apply our proposed
methods in a modern game engine and provide convincing evidence (see the

Eﬃcient Networked VR Transformation Handling Using GA

29

Figure 8. Results from Experiment A. The graph depicts
the relative error (vertical axis) that occurs if we replace
the transitional (left) or the rotational data (right) of one
every two frames by interpolated data. This interpolated
data depends on the form used to represent the original
displacement information; vectors and Euler angles (green
cross), dual quaternions (red dots) or multivectors (blue dots).
The horizontal axis refers the index of the frame for which
the data are interpolated.

video in [17]) that they outperform SoA methods, oﬀering increased QoE,
especially as the network quality, in terms of bandwidth, deteriorates.

Regarding data recording, we introduce a novel method that describes
the functionality and characteristics of an eﬃcient VR recorder with replay
capabilities. Our proposed representation forms can be used to eﬀectively
store 3D scene transformation data. In the case that speciﬁc data is inten-
tionally omitted to be recorded, for storage eﬃciency purposes, the use of the
suggested interpolation methods will generate data “close” to the omitted,
with insigniﬁcant relative error.

Regarding future work, the results of our proposed interpolation algo-
rithms can be further improved by using optimized C# Geometric Algebra
bindings to eﬃciently perform complex operations such as dual-quaternion or
multivector SLERP. Moreover, by using the data logged with the VR Recorder,
we plan to create (a) an assessment tool that evaluates how the users perform
on the given tasks,(b) intelligent agents that are able to complete tasks on
their own and (c) a no-code VR authoring tool, with which the environment
designers can develop new training modules.

The assessment tool will be developed by training supervised learning
algorithms with labelled action data, provided by the simulation designer.
For example, the designer will provide multiple session data labelled from
0 to 10, depending on whether he/she performed the action poorly (0) or
perfectly (10). The intelligent agents will be trained using such experts’ session

30 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Figure 9. Results from Experiment B. The graph depicts
the relative error (vertical axis) that occurs if we replace
the transitional (left) or the rotational data (right) of one
every three frames by interpolated data. This interpolated
data depends on the form used to represent the original
displacement information; vectors and Euler angles (green
cross), dual quaternions (red dots) or multivectors (blue
dots).The horizontal axis refers the index of the frame for
which the data are interpolated.

data and the no-code VR authoring tool will fuse all recorded data and train
machine learning algorithms in order to understand what task the designer
wants to develop. An initial step can be found in [2], where the authors
have developed a Convolutional Neural Network (CNN) which is able to
evaluate cuts in a surgical virtual environment. The transformation data
(translations and rotations) of the cuts are fed to the neural network, and
scores of the cuts, labelled by the surgical simulation designer, are the classes
of the classiﬁcation problem. In a future version, we plan to utilize dual
quaternions and multivectors as the input form of the neural networks and
examine whether data in such algebras can achieve better training results.

Also, inspired by the Quaternion Convolutional Neural Network (QCNN)
[37,32], we aspire to formulate and develop the Dual Quaternion Convolutional
Neural Network (DQCNN) and the Multivector Convolutional Neural Network
(MCNN). Lastly, QCNN algorithms can also be deployed to address the
classiﬁcation problem (similar to the one posed in [2]), especially when used
for rotational data, while keeping CNN for translational data. We assume that
the overall accuracy will increase, since the CNNs treat the transformation
data as six unrelated features, while the QCNN processes the rotations
as a hypercomplex number, and therefore takes into consideration their
interrelationship information.

Eﬃcient Networked VR Transformation Handling Using GA

31

7. Acknowledgments

This work was co-ﬁnanced by European Regional Development Fund of
the European Union and Greek national funds through the Operational
Program Competitiveness, Entrepreneurship and Innovation, under the call
RESEARCH – CREATE - INNOVATE (project codes:T1EDK-01149 and
T1EDK-01448). The project also received funding from the European Union’s
Horizon 2020 research and innovation programme under grant agreement No
871793.

References

[1] Birrenbach, T., Zbinden, J., Papagiannakis, G., Exadaktylos, A.K., Müller,
M., Hautz, W.E., Sauter, T.C.: Eﬀectiveness and utility of virtual reality
simulation as an educational tool for safe performance of COVID-19 diagnostics:
Prospective, randomized pilot trial. JMIR Serious Games 9(4), e29586 (2021)
[2] Chrysovergis, I., Kamarianakis, M., Kentros, M., Angelis, D., Protopsaltis, A.,
Papagiannakis, G.: Assessing unconstrained surgical cuttings in vr using cnns.
arXiv preprint arXiv:2205.00934 (2022)

[3] Churchill, E.F., Snowdon, D.: Collaborative virtual environments: an introduc-

tory review of issues and systems. virtual reality 3(1), 3–15 (1998)

[4] Diebel, J.: Representing attitude: Euler angles, unit quaternions, and rotation

vectors. Matrix 58(15-16), 1–35 (2006)

[5] Doran, C., Gullans, S.R., Lasenby, A., Lasenby, J., Fitzgerald, W.: Geometric

algebra for physicists. Cambridge University Press (2003)

[6] Dorst, L.: A guided tour to the plane-based geometric algebra pga. URL

https://bivector.net/PGA4CS.html

[7] Dorst, L., Fontijne, D., Mann, S.: Geometric algebra for computer science -
an object-oriented approach to geometry. The Morgan Kaufmann series in
computer graphics (2007)

[8] Dorst, L., Valkenburg, R.: Square root and logarithm of rotors in 3d conformal
geometric algebra using polar decomposition. In: Guide to Geometric Algebra
in Practice, pp. 81–104. Springer (2011)

[9] Grantcharov, T.P., Kristiansen, V.B., Bendix, J., Bardram, L., Rosenberg, J.,
Funch-Jensen, P.: Randomized clinical trial of virtual reality simulation for
laparoscopic skills training. Br. J. Surg. 91(2), 146–150 (2004)

[10] Hadﬁeld, H., Lasenby, J.: Direct Linear Interpolation of Geometric Objects in
Conformal Geometric Algebra. Advances in Applied Cliﬀord Algebras 29(4),
01 (2019)

[11] Hildenbrand, D.: Foundations of geometric algebra computing. Springer (2013)
[12] Hildenbrand, D., Pitt, J., Koch, A.: Gaalop—high performance parallel comput-
ing based on conformal geometric algebra. In: Geometric Algebra Computing,
pp. 477–494. Springer (2010)

[13] Hildenbrand, D., Zamora, J., Bayro-Corrochano, E.: Inverse Kinematics Com-
putation in Computer Graphics and Robotics Using Conformal Geometric
Algebra. Advances in Applied Cliﬀord Algebras 18(3), 699–713 (2008). DOI
10.1007/s00006-008-0096-5

32 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

[14] Hooper, J., Tsiridis, E., Feng, J.E., Schwarzkopf, R., Waren, D., Long, W.J.,
Poultsides, L., Macaulay, W., NYU Virtual Reality Consortium: Virtual reality
simulation facilitates resident training in total hip arthroplasty: A randomized
controlled trial. J. Arthroplasty 34(10), 2278–2283 (2019)

[15] Kamarianakis, M., Chrysovergis, I., Kentros, M., Papagiannakis, G.: Recording
and replaying psychomotor user actions in VR. In: Special Interest Group
on Computer Graphics and Interactive Techniques Conference Posters (SIG-
GRAPH ’22 Posters). ACM, NY, USA (2022). DOI 10.1145/3532719.3543253

[16] Kamarianakis, M., Lydatakis, N., Papagiannakis, G.: Never ‘Drop the Ball’
in the Operating Room: An Eﬃcient Hand-Based VR HMD Controller In-
terpolation Algorithm, for Collaborative, Networked Virtual Environments.
In: N. Magnenat-Thalmann, V. Interrante, D. Thalmann, G. Papagiannakis,
B. Sheng, J. Kim, M. Gavrilova (eds.) Advances in Computer Graphics, Lecture
Notes in Computer Science, pp. 694–704. Springer International Publishing,
Cham (2021). DOI 10.1007/978-3-030-89029-2_52

[17] Kamarianakis, M., Lydatakis, N., Papagiannakis, G.: Video presentation of the
paper ’Never Drop the Ball’ (2021). URL https://youtu.be/xoXrRU-2gLQ
[18] Kamarianakis, M., Papagiannakis, G.: An all-in-one geometric algorithm for
cutting, tearing, drilling deformable models. arXiv preprint arXiv:2102.07499
(2021)

[19] Kavan, L., Collins, S., Žára, J., O’Sullivan, C.: Geometric skinning with ap-

proximate dual quaternion blending. ACM Trans. Graph. 27(4) (2008)

[20] Kenwright, B.: A beginners guide to dual-quaternions: What they are, how
they work, and how to use them for 3D character hierarchies. In: WSCG 2012
- Conference Proceedings, pp. 1–10. Newcastle University, United Kingdom
(2012)

[21] Kloiber, S., Settgast, V., Schinko, C., Weinzerl, M., Fritz, J., Schreck, T.,
Preiner, R.: Immersive analysis of user motion in VR applications. The Visual
Computer 36(10-12), 1937–1949 (2020). DOI 10.1007/s00371-020-01942-1. URL
https://doi.org/10.1007/s00371-020-01942-1

[22] Lahanas, V., Loukas, C., Smailis, N., Georgiou, E.: A novel augmented reality
simulator for skills assessment in minimal invasive surgery. Surg. Endosc. 29(8),
2224–2234 (2015)

[23] Lhemedu-Steinke, Q., Meixner, G., Weber, M.: Comparing vr display with
conventional displays for user evaluation experiences. In: 2018 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), pp. 583–584 (2018). DOI
10.1109/VR.2018.8446076

[24] Molet, T., Aubel, A., Çapin, T., Carion, S., Lee, E., Magnenat-Thalmann, N.,
Noser, H., Pandzic, I., Sannier, G., Thalmann, D.: Anyone for tennis? Presence:
Teleoperators & Virtual Environments 8(2), 140–156 (1999)

[25] Nisky, I., Che, Y., Quek, Z.F., Weber, M., Hsieh, M.H., Okamura, A.M.:
Teleoperated versus open needle driving: Kinematic analysis of experienced
surgeons and novice users. In: 2015 IEEE International Conference on Robotics
and Automation (ICRA), pp. 5371–5377 (2015). DOI 10.1109/ICRA.2015.
7139949

[26] ORamaVR: Accelerating the world’s transition to medical vr training. URL

https://oramavr.com/

Eﬃcient Networked VR Transformation Handling Using GA

33

[27] Papaefthymiou, M., Hildenbrand, D., Papagiannakis, G.: An inclusive Conformal
Geometric Algebra GPU animation interpolation and deformation algorithm.
The Visual Computer 32(6-8), 751–759 (2016)

[28] Papagiannakis, G.: Geometric algebra rotors for skinned character animation

blending. In: SIGGRAPH Asia 2013 Technical Briefs, SA 2013 (2013)

[29] Papagiannakis, G., Lydatakis, N., Kateros, S., Georgiou, S., Zikas, P.: Trans-
forming medical education and training with vr using m.a.g.e.s. In: SIG-
GRAPH Asia 2018 Posters, SA ’18. Association for Computing Machinery,
New York, NY, USA (2018). DOI 10.1145/3283289.3283291. URL https:
//doi.org/10.1145/3283289.3283291

[30] Papagiannakis, G., Singh, G., Magnenat-Thalmann, N.: A survey of mobile and
wireless technologies for augmented reality systems. Computer Animation and
Virtual Worlds 19(1), 3–22 (2008)

[31] Papagiannakis, G., Zikas, P., Lydatakis, N., Kateros, S., Kentros, M., Geroniko-
lakis, E., Kamarianakis, M., Kartsonaki, I., Evangelou, G.: Mages 3.0: Tying the
knot of medical vr. In: ACM SIGGRAPH 2020 Immersive Pavilion. Association
for Computing Machinery (2020)

[32] Parcollet, T., Morchid, M., Linarès, G.: Quaternion convolutional neural net-
works for heterogeneous image processing. In: ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 8514–8518 (2019). DOI 10.1109/ICASSP.2019.8682495

[33] Ruan, J., Xie, D.: Networked vr: State of the art, solutions, and challenges.

Electronics 10(2), 166 (2021)

[34] Sharma, Y., Bettadapura, V., Plötz, T., Hammerla, N.Y., Mellor, S., McNaney,
R., Olivier, P., Deshmukh, S., McCaskie, A.W., Essa, I.: Video based assessment
of osats using sequential motion textures. In: Fifth Workshop on Modeling and
Monitoring of Computer Assisted Interventions (2014)

[35] Southgate, E.: Using screen capture video to understand learning in virtual
reality. In: 2020 IEEE Conference on Virtual Reality and 3D User Interfaces
Abstracts and Workshops (VRW), pp. 418–421 (2020). DOI 10.1109/VRW50115.
2020.00089

[36] Vilmi, O.: Real-time multiplayer software architecture. Bachelor thesis, Metropo-

lia University of Applied Sciences (2020)

[37] Yin, Q., Wang, J., Luo, X., Zhai, J., Jha, S.K., Shi, Y.Q.: Quaternion convolu-
tional neural network for color image classiﬁcation and forensics. IEEE Access
7, 20293–20301 (2019). DOI 10.1109/ACCESS.2019.2897000

[38] Zia, A., Sharma, Y., Bettadapura, V., Sarin, E.L., Essa, I.: Video and
accelerometer-based motion analysis for automated surgical skills assessment.
International Journal of Computer Assisted Radiology and Surgery 13, 443–455
(2018)

[39] Zia, A., Sharma, Y., Bettadapura, V., Sarin, E.L., Ploetz, T., Clements, M.A.,
Essa, I.: Automated video-based assessment of surgical skills for training and
evaluation in medical schools. Int. J. Comput. Assist. Radiol. Surg. 11(9),
1623–1636 (2016)

[40] Zikas, P., Papagiannakis, G., Lydatakis, N., Kateros, S., Ntoa, S., Adami, I.,
Stephanidis, C.: Scenior: An Immersive Visual Scripting system based on VR

34 Kamarianakis, Chrysovergis, Lydatakis, Kentros and Papagiannakis

Software Design Patterns for Experiential Training. The Visual Computer
36(10-12), 1965–1977 (2020). DOI 10.1007/s00371-020-01919-0

Manos Kamarianakis
Department of Mathematics & Applied Mathematics,
University of Crete,
Voutes Campus, 70013 Heraklion, Greece
Orchid ID: 0000-0001-6577-0354
e-mail: kamarianakis@uoc.gr

Ilias Chrysovergis
ORamaVR,
FoRTH (STEP-C), 70013 Heraklion, Greece
Orchid ID: 0000-0002-5434-2175
e-mail: ilias@oramavr.com

Nick Lydatakis
Department of Computer Science,
University of Crete,
Voutes Campus, 70013 Heraklion, Greece
Orchid ID: 0000-0001-8159-9956
e-mail: nick@oramavr.com

Mike Kentros
Department of Computer Science,
University of Crete,
Voutes Campus, 70013 Heraklion, Greece
Orchid ID: 0000-0002-3461-1657
e-mail: mike@oramavr.com

George Papagiannakis
Department of Computer Science,
University of Crete,
Voutes Campus, 70013 Heraklion, Greece
Orchid ID: 0000-0002-2977-9850
e-mail: papagian@ics.forth.gr

