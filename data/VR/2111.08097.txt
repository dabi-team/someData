1
2
0
2

v
o
N
5
1

]

O
R
.
s
c
[

1
v
7
9
0
8
0
.
1
1
1
2
:
v
i
X
r
a

Virtual Reality for Synergistic Surgical Training and Data
Generation

Adnan Munawara, Zhaoshuo Lia, Punit Kunjama, Nimesh Nagururua, Andy S.
Dinga, Peter Kazanzidesa, Thomas Looib, Francis X. Creightona, Russell H. Taylora
and Mathias Unberatha

aJohns Hopkins University, Baltimore, US;
bHospital for Sick Children, Toronto, Canada.

ARTICLE HISTORY
Compiled November 17, 2021

ABSTRACT
Surgical simulators not only allow planning and training of complex procedures,
but also oÔ¨Äer the ability to generate structured data for algorithm development,
which may be applied in image-guided computer assisted interventions. While there
have been eÔ¨Äorts on either developing training platforms for surgeons or data gen-
eration engines, these two features, to our knowledge, have not been oÔ¨Äered to-
gether. We present our developments of a cost-eÔ¨Äective and synergistic framework,
named Asynchronous Multibody Framework Plus (AMBF+), which generates data
for downstream algorithm development simultaneously with users practicing their
surgical skills. AMBF+ oÔ¨Äers stereoscopic display on a virtual reality (VR) device
and haptic feedback for immersive surgical simulation. It can also generate diverse
data such as object poses and segmentation maps. AMBF+ is designed with a Ô¨Çex-
ible plugin setup which allows for unobtrusive extension for simulation of diÔ¨Äerent
surgical procedures. We show one use case of AMBF+ as a virtual drilling simulator
for lateral skull-base surgery, where users can actively modify the patient anatomy
using a virtual surgical drill. We further demonstrate how the data generated can
be used for validating and training downstream computer vision algorithms.‚àó

KEYWORDS
Surgical simulation; Virtual reality; Deep learning; Computer vision

1. Introduction

EÔ¨Äective ‚Äútraining‚Äù is pertinent for both surgeons and image-guided intervention al-
gorithms. Thus, creating opportunities to learn from experience for both surgeons and
algorithms has become of substantial interest.

On one hand, surgeons can develop necessary surgical skills and 3D spatial percep-
tion by using surgical simulators with the beneÔ¨Åt of safety of a simulated world (Chan
et al. (2016), Lui and Hoy (2017), Locketz et al. (2017), Chen et al. (2018)). Beyond
improving general surgical skills, surgical simulators can improve outcomes in individ-
ual patients through the ability to generate surgical views from unique pre-operative
scans. More recently, Asynchronous Multi-Body Framework (AMBF) (Munawar et al.

Email: {amunawar, zli122}@jhu.edu

‚àóCode is available at https://github.com/LCSR-SICKKIDS/volumetric_drilling

 
 
 
 
 
 
(2019)), has recently been introduced to the Ô¨Åeld and has been designed for Ô¨Çuid
interaction with virtual surgical phantoms.

On the other hand, developing algorithms for image-guided computer-assisted sur-
gical interventions also requires ‚Äútraining‚Äù. It generally requires structured data either
for validation purposes used in geometric-based algorithms such as Simultaneous Lo-
calization and Mapping (SLAM); or for training supervision used in deep-learning
based algorithms, such as learning based segmentation (Zisimopoulos et al. (2017)).
Many prior work in synthetic medical data generation exist for diÔ¨Äerent modalities,
such as CT scans (Unberath et al. (2018)) and pathology slices (Mahmood et al. (2020))
as summarized in Chen et al. (2021). VisionBlender by Cartucho et al. (2020) uses
Blender1 to generate data for endoscopic surgeries, oÔ¨Ä-line, for development of com-
puter vision algorithms. The images generated are visually pleasing however motion
of camera and surgical instruments is only possible along pre-speciÔ¨Åed paths. Realistic
tool-tissue interaction and camera motion enable cost-eÔ¨Äective development or anal-
ysis for algorithms contingent on these information. For example, SurgVisDom2 is a
synthetic dataset with realistic tool motion designed for surgical activities recognition,
yet only designed for robotic laparoscopic surgery and not open-sourced.

Integrating the two components, i.e. generating data from realistic surgical simu-
lation with tool-tissue interaction in real-time, will oÔ¨Äer a synergistic, and therefore
cost-eÔ¨Äective solution to both problems. While surgeons practice their surgical skills
using the simulator, clinically relevant data are collected in a precisely controlled envi-
ronment for downstream development of computer vision algorithms. This can reduce
the material cost compared to cadaveric dissection (George and De (2010)) and the
need for a separate eÔ¨Äort of data generation. To this end, we introduce Asynchronous
Multi-Body Framework Plus (AMBF+), an open-sourced simulation framework that
combines the object interaction capabilities of AMBF with data generation capabil-
ities similar to those of VisionBlender. We also propose a plugin setup described in
subsection 2.1 to accommodate specialized functionalities for diÔ¨Äerent surgical pro-
cedures and diÔ¨Äerent data needed for downstream algorithms while keeping the core
simulation logic intact. By unifying the surgical training simulation and data genera-
tion into one single framework, we attempt to present a synergistic solution to both
human training and algorithm development. This framework overcomes the aforemen-
tioned limitations and allows surgeons to perform tasks similar to real surgeries while
generating/recording relevant information for vision algorithms.

To demonstrate the functionalities of AMBF+, we have developed a simulator for
skull-base surgery similar to Locketz et al. (2017). Skull-base surgery is a surgical
subspecialty in neurosurgery and otolaryngology for surgical intervention of pathology
within the base of the skull. During surgery, surgeons inspect the surgical Ô¨Åeld via
surgical microscopes and use surgical drills to remove the bony tissue and gain access
to relevant anatomy. In our virtual drilling simulator as shown in Figure 1, users can
actively drill a patient model using input devices, while synthetic data is automati-
cally generated. We further demonstrate the utility of this generated data in diÔ¨Äerent
computer vision-based tasks.

The key aspects of our development can be summarized as follows:

‚Ä¢ A virtual surgical simulation framework AMBF+ is provided to enable users to
interact with a patient model for surgical practice. The simulation provides both
visual and haptic feedback for assistance and realism.

1https://www.blender.org/
2https://surgvisdom.grand-challenge.org/

2

Figure 1. Overview of the virtual drilling simulator for skull-base surgery developed using AMBF+. A
segmented CT is rendered as a 3D virtual phantom in the simulator. Users send control input and receive haptic
feedback with Phantom OMNI3. Stereoscopic views are provided with Oculus Rift4 and the view direction is
synchronized with the user‚Äôs head.

‚Ä¢ The AMBF+ framework can generate data (RGB stereo images, depth and etc.)
in real time while the user is performing virtual surgeries, which can be used for
computer vision algorithm development and training.

‚Ä¢ We propose a plugin design to expand the AMBF+ framework without modiÔ¨Å-
cation to the core source-code such that functionalities customized for diÔ¨Äerent
surgeries can be easily incorporated.

‚Ä¢ We demonstrate one use case of AMBF+ ‚Äì a virtual drilling simulator for skull-
base surgery. We show how the generated data can be potentially used in down-
stream tasks.

2. AMBF+ System Overview

We developed AMBF+ on top of AMBF (Munawar et al. (2019)), which uses libraries
such as CHAI3D (Conti et al. (2005)) and Bullet Physics (Coumans (2015)). AMBF+
oÔ¨Äers a convenient platform for surgical robot simulations as it allows for the creation of
kinematically redundant robots and mechanisms with ease. The framework utilizes an
AMBF Description Format (ADF) for deÔ¨Åning rigid and soft-bodies, sensors, actuators
and world scene objects. The ADF Ô¨Åles are modular such that a model (consisting
of bodies, joints, sensors and actuators) can be deÔ¨Åned in multiple ADF Ô¨Åles that
can be loaded together. The parenting of bodies, joints, sensors and actuators is also
modular which means that a prospective parent or child does not need to exist in the
same ADF Ô¨Åle. This makes AMBF+ versatile for surgical robot related simulations as
surgical robots often have multiple interchangeable tools. The ADF Ô¨Åles are of three
diÔ¨Äerent types: (1) world Ô¨Åles, (2) input devices Ô¨Åles (Munawar et al. (2021)) and (3)
model Ô¨Åles. A meta-data Ô¨Åle, called the launch Ô¨Åle, is the entry point for AMBF+ and

3https://www.3dsystems.com/haptics-devices/touch
4https://www.oculus.com/facebook-horizon

3

Figure 2. Flow of loading models and world scene objects into AMBF+.

contains the Ô¨Ålepaths of a world Ô¨Åle, an input devices Ô¨Åle and possibly several model
Ô¨Åles. Fig. 2 describes the Ô¨Çow of loading ADF Ô¨Åles into AMBF+.

In AMBF+ we not only extended the rendering pipeline and the ADF speciÔ¨Åcation,
but we also implemented a modular plugin handling interface. The extension to the
rendering pipeline allowed us to generate and stream colored point-cloud data and
segmentation masks. Extension to the ADF speciÔ¨Åcation allowed us to deÔ¨Åne custom
OpenGL shaders for rendering and computation. The plugin handling interface allowed
the development of custom applications such as the volumetric-drilling simulation, thus
extending its feature set.

(a)

(b)

Figure 3.
each computational scope.

(a) The diÔ¨Äerent computational scopes of an AMBF+ simulation. (b) Corresponding plugins for

2.1. Plugin Design

An AMBF+ simulation comprises of diÔ¨Äerent hierarchical computational scopes as
shown in Fig. 3(a), where each inner block is encapsulated by the outer block to rep-
resent a higher scope in hierarchy. A higher scope has access to more resources and
control over the simulation components as compared to the lower ones. The highest

4

Launch File (launch.yaml)World FileModel FilesModel FilesModel FilesInput Devices FileWorld File (world.yaml)World Attributes:Env Model FileModel File(model.yaml)Object Attribs:Object Attribs:Object Attribs:Model ShadersObject Attribs:Object Type:Object Specific Attribs:Object ShadersPlugin FilepathsWorld Attribs:Solver Attribs:Gravity:Dimensions:World ShadersPlugin FilepathsPlugin FilepathsPlugin FilepathsafSimulatorafWorldafModel 1afObject 1 (body)afObject 2 (sensor)afObject 3 (joint)afModel 2afObject 1 (camera)afObject 2 (light)afObject 3 (vehicle)afWorldPluginafModelPluginafObjectPluginafSimulatorPluginFigure 4. Interfaces provided by the diÔ¨Äerent types of plugins.

scope is a single afSimulator instance that manages an afWorld instance. The afWorld
may contain instances of afModels and each afModel may contain several instances of
afObjects. The term afObject refers to simulation entities such as cameras, lights, bod-
ies (rigid and soft), volumes, sensors, and actuators as shown in Fig. 5. The afObjects
can be further classiÔ¨Åed into visual and non-visual objects. Rigid-bodies, soft-bodies,
and volumes are types of visual objects.

We developed plugin interfaces for each AMBF+ simulation scope (Fig. 3(b)). Each
type of plugin has a slightly diÔ¨Äerent interface, as shown in Fig. 4, and thus extends
the functionality of the scope that it is deÔ¨Åned for.

Since the entry to AMBF+ is via the launch Ô¨Åle (Fig. 2), we extended the ADF
format to incorporate the plugin speciÔ¨Åcation. Plugins can be deÔ¨Åned for afSimulator,
afWorld and afObject instances by setting them in the appropriate ADF Ô¨Åle and/or
scope. Our extension to the ADF speciÔ¨Åcation is backward compatible, as older ver-
sions of AMBF would simply ignore the plugins data-Ô¨Åeld.

2.2. Rendering Pipeline

AMBF uses OpenGL5 for rendering but provides limited access to some of its useful
features such as loading custom shaders (vertex and fragment). In AMBF+, we ex-
tended AMBF to allow the speciÔ¨Åcation of these shaders via the ADF Ô¨Åles. Similar to
plugins, shaders can be deÔ¨Åned in the world ADF Ô¨Åle, the model ADF Ô¨Åles, and for
individual afObjects as shown in Fig. 2. The shaders are then applied to all afObjects,
afObjects in the speciÔ¨Åc model or only to the speciÔ¨Åc afObject depending upon the
scope that they are speciÔ¨Åed in. Furthermore, if shaders are deÔ¨Åned at multiple scopes,
the afObject deÔ¨Åned shaders override the model deÔ¨Åned shaders and the model deÔ¨Åned
shaders override the world deÔ¨Åned shaders.

5https://www.opengl.org/
6https://www.khronos.org/opengl/wiki/Depth Test
7https://www.khronos.org/opengl/wiki/FramebuÔ¨Äer Object

5

AMBF OBJECT PLUGIN INTERFACEclose()init(afBaseObj*, objAttribs)gfxUpdate(afObject*)phxUpdate(afObject*)AMBF MODEL PLUGIN INTERFACEclose()init(afModel*, modelAttribs)onObjRemoval(afBaseObj*)onObjAdd(afBaseObj*)gfxUpdate(afModel*)phxUpdate(afModel*)AMBF WORLD PLUGIN INTERFACEclose()init(afWorld*, worldAttribs)onModelAdd(afModel*)onModelRemoval(afModel*)onObjRemoval(afBaseObj*)onObjAdd(afBaseObj*)gfxUpdate(afWorld*)phxUpdate(afWorld*)AMBF SIMULATOR PLUGIN INTERFACEgfxUpdate(afWorld*)init(argc, argv)phxUpdate(afWorld*)keyboardCB()mouseCB()windowResizeCB()close()(a)(b)(c)(d)2.2.1. Point Cloud Generation

We utilized OpenGL‚Äôs Z-buÔ¨Äer6 and FramebuÔ¨Äer objects7 to generate the point cloud
data. Our application utilizes perspective cameras from OpenGL, which induce non-
linearity in the generated depth values. The non-linear depth values need to be lin-
earized. This step can be performed on the GPU for faster processing, (in the fragment
shader (Alg. 1)) however the values must also be normalized to avoid truncation by the
fragment shader‚Äôs output. Finally, on the CPU, these normalized values are rescaled
to get the correct point could. The maximum dimensions ( (cid:126)M D) of the view frustum
are used for both the normalization and scaling. The vector (cid:126)M D is calculated as:

Figure 5. DiÔ¨Äerent types of afObjects.

M Dx = 2.0 √ó f √ó tan(f va/2); M Dy = M Dx √ó AR; M Dz = f ‚àí n

(1)

Where n, f , f va and AR correspond to the simulated camera‚Äôs near plane, far plane,

Ô¨Åeld view angle and aspect ratio, respectively.

Algorithm 1 Depth Computation Shader
1: Fx, Fy ‚Üê F ragCoord.xy
2: B0, B1, B2, B3 ‚Üê T exture2D(Fx, Fy)
3: Z ‚Üê (B3 (cid:28) 24) ‚à® (B2 (cid:28) 16) ‚à® (B1 (cid:28) 8) ‚à® B0
4: Fz ‚Üê Z/224
5: Pnorm ‚Üê [Fx, Fy, Fz, 1.0]
6: Pclip ‚Üê (P rojectionM atrix)‚àí1Pnorm
7: Pcam ‚Üê Pclip/Pclip.w
8: Nxy ‚Üê (Pcam.xy + (cid:126)M Dxy/2.0)/ (cid:126)M Dxy
9: Nz ‚Üê (Pcam.z ‚àí n)/(f ‚àí n)
10: F ragOutput ‚Üê [Nx, Ny, Nz, 1.0]

(cid:46) Input to the Shader
(cid:46) Depth packed in 4 one-byte channels
(cid:46) Bit Shifting and Logical OR

(cid:46) Projection Matrix of Simulated Camera

2.2.2. Segmentation Mask Generation

AMBF+‚Äôs rendering pipeline comprises of four passes which are shown in Fig 6(a).
Three passes are implemented on the GPU and the fourth one on the CPU. The Ô¨Årst
pass generates a 2 dimensional color image that is output to the screen. The second
pass renders to the color and depth FramebuÔ¨Äers. The values in the depth-buÔ¨Äer are
non-linear at this point. Prior to the third pass, the depth computation shaders are
loaded. These shaders are responsible for linearizing and normalizing the depth values

6

Rigid BodySoft BodyVolumeSensorActuatorJointCameraLightBase ObjectVisual ObjectsNon-Visual ObjectsVehicle(Alg. 1) on the third rendering pass. The fourth pass re-scales the normalized depth
values and computes camera space transformation of the point cloud data. During this
pass, the color FramebuÔ¨Äer generated from the second pass is also superimposed on
the point cloud to generate a colored point cloud as shown in Fig. 6(b).

We utilize the custom shader speciÔ¨Åcation, via the ADF Ô¨Åles, to load special pre-
processing shaders for visual objects prior to the second rendering pass. The sole
purpose of these shaders is to assign a speciÔ¨Åc color to the visual object while ignor-
ing the lighting calculations. The result is a segmentation mask image that can be
superimposed on the point cloud in the fourth pass as shown in Fig. 6(b).

(a)

(b)

Figure 6.
of the segmentation mask on the point cloud to generate labeled data.

(a) DiÔ¨Äerent passes in the rendering pipeline to generate a segmented point cloud. (b) Augmentation

2.3. Data Streaming and Recording

We use the Robot Operating System (ROS)8 interface due to its wide usage in the
surgical robotics community. We stream out the data generated from AMBF+ as ROS
topics. Such design allows for replay of surgical operation, easy retrieval and eÔ¨Écient
storage.

‚Ä¢ Stereo images: Stereo images (left and right) are published via a ROS image
topic by AMBF+ at a user speciÔ¨Åed frequency. The left and right stereo images
are produced by two diÔ¨Äerent cameras initialized in AMBF+. By manipulating
the OpenGL parameters of the two cameras, location and look at, the stereo view
can be set up in a highly customizable fashion. Location is used to establish the
stereo baseline and look at is used to align the principle axes of the left and right
cameras.

‚Ä¢ Depth: A depth point cloud is created as described in Section 2.3.1.
‚Ä¢ Segmentation mask: The ground truth segmentation mask is created as described

in Section 2.3.2.

‚Ä¢ Object/camera pose: Object/camera pose contains the xyz position and rotation
(either quaternion or Euler angles) with respect to the world frame. DiÔ¨Äerent
objects and cameras may be added to the AMBF+ scene and can be tracked
and controlled by AMBF+ and its Python client.

8https://www.ros.org/

7

Per Camera Rendering LoopRender to ScreenRender Color & Depth to FrameBuffersLoad/Use Preprocessing ShadersCopy Depth Buffer and Superimpose ColorLoad Depth Computation ShadersRender using Depth Computation Shader1.2.3.4.Exec. OrderScreen Output1st PassDepth Data3rd PassDepth Data + Scene Color4th PassDepth Data + Seg. Mask4th Pass‚Ä¢ Camera parameters: The intrinsic parameters of cameras in AMBF+ are es-
tablished in OpenGL fashion using the vertical Ô¨Åeld view angle (f va) which
describes the perspective frustum. The intrinsic matrix (I) can be calculated
through the following equations:

I =

Ô£Æ

Ô£∞

fx
0
0

0
fy
0

Ô£π

Ô£ª ;

cx
cy
1

fx = fy =

H

2 √ó tan

(cid:16) f va
2

(cid:17) ;

cx = W/2;

cy = H/2 (2)

Where fx and fy are the focal length in pixels in the x and y directions. W and
H are the width and height of the image produced in pixels. cx and cy describe
the x and y position of the camera principal point. Extrinsics information is
obtained from the relative transformation between the two cameras.

2.4. Stereo Display and VR Support

Depth perception is a requirement for a simulation environment targeting surgeries.
We generate a pair of stereoscopic images that are then displayed on a Virtual Reality
(VR) head-mounted display (HMD). We also support the orientation control of the
virtual cameras based on the HMD‚Äôs angular movements as a means of looking around
in the simulation scene. We rely on the OpenHMD9 package for HMD support and
tested our implementation using an Occulus Rift VR device.

2.5. Haptic Feedback

AMBF+ utilizes CHAI3D‚Äôs (Conti et al. (2005)) Ô¨Ånger proxy collision algorithm (Rus-
pini and Khatib (2001)) to provide haptic feedback of interaction with surface meshes
and volumes. Tool-cursors, where each tool-cursor encapsulates a pair of proxy/goal
spheres, are used for feedback calculation. The movement of the tool-cursor moves the
encapsulated proxy/goal spheres such that the proxy sphere is stopped at the bound-
ary of the surface or volume while the goal sphere coincides with the tool-cursor‚Äôs
pose, and is thus allowed to penetrate inside. The distance between the proxy and the
goal spheres (Œ¥pi) for each tool-cursor can then be used to generate a proportional
force feedback.

2.6. Use-case of AMBF+ ‚Äì Virtual Drilling Simulator

To demonstrate the Ô¨Çexibility of AMBF+, a drilling simulator is developed that lever-
ages the aforementioned functionalities. The scheme of the simulator is summarized
in Figure 1. We customize the following functionalities using the plugin design: 1)
customized data representation from a pre-operative CT scan, which will be rendered
at frame-rate, 2) virtual drilling with input from a haptic device (Phantom OMNI or
Oculus Rift in our setup). A video demonstration of our simulation scene is provided
in the supplementary material.

9https://github.com/OpenHMD/OpenHMD

8

2.6.1. Virtual Model Representation

The virtual patient model is obtained from a CT scan and is patient speciÔ¨Åc. We use an
oÔ¨Ä-the-shelf CT segmentation pipeline (Ding et al. (2021)) to generate the correspond-
ing segmented volume for each anatomical structure in 3D. The segmented volume is
used for three purposes: 1) to texture each anatomy with its corresponding anatomical
color, 2) to diÔ¨Äerentiate skull and other critical anatomies for visual warnings, and 3)
to generate 2D segmentation masks.

We use an array of 2-dimensional images (in PNG or JPEG format) for texture-
based volumetric rendering (Kruger and Westermann (2003)) and drilling. These im-
ages combined represent the 3D volume, with each color denoting a diÔ¨Äerent anatomy.
We provide utility scripts that can convert the data from the NRRD format to an
array of images. This allows the segmented volumetric data to be imported for use
within the virtual drilling simulator.

2.6.2. Virtual Drilling and Haptic Feedback

(a)

(b)

Figure 7.
and shaft tool-cursors. The origin of the drill mesh coincides with the center of st.

(a) Mesh model of the simulated drill. (b) Collision approximation of the simulated drill with tip

Our drilling simulator allows the users to drill the volumetric data (anatomy) with
a haptic device which is represented as a simulated drill, as shown in Fig. 7. The
CHAI3D (Conti et al. (2005)) library is used for haptic feedback logic. For a more
accurate collision response, the simulated drill is approximated by one tool-cursor (st)
at the tip and multiple tool-cursors (si, i ‚àà [0, 1, 2...]) placed along the shaft‚Äôs axis.
The tip‚Äôs tool-cursor is distinguished from the shaft tool-cursors, since only the tip of
the simulated drill can remove bone tissue. When the tip tool-cursor interacts with the
anatomy, the set of colliding voxels are retrieved and their intensity is set to 0. This
causes the volume to be ‚Äúdrilled‚Äù on the next rendering update. On the other hand,
if si interacts with the anatomy, the drill is prevented from drilling and penetrating
further. Alg. 2 summarizes our approach.

3. Evaluation and Discussion

In this section, we present evaluation on the data quality for algorithm development.
We will evaluate the surgical training components with subsequent user studies in our
future work.

9

stsiùúπùíôiDrill Mesh OriginDrill MeshAlgorithm 2 Drill Shaft Collision Algorithm
1: TD := Simulated drill‚Äôs pose
2: Ts := I4√ó4
3: (cid:126)emax ‚Üê (cid:126)0
4: st ‚Üê Tip tool-cursor
5: S ‚Üê Shaft tool-cursors [sn, ..., s1, s0]
6: Œ¥ (cid:126)xi := Fixed aligned distance between shaft tool-cursor si and st
7: for each si ‚àà S do
8:
9:

Œ¥(cid:126)pi ‚Üê compute error(si)
if ||Œ¥(cid:126)pi|| > ||emax|| then

(cid:46) I :=Identity Matrix

(cid:46) i ‚àà [0, 1, 2, ..]

10:

[ (cid:126)Ps, Rs] ‚Üê P ose(si.proxy)
Ts ‚Üê (cid:126)Ps + RsŒ¥ (cid:126)xi
smax ‚Üê si

end if
(cid:126)emax ‚Üê max((cid:126)emax, Œ¥(cid:126)pi)

11:
12:
13:
14:
15: end for
16: if ||(cid:126)emax|| > (cid:15) ‚àß smax (cid:54)= st then
17:

TD ‚Üê Ts
(cid:126)F ‚Üê control law((cid:126)emax)

18:
19: else
20:
21:

22:
23: end if

TD ‚Üê P ose(st.proxy)
remove colliding voxels()
(cid:126)F ‚Üê control law(compute error(st))

(cid:46) Error between proxy and goal

(cid:46) (cid:126)Ps := Position of si proxy
(cid:46) Rs := Rotation Matrix of si proxy

(cid:46) (cid:15) := A small value for numerical stability

(cid:46) (cid:126)F := Force feedback

(cid:46) Set the intensity of colliding voxels to zero

3.1. Geometric-based Computer Vision Algorithm - Anatomy Tracking

Vision-based pose tracking is a task where input images are processed to infer the
pose information of objects. It recently has found applications in Augmented Reality
(AR) and can potentially improve the safety of skull-base surgeries when used as a
navigation system (Mezger et al. (2013)). We demonstrate how the data generated
from the simulator can be used for evaluating vision-based tracking algorithms. In the
experiment, we use the state-of-the-art ORB SLAM V3 algorithm by Campos et al.
(2021). The algorithm extracts ORB features (Rublee et al. (2011)) from stereo images
and uses the matched features to compute the camera pose relative to the patient‚Äôs
anatomy based on geometric consistency. We refer interested readers to the paper for
more details. A video demonstration of ORB SLAM V3 running on our data can be
found in the supplementary material.

Since we have ground truth pose information available (c.f. subsection 2.3), we can
directly evaluate the accuracy of the algorithm outputs. The baseline of the generated
stereo images is set to 65 mm following stereo microscope design. We evaluate the
algorithm in two types of settings: 1) static tool with camera movement, and 2) static
camera with tool movement and anatomy modiÔ¨Åcation. In each setting, 500 frames
were recorded and used for evaluation. These sets of experiments can evaluate the
accuracy of ORB SLAM V3 when the camera moves and show if the tool movement
contaminates the pose tracking. We summarize the result in Table 1. While the ORB
SLAM V3 algorithm performs very robustly when only the drill moves, the tracking
accuracy degrades with camera motion. We plot the translation error in Figure 8. The
tracking error with camera movement is on the order of 40 mm for translation and
around 8 degrees for rotation, which is still impractical for clinical use directly (0.5
mm as in Schneider et al. (2020)). We also observed frequent tracking failures when
camera movement is large. This suggests future research opportunities of improving

10

(a)

(b)

Figure 8.
ization of depth prediction from STTR.

(a) Plot of estimated XYZ trajectory of anatomy pose tracking from ORB SLAM V3. (b) Visual-

tracking algorithms with microscopic data. The analysis with simulation generated
data can serve as a complement to image quality based analysis in Long et al. (2021)
using real images.

Table 1. Quantitative result (mean and standard deviation of L1 error) of ORB SLAM V3 applied on the
synthetic stereo microscopic data generated by the drilling simulator.

Translation Error (mm) Rotation Error (deg)

Moving camera
Moving drill

40.97 ¬± 22.40
8.1E-1 ¬± 9.1E-1

8.44 ¬± 3.07
3.2E-3 ¬± 3.6E-3

3.2. Deep Learning-based Computer Vision Algorithm - Depth Estimation

We demonstrate that data generated from our simulator can also be used for deep
learning algorithms. We conduct our experiment on the stereo depth estimation task,
where stereo images are used to densely estimate the depth of the scene. The moving
camera sequence is used as training data and the moving drill sequence is used as
testing data (cf. subsection 3.1). In the training data, the camera observes the un-
modiÔ¨Åed anatomy at diÔ¨Äerent locations. In the testing data, the virtual drill actively
changes the anatomy with a Ô¨Åxed camera location. We train the stereo depth network
STTR (Li et al. (2021)) for depth estimation from scratch with AdamW optimizer for
5 epochs with learning rate 1E-4. The testing L1 error of depth is 1.98 mm. The qual-
itative result is shown in Figure 8. The trained depth network generalizes to changes
of anatomy shape even if it is only trained with constant anatomical shape. The fully
controlled simulation environment makes such analysis easy.

We note that our evaluation only demonstrates weak generalization, i.e. we train
and evaluate the algorithm both in synthetic settings. The performance on real images
of the trained network can be limited due to the large domain gap between simulated
and real images.

11

4. Conclusion and Future Work

In this work, we report our progress towards a cost-eÔ¨Äective and synergistic frame-
work, AMBF+, unifying surgical training and data generation. It allows users to prac-
tice surgical procedures and generates relevant data in real-time simultaneously. The
framework is designed to be modular and can be extended for diÔ¨Äerent surgeries and
data. We have demonstrated one use case where a virtual skull-base surgery is simu-
lated. We have presented evaluation on data quality from the simulator and use cases
of the data in diÔ¨Äerent downstream computer vision tasks. Our future work includes
incorporating virtual Ô¨Åxture testing in the simulation environment (Li et al. (2020b),
Li et al. (2020a)) and a formal user study with surgeons and residents to evaluate the
eÔ¨Écacy of our simulator in surgical training. We also recognize the limitation of visual
realism and plan to explore learning-based style transfer (e.g., Rivoir et al. (2021)) to
mitigate the problem.

Acknowledgement

This work was supported by: 1) an agreement LCSR and MRC, 2) Cooperative Control
Robotics and Computer Vision: Development of Semi-Autonomous Temporal Bone
and Skull Base Surgery K08DC019708, 3) Intuitive research grant, 4) a research con-
tract from Galen Robotics, 5) Johns Hopkins University internal funds.

Disclosures

Russel H. Taylor is a paid consultant to Galen Robotics and has an equity interest
in that company. These arrangements have been reviewed and approved by JHU in
accordance with its conÔ¨Çict of interest policy.

References

Campos C, Elvira R, Rodr¬¥ƒ±guez JJG, Montiel JM, Tard¬¥os JD. 2021. Orb-slam3: An accurate
open-source library for visual, visual‚Äìinertial, and multimap slam. IEEE Transactions on
Robotics.

Cartucho J, Tukra S, Li Y, S Elson D, Giannarou S. 2020. Visionblender: a tool to eÔ¨Éciently
generate computer vision datasets for robotic surgery. Computer Methods in Biomechanics
and Biomedical Engineering: Imaging & Visualization.

Chan S, Li P, Locketz G, Salisbury K, Blevins NH. 2016. High-Ô¨Ådelity haptic and visual ren-
dering for patient-speciÔ¨Åc simulation of temporal bone surgery. Computer Assisted Surgery.
Chen RJ, Lu MY, Chen TY, Williamson DF, Mahmood F. 2021. Synthetic data in machine

learning for medicine and healthcare. Nature Biomedical Engineering.

Chen X, Sun P, Liao D. 2018. A patient-speciÔ¨Åc haptic drilling simulator based on virtual
reality for dental implant surgery. International journal of computer assisted radiology and
surgery. 13(11):1861‚Äì1870.

Conti F, Barbagli F, Morris D, Sewell C. 2005. Chai 3d: An open-source library for the rapid

development of haptic scenes. IEEE World Haptics. 38(1):21‚Äì29.

Coumans E. 2015. Bullet physics simulation. ACM SIGGRAPH 2015 Courses.
Ding AS, Lu A, Li Z, Galaiya D, Siewerdsen JH, Taylor RH, Creighton FX. 2021. Automated
registration-based temporal bone computed tomography segmentation for applications in
neurotologic surgery. Otolaryngology‚ÄìHead and Neck Surgery:01945998211044982.

12

George A, De R. 2010. Review of temporal bone dissection teaching: how it was, is and will

be. The Journal of Laryngology & Otology. 124(2):119‚Äì125.

Kruger J, Westermann R. 2003. Acceleration techniques for gpu-based volume rendering. In:

IEEE Visualization, 2003. VIS 2003. IEEE. p. 287‚Äì292.

Li Z, Gordon A, Looi T, Drake J, Forrest C, Taylor RH. 2020a. Anatomical mesh-based vir-
tual Ô¨Åxtures for surgical robots. In: 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE. p. 3267‚Äì3273.

Li Z, Liu X, Drenkow N, Ding A, Creighton FX, Taylor RH, Unberath M. 2021. Revisit-
ing stereo depth estimation from a sequence-to-sequence perspective with transformers. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision. p. 6197‚Äì6206.
Li Z, Shahbazi M, Patel N, O‚ÄôSullivan E, Zhang H, Vyas K, Chalasani P, Deguet A,
Gehlbach PL, Iordachita I, et al. 2020b. Hybrid robot-assisted frameworks for endomi-
croscopy scanning in retinal surgeries. IEEE transactions on medical robotics and bionics.
2(2):176‚Äì187.

Locketz GD, Lui JT, Chan S, Salisbury K, Dort JC, Youngblood P, Blevins NH. 2017.
Anatomy-speciÔ¨Åc virtual reality simulation in temporal bone dissection: perceived utility
and impact on surgeon conÔ¨Ådence. Otolaryngology‚ÄìHead and Neck Surgery.

Long Y, Li Z, Yee CH, Ng CF, Taylor RH, Unberath M, Dou Q. 2021. E-dssr: EÔ¨Écient dynamic
surgical scene reconstruction with transformer-based stereoscopic depth perception. arXiv
preprint arXiv:210700229.

Lui JT, Hoy MY. 2017. Evaluating the eÔ¨Äect of virtual reality temporal bone simulation on
mastoidectomy performance: a meta-analysis. Otolaryngology‚ÄìHead and Neck Surgery.
Mahmood F, Borders D, Chen RJ, McKay GN, Salimian KJ, Baras AS, Durr N. 2020. Deep
adversarial training for multi-organ nuclei segmentation in histopathology images. IEEE
Transactions on Medical Imaging. 39:3257‚Äì3267.

Mezger U, Jendrewski C, Bartels M. 2013. Navigation in surgery. Archives of surgery.
Munawar A, Wang Y, Gondokaryono R, Fischer GS. 2019. A real-time dynamic simulator
and an associated front-end representation format for simulating complex robots and en-
vironments. 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS):1875‚Äì1882.

Munawar A, Wu JY, Taylor RH, Kazanzides P, Fischer GS. 2021. A framework for customizable
multi-user teleoperated control. IEEE Robotics and Automation Letters. 6(2):3256‚Äì3263.
Rivoir D, PfeiÔ¨Äer M, Docea R, Kolbinger F, Riediger C, Weitz J, Speidel S. 2021. Long-term
temporally consistent unpaired video translation from simulated surgical 3d data. arXiv
preprint arXiv:210317204.

Rublee E, Rabaud V, Konolige K, Bradski G. 2011. Orb: An eÔ¨Écient alternative to sift or surf.

In: 2011 International conference on computer vision. Ieee. p. 2564‚Äì2571.

Ruspini D, Khatib O. 2001. Haptic display for human interaction with virtual dynamic envi-

ronments. Journal of Robotic Systems.

Schneider D, Hermann J, Mueller F, Braga GOB, Anschuetz L, Caversaccio M, Nolte L,
Weber S, Klenzner T. 2020. Evolution and stagnation of image guidance for surgery in the
lateral skull: A systematic review 1989‚Äì2020. Frontiers in surgery.

Unberath M, Zaech JN, Lee SC, Bier B, Fotouhi J, Armand M, Navab N. 2018. Deepdrr‚Äìa
catalyst for machine learning in Ô¨Çuoroscopy-guided procedures. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. Springer. p. 98‚Äì106.
Zisimopoulos O, Flouty E, Stacey M, Muscroft S, Giataganas P, Nehme J, Chow A, Stoy-
anov D. 2017. Can surgical simulation be used to train detection and classiÔ¨Åcation of neural
networks? Healthcare technology letters.

13

