1
2
0
2

v
o
N
5
1

]

O
R
.
s
c
[

1
v
7
9
0
8
0
.
1
1
1
2
:
v
i
X
r
a

Virtual Reality for Synergistic Surgical Training and Data
Generation

Adnan Munawara, Zhaoshuo Lia, Punit Kunjama, Nimesh Nagururua, Andy S.
Dinga, Peter Kazanzidesa, Thomas Looib, Francis X. Creightona, Russell H. Taylora
and Mathias Unberatha

aJohns Hopkins University, Baltimore, US;
bHospital for Sick Children, Toronto, Canada.

ARTICLE HISTORY
Compiled November 17, 2021

ABSTRACT
Surgical simulators not only allow planning and training of complex procedures,
but also oﬀer the ability to generate structured data for algorithm development,
which may be applied in image-guided computer assisted interventions. While there
have been eﬀorts on either developing training platforms for surgeons or data gen-
eration engines, these two features, to our knowledge, have not been oﬀered to-
gether. We present our developments of a cost-eﬀective and synergistic framework,
named Asynchronous Multibody Framework Plus (AMBF+), which generates data
for downstream algorithm development simultaneously with users practicing their
surgical skills. AMBF+ oﬀers stereoscopic display on a virtual reality (VR) device
and haptic feedback for immersive surgical simulation. It can also generate diverse
data such as object poses and segmentation maps. AMBF+ is designed with a ﬂex-
ible plugin setup which allows for unobtrusive extension for simulation of diﬀerent
surgical procedures. We show one use case of AMBF+ as a virtual drilling simulator
for lateral skull-base surgery, where users can actively modify the patient anatomy
using a virtual surgical drill. We further demonstrate how the data generated can
be used for validating and training downstream computer vision algorithms.∗

KEYWORDS
Surgical simulation; Virtual reality; Deep learning; Computer vision

1. Introduction

Eﬀective “training” is pertinent for both surgeons and image-guided intervention al-
gorithms. Thus, creating opportunities to learn from experience for both surgeons and
algorithms has become of substantial interest.

On one hand, surgeons can develop necessary surgical skills and 3D spatial percep-
tion by using surgical simulators with the beneﬁt of safety of a simulated world (Chan
et al. (2016), Lui and Hoy (2017), Locketz et al. (2017), Chen et al. (2018)). Beyond
improving general surgical skills, surgical simulators can improve outcomes in individ-
ual patients through the ability to generate surgical views from unique pre-operative
scans. More recently, Asynchronous Multi-Body Framework (AMBF) (Munawar et al.

Email: {amunawar, zli122}@jhu.edu

∗Code is available at https://github.com/LCSR-SICKKIDS/volumetric_drilling

 
 
 
 
 
 
(2019)), has recently been introduced to the ﬁeld and has been designed for ﬂuid
interaction with virtual surgical phantoms.

On the other hand, developing algorithms for image-guided computer-assisted sur-
gical interventions also requires “training”. It generally requires structured data either
for validation purposes used in geometric-based algorithms such as Simultaneous Lo-
calization and Mapping (SLAM); or for training supervision used in deep-learning
based algorithms, such as learning based segmentation (Zisimopoulos et al. (2017)).
Many prior work in synthetic medical data generation exist for diﬀerent modalities,
such as CT scans (Unberath et al. (2018)) and pathology slices (Mahmood et al. (2020))
as summarized in Chen et al. (2021). VisionBlender by Cartucho et al. (2020) uses
Blender1 to generate data for endoscopic surgeries, oﬀ-line, for development of com-
puter vision algorithms. The images generated are visually pleasing however motion
of camera and surgical instruments is only possible along pre-speciﬁed paths. Realistic
tool-tissue interaction and camera motion enable cost-eﬀective development or anal-
ysis for algorithms contingent on these information. For example, SurgVisDom2 is a
synthetic dataset with realistic tool motion designed for surgical activities recognition,
yet only designed for robotic laparoscopic surgery and not open-sourced.

Integrating the two components, i.e. generating data from realistic surgical simu-
lation with tool-tissue interaction in real-time, will oﬀer a synergistic, and therefore
cost-eﬀective solution to both problems. While surgeons practice their surgical skills
using the simulator, clinically relevant data are collected in a precisely controlled envi-
ronment for downstream development of computer vision algorithms. This can reduce
the material cost compared to cadaveric dissection (George and De (2010)) and the
need for a separate eﬀort of data generation. To this end, we introduce Asynchronous
Multi-Body Framework Plus (AMBF+), an open-sourced simulation framework that
combines the object interaction capabilities of AMBF with data generation capabil-
ities similar to those of VisionBlender. We also propose a plugin setup described in
subsection 2.1 to accommodate specialized functionalities for diﬀerent surgical pro-
cedures and diﬀerent data needed for downstream algorithms while keeping the core
simulation logic intact. By unifying the surgical training simulation and data genera-
tion into one single framework, we attempt to present a synergistic solution to both
human training and algorithm development. This framework overcomes the aforemen-
tioned limitations and allows surgeons to perform tasks similar to real surgeries while
generating/recording relevant information for vision algorithms.

To demonstrate the functionalities of AMBF+, we have developed a simulator for
skull-base surgery similar to Locketz et al. (2017). Skull-base surgery is a surgical
subspecialty in neurosurgery and otolaryngology for surgical intervention of pathology
within the base of the skull. During surgery, surgeons inspect the surgical ﬁeld via
surgical microscopes and use surgical drills to remove the bony tissue and gain access
to relevant anatomy. In our virtual drilling simulator as shown in Figure 1, users can
actively drill a patient model using input devices, while synthetic data is automati-
cally generated. We further demonstrate the utility of this generated data in diﬀerent
computer vision-based tasks.

The key aspects of our development can be summarized as follows:

• A virtual surgical simulation framework AMBF+ is provided to enable users to
interact with a patient model for surgical practice. The simulation provides both
visual and haptic feedback for assistance and realism.

1https://www.blender.org/
2https://surgvisdom.grand-challenge.org/

2

Figure 1. Overview of the virtual drilling simulator for skull-base surgery developed using AMBF+. A
segmented CT is rendered as a 3D virtual phantom in the simulator. Users send control input and receive haptic
feedback with Phantom OMNI3. Stereoscopic views are provided with Oculus Rift4 and the view direction is
synchronized with the user’s head.

• The AMBF+ framework can generate data (RGB stereo images, depth and etc.)
in real time while the user is performing virtual surgeries, which can be used for
computer vision algorithm development and training.

• We propose a plugin design to expand the AMBF+ framework without modiﬁ-
cation to the core source-code such that functionalities customized for diﬀerent
surgeries can be easily incorporated.

• We demonstrate one use case of AMBF+ – a virtual drilling simulator for skull-
base surgery. We show how the generated data can be potentially used in down-
stream tasks.

2. AMBF+ System Overview

We developed AMBF+ on top of AMBF (Munawar et al. (2019)), which uses libraries
such as CHAI3D (Conti et al. (2005)) and Bullet Physics (Coumans (2015)). AMBF+
oﬀers a convenient platform for surgical robot simulations as it allows for the creation of
kinematically redundant robots and mechanisms with ease. The framework utilizes an
AMBF Description Format (ADF) for deﬁning rigid and soft-bodies, sensors, actuators
and world scene objects. The ADF ﬁles are modular such that a model (consisting
of bodies, joints, sensors and actuators) can be deﬁned in multiple ADF ﬁles that
can be loaded together. The parenting of bodies, joints, sensors and actuators is also
modular which means that a prospective parent or child does not need to exist in the
same ADF ﬁle. This makes AMBF+ versatile for surgical robot related simulations as
surgical robots often have multiple interchangeable tools. The ADF ﬁles are of three
diﬀerent types: (1) world ﬁles, (2) input devices ﬁles (Munawar et al. (2021)) and (3)
model ﬁles. A meta-data ﬁle, called the launch ﬁle, is the entry point for AMBF+ and

3https://www.3dsystems.com/haptics-devices/touch
4https://www.oculus.com/facebook-horizon

3

Figure 2. Flow of loading models and world scene objects into AMBF+.

contains the ﬁlepaths of a world ﬁle, an input devices ﬁle and possibly several model
ﬁles. Fig. 2 describes the ﬂow of loading ADF ﬁles into AMBF+.

In AMBF+ we not only extended the rendering pipeline and the ADF speciﬁcation,
but we also implemented a modular plugin handling interface. The extension to the
rendering pipeline allowed us to generate and stream colored point-cloud data and
segmentation masks. Extension to the ADF speciﬁcation allowed us to deﬁne custom
OpenGL shaders for rendering and computation. The plugin handling interface allowed
the development of custom applications such as the volumetric-drilling simulation, thus
extending its feature set.

(a)

(b)

Figure 3.
each computational scope.

(a) The diﬀerent computational scopes of an AMBF+ simulation. (b) Corresponding plugins for

2.1. Plugin Design

An AMBF+ simulation comprises of diﬀerent hierarchical computational scopes as
shown in Fig. 3(a), where each inner block is encapsulated by the outer block to rep-
resent a higher scope in hierarchy. A higher scope has access to more resources and
control over the simulation components as compared to the lower ones. The highest

4

Launch File (launch.yaml)World FileModel FilesModel FilesModel FilesInput Devices FileWorld File (world.yaml)World Attributes:Env Model FileModel File(model.yaml)Object Attribs:Object Attribs:Object Attribs:Model ShadersObject Attribs:Object Type:Object Specific Attribs:Object ShadersPlugin FilepathsWorld Attribs:Solver Attribs:Gravity:Dimensions:World ShadersPlugin FilepathsPlugin FilepathsPlugin FilepathsafSimulatorafWorldafModel 1afObject 1 (body)afObject 2 (sensor)afObject 3 (joint)afModel 2afObject 1 (camera)afObject 2 (light)afObject 3 (vehicle)afWorldPluginafModelPluginafObjectPluginafSimulatorPluginFigure 4. Interfaces provided by the diﬀerent types of plugins.

scope is a single afSimulator instance that manages an afWorld instance. The afWorld
may contain instances of afModels and each afModel may contain several instances of
afObjects. The term afObject refers to simulation entities such as cameras, lights, bod-
ies (rigid and soft), volumes, sensors, and actuators as shown in Fig. 5. The afObjects
can be further classiﬁed into visual and non-visual objects. Rigid-bodies, soft-bodies,
and volumes are types of visual objects.

We developed plugin interfaces for each AMBF+ simulation scope (Fig. 3(b)). Each
type of plugin has a slightly diﬀerent interface, as shown in Fig. 4, and thus extends
the functionality of the scope that it is deﬁned for.

Since the entry to AMBF+ is via the launch ﬁle (Fig. 2), we extended the ADF
format to incorporate the plugin speciﬁcation. Plugins can be deﬁned for afSimulator,
afWorld and afObject instances by setting them in the appropriate ADF ﬁle and/or
scope. Our extension to the ADF speciﬁcation is backward compatible, as older ver-
sions of AMBF would simply ignore the plugins data-ﬁeld.

2.2. Rendering Pipeline

AMBF uses OpenGL5 for rendering but provides limited access to some of its useful
features such as loading custom shaders (vertex and fragment). In AMBF+, we ex-
tended AMBF to allow the speciﬁcation of these shaders via the ADF ﬁles. Similar to
plugins, shaders can be deﬁned in the world ADF ﬁle, the model ADF ﬁles, and for
individual afObjects as shown in Fig. 2. The shaders are then applied to all afObjects,
afObjects in the speciﬁc model or only to the speciﬁc afObject depending upon the
scope that they are speciﬁed in. Furthermore, if shaders are deﬁned at multiple scopes,
the afObject deﬁned shaders override the model deﬁned shaders and the model deﬁned
shaders override the world deﬁned shaders.

5https://www.opengl.org/
6https://www.khronos.org/opengl/wiki/Depth Test
7https://www.khronos.org/opengl/wiki/Framebuﬀer Object

5

AMBF OBJECT PLUGIN INTERFACEclose()init(afBaseObj*, objAttribs)gfxUpdate(afObject*)phxUpdate(afObject*)AMBF MODEL PLUGIN INTERFACEclose()init(afModel*, modelAttribs)onObjRemoval(afBaseObj*)onObjAdd(afBaseObj*)gfxUpdate(afModel*)phxUpdate(afModel*)AMBF WORLD PLUGIN INTERFACEclose()init(afWorld*, worldAttribs)onModelAdd(afModel*)onModelRemoval(afModel*)onObjRemoval(afBaseObj*)onObjAdd(afBaseObj*)gfxUpdate(afWorld*)phxUpdate(afWorld*)AMBF SIMULATOR PLUGIN INTERFACEgfxUpdate(afWorld*)init(argc, argv)phxUpdate(afWorld*)keyboardCB()mouseCB()windowResizeCB()close()(a)(b)(c)(d)2.2.1. Point Cloud Generation

We utilized OpenGL’s Z-buﬀer6 and Framebuﬀer objects7 to generate the point cloud
data. Our application utilizes perspective cameras from OpenGL, which induce non-
linearity in the generated depth values. The non-linear depth values need to be lin-
earized. This step can be performed on the GPU for faster processing, (in the fragment
shader (Alg. 1)) however the values must also be normalized to avoid truncation by the
fragment shader’s output. Finally, on the CPU, these normalized values are rescaled
to get the correct point could. The maximum dimensions ( (cid:126)M D) of the view frustum
are used for both the normalization and scaling. The vector (cid:126)M D is calculated as:

Figure 5. Diﬀerent types of afObjects.

M Dx = 2.0 × f × tan(f va/2); M Dy = M Dx × AR; M Dz = f − n

(1)

Where n, f , f va and AR correspond to the simulated camera’s near plane, far plane,

ﬁeld view angle and aspect ratio, respectively.

Algorithm 1 Depth Computation Shader
1: Fx, Fy ← F ragCoord.xy
2: B0, B1, B2, B3 ← T exture2D(Fx, Fy)
3: Z ← (B3 (cid:28) 24) ∨ (B2 (cid:28) 16) ∨ (B1 (cid:28) 8) ∨ B0
4: Fz ← Z/224
5: Pnorm ← [Fx, Fy, Fz, 1.0]
6: Pclip ← (P rojectionM atrix)−1Pnorm
7: Pcam ← Pclip/Pclip.w
8: Nxy ← (Pcam.xy + (cid:126)M Dxy/2.0)/ (cid:126)M Dxy
9: Nz ← (Pcam.z − n)/(f − n)
10: F ragOutput ← [Nx, Ny, Nz, 1.0]

(cid:46) Input to the Shader
(cid:46) Depth packed in 4 one-byte channels
(cid:46) Bit Shifting and Logical OR

(cid:46) Projection Matrix of Simulated Camera

2.2.2. Segmentation Mask Generation

AMBF+’s rendering pipeline comprises of four passes which are shown in Fig 6(a).
Three passes are implemented on the GPU and the fourth one on the CPU. The ﬁrst
pass generates a 2 dimensional color image that is output to the screen. The second
pass renders to the color and depth Framebuﬀers. The values in the depth-buﬀer are
non-linear at this point. Prior to the third pass, the depth computation shaders are
loaded. These shaders are responsible for linearizing and normalizing the depth values

6

Rigid BodySoft BodyVolumeSensorActuatorJointCameraLightBase ObjectVisual ObjectsNon-Visual ObjectsVehicle(Alg. 1) on the third rendering pass. The fourth pass re-scales the normalized depth
values and computes camera space transformation of the point cloud data. During this
pass, the color Framebuﬀer generated from the second pass is also superimposed on
the point cloud to generate a colored point cloud as shown in Fig. 6(b).

We utilize the custom shader speciﬁcation, via the ADF ﬁles, to load special pre-
processing shaders for visual objects prior to the second rendering pass. The sole
purpose of these shaders is to assign a speciﬁc color to the visual object while ignor-
ing the lighting calculations. The result is a segmentation mask image that can be
superimposed on the point cloud in the fourth pass as shown in Fig. 6(b).

(a)

(b)

Figure 6.
of the segmentation mask on the point cloud to generate labeled data.

(a) Diﬀerent passes in the rendering pipeline to generate a segmented point cloud. (b) Augmentation

2.3. Data Streaming and Recording

We use the Robot Operating System (ROS)8 interface due to its wide usage in the
surgical robotics community. We stream out the data generated from AMBF+ as ROS
topics. Such design allows for replay of surgical operation, easy retrieval and eﬃcient
storage.

• Stereo images: Stereo images (left and right) are published via a ROS image
topic by AMBF+ at a user speciﬁed frequency. The left and right stereo images
are produced by two diﬀerent cameras initialized in AMBF+. By manipulating
the OpenGL parameters of the two cameras, location and look at, the stereo view
can be set up in a highly customizable fashion. Location is used to establish the
stereo baseline and look at is used to align the principle axes of the left and right
cameras.

• Depth: A depth point cloud is created as described in Section 2.3.1.
• Segmentation mask: The ground truth segmentation mask is created as described

in Section 2.3.2.

• Object/camera pose: Object/camera pose contains the xyz position and rotation
(either quaternion or Euler angles) with respect to the world frame. Diﬀerent
objects and cameras may be added to the AMBF+ scene and can be tracked
and controlled by AMBF+ and its Python client.

8https://www.ros.org/

7

Per Camera Rendering LoopRender to ScreenRender Color & Depth to FrameBuffersLoad/Use Preprocessing ShadersCopy Depth Buffer and Superimpose ColorLoad Depth Computation ShadersRender using Depth Computation Shader1.2.3.4.Exec. OrderScreen Output1st PassDepth Data3rd PassDepth Data + Scene Color4th PassDepth Data + Seg. Mask4th Pass• Camera parameters: The intrinsic parameters of cameras in AMBF+ are es-
tablished in OpenGL fashion using the vertical ﬁeld view angle (f va) which
describes the perspective frustum. The intrinsic matrix (I) can be calculated
through the following equations:

I =





fx
0
0

0
fy
0



 ;

cx
cy
1

fx = fy =

H

2 × tan

(cid:16) f va
2

(cid:17) ;

cx = W/2;

cy = H/2 (2)

Where fx and fy are the focal length in pixels in the x and y directions. W and
H are the width and height of the image produced in pixels. cx and cy describe
the x and y position of the camera principal point. Extrinsics information is
obtained from the relative transformation between the two cameras.

2.4. Stereo Display and VR Support

Depth perception is a requirement for a simulation environment targeting surgeries.
We generate a pair of stereoscopic images that are then displayed on a Virtual Reality
(VR) head-mounted display (HMD). We also support the orientation control of the
virtual cameras based on the HMD’s angular movements as a means of looking around
in the simulation scene. We rely on the OpenHMD9 package for HMD support and
tested our implementation using an Occulus Rift VR device.

2.5. Haptic Feedback

AMBF+ utilizes CHAI3D’s (Conti et al. (2005)) ﬁnger proxy collision algorithm (Rus-
pini and Khatib (2001)) to provide haptic feedback of interaction with surface meshes
and volumes. Tool-cursors, where each tool-cursor encapsulates a pair of proxy/goal
spheres, are used for feedback calculation. The movement of the tool-cursor moves the
encapsulated proxy/goal spheres such that the proxy sphere is stopped at the bound-
ary of the surface or volume while the goal sphere coincides with the tool-cursor’s
pose, and is thus allowed to penetrate inside. The distance between the proxy and the
goal spheres (δpi) for each tool-cursor can then be used to generate a proportional
force feedback.

2.6. Use-case of AMBF+ – Virtual Drilling Simulator

To demonstrate the ﬂexibility of AMBF+, a drilling simulator is developed that lever-
ages the aforementioned functionalities. The scheme of the simulator is summarized
in Figure 1. We customize the following functionalities using the plugin design: 1)
customized data representation from a pre-operative CT scan, which will be rendered
at frame-rate, 2) virtual drilling with input from a haptic device (Phantom OMNI or
Oculus Rift in our setup). A video demonstration of our simulation scene is provided
in the supplementary material.

9https://github.com/OpenHMD/OpenHMD

8

2.6.1. Virtual Model Representation

The virtual patient model is obtained from a CT scan and is patient speciﬁc. We use an
oﬀ-the-shelf CT segmentation pipeline (Ding et al. (2021)) to generate the correspond-
ing segmented volume for each anatomical structure in 3D. The segmented volume is
used for three purposes: 1) to texture each anatomy with its corresponding anatomical
color, 2) to diﬀerentiate skull and other critical anatomies for visual warnings, and 3)
to generate 2D segmentation masks.

We use an array of 2-dimensional images (in PNG or JPEG format) for texture-
based volumetric rendering (Kruger and Westermann (2003)) and drilling. These im-
ages combined represent the 3D volume, with each color denoting a diﬀerent anatomy.
We provide utility scripts that can convert the data from the NRRD format to an
array of images. This allows the segmented volumetric data to be imported for use
within the virtual drilling simulator.

2.6.2. Virtual Drilling and Haptic Feedback

(a)

(b)

Figure 7.
and shaft tool-cursors. The origin of the drill mesh coincides with the center of st.

(a) Mesh model of the simulated drill. (b) Collision approximation of the simulated drill with tip

Our drilling simulator allows the users to drill the volumetric data (anatomy) with
a haptic device which is represented as a simulated drill, as shown in Fig. 7. The
CHAI3D (Conti et al. (2005)) library is used for haptic feedback logic. For a more
accurate collision response, the simulated drill is approximated by one tool-cursor (st)
at the tip and multiple tool-cursors (si, i ∈ [0, 1, 2...]) placed along the shaft’s axis.
The tip’s tool-cursor is distinguished from the shaft tool-cursors, since only the tip of
the simulated drill can remove bone tissue. When the tip tool-cursor interacts with the
anatomy, the set of colliding voxels are retrieved and their intensity is set to 0. This
causes the volume to be “drilled” on the next rendering update. On the other hand,
if si interacts with the anatomy, the drill is prevented from drilling and penetrating
further. Alg. 2 summarizes our approach.

3. Evaluation and Discussion

In this section, we present evaluation on the data quality for algorithm development.
We will evaluate the surgical training components with subsequent user studies in our
future work.

9

stsi𝜹𝒙iDrill Mesh OriginDrill MeshAlgorithm 2 Drill Shaft Collision Algorithm
1: TD := Simulated drill’s pose
2: Ts := I4×4
3: (cid:126)emax ← (cid:126)0
4: st ← Tip tool-cursor
5: S ← Shaft tool-cursors [sn, ..., s1, s0]
6: δ (cid:126)xi := Fixed aligned distance between shaft tool-cursor si and st
7: for each si ∈ S do
8:
9:

δ(cid:126)pi ← compute error(si)
if ||δ(cid:126)pi|| > ||emax|| then

(cid:46) I :=Identity Matrix

(cid:46) i ∈ [0, 1, 2, ..]

10:

[ (cid:126)Ps, Rs] ← P ose(si.proxy)
Ts ← (cid:126)Ps + Rsδ (cid:126)xi
smax ← si

end if
(cid:126)emax ← max((cid:126)emax, δ(cid:126)pi)

11:
12:
13:
14:
15: end for
16: if ||(cid:126)emax|| > (cid:15) ∧ smax (cid:54)= st then
17:

TD ← Ts
(cid:126)F ← control law((cid:126)emax)

18:
19: else
20:
21:

22:
23: end if

TD ← P ose(st.proxy)
remove colliding voxels()
(cid:126)F ← control law(compute error(st))

(cid:46) Error between proxy and goal

(cid:46) (cid:126)Ps := Position of si proxy
(cid:46) Rs := Rotation Matrix of si proxy

(cid:46) (cid:15) := A small value for numerical stability

(cid:46) (cid:126)F := Force feedback

(cid:46) Set the intensity of colliding voxels to zero

3.1. Geometric-based Computer Vision Algorithm - Anatomy Tracking

Vision-based pose tracking is a task where input images are processed to infer the
pose information of objects. It recently has found applications in Augmented Reality
(AR) and can potentially improve the safety of skull-base surgeries when used as a
navigation system (Mezger et al. (2013)). We demonstrate how the data generated
from the simulator can be used for evaluating vision-based tracking algorithms. In the
experiment, we use the state-of-the-art ORB SLAM V3 algorithm by Campos et al.
(2021). The algorithm extracts ORB features (Rublee et al. (2011)) from stereo images
and uses the matched features to compute the camera pose relative to the patient’s
anatomy based on geometric consistency. We refer interested readers to the paper for
more details. A video demonstration of ORB SLAM V3 running on our data can be
found in the supplementary material.

Since we have ground truth pose information available (c.f. subsection 2.3), we can
directly evaluate the accuracy of the algorithm outputs. The baseline of the generated
stereo images is set to 65 mm following stereo microscope design. We evaluate the
algorithm in two types of settings: 1) static tool with camera movement, and 2) static
camera with tool movement and anatomy modiﬁcation. In each setting, 500 frames
were recorded and used for evaluation. These sets of experiments can evaluate the
accuracy of ORB SLAM V3 when the camera moves and show if the tool movement
contaminates the pose tracking. We summarize the result in Table 1. While the ORB
SLAM V3 algorithm performs very robustly when only the drill moves, the tracking
accuracy degrades with camera motion. We plot the translation error in Figure 8. The
tracking error with camera movement is on the order of 40 mm for translation and
around 8 degrees for rotation, which is still impractical for clinical use directly (0.5
mm as in Schneider et al. (2020)). We also observed frequent tracking failures when
camera movement is large. This suggests future research opportunities of improving

10

(a)

(b)

Figure 8.
ization of depth prediction from STTR.

(a) Plot of estimated XYZ trajectory of anatomy pose tracking from ORB SLAM V3. (b) Visual-

tracking algorithms with microscopic data. The analysis with simulation generated
data can serve as a complement to image quality based analysis in Long et al. (2021)
using real images.

Table 1. Quantitative result (mean and standard deviation of L1 error) of ORB SLAM V3 applied on the
synthetic stereo microscopic data generated by the drilling simulator.

Translation Error (mm) Rotation Error (deg)

Moving camera
Moving drill

40.97 ± 22.40
8.1E-1 ± 9.1E-1

8.44 ± 3.07
3.2E-3 ± 3.6E-3

3.2. Deep Learning-based Computer Vision Algorithm - Depth Estimation

We demonstrate that data generated from our simulator can also be used for deep
learning algorithms. We conduct our experiment on the stereo depth estimation task,
where stereo images are used to densely estimate the depth of the scene. The moving
camera sequence is used as training data and the moving drill sequence is used as
testing data (cf. subsection 3.1). In the training data, the camera observes the un-
modiﬁed anatomy at diﬀerent locations. In the testing data, the virtual drill actively
changes the anatomy with a ﬁxed camera location. We train the stereo depth network
STTR (Li et al. (2021)) for depth estimation from scratch with AdamW optimizer for
5 epochs with learning rate 1E-4. The testing L1 error of depth is 1.98 mm. The qual-
itative result is shown in Figure 8. The trained depth network generalizes to changes
of anatomy shape even if it is only trained with constant anatomical shape. The fully
controlled simulation environment makes such analysis easy.

We note that our evaluation only demonstrates weak generalization, i.e. we train
and evaluate the algorithm both in synthetic settings. The performance on real images
of the trained network can be limited due to the large domain gap between simulated
and real images.

11

4. Conclusion and Future Work

In this work, we report our progress towards a cost-eﬀective and synergistic frame-
work, AMBF+, unifying surgical training and data generation. It allows users to prac-
tice surgical procedures and generates relevant data in real-time simultaneously. The
framework is designed to be modular and can be extended for diﬀerent surgeries and
data. We have demonstrated one use case where a virtual skull-base surgery is simu-
lated. We have presented evaluation on data quality from the simulator and use cases
of the data in diﬀerent downstream computer vision tasks. Our future work includes
incorporating virtual ﬁxture testing in the simulation environment (Li et al. (2020b),
Li et al. (2020a)) and a formal user study with surgeons and residents to evaluate the
eﬃcacy of our simulator in surgical training. We also recognize the limitation of visual
realism and plan to explore learning-based style transfer (e.g., Rivoir et al. (2021)) to
mitigate the problem.

Acknowledgement

This work was supported by: 1) an agreement LCSR and MRC, 2) Cooperative Control
Robotics and Computer Vision: Development of Semi-Autonomous Temporal Bone
and Skull Base Surgery K08DC019708, 3) Intuitive research grant, 4) a research con-
tract from Galen Robotics, 5) Johns Hopkins University internal funds.

Disclosures

Russel H. Taylor is a paid consultant to Galen Robotics and has an equity interest
in that company. These arrangements have been reviewed and approved by JHU in
accordance with its conﬂict of interest policy.

References

Campos C, Elvira R, Rodr´ıguez JJG, Montiel JM, Tard´os JD. 2021. Orb-slam3: An accurate
open-source library for visual, visual–inertial, and multimap slam. IEEE Transactions on
Robotics.

Cartucho J, Tukra S, Li Y, S Elson D, Giannarou S. 2020. Visionblender: a tool to eﬃciently
generate computer vision datasets for robotic surgery. Computer Methods in Biomechanics
and Biomedical Engineering: Imaging & Visualization.

Chan S, Li P, Locketz G, Salisbury K, Blevins NH. 2016. High-ﬁdelity haptic and visual ren-
dering for patient-speciﬁc simulation of temporal bone surgery. Computer Assisted Surgery.
Chen RJ, Lu MY, Chen TY, Williamson DF, Mahmood F. 2021. Synthetic data in machine

learning for medicine and healthcare. Nature Biomedical Engineering.

Chen X, Sun P, Liao D. 2018. A patient-speciﬁc haptic drilling simulator based on virtual
reality for dental implant surgery. International journal of computer assisted radiology and
surgery. 13(11):1861–1870.

Conti F, Barbagli F, Morris D, Sewell C. 2005. Chai 3d: An open-source library for the rapid

development of haptic scenes. IEEE World Haptics. 38(1):21–29.

Coumans E. 2015. Bullet physics simulation. ACM SIGGRAPH 2015 Courses.
Ding AS, Lu A, Li Z, Galaiya D, Siewerdsen JH, Taylor RH, Creighton FX. 2021. Automated
registration-based temporal bone computed tomography segmentation for applications in
neurotologic surgery. Otolaryngology–Head and Neck Surgery:01945998211044982.

12

George A, De R. 2010. Review of temporal bone dissection teaching: how it was, is and will

be. The Journal of Laryngology & Otology. 124(2):119–125.

Kruger J, Westermann R. 2003. Acceleration techniques for gpu-based volume rendering. In:

IEEE Visualization, 2003. VIS 2003. IEEE. p. 287–292.

Li Z, Gordon A, Looi T, Drake J, Forrest C, Taylor RH. 2020a. Anatomical mesh-based vir-
tual ﬁxtures for surgical robots. In: 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE. p. 3267–3273.

Li Z, Liu X, Drenkow N, Ding A, Creighton FX, Taylor RH, Unberath M. 2021. Revisit-
ing stereo depth estimation from a sequence-to-sequence perspective with transformers. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision. p. 6197–6206.
Li Z, Shahbazi M, Patel N, O’Sullivan E, Zhang H, Vyas K, Chalasani P, Deguet A,
Gehlbach PL, Iordachita I, et al. 2020b. Hybrid robot-assisted frameworks for endomi-
croscopy scanning in retinal surgeries. IEEE transactions on medical robotics and bionics.
2(2):176–187.

Locketz GD, Lui JT, Chan S, Salisbury K, Dort JC, Youngblood P, Blevins NH. 2017.
Anatomy-speciﬁc virtual reality simulation in temporal bone dissection: perceived utility
and impact on surgeon conﬁdence. Otolaryngology–Head and Neck Surgery.

Long Y, Li Z, Yee CH, Ng CF, Taylor RH, Unberath M, Dou Q. 2021. E-dssr: Eﬃcient dynamic
surgical scene reconstruction with transformer-based stereoscopic depth perception. arXiv
preprint arXiv:210700229.

Lui JT, Hoy MY. 2017. Evaluating the eﬀect of virtual reality temporal bone simulation on
mastoidectomy performance: a meta-analysis. Otolaryngology–Head and Neck Surgery.
Mahmood F, Borders D, Chen RJ, McKay GN, Salimian KJ, Baras AS, Durr N. 2020. Deep
adversarial training for multi-organ nuclei segmentation in histopathology images. IEEE
Transactions on Medical Imaging. 39:3257–3267.

Mezger U, Jendrewski C, Bartels M. 2013. Navigation in surgery. Archives of surgery.
Munawar A, Wang Y, Gondokaryono R, Fischer GS. 2019. A real-time dynamic simulator
and an associated front-end representation format for simulating complex robots and en-
vironments. 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS):1875–1882.

Munawar A, Wu JY, Taylor RH, Kazanzides P, Fischer GS. 2021. A framework for customizable
multi-user teleoperated control. IEEE Robotics and Automation Letters. 6(2):3256–3263.
Rivoir D, Pfeiﬀer M, Docea R, Kolbinger F, Riediger C, Weitz J, Speidel S. 2021. Long-term
temporally consistent unpaired video translation from simulated surgical 3d data. arXiv
preprint arXiv:210317204.

Rublee E, Rabaud V, Konolige K, Bradski G. 2011. Orb: An eﬃcient alternative to sift or surf.

In: 2011 International conference on computer vision. Ieee. p. 2564–2571.

Ruspini D, Khatib O. 2001. Haptic display for human interaction with virtual dynamic envi-

ronments. Journal of Robotic Systems.

Schneider D, Hermann J, Mueller F, Braga GOB, Anschuetz L, Caversaccio M, Nolte L,
Weber S, Klenzner T. 2020. Evolution and stagnation of image guidance for surgery in the
lateral skull: A systematic review 1989–2020. Frontiers in surgery.

Unberath M, Zaech JN, Lee SC, Bier B, Fotouhi J, Armand M, Navab N. 2018. Deepdrr–a
catalyst for machine learning in ﬂuoroscopy-guided procedures. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. Springer. p. 98–106.
Zisimopoulos O, Flouty E, Stacey M, Muscroft S, Giataganas P, Nehme J, Chow A, Stoy-
anov D. 2017. Can surgical simulation be used to train detection and classiﬁcation of neural
networks? Healthcare technology letters.

13

