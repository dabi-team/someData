Unsupervised Flow-Aligned
Sequence-to-Sequence Learning for Video Restoration

Jing Lin * 1 Xiaowan Hu * 1 Yuanhao Cai 1 Haoqian Wang† 1
Youliang Yan 2 Xueyi Zou† 2 Yulun Zhang 3 Luc Van Gool 3

2
2
0
2

n
u
J

6
1

]

V
C
.
s
c
[

2
v
5
9
1
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

How to properly model the inter-frame relation
within the video sequence is an important but
unsolved challenge for video restoration (VR).
In this work, we propose an unsupervised ﬂow-
aligned sequence-to-sequence model (S2SVR)
to address this problem. On the one hand, the
sequence-to-sequence model, which has proven
capable of sequence modeling in the ﬁeld of natu-
ral language processing, is explored for the ﬁrst
time in VR. Optimized serialization modeling
shows potential in capturing long-range depen-
dencies among frames. On the other hand, we
equip the sequence-to-sequence model with an
unsupervised optical ﬂow estimator to maximize
its potential. The ﬂow estimator is trained with
our proposed unsupervised distillation loss, which
can alleviate the data discrepancy and inaccurate
degraded optical ﬂow issues of previous ﬂow-
based methods. With reliable optical ﬂow, we can
establish accurate correspondence among multi-
ple frames, narrowing the domain difference be-
tween 1D language and 2D misaligned frames
and improving the potential of the sequence-to-
sequence model. S2SVR shows superior perfor-
mance in multiple VR tasks, including video de-
blurring, video super-resolution, and compressed
video quality enhancement. https://github.
com/linjing7/VR-Baseline

1. Introduction

Video restoration (VR) aims to reconstruct high-quality
(HQ) video from its degraded low-quality (LQ) counterpart,
including video deblurring (Xiang et al., 2020), video super-

*Equal contribution 1Shenzhen International Graduate School,
Tsinghua University 2Huawei Noah’s Ark Lab 3ETH Zürich. Cor-
respondence to: Xueyi Zou <zouxueyi@huawei.com>, Haoqian
Wang <wanghaoqian@tsinghua.edu.cn>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Figure 1. Optical ﬂow estimated from LQ and HQ videos respec-
tively (top), and visual comparison of the aligned frames (bottom).

resolution (SR) (Sajjadi et al., 2018; Chan et al., 2021), and
compressed video enhancement (Guan et al., 2019).

Task-driven networks often have complex structures that
are elaborately designed for a speciﬁc task. These methods
may be inapplicable when transferred to a new scenario
or a different video restoration task (Yang et al., 2018b;
Deng et al., 2020; Cao et al., 2021). Therefore, it is of great
signiﬁcance to explore a uniﬁed and versatile framework
that can be used for multiple video restoration tasks.

Early works simply extend single image restoration (Dai
et al., 2015; Shahar et al., 2011; Liao et al., 2015; Cai et al.,
2021) to video restoration. These image-based methods ig-
nore inter-frame correlation, leading to limited performance.
Some CNN-based methods (Wang et al., 2019; Pan et al.,
2020; Deng et al., 2020) utilize information from the frames
within a short temporal window. The ignorance of distant
frames signiﬁcantly limits the performance of these meth-
ods. Some researchers use the recurrent neural network
(RNN) (Isobe et al., 2020; Yang et al., 2019; Zhong et al.,
2020; Chan et al., 2021) to propagate the hidden state in the
time domain to expand the temporal receptive ﬁeld. How-
ever, as analyzed in (Jozefowicz et al., 2015), RNN suffers
from both exploding and vanishing gradients. As a result,
RNN is difﬁcult to learn the long-term dependencies and
can not be stacked into very deep models, limiting the repre-
sentation capacity of restoration network. The transformer-
based model (Cao et al., 2021; Lin et al., 2022a) can process
a video sequence in parallel with self-attention mechanism.
Nonetheless, the model complexity is quadratic to the num-
ber of tokens. For video restoration with an immense num-
ber of tokens, modeling long-range dependencies means
huge computational costs and memory occupation. Thus,

LQ FlowLQ-AlignedLQ FlowLQ-AlignedHQ FlowHQ-AlignedHQ FlowHQ-Aligned 
 
 
 
 
 
Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

the problem of modeling long-term inter-frame relations
with an affordable cost remains formidable.

Based on the sequence nature of videos, our insight into
this problem is to treat it as a sequence modeling task
and try to solve it with the sequence-to-sequence (seq2seq)
model. Seq2seq model has proven capable of sequence mod-
eling (Sutskever et al., 2014; Chopra et al., 2016; Ott et al.,
2018; Chen et al., 2018) in the ﬁeld of natural language
processing (NLP), showing great potential in modeling the
inter-frame relation within the video sequence. Seq2seq
model is devised to serially encode the input sequence into
latent vectors and then dynamically decode a target sequence
out of that representations. However, the migration of the
seq2seq model is inevitably hindered by the domain discrep-
ancy between NLP and VR. The video signal is composed
of multiple misaligned 2D frames, while the seq2seq model
can only handle continuous 1D input (e.g., , language se-
quence, time series) canonically. So we need to establish
accurate correspondences among multiple frames by per-
forming a spatial alignment with optical ﬂow estimator.

Previous ﬂow-based (Wedel et al., 2009; Sun et al., 2018;
Teed & Deng, 2020) methods perform spatial alignment
with a pretrained optical ﬂow network. (Chan et al., 2021)
prove that feature alignment, i.e., estimating optical ﬂow
from the LQ videos and using it to warp the hidden state,
can yield a better restoration result than image alignment.
However, these ﬂow-based methods may be suboptimal and
suffer from the following issues: Firstly, the data discrep-
ancy between synthetic ﬂow dataset and real-world video
affects the performance of the pretrained optical ﬂow mod-
ule in VR. Secondly, the optical ﬂow estimated from the
LQ input video (LQ ﬂow) may be unreliable since the video
degradation may seriously distort video contents and break
pixel-wise correspondences between frames (Zheng et al.,
2021). As shown in Fig. 1, the LQ ﬂows lose some motion
details, and the frames aligned by the LQ ﬂows (LQ-aligned
frames) contain blurry edges. In contrast, the HQ ﬂow is
more detailed, and the HQ-aligned frames contain sharper
semantics. Besides, for feature alignment, the motion infor-
mation estimated from the LQ video may be inconsistent
with that of the hidden state, which is expected to be spa-
tially aligned with the HQ video. So some artifacts will be
brought when the LQ ﬂow is used for feature alignment.

We attempt to address the data discrepancy and inaccurate
LQ ﬂow issues with unsupervised distillation optical ﬂow
loss. To be speciﬁc, we train an optical ﬂow estimator on the
VR dataset with unsupervised loss. The data discrepancy
naturally disappear since the training and testing dataset
both come from the real-world VR dataset. Furthermore,
a novel data distillation loss is designed to generate more
accurate LQ ﬂows, in which the optical ﬂows estimated
from the HQ video serve as the pseudo-labels of the LQ

ﬂows. This loss encourages the LQ ﬂows to imitate the HQ
ﬂows, which are more accurate and spatial consistent with
the motion information of the hidden state.

Therefore,
the unsupervised ﬂow-aligned sequence-to-
sequence model is proposed for video restoration tasks
(S2SVR). We migrate and improve the seq2seq model from
NLP to VR task, and maximize the potential of the seq2seq
model with an unsupervised optical ﬂow estimator. In a
nutshell, our contributions can be summarized as follows:

• This is the ﬁrst VR work to explore the sequence-to-
sequence model, which comes from NLP and is intrin-
sically suitable for video sequence modeling.

• The proposed unsupervised distillation optical ﬂow
loss alleviates the data discrepancy and inaccurate LQ
ﬂow issues of previous ﬂow-based methods, narrowing
the domain difference between NLP and VR.

• Extensive experiments show that our method achieves
state-of-the-art performance in three typical video
restoration tasks, including video deblurring, video
super-resolution, and compressed video enhancement.

2. Related Work

2.1. Video Restoration

Early work (Takeda et al., 2009; Shahar et al., 2011; Dai
et al., 2015) adopt an image restoration model for video
restoration and do not take advantage of information in
the neighbouring frames. The ignorance of the inter-frame
correlation severely limits the restoration result. Some CNN-
based methods (Deng et al., 2020; Tian et al., 2020) employ
deformable convolution to perform feature-level alignment.
The RNN-based methods design the recurrent structure and
attempt to model the long-term dependencies by propagat-
ing the hidden state (Isobe et al., 2020; Yang et al., 2019;
Zhong et al., 2020).
(Chan et al., 2021) prove that the
combination of bidirectional propagation and optical ﬂow
estimation can achieve ideal results. (Deng et al., 2021) pro-
pose a recurrent model with separable-patch architecture
and multi-scale integration scheme for fast and accurate
video deblurring. However, the RNN-based methods inevi-
dently suffer from the vanishing gradient problem and have
difﬁculty in capturing the long-range temporal dependencies.
Recently, the emerging Transformer model has been applied
in image and video restoration tasks (Cai et al., 2022a; Liang
et al., 2022; Lin et al., 2022b; Cao et al., 2021; Cai et al.,
2022b). Nonetheless, the token-based self-attention module
has enormous computational and memory cost in restoring
long video sequence. Thus, the problem of effectively mod-
eling long-range temporal dependencies within the video
sequence remains formidable.

2.2. Sequence-to-Sequence Learning
Seq2seq model is ﬁrst proposed by (Sutskever et al., 2014)
for the machine translation task in which a long short-term

Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

Figure 2. The architecture of the proposed unsupervised ﬂow-aligned seq2seq model (S2SVR). The modules with different background
colors on the right show the internal details of (a) local attention, (b) motion compensation and (c) Encoder (Decoder).

memory (LSTM) encodes the input sequence into a latent
representation and then another LSTM decodes the target se-
quence out of that representation. The model is intrinsically
suitable for long-range coding tasks. Various variants of the
seq2seq model have been applied to many sequence mod-
eling tasks, such as speech recognition (Venugopalan et al.,
2015), time series analysis (Kuznetsov & Mariet, 2019; He
et al., 2021), and text summarization (Shi et al., 2021). Due
to the fundamental difference between video and language,
the potential of this serialized encoding-decoding structure
in assisting continuous-frame VR is unexplored.

2.3. Optical Flow Estimation
With the development of deep learning, some optical ﬂow
estimation networks (Sun et al., 2018; Teed & Deng, 2020)
trained on synthetic datasets have achieved better results
than non-learning methods (Mémin & Pérez, 1998; Wedel
et al., 2009). The domain difference between synthetic opti-
cal ﬂow and real-world optical ﬂow datasets leads to limited
model performance. (Wang et al., 2018a) suggest using an
unsupervised optical ﬂow estimator to circumvent the need
for labels. (Wang et al., 2018b) improve the performance
of unsupervised optical methods by proposing a new warp-
ing module to facilitate large motion learning and model
occlusion explicitly. (Shi et al., 2017) train a task-oriented
ﬂow module jointly with the video enhancement module
in the supervision of L1 loss. But the jointly-trained ﬂow
module becomes unsuitable when cooperating with other
video processing modules. Besides, they have not solved the
problem that it’s difﬁcult to estimate accurate motion infor-
mation from the severely degraded input frames. Based on
the LQ-HQ paired characteristics of VR tasks, we propose a
data distillation loss to improve the quality of the LQ ﬂows.

3. Method
In this section, we present our S2SVR model. We ﬁrst intro-
duce the overall framework of the seq2seq model. Then, we
explain the unsupervised distillation optical ﬂow method,
which narrow the domain discrepancy between NLP and
VR and improve the potential of the seq2seq model in VR.

3.1. Sequence-to-Sequence Learning

To promise that the scalable seq2seq architectures and their
efﬁcient implementations can be preserved, S2SVR follows
the seq2seq framework from NLP as closely as possible. As
shown in Fig. 2, S2SVR is composed of four components:
encoder, decoder, local attention, and optical ﬂow estimator.

For notation, we use capital
letters to represent se-
quences,(e.g., X,Y ), lower case to denote individual frames
in a sequence, (e.g., x1,x2). Let X = {x1, x2, . . . , xN }
represent the input low-quality video sequence and Y =
{y1, y2, . . . , yN } be the corresponding high-quality video
sequence, where N is the length of the sequence. The goal of
our S2SVR is to estimate the conditional probability of the
target sequence respective to the input sequence P (Y |X).

Encoder. Firstly, the encoder read sequentially each xi ∈ X
and transforms the source sequence into a list of latent
vectors Z = {z1, z2, . . . , zN }:

zi = Fe(zi−1, xi),

(1)

where zi denotes the latent vector at time step i, and Fe
denotes the function of the encoder, which in our implemen-
tation is a residual stacked ConvGRU (ResConvGRU). The
ResConvGRU will be introduced in the next subsection.

Decoder. Next, the decoder sequentially produces the out-

DecoderDecoder1ic−is1is−MEncoder1z1x2xNx2zNzMDecoder2is−Mic2ic−.  .  ..  .  .M.  .  .1is−Local Attention.  .  ..  .  .Latent VectorsS2SVRWarptx-1tx1ts−Optical Flow Estimatorto1ts−irz−+irz…ConvConvtanh1is−ic…ResConvGRUResConvGRUResConvGRU1ltz−1ltz−ConvGRURBRB…ltzConvConvReLURBResConvGRU(c)  Encoder (Decoder)EncoderEncoder(a)  Local Attention(b)  M :Motion Compensation11ltz+−21ltz+−1Ltz−1iy−iy2iy−1z.  .  ..  .  .2z1Nz−2iy−1iy−3iy−1ltz+2ltz+tzUnsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

put video based on the encoded vectors. Speciﬁcally, using
the chain rule, the conditional probability P (Y |X) can be
decomposed as:

P (Y |X) = P (y1, y2, . . . , yN |z1, z2, . . . , zN )

=

N
(cid:89)

t=1

P (yt|y1, . . . , yt−1; z1, . . . , zN ).

(2)

We serially generate the subsequent output based on the
source sequence encoding and the decoded sequence so far:

yt = Fd(y1, y2, . . . , yt−1; z1, z2, . . . , zN ).

(3)

Figure 3. Illustration of the unsupervised optical ﬂow method.

Fd represents the decoder, which is composed of a ResCon-
vGRU and a feed-forward network. ResConvGRU gener-
ates a hidden state si, and then si passes through the feed-
forward network to produce the output frame:

si = Fr(si−1, yi−1, ci),
yi = Ff (si),

(4)

where Fr is the ResConvGRU and Ff denotes the feed-
forward network. si, yi refer to the hidden state of ResCon-
vGRU and output frame at ith time step, respectively. And
ci is a context vector generated by the local attention module
based on the latent vectors Z = {z1, z2, . . . , zN }.

Local Attention. As shown in Fig. 2(a), the attention mod-
ule generates a context vector ci for each time step, allowing
the decoder to extract information from different parts of
the input sequence. Speciﬁcally, we represent the context
vector ci as a weighted sum of a subset of the latent vectors:

ci =

i+r
(cid:88)

j=i−r

αijzj,

where r is the the subset radius and the weight αij is:

αij =

exp(eij)
k=i−r exp(eik)

(cid:80)i+r

.

(5)

(6)

eij = Fa(si−1, zj) is an attention model scoring the corre-
spondence between the ith input and the jth output based
on si−1 and zj. Similar to (Shahar et al., 2011), a two-layer
feed-forward network is adopted as the attention model:

eij = Va · tanh(Wa[si−1, zj]),

(7)

where Va and Wa denote the ﬁrst and second convolution
layers of the feed-forward network, respectively. And [·, ·]
refers to concatenation along the channel dimension.

Motion Compensation. To improve the performance of
the seq2seq model in VR, we need to establish accurate
spatial correspondences among multiple frames. Similar
to previous methods (Isobe et al., 2020; Yang et al., 2019;

Zhong et al., 2020; Chan et al., 2021), we adopt an optical
ﬂow estimator for motion compensation. Speciﬁcally, as
shown in Fig. 2(b), we employ a ﬂow estimator to predict
the motion between two consecutive frames. Then we warp
the hidden state of ResConvGRU at last time step st−1,
making it spatially aligned with the input at the current step:

ot = Fo(xt, xt−1),
ˆst−1 = Fw(st−1, ot),

(8)

where Fo and Fw respectively refer to the optical ﬂow es-
timator and spatial warping module. ot is the optical ﬂow
ﬁeld between the adjacent input frames xt and xt−1.

3.2. Residual Stacked ConvGRU

We use a deep-stacked ConvGRU for both the encoder and
the decoder. Considering the video characteristics, as shown
in Fig. 2(c), we make two modiﬁcations to the original
ConvGRU. Firstly, to improve the image processing ability,
several residual blocks are concatenated after the ConvGRU.
Besides, motivated by the idea of modeling the difference
between an intermediate layer’s output and the target, we
introduce residual among the layers in a stack. We deﬁne
the ConvGRU and residual blocks as Fg(·) and Fb(·):

t = zl−1
zl

t + Fb(Fg(zl

t−1, zl−1
t

)),

(9)

where zl
t denote the hidden state of lth ConvGRU at time
step t. In this way, the vanishing gradient problem can be
addressed, allowing us to model the long-term temporal
dependencies. More details are provided in the appendix.

3.3. Unsupervised Optical Flow Estimator

As analyzed in Sec. 1, previous ﬂow-based motion compen-
sation methods suffer from the data discrepancy between
synthesized and real-world datasets, as well as inaccurate
LQ ﬂows. To solve these problems, we propose an unsuper-
vised scheme equipped with a novel distillation loss to train
the ﬂow estimator on the VR dataset as shown in Fig. 3.

Let X denotes a LQ input video, and Y is the corresponding
HQ video. Our goal is to train a ﬂow network Fo that can es-
timate accurate motion information from the LQ videos (HQ

Student NetWarpℒsm(𝐅12𝑦)Teacher NetWarpStep 1 Step 2 ℒdis(𝐅12𝑥,𝐅12𝑦)ℒsm(𝐅12𝑥)ℒph(𝐅12𝑦)ℒph(𝐅12𝑥)Resize𝑥2𝑥1𝑦2𝑦1𝐅12𝑦𝐅21𝑦𝐅12𝑥𝐅21𝑥𝑂𝑦𝑂𝑥Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

videos are unavailable during inference) by predicting the
optical ﬂow Fx
12 for two consecutive LQ frames {x1, x2}:

Fx

12 = Fo(x1, x2).

(10)

The unsupervised scheme is summarized in Algorithm 1 to
better understand the proposed unsupervised optical ﬂow
estimation method. Firstly, we train a teacher ﬂow estima-
tion network parameterized by θt on the HQ videos with
photometric loss and smooth loss. After convergence, we
use the pretraied teacher estimator to generate pseudo-labels
and train a student ﬂow network parameterized by θo on
the LQ video. In the following, we explain the proposed
unsupervised optical ﬂow training scheme step by step.

Step 1. We train an optical ﬂow estimator Ft with photomet-
ric loss and smooth loss on the HQ video Y . This optical
ﬂow estimator Ft will be frozen and serves as a teacher
network in the next step. The photometric loss (Yu et al.,
2016) is based on the assumption that the same object in
two consecutive frames must have similar intensities:

Lph(Fy

12) =

(cid:88)

p

ρ(y1(p), y2(p + Fy

12(p))) · Oy(p), (11)

where p is the coordinate and Oy is the occlusion mask to
discard the loss on the occurred region generated by the
bidirectional checking (Wang et al., 2018b), ρ(·) is the (cid:96)1
loss, and Fy
12 is the optical ﬂow ﬁeld for two consecutive
frames in the HQ videos Y :

Fy

12 = Ft(y1, y2).

(12)

Further, we adopt a one-order smooth loss (Godard et al.,
2017) to encourage collinearity of neighboring ﬂows:

Lsm(Fy

12) =

(cid:88)

(cid:88)

d∈x,y

p

|∂dFy

12(p)|e−|∂dy1(p)|

(13)

And then we formulate the loss used in the ﬁrst step as:

L = ωph · Lph(Fy

12) + ωsm · Lsm(Fy

12).

(14)

We respectively set the weights ωph and ωsm to 0.15 and 50.

Step 2. Now we have trained a teacher optical ﬂow estimator
Ft which can predict the accurate optical ﬂow Fy
12 for two
consecutive HQ frames {y1, y2} ∈ Y :

Fy

12 = Ft(y1, y2).

(15)

Based on the assumption that the HQ ﬂow is more accurate
for motion compensation, we use Fy
12 as the pseudo-labels
of the LQ ﬂows F x
12 and and propose the distillation loss:

Ldis(Fx

12, Fy

12) =

|Fy

12(p) − Fu(Fx

12)(p)|,

(16)

(cid:88)

p

Algorithm 1 Unsupervised Distillation Optical Flow Loss
Inputs: teacher and student net parameterized by θt, θo,
cost function parameters: loss weights {ωph, ωsm, ωdis},
optimization parameters: number of iterations T
Output: pretrained student network
// Step1: train the teacher network
for j = 0 to T do

compute photometric loss Lph (using Eq. (11))
compute smooth loss Lsm (using Eq. (13))
Ltot = ωph · Lph + ωsm · Lsm, ∇L(θt) = ∂Ltot
∂θt
θt = θt − α∇L(θt)

,

end for
// Step2: train the student network
for j = 0 to T do

compute photometric loss Lph (using Eq. (11))
compute smooth loss Lsm (using Eq. (13))
compute data distillation loss Ldis (using Eq. (16))
Ltot = ωphLph + ωsmLsm + ωdisLdis, ∇L(θo) = ∂Ltot
∂θo
θo = θo − α∇L(θo)

,

end for
Return student network parameters θo

where Fu is a upsample operation to ensure that Fx
12 has
the same size as Fy
12 in video super-resolution task. Along
with the photometric loss and smoothness regularization,
we train the student ﬂow estimator Fo on the LQ dataset:

12, Fy

L = ωphLph(Fx

12) + ωdisLdis(Fx

12) + ωsmLsm(Fx

12).
(17)
We set the weights to {ωph = 0.15, ωsm = 50, ωdis = 0.1}.
The student network will be later used as our optical ﬂow
estimator for motion compensation as in Eq. (8). In imple-
mentation, we adopt a lightweight ﬂow model pwclite (Liu
et al., 2020) as our optical ﬂow network.

4. Experiments
4.1. Implementation Details

Datasets. For video SR, the benchmark datasets consist
of REDS4 (Nah et al., 2019a) and Vimeo-90K-T (Xue
et al., 2019). For video deblurring, we use the GOPRO
dataset (Nah et al., 2017), where 22 videos are used for
training and 11 videos for testing. For compressed video
enhancement, our models are trained with the MFQEv2
dataset (Guan et al., 2019) including 108 lossless videos.
We adopt the dataset from ITU-T (Ohm et al., 2012) contain-
ing 18 videos for evaluation. We compress videos by HEVC
reference software HM16.5 under Low Delay P (LDP) con-
ﬁguration (Guan et al., 2019; Deng et al., 2020). Evaluation
metrics include PSNR and SSIM (Wang et al., 2004).

Settings. Models are trained with nature videos and their
degraded counterparts. During unsupervised optical ﬂow
training, the learning rate is set to 1 × 10−4. And during
restoration training, the initial learning rate of the ﬂow esti-

Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

Figure 4. Visual comparison of video 4× SR results on the REDS4 (Nah et al., 2019a) dataset. Please zoom in for a better comparison.

-

Params

REDS4

5.8 M 28.63 / 0.8251

Vimeo-90K-T
- 26.14 / 0.7292 31.32 / 0.8684
- 27.98 / 0.7990 33.08 / 0.9054

Methods
Bicubic
TOFlow
DUF
RBPN
12.2 M 30.09 / 0.8590 37.07 / 0.9435
3.3 M 30.53 / 0.8699 37.09 / 0.9446
EDVR-M
EDVR
20.6 M 31.09 / 0.8800 37.61 / 0.9489
3.0 M 29.63 / 0.8502 36.14 / 0.9363
PFNL
- 30.88 / 0.8750 37.32 / 0.9465
MuCAN
BasicVSR
6.3 M 31.42 / 0.8909 37.18 / 0.9450
IconVSR
8.7 M 31.67 / 0.8948 37.47 / 0.9476
VSR-Transformer 32.6 M 31.19 / 0.8815 37.71 / 0.9494
13.4 M 31.96 / 0.8988 37.63 / 0.9490
S2SVR (Ours)

Table 1. Quantitative comparison (PSNR/SSIM) on the video SR
dataset REDS4 and Vimeo-90K-T. Bold and underlined text indi-
cate the best and the second-best performance, respectively.

mator and the other modules are set to 5×10−5 and 2×10−4,
respectively. We use PyTorch to implement our models and
train them on 8 Tesla V100 GPUs. More details are provided
in the supplementary material due to space limitation.

4.2. Video Super-Resolution

Quantitative Comparison. We compare our method with
previous methods: TOFlow (Xue et al., 2019), DUF (Jo et al.,
2018), RBPN (Haris et al., 2019), EDVR-M (Wang et al.,
2019), EDVR (Wang et al., 2019), PFNL (Yi et al., 2019),
MuCAN (Li et al., 2020), BasicVSR (Chan et al., 2021),
IconVSR (Chan et al., 2021), and VSR-Transformer (Cao
et al., 2021). As shown in Tab. 1, it is clear that our method
outperforms all other models by a large margin on the
REDS4 dataset. Speciﬁcally, our S2SVR model achieves
0.29dB gain over the suboptimal model and 0.77dB over
the VSR-Transformer model in PSNR. For Vimeo-90K-T,

Methods
Tao et al.
Su et al.
Kim et al.
Nah et al.
EDVR
STFAN
TSP
UHDVD
S2SVR (Ours)

Params
-
15.30 M
-
-
23.6 M
5.37 M
16.19 M
-
8.44 M

PSNR (dB)
30.29
27.31
26.82
29.97
26.83
28.59
31.67
31.33
31.81

SSIM
0.9014
0.8255
0.8245
0.8947
0.8426
0.8608
0.9279
0.9210
0.9231

Table 2. Video deblurring performance comparison and model pa-
rameter analysis on the GOPRO dataset (Nah et al., 2017).

our performance is slightly lower than VSR-Transformer,
but S2SVR only requires 41% parameters compared with
the latter. It shows that we only need half the parameters to
obtain comparable performance to transformer-based mod-
els. Note that Vimeo-90K-T contains sequences with seven
frames. So it also indicates that our method performs better
in restoring long sequences. Serialized modeling of seq2seq
models and accurate optical ﬂow estimation facilitates the
capture of long-range inter-frame dependencies.

Visual Comparison. From the comparison with other meth-
ods in Fig. 4, our S2SVR network has shown great advan-
tages in the restoration of textures and structural details,
such as license plate numbers, pane lines, and hairs. Our re-
sults are more reliable and detailed, while the other methods
suffer from excessive smoothing and content distortion.

4.3. Video Deblurring
Quantitative Comparison. We compare our method
against state-of-the-art algorithms, including Tao et al.
(Tao et al., 2018), Su et al. (Su et al., 2017), Kim et al.

LREDVRS2SVR (Ours)GTBasicVSRIconVSRUnsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

QP

Approach

AR-CNN

DnCNN

DS-CNN

MFQE 1.0

MFQE 2.0

STDF-R3L

(Dong et al., 2015) (Zhang et al., 2017) (Yang et al., 2018a) (Yang et al., 2018b) (Guan et al., 2019) (Deng et al., 2020)

S2SVR
(Ours)

A

B

37

C

D

E

Metrics

Trafﬁc
PeopleOnStreet
Kimono
ParkScene
Cactus
BQTerrace
BasketballDrive
RaceHorses
BQMall
PartyScene
BasketballDrill
RaceHorses
BQSquare
BlowingBubbles
BasketballPass
FourPeople
Johnny
KristenAndSara

Average

0.239 / 47
0.346 / 75
0.219 / 65
0.136 / 38
0.190 / 38
0.195 / 28
0.229 / 55
0.219 / 43
0.275 / 68
0.107 / 38
0.247 / 58
0.268 / 55
0.080 / 8
0.164 / 35
0.259 / 58
0.373 / 50
0.247 / 10
0.409 / 50
0.233 / 45

0.238 / 57
0.414 / 82
0.244 / 75
0.141 / 50
0.195 / 48
0.201 / 38
0.251 / 58
0.253 / 65
0.281 / 68
0.131 / 48
0.331 / 68
0.311 / 73
0.129 / 18
0.184 / 58
0.307 / 75
0.388 / 60
0.315 / 40
0.421 / 60
0.263 / 58

0.286 / 60
0.416 / 85
0.249 / 75
0.153 / 50
0.239 / 58
0.257 / 48
0.282 / 65
0.267 / 63
0.330 / 80
0.174 / 58
0.352 / 68
0.318 / 75
0.201 / 38
0.228 / 68
0.335 / 78
0.459 / 70
0.378 / 40
0.481 / 60
0.300 / 63

∆PSNR / ∆SSIM
0.497 / 90
0.802 / 137
0.495 / 113
0.391 / 103
0.439 / 88
0.270 / 48
0.406 / 80
0.340 / 55
0.507 / 103
0.217 / 73
0.477 / 90
0.507 / 113
-0.010 / 15
0.386 / 120
0.628 / 138
0.664 / 85
0.548 / 55
0.655 / 75
0.455 / 88

0.585 / 102
0.920 / 157
0.550 / 118
0.457 / 123
0.501 / 100
0.403 / 67
0.465 / 83
0.394 / 80
0.618 / 120
0.363 / 118
0.579 / 120
0.594 / 143
0.337 / 65
0.533 / 170
0.728 / 155
0.734 / 95
0.604 / 68
0.754 / 85
0.562 / 109

0.730 / 115
1.250 / 196
0.850 / 161
0.590 / 147
0.770 / 138
0.630 / 106
0.750 / 123
0.550 / 135
0.990 / 180
0.680 / 194
0.790 / 149
0.830 / 208
0.640 / 125
0.740 / 226
1.080 / 212
0.940 / 117
0.810 / 88
0.970 / 96
0.830 / 151

0.851 / 138
1.385 / 216
1.055 / 195
0.649 / 165
0.828 / 152
0.654 / 115
0.972 / 157
0.854 / 203
1.080 / 205
0.628 / 236
0.949 / 179
1.010 / 237
0.886 / 141
0.710 / 230
1.110 / 222
1.021 / 136
0.976 / 120
1.035 / 113
0.925 / 176

Table 3. Overall comparison of compressed video enhancement for ∆PSNR (dB) and ∆SSIM (×10−4) over test sequences at QP=37.
We experiment with ﬁve different video resolutions: A (2,560×1,600), B (1,920×1,080), C (832×480), D (480×240), E (1,280×720).

Figure 5. Visual comparison of video deblurring results on the GOPRO (Nah et al., 2017) dataset. Please zoom in for a better comparison.

(Hyun Kim et al., 2017), Nah et al. (Nah et al., 2019b),
EDVR (Wang et al., 2019), STFAN (Zhou et al., 2019),
TSP (Pan et al., 2020), and UHDVD (Deng et al., 2021).
The Tab. 2 shows the quantitative results on the GOPRO
dataset (Nah et al., 2017). Our proposed method performs
favorably against other methods and has an absolute advan-
tage on PSNR in video deblurring. Speciﬁcally, the S2SVR
model achieves a performance gain of 0.14dB on the dataset
with a lightweight structure. We also report the size of the
open-source model in Tab. 2. As the largest model, EDVR’s
parameter is up to 23M, but its performance is unsatisfactory.
Our S2SVR network contains 8.44M parameters. Compared

with the TSP (Pan et al., 2020), our model achieves a higher
PSNR performance with only one-half of its size.

Visual Comparison. From the comparison results in Fig. 5,
it can be seen that our method can restore the original struc-
ture as much as possible from the severely degraded scene.
Digital restoration of blurred scenes is difﬁcult. It can be
seen that no other method except ours can guarantee the
semantics while still retaining the satisfying visual results.

4.4. Compressed Video Enhancement
Quantitative Comparison. We evaluate the performance
of compressed video enhancement by ∆PSNR and ∆SSIM,
which measure the PSNR and SSIM improvement after the

BlurredGTSTFANEDVRSRNDBNTSPS2SVR(Ours)Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

Method

Params
PSNR (dB)

RAFT

Pwclite

Sup.
4.81 M
31.88

Unsup.
-
-

Sup.

Unsup.
2.24 M 2.24 M
31.96
31.79

Table 4. Model analysis with supervised (Sup.) and unsupervised
(Unsup.) optical ﬂow estimation model training.

Length
5
15
30
50

EDVR-M EDVR
28.05
28.80
28.01
27.77

27.78
28.47
27.76
27.52

S2SVR (Ours)
28.10 (+ 0.05)
29.25 (+ 0.45)
28.53 (+ 0.52)
28.34 (+ 0.57)

Table 5. Quantitative comparison on sequences of different length.

with a lower cost. These results show the effectiveness of
our unsupervised optical ﬂow method.

Motion Compensation Visualization. We visualize the
feature saliency maps with (W/I) and without (W/O) motion
compensation in Fig. 7. Obviously, the video frame will
lose lots of motion details and texture edges without mo-
tion compensation. It is caused by the misalignment among
multiple frames, which limits the potential of the seq2seq
model in VR. In contrast, with motion compensation, the
feature map is much sharper and preserves more movement
details, which beneﬁts from our accurate optical ﬂow estima-
tion. Motion compensation narrows the domain difference
between NLP and VR, facilitating information aggregation.

Long Sequence Reconstruction. To validate the effective-
ness of our S2SVR in capturing long-range temporal de-
pendencies, we separate a video in REDS4 dataset into 4
segments with different lengths, including 5, 15, 30, and
50 frames, respectively. And we use S2SVR, EDVR, and
EDVR-M to restore these sequences independently. Our
method performs the best among the three methods in
Tab. 5. And the longer the sequence is, the more superior our
S2SVR shows. It suggests that our method has an excellent
performance in modeling long-range dependencies.

6. Conclusions

In this paper, we propose an unsupervised ﬂow-aligned
seq2seq model for multiple video restoration tasks. Our
work aims at solving the challenges of properly model-
ing the inter-frame relation within the video sequence. The
sequence-to-sequence learning is explored for the ﬁrst time
in VR to capture long-term temporal dependencies at a
low cost. What’s more, we design an unsupervised optical
method equipped with a novel distillation loss to improve the
performance of the seq2seq model in VR. Extensive experi-
ments show that the proposed method achieves comparable
performance in video deblurring, video super-resolution,
and compressed video quality enhancement tasks with mod-
erate model size, especially in long sequence VR.

Acknowledgements: This work is partially supported by
the NSFC fund (61831014), the Shenzhen Science and Tech-
nology Project under Grant (CJGJZD20200617102601004,
JSGG20210802153150005).

Figure 6. Visual comparison on Video BasketballPass at QP = 37.

Figure 7. Visualization with and without motion compensation.

enhancement. We compare S2SVR with AR-CNN (Dong
et al., 2015), DnCNN (Zhang et al., 2017), DS-CNN (Yang
et al., 2018a), MFQE 1.0 (Yang et al., 2018b), MFQE 2.0
(Guan et al., 2019), and STDF-R3L (Deng et al., 2020).
As shown in Tab. 3, S2SVR almost outperforms all com-
pared methods in ∆PSNR and ∆SSIM by a large margin.
On the RaceHorses dataset with the 832 ×480 input size,
our method can outperform the latest method by at least
0.304dB, which shows the superiority of our method in
processing long video sequences with large motion.

Visual Comparison. As shown in Fig. 6, some structural
distortions and color deviations make the results of previous
methods unconvincing. Our S2SVR network guarantees the
basic structural texture and semantic content. Due to space
limitations, we put more comparisons in the supplementary.

5. Ablation Study

Unsupervised Optical Flow Estimator. To demonstrate
the effectiveness of the unsupervised training scheme, we
retrain our optical ﬂow network pwclite in a supervised
manner with the optical ﬂow dataset FlyingChairs (Dosovit-
skiy et al., 2015). We also adopt pre-trained RAFT (Teed &
Deng, 2020), the SOTA supervised optical ﬂow network, as
our optical ﬂow estimator. As shown in Tab. 4, the pwclite
trained with our unsupervised distillation loss can outper-
form the supervised counterpart by 0.17 dB. It indicates that
the ﬂow estimator trained in our unsupervised scheme ﬁts
the VR tasks well. Notably, it achieves a better result than
the state-of-the-art supervised method RAFT by 0.08dB

HEVCSTDFMFQES2SVR (Ours)Raw(a) Without Motion Compensation       (b) With Motion Compensation      Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

References

Cai, Y., Hu, X., Wang, H., Zhang, Y., Pﬁster, H., and Wei, D.
Learning to generate realistic noisy images via pixel-level
noise-aware adversarial training. In NeurIPS, 2021.

Cai, Y., Lin, J., Hu, X., Wang, H., Yuan, X., Zhang, Y., Tim-
ofte, R., and Van Gool, L. Mask-guided spectral-wise
transformer for efﬁcient hyperspectral image reconstruc-
tion. In CVPR, 2022a.

Cai, Y., Lin, J., Lin, Z., Wang, H., Zhang, Y., Pﬁster, H., Tim-
ofte, R., and Van Gool, L. Mst++: Multi-stage spectral-
wise transformer for efﬁcient spectral reconstruction. In
CVPRW, 2022b.

Guan, Z., Xing, Q., Xu, M., Yang, R., Liu, T., and Wang,
Z. Mfqe 2.0: A new approach for multi-frame quality
enhancement on compressed video. TPAMI, 2019.

Haris, M., Shakhnarovich, G., and Ukita, N. Recurrent back-
projection network for video super-resolution. In CVPR,
2019.

He, D., Zheng, Y., Sun, B., Wang, Y., and Qin, H. Checker-
board context model for efﬁcient learned image compres-
sion. In CVPR, 2021.

Hyun Kim, T., Mu Lee, K., Scholkopf, B., and Hirsch, M.
Online video deblurring via dynamic temporal blending
network. In ICCV, 2017.

Cao, J., Li, Y., Zhang, K., and Van Gool, L. Video super-
resolution transformer. arXiv preprint arXiv:2106.06847,
2021.

Isobe, T., Jia, X., Gu, S., Li, S., Wang, S., and Tian, Q. Video
super-resolution with recurrent structure-detail network.
In ECCV, 2020.

Chan, K. C., Wang, X., Yu, K., Dong, C., and Loy, C. C.
Basicvsr: The search for essential components in video
super-resolution and beyond. In CVPR, 2021.

Chen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey,
W., Foster, G., Jones, L., Parmar, N., Schuster, M., Chen,
Z., et al. The best of both worlds: Combining recent
advances in neural machine translation. In ACL, 2018.

Chopra, S., Auli, M., and Rush, A. M. Abstractive sentence
summarization with attentive recurrent neural networks.
In NAACL-HLT, 2016.

Dai, Q., Yoo, S., Kappeler, A., and Katsaggelos, A. K.
Dictionary-based multiple frame video super-resolution.
In ICIP, 2015.

Deng, J., Wang, L., Pu, S., and Zhuo, C. Spatio-temporal
deformable convolution for compressed video quality
enhancement. In AAAI, 2020.

Deng, S., Ren, W., Yan, Y., Wang, T., Song, F., and Cao, X.
Multi-scale separable network for ultra-high-deﬁnition
video deblurring. In CVPR, 2021.

Dong, C., Deng, Y., Loy, C. C., and Tang, X. Compression
artifacts reduction by a deep convolutional network. In
ICCV, 2015.

Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas,
C., Golkov, V., Van Der Smagt, P., Cremers, D., and Brox,
T. Flownet: Learning optical ﬂow with convolutional
networks. In ICCV, 2015.

Godard, C., Mac Aodha, O., and Brostow, G. J. Unsuper-
vised monocular depth estimation with left-right consis-
tency. In CVPR, 2017.

Jo, Y., Oh, S. W., Kang, J., and Kim, S. J. Deep video
super-resolution network using dynamic upsampling ﬁl-
In CVPR,
ters without explicit motion compensation.
2018.

Jozefowicz, R., Zaremba, W., and Sutskever, I. An empirical
exploration of recurrent network architectures. In ICML,
2015.

Kuznetsov, V. and Mariet, Z. Foundations of sequence-to-
sequence modeling for time series. In AISTATS, 2019.

Li, W., Tao, X., Guo, T., Qi, L., Lu, J., and Jia, J. Mu-
can: Multi-correspondence aggregation network for video
super-resolution. In ECCV, 2020.

Liang, J., Cao, J., Fan, Y., Zhang, K., Ranjan, R., Li, Y.,
Timofte, R., and Van Gool, L. Vrt: A video restoration
transformer. arXiv preprint arXiv:2201.12288, 2022.

Liao, R., Tao, X., Li, R., Ma, Z., and Jia, J. Video super-
resolution via deep draft-ensemble learning. In ICCV,
2015.

Lin, J., Cai, Y., Hu, X., Wang, H., Yan, Y., Zou, X., Ding, H.,
Zhang, Y., Timofte, R., and Van Gool, L. Flow-guided
sparse transformer for video deblurring. arXiv preprint
arXiv:2201.01893, 2022a.

Lin, J., Cai, Y., Hu, X., Wang, H., Yuan, X., Zhang, Y.,
Timofte, R., and Van Gool, L. Coarse-to-ﬁne sparse
transformer for hyperspectral image reconstruction. arXiv
preprint arXiv:2203.04845, 2022b.

Liu, L., Zhang, J., He, R., Liu, Y., Wang, Y., Tai, Y., Luo, D.,
Wang, C., Li, J., and Huang, F. Learning by analogy: Re-
liable supervision from transformations for unsupervised
optical ﬂow estimation. In CVPR, 2020.

Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

Mémin, E. and Pérez, P. Dense estimation and object-based
segmentation of the optical ﬂow with robust techniques.
TIP, 1998.

Tao, X., Gao, H., Shen, X., Wang, J., and Jia, J. Scale-
recurrent network for deep image deblurring. In CVPR,
2018.

Nah, S., Hyun Kim, T., and Mu Lee, K. Deep multi-scale
convolutional neural network for dynamic scene deblur-
ring. In CVPR, 2017.

Nah, S., Baik, S., Hong, S., Moon, G., Son, S., Timofte, R.,
and Mu Lee, K. Ntire 2019 challenge on video deblurring
In CVPRW,
and super-resolution: Dataset and study.
2019a.

Nah, S., Son, S., and Lee, K. M. Recurrent neural networks
with intra-frame iterations for video deblurring. In CVPR,
2019b.

Ohm, J.-R., Sullivan, G. J., Schwarz, H., Tan, T. K., and
Wiegand, T. Comparison of the coding efﬁciency of video
coding standards—including high efﬁciency video coding
(hevc). TCSVT, 2012.

Ott, M., Edunov, S., Grangier, D., and Auli, M. Scaling neu-
ral machine translation. arXiv preprint arXiv:1806.00187,
2018.

Pan, J., Bai, H., and Tang, J. Cascaded deep video deblurring

using temporal sharpness prior. In CVPR, 2020.

Sajjadi, M. S., Vemulapalli, R., and Brown, M. Frame-

recurrent video super-resolution. In CVPR, 2018.

Shahar, O., Faktor, A., and Irani, M. Space-time super-

resolution from a single video. IEEE, 2011.

Shi, T., Keneshloo, Y., Ramakrishnan, N., and Reddy, C. K.
Neural abstractive text summarization with sequence-to-
sequence models. ACM Transactions on Data Science,
2021.

Shi, X., Gao, Z., Lausen, L., Wang, H., Yeung, D.-Y., Wong,
W.-k., and Woo, W.-c. Deep learning for precipitation
nowcasting: A benchmark and a new model. In NeurIPS,
2017.

Su, S., Delbracio, M., Wang, J., Sapiro, G., Heidrich, W., and
Wang, O. Deep video deblurring for hand-held cameras.
In CVPR, 2017.

Sun, D., Yang, X., Liu, M.-Y., and Kautz, J. Pwc-net: Cnns
for optical ﬂow using pyramid, warping, and cost volume.
In CVPR, 2018.

Teed, Z. and Deng, J. Raft: Recurrent all-pairs ﬁeld trans-

forms for optical ﬂow. In ECCV, 2020.

Tian, Y., Zhang, Y., Fu, Y., and Xu, C. Tdan: Temporally-
deformable alignment network for video super-resolution.
In CVPR, 2020.

Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R.,
Darrell, T., and Saenko, K. Sequence to sequence-video
to text. In ICCV, 2015.

Wang, X., Chan, K. C., Yu, K., Dong, C., and Change Loy,
C. Edvr: Video restoration with enhanced deformable
convolutional networks. In CVPRW, 2019.

Wang, Y., Yang, Y., Yang, Z., Zhao, L., Wang, P., and Xu, W.
Occlusion aware unsupervised learning of optical ﬂow.
In CVPR, 2018a.

Wang, Y., Yang, Y., Yang, Z., Zhao, L., Wang, P., and Xu, W.
Occlusion aware unsupervised learning of optical ﬂow.
In CVPR, 2018b.

Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli,
E. P. Image quality assessment: from error visibility to
structural similarity. TIP, 2004.

Wedel, A., Cremers, D., Pock, T., and Bischof, H. Structure-
and motion-adaptive regularization for high accuracy op-
tic ﬂow. In ICCV, 2009.

Xiang, X., Wei, H., and Pan, J. Deep video deblurring using

sharpness features from exemplars. TIP, 2020.

Xue, T., Chen, B., Wu, J., Wei, D., and Freeman, W. T.
Video enhancement with task-oriented ﬂow. IJCV, 2019.

Yang, R., Xu, M., Liu, T., Wang, Z., and Guan, Z. Enhancing
quality for hevc compressed videos. TCSVT, 2018a.

Yang, R., Xu, M., Wang, Z., and Li, T. Multi-frame quality
enhancement for compressed video. In CVPR, 2018b.

Yang, R., Sun, X., Xu, M., and Zeng, W. Quality-gated
convolutional lstm for enhancing compressed video. In
ICME, 2019.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
quence learning with neural networks. arXiv preprint
arXiv:1409.3215, 2014.

Yi, P., Wang, Z., Jiang, K., Jiang, J., and Ma, J. Progressive
fusion video super-resolution network via exploiting non-
local spatio-temporal correlations. In ICCV, 2019.

Takeda, H., Milanfar, P., Protter, M., and Elad, M. Super-
resolution without explicit subpixel motion estimation.
TIP, 2009.

Yu, J. J., Harley, A. W., and Derpanis, K. G. Back to ba-
sics: Unsupervised learning of optical ﬂow via brightness
constancy and motion smoothness. In ECCV, 2016.

Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration

Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.
Beyond a gaussian denoiser: Residual learning of deep
cnn for image denoising. TIP, 2017.

Zheng, H., Li, X., Liu, F., Jiang, L., Zhang, Q., Li, F., Dang,
Q., and He, D. Adaptive spatial-temporal fusion of multi-
objective networks for compressed video perceptual en-
hancement. In CVPR, 2021.

Zhong, Z., Gao, Y., Zheng, Y., and Zheng, B. Efﬁcient
spatio-temporal recurrent neural network for video de-
blurring. In ECCV, 2020.

Zhou, S., Zhang, J., Pan, J., Xie, H., Zuo, W., and Ren, J.
Spatio-temporal ﬁlter adaptive network for video deblur-
ring. In ICCV, 2019.

