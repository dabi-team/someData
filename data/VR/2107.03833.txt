Towards Collaborative Photorealistic VR Meeting
Rooms

Alexander Schäfer
Alexander.Schaefer@dfki.uni-kl.de
TU Kaiserslautern

Gerd Reis
German Research Center for Artificial
Intelligence

Didier Stricker
German Research Center for Artificial
Intelligence
TU Kaiserslautern

1
2
0
2

l
u
J

2
1

]

C
H
.
s
c
[

2
v
3
3
8
3
0
.
7
0
1
2
:
v
i
X
r
a

Figure 1: Our virtual reality meeting room using the proposed method.

ABSTRACT
When designing 3D applications it is necessary to find a
compromise between cost (e.g. money, time) and achievable
realism of the virtual environment. Reusing existing assets
has an impact on the uniqueness of the application and cre-
ating high quality 3D assets is very time consuming and
expensive. We aim for a low cost, high quality and minimal
time effort solution to create virtual environments. This pa-
per’s main contribution is a novel way of creating a virtual
meeting application by utilizing augmented spherical images
for photo realistic virtual environments.

CCS CONCEPTS
• Human-centered computing → Mixed / augmented
reality; Virtual reality; Collaborative interaction; Ges-
tural input.

KEYWORDS
virtual reality, augmented reality, mixed reality, virtual meet-
ing, teleconferencing, telepresence, spherical images

ACM Reference Format:
Alexander Schäfer, Gerd Reis, and Didier Stricker. 2019. Towards
Collaborative Photorealistic VR Meeting Rooms. In Mensch und
Computer 2019 (MuC ’19), September 8–11, 2019, Hamburg, Germany.

MuC ’19, September 8–11, 2019, Hamburg, Germany
© 2019 Association for Computing Machinery.
This is the author’s version of the work. It is posted here for your personal
use. Not for redistribution. The definitive Version of Record was published
in Mensch und Computer 2019 (MuC ’19), September 8–11, 2019, Hamburg,
Germany, https://doi.org/10.1145/3340764.3344466.

ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3340764.
3344466

1 INTRODUCTION
Meetings that require physical presence in other countries
are often a time consuming overhead in addition to be costly.
A great deal of money is spent on travel expenses while also
reducing the working hours of employees by having them
fly several hours from one end of the earth to the other. In
today’s world, where everyone is connected via the internet
and where it is possible to communicate very quickly with
each other, such meetings should rarely be necessary. This is
often not the case in reality. Back in 2008, IBM realized the
potential of virtual conferences and saved approx. 320,000 $
by having one of the biggest virtual conferences with over
200 participants according to Kantonen et al. [9]. Andres et al.
[1] came to the conclusion that a physical face-to-face meet-
ing is a far superior approach than having a video conference
(e.g. Skype) in terms of productivity. The loss of social pres-
ence while having a phone or video call is an important point
of their conclusion. Our work focuses on a virtual meeting
scenario which provides tools for collaboration over long dis-
tances while maintaining a fully immersed experience. Our
system allows an immersive experience to be created in mere
minutes by using spherical images for virtual environments.
Virtual objects are placed within those environments which
can be viewed and interacted with by users in a virtually
enriched experience (see Figure 1). We use multiple spherical
images from different view points to create an illusion which
represents an authentic environment. Hand tracking is used
as input and interaction method. During our work this ap-
proach has proven itself to be very successful in different

 
 
 
 
 
 
MuC ’19, September 8–11, 2019, Hamburg, Germany

Schäfer et al.

to analyze human robot collaboration in VR. They state that
they used a virtual scene with at least 42 new 3D models cre-
ated and image textures from real industrial workplaces for
a more realistic environment while also adding several aux-
iliary parts and objects to the scene. While Matsas et al. [12]
did not mention how much time it took to create this scene,
creating this specific virtual environment must have been a
large overhead. The work of Hilfert et. al [5] tries to address
this overhead issue while De Dinechin and Paljic et al. [3]
mention 360-degree images as an easy-to-use and low-cost
alternative to acquire a 3D scene. Virtual service journeys
where evaluated by Boletsis et al. [2], where one evaluation
method used 360-degree images from Google Street View
for a prototyping phase. They mentioned that this approach
was inexpensive in terms of man-hours and equipment costs
compared to the other used prototype, a real world touring
scenario. The Game Experience Questionnaire [6] was used
to evaluate a direct comparison between prototypes using
the real environment and a virtual environment using 360-
degree images of the visited places. They conclude that there
was no big statistical difference in both prototypes which
elevates the VR simulation prototype as a useful tool with
significantly less expenses. A VR driving simulator for UI
prototyping was done by Schroeter et al. [15]. They used 180-
degree videos and a high quality car model in an automated
driving scenario for rapid prototyping of UI inside the car.

Hand and Gesture Interaction Methods
There are several options available for gesture and hand
based interaction. A Microsoft Kinect was used by Oikono-
midis et al. [13] for a markerless tracking of the hands. Hand
tracking utilizing machine learning was done by Malik et
al. [11]. A colored glove was used by Wang et al. [18] to
estimate hand pose from single RGB images. The leap mo-
tion controller (LMC) provides a tracking accuracy between
0.01mm and 0.5mm of the fingertip according to Smeragli-
uolo et al. [16]. We chose the LMC for our system since it is
easy to integrate and pose estimation processing is done on
a dedicated device.

3 COMPONENTS FOR A VIRTUAL MEETING

ROOM

To create a virtual meeting room experience, we focused
on three core aspects and their separate roles (illustrated in
Figure 2): virtual environment, users, and shared interactive
elements.

Virtual Environments are essential to immerse a user in
virtual worlds. In our case it is desired to create a realistic
experience that includes all important aspects of a common
meeting room. A room with multiple seats, a big table and a
projector is easily identified as a meeting room. The typical

Figure 2: Schematic overview of our approach. We separate
three different roles in a virtual meeting.

setups while maintaining a high quality virtual environment
with almost no effort.

The contribution of this work is as follows:
• A novel VR application approach by utilizing multiple
spherical images to provide a photo realistic enriched
virtual world

• Utilizing hand gestures for interactive 3D elements
in enriched virtual worlds in the context of virtual
conferences

2 RELATED WORK

Virtual Conferences
A notably earlier work is from Kantonen et al. [9] where
they propose a system for teleconferencing by combining AR
mechanics with the game environment Second Life, which
was a very popular online social game. Jo et al. [8] used
3D constructed environments and real environments (video
background) in combination with different types of avatars.
They showed that video based backgrounds are much more
immersive and provide higher sense of co-presence than 3D
replicated environments. A collaborative AR experience by
using scene matching between two seperate rooms was also
done by Jo et al. [7]. Their main component was a kinect sen-
sor and a visual marker to match scenes and retarget avatar
motions to fit in the physical place where each participant is
located.

Virtual Environments
The presentation of the following works are mainly focused
on the virtual environments used to create applications.
Lovreglio et al. [10] used a complex 3D scene for prototyping
a virtual earthquake simulation. While it allows a more fine
grained control of the virtual environment, a modeled 3D
scene lacks realism. Matsas et al. [12] created an application

Towards Collaborative Photorealistic VR Meeting Rooms

MuC ’19, September 8–11, 2019, Hamburg, Germany

Figure 3: Multiple spherical images share a virtual TV and projector.

way of creating such a room is to create all necessary 3D
assets and properly align them. Depending on the desired
amount of realism this process can be very costly and time
consuming. Another option is to take photos of a room and
then recreate it with 3D assets. We found that approach to
be too expensive and provide a solution which is indepen-
dent of the complexity of a room and still provides very high
realism. This is why we chose spherical images as a source
to create photo realistic virtual worlds. The visual advantage
compared to traditional artificially created 3D environments
is enormous, they can capture the appearance of a room
with every detail in a fracture of a second. Additionally it
is possible to use HDR imaging techniques for even better
visual impressions. Spherical images are one of our main
components in creating a virtual meeting room.

Users need visual representation and the ability to interact
with the virtual environment. Since the visualization method
we use depends on real environments, we have aimed not
to use any controllers and relied on gesture and hand based
input methods, since it is a natural way of interaction in
the real world. Users are represented by a virtual avatar in
order to be perceived by others in the virtual environment.
Seeing the hands and gestures of other users is a great way
to enhance the overall collaboration experience [14, 17]. The
hands are primarily used for interaction with virtual objects
shared across all participants in the virtual meeting.

Shared Interactive Elements in meeting rooms are used
to establish an information exchange between all persons
present in a room. Those objects can be viewed, changed
and interacted with by each participant. It is critical for a
virtual meeting room experience to identify necessary com-
ponents that enable a true meeting experience. In our case

we restricted it to a projector and a TV to be those shared
interactive elements, but this can be extended to any other
element.

4 IMPLEMENTATION

Overview
Spherical images are used for the visualization of the virtual
environment. We acquire spherical images of the resolution
5376 x 2688. Unity game engine was used for visualization
and coupling of the hand tracking to our virtual simulation.
The system supports all headsets supported by the OpenVR
SDK (HTC Vive, Oculus Rift, ...).

We took several spherical images inside a meeting room,
each centered at a spot where normally a participant would
sit during a meeting. This enables the user to move to differ-
ent positions in the room. Figure 2 sketches our approach
while Figure 3 shows the result of the applied approach.

For collaboration, we implemented two possible tools in-
side our meeting room, a TV and a surface where a projector
could project images on the wall. Those two tools can be
interpreted as shared interactive elements to be seen by sev-
eral users at the same time during a meeting. The identified
objects share the same content for each participant but are
placed at different positions inside the spherical viewer (see
Figure 3). The current implementation uses manual place-
ment of the shared elements. This will be extended to an
autonomous process by using camera pose estimation meth-
ods. Multiple users can interact and observe the actions of
each other in the meeting room. Currently, users see each
other as a minimalistic avatar with hands attached to them
(see Figure 4). The fully rigged hands from the leap motion
controller are streamed over network to enable users to see
each others exact hand movements. For some interactions
we simulated eye gaze by using a forward vector of the VR

MuC ’19, September 8–11, 2019, Hamburg, Germany

Schäfer et al.

• Take images on desired seating positions
• Pose calibration of each seating position.
• Calibrate the pose of shared interactive elements
Taking images of the desired seating positions is straightfor-
ward but it has proven to be better if rotation stays the same.
Also the height of the camera should be at eye level in seating
position. Identifying the pose of each seating position can be
done with an automated camera calibration framework. Iden-
tifying the pose of shared interactive elements has to be done
manually at this time but can be extended to an automated
process by utilizing camera pose estimation methods as well
as sparse 3D reconstruction of the environment. Necessary
components like gesture interaction, networking, visual rep-
resentation of shared objects, spherical viewer, user avatars
etc. can be reused easily.

5 CONCLUSION AND FUTURE WORK
In this paper, we presented a method for fast and realistic
creation of immersive virtual environments. To avoid creat-
ing complex 3D assets which are expensive, spherical images
are used to create a photo realistic environment. The idea
to use multiple spherical images showing the same scene
but from different viewpoints while adding shared virtual
objects between viewpoints is presented. This approach en-
ables realistic and believable virtual environments which
can be created in a short amount of time. Additionally, our
approach allows us to create virtual meeting experiences
in almost any meeting room by taking multiple spherical
images. Since we used the game engine Unity, it is easily
possible to reuse all of the critical components like spheri-
cal image viewer, shared virtual objects, hand based gesture
interaction and networking code. The presented system pro-
vides the opportunity for all basic interactions necessary to
give a virtual presentation, including shared virtual objects
(projector, TV) between viewpoints and hand/gesture inter-
action. This approach is easily extended for prototyping and
proof-of-concept in other scenarios like novel car interfaces
or training scenarios. In the current state it is possible for
multiple users to collaborate together in the room but there is
only a minimal visual representation of each user. The addi-
tion of more suitable avatars as well as sparse reconstruction
to realize faithful occlusion of more complex avatars will
round off the collaboration experience.

ACKNOWLEDGMENTS
This work was partially funded by the Offene Digitalisierungsal-
lianz Pfalz which is part of the Innovative Hochschule. Grant
Number 03IHS075B.

Figure 4: Virtual objects can be placed by the user in the vir-
tual environment to enable interaction.

headset position in virtual space. This is used for certain
gestures e.g. a swiping gesture towards the projector will
change slides only when the projector is being looked at. We
also use a menu which appears when the left palm is facing
the VR headset. The user can grab objects from the menu
and place them in the virtual world, where they expand and
allow interaction (See Figure 4). Interaction possibilities in-
clude: moving to a different seat, changing projector slides
and preview for upcoming slides.

Multi User Experience
To implement a multi user experience, it is necessary to dis-
tinguish between the local and remote (other) users. We place
the local user always at position (x,y,z) = (0,0,0) in the world
coordinate system. A sphere which has the spherical image
as texture applied to it is placed on that position to achieve
a spherical viewer experience for the local user. In order to
plausibly position the other users, it is important to know
the camera pose of the other viewing points i.e. photos taken
at other positions. Gava et al. [4] provides an overview of the
method we used for automated camera position detection.
Without proper camera alignment the user positions will not
be consistent. Therefore it is required to calibrate all user
spheres into a common coordinate system. Each possible
viewing position requires the following information: image
texture, pose of shared interactive elements and pose of the
other viewing points (user positions). Our network code syn-
chronizes the position of players and the states of interactive
elements as well as the fully rigged hands. We deemed it
necessary to see both hands and full motion of each of the
other users hands since it is an improvement for the overall
user experience of the system.

Creating a new VR Meeting Room
Creating a virtual reality meeting room experience for a new
room has few prerequirements:

REFERENCES
[1] Hayward P. Andres. 2002. A comparison of face-to-face and vir-
tual software development teams. Team Performance Management:

Towards Collaborative Photorealistic VR Meeting Rooms

MuC ’19, September 8–11, 2019, Hamburg, Germany

An International Journal 8 (2002), 39–48.
13527590210425077

https://doi.org/10.1108/

Computing Systems. ACM, 179–188.

[18] Robert Y Wang and Jovan Popović. 2009. Real-time hand-tracking
with a color glove. ACM transactions on graphics (TOG) 28, 3 (2009),
63.

[2] Costas Boletsis. 2018. Virtual Reality for Prototyping Service Journeys.
Multimodal Technologies and Interaction 2, 2 (2018), 14. https://doi.
org/10.3390/mti2020014

[3] Gregoire Dupont de Dinechin and Alexis Paljic. 2019. Automatic
Generation of Interactive 3D Characters and Scenes for Virtual Reality
from a Single-Viewpoint 360-Degree Video. (2019). https://doi.org/10.
1109/VR.2019.8797969

[4] Christiano Couto Gava and Didier Stricker. 2015. A generalized struc-
ture from motion framework for central projection cameras. In Inter-
national Joint Conference on Computer Vision, Imaging and Computer
Graphics. Springer, 256–273.

[5] Thomas Hilfert and Markus König. 2016. Low-cost virtual reality envi-
ronment for engineering and construction. Visualization in Engineering
4, 1 (2016), 2.

[6] WA IJsselsteijn, YAW De Kort, and Karolien Poels. 2013. The game
experience questionnaire. Eindhoven: Technische Universiteit Eindhoven
(2013).

[7] Dongsik Jo, Ki-Hong Kim, and Gerard Jounghyun Kim. 2015. Space-
Time: adaptive control of the teleported avatar for improved AR tele-
conference experience. Computer Animation and Virtual Worlds 26,
3-4 (2015), 259–269.

[8] Dongsik Jo, Ki-hong Kim, and Gerard Jounghyun Kim. 2017. Effects
of Avatar and Background Types on Users’ Co-presence and Trust
for Mixed Reality-Based Teleconference Systems. Casa 2017 (2017),
27–36.

[9] Tuomas Kantonen, Charles Woodward, and Neil Katz. 2010. Mixed real-
ity in virtual world teleconferencing. Proceedings - IEEE Virtual Reality
October (2010), 179–182. https://doi.org/10.1109/VR.2010.5444792

[10] Ruggiero Lovreglio, Vicente Gonzalez, Zhenan Feng, Robert Amor,
Michael Spearpoint, Jared Thomas, Margaret Trotter, and Rafael Sacks.
2018. Prototyping virtual reality serious games for building earthquake
preparedness: The Auckland City Hospital case study. Advanced Engi-
neering Informatics 38 (2018), 670–682. https://doi.org/10.1016/j.aei.
2018.08.018 arXiv:1802.09119

[11] Jameel Malik, Ahmed Elhayek, and Didier Stricker. 2018. Structure-
Aware 3D Hand Pose Regression from a Single Depth Image. In Inter-
national Conference on Virtual Reality and Augmented Reality. Springer,
3–17.

[12] Elias Matsas, George Christopher Vosniakos, and Dimitris Batras.
2018. Prototyping proactive and adaptive techniques for human-
robot collaboration in manufacturing using virtual reality. Robotics
and Computer-Integrated Manufacturing 50, October (2018), 168–180.
https://doi.org/10.1016/j.rcim.2017.09.005

[13] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A Argyros. 2011.
Efficient model-based 3D tracking of hand articulations using Kinect..
In BmVC, Vol. 1. 3.

[14] Thammathip Piumsomboon, Arindam Day, Barrett Ens, Youngho Lee,
Gun Lee, and Mark Billinghurst. 2017. Exploring enhancements for
remote mixed reality collaboration. In SIGGRAPH Asia 2017 Mobile
Graphics & Interactive Applications. ACM, 16.

[15] Ronald Schroeter and Michael A. Gerber. 2018. A Low-Cost VR-Based
Automated Driving Simulator for Rapid Automotive UI Prototyping.
(2018), 248–251. https://doi.org/10.1145/3239092.3267418

[16] Anna H. Smeragliuolo, N. Jeremy Hill, Luis Disla, and David Putrino.
2016. Validation of the Leap Motion Controller using markered motion
capture technology. Journal of Biomechanics 49, 9 (2016), 1742–1750.
https://doi.org/10.1016/j.jbiomech.2016.04.006

[17] Rajinder S Sodhi, Brett R Jones, David Forsyth, Brian P Bailey, and
Giuliano Maciocci. 2013. BeThere: 3D mobile collaboration with spatial
input. In Proceedings of the SIGCHI Conference on Human Factors in

