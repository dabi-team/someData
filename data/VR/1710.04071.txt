JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Automatic Salient Object Detection for Panoramic
Images Using Region Growing and Fixation
Prediction Model

Chunbiao Zhu, Student Member, IEEE, Kan Huang, Thomas H. Li, Ge Li, Member, IEEE,

8
1
0
2

r
p
A
0
1

]

V
C
.
s
c
[

6
v
1
7
0
4
0
.
0
1
7
1
:
v
i
X
r
a

Abstract—Almost all previous works on saliency detection
have been dedicated to conventional images, however, with the
outbreak of panoramic images due to the rapid development
it is becoming more challenging,
of VR or AR technology,
meanwhile valuable for extracting salient contents in panoramic
images.

In this paper, we propose a novel bottom-up salient object
detection framework for panoramic images. First, we employ
a spatial density estimation method to roughly extract object
proposal regions, with the help of region growing algorithm.
Meanwhile, an eye ﬁxation model is utilized to predict visually
attractive parts in the image from the perspective of the human
visual search mechanism. Then, the previous results are combined
by the maxima normalization to get the coarse saliency map.
Finally, a reﬁnement step based on geodesic distance is utilized
for post-processing to derive the ﬁnal saliency map.

To fairly evaluate the performance of the proposed approach,
we propose a high-quality dataset of panoramic images (SalPan).
Extensive evaluations demonstrate the effectiveness of our pro-
posed method on panoramic images and the superiority of the
proposed method against other methods.

Index Terms—Saliency detection, eye ﬁxation, region growing,

panoramic images.

I. INTRODUCTION

A N inherent and powerful ability of human eyes is to

quickly capture the most conspicuous regions from a
scene, and passes them to high-level visual cortexes. The
neurobiological attention mechanism reduces the complexity
of visual analysis and thus makes human visual system con-
siderably efﬁcient in complex scenes.

Early work [17] on computing saliency aimed to model
and predict human gaze on images. Recently the ﬁeld has
expanded to include the segmentation of entire salient regions
or objects [1]. Most works [22], [56], [57], [55], [60], [54], [?]
extract salient regions which exhibit highly distinctive features
compared to their surrounding regions, based on the concept
of center-surround contrast. Moreover, additional prior knowl-
edge for spatial layout of foreground objects and background
can be also used [46], [61], [33], [7]. These assumptions
have been successfully employed to improve the performance
of saliency detection for conventional images with common
aspect ratios. However, existing bottom-up methods [32] show
somewhat poor performance for complex situations. With the
development of neural networks, some algorithms [50] adopt

The authors are with the School of Electronic and Computer Engineering,
Peking University Shenzhen Graduate School, Shenzhen 518055, China (e-
mail: zhuchunbiao@pku.edu.cn; khuang@pkusz.edu.cn; thomas.li@gpower-
semi.com; gli@pkusz.edu.cn).

deep learning based methods to deal with this problem. This
trend is analogous to the biological evolution process, which
can be regarded as evolutionary theory of saliency detection
(shown in the right of Fig.1).

Fig. 1. The comparison of existing saliency detection algorithm between
conventional images and panoramic images.

Recently, panoramic images, which yield wide ﬁelds of
view, become popular in all kinds of media contents and
draw much attention in many practical applications. However,
to accurately calculate visual saliency of panoramic images,
both traditional methods and deep learning based methods
proposed in recent years would fail in some cases of panoramic
images, where complex background is present (shown in the
left of Fig.1). Thus we propose an effective saliency detection
framework for panoramic images.

In this work, we propose an automatic salient object detec-
tion framework for panoramic images using a dual-stage re-
gion growing and ﬁxation prediction model. Panoramic images
exert several distinct characteristics compared to conventional
images. We ﬁnd that spatial density patterns is useful for
images with high resolutions. Therefore, we ﬁrst employ a
spatial density pattern detection method for the panoramic
image to roughly extract the proposal regions, and use a dual-
stage region growing process to get the proposed regions.
Meanwhile, the eye ﬁxation model is deployed in the frame-
work to locate the focus of visual attention in images, which
is inspired by the mechanisms of the human vision system.
Then, the previous saliency information is combined by the
maxima normalization to get the coarse saliency map. Finally,
a geodesic reﬁnement is employed to derive the ﬁnal saliency
map.

The main contributions of this paper are listed as follows:
1. A new automatic salient object detection framework is
proposed for panoramic images, which combines the usage of

Panoramic Image BSCA Amulet OURS  Bad  Bad  GoodFor panoramic images  Using conventional method  Using deep learning method  Using the proposed methodFor conventional imagesImageAmuletBSCA  Bad  Good     Ground TruthVS 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

region growing and eye ﬁxation model. Density map is ﬁrst
introduced in our work as a feature representation for saliency
calculation.

2. To fairly evaluate the performance of the proposed
method, we build a new high-quality panoramic dataset (Sal-
Pan) with an accurate ground truth annotation method which
can eliminate the ambiguous of salient objects. This SalPan
dataset will be released publicly after publication.

3. Compared with both conventional algorithms and deep
learning based algorithms, the proposed method achieves the
state-of-the-art performance on SalPan dataset and other large-
scale salient object detection datasets including the recent
ECSSD [36], DUT-OMRON [51] and SED [4]. In addition, the
proposed algorithm is fast on both modern CPUs and GPUs.
4. The proposed framework can also be tailored for small

target detection.

To our thoughts,

the
perception characteristics of the human visual system for
large-scale visual contents over a wide ﬁeld of view.

this research may help to exploit

The rest of this paper is organized as follows. Section 2
brieﬂy reviews related work. Sections 3 explains the proposed
framework. Section 4 presents the experimental results. Sec-
tion 5 discusses some practical guidelines. Section 6 concludes
this paper.

II. RELATED WORKS

In this section, we have a brief review of classical eye ﬁxa-
tion models, traditional saliency algorithms and deep learning
based saliency detection methods.

A. Eye Fixation Model

The ﬁrst models for saliency prediction were biologically
inspired and based on a bottom-up computational model that
extracted low-level visual features such as intensity, color,
orientation, texture and motion at multiple scales. Itti et al. [17]
proposed a model that combines multiscale low-level features
to create a saliency map. Harel et al. [6] presented a graph-
based alternative that starts from low-level feature maps and
creates Markov chains over various image maps, treating the
equilibrium distribution over map locations as activation and
saliency values. Since the eye ﬁxation model can mimic the
process of the human visual system,
thus, we embed the
ﬁxation prediction model into our framework.

B. Traditional Saliency Algorithm

Saliency detection for conventional images could be imple-
mented based on either top-down or bottom-up models. Top-
down models [31], [11], [10], [49], [?] required high level
interpretation usually provided by training sets in supervised
learning. Contextual saliency was formulated according to the
study of visual cognition: global scene context of an image
was highly associated with a salient object [31]. The most
distinct features were selected by information theory based
methods [10]. Salient objects were detected by joint learning
of a dictionary for object features and conditional random
ﬁeld classiﬁers for object categorization [49]. While these

supervised approaches can effectively detect salient regions
and perform overall better than bottom-up approaches, it is
still expensive to perform the training process, especially data
collection.

In contrary, bottom-up models [32], [37], [58], [23], [59],
[?], [?] did not require prior knowledge such as object cate-
gories, but obtained saliency maps by using low level features
based on the center-surround contrast. They computed feature
distinctness of a target region, e.g., pixel, patch or superpixel,
compared to its surrounding regions locally or globally. For
example, feature difference was computed across multiple
scales, where a ﬁne scale feature map represented the feature
of each pixel while a coarse scale feature map described the
features of surrounding regions [17]. Also, to compute center-
surround feature contrast, spatially neighboring pixels were
assigned different weights [13], or random walk on a graph
was used [19].

Bottom-up based approaches did not need data collection
and training process, consequently requiring little prior knowl-
edge. These advantages make bottom-up approaches more
efﬁcient and easy to implement
in a wide range of real
computer vision applications. A complete survey of these
methods is beyond the scope of this paper and we refer the
readers to a recent survey paper [5] for details. In this paper,
we focus on the bottom-up approach.

C. Deep Learning based Saliency Detection Method

With the performance of deep convolutional neural achiev-
ing near human-level performance in image classiﬁcation and
recognition task. Many algorithms adopt deep learning based
methods [50], [43], [21], [12], [44], [20], [53]. Instead of con-
structing hand-craft features, this kind of top-down methods
have achieved state-of-the-art performance on many saliency
detection datasets. However, deep learning based algorithms
exist the following limitations: 1) need a large number of
annotated data for training. (2) very time-consuming in the
learning process even with GPU of high computation ability.
(3) training instances are not uniformly sampled. (4) sensitive
to noise in training samples and image resolution.

Bottom-up based approaches do not need data collection and
training process, consequently requiring little prior knowledge.
These make bottom-up approaches more suitable for real-time
applications. Meanwhile, bottom-up methods are not sensitive
to image scale, capacity and type. These advantages make
bottom-up approaches more efﬁcient and easy to panoramic
saliency detection. In this paper, we focus on the bottom-up
approach.

III. THE PROPOSED FRAMEWORK

Here we describe the proposed framework in four main
steps. First, we employ a dual-stage region-growing algorithm
for salient regions proposal. Second, an eye ﬁxation prediction
model
is used to ﬁnd visually attractive locations in the
image, which is then combined with proposal regions in
the previous step to form a reliable saliency map. Third, a
maxima normalization is utilized to optimize the previous
saliency information. Fourth, the ﬁnal saliency map is obtained

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 2. The framework of the proposed algorithm. Top: Dual-stage region-growing method. Bottom: Fixation prediction algorithm. The temporal outputs of
the two pathways are integrated using maxima normalization. The ﬁnal output is reﬁned by geodesic reﬁnement.

after a reﬁnement step based on geodesic distance. The main
framework is depicted in Fig. 2s.

A. A Dual-stage Region-growing Based Detection

In this section, we ﬁrst use the efﬁcient Simple Linear
the
Iterative Clustering (SLIC) algorithm [28] to segment
image into smaller superpixel elements in order to capture
the essential local structural information of the image. Then ,
we roughly extract regions that have signiﬁcant and different
densities compared with its neighbors. After obtaining the
proposal regions, we use a dual-stage seed estimation and
growing to improve previous map. One is the foreground seeds
based ranking process, the other is the background seeds based
ranking process. Finally, we merge the two detection results
to get the proposed salient regions.

1) Density Map: The notion of density comes from fractal
theory. Density measures locally the variation of a quantity
over a number of scales. It has been used before for texture
classiﬁcation [30]. The idea is that at small scales naturally
occurring objects (here the texture) change over scale in a
way, that can be modeled as an exponential function of the
scale. Thus the exponent of the function is a good statistical
descriptor of the object which is invariant to a wide range of
spatial transformations. In this work we focus on the density
variation property of the density map.

Let I(x) be a grayscale image and let µ(x, r) be a measure
on R2. For our purposes we choose µ(x, r) to be the sum
of image intensities in a disk of radius r around point x, i.e.
µ(x, r) = (cid:80)
||y−x||≤r I(y). We use the power law to express
µ as a function of r:

µ(x, r) = krd(x),

log(µ(x, r)) = log k + d(x) log r,

d(x) = lim inf

log(µ(x, r))
log r

.

(1)

(2)

(3)

We deﬁne the exponent d(x), also known as Holder expo-
nent, to be the local density function of image I(x) at point
x. Intuitively, it measures the degree of regularity of intensity
variation in a local neighbourhood around point x.

The density map values are the same within regions of
different
intensity as well as within regions of smoothly
varying intensity. In essence, the density map preserves im-
portant
intensity
textural features by responding to abrupt
discontinuities and avoiding smoothly varying regions. In our
work, the obtained density map is utilized as a feature map
the region-
for coarse salient region proposals. We adopt
based contrast method for this purpose. GrabCut [35]is used
to segment the image into a number of regions, then saliency
computation is performed on these regions. The saliency value
of each regions is derived as follows:

Sb(rk) =

(cid:88)

rk(cid:54)=ri

w(ri)Dr(rk, ri),

(4)

where w(ri) is the weight of region ri and Dr(·) is the density
distance metric between the two regions.
n1(cid:88)

n2(cid:88)

f (d1, i)f (d2, j)D(d1, i, d2, j),

(5)

Dr(r1, r2) =

i=1

j=1

where f (dk, i) is the probability of the i-th density dk,i among
all nk densities in the k-th region rk, k = {1, 2}.

In the end, the map is binarized to obtain a mask, which
could be treated as object region proposals. The mask map is
denoted Sb .

2) Foreground Seeds based Region Growing: We denote
the map generated by previous section, Sb, which indicates
proposed density regions.

Unlike previous works[24], [48] that treat some regions as
certain seeds, we provide a more ﬂexible scheme for seeds
estimation. We deﬁne two types of seed elements: strong seeds
and weak seeds. Strong seeds have high probability of belong-
ing to foreground/background while weak seeds have relatively
low probability of belonging to foreground/background. For
foreground seeds, the two types of seeds are selected by:

C +

f ore = {i|Sp(i) >= 2 · mean(Sp)},

C −

f ore = {i|Sp(i) >= mean(Sp) and

Sp(i) < 2 · mean(Sp)},

(6)

(7)

where C + denotes the set of strong seeds and C − weak seeds,
i represent ith superpixel. mean(.) is the averaging function. It

RGB color channelsPanoramic image Dual-stage Region-growing  Fixation PredictionFixation mapRGBRGBRGB channels mapsBackground regionsBackground seeds growingForeground seeds growingForeground regionsDensity regionsSLIC  Geodesic  Refine  Max NormalizeFinal  saliency  mapProposed  regionsCoarse saliency mapJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

is obvious from formula (6)(7) that elements of higher degree
of surroundedness are more likely to be chosen as strong
foreground seeds, which is consistent with intuition.

For saliency calculation based on given seeds, a ranking
method in [52] that exploits the intrinsic manifold structure
of data for graph labelling is utilized. The ranking method is to
rank the relevance of every element to the given set of seeds.
We construct a graph that can represent an whole image as
in work [48], where each node is a superpixel generated by
SLIC.

The ranking procedure is as follows: Given a graph G =
(V, E) ,where the nodes are V and the edges E are weighted
by an afﬁnity matrix W = [wij]n×n. The degree matrix is
deﬁned by D = diag{d11, ..., dnn}, where dii = (cid:80)
j wij.
The ranking function is given by:

g∗ = (D − αW )−1y.

(8)

The g∗ is the resulting vector which stores the ranking
results of each element. The y = [y1, y2, ..., yn]T is a vector
indicating the seed queries.

In this work, the weight between two nodes is deﬁned by:

wij = e−

(cid:107)ci−cj (cid:107)
σ2

,

(9)

where ci and cj denote the mean of the superpixels corre-
sponding to two nodes in the CIE LAB color space, and σ is
a constant that controls the strength of the weight.

Different from [48] that deﬁne yi = 1 if i is a query and
yi = 0 otherwise, we deﬁne yi as the strength of the query
extra. That is, yi = 1 if i is a strong query, and yi = 0.5 if i
is a weak query, and yi = 0 otherwise.

For foreground seeds based ranking, all elements are ranked

by formula (9) given the sets of seeds in (6)(7).

3) Background Seeds based Region Growing: Complemen-
tary to foreground seeds estimation and growing, background
seeds estimation and growing aims to extract regions that
are different from background in feature distribution. We ﬁrst
select a set of background seeds and then calculate saliency
of every image element according to its relevance to these
seeds. We divide the elements on image border into two
categories(strong seeds and weak seeds) as in foreground
situation. We denote the average value of all border elements
as c. The euclidean distance between each feature vector and
the average feature vector is computed by dc = dist(c, c),
the average of dc is denoted by dc. The background seeds are
estimated by:

(10)

(11)

C +

back = {i|dc(i) >= 2 · dc},

C −

back = {i|dc(i) >= dc and dc(i) < 2 · dc},
back denotes strong background seeds, C −

where C +
weak background seeds.

back denotes

Similar to foreground situation,

the value of indication
vector for background seeds y is yi = 1 if i belongs to C +
back,
yi = 0.5 if i belongs to C −
back and 0 otherwise. Relevance
of each element to background seeds is computed by formula
(8). Elements in resulting vector g∗ indicates the relevance of
a node to the background queries, and its complement is the

saliency measure.The saliency map using these background
seeds can be written as:

S(i) = 1 − g∗(i)

i = 1, 2, ..., N.

(12)

The background saliency map and foreground saliency map
is combined by multiplication to form a coarse saliency map,
as shown in the end of stage 1 in Fig.2.

B. Fixation prediction

Whether a location is salient or not largely depends on how
much it attracts human attention. A large number of recent
works on eye ﬁxation prediction have revealed more or less the
nature of this issue. Eye ﬁxation prediction models simulate
the mechanisms of the human visual system, and thus can
predict the probability of a location to attract human attention.
So in this section, we use eye ﬁxation model to help us ensure
which region has more power to grab human attention.

Panoramic images are often with wide ﬁelds of view, and
consequently are computationally more expensive compared
with conventional images. Algorithms based on color contrast,
local information are not suitable for being a preprocessing
step for panoramic images, for these algorithms are time-
consuming and would spend a lot of computational resources.
Thus we are seeking a more efﬁcient method to help us to
rapidly scan the image and roughly locate where attract human
attention. Obviously Fixation prediction models in frequency
domain ﬁt this demand, for these models are computationally
efﬁcient and easy to implement.

The signature model approximately isolate the spatial sup-
port of foreground by taking the sign of the mixture signal
x in the transformed domain and then transform it back to
spatial domain, i.e., by computing the reconstructed image
x = IDCT [sign((cid:98)x)]. (cid:98)x stands for DCT transform of x. The
image signature is deﬁned as

IS(x) = sign(DCT (x)).

(13)

And the saliency map is formed by smoothing the squared
reconstructed image deﬁned above

Sm = g ∗ (x ◦ x),

(14)

where g is a Gaussian kernel.

The image signature is a simple yet powerful descriptor of
natural scenes, and it can be used to approximate the spatial
location of a sparse foreground hidden in a spectrally sparse
background. Compared with other eye ﬁxation models, image
signature has a more efﬁcient implementation, which runs
faster than all other competitors.

To combine the proposed salient regions in previous section
with saliency map Sm produced by image signature, we assign
the saliency value of the proposed salient regions by averaging
the saliency values of all its pixels inside. For convenience we
denote the resulted saliency map as Sp. That is, for a proposed
region p, its saliency value is deﬁned as

Sp(i) = (

(cid:88)

i∈p

Sm(i))/A(p)

, i ∈ p,

(15)

where A(p) denote the number of pixels in the pth region.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

As shown in Fig.3, ﬁxation prediction model reliably locate
visually attractive positions, while density detection pops out
a number of proposal regions. One is for reliable but fuzzy
locating saliency, while the other is for rich pop-out object
proposals. These two step are therefore complementary.

Fig. 3. The illustration of complementary between density detection and
ﬁxation prediction.

C. Maxima Normalization

Fusing saliency detection results of multiple models has
been recognized as a challenging task since the candidate
models are usually developed based on different cues or
assumptions. Fortunately, in our case, the integration problem
is relatively easier since we only consider the outputs from two
pathways. Since there is no prior knowledge or other top-down
guidance can be used, it is safer to utilize the map statistics
to determine the importance of each pathway. Intuitively
in the ﬁnal integration stage, we combine the results from
two pathways by summing them after Maxima Normalization
(MN) (Algorithm 1).

Algorithm 1 Maxima Normalization Nmax(Sp, t)
Input: Previous map Sp, thresh of local maxima t = 0.1;
Output: Normalized Saliency Map SN ;
1: Set the number of maxima NM = 0 ;
2: Set the sum of the maxima VM = 0 ;
3: Set Global Maxima GM = max(S) ;
4: for all pixel (x, y) of S do :
if S(x, y) > t then
5:
6: R = S(i, j)|i = x − 1, x + 1, j = y − 1, y + 1 .
7:
8:
9:
10:
11:
12: end for
13: SN = S · (GM − VM /NM )2/GM .
14: return return Normalized map SN

if S(x, y) > max(R) then
VM = VM + S(x, y) .
NM = NM + 1 .
end for

end for

The Maxima Normalization operator Nmax(.) was originally
proposed for the integration of conspicuous maps from multi-
ple feature channels [17], which has been demonstrated very
effective and has a very convincing psychological explanation.

D. Geodesic reﬁnement

The ﬁnal step of our proposed approach is reﬁnement
with geodesic distance [39]. The motivation underlying this
operation is based on thought that determining saliency of an
element as weighted sum of saliency of its surrounding ele-
ments, where weights are corresponding to Euclidean distance,
has a limited performance in uniformly highlighting salient
object. We tend to ﬁnd a solution that could enhance regions
of salient object more uniformly. From recent works [61], we
found the weights may be sensitive to geodesic distance.

The input image is ﬁrst segmented into a number of su-
perpixels based on linear spectral clustering method [13] and
the posterior probability of each superpixel is calculated by
maxima normalization operation SN of all its pixels inside.
For jth superpixel, if its posterior probability is labeled as
SN (j), thus the saliency value of the qth superpixel is reﬁned
by geodesic distance as follows:

S(q) =

J
(cid:88)

j=1

wqj · SN (j),

(16)

where J is the total number of superpixels, and wqj will be a
weight based on geodesic distance [61] between qth superpixel
and jth superpixel.

First, an undirected weight graph has been constructed con-
necting all adjacent superpixels (ak, ak+1) and assigning their
weight dc(ak, ak+1) as the Euclidean distance between their
saliency values which are derived in the previous section. Then
the geodesic distance between two superpixels dg(p, i) can be
deﬁned as accumulated edge weights along their shortest path
on the graph:

dg(p, i) =

min
a1=p,a2,a3,...,an=i

n−1
(cid:88)

k=1

dc(ak, ak+1).

(17)

In this way we can get geodesic distance between any two
superpixels in the image. Then the weight δpi is deﬁned as

wqj = exp{−

d2
g(p, i)
2σ2
c

},

(18)

where σc is the deviation for all dc values. From formula (6)
we can easily conclude that when p and i are in ﬂat region,
saliency value of i would have a higher contribution to saliency
value of p, and when p and i are in different regions between
which a steep slope is existed, saliency value of i tends to
have a less contribution to saliency value of p.

Since an object often contains some homogenous parts, the
initial saliency value of a superpixel could be spread to the
other connected homogenous parts, indicating the propagation
may be achieved through connectivity. This can also be
observed in the background where image background can
be divided into large homogenous regions. Noting that the
coarse saliency could render more saliency to the target object
while less to the background (Fig.2). Hence, low saliency in
background part could be spread over the entire background
region after propagation. Eventually, the background could be
suppressed to very low saliency values.

Density RegionsPrediction LocationsInputOutput  Rich  ReliableJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

value, AUC value, MAE (Mean Absolute Error) value, time-
consuming with coding type and F-measure.

Besides, We also adopt a new structural similarity mea-
sure known as Structure-measure (S-measure, proposed in
ICCV17) [8] which simultaneously evaluates region-aware and
object aware structural similarities between a saliency map and
a ground-truth map.

The precision is deﬁned as:

P recision =

(cid:107)pi | d(pi) ≥ dt ∩ pg(cid:107)
(cid:107)pi | d(pi) ≥ dt(cid:107)

,

(19)

where pi | d(pi) ≥ dt indicates the set that binarized from
a saliency map using threshold dt. pg is the set of pixels
belonging to groundtruth salient object.

The recall is deﬁned as:

Recall =

(cid:107)pi | d(pi) ≥ dt ∩ pg(cid:107)
(cid:107)pg(cid:107)

.

(20)

The precision-recall curve is plotted by connecting the P-R
scores for all thresholds.

The MAE is formulated as:
(cid:80)N

M AE =

i=1 (cid:107)GTi − Si(cid:107)
N

,

(21)

where N is the number of the testing images, GTi is the area
of the ground truth of an image i, Si is the area of the result
of an image i.

The F-measure is formulated as:

F − measure =

(1 + β2) × P recision × Recall
β2 × P recision + Recall

,

(22)

where β2 is set to 0.3 as did in many literatures.

The S-measure is formulated as:

S − measure = S = α × So + (1 − α) × Sr,

(23)

where So and Sr are region-aware and object-aware structural
similarity evaluation value, respectively.

C. Ablation study

We ﬁrst validate the effectiveness of each step in our
method: foreground seeds based region-growing (R-F), back-
ground seeds based region-growing (R-B),a dual-stage region-
growing based detection (R-M), ﬁxation prediction saliency
detection (FP), maxima normalization fusion (MN) and
geodesic reﬁnement (GR). Table.I shows the validation results
on SalPan dataset. We can clear see the accumulated process-
ing gains after each step, and the ﬁnal saliency results shows
a good performance. After all, it proves that each steps in our
algorithm is effective for generating the ﬁnal saliency maps.

*

MAE
F-m
S-m

Step 1
R-B
.287
.694
.731

R-M
.271
.715
.737

R-F
.291
.689
.727

Step 2
FP
.311
.646
.694

Step 3
MN
.256
.734
.752

Step 4
GR
.231
.767
.783

TABLE I
THE RESULTS OF MAE, F-MEASURE AND S-MEASURE AT EACH STEPS OF
THE PROPOSED ALGORITHM. STEP 1 AND STEP 2 ARE THE CONCURRENT
PROCEDURE.

Fig. 4. The process of generating the ground truth for the SalPan dataset.

IV. EXPERIMENTS

To fairly evaluate the performance of the proposed frame-
work, we build a new dataset of panoramic landscape im-
ages (SalPan), and evaluate the performance of the proposed
saliency detection algorithm compared with 12 state-of-the-
arts methods. More experimental analysis on the effectiveness
of our method are given as follows.

A. Datasets

We collect a new panoramic dataset SalPan composed of
123 panoramic images. In general, saliency may be ambiguous
in images with highly complex scenes and wide ﬁelds of view,
thus we propose an accurate ground truth annotation method
to eliminate the ambiguous.

To build a ground-truth for SalPan dataset we ﬁrst conducted
a comprehensive user study. To identify where participants
were looking while watching the ﬁlms, we monitor their
eye movements using a Gazepoint GP3 Eye Tracker. Image
presentation was controlled using the Gazepoint Analysis
Standard software. For the purpose of the study we recruited
10 participants (5 males, 5 females). Ages ranged from 20 to
57 with the mean age of 25. All the participants had normal or
corrected-to-normal vision and were naive to the underlying
purposes of the experiment. Then, we record the tracking path
and allocate values to each eye movement location according
to the times of visual attention. Finally, we pick up those
values above the average and label the ground truth with
complete objects corresponding attention points.

Meanwhile, we make the image acquisition and image
annotation independent to each other, we can avoid dataset
design bias, namely a speciﬁc type of bias that is caused
by experimenters’ unnatural selection of dataset images. After
picking up the salient regions, we adhere to the following rules
to build the ground truth:

• disconnected regions of the same objects are labeled

separately;

• solid regions are used to approximate hollow objects, such

as bike wheels.

The process of labeling the ground truth is shown in Fig.

4.

B. Evaluation indicators

Experimental evaluations are based on standard measure-
ments including precision-recall curve, precision value, recall

Input ImageGround TruthSaliency Value = { Fixation Score | Fixation Score   Mean Value }Eye-tracking experimentImage with fixationAfter filtrationJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Fig. 5. Quantitative evaluation on SalPan dataset. (a)shows the PR curve. (b)shows the comparison of precision, recall and F-measure scores. (c)shows
F-measure curve. (d)shows S-measure scores.

Fig. 6. Visual comparisons of different saliency algorithms on SalPan dataset.

Fig. 8 show two group visual process of each steps of the
proposed method. Step 1: Dual-stage Region growing. Step
2: Fixation Prediction. Step 3: Max Normalization. Step 4:
Geodesic Reﬁnement. From the process shown in the ﬁgure,
we can see the details of our framework and how each step
contribute to the ﬁnal result.

D. Comparison

To illustrate the effectiveness of our algorithm, we com-
pare our proposed methods with other 12 state-of-the-art
ones including 9 conventional algorithms (FT [1], NPS [15],
CB [18], SPL [26], MSS [41], BSCA [32], LPS [23], HS [37],
DRFI [42]) and 3 deep learning based algorithms (KSR [45],
WSS [43] and Amulet [50]). We use the codes provided by the
authors to reproduce their experiments. For all the compared
methods, we use the default settings suggested by the authors.
Fig. 5 compares the PR curves, where we see that the
proposed algorithm achieves a much higher performance than
that of the existing methods.

As observed in Fig. 5, we see that the proposed algorithm
has a higher F-measure score and S-measure score than any
other competitors.

We measure the MAE value, precision value, recall value,
AUC value, F-measure value and S-measure value using a
resulting saliency map against the ground truth saliency map,
which are shown in Table.II. We have seen that the proposed

algorithm also achieves the best performance and outperforms
all other compared methods.

We also compare the average execution times of the pro-
posed algorithm and the other methods in Table II. Most of
the methods including the proposed one are implemented using
MATLAB and executed on an Intel i7 3.4 GHz CPU with 16
GB RAM. Results show that most of the existing methods
consume more time than the proposed algorithm.

In summary, from the comparison, we can conclude that our
saliency results are more robustness and efﬁcient on SalPan
dataset. Besides,
the visual comparisons shown in Fig. 6
clearly demonstrate the advantages of the proposed method.
We can see that our method can extract both single and
multiple salient objects precisely. In contrast, the compared
methods may fail in some situations.

V. DISCUSSION

Comparison in conventional datasets. To further fairly
verify the performance of the proposed method. We also
compare our algorithm with other 12 state-of-the-art ones on
3 public saliency detection datasets, including ECSSD [36],
DUT-OMRON [51] and SED [4].

Visual comparisons of salient region detection results are
shown in Fig. 9. GT represents Ground Truth. The proposed
method also shows good results on conventional datasets,
which is in consistent with panoramic datasets.

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91RecallPrecisionOURSAmuletWSSKSRDRFIHSLPSBSCANPSFTCBSPLMSS00.10.20.30.40.50.60.70.8PrecisionRecallF-measure(a)(b)(c)(d)00.10.20.30.40.50.60.70.80.9S-measure00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91ThresholdF-MeasureOURSAmuletWSSKSRDRFIHSLPSBSCANPSFTCBSPLMSSImage#1GTAmuletHSOURSImage#2GTAmuletHSOURSImage#3Image#4GTGTAmuletAmuletWSSWSSKSRFTNPSBSCALPSHSDRFIOURSOURSKSRFTNPSBSCALPSHSDRFIWSSKSRFTNPSBSCALPSDRFIWSSKSRFTNPSBSCALPSDRFICBSPLMSSCBSPLCBSPLCBSPLMSSMSSMSSJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

*
FT
NPS
CB
SPL
MSS
BSCA
LPS
HS
DRFI
KSR
WSS
Amulet
OURS

MAE Value
0.3868
0.3613
0.4238
0.3125
0.4627
0.4137
0.3697
0.4090
0.3592
0.3222
0.3090
0.3179
0.2744

AUC Value
0.5072
0.4213
0.6515
0.4517
0.5221
0.5115
0.5184
0.6816
0.7021
0.4608
0.6507
0.6321
0.8024

Precision Value
0.4942
0.3973
0.5112
0.4324
0.5185
0.4716
0.4867
0.5212
0.6319
0.4345
0.6687
0.5962
0.7614

Recall Value
0.4286
0.4195
0.5398
0.3861
0.4234
0.4067
0.4794
0.5692
0.5710
0.3507
0.6380
0.6119
0.6753

F-measure
0.4643
0.4053
0.5125
0.4060
0.4621
0.4360
0.4810
0.5597
0.6072
0.4056
0.6546
0.6069
0.7142

S-measure
0.5580
0.4722
0.6095
0.5156
0.5912
0.6079
0.5812
0.6395
0.6402
0.5397
0.7579
0.7121
0.7832

Time(s)
0.593
2.397
4.196
1.926
3.893
1.876
2.193
2.896
1.903
2.537
2.751
3.216
1.075

Code Type
C
M & C
M & C
M & C
M & C
M & C
M & C
M & C
M & C
M & C
M & C
M & C
M & C

TABLE II
EVALUATION INDICATORS ON SALPAN DATASET. M REPRESENTS MATLAB, C REPRESENTS C++. THE BEST TWO SCORES ARE SHOWN IN RED AND BLUE
COLORS, RESPECTIVELY.

VI. CONCLUSION

In this paper, we proposed a novel bottom-up saliency
detection framework for panoramic images. We ﬁrst employed
a spatial density pattern detection method based on dual-stage
region growing for the panoramic image to obtain the proposal
regions. Meanwhile, the ﬁxation prediction model was em-
bedded into the framework to locate the focus of attention in
images. Then, the previous saliency information was combined
by the maxima normalization to obtain the coarse saliency
map. Finally, a geodesic reﬁnement was utilized to get the
ﬁnal saliency map.
Experimental

the proposed
results demonstrated that
saliency detection algorithm provided reliable saliency maps
for panoramic images, and outperformed the recent state-of-
the-art saliency detection methods qualitatively and quantita-
tively. Moreover, the proposed algorithm also yielded compet-
itive performance to the existing saliency detection methods
on the conventional image dataset with common aspect ratios.
Our future research efforts include the extension of SalPan
dataset and the evaluation of algorithm performance on more
diverse panoramic scenes. To encourage future work, we will
make the source codes, experiment data, SalPan dataset for
the research community public.

ACKNOWLEDGMENT

This work was supported by the grant of National Nat-
ural Science Foundation of China(No.U1611461), the grant
of Science and Technology Planning Project of Guangdong
Province, China(No.2014B090910001) and the grant of Shen-
zhen PeacockPlan(No.20130408-183003656).

REFERENCES

[1] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned
salient region detection. In Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on, pages 1597–1604, 2009.
[2] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of
image windows. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 34(11):2189, 2012.

[3] A. Borji. Boosting bottom-up and top-down visual features for saliency
estimation. In Computer Vision and Pattern Recognition, pages 438–445,
2012.

[4] A. Borji. What is a salient object? a dataset and a baseline model
IEEE Transactions on Image Processing,

for salient object detection.
24(2):742, 2015.

Fig. 7. The proposed algorithm is applied in small target detection. (a1)-
(a5) represent different frames of original video.(b1)-(b5) represent different
frames of the proposed algorithm detection results. (c1)-(c5) represent differ-
ent frames of the ground truth.

Small target detection. It is very interesting to ﬁnd that
the proposed automatic salient object detection algorithm for
panoramic images using region growing and eye ﬁxation
model is also valid for small target detection. We present a
part of our experimental results on the small target dataset [27].
Small target detection plays an important role in many com-
puter vision tasks, including early alarming system, remote
tracking. The experimental results by
sensing and visual
applying the proposed algorithm to small target detections in
Fig. 5, respectively, which support our claim.

The underlying reason why the proposed algorithm can be
applied in small target detection and conventional images’
saliency detection is that: the panoramic images also contain
small objects and big objects, so, the image with the small
target and conventional image can be seen as the part of its.
Therefore, we claim that the proposed automatic salient
object detection algorithm is not only conﬁned to panoramic
images but also conventional images and the images with small
target.

Limitation of the proposed method. Since our method
is bottom-up methods, which is not sensitive to the edge
information. It sometimes fails to render complete boundary
information in very complex scenarios. We hope to mitigate
this issue by exploring various forms of edge processing
methods in the future.

(a1) Image#001(a2) Image#100(a3) Image#200(a4) Image#300(a5) Image#400(b1) OURS#001(b2) OURS#100(b3) OURS#200(b4) OURS#300(b5) OURS#400(c1) GT#001(c2) GT#100(c3) GT#200(c4) GT#300(c5) GT#400JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

Fig. 8. Visual process of detailed steps of the proposed method. Step 1: Dual-stage Region growing. Step 2: Fixation Prediction. Step 3: Max Normalization.
Step 4: Geodesic Reﬁnement.

[5] A. Borji, M. M. Cheng, H. Jiang, and J. Li. Salient object detection: A

survey. Eprint Arxiv, 16(7):3118, 2014.

[6] M. Cerf, J. Harel, W. Einh?user, and C. Koch. Predicting human gaze
using low-level saliency combined with face detection. In International
Conference on Neural Information Processing Systems, pages 241–248,
2008.

[7] M. M. Cheng, J. Warrell, W. Y. Lin, S. Zheng, V. Vineet, and N. Crook.
Efﬁcient salient region detection with soft image abstraction. In IEEE
International Conference on Computer Vision, pages 1529–1536, 2013.
[8] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji. Structure-measure:
A new way to evaluate foreground maps. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 4548–
4557, 2017.

[9] S. Frintrop. General object tracking with a component-based target de-
scriptor. In IEEE International Conference on Robotics and Automation,
pages 4531–4536, 2010.

[10] D. Gao, S. Han, and N. Vasconcelos. Discriminant saliency, the detection
of suspicious coincidences, and applications to visual recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(6):989–
1005, 2009.

[11] D. Gao and N. Vasconcelos. Discriminant saliency for visual recognition

from cluttered scenes. Advances in Neural Information Processing
Systems, 17:481–488, 2004.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In IEEE
International Conference on Computer Vision, pages 1026–1034, 2016.
[13] A. Hornung, Y. Pritch, P. Krahenbuhl, and F. Perazzi. Saliency ﬁlters:
Contrast based ﬁltering for salient region detection. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 733–740, 2012.

[14] X. Hou and L. Zhang. Saliency detection: A spectral residual approach.
In IEEE Conference on Computer Vision and Pattern Recognition, pages
1–8, 2007.

[15] X. Hou and L. Zhang. Dynamic visual attention: Searching for coding
In Conference on Neural Information Processing
length increments.
Systems, Vancouver, British Columbia, Canada, December, pages 681–
688, 2009.

[16] L. Itti. Automatic foveation for video compression using a neurobiologi-
cal model of visual attention. IEEE Transactions on Image Processing A
Publication of the IEEE Signal Processing Society, 13(10):1304, 2004.
[17] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual
attention for rapid scene analysis. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(11):1254–1259, 1998.

SLICImageRGB based Fixation Map Fixation MapSaliency MapProposed RegionsDensity RegionsSourceStep 1Step 2Step 3Step 4Coarse Saliency MapBackground Seeds GrowingForeground Seeds GrowingBackground RegionsForeground RegionsImageSLICBackground Seeds GrowingForeground Seeds GrowingForeground RegionsDensity RegionsBackground RegionsProposed RegionsRGB based Fixation Map Fixation MapCoarse Saliency MapGroup 1Group 2Saliency MapSourceStep 1Step 2Step 3Step 4JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

Fig. 9. Visual comparison with 12 state-of-the-art ones including 3 deep learning based algorithms (KSR [45], WSS [43], Amulet [50]) and 9 conventional
algorithms (FT [1], NPS [15], CB [18], SPL [26], MSS [41], BSCA [32], LPS [23], HS [37], DRFI [42]).

SourceGTOURSAmuletWSSKSRDRFIHSLPSBSCAMSSSPLCBNPSFTJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

ranking for saliency detection. In European Conference on Computer
Vision, pages 450–466. Springer, 2016.

[46] Y. Wei, F. Wen, W. Zhu, and J. Sun. Geodesic Saliency Using

Background Priors. Springer Berlin Heidelberg, 2012.

[47] C. Yang, L. Zhang, and H. Lu. Graph-regularized saliency detection
IEEE Signal Processing Letters,

with convex-hull-based center prior.
20(7):637–640, 2013.

[48] C. Yang, L. Zhang, H. Lu, X. Ruan, and M. H. Yang. Saliency detection
In IEEE Conference on Computer

via graph-based manifold ranking.
Vision and Pattern Recognition, pages 3166–3173, 2013.

[49] J. Yang. Top-down visual saliency via joint crf and dictionary learning.
In Computer Vision and Pattern Recognition, pages 2296–2303, 2012.
[50] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. Amulet: Aggre-
gating multi-level convolutional features for salient object detection. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 202–211, 2017.

[51] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection by multi-
In Computer Vision and Pattern Recognition,

context deep learning.
pages 1265–1274, 2015.

[52] Zhou, Dengyong, Weston, Jason, Gretton, Arthur, Bousquet, Olivier,
Sch?lkopf, and Bernhard. Ranking on data manifolds. Advances in
Neural Information Processing Systems, pages 169–176, 2013.

[53] C. Zhu, X. Cai, H. Kan, L. Thomas.H, and L. Ge. Pdnet: Prior-model
In 2018

guided depth-enhanced network for salient object detection.
International Conference on Multimedia and Expo, 2018.

[54] C. Zhu, K. Huang, and G. Li. An innovative saliency guided roi selection
In DCC, pages 438–438,

model for panoramic images compression.
2018.

[55] C. Zhu and G. Li. A multilayer backpropagation saliency detection
algorithm and its applications. Multimedia Tools and Applications, Mar
2018.

[56] C. Zhu, G. Li, X. Guo, W. Wang, and R. Wang. A multilayer
backpropagation saliency detection algorithm based on depth mining.
In CAIP, pages 14–23, 2017.

[57] C. Zhu, G. Li, W. Wang, and R. Wang. An innovative salient object
detection using center-dark channel prior. In 2017 ICCVW, pages 1509–
1515, Oct 2017.

[58] C. Zhu, G. Li, W. Wang, and R. Wang. An innovative salient object
In The IEEE International

detection using center-dark channel prior.
Conference on Computer Vision (ICCV), Oct 2017.

[59] C. Zhu, G. Li, W. Wang, and R. Wang. Salient object detection with
complex scene based on cognitive neuroscience. In 2017 IEEE Third
International Conference on Multimedia Big Data (BigMM), pages 33–
37, April 2017.

[60] C. Zhu, T. H. Li, and G. Li. Towards automatic wild animal detection in
low quality camera-trap images using two-channeled perceiving residual
pyramid networks. In 2017 ICCVW, pages 2860–2864, Oct 2017.
[61] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimization from robust
In Computer Vision and Pattern Recognition,

background detection.
pages 2814–2821, 2014.

[18] H. Jiang, J. Wang, Z. Yuan, T. Liu, N. Zheng, and S. Li. Automatic
salient object segmentation based on context and shape prior. In British
Machine Vision Conference, 2011.

[19] J. S. Kim, J. Y. Sim, and C. S. Kim. Multiscale saliency detection using
random walk with restart. IEEE Transactions on Circuits and Systems
for Video Technology, 24(2):198–210, 2014.

[20] G. Li and Y. Yu. Visual saliency based on multiscale deep features. In
Computer Vision and Pattern Recognition, pages 5455–5463, 2015.
[21] G. Li and Y. Yu. Deep contrast learning for salient object detection. In
Computer Vision and Pattern Recognition, pages 478–487, 2016.
[22] G. Li and C. Zhu. A three-pathway psychobiological framework of
salient object detection using stereoscopic technology. In 2017 ICCVW,
pages 3008–3014, Oct 2017.

[23] H. Li, H. Lu, Z. Lin, X. Shen, and B. Price.

Inner and Inter Label
Propagation: Salient Object Detection in the Wild. New Park Pub.,,
2015.

[24] X. Li, H. Lu, L. Zhang, R. Xiang, and M. H. Yang. Saliency detection
via dense and sparse reconstruction. In IEEE International Conference
on Computer Vision, pages 2976–2983, 2013.

[25] C. C. Lin, S. U. Pankanti, K. N. Ramamurthy, and A. Y. Aravkin.
In IEEE Conference
Adaptive as-natural-as-possible image stitching.
on Computer Vision and Pattern Recognition, pages 1155–1163, 2015.
[26] Z. Liu, X. Zhang, S. Luo, and O. L. Meur. Superpixel-based spatiotem-
poral saliency detection. IEEE Transactions on Circuits and Systems for
Video Technology, 24(9):1522–1540, 2014.

[27] J. Lou, W. Zhu, H. Wang, and M. Ren. Small target detection combining
regional stability and saliency in a color image. Multimedia Tools and
Applications, 76(13):14781–14798, 2017.

[28] A. Lucchi, K. Smith, R. Achanta, G. Knott, and P. Fua. Supervoxel-
based segmentation of mitochondria in em image stacks with learned
shape features. IEEE Transactions on Medical Imaging, 31(2):474–486,
2012.

[29] Y. Luo, J. Yuan, P. Xue, and Q. Tian. Saliency density maximization
for efﬁcient visual objects discovery. IEEE Transactions on Circuits and
Systems for Video Technology, 21(12):1822–1834, 2011.

[30] D. R. G. J. Miller. Viewpoint invariant texture description using fractal
analysis. International Journal of Computer Vision, 83(1):85–100, 2009.
[31] A. Oliva, A. Torralba, M. S. Castelhano, and J. M. Henderson. Top-
In International
down control of visual attention in object detection.
Conference on Image Processing, 2003. ICIP 2003. Proceedings, pages
I–253–6 vol.1, 2003.

[32] Y. Qin, H. Lu, Y. Xu, and H. Wang. Saliency detection via cellular
automata. In Computer Vision and Pattern Recognition, pages 110–119,
2015.

[33] M. Ran, A. Tal, and L. Zelnikmanor. What makes a patch distinct? In
IEEE Conference on Computer Vision and Pattern Recognition, pages
1139–1146, 2013.

[34] R. Rosenholtz, A. Dorai, and R. Freeman. Do predictions of visual
perception aid design? Acm Transactions on Applied Perception, 8(2):1–
20, 2011.

[35] C. Rother, V. Kolmogorov, and A. Blake. ”grabcut”: interactive fore-
ground extraction using iterated graph cuts. In ACM SIGGRAPH, pages
309–314, 2004.

[36] J. Shi, Q. Yan, X. Li, and J. Jia. Hierarchical image saliency detection
on extended cssd. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(4):717–729, 2016.

[37] J. Shi, Q. Yan, X. Li, and J. Jia. Hierarchical image saliency detection
on extended cssd. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(4):717, 2016.

[38] C. Siagian and L. Itti. Biologically inspired mobile robot vision
localization. IEEE Transactions on Robotics, 25(4):861–873, 2009.
[39] J. B. Tenenbaum, S. V. De, and J. C. Langford. A global geo-
Science,

metric framework for nonlinear dimensionality reduction.
290(5500):2319, 2000.

[40] J. Tilke, K. Ehinger, F. Durand, and A. Torralba. Learning to predict

where humans look. 30(2):2106–2113, 2009.

[41] N. Tong, H. Lu, L. Zhang, and R. Xiang. Saliency detection with multi-
IEEE Signal Processing Letters, 21(9):1035–1039,

scale superpixels.
2014.

[42] J. Wang, H. Jiang, Z. Yuan, M.-M. Cheng, X. Hu, and N. Zheng. Salient
object detection: A discriminative regional feature integration approach.
International Journal of Computer Vision, 123(2):251–268, Jun 2017.

[43] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan.
Learning to detect salient objects with image-level supervision.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 136–145, 2017.

[44] L. Wang, L. Wang, H. Lu, P. Zhang, and R. Xiang. Saliency detection
In European Conference

with recurrent fully convolutional networks.
on Computer Vision, pages 825–841, 2016.

[45] T. Wang, L. Zhang, H. Lu, C. Sun, and J. Qi. Kernelized subspace

