9
1
0
2

l
u
J

1

]

V
C
.
s
c
[

2
v
2
9
2
6
0
.
7
0
7
1
:
v
i
X
r
a

STag: A Stable Fiducial Marker System

Burak Benligiray

Cihan Topal

Cuneyt Akinlar

Abstract

Fiducial markers provide better-deﬁned features than the ones natu-
rally available in the scene. For this reason, they are widely utilized in
computer vision applications where reliable pose estimation is required.
Factors such as imaging noise and subtle changes in illumination induce
jitter on the estimated pose. Jitter impairs robustness in vision and
robotics applications, and deteriorates the sense of presence and immer-
sion in AR/VR applications. In this paper, we propose STag, a ﬁducial
marker system that provides stable pose estimation. STag is designed to
be robust against jitter factors, thus sustains pose stability better than the
existing solutions. This is achieved by utilizing geometric features that
can be localized more repeatably. The outer square border of the marker
is used for detection and homography estimation. This is followed by a
novel homography reﬁnement step using the inner circular border. After
reﬁnement, the pose can be estimated stably and robustly across viewing
conditions. These features are demonstrated with a comprehensive set
of experiments, including comparisons with the state of the art ﬁducial
marker systems.

1

Introduction

Fiducial markers are artiﬁcial patterns that combine fast and accurate pose
estimation with easy and inexpensive deployment [1]. These properties make
them particularly useful for prototyping. They were ﬁrst proposed to be used
for augmented reality [2, 3, 4], and are still relevant in the recent virtual reality
boom [5]. In addition, they see increasing use in systems that require to interact
precisely and responsively with the real world, including robotics [6, 7, 8].

Stability is a key metric of pose estimation, and is especially critical for
certain applications. For example, when using the estimated pose in a mixed
reality application, instability causes jittery graphics, which is more immersion-
breaking than a constant misalignment. Pose estimation instability is also highly
undesirable in control applications, as it causes oscillatory behavior. Therefore,
ﬁducial marker systems should aim to improve stability along with the usual
performance metrics, such as detection robustness, speed and library size.

Accepted to be published in Image and Vision Computing.
Source code, supplementary video: https://github.com/bbenligiray/stag.

1

 
 
 
 
 
 
Figure 1: The poses estimated with the proposed marker are used to render 3D
boxes.

The marker pose is estimated using the geometric feature localizations. For
better pose stability, these localizations have to be repeatable. Therefore, using
geometric features that can be localized better will improve pose stability. In
addition, a marker design must provide adequate projective constraints to esti-
mate the pose. To satisfy both of these criteria, we propose a combination of
geometric features to be used as the marker design.

In this paper, we propose STag, a ﬁducial marker system that provides
stable localization without using any temporal ﬁltering. This is achieved by
the hybrid marker design seen in Figure 1. The outer square border is used
for detection and homography estimation, and the inner circular border is used
to reﬁne the estimated homography. Since both of these features are simple,
they do not degrade when seen from long distances and acute viewing angles,
resulting in robust detection. The homography reﬁnement step that provides
stability is unique in that it uses a single conic correspondence to optimize
the estimated homography. To support the use of a wide range of numbers of
markers, we generated various marker libraries by adapting the lexicographic
generation algorithm.

2 Related Work

ARToolkit [9] markers are encoded with patterns chosen by the user, each of
which is associated with an ID (see Figure 2a). These patterns are densely
sampled and decoded using a nearest neighbor method. This approach has
error detection and correction functionality, but its performance depends on
the speciﬁc set of patterns in the library. Matrix-like coding option is added to
ARToolkit with an extension [16].

ARTag [10] is the ﬁrst marker system with forward error correction capability
based on digital coding methods (see Figure 2b). This inﬂuenced the following

2

(a) ARToolkit [9]

(b) ARTag [10]

(c) AprilTag [6]

(d) ArUco [11]

(e) TRIP [12]

(f) RUNE-Tag [13] (g) ChromaTag [14]

(h) CCTag [15]

Figure 2: Marker designs of some of the notable ﬁducial marker systems in the
literature.

studies to place a heavier emphasis on coding. Additionally, ARTag is the ﬁrst
marker system to detect markers from edges.

AprilTag [6] shares the square barcode type design (see Figure 2c), but the
user chooses the size of the encoding grid. Other recent marker systems also
provide marker variants with diﬀerent bandwidths [17, 11]. AprilTag is the ﬁrst
marker system to use lexicodes, greedily generated codes with error detection
and correction capabilities.

ArUco [11] has the unique feature of calculating an occlusion mask with
tiled markers, which can prove useful in mixed reality applications (see Fig-
ure 2d). The original ArUco marker library is created using a stochastic lexicode
generation algorithm. Better libraries are added using a mixed integer linear
programming approach [18].

TRIP [12] uses a circular design (see Figure 2e). Markers have a ring in
the middle for detection, and two outer rings for binary encoding. Outer rings
are divided into aligned slices. The black slice pair indicates the starting point
of the encoding. Relying on singular features to resolve rotation ambiguity for
circular designs is not unique to TRIP [19, 20]. Having such regions is not
preferable, as occluding them will prevent the marker from being detected.

RUNE-Tag [13, 17] does not have a border in the traditional sense. The
coding dots are arranged in a circular shape, and act as candidate generating
features (see Figure 2f). The pose is estimated by using the coding dots as
point correspondences. This approach results in a ﬁnite pose ambiguity similar
to square markers, which can be resolved by decoding. The ﬁnely detailed
design provides many correspondence points, resulting in stable pose estimation.
However, these ﬁne details are prone to degradation under suboptimal viewing
conditions.

3

The literature tends to use monochromatic designs, as using colors will re-
quire certain photometric assumptions, or a calibration step [1]. ChromaTag [14]
modiﬁes AprilTag to use colors for both candidate detection and decoding (see
Figure 2g). The detection algorithm only runs on areas where both violet and
green are present, which decreases the detection time signiﬁcantly.

The systems we have mentioned until now are designed for pose estimation
from a single marker, which is useful for applications such as augmented real-
ity and robotics. Square designs are dominant among these markers, as their
corners provide the four correspondences required to estimate the homography.
Photogrammetry applications come with a slightly diﬀerent requirement: Each
marker needs to provide a single highly accurate correspondence. In these appli-
cations, circular markers, and especially markers composed of concentric circles
excel.

CCTag [15] is an improvement over the concentric circles commonly used in
photogrammetry (see Figure 2h). Its main contribution is robustness against
motion blur, an important issue in tracking applications. Due to the reasons
described above, CCTag does not estimate the pose of the marker, but only
locates the projection of the marker center, similar to some earlier circular
ﬁducial markers [21].

3 Stable Marker Design

To understand how stability can be improved, let us discuss what causes in-
stability. To estimate the pose of a ﬁducial marker, we localize its geometric
features, and use them as correspondences. Therefore, the uncertainty in the
estimated pose is a result of the uncertainty in feature localizations. We are
going to investigate if we can improve stability further by utilizing geometric
features that can be localized more certainly.

3.1 Geometric Features

Various geometric features can be used to generate correspondences for pose
estimation:

• Lines are used in square markers [10, 11]. This is by far the most common
solution in the literature, and does not have any obvious disadvantages.

• Points are used as correspondences in dot patterns [22, 17]. These markers
are composed of ﬁne dots, which are diﬃcult to detect robustly from acute
viewing angles and long distances.

• Conics are used as correspondences in circular markers [12, 15]. A single
conic correspondence is not adequate to estimate the pose, which is why
these markers either use additional geometric features [12], or do not allow
pose estimation with a single marker [21, 15].

4

(a) Quad localization

(b) Ellipse localization

Figure 3: A square and a circle are uniformly sampled at 40 points, which are
perturbed with a Gaussian noise with 0.04 standard deviation along each axis.
A quad is ﬁtted to the samples from the square, and an ellipse is ﬁtted to the
samples from the circle. Localization errors are shown with red lines.

We are going to argue that the traditional square border has a critical draw-
back compared to the circular border: It cannot be localized as stably. There-
fore, circular borders should be utilized for utmost pose stability. The problem
of a single conic correspondence under-deﬁning the pose still stands, to which
we are going to propose a novel solution to in Section 4.3.

The square border of the ﬁducial marker appears as a quadrilateral (short-
ened as quad) on the image. To localize the quad, we sample points from each
vertex of the quad (e.g., by edge detection), and ﬁt lines to individual point
groups. Either these lines, or their intersections can be used to estimate the
marker pose. Each line represents an exclusive quarter of the information that
is used to estimate the pose, which means that the localization error of a line
cannot be compensated for by the others.

The circular border of the ﬁducial marker appears as an ellipse on the image.
Diﬀerently from the quad, the ellipse cannot be represented as a combination
of independent parts. For this reason, a single ﬁtting operation is applied to
all samples. This can be seen as the pose information being distributed to the
entire shape, which allows it to be recovered more robustly.

Let us consider a square and a circle, both centered on the origin with an
area of 1. We uniformly sample 40 points along these shapes, and perturb them
with Gaussian noise. The quad is localized by ﬁtting a line to each 10 samples
with least squares. The ellipse is localized by ﬁtting an ellipse to all samples
with a least squares method [23]. To measure the mean localization error, the
original shape is uniformly sampled at 40 points, and the average distance of
these samples to the ﬁtted shape is calculated (see Figure 3).

The described experiment is run 10,000 times with standard deviation values

5

Figure 4: Mean localization errors with varying sampling noise standard devia-
tion. Ellipse is localized better across all parameters

between 0 and 0.04. See Figure 4 for the results. As expected, the ellipse is
localized better than the quad for all standard deviation values. See Section 5.4
for an experiment with real images.

3.2 Proposed Marker Design

In Section 3.1, we argued that a circular border can be localized more stably. By
this motivation, the proposed stable marker design contains an inner circular
border. Since a single conic correspondence is not adequate to estimate the
pose, additional geometric features need to be used. An outer square border
supplies the additional constraints, as seen in Figure 5.

The outer square border of the proposed marker is utilized to detect the
marker and estimate a rough homography. After detecting the marker and
validating it with its encoding, the circular border is localized and used to
reﬁne the previously estimated homography. The encoding area inside the inner
circular border is ﬁlled with disk-shaped bit representations, similar to [4]. 48
disks are packed eﬃciently inside the circular area using a simulated annealing
method [24] (see Figure 5a).

After encoding the marker as in Figure 5b, the code pattern is morpho-
logically dilated and eroded repeatedly, resulting in the encoding pattern in
Figure 5c. Filling the gaps between neighboring bit representations allows the
code to be read correctly in the case of slight localization errors. It also reduces
high frequency elements in the pattern, resulting in less edge detections. Finally,

6

00.0050.0100.0150.0200.0250.0300.0350.04000.0020.0040.0060.0080.0100.0120.0140.016MeanLocalizationErrorSamplingNoiseStandardDeviationQuadEllipse(a)

(b)

(c)

(d)

(e)

(f)

Figure 5: (a) shows the tiling of bit representations in the marker. (b) and (c)
are markers before and after morphological operations, respectively. (d)-(f) are
example markers from libraries with diﬀerent minimum Hamming distances.

it improves robustness against blooming and reﬂection eﬀects, especially when
the printing material is glossy. See Figure 5d-f for additional example markers
from libraries with diﬀerent minimum Hamming distances.

3.3 Encoding

We generated lexicographic marker libraries, as ﬁrst done in [6]. The lexicode
generation algorithm tests potential codewords and chooses the ones that satisfy
an arbitrary constraint. For the traditional version, this constraint is that if a
codeword is to be selected, it should be at least a predetermined Hamming
distance away from the previously chosen codewords [25]. This can be adapted
to marker lexicode generation by also testing for the circular permutations of
the codewords.

The described lexicode generation algorithm is an exhaustive search of an
n-dimensional binary space, n being the length of the codewords. This cor-
responds to a time complexity of O(2n). With 48-bit long codewords, the
algorithm requires an unreasonable amount of time to ﬁnish, even with a GPU
implementation. To overcome this problem, we took a two-step hierarchical ap-
proach. We ﬁrst generated 12-bit long codewords, then generated the full-length
codewords as 4-combinations of these.

We generated libraries with diﬀerent sizes and Hamming distances to ac-
commodate for a variety of applications. Smaller libraries with higher minimum
Hamming distances can be used for small-scale AR applications where occlusion

7

Table 1: STag library sizes with respective minimum Hamming distances.

Minimum HD
Library Size

11
22,309

13
2,884

15
766

17
157

19
38

21
12

23
6

Figure 6: Marker libraries with diﬀerent sizes and maximum bit error ra-
tio (BER) correction capabilities.

is common. Larger libraries are more suitable for navigation through large inte-
rior spaces or inventory applications. The resulting library sizes are presented
in Table 1.

The main performance metric for a coding scheme is the amount of error
correction it can provide for various library sizes. Bit error ratio (BER) is the
number of error bits divided by the number of total bits. A marker library that
can correct 0.1 BER will work when at most 10% of the coding area is read
incorrectly. By using this metric, we can compare libraries with diﬀerent code
lengths.

See Figure 6 for a comparison of ARToolKit+ [16], ArUco [18], RUNE-
Tag [13], and STag libraries. Marker variants with diﬀerent code lengths are
plotted separately. The horizontal axis is in logarithmic scale, because improv-
ing error correction capability reduces library size exponentially.

ARToolKit+ implementation provides no error correction capability. ArUco
variants grow more eﬃcient with longer codewords. Both RUNE-Tag libraries
are eﬃcient, but they do not provide the best performance across the whole
scale. ArUco 7×7, RUNE-129 and STag libraries lie on the same line, which

8

11010010001000010000000.050.10.150.20.25MaximumBERCorrectionLibrarySizeARToolKit+ArUco4×4ArUco5×5ArUco6×6ArUco7×7RUNE-43RUNE-129STagFigure 7: Flowchart of the detection algorithm for the proposed marker system
(best viewed in color).

implies a theoretical upper bound. STag libraries cover a large range of library
sizes, and provide the best or near-best performance.

4 Detection Algorithm

The detection algorithm is composed of three main parts, as shown in Figure 7.
In the candidate detection part, which is discussed in Section 4.1, we detect all
quads in the image. These candidates are validated ﬁrstly by their shape, and
secondly by their encoding, which is discussed in Section 4.2. For the candidates
that are validated to be markers, the computationally intensive homography re-
ﬁnement step is run, as described in Section 4.3. After homography reﬁnement,
the pose (i.e., [R∣t]) can be estimated using [26].

4.1 Candidate Detection

The ﬁrst step of the candidate detection algorithm is to detect edge segments
as contiguous arrays of pixels, which signiﬁcantly eases further processing. For
this task, we employ the EDPF algorithm [27], which determines anchor points
in the image, and joins them by aiming to maximize the total gradient response
along its path. Then, it validates these edge chains according to the Helmholtz
principle to obtain native and well-localized edge segments. The edge segments
detected in a sample image are shown in Figure 7b.

As the next step, we process the edge segments to ﬁnd the linear borders of
the markers. This is done by ﬁtting a line to the beginning pixels of an edge
segment, and extending this line as long as the following edge pixels are on it.
This operation is repeated until all edge segments are processed. This approach
avoids repeated line ﬁtting operations, which become expensive with a large
number of iterations [28]. In Figure 7c, the extracted line segments are shown.

9

˜HH(a)OriginalImage(b)EdgeSegmentDetection(c)LineSegmentExtraction(d)CornerDetection(e)QuadDetection(f)PerspectiveValidation(g)Decoding(h)EllipseLocalization(j)HomographyReﬁnementCandidateDetectionCandidateValidationHomographyReﬁnementFigure 8: The marker is detected successfully under heavy occlusion.

Corners are detected by intersecting the line segments extracted consecu-
tively from the same edge segment. Detected corners for the sample image is
shown in Figure 7d. Following this, three consecutive corners are used to detect
the quads on the image. This allows the detection algorithm to detect quads
even when a corner is occluded. See all quads detected on the sample image in
Figure 7e.

As seen in Figure 6, STag libraries provide the highest BER correction capa-
bility. This means that the markers can be decoded robustly against occlusion.
Together with the detection algorithm described in this section, we can detect
markers even under signiﬁcant occlusion. See Figure 8 and the supplementary
video for examples.

4.2 Candidate Validation

We propose a novel candidate validation algorithm that utilizes the candidate
shape. Speciﬁcally, we eliminate candidates based on the imposed perspective
distortion. Here, perspective distortion is in reference to the aspect of a pro-
jective transformation excluding the part that can be represented as an aﬃne
transformation. The objective is to eliminate as much false candidates as pos-
sible, which will decrease the false positive rate.

Depth (α) is the distance from a point to the camera. The relative depth
(αrel) of an object is the ratio of the largest depth (αmax) to the smallest depth
(αmin) of the object. The eﬀect of perspective distortion becomes increasingly
apparent as αrel of the object increases. If we can ﬁnd αrel of the candidates,
we can eliminate them accordingly, assuming αrel will be bounded by some
constraints.

Both opposite edges of a square are parallel, and lines extending along these
parallel edges intersect at two distinct points at inﬁnity. The line at inﬁnity (l∞)

10

Figure 9: A marker candidate with corners c′
transformation have projected the line at inﬁnity to l′
using the intersections i1 and i2. Distances from c′

i. The projective
∞, which can be found by
∞ are shown with di.

i to l′

i and vertices l′

passes through these two points at inﬁnity. When a projective transformation
is applied, each of these points at inﬁnity are projected on a respective ﬁnite
vanishing point. The line passing through this vanishing point pair is the image
of l∞ (see Figure 9). The projection of l∞ can also be found by using the
respective homography matrix [29]. The points closest to and farthest to l′
∞
will be two of the four corners of the quad. The distance between l′
∞ and points
on the object (di) has a linearly negative relationship with the depth of the
said point (αi) [30]. Using this relationship, we can ﬁnd the relative depth of
a planar object using Equation 1 (k is a scalar, refer to Figure 9 for further
notation).

αrel =

αmax
αmin

=

min

kd−1
kd−1

max

=

dmax
dmin

(1)

√

αrel of the projected quad indicates the amount of perspective distortion it
underwent. A threshold is needed to eliminate candidates based on this metric.
Assume that 10 cm×10 cm markers will be used in an application and none
of them will approach the camera more than 20 cm (αmin). We can calculate
αmax to be 20 + 10
2 cm, which results in the maximum αrel to be 1.707.
Therefore, we can safely eliminate all quads that have a larger αrel than this
threshold. If the eliminated quads indicated with red in Figure 7f are examined,
it is seen that only the ones that can not be a valid projection of a square
shape are eliminated. We are going to show the beneﬁt of this validation step
experimentally in Section 5.2.

We estimate the homography matrix, H, of a candidate by using its corners
as correspondence points [29]. There are four ways of matching the correspond-
ing corners, of which an arbitrary one is used. The projections of bit represen-
tations are sampled using this homography to obtain the embedded codeword.

11

l01l02l03l04c01c02c03c04i1i2l0∞d1d2d3d4(a)

(b)

(c)

(d)

Figure 10: (a) A marker with incorrectly localized corners and the respective
expected ellipse location. (b) Edge segment loops inside the marker detection.
(c) Edge segment loops backprojected to the marker plane. (d) The ellipse ﬁtted
to the chosen edge segment loop.

We keep all codewords in the library and their circular rotations in a list. The
read codeword is compared with each element in the list by XORing and doing
a population count. If the Hamming distance between the read codeword and a
codeword in the list is smaller than or equal to the maximum number of bits to
be corrected, the respective rotation and ID is associated with the candidate.
If the arbitrarily chosen rotation is found to be incorrect, the homography is
If the codeword can not be found
updated with the correct correspondence.
in the ID list, then that marker is also eliminated after decoding (see the red
quads in Figure 7g).

4.3 Homography Reﬁnement

At this point, we have a set of markers that are validated by decoding. We are
going to localize the inner circular border as an ellipse, and utilize the conic
information to reﬁne the estimated homography. Let us illustrate the ellipse
localization process. In Figure 10a, we present an example with exaggerated
localization error for better viewing. We ﬁrst ﬁnd the edge segment loops inside
the marker detection (see Figure 10b) and backproject them to the marker plane
(see Figure 10c). It is expected that among the edge segment loops, the one
from the border ellipse will be the most similar to the inner circular border. We
estimate this similarity by sparsely sampling the distances between the inner
circular border and the back projected edge segment loops. An ellipse is ﬁtted
to the edge segment loop whose back projection is the most similar to the inner
circular border [23] (see Figure 10d).

We detect the absence of a suitable edge segment loop by thresholding the
similarity between the backprojected edge segment loops and the inner circular
border. Since this metric is calculated in the marker plane, the shape and size of
the detection is normalized. Thus, the threshold acts equivalently in all poses.
If a suitable ellipse could not be detected, homography reﬁnement is omitted
(e.g., markers #122 and #206 in Figure 7j). This is a deliberate design choice,

12

because we have observed that homography reﬁnement with a partially occluded
ellipse does not beneﬁt stability, and in some cases even degrades it.

In the following discussion, we are going to be considering circles and ellipses
as conic sections, which are represented as matrices. Capital letters refer to
matrices, bold-face letters refer to vectors, and plain letters refer to scalars.
Where x is a point in homogeneous coordinates and x′ is its projection, the 2D
transformation is represented as such [29]:

x′ = Hx

(2)

Under the same transformation as Equation 2, the projection of a conic section
is commonly represented as the following two equivalent equations:

C ′ = H −T CH −1

C = H T C ′H

(3)

(4)

Let C be the inner circular border, whose projection is localized as an ellipse
C ′. When we back project C ′ to the marker plane with Equation 4, we get
an ellipse ˜C, which does not coincide with C perfectly. This is because the
initial homography estimated using the marker corners is slightly incorrect. Our
approach is going to be to optimize H to make C and ˜C as similar as possible.
To optimize H, we need a similarity metric between the ellipse ˜C and the
circle C. Where a is the semi-major axis, b is the semi-minor axis , (ex, ey) is
the center of ˜C, and r is the radius, (cx, cy) is the center of C, respectively, the
metric we propose is:
√

(cid:15) =

(ex − cx)2 + (ey − cy)2 + (a − r)2 + (b − r)2

(5)

These distances are in the marker plane, thus they are normalized for pose.

A single circle–ellipse correspondence is not adequate to estimate the ho-
mography of a marker. We use the homography estimated using the corners
as a starting point, and minimize (cid:15) in Equation 5 using the Nelder–Mead
method [31]. This way, the previously estimated homography is reﬁned such
that the ellipse detection is back projected directly on the inner circular border.
Since the ellipse is localized more correctly than the marker corners, this im-
proves the stability of the localization. See Section 5.4 for an experiment that
demonstrates the beneﬁts of homography reﬁnement.

4.4 Running Time and Parallelization

Since marker detection algorithms have to run in real time, it is important for
them to both run fast and be parallelizable in case a further speed up is needed.
In this section, we are going to analyze the proposed detection algorithm in
this regard. See Table 2 for the steps of the detection algorithm for a 1280×720
image with a cluttered scene containing a single marker.

13

Table 2: Running times of detection algorithm steps in milliseconds.

7 ]

[ 2

F

L i n

D

E
1.0

8 ]

e s [ 2

n

a

C
0.7

a t e

d i d

n

c ti o

a t e

d i d

n

L

a ti o

s e

c

o

V

a li d
E lli p
1.1

e t e

D

n

a

C
0.6

P

D

E
22.6

n

a ti o

a li z

o m o
H
0.8

g r a

y

h

p

n t

e m e

n

ﬁ

e

R

a l

t

o

T
26.8

The ﬁrst thing that meets the eye is that EDPF constitutes more than 80%
of the running time. In fact, the total running time is higher than Table 7 simply
because of the larger number of edge segments extracted from the background
clutter. As the name implies, the Edge Drawing algorithm is partly sequential.
However, a CUDA implementation of this algorithm has resulted in up to ×12
speedup even with older model GPUs [32]. This would correspond to a ×4.4
speedup in the STag detection algorithm by itself.

The rest of the detection algorithm is considerably more parallelizable than
the edge segment detection step. EDLines and candidate detection process edge
segments independently. Considering that there are hundreds of edge segments
in a typical image, processing of these can be parallelized with arbitrary gran-
ularity. Following this, candidate validation operates on candidates indepen-
dently, and around 3.7 candidates can be expected on a typical scene (derived
from the results in Table 3). Finally, ellipse localization and homography re-
ﬁnement only operate on successfully decoded markers. In the case that there
are multiple markers on the scene, these steps are also easily parallelizable. Al-
ternatively, these two ﬁnal steps can be omitted if there are many markers on
the scene, as a large number of correspondences from multiple markers would
already provide a stable pose.

5 Experiments

We compared the proposed marker system with ARToolKit+ [16], ArUco [11]
and RUNE-Tag [13] in a series of experiments. These marker systems are com-
parable with ours, in that they both provide large libraries and allow pose es-
timation with a single marker. The literature uses warped synthetic images for
pose experiments, which is not an adequate approximation. All our experiments
are done on real images.

5.1 Detection

We used an indoor scene recognition dataset with 15,620 images from 67 cate-
gories to represent markerless scenes [33]. See Table 3 for library characteristics
and respective numbers of candidates and false positives. We used the error
correction capability of each library to its fullest extent. While both ArUco and

14

Table 3: Marker library characteristics and detection performance at the Indoor
Scene Recognition Dataset [33].

Bits

Library Size

ArUco [11, 18]

ARToolKit+ [16]

Error
Correction

Number of
Candidates

Marker System Marker Library

Validation
Failure
Probability1
N/A
6.2E-8
2.4E-06
7.4E-05
5.1E-05
8.2E-05
5.9E-06
2.1E-06
3.7E-06
9.3E-07
1.4E-06
9.6E-07
1.7E-07
3.7E-07
4.2E-08
3.3E-09
2.5E-10
5.53E-09
2.0E-10
N/A
N/A
1.7E-10
5.6E-10
4.1E-09
4.7E-09
4.4E-09
2.8E-09
5.3E-08
1 This metric shows the probability of a false positive for a marker library size of 1 with no error correction. Its objective is to assess
false positive rates without penalizing larger marker libraries and higher error correction capabilities. Lower is better.
2 Rune-43 is not provided by the authors

simple-id
BCH-id
ARUCO ORIGINAL
4X4 50
4X4 100
4X4 250
4X4 1000
5X5 50
5X5 100
5X5 250
5X5 1000
6X6 50
6X6 100
6X6 250
6X6 1000
7X7 50
7X7 100
7X7 250
7X7 1000
Rune-432
Rune-129
HD11
HD13
HD15
HD17
HD19
HD21
HD23

Number of
False
Positives
0
3
785
2349
3250
13018
1875
269
934
294
1821
979
177
939
212
27
2
112
4
N/A
0
7
6
23
11
5
2
38

11739
11739
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
317741
N/A
0
57893
57893
57893
57893
57893
57893
57893

512
4096
1024
50
100
250
1000
50
100
250
1000
50
100
250
1000
50
100
250
1000
762
17000
22309
2884
766
157
38
12
6

36
36
16
16
16
16
16
25
25
25
25
36
36
36
36
49
49
49
49
43
129
48
48
48
48
48
48
48

0
0
0
1
1
1
0
3
3
2
2
6
5
5
4
9
8
8
6
6
14
5
6
7
8
9
11
13

RUNE-Tag [17]

STag

15

Figure 11: A frame from the supplementary video that demonstrates detection
performance. See https://github.com/bbenligiray/stag for the video.

STag are trying to detect square markers, STag detects signiﬁcantly less false
candidates. RUNE-Tag detects no candidates in the entire dataset.

Looking at the number of false positives in Table 3, we see that STag returns
few false positives even with extreme error correction. Compared to STag,
ArUco has returned more false positives. To eliminate the eﬀect of the number
of candidates, amount of error correction and library size to the discussion, we
developed a metric named Validation Failure Probability:

Validation Failure Probability =

No. False Positives

No. Candidates×Library Size×2

Error Correction
Bits

(6)
By this metric, we see that the reduced number of false positives from STag is not
only due to the lower number of candidates, but also a better performing marker
library generation scheme. Moreover, the validation performance is consistent
for diﬀerent library sizes and error correction rates.

For a qualitative experiment about detection with markers on the scene, see
the supplementary video (see Fig. 11). A marker of ARToolKit+, ArUco, RUNE-
Tag and STag are printed on a sheet. They are moved around, rotated and
occluded to compare the detection algorithms of the respective systems. RUNE-
Tag cannot be detected when the markers are further away. The temporal
tracking of ARToolKit+ results in mislocalization when the marker is moving
rapidly. ArUco and ARToolKit+ are not very robust against occlusion. STag is
detected consistently throughout the video, is robust against occlusion, and is
localized stably due to homography reﬁnement. Note that the processing was
done oﬄine, as RUNE-Tag cannot be run with this frame rate in real time.

5.2 Perspective Validation

Let us demonstrate the beneﬁts of the proposed shape-based candidate valida-
tion method described in Section 4.2 using the same indoor scene recognition
dataset [33]. A total of 94,427 quads were detected in the entire dataset, and

16

Table 4: The number of false positive detections with maximum error correction
in the indoor scene recognition dataset [33] with and without the candidate
validation method described in Section 4.2.

Minimum HD 11
w/o validation 15

w/ validation

7

13
16

6

15
34

23

17
13

11

19
6

5

21
6

2

23
98

38

Figure 12: A jig is built to manipulate the viewing angle without moving the
camera. The marker printed on paper is aﬃxed tightly to the jig by taping its
corners. The webcam is mounted on a tripod at marker height, aimed straight
at the center of the marker. The viewing angle and distance is estimated using
additional markers.

this number was reduced to 57,893 after being validated based on perspective.
To clearly see the eﬀect of this reduction, we ran the decoding algorithm with
maximum error correction capability (half of the minimum Hamming distance),
which typically returns a large number of false positives. See Table 4 for the
results. Indeed, a signiﬁcant portion of the eliminated candidates were going to
be detected as false positives.

5.3 Setup for Stability Experiments

For stability experiments, we captured images of a stationary marker using a
stationary camera. Ideally, the localization and pose diﬀerences between these
images should be zero. Although the marker detection algorithms are determin-
istic, imaging noise causes localization diﬀerences between the images, which
result in pose diﬀerences. This reﬂects to the end user as instability and jitter
in pose. We calculated the standard deviation of these diﬀerences to compare
the stability of diﬀerent marker systems.

See Figure 12 for the experimental setup of the stability experiments. A
15 cm-wide marker printed on paper is tightly aﬃxed to a jig built for easy
pose manipulation. We chose to print the markers on paper rather than rigid
material such as polystyrene tiles, because this better represents the typical

17

Figure 13: Localization stability before reﬁnement and after reﬁnement. Before
reﬁnement, marker center stability is very low. Reﬁning with the well-localized
ellipse results in a signiﬁcant improvement in stability.

usage scenario. A Logitech C920 webcam is mounted on a tripod at marker level
and aimed directly at the center of the marker. The viewing angle and distance
are adjusted by keeping the camera stationary and manually manipulating the
jig, taking the pose estimated from additional markers as reference. The pose is
set up to 1○ and 1 cm accuracy. For each marker system, distance and viewing
angle, 1000 frames were captured (i.e., each point in the following ﬁgures is the
standard deviation over 1000 frames).

The camera’s focus and exposure were set manually, and kept the same for all
experiments. Anti-ﬂicker was on and lens distortion was rectiﬁed beforehand.
The captured images were grayscale and of 1280×720 resolution. The light-
ing was completely artiﬁcial, coming from overhead sources. Since the camera
did not move, its relative position with respect to the lighting sources did not
change. In addition, we made sure that there was no signiﬁcant air ﬂow in the
environment during the experiments that could move the markers.

5.4 Beneﬁts of Homography Reﬁnement

In this section, we are going to demonstrate that the circular border is localized
more stably than the square border in real images, and the proposed homogra-
phy reﬁnement step improves localization stability. For this speciﬁc experiment,
a 10 cm-wide marker is positioned 100 cm away from the camera. Images with
a resolution of 1920 × 1080 are captured from a series of viewing angles. The
stability of the detection is represented by the standard deviation of the cen-
ter localization. The marker centers before and after reﬁnement are localized

18

0◦10◦20◦30◦40◦50◦60◦70◦00.040.080.120.160.20StandardDeviationofCenter(px)ViewingAngleBeforereﬁnementAfterreﬁnementEllipseby projecting the marker center on the image with the respective homography
matrix. See Figure 13 for the results. Before reﬁnement, the marker center
is unstable. Moreover, the localization is not robust against the changes in
viewing angles. The center of the ellipse we have located is signiﬁcantly more
stable. After homography reﬁnement, the marker center gains stability across
all viewing angles.

5.5 Pose Estimation Stability

ArUco [11] and RUNE-Tag [13] uses OpenCV’s Perspective-n-Point implemen-
tation to estimate the pose from a single marker. For coplanar correspondence
points, this implementation estimates the homography matrix, decomposes it as
described in [34] and reﬁnes it to minimize the projection error if there are re-
dundantly many correspondence points. The method described in [34] requires
the homography matrix to be estimated using at least two images where the
orientation of the planar object is diﬀerent. With a single image of a planar
object, the pose estimation has four diﬀerent solutions of which two satisfy the
cheirality constraints [35]. Therefore, there is an ambiguity between two solu-
tions. Especially when the marker is seen from far away or an acute viewing
angle, the estimated pose switches between the two possible solutions randomly,
causing a signiﬁcant amount of pose jitter.

ARToolkit+ uses the method proposed in [26] to ﬁnd both possible solutions
and select the one that returns the smaller reprojection error in the object-
space. We should note that in practice, this method does not always converge
to the correct solution when only 4 marker corners are used, as we have observed
jittering due to pose ambiguity while using it. Nevertheless, it is obviously supe-
rior over traditional homography decomposition for diﬃcult viewing conditions,
which is why we have also used it to estimate the pose of a single marker for
STag.

The estimated pose has 6 degrees of freedom, thus it is diﬃcult to quan-
tify the amount of jitter in the pose with a single metric. For this reason, we
investigated the jitter with diﬀerent metrics. Namely, we checked jitter in ro-
tation, translation, and center localization. To quantify the jitter in rotation,
we found an average rotation across the frames. Axis–angle representation of
rotation is composed of a direction and a magnitude. By using the magnitude
part, we can represent the diﬀerence from the mean rotation, which was used to
calculate the deviation in degrees. Quantifying the jitter in translation is rather
straightforward. A mean translation vector is found for all frames, then the L2
distance from this mean is used to calculate the standard deviation in metric
units. Finally, the center of the marker is localized by using the camera matrix
K and the pose matrix [R∣t] to project the marker center on the image. The
standard deviation is found using the L2 distance on the image in pixel units.
Note that the pose ambiguity between the two possible solutions only causes
jitter in rotation, and should not aﬀect translation or marker center localization.

19

Table 5: Number of false negative detections out of 1000 frames.

Viewing Angle
ARToolKit+
ArUco
RUNE-Tag
STag

0○
0
0
0
0

5○
0
0
0
0

10○
0
0
0
0

15○
0
0
0
0

20○
0
0
420
0

25○
0
0
0
0

30○
0
0
11
0

35○
0
0
970
0

40○
0
0
1000
0

45○
0
0
1000
0

50○
0
0
1000
0

55○
0
0
1000
0

60○
0
0
1000
0

65○
0
0
1000
0

70○
1000
0
1000
0

75○
1000
0
1000
0

80○
1000
416
1000
4

85○
1000
1000
1000
1000

5.5.1 Diﬀerent Viewing Angles

For this experiment, we captured images of the markers from diﬀerent viewing
angles using the setup described in Section 5.3. Let us start by the detection
accuracies (see Table 5). We highlighted the cases where more than half of the
frames returned false negative detections in red, and we will not report stability
metrics for these cases. For the rest, we did not penalize the false negative
detections in our stability calculations.

Both ArUco and STag are highly robust against diﬃcult viewing angle con-
ditions, yet STag performs signiﬁcantly better from 80○. ARToolKit+ cannot
detect markers seen from an angle of 70○ or more, yet is rather robust for lower
viewing angles. RUNE-Tag quickly stops being detectable as viewing angle in-
creases, and is not very robust even in smaller viewing angles. Seeing that
RUNE-Tag is not prone to false positive detection (see Table 3), but fails to
detect markers under suboptimal conditions, we can conclude that RUNE-Tag
has high precision, but poor recall.

See Figure 14 for the rotation and translation stability across viewing an-
gles. The proposed marker system is both stable and detectable across viewing
angles. However, its stability suﬀers under more acute viewing angles both due
to localization diﬃculties and pose ambiguity. While RUNE-Tag achieves high
stability due to many correspondence points, these ﬁne dots also degrade eas-
ily, resulting in unreliable detection. ARToolkit+ localizes the marker corners
coarsely, which causes its stability to depend on where the corner localizations
fall on the quantization grid. ArUco has considerable jitter both due to unstable
localization (seen on Figure 14b) and high pose ambiguity. In fact, standard
deviation of pose rotation for ArUco was very high (11.70○ on average), which
is why we were not able to illustrate it in Figure 14a. This is to be expected, as
the rotation diﬀerence between the two ambiguous poses tends to be a lot more
than the jitter caused by localization instability.

See Figure 15 for the localization jitter given in pixel units. We can see that
these results are somewhat similar to the translation jitter results, as both are
not aﬀected by the pose ambiguity. Again, the proposed system can be detected
robustly, and its center can be localized stably.

Finally, let us illustrate the diﬀerence in stability qualitatively. See Figure 16
for 500 pose axes drawn on markers seen from 30○. Note that we have eliminated
incorrect poses due to pose ambiguity, so this ﬁgure only illustrates the jitter
caused by localization instability. When the estimated pose is stable, the axes
will overlap, resulting in a single solid axis. In contrast, an unstable pose will
result in scattered axes. The axes from ARToolKit+ and ArUco seem to be

20

(a) Rotation

(b) Translation

Figure 14: Pose rotation and translation stability with diﬀerent viewing angles.
Standard deviation of pose rotation for ArUco was 11.70○ on average, which
could not be shown in this scale.

21

0◦10◦20◦30◦40◦50◦60◦70◦80◦ARToolKit+RUNE-TagSTag0◦0.15◦0.30◦0.45◦0.60◦0.75◦StandardDeviationofPoseRotationViewingAngle0◦10◦20◦30◦40◦50◦60◦70◦80◦ARToolKit+ArUcoRUNE-TagSTag00.10.20.30.40.5StandardDeviationofPoseTranslation(cm)ViewingAngleFigure 15: Localization stability with diﬀerent viewing angles.

(a) ARToolKit+

(b) ArUco

(c) RUNE-Tag

(d) STag

Figure 16: 500 pose axes are plotted on markers seen from 30○. The instability
in the estimated pose causes the axes to appear scattered. The incorrect pose
estimations due to the ambiguity have been eliminated manually.

22

0◦10◦20◦30◦40◦50◦60◦70◦80◦ARToolKit+ArUcoRUNE-TagSTag00.070.140.210.280.35StandardDeviationofCenter(px)ViewingAngleTable 6: Number of false negative detections out of 1000 frames.

Viewing Distance (cm)
ARToolKit+
ArUco
RUNE-Tag
STag

100
0
0
0
0

125
0
0
1000
0

150
0
0
1000
0

175
0
0
1000
0

200
0
0
1000
0

225
0
0
1000
0

250
0
0
1000
0

275
0
0
1000
0

300
0
0
1000
0

Table 7: Running times of each algorithm with 1280 × 720 images with a single
marker on the scene.

Running time (ms)

2.6

10.0

579.9

18.1

ARToolKit+ ArUco RUNE-Tag

STag

scattered fairly uniformly. The axes from RUNE-Tag seem quite solid, yet there
are a few outliers. On the other hand, the axes drawn on the proposed system
appear to be overlapping, meaning that the proposed pose is stable.

5.5.2 Diﬀerent Viewing Distances
We have repeated the experiment in Section 5.5.1 with a constant 0○ viewing
angle and diﬀerent viewing distances. See Table 6 for detection performances.
Similar to the case with large viewing angles, RUNE-Tag cannot be detected
when viewed from afar, as its coding circles appear too small to be detected.
Since the other systems are use a thick outer border for detection, they can be
detected from a distance.

See Figure 17 for pose stability across viewing distances. Unlike changing
viewing distance, pose ambiguity becomes the dominant factor in pose stabil-
ity, which is apparent from the rather large standard deviations in Figure 17a.
ARToolkit+ and STag are aﬀected less because they use [26] to estimate the
pose. ARToolkit+ is aﬀected even less than STag, as its corners are localized in
a more coarse manner, which seems to be an advantage in the case where the
marker appears smaller. Compared to pose rotation standard deviations, pose
translation standard deviations are much more comparable for all methods (see
Figure 17b).

Finally, we localized the marker centers using [R∣t]. It is interesting to see
that the marker center stability metrics in Figure 15 and Figure 18 are more
similar compared to stabilities in pose. This means that using the markers
from a large distance does not degrade their localization stability as much as it
degrades their pose stability.

5.6 Running Times

Marker detection algorithms tend to be embarrassingly parallelizable. To nullify
the eﬀects of diﬀerent parallel implementations, we used a single core of a 3.70

23

(a) Rotation

(b) Translation

Figure 17: Pose rotation and translation stability with diﬀerent viewing dis-
tances.

24

100125150175200225250275300ARToolKit+ArUcoRUNE-TagSTag0◦2.5◦5◦7.5◦10◦12.5◦StandardDeviationofPoseRotationViewingDistance(cm)100125150175200225250275300ARToolKit+ArUcoRUNE-TagSTag01.534.567.5StandardDeviationofPoseTranslation(cm)ViewingDistance(cm)Figure 18: Localization stability with diﬀerent viewing distances.

GHz Intel Xeon processor to run the experiments. See Table 7 for average
running times per image for the experiment in Section 5.5. We can see that
ARToolKit+ is very fast, both ArUco and STag perform in real-time, while
RUNE-Tag is considerably slower.

6 Conclusion

The main contribution of the proposed marker system is improving stability
with a homography reﬁnement step. This is achieved by estimating the ho-
mography with the outer square border and reﬁning it with the inner circular
border. To our knowledge, this problem and the respective solution is unique
in the literature in the way that a single conic correspondence is used, without
additional projective constraints such as a point [12].

The proposed solution is signiﬁcantly more stable than ArUco [11]. Since
it does not depend on a large number of ﬁne details, it is signiﬁcantly more
robust across viewing conditions and runs an order of magnitude faster than
RUNE-Tag [13]. However, we have seen that its localization characteristics are
somewhat more prone to pose ambiguity compared to ARToolkit+ [16]. Con-
sidering this pose ambiguity is an important factor in pose estimation stability,
future work should focus on marker designs that takes this issue into account.
For the multi-marker case, the inner circular borders can be used to estimate
the pose with a multiple conic correspondence approach [36]. The alternative
we have proposed is using the ellipse centers as stable, yet inaccurate correspon-
dences of marker centers. This may cause some inaccuracy, but the stability of
the point correspondences will yield an even more stable pose estimation.

25

100125150175200225250275300ARToolKit+ArUcoRUNE-TagSTag00.070.140.210.280.35StandardDeviationofCenter(px)ViewingDistance(cm)References

[1] M. Fiala, “Designing highly reliable ﬁducial markers,” IEEE Trans. Pattern Anal.

Mach. Intell., vol. 32, no. 7, pp. 1317–1324, 2010.

[2] J. Rekimoto, “Matrix: A realtime object identiﬁcation and registration method
for augmented reality,” in Proc. Asia Paciﬁc Conf. Comput. Human Interaction,
pp. 63–68, 1998.

[3] H. Kato, M. Billinghurst, I. Poupyrev, K. Imamoto, and K. Tachibana, “Vir-
tual object manipulation on a table-top AR environment,” in Proc. Int. Symp.
Augmented Reality,, pp. 111–119, 2000.

[4] X. Zhang, S. Fronz, and N. Navab, “Visual marker detection and decoding in
AR systems: A comparative study,” in Proc. IEEE/ACM Int. Symp. Mixed and
Augmented Reality, pp. 97–106, 2002.

Magazine,

[5] Make
in
exclusive-see-the-secret-prototypes-we-found-in-valves-vr-lab/,
2016.

Found
http://makezine.com/2016/06/21/

Prototypes We

the
Lab.”

Valve’s

Secret

“See

VR

[6] E. Olson, “AprilTag: A robust and ﬂexible visual ﬁducial system,” in Proc. IEEE

Int. Conf. Robotics and Automation, pp. 3400–3407, 2011.

[7] Amazon, “Amazon Prime Air’s First Customer Delivery.” https://youtu.be/

vNySOrI2Ny8?t=1m13s, 2016.

[8] Boston Dynamics,

“Atlas, The Next Generation.” https://youtu.be/

rVlhMGQgDkY?t=1m23s, 2016.

[9] H. Kato and M. Billinghurst, “Marker tracking and HMD calibration for a video-
based augmented reality conferencing system,” in Proc. IEEE/ACM Int. Work-
shop Augmented Reality, pp. 85–94, 1999.

[10] M. Fiala, “ARTag, a ﬁducial marker system using digital techniques,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition, pp. 590–596, 2005.

[11] S. Garrido-Jurado, R. Mu˜noz-Salinas, F. J. Madrid-Cuevas, and M. J. Mar´ın-
Jim´enez, “Automatic generation and detection of highly reliable ﬁducial markers
under occlusion,” Pattern Recognition, vol. 47, no. 6, pp. 2280–2292, 2014.

[12] D. L´opez de Ipi˜na, P. R. S. Mendon¸ca, and A. Hopper, “TRIP: A low-cost vision-
based location system for ubiquitous computing,” Personal and Ubiquitous Com-
puting, vol. 6, no. 3, pp. 206–219, 2002.

[13] F. Bergamasco, A. Albarelli, L. Cosmo, E. Rodola, and A. Torsello, “An accurate
and robust artiﬁcial marker based on cyclic codes,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 38, no. 12, pp. 2359–2373, 2016.

[14] J. DeGol, T. Bretl, and D. Hoiem, “ChromaTag: A Colored Marker and Fast

Detection Algorithm,” in Proc. IEEE Int. Conf. Comput. Vision, 2017.

[15] L. Calvet, P. Gurdjos, C. Griwodz, and S. Gasparini, “Detection and accurate
localization of circular ﬁducials under highly challenging conditions,” in Proc.
IEEE Conf. Comput. Vision and Pattern Recognition, pp. 562–570, 2016.

[16] D. Wagner and D. Schmalstieg, “ARToolKitPlus for pose tracking on mobile

devices,” in Comput. Vision Winter Workshop, 2007.

26

[17] F. Bergamasco, A. Albarelli, E. Rodola, and A. Torsello, “RUNE-Tag: A high
accuracy ﬁducial marker with strong occlusion resilience,” in Proc. IEEE Conf.
Comput. Vision and Pattern Recognition, pp. 113–120, 2011.

[18] S. Garrido-Jurado, R. Mu˜noz-Salinas, F. Madrid-Cuevas, and R. Medina-
Carnicer, “Generation of ﬁducial marker dictionaries using mixed integer linear
programming,” Pattern Recognition, vol. 51, pp. 481–491, 2016.

[19] L. Naimark and E. Foxlin, “Circular data matrix ﬁducial system and robust image
processing for a wearable vision-inertial self-tracker,” in Proc. IEEE/ACM Int.
Symp. Mixed and Augmented Reality, 2002.

[20] A. Xu and G. Dudek, “Fourier Tag: A smoothly degradable ﬁducial marker
system with conﬁgurable payload capacity,” in Proc. Canadian Conf. Comput.
and Robot Vision, pp. 40–47, 2011.

[21] J. Sattar, E. Bourque, P. Giguere, and G. Dudek, “Fourier tags: Smoothly degrad-
able ﬁducial markers for use in human-robot interaction,” in Proc. Conf. Comput.
and Robot Vision, pp. 165–174, 2007.

[22] H. Uchiyama and H. Saito, “Random dot markers,” in Proc. Virtual Reality Conf.,

pp. 35–38, 2011.

[23] A. Fitzgibbon, M. Pilu, and R. B. Fisher, “Direct least square ﬁtting of ellipses,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 5, pp. 476–480, 1999.

[24] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, “Optimization by simmulated

annealing,” Science, vol. 220, no. 4598, pp. 671–680, 1983.

[25] J. Conway and N. Sloane, “Lexicographic codes: Error-correcting codes from
game theory,” IEEE Trans. on Inf. Theory, vol. 32, no. 3, pp. 337–348, 1986.

[26] G. Schweighofer and A. Pinz, “Robust pose estimation from a planar target,”

IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 12, pp. 2024–2030, 2006.

[27] C. Akinlar and C. Topal, “EDPF: A real-time parameter-free edge segment de-
tector with a false detection control,” Int. J. of Pattern Recognition and Artiﬁcial
Intell., vol. 26, no. 1, p. 862872, 2012.

[28] C. Akinlar and C. Topal, “EDLines: A real-time line segment detector with a false
detection control,” Pattern Recognition Letters, vol. 32, no. 13, pp. 1633–1642,
2011.

[29] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. Cam-

bridge University Press, 2003.

[30] G. Sparr, “Depth computations from polyhedral images,” Image and Vision Com-

puting, vol. 10, no. 10, pp. 683–688, 1992.

[31] J. A. Nelder and R. Mead, “A simplex method for function minimization,” The

Computer Journal, vol. 7, no. 4, pp. 308–313, 1965.

[32] O. Ozsen, C. Topal, and C. Akinlar, “Parallelizing edge drawing algorithm on
CUDA,” in Proc. IEEE Int. Conf. Emerging Signal Process. Applicat., pp. 79–82,
2012.

[33] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in Proc. IEEE Conf.

Comput. Vision and Pattern Recognition, pp. 413–420, 2009.

[34] Z. Zhang, “A ﬂexible new technique for camera calibration,” IEEE Trans. Pattern

Anal. Mach. Intell., vol. 22, no. 11, pp. 1330–1334, 2000.

27

[35] R. I. Hartley, “Chirality,” Int. J. of Comput. Vision, vol. 26, no. 1, pp. 41–61,

1998.

[36] J. Kannala, M. Salo, and J. Heikkil¨a, “Algorithms for computing a planar ho-
mography from conics in correspondence,” in Proc. British Mach. Vision Conf.,
pp. 77–86, 2006.

28

