0

0
2
0
2

n
u
J

0
1

]

R
C
.
s
c
[

3
v
7
9
7
5
0
.
2
0
8
1
:
v
i
X
r
a

Security and Privacy Approaches in Mixed Reality:
A Literature Survey

JAYBIE A. DE GUZMAN, University of New South Wales and Data 61, CSIRO
KANCHANA THILAKARATHNA, University of Sydney and Data 61, CSIRO
ARUNA SENEVIRATNE, University of New South Wales and Data 61, CSIRO

Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision,
sensor fusion, and realistic display technologies. With most of the research and development focused on
delivering the promise of MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and to look into the latest security
and privacy work on MR. Specifically, we list and review the different protection approaches that have been
proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related
technologies such as augmented reality (AR), virtual reality (VR), and human-computer interaction (HCI) as
crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of investigation, implementation, and
evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy
are also discussed.

CCS Concepts: •Human-centered computing → Mixed / augmented reality; •Security and privacy →
Privacy protections; Usability in security and privacy; •General and reference → Surveys and overviews;

Additional Key Words and Phrases: Mixed Reality, Augmented Reality, Privacy, Security

1 INTRODUCTION

Mixed reality (MR) was used to pertain to the various devices – specifically, displays – that
encompass the reality-virtuality continuum as seen in Figure 1 (Milgram et al. 1994). This means
that augmented reality (AR) systems and virtual reality (VR) systems are MR systems but, if
categorized, will lie on different points along the continuum. Presently, mixed reality has a hybrid
definition that combines aspects of AR and VR to deliver rich services and immersive experiences
(Curtin 2017), and allow interaction of real objects with synthetic virtual objects and vice versa. By
combining the synthetic presence offered by VR and the extension of the real world by AR, MR
enables a virtually endless suite of applications that is not offered by current AR and VR platforms,
devices, and applications.

Advances in computer vision – particularly in object sensing, tracking, and gesture identification
– , sensor fusion, and artificial intelligence has furthered the human-computer interaction as well as
the machine understanding of the real-world. At the same time, advances in 3D rendering, optics –
such as projections, and holograms –, and display technologies have made possible the delivery of
realistic virtual experiences. All these technologies make MR possible. As a result, MR can now
allow us to interact with machines and each other in a totally different manner: for example, using
gestures in the air instead of swiping in screens or tapping on keys. The output of our interactions,
also, will no longer be confined within a screen. Instead, outputs will now be mixed with our
real-world experience, and possibly sooner we may not be able to tell which is real and synthetic.

A published verion of this work is available at https://doi.org/10.1145/3359626. Please cite the published version as:
Jaybie A. De Guzman, Kanchana Thilakarathna, and Aruna Seneviratne. 2019. Security and Privacy Approaches in Mixed
Reality: A Literature Survey. ACM Comput. Surv. 52, 6, Article 110 (October 2019), 37 pages.

, Vol. 0, No. 0, Article 0. Publication date: 0.

 
 
 
 
 
 
Fig. 1. Milgram and Kishino’s Reality and Virtuality Continuum (Milgram and Kishino 1994), and relative
positions of where AR, VR, and AV are along the continuum. AR sits near the real environment as its primary
intention is to augment synthetic objects to the physical world while VR sits near the virtual environment
with varying degree of real-world information being feed in to the virtual experience. A third intermediary
type, augmented virtuality, sits in the middle where actual real-world objects are integrated into the virtual
environment and thus intersecting with both AR and VR.

Fig. 2. Statistics from Scopus: The overall percentage of papers with “security” and “privacy” keywords, and,
after removing papers with just keyword mentions, the percentage of actual papers on security and privacy.

Recently released MR devices such as Microsoft’s Hololens and the Magic Leap demonstrates what
these MR devices can do. They allows users to interact with holographic augmentations in a more
seamless and direct manner.

Most of the work on MR for the past two decades have been focused on delivering the necessary
technology to make MR a possibility. As the necessary technology is starting to mature, MR
devices, like any other technology, will become more available and affordable. Consequently, the
proliferation of these devices may entail security and privacy implications which may not yet be
known. For example, it has been demonstrated how facial images captured by a web camera can
be cross-matched with publicly available online social network (OSN) profile photos to match the
names with the faces and further determine social security numbers (Acquisti 2011). With most MR
devices coming out in a wearable, i.e. head-mounted, form-factor and having at least one camera to
capture the environment, it will be easy to perform such facial matching tasks in the wild without
the subjects knowing it. Security and privacy, most often than not, comes as an afterthought, and
we are observing a similar trend in MR as further shown in this survey paper.

To systematically capture the trend, we used the search tool of Scopus to initially gather AR
and MR literature. We further identified works with security and privacy from this gathered list.
Although, Scopus does not index literature from all resources, particularly in security and privacy
research, the search tool can include secondary documents that have been cited by Scopus-indexed
documents which now effectively includes most security and privacy literature such as those from
USENIX Security Symposium, the Internet Society’s Networks and Distributed System Security
(NDSS) Symposium, and so on. Figure 2 shows the yearly percentage of these papers. Despite the

2

Mixed Reality (MR)Real EnvironmentAugmented Reality (AR)Virtual EnvironmentVirtual Reality (VR)Augmented Virtuality (AV)0.01.02.03.04.05.06.07.01997199920012003200520072009201120132015Percentage	(%)YearsPercentage	of	papers	with	security	and	privacy	keyword	mentionsPercentage	of	actual	security	and	privacy	work	in	AR/MR	Literatureincreasing percentage from 0.7% in 1997 to 5.8% in 2016, most only mention security and privacy,
and only a few (1.42% for 2016) are actually discussing the impacts, or presenting security and
privacy approaches applied to, or using AR/VR/MR. Nonetheless, we supplement the gathered
works from Scopus by separately searching for AR/VR/MR works with security and privacy from
Google Scholar, IEEE Xplore, ACM Digital Library, and other specific venues covering computer
vision, human-machine interfaces, and other related technologies.

Previous Surveys and Meta-analyses
Early surveys on AR and MR, have been focused on categorizing the existing technologies then.
In 1994, a taxonomy for classifying mixed reality displays based on the user-interface – from
monitor-based video displays to completely immersive environments – were presented and these
devices were plotted along a reality-virtuality continuum (Milgram and Kishino 1994). On the
other hand, in contrast to this one-dimensional continuum, two different classifications for mixed
reality were also presented: (1) a two-dimensional categorization of shared space or collaborative
mixed reality technologies according to concepts of transportation1 and artificiality2, and (2) a
one-dimensional classification based on spatiality3 (Benford et al. 1996).

Succeeding endeavours have focused on collecting all relevant technologies necessary to AR
and VR. The various early challenges – such as matching the real and virtual displays, aligning
the virtual objects with the real world, and the various errors that needs to be addressed such as
optical distortion, misalignment, and tracking – have been discussed in broad (Azuma 1997). It was
complemented with a following survey that focuses on the enabling technologies, interfacing, and
visualization (Azuma et al. 2001). A much more recent survey updated the existing challenges to
the following: performance, alignment, interaction, mobility, and visualization (Rabbi and Ullah
2013). Another one looked into a specific type of AR, mobile AR, and looked into the different
technologies that enable mobility with AR (Chatzopoulos et al. 2017). Lastly, a review of the various
head-mounted display (HMD) technologies for consumer electronics (Kress and Starner 2013) was
also undertaken. While all of these different challenges and technologies are important to enable
AR, none of these survey or review papers have focused onto the fundamental issues of security
and privacy in AR or MR.

A few others have pointed out the non-technical issues such as ethical considerations (Heimo
et al. 2014) and value-sensitive design approaches (Friedman and Kahn Jr 2000) that pushes to
consider data ownership, privacy, secrecy, and integrity. A much recent work emphasized the three
aspects for protection in AR – input, data access, and output – over varying system complexity
(from single to multiple applications, and, eventually, to multiple systems) (Roesner et al. 2014b). In
supplement, we expand from these three aspects, and include interaction and device protection as
equally important aspects and dedicate separate discussions for both. A very recent survey collected
offensive and defensive approaches on wearables (Shrestha and Saxena 2017) and it included some
strategies for wearable eye-wears and HMDs. In this work, we expand the coverage to include all
MR platforms and setups (including non-wearables) and present a more specific and, yet, wider
exposition of MR security and privacy approaches.

1Transportation refers to the extent to which the users are transported from their physical locality to a virtual or remote
space.
2Artificiality is the extent to which the user space has been synthesized from a real physical space.
3Spatiality is the extent to which properties of natural space and movement is supported by the shared space.

3

Contributions
To the best of our knowledge, this is the first survey on the relevant security and privacy challenges
and approaches on mixed reality. To this end, this work makes the following contributions:

(1) We provide a data-centric categorization of the various works which categorizes them to five
major aspects, and, further, to subcategories, and present generic system block diagrams to
capture the different mechanisms of protection.

(2) We include a collection of other relevant work not necessarily directed to mixed reality but is

expected to be related to or part of the security and privacy considerations for MR.

(3) Lastly, we identified the target security and privacy properties of these approaches and present
them in a summary table to show the distribution of strategies of protection among the proper-
ties.

Before proceeding to the review of the various security and privacy work, we clarify that we
do not focus on network security and related topics. We will rely on the effectiveness of existing
security and privacy measures that protects the communication networks, and data transmission,
in general.

The rest of the survey proceeds as follows. §2 lists the different security and privacy properties,
and explains the categorization used to sort the different approaches. The details of these approaches
and which properties they address are discussed in §3. Lastly, in §4, the open challenges and future
directions are discussed before finally concluding this survey in §5.

2 DEFINING AND CATEGORIZING SECURITY AND PRIVACY IN MIXED REALITY
In this section, we first present the important security and privacy requirements that needs to be
considered in designing or using MR. Then, we present an an overall categorization that focuses on
the flow of data within an MR system.

2.1 General Security and Privacy Requirements
We derive security and privacy properties from three models and combine them to have an over-
arching model from which we can refer or qualify the different approaches (both defensive and
offensive strategies) that will be discussed in this survey paper. Table 1 lists the thirteen combined
security and privacy properties from Microsoft’s Security Development Lifecycle (Howard and Lipner
2006), PriS (Kalloniatis et al. 2008), and LINDDUN (Deng et al. 2011) and their corresponding threats.
The first six are security properties while the remaining are considered as privacy properties. The
confidentiality property is the only common among the three models and is considered as both a
security and privacy property. Obviously, since the Microsoft’s SDL focuses primarily on security,
all of its associated properties target security. PriS has a roughly balanced privacy and security
targeted properties. On the other hand, LINDUNN’s properties are privacy-targeted, and are further
categorized into hard privacy (from confidentiality down to plausible deniability) and soft privacy
(the bottom two properties).

It is interesting to note that some security properties are conversely considered as privacy threats,
such as non-repudiation and plausible deniability. This highlights the differences in priority that an
organization, user, or stakeholder can put into these properties or requirements. However, these
properties are not necessarily mutually exclusive and can be desired at the same time. Moreover,
the target entity (or element) adds another dimension to this list. Specifically, these properties can
be applied to the following entities or data flow elements: pieces of data, data flow, process, and
data storage. We highlight these co-existence as well as example target entities as we go through
each of these properties one-by-one.

4

Table 1. Combined general security and privacy properties and their corresponding threats

Property

Threat

Model

PriS 2008

Microsoft’s
SDL 2006

LINDDUN
2011

Integrity

Non-repudiation

Availability

Authorization

Authentication

Identification

Confidentiality

Tampering

Repudiation

Denial of Service

Elevation of Privilege

Spoofing

Anonymity

Disclosure of Information

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Anonymity & Pseudonymity

Identifiability

Unlinkability

Linkability

Unobservability & Undetectability

Detectability

Plausible Deniability

Content Awareness

Non-repudiation

Unawareness

Policy & Consent Compliance

Non-compliance

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(1) Integrity – The data, flow, or process in MR is not and cannot be tampered or modified. This
is to ensure that, for example, the visual targets are detected correctly and the appropriate
augmentations are displayed accordingly. No unauthorized parties should be able to modify
any element in MR.

(2) Non-repudiation – Any modification, or generation of data, flow, or process cannot be denied
especially if the entity is an essential one or an adversary was able to perform such modifications
or actions. When necessary, the modifier or generator of the action should be identified and
cannot deny that it was their action. In privacy, however, the converse is desired.

(3) Availability – All necessary data, flow, or process for MR should be available in order to satisfy
and accomplish the targeted or intended service. An adversary should not be able to impede
the availability of these entities or resources.

(4) Authorization and Access Control – All actions should be originated from authorized and
verifiable parties. The same actions are also actuated according to their appropriate access
privileges. For example, a navigation application requires GPS information but does not
necessarily require camera view. Or only the augmentations from applications that have been
authorized to deliver augmentations should be rendered.

(5) Identification – All actions should be identified to the corresponding actor, i.e. user or party.
In a security context, this is interrelated with authorization and authentication properties.
Verified identities are used for authorizing access control. Unidentified parties can be treated as
adversaries to prevent unidentifiable and untraceable attacks. In sensitive situations, e.g. an
attack has occurred, anonymity is not desired.

(6) Authentication – Only the legitimate users of the device or service should be allowed to access
the device or service. Their authenticity should be verified through a feasible authentication
method. Then, identification or authorization can follow after a successful authentication.
(7) Confidentiality – All actions involving sensitive or personal data, flow, or process should follow
the necessary authorization, and access control policies. Parties that are not authorized should
not have access to these confidential entities. All entities can be assumed as confidential
especially personal and re-identifiable data.

5

(8) Anonymity & Pseudoymity – Users should be able to remove their association or relationship to
the data, flow, or process. Likewise, a pseudonym can be used to link the entities but should
not be linked back to user identities. Moreover, an adversary should not be able to identify the
user from combinations of these entities. However, in the security context, anonymity is not
desired especially when adversaries need to be identified.

(9) Unlinkability – Any link or relationship of the user or party to the data, flow, or process as well
as among the entities (e.g. data to data, data to flow, and so on) cannot be distinguished by an
adversary.

(10) Undetectability & Unobservability – Any entities’ existence cannot be ensured or distinguished
by an attacker. Or an entity can be deemed unobservable or undetectable by an adversary that
does not require it. Or the entity cannot be distinguished from randomly generated entities.
For example, an MR game like PokemonGo needs access to the camera view of the user device
and determine the ground plane to place the Pokemon on the ground as viewed by the user,
but the game does not need to know what the other objects within view are.

These two properties can be extended to include latent privacy protection.

It is the
protection of entities that are not necessitated by an application or service but can be in the
same domain as their target entities. This includes bystander privacy.

(11) Plausible Deniability – The user or party should be able to deny their relationship to the entities,
or of among themselves. This is a converse of the non-repudiation security property which
is actually the corresponding threat for plausible deniability (or repudiation). However, this
property is essential when the relationship to personal sensitive data should not be compromised,
while non-repudiation is essential if, for example, an adversarial action should not be denied by
an attacker.

(12) Content Awareness – The user should be aware of all data, flows or processes divulged especially
those of sensitive entities. And that they should be aware that they have released the necessary
amount of information or if it was too much.

(13) Policy and consent Compliance – The system should follow or policies specified that aims to
protect the user’s privacy or security. There should be a guarantee that systems, especially
third-party applications or services, follow these policies and considers the consent allowed to
them.

For every approach that will be discussed in the next section (§3), we will identify which
properties they are trying to address. Moreover, there are other ‘soft’ properties (i.e. reliability, and
safety.) that we will liberally use in the discussions. We categorize these approaches in a data- and
data flow-centric fashion which is explained in the next subsection.

2.2 Categorizing the Threats
Figure 3 presents an example of an MR environment with the supporting data services and shows
how data flows within the environment, and through the data services. The left-half of the diagram
shows the ‘view’ of the mixed reality device which, in this example, is a see-through MR head-
mounted device (HMD) or, simply, an MR eye-wear. Within the view are the physical objects which
are ‘seen’ by the MR device as indicated by the green arrows. The synthetic augmentations are
shown in the diagram which are represented by the red arrows. The right-half of the diagram
shows the various supporting data services that processes the data. The bi-directional solid arrows
represent the access of these applications to captured data and the delivery of outputs to be
augmented. Representative points of the five aspects of protection within the data flow are also
labelled in Figure 3 and we use these labels to further explain the categorization as follows.

6

Fig. 3. A mixed reality environment with the supporting data services as well as example points of protection
as labelled: (1) contents of the display monitor, (2) access to stored data, (3) virtual display for content, e.g.
information about the contents of a smart mug, (4) collaborating with other users, and (5) device access to
the mixed reality eye-wear.

(1) Input Protection – This first category focuses on the challenges in ensuring security and privacy
of data that is gathered and inputted to the MR platform. These data can contain sensitive
information. For example, in Figure 3, the MR eye-wear can capture the sensitive information
on the user’s desktop screen (labelled 1) such as e-mails, chat logs, and so on. These are
user-sensitive information that needs to be protected. Similarly, the same device can also
capture information that may not be sensitive to the user but may be sensitive to other entities
such as bystanders. This is called bystander privacy. Aside from readily sensitive objects, the
device may capture other objects in the environment that are seemingly benign (or subtle) and
were not intended to be shared but can be used by adversaries to infer knowledge about the
users/bystanders. These necessary protections can be mapped to properties of confidentiality,
unobservability & undetectability, and content awareness. The input protection approaches are
discussed in §3.1.

(2) Data Protection – After sensing, data is usually collected by the system in order for it to be
processed. Data from multiple sensors or sources are aggregated, and, then, stored in a database
or other forms of data storage. Applications, then, need to access these data in order to deliver
output in the form of user-consumable information or services. However, almost all widely
used computing platforms allows applications to collect and store data individually (as shown
in the access of supporting data services labelled 2 in Figure 3) and the users have no control
over their data once it has been collected and stored by these applications. A lot of security
and privacy risks have been raised concerning the access and use of user data by third party
agents, particularly, on user data gathered from wearable (Felt et al. 2012), mobile (Lee et al.
2015), and on-line activity (Ren et al. 2016). MR technology faces even greater risks as richer
information can be gathered using its highly sensitive sensors. For data protection, there are a
lengthy list of properties that needs to be addressed such as integrity, availability, confidentiality,
unlinkability, anonymity & pseudonymity, and plausible deniability among others. §3.2 will
present a discussion of the data protection approaches.

7

mixed reality pet  ~90 ⁰C~2g sugar1 unread SMS1 New EmailDataUserApplicationsWeb ServicesSocial NetworksView of the mixed reality glassesmixed reality device13452Mixed Reality Environment←  →Supporting Data ServicesLegend:output data flowinput data flowin-network data flowDataNetworkSocialDataServiceWebDataApplicationRemote(3) Output Protection – After processing the data, applications send outputs to the mixed reality
device to be displayed or rendered. However, in MR, applications may inadvertently have access
to outputs of other applications. If an untrusted application has access to other outputs, then
it can potentially modify those outputs making them unreliable. For example, in the smart
information (labelled 3) hovering over the cup in Figure 3, malicious applications can modify
the sugar level information. Also, it is possible that one application’s output is another’s input
which necessitates multiple application access to an output object. The integrity, availability,
and policy compliance as well as reliability properties of these outputs has to be ensured. All
these should be safely considered in output protection. The approaches are discussed in §3.3.
(4) User Interaction Protection – MR mixes or includes the utilization of other sensing and display
interfaces to allow immersive interactions. Examples of these are room-scale interfaces4 which
allow multiple users to interact in the same MR space – combining virtual and physical spaces,
or the virtual “tansportation” of users in MR video-conferecing5 to allow them to seemingly
co-exist in the same virtual space while being fully-aware of their physical space. Thus, we
expand the coverage of protection to ensure protected sharing and collaborations (labelled 4 in
Figure 3) in MR.

In contrast to current widely adapted technologies like computers and smart phones, MR
can enable entirely new and different ways of interacting with the world, with machines, and
with other users. One of the key expectations is how users can share MR experiences with
assurance of security and privacy of information. Similar to data protection, there is a number
of properties that is necessary in interaction protection namely non-repudiation, authorization,
authentication, identifiability, and policy & consent compliance. Details of the approaches in
protecting user interactions are discussed in §3.4.

(5) Device Protection – This last category focuses on the actual physical MR device, and the physical
input and ouput interfaces of these devices. By extension, implicitly protects data that goes
through all the other four aspects by ensuring device-level protection. Authentication, autho-
rization, and identification are among the most important properties for device protection.
In §3.5, the different novel approaches in device access and physical display protection are
discussed.

Figure 4 shows these categories with their further subcategories. A simplified representation of
the process described in Figure 3 is shown as a pipeline in Figure 5 which now has three essential
blocks: detection, transformation, and rendering. The first three categories are directly mapped to
the associated risks with the main blocks of the processing pipeline – protecting how applications,
during the transformation stage, access real-world input data gathered during detection, which may
be sensitive, and generate reliable outputs during rendering. The detection focuses on gathering
information such as user view orientation and direction, location, and surrounding objects. Thus,
the detection process primarily depends on the sensing capabilities of the device. After detection,
the information gathered will be transformed or processed to deliver services. Depending on the
service or application, different transformations are used. Finally, the results of the transformation
are delivered to the user by rendering it through the device’s output interfaces. However, actual
approaches may actually be applied beyond the simple boundaries we have defined here. Thus,
some input and output protection approaches are actually applied in the transformation stage,
while some data access protection approaches, e.g. data aggregation, are usually applied in the

4Theoriz studio designed and developed a room-scale MR demonstration (Link: http://www.theoriz.com/portfolio/mixed-
reality-project/).
5Microsoft’s Holoportation demonstrates virtual teleportation in real-time (Link: https://www.microsoft.com/en-
us/research/project/holoportation-3/).

8

Fig. 4. A data-centric categorization of the various security and privacy work or approaches on mixed reality
and related technologies.

Fig. 5. Generic mixed reality pipeline with the application target areas of three out of the five categories.

detection and rendering stages. Furthermore, the interaction protection and device protection
approaches cannot be directly laid out along the pipeline unlike the other three as the intended
targets of these two categories transcend this pipeline.

The presented categorization does not exclusively delineate the five aspects, and it is significant
to note that most of the approaches that will be discussed can fall under more than one category or
subcategory. Notwithstanding, this survey paper complements the earlier surveys by presenting
an up-to-date collection of security and privacy research and development for the past two decades
on MR and related technologies and categorizing these various works according to the presented
data-centric categorization. The next section proceeds in discussing these various approaches that
have been done to address each of the five major aspects.

3 SECURITY AND PRIVACY APPROACHES
The MR environment diagram in Figure 3 shows how applications or third-party services access and
utilize user data, and how once these applications have been granted access to those resources, they
may now have indefinite access to them. Most file systems of popular mobile platforms (i.e. Android
and iOS) have a dedicated file system or database for each application. There are additional measures
for security and privacy such as permission control, and application sand-boxing; however, despite
these existing protection mechanisms, there are still pertinent threats that are not addressed. Thus,
there is a great deal of effort to investigate data protection mechanisms. Most approaches rely
on inserting an intermediary protection layer (as shown in Figure 6) that enables in- and out-flow
control of data from trusted elements to untrusted ones.

In the following subsections, we present the various security and privacy work that has been
done on MR and related technologies, especially on AR. We have organized these approaches

The rest of the figures in this survey paper will represent limited or less-privileged data flow with broken arrow lines, while
privileged ones are represented by complete arrows as shown in Fig.6’s legend.

9

Security and PrivacyApproaches to Mixed RealityOutput ProtectionData Access ProtectionInput ProtectionInteraction ProtectionDevice ProtectionProtected RenderingReliable OutputsProtected SharingProtected CollaborationAuthenticationPhysical ProtectionExtrinsic ProtectionIntrinsic ProtectionUser Input ProtectionProtected DisplaysData AggregationData ProcessingData StorageDetectionTransformationRenderingEnvironmentEnvironmentInput ProtectionData Access ProtectionOutput  ProtectionFig. 6. Shows a generic block diagram that inserts an intermediary protection layer between the applica-
tions and device resources. (Data flows to and from third-party applications are now limited or less-privileged
as represented by the broken arrows.)

according to the five major categories and, if applicable, to their further subcategories. There
may be instances that presented solutions may address several aspects and may fall on more than
one category. For these cases, we focus on the primary objective or challenges focused on their
approach. We first discuss the threats specific to that category and, then, follow it with the security
and privacy approaches, and strategies both in literature and those used in existing systems.

3.1 Input Protection
Perhaps, the main threat to input protection, as well as to the other four categories, is the unautho-
rized and/or unintended disclosure of information – may it be of actual data entities or of their flow.
These vulnerable inputs can be categorized in to two based on the user intention. Targeted physical
objects and other non-user-intended inputs are both captured from the environment usually for
visual augmentation anchoring. We can collectively call these inputs as passive, while those that
are intentionally provided by users, such as gestures, can be considered as active inputs.

3.1.1 Threats to Passive Inputs: Targeted and Non-intended Latent Data. Aside from
threats to confidentiality (i.e. information disclosure), the two other main threats in the input side
are detectability and user content unawareness. Both stems from the fact that these MR systems (just
like any other service that employs a significant number of sensors) collects a lot of information,
and among these are necessary and sensitive information alike. As more of these services becomes
personalized, the sensitivity of these information increases. These threats are very evident with
visual data. MR, as well as AR and VR, requires the detection of targets, i.e. objects or contexts,
in the real environment, but other non-necessary and sensitive information are captured as well.
These objects become detectable and users may not intend for these latent information or contexts
to be detected. Likewise, as they continuously use their MR device, users may not be made aware
that the applications running on their device are also able to collect information about these objects
and contexts.

Protection Approaches. The most common input protection approaches usually involves the
removal of latent and sensitive information from the input data stream. These approaches are
generally called input sanitization techniques (see samples labelled 1 and 2 in Figure 7). These are

10

Trusted Mixed Reality Platform13452Mixed Reality Environment←  →Supporting Data ServicesIntermediary Protection LayerOther Outputs3D Display... Camera + DepthOther Sensors... App 1App 2App 3App N. . .Data1AppData2AppData3AppDataNApp1 unread SMS1 New EmailLegend:  Privileged       Less-privilegedoutput data flowinput data flowtwo-way data flowFig. 7. Example strategies for input protection: 1) information reduction or partial sanitization, e.g. from RGB
facial information to facial outline only; 2) complete sanitization or blocking; or 3) skeletal information instead
of raw hand video capture.

usually implemented as an intermediary layer between the sensor interfaces and the applications as
shown in Figure 6. In general, this protection layer acts as an input access control mechanism aside
from sanitization. These techniques can further be categorized according to the policy enforcement
– whether intrinsic or extrinsic policies for protection are used. With intrinsic enforcement, the
user, device, or system itself imposes the protection policies that dictates the input sanitization that
is applied. On the other hand, extrinsic input protection arises from the need for sensitive objects
external to the user that are not considered by the intrinsic policies. In the following subsections,
the sanitization techniques are presented as either intrinsic or extrinsic approaches.

(1) Intrinsic input sanitization policies are usually user-defined. For example, the Darkly system
(Jana et al. 2013b) for perceptual applications uses OpenCV in its intermediary input protec-
tion layer to implement a multi-level feature sanitation. The basis for the level or degree of
sanitization are the user-defined policies. The users can impose different degrees of sensitivity
permissions which affects the amount of detail or features which can be provided to the appli-
cations, i.e. stricter policies mean less features are provided. For example, facial information
can vary from showing facial feature contours (of eyes, nose, brows, mouth, and so on) to just
the head contour depending on the user’s preferences. The user can actively control the level
of information that is provided to the applications. Thus, aside from providing undetectability
& unobservability, and content awareness to users, Darkly also provides a form of authorization
through information access control, specifically a least privilege access control.

Context-based Sanitization. A context-based intrinsic sanitization framework (Zarepour
et al. 2016) improves on the non-contextual policies of Darkly. It determines if there are
sensitive objects in the captured images, like faces or car registration plates, and automatically
implements sanitization. Sensitive features are sanitized by blurring them out, while images of
sensitive locations (e.g. bathrooms) are deleted entirely. Similarly, PlaceAvoider (Templeman
et al. 2014) also detects images as sensitive or not, depending on the features extracted from the
image, but deletion is not automatic and still depends on the user. Despite the context-based
nature of the sanitization, the policy that governs how to interpret the extracted contexts are
still user-defined, thus, we consider both sanitization techniques as intrinsic. However, intrinsic
policy enforcement can be considered as self-policing which can potentially have a myopic
view of privacy preferences of other users and objects. Furthermore, intrinsic policies can only
protect the inputs that are explicitly identified in the policies.

11

Input PolicyInput Intermediary Protection Layer123Video Sanitization. The previously discussed sanitization techniques were targeted for
generic capturing devices and were mostly sanitizing images and performs the sanitization
after the image is stored. For MR platforms that require real-time video feed, there is a need
for live and on-the-fly sanitization of data to ensure security and privacy. A privacy-sensitive
visual monitoring (Szczuko 2014) system was implemented by removing persons from a video
surveillance feed and render 3D animated humanoids in place of the detected and visually-
removed persons. Another privacy-aware live video analytic system called OpenFace-RTFace
(Wang et al. 2017) focused on performing fast video sanitization by combining it with face
recognition. The OpenFace-RTFace system lies near the edge of the network, or on cloudlets.
Similar approaches to edge or cloud-assisted information sanitization can potentially be utilized
for MR.

(2) Extrinsic input sanitization receives input policies, e.g. privacy preferences, from the environ-
ment. An early implementation (Truong et al. 2005) involved outright capture interference
to prevent sensitive objects from being captured by unauthorized visual capturing devices. A
camera-projector set up is used. The camera detects unauthorized visual capture devices, and
the projector beams a directed light source to “blind” the unauthorized device. This technique
can be generalized as a form of a physical access control, or, specifically, a deterrent to physical
or visual access. However, this implementation requires a dedicated set up for every sensitive
space or object, and the light beams can be disruptive to regular operation.

Other approaches involves the use of existing communication channels or infrastructure
for endorsing or communicating policies to capture devices, and to ensure that enforcement is
less disruptive. The goal was to implement a fine-grained permission layer to “automatically”
grant or deny access to continuous sensing or capture of any real-world object. A simple
implementation on a privacy-aware see-through system (Hayashi et al. 2010) allowed other
users that are “seen-through” to be blurred out or sanitized and shown as human icons only
if the viewer is not their friend. However, this requires that users have access to the shared
database and explicitly identify friends. Furthermore, enabling virtually anyone or, in this case,
anything to specify policies opens new risks such as forgery, and malicious policies.

To address authenticity issues in this so called world-driven access control, policies can
be transmitted as digital certificates (Roesner et al. 2014c) using a public key infrastructure
(PKI). Thus, the PKI provides cryptographic protection to media access and sanitization policy
transmission. However, the use of a shared database requires that all possible users’ or sensitive
objects’ privacy preferences have to be pushed to this shared database. Furthermore, it excludes
or, unintentionally, leaves out users or objects that are not part of the database –or, perhaps,
are unaware – which, then, defeats the purpose of a world-driven protection.

I-pic (Aditya et al. 2016) removes the involvement of shared databases. Instead users
endorse privacy choices via a peer-to-peer approach using Bluetooth Low Energy (BLE) devices.
However, I-pic is only a capture-or-no system. PrivacyCamera (Li et al. 2016b) is another
peer-to-peer approach but is not limited to BLE. Also, it performs face blurring, instead of
just capture-or-no, using endorsed GPS information to determine if sensitive users are within
camera view. On the other hand, Cardea (Shu et al. 2016) allows users to use hand gestures to
endorse privacy choices. In Cardea, users can show their palms to signal protection while a
peace-sign to signal no need for protection. However, these three approaches are primarily
targeted for bystander privacy protection, i.e. facial information sanitization.

MarkIt (Raval et al. 2014) can provide protection to any user or object uses privacy
markers and gestures (similar to Cardea) to endorse privacy preferences to cameras. It was

12

integrated to Android’s camera subsystem to prevent applications from leaking private informa-
tion (Raval et al. 2016) by sanitizing sensitive media. This is a step closer to automatic extrinsic
input sanitization, but it requires visual markers in detecting sensitive objects. Furthermore,
all these extrinsic approaches have only been targeted for visual capture applications and not
with AR- or MR-specific ones.

3.1.2 Threats to Gestures and other Active User Inputs. Another essential input that needs
to be protected is gesture input. We put a separate emphasis on this as gesture inputs entails
a ‘direct’ command to the system, while the previous latent and user inputs do not necessarily
invoke commands. Currently, the most widely adopted user input interfaces are the tactile types,
specifically, the keyboard, computer mouse, and touch interfaces. However, these current tactile
inputs are limited by the dimension7 of space that they are interacting with and some MR devices
now don’t have such interfaces. Also, these input interface types are prone to a more physical
threat such as external inference or shoulder-surfing attack. From which, threats such as spoofing,
denial of service, or tampering may arise.

Furthermore, there is a necessity for new user input interfaces to allow three-dimensional inputs.
Early approaches used gloves (Dorfmuller-Ulhaas and Schmalstieg 2001; Thomas and Piekarski
2002) that can determine hand movements, but advances in computer vision have led to tether-
and glove-free 3D interactions. Gesture inference from smart watch movement have also been
explored as a possible input channel, particularly on finger-writing inference (Xu et al. 2015). Now,
vision-based natural user interfaces (NUI), such as the Leap Motion (Zhao and Seah 2016) and
Microsoft Xbox Kinect, have long been integrated with MR systems to allow users to interact with
virtual objects beyond two dimensions. This allows the use of body movement or gestures as input
channels and move away from keypad and keyboards. However, the use of visual capture to detect
user gestures or using smart watch movement to detect keyboard strokes means that applications
that require gesture inputs can inadvertently capture other sensitive inputs (Maiti et al. 2016).
Similar latent privacy risks such as detectability and content unawareness arise. Thus, while new
ways of interacting in MR are being explored, security and privacy should also be ensured.

Protection through abstraction. Prepose (Figueiredo et al. 2016) provides secure gesture detection
and recognition as an intermediary layer (as in Figure 6). The Prepose core only sends gesture
events to the applications, which effectively removes the necessity for untrusted applications to
have access to the raw input feed. Similar to Darkly, it provides least privilege access control to
applications, i.e. only the necessary gesture event information is transmitted to the third party
applications and not the raw gesture feed.

A preceding work to Prepose implemented the similar idea of inserting a hierarchical recognizer
(Jana et al. 2013a) as an intermediary input protection layer. They inserted Recognizers to the Xbox
Kinect to address input sanitization as well as to provide input access control. The policy is user-
defined; thus, it is an intrinsic approach. Similarly, the goal is to implement a least privilege approach
to application access to inputs – applications are only given the least amount of information
necessary to run. For example, a dance game in Xbox, e.g. Dance Central or Just Dance, only needs
body skeletal (similar to sample labelled 3 in Figure 7) movement information, and it does not need
facial information, thus, the dance games are only provided with the moving skeletal information
and not the raw video feed of the user while playing. To handle multiple levels of input policies, the
recognizer implements a hierarchy of privileges in a tree structure, with the root having highest

7Keyboards and other input pads can be considered as one-dimensional interfaces, while the mouse and the touch interfaces
provides two-dimensional space interactions with limited third dimension using scroll, pan, and zoom capabilities.

13

privilege, i.e. access to RGB and depth information, and the leaves having lesser privileges, i.e.
access to skeletal information.

3.1.3 Remaining Challenges in Input Protection. Most of the approaches to input protection are
founded on the idea of least privilege. However, it requires that the intermediary layer, e.g. the
Recognizers, must know what type of inputs or objects the different applications will require.
Prepose addresses this for future gestures but not for future objects. For example, an MR painting
application may require the detection of different types of brushes but the current recognizer does
not know how to ‘see’ or detect the brushes. Extrinsic approaches like MarkIt try to address
this by using markers to tell which objects can and cannot be seen. What seemingly arises now
is the need to have a dynamic abstraction and/or sanitization of both pre-determined and future
sensitive objects. In §3.5.2, we will focus on device-level protection approaches to protect user
activity involving input interfaces.

3.2 Data Protection
We can divide the different data protection techniques based on the data flow. First, after sensing,
data is gathered and aggregated, thus, protected data aggregation is necessary. Then, to deliver
output, applications will need to process the data, thus, privacy-preserving data processing is
required. Ultimately, the data storage has to be protected as well. Thus, three major data protection
layers arise: aggregation, processing, and storage.

Generally, the aim of these data protection approaches is to allow services or third-party applica-
tions to learn something without leaking unnecessary and/or personally identifiable information.
Usually, these protection approaches use privacy definitions such as k-anonymity, and differen-
tial privacy. k-anonymity (Samarati 2001; Samarati and Sweeney 1998) ensures that records are
unidentifiable from at least k-1 other records. It usually involves data perturbation or manipulation
techniques to ensure privacy, but suffers from scaling problems, i.e. larger data dimensions, which
can be expected from MR platforms and devices with a high number of sensors or input data sources.
Differentially private algorithms (Dwork et al. 2014), on the other hand, inserts randomness to
data to provide plausible deniability and unlinkability. The guaranteed privacy of differentially
private algorithms is well-studied (McSherry and Talwar 2007). Ultimately, there are other privacy
definitions or metrics used in the literature. In the following subsections, we now focus on the
different threats and approaches to data aggregation, processing, and storage.

3.2.1 Threats to Data Collection and Aggregation. Essentially, data collection also falls under
the input category but we transfer the focus to data after sensing and how systems, applications,
and services handle data as a whole. The main threats to data collection are tampering, denial of
service, and unauthorized access among others. These are primarily security threats. For example,
an adversary can tamper MR targets to elicit a different response from the system or to outright
deny a service. For these types of threats, we will focus on these later under device protection on
§3.5.

Aside from security threats, linkability, detectability, and identifiability are some of the privacy
threats that results from continuous or persistent collection of data. Aggregation further aggravates
these threats due to the increase in the number of channels and the amount of information.

Protected Data Collection and Aggregation. Protected data collection and aggregation approaches
are also implemented as an intermediate layer as in Figure 6. Usually, data perturbation or similar
mechanisms are run on this intermediary layer to provide a privacy guarantee, e.g. differential
privacy or k-anonymity.

14

(1) Privacy-preserving data collection and aggregation. RAPPOR or randomized response (Erlingsson
et al. 2014) is an example of a differentially-private data collection and aggregation algorithm. It
is primarily applied for privacy-preserving crowd-sourced information such as those collected
by Google for their Maps services. Privacy-preserving data aggregation (PDA) has also been
adopted for information collection systems (He et al. 2007, 2011) with multiple data collection
or sensor points, such as wireless sensor networks or body area networks. The premise of PDA
is to get aggregate statistic or information without knowing individual information, whether
of individual sensors or users. A similar PDA approach specific to MR data is still yet to be
designed, developed, and evaluated.

(2) Using abstraction for authorized sensor access. SemaDroid (Xu and Zhu 2015), on the other hand,
is a device level protection approach. It is a privacy-aware sensor management framework that
extends the current sensor management framework of Android and allows users to specify and
control fine-grained permissions to applications accessing sensors. Just like the abstraction
strategies in input protection, SemaDroid is implemented as an intermediary protection layer
that provides users application access control or authorization to sensors and sensor data. What
differentiates it from a the input protection techniques are the application of auditing and
reporting of potential leakage and applying them to a privacy bargain. This allows users to ‘trade’
their data or privacy in exchange for services from the applications. There are a significant
number of work on privacy bargain and the larger area of privacy economics but we will not
be elaborating on it further and point the readers to Acquisti’s work on privacy economics
(Acquisti et al. 2016).

3.2.2 Threats to Data Processing. After collection, most services will have to process the data
immediately to deliver outputs fast and real-time. Similar to data collection, the same privacy threats
of information disclosure, linkability, detectability, and identifiability holds. During processing,
third-party applications or services can directly access user data which may contain sensitive or
personal information if no protection measures are implemented.

Protection Approaches. The same premise holds: applications process data to deliver services
but with data security and user privacy in mind. Both secure and privacy-preserving data process-
ing algorithms can be applied to achieve both security and privacy properties such as integrity,
confidentiality, unlinkability, and plausible deniability among others.

(1) Encryption-based techniques. Homomorphic encryption (HE) allows queries or computations
over encrypted data. There are varying levels of homomorphism from partial, such as Paillier
encryption (Paillier 1999) which is homomorphic in addition (and, to some extent, multiplica-
tion), to fully homomorphic encryption (FHE) (Gentry 2009). Upon collection, data is encrypted
first and homomorphism allows third-party service to make computations over data without
decryption. This technique is primarily used for remote data processing especially when data
processors are not trusted.

In visual data processing, encryption-based techniques have been used for image feature
extraction, and matching for various uses such as image search, and context or object detection.
He-Sift (Hsu et al. 2011) performs bit-reversing and local encryption to the raw image before
feature description using SIFT8 (Lowe 2004). The goal was to make dominant features, which
can be used for context inference, recessive. As a result, feature extraction, description, and
matching are all performed in the encrypted domain. A major drawback is the very slow
computation time due to the near full-homomorphism used as well as the approach being

8SIFT or Scale-invariant Feature Transform is a popular image feature extraction and description algorithm

15

Fig. 8. Generic block diagrams of two different data protection approaches: 1) cryptographic technique using
secure multi-party computation where two or more parties exchange secrets (1.1 and 1.3) to extract combined
knowledge (1.2 and 1.4) without the need for divulging or decrypting each others data share; and 2) personal
data stores with “trusted” applets.

algorithm-specific. Using leveled HE can reduce the computation time of He-Sift (Jiang et al.
2017).

Other improvements utilizes big data computation techniques to expedite secure image
processing such as the use of a combination of MapReduce and ciphertext-policy attribute-based
encryption (Zhang et al. 2014), or the use of Google’s Encrypted BigQuery Client for Paillier HE
computations (Ziad et al. 2016).

(2) Secret Sharing or Secure Multi-party Computation. Data can be split among untrusted parties
assuming that information can only be inferred when the distributed parts are together. Secure
multi-party computation (SMC) or secret sharing allows computation of data from two or more
sources without necessarily knowing about the actual data each source has. The diagram
labelled 1 in Figure 8 shows a possible SMC setup. For example, App 1 requires data from
another party (could be anohter application) to provide a certain service. It encrypts its share
of the data (step 1.0) and sends it to the other party (1.1). The other party then encrypts its
other share (1.2) and sends it to App 1 (1.3). Both can compute the results over the combined
encrypted shares without the need to decrypt their shares.

There are various approaches in performing secret sharing such as garbled circuits (Huang
et al. 2011; Yao 1986), and other non-cryptographic-based ones. For example, SecSift (Qin
et al. 2014a,b) improves on the computation time of He-Sift by using a somewhat homomor-
phic encryption. They split or distribute the SIFT feature computation tasks among a set of
“independent, co-operative cloud servers [to] keep the outsourced computation procedures
as simple as possible [and] avoid utilizing homomorphic encryption.” There are two primary
image processing tasks that SecSIFT provide (Qin et al. 2014c): color histogram and layout
descriptors. These two tasks allows for a variety of image processing tasks compared to He-Sift
which is specific to feature matching.

P3 (Ra et al. 2013) focuses on photo sharing and uses two-party secret sharing by “by
splitting a photo into a public part, which contains most of the volume (in bytes) of the
original, and a secret part which contains most of the original’s information.” It uses AES-based

16

Trusted Mixed Reality PlatformSecure Multi-party ComputationPersonalData StoreApplet 2Applet 3Applet N. . .13452Mixed Reality Environment←  →Supporting Data ServicesOther Outputs3D Display... Camera + DepthOther SensorsApp 1App 2App 3App N. . .Data1AppData2AppData3AppDataNApp2OtherPartyDataPartyOther1... 1.01.11.21.41.3symmetric keys to encrypt the secret part and allows the use of a tunable parameter between
storage/bandwidth and privacy. This approach, however, is JPEG-format specific.

A privacy-preserving virtual cloth try-on (Sekhavat 2017) service used secret sharing and
secure two-party computation. The anthropometric information9 of the user is split between
the user’s mobile device and the server, and are both encrypted. The server has a database
of clothing information. The server can then compute a 3D model of the user wearing the
piece of clothing by combining the anthropometric information and the clothing information
to generate an encrypted output which is sent to the user device. The user device decrypts the
result and combines it with the local secret to reveal the 3D model of the user “wearing” the
piece of clothing.

(3) Virtual Reconstructions. We can take advantage of the artificiality that MR provides. Instead
of providing complete 3D (RGB+depth) data, a 3D sanitized or ‘salted’ virtual reconstruction
of the physical space can be provided to third-party applications. For example, instead of
showing the 3D capture of a table in the scene with all 3D data of the objects on the table, a
generalized horizontal platform or surface can be provided. The potentially sensitive objects on
the table are kept confidential. A tunable parameter balances between sanitization (or salting)
and reconstruction latency. Using this tunability, similar notions of privacy guarantee can
be provided such as differential privacy and k-anonymity. However, this approach is yet to
be realized but virtual reconstruction has been used to address delayed alignment issues in
AR (Waegel 2014). This approach can work well with other detection (§3.1.2) and rendering
(§3.3.2) strategies of sanitization and abstraction as well as in privacy-centred collaborative
interactions (§3.4.1). This approach also opens the possibility to have an active defence strategy
where ’salted’ reconstructions are offered as a honeypot to adversaries.

All these techniques complement each other and can be used simultaneously on a singular system.
Inevitably, we understand the equal importance of all these technologies and how they can be used
on MR, but these data protection techniques are technology agnostic. Therefore, any or all of these
techniques can be applied to MR and it will only be a matter of whether the technique is appropriate
for the amount of data and level of sensitivity of data that is tackled in MR environments.

3.2.3 Threats to Data Storage. After collection and aggregation, applications store user data
on separate databases in which users have minimal or no control over. Privacy concerns on how
these applications use user data beyond the expected utility to the user have been posed (Felt et al.
2012; Lee et al. 2015; Ren et al. 2016). Aside from these privacy threats, there are inherent security
threats such as tampering, unauthorized access, and spoofing that are faced by data storage.

Data Storage Solutions to Protection. When trustworthiness is not ensured, protected data storage
solutions, such as personal data stores (PDS), with managed application access permission control
is necessary. PDSs allows the users to have control over their data, and which applications have
access to it. In addition, further boundary protection and monitoring can be enforced on the flow
of data in and out of the PDS. Figure 8 shows a generic block diagram (labelled 2) of how a PDS
protects the user data by running it in a protected sand-box machine that may monitor the data
that is provided to the applications. Usually, applet versions (represented by the smaller App blocks
within the PDS) of the applications run within the sand-box. Various PDS implementations have
been proposed such as the personal data vaults (PDV), OpenPDS, and the Databox.
(1) The PDV was one of the earlier implementations of a PDS but it only supports a few number of
data sources, i.e. location information, and does not have an application programming interface

9Anthropometric information are body measurements of an individual that capture size, shape, body composition, and so
on.

17

(or API) for other data sources or types. Nonetheless, they demonstrated how data storage and
data sharing to applications can be decoupled (Mun et al. 2010).

(2) OpenPDS improves on the lack of API of the PDV. OpenPDS allows any application to have
access to user data through SafeAnswers (de Montjoye et al. 2014). SafeAnswers (SA) are
pre-submitted and pre-approved query application modules (just like an applet in Figure 8)
which allows applications to retrieve results from the PDS using. However, the necessity of
requiring applications to have a set of pre-approved queries reduces the flexibility of openPDS.
(3) Databox also involves the use of a sandbox machine where users store their data, and applica-
tions run a containerized piece of query application that is trusted. The running of containers
in the sandbox allows users to have control of what pieces of information can exit from the
sandbox. However, similar to SafeAnswers requiring application developers to develop a con-
tainerized application for their queries may be a hindrance to adaptation. Despite that, Databox
pushes for a privacy ecosystem which empowers users to trade their data for services similar
to privacy bargains in a privacy economy (Crabtree et al. 2016). This privacy ecosystem can
possibly assist in addressing the adaptability issues of developing containerized privacy-aware
query applications, because users can now demand service in exchange for their data.

3.2.4 Remaining Challenges in Data Protection. Most of the data protection techniques discussed
were implemented on generic systems and not necessarily MR-targeted. MR data is expected to be
visual-heavy, and information collected is not only confined to users but also of other externally
sensitive information that can be captured. Moreover, there are necessary modifications that
applications have to partake in order to implement these data protection strategies. Aside from
implementation complexity are the additional resources necessary such as the inherent need of
memory, and compute capacity to use encryption-based schemes. There are attempts to eliminate
the necessity of code modification such as GUPT (Mohan et al. 2012), which focuses on the sampling
and aggregation process to ensure distribution of the differential privacy budget and eliminating the
need for costly encryption. Also, combining these techniques with protected sensor management
and data storage to provide confidentiality through sanitization and authorized access control is
promising.

3.3 Output Protection
The prime value of MR is to deliver immersive experiences. To achieve that, applications ship
services and experiences in the form of rendered outputs. In general, there are three possible types
of outputs in MR systems: real-world anchored outputs, non-anchored outputs, and outputs of
external displays. The first two types are both augmented outputs. The last type refers to outputs
of other external displays which can be utilized by MR systems, and vice versa. Protecting these
outputs is of paramount importance aside from ensuring input and data protection. As a result,
there are three enduring points or aspects of protection when it comes to the output: protecting
external displays, output control, and protected rendering.

3.3.1 Threats to Output Reliability and User Safety. Current ‘reality’ systems have loose
output access control. As a result, adversaries can potentially tamper or spoof outputs that can
compromise user safety. In addition, reliability is also compromised resulting to threats such as
denial of service, and policy & consent non-compliance.

Safe and Reliable Outputs. Output control policies can be used as a guiding framework on how
MR devices will handle outputs from third-party applications. This includes the management
of rendering priority which could be in terms of synthetic object transparency, arrangement,
occlusion, and other possible spatial attributes. An output access control framework (Lebeck et al.

18

2016) with an object-level of granularity have been proposed to make output handling enforcement
easier. It can be implemented as an intermediary layer, as in Figure 6, and follows a set of output
policies. In a follow up work, they presented a design framework (Lebeck et al. 2017) for output
policy specification and enforcement which combined output policies from Microsft’s HoloLens
Developer guidelines, and the U.S. Department of Transportation’s National Highway Traffic
Safety Administration (NHTSA) (for user safety in automobile-installed AR). Here are two example
descriptions of their policies: “Don’t obscure pedestrians or road signs” is inspired from the NHTSA;
“Don’t allow AR objects to occlude other AR objects” is inspired from the HoloLens’s guidelines.
They designed a prototype platform called Arya that will implement the application output control
based on the output policies specified and evaluated Arya on various simulated scenarios. As of
yet, Arya is the only AR or MR output access control approach in the literature.

3.3.2 Threats during Rendering. Other MR environments incorporates any surface or medium
as a possible output display medium. For example, when a wall is used as a display surface in an MR
environment, the applications that use it can potentially capture the objects or other latent and/or
sensitive information within the wall during the detection process. This specific case intersects
very well with the input category because what is compromised here is the sensitive information
that can be captured in trying to determine the possible surfaces for displaying.

Privacy-preserving Rendering. Applications that requires such displays do not need to know what
the contents in the wall are. It only has to know that there is a surface that can be used as a
display. Protected output rendering protects the medium and, by extension, whatever is in the
medium. Least privilege has been used in this context (Vilk et al. 2014). For example, in a room-scale
MR environment, only the skeletal information of the room, and the location and orientation of
the detected surfaces (or display devices) is made known to the applications that wish to display
content on these display surfaces (Vilk et al. 2015). This utilizes the same abstraction strategy which
have been used both in input and data protection to provide unobservability & undetectability and
authorized access. This example of room-scale MR environments is usually used for collaborative
purposes.

3.3.3 Threats to Output Displays. Output displays are vulnerable to physical inference threats
or visual channel exploits such as shoulder-surfing attacks. These are the same threats to user
inputs (§3.1.2) especially when the input and output interfaces are on the same medium or are
integrated together such as touch screens.

Protecting outputs from external inference. To provide secrecy and privacy on certain sensitive
contexts which requires output confidentiality (e.g. ATM bank transactions), MR can be leveraged
to provide this kind of protection. This time, MR capabilities are leveraged for output defense
strategies.

(1) Content hiding methods. EyeGuide (Eaddy et al. 2004) used a near-eye HMD to provide a
navigation service that delivers secret and private navigation information augmented on a
public map display. Because the EyeGuide display is practically secret, shoulder surfing is
prevented.

Other approaches involve the actual hiding of content. For example, VRCodes (Woo et al.
2012) takes advantage of rolling shutter to hide codes from human eyes but can be detected by
cameras at a specific frame rate. A similar approach has been used to hide AR tags in video (Lin
et al. 2017). This type of technique can hide content from human attackers but is still vulnerable
to machine-aided inference or capture.

19

(2) Visual cryptography. Secret display approaches have also been used in visual cryptographic
techniques such as visual secret sharing (VSS) schemes. VSS allows the ‘mechanical’ decryption
of secrets by overlaying the visual cipher with the visual key. However, classical VSS was
targeted for printed content (Chang et al. 2010) and requires strict alignment which is difficult
even for AR and MR displays, particularly handhelds and HMDs. The VSS technique can then
be relaxed by using code-based secret sharing, e.g. barcodes, QR codes, 2D barcodes, and so
on. The ciphers are publicly viewable while the key is kept secret. An AR-device can then
be used to read the cipher and augment the decrypted content over the cipher. This type of
visual cryptography have been applied to both print (Simkin et al. 2014) and electronic displays
(Andrabi et al. 2015; Lantz et al. 2015).

Electronic displays are, however, prone to attacks from malicious applications which has
access to the display. One of these possible attacks is cipher rearrangement for multiple ciphers.
To prevent such in untrusted electronic displays, a visual ordinal cue (Fang and Chang 2010)
can be combined with the ciphers to provide the users immediate signal if they have been
rearranged.

These techniques can also be used to provide protection for sensitive content on displays
during input sensing. Instead of providing privacy protection through post-capture sanitization,
the captured ciphers will remain secure as long as the secret shares or keys are kept secure. Thus,
even if the ciphers are captured during input sensing, the content stays secure. In general, these
visual cryptography and content-hiding methods provide visual access control, and information
protection in shared or public resources. More device-level examples of this technique are
discussed in §3.5.2.

3.3.4 Remaining Challenges in Output Protection. Similar to input protection, output protection
strategies can use the same abstraction approach applied as an intermediary access control layer
between applications and output interfaces or rendering resources. To enforce these output
abstractions, a reference policy framework has to exist through which the abstraction is based upon.
As a result, perhaps, the biggest challenge is the specification and enforcement of these policies –
particularly on who will specify them and how will they be effectively enforced. In the output side,
risks and dangers are more imminent because adversaries are about to actuate or have already
actuated the malicious response or output. Thus, these access control strategies and policies are
significant approaches to output protection.

Likewise, malicious inference or capture of outputs present the same threats as input inference.

§3.5.2 will focus on device-level protection approaches to output interfaces and displays.

3.4 Protecting User Interactions
When it comes to collaboration, technologies such as audio-visual teleconferencing, and computer-
supported collaborative work (or CSCW) have been around to enable live sharing of information
among multiple users. These are called shared space technologies as more than one user interacts
in the same shared space as shown in Figure 9a. And MR offers a much more immersive sharing
space.

Concerns on the boundaries – “transparent boundaries between physical and synthetic spaces”
– in MR and on the directionality of these boundaries have been raised (Benford et al. 1998). The
directionality can influence the balance of power, mutuality and privacy between users in shared
spaces. For example, the boundary (labelled 1) in Figure 9b allows User 2 to receive full information
(solid arrow labelled 2) from User 1 while User 1 receives partial information (broken arrow labelled
3) from User 2. The boundary enables an ‘imbalance of power’ which can have potential privacy
and ethical effects on the users.

20

(a) A simplified virtual shared space
diagram

(b) A possible separation in the
underlying physical space which
creates boundaries between users,
and devices.

Fig. 9. Shared Spaces

(c) A collaborative space with both
a shared space and a private space.

Early attempts on ensuring user privacy in a collaborative MR context was to provide users with
a private space, as shown in Figure 9c, that displays outputs only for that specific user while having
a shared space for collaboration. For example, the PIP or personal interaction panel was designed
to serve as a private interface for actions and tasks that the user does not want to share to the
collaborative virtual space (Szalav´ari and Gervautz 1997). It is composed of a tracked “dumb” panel
and a pen. It was used as a gaming console to evaluate its performance. In the following subsection,
we first take a look at some other early work on collaborative interactions, and, then, proceed with
examples of how to manage privacy in shared spaces.

3.4.1 Threats during Collaborative Interactions. Early collaborative platform prototypes
(Billinghurst and Kato 1999; Grasset and Gascuel 2002; Hua et al. 2004; Regenbrecht et al. 2002;
Schmalstieg and Hesina 2002) demonstrated fully three-dimensional collaboration in MR. However,
none have addressed the concerns raised from information sharing due to the boundaries created
by shared spaces. An adversarial user can potentially tamper, spoof, or repudiate malicious actions
during these interactions. As a result, legitimate users may suffer denial of service and may be
unaware that their personal data may have been captured and, then, leaked.

Protecting Collaborative Interactions. To provide protection during interactions, policy speci-
fication from users and its enforcement are the primary strategies. The system through which
these interactions are coursed through should support the specification and enforcement of these
policies.

(1) Enabling user-originated policies. Emmie (Environmental Management for Multi-user Informa-
tion Environments) (Butz et al. 1999) is a hybrid multi-interface collaborative environment
which uses AR as a 3D interface at the same time allowing users to specify privacy of certain
information or objects through privacy lamps and vampire mirrors (Butz et al. 1998). EMMIE’s
privacy lamps are virtual lamps that ‘emit’ a light cone in which users can put objects within
the light cone to mark these objects as private. On the other hand, the vampire mirrors are
used to determine privacy of objects by showing full reflections of public objects while private
objects are either invisible or transparent. However, the privacy lamps and vampire mirrors
only protect virtual or synthetic content and does not provide protection to real-world objects.
Kinected Conference (DeVincenzi et al. 2011) allows the participants to use gestures to
impose a temporary private session during a video conference. Aside from that, they imple-
mented synthetic focusing using Microsoft Kinect’s depth sensing capability – other participants

21

are blurred in order to direct focus on a participant who is speaking –, and augmented graphics
hovering above the user’s heads to show their information such as name, shared documents,
and speaking time. The augmented graphics serve as feed-through information to deliver signals
that would have been available in a shared physical space but is not readily cross-conveyed
between remote physical spaces.

(2) Feed-through signalling. SecSpace (Reilly et al. 2014) explores a feed-through mechanism to
allow a more natural approach to user management of privacy in a collaborative MR envi-
ronment. In contrast to Kinected Conference’s gesture-based privacy session, and Emmie’s
privacy lamps and vampire mirrors, users in SecSpace are provided feed-through information
that would allow them to negotiate their privacy preferences. Figure 9b shows an example
situation in which User n enters the shared space (labelled 4) on the same physical space as
User 2 which triggers an alarm (labelled 5) or notification for User 1. The notification serves
as a feed-through signalling that crosses over the MR boundary. By informing participants of
such information, an imbalance of power can be rebalanced through negotiations.

Non-AR Feed-through signalling have also been used in a non-shared space context
like the candid interactions (Ens et al. 2015) which uses wearable bands that lights up in
different colors depending on the smart-phone activity of the user, or other wearable icons that
change shape, again, depending on which application the icon is associated to. However, the
pervasive nature of these feed-through mechanisms can still pose security and privacy risks,
thus, these mechanisms should be regulated and properly managed. In addition, the necessary
infrastructure, especially for SecSpace, to enable this pervasive feed-through system may be a
detriment to adaptation. A careful balance between the users’ privacy in a shared space and
the utility of the space as a communication medium is ought to be sought.

(3) Private and public space interactions. Competitive gaming demands secrecy and privacy in order
to make strategies while performing other tasks in a shared environment. Thus, it is a very apt
use case for implementing user protection in a shared space. Private Interaction Panels (or
PIPs) demonstrates a gaming console functionality where a region that is defined within the
PIP panel serves as a private region (Szalav´ari et al. 1998). For example, in a game of Mah-jongg,
the PIP panel serves as the user’s space for secret tiles while all users can see the public tiles
through their HMDs. The PIP pen is used to pick-and-drop tiles between the private space and
the public space. On the other hand, TouchSpace implements a larger room-scale MR game. It
uses an HMD that can switch between see-through AR and full VR, an entire floor as shared
game space with markers, and a wand for user interactions with virtual objects (Cheok et al.
2002). Essentially, Emmie’s privacy lamps and mirrors also act as private spaces.

BragFish (Xu et al. 2008) implements a similar idea on privacy to that of the PIP with the
use of a handheld AR device, i.e. Gizmondo. In BragFish, a game table with markers serves as
the shared space, while each user has the handheld AR that serves as the private space for each
user. The handheld AR device has a camera that is used to “read” the markers associated to a
certain game setting, and it frees the user from the bulky HMDs as in PIP and TouchSpace.
The Gizmondo handheld device has also been used in another room-scale AR game (Mulloni
et al. 2008). Similarly, camera phones have been used as a handheld AR device in a table top
marker-based setup for collaborative gaming (Henrysson et al. 2005).

In collaborative mixed reality, aside from policy enforcement and feed-through signalling, there
are two basic spaces that can be utilised during interactions: a shared space for public objects,
and a private space for user-sensitive tasks such as making strategies. All examples, such as PIP
and BragFish, assumes that each user can freely share and exchange content or information
through a shared platform or interaction channel. Furthermore, these privacy-sensitive shared

22

space approaches are also, to some extent, inherently distributed which provides further security
and privacy. In the next section, we focus on how this simple act of sharing can be protected in a
mixed reality context without a pre-existing shared channel.

3.4.2 Threats to Sharing Initialization. All those shared space systems that were previously
discussed rely on a unified architecture to enable interactions and sharing on the same channel.
However, there might be cases that sharing is necessary but no pre-exisiting channel exists, or an
entire architecture, just like in SecSpace or EMMIE, to support sharing is not readily available.
Thus, a sharing channel needs to be initialized. The same threats of spoofing and unauthorized
access from Personal Area Networks such as ZigBee or Bluetooth arises.

Securing Sharing Channels. Likewise, similar techniques of out-of-band channels can be used to
achieve a secure channel initialization. Looks Good To Me (LGTM) is an authentication protocol
for device-to-device sharing (Gaebel et al. 2016). It is leveraged on the camera/s and wireless
capabilities of existing AR HMDs. Specifically, it uses the combination of distance information
through wireless localization and facial recognition information to cross-authenticate users. In
other words, using the AR HMD that has a camera and wireless connectivity, users can simply
look at each other to authenticate and initiate sharing. HoloPair, on the other hand, avoids the
use of wireless localization, which may be unavailable (and inefficient) to most current devices,
and instead utilizes exchange of visual cues between users to confirm the shared secret (Sluganovic
et al. 2017). Both uses the visual channel as an out-of-band channel.

3.4.3 Remaining Challenges in Sharing and Interactions. The most apparent challenge are the
varying use cases with which users interact or share. Depending on the context and/or situation,
privacy and security concerns, as well as the degree of concern, can vary. For example, feed-through
signalling may be necessary in classroom scenarios to inform teachers when students enter and
leave the classroom. However, there would also be occasions that it could be perceived too invasive
or counter-intuitive, say, during military operations, i.e. negotiations in the field, and the like. Thus,
there is a great deal of subjectivity to determine what is the most effective protection mechanism
during sharing or interactions. And, perhaps, before everything else, we should ask first: “Who or
what are we protecting?”.

3.5 Device Protection
Given the capabilities of these MR devices, various privacy and security risks and concerns have
been raised. Various data protection approaches have also been proposed in the previous subsections.
To complement these approaches, the devices themselves have to be protected as well. There are
two general aspects that needs protection in the device level: device access, and display protection.

3.5.1 Threats to Device Access. The primary threats to device access are identity spoofing and

unauthorized access.

Novel Authentication Strategies. Device access control ensures that authorized users are provided
access while unauthorized ones are barred. Currently, password still remains as the most utilized
method for authentication (Dickinson 2016). To enhance protection, multi-factor authentication
(MFA) is now being adopted, which uses two or more independent methods for authentication. It
usually involves the use of the traditional password method coupled with, say, a dynamic key that
can be sent to the user via SMS, email, or voice call. The two-factor variant has been recommended
as a security enhancement, particularly in on-line services like E-mail, cloud storage, e-commerce,
banking, and social networks.

23

Aside from passwords are pin-based and pattern-based methods that are popular as mobile device
authentication methods. A recent study (George et al. 2017) evaluated the usability and security of
these established pin- and pattern-based authentication methods in virtual interfaces and showed
comparable results in terms of execution time compared to the original non-virtual interface. Now,
we take a look at other novel authentication methods that is leveraged on the existing and potential
capabilities of MR devices.

(1) Gesture- and Active Physiological-based Authentication. We look at the various possible gestures
that can easily be captured by MR devices, specifically finger, hand, and head gestures. Mid-air
finger and hand gestures have been shown to achieve an accuracy between 86-91% (based on
corresponsing accuracy from the equal error rate or EER) using a 3D camera-based motion
controller, i.e. Leap Motion, over a test population of 200 users (Aslan et al. 2014). A combination
of head gestures and blinking gestures triggered by a series of images shown through the AR
HMD have also been evaluated and promises an approximately 94% of balanced accuracy rate
in user identification over a population of 20 users (Rogers et al. 2015). On the other hand,
Headbanger uses head-movements triggered by an auditory cue (i.e. music) and achieved a
True Acceptance Rate (TAR) of 95.7% over a test population of 30 users (Li et al. 2016a). Other
possible gestures or active physiological signals, such as breathing (Chauhan et al. 2017), are
also potential methods.

(2) Passive Physiological-based Authentication. Passive methods include physiological or biometric
signals. Physiological-signal-based key agreement or (PSKA) (Venkatasubramanian et al. 2010)
used PPG features locked in a fuzzy-vault for secure inter-sensor communications for body
area networks or BAN. Despite existing MR devices not having PPG sensing capabilities, the
PSKA method can be utilized for specific use cases when MR devices need to communicate
with other devices in a BAN such as other wearables which can potentially be PPG sensing
capable. On the other hand, SkullConduct (Schneegass et al. 2016) uses the bone conduction
capability of the Google Glass for user identification (with TAR of 97%) and authentication
(EER of 6.9%). All these novel methods show promise on how latent gestures, physiological
signals, and device capabilities can be leveraged for user identification and authentication.
(3) Multi-modal Biometric Authentication combines two or more modes in a singular method instead
of involving other methods or bands of communication is called multi-modal authentication.
One multi-modal method combines facial, iris, and periocular information for user authenti-
cation and has an EER of 0.68% (Raja et al. 2015). GazeTouchPass combines gaze gestures
and touch keys as a singular pass-key for smart phones to counter shoulder-surfing attacks on
touch-based pass keys (Khamis et al. 2016). These types of authentication methods can readily
be applied to MR devices that has gaze tracking and other near-eye sensors.

3.5.2 Threats to Physical Interfaces. As discussed in §3.1.2 and §3.3.3, MR interfaces are
vulnerable from malicious inference which leads to disclosure of input activity, and/or output display
information. Currently available personal AR or MR see-through HMDs project or display content
through lenses. The displayed content on the see-through lenses can leak and be observed externally.
Visual capture devices, say, a camera, can be used to capture and extract information from the
display leakage. External input interfaces suffer from the same inference and side-channel attacks
such as shoulder-surfing.

Protection Approaches. There are optical and visual strategies that can be used to provide interface
and activity confidentiality and unobservability. Figure 10 shows example strategies of optical
blocking and visual cryptography.

24

Fig. 10. Sample interface and display protection strategies: 1) inserting a polarizer to prevent or block display
leakage; and 2) visual cryptography, e.g. using secret augmentations (2.2) through decryption (2.1) of encrypted
public interfaces (2.0). All elements to the left of the optical display element are considered vulnerable to
external inference or capture.

(1) Optical strategies have been proposed, such as the use of polarization on the outer layer (as in
Figure 10 labelled 1), use of narrowband illumination, or a combination of the two to maximize
display transmission while minimizing leakage (Kohno et al. 2016). As of yet, this is the only
work on MR display leakage protection using optical strategies.

There are other capture protection strategies that have been tested on non-MR devices
which allows objects to inherently or actively protect themselves. For example, the TaPS
widgets use optical reflective properties of a scattering foil to only show content at a certain
viewing angle, i.e. 90°(M¨ollers and Borchers 2011).

Active camouflaging techniques have also been used, particularly on mobile phones, which
allows the screen to blend with its surrounding just like a chameleon (Pearson et al. 2017). Both
TaPS widgets and the chameleon-inspired camouflaging are physically hiding sensitive objects
or information from visual capture. The content-hiding methods discussed in §3.3.3 to hide
outputs are also optical strategies.

(2) Visual cryptography and scrambling techniques for display protection have also been discussed
in §3.3.3. The same can also be used for protecting sensitive input interfaces. EyeDecrypt (Forte
et al. 2014) uses visual cryptography technique to protect input/output interfaces, say, ATM PIN
pads, as shown in Figure 10 labelled 2. The publicly viewable input interface is encrypted (step
2.0), and the secret key is kept or known by the user. The user uses an AR device to view the
encrypted public interface and, through the secret key, is visually decrypted (step 2.1). As a
result, only the user can see the actual input interface through the AR display (step 2.2). It
utilizes out-of-band channels to securely transmit the cryptographic keys between two parties
(i.e. the client, through the ATM interface, and the bank). It also provides defence if the viewing
device, i.e. the AR HMD, is untrusted by performing the visual decryption in a secure server
rather than on the AR device itself.

Another AR-based approach secretly scrambles keyboard keys to hide typing activity from
external inference (Maiti et al. 2017). Only the user through the AR device can see the actual key
arrangement of the keyboard. However, these techniques greatly suffer from visual alignment

25

Trusted Mixed Reality Platform13452Mixed Reality Environment←  →Supporting Data ServicesOther Outputs3D Display... Camera + DepthOther SensorsApp 1App 2App 3App N. . .Data1AppData2AppData3AppDataNApp... 2.022.12.21.111.0optical display elementpolarizerissues, i.e. aligning the physical display with the objects rendered through the augmented
display.

3.5.3 Remaining Challenges in Device Protection. Despite the use-cases with visual cryptography
using AR or MR displays, the usability of this technique is still confined to specific sensitive use
cases due to the requirements of alignment. Also, this type of protection is only applicable to
secrets that are pre-determined, specifically, information or activities that are known to be sensitive,
such as password input or ATM PIN input. These techniques are helpful in providing security and
privacy during such activities in shared or public space due to the secrecy provided by the near-eye
displays which can perform the decryption and visual augmentation. Evidently, it only protects
the output or displayed content of external displays but not the actual content which are displayed
through the AR or MR device.

We have presented both defensive and offensive, as well as active and passive, strategies to
device protection. Nonetheless, there are still numerous efforts on improving the input and output
interfaces for these devices and it is opportune to consider in parallel the security and privacy
implications of these new interfaces.

3.6 Summary of Security and Privacy Approaches in Mixed Reality
Finally, we present in Table 2 an over-all comparison of these approaches based on which security
and privacy properties they are addressing and to what extent, which can either be significant,
partial or none.

Generalizations and gaps. Unsurprisingly, all approaches are targeting confidentiality or pre-
venting information disclosure but others are much better at achieving it than most. However, the
approach may only be providing significant protection on their specific space or target entity. For
example, among the input protection approaches, only two provide significant confidentiality:
PrePose only provides gesture events to requiring applications while raw input capture feed is
kept hidden; and Recognizers-approach only provides the necessary recognizer data to requiring
applications while keeping raw RGB and depth information hidden. But they do not provide any
further protection outside their target scope.

Aside from confidentiality, most of the input protection approaches are providing authorization
or access control, undetectability, policy compliance, anonymity, and content awareness. It is to no
surprise as well because most of the approaches are implementing media sanitization (§3.1.1) and/or
access control (§3.1.2) to provide anonymity and undetectability of users inputs, and bystanders.
However, none of the approaches are targeting unlinkability despite the provided anonymity and
undetectability. This is due to the fact that these two properties are only provided through the
access control or abstractions by most approaches. Once applications have been provided access to
the media, resource, or their corresponding abstractions, anonymity and undetectability no longer
hold and, thus, unlinkability as well. Therefore, further protection is necessary once data flows
beyond the input interfaces.

To provide a protection that targets a combination of anonymity, undetectability, and unlinkabil-
ity, most strategies use techniques that ensure statistical privacy guarantees such as differential
privacy and k-anonymity. Among the data protection approaches listed in the summary, only
Rappor and the virtual cloth try-on provides a combined protection on those three properties:
Rappor uses randomised response to provide differential privacy during data collection, while the
privacy-preserving virtual cloth try-on uses secure two-party computation. Other data protection
approaches, specifically, protected data processing (§3.2.2) primarily provides data integrity, unde-
tectability, and plausible deniability through cryptographic approaches, while protected data storage

26

Table 2. Summary of MR approaches that have been discussed, and which security and privacy properties
are addressed by each approach and to what extent. The number of times a property is targeted is counted.

Section

Approach

Input Protection Approaches

Darkly (Jana et al. 2013b)‡
Context-based sanitization (Zarepour et al. 2016)‡
PlaceAvoider (Templeman et al. 2014)†
3D humanoids replace humans (Szczuko 2014)‡
OpenFace/RTFace (Wang et al. 2017)†
Capture-resistant spaces (Truong et al. 2005)◦
See-through vision (Hayashi et al. 2010)‡
World-driven access control (Roesner et al. 2014c)‡
I-pic (Aditya et al. 2016)†
PrivacyCamera (Li et al. 2016b)†
Cardea (Shu et al. 2016)†
MarkIt (Raval et al. 2014, 2016)†
PrePose (Figueiredo et al. 2016)‡
Recognizers (Jana et al. 2013a)‡

Data Protection Approaches

Randomized Response (Erlingsson et al. 2014)◦
SemaDroid(Xu and Zhu 2015)◦
HE-sift (Jiang et al. 2017)◦
Leveled HE-sift(Zhang et al. 2014)◦
CryptoImg (Ziad et al. 2016)◦
SecSIFT (Qin et al. 2014a,b,c)◦
P3 (Ra et al. 2013)◦
Cloth Try-on (Sekhavat 2017)‡
PDV (Mun et al. 2010)◦
OpenPDS (de Montjoye et al. 2014)◦
DataBox(Crabtree et al. 2016)◦

n

u

diatio
Availability
n-Rep

o
N

n

orizatio
Auth

n

n

Authenticatio
Identificatio

dentiality

fi
n
o
C

mity

y
n
o
n
A

kability
nlin
U

detectability
Deniability

n
U

wareness

A

pliance

m
o
C

3

(cid:88)

(cid:88)
(cid:88)

1

(cid:88)

0

12

0

0

14

13

0

(cid:88)(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)

(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88) (cid:88)

5

5

0

0

11

5

2

14

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

8

0

12

14

(cid:88)(cid:88) (cid:88)
(cid:88)(cid:88) (cid:88)
(cid:88)(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88)

7

2

2

(cid:88)(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)(cid:88)

(cid:88)(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88) (cid:88) (cid:88) (cid:88)(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88) (cid:88)
(cid:88)(cid:88) (cid:88)

Integrity

0

8

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)

Output Protection Approaches

1

1

4

4

0

0

8

2

0

6

0

2

2

Arya (Lebeck et al. 2016, 2017)‡
SurroundWeb (Vilk et al. 2015, 2014)‡
EyeGuide (Eaddy et al. 2004)‡
VR Codes (Woo et al. 2012)‡
Psycho-visual modulation (Lin et al. 2017)‡
Visual cryptography using AR (Lantz et al. 2015)‡
Secret messages using AR (Andrabi et al. 2015)‡
Mobile visual cryptography (Fang and Chang 2010)‡

(cid:88) (cid:88) (cid:88) (cid:88)(cid:88)

(cid:88)
(cid:88)(cid:88)

(cid:88)
(cid:88)

(cid:88) (cid:88)(cid:88)

(cid:88)
(cid:88) (cid:88)
(cid:88)(cid:88)
(cid:88)
(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88) (cid:88)

Interaction Protection Approaches

2

4

4

Emmie (Butz et al. 1998, 1999)‡
Kinected Conference(DeVincenzi et al. 2011)‡
SecSpace (Reilly et al. 2014)‡
Candid Interactions (Ens et al. 2015)◦
Pip (Szalav´ari et al. 1998; Szalav´ari and Gervautz 1997)‡
TouchSpace (Cheok et al. 2002)‡
BragFish (Xu et al. 2008)‡
LooksGoodToMe(Gaebel et al. 2016)†
HoloPair (Sluganovic et al. 2017)†

(cid:88) (cid:88)
(cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88)(cid:88) (cid:88)(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88)(cid:88) (cid:88)(cid:88) (cid:88)(cid:88)

(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88)
(cid:88)

6

(cid:88)

2

4

9

1

0

(cid:88)
(cid:88) (cid:88)

(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)

5

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)(cid:88)

0

9

7

(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)(cid:88)
(cid:88)(cid:88) (cid:88)(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88)

Device Protection Approaches

2

0

0

11

8

9

13

0

0

3

0

0

0

Seamless & secure VR (George et al. 2017)‡
Mid-air authentication gestures (Aslan et al. 2014)†
Head and blinking gestures (Rogers et al. 2015)†
HeadBanger (Li et al. 2016a)‡
Pska (Venkatasubramanian et al. 2010)◦
SkullConduct (Schneegass et al. 2016)†
Facial multi-modal authentication (Raja et al. 2015)†
GazeTouchPass (Khamis et al. 2016)†
Polarization (Kohno et al. 2016)‡
Taps Widget (M¨ollers and Borchers 2011)◦
Chameleon-like (Pearson et al. 2017)◦
EyeDecrypt (Forte et al. 2014)‡
Preventing keystroke inference (Maiti et al. 2017)‡

(cid:88)
(cid:88)

(cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)

(cid:88)(cid:88)
(cid:88)
(cid:88)

(cid:88) (cid:88)(cid:88)

(cid:88)

(cid:88)(cid:88)
(cid:88)
(cid:88)

25
The extent of each approach was either (cid:88)(cid:88) significantly addressing, (cid:88) partially addressing, or not (to minimally) addressing the security and privacy
properties. The approaches have been applied to either an ‡ MR context, a † proto-MR context, or a ◦ non-MR context.

Total Count

38

13

13

10

13

55

36

25

21

9

7

2

27

3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.1
3.1.2
3.1.2

3.2.1
3.2.1
3.2.2
3.2.2
3.2.2
3.2.2
3.2.2
3.2.2
3.2.3
3.2.3
3.2.3

3.3.1
3.3.2
3.3.3
3.3.3
3.3.3
3.3.3
3.3.3
3.3.3

3.4.1
3.4.1
3.4.1
3.4.1
3.4.1
3.4.1
3.4.1
3.4.2
3.4.2

3.5.1
3.5.1
3.5.1
3.5.1
3.5.1
3.5.1
3.5.1
3.5.1
3.5.2
3.5.2
3.5.2
3.5.2
3.5.2

Fig. 11. Distribution of categories among the thirteen properties.

(§3.2.3) primarily provides integrity, availability, and anonymity through authorized access to user
data.

Output protection approaches are primarily targeting undetectability of outputs through visual
hiding or cryptography (§3.3.3), while abstraction strategies in output (§3.3.1 and 3.3.2), just like
in input abstraction, provide undetectability through enforced policy compliance and authorized
access to output or rendering resources as well as output data awareness. Output policies ensure
the availability, and reliability of outputs which, then, leads to user safety.

Awareness and policy & consent compliance are two other paramount properties targeted by
the interaction protection approaches. Their primary goal is to deliver fair interactions among
users despite of the ‘boundaries’ that exist in shared spaces (§3.4.1) either through user-mediated
information release or feed-forwarding. Strategies on sharing initialization in MR (§3.4.2) primarily
focused on security properties of non-repudiation, authentication, and identification as well as
integrity, availability, and authorization.

Lastly, device protection approaches primarily target authorization, identification, and confi-
dentiality throughauthentication strategies that capitalizes on MR capabilities. In addition, visual
protection strategies focuses on providing physical undetectability and prevents unauthorized
inference from MR displays and devices.

The categorization, to some extent, has localised the targeted properties by the approaches.
Together, the first three categories roughly target most properties but are slightly privacy leaning
especially for the input protection approaches. On the other hand, the last two categories were
more security leaning especially for the device protection strategies. The concentration of check
marks ((cid:88)(cid:88), (cid:88)) highlights these preferred properties by the categories.

Figure 11 visually shows the distribution of the approaches among the thirteen properties sorted
according to their category. Excluding confidentiality, the most targeted properties are authorization
and undetectability & unobservability. The next most targeted are content awareness and policy &
consent compliance. Consequently, it is evident with this visualized graph that protection approaches
still primarily lack provisions for unlinkability and plausible deniability. It is of no surprise that data
protection approaches (which are mostly generic, non-MR targeted) are the ones that primarily
target these two properties. Thus, there is a great opportunity to explore, investigate, design, and
develop strategies for MR that targets them on top of the evidently targeted properties of existing
approaches.

28

 0 10 20 30 40 50 60IntegrityNon-RepudiationAvailabilityAuthorizationAuthenticationIdentificationConfidentialityAnonymityUnlinkabilityUndetectabilityPlausible DeniabilityContent AwarenessCompliancePaper CountInput ProtectionData ProtectionOutput ProtectionInteraction ProtectionDevice Protection4 OPEN CHALLENGES
The previous section has elaborated on the different security and privacy approaches in the research
literature, and highlighted the remaining challenges for each category. Now, we present the high-
level open challenges brought about by existing and upcoming MR devices, and our potential future
with MR.

Security and Privacy of Existing Devices and Platforms. Most of the security and privacy
work discussed in §3 have only been applied on predecessor or prototype platforms of MR. Now,
we take a broader look on the security and privacy aspects of existing devices and platforms. Most
of the current tool kits, platforms, and devices have not considered the security and privacy risks
that were highlighted in this survey. There was an effort to pinpoint risks and identify long-term
protection for some platforms, specifically Wikitude, and Layar (McPherson et al. 2015) but no
actual nor systematic evaluation was done on both the risks and the proposed protection.

A systematic security and privacy analysis of MR applications, devices, and platforms has to be
performed in order to identify other potential and latent risks. For example, Hololens’ 3D scanning
capability should be investigated whether its resolution can potentially be used to detect heartbeats
or other physiological signals of bystanders which are considered to be sensitive information. Then,
we can evaluate these systems against further security and privacy requirements that address
beyond the known threats and exploits. Upon consideration of these requirements, we can, then,
come up with protection mechanisms targeted for these devices and platforms.

Regulation of Mixed Reality applications. Moreover, the current MR devices and platforms
now provides a peek on what the future devices will offer. Given the capabilities of these devices,
both in sensing and rendering, there are numerous concerns on security and privacy that have been
raised and needs to be addressed. In fact, real vulnerabilities have already been revealed particularly
on the numerous available MR applications For example, Pokemon Go’s network messages between
the application and their servers on its early releases were vulnerable to man-in-the-middle attacks
(Colceriu 2016) which allows hackers to get precise location information of users. In addition to
these were the numerous malicious versions of the application (Burgess 2016; Frink 2016) that has
been downloaded and used by many users. Thus, there is a need for regulation, whether applied
locally on the devices or platforms, or in a larger scale through a much stringent reputation- and
trust-based ecosystem of applications.

Native Support. Furthermore, the entire MR pipeline processing (of detection, transformation,
and rendering as shown in Figure 5) is performed by the applications themselves and in a very
monolithic manner. They would need direct access to sensing interfaces to perform detection,
and to output interfaces for rendering. Once provided permission, AR applications, inevitably,
have indefinite access to these resources and their data. Native, operating system-level support for
detection and rendering can provide an access control framework to ensure security and privacy
of user data. OS-level abstractions have been proposed (D’Antoni et al. 2013) to “expose only the
data required to [third-party] applications”. Apple’s ARkit is one of those moves to provide native
support to MR application development on their platforms, but it’s granularity of access control is
still coarse and similar risks are still present.

Targeted Data Protection. As highlighted in §3.6, there is a lack of MR-targeted data protection
approaches except for one specific use case on virtual cloth try-ons. Due to the highly rich and
dynamic data that is associated to MR, generic data protection approaches may not be readily
applicable and translatable to MR systems. Thus, there is a huge necessity to design and evaluate

29

data protection approaches (e.g. encryption, privacy-preserving algorithms, and personal data
stores) targeted for MR data.

User welfare, society, policy, and ethics. Possible social consequences (Feiner 1999) may arise
from the user’s appearance while wearing the device, and, further, implications on bystander privacy
which have already been brought up several times. Existing (universal, ethical, and legal) concepts
on privacy and policy may need to be revisited to catch up with MR becoming mainstream. As
these technologies will be delivering information overlaid on the physical space, the correctness,
safety, and legality of this information has to be ensured (Roesner et al. 2014a). This is now more
evident with demonstrations such as the Google Duplex, and their Visual Positioning System10.

Other user welfare challenges include the reduction of physical burden from wearing the device;
eye discomfort from occlusion problems, incorrect focus, or output jitter; and the cognitive disconnect
from the loss of tactile feedback when “interacting” with the virtual objects.

Profitability and Interoperability. Ultimately, before MR becomes widely adopted, interop-
erability and profitability are additional aspects that needs to be considered. Manufacturers and
developers has to make sure the profitability of these devices once delivered to the market (Kroeker
2010). Subsequently, ensuring the interoperability of these devices with existing technologies is key
to user adoption of AR and MR devices. There are already demonstrations of how web browsing
can be interoperable between AR and non-AR platforms (Lee et al. 2009). As future and upcoming
MR services will be web-based or web-assisted, this can serve as a basis.

Networking Challenges. Due to the expected increase of MR applications, services, platforms,
and devices, network demand is also expected to increase for these services to be delivered to the
users at an acceptable latency. And it is not only confined to delivery or transmission of MR data,
but also to processing (i.e. data transformation) and rendering. Current data rates that are provided
by existing communication networks may not be ready for such demand (Braud et al. 2017). Thus, it
is an imperative that current and upcoming network design and infrastructure has to be rethought
and remains a challenge.

Smart Future. Overall, the same with all the other technologies that try to push through the
barriers, challenges on processing, data storage, communication, energy management, and battery
still remains at the core. Despite all these, a smart world future is still in sight where smart objects
integrate to form smart spaces that, then, form smart hyperspaces (Ma et al. 2005). This smart
world will be composed of ‘things’ with enough intelligence to be self-aware and manage its own
processing, security, and energy. MR has been shown to enable a cross-reality space (Lifton et al.
2009) – freely crossing from physical to synthetic and vice versa – that can allow these self-aware
devices to understand the real-world while managing all information virtually. Lastly, we reiterate
that existing networks have to be analysed and evaluated if it can support this future and should
be redesigned if necessary.

5 CONCLUSION
This is the first survey to take in the endeavour of collecting, categorizing, and reviewing various
security and privacy approaches in MR. We have raised various known and latent security and
privacy risks associated with the functionalities of MR and gathered a comprehensive collection of
security and privacy approaches on MR and related technologies. We have identified five aspects of
protection namely input, data access, output, interactivity, and device integrity, and categorized the
approaches according to these five aspects. Furthermore, we identify which security and privacy

10Both were demonstrated during Google I/O 2018

30

properties are targeted by the approaches using a list of thirteen known properties. We, then, used
these properties to present a high-level description of the approaches and use it to compare them.
Among the properties, aside from confidentiality, authorization and undetectability were the most
targeted, while there is a lack in the provision of other properties such as unlinkability and plausible
deniability. Therefore, it is opportune to design, investigate, and implement security and privacy
mechanisms that can be integrated with existing and upcoming MR systems, while their utilization
and adoption are still not widespread.

A MIXED REALITY HARDWARE
Table A1 lists some commercially available HMDs. Most of the devices listed are partially see-
through devices such as the Recon Jet11, and the Vuzix M100/M30012 smart glasses. These smart
glasses sprung from the wearable space and are, possibly, the most popularly known and widely
used types of HMDs. They use a small enough video display positioned near one eye (usually on
the right) so as not to entirely block the user’s FOV. This is in comparison to video see-through
HMDS which employ a video camera usually front-facing, as in the HTC Vive13, to capture the
environment, then, feed the video to a display propped in front of the user’s field of view (FOV).
The other type is the optical see-through HMD which uses specialised optics to display objects
within the user’s FOV. The special optics are configured in such a way to allow light to pass through
and let the users see their physical surroundings unlike the displays on the video see-through
types. Epson’s Moverio BT-30014 provides binocular visual augmentations using a prism projection
mechanism and accepts user inputs using a capacitive muti-touch pad tethered to the eye-wear.
ODG’s R715 smart glasses is another optical see-through device but accepts user inputs using
conventional approaches, i.e. mouse and keyboard. In terms of natural user gesture support,
the leading devices are the Meta 2 glasses and Microsoft’s Hololens. The Meta 216 delivers 3D
holographic objects and utilizes environment tracking to allow a virtual 360°interface, and allows
users to drag, pull, and drop objects within the space. However, it is tethered to a dedicated
machine which has to have a dedicated graphics processor. Microsoft’s HoloLens provides a similar
experience to that of the Meta 2 but untethered. In early 2016, Microsoft started shipping out
developer editions, and they expect that developers will create numerous services and applications
around the HoloLens (Goode and Warren 2015). The Hololens17 packs two high-definiton light
engines, holographic lenses, an inertial measurement unit (IMU), 4 environment understanding
cameras, a depth camera, and a dedicated holographics processing unit aside from the regular
hardware (e.g. processor, memory, and battery) that the other devices have.

B MIXED REALITY SOFTWARE
Table A2 lists some popular software tool kits and platforms. Most of these are multi-operating
system and vision-based. Specifically, they are utilizing monocular (single vision) detection for
2D, 3D , and (for some) synchronous localization and mapping (or SLAM). Of these multi-OS,

11Recon Jet, (Accessed 4 Aug 2017), https://www.reconinstruments.com/products/jet
12Products, (Accessed 4 Aug 2017), https://www.vuzix.com/Products
13Buy VIVE Hardware, (Accessed 4 Aug 2017), https://www.vive.com/au/product/
14Moverio BT-300 Smart Glasses
Work/Wearables/Smart-Glasses/Moverio-BT-300-Smart-Glasses-%28AR-Developer-Edition%29-/p/V11H756020
15R-7 Smartglasses System, (Accessed 4 Aug 2017), https://shop.osterhoutgroup.com/products/r-7-glasses-system
16The Meta 2: Made for AR App Development, (Accessed 4 Aug 2017), https://buy.metavision.com/
17HoloLens hardware details,
reality/hololens hardware details/

(AR/Developer Edition),

(Accessed 4 Aug 2017), https://developer.microsoft.com/en-us/windows/mixed-

(Accessed 4 Aug 2017), https://epson.com/For-

31

Table A1. Commercially available MR HMDs

Device

Sensors

Controls or UI

Processor

OS

Outputs

Connectivity

Google Glass
1.0
Explorer
Edition

Google Glass
2.0 Enterprise
Edition

ReconJet Pro

Vuzix M300

Vuzix M100

ODG R-7

Epson Move-
rio BT-300

HTC Vive*

Camera (5MP still/ 720p
video), Gyroscope, ac-
compass,
celerometer,
light
proximity
and
sensor, microphone

Camera (5MP still/ 720p
video), Gyroscope, ac-
compass,
celerometer,
light
proximity
and
sensor, microphone

Camera,
Gyroscope,
accelerometer, compass,
pressure, IR

(13MP
Camera
video),
still/1080p
proximity (inward and
outward),
gyroscope,
accelerometer, compass,
3-DoF
tracking,
dual microphones

head

Camera (5MP still/1080p
proximity (in-
video),
outward),
ward
and
accelerom-
gyroscope,
eter,
3-DoF
compass,
head tracking, light, dual
microphones

Camera (1080p @ 60fps,
720p @ 120fps), 2 ac-
celerometer, 2 gyroscope,
2 magnetometer, altitude,
humidity, light, 2 micro-
phones

Camera (5MP with mo-
tion tracking), magnetic,
acceleromter, gyroscope,
microphone

Camera, gyroscope, ac-
celerometer,
laser posi-
tion sensor, microphones

Touchpad,
mote
via
app
Android)

re-
control
(iOS,

2 buttons, 4-axis
optical touchpad

4 buttons, 2-axis
re-
touchpad,
control
mote
via
(iOS,
app
Android), voice

buttons,

re-
4
control
mote
via
(iOS,
app
Android), voice,
gestures

at least 2 buttons,
trackpad, Blue-
tooth keyboard

handheld

Two
controllers
with
triggers;
external
stations
tracking

trackpad,
two
base
for

Meta 2 Glasses

Camera
hand
tracking, 3 microphones

(720p),

hand gestures

Microsoft
HoloLens

buttons,
and gestures

Voice

4 Environment under-
standing cameras, depth
camera, 2MP camera, 1
MR capture camera,
in-
ertial measurement unit,
light, 4 microphones

Touchpad,
mote
via
app
Android)

re-
control
(iOS,

TI
OMAP
4430 1.2GHz
dual
core
(ARMv7)

Android
4.4

Intel Atom
(32-bit)

Android
4.4

dual-
ARM

1GHz
core
Cortex-A9

ReconOS
4.4
(Android-
based)

Dual-core In-
tel Atom

Android
6.0

(LCoS,
640x360
prism),
Bone
coduction audio
transducer

640x360
prism),
speaker

(LCoS,
Audio

802.11

WiFi
Bluetooth,
croUSB

b/g,
mi-

WiFi 802.11 dual-
a/b/g/n/ac,
band
Bluetooth
LE,
microUSB

16:9 WQVGA,
Audio speaker

802.11

WiFi
Bluetooth,
microUSB

b/g,
GPS,

screen
16:9
to 5
equivalent
in screen at 17 in,
Audio speaker

WiFi 802.11 dual-
a/b/g/n/ac,
band
Bluetooth
4.1/2.1,
microUSB

OMAP
TI
4460 1.2GHz

Android
4.04

16:9 WQVGA
(equivalent
to
4 in at 14 in),
Audio speaker

WiFi 802.11 b/g/n,
Bluetooth,
mi-
croUSB

trackpad
ered)

(teth-

Intel Atom
5,
1.44GHz
quad-core

Android
5.1

Dual 720p 16:9
stereoscopic
dis-
see-thru
plays,
Stereo
Audio ports with
earbuds

Binocular
1280x720
parent
Earphones

trans-
display,

Binocular
1080x1200 (110°
FoV), Audio jack

WiFi 802.11ac, Blue-
tooth 4.1, GNSS
(GPS/GLONASS),
microUSB

WiFi 802.11 dual-
a/b/g/n/ac,
band
Bluetooth,
GPS,
microUSB

HDMI or Display
Port and USB 3.0

(90°
2550x1440
FoV), 4 surround
sound speakers

HDMI

WiFi 802.11ac, Blue-
tooth 4.1 LE, mi-
croUSB

16:9 see-through
holographic
lenses (30° FoV),
Audio (surround)
speakers, audio
jack

ReticleOS
(Android-
based)

Qualcomm
Snapdrag-
onTM 805
2.7GHz
quad-core

Windows
for
host
computer
(SteamVR
3D
en-
gine)

Windows
for
host
computer
(Unity
SDK 3D
engine)

Windows
10

host

(directly
via
computer,
GPU recom-
mended)

host

(directly
via
computer,
GPU recom-
mended)

32-
and a

Intel
bit,
dedicated
holographic
processing
unit (HPU)

32

Table A2. Software Platforms, Toolkits, and Application Programming Interfaces

Software Tool Kit

Detection or Tracking

Platform

ARToolKit

2D objects

Android, iOS, Linux, ma-
cOS, Unity3D

Development
Language
C, C++, C#,
Java

Licensing

Current Version

Free
source)

(open

ARToolKit6
(2017)

Layar v7.0

2D objects

Android, iOS, Blackberry HTTP (REST-

Paid

3D objects, marker-
based visual SLAM

3D objects, marker-
based visual SLAM

ful), JSON

Android, iOS, Windows
(selected devices), Unity
3D

C#,

C++,
Objective-C,
Java

Android, iOS

Javascript

3D objects, visual SLAM Android,

iOS, macOS,

Windows, Unity3D

C++,

C,
Objective-
C, Java

Free (with paid
versions)

Vuforia 6.5

Paid (with 30-
day free trial)

Zapworks Studio
(2017)

Free (with paid
version)

EasyAR SDK 2.0

kudan

3D object, visual SLAM

Android, iOS, Unity 3D

3D objects, visual SLAM Android,

iOS,

Smart
Glasses (Epson Moverio,
ODG, Vuzix), Unity3D

3D objects, visual and
depth SLAM

iOS

Objective-C,
Java

Free (with paid
versions)

kudan v1.0

Objective-
C,
Javascript

Java,

Paid

wikitude SDK 7.0

Objective-C

Free

ARKit (2017)

Layar

Vuforia

Zapbox

EasyAR

Wikitude

ARkit

OSVR

2D objects, orientation-
based SLAM

Vuzix, OSVR, HTC Vive,
Oculus, SteamVR

C, C++, C#,
JSON
for
plug-ins)

Free
source)

(open

Hacker
OSVR
Development Kit
(HDK) 2.0

Meta 2 SDKs

Windows Mixed Re-
ality

3D objects, visual and
depth SLAM

3D objects; visual, depth,
and
orientation-based
SLAM

Meta 2 glasses

Windows MR
HoloLens, Unity

and

C#,
Javascript

C#,
Javascript

C++,

Free

Meta 2 SDKs

C++,

Free

Windows MR API

vision-based tool kits, only the ARToolkit18 is free and open source. Most of the paid tool kits,
on the other hand, have cloud support and are primarily focused on content and service creation,
and application development. Furthermore, most of these tool kits such as Wikitude19, Layar20,
and Zapbox21 are also shipped as platform applications that can be used to run other applications
developed using the same tool kit.

There are also platform- and device-dependent tool kits listed. The ARkit22 is specifically for
Apple’s devices. The OSVR (Open Source Virtual Reality)23 is primarily for VR applications of
specific smart glass devices, e.g HTC Vive and some Vuzix glasses, but can also be used for AR
functionalities if devices have environment sensing capabilities such as the front-facing cameras
of the Vive. The Meta 2 SDKs24 are obviously for the Meta 2 glasses. Similarly, the Windows MR
APIs25 are for the Windows MR devices and the Hololens. These tool kits are primarily designed
for the functionalities of these devices which includes other environmental sensing capabilities, e.g.
depth sensing and head tracking, aside from the regular camera-based sensing.

18ARToolKit6, (Accessed August 4, 2017), https://artoolkit.org/
19wikitude, (Accessed August 4, 2017), https://www.wikitude.com/
20Layar, (Accessed August 4, 2017), https://www.layar.com/
21Zapbox: Mixed Reality for Everyone, (Accessed August 4, 2017), https://www.zappar.com/zapbox/
22ARKit - Apple Developer, (Accessed August 4, 2017), https://developer.apple.com/arkit/
23What is OSVR?, (Accessed August 4, 2017), http://www.osvr.org/what-is-osvr.html
24Meta 2 SDK Features, (Accessed August 4, 2017), https://www.metavision.com/develop/sdkfeatures
25Windows Mixed Reality, (Accessed August 4, 2017), https://developer.microsoft.com/en-us/windows/mixed-reality

33

C SUMMARIZING SECURITY AND PRIVACY STRATEGIES
The previous discussions on the different security and privacy approaches were arranged based on
the categorization presented in §2.2. Now, we present an over-all comparison of these approaches
based on which security and privacy controls have been used by these approaches. We also highlight
the focus of the different approaches discussed and, consequently, reveal some shortcomings, gaps,
or potential areas of further study or investigation.

For Table A3, we use the US National Institute of Standard and Technology’s (NIST) listed
security and privacy controls (NIST 2013) as basis to formulate eleven generic control families
which we use to generalize and compare the approaches from a high-level perspective. The 11
security and privacy controls are as follows: resource access control and policy; device authentication
and identification; media protection policy; media access; media sanitization; physical access control,
and access to display devices; information in shared resources and information sharing; transmission
confidentiality and integrity; cryptographic protection, key establishment and management, and/or
PKI certificates; distributed processing and storage; and out-of-band channels. We add an additional
control family that utilises probabilistic strategies which makes the total number of families to
twelve. We manually identify the controls used by the approaches and to what extent it has been
applied (indicated by the shade on the boxes).

Most of the input protection approaches are fully implementing media sanitization as a form of
control, as well as media protection and access policies, to some extent. Two of them were using
resource access control policies to limit or provide abstraction on how applications have access
to the input resources, i.e. sensing interfaces, and they further implemented least privilege access
control.

Most of the data protection approaches discussed are non-MR, and only one of them (including
the approaches in other categories) utilises probabilistic controls to provide protection. In addition,
only one work as well has actually been applied in an MR context. Therefore, there is big opportunity
to look into MR-targeted data protection approaches in all three major data aspects – collection,
processing, and storage – and possibly capitalizing on probabilistic strategies.

The first three output protection approaches were partially providing protection of outputs in
shared resources, and physical access control to the output interfaces. Similarly, the next three
approaches were providing protection in shared resources but in a full extent, as well as transmission
confidentiality and cryptographic protection. The two remaining output approaches were more
focused on the access the applications have to the output resources, i.e. displays.

The protection approaches for user interactions are providing protection, primarily, in an actual
sharing activity in a shared space. Thus, these interaction protection approaches are addressing both
aspects of information sharing and information in shared resources. Furthermore, most interaction
protection approaches are also using distributed processing controls.

Among all the approaches, only the device protection ones are using device authentication
controls, while the last five device protection approaches were more focused on the physical aspect
of device access.

REFERENCES
Alessandro Acquisti. 2011. Privacy in the age of augmented reality. (2011).
Alessandro Acquisti, Curtis R Taylor, and Liad Wagman. 2016. The economics of privacy. (2016).
Paarijaat Aditya, Rijurekha Sen, Peter Druschel, Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele, Bobby
Bhattacharjee, and Tong Tong Wu. 2016. I-pic: A platform for privacy-compliant image capture. In Proceedings of the
14th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 235–248.

Sarah J Andrabi, Michael K Reiter, and Cynthia Sturton. 2015. Usability of Augmented Reality for Revealing Secret Messages

to Users but Not Their Devices.. In SOUPS. 89–102.

34

Table A3. Summary of MR approaches that have been discussed, and which security and privacy controls
are applied by each approach and to what extent.

Identification
Protection
Authenticaiton
Control
Resource
Device
Media
Access
Policy

&

Sharing,
Sanitization
Resources
Cryptographic
Confidentiality
Transmission
Control
Control
Information
Physical
Media
Media
Shared
Access
Access

Distributed
Processing

Out-of-band
Channel

Probabilistic

Approach

Darkly (Jana et al. 2013b)‡
Context-based sanitization (Zarepour et al. 2016)‡
PlaceAvoider (Templeman et al. 2014)†
3D humanoids replace humans (Szczuko 2014)◦
OpenFace/RTFace (Wang et al. 2017)†
Capture-resisitant spaces (Truong et al. 2005)◦
See-through vision (Hayashi et al. 2010)‡
World-driven (Roesner et al. 2014c)‡
I-pic (Aditya et al. 2016)†
PrivacyCamera (Li et al. 2016b)†
Cardea (Shu et al. 2016)†
MarkIt (Raval et al. 2014, 2016)†
PrePose (Figueiredo et al. 2016)‡
Recognizers (Jana et al. 2013a)‡

Rappor (Erlingsson et al. 2014)◦
SemaDroid (Xu and Zhu 2015)◦
HE-sift (Jiang et al. 2017)◦
Leveled HE-sift(Zhang et al. 2014)◦
CryptoImg (Ziad et al. 2016)◦
SecSift (Qin et al. 2014a,b,c)◦
P3 (Ra et al. 2013)◦
Cloth try-on (Sekhavat 2017)‡
PDV (Mun et al. 2010)◦
OpenPDS (de Montjoye et al. 2014)◦
DataBox (Crabtree et al. 2016)◦

Arya (Lebeck et al. 2016, 2017)‡
SurroundWeb (Vilk et al. 2015, 2014)‡
EyeGuide (Eaddy et al. 2004)‡
VR Codes (Woo et al. 2012)‡
Psycho-visual modulation (Lin et al. 2017)‡
Visual cryptography using AR (Lantz et al. 2015)‡
Secret messages using AR (Andrabi et al. 2015)‡
Mobile visual cryptography (Fang and Chang 2010)‡

Input Protection Approaches

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)*
(cid:4)*

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:4)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:4)

(cid:1)*
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:1)

Data Protection Approach/es

(cid:3)
(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:1)
(cid:1)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:4)
(cid:4)

Output Protection Approaches

(cid:4)
(cid:1)*
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

Interaction Protection Approaches

Emmie (Butz et al. 1998, 1999)‡
Kinected Conferences (DeVincenzi et al. 2011)‡
SecSpace (Reilly et al. 2014)‡
Candid Interactions (Ens et al. 2015)◦
Pip (Szalav´ari et al. 1998; Szalav´ari and Gervautz
1997)‡
TouchSpace (Cheok et al. 2002)‡
BragFish (Xu et al. 2008)‡
LooksGoodTome (Gaebel et al. 2016)†
HoloPair (Sluganovic et al. 2017)†

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

Seamless and secure VR (George et al. 2017)‡
Mid-air authentication gestures (Aslan et al. 2014)†
Head and blinking gestures (Rogers et al. 2015)†
HeadBanger (Li et al. 2016a)‡
PSKA (Venkatasubramanian et al. 2010)◦
SkullConduct (Schneegass et al. 2016)†
Facial multi-modal authentication (Raja et al. 2015)†
GazeTouchPass (Khamis et al. 2016)†
Polarization (Kohno et al. 2016)‡
Taps Widget (M¨ollers and Borchers 2011)◦
Chameleon-like (Pearson et al. 2017)◦
EyeDecrypt (Forte et al. 2014)‡
Preventing keystroke inference (Maiti et al. 2017)‡

Device Protection Approaches

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:1)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:3)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:3)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:4)
(cid:1)
(cid:1)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:4)
(cid:4)
(cid:4)
(cid:1)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:1)
(cid:4)
(cid:4)
(cid:4)

(cid:4)
(cid:4)
(cid:4)
(cid:1)
(cid:1)

(cid:1)
(cid:1)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:1)
(cid:1)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:4)
(cid:1)
(cid:4)
(cid:4)
(cid:1)
(cid:4)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:1)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:1)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:1)

(cid:1)
(cid:1)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:3)

(cid:3)
(cid:3)
(cid:4)
(cid:4)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:1)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:4)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

The extent of each control was either (cid:4) fully applied, or (cid:1) partially applied, while the blank (cid:3) indicates not applied, not applicable, or not mentioned in
the corresponding approach. The controls have been applied to either an ‡ MR context, a † proto-MR context, or a ◦ non-MR context.

35

Ilhan Aslan, Andreas Uhl, Alexander Meschtscherjakov, and Manfred Tscheligi. 2014. Mid-air authentication gestures: an
exploration of authentication based on palm and finger motions. In Proceedings of the 16th International Conference on
Multimodal Interaction. ACM, 311–318.

Ronald Azuma, Yohan Baillot, Reinhold Behringer, Steven Feiner, Simon Julier, and Blair MacIntyre. 2001. Recent advances

in augmented reality. IEEE computer graphics and applications 21, 6 (2001), 34–47.

Ronald T Azuma. 1997. A survey of augmented reality. Presence: Teleoperators and virtual environments 6, 4 (1997), 355–385.
Steve Benford, Chris Brown, Gail Reynard, and Chris Greenhalgh. 1996. Shared spaces: transportation, artificiality, and

spatiality. In Proceedings of the 1996 ACM conference on Computer supported cooperative work. ACM, 77–86.

Steve Benford, Chris Greenhalgh, Gail Reynard, Chris Brown, and Boriana Koleva. 1998. Understanding and constructing
shared spaces with mixed-reality boundaries. ACM Transactions on computer-human interaction (TOCHI) 5, 3 (1998),
185–223.

Mark Billinghurst and Hirokazu Kato. 1999. Collaborative mixed reality. In Proceedings of the First International Symposium

on Mixed Reality. 261–284.

T. Braud, F. H. Bijarbooneh, D. Chatzopoulos, and P. Hui. 2017. Future Networking Challenges: The Case of Mobile

Augmented Reality. (2017), 1796-1807 pages. https://doi.org/10.1109/ICDCS.2017.48

Matt Burgess. 2016. Malicious Pok´emon Go app is putting Android phones at risk. (2016). http://www.wired.co.uk/article/

pokemon-go-malicious-android-app-problems

Andreas Butz, Clifford Beshers, and Steven Feiner. 1998. Of vampire mirrors and privacy lamps: Privacy management in
multi-user augmented environments. In Proceedings of the 11th annual ACM symposium on User interface software and
technology. ACM, 171–172.

Andreas Butz, Tobias H¨ollerer, Steven Feiner, Blair MacIntyre, and Clifford Beshers. 1999. Enveloping Users and Computers
in a Collaborative 3D Augmented Reality. In Proceedings of the 2nd IEEE and ACM International Workshop on Augmented
Reality (IWAR ’99). IEEE Computer Society, Washington, DC, USA, 35–.

Joy Jo-Yi Chang, Ming-Jheng Li, Yi-Chun Wang, and Justie Su-Tzu Juan. 2010. Two-image encryption by random grids. In

Communications and Information Technologies (ISCIT), 2010 International Symposium on. IEEE, 458–463.

Dimitris Chatzopoulos, Carlos Bermejo, Zhanpeng Huang, and Pan Hui. 2017. Mobile Augmented Reality Survey: From

Where We Are to Where We Go. IEEE Access (2017).

Jagmohan Chauhan, Yining Hu, Suranga Seneviratne, Archan Misra, Aruna Seneviratne, and Youngki Lee. 2017. BreathPrint:
Breathing Acoustics-based User Authentication. In Proceedings of the 15th Annual International Conference on Mobile
Systems, Applications, and Services. ACM, 278–291.

Adrian David Cheok, Xubo Yang, Zhou Zhi Ying, Mark Billinghurst, and Hirokazu Kato. 2002. Touch-space: Mixed reality
game space based on ubiquitous, tangible, and social computing. Personal and ubiquitous computing 6, 5-6 (2002),
430–442.

Alina Colceriu. 2016. Catching Pokemon GO in Your Network.

(2016). https://www.ixiacom.com/company/blog/

catching-pokemon-go-your-network

Andy Crabtree, Tom Lodge, James Colley, Chris Greenhalgh, Richard Mortier, and Hamed Haddadi. 2016. Enabling the new
economic actor: data protection, the digital economy, and the Databox. Personal and Ubiquitous Computing 20, 6 (2016),
947–957.

Keith Curtin. 2017. Mixed Reality will be most important tech of 2017. (Jan. 2017). https://thenextweb.com/insider/2017/01/

07/mixed-reality-will-be-most-important-tech-of-2017

Loris D’Antoni, Alan M Dunn, Suman Jana, Tadayoshi Kohno, Benjamin Livshits, David Molnar, Alexander Moshchuk, Eyal
Ofek, Franziska Roesner, T Scott Saponas, et al. 2013. Operating System Support for Augmented Reality Applications.. In
HotOS, Vol. 13. 21–21.

Yves-Alexandre de Montjoye, Erez Shmueli, Samuel S Wang, and Alex Sandy Pentland. 2014. openpds: Protecting the

privacy of metadata through safeanswers. PloS one 9, 7 (2014), e98790.

Mina Deng, Kim Wuyts, Riccardo Scandariato, Bart Preneel, and Wouter Joosen. 2011. A privacy threat analysis framework:

supporting the elicitation and fulfillment of privacy requirements. Requirements Engineering 16, 1 (2011), 3–32.

Anthony DeVincenzi, Lining Yao, Hiroshi Ishii, and Ramesh Raskar. 2011. Kinected conference: augmenting video imaging
with calibrated depth and audio. In Proceedings of the ACM 2011 conference on Computer supported cooperative work. ACM,
621–624.

Ben Dickinson. 2016. 5 authentication methods putting passwords to shame. (March 2016). https://thenextweb.com/insider/

2016/03/31/5-technologies-will-flip-world-authentication-head

Klaus Dorfmuller-Ulhaas and Dieter Schmalstieg. 2001. Finger tracking for interaction in augmented environments. In

Augmented Reality, 2001. Proceedings. IEEE and ACM International Symposium on. IEEE, 55–64.

Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. Foundations and Trends® in

Theoretical Computer Science 9, 3–4 (2014), 211–407.

36

Marc Eaddy, Gabor Blasko, Jason Babcock, and Steven Feiner. 2004. My own private kiosk: Privacy-preserving public

displays. In Wearable Computers, 2004. ISWC 2004. Eighth International Symposium on, Vol. 1. IEEE, 132–135.

Barrett Ens, Tovi Grossman, Fraser Anderson, Justin Matejka, and George Fitzmaurice. 2015. Candid interaction: Revealing
hidden mobile and wearable computing activities. In Proceedings of the 28th Annual ACM Symposium on User Interface
Software & Technology. ACM, 467–476.

´Ulfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Randomized aggregatable privacy-preserving ordinal
response. In Proceedings of the 2014 ACM SIGSAC conference on computer and communications security. ACM, 1054–1067.
Chengfang Fang and Ee-Chien Chang. 2010. Securing interactive sessions using mobile device through visual channel and

visual inspection. In Proceedings of the 26th Annual Computer Security Applications Conference. ACM, 69–78.

Steven K Feiner. 1999. The importance of being mobile: some social consequences of wearable augmented reality systems.

In Augmented Reality, 1999.(IWAR’99) Proceedings. 2nd IEEE and ACM International Workshop on. IEEE, 145–148.

Adrienne Porter Felt, Elizabeth Ha, Serge Egelman, Ariel Haney, Erika Chin, and David Wagner. 2012. Android permissions:
User attention, comprehension, and behavior. In Proceedings of the eighth symposium on usable privacy and security.
ACM, 3.

Lucas Silva Figueiredo, Benjamin Livshits, David Molnar, and Margus Veanes. 2016. Prepose: Privacy, Security, and Reliability

for Gesture-Based Programming. In Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 122–137.

Andrea G Forte, Juan A Garay, Trevor Jim, and Yevgeniy Vahlis. 2014. EyeDecrypt – Private interactions in plain sight. In

International Conference on Security and Cryptography for Networks. Springer, 255–276.

Batya Friedman and Peter H Kahn Jr. 2000. New directions: A Value-Sensitive Design approach to augmented reality. In

Proceedings of DARE 2000 on designing augmented reality environments. ACM, 163–164.

Lyle Frink. 2016.

UPDATE: Augmented Malware with Pok´emon Go.

(2016).

https://blog.avira.com/

augmented-malware-pokemon-go/

Ethan Gaebel, Ning Zhang, Wenjing Lou, and Y Thomas Hou. 2016. Looks Good To Me: Authentication for Augmented

Reality. In Proceedings of the 6th International Workshop on Trustworthy Embedded Devices. ACM, 57–67.

Craig Gentry. 2009. A fully homomorphic encryption scheme. Stanford University.
Ceenu George, Mohamed Khamis, Emanuel von Zezschwitz, Marinus Burger, Henri Schmidt, Florian Alt, and Heinrich
Hussmann. 2017. Seamless and Secure VR: Adapting and Evaluating Established Authentication Systems for Virtual
Reality.

Lauren Goode and Tom Warren. 2015. This is what Microsoft HoloLens is really like. (2015). https://www.theverge.com/

2016/4/1/11334488/microsoft-hololens-video-augmented-reality-ar-headset-hands-on

Rapha¨el Grasset and Jean-Dominique Gascuel. 2002. MARE: Multiuser Augmented Reality Environment on Table Setup.
In ACM SIGGRAPH 2002 Conference Abstracts and Applications (SIGGRAPH ’02). ACM, New York, NY, USA, 213–213.
https://doi.org/10.1145/1242073.1242226

Masayuki Hayashi, Ryo Yoshida, Itaru Kitahara, Yoshinari Kameda, and Yuichi Ohta. 2010. An installation of privacy-safe

see-through vision. Procedia-Social and Behavioral Sciences 2, 1 (2010), 125–128.

Wenbo He, Xue Liu, Hoang Nguyen, Klara Nahrstedt, and Tarek Abdelzaher. 2007. Pda: Privacy-preserving data aggregation
in wireless sensor networks. In INFOCOM 2007. 26th IEEE International Conference on Computer Communications. IEEE.
IEEE, 2045–2053.

Wenbo He, Xue Liu, Hoang Viet Nguyen, Klara Nahrstedt, and Tarek Abdelzaher. 2011. PDA: privacy-preserving data

aggregation for information collection. ACM Transactions on Sensor Networks (TOSN) 8, 1 (2011), 6.

Olli I Heimo, Kai K Kimppa, Seppo Helle, Timo Korkalainen, and Teijo Lehtonen. 2014. Augmented reality-Towards an
ethical fantasy?. In Ethics in Science, Technology and Engineering, 2014 IEEE International Symposium on. IEEE, 1–7.
A. Henrysson, M. Billinghurst, and M. Ollila. 2005. Face to face collaborative AR on mobile phones. In Fourth IEEE and ACM
International Symposium on Mixed and Augmented Reality (ISMAR’05). 80–89. https://doi.org/10.1109/ISMAR.2005.32

Michael Howard and Steve Lipner. 2006. The security development lifecycle. Vol. 8. Microsoft Press Redmond.
Chao-Yung Hsu, Chun-Shien Lu, and Soo-Chang Pei. 2011. Homomorphic encryption-based secure SIFT for privacy-
preserving feature extraction. In Media Watermarking, Security, and Forensics III, Vol. 7880. International Society for
Optics and Photonics, 788005.

Hong Hua, Leonard D Brown, and Chunyu Gao. 2004. SCAPE: supporting stereoscopic collaboration in augmented and

projective environments. IEEE Computer Graphics and Applications 24, 1 (2004), 66–75.

Yan Huang, David Evans, Jonathan Katz, and Lior Malka. 2011. Faster Secure Two-Party Computation Using Garbled

Circuits.. In USENIX Security Symposium, Vol. 201.

Suman Jana, David Molnar, Alexander Moshchuk, Alan M Dunn, Benjamin Livshits, Helen J Wang, and Eyal Ofek. 2013a.

Enabling Fine-Grained Permissions for Augmented Reality Applications with Recognizers.. In USENIX Security.

Suman Jana, Arvind Narayanan, and Vitaly Shmatikov. 2013b. A Scanner Darkly: Protecting user privacy from perceptual

applications. In Security and Privacy (SP), 2013 IEEE Symposium on. IEEE, 349–363.

37

Linzhi Jiang, Chunxiang Xu, Xiaofang Wang, Bo Luo, and Huaqun Wang. 2017. Secure outsourcing SIFT: Efficient and
Privacy-preserving Image Feature Extraction in the Encrypted Domain. IEEE Transactions on Dependable and Secure
Computing (2017).

Christos Kalloniatis, Evangelia Kavakli, and Stefanos Gritzalis. 2008. Addressing privacy requirements in system design: the

PriS method. Requirements Engineering 13, 3 (2008), 241–255.

Mohamed Khamis, Florian Alt, Mariam Hassib, Emanuel von Zezschwitz, Regina Hasholzner, and Andreas Bulling. 2016.
Gazetouchpass: Multimodal authentication using gaze and touch on mobile devices. In Proceedings of the 2016 CHI
Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 2156–2164.

Tadayoshi Kohno, Joel Kollin, David Molnar, and Franziska Roesner. 2016. Display Leakage and Transparent Wearable

Displays: Investigation of Risk, Root Causes, and Defenses. Technical Report.

Bernard Kress and Thad Starner. 2013. A review of head-mounted displays (HMD) technologies and applications for

consumer electronics. In Proc. SPIE, Vol. 8720. 87200A.

Kirk L Kroeker. 2010. Mainstreaming augmented reality. Commun. ACM 53, 7 (2010), 19–21.
Patrik Lantz, Bjorn Johansson, Martin Hell, and Ben Smeets. 2015. Visual Cryptography and Obfuscation: A Use-Case
for Decrypting and Deobfuscating Information Using Augmented Reality. In International Conference on Financial
Cryptography and Data Security. Springer, 261–273.

Kiron Lebeck, Tadayoshi Kohno, and Franziska Roesner. 2016. How to safely augment reality: Challenges and Directions. In

Proceedings of the 17th International Workshop on Mobile Computing Systems and Applications. ACM, 45–50.

Kiron Lebeck, Kimberly Ruth, Tadayoshi Kohno, and Franziska Roesner. 2017. Securing Augmented Reality Output. In

Security and Privacy (SP), 2017 IEEE Symposium on. IEEE, 320–337.

Linda Lee, Serge Egelman, Joong Hwa Lee, and David Wagner. 2015. Risk Perceptions for Wearable Devices. (2015).

arXiv:1504.05694

Ryong Lee, Daisuke Kitayama, Yong-Jin Kwon, and Kazutoshi Sumiya. 2009. Interoperable augmented web browsing for
exploring virtual media in real space. In Proceedings of the 2nd International Workshop on Location and the Web. ACM, 7.
Ang Li, Qinghua Li, and Wei Gao. 2016b. PrivacyCamera: Cooperative Privacy-Aware Photographing with Mobile Phones.
In Sensing, Communication, and Networking (SECON), 2016 13th Annual IEEE International Conference on. IEEE, 1–9.
Sugang Li, Ashwin Ashok, Yanyong Zhang, Chenren Xu, Janne Lindqvist, and Macro Gruteser. 2016a. Whose move is it
anyway? Authenticating smart wearable devices using unique head movement patterns. In Pervasive Computing and
Communications (PerCom), 2016 IEEE International Conference on. IEEE, 1–9.

Joshua Lifton, Mathew Laibowitz, Drew Harry, Nan-Wei Gong, Manas Mittal, and Joseph A Paradiso. 2009. Metaphor and

manifestation cross-reality with ubiquitous sensor/actuator networks. IEEE Pervasive Computing 8, 3 (2009).

Pei-Yu Lin, Bin You, and Xiaoyong Lu. 2017. Video exhibition with adjustable augmented reality system based on temporal

psycho-visual modulation. EURASIP Journal on Image and Video Processing 2017, 1 (2017), 7.

David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision 60,

2 (2004), 91–110.

Jianhua Ma, Laurence Tianruo Yang, Bernady O Apduhan, Runhe Huang, Leonard Barolli, Makoto Takizawa, and Timothy K
Shih. 2005. A walkthrough from smart spaces to smart hyperspaces towards a smart world with ubiquitous intelligence.
In Parallel and Distributed Systems, 2005. Proceedings. 11th International Conference on, Vol. 1. IEEE, 370–376.

Anindya Maiti, Oscar Armbruster, Murtuza Jadliwala, and Jibo He. 2016. Smartwatch-based keystroke inference attacks and
context-aware protection mechanisms. In Proceedings of the 11th ACM on Asia Conference on Computer and Communica-
tions Security. ACM, 795–806.

Anindya Maiti, Murtuza Jadliwala, and Chase Weber. 2017. Preventing shoulder surfing using randomized augmented
reality keyboards. In Pervasive Computing and Communications Workshops (PerCom Workshops), 2017 IEEE International
Conference on. IEEE, 630–635.

Richard McPherson, Suman Jana, and Vitaly Shmatikov. 2015. No Escape From Reality: Security and Privacy of Augmented
Reality Browsers. In Proceedings of the 24th International Conference on World Wide Web (WWW ’15). International
World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 743–753. https:
//doi.org/10.1145/2736277.2741657

Frank McSherry and Kunal Talwar. 2007. Mechanism design via differential privacy. In Foundations of Computer Science,

2007. FOCS’07. 48th Annual IEEE Symposium on. IEEE, 94–103.

Paul Milgram and Fumio Kishino. 1994. A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information

and Systems 77, 12 (1994), 1321–1329.

Paul Milgram, Haruo Takemura, Akira Utsumi, Fumio Kishino, et al. 1994. Augmented reality: A class of displays on the

reality-virtuality continuum. In Telemanipulator and telepresence technologies, Vol. 2351. 282–292.

Prashanth Mohan, Abhradeep Thakurta, Elaine Shi, Dawn Song, and David Culler. 2012. GUPT: privacy preserving data
analysis made easy. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data. ACM,
349–360.

38

Max M¨ollers and Jan Borchers. 2011. TaPS widgets: interacting with tangible private spaces. In Proceedings of the ACM

International Conference on Interactive Tabletops and Surfaces. ACM, 75–78.

Alessandro Mulloni, Daniel Wagner, and Dieter Schmalstieg. 2008. Mobility and social interaction as core gameplay
elements in multi-player augmented reality. In Proceedings of the 3rd international conference on Digital Interactive Media
in Entertainment and Arts. ACM, 472–478.

Min Mun, Shuai Hao, Nilesh Mishra, Katie Shilton, Jeff Burke, Deborah Estrin, Mark Hansen, and Ramesh Govindan. 2010.
Personal data vaults: a locus of control for personal data streams. In Proceedings of the 6th International COnference.
ACM, 17.

Joint Task Force Transformation Initiative NIST. 2013. Security and Privacy Controls for Federal Information Systems and

Organizations. Technical Report.

Pascal Paillier. 1999. Public-key cryptosystems based on composite degree residuosity classes. In International Conference on

the Theory and Applications of Cryptographic Techniques. Springer, 223–238.

Jennifer Pearson, Simon Robinson, Matt Jones, Anirudha Joshi, Shashank Ahire, Deepak Sahoo, and Sriram Subramanian.
2017. Chameleon devices: investigating more secure and discreet mobile interactions via active camouflaging. In
Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 5184–5196.

Zhan Qin, Jingbo Yan, and Kui Ren. 2014a. Private image computation: the case of cloud based privacy-preserving SIFT. In

Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 179–180.

Zhan Qin, Jingbo Yan, Kui Ren, Chang Wen Chen, and Cong Wang. 2014b. Towards efficient privacy-preserving image
feature extraction in cloud computing. In Proceedings of the 22nd ACM international conference on Multimedia. ACM,
497–506.

Zhan Qin, Jingbo Yan, Kui Ren, Chang Wen Chen, Cong Wang, and Xinwen Fu. 2014c. Privacy-preserving outsourcing of

image global feature detection. In Global Communications Conference (GLOBECOM), 2014 IEEE. IEEE, 710–715.

Moo-Ryong Ra, Ramesh Govindan, and Antonio Ortega. 2013. P3: Toward Privacy-Preserving Photo Sharing.. In Nsdi.

515–528.

Ihsan Rabbi and Sehat Ullah. 2013. A survey on augmented reality challenges and tracking. Acta Graphica znanstveni

ˇcasopis za tiskarstvo i grafiˇcke komunikacije 24, 1-2 (2013), 29–46.

Kiran B Raja, Ramachandra Raghavendra, Martin Stokkenes, and Christoph Busch. 2015. Multi-modal authentication system
for smartphones using face, iris and periocular. In Biometrics (ICB), 2015 International Conference on. IEEE, 143–150.
Nisarg Raval, Animesh Srivastava, Kiron Lebeck, Landon Cox, and Ashwin Machanavajjhala. 2014. Markit: Privacy markers
for protecting visual secrets. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous
Computing: Adjunct Publication. ACM, 1289–1295.

Nisarg Raval, Animesh Srivastava, Ali Razeen, Kiron Lebeck, Ashwin Machanavajjhala, and Lanodn P Cox. 2016. What you
mark is what apps see. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and
Services. ACM, 249–261.

H.T. Regenbrecht, M. Wagner, and G. Baratoff. 2002. MagicMeeting: A Collaborative Tangible Augmented Reality System.

Virtual Reality 6, 3 (01 Oct 2002), 151–166.

Derek Reilly, Mohamad Salimian, Bonnie MacKay, Niels Mathiasen, W Keith Edwards, and Juliano Franz. 2014. SecSpace:
prototyping usable privacy and security for mixed reality collaborative environments. In Proceedings of the 2014 ACM
SIGCHI symposium on Engineering interactive computing systems. ACM, 273–282.

Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud Legout, and David Choffnes. 2016. Recon: Revealing and controlling pii
leaks in mobile network traffic. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications,
and Services. ACM, 361–374.

Franziska Roesner, Tamara Denning, Bryce Clayton Newell, Tadayoshi Kohno, and Ryan Calo. 2014a. Augmented reality:
hard problems of law and policy. In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous
computing: adjunct publication. ACM, 1283–1288.

Franziska Roesner, Tadayoshi Kohno, and David Molnar. 2014b. Security and Privacy for Augmented Reality Systems.

Commun. ACM 57, 4 (April 2014), 88–96. https://doi.org/10.1145/2580723.2580730

Franziska Roesner, David Molnar, Alexander Moshchuk, Tadayoshi Kohno, and Helen J Wang. 2014c. World-driven access
control for continuous sensing. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 1169–1181.

Cynthia E Rogers, Alexander W Witt, Alexander D Solomon, and Krishna K Venkatasubramanian. 2015. An approach for
user identification for head-mounted displays. In Proceedings of the 2015 ACM International Symposium on Wearable
Computers. ACM, 143–146.

Pierangela Samarati. 2001. Protecting respondents identities in microdata release. IEEE transactions on Knowledge and Data

Engineering 13, 6 (2001), 1010–1027.

Pierangela Samarati and Latanya Sweeney. 1998. Protecting Privacy when Disclosing Information: k-Anonymity and Its

Enforcement through Generalization and Suppression. Technical Report.

39

Dieter Schmalstieg and Gerd Hesina. 2002. Distributed applications for collaborative augmented reality. In Virtual Reality,

2002. Proceedings. IEEE. IEEE, 59–66.

Stefan Schneegass, Youssef Oualil, and Andreas Bulling. 2016. SkullConduct: Biometric user identification on eyewear
computers using bone conduction through the skull. In Proceedings of the 2016 CHI Conference on Human Factors in
Computing Systems. ACM, 1379–1384.

Yoones A Sekhavat. 2017. Privacy preserving cloth try-on using mobile augmented reality. IEEE Transactions on Multimedia

19, 5 (2017), 1041–1049.

Prakash Shrestha and Nitesh Saxena. 2017. An Offensive and Defensive Exposition of Wearable Computing. ACM Computing

Surveys (CSUR) 50, 6 (2017), 92.

Jiayu Shu, Rui Zheng, and Pan Hui. 2016. Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras. arXiv

preprint arXiv:1610.00889 (2016).

Mark Simkin, Dominique Schr¨oder, Andreas Bulling, and Mario Fritz. 2014. Ubic: Bridging the gap between digital

cryptography and the physical world. In European Symposium on Research in Computer Security. Springer, 56–75.

Ivo Sluganovic, Matej Serbec, Ante Derek, and Ivan Martinovic. 2017. HoloPair: Securing Shared Augmented Reality Using
Microsoft HoloLens. In Proceedings of the 33rd Annual Computer Security Applications Conference (ACSAC 2017). ACM,
New York, NY, USA, 250–261. https://doi.org/10.1145/3134600.3134625

Zsolt Szalav´ari, Erik Eckstein, and Michael Gervautz. 1998. Collaborative gaming in augmented reality. In Proceedings of the

ACM symposium on Virtual reality software and technology. ACM, 195–204.

Zsolt Szalav´ari and Michael Gervautz. 1997. The personal interaction Panel–a Two-Handed interface for augmented reality.

In Computer graphics forum, Vol. 16. Wiley Online Library.

Piotr Szczuko. 2014. Augmented reality for privacy-sensitive visual monitoring. In International Conference on Multimedia

Communications, Services and Security. Springer, 229–241.

Robert Templeman, Mohammed Korayem, David J Crandall, and Apu Kapadia. 2014. PlaceAvoider: Steering First-Person

Cameras away from Sensitive Spaces.. In NDSS.

Bruce H Thomas and Wayne Piekarski. 2002. Glove based user interaction techniques for augmented reality in an outdoor

environment. Virtual Reality 6, 3 (2002), 167–180.

Khai Truong, Shwetak Patel, Jay Summet, and Gregory Abowd. 2005. Preventing camera recording by designing a

capture-resistant environment. UbiComp 2005: Ubiquitous Computing (2005), 903–903.

Krishna K Venkatasubramanian, Ayan Banerjee, and Sandeep Kumar S Gupta. 2010. PSKA: Usable and secure key agreement

scheme for body area networks. IEEE Transactions on Information Technology in Biomedicine 14, 1 (2010), 60–68.

John Vilk, David Molnar, Benjamin Livshits, Eyal Ofek, Chris Rossbach, Alexander Moshchuk, Helen J Wang, and Ran Gal.
2015. Surroundweb: Mitigating privacy concerns in a 3d web browser. In Security and Privacy (SP), 2015 IEEE Symposium
on. IEEE, 431–446.

John Vilk, David Molnar, Eyal Ofek, Chris Rossbach, Benjamin Livshits, Alexander Moshchuk, Helen J Wang, and Ran Gal.

2014. Least Privilege Rendering in a 3D Web Browser. Technical Report.

Ky Waegel. 2014. [Poster] A reconstructive see-through display. In Mixed and Augmented Reality (ISMAR), 2014 IEEE

International Symposium on. IEEE, 319–320.

Junjue Wang, Brandon Amos, Anupam Das, Padmanabhan Pillai, Norman Sadeh, and Mahadev Satyanarayanan. 2017. A

Scalable and Privacy-Aware IoT Service for Live Video Analytics. (2017).

Grace Woo, Andrew Lippman, and Ramesh Raskar. 2012. VRCodes: Unobtrusive and active visual codes for interaction by
exploiting rolling shutter. In Mixed and Augmented Reality (ISMAR), 2012 IEEE International Symposium on. IEEE, 59–64.
Chao Xu, Parth H Pathak, and Prasant Mohapatra. 2015. Finger-writing with smartwatch: A case for finger and hand
gesture recognition using smartwatch. In Proceedings of the 16th International Workshop on Mobile Computing Systems
and Applications. ACM, 9–14.

Yan Xu, Maribeth Gandy, Sami Deen, Brian Schrank, Kim Spreen, Michael Gorbsky, Timothy White, Evan Barba, Iulian
Radu, Jay Bolter, et al. 2008. BragFish: exploring physical and social interaction in co-located handheld augmented
reality games. In Proceedings of the 2008 international conference on advances in computer entertainment technology. ACM,
276–283.

Zhi Xu and Sencun Zhu. 2015. Semadroid: A privacy-aware sensor management framework for smartphones. In Proceedings

of the 5th ACM Conference on Data and Application Security and Privacy. ACM, 61–72.

Andrew Chi-Chih Yao. 1986. How to generate and exchange secrets. In Foundations of Computer Science, 1986., 27th Annual

Symposium on. IEEE, 162–167.

Eisa Zarepour, Mohammadreza Hosseini, Salil S Kanhere, and Arcot Sowmya. 2016. A context-based privacy preserving
framework for wearable visual lifeloggers. In Pervasive Computing and Communication Workshops (PerCom Workshops),
2016 IEEE International Conference on. IEEE, 1–4.

Lan Zhang, Taeho Jung, Puchun Feng, Xiang-Yang Li, and Yunhao Liu. 2014. Cloud-based privacy preserving image storage,

sharing and search. arXiv preprint arXiv:1410.6593 (2014).

40

Juncheng Zhao and Hock Soon Seah. 2016. Interaction in marker-less augmented reality based on hand detection using
leap motion. In Proceedings of the 15th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in
Industry-Volume 1. ACM, 147–150.

M Tarek Ibn Ziad, Amr Alanwar, Moustafa Alzantot, and Mani Srivastava. 2016. Cryptoimg: Privacy preserving processing

over encrypted images. In Communications and Network Security (CNS), 2016 IEEE Conference on. IEEE, 570–575.

Received June 2018; revised 2018

41

