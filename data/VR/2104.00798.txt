FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds

Haiyan Wang1,2‚àó

Jiahao Pang1* Muhammad A. Lodhi1
1InterDigital

2The City College of New York

Yingli Tian2

Dong Tian1

hwang005@citymail.cuny.edu, jiahao.pang@interdigital.com

muhammad.lodhi@interdigital.com, ytian@ccny.cuny.edu, dong.tian@interdigital.com

1
2
0
2
c
e
D
6

]

V
C
.
s
c
[

2
v
8
9
7
0
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

Scene Ô¨Çow depicts the dynamics of a 3D scene, which is
critical for various applications such as autonomous driving,
robot navigation, AR/VR, etc. Conventionally, scene Ô¨Çow
is estimated from dense/regular RGB video frames. With
the development of depth-sensing technologies, precise 3D
measurements are available via point clouds which have
sparked new research in 3D scene Ô¨Çow. Nevertheless, it
remains challenging to extract scene Ô¨Çow from point clouds
due to the sparsity and irregularity in typical point cloud
sampling patterns. One major issue related to irregular
sampling is identiÔ¨Åed as the randomness during point set ab-
straction/feature extraction‚Äîan elementary process in many
Ô¨Çow estimation scenarios. A novel Spatial Abstraction with
Attention (SA2) layer is accordingly proposed to alleviate
the unstable abstraction problem. Moreover, a Temporal
Abstraction with Attention (TA2) layer is proposed to rec-
tify attention in temporal domain, leading to beneÔ¨Åts with
motions scaled in a larger range. Extensive analysis and
experiments veriÔ¨Åed the motivation and signiÔ¨Åcant perfor-
mance gains of our method, dubbed as Flow Estimation
via Spatial-Temporal Attention (FESTA), when compared to
several state-of-the-art benchmarks of scene Ô¨Çow estimation.

1. Introduction

Our world is dynamic. To promptly predict and respond
to the ever changing surroundings, humans are able to per-
ceive a moving scene and decipher the 3D motion of indi-
vidual objects. This capability to capture and infer from
scene dynamics is also desirable for computer vision appli-
cations. For instance, a self-driving car can maneuver its
actions upon perceiving the motions in its surroundings [19];
whereas a robot can exploit the scene dynamics to facilitate
its localization and mapping process [2]. Moreover, with ad-
vances in depth-sensing technologies, especially the LiDAR
technologies [7], point cloud data have become a common
conÔ¨Åguration in such applications.

*Authors contributed equally. Work done while Haiyan Wang was an

intern at InterDigital.

(a) FPS

(b) Grouping

(c) AP

(d) Regrouping

Figure 1: Given two consecutive point clouds, down-sampled
points produced by (a) Farthest Point Sampling (FPS, in dark red)
are different which make them intractable in scene Ô¨Çow estima-
tion. However, by appending our (c) Aggregate Pooling (AP),
stable corresponding points (in blue) are synthesized for scene Ô¨Çow
estimation.

To describe the motion of individual points in the 3D
world, scene Ô¨Çow extends 2D optical Ô¨Çow to the 3D vector
Ô¨Åeld representing the 3D scene dynamics [28]. Hence, just
like 2D optical Ô¨Çow needs to be estimated from video frames
comprising images [31, 10], 3D scene Ô¨Çow needs to be in-
ferred from point cloud data [9]. However, it is non-trivial
to accurately estimate scene Ô¨Çow from point clouds.

Unstable abstraction: Pioneered by PointNet [25]
and its extension, PointNet++ [26], deep neural networks
(DNNs) have recently been enabled to directly consume 3D
point clouds for various vision tasks. As shown in Figure 1a
and Figure 1b, the grouping based on the Farthest Point
Sampling (FPS) is widely utilized during the feature extrac-
tion process. It is treated as a basic point set abstraction
unit for segmentation as well as scene Ô¨Çow estimation, e.g.,
FlowNet3D [16] and MeteorNet [17]. The naive FPS is sim-
ple and computationally affordable, but problematic. Given
two object point clouds, both representing the same mani-
fold, FPS would likely down-sample them differently [21]
(see Figure 1a). This inconsistency due to randomness in
naive FPS is undesired for vision and machine tasks. With

Spatial Abstraction with Attention1stPC2ndPC1stPC2ndPC1stPC2ndPC1stPC2ndPCSpatial Abstraction with Attention1stPC2ndPC1stPC2ndPC1stPC2ndPC1stPC2ndPCSpatial Abstraction with Attention1stPC2ndPC1stPC2ndPC1stPC2ndPC1stPC2ndPCSpatial Abstraction with Attention1stPC2ndPC1stPC2ndPC1stPC2ndPC1stPC2ndPC 
 
 
 
 
 
two differently down-sampled point clouds, the subsequent
grouping and abstraction would lead to two dissimilar sets
of local features. Thus, it becomes intractable to estimate the
scene Ô¨Çow when comparing the features extracted via FPS.
To resolve this problem, we propose a Spatial Abstraction
with Attention (SA2) layer which adaptively down-samples
and abstracts the input point clouds. Compared to FPS, our
SA2 layer utilizes a trainable Aggregate Pooling (AP) mod-
ule to generate much more stable down-sampled points, e.g.,
blue points in Figure 1c. They deÔ¨Åne attended regions [6]
(e.g., green circles in Figure 1d) for subsequent processing.
Motion coverage: Similar to many deep matching algo-
rithms for stereo matching and optical Ô¨Çow estimation, it is
difÔ¨Åcult to have a single DNN that can accurately estimate
both large-scale motion and small-scale motion [11, 23]. To
tackle this problem, we iterate the network for Ô¨Çow reÔ¨Åne-
ment with a proposed Temporal Abstraction with Attention
(TA2) layer. It shifts the temporal attended regions to the
more correspondent areas according to the initial scene Ô¨Çow
obtained at the Ô¨Årst iteration.

In summary, we adaptively shift the attended regions
when seeking abstraction from one point cloud spatially, and
when fusing information across two point clouds temporally.
We name our proposal Flow Estimation via Spatial-Temporal
Attention, or FESTA for short. The main contributions of
our work are listed as follows:

(i) We propose the SA2 layer for stable point cloud ab-
straction. It shifts the FPS down-sampled points to
invariant positions for deÔ¨Åning the attended regions,
regardless of how the point clouds were sampled from
the scene manifold. Effectiveness of the SA2 layer is
veriÔ¨Åed both theoretically and empirically.

(ii) We propose the TA2 layer to estimate both small- and
large- scale motions. It emphasizes the regions that are
more likely to Ô¨Ånd good matches between the point
clouds, regardless of the scale of the motion.

(iii) Our proposed FESTA architecture achieves the state-of-
the-art performance for 3D point cloud scene Ô¨Çow esti-
mation on both synthetic and real world benchmarks.
Our method signiÔ¨Åcantly outperforms the state-of-the-
art methods of scene Ô¨Çow estimation.

2. Related Work

Recent studies on scene Ô¨Çow estimation mainly extend
methodologies for 2D optical Ô¨Çow estimation to 3D point
clouds. We Ô¨Årst review the related research on optical Ô¨Çow
estimation [27], then turn to deep learning methods for point
cloud processing and scene Ô¨Çow estimation.

Optical Ô¨Çow estimation: Optical Ô¨Çow estimation and its
variant, stereo matching, both look for pixel-wise correspon-
dence given a pair of 2D images. Though conventionally
solved with hand-crafted pipelines, recent proposals based

on end-to-end DNNs achieve unprecedented performance.
Among these methods, FlowNet [4, 18] is the very Ô¨Årst
trial, which adopts the popular hour-glass structure with skip
connections. This basic DNN architecture is remarkably
successful for Ô¨Ånding correspondence on images [34, 3]. It
is even extended to 3D point clouds for scene Ô¨Çow estima-
tion, e.g., FlowNet3D [16] and HPLFlowNet[8]. However,
it is difÔ¨Åcult to estimate both small- and large-scale mo-
tions using one hour-glass architecture. Thus a succeeding
work, FlowNet2 [11], stacks independent FlowNet modules
to boost the performance, at the price of a larger model. Dif-
ferently, we resolve the problem with the TA2 layer, which
efÔ¨Åciently reuses part of the network for reÔ¨Ånement.

Deep learning on point clouds: Point cloud data is usu-
ally preprocessed, e.g., voxelized, so as to comply with
deep learning frameworks justiÔ¨Åed for regular images/videos.
Emerging techniques for native learning on point clouds re-
lieve this need for format conversion. The seminal work,
PointNet [25] directly operates on input points and produces
a feature depicting the object geometry. The learned features
achieve point permutation-invariance through a pooling op-
eration. PointNet++ [30] applies FPS, followed by nearest-
neighbor (NN) grouping and PointNet to abstract an input
point cloud. This abstraction step has become a popular
elementary unit to digest point clouds. Recent works, such
as [35, 22, 15], propose complicated DNN architectures for
the abstraction step; while our SA2 layer is a lightweight
module to serve the same purpose. Moreover, these works
limit themselves by selecting existing points from the point
cloud while we synthesize new points to better represent the
underlying geometry.

Scene Ô¨Çow estimation: The task of 3D scene Ô¨Çow es-
timation was Ô¨Årst introduced by Vedula et al. [28]. It is
conventionally estimated from RGB-D videos [31] or stereo
videos [10]. Only with the advent of deep learning, has the
3D scene Ô¨Çow estimation problem directly over point cloud
data been enabled [9].

FlowNet3D [16] is the Ô¨Årst work directly learning scene
Ô¨Çow from 3D point cloud data. It ‚Äúconverts‚Äù the FlowNet [4]
architecture from the 2D image domain (with the convolu-
tional neural network) to point cloud data (with PointNet).
A follow-up work, FlowNet3D++ [26], improves the per-
formance by explicitly supervising the Ô¨Çow vectors with
both their magnitudes and orientations; while the recently
proposed PointPWC-Net [32] estimates the scene Ô¨Çow in a
coarse-to-Ô¨Åne manner by fusing the hierarchical point cloud
features. Other notable methods include HPLFlowNet [8]
applying the concepts of permutohedral lattice [1] to extract
structural information and [20] which is a self-supervised
approach. However, most of the efforts apply FPS to down-
sample the input point clouds and introduce the unstable
abstraction problem as mentioned. In contrast, we propose
the SA2 layer to retrieve invariant down-sampled points,

Figure 2: Our proposed FESTA architecture. On top of the FlowNet/FlowNet3D backbone, we specially incorporate the spatial-temporal
mechanism with the proposed SA2 and TA2 layers.

which greatly beneÔ¨Åt the subsequent matching process.

mask.

3. Framework Overview

3.1. Architecture Design

The architecture of our proposal, FESTA, is shown in
Figure 2, which follows the backbone of FlowNet3D [16]
and FlowNet [4] with an hour-glass structure. Each feature
produced by network layers consists of a representative point
accompanied by a local descriptor, e.g., the Spatial Abstrac-
tion with Attention (SA2) layer generates n1/8 such features
for the Ô¨Årst point cloud. Given two input point clouds, they
are respectively consumed by shared SA2 layers to extract
two sets of features, which we call the spatial features. Then
a proposed Temporal Abstraction with Attention (TA2) layer
serves as a coupling module to fuse the spatial features with
the Ô¨Årst point cloud serving as the reference. Its output is
another set of features that we called the temporal feature.
Different from the spatial features, the temporal features fuse
information of both point clouds, from which the 3D scene
Ô¨Çow can be extracted. After that, several Set Abstraction
layers and Set Up-Conv layers from FlowNet3D [16] are
appended to digest the temporal features, which complete
the hour-glass structure. The outputs of the last Set Up-Conv
layer are a set of point-wise features, associated with each
point in the Ô¨Årst point cloud. To extract point-wise scene
Ô¨Çow, we simply apply shared MLP layers to convert each
point-wise feature to a scene Ô¨Çow vector.

Inspired by [12], we also estimate a binary mask indicat-
ing the existence of the scene Ô¨Çow vector for each point in
the Ô¨Årst point cloud. In practice, the scene Ô¨Çow vectors may
not be available due to occlusion and motion out of the Ô¨Åeld
of view, etc. The indication of their existence may serve
as side information to help subsequent tasks. Similar to the
computation of 3D scene Ô¨Çow, dedicated MLP layers are
applied to convert the point-wise features to the existence

To enhance the scene Ô¨Çow estimation accuracy, especially
to tackle motion of all ranges, we partially re-iterate our
network with a feedback connection. Though it is possible
to run TA2 many iterations, we found that running it twice
achieves a good trade-off between computational cost and
estimation accuracy. Note that similar to [16], our FESTA ar-
chitecture can be easily adapted to take additional attributes
(e.g., RGB colors) as inputs. Please refer to the supplemen-
tary material for more details of our architecture.

3.2. Loss Function Design

To effectively train the proposed FESTA in an end-to-
end fashion, we evaluate both iteration outputs against the
ground truth Ô¨Çow. For each iteration, we Ô¨Årst compute an (cid:96)2
loss between the ground-truth scene Ô¨Çow and the estimated
one [16]. This (cid:96)2 loss is denoted by L(i)
F , with i being the
iteration index. Then the existence mask estimation is cast
as a point-wise binary classiÔ¨Åcation problem [12], and a
cross-entropy loss can be calculated against the ground-truth
existence mask, denoted by L(i)
M . The loss of the i-th iteration
is Ô¨Ånally given by:

L(i) = ¬µL(i)

F + (1 ‚àí ¬µ)L(i)
M .

(1)

Our total loss for end-to-end training aggregates losses of
both iterations, i.e.,

Ltot = (1 ‚àí Œª)L(1) + ŒªL(2).

(2)

Note that in (1) and (2), the hyper-parameters ¬µ, Œª ‚àà [0, 1].
Empirically, we set ¬µ = 0.8 and Œª = 0.7.

4. Spatial-Temporal Attention

4.1. Spatial Abstraction with Attention

Design of the SA2 layer: Key steps of our proposed Spa-
tial Abstraction with Attention (SA2) layer are illustrated

Scene Flow: ùëõ"√ó3SA2Set Up-ConvSkip Connections1stPoint Cloud: ùëõ"√ó32ndPoint Cloud: ùëõ%√ó3Feature-to-Mask MLPFeature-to-Flow MLPBinary Mask: ùëõ"11‚Ä¶1SharedFeedback Flow:‚ÅÑùëõ"8√ó3ùëõ"1283512ùëõ"8364ùëõ"83128ùëõ"3256SpatialOutputTemporalTA2Set AbstractionSA2Flow Interpolationin Figure 1. Farthest Point Sampling (FPS) followed by the
Nearest Neighbor (NN) grouping (Figure 1a, Figure 1b) are
inherited from PointNet++ [26] to divide the point cloud
into groups as initial steps. However, as mentioned in Sec-
tion 1, pure FPS-based abstraction produces unstable down-
sampled points that would tamper the scene Ô¨Çow estimation.
Herein, the design of the abstraction is motivated to reÔ¨Çect
the intrinsic geometry of the manifold M, that is invariant
to the randomness in the sampling pattern. In this work,
we approach this goal with a proposed Aggregate Pooling
(AP) module. Following FPS grouping, a point set is down-
sampled using a synthesized point (Figure 1c). Then each
newly down-sampled point deÔ¨Ånes its own attended region
via another NN grouping step, leading to a new grouping
scheme more suitable for a subsequent point-wise task (Fig-
ure 1d), i.e., scene Ô¨Çow estimation in our case. Similar to
PointNet++ [26], Ô¨Ånally we feed the new groups of points
to a shared PointNet to extract their local descriptors. The
descriptors and the associated down-sampled points consti-
tute the output of the SA2 layer, i.e., the spatial features. For
example, see the n1
8 √ó 67 matrix produced by the SA2 layer
in Figure 2.

Aggregate Pooling: The proposed AP module consumes
a group of k points and generates a synthesized point to
represent the whole group. As shown in Figure 3, it consists
of a PointNet [25] and a point aggregation step. The PointNet
computes k point-level descriptors with MLPs and a group-
level descriptor with a max-pooling operator. This PointNet
is shared among all groups of points in the point cloud. The
point aggregation step then computes the weighted average
of all the points in the group to synthesize a representative
point.

SpeciÔ¨Åcally, the point aggregation step measures the rep-
resentativeness of a point (say, the i-th point) in the group
with the similarity between its point-level descriptor (de-
noted by fi) and the group-level descriptor (denoted by fg).
Among different ways to measure the vector similarity (e.g.,
Euclidean distance, correlation coefÔ¨Åcient), we choose the
dot-product metric like [29] for its simplicity. The obtained
afÔ¨Ånity values are then passed to a softmax function for nor-
malization, resulting in a set of weights summed up to 1.
Mathematically, the weight wi for the i-th point is

wi = exp (cid:0)f T

i fg

(cid:1) ¬∑

(cid:20)(cid:88)k

j=1

(cid:21)‚àí1

exp (cid:0)f T

j fg

(cid:1)

.

(3)

Suppose the points in the group are si = (xi, yi, zi), 1 ‚â§
i ‚â§ k, then the synthesized point is simply (cid:80)k

i=1 wi ¬∑ si.

Analysis: Now we attempt to understand the mechanism
of the SA2 layer to generate more stable points. With the
FPS and the grouping steps (Figure 1a and Figure 1b), we
have two lists of point sets from the input point cloud pair.
We Ô¨Årst focus on a pair of point sets among the two lists
that are sampled over the same Riemannian manifold patch

Figure 3: Block diagram of the Aggregate Pooling (AP) module.

M through the same sampling probability distribution p(s).
Each point set is to be processed by the AP module.

Given sufÔ¨Åcient number of points in both point sets char-
acterizing the geometry M, their group-level descriptors
should be similar [25], denoted by fg. In particular, if in-
creasingly many 3D points are sampled from M according
to p, then by deÔ¨Ånition, the synthesized points from both
groups converge to the following integration on M [14]:
w (cid:0)f (s)Tfg

(cid:1) p(s) ¬∑ s ds,

s(cid:48) =

(4)

(cid:90)

1
Œ±

M

where f (s) is the point-level descriptor of the point s, func-
tion w(¬∑) converts the dot-product measure f (s)Tfg to weight
(cid:1) p(s) ds is a nor-
as is done by (3), and Œ± = (cid:82)
malization factor. Please refer to the supplementary material
for more detailed analysis.

M w (cid:0)f (s)Tfg

Since the AP module converges to a Ô¨Åxed location s(cid:48) over
M, the SA2 layer is expected to converge over a point cloud
scene. Empirical evidence is to be provided in Section 5.1
through a segmentation experiment.
Lastly, as the weight w (cid:0)f (s)Tfg

(cid:1) is computed by a learn-
able network, the SA2 layer is adaptable to down-stream
tasks by generating novel, task-aware down-sampled points.

4.2. Temporal Abstraction with Attention

Mechanisms of the TA2 layer: The Temporal Abstrac-
tion with Attention (TA2) layer aims to aggregate the spatial
features of both point clouds given an initial scene Ô¨Çow.
During the Ô¨Årst iteration without an initial scene Ô¨Çow pre-
sented, it behaves the same as the Flow Embedding layer
in FlowNet3D [16]. SpeciÔ¨Åcally, for each point (say, A) in
the Ô¨Årst down-sampled point cloud, we Ô¨Årst perform a NN
grouping step from the second point cloud, which forms a
group of points (centered at A) from the second point cloud,
as shown in the left of Figure 4. Then the grouped points, the
point A and its associated descriptor, are sent to a subsequent
PointNet to extract another local descriptor. More details of
this extraction step can be found at [16]. The down-sampled
point cloud and the new descriptors form the temporal fea-
tures, e.g., the n1
8 √ó 131 matrix produced by the TA2 layer
in Figure 2.

Group-Level Featureùë•"ùë¶"ùëß"ùë•%ùë¶%ùëß%ùë•&ùë¶&ùëß&Shared MLPs‚Ä¶‚Ä¶‚Ä¶Max PoolingPoint Group‚Ä¶Weight Estimateùë§"ùë§%ùë§&ùëßùë¶ùë•Weighted AverageSynthesized PointPoint-Level DescriptorsPointNetPoint AggregationGroup-Level DescriptorFigure 4: Two iterations of the TA2 layer. The blue circle in the
left one demonstrates the attended region for the 1st iteration, which
is inaccurate for the points correspondence. The green circle in the
right one drags the attended region to the correspondent area by the
initial Ô¨Çow estimated in the 1st iteration.

During the second iteration, we reuse the spatial features
generated by the SA2 layers at the Ô¨Årst iteration and feed
them to the TA2 layer (Figure 2). However, now with an
initial scene Ô¨Çow corresponding to the Ô¨Årst down-sampled
point cloud available, we translate the search regions accord-
ing to each of the scene Ô¨Çow vectors. SpeciÔ¨Åcally, suppose
the coordinate of A is (xA, yA, zA) and its initial scene Ô¨Çow
vector is (uA, vA, wA), then the NN grouping is performed
centered at (xA + uA, yA + vA, zA + wA), see the right of
Figure 4. Note that to obtain the initial scene Ô¨Çow for the Ô¨Årst
down-sampled point cloud, it requires an extra interpolation
step, i.e., the Flow Interpolation module in Figure 2. We im-
plement it as a simple deterministic module without trainable
parameters. To estimate the scene Ô¨Çow vector at a certain
point, it computes the average scene Ô¨Çow vector in the neigh-
borhood of that point. Please refer to the supplementary
material for more details.

Analysis: Intuitively, the NN grouping step search for
all points in the second point cloud that ‚Äúappear close‚Äù to
A; its search range deÔ¨Ånes an attended region (the blue
circle in Figure 4). By only estimating the scene Ô¨Çow in
one pass, it requires to choose a universal search radius for
all ranges of motion. However, when the attended region
(or equivalently, the search radius r1 in Figure 4) is too
small, it fails to capture large-scale motion; while for a large
attended region or a large r1, it includes too many candidates
from the second point cloud and harms the granularity of the
estimation (especially for small-scale motion). This problem
generally exists for not only scene Ô¨Çow estimation but also
related problems such as stereo matching [23] and optical
Ô¨Çow estimation [11].

By introducing a second iteration, our TA2 layer accord-
ingly shifts the attended regions to conÔ¨Ådent areas that are
more likely to observe good matches from the second point
cloud. Consequently, for the Ô¨Årst iteration, it is more critical
to identify a ‚Äúcorrect‚Äù direction than a ‚Äúcorrect‚Äù result. It is
reÔ¨Çected in the selection of hyper-parameter Œª = 0.7. More-
over, with a rough knowledge about how the second point
cloud moves, the attended region at the second iteration (or
radius r2 in Figure 4) can be further reduced to search for

Figure 5: Compared to FPS, our proposal generates more stable
down-sampled point clouds.

more reÔ¨Åned matching candidates.

5. Experimentation

This section Ô¨Årst veriÔ¨Åes how the SA2 layer serves as
a stable point cloud abstraction unit. Then the proposed
FESTA architecture is evaluated for 3D scene Ô¨Çow estima-
tion. Finally, we inspect how the key components contribute
to the FESTA framework by an ablation study.

5.1. Abstraction with the SA2 Layer

The proposed SA2 layer generally provides an alternative
abstraction than FPS-like methods. It is desired to explicitly
study its stability as pointed out in Section 4.1.
In this
test, we design a dedicated object segmentation process, as
this per-point task has minimum additional procedure than
abstraction, if compared to FESTA framework. Note that the
segmentation serves as a test bed to verify the stability of
SA2, rather than to claim state-of-the-art segmentation.

Set-up: We build up a scene point cloud dataset using
object point clouds from ModelNet40 [33]. This scene point
cloud dataset contains 104 scenes; each scene contains 3 to
6 objects packed within a sphere-shaped container of radius
3. Moreover, all objects in a scene are normalized within a
sphere of radius r, and are situated with a distance of at least
2 between their object centers. Evidently, by enlarging the
objects with a bigger radius r, they are more likely to collide
with one another, making it more challenging to distinguish
and segment the objects. We prepare 4 versions of the scene
point cloud dataset with different object radii r ranging from
1 to 1.8. Our object segmentation network is built upon the
PointNet++ [26] by replacing the FPS groupings with our
proposed SA2 layers.

Abstraction stability: We Ô¨Årst evaluate the stability of
our down-sampled point clouds using the above segmenta-
tion framework with dataset radius r = 1.2. Given a scene in
our multi-object dataset, we randomly pick n ‚àà [256, 2048]
points from it for 100 different times, resulting in 100 input

Temporal Abstraction with Attention1stPoint Cloud2ndPoint CloudShifted PointInitial Flowùëüùëü11stIter.ùê¥ùê¥ùëüùëü22ndIter.ùê¥ùê¥25050075010001250150017502000Number of Points0.100.120.140.160.180.200.220.24Average Chamfer DistanceFPSSA¬≤clearer separation between objects, which is highly preferred
for segmentation.

5.2. Scene Flow Estimation with FESTA

Combining the SA2 and the TA2 layers, we evaluate the

proposed FESTA architecture for scene Ô¨Çow estimation.

Datasets: Our experiments are conducted on two popu-
lar datasets, the FlyingThings3D [18] and the KITTI Scene
Flow [7] (referred to as KITTI) datasets. Both of them are
originally designed for matching tasks in the image domain
(e.g., stereo matching). Recently, Liu et al. [16] converted
them for scene Ô¨Çow estimation from 3D point clouds. The
FlyingThings3D dataset is a synthetic dataset with 20,000
and 2,000 point cloud pairs for training and testing, respec-
tively. In addition to the point cloud geometry, the RGB
colors and the binary existence masks are also available.
Different from the FlyingThings3D, KITTI is a real dataset
collected by LiDAR sensors, and contains incomplete ob-
jects. The KITTI dataset has 150 point cloud pairs with
available ground-truth scene Ô¨Çow. Similar to [16, 8] and
others, we only use the geometry (point coordinates) when
computing the 3D scene Ô¨Çow.

Benchmarks and evaluation metrics: We compare our
FESTA with the following methods, listed in chronologi-
cal order: FlowNet3D [16], HPLFlowNet [8], PointPWC-
Net [32], MeteorNet [16], FlowNet3D++[30], and a self-
supervised method, Just Go with the Flow [20]. The scene
Ô¨Çow quality is Ô¨Årst evaluated with End-Point-Error (EPE),
which calculates the mean Euclidean distance between the
ground-truth Ô¨Çow and the prediction. We also adopt two
additional metrics from [8], Acc Strict and Acc Relax.1
Both Acc Strict and Acc Relax aim to measure estimation
accuracy, but with different thresholds. Acc Strict mea-
sures the percentage of points satisfying EPE < 0.05m or
relative error < 5%; while Acc Relax measures the percent-
age of points with EPE < 0.1m or relative error < 10%.

Implementation details: The proposed FESTA is trained
with two conÔ¨Ågurations over FlyingThings3D dataset fol-
lowing the FlowNet3D [16], with geometry-only and with
additional RGB attributes. The two conÔ¨Ågurations are both
trained with the Adam optimizer [13] for 500 epochs, with
a batch size of 32 and a learning rate of 0.001. The size
of the input point clouds are all set to 2048. All experi-
ments are performed in the PyTorch [24] framework. For
the geometry-only conÔ¨Åguration, the inference is performed
on both FlyingThings3D and KITTI. In other words, the
model is never tuned over KITTI, similarly done in [16, 8]
and others. With RGB attributes available, the inference is
conducted on FlyingThing3D. Quantitative and qualitative
evaluations are performed.

Quantitative evaluation: Quantitative results are re-

1They are called Acc3D Strict and Acc3D Relax in [8]. We remove ‚Äú3D‚Äù

since we do not need to distinguish with 2D cases.

Figure 6: Down-sampled points of FPS (red) and our SA2 layer
(blue) for the object segmentation task.

Table 1: Object segmentation accuracy (%).

Methods

PointNet++ (FPS)
Ours (SA2)

Object radius r

1.0

92.56
93.21

1.2

88.74
90.48

1.5

65.87
80.10

1.8

43.07
69.18

point clouds representing the same 3D scene. Then we feed
these point clouds as inputs to both FPS-based and SA2-
based segmentation network to obtain down-sampled point
sets containing only 64 points. For a stable down-sampling
procedure, the down-sampled results from the 100 point
clouds should be similar to one another. To evaluate the sim-
ilarity, we compute the Chamfer Distances (CD) [5] between
any two down-sampled point clouds, then take an average to
characterize the stability. A smaller average CD means more
stable down-sampling. Further averaging over 30 scenes
randomly selected from our dataset is performed. Finally,
as shown in Figure 5, SA2 always produces more stable
down-sampled results than FPS. Especially, for n > 1000,
our approach even reduces the average CD of FPS by about
50%, which conÔ¨Årms the superior stability of the proposed
SA2 layer. The plot for SA2 also veriÔ¨Åes the analysis in
Section 4.1, which indicated that as the sampling becomes
denser the down-sampled points become more stable.

Evaluation: Having tested the SA2 layer, we turn to un-
derstand how its stable abstraction beneÔ¨Åts the segmentation
performance. Based on FPS and the SA2 layer, two segmen-
tation networks are trained on all the 4 versions of the dataset
(object radii r ranging from 1 to 1.8) with the cross-entropy
loss. Table 1 compares the performance of SA2 based seg-
mentation with FPS-based segmentation, i.e., PointNet++,
showing that SA2 always provides higher segmentation ac-
curacies. As segmentation difÔ¨Åculty increases, our approach
out-performs PointNet++ by a larger margin, e.g., our accu-
racy is 26% higher than PointNet++ when r = 1.8.

We demonstrate the down-sampled points of scene point
clouds in Figure 6, where the grey points depict the input
point cloud; while the red and the blue points are those
sampled by FPS and SA2, respectively. Thanks to the stable
abstraction as veriÔ¨Åed earlier, the SA2 layer consistently
generates points belonging to distinctive objects and exhibits

Figure 7: Scene Ô¨Çow estimation on the FlyingThings3D dataset between 1st PC (in red), 2nd PC (in green). The results of our proposed
FESTA architecture is shown with warped PC (in blue) ‚Äì 1st PC warped by the scene Ô¨Çow.

Table 2: Quantitative evaluation on the FlyingThings3D and the KITTI datasets.

Methods

FlowNet3D [16]
HPLFlowNet [8]
PointPWC-Net [32]
MeteorNet [17]
FlowNet3D++ [30]
Just Go w/ Flow [20]
FESTA (Ours)

FlyingThings3D, geo.+RGB

FlyingThings3D, geo.-only

KITTI, geo.-only

EPE (m)

Acc S. (%)

Acc R. (%)

EPE (m)

Acc S. (%)

Acc R. (%)

EPE (m)

Acc S. (%)

Acc R. (%)

0.1694
0.1318
0.1205
-
0.1369
-
0.1113

25.37
32.78
39.45
-
30.33
-
43.12

57.85
63.22
67.81
-
63.43
-
74.42

0.1705
0.1453
0.1310
0.2090
0.1553
-
0.1253

23.71
29.46
34.22
-
28.50
-
39.52

56.05
61.91
65.78
52.12
60.39
-
71.24

0.1220
0.1190
0.1094
0.2510
0.2530
0.1220
0.0936

18.53
30.83
35.98
-
-
25.37
44.85

57.03
64.76
73.84
-
-
57.85
83.35

ported in Table 2, where the proposed FESTA consistently
outperforms the competing methods with signiÔ¨Åcant gains.
For example, regarding to Acc Strict values for geometry
only, our FESTA improves over the state-of-the-art method,
PointPWC-Net, by 5.3% for FlyingThings3D or 8.9% for
KITTI; and improves over our backbone, FlowNet3D, by
15.8% for FlyingThings3D or 26.3% for KITTI. Encour-
agingly, when inspecting conÔ¨Ågurations with and without
RGB over FlyingThings3D, it is noticed that in most cases,
with geometry alone, our FESTA surpasses the competitors
even when they take extra RGB attributes. For instance, our
FESTA (geometry only) achieves an EPE of 0.1253, lower
than that of FlowNet++ (geometry+RGB) which is 0.1369.
We compare our model size and run time to representative
methods and report in Table 3, where the run time is evalu-
ated on a Nvidia GTX 1080 Ti GPU with 11 GB memory.
We conÔ¨Årm that our superior performance is achieved by
a model of size 16.1 MB, similar to FlowNet3D which is
14.9 MB. It is much smaller than other competing methods,
PointPWC-Net and HPLFlowNet. Moreover, by removing
the TA2, we take similar run time as FlowNet3D, but still
greatly reduce its EPE from 0.1705 (Table 2) to 0.1381 (to
be seen in ablation study, Table 4).

Qualitative evaluation: Visualization of the scene Ô¨Çow
obtained by FESTA are shown in Figure 7 for FlyingTh-
ings3D and in Figure 8 for KITTI. Selected parts are zoomed
in for a better illustration. In each example, red points and

Table 3: Evaluation of model size and run time. F. - FlowNet3D
[16]; H. - HPLFlowNet [8]; P. - PointPWC-Net [32].

Metrics
Size (MB)
Time (ms)

F.
14.9
34.9

H.
231.8
93.1

P.
30.1
38.5

FESTA w/o TA2
16.1
35.2

FESTA (Ours)
16.1
67.8

green points represent the Ô¨Årst and the second point cloud
frames, respectively. Blue points represent the warped point
cloud, generated by translating each point in the Ô¨Årst point
cloud according to the estimated scene Ô¨Çow vector. With
more accurate scene Ô¨Çow vectors, the warped point cloud
gets more overlapped with the second point cloud. It can be
seen that for all cases in Figure 7, 8, our predicted scene Ô¨Çow
produces warped point clouds that are highly overlapped
with the second point cloud. It afÔ¨Årms the effectiveness
of our proposed FESTA on scene Ô¨Çow estimation. Be re-
minded that our network has never observed any data from
the KITTI dataset; however, it still successfully generalizes
to KITTI and captures the dynamics solely based on the
point coordinates.

5.3. Ablation Study

The ablation study is performed with geometry only con-
Ô¨Åguration. We investigate the beneÔ¨Åts of individual compo-
nents in our FESTA architecture. SpeciÔ¨Åcally, we consider
the following three variants:

(i) Replace the SA2 layers as the simple FPS grouping
followed by feature extraction in PointNet++ [26] (or

Figure 8: Scene Ô¨Çow estimation on the KITTI dataset between 1st PC (in red), 2nd PC (in green). The results of our proposed FESTA
architecture is shown with warped PC (in blue) ‚Äì 1st PC warped by the scene Ô¨Çow.

Table 4: Evaluation of different variants of FESTA.

Data.

Fly.

KI.

SA2 TA2 Mask
(cid:88)
√ó
(cid:88)
√ó
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
√ó
(cid:88)
√ó
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
√ó
(cid:88)
(cid:88)
(cid:88)
√ó
(cid:88)

FlowNet3D [16]);

EPE (m)

Acc S. (%)

Acc R. (%)

0.1402
0.1381
0.1289
0.1253
0.1163
0.1027
0.0955
0.0936

33.15
34.70
37.91
39.52
30.10
41.07
42.74
44.85

66.14
67.36
69.05
71.24
70.85
80.04
80.15
83.35

(ii) Replace the TA2 layer as the Flow Embedding layer in
FlowNet3D, i.e., the second iteration is removed; and

(iii) Remove the outputting of existence mask, i.e., the net-

work is trained only over 3D scene Ô¨Çow.

Performance of the mentioned variants of FESTA are
reported in Table 4. The best- and the worst- performing
metrics are highlighted with bold faces and with underlines,
respectively. We see that both the SA2 and the TA2 layers
bring substantial gain to our model; while generally, the SA2
layer is slightly more effective than the TA2 layer. More-
over, by jointly estimating an additional existence mask, our
scene Ô¨Çow quality further improves. That is because the
ground-truth existence mask provides extra clues about the
motion [12], which supervises the network to capture the
dynamics more precisely.

Additionally, we further examine how the SA2 and the
TA2 layers beneÔ¨Åt the estimation of motion at different scales.
SpeciÔ¨Åcally, we classify 3D points in the FlyingThings3D
test set according to their ground-truth Ô¨Çow vector magni-
tudes. For each bin of scene Ô¨Çow magnitude, we compute
an average of relative error achieved by our FESTA, and
count the bin size. In this way, we plot a relative error curve
(in green) of FESTA on different scene Ô¨Çow magnitudes,
as shown in Figure 9. We similarly plot the curves for the
variants without SA2 and TA2 in blue and red, respectively.
By comparing the red and the green curves, we see that
the TA2 layer greatly improves the performance of large-

Figure 9: For ground-truth motion with different magnitudes, the
variants of FESTA perform differently.

scale motion, which is expected because the TA2 directly
shifts its attended region according to an initial scene Ô¨Çow.
Differently, the SA2 layer beneÔ¨Åts scene Ô¨Çow with smaller
magnitudes. That is because the SA2 layer is able to adjust
its attended region gently‚Äîthe convex hull of an input point
group (Section 4.1), which enhances the granularity of the
estimation and beneÔ¨Åts mainly small-scale motion.

6. Conclusion

We propose a new spatial-temporal attention mechanism
to estimate 3D scene Ô¨Çow from point clouds. The effective-
ness of our proposed Flow Estimation via Spatial-Temporal
Attention (FESTA) has been proven by our state-of-the-
art performance of extensive experiments. Essentially, our
spatial-temporal attention mechanism successfully rectiÔ¨Åes
the region of interest (RoI) based on the feedback from ear-
lier trials. Its rationale resembles existing literature utilizing
the attention mechanism, such as [6] for image recognition
and [36] for sentence modeling. For future research, we plan
to investigate the potentials of the SA2 and TA2 layers for
different point cloud processing tasks, such as classiÔ¨Åcation,
registration, and compression.

12345Ground-Truth Scene Flow Mangnitude (m)0.060.080.100.120.140.16Relative ErrorData DistributionFESTAFESTA w/o SA¬≤FESTA w/o TA¬≤010203040506070Percent of dataReferences

[1] Andrew Adams, Jongmin Baek, and Myers Abraham Davis.
Fast high-dimensional Ô¨Åltering using the permutohedral lat-
tice. In Computer Graphics Forum, volume 29, pages 753‚Äì
762. Wiley Online Library, 2010. 2

[2] Pablo F Alcantarilla, Jos¬¥e J Yebes, Javier Almaz¬¥an, and
Luis M Bergasa. On combining visual SLAM and dense scene
Ô¨Çow to increase the robustness of localization and mapping
in dynamic environments. In IEEE International Conference
on Robotics and Automation, pages 1290‚Äì1297, 2012. 1
[3] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo match-
ing network. In IEEE Conf. Comput. Vis. Pattern Recog.,
pages 5410‚Äì5418, 2018. 2

[4] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser,
Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt,
Daniel Cremers, and Thomas Brox. FlowNet: Learning opti-
cal Ô¨Çow with convolutional networks. In Int. Conf. Comput.
Vis., pages 2758‚Äì2766, 2015. 2, 3

[5] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3D object reconstruction from a single
image. In IEEE Conf. Comput. Vis. Pattern Recog., pages
605‚Äì613, 2017. 6

[6] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see
better: Recurrent attention convolutional neural network for
Ô¨Åne-grained image recognition. In IEEE Conf. Comput. Vis.
Pattern Recog., pages 4438‚Äì4446, 2017. 2, 8

[7] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? The KITTI vision benchmark
In IEEE Conf. Comput. Vis. Pattern Recog., pages
suite.
3354‚Äì3361, 2012. 1, 6

[8] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and
Panqu Wang. HPLFlowNet: Hierarchical permutohedral
lattice FlowNet for scene Ô¨Çow estimation on large-scale point
clouds. In IEEE Conf. Comput. Vis. Pattern Recog., pages
3254‚Äì3263, 2019. 2, 6, 7

[9] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu,
and Mohammed Bennamoun. Deep learning for 3D point
clouds: A survey. IEEE Trans. Pattern Anal. Mach. Intell.,
2020. 1, 2

[10] Evan Herbst, Xiaofeng Ren, and Dieter Fox. RGB-D Ô¨Çow:
Dense 3-D motion estimation using color and depth. In IEEE
International Conference on Robotics and Automation, pages
2276‚Äì2282, 2013. 1, 2

[11] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. FlowNet 2.0: Evolu-
tion of optical Ô¨Çow estimation with deep networks. In IEEE
Conf. Comput. Vis. Pattern Recog., pages 2462‚Äì2470, 2017.
2, 5

[12] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas Brox.
Occlusions, motion and depth boundaries with a generic net-
work for disparity, optical Ô¨Çow or scene Ô¨Çow estimation. In
Eur. Conf. Comput. Vis., pages 614‚Äì630, 2018. 3, 8

[13] Diederick P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Int. Conf. Learn. Represent., 2015.
6

[14] Erwin Kreyszig. Introductory Functional Analysis with Ap-

plications, volume 1. Wiley New York, 1978. 4

[15] Itai Lang, Asaf Manor, and Shai Avidan. SampleNet: Differ-
entiable point cloud sampling. In IEEE Conf. Comput. Vis.

Pattern Recog., pages 7578‚Äì7588, 2020. 2

[16] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
FlowNet3D: Learning scene Ô¨Çow in 3D point clouds. In IEEE
Conf. Comput. Vis. Pattern Recog., pages 529‚Äì537, 2019. 1,
2, 3, 4, 6, 7, 8

[17] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. MeteoNet:
Deep learning on dynamic 3D point cloud sequences. In Int.
Conf. Comput. Vis., pages 9246‚Äì9255, 2019. 1, 7

[18] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity, op-
tical Ô¨Çow, and scene Ô¨Çow estimation. In IEEE Conf. Comput.
Vis. Pattern Recog., pages 4040‚Äì4048, 2016. 2, 6

[19] Moritz Menze and Andreas Geiger. Object scene Ô¨Çow for
autonomous vehicles. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 3061‚Äì3070, 2015. 1

[20] Himangi Mittal, Brian Okorn, and David Held. Just go with
the Ô¨Çow: Self-supervised scene Ô¨Çow estimation. In IEEE
Conf. Comput. Vis. Pattern Recog., pages 11177‚Äì11185, 2020.
2, 6, 7

[21] Carsten Moenning and Neil A Dodgson. Fast marching far-
thest point sampling. Technical report, University of Cam-
bridge, Computer Laboratory, 2003. 1

[22] Ehsan Nezhadarya, Ehsan Taghavi, Ryan Razani, Bingbing
Liu, and Jun Luo. Adaptive hierarchical down-sampling for
point cloud classiÔ¨Åcation. In IEEE Conf. Comput. Vis. Pattern
Recog., pages 12956‚Äì12964, 2020. 2

[23] Jiahao Pang, Wenxiu Sun, Chengxi Yang, Jimmy Ren,
Ruichao Xiao, Jin Zeng, and Liang Lin. Zoom and learn:
Generalizing deep stereo matching to novel domains.
In
IEEE Conf. Comput. Vis. Pattern Recog., pages 2070‚Äì2079,
2018. 2, 5

[24] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in PyTorch. In Advances in Neural Information
Processing Systems Workshop, 2017. 6

[25] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
PointNet: Deep learning on point sets for 3D classiÔ¨Åcation
and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog.,
pages 652‚Äì660, 2017. 1, 2, 4

[26] Charles R. Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
Net++: Deep hierarchical feature learning on point sets in a
metric space. In Advances in Neural Information Processing
Systems, pages 5099‚Äì5108, 2017. 1, 2, 4, 5, 7

[27] Daniel Scharstein and Richard Szeliski. A taxonomy and eval-
uation of dense two-frame stereo correspondence algorithms.
Int. J. Comput. Vis., 47(1-3):7‚Äì42, 2002. 2

[28] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins,
and Takeo Kanade. Three-dimensional scene Ô¨Çow. In Int.
Conf. Comput. Vis., volume 2, pages 722‚Äì729, 1999. 1, 2
[29] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming
He. Non-local neural networks. In IEEE Conf. Comput. Vis.
Pattern Recog., pages 7794‚Äì7803, 2018. 4

[30] Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor
Prisacariu, and Min Chen. FlowNet3D++: Geometric losses
for deep scene Ô¨Çow estimation. In The IEEE Winter Confer-
ence on Applications of Computer Vision, pages 91‚Äì98, 2020.
2, 6, 7

[31] Andreas Wedel, Clemens Rabe, Tobi Vaudrey, Thomas Brox,
Uwe Franke, and Daniel Cremers. EfÔ¨Åcient dense scene Ô¨Çow
from sparse or dense stereo data. In Eur. Conf. Comput. Vis.,
pages 739‚Äì751. Springer, 2008. 1, 2

[32] Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, and Li
Fuxin. PointPWC-Net: A coarse-to-Ô¨Åne network for super-
vised and self-supervised scene Ô¨Çow estimation on 3D point
clouds. In Eur. Conf. Comput. Vis., 2019. 2, 6, 7

[33] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3D
ShapeNets: A deep representation for volumetric shapes. In
IEEE Conf. Comput. Vis. Pattern Recog., pages 1912‚Äì1920,
2015. 5

[34] Gengshan Yang and Deva Ramanan. Volumetric correspon-
dence networks for optical Ô¨Çow. In Adv. Neural Inform. Pro-
cess. Syst., pages 794‚Äì805, 2019. 2

[35] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinx-
ian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds
with self-attention and Gumbel subset sampling. In IEEE
Conf. Comput. Vis. Pattern Recog., pages 3323‚Äì3332, 2019.
2

[36] Wenpeng Yin, Hinrich Sch¬®utze, Bing Xiang, and Bowen Zhou.
ABCNN: Attention-based convolutional neural network for
modeling sentence pairs. Transactions of the Association for
Computational Linguistics, 4:259‚Äì272, 2016. 8

