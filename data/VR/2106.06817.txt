Evaluating Foveated Video Quality Using Entropic
Differencing

Yize Jin
The University of Texas at Austin
Austin, USA
yizejin@utexas.edu

Anjul Patney
Facebook Reality Labs
Seattle, USA
rwebb@fb.com

Alan Bovik
The University of Texas at Austin
Austin, USA
bovik@ece.utexas.edu

1
2
0
2

n
u
J

2
1

]

V

I
.
s
s
e
e
[

1
v
7
1
8
6
0
.
6
0
1
2
:
v
i
X
r
a

Abstract—Virtual Reality is regaining attention due to recent
advancements in hardware technology. Immersive images / videos
are becoming widely adopted to carry omnidirectional visual
information. However, due to the requirements for higher spatial
and temporal resolution of real video data, immersive videos
require signiﬁcantly larger bandwidth consumption. To reduce
stresses on bandwidth, foveated video compression is regaining
popularity, whereby the space-variant spatial resolution of the
retina is exploited. Towards advancing the progress of foveated
video compression, we propose a full reference (FR) foveated im-
age quality assessment algorithm, which we call foveated entropic
differencing (FED), which employs the natural scene statistics of
bandpass responses by applying differences of local entropies
weighted by a foveation-based error sensitivity function. We
evaluate the proposed algorithm by measuring the correlations
of the predictions that FED makes against human judgements
on the newly created 2D and 3D LIVE-FBT-FCVR databases
for Virtual Reality (VR). The performance of the proposed
algorithm yields state-of-the-art as compared with other existing
full reference algorithms. Software for FED has been made
available at: http://live.ece.utexas.edu/research/Quality/FED.zip

Index Terms—Entropic Differencing, foveated video quality

assessment, contrast sensitivity function

I. INTRODUCTION

In virtual reality (VR), immersive images and videos are
important forms of media that carry visual information cov-
ering all 360 degrees. Given the highest resolution of the
human vision system (HVS) of about 120 pixels per degree
(PPD), to reach a desirable viewing experience, the spatial
resolution of immersive videos should reach at least 8K. In
addition, immersive videos should include high frame rates
(HFR) to reduce motion sickness, hence demanding even
higher bandwidth consumption.

To reduced the stresses imposed by such bandwidth-hungry
immersive VR systems, foveated image / video compression
is regaining relevance, due to the availability of consumer
eye tracking devices that can be easily integrated into HMDs.
Foveated image / video compression exploits the reduced vi-
sual acuity in the visual periphery, and assign higher compres-
sion ratio / quantization parameters (QP) to higher eccentricity
relative to the fovea. Early implementations of the technique
appeared more than two decades ago [1]–[4], however, neither

The authors thank Facebook Reality Labs for fruitful discussions and

supports.

the driving need nor affordable eye tracking devices were
mature.

Recently developed foveated compression algorithms, [5],
[6], are usually supported by user studies which serve to
verify the efﬁcacy of the algorithms. However, although user /
subjective evaluations are the most reliable method of quality
assessment, they are also known to be expensive and time
consuming, making it necessary to devise objective foveated
quality assessment algorithms. Towards the development of
objective algorithms, the authors of [7], [8] developed 2D and
3D foveated / compressed video quality databases in Virtual
Reality, and evaluated a wide variety of quality assessment
(QA) algorithms on the databases. Among available full ref-
erence (FR) foveated algorithms, Foveated Wavelet Quality
Index (FWQI) [9] weights the error between the reference
image and the foveated / distorted image in the wavelet
domain, using a contrast sensitivity function (CSF) model [4],
and a visually detectable noise threshold model [10]. The
Foveation-based Content Adaptive SSIM (FA-SSIM) model
[11] weights the SSIM index with a foveation-based CSF
which includes the retinal velocity and the corner velocity.

An important category of IQA algorithms relies on the
statistical regularities of natural images (natural scene statistics
– NSS). NSS-based QA was ﬁrst used in the Information Fi-
delity Criterion (IFC) [12] and the Visual Information Fidelity
(VIF) models [13], where Gaussian scale mixtures (GSM)
[14] were used to characterize natural images in the wavelet
domain, and image quality is captured by the conditional
mutual information between the reference image model and
the distorted image model. The creators of the Reduced Refer-
ence Entropic Differencing (RRED) [15] and Spatio-Temporal
Entropic Differencing (STRRED) [16] models showed that,
rather than computing conditional mutual information, which
requires full reference information, a reduced amount of ref-
erence information can be obtained by computing the average
differences of scaled local entropies in the wavelet domain.
The authors of Speed-QA [17] showed that local entropic
differencing can be applied in the spatial domain, achieving
comparable performance to RRED or STRRED with much
higher efﬁciency.

We propose an NSS-based foveated image quality assess-
ment algorithm which model the bandpass-ﬁltered images as
GSM [14], and, for each subband, instead of scaling local

 
 
 
 
 
 
entropies with local scalar factors, we weight the entropy
map using a foveation-based error sensitivity function [9],
with frequency set to the average frequency of the subband.
Then the entropy differences are calculated and averaged
across subbands. We have found that the proposed Foveated
Entropic Differencing (FED) algorithm achieves state-of-the-
art performance on both the 2D and 3D LIVE-FBT-FCVR
databases [8] when applied on a frame-by-frame basis.

II. FOVEATION BASED ERROR SENSITIVITY

We employ the foveation-based error sensitivity function

used in FWQI [9] for given frequency f and eccentricity e:

Sf (f, e) =

(cid:40) CS(f,e)

CS(f,0) = exp(−0.0461f · e),
0,

f ≤ fm(e),
f > fm(e),

(1)
where CS(f, e) is the contrast sensitivity function in [4]
deﬁned by the inverse of the contrast threshold CT .

CS(f, e) =

1
CT (f, e)

CT (f, e) = CT0 exp

αf

(cid:18)

(cid:19)

,

e + e2
e2

(2)

(3)

where CT is the contrast threshold, CT0 is a constant minimal
contrast threshold, f and e are spatial frequency and the eccen-
tricity, respectively, α is the spatial frequency decay constant,
and e2 is the constant half-resolution eccentricity. We follow
the parameter values given in [4]: CT0 = 1/64, α = 0.106,
and e2 = 2.3.

In (1) fm is the combined cutoff frequency for a given

eccentricity:

fm(e) = min(fc(e), fd)

(4)

where the critical frequency fc for a given eccentricity e is
computed by setting CT = 1.0 in (3), which is the maximum
possible contrast:

fc(e) =

e2 ln (1/CT0)
(e + e2)α

(5)

and fd is the display Nyquist frequency: fd = d/2 (ci-
cles/degree), where d is the display resolution:

d ≈

πM v
180

(pixels/degree)

(6)

where M is the with of the picture in pixel, v is the viewing
distance measured in image width, and can be related to the
ﬁeld of view (FOV) when gazing at the center of the image
by:

v =

cot(F OV /2)
2

III. THE PROPOSED ALGORITHM

NSS-based FR / RR IQA algorithms rely on the strong
statistical regularities of natural images: bandpass coefﬁcients
of natural images follow heavy-tailed distributions [14], and
divisively normalizing the bandpass coefﬁcients using local
variances causes the empirical distribution strongly tend to-
wards a decorrelated normal Gaussian distribution [18]. The

(a) Difference of Gaussian

(b) Dirac-convolved Rectangular

Fig. 1. A DoG and a 2D Dirac-convolved rectangular in the frequency domain.
We set σ1 = r1 and σ2 = r2, where σ1 and σ2 are the two variances of
Gaussians in the DoG, and r1 and r2 are the inner and outer radii of the
Dirac-convolved rectangular. It can be seen that the DoG has much larger
frequency coverage.

authors of [15], [17], [19] have explored DCT decomposition,
wavelet decompositions, and spatial predictive coding ﬁlters.
The proposed FED model is based on the idea of combining
measurements of statistical regularities (or lack thereof) with
foveation-based error sensitivity functions. A wide range of
zero-DC bandpass ﬁlters can be used to reveal these statistical
regularities. We assume that when restricted to a narrow band
in the frequency domain, as shown in Fig. 1b, the NSS of the
bandpass coefﬁcients often reﬂect distortions expressed in the
particular frequency band. Therefore, the entropy differences
computed from the NSS may be used to capture local per-
ceptual distortions within subbands. A foveation-based error
sensitivity function [9] tuned to a speciﬁc narrow subband can
be applied to weight spatial entropy differences from each such
subband to approximately weight the importance of perceptual
distortions in the fovea and in the periphery.

A. Modeling Bandpass Coefﬁcients

We deﬁne {Bk}N , k ∈ {1, ..., N } to be a ﬁlterbank of
zero-DC bandpass ﬁlters, and { ˆIk} corresponding bandpass
reponses:

ˆIk(i, j) = Bk(i, j) ∗ I(i, j),

(8)

where ∗ and I(i, j) are the convolution operator and the input
image, respectively. We model a b × b bandpass response
in ˆIk as a GSM vector: x ∼ zU, where ∼ denotes equal
in distribution, z indicates the mixing multiplier which is
independent of U, and U is a zero-mean Gaussian vector with
covariance matrix Cu. The distribution of the GSM vector can
be expressed as:

(7)

(cid:90)

fx(x) ∼

1
(2π)d/2|z2Cu|1/2

exp

(cid:18) −xT Cu
z2

−1x

(cid:19)

fz(z)dz

(9)
We found the choice of ﬁlterbanks also important. One

possible choice is Difference of Gaussian (DoG) ﬁlters:

DoG(r; σ1, σ2) =
1
√

exp

(cid:18)

σ1

2π

(cid:19)

−

−

r2
2σ2
1

1
√

2π

σ2

(cid:18)

exp

−

(cid:19)

,

r2
2σ2
2

(10)

where σ1 and σ2 are standard deviations of the Gaussian
functions, and r is the distance between the pixel (i, j) and
the gaze center (i0, j0):

frequency of the ﬁlter Bk, e be the eccentricity in degrees of
the block (p, q), which is computed as follows:

r = (cid:112)(i − i0)2 + (j − j0)2

(11)

e(p, q) = tan−1

(cid:32) (cid:112)(bp − i0)2 + (bq − j0)2
vM

(cid:33)

(19)

However, DoG ﬁlters are not desirable because of their large
frequency coverage when trying to extract high-frequency
information, as shown in Fig. 1a. This is because when
computing the CSF of the subband coefﬁcients, it is desired
that each bandpass ﬁlter only cover a narrow frequency band,
so that the frequency band used in the CSF is precise. So
we designed a bank of ﬁlters in the frequency domain by
convolving an isotropic (circular) Dirac delta function with an
ideal rectangular ﬁlter, as shown in Fig. 1b:

˜Bk(r) = δ(r − rk) ∗ rect(r),

(12)

where ˜Bk is Bk represented in the frequency domain, rk is the
center frequency of the kth subband, rect(r) is a rectangular
function with width parameter rb:

(cid:40)

rect(r) =

1, |r| < rb,
0, otherwise

The convolution in (12) can be easily computed as:
(cid:40)

˜Bk(r) =

1, |r − rk| < rb,
0, otherwise

(13)

(14)

B. Foveated Entropic Differencing

Let Cpqkr and Cpqkd be the b × b bandpass coefﬁcients of
the reference and distorted luminance image block (p, q) using
ﬁlter Bk respectively. The bandpass coefﬁcients are described
by the GSM model:

Cpqkr = ZpqkrUpqkr, Cpqkd = ZpqkdUpqdk,

(15)

where Zpqkr and Zpqkd are random scalar variables and
Upqkr ∼ N (0, KUkr and Umkd ∼ N (0, KUkd ) are Gaussian
random vectors. By passing the bandpass coefﬁcients through
a hypothetical neural noise model, we have:

C

(cid:48)

pqkr = Cpqkr + Wpqkr, C

(cid:48)

pqkd = Cpqkd + Wpqkd,

(16)

wIN ) and Wpqkd ∼ N (0, σ2

where Wpqkr ∼ N (0, σ2
wIN ).
Since bandpass coefﬁcients tends towards a multivariate Gaus-
sian distribution when conditioned on realizations of Zpqkr and
Zpqkd, conditional local entropies are computed as:

(cid:48)

h(C

pqkr|zpqkr) = const. +

(cid:48)

h(C

pqkd|zpqkd) = const. +

1
2

1
2

log[|z2

pqkrKUkr + σ2

wIN |] (17)

log[|z2

pqkdKUkd + σ2

wIN |] (18)

In [15], [17] and [16], the local entropies are scaled by scalar
factors γpqkr = log(1+s2
pqkd), here
instead we weight the local entropies using the foveation based
error sensitivity function. Speciﬁcally, let fk be the average

pqkr) and γpqkd = log(1+s2

where (i0, j0) is the gazing center. We ﬁrst normalize the
foveation-base error sensitivity function (Equation (1)) as:

SN

k (p, q) =

Sf (fk, e(p, q))
p,q Sf (fk, e(p, q))

(cid:80)

(20)

where we have assumed equal contribution of each subband.
Then the foveated entropic differencing map for subband k is
deﬁned as:

Dk(p, q) = SN

k (p, q) · (h(C

(cid:48)

pqkr|zpqkr) − h(C

(cid:48)

pqkd|zpqkd)),

(21)
and the ﬁnal quality prediction score is computed by taking
the summation of the absolute entropy differences:

F ED =

(cid:88)

k,p,q

|Dk(p, q)|

(22)

IV. EXPERIMENTS

A. Evaluation Framework

The algorithms were tested and compared on the LIVE-
FBT-FCVR databases [8], where subjective opinion scores
(MOS) and difference MOS (DMOS) were collected for 180
foveated / compressed images and 10 reference (190 total) 8K
immersive videos.

The foveation distortions in the subjective study of [8]
were created in real time. During the playback, the system
reads three immersive (equirectangular) videos, each video
uniformly pre-compressed with one of 5 QP levels, and
creates the foveation distortion by combining three videos with
descending quality order from the fovea to the periphery, given
gazing data obtained from the eye tracker.

To recover the foveation experience, we adopt a viewport-
based assessment framework as also mentioned in [7], where
for each immersive video we sample 18 viewports videos, and
set the resolution of each viewport to 1024x1024, and the FOV
to be 90◦. The viewports distribute uniformly on a sphere in
terms of longitude and latitude.

B. Parameter Settings

The display resolution d and the display Nyquist frequency

fd can then be computed as:

d ≈

πM v
180

= 8.94 (pixels/degree),

fd = 4.47 (cycles/degree)

(23)

(24)

We then divided the frequency band from d to 0 uniformly
into n subbands, and set n = 12. We set the block size b to 4
and the noise level σw to 0.1.

TABLE I
PERFORMANCE OF ALGORITHMS ON 2D AND 3D LIVE-FBT-FCVR
DATABASES. THE BEST PERFORMING ALGORITHM IS BOLDFACED.

IQA Models

The 2D database

The 3D database

Performances

PSNR [20]
SSIM [20]
MS-SSIM [21]
VIF [13]
S-RRED [16]
Speed-IQA [17]
FSIM [22]
FWQI [9]
FASSIM [11]
FovFilt∗
FED†

PLCC↑
0.6941
0.7260
0.7288
0.8102
0.7896
0.7760
0.7712
0.7906
0.7573
0.8484
0.8971

SROCC↑
0.6954
0.7191
0.7243
0.8068
0.7885
0.7866
0.7808
0.7848
0.7418
0.8358
0.8954

PLCC↑
0.4379
0.4184
0.5337
0.6536
0.5604
0.5084
0.5904
0.8041
0.7549
0.6000
0.8276

SROCC↑
0.4418
0.4429
0.5531
0.6765
0.5744
0.4959
0.6273
0.7841
0.7401
0.6224
0.8364

∗Foveated ﬁltering is simply applied before SpEED-IQA.
†The number of subband is set to 12.

C. Overall Performance

We compared FED with several leading existing IQA algo-
rithms on both the 2D and 3D LIVE-FBT-FCVR databases:
PSNR [20], SSIM [20], MS-SSIM [21], VIF [13], S-RRED
[16], SpeedIQA [17], FSIM [22], FWQI [9], and FA-SSIM
[11]. For each immersive video, each algorithm was evaluated
on every viewport video on a frame-by-frame basis, and the
scores obtained on each viewport frame were then averaged
across all frames and 18 viewport videos.

Pearson’s linear correlation coefﬁcient (PLCC), Spearman’s
rank order correlation coefﬁcient (SROCC), Kendall’s rank
order correlation coefﬁcient (KROCC), and root mean square
(RMSE) between the DMOS and algorithm predictions are
used as the criteria for evaluating the performance of the
algorithms. Before computing PLCC and RMSE, a four-
parameter logistic non-linearity was employed [23]:

Q(x) = β2 +

β1 − β2
1 + exp(− x−β3
|β4| )

(25)

For foveated QA, one intuitive solution would be ﬁrst apply
a foveated ﬁlter to both reference and distorted images, and
use traditional QA methods to evaluated the ﬁltered image
pairs. So we also included as a baseline algorithm the method
of using SpEED-IQA on foveated reference-distorted image
pairs. We used the same foveated ﬁltering method as in [24].
The results are shown in TABLE I. The proposed FED
model outperformed existing FR models, foveated and non-
foveated, by a large margin on both 2D and 3D databases.
In addition, the FED model also outperformed the intuitive
foveated ﬁltering model (FovFilt), showing that it can effec-
tively capture foveation distortions.

D. Number of Subbands

We also studied the effects of using different numbers
(n = 6, 8, 10, 12) of subbands, as shown in TABLE II. It
can be seen that the performance is improved when using
more subbands, since frequency information is more precisely
represented, and foveation distortions are better captured. We

TABLE II
PERFORMANCE WHEN USING DIFFERENT NUMBERS OF SUBBANDS ON THE
2D LIVE-FBT-FCVR DATABASE. THE BEST PERFORMING ALGORITHM IS
BOLDFACED.

#Subbands

6
8
10
12

PLCC↑
0.8701
0.8860
0.8930
0.8971

Performances
SROCC↑ KROCC↑∗

0.8736
0.8823
0.8884
0.8954

0.6900
0.7030
0.7109
0.7196

RMSE↓
4.73
4.57
4.43
4.35

∗Kendall’s rank order correlation coefﬁcient.

TABLE III
PERFORMANCE WHEN USING DIFFERENT FILTERBANKS ON THE 2D
LIVE-FBT-FCVR DATABASE. THE BEST PERFORMING ALGORITHM IS
BOLDFACED.

Performances

Filterbanks

PLCC↑
0.7768
0.8399
0.8971

SROCC↑
0.7585
0.8302
0.8954

DoG
Triangular†
Rectangular†
∗Kendall’s rank order correlation coefﬁcient.
†The function is convolved by n Dirac delta function.

KROCC↑∗
0.5720
0.6432
0.7196

RMSE↓
6.18
5.35
4.35

also found that increasing the subband number over 12 leads
to limited performance gain but higher complexity.

E. Using Different Filterbanks

In addition to Dirac-convolved ”ideal” low-pass ﬁlterbanks,
we also tested DoG ﬁlterbanks, and a ﬁlterbank composed of
isotropic Dirac-convolved triangular functions in the frequency
domain. A rotationaly symmetric Dirac-convolved triangular
frequency band introduces less Gibbs phenomena than using
a rectangular frequency band, but also removes information
in each subband. A Dirac-convolved triangular function is
deﬁned by:

˜Bk(r) =

(cid:40) rb−|r−rk|

rb

, |r − rk| < rb,

0, otherwise

(26)

where r is deﬁned as in (11), and rk is the center frequency.
Their performances are compared in TABLE III. It can be seen
that, although there are severe spatial Gibbs phenomena when
using Dirac-convolved rectangular functions, the statistics of
the corresponding bandpass coefﬁcients are less affected when
capturing perceptual distortions. However, the results suggest
that, for foveated quality assessment, large frequency bands
(DoGs) and loss of information in each frequency band
(isotropic Dirac-convolved triangular functions) may cause
degraded performance.

V. CONCLUSION
We propose a foveated video quality model called FED that
combines NSS-based QA features with a foveation-based error
sensitivity function, based on the assumption that bandpass ﬁl-
tering restricts spatial entropy differences to capture perceptual
distortions within a given frequency subband. We found that
FED yields SOTA performance on the 2D and 3D LIVE-FBT-
FCVR databases.

[21] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural
similarity for image quality assessment,” in The Thrity-Seventh Asilomar
Conference on Signals, Systems Computers, 2003, 2003, vol. 2, pp.
1398–1402 Vol.2.

[22] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “Fsim: A feature similarity
IEEE Transactions on Image

index for image quality assessment,”
Processing, vol. 20, no. 8, pp. 2378–2386, 2011.

[23] VQEG,

“Final report from the video quality experts group on the
validation of objective quality metrics for video quality assessment,”
.

[24] C. Bradley, J. Abrams, and W. S. Geisler,

“Retina-v1 model of
detectability across the visual ﬁeld,” Journal of Vision, vol. 14, no.
12, pp. 1534–7362, 2014.

REFERENCES

[1] P. L. Silsbee, A. C. Bovik, and Dapang Chen, “Visual pattern image
sequence coding,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 3, no. 4, pp. 291–301, 1993.

[2] T. H. Reeves and J. A. Robinson,

“Adaptive foveation of mpeg
video,” in Proceedings of the Fourth ACM International Conference
on Multimedia, New York, NY, USA, 1997, MULTIMEDIA ’96, p.
231–241, Association for Computing Machinery.

[3] Philip Kortum and Wilson S. Geisler, “Implementation of a foveated
image coding system for image bandwidth reduction,” in Human Vision
and Electronic Imaging, Bernice E. Rogowitz and Jan P. Allebach, Eds.
International Society for Optics and Photonics, 1996, vol. 2657, pp. 350
– 360, SPIE.

[4] Wilson S. Geisler and Jeffrey S. Perry, “Real-time foveated multiresolu-
tion system for low-bandwidth video communication,” in Human Vision
and Electronic Imaging III, Bernice E. Rogowitz and Thrasyvoulos N.
Pappas, Eds. International Society for Optics and Photonics, 1998, vol.
3299, pp. 294 – 305, SPIE.

[5] Gazi Karam Illahi, Thomas Van Gemert, Matti Siekkinen, Enrico
Masala, Antti Oulasvirta, and Antti Yl¨a-J¨a¨aski, “Cloud gaming with
foveated video encoding,” ACM Trans. Multimedia Comput. Commun.
Appl., vol. 16, no. 1, Feb. 2020.

[6] Jihoon Ryoo, Kiwon Yun, Dimitris Samaras, Samir R. Das, and Gregory
Zelinsky, “Design and evaluation of a foveated video streaming service
for commodity client devices,” New York, NY, USA, 2016, MMSys
’16, Association for Computing Machinery.

[7] Yize Jin, Meixu Chen, Todd Goodall Bell, Zhaolin Wan, and Alan Bovik,
“Study of 2D foveated video quality in virtual reality,” in Applications
of Digital Image Processing XLIII. International Society for Optics and
Photonics, 2020, vol. 11510, pp. 18 – 26, SPIE.

[8] Yize Jin, Meixu Chen, Todd Goodall, Anjul Patney, and Alan C. Bovik,
“Subjective and objective quality assessment of 2d and 3d foveated video
compression in virtual reality,” IEEE Transactions on Image Processing,
submitted.

[9] Zhou Wang, Alan Conrad Bovik, Ligang Lu, and Jack L. Kouloheris,
in Applications of Digital
“Foveated wavelet image quality index,”
Image Processing XXIV, Andrew G. Tescher, Ed. International Society
for Optics and Photonics, 2001, vol. 4472, pp. 42 – 52, SPIE.

[10] A. B. Watson, G. Y. Yang, J. A. Solomon, and J. Villasenor, “Visibility
of wavelet quantization noise,” IEEE Transactions on Image Processing,
vol. 6, no. 8, pp. 1164–1175, 1997.

[11] S. Rimac-Drlje, G. Martinovi´c, and B. Zovko-Cihlar, “Foveation-based
content adaptive structural similarity index,” in 2011 18th International
Conference on Systems, Signals and Image Processing, 2011, pp. 1–4.
[12] H. R. Sheikh, A. C. Bovik, and G. de Veciana, “An information ﬁdelity
criterion for image quality assessment using natural scene statistics,”
IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2117–
2128, 2005.

[13] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,”
IEEE Transactions on Image Processing, vol. 15, no. 2, pp. 430–444,
2006.

[14] Martin J. Wainwright and Eero P. Simoncelli,

“Scale mixtures of
gaussians and the statistics of natural images,” Cambridge, MA, USA,
1999, NIPS’99, p. 855–861, MIT Press.

[15] R. Soundararajan and A. C. Bovik, “Rred indices: Reduced reference
entropic differencing for image quality assessment,” IEEE Transactions
on Image Processing, vol. 21, no. 2, pp. 517–526, 2012.

[16] R. Soundararajan and A. C. Bovik,

“Video quality assessment by
reduced reference spatio-temporal entropic differencing,” IEEE Trans-
actions on Circuits and Systems for Video Technology, vol. 23, no. 4,
pp. 684–694, 2013.

[17] C. G. Bampis, P. Gupta, R. Soundararajan, and A. C. Bovik, “Speed-
qa: Spatial efﬁcient entropic differencing for image and video quality,”
IEEE Signal Processing Letters, vol. 24, no. 9, pp. 1333–1337, 2017.

[18] D. L. Ruderman, “The statistics of natural images,” Netw. Comput.

Neural Syst., vol. 5, no. 4, pp. 517–548, 1994.

[19] R. Reininger and J. Gibson, “Distributions of the two-dimensional dct
coefﬁcients for images,” IEEE Transactions on Communications, vol.
31, no. 6, pp. 835–839, 1983.

[20] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

