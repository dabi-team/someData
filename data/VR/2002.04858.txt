Adaptive Task Partitioning at Local Device or
Remote Edge Server for Ofﬂoading in MEC

Jianhui Liu, Qi Zhang
DIGIT, Department of Engineering, Aarhus University, Denmark
Email: {jianhui.liu, qz}@eng.au.dk

0
2
0
2

b
e
F
2
1

]
I

N
.
s
c
[

1
v
8
5
8
4
0
.
2
0
0
2
:
v
i
X
r
a

Abstract—Mobile edge computing (MEC) is one of the promis-
ing solutions to process computational-intensive tasks for the
emerging time-critical Internet-of-Things (IoT) use cases, e.g.,
virtual reality (VR), augmented reality (AR), autonomous vehicle.
The latency can be reduced further, when a task is partitioned
and computed by multiple edge servers’ (ESs) collaboration.
However, the state-of-the-art work studies the MEC-enabled
ofﬂoading based on a static framework, which partitions tasks
at either the local user equipment (UE) or the primary ES. The
dynamic selection between the two ofﬂoading schemes has not
been well studied yet. In this paper, we investigate a dynamic
ofﬂoading framework in a multi-user scenario. Each UE can
decide who partitions a task according to the network status,
e.g., channel quality and allocated computation resource. Based
on the framework, we model the latency to complete a task,
and formulate an optimization problem to minimize the average
latency among UEs. The problem is solved by jointly optimizing
task partitioning and the allocation of the communication and
computation resources. The numerical results show that, com-
pared with the static ofﬂoading schemes, the proposed algorithm
achieves the lower latency in all tested scenarios. Moreover,
both mathematical derivation and simulation illustrate that the
wireless channel quality difference between a UE and different
ESs can be used as an important criterion to determine the right
scheme.

Index Terms—Mobile edge computing, IoT, time critical, com-

putation ofﬂoading, adaptive task partitioning.

I. INTRODUCTION

A variety of the emerging Internet-of-Things (IoT) use
cases, e.g., virtual reality (VR), augmented reality (AR),
autonomous vehicle, factory automation, and remote surgery
etc., require real-time control and steering of cyber physical
systems. These use cases often are challenged by processing
computation-intensive tasks within the latency constraint of
millisecond level [1], [2]. However, due to limit computation
resource, it is difﬁcult for user equipment (UE) to complete a
task within the latency constraint merely by local processing.
Mobile edge computing (MEC) is considered as a promising
paradigm to fulﬁll the stringent latency requirement [3], [4].
Compared with the core Cloud server, edge server (ES) can
provide computation capability in close proximity to UE.

In the MEC paradigm, computation ofﬂoading is one of the
key components, which has been studied in many literature.
Work in [5] and [6] focused on the optimal strategy to ofﬂoad
the entire task to one ES. Lyu et al. [5] jointly optimized
the ofﬂoading admission decisions and computation resource
allocation among users to guarantee delays and energy saving.
Liu et al. [6] maximized reliability of computation ofﬂoading

subject to latency constraint by dynamic selections of trans-
mission rates and ESs.

Although MEC has potential to enhance the computation
capability of UEs, the resource of one ES is limited, compared
to that of core Cloud server. However, considering the dense
deployment of the future network, it is promising to further
shorten the latency by collaboration of multi-ES. In other
words, one task can be partitioned into several sub-tasks and
computed by multiple ESs in parallel, which is known as task
partitioning ofﬂoading. In the state-of-the-art work, researchers
mainly studied the task partitioning scheme based on a static
framework, i.e., partitioning a task at either the local UE or the
associated ES. Work in [7]–[9] considered to partition a task
at UE, and subsequently ofﬂoad the sub-tasks to different ESs
via wireless channel. Dinh et al. [7] minimized both energy
consumption and tasks runtime by coupling sub-task allocation
decisions and CPU frequency scaling at UE. To make tradeoff
between latency and reliability of computation ofﬂoading,
Liu et al. [8] proposed heuristic algorithms to allocate and
sequentially ofﬂoad sub-tasks to different ESs. They further
improved the task model with the directed acyclic graph and
scheduled the sub-task ofﬂoading in [9]. Other work [10]–[13]
ofﬂoaded the entire task to a primary ES ﬁrst, at which the task
was subsequently partitioned and distributed to other ESs. Re-
gardless of the transmission latency from UEs to the primary
ES, Chiu et al. proposed algorithms to minimize the latency
to cooperatively complete a task by multiple ESs for single-
UE [10] and multi-UE [11] scenarios, respectively. Ren et al.
[13] studied the collaboration between Edge Computing and
Cloud Computing, and proposed an optimal task partitioning
strategy for latency minimization. Wang et al. [12] minimized
the latency and energy consumption to ofﬂoad a task to one
ES, and extended the proposed algorithm to multi-ES scenario
on the assumption that the primary ES would partition and
distribute sub-tasks to other ESs.

The local partitioning schemes in [7]–[9] ofﬂoad the sub-
tasks separately via the different wireless links between the
UE and the ESs, where a poor wireless link between the UE
and one of the ESs could affect the overall latency. The other
schemes in [10]–[13] select the primary ES which has the best
wireless link, and ofﬂoad the entire task to it. Then the primary
ES partitions the task and distributes the sub-tasks among mul-
tiple ESs via high quality links. However, partitioning at the
primary ES does not always outperform the local partitioning
schemes, as it takes two hops to transmit the task data to the

 
 
 
 
 
 
Primary ES

time

Secondary ES

Primary ES

time

Secondary ES

1UE

Primary ES

.
.
.

Primary ES

iUE

Forwarding link

(a) DoU scheme

1UE

.
.
.

iUE

.
.
.

MUE

iUE

.
.
.

MUE

Secondary ES

Secondary ES

(a) Task partitioning decision on UE

(b) Task partitioning decision on ES

Fig. 1. Two static ofﬂoading frameworks of task partitioning.

ﬁnal computing ESs. To minimize the overall latency, in this
paper, an adaptive ofﬂoading framework is proposed in multi-
user scenario to select the optimal scheme to process a task in
MEC system, i.e., partitioning the task at either the local UE or
the primary ES. The key factors that determine the selection,
e.g., quality of wireless links and computation resource at
ESs, are studied in different scenarios. Based on the proposed
framework, the latency to complete a task is minimized by
optimizing task partitioning and allocating communication and
computation resources among UEs.

The rest of the paper is organized as follows. Section II de-
scribes the system model and problem formulation. In Section
III, we investigate the optimal task partitioning and resource
allocation strategy. Sections IV presents the numerical results.
Finally, Section V concludes the paper.

II. SYSTEM MODEL AND PROBLEM FORMULATION

In this section, the system model is introduced ﬁrst, fol-
lowed by latency analysis on the two static ofﬂoading frame-
works. The optimization problem is formulated in the end.

As illustrated in Fig. 1, we consider an MEC system with
collaboration of two ESs. One of the ESs is regarded as the
primary ES which is responsible for UE association, control
signal exchange and resource scheduling before computation
ofﬂoading starts, while the other is the secondary ES. Note
that the secondary ES is set to share the computation burden
of the primary ES and shorten the latency further. The com-
putation resource of the primary and secondary ESs are f p
and f s, respectively. The primary ES is associated with M
UEs denoted by a set M, where M = {1, 2, · · · , M }. The
computation task from UE i is denoted by a tuple (bi, αi),
where bi is the input data size of the task (in bits) and αi
denotes the required number of CPU cycles for one data bit (in
cycles/bit). Each task can be computed cooperatively by the
two ESs. Since one computation task in some applications,
such as video compression and speech recognition, can be
partitioned into independent segments [3], [13], we use λi to
denote the proportion of the sub-task that is assigned from UE
i to the primary ES (λi ∈ [0, 1]). In other word, the primary
and secondary ES require to execute sub-tasks of λiαibi and
(1 − λi)αibi CPU cycles for UE i, respectively.

iUE

(b) DoE scheme

Trans. on 
wireless link

Trans. on 
forwarding link

Comp. at 
ESs

Fig. 2. Latency illustration for the two static ofﬂoading schemes of task
partitioning.

P

i∈M f s

i∈M f p

i and f s

The computation and communication resources are shared
by UEs in the MEC system. We assume that f p
i denote
the allocated computation resource of the primary and sec-
i ≤ f p and
ondary ESs to UE i, respectively, where
i ≤ f s. UEs access ESs with orthogonal frequency-
division multiplexing access (OFDMA) scheme on wireless
P
channel. The total number of resource blocks (RBs) in the
MEC system is Nrb. Given that UE i is assigned with ni
RBs, we have
i∈M ni ≤ Nrb. The available transmission
rates (in bits per second) of a RB from UE i to the primary
and secondary ESs can be adjusted based on channel state
information (CSI), which are denoted by rp
i , respec-
tively. In addition, as UEs normally prefer to be associated
with a closer ES, we assume rp
i . The task can be also
distributed through a forwarding link from the primary ES to
the secondary ES as shown in Fig. 1(b). The forwarding link
normally has higher bandwidth than wireless channel. Here
we denote the average transmission rate of the link as Ri
for UE i. Due to the fact that the size of a task’s output is
normally much smaller than the input one, the latency caused
by downlink transmission is negligible [7]–[9].

i and rs

i ≥ rs

P

A. Latency Analysis

To ofﬂoad computation tasks to the two ESs, each UE has
two different options to partition a task: either at the local
UE or at the primary ES, as shown in Fig. 1. The latency
illustration of the two schemes is compared in Fig. 2 and
analyzed as follows.

1) Make Task Partitioning Decision on UE (DoU): In this
scheme, a UE partitions the task into two sub-tasks locally, and
subsequently transmits sub-tasks to ESs via wireless channel.
In this paper, we consider to transmit sub-tasks sequentially,
which is more efﬁcient than ofﬂoading in parallel proofed by
[8]. Speciﬁcally, UE i ofﬂoads the sub-task to the primary
ES ﬁrst, using all the allocated RBs, where the transmission
latency is λibi
. It will start the ofﬂoading to the secondary ES
nirp
i
after completing the previous transmission. Considering both
communication and computation delay, the latency to complete
the corresponding sub-task at the primary and secondary ES

+ (1−λi)bi
is λibi
, respectively.
nirp
nirs
i
i
Therefore, the overall latency to complete the task of UE i is

+ (1−λi)αibi
f s
i

and λibi
nirp
i

+ λiαibi
f p
i

T u
i =

λibi
nirp
i

+max

λiαibi
f p
i

,

(1 − λi)bi
nirs
i

(cid:26)

+

(1 − λi)αibi
f s
i

(cid:27)

. (1)

2) Make Task Partitioning Decision on ES (DoE): In this
scheme, a UE ofﬂoads the entire task to the primary ES ﬁrst
bi
via wireless channel, of which the transmission delay is
.
nirp
i
Subsequently, the primary ES partitions the task and starts to
process one part of the task. Meanwhile, the other part of the
task is distributed to the secondary ES via the forwarding link.
The latency to process sub-task at the primary ES is λiαibi
,
f p
i
while the latency introduced by the secondary ES is (1−λi)bi
(1−λi)αibi
f s
i

+
. Therefore, the overall latency to complete the task

Ri

of UE i is obtained as

+max

T e
i =

bi
nirp
i

λiαibi
f p
i
B. Problem Formulation

(cid:26)

,

(1 − λi)bi
Ri

+

(1 − λi)αibi
f s
i

(cid:27)

. (2)

According to (1) and (2), the latency to complete a task de-
pends on the available resource, e.g., communication data rate
and computation resource, for both DoU and DoE schemes.
Each UE should select the scheme properly to minimize the
overall latency Ti, i.e., T ∗
i }. Moreover, due
to the constrained communication and computation resources
in the MEC system, the allocation of RBs and computation
resource at ESs has to be optimized among multiple UEs si-
multaneously. Therefore, we formulate a latency minimization
problem as

i = min{T u

i , T e

P1:

min
x,λ,n,f p,f s

1
M Xi∈M

βi [xiT u

i + (1 − xi)T e
i ]

s.t.

ni ≤ Nrb, ni > 0,

Xi∈M

Xi∈M

i ≤ f p, f p
f p

i > 0,

i ≤ f s, f s
f s

i > 0,

Xi∈M
xi ∈ {0, 1}, λi ∈ [0, 1], ∀i ∈ M,

(3a)

(3b)

(3c)

(3d)

P

where βi is a positive weight depending on the service priority
i∈M βi = 1. x, λ, n, f p and f s denote
of the UE i and
the sets of scheme selection indicators, task partitioning ratios,
assigned number of RBs and allocated computation resource
for UEs in the MEC system, i.e., x = {x1, · · · , xM }, λ =
{λ1, · · · , λM }, n = {n1, · · · , nM }, f p = {f p
M } and
f s = {f s
N }, respectively. If UE i chooses the DoU
scheme, we have xi = 1, otherwise, xi = 0. The constraints
(3a)-(3c) present the limit of communication and computation
resources. Due to the expression of the objective function, the
problem P1 is a non-convex optimization problem.

1 , · · · , f p

1 , · · · , f s

III. DYNAMIC TASK PARTITIONING SCHEME

In this section, a dynamic task partitioning scheme is
proposed. We ﬁrst analyze the optimal task partitioning strat-

egy, then optimally allocate communication and computation
resources to multiple UEs.

A. Optimal Selection of Task Partitioning

As the objective function in problem P1 is too complicated
to be solved directly, we ﬁrst derive the optimal task allocation
for both DoU and DoE schemes, i.e., λu∗
i , respectively,
given the assigned number of RBs, ni, and computation
resource, f p

i and λe∗

i and f s
i .

A task is completed cooperatively by the primary and
secondary ESs. Intuitively, the latency is minimized when
the two ESs can complete the allocated sub-tasks simulta-
neously [8]. For instance, according to (1), the optimal task
partitioning ratio for DoU scheme can be obtained, when
λiαibi
f p
i

= (1−λi)bi
nirs
i

, i.e.,

+ (1−λi)αibi
f s
i
1/(nirs

λu∗
i =

i ) + αi/f s
i

1/(nirs

i ) + αi/f s

i + αi/f p

i

.

(4)

Similarly, according to (2), the optimal task partitioning ratio
for DoE scheme is

λe∗
i =

1/Ri + αi/f s
i

1/Ri + αi/f s

i + αi/f p

i

.

(5)

Substitute (4) and (5) into (1) and (2), respectively, the minimal
latency to complete a task for UE i using DoU and DoE
schemes is obtained as

,

+

λu∗
i

T u∗
i

(λu∗

i ) = bi (cid:18)

1
nirp
i
1
nirp
i
To select the optimal scheme between the DoU and DoE
i = 0. Therefore, the optimal strategy

schemes, we let T e∗
to partition a task for UE i is obtained as follows.

αi
f p
i (cid:19)
αi
f p
i

i ) = bi (cid:18)

i −T u∗

λe∗
i (cid:19)

(λe∗

T e∗
i

(7)

(6)

+

.

i and f s

Theorem 1 (Task Partitioning Decision on UE or ES): Given
ni, f p
i , UE i can select task partitioning scheme
between the DoU and DoE based on the following conditions:
− 1 ≥ 0, the DoU scheme is selected, i.e.,

+ ηi

− 1 < 0, the DoE scheme is selected, i.e.,

• if nirs
i
Ri
xi = 1;
• if nirs
i
Ri
xi = 0,
where ηi = f p

+ ηi

rs
i
rp
i

rs
i
rp
i
+ f p
i
f s
i

i
αiRi

i /rp

i , nirs

i /Ri, and f p

+ 1 and ηi ≥ 1.
Note that, as mentioned in the Section II, we have 0 <
i /rp
rs
i ≤ 1. In addition, the link capacity between ESs is nor-
mally larger than that on wireless channel, i.e., nirs
i /Ri ≤ 1.
Therefore, Theorem 1 can be illustrated by Fig. 3. It reveals
that the optimal task partitioning scheme mainly depends on
i /f s
the rs
i , i.e., the ratio of wireless
link quality between the two ESs, the ratio of transmission
rate between the wireless channel and the forwarding link,
and the ratio of computation resource between the primary
and secondary ESs. If rs
is larger than 1/ηi, UE i is
i /rp
preferable to partition a task locally. When rs
i < 1/ηi, the
selection between the DoE and DoU schemes mainly depends
on nirs
leads to the smaller
1/ηi, which means the DoU scheme could outperform DoE
scheme with higher probability.

i /Ri. Moreover, the higher f p

i /f s
i

i /rp

i

1

0

DoU

DoE

1

Fig. 3.

Illustration of optimal selection between DoU and DoE schemes.

B. Joint Resource Allocation

So far, we have obtained the optimal
i and f s

task partitioning
strategy for UE i when ni, f p
i are given. In this
subsection, we will study the allocation of the communication
and computation resource among UEs, i.e., n, f p and f s.

i , we have T ∗

Denoting the optimal latency to complete the task of UE i
as T ∗
i }. Based on the objec-
i
tive function in P1 and Theorem 1, the latency of dynamic
ofﬂoading for UE i could be estimated by

i = min{T u∗

, T e∗

˜Ti =

Su
i
i + Se
Su
i

T u∗
i +

Se
i
i + Se
Su
i

T e∗
i

,

(8)

where Su and Se denote the size of trapezoids representing the
selection of DoU and DoE schemes in Fig. 3, respectively. We
2 − nirs
.
have
Su
Therefore, the P1 can be transformed into

= 1 − Se
Su

/(2ηi) and

Se
i
i +Se
i

Su
i
i +Se
i

Ri (cid:17)

i
i +Se
i

=

Su

(cid:16)

i

P2: min

n,f p,f s

s.t.

βi ˜Ti

1
M Xi∈M
(3a) − (3d).

(9a)

Relaxing n from integer space to real number space, the
optimal n, f p and f s in P2 can be searched by the interior-
point algorithm (IPA). Based on the allocation results, the
optimal task partitioning scheme can be selected according
to Theorem 1.

C. Compatibility with Generic Scenarios

The proposed dynamic task partitioning scheme can be
further extended to more general scenarios. We assume the
number of secondary ESs is N . When N > 1, the optimal
task partitioning ratios for the DoU and DoE schemes are

λu∗
i =

λu
i,s

λe∗
i =

i,1) + αi/f s
i,1
+αi/f p
(cid:3)
i

1/(nirs
λu
i,s
(cid:2)
1/(nirs
(cid:2)
λe
i,s
1/Ri + αi/f s
i,1
(cid:0)

i,1)+αi/f s
i,1
(cid:3)
1/Ri + αi/f s
i,1
+ αi/f p
(cid:1)
(cid:0)
i

,

λe
i,s
i,j and f s
i,j are the assigned RBs and computation
i,s and

where rs
resource to the secondary ES j (j ≤ N ), respectively. λu

(cid:1)

,

(10)

(11)

TABLE I
DEFAULT SIMULATION PARAMETERS

Parameter

Wireless channel bandwidth
The number of resource blocks Nrb
Received SNR at ESs
Computing capacity at ESs f p, f s
Forwarding link capacity between ESs Ri
Task data size bi
Computation intensity αi

Value

20 MHz
100
[0, 30] dB
[2, 3] × 1010 cycles/s
[50, 100] Mbps
[0.8, 1.2] Mbits
248 cycles/bit

λe
i,s are presented as

1+

1+

N
j=2
(cid:18)
(10) and (11), f s

P

Qj

i,k

−1

P

Qj−1

i,k) (cid:19)

(cid:18)
k=1 αi/f s
k=2(1/Ri+αi/f s
i and rs
λe
i,s
(cid:2)
λu
i,s
h

1/Ri + αi/f s
i,1
(cid:1)
(cid:0)
i,1 + niαi/f s
1/rs
i,1
(cid:0)

(cid:1)

˜f s
i = αi

˜rs
i =

N
j=2

k=1 αi/f s

Qj−1
k=2[1/(nirs

i,k
i,k)+αi/f s

Qj

i,k] (cid:19)

−1

,

,

respectively. Based on

i in (4) and (5) can be replaced by

−1

− 1/Ri

(cid:3)

− niαi/ ˜f s
i

,
−1

.

(12)

(13)

i

In this way, the optimal scheme selection and resource allo-
cation can be obtained following Section III-A and III-B.

IV. NUMERICAL RESULTS

In this section, we present the simulation results to study
the performance of the proposed algorithms. The bandwidth
of wireless channel is 20 MHz with 100 RBs in the MEC
system. The signal-to-noise ratio (SNR) between UEs and ESs
is obtained by a uniform distribution, ranging from 0 dB to
30 dB. Based on different SNRs, UEs dynamically adjust the
modulation and coding scheme (MCS) to guarantee a constant
BLER of 10−3. The SNR-MCS mapping is achieved by the
link abstraction model proposed by [14]. Various MCSs are
considered combining different modulations (QPSK, 16QAM,
64QAM) and varying coding rates ranging from 1/9 to 9/10.
The computation resource of ESs ranges from 2×1010 cycles/s
to 3×1010 cycles/s [13]. The forwarding link capacity between
the two ESs follows a uniform distribution with Ri ∈ [50, 100]
Mbps. The task data size of UEs ranges from 0.8 Mbits
to 1.2 Mbits (e.g. a VR-video frame of 4K resolution is
4096 × 2160 pixels with 24 bit/pixel, using a compression
ratio of 300:1 [15]). The computation intensity of tasks α is set
to 238 cycles/bit [16]. The default simulation parameters are
summarized in Table I. The numerical results in this section are
based on an average value over 5000 Monte Carlo simulations.
We present the average gains of latency from the proposed
algorithm to the static DoU and DoE schemes, i.e., 1 − T Prop.
T DoU
and 1 − T Prop.
T DoE , respectively. In addition, the DoU and DoE
schemes mean all the UEs in the system partition task lo-
cally and remotely, respectively. To illustrate the necessity of
the proposed scheme, four different scenarios are studied as
follows.

A. High Forwarding Link Capacity

In this scenario, we consider the capacity of the forwarding
i.e.,

link is much higher than that of the wireless link,

l

m
h
t
i
r
o
g
a
d
e
s
o
p
o
r
p
m
o
r
f

y
c
n
e
t
a

l

f
o

i

n
a
g
e
g
a
r
e
v
A

0.1

0.08

0.06

0.04

0.02

0

-0.02

0.2

0.3

Gain to DoU, 3 UEs
Gain to DoE, 3 UEs
Gain to DoU, 8 UEs
Gain to DoE, 8 UEs

0.4

0.5

0.8
0.6
Average wireless link quality ratio rs
i

0.7

/rp
i

m
h
t
i
r
o
g
a

l

d
e
s
o
p
o
r
p
m
o
r
f

y
c
n
e
t
a

l

f
o

i

n
a
g

e
g
a
r
e
v
A

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

-0.02

0.2

0.3

0.9

1

Gain to DoU, fp/fs=2
Gain to DoE, fp/fs=2
Gain to DoU, fp/fs=1
Gain to DoE, fp/fs=1
Gain to DoU, fp/fs=0.5
Gain to DoE, fp/fs=0.5

0.4

0.5

0.8
0.6
Average wireless link quality ratio rs
i

0.7

0.9

1

/rp
i

Fig. 4. Impact of the number of UEs in high forwarding link capacity scenario.

Fig. 5.

Impact of computation resource ratio between ESs on latency.

i /rp

Ri ≫ Nrbrp
i . The data rate on the forwarding link Ri is set
to 1 Gbps. Fig. 4 shows the latency comparison of different
ofﬂoading schemes with varying average ratio of wireless
link quality between the two ESs, rs
i . The curves in the
ﬁgure illustrates the average gain of latency from the proposed
algorithm to the other two schemes. Since the average latency
gain is higher than zero in different settings, the proposed
algorithm outperforms both DoU and DoE schemes. Com-
pared with the DoU scheme, the DoE scheme achieves lower
latency if the average rs
is smaller than 0.5. However,
with rs
increasing, the latency of two schemes goes in
opposite directions. We call the point that two schemes obtain
the same latency as the inﬂection point, which approximates
to rs
i = 0.5 in this ﬁgure. The inﬂection point refers
to a selection indicator of the proposed dynamic ofﬂoading
algorithm. The inﬂection point with smaller rs
implies
DoU scheme has a higher probability to outperform DoE
scheme. Moreover, the number of UEs in the MEC system
have little effect on the average latency.

i /rp

i /rp

i /rp

i /rp

i

i

i

i /rp

In Fig. 5, we investigate the gains of the proposed algorithm
with varying average f p/f s. The number of UEs here is set
to 8. If rs
i is larger than 0.7, it is preferred to partition task
at local UE, regardless of f p/f s. Moreover, with the average
f p/f s decreasing, the average rs
i of the inﬂection points
increases. The reason lies in that more proportion of a task
tends to be ofﬂoaded to the secondary ES, if the computation
resource of the primary ES reduces.

i /rp

B. Limited Computing Capacity at the Primary ES

This scenario illustrates the impact on the latency to com-
plete a task if the computation resource at the primary ES is
much lower than that at the secondary ES. Here f p/f s is set
to 0.2. Fig. 6 shows that the proposed algorithm obtains the
optimal latency. The gain of the proposed algorithm to the
DoU scheme increases with the number of UEs increasing,
while the gain to the DoE scheme shows the opposite tendency.
Moreover, if the number of UEs grows, more task data will be
ofﬂoaded to the secondary ES, of which channel quality has
more effect on the optimal scheme selection. As a result, the

m
h
t
i
r
o
g
a

l

d
e
s
o
p
o
r
p
m
o
r
f

y
c
n
e
t
a

l

f
o

i

n
a
g

e
g
a
r
e
v
A

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

-0.02

0.2

0.3

Gain to DoU, 3 UEs
Gain to DoE, 3 UEs
Gain to DoU, 8 UEs
Gain to DoE, 8 UEs

0.4

0.5

0.8
0.6
Average wireless link quality ratio rs
i

0.7

0.9

1

/rp
i

Fig. 6. Latency comparisons if computation resource of primary ES is limited.

inﬂection point with 8 UEs is at the right side of that with 3
UEs.

C. Communication Latency Dominated

The communication latency dominated scenario means the
latency to transmit a task on wireless channel is much larger
to compute the task at ESs. It happens when
than that
the communication resource is limited. In this scenario, we
assume the computation intensity of a task is 45 cycles/bit
(e.g., application of gzip compression [16]), which leads to
the average ratio of the wireless transmission time to the
computation time is around 6:1. In Fig. 7, it shows the average
gain of latency from the proposed algorithm to the other two
schemes in different network settings. We observe that, if the
number of UE is 8, the average rs
i of the inﬂection points
is close to 0.3. When there are 3 UEs in the MEC system,
the DoU scheme will be the better choice regardless of the
wireless channel quality ratio. It is because the DoE scheme
has to transmit the whole task to ES ﬁrst via wireless channel.
However, it is also worth noticing that, compared with the
results in Fig 4 and Fig. 6, the maximal gains of the proposed
algorithm here is smaller, which is less than 5% for the case

i /rp

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
m
h
t
i
r
o
g
a

l

d
e
s
o
p
o
r
p
m
o
r
f

y
c
n
e
t
a

l

f
o

i

n
a
g

e
g
a
r
e
v
A

0.05

0.04

0.03

0.02

0.01

0

-0.01

0.2

0.3

Gain to DoU, 3 UEs
Gain to DoE, 3 UEs
Gain to DoU, 8 UEs
Gain to DoE, 8 UEs

0.5

0.4

0.8
0.6
Average wireless link quality ratio rs
i

0.7

0.9

1

/rp
i

Fig. 7. Latency comparisons if communication latency is dominated.

m
h

t
i
r
o
g
a

l

d
e
s
o
p
o
r
p
m
o
r
f

y
c
n
e
t
a

l

i

f
o
n
a
g
e
g
a
r
e
v
A

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

-0.02

0.2

0.3

Gain to DoU, 3 UEs
Gain to DoE, 3 UEs
Gain to DoU, 8 UEs
Gain to DoE, 8 UEs

0.4

0.5

0.8
0.6
Average wireless link quality ratio rs
i

0.7

0.9

1

/rp
i

Fig. 8. Latency comparisons if computation latency is dominated.

i = 1. It means that DoU and DoE have approximate

i /rp
rs
latency performance in this scenario.

D. Computation Latency Dominated

In this scenario, the latency to compute the task dominates
in the overall latency. We consider the computation resource
of each ES follows a uniform distribution ranging from 6×109
cycles/s to 10 × 109 cycles/s, where the average latency ratio
of the communication to the computation is about 1:3. In Fig.
8 the inﬂection points of the cases with different UEs are close
to each other. The DoU schemes will achieve lower latency if
i /rp
rs
i > 0.5, whereas it is preferable to partition task at the
primary ES if rs

i /rp

i < 0.45.

V. CONCLUSION

In this paper, we study the optimal ofﬂoading scheme of task
partitioning for time-critical services in the MEC system. We
propose an algorithm to dynamically select the schemes be-
tween DoU and DoE for latency minimization. It optimizes the
task partitioning, as well as the allocation of communication
and computation resource in multi-UE scenario. The numerical
results show that, compared with the static DoU and DoE

i /rp

schemes, the proposed algorithm achieves lower latency in
various scenarios. It is applicable to the design of the dynamic
ofﬂoading framework in MEC system. The ratio of wireless
channel quality between ESs, i.e., rs
i , can be used as a key
criterion to determine the selection of task partitioning. It is
preferred to partition a task at the primary ES if rs
i ≪ 1.
With rs
increasing, the latency of DoE scheme gradually
exceeds that of DoU scheme. The other factors, e.g., the ratio
of computation resource between ESs, as well as the number
of UEs, mainly affect the inﬂection point that indicates the
conversion of the better selection at certain rs

i /rp

i /rp

i

i /rp
i .

REFERENCES

[1] Q. Zhang and F. H. Fitzek, “Mission critical iot communication in 5g,”
in Future Access Enablers of Ubiquitous and Intelligent Infrastructures.
Springer, 2015, pp. 35–41.

[2] Q. Zhang, J. Liu, and G. Zhao, “Towards 5g enabled tactile robotic

telesurgery,” arXiv preprint arXiv:1803.03586, 2018.

[3] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey
on mobile edge computing: The communication perspective,” IEEE
Communications Surveys & Tutorials, vol. 19, no. 4, pp. 2322–2358,
2017.

[4] M. S. Elbamby, C. Perfecto, C.-F. Liu, J. Park, S. Samarakoon, X. Chen,
and M. Bennis, “Wireless edge computing with latency and reliability
guarantees,” arXiv preprint arXiv:1905.05316, 2019.

[5] X. Lyu, H. Tian, W. Ni, Y. Zhang, P. Zhang, and R. P. Liu, “Energy-
efﬁcient admission of delay-sensitive tasks for mobile edge computing,”
IEEE Transactions on Communications, vol. 66, no. 6, pp. 2603–2616,
2018.

[6] J. Liu and Q. Zhang, “Edge computing enabled mobile augmented reality
with imperfect channel knowledge,” in IEEE European Wireless, 2019,
pp. 1–6.

[7] T. Q. Dinh, J. Tang, Q. D. La, and T. Q. Quek, “Ofﬂoading in mobile
edge computing: Task allocation and computational frequency scaling,”
IEEE Transactions on Communications, vol. 65, no. 8, pp. 3571–3584,
2017.

[8] J. Liu and Q. Zhang, “Ofﬂoading schemes in mobile edge computing
for ultra-reliable low latency communications,” IEEE Access, vol. 6, pp.
2169–3536, 2018.

[9] ——, “Code-partitioning ofﬂoading schemes in mobile edge computing
for augmented reality,” IEEE Access, vol. 7, pp. 11 222 – 11 236, 2019.
[10] T.-C. Chiu, W.-H. Chung, A.-C. Pang, Y.-J. Yu, and P.-H. Yen, “Ultra-
low latency service provision in 5g fog-radio access networks,” in 2016
IEEE 27th Annual International Symposium on Personal, Indoor, and
Mobile Radio Communications (PIMRC).

IEEE, 2016, pp. 1–6.

[11] T.-C. Chiu, A.-C. Pang, W.-H. Chung, and J. Zhang, “Latency-driven fog
cooperation approach in fog radio access networks,” IEEE Transactions
on Services Computing, 2018.

[12] Y. Wang, M. Sheng, X. Wang, L. Wang, and J. Li, “Mobile-edge com-
puting: Partial computation ofﬂoading using dynamic voltage scaling,”
IEEE Transactions on Communications, vol. 64, no. 10, pp. 4268–4282,
2016.

[13] J. Ren, G. Yu, Y. He, and Y. Li, “Collaborative cloud and edge computing
for latency minimization,” IEEE Transactions on Vehicular Technology,
2019.

[14] M. Mezzavilla, M. Miozzo, M. Rossi, N. Baldo, and M. Zorzi, “A
lightweight and accurate link abstraction model for the simulation of
lte networks in ns-3,” in Proceedings of the 15th ACM international
conference on Modeling, analysis and simulation of wireless and mobile
systems. ACM, 2012, pp. 55–60.

[15] E. Bastug, M. Bennis, M. Médard, and M. Debbah, “Toward intercon-
nected virtual reality: Opportunities, challenges, and enablers,” IEEE
Communications Magazine, vol. 55, no. 6, pp. 110–117, 2017.

[16] A. P. Miettinen and J. K. Nurminen, “Energy efﬁciency of mobile clients

in cloud computing.” HotCloud, vol. 10, pp. 4–4, 2010.

 
 
 
 
 
 
 
 
 
 
 
 
