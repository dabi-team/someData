6
1
0
2

r
p
A
1
2

]

V
C
.
s
c
[

1
v
5
9
1
6
0
.
4
0
6
1
:
v
i
X
r
a

Articulated Hand Pose Estimation Review

Emad Barsoum
Columbia University
eb2871@columbia.edu

Abstract

With the increase number of companies focusing on commercial-
izing Augmented Reality (AR), Virtual Reality (VR) and wearable
devices, the need for a hand based input mechanism is becoming essen-
tial in order to make the experience natural, seamless and immersive.
Hand pose estimation has progressed drastically in recent years due to
the introduction of commodity depth cameras.

Hand pose estimation based on vision is still a challenging problem
due to its complexity from self-occlusion (between ﬁngers), close simi-
larity between ﬁngers, dexterity of the hands, speed of the pose and the
high dimension of the hand kinematic parameters. Articulated hand
pose estimation is still an open problem and under intensive research
from both academia and industry.

The 2 approaches used for hand pose estimation are: discriminative
and generative. Generative approach is a model based that tries to ﬁt
a hand model to the observed data. Discriminative approach is ap-
pearance based, usually implemented with machine learning (ML) and
require a large amount of training data. Recent hand pose estimation
uses hybrid approach by combining both discriminative and generative
methods into a single hand pipeline.

In this paper, we focus on reviewing recent progress of hand pose
estimation from depth sensor. We will survey discriminative methods,
generative methods and hybrid methods. This paper is not a compre-
hensive review of all hand pose estimation techniques, it is a subset of
some of the recent state-of-the-art techniques.

Keywords: Hand pose estimation; Hand tracking; VI; HCI; NUI

1

Introduction

Hand pose estimation and gesture recognition provide a natural input mech-
anism for Human Computer Interaction (HCI) especially in the area of AR

1

 
 
 
 
 
 
and VR. Nevertheless, they are also important scenarios in which the user
cannot touch the computing device such as medical doctor during operation,
or someone is eating and want to change the playing song or change the read-
ing page (dirty hand scenario), or 10 experience (TV experience) in which
you are far away from the screen, or during a presentation in which you want
to change the current slide or highlight a section during the presentation,
and many more.

Some of the non-vision methods used for hand tracking are using gloves
with sensors. The main advantage of using gloves is its accuracy and perfor-
mance compared to vision based state-of-the-art methods, especially in case
of heavy occlusion; however, the gap is getting closer. The problems of using
gloves are that they are costly, require calibration and not the most natural
way for the user to wear yet another device. An advantage of wearing gloves
beside accuracy is that they can provide a force feedback.

Other vision based hand tracking uses marker in the hand for easy seg-
mentation and part detection, marker usually can be a colored gloves [1] or
painted hand [2, 3] (used oﬄine to capture ground truth) or any other form
of marker. The advantage of this approach is that it solves a lot of the chal-
lenges of marker-less hand pose estimation, the marker can be pretty cheap,
depend on the marker it might not need calibration and the vision algorithm
for the marker usually fast and less complicated than the marker-less pose
estimation. However, the main issue is that using marker is not natural or
as accurate as glove methods. One big advantage of using marker, is that it
can be used to create labeled training data [2, 3] for the marker-less training
algorithm, which as we see later is the most time consuming part and labor
intensive.

There are a lot of literature on hand gesture recognition [4, 5] and hand
pose estimation [6, 7, 8, 9, 10, 2, 3, 11, 12]; for hand pose estimation, it seems
that recently most literature converged on high level architecture of what the
hand pose pipeline should look like. And the focus is to improve the diﬀerent
algorithms in each of the pipeline stages. Also, due to Microsoft Kinect and
the introduction of a relatively cheap depth sensor, most papers now focus
on RGB-D data from depth sensor to estimate hand pose. Nonetheless,
there are some papers that tried to estimate 3D hand pose from 2D image
with the help of a hand model or depth estimate of the hand(Fanello14).
The problem in extracting hand pose from 2D image is that it is many to
one mapping, which mean you can have 2 diﬀerent 3D hand poses projected
to the same 2D pose using temporal might help in this case by providing
some context.

As for hand gesture recognition, gross hand gestures has a lot of attention

2

both from academia and industries because it is relatively easier than hand
pose estimation and more suitable for user interface (UI) interaction (i.e.
Hand gesture in Xbox one, Samsung TVetc). For UI controls, we just need
discrete set of gestures such as grab, pick, hand open, hand closeetc, and
motion between any 2 endpoints for scrolling. Hand gestures also might be
beneﬁcial to speed up hand pose estimation, for example if we know the
current gesture we can reduce the search space on some of the algorithms
that uses model or search based techniques.

There are a third type that can be consider a subset of hand pose esti-
mation or super-set of hand gesture, which is partial hand pose estimation
[13, 4]. The idea here is that for some applications, there is no need to
the full hand pose estimation, knowing the ﬁngers location and tracking the
hand is enough to provide pointing, zooming and other dynamic gestures
for the application.

Gesture recognition and partial hand pose estimation are usually inferred
from the observed data directly, there are heuristics, image based search and
machine learning techniques for hand gesture recognition. As for hand pose
estimation, there are 3 types of pipeline used: appearance based approach
similar to hand gesture which is a regression problem [10, 2, 11, 12], model
based ﬁtting which tries to ﬁt hand model to the observed data [6, 7] and
a hybrid approach which uses both techniques in a single pipeline [9, 3].
Inferred from observed data directly is called discriminative approach and
ﬁtting a model is called generative approach.

The focus of this paper is on hand pose estimation from depth data. I
will discuss some of the hand pose estimation from 2D images and some of
the hand gesture techniques, but the in-depth discussion and analysis will
focus on articulated hand pose estimation from depth data. Pretty much
estimating the 20+ degree of freedom (DOF) parameters of the human hand.

1.1 Related works

To the best of my knowledge, the latest comprehensive review work on hand
pose estimation and hand gesture was published in 2007 [13], the focus of
[13] paper was primarily vision based hand pose estimation.

In [13], they divided pose estimation into two categories:

1. Appearance based: inferring the hand gesture or pose directly from the
observed visual data without the help of a model. This usually im-
plemented using machine learning (ML) which require a large training

3

data or with Inverse Kinematic (IK) or a hybrid approach.

2. Model based: this approach generate multiple hand model usually
called hypothesis and it tries to ﬁnd the model that best ﬁt the ob-
served data. Pretty much it convert the problem into a high dimension
optimization problem, ﬁnding the model that minimizing a certain cost
function.

There are a lot of progress and improvements since 2007 in vision based
articulated hand pose estimation, especially from depth sensor. This paper
will focus primarily on the state of the art works done post 2007.

A more recent paper [14] in 2015 focused on comparing 13 hand pose
estimation algorithms from a single depth frame and evaluated their per-
formance on various publicly available dataset. Furthermore, [14] created a
new hand training dataset that is more diverse and complex to existing one.
[14] focus primarily on comparing the quality of each of the 13 algorithms
using a common training dataset, this paper focus on reviewing the latest
state-of-the-art hand pose estimation algorithms.

There are also older reviews [15, 16] for hand gesture; however, those
reviews focused mainly on gesture recognition and not pose estimation which
is a more challenging problem.

1.2 Outline

The remaining of this paper will be organized as follow, in section (2) we
will provide a high level overview of hand pose estimation pipeline and the
diﬀerent hand pose estimation architectures, we will also discuss why it is
still a challenging problem. Next, each of the stage shown in section (2)
will have their own section for in depth analysis and comparison between
latest state-of-the-art. Therefore, next section will be segmentation section
(3) which will focus on the various hand segmentation algorithms, followed
by the initializer section (4) which focus on appearance based hand pose
estimation methods, then followed by the tracking section (5) which focus
on the model base hand pose estimation.

Recently, there was 2 deep learning explorations for hand pose estimation
that do not ﬁt in the mentioned layout, so following the tracking section is
the deep learning section(6), which focus on the 2 deep learning papers for
hand pose estimation. Next, we will discuss the current state-of-aﬀair of
hand pose dataset in section (7), followed by current vision based hand pose
limitations in section (8), then followed by future directions in section (9).
Lastly, we conclude our ﬁndings in section (10).

4

2 Pose estimation overview

Vision based pose estimation is the process of inferring the 3D positions
of each of the hand joints from a visual input. Although, hand pose esti-
mation is similar in concept to human body pose estimation and some of
the hand pose algorithms are inspired or taken from body pose estimation
methods, there are subtle diﬀerences that make hand pose estimation more
challenging. Such as the similarity between ﬁngers, dexterity of the hand
and self-occlusion.

Figure 1: An example of hand model with 26 DOF.

Here some of the challenges for hand pose estimation, the below list
assume a non-wearing glove single hand. For more challenging cases are
addressed in section (8) which discuss current hand pose estimation limita-
tions.

1. Robust hand segmentation: while this might seem to be a solved
problem, segmenting the hand reliably under unconstrained condition
is a diﬃcult task. Hand segmentation is crucial for the quality of the
hand pose estimation, most of the reviewed techniques in this paper,
their success depend heavily on a good hand segmentation.

5

2. Degree of freedom (DOF): hand pose have 20+ DOF that need to
be recovered, which is an extremely diﬃcult problem, shown in ﬁgure
(1).

3. Hand shape: not all hands are the same, they vary from one per-
son to another. The need to estimate or learn hand shape add more
challenges.

4. Self-occlusion: in a lot of hand poses, the occlusion come from ﬁngers

occluding each other, which make estimating the pose diﬃcult.

5. Speed: To estimate hand pose, we are dealing with high dimension-
ality, huge amount of data and complex algorithms. Most algorithms
on hand pose estimation are not fast enough for the task that they are
trying to solve and some of them require a high end PC or GPU to
run in real-time. Furthermore, the amount of time taken by the hand
pose estimation algorithm is an added latency to the hand input, in
some cases the estimated hand pose wont match current hand pose
due to latency.

2.1 Pose estimation pipeline

Vision based mark-less hand pose estimation has improved drastically in
recent years, the two approaches used for hand pose estimation are discrim-
inative approach and generative approach. Discriminative approach is an
appearance based approach, which mean it infer the hand pose from the
input data directly. Generative approach uses a hand model and tries to ﬁt
the hand model to the observed data.

Some hand pose pipelines use discrimininative approach only [17, 10,
2, 12, 11], others use generative approach only [6, 7], and another use a
hybrid approach that combine both discriminative and generative method
in a single pipeline [9, 3]. Also, [2] uses discriminative approach for the hand
pose pipeline and generative approach to generate the ground truth training
data in order to train their discriminative pipeline. Training data is one of
the biggest bottleneck for hand pose estimation, the in-depth discussion of
hand tracking dataset is in section (7).

Figure (2) is a high level architecture of a hybrid hand pose estimation
pipeline. The initializer stage help to bootstrap and recover the tracking
stage in case of tracking failure or during the ﬁrst frame. The tracking part
in this pipeline is the most expensive part in term of compute, it tries to

6

ﬁnd a hand model that explains the observed data. This pipeline is the most
robust but the most costly in term of compute.

Figure 2: A hybrid hand pose estimation pipeline.

Figure (3) is a high level architecture of discriminant hand pose estima-
tion pipeline. This pipeline usually use machine learning (ML) and require
a lot of data. One of the main disadvantage of this pipeline is that it does
not take into consideration previous result. So the output can be jittery,
this can easily be ﬁxed by smoothing the output with the previous output.

Figure 3: Discriminative hand pose estimation pipeline.

Figure (4) is a high level architecture of a generative hand pose esti-
mation pipeline. This pipeline tries to ﬁnd a hand model that explain the

7

observed data. The main issue of this pipeline is that it does not recover
from tracking failure, it assumes that the hand pose changes between frames
are minimum.

Figure 4: Generative hand pose estimation pipeline.

Figure (5) shows model based hand tracking, the orange piece is only
available if we have an initializer stage, otherwise it does not exist. The hand
model generators generate multiple hand models around previous frame
hand joint result and the initializer output. Those hand models are called
hypothesis and they are the input to the optimizer. The optimizer (called
”Find Best Hand Model” in the diagram) tries to ﬁnd which hypothesis
explains the observed data the best using a cost function that measure the
discrepancy between the observed data and the hand model.

8

Figure 5: Inside the hand tracking stage, the orange circle is only when the
initializer is present.

Next, we will go in depth for each of the hand pose pipeline stages. One
advantage of having well deﬁned pipeline is that we can pick and choose the
algorithm used for each stage without impacting the rest.

3 Segmentation

One of the disadvantages of using model ﬁtting, is that it is sensitive to
segmentation. If the hand segmentation is not accurate the tracking part of
most of the techniques mentioned in this paper will fall apart. This is why
hand segmentation is crucial aspect for the success of hand pose estimation
and it needs more attention.

9

Although hand segmentation might seem an easy problem at ﬁrst, seg-
menting the hand in an unconstrained environment is still an unsolvable
problem. Here some of the challenges for hand segmentation:

1. Hand does not have distinct features similar to human faces.

2. Hand is a non-rigid body part, which mean for each pose the shape of

the hand is diﬀerent.

3. Depend on the hand pose, the shadow in the hand can change.

4. For real-time hand tracking pipeline, hand segmentation is the ﬁrst
stage in the pipeline and it needs to be extremely fast in order for
the rest of the pipeline, which is more computational intensive, to ﬁt
within the real-time constrains.

5. Using Machine learning approach, the most challenging part is to have
good coverage for the non-hand cases. Which is extremely diﬃcult.

Figure (6) shows hand parts classiﬁcation output, the output of the ML
hand segmentation can be binary classiﬁer (hand or no hand) or multiclass
classiﬁer (no hand or hand parts).

Figure 6: Example of hand parts classiﬁcation.

10

As shown next, most of the current hand segmentation algorithms have

some assumptions and added constrains.

3.1 Color or IR skin based segmentation

A lot of literature focused on skin based detector for face recognition [18]
and hand tracking [6, 7, 19]. Skin color detection is attractive for hand
segmentation due to its speed, simplicity and the uniqueness of human skin
color. Skin detector is usually implemented heuristically, probabilistically
or using Machine Learning.

Heuristic methods are based on a color space or combination of color
spaces [18, 20, 21, 22], the preferred color space is the one that separate
chrominance channels from the luminance channel in order to be resilient
to illumination changes, [23] did comparative study for diﬀerent skin color
models for human face detection in color image which also apply to hand
segmentation.

For probabilistic methods, the idea is to create a probability distribution
that provide the probability for each pixel in the image if it is a skin or not.
[6, 7, 19] implemented a Bayesian classiﬁer, they used YUV422 color space
and ignored the Y channel, which corresponding to illumination, in order
to reduce illumination dependency and the amount of data.
[19] involved
training phase and an adaptive detection phase, the training phase trained
oﬄine on training dataset and the adaptive phase combine the prior prob-
ability from the training phase and the prior probability from the previous
N frames to cope with illumination changes.

For the machine learning methods, the idea is to train a machine learning
algorithm on the input image to distinguish between skin and non-skin area.
[8] trained a random decision forest on Infrared (IR) signal to infer depth
from IR skin tone. The training data was capture with the help of a depth
sensor registered to the IR sensor, with the assumption that the hand is
the closest object to the camera. With the tagged data they run a random
decision forest in order to infer depth value from IR skin tone. The tagged
data was for each IR frame there is a corresponding depth frame in which
the skin pixel has depth and the non-skin pixel has zero depth. There are 2
advantages for this approach:

1. IR signal is the same under most lighting conditions, which mean the
variation of illumination problem in color image does not apply here.

2. They infer depth for each skin pixel not only skin or not skin, which

provide more data for hand tracker.

11

While skin based hand segmentation is attractive, it suﬀers from a lot of
problems that make it insuﬃcient for general purpose hand pose estimation:

1. For color image, even using chrominance channels only, is not suﬃcient

to protect against illumination changes.

2. Skin color detector assumes that no other object in the scene have the
same color, which is not true. Even for IR skin tone detector, there
are some objects with similar IR level as the human skin.

3. If the person is wearing a short sleeve, skin color detector will segment
the rest of the arm which might break hand tracking (Hand model
used in [3] include part of the arm to work around this issue).

3.2 Temperature based segmentation

In order to provide a robust hand segmentation that work across diﬀerent
lighting conditions and cluttered background, [4] segment the hand from
a passive IR image using a thermal vision infrared camera. The idea is
that normal body temperature is constant, so using thermal imaging, [4]
segmented the hand with a single threshold that matches body temperature.
Although, this method work under diﬀerent lighting condition and busy
background, it assumes that body temperature is constant. Which might
not be the case, if someone is sick, or his or her body temperature is a little
oﬀ because of the weather. Also, it assumes that no other object in the
scene have the same temperature as the human body.

3.3 Marker based segmentation

In order to increase the robustness and speed of the hand segmentation, [1]
segmented the hand using a colored gloves. The glove actually provided a
unique color for each part of the hand to help not only the segmentation
part but the pose estimation part also. While this approach worked in [1]
scenario, it assumed that no other object in the scene have the same color as
the glove. Also, wearing a glove in order to do hand tracking is not natural
for natural user interaction.

Instead of wearing a glove, [2, 3] colored the actual hand in order to
segment and estimate part of the hand. Although, we shouldn’t expect
people to color their hand in order to use their hand as input mechanism
and even with colored hands it still have the same issues as the glove method.
[2, 3] used the colored hand to generate training data only, and then they

12

used machine learning algorithm to train on the generated data. Manually
tagging the hand is cumbersome, error prone and does not scale, so painting
the hand is a good solution to automate the tagging process.

3.4 Depth based segmentation

Depth image provide the depth value at each pixel in the scene. One of the
big advantage of the depth data is that because we know how far the hand
is in the scene, we can roughly estimate heuristically a bounding box around
the hand regardless of how far or how close the hand is from the camera. In
essence, depth data is hugely beneﬁcial in writing a scale invariant detector.
For depth based hand segmentation, one of the assumptions commonly
made is that the hand is the closest object to the sensor [9, 12], which is not
always true especially in oﬃce environment where part of the desk is visible
to the depth sensor. In [9], they assumed the hand is the closest object to the
sensor and used connected component analysis to ﬁnd all depth pixels belong
to the hand, in order to avoid having the wrist as part of the segmentation,
they wore a black band around the wrist to create a depth void.

[7] segmented the hand using both skin based approach from a color
image and a depth based approach from a depth sensor. They used the
depth data to limit the search space of the hand location, and they used
skin color detector from [19] to segment the actual hand. They limited the
search space to be within of +/- 25 cm from the previous frame.

3.5 ML based segmentation

[3, 2] used random forest for pixel wise classiﬁcation of the hand, their
algorithm is based on the human pose estimation work from [24]. In [3],
they used 2 steps process to segment the hand.

1. Using the output of Kinect body tracker to provide a rough estimate

of the hand position.

2. Kinect body tracker hand position is not always precise and does not
work closer than 0.5 meter from the sensor. So the second step is
Machine learning (ML), a pixel wise classiﬁer from [24] that classify
pixels that belong to the hand from those belonging to the forearm or
background.

13

Figure 7: From [3] Hand segmentation:left training data, right test data and
last row shows failure case

In order to automate the process of tagging segmented hand, [3] capture
a video sequence of a painted hands from a Time-of-Flight (ToF) depth
sensor, and from a calibrated color camera that is registered to the depth
sensor. Then a semi-automatic color segmentation algorithm applied to the
captured data in order to produce pixel wise ground truth for ﬁnger, palm
and forearm. As shown in 8, each ﬁnger is painted with diﬀerent color,
and the palm also is painted with a diﬀerent color. All the participants are
wearing long sleeve with uniform white color.

14

Figure 8: From [3] Hand segmentation ground truth

4

Initializer

The function of the initializer is to infer the most likely hand pose or poses
(called hypothesis) that explain current input data.
Its main purpose is
to help recovering tracking failure and provide estimate of the initial set of
hand poses for the tracker in order to constrain the optimizer search space.
The better the initializer in estimating the hand pose, the less work and
less compute is needed by the tracker to ﬁne tune the ﬁnal result. Hand
pose estimation algorithms in the initializer stage are appearance based
techniques (discriminative), which mean they estimate hand pose based on
the input frame directly without a hand model to ﬁt.

Most discriminative hand pose estimation and gesture recognition algo-
rithms can be used as initializer in the hand pose pipeline described in this
paper.

Most of the works done in the initializer can be categorized into four

diﬀerent categories:

1. Heuristics: [9] used heuristics to estimate ﬁnger tip locations and palm

direction, then used those to estimate the ﬁnal hand pose.

15

2. Inverse Kinematic (IK): using hand IK [2] to estimate hand joint lo-

cations.

3. Machine learning: use ML [25, 3] to estimate hand pose directly from

the data.

4. Image Retrieval: [17] treat hand pose estimation as image search from

a large database of hand poses.

4.1 Heuristics

Heuristic techniques are the least reliable method, because they are usually
based on a lot of assumptions and speciﬁc hand scale.

To ﬁnd ﬁnger tips [9] used the extreme points in the geodesic distance
for both the 2D XY plane and 1D Z direction. [9] tried to use the 3D point
cloud [26, 27] instead of the 2D XY and 1D Z, however, this approach did
not work well with ﬁngers. Each of the top extreme points are considered
ﬁnger tip proposals, the next step is to ﬁnd which one is a real ﬁngertip and
which one is not.

To evaluate each ﬁnger tip proposal, [9] grew a ﬁnger segment for each
ﬁnger tip proposal then checked if its geometry is similar to a ﬁnger or
not. Finger geometry similarities are done using heuristics and template
matching. After the evaluation, the direction of each ﬁnger tips is estimated
using principal component analysis (PCA). Then, all ﬁnger segments are
removed from the 3D cloud, the remaining blob is the palm, the direction
of the palm is also estimated using PCA.

To estimate the hand pose, [9] used ﬁnger tips, ﬁnger directions and
palm direction as constrains. From forward kinematic, [9] derived ﬁnger tips,
ﬁnger directions and palm direction from the hand model. Therefore, the
optimal hand pose is the one that minimize the delta between the observed
ﬁnger tips, ﬁnger directions and palm directions, and the model ﬁnger tips,
ﬁnger directions and palm directions. Because the ﬁnger identity is not
known, [9] enumerate all possible ﬁnger combinations and select the one
that return the minimum cost function.

4.2

Inverse Kinematic (IK)

Inverse kinematics (IK), used in hand pose estimation, is simply the solution
to a non-linear hand kinematic equations or objective function based on a
certain hand model and the end eﬀectors (such as hand joints or ﬁnger tips).
Hand model in this context is rigid bodies connected via joints. The end

16

eﬀectors are the estimated 3D joint and/or ﬁnger tips locations in the depth
frame. So in essence, from a set of estimated 3D joint locations, we generate
a set of non-linear equations based on the kinematic of the hand model and
its constrains, then we try to ﬁnd a solution. Or a cost function that evaluate
how the model align to the observed data.

The non-linear equations do not have close form solution for a complex
structure such as hand model, so in order to solve the equations we need to
use optimization techniques. The solutions of these equations are the joint
conﬁgurations for the hand model which is usually the 3D coordinate of each
joint. A review to diﬀerent IK techniques is given in [28].

[2] estimated the joint locations from a heatmap generated by a trained
convolution neural network (ConvNet) from a single depth frame, their Con-
vNet is discussed in detail in section (6). Then, they used IK to recover the
pose.

The heatmap generated from [2] ConvNet contains 3D or 2D feature
points corresponding to the hand joint in the depth image. The (x, y) are
the coordinate of the feature point in the depth image and the z is the actual
depth value, 2D feature points if the depth value is zero. Using those feature
points they minimized an objective function to align the hand model to the
inferred features. Equation (1) shows the objective function used in [2]:

n
(cid:88)

f (m) =

[∇i(m)] + Φ(C)

i=1
(cid:40)

∇i(m) =

(cid:107)(u, v, d)t
(cid:107)(u, v)t

i − (u, v, d)m
i (cid:107)2,

i − (u, v)m

i (cid:107)2,

(1)

If dt
i (cid:54)= 0.
otherwise.

Where (u, v, d)t

is the
model feature position i from current pose estimate. And Φ(C) is a penalty
constrain.

i feature position i from the heatmap and (u, v, d)m
i

To ﬁnd the best model that align with the observed feature points, [2]
used Particle Swarm Optimization (PSO) algorithm. The advantage of PSO
is that it can be parallelized and it is resilient to local optima.

One problem of IK is that it does not perform well with occlusion, such

as joints that are not visible to the camera.

4.3 Machine Learing (ML)

In this technique, we can turn the initializer problem into a regression prob-
lem, train ML on an input data to predict the hand pose. However, regres-

17

sion on high dimension data is diﬃcult in practice [29, 25, 3].
[25, 3] split
the regression problem into two sub-problems called levels or stages, ﬁrst
level predict global features, called Global Expert Network (GEN) in [25],
and second level predict local features, called Local Expert Network (LEN)
in [25]. This split is also called coarse to ﬁne tune [10, 14, 11].

In [3], they used discriminative ferns ensembles [5] for the ﬁrst level, and

decision jungle [30] for the second level:

1. Level 1: In this level [3] used discrimination ferns ensembles to infer
the global rotation of the hand. The global rotation is quantized to
128 discrete bins.

2. Level 2: This level is condition on the output of level 1, there is a
classiﬁer for each bin. So in this level, there are 128 classiﬁers for each
of the 128 bins. Decision Jungles was used because its small memory
footprint allowed it to scale to 128 classiﬁers. Level 2 predicts:

• Global translation.

• Global rotation.

• Pose cluster from one of the following clusters: open, ﬂat, half

open, closed, pointing, pinching.

4.4

Image Retrieval

In this category, the problem of ﬁnding per image hand pose is treated as
a content base image retrieval (CBIR), by simply index a large database
of image poses with their corresponding hand pose parameters. Then, for
an input image extract its features and ﬁnd the closest hand pose from the
hand poses database that matches the input image features. The closest
match is the result.

[17] created a large database of rendered hand poses where each entry
contains the hand pose parameters that generated this hand pose view. This
part is a preprocessing part that can be done oﬄine.

Now for an input image, [17] ﬁnd the closest hand pose entry in the
database, and return the hand pose parameters associated with this entry.
The returned hand pose parameters are the hand pose result for the input
image.

The problem of this method is that it will require a pretty large database
in order to accommodate the analog nature of hand pose. Nevertheless, it
can be used as ﬁrst layer in the machine learning approach.

18

5 Tracking

The goal of the tracking is to estimate the current hand pose from multiple
hypothesis and the observed data, these hypothesis are generated from the
initializer and previous hand poses, as shown in ﬁgure (9). The main purpose
of the multiple hypothesis is to reduce the number of hand poses that the
tracking need to evaluate and constrain the search space. Hand hypothesis
is a model of the hand and the evaluation is a cost function that takes a
hand model and the observed data as input, and return a single number
that measure how close is this hypothesis to the observed data.

Figure 9: Optimizer in model based hand tracker.

Due the number of parameters required by the hand model, even with
a limited number of initial hypothesis, perturb the model parameters will
In order,
result a huge number of hypothesis that need to be evaluated.
to eﬃciently search the parameter space of the hand model, most papers
use stochastic evolutionary optimization techniques. So for hand tracking
to work, we need the following 2 parts:

1. A good hand model that can express the required hand poses and the
corresponding cost function that measure the discrepancy between the
observed data and the model.

2. An optimization technique to search the hand model parameter space
in order to ﬁnd the best hypothesis that explain the observed data.

19

Having a hand model (hypothesis) and a cost function that measure the
discrepancy between the hand model and the observed data, the goal of
the optimizer is to ﬁnd the best hypothesis that explain the observed data
according to the cost function.

The cost function depends heavily on the selected hand model, the type
of observed data and the assumption made to reduce the evaluation of the
cost function.

5.1 Hand model and cost function

Human hand contains many moving parts that interact with each other
and provide complex articulation. In order to model the hand, there are
a variety of options depend on the balance required between accuracy and
performance. The selected hand model and the input signal dictate the
design of the cost function.

The characteristic of a good objective function for hand tracking is as

follow:

1. Need to provide a measure of how close a hand model is to the observed

data without ambiguity.

2. Need to have constrains against trivial solution that break the kine-
matic or the anatomy of the hand. Such as overlapping ﬁngers, bone
angles that are physically impossible...etc.

3. For real-time system, the objective function is called for each hypoth-

esis. So the evaluation of the objective function need to be fast.

Hand model used in literature varies from simple model consistent of
basic geometries [9, 6, 7] to a more sophisticated model consistent of full
3D mesh of the hand [3, 2]. From performance perspective, there are two
bottlenecks related to hand models:

1. To evaluate a hand model with a set of parameters, the hand model
need to be rendered ﬁrst. Which occur for each hypothesis evaluation
per frame.

2. Once we have a rendered hand mode, the evaluation itself measure
the discrepancy between two 3D point clouds, one from the observed
depth and another from the synthetic hand model.

Both of the above operation are computation expensive.

20

5.1.1 Sphere based hand model

One of the most simple hand model, is sphere based hand model from [9]
as shown in Figure 10. In this presentation, to present a hand, all what we
need is the center of each of the spheres and their corresponding radius. [9]
adopted 26 degrees of freedom (DOF) similar to [7, 1].

Figure 10: From [9] Sphere based hand model and its corresponding DOF.

In order to approximate the hand, [9] used 48 spheres: 6 for each ﬁnger
except the thumb ﬁnger, 8 for the thumb ﬁnger and 16 for the palm. The
number of spheres chosen for each ﬁnger and the palm were entered manu-
ally. The sphere size and center were set empirically from the polygon mesh
model in [6]. The model is ﬁxed in size and not adaptable to diﬀerent hand
sizes.

One of the huge beneﬁt of the sphere model is that its cost function is
relatively fast due to the fact that points on the surface of the hand model
are simply points on a sphere which can be evaluated with a single equation.

The cost function used in [9] is composed of three terms, shown in equa-

tion (2):

• Align point cloud to model M , in order to compute this term in real-
time [9] down sampled the point cloud randomly to 256 points (this
will aﬀect the number of local optima because of the addition artifacts
from down sample).

• Force the model to lie inside the cloud.

• Penalize self-collision.

21

E(P, M ) = λ ·

(cid:88)

D(p, sx(p))2 +

p∈sub(P )

B(ci, D)2 +

(cid:88)

i

(cid:88)

i

L(si, sj)2

(2)

Where P is the point cloud and M is the sphere based hand model. D(.)
align sub-sampled point cloud to the hand model, for each point cloud D
compute the distance between this point to the surface of the closest sphere.
B(.) forces the model to lie inside the point cloud, by project each sphere
into the depth map, then measure the distance between the actual depth
and the projected sphere depth. L(.) penalize self-collision between neighbor
ﬁngers, by check if the spheres from both ﬁnger overlap or not.

5.1.2 Geometry based hand model

[6] used a hand model based on a number geometry primitives as shown
in ﬁgure (11).
[6] uses elliptic cylinder and two ellipsoids for palm, three
cones and four spheres for each ﬁnger except the thumb, two cones and three
spheres for the thumb.

Figure 11: From [6] Basic geometry based hand model.

[6] captured multiple images of the hand pose from multiple cameras
surrounded the hand, then they projected the 3D hand model to each of the
camera view using the camera calibration data. Therefore, the result is N
images that capture the real hand pose and another N images that capture
the hypothesis. And the goal became how to compare the N observed 2D
images with the N rendered 2D images, in order to evaluate the hypothesis.
In order to evaluate each hand hypothesis relative to the observed data,
[6] generated descriptors from the observed data and from the synthetic
model, then compare them with some objective function deﬁne in equation

22

(3). The hand pose in [6] was captured with multiple cameras simultaneous,
so each capture is a multi-frame capture, the computed descriptors in each
of the observed frame are:

• Segmentation mask of the hand, generated using skin color to segment

the hand from background.

• A distance transform of the edge map of the image. The edge map

was computed using Canny Edge Detector[31].

Using the same notation as [6], each image I, from the multi-frame cap-
ture, will have segmentation mask os(I) and distance transform map od(I).
In order to compute the equivalent mask and map from the hypothesis, [6]
render each hypothesis to each of the cameras using the camera calibra-
tion data C(I). Then from the rendered image [6] generate the same mask
and map as the one from the observed data. The segmentation mask for
a hypothesis h corresponding to image I is rs(h, C(I)) and the distance
transform for the same hypothesis corresponding to image I is rd(h, C(I)).
The hypothesis evaluation used by [6] is a distance measure between
hand pose hypothesis h and the observed multi-frame data M , this distance
indicate how closely this hypothesis match the observed data. Here the
evaluation function used by [6]:

E(h, M ) =

(cid:88)

I∈M

D(I, h, C(I)) + λk · kc(h)

(3)

At high level, equation (3) has two terms:

1. (cid:80)

I∈M D(I, h, C(I)) this term is responsible for measuring how close

the hypothesis is to the observed data.

2. λk · kc(h) this term is a penalty term for kinematically implausible

hand conﬁgurations.

From equation (3), h is the hypothesis, M is the observed multi-frame,
I is an image in M , C(I) is the camera calibration for the camera that
captured image I, λk is a normalization factor and the sum is over all images
in this multi-frame. D from equation (3) is deﬁned as follow:

D(I, h, c) =

(cid:80) os(I) ⊕ rs(h, c)
(cid:80) os(I) + (cid:80) rs(h, c) + (cid:15)

+ λ

(cid:80) od(I) · rs(h, c)
(cid:80) re(h, c) + (cid:15)

(4)

Where os(I), od(I), rs(h, c), and re(h, c) are the mask and map for both
observed image and rendered hypothesis, the term (cid:15) is to avoid dividing by

23

zero, the symbol ⊕ is the logical XOR which will return zero if both elements
match, and the sum is over the entire mask and map.

The above cost function satisfy most of the requirements of a good cost
function, except the dis-ambiguity requirement. This is due to the fact that
it projects the 3D model into a 2D image which lose information. To mitigate
that they surrounded the hand with multiple camera in 360 degree fashion,
which make it diﬃcult and awkward to use in 3D interaction scenario.

The same author in [7] implemented similar algorithm with the same
hand model but using depth data from a single Microsoft Kinect sensor
instead of multiple RGB sensors. The model was rendered into 3D depth
cloud instead of 2D projection.

5.1.3 Mesh based hand model

Full mesh model of the hand is currently the most accurate model but also
[3] used a
the most expensive model in term of computational resource.
full mesh model of the hand that include the wrist for their optimization
stage; in contrast, [2] used the accurate model to generate ground truth of
the hand poses for their training algorithm, as discussed in section (6).

Figure (12) shows the mesh based hand model used by [3]. The left
image is the kinematic of the hand used in [3], center and right are possible
hand model generated by standard linear blend skinning from [32].

Figure 12: From [3] Mesh based hand model.

For the scoring function, [3] render each hypothesis (hand model) into
a synthetic depth compatible with the real depth data, then compare the
synthetic depth with the real depth directly. [3] deﬁne a function that takes
the base mesh and the pose parameters θ of the hand as input, and output

24

the synthetic depth. As shown in equation (5), rij is the synthetic depth
pixel at index i and j, for no hand pixel the value equal to the background.

R(θ; V ) = {rij|0 < i < H, 0 < j < W }

And here the scoring function:

E(Zd, R) =

(cid:88)

ij

vijρ(zij − rij)

Where ρ(.) is a truncated linear function kernel.

(5)

(6)

As shown above the cost function is pretty simple, however, the main
computation task is the rendering of the hand 3D mesh model for each
hypothesis.

5.2 Optimization

The tracking part of the hand pose pipeline turns the hand tracking prob-
lem into an optimization problem, its goal is to ﬁnd the parameters of the
hand model that minimize the cost function. Because it is an optimization
problem, most optimization techniques can be applied. However, the are
multiple issues speciﬁc to hand tracking optimization that put constrain on
the type of optimizer used:

• The parameter space of the hand model is a high dimension space,

usually around 27 dimensions as we will see later.

• The parameter space contains a lot of local optima.

• The cost function is expensive in term of compute, because it requires
rendering the hand model and compare it to the input 3D point cloud.
Both operation are expensive in term of compute.

The input to the optimizer is the result of the initializer and previous

frame result.

In [13] 2007 survey, they reviewed the following optimization techniques
used in hand pose estimation: GaussNewton method [33], Nelder Mead
Simplex (NMS) [34], Genetic Algorithms (GAs) and Simulated Annealing
(SA) [35], Stochastic Gradient Descent (SGD) [36], Stochastic Meta Descent
(SMD) [36], and Unscented Kalman Filter (UKF) [37].

Most recent publications in hand pose estimation use evolutionary algo-
rithm to search the parameters space of the hand model. [6] in 2010 showed

25

that Particle Swarm Optimization (PSO) can be used eﬃciently to ﬁnd the
right hand hypothesis that explain the observed data. Current state of the
art hand pose estimation uses hybrid approach: PSO to explore the param-
eter space and be more resilient to local optima, and another algorithm to
speed up the convergence. Next we will go in depth to each of the PSO
variation used for hand tracking.

5.2.1 Particle Swarm Optimization (PSO)

Particle Swarm Optimization (PSO) is evolutionary computation and population-
based optimization technique, inspired by social behavior of bird ﬂocking
and the ﬁeld of evolution computation. PSO was introduced in particle
swarm optimization [38] and described in details in Swarm Intelligence [39].
PSO optimize an objective function by keeping track of a population of can-
didate solutions each called particle, in each iteration (called generation)
each of those particles move in the solution space using a simple mathemat-
ical formula that depends on the evaluation of the objective function (called
ﬁtness) at each particle; the global best solution is updated in each iteration
and shared across all particles.

In PSO each particle store its current position and current velocity, in
addition to the position in which it had the best score of the ﬁtness func-
tion. Also, the global best position (solution candidate) across all particles
is stored and kept up-to-date.

Here the high level steps of PSO:

1. Initialize the particles at random, the number of particles is given as

an input.

2. Evaluate the ﬁtness function or objective function for each particles.

3. Update individual best ﬁtness’s and update the global best ﬁtness.

4. Update the velocity and position for each particle

5. Repeat until the global best ﬁtness meet certain threshold or the num-

ber of iteration exceed a certain threshold.

Here the equation to update the velocity and position:

vi(t + 1) = wvi(t) + c1r1[xi(t) − xi(t)] + c2r2[gi(t) − xi(t)]

26

xi(t + 1) = xi(t) + vi(t + 1)

Where the subscript i indicate which particle, vi(t) is the velocity of
particle i at iteration t, xi(t) is the position of particle i at iteration t, xi(t)
is the best position of particle i at iteration t and gi(t) is the global best
solution at time t. w, c1 and c2 are parameters provided by the user, and
r1 and r2 are random samples of a uniform distribution between 0 and 1,
generated at each iteration.

Advantage of PSO are that it is easy to implement, few parameters
to tune, easy to parallelize and resilient to local optima. The reason for its
resilient to local optima is due to the fact that each particle explore diﬀerent
area in the search space at the same generation.

A problem can arise in PSO is particle premature collapse [38], which is
caused by premature conversion to a local optima, this usually happen in
high dimension data. A mitigation against particle premature, is to have
multiple global best ﬁtness one for each sub-swarm particles. So in essence
split the particles into a clusters where each cluster has its own global best
ﬁtness. Another issue, depend on the cost function PSO might be slow to
converge [9].

There are a lot of variation of PSO to mitigate some of its issues and
there are a lot of hybrid approaches which is mixing PSO with other tech-
niques.

To the best of my knowledge, the ﬁrst use of PSO in hand pose estimation
was introduced by [6], [6] shows that PSO can be used successfully in hand
tracking. PSO was used to ﬁnd the optimal 3D hand model parameters that
best ﬁt or explain the observed data. The hand model used has 26 DOF
encoded in 27 parameters, so the search space is 27 dimensions and each
particle position is 27 dimensions vector encapsulating the parameters of
the hand model. Each instance of a hand model is called hypothesis, so the
objective of PSO is to ﬁnd the best hypothesis that explain the observed
data.

5.2.2

Iterated Closest Point (ICP)

ICP [40, 41] is widely used iterative algorithm to align two point clouds, by
ﬁxing one (the observed or scanned point cloud) while keep changing the
other until they align. ICP converge fast and suitable for real-time applica-
tion, but it can easy be trapped in local optima.

27

Here the high level steps for ICP:

1. For each point in the source cloud ﬁnd the closest point in the observed

cloud.

2. Estimate the rotation and translation transform between both clouds

using mean square error.

3. Apply the transform estimated in the previous step to the source data.

4. Iterate until the mean square error is below certain threshold or it

exhausted a maximum number of iteration.

[42] generalize ICP algorithm to articulated structure with multiple parts
connected with joint or point of articulation. Their experiment showed
promising results for both upper body tracking and hand tracking. They
represented articulated structure as a tree, where each node is a rigid body
part and the edge represent the joint that is connected two body parts.
Also, one of the node is arbitrarily selected as the root node. The root node
transform is relative to the world coordinate, and all other node transforms
are relative to their parents. Each transform between a child node and its
parent is constrained by the degree of freedom of this joint. Each body part
is presented by a set of points.

The extension that [42] provided to generalize ICP for articulated struc-
ture, instead of trying to ﬁnd all closest points from the source to the ob-
served data, they pick a body part ﬁrst, then ﬁnd the correspondence of this
body part to the observed data while enforce the joint constrains.

5.2.3

ICP-PSO

To take advantage of ICP fast convergence, and of PSO resilient to local
optima and search space exploration, [9] implemented a hybrid approach
that uses both methods to ﬁnd the closest hand model parameters that
matches the observed data, they called the new hybrid algorithm ICP-PSO.
The observed that was captured from single Time-of-Flight(ToF) sensor.
The result was a real-time hand tracking from a single depth sensor.

[9] combined both algorithm by performing ICP for each particle before
move to the next PSO generation. Pretty much they used ICP to fast track
the convergence at the local particle level.

The hybrid ICP-PSO algorithm used by[9] is shown in Algorithm (1)

below:

28

Algorithm 1 ICP-PSO Algorithm
1: procedure ICP-PSO(PreviousHandPose, InitializerResult)
2:

ps ← GenerateRandomP articles(P reviousHandP ose, InitializerResult)
for each generation do
for each particle do

compute correspondences
for m times do

(cid:46) ICP each particle

gradient decent on a random parameter

end for

end for
k-mean clustering for all particles
particle swarm update for each cluster

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

end for
return best particle

13:
14: end procedure

As shown above, the k-mean clustering is used to avoid PSO premature

convergence.

Figure (13), from [9], shows hand tracking comparison between PSO,
ICP and ICP-PSO. The red rectangle highlight the images that has large
error. As shown above, ICP-PSO outperform both PSO and ICP alone. To
reach a real-time performance [9] used a sparse subset of the input depth
data, which will make the data less smooth and will increase local optima
from the introduced artifacts.
In this setup, ICP-PSO performed better
than PSO only or ICP only.

5.2.4 Genetic Algorithm (GA)

Genetic Algorithm [43] is a search algorithm inspired by the process of natu-
ral selection and genetics in evolution. GA used successfully in optimization
problem, especially in problems with little known or large search space. GA
depends on a ﬁtness function that evaluate how good each candidate so-
lution in the population is, using techniques similar to evolution such as
selection, mutation and crossover, GA will generate a new population from
existing one that provide better scoring. After each generation, the search
space should move to areas with high value of ﬁtness function, which mean
closer to the solution.

Here the 3 main operations per each generation for GA:

29

Figure 13: From [9] PSO versus ICP versus ICP-PSO

1. Selection: This step select which individuals will have oﬀspring, the
selection is based on a probability associated to each individual. The
probability is proportional to individual ﬁtness score.

2. Crossover: Crossover does not occur in every generation, it occurs
based on probability. When it occurs, each produced oﬀspring will
share parts from 2 parents (2 parts each from diﬀerent parent will
crossover to produce a new individual).

3. Mutation: Mutation also happen with probability, it randomly changes

some of the parameters of individual oﬀspring.

The above operations happen at each generation until it converge or

exceed a max number of generations.

To the best of my knowledge the ﬁrst use of GA in hand tracking was

from [35]. In [35] they used 2 optimization techniques:

30

1. GA: To reach close to the solution quickly in the hand model param-

eters search space.

2. Simulated Annealing (SA): For local search to ﬁnd the best model.

In essence, [35] used GA to reduce the search space and SA to ﬁne tune

the ﬁnal solution.

5.2.5 PSO + GA

In [3] they combined features from both PSO and GA, based on a modiﬁed
version of HGAPSO [44], for their hand tracking stage. According to [3] the
crucial aspect of PSO is how the next generation of particles are populated
from current generation and their scores.

Using [3] terminologies, the algorithm has a population of P particles
p=1 and their corresponding score {Ep}P
{Θp}P
p=1. The main loop is a standard
PSO loop, with 2 levels of randomization as follow:

• Each generation: adjust only ﬁngers, for 50% of the particles, a random

digit is chosen. Its abduction or ﬂexion is adjusted.

• Each third generation: select 50% of best performing particles (called

elite in [44]) and within these 50% do the following:

– Perform local perturbation on 30% of the particles.

– Replace 50% of the particles by drawing new one from a set of

poses.

– Perform GA, splicing or crossover operation, on the remaining
20% of the particles. By selected a random particles from top
50% to replace this particle.

Most of the parameters tuning, percentage and methods performed above
are from intuition and experimentation. So there is a room for more optimal
solution.

In ﬁgure (14) left most column contains the initial pose, and the seven

other columns show draw from the every third generation.

31

Figure 14: From [3] draw for every third generation

6 Deep learning

The huge success of deep learning in object recognition and other vision tasks
is primarily because the availability of huge amount of images (tagged and
non-tagged) and the computational power available today. In case of hand
pose estimation, the amount of tagged hand poses from RGB-D available
today are still low compared to image recognition dataset. Which might
explain why deep learning usage in hand pose estimation is low and did not
yet provide the impact that it did for other vision tasks.

To the best of my knowledge, there are only two deep learning publica-
tions for hand pose estimation [2, 12], both of them are pretty recent 2014
[2] and 2015 [12]. In this section we will discuss both implementations and
how they might ﬁt into the general architecture of hand pose estimation.

In [2] they trained a convolution neural network (ConvNet) to generate
a heat map that highlight all the hand joints visible to the sensor, as shown
in ﬁgure (15). The input to the ConvNet is a segmented and preproccessed
hand, for the segmentation they used pixel-wise hand classiﬁer similar to
[24]. In the ﬁnal stage, they used Inverse Kinematic (IK) to ﬁnd the hand
model from the heat map that minimize an objective function.

The heatmap is a 2D image that contains the (x, y) coordinate of each
visible hand joint and the corresponding depth value is from the depth map,
the problem in using IK to ﬁnd the hand pose based only on the heatmap
and not taking into consideration the actual observed depth data, is that it
wont be reliable for hidden joints. Nevertheless, the ConvNet stage can be
used in the initializer, from section (4), in the general architecture that we

32

discussed previously, and a more robust optimizer can be used to ﬁnd the
ﬁnal hand pose.

An interested work by [2], is how they generated their tagged training
data. They generated two set of data, one for the segmentation part and
another for the ConvNet part.

• Segmentation training data: Similar to [3], they used the painted hand
approach to automate the process of generating hand segmentation
ground truth.

• ConvNet training data: they used PSO with partial randomization(PrPSO)
[45] and high quality hand model (Linear Blend Skinning[LBS]), oﬄine
in order to ﬁnd the best hand pose that explain the depth data based
on a modiﬁed version of [7] algorithms. They ﬁne tune the output of
the PSO with Nelder-Mead optimization algorithm [46]. The result
of the tracking is the ground truth for the ConvNet, doing the model
based tracking part oﬄine means that only quality matter and not
performance.

Figure 15: From [2] ConvNet architecture to generate the heatmap

33

Figure 16: From [2] Heatmap result with feature points overlaid.

Figure 17: From [2] One of the feature detector ConvNet.

As shown in ﬁgure (15), the input depth is segmented using a random
decision forest (RDF) to generate a depth map that have depth values only
on the hand pixels and background value otherwise. Then, the segmented
hand is preprocessed, the output of the preproccessing is 3 diﬀerent ﬁxed
resolutions in which each is fed to a ConvNet feature detector shown in de-
tails in ﬁgure (17), the output of the detectors is fed into 2 layers full neural
network to generate the heatmap. And example of heatmap result is shown
in ﬁgure (16).

In [12], they used diﬀerent approach than [2] in their Convolution Neural

34

Network(CNN) architecture. They used two networks: one to infer the world
position of each hand joint, and the other to ﬁne tune the result, so it is
similar to coarse to ﬁne layers from [10, 11]. For the segmentation part,
[12] simply assumed that the hand is the closest object to the sensor, they
created a 3D bounding box around the hand, resize it to 128x128 pixels,
then normalized the depth to be in [−1, 1] range.

One of the contribution by [12] in CNN for hand tracking is the incor-
poration of hand kinematic constrains in the network. Due to the strong
correlation between diﬀerent 3D hand joint locations because of hand kine-
matic, it is possible to represent the parameters of the hand in a lower
dimensional space [47].
In order to embed and enforce such constrain in
CNN, [12] added a bottleneck layer with less neurons than needed by full
pose representation, this bottleneck layer force the network to learn a low
dimension presentation of the hand pose.

Figure 18: From [12] diﬀerent CNN architectures used in stage 1

Figure (18) shows the diﬀerent network architectures tried by [12] with
the addition of the bottleneck layer. (a) shows a shallow CNN network, (b)
shows a deep CNN network and (c) shows the multi-scale network. (d) is
the ﬁnal architecture with the bottleneck layer, the Multi-layer Network is
simply one of the networks in (a), (b), and (c). According to [12] (c) per-

35

formed best followed by (b) then (a).

Next stage is another network that reﬁne the result from ﬁrst stage,
the architecture of this network is called Reﬁnement with Overlapping Re-
gions(ORRef) by [12]. The input to the network is several patches with
diﬀerent sizes centered around joint location from the the ﬁrst stage. The
pooling is done only on the large patches, and the size of the pooling regions
depend on the patch size. The reason for no pooling in small patches is
accuracy.

Figure 19: From [12] CNN architectures used in stage 2

As shown in ﬁgure (19), there are multiple reﬁnement networks one per

joint.

Both deep learning approaches discussed in this section are per frame
regression of the hand pose joints, they do not take previous frame result
into consideration.

7 Dataset

Although there are a great progress in recent years for 3D hand pose estima-
tion from single depth camera, the amount of training hand data available
publicly are still pretty low. And within those available the dataset are lim-
ited in number of subjects and poses as we will see later. One of the main
problem in generating tagged hand pose data is that it is time consuming,
error prone and diﬃcult to scale the process unless we introduce some form
of automation or semi-automation.

36

Some of the works that we discuss here come up with innovative way to

automate the process of tagging hands in images, as follow:

• For hand segmentation the main method used is coloring the hand as

shown in [3, 2].

• For hand pose, [2, 11] used a slow but high quality hand pose estima-
tion pipeline in order to generate ground truth oﬄine, then manually
corrected the result.

• Another approach for hand pose is to use synthetic pipeline to generate
a ground truth for hand pose estimation, assuming that the synthetic
pipeline is accurate enough to mimic human hand as shown in [48].
[49] is a well known open source library that generate ﬂoating synthetic
hand pose. Even if the hand synthetic dataset is accurate, we still need
a real hand data for verﬁcation and testing.

Table (1) shows some of the available non-synthetic hand pose dataset:

37

Comment
While NYU contains a good
set of complex hand pose
ground truth, the problem is
that the training data,
for
both hand pose and segmen-
tation, are from one subject
only and the test data are
from 2 subjects only. Which
mean the data is biased. Fur-
thermore, 73K frames is very
little.

While it is better than NYU
data in term of the number of
subjects and variety of hand
size. It has 2 problems; ﬁrst,
the total number of original
data is 22K only, and sec-
ond, the captured hand poses
are not as complex as NYU
dataset.

Although this dataset con-
tains a good number of sub-
jects and gestures,
it is not
large enough.

Dataset Description
NYU [2]

ICL [10]

NYU dataset contains 72757
training hand pose frames
of RGB-D data with ground
truth information and 8252
test set. The data are from
3 Kinects camera:
a front
view and 2 side views. The
dataset also contains 6736
tagged depth frames for seg-
mentation. The training data
are from one subject and the
test data are from 2 subjects.
ICL data was captured from
Interactive
Intels Creative
Gesture Time of Flight(ToF)
Camera. They used [50] to
generate the ground truth for
each frame than manually
reﬁne it. The data is from
subjects with
10 diﬀerent
diﬀerent hand size.
Total
number of captured ground
truth is 20K images, with
rotation applied to this data,
the ﬁnal frame count is 180K.
MSRA [11] Captured 76500 depth images
from 9 subjects using Intels
Creative Interactive Camera.
The ground truth was gen-
erated using the optimization
method in [9], then manually
corrected. Each subject was
asked to do 17 gestures cho-
sen from American Sign Lan-
guage under large view point
and fast motion.

Table 1: Some of the publicly available non-synthetic hand pose dataset.

38

8 Limitations and challenges

Even with the huge progress in recent years for vision based mark-less hand
pose estimation, current state of the art still far away from human level
recognition and still does not match the non-vision gloves based approach.
Looking at the state of the art hand pose estimation reviewed in this pa-
per, we can see that most of them assume single hand, the background not
clutter or busy, no glove on the hand, the hand scale is predetermine and
the hand is empty (not holding any object). Some of those constrains might
be acceptable in certain cases, however, for general and reliable hand pose
estimation, and other cases, we need to address those constrains.

Here the list of current hand pose estimation challenges:

• Hand segmentation: Hand segmentation is still a challenging prob-
lem especially with clutter background and sleeve. Short sleeve make
it diﬃcult to segment the hand from the wrist up without taking part
of the arm. The most promising solution for hand segmentation is
machine learning approach such as the pixel wise classiﬁcation from
[3, 2], which classify if a pixel belong to a hand or non-hand. The
limitation of this approach is that it require a lot of training data.

• Two hands: Most of the focus for hand pose estimation is using one
hand, and for 2 hands they simply run 2 separate hand pose pipelines.
The problem in this case, beside doubling the computation amount, is
that it does not work when both hands interact with each other and
occlude each other. To the best of my knowledge, non of the publicly
available training dataset have 2 hands in a single scene interacting
with each other. 2 hands complicate all hand pose estimation pipeline
stages.

• Object grabbing: Grabbing an object by hand is another challenge,
there are 2 problems from hand pose estimation perspective when the
hand is holding an object:

1. The object will occlude part of the hand, which will make pose

estimation more diﬃcult.

2. If the object is not properly segmented from the hand, it can
cause false hand pose estimation because the hand pipeline can
treat part of the object as hand part.

39

• Scale: Not all hands are created equal, the shape of the hand vary
from one individual to another, especially between kids, women and
men. Each have diﬀerent scale and slight diﬀerent shape. Most of
the publications reviewed here assumed a ﬁxed scale. We need a way
to estimate the hand scale and shape or learn them, this can happen
at the initializer stage or the hand tracking stage. In hand tracking
stage, scale and shape can be added as parameters to the hand model,
in essence increase the degree of freedom of the hand model.

• Gloves and sleeves: None of the literature reviewed in this paper
support wearing gloves except when it is used as a marker. All publicly
available hand pose dataset are gloves free. Also, sleeve is another
problem depend on its materials and how long or short the sleeve is,
it will usually aﬀect the segmentation process which in turn will aﬀect
all downstream stages.

• Dataset: Compared to object recognition and classiﬁcation the num-
ber of hand pose dataset available publicly is still pretty low, and most
of what is available does not cover some of the challenges mentioned in
this list. The lack of wide variety of hand pose dataset make it diﬃcult
to compare various hand pose estimation algorithms, [11] created their
own dataset because existing one was not complex enough.

• Degree of freedom (DOF): Hand kinematic has a high number
DOF, which complicate the optimizer and can easily fall to local op-
tima.

• Computation expensive: Latest state of the art hand pose estima-
tion algorithms are pretty expensive in term of resource and run close
to real-time using high end machine.

9 Future directions

In this section, we will focus on what direction merit more focus in order to
progress hand pose estimation forward. One of the most important aspect
that begin to get attention recently [14, 11] is the availability of a large pub-
licly tagged hand pose dataset that cover variety of hand poses from diﬀerent
subjects with diﬀerent hand sizes. The poses need to contain challenging
hand poses in a cluttered background.

The other important aspect is the direction of the hand algorithm itself.
The most promising direction of hand pipeline is to have hybrid approach

40

discriminative and generative in a single pipeline [9, 3]. However, the current
bottleneck in this approach in the optimization part which require to search
a high dimension space plagued with local optima, and in each evaluation it
needs to render a complex hand model. So, the focus should be to improve
the initializer in order to make the per frame prediction as good to the ﬁnal
result as possible, that will make the tracking part much simpler.

Another part that need attention is hand segmentation, current hand
segmentation works in most cases except with complex scene such as clut-
tered backgrounds. The most promising algorithm for hand segmentation is
machine learning based algorithm that classify each pixel.
Next, we will discuss each of the above items in depth.

9.1 Dataset

There is a huge need for a large training dataset for hand pose estimation and
hand segmentation on variety of subjects that can be used for benchmark
between algorithms in order to move vision based hand pose quality forward.
Furthermore, the complexity of the hand poses in the training set need to
cover wide range of complexity and some of the issues that we listed in
section (8) such as 2 hands manipulation, wearing gloves, diﬀerent hand
sizes and variety of sleeves.

Generating ground truth from synthetic data is a great way to scale up
the training and test hand data creation, in addition to bootstrap hand pose
algorithms. Nevertheless, synthetic hand data does not replace real data.
And it is very diﬃcult to generate synthetic training data for segmentation,
due to the need of wide variety of diﬀerent backgrounds.

Here some of the possible improvements for synthetic hand data:

• The ability to simulate gloves and sleeves with a wide variety of ma-

terials.

• The ability to render wide variety of hand poses with speed and dex-

terity similar to human hand.

• The ability to add speciﬁc camera noise model and depth artifacts

such as for ToF sensor multipath, mixed pixels...etc.

• The ability to simulate cluttered background.

• The ability to render hand model in infrared (IR) and color frame.

Newer depth sensor support both streams.

41

• The ability to simulate 2 hands and their interaction to each other.

For generating training hand data based on real capture, the 2 promising

directions to reduce the time required to tag each frame are:

1. Segmentation training data: [3, 2] registered a color camera to the
depth camera and painted their subject hands with a speciﬁc color.
Then, they used a color segmentation algorithms to segment the hand
and produce the needed ground truth for hand segmentation. This
approach help automate the process of generating training data for
hand semgentation which is a tedious and slow task if done manually.

2. Hand pose training data: [2, 11] used a high quality hand model
to generate the initial ground truth by running an optimizer on the
input data, then manually adjust the result. The advantage of this
process is that it is semi-automatic and the only manual work needed
is for quality assurance. The use of high quality hand model is not an
issue here performance wise, because this process run oﬄine.

Another direction that will help moving vision based hand pose estima-
tion ﬁeld forward is to have competitions similar to what available in object
classiﬁcation, such as PASCAL Visual Object Classes (VOC) [51] and Ima-
geNet [52]. These competitions in object classiﬁcation helped spur contest
between diﬀerent universities that caused improvement in image recogni-
tion algorithms each year and helped increase the training data for object
recognition (now in millions).

9.2 Hand pose

If we have a perfect hand initializer, then we wont need a hand tracker, and if
we have a perfect hand tracker, then we wont need an initializer. However, in
reality we need the initializer for hand tracking loss and the hand tracking to
ﬁne tune the initializer estimate. Therefore, the most promising solution for
hand pose estimation is the one that uses both discriminative and generative
approaches in a single pipeline.

Most of the hand pose estimation algorithms reviewed in this paper,
still use heuristics and magic numbers from intuition. So there is a room
for improvement by ﬁnding the optimium values and replace heuristics with
more theoritically sound algorithms or machine learning. For example, in [3]
all the percentages selected for the PSO+GA algorithm were from intuition

42

and try-and-error. Therefore, focusing on ﬁne tune to parameters might
result better hand pose estimation.

Furthermore, the initializer stage need more attention. The focus should
be to make the per frame hand pose estimation as close as possible to the
ﬁnal result, that will reduce the dependency on the hand tracking part
which is the current performance bottleneck. The reason for that is that
the optimizer inside the hand tracking search a high dimension space for a
hand model that explain the observed data, this hyperdimension space has
a lot of local optima. And in order to evaluate the discrepancy between the
hand model and the observed data, the algorithm need to render the hand
model then apply a complex cost function to do the comparison. Each of
these steps are costly in term of performance and error prone.
The 2 promising direction in the initializer stage are:

• Machine learning (ML): Multi-layers ML algorithms look promis-
ing, by multi-layers we mean coarse to ﬁne tune hand pose estimation
[3, 11, 10, 12]. The ﬁrst layer usually compute global parameters of
the hand such as rotation, orientation and location. Given those pa-
rameters the second layer infer local hand parameters.

• Image search: Content based image retrievel (CBIR) could estimate
discrete poses of the hand [17, 5], then we feed this pose to ML algo-
rithm that predict the ﬁnal hand pose. In concept, CBIR will act as
the ﬁrst layer in multi-layers ML system for hand initializer.

Another important aspect is benchmark, in order to improve on current
algorithms, we need to be able to compare various hand pose estimation
algorithms together using a publicly available benchmark. Moreover, the
benchmark should not be only on the ﬁnal result, it should also cover each
stage.

For segmentation and initializer, the most promising approach is machine
learning (ML) approach that classify each pixel as a hand or not a hand.
Which depend heavily on the availability of large training dataset.

10 Conclusion

Articulated hand pose estimation based on vision without marker is a chal-
lenging and open problem. Hand pose provides a natural interaction in a lot
of important scenarios such as TV, Car, 3D manipulation, Virtual Reality
(VR) and Augmented Reality (AR). There is a need to have a reliable and

43

robust vision based hand pose estimation in real-time under unconstrained
condition, especially with the proliferation of wearable devices.

In this paper, we reviewed various hand pose pipelines and reviewed
in-depth current state-of-the-art hand pose estimation algorithms, we also
looked at each of the hand pipeline stage in detail. Current state-of-the-
art almost solved hand pose estimation challenge for a single isolated non-
wearing gloves hand with somewhat challenging background. Nevertheless,
we are still far away from human level recognition and the ability to infer
hand pose in unconstrained environment, especially the 2 hands interaction
case and hold an object case.

We also show the importance and diﬃculty to have a large training
dataset in order to progress vision based hand pose estimation quality fur-
ther. Furthermore, we discussed which stage in the hand pose pipeline need
more attention and which technique show promising.

Articulated hand pose estimation has the potential to revolutionize the
way we interact with technology, by making the interaction natural and
seamless. And with the recent progress in hand pose estimation, we are
getting closer to achieve this goal.

Acknowledgment

I would like to thank Professor John Ronald Kender for giving me the op-
portunity to investigate in-depth hand pose estimation algorithms and gain
breadth of knowledge for most vision based hand pose estimation.

References

[1] Robert Y. Wang and Jovan Popovi´c. Real-time hand-tracking with a
color glove. In ACM SIGGRAPH 2009 Papers, SIGGRAPH ’09, pages
63:1–63:8, New York, NY, USA, 2009. ACM.

[2] Jonathan Tompson, Murphy Stein, Yann Lecun, and Ken Perlin. Real-
time continuous pose recovery of human hands using convolutional net-
works. ACM Trans. Graph., 33(5):169:1–169:10, September 2014.

[3] Toby Sharp, Cem Keskin, Duncan Robertson, Jonathan Taylor, Jamie
Shotton, David Kim, Christoph Rhemann, Ido Leichter, Alon Vinnikov,
Yichen Wei, Daniel Freedman, Pushmeet Kohli, Eyal Krupka, Andrew
Fitzgibbon, and Shahram Izadi. Accurate, robust, and ﬂexible real-time
hand tracking. CHI, April 2015.

44

[4] Kenji Oka, Yoichi Sato, and Hideki Koike. Real-time tracking of mul-
tiple ﬁngertips and gesture recognition for augmented desk interface
systems. In Proceedings of the Fifth IEEE International Conference on
Automatic Face and Gesture Recognition, FGR ’02, pages 429–, Wash-
ington, DC, USA, 2002. IEEE Computer Society.

[5] Eyal Krupka, Alon Vinnikov, Ben Klein, Aharon Bar-Hillel, Daniel
Freedman, and Simon Stachniak. Discriminative ferns ensemble for
hand pose recognition. In 2014 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-
28, 2014, pages 3670–3677, 2014.

[6] Iasonas Oikonomidis, Nikolaos Kyriazis, and Antonis A. Argyros. Mark-
erless and eﬃcient 26-dof hand pose recovery. In Proceedings of the 10th
Asian Conference on Computer Vision - Volume Part III, ACCV’10,
pages 744–757, Berlin, Heidelberg, 2011. Springer-Verlag.

[7] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A. Argyros. Ef-
ﬁcient model-based 3d tracking of hand articulations using kinect. In
British Machine Vision Conference, BMVC 2011, Dundee, UK, August
29 - September 2, 2011. Proceedings, pages 1–11, 2011.

[8] Sean Ryan Fanello, Cem Keskin, Shahram Izadi, Pushmeet Kohli,
David Kim, David Sweeney, Antonio Criminisi, Jamie Shotton,
Sing Bing Kang, and Tim Paek. Learning to be a depth camera for close-
range human capture and interaction. ACM Trans. Graph., 33(4):86:1–
86:11, July 2014.

[9] Chen Qian, Xiao Sun, Yichen Wei, Xiaoou Tang, and Jian Sun. Real-
time and robust hand tracking from depth. In 2014 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2014, Columbus,
OH, USA, June 23-28, 2014, pages 1106–1113, 2014.

[10] Danhang Tang, Hyung Jin Chang, A. Tejani, and Tae-Kyun Kim. La-
tent regression forest: Structured estimation of 3d articulated hand
posture. In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 3786–3793, June 2014.

[11] Xiao Sun, Yichen Wei, Shuang Liang, Xiaoou Tang, and Jian Sun.

Cascaded hand pose regression. June 2015.

[12] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit. Hands deep in
deep learning for hand pose estimation. CoRR, abs/1502.06807, 2015.

45

[13] Ali Erol, George Bebis, Mircea Nicolescu, Richard D. Boyle, and Xander
Twombly. Vision-based hand pose estimation: A review. Comput. Vis.
Image Underst., 108(1-2):52–73, October 2007.

[14] James Steven Supancic III, Gr´egory Rogez, Yi Yang, Jamie Shotton,
and Deva Ramanan. Depth-based hand pose estimation: methods,
data, and challenges. CoRR, abs/1504.06378, 2015.

[15] Vladimir I. Pavlovic, Rajeev Sharma, and Thomas S. Huang. Visual
interpretation of hand gestures for human-computer interaction: A re-
view. IEEE Trans. Pattern Anal. Mach. Intell., 19(7):677–695, July
1997.

[16] Ying Wu and T.S. Huang. Hand modeling, analysis and recognition.

Signal Processing Magazine, IEEE, 18(3):51–60, May 2001.

[17] Vassilis Athitsos and Stan Sclaroﬀ. Estimating 3d hand pose from
a cluttered image.
In 2003 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 2003), 16-22 June
2003, Madison, WI, USA, pages 432–442, 2003.

[18] Ming-Hsuan Yang, D. Kriegman, and N. Ahuja. Detecting faces in
images: a survey. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 24(1):34–58, Jan 2002.

[19] AntonisA. Argyros and ManolisI.A. Lourakis. Real-time tracking of
multiple skin-colored objects with a possibly moving camera. In Tom
Pajdla and Ji Matas, editors, Computer Vision - ECCV 2004, volume
3023 of Lecture Notes in Computer Science, pages 368–379. Springer
Berlin Heidelberg, 2004.

[20] D. Chai and K.N. Ngan. Locating facial region of a head-and-shoulders
color image. In Automatic Face and Gesture Recognition, 1998. Pro-
ceedings. Third IEEE International Conference on, pages 124–129, Apr
1998.

[21] M.J. Jones and J.M. Rehg. Statistical color models with application
to skin detection. In Computer Vision and Pattern Recognition, 1999.
IEEE Computer Society Conference on., volume 1, pages –280 Vol. 1,
1999.

[22] D. Saxe and R. Foulds. Toward robust skin identiﬁcation in video
images. In Automatic Face and Gesture Recognition, 1996., Proceedings
of the Second International Conference on, pages 379–384, Oct 1996.

46

[23] J.-C. Terrillon, M.N. Shirazi, H. Fukamachi, and S. Akamatsu. Com-
parative performance of diﬀerent skin chrominance models and chromi-
nance spaces for the automatic detection of human faces in color images.
In Automatic Face and Gesture Recognition, 2000. Proceedings. Fourth
IEEE International Conference on, pages 54–61, 2000.

[24] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
A. Kipman, and A. Blake. Real-time human pose recognition in parts
from single depth images. In Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, pages 1297–1304, June 2011.

[25] Cem Keskin, Furkan K$#305;ra$#231;, Yunus Emre Kara, and Lale
Akarun. Hand pose estimation and hand shape classiﬁcation using
multi-layered randomized decision forests. In Proceedings of the 12th
European Conference on Computer Vision - Volume Part VI, ECCV’12,
pages 852–863, Berlin, Heidelberg, 2012. Springer-Verlag.

[26] Andreas Baak, Meinard Mller, Gaurav Bharaj, Hans-Peter Seidel, and
Christian Theobalt. A data-driven approach for real-time full body
pose reconstruction from a depth camera. In Consumer Depth Cam-
eras for Computer Vision, Advances in Computer Vision and Pattern
Recognition, pages 71–98. Springer London, 2013.

[27] C. Plagemann, V. Ganapathi, D. Koller, and S. Thrun. Real-time
identiﬁcation and localization of body parts from depth images.
In
Robotics and Automation (ICRA), 2010 IEEE International Confer-
ence on, pages 3108–3113, May 2010.

[28] Luis Unzueta, Manuel Peinado, Ronan Boulic, and ´Angel Suescun. Full-
body performance animation with sequential inverse kinematics. Graph.
Models, 70(5):87–104, September 2008.

[29] Min Sun, P. Kohli, and J. Shotton. Conditional regression forests for
human pose estimation. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 3394–3401, June 2012.

[30] Jamie Shotton, Toby Sharp, Pushmeet Kohli, Sebastian Nowozin,
John M. Winn, and Antonio Criminisi. Decision jungles: Compact
and rich models for classiﬁcation. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meeting held December 5-8,
2013, Lake Tahoe, Nevada, United States., pages 234–242, 2013.

47

[31] J Canny. A computational approach to edge detection. IEEE Trans.

Pattern Anal. Mach. Intell., 8(6):679–698, June 1986.

[32] J. Taylor, R. Stebbing, V. Ramakrishna, C. Keskin, J. Shotton, S. Izadi,
A. Hertzmann, and A. Fitzgibbon. User-speciﬁc hand modeling from
monocular depth sequences. In Computer Vision and Pattern Recogni-
tion (CVPR), 2014 IEEE Conference on, pages 644–651, June 2014.

[33] J.M. Rehg and T. Kanade. Digiteyes: vision-based hand tracking for
human-computer interaction. In Motion of Non-Rigid and Articulated
Objects, 1994., Proceedings of the 1994 IEEE Workshop on, pages 16–
22, Nov 1994.

[34] Hocine Ouhaddi and Patrick Horain. 3d hand gesture tracking by model
registration. Workshop on Synthetic-Natural Hybrid Coding and Three
Dimensional Imaging, pages 70–73, 1999.

[35] K. Nirei, H. Saito, M. Mochimaru, and S. Ozawa. Human hand tracking
from binocular image sequences. In Industrial Electronics, Control, and
Instrumentation, 1996., Proceedings of the 1996 IEEE IECON 22nd
International Conference on, volume 1, pages 297–302 vol.1, Aug 1996.

[36] M. Bray, E. Koller-Meier, P. Muller, L. Van Gool, and N.N. Schrau-
dolph. 3d hand tracking by rapid stochastic gradient descent using
a skinning model.
In Visual Media Production, 2004. (CVMP). 1st
European Conference on, pages 59–68, March 2004.

[37] B. Stenger, P.R.S. Mendonca, and R. Cipolla. Model-based 3d tracking
of an articulated hand. In Computer Vision and Pattern Recognition,
2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society
Conference on, volume 2, pages II–310–II–315 vol.2, 2001.

[38] James Kennedy and Russell C. Eberhart. Particle swarm optimiza-
tion. In Proceedings of the IEEE International Conference on Neural
Networks, pages 1942–1948, 1995.

[39] James Kennedy, Russell C Eberhart, and Y Shi. Swarm intelligence.

2001. Kaufmann, San Francisco, 1:700–720, 2001.

[40] Paul J. Besl and Neil D. McKay. A method for registration of 3-d
shapes. IEEE Trans. Pattern Anal. Mach. Intell., 14(2):239–256, Febru-
ary 1992.

48

[41] Szymon Rusinkiewicz and Marc Levoy. Eﬃcient variants of the ICP
algorithm. In Third International Conference on 3D Digital Imaging
and Modeling (3DIM), June 2001.

[42] S. Pellegrini, K. Schindler, and D. Nardi. A generalisation of the
icp algorithm for articulated bodies.
In Proceedings of the British
Machine Vision Conference, pages 87.1–87.10. BMVA Press, 2008.
doi:10.5244/C.22.87.

[43] David E. Goldberg. Genetic Algorithms in Search, Optimization and
Machine Learning. Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 1st edition, 1989.

[44] Chia-Feng Juang. A hybrid of genetic algorithm and particle swarm
optimization for recurrent network design. Trans. Sys. Man Cyber.
Part B, 34(2):997–1006, April 2004.

[45] T. Yasuda, K. Ohkura, and Y. Matsumura. Extended pso with partial
randomization for large scale multimodal problems. In World Automa-
tion Congress (WAC), 2010, pages 1–6, Sept 2010.

[46] Paul Tseng. Fortiﬁed-descent simplicial search method: A general ap-

proach. SIAM Journal on Optimization, 10(1):269–288, 1999.

[47] Ying Wu, J.Y. Lin, and T.S. Huang. Capturing natural hand articula-
tion. In Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE
International Conference on, volume 2, pages 426–432 vol.2, 2001.

[48] Gr´egory Rogez, James Steven Supancic III, Maryam Khademi, Jos´e
Mar´ıa Mart´ınez Montiel, and Deva Ramanan. 3d hand pose detection
in egocentric RGB-D images. CoRR, abs/1412.0065, 2014.

[49] Marin ˇSari´c. Libhand: A library for hand articulation, 2011. Version

0.9.

[50] Stan Melax, Leonid Keselman, and Sterling Orsten. Dynamics based
3d skeletal hand tracking. In Proceedings of Graphics Interface 2013,
GI ’13, pages 63–70, Toronto, Ont., Canada, Canada, 2013. Canadian
Information Processing Society.

[51] Mark Everingham, Luc Van Gool, ChristopherK.I. Williams, John
Winn, and Andrew Zisserman. The pascal visual object classes (voc)
challenge. International Journal of Computer Vision, 88(2):303–338,
2010.

49

[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision (IJCV), 2015.

50

