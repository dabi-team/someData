1
2
0
2

t
c
O
3
2

]

C
O
.
h
t
a
m

[

1
v
1
8
2
2
1
.
0
1
1
2
:
v
i
X
r
a

On Seven Fundamental Optimization Challenges

in Machine Learning

Dissertation by

Konstantin Mishchenko

In Partial Fulﬁllment of the Requirements

For the Degree of

Doctor of Philosophy

King Abdullah University of Science and Technology

Thuwal, Kingdom of Saudi Arabia

October 2021

 
 
 
 
 
 
2

EXAMINATION COMMITTEE PAGE

The dissertation of Konstantin Mishchenko is approved by the examination committee

Committee Chairperson: Peter Richt´arik

Committee Members: Lawrence Carin, Bernard Ghanem, Suvrit Sra, Wotao Yin

3

© October 2021

Konstantin Mishchenko

All Rights Reserved

4

ABSTRACT

On Seven Fundamental Optimization Challenges in Machine Learning

Konstantin Mishchenko

Many recent successes of machine learning went hand in hand with advances in opti-
mization. The exchange of ideas between these ﬁelds has worked both ways, with machine
learning building on standard optimization procedures such as gradient descent, as well as
with new directions in the optimization theory stemming from machine learning applica-
tions. In this thesis, we discuss new developments in optimization inspired by the needs
and practice of machine learning, federated learning, and data science. In particular, we
consider seven key challenges of mathematical optimization that are relevant to modern
machine learning applications, and develop a solution to each.

Our ﬁrst contribution is the resolution of a key open problem in Federated Learning:
we establish the ﬁrst theoretical guarantees for the famous Local SGD algorithm in the
crucially important heterogeneous data regime. As the second challenge, we close the gap
between the upper and lower bounds for the theory of two incremental algorithms known
as Random Reshuﬄing (RR) and Shuﬄe-Once that are widely used in practice, and in fact
set as the default data selection strategies for SGD in modern machine learning software.
Our third contribution can be seen as a combination of our new theory for proximal RR and
Local SGD yielding a new algorithm, which we call FedRR. Unlike Local SGD, FedRR is the
ﬁrst local ﬁrst-order method that can provably beat gradient descent in communication
complexity in the heterogeneous data regime. The fourth challenge is related to the class
of adaptive methods.
In particular, we present the ﬁrst parameter-free stepsize rule for
gradient descent that provably works for any locally smooth convex objective. The ﬁfth
challenge we resolve in the aﬃrmative is the development of an algorithm for distributed
optimization with quantized updates that preserves global linear convergence of gradient
descent. Finally, in our sixth and seventh challenges, we develop new VR mechanisms
applicable to the non-smooth setting based on proximal operators and matrix splitting.

In all cases, our theory is simpler, tighter and uses fewer assumptions than the prior
literature. We accompany each chapter with numerical experiments to show the tightness
of the proposed theoretical results.

5

ACKNOWLEDGEMENTS

First and foremost, I would like to thank my supervisor Peter Richt´arik for his continous
guidance throughout my PhD. The discussions and projects that I had with him helped
me shape my research interests and navigate my eﬀorts towards a combination of rigor,
simplicity and mathematical beauty, which resulted in many papers that I can be proud
of. Above all, with Peter Richt´arik’s help, I quickly became able to work independently
and collaborate with people from diﬀerent countries and backgrounds, which lays a solid
foundation for my future work.

I am grateful to my coauthors for making this thesis happen. I cannot imagine myself
achieving the same research progress without the help of the people that I worked with.
I was lucky to have had many short and long research visits, and many eye-opening

and inspiring discussions. I would like to particularly thank:
J´erˆome Malick and Franck Iutzeler for hosting me at Universit´e Grenoble Alpes when I
was writing my MSc thesis in France.
Carola-Bibiane Sch¨onlieb for hosting me at the University of Cambridge two times.
Matthias Ehrhardt who hosted me at the University of Bath.
Mert G¨urb¨uzbalaban for hosting me at Rutgers University.
Aryan Mokhtari for hosting me at MIT.
Federico Vaggi for hosting me as an intern at Amazon.
Dmitriy Drusvyatskiy for letting me visit the University of Washington.
Lin Xiao for my short visit to Microsoft Research at Redmond.
Martin Jaggi for hosting me at EPFL.
Stephen Boyd for hosting me at Stanford University.
Alexander Gasnikov for hosting me at the Moscow Institute of Physics and Technology.
Courtney Paquette and Nicolas Le Roux for hosting my internship at Google Brain. I am
also thankful to Fabian Pedregosa for his support before, during and after the internship.
I am very happy that my PhD allowed me to meet many brilliant people. I would not
have succeeded in my projects without the help of Laurent Condat, Eduard Gorbunov, Filip
Hanzely, Samuel Horv´ath, Ahmed Khaled, Dmitry Kovalev, Yura Malitsky, Xun Qian, Adil
Salim, Alibek Sailanbayev, Egor Shulgin, Martin Tak´aˇc, and Bokun Wang.
I was lucky
to have had discussions with many more great minds, including Ahmet Alacaoglu, L´eon
Bottou, Aaron Defazio, Sai Praneeth Karimireddy, James Martens, Dmitrii Ostrovskii,
Boris Polyak, Sebastian U. Stich, Junzi Zhang, and Nikita Zhivotovskiy.
I also wish to
say a big “Thank you!” to Donald Goldfarb for his kind words about my work.

It was also a great pleasure to be part of our research group and I thank its past
and current members for many interesting conversations: El Houcine Bergou, Konstantin
Burlachenko, Aritra Dutta, Elnur Gasanov, Robert M. Gower, Slavom´ır Hanzely, Jakub
Koneˇcn´y, Zhize Li, Nicolas Loizou, Grigory Malinovsky, Mher Safaryan, and Igor Sokolov.
Finally, I would like to thank Olivier Bousquet who recommended me to “start with a

PhD no matter what” when I was in doubt about my future.

6

TABLE OF CONTENTS

Examination Committee Page

Copyright

Abstract

Acknowledgements

Table of Contents

List of Abbreviations

List of Figures

List of Tables

1 Introduction

Provably Works

1.1 Challenge 1: Convergence of Local SGD for Federated Learning in the
Heterogeneous Data Regime . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Challenge 2: Convergence of Random Reshuﬄing . . . . . . . . . . . . .
1.3 Challenge 3: Going Beyond Local SGD in Federated Learning . . . . . .
1.4 Challenge 4: The First Adaptive Stepsize Rule for Gradient Descent that
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Challenge 5: Achieving Fast Rates in Distributed Optimization with Quan-
tization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 Challenge 6: Developing Variance Reduction for Proximal Operators . . .
1.7 Challenge 7: Designing Variance-Reduced Algorithms with Splitting . . .
1.8 Excluded papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.9 Theoretical Background for Most Chapters . . . . . . . . . . . . . . . .
1.10 Basic Facts and Notation . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.1 Random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.2 Norms and products . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.3 Function properties
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.4 Bregman divergence . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.5 Proximal operator
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.10.6 Monotone operators . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

3

4

5

6

13

14

18

20

22
24
26

27

29
30
32
33
33
34
34
35
35
36
38
39

1.10.7 Diﬀerences in section-speciﬁc notation . . . . . . . . . . . . . . . . .

40

7

2 Convergence of Local SGD for Federated Learning in the Heteroge-

neous Data Regime
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Related Work
Local SGD with identical data . . . . . . . . . . . . . . . . . . . . . .
2.2.1
. . . . . . . . . . . . . . . . . .
Local SGD with heterogeneous data
2.2.2
2.3 Settings and Contributions . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Convergence Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Identical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1
2.4.2 Heterogeneous data . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1 Variance measures . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Identical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2
2.5.3 Heterogeneous data . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Convergence of Random Reshuﬄing

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1
3.2 Related Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Settings and Contributions . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 New and improved convergence rates for RR, SO and IG . . . . . . . .
3.3.2 More general assumptions on the function class
. . . . . . . . . . . .
3.4 Convergence Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 Main result: strongly convex objectives . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
3.4.2 Non-strongly convex objectives
3.4.3 Non-convex objectives . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 Experiments

4 Going Beyond Local SGD in Federated Learning

4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Convergence Theory for Strongly Convex Losses f1, . . . , fn . . . . . . . .
4.6 Convergence Theory for Strongly Convex Regularizer ψ . . . . . . . . .
4.7 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.8 Federated Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.9 Experiments

5 The First Adaptive Stepsize Rule for Gradient Descent that Provably

Works
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1
5.2 Related Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Convergence Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43
43
45
45
46
47
49
50
53
55
55
55
56

57
57
58
59
60
60
61
61
65
66
68

69
69
70
71
73
74
76
77
79
82

84
84
85
87

8

Local smoothness of f . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3.1
. . . . . . . . . . . . . . . . . . . . . . . .
5.3.2 Analysis without descent
5.3.3
f is locally strongly convex . . . . . . . . . . . . . . . . . . . . . . .
5.4 Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.1 Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.2 Uniting our steps with stochastic gradients . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Experiments

87
88
91
91
92
93
94

6 Achieving Fast Rates in Distributed Optimization with Quantization

98
98
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1
99
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Related Work
6.3 Settings and Contributions . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.4 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.5 Convergence Theory for Strongly Convex Losses
. . . . . . . . . . . . . 103
6.5.1 Decreasing stepsizes . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.6 Convergence Theory for Non-Convex Losses
. . . . . . . . . . . . . . . 106
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.7 Experiments

7 Developing Variance Reduction for Proximal Operators

110
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.1
7.2 Related Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.3 Settings and Contributions . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.4 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
7.5 Gradient Estimators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.6 Convergence Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.6.1 O( 1
K ) convergence for general convex problem . . . . . . . . . . . . . 117
7.6.2 O( 1
K2 ) convergence for strongly convex f . . . . . . . . . . . . . . . . 118
Linear convergence for linear non-smoothness . . . . . . . . . . . . . . 118
7.6.3
Linear convergence if all gj are smooth . . . . . . . . . . . . . . . . . 119
7.6.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
7.7 Experiments

8 Designing Variance-Reduced Algorithms with Splitting

122
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.1
8.2 Related Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.3 Primal–Dual Formulations and Optimality Conditions . . . . . . . . . . . 125
8.4 Davis–Yin Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
8.5 Primal–Dual Optimization Algorithms . . . . . . . . . . . . . . . . . . . 127
8.5.1 A new primal–dual algorithm: the PDDY algorithm . . . . . . . . . . 128
8.5.2 The PD3O algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 128
8.5.3 The Condat–V˜u algorithm . . . . . . . . . . . . . . . . . . . . . . . . 129
8.6 Stochastic Primal–Dual Algorithms: Non-Asymptotic Analysis . . . . . . 129
8.6.1 The Stochastic PD3O algorithm . . . . . . . . . . . . . . . . . . . . 131
8.6.2 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Linearly constrained or decentralized optimization . . . . . . . . . . . 132
8.6.3

9

9 Concluding Remarks

133
9.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
9.2 Future Research Work . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
Federated learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
9.2.1
9.2.2 Random Reshuﬄing . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
9.2.3 Variance reduction and splitting . . . . . . . . . . . . . . . . . . . . . 135
9.2.4 Adaptive methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
9.2.5 Communication eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . 135

References

Appendices

A Tables of Frequently Used Notation

137

167

167

B Appendix for Chapter 2

170
B.1 Proofs for Identical Data under Assumption 2.3.2 . . . . . . . . . . . . . 170
B.1.1 Proof of Lemma 2.4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 170
B.1.2 Two more lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
B.1.3 Proof of Theorem 2.4.2 . . . . . . . . . . . . . . . . . . . . . . . . . 173
B.1.4 Proof of Theorem 2.4.4 . . . . . . . . . . . . . . . . . . . . . . . . . 174
B.2 Proofs for Identical Data under Assumption 2.3.3 . . . . . . . . . . . . . 175
B.2.1 Preliminary lemmas
. . . . . . . . . . . . . . . . . . . . . . . . . . . 175
B.2.2 Proof of Theorem 2.4.6 . . . . . . . . . . . . . . . . . . . . . . . . . 183
B.2.3 Proof of Theorem 2.4.8 . . . . . . . . . . . . . . . . . . . . . . . . . 185
B.3 Proofs for Heterogeneous Data . . . . . . . . . . . . . . . . . . . . . . 186
B.3.1 Preliminary lemmas
. . . . . . . . . . . . . . . . . . . . . . . . . . . 186
B.3.2 Proof of Theorem 2.4.10 . . . . . . . . . . . . . . . . . . . . . . . . 193
B.4 Extra Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
. . . . . . . . . . . . . . . . 195
B.5 Discussion of Dieuleveut and Patel (2019)

C Appendix for Chapter 3

197
C.1 Additional Experiment Details . . . . . . . . . . . . . . . . . . . . . . . 197
C.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
C.3 A Lemma for Sampling without Replacement . . . . . . . . . . . . . . . 198
. . . . . . . . . 199
C.4 Proofs for Convex Objectives (Sections 3.4.1 and 3.4.2)
C.4.1 Proof of Proposition 3.4.3 . . . . . . . . . . . . . . . . . . . . . . . . 199
C.4.2 Proof remainder for Theorem 3.4.4 . . . . . . . . . . . . . . . . . . . 200
C.4.3 Proof of complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
C.4.4 Two lemmas for Theorems 3.4.5 and 3.4.6 . . . . . . . . . . . . . . . 202
C.4.5 Proof of Theorem 3.4.5 . . . . . . . . . . . . . . . . . . . . . . . . . 205
C.4.6 Proof of Theorem 3.4.6 . . . . . . . . . . . . . . . . . . . . . . . . . 206
C.4.7 Proof of complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
C.5 Proofs for Non-Convex Objectives (Section 3.4.3)
. . . . . . . . . . . . 208
C.5.1 Proof of Proposition 3.4.8 . . . . . . . . . . . . . . . . . . . . . . . . 208

10

C.5.2 Finding a per-epoch recursion . . . . . . . . . . . . . . . . . . . . . . 208
C.5.3 Bounding the backward per-epoch deviation . . . . . . . . . . . . . . 210
C.5.4 A lemma for solving the non-convex recursion . . . . . . . . . . . . . 211
C.5.5 Proof of Theorem 3.4.9 . . . . . . . . . . . . . . . . . . . . . . . . . 212
C.5.6 Proof of complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
C.6 Convergence Results for IG . . . . . . . . . . . . . . . . . . . . . . . . 215
C.6.1 Preliminary lemmas for Theorem C.6.1 . . . . . . . . . . . . . . . . . 217
C.6.2 Proof of Theorem C.6.1 . . . . . . . . . . . . . . . . . . . . . . . . . 221

D Appendix for Chapter 4

224
D.1 Proof of Theorem 4.4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . 224
D.2 Main Convergence Proofs . . . . . . . . . . . . . . . . . . . . . . . . . 224
D.2.1 A key lemma for shuﬄing-based methods . . . . . . . . . . . . . . . . 224
D.2.2 Proof of Theorem 4.5.1 . . . . . . . . . . . . . . . . . . . . . . . . . 226
D.2.3 Proof of Theorem 4.6.1 . . . . . . . . . . . . . . . . . . . . . . . . . 227
D.3 Convergence of SGD (Proof of Theorem 4.5.2) . . . . . . . . . . . . . . 228
D.4 Proofs for Decreasing Stepsize . . . . . . . . . . . . . . . . . . . . . . . 230
D.4.1 Proof of Theorem 4.7.1 . . . . . . . . . . . . . . . . . . . . . . . . . 233
D.5 Proof of Theorem 4.7.2 for Importance Resampling . . . . . . . . . . . . 233
D.6 Proofs for Federated Learning . . . . . . . . . . . . . . . . . . . . . . . 234
D.6.1 Lemma for the extended proximal operator . . . . . . . . . . . . . . . 234
D.6.2 Proof of Lemma 4.8.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 234
D.6.3 Proof of Lemma 4.8.2 . . . . . . . . . . . . . . . . . . . . . . . . . . 235
D.6.4 Proof of Theorem 4.8.3 . . . . . . . . . . . . . . . . . . . . . . . . . 235
D.6.5 Proof of Theorem 4.8.4 . . . . . . . . . . . . . . . . . . . . . . . . . 236
D.7 Federated Experiments and Experimental Details . . . . . . . . . . . . . 236

E Appendix for Chapter 5

238
E.1 Missing Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
E.2 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
E.2.1 More general update . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
f is L-smooth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
E.2.2
E.3 Stochastic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
E.3.1 Diﬀerent samples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
E.3.2 Same sample: overparameterized models . . . . . . . . . . . . . . . . 247
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
E.4 Experiments Details

F Appendix for Chapter 6

249
F.1 Block p-Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
F.2 Proof of Theorem 6.4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . 250
F.3 Proof of Lemma 6.5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
F.4 Strongly Convex Case: Optimal Number of Nodes
. . . . . . . . . . . . 252
F.5 Quantization Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
F.6 Proof of Theorem 6.5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . 256
F.7 Proof of Corollary 6.5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . 260

11

F.8 Strongly Convex Case: Decreasing Stepsize . . . . . . . . . . . . . . . . 260
F.9 Non-Convex Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
F.10 Momentum Version of DIANA . . . . . . . . . . . . . . . . . . . . . . . 265
F.11 Analysis of DIANA with α = 0 and h0
i = 0 . . . . . . . . . . . . . . . . 271
F.11.1 Convergence Rate of TernGrad . . . . . . . . . . . . . . . . . . . . . 271
F.11.2 Technical lemmas
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
F.11.3 Non-convex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
F.11.4 Momentum version . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
F.11.5 Strongly convex analysis . . . . . . . . . . . . . . . . . . . . . . . . . 279
F.11.6 Decreasing stepsize
. . . . . . . . . . . . . . . . . . . . . . . . . . . 281
F.11.7 Performance of DIANA, QSGD and TernGrad on the Rosenbrock function282
F.11.8 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
F.11.9 MPI: broadcast, reduce and gather
. . . . . . . . . . . . . . . . . . . 283
F.11.10Performance of GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
F.11.11DIANA vs. TernGrad, SGD and QSGD . . . . . . . . . . . . . . . . . 286
F.11.12Computational Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . 286

G Appendix for Chapter 7

293
G.1 Applications
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
G.1.1 Constrained optimization . . . . . . . . . . . . . . . . . . . . . . . . 293
G.1.2 Dantzig selector
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
G.1.3 Decentralized optimization . . . . . . . . . . . . . . . . . . . . . . . 294
G.1.4 Support-vector machine (SVM) . . . . . . . . . . . . . . . . . . . . . 295
G.1.5 Overlapping group Lasso
. . . . . . . . . . . . . . . . . . . . . . . . 295
G.1.6 Fused Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
G.1.7 Square-root Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
G.2 Relation to Existing Methods
. . . . . . . . . . . . . . . . . . . . . . . 297
G.2.1 SDCA, Dykstra’s algorithm and the Kaczmarz method . . . . . . . . . 297
G.2.2 Accelerated Kaczmarz . . . . . . . . . . . . . . . . . . . . . . . . . . 298
G.2.3 ADMM and Douglas–Rachford Splitting . . . . . . . . . . . . . . . . 298
G.2.4 Point–SAGA, SAGA, SVRG and Proximal GD . . . . . . . . . . . . . 299
. . . . . . . . . . . . . . . . 299
G.2.5 Stochastic Primal–Dual Hybrid Gradient
G.3 Evaluating Proximal Operators
. . . . . . . . . . . . . . . . . . . . . . 301
G.4 Optimality Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
G.5 Convergence Proofs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
. . . . . . . . . . . . . . . 304
G.5.1 Proof of Lemma 7.5.2 (Gradient Descent)
G.5.2 Key lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
G.5.3 Convergence of Bregman divergence to 0 almost surely
. . . . . . . . 307
(cid:1) rate) . . . . . . . . . . . . . . . . . . 308
G.5.4 Proof of Theorem 7.6.1 (O (cid:0) 1
K
G.5.5 Proof of Theorem 7.6.3 (O( 1
K2 ) rate) . . . . . . . . . . . . . . . . . . 308
G.5.6 Proof of Theorem 7.6.4 (O( 1
K ) rate of SGD) . . . . . . . . . . . . . . 310
G.5.7 Proof of Theorem 7.6.5 (linear rate for gj = φj(A(cid:62)
. . . . . . . . 312
j x))
G.5.8 Proof of Theorem 7.6.7 (linear constraints) . . . . . . . . . . . . . . . 314
. . . . . . . . . . . . . . . . . . 315
G.5.9 Proof of Theorem 7.6.8 (smooth gj)
. . . . . . . . . . . . . . . 316
G.5.10 Proof of Corollary 7.6.9 (optimal stepsize)

12

. . . . . . . . . . . . . . . 317
G.5.11 Proof of Lemma 7.5.3 (SVRG and SAGA)
G.5.12 Proof of Lemma 7.5.4 (SGD) . . . . . . . . . . . . . . . . . . . . . . 321
. . . . . . . . . . . . . . . . . . . . . . . . . . 321
G.6 Additional Experiments

H Appendix for Chapter 8

323
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
H.1 Experiments
. . . . . . . . . . . . . . . . 324
H.2 Proofs Related to Primal–Dual Optimality
. . . . . . . . . . . . . . . . . . . . . . . . . . 324
H.2.1 Optimality conditions
H.2.2 Proof of Lemma 8.3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 325
H.3 Proof of Lemma 8.4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
. . . . . . . . . . . 327
H.4 Proofs Related to the PDDY and PD3O Algorithms
H.4.1 Resolvent calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
H.5 Proofs Related to the Condat–V˜u Algorithm . . . . . . . . . . . . . . . 328
H.5.1 Resolvent calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
. . . . . . 329
H.5.2 Algorithm 3.2 of [52] as an instance of Davis–Yin Splitting
H.5.3 Algorithm 3.1 of [52] as an instance of Davis–Yin Splitting
. . . . . . 329
H.5.4 Cocoercivity parameter of Q−1C . . . . . . . . . . . . . . . . . . . . 330
. . . . . . . . . . . . . . . . . . . . . . . . 330
H.6 Linear Convergence Results
H.6.1 The Stochastic PD3O algorithm . . . . . . . . . . . . . . . . . . . . 331
H.6.2 The Stochastic PDDY algorithm . . . . . . . . . . . . . . . . . . . . 332
H.7 Proofs Related to the Stochastic PDDY Algorithm . . . . . . . . . . . . 332
H.7.1 Proof of Theorem 8.6.4 . . . . . . . . . . . . . . . . . . . . . . . . . 335
H.7.2 Proof of Theorem H.6.2 . . . . . . . . . . . . . . . . . . . . . . . . . 336
H.8 Proofs Related to the Stochastic PD3O algorithm . . . . . . . . . . . . 336
H.8.1 Proof of Theorem 8.6.3 . . . . . . . . . . . . . . . . . . . . . . . . . 340
H.8.2 Proof of Theorem H.6.1 . . . . . . . . . . . . . . . . . . . . . . . . . 340
H.9 Convergence Results for LiCoSGD . . . . . . . . . . . . . . . . . . . . . 341
H.9.1 Proof of Theorem 8.6.5 . . . . . . . . . . . . . . . . . . . . . . . . . 342
H.10 Application of PriLiCoSGD to Stochastic Decentralized Optimization . . 343

I Published Papers

J Preprints

346

347

13

LIST OF ABBREVIATIONS

Alternating Direction Method of Multipliers

ADMM
AdGD-accel Adaptive Accelerated Gradient Descent
Adaptive Gradient Descent
AdGD
Adaptive Stochastic Gradient Descent
AdSGD
Barzilai–Borwein
BB
Douglas–Rachford
DR
Davis–Yin Splitting
DYS
Empirical Risk Minimization
ERM
Federated Learning
FL
Federated Random Reshuﬄing
FedRR
Gradient Descent
GD
Incremental Average Gradient
IAG
Incremental Gradient
IG
Least absolute shrinkage and selection operator
Lasso
Linearly Constrained Stochastic Gradient De-
LiCoSGD
scent
Message Passing Interface
Primal–Dual Davis–Yin
Primal–Dual Hybrid Gradient

MPI
PDDY
PDHG
PriLiCoSGD Primal Linearly Constrained Stochastic Gradient

ProxRR
QSGD
RR
SAG
SDCA
SDM
SGD
SO
SVM
SVRG
VR

Descent
Proximal Random Reshuﬄing
Quantized Stochastic Gradient Descent
Random Reshuﬄing
Stochastic Average Gradient
Stochastic Dual Coordinate Ascent
Stochastic Decoupling Method
Stochastic Gradient Descent
Shuﬄe-Once
Support-Vector Machine
Stochastic Variance Reduced Gradient
Variance Reduction

14

LIST OF FIGURES

1.1

Illustration of property (1.25) with characteristic function of a linear sub-
space, ψ(x) = χ{x : a(cid:62)x=b}. In this case the proximal operator returns the
projection of a point onto the subspace, and Inequality (1.25) becomes
identity and follows from Pythagorean theorem. . . . . . . . . . . . . . .

opt.

opt is the same as σ2

2.2 Results on ‘a9a’ dataset, with stepsize 1

2.1 The eﬀect of the dataset and number of workers M on the variance pa-
rameters. Left: ‘a8a’, middle: ‘mushrooms’, right: ‘w8a’ dataset. We use
uniform sampling of data points, so σ2
dif with M = 1,
while for higher values of M the value of σ2
dif might be drastically larger
than σ2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
L . For any value of local iterations
H the method converged to a neighborhood within a small number of
communication rounds due to large stepsizes. . . . . . . . . . . . . . . .
2.3 Convergence on heterogeneous data with diﬀerent number of local steps
on the ‘a5a’ dataset. 1 local step corresponds to fully synchronized gradi-
ent descent. Left: convergence in terms of communication rounds, which
shows a clear advantage of Local GD when only limited accuracy is re-
quired. Mid plot: wall-clock time might improve only slightly if communi-
cation is cheap. Right: what changes with diﬀerent communication cost.
2.4 Convergence of Local SGD on heterogeneous data with diﬀerent number
. . . . . . . . . . . . . . . . . . . .

of local steps on the ‘a5a’ dataset.

3.1 Top:

3.2 Estimated variance at the optimum, σ2

i − x(cid:63)(cid:107)2, right: convergence of SO with diﬀerent permutations .

‘w8a’
‘real-sim’ dataset (N = 72, 309; d = 20, 958), middle row:
dataset (N = 49, 749; d = 300), bottom: ‘RCV1’ dataset (N = 804, 414;
d = 47, 236). Left: convergence of f (xk
i ), middle column: convergence of
(cid:107)xk
. . .
(cid:63), for the ‘w8a’ dataset.
Left: the values of variance for diﬀerent mini-batch sizes with γ = 1
L.
Middle: variance with ﬁxed mini-batch size 64 for diﬀerent γ, starting
with γ = 1
L . Right: the empirical distribution
of σ2
L and mini-batch
size 64. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Shuﬄe for 500, 000 sampled permutations with γ = 1

L and ending with γ = 10−4

Shuﬄe and σ2

39

52

54

54

55

67

68

15

4.1 Experimental results for problem (4.14). The ﬁrst two plots show with
average and conﬁdence intervals estimated on 20 random seeds and clearly
demonstrate that one can save a lot of proximal operator computations
In the last plot, we show convergence oscillations of
with our method.
ProxSO by sampling 20,000 permutations and choosing the best and the
. . . . . . . . . . . . . . . . . . . . . .
worst performing among them.

82

. . . . . . . . . . . . . . . .

95
5.1 Results for the logistic regression problem.
5.2 Results for matrix factorization. The objective is neither convex nor smooth. 95
96
. . . .
5.3 Results for the non-smooth subproblem from cubic regularization.
5.4 Additional results for the logistic regression problem.
96
. . . . . . . . . . .
5.5 Results for training ResNet-18 on CIFAR-10. Labels for AdGD correspond
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .

5.6 Results for training DenseNet-121 on CIFAR-10.

to how γk was estimated.

97
97

6.1 Comparison of DIANA (β = 0.95) with QSGD, TernGrad and DQGD on

the logistic regression problem for the “mushrooms” dataset.

. . . . . . 108

6.2 Comparison of performance (images/second) for various number of GPUs/MPI
processes and sparse communication DIANA (2bit) vs. Reduce with 32bit
ﬂoat (FP32). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

6.3 Evolution of training (left) and testing (right) accuracy on CIFAR-10, using
4 algorithms: DIANA, SGD, QSGD and TernGrad. We have chosen the
best runs over all tested hyper-parameters.

. . . . . . . . . . . . . . . . 109

7.1 Left: convergence of the Stochastic Decoupling method, Kaczmarz and
accelerated Kaczmarz of [83] when solving Wx = b with random positive-
deﬁnite W ∈ Rd×d, where d = 100. It is immediate to observe that the
method we propose performs on a par with the accelerated Sketch-and-
Project. Right: linear regression with A9a dataset from LIBSVM [44] with
ﬁrst 50 observation used as linear constraints. We compare convergence of
SVRG, SAGA and SGD with full projections (labeled as ’SVRG’, ’SAGA’,
’SGD’) to the same methods combined with Algorithm 11 (labeled as
’Double-’).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

B.1 Results on regularized logistic regression with shared data, ‘a9a’ dataset,
L . With more local iterations, fewer communication rounds

with stepsize 0.05
are required to get to a neighborhood of the solution.

. . . . . . . . . . 195
B.2 Same experiment as in Figure 2.3, performed on the ‘mushrooms’ dataset. 196

D.1 Experimental results for parallel training. Left: comparison in terms of

communication rounds, right: in terms of data passes

. . . . . . . . . . 237

F.1 Illustration of the workings of DIANA, QSGD and TernGrad on the Rosen-

brock function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282

F.2 Comparison of the inﬂuence of the block sizes on convergence for ”mush-

rooms” (ﬁrst row), ”a5a” (second row) datasets. . . . . . . . . . . . . . 284

16

F.3 Typical communication cost using broadcast, reduce and gather for 64
and 32 FP using 4 (solid) resp 128 (dashed) MPI processes. See suppl.
material for details about the network.

. . . . . . . . . . . . . . . . . . 285

F.4 Time to communicate a vectors with diﬀerent lengths for diﬀerent methods
as a function of # of MPI processes. One can observe that Gather 2bit
is not having nice scaling. We also show that the proposed Multi-Gather
communication still achieves a nice scaling when more MPI processes are
used.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285

F.5 The duration of communication for MPI Broadcast, MPI Reduce and MPI
Gather. We show how the communication time depends on the size of the
vector in Rd (x-axis) for various # of MPI processes. In this experiment,
we have run 4 MPI processes per computing node. For Broadcast and
Reduce we have used a single precision ﬂoating point number. For Gather
we used 2bits per dimension. For longer vectors and large number of MPI
processes, one can observe that Gather has a very weird scaling issue. It
turned out to be some weird behaviour of Cray-MPI implementation.
F.6 The duration of communication for MPI Broadcast, MPI Reduce for single
precision (FP32) and double precision (FP64) ﬂoating numbers. We show
how the communication time depends on the size of the vector in Rd (x-
axis) for various # of MPI processes.
In this experiment, we have run 4
MPI processes per computing node. We have used Cray implementation
of MPI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288

. . 287

F.7 The performance (images/second) of NVIDIA Tesla P100 GPU on 4 dif-
ferent problems as a function of batch size. We show how diﬀerent choice
of batch size aﬀects the speed of function evaluation and gradient evalua-
tion. For vgg a, we have run out of memory on GPU for batch size larger
than 128 (gradient evaluation) and 256 for function evaluation. Clearly,
this graph suggest that choosing small batch size leads to small utilization
of GPU. Note that using larger batch size do not necessary reduce the
training process.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289

F.8 Comparison of performance (images/second) for various number of GPUs/MPI
processes and sparse communication DIANA (2bit) vs. Reduce with 32bit
ﬂoat (FP32). We have run 4 MPI processes on each node. Each MPI
process is using single P100 GPU. Note that increasing MPI from 4 to 8
will not bring any signiﬁcant improvement for FP32, because with 8 MPI
processes, communication will happen between computing nodes and will
be signiﬁcantly slower when compare to the single node communication
with 4MPI processes.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 290

F.9 Evolution of training and testing accuracy for 3 diﬀerent problems, using
4 algorithms: DIANA, SGD, QSGD and TernGrad. We have chosen the
best runs over all tested hyper-parameters.

. . . . . . . . . . . . . . . . 291

F.10 Evolution of sparsity of the quantized gradient for 3 diﬀerent problems and

3 algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292

17

F.11 Comparison of a time needed to update weights after a reduce vs. the time
needed to update the weights when using a sparse update from DIANA
using 4-128 MPI processes and 10% sparsity.

. . . . . . . . . . . . . . 292

G.1 Comparison of SVRG with precise projection onto all constraints (labeled

as ’SVRG’) to our stochastic version of SVRG (labeled as ’Double-SVRG’). 322

H.1 Results for the PCA-Lasso experiment. Left: convergence in the objective,

middle: convergence in norm, right: the eﬀect of the stepsizes.

. . . . . 324

H.2 Results for the MNIST experiment. Left: convergence in the objective,

middle: convergence in norm, right: the eﬀect of the stepsizes.

. . . . . 324

H.3 Results for the Fused Lasso experiment. Left: convergence with respect
to the objective function, middle: convergence in norm, right: illustration
of the eﬀect of the stepsizes.

. . . . . . . . . . . . . . . . . . . . . . . 325

18

LIST OF TABLES

1.1 A summary of the results obtained in this thesis.

. . . . . . . . . . . . .

42

2.1 Existing theoretical bounds for Local SGD for identical data with convex
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Existing theoretical bounds for Local SGD with heterogeneous data. . . .

objectives.

3.1 Number of individual gradient evaluations needed by RR to reach an ε-
accurate solution (deﬁned in Section 3.4). Logarithmic factors and con-
stants that are not related to the assumptions are ignored. For non-convex
objectives, A and B are the constants given by Assumption 3.4.7. . . . .

45
46

62

6.1 Comparison of DIANA and related methods. Here “lin. rate” means that
linear convergence either to a ball around the optimum or to the optimum
was proved, “loc. data” describes whether or not authors assume that fi
is available at node i only, “non-smooth” means support for a non-smooth
regularizer, “momentum” says whether or not authors consider momentum
in their algorithm, and “block quant.” means theoretical justiﬁcation for
using block quantization.

. . . . . . . . . . . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . 104

6.2 Summary of iteration complexity results.
6.3 The leading term of the iteration complexity of DIANA in the strongly con-
vex case (Theorem 6.5.4, Corollary 6.5.5 and Lemma 6.5.3). Logarithmic
dependence on 1/ε is suppressed. Condition number: κ = L
µ .

. . . . . . 105

7.1 Selected special cases of our method. For Dykstra’s algorithm, C1, . . . , Cm
are closed convex sets; and we wish to ﬁnd projection onto their intersec-
tion. Randomized Kaczmarz is a special case for linear constraints (i.e.,
Cj = {x : a(cid:62)
j x = bj}). We do not prove convergence under the same
assumptions as Point–SAGA as they require strong convexity and smooth-
ness of each gj, but the algorithm is still a special case.

. . . . . . . . . 112

7.2 Summary of iteration complexity results that we proved. We assume by
default that all functions are convex, but provide diﬀerent rates based on
whether f is strongly convex (scvx) and whether g1, . . . , gm are smooth
functions, which is represented by the check marks. . . . . . . . . . . . . 113

A.1 Summary of frequently used notation. . . . . . . . . . . . . . . . . . . . 167
. . . . . . . . . . . . . . . . . . . . . . 168
A.2 Notation speciﬁc to Chapter 2.
. . . . . . . . . . . . . . . . . . 168
A.3 Notation speciﬁc to Chapters 3 and 4.

19

A.4 Notation speciﬁc to Chapter 5.
A.5 Notation speciﬁc to Chapter 6.
A.6 Notation speciﬁc to Chapter 7.
A.7 Notation speciﬁc to Chapter 8.

. . . . . . . . . . . . . . . . . . . . . . 168
. . . . . . . . . . . . . . . . . . . . . . 168
. . . . . . . . . . . . . . . . . . . . . . 169
. . . . . . . . . . . . . . . . . . . . . . 169

F.1 Approximate optimal number of blocks for diﬀerent dataset and conﬁgu-

rations. Momentum equals zero for all experiments.

. . . . . . . . . . . 283
F.2 Deep Neural Networks used in the experiments. . . . . . . . . . . . . . . 286

G.1 Selected applications of Algorithm 11 for solving problem (7.1).

. . . . . 294

20

Chapter 1

Introduction

One of the ﬁrst questions asked by machine learning practitioners when trying to build
a model for ﬁnding patterns in and gaining insights from real-world data pertains to the
choice of an appropriate model class. This is typically an exercise in applying prior expert
knowledge and experience. Subsequently, they need to choose the right model inside the
selected model class, which is performed by a suitable training algorithm.

One of the most universal approaches to building such algorithms is to pick an error

function, and minimize it over the space of parameters characterizing the model class:

f (x).

min
x∈Rd

(1.1)

That is, training is performed via optimization. As it turns out, there is an abundance
of optimization algorithms applicable or even speciﬁcally designed for minimizing such
error functions appearing in machine learning. As explained in the very ﬁrst sentences of
the seminal book of Nesterov [186], the explosion of theoretical research in mathematical
optimization happened after it was observed in the mid eighties that the theoretical com-
plexity of many optimization algorithms often captures their practical performance very
well. Since then, theoretical complexity has by many been considered as a key eﬃciency
measure of optimization algorithms, one that is equal and in some aspects superior to their
performance in numerical experiments, which is the main reason why optimization theory
is so highly regarded. For example, complexity theory leads to deep understanding, which
acts as an important catalyst in the development of new and more eﬃcient algorithms,
and a guide in the selection of the most appropriate method for a given application.

Motivated by this observation, in this thesis we set out to study the convergence rates
of several optimization algorithms of key importance to machine learning in general, and
distributed and federated learning in particular, in order to shed light on their superior em-
pirical performance. Equally importantly, we also propose several new eﬃcient algorithms,
providing improved, tight and insightful theoretical convergence rates.

This thesis is motivated by seven key theoretical challenges and open problems
in optimization for machine learning.

However, before formulating the challenges that we plan to address, let us ﬁrst brieﬂy
outline the relevant optimization history, and provide the necessary background for the
algorithms that we shall analyze in the remainder of this thesis.

The ﬁeld of mathematical optimization has been around for a long time and has gone
through several markedly distinct stages. Starting with the early works of Newton [189]

21

and Cauchy [38], the ﬁrst optimization algorithms were used on small-dimensional prob-
lems that could be solved by hand. While the introduction of computers and new algo-
rithms, such as the simplex method and Karmarkar’s algorithm for linear programming,
allowed the practitioners to solve larger problem instances, their sizes were still quite
limited.

The modern state of the optimization ﬁeld, however, goes hand in hand with the
development of high-dimensional machine learning models, and relies on the common
availability of extremely fast computers. This has led to a change of paradigm, and many
new and exciting optimization algorithms were proposed in the past two decades. For ex-
ample, larger models (i.e., models described by a larger number d of parameters) required
ﬁner regularization procedures, which served as the inspiration for the development of
many new proximal algorithms (Parikh and Boyd [198]). Further, the emergence of new
parallel processing hardware and the coupling of computers into clusters paved the way
for new parallel and distributed optimization algorithms (Bertsekas and Tsitsiklis [23]),
which brought new theoretical challenges, most notably the challenge of taming the high
communication cost of distributed methods.

Perhaps the most striking characteristic of the modern era of optimization lies in
the use of huge-scale datasets to formulate optimization problems. Such problems are
characterized by the ﬁnite-sum structure of the objective,

f (x) =

1
n

n
(cid:88)

i=1

fi(x),

(1.2)

where n is typically the number of training data points. As both the dimension d and
the number of data points n grow beyond millions or even billions, standard optimization
algorithms, such as Gradient Descent (GD) and Newton’s method, become either inap-
plicable or ineﬃcient.
Indeed, it is often impossible to make more than a few hundred
“passes”1 over large datasets, while Gradient Descent may require orders of magnitude
more passes to converge. Newton’s method, on the other hand, does not scale well with
the dimension, and even quasi-Newton methods, such as BFGS (Broyden [34]; Fletcher
[76]; Goldfarb [79]; Shanno [245]), do not break this limit due to memory issues.

To make iterations cheap, a popular approach to solving problem (1.1) is to use in-
cremental updates that do not require processing all data in each iteration (Nedi´c and
Bertsekas [179]). The most famous example of such an algorithm is Stochastic Gradient
Descent (SGD) (Robbins and Monro [224]). This method uses a noisy estimate of the
gradient and runs much faster than its deterministic counterpart, Gradient Descent (Bot-
tou [30]). Moreover, Nemirovski and Yudin [182] established that under a certain noise
condition, SGD is provably optimal for the minimization of strongly convex functions. To
provide the reader with early intuition, let us write the update of a generic SGD method
explicitly:

xk+1 = xk − γk∇f (xk; ξk).

Here xk+1, xk ∈ Rd are the new and the current iterates, d is the problem dimension,

1One pass typically refers to work equivalent to the evaluation of the n gradients ∇f1, . . . , ∇fn.

22

γk > 0 is a stepsize, and ∇f (xk; ξk) is a stochastic gradient, whose expectation over
random sample ξk is equal to the gradient ∇f (xk) of function f that we aim to minimize.
The popularity of SGD in practice inspires us to study theoretically its extensions as well
as to look at algorithms that may outperform SGD in speciﬁc scenarios.

The optimality of SGD has its limits too. For instance, for objectives that are deﬁned
as sums of a ﬁnite number of terms, as in (1.2), SGD can take more time to converge
than Gradient Descent if a low-error solution is required. The cheap iterations of SGD
eventually become less useful than the expensive gradient-descent iteration due to the
variance inherent in stochastic approximation of the gradients. Achieving the best of both
worlds was an elusive goal until the breakthrough work by Le Roux et al. [131] proposed
the SAG algorithm that attained the iteration convergence rate of Gradient Descent and
the iteration cost of SGD. The paper of Le Roux et al. [131] received the Lagrange Prize
in Continuous Optimization for this achievement and its impact on the ﬁeld, awarded in
2018, just six years after it was published. This discovery subsequently sparked a series of
works on the topic and many other practical variants appeared thereupon, most cited of
which is the SVRG paper of Johnson and Zhang [108]. At the same time, it continued to
be challenging to combine variance reduction with many other techniques that matter in
structured and statistics-based optimization, most importantly with stochastic proximal
operators and matrix splitting.

The goal of this thesis is to address some of the key challenges that are relevant to
the practice of machine learning. Some of these challenges became apparent only recently
while others have been around, unsolved, for a long time. Of special interest to us will be
algorithms based on stochastic updates, including SGD, and variance-reduced methods.
Having provided a quick and broad overview of the ﬁeld, we shall now elaborate in more
detail on the challenges considered in this thesis. For the reader interested in a quick
summary, we provide Table 1.1.

1.1 Challenge 1: Convergence of Local SGD for Federated Learn-

ing in the Heterogeneous Data Regime

Motivation. As machine learning models consume a lot of data during training, preser-
vation of privacy becomes an important goal. How can we train machine learning models
on private data without directly sharing it? The current way to make this possible is to
equip data owners with the task of training a model directly where the data are stored, and
ask them to periodically communicate model updates with an orchestrating server. This
approach to training models is called federated learning (Koneˇcn´y et al., [124]; McMahan
et al., [162]).

Privacy is not the only motivation for federated learning. Soon after edge devices,
such as mobile phones, became capable of computation, it became apparent that the
communication costs of training on edge devices far exceeds that of computation.
It is
much more eﬃcient to use the devices to perform training instead of transferring all the
data for centralized processing. Unfortunately, this makes SGD inapplicable in its standard
form because it requires a synchronization after every gradient step. A simple remedy that
works surprisingly well is to run SGD locally on each device and only communicate the

23

ﬁnal iterate of the local run. This idea dates back to the nineties (Mangasarian, [159]),
yet only recently we started to understand why it works so well. Moreover, it became
really useful mostly because federated learning posed unprecedented constraints on the
algorithms that are supposed to perform it, while the popularity of the area exploded in
the recent years due to quickly arising applications.

In October 2020, Forbes2 placed federated learning as the second of three emerging
areas of AI. This placement followed the extremely rapid growth of the research on fed-
erated learning and how it can be used for various applications. As sometimes argued
(Kairouz et al., [111]; McMahan et al., [162]), federated learning has the potential to
be the primary way every machine learning model for mobile devices is trained. These
promises have attracted a lot of attention to the area: the pioneering works of McMahan
et al. [162] and Koneˇcn´y et al. [124] have been cited 3,400+ times and 1,600+ times in
just 5 years, respectively, and a recent survey of Kairouz et al. [111] has been cited 900+
times in less than 2 years. As promising as it is, federated learning is also extremely hard
as a research problem. This area overlaps with many mathematical and engineering ﬁelds,
but its optimization side is particularly nontrivial.
Indeed, in contrast to the standard
centralized learning scenario, federated learning was formulated as a problem with data
stored in private on unreliable devices that have limited ability to communicate (McMahan
et al., [162]).

At the heart of federated learning lies the Federated Averaging algorithm—a variant
of Local SGD that can ﬁnd a solution with little communication. While the theory of
local methods started in the nineties with the work of Mangasarian [159], it did not get
full recognition until federated learning turned out to be the right application. Once this
became clear, the tables turned and the theory got quickly behind the practice. Without
understanding the convergence rates of Local SGD, it is particularly hard for any new
method to be shown superior to it. And the demand for a solid theory is particularly
high due to the interest in deploying federated learning in applications such as speech
recognition, medical research and mobile applications (Kairouz et al., [111]). To at least
address the present gap in theory versus practice, we are motivated to study the precise
rates of Local SGD, which is the ﬁrst challenge that we will address in this thesis.

In Chapter 2, we provide a new analysis of Local SGD, removing unnec-
Contributions.
essary assumptions and elaborating on the diﬀerence between two data regimes: identical
and heterogeneous.
In the heterogeneous case, which is of key importance in federated
learning, our analysis is the ﬁrst one to show that Local SGD works even when the gradi-
ents do not satisfy any variant of bounded dissimilarity. This was considered an important
open problem in federated learning. However, even in the case of identically sampled data,
we improve the existing theory and provide values for the optimal stepsize and optimal
number of local iterations. Our bounds are based on a new notion of variance that is
speciﬁc to Local SGD methods with heterogeneous data. The tightness of our results is
guaranteed by recovering known statements when we specialize them to H = 1, where H
is the number of local steps. Our empirical evidence further validates the severe impact
of data heterogeneity on the performance of Local SGD.

2https://www.forbes.com/sites/robtoews/2020/10/12/the-next-generation-of-artiﬁcial-intelligence/

24

Before our work, the main focus of research in optimization for federated learning was
on the data sampled from identical (Stich, [252]) or almost identical (Jiang and Agrawal,
[107]) distributions. Such results can guarantee an improvement when the goal is to train
a model faster by parallelizing SGD over multiple devices. In federated learning, however,
the data come from various sources; for instance, from mobile users living in diﬀerent
regions or even countries.
In this case, assuming similarity between gradients is rather
limiting and even unrealistic. Our results, in contrast, do not require any global bound on
gradient dissimilarity, and depend on the gradient norms at the solution only.

Paper. The chapter is based on the paper:

[116] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. Tighter
theory for local SGD on identical and heterogeneous data.
In Proceedings
of the 23rd International Conference on Artiﬁcial Intelligence and Statistics,
pages 4519–4529. PMLR, 2020.

1.2 Challenge 2: Convergence of Random Reshuﬄing

Motivation. Theoretical understanding of incremental gradient methods constitutes a
rather long-standing challenge. These methods are tailored to optimization problems (1.1)
with f being of the “ﬁnite-sum” form (1.2), all relying on the iteration

xk+1 = xk − γk∇fik(xk).

While in SGD the index ik is picked independently in each iteration from some ﬁxed prob-
ability law, i.e., one performs data sampling with replacement, the world of incremental
methods is much richer as it allows for virtually arbitrary rules for the selection of the next
datapoint ik to learn from.

Of particular interest to us in this thesis are incremental gradient methods of the
random permutation/shuﬄing/reshuﬄing variety. This is because despite the common
recognition of SGD as the workhorse behind many successes in machine learning and deep
learning, in practice, SGD is virtually always superseded by the incremental algorithm
known as Random Reshuﬄing (RR), which is based on a sampling without replacement
approach to the selection of the data indices {ik}k. That is, in RR, a random permuta-
tion/reshuﬄing of the data points {1, 2, . . . , n} is selected at the beginning of each epoch,
and this order subsequently dictates the selection of the indices {ik}k. It is worthwhile to
remark that RR data selection strategies are the default in modern deep learning software.
Despite the practical superiority of RR over SGD, virtually all theoretical eﬀort is di-
rected towards the understanding of SGD type methods, and a proper understanding of
methods based on random shuﬄing/reshuﬄing of data in general, and RR in particular,
remains elusive. One of the key reasons for the disproportionate focus on SGD is that it
is much easier to explain theoretically. For example, one can view it as an instance of the
long-studied and well-understood Stochastic Approximation of Robbins and Monro [224],
and immediately obtain a convergence rate. SGD has also been combined with iter-
ate averaging by Polyak and Juditsky [208], which further reﬁnes the convergence rates

25

and keeps the theory still very simple. Nevertheless, the theory for Random Reshuﬄing
has been developing at a much slower pace and its theoretical superiority to SGD was
established only recently.

The history of incremental and shuﬄing-based methods is quite extensive. The algo-
rithms gained their popularity in the eighties under the name of online backpropagation
algorithm (G¨urb¨uzbalaban et al., [90]), and the theoretical development of these methods
started more than thirty years ago, see, for instance, the survey by Bertsekas [21] for more
details. Nevertheless, until now, even for the earliest versions of the most basic incre-
mental algorithms, the convergence has not been fully understood. As we shall see later,
it was long unknown even where exactly the intermediate iterates of those algorithms
converge to. In particular, these algorithms exhibit strong oscillations, which turned out
to be quite hard to explain. Explaining them, on the other hand, leads to a signiﬁcantly
improved analysis that we present in this thesis.

We also highlight that the generality of shuﬄing-based methods allows for them to
be applied in federated learning. Just as Random Reshuﬄing outperforms SGD, a local
variant of RR should at least be on par with Local SGD. However, no such algorithm is
known in the literature and tackling it without understanding RR itself would not give
us a clear comparison. Therefore, we ﬁrst approach RR as a challenge itself, and only
later proceed to obtain a local variant of RR, which we cal FedRR. Equipped with a
tighter theory for Local SGD that we obtain in this thesis too, we will be able to give a
comprehensive comparison of FedRR and Local SGD.

Contributions.
In Chapter 3, we improve upon existing theory of Random Reshuﬄing in
several ways and provide guarantees that match existing lower bounds. In prior literature
for strongly convex and smooth functions, RR was shown to converge faster than SGD
if 1) the stepsize is small, 2) the gradients are bounded, and 3) the number of epochs
is large. However, large stepsizes are crucial for fast convergence at initialization, the
gradients cannot be bounded for strongly convex functions, and small-epoch convergence
is of high value if the time budget is limited. Thus, we provide a theory without these 3
assumptions, and, in addition, improve the dependence on the condition number from κ2
κ) . Furthermore, we show that the power of RR comes
to κ (respectively from κ to
from a fundamentally diﬀerent type of variance, which is based on the notion of Bregman
divergence. We argue through theory and experiments that the new variance type gives an
additional justiﬁcation of the superior performance of RR. To go beyond strong convexity,
we present several results for non-strongly convex and non-convex objectives. We show
that in all cases, our theory improves upon existing literature.

√

Finally, we prove fast convergence of the Shuﬄe-Once (SO) algorithm, which shuﬄes
the data only once, at the beginning of the optimization process. Our theory for strongly
convex objectives matches the known lower bounds for both RR and SO and substantiates
the common practical heuristic of shuﬄing once or only a few times. As a byproduct of
our analysis, we also get new results for the Incremental Gradient algorithm (IG), which
does not shuﬄe the data at all.

Paper. The chapter is based on the paper:

26

[169] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Random
reshuﬄing: simple analysis with vast improvements. Advances in Neural In-
formation Processing Systems, 33:17309–17320, 2020.

1.3 Challenge 3: Going Beyond Local SGD in Federated Learning

Motivation. As outlined above, incremental algorithms have been widely used in prac-
tice, and we managed to close some of the gaps in the theory of these methods in Chap-
ter 3. The practical and theoretical success of RR naturally raises the question whether it
is possible to successfully employ sampling without replacement in federated learning as
well.

Since in Chapter 2 we studied Local SGD—a method of key importance to federated
learning—and provided the ﬁrst guarantees for it in the heterogeneous case, it is natural
to ask whether the theoretical tools developed therein can be combined with the new tools
developed in our study of RR in order to improve upon the state-of-the-art Local SGD
rate. This is highly desired, as even the best known communication complexity results
for Local SGD do not, in general, improve upon the communication complexity of simple
Gradient Descent, which casts a deep shadow onto the current state of theory in federated
learning.

Contributions. We answer the above challenge in the aﬃrmative. Our new method,
FedRR, is the ﬁrst local-based gradient method that beats Gradient Descent (and hence
also Local SGD) in communication complexity.

In Chapter 4, we propose two new algorithms: Proximal Random Reshuﬃng (ProxRR)
and Federated Random Reshuﬃng (FedRR). The ﬁrst algorithm, ProxRR, solves compos-
ite convex ﬁnite-sum minimization problems. These are problems in which the objective
is the sum of the average of n smooth objectives as in (1.2), and of a (potentially non-
smooth) convex regularizer. This problem is of independent interest, as ProxRR is the
ﬁrst RR-based method that can provably solve proximal problems. However, the develop-
ment of ProxRR should also be seen as an intermediary step towards obtaining our second
algorithm, FedRR. Indeed, we obtain the second algorithm, FedRR, as a special case of
ProxRR applied to a carefully designed reformulation of the distributed problem appearing
in federated learning, allowing for both homogeneous and heterogeneous data.

We study the convergence properties of both methods with constant and decreasing
stepsizes, and show that they have considerable advantages over Proximal and Local SGD.
In particular, our methods have superior complexities, and ProxRR evaluates the proximal
operator once per epoch only. When the proximal operator is expensive to compute, this
small diﬀerence makes ProxRR up to n times faster than algorithms that evaluate the
proximal operator in every iteration. We give examples of practical optimization tasks
where the proximal operator is diﬃcult to compute and ProxRR has a clear advantage.
When specializing to the federated learning setting, our FedRR algorithm needs to com-
municate only after every local pass over local data is done. In contrast to the theory of
Local SGD, which requires dividing the stepsize by n when n local steps are performed,
our theory allows for stepsizes that do not depend on n at all.

27

We note that our results considerably improve upon the complexity of Local SGD.
Since incremental algorithms use the ﬁnite-sum structure of the objective, they are not
subject to the lower bounds established for Local SGD by Woodworth et al. [277]. This
allows our algorithm FedRR to beat Local SGD after a certain number of iterations,
regardless of how heterogeneous the data are.

Paper. The chapter is based on the paper:

[170] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Proximal
and federated random reshuﬄing. arXiv preprint arXiv:2102.06704, 2021.

1.4 Challenge 4: The First Adaptive Stepsize Rule for Gradient

Descent that Provably Works

Motivation. To run Gradient Descent, one needs to use a stepsize that depends on
problem-speciﬁc constants such as the Lipschitz parameter of the objective’s gradient
(Nesterov, [186]) or the objective value at the optimum (Polyak, [206]). How can we
circumvent this requirement if we do not know the required constants in advance? This
fundamental question has been important to the ﬁeld of optimization for many decades
despite the signiﬁcant changes in applications.

Loosely speaking, algorithms that are able to run without explicit knowledge of some
problem-speciﬁc constants and that can perform comparably to a similar algorithm that
has explicit access to these constants are called adaptive [135]. As we mentioned earlier,
tight theoretical complexity of an algorithm is quite often closely reﬂected in its practical
performance. Of all counterexamples to this rule, one that stands out the most is the
class of adaptive methods.
In fact, the methods that are provably not convergent are
often among the most popular ones. For instance, Adam (Kingma and Ba, [120]), for
which Reddi et al., [219] gave convex counterexamples, is much more frequently used in
practice than its theory-based-counterpart Adagrad (Duchi et al., [68]). Indeed, as of now,
the work that proposed Adam has 86,000+ citations, while the paper on Adagrad has
only 9,100+ citations. Similarly, in a recent work, Burdakov et al. [35] identiﬁed a simple
convex counterexample for the widely adopted Barzilai–Borwein (BB) method (Barzilai
and Borwein, [11]), which has been praised for its numerical performance (Wright, [278])
and has accumulated 2,600+ citations.

The history of adaptive methods started long ago before Adagrad and Adam were
introduced. The early approaches to adaptive parameter estimation include the seminal
works of Armijo [7] and Polyak [206] that proposed what later became to be known as
Armijo line-search and Polyak’s stepsize. The Barzilai–Borwein method was proposed two
decades later (Barzilai and Borwein, [11]) and soon after gained popularity for its eﬃciency
in practice. At ﬁrst, it even appeared that BB might be provably convergent as a few
years after its introduction it was shown to work for quadratic problems by Raydan [216].
Despite the fact that line search and Polyak’s rule appeared more than half a century ago
and BB has never had any theory for non-quadratic functions, their ability to estimate
objective parameters still attracts considerable attention (Tan et al., [259]; Hazan and
Kakade, [100]; Vaswani et al., [268]; Loizou et al., [149]). This ability is, in fact, even

28

more important for machine learning applications, because the corresponding problems
rarely admit closed-form expressions for problem constants such as gradient Lipschitzness.
In classical problems, this constant can be often computed exactly. For instance, it is
In contrast,
the maximum singular value of the data matrix in least-squares regression.
objectives such as those appearing in neural network training may have unknown or inﬁnite
global smoothness constants (Zhang et al., [292]), and hence only a local estimation can
potentially work in practice.

The empirical success of adaptive methods gives ample motivation to start developing
theory for adaptive methods. Surprisingly, however, there exists no known closed-form
stepsize for Gradient Descent that is completely parameter-free and would provably con-
verge. Methods such as normalized gradient (Shor, [247]) and Adagrad (Duchi et al.,
[68]) need constants related to the distance from the solution, while Polyak’s stepsize rule
requires the knowledge of the optimal function value. Further, while line search proce-
dures are parameter-free, they are not given in a closed-form, require subroutines to be
run, and work only for globally smooth objectives.

Contributions. Therefore, one of our goals in this thesis is to provide the ﬁrst stepsize
for Gradient Descent that requires access to no information beyond the gradients them-
selves.
In particular, in Chapter 5, we shall present a proof that Gradient Descent with
our stepsize rule can provably minimize any locally smooth convex function.

More speciﬁcally, we prove that two simple rules are suﬃcient to automate Gradient
Descent: 1) do not increase the stepsize too fast, and 2) do not overstep the local
curvature. Namely, our theory guarantees than for a stepsize sequence γ1, . . . , γk, . . . to
work, the only two requirements are

γ2
k ≤

γk ≤

(cid:18)

(cid:19)

1 +

γ2
k−1,

γk−1
γk−2
(cid:107)xk − xk−1(cid:107)
2(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)

,

Our method does not need any line search, and works without knowing the functional val-
ues or any other information about the objective except for the gradients. By choosing γk
per the rules above, we obtain a method adaptive to the local geometry, with convergence
guarantees depending on the smoothness in a neighborhood of a solution only. Given that
the problem is convex, our method converges even if the global smoothness constant is
inﬁnity. As an illustration, it can minimize an arbitrary twice continuously diﬀerentiable
convex function. We examine its performance on a range of convex and non-convex
problems, including logistic regression, matrix factorization, and neural network training.
To the best of our knowledge, our stepsize rule for Gradient Descent is the only one that
provably gives convergence for non-quadratic functions. Many other attempts to obtain
adaptive stepsize rule work either only for quadratics (Raydan, [216]), self-concordant
functions with known self-concordance parameter (Gao and Goldfarb, [77]), or require
knowledge of the problem conditioning (Tan et al., [259]). The immense interest in
adaptive methods, which can be seen from the number of citations to the aforementioned
papers, points to only one explanation for the apparent scarcity of the results on the topic:

the technical diﬃculty of obtaining such results.

29

Paper. The chapter is based on the paper:

[158] Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent
without descent.
In Proceedings of the 37th International Conference on
Machine Learning, volume 119, pages 6702–6712. PMLR, 2020.

1.5 Challenge 5: Achieving Fast Rates in Distributed Optimiza-

tion with Quantization

Motivation. While Local SGD and similar algorithms have been very successful in ad-
dressing the communication challenge in federated learning, they also have limitations
(Woodworth et al., [277]). An alternative way to make communication cheaper is to
apply lossy compression to the communicated vectors (Alistarh et al., [5]; Wen et al.,
[276]). The choice of compression plays an important role in how the communication is
performed, and the idea of sending only the sign of the update vector has been particularly
popular (Bernstein et al., [20]). Another popular technique is to send the coordinates with
the largest magnitudes only (Stich et al., [254]), which requires sorting the uncompressed
vector and may be thereby a bit slower.

To quantify the overall beneﬁt of relying on compressed communication, we need to
consider two factors: the per-iteration savings coming from compression, and the increase
in the iteration complexity resulting from compression. Since we send less information,
it is only reasonable to expect that the iteration complexity would get worse. Therefore,
if the compressed update requires ω ≥ 1 times
we ask the following natural question:
fewer bytes to send, how does the iteration complexity depend on ω? As it turns out,
the main drawback of using lossy compression is the requirement to use ω-times smaller
stepsizes, and the resulting slowdown in the convergence rates of existing methods is
proportional to ω, too. Thus, in general, there might be little or no beneﬁt from applying
compression. The error-feedback technique proposed by Stich et al. [254] and reﬁned by
Stich and Karimireddy [255] allows to partially improve the rate when the complexity is
driven by the noise of stochastic gradients but, unfortunately, it is still incapable of ﬁxing
the stepsize requirement. Our next goal in this thesis, therefore, is to ﬁnd a new algorithm
that does not suﬀer from requiring small stepsizes when the noise is mild.

Several other methods based on the compression (e.g., sparsiﬁcation and/or quan-
tization) of the updates were recently proposed, including QSGD (Alistarh et al., [5]),
TernGrad (Wen et al., [276]), SignSGD (Bernstein et al., [20]), and DQGD (Khirirat et
al., [118]). However, all of these methods suﬀer from severe issues, such as the inability
to converge to the true optimum in the batch mode, inability to work with a non-smooth
regularizer, and slow convergence rates.

Contributions. We propose a new distributed learning method—DIANA—which re-
solves these issues via a new algorithmic tool: compression of gradient diﬀerences. DIANA
is the ﬁrst variance-reduction mechanism for distributed training which can progressively

30

reduce the variance introduced by gradient compression.
DQGD what SVRG is to SGD.

In other words, DIANA is to

We perform a theoretical analysis in the strongly convex and non-convex settings and
show that our rates are superior to the existing rates. Moreover, our analysis of block-
quantization and diﬀerences between (cid:96)2 and (cid:96)∞ quantization closes one of the gaps in
theory and practice. Finally, by applying our analysis technique to TernGrad, we establish
the ﬁrst convergence rate for this method.

The idea of diﬀerence quantization proposed in our work has proved to be very helpful
and the results of Chapter 6 were extended in a number of works. In a follow-up paper
(Horv´ath et al., [103]), we generalized it to arbitrary unbiased compressors, and combined
it with a secondary variance reduction mechanism which allows to compress stochastic
gradients without suﬀering rate deterioration.

Our idea was extended by many in various other ways, in particular, to server-side
compression by Liu et al. [147], device sampling by Philippenko and Dieuleveut [204], ac-
celeration by Li et al. [141], second-order methods for generalized linear models by Islamov
et al. [104], second-order methods for federated learning by Safaryan et al. [230], acceler-
ation via matrix smoothness by Safaryan et al. [229], integer compression for SwitchML
by Mishchenko et al. [175], and to biased compression by Gorbunov et al. [82]. Even more
variants as well as a uniﬁcation for their analysis were obtained by Gorbunov et al. [81].

Paper. The chapter is based on the paper:

[165] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak´aˇc, and Peter
Richt´arik. Distributed learning with compressed gradient diﬀerences. arXiv
preprint arXiv:1901.09269, 2019.

1.6 Challenge 6: Developing Variance Reduction for Proximal

Operators

Motivation. The continued interest in algorithms that process the data eﬃciently re-
sulted in a steady development of incremental and stochastic algorithms. Nevertheless,
these algorithms are not always optimal. A well-known limitation of SGD and RR is that
they might escape from a solution if the stochastic gradients do not converge to zero.
In 2005, the Incremental Average Gradient (IAG) method, proposed by Blatt et al. [27],
managed to bypass this limitation in practice. However, the corresponding convergence
rates were not promising. A true revolution started when the stochastic counterpart of
IAG, known as Stochastic Average Gradient (SAG), was shown by Le Roux et al. [131] to
converge at a much faster rate than IAG, SGD and other similar incremental algorithms.
SAG and other algorithms that have cheap iterations but can achieve the rate of Gradient
Descent became known as variance-reduced methods, and include SDCA (Shalev-Shwartz
and Zhang, [240]), SAGA (Defazio et al., [62]), SVRG (Johnson and Zhang, [108]), S2GD
(Koneˇcn´y and Richt´arik, [125]), MISO (Mairal et al., [155]; Qian et al., [211]), QUARTZ
(Qu et al., [212]), and JacSketch (Gower et al., [88]). For a recent review of variance
reduced methods for machine learning, we refer the reader to Gower et al. [87].

31

Many further extensions of variance reduction have been proposed. For instance,
proximal (Xiao and Zhang, [279]), mini-batch (Koneˇcn´y et al., [123]), accelerated (Allen-
Zhu, [6]; Snang et al., [244]), and loopless SVRG (Kovalev et al., [126]; Qian et al.,
[210]). The focus of these works, however, was directed towards improving the use of
stochastic gradients.

At the same time, many convex problems come with complicated constraints and reg-
ularizers, such as Group Lasso, that require expensive computation of proximal operators.
For such problems, the speed advantage coming from cheap gradient computation fades
away once we take into account the time required to tackle the regularizers. To make
variance reduction useful in these settings, one has to ﬁnd a way to make the computa-
tion of the proximal operator inexpensive, too. Notwithstanding the eﬀorts to tackle this
challenge, for instance by Ryu and Yin [228], Defazio [61], and Pedregosa et al. [202], it
has been unresolved in its general form.

Contributions. Motivated by these needs, we consider the problem of minimizing the
sum of three convex functions: i) a smooth function f in the form of an expectation or
a ﬁnite average, ii) a non-smooth function g in the form of a ﬁnite average of proximable
functions gj, and iii) a proximable regularizer ψ. We design a variance reduced method
which is able to progressively learn the proximal operator of g via the computation of
the proximal operator of a single randomly selected function gj in each iteration only.
Our method can provably and eﬃciently accommodate many strategies for the estimation
of the gradient of f , including via standard and variance-reduced stochastic estimation,
eﬀectively decoupling the smooth part of the problem from the non-smooth part. We
prove a number of iteration complexity results, including a general O( 1
K2 )
rate in the case of strongly convex smooth f , and several linear rates in special cases,
including accelerated linear rate. For example, our method achieves a linear rate for the
problem of minimizing a strongly convex function f subject to linear constraints under no
assumption on the constraints beyond consistency. When combined with SGD or SAGA
estimators for the gradient of f , this leads to a very eﬃcient method for empirical risk
minimization.

K ) rate, O( 1

Our method generalizes several existing algorithms, including forward-backward split-
ting, Douglas–Rachford splitting, Proximal SGD, Proximal SAGA, SDCA, Randomized
Kaczmarz and Point–SAGA. However, our method leads to new methods in special cases;
for instance, we obtain the ﬁrst randomized variant of the Dykstra’s method for projection
onto the intersection of closed convex sets. The uniﬁed analysis proposed in our work
might be of interest on its own as is gives a simple way to derive multiple methods at
once.

Paper. The chapter is based on the paper:

[174] Konstantin Mishchenko and Peter Richt´arik. A stochastic decoupling
method for minimizing the sum of smooth and non-smooth functions. arXiv
preprint arXiv:1905.11535, 2019.

32

1.7 Challenge 7: Designing Variance-Reduced Algorithms with

Splitting

Motivation. Regularized objectives, such as PC-Lasso (Tay et al., [261]), sometimes
regularize a linear transformation of the parameter vector instead of regularizing the pa-
rameters themselves.
If the regularizer is a non-smooth function, it is recommended to
use algorithms based on matrix splitting and proximal operators (Davis and Yin, [60]).
Such algorithms provide eﬃcient update rules for the regularizer, but, unfortunately, they
ignore the ample cost of computing gradients of the other part of the objective function.
Can we ﬁnd a way to design an algorithm that combines the best of both worlds: use
variance-reduced gradient updates and split matrix multiplication from proximal opera-
tors?

Contributions. To answer this question, in Chapter 8, we consider the task of minimiz-
ing the sum of three convex functions, where the ﬁrst one f is smooth, the second one is
non-smooth and proximable, and the third one is the composition of a non-smooth prox-
imable function with a linear operator L. This template problem has many applications
in machine learning and signal processing.

We propose a new primal–dual algorithm called PDDY to solve such problem. PDDY
can be seen as an instance of Davis–Yin Splitting involving operators which are mono-
tone under a new metric depending on L. This representation of PDDY eases the non-
asymptotic analysis of PDDY: it allows us to prove its sublinear convergence (respectively
linear convergence if strong convexity is involved).

Moreover, our proof technique easily extends to the case where a variance reduced
stochastic gradient of f is used instead of the full gradient. Besides, we obtain as a special
case a linearly converging algorithm for the minimization of a strongly convex function f
under linear constraints Lx = b. This algorithm can be applied to decentralized optimiza-
tion problems and competes with other approaches speciﬁcally designed for decentralized
optimization.

Finally, we show that three other primal–dual algorithms (the two Condat–V˜u al-
gorithms and the PD3O algorithm) can be seen as Davis–Yin Splitting under a metric
depending on L. Such representation was not known for the Condat–V˜u algorithms. We
show again that this representation eases the non-asymptotic analysis of PD3O in the
case where a variance reduced stochastic gradient is used. Our theory covers several
settings that are not tackled by any existing algorithm; we illustrate their importance
with real-world applications and we show the eﬃciency of our algorithms by numerical
experiments.

Paper. The chapter is based on the paper:

[234] Adil Salim, Laurent Condat, Konstantin Mishchenko, and Peter Richt´arik.
arXiv
Dualize, split, randomize: fast nonsmooth optimization algorithms.
preprint arXiv:2004.02635, 2020.

1.8 Excluded papers

33

To make the thesis more self-consistent, some papers were excluded from it. In particular,
we do not discuss here the following works that were ﬁnished during my PhD studies:

• Stochastic algorithms for constrained minimization [173].

• Variance reduction for coordinate descent with non-separable regularizer [95].

• A work on delay-tolerant asynchronous gradient method [168] and its extended

version with more general analysis [167].

• Tackling communication bottleneck by update sparsiﬁcation [166].

• Variance-reduced extension of the algorithm presented in Chapter 6 [103].

• Improved analysis and extension of variance reduction based on the MISO algorithm[211].

• Equivalence between the celebrated Sinkhorn algorithm and mirror descent applied

to a certain objective [164].

• A variance-reduced Newton method and its cubic-regularized version [127].

• A quasi-Newton asynchronous method [249].

• Integer compression for communication-eﬃcient distributed training [175].

• Hierarchical time series regression [172].

We note that some of these papers have been presented as part of the PhD thesis by Filip
Hanzely [94] and are excluded for this reason.

1.9 Theoretical Background for Most Chapters

To avoid introducing the same assumptions and standard results in each chapter, we shall
now discuss some of them here, excluding those facts that are speciﬁc to a single chapter.
We also brieﬂy mention some key distinctions of the notation used in speciﬁc chapters in
this introduction section.

The notions of convex analysis and operator theory that we introduce are very standard
and can be found in textbooks. For instance, we recommend the reader to consult Boyd
and Vandenberghe [32], Nesterov [186], and Bauschke and Combettes [15] for details and
proofs.

34
1.10 Basic Facts and Notation

In all chapters, we are going to be solving in one way or another minimization of a given
function over Rd. The main diﬀerentiable part of the objective is always denoted as
If there is a single
f , while the overall objective, if diﬀerent from f , is denoted by P .
regularization term in the objective, it is denoted by ψ, so most of the time we will be
solving the problem

(cid:104)

(cid:105)
P (x) def= f (x) + ψ(x)

.

min
x∈Rd

Function f might have diﬀerent forms depending on the context.
In particular, we will
sometimes consider it to be of an expectation form with respect to some random variable
ξ:

M

(cid:80)n

(cid:80)M

f (x) def= Eξ [f (x; ξ)] .
Alternatively, f may take a ﬁnite-sum form. For instance, we may use the notation
f (x) = 1
n

m=1 fm(x), depending on context.

i=1 fi(x) or f (x) = 1

We denote the optimal value of P (or f ) as P ∗ (or f ∗). In case we need to work with a
solution of the problem, we denote it by x∗, and the set of all solutions by X (cid:63). We denote
scalars and vectors with standard letters, for instance, α ∈ R or x ∈ Rd. The iteration
index is usually denoted with k, so that xk is the main iterate of the considered algorithm.
Matrices and linear operators are denoted with bold capital letters, for instance, L. For
any positive integer n ≥ 1 we deﬁne [n] def= {1, 2, . . . , n}. We denote by (xk)k the inﬁnite
sequence of elements with values x0, x1, x2, . . . .

We use (cid:104)·, ·(cid:105) to denote the standard Euclidean scalar product of two vectors, and (cid:107) · (cid:107)
to denote the associated Euclidean norm. For any p ≥ 1, we denote by (cid:107) · (cid:107)p the (cid:96)p norm
of a vector and we drop the subscript when p = 2. For a matrix A ∈ Rd×m, we denote by
(cid:107)A(cid:107) its operator norm, and by (cid:107)A(cid:107)2,1 = (cid:80)m
j=1 (cid:107)Aj(cid:107) the (cid:96)2,1 norm, where Aj is the j-th
column of matrix A. We denote by λmin(A) and λ+
min(A) the smallest and the smallest
positive eigenvalues of A, respectively.

1.10.1 Random vectors

For any ﬁxed vector h ∈ Rd, the variance of a random vector X with ﬁnite second moment
can be decomposed as follows:

E (cid:2)(cid:107)X − E [X] (cid:107)2(cid:3) = E (cid:2)(cid:107)X − h(cid:107)2(cid:3) − (cid:107)E[X] − h(cid:107)2.

(1.3)

In particular, if we plug in h = 0 and rearrange the terms, we get the standard variance
decomposition formula

E (cid:2)(cid:107)X(cid:107)2(cid:3) = E (cid:2)(cid:107)X − E [X](cid:107)2(cid:3) + (cid:107)E [X](cid:107)2.

(1.4)

If, in addition, X takes only a ﬁnite number of values, we get

35

1
n

n
(cid:88)

i=1

(cid:107)Xn(cid:107)2 =

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi −

1
n

n
(cid:88)

j=1

Xj

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

Xi

As a consequence of (1.4) we also have that

E (cid:2)(cid:107)X − E [X](cid:107)2(cid:3) ≤ E (cid:2)(cid:107)X(cid:107)2(cid:3) .

In the case when X takes a ﬁnite number of values, inequality (1.6) simpliﬁes to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

Xi

≤

1
n

n
(cid:88)

i=1

(cid:107)Xi(cid:107)2.

After multiplying both sides of (1.7) by n2, we get

(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

Xi

≤ n

n
(cid:88)

i=1

(cid:107)Xi(cid:107)2.

1.10.2 Norms and products

(1.5)

(1.6)

(1.7)

(1.8)

We now state some straightforward linear algebra results. Firstly, for any two vectors
a, b ∈ Rd, we have

2 (cid:104)x, y(cid:105) = (cid:107)x(cid:107)2 + (cid:107)y(cid:107)2 − (cid:107)x − y(cid:107)2.

(1.9)

We will also use the following facts, which are sometimes referred to as Young’s

inequality:

(cid:107)x + y(cid:107)2 ≤ 2(cid:107)x(cid:107)2 + 2(cid:107)y(cid:107)2,

2 (cid:104)x, y(cid:105) ≤ ζ(cid:107)x(cid:107)2 + ζ −1(cid:107)y(cid:107)2 for all x, y ∈ Rd and ζ > 0.

(1.10)

(1.11)

Finally, for any 0 ≤ α ≤ 1 and x, y ∈ Rd, we have

(cid:107)αx + (1 − α)y(cid:107)2 = α(cid:107)x(cid:107)2 + (1 − α)(cid:107)y(cid:107)2 − α(1 − α)(cid:107)x − y(cid:107)2.

(1.12)

1.10.3 Function properties

We say that an extended real-valued function f : Rd → R ∪ {+∞} is proper if its domain,

dom f def= {x : f (x) < +∞},

is nonempty. We say that it is convex (respectively closed) if its epigraph,

epi f def= {(x, t) ∈ Rd × R : f (x) ≤ t},

36

is a convex (respectively closed) set. Equivalently, f is convex if dom f is a convex set
and

f (αx + (1 − α)y) ≤ αf (x) + (1 − α)f (y)

for all x, y ∈ dom f and α ∈ (0, 1). Finally, f is µ-strongly convex if f (x) − µ
convex.

2 (cid:107)x(cid:107)2 is

We deﬁne the subdiﬀerential of f as the set-valued operator

∂f : x ∈ Rd (cid:55)→ {g ∈ Rd : (∀y ∈ Rd) f (x) + (cid:104)y − x, g(cid:105) ≤ f (y)}.

If f is diﬀerentiable at x ∈ Rd, then ∂f (x) = {∇f (x)}, where ∇f (x) denotes the
gradient of f at x.

We denote by f ∗ the conjugate of f , deﬁned by

f ∗ : x (cid:55)→ sup
y∈Rd

{(cid:104)x, y(cid:105) − f (y)},

which is always convex, proper and closed. Finally, given any convex set C ⊂ Rd, we
deﬁne the indicator function

χC(x) def=

(cid:40)

if x ∈ C

0,
+∞, otherwise

.

Note that this function is always convex, proper and closed. For brevity, if C = {b} with
some b ∈ Rd, we denote χb

def= χ{b}.

Finally, let us introduce the following standard result for convex functions.

Proposition 1.10.1 (Jensen’s inequality). For any convex function f and any vectors
x1, . . . , xM we have

(cid:32)

f

1
M

M
(cid:88)

m=1

(cid:33)

xm

≤

1
M

M
(cid:88)

m=1

f (xm).

(1.13)

1.10.4 Bregman divergence

To simplify the notation and proofs, it is convenient to work with Bregman divergences.
It is important to note that the Bregman divergence of a convex function is always non-
negative and is a (non-symmetric) notion of “distance” between x and y. For x(cid:63) ∈ X (cid:63),
the quantity Df (x, x(cid:63)) serves as a generalization of the functional gap f (x) − f (x(cid:63)) in
cases when ∇f (x(cid:63)) (cid:54)= 0.

We denote the Bregman divergence associated with function f and arbitrary x, y as

Df (x, y) def= f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Moreover, a continuously diﬀerentiable function f is called L-smooth if its gradient is

L-Lipschitz, i.e., if

37

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

∀x, y ∈ Rd.

(1.14)

A very important consequence of L-smoothness is that it implies an upper bound on the
Bregman divergence of f :

Df (x, y) ≤

L
2

(cid:107)x − y(cid:107)2 ,

∀x, y ∈ Rd.

(1.15)

If f is µ-strongly convex, then we also have

µ
2

(cid:107)y − x(cid:107)2 ≤ Df (x, y),

∀x, y ∈ Rd.

(1.16)

The most basic consequence of function smoothness is that the squared gradient
norm can be upper bounded with the functional gap. This is formalized in the following
proposition.

Proposition 1.10.2. Let f be L-smooth and lower bounded by f (cid:63) ∈ R, then

(cid:107)∇f (x)(cid:107)2 ≤ 2L(f (x) − f (cid:63)),

∀x ∈ Rd.

(1.17)

Since many of our proofs are easier to write when one uses Bregman divergences, we

will formulate the next two lemmas in terms of Df (·, ·).

Proposition 1.10.3. Let f be convex and L-smooth, then

∀x, y ∈ Rd,
(cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ 2LDf (x, y),
(cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ L (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

∀x, y ∈ Rd.

(1.18)
(1.19)

Sometimes, to make the analysis tighter, we require the following statement.

Proposition 1.10.4. Let f be diﬀerentiable and µ-strongly convex. Then

µ
2

(cid:107)x − y(cid:107)2 + Df (x, y) ≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

∀x, y ∈ Rd,

(1.20)

µ(cid:107)x − y(cid:107)2 ≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

∀x, y ∈ Rd.

(1.21)

Moreover, if f is also L-smooth, then

µL
L + µ

(cid:107)x − y(cid:107)2 +

1
L + µ

holds for all x, y ∈ Rd.

(cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105)

(1.22)

The last inequality is the tightest inequality one can get and, in particular, it implies

(1.19) when µ = 0.

1.10.5 Proximal operator

38

To solve problems with non-smooth regularizer, one of the best approaches is to use
proximal operator. Given γ > 0, the proximal operator for function ψ is deﬁned as

proxγψ(u) def= arg min

v

(cid:26)

γψ(v) +

(cid:107)v − u(cid:107)2

(cid:27)

.

1
2

Let us state some basic and well-known properties of the regularized objectives. Firstly, the
following lemma explains why the solution of (3.1) is a ﬁxed point of the proximal-gradient
step for any stepsize.

Proposition 1.10.5. Let ψ be proper, closed and convex. Then point x(cid:63) is a minimizer
of P (x) = f (x) + ψ(x) if and only if for any γ > 0 we have

x(cid:63) = proxγψ(x(cid:63) − γ∇f (x(cid:63))).

Proof. This follows by writing the ﬁrst-order optimality conditions for problem (3.1), see
(cid:4)
[198, p.32] for a full proof.

The proposition above only shows that proximal-gradient step does not hurt if we
are at the solution. In addition, we will rely on the following a bit stronger result which
postulates that the proximal operator is a contraction (respectively strong contraction) if
the regularizer ψ is convex (respectively strongly convex).

Lemma 1.10.6. Let ψ by proper and closed. If ψ is µ-strongly convex with µ ≥ 0, then
for any γ > 0 we have

(cid:107)proxγψ(x) − proxγψ(y)(cid:107)2 ≤

1
1 + 2γµ

(cid:107)x − y(cid:107)2,

(1.23)

for all x, y ∈ Rd.

1

Proof. Let u def= proxγnψ(x) and v def= proxγnψ(y). By deﬁnition, u = arg minw{ψ(w) +
2γn(cid:107)w −x(cid:107)2}. By ﬁrst-order optimality, we have 0 ∈ ∂ψ(u) + 1
γn(u−x) or simply x−u ∈
γn∂ψ(u). Using a similar argument for v, we get x − u − (y − v) ∈ γn(∂ψ(u) − ∂ψ(v)).
Thus, by strong convexity of ψ, we get

(cid:104)x − u − (y − v), u − v(cid:105) ≥ γµn(cid:107)u − v(cid:107)2.

Hence,

(cid:107)x − y(cid:107)2 = (cid:107)u − v + (x − u − (y − v))(cid:107)2

= (cid:107)u − v(cid:107)2 + 2 (cid:104)x − u − (y − v), u − v(cid:105) + (cid:107)x − u − (y − v)(cid:107)2
≥ (cid:107)u − v(cid:107)2 + 2 (cid:104)x − u − (y − v), u − v(cid:105)
≥ (1 + 2γµn)(cid:107)u − v(cid:107)2.

(cid:4)

39

Figure 1.1: Illustration of property (1.25) with characteristic function of a linear subspace,
ψ(x) = χ{x : a(cid:62)x=b}. In this case the proximal operator returns the projection of a point
onto the subspace, and Inequality (1.25) becomes identity and follows from Pythagorean
theorem.

Another important property of the proximal operator is ﬁrm nonexpansiveness given

below.

Proposition 1.10.7. Let ψ : Rd → R ∪ {+∞} be a proper closed convex function. Then
its proximal operator is ﬁrmly nonexpansive. That is, for all γ ∈ R,

(cid:107)proxγψ(x) − proxγψ(y)(cid:107)2

≤ (cid:107)x − y(cid:107)2 −

(cid:18)

1 +

1
γLψ

(cid:19)

(cid:107)x − proxγψ(x) − (y − proxγψ(y))(cid:107)2,

(1.24)

(1.25)

where Lψ ∈ R ∪ {+∞} is the smoothness constant of function ψ (for non-smooth
functions, Lψ = +∞).
In particular, regardless of the smoothness properties of ψ, we
have

(cid:107)proxγψ(x) − proxγψ(y)(cid:107)2 ≤ (cid:107)x − y(cid:107)2,

(1.26)

Moreover, we would like to note that Equation 1.25 is tight if ψ(x) = χ{x : a(cid:62)x=b} for

some a ∈ Rd and b ∈ R, as is shown in Figure 1.1.

1.10.6 Monotone operators

Consider a set-valued operator M : Z ⇒ Z. The inverse M −1 of M is deﬁned by the
relation

z(cid:48) ∈ M (z) ⇔ z ∈ M −1(z(cid:48)).

The set of zeros of M is

zer(M ) = M −1(0) = {z ∈ Z, 0 ∈ M (z)}.

The operator M is monotone if

(cid:104)w − w(cid:48), z − z(cid:48)(cid:105) ≥ 0

40

whenever u ∈ M (z) and u(cid:48) ∈ M (z(cid:48)), and strongly monotone if there exists µ > 0, such
that

(cid:104)w − w(cid:48), z − z(cid:48)(cid:105) ≥ µ(cid:107)z − z(cid:48)(cid:107)2.

The resolvent operator of M is deﬁned by

JM = (I + M )−1,

If M is monotone, then JM (z) is either empty or single-
where I denotes the identity.
valued. M is maximal monotone if JM (z) is single-valued, for every z ∈ Z. We identify
single-valued operators as operators from Z to Z. If f is convex, proper and closed, then
∂f is maximal monotone,

J∂f = proxf ,

zer(∂f ) = arg min f,

and (∂f )−1 = ∂f ∗.

A single-valued operator M on Z is α-cocoercive if

α(cid:107)M (z) − M (z(cid:48))(cid:107)2 ≤ (cid:104)M (z) − M (z(cid:48)), z − z(cid:48)(cid:105).

The resolvent of a maximal monotone operator is 1-cocoercive. In addition, ∇f is 1/ν-
cocoercive for any ν-smooth function f .

Let X , Y be real Hilbert spaces and let L : X → Y be a linear operator. The adjoint

of L is denoted by L∗ : Y → X , and the operator norm of L is deﬁned as

(cid:107)L(cid:107) = sup{(cid:107)Lx(cid:107), x ∈ X , (cid:107)x(cid:107) ≤ 1}.

The largest eigenvalue of LL∗ is

(cid:107)LL∗(cid:107) = (cid:107)L(cid:107)2 = (cid:107)L∗(cid:107)2.

Let P : Z → Z be a linear and symmetric operator (P∗ = P). P is positive semideﬁnite
if

(cid:104)Pz, z(cid:105) ≥ 0

for every z ∈ Z, and positive deﬁnite if, additionally, (cid:104)Pz, z(cid:105) = 0 implies z = 0. In this
latter case, the inner product induced by P is deﬁned by (cid:104)z, z(cid:48)(cid:105)P = (cid:104)Pz, z(cid:48)(cid:105) and the norm
induced by P is deﬁned by

P = (cid:104)z, z(cid:105)P.
We denote by ZP the space Z endowed with (cid:104)·, ·(cid:105)P.

(cid:107)z(cid:107)2

1.10.7 Diﬀerences in section-speciﬁc notation

Many of the chapters deal with ﬁnite-sum problems but use slightly diﬀerent notation for
the number of functions used. We use M to denote the number of machines in Chapters 2
and 4, so the ﬁnite sum in the objective has M summands. At the same time, the number
of terms that correspond to data points is denoted by n in Chapters 3, 4, 7, 8. The linear
operators are denoted by A1, . . . , Am in Chapter 7 and by L in Chapter 8.

41

In Chapter 2, we pay particular attention to the nature of stochastic gradients. There-
fore, we will be denoting diﬀerent data/noise distributions as D1, . . . , DM . Correspond-
ingly, we will use Eξ∼Dm[·] to denote the expectation with respect to distribution Dm,
where m ∈ [M ].

For the reader’s convenience, we provide notation tables in Appendix A. They sum-
marize the notation we introduce above as well as provide additional details for speciﬁc
chapters.

42

Table 1.1: A summary of the results obtained in this thesis.

Challenge

Convergence of Local
SGD in heterogeneous
federated learning

Tight convergence
guarantees for Random
Reshuﬄing

Summary

We provide the ﬁrst convergence result for Local
SGD when the data are heterogeneous.

We provide upper bounds for strongly convex,
convex and non-convex objectives that are simpler
and tighter than prior work. Moreover, our upper
bound is the ﬁrst one to match the lower bound.

Going beyond Local
SGD in federated
learning

We design a new algorithm called FedRR that
uses the update of random reshuﬄing in
composition with periodic communication of Local
SGD, but has a much better asymptotic
communication complexity than Local SGD.

The ﬁrst adaptive
stepsize rule for
Gradient Descent that
provably works

We develop the ﬁrst stepsize rule for Gradient
Descent that can provably adapt to the local
geometry of any convex objective by estimating
the local Lipschitz constant of the gradients.

Achieving fast rates in
distributed optimization
with quantization

We present the ﬁrst distributed algorithm with
compressed communication that provably
preserves fast linear rates of Gradient Descent.

Developing variance
reduction for proximal
operators

We develop a uniﬁed theory for a family of
methods that converge linearly while having
access only to stochastic gradients and proximal
operators.

Chapter

Chapter 2

Chapter 3

Chapter 4

Chapter 5

Chapter 6

Chapter 7

Designing
variance-reduced
algorithms with
splitting

We combine gradients estimation with matrix
splitting to obtain a number of linearly-convergent
algorithms.

Chapter 8

43

Chapter 2

Convergence of Local SGD for Federated Learning in the
Heterogeneous Data Regime

2.1 Introduction

Modern hardware increasingly relies on the power of uniting many parallel units into one
system. This approach requires optimization methods that target speciﬁc issues arising
in distributed environments such as decentralized data storage. Not having data in one
place implies that computing nodes have to communicate back and forth to keep moving
toward the solution of the overall problem. A number of eﬃcient ﬁrst-, second-order and
dual methods that are capable of reducing the communication overhead existed in the
literature for a long time, some of which are in certain sense optimal.

Yet, when federated learning (FL) showed up, it turned out that the problem of bal-
ancing the communication and computation had not been solved. On the one hand,
Mini-Batch Stochastic Gradient Descent (SGD), which averages the result of stochastic
gradient steps computed in parallel on several machines, again demonstrated its compu-
tation eﬃciency. Seeking communication eﬃciency, Koneˇcn´y et al. [124] and McMahan
et al. [162]) proposed to use a natural variant of Mini-Batch SGD—Local SGD (Algo-
rithm 1), which does a few SGD iterations locally on each involved node and only then
computes the average. This approach saves a lot of time on communication, but, unfor-
tunately, in terms of theory things were not as great as in terms of practice and there are
still gaps in our understanding of Local SGD.

The idea of Local SGD in fact is not recent, it traces back to the work of Mangasar-
ian [159] and has since been popular among practitioners from diﬀerent communities.
An asymptotic analysis can be found in [159] and quite a few recent papers proved new
convergence results, making the bounds tighter with every work. The theory has been
developing in two important regimes: identical and heterogeneous data.

The identical data regime is more of interest if the data are actually stored in one
place. In that case, we can access it on each computing device at no extra cost and get a
fast, scalable method. Although not very general, this framework is already of interest to
a wide audience due to its eﬃciency in training large-scale machine learning models [143].
The ﬁrst contribution of this chapter is to provide the fastest known rate of convergence
for this regime under weaker assumptions than in prior work.

Federated learning, however, is done on a very large number of mobile devices, and is
operating in a highly non-i.i.d. regime. To address this, we present the ﬁrst analysis of
Local SGD that applies to arbitrarily heterogeneous data, while all previous works assumed
a certain type of similarity between the data or local gradients.

To explain the challenge of heterogeneity better, let us introduce the problem we are

Algorithm 1 Local Stochastic Gradient Descent ( Local SGD )
Require: Stepsize γ > 0, initial vector x0 = x0

m for all m ∈ [M ], number of steps K,

44

synchronization timesteps k1, k2, . . .

1: for k = 0, 1, . . . , K − 1 do
2:

for m = 1, . . . , M in parallel do

3:
4:
5:
6:
7:
8:

9:

i.i.d.∼ Dm

Sample ξm
if data are identical then
m = ∇f (xk

Compute gk

m; ξm) such that E (cid:2)gk

m | xk
m

(cid:3) = ∇f (xk
m)

else

Compute gk

m = ∇fm(xk

m; ξm) such that E (cid:2)gk

m | xk
m

(cid:3) = ∇fm(xk
m)

end if

xk+1
m =

(cid:80)M

(cid:40) 1
j=1(xk
m
m − γgk
xk
m,

j − γgk

j ),

if k = kp for some p ∈ N
otherwise.

end for

10:
11: end for

trying to solve. Given that there are M devices and corresponding local losses fm : Rd →
R, we want to ﬁnd

(cid:34)

min
x∈Rd

f (x) =

1
M

M
(cid:88)

m=1

(cid:35)

fm(x)

.

(2.1)

In the case of identical data, we are able to obtain on each node an unbiased estimate
of the gradient ∇f .
In the case of heterogeneous data, m-th node can only obtain an
unbiased estimate of the gradient ∇fm. Data similarity can then be formulated in terms
of the diﬀerences between functions f1, . . . , fM . If the underlying data giving rise to the
loss functions are i.i.d., the function share optima and one could even minimize them
separately, averaging the results at the end. We will demonstrate this rigorously later in
this chapter.

If the data are dissimilar, however, we need to be much more careful since running SGD
locally will yield solutions of local problems. Clearly, their average might not minimize the
true objective (2.1), and this poses signiﬁcant issues for the convergence of Local SGD.
To properly discuss the eﬃciency of Local SGD, we also need a practical way of quan-
tifying it. Normally, a method’s eﬃciency is measured by the total number of times each
function fm is touched and the cost of the touches. On the other hand, in distributed
learning we also care about how much information each computing node needs to commu-
nicate. In fact, when communication is as expensive as is the case in FL, we predominantly
care about communication. The question we address in this chapter, thus, can be posed
as follows: how many times does each node need to communicate if we want to solve (2.1)
up to accuracy ε? Equivalently, we can ask for the optimal synchronization interval length
between communications, H, i.e., how many computation steps per one communication
we can allow for. We next review related work and then present our contributions.

Table 2.1: Existing theoretical bounds for Local SGD for identical data with convex
objectives.

45

Unbounded
gradient

H = K
convergent

# communications
f strongly convex

# communications
f convex

Reference

(cid:55)

(cid:55)

(cid:51)
(cid:51)
(cid:51)

(cid:55)

(cid:55)

(cid:55)
(cid:55)
(cid:51)

√

Ω(

M K)
(cid:17)

(cid:16)√

Ω

M K
˜Ω (M )
˜Ω (cid:0)M 1/3K 1/3(cid:1)b
˜Ω(M )

(cid:55)

(cid:55)

[252]

[12]

Ω(M 3/2K 1/2)
—
Ω(M 3/2K 1/2)

[255]
[92]
THIS THESIS

a C(K) denotes the minimum number of communication steps required at each of K

iterations to achieve a linear speedup in the number of nodes M .

b The PL inequality, a generalization of strong convexity, is assumed in [92], but for

comparison we specialize to strong convexity.

2.2 Related Work

While Local SGD has been used among practitioners for a long time, see, e.g., [55, 161],
its theoretical analysis has been limited until recently. Early theoretical work on the
convergence of local methods exists as in [159], but no convergence rate was given there.
The previous work can mainly be divided into two groups: those assuming identical data
(that all nodes have access to the same dataset) and those that allow each node to
hold its own dataset. As might be expected, the analysis in the latter case is more
challenging, more limited, and usually shows worse rates. We note that in recent work
more sophisticated local stochastic gradient methods have been considered, for example
with momentum [284, 272], with quantization [221, 12], with adaptive stepsizes [280] and
with various variance-reduction methods [142, 246, 113]. Our work is complimentary to
these approaches, and provides improved rates and analysis for the vanilla method.

2.2.1 Local SGD with identical data

The analysis of Local SGD in this setting shows that a reduction in communication is
possible without aﬀecting the asymptotic convergence rate of Mini-Batch SGD with M
nodes (albeit with usually worse dependence on constants). An overview of related work
on Local SGD for convex objectives is given in Table 2.1. We note that analysis for
non-convex objectives has been carried out in a few recent works [298, 271, 107], but our
focus in this thesis is on convex objectives and hence they were not included in Table 2.1.
The comparison shows that we attain superior rates in the strongly convex setting to
previous work with the exception of the concurrent work of Stich and Karimireddy [255]
and we attain these rates under less restrictive assumptions on the optimization process
compared to them. We further provide a novel analysis in the convex case, which has
not been previously explored in the literature, with the exception of [255]. Their analysis

Table 2.2: Existing theoretical bounds for Local SGD with heterogeneous data.

46

Unbounded

Unbounded

# communications

# communications

# communications

gradient

dissimilarity/diversity

f strongly convex

f convex

f non-convex

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

—

—

(cid:16)√

(cid:17)

M K

Ω
Ω (cid:0)M 1/3K 1/3(cid:1)

—

—

—

—

—
Ω (cid:0)M 3/4K 3/4(cid:1)

Ω (cid:0)M 3/4K 3/4(cid:1)
Ω(K)
Ω (cid:0)M 3/4K 3/4(cid:1)
Ω (cid:0)M 3/2K 1/2(cid:1)

Reference

[285]

[107]

[12]

[93]

—

THIS THESIS

attains the same communication complexity but is much more pessimistic about possible
values of H.
In particular, it does not recover the convergence of one-shot averaging,
i.e., substituting H = K or even H = K/M gives noninformative bounds, unlike our
Theorem 2.4.2.

In addition to the works listed in the table, Dieuleveut and Patel [64] also analyze Local
SGD for identical data under a Hessian smoothness assumption in addition to gradient
smoothness, strong convexity, and uniformly bounded variance. However, we believe that
there are issues in their proof that we explain in Section B.5 in the supplementary material.
As a result, the work is excluded from the table.

2.2.2 Local SGD with heterogeneous data

An overview of related work on Local SGD in this setting is given in Table 2.2. In addition
to the works in Table 2.2, Wang et al. [273] analyze a local gradient descent method under
convexity, bounded dissimilarity, and bounded gradients, but do not show convergence to
arbitrary precisions. Li et al. [140] analyze Federated Averaging (discussed below) in the
strongly convex and non-convex cases under bounded gradient norms. However, their
result is not included in Table 2.2 because in the more general setting of Federated
Averaging, their analysis and experiments suggest that retaining a linear speedup is not
possible.

Local SGD is at the core of the Federated Averaging algorithm which is popular in
federated learning applications [124]. Essentially, Federated Averaging is a variant of
Local SGD with participating devices sampled randomly. This algorithm has been used
in several machine learning applications such as mobile keyboard prediction [98], and
strategies for improving its communication eﬃciency were explored in [124]. Despite its
empirical success, little is known about convergence properties of this method and it has
been observed to diverge when too many local steps are performed [162]. This is not so
surprising as the majority of common assumptions are not satisﬁed; in particular, the data
are typically very non-i.i.d. [162], so the local gradients can point in diﬀerent directions.
This property of the data can be written for any vector x and indices i, j as

(cid:107)∇fi(x) − ∇fj(x)(cid:107) (cid:29) 1.

Unfortunately, it is very hard to analyze local methods without assuming a bound on
the dissimilarity of ∇fi(x) and ∇fj(x). For this reason, almost all prior work assumed

47

some regularity notion over the functions such as bounded dissimilarity [284, 140, 285,
273] or bounded gradient diversity [93] and addressed other less challenging aspects of
federated learning such as decentralized communication, non-convexity of the objective
or unbalanced data partitioning. In fact, a common way to make the analysis simple is to
assume Lipschitzness of local functions, (cid:107)∇fi(x)(cid:107) ≤ G for any x and i. We argue that this
assumption is pathological and should be avoided when seeking a meaningful convergence
bound. First of all, in unconstrained strongly convex minimization this assumption cannot
be satisﬁed, and both are assumed by Stich [252]. Second, there exists at least one
method, whose convergence is guaranteed under bounded variance [109], but in practice
the method diverges [45, 171]. Finally, under the bounded gradients assumption we have

(cid:107)∇fi(x) − ∇fj(x)(cid:107) ≤ (cid:107)∇fi(x)(cid:107) + (cid:107)∇fj(x)(cid:107) ≤ 2G.

In other words, we lose control over the diﬀerence between the functions. Since G bounds
not just dissimilarity, but also the gradients themselves, it makes the statements less
insightful or even vacuous. For instance, it is not going to be tight if the data are actually
i.i.d. since G in that case will remain a positive constant. In contrast, we will show that
the rate should depend on a much more meaningful quantity,

σ2
dif

def=

1
M

M
(cid:88)

m=1

Eξm∼Dm

(cid:2)(cid:107)∇fm(x(cid:63); ξm)(cid:107)2(cid:3) ,

where x(cid:63) is a ﬁxed minimizer of f and fm(·; ξm) for ξm ∼ D are stochastic realizations
of fm (see the next section for the setting). Obviously, for all nondegenerate sampling
is ﬁnite and serves as a natural measure of variance
distributions Dm the quantity σdif
in local methods. We note that an attempt to get more general convergence statement
has been made by Li et al. [139], but unfortunately their guarantee is strictly worse than
that of Mini-Batch Stochastic Gradient Descent (SGD). In the overparameterized regime
where σdif = 0, Zhang and Li [291] prove the convergence of Local SGD with arbitrary
H.

In an earlier workshop paper [115], we explicitly analyzed Local Gradient Descent
(Local GD) as opposed to Local SGD, where there is no stochasticity in the gradients. An
analysis of Local GD for non-convex objectives with the PL inequality and under bounded
gradient diversity was subsequently carried out by Haddadpour and Mahdavi [93].

2.3 Settings and Contributions

We use a notation similar to that of Stich [252] and denote the sequence of time stamps
when synchronization happens as (kp)∞
M at
time k ≥ 0 we deﬁne

p=1. Given stochastic gradients gk

2 , . . . , gk

1 , gk

gk def=

1
M

M
(cid:88)

m=1

m, ¯gk
gk
m

def= E (cid:2)gk

m

(cid:3) =

(cid:40)

∇f (xk
m)
∇fm(xk
m) otherwise.

for identical data

, ¯gk def= E (cid:2)gk(cid:3) .

We deﬁne an epoch to be a sequence of timesteps between two synchronizations:

48

for p ∈ N an epoch is the sequence kkp, kkp+1, . . . , kkp+1−1. We summarize some of the
notation used in Table A.2.

The assumptions of this chapter are a bit diﬀerent from that of others because we
consider M potentially diﬀerent distribution D1, . . . , DM . As we shall show below, this
distinction is quite important because when the data are identical, the convergence be-
comes signiﬁcantly faster.

Assumption 2.3.1. Assume that the set of minimizers of (2.1) is nonempty. Each fm
is µ-strongly convex for µ ≥ 0 and L-smooth. That is, for all x, y ∈ Rd

µ
2

(cid:107)x − y(cid:107)2 ≤ fm(x) − fm(y) − (cid:104)∇fm(y), x − y(cid:105) ≤

L
2

(cid:107)x − y(cid:107)2.

When µ = 0, we say that each fm is just convex. When µ (cid:54)= 0, we deﬁne κ def= L
condition number.

µ , the

Assumption 2.3.1 formulates our requirements on the overall objective. Next, we
have two diﬀerent sets of assumptions on the stochastic gradients that model diﬀerent
scenarios, which also lead to diﬀerent convergence rates.

Assumption 2.3.2. Given a function h, a point x ∈ Rd, and a sample ξ ∼ D drawn i.i.d.
according to a distribution D, the stochastic gradients ∇h(x; ξ) satisfy Eξ∼D [∇h(x; ξ)] =
∇h(x), Eξ∼D

(cid:2)(cid:107)∇h(x; ξ) − ∇h(x)(cid:107)2(cid:3) ≤ σ2.

Assumption 2.3.2 holds for example when ∇h(x; ξ) = ∇h(x)+ξ for a random variable
(cid:2)(cid:107)ξ(cid:107)2(cid:3) ≤ σ2. Assumption 2.3.2, however,
ξ of expected bounded squared norm: Eξ∼D
typically does not hold for ﬁnite-sum problems where g(x; ξ) is a gradient of one of the
functions in the ﬁnite-sum. To capture this setting, we consider the following assumption:

Assumption 2.3.3. Given an L-smooth and µ-strongly convex (possibly with µ = 0)
function h : Rd → R written as an expectation h = Eξ∼D [h(x; ξ)], we assume that a
stochastic gradient g = g(h, x; ξ) is computed by g(h, x; ξ) = ∇h(x; ξ). We assume that
h(·, ξ) : Rd → R is almost-surely L-smooth and µ-strongly convex (with the same L and
µ as h).

When Assumption 2.3.3 is assumed in the identical data setting, we assume it is
satisﬁed on each node m ∈ [M ] with h = f and distribution Dm, and we deﬁne as a
measure of variance at the optimum

σ2
opt

def=

1
M

M
(cid:88)

m=1

Eξm∼Dm

(cid:2)(cid:107)∇f (x(cid:63); ξm)(cid:107)2(cid:3) .

Whereas in the heterogeneous data setting we assume that it is satisﬁed on each node
m ∈ [M ] with h = fm and distribution Dm, and we analogously deﬁne

σ2
dif

def=

1
M

M
(cid:88)

m=1

Eξm∼Dm

(cid:2)(cid:107)∇fm(x(cid:63); ξm)(cid:107)2(cid:3) .

49

Assumption 2.3.3 holds, for example, for ﬁnite-sum optimization problems with uni-
form sampling and permits direct extensions to more general settings such as expected
smoothness [84].

Our contributions. In this chapter, we achieve the following:

1. In the identical data setting under Assumptions 2.3.1 and 2.3.2 with µ > 0, we

prove that the iteration complexity of Local SGD to achieve ε-accuracy is

˜O

(cid:18) σ2

(cid:19)

µ2M ε

in squared distance from the optimum provided that K = Ω (κ (H − 1)). This
improves the communication complexity in prior work (see Table 2.1) with a tighter
results compared to concurrent work (recovering convergence for H = 1 and H =
K). When µ = 0 we have that the iteration complexity of Mini-Batch SGD to
attain an ε-accurate solution in functional suboptimality is

(cid:32)

O

L2 (cid:107)x0 − x(cid:63)(cid:107)4
M ε2

+

σ4
L2M ε2

(cid:33)

,

provided that K = Ω (M 3H 2). We further show that the same ε-dependence holds
in both the µ > 0 and µ = 0 cases under Assumption 2.3.3. This has not been
explored in the literature on Local SGD before, and hence we obtain the ﬁrst results
that apply to arbitrary convex and smooth ﬁnite-sum problems.

2. When the data on each node is diﬀerent and Assumptions 2.3.1 and 2.3.3 hold
with µ = 0, the iteration complexity needed by Local SGD to achieve an ε-accurate
solution in functional suboptimality is

(cid:32)

O

L2 (cid:107)x0 − x(cid:63)(cid:107)4
M ε2

+

σ4
dif
L2M ε2

(cid:33)

provided that K = Ω(M 3H 4). This improves upon previous work by not requiring
any restrictive assumptions on the gradients and is the ﬁrst analysis to capture true
data heterogeneity between diﬀerent nodes.

3. We verify our results by experimenting with logistic regression on multiple datasets,

and investigate the eﬀect of heterogeneity on the convergence speed.

2.4 Convergence Theory

The following quantity is crucial to the analysis of both variants of Local SGD, and
measures the deviation of the iterates from their average ˆxk over an epoch:

V k def=

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

where ˆxk def=

1
M

M
(cid:88)

m=1

xk
m.

50

To prove our results, we follow the line of work started by Stich [252] and ﬁrst show a
recurrence similar to that of SGD up to an error term proportional to V k, then we bound
each V k term individually or the sum of V k’s over an epoch. All proofs are relegated to
the supplementary material.

2.4.1 Identical data

Our ﬁrst lemma presents a bound on the sequence of the V k in terms of the synchroniza-
tion interval H.

Lemma 2.4.1. Choose a stepsize γ > 0 such that γ ≤ 1
2L . Under Assumptions 2.3.1,
and 2.3.2 we have that for Algorithm 1 with maxp |kp − kp+1| ≤ H and with identical
data, for all k ≥ 1

E (cid:2)V k(cid:3) ≤ (H − 1) γ2σ2.

Combining Lemma 2.4.1 with perturbed iterate analysis as in [252] we can recover the

convergence of Local SGD for strongly-convex functions:

Theorem 2.4.2. Suppose that Assumptions 2.3.1, and 2.3.2 hold with µ > 0. Then for
Algorithm 1 run with identical data, a constant stepsize γ > 0 such that γ ≤ 1
4L , and
H ≥ 1 such that maxp |kp − kp+1| ≤ H,

E

(cid:104)(cid:13)
(cid:13)ˆxK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K(cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γσ2
µM

+

2Lγ2 (H − 1) σ2
µ

.

(2.2)

By (2.2) we see that the convergence of Local SGD is the same as Mini-Batch SGD
plus an additive error term which can be controlled by controlling the size of H, as the
next corollary and the successive discussion show.

Corollary 2.4.3. Choosing γ = 1
µa, with a = 4κ + k for k > 0 and we take K = 2a log a
steps. Then substituting in (2.2) and using that 1 − x ≤ exp(−x) and some algebraic
manipulation we can conclude that,

E

(cid:104)(cid:13)
(cid:13)ˆxK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:32)

(cid:107)x0 − x(cid:63)(cid:107)2
K 2

+

σ2
µ2M K

+

κσ2(H − 1)
µ2K 2

(cid:33)

.

where ˜O(·) ignores polylogarithmic and constant numerical factors.

Recovering fully synchronized Mini-Batch SGD. When H = 1 the error term

vanishes and we obtain directly the ordinary rate of Mini-Batch SGD.

Linear speedup in the number of nodes M . We see that choosing H = O(K/M )
leads to an asymptotic convergence rate of ˜O
which shows the same linear
speedup of Mini-Batch SGD but with worse dependence on κ. The number of communi-
cations in this case is then C(K) = K/H = ˜Ω(M ).

(cid:16) σ2κ
µ2M K

(cid:17)

Local SGD vs. Mini-Batch SGD. We assume that the statistical σ2/K dependence
dominates the dependence on the initial distance (cid:107)x0 − x(cid:63)(cid:107)2/K 2. From Corollary 2.4.3,

51

we see that in order to achieve the same convergence guarantees as Mini-Batch SGD, we
(cid:1), achieving a communication complexity of O (κM ). This is only
must have H = O (cid:0) K
possible when K > κM .
It follows that given a number of steps K the optimal H is
H = 1 + (cid:98)K/(κM )(cid:99) achieving a communication complexity of ˜Ω (min(K, κM )).

κM

One-shot averaging. Putting H = K+1 yields a convergence rate of ˜O(σ2κ/(µ2K)),
showing no linear speedup but showing convergence, which improves upon all previous
work. However, we admit that simply using Jensen’s inequality to bound the distance
of the average iterate E
would yield a better asymptotic convergence rate
of ˜O(σ2/(µ2K)). Under a Lipschitz Hessian assumption, Zhang et al. [293] show that
one-shot averaging can attain a linear speedup in the number of nodes, so one may do
analysis of Local SGD under this additional assumption to try to remove this gap, but
this is beyond the scope of our work.

(cid:104)(cid:13)
(cid:13)ˆxK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

Similar results can be obtained for weakly convex functions, as the next Theorem

shows.

Theorem 2.4.4. Suppose that Assumptions 2.3.1, 2.3.2 hold with µ = 0 and that a
constant stepsize γ such that γ ≥ 0 and γ ≤ 1
4L is chosen and that Algorithm 1 is run for
identical data with H ≥ 1 such that supp |kp − kp+1| ≤ H, then for ¯xK = 1
K

k=1 ˆxk,

(cid:80)K

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

2
γK

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)

2γσ2
M

+ 4γ2Lσ2 (H − 1) .

(2.3)

Theorem 2.4.4 essentially tells the same story as Theorem 2.4.2: convergence of
Local SGD is the same as Mini-Batch SGD up to an additive constant whose size can be
controlled by controlling H.

Corollary 2.4.5. Assume that K ≥ M . Choosing γ =
we have,

√

M
√

4L

K

, then substituting in (2.3)

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

8L(cid:107)x0 − x(cid:63)(cid:107)2
M K

√

+

σ2
√
M K

2L

+

σ2M (H − 1)
LK

.

√

Linear speedup and optimal H. From Corollary 2.4.5 we see that if we choose
KM −3/2) then we obtain a linear speedup, and the number of communication
H = O(
steps is then C = K/H = Ω (cid:0)M 3/2K 1/2(cid:1), and we get that the optimal H is then
H = 1 + (cid:4)K 1/2M −3/2(cid:5).

The previous results were obtained under Assumption 2.3.2. Unfortunately, this as-
sumption does not easily capture the ﬁnite-sum minimization scenario where f (x) =
1
m is sampled uniformly at random from the
n
sum.

i=1 fi(x) and each stochastic gradient gk

(cid:80)n

Using smaller stepsizes and more involved proof techniques, we can show that our
results still hold in the ﬁnite-sum setting. For strongly-convex functions, the next theorem
shows that the same convergence guarantee as Theorem 2.4.2 can be attained.

Theorem 2.4.6. Suppose that Assumptions 2.3.1 and 2.3.3 hold with µ > 0. Suppose
that Algorithm 1 is run for identical data with maxp |kp − kp+1| ≤ H for some H ≥ 1

52

Figure 2.1: The eﬀect of the dataset and number of workers M on the variance parameters.
Left: ‘a8a’, middle: ‘mushrooms’, right: ‘w8a’ dataset. We use uniform sampling of data
points, so σ2
dif with M = 1, while for higher values of M the value of
dif might be drastically larger than σ2
σ2

opt is the same as σ2

opt.

and with a stepsize γ > 0 chosen such that γ ≤ min

any timestep k such that synchronization occurs,

(cid:26)

1
4L(1+ 2

M )

,

1
µ+8L(H−1)

(cid:27)

. Then for

E

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)k (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

opt

2γσ2
µM

+

4σ2

optγ2 (H − 1) L
µ

.

(2.4)

As a corollary, we can obtain an asymptotic convergence rate by choosing speciﬁc

stepsizes γ and H.

Corollary 2.4.7. Let a = 18κk for some k > 0, let H ≤ k and choose γ = 1
We substitute in (2.4) and take K = 18a log a steps, then

µa ≤ 1

9LH .

E

(cid:104)(cid:13)
(cid:13)ˆxK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:32)

(cid:107)x0 − x(cid:63)(cid:107)2
K 2

+

σ2
opt
µ2M K

+

σ2
optκ(H − 1)
µ2K 2

(cid:33)
.

Substituting H = 1 + (cid:98)t/M (cid:99) = 1 + (cid:98)K/(18κM )(cid:99) in Corollary 2.4.7 we get an
asymptotic convergence rate of ˜O
. This preserves the rate of Mini-Batch SGD
up to problem-independent constants and polylogarithmic factors, but with possibly fewer

opt
KM

(cid:16) σ2

(cid:17)

02004006008001000Local minibatch size10−310−210−1100Varianceσ2optσ2dif, M=10σ2dif, M=100σ2dif, M=2500100200300400500Local minibatch size10−410−310−2Varianceσ2optσ2dif, M=10σ2dif, M=100σ2dif, M=2500100200300400500Local minibatch size10−310−210−1Varianceσ2optσ2dif, M=10σ2dif, M=100σ2dif, M=250communication steps.

53

Theorem 2.4.8. Suppose that Assumptions 2.3.1 and 2.3.3 hold with µ = 0, that a
stepsize γ ≤ 1
10LH is chosen and that Algorithm 1 is run on M ≥ 2 nodes with identical
data and with supp |kp − kp+1| ≤ H, then for any timestep K such that synchronization
occurs we have for ¯xK = 1
K

k=1 ˆxk that

(cid:80)K

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

10(cid:107)x0 − x(cid:63)(cid:107)2
γK

+

20γσ2
M

opt

+ 40γ2Lσ2

opt (H − 1) .

(2.5)

Corollary 2.4.9. Let H ≤
it into (2.5) yields

√
K√
M

, then for γ =

√

M
√

10L

K

we see that γ ≤ 1

10LH , and plugging

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

100L(cid:107)x0 − x(cid:63)(cid:107)2
KM

√

+

2σ2
√

opt
KM

L

2σ2

optM (H − 1)
5LK

.

This is the same result as Corollary 2.4.5, and hence we see that choosing H =

O (cid:0)K 1/2M −3/2(cid:1) (when K > M 3) yields a linear speedup in the number of nodes M .

2.4.2 Heterogeneous data

We next show that for arbitrarily heterogeneous convex objectives, the convergence of
Local SGD is the same as Mini-Batch SGD plus an error that depends on H.

Theorem 2.4.10. Suppose that Assumptions 2.3.1 and 2.3.3 hold with µ = 0 and
for heterogeneous data. Then for Algorithm 1 run for diﬀerent data with M ≥ 2,
, then we
maxp |kp − kp+1| ≤ H, and a stepsize γ > 0 such that γ ≤ min
have

1
8L(H−1)

(cid:110) 1
4L,

(cid:111)

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

4(cid:107)x0 − x(cid:63)(cid:107)2
γK

+

20γσ2
dif
M

+ 16γ2L(H − 1)2σ2

dif.

where ¯xK def= 1
K

(cid:80)K−1

i=0 ˆxi.

Dependence on σdif. We see that the convergence guarantee given by Theo-
rem 2.4.10 shows a dependence on σdif, which measures the heterogeneity of the data
distribution.
In typical (non-federated) distributed learning settings where data are dis-
tributed before starting training, this term can vary quite signiﬁcantly depending on how
the data are distributed.

Dependence on H. We further note that the dependence on H in Theorem 2.4.10 is
quadratic rather than linear. This translates to a worse upper bound on the synchronization
interval H that still allows convergence, as the next corollary shows.
√
K√
M

8HL , and hence applying the

, then γ =

≤ 1

M
√

8L

K

√

Corollary 2.4.11. Choose H ≤
result of Theorem 2.4.10,

E (cid:2)f (¯xK) − f (x(cid:63))(cid:3) ≤

32L(cid:107)x0 − x(cid:63)(cid:107)2
M K

√

+

5σ2
dif
√
M K

2L

+

difM (H − 1)2
σ2
4LK

.

54

Figure 2.2: Results on ‘a9a’ dataset, with stepsize 1
L . For any value of local iterations
H the method converged to a neighborhood within a small number of communication
rounds due to large stepsizes.

Figure 2.3: Convergence on heterogeneous data with diﬀerent number of local steps on
the ‘a5a’ dataset. 1 local step corresponds to fully synchronized gradient descent. Left:
convergence in terms of communication rounds, which shows a clear advantage of Local
GD when only limited accuracy is required. Mid plot: wall-clock time might improve only
slightly if communication is cheap. Right: what changes with diﬀerent communication
cost.

Optimal H. By Corollary 2.4.11 we see that the optimal value of H is H = 1 +
convergence rate. Thus, the same convergence

(cid:4)K 1/4M −3/4(cid:5), which gives O

(cid:16) 1√

(cid:17)

M K

55

Figure 2.4: Convergence of Local SGD on heterogeneous data with diﬀerent number of
local steps on the ‘a5a’ dataset.

rate is attained provided that communication is more frequent compared to the identical
data regime.

2.5 Experiments

All experiments described below were run on logistic regression problem with (cid:96)2 regular-
ization of order 1
n . The datasets were taken from the LIBSVM library [44]. The code was
written in Python using MPI [59] and run on Intel(R) Xeon(R) Gold 6146 CPU @3.20GHz
cores in parallel.

2.5.1 Variance measures

dif and σ2

We provide values of σ2
opt in Figure 2.1 for diﬀerent datasets, mini-batch sizes
and M . The datasets were split evenly without any data reshuﬄing and no overlaps. For
any M > 1, the value of σdif is lower bounded by 1
m=1 (cid:107)∇fm(x(cid:63))(cid:107)2 which explains
M
the diﬀerence between identical and heterogeneous data.

(cid:80)M

2.5.2 Identical data

L and 0.05

For identical data we used M = 20 nodes and ’a9a’ dataset. We estimated L numerically
and ran two experiments, with stepsizes 1
L and minibatch size equal 1. In both
cases we observe convergence to a neighborhood, although of a diﬀerent radius. Since we
run the experiments on a single machine, the communication is very cheap and there is
little gain in time required for convergence. However, the advantage in terms of required
communication rounds is self-evident and can lead to signiﬁcant time improvement under
slow communication networks. The results are provided here in Figure 2.2 and in the
supplementary material in Figure B.1.

2.5.3 Heterogeneous data

56

Since our architecture leads to a very speciﬁc trade-oﬀ between computation and com-
munication, we provide plots for the case the communication time relative to gradient
computation time is higher or lower. To see the impact of σdif, in all experiments we use
full gradients ∇fm and constant stepsize 1
L. The data partitioning is not i.i.d. and is done
based on the index in the original dataset. The results are provided in Figure 2.3 and in
the supplementary material in Figure B.2. In cases where communication is signiﬁcantly
more expensive than gradient computation, local methods are much faster for imprecise
convergence.

57

Chapter 3

Convergence of Random Reshuﬄing

3.1 Introduction

We study the ﬁnite-sum minimization problem

(cid:104)
f (x) =

min
x∈Rd

1
n

n
(cid:88)

i=1

(cid:105)
,
fi(x)

(3.1)

where each fi : Rd → R is diﬀerentiable and smooth, and we are particularly interested in
the big data machine learning setting where the number of functions n is large. Thanks to
their scalability and low memory requirements, ﬁrst-order methods are especially popular
in this setting [31]. Stochastic ﬁrst-order algorithms in particular have attracted a lot
of attention in the machine learning community and are often used in combination with
various practical heuristics. Explaining these heuristics may lead to further development
of stable and eﬃcient training algorithms. In this chapter, we aim at better and sharper
theoretical explanation of one intriguingly simple but notoriously elusive heuristic: data
permutation/shuﬄing.

In particular, the goal of this chapter is to obtain deeper theoretical understanding of
methods for solving (3.1) which rely on random or deterministic permutation/shuﬄing of
the data {1, 2, . . . , n} and perform incremental gradient updates following the permuted
order. We study three methods which belong to this class, described next.

An immensely popular but theoretically elusive method belonging to the class of
data permutation methods is the Random Reshuﬄing (RR) algorithm (see Algo-
In each epoch
rithm 2). This is the method we pay most attention to in this chapter.
t of RR, we sample indices π0, π1, . . . , πn−1 without replacement from {1, 2, . . . , n}, i.e.,
{π0, π1, . . . , πn−1} is a random permutation of the set {1, 2, . . . , n}, and proceed with n
iterates of the form

i+1 = xk
xk

i − γ∇fπi(xk

i ),

where γ > 0 is a stepsize. We then set xk+1 = xk
n, and repeat the process for a total of
T epochs. Notice that in RR, a new permutation/shuﬄing is generated at the beginning
of each epoch, which is why the term reshuﬄing is used.

Furthermore, we consider the Shuﬄe-Once (SO) algorithm, which is identical to
RR with the exception that it shuﬄes the dataset only once—at the very beginning—and
then reuses this random permutation in all subsequent epochs (see Algorithm 3). Our
results for SO follow as corollaries of the tools we developed in order to conduct a sharp
analysis of RR.

Finally, we also consider the Incremental Gradient (IG) algorithm, which is identical

58

to SO, with the exception that the initial permutation is not random but deterministic.
Hence, IG performs incremental gradient steps through the data in a cycling fashion. The
ordering could be arbitrary, e.g., it could be selected implicitly by the ordering the data
comes in, or chosen adversarially. Again, our results for IG follow as a byproduct of our
eﬀorts to understand RR.

Algorithm 2 Random Reshuﬄing (RR)

Algorithm 3 Shuﬄe-Once (SO)

Require: Stepsize γ > 0,

initial vector

Require: Stepsize γ > 0,

initial vector

x0 = x0

0 ∈ Rd, number of epochs T
1: for epochs k = 0, 1, . . . , T − 1 do
2:

Sample

a

permutation

π0, π1, . . . , πn−1 of {1, 2, . . . , n}
for i = 0, 1, . . . , n − 1 do
i − γ∇fπi(xk
i )

3:
4:
5:
6:
7: end for

i+1 = xk
xk
end for
xk+1 = xk
n

x0 = x0

0 ∈ Rd, number of epochs T

1: Sample a permutation π0, π1, . . . , πn−1

of {1, 2, . . . , n}

2: for epochs k = 0, 1, . . . , T − 1 do
for i = 0, 1, . . . , n − 1 do
3:
i − γ∇fπi(xk
i )
4:
5:
6:
7: end for

i+1 = xk
xk
end for
xk+1 = xk
n

3.2 Related Work

RR is usually contrasted with its better-studied sibling Stochastic Gradient Descent (SGD),
in which each πi is sampled uniformly with replacement from {1, 2, . . . , n}. RR often
converges faster than SGD on many practical problems [29, 218], is more friendly to
cache locality [19], and is in fact standard in deep learning [257].

The convergence properties of SGD are well-understood, with tightly matching lower
and upper bounds in many settings [215, 65, 193]. Sampling without replacement allows
RR to leverage the ﬁnite-sum structure of (3.1) by ensuring that each function contributes
to the solution once per epoch. On the other hand, it also introduces a signiﬁcant
complication: the steps are now biased. Indeed, in any iteration i > 0 within an epoch,
we face the challenge of not having (conditionally) unbiased gradients since

E (cid:2)∇fπi(xk

i ) | xk
i

(cid:3) (cid:54)= ∇f (xk

i ).

This bias implies that individual iterations do not necessarily approximate a full gradient
descent step. Hence, in order to obtain meaningful convergence rates for RR, it is necessary
to resort to more involved proof techniques. In recent work, various convergence rates have
been established for RR. However, a satisfactory, let alone complete, understanding of the
algorithm’s convergence remains elusive. For instance, the early line of attack pioneered
by Recht and R´e [217] seems to have hit the wall as their noncommutative arithmetic-
geometric mean conjecture is not true [129]. The situation is even more pronounced with
the SO method, as Safran and Shamir [231] point out that there are no convergence
results speciﬁc for the method, and the only convergence rates for SO follow by applying
the worst-case bounds of IG. Rajput et al. [213] state that a common practical heuristic

59

is to use methods like SO that do not reshuﬄe the data every epoch. Indeed, they add
that “current theoretical bounds are insuﬃcient to explain this phenomenon, and a new
theoretical breakthrough may be required to tackle it”.

IG has a long history owing to its success in training neural networks [151, 89], and
its asymptotic convergence has been established early [160, 24]. Several rates for non-
smooth and smooth cases were established by [179, 140, 90, 283] and [192]. Using IG
poses the challenge of choosing a speciﬁc permutation for cycling through the iterates,
which Nedi´c and Bertsekas [179] note to be diﬃcult. Bertsekas [21] gives an example
that highlights the susceptibility of IG to bad orderings compared to RR. Yet, thanks to
G¨urb¨uzbalaban et al. [91] and Haochen and Sra [97], RR is known to improve upon both
SGD and IG for twice-smooth objectives. Nagaraj et al. [178] also study convergence of
RR for smooth objectives, and Safran and Shamir citeSafran2020good as well as Rajput
et al. Rajput2020 give lower bounds for RR and related methods.

3.3 Settings and Contributions

In this chapter, we study the convergence behavior of the data-permutation methods RR,
SO and IG. While existing proof techniques succeed in obtaining insightful bounds for RR
and IG, they fail to fully capitalize on the intrinsic power reshuﬄing and shuﬄing oﬀers,
and are not applicable to SO at all1. Our proof techniques are dramatically novel, simple,
more insightful, and lead to improved convergence results, all under weaker assumptions
on the objectives than prior work.

We will derive results for strongly convex, convex as well as non-convex objectives. To
compare between the performance of ﬁrst-order methods, we deﬁne an ε-accurate solution
as a point ˜x ∈ Rd that satisﬁes (in expectation if ˜x is random)

(cid:107)∇f (˜x)(cid:107) ≤ ε,

or

(cid:107)˜x − x(cid:63)(cid:107)2 ≤ ε,

or

f (˜x) − f (x(cid:63)) ≤ ε

for non-convex, strongly convex, and non-strongly convex objectives, respectively, and
where x(cid:63) is assumed to be a minimizer of f if f is convex. We then measure the
performance of ﬁrst-order methods by the number of individual gradients ∇fi(·) they
access to reach an ε-accurate solution.

Our ﬁrst assumption is that the objective is bounded from below and smooth. This

assumption is used in all of our results and is widely used in the literature.

Assumption 3.3.1. The objective f and the individual losses f1, . . . , fn are all L-smooth,
i.e., their gradients are L-Lipschitz. Further, f is lower bounded by some f (cid:63) ∈ R. If f is
convex, we also assume the existence of a minimizer x(cid:63) ∈ Rd.

Assumption 3.3.1 is necessary in order to obtain better convergence rates for RR
compared to SGD, since without smoothness the SGD rate is optimal and cannot be
improved [178].

1As we have mentioned before, the best known bounds for SO are those which apply to IG also, which

means that the randomness inherent in SO is wholly ignored.

60
3.3.1 New and improved convergence rates for RR, SO and IG

If each fi

In Section 3.4, we analyze the RR and SO methods and present novel convergence rates
for strongly convex, convex, and non-convex smooth objectives. Our results for RR are
summarized in Table 3.1.
• Strongly convex case.

is strongly convex, we introduce a new proof
technique for studying the convergence of RR/SO that allows us to obtain a better
dependence on problem constants, such as the number of functions n and the condition
number κ, compared to prior work (see Table 3.1). Key to our results is a new notion of
variance speciﬁc to RR/SO (see Deﬁnition 3.4.2), which we argue explains the superior
convergence of RR/SO compared to SGD in many practical scenarios. Our result for
SO tightly matches the lower bound of [231]. We prove similar results in the more
general setting when each fi is convex and f is strongly convex (see Theorem 3.4.5),
but in this case we are forced to use smaller stepsizes.

• Convex case. For convex but not necessarily strongly convex objectives fi, we give the
ﬁrst result showing that RR/SO can provably achieve better convergence than SGD for
a large enough number of iterations. This holds even when comparing against results
that assume second-order smoothness, like the result of [97].

• Non-convex case. For non-convex objectives fi, we obtain for RR a much better

dependence on the number of functions n compared to the prior work of [192].

Furthermore, in the appendix we formulate and prove convergence results for IG for
strongly convex objectives, convex, and non-convex objectives as well. The bounds are
worse than RR by a factor of n in the noise/variance term, as IG does not beneﬁt from
randomization. Our result for strongly convex objectives tightly matches the lower bound
of [231] up to an extra iteration and logarithmic factors, and is the ﬁrst result to tightly
match this lower bound.

3.3.2 More general assumptions on the function class

Previous non-asymptotic convergence analyses of RR either obtain worse bounds that ap-
ply to IG, e.g., [283, 192], or depend crucially on the assumption that each fi is Lipschitz
[178, 97, 1]. Unfortunately, requiring each fi to be Lipschitz contradicts strong convex-
ity [190] and is furthermore not satisﬁed in least square regression, matrix factorization, or
for neural networks with smooth activations. In contrast, our work is the ﬁrst to show how
to leverage randomization to obtain better rates for RR without assuming each fi to be
Lipschitz. In concurrent work, Ahn et al. [2] also obtain a result for non-convex objectives
satisfying the Polyak-(cid:32)Lojasiewicz inequality, a generalization of strong convexity. Their
result holds without assuming bounded gradients or bounded variance, but unfortunately
with a worse dependence on κ and n when specialized to µ-strongly convex functions.
• Strongly convex and convex case. For strongly convex and convex objectives we do
not require any assumptions on the functions used beyond smoothness and convexity.
• Non-convex case. For non-convex objectives we obtain our results under a signiﬁcantly
more general assumption than the bounded gradients assumptions employed in prior

work. Our assumption is also provably satisﬁed when each function fi is lower bounded,
and hence is not only more general but also a more realistic assumption to use.

61

3.4 Convergence Theory

The following quantity is key to our analysis and serves as an asymmetric distance between
two points measured in terms of functions.
Deﬁnition 3.4.1. For any i, the quantity Dfi(x, y) def= fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105) is
the Bregman divergence between x and y associated with fi.

It is well-known that if fi is L-smooth and µ-strongly convex, then for all x, y ∈ Rd

µ
2

(cid:107)x − y(cid:107)2 ≤ Dfi(x, y) ≤

L
2

(cid:107)x − y(cid:107)2,

(3.2)

so each Bregman divergence is closely related to the Euclidian distance. Moreover, the
diﬀerence between the gradients of a convex and L-smooth fi is related to its Bregman
divergence by

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L · Dfi(x, y).

(3.3)

3.4.1 Main result: strongly convex objectives

Before we proceed to the formal statement of our main result, we need to present the
central ﬁnding of this chapter. The analysis of many stochastic methods, including SGD,
rely on the fact that the iterates converge to x(cid:63) up to some noise. This is exactly where
we part ways with the standard analysis techniques, since, it turns out, the intermediate
iterates of shuﬄing algorithms converge to some other points. Given a permutation π,
the real limit points are deﬁned below,

x(cid:63)
i

def= x(cid:63) − γ

i−1
(cid:88)

j=0

∇fπj (x(cid:63)),

i = 1, . . . , n − 1.

(3.4)

In fact, it is predicted by our theory and later validated by our experiments that within
an epoch the iterates go away from x(cid:63), and closer to the end of the epoch they make a
sudden comeback to x(cid:63).

The second reason the vectors introduced in Equation (3.4) are so pivotal is that they
allow us to deﬁne a new notion of variance. Without it, there seems to be no explanation
for why RR sometimes overtakes SGD from the very beginning of optimization process.
We deﬁne it below.

Deﬁnition 3.4.2 (Shuﬄing variance). Given a stepsize γ > 0 and a random permutation
π of {1, 2, . . . , n}, deﬁne x(cid:63)

i as in (3.4). Then, the shuﬄing variance is given by

σ2
Shuﬄe

def= max

i=1,...,n−1

(cid:20) 1
γ

E (cid:2)Dfπi

(x(cid:63)

i , x(cid:63))(cid:3)

(cid:21)

,

(3.5)

where the expectation is taken with respect to the randomness in the permutation π.

62

Table 3.1: Number of individual gradient evaluations needed by RR to reach an
ε-accurate solution (deﬁned in Section 3.4). Logarithmic factors and constants
that are not related to the assumptions are ignored. For non-convex objectives,
A and B are the constants given by Assumption 3.4.7.

Assumptions

µ-Strongly

Non-strongly

Non-convex

Citation

N.L.(1) U.V.(2)
(cid:51)

(cid:51)

convex

convex

(cid:55)

(cid:55)

(cid:51)

(cid:55)

κ2n + κnσ(cid:63)
√
ε

µ
√
κ2n + κ
µ

nG
√
ε

–
µε + κ2nσ(cid:63)
κ2n√

√

(4)

(5)

µ
ε
√
nGα3/2
ε
µ
√

√

κα

ε1/α + κ

(cid:51) κn +

√
√

n

µε + κ

nG0
√
ε

µ

(6)

(cid:51)

κ +

κn +

(7)

√

κnσ(cid:63)
√
µ
ε
√

κnσ(cid:63)
√
ε
µ

–
ε + G2D2
LD2
–

ε2

(3)

–

–

–

–

–

Ln

ε2 + LnG
ε3
–

–

–

[283]

[178]

[192]

[192]

[1]

[2]

Ln
ε +

√

Lnσ(cid:63)
ε3/2

Ln

ε2 + L

√

√

n(B+
ε3

A)

This thesis

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(1) Support

for

non-Lipschitz

functions
maxi=1,...,n (cid:107)∇fi(x)(cid:107) ≤ G for all x ∈ Rd and some G > 0.
1
(cid:63) ≤ G2 and B2 ≤ G2.
n

i=1 (cid:107)∇fi(x(cid:63))(cid:107)2 def= σ2

proofs without

(N.L.):

(cid:80)n

assuming

that
Note that

(2) Unbounded variance (U.V.): there may be no constant σ such that Assumption 3.4.7 holds
with A = 0 and B = σ. Note that when the individual gradients are bounded, the variance
is automatically bounded too.

(3) Nagaraj et al. [178] require, for non-strongly convex functions, projecting at each iteration

onto a bounded convex set of diameter D. We study the unconstrained problem.

(4) For strongly convex, Nguyen et al. [192] bound f (x)−f (x(cid:63)) rather than squared distances,

hence we use strong convexity to translate their bound into a bound on (cid:107)x − x(cid:63)(cid:107)2.

(5) The constant α > 2 is a parameter to be speciﬁed in the stepsize used by Ahn et al. [1].

Their full bound has several extra terms but we include only the most relevant ones.

(6) The result of [2] holds when f satisﬁes the Polyak-(cid:32)Lojasiewicz inequality, a generalization of
strong convexity. We nevertheless specialize it to strong convexity for our comparison. The
def= supx:f (x)≤f (x0) maxi∈[n] (cid:107)∇fi(x)(cid:107). Note that σ(cid:63) ≤ G0.
constant G0 is deﬁned as G0
We show a better complexity for PL functions under bounded variance in Theorem 3.4.9.
L , but it asks for each

(7) This result is the ﬁrst to show that RR and SO work with any γ ≤ 1

fi to be strongly convex. The second result assumes that only f is strongly convex.

63

Naturally, σ2

Shuﬄe depends on the functions f1, . . . , fn, but, unlike SGD, it also depends
in a non-trivial manner on the stepsize γ. The easiest way to understand the new notation
is to compare it to the standard deﬁnition of variance used in the analysis of SGD. We
argue that σ2
Shuﬄe is the natural counter-part for the standard variance used in SGD. We
relate both of them by the following upper and lower bounds:

Proposition 3.4.3. Suppose that each of f1, f2, . . . , fn is µ-strongly convex and L-
smooth. Then γµn
(cid:63), where σ2
(cid:63)

i=1 (cid:107)∇fi(x(cid:63))(cid:107)2.

Shuﬄe ≤ γLn

(cid:63) ≤ σ2

(cid:80)n

def= 1
n

4 σ2

8 σ2

In practice, σ2

Shuﬄe may be much closer to the lower bound than the upper bound;
see Section 3.5. This leads to a dramatic diﬀerence in performance and provides addi-
tional evidence of the superiority of RR over SGD. The next theorem states how exactly
convergence of RR depends on the introduced variance.

Theorem 3.4.4. Suppose that the functions f1, . . . , fn are µ-strongly convex and that
Assumption 3.3.1 holds. Then for Algorithms 2 or 3 run with a constant stepsize γ ≤ 1
L,
the iterates generated by either of the algorithms satisfy

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γσ2
Shuﬄe
µ

.

Proof. The key insight of our proof is that the intermediate iterates xk
converge to x(cid:63), but rather converge to the sequence x(cid:63)
this intuition in mind, it makes sense to study the following recursion:

2, . . . do not
2, . . . deﬁned by (3.4). Keeping

1, xk

1, x(cid:63)

E (cid:2)(cid:107)xk
= E (cid:2)(cid:107)xk

i+1 − x(cid:63)
i − x(cid:63)

i+1(cid:107)2(cid:3)
i (cid:107)2 − 2γ (cid:10)∇fπi(xk

i ) − ∇fπi(x(cid:63)), xk

i − x(cid:63)
i

(cid:11) + γ2(cid:107)∇fπi(xk

i ) − ∇fπi(x(cid:63))(cid:107)2(cid:3) .
(3.6)

Once we have this recursion, it is useful to notice that the scalar product can be decom-
posed as

(cid:10)∇fπi(xk

i ) − ∇fπi(x(cid:63)), xk

i − x(cid:63)
i

(cid:11) = [fπi(x(cid:63)

i ), x(cid:63)

i − xk
i

i ) − fπi(xk

i ) − (cid:10)∇fπi(xk
i ) − fπi(x(cid:63)) − (cid:10)∇fπi(x(cid:63)), xk
i ) − fπi(x(cid:63)) − (cid:104)∇fπi(x(cid:63)), x(cid:63)
(xk
i , xk

i , x(cid:63)) − Dfπi

i ) + Dfπi

(cid:11)]
i − x(cid:63)(cid:11)]
i − x(cid:63)(cid:105)]
i , x(cid:63)).
(x(cid:63)

+ [fπi(xk
− [fπi(x(cid:63)
(x(cid:63)

= Dfπi

(3.7)

This decomposition is, in fact, very standard and is a special case of the so-called three-
point identity [46]. So, it should not be surprising that we use it.
The rest of the proof relies on obtaining appropriate bounds for the terms in the recursion.
Firstly, we bound each of the three Bregman divergence terms appearing in (3.7). By µ-
strong convexity of fi, the ﬁrst term in (3.7) satisﬁes

(cid:107)xk

i − x(cid:63)

i (cid:107)2

(3.2)
≤ Dfπi

µ
2

(x(cid:63)

i , xk

i ),

which we will use to obtain contraction. The second term in (3.7) can be bounded via

64

1
2L

(cid:107)∇fπi(xk

i ) − ∇fπi(x(cid:63))(cid:107)2

(3.3)
≤ Dfπi

(xk

i , x(cid:63)),

which gets absorbed in the last term in the expansion of (cid:107)xk
i+1 − x(cid:63)
of the third divergence term in (3.7) is trivially bounded as follows:

i+1(cid:107)2. The expectation

E (cid:2)Dfπi

(x(cid:63)

i , x(cid:63))(cid:3) ≤ max

i=1,...,n−1

(cid:2)E (cid:2)Dfπi

(x(cid:63)

i , x(cid:63))(cid:3)(cid:3) = γσ2

Shuﬄe.

Plugging these three bounds back into (3.7), and the resulting inequality into (3.6), we
obtain

E (cid:2)(cid:107)xk

i+1 − x(cid:63)

i+1(cid:107)2(cid:3) ≤ E (cid:2)(1 − γµ)(cid:107)xk
≤ (1 − γµ)E (cid:2)(cid:107)xk

i − x(cid:63)
i − x(cid:63)

i (cid:107)2 − 2γ(1 − γL)Dfπi
i (cid:107)2(cid:3) + 2γ2σ2

Shuﬄe.

(xk

i , x(cid:63))(cid:3) + 2γ2σ2

Shuﬄe
(3.8)

The rest of the proof is just solving this recursion, and is relegated to Section C.4.2 in
(cid:4)
the appendix.

We show (Corollary C.4.1 in the appendix) that by carefully controlling the stepsize,

the ﬁnal iterate of RR after T epochs satisﬁes

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:18)

(cid:18)

exp

−

(cid:19)

µnT
L

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)

κσ2
(cid:63)
µ2nT 2

(cid:19)

,

(3.9)

where the ˜O(·) notation suppresses absolute constants and polylogarithmic factors. Note
that Theorem 3.4.4 covers both RR and SO, and for SO, [231] give almost the same lower
bound. Stated in terms of the squared distance from the optimum, their lower bound is

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

(cid:18)

(cid:26)

= Ω

min

1,

σ2
(cid:63)
µ2nT 2

(cid:27)(cid:19)

,

√

√

√

where we note that in their problem κ = 1. This translates to sample complexity
ε)) for ε ≤ 12. Specializing κ = 1 in Equation (3.9) gives the sample
O (
nσ(cid:63)/(µ
complexity of ˜O (1 +
ε)), matching the optimal rate up to an extra iteration.
More recently, Rajput et al. [213] also proved a similar lower bound for RR. We emphasize
that Theorem 3.4.4 is not only tight, but it is also the ﬁrst convergence bound that applies
to SO. Moreover, it also immediately works if one permutes once every few epochs, which
interpolates between RR and SO mentioned by Rajput et al. [213].

nσ(cid:63)/(µ

√

Comparison with SGD To understand when RR is better than SGD, let us borrow
a convergence bound for the latter. Several works have shown (e.g., see [181, 253]) that

2In their problem, the initialization point x0 satisﬁes (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2
(cid:13)

≤ 1 and hence asking for accuracy

ε > 1 does not make sense.

for any γ ≤ 1

2L the iterates of SGD satisfy

65

E

(cid:104)(cid:13)
(cid:13)xnT

SGD − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γσ2
(cid:63)
µ

.

Shuﬄe or σ2

Thus, the question as to which method will be faster boils down to which variance is
smaller: σ2
(cid:63). According to Proposition 3.4.3, it depends on both n and the
stepsize. Once the stepsize is suﬃciently small, σ2
(cid:63), but
this might not be true in general. Similarly, if we partition n functions into n/τ groups,
Shuﬄe as O (1/τ 2), so
i.e., use mini-batches of size τ , then σ2
RR can become faster even without decreasing the stepsize. We illustrate this later with
numerical experiments.

Shuﬄe becomes smaller than σ2

(cid:63) decreases as O (1/τ ) and σ2

While Theorem 3.4.4 requires each fi to be strongly convex, we can also obtain results
in the case where the individual strong convexity assumption is replaced by convexity.
However, in such a case, we need to use a smaller stepsize, as the next theorem shows.

Theorem 3.4.5. Suppose that each fi is convex, f is µ-strongly convex, and Assump-
tion 3.3.1 holds. Then provided the stepsize satisﬁes γ ≤ 1√
the ﬁnal iterate generated
by Algorithms 2 or 3 satisﬁes

2Ln

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤

(cid:16)

1 −

γµn
2

(cid:17)T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ2κnσ2
(cid:63).
(cid:13)

√

(cid:16)

(cid:17)

κn +

It is not diﬃcult to show that by properly choosing the stepsize γ, the guarantee given
by Theorem 3.4.5 translates to a sample complexity of ˜O
, which matches
the dependence on the accuracy ε in Theorem 3.4.4 but with κ(n−1) additional iterations
(cid:17)
in the beginning. For κ = 1, this translates to a sample complexity of ˜O
In concurrent work, Ahn
which is worse than the lower bound of [231] when ε is large.
et al. [2] obtain in the same setting a complexity of ˜O
(for a constant
α > 2), which requires that each fi is Lipschitz and matches the lower bound only when
the accuracy ε is large enough that 1/ε1/α ≤ 1. Obtaining an optimal convergence
guarantee for all accuracies ε in the setting of Theorem 3.4.5 remains open.

1/ε1/α +

κnσ(cid:63)
√
ε
µ

nσ(cid:63)
√
ε

n +

nG
√
ε

(cid:17)

(cid:16)

(cid:16)

√

√

µ

µ

3.4.2 Non-strongly convex objectives

We also make a step towards better bounds for RR/SO without any strong convexity at
all and provide the following convergence statement.

Theorem 3.4.6. Let functions f1, f2, . . . , fn be convex. Suppose that Assumption 3.3.1
holds. Then for Algorithm 2 or Algorithm 3 run with a stepsize γ ≤ 1√
, the average

2Ln

iterate ˆxT

def= 1
T

(cid:80)T

j=1 xj satisﬁes

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
2γnT

+

γ2Lnσ2
(cid:63)
4

.

66

Unfortunately, the theorem above relies on small stepsizes, but we still deem it as a
valuable contribution, since it is based on a novel analysis. Indeed, the prior works showed
that RR approximates a full gradient step, but we show that it is even closer to the implicit
gradient step, see the appendix.

To translate the recursion in Theorem 3.4.6 to a complexity, one can choose a small

stepsize and obtain (Corollary C.4.5 in the appendix) the following bound for RR/SO:

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) = O

(cid:32)

L(cid:107)x0 − x(cid:63)(cid:107)2
T

+

L1/3 (cid:107)x0 − x(cid:63)(cid:107)4/3 σ2/3
n1/3T 2/3

(cid:63)

(cid:33)

.

Stich [253] gives a convergence upper bound of O

(cid:18) L(cid:107)x0−x(cid:63)(cid:107)2

nT

(cid:19)

+

σ(cid:63)(cid:107)x0−x(cid:63)(cid:107)
√
nT

for SGD.

Comparing upper bounds, we see that RR/SO beats SGD when the number of epochs

satisﬁes T ≥

L2(cid:107)x0−x(cid:63)(cid:107)2
σ2
(cid:63)

n

. To the best of our knowledge, there are no strict lower bounds

in this setting. Safran and Shamir [231] suggest a lower bound of Ω
by
setting µ to be small in their lower bound for µ-strongly convex functions, however this
bound may be too optimistic.

nT 3 + σ(cid:63)

nT

(cid:16) σ(cid:63)√

(cid:17)

3.4.3 Non-convex objectives

For non-convex objectives, we formulate the following assumption on the gradients vari-
ance.
Assumption 3.4.7. There exist nonnegative constants A, B ≥ 0 such that for any x ∈ Rd
we have,

(cid:107)∇fi(x) − ∇f (x)(cid:107)2 ≤ 2A (f (x) − f (cid:63)) + B2.

(3.10)

1
n

n
(cid:88)

i=1

Assumption 3.4.7 is quite general: if there exists some G > 0 such that (cid:107)∇fi(x)(cid:107) ≤ G
for all x ∈ Rd and i ∈ {1, 2, . . . , n}, then Assumption 3.4.7 is clearly satisﬁed by setting
A = 0 and B = G. Assumption 3.4.7 also generalizes the uniformly bounded variance
assumption commonly invoked in work on non-convex SGD, which is equivalent to (3.10)
with A = 0. Assumption 3.4.7 is a special case of the Expected Smoothness assumption of
[117], and it holds whenever each fi is smooth and lower-bounded, as the next proposition
shows.

Proposition 3.4.8. [117, special case of Proposition 3] Suppose that f1, f2, . . . , fn are
lower bounded by f (cid:63)
n respectively and that Assumption 3.3.1 holds. Then there
exist constants A, B ≥ 0 such that Assumption 3.4.7 holds.

2 , . . . , f (cid:63)

1 , f (cid:63)

We now give our main convergence theorem for RR without assuming convexity.

Theorem 3.4.9. Suppose that Assumptions 3.3.1 and 3.4.7 hold. Then for Algorithm 2
(cid:16) 1
run for T epochs with a stepsize γ ≤ min

we have

(cid:17)

2Ln,

1
(AL2n2T )1/3

min
t=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤

12 (f (x0) − f (cid:63))
γnT

+ 2γ2L2nB2.

67

Figure 3.1: Top: ‘real-sim’ dataset (N = 72, 309; d = 20, 958), middle row: ‘w8a’ dataset
(N = 49, 749; d = 300), bottom: ‘RCV1’ dataset (N = 804, 414; d = 47, 236). Left:
convergence of f (xk
i − x(cid:63)(cid:107)2, right: convergence of
SO with diﬀerent permutations .

i ), middle column: convergence of (cid:107)xk

If, in addition, A = 0 and f satisﬁes the Polyak-(cid:32)Lojasiewicz inequality with µ > 0, i.e.,
(cid:107)∇f (x)(cid:107)2 ≥ 2µ(f (x) − f (cid:63)) for any x ∈ Rd, then

E (cid:2)f (xT ) − f (cid:63)(cid:3) ≤

(cid:16)

1 −

(cid:17)T

γµn
2

(f (x0) − f (cid:63)) + γ2κLnB2.

Comparison with SGD. From Theorem 3.4.9, one can recover the complexity that
we provide in Table 3.1, see Corollary C.5.5 in the appendix. Let’s ignore some constants
not related to our assumptions and specialize to uniformly bounded variance. Then, the
√
n
sample complexity of RR, KRR ≥ L
ε2 (
ε ), becomes better than that of SGD,
KSGD ≥ L
nε ≤ σ.

n + σ

√

√

ε2 ), whenever

ε2 (1 + σ2

0255075100125150Data passes10−610−510−410−310−210−1100f(x)−f*SGDIGShuffle-onceRR0255075100125150Data passes10−210−1100101102103104‖x−x*‖2SGDIGShuffle-onceRR01234Data passes10−3f(x)−f*AverageWorst sampled shuffleBest sampled shuffle050100150200250Data passes10−810−610−410−2100f(x)−f*SGDIGShuffle-onceRR050100150200250Data passes10−510−310−1101‖x−x*‖2SGDIGShuffle-onceRR01234Data passes10−510−4f(x)−f*AverageWorst sampled shuffleBest sampled shuffle051015202530Data passes10−510−410−310−210−1f(x)−f*SGDIGShuffle-onceRR051015202530Data passes100101102103104‖x−x*‖2SGDIGShuffle-onceRR01234Data passes10−5f(x)−f*AverageWorst sampled shuffleBest sampled shuffle3.5 Experiments

68

We run our experiments on the (cid:96)2-regularized logistic regression problem given by

1
N

N
(cid:88)

i=1

(cid:0)−(cid:0)bi log (cid:0)h(a(cid:62)

i x)(cid:1) + (1 − bi) log (cid:0)1 − h(a(cid:62)

i x)(cid:1)(cid:1)(cid:1) +

λ
2

(cid:107)x(cid:107)2,

where (ai, bi) ∈ Rd × {0, 1}, i = 1, . . . , N are the data samples and h : t → 1/(1 + e−t)
is the sigmoid function. For better parallelism, we use mini-batches of size 512 for all
√
methods and datasets. We set λ = L/
N and use stepsizes decreasing as O(1/t).
See the appendix for more details on the parameters used, implementation details, and
reproducibility.

Reproducibility. Our code is provided at https://github.com/konstmish/random reshuﬄing.

All used datasets are publicly available and all additional implementation details are pro-
vided in the appendix.

Observations. One notable property of all shuﬄing methods is that they converge
with oscillations, as can be seen in Figure 3.1. There is nothing surprising about this as the
proof of our Theorem 3.4.4 shows that the intermediate iterates converge to x(cid:63)
i instead of
x(cid:63). It is, however, surprising how striking the diﬀerence between the intermediate iterates
within one epoch can be.

Next, one can see that SO and RR converge almost the same way, which is in line with
Theorem 3.4.4. On the other hand, the contrast with IG is dramatic, suggesting existence
of bad permutations. The probability of getting such a permutation seems negligible; see
the right plot in Figure 3.2.

Finally, we remark that the ﬁrst two plots in Figure 3.2 demonstrate the importance
of the new variance introduced in Deﬁnition 3.4.2. The upper and lower bounds from
Proposition 3.4.3 are depicted in these two plots and one can observe that the lower
bound is often closer to the actual value of σ2
Shuﬄe than the upper bound. And the fact
that σ2
(cid:63) explains why RR often outperforms
SGD starting from early iterations.

Shuﬄe very quickly becomes smaller than σ2

Figure 3.2: Estimated variance at the optimum, σ2
the values of variance for diﬀerent mini-batch sizes with γ = 1
ﬁxed mini-batch size 64 for diﬀerent γ, starting with γ = 1
Right: the empirical distribution of σ2
and mini-batch size 64.

(cid:63), for the ‘w8a’ dataset. Left:
L . Middle: variance with
L and ending with γ = 10−4
L .
Shuﬄe for 500, 000 sampled permutations with γ = 1
L

Shuﬄe and σ2

100101102103Batch size10−610−410−2100102Variance at x⋆σ2*γLn4σ2*, γ=1Lγμn8σ2*, γ=1Lσ2Shuffle, γ=1L0200040006000800010000Stepsize ratio 1γL10−710−510−310−1Variance at x⋆σ2*γLn4σ2*γμn8σ2*σ2Shuffle10−410−310−210−1Variance at x⋆100101102103104FrequencyNo shufflingShuffle69

Chapter 4

Going Beyond Local SGD in Federated Learning

4.1 Introduction

Modern theory and practice of training supervised machine learning models is based on the
paradigm of regularized empirical risk minimization (ERM) [237]. While the ultimate goal
of supervised learning is to train models that generalize well to unseen data, in practice
only a ﬁnite dataset is available during training. Settling for a model merely minimizing
the average loss on this training set—the empirical risk—is insuﬃcient, as this often
leads to over-ﬁtting and poor generalization performance in practice. Due to this reason,
empirical risk is virtually always amended with a suitably chosen regularizer whose role
is to encode prior knowledge about the learning task at hand, thus biasing the training
algorithm towards better performing models.

The regularization framework is quite general and perhaps surprisingly it also allows
us to consider methods for federated learning (FL)—a paradigm in which we aim at
training model for a number of clients that do not want to reveal their data [124, 162,
111]. The training in FL usually happens on devices with only a small number of model
updates being shared with a global host. To this end, Federated Averaging algorithm
has emerged that performs Local SGD updates on the clients’ devices and periodically
aggregates their average. Its analysis usually requires special techniques and deliberately
constructed sequences hindering the research in this direction. We shall see, however, that
the convergence of our FedRR follows from merely applying our algorithm for regularized
problems to a carefully chosen reformulation.

Formally, regularized ERM problems are optimization problems of the form

(cid:104)
P (x) def=

min
x∈Rd

1
n

n
(cid:88)

i=1

fi(x) + ψ(x)

(cid:105)
,

(4.1)

where fi : Rd → R is the loss of model parameterized by vector x ∈ Rd on the i-th training
data point, and ψ : Rd → R ∪ {+∞} is a regularizer. Let [n] def= {1, 2, . . . , n}. We shall
make the following assumption throughout the paper without explicitly mentioning it:

Assumption 4.1.1. The functions fi are Li-smooth and convex, and the regularizer ψ
is proper, closed and convex. Let Lmax

def= maxi∈[n] Li.

In some results we will additionally assume that either the individual functions fi, or
their average f def= 1
i fi, or the regularizer ψ are µ-strongly convex. Whenever we need
n
such additional assumptions, we will make this explicitly clear. While all these concepts
are standard, we review them brieﬂy in Section 1.10.

(cid:80)

70

Proximal SGD. When the number n of training data points is huge, as is increasingly
common in practice, the most eﬃcient algorithms for solving (4.1) are stochastic ﬁrst-
order methods, such as Stochastic Gradient Descent (SGD) [28], in one or another of its
many variants proposed in the last decade [244, 203]. These method almost invariably
rely on alternating stochastic gradient steps with the evaluation of the proximal operator

proxγψ(x) def= arg min
z∈Rd

(cid:26)

γψ(z) +

(cid:107)z − x(cid:107)2

(cid:27)

.

1
2

The simplest of these has the form

xk+1
SGD = proxγkψ(xk

SGD − γk∇fik(xk

SGD)),

(4.2)

where ik is an index from {1, 2, . . . , n} chosen uniformly at random, and γk > 0 is
a properly chosen stepsize. Our understanding of (4.2) is quite mature; see [81] for a
general treatment which considers methods of this form in conjunction with more advanced
stochastic gradient estimators in place of ∇fik.

Applications such as training sparse linear models [263], nonnegative matrix factor-
ization [133], image deblurring [226, 33], and training with group selection [288] all rely
on the use of hand-crafted regularizes. For most of them, the proximal operator can
be evaluated eﬃciently, and SGD is near or at the top of the list of eﬃcient training
algorithms.

Random Reshuﬄing. A particularly successful variant of SGD is based on the idea
of random shuﬄing (permutation) of the training data followed by n iterations of the
form (4.2), with the index ik following the pre-selected permutation [30]. This process
is repeated several times, each time using a new freshly sampled random permutation of
the data, and the resulting method is known under the name Random Reshuﬄing (RR).1
When the same permutation is used throughout, the technique is known under the name
Shuﬄe-Once (SO).

One of the main advantages of this approach is rooted in its intrinsic ability to avoid
cache misses when reading the data from memory, which enables a signiﬁcantly faster
implementation. Furthermore, RR is often observed to converge in fewer iterations than
SGD in practice. This can intuitively be ascribed to the fact that while due to its sampling-
with-replacement approach SGD can miss to learn from some data points in any given
epoch, RR will necessarily learn from each data point in each epoch.

4.2 Related work

Understanding the random reshuﬄing trick, and why it works, has been a non-trivial
open problem for a long time [29, 217, 91, 97]. Until recent development which lead to
a signiﬁcant simpliﬁcation of the convergence analysis technique and proofs (see Chap-
ter 3), prior state of the art relied on long and elaborate proofs requiring sophisticated
arguments and tools, such as analysis via the Wasserstein distance [178], and relied on

1While we will comment on this in more detail later, RR is not known to converge in the proximal

setting, i.e., if ψ (cid:54)= 0. Moreover, it is not even clear if this is the right proximal extension of RR.

Algorithm 4 Proximal Random Reshuﬄing (ProxRR) and Shuﬄe-Once (ProxSO)

71

Require: Stepsizes γk > 0, initial vector x0 ∈ Rd, number of epochs T
1: Sample a permutation π = (π0, π1, . . . , πn−1) of [n] (Do step 1 only for ProxSO)
2: for epochs k = 0, 1, . . . , T − 1 do
3:

Sample a permut. π = (π0, π1, . . . , πn−1) of [n] (Do step 3 only for ProxRR)
0 = xk
xk
for i = 0, 1, . . . , n − 1 do
i − γk∇fπi(xk
i )

4:
5:
6:
7:
8:
9: end for

i+1 = xk
xk
end for
xk+1 = proxγknψ(xk
n)

a signiﬁcant number of strong assumptions about the objective [242, 97]. In alternative
recent development, Ahn et al. [1] also develop new tools for analyzing the convergence of
Random Reshuﬄing, in particular using decreasing stepsizes and for objectives satisfying
the Polyak-(cid:32)Lojasiewicz condition, a generalization of strong convexity [206, 150].

The diﬃculty of analyzing RR has been the main obstacle in the development of even
some of the most seemingly benign extensions of the method. Indeed, while all these are
well understood in combination with its much simpler-to-analyze cousin SGD, to the best
of our knowledge, there exists no theoretical analysis of proximal, parallel, and importance
sampling variants of RR with both constant and decreasing stepsizes, and in most cases
it is not even clear how should such methods be constructed. Empowered by and building
on the advances Chapter 3, in this chapter we address all these challenges.

4.3 Contributions

In this section we outline the key contributions of our work, and also oﬀer a few intuitive
explanations motivating some of the development.

From RR to ProxRR. Despite rich literature on Proximal SGD [81], it is not obvi-
ous how one should extend RR to solve problem (4.1) when a nonzero regularizer ψ is
present.
Indeed, the standard practice for SGD is to apply the proximal operator after
each stochastic step [69], i.e., in analogy with (4.2). On the other hand, RR is motivated
by the fact that a data pass approximates the full gradient step. If we apply the proximal
operator after each iteration of RR, we would no longer approximate the full gradient after
an epoch, as illustrated by the next example.
Example 1. Let n = 2, ψ(x) = 1
2(cid:107)x(cid:107)2, f1(x) = (cid:104)c1, x(cid:105), f2(x) = (cid:104)c2, x(cid:105) with some c1, c2 ∈
Rd, c1 (cid:54)= c2. Let x0 ∈ Rd, γ > 0 and deﬁne x1 = x0 − γ∇f1(x0), x2 = x1 − γ∇f2(x1).
Then, we have prox2γψ(x2) = prox2γψ(x0 − 2γ∇f (x0)). However, if ˜x1 = proxγψ(x0 −
γ∇f1(x0)) and ˜x2 = proxγψ(x1 − γ∇f2(˜x1)), then ˜x2 (cid:54)= prox2γψ(x0 − 2γ∇f (x0)).

Motivated by this observation, we propose ProxRR (Algorithm 4), in which the proxi-
mal operator is applied at the end of each epoch of RR, i.e., after each pass through all
randomly reshuﬄed data.

72

A notable property of Algorithm 4 is that only a single proximal operator evaluation
is needed during each data pass. This is in sharp contrast with the way Proximal SGD
works, and oﬀers signiﬁcant advantages in regimes where the evaluation of the proximal
mapping is expensive (e.g., comparable to the evaluation of n gradients ∇f1, . . . , ∇fn).
We establish several convergence results for ProxRR, of which we highlight two here.
Both oﬀer a linear convergence rate with a ﬁxed stepsize to a neighborhood of the so-
lution. Firstly, in the case when each fi is µ-strongly convex, we prove the rate (see
Theorem 4.5.1)

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γ2σ2
µ

rad

,

where γk = γ ≤ 1
cise deﬁnition, see (4.4)).
(cid:107)∇f (x(cid:63))(cid:107)2, n, Lmax and the more common quantity σ2
(cid:63)

rad is a shuﬄing radius constant (for pre-
In Theorem 4.4.3 we bound the shuﬄing radius in terms of
def= 1
i=1 (cid:107)∇fi(x(cid:63)) − ∇f (x(cid:63))(cid:107)2.
n

is the stepsize, and σ2

(cid:80)n

Lmax

Secondly, if ψ is µ-strongly convex, we prove the rate

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

rad

γ2σ2
µ

,

where γk = γ ≤ 1

Lmax

is the stepsize (see Theorem 4.6.1).

Both mentioned rates show exponential (linear in logarithmic scale) convergence to a
neighborhood whose size is proportional to γ2σ2
rad. Since we can choose γ to be arbitrarily
small or periodically decrease it, this implies that the iterates converge to x(cid:63) in the limit.
Moreover, we show in Section 4.5 that when γ = O( 1
T 2 ), which is
superior to the O( 1
T ) error of SGD.

T ) the error is O( 1

Decreasing stepsizes. The convergence of RR is not always exact and depends on
the parameters of the objective. Similarly, if the shuﬄing radius σ2
rad is positive, and we
wish to ﬁnd an ε-approximate solution, the optimal choice of a ﬁxed stepsize for ProxRR
will depend on ε. This deﬁciency can be ﬁxed by using decreasing stepsizes in both
vanilla RR [1] and in SGD [253]. We adopt the same technique to our setting. However,
we depart from [1] by only adjusting the stepsize once per epoch rather than at every
iteration, similarly to the concurrent work of [266] on RR with momentum. For details,
see Section 4.7.

Importance sampling for ProxRR. While importance sampling is a well established
technique for speeding up the convergence of SGD [295, 117], no importance sampling
variant of RR has been proposed nor analyzed. This is not surprising since the key property
of importance sampling in SGD—unbiasedness—does not hold for RR. Our approach to
equip ProxRR with importance sampling is via a reformulation of problem (4.1) into a
In particular, for each i ∈ [n] we
similar problem with a larger number of summands.
include ni copies of the function 1
i ni functions
ni
constructed this way. The value of ni depends on the “importance” of fi, described below.
We then apply ProxRR to this reformulation.

fi, and then take average of all N = (cid:80)

If fi is Li-smooth for all i ∈ [n] and we let ¯L def= 1

¯L (cid:101). It
is easy to show that N ≤ 2n, and hence our reformulation leads to at most a doubling of

i Li, then we choose ni = (cid:100) Li

n

(cid:80)

73

the number of functions forming the ﬁnite sum. However, the overall complexity of ProxRR
applied to this reformulation will depend on ¯L instead of maxi Li (see Theorem 4.7.2),
which can lead to a signiﬁcant improvement. For details of the construction and our
complexity results, see Section 4.7.

Application to federated learning. In Section 4.8 we describe an application of our

results to federated learning [124, 162, 111].

Results for SO. All of our results apply to the Shuﬄe-Once algorithm as well. For
simplicity, we center the discussion around RR, whose current theoretical guarantees in
the non-convex case are better than that of SO. Nevertheless, the other results are the
same for both methods, and ProxRR is identical to ProxSO in terms of our theory too. A
study of the empirical diﬀerences between RR and SO can be found in Chapter 3.

4.4 Preliminaries

In our analysis, we build upon the notions of limit points and shuﬄing variance introduced
in Chapter 3 for vanilla (i.e., non-proximal) RR. Given a stepsize γ > 0 (held constant
during each epoch) and a permutation π of {1, 2, . . . , n}, the inner loop iterates of RR/SO
converge to a neighborhood of intermediate limit points x(cid:63)

1, x(cid:63)

2, . . . , x(cid:63)

n deﬁned by

x(cid:63)
i

def= x(cid:63) − γ

i−1
(cid:88)

j=0

∇fπj (x(cid:63)),

i = 1, . . . , n.

(4.3)

The intuition behind this deﬁnition is fairly simple: if we performed i steps starting at x(cid:63),
we would end up close to x(cid:63)

i . To quantify the closeness, we deﬁne the shuﬄing radius.

Deﬁnition 4.4.1 (Shuﬄing radius). Given a stepsize γ > 0 and a random permutation π
of {1, 2, . . . , n} used in Algorithm 4, deﬁne x(cid:63)
i (γ, π) as in (4.3). Then, the shuﬄing
radius is deﬁned by

i = x(cid:63)

rad(γ) def= max
σ2

i=0,...,n−1

(cid:20) 1
γ2

Eπ

(cid:2)Dfπi

(cid:21)
i , x(cid:63))(cid:3)

(x(cid:63)

,

(4.4)

where the expectation is taken with respect to the randomness in the permutation π. If
there are multiple stepsizes γ1, γ2, . . . used in Algorithm 4, we take the maximum of all
of them as the shuﬄing radius, i.e.,

σ2
rad

def= max
k≥1

σ2
rad(γk).

The shuﬄing radius is related by a multiplicative factor in the stepsize to the shuﬄing
variance introduced in the previous Chapter. When the stepsize is held ﬁxed, the diﬀerence
between the two notions is minimal but when the stepsize is decreasing, the shuﬄing radius
is easier to work with, since it can be upper bounded by problem constants independent
of the stepsizes. To prove this upper bound, we rely on a lemma from Chapter 3 that
bounds the variance when sampling without replacement.

74

(cid:80)n

i=1 Xi be their mean, and let σ2 = 1

Lemma 4.4.2 (Lemma 1 in Chapter 3). Let X1, . . . , Xn ∈ Rd be ﬁxed vectors, let
(cid:13)Xi − ¯X(cid:13)
(cid:13)
2 be their variance.
¯X = 1
(cid:13)
n
Fix any i ∈ [n] and let Xπ0, . . . , Xπi−1 be sampled uniformly without replacement from
{X1, . . . , Xn} and ¯Xπ = 1
j=0 Xπj be their average. Then, the sample average and
variance are given by

(cid:80)i−1

(cid:80)n

i=1

n

i

E (cid:2) ¯Xπ

(cid:3) = ¯X,

E

(cid:104)(cid:13)
(cid:13) ¯Xπ − ¯X(cid:13)
(cid:13)

2(cid:105)

=

n − i
i(n − 1)

σ2.

(4.5)

Armed with Lemma 4.4.2, we can upper bound the shuﬄing radius using the smooth-
(cid:63) of the gradient vectors

ness constant Lmax, size of the vector ∇f (x(cid:63)) and the variance σ2
∇f1(x(cid:63)), ∇f2(x(cid:63)), . . . , ∇fn(x(cid:63)).

Theorem 4.4.3. For any stepsize γ > 0 and any random permutation π of {1, 2, . . . , n}
we have

σ2
rad ≤

(cid:16)

Lmax
2

n

n(cid:107)∇f (x(cid:63))(cid:107)2 +

(cid:17)

,

σ2
(cid:63)

1
2

where x(cid:63) is a solution of Problem (4.1) and σ2

(cid:63) is the population variance at the optimum

σ2
(cid:63)

def=

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x(cid:63)) − ∇f (x(cid:63))(cid:107)2.

(4.6)

All proofs are relegated to the supplementary material. In order to better understand
the bound given by Theorem 4.4.3, note that if there is no proximal operator (i.e., ψ = 0)
then ∇f (x(cid:63)) = 0 and we get that σ2
. This recovers the existing upper bound
4
on the shuﬄing variance from Chapter 3 for vanilla RR. On the other hand, if ∇f (x(cid:63)) (cid:54)= 0
then we get an additive term of size proportional to the squared norm of ∇f (x(cid:63)).

rad ≤ Lmaxnσ2

(cid:63)

4.5 Convergence Theory for Strongly Convex Losses f1, . . . , fn

Our ﬁrst theorem establishes a convergence rate for Algorithm 4 applied with a constant
stepsize to Problem (4.1) when each objective fi is strongly convex. This assumption is
commonly satisﬁed in machine learning applications where each fi represents a regularized
loss on some data points, as in (cid:96)2 regularized linear regression and (cid:96)2 regularized logistic
regression.

Theorem 4.5.1. Let Assumption 4.1.1 be satisﬁed. Further, assume that each fi
µ-strongly convex. If Algorithm 4 is run with constant stepsize γk = γ ≤ 1
iterates generated by the algorithm satisfy

is
, then the

Lmax

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γ2σ2
µ

rad

.

We can convert the guarantee of Theorem 4.5.1 to a convergence rate by properly
tuning the stepsize and using the upper bound of Theorem 4.4.3 on the shuﬄing ra-
, then we obtain
dius. In particular, if we choose the stepsize as γ = min

(cid:111)

√

√

,

(cid:110) 1
Lmax

εµ
2σrad

75

Algorithm 5 Proximal SGD
Require: Stepsizes γk > 0, initial vector x0 ∈ Rd, number of steps K
1: for steps k = 0, 1, . . . , K − 1 do
2:
3:
4: end for

Sample ik uniformly at random from [n]
xk+1 = proxγkψ(xk − γk∇fik(xk))

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= O (ε) provided that the total number of iterations KRR = nT is at

least

(cid:18)

KRR ≥

κ +

√
√

κn
εµ

√
(

n (cid:107)∇f (x(cid:63))(cid:107) + σ(cid:63))

(cid:19)

log

(cid:18) 2(cid:107)x0 − x(cid:63)(cid:107)2
ε

(cid:19)

,

(4.7)

where κ def= Lmax/µ.

Comparison with vanilla RR. If there is no proximal operator, then (cid:107)∇f (x(cid:63))(cid:107) = 0
and we recover the earlier result from Chapter 3 on the convergence of RR without
proximal operator, which is optimal in ε up to logarithmic factors. On the other hand,
when the proximal operator is nonzero, we get an extra term in the complexity proportional
to (cid:107)∇f (x(cid:63))(cid:107): thus, even when all the functions are the same (i.e., σ(cid:63) = 0), we do not
recover the linear convergence of Proximal Gradient Descent [112, 16]. This can be easily
explained by the fact that Algorithm 4 performs n gradient steps per one proximal step.
Hence, even if f1 = · · · = fn, Algorithm 4 does not reduce to Proximal Gradient Descent.
We note that other algorithms for composite optimization which may not take a proximal
step at every iteration (for example, using stochastic projection steps) also suﬀer from
the same dependence [201].

Comparison with Proximal SGD. In order to compare (4.7) against the complexity
of Proximal SGD (Algorithm 5), we recall the following simple result on the convergence
of Proximal SGD. The result is standard [181, 84], with the exception that we present
it in a slightly generalized in that we also consider the case when ψ is strongly convex.
Our proof is a minor modiﬁcation of that in [84], and we oﬀer it in the appendix for
completeness.

Theorem 4.5.2 (Proximal SGD). Let Assumption 4.1.1 hold. Further, suppose that
either f def= 1
i=1 fi is µ-strongly convex or that ψ is µ-strongly convex. If Algorithm 5
n
is run with a constant stepsize γk = γ > 0 satisfying γ ≤ 1
, then the ﬁnal iterate
returned by the algorithm after K steps satisﬁes

(cid:80)n

2Lmax

E

(cid:104)(cid:13)
(cid:13)xK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γσ2
(cid:63)
µ

.

Furthermore, by choosing the stepsize γ as γ = min
(cid:104)(cid:13)
(cid:13)xK − x(cid:63)(cid:13)
(cid:13)

= O (ε) provided that the number of iterations is at least

2(cid:105)

(cid:110) 1

2Lmax

, εµ
4σ(cid:63)

(cid:111)

, we get that

E

(cid:18)

KSGD ≥

κ +

(cid:19)

σ2
(cid:63)
εµ2

log

(cid:18) 2(cid:107)x0 − x(cid:63)(cid:107)2
ε

(cid:19)

.

(4.8)

76

By comparing between the iteration complexities KSGD (given by (4.8)) and KRR
(given by (4.7)), we see that ProxRR converges faster than Proximal SGD whenever the
target accuracy ε is small enough to satisfy

ε ≤

(cid:18)

1
Lmaxnµ

σ4
(cid:63)
n(cid:107)∇f (x(cid:63))(cid:107)2 + σ2
(cid:63)

(cid:19)

.

Furthermore, the comparison is much better when we consider proximal iteration complex-
ity (number of proximal operator access), in which case the complexity of ProxRR (4.7) is
reduced by a factor of n (because we take one proximal step every n iterations) while the
proximal iteration complexity of Proximal SGD remains the same as (4.8).
In this case,
ProxRR is better whenever the accuracy ε satisﬁes

ε ≥

or,

ε ≤

n
Lmaxµ
n
Lmaxµ

(cid:3)

(cid:63)

(cid:2)n(cid:107)∇f (x(cid:63))(cid:107)2 + σ2
(cid:20)

σ4
(cid:63)
n(cid:107)∇f (x(cid:63))(cid:107)2 + σ2
(cid:63)

(cid:21)

.

Therefore we can see that if the target accuracy is large enough or small enough, and if
the cost of proximal operators dominates the computation, ProxRR is much quicker to
converge than Proximal SGD.

4.6 Convergence Theory for Strongly Convex Regularizer ψ

In Theorem 4.5.1, we assume that each fi is µ-strongly convex. This is motivated by
the common practice of using (cid:96)2 regularization in machine learning. However, applying
(cid:96)2 regularization in every step of Algorithm 4 can be expensive when the data are sparse
and the iterates xk
i which
can be much more expensive than computing sparse gradients ∇fi(xk
i ). Alternatively, we
may instead choose to put the (cid:96)2 regularization inside ψ and only ask that ψ be strongly
convex—this way, we can save a lot of time as we need to access each coordinate of the
dense iterates xk
i only once per epoch rather than every iteration. Theorem 4.6.1 gives a
convergence guarantee in this setting.

i are dense, because it requires accessing each coordinate of xk

Theorem 4.6.1. Let Assumption 4.1.1 be satisﬁed. Further, assume that ψ is µ-strongly
, where Lmax =
convex.
maxi Li, then the iterates generated by the algorithm satisfy

If Algorithm 4 is run with constant stepsize γk = γ ≤ 1

Lmax

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

rad

γ2σ2
µ

.

By making a speciﬁc choice for the stepsize used by Algorithm 4, we can obtain a

convergence guarantee using Theorem 4.6.1. Choosing the stepsize as

γ = min

(cid:26) 1

Lmax

√

εµ
σrad

,

(cid:27)

.

(4.9)

Then E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= O (ε) provided that the total number of iterations satisﬁes

77

(cid:18)

K ≥

κ +

σrad/µ
√
εµ

(cid:19)

+ n

log

(cid:18) 2(cid:107)x0 − x(cid:63)(cid:107)2
ε

(cid:19)

.

(4.10)

This can be converted to a bound similar to (4.7) by using Theorem 4.4.3, in which
(cid:1) term when only the
case the only diﬀerence between the two cases is an extra n log (cid:0) 1
ε term
regularizer ψ is µ-strongly convex. Since for small enough accuracies the 1/
dominates, this diﬀerence is minimal.

√

ε

4.7 Extensions

Before turning to applications, we discuss two extensions to the theory that signiﬁcantly
matter in practice: using decreasing stepsizes and applying importance resampling.

Decreasing stepsizes. Using the theoretical stepsize (4.9) requires knowing the
desired accuracy ε ahead of time as well as estimating σrad.
It also results in extra
polylogarithmic factors in the iteration complexity (4.10), a phenomenon observed and
ﬁxed by using decreasing stepsizes in both vanilla RR [1] and in SGD [253]. We show that
we can adopt the same technique to our setting. However, we depart from the stepsize
scheme of [1] by only varying the stepsize once per epoch rather than every iteration. This
is closer to the common practical heuristic of decreasing the stepsize once every epoch or
once every few epochs [257, 266]. The stepsize scheme we use is inspired by the schemes
of [253, 117]: in particular, we ﬁx T > 0, let k0 = (cid:100)T /2(cid:101), and choose the stepsizes γk > 0
by

(cid:40) 1

γk =

Lmax

7
µn(s+k−k0)

if T ≤ Lmax
if T > Lmax

2µn or k ≤ k0,
2µn and k > k0,

(4.11)

where s def= 7Lmax/(4µn). Hence, we ﬁx the stepsize used in the ﬁrst T /2 iterations
and then start decreasing it every epoch afterwards. Using this stepsize schedule, we can
obtain the following convergence guarantee when each fi is smooth and convex and the
regularizer ψ is µ-strongly convex.

Theorem 4.7.1. Suppose that each fi is Lmax-smooth and convex, and that the regular-
izer ψ is µ-strongly convex. Fix T > 0. Then choosing stepsizes γk according to (4.11)
we have that γk ≤ 1
for all t and the ﬁnal iterate generated by Algorithm 4 satisﬁes

Lmax

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

(cid:18)

(cid:18)

= O

exp

−

(cid:19)

nT
κ + 2n

(cid:107)x0 − x(cid:63)(cid:107)2 +

σ2
rad
µ3n2T 2

(cid:19)

,

where κ def= Lmax/µand O(·) hides absolute (non-problem-speciﬁc) constants.

This guarantee holds for any number of epochs T > 0. We believe a similar guarantee
can be obtained in the case each fi is strongly-convex and the regularizer ψ is just convex,
but we did not include it as it adds little to the overall message.

78

(cid:80)n

Importance resampling. Suppose that each fi is Li-smooth. Then the iteration
complexities of both SGD and RR depend on Lmax/µ, where Lmax is the maximum
smoothness constant among the smoothness constants L1, L2, . . . , Ln. The maximum
smoothness constant can be arbitrarily worse than the average smoothness constant L =
1
i=1 Li. This situation is in contrast to the complexity of gradient descent which
n
depends on the smoothness constant Lf of f = 1
i=1 fi, for which we have Lf ≤ L.
n
This is a problem commonly encountered with stochastic optimization methods and may
cause signiﬁcantly degraded performance in practical optimization tasks in comparison
with deterministic methods [260].

(cid:80)n

Importance sampling is a common technique to improve the convergence of SGD
fi with probability pi proportional to Li, where

(Algorithm 5): we sample function L
Li
L def= 1
i=1 Li. In that case, the SGD update is still unbiased since
n

(cid:80)n

Ei

(cid:20) L
Li

(cid:21)

fi

=

n
(cid:88)

i=1

pi

L
Li

fi = f.

Moreover, the smoothness of function L
fi is L for any i, so the guarantees would depend
Li
on L instead of maxi=1,...,n Li.
Importance sampling successfully improves the iteration
complexity of SGD to depend on L [181], and has been investigated in a wide variety of
settings [86, 81].

Importance sampling is a neat technique but it relies heavily on the fact that we use
unbiased sampling. How can we obtain a similar result if inside any permutation the
sampling is biased? The answer requires us to think again as to what happens when we
replace fi with L
fi. To make sure the problem remains the same, it is suﬃcient to have
Li
L
is not necessarily integer, we
Li
should use ni =

fi inside a permutation exactly Li
L
(cid:108) Li
(cid:109)
L

times. And since Li
L

and solve

min
x∈Rd

1
N

n
(cid:88)

i=1

(cid:16) 1
ni
(cid:124)

1
ni

fi(x) + · · · +

(cid:123)(cid:122)
ni times

(cid:17)

fi(x)

+ ψ(x),

(cid:125)

(4.12)

(cid:109)

(cid:108) L1
L

(cid:108) Ln
where N def= n1 + · · · + nn =
. Clearly, this problem is equivalent to the
L
original formulation in 4.1. At the same time, we have improved all smoothness constants
to L. It might seem that that the new problem has more functions, but it turns out that
the new number of functions satisﬁes N ≤ 2n, so any related costs, such as longer loops
or storing duplicates of the data, are negligible, as the next theorem shows.

+ · · · +

(cid:109)

Theorem 4.7.2. For every i, assume that each fi is convex and Li-smooth, and let ψ
be µ-strongly convex. Then, the number of functions N in (4.12) satisﬁes N ≤ 2n, and
Algorithm 4 applied to problem (4.12) has the same complexity as (4.10) but proportional
to L rather than Lmax.

4.8 Federated Learning

79

Let us consider now the problem of minimizing the average of N = (cid:80)M
that are stored on M devices, which have N1, . . . , NM samples correspondingly,

m=1 Nm functions

min
x∈Rd

1
N

M
(cid:88)

m=1

Fm(x) + R(x), Fm(x) =

Nm(cid:88)

j=1

fmj(x).

For example, fmj(x) can be the loss associated with a single sample (Xmj, ymj), where
pairs (Xmj, ymj) follow a distribution Dm that is speciﬁc to device m. An important
instance of such formulation is federated learning, where M devices train a shared model
by communicating periodically with a server. We normalize the objective in (4.8) by N
as this is the total number of functions after we expand each Fm into a sum. We denote
the solution of (4.8) by x(cid:63).

Extending the space. To rewrite the problem as an instance of (4.1), we are going to
consider a bigger product space, which is sometimes used in distributed optimization [25].
Let us deﬁne n def= max{N1, . . . , Nm} and introduce ψC, the consensus constraint,

ψC(x1, . . . , xM ) =

(cid:40)

x1 = · · · = xM

0,
+∞, otherwise

.

By introducing dummy variables x1, . . . , xM and adding the constraint x1 = · · · = xM ,
we arrive at the intermediate problem

min
x1,...,xM ∈Rp

1
N

M
(cid:88)

m=1

Fm(xm) + (R + ψC)(x1, . . . , xM ),

where R + ψC is deﬁned, with a slight abuse of notation, as

(R + ψC)(x1, . . . , xM ) =

(cid:40)

R(x1), x1 = · · · = xM
+∞,

otherwise.

Since we have replaced R with a more complicated regularizer R + ψC, we need to
understand how to compute the proximal operator of the latter. We show (Lemma D.6.1
in the supplementary) that the proximal operator of (R + ψC) is merely the projection
onto {(x1, . . . , xM ) | x1 = · · · = xM } followed by the proximal operator of R with a
smaller stepsize.

Reformulation. To have n functions in every Fm, we write Fm as a sum with extra

n − Nm zero functions, fmj(x) ≡ 0 for any j > Nm, so that

Fm(xm) =

n
(cid:88)

j=1

fmj(xm) =

Nm(cid:88)

j=1

fmj(xm) +

n
(cid:88)

0.

j=Nm+1

We can now stick the vectors together into x = (x1, . . . , xM ) ∈ RM ·d and multiply the
objective by N

n , which gives the following reformulation:

80

min
x∈RM ·d

1
n

n
(cid:88)

i=1

fi(x) + ψ(x),

(4.13)

where ψ(x) def= N

n (R + ψC) and

fi(x) = fi(x1, . . . , xM ) def=

M
(cid:88)

m=1

fmi(xm).

In other words, function fi(x) includes i-th data sample from each device and contains at
most one loss from every device, while Fm(x) combines all data losses on device m. Note
that the solution of (4.13) is x(cid:63) def= ((x(cid:63))(cid:62), . . . , (x(cid:63))(cid:62))(cid:62) and the gradient of the extended
function fi(x) is given by

∇fi(x) = (∇f1i(x1)(cid:62), · · · , ∇fM i(xM )(cid:62))(cid:62)

Therefore, a stochastic gradient step that uses ∇fi(x) corresponds to updating all local
models with the gradient of i-th data sample, without any communication.

Algorithm 4 for this speciﬁc problem can be written in terms of x1, . . . , xM , which
results in Algorithm 6. Note that since fmi(xi) depends only on xi, computing its gradient
does not require communication. Only once the local epochs are ﬁnished, the vectors are
averaged as the result of projecting onto the set {(x1, . . . , xM ) | x1 = · · · = xM }. The
full description of our FedRR is given in Algorithm 6.

Reformulation properties. To analyze FedRR, the only thing that we need to do
is understand the properties of the reformulation (4.13) and then apply Theorem 4.5.1
or Theorem 4.6.1. The following lemma gives us the smoothness and strong convexity
properties of (4.13).

Lemma 4.8.1. Let function fmi be Li-smooth and µ-strongly convex for every m. Then,
fi from reformulation (4.13) is Li-smooth and µ-strongly convex.

The previous lemma shows that the conditioning of the reformulation is κ = Lmax
just
as we would expect. Moreover, it implies that the requirement on the stepsize remains
exactly the same: γ ≤ 1
rad, which plays a
key role in the convergence bounds for ProxRR and ProxSO. Our next goal, thus, is to
obtain an upper bound on σ2
rad, which would allow us to have a complexity for FedRR and
FedSO. To ﬁnd it, let us deﬁne

. What remains unknown is the value of σ2

Lmax

µ

σ2
m,(cid:63)

def=

1
Nm

n
(cid:88)

j=1

(cid:13)
(cid:13)∇fmj(x(cid:63)) −

1
Nm

∇Fm(x(cid:63))(cid:13)
2,
(cid:13)

which is the variance of local gradients on device m. This quantity characterizes the
convergence rate of Local SGD [287], so we should expect it to appear in our bounds too.
The next lemma explains how to use it to upper bound σ2

rad.

Algorithm 6 Federated Random Reshuﬄing (FedRR) and Shuﬄe-Once (FedSO)

81

Require: Stepsize γ > 0, initial vector x0 = x0
1: For each m,
(Only FedSO)

sample permutation π0,m, π1,m, . . . , πNm−1,m of {1, 2, . . . , Nm}

0 ∈ Rd, number of epochs T

for m = 1, . . . , M locally in parallel do

2: for epochs k = 0, 1, . . . , T − 1 do
3:
4:
5:

permutation

xk,m
0 = xk
Sample
(Only FedRR)

π0,m, π1,m, . . . , πNm−1,m

of

{1, 2, . . . , Nm}

for i = 0, 1, . . . , Nm − 1 do

i − γ∇fπi,m(xk,m

i

)

6:
7:
8:
9:
10:
11:
12:
13: end for

xk,m
i+1 = xk,m
end for
n = xk,m
xk,m
Nm
end for
(cid:80)M
zk+1 = 1
M
xk+1 = proxγ N

n

m=1 xk,m
M R(zk+1)

Lemma 4.8.2. The shuﬄing radius σ2

rad of the reformulation (4.13) is upper bounded by

σ2
rad ≤ Lmax

M
(cid:88)

(cid:16)

m=1

(cid:107)∇Fm(x(cid:63))(cid:107)2 +

(cid:17)

.

σ2
m,(cid:63)

n
4

The lemma shows that the upper bound on σ2
m=1 σ2

m,(cid:63) as well as on the local gradient norms (cid:80)M

rad depends on the sum of local variances
m=1 (cid:107)∇Fm(x(cid:63))(cid:107)2. Both of these sums

(cid:80)M

appear in the existing literature on convergence of Local GD/SGD [115, 277, 287].

Equipped with the variance bound, we are ready to present formal convergence results.
For simplicity, we will consider heterogeneous and homogeneous cases separately and
assume that N1 = · · · = NM = n. To further illustrate generality of our results, we
will present the heterogeneous assuming strong convexity R and the homogeneous under
strong convexity of functions fmi.

Heterogeneous data. In the case when the data are heterogeneous, we provide the
ﬁrst local RR method. We can apply either Theorem 4.5.1 or Theorem 4.6.1, but for
brevity, we give only the corollary obtained from Theorem 4.6.1.

Theorem 4.8.3. Assume that functions fmi are convex and Li-smooth for each m and
i.
, then we have for the iterates produced by
Algorithm 6

If R is µ-strongly convex and γ ≤ 1

Lmax

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γ2Lmax
M µ

M
(cid:88)

(cid:16)
(cid:107)∇Fm(x(cid:63))(cid:107)2 +

m=1

N
4M

(cid:17)

.

σ2
m,(cid:63)

Homogeneous data. For simplicity, in the homogeneous (i.e., i.i.d.) data case we

82

Figure 4.1: Experimental results for problem (4.14). The ﬁrst two plots show with average
and conﬁdence intervals estimated on 20 random seeds and clearly demonstrate that one
can save a lot of proximal operator computations with our method. In the last plot, we
show convergence oscillations of ProxSO by sampling 20,000 permutations and choosing
the best and the worst performing among them.

provide guarantees without the proximal operator. Since then we have F1(x) = · · · =
j=1 (cid:107)∇fmj(x(cid:63))(cid:107)2. The
FM (x), for any m it holds ∇Fm(x(cid:63)) = 0, and thus σ2
full variance is then given by

m,(cid:63) = 1
n

(cid:80)n

M
(cid:88)

m=1

σ2
m,(cid:63) =

1
n

M
(cid:88)

n
(cid:88)

m=1

i=1

(cid:107)∇fmi(x(cid:63))(cid:107)2 =

N
n

(cid:63) = M σ2
σ2
(cid:63),

where σ2
(cid:63)

def= 1
N

(cid:80)n

i=1

(cid:80)M

m=1 (cid:107)∇fmi(x(cid:63))(cid:107)2 is the variance of the gradients over all data.

Theorem 4.8.4. Let R(x) ≡ 0 (no prox) and the data be i.i.d., that is ∇Fm(x(cid:63)) = 0
for any m, where x(cid:63) is the solution of (4.8). If each fmj is Lmax-smooth and µ-strongly
convex, then the iterates of Algorithm 6 satisfy

E (cid:2)(cid:107)xT − x(cid:63)(cid:107)2(cid:3) ≤ (1 − γµ)nT (cid:107)x0 − x(cid:63)(cid:107)2 +

γ2LmaxN σ2
(cid:63)
M µ

,

where σ2
(cid:63)

def= 1
N

(cid:80)n

i=1

(cid:80)M

m=1 (cid:107)∇fmi(x(cid:63))(cid:107)2.

The most important part of this result is that the last term in Theorem 4.8.4 has a
factor of M in the denominator, meaning that the convergence bound improves with the
number of devices involved.

4.9 Experiments2

We look at the logistic regression loss with the elastic net regularization,

1
N

N
(cid:88)

i=1

fi(x) + λ1(cid:107)x(cid:107)1 +

λ2
2

(cid:107)x(cid:107)2,

(4.14)

2Our code: https://github.com/konstmish/rr prox fed

020040060080010001200Data passes10-510-410-310-210-1P(x)¡P⋆SGDRR (iteration prox)RR (epoch prox)020000400006000080000Prox steps10-810-610-410-2100P(x)¡P⋆SGDRR (iteration prox)RR (epoch prox)0.00.51.01.52.0Data passes0.00050.0010.002P(x)¡P⋆AverageWorst shuffleBest shufflewhere each fi : Rd → R is deﬁned as

83

fi(x) def= −(cid:0)bi log (cid:0)h(a(cid:62)

i x)(cid:1) + (1 − bi) log (cid:0)1 − h(a(cid:62)

i x)(cid:1)(cid:1)

and where (ai, bi) ∈ Rd ×{0, 1}, i = 1, . . . , N are the data samples, h : t → 1/(1+e−t) is
the sigmoid function, and λ1, λ2 ≥ 0 are parameters. We set mini-batch sizes to 1 for all
methods and use theoretical stepsizes, without any tuning. We denote the version of RR
that performs proximal operator step after each iteration as ‘RR (iteration prox)’. We give
more details in the supplementary. From the experiments, we can see that all methods
behave more or less the same way. However, the algorithm that we propose needs only a
small fraction of proximal operator evaluations, which gives it a huge advantage whenever
the operator takes more time to compute than stochastic gradients.

84

Chapter 5

The First Adaptive Stepsize Rule for Gradient Descent that
Provably Works

5.1 Introduction

Since the early days of optimization it was evident that there is a need for algorithms
that are as independent from the user as possible. First-order methods have proven to be
versatile and eﬃcient in a wide range of applications, but one drawback has been present
all that time: the stepsize. Despite certain success stories, line search procedures and
adaptive online methods have not removed the need to manually tune the optimization
parameters. Even in smooth convex optimization, which is often believed to be much
simpler than the non-convex counterpart, robust rules for stepsize selection have been
elusive. The purpose of this chapter is to remedy this deﬁciency.

The problem formulation that we consider is the basic unconstrained optimization

problem

min
x∈Rd

f (x),

(5.1)

where f : Rd → R is a diﬀerentiable function. Throughout the chapter, we assume that
(5.1) has a solution and we denote its optimal value by f (cid:63).

The simplest and most known approach to this problem is the Gradient Descentmethod
(GD), whose origin can be traced back to Cauchy [38, 137]. Although it is probably the
oldest optimization method, it continues to play a central role in modern algorithmic
theory and applications. Its deﬁnition can be written in a mere one line,

xk+1 = xk − γ∇f (xk),

k ≥ 0,

(5.2)

where x0 ∈ Rd is arbitrary and γ > 0. Under assumptions that f is convex and L–smooth
(equivalently, ∇f is L-Lipschitz) that is

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

∀x, y,

(5.3)

one can show that GD with γ ∈ (0, 2
with γ = 1

L the convergence rate [66] is

L ) converges to an optimal solution [206]. Moreover,

f (xk) − f (cid:63) ≤

L(cid:107)x0 − x(cid:63)(cid:107)2
2(2k + 1)

,

(5.4)

where x(cid:63) is any solution of (5.1). Note that this bound is not improvable [66].

We identify four important challenges that limit the applications of Gradient Descent

even in the convex case:

85

1. GD is not general: many functions do not satisfy (5.3) globally.

2. GD is not a free lunch: one needs to guess γ, potentially trying many values before

a success.

3. GD is not robust: failing to provide γ < 2

L may lead to divergence.

4. GD is slow: even if L is ﬁnite, it might be arbitrarily larger than local smoothness.

5.2 Related Work

Certain ways to address some of the issues above already exist in the literature. They
include line search, adaptive Polyak’s stepsize, Mirror Descent, Dual Preconditioning, and
stepsize estimation for subgradient methods. We discuss them one by one below, in a
process reminiscent of cutting oﬀ Hydra’s limbs: if one issue is ﬁxed, two others take its
place.

The most practical and generic solution to the aforementioned issues is known as
line search (or backtracking). This direction of research started from the seminal works
of Goldstein [80] and Armijo [7] and continues to attract attention, see [17, 235] and
references therein. In general, at each iteration the line search executes another subroutine
with additional evaluations of ∇f and/or f until some condition is met. Obviously, this
makes each iteration more expensive.

At the same time, the famous Polyak’s stepsize [207] stands out as a very fast alter-
native to Gradient Descent. Furthermore, it does not depend on the global smoothness
constant and uses the current gradient to estimate the geometry. The formula might look
deceitfully simple, γk = f (xk)−f (cid:63)
(cid:107)∇f (xk)(cid:107)2 , but there is a catch: it is rarely possible to know f (cid:63).
This method, again, requires the user to guess f (cid:63). What is more, with γ it was ﬁne to
underestimate it by a factor of 10, but the guess for f (cid:63) must be tight, otherwise it has
to be reestimated later [100].

Seemingly no issue is present in the Barzilai–Borwein stepsize. Motivated by the

quasi-Newton schemes, Barzilai and Borwein [11] suggested using steps

γk =

(cid:104)xk − xk−1, ∇f (xk) − ∇f (xk−1)(cid:105)
(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)2

.

Alas, the convergence results regarding this choice of γk are very limited and the only
known case where it provably works is quadratic problems [216, 58].
In general it may
not work even for smooth strongly convex functions, see the counterexample in [35].

Other more interesting ways to deal with non-Lipschitzness of ∇f use the problem
structure. The ﬁrst method, proposed in [26] and further developed in [13], shows that
the Mirror Descent method [182], which is another extension of GD, can be used with a
ﬁxed stepsize, whenever f satisﬁes a certain generalization of (5.3). In addition, Maddison
et al. [154] proposed the Dual Preconditioning method—another reﬁned version of GD.
Similarly to the former technique, it also goes beyond the standard smoothness assumption
of f , but in a diﬀerent way. Unfortunately, these two simple and elegant approaches cannot

86

resolve all issues yet. First, not many functions fulﬁll respective generalized conditions.
And secondly, both methods still get us back to the problem of not knowing the allowed
range of stepsizes.

A whole branch of optimization considers adaptive extensions of GD that deal with
functions whose (sub)gradients are bounded. Probably the earliest work in that direction
was written by Shor [247]. He showed that the method

xk+1 = xk − γk

gk
(cid:107)gk(cid:107)

,

where gk ∈ ∂f (xk) is a subgradient, converges for properly chosen sequences (γk)k, see,
e.g., Section 3.2.3 in [186]. Moreover, γk requires no knowledge about the function
whatsoever.

Similar methods that work in online setting such as Adagrad [68, 163] received a lot of
attention in recent years and remain an active topic of research [275]. Methods similar to
Adagrad—Adam [120, 219], RMSprop [265] and Adadelta [290]—remain state-of-the-art
for training neural networks. The corresponding objective is usually neither smooth nor
convex, and the theory often assumes Lipschitzness of the function rather than of the
gradients. Therefore, this direction of research is mostly orthogonal to ours, although we
do compare with some of these methods in our neural networks experiment.

We also note that without momentum Adam and RMSprop reduce to signSGD [20],
which is known to be non-convergent for arbitrary stepsizes on a simple quadratic prob-
lem [114].

In a close relation to ours is the recent work [157], where there was proposed an
adaptive golden ratio algorithm for monotone variational inequalities. As it solves a more
general problem, it does not exploit the structure of (5.1) and, as most variational in-
equality methods, has a more conservative update. Although the method estimates the
smoothness, it still requires an upper bound on the stepsize as input.

Contribution. We propose a new version of GD that at no cost resolves all aforemen-
tioned issues. The idea is simple, and it is surprising that it has not been yet discovered.
In each iteration we choose γk as a certain approximation of the inverse local Lipschitz
constant. With such a choice, we prove that convexity and local smoothness of f are
suﬃcient for convergence of iterates with the complexity O(1/k) for f (xk) − f (cid:63) in the
worst case.

Discussion. Let us now brieﬂy discuss why we believe that proofs based on monotonicity
and global smoothness lead to slower methods.

Gradient Descent is by far not a recent method, so there have been obtained opti-
mal rates of convergence. However, we argue that adaptive methods require rethinking
optimality of the stepsizes. Take as an example a simple quadratic problem, f (x, y) =
2x2 + δ
1
2y2, where δ (cid:28) 1. Clearly, the smoothness constant of this problem is equal to
L = 1 and the strong convexity one is µ = δ.
If we run GD from an arbitrary point
(x0, y0) with the “optimal” stepsize γ = 1
L = 1, then one iteration of GD gives us
(x1, y1) = (0, (1 − δ)y0), and similarly (xk, yk) = (0, (1 − δ)ky0). Evidently for δ small

87

enough it will take a long time to converge to the solution (0, 0).
Instead GD would
converge in two iterations if it adjusts its step after the ﬁrst iteration to γ = 1
δ .

Nevertheless, all existing analyses of the Gradient Descent with L-smooth f use step-

sizes bounded by 2/L. Besides, functional analysis gives

f (xk+1) ≤ f (xk) − γ

(cid:16)

1 −

(cid:17)

γL
2

(cid:107)∇f (xk)(cid:107)2,

from which 1/L can be seen as the “optimal” stepsize. Alternatively, we can assume that
f is µ-strongly convex, and the analysis in norms gives

(cid:107)xk+1 − x(cid:63)(cid:107)2 ≤

(cid:16)

1 − 2

(cid:17)

γLµ
L + µ

(cid:107)xk − x(cid:63)(cid:107)2 − γ

(cid:16) 2

L + µ

(cid:17)

− γ

(cid:107)∇f (xk)(cid:107)2,

whence the “optimal” step is

2
L+µ.

Finally, line search procedures use some certain type of monotonicity, for instance
ensuring that f (xk+1) ≤ f (xk) − c(cid:107)∇f (xk)(cid:107)2 for some c > 0. We break with this
tradition and merely ask for convergence in the end.

5.3 Convergence Theory

5.3.1 Local smoothness of f

Recall that a mapping is locally Lipschitz if it is Lipschitz over any compact set of its
domain. A function f with (locally) Lipschitz gradient ∇f is called (locally) smooth. It
is natural to ask whether some interesting functions are smooth locally, but not globally.
It turns out there is no shortage of examples, most prominently among highly nonlinear
In R, they include x (cid:55)→ exp(x), log(x), tan(x), xp, for p > 2, etc. More
functions.
generally, they include any twice diﬀerentiable f , since ∇2f (x), as a continuous mapping,
is bounded over any bounded set C. In this case, we have that ∇f is Lipschitz on C, due
to the mean value inequality

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ max
z∈C

(cid:107)∇2f (z)(cid:107)(cid:107)x − y(cid:107),

∀x, y ∈ C.

Algorithm 7 that we propose is just a slight modiﬁcation of GD. The quick explana-
tion why local Lipschitzness of ∇f does not cause us any problems, unlike most other
methods, lies in the way we prove its convergence. Whenever the stepsize γk satisﬁes two
inequalities1

(cid:40)

k ≤ (1 + θk−1)γ2
γ2
(cid:107)xk−xk−1(cid:107)
2(cid:107)∇f (xk)−∇f (xk−1)(cid:107),
γk ≤

k−1,

1It can be shown that
(cid:107)xk−xk−1(cid:107)2
[3(cid:107)∇f (xk)(cid:107)2−4(cid:104)∇f (xk),∇f (xk−1)(cid:105)]+
main text for its simplicity.

instead of

, where [a]+

the second condition it

k ≤
def= max{0, a}, but we prefer the option written in the

is enough to ask for γ2

Algorithm 7 Adaptive Gradient Descent (AdGD)

88

1: Input: x0 ∈ Rd, γ0 > 0, θ0 = +∞, number of steps K
2: x1 = x0 − γ0∇f (x0)
3: for k = 1, 2, . . . , K − 1 do

(cid:107)xk−xk−1(cid:107)
2(cid:107)∇f (xk)−∇f (xk−1)(cid:107)

(cid:111)

(cid:110)(cid:112)1 + θk−1γk−1,

γk = min
xk+1 = xk − γk∇f (xk)
θk = γk
γk−1

4:

5:
6:
7: end for

independently of the properties of f (apart from convexity), we can show that the iterates
(xk)k remain bounded. Here and everywhere else we use the convention 1/0 = +∞, so
if ∇f (xk) − ∇f (xk−1) = 0, the second inequality can be ignored. In the ﬁrst iteration,
In this case,
when θ0 = +∞, it might happen that L0 = 0 and γ1 = min{+∞, +∞}.
we suppose that any choice of γ1 > 0 is possible.

Although Algorithm 7 needs x0 and γ0 as input, this is not an issue as one can simply
ﬁx x0 = 0 and γ0 = 10−10. Equipped with a tiny γ0, we ensure that x1 will be close
enough to x0 and likely will give a good estimate for γ1. Otherwise, this has no inﬂuence
on further steps.

5.3.2 Analysis without descent

It is now time to show our main contribution, the new analysis technique. The tools that
we are going to use are the well-known Cauchy-Schwarz and convexity inequalities.
In
addition, our methods are related to potential functions [262], which is a powerful tool
for producing tight bounds for GD.

Another divergence from the common practice is that our main lemma includes not
only xk+1 and xk, but also xk−1. This can be seen as a two-step analysis, while the
majority of optimization methods have one-step bounds. However, as we want to adapt
to the local geometry of our objective, it is rather natural to have two terms to capture
the change in the gradients.

Now, it is time to derive a characteristic inequality for a speciﬁc Lyapunov energy.

Lemma 5.3.1. Let f : Rd → R be convex and diﬀerential and let x(cid:63) be any solution of
(5.1). Then for (xk)k generated by Algorithm 7 it holds

(cid:107)xk+1 − x(cid:63)(cid:107)2 +

1
2

(cid:107)xk+1 − xk(cid:107)2 + 2γk(1 + θk)(f (xk) − f (cid:63))

≤ (cid:107)xk − x(cid:63)(cid:107)2 +

1
2

(cid:107)xk − xk−1(cid:107)2 + 2γkθk(f (xk−1) − f (cid:63)).

(5.5)

Proof. Let k ≥ 1. We start from the standard way of analyzing GD:

(cid:107)xk+1 − x(cid:63)(cid:107)2 = (cid:107)xk − x(cid:63)(cid:107)2 + 2 (cid:10)xk+1 − xk, xk − x(cid:63)(cid:11) + (cid:107)xk+1 − xk(cid:107)2
(cid:10)∇f (xk), x(cid:63) − xk(cid:11) + (cid:107)xk+1 − xk(cid:107)2.

= (cid:107)xk − x(cid:63)(cid:107)2 + 2γk

As usually, we bound the scalar product by convexity of f :

89

2γk

(cid:10)∇f (xk), x(cid:63) − xk(cid:11) ≤ 2γk(f (cid:63) − f (xk)),

(5.6)

which gives us

(cid:107)xk+1 − x(cid:63)(cid:107)2 ≤ (cid:107)xk − x(cid:63)(cid:107)2 − 2γk(f (xk) − f (cid:63)) + (cid:107)xk+1 − xk(cid:107)2.

(5.7)

These two steps have been repeated thousands of times, but now we continue in a com-
pletely diﬀerent manner. We have precisely one “bad” term in (5.7), which is (cid:107)xk+1−xk(cid:107)2.
We will bound it using the diﬀerence of gradients:

(cid:107)xk+1 − xk(cid:107)2 = 2(cid:107)xk+1 − xk(cid:107)2 − (cid:107)xk+1 − xk(cid:107)2

(5.8)

= −2γk(cid:104)∇f (xk), xk+1 − xk(cid:105) − (cid:107)xk+1 − xk(cid:107)2
= 2γk(cid:104)∇f (xk) − ∇f (xk−1), xk − xk+1(cid:105)

+ 2γk(cid:104)∇f (xk−1), xk − xk+1(cid:105) − (cid:107)xk+1 − xk(cid:107)2.

(5.9)

Let us estimate the ﬁrst two terms in the right-hand side above. First, deﬁnition of γk,
followed by Cauchy-Schwarz and Young’s inequalities, yields

2γk(cid:104)∇f (xk) − ∇f (xk−1), xk − xk+1(cid:105) ≤ 2γk(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107)

≤ (cid:107)xk − xk−1(cid:107)(cid:107)xk − xk+1(cid:107)

≤

1
2

(cid:107)xk − xk−1(cid:107)2 +

1
2

(cid:107)xk+1 − xk(cid:107)2.

(5.10)

Secondly, by convexity of f ,

2γk(cid:104)∇f (xk−1), xk − xk+1(cid:105) =

2γk
γk−1

(cid:104)xk−1 − xk, xk − xk+1(cid:105) = 2γkθk(cid:104)xk−1 − xk, ∇f (xk)(cid:105)

≤ 2γkθk(f (xk−1) − f (xk)).

(5.11)

Plugging (5.10) and (5.11) in (5.8), we obtain

(cid:107)xk+1 − xk(cid:107)2 ≤

1
2

(cid:107)xk − xk−1(cid:107)2 −

1
2

(cid:107)xk+1 − xk(cid:107)2 + 2γkθk(f (xk−1) − f (xk)).

Finally, using the produced estimate for (cid:107)xk+1 − xk(cid:107)2 in (5.7), we deduce the desired
(cid:4)
inequality (5.5).

The above lemma already might give a good hint why our method works. From
inequality (5.5) together with condition γ2
k−1, we obtain that the Lyapunov
energy—the left-hand side of (5.5)—is decreasing. This gives us boundedness of (xk)k,
which is often the key ingredient for proving convergence. In the next theorem we formally
state our result.

k ≤ (1+θk−1)γ2

Theorem 5.3.2. Suppose that f : Rd → R is convex with locally Lipschitz gradient ∇f .

Then (xk)k generated by Algorithm 7 converges to a solution of (5.1) and we have that

90

where

f (ˆxK) − f (cid:63) ≤

D
2Sk

= O

(cid:17)

,

(cid:16) 1
K

ˆxK =

γK(1 + θK)xK + (cid:80)K−1

i=1 wixi

SK

,

wi = γi(1 + θi) − γi+1θi+1,

SK = γK(1 + θK) +

K−1
(cid:88)

i=1

wi =

K
(cid:88)

i=1

γi + γ1θ1,

and D is a constant that explicitly depends on the initial data and the solution set, see
(5.12).

Our proof will consist of two parts. The ﬁrst one is a straightforward application of
Lemma 5.3.1, from which we derive boundedness of (xk)k and complexity result. Due to
its conciseness, we provide it directly after this remark. In the second part, we prove that
the whole sequence (xk)k converges to a solution. Surprisingly, this part is a bit more
technical than expected, and thus we postpone it to the appendix.

Proof. (Boundedness and complexity result.)

Fix any x(cid:63) from the solution set of eq. (5.1). Telescoping inequality (5.5), we deduce

(cid:107)xk+1 − x(cid:63)(cid:107)2 +

1
2

(cid:107)xk+1 − xk(cid:107)2 + 2γk(1 + θk)(f (xk) − f (cid:63))

+ 2

k−1
(cid:88)

i=1

[γi(1 + θi) − γi+1θi+1](f (xi) − f (cid:63))

≤ (cid:107)x1 − x(cid:63)(cid:107)2 +

1
2

(cid:107)x1 − x0(cid:107)2 + 2γ1θ1[f (x0) − f (cid:63)] def= D.

(5.12)

Note that by deﬁnition of γk, the second line above is always nonnegative. Thus, the
sequence (xk)k is bounded. Since ∇f is locally Lipschitz, it is Lipschitz continuous on
bounded sets. It means that for the set C = conv{x(cid:63), x0, x1, . . . }, which is bounded as
the convex hull of bounded points, there exists L > 0 such that

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107) ∀x, y ∈ C.

Clearly, γ1 =
other words, the sequence (γk)k is separated from zero.

2(cid:107)∇f (x1)−∇f (x0)(cid:107) ≥ 1

(cid:107)x1−x0(cid:107)

2L , thus, by induction one can prove that γk ≥ 1

2L , in

Now we want to apply the Jensen’s inequality for the sum of all terms f (xi) − f (cid:63) in
the left-hand side of (5.12). Notice, that the total sum of coeﬃcients at these terms is

γk(1 + θk) +

k−1
(cid:88)

i=1

[γi(1 + θi) − γi+1θi+1] =

k
(cid:88)

i=1

γi + γ1θ1 = Sk

Thus, by Jensen’s inequality,

91

D
2

≥

LHS of (5.12)
2

≥ Sk(f (ˆxk) − f (cid:63)),

where ˆxk is given in the statement of the theorem. By this, the ﬁrst part of the proof is
(cid:4)
complete. Convergence of (xk)k to a solution is provided in the appendix.

As we have shown that γi ≥ 1

2L for all i, we have a theoretical upper bound f (ˆxk) −
k . Note that in practice, however, (γk) might be much larger than the pessimistic
2L, which we observe in our experiments together with a faster convergence.

f (cid:63) ≤ DL
lower bound 1

5.3.3 f is locally strongly convex

Since one of our goals is to make optimization easy to use, we believe that a good method
should have state-of-the-art guarantees in various scenarios. For strongly convex functions,
this means that we want to see linear convergence, which is not covered by normalized GD
or online methods. In section 5.3.1 we have shown that Algorithm 7 matches the O(1/ε)
µ log 1
complexity of GD on convex problems. Now we show that it also matches O( L
ε )
complexity of GD when f is locally strongly convex. Similarly to local smoothness, we
call f locally strongly convex if it is strongly convex over any compact set of its domain.
For proof simplicity, instead of using bound γk ≤ (cid:112)1 + θk−1γk−1 as in step 4 of
Algorithm 7 we will use a more conservative bound γk ≤
2 γk−1 (otherwise the
derivation would be too technical). It is clear that with such a change Theorem 5.3.2 still
holds true, so the sequence is bounded and we can rely on local smoothness and local
strong convexity.

1 + θk−1

(cid:113)

Theorem 5.3.3. Suppose that f : Rd → R is locally strongly convex and ∇f is locally
Lipschitz. Then (xk)k generated by Algorithm 7 (with the modiﬁcation mentioned above)
converges to the solution x(cid:63) of (5.1). The complexity to get (cid:107)xk −x(cid:63)(cid:107)2 ≤ ε is O(κ log 1
ε ),
where κ = L
µ and L, µ are the smoothness and strong convexity constants of f on the set
C = conv{x(cid:63), x0, x1, . . . }.

We want to highlight that in our rate κ depends on the local Lipschitz and strong
convexity constants L and µ, which is meaningful even when these properties are not
satisﬁed globally. Similarly, if f is globally smooth and strongly convex, our rate is still
faster as it depends on the smaller local constants.

5.4 Heuristics

In this section, we describe several extensions of our method. We do not have a full theory
for them, but believe that they are of interest in applications.

Algorithm 8 Adaptive Accelerated Gradient Descent (AdGD-accel)

92

1: Input: x0 ∈ Rd, γ0 > 0, γ0 > 0, θ0 = Θ0 = +∞, number of steps K
2:
3: y1 = x1 = x0 − γ0∇f (x0)
4: for k = 1, 2, . . . , K − 1 do

(cid:107)xk−xk−1(cid:107)
2(cid:107)∇f (xk)−∇f (xk−1)(cid:107)

2 γk−1,
2 µk−1, (cid:107)∇f (xk)−∇f (xk−1)(cid:107)

2(cid:107)xk−xk−1(cid:107)

(cid:111)

(cid:111)

5:

6:

7:

γk = min

1 + θk−1

(cid:110)(cid:113)

(cid:110)(cid:113)

µk = min

√
√

1 + Θk−1
√

1/γk−

µk

√

µk

βk =
1/γk+
yk+1 = xk − γk∇f (xk)
xk+1 = yk+1 + βk(yk+1 − yk)
, Θk = µk
θk = γk
µk−1
γk−1

8:
9:
10:
11: end for

5.4.1 Acceleration

Suppose that f is µ-strongly convex. One version of the accelerated gradient method
proposed by Nesterov [186] is

yk+1 = xk −

1
L
xk+1 = yk+1 + β(yk+1 − yk),

∇f (xk),

√
√

√
√

L−
L+

µ
µ

where β =
1
2L by

. Adaptive Gradient Descent for strongly convex f eﬃciently estimated

(cid:26)(cid:114)

γk = min

1 +

θk−1
2

γk−1,

(cid:107)xk − xk−1(cid:107)
2(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)

(cid:27)

.

What about the strong convexity constant µ? We know that it equals to the inverse
smoothness constant of the conjugate f ∗(y) def= supx{(cid:104)x, y(cid:105) − f (x)}. Thus, it is tempting
to estimate this inverse constant just as we estimated inverse smoothness of f , i.e., by
formula

(cid:26)(cid:114)

µk = min

1 +

Θk−1
2

µk−1,

(cid:107)pk − pk−1(cid:107)
2(cid:107)∇f ∗(pk) − ∇f ∗(pk−1)(cid:107)

(cid:27)

where pk and pk−1 are some elements of the dual space and Θk = µk
µk−1. A natural choice
then is pk = ∇f (xk) since it is an element of the dual space that we use. What is its

93

value? It is well known that ∇f ∗(∇f (x)) = x, so we come up with the update rule

(cid:26)(cid:114)

µk = min

1 +

Θk−1
2

µk − 1,

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)
2(cid:107)xk − xk−1(cid:107)

(cid:27)

,

and hence we can estimate β by βk =

√
√

1/γk−

1/γk+

√

√

µk

µk

.

We summarize our arguments in Algorithm 8. Unfortunately, we do not have any

theoretical guarantees for it.

Estimating strong convexity parameter µ is important in practice. Most common
approaches rely on restarting technique proposed by Nesterov [184], see also [74] and
references therein. Unlike Algorithm 8, these works have theoretical guarantees, however,
the methods themselves are more complicated and still require tuning of other unknown
parameters.

5.4.2 Uniting our steps with stochastic gradients

Here we would like to discuss applications of our method to the problem

E [f (x; ξ)] ,

min
x

where f (·; ξ) is almost surely L-smooth and µ-strongly convex. Assume that at each
iteration we get sample ξk to make a stochastic gradient step,

xk+1 = xk − γk∇f (xk; ξk).

Then, we have two ways of incorporating our stepsize into SGD. The ﬁrst is to reuse
∇f (xk; ξk) to estimate Lk = (cid:107)∇f (xk;ξk)−∇f (xk−1;ξk)(cid:107)
, but this would make γk∇f (xk; ξk)
biased. Alternatively, one can use an extra sample to estimate Lk, but this is less intuitive
since our goal is to estimate the curvature of the function used in the update.

(cid:107)xk−xk−1(cid:107)

We give a full description in Algorithm 9. We remark that the option with a biased
estimate performed much better in our experiments with neural networks. The theorem
below provides convergence guarantees for both cases, but with diﬀerent assumptions.

Theorem 5.4.1. Let f (·; ξ) be L-smooth and µ-strongly convex almost surely. Assuming
2κ and estimating Lk with ∇f (·; ζ k), the complexity to get E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) ≤ ε
α ≤ 1
is not worse than O
. Furthermore, if the model is overparameterized, i.e.,
∇f (x(cid:63); ξ) = 0 almost surely, then one can estimate Lk with ξk and the complexity is
O (cid:0)κ2 log 1

ε log κ

(cid:16) κ2

(cid:1).

(cid:17)

ε

ε

Note that in both cases we match the known dependency on ε up to logarithmic terms,

but we get an extra κ as the price for adaptive estimation of the stepsize.

Another potential application of our techniques is estimation of decreasing stepsizes
in SGD. The best known rates for SGD [253], are obtained using γk that evolves as
O
. This requires estimates of both smoothness and strong convexity, which can

(cid:16) 1

(cid:17)

L+µk

Algorithm 9 Adaptive SGD (AdSGD)

94

1: Input: x0 ∈ Rd, γ0 > 0, θ0 = +∞, ξ0, α > 0, number of steps K
2: x1 = x0 − γ0∇f (x0; ξ0)
3: for k = 1, 2, . . . , K − 1 do
4:

5:

6:

7:

(cid:107)xk−xk−1(cid:107)

Sample ξk and optionally ζ k
Option I (biased): Lk = (cid:107)∇f (xk;ξk)−∇f (xk−1;ξk)(cid:107)
Option II (unbiased): Lk = (cid:107)∇f (xk;ζk)−∇f (xk−1;ζk−1)(cid:107)
γk = min
xk+1 = xk − γk∇f (xk; ξk)
θk = γk
γk−1

(cid:110)(cid:112)1 + θk−1γk−1, α

(cid:107)xk−xk−1(cid:107)

(cid:111)

Lk

8:
9:
10: end for

be borrowed from the previous discussion. We leave rigorous proof of such schemes for
future work.

5.5 Experiments

In the experiments2, we compare our approach with the two most related methods: GD
and Nesterov’s Accelerated method for convex functions [183]. Additionally, we consider
line search, Polyak step, and Barzilai–Borwein method. For neural networks we also include
a comparison with SGD, SGDm and Adam.

i x)) + λ

Logistic regression. The logistic loss with (cid:96)2-regularization is given by 1
i=1 log(1 +
n
exp(−bia(cid:62)
2 (cid:107)x(cid:107)2, where n is the number of observations, λ > 0 is a regularization
parameter, and (ai, bi) ∈ Rd ×R, i = 1, . . . , n, are the observations. We use ‘mushrooms’
and ‘covtype’ datasets to run the experiments. We choose λ proportionally to 1
n as often
done in practice. Since we have closed-form expressions to estimate L = 1
4n(cid:107)A(cid:107)2 + λ,
where A = (a(cid:62)
L in GD and its acceleration. The results
are provided in Figure 5.1.

n )(cid:62), we used stepsize 1

1 , . . . , a(cid:62)

(cid:80)n

2(cid:107)UV(cid:62) − A(cid:107)2

Matrix factorization. Given a matrix A ∈ Rm×n and r < min{m, n}, we want to
F for U ∈ Rm×r and V ∈ Rn×r.
solve minX=[U,V] f (X) = f (U, V) = 1
It is a non-convex problem, and the gradient ∇f is not globally Lipschitz. With some
tuning, one still can apply GD and Nesterov’s accelerated method, but—and we want to
emphasize it—it was not a trivial thing to ﬁnd the steps in practice. The steps we have
chosen were almost optimal, namely, the methods did not converge if we doubled the
steps. In contrast, our methods do not require any tuning, so even in this regard they are
much more practical. For the experiments we used Movilens 100K dataset [99] with more
than million entries and several values of r = 10, 20, 30. All algorithms were initialized
at the same point, chosen randomly. The results are presented in Figure 5.2.

2See https://github.com/ymalitsky/adaptive gd

95

(a) Mushrooms dataset, objec-
tive

(b) Mushrooms dataset, step-
size

(c) Covtype dataset, objective

Figure 5.1: Results for the logistic regression problem.

(a) r = 10

(b) r = 20

(c) r = 30

Figure 5.2: Results for matrix factorization. The objective is neither convex nor smooth.

In cubic regularization of Newton method [188], at each iteration
Cubic regularization.
6 (cid:107)x(cid:107)3, where g ∈ Rd, H ∈ Rd×d and
we need to minimize f (x) = g(cid:62)x + 1
2x(cid:62)Hx + M
M > 0 are given. This objective is smooth only locally due to the cubic term, which is our
motivation to consider it. g and H were the gradient and the Hessian of the logistic loss
with the ‘covtype’ dataset, evaluated at x = 0 ∈ Rd. Although the values of M = 10,
20, 100 led to similar results, they also required diﬀerent numbers of iterations, so we
present the corresponding results in Figure 5.3.

Barzilai–Borwein, Polyak and line searches. We have started this chapter with an
overview of diﬀerent approaches to tackle the issue of a stepsize for GD. Now, we demon-
strate some of those solutions. We again consider the (cid:96)2-regularized logistic regression
(same setting as before) with ‘mushrooms’, ‘covtype’, and ‘w8a’ datasets.

In Figure 5.4 (left) we see that the Barzilai–Borwein method can indeed be very fast.
However, as we said before, it lacks a theoretical basis and Figure 5.4 (middle) illustrates
this quite well. Just changing one dataset to another makes both versions of this method to
diverge on a strongly convex and smooth problem. Polyak’s method consistently performs
well (see Figure 5.4 (left and middle)), however, only after it was fed with f (cid:63) that we
found by running another method. Unfortunately, for logistic regression there is no way
to guess this value beforehand.

Finally, line search for GD (Armijo version) and Nesterov GD (implemented as in [184])
eliminates the need to know the stepsize, but this comes with a higher price per iteration

050010001500200025003000Iteration10-1610-1310-1010-710-410-1f(xk)−f∗GDNesterovAdGDAdGD-accelNesterov-strong0100200300400500Iteration100101102103104λkAdGD1/L1/µ0200040006000800010000Iteration10-1210-1010-810-610-410-2f(xk)−f∗GDNesterovAdGDAdGD-accelNesterov-strong0100020003000400050006000Iteration1010107104101102105f(xk)GDNesterovAdGDAdGD-accel050001000015000200002500030000Iteration1010107104101102105f(xk)GDNesterovAdGDAdGD-accel020000400006000080000100000Iteration107105103101101103105f(xk)GDNesterovAdGDAdGD-accel96

(a) M = 10

(b) M = 20

(c) M = 100

Figure 5.3: Results for the non-smooth subproblem from cubic regularization.

(a) Mushrooms dataset, objec-
tive

(b) Covtype dataset, objective

(c) W8a dataset, objective

Figure 5.4: Additional results for the logistic regression problem.

as Figure 5.4 (right) shows. Actually in all our experiments for logistic regression with
diﬀerent datasets one iteration of Armijo line search was approximately 2 times more
expensive than AdGD, while line search for Nesterov GD was 4 times more expensive. We
note that these observations are consistent with the theoretical derivations in [184].

Neural networks. We use standard ResNet-18 and DenseNet-121 architectures imple-
mented in PyTorch [200] and train them to classify images from the CIFAR-10 dataset [128]
with cross-entropy loss.

√

. We ran it with

We use batch size 128 for all methods. For our method, we observed that 1
Lk

works
1 + νθk in the other factor with values of ν from

better than 1
2Lk
{1, 0.1, 0.05, 0.02,
0.01} and ν = 0.02 performed the best. For reference, we provide the result for the
theoretical estimate as well and value ν = 0.1 in the plot with estimated stepsizes. The
results are depicted in Figures 5.5 and 5.6 and other details are provided in appendix E.4.
We can see that, surprisingly, our method achieves better test accuracy than SGD
despite having the same train loss. At the same time, our method is signiﬁcantly slower at
the early stage and the results are quite noisy for the ﬁrst 75 epochs. Another observation
is that the smoothness estimates are very non-uniform and γk plummets once train loss
becomes small.

01000200030004000Iteration10-1010-710-410-1102f(xk)−f∗GDNesterovAdGDAdGD-accel050010001500200025003000Iteration10-1010-710-410-1102f(xk)−f∗GDNesterovAdGDAdGD-accel02004006008001000Iteration10-1010-710-410-1102f(xk)−f∗GDNesterovAdGDAdGD-accel0500100015002000# matrix-vector multiplications10-1610-1310-1010-710-410-1f(xk)−f∗BB1BB2AdGDPolyak0200040006000800010000# matrix-vector multiplications10-710-410-1102105f(xk)−f∗BB1BB2AdGDPolyak0100020003000400050006000# matrix-vector multiplications10-1510-1210-910-610-3100f(xk)−f∗ArmijoNesterov-LSAdGDAdGD-accel97

(a) Test accuracy

(b) Stepsize

(c) Train loss

Figure 5.5: Results for training ResNet-18 on CIFAR-10. Labels for AdGD correspond to
how γk was estimated.

(a) Test accuracy

(b) Stepsize

(c) Train loss

Figure 5.6: Results for training DenseNet-121 on CIFAR-10.

050100150200250Epoch808284868890929496Test accuracySGDSGDmAdamAdSGD, (1+k1,0.5/Lk)AdSGD, (1+k1/50,1/Lk)050100150200250Epoch10-310-210-1100101102λkSGDAdSGD, (1+θk−1,0.5/Lk)AdSGD, (1+θk−1/10,1/Lk)AdSGD, (1+θk−1/50,1/Lk)050100150200250Epoch10-410-310-210-1100Train lossSGDSGDmAdamAdSGD, (1+θk−1,0.5/Lk)AdSGD, (1+θk−1/50,1/Lk)050100150200Epoch808284868890929496Test accuracySGDSGDmAdamAdSGD, (1+θk−1,0.5/Lk)AdSGD, (1+θk−1/50,1/Lk)050100150200Epoch10-310-210-1100101102λkSGDAdSGD, (1+θk−1,0.5/Lk)AdSGD, (1+θk−1/10,1/Lk)AdSGD, (1+θk−1/50,1/Lk)050100150200Epoch10-410-310-210-1100Train lossSGDSGDmAdamAdSGD, (1+θk−1,0.5/Lk)AdSGD, (1+θk−1/50,1/Lk)98

Chapter 6

Achieving Fast Rates in Distributed Optimization with
Quantization

6.1 Introduction

Big machine learning models are typically trained in a distributed fashion, with the training
data distributed across several workers, all of which compute in parallel an update to the
model based on their local data. For instance, they can all perform a single step of
Gradient Descent (GD ) or Stochastic Gradient Descent (SGD ). These updates are then
sent to a parameter server which performs aggregation (typically this means just averaging
of the updates) and then broadcasts the aggregated updates back to the workers. The
process is repeated until a good solution is found.

When doubling the amount of computational power, one usually expects to see the
learning process ﬁnish in half time. If this is the case, the considered system is called to
scale linearly. For various reasons, however, this does not happen, even to the extent that
the system might become slower with more resources. At the same time, the surge of
big data applications increased the demand for distributed optimization methods, often
requiring new properties such as ability to ﬁnd a sparse solution. It is, therefore, of great
importance to design new methods that are versatile, eﬃcient and scale linearly with the
amount of available resources.
In fact, the applications vary a lot in their desiderata.
There is a rising interest in federated learning [124], where the main concerns include
the communication cost and ability to use local data only in an attempt to provide a
certain level of privacy.
In high-dimensional machine learning problems, non-smooth (cid:96)1-
penalty is often utilized, so one wants to have a support for proximable regularization.
The eﬃciency of deep learning, in contrast, is dependent on heavy-ball momentum and
non-convex convergence to criticality, while sampling from the full dataset might not be
an issue. In this chapter, we try to address all of these questions.

Communication as the bottleneck. The key aspects of distributed optimization
In general, evaluating full
eﬃciency are computational and communication complexity.
gradients is intractable due to time and memory restrictions, so computation is made cheap
by employing stochastic updates. On the other hand, in typical distributed computing
architectures, communication is much slower (see Figure F.3 for our experiments with
communication cost of aggregating and broadcasting) than a stochastic update, and the
design of a training algorithm needs to ﬁnd a trade-oﬀ between them.

6.2 Related Work

99

There have been considered several ways of dealing with the issue of slow communication.
One of the early approaches is to have each worker perform a block descent step, which
leads to the Hydra family of methods [222, 75]. By choosing the size of the block, one
directly chooses the amount of data that needs to be communicated. An alternative idea
is for each worker to do more work between communication rounds (e.g., by employing a
more powerful local solver, such as a second order method), so that computation roughly
balances out with communication. The key methods in this sphere include CoCoA and its
variants [105, 152, 248], DANE [243], DiSCO [294, 153], DANCE [106] and AIDE [220].
Update compression via randomized sparsiﬁcation and/or quantization. Prac-
titioners suggested a number of heuristics to ﬁnd a remedy for the communication botlle-
neck. Of special interest to this chapter is the idea of compressing SGD updates, proposed
in [236]. Building oﬀ of this work, Alistarh et al. [5] designed a variant of SGD that guar-
antees convergence with compressed updates. Other works with SGD update structure
include [122, 20, 118]. Despite proving a convergence rate, Alistarh et al. [5] also left
many new questions open and introduced an additional, unexplained, heuristic of quan-
tizing only vector blocks. Moreover, their analysis implicitly makes an assumption that
all data should be available to each worker, which is hard and sometimes even impossible
to satisfy.
In a concurrent with [5] work [276], the TernGrad method was analyzed for
stochastic updates that in expectation have positive correlation with the vector pointing
to the solution. While giving more intuition about convergence of quantized methods,
this work used (cid:96)∞ norm for quantization, unlike (cid:96)2-quantization of [5].

6.3 Settings and Contributions

The problem. Let fi : Rd → R be the loss of model x obtained on data points belonging
to distribution Di, i.e., fi(x) def= Eξ∼Di[f (x; ξ)] and ψ : Rd → R ∪ {+∞} be a proper
closed convex regularizer. In this chapter, we focus on the problem of training a machine
learning model via regularized empirical risk minimization:

(cid:34)

min
x∈Rd

f (x) + ψ(x) def=

1
M

M
(cid:88)

i=1

(cid:35)

fi(x) + ψ(x)

.

(6.1)

We do not assume any kind of similarity between distributions D1, . . . , DM .

We also need to introduce several key concepts and ingredients that come together
to make the algorithm. In each iteration k of DIANA, each node will sample an unbiased
estimator of the local gradient. We assume that these gradients have bounded variance.

Assumption 6.3.1 (Stochastic gradients). For every i = 1, 2, . . . , M , E[gk
i
∇fi(xk). Moreover, the variance is bounded:

| xk] =

E (cid:2)(cid:107)gk

i − ∇fi(xk)(cid:107)2(cid:3) ≤ σ2
i .

(6.2)

100

rate” means that
Table 6.1: Comparison of DIANA and related methods. Here “lin.
linear convergence either to a ball around the optimum or to the optimum was proved,
“loc. data” describes whether or not authors assume that fi is available at node i only,
“non-smooth” means support for a non-smooth regularizer, “momentum” says whether or
not authors consider momentum in their algorithm, and “block quant.” means theoretical
justiﬁcation for using block quantization.

Method

Linear rate

Local data Non-smooth Momentum Block quant.

DIANA (New!)
QSGD [5]
TernGrad [276]
DQGD [118]
QSVRG [5]

(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

Note that gk def= 1
M

(cid:80)M

i=1 gk
i

is an unbiased estimator of ∇f (xk):

E[gk | xk] =

1
M

M
(cid:88)

i=1

∇fi(xk) = ∇f (xk).

(6.3)

Let σ2 def= 1
M
variance is bounded above by

i=1 σ2

(cid:80)M

i . By independence of the random vectors (gk

i − ∇fi(xk))M

i=1, its

E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2 | xk(cid:3) ≤

σ2
M

.

(6.4)

Notation. By sign(t) we denote the sign of t ∈ R (-1 if t < 0, 0 if t = 0 and 1 if t >
0). The j-th element of a vector x ∈ Rd is denoted as x(j). For x = (x(1), . . . , x(d)) ∈ Rd
and p ≥ 1, we let (cid:107)x(cid:107)p = ((cid:80)
i |x(i)|p)1/p. Note that (cid:107)x(cid:107)1 ≥ (cid:107)x(cid:107)p ≥ (cid:107)x(cid:107)∞ for all x.
By (cid:107)x(cid:107)0 we denote the number of nonzero elements of x. Detailed description of the
notation is in Table A.5 in the appendix.

DIANA. We develop a distributed gradient-type method with compression of gradient

diﬀerences, which we call DIANA (Algorithm 10).

Rate in the strongly convex case. We show that when applied to the smooth
strongly convex minimization problem with arbitrary closed convex regularizer, DIANA
has the iteration complexity

(cid:32)(cid:32)(cid:114)

O

(cid:32)

+ κ

1 +

(cid:114)

d
m

1
M

(cid:33)(cid:33)

(cid:33)

,

1
ε

log

d
m

to a ball with center at the optimum (see Sec 6.5, Theorem 6.5.4 and Corollary 6.5.5
(cid:1) iteration complexity
for the details).
(see Sec 6.5.1, Theorem 6.5.6 and Corollary 6.5.7 for the details). Unlike in [118], in a
noiseless regime our method converges to the exact optimum, and at a linear rate.

In the case of decreasing stepsize we show O (cid:0) 1

ε

Rate in the non-convex case. We prove that DIANA also works for smooth non-

convex problems and get the iteration complexity

101

O

(cid:18) 1
ε2

(cid:18) L2(f (x0) − f ∗)2
M 2α2
p

+

σ4
(1 + M αp)2

(cid:19)(cid:19)

(see Sec 6.6, Theorem 6.6.2 and Corollary 6.6.3 for the details).

DIANA with momentum. We study momentum version of DIANA for the case
of smooth non-convex objective with constant regularizer and fi = f (see Sec F.10,
Theorem F.10.1 and Corollary F.10.2 for the details). We summarize a few key features
of our complexity results established in Table 6.1.

First rate for TernGrad. We provide ﬁrst convergence rate of TernGrad and pro-
vide new tight analysis of 1-bit QSGD under less restrictive assumptions for both smooth
strongly convex objectives with arbitrary closed convex regularizer and non-convex objec-
tive (see Sec 6.4 for the detailed comparison). Both of these methods are just special cases
of our Algorithm 16 which is also a special case of Algorithm 10 with α = 0 and h0
i = 0
iteration complexity of convergence
for all i. We show that Algorithm 16 has O
to the ball with center at the optimum in the case of the smooth strongly convex mini-
mization problem with arbitrary closed convex regularizer (see Sec F.11, Theorem F.11.9)
and

(cid:16) κ
M αp

(cid:17)

O

(cid:18) 1
ε2

(cid:18) L2(f (x0) − f (x(cid:63)))2
M 2α2
p

+

σ4
(1 + M αp)2

(cid:19)(cid:19)

in the case of non-convex minimization problem (see Theorem F.11.5 and Corollary F.11.6).
QSGD and TernGrad with momentum. We study momentum version of DIANA
for α = 0, h0
i = 0 and, in particular, we propose momentum versions of (1-bit) QSGD and
TernGrad the case of smooth non-convex objective with constant regularizer and fi = f
(see Sec F.11.4, Theorem F.11.7 and Corollary F.11.8).

Optimal norm power. We ﬁnd the answer for the following question: which (cid:96)p norm
to use for quantization in order to get the best iteration complexity of the algorithm?
It is easy to see that all the bounds that we propose depend on 1
where αp is an
αp
increasing function of 1 ≤ p ≤ ∞ (see Lemma 6.5.3 for the details). That is, for both
Algorithm 10 and 16 the iteration complexity reduces when p is growing and the best
iteration complexity for our algorithms is achieved for p = ∞. This implies that TernGrad
has better iteration complexity than 1-bit QSGD.

First analysis for block-quantization. We give a ﬁrst analysis of block-quantization

(i.e., bucket-quantization), which was mentioned in [5] as a useful heuristic.

6.4 The Algorithm

In this section we describe our main method—DIANA.

Quantization. DIANA applies random compression (quantization) to gradient diﬀer-
ences, which are then communicated to a parameter server. We now deﬁne the random
quantization transformations used. Our ﬁrst quantization operator transforms a vector
∆ ∈ Rd into a random vector ˆ∆ ∈ Rd whose entries belong to the set {−t, 0, t} for some
t > 0.

Algorithm 10 DIANA (M nodes)

102

learning rates α > 0 and (γk)k≥0, initial vectors x0, h0
(cid:80)M

1: Input:
h0 = 1
M
parameter 0 ≤ β < 1, number of steps K

i , quantization parameter p ≥ 1, sizes of blocks (dl)M

i=1 h0

1, . . . , h0

M ∈ Rd and
l=1, momentum

2: v0 = ∇f (x0)
3: for k = 0, 1, . . . , K − 1 do
4:
5:
6:
7:
8:
9:

Broadcast xk to all workers
for i = 1, . . . , n in parallel do
i such that E[gk
i ∼ Quantp(∆k

Sample gk
Sample ˆ∆k

end for
(cid:80)M
ˆ∆k = 1
ˆ∆k
i
i=1
M
(cid:80)M
i = hk + ˆ∆k
ˆgk = 1
i=1 ˆgk
M
vk = βvk−1 + ˆgk
xk+1 = proxγkψ
(cid:80)M
hk+1 = 1
M

(cid:0)xk − γkvk(cid:1)

i=1 hk+1

10:
11:
12:

13:
14: end for

i = hk + α ˆ∆k

i | xk] = ∇fi(xk) and let ∆k
l=1) and let hk+1
i = hk
i , (dl)m

i = gk
i +α ˆ∆k

i − hk
i
i and ˆgk

i = hk

i + ˆ∆k

i

Deﬁnition 6.4.1 (p-quantization). Let ∆ ∈ Rd and let p ≥ 1.
(cid:101)∆ = ∆. If ∆ (cid:54)= 0, we deﬁne (cid:101)∆ by setting

If ∆ = 0, we deﬁne

(cid:101)∆(j) = (cid:107)∆(cid:107)psign(∆(j))ξ(j),

j = 1, 2, . . . , d,

where ξ(j) ∼ Be (cid:0)|∆(j)|/(cid:107)∆(cid:107)p

(cid:1) are Bernoulli random variables1. Note that

(cid:101)∆ = (cid:107)∆(cid:107)p sign(∆) ◦ ξ,

(6.5)

(6.6)

where sign is applied elementwise, and ◦ denotes the Hadamard (i.e., elementwise)
product. We say that (cid:101)∆ is p-quantization of ∆. When sampling (cid:101)∆, we shall write
(cid:101)∆ ∼ Quantp(∆).

In addition, we consider a block variant of p-quantization operators. These are deﬁned

and their properties studying in Section in the appendix.

Communication cost.

If b bits are used to encode a ﬂoat number, then at most
0 (log (cid:107) ˆ∆(cid:107)0 + log 2 + 1) + b bits are needed to communicate ˆ∆ with Elias
In our next result, we given an upper bound on the

C( ˆ∆) def= (cid:107) ˆ∆(cid:107)1/2
coding (see Theorem 3.3 in [5]).
expected communication cost.
Theorem 6.4.2 (Expected sparsity). Let 0 (cid:54)= ∆ ∈ R ˜d and (cid:101)∆ ∼ Quantp(∆) be its
p-quantization. Then

E(cid:107) (cid:101)∆(cid:107)0 =

(cid:107)∆(cid:107)1
(cid:107)∆(cid:107)p

≤ (cid:107)∆(cid:107)1−1/p

0

≤ ˜d1−1/p,

(6.7)

1That is, ξ(j) = 1 with probability |∆(j)|/(cid:107)∆(cid:107)p (observe that this quantity is always upper bounded

by 1) and ξ(j) = 0 with probability 1 − |∆(j)|/(cid:107)∆(cid:107)p.

Cp

def= E

(cid:104)

(cid:105)
C( (cid:101)∆)

≤

103
(cid:107)∆(cid:107)1/2
1
(cid:107)∆(cid:107)1/2
p

(log ˜d + log 2 + 1) + b.

(6.8)

All expressions in (6.7) and (6.8) are increasing functions of p.

i − hk
i

DIANA. In DIANA, each machine i ∈ {1, 2, . . . , n} ﬁrst computes a stochastic gra-
dient gk
i at current iterate xk. We do not quantize this information and send it oﬀ to the
parameter server as that approach would not converge for ψ (cid:54)= 0. Instead, we maintain
memory hk
i at each node i (initialized to arbitrary values), and quantize the diﬀerence
def= gk
instead. Both the node and the parameter server update hk
δk
in an appro-
i
i
priate manner, and a proximal gradient descent step is taken with respect to direction
vk = βvk−1 + ˆgk, where 0 ≤ β ≤ 1 is a momentum parameter, whereas ˆgk is an unbi-
ased estimator of the full gradient, assembled from the memory hk
i and the transmitted
quantized vectors. Note that we allows for block quantization for more ﬂexibility.
In
practice, we want the transmitted quantized vectors to be much easier to communicate
than the full dimensional vector in Rd, which can be tuned by the choice of p deﬁning the
quantization norm, and the choice of blocks.
Relation to QSGD and TernGrad.

If the initialization is done with h0 = 0 and
α = 0, our method reduces to either 1-bit QSGD or TernGrad with p = 2 and p = ∞
respectively. We unify them in the Algorithm 16. We analyze this algorithm (i.e., DIANA
with α = 0 and h0
i = 0) in three cases: i) smooth strongly convex objective with arbitrary
closed convex regularizer; ii) smooth non-convex objective with constant regularizer; iii)
smooth non-convex objective with constant regularizer for the momentum version of the
algorithm. We notice, that in the original paper [276], the authors do not provide the rate
of convergence for TernGrad, and we get the convergence rate for the three aforementioned
situations as a special case of our results. Moreover, we emphasize that our analysis is
new even for 1-bit QSGD, since in the original paper [5], the authors consider only the
case of bounded gradients (E (cid:2)(cid:107)gk(cid:107)2(cid:3) ≤ B2), which is a very restrictive assumption, and
they do not provide rigorous analysis of block-quantization. In contrast, we consider more
general case of block-quantization and assume only that the variance of the stochastic
gradients is bounded, which is strictly less restrictive since the inequality E (cid:2)(cid:107)gk(cid:107)2(cid:3) ≤ B2
implies E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)gk(cid:107)2(cid:3) ≤ B2.

We obtain the convergence rate for arbitrary p ≥ 1 for the three aforementioned cases
(see Theorems F.11.5, F.11.7, F.11.9, F.11.10 and Corollaries F.11.6, F.11.8, F.11.11 for
the details) and all obtained bounds becomes better when p is growing, which means
that TernGrad has better iteration complexity than QSGD and, more generally, the best
iteration complexity attains for (cid:96)∞ norm quantization.

6.5 Convergence Theory for Strongly Convex Losses

Let us introduce two key assumptions of this section.

Assumption 6.5.1 (L–smoothness). We say that a function f is L-smooth if

f (x) ≤ f (y) + (cid:104)∇f (y), x − y(cid:105) +

L
2

(cid:107)x − y(cid:107)2,

∀x, y.

(6.9)

Table 6.2: Summary of iteration complexity results.

104

Block quant.

Local data Non-convex

Str. convex ψ Momentum α > 0

Theorem

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)

6.6.2
F.10.1
6.5.4, 6.5.6
F.11.5
F.11.7
F.11.9, F.11.10

Assumption 6.5.2 (µ-strong convexity). f is µ-strongly convex, i.e.,

f (x) ≥ f (y) + (cid:104)∇f (y), x − y(cid:105) +

µ
2

(cid:107)x − y(cid:107)2,

∀x, y.

(6.10)

For 1 ≤ p ≤ +∞, deﬁne

αp(d) def= inf

x(cid:54)=0,x∈Rd

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

.

(6.11)

Lemma 6.5.3. αp is increasing as a function of p and decreasing as a function of d. In
particular, α1 ≤ α2 ≤ α∞, and moreover, α1(d) = 1
d)
and, as a consequence, for all positive integers (cid:101)d and d the following relations holds
α1( (cid:101)d) = α1(d)d

d , α2(d) = 1√

, α2( (cid:101)d) = α2(d)

(cid:113) d
, and α∞( (cid:101)d) = α∞(d)(1+
√
(cid:101)d
(cid:101)d)

, α∞(d) =

2
√
(1+

(1+

d)

√

(cid:101)d

.

d

Theorem 6.5.4. Assume the functions f1, . . . , fM are L–smooth and µ–strongly convex.
Choose stepsizes α > 0 and γk = γ > 0, block sizes (dl)m
dl, and
parameter c > 0 satisfying the following relations:

l=1, where (cid:101)d = max
l=1,...,m

1 + M cα2
1 + M cα
(cid:26) α
µ

,

γ ≤ min

≤ αp

def= αp( (cid:101)d),

2
(µ + L)(1 + cα)

(cid:27)

.

For any k ≥ 0, deﬁne the Lyapunov function

V k def= (cid:107)xk − x(cid:63)(cid:107)2 +

cγ2
M

M
(cid:88)

i=1

(cid:107)hk

i − h∗

i (cid:107)2,

where x(cid:63) is the solution of (6.1) and h∗ def= ∇f (x(cid:63)). Then for all k ≥ 0,

E (cid:2)V k(cid:3) ≤ (1 − γµ)kV 0 +

γ
µ

(1 + M cα)

σ2
M

.

(6.12)

(6.13)

(6.14)

(6.15)

This implies that as long as K ≥ 1

γµ log V 0

ε , we have E (cid:2)V K(cid:3) ≤ ε + γ

µ(1 + M cα) σ2
M .

In particular, if we set γ to be equal to the minimum in (6.13), then the leading term

105

Table 6.3: The leading term of the iteration complexity of DIANA in the strongly convex
case (Theorem 6.5.4, Corollary 6.5.5 and Lemma 6.5.3). Logarithmic dependence on 1/ε
is suppressed. Condition number: κ = L
µ .

p

1

2

∞ 1 +

2d

Iteration complexity
m + (κ + 1)A; A = (cid:0) 1
2 − 1
√
(cid:16) 1
2 − 1
d√
2
m + (κ + 1)B; B =
(cid:18)
(cid:113) d

m + (κ + 1)C; C =

(cid:1)

M + d
M +

(cid:17)

M m
√
d
√
m
√

M

1

2 − 1

M + 1+

2M

(cid:19)

d
m

κ = Θ(M )
O (cid:0)M + d
(cid:1)
m
(cid:18)
(cid:113) d
m
(cid:113) d
m

M +

M +

(cid:18)

O

O

(cid:19)

(cid:19)

κ = Θ(M 2)
O (cid:0)M 2 + M d
m
√
(cid:16)
d√
M 2 + M
m

(cid:1)

O

(cid:16)

O

M 2 + M

√
d√
m

(cid:17)

(cid:17)

in the iteration complexity bound is 1

γµ = max

(cid:110) 1

α, (µ+L)(1+cα)

2µ

(cid:111)

.

2 , c = 4(1−αp)
Corollary 6.5.5. Let κ = L
. Then
the conditions (6.12) and (6.13) are satisﬁed, and the leading iteration complexity term
is equal to

µ , α = αp

2
(L+µ)(1+cα)

, and γ = min

M α2
p

(cid:110) α
µ ,

(cid:111)

1
γµ

= max

(cid:26) 2
αp

, (κ + 1)

(cid:18)1
2

−

1
M

+

1
M αp

(cid:19)(cid:27)

.

(6.16)

This is a decreasing function of p, and hence p = +∞ is the optimal choice.

In Table 6.3 we calculate the leading term (6.16) in the complexity of DIANA for
p ∈ {1, 2, +∞}, each for two condition number regimes: n = κ (standard) and n = κ2
(large).

Matching the rate of gradient descent for quadratic size models. Note that
as long as the model size is not too big; in particular, when d = O(min{κ2, M 2}), the
linear rate of DIANA with p ≥ 2 is O(κ log(1/ε)), which matches the rate of gradient
descent.

Optimal block quantization. If the dimension of the problem is large, it becomes
reasonable to quantize vector’s blocks, also called blocks. For example, if we had a vector
which consists of 2 smaller blocks each of which is proportional to the vector of all ones,
In the real world, we
we can transmit just the blocks without any loss of information.
have a similar situation when diﬀerent parts of the parameter vector have diﬀerent scale.
A straightforward example is deep neural networks, layers of which have pairwise diﬀerent
scales. If we quantized the whole vector at once, we would zero most of the update for
the layer with the smallest scale.

√

Our theory says that if we have M workers, then the iteration complexity increase of
M . However, if quantization is applied to a block of size M 2, then
quantization is about
this number becomes 1, implying that the complexity remains the same. Therefore, if one
uses about 100 workers and splits the parameter vector into parts of size about 10,000,
the algorithm will work as fast as SGD, while communicating bits instead of ﬂoats!

d

Some consideration related to the question of optimal number of nodes are included

in Section F.4.

6.5.1 Decreasing stepsizes

106

We now provide a convergence result for DIANA with decreasing stepsizes, obtaining a
O(1/k) rate.

Theorem 6.5.6. Assume that f is L-smooth, µ-strongly convex and we have access to
(cid:111)
its gradients with bounded noise. Set γk = 2
for some numbers α > 0 and c > 0 satisfying 1+M cα2
we have

1+M cα ≤ αp. After K iterations of DIANA

µk+θ with some θ ≥ 2 max

α, (µ+L)(1+cα)

(cid:110) µ

2

E (cid:2)V K(cid:3) ≤

1
ηK + 1

(cid:26)

V 0, 4

max

(1 + M cα)σ2
M θµ

(cid:27)

,

where η def= µ
gradient noise.

θ , V k = (cid:107)xk − x(cid:63)(cid:107)2 + cγk

M

(cid:80)M

i=1 (cid:107)h0

i − h∗

i (cid:107)2 and σ2 is the variance of the

Corollary 6.5.7. If we choose α = αp

(cid:110)

µ
αp

max

then θ = Θ

4, 2(κ+1)
(cid:17)
(cid:16) µ
αp

M + (κ+1)(M −2)

M

αp

2 , c = 4(1−αp)

α, (µ+L)(1+cα)
(cid:111)
, then there are three regimes: i) if 1 = max (cid:8)1, κ

, θ = 2 max

M α2
p

2

(cid:110) µ

(cid:111)

=

M , καp

(cid:9),

and to achieve E (cid:2)V k(cid:3) ≤ ε we need at most

O

(cid:18) 1
αp

(cid:18)

V 0 +

(1 − αp)σ2
M µ2

(cid:19) 1
ε

(cid:19)

O

(cid:18) κ

(cid:18)

M αp

V 0 +

(1 − αp)σ2
µL

(cid:17)

(cid:16) L
M αp
(cid:19) 1
ε

(cid:19)

iterations; ii) if κ
we need at most

M = max (cid:8)1, κ

M , καp

(cid:9), then θ = Θ

and to achieve E (cid:2)V k(cid:3) ≤ ε

iterations; iii) if καp = max (cid:8)1, κ
M , καp
need at most
(cid:18)
(cid:18)

O

κ

iterations.

V 0 +

(1 − αp)σ2
µLnαp

(cid:19)

(cid:19) 1
ε

(cid:9), then θ = Θ (L) and to achieve E (cid:2)V k(cid:3) ≤ ε we

6.6 Convergence Theory for Non-Convex Losses

In this section we consider the non-convex case under the following assumption which we
call bounded data dissimilarity.

Assumption 6.6.1 (Bounded data dissimilarity). We assume that there exists a constant
ζ ≥ 0 such that for all x ∈ Rd

1
M

M
(cid:88)

i=1

(cid:107)∇fi(x) − ∇f (x)(cid:107)2 ≤ ζ 2.

(6.17)

107

In particular, Assumption 6.6.1 holds with ζ = 0 when all fi’s are the same up to some
additive constant (i.e., each worker samples from one dataset). We note that it is also
possible to extend our analysis to a more general assumption with extra O((cid:107)∇f (x)(cid:107)2)
term in the right-hand side of (6.17). However, this would overcomplicate the theory
without providing more insight.

Theorem 6.6.2. Assume that ψ is constant and Assumption 6.6.1 holds. Also assume
that f is L-smooth, stepsizes α > 0 and γk = γ > 0 and parameter c > 0 satisfying
1+M cα2
1+M cα ≤ αp, γ ≤

L(1+2cα) and xK is chosen randomly from {x0, . . . , xK−1}. Then

2

E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3) ≤

2
K

Λ0
γ(2 − Lγ − 2cαLγ)

+

(1 + 2cnα)Lγ
2 − Lγ − 2cαLγ

σ2
M

+

4cαLγζ 2
2 − Lγ − 2cαLγ

,

where Λk def= f (xk) − f ∗ + c Lγ2
2

1
M

(cid:80)M

i=1 (cid:107)hk

i − h∗

i (cid:107)2 for any k ≥ 0.

Corollary 6.6.3. Set α = αp
L(4+(M −4)αp)
algorithm for K iterations. Then, the ﬁnal accuracy is at most

2 , c = 4(1−αp)

, γ =

M α2
p

M αp

, h0 = 0 and run the

√

K

2
√
K

L(4 + (M − 4)αp)
M αp

Λ0 +

1
√
K

(4 − 3αp)σ2
4 + (M − 4)αp

+

8(1 − αp)ζ 2
(4 + (M − 4)αp)

.

√

K

Moreover, if the ﬁrst term in Corollary 6.6.3 is leading and 1

M = Ω(αp), the resulting
complexity is O( 1√
), i.e., the same as that of SGD . For instance, if suﬃciently large
K
mini-batches are used, the former condition holds, while for the latter it is enough to
quantize vectors in blocks of size O(M 2).

6.7 Experiments

Following advice from [5], we encourage the use of blocks when quantizing large vectors.
To this eﬀect, a vector can decomposed into a number of blocks, each of which should
then be quantized separately. If coordinates have diﬀerent scales, as is the case in deep
learning, it will prevent undersampling of those with typically smaller values. Moreover,
our theoretical results predict that applying quantization to blocks or layers will result in
superlinear acceleration.

In our convex experiments, the optimal values of α were usually around mini

where the minimum is taken with respect to blocks and di are their sizes.

1√
di

,

is favorable to more uniform diﬀerences gk

Finally, higher mini-batch sizes make the sampled gradients less noisy, which in turn
i − hk
Detailed description of the experiments can be found in Section F.11.6 as well as extra

i and faster convergence.

numerical results.

DIANA with momentum works best. We implement DIANA, QSGD, TernGrad
and DQGD in Python using MPI4PY for processes communication. This is then tested
on a machine with 24 cores, each is Intel(R) Xeon(R) Gold 6146 CPU @ 3.20GHz. The
problem considered is binary classiﬁcation with logistic loss and (cid:96)2 penalty, chosen to be
of order 1/N , where N is the total number of data points. We experiment with choices of

108

α, choice of norm type p, diﬀerent number of workers and search for optimal block sizes.
h0
is always set to be zero vector for all i. We observe that for (cid:96)∞-norm the optimal
i
block size is signiﬁcantly bigger than for (cid:96)2-norm. Here, however, we provide Figure 6.1
to show how vast the diﬀerence is with other methods.

Figure 6.1: Comparison of DIANA (β = 0.95) with QSGD, TernGrad and DQGD on the
logistic regression problem for the “mushrooms” dataset.

DIANA vs. MPI. In Figure 6.2 we compare the performance of DIANA vs. doing
a MPI reduce operation with 32bit ﬂoats. The computing cluster had Cray Aries High
Speed Network. However, for DIANA we used 2bit per dimension and have experienced a
strange scaling behaviour, which was documented also in [199]. In our case, this aﬀected
speed for alexnet and vgg a beyond 64 or 32 MPI processes respectively. For more detailed
experiments, see Section F.11.6.

Train and test accuracy on CIFAR-10. In the next experiments, we run QSGD [5],
TernGrad [276], SGD with momentum and DIANA on CIFAR-10 dataset for 3 epochs.
We have selected 8 workers and run each method for learning rate from {0.1, 0.2, 0.05}.
For QSGD, DIANA and TernGrad, we also tried various quantization bucket sizes in
{32, 128, 512}. For QSGD we have chosen 2, 4, 8 quantization levels. For DIANA we
quantization bucket sizes } and have selected initial h0 = 0.
have chosen α ∈ {0, 1.0/
For DIANA and SGD we also run a momentum version, with a momentum parameter in
{0, 0.95, 0.99}. For DIANA we also run with two choices of norm (cid:96)2 and (cid:96)∞. For each
experiment we have selected softmax cross entropy loss. CIFAR-10-DNN is a convolutional
DNN described here https://github.com/kuangliu/pytorch-cifar/blob/master/
models/lenet.py.

√

In Figure 6.3 we show the best runs over all the parameters for all the methods. We

notice that DIANA and SGD signiﬁcantly outperform other methods.

024681012Number of epochs104103102101100Functional accuracyTernGradQSGDDQGDDIANA momentum109

Figure 6.2: Comparison of performance (images/second) for various number of GPUs/MPI
processes and sparse communication DIANA (2bit) vs. Reduce with 32bit ﬂoat (FP32).

Figure 6.3: Evolution of training (left) and testing (right) accuracy on CIFAR-10, using 4
algorithms: DIANA, SGD, QSGD and TernGrad. We have chosen the best runs over all
tested hyper-parameters.

  4  8 16 32 64128MPI050000100000150000200000250000300000images/secondLeNet, batch size: 64methodFP32Diana  4  8 16 32 64128MPI050000100000150000200000250000images/secondCifarNet, batch size: 32methodFP32Diana  4  8 16 32 64128MPI050001000015000200002500030000images/secondalexnet v2, batch size: 128methodFP32DianaDiana - MultiGather  4  8 16 32 64128MPI050010001500200025003000images/secondvgg a, batch size: 32methodFP32DianaDiana - MultiGather0.00.51.01.52.02.53.0epoch0.10.20.30.40.5Train AccuracyCifar10-DNNmethodQSGDTernGradSGDDIANA0.00.51.01.52.02.53.0epoch0.10.20.30.40.5Test AccuracyCifar10-DNNmethodQSGDTernGradSGDDIANA110

Chapter 7

Developing Variance Reduction for Proximal Operators

7.1 Introduction

In this chapter we address optimization problems of the form

(cid:34)

min
x∈Rd

P (x) def= f (x) +

1
m

m
(cid:88)

j=1

(cid:35)

gj(x) + ψ(x)

,

(7.1)

where f : Rd → R is a smooth convex function, and ψ, g1, . . . , gm : Rd → R ∪ {+∞} are
proper closed convex functions, admitting eﬃciently computable proximal operators. We
also assume throughout that dom (P ) def= {x : P (x) < +∞} (cid:54)= ∅ and, moreover, that the
set of minimizers of (7.1), X (cid:63), is non-empty.

The main focus of this chapter is on how the diﬃcult non-smooth term

g(x) def=

1
m

m
(cid:88)

j=1

gj(x)

(7.2)

should be treated in order to construct an eﬃcient algorithm for solving the problem.
We are speciﬁcally interested in the case when m is very large, and when the proximal
operators of g and g + ψ are impossible or prohibitively diﬃcult to evaluate. We thus
need to rely on splitting approaches which make calls to proximal operators of functions
{gj} and ψ separately.

7.2 Related Work

Existing methods for solving problem (7.1) can eﬃciently handle the case m = 1 only [3].
There were a few attempts to design methods capable of handling the general m case, such
as [6, 202, 228] and [61]. None of the existing methods oﬀer a linear rate for non-smooth
problem except for random projection.
In cases when sublinear rates are established,
the assumptions on the functions gj are very restrictive. For instance, the results in
citeallen2017katyusha are limited to Lipschitz continuous gj only, and Defazio [61] assumes
gj to be strongly convex. This is very unfortunate because the majority of problems
appearing in popular data science and machine learning applications lack these properties.
For instance, if we want to ﬁnd a minimum of a smooth function over the intersection of
m convex sets, gj will be characteristic functions of sets, which are neither Lipschitz nor
strongly convex.

111

Applications. There is a long list of applications of the non-smooth ﬁnite-sum prob-
lem (7.1), including convex feasibility [14], constrained optimization [194], decentralized
optimization [180], support vector machine [56], Dantzig selector [37], overlapping group
Lasso [288], and fused Lasso. In Appendix G.1 we elaborate in detail how these problems
can be mapped to the general problem (7.1) (in particular, see Table G.1).

Variance reduction. Stochastic variance-reduction methods are a major break-
through of the last decade, whose success started with the Stochastic Dual Coordinate
Ascent (SDCA) method [241] and the invention of the Stochastic Average Gradient (SAG)
method [131]. Variance reduction has attracted enormous attention and now its reach
covers strongly convex, convex and non-convex [136] stochastic problems. Despite being
originally developed for ﬁnite-sum problems, variance reduction was shown to be appli-
cable even to problems with f expressed as a general expectation [134, 191]. Further
generalizations and extensions include variance reduction for minimax problems [196], co-
ordinate descent in the general ψ case [95], and minimization with arbitrary sampling [86].
However, very little is known about variance reduction for non-smooth ﬁnite sum problems.

7.3 Settings and Contributions

The departure point of our work is the observation that there is a class of non-smooth
problems for which variance reduction is not required; these are the linear feasibility prob-
lems: given A ∈ Rm×d and b ∈ Rm, ﬁnd x ∈ Rd such that Ax = b. Assuming the
system is consistent, this problem can be cast as an instance of (7.1), with ψ ≡ 0 ,
f (x) = 1
2(cid:107)x(cid:107)2 and gj corresponding to the characteristic function of the j-th equation in
the system. Eﬃcient SGD methods (or equivalently, randomized projection methods) with
linear convergence rates were recently developed for this problem [85, 223, 267], as well
as accelerated variants [267, 223, 83] whose linear rate yields a quadratic improvement
in the iteration complexity. However, it is not known whether these or similar linear rates
could be obtained when one considers f to be an arbitrary smooth and strongly convex
function. While our work was originally motivated by the quest to answer this question,
and we answer in the aﬃrmative, we were able to build a much more general theory, as
we explain below.

We now summarize some of the most important contributions of this chapter:
First variance reduction for g. We propose a variance-reduction strategy for pro-
gressively approximating the proximal operator of the average of a large number of non-
smooth functions gj via only evaluating the proximal operator of a single function gj in
each iteration. That is, unlike existing approaches, we are able to treat the diﬃcult term
(7.2) for any m. Combined with a gradient-type step in f (we allow for multiple ways in
which the gradient estimator is built; more on that below), and a proximal step for ψ, this
leads to a new and remarkably eﬃcient method (Algorithm 11) for solving problem (7.1).
Compatibility with any gradient estimator for f . Our variance-reduction scheme
for the non-smooth term g is decoupled from the way we choose to construct gradient
estimators for f . This allows us to use the most eﬃcient and suitable estimators depending
on the structure of f .
In this regard, two cases are of particular importance: i) f (x) =
Eξ[f (x; ξ)], where f (·; ξ) : Rd → R is almost surely convex and smooth, and ii) f =

112

Table 7.1: Selected special cases of our method. For Dykstra’s algorithm, C1, . . . , Cm are
closed convex sets; and we wish to ﬁnd projection onto their intersection. Randomized
Kaczmarz is a special case for linear constraints (i.e., Cj = {x : a(cid:62)
j x = bj}). We do not
prove convergence under the same assumptions as Point–SAGA as they require strong
convexity and smoothness of each gj, but the algorithm is still a special case.

Method

f

Forward–Backward

f1 = f , n = 1

Douglas–Rachford

Proximal SGD

Proximal SAGA

SDCA

Randomized Dykstra’s algorithm

Randomized Kaczmarz method

0
Eξ[f (·; ξ)]
(cid:80)

i fi

1
n

1

1

2 (cid:107)x − x0(cid:107)2
2 (cid:107)x − x0(cid:107)2
2 (cid:107)x − x0(cid:107)2

1

gj

0

ψ

ψ

g1 = g, m = 1 ψ

γ

< 2
L

any

Citation

[185, 49]

[144]

0

0

gj

χCj

χ{x:a(cid:62)

j x=bj }

0

[69]

[62]

ψ ≤ 1
4L
ψ ≤ 1
5L
γ = 1
m
γ = 1
m
γ = 1
m [110, 256]

NEW

[241]

0

0

Point–SAGA

0

gj

0

Condat–V˜u algorithm

f1 = f , n = 1

g1 = g, m = 1 ψ

any

< 2
L

[61]

[270, 52]

(cid:80)

i fi, where {fi} are convex and smooth.

1
In case i) one may consider the standard
n
stochastic gradient estimator ∇f (xk; ξk), or a mini-batch variant thereof, and in case ii)
one may consider the batch gradient ∇f (xk) if n is small, or a variance-reduced gradient
estimator, such as SVRG [108, 126] or SAGA [62, 209], if n is large. Our general analysis
allows for any estimator to be used as long as it satisﬁes a certain technical assumption
(Assumption 7.5.1).
In particular, to illustrate the versatility of our approach, we show
that this assumption holds for estimators used by Gradient Descent, SVRG, SAGA and
over-parameterized SGD.

Future-proof design. Our analysis is compatible with a wide array of other estimators
of the gradient of f beyond the speciﬁc ones listed above. Therefore, new speciﬁc variants
of our generic method for solving problem (7.1) can be obtained in the future by marrying
any such new estimators with our variance-reduction strategy for the non-smooth ﬁnite
sum term g.

Special cases. Special cases of our method include Randomized Kaczmarz method [110,

256], Douglas–Rachford splitting [144], Forward–Backward splitting [185, 49], a variant
of SDCA [241], and Point–SAGA [61]. Also, we obtain the ﬁrst randomized variant of
the famous Dykstra’s algorithm [72] for projection onto the intersection of convex sets.
These special cases are summarized in Table 7.1.

Sublinear rates. We ﬁrst prove convergence of the iterates to the solution set in a
Bregman sense, without quantifying the rate (see Appendix G.5.3). Next, we establish
(cid:1) rate with constant stepsizes under no assumption on problem (7.1) beyond the
O (cid:0) 1
existence of a solution and a few technical assumptions (see Theorem 7.6.1). The rate
improves to O (cid:0) 1
(cid:1) once we assume strong convexity of f , and allow for carefully designed
K2
decreasing stepsizes (see Theorem 7.6.3).

K

Linear rate in the non-smooth case with favourable data. Consider the spe-

113

Table 7.2: Summary of iteration complexity results that we proved. We assume by default
that all functions are convex, but provide diﬀerent rates based on whether f is strongly
convex (scvx) and whether g1, . . . , gm are smooth functions, which is represented by the
check marks.

Problem

f scvx

gj smooth Oracle

Rate

Theorem

E [f (x; ξ)] + 1
m

1
n

n
(cid:80)
i=1

fi(x) + 1
m

m
(cid:80)
j=1

m
(cid:80)
j=1

gj(x) + ψ(x)

gj(x) + ψ(x)

1
n

n
(cid:80)
i=1

fi(x) + 1
m

m
(cid:80)
j=1

φj(A(cid:62)

j x)

(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)

SGD

GD,
SVRG
and
SAGA

(cid:17)

(cid:16) 1√
O
O (cid:0) 1
O (cid:0) 1
O (cid:0) 1
K 2
Linear

K

K

K
(cid:1)
(cid:1)
(cid:1)

7.6.2

7.6.4

7.6.1

7.6.3

7.6.8

Linear

7.6.5, 7.6.7

cial case of (7.1) with f being strongly convex, ψ ≡ 0 and gj(x) = φj(A(cid:62)
j x), where
φj : Rdj → R ∪ {+∞} are proper closed convex functions, and Aj ∈ Rd×dj are given
(data) matrices:

f (x) +

min
x∈Rd

1
m

m
(cid:88)

j=1

φj(A(cid:62)

j x).

(7.3)

Let us deﬁne

A def= [A1, . . . , Am] ∈ Rd×(cid:80)
If the smallest eigenvalue of A(cid:62)A is positive, i.e., λmin(A(cid:62)A) > 0, then our method
converges linearly (see Theorem 7.6.5; and note that this can only happen if (cid:80)
j dj ≤ d).
Moreover, picking j with probability proportional to (cid:107)Aj(cid:107) is optimal (Corollary 7.6.6). In
j x=bj }(x) for some vectors b1 ∈ Rd1, . . . , bm ∈ Rd1,
the special case when φj(y) = χ{x : A(cid:62)
i.e., if we are minimizing a strongly convex function under a linear constraint,

j dj .

(cid:8)f (x) : A(cid:62)x = b(cid:9) ,

min
x∈Rd

then the rate is linear even if A(cid:62)A is not positive deﬁnite1. The rate will depend on
λ+
min(A(cid:62)A), i.e., the smallest positive eigenvalue (see Theorem 7.6.7).

Linear and accelerated rate in the smooth case. If g1, . . . , gm are smooth func-
tions, the rate is linear (see Theorem 7.6.8). If m is big enough, then it is also accelerated
(Corollary 7.6.9). A summary of our iteration complexity results is provided in Table 7.2.
Related work. The problems that we consider recently received a lot of attention.
(cid:1)

However, we are the ﬁrst to show linear convergence on non-smooth problems. O (cid:0) 1
convergence with stochastic variance reduction was obtained in [228] and [202], although

K

1By χC(x) we denote the characteristic function of the set C, deﬁned as χC(x) = 0 if x ∈ C and

χC(x) = +∞ if x /∈ C

Algorithm 11 Stochastic Decoupling Method (SDM)

114

Require: Stepsize γ, initial vectors x0, y0

1, . . . , y0

m ∈ Rd, probabilities p1, . . . , pm, oracle

that gives gradient estimates, number of steps K

4:
5:

(cid:80)m

1: for k = 0, 1, . . . , K − 1 do
2:
3:

Produce an estimate vk of ∇f (xk), e.g., vk = ∇f (xk)
yk = 1
j=1 yk
j
m
zk = proxγψ(xk − γvk − γyk)
Sample j from {1, . . . , m} with probabilities {p1, . . . , pm} and set ηj = γ
mpj
xk+1 = proxηj gj (zk + ηjyk
j )
yt+1
(zk − xk+1)
j = yk
7:
8: end for

j + 1
ηj

(cid:46) yk+1
j

6:

∈ ∂gj(xk+1)

(cid:1) rate as we do. On the other hand, works such as [289,
both works do not have O (cid:0) 1
K2
(cid:1) convergence, but only with all functions from f and g
39] managed to prove O (cid:0) 1
K2
(cid:1) for constrained minimization can be found
used at every iteration. Stochastic O (cid:0) 1
K2
(cid:1) rate) and
in [173]. There is also a number of works that consider parallel [63] (O (cid:0) 1
stochastic [297, 148] variants of ADMM, which work with one non-smooth term composed
with a linear transformation. To show linear convergence they require matrix in the
transformation to be positive-deﬁnite. Variance-reduced ADMM for compositions, which
is an orthogonal direction to ours, was considered in [286]. There is a method for non-
smooth problems with f ≡ 0 and proximal operator preconditioning that was analyzed
in detail in [40], we discuss the relation to it in Appendix G.2.5. Many methods were
designed to work with non-smooth functions in parallel only, and one can obtain more of
them from three-operator splitting methods such as the Condat–V˜u algorithm [270, 52].
Several works obtained linear convergence for smooth g [67, 196]. Coordinate descent
methods for two non-smooth functions were considered in [3].

K

We will make the following assumption related to optimality conditions.

Assumption 7.3.1. There exists x(cid:63) ∈ X (cid:63) and vectors y(cid:63)
and r(cid:63) ∈ ∂ψ(x(cid:63)) such that ∇f (x(cid:63)) + 1
j + r(cid:63) = 0.
m

j=1 y(cid:63)

(cid:80)m

1 ∈ ∂g1(x(cid:63)), . . . , y(cid:63)

m ∈ ∂gm(x(cid:63))

Throughout the chapter, we will assume that some x(cid:63) and y(cid:63)

m satisfying As-
sumption 7.3.1 are ﬁxed and all statements relate to these objects. We will denote
y(cid:63) def= 1
j . A commentary and further details related to this assumption can be
m
found in Appendix G.4.

1, . . . , y(cid:63)

j=1 y(cid:63)

(cid:80)m

7.4 The Algorithm

Our method is very general and can work with diﬀerent types of gradient update. One
only needs to have for each xk an estimate of the gradient vk such that E (cid:2)vk(cid:3) = ∇f (xk)
plus an additional assumption about its variance. We also maintain an estimate yk of
full proximal step with respect to g, which allows us to make an intermediate step zk =
proxγψ(xk − γvk − γyk). The key idea of this chapter is then to combine it with variance

115

In fact, it mimics variance-reduction step from [61],
reduction in the non-smooth part.
which was motivated by the SAGA algorithm [62]. Essentially, the expression above for
zk does not allow for update of yk, so we do one more step,

xk+1 = proxηj gj (zk + ηjyk

j ).

This can additionally be rewritten using the identity proxγg(x) ∈ x − γ∂g(proxγg(x)) as

xk+1 ∈ xk − γ(vk + ∂ψ(zk) + yk) − ηj(∂gj(xk+1) − yk

j ) ≈ proxγ(ψ+g)(xk − γ∇f (xk)).

To make sure that the approximation works, we want to make yk
which we do not know in advance. However, we do it in hindsight by updating yk+1
a particular subgradient from ∂gj(xk+1), namely

j be close to ∂gj(xk+1),
with

j

yk+1
j =

1
ηj

(zk + ηjyk

j − proxηj gj (zk + ηjyk

j )) ∈ ∂gj(xk+1).

We also need to accurately estimate ∇f (xk), and there several options for this. The
simplest choice is setting vk = ∇f (xk). Often this is too expensive and one can instead
construct vk using a variance-reduction technique, such as SAGA [62] (see Algorithm 12).
To a reader familiar with Fenchel duality, it might be of some interest that there is an
explanation of our ideas using the dual. Indeed, Problem (7.1) can be recast into

min
x

max
y1,...,ym

f (x) + ψ(x) +

1
m

m
(cid:88)

j=1

x(cid:62)yj −

1
m

m
(cid:88)

j=1

g(cid:63)
j (yj) ,

where g(cid:63)

j is the Fenchel conjugate of gj. Then, the proximal gradient step in x would be

(cid:32)

z = proxγψ

x − γ∇f (x) − γ

(cid:33)

yj

.

1
m

m
(cid:88)

j=1

In contrast, our update in yj is a proximal block-coordinate ascent step, so the overall
process is akin to Proximal Alternating Gradient Descent-Ascent (see [25, 51] for related
ideas). However, this is neither how we developed nor analyze the method, so this should
not be seen as a formal explanation.

7.5 Gradient Estimators

Since we want to have analysis that puts many diﬀerent methods under the same umbrella,
we need an assumption that is easy to satisfy. In particular, the following will ﬁt our needs.

Assumption 7.5.1. Let wk def= xk − γvk and w(cid:63) def= x(cid:63) − γ∇f (x(cid:63)). We assume that
the oracle produces vk and (potentially) updates some other variables in such a way that
for some constants γmax > 0, ω > 0 and nonnegative sequence {Mk}+∞
k=0, such that the
following holds for any γ ≤ γmax:

116

Algorithm 12 SAGA Oracle
Require: xk, table of past gradients ∇f1(uk
1: Sample subset S from {1, . . . , n} of size τ
2: vk = 1
τ
3: For all i ∈ S update ∇fi(uk+1
4: return vk

i )(cid:1) + αk
) with uk+1

(cid:0)∇fi(xk) − ∇fi(uk

(cid:80)

i∈S

i

i = xk

1), . . . , ∇fn(uk

n) and their average αk

(a) If f is convex, then

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3) + Mk+1 ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − ωγE (cid:2)Df (xk, x(cid:63))(cid:3) + Mk.

(b) If f is µ-strongly convex, then either Mk = 0 for all k or there exists ρ > 0 such

that

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3) + Mk+1 ≤ (1 − ωγµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + (1 − ρ)Mk.

We note that we could easily make a slightly diﬀerent assumption to allow for a
strongly convex ψ, but this would be at the cost of analysis clarity. Since the assumption
above is already quite general, we choose to stick to it and claim without a proof that in
the analysis it is possible to transfer strong convexity from f to ψ.

Another observation is that part (a) of Assumption 7.5.1 implies its part (b) with
ω/2. However, to achieve tight bounds for Gradient Descent we need to consider them
separately.

Lemma 7.5.2 (Proof in Appendix G.5.1). If f is convex, the Gradient Descent estimate,
vk = ∇f (xk), satisﬁes Assumption 7.5.1(a) with any γmax < 2
L, ω = 2 − γmaxL and
Mk = 0. If f is µ-strongly convex, Gradient Descent satisﬁes Assumption 7.5.1(b) with
γmax = 2

L+µ, ω = 1 and Mk = 0.

Since Mk = 0 for Gradient Descent, one can ignore ρ in the convergence results or

treat it as +∞.

(cid:80)n

E (cid:2)(cid:107)∇fi(uk

Lemma 7.5.3 (Proof in Appendix G.5.11). In SVRG and SAGA, if fi is L-smooth and
convex for all i, Assumption 7.5.1(a) is satisﬁed with γmax = 1
3 and Mk =
3γ2
i = uk is the reference point of
n
the current loop, and in SAGA uk
is the point whose gradient is stored in memory for
i
If f is also strongly convex, then Assumption 7.5.1 holds with γmax = 1
function fi.
5L,
ω = 1, ρ = 1
3n and the same Mk.

i ) − ∇fi(x(cid:63))(cid:107)2(cid:3) , where in SVRG uk

6L, ω = 1

i=1

Lemma 7.5.4 (Proof in Appendix G.5.12). Assume that at an optimum x(cid:63) the variance
of stochastic gradients is ﬁnite, i.e.,

σ2
(cid:63)

def= Eξ

(cid:2)(cid:107)∇f (x(cid:63); ξ) − ∇f (x(cid:63))(cid:107)2(cid:3) < +∞.

Then, SGD that terminates after at most K iterations satisﬁes Assumption 7.5.1(a)
with γmax = 1
k=0 is given by

In this case, sequence {Mk}K

4L , ω = 1 and ρ = 0.

117

Mk = 2γ2(K − k)σ2
with γmax = 1

2L, ω = 1 and Mk = 0.

(cid:63). If f is strongly convex and σ(cid:63) = 0, it satisﬁes Assumption 7.5.1(b)

There are two important cases for SGD. If the model is overparameterized, i.e., σ(cid:63) ≈ 0,
we get almost the same guarantees for SGD as for GD. If, σ(cid:63) (cid:29) 0, then one needs to
in order to keep M0 away from +∞. This eﬀectively changes
choose γ = O
(cid:16) 1√
(cid:1) rate for
the O (cid:0) 1
strongly convex case requires a separate proof.

, see Corollary 7.6.2. Moreover, obtaining a O (cid:0) 1

(cid:16) 1√
(cid:1) rate to O

KL

(cid:17)

(cid:17)

K

K

K

7.6 Convergence Theory

Let

ν def= min
j=1,...,m

1
ηjLj

= min

j=1,...,m

m

pj
γLj

,

where Lj ∈ R ∪ {+∞} is the smoothness constant of gj, in most cases giving Lj = +∞
and ν = 0. Tho goal of our analysis is to show that with introducing new term in the
Lyapunov function,

Y k def= (1 + ν)

η2
l

E (cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3) ,

m
(cid:88)

the convergence is not signiﬁcantly hurt. This term will be always incorporated in the full
Lyapunov function deﬁned as

l=1

Lk def= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + Mk + Y k,

where Mk is from Assumption 7.5.1. In the proof of O (cid:0) 1
(cid:1) rate we will use decreasing
K2
stepsizes and Y k will be deﬁned slightly diﬀerently, but except for this, it is going to be
the same Lyapunov function everywhere.

7.6.1 O( 1

K ) convergence for general convex problem
Theorem 7.6.1 (Proof in Appendix G.5.4). Assume f is L-smooth and µ-strongly convex,
g1, . . . , gm, ψ are proper, closed and convex. If we use a method for generating vk which
satisﬁes Assumption 7.5.1 and γ ≤ γmax, then

E (cid:2)Df (xK, x(cid:63))(cid:3) ≤

1
ωγK

L0,

where L0 def= (cid:107)x0 − x(cid:63)(cid:107)2 + M0 + (cid:80)m

l=1 η2

l (cid:107)y0

l − y(cid:63)

l (cid:107)2 and xK def= 1

K

(cid:80)k−1

l=0 xl.

If ψ ≡ 0 and gj ≡ 0 for all j, then this transforms into O( 1

E (cid:2)f (xK) − minx f (x)(cid:3), which is the correct rate of SGD.

K ) convergence of

The next result takes care of the case when SGD is used, which requires special

consideration.

118

Corollary 7.6.2. If we use SGD for k iterations with constant stepsize, the method
converges to a neighborhood of radius M0
If we choose the stepsize γ =
(cid:17)

(cid:17)

γk = 2γσ2
(cid:63).
(cid:16) 1√

), and we recover O

, then 2γσ2

(cid:63) = O( 1√
K

(cid:16) 1
√
L

K

Θ

rate.

K

7.6.2 O( 1

K 2 ) convergence for strongly convex f
In this section, we consider a variant of Algorithm 11 with time-varying stepsizes,

zk = proxγkψ(xk − γkvk − γkyk),

xk+1 = proxηk,j gj (zk + ηk,jyk

j ).

Theorem 7.6.3 (Proof in Appendix G.5.5). Consider updates with time-varying stepsizes,
γk =

for j = 1, . . . , m, where a ≥ 2 max

µω(a+k+1) and ηk,j = γk
mpj

. Then

(cid:110) 1

ωµγmax

, 1
ρ

(cid:111)

2

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

a2
(K + a − 1)2 L0,

where L0 = (cid:107)x0 − x(cid:63)(cid:107)2 + M0 + (cid:80)m

j=1 η2

0,j(cid:107)y0

j − y(cid:63)

j (cid:107)2.

This improves upon O( 1

matches the bound in [40].

K ) convergence proved in [61] under similar assumptions and

In Corollary 7.6.2 we obtained O( 1√
K

) rate for SGD with σ(cid:63) (cid:54)= 0. It is not surprising
that the rate is worse as it is so even with g ≡ 0. For standard SGD we are able to
improve the guarantee above to O( 1

K ) when the objective is strongly convex.

Theorem 7.6.4 (Proof in Appendix G.5.6). Assume f is µ-strongly convex, f (·; ξ) is
almost surely convex and L-smooth. Let the update be produced by SGD, i.e., vk =
∇f (xk; ξk), and let us use time-varying stepsizes γk−1 = 2

a+µk with a ≥ 4L. Then

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

8σ2
(cid:63)
µ(a + µK)

+

a2
(a + µK)2 L0.

7.6.3 Linear convergence for linear non-smoothness

We now provide two linear convergence rates in the case when ψ ≡ 0 and gj(x) =
φj(A(cid:62)

j x).

Theorem 7.6.5 (Proof in Appendix G.5.7). Assume that f is µ-strongly convex, ψ ≡ 0,
gj(x) = φj(A(cid:62)
j x) for j = 1, . . . , m and take a method satisfying Assumption 7.5.1 with
ρ > 0. Then, if γ ≤ γmax,

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ (1 − min{ρ, ωγµ, ρA})K L0,

where ρA

def= λmin(A(cid:62)A) minj

(cid:17)2

(cid:16) pj
(cid:107)Aj (cid:107)

, and L0 def= (cid:107)x0 −x(cid:63)(cid:107)2 +M0 +(cid:80)m

j=1 γ2

j (cid:107)y0

j −y(cid:63)

j (cid:107)2.

Corollary 7.6.6. If oracle from Algorithm 12 (SAGA) is used with probabilities pj ∝ (cid:107)Aj(cid:107),
then to get E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ ε, it is enough to run it for

119

(cid:18)(cid:18)

K = O

n +

L
µ

+

(cid:107)A(cid:107)2
λmin(A(cid:62)A)

2,1

(cid:19)

log

(cid:19)

1
ε

iterations.

Now let us show that this can be improved to depend only on positive eigenvalues if

the problem is linearly constrained.

Theorem 7.6.7 (Proof in Appendix G.5.8). Under the same assumptions as in Theo-
rem 7.6.5 and assuming, in addition, that gj = χ{x:A(cid:62)

j x=bj } it holds

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) ≤ (1 − min{ρ, ωγµ, ρA})kL0

with ρA = λ+
eigenvalue of A(cid:62)A.

min(A(cid:62)A) minj

(cid:17)2

(cid:16) pj
(cid:107)Aj (cid:107)

, i.e., ρA depends only on the smallest positive

One implication of Theorem 7.6.7 is that just by taking a solver such as SVRG we
immediately obtain a method for decentralized optimization that will converge linearly.
Furthermore, if the problem is ill-conditioned or the communication graph is well condi-
tioned, the leading term is still L
µ , meaning that the rate for decentralized method is the
same as for centralized up to constant factors. In Appendix G.5.8, we also give a version
of our method specialized to the linearly constrained problem that requires only one extra
vector, yk.

7.6.4 Linear convergence if all gj are smooth

Theorem 7.6.8 (Proof in Appendix G.5.9). Assume that f is L-smooth and µ-strongly
convex, gj is Lj-smooth for all j, Assumption 7.5.1(b) is satisﬁed and γ ≤ γmax. Then,
Algorithm 11 converges as

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

(cid:18)

(cid:26)

1 − min

ωγµ, ρ,

ν
m(1 + ν)

(cid:27)(cid:19)K

L0,

where ν def= minj=1,...,m

1
ηj Lj

.

Based on the theorem above, we suggest to choose probabilities pj to maximize ν,
If pj = Lj
= 1
(cid:80)m
γL

, then ν = minj=1,...,m

mpj
γLj

k=1 Lk

which can be done by using pj ∝ Lj.
with L def= 1
m

j=1 Lj.

(cid:80)m

Corollary 7.6.9 (Proof in Appendix G.5.10). Choose as solver for f SVRG or SAGA
without mini-batching, which satisfy Assumption 7.5.1 with γmax = 1
3n, and
def= Lg and p1 = · · · = pm. Deﬁne
consider for simplicity situation where L1 = · · · = Lm

5L and ρ = 1

120

def= (ωµmLg)− 1

γbest
to get E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ ε is

2 , and set the stepsize to γ = min{γmax, γbest}. Then the complexity

(cid:32)(cid:32)

O

n + m +

(cid:115)

mLg
µ

(cid:33)

log

(cid:33)

.

1
ε

L
µ

+

Notably, the rate in Corollary 7.6.9 is accelerated in g, suggesting that the proposed
update is in some cases optimal. Moreover, if m becomes large, the last term is dominating
everything else meaning that acceleration in f might not be needed at all.

7.7 Experiments

In this experiment, we ﬁrst generate a matrix
Randomly generated linear system.
with independent Gaussian entries of zero mean and scale 1√
, where d = 100, and after
d
that we set W ∈ Rd×d to be the product of the generated matrix with itself plus identity
matrix with coeﬃcient 10−2 to make sure W is positive deﬁnite. We also generated a
random vector x(cid:63) ∈ Rd and took b = Wx(cid:63). The problem is to solve Wx = b, or,
equivalently, to minimize (cid:107)Wx − b(cid:107)2. We made this choice because it makes estimation
of the parameters of accelerated Sketch-and-Project easier.

To run our method, we choose

and

f (x) =

1
2

(cid:107)x(cid:107)2

j = 1, . . . , d,

gj(x) = χ{x:w(cid:62)

j x=bj }(x),
j x=bj }(x) is the characteristic function, whose value is 0 if w(cid:62)
where χ{x : w(cid:62)
j x = bj and
+∞ otherwise. Then, the proximal operator of gj is the projection operator onto the
corresponding constraint. We found that the choice of stepsize is important for fast
convergence and that the value approximately equal 1.3 · 10−4 (cid:28) 1 = 2
(L+µ) led to the
best performance for this matrix.

We compare our method to the accelerated Sketch-and-Project method of [83] using
optimal parameters. The other method that we consider is classic Kaczmarz method that
projects onto randomly chosen constraint. We run all methods with uniform sampling.

Linear regression with linear constraints. We took A9a dataset from LIBSVM
and ran (cid:96)2-regularized linear regression, using ﬁrst 50 observations of the dataset as tough
constraints. We compare iteration complexity to precise projection onto all constraints and
observe that it takes almost the same number of iterations, although stochastic iterations
are signiﬁcantly cheaper. For each method we chose mini-batch of size 20 and stepsizes
of order 1

L for all methods.

More experiments are provided in Appendix G.6.

121

Figure 7.1: Left: convergence of the Stochastic Decoupling method, Kaczmarz and accel-
erated Kaczmarz of [83] when solving Wx = b with random positive-deﬁnite W ∈ Rd×d,
where d = 100. It is immediate to observe that the method we propose performs on a par
with the accelerated Sketch-and-Project. Right: linear regression with A9a dataset from
LIBSVM [44] with ﬁrst 50 observation used as linear constraints. We compare conver-
gence of SVRG, SAGA and SGD with full projections (labeled as ’SVRG’, ’SAGA’, ’SGD’)
to the same methods combined with Algorithm 11 (labeled as ’Double-’).

0100000200000300000400000500000Iteration101410121010108106104102100||Wx - b||KaczmarzAccelerated Sketch-and-ProjectDouble0255075100125150175200Data passes1010108106104102100||xx*||2Double-SAGASAGADouble-SVRGSVRGDouble-SGDSGD122

Chapter 8

Designing Variance-Reduced Algorithms with Splitting

8.1 Introduction

Many problems in statistics, machine learning or signal processing can be formulated
as high-dimensional convex optimization problems [197, 250, 10, 205, 42, 251]. These
optimization problems typically involve a smooth term F and a non-smooth regularization
G, and are often solved using a (variant of) the Proximal Stochastic Gradient Descent
(SGD) [8]. However, in many cases, G is not proximable, i.e., its proximity operator does
not admit a closed form expression.

In particular, structured regularizations [42, 57] like the total variation regularization
over a graph [53, 70, 33, 274] or the overlapping group Lasso [10] are known to have
an expensive proximity operators [232]. Another example is the case of linear constraints
on the optimization problem. This corresponds to G being an indicator function and the
proximity operator of G being the projection onto the constraints space. This projection
requires the resolution of a high-dimensional linear system [15] often intractable. The
context of decentralized optimization [281], in which a network of computing agents
aims at jointly minimizing an objective function by performing local computations and
exchanging information along the edges, is a particular case of the context of linearly
constrained optimization.
In this particular case, projecting onto the constraints space
is equivalent to averaging across the network, which is prohibited. Finally, when G is a
sum of several regularizers, G is not proximable even if the regularizers are proximable,
because the proximity operator is not linear.

Although in these examples G is not proximable, G takes the form G = ψ + H ◦ L
where ψ, H are proximable and L is a linear operator1. Therefore, in this chapter we
study the problem

min
x∈X

f (x) + ψ(x) + H(Lx),

(8.1)

where X is a real Hilbert space, f is a smooth convex function, ψ, H are convex, possibly
non-smooth, functions and L is a linear operator. To solve Problem (8.1), we recast it
as ﬁnding a zero of the sum of three operators which are monotone in a primal–dual
product space, under a metric depending on the problem. Then, we apply Davis–Yin
Splitting (DYS) [60], a generic method for this type of monotone inclusions. This way, we
recover existing algorithms but we also discover a new one, which we call the Primal–Dual
Davis–Yin (PDDY) algorithm. Moreover, this DYS representation of our algorithms allow
us to use an important inequality regarding DYS for their analysis. More precisely, we

1In these contexts, H ◦ L is not proximable as well (the symbol ◦ stands for the composition of

functions).

123

can apply Lemma 8.4.2 below by instanciating the monotone operators and replacing the
inner product by the inner product under which we apply DYS. Thanks to this, we can
afterwards painlessly replace the gradient ∇f by a stochastic variance-reduced estimator
(see Section 8.6), which can be much cheaper to evaluate. This machinery allows us to
obtain new convergence rates for the stochastic primal–dual algorithms studied in this
chapter, and opens the door to a new class of randomized proximal algorithms for large-
scale convex non-smooth optimization.

8.2 Related Work

Splitting algorithms: Algorithms allowing to minimize a function involving several non-
smooth proximable terms are called splitting algorithms. At the core of splitting algorithms
is the Douglas–Rachford (or ADMM) algorithm [145, 78] which is, under reasonable
assumptions, the only splitting algorithm that can minimize the sum of two non-smooth
functions ψ + H [227]. To minimize G = ψ + H ◦ L, the Douglas–Rachford algorithm
can be generalized to the Primal–Dual Hybrid Gradient (PDHG) algorithm, also called
Chambolle–Pock algorithm [41]. Behind the success of PDHG is the ability to handle
such a composite function G and hence the regularizations mentioned above. However, in
signal processing and machine learning applications, the objective function usually involves
a smooth data ﬁtting term f . In order to cover these applications, splitting algorithms like
Condat–V˜u [52, 270] and PD3O [282] were proposed to solve the Problem (8.1). These
algorithms are primal–dual in nature, i.e., their iterates take the form (xk, yk) ∈ X × Y,
where Y is another real Hilbert space, xk converges to a solution of Problem (8.1) and
yk converges to a solution of a dual of Problem (8.1). All Hilbert spaces are supposed of
ﬁnite dimension.

DYS and monotone operators: The notion of monotone operator [15] generalizes the
notion of subdiﬀerential of a convex function. The problem of ﬁnding a zero of a monotone
operator ﬁnds many applications in optimization, beyond the case of a subdiﬀerential.
Davis-Yin Splitting [60] is a method for ﬁnding a zero of the sum of three monotone
operators. When the three monotone operators are subdiﬀerentials, DYS boils down
to an optimization algorithm for solving Problem (8.1) with L = I. When one of the
three monotone operators is equal to zero, the DYS boils down to the Forward–Backward
Splitting (FBS). If the two monotone operators of the FBS are subdiﬀerentials, the FBS
boils down to the standard proximal gradient algorithm. But the FBS goes beyond the case
of subdiﬀerentials. For instance, the Condat–V˜u algorithms can be seen as instances of the
FBS involving monotone operators which are not subdiﬀerentials. The FBS representation
of the Condat–V˜u algorithms is the cornerstone of their analysis in [52].

Stochastic splitting algorithms:

In machine learning applications, the gradient of f
is often intractable and replaced by a cheaper stochastic gradients. These stochastic
gradients can be classiﬁed in two classes: variance reduced (VR) stochastic gradients [108,
62, 61, 81] and generic stochastic gradients, see, e.g., [177, 130]. VR stochastic gradients
are stochastic gradient estimators of the full gradient that ensure convergence to an
exact solution, as for deterministic algorithms. The variance reduction allows to speedup
stochastic algorithms and eventually recover the convergence rates of their deterministic

124

In the case where L = I, Problem (8.1) was considered with generic
counterparts.
stochastic gradients in [289] and with VR stochastic gradients in [202].
In the general
case L (cid:54)= I that is of interest in this chapter, the resolution of (8.1) was considered with
a generic stochastic gradient in [296].

Contributions and technical challenges.
In this chapter we consider the resolution
of Problem (8.1) with VR stochastic gradients, and make several contributions w.r.t. the
understanding of primal–dual algorithms.

We ﬁrst propose a new algorithm called Primal–Dual Davis–Yin (PDDY) to solve (8.1).
This algorithm is obtained as a carefully designed instance of the DYS involving operators
which are monotone under a metric depending on L. This DYS representation allows us
to prove convergence rates for PDDY, a task which would be lengthy and technical if such
a representation was not obtained prior to proving the convergence rates. More precisely,
we analyze PDDY with a deterministic gradient and with a variance reduced stochastic
gradient. Both settings are cast into a single assumption which can be elegantly plugged
into our analysis of PDDY, thanks to the ﬂexibility of our framework.

We believe that representing primal–dual algorithms as ”simpler” algorithms using
monotone operators in a primal–dual product space is important for understanding the
primal–dual algorithms, as illustrated above for PDDY. Therefore, our second contribution
is to show how the Condat–V˜u algorithms and the PD3O algorithm can be viewed as
instances of the DYS involving operators which are monotone under a metric depending
on L2. Such representation was not known for the Condat–V˜u algorithms. Finally, we
use again this DYS representation to study the PD3O algorithm with a VR stochastic
gradient.

One byproduct of our results is the discovery of one of the ﬁrst linearly converging
algorithm for the minimization of a smooth strongly convex function under linear con-
straints [174].
In the particular case where a full gradient is used and L is a gossip
matrix [281], this algorithm leads to a decentralized algorithm whose complexity com-
petes with optimization algorithms designed speciﬁcally for the decentralized optimization
problem, see [281].

In summary, our contributions are the following:

• We propose a new primal–dual algorithm called PDDY to solve Problem (8.1)

• We propose new generalizations of PDDY and PD3O using a VR stochastic gradient.
These are the ﬁrst variance reduced algorithms to tackle Problem (8.1). We leverage
the DYS representation of PDDY and PD3O to prove convergence rates which are
faster than the convergence rates of non variance reduced algorithms like [296].

• We propose a new stochastic algorithm for the smooth strongly convex decentralized
optimization problem and prove convergence rates for this algorithm. In the particu-
lar case of full gradients, its convergence rates is competitive with the decentralized
algorithms listed in the Table 1 of [281].

2These monotone operators are not subdiﬀerentials in general.

125

• We show that the Condat–V˜u algorithms are instances of the DYS involving mono-

tone operators under a new metric.

The choice of studying stochastic versions of PDDY and PD3O instead of the Condat–
V˜u algorithms is motivated by the fact that PDDY and PD3O allow for larger stepsizes.
Stochastic versions of the Condat–V˜u algorithms could also be analyzed within our frame-
work.

The remainder is organized as follows. In the next section we present the primal–dual
formulation of Problem (8.1), and the associated monotone operators. In Section 8.4 we
recall the DYS along with an important inequality regarding its analysis. Then, in Sec-
tion 8.5 we present the four primal–dual algorithms studied in this chapter and their DYS
representation. Their stochastic versions and associated convergence rates are presented
in Section 8.6. The numerical experiments, the convergence proofs and the decentralized
application of our algorithms are postponed to the appendix.

8.3 Primal–Dual Formulations and Optimality Conditions

The necessary notions and notations of convex analysis and operator theory are introduced
in Chapter 1. Let X and Y be ﬁnite-dimensional real Hilbert spaces, L : X → Y be a
linear operator, f, ψ and H be convex, closed and proper. We assume that f is ν-
smooth, for some ν > 0. We assume, as usual, that there exists x(cid:63) ∈ X such that
0 ∈ ∇f (x(cid:63))+∂ψ(x(cid:63))+L∗∂H(Lx(cid:63)). Then x(cid:63) is solution to (8.1). For instance, a standard
qualiﬁcation constraint for this condition to hold is that 0 belongs to the relative interior of
dom (H) − Ldom (ψ) [50]. Therefore, there exists y(cid:63) ∈ Y such that (x(cid:63), y(cid:63)) ∈ zer(M ),
where M is the set-valued operator deﬁned by

M (x, y) def=

(cid:20)∇f (x) + ∂ψ(x) + L∗y

(cid:21)
+ ∂H ∗(y)

−Lx

.

(8.2)

In other words, there exist r(cid:63) ∈ ∂ψ(x(cid:63)) and h(cid:63) ∈ ∂H ∗(y(cid:63)) such that

(cid:21)
(cid:20)0
0

=

(cid:20)∇f (x(cid:63)) + r(cid:63) + L∗y(cid:63)

(cid:21)

−Lx(cid:63)

+ h(cid:63)

.

(8.3)

Conversely, for every solution (x(cid:63), y(cid:63)) ∈ zer(M ), x(cid:63) is a solution to (8.1). In the sequel,
we let (x(cid:63), y(cid:63)) ∈ zer(M ) and r(cid:63), h(cid:63) be any elements such that Equation (8.3) holds.

The inclusion (8.3) characterizes the ﬁrst-order optimality conditions associated with

the convex–concave Lagrangian function deﬁned as

L(x, y) def= (f + ψ)(x) − H ∗(y) + (cid:104)Lx, y(cid:105).

(8.4)

For every x ∈ X , y ∈ Y, we deﬁne the duality gap at (x, y) as L(x, y(cid:63)) − L(x(cid:63), y). Then

Lemma 8.3.1 (Duality gap). For every x ∈ X , y ∈ Y, we have

L(x, y(cid:63)) − L(x(cid:63), y) = Df (x, x(cid:63)) + Dψ(x, x(cid:63)) + DH ∗(y, y(cid:63)),

(8.5)

126

where the Bregman divergence of the smooth function f between any two points x, x is
Df (x, x(cid:48)) def= f (x)−f (x(cid:48))−(cid:104)∇f (x(cid:48)), x−x(cid:48)(cid:105), and Dψ(x, x(cid:63)) def= ψ(x)−ψ(x(cid:63))−(cid:104)r(cid:63), x−x(cid:63)(cid:105),
DH ∗(y, y(cid:63)) def= H ∗(y) − H ∗(y(cid:63)) − (cid:104)h(cid:63), y − y(cid:63)(cid:105).

For every x ∈ X , y ∈ Y, Lemma 8.3.1 and the convexity of f, ψ, H ∗ imply that

L(x(cid:63), y) ≤ L(x(cid:63), y(cid:63)) ≤ L(x, y(cid:63)).

(8.6)

So, the duality gap L(x, y(cid:63)) − L(x(cid:63), y) is nonnegative, and it is zero if x is a solution to
Problem (8.1) and y is a solution to the dual problem miny∈Y(f + ψ)∗(−L∗y) + H ∗(y),
see Section 15.3 of [15]. The converse is true under mild assumptions, for instance strict
convexity of the functions around x(cid:63) and y(cid:63).

Finally, one can check that the operator M deﬁned in (8.2) is monotone. Moreover,

we have

M (x, y) =

(cid:21)

(cid:20)∂ψ(x)
0

(cid:20)

=

0
∂H ∗(y)

(cid:20)

+

(cid:21)

+

L∗y

(cid:21)
−Lx + ∂H ∗(y)
(cid:21)
(cid:20) ∂ψ(x) + L∗y
−Lx

(cid:21)

(cid:20)∇f (x)
0
(cid:21)
(cid:20)∇f (x)
0

+

+

(8.7)

(8.8)

and each term at the right hand side of (8.7) or (8.8) is maximal monotone, see Corollary
25.5 in [15].

8.4 Davis–Yin Splitting

Solving the optimization problem (8.1) boils down to ﬁnding a zero (x(cid:63), y(cid:63)) of the mono-
tone operator M deﬁned in (8.2), which can be written as the sum of three monotone
operators, like in (8.7) or (8.8). The Davis–Yin Splitting (DYS) algorithm [60], is dedi-
cated to this problem; that is, ﬁnd a zero of the sum of three monotone operators, one
of which is cocoercive.

Let Z be a real Hilbert space. Let ˜A, ˜B, ˜C be maximal monotone operators on Z.
We assume that ˜C is ξ-cocoercive, for some ξ > 0. The DYS algorithm, denoted by
DY S( ˜A, ˜B, ˜C) and shown above, aims at ﬁnding an element in zer( ˜A + ˜B + ˜C) (cid:54)= ∅.
The ﬁxed points of DY S( ˜A, ˜B, ˜C) are the triplets (v(cid:63), z(cid:63), u(cid:63)) ∈ Z 3, such that

z(cid:63) = Jγ ˜B(v(cid:63)),

u(cid:63) = Jγ ˜A

(cid:0)2z(cid:63) − v(cid:63) − γ ˜C(z(cid:63))(cid:1),

u(cid:63) = z(cid:63).

(8.9)

These ﬁxed points are related to the zeros of ˜A+ ˜B + ˜C as follows, see Lemma 2.2 in [60]:
for every (v(cid:63), z(cid:63), u(cid:63)) ∈ Z 3 satisfying (8.9), z(cid:63) ∈ zer( ˜A + ˜B + ˜C). Conversely, for every
z(cid:63) ∈ zer( ˜A + ˜B + ˜C), there exists (v(cid:63), u(cid:63)) ∈ Z 2, such that (v(cid:63), z(cid:63), u(cid:63)) satisﬁes (8.9).
We have [60]:

Lemma 8.4.1 (Convergence of the DYS algorithm). Suppose that γ ∈ (0, 2ξ). Then the
sequences (vk)k, (zk)k, (uk)k generated by DY S( ˜A, ˜B, ˜C) converge to some elements
v(cid:63), z(cid:63), u(cid:63) in Z, respectively. Moreover, (v(cid:63), z(cid:63), u(cid:63)) satisﬁes (8.9) and u(cid:63) = z(cid:63) ∈ zer( ˜A +
˜B + ˜C).

127

Davis–Yin Splitting DYS( ˜A, ˜B, ˜C) [60]

1: Input: v0 ∈ Z, γ > 0, number of

steps K

2: for k = 0, 1, 2, . . . , K − 1 do
3:

zk = Jγ ˜B(vk)
uk+1 = Jγ ˜A(2zk − vk − γ ˜C(zk))
vk+1 = vk + uk+1 − zk

4:

5:
6: end for

PriLiCoSGD
(cid:0)deterministic version: gk+1 = ∇f (xk)(cid:1)
1: Input: x0 ∈ X , γ > 0, τ > 0, number

of steps K

2: for k = 0, 1, 2, . . . , K − 1 do
tk+1 = xk − γgk+1
3:
ak+1 = ak + τW(tk+1 − γak) − τ c
4:
xk+1 = tk+1 − γak+1
5:
6: end for

Stochastic PDDY
(cid:0)deterministic version: gk+1 = ∇f (xk)(cid:1)
1: Input: p0 ∈ X , y0 ∈ Y, γ > 0, τ > 0,

Stochastic PD3O
(cid:0)deterministic version: gk+1 = ∇f (xk)(cid:1)
1: Input: p0 ∈ X , y0 ∈ Y, γ > 0, τ > 0,

number of steps K

number of steps K

2: for k = 0, 1, 2, . . . , K − 1 do
yk+1 = proxτ H ∗
3:

(cid:0)yk + τ L(pk −

γL∗yk)(cid:1)

xk = pk − γL∗yk+1
sk+1 = proxγψ
pk+1 = pk + sk+1 − xk

4:
5:
6:
7: end for

(cid:0)2xk − pk − γgk+1(cid:1)

2: for k = 0, 1, 2, . . . , K − 1 do
xk = proxγψ(pk)
3:
wk = 2xk − pk − γgk+1
4:
yk+1 = proxτ H ∗
5:

(cid:0)yk + τ L(wk −

γL∗yk)(cid:1)

pk+1 = xk − γgk+1 − γL∗yk+1

6:
7: end for

The following equality, proved in the Appendix, is at the heart of the convergence

proofs:

Lemma 8.4.2 (Fundamental equality of the DYS algorithm). Let (vk, zk, uk) ∈ Z 3 be
the iterates of the DYS algorithm, and (v(cid:63), z(cid:63), u(cid:63)) ∈ Z 3 be such that (8.9) holds. Then,
for every k ≥ 0, there exist bk ∈ ˜B(zk), b(cid:63) ∈ ˜B(z(cid:63)), ak+1 ∈ ˜A(uk+1) and a(cid:63) ∈ ˜A(u(cid:63))
such that

(cid:107)vk+1 − v(cid:63)(cid:107)2 = (cid:107)vk − v(cid:63)(cid:107)2 − 2γ(cid:104)bk − b(cid:63), zk − z(cid:63)(cid:105) − 2γ(cid:104) ˜C(zk) − ˜C(z(cid:63)), zk − z(cid:63)(cid:105)

(8.10)
− 2γ(cid:104)ak+1 − a(cid:63), uk+1 − u(cid:63)(cid:105) − γ2(cid:107)ak+1 + bk − (a(cid:63) + b(cid:63)) (cid:107)2 + γ2(cid:107) ˜C(zk) − ˜C(z(cid:63))(cid:107)2.

8.5 Primal–Dual Optimization Algorithms

We now set Z def= X × Y, where X and Y are the spaces deﬁned in Sect. 8.3. To
solve the primal–dual problem (8.7) or (8.8), which consists in ﬁnding a zero of the
sum A + B + C of 3 operators in Z, of which C is cocoercive, a natural idea is to
apply the Davis–Yin algorithm DYS(A, B, C). But the resolvent of ˜A or ˜B is often
intractable. In this section, we show that preconditioning is the solution; that is, we exhibit
a positive deﬁnite linear operator P, such that DYS(P−1A, P−1B, P−1C) is tractable.

128

Since P−1A, P−1B, P−1C are monotone operators in ZP, the algorithm will converge to
a zero of P−1A + P−1B + P−1C, or, equivalently, of A + B + C.

Let us apply this idea in four diﬀerent ways.

8.5.1 A new primal–dual algorithm: the PDDY algorithm

Let γ > 0 and τ > 0 be real parameters. We introduce the four operators on Z, using
matrix-vector notations:

A(x, y) =

C(x, y) =

(cid:21)

(cid:20)
L∗y
−Lx + ∂H ∗(y)
(cid:20)I
(cid:21)
(cid:20)∇f (x)
0 γ
0

, P =

, B(x, y) =

(cid:21)
(cid:20)∂ψ(x)
0

,

0
τ I − γ2LL∗

(cid:21)

.

(8.11)

Note that P is positive deﬁnite if and only if γτ (cid:107)L(cid:107)2 < 1. Since A, B, C are maximal
monotone in Z, P−1A, P−1B, P−1C are maximal monotone in ZP. Moreover, P−1C is
1/ν-cocoercive in ZP. Importantly, we have:

P−1C : (x, y) (cid:55)→ (cid:0)∇f (x), 0(cid:1),

JγP−1B : (x, y) (cid:55)→ (cid:0)proxγψ(x), y(cid:1),

JγP−1A : (x, y) (cid:55)→ (x(cid:48), y(cid:48)), where

(cid:22) y(cid:48) = proxτ H ∗

(cid:0)y + τ L(x − γL∗y)(cid:1)

x(cid:48) = x − γL∗y(cid:48).

(8.12)

(8.13)

We plug these explicit steps into the Davis–Yin algorithm (P−1B, P−1A, P−1C) and
we identify the variables as vk = (pk, qk), zk = (xk, yk), uk = (sk, hk). After some
simpliﬁcations, we obtain the new Primal–Dual Davis–Yin (PDDY) algorithm, shown
above, for Problem (8.1). Note that it can be written with only one call to L and L∗ per
iteration. Also, the PDDY algorithm can be overrelaxed [54], since this possibility exists
for the Davis–Yin algorithm. We have:

Theorem 8.5.1 (Convergence of the PDDY algorithm). Suppose that γ ∈ (0, 2/ν) and
that τ γ(cid:107)L(cid:107)2 < 1. Then the sequences (xk)k and (sk)k (resp. the sequence (qk)k∈N)
generated by the PDDY algorithm converge to some solution x(cid:63) to Problem (8.1) (resp.
some y(cid:63) ∈ arg min(f + ψ)∗ ◦ (−L∗) + H ∗).

Proof. Under the assumptions of Theorem 8.5.1, P is positive deﬁnite. Then the result
(cid:4)
follows from Lemma 8.4.1 applied in ZP and from the analysis in Sect. 8.3.

Whether the PDDY algorithm is a good alternative in practice to the PD3O or the
Condat–V˜u algorithm, which solve the same problems, should be considered on a case-
by-case basis. In particular, their memory requirements can be diﬀerent, and depend on
which operations are performed in-place.

8.5.2 The PD3O algorithm

We consider the same notations as in the previous section. We switch the roles of A and
B and consider DYS(P−1A, P−1B, P−1C). Then we recover exactly the PD3O algorithm
proposed in [282], shown above. Although it is not derived this way, its interpretation as

129

a primal–dual Davis–Yin algorithm is mentioned by its author. Its convergence properties
are the same as for the PDDY algorithm, as stated in Theorem 8.5.1.

We can note that in a recent work [195], the PD3O algorithm has been shown to
be an instance of the Davis–Yin algorithm, with a diﬀerent reformulation, which does
not involve duality. Whether this connection could yield diﬀerent insights on the PD3O
algorithm is left for future investigation.

8.5.3 The Condat–V˜u algorithm

Let γ > 0 and τ > 0 be real parameters. We want to study the decomposition (8.8)
instead of (8.7). For this, we deﬁne the operators

¯A(x, y) =

(cid:21)
, ¯B(x, y) =

(cid:20) ∂ψ(x) + L∗y
−Lx

(cid:21)
(cid:20)K 0
,
I
0
(8.14)
where K def= γ
In that case,
since ¯A, ¯B, C are maximal monotone in Z = X × Y, Q−1 ¯A, Q−1 ¯B, Q−1C are maximal
monotone in ZQ. Moreover, we have:

If γτ (cid:107)L(cid:107)2 < 1, K and Q are positive deﬁnite.

(cid:21)
(cid:20)
0
∂H ∗(y)

(cid:21)
(cid:20)∇f (x)
0

τ I − γ2L∗L.

, C(x, y) =

, Q =

Q−1C : (x, y) (cid:55)→ (cid:0)K−1∇f (x), 0(cid:1),

JγQ−1 ¯B : (x, y) (cid:55)→ (cid:0)x, proxγH ∗(y)(cid:1),

(8.15)

JγQ−1 ¯A : (x, y) (cid:55)→ (x(cid:48), y(cid:48)), where

(cid:22) x(cid:48) = proxτ ψ

(cid:0)(I − τ γL∗L)x − τ L∗y(cid:1)

y(cid:48) = y + γLx(cid:48).

(8.16)

As proved in the Appendix, if we plug these explicit steps into the Davis–Yin algorithm
DYS(Q−1 ¯A, Q−1 ¯B, Q−1C) or DYS(Q−1 ¯B, Q−1 ¯A, Q−1C), we recover the two forms of
the Condat–V˜u algorithm [52, 270]; that is, Algorithms 3.1 and 3.2 of [52], respectively.
The Condat–V˜u algorithm has the form of a primal–dual forward–backward algorithm [101,
47, 121, 54]. But we have just seen that it can be viewed as a primal–dual Davis–Yin
algorithm, with a diﬀerent metric, as well. Hence, convergence follows from Lemma 8.4.1;
a technical point is to determine the value of ξ, the cocoercivity constant of Q−1C in
ZQ. We prove in the Appendix that we recover the same conditions on τ and γ as in
Theorem 3.1 of [52].

8.6 Stochastic Primal–Dual Algorithms: Non-Asymptotic Analy-

sis

We now introduce stochastic versions of the PD3O and PDDY algorithms; we omit the
analysis of the stochastic version of the Condat–V˜u algorithm, which is the same, with
added technicalities due to cocoercivity with respect to the metric induced by Q in (8.14).
Moreover, linear convergence results under stronger assumptions are deferred to the Ap-
pendix. Our approach has a ‘plug-and-play’ ﬂavor: we show that we have all the ingredi-
ents to leverage the uniﬁed theory of stochastic gradient estimators recently presented in
[81].

130

In the stochastic versions of the algorithms, the gradient ∇f (xk) is replaced by
a stochastic gradient gk+1. More precisely, we consider a ﬁltered probability space
(Ω, F , (Fk)k, P), an (Fk)k-adapted stochastic process (gk)k, we denote by E the math-
ematical expectation and by Ek the conditional expectation w.r.t. Fk. The following
assumption is made on the process (gk)k.

Assumption 8.6.1. There exist α, β, δ ≥ 0, ρ ∈ (0, 1] and a (Fk)k-adapted stochastic
process denoted by (σk)k, such that, for every k ∈ N

Ek[gk+1] = ∇f (xk),
Ek[(cid:107)gk+1 − ∇f (x(cid:63))(cid:107)2] ≤ 2αDf (xk, x(cid:63)) + βσ2
Ek[σ2

k + 2δDf (xk, x(cid:63)).

k+1] ≤ (1 − ρ)σ2

k ,

Assumption 8.6.1 is a consequence of the smoothness of f and the choice of the
stochastic gradient estimator, see [81]. Assumption 8.6.1 is satisﬁed by several stochastic
gradient estimators used in machine learning, including some kinds of coordinate de-
scent [95], variance reduction [62, 102, 86, 126], and also compressed gradients used
to reduce the communication cost in distributed optimization [103], see Table 1 in [81].
Also, the full gradient estimator deﬁned by gk+1 = ∇f (xk) satisﬁes Assumption 8.6.1
with α = ν, the smoothness constant of f , σk ≡ 0, ρ = 1, and δ = β = 0, see Theorem
2.1.5 in [187]. The Loopless SVRG estimator [102, 126] also satisﬁes Assumption 8.6.1.

(cid:80)n

Proposition 8.6.2 (Loopless SVRG estimator). Assume that f is written as a ﬁnite sum
i=1 fi, where for every i ∈ {1, . . . , n}, fi : X → R is a νi-smooth convex
f = 1
n
function. Let p ∈ (0, 1), and (Ω, F , P) be a probability space. On (Ω, F , P), consider:
• a sequence of i.i.d. random variables (θk)k with Bernoulli distribution of parameter p,
• a sequence of i.i.d. random variables (ξk)k with uniform distribution over {1, . . . , n},
• the sigma-ﬁeld Fk generated by (θk, ξk)0≤j≤k and a (Fk)k-adapted stochastic process
(xk)k,
• a stochastic process (˜xk)k deﬁned by ˜xk+1 = θk+1xk + (1 − θk+1)˜xk,
• a stochastic process (gk)k deﬁned by gk+1 = ∇f (xk; ξk+1) − ∇f (˜xk; ξk+1) + ∇f (˜xk).
Then, the process (gk)k satisﬁes Assumption 8.6.1 with α = 2 maxi∈{1,...,n} νi, β = 2,

ρ = p, δ = αp/2, and

σ2
k =

1
n

n
(cid:88)

i=1

Ek

(cid:2)(cid:107)∇fi(˜xk) − ∇fi(x(cid:63))(cid:107)2(cid:3) .

Proof. The proof is the same as the proof of Lemma A.11 of [81]. Although this Lemma
is only stated for (xk) generated by a speciﬁc algorithm, it remains true for any (Fk)-
(cid:4)
adapted stochastic process (xk).

We can now exhibit our main results; the details are provided in the Appendix. In a
nutshell, P−1C(zk) is replaced by the stochastic outcome P−1(gk+1, 0) and the last term
of Equation (8.10), which is nonnegative, is handled using Assumption 8.6.1.

8.6.1 The Stochastic PD3O algorithm

131

We denote by (cid:107) · (cid:107)P the norm induced by P on Z. The Stochastic PD3O algorithm,
(cid:1) ergodic convergence in the general case:
shown above, has O (cid:0) 1

K

Theorem 8.6.3 (Convergence of the Stochastic PD3O algorithm). Suppose that As-
sumption 8.6.1 holds. Let κ def= β/ρ, γ, τ > 0 be such that γ ≤ 1/2(α + κδ) and
γτ (cid:107)L(cid:107)2 < 1. Set V 0 def= (cid:107)v0 − v(cid:63)(cid:107)2

0, where v0 = (p0, y0). Then,

P + γ2κσ2

E (cid:2)L(¯xK, y(cid:63)) − L(x(cid:63), ¯yK+1)(cid:3) ≤

V 0
Kγ

,

where ¯xK = 1
K

(cid:80)K−1

j=0 xj and ¯yK+1 = 1

K

(cid:80)K

j=1 yj.

In the deterministic case gk+1 = ∇f (xk), we recover the same rate as in [282, Theorem

2].

Remark 1 (Primal–Dual gap). Deriving a similar bound on the stronger primal–dual gap
(f + ψ + H ◦ L)(¯xk) + ((f + ψ)∗ ◦ (−L) + H ∗)(¯yk) requires additional assumptions;
for instance, even for the Chambolle–Pock algorithm, which is the particular case of
the PD3O, PPDY and Condat–V˜u algorithm when f ≡ 0, the best available result [43,
Theorem 1] is not stronger than Theorem 8.6.3

Remark 2 (Particular case of SGD). In the case where H = 0 and L = 0, the Stochastic
PD3O algorithm boils down to Proximal Stochastic Gradient Descent (Proximal SGD)
and Theorem 8.6.3 implies that

E (cid:2)(f + ψ)(¯xK) − (f + ψ)(x(cid:63))(cid:3) ≤

V 0
Kγ

.

This O (cid:0) 1
(cid:1) ergodic convergence rate uniﬁes known results on SGD in the non-strongly-
convex case, where the stochastic gradient satisﬁes Assumption 8.6.1. This covers coor-
dinate descent and variance-reduced versions, as discussed previously.

K

8.6.2 The algorithm

We now analyze the proposed Stochastic PDDY algorithm, shown above. For it too, we
have O (cid:0) 1

(cid:1) ergodic convergence in the general case:

K

Theorem 8.6.4 (Convergence of the Stochastic PDDY algorithm). Suppose that As-
sumption 8.6.1 holds. Let κ def= β/ρ, γ, τ > 0 be such that γ ≤ 1/2(α + κδ) and
γτ (cid:107)L(cid:107)2 < 1. Deﬁne V 0 def= (cid:107)v0 − v(cid:63)(cid:107)2

0, where v0 = (p0, y0). Then,

P + γ2κσ2

E (cid:2)Df (¯xK, x(cid:63)) + DH ∗(¯yK+1, y(cid:63)) + Dψ(¯sK+1, s(cid:63))(cid:3) ≤

V 0
Kγ

,

where ¯xK = 1
K

(cid:80)K−1

j=0 xj, ¯yK+1 = 1

K

(cid:80)K

j=1 yj and ¯sK+1 = 1

K

(cid:80)K

j=1 sj.

132

8.6.3 Linearly constrained or decentralized optimization

In this section, we set ψ = 0 and H : y (cid:55)→ (0 if y = b, +∞ else), for some b ∈ ran(L).
In this case, Problem (8.1) boils down to minx f (x) s.t. Lx = b. The stochastic PD3O
and PDDY algorithms both revert to the same algorithm, shown in the Appendix, which
we call Linearly Constrained Stochastic Gradient Descent (LiCoSGD). Note that it is fully
split: it does not make use of projections onto the aﬃne space {x ∈ X , Lx = b} and only
makes calls to L and L∗.

Theorem 8.6.5 (Linear convergence of LiCoSGD with f strongly convex). Suppose that
Assumption 8.6.1 holds, that f is µf -strongly convex, for some µf > 0, and that y0 ∈
Range (L). Let y(cid:63) be the unique element of Range (L) such that ∇f (x(cid:63)) + L∗y(cid:63) = 0,
and λ+
min(L∗L) > 0 be the smallest positive eigenvalue of L∗L. For every κ > β/ρ and
every γ, τ > 0 such that γ ≤ 1
α+κδ and γτ (cid:107)L(cid:107)2 < 1, we deﬁne

V k def= (cid:107)xk − x(cid:63)(cid:107)2 + (cid:0)1 + τ γλ+

min(L∗L)(cid:1) (cid:107)yk − y(cid:63)(cid:107)2

γ,τ + κγ2E (cid:2)σ2

k

(cid:3) ,

(8.17)

and

(cid:18)

r def= max

1 − γµf , 1 − ρ +

β
κ

,

1
1 + τ γλ+
min(L∗L)

(cid:19)

< 1.

Then, for every k ≥ 0,

E (cid:2)V k(cid:3) ≤ rkV 0.

(8.18)

(8.19)

(cid:80)M

Furthermore, LiCoSGD can be written using W = L∗L, c = L∗b and primal variables
in X only; this version, called PriLiCoSGD, is shown above. Now, consider that f =
1
m=1 fm is a ﬁnite sum of functions, that W is a gossip matrix of a network with
M
M nodes [281], and that c = 0. We obtain a new decentralized algorithm, shown and
discussed in the Appendix. Theorem 8.6.5 applies and shows that, with the full gradient, ε-
(cid:1) iterations, where κ is the condition number
accuracy is reached after O (cid:0)(κ + κW) log 1
of f and κW = (cid:107)W(cid:107)
. This rate is better or equivalent to the one of recently proposed
decentralized algorithms, like EXTRA, DIGing, NIDS, NEXT, Harness, Exact Diﬀusion,
see Table 1 of [281], [138, Theorem 1] and [4]. With a stochastic gradient, the rate of
our algorithm is also better than [176, Equation 99].

λ+
min(W)

ε

133

Chapter 9

Concluding Remarks

In this concluding chapter, we summarize the obtained results and outline potential future
directions and open problems.

9.1 Summary

In this thesis, we have addressed several issues arising when applying optimization to
machine learning problems. Our particular interest was in stochastic ﬁrst-order methods
that scale best on problems where both the dimension and the number of data samples
are large.

In Chapter 2, we established several upper bounds for Local SGD under diﬀerent
settings of the noise and data heterogeneity. As a follow-up work of Woodworth et
al. [277] showed, our results are optimal in certain regimes, although there are some
remaining gaps. Many other works tried to reﬁne (Woodworth et al., [277]) or build on
top of our results. For instance, Karimireddy et al. [113] proposed a variance-reduction
technique to tackle data heterogeneity, and Malinovskiy et al. [156] designed new methods
based on ﬁxed-point iterations.

To improve the algorithms proposed in Chapter 2, in Chapter 3 we obtained tight
results for Random Reshuﬄing, and then obtained a local RR algorithm in Chapter 4.
Moreover, our results in Chapter 3 are of independent interest as they close a number of
open questions on convergence of incremental methods that date back to the eighties.

In Chapters 7 and 8, we developed new variance-reduction and splitting methods for
structured optimization. Our algorithm from both chapters were subsequently shown by
Salim et al. [233] to be optimal in terms of the required number of matrix-vector multi-
plications when the conditioning of the smooth objective, κ = L
µ , and of the constraints,
κA = λmax(A)
, are of the same order, i.e., κ = Θ(κA). To the best of our knowledge,
λ+
min(A)
the results from these two chapters were also the ﬁrst ones to show that one can obtain
linear rates of convergence for arbitrary problems with linear constraints, provided that the
objective is smooth and strongly convex. The prior literature provided such guarantees for
the case f (x) = 1
2(cid:107)x − x0(cid:107)2, while we only require f to be upper- and lower-bounded by
a quadratic. Our methods also generalize many classic splitting techniques and stochastic
algorithms, such as the Kaczmarz method, PDDY, PDHG and Condat–V˜u algorithm.

In Chapter 5, we addressed a question that is ubiquitous in optimization: stepsize
In particular, we provided the ﬁrst stepsize rule for gradient descent that
estimation.
converges with O (cid:0) 1
(cid:1) rate on any convex problem that is locally smooth. Not only
it does not require knowledge of the smoothness constant, it does not need any other

k

134

parameters either, which is in contrast to all other stepsize rules for gradient descent.
Unlike the classical line search strategies, our stepsize does not require any subroutines
and can adapt to the local curvature arbitrarily many times.

In Chapter 6, we proposed the ﬁrst technique for learning the gradients under quan-
tized communication in distributed learning.
In particular, we proposed to quantize the
diﬀerences of gradients instead of gradients themselves and built estimators based on
this information. This approach allowed us to improve the communication complexity of
distributed ﬁrst-order method and provide convergence guarantees under larger stepsizes.
Moreover, a number of works developed extensions of our algorithm to various settings,
see for instance (Horv´ath et al., [103]; Liu et al., [147]; Philippenko and Dieuleveut, [204];
Liu et al., [141]; Gorbunov et al., [82]; Gorbunov et al., [81]).

9.2 Future Research Work

As we discovered answers to some of the important challenges or their aspects, we also
faced new challenges and can see new gaps between theory and practice. Below, we brieﬂy
provide a few directions which we personally consider to be important and challenging.

9.2.1 Federated learning

Despite all progress, many questions remain open even regarding convergence of Local
SGD. First of all, in a work that built on top of our results by Woodworth et al. [277], it
was pointed out that the current best upper bounds still do not match the available lower
bounds. Unfortunately, it remains unknown whether the lower or the upper bounds are
to be blamed for this gap. We believe that further improvements upon the bounds may
lead to a better understanding and potential development of new methods that ﬁx the
technical issues of those works.

Another question that we did not address in the thesis is whether one can use Local
SGD to perform meta learning. Since the number of devices in federated learning can be
vast, it might be impossible to obtain a single model that performs well on all of those
devices. This motivates us to study local methods in the context of learning multiple
models with some level of personalization, as was studied, for instance, by Hanzely and
Richt´arik [96]. At the moment, this question has received substantially less attention,
however, we believe that a proper study may lead to more breakthroughs in the area.

9.2.2 Random Reshuﬄing

As we mentioned in Chapter 3, our new analysis of Random Reshuﬄing under strong
convexity is optimal since it yields complexity that matches the lower bounds of Safran
and Shamir [231] and Rajput et al. [213]. At the same time, neither our analysis nor any
other available in the literature provides guarantees showing that Random Reshuﬄing is
better than Gradient Descent in the non-convex regime. Among the remaining questions,
we want to particularly emphasize that no deterministic strategy to choose permutation is
known to have matching convergence guarantees with Random Reshuﬄing, not to mention
giving better guarantees. After our work was published, it was shown by Rajput et al. [214]

135

that in some cases one can achieve a better rate by reversing the previously sampled
permutation and using this instead of sampling a completely new one. Unfortunately,
no results are yet available for non-quadratic functions showing that this procedure or
any other sampling strategy can beat Random Reshuﬄing. Since Random Reshuﬄing is
widely used in practice, we expect that any improved version could have a huge impact
on the applications, and hence we believe it is an important direction to pursue.

9.2.3 Variance reduction and splitting

One can mention that in Chapter 7, we required access to the proximal operator of any
In Chapter 8, in
term in the summation, and we did not split matrix multiplications.
contrast, we managed to split the linear operator from the proximal operator. However,
we required all proximal operators to be used at each iteration. Since both splitting and
variance reduction are individually possible in this setting, it is reasonable to expect that
one can combine them to obtain a faster method for applications with regularization of
linear transformations, such as the PC-Lasso proposed by Tay et al. [261]. Unfortunately,
we are not aware of any method that achieves this.

9.2.4 Adaptive methods

We now suggest several promising directions for the extensions of our results. First and
foremost, a great challenge for us is to obtain theoretical guarantees of the proposed
method in the non-convex setting. Unfortunately, we are not aware of any generic ﬁrst-
order method for non-convex optimization that does not rely on the descent lemma (or its
generalization), see, e.g., (Attouch et al., [9]). We hope to see more work on this side as
non-convex problems pose a great challenge and often have more complicated structure
than convex ones.

We also want to point out that our proposed stepsize requires the objective to be
diﬀerentiable and smooth, which limits the potential applications of our method. Since
the transition from smooth to composite minimization (Nesterov, [184]) in classical ﬁrst-
order methods is rather straightforward, one would expect that it is trivial to break this
limit for our method too. Unfortunately, the proposed proof of Algorithm 7 does not seem
to provide any route for generalization and we hope there is some way of resolving this
issue.

Finally, we note that the derived bounds for the stochastic case have a suboptimal
dependency on κ. However, it is not clear to us whether one can extend the techniques
from the deterministic analysis to improve the rate. We hope to see more progress on
adaptive stochastic algorithms, which are particularly challenging because adaptivity can
often break unbiasedness.

9.2.5 Communication eﬃciency

The theoretical advances in various aspects of communication eﬃciency that we discussed
before have not yet led to a method that would completely eliminate the communication
bottleneck. Even though the theory sometimes allows for free reduction of communication

136

complexity, there is a discrepancy between the theoretical improvement and the practical
performance of communication-eﬃcient methods that was studied in detail by Dutta et
al. [71]. The primary reason for this discrepancy lies in the diﬃculty of implementing
eﬃcient communication primitives for the known quantization operators. For instance,
the binary compression considered in Chapter 6 requires Gather operation, which is not
as eﬃcient as Reduce used in standard SGD. Moreover, Dutta et al. [71] point out that
vector compression might take longer than communication without compression, so the
advantage is lost for compressors with expensive computation. We believe that new
compressors are required that are capable of running Reduce operations without any
computational overheads. An example of such an approach is the recently proposed
IntSGD compressor of Mishchenko et al. [175]; we hope to see even ﬁner compressors in
the future.

137

REFERENCES

[1] Kwangjun Ahn and Suvrit Sra. On tight convergence rates of without-replacement
SGD. arXiv preprint arXiv:2004.08657, 2020. (Cited on pages 60, 62, 71, 72, and 77)

[2] Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuﬄing: optimal rates
without component convexity and large epoch requirements. In Advances in Neural
Information Processing Systems, volume 33, pages 17526–17535, 2020. (Cited on

pages 60, 62, and 65)

[3] Ahmet Alacaoglu, Quoc Tran-Dinh, Olivier Fercoq, and Volkan Cevher. Smooth
In
primal-dual coordinate descent algorithms for nonsmooth convex optimization.
Advances in Neural Information Processing Systems, volume 30, pages 5852–5861,
2017. (Cited on pages 110 and 114)

[4] Sulaiman A. Alghunaim, Ernest K. Ryu, Kun Yuan, and Ali H. Sayed. Decentralized
proximal gradient algorithms with linear convergence rates. IEEE Transactions on
Automatic Control, 2020. (Cited on page 132)

[5] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
communication-eﬃcient SGD via gradient quantization and encoding. In Advances
in Neural Information Processing Systems, volume 30, pages 1709–1720, 2017.

(Cited on pages 29, 99, 100, 101, 102, 103, 107, 108, and 286)

[6] Zeyuan Allen-Zhu. Katyusha: the ﬁrst direct acceleration of stochastic gradient
methods. The Journal of Machine Learning Research, 18(1):8194–8244, 2017.

(Cited on pages 31 and 110)

[7] Larry Armijo. Minimization of functions having Lipschitz continuous ﬁrst partial

derivatives. Paciﬁc Journal of Mathematics, 16(1):1–3, 1966.

(Cited on pages 27

and 85)

[8] Yves F. Atchad´e, Gersende Fort, and Eric Moulines. On perturbed proximal gradient
algorithms. The Journal of Machine Learning Research, 18(1):310–342, 2017. (Cited

on page 122)

138

[9] Hedy Attouch, J´erˆome Bolte, and Benar F. Svaiter. Convergence of descent methods
for semi-algebraic and tame problems: proximal algorithms, forward–backward split-
ting, and regularized Gauss–Seidel methods. Mathematical Programming, 137(1-
2):91–129, 2013. (Cited on page 135)

[10] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Opti-
mization with sparsity-inducing penalties. Foundations and Trends® in Machine
Learning, 4(1):1–106, 2012. (Cited on page 122)

[11] Jonathan Barzilai and Jonathan M. Borwein. Two-point step size gradient methods.

IMA Journal of Numerical Analysis, 8(1):141–148, 1988. (Cited on pages 27 and 85)

[12] Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-
SGD: distributed SGD with quantization, sparsiﬁcation and local computations.
In Advances in Neural Information Processing Systems, volume 32, pages 14668–
14679, 2019. (Cited on pages 45 and 46)

[13] Heinz H. Bauschke, J´erˆome Bolte, and Marc Teboulle. A descent lemma beyond
Lipschitz gradient continuity: ﬁrst-order methods revisited and applications. Math-
ematics of Operations Research, 42(2):330–348, 2016. (Cited on page 85)

[14] Heinz H. Bauschke and Jonathan M. Borwein. On projection algorithms for solving
convex feasibility problems. SIAM Review, 38(3):367–426, 1996. (Cited on page 111)

[15] Heinz H. Bauschke and Patrick L. Combettes. Convex analysis and monotone
operator theory in Hilbert spaces. Springer, New York, 2nd edition, 2017. (Cited on

pages 33, 122, 123, 126, and 325)

[16] Amir Beck. First-order Methods in Optimization. MOS-SIAM Series on Optimiza-

tion, 2017. (Cited on pages 75 and 303)

[17] J. Yunier Bello-Cruz and Tran T. A. Nghia. On the convergence of the forward–
backward splitting method with linesearches. Optimization Methods and Software,
31(6):1209–1238, 2016. (Cited on page 85)

[18] Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal
recovery of sparse signals via conic programming. Biometrika, 98(4):791–806, 2011.

(Cited on page 293)

[19] Yoshua Bengio. Practical recommendations for gradient-based training of deep
architectures. Neural Networks: Tricks of the Trade, page 437–478, 2012. (Cited on

page 58)

139

[20] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar. signSGD: compressed optimisation for non-convex problems. In Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80,
pages 559–568. PMLR, 2018. (Cited on pages 29, 86, and 99)

[21] Dimitri P. Bertsekas. Incremental gradient, subgradient, and proximal methods for
convex optimization: a survey. Optimization for Machine Learning, 2010(1-38):3,
2011. (Cited on pages 25 and 59)

[22] Dimitri P. Bertsekas. Convex optimization algorithms. Athena Scientiﬁc Belmont,

2015. (Cited on page 307)

[23] Dimitri P. Bertsekas and John N. Tsitsiklis. Parallel and distributed computation:

numerical methods. Prentice-Hall, 1989. (Cited on page 21)

[24] Dimitri P. Bertsekas and John N. Tsitsiklis. Gradient convergence in gradient meth-

ods with errors. SIAM Journal on Optimization, 10(3):627–642, 2000.

(Cited on

page 59)

[25] Pascal Bianchi, Walid Hachem, and Franck Iutzeler. A coordinate descent primal-
dual algorithm and application to distributed asynchronous optimization.
IEEE
Transactions on Automatic Control, 61(10):2947–2957, 2015.

(Cited on pages 79

and 115)

[26] Benjamin Birnbaum, Nikhil R. Devanur, and Lin Xiao. Distributed algorithms via
In Proceedings of the 12th ACM conference

gradient descent for Fisher markets.
on Electronic commerce, pages 127–136, 2011. (Cited on page 85)

[27] Doron Blatt, Alfred O. Hero, and Hillel Gauchman. A convergent incremental gradi-
ent method with a constant step size. SIAM Journal on Optimization, 18(1):29–51,
2007. (Cited on page 30)

[28] Antoine Bordes, L´eon Bottou, and Patrick Gallinari. SGD-QN: careful quasi-Newton
stochastic gradient descent. The Journal of Machine Learning Research, 10:1737–
1754, 2009. (Cited on page 70)

[29] L´eon Bottou. Curiously fast convergence of some stochastic gradient descent al-
gorithms. Unpublished open problem oﬀered to the attendance of the SLDS 2009
conference, 2009. (Cited on pages 58, 70, and 197)

140

[30] L´eon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the

Trade, pages 421–436. Springer, 2012. (Cited on pages 21 and 70)

[31] L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-
scale machine learning. SIAM Review, 60(2):223–311, 2018. (Cited on page 57)

[32] Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge

University Press, 2004. (Cited on page 33)

[33] Kristian Bredies, Karl Kunisch, and Thomas Pock. Total generalized variation.

SIAM Journal on Imaging Sciences, 3(3):492–526, 2010.

(Cited on pages 70, 122,

and 293)

[34] Charles G. Broyden. Quasi-Newton methods and their application to function min-
imisation. Mathematics of Computation, 21(99):368–381, 1967. (Cited on page 21)

[35] Oleg Burdakov, Yuhong Dai, and Na Huang. Stabilized Barzilai-Borwein method.

Journal of Computational Mathematics, 37(6):916–936, 2019.

(Cited on pages 27

and 85)

[36] Emmanuel J. Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty prin-
ciples: exact signal reconstruction from highly incomplete frequency information.
IEEE Transactions on Information Theory, 52(2):489–509, 2006. (Cited on page 293)

[37] Emmanuel J. Cand`es and Terence Tao. The Dantzig selector: statistical estimation
when p is much larger than n. The Annals of Statistics, 35(6):2313–2351, 2007.

(Cited on pages 111, 293, and 294)

[38] Augustin Cauchy. M´ethode g´en´erale pour la r´esolution des systemes d’´equations
Comptes Rendus des S´eances de l’Acad´emie des Sciences,

simultan´ees.
25(1847):536–538, 1847. (Cited on pages 21 and 84)

[39] Volkan Cevher, B˘ang C. V˜u, and Alp Yurtsever. Stochastic forward-Douglas-
Rachford splitting for monotone inclusions. Technical report, Springer International
Publishing, 2018. (Cited on page 114)

[40] Antonin Chambolle, Matthias J. Ehrhardt, Peter Richt´arik, and Carola-Bibiane
Sch¨onlieb. Stochastic primal-dual hybrid gradient algorithm with arbitrary sam-
pling and imaging applications. SIAM Journal on Optimization, 28(4):2783–2808,
2018. (Cited on pages 114, 118, 299, and 301)

141

[41] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for con-
vex problems with applications to imaging. Journal of Mathematical Imaging and
Vision, 40(1):120–145, 2011. (Cited on pages 123 and 330)

[42] Antonin Chambolle and Thomas Pock. An introduction to continuous optimization

for imaging. Acta Numerica, 25:161–319, 2016. (Cited on page 122)

[43] Antonin Chambolle and Thomas Pock. On the ergodic convergence rates of a ﬁrst-
order primal–dual algorithm. Mathematical Programming, 159(1):253–287, 2016.

(Cited on pages 131 and 330)

[44] Chih-Chung Chang and Chih-Jen Lin. LibSVM: a library for support vector machines.
ACM Transactions on Intelligent Systems and Technology, 2(3):27, 2011. (Cited on

pages 15, 55, 121, and 323)

[45] Tatjana Chavdarova, Gauthier Gidel, Fran¸cois Fleuret, and Simon Lacoste-Julien.
Reducing noise in GAN training with variance reduced extragradient. In Advances
in Neural Information Processing Systems, volume 32, pages 391–401. Curran As-
sociates, Inc., 2019. (Cited on page 47)

[46] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization
algorithm using Bregman functions. SIAM Journal on Optimization, 3(3):538–543,
1993. (Cited on pages 63 and 225)

[47] Patrick L. Combettes, Laurent Condat, Jean-Christophe Pesquet, and B˘ang C.
V˜u. A forward-backward view of some primal-dual optimization methods in image
recovery. In IEEE International Conference on Image Processing, pages 4141–4145,
2014. (Cited on page 129)

[48] Patrick L. Combettes and Lilian E. Glaudin. Proximal activation of smooth func-
tions in splitting algorithms for convex image recovery. SIAM Journal on Imaging
Sciences, 12(4):1905–1935, 2019. (Cited on page 331)

[49] Patrick L. Combettes and Jean-Christophe Pesquet. Fixed-Point Algorithms for
Inverse Problems in Science and Engineering, chapter Proximal splitting methods
in signal processing, pages 185–212. Springer Optimization and Its Applications.
Springer, 2011. (Cited on page 112)

[50] Patrick L. Combettes and Jean-Christophe Pesquet. Primal-dual splitting algorithm
for solving inclusions with mixtures of composite, Lipschitzian, and parallel-sum

142

type monotone operators. Set-Valued and Variational Analysis, 20(2):307–330,
2012. (Cited on page 125)

[51] Patrick L. Combettes and Jean-Christophe Pesquet. Stochastic quasi-Fej´er block-
coordinate ﬁxed point iterations with random sweeping. SIAM Journal on Opti-
mization, 25(2):1221–1248, 2015. (Cited on page 115)

[52] Laurent Condat. A primal–dual splitting method for convex optimization involving
Lipschitzian, proximable and linear composite terms. Journal of Optimization The-
ory and Applications, 158(2):460–479, 2013.

(Cited on pages 12, 112, 114, 123, 129,

323, 329, and 330)

[53] Laurent Condat. Discrete total variation: new deﬁnition and minimization. SIAM

Journal on Imaging Sciences, 10(3):1258–1290, 2017. (Cited on page 122)

[54] Laurent Condat, Daichi Kitahara, Andr´es Contreras, and Akira Hirabayashi. Proxi-
mal splitting algorithms: a tour of recent advances, with new twists. arXiv preprint
arXiv:1912.00137, 2019. (Cited on pages 128, 129, and 327)

[55] Gregory F. Coppola. Iterative parameter mixing for distributed large-margin training
of structured predictors for natural language processing. PhD thesis, University of
Edinburgh, UK, 2015. (Cited on page 45)

[56] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning,

20(3):273–297, 1995. (Cited on pages 111, 293, and 295)

[57] Daniel Cremers, Thomas Pock, Kalin Kolev, and Antonin Chambolle. Convex re-
laxation techniques for segmentation, stereo and multiview reconstruction. Markov
Random Fields for Vision and Image Processing, 2011. (Cited on page 122)

[58] Yu-Hong Dai and Li-Zhi Liao. R-linear convergence of the Barzilai and Borwein
gradient method. IMA Journal of Numerical Analysis, 22(1):1–10, 2002. (Cited on

page 85)

[59] Lisandro D. Dalcin, Rodrigo R. Paz, Pablo A. Kler, and Alejandro Cosimo. Parallel
distributed computing using Python. Advances in Water Resources, 34(9):1124–
1139, 2011. (Cited on page 55)

[60] Damek Davis and Wotao Yin. A three-operator splitting scheme and its optimization

applications. Set-valued and variational analysis, 25(4):829–858, 2017.

(Cited on

pages 32, 122, 123, 126, 127, and 330)

143

[61] Aaron Defazio. A simple practical accelerated method for ﬁnite sums. In Advances
in Neural Information Processing Systems, volume 29, pages 676–684, 2016. (Cited

on pages 31, 110, 112, 115, 118, 123, and 301)

[62] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: a fast incremental
gradient method with support for non-strongly convex composite objectives.
In
Advances in Neural Information Processing Systems, volume 27, pages 1646–1654,
2014. (Cited on pages 30, 112, 115, 123, and 130)

[63] Wei Deng, Ming-Jun Lai, Zhimin Peng, and Wotao Yin. Parallel multi-block ADMM
with o(1/k) convergence. Journal of Scientiﬁc Computing, 71(2):712–736, 2017.

(Cited on page 114)

[64] Aymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-oﬀs for local-
SGD with large step size. In Advances in Neural Information Processing Systems,
volume 32, pages 13579–13590, 2019. (Cited on pages 46 and 195)

[65] Yoel Drori and Ohad Shamir. The complexity of ﬁnding stationary points with
In Proceedings of the 37th International Conference

stochastic gradient descent.
on Machine Learning, pages 2658–2667. PMLR, 2020. (Cited on page 58)

[66] Yoel Drori and Marc Teboulle. Performance of ﬁrst-order methods for smooth con-
vex minimization: a novel approach. Mathematical Programming, 145(1-2):451–
482, 2014. (Cited on page 84)

[67] Simon S. Du and Wei Hu. Linear convergence of the primal-dual gradient method
for convex-concave saddle point problems without strong convexity. In Proceedings
of the 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages
196–205. PMLR, 2019. (Cited on page 114)

[68] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods
for online learning and stochastic optimization. The Journal of Machine Learning
Research, 12:2121–2159, 2011. (Cited on pages 27, 28, and 86)

[69] John C. Duchi and Yoram Singer. Eﬃcient online and batch learning using forward
backward splitting. The Journal of Machine Learning Research, 10(Dec):2899–2934,
2009. (Cited on pages 71 and 112)

[70] Joan Duran, Michael Moeller, Catalina Sbert, and Daniel Cremers. Collaborative
total variation: a general framework for vectorial TV models. SIAM Journal on
Imaging Sciences, 9(1):116–151, 2016. (Cited on page 122)

144

[71] Aritra Dutta, El Houcine Bergou, Ahmed M. Abdelmoniem, Chen-Yu Ho,
Atal Narayan Sahu, Marco Canini, and Panos Kalnis. On the discrepancy between
the theoretical analysis and practical implementations of compressed communica-
tion for distributed deep learning. In Proceedings of the 34th AAAI Conference on
Artiﬁcial Intelligence, number 04, pages 3817–3824, 2020. (Cited on page 136)

[72] Richard L. Dykstra. An algorithm for restricted least squares regression. Journal of
the American Statistical Association, 78(384):837–842, 1983. (Cited on page 112)

[73] Matthias J. Ehrhardt, Pawel Markiewicz, Antonin Chambolle, Peter Richt´arik,
Jonathan Schott, and Carola-Bibiane Sch¨onlieb. Faster PET reconstruction with a
stochastic primal-dual hybrid gradient method. In Wavelets and Sparsity XVII, vol-
ume 10394, page 103941O. International Society for Optics and Photonics, 2017.

(Cited on page 299)

[74] Olivier Fercoq and Zheng Qu. Adaptive restart of accelerated gradient meth-
IMA Journal of Numerical Analysis,

ods under local quadratic growth condition.
39(4):2069–2095, 2019. (Cited on page 93)

[75] Olivier Fercoq, Zheng Qu, Peter Richt´arik, and Martin Tak´aˇc. Fast distributed
coordinate descent for minimizing non-strongly convex losses.
IEEE International
Workshop on Machine Learning for Signal Processing, pages 1–6, 2014. (Cited on

page 99)

[76] Roger Fletcher. A new approach to variable metric algorithms. The Computer

Journal, 13(3):317–322, 1970. (Cited on page 21)

[77] Wenbo Gao and Donald Goldfarb. Quasi-Newton methods: superlinear convergence
without line searches for self-concordant functions. Optimization Methods and
Software, 34(1):194–217, 2019. (Cited on page 28)

[78] Roland Glowinski and Americo Marroco. Sur l’approximation, par ´el´ements ﬁnis
d’ordre un, et la r´esolution, par p´enalisation-dualit´e d’une classe de probl`emes de
dirichlet non lin´eaires. ESAIM: Mathematical Modelling and Numerical Analysis-
Mod´elisation Math´ematique et Analyse Num´erique, 9(R2):41–76, 1975. (Cited on

page 123)

[79] Donald Goldfarb. A family of variable-metric methods derived by variational means.

Mathematics of Computation, 24(109):23–26, 1970. (Cited on page 21)

145

[80] Allen A. Goldstein. Cauchy’s method of minimization. Numerische Mathematik,

4(1):146–150, 1962. (Cited on page 85)

[81] Eduard Gorbunov, Filip Hanzely, and Peter Richt´arik. A uniﬁed theory of SGD:
variance reduction, sampling, quantization and coordinate descent. In Proceedings
of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, pages
680–690. PMLR, 2020. (Cited on pages 30, 70, 71, 78, 123, 129, 130, 134, 228, and 331)

[82] Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt´arik. Lin-
early converging error compensated SGD. In Advances in Neural Information Pro-
cessing Systems, volume 33, pages 20889–20900, 2020. (Cited on pages 30 and 134)

[83] Robert M. Gower, Filip Hanzely, Peter Richt´arik, and Sebastian U. Stich. Accel-
erated stochastic matrix inversion: general theory and speeding up BFGS rules for
faster second-order optimization.
In Advances in Neural Information Processing
Systems, volume 31, pages 1619–1629, 2018. (Cited on pages 15, 111, 120, and 121)

[84] Robert M. Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and
Peter Richt´arik. SGD: general analysis and improved rates. In Proceedings of the
36th International Conference on Machine Learning, volume 97, pages 5200–5209.
PMLR, 2019. (Cited on pages 49, 75, and 197)

[85] Robert M. Gower and Peter Richt´arik. Randomized iterative methods for linear
systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660–1690,
2015. (Cited on page 111)

[86] Robert M. Gower, Peter Richt´arik, and Francis Bach. Stochastic quasi-gradient
methods: variance reduction via Jacobian sketching. Mathematical Programming,
pages 1–58, 2020. (Cited on pages 78, 111, 130, and 322)

[87] Robert M. Gower, Mark Schmidt, Francis Bach, and Peter Richt´arik. Variance-
reduced methods for machine learning. Proceedings of the IEEE, 108(11):1968–
1983, 2020. (Cited on page 30)

[88] Robert Mansel Gower, Peter Richt´arik, and Francis Bach. Stochastic quasi-gradient
methods: variance reduction via Jacobian sketching. Mathematical Programming,
(188):135–192, 2020. (Cited on page 30)

[89] Luigi Grippo. A class of unconstrained minimization methods for neural network
training. Optimization Methods and Software, 4(2):135–150, 1994. (Cited on page 59)

146

[90] Mert G¨urb¨uzbalaban, Asuman ¨Ozda˘glar, and Pablo A. Parrilo. Convergence rate
of incremental gradient and incremental Newton methods. SIAM Journal on Opti-
mization, 29(4):2542–2565, 2019. (Cited on pages 25 and 59)

[91] Mert G¨urb¨uzbalaban, Asuman ¨Ozda˘glar, and Pablo A. Parrilo. Why random reshuf-
ﬂing beats stochastic gradient descent. Mathematical Programming, 2019. (Cited

on pages 59 and 70)

[92] Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck
Cadambe. Local SGD with periodic averaging: tighter analysis and adaptive syn-
chronization.
In Advances in Neural Information Processing Systems, volume 32,
pages 11080–11092, 2019. (Cited on page 45)

[93] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent

methods in federated learning. arXiv preprint arXiv:1910.14425, 2019.

(Cited on

pages 46 and 47)

[94] Filip Hanzely. Optimization for Supervised Machine Learning: Randomized Algo-
rithms for Data and Parameters. PhD thesis, King Abdullah University of Science
and Technology, 2020. (Cited on page 33)

[95] Filip Hanzely, Konstantin Mishchenko, and Peter Richt´arik. SEGA: variance reduc-
tion via gradient sketching. In Advances in Neural Information Processing Systems,
volume 31, pages 2082–2093, 2018. (Cited on pages 33, 111, 130, and 346)

[96] Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and

local models. arXiv preprint arXiv:2002.05516, 2020. (Cited on page 134)

[97] Jeﬀ Haochen and Suvrit Sra. Random shuﬄing beats SGD after ﬁnite epochs. In
Proceedings of the 36th International Conference on Machine Learning, volume 97,
pages 2624–2633. PMLR, 2019. (Cited on pages 59, 60, 70, and 71)

[98] Andrew Hard, Kanishka Rao, Rajiv Mathews, Fran¸coise Beaufays, Sean Augenstein,
Hubert Eichner, Chlo´e Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018. (Cited on page 46)

[99] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: history and
context. ACM Transactions on Interactive Intelligent Systems, 5(4):19, 2016. (Cited

on page 94)

147

[100] Elad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv preprint

arXiv:1905.00313, 2019. (Cited on pages 27 and 85)

[101] Bingsheng S. He and Xiaoming Yuan. Convergence analysis of primal-dual algo-
rithms for a saddle-point problem: from contraction perspective. SIAM Journal on
Imaging Sciences, 5(1):119–149, 2012. (Cited on page 129)

[102] Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.
Variance reduced stochastic gradient descent with neighbors. In Advances in Neural
Information Processing Systems, volume 28, pages 2305–2313, 2015.

(Cited on

pages 130 and 317)

[103] Samuel Horv´ath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian U. Stich, and
Peter Richt´arik. Stochastic distributed learning with gradient quantization and
variance reduction. arXiv preprint arXiv:1904.05115, 2019.

(Cited on pages 30, 33,

130, 134, and 348)

[104] Rustem Islamov, Xun Qian, and Peter Richt´arik. Distributed second order methods

with fast rates and compressed communication. 2021. (Cited on page 30)

[105] Martin Jaggi, Virginia Smith, Martin Tak´aˇc, Jonathan Terhorst, Sanjay Krishnan,
Thomas Hofmann, and Michael I. Jordan. Communication-eﬃcient distributed dual
In Advances in Neural Information Processing Systems, vol-
coordinate ascent.
ume 27, 2014. (Cited on page 99)

[106] Majid Jahani, Xi He, Chenxin Ma, Aryan Mokhtari, Dheevatsa Mudigere, Alejandro
Ribeiro, and Martin Tak´ac. Eﬃcient distributed hessian free algorithm for large-
scale empirical risk minimization via accumulating sample strategy. In Proceedings
of the 23rd International Conference on Artiﬁcial Intelligence and Statistics, pages
2634–2644. PMLR, 2020. (Cited on page 99)

[107] Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learn-
ing with sparse and quantized communication. In Advances in Neural Information
Processing Systems, volume 31, pages 2525–2536, 2018.

(Cited on pages 24, 45,

and 46)

[108] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using pre-
dictive variance reduction. In Advances in Neural Information Processing Systems,
volume 26, pages 315–323, 2013. (Cited on pages 22, 30, 112, and 123)

148

[109] Anatoli B. Juditsky, Arkadi S. Nemirovski, and Claire Tauvel. Solving variational
inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58,
2011. (Cited on page 47)

[110] Stefan Kaczmarz. Angenaherte auﬂosung von systemen linearer glei-chungen. Bul-
letin International de l’Acad´emie Polonaise des Sciences et des Lettres. Classe des
Sciences Math´ematiques et Naturelles, pages 355–357, 1937. (Cited on page 112)

[111] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. Advances and open problems in federated learning. Foun-
dations and Trends® in Machine Learning, 14(1), 2021.

(Cited on pages 23, 69,

and 73)

[112] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient
In Joint
and proximal-gradient methods under the Polyak-(cid:32)lojasiewicz condition.
European Conference on Machine Learning and Knowledge Discovery in Databases,
page 795–811. Springer, 2016. (Cited on page 75)

[113] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebas-
tian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: stochastic controlled
In Proceedings of the 37th International Con-
averaging for federated learning.
ference on Machine Learning, pages 5132–5143. PMLR, 2020.

(Cited on pages 45

and 133)

[114] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi.
Error feedback ﬁxes SignSGD and other gradient compression schemes. In Proceed-
ings of the 36th International Conference on Machine Learning, pages 3252–3261.
PMLR, 2019. (Cited on page 86)

[115] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. First analysis of local
GD on heterogeneous data. The 2nd International Workshop on Federated Learning
for Data Privacy and Conﬁdentiality, arXiv preprint arXiv:1909.04715, 2019. (Cited

on pages 47, 81, and 348)

[116] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. Tighter theory for
local SGD on identical and heterogeneous data. In Proceedings of the 23rd Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 4519–4529. PMLR,
2020. (Cited on pages 24, 236, and 346)

149

[117] Ahmed Khaled and Peter Richt´arik. Better theory for SGD in the nonconvex world.

arXiv preprint arXiv:2002.03329, 2020. (Cited on pages 66, 72, 77, 208, 211, and 230)

[118] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed
learning with compressed gradients. arXiv preprint arXiv:1806.06573, 2018. (Cited

on pages 29, 99, and 100)

[119] Seung-Jean Kim, Kwangmoo Koh, Stephen P. Boyd, and Dimitry Gorinevsky. (cid:96)1

trend ﬁltering. SIAM Review, 51(2):339–360, 2009. (Cited on page 293)

[120] Diederik Kingma and Jimmy Ba. Adam: a method for stochastic optimization.
International Conference on Learning Representations, 2014. (Cited on pages 27 and 86)

[121] Nikos Komodakis and Jean-Christophe Pesquet. Playing with duality: an overview
of recent primal-dual approaches for solving large-scale optimization problems. IEEE
Signal Processing Magazine, 32(6):31–54, 2015. (Cited on page 129)

[122] Jakub Koneˇcn`y and Peter Richt´arik. Randomized distributed mean estimation:
accuracy vs. communication. Frontiers in Applied Mathematics and Statistics, 4:62,
2018. (Cited on page 99)

[123] Jakub Koneˇcn´y, Jie Lu, Peter Richt´arik, and Martin Tak´aˇc. Mini-batch semi-
stochastic gradient descent in the proximal setting. IEEE Journal of Selected Topics
in Signal Processing, 10(2):242–255, 2016. (Cited on page 31)

[124] Jakub Koneˇcn´y, H. Brendan McMahan, Felix Yu, Peter Richt´arik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: strategies for improving communi-
cation eﬃciency. In NIPS Private Multi-Party Machine Learning Workshop, 2016.

(Cited on pages 22, 23, 43, 46, 69, 73, and 98)

[125] Jakub Koneˇcn´y and Peter Richt´arik. Semi-stochastic gradient descent methods.
Frontiers in Applied Mathematics and Statistics, pages 1–14, 2017. (Cited on page 30)

[126] Dmitry Kovalev, Samuel Horv´ath, and Peter Richt´arik. Don’t jump through hoops
and remove those loops: SVRG and Katyusha are better without the outer loop. In
Proceedings of the 31st International Conference on Algorithmic Learning Theory,
volume 117, pages 451–467. PMLR, 2020. (Cited on pages 31, 112, and 130)

[127] Dmitry Kovalev, Konstantin Mishchenko, and Peter Richt´arik. Stochastic Newton
and cubic Newton methods with simple local linear-quadratic rates. NeurIPS Beyond

150

First-Order Methods in ML Workshop, arXiv preprint arXiv:1912.01597, 2019. (Cited

on pages 33 and 347)

[128] Alex Krizhevsky and Geoﬀrey Hinton. Learning multiple layers of features from tiny

images. Technical report, Citeseer, 2009. (Cited on page 96)

[129] Zehua Lai and Lek-Heng Lim. Recht-R´e noncommutative arithmetic-geometric
In Proceedings of the 37th International Conference on

mean conjecture is false.
Machine Learning, pages 5608–5617. PMLR, 2020. (Cited on page 58)

[130] Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learn-

ing. Springer, 2020. (Cited on page 123)

[131] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method
with an exponential convergence rate for ﬁnite training sets. In Advances in Neural
Information Processing Systems, volume 25, pages 2663–2671, 2012.

(Cited on

pages 22, 30, and 111)

[132] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. AT&T Labs,

2010. (Cited on page 323)

[133] Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by non-negative

matrix factorization. Nature, 401(6755):788–791, 1999. (Cited on page 70)

[134] Lihua Lei and Michael I. Jordan. Less than a single pass: stochastically controlled
stochastic gradient. In Proceedings of the 20th International Conference on Artiﬁ-
cial Intelligence and Statistics, volume 54, pages 148–156. PMLR, 2017. (Cited on

page 111)

[135] Lihua Lei and Michael I. Jordan. On the adaptivity of stochastic gradient-based

optimization. SIAM Journal on Optimization, 30(2):1473–1500, 2020.

(Cited on

page 27)

[136] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I. Jordan. Non-convex ﬁnite-sum
In Advances in Neural Information Processing

optimization via SCSG methods.
Systems, volume 30, pages 2348–2358, 2017. (Cited on page 111)

[137] Claude Lemar´echal. Cauchy and the gradient method. Documenta Mathematica

Extra, 251:254, 2012. (Cited on page 84)

[138] Huan Li and Zhouchen Lin. Revisiting EXTRA for smooth distributed optimization.
SIAM Journal on Optimization, 30(3):1795–1821, 2020. (Cited on page 132)

151

[139] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings
of Machine Learning and Systems, volume 2, pages 429–450, 2020. (Cited on page 47)

[140] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Jason D. Lee. Incremental methods
for weakly convex optimization. arXiv preprint arXiv:1907.11687, 2019. (Cited on

pages 46, 47, and 59)

[141] Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik. Acceleration for com-
pressed gradient descent in distributed and federated optimization. In Proceedings of
the 37th International Conference on Machine Learning, pages 5895–5904. PMLR,
2020. (Cited on pages 30 and 134)

[142] Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei
Cheng. Variance reduced local SGD with lower communication complexity. arXiv
preprint arXiv:1912.12844, 2019. (Cited on page 45)

[143] Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large
mini-batches, use local SGD. In International Conference on Learning Representa-
tions, 2020. (Cited on page 43)

[144] Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two
nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964–979, 1979.

(Cited on page 112)

[145] Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two
nonlinear operators. SIAM Journal on Numerical Analysis, 16(6):964–979, 1979.

(Cited on page 123)

[146] Ji Liu and Stephen J. Wright. An accelerated randomized Kaczmarz algorithm.

Mathematics of Computation, 85(297):153–178, 2016. (Cited on page 298)

[147] Xiaorui Liu, Yao Li, Jiliang Tang, and Ming Yan. A double residual compression
algorithm for eﬃcient distributed learning. In Proceedings of the 23rd International
Conference on Artiﬁcial Intelligence and Statistics, pages 133–143. PMLR, 2020.

(Cited on pages 30 and 134)

[148] Yuanyuan Liu, Fanhua Shang, and James Cheng. Accelerated variance reduced
stochastic ADMM. In Proceedings of the 31st AAAI Conference on Artiﬁcial Intel-
ligence, number 1, pages 2287–2293, 2017. (Cited on page 114)

152

[149] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien.
Stochastic Polyak step-size for SGD: an adaptive learning rate for fast convergence.
In Proceedings of the 24th International Conference on Artiﬁcial Intelligence and
Statistics, pages 1306–1314. PMLR, 2021. (Cited on page 27)

[150] Stanislaw Lojasiewicz. A topological property of real analytic subsets. Les ´equations

aux d´eriv´ees partielles, 117:87–89, 1963. (Cited on page 71)

[151] Zhi-Quan Luo. On the convergence of the LMS algorithm with adaptive learning
rate for linear feedforward networks. Neural Computation, 3(2):226–245, 1991.

(Cited on page 59)

[152] Chenxin Ma, Jakub Koneˇcn´y, Martin Jaggi, Virginia Smith, Michael I. Jordan, Peter
Richt´arik, and Martin Tak´aˇc. Distributed optimization with arbitrary local solvers.
Optimization Methods and Software, 32(4):813–848, 2017. (Cited on page 99)

[153] Chenxin Ma and Martin Tak´aˇc. Partitioning data on features or samples in
communication-eﬃcient distributed optimization? The 8th NIPS Workshop on
Optimization for Machine Learning, arXiv preprint arXiv:1510.06688, 2015. (Cited

on page 99)

[154] Chris J. Maddison, Daniel Paulin, Yee Whye Teh, and Arnaud Doucet. Dual space
preconditioning for gradient descent. SIAM Journal on Optimization, 31(1):991–
1016, 2021. (Cited on page 85)

[155] Julien Mairal. Incremental majorization-minimization optimization with application
to large-scale machine learning. SIAM Journal on Optimization, 25(2):829–855,
2015. (Cited on page 30)

[156] Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter
Richt´arik. From local SGD to local ﬁxed-point methods for federated learning.
In Proceedings of the 37th International Conference on Machine Learning, pages
6692–6701. PMLR, 2020. (Cited on page 133)

[157] Yura Malitsky. Golden ratio algorithms for variational inequalities. Mathematical

Programming, pages 1–28, 2019. (Cited on page 86)

[158] Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without
descent. In Proceedings of the 37th International Conference on Machine Learning,
volume 119, pages 6702–6712. PMLR, 2020. (Cited on pages 29, 197, and 346)

153

[159] Olvi L. Mangasarian. Parallel gradient distribution in unconstrained optimization.

SIAM Journal on Control and Optimization, 33(6):1916–1925, 1995.

(Cited on

pages 23, 43, and 45)

[160] Olvi L. Mangasarian and Mikhail V. Solodov. Serial and parallel backpropagation
convergence via nonmonotone perturbed minimization. Optimization Methods and
Software, 4(2):103–116, 1994. (Cited on page 59)

[161] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for
In Human Language Technologies: The 2010 Annual
the structured perceptron.
Conference of the North American Chapter of the Association for Computational
Linguistics, pages 456–464, 2010. (Cited on page 45)

[162] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Ag¨uera y Arcas. Communication-eﬃcient learning of deep networks from decen-
In Proceedings of the 20th International Conference on Artiﬁcial
tralized data.
Intelligence and Statistics, pages 1273–1282. PMLR, 2017. (Cited on pages 22, 23, 43,

46, 69, and 73)

[163] H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for
In Proceedings of the 23rd Annual Conference on

online convex optimization.
Learning Theory, 2010. (Cited on page 86)

[164] Konstantin Mishchenko. Sinkhorn algorithm as a special case of stochastic mir-
ror descent. NeurIPS Workshop on Optimal Transport & Machine learning, arXiv
preprint arXiv:1909.06918, 2019. (Cited on pages 33 and 347)

[165] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak´aˇc, and Peter Richt´arik.
arXiv preprint

Distributed learning with compressed gradient diﬀerences.
arXiv:1901.09269, 2019. (Cited on pages 30 and 347)

[166] Konstantin Mishchenko, Filip Hanzely, and Peter Richt´arik. 99% of worker-master
In 36th Conference on

communication in distributed optimization is not needed.
Uncertainty in Artiﬁcial Intelligence. AUAI, 2020. (Cited on pages 33 and 346)

[167] Konstantin Mishchenko, Franck Iutzeler, and J´erˆome Malick. A distributed ﬂex-
ible delay-tolerant proximal gradient algorithm. SIAM Journal on Optimization,
30(1):933–959, 2020. (Cited on pages 33 and 346)

[168] Konstantin Mishchenko, Franck Iutzeler, J´erˆome Malick, and Massih-Reza Amini. A
delay-tolerant proximal-gradient algorithm for distributed learning. In Proceedings of

154

the 35th International Conference on Machine Learning, pages 3587–3595. PMLR,
2018. (Cited on pages 33 and 346)

[169] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Random reshuﬄing:
simple analysis with vast improvements. Advances in Neural Information Processing
Systems, 33:17309–17320, 2020. (Cited on pages 26, 224, and 346)

[170] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Proximal and feder-
ated random reshuﬄing. arXiv preprint arXiv:2102.06704, 2021. (Cited on pages 27

and 347)

[171] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt´arik, and Yura
In Proceedings of the 23rd Interna-
Malitsky. Revisiting stochastic extragradient.
tional Conference on Artiﬁcial Intelligence and Statistics, pages 4573–4582. PMLR,
2020. (Cited on pages 47 and 346)

[172] Konstantin Mishchenko, Mallory Montgomery, and Federico Vaggi. A self-supervised
approach to hierarchical forecasting with applications to groupwise synthetic con-
trols. ICML Time Series Workshop, arXiv preprint arXiv:1906.10586, 2019. (Cited

on pages 33 and 347)

[173] Konstantin Mishchenko and Peter Richt´arik. A stochastic penalty model for convex
and nonconvex optimization with big constraints. arXiv preprint arXiv:1810.13387,
2018. (Cited on pages 33, 114, and 347)

[174] Konstantin Mishchenko and Peter Richt´arik. A stochastic decoupling method
arXiv preprint

for minimizing the sum of smooth and non-smooth functions.
arXiv:1905.11535, 2019. (Cited on pages 31, 124, and 347)

[175] Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, and Peter Richt´arik.
arXiv preprint

stochastic gradients.

IntSGD: ﬂoatless
compression of
arXiv:2102.08374, 2021. (Cited on pages 30, 33, 136, and 347)

[176] Aryan Mokhtari and Alejandro Ribeiro. DSA: decentralized double stochastic aver-
aging gradient algorithm. The Journal of Machine Learning Research, 17(1):2165–
2199, 2016. (Cited on page 132)

[177] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approxima-
tion algorithms for machine learning. Advances in Neural Information Processing
Systems, 24:451–459, 2011. (Cited on page 123)

155

[178] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement:
sharper rates for general smooth convex functions. In Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97, pages 4703–4711. PMLR,
2019. (Cited on pages 59, 60, 62, and 70)

[179] Angelia Nedi´c and Dimitri P. Bertsekas. Incremental subgradient methods for non-
diﬀerentiable optimization. SIAM Journal on Optimization, 12(1):109–138, 2001.

(Cited on pages 21 and 59)

[180] Angelia Nedi´c and Asuman ¨Ozda˘glar. Distributed subgradient methods for multi-
agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009.

(Cited on pages 111, 293, and 294)

[181] Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent,
weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural
Information Processing Systems, volume 27, pages 1017–1025. Curran Associates,
Inc., 2014. (Cited on pages 64, 75, and 78)

[182] Arkadi S. Nemirovski and David B. Yudin. Problem complexity and method eﬃ-
ciency in optimization. John Wiley & Sons, Inc., New York, 1983. (Cited on pages 21

and 85)

[183] Yurii Nesterov. A method for unconstrained convex minimization problem with
the rate of convergence O(1/k2). Doklady Akademii Nauk SSSR, 269(3):543–547,
1983. (Cited on page 94)

[184] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical

Programming, 140(1):125–161, 2013. (Cited on pages 93, 95, 96, and 135)

[185] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical

Programming, 140(1):125–161, 2013. (Cited on page 112)

[186] Yurii Nesterov. Introductory lectures on convex optimization: a basic course, vol-

ume 87. Springer, 2013. (Cited on pages 20, 27, 33, 86, 92, and 238)

[187] Yurii Nesterov. Lectures on Convex Optimization, volume 137. Springer, 2018.

(Cited on page 130)

[188] Yurii Nesterov and Boris T. Polyak. Cubic regularization of Newton method and its
global performance. Mathematical Programming, 108(1):177–205, 2006. (Cited on

page 95)

156

[189] Isaac Newton. Philosophiae naturalis principia mathematica. William Dawson &

Sons Ltd., London, 1687. (Cited on page 20)

[190] Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richt´arik, Katya
Scheinberg, and Martin Tak´aˇc. SGD and Hogwild!
convergence without the
bounded gradients assumption. In Proceedings of the 35th International Conference
on Machine Learning, pages 3747–3755. PMLR, 2018. (Cited on page 60)

[191] Lam M. Nguyen, Katya Scheinberg, and Martin Tak´aˇc. Inexact SARAH algorithm
for stochastic optimization. Optimization Methods and Software, 36(1):237–258,
2021. (Cited on page 111)

[192] Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten
van Dijk. A uniﬁed convergence analysis for shuﬄing-type gradient methods. arXiv
preprint arXiv:2002.08246, 2020. (Cited on pages 59, 60, 62, and 209)

[193] Phuong Ha Nguyen, Lam M. Nguyen, and Marten van Dijk. Tight dimension
independent lower bound on the expected convergence rate for diminishing step
sizes in SGD. In Advances in Neural Information Processing Systems, volume 32,
pages 3660–3669. Curran Associates, Inc., 2019. (Cited on page 58)

[194] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Series in
Operations Research and Financial Engineering. Springer, 2006. (Cited on page 111)

[195] Daniel O’Connor and Lieven Vandenberghe. On the equivalence of the primal-dual
hybrid gradient method and Douglas–Rachford splitting. Mathematical Program-
ming, 179(1):85–108, 2020. (Cited on pages 129 and 342)

[196] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods
for saddle-point problems. In Advances in Neural Information Processing Systems,
volume 29, pages 1416–1424, 2016. (Cited on pages 111 and 114)

[197] Daniel P. Palomar and Yonina C. Eldar. Convex optimization in signal processing
and communications. Cambridge University Press, 2010. (Cited on page 122)

[198] Neal Parikh and Stephen P. Boyd. Proximal algorithms. Foundations and Trends®

in Optimization, 1(3):127–239, 2014. (Cited on pages 21, 38, 237, and 298)

[199] Sco Parker, Sudheer Chunduri, Kevin Harms, and Krishna Kandalla. Performance
evaluation of MPI on Cray XC40 xeon phi systems. In Cray User Group Proceedings,
2018. (Cited on pages 108, 283, and 286)

157

[200] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas K¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. PyTorch: an imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems, volume 32, pages 8024–8035.
2019. (Cited on page 96)

[201] Andrei Patrascu and Paul Irofti. Stochastic proximal splitting algorithm for com-

posite minimization. Optimization Letters, pages 1–19, 2021. (Cited on page 75)

[202] Fabian Pedregosa, Kilian Fatras, and Mattia Casotto. Proximal splitting meets
variance reduction. In Proceedings of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pages 1–10. PMLR, 2019.

(Cited on pages 31, 110, 113,

and 124)

[203] Nhan H. Pham, Lam M. Nguyen, Dzung T. Phan, and Quoc Tran-Dinh. Prox-
SARAH: an eﬃcient algorithmic framework for stochastic composite nonconvex
optimization. The Journal of Machine Learning Research, 21(110):1–48, 2020.

(Cited on page 70)

[204] Constantin Philippenko and Aymeric Dieuleveut. Bidirectional compression in het-
erogeneous settings for distributed or federated learning with partial participation:
tight convergence guarantees. arXiv preprint arXiv:2006.14591, 2020.

(Cited on

pages 30 and 134)

[205] Nicholas G. Polson, James G. Scott, and Brandon T. Willard. Proximal algorithms
in statistics and machine learning. Statistical Science, 30(4):559–581, 2015. (Cited

on page 122)

[206] Boris T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-
tel’noi Matematiki i Matematicheskoi Fiziki, 3(4):643–653, 1963. (Cited on pages 27,

71, and 84)

[207] Boris T. Polyak. Minimization of nonsmooth functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 9(3):509–521, 1969. (Cited on page 85)

[208] Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation
by averaging. SIAM journal on control and optimization, 30(4):838–855, 1992.

(Cited on page 24)

158

[209] Xun Qian, Zheng Qu, and Peter Richt´arik. SAGA with arbitrary sampling.

In
Proceedings of the 36th International Conference on Machine Learning. PMLR,
2019. (Cited on page 112)

[210] Xun Qian, Zheng Qu, and Peter Richt´arik. L-SVRG and L-Katyusha with arbitrary

sampling. Journal of Machine Learning Research, 22(112):1–47, 2021.

(Cited on

page 31)

[211] Xun Qian, Alibek Sailanbayev, Konstantin Mishchenko, and Peter Richt´arik. MISO
is making a comeback with better proofs and rates. arXiv preprint arXiv:1906.01474,
2019. (Cited on pages 30, 33, and 347)

[212] Zheng Qu, Peter Richt´arik, and Tong Zhang. Quartz: randomized dual coordi-
nate ascent with arbitrary sampling. In Advances in Neural Information Processing
Systems, volume 28, pages 865–873, 2015. (Cited on page 30)

[213] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the conver-
gence gap of SGD without replacement.
In Proceedings of the 37th International
Conference on Machine Learning, pages 7964–7973. PMLR, 2020. (Cited on pages 58,

64, and 134)

[214] Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos. Permutation-based
SGD: is random optimal? arXiv preprint arXiv:2102.09718, 2021. (Cited on page 134)

[215] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent
In Proceedings of the 29th
optimal for strongly convex stochastic optimization.
International Conference on Machine Learning, page 1571–1578. Omnipress, 2012.

(Cited on page 58)

[216] Marcos Raydan. On the Barzilai and Borwein choice of steplength for the gradient
method. IMA Journal of Numerical Analysis, 13(3):321–326, 1993. (Cited on pages 27,

28, and 85)

[217] Benjamin Recht and Christopher R´e. Toward a noncommutative arithmetic-
geometric mean inequality: conjectures, case-studies, and consequences.
In Pro-
ceedings of the 25th Annual Conference on Learning Theory, volume 23, page
11.1–11.24, 2012. Edinburgh, Scotland. (Cited on pages 58 and 70)

[218] Benjamin Recht and Christopher R´e. Parallel stochastic gradient algorithms for
large-scale matrix completion. Mathematical Programming Computation, 5(2):201–
226, 2013. (Cited on page 58)

159

[219] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam
and beyond. In International Conference on Learning Representations, 2018. (Cited

on pages 27 and 86)

[220] Sashank J. Reddi, Jakub Koneˇcn´y, Peter Richt´arik, Barnab´as P´ocz´os, and Alex
Smola. AIDE: fast and communication eﬃcient distributed optimization. arXiv
preprint arXiv:1608.06879, 2016. (Cited on page 99)

[221] Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and
Ramtin Pedarsani. FedPAQ: a communication-eﬃcient federated learning method
with periodic averaging and quantization. In Proceedings of the 23rd International
Conference on Artiﬁcial Intelligence and Statistics, pages 2021–2031. PMLR, 2020.

(Cited on page 45)

[222] Peter Richt´arik and Martin Tak´aˇc. Distributed coordinate descent method for learn-
ing with big data. The Journal of Machine Learning Research, 17(75):1–25, 2016.

(Cited on page 99)

[223] Peter Richt´arik and Martin Tak´aˇc. Stochastic reformulations of linear systems: algo-
rithms and convergence theory. SIAM Journal on Matrix Analysis and Applications,
41(2):487–524, 2020. (Cited on page 111)

[224] Herbert Robbins and Sutton Monro. A stochastic approximation method. The

annals of mathematical statistics, pages 400–407, 1951. (Cited on pages 21 and 24)

[225] Ralph Tyrell Rockafellar. Convex analysis. Princeton University Press, 2015. (Cited

on pages 303 and 304)

[226] Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based
noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1-4):259–268,
1992. (Cited on page 70)

[227] Ernest K. Ryu. Uniqueness of DRS as the 2 operator resolvent-splitting and impos-
sibility of 3 operator resolvent-splitting. Mathematical Programming, 182(1):233–
273, 2020. (Cited on page 123)

[228] Ernest K. Ryu and Wotao Yin. Proximal-proximal-gradient method. Journal of

Computational Mathematics, 37(6), 2019. (Cited on pages 31, 110, and 113)

[229] Mher Safaryan, Filip Hanzely, and Peter Richt´arik. Smoothness matrices beat
smoothness constants: better communication compression techniques for dis-
tributed optimization. arXiv preprint arXiv:2102.07245, 2021. (Cited on page 30)

160

[230] Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richt´arik.
making Newton-type methods applicable to federated learning.
arXiv:2106.02969, 2021. (Cited on page 30)

FedNL:
arXiv preprint

[231] Itay Safran and Ohad Shamir. How good is SGD with random shuﬄing?

In
Proceedings of the 33rd Annual Conference on Learning Theory, volume 125, pages
3250–3284. PMLR, 2020. (Cited on pages 58, 60, 64, 65, 66, 134, and 216)

[232] Adil Salim, Pascal Bianchi, and Walid Hachem. Snake: a stochastic proximal
IEEE Transactions

gradient algorithm for regularized problems over large graphs.
on Automatic Control, 64(5):1832–1847, 2019. (Cited on page 122)

[233] Adil Salim, Laurent Condat, Dmitry Kovalev, and Peter Richt´arik. An optimal
algorithm for strongly convex minimization under aﬃne constraints. arXiv preprint
arXiv:2102.11079, 2021. (Cited on page 133)

[234] Adil Salim, Laurent Condat, Konstantin Mishchenko, and Peter Richt´arik. Du-
fast nonsmooth optimization algorithms. arXiv preprint

alize, split, randomize:
arXiv:2004.02635, 2020. (Cited on pages 32 and 348)

[235] Saverio Salzo. The variable metric forward-backward splitting algorithm under mild
diﬀerentiability assumptions. SIAM Journal on Optimization, 27(4):2153–2181,
2017. (Cited on page 85)

[236] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
DNNs. In Fifteenth Annual Conference of the International Speech Communication
Association, pages 1058–1062, 2014. (Cited on page 99)

[237] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from

theory to algorithms. Cambridge University Press, 2014. (Cited on page 69)

[238] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: primal estimated
In Proceedings of the 24th International Conference

subgradient solver for SVM.
on Machine Learning, pages 807–814. PMLR, 2007. (Cited on page 295)

[239] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pega-
sos: primal estimated sub-gradient solver for SVM. Mathematical Programming,
127(1):3–30, 2011. (Cited on page 295)

161

[240] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods
for regularized loss minimization. Journal of Machine Learning Research, 14(1):567–
599, 2013. (Cited on page 30)

[241] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent meth-
ods for regularized loss minimization. The Journal of Machine Learning Research,
14:567–599, 2013. (Cited on pages 111, 112, and 297)

[242] Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In
Advances in Neural Information Processing Systems, volume 33, page 46–54. Curran
Associates Inc., 2016. (Cited on page 71)

[243] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-eﬃcient distributed
optimization using an approximate Newton-type method.
In Proceedings of the
31st International Conference on Machine Learning, volume 32, pages 1000–1008.
PMLR, 2014. (Cited on page 99)

[244] Fanhua Shang, Licheng Jiao, Kaiwen Zhou, James Cheng, Yan Ren, and Yufei Jin.
In Asian Conference on Machine Learning,

ASVRG: accelerated proximal SVRG.
volume 95, pages 815–830. PMLR, 2018. (Cited on pages 31 and 70)

[245] David F. Shanno. Conditioning of quasi-Newton methods for function minimization.

Mathematics of computation, 24(111):647–656, 1970. (Cited on page 21)

[246] Pranay Sharma, Prashant Khanduri, Saikiran Bulusu, Ketan Rajawat, and
Pramod K. Varshney. Parallel restarted SPIDER – communication eﬃcient dis-
tributed nonconvex optimization with optimal computation complexity.
arXiv
preprint arXiv:1912.06036, 2019. (Cited on page 45)

[247] Naum Z. Shor. An application of the method of gradient descent to the solution of
the network transportation problem. Materialy Naucnovo Seminara po Teoretich-
eskim i Prikladnym Voprosam Kibernetiki i Isstedovaniu Operacii, 1:9–17, 1962.

(Cited on pages 28 and 86)

[248] Virginia Smith, Simone Forte, Ma Chenxin, Martin Tak´aˇc, Michael I. Jordan, and
Martin Jaggi. CoCoA: a general framework for communication-eﬃcient distributed
optimization. The Journal of Machine Learning Research, 18:1–49, 2018. (Cited on

page 99)

[249] Saeed Soori, Konstantin Mishchenko, Aryan Mokhtari, Maryam Mehri Dehnavi, and
Mert G¨urb¨uzbalabann. DAve-QN: a distributed averaged quasi-Newton method

162

In Proceedings of the 23rd International
with local superlinear convergence rate.
Conference on Artiﬁcial Intelligence and Statistics, pages 1965–1976. PMLR, 2020.

(Cited on pages 33 and 346)

[250] Jean-Luc Starck, Fionn Murtagh, and Jalal M. Fadili. Sparse image and signal pro-
cessing: wavelets, curvelets, morphological diversity. Cambridge University Press,
2010. (Cited on page 122)

[251] Georgios Stathopoulos, Harsh A. Shukla, Alexander Szuecs, Ye Pu, and Colin Jones.
Operator splitting methods in control. Foundations and Trends® in Systems and
Control, 3:249–362, 2016. (Cited on page 122)

[252] Sebastian U. Stich. Local SGD converges fast and communicates little. In Interna-
tional Conference on Learning Representations, 2019. (Cited on pages 24, 45, 47, 50,

173, 177, 178, and 195)

[253] Sebastian U. Stich. Uniﬁed optimal analysis of the (stochastic) gradient method.
arXiv preprint arXiv:1907.04232, 2019. (Cited on pages 64, 66, 72, 77, 93, 197, and 230)

[254] Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed SGD
with memory. Advances in Neural Information Processing Systems, 31:4447–4458,
2018. (Cited on page 29)

[255] Sebastian U. Stich and Sai Praneeth Karimireddy. The error-feedback framework:
better rates for SGD with delayed gradients and compressed updates. The Journal
of Machine Learning Research, 21:1–36, 2020. (Cited on pages 29 and 45)

[256] Thomas Strohmer and Roman Vershynin. A randomized Kaczmarz algorithm with
exponential convergence. Journal of Fourier Analysis and Applications, 15(2):262–
278, 2009. (Cited on page 112)

[257] Ruo-Yu Sun. Optimization for deep learning: an overview. Journal of the Operations

Research Society of China, 8(2):249–294, 2020. (Cited on pages 58 and 77)

[258] Martin Tak´aˇc, Avleen Bijral, Peter Richt´arik, and Nathan Srebro. Mini-batch primal
and dual methods for SVMs. In Proceedings of the 30th International Conference
on Machine Learning, pages 537–552. PMLR, 2013. (Cited on page 295)

[259] Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-Borwein step
size for stochastic gradient descent. In Advances in Neural Information Processing
Systems, volume 30, pages 685–693. PMLR, 2016. (Cited on pages 27 and 28)

163

[260] Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, and Mike Davies. The
practicality of stochastic optimization in imaging inverse problems. arXiv preprint
arXiv:1910.10100, 2019. (Cited on page 78)

[261] Jingyi K. Tay, Jerome Friedman, and Robert Tibshirani. Principal component-
guided sparse regression. Canadian Journal of Statistics, 2021. (Cited on pages 32,

135, and 323)

[262] Adrien Taylor and Francis Bach. Stochastic ﬁrst-order methods: non-asymptotic
In Proceedings of the 32nd
and computer-aided analyses via potential functions.
Conference on Learning Theory, volume 99, pages 2934–2992. PMLR, 2019. (Cited

on page 88)

[263] Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the
Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996. (Cited

on page 70)

[264] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight.
Sparsity and smoothness via the fused Lasso. Journal of the Royal Statistical Soci-
ety: Series B (Statistical Methodology), 67(1):91–108, 2005. (Cited on page 296)

[265] Tijmen Tieleman and Geoﬀrey Hinton. Lecture 6.5—RmsProp: divide the gradient
by a running average of its recent magnitude. COURSERA: Neural Networks for
Machine Learning, 2012. (Cited on page 86)

[266] Trang H. Tran, Lam M. Nguyen, and Quoc Tran-Dinh. Shuﬄing gradient-based
methods with momentum. arXiv preprint arXiv:2011.11884, 2020. (Cited on pages 72

and 77)

[267] Stephen Tu, Shivaram Venkataraman, Ashia C. Wilson, Alex Gittens, Michael I.
Jordan, and Benjamin Recht. Breaking locality accelerates block Gauss-Seidel. In
Proceedings of the 34th International Conference on Machine Learning, volume 70,
pages 3482–3491. PMLR, 2017. (Cited on page 111)

[268] Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
Simon Lacoste-Julien. Painless stochastic gradient: interpolation, line-search, and
convergence rates.
In Advances in Neural Information Processing Systems, vol-
ume 32, pages 3732––3745. Curran Associates, Inc., 2019. (Cited on page 27)

[269] Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick P´erez, and
Jean Ponce. Unsupervised image matching and object discovery as optimization. In

164

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pages 8287–8296, 2019. (Cited on page 293)

[270] B˘ang C. V˜u. A splitting algorithm for dual monotone inclusions involving cocoercive
operators. Advances in Computational Mathematics, 38(3):667–681, 2013. (Cited

on pages 112, 114, 123, and 129)

[271] Jianyu Wang and Gauri Joshi. Cooperative SGD: a uniﬁed framework for the
design and analysis of communication-eﬃcient SGD algorithms. arXiv preprint
arXiv:1808.07576, 2018. (Cited on page 45)

[272] Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael G. Rabbat. SlowMo:
improving communication-eﬃcient distributed SGD with slow momentum. In Inter-
national Conference on Learning Representations, 2019. (Cited on page 45)

[273] Shiqiang Wang, Tiﬀany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya,
Ting He, and Kevin Chan. When edge meets learning: adaptive control for resource-
constrained distributed machine learning. In IEEE Conference on Computer Com-
munications, pages 63–71, 2018. (Cited on pages 46 and 47)

[274] Yu-Xiang Wang, James Sharpnack, Alex Smola, and Ryan Tibshirani. Trend ﬁl-
tering on graphs. In Proceedings of the 18th International Conference on Artiﬁcial
Intelligence and Statistics, volume 38, pages 1042–1050. PMLR, 2015.

(Cited on

page 122)

[275] Rachel Ward, Xiaoxia Wu, and L´eon Bottou. AdaGrad stepsizes: sharp convergence
over nonconvex landscapes. In Proceedings of the 36th International Conference on
Machine Learning, volume 97, pages 6677–6686. PMLR, 2019. (Cited on page 86)

[276] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. TernGrad: ternary gradients to reduce communication in distributed deep
learning. In Advances in Neural Information Processing Systems, volume 30, pages
1509–1519. PMLR, 2017. (Cited on pages 29, 99, 100, 103, 108, 271, and 286)

[277] Blake E. Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local
In Advances in Neural Information
SGD for heterogeneous distributed learning.
Processing Systems, volume 33, pages 6281–6292. Curran Associates, Inc., 2020.

(Cited on pages 27, 29, 81, 133, and 134)

[278] Stephen J. Wright. Optimization algorithms in machine learning. NIPS Tutorial,

2010. (Cited on page 27)

165

[279] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive
variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014. (Cited

on page 31)

[280] Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, and Haibin Lin. Local AdaAlter:
communication-eﬃcient stochastic gradient descent with adaptive learning rates.
arXiv preprint arXiv:1911.09030, 2019. (Cited on page 45)

[281] Jinming Xu, Ye Tian, Ying Sun, and Gesualdo Scutari. Distributed algorithms
for composite optimization: uniﬁed and tight convergence analysis. arXiv preprint
arXiv:2002.11534, 2020. (Cited on pages 122, 124, and 132)

[282] Ming Yan. A new primal–dual algorithm for minimizing the sum of three functions
with a linear operator. Journal of Scientiﬁc Computing, 76(3):1698–1717, 2018.

(Cited on pages 123, 128, 131, 331, and 332)

[283] Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali H. Sayed. Stochastic learning
under random reshuﬄing with constant step-sizes. In IEEE Transactions on Signal
Processing, volume 67, pages 474–489, 2019. (Cited on pages 59, 60, and 62)

[284] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication
eﬃcient momentum SGD for distributed non-convex optimization. In Proceedings
of the 36th International Conference on Machine Learning. PMLR, 2019. (Cited on

pages 45 and 47)

[285] Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster conver-
gence and less communication: demystifying why model averaging works for deep
learning.
In Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence,
volume 33, page 5693–5700, 2019. (Cited on pages 46 and 47)

[286] Yue Yu and Longbo Huang. Fast stochastic variance reduced ADMM for stochastic
composition optimization. In Proceedings of the 26th International Joint Conference
on Artiﬁcial Intelligence, pages 3364–3370, 2017. (Cited on page 114)

[287] Honglin Yuan, Manzil Zaheer, and Sashank J. Reddi. Federated composite opti-

mization. arXiv preprint arXiv:2011.08474, 2020. (Cited on pages 80 and 81)

[288] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy), 68(1):49–67, 2006. (Cited on pages 70, 111, 293, and 295)

166

[289] Alp Yurtsever, B˘ang C. V˜u, and Volkan Cevher. Stochastic three-composite convex
minimization. In Advances in Neural Information Processing Systems, volume 29,
pages 4329–4337, 2016. (Cited on pages 114 and 124)

[290] Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. arXiv preprint

arXiv:1212.5701, 2012. (Cited on page 86)

[291] Chi Zhang and Qianxiao Li. Distributed optimization for over-parameterized learn-

ing. arXiv preprint arXiv:1906.06205, 2019. (Cited on page 47)

[292] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clip-
ping accelerates training: a theoretical justiﬁcation for adaptivity. In International
Conference on Learning Representations, 2019. (Cited on page 28)

[293] Yuchen Zhang, John C. Duchi, Michael I. Jordan, and Martin J. Wainwright.
Information-theoretic lower bounds for distributed statistical estimation with com-
In Advances in Neural Information Processing Systems,
munication constraints.
volume 26, pages 2328–2336, 2013. (Cited on page 51)

[294] Yuchen Zhang and Lin Xiao. DiSCO: distributed optimization for self-concordant
In Proceedings of the 32nd International Conference on Machine

empirical loss.
Learning, volume 37, pages 362–370. PMLR, 2015. (Cited on page 99)

[295] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for
regularized loss minimization. In Proceedings of the 32nd International Conference
on Machine Learning, volume 37, pages 1–9. PMLR, 2015. (Cited on page 72)

[296] Renbo Zhao and Volkan Cevher. Stochastic three-composite convex minimization
In Proceedings of the 21st International Conference on
with a linear operator.
Artiﬁcial Intelligence and Statistics, pages 765–774. PMLR, 2018. (Cited on page 124)

[297] Shuai Zheng and James T. Kwok. Fast-and-light stochastic ADMM. In Proceedings
of the 25th International Joint Conferences on Artiﬁcial Intelligence Organization,
pages 2407–2613, 2016. (Cited on page 114)

[298] Fan Zhou and Guojing Cong. On the convergence properties of a K-step averaging
stochastic gradient descent algorithm for nonconvex optimization. Proceedings of
the 27th International Joint Conference on Artiﬁcial Intelligence, pages 3219–3227,
2018. (Cited on page 45)

167

APPENDICES

Appendix A

Tables of Frequently Used Notation

Table A.1: Summary of frequently used notation.

For all chapters
Basic

k
E [·]
Eξ [·]
Ek [·]
(cid:104)·, ·(cid:105), (cid:107) · (cid:107)

min(·)

(cid:96)p norm
λmin(·), λ+
λmax(·)
∇h(x)
∇2h(x)

Iteration/epoch counter
Expectation
Expectation with respect to the variable ξ
Expectation conditioned on the information up to iteration k
Standard inner product and norm in Rd (cid:104)x, y(cid:105) = (cid:80)d

(cid:16)(cid:80)d

l=1 xp

(cid:17) 1

p

l

, p ∈ [1, +∞]

(cid:107)x(cid:107)p =
The smallest and the smallest positive eigenvalues of a matrix
The largest eigenvalue of a matrix
Gradient of a diﬀerentiable function h
The Hessian of a twice diﬀerentiable function h

i=1 xiyi; (cid:107)x(cid:107) = (cid:112)(cid:104)x, x(cid:105)

Objective

P
d
f : Rd → R
fj : Rd → R
f (·; ξ) : Rd → R
n
M
ψ : Rd → R ∪ {∞}
x(cid:63)
L
µ
κ

Full objective if there is a non-smooth term
Dimension of the primal space x ∈ Rd
Smooth part of the objective
Diﬀerentiable function (1 ≤ j ≤ n) if ﬁnite-sum setting
Almost-surely diﬀerentiable function in expectation setting
Number of samples in ﬁnite-sum setting
Number of nodes in parallel setting
Non-smooth part of the objective (proper, closed, convex)
Global optimum
(Global) smoothness constant of f
Strong convexity constant
The condition number κ = L/µ for strongly convex objectives

Other

[n]
Dh(x, y)
dom (h)
∂h
proxγh(x)
γ
γk

Set {1, 2, . . . , n}
Bregman distance Dh(x, y) = h(x) − h(y) − (cid:104)∇h(y), x − y(cid:105)
Domain of h, dom (h) = {x : h(x) < +∞}
Subdiﬀerential of h, ∂h(x) = {s : h(y) ≥ h(x) + (cid:104)s, y − x(cid:105)}, x ∈ dom (h)
Proximal operator of a function h, proxγhi(x) = arg minu
Fixed stepsize
Stepsize at iteration k

h(u) + 1

(cid:110)

2γ (cid:107)u − x(cid:107)2(cid:111)

168

Table A.2: Notation speciﬁc to Chapter 2.

Chapter 2

gk
m
xk
m
gk
¯gk
ˆxk
rk
σ2
σ2
σ2
dif
k1, k2, . . . , kp
H

opt

Stochastic gradient at time k on node m
Local iterate at time k on node m
The average of stochastic gradients across nodes at time k
Expected value of gk, E (cid:2)gk(cid:3) = ¯gk
The average of all local iterates at time k
The deviation of the average iterate from the optimum ˆxk − x(cid:63) at time k
Global uniform bound on the variance of the stochastic gradients for identical data
The variance of the stochastic gradients at the optimum for identical data
The variance of the stochastic gradients at the optimum for heterogeneous data
Timesteps at which synchronization happens in Local SGD
Maximum number of local steps between communications, H = maxp |kp − kp+1|

Table A.3: Notation speciﬁc to Chapters 3 and 4.

Chapters 3 and 4

A permutation π = (π0, π1, . . . , πn−1) of {1, 2, . . . , n}
Fixed for Shuﬄe-Once and resampled every epoch for Random Reshuﬄing.
The current iterate after i steps in epoch k, for 0 ≤ i ≤ n
The sum of gradients used over epoch k such that xk+1 = xk − γgk
The variance of the individual loss gradients from the average loss at point xk
Assumption 3.4.7 constants
Functional suboptimality, δk = f (xk) − f (cid:63), where f (cid:63) = inf x f (x)
The squared iterate distance from an optimum for convex losses rk = (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

Table A.4: Notation speciﬁc to Chapter 5.

Chapter 5

Estimated gradient Lipschitz constant
Adaptive stepsize
Estimated strong convexity
Stepsize ratio, θk = γk
γk−1
Strong convexity ratio, Θk = µk
µk−1
Estimated momentum
The weight of xi in the ergodic iterate ˆxK
Lyapunov function

Table A.5: Notation speciﬁc to Chapter 6.

Chapter 6

The sign of t (−1 if t < 0, 0 if t = 0 and 1 if t > 1)
The j-th element of x ∈ Rd
Stochastic gradient at time k on node i
The average of stochastic gradients across nodes at time k
Variance of the stochastic gradient gk
i
Variance of the stochastic gradient gk
Stochastic approximation of ∇fi(x(cid:63))
∆k
Full p-quantization of vector ∆

i = gk

i − hk
i

π

xk
i
gk
σ2
k
A, B
δk
rk

Lk
γk
µk
θk
Θk
βk
wi
Ψk

sign(t)
x(j)
gk
i
gk
σ2
i
σ2
hk
i
∆k
i
Quantp(∆)

dl
m
β
ˆ∆k
i
ˆ∆
ˆgk
i
ˆgk
vk
Ψl(x)

Ψ(x)

(cid:101)d

V k
ζ
EQk [·]

X (cid:63)
σ2
(cid:63)

g

gj
φj
vk
yk
1 , . . . , yk
j
pj
γ
ηj
Lk
Y k
Lj
ν

χC(x)

ΠC(x)

169

Size of the l-th block for quantization
Number of blocks for quantization
Momentum parameter
Block p-quantization of ∆k
ˆ∆k = 1
ˆ∆k
i
M
Stochastic approximation of ∇fi(xk)
gk = 1
M
Stochastic gradient with momentum vk = βvk−1 + ˆgk
Variance of the l-th block Ψl(x) = (cid:107)x(l)(cid:107)1(cid:107)x(l)(cid:107)p − (cid:107)x(l)(cid:107)2

i = gk

i − hk
i

i=1 gk
i

(cid:80)M

(cid:80)M

i=1

Variance of the quantized vector Ψ(x) =

Ψl(x)

m
(cid:80)
l=1

dl

l=1,...,m

(cid:101)d = max
Lyapunov function V k = (cid:107)xk − x(cid:63)(cid:107)2 + cγ2
M
Data dissimilarity, 1
M
Expectation with respect to quantization

i=1 (cid:107)hk
i=1 (cid:107)∇fi(x) − ∇f (x)(cid:107)2 ≤ ζ 2

(cid:80)M

(cid:80)M

i − h(cid:63)
i (cid:107)

Table A.6: Notation speciﬁc to Chapter 7.

Chapter 7

Set of optimal solutions X (cid:63) = {x(cid:63) ∈ Rd : P (x) ≥ P (x(cid:63)) ∀x ∈ Rd}
Gradient noise at optimum σ2
g(x) = 1
m

(cid:2)(cid:107)∇f (x(cid:63); ξ) − ∇f (x(cid:63))(cid:107)2(cid:3)

gj(x) (proper, closed, convex)

(cid:63) = Eξ

m
(cid:80)
j=1

j x), Aj ∈ Rd×dj

gj : Rd → R ∪ {+∞} (proper, closed, convex)
Structured form of gj gj(x) = φj(A(cid:62)
Estimator of ∇f (xk)
Dual variables
Probability of selecting index j
Stepsize associated with f and ψ
Stepsize associated with gj, ηj = γ
mpj
Lyapunov function Lk = E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + Mk + Y k
E (cid:2)(cid:107)yk
Dual distance to solution Y k = (1 + ν) (cid:80)m
Smoothness constant of gj, Lj ∈ R ∪ {+∞} (Lj = +∞ if gj is non-smooth)
(ν = 0 if any gj is non-smooth)
ν = minj=1,...,m

l − y(cid:63)

l=1 η2
l

l (cid:107)2(cid:3)

1
ηj Lj

Characteristic function of a set C, χC(x) =

(cid:40)

0
x ∈ C
+∞ x /∈ C

Projection onto a set C, ΠC(x) = proxχC (x) = arg minu∈C (cid:107)u − x(cid:107)

Table A.7: Notation speciﬁc to Chapter 8.

L
X
Y
L : X × Y → R
y(cid:63)
τ
P
(Ω, F , (Fk)k, P)
V k

Chapter 8

Linear operator from the objective
Primal space
Dual space
Lagrangian function
Dual solution
Dual stepsize
Linear operator that deﬁnes the primal-dual metric
Probability space
Lyapunov function

170

Appendix B

Appendix for Chapter 2

B.1 Proofs for Identical Data under Assumption 2.3.2

B.1.1 Proof of Lemma 2.4.1

Proof. Let k ∈ N be such that kp ≤ k ≤ kp+1 − 1. Recall that for a time k such that
m and ˆxk+1 = ˆxk −γgk. Hence for the expectation
kp ≤ k < kp+1 we have xk+1
2, . . . , xk
1, xk
conditional on xk
2(cid:105)

m −γgk
M we have:

m = xk

2(cid:105)

E

(cid:104)(cid:13)
(cid:13)xk+1

m − ˆxk+1(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)gk
(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)
m − gk(cid:13)
(cid:13)

2(cid:105)

− 2γE (cid:2)(cid:10)xk

m − ˆxk, gk

− 2γ (cid:10)xk

m − ˆxk, ∇f (xk

m − gk(cid:11)(cid:3)
m)(cid:11)

+ γ2E

(cid:13)xk

= (cid:13)
= (cid:13)

m − ˆxk(cid:13)
2
(cid:13)
m − ˆxk(cid:13)
2
(cid:13)

(cid:13)xk
+ 2γ (cid:10)xk

+ γ2E
m − ˆxk, ¯gk(cid:11) .

Averaging both sides and letting V k = 1
M

(cid:80)

m

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2, we have
(cid:13)

E (cid:2)V k+1(cid:3) = V k +

= V k +

γ2
M

γ2
M

(cid:88)

E

m

(cid:88)

E

m

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

−

−

2γ
M

2γ
M

(cid:88)

m

(cid:88)

m

Now note that by expanding the square we have,

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) + 2γ (cid:10)ˆxk − ˆxk, ¯gk(cid:11)
(cid:125)
(cid:124)

(cid:123)(cid:122)
=0

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) .

(B.1)

E

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

= E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk(cid:13)
(cid:13)

2(cid:105)

+ E

(cid:104)(cid:13)
(cid:13)¯gk − gk(cid:13)
(cid:13)

2(cid:105)

+ 2E (cid:2)(cid:10)gk

m − ¯gk, ¯gk − gk(cid:11)(cid:3) .

(B.2)

We decompose the ﬁrst term in the last equality again by expanding the square,

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk(cid:13)
(cid:13)

2(cid:105)

= E

= E

(cid:104)(cid:13)
(cid:13)gk
(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

2(cid:105)

(cid:13)
(cid:13)

+ (cid:13)
+ (cid:13)

(cid:13)¯gk

(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)
m − ¯gk(cid:13)
2
(cid:13)

+ 2E (cid:2)(cid:10)gk

m − ¯gk

m − ¯gk(cid:11)(cid:3)

+ 2 (cid:10)¯gk
(cid:124)

m − ¯gk

m, ¯gk
m − ¯gk(cid:11)
(cid:125)

m, ¯gk
(cid:123)(cid:122)
=0

= E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

+ (cid:13)

(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

.

Plugging this into (B.2) we have,

171

E

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

= E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

+ (cid:13)

(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

+ E

(cid:104)(cid:13)
(cid:13)¯gk − gk(cid:13)
(cid:13)

2(cid:105)

+ 2E (cid:2)(cid:10)gk

m − ¯gk, ¯gk − gk(cid:11)(cid:3) .

Now average over m:

1
M

(cid:88)

E

m

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

=

1
M

− 2E

(cid:88)

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

+

m
(cid:104)(cid:13)
(cid:13)¯gk − gk(cid:13)
(cid:13)

2(cid:105)

,

1
M

(cid:88)

m

(cid:13)
(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

+ E

(cid:104)(cid:13)
(cid:13)¯gk − gk(cid:13)
(cid:13)

2(cid:105)

where we used that by deﬁnition 1
M

(cid:80)M

m=1 gk

m = gk. Hence,

1
M

(cid:88)

E

m

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

=

≤

1
M

1
M

(cid:88)

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

+

m
(cid:88)

m

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

+

1
M

1
M

(cid:88)

m
(cid:88)

m

(cid:13)
(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

− E

(cid:104)(cid:13)
(cid:13)¯gk − gk(cid:13)
(cid:13)

2(cid:105)

(cid:13)
(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

.

(B.3)

Now note that for the ﬁrst term in (B.3) we have by Assumption 2.3.2,

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

= E

For the second term in (B.3) we have

(cid:104)(cid:13)
(cid:13)gk

m − ∇f (xk

m)(cid:13)
(cid:13)

2(cid:105)

≤ σ2.

(B.4)

(cid:13)
(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

= (cid:13)

(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)∇f (ˆxk) − ¯gk(cid:13)
2
(cid:13)

+ 2 (cid:10)¯gk

m − ∇f (ˆxk), ∇f (ˆxk) − ¯gk(cid:11) .

Averaging over m,

M
(cid:88)

(cid:13)
(cid:13)¯gk

m − ¯gk(cid:13)
2
(cid:13)

1
M

=

=

=

m=1
1
M

(cid:88)

m
(cid:88)

1
M

1
M

(cid:13)
(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)∇f (ˆxk) − ¯gk(cid:13)
2
(cid:13)

+ 2 (cid:10)¯gk − ∇f (ˆxk), ∇f (ˆxk) − ¯gk(cid:11)

(cid:13)
(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)∇f (ˆxk) − ¯gk(cid:13)
2
(cid:13)

− 2(cid:13)

(cid:13)∇f (ˆxk) − ¯gk(cid:13)
2
(cid:13)

m
(cid:88)

m

(cid:13)
(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

− (cid:13)

(cid:13)∇f (ˆxk) − ¯gk(cid:13)
2
(cid:13)

≤

1
M

(cid:88)

m

(cid:13)
(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

,

where we used the fact that 1
M
tion. Now we bound (cid:13)

m = ¯gk, which comes from the linearity of expecta-
m ¯gk
m − ∇f (ˆxk)(cid:13)
2 in the last inequality by smoothness and then use
(cid:13)

(cid:13)¯gk

(cid:80)

that Jensen’s inequality implies (cid:80)M

172
m=1(f (ˆxk) − f (xk

m)) ≤ 0,

1
M

(cid:88)

m

(cid:13)
(cid:13)¯gk

m − ∇f (ˆxk)(cid:13)
2
(cid:13)

=

1
M

(1.18)
≤

(cid:88)

(cid:13)
(cid:13)∇f (xk

m) − ∇f (ˆxk)(cid:13)
2
(cid:13)

m
1
M

(cid:88)

m

2L(f (ˆxk) − f (xk

m) − (cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11))

≤

2L
M

(cid:88)

m

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) .

Plugging in (B.5) and (B.4) into (B.3) we have,

1
M

(cid:88)

E

m

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

≤ σ2 +

2L
M

(cid:88)

m

Plugging (B.6) into (B.1), we get

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) .

(B.5)

(B.6)

E (cid:2)V k+1(cid:3) ≤ V k + γ2σ2 −

= V k + γ2σ2 −

2γ
M

m
2γ(1 − γL)
M

(cid:88)

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) +

2Lγ2
M

(cid:88)

m

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11)

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11)

(cid:88)

m

(B.7)

(1.16)
≤ (1 − γ(1 − γL)µ) V k + γ2σ2.

Using that γ ≤ 1

2L we can conclude,

E (cid:2)V k+1(cid:3) ≤

(cid:16)

1 −

(cid:17)

γµ
2

V k + γ2σ2

≤ V k + γ2σ2.

Taking expectations and iterating the above inequality,

E (cid:2)V k(cid:3) ≤ E (cid:2)V kp(cid:3) + γ2σ2 (k − kp)

≤ E (cid:2)V kp(cid:3) + γ2σ2 (kp+1 − kp − 1)
≤ E (cid:2)V kp(cid:3) + γ2σ2 (H − 1) .

It remains to notice that by assumption we have V kp = 0.

(cid:4)

B.1.2 Two more lemmas

173

Lemma B.1.1. [252].
identical data. Suppose that f satisﬁes Assumption 2.3.1 and that γ ≤ 1

m)k be the iterates generated by Algorithm 1 run with

Let (xk

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)E

2(cid:105)

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)
E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γLE (cid:2)V k(cid:3) .

+ γ2E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

−

γ
2

2L . Then,
2(cid:105)

(B.8)

(cid:4)

Proof. This is Lemma 3.1 in [252].

Lemma B.1.2. Suppose that Assumption 2.3.2 holds. Then,

E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

≤

σ2
M

.

Proof. This is Lemma 3.2 in [252]. Because the stochastic gradients gk
we have that the variance of their sum is the sum of their variances, hence

m are independent

E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

=

1
M 2

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

M
(cid:88)

m − ¯gk
gk
m

m=1

2

 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M 2

M
(cid:88)

m=1

E

(cid:104)(cid:13)
(cid:13)gk

m − ¯gk
m

2(cid:105)

(cid:13)
(cid:13)

≤

σ2
M

.

(cid:4)

B.1.3 Proof of Theorem 2.4.2

Proof. Combining Lemma B.1.1 and Lemma B.1.2, we have

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)E

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

γ2σ2
M

−

γ
2

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γLE (cid:2)V k(cid:3) .
(B.9)

Using Lemma 2.4.1 we can upper bound the E (cid:2)V k(cid:3) term in (B.9):

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1−γµ)E

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

γ2σ2
M

−

γ
2

E (cid:2)Df (ˆxk, x(cid:63))(cid:3)+2γ3L (H − 1) σ2.

Letting rk+1 = ˆxk+1 − x(cid:63), we have

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

γ2σ2
M

+ 2γ3L (H − 1) σ2.

Recursing the above inequality we have,

E

(cid:104)(cid:13)
(cid:13)rK(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

(cid:32)K−1
(cid:88)

k=0

(1 − γµ)k

(cid:33) (cid:18) γ2σ2
M

+ 2γ3L (H − 1) σ2

(cid:19)

.

Using that (cid:80)K−1

k=0 (1 − γµ)k ≤ (cid:80)∞

174
k=0 (1 − γµ)k = 1

γµ we have,

E

(cid:104)(cid:13)
(cid:13)rK(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

γσ2
µM

+

2γ2L (H − 1) σ2
µ

,

which is the claim of this theorem.

(cid:4)

B.1.4 Proof of Theorem 2.4.4

Proof. Let rk = ˆxk − x(cid:63), then putting µ = 0 in Lemma B.1.1 and combining it with
Lemma B.1.2, we have

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

γ2σ2
M

−

γ
2

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γLE (cid:2)V k(cid:3) .

Further using Lemma 2.4.1,

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

γ2σ2
M

−

γ
2

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ3L (H − 1) σ2.

Rearranging we have,

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤ E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

− E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

+

γ
2

γ2σ2
M

+ 2γ3L (H − 1) σ2.

Averaging the above equation as k varies between 0 and K − 1,

γ
2K

K−1
(cid:88)

k=0

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤

1
K

K−1
(cid:88)

(cid:16)

k=0

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

− E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)(cid:17)

+ 2γ3L (H − 1) σ2

(cid:19)

K−1
(cid:88)

+

1
K

k=0
(cid:107)x0 − x(cid:63)(cid:107)2 − E

(cid:18) γ2σ2
M
(cid:104)(cid:13)
(cid:13)rK(cid:13)
(cid:13)

2(cid:105)

=

≤

(cid:107)x0 − x(cid:63)(cid:107)2
K

K

+

γ2σ2
M

+

γ2σ2
M

+ 2γ3L (H − 1) σ2

+ 2γ3L (H − 1) σ2.

(B.10)

By Jensen’s inequality we have Df (¯xK, x(cid:63)) ≤ 1
K
we have,

(cid:80)K−1

k=0 Df (ˆxk, x(cid:63)). Using this in (B.10)

E (cid:2)Df (¯xK, x(cid:63))(cid:3) ≤

γ
2

(cid:107)x0 − x(cid:63)(cid:107)2
K

+

γ2σ2
M

+ 2γ3L (H − 1) σ2.

Dividing both sides by γ/2 yields the theorem’s claim.

(cid:4)

B.2 Proofs for Identical Data under Assumption 2.3.3

175

B.2.1 Preliminary lemmas

Lemma B.2.1 (Individual gradient variance bound). Assume that Assumption 2.3.3 holds
with identical data, then for all k ≥ 0 and m ∈ [M ] we have

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

≤ 4LDf (xk

m, x(cid:63)) + 2σ2
m,

(B.11)

where σ2
m

def= Eξm∼Dm

(cid:2)(cid:107)∇f (x(cid:63); ξm)(cid:107)2(cid:3) is the noise at the optimum on the m-th node.

Proof. Using that gk

m; ξm) for some ξm ∼ Dm,

m = ∇f (xk
m; ξm)(cid:13)
2
(cid:13)

(cid:13)
(cid:13)gk
m

(cid:13)
2
(cid:13)

= (cid:13)

(1.10)

(cid:13)∇f (xk
≤ 2(cid:13)

(cid:13)∇f (xk
≤ 4L (cid:0)f (xk

(1.18)

m; ξm) − ∇f (x(cid:63); ξm)(cid:13)
2
(cid:13)
m; ξm) − f (x(cid:63); ξm) − (cid:10)∇f (x(cid:63); ξm), xk

+ 2(cid:107)∇f (x(cid:63); ξm)(cid:107)2

m − x(cid:63)(cid:11)(cid:1) + 2(cid:107)∇f (x(cid:63); ξm)(cid:107)2.

Taking expectations and using that E [∇f (x(cid:63); ξ)] = ∇f (x(cid:63)) = 0 we get,

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

≤ 4L (cid:0)f (xk

m) − f (x(cid:63))(cid:1) + 2σ2
m, x(cid:63)) + 2σ2
m.

m

= 4LDf (xk

(cid:4)

Lemma B.2.2 (Average gradient variance reduction). Assume that Assumption 2.3.3
holds with identical data, then for all k ≥ 0 and for M nodes we have,

E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

≤

opt

2σ2
M

+

4L
M 2

M
(cid:88)

m=1

Df (xk

m, x(cid:63)).

(B.12)

Proof. Using the deﬁnition of gk and ¯gk,

E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

= E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

m − ∇f (xk
gk

2



(cid:13)
(cid:13)
(cid:13)
m)
(cid:13)
(cid:13)

=

1
M 2

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

M
(cid:88)

m=1

(cid:0)gk

m − ∇f (xk

 .

(B.13)

2

(cid:13)
(cid:13)
m)(cid:1)
(cid:13)
(cid:13)
(cid:13)

The sum in (B.13) is the variance of a sum of independent random variables and hence can
be decomposed into the sum of their individual variances which we can use Lemma B.2.1

to bound:

E

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

=

1
M 2

176

M
(cid:88)

E

m=1
M
(cid:88)

(1.6)
≤

1
M 2

(B.11)
≤

1
M 2

m=1
M
(cid:88)

m=1

(cid:104)(cid:13)
(cid:13)gk

m − ∇f (xk

m)(cid:13)
(cid:13)

2(cid:105)

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

(cid:0)2σ2

m + 4LDf (xk

m, x(cid:63))(cid:1)

=

opt

2σ2
M

+

4L
M 2

M
(cid:88)

m=1

Df (xk

m, x(cid:63)),

where in the last equality we used that σ2

opt is by deﬁnition equal to (cid:80)M

m=1 σ2

m/M .

(cid:4)

The next Lemma bounds the optimality gap across one iteration when the descent step
m), i.e., when the expectation of the Local SGD update is

m=1 ∇f (xk

(cid:80)M

is ˆxk+1 = ˆxk − γ
M
used.

Lemma B.2.3 (Perturbed iterate analysis). Suppose that Assumptions 2.3.1, and 2.3.3
hold with identical data. Then,

(cid:13)ˆxk+1 − γ¯gk − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)
(cid:18)(cid:18)
2γ
M

M
(cid:88)

+

m=1

+ 2γLV k
(cid:19)

γL −

1
2

(cid:0)f (xk

m) − f (x(cid:63))(cid:1) −

2(cid:19)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

µ
2

(B.14)

Proof. This is the ﬁrst part of Lemma 3.1 in [252] and we reproduce it for completeness:

177

(cid:13)ˆxk − x(cid:63) − γ¯gk(cid:13)
(cid:13)
2
(cid:13)
+ γ2(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
= (cid:13)
2
(cid:13)

(cid:13)¯gk(cid:13)
2
(cid:13)

− 2γ (cid:10)ˆxk − x(cid:63), ¯gk(cid:11)

= (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ γ2(cid:13)

(cid:13)¯gk(cid:13)
2
(cid:13)

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇f (xk

m)(cid:11)

(1.7)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+

M
(cid:88)

(cid:13)
(cid:13)∇f (xk

m)(cid:13)
2
(cid:13)

−

γ2
M

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − xk

m + xk

m − x(cid:63), ∇f (xk

m)(cid:11)

= (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+

γ2
M

(cid:13)
(cid:13)∇f (xk

m) − ∇f (x(cid:63))(cid:13)
2
(cid:13)

−

2γ
M

M
(cid:88)

m=1

(cid:10)xk

m − x(cid:63), ∇f (xk

m)(cid:11)

m=1
M
(cid:88)

m=1

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11)

(1.18)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+

2Lγ2
M

M
(cid:88)

m=1

(cid:0)f (xk

m) − f (x(cid:63))(cid:1) −

2γ
M

M
(cid:88)

m=1

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11)

(cid:10)xk

m − x(cid:63), ∇f (xk

m)(cid:11)

(1.16)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+

2γ
M

M
(cid:88)

(cid:16)

m=1

(γL − 1) (cid:0)f (ˆxk

m) − f (x(cid:63))(cid:1) −

µ
2

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
(cid:13)

2(cid:17)

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11) .

(B.15)

To bound the last term in (B.15) we use the generalized Young’s inequality 2 (cid:104)a, b(cid:105) ≤
ζ(cid:107)a(cid:107)2 + ζ −1(cid:107)b(cid:107)2 with ζ = 2L:

−2 (cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11) (1.11)

≤ 2L(cid:13)

+

m − ˆxk(cid:13)
2
(cid:13)

(cid:13)xk
m − ˆxk(cid:13)
2
(cid:13)

+

(cid:13)
(cid:13)∇f (xk

1
2L
(cid:13)
(cid:13)∇f (xk

m)(cid:13)
2
(cid:13)
m) − f (x(cid:63))(cid:13)
2
(cid:13)

= 2L(cid:13)

(cid:13)xk

(1.18)

≤ 2L(cid:13)

(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

m) − f (x(cid:63))(cid:1) .

(B.16)

1
2L
+ (cid:0)f (xk

Finally, using (B.16) in (B.15) we get,

(cid:13)ˆxk − γ¯gk − x(cid:63)(cid:13)
(cid:13)
(cid:13)

2 (B.15),(B.16)
≤

(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

+

2γL
M

M
(cid:88)

m=1

(cid:13)
(cid:13)ˆxk − xk
m

(cid:13)
2
(cid:13)

+

2γ
M

M
(cid:88)

(cid:18)(cid:18)

γL −

m=1

(cid:19)

1
2

(cid:0)f (ˆxk

m) − f (x(cid:63))(cid:1) −

2(cid:19)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

µ
2

178

(cid:4)

Lemma B.2.4 (Single-iterate optimality gap analysis). Suppose that Assumptions 2.3.1
and 2.3.3 hold with identical data. Choose a stepsize γ > 0 such that γ ≤

where M is the number of nodes, then for expectation conditional on xk
have

1
4L(1+ 2

M )
M we

1, xk

2, . . . , xk

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ 2γLV k +

2γ2σ2
M

opt

−

γ
2

(cid:0)f (ˆxk) − f (x(cid:63))(cid:1) ,
(B.17)

(cid:13)
where ˆxk = 1
(cid:13)xk
M
the diﬀerent nodes from their mean at timestep k.

m and V k def= 1

m=1 xk

(cid:80)M

(cid:80)M

m=1

M

m − ˆxk(cid:13)
2 is the iterate variance across
(cid:13)

Proof. This is a modiﬁcation of Lemma 3.1 in [252]. For expectation conditional on
(xk

m=1 and using Lemma B.2.3,

m)M

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105) (1.4)

= (cid:13)

(cid:13)ˆxk − x(cid:63) − γ¯gk(cid:13)
2
(cid:13)

+ γ2E

(B.14)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ 2γLV k + γ2E

2(cid:105)

(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)
(cid:104)(cid:13)
(cid:13)gk − ¯gk(cid:13)
(cid:13)

2(cid:105)

+

2γ
M

M
(cid:88)

m=1

(cid:18)(cid:18)

γL −

(cid:19)

1
2

(cid:0)f (xk

m) − f (x(cid:63))(cid:1) −

2(cid:19)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

µ
2

Now use Lemma B.2.2 to bound (cid:13)

(cid:13)gk − ¯gk(cid:13)
2:
(cid:13)

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105) (B.12)

≤ (cid:13)

+ 2γLV k +

2γ2σ2
M

opt

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)
(cid:18)(cid:18)

M
(cid:88)

γL +

+

2γ
M

m=1

2γL
M

−

(cid:19)

1
2

(cid:0)f (xk

m) − f (x(cid:63))(cid:1) −

2(cid:19)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

µ
2

(B.18)

We now use that the stepsize γ ≤

1
4L(1+ 2

M )

to bound the last term in (B.18),

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ 2γLV k +

opt

2γ2σ2
M
m) − f (x(cid:63))(cid:1) −

+

2γ
M

M
(cid:88)

(cid:18)

m=1

−

1
4

(cid:0)f (xk

2(cid:19)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

µ
2

(B.19)

Applying Jensen’s inequality from Proposition 1.10.1 to 1
4

(cid:0)f (xk

m) − f (x(cid:63))(cid:1)+ µ

2

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
2,
(cid:13)

we obtain

−

1
M

M
(cid:88)

m=1

(cid:18) 1
4

(cid:0)f (xk

m) − f (x(cid:63))(cid:1) +

µ
2

Plugging (B.20) in (B.19), we get

179

2(cid:19) (1.13)
m − x(cid:63)(cid:13)
(cid:13)

≤ −

(cid:13)
(cid:13)xk

−

(cid:0)f (ˆxk) − f (x(cid:63))(cid:1)

(B.20)

(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

.

(B.21)

1
4
µ
2

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105) (B.19),(B.20)

≤

(1 − γµ) (cid:13)
− γ
2

(cid:13)ˆxk − x(cid:63)(cid:13)
2 + 2γLV k + 2γ2σ2
(cid:13)
(cid:0)f (ˆxk) − f (x(cid:63))(cid:1) ,

M

opt

which is the claim of this lemma.

(cid:4)

Lemma B.2.5 (Bounding the deviation of the gradients from their average). Under
Assumptions 2.3.1 and 2.3.3 for identical data we have for all k ≥ 0,

1
M

M
(cid:88)

E

m=1





(cid:13)
(cid:13)
gk
(cid:13)
m −
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

gk
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
 ≤ 2σ2

opt +

4L
M

M
(cid:88)

m=1

Df (xk

m, x(cid:63)).

(B.22)

Proof. We start by the variance bound,

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
gk
(cid:13)
m −
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

gk
m

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1.5)
=

M
(cid:88)

(cid:13)
(cid:13)gk
m

(cid:13)
2
(cid:13)

−

1
M

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

gk
m

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m=1
M
(cid:88)

m=1

(cid:13)
(cid:13)gk
m

(cid:13)
2
(cid:13)

.

≤

1
M

We now take expectations and use Lemma B.2.1:

1
M

M
(cid:88)

E

m=1





(cid:13)
(cid:13)
gk
(cid:13)
m −
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

gk
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 ≤

≤

1
M

1
M

M
(cid:88)

m=1

M
(cid:88)

m=1

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

(cid:0)2σ2

m + 4LDf (xk

m, x(cid:63))(cid:1)

= 2σ2

opt +

4L
M

M
(cid:88)

m=1

Df (xk

m, x(cid:63)).

(cid:4)

Lemma B.2.6. Suppose that Assumptions 2.3.1, and 2.3.3 hold for identical data.
Choose γ ≤ 1

2L , then for all k ≥ 0:
E (cid:2)V k+1(cid:3) ≤ (1 − γµ) E (cid:2)V k(cid:3) + 2γE (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt.

(B.23)

180

Proof. If k+1 = kp for some p ∈ N then the left hand side is zero and the above inequality
is trivially satisﬁed. If not, then recall that xk+1
m and ˆxk+1 = ˆxk − γgk where
(cid:3) = ∇f (xk
E (cid:2)gk
m) and gk = 1
m. Hence, for the expectation conditional on
m
M
m)M
(xk
m=1 we have
(cid:104)(cid:13)
(cid:13)xk+1

m − ˆxk+1(cid:13)
(cid:13)

m = xk

m − γgk

m=1 gk

= E

(cid:80)M

2(cid:105)

2(cid:105)

E

(cid:104)(cid:13)
(cid:13)xk
(cid:104)(cid:13)
(cid:13)xk
(cid:104)(cid:13)
(cid:13)xk

m − ˆxk − γ (cid:0)gk
(cid:13)
(cid:13)

m − ˆxk
m

2(cid:105)

+ γ2E

m − ˆxk
m

2(cid:105)

(cid:13)
(cid:13)

+ γ2E

m − gk(cid:1)(cid:13)
(cid:13)
(cid:104)(cid:13)
m − gk(cid:13)
(cid:13)gk
(cid:13)
(cid:104)(cid:13)
m − gk(cid:13)
(cid:13)gk
(cid:13)

2(cid:105)

2(cid:105)

= E

= E

− 2γE (cid:2)(cid:10)xk

m − ˆxk, gk

− 2γ (cid:10)xk

m − ˆxk, ∇f (xk

m − gk(cid:11)(cid:3)
m) − ¯gk(cid:11) ,

where ¯gk = E

(cid:104) 1
M

(cid:80)M

m=1 gk
m

(cid:105)

= 1
M

(cid:80)M

m=1 ∇f (xk

m). Averaging over m in the last equality,

E (cid:2)V k+1(cid:3) = V k +

M
(cid:88)

E

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

γ2
M

m=1
M
(cid:88)

m=1
M
(cid:88)

m=1

γ2
M

+

2γ
M

(cid:10)xk

m − ˆxk, ¯gk(cid:11)

= V k +

E

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

+ 2γ (cid:10)ˆxk − ˆxk, ¯gk(cid:11)
(cid:125)

(cid:124)

(cid:123)(cid:122)
=0

= V k +

γ2
M

M
(cid:88)

E

m=1

(cid:104)(cid:13)
(cid:13)gk

m − gk(cid:13)
(cid:13)

2(cid:105)

−

2γ
M

M
(cid:88)

m=1

−

2γ
M

M
(cid:88)

m=1

−

2γ
M

M
(cid:88)

m=1

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11)

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11)

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) .

(B.24)

We now use Lemma B.2.5 to bound the second term in (B.24),

E (cid:2)V k+1(cid:3) (B.22)

≤ V k +

4Lγ2
M

M
(cid:88)

m=1

Df (xk

m, x(cid:63)) + 2γ2σ2

opt −

2γ
M

M
(cid:88)

m=1

We now use Assumption 2.3.1 to bound the last term in (B.25):

(cid:10)ˆxk − xk

m, ∇f (xk

m)(cid:11) (1.16)

≤ f (ˆxk) − f (xk

m) −

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

.

µ
2

(cid:10)xk

m − ˆxk, ∇f (xk

m)(cid:11) .

(B.25)

(B.26)

Plugging (B.26) into (B.25),

181

E (cid:2)V k+1(cid:3) ≤ (1 − γµ) V k + 2γ2σ2

opt +

4Lγ2
M

M
(cid:88)

m=1

(cid:0)f (xk

m) − f (x(cid:63))(cid:1)

+

2γ
M

M
(cid:88)

m=1

(cid:0)f (ˆxk) − f (xk

m)(cid:1) .

(B.27)

(B.28)

Using that γ ≤ 1

2L in (B.27),

E (cid:2)V k+1(cid:3) ≤ (1 − γµ) V k + 2γ2σ2

opt +

2γ
M

M
(cid:88)

m=1

= (1 − γµ) V k + 2γ2σ2

opt + 2γ (cid:0)f (ˆxk) − f (x(cid:63))(cid:1) .

(cid:0)f (xk

m) − f (x(cid:63)) + f (ˆxk) − f (xk

m)(cid:1)

Taking unconditional expectations and using the tower property yields the lemma’s state-
(cid:4)
ment.

Lemma B.2.7 (Epoch iterate deviation bound). Suppose that Assumptions 2.3.1, and
2.3.3 hold with identical data. Assume that Algorithm 1 is run with stepsize γ > 0, let
p ∈ N be such that kp is a synchronization point then for v = kp+1 − 1 we have for
α def= 1 − γµ,

v
(cid:88)

k=kp

αv−k · E (cid:2)V k(cid:3) ≤

2γ (H − 1)
α

v
(cid:88)

k=kp

Proof. We start with Lemma B.2.6,

αv−k · E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt (H − 1)

v
(cid:88)

k=kp

αv−k.

E (cid:2)V k(cid:3) ≤ (1 − γµ) E (cid:2)V k−1(cid:3) + 2γE (cid:2)Df (ˆxk−1, x(cid:63))(cid:3) + 2γ2σ2
= α · E (cid:2)V k−1(cid:3) + 2γE (cid:2)Df (ˆxk−1, x(cid:63))(cid:3) + 2γ2σ2

opt.

opt

By assumption there is some synchronization point p ∈ N such that kp ≤ k ≤ kp+1 − 1
and kp+1 − kp ≤ H, recursing the above inequality until kp and using that V kp = 0,

E (cid:2)V k(cid:3) ≤ αk−kpE (cid:2)V kp(cid:3) + 2γ

k−1
(cid:88)

j=kp

αk−j−1E (cid:2)Df (ˆxj, x(cid:63))(cid:3) + 2σ2

opt

k−1
(cid:88)

j=kp

γ2αk−1−j

=

2γ
α

k−1
(cid:88)

j=kp

αk−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3) + 2γ2σ2

opt

t−1
(cid:88)

j=kp

αk−1−j.

(B.29)

The second term in (B.29) can be bounded as follows: because α ≤ 1 then αk−1−j ≤ 1

for j ≤ k − 1, hence for k ≤ kp+1 − 1

182

2γ2σ2

opt

t−1
(cid:88)

j=kp

αk−1−j ≤ 2γ2σ2

opt

k−1
(cid:88)

1

= 2γ2σ2
≤ 2γ2σ2
≤ 2γ2σ2

j=kp
opt (k − kp)
opt (kp+1 − kp − 1)
opt (H − 1) .

(B.30)

Using (B.30) in (B.29),

E (cid:2)V k(cid:3) ≤

2γ
α

k−1
(cid:88)

j=kp

αk−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3) + 2γ2σ2

opt (H − 1) .

(B.31)

Then summing up (B.31) weighted by αv−k for v = kp+1 − 1,

v
(cid:88)

k=kp

αv−kE (cid:2)V k(cid:3) ≤

2γ
α

v
(cid:88)

αv−k

k−1
(cid:88)

k=kp

j=kp

αk−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3) + 2γ2

v
(cid:88)

k=kp

αv−kσ2

opt (H − 1) .

(B.32)

We now bound the ﬁrst term in (B.32) by adding more terms in the inner sum, since
Df (ˆxj, x(cid:63)) ≥ 0 and k − 1 ≤ v − 1 ≤ v:

v
(cid:88)

αv−k

k−1
(cid:88)

k=kp

j=kp

αk−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3) ≤

=

v
(cid:88)

k=kp
v
(cid:88)

αv−k

v
(cid:88)

j=kp

αk−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3)

v
(cid:88)

αv−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3)

k=kp

j=kp

= (v − kp)

v
(cid:88)

j=kp

αv−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3)

= (kp+1 − kp − 1)

v
(cid:88)

j=kp

αv−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3)

≤ (H − 1)

v
(cid:88)

j=kp

αv−jE (cid:2)Df (ˆxj, x(cid:63))(cid:3) .

(B.33)

Combining (B.33) and (B.32) we have,

v
(cid:88)

k=kp

αv−k · E (cid:2)V k(cid:3) ≤

2γ (H − 1)
α

v
(cid:88)

j=kp

αv−j · E (cid:2)Df (ˆxj, x(cid:63))(cid:3) + 2γ2σ2

opt (H − 1)

Finally, renaming the variable j gives us the claim of this lemma.

v
(cid:88)

k=kp

αv−k.

(cid:4)

B.2.2 Proof of Theorem 2.4.6

183

Proof. Let (kp)p index all the times k at which communication and averaging happen.
Taking expectations in Lemma B.2.4 and letting rk = ˆxk − x(cid:63),

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+ 2γLE (cid:2)V k(cid:3) +

2γ2σ2
M

opt

−

γ
2

E (cid:2)Df (ˆxk, x(cid:63))(cid:3)

(B.34)

= (1 − γµ) E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

(cid:16)

2γLE (cid:2)V k(cid:3) −

Df (ˆxk, x(cid:63))

(cid:17)

+

γ
2

2γ2σ2
M

opt

.

(B.35)

Let K = kp − 1 for some p ∈ N, then expanding out E

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

2(cid:105)

in (B.34),

E

(cid:104)(cid:13)
(cid:13)rK+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K+1 E

(cid:104)(cid:13)
(cid:13)ˆx0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

K
(cid:88)

k=0

(1 − γµ)K−i 2γ2σ2
M

opt

+

K
(cid:88)

i=0

(1 − γµ)K−i (cid:16)

2γLE (cid:2)V i(cid:3) −

(cid:17)
Df (ˆxi, x(cid:63))

γ
2

≤ (1 − γµ)K+1 E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

opt

2γσ2
µM

+

γ
2

K
(cid:88)

i=0

(1 − γµ)K−i E (cid:2)4LV i − Df (ˆxi, x(cid:63))(cid:3) .

(B.36)

It remains to bound the last term in (B.36). We have

K
(cid:88)

i=0

(1 − γµ)K−i(4LE (cid:2)V i(cid:3) − Df (ˆxi, x(cid:63)))

=

=

p
(cid:88)

kl−1
(cid:88)

l=1

i=kl−1

(1 − γµ)K−i (cid:0)4LE (cid:2)V i(cid:3) − Df

(cid:0)ˆxi, x(cid:63)(cid:1)(cid:1)

p
(cid:88)

l=1

(1 − γµ)K−(kl−1)

kl−1
(cid:88)

kl−1

(1 − γµ)kl−1−i E (cid:2)4LV i − Df

(cid:0)ˆxi, x(cid:63)(cid:1)(cid:3) ,

(B.37)

where in the ﬁrst line we just count i by decomposing it over all the communication
intervals. Fix l ∈ N and let vl = kl − 1. Then by Lemma B.2.7 we have,

(1 − γµ)vl−i E (cid:2)V i(cid:3) ≤

vl(cid:88)

i=kl

2γ(H − 1)
α

vl(cid:88)

i=kl

αvl−iE (cid:2)Df (ˆxi, x(cid:63))(cid:3) +

vl(cid:88)

i=kl

αvl−i2γ2σ2(H − 1),

(B.38)

where α = 1 − γµ. Using (B.38) in (B.37),

184

vl(cid:88)

4L

(1 − γµ)vl−i E (cid:2)V i(cid:3) −

vl(cid:88)

i=kl−1

(1 − γµ)vl−i E (cid:2)Df (ˆxi, x(cid:63))(cid:3)

i=kl−1

≤ 4L

2γ (H − 1)
1 − γµ

vl(cid:88)

i=kl−1

(1 − γµ)vl−i E (cid:2)Df (ˆxi, x(cid:63))(cid:3)

+ 4L

vl(cid:88)

i=kl−1

(1 − γµ)vl−i 2γ2σ2

opt (H − 1) −

vl(cid:88)

i=kl−1

(1 − γµ)vl−i E (cid:2)Df (ˆxi, x(cid:63))(cid:3)

vl(cid:88)

=

(1 − γµ)vl−i 8γ2σ2

opt (H − 1) L

(cid:18)

1 −

(cid:19)

8γL (H − 1)
1 − γµ

(1 − γµ)vl−i E (cid:2)Df (ˆxi, x(cid:63))(cid:3)

i=kl−1
vl(cid:88)

−

i=kl−1
vl(cid:88)

≤

(1 − γµ)vl−i 8γ2σ2

opt (H − 1) L,

(B.39)

i=kl−1

where in the third line we used that our choice of γ guarantees that 1 − 8γLH
(B.39) in (B.37),

1−γµ ≥ 0. Using

K
(cid:88)

i=0

(1 − γµ)K−iE (cid:2)4LV i − Df (ˆxi, x(cid:63))(cid:3)

≤

≤

=

=

≤

p
(cid:88)

l=1

p
(cid:88)

l=1

(1 − γµ)K−(kl−1)

(1 − γµ)K−(kl−1)

kl−1
(cid:88)

i=kl−1

kl−1
(cid:88)

i=kl−1

(1 − γµ)kl−1−i E (cid:2)4LV i − Df

(cid:0)ˆxi, x(cid:63)(cid:1)(cid:3)

(1 − γµ)kl−1−i 8γ2σ2

opt (H − 1) L

p
(cid:88)

kl−1
(cid:88)

kl=1

i=kl−1

(1 − γµ)K−i 8γ2σ2 (H − 1) L

K
(cid:88)

(1 − γµ)K−i 8γ2σ2

opt (H − 1) L

i=0
8σ2

optγ (H − 1) L
µ

.

(B.40)

185

Using (B.40) in (B.36),

E

(cid:104)(cid:13)
(cid:13)rK+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K+1 E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

opt

2γσ2
µM

+

γ
2

K
(cid:88)

i=0

(1 − γµ)K−i E (cid:2)4LV i − Df (ˆxi, x(cid:63))(cid:3)

≤ (1 − γµ)K+1 E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

opt

2γσ2
µM

+

4σ2

optγ2 (H − 1) L
µ

,

which is the claim of the theorem.

(cid:4)

B.2.3 Proof of Theorem 2.4.8

Proof. Start with Lemma B.2.4 with µ = 0, then the conditional expectations satisﬁes

E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105) (B.17)

≤ (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ 2γLV k +

2γ2σ2
M

opt

−

γ
2

Df (ˆxk, x(cid:63)).

Taking full expectations and rearranging,

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤ E

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

− E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γLE (cid:2)V k(cid:3) +

γ
2

2γ2σ2
M

opt

.

Averaging as k varies from 0 to K − 1,

γ
2K

K−1
(cid:88)

k=0

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤

1
K

+

K−1
(cid:88)

(cid:16)

E

(cid:104)(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

− E

(cid:104)(cid:13)
(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)(cid:17)

k=0

2γL
K

K−1
(cid:88)

k=0

E (cid:2)V k(cid:3) +

2γ2σ2
M

opt

=

1
K

(cid:16)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
2 − E
(cid:13)

(cid:104)(cid:13)
(cid:13)ˆxK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)(cid:17)

+

2γL
K

K−1
(cid:88)

k=0

E (cid:2)V k(cid:3)

+

opt

2γ2σ2
M
(cid:107)x0 − x(cid:63)(cid:107)2
K

≤

+

2γL
K

K−1
(cid:88)

k=0

E (cid:2)V k(cid:3) +

2γ2σ2
M

opt

.

(B.41)

To bound the sum of deviations in (B.41), we use Lemma B.2.7 with µ = 0 (and noticing
that because µ = 0 we have α = 1),

kp+1−1
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤

kp+1−1
(cid:88)

k=kp

(cid:0)2γ(H − 1)E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt(H − 1)(cid:1) .

(B.42)

186

Since by assumption K is a synchronization point, there is some l ∈ N such that K = kl.
To estimate the sum of deviations in (B.41) we use double counting to decompose it over
each epoch, use (B.42), and then use double counting again:

K−1
(cid:88)

k=0

E (cid:2)V k(cid:3) =

l−1
(cid:88)

kp+1−1
(cid:88)

p=0

k=kp

E (cid:2)V k(cid:3)

l−1
(cid:88)

kp+1−1
(cid:88)

(B.42)
≤

p=0

k=kp

(cid:0)2γ(H − 1)E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt (H − 1)(cid:1)

=

K−1
(cid:88)

k=0

(cid:0)2γ (H − 1) E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt(H − 1)(cid:1) .

(B.43)

Using (B.43) in (B.41) and rearranging we get,

γ
2K

K−1
(cid:88)

k=0

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
K

+

2γ2σ2
M

opt

+

2γL
K

K−1
(cid:88)

k=0

(cid:0)2γ(H − 1)E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 2γ2σ2

opt(H − 1)(cid:1) .

Therefore,

γ
2K

K−1
(cid:88)

k=0

(1 − 8γ(H − 1)L) E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
K

+

2γ2σ2
M

opt

+ 4γ3Lσ2

opt(H − 1).

By our choice of γ we have that 1 − 8γL(H − 1) ≥ 2
get

10. Using this with some algebra, we

γ
10K

K−1
(cid:88)

k=0

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
K

+

2γ2σ2
M

opt

+ 4γ3Lσ2

opt(H − 1).

Dividing both sides by γ/10 and using Jensen’s inequality yields the theorem’s claim. (cid:4)

B.3 Proofs for Heterogeneous Data

B.3.1 Preliminary lemmas

Lemma B.3.1. Suppose that Assumptions 2.3.1 and 2.3.3 hold with µ ≥ 0 for hetero-
geneous data. Then for expectation conditional on xk
m and for M ≥ 2, we
have

2, . . . , xk

1, xk

E

(cid:104)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2(cid:105)

≤ 2L2V k + 8LDf (ˆxk, x(cid:63)) +

4σ2
dif
M

.

(B.44)

Proof. Starting with the left-hand side,

187

E

(cid:104)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2(cid:105) (1.10)

≤ 2E

(cid:34)(cid:13)
(cid:13)
gk −
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

∇fm(ˆxk; ξm)

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2E

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

n
(cid:88)

m=1

∇fm(ˆxk; ξm)

(cid:35)

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(B.45)

To bound the ﬁrst term in (B.45), we use L-smoothness of fm(·; ξm) to obtain

2E





(cid:13)
(cid:13)
gk −
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
∇fm(ˆxk; ξm)
(cid:13)
(cid:13)
(cid:13)

2
 = 2E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

(∇fm(xk

(cid:13)
(cid:13)
m; ξm) − ∇fm(ˆxk; ξm))
(cid:13)
(cid:13)
(cid:13)

2



M
(cid:88)

E

(cid:104)(cid:13)
(cid:13)∇fm(xk

m; ξm) − ∇fm(ˆxk; ξm)(cid:13)
(cid:13)

2(cid:105)

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

,

(B.46)

where in the second inequality we have used Jensen’s inequality and the convexity of the
map x (cid:55)→ (cid:107)x(cid:107)2. For the second term in (B.45), we have

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
∇fm(ˆxk; ξm)
(cid:13)
(cid:13)
(cid:13)

2

(1.4)
= E



1
M

M
(cid:88)

m=1

∇fm(ˆxk; ξm) −

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
∇fm(ˆxk)
(cid:13)
(cid:13)
(cid:13)

2



1
M

M
(cid:88)

m=1

∇fm(ˆxk)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(B.47)

≤

≤

2
M

m=1
M
(cid:88)

m=1

2L2
M





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

For the ﬁrst term in (B.47) we have by the independence of ξ1, ξ2, . . . , ξm,

188

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

∇fm(ˆxk; ξm) −

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
∇fm(ˆxk)
(cid:13)
(cid:13)
(cid:13)

2



M
(cid:88)

E

(cid:104)(cid:13)
(cid:13)∇fm(ˆxk; ξm) − ∇fm(ˆxk)(cid:13)
(cid:13)

2(cid:105)

E

(cid:104)(cid:13)
(cid:13)∇fm(ˆxk; ξm)(cid:13)
(cid:13)

2(cid:105)

=

1
M 2

m=1
M
(cid:88)

1
M 2

m=1
M
(cid:88)

2
M 2

(1.6)
≤

(1.10)
≤

(1.18)
≤

4L
M 2

m=1
M
(cid:88)

m=1

Dfm(ˆxk, x(cid:63)) +

2σ2
dif
M

E

(cid:104)(cid:13)
(cid:13)∇fm(ˆxk; ξm) − ∇fm(x(cid:63); ξm)(cid:13)
(cid:13)

2(cid:105)

+

2
M 2

M
(cid:88)

m=1

E (cid:2)(cid:107)∇fm(x(cid:63); ξm)(cid:107)2(cid:3)

=

4L
M

Df (ˆxk, x(cid:63)) +

2σ2
dif
M

.

Using this in (B.47) we have,

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

∇fm(ˆxk; ξm)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 ≤

=

4L
M

4L
M

Df (ˆxk, x(cid:63)) +

Df (ˆxk, x(cid:63)) +

2σ2
dif
M

2σ2
dif
M

+ E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

+ (cid:13)

(cid:13)∇f (ˆxk)(cid:13)
2
(cid:13)

.

(cid:13)
(cid:13)
∇fm(ˆxk)
(cid:13)
(cid:13)
(cid:13)

2



Now notice that

(cid:13)∇f (ˆxk)(cid:13)
(cid:13)
2
(cid:13)

= (cid:13)

(cid:13)∇f (ˆxk) − ∇f (x(cid:63))(cid:13)
2
(cid:13)

≤ 2LDf (ˆxk, x(cid:63)).

Using this in the previous inequality we get

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

∇fm(ˆxk; ξm)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:18)

 ≤ 2L

1 +

(cid:19)

2
M

Df (ˆxk, x(cid:63)) +

2σ2
dif
M

.

Because M ≥ 2 we have 1 + 2

M ≤ 2, hence

E





1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)
∇fm(ˆxk; ξm)
(cid:13)
(cid:13)
(cid:13)

2
 ≤ 4LDf (ˆxk, x(cid:63)) +

2σ2
dif
M

.

(B.48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Combining (B.46) and (B.48) in (B.45) we have,

189

E

(cid:104)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2(cid:105)

≤ 2L2V k + 8LDf (ˆxk, x(cid:63)) +

4σ2
dif
M

.

(cid:4)

Lemma B.3.2. Suppose that Assumption 2.3.1 holds with µ ≥ 0 for heterogeneous data
(i.e., it holds for each fm for m = 1, 2, . . . , M ). Then we have,

−

2
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11) ≤ −2Df (ˆxk, x(cid:63)) − µ(cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ LV k.

(B.49)

Proof. Starting with the left-hand side,

−2 (cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11) = −2 (cid:10)xk

m − x(cid:63), ∇fm(xk

m)(cid:11) − 2 (cid:10)ˆxk − xk

m, ∇fm(xk

m)(cid:11) .

The ﬁrst term in (B.50) is bounded by strong convexity:

− (cid:10)xk

m − x(cid:63), ∇fm(xk

m)(cid:11) ≤ fm(x(cid:63)) − fm(xk

m) −

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
2
(cid:13)

.

µ
2

For the second term, we use L-smoothness,

− (cid:10)ˆxk − xk

m, ∇fm(xk

m)(cid:11) ≤ fm(xk

m) − fm(ˆxk) +

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

.

L
2

(B.50)

(B.51)

(B.52)

Combining (B.52) and (B.51) in (B.50),

−2 (cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11) ≤ 2

(cid:16)

fm(x(cid:63)) − fm(xk

m) −

+ 2

= 2

Averaging over m,

(cid:18)

fm(xk

m) − fm(ˆxk) +

(cid:18)

fm(x(cid:63)) − fm(ˆxk) −

µ
2
L
2
µ
2

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
(cid:13)

2(cid:17)

2(cid:19)
m − ˆxk(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
2
(cid:13)

2(cid:19)
m − ˆxk(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

.

+

L
2

−

2
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11) ≤ −2 (cid:0)f (ˆxk) − f (x(cid:63))(cid:1) −

µ
M

M
(cid:88)

m=1

(cid:13)
(cid:13)xk

m − x(cid:63)(cid:13)
2
(cid:13)

+

L
M

M
(cid:88)

m=1

(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
2
(cid:13)

.

Note that the ﬁrst term is the Bregman divergence Df (ˆxk, x(cid:63)), and using Jensen’s in-

equality we have − 1
M

(cid:80)M

m=1

190
2 ≤ −(cid:13)
m − x(cid:63)(cid:13)
(cid:13)

(cid:13)
(cid:13)xk

(cid:13)ˆxk − x(cid:63)(cid:13)
2, hence
(cid:13)

−

2
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11) ≤ −2Df (ˆxk, x(cid:63)) − µ(cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

which is the claim of this lemma.

+ LV k,

(cid:4)

Lemma B.3.3. Suppose that Assumptions 2.3.1 and 2.3.3 hold for Algorithm 1 with
heterogeneous data and with supp |kp − kp+1| ≤ H. Let p ∈ N, then for v = kp+1 − 1
and γ ≤

1

4L(H−1) we have,

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤ 8Lγ2 (H − 1)2

v
(cid:88)

k=kp

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 4γ2 (H − 1)2

v
(cid:88)

k=kp

σ2
dif.

(B.53)

Proof. Let k be such that kp ≤ k ≤ kp+1 − 1 = v. From the deﬁnition of V k,

E (cid:2)V k(cid:3) =

=

1
M

1
M

M
(cid:88)

E

m=1

M
(cid:88)

m=1

E

(cid:104)(cid:13)
(cid:13)xk

m − ˆxk(cid:13)
(cid:13)

2(cid:105)





(cid:13)

(cid:13)
(cid:13)
xm
(cid:13)
(cid:13)
(cid:13)

kp − γ





gi
m

 −

xkp − γ

k−1
(cid:88)

i=kp

k−1
(cid:88)

i=kp

gi

Using that xkp = xm

kp for all m we have,



2



 .

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E (cid:2)V k(cid:3) =

γ2
M

E





m=1

M
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
γ2 (k − kp)
M

(1.7)
≤

(1.6)
≤

γ2 (k − kp)
M

k−1
(cid:88)

i=kp

m − gi
gi

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

M
(cid:88)

k−1
(cid:88)

E

(cid:104)(cid:13)
(cid:13)gi

m − gi(cid:13)
(cid:13)

2(cid:105)

m=1

i=kp

M
(cid:88)

t−1
(cid:88)

m=1

i=kp

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

≤

γ2 (H − 1)
M

M
(cid:88)

k−1
(cid:88)

m=1

i=kp

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

,

where in the third line we used that gi = 1
M

(cid:80)M

m=1 gi

m and 1
M

(cid:80)M

m=1

E

(cid:104)
(cid:107)gi

m − gi(cid:107)2(cid:105)

≤

1
M

(cid:80)M

m=1

E

(cid:104)

m(cid:107)2(cid:105)

(cid:107)gi

, while in the fourth line we used that k −kp ≤ kp+1 −kp −1 ≤ H −1.

Summing up as k varies from kp to v,

191

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤

v
(cid:88)

k=kp

γ2(H − 1)
M

M
(cid:88)

k−1
(cid:88)

m=1

i=kp

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

.

Because k − 1 ≤ v − 1 ≤ v we can upper bound the inner sum as follows

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤

≤

v
(cid:88)

k=kp

v
(cid:88)

k=kp

γ2(H − 1)
M

γ2(H − 1)
M

M
(cid:88)

k−1
(cid:88)

m=1

i=kp

M
(cid:88)

v−1
(cid:88)

m=1

i=kp

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

=

γ2(H − 1) (v − kp)
M

M
(cid:88)

v−1
(cid:88)

m=1

k=kp

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

≤

≤

γ2(H − 1)2
M

γ2(H − 1)2
M

M
(cid:88)

v−1
(cid:88)

m=1

k=kp

M
(cid:88)

v
(cid:88)

m=1

i=kp

E

(cid:104)(cid:13)
(cid:13)gk
m

2(cid:105)

(cid:13)
(cid:13)

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

.

(B.54)

To bound the gradient norm term in (B.54), we have

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

≤ 3E

(cid:104)(cid:13)
(cid:13)gi

m − ∇fm(ˆxi; ξm)(cid:13)
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)∇fm(ˆxi; ξm) − ∇fm(x(cid:63); ξm)(cid:13)
(cid:13)

+ 3E
+ 3E (cid:2)(cid:107)∇fm(x(cid:63); ξm)(cid:107)2(cid:3) .

2(cid:105)

The ﬁrst term in (B.55) can be bounded by smoothness:

E

(cid:104)(cid:13)
(cid:13)gi

m − ∇fm(ˆxi; ξm)(cid:13)
(cid:13)

2(cid:105)

= E

m; ξm) − ∇fm(ˆxi; ξm)(cid:13)
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)∇fm(xi
(cid:104)(cid:13)
(cid:13)xi

m − ˆxi(cid:13)
(cid:13)

2(cid:105)

.

≤ L2E

The second term in (B.55) can be bounded by smoothness and convexity:

E

(cid:104)(cid:13)
(cid:13)∇fm(ˆxi; ξm) − ∇fm(x(cid:63); ξm)(cid:13)
(cid:13)

2(cid:105) (1.18)

≤ 2LE (cid:2)Dfm(ˆxi, x(cid:63))(cid:3) .

(B.55)

(B.56)

(B.57)

(B.58)

Using (B.58) and (B.57) in (B.55) and averaging with respect to m,

192

1
M

M
(cid:88)

m=1

E

(cid:104)(cid:13)
(cid:13)gi
m

2(cid:105)

(cid:13)
(cid:13)

≤

3L2
M

m=1

M
(cid:88)

E

(cid:104)(cid:13)
(cid:13)xi

m − ˆxi(cid:13)
(cid:13)

2(cid:105)

+ 6LDf (ˆxi, x(cid:63)) + 3σ2
dif

= 3L2E (cid:2)V i(cid:3) + 6LE (cid:2)Df (ˆxi, x(cid:63))(cid:3) + 3σ2

dif.

(B.59)

Using (B.59) in (B.55),

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤ γ2 (H − 1)2

v
(cid:88)

k=kp

E (cid:2)3L2V k + 6LDf (ˆxk, x(cid:63)) + 3σ2

dif

(cid:3) .

Noticing that the sum (cid:80)v

k=kp

E (cid:2)V k(cid:3) appears in both sides, we can rearrange

(cid:0)1 − 3γ2 (H − 1)2 L2(cid:1)

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤ 6Lγ2 (H − 1)2

v
(cid:88)

k=kp

E (cid:2)Df (ˆxk, x(cid:63))(cid:3)

+ 3γ2 (H − 1)2

v
(cid:88)

k=kp

σ2
dif.

Finally using that our choice γ implies that 1 − 3γ2 (H − 1)2 L2 ≥ 3

4 we have,

v
(cid:88)

k=kp

E (cid:2)V k(cid:3) ≤ 8Lγ2 (H − 1)2

v
(cid:88)

k=kp

E (cid:2)Df (ˆxk, x(cid:63))(cid:3) + 4γ2 (H − 1)2

v
(cid:88)

k=kp

σ2
dif.

(cid:4)

Lemma B.3.4 (Optimality gap single recursion). Suppose that Assumptions 2.3.1 and
2.3.3 hold for Algorithm 1 with heterogeneous data and with M ≥ 2. Then for any γ ≥ 0
1, xk
we have for expectation conditional on xk

2, . . . , xk
m,

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

+ γL (1 + 2γL) V k − 2γ (1 − 4γL) Df (ˆxk, x(cid:63)) +

4γ2σ2
dif
M
(B.60)

,

where rk def= ˆxk − x(cid:63). In particular, if γ ≤ 1

8L , then

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

+

5
4

γLV k −

γ
2

Df (ˆxk, x(cid:63)) +

4γ2σ2
dif
M

,

(B.61)

Proof. First note that ˆxk+1 = ˆxk − γgk is always true (regardless of whether or not

synchronization happens), hence

193

(cid:13)ˆxk+1 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

= (cid:13)
= (cid:13)

(cid:13)ˆxk − γgk − x(cid:63)(cid:13)
2
(cid:13)
(cid:13)gk(cid:13)
+ γ2(cid:13)
(cid:13)ˆxk − x(cid:63)(cid:13)
2
2
(cid:13)
(cid:13)

− 2γ (cid:10)ˆxk − x(cid:63), gk(cid:11)

= (cid:13)

(cid:13)ˆxk − x(cid:63)(cid:13)
2
(cid:13)

+ γ2(cid:13)

(cid:13)gk(cid:13)
2
(cid:13)

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), gk

m

(cid:11) .

Taking conditional expectations then using Lemmas B.3.1 and B.3.2,

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

+ γ2E

(cid:104)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2(cid:105)

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11)

(B.44)

≤ (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

(cid:18)

2L2V k + 8LDf (ˆxk, x(cid:63)) +

+ γ2

(cid:19)

4σ2
dif
M

−

2γ
M

M
(cid:88)

m=1

(cid:10)ˆxk − x(cid:63), ∇fm(xk

m)(cid:11)

(cid:13)rk(cid:13)
2
(cid:13)

(B.49)

≤ (1 − γµ) (cid:13)
4γ2σ2
dif
M

+

.

+ γL (1 + 2γL) V k − 2γ (1 − 4γL) Df (ˆxk, x(cid:63))

If γ ≤ 1

8L , then 1 − 4γL ≥ 1

2 and 1 + 2γL ≤ 5

4, and hence

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

+

5
4

γLV k −

γ
2

Df (ˆxk, x(cid:63)) +

4γ2σ2
dif
M

,

as claimed.

(cid:4)

B.3.2 Proof of Theorem 2.4.10

Proof. Start with Lemma B.3.4 with µ = 0,

E

(cid:104)(cid:13)
(cid:13)rk+1(cid:13)
(cid:13)

2(cid:105)

≤ (cid:13)

(cid:13)rk(cid:13)
2
(cid:13)

+

(cid:18) 5
2

γ
2

LV k − Df (ˆxk, x(cid:63))

(cid:19)

+

4γ2σ2
dif
M

.

Taking unconditional expectations and summing up,

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

≤

K
(cid:88)

i=1

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

K−1
(cid:88)

i=0

γ
2

K−1
(cid:88)

E

i=0

(cid:20)5
2

LV i − Df (ˆxi, x(cid:63))

(cid:21)

+

K−1
(cid:88)

i=0

4γ2σ2
dif
M

.

(B.62)

Using that K = kp for some p ∈ N, we can decompose the second term by double
counting and bound it by Lemma B.3.3,

194

K−1
(cid:88)

E

i=0

(cid:20)5
2

(cid:21)
LV i − Df (ˆxi, x(cid:63))

=

p
(cid:88)

kl−1
(cid:88)

l=1

i=kl−1

p
(cid:88)

kl−1
(cid:88)

l=1

i=kl−1

≤

E

(cid:20) 5
2

(cid:21)
LV i − Df (ˆxi, x(cid:63))

(cid:0)20L2γ2(H − 1)2 − 1(cid:1) E (cid:2)Df (ˆxi, x(cid:63)(cid:3)

p
(cid:88)

kl−1
(cid:88)

+

l=1

i=kl−1

10Lγ2(H − 1)2σ2

dif.

By assumption on γ we have that 20L2γ2(H − 1)2 − 1 ≤ − 1
double counting again we have,

2, using this and then using

K−1
(cid:88)

E

i=0

(cid:20)5
2

(cid:21)
LV i − Df (ˆxi, x(cid:63))

≤ −

= −

1
2

1
2

p
(cid:88)

kl−1
(cid:88)

l=1

i=kl−1

E (cid:2)Df (ˆxi, x(cid:63))(cid:3) +

p
(cid:88)

kl−1
(cid:88)

l=1

i=kl−1

10Lγ2(H − 1)2σ2
dif

K−1
(cid:88)

i=0

E (cid:2)Df (ˆxi, x(cid:63))(cid:3) +

K−1
(cid:88)

i=0

10Lγ2(H − 1)2σ2

dif.

Using this in (B.62),

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

≤

K
(cid:88)

i=1

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

−

K−1
(cid:88)

i=0

γ
4

K−1
(cid:88)

i=0

E (cid:2)Df (ˆxi, x(cid:63))(cid:3)

+

K−1
(cid:88)

i=0

(cid:18)

5Lγ3(H − 1)2σ2

dif +

4γ2σ2
dif
M

(cid:19)

.

Rearranging, we get

γ
4

K−1
(cid:88)

i=0

E (cid:2)Df (ˆxi, x(cid:63))(cid:3) ≤

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

−

K−1
(cid:88)

i=0

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

K
(cid:88)

i=1

+

K−1
(cid:88)

i=0

(cid:18)

5Lγ3(H − 1)2σ2

dif +

(cid:19)

4γ2σ2
dif
M

= (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 − E
(cid:13)

2(cid:105)

(cid:104)(cid:13)
(cid:13)rk(cid:13)
(cid:13)

+

K−1
(cid:88)

(cid:18)

i=0

5Lγ3(H − 1)2σ2

dif +

(cid:19)

4γ2σ2
dif
M

≤ (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + K
(cid:13)

5Lγ3(H − 1)2σ2

dif +

(cid:18)

4γ2σ2
dif
M

(cid:19)

.

195

Figure B.1: Results on regularized logistic regression with shared data, ‘a9a’ dataset, with
stepsize 0.05
L . With more local iterations, fewer communication rounds are required to get
to a neighborhood of the solution.

Dividing both sides by γK/4, we get

1
K

K−1
(cid:88)

i=0

E (cid:2)Df (ˆxi, x(cid:63))(cid:3) ≤

4(cid:107)x0 − x(cid:63)(cid:107)2
γK

+

20γσ2
dif
M

+ 16γ2L(H − 1)2σ2

dif.

Finally, using Jensen’s inequality and the convexity of f we get the required claim.

(cid:4)

B.4 Extra Experiments

Figure B.1 shows experiments done with identical data and Figure B.2 shows experiments
done with heterogeneous data in the same setting as described in the main text but with
diﬀerent datasets.

B.5 Discussion of Dieuleveut and Patel (2019)

An analysis of Local SGD for identical data under strong convexity, Lipschitzness of ∇f ,
uniformly bounded variance, and Lipschitzness of ∇2f is given in [64], where they obtain
a similar communication complexity to [252] without bounded gradients. However, in the
proof of their result for general non-quadratic functions (Proposition S20) they make the
following assumption, rewritten in our notation:



G = sup

p

1 + M LHγ



(cid:13)ˆxk − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

 < ∞,

kp+1−1
(cid:88)

k=kp

where LH is the Lipschitz constant of the Hessian of f (assumed thrice diﬀerentiable).
Their discussion of G speculates on the behaviour of iterate distances, e.g., saying that
if they are bounded, then the guarantee is good. Unfortunately, assuming this quantity
bounded implies that gradients are bounded as well, making the improvement over [252]
unclear to us. Furthermore, as G depends on the algorithm’s convergence (it is the

196

distance from the optimum evaluated at various points), assuming it is bounded to prove
convergence to a compact set results in a possibly circular argument. Since G is also used
as an upper bound on H in their analysis, it is not possible to calculate the communication
complexity.

Figure B.2: Same experiment as in Figure 2.3, performed on the ‘mushrooms’ dataset.

197

Appendix C

Appendix for Chapter 3

C.1 Additional Experiment Details

τ (cid:101) − 1(cid:1).

Objective properties. To better correspond to the theoretical setting of our main result,
we use (cid:96)2 regularization in every element of the ﬁnite-sum. To obtain mini-batches for
the RR, SO and IG we permute the dataset and then split it into n = (cid:100) N
τ (cid:101) groups of sizes
τ, . . . , τ, N − τ (cid:0)(cid:100) N
In other words, the ﬁrst n − 1 groups are of size τ and the
remaining samples go to the last group. For SO and IG, we split the data only once, and
for RR, we do this at the beginning of each epoch. The permutation of samples used in
IG is the one in which the datasets are stored online. The smoothness constant of the
sum of logistic regression losses admits a closed form expression Lf = 1
4N (cid:107)A(cid:107)2 + λ. The
individual losses are Lmax-smooth with Lmax = maxi=1,...,n (cid:107)ai(cid:107)2 + λ.

Stepsizes. For all methods in Figure 3.1, we keep the stepsize equal to 1

L for the
ﬁrst k0 = (cid:98)K/40(cid:99) iterations, where K is the total number of stochastic steps. This
is important to ensure that there is an exponential convergence before the methods
reach their convergence neighborhoods [253]. After the initial k0 iterations, the step-
and for
sizes used for RR, SO and IG were chosen as γk = min

(cid:110) 1
L,
SGD as γk = min
. Although these stepsizes for RR are commonly
used in practice [29], we do not analyze them and leave decreasing-stepsize analysis for
future work. We also note that although L is generally not available, it can be esti-
mated using empirically observed gradients [158]. For our experiments, we estimate L of
mini-batches of size τ using the closed-form expressions from Proposition 3.8 in [84] as
L ≤ n(τ −1)
τ (n−1)Lmax. The conﬁdence intervals in Figure 3.1 are estimated using
20 random seeds.

τ (n−1)Lf + n−τ

3
µ max{1,k−k0}

2
µ max{1,k−k0}

(cid:110) 1
L ,

(cid:111)

(cid:111)

For the experiments in Figure 3.2, we estimate the expectation from (3.5) with 20
permutations, which provides suﬃciently stable estimates. In addition, we use L = Lmax
(instead of using the batch smoothness of [84]) as the plots in this ﬁgure use diﬀerent
mini-batch sizes and we want to isolate the eﬀect of reducing the variance by mini-batching
from the eﬀect of changing L.

SGD implementation. For SGD, we used two approaches to mini-batching. In the
ﬁrst, we sampled τ indices from {1, . . . , N } and used them to form the mini-batch, where
N is the total number of data samples. In the second approach, we permuted the data
once and then at each iteration, we only sampled one index i and formed the mini-batch
from indices i, (i + 1) mod N, . . . , (i + τ − 1) mod N . The latter approach is much
more cash-friendly and runs signiﬁcantly faster, while the iteration convergence was the
same in our experiments. Thus, we used the latter option to produce the ﬁnal plots.

198

For all plots and methods, we use zero initialization, x0 = (0, . . . , 0)(cid:62) ∈ Rd. We
obtain the optimum, x(cid:63), by running Nesterov’s accelerated gradient method until it reaches
machine precision. The plots in the right column in Figure 3.2 were obtained by initializing
the methods at an intermediate iterate of Nesterov’s method, and we found the average,
best and worst results by sampling 1,000 permutations.

C.2 Notation

We deﬁne the epoch total gradient gk as

gk def=

n−1
(cid:88)

i=0

∇fπi(xk

i ).

We deﬁne the variance of the local gradients from their average at a point xk as

σ2
k

def=

1
n

n
(cid:88)

j=1

(cid:13)∇fj(xk) − ∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

.

By Ek [·] we denote the expectation conditional on all information prior to iteration k,
including xk. To avoid issues with the special case n = 1, we use the convention 0/0 = 0.
A summary of the notation used in this chapter is given in Table A.3.

C.3 A Lemma for Sampling without Replacement

The following algorithm-independent lemma characterizes the variance of sampling a num-
ber of vectors from a ﬁnite set of vectors, without replacement. It is a key ingredient in
our results on the convergence of the RR and SO methods.

Lemma C.3.1. Let X1, . . . , Xn ∈ Rd be ﬁxed vectors, X def= 1
i=1 Xi be their average
and σ2 def= 1
be the population variance. Fix any m ∈ {1, . . . , n}, let
n
Xπ1, . . . Xπk be sampled uniformly without replacement from {X1, . . . , Xn} and X π be
their average. Then, the sample average and variance are given by

(cid:13)
(cid:13)Xi − X(cid:13)
2
(cid:13)

(cid:80)n

i=1

n

(cid:80)n

E (cid:2)X π

(cid:3) = X,

E

(cid:104)(cid:13)
(cid:13)X π − X(cid:13)
(cid:13)

2(cid:105)

=

n − m
m(n − 1)

σ2.

(C.1)

Proof. The ﬁrst claim follows by linearity of expectation and uniformity of sampling:

E (cid:2)X π

(cid:3) =

1
m

m
(cid:88)

i=1

E [Xπi] =

1
m

m
(cid:88)

i=1

X = X.

To prove the second claim, let us ﬁrst establish that the identity cov(Xπi, Xπj ) = − σ2

n−1

holds for any i (cid:54)= j. Indeed,

199

cov(Xπi, Xπj ) = E (cid:2)(cid:10)Xπi − X, Xπj − X(cid:11)(cid:3) =

1
n(n − 1)

n
(cid:88)

n
(cid:88)

l=1

p=1,p(cid:54)=l

(cid:10)Xl − X, Xp − X(cid:11)

n
(cid:88)

n
(cid:88)

(cid:10)Xl − X, Xp − X(cid:11) −

p=1
(cid:42)

Xl − X,

l=1

n
(cid:88)

l=1

(cid:43)

(Xp − X)

−

σ2
n − 1

n
(cid:88)

p=1

1
n(n − 1)

n
(cid:88)

l=1

(cid:13)
(cid:13)Xl − X(cid:13)
2
(cid:13)

=

=

1
n(n − 1)

1
n(n − 1)

= −

σ2
n − 1

.

This identity helps us to establish the formula for sample variance:

E

(cid:104)(cid:13)
(cid:13)X π − X(cid:13)
(cid:13)

2(cid:105)

=

1
k2

k
(cid:88)

k
(cid:88)

cov(Xπi, Xπj )

i=1

j=1
(cid:34) k

(cid:88)

E

(cid:18)

=

=

1
m2

1
m2

(cid:13)
(cid:13)Xπi − X(cid:13)
2
(cid:13)

(cid:35)

+

k
(cid:88)

m
(cid:88)

cov(Xπi, Xπj )

i=1

mσ2 − m(m − 1)

i=1
(cid:19)

σ2
n − 1

j=1,j(cid:54)=i

=

n − m
m(n − 1)

σ2.

(cid:4)

C.4 Proofs for Convex Objectives (Sections 3.4.1 and 3.4.2)

C.4.1 Proof of Proposition 3.4.3

Proof. Let us start with the upper bound. Fixing any i such that 1 ≤ i ≤ n − 1, we have
i(n − i) ≤ n2

and using smoothness and Lemma C.3.1 leads to

4 ≤ n(n−1)

2

E (cid:2)Dfπi

i , x(cid:63))(cid:3) (1.15)
≤

(x(cid:63)

L
2

E (cid:2)(cid:107)x(cid:63)

i − x(cid:63)(cid:107)2(cid:3) =

(C.1)
=

≤

(cid:13)
(cid:13)
γ∇fπk(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

2





E



L
2

i−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
γ2Li(n − i)
2(n − 1)

k=0

γ2Ln
4

σ2
(cid:63).

σ2
(cid:63)

To obtain the upper bound, it remains to take maximum with respect to i on both sides
and divide by γ. To prove the lower bound, we use strong convexity and the fact that

maxi i(n − i) ≥ n(n−1)

4

holds for any integer n. Together, this leads to

200

max
i

E (cid:2)Dfπi

as desired.

i , x(cid:63))(cid:3) (1.16)

(x(cid:63)

≥ max

i

µ
2

E (cid:2)(cid:107)x(cid:63)

i − x(cid:63)(cid:107)2(cid:3) = max

i

γ2µi(n − i)
2(n − 1)

σ2
(cid:63) ≥

γ2µn
8

σ2
(cid:63),

(cid:4)

C.4.2 Proof remainder for Theorem 3.4.4

Proof. We start from (3.8) proved in the main text:

E (cid:2)(cid:107)xk

i+1 − x(cid:63)

i+1(cid:107)2(cid:3) ≤ (1 − γµ)E (cid:2)(cid:107)xk

i − x(cid:63)

i (cid:107)2(cid:3) + 2γ2σ2

Shuﬄe.

Since xk+1 − x(cid:63) = xk
the epoch level recursion

n − x(cid:63)

n and xk − x(cid:63) = xk

0 − x(cid:63)

0, we can unroll the recursion, obtaining

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)n E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ2σ2

Shuﬄe

(cid:33)

(1 − γµ)i

.

(cid:32)n−1
(cid:88)

i=0

Unrolling this recursion across T epochs, we obtain

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2+2γ2σ2
(cid:13)

Shuﬄe

(cid:32)n−1
(cid:88)

i=0

(1 − γµ)i

(cid:33) (cid:32)T −1
(cid:88)

j=0

(cid:33)

(1 − γµ)nj

.

(C.2)
The product of the two sums in (C.2) can be bounded by reparameterizing the summation
as follows:

(cid:32)T −1
(cid:88)

j=0

(1 − γµ)nj

(cid:33) (cid:32)n−1
(cid:88)

i=0

(cid:33)

(1 − γµ)i

=

T −1
(cid:88)

n−1
(cid:88)

j=0

i=0

(1 − γµ)nj+i

=

nT −1
(cid:88)

k=0

(1 − γµ)k ≤

∞
(cid:88)

k=0

(1 − γµ)k =

1
γµ

.

Plugging this bound back into (C.2), we ﬁnally obtain the bound

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + 2γ
(cid:13)

σ2

Shuﬄe
µ

.

(cid:4)

C.4.3 Proof of complexity

In this subsection, we show how we get from Theorem 3.4.4 the complexity for strongly
convex functions.

201

Corollary C.4.1. Under the same conditions as those in Theorem 3.4.4, we choose step-
size

γ = min

(cid:26) 1
L

,

2
µnT

(cid:18) (cid:107)x0 − x(cid:63)(cid:107) µT
√

log

√

(cid:19)(cid:27)

n

.

κσ(cid:63)

The ﬁnal iterate xT then satisﬁes

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:18)

(cid:18)

exp

−

(cid:19)

µnT
L

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)

κσ2
(cid:63)
µ2nT 2

(cid:19)

,

where ˜O(·) denotes ignoring absolute constants and polylogarithmic factors. Thus, in
order to obtain error (in squared distance to the optimum) less than ε, we require that
the total number of iterations nT satisﬁes

nT = ˜Ω

(cid:18)

κ +

√

κnσ(cid:63)
√
ε
µ

(cid:19)

.

Proof. Applying Theorem 3.4.4, the ﬁnal iterate generated by Algorithms 2 or 3 after T
epochs satisﬁes

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + 2γ
(cid:13)

σ2

Shuﬄe
µ

.

Using Proposition 3.4.3 to bound σ2

Shuﬄe, we get

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ2κnσ2
(cid:63).
(cid:13)

(C.3)

We now have two cases:

• Case 1: If 1

L ≤ 2

µnT log

(cid:18) (cid:107)x0−x(cid:63)(cid:107)µT

√

κσ(cid:63)

(cid:19)

√

n

, then using γ = 1

L in (C.3) we have

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

(cid:16)

(cid:16)

≤

≤

1 −

1 −

µ
L
µ
L

(cid:17)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

(cid:17)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

κnσ2
(cid:63)
L2
4κσ2
µ2nT 2 log2
(cid:63)

(cid:18) (cid:107)x0 − x(cid:63)(cid:107) µT
√

κσ(cid:63)

√

(cid:19)

n

.

Using that 1 − x ≤ exp(−x) in the previous inequality, we get

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:18)

(cid:18)

exp

−

(cid:19)

µnT
L

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)

κσ2
(cid:63)
µ2nT 2

(cid:19)

,

(C.4)

where ˜O(·) denotes ignoring polylogarithmic factors and absolute (non-problem speciﬁc)
constants.

• Case 2: If

2
µnT log

(cid:18) (cid:107)x0−x(cid:63)(cid:107)µT

√

n

(cid:19)

√

κσ(cid:63)

< 1

L , recall that by Theorem 3.4.4,

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ2κnσ2
(cid:63).
(cid:13)

(C.5)

Plugging in γ = 2

µnT log

202
(cid:19)

(cid:18)(cid:107)x0−x(cid:63)(cid:107)µT

√

n

√

κσ(cid:63)

, the ﬁrst term in (C.5) satisﬁes

(1 − γµ)nT (cid:13)

2 ≤ exp (−γµnT ) (cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
(cid:18)

(cid:13)x0 − x(cid:63)(cid:13)
2
(cid:13)
(cid:18) (cid:107)x0 − x(cid:63)(cid:107) µT
√

κσ(cid:63)

√

(cid:19)(cid:19)

n

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

= exp

−2 log

=

κσ2
(cid:63)
µ2nT 2 .

Furthermore, the second term in (C.5) satisﬁes

γ2κnσ2

(cid:63) =

4κσ2
µ2nT 2 log2
(cid:63)

(cid:18) (cid:107)x0 − x(cid:63)(cid:107) µT
√

κσ(cid:63)

√

n

(cid:19)

.

Substituting (C.6) and (C.7) into (C.5), we get

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= ˜O

(cid:18) κσ2
(cid:63)
µ2nT 2

(cid:19)

.

This concludes the second case.

(C.6)

(C.7)

(C.8)

It remains to take the maximum of (C.4) from the ﬁrst case and (C.8) from the second
(cid:4)
case.

C.4.4 Two lemmas for Theorems 3.4.5 and 3.4.6

In order to prove Theorems 3.4.5 and 3.4.6, it will be useful to deﬁne the following
quantity.

1, . . . , xk
Deﬁnition C.4.2. Let xk
deﬁne the forward per-epoch deviation over the k-th epoch V k as

n be iterates generated by Algorithms 2 or 3. We

0, xk

V k def=

n−1
(cid:88)

i=0

(cid:13)
(cid:13)xk

i − xk+1(cid:13)
2
(cid:13)

.

(C.9)

We will now establish two lemmas. First, we will show that V k can be eﬃciently upper
bounded using Bregman divergences and the variance at the optimum. Subsequently use
this bound to establish the convergence of RR/SO.

Bounding the forward per-epoch deviation

Lemma C.4.3. Consider the iterates of Random Reshuﬄing (Algorithm 2) or Shuﬄe-
Once (Algorithm 3).
If the functions f1, . . . , fn are convex and Assumption 3.3.1 is
satisﬁed, then

E (cid:2)V k(cid:3) ≤ 4γ2n2L

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i )(cid:3) +

1
2

γ2n2σ2
(cid:63),

(C.10)

203

where V k is deﬁned as in Deﬁnition C.4.2, and σ2
by σ2
(cid:63)

i=1 (cid:107)∇fi(x(cid:63))(cid:107)2.

def= 1
n

(cid:80)n

(cid:63) is the variance at the optimum given

Proof. For any ﬁxed m ∈ {0, . . . , n − 1}, by deﬁnition of xk
decomposition

m and xk+1 we get the

m − xk+1 = γ
xk

n−1
(cid:88)

i=m

∇fπi(xk

i ) = γ

n−1
(cid:88)

i=m

(∇fπi(xk

i ) − ∇fπi(x(cid:63))) + γ

n−1
(cid:88)

i=m

∇fπi(x(cid:63)).

Applying Young’s inequality to the sums above yields

(cid:107)xk

m − xk+1(cid:107)2

(1.10)
≤ 2γ2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=m

(∇fπi(xk

(cid:13)
2
(cid:13)
i ) − ∇fπi(x(cid:63)))
(cid:13)
(cid:13)
(cid:13)

+ 2γ2

∇fπi(x(cid:63))

(1.8)
≤ 2γ2n

n−1
(cid:88)

i=m

(cid:13)
(cid:13)∇fπi(xk

i ) − ∇fπi(x(cid:63))(cid:13)
2
(cid:13)

(1.18)
≤ 4γ2Ln

≤ 4γ2Ln

n−1
(cid:88)

i=m

n−1
(cid:88)

i=0

Dfπi

(x(cid:63), xk

i ) + 2γ2

Dfπi

(x(cid:63), xk

i ) + 2γ2

Summing up and taking expectations leads to

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=m

n−1
(cid:88)

i=m

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2γ2

∇fπi(x(cid:63))

n−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=m
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=m
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇fπi(x(cid:63))

∇fπi(x(cid:63))

.

E

2(cid:105)

m=0

n−1
(cid:88)

n−1
(cid:88)

(cid:104)(cid:13)
(cid:13)xk

≤ 4γ2Ln2

m − xk+1(cid:13)
(cid:13)

(cid:13)
(cid:13)
∇fπi(x(cid:63))
(cid:13)
(cid:13)
(cid:13)
(C.11)
We now bound the second term in the right-hand side of (C.11). First, using Lemma C.3.1,
we get

i )(cid:3)+2γ2

E (cid:2)Dfπi

(x(cid:63), xk

n−1
(cid:88)

n−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=0

i=k

i=0

E



2
 .



E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

∇fπi(x(cid:63))

2
 = (n − k)2E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n − k

n−1
(cid:88)

i=k

∇fπi(x(cid:63))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2



= (n − k)2

k
(n − k)(n − 1)

σ2
(cid:63)

=

k(n − k)
n − 1

σ2
(cid:63).

Next, by summing this for k from 0 to n − 1, we obtain

n−1
(cid:88)

E

k=0





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

∇fπi(x(cid:63))

2
 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

k=0

k(n − k)
n − 1

σ2
(cid:63) =

1
6

n(n + 1)σ2

(cid:63) ≤

n2σ2
(cid:63)
4

,

where in the last step we also used n ≥ 2. The result follows.

(cid:4)

204

Finding a per-epoch recursion

Lemma C.4.4. Assume that functions f1, . . . , fn are convex and that Assumption 3.3.1
is satisﬁed.
If Random Reshuﬄing (Algorithm 2) or Shuﬄe-Once (Algorithm 3) is run
with a stepsize satisfying γ ≤ 1√

, then

2Ln

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γnE (cid:2)f (xk+1) − f (cid:63)(cid:3) +

γ3Ln2σ2
(cid:63)
2

.

Proof. Deﬁne the sum of gradients used in the k-th epoch as gk def= (cid:80)n−1
i=0 ∇fπi(xk
will use gk to relate the iterates xk and xk+1. By deﬁnition of xk+1, we can write

i ). We

xk+1 = xk

n = xk

n−1 − γ∇fπn−1(xk

n−1) = · · · = xk

0 − γ

n−1
(cid:88)

i=0

∇fπi(xk

i ).

Further, since xk

0 = xk, we see that xk+1 = xk − γgk, which leads to
(cid:107)xk − x(cid:63)(cid:107)2 = (cid:107)xk+1 + γgk − x(cid:63)(cid:107)2 = (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ (cid:10)gk, xk+1 − x(cid:63)(cid:11) + γ2(cid:107)gk(cid:107)2

≥ (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ (cid:10)gk, xk+1 − x(cid:63)(cid:11)

= (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ

n−1
(cid:88)

i=0

(cid:10)∇fπi(xk

i ), xk+1 − x(cid:63)(cid:11) .

Observe that for any i, we have the following decomposition

(cid:10)∇fπi(xk

i ), xk+1 − x(cid:63)(cid:11) = [fπi(xk+1) − fπi(x(cid:63))] + [fπi(x(cid:63)) − fπi(xk
− [fπi(xk+1) − fπi(xk

i ) − (cid:10)∇fπi(xk
(x(cid:63), xk

i ) − (cid:10)∇fπi(xk
(cid:11)]
(xk+1, xk

i ), xk+1 − xk
i
i ) − Dfπi

i ).

= [fπi(xk+1) − fπi(x(cid:63))] + Dfπi

i ), x(cid:63) − xk
i

(cid:11)]

Summing the ﬁrst quantity in (C.12) over i from 0 to n − 1 gives

n−1
(cid:88)

i=0

[fπi(xk+1) − fπi(x(cid:63))] = n(f (xk+1) − f (cid:63)).

(C.12)

Now, we can bound the third term in the decomposition (C.12) using L-smoothness as
follows:

Dfπi

(xk+1, xk

i ) ≤

(cid:107)xk+1 − xk

i (cid:107)2.

L
2

By summing the right-hand side over i from 0 to n − 1 we get the forward deviation over

an epoch V k, which we bound by Lemma C.4.3 to get

205

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(xk+1, xk

i )(cid:3) (C.9)
≤

L
2

E (cid:2)V k(cid:3) (C.10)

≤ 2γ2L2n2

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i )(cid:3) +

γ2Ln2σ2
(cid:63)
4

.

Therefore, we can lower-bound the sum of the second and the third term in (C.12) as

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i ) − Dfπi

(xk+1, xk

i )(cid:3) ≥

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i )(cid:3)

− 2γ2L2n2

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i )(cid:3) −

γ2Ln2σ2
(cid:63)
4

n−1
(cid:88)

i=0

E (cid:2)Dfπi

(x(cid:63), xk

i )(cid:3) −

γ2Ln2σ2
(cid:63)
4

≥ (1 − 2γ2L2n2)

≥ −

γ2Ln2σ2
(cid:63)
4

,

where in the third inequality we used that γ ≤ 1√
Plugging this back into the lower-bound on (cid:107)xk − x(cid:63)(cid:107)2 yields

and that Dfπi

2Ln

(x(cid:63), xk

i ) is nonnegative.

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≥ E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) + 2γnE (cid:2)f (xk+1) − f (cid:63)(cid:3) −

γ3Ln2σ2
(cid:63)
2

.

Rearranging the terms gives the result.

C.4.5 Proof of Theorem 3.4.5

Proof. We can use Lemma C.4.4 and strong convexity to obtain

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ E

2(cid:105)

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)
(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

(1.16)
≤ E

− 2γnE (cid:2)f (xk+1) − f (cid:63)(cid:3) +

2(cid:105)

− γnµE

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

γ3Ln2σ2
(cid:63)
2
γ3Ln2σ2
(cid:63)
2

+

(cid:4)

,

whence

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤

=

≤

1
1 + γµn
1
1 + γµn
(cid:16)

1 −

(cid:18)

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

(cid:19)

γ3Ln2σ2
(cid:63)
2

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

1
1 + γµn

γ3Ln2σ2
(cid:63)
2

γµn
2

(cid:17)

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

γ3Ln2σ2
(cid:63)
2

.

Recursing for T iterations, we get that the ﬁnal iterate satisﬁes

206

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤

(cid:16)

1 −

(cid:16)

1 −

(cid:16)

1 −

≤

=

(cid:17)T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

(cid:17)T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γ3Ln2σ2
(cid:63)
2

γ3Ln2σ2
(cid:63)
2

(cid:32)T −1
(cid:88)

(cid:16)

1 −

(cid:19)

j=0
(cid:18) 2
γµn

(cid:17)T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ2κnσ2
(cid:63).
(cid:13)

γµn
2

γµn
2
γµn
2

(cid:33)

(cid:17)j

γµn
2

C.4.6 Proof of Theorem 3.4.6

Proof. We start with Lemma C.4.4, which states that the following inequality holds:

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

− 2γnE (cid:2)f (xk+1) − f (x(cid:63))(cid:3) +

Rearranging the result leads to

2γnE (cid:2)f (xk+1) − f (x(cid:63))(cid:3) ≤ E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

− E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

Summing these inequalities for k = 0, 1, . . . , T − 1 gives

γ3Ln2σ2
(cid:63)
2

.

γ3Ln2σ2
(cid:63)
2

.

2γn

T −1
(cid:88)

k=0

E (cid:2)f (xk+1) − f (x(cid:63))(cid:3) ≤

T −1
(cid:88)

k=0

(cid:16)

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

− E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)(cid:17)

+

(cid:63)T

γ3Ln2σ2
2

= (cid:107)x0 − x(cid:63)(cid:107)2 − E

2(cid:105)

+

(cid:63)T

γ3Ln2σ2
2

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)
(cid:63)T

γ3Ln2σ2
2

,

≤ (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

and dividing both sides by 2γnT , we get

1
T

T −1
(cid:88)

k=0

E (cid:2)f (xk+1) − f (x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
2γnT

+

γ2Lnσ2
(cid:63)
4

.

Finally, using convexity of f , the average iterate ˆxT

def= 1
T

(cid:80)T

k=1 xk satisﬁes

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

1
T

T
(cid:88)

k=1

E (cid:2)f (xk) − f (x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
2γnT

+

γ2Lnσ2
(cid:63)
4

.

(cid:4)

(cid:4)

C.4.7 Proof of complexity

207

Corollary C.4.5. Under the same conditions as Theorem 3.4.6, choose the stepsize

γ = min






√

1
2Ln

,

(cid:32)

(cid:107)x0 − x(cid:63)(cid:107)2
Ln2T σ2
(cid:63)

(cid:33)1/3


.



Then

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

L(cid:107)x0 − x(cid:63)(cid:107)2
√
2T

+

3L1/3 (cid:107)x0 − x(cid:63)(cid:107)4/3 σ2/3
4n1/3T 2/3

(cid:63)

.

We can guarantee E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤ ε2 provided that the total number of iterations
satisﬁes

T n ≥

2(cid:107)x0 − x(cid:63)(cid:107)2√
ε2

Ln

max

(cid:110)√

2Ln,

(cid:111)

.

σ(cid:63)
ε

Proof. We start with the guarantee of Theorem 3.4.6:

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
2γnT

+

γ2Lnσ2
(cid:63)
4

.

(C.13)

We now have two cases depending on the stepsize:

• Case 1: If γ = 1√

2Ln

(cid:18) (cid:107)x0−x(cid:63)(cid:107)2
Ln2T σ2
(cid:63)

≤

(cid:19)1/3

, then plugging this γ into (C.13) gives

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

≤

=

L(cid:107)x0 − x(cid:63)(cid:107)2
√
2T
L(cid:107)x0 − x(cid:63)(cid:107)2
√
2T
L(cid:107)x0 − x(cid:63)(cid:107)2
√
2T

+

+

+

γ2Lnσ2
(cid:63)
4
(cid:107)x0 − x(cid:63)(cid:107)2
Ln2T σ2
(cid:63)

(cid:32)

(cid:33)2/3

Lnσ2
(cid:63)
4

L1/3σ2/3

(cid:63) (cid:107)x0 − x(cid:63)(cid:107)4/3
4n1/3T 2/3

.

(C.14)

• Case 2: If γ =

(cid:18) (cid:107)x0−x(cid:63)(cid:107)2
Ln2T σ2
(cid:63)

(cid:19)1/3

≤ 1√

2Ln

, then plugging this γ into (C.13) gives

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

=

(cid:63)

L1/3 (cid:107)x0 − x(cid:63)(cid:107)4/3 σ2/3
2n1/3T 2/3
3L1/3 (cid:107)x0 − x(cid:63)(cid:107)4/3 σ2/3
4n1/3T 2/3

(cid:63)

+

L1/3σ2/3

(cid:63) (cid:107)x0 − x(cid:63)(cid:107)4/3
4n1/3T 2/3

.

(C.15)

Combining (C.14) and (C.15), we see that in both cases we have

E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤

L(cid:107)x0 − x(cid:63)(cid:107)2
√
2T

+

3L1/3 (cid:107)x0 − x(cid:63)(cid:107)4/3 σ2/3
4n1/3T 2/3

(cid:63)

.

208

Translating this to sample complexity, we can guarantee that E (cid:2)f (ˆxT ) − f (x(cid:63))(cid:3) ≤ ε2
provided

nT ≥

2(cid:107)x0 − x(cid:63)(cid:107)2√
ε2

Ln

(cid:110)√

Ln,

max

(cid:111)

.

σ(cid:63)
ε

(cid:4)

C.5 Proofs for Non-Convex Objectives (Section 3.4.3)

C.5.1 Proof of Proposition 3.4.8

Proof. This proposition is a special case of Lemma 3 in [117] and we prove it here for
completeness. Let x ∈ Rd. We start with (1.17) (which does not require convexity)
applied to each fi:

(cid:107)∇fi(x)(cid:107)2 ≤ 2L (fi(x) − f (cid:63)

i ) .

Averaging, we derive

(cid:32)

(cid:107)∇fi(x)(cid:107)2 ≤ 2L

f (x) −

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

(cid:33)

f (cid:63)
i

(cid:32)

= 2L (f (x) − f (cid:63)) + 2L

f (cid:63) −

(cid:33)

f (cid:63)
i

.

1
n

n
(cid:88)

i=1

Note that because f (cid:63) is the inﬁmum of f (·) and 1
n
f (cid:63) − 1
n

i ≥ 0. We may now use the variance decomposition

i=1 f (cid:63)

i=1 f (cid:63)
i

(cid:80)n

(cid:80)n

is a lower bound on f then

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇f (x)(cid:107)2

(1.5)
=

≤

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:107)∇fi(x)(cid:107)2 − (cid:107)∇f (x)(cid:107)2

(cid:107)∇fi(x)(cid:107)2

(cid:32)

≤ 2L (f (x) − f (cid:63)) + 2L

f (cid:63) −

(cid:33)

f (cid:63)
i

.

1
n

n
(cid:88)

i=1

It follows that Assumption 3.4.7 holds with A = L and B2 = 2L (cid:0)f (cid:63) − 1

n

(cid:80)n

i=1 f (cid:63)
i

(cid:1). (cid:4)

C.5.2 Finding a per-epoch recursion

For this subsection and the rest of this section, we need to deﬁne the following quantity:

Deﬁnition C.5.1. For Algorithm 2 we deﬁne the backward per-epoch deviation at timestep
k by

V k def=

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)xk

i − xk(cid:13)
2
(cid:13)

.

209

We will study the convergence of Algorithm 2 for non-convex objectives as follows: we
ﬁrst derive a per-epoch recursion that involves V k in Lemma C.5.2, then we show that
V k can be bounded using smoothness and probability theory in Lemma C.5.3, and ﬁnally
combine these two to prove Theorem 3.4.9.

Lemma C.5.2. Suppose that Assumption 3.3.1 holds. Then for iterates xk generated by
Algorithm 2 with stepsize γ ≤ 1

Ln, we have

f (xk+1) ≤ f (xk) −

γn
2

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+

γL2
2

V k,

(C.16)

where V k is deﬁned as in Deﬁnition C.5.1.

Proof. Our approach for establishing this lemma is similar to that of [192, Theorem 1],
which we became aware of in the course of preparing this manuscript. Recall that xk+1 =
xk − γgk, where gk = (cid:80)n−1

i ). Using L-smoothness of f , we get

i=0 ∇fπi(xk

f (xk+1)

(1.15)

≤ f (xk) + (cid:10)∇f (xk), xk+1 − xk(cid:11) +
(cid:28)

(cid:29)

= f (xk) − γn

∇f (xk),

+

gk
n

(cid:32)

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1.9)
= f (xk) −

= f (xk) −

γn
2

γn
2

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

gk
n

.

+

γn
2

(cid:13)
(cid:13)
∇f (xk) −
(cid:13)
(cid:13)

L
2
γ2Ln2
2
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:13)xk+1 − xk(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2
gk
(cid:13)
(cid:13)
(cid:13)
(cid:13)
n
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∇f (xk) −
(cid:13)
(cid:13)

gk
n

−

−

γn
2

(1 − Lγn)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

gk
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

2(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

gk
n

+

γ2Ln2
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

gk
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(C.17)

By assumption, we have γ ≤ 1

Ln, and hence 1 − Lγn ≥ 0. Using this in (C.17), we get

f (xk+1) ≤ f (xk) −

γn
2

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+

γn
2

(cid:13)
(cid:13)
∇f (xk) −
(cid:13)
(cid:13)

gk
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

.

(C.18)

For the last term in (C.18), we note

(cid:13)
(cid:13)
∇f (xk) −
(cid:13)
(cid:13)

gk
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

=

(1.7)
≤

(1.14)
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n−1
(cid:88)

i=0

(cid:2)∇fπi(xk) − ∇fπi(xk

i )(cid:3)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

1
n

n−1
(cid:88)

i=0
n−1
(cid:88)

i=0

(cid:13)
(cid:13)∇fπi(xk) − ∇fπi(xk

i )(cid:13)
2
(cid:13)

L2(cid:13)

(cid:13)xk − xk
i

(cid:13)
2
(cid:13)

=

L2
n

V k.

Plugging in (C.19) into (C.18) yields the lemma’s claim.

(C.19)

(cid:4)

210

C.5.3 Bounding the backward per-epoch deviation

Lemma C.5.3. Suppose that Assumption 3.3.1 holds (with each fi possibly non-convex)
and that Algorithm 2 is used with a stepsize γ ≤ 1

2Ln. Then

Ek

(cid:2)V k(cid:3) ≤ γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ γ2n2σ2
k,

(C.20)

where V k is deﬁned as in Deﬁnition C.5.1 and σ2
k

def= 1
n

(cid:80)n

j=1

Proof. Let us ﬁx any m ∈ [1, n − 1] and ﬁnd an upper bound for Ek
note that

m = xk − γ
xk

∇fπi(xk

i ).

m−1
(cid:88)

Therefore, by Young’s inequality, Jensen’s inequality and gradient Lipschitzness

i=0

(cid:13)∇fj(xk) − ∇f (xk)(cid:13)
(cid:13)
2.
(cid:13)
2(cid:105)

(cid:104)(cid:13)
(cid:13)xk

m − xk(cid:13)
(cid:13)

. First,

= γ2Ek

Ek

(cid:2)(cid:107)xk
m − xk(cid:107)2(cid:3)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)


m−1
(cid:88)



(cid:13)
(cid:13)
∇fπi(xk
(cid:13)
i )
(cid:13)
(cid:13)

2



i=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m−1
(cid:88)

i=0

(1.10)
≤ 2γ2Ek



(cid:0)∇fπi(xk

i ) − ∇fπi(xk)(cid:1)

2
 + 2γ2Ek

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m−1
(cid:88)

i=0

2



(cid:13)
(cid:13)
∇fπi(xk)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
∇fπi(xk)
(cid:13)
(cid:13)
(cid:13)



(1.8)
≤ 2γ2k

m−1
(cid:88)

i=0

Ek

(1.14)
≤ 2γ2L2k

m−1
(cid:88)

i=0

(cid:104)(cid:13)
(cid:13)∇fπi(xk

i ) − ∇fπi(xk)(cid:13)
(cid:13)

2(cid:105)

+ 2γ2Ek

Ek

(cid:2)(cid:107)xk

i − xk(cid:107)2(cid:3) + 2γ2Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m−1
(cid:88)

i=0

m−1
(cid:88)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=0
(cid:13)
(cid:13)
∇fπi(xk)
(cid:13)
(cid:13)
(cid:13)

2
.

Let us bound the second term. For any i we have Ek
Lemma C.3.1 (with vectors ∇fπ0(xk), ∇fπ1(xk), . . . , ∇fπk−1(xk)) we obtain

(cid:2)∇fπi(xk)(cid:3) = ∇f (xk), so using

Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m−1
(cid:88)

i=0

(cid:13)
(cid:13)
∇fπi(xk)
(cid:13)
(cid:13)
(cid:13)

2

(1.5)

= m2(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ m2Ek







(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m−1
(cid:88)

i=0

(cid:13)
(cid:13)
(∇fπi(xk) − ∇f (xk))
(cid:13)
(cid:13)
(cid:13)

2



(C.1)

≤ m2(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+

m(n − m)
n − 1

σ2
k.

where σ2
k

def= 1
n

(cid:80)n

j=1

211
(cid:13)∇fj(xk) − ∇f (xk)(cid:13)
(cid:13)
2. Combining the produced bounds yields
(cid:13)

Ek

(cid:104)(cid:13)
(cid:13)xk

m − xk(cid:13)
(cid:13)

2(cid:105)

≤ 2γ2L2k

m−1
(cid:88)

i=0

Ek

(cid:104)(cid:13)
(cid:13)xk

i − xk(cid:13)
(cid:13)

2(cid:105)

+ 2γ2k2(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 2γ2 m(n − m)

n − 1

σ2
k

≤ 2γ2L2kE (cid:2)V k(cid:3) + 2γ2k2(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 2γ2 m(n − m)

n − 1

σ2
k,

whence

E (cid:2)V k(cid:3) =

n−1
(cid:88)

m=0

Ek

(cid:2)(cid:107)xk

m − xk(cid:107)2(cid:3)

≤ γ2L2n(n − 1)E (cid:2)V k(cid:3) +

γ2(n − 1)n(2n − 1)(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+

1
3

1
3

γ2n(n + 1)σ2
k.

Since E (cid:2)V k(cid:3) appears in both sides of the equation, we rearrange and use that γ ≤ 1
by assumption, which leads to

2Ln

E (cid:2)V k(cid:3) ≤

(1 − γ2L2n(n − 1))E (cid:2)V k(cid:3)

4
3
4
≤
9
≤ γ2n3(cid:13)

γ2(n − 1)n(2n − 1)(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ γ2n2σ2
k.

+

4
9

γ2n(n + 1)σ2
k

(cid:4)

C.5.4 A lemma for solving the non-convex recursion

Lemma C.5.4. Suppose that there exist constants a, b, c ≥ 0 and nonnegative sequences
(sk)T

k=0 such that for any k satisfying 0 ≤ k ≤ T we have the recursion

k=0, (qk)T

sk+1 ≤ (1 + a) sk − bqk + c.

(C.21)

Then, the following holds:

min
k=0,...,T −1

qk ≤

(1 + a)T
bT

s0 +

c
b

.

(C.22)

Proof. The ﬁrst part of the proof (for a > 0) is a distillation of the recursion solution in
Lemma 2 of [117] and we closely follow their proof. Deﬁne

wk

def=

1
(1 + a)k+1 .

Note that wk (1 + a) = wk−1 for all k. Multiplying both sides of (C.21) by wk,

wksk+1 ≤ (1 + a) wksk − bwkqk + cwk = wk−1sk − bwkqk + cwk.

Rearranging, we get bwkqk ≤ wk−1sk − wksk+1 + cwk. Summing up as k varies from 0 to
T − 1 and noting that the sum telescopes leads to

212

T −1
(cid:88)

k=0

bwkqk ≤

T −1
(cid:88)

k=0

(wk−1sk − wksk+1) + c

T −1
(cid:88)

k=0

wk

T −1
(cid:88)

k=0

wk

= w0s0 − wT −1sT + c

≤ w0s0 + c

T −1
(cid:88)

k=0

wk.

Let WT = (cid:80)T −1

k=0 wk. Dividing both sides by WT , we get

1
WT

T −1
(cid:88)

k=0

bwkqk ≤

w0s0
WT

+ c.

Note that the left-hand side of (C.23) satisﬁes

b min

k=0,...,T −1

qk ≤

1
WT

T −1
(cid:88)

k=0

bwkqk.

For the right-hand side of (C.23), we have

WT =

T −1
(cid:88)

k=0

wk ≥ T min

k=0,...,T −1

wk = T wT −1 =

T
(1 + a)T .

Substituting with (C.25) in (C.24) and dividing both sides by b, we ﬁnally get

min
k=0,...,T −1

qk ≤

(1 + a)T
bT

s0 +

c
b

.

(C.23)

(C.24)

(C.25)

(cid:4)

C.5.5 Proof of Theorem 3.4.9

Proof. Without PL. Taking expectation in Lemma C.5.2 and then using Lemma C.5.3,
we have that for any t ∈ {0, 1, . . . , T − 1},

(cid:2)f (xk+1)(cid:3) (C.16)

≤ f (xk) −

Ek

(C.20)
≤ f (xk) −

= f (xk) −

γn
2
γn
2
γn
2

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+

(cid:13)
(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+

(cid:0)1 − γ2L2n2(cid:1) (cid:13)

(cid:16)

(cid:2)V k(cid:3)

Ek

γL2
2
γL2
2
(cid:13)∇f (xk)(cid:13)
2
(cid:13)

γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

(cid:17)

+ γ2n2σ2
k

+

γ3L2n2σ2
k
2

.

213
def= f (xk) − f (cid:63). Adding −f (cid:63) to both sides and using Assumption 3.4.7,

Let δk

Ek [δk+1] ≤ δk −

γn
2

(cid:0)1 − γ2L2n2(cid:1) (cid:13)

≤ (cid:0)1 + γ3AL2n2(cid:1) δk −

+

(cid:13)∇f (xk)(cid:13)
2
(cid:13)
γn
2

(cid:0)1 − γ2L2n2(cid:1) (cid:13)

γ3L2n2σ2
k
2
(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+

γ3L2n2B2
2

.

Taking unconditional expectations in the last inequality and using that by assumption on
γ we have 1 − γ2L2n2 ≥ 1

2, we get the estimate

E [δk+1] ≤ (cid:0)1 + γ3AL2n2(cid:1) E [δk] −

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+

γn
4

γ3L2n2B2
2

.

(C.26)

Comparing (C.21) with (C.26) veriﬁes that the conditions of Lemma C.5.4 are readily
satisﬁed. Applying the lemma, we get

min
k=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤

4 (1 + γ3AL2n2)T
γnT

(cid:0)f (x0) − f (cid:63)(cid:1) + 2γ2L2nB2.

Using that 1 + x ≤ exp(x) and that the stepsize γ satisﬁes γ ≤ (AL2n2T )−1/3, we have

(cid:0)1 + γ3AL2n2(cid:1)T ≤ exp (cid:0)γ3AL2n2T (cid:1) ≤ exp (1) ≤ 3.

Using this in the previous bound, we ﬁnally obtain

min
k=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤

12 (f (x0) − f (cid:63))
γnT

+ 2γ2L2nB2.

With PL. Now we additionally assume that A = 0 and that 1

2(cid:107)∇f (x)(cid:107)2 ≥ µ(f (x) −

f (cid:63)). Then, (C.26) yields

E [δk+1] ≤ E [δk] −

≤ E [δk] −

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

+

E (cid:2)f (xk) − f (cid:63)(cid:3) +

γ3L2n2B2
2
γ3L2n2B2
2

E

γn
4
γµn
2
(cid:17)

(cid:16)

=

1 −

γµn
2

E [δk] +

γ3L2n2B2
2

.

As in the proof of Theorem 3.4.5, we recurse this bound to x0:

E [δT ] ≤

≤

=

(cid:16)

(cid:16)

1 −

1 −

(cid:16)

1 −

(cid:17)T

(cid:17)T

(cid:17)T

γµn
2

γµn
2
γµn
2

δ0 +

δ0 +

γ3L2n2B2
2

γ3L2n2B2
2

δ0 + γ2κLnB2.

(cid:17)j

γµn
2

T −1
(cid:88)

(cid:16)

1 −

j=0
2
γµn

214

(cid:4)

C.5.6 Proof of complexity

Corollary C.5.5. Choose the stepsize γ as

γ = min

(cid:26) 1
2Ln

,

1
A1/3L2/3n2/3T 1/3 ,

ε
√

2L

nB

(cid:27)

.

Then the minimum gradient norm satisﬁes

min
k=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤ ε2

provided the total number of iterations satisﬁes

T n ≥

√

48δ0L
ε2

n

max

(cid:26)√

n,

√

6δ0A
ε

(cid:27)

.

,

B
ε

Proof. From Theorem 3.4.9

min
k=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤

12 (f (x0) − f (cid:63))
γnT

+ 2γ2L2nB2.

Note that by condition on the stepsize 2L2γ2nB2 ≤ ε2/2, hence

min
k=0,...,T −1

E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤

12 (f (x0) − f (cid:63))
γnT

+

ε2
2

.

Thus, to make the squared gradient norm smaller than ε2 we require

12 (f (x0) − f (cid:63))
γnT

≤

ε2
2

,

or equivalently

nT ≥

24 (f (x0) − f (cid:63))
ε2γ

=

24δ0
ε2 max

(cid:26)

2Ln, (cid:0)AL2n2T (cid:1)1/3 ,

√

2L

(cid:27)

nB

ε

,

(C.27)

def= f (x0) − f (cid:63) and where we plugged in the value of the stepsize γ we use. Note
where δ0
that nT appears on both sides in the second term in the maximum in (C.27), hence we
can cancel out and simplify:

nT ≥

24δ0
ε2 (AL2n2T )1/3 ⇐⇒ nT ≥

√

(24δ0)3/2L

ε3

An

.

Using this simpliﬁed bound in (C.27) we obtain that mink=0,...,T −1 E

(cid:104)(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)

2(cid:105)

≤ ε2

Algorithm 13 Incremental Gradient (IG)

215

0 ∈ Rd, number of epochs T

Require: Stepsize γ > 0, initial vector x0 = x0
1: for epochs k = 0, 1, . . . , T − 1 do
for i = 0, 1, . . . , n − 1 do
2:
i − γ∇fi+1(xk
i )
3:
4:
5:
6: end for

i+1 = xk
xk
end for
xk+1 = xk
n

provided

nT ≥

√

48δ0L
ε2

n

max

(cid:26)√

n,

√

6δ0A
ε

(cid:27)

.

,

B
ε

(cid:4)

C.6 Convergence Results for IG

In this section we present results that are extremely similar to the previously obtained
bounds for RR and SO. For completeness, we also provide a full description of IG in
Algorithm 13.

Theorem C.6.1. Suppose that Assumption 3.3.1 is satisﬁed. Then we have the following
results for the Incremental Gradient method:
• If each fi is µ-strongly convex: if γ ≤ 1

L , then

(cid:13)xT − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γ2Ln2σ2
(cid:63)
µ

.

By carefully choosing the stepsize as in Corollary C.4.1, we see that this result im-
plies that IG has sample complexity ˜O
κ +
in order to reach a point ˜x with
(cid:107)˜x − x(cid:63)(cid:107)2 ≤ ε.

κnσ(cid:63)
√
ε
µ

(cid:16)

(cid:17)

√

• If f is µ-strongly convex and each fi is convex: if γ ≤ 1√

2nL

, then

(cid:13)xT − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

≤

(cid:16)

1 −

γµn
2

(cid:17)T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + 2γ2κn2σ2
(cid:63).
(cid:13)

Using the same approach for choosing the stepsize as Corollary C.4.1, we see that IG
in this setting reaches an ε-accurate solution after ˜O
individual gradient
accesses.

κnσ(cid:63)
√
ε
µ

nκ +

(cid:17)

(cid:16)

√

• If each fi is convex: if γ ≤ 1√

2nL

, then

f (ˆxT ) − f (x(cid:63)) ≤

(cid:107)x0 − x(cid:63)(cid:107)2
2γnT

+

γ2Ln2σ2
(cid:63)
2

,

216

(cid:80)T

where ˆxT def= 1
, then the
T
average of iterate generated by IG is an ε-accurate solution (i.e., f (ˆxT ) − f (x(cid:63)) ≤ ε)
provided that the total number of iterations satisﬁes

k=1 xk. Choosing the stepsize γ = min

ε
Lnσ(cid:63)

2nL

√

,

(cid:110) 1√

√

(cid:111)

nT ≥

(cid:107)x0 − x(cid:63)(cid:107)2
ε

max

(cid:40)√

8nL,

√

(cid:41)

.

Lσ(cid:63)n
√
ε

• If each fi is possibly non-convex: if Assumption 3.4.7 holds with constants A, B ≥ 0

and γ ≤ min

(cid:110) 1√

,

1
(4L2n3AT )1/3

8nL

(cid:111)

, then

min
k=0,...,T −1

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

≤

12 (f (x0) − f (cid:63))
γnT

+ 8γ2L2n2B2.

Using an approach similar to Corollary C.5.5, we can establish that IG reaches a point
with gradient norm less than ε provided that the total number of iterations exceeds

nT ≥

48 (f (x0) − f (cid:63)) Ln
ε2

max

(cid:40)√

(cid:112)24 (f (x0) − f (cid:63)) A
ε

,

2B
ε

2,

(cid:41)

.

The proof of Thoerem C.6.1 is given in the rest of the section, but ﬁrst we brieﬂy
discuss the convergence rates and the relation of the result on strongly convex objectives
to the lower bound of [231].

Discussion of the convergence rates. A brief comparison between the sample com-
plexities given for IG in Theorem C.6.1 and those given for RR (in Table 3.1) reveals that
IG has similar rates to RR but with a worse dependence on n in the variance term (the
term associated with σ(cid:63) in the convex case and B in the non-convex case), in particular
n. This diﬀerence is signiﬁcant in the large-scale machine
IG is worse by a factor of
learning regime, where the number of data points n can be on the order of thousands to
millions.

√

Discussion of existing lower bounds.
κ = 1)

[231] give the lower bound (in a problem with

(cid:13)xT − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

= Ω

(cid:18) σ2
(cid:63)
µ2T 2

(cid:19)

.

This implies a sample complexity of O

(cid:17)

(cid:16) nσ(cid:63)
√
ε
µ

, which matches our upper bound (up

to an extra iteration and log factors) in the case each fi is strongly convex and κ = 1.

C.6.1 Preliminary lemmas for Theorem C.6.1

Two lemmas for convex objectives

217

Lemma C.6.2. Consider the iterates of Incremental Gradient (Algorithm 13). Suppose
that functions f1, . . . , fn are convex and that Assumption 3.3.1 is satisﬁed. Then it holds

n−1
(cid:88)

k=0

(cid:13)
(cid:13)xk

m − xk+1(cid:13)
2
(cid:13)

≤ 4γ2Ln2

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk

i ) + 2γ2n3σ2
(cid:63),

(C.28)

where σ2

(cid:63) is the variance at the optimum given by σ2
(cid:63)

def= 1
n

(cid:80)n

i=1 (cid:107)∇fi(x(cid:63))(cid:107)2.

Proof. The proof of this Lemma is similar to that of Lemma C.4.3 but with a worse
dependence on the variance term, since there is no randomness in IG. Fix any m ∈
{0, . . . , n − 1}. It holds by deﬁnition

m − xk+1 = γ
xk

n−1
(cid:88)

i=k

∇fi+1(xk

i ) = γ

n−1
(cid:88)

i=k

(∇fi+1(xk

i ) − ∇fi+1(x(cid:63))) + γ

n−1
(cid:88)

i=k

∇fi+1(x(cid:63)).

Applying Young’s inequality to the sums above yields

(cid:107)xk

m − xk+1(cid:107)2

(1.10)
≤ 2γ2

(∇fi+1(xk

(cid:13)
2
(cid:13)
i ) − ∇fi+1(x(cid:63)))
(cid:13)
(cid:13)
(cid:13)

+ 2γ2

n−1
(cid:88)

i=k

n−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

n−1
(cid:88)

i=k

(1.8)
≤ 2γ2n

(cid:13)
(cid:13)∇fi+1(xk

i ) − ∇fi+1(x(cid:63))(cid:13)
2
(cid:13)

+ 2γ2

(1.18)
≤ 4γ2Ln

≤ 4γ2Ln

n−1
(cid:88)

i=k

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk

i ) + 2γ2

Dfi+1(x(cid:63), xk

i ) + 2γ2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

n−1
(cid:88)

i=k

i=k
(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

.

Summing up,

n−1
(cid:88)

m=0

(cid:13)
(cid:13)xk

m − xk+1(cid:13)
2
(cid:13)

≤ 4γ2Ln2

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk

i ) + 2γ2

n−1
(cid:88)

m=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

.

(C.29)

We now bound the second term in (C.29). We have

218

n−1
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n−1
(cid:88)

i=k

(cid:13)
2
(cid:13)
∇fi+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

(1.8)
≤

n−1
(cid:88)

(n − k)

n−1
(cid:88)

(cid:107)∇fi+1(x(cid:63))(cid:107)2

k=0
n−1
(cid:88)

k=0
n−1
(cid:88)

k=0

≤

=

(n − k)

i=k
n−1
(cid:88)

i=0

(cid:107)∇fi+1(x(cid:63))(cid:107)2

(n − k) nσ2

(cid:63) =

n2(n + 1)
2

(cid:63) ≤ n3σ2
σ2
(cid:63).

(C.30)

Using (C.30) in (C.29), we derive

n−1
(cid:88)

k=0

(cid:13)
(cid:13)xk

m − xk+1(cid:13)
2
(cid:13)

≤ 4γ2Ln2

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk

i ) + 2γ2n3σ2
(cid:63).

(cid:4)

Lemma C.6.3. Assume the functions f1, . . . , fn are convex and that Assumption 3.3.1
is satisﬁed. If Algorithm 13 is run with a stepsize γ ≤ 1√

, then

2Ln

(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

≤ (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

− 2γn (cid:0)f (xk+1) − f (x(cid:63))(cid:1) + γ3Ln3σ2
(cid:63).

Proof. The proof for this lemma is identical to Lemma C.4.4 but with the estimate
i − xk+1(cid:13)
of Lemma C.6.2 used for (cid:80)n−1
2 instead of Lemma C.4.3. We only in-
(cid:13)
clude it for completeness. Deﬁne the sum of gradients used in the k-th epoch as
gk def= (cid:80)n−1

i ). By deﬁnition of xk+1, we have xk+1 = xk − γgk. Using this,
(cid:107)xk − x(cid:63)(cid:107)2 = (cid:107)xk+1 + γgk − x(cid:63)(cid:107)2 = (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ (cid:10)gk, xk+1 − x(cid:63)(cid:11) + γ2(cid:107)gk(cid:107)2

i=0 ∇fi+1(xk

(cid:13)
(cid:13)xk

i=0

≥ (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ (cid:10)gk, xk+1 − x(cid:63)(cid:11)

= (cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γ

n−1
(cid:88)

i=0

(cid:10)∇fi+1(xk

i ), xk+1 − x(cid:63)(cid:11) .

For any i we have the following decomposition

(cid:10)∇fi+1(xk

i ), xk+1 − x(cid:63)(cid:11) = [fi+1(xk+1) − fi+1(x(cid:63))]
+ [fi+1(x(cid:63)) − fi+1(xk
− [fi+1(xk+1) − fi+1(xk

i ) − (cid:10)∇fi+1(xk

i ), xk

i ) − (cid:10)∇fi+1(xk
= [fi+1(xk+1) − fi+1(x(cid:63))] + Dfi+1(x(cid:63), xk

i − x(cid:63)(cid:11)]
i ), xk+1 − xk
i
i ) − Dfi+1(xk+1, xk

(cid:11)]

(C.31)

i ).
(C.32)

Summing the ﬁrst quantity in (C.32) over i from 0 to n − 1 gives

219

n−1
(cid:88)

[fi+1(xk+1) − fi+1(x(cid:63))] = n(f (xk+1) − f (cid:63)).

i=0

Now let us work out the third term in the decomposition (C.32) using L-smoothness,

Dfi+1(xk+1, xk

i ) ≤

L
2

(cid:107)xk+1 − xk

i (cid:107)2.

We next sum the right-hand side over i from 0 to n − 1 and use Lemma C.6.2

n−1
(cid:88)

i=0

Dfi+1(xk+1, xk

i ) ≤

L
2

n−1
(cid:88)

i=0

(cid:13)
(cid:13)xk+1 − xk
i

(cid:13)
2
(cid:13)

(C.28)
≤ 2γ2L2n2

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk

i ) + γ2Ln3σ2
(cid:63).

Therefore, we can lower-bound the sum of the second and the third term in (C.32) as

n−1
(cid:88)

i=0

(Dfi+1(x(cid:63), xk

i ) − Dfi+1(xk+1, xk

i )) ≥

n−1
(cid:88)

i=0

Dfi+1(x(cid:63), xk
i )

(cid:32)

−

2γ2L2n2

n−1
(cid:88)

Dfi+1(x(cid:63), xk

i ) − γ2Ln3σ2
(cid:63)

(cid:33)

i=0
n−1
(cid:88)

i=0

= (1 − 2γ2L2n2)

≥ −γ2Ln3σ2
(cid:63),

Dfi+1(x(cid:63), xk

i ) − γ2Ln3σ2
(cid:63)

where in the third inequality we used that γ ≤ 1√
Plugging this back into the lower-bound on (cid:107)xk − x(cid:63)(cid:107)2 yields

2Ln

and that Dfi+1(x(cid:63), xk

i ) is nonnegative.

(cid:107)xk − x(cid:63)(cid:107)2 ≥ (cid:13)

(cid:13)xk+1 − x(cid:63)(cid:13)
2
(cid:13)

+ 2γn (cid:0)f (xk+1) − f (cid:63)(cid:1) − γ3Ln3σ2
(cid:63).

Rearranging the terms gives the result.

(cid:4)

A lemma for non-convex objectives

Lemma C.6.4. Suppose that Assumption 3.3.1 holds. Suppose that Algorithm 13 is used
with a stepsize γ > 0 such that γ ≤ 1

2Ln. Then we have,

n
(cid:88)

i=1

(cid:13)
(cid:13)xk

i − xk(cid:13)
2
(cid:13)

≤ 4γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 4γ2n3σ2
k,

(C.33)

where σ2
k

def= 1
n

(cid:80)n

j=1

220
(cid:13)∇fj(xk) − ∇f (xk)(cid:13)
(cid:13)
2.
(cid:13)

Proof. Let i ∈ {1, 2, . . . , n}. Then we can bound the deviation of a single iterate as,

(cid:13)
(cid:13)xk

i − xk(cid:13)
2
(cid:13)

=

(cid:13)
(cid:13)
xk
(cid:13)
0 − γ
(cid:13)
(cid:13)

i−1
(cid:88)

j=0

∇fj+1(xk

j ) − xk

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= γ2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i−1
(cid:88)

j=0

(1.8)
≤ γ2i

i−1
(cid:88)

j=0

∇fj+1(xk
j )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)∇fj+1(xk

j )(cid:13)
2
(cid:13)

.

Because i ≤ n, we have

(cid:13)
(cid:13)xk

i − xk(cid:13)
2
(cid:13)

≤ γ2i

i−1
(cid:88)

j=0

(cid:13)
(cid:13)∇fi+1(xk

j )(cid:13)
2
(cid:13)

≤ γ2n

i−1
(cid:88)

j=0

(cid:13)
(cid:13)∇fi+1(xk

j )(cid:13)
2
(cid:13)

≤ γ2n

n−1
(cid:88)

j=0

(cid:13)
(cid:13)∇fi+1(xk

j )(cid:13)
2
(cid:13)

.

(C.34)

Summing up allows us to estimate V k:

V k =

(C.34)
≤

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:13)
(cid:13)xk

i − xk(cid:13)
2
(cid:13)

(cid:32)

γ2n

n−1
(cid:88)

j=0

(cid:33)

(cid:13)
(cid:13)∇fj+1(xk

j )(cid:13)
2
(cid:13)

= γ2n2

n−1
(cid:88)

j=0

(cid:13)
(cid:13)∇fj+1(xk

j )(cid:13)
2
(cid:13)

(1.10)
≤ 2γ2n2

= 2γ2n2

n−1
(cid:88)

j=0

n−1
(cid:88)

j=0

(cid:16)(cid:13)
(cid:13)∇fj+1(xk

j ) − ∇fi+1(xk)(cid:13)
2
(cid:13)

+ (cid:13)

(cid:13)∇fj+1(xk)(cid:13)
(cid:13)

2(cid:17)

(cid:13)
(cid:13)∇fj+1(xk

j ) − ∇fj+1(xk)(cid:13)
2
(cid:13)

+ 2γ2n2

n−1
(cid:88)

j=0

(cid:13)∇fj+1(xk)(cid:13)
(cid:13)
2
.(C.35)
(cid:13)

For the ﬁrst term in (C.35) we can use the smoothness of individual losses and that
0 = xk:
xk

(cid:13)
(cid:13)∇fj+1(xk

j ) − ∇fj+1(xk)(cid:13)
(cid:13)

2 (1.14)

≤ L2

n−1
(cid:88)

j=0

n−1
(cid:88)

j=0

(cid:13)
(cid:13)xk

j − xk(cid:13)
2
(cid:13)

= L2

n−1
(cid:88)

j=1

(cid:13)
(cid:13)xk

j − xk(cid:13)
2
(cid:13)

= L2V k.

(C.36)

The second term in (C.35) is a sum over all the individual gradient evaluated at the
same point xk. Hence, we can drop the permutation subscript and then use the variance

decomposition:

221

n−1
(cid:88)

j=0

(cid:13)∇fi+1(xk)(cid:13)
(cid:13)
2
(cid:13)

=

n
(cid:88)

j=1

(cid:13)∇fj(xk)(cid:13)
(cid:13)
2
(cid:13)

(1.5)

= n(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

n
(cid:88)

+

(cid:13)∇fj(xk) − ∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

= n(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

j=1
+ nσ2
k.

(C.37)

We can then use (C.36) and (C.37) in (C.35),

V k ≤ 2γ2L2n2V k + 2γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 2γ2n3σ2
k.

Since V k shows up in both sides of the equation, we can rearrange to obtain

(cid:0)1 − 2γ2L2n2(cid:1) V k ≤ 2γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 2γ2n3σ2
k.

If γ ≤ 1

2Ln, then 1 − 2γ2L2n2 ≥ 1

2 and hence

V k ≤ 4γ2n3(cid:13)

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 4γ2n3σ2
k.

(cid:4)

C.6.2 Proof of Theorem C.6.1

Proof. • If each fi is µ-strongly convex: The proof follows that of Theorem 3.4.4.

Deﬁne

First, we have

i = x(cid:63) − γ
x(cid:63)

i−1
(cid:88)

j=0

∇fj+1(x(cid:63)).

(cid:107)xk
= (cid:107)xk

i+1 − x(cid:63)
i − x(cid:63)

i+1(cid:107)2
i (cid:107)2 − 2γ (cid:10)∇fi+1(xk

i ) − ∇fi+1(x(cid:63)), xk

i − x(cid:63)
i

(cid:11) + γ2(cid:107)∇fi+1(xk

i ) − ∇fi+1(x(cid:63))(cid:107)2.

Using the same three-point decomposition as Theorem 3.4.4 and strong convexity, we
have

− (cid:10)∇fi+1(xk

i ) − ∇fi+1(x(cid:63)), xk

i − x(cid:63)
i

(cid:11) = −Dfi+1(x(cid:63)
µ
(cid:13)
(cid:13)xk
≤ −
2

i ) − Dfi+1(xk
i , xk
(cid:13)
2
− Dfi+1(xk
i − x(cid:63)
(cid:13)
i

i , x(cid:63)) + Dfi+1(x(cid:63)
i , x(cid:63)) + Dfi+1(x(cid:63)

i , x(cid:63))
i , x(cid:63)).

Using smoothness and convexity

1
2L

(cid:13)
(cid:13)∇fi+1(xk

i ) − ∇fi+1(x(cid:63))(cid:13)
2
(cid:13)

≤ Dfi+1(xk

i , x(cid:63)).

Plugging in the last two inequalities into the recursion, we get

222

(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

(cid:13)
2
(cid:13)

≤ (1 − γµ) (cid:13)
≤ (1 − γµ) (cid:13)

(cid:13)xk
(cid:13)xk

i − x(cid:63)
i
i − x(cid:63)
i

(cid:13)
2
(cid:13)
(cid:13)
2
(cid:13)

− 2γ (1 − γL) Dfi+1(xk
+ 2γDfi+1(x(cid:63)

i , x(cid:63)).

i , x(cid:63)) + 2γDfi+1(x(cid:63)

i , x(cid:63)).
(C.38)

For the last Bregman divergence, we have

Dfi+1(x(cid:63)

i , x(cid:63))

(1.15)
≤

=

(1.8)
≤

=

Plugging this into (C.38), we get

(cid:107)x(cid:63)

L
2
γ2L
2

i − x(cid:63)(cid:107)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i−1
(cid:88)

j=0

(cid:13)
2
(cid:13)
∇fj+1(x(cid:63))
(cid:13)
(cid:13)
(cid:13)

γ2Li
2

i−1
(cid:88)

j=0

(cid:107)∇fj+1(x(cid:63))(cid:107)2

γ2Lin
2

σ2
(cid:63) ≤

γ2Ln2
2

σ2
(cid:63).

(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

(cid:13)
2
(cid:13)

≤ (1 − γµ) (cid:13)

(cid:13)xk

i − x(cid:63)
i

(cid:13)
2
(cid:13)

+ γ3Ln2σ2
(cid:63).

We recurse and then use that x(cid:63)

n = x(cid:63), xk+1 = xk

n, and that x(cid:63)

0 = x(cid:63), obtaining

= (cid:13)

(cid:13)xk

n − x(cid:63)
n

(cid:13)
2
(cid:13)

≤ (1 − γµ)n (cid:13)

(cid:13)xk

0 − x(cid:63)
0

(cid:13)
2
(cid:13)

+ γ3Ln2σ2
(cid:63)

= (1 − γµ)n (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

+ γ3Ln2σ2
(cid:63)

n−1
(cid:88)

j=0

n−1
(cid:88)

j=0

(1 − γµ)j

(1 − γµ)j .

(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

Recursing again,

(cid:13)xT − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

≤ (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ3Ln2σ2
(cid:13)
(cid:63)

n−1
(cid:88)

(1 − γµ)j

T −1
(cid:88)

k=0

(1 − γµ)nt

= (1 − γµ)nT (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + γ3Ln2σ2
(cid:13)
(cid:63)

j=0

nT −1
(cid:88)

k=0

≤ (1 − γµ)nT (cid:13)

= (1 − γµ)nT (cid:13)

γ3Ln2σ2
(cid:63)
γµ

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
2 + γ2κn2σ2
(cid:63).
(cid:13)

(1 − γµ)k

• If f is µ-strongly convex and each fi is convex: the proof is identical to that of
Theorem 3.4.5 but using Lemma C.6.3 instead of Lemma C.4.4, and we omit it for
brevity.

• If each fi

is convex:

the proof is identical to that of Theorem 3.4.6 but using

Lemma C.6.3 instead of Lemma C.4.4, and we omit it for brevity.

223

• If each fi is possibly non-convex: note that Lemma C.5.2 also applies to IG without

change, hence if γ ≤ 1

Ln we have

f (xk+1) ≤ f (xk) −

γn
2

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+

γL2
2

n
(cid:88)

i=1

(cid:13)
(cid:13)xk − xk
i

(cid:13)
2
(cid:13)

.

We may then apply Lemma C.6.4 to get for γ ≤ 1
2Ln

f (xk+1) ≤ f (xk) −

= f (xk) −

γn
2
γn
2

(cid:16)

γL2
4γ2n3(cid:13)
(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)
2
(cid:13)∇f (xk)(cid:13)
(cid:0)1 − 4γ2L2n2(cid:1) (cid:13)
2
(cid:13)

+

(cid:13)∇f (xk)(cid:13)
2
(cid:13)

+ 4γ2n3σ2
k

(cid:17)

+ 2γ3L2n3σ2
k.

Using that γ ≤ 1√

8Ln

and subtracting f (cid:63) from both sides, we derive

f (xk+1) − f (cid:63) ≤ (cid:0)f (xk) − f (cid:63)(cid:1) −

γn
4

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+ 2γ3L2n3σ2
k.

Using Assumption 3.4.7, we get

f (xk+1)−f (cid:63) ≤ (cid:0)1 + 4γ3L2An3(cid:1) (cid:0)f (xk) − f (cid:63)(cid:1)−

γn
4

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

+2γ3L2n3B2. (C.39)

Applying Lemma C.5.4 to (C.39), thus, gives

min
k=0,...,T −1

(cid:13)∇f (xk)(cid:13)
(cid:13)
2
(cid:13)

≤

4 (1 + 4γ3L2An3)T
γnT

(cid:0)f (x0) − f (cid:63)(cid:1) + 8γ2L2n2B2.

(C.40)

Note that by our assumption on the stepsize, 4γ3L2An3T ≤ 1, hence,

(cid:0)1 + 4γ3L2An3(cid:1)T ≤ exp (cid:0)4γ3L2An3T (cid:1) ≤ exp (1) ≤ 3.

It remains to use this in (C.40).

(cid:4)

224

Appendix D

Appendix for Chapter 4

D.1 Proof of Theorem 4.4.3

Proof. By the Li-smoothness of fi and the deﬁnition of x(cid:63)
divergence in (4.4) with the bound

i , we can replace the Bregman

E (cid:2)Dfπi

i , x(cid:63))(cid:3) (1.15)

≤ E

(x(cid:63)

(cid:20)Lπi
2

(cid:107)x(cid:63)

i − x(cid:63)(cid:107)2

(cid:21)

≤

Lmax
2

E (cid:2)(cid:107)x(cid:63)

i−1
(cid:88)

i − x(cid:63)(cid:107)2(cid:3)
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

i−1
(cid:88)

1
i

j=0

j=0

E

E

2(cid:35)

(cid:13)
(cid:13)
∇fπj (x(cid:63))
(cid:13)
(cid:13)

2(cid:35)

(cid:13)
(cid:13)
∇fπj (x(cid:63))
(cid:13)
(cid:13)

E

(cid:104)(cid:13)
(cid:13) ¯Xπ

2(cid:105)

(cid:13)
(cid:13)

,

(D.1)

(3.4)
=

γ2Lmax
2

=

=

γ2Lmaxi2
2

γ2Lmaxi2
2

where ¯Xπ = 1
j
applying Lemma 4.4.2 we get

j=0 Xπj with Xj

(cid:80)i−1

def= ∇fj(x(cid:63)) for j = 1, 2, . . . , n. Since ¯X = ∇f (x(cid:63)), by

E

(cid:104)(cid:13)
(cid:13) ¯Xπ

2(cid:105)

(cid:13)
(cid:13)

= (cid:13)

(cid:13) ¯X(cid:13)
2 + E
(cid:13)

(cid:104)(cid:13)
(cid:13) ¯Xπ − ¯X(cid:13)
(cid:13)

2(cid:105) (4.5)+(4.6)

=

(cid:107)∇f (x(cid:63))(cid:107)2 +

n − i
i(n − 1)

σ2
(cid:63).

(D.2)

It remains to combine (D.1) and (D.2), use the bounds i2 ≤ n2 and i(n − i) ≤ n(n−1)
,
which holds for all i ∈ {0, 1, . . . , n − 1}, and divide both sides of the resulting inequality
(cid:4)
by γ2.

2

D.2 Main Convergence Proofs

D.2.1 A key lemma for shuﬄing-based methods

The intermediate limit points x(cid:63)
i are extremely important for showing tight convergence
guarantees for Random Reshuﬄing even without proximal operator. The following lemma
illustrates that by giving a simple recursion, whose derivation follows [169, Proof of The-
orem 1]. The proof is included for completeness.

Lemma D.2.1 (Theorem 1 in [169]). Suppose that each fi is Li-smooth and λ-strongly
convex (where λ = 0 means each fi is just convex). Then the inner iterates generated by

Algorithm 4 satisfy

225

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ (1 − γλ) E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

− 2γ (1 − γLmax) E (cid:2)Dfπi

(xk

i , x(cid:63))(cid:3)

+ 2γ3σ2

rad,

where x(cid:63)

i is as in (3.4), i = 0, 1, . . . , n − 1, and x(cid:63) is any minimizer of P .

Proof. By deﬁnition of xk

i+1 and x(cid:63)

i+1, we have

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

= E

(cid:104)(cid:13)
(cid:13)xk

2(cid:105)

(cid:13)
(cid:13)

i − x(cid:63)
i
(cid:104)(cid:13)
(cid:13)∇fπi(xk

− 2γE (cid:2)(cid:10)∇fπi(xk
i ) − ∇fπi(x(cid:63))(cid:13)
(cid:13)

+ γ2E

.

i ) − ∇fπi(x(cid:63)), xk
2(cid:105)

i − x(cid:63)
i

Note that the third term in (D.4) can be bounded as

(cid:13)
(cid:13)∇fπi(xk

i ) − ∇fπi(x(cid:63))(cid:13)
2
(cid:13)

≤ 2Lmax · Dfπi

(xk

i , x(cid:63)).

(D.3)

(cid:11)(cid:3)

(D.4)

(D.5)

We may rewrite the second term in (D.4) using the three-point identity [46, Lemma 3.1]
as

(cid:10)∇fπi(xk

i ) − ∇fπi(x(cid:63)), xk

i − x(cid:63)
i

(cid:11) = Dfπi

(x(cid:63)

i , xk

i ) + Dfπi

(xk

i , x(cid:63)) − Dfπi

(x(cid:63)

i , x(cid:63)). (D.6)

Combining (D.4), (D.5), and (D.6) we obtain

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

− 2γ · E (cid:2)Dfπi
− 2γ (1 − γLmax) E (cid:2)Dfπi

i )(cid:3) + 2γ · E (cid:2)Dfπi
(xk

i , x(cid:63))(cid:3) .

i , xk

(x(cid:63)

Using λ-strong convexity of fπi, we derive

λ
2

(cid:13)
(cid:13)xk

i − x(cid:63)
i

(cid:13)
2
(cid:13)

≤ Dfπi

(x(cid:63)

i , xk

i ).

Furthermore, by the deﬁnition of shuﬄing radius (Deﬁnition 4.4.1), we have

E (cid:2)Dfπi

(x(cid:63)

i , x(cid:63))(cid:3) ≤ max

i=0,...,n−1

E (cid:2)Dfπi

(x(cid:63)

i , x(cid:63))(cid:3) = γ2σ2

rad.

Using (D.8) and (D.9) in (D.7) yields (D.3).

(x(cid:63)

i , x(cid:63))(cid:3)

(D.7)

(D.8)

(D.9)

(cid:4)

D.2.2 Proof of Theorem 4.5.1

Proof. Starting with Lemma D.2.1 with λ = µ, we have

226

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ (1 − γµ) E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

− 2γ (1 − γLmax) E (cid:2)Dfπi

(xk

i , x(cid:63))(cid:3)

+ 2γ3σ2

rad.

Since Dfπ (xk
bining this with the fact that the stepsize satisﬁes γ ≤ 1

i , x(cid:63)) is a Bregman divergence of a convex function, it is nonnegative. Com-

, we have

Lmax

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ (1 − γµ) E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

+ 2γ3σ2

rad.

Unrolling this recursion for n steps, we get

E

(cid:104)(cid:13)
(cid:13)xk

n − x(cid:63)
n

2(cid:105)

(cid:13)
(cid:13)

≤ (1 − γµ)n E

(cid:104)(cid:13)
(cid:13)xk

0 − x(cid:63)
0

2(cid:105)

(cid:13)
(cid:13)

+ 2γ3σ2

rad

= (1 − γµ)n E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

rad

(1 − γµ)j

(cid:33)

(cid:33)

(1 − γµ)j

,

(D.10)

(cid:32)n−1
(cid:88)

j=0
(cid:32)n−1
(cid:88)

j=0

where we used the fact that xk
Proposition 1.10.5 that

0 − x(cid:63)

0 = xk − x(cid:63). Since x(cid:63) minimizes P , we have by

(cid:32)

x(cid:63) = proxγnψ

x(cid:63) − γ

n−1
(cid:88)

i=0

(cid:33)

∇fπi(x(cid:63))

= proxγnψ (x(cid:63)

n) .

Moreover, by Lemma 1.10.6 we obtain that

(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

Using this in (D.10) yields

= (cid:13)

(cid:13)proxγnψ(xk

n) − proxγnψ(x(cid:63)

n)(cid:13)
2
(cid:13)

≤ (cid:13)

(cid:13)xk

n − x(cid:63)
n

(cid:13)
2
(cid:13)

.

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)n E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

rad

(cid:33)

(1 − γµ)j

.

(cid:32)n−1
(cid:88)

j=0

We now unroll this recursion again for T steps

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)nT E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

rad

(cid:32)n−1
(cid:88)

j=0

(1 − γµ)j

(cid:33) (cid:32)T −1
(cid:88)

i=0

(cid:33)

(1 − γµ)ni

.

(D.11)

Following the analysis of Chapter 3, we rewrite and bound the product in the last term as

227

(cid:32)n−1
(cid:88)

j=0

(1 − γµ)j

(cid:33) (cid:32)T −1
(cid:88)

i=0

(cid:33)

(1 − γµ)ni

=

n−1
(cid:88)

T −1
(cid:88)

j=0

i=0

(1 − γµ)ni+j

=

≤

nT −1
(cid:88)

(1 − γµ)k

k=0
∞
(cid:88)

k=0

(1 − γµ)k =

1
γµ

.

It remains to plug this bound into (D.11).

(cid:4)

D.2.3 Proof of Theorem 4.6.1

Proof. Starting with Lemma D.2.1 with λ = 0, we have

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

− 2γ (1 − γLmax) E (cid:2)Dfπi

(xk

i , x(cid:63))(cid:3) + 2γ3σ2

rad.

Since γ ≤ 1

Lmax

and Dfπ (xk

i , x(cid:63)) is nonnegative we may simplify this to

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

+ 2γ3σ2

rad.

Unrolling this recursion over an epoch we have

E

(cid:104)(cid:13)
(cid:13)xk

n − x(cid:63)
n

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

0 − x(cid:63)
0

2(cid:105)

(cid:13)
(cid:13)

+ 2γ3σ2

radn = E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

radn. (D.12)

Since x(cid:63) minimizes P , we have by Proposition 1.10.5 that

(cid:32)

x(cid:63) = proxγnψ

x(cid:63) − γ

n−1
(cid:88)

i=0

(cid:33)

∇fπi(x(cid:63))

= proxγnψ (x(cid:63)

n) .

Hence, xk+1 − x(cid:63) = proxγnψ(xk

n) − proxγnψ(x(cid:63)

n). We may now use Lemma 1.10.6 to get

(1 + 2γµn) E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ E

(cid:104)(cid:13)
(cid:13)xk

n − x(cid:63)
n

2(cid:105)

(cid:13)
(cid:13)

.

Combining this with (D.12), we obtain

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤

1
1 + 2γµn

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

2γ3σ2
radn
1 + 2γµn

.

We may unroll this recursion again, this time for T steps, and then use that (cid:80)T −1
(cid:80)∞

j=1 (1 + 2γµn)−j = 1/(2γµn):

j=1 (1 + 2γµn)−j ≤

228

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

2γ3σ2
radn
1 + 2γµn

(cid:32)T −1
(cid:88)

j=0

(cid:33)

(1 + 2γµn)−j

= (1 + 2γµn)−T E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

radn

(cid:32) T

(cid:88)

(cid:33)

(1 + 2γµn)−j

≤ (1 + 2γµn)−T E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ3σ2

radn

= (1 + 2γµn)−T E

(cid:104)(cid:13)
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+

rad

γ2σ2
µ

.

j=1
1
2γµn

(cid:4)

D.3 Convergence of SGD (Proof of Theorem 4.5.2)

Proof. We will prove the case when ψ is µ-strongly convex. The other result follows as a
straightforward special case of [81, Theorem 4.1]. We start by analyzing one step of SGD
with stepsize γk = γ and using Proposition 1.10.5

(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)
2
(cid:13)

= (cid:13)

(cid:13)proxγψ(xk − γ∇f (xk; ξ)) − proxγψ(x(cid:63) − γ∇f (x(cid:63)))(cid:13)
2
(cid:13)
(cid:13)xk − γ∇f (xk; ξ) − (x(cid:63) − γ∇f (x(cid:63)))(cid:13)
(cid:13)
2
(cid:13)

.

≤

1
1 + 2γµ

(D.13)

We may write the squared norm term in (D.13) as

(cid:13)xk − γ∇f (xk; ξ) − (x(cid:63) − γ∇f (x(cid:63)))(cid:13)
(cid:13)
2
(cid:13)
− 2γ (cid:10)xk − x(cid:63), ∇f (xk; ξ) − ∇f (x(cid:63))(cid:11) + γ2(cid:13)
= (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

(cid:13)∇f (xk; ξ) − ∇f (x(cid:63))(cid:13)
2
.
(cid:13)
(D.14)

Recall that we denote by Ek [·] expectation conditional on xk. Note that the gradient es-
timate is conditionally unbiased, i.e., that Ek
i=1 ∇fi(xk) = ∇f (xk).
Hence, taking conditional expectation in (D.14) and using unbiasedness we have

(cid:2)∇f (xk; ξ)(cid:3) = 1

(cid:80)n

n

Ek
= (cid:13)

(cid:104)(cid:13)
(cid:13)xk − γ∇f (xk; ξ) − (x(cid:63) − γ∇f (x(cid:63)))(cid:13)
(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

− 2γ (cid:10)xk − x(cid:63), ∇f (xk) − ∇f (x(cid:63))(cid:11) + γ2Ek

2(cid:105)

(cid:104)(cid:13)
(cid:13)∇f (xk; ξ) − ∇f (x(cid:63))(cid:13)
(cid:13)
(D.15)

2(cid:105)

.

By the convexity of f we have

(cid:10)xk − x(cid:63), ∇f (xk) − ∇f (x(cid:63))(cid:11) ≥ Df (xk, x(cid:63)).

229

Furthermore, we may estimate the third term in (D.15) by ﬁrst using the fact that
(cid:107)x + y(cid:107)2 ≤ 2(cid:107)x(cid:107)2 + 2(cid:107)y(cid:107)2 for any two vectors x, y ∈ Rd

(cid:104)(cid:13)
(cid:13)∇f (xk; ξ) − ∇f (x(cid:63))(cid:13)
(cid:13)

2(cid:105)

Ek

≤ 2Ek

(cid:104)(cid:13)
(cid:13)∇f (xk; ξ) − ∇f (x(cid:63); ξ)(cid:13)
(cid:13)

2(cid:105)

(cid:2)(cid:107)∇f (x(cid:63); ξ) − ∇f (x(cid:63))(cid:107)2(cid:3)
2(cid:105)

+ 2Ek
(cid:104)(cid:13)
(cid:13)∇f (xk; ξ) − ∇f (x(cid:63); ξ)(cid:13)
(cid:13)

= 2Ek

+ 2σ2
(cid:63).

We now use that by the Lmax-smoothness of fi we have that
(cid:13)∇fi(xk) − ∇fi(x(cid:63))(cid:13)
(cid:13)
2
(cid:13)

≤ 2Lmax · Dfi(xk, x(cid:63)).

Hence

(cid:104)(cid:13)
(cid:13)∇f (xk; ξ) − ∇f (x(cid:63); ξ)(cid:13)
(cid:13)

2(cid:105)

Ek

=

≤

1
n

i=1
2Lmax
n

n
(cid:88)

(cid:13)∇fi(xk) − ∇fi(x(cid:63))(cid:13)
(cid:13)
2
(cid:13)

n
(cid:88)

(cid:2)fi(xk) − fi(x(cid:63)) − (cid:10)∇fi(x(cid:63)), xk − x(cid:63)(cid:11)(cid:3)

i=1

= 2Lmax
= 2LmaxDf (xk, x(cid:63)).

(cid:2)f (xk) − f (x(cid:63)) − (cid:10)∇f (x(cid:63)), xk − x(cid:63)(cid:11)(cid:3)

(D.16)

Combining equations (D.15)–(D.16) we obtain

(cid:104)(cid:13)
(cid:13)xk − γ∇f (xk; ξ) − (x(cid:63) − γ∇f (x(cid:63)))(cid:13)
(cid:13)

2(cid:105)

Ek

≤ (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)
+ 2γ2σ2
(cid:63).

− 2γ (1 − 2γLmax) Df (xk, x(cid:63))

Since γ ≤ 1
the convexity of f we arrive at

2Lmax

by assumption we have that 1 − 2γLmax ≥ 0. Since Df (xk, x(cid:63)) ≥ 0 by

(cid:104)(cid:13)
(cid:13)xk − γ∇f (xk; ξ) − (x(cid:63) − γ∇f (x(cid:63)))(cid:13)
(cid:13)

2(cid:105)

Ek

≤ (cid:13)

(cid:13)xk − x(cid:63)(cid:13)
2
(cid:13)

+ 2γ2σ2
(cid:63).

Taking unconditional expectation and combining (D.18) with the last equation we have

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤

=

≤

1
1 + 2γµ
1
1 + 2γµ
1
1 + 2γµ

(cid:16)

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ2σ2
(cid:63)

(cid:17)

E

E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)
(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

2(cid:105)

+

2γ2σ2
(cid:63)
1 + 2γµ

+ 2γ2σ2
(cid:63).

230

To simplify this further, we use that for any x ≤ 1
γµ ≤ µ

≤ 1

2, hence

2Lmax

2 we have that

1

1+2x ≤ 1 − x and that

E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ) E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

+ 2γ2σ2
(cid:63).

Recursing the above inequality for K steps yields

E

(cid:104)(cid:13)
(cid:13)xK − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 − γµ)K (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + 2γ2σ2
(cid:13)
(cid:63)

≤ (1 − γµ)K (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 + 2γ2σ2
(cid:13)
(cid:63)

= (1 − γµ)K (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

2γσ2
(cid:63)
µ

.

(cid:33)

(1 − γµ)k

(cid:33)

(1 − γµ)k

(cid:32)K−1
(cid:88)

k=0
(cid:32) ∞
(cid:88)

k=0

(cid:4)

D.4 Proofs for Decreasing Stepsize

We ﬁrst state and prove the following algorithm-independent lemma. This lemma plays
a key role in the proof of Theorem 4.7.1 and is heavily inspired by the stepsize schemes
of [253] and [117] and their proofs.

Lemma D.4.1. Suppose that there exist constants a, b, c ≥ 0 such that for all γk ≤ 1
b
we have

(1 + γkan) rk+1 ≤ rk + γ3

kc.

(D.17)

Fix T > 0. Let k0 = (cid:100) T

2 (cid:101). Then choosing stepsizes γk > 0 by

where s = 7b

2an. Then

γk =

(cid:40) 1
b ,

7
an(s+k−k0)

if k ≤ k0 or T ≤ b
an,
if k > k0 and T > b
an,

rT ≤ exp

(cid:18)

−

nT
2 (b/a + n)

(cid:19)

r0 +

1421c
a3n3T 2 .

Proof. If T ≤ 7b

an, then we have γk = γ = 1

b for all k. Hence recursing we have,

rT ≤ (1 + γan)−T r0 +

γ3c
γan

= (1 + γan)−T r0 +

γ2c
an

.

Note that

1

1+x ≤ exp(− x

1+x ) for all x, hence

231

Substituting for γ yields

rT ≤ exp

(cid:18)

−

γanT
1 + γan

(cid:19)

r0 +

γ2c
an

rT ≤ exp

(cid:19)

(cid:18)

−

nT
b/a + n

r0 +

c
b2an

.

Note that by assumption we have 1

b ≤ 7
(cid:18)

T an, hence
(cid:19)

nT
b/a + n

rT ≤ exp

−

r0 +

49c
T 2a3n3 .

(D.18)

If T > 7b

an, then we have for the ﬁrst phase when k ≤ k0 with stepsize γk = 1

b that

rk0 ≤ exp

(cid:19)

(cid:18)

−

nk0
b/a + n

r0 +

c
b2an

(cid:18)

≤ exp

−

(cid:19)

nT
2(b/a + n)

r0 +

c
b2an

.

(D.19)

Then for k > k0 we have

(1 + γkan) rk+1 ≤ rT + γ3

kc = rT +

73c
a3n3 (s + k − k0)3 .

Multiplying both sides by (s + k − k0)3 yields

(s + k − k0)3 (1 + γkan) rk+1 ≤ (s + k − k0)3 rT +

73c
a3n3 .

(D.20)

Note that because k and k0 are integers and k > k0, we have that k − k0 ≥ 1 and
therefore s + k − k0 ≥ 1. We may use this to lower bound the multiplicative factor in the
left hand side of (D.20) as

(s + k − k0)3 (1 + γkan) = (s + k − k0)3

(cid:19)

(cid:18)

1 +

7
s + k − k0
= (s + k − k0)3 + 7 (s + k − k0)2
= (s + k − k0)3 + 3 (s + k − k0)2 + 3 (s + k − k0)2

+ (s + k − k0)2

≥ (s + k − k0)3 + 3 (s + k − k0)2 + 3 (s + k − k0) + 1
= (s + k + 1 − k0)3 .

(D.21)

Using (D.21) in (D.20) we obtain

(s + k + 1 − k0)3 rk+1 ≤ (s + k − k0)3 rk +

73c
a3n3 .

Let wk = (s + k − k0)3. Then we can rewrite the last inequality as

232

wk+1rk+1 − wkrk ≤

73c
a3n3 .

Summing up and telescoping from k = k0 to T yields

wT rT ≤ wk0rk0 +

73c
a3n3 (T − k0) .

Note that wk0 = s3 and wT = (s + T − k0)3. Hence,

rT ≤

≤

s3

(s + T − k0)3 rk0 +
(s + T − k0)3 rk0 +

s3

T − k0
s + T − k0

73c
a3n3 (s + T − k0)2
73c
a3n3 (s + T − k0)2 .

Since we have s + T − k0 ≥ T − k0 ≥ T /2, it holds

rT ≤

8s3
T 3 rk0 +

4 · 73c
a3n3T 2 .

(D.22)

The bound in (D.19) can be rewritten as

s3
T 3 rk0 ≤

s3
T 3 exp

(cid:18)

−

nT
2 (b/a + n)

(cid:19)

r0 +

s3c
b2anT 3 .

We now rewrite the last inequality, use that T > 2s and further use the fact that s = 7b
2an:

s3
T 3 rk0 ≤

≤

=

(cid:17)3

(cid:16) s
T
(cid:124) (cid:123)(cid:122) (cid:125)
≤1/8
1
8
1
8

exp

exp

(cid:18)

exp

−

nT
2 (b/a + n)

(cid:19)

r0 +

s2c
b2anT 2

(cid:17)

(cid:16) s
T
(cid:124) (cid:123)(cid:122) (cid:125)
≤1/2

(cid:18)

−

(cid:18)

−

nT
2 (b/a + n)
nT
2 (b/a + n)

(cid:19)

(cid:19)

r0 +

r0 +

s2c
2b2anT 2
72c
8a3n3T 2 .

Plugging in the estimate of (D.23) into (D.22) we obtain

rT ≤ exp

−

(cid:18)

(cid:18)

= exp

−

nT
2 (b/a + n)
nT
2 (b/a + n)

(cid:19)

(cid:19)

r0 +

r0 +

72c
a3n3T 2 +
1421c
a3n3T 2 .

4 · 73c
a3n3T 2

(D.23)

(D.24)

Taking the maximum of (D.18) and (D.24) we see that for any T > 0 we have

233

rT ≤ exp

(cid:18)

−

nT
2 (b/a + n)

(cid:19)

r0 +

1421c
a3n3T 2 .

(cid:4)

D.4.1 Proof of Theorem 4.7.1

Proof. Start with Lemma D.2.1 with λ = 0, L = Lmax, and γ = γk,

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

− 2γ (1 − γLmax) E (cid:2)Dfπi

(xk

i , x(cid:63))(cid:3) + 2γ3

kσ2

rad.

Since γ ≤ 1

Lmax

and Dfπ (xk

i , x(cid:63)) is nonnegative we may simplify this to

E

(cid:104)(cid:13)
(cid:13)xk

i+1 − x(cid:63)

i+1

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

i − x(cid:63)
i

2(cid:105)

(cid:13)
(cid:13)

+ 2γ3

kσ2

rad.

Unrolling this recursion for n steps we get

E

(cid:104)(cid:13)
(cid:13)xk

n − x(cid:63)
n

2(cid:105)

(cid:13)
(cid:13)

≤ E

(cid:104)(cid:13)
(cid:13)xk

0 − x(cid:63)
0

2(cid:105)

(cid:13)
(cid:13)

+ 2nγ3

kσ2

rad.

By Lemma 1.10.6 and a similar reasoning to Theorem 4.6.1 we have

(1 + 2γkµn) E

(cid:104)(cid:13)
(cid:13)xk+1 − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ E

(cid:104)(cid:13)
(cid:13)xk − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

We may then use Lemma D.4.1 to obtain that

+ 2γ3

kσ2

rad.

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

(cid:18)

≤ exp

−

(cid:18)

= O

exp

(cid:19)

(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)

nT
2(Lmax/µ + n)
(cid:18)
nT
(cid:13)x0 − x(cid:63)(cid:13)
(cid:13)
2 +
(cid:13)
κ + 2n

−

(cid:19)

σ2
rad
µ3n2T 2

356σ2
rad
µ3n2T 2
(cid:19)

.

(cid:4)

D.5 Proof of Theorem 4.7.2 for Importance Resampling

Proof. We show that N ≤ 2n as the rest of the theorem’s claim trivially follows from
Theorem 4.6.1. Firstly, note that for any number a ∈ R we have (cid:100)a(cid:101) ≤ a + 1. Therefore,

N =

n
(cid:88)

i=1

(cid:25)

(cid:24) Li
L

≤

n
(cid:88)

i=1

(cid:18)Li
L

(cid:19)

+ 1

= n +

n
(cid:88)

i=1

Li
L

= 2n.

(cid:4)

D.6 Proofs for Federated Learning

234

D.6.1 Lemma for the extended proximal operator

Lemma D.6.1. Let ψC be the consensus constraint and R be a closed convex proximable
function. Suppose that x1, x2, . . . , xM are all in Rd. Then,

proxγ(R+ψC )(x1, . . . , xM ) = prox γ

M R(x),

where x = 1
M

(cid:80)M

m=1 xm.

Proof. We have,

proxγ(R+ψC )(x1, . . . , xM ) =








prox γ
M R(x)
...
M R(x)
prox γ


 with x =

1
M

M
(cid:88)

m=1

xm.

This is a simple consequence of the deﬁnition of the proximal operator. Indeed, the result
of proxγ(R+ψC ) must have blocks equal to some vector z such that

(cid:40)

z = arg min

x

γR(x) +

(cid:40)

= arg min

x

γR(x) +

(cid:26)

= arg min

x

γR(x) +

1
2

1
2

1
2

M
(cid:88)

(cid:107)x − xm(cid:107)2

(cid:41)

m=1
M
(cid:88)

(cid:0)(cid:107)x − x(cid:107)2 + 2 (cid:104)x − x, x − xm(cid:105)) + (cid:107)x − xm(cid:107)2(cid:1)

m=1

M (cid:107)x − x(cid:107)2

(cid:27)

= prox γ

M R(x).

(cid:41)

(cid:4)

D.6.2 Proof of Lemma 4.8.1

Proof. Given some vectors x, y ∈ Rd·M ,
(x(cid:62)

M )(cid:62), y = (y(cid:62)

1 , . . . , x(cid:62)

1 , . . . , y(cid:62)

let us use their block representation x =

M )(cid:62). Since we use the Euclidean norm, we have

(cid:107)∇fi(x)−∇fi(y)(cid:107)2 =

M
(cid:88)

m=1

(cid:107)∇fmi(xm)−∇fmi(ym)(cid:107)2 ≤

M
(cid:88)

m=1

L2

i (cid:107)xm−ym(cid:107)2 = L2

i (cid:107)x−y(cid:107)2.

We can obtain a lower bound by doing the same derivation and applying strong convexity
instead of smoothness:

M
(cid:88)

m=1

(cid:107)∇fmi(xm) − ∇fmi(ym)(cid:107)2 ≥ µ2

M
(cid:88)

m=1

(cid:107)xm − ym(cid:107)2 = µ2(cid:107)x − y(cid:107)2.

Thus, we have µ(cid:107)x − y(cid:107) ≤ (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ Li(cid:107)x − y(cid:107), which is exactly µ-strong
(cid:4)
convexity and Li-smoothness of fi.

D.6.3 Proof of Lemma 4.8.2

Proof. By Theorem 4.4.3 we have

235

σ2
rad ≤

(cid:16)

Lmax
2

n2(cid:107)∇f (x(cid:63))(cid:107)2 +

(cid:17)

.

σ2
(cid:63)

n
2

Due to the separable structure of f , we have for the variance term

nσ2
(cid:63)

def=

n
(cid:88)

i=1

(cid:107)∇fi(x(cid:63)) − ∇f (x(cid:63))(cid:107)2 =

n
(cid:88)

M
(cid:88)

i=1

m=1

(cid:13)
(cid:13)
∇fmi(x(cid:63)) −
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
∇Fm(x(cid:63))
(cid:13)
(cid:13)

1
n

.

The expression inside the summation is not exactly the variance due to the diﬀerent
normalization: 1
. Nevertheless, we can expand the norm and try to get
the actual variance:

n instead of

1
Nm

(cid:13)
2
(cid:13)
∇Fm(x(cid:63))
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)
∇fmi(x(cid:63)) −
(cid:13)
(cid:13)
Nm(cid:88)

=

(cid:18)(cid:13)
(cid:13)
∇fmi(x(cid:63)) −
(cid:13)
(cid:13)
Nm(cid:88)

(cid:10)∇fmi(x(cid:63)) −

i=1

+ 2

i=1

= Nmσ2

m,(cid:63) + Nm

(cid:16) 1
Nm

1
Nm

(cid:13)
2
(cid:13)
∇Fm(x(cid:63))
(cid:13)
(cid:13)

+

(cid:16) 1
Nm

−

(cid:17)2

1
n

(cid:19)

(cid:107)∇Fm(x(cid:63))(cid:107)2

∇Fm(x(cid:63)),

(cid:16) 1
Nm

−

(cid:17)

1
n

∇Fm(x(cid:63))(cid:11)

(cid:17)2

(cid:107)∇Fm(x(cid:63))(cid:107)2

1
Nm
1
n

−

≤ nσ2

m,(cid:63) + (cid:107)∇Fm(x(cid:63))(cid:107)2.

Moreover, the gradient term has the same block structure, so

n2(cid:107)∇f (x(cid:63))(cid:107)2 = n2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
∇fi(x(cid:63))
(cid:13)
(cid:13)

=

M
(cid:88)

m=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

∇fmi(x(cid:63))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

M
(cid:88)

m=1

(cid:107)∇Fm(x(cid:63))(cid:107)2.

Plugging the last two bounds back inside the upper bound on σ2
statement.

rad, we deduce the lemma’s
(cid:4)

D.6.4 Proof of Theorem 4.8.3

Proof. Since we assume that N1 = · · · = NM = n, we have N
convexity constant of ψ = N
we obtain

n (R + ψC) is equal to N

M = n and the strong
M = µ. By applying Theorem 4.6.1

n · µ

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γ2σ2
µ

rad

.

Since xT = proxγN (R+ψC )(xT −1
), we have xT ∈ C, i.e., all of its blocks are equal to
each other and we have xT = ((xT )(cid:62), . . . , (xT )(cid:62))(cid:62). Since we use the Euclidean norm, it

n

also implies

236

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

= M (cid:107)xT − x(cid:63)(cid:107)2.

The same is true for x0, so we need to divide both sides of the upper bound on (cid:107)xT −x(cid:63)(cid:107)2
by M . Doing so together with applying Lemma 4.8.2 yields

E

(cid:104)(cid:13)
(cid:13)xT − x(cid:63)(cid:13)
(cid:13)

2(cid:105)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

≤ (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

= (1 + 2γµn)−T (cid:13)

(cid:13)x0 − x(cid:63)(cid:13)
2 +
(cid:13)

γ2σ2
rad
M µ
γ2Lmax
M µ

γ2Lmax
M µ

M
(cid:88)

(cid:16)

(cid:107)∇Fm(x(cid:63))(cid:107)2 +

m=1
M
(cid:88)

(cid:16)

m=1

(cid:107)∇Fm(x(cid:63))(cid:107)2 +

(cid:17)

σ2
m,(cid:63)

n
4

N
4M

(cid:17)

.

σ2
m,(cid:63)

(cid:4)

D.6.5 Proof of Theorem 4.8.4

Proof. According to Lemma 4.8.1, each fi is µ-strongly convex and Lmax-smooth, so we
obtain the result by trivially applying Theorem 4.5.1 and upper bounding σ2
rad the same
(cid:4)
way as in the proof of Theorem 4.8.3.

D.7 Federated Experiments and Experimental Details

We also compare the performance of FedRR and Local SGD on homogeneous (i.e., i.i.d.)
data. Since Local SGD requires smaller stepsizes to converge, it is signiﬁcantly slower
at initialization, as can be seen in Figure D.1. FedRR, however, does not need small
initial stepsize and very quickly converges to a noisy neighborhood of the solution. The
advantage is clear both from the perspective of the number of communication rounds and
data passes.

To illustrate the severe impact of the number of local steps in Local SGD we show
results with diﬀerent number of local steps. The blue line shows Local SGD that takes the
number of steps equivalent to full pass over the data by each node. The orange line takes
5 times fewer local steps. Clearly, the latter performs better in terms of communication
rounds and local steps, making it clear that Local SGD scales worse with the number of
local steps. This phenomenon is well-understood and has been in discussed by [116].

Implementation details. For each i, we have Li = 1

N and tune
λ1 to obtain a solution with less than 50% coordinates (exact values are provided in the
code). We use stepsizes decreasing as O( 1
k ) for all methods. We use the ‘a1a’ dataset
for the experiment with (cid:96)1 regularization.

4(cid:107)ai(cid:107). We set λ2 = L

The experiment for the comparison of FedRR and Local SGD uses no (cid:96)1 regularization
and λ2 = L
N . We choose the stepsizes according to the theory of Local SGD and FedRR.
As per Theorem 3 in [116], the stepsizes for Local SGD must satisfy γk = O(1/(LH)),
where H is the number of local steps. The parallelization of local runs is done using the

237

Figure D.1: Experimental results for parallel training. Left: comparison in terms of
communication rounds, right: in terms of data passes

Ray package1. We use the ‘mushrooms’ dataset for this experiment.

Proximal operator calculation. It is well-known (see, for instance, [198]) that the

proximal operator for ψ(x) = λ1(cid:107)x(cid:107)1 + λ2

2 (cid:107)x(cid:107)2 is given by

proxγψ(x) =

1
1 + γλ2

proxγλ1(cid:107)·(cid:107)1(x),

where the j-th coordinate of proxγλ1(cid:107)·(cid:107)1(x) is

[proxγλ1(cid:107)·(cid:107)1(x)]j =

(cid:40)

sign([x]j)(|[x]j| − γλ1),
0,

if |[x]j| ≥ γλ1,
otherwise.

1https://ray.io/

020406080100120140Communication rounds10-610-510-410-310-210-1100f(x)¡f¤Local-SGD, full passLocal-SGD, partial passFedRR020406080100120140Data passes10-610-510-410-310-210-1100f(x)¡f¤Local-SGD, full passLocal-SGD, partial passFedRR238

Appendix E

Appendix for Chapter 5

E.1 Missing Proofs

Recall that in the proof of Theorem 5.3.2 we only showed boundedness of the iterates
It remains to show that sequence (xk)k converges
and complexity for minimizing f (x).
to a solution. For this, we need some variation of the Opial lemma.

Lemma E.1.1. Let (xk)k and (ak)k be two sequences in Rd and R+ respectively. Suppose
that (xk)k is bounded, its cluster points belong to X ⊂ Rd and it also holds that

(cid:107)xk+1 − x(cid:107)2 + ak+1 ≤ (cid:107)xk − x(cid:107)2 + ak,

∀x ∈ X .

(E.1)

Then (xk)k converges to some element in X .

Proof. Let ¯x1, ¯x2 be any cluster points of (xk)k. Thus, there exist two subsequences
(xki)i and (xkj )j such that xki → ¯x1 and xkj → ¯x2. Since (cid:107)xk − x(cid:107)2 + ak is nonnegative
and bounded, limk→∞((cid:107)xk − x(cid:107)2 + ak) exists for any x ∈ X . Let x = ¯x1. This yields

lim
k→∞

((cid:107)xk − ¯x1(cid:107)2 + ak) = lim
i→∞
= lim
j→∞

((cid:107)xki − ¯x1(cid:107)2 + aki) = lim
i→∞
((cid:107)xkj − ¯x1(cid:107)2 + akj ) = (cid:107)¯x2 − ¯x1(cid:107)2 + lim
j→∞

aki

akj .

Hence, limi→∞ aki = limj→∞ akj + (cid:107)¯x1 − ¯x2(cid:107)2. Doing the same with x = ¯x2 instead of
x = ¯x1, yields limj→∞ akj = limi→∞ aki + (cid:107)¯x1 − ¯x2(cid:107)2. Thus, we obtain that ¯x1 = ¯x2,
(cid:4)
which ﬁnishes the proof.

Another statement that we need here is the following tightening of the convexity

property.

Lemma E.1.2 (Theorem 2.1.5, [186]). Let C be a closed convex set in Rd. If f : C → R
is convex and L-smooth, then ∀x, y ∈ C it holds

f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) ≥

1
2L

(cid:107)∇f (x) − ∇f (y)(cid:107)2.

(E.2)

Proof of Theorem 5.3.2. (Convergence of (xk)k)

Note that in the ﬁrst part we have already proved that (xk)k is bounded and that ∇f

is L-Lipschitz on C = conv{x(cid:63), x0, x1, . . . }. Invoking Lemma E.1.2, we deduce that

γk(f (x(cid:63)) − f (xk)) ≥ γk(cid:104)∇f (xk), x(cid:63) − xk(cid:105) +

γk
2L

(cid:107)∇f (xk)(cid:107)2.

(E.3)

239

This indicates that instead of using inequality (5.6) in the proof of Lemma 5.3.1, we could
use a better estimate (E.3). However, we want to emphasize that we did not assume that
∇f is globally Lipschitz, but rather obtained Lipschitzness on C as an artifact of our
analysis. Clearly, in the end this improvement gives us an additional term γk
L (cid:107)∇f (xk)(cid:107)2
in the left-hand side of (5.5), that is

(cid:107)xk+1 − x(cid:63)(cid:107)2 +

1
2

(cid:107)xk+1 − xk(cid:107)2 + 2γk(1 + θk)(f (xk) − f (cid:63)) +
1
2

≤ (cid:107)xk − x(cid:63)(cid:107)2 +

(cid:107)xk − xk−1(cid:107)2 + 2γkθk(f (xk−1) − f (cid:63)).

(E.4)

γk
L

(cid:107)∇f (xk)(cid:107)2

Thus, telescoping (E.4), one obtains that (cid:80)k
2L , one has
that ∇f (xk) → 0. Now we might conclude that all cluster points of (xk)k are solutions
of (5.1).

γk
L (cid:107)∇f (xk)(cid:107)2 ≤ D. As γk ≥ 1

i=1

Let X be the solution set of (5.1) and ak = 1

2(cid:107)xk −xk−1(cid:107)2 +2γkθk(f (xk−1)−f (cid:63)). We
want to ﬁnish the proof applying Lemma E.1.1. To this end, notice that inequality (5.5)
(cid:4)
yields (E.1), since γk+1θk+1 ≤ (1 + θk)γk. This completes the proof.

Proof of Theorem 5.3.3.
First of all, we note that using the stricter inequality γk ≤
2 γk−1 does not change
the statement of Theorem 5.3.2. Hence, xk → x(cid:63) and there exist µ, L > 0 such that f is
µ-strongly convex and ∇f is L-Lipschitz on C = conv{x(cid:63), x0, x1, . . . }. Secondly, due to
local strong convexity, (cid:107)∇f (xk) − ∇f (xk−1)(cid:107) ≥ µ(cid:107)xk − xk−1(cid:107), and hence γk ≤ 1
2µ for
k ≥ 1.

1 + θk−1

(cid:113)

Now we tighten some steps in the analysis to improve bound (5.6). By strong convexity,

γk(cid:104)∇f (xk), x(cid:63) − xk(cid:105) ≤ γk(f (x(cid:63)) − f (xk)) − γk

µ
2

(cid:107)x(cid:63) − xk(cid:107)2.

By L-smoothness and bound γk ≤ 1
2µ,

γk(cid:104)∇f (xk), x(cid:63) − xk(cid:105) ≤ γk(f (x(cid:63)) − f (xk)) − γk

(cid:107)∇f (xk)(cid:107)2

1
2L
(cid:107)xk+1 − xk(cid:107)2

= γk(f (cid:63) − f (xk)) −

≤ γk(f (cid:63) − f (xk)) −

1
2Lγk
µ
L

(cid:107)xk+1 − xk(cid:107)2.

Together, these two bounds give us

γk(cid:104)∇f (xk), x(cid:63) − xk(cid:105) ≤ γk(f (cid:63) − f (xk)) − γk

µ
4

(cid:107)xk − x(cid:63)(cid:107)2 −

µ
2L

(cid:107)xk+1 − xk(cid:107)2.

We keep inequality (5.11) and the rest of the proof as is. Then the strengthen analog

240

Algorithm 14 Adaptive Gradient Descent (general update)
1: Input: x0 ∈ Rd, γ0 > 0, θ0 = +∞, α ∈ (0, 1), β = 1
2: x1 = x0 − γ0∇f (x0)
3: for k = 1, 2, . . . , K − 1 do

α(cid:107)xk−xk−1(cid:107)
(cid:107)∇f (xk)−∇f (xk−1)(cid:107)

(cid:111)

(cid:110)(cid:113) 1

β + θk−1γk−1,

γk = min
xk+1 = xk − γk∇f (xk)
θk = γk
γk−1

4:

5:
6:
7: end for

2(1−α) , number of steps K

of (5.5) will be

(cid:107)xk+1 − x(cid:63)(cid:107)2 +

(cid:18)

1
2

1 +

(cid:16)

(cid:16)

1 −

1 −

≤

≤

(cid:17)

(cid:17)

γkµ
2
γkµ
2

(cid:107)xk − x(cid:63)(cid:107)2 +

(cid:107)xk − x(cid:63)(cid:107)2 +

(cid:19)

2µ
L
1
2
1
2

(cid:107)xk+1 − xk(cid:107)2 + 2γk(1 + θk)(f (xk) − f (cid:63))

(cid:107)xk − xk−1(cid:107)2 + 2γkθk(f (xk−1) − f (cid:63))

(cid:107)xk − xk−1(cid:107)2 + 2γk−1

(cid:18)

1 +

(cid:19)

θk−1
2

(f (xk−1) − f (cid:63)),

(E.5)

where in the last inequality we used our new condition on γk. Under the new update we
have contraction in every term: 1 − γkµ
L+2µ in the second and
in the ﬁrst,
2
1+θk−1/2
2(1+θk−1) in the last one.
1+θk−1

1+2µ/L = 1 − 2µ

= 1 − θk−1

1

To further bound the last contraction, recall that γk ∈
κ for any k > 1, where κ def= L

θk = γk
γk−1
increases with θ > 0, this implies
Ψk+1 (the left-hand side of (E.5)) we have

2(1+θk−1) ≥ 1

≥ 1

θk−1

µ . Since the function θ (cid:55)→ θ

1+θ monotonically
2(κ+1) when k > 2. Thus, for the full energy

(cid:105)

(cid:104) 1
2L , 1

2µ

for k ≥ 1. Therefore,

(cid:18)

Ψk+1 ≤

1 − min

(cid:26) γkµ
2

,

1
2(κ + 1)

,

2µ
L + 2µ

(cid:27)(cid:19)

Ψk.

Using simple bounds γkµ
(1 − 1

L+2µ = 2
2 ≥ 1
4κ ,
4κ)Ψk for k > 2. This gives O (cid:0)κ log 1

2µ

ε

κ+2 ≥ 1

4κ, and

1

2(κ+1) ≥ 1

(cid:1) convergence rate.

4κ , we obtain Ψk+1 ≤
(cid:4)

E.2 Extensions

E.2.1 More general update

One may wonder how ﬂexible the update for γk in Algorithm 7 is? For example, is it
1 + θkγk−1 and put 2 in the denominator
necessary to upper bound the stepsize with
of
2(cid:107)∇f (xk)−∇f (xk−1)(cid:107)? Algorithm 14 that we present here partially answers this question.
Obviously, Algorithm 7 is a particular case of Algorithm 14 with α = 1

(cid:107)xk−xk−1(cid:107)

√

2 and β = 1.

Theorem E.2.1. Suppose that f : Rd → R is convex with locally Lipschitz gradient ∇f .
Then (xk)k generated by Algorithm 14 converges to a solution of (5.1) and we have that

241

where

f (ˆxK) − f (cid:63) ≤

D
2SK

= O

(cid:17)

,

(cid:16) 1
K

ˆxK =

γK(1 + θKβ)xK + (cid:80)K−1

i=1 wixi

SK

,

wi = γi(1 + θiβ) − γi+1θi+1β,

SK = γK(1 + θKβ) +

K−1
(cid:88)

i=1

wi =

K
(cid:88)

i=1

γi + γ1θ1β,

and D is a constant that explicitly depends on the initial data and the solution set.

Proof. Let x(cid:63) be arbitrary solution of (5.1). We note that equations (5.7) and (5.8) hold
for any variant of GD, independently of γk, α, β. With the new rule for γk, from (5.8) it
follows

(cid:107)xk+1 − xk(cid:107)2 ≤ 2γkθk(f (xk−1) − f (xk)) − (cid:107)xk+1 − xk(cid:107)2

+ 2γk(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107)

≤ 2γkθk(f (xk−1) − f (xk)) − (1 − α)(cid:107)xk+1 − xk(cid:107)2 + α(cid:107)xk − xk−1(cid:107)2,

which, after multiplication by β and reshuﬄing the terms, becomes

β(2 − α)(cid:107)xk+1 − xk(cid:107)2 + 2βγkθk(f (xk) − f (cid:63)) ≤ αβ(cid:107)xk − xk−1(cid:107)2 + 2βγkθk(f (xk−1) − f (cid:63)).

Adding (5.7) and the latter inequality gives us

(cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γk(1 + θkβ)(f (xk) − f (cid:63)) + (2β − αβ − 1)(cid:107)xk+1 − xk(cid:107)2
≤ (cid:107)xk − x(cid:63)(cid:107)2 + 2γkθkβ(f (xk−1) − f (cid:63)) + αβ(cid:107)xk+1 − xk(cid:107)2.

Notice that by β = 1

2(1−α), we have 2β − αβ − 1 = αβ and hence,

(cid:107)xk+1 − x(cid:63)(cid:107)2 + 2γk(1 + θkβ)(f (xk) − f (cid:63)) + αβ(cid:107)xk+1 − xk(cid:107)2
≤ (cid:107)xk − x(cid:63)(cid:107)2 + 2γkθkβ(f (xk−1) − f (cid:63)) + αβ(cid:107)xk+1 − xk(cid:107)2.

As a sanity check, we can see that with α = 1
with (5.5).

2 and β = 1, the above inequality coincides

Telescoping this inequality, we deduce

242

(cid:107)xk+1 − x(cid:63)(cid:107)2 + αβ(cid:107)xk+1 − xk(cid:107)2 + 2γk(1 + θkβ)(f (xk) − f (cid:63))

k−1
(cid:88)

[γi(1 + θiβ) − γi+1θi+1β](f (xi) − f (cid:63))

+ 2

i=1

≤ (cid:107)x1 − x(cid:63)(cid:107)2 + αβ(cid:107)x1 − x0(cid:107)2 + 2γ1θ1β[f (x0) − f (cid:63)] def= D.

(E.6)

Note that because of the way we deﬁned stepsize, γi(1 + θiβ) − γi+1θi+1β ≥ 0. Thus,
the sequence (xk)k is bounded. Since ∇f is locally Lipschitz, it is Lipschitz continu-
ous on bounded sets. Let L be a Lipschitz constant of ∇f on a bounded set C =
conv{x(cid:63), x1, x2, . . . }.
2, then 1

β > 1 and similarly to Theorem 5.3.2 we might conclude that γk ≥ α
L
Instead, we prove that
2 we cannot do this.

If α ≤ 1

for all k. However, for the case α > 1
γk ≥ 2α(1−α)

, which suﬃces for our purposes.
Let m, n ∈ N be the smallest numbers such that β− 1

L

2m ≥ 1− 1

want to prove that for any k it holds γk ≥ 2α(1−α)
elements γk, γk+1, . . . , γk+m+n at least one is no less than α
induction. First, note that the second bound always satisﬁes
k, which also implies that γ1 ≥ α
L and γk < α
assume that γk−1 ≥ α
that the second bound is not active for γk, . . . , γk+j−1, i.e., γk+i =
for i < j.

2β and (1+ 1
and among every m + n + 1 consecutive
L. We shall prove this by
(cid:107)∇f (xk)−∇f (xk−1)(cid:107) ≥ α
L for all
L . If for all k we have γk ≥ α
L, then we are done. Now
L for some k. Choose the largest j (possibly inﬁnite) such
β + θk+i−iγk+i−1

2 ≥ β. We

α(cid:107)xk−xk−1(cid:107)

(cid:113) 1

L

β ) n

Let us prove that γk, . . . , γk+j−1 ≥ 2α(1−α)
β + θk+i−1 for all i = 0, . . . , j − 1. Recall that β > 1, and thus,

L

(cid:113) 1

. The deﬁnition of j yields θk+i =

θk ≥ β− 1
2 ,

θk+1 ≥

(cid:115)

1
β

+

(cid:114) 1
β

≥ β− 1
4 ,

. . . ,

θk+i ≥

(cid:114) 1
β

for all i < j. Now it remains to notice that for any i < j

+ β− 1

2i ≥ β− 1

2i+1

γk+i
γk−1

= θkθk+1 . . . θk+i ≥ β− 1

2 − 1

4 −···− 1

2i+1 ≥

1
β

= 2(1 − α)

and hence γk+i ≥ 2(1 − α)γk−1 ≥ 2α(1−α)
L
the second bound is active, i.e., γk+j ≥ α
Otherwise, note

.

If j ≤ m + n, then at (k + j)-th iteration
L , and we are done with the other claim as well.

θk+m−1 ≥ β− 1

2m ≥ 1 −

1
2β

,

so θk+m =

(cid:113) 1

β + θk+m−1 ≥

(cid:113) 1

β + 1 − 1

2β =

(cid:113)

1 + 1

2β and for any i ∈ [m, j − 2] we have

243

L, θ0 = +∞, number of steps K

Algorithm 15 Adaptive Gradient Descent (L is known)
1: Input: x0 ∈ Rd, γ0 = 1
2: x1 = x0 − γ0∇f (x0)
3: for k = 1, 2, . . . , K − 1 do
Lk = (cid:107)∇f (xk)−∇f (xk−1)(cid:107)
4:
γk = min
xk+1 = xk − γk∇f (xk)
θk = γk
γk−1

(cid:110)(cid:112)1 + θk−1γk−1,

γk−1L2 + 1
2Lk

(cid:107)xk−xk−1(cid:107)

(cid:111)

5:

1

6:
7:
8: end for

θk+i+1 =

(cid:113) 1

β + θk+i ≥

(cid:113) 1

β + 1. Thus,

γk+m+n = γk−1

(cid:16)k+m−1
(cid:89)

θl

(cid:17)(cid:16)k+m+n
(cid:89)

(cid:17)

θl

≥ γk−1

l=k

l=k+m

(cid:114)

1
β

1 +

(cid:16)

1
2β

1 +

1
β

(cid:17) n

2 ≥ γk−1 ≥

α
L

,

so we have shown the second claim too.
To conclude, in both cases α ≤ 1
Applying the Jensen inequality for the sum of all terms f (xi) − f (cid:63) in the left-hand

2, we have Sk = Ω(k).

2 and α > 1

side of (E.6), we obtain

D
2

≥

LHS of (E.6)
2

≥ SK(f (ˆxK) − f (cid:63)),

where ˆxK is deﬁned in the statement of the theorem. Finally, convergence of (xk)k can
(cid:4)
be proved in a similar way as Theorem 5.3.2.

E.2.2 f is L-smooth

Often, it is known that f is smooth and even some estimate for the Lipschitz constant
L of ∇f is available. In this case, we can use slightly larger steps, since instead of just
convexity the stronger inequality in Lemma E.1.2 holds. To take advantage of it, we
present a modiﬁed version of Algorithm 7 in Algorithm 15. Note that we have chosen to
modify Algorithm 7 and not its more general variant Algorithm 14 only for simplicity.

Theorem E.2.2. Let f be convex and L-smooth. Then for (xk)k generated by Algo-
rithm 15 inequality (5.5) holds. As a corollary, it holds for some ergodic vector ˆxK that
f (ˆxK) − f (cid:63) = O (cid:0) 1

(cid:1).

K

Proof. Proceeding similarly as in Lemma 5.3.1, we have

(cid:107)xk+1 − x(cid:63)(cid:107)2 = (cid:107)xk − x(cid:63)(cid:107)2 − 2γk

(cid:10)∇f (xk), xk − x(cid:63)(cid:11) + (cid:107)xk+1 − xk(cid:107)2.

(E.7)

By convexity of f and Lemma E.1.2,

244

2γk(cid:104)∇f (xk), x(cid:63) − xk(cid:105)

(E.2)
≤ 2γk(f (x(cid:63)) − f (xk) −
1
γkL

= 2γk(f (cid:63) − f (xk)) −

(cid:107)∇f (xk)(cid:107)2)

1
2L
(cid:107)xk+1 − xk(cid:107)2.

(E.8)

As in (5.8), we have

(cid:107)xk+1−xk(cid:107)2 = 2γk(cid:104)∇f (xk)−f (xk−1), xk−xk+1(cid:105)+2γk(cid:104)f (xk−1), xk−xk+1(cid:105)−(cid:107)xk+1−xk(cid:107)2.
(E.9)
Again, instead of using merely convexity of f , we combine it with Lemma E.1.2. This
gives

2γk(cid:104)∇f (xk−1), xk − xk+1(cid:105) =

2γk
γk−1

(cid:104)xk−1 − xk, xk − xk+1(cid:105)

= 2γkθk(cid:104)xk−1 − xk, ∇f (xk)(cid:105)
(E.2)
≤ 2γkθk(f (xk−1) − f (xk)) −

γkθk
L

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)2.

(E.10)

L (cid:107)∇f (xk)−∇f (xk−1)(cid:107)2,
Since now we have two additional terms
we can do better than (5.10). But ﬁrst we need a simple, yet a bit tedious fact. By our
choice of γk, in every iteration γk ≤ 1
. We want
to show that it implies

with Lk = (cid:107)∇f (xk)−∇f (xk−1)(cid:107)

γkL(cid:107)xk+1−xk(cid:107)2 and γkθk

(cid:107)xk−xk−1(cid:107)

γk−1L2 + 1
2Lk
√

1

(cid:18)

(cid:19)

θk
L

≤

1
Lk

,

(E.11)

which is equivalent to γk −
γk−1L − 1
inequality t2 − t
2Lk

√

2

γk −

√
γk√
γk−1L − 1
2Lk
≤ 0 are

≤ 0. Nonnegative solutions of the quadratic

0 ≤ t ≤

1
γk−1L

√
2

+

1
2

(cid:115)

1
γk−1L2 +

2
Lk

=

1
γk−1L

√
2

(cid:32)

(cid:115)

1 +

1 +

(cid:33)

.

2γk−1L2
Lk

Let us prove that

(cid:113) 1

γk−1L2 + 1
2Lk

falls into this segment and, hence,

√

γk does as well.

Using a simple inequality 4 + a ≤ (1 +

1 + a)2, for a > 0, we obtain

√

1
γk−1L2 +

1
2Lk

=

1
4γk−1L2

(cid:0)4 +

2γk−1L2
Lk

(cid:1) ≤

1
4γk−1L2

(cid:32)

(cid:115)

1 +

1 +

(cid:33)2

.

2γk−1L2
Lk

This conﬁrms that (E.11) is true. Thus, by Cauchy-Schwarz and Young’s inequalities, one

has

245

2γk(cid:104)∇f (xk) − ∇f (xk−1), xk − xk+1(cid:105)
≤ 2γk(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107)

(cid:18)

= 2

√

θk
L

γk −
√
2

+

θk

L

(cid:19)

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107)

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107)

(E.11)
≤

1
Lk

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)(cid:107)xk − xk+1(cid:107) +

γkθk
L

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)2

+

1
γkL

(cid:107)xk+1 − xk(cid:107)2

= (cid:107)xk − xk−1(cid:107)(cid:107)xk − xk+1(cid:107) +

γkθk
L

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)2 +

1
γkL

(cid:107)xk+1 − xk(cid:107)2

≤

1
2

(cid:107)xk − xk−1(cid:107)2 +

1
2

(cid:107)xk+1 − xk(cid:107)2 +

γkθk
L

(cid:107)∇f (xk) − ∇f (xk−1)(cid:107)2 +

1
γkL

Combining everything together, we obtain the statement of the theorem.

(cid:107)xk+1 − xk(cid:107)2.

(E.12)
(cid:4)

E.3 Stochastic Analysis

E.3.1 Diﬀerent samples

Consider the following version of SGD, in which we have two samples at each iteration,
ξk and ζ k to compute

(cid:26)(cid:112)

γk = min

1 + θkγk−1,

α(cid:107)xk − xk−1(cid:107)
(cid:107)∇f (xk; ζ k) − ∇f (xk−1; ζ k)(cid:107)

(cid:27)

,

xk+1 = xk − γk∇f (xk; ξk).

As before, we assume that θ0 = +∞, so γ1 =

α(cid:107)x1−x0(cid:107)
(cid:107)∇f (x1;ζ1)−∇f (x0;ζ1)(cid:107).

Lemma E.3.1. Let f (·; ξ) be L-smooth µ-strongly convex almost surely. It holds for γk
produced by the rule above

α
L

≤ γk ≤

α
µ

a.s.

(E.13)

Proof. Let us start with the upper bound. Strong convexity of f (·; ζ k) implies that (cid:107)x −
1 + θkγk−1, α/µ(cid:9) ≤
y(cid:107) ≤ 1
α/µ a.s.

µ(cid:107)∇f (x; ζ k)−∇f (y; ζ k)(cid:107) for any x, y. Therefore, γk ≤ min (cid:8)√

On the other hand, L-smoothness gives

γk ≥ min

(cid:110)(cid:112)

1 + θkγk−1, α/L

(cid:111)

≥ min {γk−1, α/L}

246

almost surely. Iterating this inequality, we obtain the stated lower bound.

(cid:4)

Proposition E.3.2. Denote σ2 def= E [(cid:107)∇f (x(cid:63); ξ)(cid:107)2] and assume f to be almost surely
L-smooth and convex. Then it holds for any x

E (cid:2)(cid:107)∇f (x; ξ)(cid:107)2(cid:3) ≤ 4L(f (x) − f (cid:63)) + 2σ2.

(E.14)

Another fact that we will use is a strong convexity bound, which states for any x, y

(cid:104)∇f (x), x − y(cid:105) ≥

µ
2

(cid:107)x − y(cid:107)2 + f (x) − f (y).

(E.15)

Theorem E.3.3. Let f (·; ξ) be L-smooth and µ-strongly convex almost surely.
choose some α ≤ µ

2L , then

If we

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) ≤ exp

(cid:16)

−k

(cid:17)

µα
L

C0 + α

σ2
µ2 ,

def= 2(1 + 2γ2

0L2)(cid:107)x0 − x(cid:63)(cid:107)2 + 4γ2

where C0
Proof. Under our assumptions on α ≤ µ
2L , we have γk ≤ α
of ξk, we have E (cid:2)γk∇f (xk; ξk)(cid:3) = E [γk] E (cid:2)∇f (xk)(cid:3) and

0σ2 and σ2 def= E [(cid:107)∇f i(x(cid:63); ξ)(cid:107)2].
µ ≤ 1

2L. Since γk is independent

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3)
= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2E (cid:2)γk
≤ E (cid:2)(1 − γkµ)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2E (cid:2)γk(f (xk) − f (cid:63))(cid:3) + E (cid:2)γ2
(cid:104)
≤ E (cid:2)(1 − γkµ)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2E

(cid:10)∇f (xk), xk − x(cid:63)(cid:11)(cid:3) + E (cid:2)γ2

(E.14)

(E.15)

k

k

(cid:105)
(f (xk) − f (cid:63))

(cid:3) E (cid:2)(cid:107)∇f (xk; ξk)(cid:107)2(cid:3)

(cid:3) E (cid:2)(cid:107)∇f (xk; ξk)(cid:107)2(cid:3)

+ E (cid:2)γ2

k

(cid:3) σ2

(E.13)

≤ E [1 − γkµ] E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + α

γk (1 − 2γkL)
(cid:125)

(cid:123)(cid:122)
(cid:124)
≥0
E [γk] σ2
µ

.

Therefore, if we subtract α σ2

µ2 from both sides, we obtain

(cid:20)

E

(cid:107)xk+1 − x(cid:63)(cid:107)2 − α

(cid:21)

σ2
µ2

≤ E [1 − γkµ] E

(cid:20)

(cid:107)xk − x(cid:63)(cid:107)2 − α

(cid:21)

.

σ2
µ2

If E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) ≤ α σ2
Otherwise, we can reuse the produced bound to obtain

µ2 for some k, it follows that E [(cid:107)xt − x(cid:63)(cid:107)2] ≤ α σ2

µ2 for any t ≥ k.

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤

k
(cid:89)

t=1

E [1 − γtµ] (cid:107)x1 − x(cid:63)(cid:107)2 + α

σ2
µ2 .

By inequality 1 − x ≤ e−x, we have (cid:81)k

t=1

E [1 − γtµ] ≤ exp

(cid:16)

−µ (cid:80)k

t=0 γt

(cid:17)

. In addition,

recall that in accordance with (E.13) we have γk ≥ α

L. Thus,

247

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ exp

(cid:16)

−kα

(cid:17)

µ
L

E (cid:2)(cid:107)x1 − x(cid:63)(cid:107)2(cid:3) + α

σ2
µ2 .

It remains to mention that

E (cid:2)(cid:107)x1 − x(cid:63)(cid:107)2(cid:3) ≤ 2(cid:107)x0 − x(cid:63)(cid:107)2 + 2γ2

0

E (cid:2)(cid:107)∇f (x0; ξ0)(cid:107)2(cid:3)

(E.14)
≤ 2(cid:107)x0 − x(cid:63)(cid:107)2 + 2γ2
0

(cid:0)2L2(cid:107)x0 − x(cid:63)(cid:107)2 + 2σ2(cid:1) .

This gives the following corollary.

(cid:4)

2L with η ≤ 1. Then, to achieve E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) =
Corollary E.3.4. Choose α = η µ
(cid:16) L2
ηµ2 log 1+γ2
O(ε + ησ2) we need only k = O
iterations. If we choose η proportionally
(cid:1) complexity.
to ε, it implies O (cid:0) 1

(cid:17)

ε

0

ε log 1

ε

E.3.2 Same sample: overparameterized models

Assume additionally that the model is overparameterized, i.e., ∇f (x(cid:63); ξ) = 0 with prob-
In that case, we can prove that one can use the same stochastic sample to
ability one.
compute the stepsize and to move the iterate. The update becomes

(cid:26)(cid:112)

γk = min

1 + θkγk−1,

α(cid:107)xk − xk−1(cid:107)
(cid:107)∇f (xk; ξk) − ∇f (xk−1; ξk)(cid:107)

(cid:27)

,

xk+1 = xk − γk∇f (xk; ξk).

Theorem E.3.5. Let f (·; ξ) be L-smooth, µ-strongly convex and satisfy ∇f (x(cid:63); ξ) = 0
with probability one. If we choose α ≤ µ
L , then

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ exp

(cid:16)

−Kα

(cid:17)

µ
L

C0,

where C0

def= 2(1 + γ2

0L2)(cid:107)x0 − x(cid:63)(cid:107)2.

Proof. Now γk depends on ξk, so we do not have an unbiased update anymore. However,
under the new assumption, ∇f (x(cid:63); ξk) = 0, so we can write

(cid:10)∇f (xk; ξk), xk − x(cid:63)(cid:11) (E.15)
≥

µ
2

(cid:107)xk − x(cid:63)(cid:107)2 + f (xk; ξk) − f (x(cid:63); ξk).

In addition, L-smoothness and convexity of f (·; ξk) give

(cid:107)∇f (xk; ξk)(cid:107)2 ≤ 2L(f (xk; ξk) − f (x(cid:63); ξk)).

Since our choice of α implies γk ≤ 1

L, we conclude that

248

(cid:107)xk+1 − x(cid:63)(cid:107)2 = (cid:107)xk − x(cid:63)(cid:107)2 − 2γk

(cid:10)∇f (xk; ξk), xk − x(cid:63)(cid:11) + γ2

k(cid:107)∇f (xk; ξk)(cid:107)2
≤ (1 − γkµ)(cid:107)xk − x(cid:63)(cid:107)2 − 2γk(1 − γkL)(f (xk; ξk) − f (x(cid:63); ξk))
≤ (1 − γkµ)(cid:107)xk − x(cid:63)(cid:107)2.

Furthermore, as (cid:107)∇f (x0; ξ0)(cid:107) = (cid:107)∇f (x0; ξ0) − ∇f (x(cid:63); ξ0)(cid:107) ≤ L(cid:107)x0 − x(cid:63)(cid:107), we also get
a better bound on E [(cid:107)x1 − x(cid:63)(cid:107)2], namely

E (cid:2)(cid:107)x1 − x(cid:63)(cid:107)2(cid:3) ≤ 2(cid:107)x0 − x(cid:63)(cid:107) + 2γ2

0

E (cid:2)(cid:107)∇f (x0; ξ0)(cid:107)2(cid:3) ≤ 2(1 + γ2

0L2)(cid:107)x0 − x(cid:63)(cid:107).

(cid:4)

E.4 Experiments Details

Here we provide some omitted details of the experiments with neural networks. We took
the implementation of neural networks from a publicly available repository1. All methods
were run with standard data augmentation and no weight decay. The conﬁdence intervals
for ResNet-18 are obtained from 5 diﬀerent random seeds and for DenseNet-121 from 3
seeds.

In our ResNet-18 experiments, we used the default parameters for Adam. SGD was
used with a stepsize divided by 10 at epochs 120 and 160 when the loss plateaus. Log grid
search with a factor of 2 was used to tune the initial stepsize of SGD and the best initial
value was 0.2. Tuning was done by running SGD 3 times and comparing the average of
test accuracies over the runs at epoch 200. For the momentum version (SGDm) we used
the standard values of momentum and initial stepsize for training residual networks, 0.9
and 0.1 correspondingly. We used the same parameters for DenseNet-121 without extra
tuning.

For our method we used the variant of SGD

xk+1 = xk − γk∇f (xk; ξk),

with γk computed using ξk as well (biased option). We did not test stepsizes that use
values other than 1
, so it is possible that other options will perform better.
Lk
Moreover, the coeﬃcient before θk−1 might be suboptimal too.

and 1
2Lk

1https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py

249

Appendix F

Appendix for Chapter 6

F.1 Block p-Quantization

We now introduce a block version of p-quantization. We found these quantization oper-
ators to have better properties in practice.

Deﬁnition F.1.1 (block-p-quantization). Let ∆ = (∆(1), ∆(2), . . . , ∆(m)) ∈ Rd, where
∆(1) ∈ Rd1, . . . , ∆(m) ∈ Rdm, d1 + . . . + dm = d and dl > 1 for all l = 1, . . . , m.
We say that ˆ∆ is p-quantization of ∆ with sizes of blocks (dl)m
l=1 and write ˆ∆ ∼
Quantp(∆, (dl)m

l=1) if ˆ∆(l) ∼ Quantp(∆) for all l = 1, . . . , m.

In other words, we quantize blocks called blocks of the initial vector. Note that in
the special case when m = 1 we get full quantization: Quantp(∆, (dl)m
l=1) = Quantp(∆).
Note that we do not assume independence of the quantization of blocks or independence
of ξ(j). Lemma F.1.2 in the appendix states that ˆ∆ is an unbiased estimator of ∆, and
gives a formula for its variance.

Next we show that the block p-quantization operator ˆ∆ introduced in Deﬁnition F.1.1

is an unbiased estimator of ∆, and give a formula for its variance.

Lemma F.1.2. Let ∆ ∈ Rd and ˆ∆ ∼ Quantp(∆). Then for l = 1, . . . , m

(cid:105)
(cid:104) ˆ∆(l)

E

= ∆(l),

(cid:104)

(cid:107) ˆ∆(l) − ∆(l)(cid:107)2(cid:105)

E

= Ψl(∆),

(cid:105)
(cid:104) ˆ∆

E

= ∆,

(cid:104)

(cid:107) ˆ∆ − ∆(cid:107)2(cid:105)

E

= Ψ(∆),

(F.1)

(F.2)

where x = (x(1), x(2), . . . , x(m)), Ψl(x) def= (cid:107)x(l)(cid:107)1(cid:107)x(l)(cid:107)p − (cid:107)x(l)(cid:107)2 ≥ 0, and Ψ(x) def=
m
(cid:80)
Ψl(x) ≥ 0. Thus, ˆ∆ is an unbiased estimator of ∆. Moreover, the variance of ˆ∆ is a
l=1
decreasing function of p, and is minimized for p = ∞.

Proof. Note that the ﬁrst part of (F.2) follows from the ﬁrst part of (F.1) and the second
part of (F.2) follows from the second part of (F.1) and

(cid:107) ˆ∆ − ∆(cid:107)2 =

m
(cid:88)

l=1

(cid:107) ˆ∆(l) − ∆(l)(cid:107)2.

Therefore, it is suﬃcient to prove (F.1).

If ∆(l) = 0, the statements follow trivially.

250

Assume ∆(l) (cid:54)= 0. In view of (6.5), we have
(cid:104) ˆ∆(j)(l)
(cid:105)

= (cid:107)∆(l)(cid:107)psign(∆(j)(l))E (cid:2)ξ(j)

E

(cid:3) = (cid:107)∆(l)(cid:107)psign(∆(j)(l))|∆(j)(l)|/(cid:107)∆(l)(cid:107)p = ∆(j)(l),

which establishes the ﬁrst claim. We can write

(cid:104)

(cid:107) ˆ∆(l) − ∆(l)(cid:107)2(cid:105)

E

= E

= E

( ˆ∆(j)(l) − ∆(j)(l))2

(cid:35)

( ˆ∆(j)(l) − E

(cid:104) ˆ∆(j)(l))2(cid:105)

(cid:35)

(cid:34)

(cid:88)

j

(cid:34)

(cid:88)

j

(6.5)
= (cid:107)∆(l)(cid:107)2
p

= (cid:107)∆(l)(cid:107)2
p

(cid:88)

j

(cid:88)

j

sign2(∆(j)(l))E (cid:2)(ξ(j) − E (cid:2)ξ(j)

(cid:3))2(cid:3)

sign2(∆(j)(l))

|∆(j)(l)|
(cid:107)∆(l)(cid:107)p

(1 −

|∆(j)(l)|
(cid:107)∆(l)(cid:107)p

)

=

(cid:88)

j

|∆(j)(l)|((cid:107)∆(l)(cid:107)p − |∆(j)(l)|)

= (cid:107)∆(l)(cid:107)1(cid:107)∆(l)(cid:107)p − (cid:107)∆(l)(cid:107).

(cid:4)

F.2 Proof of Theorem 6.4.2

Let 1[·] denote the indicator random variable of an event.
(cid:107)∆(cid:107)psign(∆(j))ξ(j), where ξ(j) ∼ Be(|∆(j)|/(cid:107)∆(cid:107)p). Therefore,

In view of (6.5), ˆ∆(j) =

(cid:107) ˆ∆(cid:107)0 =

1[ ˆ∆(j)(cid:54)=0] =

d
(cid:88)

j : ∆(j)(cid:54)=0

1[ξ(j)=1],

d
(cid:88)

j=1



which implies that

E

(cid:104)
(cid:107) ˆ∆(cid:107)0

(cid:105)

= E





d
(cid:88)

1[ξ(j)=1]

 =

d
(cid:88)

(cid:104)

E

1[ξ(j)=1]

(cid:105)

=

d
(cid:88)

|∆(j)|
(cid:107)∆(cid:107)p

=

(cid:107)∆(cid:107)1
(cid:107)∆(cid:107)p

.

j : ∆(j)(cid:54)=0

j : ∆(j)(cid:54)=0

j : ∆(j)(cid:54)=0

To establish the ﬁrst clam, it remains to recall that for all x ∈ Rd and 1 ≤ q ≤ p ≤ +∞,
one has the bound

(cid:107)x(cid:107)p ≤ (cid:107)x(cid:107)q ≤ (cid:107)x(cid:107)1/q−1/p

(cid:107)x(cid:107)p,

0

and apply it with q = 1.

The proof of the second claim follows the same pattern, but uses the concavity of
√

t (cid:55)→

t and Jensen’s inequality in one step.

F.3 Proof of Lemma 6.5.3

251

αp(d) is increasing as a function of p because (cid:107) · (cid:107)p is decreasing as a function of p.
Moreover, αp(d) is decreasing as a function of d since if we have d < b then

αp(b) = inf

x(cid:54)=0,x∈Rb

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

(cid:54) inf

x(cid:54)=0,x∈Rb
d

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

= inf

x(cid:54)=0,x∈Rd

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

,

where Rb
d
this bound is tight. Therefore,

def= {x ∈ Rb : x(d+1) = . . . = x(b) = 0}. It is known that (cid:107)x(cid:107)
(cid:107)x(cid:107)1

≥ 1√
d

, and that

and

α1(d) = inf

x(cid:54)=0,x∈Rd

(cid:107)x(cid:107)2
(cid:107)x(cid:107)2
1

=

1
d

α2(d) = inf

x(cid:54)=0,x∈Rd

(cid:107)x(cid:107)
(cid:107)x(cid:107)1

=

1
√
d

.

Let us now establish that α∞(d) = 2
√
1+

d

. Note that

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)∞

=

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
x
(cid:107)x(cid:107)∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x
(cid:107)x(cid:107)∞

x
(cid:107)x(cid:107)∞

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

=

(cid:13)
(cid:13)
(cid:13)∞

x
(cid:107)x(cid:107)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

x
(cid:107)x(cid:107)∞

Therefore, w.l.o.g. one can assume that (cid:107)x(cid:107)∞ = 1. Moreover, signs of coordinates of
vector x do not inﬂuence aforementioned quantity either, so one can consider only x ∈ Rd
+.
In addition, since (cid:107)x(cid:107)∞ = 1, one can assume that x(1) = 1. Thus, our goal now is to
show that the minimal value of the function

f (x) =

(2) + . . . + x2
1 + x2
(d)
1 + x(2) + . . . + x(d)

on the set M = {x ∈ Rd | x(1) = 1, 0 ≤ x(j) ≤ 1, j = 2, . . . , d} is equal to
Cauchy-Schwartz inequality: x2
(2) + . . . + x2
and it becomes equality if
and only if all x(j), j = 2, . . . , d are equal. It means that if we ﬁx x(j) = a for j = 2, . . . , d
and some 0 ≤ a ≤ 1 than the minimal value of the function

(d) ≥ (x(2)+...+x(d))2

2
√
1+

. By

d−1

d

g(a) =

1 + ((d−1)a)2
1 + (d − 1)a

d−1

=

1 + (d − 1)a2
1 + (d − 1)a

on [0, 1] coincides with minimal value of f on M . The derivative

g(cid:48)(a) =

2(d − 1)a
1 + (d − 1)a

−

(d − 1)(1 + (d − 1)a2)
(1 + (d − 1)a)2

has the same sign on [0, 1] as the diﬀerence a − 1+(d−1)a2

2(1+(d−1)a) , which implies that g attains

252

its minimal value on [0, 1] at such a that a = 1+(d−1)a2
which satisﬁes

2(1+(d−1)a).

It remains to ﬁnd a ∈ [0, 1]

a =

1 + (d − 1)a2
2(1 + (d − 1)a)

,

a ∈ [0, 1] ⇐⇒ (d − 1)a2 + 2a − 1 = 0,

a ∈ [0, 1].

This quadratic equation has unique positive solution a(cid:63) = −1+
. It implies that α(d) = 2
calculations show that g(a(cid:63)) = 2
√
√
1+
1+

d−1 = 1
.

1+

d

d

d

√

< 1. Direct

d

√

F.4 Strongly Convex Case: Optimal Number of Nodes

In practice one has access to a ﬁnite dataset, consisting of N data points, where N is
very large, and wishes to solve an empirical risk minimization (“ﬁnite-sum”) of the form

f (x) =

min
x∈Rd

1
N

N
(cid:88)

i=1

φi(x) + ψ(x),

(F.3)

is L–smooth and µ-strongly convex.

If M ≤ N compute nodes of a
where each φi
distributed system are available, one may partition the N functions into M groups,
G1, . . . , GM , each of size |Gi| = N/n, and deﬁne fm(x) = M
φi(x). Note that it
N
still holds that

i∈Gm

(cid:80)

f (x) =

1
M

M
(cid:88)

i=1

fi(x) + ψ(x).

Moreover, each fm is also L–smooth and µ–strongly convex.

This way, we have ﬁt the original (and large) problem (F.3) into our framework. One
may now ask the question: How many many nodes M should we use (other things
If what we care about is iteration complexity, then insights can be gained
equal)?
if p = 2, then the complexity is W (M ) def=
by investigating (6.16). For instance,
(cid:16) 1
2 − 1
. The optimal choice is to choose M so that the
max

√
d
√
m

M +

(cid:17)(cid:111)

M

term − 1
for the optimal number of nodes M (cid:63) = M (d) def= 2

m becomes (roughly) equal to 1

M

2: − 1

M +
(cid:16)(cid:113) d

m = 1
M
(cid:17)
m − 1

2. This gives the formula
, and the resulting iteration

(cid:110) 2
√
d√
m , (κ + 1)
√
d
M +
√

√
d
√

complexity is W (M (cid:63)) = max
it makes sense to use more nodes for larger models (big d).

(cid:110) 2
√
d√
m , κ + 1

(cid:111)

. Note that M (d) is increasing in d. Hence,

F.5 Quantization Lemmas

Consider iteration k of the DIANA method (Algorithm 10). Let EQk be the expectation
with respect to the randomness inherent in the quantization steps ˆ∆k
i , (dl)m
for i = 1, 2, . . . , n (i.e., we condition on everything else).

i ∼ Quantp(∆k

l=1)

253

Lemma F.5.1. For all iterations k ≥ 0 of DIANA and i = 1, 2, . . . , n we have the
identities

(cid:2)ˆgk

i

(cid:3) = gk
i ,

EQk

(cid:2)(cid:107)ˆgk

i − gk

i (cid:107)2(cid:3) = Ψ(∆k
i )

(F.4)

EQk

and

EQk

(cid:2)ˆgk(cid:3) = gk def=

1
M

M
(cid:88)

i=1

gk
i ,

EQk

(cid:2)(cid:107)ˆgk − gk(cid:107)2(cid:3) =

1
M 2

M
(cid:88)

i=1

Ψ(∆k

i ).

(F.5)

Furthermore, letting h(cid:63) = ∇f (x(cid:63)), and invoking Assumption 6.3.1, we have

E (cid:2)ˆgk(cid:3) = ∇f (xk),

E (cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)∇f (xk) − h(cid:63)(cid:107)2(cid:3) +

1
M 2

M
(cid:88)

i=1

E (cid:2)Ψ(∆k

i )(cid:3) +

σ2
M

.

(F.6)

(F.7)

Proof.

(i) Since ˆgk

i = hk

i + ˆ∆k

i and ∆k

i − hk

i , we can apply Lemma F.1.2 and

obtain

EQk

(cid:2)ˆgk

i

(cid:3) = hk

i + EQk

(cid:105) (F.2)

= hk

i + ∆k

i = gk
i .

i = gk
(cid:104) ˆ∆k

i

i − gk

Since ˆgk
identity in (F.4).

i = ˆ∆k

i − ∆k

i , applying the second part of Lemma 1 gives the second

(ii) The ﬁrst part of (F.5) follows directly from the ﬁrst part of (F.4):

EQk

(cid:2)ˆgk(cid:3) = EQk

(cid:34)

1
M

M
(cid:88)

i=1

(cid:35)

ˆgk
i

=

1
M

M
(cid:88)

i=1

EQk

(cid:2)ˆgk

i

(cid:3) (F.4)
=

1
M

M
(cid:88)

i=1

(F.5)
= gk.

gk
i

The second part in (F.5) follows from the second part of (F.4) and independence
of ˆgk

1 , . . . , ˆgk

M .

(iii) The ﬁrst part of (F.7) follows directly from the ﬁrst part of (F.5) and the assumption
It remains to establish the second

is and unbiased estimate of ∇fi(xk).

that gk
i
part of (F.7). First, we shall decompose

EQk

(cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2(cid:3)

(1.3)
= EQk
(F.5)
= EQk

(cid:2)ˆgk(cid:3) (cid:107)2(cid:3) + (cid:107)EQk
(cid:2)(cid:107)ˆgk − EQk
(cid:2)(cid:107)ˆgk − gk(cid:107)2(cid:3) + (cid:107)gk − h(cid:63)(cid:107)2

(cid:2)ˆgk(cid:3) − h(cid:63)(cid:107)2

(F.5)
=

1
M 2

M
(cid:88)

i=1

Ψ(∆k

i ) + (cid:107)gk − h(cid:63)(cid:107)2.

Further, applying variance decomposition (1.3), we get

254

E (cid:2)(cid:107)gk − h(cid:63)(cid:107)2 | xk(cid:3) (1.3)

= E (cid:2)(cid:107)gk − E[gk | xk](cid:107)2 | xk(cid:3) + (cid:107)E[gk | xk] − h(cid:63)(cid:107)2
= E (cid:2)(cid:107)gk − ∇f (xk)(cid:107)2 | xk(cid:3) + (cid:107)∇f (xk) − h(cid:63)(cid:107)2

(6.3)

(6.4)
≤

σ2
M

+ (cid:107)∇f (xk) − h(cid:63)(cid:107)2.

Combining the two results, we get

E (cid:2)EQk[(cid:107)ˆgk − h(cid:63)(cid:107)2] | xk(cid:3)] ≤

1
M 2

M
(cid:88)

i=1

E (cid:2)Ψ(∆k

i ) | xk(cid:3) +

σ2
M

+ (cid:107)∇f (xk) − h(cid:63)(cid:107)2.

After applying full expectation, and using tower property, we get the result.

(cid:4)

Lemma F.5.2. Let x(cid:63) be a solution of (6.1) and let h(cid:63)
For every i, we can estimate the ﬁrst two moments of hk+1

i = ∇fi(x(cid:63)) for i = 1, 2, . . . , d.
i

as

(cid:2)hk+1

EQk
(cid:2)(cid:107)hk+1
i − h(cid:63)

i

(cid:3) = (1 − α)hk
i (cid:107)2(cid:3) = (1 − α)(cid:107)hk

EQk

i + αgk
i ,
i − h(cid:63)
i (cid:107)2 + α(cid:107)gk
m
(cid:88)

(cid:107)∆k

i (cid:107)2 − α

(cid:32)

l=1

− α

i − h(cid:63)

i (cid:107)2

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:33)

.

Proof. Since

and ∆k

i = gk

i − hk

EQk

(cid:2)hk+1

i

hk+1
i = hk

i + α ˆ∆k
i
i , in view of Lemma F.1.2 we have
(cid:104) ˆ∆k

= hk

= hk

i + αEQk

(cid:105) (F.2)

(cid:3) (F.9)

i + α∆k

i

i = (1 − α)hk

i + αgk
i ,

(F.8)

(F.9)

(F.10)

which establishes the ﬁrst claim. Further, using (cid:107)∆k

i (cid:107)2 =

m
(cid:80)
l=1

(cid:107)∆k

i (l)(cid:107)2 we obtain

255

EQk

(cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2(cid:3)

(1.3)
=

(F.10)+(F.9)
=

(F.2)
=

(1.12)
=

=

i − h(cid:63)

(cid:107)EQk[hk+1
(cid:107)(1 − α)hk

i + αgk

i ](cid:107)2 + EQk
i − h(cid:63)

(cid:2)(cid:107)hk+1

i − EQk
(cid:104)
(cid:107) ˆ∆k

i (cid:107)2 + α2EQk

(cid:2)hk+1

i

i − EQk

(cid:3) (cid:107)2(cid:3)
(cid:104) ˆ∆k

i

(cid:105)

(cid:107)2(cid:105)

i − h(cid:63)
(cid:107)(1 − α)(hk
m
(cid:88)

+α2

i ) + α(gk

i − h(cid:63)

i )(cid:107)2

((cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p − (cid:107)∆k

i (l)(cid:107)2)

l=1
i − h(cid:63)

i (cid:107)2 + α(cid:107)gk

i − h(cid:63)

i (cid:107)2 − α(1 − α)(cid:107)∆k

i (cid:107)2

(1 − α)(cid:107)hk
m
(cid:88)

+α2

l=1
(1 − α)(cid:107)hk
m
(cid:88)

+α2

((cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p) − α2(cid:107)∆k

i (cid:107)2

i − h(cid:63)

i (cid:107)2 + α(cid:107)gk

i − h(cid:63)

i (cid:107)2

((cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p) − α(cid:107)∆k

i (cid:107)2.

l=1

Lemma F.5.3. We have

E (cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2 | xk(cid:3) ≤ (cid:107)∇f (xk) − h(cid:63)(cid:107)2 +

(cid:18) 1
αp

− 1

(cid:19) 1
M 2

M
(cid:88)

i=1

(cid:107)∇fi(xk) − hk

i (cid:107)2 +

(cid:4)

.

σ2
αpM
(F.11)

Proof. Since αp = αp( max
l=1,...,m

dl) and αp(dl) = inf

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

, we have for a particular

choice of x = ∆k

i (l) that αp ≤ αp(dl) ≤

x(cid:54)=0,x∈Rdl
(cid:107)∆k
i (l)(cid:107)2
i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:107)∆k

. Therefore,

Ψ(∆k

i ) =

m
(cid:88)

l=1

Ψl(∆k

i ) =

m
(cid:88)

((cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)∞ − (cid:107)∆k

i (l)(cid:107))

l=1
m
(cid:88)

l=1

(cid:18) 1
αp

≤

(cid:19)

− 1

(cid:107)∆k

i (l)(cid:107)2 =

(cid:19)

− 1

(cid:18) 1
αp

(cid:107)∆k

i (cid:107)2.

This can be applied to (F.7) in order to obtain

E (cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2 | xk(cid:3) ≤ (cid:107)∇f (xk) − h(cid:63)(cid:107)2 +

≤ (cid:107)∇f (xk) − h(cid:63)(cid:107)2 +

1
M 2

1
M 2

M
(cid:88)

i=1
M
(cid:88)

i=1

E (cid:2)Ψ(∆k

i ) | xk(cid:3) +

σ2
M

(cid:19)

− 1

(cid:18) 1
αp

E (cid:2)(cid:107)∆k

i (cid:107)2 | xk(cid:3) +

σ2
M

.

Note that for every i we have E (cid:2)∆k

i − hk

i | x(cid:3) = ∇fi(xk) − hk

i , so

256
i | xk(cid:3) = E (cid:2)gk

E (cid:2)(cid:107)∆k

i (cid:107)2 | xk(cid:3) (1.3)

= (cid:107)∇fi(xk) − hk
≤ (cid:107)∇fi(xk) − hk

i (cid:107)2 + E (cid:2)(cid:107)gk
i (cid:107)2 + σ2
i .

i − ∇fi(xk)(cid:107)2 | xk(cid:3)

Summing the produced bounds, we get the claim.

(cid:4)

F.6 Proof of Theorem 6.5.4

Proof. Note that x(cid:63) is a solution of (6.1) if and only if x(cid:63) = proxγR(x(cid:63) − γh(cid:63)) (this holds
for any γ > 0). Using this identity together with the nonexpansiveness of the proximal
operator, we shall bound the ﬁrst term of the Lyapunov function:

EQk

(cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) = EQk

(cid:2)(cid:107)proxγψ(xk − γˆgk) − proxγψ(x(cid:63) − γh(cid:63))(cid:107)2(cid:3)
(cid:2)(cid:107)xk − γˆgk − (x(cid:63) − γh(cid:63))(cid:107)2(cid:3)

(1.26)
≤ EQk
= (cid:107)xk − x(cid:63)(cid:107)2 − 2γEQk
= (cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)gk − h(cid:63), xk − x(cid:63)(cid:11) + γ2EQk
(F.5)

(cid:2)(cid:10)ˆgk − h(cid:63), xk − x(cid:63)(cid:11)(cid:3) + γ2EQk

(cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2(cid:3)

(cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2(cid:3) .

Next, taking conditional expectation on both sides of the above inequality, and using
(6.3), we get

E (cid:2)EQk

(cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) | xk(cid:3) ≤ (cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)

+ γ2E (cid:2)EQk[(cid:107)ˆgk − h(cid:63)(cid:107)2] | xk(cid:3) .

Taking full expectation on both sides of the above inequality, and applying the tower
property and Lemma F.5.1 leads to

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3) + γ2E (cid:2)(cid:107)ˆgk − h(cid:63)(cid:107)2(cid:3)

(F.7)

≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

+γ2E (cid:2)(cid:107)∇f (xk) − h(cid:63)(cid:107)2(cid:3) +

γ2
M 2

M
(cid:88)

E (cid:2)Ψ(∆k

i )(cid:3) +

γ2σ2
M

i=1

≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

+

γ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) +

γ2
M 2

M
(cid:88)

i=1

E (cid:2)Ψ(∆k

i )(cid:3) +

γ2σ2
M

,

(F.12)

where the last inequality follows from the identities ∇f (xk) = 1
M
1
M

i and an application of Jensen’s inequality.

i=1 h(cid:63)

(cid:80)M

(cid:80)M

i=1 fi(xk), h(cid:63) =

Averaging over the identities (F.8) for i = 1, 2, . . . , n in Lemma F.5.2, we get

257

1
M

M
(cid:88)

i=1

EQk

(cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2(cid:3) =

1 − α
M

M
(cid:88)

(cid:107)hk

i − h(cid:63)

i (cid:107)2 +

i=1
M
(cid:88)

i=1

−

α
M

(cid:32)

(cid:107)∆k

i (cid:107)2 − α

α
M

M
(cid:88)

i=1

(cid:107)gk

i − h(cid:63)

i (cid:107)2

m
(cid:88)

l=1

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:33)

.

Applying expectation to both sides, and using the tower property, we get

E (cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2(cid:3) =

1
M

M
(cid:88)

i=1

Since

1 − α
M

−

α
M

i=1
M
(cid:88)

E

i=1

M
(cid:88)

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3) +

α
M

M
(cid:88)

i=1

E (cid:2)(cid:107)gk

i − h(cid:63)

i (cid:107)2(cid:3)

(cid:34)

(cid:107)∆k

i (cid:107)2 − α

m
(cid:88)

l=1

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:35)

.

(F.13)

E[(cid:107)gk

i −h(cid:63)

i (cid:107)2 | xk]

(1.3)
= (cid:107)∇fi(xk)−h(cid:63)

i (cid:107)2+E[(cid:107)gk

i −∇fi(xk)(cid:107)2 | xk]

(6.2)
≤ (cid:107)∇fi(xk)−h(cid:63)

i (cid:107)2+σ2
i ,

the second term on the right-hand side of (F.13) can be bounded above as

E (cid:2)(cid:107)gk

i − h(cid:63)

i (cid:107)2(cid:3) ≤ E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) + σ2
i .

(F.14)

Plugging (F.14) into (F.13) leads to the estimate

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2(cid:3) ≤

1 − α
M

+ ασ2 −

M
(cid:88)

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3) +

α
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3)

i=1

α
M

(cid:34)

M
(cid:88)

E

i=1

(cid:107)∆k

i (cid:107)2 − α

m
(cid:88)

l=1

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:35)

.

(F.15)

Adding (F.12) with the cγ2 multiple of (F.15), we get an upper bound one the Lya-

punov function:

E (cid:2)V k+1(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

(1 − α)cγ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3)

+

+

γ2(1 + αc)
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

γ2
M 2

M
(cid:88)

m
(cid:88)

i=1

l=1

E (cid:2)T k

i (l)(cid:3) + (M cα + 1)

γ2σ2
M

,

(F.16)

where

258

i (l) def= (cid:2)(cid:0)(cid:107)∆k
T k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p − (cid:107)∆k

i (l)(cid:107)2(cid:1) − nαc (cid:0)(cid:107)∆k

i (l)(cid:107)2 − α(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:1)(cid:3) .

i (l) ≤ 0 for all ∆k
We now claim that due to our choice of α and c, we have T k
which means that we can bound this term away by zero. Indeed, note that T i
∆k

k(l) ≤ 0 can be equivalently written as

i (l) (cid:54)= 0, then T i

i (l) = 0. If ∆k

i (l) ∈ Rdl,
k(l) = 0 for

1 + M cα2
1 + M cα

≤

i (l)(cid:107)2
(cid:107)∆k
i (l)(cid:107)1(cid:107)∆k

(cid:107)∆k

i (l)(cid:107)p

.

However, this inequality holds since in view of the ﬁrst inequality in (6.12) and the deﬁ-
nitions of αp and αp(dl), we have

1 + M cα2
1 + M cα

(6.12)
≤ αp ≤ αp(dl)

(6.11)
=

inf
x(cid:54)=0,x∈Rdl

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

≤

Therefore, from (F.16) we get

i (l)(cid:107)2
(cid:107)∆k
i (l)(cid:107)1(cid:107)∆k

(cid:107)∆k

i (l)(cid:107)p

.

E (cid:2)V k+1(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

(1 − α)cγ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3)

+

γ2(1 + αc)
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3)

− 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3) + (M cα + 1)

γ2σ2
M

.

(F.17)

The next trick is to split ∇f (xk) into the average of ∇fi(xk) in order to apply strong

convexity of each term:

E (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

(F.18)

=

(1.22)
≥

1
M

1
M

M
(cid:88)

i=1
M
(cid:88)

i=1

E (cid:2)(cid:10)∇fi(xk) − h(cid:63)

i , xk − x(cid:63)(cid:11)(cid:3)

E

(cid:20) µL
µ + L

(cid:107)xk − x(cid:63)(cid:107)2 +

1
µ + L

(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2

(cid:21)

=

µL
µ + L

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

1
µ + L

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) .

(F.19)

Plugging these estimates into (F.17), we obtain

259

E (cid:2)V k+1(cid:3) ≤

(cid:18)

1 −

(cid:19)

2γµL
µ + L

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

(1 − α)cγ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3)

(cid:18)

+

γ2(1 + αc) −

2γ
µ + L

(cid:19) 1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) + (M cα + 1)

γ2σ2
M

.

(F.20)

Notice that in view of the second inequality in (6.13), we have γ2(1 + αc) − 2γ

µ+L ≤ 0.
i , xk − x(cid:63)(cid:105).
Moreover, since fi is µ–strongly convex, we have µ(cid:107)xk − x(cid:63)(cid:107)2 ≤ (cid:104)∇fi(xk) − h(cid:63)
Applying the Cauchy-Schwarz inequality to further bound the right-hand side, we get the
inequality µ(cid:107)xk − x(cid:63)(cid:107) ≤ (cid:107)∇fi(xk) − h(cid:63)
i (cid:107). Using these observations, we can get rid of the
term on the second line of (F.20) and absorb it with the ﬁrst term, obtaining

E (cid:2)V k+1(cid:3) ≤ (cid:0)1 − 2γµ + γ2µ2 + cαγ2µ2(cid:1) E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3)

+

(1 − α)cγ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3) + (M cα + 1)

γ2σ2
M

.

(F.21)

It follows from the second inequality in (6.13) that

1 − 2γµ + γ2µ2 + cαγ2µ2 ≤ 1 − γµ.

Moreover, the ﬁrst inequality in (6.13) implies that 1 − α ≤ 1 − γµ. Consequently, from
(F.21) we obtain the recursion

E (cid:2)V k+1(cid:3) ≤ (1 − γµ)E (cid:2)V k(cid:3) + (M cα + 1)

γ2σ2
M

.

Finally, unrolling the recurrence leads to

E (cid:2)V k(cid:3) ≤ (1 − γµ)kV 0 +

k−1
(cid:88)

(1 − γµ)lγ2(1 + M cα)

≤ (1 − γµ)kV 0 +

l=0
∞
(cid:88)

(1 − γµ)lγ2(1 + M cα)

= (1 − γµ)kV 0 +

l=0
γ
µ

(1 + M cα)

σ2
M

.

σ2
M

σ2
M

(cid:4)

F.7 Proof of Corollary 6.5.5

260

(cid:111)
Corollary F.7.1. Let κ = L
.
Then the conditions (6.12) and (6.13) are satisﬁed, and the leading term in the iteration
complexity bound is equal to

2 , c = 4(1−αp)

µ , α = αp

, and γ = min

2
(L+µ)(1+cα)

(cid:110) α
µ ,

M α2
p

1
γµ

= max

(cid:26) 2
αp

, (κ + 1)

(cid:18)1
2

−

1
M

+

1
M αp

(cid:19)(cid:27)

.

(F.22)

This is a decreasing function of p. Hence, from iteration complexity perspective, p = +∞
is the optimal choice.

Proof. Condition (6.13) is satisﬁed since γ = min
(6.12) is also satisﬁed:

(cid:110) α
µ ,

2
(L+µ)(1+cα)

(cid:111)

. Now we check that

1 + M cα2
1 + M cα

1
αp

· α2
p
4
· αp
2

·

1
αp

=

=

1 + M · 4(1−αp)
M α2
p
1 + M · 4(1−αp)
M α2
p
2 − αp
αp + 2(1 − αp)

= 1.

Since α = αp

2 and c = 4(1−αp)

M α2
p

we have

1 + αc = 1 +

2(1 − αp)
M αp

= 1 −

2
M

+

2
M αp

and, therefore,

1
γµ

= max

(cid:26) 1
α

,

L + µ
2µ

(cid:27)

(1 + cα)

= max

(cid:26) 2
αp

, (κ + 1)

(cid:18) 1
2

−

1
M

+

1
M αp

(cid:19)(cid:27)

,

which is a decreasing function of p, because αp increases when p increases.

(cid:4)

F.8 Strongly Convex Case: Decreasing Stepsize

Lemma F.8.1. Let a sequence (ak)k satisfy inequality ak+1 ≤ (1 − γkµ)ak + γ2
positive γk ≤ γ0 with some constants µ > 0, ν > 0, γ0 > 0. Further, let θ ≥ 2
γ0
C such that ν ≤ µθ

4 C and a0 ≤ C. Then, it holds

kν for any
and take

ak ≤

C
µ
θ k + 1

if we set γk = 2

µk+θ .

Proof. We will show the inequality for ak by induction. Since inequality a0 ≤ C is one of

261

our assumptions, we have the initial step of the induction. To prove the inductive step,
consider

ak+1 ≤ (1 − γkµ)ak + γ2

kν ≤

(cid:18)

1 −

2µ
µk + θ

(cid:19) θC

µk + θ

+ θµ

C
(µk + θ)2 .

To show that the right-hand side is upper bounded by
multiplying both sides by (µk + θ)(µk + µ + θ)(θC)−1,

θC

µ(k+1)+θ , one needs to have, after

(cid:18)

1 −

(cid:19)

2µ
µk + θ

(µk + µ + θ) + µ

µk + µ + θ
µk + θ

≤ µk + θ,

which is equivalent to

µ − µ

µk + µ + θ
µk + θ

≤ 0.

The last inequality is trivially satisﬁed for all k ≥ 0.

(cid:4)

We are not ready to prove Theorem 6.5.6. Below we repeat its statement and prove

the proof right afterwards.

Theorem F.8.2. Assume that f is L-smooth, µ-strongly convex and we have access to
(cid:111)
its gradients with bounded noise. Set γk = 2
for some numbers α > 0 and c > 0 satisfying 1+M cα2
we have

1+M cα ≤ αp. After k iterations of DIANA

µk+θ with some θ ≥ 2 max

α, (µ+L)(1+cα)

(cid:110) µ

2

E (cid:2)V k(cid:3) ≤

1
ηk + 1

(cid:26)

V 0, 4

max

(1 + M cα)σ2
M θµ

(cid:27)

,

where η def= µ
of the gradient noise.

θ , V k = (cid:107)xk − x(cid:63)(cid:107)2 + cγk

M

(cid:80)M

i=1 (cid:107)h0

i − h(cid:63)

i (cid:107)2 and σ is the standard deviation

Proof. To get a recurrence, let us recall an upper bound we have proved before:

E (cid:2)V k+1(cid:3) ≤ (1 − γkµ)E (cid:2)V k(cid:3) + (γk)2(1 + M cα)

σ2
M

.

Having that, we can apply Lemma F.8.1 to the sequence E (cid:2)V k(cid:3). The constants for the
lemma are: ν = (1+M cα) σ2
, and µ is the strong convexity
(cid:4)
constant.

V 0, 4 (1+M cα)σ2

M , C = max

M θµ

(cid:110)

(cid:111)

Corollary F.8.3. If we choose α = αp

µ
αp

max

(cid:110)

4, 2(κ+1)

M + (κ+1)(M −2)

M

αp

(cid:111)

, then there are three regimes:

2 , c = 4(1−αp)

M α2
p

, θ = 2 max

(cid:110) µ

α , (µ+L)(1+cα)

2

(cid:111)

=

1) if 1 = max (cid:8)1, κ

M , καp

(cid:9), then θ = Θ

(cid:17)

(cid:16) µ
αp

and to achieve E (cid:2)V K(cid:3) ≤ ε we need at

262

K = O

(cid:18)

(cid:18) 1
αp

V 0 +

(1 − αp)σ2
M µ2

(cid:19) 1
ε

(cid:19)

most

iterations;

2) if κ

M = max (cid:8)1, κ

M , καp

(cid:9), then θ = Θ

(cid:17)

(cid:16) L
M αp

and to achieve E (cid:2)V K(cid:3) ≤ ε we need

K = O

(cid:18) κ

M αp

(cid:18)

V 0 +

(1 − αp)σ2
µL

(cid:19) 1
ε

(cid:19)

3) if καp = max (cid:8)1, κ

M , καp

(cid:9), then θ = Θ (L) and to achieve E (cid:2)V K(cid:3) ≤ ε we need at

(cid:18)

K = O

κ

(cid:18)

V 0 +

(1 − αp)σ2
µLM αp

(cid:19)

(cid:19) 1
ε

at most

iterations;

most

iterations.

Proof. First of all, let us show that c = 4(1−αp)
M α2
p

and α satisfy inequality 1+M cα2

1+M cα ≤ αp:

1 + M cα2
1 + M cα

1
αp

· α2
p
4
· αp
2

·

1
αp

1 + M · 4(1−αp)
M α2
p
1 + M · 4(1−αp)
M α2
p
2 − αp
αp + 2(1 − αp)

=

=

= 1.

Moreover, since

1 + cα = 1 +

2(1 − αp)
M αp

=

2 + (M − 2)αP
M αp

we can simplify the deﬁnition of θ:

θ = 2 max

,

(cid:26) µ
α
(cid:26)

(cid:27)

(µ + L) (1 + cα)
2
2(κ + 1)
M
κ
M

, καp

(cid:111)(cid:19)

+

1,

(cid:110)

.

µ
αp

=

max

4,

= Θ

max

(cid:18) µ
αp

(κ + 1)(M − 2)
M

(cid:27)

αp

Using Theorem 6.5.6, we get in the case:
(cid:9), then θ = Θ

1) if 1 = max (cid:8)1, κ

M , καp

and to achieve E (cid:2)V K(cid:3) ≤ ε we need at most

(cid:17)

(cid:16) µ
αp

, η = Θ (αp), 4(1+M cα)σ2

M θµ

= Θ

(cid:16) (1−αp)σ2
M µ2

(cid:17)

K = O

(cid:18) 1
αp

(cid:18)

V 0 +

max

(1 − αp)σ2
M µ2

(cid:19) 1
ε

(cid:19)

iterations;

263

2) if κ

M = max (cid:8)1, κ
(cid:17)
(cid:16) (1−αp)σ2
µL

(cid:9), then θ = Θ

M , καp

, η = Θ
and to achieve E (cid:2)V k(cid:3) ≤ ε we need at most

(cid:17)

(cid:16) L
M αp

Θ

(cid:17)

(cid:16) αpM
κ

, 4(1+M cα)σ2
M θµ

=

K = O

(cid:18) κ

M αp

(cid:18)

V 0 +

(1 − αp)σ2
µL

(cid:19) 1
ε

(cid:19)

iterations;

3) if καp = max (cid:8)1, κ

M , καp
and to achieve E (cid:2)V K(cid:3) ≤ ε we need at most

(cid:9), then θ = Θ (L), η = Θ (cid:0) 1

κ

(cid:1), 4(1+M cα)σ2

M θµ

= Θ

(cid:16) (1−αp)σ2
µLM αp

(cid:17)

(cid:18)

K = O

κ

(cid:18)

V 0 +

(1 − αp)σ2
µLM αp

(cid:19)

(cid:19) 1
ε

iterations.

F.9 Non-Convex Analysis

(cid:4)

Theorem F.9.1. Assume that ψ is constant and Assumption 6.6.1 holds. Also assume
that f is L-smooth, stepsizes α > 0 and γk = γ > 0 and parameter c > 0 satisfying
1+M cα2
1+M cα ≤ αp, γ ≤

L(1+2cα) and xK is chosen randomly from {x0, . . . , xK−1}. Then

2

E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3) ≤

2
K

Λ0
γ(2 − Lγ − 2cαLγ)

+

(1 + 2cM α)Lγ
2 − Lγ − 2cαLγ

σ2
M

+

4cαLγζ 2
2 − Lγ − 2cαLγ

,

where Λk def= f (xk) − f (cid:63) + c Lγ2
2

1
M

(cid:80)M

i=1 (cid:107)hk

i − h(cid:63)

i (cid:107)2.

Proof. The assumption that ψ is constant implies that xk+1 = xk − γˆgk and h(cid:63) = 0.
Moreover, by smoothness of f

E (cid:2)f (xk+1)(cid:3)

≤ E (cid:2)f (xk)(cid:3) + E (cid:2)(cid:10)∇f (xk), xk+1 − xk(cid:11)(cid:3) +

E (cid:2)(cid:107)xk+1 − xk(cid:107)2(cid:3)

(6.9)

(F.5)

≤ E (cid:2)f (xk)(cid:3) − γE (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +
(cid:18)

(cid:19)

(F.7)

≤ E (cid:2)f (xk)(cid:3) −

γ −

Lγ2
2
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

Lγ2
2

L
2
E (cid:2)(cid:107)ˆgk(cid:107)2(cid:3)

+

Lγ2
2

1
M 2

M
(cid:88)

i=1

E (cid:2)Ψ(∆k

i )(cid:3) +

Lγ2
2M 2

M
(cid:88)

i=1

σ2
i .

Denote Λk def= f (xk) − f (cid:63) + c Lγ2
2

1
M

(cid:80)M

i=1 (cid:107)hk

i − h(cid:63)

i (cid:107)2. Due to Assumption 6.6.1 we can

rewrite the equation (F.8) after summing it up for i = 1, . . . , n in the following form

264

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2 | xk(cid:3)

1 − α
M

M
(cid:88)

i=1
(cid:32)

(cid:107)hk

i − h(cid:63)

i (cid:107) +

α
M

M
(cid:88)

i=1

−α

(cid:107)∆k

i (cid:107)2 − α

m
(cid:88)

l=1

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:33)

E (cid:2)(cid:107)gk

i − h(cid:63)

i (cid:107)2 | xk(cid:3)

≤

≤

1 − α
M

M
(cid:88)

i=1
(cid:32)

(cid:107)hk

i − h(cid:63)

i (cid:107) +

−α

(cid:107)∆k

i (cid:107)2 − α

(6.6.1)
≤

1 − α
M

M
(cid:88)

i=1
(cid:32)

(cid:107)hk

i − h(cid:63)

i (cid:107) +

−α

(cid:107)∆k

i (cid:107)2 − α

2α
M

M
(cid:88)

i=1

E (cid:2)(cid:107)gk

i (cid:107)2 | xk(cid:3) +

(cid:33)

2α
M

M
(cid:88)

i=1

(cid:107)h(cid:63)

i − h(cid:63)
(cid:124)(cid:123)(cid:122)(cid:125)
0

(cid:107)2

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

2α
M

M
(cid:88)

i=1

(cid:107)∇fi(xk)(cid:107)2 +

(cid:33)

2α
M

M
(cid:88)

i=1

i + 2αζ 2
σ2

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

m
(cid:88)

l=1

m
(cid:88)

l=1

(6.6.1)+(1.3)
≤

1 − α
M

M
(cid:88)

i=1
(cid:32)

(cid:107)hk

i − h(cid:63)

i (cid:107) + 2α(cid:107)∇f (xk)(cid:107)2 + 2ασ2 + 4αζ 2

−α

(cid:107)∆k

i (cid:107)2 − α

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:33)

,

m
(cid:88)

l=1

If we add it to the bound above, we get

E (cid:2)Λk+1(cid:3) = E (cid:2)f (xk+1) − f (cid:63)(cid:3) + c

Lγ2
2

1
M

≤ E (cid:2)f (xk) − f (cid:63)(cid:3) − γ

(cid:18)

1 −

Lγ
2

M
(cid:88)

i=1

E (cid:2)(cid:107)hk+1

i − h(cid:63)

i (cid:107)2(cid:3)

− cαLγ

(cid:19)

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

+ (1 − α)c

Lγ2
2

1
M

+ (1 + 2cM α)

Lγ2
2

M
(cid:88)

i=1
σ2
M

where

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3) +

Lγ2
2

1
M 2

M
(cid:88)

m
(cid:88)

i=1

l=1

E (cid:2)T k

i (l)(cid:3)

+ 2cαLγ2ζ 2,

i (l) def= (cid:2)((cid:107)∆k
T k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p − (cid:107)∆k

i (l)(cid:107)2) − ncα((cid:107)∆k

i (l)(cid:107)2 − α(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p)(cid:3) .

As we have shown before, we have T k

i (l) ≤ 0 for all ∆k

i (l) ∈ Rdl. Putting all together we

have

E (cid:2)Λk+1(cid:3) ≤ E (cid:2)f (xk) − f (cid:63)(cid:3) + c

265

Lγ2
2

1
M

E (cid:2)(cid:107)hk

i − h(cid:63)

i (cid:107)2(cid:3) + 2cαLγ2ζ 2

M
(cid:88)

i=1
(cid:18)

+ (1 + 2cM α)

Lγ2
2

σ2
M

− γ

1 −

(cid:19)

− cαLγ

Lγ
2

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) .

Due to γ ≤
rearrange the terms and rewrite the last bound as

L(1+2cα) the coeﬃcient before (cid:107)∇f (xk)(cid:107)2 is positive. Therefore, we can

2

E[(cid:107)∇f (xk)(cid:107)2] ≤ 2

E (cid:2)Λk(cid:3) − E (cid:2)Λk+1(cid:3)
γ(2 − Lγ − 2cαLγ)

+

(1 + 2cM α)Lγ
2 − Lγ − 2cαLγ

σ2
M

+

4cαLγζ 2
2 − Lγ − 2cαLγ

.

Summing from 0 to k − 1 results in telescoping of the right-hand side, giving

E[(cid:107)∇f (xl)(cid:107)2] ≤ 2

Λ0 − E (cid:2)Λk(cid:3)
γ(2 − Lγ − 2cαLγ)

+ k

(1 + 2cM α)Lγ
2 − Lγ − 2cαLγ

σ2
M

+ k

4cαLγζ 2
2 − Lγ − 2cαLγ

.

k−1
(cid:88)

l=0

Note that E (cid:2)Λk(cid:3) is nonnegative and, thus, can be dropped. After that, it suﬃces to
divide both sides by k and rewrite the left-hand side as E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) where expectation
(cid:4)
is taken w.r.t. all randomness.
Corollary F.9.2. Set α = αp

, γ =

M αp

√

2 , c = 4(1−αp)

M α2
p

K
algorithm for K iterations. Then, the ﬁnal accuracy is at most

L(4+(M −4)αp)

, h0 = 0 and run the
L(4+(M −4)αp)
M αp

Λ0 +

2√
K

1√
K

(4−3αp)σ2
4+(M −4)αp

+ 8(1−αp)ζ2
√
(4+(M −4)αp)

K

.

Proof. Our choice of α and c implies

cα =

2(1 − αp)
nαp

,

1 + 2cα =

4 + (M − 4)αp
M αp

,

1 + 2cM α =

4 − 3αp
αp

.

Using this and the inequality γ =
2cαLγ = 2 − (1 + 2cα)Lγ ≥ 1. Putting all together we obtain

L(4+(M −4)αp)

≤

K

√

M αp

M αp

L(4+(M −4)αp) we get 2 − Lγ −

2
K

Λ0
γ (2 − Lγ − 2cαLγ)

+ (1 + 2cM α)

≤

2
√
K

L(4 + (M − 4)αp)
M αp

Λ0 +

1
√
K

Lγ
2 − Lγ − 2cαLγ
(4 − 3αp)σ2
4 + (M − 4)αp

+

σ2
M

+

4cαLγζ 2
2 − Lγ − 2cαLγ

8(1 − αp)ζ 2
(4 + (M − 4)αp)

√

.

K

(cid:4)

F.10 Momentum Version of DIANA

Theorem F.10.1. Assume that f is L-smooth, ψ ≡ const, h0
holds. Choose 0 ≤ α < αp, β < 1 − α and γ < 1−β2

i = 0 and Assumption 6.6.1
(1−β)2α ≤

2L(2ω−1), such that

β2

1−β2−2Lγ(2ω−1)
γ2L2δ

, where δ def= 1 + 2
M
sample xK uniformly from {x0, . . . , xK−1}. Then

− 1

(cid:16) 1
αp

(cid:17) (cid:16)

1 + α

1−α−β

(cid:17)

and ω def= M −1

M + 1
M αp

, and

266

E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3) ≤

+ 2γ

Lσ2
(1 − β)2M

(cid:19)

− 2

(cid:18) 3
αp

+ 2γ2 L2β2σ2
(1 − β)5M

(cid:19)

− 2

(cid:18) 3
αp

4(f (z0) − f (cid:63))
γK
+ 3γ2 L2β2ζ 2
(1 − β)5M

(cid:19)

− 1

.

(cid:18) 1
αp

Proof. The main idea of the proof is to ﬁnd virtual iterates zk whose recursion would
satisfy zk+1 = zk − γ
1−β ˆgk. Having found it, we can prove convergence by writing a
recursion on f (zk). One possible choice is deﬁned below:

zk def= xk −

γβ
1 − β

vk−1,

(F.23)

where for the edge case k = 0 we simply set v−1 = 0 and z0 = x0. Although zk is just
a slight perturbation of xk, applying smoothness inequality (6.9) to it produces a more
convenient bound than the one we would have if used xk. But ﬁrst of all, let us check
that we have the desired recursion for zk+1:

zk+1

(F.23)
= xk+1 −

vk

γβ
1 − β
γ
vk
1 − β
γβ
1 − β
γ
1 − β

ˆgk.

= xk −

= xk −

(F.23)
= zk −

vk−1 −

γ
1 − β

ˆgk

Now, it is time to apply smoothness of f :

E (cid:2)f (zk+1)(cid:3) ≤ E

(cid:20)
f (zk) + (cid:10)∇f (zk), zk+1 − zk(cid:11) +

(cid:21)

(cid:107)zk+1 − zk(cid:107)2

L
2

(F.23)
= E

(cid:20)
f (zk) −

γ
1 − β

(cid:10)∇f (zk), ˆgk(cid:11) +

Lγ2

2(1 − β)2 (cid:107)ˆgk(cid:107)2

(cid:21)

. (F.24)

The scalar product in (F.24) can be bounded using the fact that for any vectors a and b
one has − (cid:104)a, b(cid:105) = 1

2((cid:107)a − b(cid:107)2 − (cid:107)a(cid:107)2 − (cid:107)b(cid:107)2). In particular,

(cid:0)(cid:107)∇f (xk) − ∇f (zk)(cid:107)2 − (cid:107)∇f (xk)(cid:107)2 − (cid:107)∇f (zk)(cid:107)2(cid:1)

− (cid:10)∇f (zk), ∇f (xk)(cid:11) =

≤

=

1
2
1
2
γ2L2β2
2(1 − β)2 (cid:107)vk−1(cid:107)2 −

(cid:0)L2(cid:107)xk − zk(cid:107)2 − (cid:107)∇f (xk)(cid:107)2(cid:1)

1
2

(cid:107)∇f (xk)(cid:107)2.

The next step is to come up with an inequality for E (cid:2)(cid:107)vk(cid:107)2(cid:3). Since we initialize v−1 = 0,
one can show by induction that

267

vk =

k
(cid:88)

l=0

βlˆgk−l.

Deﬁne B def= (cid:80)k

l=0 βl = 1−βk+1

1−β . Then, by Jensen’s inequality

E (cid:2)(cid:107)vk(cid:107)2(cid:3) = B2E

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

k
(cid:88)

l=0

βl
B

ˆgk−l

2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ B2

k
(cid:88)

l=0

βl
B

E (cid:2)(cid:107)ˆgk−l(cid:107)2(cid:3) .

Since α < αp ≤ αp(dl) ≤

i (l)(cid:107)2
(cid:107)∆k
i (l)(cid:107)1(cid:107)∆k

(cid:107)∆k

i (l)(cid:107)p

for all i, k and l, we have

(cid:107)∆k

i (l)(cid:107)2 − α(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p ≥ (αp − α)(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p ≥ 0

for the case when ∆k
0. Taking into account this and the following equality

i (l) (cid:54)= 0. When ∆k

i (l) = 0 we simply have (cid:107)∆k

i (l)(cid:107)2−α(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p =

(cid:107)∆k

i (cid:107)2 − α

m
(cid:88)

l=1

we get from (F.8)

(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p =

m
(cid:88)

l=1

(cid:0)(cid:107)∆k

i (l)(cid:107)2 − α(cid:107)∆k

i (l)(cid:107)1(cid:107)∆k

i (l)(cid:107)p

(cid:1) ,

E (cid:2)(cid:107)hk

i (cid:107)2(cid:3) ≤ (1 − α)E (cid:2)(cid:107)hk−1
≤ (1 − α)2E (cid:2)(cid:107)hk−2

(cid:107)2(cid:3) + αE (cid:2)(cid:107)gk−1
(cid:107)2(cid:3) + α(1 − α)E (cid:2)(cid:107)gk−2

(cid:107)2(cid:3)

i

i

i

≤ . . . ≤ (1 − α)k (cid:107)h0

i (cid:107)2
(cid:124) (cid:123)(cid:122) (cid:125)
0

k−1
(cid:88)

(1 − α)jE

(cid:104)

+α

j=0

i

(cid:107)2(cid:3) + αE (cid:2)(cid:107)gk−1
(cid:107)2(cid:105)

(cid:107)gk−1−j
i

.

i

(cid:107)2(cid:3)

Next, let us use bounded variance of the stochastic gradients to improve the upper bound
to

E (cid:2)(cid:107)hk

i (cid:107)2(cid:3) ≤ α

≤ α

= α

k−1
(cid:88)

j=0

k−1
(cid:88)

j=0

k−1
(cid:88)

j=0

(1 − α)jE (cid:2)(cid:107)∇fi(xk−1−j)(cid:107)2(cid:3) + α

k−1
(cid:88)

(1 − α)jσ2
i

j=0

(1 − α)jE (cid:2)(cid:107)∇fi(xk−1−j)(cid:107)2(cid:3) + α ·

σ2
i
1 − (1 − α)

(1 − α)jE (cid:2)(cid:107)∇fi(xk−1−j)(cid:107)2(cid:3) + σ2
i .

Under our special assumptions, inequality (F.11) gives us

268

E (cid:2)(cid:107)ˆgk(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

≤ E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

(cid:18) 1
αp

− 1

(cid:19) 1
M 2

M
(cid:88)

i=1

2
M 2

(cid:18) 1
αp

(cid:19) M
(cid:88)

− 1

i=1

E (cid:2)(cid:107)∇fi(xk) − hk

(cid:124)
(cid:123)(cid:122)
≤2(cid:107)∇fi(xk)(cid:107)2+2(cid:107)hk

i (cid:107)2(cid:3)
(cid:125)
i (cid:107)2

+

σ2
αpM

(cid:107)∇fi(xk)(cid:107)2

(cid:19) M
(cid:88)

− 1

E (cid:2)(cid:107)hk

i (cid:107)2(cid:3) +

σ2
αpM

i=1
(cid:19)(cid:19)

− 1

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

2
M

(cid:18) 1
αp

(cid:19)

− 1

ζ 2

(cid:19) M
(cid:88)

− 1

E (cid:2)(cid:107)hk

i (cid:107)2(cid:3) +

σ2
αpM

i=1
(cid:19)(cid:19)

− 1

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:19) M
(cid:88)

− 1

k−1
(cid:88)

(1 − α)jE (cid:2)(cid:107)∇fi(xk−1−j)(cid:107)2(cid:3)

i=1

j=0
(cid:19) 2σ2 + 2ζ 2
M
(cid:19)(cid:19)

+

σ2
αpM

− 1

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

+

2
M 2

(6.17)
≤

(cid:18)

1 +

2
M

+

2
M 2

(cid:18)

≤

1 +

2
M

(cid:18) 1
αp
(cid:18) 1
αp
(cid:18) 1
αp
(cid:18) 1
αp
(cid:18) 1
αp

− 1

(cid:18) 1
αp

+

+

2α
M 2
(cid:18) 1
αp
2
M

+

2α
M

(cid:18) 1
αp

(cid:18)

(6.17)
≤

1 +

(cid:19) k−1
(cid:88)

− 1

(1 − α)jE (cid:2)(cid:107)∇f (xk−1−j)(cid:107)2(cid:3)

j=0
(cid:19) 2σ2 + 2ζ 2
M
(cid:19)(cid:19)

+

(cid:18) 1
αp

(cid:18)

≤

1 +

2
M

− 1

(cid:18) 1
αp

− 1

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

+

σ2
αpM

+

2α
M

(cid:18) 1
αp

(cid:19) k−1
(cid:88)

− 1

j=0

(1 − α)jζ 2

(cid:19) k−1
(cid:88)

− 1

(1 − α)jE (cid:2)(cid:107)∇f (xk−1−j)(cid:107)2(cid:3)

(cid:18) 1
αp

+

+

2α
M
(cid:18) 1
αp

j=0
(cid:19) 2σ2 + 3ζ 2
M

− 1

+

σ2
αpM

.

Using this, we continue our evaluation of E (cid:2)(cid:107)vk(cid:107)2(cid:3):

269

E (cid:2)(cid:107)vk(cid:107)2(cid:3) ≤ B

k
(cid:88)

l=0

βl

(cid:18)

1 +

2
M

(cid:18) 1
αp

(cid:19)(cid:19)

− 1

E (cid:2)(cid:107)∇f (xk−l)(cid:107)2(cid:3)

+B

(cid:18) 1
αp

− 1

(cid:19) 2α
M

k
(cid:88)

k−l−1
(cid:88)

l=0

j=0

βl(1 − α)jE (cid:2)(cid:107)∇f (xk−l−1−j)(cid:107)2(cid:3)

+B

k
(cid:88)

l=0

βl

(cid:18)(cid:18) 1
αp

− 1

(cid:19) 2σ2 + 3ζ 2
M

+

(cid:19)

.

σ2
αpM

Now we are going to simplify the double summation:

k
(cid:88)

k−l−1
(cid:88)

l=0

j=0

βl(1 − α)jE (cid:2)(cid:107)∇f (xk−l−1−j)(cid:107)2(cid:3) =

k
(cid:88)

k−l−1
(cid:88)

l=0

j=0

βl(1 − α)k−l−1−jE (cid:2)(cid:107)∇f (xj)(cid:107)2(cid:3)

=

=

≤

k−1
(cid:88)

j=0

k−1
(cid:88)

j=0

k
(cid:88)

j=0

E (cid:2)(cid:107)∇f (xj)(cid:107)2(cid:3)

k−j−1
(cid:88)

l=0

βl(1 − α)k−l−1−j

E (cid:2)(cid:107)∇f (xj)(cid:107)2(cid:3) ·

(1 − α)k−j − βk−j
1 − α − β

E (cid:2)(cid:107)∇f (xj)(cid:107)2(cid:3) ·

(1 − α)k−j
1 − α − β

=

1
1 − α − β

k
(cid:88)

j=0

(1 − α)jE (cid:2)(cid:107)∇f (xk−j)(cid:107)2(cid:3) .

Note that B def=

k
(cid:80)
l=0

βl ≤ 1

1−β . Putting all together we get

E (cid:2)(cid:107)vk(cid:107)2(cid:3) ≤

δ
1 − β

k
(cid:88)

(1 − α)lE (cid:2)(cid:107)∇f (xk−l)(cid:107)2(cid:3) +

σ2
M (1 − β)2

(cid:18) 3
αp

(cid:19)

− 2

l=0

+

3ζ 2
M (1 − β)2

(cid:19)

− 1

,

(cid:18) 1
αp

where δ def= 1 + 2
M

(cid:16) 1
αp

(cid:17) (cid:16)

− 1

1 + α

1−α−β

γ3L2β2
2(1 − β)3

E (cid:2)(cid:107)vk−1(cid:107)2(cid:3) ≤

γ3L2β2δ
2(1 − β)4

(cid:17)

, and as a result

k−1
(cid:88)

(1 − α)k−1−lE (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

l=0
γ3L2β2σ2
2M (1 − β)5

+

(cid:18) 3
αp

(cid:19)

− 2

+

3γ3L2β2ζ 2
2M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

.

To sum up, we have

270

E (cid:2)f (zk+1)(cid:3) ≤ E (cid:2)f (zk)(cid:3) −

γ
2(1 − β)

(cid:18)

1 −

(cid:19)

Lγω
1 − β

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:18) Lγ2α(ω − 1)

+

+

σ2
M

2(1 − β)2 +
(cid:18) 3
αp

− 2

γ3L2β2δ
2(1 − β)4

(cid:19) k−1
(cid:88)

(1 − α)k−1−lE (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

(cid:19) (cid:18) Lγ2

2(1 − β)2 +

l=0
γ3L2β2
2(1 − β)5

(cid:19)

+

3γ3L2β2ζ 2
2M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

.

Telescoping this inequality from 0 to K − 1, we get

E (cid:2)f (zK) − f (z0)(cid:3)
(cid:18) 3
αp
(cid:32)(cid:18) Lγα(ω − 1)

(cid:19) (cid:18) Lγ2

σ2
M

≤ K

− 2

K−2
(cid:88)

2(1 − β)2 +

+

+

+

≤ K

γ
2

γ
2

γ
2
σ2
M

+

γ
2

(1 − β)2 +

l=0

K−2
(cid:88)

1
1 − β
(cid:19)

(cid:18) Lγω
(1 − β)2 −
1
1 − β
(cid:19) (cid:18) Lγ2

l=0
(cid:18) Lγω
(1 − β)2 −
(cid:18) 3
αp
(cid:18) γ2L2β2δ
(1 − β)4α

K−1
(cid:88)

− 2

+

l=0

2(1 − β)2 +
Lγ

(cid:19)

γ3L2β2
2(1 − β)5

+ K

3γ3L2β2ζ 2
2M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

γ2L2β2δ
(1 − β)4

(cid:19) K−1
(cid:88)

(1 − α)k(cid:48)−1−l

(cid:33)

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

k(cid:48)=l+1

(cid:19)

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

E (cid:2)(cid:107)∇f (xK−1)(cid:107)2(cid:3)

(cid:19)

γ3L2β2
2(1 − β)5

+ K

3γ3L2β2ζ 2
2M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

(1 − β)2 (2ω − 1) −

(cid:19)

1
1 − β

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

It holds f (cid:63) ≤ f (zK) and our assumption on β implies that γ2L2β2δ
1−β ≤ − 1
1

2, so it all results in

(1−β)4α + Lγ

(1−β)2 (2ω − 1) −

1
K

K−1
(cid:88)

l=0

(cid:107)∇f (xl)(cid:107)2 ≤

4(f (z0) − f (cid:63))
γK
+ 2γ2 L2β2σ2
(1 − β)5M

+ 2γ

Lσ2
(1 − β)2M

(cid:18) 3
αp

(cid:19)

− 2

(cid:18) 3
αp

(cid:19)

− 2

+

3γ2L2β2ζ 2
M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

.

Since xK is sampled uniformly from {x0, . . . , xK−1}, the left-hand side is equal to E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3).
Also note that z0 = x0.

(cid:4)

Corollary F.10.2. If we set γ =

1−β2

KL(2ω−1)

√

2

and β such that

β2

(1−β)2α ≤ 4K(2ω−1)

δ

with

K > 1, then the error after K iterations is at most

271

1
√
K
1
K

+

(cid:18) 8L(2ω − 1)(f (x0) − f (cid:63))
1 − β2
(1 + β)4β2σ2
2(1 − β)(2ω − 1)αpM

(cid:18) 3
αp

+

(1 + β)σ2
(2ω − 1)αpM (1 − β)

(cid:18) 3
αp

(cid:19)(cid:19)

− 2

(cid:19)

− 2

+

1
K

3(1 + β)4β2ζ 2
4(1 − β)(2ω − 1)αpM

(cid:19)

− 1

.

(cid:18) 1
αp

Proof. Our choice of γ =

1−β2

KL(2ω−1)

√

2

implies that

β2
(1 − β)2α

≤

4K (2ω − 1)
δ

⇐⇒

β2
(1 − β)2α

≤

1 − β2 − 2Lγ (2ω − 1)
γ2L2δ

.

After that, it remains to plug-in γ =

1−β2

KL(2ω−1)

in

√

2

(cid:19)

− 2

+

4(f (z0) − f (cid:63))
γK

+

2γLσ2
(1 − β)2M

(cid:18) 3
αp

to get the desired result.

2γ2L2β2σ2
(1 − β)5M

(cid:18) 3
αp

(cid:19)

− 2

+

3γ2L2β2ζ 2
M (1 − β)5

(cid:18) 1
αp

(cid:19)

− 1

(cid:4)

F.11 Analysis of DIANA with α = 0 and h0

i = 0

F.11.1 Convergence Rate of TernGrad

Here we give the convergence guarantees for TernGrad and provide upper bounds for this
In the
method. The method coincides with Algorithm 16 for the case when p = ∞.
original paper [276] no convergence rate was given and we close this gap.

1 = h0

To maintain consistent notation we rewrite the TernGrad in notation which is close
to the notation we used for DIANA. Using our notation it is easy to see that TernGrad is
DIANA with h0
i = 0
for all i = 1, 2, . . . , n and k ≥ 1. What is more, this observation tells us that Lemma
F.5.1 holds for the iterates of TernGrad too. What is more, in the original paper [276]
the quantization parameter p was chosen as ∞. We generalize the method and we don’t
restrict our analysis only on the case of (cid:96)∞ sampling.

M = 0, α = 0 and p = ∞. Firstly, it means that hk

2 = . . . = h0

As it was in the analysis of DIANA our proofs for TernGrad work under Assump-

tion 6.3.1.

F.11.2 Technical lemmas

First of all, we notice that since TernGrad coincides with DIANA, having hk
i = 0, i, k ≥ 1,
α = 0 and p = ∞, all inequalities from Lemma F.5.1 holds for the iterates of TernGrad
as well because ∆k

i = gk

i and ˆ∆k

Lemma F.11.1. Assume γ ≤

L((M −1)αp+1). Then

i = ˆgk
i .
M αp

(cid:18)

2γµ

1 −

γL((M − 1)αp + 1)
2M αp

(cid:19)

≥ γµ.

(F.25)

272

Algorithm 16 DIANA with α = 0 and h0
TernGrad for p = ∞ (SGD)
1: Input: stepsizes (γk)k, initial vector x0, quantization parameter p ≥ 1, sizes of blocks

i = 0; equivalent to QSGD for p = 2 (1-bit)/

(dl)m

l=1, momentum parameter 0 ≤ β < 1, number of steps K

Broadcast xk to all workers
for i = 1, . . . , n do in parallel do

2: v0 = ∇f (x0)
3: for k = 1, 2, . . . , K − 1 do
4:
5:
6:
7:
8:
9:
10:
11:
12: end for

end for
ˆgk = 1
i=1 ˆgk
i
M
vk = βvk−1 + ˆgk
xk+1 = proxγkψ

Sample gk
Sample ˆgk

(cid:80)M

(cid:0)xk − γkvk(cid:1)

i such that E[gk
i ∼ Quantp(gk

i | xk] = ∇fi(xk)
i , (dl)m

l=1)

Proof. Since γ ≤

M αp

L((M −1)αp+1) we have

(cid:18)

2γµ

1 −

γL((M − 1)αp + 1)
2M αp

(cid:19)

(cid:18)

≥ 2γµ

1 −

(cid:19)

1
2

= γµ.

(cid:4)

Lemma F.11.2. Assume γ ≤
of f . Then

L(1+κ(1−αp)/(M αp)), where κ def= L

1

µ is the condition number

r ≥ γµ,

(F.26)

where r = 2µγ − γ2 (cid:16)

µL + L2(1−αp)

M αp

(cid:17)

.

Proof. Since γ ≤

1
L(1+κ(1−αp)/(M αp)) =

µM αp

µM αpL+L2(1−αp) we have

M αpr = γ (cid:0)2µM αp − γ (cid:0)µM αpL + L2(1 − αp)(cid:1)(cid:1) ≥ γµM αp,

whence r ≥ γµ.

Lemma F.11.3. Assume γ ≤

2M αp

(µ+L)(2+(M −2)αp). Then

(cid:4)

2γµ − γ2µ2

(cid:18)

1 +

(cid:19)

2(1 − αp)
M αp

≥ γµ.

(F.27)

Proof. Since γ ≤

2M αp

(µ+L)(2+(M −2)αp) we have

γµ ≤

2µM αp
(µ + L)(2 + (M − 2)αp)

≤

(µ + L)M αp
(µ + L)(2 + (M − 2)αp)

=

M αp
2 + (M − 2)αp

,

whence

273

2γµ − γ2µ2

(cid:18)

1 +

(cid:19)

2(1 − αp)
M αp

≥ 2γµ − γµ

M αp
2 + (M − 2)αp

(cid:18)

1 +

(cid:19)

2(1 − αp)
M αp

= 2γµ − γµ = γµ.

Lemma F.11.4. Assume that each function fi is L-smooth and ψ is a constant function.
Then for the iterates of Algorithm 16 with γk = γ we have

(cid:4)

E (cid:2)Θk+1(cid:3) ≤ E (cid:2)Θk(cid:3) +

+

γ2L
2M 2

(cid:18) γ2L
2
(cid:18) 1
αp

(cid:19)

− γ

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:19) M
(cid:88)

− 1

i=1

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) +

γ2Lσ2
2M

,

(F.28)

where Θk = f (xk) − f (x(cid:63)) and σ2 def= 1
M

(cid:80)M

i=1 σ2
i .

Proof. Since ψ is a constant function we have xk+1 = xk − γˆgk. Moreover, from the
L-smoothness of f we have

E (cid:2)Θk+1(cid:3) ≤ E (cid:2)Θk(cid:3) + E (cid:2)(cid:104)∇f (xk), xk+1 − xk(cid:105)(cid:3) +

(cid:107)xk+1 − xk(cid:107)2

= E (cid:2)Θk(cid:3) − γE (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +
(cid:18) γ2L
2

≤ E (cid:2)Θk(cid:3) +

γ2L
2
E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

− γ

(cid:19)

E

(F.7)

L
2
(cid:104)(cid:13)
(cid:13)ˆgk(cid:13)
(cid:13)

2(cid:105)

+

γ2L
2M 2

M
(cid:88)

m
(cid:88)

i=1

l=1

E (cid:2)(cid:107)gk

i (l)(cid:107)1(cid:107)gk

i (l)(cid:107)p − (cid:107)gk

i (l)(cid:107)2(cid:3) +

γ2L
2M 2

M
(cid:88)

i=1

σ2
i ,

where the ﬁrst equality follows from xk+1 − xk = ˆgk, E (cid:2)ˆgk | xk(cid:3) = ∇f (xk) and the
=
tower property of mathematical expectation. By deﬁnition αp(dl) = inf

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

x(cid:54)=0,x∈Rdl

(cid:32)

(cid:33)−1

sup
x(cid:54)=0,x∈Rdl

(cid:107)x(cid:107)1(cid:107)x(cid:107)p
(cid:107)x(cid:107)2

and αp = αp( max
l=1,...,m

dl) which implies

E (cid:2)(cid:107)gk

i (l)(cid:107)1(cid:107)gk

i (l)(cid:107)p − (cid:107)gk

i (l)(cid:107)2(cid:3) = E

(cid:20)
(cid:107)gk

i (l)(cid:107)2

(cid:18) (cid:107)gk

i (l)(cid:107)p

(cid:19)(cid:21)

− 1

i (l)(cid:107)1(cid:107)gk
(cid:107)gk
i (l)(cid:107)2
i (l)(cid:107)2(cid:3)
E (cid:2)(cid:107)gk

≤

≤

(cid:18) 1

(cid:19)

− 1

αp(dl)

(cid:19)

− 1

(cid:18) 1
αp

E (cid:2)(cid:107)gk

i (l)(cid:107)2(cid:3) .

Since (cid:107)gk

i (cid:107)2 =

m
(cid:80)
l=1

(cid:107)gk

i (l)(cid:107)2 we have

274

E (cid:2)Θk+1(cid:3) ≤ E (cid:2)Θk(cid:3) +

+

γ2L
2M 2

(cid:18) γ2L
2
(cid:18) 1
αp

(cid:19)

− γ

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:19) M
(cid:88)

− 1

i=1

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) +

γ2Lσ2
2M

,

(cid:4)

where σ2 = 1
M

(cid:80)M

i=1 σ2
i .

F.11.3 Non-convex analysis

Theorem F.11.5. Assume that ψ is constant and Assumption 6.6.1 holds. Also assume
L((M −1)αp+1) and xK is chosen randomly from {x0, . . . , xK−1}.
that f is L-smooth, γ ≤
Then

M αp

E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3) ≤

2
K

f (x0) − f (x(cid:63))
2 − γ L((M −1)αp+1)

M αp

(cid:16)

γ

(cid:17) +

γL (σ2 + (1 − αp)ζ 2)
M αp

.

Proof. Recall that we deﬁned Θk as f (xk) − f (x(cid:63)) in Lemma F.11.4. From (F.28) we
have

E[Θk+1] ≤ E[Θk] +

(cid:18)γ2L
2
(cid:18) 1
αp

(cid:19)

− γ

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:19) M
(cid:88)

− 1

i=1

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) +

γ2Lσ2
2M

.

γ2L
2M 2

+

Using variance decomposition

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) = E (cid:2)(cid:107)∇fi(xk)(cid:107)2(cid:3) + E (cid:2)(cid:107)gk

i − ∇f (xk)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)∇fi(xk)(cid:107)2(cid:3) + σ2
i ,

we get

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) ≤

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk)(cid:107)2(cid:3) + σ2

(1.3)

= E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

1
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − ∇f (xk)(cid:107)2(cid:3) + σ2

(6.17)

≤ E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) + ζ 2 + σ2.

Putting all together we obtain

275

E[Θk+1] ≤ E[Θk] +

(M − 1)αp + 1
M αp

(cid:19)

− γ

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) +

γ2Lσ2
2M αp

·

(cid:18) γ2L
2
γ2Lζ 2(1 − αp)
2M αp

+

.

(F.29)

Since γ ≤

M αp

L((M −1)αp+1) the factor

(cid:16) γ2L
2

· (M −1)αp+1
M αp

− γ

(cid:17)

is negative and therefore

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3) ≤

E (cid:2)Θk(cid:3) − E (cid:2)Θk+1(cid:3)
(cid:16)
1 − γ L((M −1)αp+1)

2M αp

γ

(cid:17) +

γL (σ2 + (1 − αp)ζ 2)
2M αp − γL((M − 1)αp + 1)

.

Telescoping the previous inequality from 0 to K −1 and using γ ≤

M αp

L((M −1)αp+1) we obtain

1
K

K−1
(cid:88)

l=0

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3) ≤

2
K

γ

E [Θ0] − E (cid:2)ΘK(cid:3)
(cid:16)
2 − γ L((M −1)αp+1)

M αp

(cid:17) +

γL (σ2 + (1 − αp)ζ 2)
M αp

.

It remains to notice that the left-hand side is just E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3), ΘK ≥ 0 and E [Θ0] =
(cid:4)
f (x0) − f (x(cid:63)).

Corollary F.11.6. If we choose γ =

M αp

L((M −1)αp+1)

√

then the rate we get is

K

2
√
K

L

(M − 1)αp + 1
M αp

(cid:0)f (x0) − f (x(cid:63))(cid:1) +

1
√
K

σ2 + (1 − αp)ζ 2
(1 + (M − 1)αp)

.

Proof. Our choice of γ =

M αp

L((M −1)αp+1)

√

≤

K

1. After that it remains to notice that for our choice of γ =

M αp

L((M −1)αp+1) implies that 2−γ L((M −1)αp+1)
M αp
we have

M αp

√

L((M −1)αp+1)

K

2
K

(cid:16)

γ

f (x0) − f (x(cid:63))
2 − γ L((M −1)αp+1)

(cid:17) +

γL (σ2 + (1 − αp)ζ 2)
M αp

≤

2
√
K

L

M αp
(M − 1)αp + 1
M αp

(cid:0)f (x0) − f (x(cid:63))(cid:1) +

1
√
K

σ2 + (1 − αp)ζ 2
(1 + (M − 1)αp)

.

≥

(cid:4)

F.11.4 Momentum version

Theorem F.11.7. Assume that f is L-smooth, ψ ≡ const, α = 0, hi = 0 and Assump-
tion 6.6.1 holds. Choose β < 1 and γ < 1−β2
, where

(1−β)3 ≤ 1−β2−2Lγω

2Lω such that

γ2L2ω

β2

ω def= M −1

M + 1
M αp

and sample xK uniformly from {x0, . . . , xK−1}. Then

276

E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3) ≤

4(f (z0) − f (cid:63))
γK

+ 2γ

Lσ2

αpM (1 − β)2 + 2γ2

L2β2σ2
(1 − β)5αpM

+ 2γ2 L2β2(1 − αp)ζ 2
2(1 − β)5αpM

.

Proof. The main idea of the proof is to ﬁnd virtual iterates zk whose recursion would
satisfy zk+1 = zk − γ
1−β ˆgk. Having found it, we can prove convergence by writing a
recursion on f (zk). One possible choice is deﬁned below:

zk def= xk −

γβ
1 − β

vk−1,

(F.30)

where for the edge case k = 0 we simply set v−1 = 0 and z0 = x0. Although zk is just
a slight perturbation of xk, applying smoothness inequality (6.9) to it produces a more
convenient bound than the one we would have if used xk. But ﬁrst of all, let us check
that we have the desired recursion for zk+1:

zk+1

(F.30)
= xk+1 −

vk

γβ
1 − β
γ
vk
1 − β
γβ
1 − β
γ
1 − β

ˆgk.

= xk −

= xk −

(F.30)
= zk −

vk−1 −

γ
1 − β

ˆgk

Now, it is time to apply smoothness of f :

E[f (zk+1)] ≤ E

(cid:20)
f (zk) + (cid:10)∇f (zk), zk+1 − zk(cid:11) +

(cid:21)

(cid:107)zk+1 − zk(cid:107)2

L
2

(F.30)
= E

(cid:20)
f (zk) −

γ
1 − β

(cid:10)∇f (zk), ˆgk(cid:11) +

Lγ2

2(1 − β)2 (cid:107)ˆgk(cid:107)2

(cid:21)

. (F.31)

Under our special assumption inequality (F.11) simpliﬁes to

E (cid:2)(cid:107)ˆgk(cid:107)2 | xk(cid:3) ≤ (cid:107)∇f (xk)(cid:107)2 +

(6.17)
≤ (cid:107)∇f (xk)(cid:107)2 +

(cid:18) 1
αp
(cid:18) 1
αp

− 1

(cid:19) 1
M 2
(cid:19) 1
M

M
(cid:88)

i=1

(cid:107)∇fi(xk)(cid:107)2 +

σ2
αpM

− 1

(cid:107)∇f (xk)(cid:107)2 +

(cid:18) 1
αp

− 1

(cid:19) ζ 2
M

+

σ2
αpM

.

The scalar product in (F.31) can be bounded using the fact that for any vectors a and b

one has −2 (cid:104)a, b(cid:105) = (cid:107)a − b(cid:107)2 − (cid:107)a(cid:107)2 − (cid:107)b(cid:107)2. In particular,

277

(cid:0)(cid:107)∇f (xk) − ∇f (zk)(cid:107)2 − (cid:107)∇f (xk)(cid:107)2 − (cid:107)∇f (zk)(cid:107)2(cid:1)

−

2γ
1 − β

(cid:10)∇f (zk), ∇f (xk)(cid:11) =

≤

=

γ
1 − β
γ
1 − β
γ3L2β2
(1 − β)3 (cid:107)vk−1(cid:107)2 −

(cid:0)L2(cid:107)xk − zk(cid:107)2 − (cid:107)∇f (xk)(cid:107)2(cid:1)

γ
1 − β

(cid:107)∇f (xk)(cid:107)2.

The next step is to come up with an inequality for E (cid:2)(cid:107)vk(cid:107)2(cid:3). Since we initialize v−1 = 0,
one can show by induction that

vk =

k
(cid:88)

l=0

βlˆgk−l.

Deﬁne B def= (cid:80)k

l=0 βl = 1−βk+1

1−β . Then, by Jensen’s inequality

E[(cid:107)vk(cid:107)2] = B2E

k
(cid:88)

≤ B2

βl
B

E (cid:2)(cid:107)ˆgk−l(cid:107)2(cid:3)

k
(cid:88)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

βl
B

ˆgk−l

2(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

l=0
(cid:18)(cid:18) M − 1

l=0
(cid:19)

+

1
M αp

M

≤ B

k
(cid:88)

l=0

βl

E (cid:2)(cid:107)∇f (xk−l)(cid:107)2(cid:3) +

(cid:18) 1
αp

− 1

(cid:19) ζ 2
M

+

(cid:19)

.

σ2
αpM

Note that B ≤ 1

1−β , so

γ3L2β2
2(1 − β)3

E (cid:2)(cid:107)vk−1(cid:107)2(cid:3) ≤

γ3L2β2
2(1 − β)5

σ2
αpM

+

γ3L2β2
2(1 − β)5

(1 − αp)ζ 2
αpM

+ ω

γ3L2β2
2(1 − β)4

k−1
(cid:88)

l=0

βk−1−lE (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

with ω def= M −1

M + 1
M αp

. We, thus, obtain

E[f (zk+1)] ≤ E[f (zk)] −

γ
2(1 − β)

+

+

Lγ2σ2
2M αp(1 − β)2 +
γ3L2β2(1 − αp)ζ 2
2(1 − β)5αpM

E (cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3)

(cid:19)

(cid:18)

1 −

Lγω
1 − β
γ3L2β2σ2
2(1 − β)5αpM

+ ω

γ3L2β2
2(1 − β)4

k−1
(cid:88)

l=0

βk−1−lE (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3) .

Telescoping this inequality from 0 to K − 1, we get

278

E[f (zK) − f (z0)] ≤ K

(cid:18)

Lγ2σ2
2αpM (1 − β)2 +
(cid:32)
k−1
γ2L2β2
(cid:88)
(1 − β)4

ω

K−2
(cid:88)

k(cid:48)=l+1
(cid:19)
1
1 − β

l=0
(cid:18) Lγω
(1 − β)2 −
(cid:18)
Lγ2σ2
2αpM (1 − β)2 +
(cid:18)
γ2L2β2
(1 − β)5 +

K−1
(cid:88)

ω

l=0

+

+

γ
2

γ
2

≤ K

+

γ
2

γ3L2β2σ2
2(1 − β)5αpM

+

γ3L2β2(1 − αp)ζ 2
2(1 − β)5αpM

(cid:19)

βk(cid:48)−1−l +

Lγω
(1 − β)2 −

1
1 − β

(cid:33)

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3)

E (cid:2)(cid:107)∇f (xK−1)(cid:107)2(cid:3)

γ3L2β2σ2
2(1 − β)5αpM

+

γ3L2β2(1 − αp)ζ 2
2(1 − β)5αpM

(cid:19)

Lγω
(1 − β)2 −

(cid:19)

1
1 − β

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3) .

It holds f (cid:63) ≤ f (zK) and our assumption on β implies that ω γ2L2β2
so it all results in

(1−β)5 + Lγω

(1−β)2 − 1

1−β ≤ − 1
2,

1
K

K−1
(cid:88)

l=0

E (cid:2)(cid:107)∇f (xl)(cid:107)2(cid:3) ≤

4(f (z0) − f (cid:63))
γK

+ 2γ

Lσ2

αpM (1 − β)2 + 2γ2

L2β2σ2
(1 − β)5αpM

+ 2γ2 L2β2(1 − αp)ζ 2
2(1 − β)5αpM

.

Since xK is sampled uniformly from {x0, . . . , xK−1}, the left-hand side is equal to E (cid:2)(cid:107)∇f (xK)(cid:107)2(cid:3).
Finally, note that z0 = x0 and f (z0) = f (x0).

(cid:4)

Corollary F.11.8. If we set γ = 1−β2
KLω
2
(1−β)3 ≤ 4Kω with K > 1, then the error after K iterations is at most

, where ω = M −1

M + 1
M αp

β2

√

, and β such that

1
√
K

(cid:18) 8Lω(f (x0) − f (cid:63))
1 − β2

+

(1 + β)σ2
ωαpM (1 − β)

(cid:19)

+

1
K

(1 + β)4β2σ2
2(1 − β)ωαpM

+

1
K

(1 + β)4β2(1 − αp)ζ 2
2(1 − β)ωαpM

.

Proof. Our choice of γ = 1−β2
KLω

√

2

implies that

β2
(1 − β)3 ≤

1 − β2 − 2Lγω
γ2L2ω

⇐⇒

β2

(1 − β)3 ≤ 4kω.

Now it remains to plug-in γ = 1−β2
kLω
2γ2 L2β2σ2
2(1−β)ωαpM to get the desired result.

(1−β)5αpM + 1

(1+β)4β2(1−αp)ζ2

K

√

2

in the expression 4(f (z0)−f (cid:63))

γK

+ 2γ

Lσ2
αpM (1−β)2 +
(cid:4)

279
F.11.5 Strongly convex analysis

Theorem F.11.9. Assume that each function fi is µ-strongly convex and L-smooth.
Choose stepsizes γk = γ > 0 satisfying

γ ≤

2M αp
(µ + L)(2 + (M − 2)αp)

.

(F.32)

If we run Algorithm 16 for K iterations with γk = γ, then

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ (1 − γµ)K(cid:107)x0 − x(cid:63)(cid:107)2 +

(cid:32)

γ
µ

σ2
M αp

+

2(1 − αp)
M 2αp

(cid:33)

(cid:107)h(cid:63)

i (cid:107)2

,

M
(cid:88)

i=1

where σ2 def= 1
M

(cid:80)M

i=1 σ2

i and h(cid:63)

i = ∇fi(x(cid:63)).

Proof. In the similar way as we did in the proof of Theorem 6.5.4 one can derive inequal-
ity (F.12) for the iterates of TernGrad:

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

+

γ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) +

E (cid:2)Ψ(gk

i )(cid:3) +

γ2σ2
M

.

γ2
M 2

M
(cid:88)

i=1

(cid:33)−1

sup
x(cid:54)=0,x∈Rdl

(cid:107)x(cid:107)1(cid:107)x(cid:107)p
(cid:107)x(cid:107)2

and αp = αp( max
l=1,...,m

dl)

By deﬁnition αp(dl) = inf

x(cid:54)=0,x∈Rdl

(cid:107)x(cid:107)2
(cid:107)x(cid:107)1(cid:107)x(cid:107)p

=

which implies

(cid:32)

E (cid:2)Ψl(gk

i )(cid:3) = E (cid:2)(cid:107)gk
(cid:18) 1

≤

i (l)(cid:107)p − (cid:107)gk
i (l)(cid:107)1(cid:107)gk
(cid:19)

− 1

E (cid:2)(cid:107)gk

i (l)(cid:107)2(cid:3) ≤

i (l)(cid:107)2(cid:3) = E
(cid:18) 1
αp

αp(dl)

(cid:20)

(cid:107)gk

i (l)(cid:107)2
(cid:19)

− 1

E (cid:2)(cid:107)gk

(cid:18) (cid:107)gk

i (l)(cid:107)1(cid:107)gk
(cid:107)gk
i (l)(cid:107)2
i (l)(cid:107)2(cid:3) .

i (l)(cid:107)p

(cid:19)(cid:21)

− 1

Moreover,

(cid:107)gk

i (cid:107)2 =

m
(cid:88)

l=1

(cid:107)gk

i (l)(cid:107)2, Ψ(gk

i ) =

m
(cid:88)

l=1

Ψl(gk

i ).

This helps us to get the following inequality

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3) +

γ2σ2
M

+

γ2
M

M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) +

γ2
M 2

(cid:18) 1
αp

(cid:19) M
(cid:88)

− 1

i=1

E (cid:2)(cid:107)gk

i (cid:107)2(cid:3) .

Using tower property of mathematical expectation and the fact that

E (cid:2)(cid:107)gk

i (cid:107)2 | xk(cid:3) = E (cid:2)(cid:107)gk

i − ∇fi(xk)(cid:107)2 | xk(cid:3) + (cid:107)∇fi(xk)(cid:107)2 ≤ σ2

i + (cid:107)∇fi(xk)(cid:107)2,

we obtain

280

E[(cid:107)gk

i (cid:107)2] ≤ E[(cid:107)∇fi(xk)(cid:107)2] + σ2

i ≤ 2E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3) + 2(cid:107)h(cid:63)

i (cid:107)2 + σ2
i ,

where the last inequality follows from the fact that for all x, y ∈ RM the inequality
(cid:107)x + y(cid:107)2 ≤ 2 ((cid:107)x(cid:107)2 + (cid:107)y(cid:107)2) holds. Putting all together we have

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) − 2γE (cid:2)(cid:10)∇f (xk) − h(cid:63), xk − x(cid:63)(cid:11)(cid:3)

(cid:18)

1 +

+

γ2
M

2(1 − αp)
M αp

(cid:19) M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3)

+

2γ2(1 − αp)
M 2αp

M
(cid:88)

i=1

(cid:107)h(cid:63)

i (cid:107)2 +

γ2σ2
M αp

.

Using the splitting trick (F.19) we get

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤

(cid:18)

1 −

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3)

2γµL
µ + L
(cid:18)

(cid:19)

(cid:18)

γ2

1 +

+

1
M

(cid:19)

2(1 − αp)
M αp

−

2γ
µ + L

(cid:19) M
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − h(cid:63)

i (cid:107)2(cid:3)

+

2γ2(1 − αp)
M 2αp

M
(cid:88)

i=1

(cid:107)h(cid:63)

i (cid:107)2 +

γ2σ2
M αp

.

(F.33)

(cid:16)

γ2 (cid:16)

(cid:17)

(cid:17)

2M αp

(µ+L)(2+(M −2)αp) the term

is nonnegative. More-
Since γ ≤
i , xk − x(cid:63)(cid:105).
over, since fi is µ–strongly convex, we have µ(cid:107)xk − x(cid:63)(cid:107)2 ≤ (cid:104)∇fi(xk) − h(cid:63)
Applying the Cauchy-Schwarz inequality to further bound the right-hand side, we get the
inequality µ(cid:107)xk − x(cid:63)(cid:107) ≤ (cid:107)∇fi(xk) − h(cid:63)
i (cid:107). Using these observations, we can get rid of the
second term in the (F.33) and absorb it with the ﬁrst term, obtaining

1 + 2(1−αp)
M αp

− 2γ
µ+L

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤

(cid:18)

1 − 2γµ + γ2µ2

(cid:18)

1 +

2(1 − αp)
M αp

(cid:19)(cid:19)

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3)

+

2γ2(1 − αp)
M 2αp

M
(cid:88)

i=1

(cid:107)h(cid:63)

i (cid:107)2 +

γ2σ2
M αp

(F.27)

≤ (1 − γµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + γ2

(cid:32)

σ2
M αp

+

2(1 − αp)
M 2αp

(cid:33)

(cid:107)h(cid:63)

i (cid:107)2

.

M
(cid:88)

i=1

281

Finally, unrolling the recurrence leads to

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) ≤ (1 − γµ)k(cid:107)x0 − x(cid:63)(cid:107)2 +

≤ (1 − γµ)k(cid:107)x0 − x(cid:63)(cid:107)2 +

= (1 − γµ)k(cid:107)x0 − x(cid:63)(cid:107)2 +

k−1
(cid:88)

(1 − γµ)lγ2

l=0
∞
(cid:88)

(1 − γµ)lγ2

(cid:32)

(cid:32)

σ2
M αp

σ2
M αp

+

+

l=0
(cid:32)

γ
µ

σ2
M αp

+

2(1 − αp)
M 2αp

M
(cid:88)

2(1 − αp)
M 2αp

2(1 − αp)
M 2αp
(cid:33)

(cid:107)h(cid:63)

i (cid:107)2

.

(cid:33)

(cid:33)

(cid:107)h(cid:63)

i (cid:107)2

(cid:107)h(cid:63)

i (cid:107)2

M
(cid:88)

i=1

M
(cid:88)

i=1

i=1

(cid:4)

F.11.6 Decreasing stepsize

Theorem F.11.10. Assume that f is L-smooth, µ-strongly convex and we have access
µk+θ with some θ ≥ (µ+L)(2+(M −2)αp)
to its gradients with bounded noise. Set γk = 2
.
After K iterations of Algorithm 16 we have

2M αp

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

(cid:40)

max

(cid:107)x0 − x(cid:63)(cid:107)2,

1
ηK + 1

(cid:32)

4
µθ

σ2
M αp

+

2(1 − αp)
M 2αp

M
(cid:88)

i=1

(cid:33)(cid:41)

(cid:107)h(cid:63)

i (cid:107)2

,

(cid:80)M

where η def= µ

θ , σ2 = 1

i = ∇fi(x(cid:63)).
Proof. To get a recurrence, let us recall an upper bound we have proved before in Theo-
rem F.11.9:

i and h(cid:63)

i=1 σ2

M

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) ≤ (1 − γkµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + γ2

k

(cid:32)

σ2
M αp

+

2(1 − αp)
M 2αp

(cid:33)

(cid:107)h(cid:63)

i (cid:107)2

.

M
(cid:88)

i=1

Having that, we can apply Lemma F.8.1 to the sequence E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3). The constants
(cid:80)M
for the Lemma are: ν =

and

i=1 (cid:107)h(cid:63)

i (cid:107)2(cid:17)

+ 2(1−αp)
M 2αp

(cid:16) σ2
M αp

(cid:40)

C = max

(cid:107)x0 − x(cid:63)(cid:107)2,

(cid:32)

4
µθ

σ2
M αp

+

2(1 − αp)
M 2αp

M
(cid:88)

i=1

(cid:33)(cid:41)

(cid:107)h(cid:63)

i (cid:107)2

.

Corollary F.11.11. If we choose θ = (µ+L)(2+(M −2)αp)
ε we need at most

2M αp

(cid:4)
, then to achieve E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

(cid:32)

O

κ(1 + M αp)
M αp

(cid:40)

max

(cid:107)x0 − x(cid:63)(cid:107)2,

M αp
(1 + M αp)µL

(cid:32)

σ2
M αp

+

1 − αp
M 2αp

(cid:107)h(cid:63)

i (cid:107)2

(cid:33)(cid:41)

(cid:33)

1
ε

M
(cid:88)

i=1

iterations, where κ def= L

µ is the condition number of f .

Proof. If θ = (µ+L)(2+(M −2)αp)
(cid:17)

(cid:16) M αp

2M αp

= Θ

µθ =
. Putting all together and using the bound from Theorem F.11.10 we get
(cid:4)

, then η = Θ

κ(1+M αp)

Θ
µL(1+M αp)
the desired result.

and 1

(cid:16) M αp

(cid:17)

282
(cid:16) L(1+M αp)
M αp

(cid:17)

F.11.7 Performance of DIANA, QSGD and TernGrad on the Rosen-

brock function

In Figure F.1 we illustrate the workings of DIANA, QSGD and TernGrad with 2 workers
on the 2-dimensional (non-convex) Rosenbrock function:

f (x, y) = (x − 1)2 + 10(y − x2)2,

decomposed into average of f1 = (x + 16)2 + 10(y − x2)2 + 16y and f2 = (x − 18)2 +
10(y − x2)2 − 16y + const. Each worker has access to its own piece of the Rosenbrock
function with parameter a = 1 and b = 10. The gradients used are not stochastic, and
we use 1-bit version of QSGD, so it also coincides with QGD in that situation. For all
methods, its parameters were carefully tuned except for momentum and α, which were
simply set to 0.9 and 0.5 correspondingly. We see that DIANA vastly outperforms the
competing methods.

Figure F.1: Illustration of the workings of DIANA, QSGD and TernGrad on the Rosenbrock
function.

F.11.8 Logistic regression

We consider the logistic regression problem with (cid:96)2 and (cid:96)1 penalties for mushrooms dataset
from LIBSVM. In our experiments we use (cid:96)1-penalty coeﬃcient l1 = 2·10−3 and (cid:96)2-penalty
coeﬃcient l2 = L
M . The coeﬃcient l1 is adjusted in order to have sparse enough solution
(≈ 20% nonzero values). The main goal of this series of experiment is to compare the
optimal parameters for (cid:96)2 and (cid:96)∞ quantization.

321012420246DianaQSGD/QGDTernGradOptimumWhat α is better to choose?

283

We run DIANA with zero momentum (β = 0) and obtain in our experiments that, actually,
it is not important what α to choose for both (cid:96)2 and (cid:96)∞ quantization. The only thing
that we need to control is that α is small enough.

What is the optimal block-size?

Since α is not so important, we run DIANA with α = 10−3 and zero momentum (β =
0) for diﬀerent block sizes (see Figure F.2). For the choice of (cid:96)∞ quantization in our
experiments it is always better to use full quantization. In the case of (cid:96)2 quantization it
if the regularization is big then optimal block-size ≈ 25
depends on the regularization:
(dimension of the full vector of parameters is d = 112), but if the regularization is small
it is better to use small block sizes.

Table F.1: Approximate optimal number of blocks for diﬀerent dataset and conﬁgurations.
Momentum equals zero for all experiments.

Dataset

N

d

Number of workers Quantization Optimal block size

mushrooms
mushrooms
mushrooms
mushrooms
a5a
a5a
a5a
a5a

8124
8124
8124
8124
6414
6414
6414
6414

112
112
112
112
122
122
122
122

10
10
20
20
10
10
20
20

(cid:96)2
(cid:96)∞
(cid:96)2
(cid:96)∞
(cid:96)2
(cid:96)∞
(cid:96)2
(cid:96)∞

25
112
25
112
25
112
25
112

DIANA vs. QSGD vs. TernGrad vs. DQGD

F.11.9 MPI: broadcast, reduce and gather

In our experiments, we are running 4 MPI processes per physical node. Nodes are con-
nected by Cray Aries High Speed Network.

We utilize 3 MPI collective operations, Broadcast, Reduce and Gather. When im-
plementing DIANA, we could use P2P communication, but based on our experiments,
we found that using Gather to collect data from workers signiﬁcantly outperformed P2P
communications.

In Figure F.4 we show the duration of diﬀerent communications for various MPI pro-
cesses and message length. Note that Gather 2bit do not scale linearly (as would be
It turns out, we are not the only one who observed such a weird behavior
expected).
when using cray MPI implementation (see [199] for a nice study obtained by a team from
Argonne National Laboratory). To correct for the unexpected behavior, we have per-

284

Figure F.2: Comparison of the inﬂuence of the block sizes on convergence for ”mush-
rooms” (ﬁrst row), ”a5a” (second row) datasets.

formed MPI Gather multiple times on shorter vectors, such that the master node obtained
all data, but in much faster time (see Multi-Gather 2bit).

0102030405060Time (in seconds)104103102101100Functional accuracyNumber of workers = 10, p = 2block size=6block size=25block size=1120102030405060Time (in seconds)104103102101100Functional accuracyNumber of workers = 10, p = infblock size=6block size=25block size=1120102030405060Time (in seconds)104103102101100Functional accuracyNumber of workers = 20, p = 2block size=6block size=25block size=1120102030405060Time (in seconds)104103102101100Functional accuracyNumber of workers = 20, p = infblock size=6block size=25block size=112050100150200250Time (in seconds)103102101100Functional accuracyNumber of workers = 10, p = 2block size=6block size=25block size=122050100150200250Time (in seconds)103102101100Functional accuracyNumber of workers = 10, p = infblock size=6block size=25block size=122050100150200250Time (in seconds)103102101100Functional accuracyNumber of workers = 20, p = 2block size=6block size=25block size=122050100150200250Time (in seconds)103102101100Functional accuracyNumber of workers = 20, p = infblock size=6block size=25block size=122285

Figure F.3: Typical communication cost using broadcast, reduce and gather for 64 and
32 FP using 4 (solid) resp 128 (dashed) MPI processes. See suppl. material for details
about the network.

Figure F.4: Time to communicate a vectors with diﬀerent lengths for diﬀerent methods
as a function of # of MPI processes. One can observe that Gather 2bit is not having
nice scaling. We also show that the proposed Multi-Gather communication still achieves
a nice scaling when more MPI processes are used.

F.11.10 Performance of GPU

In Table F.2, we list the DNN networks that we have experimented with in this chapter.
Figure F.7 shows the performance of a single P100 GPU for diﬀerent batch size, DNN

network and operation.

10121416182022242628log2(d)106105104103102101100101log10(time)Communication 4 vs. 128 MPI processestypeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32Gather 2bitMPI4128  4  8 16 32 64128MPI102101time [seconds]Message length: 224typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32Gather 2bit  4  8 16 32 64128MPI102101100time [seconds]Message length: 225typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32Gather 2bitMulti-Gather 2bit  4  8 16 32 64128MPI102101100time [seconds]Message length: 226typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32Gather 2bitMulti-Gather 2bit  4  8 16 32 64128MPI101100time [seconds]Message length: 227typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32Gather 2bitMulti-Gather 2bitTable F.2: Deep Neural Networks used in the experiments.

286

Model

d

# classes

Input shape

LeNet
CifarNet
alexnet v2
vgg a

3.2M
1.7M
50.3M
132.8M

10
10
1,000
1,000

28 × 28 × 3
32 × 32 × 3
224 × 224 × 3
224 × 224 × 3

F.11.11 DIANA vs. TernGrad, SGD and QSGD

In Figure F.8 we compare the performance of DIANA vs. doing an MPI reduce operation
with 32bit ﬂoats. The computing cluster had Cray Aries High Speed Network. However,
for DIANA we used 2bit per dimension, we have experienced an weird scaling behaviour,
which was documented also in[199]. In our case, this aﬀected speed for alexnet and vgg a
beyond 64 or 32 MPI processes respectively. For more detailed experiments, see Sec-
tion F.11.9. In order to improve the speed of Gather, we impose a Multi-Gather strategy,
when we call Gather multiple-times on shorter vectors. This signiﬁcantly improved the
communication cost of Gather (see Figure F.5) and leads to much nicer scaling – see
green bars – DIANA-MultiGather in Figure F.8).

√

In the next experiments, we run QSGD [5], TernGrad [276], SGD with momen-
tum and DIANA on MNIST dataset and CIFAR-10 dataset for 3 epochs. We have
selected 8 workers and run each method with stepsize chosen from {0.1, 0.2, 0.05}.
For QSGD, DIANA and TernGrad, we also tried various quantization bucket sizes in
{32, 128, 512}. For QSGD we have chosen 2, 4, 8 quantization levels. For DIANA we
quantization bucket sizes } and have selected initial h = 0.
have chosen α ∈ {0, 1.0/
For DIANA and SGD we also run a momentum version, with a momentum parameter
in {0, 0.95, 0.99}. For DIANA we also run with two choices of norm (cid:96)2 and (cid:96)∞. For
each experiment we have selected softmax cross entropy loss. MNIST-Convex is a simple
DNN with no hidden layer, MNIST-DNN is a convolutional NN described here https:
//github.com/floydhub/mnist/blob/master/ConvNet.py and CIFAR-10-DNN is a
convolutional DNN described here https://github.com/kuangliu/pytorch-cifar/
blob/master/models/lenet.py. In Figure F.9 we show the best runs over all the pa-
rameters for all the methods. For MNIST-Convex SGD and DIANA makes use of the
momentum and dominate all other algorithms. For MNIST-DNN situation is very similar.
For CIFAR-10-DNN both DIANA and SGD have signiﬁcantly outperform other methods.

In Figure F.10 show the evolution of sparsity of the quantized gradient for the 3 prob-
lems and DIANA, QSGD and TernGrad. For MNIST-DNN, it seems that the quantized
gradients are becoming sparser as the training progresses.

F.11.12 Computational Cost

287

Figure F.5: The duration of communication for MPI Broadcast, MPI Reduce and MPI
Gather. We show how the communication time depends on the size of the vector in Rd
(x-axis) for various # of MPI processes. In this experiment, we have run 4 MPI processes
per computing node. For Broadcast and Reduce we have used a single precision ﬂoating
point number. For Gather we used 2bits per dimension. For longer vectors and large
number of MPI processes, one can observe that Gather has a very weird scaling issue. It
turned out to be some weird behaviour of Cray-MPI implementation.

10121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 4typeBroadcast FP32Reduce FP32Gather 2bit10121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 8typeBroadcast FP32Reduce FP32Gather 2bit10121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 16typeBroadcast FP32Reduce FP32Gather 2bitMulti-Gather 2bit10121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 32typeBroadcast FP32Reduce FP32Gather 2bitMulti-Gather 2bit10121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 64typeBroadcast FP32Reduce FP32Gather 2bitMulti-Gather 2bit10121416182022242628log2(d)104103102101100time [seconds]$# of MPI processes: 128typeBroadcast FP32Reduce FP32Gather 2bitMulti-Gather 2bit288

Figure F.6: The duration of communication for MPI Broadcast, MPI Reduce for single
precision (FP32) and double precision (FP64) ﬂoating numbers. We show how the com-
munication time depends on the size of the vector in Rd (x-axis) for various # of MPI
processes.
In this experiment, we have run 4 MPI processes per computing node. We
have used Cray implementation of MPI.

10121416182022242628log2(d)106105104103102101100101time [seconds]$# of MPI processes: 4typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP3210121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 8typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP3210121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 16typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP3210121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 32typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP3210121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 64typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP3210121416182022242628log2(d)105104103102101100101time [seconds]$# of MPI processes: 128typeBroadcast FP64Broadcast FP32Reduce FP64Reduce FP32289

Figure F.7: The performance (images/second) of NVIDIA Tesla P100 GPU on 4 diﬀerent
problems as a function of batch size. We show how diﬀerent choice of batch size aﬀects
the speed of function evaluation and gradient evaluation. For vgg a, we have run out of
memory on GPU for batch size larger than 128 (gradient evaluation) and 256 for function
evaluation. Clearly, this graph suggest that choosing small batch size leads to small
utilization of GPU. Note that using larger batch size do not necessary reduce the training
process.

  16  32  64 128 256 51210242048batch size103104105images / secondmodelvgg aLeNetalexnet v2CifarNetoperationgradientfunction value290

Figure F.8: Comparison of performance (images/second) for various number of GPUs/MPI
processes and sparse communication DIANA (2bit) vs. Reduce with 32bit ﬂoat (FP32).
We have run 4 MPI processes on each node. Each MPI process is using single P100 GPU.
Note that increasing MPI from 4 to 8 will not bring any signiﬁcant improvement for FP32,
because with 8 MPI processes, communication will happen between computing nodes and
will be signiﬁcantly slower when compare to the single node communication with 4MPI
processes.

  4  8 16 32 64128MPI050000100000150000200000250000300000images/secondLeNet, batch size: 64methodFP32Diana  4  8 16 32 64128MPI050000100000150000200000250000images/secondCifarNet, batch size: 32methodFP32Diana  4  8 16 32 64128MPI050001000015000200002500030000images/secondalexnet v2, batch size: 128methodFP32DianaDiana - MultiGather  4  8 16 32 64128MPI050010001500200025003000images/secondvgg a, batch size: 32methodFP32DianaDiana - MultiGather291

Figure F.9: Evolution of training and testing accuracy for 3 diﬀerent problems, using 4
algorithms: DIANA, SGD, QSGD and TernGrad. We have chosen the best runs over all
tested hyper-parameters.

0.00.51.01.52.02.53.0epoch0.10.20.30.40.50.60.70.80.9Train AccuracyMnist-ConvexmethodSGDDIANATernGradQSGD0.00.51.01.52.02.53.0epoch0.20.40.60.8Train AccuracyMnist-DNNmethodSGDDIANAQSGDTernGrad0.00.51.01.52.02.53.0epoch0.10.20.30.40.5Train AccuracyCifar10-DNNmethodQSGDTernGradSGDDIANA0.00.51.01.52.02.53.0epoch0.10.20.30.40.50.60.70.80.9Test AccuracyMnist-ConvexmethodSGDDIANATernGradQSGD0.00.51.01.52.02.53.0epoch0.20.40.60.8Test AccuracyMnist-DNNmethodSGDDIANAQSGDTernGrad0.00.51.01.52.02.53.0epoch0.10.20.30.40.5Test AccuracyCifar10-DNNmethodQSGDTernGradSGDDIANA292

Figure F.10: Evolution of sparsity of the quantized gradient for 3 diﬀerent problems and
3 algorithms.

Figure F.11: Comparison of a time needed to update weights after a reduce vs. the time
needed to update the weights when using a sparse update from DIANA using 4-128 MPI
processes and 10% sparsity.

0.00.51.01.52.02.53.0epoch0.10.20.30.40.50.6densityMnist-ConvexmethodDIANA-2DIANA-QSGD 2bitQSGD 4bitQSGD 8bitTernGrad0.00.51.01.52.02.53.0epoch0.050.100.150.200.250.300.350.40densityMnist-DNNmethodDIANA-2DIANA-QSGD 2bitQSGD 4bitQSGD 8bitTernGrad0.00.51.01.52.02.53.0epoch0.050.100.150.200.25densityCifar10-DNNmethodDIANA-2DIANA-QSGD 2bitQSGD 4bitQSGD 8bitTernGrad101214161820222426log2(d)107105103101101time [seconds]operFP32Bin 4MPI ProcessesBin 8MPI ProcessesBin 16MPI ProcessesBin 32MPI ProcessesBin 64MPI ProcessesBin 128MPI Processes293

Appendix G

Appendix for Chapter 7

G.1 Applications

In this section we list a number of selected applications for our method:

• Compressed sensing [36].

• Total Generalized Variance (TGV) for image denoising [33].

• Decentralized optimization over networks [180].

• Support-vector machine [56].

• Dantzig selector [37].

• Group-Lasso [288].

• Network utility maximization.

• Square-root Lasso [18].

• (cid:96)1 trend ﬁltering [119].

• Convex relaxation of unsupervised image matching and object discovery [269].

In the rest of this section we formulate some of them explicitly. A summary of the

mapping of these problems to the structure of problem (7.1) is provided in Table G.1.

G.1.1 Constrained optimization

Let Cj ⊆ Rd be closed convex sets with a non-empty intersection and consider the con-
strained composite optimization problem

min f (x) + ψ(x)

subject to

x ∈ ∩m

j=1Cj.

If we let gj ≡ χCj be the characteristic function of Cj, deﬁned as follows: χCj (x) = 0 for
x ∈ Cj and χCj (x) = +∞ for x /∈ Cj, this problem can be written in the form

f (x) + ψ(x) +

min
x∈Rd

1
m

m
(cid:88)

j=1

.

χCj (x)
(cid:124) (cid:123)(cid:122) (cid:125)
gj (x)

(G.1)

Table G.1: Selected applications of Algorithm 11 for solving problem (7.1).

294

Special case of problem (7.1)

f (x)

gj(x)

Constrained optimization (G.1)
Convex projection
Convex feasibility
Dantzig selector (G.2)
Decentralized optimization (G.3)
Support vector machine (G.4)
Overlapping group Lasso (G.5)
Fused Lasso (G.6)
Fused Lasso (G.7)

1

χCj (x)
f (x)
2(cid:107)x − x0(cid:107)2
χCj (x)
χCj (x)
0
(x)
χBj
0
fi(xi)
j x=0}(x)
χ{x : w(cid:62)
2 (cid:107)x(cid:107)2, n = 1 max{0, 1 − bja(cid:62)
2(a(cid:62)
i x − bi)2
i x − bi)2

(cid:107)x(cid:107)Gj
(x)
χCε
λ2|Dj:x|

i x − bi)2

λ

j

f (x) = λ
fi(x) = 1
1
2(a(cid:62)
2(a(cid:62)

1

j x}

ψ(x)

ψ(x)
0
0
(cid:107)x(cid:107)1
0
0
0
λ(cid:107)x(cid:107)1
λ1(cid:107)x(cid:107)1

For f (x) = 1

2(cid:107)x−x0(cid:107)2 and ψ ≡ 0, this specialized to the best approximation problem.

For f ≡ 0 and ψ ≡ 0, this problem specializes to the convex feasibility problem.

G.1.2 Dantzig selector

Dantzig selector [37] solves the problem of estimating sparse parameter x from a linear
model. Given an input matrix A ∈ Rm×d, output vector b ∈ Rm and threshold parameter
λ ≥ 0, deﬁne

Bλ

def= {x : (cid:107)A(cid:62)(b − Ax)(cid:107)∞ ≤ λ} =

m
(cid:92)

Bj
λ,

def= (cid:8)x : (cid:12)
(cid:12)

where Bj
λ
to ﬁnd the solution to

(cid:0)A(cid:62)(b − Ax)(cid:1)

j

j=1

(cid:12)
(cid:12) ≤ λ(cid:9). The goal of the Dantzig selector problem is

which can equivalently be written in the ﬁnite-sum form

min
x∈Rd

(cid:107)x(cid:107)1 + χBλ(x),

+

1
m

m
(cid:88)

j=1

min
x∈Rd

(cid:107)x(cid:107)1
(cid:124)(cid:123)(cid:122)(cid:125)
ψ(x)

.

λ

(x)
χBj
(cid:124) (cid:123)(cid:122) (cid:125)
gj (x)

(G.2)

G.1.3 Decentralized optimization

The problem of minimizing the sum of functions over a network [180] can be reformulated
as

min
x=(x1,...,xn)

1
n

n
(cid:88)

i=1

fi(xi) + χ{x : Wx=0}(x),

295

where W is a matrix such that Wx = 0 if and only if x1 = · · · = xn. Functions
f1, . . . , fn are stored on diﬀerent nodes and each node has access only to its own function.
Matrix W is often derived from a communication graph, which deﬁnes how the nodes can
m)(cid:62), we rewrite the problem
communicate with each other. Formally, if W = (w(cid:62)
above as

1 , . . . , w(cid:62)

min
x=(x1,...,xn)

1
n

n
(cid:88)

i=1

+

1
m

fi(xi)
(cid:124) (cid:123)(cid:122) (cid:125)
fi(x)

m
(cid:88)

j=1

j x=0}(x)
χ{x : w(cid:62)
(cid:123)(cid:122)
(cid:125)
(cid:124)
gj (x)

.

(G.3)

G.1.4 Support-vector machine (SVM)

Support-vector machine [56] is a very popular method for supervised classiﬁcation. The
primal formulation of SVM is given by

min
x∈Rd

λ
(cid:107)x(cid:107)2
2
(cid:124) (cid:123)(cid:122) (cid:125)
f (x)

+

1
m

m
(cid:88)

j=1

max{0, 1 − bja(cid:62)
(cid:123)(cid:122)
(cid:124)
gj (x)

,
j x}
(cid:125)

(G.4)

where a1, . . . , am ∈ Rd and b1, . . . , bm are the features and the outputs.
verify that for gj(x) = max{0, 1 − bja(cid:62)
j x} the proximal operator is given by

It is easy to

proxηj gj (x) = x + Π[0,ηj ]

(cid:18) 1 − bja(cid:62)
j x
(cid:107)aj(cid:107)2

(cid:19)

bjaj.

The celebrated stochastic subgradient descent method Pegasos [238, 239, 258] for

SVMs achieves only slow O( 1

K ) rate.

G.1.5 Overlapping group Lasso

This is a generalization of Lasso proposed in [288] to eﬃciently select groups of features
that are most valuable for the given objective. Let us assume that we are given sets of
i , where [x]i is the i-th
indices G1, . . . , Gm ⊆ {1, . . . , d} and let (cid:107)x(cid:107)Gj
coordinate of vector x. Then, assuming that we are given vectors a1, . . . , an ∈ Rd and
scalars b1, . . . , bn, the objective we want to minimize is

def= (cid:112)(cid:80)

i∈G[x]2

min
x∈Rd

1
n

n
(cid:88)

i=1

1
2
(cid:124)

(a(cid:62)

i x − bi)2
(cid:125)

(cid:123)(cid:122)
fj (x)

+

1
m

m
(cid:88)

j=1

.

(cid:107)x(cid:107)Gj
(cid:124) (cid:123)(cid:122) (cid:125)
gj (x)

(G.5)

It is easy to verify that if gj(x) = (cid:107)x(cid:107)Gj , then

[proxηj gj (x)]i =

(cid:40)[x]i,
max

(cid:16)

(cid:110)

0,

1 − ηj
(cid:107)x(cid:107)Gj

(cid:17)(cid:111)

[x]i,

if i (cid:54)∈ Gj,
if i ∈ Gj.

Vector yk
coordinates of yk

j from Gj.

j will always have at most |Gj| nonzeros, so one can store in memory only the

296

G.1.6 Fused Lasso

The Fused Lasso problem [264] is deﬁned as

min
x∈Rd

1
n

n
(cid:88)

i=1

1
2
(cid:124)

(a(cid:62)

i x − bi)2
(cid:125)

(cid:123)(cid:122)
fi(x)

+

1
d − 1

d−1
(cid:88)

j=1

+ λ(cid:107)x(cid:107)1
(cid:124) (cid:123)(cid:122) (cid:125)
ψ(x)

,
(x)
χCε
j
(cid:124) (cid:123)(cid:122) (cid:125)
gj (x)

(G.6)

def= {x : |[x]j − [x]j+1| ≤ ε} , [x]j is the j-th entry of vector x, a1, . . . , an ∈ Rd

where Cε
j
and b1, . . . , bn ∈ R are given vectors and scalars, ε is given thresholding parameter.

Another formulation of the Fused Lasso is done by using penalty functions. Deﬁne D
to be zero everywhere except for Di,i = 1 and Di,i+1 = −1 with i = 1, . . . , d − 1. Note
that (cid:107)Dx(cid:107)1 = (cid:80)m
j=1 |Dj:x|, where m is the number of rows of D. Then the reformulated
objective is

min
x

1
n

n
(cid:88)

i=1

1
2
(cid:124)

(a(cid:62)

i x − bi)2
(cid:125)

(cid:123)(cid:122)
fi(x)

+

1
m

+ λ1(cid:107)x(cid:107)1
(cid:124) (cid:123)(cid:122) (cid:125)
ψ(x)

m
(cid:88)

j=1

.

λ2|Dj:x|
(cid:124) (cid:123)(cid:122) (cid:125)
gj (x)

(G.7)

In our notation, this means A = D(cid:62) and A(cid:62)A is a tridiagonal matrix given by










A(cid:62)A =

2 −1
−1

2 −1
−1

2 −1










.

. . . −1
2
−1

Let W be a tridiagonal matrix of size (d − 1) × (d − 1) with a on its main diagonal
It can be shown that its eigenvalues are given by
and b on the other two diagonals.
(cid:1) π(cid:1) =
λk(W) = a+2|b| cos (cid:0) kπ
2 − 2 cos (cid:0) π
6. Therefore, if in (G.6) or (G.7) λ1 = 0, we
guarantee linear convergence with the aforementioned constants.

(cid:1), k = 1, . . . , d−1. Thus, λmin(A(cid:62)A) = 2+2 cos (cid:0)(cid:0)1 − 1

d
2d2 and minj

(cid:107)Aj (cid:107)2 = 1

(cid:1) ≈ 1

d

d

1

G.1.7 Square-root Lasso

The approach gets its name from minimizing the square root of the regular least squares,
i.e., (cid:107)Dw − b(cid:107) instead of (cid:107)Dw − b(cid:107)2. This is then combined with (cid:96)1-penalty for feature
selection, which gives the objective

min
w∈Rd

(cid:107)Dw − b(cid:107) + λ(cid:107)w(cid:107)1.

Equivalently, by introducing a new variable z we can put constraints Dj:x − [z]j = 0
for j = 1, . . . , m, which can be written as a(cid:62)
j )(cid:62) and

j (w(cid:62), z(cid:62))(cid:62) = 0 with aj = (Dj:, e(cid:62)

ej

def= (0, 0, . . . , 1
(cid:124)(cid:123)(cid:122)(cid:125)
j

, . . . , 0). Then, the reformulation is

297

min
x=(w,z)∈Rd+m

1
m

m
(cid:88)

j=1

χ{x:a(cid:62)
j x=0}
(cid:125)
(cid:123)(cid:122)
(cid:124)
gj (x)=gj (w,z)

+ (cid:107)z − b(cid:107) + λ(cid:107)w(cid:107)1
(cid:125)
(cid:123)(cid:122)
ψ(x)=ψ(w,z)

(cid:124)

.

The proximal operator of ψ is that of a block-separable function, which is easy to evaluate:

proxγψ(x) =

(cid:18)proxγλ(cid:107)·(cid:107)1(w)
proxγ(cid:107)·−b(cid:107)(z)

(cid:19)

.

G.2 Relation to Existing Methods

G.2.1 SDCA, Dykstra’s algorithm and the Kaczmarz method

Here we formulate SDCA [241], Dykstra’s algorithm and Kaczmarz method. SDCA is a
method for solving

min
x∈Rd

1
m

m
(cid:88)

j=1

gj(x) +

1
2

(cid:107)x − x0(cid:107)2.

If j is sampled uniformly from {1, . . . , m}, SDCA iterates can be deﬁned by the following
recursion,

xk+1 = proxηgj (xk + yk
j ),
yk+1
j + xk − xk+1,
j = yk

If we restrict our attention to characteristic functions, i.e.,

gj(x) = χCj (x) =

(cid:40)

0,
if x ∈ Cj
+∞, otherwise

,

then the proximal operator step is replaced with projection:

xk+1 = ΠCj (xk + yk

j ).

This is known as Dykstra’s algorithm. Finally, if Cj = {x : a(cid:62)
to random projections, i.e.,

j x = bj}, then it boils down

which is the method of Kaczmarz.

xk+1 = Π{a(cid:62)

j x=bj }(xk),

Theorem G.2.1. Consider the regularized minimization problem of SDCA, which is

298

min
x

1
m

m
(cid:88)

j=1

gj(x) +

1
2

(cid:107)x − x0(cid:107)2

with convex g1, . . . , gm. Then, SDCA is a special cases of Algorithm 11 obtained by
2(cid:107)x − x0(cid:107)2, ψ(x) ≡ 0, stepsize γ = 1
applying it with f (x) = 1
1 =
· · · = y0
m = 0. Furthermore, if we consider special case gj = χCj , where Cj (cid:54)= ∅ is a closed
convex set, then we also obtain Dykstra’s algorithm, and if every Cj is a linear subspace,
then we recover the Kaczmarz method.

m and initialization y0

Proof. Consider the iterates of SDM. We will show by induction that yk = x0 − xk and
xk+1 = proxηj gj (xk + ηjyk
j ). Indeed, it holds for y0 by initialization, and then by induction
assumption we have

zk = xk − γ(xk − x0) − γyk = xk − γ(xk − x0) − γ(x0 − xk) = xk.

Therefore, if we denote yk
j

def= ηjyk

j , then

xk+1 = proxηj gj (xk + yk

j ),

which is the update rule of xk+1 in SDCA. Moreover, we have

j = γyk+1
yk+1

j = γyk

j + xk − xk+1 = yk

j + xk − xk+1.

Finally, by induction assumption it holds yk = x0 − xk, whence

yk+1 = yk +

1
m

(yk+1

j − yk

j ) = yk + zk − xk+1 = x0 − xk + xk − xk+1 = x0 − xk+1,

which yields our induction step and the proof itself.

(cid:4)

G.2.2 Accelerated Kaczmarz

Accelerated Kaczmarz [146] performs the following updates:

zk = (1 − αk)xk − αkyk,

xk+1 = Π{x:a(cid:62)
yk+1 = yk + νk(zk − xk+1) + (1 − βk)(zk − yk)

i x=bi}(zk),

with some parameters αk, νk, βk. While the original analysis [146] suggests βk < 1, our
method gives the same update when f (x) = 1

2(cid:107)x(cid:107)2, ψ ≡ 0, αk = γ, βk = 1, νk = 1
γn.

G.2.3 ADMM and Douglas–Rachford Splitting

ADMM, also known as Douglas–Rachford Splitting, in its simplest form as presented
in [198] is a special case of Algorithm 11 when f ≡ 0 and m = 1.

299

G.2.4 Point–SAGA, SAGA, SVRG and Proximal GD

In the trivial case f ≡ 0 and ψ ≡ 0, we recover Point–SAGA. Methods such as SAGA,
SVRG and Proximal Gradient Descent are obtained, in contrast, by setting g ≡ 0. We
would like to mention that introducing g does not change the stepsizes for which those
methods work, e.g., Gradient Descent works with arbitrary γ < 2
L, which is tight. The
similarity suggests that small γ should be used when solving this problem and this obser-
vation is validated by our experiments.

G.2.5 Stochastic Primal–Dual Hybrid Gradient

The relation to the Stochastic Primal–Dual Hybrid Gradient (SPDHG) is complicated.
On the one hand, SPDHG is a general method with three parameters and it preconditions
proximal operators with matrices, so our method cannot be its strict generalization. On
the other hand, SPDHG does not allow for f . Moreover, when f ≡ 0 and some parameters
are set to speciﬁc values in SPDHG, the methods coincide, but the guarantees are not the
same. In particular, we show below that one of the parameters in SPDHG, θ, should be set
to 1, in which case linear convergence for smooth g1, . . . , gm was not known for SPDHG.
Therefore, the tools developed in this chapter can potentially lead to new discoveries
about full version of SPDHG as well.

Let us now formulate the method explicitly. After a simple rescaling of the functions,

SPDHG from [73] can be formulated as a method to solve the problem

min
x∈Rd

1
m

m
(cid:88)

j=1

φj(A(cid:62)

j x) + ψ(x).

(G.8)

Renaming the variables for our convenience and choosing for simplicity uniform probabil-
ities of sampling j from {1, . . . , m}, the update rules of SPDHG can be written as

wk = proxγψ(wk−1 − γyk),
yk+1
j wk + yk
j = proxσφ(cid:63)
j
1
m

yk+1 = yk +

Aj(yk+1

(σA(cid:62)

j ),

j ),

j − yk
j − yk

j ),

yk+1 = yk + θAj(yk+1

where γ, σ and θ are the method’s parameters and φ(cid:63)
initialization that we are interested in is with y0 = 1
m

j is the Fenchel conjugate of φj. The
j , y0 = y0, w0 = x0.

j=1 y0

(cid:80)m

One can immediately see that one big diﬀerence with our approach is that the method
puts Aj outside of the proximal operator, which also leads to diﬀerent iteration complexity.
In particular, when φ1, . . . , φm are smooth, the complexity proved in [40] is

(cid:32)(cid:32)

O

m +

(cid:107)Aj(cid:107)

(cid:115)

(cid:33)

Lφ
µψ

log

(cid:33)

,

1
ε

m
(cid:88)

j=1

where µψ is the strong convexity constant of ψ and Lφ is the smoothness constant of

φ1, . . . , φm. Since function gj(x) = φj(A(cid:62)
Corollary 7.6.9 with µ-strongly convex and L-smooth f is

300
j x) is at most Lφ(cid:107)Aj(cid:107)2 smooth, our rate from

(cid:32)(cid:32)

O

n + m +

(cid:115)

+

m

L
µ

Lφ
µ

max
j

(cid:33)

(cid:107)Aj(cid:107)

log

(cid:33)

.

1
ε

If, in addition, we use sampling with probabilities proportional to (cid:107)Aj(cid:107), then we can
achieve

(cid:32)(cid:32)

O

n + m +

L
µ

+

1
√
m

m
(cid:88)

j=1

(cid:115)

(cid:107)Aj(cid:107)

(cid:33)

Lφ
µ

log

(cid:33)

.

1
ε

We do not prove this, but the complexity for our method will be similar if we use
strongly convex ψ rather than f , so our rates should match or be even be superior to that
of SPDHG, at the cost of evaluating potentially harder proximal operators.

Now, let us prove that our method is indeed connected to SPDHG via choice of θ = 1

and γσ = 1.

Theorem G.2.2. If we apply SPDHG with identity matrices Aj = I, i.e., φj(x) =
gj(x), and choose parameters θ = 1 and γσ = 1, then it is algorithmically equivalent to
Algorithm 11 with f ≡ 0.

Proof. Since φj and gj are the same, we will use in the proof gj only.

First, mention that it is straightforward to show by induction that yk = 1
m

j=1 yk
j ,
which coincides with our update. Our goal is to show by induction that in SPDHG it
holds

(cid:80)m

wk−1 − γyk = xk − γyk,

where we deﬁne sequence xk as

xk+1 def= proxγgj (wk + γyk

j ) = prox 1

σ gj

(wk + γyk

j ).

We will see that implicitly xk+1 is present in every update of SPDHG. To this end, let us
ﬁrst rewrite the update for yk+1

. We have by Moreau’s identity

j

yk+1
j = proxσg(cid:63)

j

(σwk + yk

j ) = σwk + yk

j − σprox 1

σ gj

(cid:32)

σwk + yk
j
σ

(cid:33)

.

Since we consider σ = 1

γ , it transforms into

yk+1
j = yk

j +

(cid:16)

1
γ

wk − proxηgj (wk + γyk
j )

(cid:17)

= yk

j +

1
γ

(cid:0)wk − xk+1(cid:1)

The only missing thing is rewriting update for wk in terms of xk and yk. From the update

rule for yk+1

j

we derive

301

yk+1 = yk + θ(yk+1

j − yk

j ) = yk +

θ
γ

(wk − xk+1).

Hence,

wk+1 = proxγψ(wk − γyk+1) = proxγψ(wk − γyk+1 − θ(wk − xk+1))

θ=1= proxγψ(xk+1 − γyk+1).

Thus, updates for wk, yk

j and yk completely coincide under this choice of parameters. (cid:4)

Since our method under f ≡ 0 reduces to Point–SAGA, we obtain the following result

that was unknown.

Corollary G.2.3. Point–SAGA [61] is a special case of Stochastic Primal–Dual Hybrid
Gradient [40].

G.3 Evaluating Proximal Operators

For some functions, the proximal operator admits a closed form solution, for instance if
gj(x) = χ{x : a(cid:62)

j x=bj }(x), then

proxηj gj (x) = x −

a(cid:62)
j x − bj
(cid:107)aj(cid:107)2 aj.

If, however, the proximal operator is not given in a closed form, then it is still possible to
j x), Aj ∈ Rd×dj , then the proximal operator is the
eﬃciently evaluate it.
solution of a dj-dimensional strongly convex problem.

If gj = φj(A(cid:62)

Lemma G.3.1. Let φj : Rdj → R be a convex lower semi-continuous function such that
Range (cid:0)A(cid:62)

(cid:1) has a point of dom (φ). If gj(x) = φj(A(cid:62)

j x), then

j

x − proxηj gj (x) ∈ Range (Aj) .

Proof. Let us ﬁx x. Any vector z ∈ Rd can be decomposed as z = x + Ajβ + w, where
β ∈ Rdj and A(cid:62)

j w = 0, from which it also follows gj(z) = φj(A(cid:62)

j Ajβ). Then

j x + A(cid:62)

proxηj gj (x) def= arg min
z∈Rd

(cid:26)

ηjφj(A(cid:62)

j z) +

(cid:27)

(cid:107)z − x(cid:107)2

1
2

= arg min

z=x+Aj β+w

= arg min

z=x+Aj β+w

(cid:26)

ηjφj(A(cid:62)

j x + A(cid:62)

j Ajβ) +

(cid:26)

ηjφj(A(cid:62)

j x + A(cid:62)

j Ajβ) +

(cid:27)

(cid:107)Ajβ + w(cid:107)2

(cid:107)Ajβ(cid:107)2 +

(cid:27)

.

(cid:107)w(cid:107)2

1
2

1
2
1
2

Clearly, the last expression achieves its minimum only when w = 0.

(cid:4)

302

We can simplify the expression for the proximal operator even further if Aj is of full
column rank, for instance if it is just a single nonzero row. It is straightforward to verify
that for any matrix B ∈ Rd1×d2, constant vector c ∈ Rd2 and function Φ with a unique
minimizer and dom (Φ(Bβ + c)) (cid:54)= ∅ it holds

arg

min
β=B(α+c),β∈Rd2

Φ(β) = arg

min
β=B(α+c),β∈Rd2

Φ(B(α + c))

= B arg

min
u=α+c,α∈Rd1

Φ(α + c)

(cid:16)

= B

arg min
u∈Rd1

Φ(u) − c

(cid:17)

.

(G.9)

Since we know by Lemma G.3.1 that u def= proxηj gj (x) = x + Ajβj for some βj ∈
Rdj , we can write the necessary and suﬃcient optimality condition for u by repeatedly
applying (G.9)

proxηj gj (x)

(cid:26)

= arg

min
u=x+Aj β, β∈Rdj

(G.9)
= x + Aj arg min
β∈Rdj
(cid:26)

= x + Aj arg min
β∈Rdj

φj

(G.9)
= x + Aj(A(cid:62)

j Aj)−1 arg min
α=A(cid:62)

j Aj β

(G.9)
= x + Aj(A(cid:62)

j Aj)−1(cid:16)

Note that

(cid:27)

φj(A(cid:62)

(cid:107)x − u(cid:107)2

j u) +

1
2ηj
j (x + Ajβ)(cid:1) +

(cid:0)A(cid:62)

(cid:26)

φj

(cid:27)

(cid:107)Ajβ(cid:107)2

1
2ηj
1
2ηj
j x + α(cid:1) +

j Ajβ(cid:1) +
(cid:26)

(cid:0)A(cid:62)

φj

(cid:26)

(cid:0)A(cid:62)

j x + A(cid:62)

(cid:107)Aj(A(cid:62)

j Aj)−1A(cid:62)

j Ajβ(cid:107)2

(cid:27)

(cid:107)Aj(A(cid:62)

j Aj)−1α(cid:107)2

(cid:27)

1
2ηj

arg min
θ=α+A(cid:62)

j x

φj (θ) +

(cid:107)Aj(A(cid:62)

j Aj)−1(θ − A(cid:62)

j x)(cid:107)2

(cid:27)

− A(cid:62)

j x

(cid:17)

.

1
2ηj

(cid:107)Aj(A(cid:62)

j Aj)−1(θ − A(cid:62)

j x)(cid:107)2 = (θ − A(cid:62)
= (cid:107)θ − A(cid:62)

j x)(cid:62)(A(cid:62)
j x(cid:107)2

j Aj)−1A(cid:62)
j Aj )−1,

(A(cid:62)

j Aj(A(cid:62)

j Aj)−1(θ − A(cid:62)

j x)

where for any positive semi-deﬁnite matrix W we denote (cid:107)x(cid:107)2
W
(x) def= arg minθ{φj(θ) + 1
similarly proxW
ηj φj
2ηj

W}, we obtain

(cid:107)θ − x(cid:107)2

def= x(cid:62)Wx. Denoting

proxηj gj (x) = x + Aj(A(cid:62)

(cid:18)

(cid:26)

j Aj)−1
j Aj)−1 (cid:16)

arg min
θ∈Rdj
(A(cid:62)
ηj φj

prox

j Aj )−1

(cid:0)A(cid:62)

1
2ηj
j x(cid:1) − A(cid:62)
j x

(cid:17)

.

φj (β) +

(cid:107)θ − A(cid:62)

j x(cid:107)(A(cid:62)

j Aj )−1

(cid:27)

(cid:19)

− A(cid:62)

j x

= x + Aj(A(cid:62)

Thus, we only need to know how to eﬃciently evaluate prox
λ > 0 and z ∈ Rdj , assuming that matrix (A(cid:62)

(z) for arbitrary
j Aj)−1 can be precomputed. For example,

j Aj )−1

(A(cid:62)
λφj

if Aj = aj ∈ Rd, then

303

prox

j aj )−1

(a(cid:62)
ηj φj

(x) = proxηj (cid:107)aj (cid:107)2φj (x).

If, in addition, φj : R → R is given by

φj(z) =

(cid:40)

bjz,
if z ≤ 0,
cjz, otherwise

with some bj, cj ∈ R, bj < cj, then proxλφj (z) = z − λbj for z ≤ λbj, proxλφj (z) = 0
for z ∈ (λbj, λcj] and proxλφj (z) = z − λcj for z > λcj. Therefore,

proxηj gj (x) =






x −

x − ηjajbj,
a(cid:62)
j x
(cid:107)aj (cid:107)2 aj,
x − ηjajcj,

j x ≤ (cid:107)aj(cid:107)2bj,

if a(cid:62)
if (cid:107)aj(cid:107)2bj ≤ a(cid:62)
otherwise

j x ≤ (cid:107)aj(cid:107)2cj,

.

Note that if (cid:107)aj(cid:107)2bj ≤ a(cid:62)

j x ≤ (cid:107)aj(cid:107)2cj, then a(cid:62)

j proxηj gj (x) = 0.

G.4 Optimality Conditions

We now comment on the nature of Assumption 7.3.1. In view of the ﬁrst-order necessary
and suﬃcient condition for the solution of (7.1), we have

x(cid:63) ∈ X (cid:63) ⇔ 0 ∈ ∂P (x(cid:63)) = ∇f (x(cid:63)) + ∂(g + ψ)(x(cid:63)).

By the weak sum rule [16, Corollary 3.38], we have

∂P (x) ⊇ ∇f (x) +

1
m

m
(cid:88)

j=1

∂gj(x) + ∂ψ(x)

for all x ∈ dom (P ) ⊇ X (cid:63). Under the regularity condition ∩k
j=k+1
ri(dom (gj)) ∩ ri(dom (ψ)) (cid:54)= ∅, where g1, . . . , gk are polyhedral functions, the inclu-
sions becomes an identity [225, Theorem 23.8], which means that Assumption 7.3.1 is
satisﬁed.

j=1(dom (gj)) ∩m

For functions gj of the form gj(x) = φj(A(cid:62)

j x), where φj : Rdj → R ∪ {+∞} are
proper closed convex functions and Aj ∈ Rd×dj , we shall instead consider the following
(slightly stronger) assumption:

Assumption G.4.1. There exists x(cid:63) ∈ X (cid:63) and vectors y(cid:63)
Am∂φm(A(cid:62)

mx(cid:63)) and r(cid:63) ∈ ∂ψ(x(cid:63)) such that ∇f (x(cid:63)) + 1
m

1 ∈ A1∂φ1(A(cid:62)
(cid:80)m

j + r(cid:63) = 0.

j=1 y(cid:63)

1 x(cid:63)), . . . , y(cid:63)

m ∈

Since Aj∂φj(A(cid:62)

tion G.4.1 is indeed stronger than Assumption 7.3.1.
from ri(dom (gj)), or gj is polyhedral and Range (cid:0)A(cid:62)

j x) ⊆ ∂gj(x) for all x ∈ dom (gj) [16, Theorem 3.43], Assump-
(cid:1) contains a point
(cid:1) contains a point from mere

If Range (cid:0)A(cid:62)

j

j

304

dom (gj), then ∂gj(x) = Aj∂φj(A(cid:62)
assumptions are the same.

j x) for any x [225, Theorem 23.9], and these two

Below we provide another stationarity condition that shows why x(cid:63) is a ﬁxed-point of

our method.

Lemma G.4.2 (Optimality conditions). Let x(cid:63) be a solution of (7.1) and let Assump-
tion 7.3.1 be satisﬁed. Then for any γ, ηj ∈ R,

x(cid:63) = proxγψ(x(cid:63) − γ∇f (x(cid:63)) − γy(cid:63)),

x(cid:63) = proxηj gj (x(cid:63) + ηjy(cid:63)

j ).

Proof. Let

z = proxγψ(x(cid:63) − γ∇f (x(cid:63)) − γy(cid:63)) = arg min
u

{γψ(u) +

1
2

(cid:107)u − (x(cid:63) − γ∇f (x(cid:63)) − γy(cid:63))(cid:107)2}.

Since ψ is convex, the problem inside arg min is strongly convex, and the necessary and
suﬃcient condition for z to be its solution is

0 ∈ z − x(cid:63) + γ∇f (x(cid:63)) + γy(cid:63) + γ∂ψ(z).

By Assumption 7.3.1 it holds for z = x(cid:63), implying the ﬁrst equation that we want to prove.
The second one follows by exactly the same argument applied to arg minu{ηjgj(u)+ 1
2(cid:107)u−
(cid:4)
(x(cid:63) + ηjy(cid:63)

j )(cid:107)2}.

G.5 Convergence Proofs

In this section, we provide the proofs of our convergence results. Each lemma, theorem
and corollary is ﬁrst restated and only then proved to simplify the reading.

G.5.1 Proof of Lemma 7.5.2 (Gradient Descent)

Here we prove that Gradient Descent update on f satisﬁes our assumption on the method
with the best possible stepsizes.

Lemma G.5.1 (Same as Lemma 7.5.2). If f is convex, Gradient Descent satisﬁes As-
sumption 7.5.1(a) with any γmax < 2
L , ω = 2 − γmaxL and Mk = 0. If f is µ-strongly
convex, Gradient Descent satisﬁes Assumption 7.5.1(b) with γmax = 2
L+µ, ω = 1 and
Mk = 0.

Proof. Since we consider Gradient Descent, we have

wk = xk − γ∇f (xk).

First, if f is convex and smooth, then for any γ ≤ γmax < 2
L

305

(cid:107)wk − w(cid:63)(cid:107)2 = (cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11) + γ2(cid:107)∇f (xk) − ∇f (x(cid:63))(cid:107)2
≤ (cid:107)xk − x(cid:63)(cid:107)2 − γ(2 − γmaxL) (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11)

− γγmaxL (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11) + γγmax(cid:107)∇f (xk) − ∇f (x(cid:63))(cid:107)2
≤ (cid:107)xk − x(cid:63)(cid:107)2 − γ(2 − γmaxL) (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11)

(1.19)

(1.20)
≤ (cid:107)xk − x(cid:63)(cid:107)2 − γ (2 − γmaxL) Df (xk, x(cid:63)).

Now let us consider µ-strongly convex f . We have

(cid:107)wk − w(cid:63)(cid:107)2 = (cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11) + γ2(cid:107)∇f (xk) − ∇f (x(cid:63))(cid:107)2

(1.22)
≤

(cid:18)

1 −

(cid:18)

(1.21)
≤

1 −

(cid:19)

(cid:19)

2γµL
L + µ
2γµL
L + µ

(cid:107)x − y(cid:107)2 − γ

(cid:107)x − y(cid:107)2 − γ

(cid:18) 2

L + µ

(cid:18) 2

L + µ

(cid:19)

(cid:19)

− γ

− γ

(cid:107)∇f (xk) − ∇f (x(cid:63))(cid:107)2

µ2(cid:107)xk − x(cid:63)(cid:107)2

= (1 − γµ)2(cid:107)xk − x(cid:63)(cid:107)2
≤ (1 − γµ)(cid:107)xk − x(cid:63)(cid:107)2.

The last step simply uses 1 − γµ ≤ 1, which, of course, makes our guarantees slightly
weaker, but, on the other hand, puts Gradient Descent under the umbrella of Assump-
(cid:4)
tion 7.5.1.

G.5.2 Key lemma

The result below is the most important lemma of this chapter as it lies at the core of
our analysis. It provides a very generic statement about the step with stochastic proximal
operators. At the same time, it is a mere corollary of ﬁrm nonexpansiveness of the proximal
operator.

Lemma G.5.2. Let zk = proxγψ(wk − γyk) and xk+1 = proxηj gj (zk + ηjyk
is sampled from {1, . . . , m} with probabilities {p1, . . . , pm}, ηj = γ
mpj
number. If yk+1

(zk − xk+1) and yk+1

l for all l (cid:54)= j, it holds

j = yk

l = yk

j + 1
ηj

j ), where j
and γ is a positive

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2 + Y k+1(cid:3)

(cid:20)

≤ E

(cid:107)wk − w(cid:63)(cid:107)2 +

(cid:18)

1 −

(cid:19)

ν
m(1 + ν)

Y k − (cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2

(cid:21)

,

where ν def= minj=1,...,m

1
ηj Lj

and Lj ∈ R ∪ {+∞} is the smoothness constant of gj.

Proof. Mention that x(cid:63) = proxηj gj (x(cid:63) + ηjy(cid:63)

j ) by optimality condition.

In addition, it

holds by deﬁnition yk+1

j = 1
ηj

(zk + ηjyk

j − xk+1), so property (1.25) yields

306

(cid:107)xk+1 − x(cid:63)(cid:107)2 +

(cid:18)

1 +

(cid:19)

1
ηjLj

j (cid:107)yk+1
η2

j − y(cid:63)

j (cid:107)2 ≤ (cid:107)zk + ηjyk

j − (x(cid:63) + ηjy(cid:63)

j )(cid:107)2

and we can replace 1 + 1
ηj Lj

with 1 + ν since ν ≤ 1
ηj Lj
Let Ej be the expectation with respect to sampling of j. Then, we observe

.

Ej

(cid:2)(cid:107)zk + ηjyk

j − (x(cid:63) + ηjy(cid:63)

j )(cid:107)2(cid:3)

= (cid:107)zk − x(cid:63)(cid:107)2 + Ej

(cid:107)yk

j − y(cid:63)

j (cid:107)2

(cid:21)

+ 2

(cid:28)

zk − x(cid:63), γEj

(cid:20) 1
mpj

(cid:21)(cid:29)

(yk

j − y(cid:63)
j )

= (cid:107)zk − x(cid:63)(cid:107)2 +

(cid:107)yk

l − y(cid:63)

l (cid:107)2 + 2γ (cid:10)zk − x(cid:63), yk − y(cid:63)(cid:11) .

(G.10)

(cid:20) γ2
m2p2
j
m
1
(cid:88)
pl

l=1

γ2
m2

Denote w(cid:63) def= x(cid:63) − γ∇f (x(cid:63)). Another optimality condition from Lemma G.4.2 is x(cid:63) =
proxγψ(w(cid:63) − γy(cid:63)), so let us use (1.25) one more time to obtain

(cid:107)zk − x(cid:63)(cid:107)2 ≤ (cid:107)wk − γyk − (w(cid:63) − γy(cid:63))(cid:107)2 − (cid:107)wk − γyk − zk − (w(cid:63) − γy(cid:63) − x(cid:63))(cid:107)2
= γ2(cid:107)yk − y(cid:63)(cid:107)2 − 2γ (cid:10)wk − w(cid:63), yk − y(cid:63)(cid:11) + (cid:107)wk − w(cid:63)(cid:107)2

− (cid:107)wk − γyk − zk − (w(cid:63) − γy(cid:63) − x(cid:63))(cid:107)2.

Furthermore,

(cid:107)wk − γyk − zk − (w(cid:63) − γy(cid:63) − x(cid:63))(cid:107)2 = (cid:107)wk − zk − (w(cid:63) − x(cid:63))(cid:107)2 + γ2(cid:107)yk − y(cid:63)(cid:107)2

− 2γ (cid:10)wk − zk − (w(cid:63) − x(cid:63)), yk − y(cid:63)(cid:11) ,

so

(cid:107)zk − x(cid:63)(cid:107)2 ≤ −2γ (cid:10)wk − w(cid:63), yk − y(cid:63)(cid:11) + 2γ (cid:10)wk − zk − (w(cid:63) − x(cid:63)), yk − y(cid:63)(cid:11)

+ (cid:107)wk − w(cid:63)(cid:107)2 − (cid:107)wk − zk − (w(cid:63) − x(cid:63))(cid:107)2

= (cid:107)wk − w(cid:63)(cid:107)2 − 2γ (cid:10)zk − x(cid:63), yk − y(cid:63)(cid:11) − (cid:107)wk − zk − (w(cid:63) − x(cid:63))(cid:107)2.

Together with the previously obtained bounds it adds up to

Ej

(cid:2)(cid:107)zk + ηjyk

j − (x(cid:63) + ηjy(cid:63)

j )(cid:107)2(cid:3) ≤ (cid:107)wk − w(cid:63)(cid:107)2 +

γ2
m2

m
(cid:88)

l=1

1
pl

(cid:107)yk

l − y(cid:63)

l (cid:107)2

To get the expression in the left-hand side of this lemma’s statement, let us add the

− (cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2.

missing sum and evaluate its expectation:

307

(cid:34) m
(cid:88)

E

l=1

(cid:35)

l (cid:107)yk+1
η2

l − y(cid:63)

l (cid:107)2

= E (cid:2)(cid:107)yk+1

j − y(cid:63)

j (cid:107)2(cid:3) + E

l (cid:107)yk+1
η2

l − y(cid:63)

l (cid:107)2

(cid:35)

.

(cid:34)

(cid:88)

l(cid:54)=j

Clearly, all summands in the last sum were not changed at iteration k, so

Ej

(cid:34)

(cid:88)

l(cid:54)=j

l (cid:107)yk+1
η2

l − y(cid:63)

l (cid:107)2

(cid:35)

= Ej

(cid:34)

(cid:88)

l(cid:54)=j

(cid:35)

l (cid:107)yk
η2

l − y(cid:63)

l (cid:107)2

=

=

m
(cid:88)

l=1
m
(cid:88)

l=1

(1 − pl)η2

l (cid:107)yk

l − y(cid:63)

l (cid:107)2

l (cid:107)yk
η2

l − y(cid:63)

l (cid:107)2 −

γ2
m2

m
(cid:88)

l=1

1
pl

(cid:107)yk

l − y(cid:63)

l (cid:107)2.

The negative sum will cancel out with the same in equation (G.10) and we conclude the
(cid:4)
proof.

G.5.3 Convergence of Bregman divergence to 0 almost surely

Here we formulate a result that we only brieﬂy mentioned in the main text. It states that
for convex problems, Bregman divergence Df (xk, x(cid:63)) almost surely converges to 0. To
show it, let us borrow the classical result on supermartingale convergence.

Proposition G.5.3 ([22], Proposition A.4.5). Let (X k)k, (Y k)k, (Z k)k be three se-
quences of nonnegative random variables and let (F k)k be a sequence of σ-algebras such
that F k ⊂ F k+1 for all k. Assume that:

• The random variables X k, Y k, Z k are nonnegative and F k-measurable.

• For each k, we have E[X k+1 | F k] ≤ X k − Y k + Z k.

• There holds, with probability 1,

∞
(cid:88)

k=0

Z k < ∞.

Then X k converges to a nonnegative random variable X and we have (cid:80)∞
with probability 1.

k=0 Y k < ∞

Theorem G.5.4. Take a method that satisﬁes Assumption 7.5.1(a), a stepsize γ ≤
γmax and an optimum x(cid:63) satisfying Assumption 7.3.1. Then, with probability 1 it holds
Df (xk, x(cid:63)) → 0.

Proof. Fix any solution x(cid:63), y(cid:63)
m, . . . , xk, yk
be the σ-algebra generated by all random variables prior to moment k, and let M

m. Let F k = σ(x0, y0

1, . . . , y0

1, . . . , y(cid:63)

1 , . . . , yk
m)
be Mk

k

conditioned on F k, i.e., M
the assumptions of Proposition G.5.3 are satisﬁed for sequences

k def= Mk|F k, from which it follows Mk = E

k(cid:105)

(cid:104)
M

. Then,

308

X k = (cid:107)xk − x(cid:63)(cid:107)2 + M

k

+ (1 + ν)

m
(cid:88)

l=1

Y k = ωγDf (xk, x(cid:63)),
Z k = 0.

l (cid:107)yk
η2

l − y(cid:63)

l (cid:107)2,

Therefore, we have that (cid:80)∞
Df (xk, x(cid:63)) → 0.

k=0 Y k < ∞ and Y k → 0 almost surely, from which it follows
(cid:4)

The almost sure guarantee is not applicable to SGD which has M0 proportional to
the number of iterations. We leave its analysis as well as analysis of convergence of xk to
an optimum for future work.

G.5.4 Proof of Theorem 7.6.1 (O (cid:0) 1
K
Below we provide the proof of O (cid:0) 1
Theorem G.5.5 (Same as Theorem 7.6.1). Assume f is L-smooth and µ-strongly convex,
g1, . . . , gm, ψ are convex, closed and lower semi-continuous. Take a method satisfying
Assumption 7.5.1 and γ ≤ γmax, then

(cid:1) rate)
(cid:1) rate for general convex functions.

K

E (cid:2)Df (xK, x(cid:63))(cid:3) ≤

1
ωγK

L0,

where L0 def= (cid:107)x0 − x(cid:63)(cid:107)2 + M0 + (cid:80)m
Proof. Recall that

l=1 η2

l (cid:107)y0

l − y(cid:63)

l (cid:107)2 and xK def= 1

K

(cid:80)K−1

k=0 xk.

Lk def= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + Mk + Y k,

and by Assumption 7.5.1 combined with Lemma G.5.2

Lk+1 ≤ Lk − ωγE (cid:2)Df (xk, x(cid:63))(cid:3) .

Telescoping this inequality from 0 to K − 1, we obtain

(cid:34)K−1
(cid:88)

(cid:35)
Df (xl, x(cid:63))

E

≤

l=0

1
ωγ

(L0 − LK) ≤

1
ωγ

L0.

By convexity of f , the left-hand side is lower bounded by KE (cid:2)Df (xK, x(cid:63))(cid:3), so dividing
(cid:4)
both sides by K ﬁnishes the proof.

G.5.5 Proof of Theorem 7.6.3 (O( 1
In this subsection, we show the O (cid:0) 1
K2

(cid:1) rate.

K 2 ) rate)

309

Theorem G.5.6 (Same as Theorem 7.6.3). Consider updates with time-varying stepsizes,
. Then, it
γk−1 =
holds

for j = 1, . . . , m, where a ≥ 2 max

ωµ(a+k) and ηk,j = γk
mpj

(cid:110) 1

ωµγmax

, 1
ρ

(cid:111)

2

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

a2
(K + a − 1)2 L0,

where L0 = (cid:107)x0 − x(cid:63)(cid:107)2 + M0 + (cid:80)m

l=1 η2

0,l(cid:107)y0

l − y(cid:63)

l (cid:107)2.

Proof. For this proof, we redeﬁne the sequence Y k to have time-varying stepsizes:

Y k def=

m
(cid:88)

l=1

η2
k,l

E (cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3) .

Before writing a new recurrence, let us note that

(1 − ωγkµ)

(cid:19)2

(cid:18)γk−1
γk

=

(cid:0)1 − 2

a+k

(cid:1) (a + k)2

(a + k − 1)2

=

(a + k − 2)(a + k)
(a + k − 1)2

< 1,

so 1 − ωγkµ ≤
seen in other proofs, but the stepsizes in the right-hand side are now time-dependent:

. Then, Lemma G.5.2 gives a similar recurrence to what we have

(cid:17)2

(cid:16) γk
γk−1

Lk+1 = E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) + Mk+1 + Y k+1

≤ (1 − ωγkµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + (1 − ρ)Mk +

m
(cid:88)

η2
k,l

E (cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3)

≤ (1 − ωγkµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 + Mk(cid:3) +

≤

=

(cid:19)2

(cid:19)2

(cid:18) γk
γk−1
(cid:18) γk
γk−1

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 + Mk(cid:3) +

(cid:18) γk
γk−1

Lk.

Recursing this inequality yields

l=1
(cid:19) m
(cid:88)

η2
k,l

E(cid:107)yk

l − y(cid:63)

l (cid:107)2

(cid:18) γk
γk−1

l=1
(cid:19)2 m
(cid:88)

l=1

η2
k,l

E(cid:107)yk

l − y(cid:63)

l (cid:107)2

Lk+1 ≤ L0

(cid:19)2

k
(cid:89)

k=1

(cid:18) γk
γk−1

(cid:19)2

=

(cid:18)γk
γ0

L0 =

(cid:18) a

(cid:19)2

a + k

L0.

It remains to plug-in k = K − 1.

(cid:4)

310

G.5.6 Proof of Theorem 7.6.4 (O( 1

K ) rate of SGD)

Here we consider the case where f (x) is given as expectation parameterized by a random
variable ξ,

f (x) = Eξ [f (x; ξ)] .

While it is often assumed in the literature that E(cid:107)∇f (x; ξ) − ∇f (x)(cid:107)2 ≤ σ2 uniformly
over x, we do not need this assumption and bound the variance using the following lemma.

Lemma G.5.7. Let wk = xk − γ∇f (xk; ξk), where random function f (x; ξ) is almost
surely convex and L-smooth. Then,

E (cid:2)(cid:107)∇f (xk; ξk) − ∇f (x(cid:63))(cid:107)2(cid:3) ≤ 4LE (cid:2)Df (xk, x(cid:63))(cid:3) + 2σ2
(cid:63),

(G.11)

def= E [(cid:107)∇f (x(cid:63); ξ) − ∇f (x(cid:63))(cid:107)2], i.e., σ2

where σ2
(cid:63)
than one x(cid:63) exists, take the one that minimizes σ2
(cid:63).

(cid:63) is the variance at an optimum. If more

Proof. This proof is based on existing results for SGD and goes in a very standard way.
By Young’s inequality

E (cid:2)(cid:107)∇f (xk; ξk) − ∇f (x(cid:63))(cid:107)2(cid:3) ≤ 2E (cid:2)(cid:107)∇f (xk; ξk) − ∇f (x(cid:63); ξk)(cid:107)2(cid:3)
+ 2E (cid:2)(cid:107)∇f (x(cid:63); ξk) − ∇f (x(cid:63))(cid:107)2(cid:3)

(1.18)

≤ 4LE (cid:2)Df (·;ξk)(xk, x(cid:63))(cid:3) + 2σ2
= 4LE (cid:2)Df (xk, x(cid:63))(cid:3) + 2σ2
(cid:63).

(cid:63)

In the proof of Theorem 7.6.4 we will again need time-varying stepsize and Y k should

be deﬁned as

(cid:4)

Y k def=

m
(cid:88)

j=1

η2
k,j

E (cid:2)(cid:107)yk

j − y(cid:63)

j (cid:107)2(cid:3) .

But before let us prove a simple statement about sequences with contraction and additive
error.

Lemma G.5.8. Assume that sequence (Lk)k satisﬁes inequality Lk+1 ≤
2γ2

(cid:63) with some constant σ(cid:63) ≥ 0. Then, it holds

kσ2

(cid:17)2

(cid:16) γk
γk−1

Lk +

Lk ≤

(cid:19)2

(cid:18) γk−1
γ0

L0 + 2kγ2

k−1σ2
(cid:63).

Proof. We will prove the inequality by induction. For k = 0 it is straightforward. The

induction step follows from

311

Lk + 2γ2

kσ2
(cid:63)

(cid:19)2

Lk+1 ≤

≤

=

(cid:18) γk
γk−1
(cid:18) γk
γk−1
(cid:19)2

(cid:18) γk
γ0

(cid:19)2 (cid:18) γk−1
γ0

(cid:19)2

L0 + 2

(cid:19)2

(cid:18) γk
γk−1

(γk−1)2kσ2

(cid:63) + 2kγ2

k−1σ2
(cid:63)

L0 + 2(k + 1)γ2

k−1σ2
(cid:63).

(cid:4)

Now we are ready to prove the theorem.

Theorem G.5.9 (Same as Theorem 7.6.4). Assume f is µ-strongly convex, f (·; ξ) is
almost surely convex and L-smooth. Let the update be produced by SGD, i.e., vk =
∇f (xk; ξk), and let us use time-varying stepsizes γk−1 = 2
a+µk with a ≥ 4L. Then, it
holds

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

8σ2
(cid:63)
µ(a + µK)

+

a2
(a + µK)2 L0.

Proof. It holds by Lemma G.5.7

E (cid:2)(cid:107)∇f (xk; ξk) − ∇f (x(cid:63))(cid:107)2(cid:3) ≤ 4LE (cid:2)Df (xk, x(cid:63))(cid:3) + 2σ2
(cid:63).

Therefore, for wk def= xk − γkvk = xk − γk∇f (xk; ξk) and w(cid:63) def= x(cid:63) − γk∇f (x(cid:63)) we have

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3)
= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γk
≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γk

(G.11)

(cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11) + γ2

(cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11) + 4γ2

k(cid:107)∇f (xk; ξk) − ∇f (x(cid:63))(cid:107)2(cid:3)
(cid:3)

kLDf (xk, x(cid:63)) + 2γ2

kσ2
(cid:63)

(cid:104)
(1.20)
≤ E
(1 − γkµ)(cid:107)xk − x(cid:63)(cid:107)2 − 2γk(1 − 2γkL
(cid:125)

(cid:124)

(cid:123)(cid:122)
≥0

≤ (1 − γkµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + 2γ2

kσ2
(cid:63).

)Df (xk, x(cid:63)) + 2γ2

kσ2
(cid:63)

(cid:105)

Using the same argument as in the proof of Theorem 7.6.3, we can show that 1 −
. Combining these results with Lemma G.5.2, we obtain for Lk+1 def=

(cid:17)2

γkµ ≤

(cid:16) γk
γk−1

E (cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) + Y k+1 the following bound:

312

Lk+1 ≤ (1 − γkµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

m
(cid:88)

η2
k,l

E (cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3) + 2γ2

kσ2
(cid:63)

≤

=

(cid:19)2

(cid:19)2

(cid:18) γk
γk−1
(cid:18) γk
γk−1

E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

Lk + 2γ2

kσ2
(cid:63).

(cid:19)2

l=1

(cid:18) γk
γk−1

Y k + 2γ2

kσ2
(cid:63)

By Lemma G.5.8

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ LK ≤

(cid:19)2

(cid:18) γK−1
γ0

L0 + 2Kγ2

k−1σ2

(cid:63) ≤

a2

(a + µK)2 L0 +

8k
(a + µK)µK

σ2
(cid:63).

(cid:4)

G.5.7 Proof of Theorem 7.6.5 (linear rate for gj = φj(A(cid:62)

j x))

Let us now show linear convergence of our method when the consider problem has linear
structure, i.e., gj(x) = φj(A(cid:62)

j x).

We ﬁrst need a lemma on the nature of yk

1 , . . . , yk

m in the considered case.

Lemma G.5.10. Let the proximal sum be of the form 1
m
matrices Aj ∈ Rd×dj , and y0
satisﬁed, for any k and j we have

j x) with some
j for j = 1, . . . , m. Then, if Assumption G.4.1 is

j=1 φj(A(cid:62)

j = Ajβ0

(cid:80)m

j = Ajβk
yk
j ,

yk =

1
m

m
(cid:88)

j=1

yk
j =

1
m

Aβk,

j = Ajβ(cid:63)
y(cid:63)
j ,

y(cid:63) =

1
m

m
(cid:88)

j=1

y(cid:63)
j =

1
m

Aβ(cid:63)

with some vectors βk
1)(cid:62), . . . , (β(cid:63)
((β(cid:63)

m)(cid:62))(cid:62) and A def= [A1, . . . , Am].

i , β(cid:63)

i ∈ Rdi with i = 1, . . . , m, βk def= ((βk

1 )(cid:62), . . . , (βk

m)(cid:62))(cid:62), β(cid:63) def=

Proof. By deﬁnition yk+1
j = yk
Lemma G.3.1 there exists βk+1
j − ηjAjβk+1
zk + ηjyk
The claims about y(cid:63)

j

j

and, thus, yk+1
1, . . . , y(cid:63)

j + 1
ηj

(zk − xk+1) = 1
ηj

(zk + ηjyk

∈ ∂φj(A(cid:62)

j xk+1) such that xk+1 = proxηj gj (zk + ηjyk

j = Ajβk+1

j

. Therefore, we also have yk = 1

j − xk+1). In addition, by
j ) ∈
m Aβk.
(cid:4)

m and y(cid:63) follow from Assumption G.4.1.

Now it is time to prove Theorem 7.6.5.

Theorem G.5.11 (Same as Theorem 7.6.5). Assume that f is µ-strongly convex, ψ ≡ 0,
gj(x) = φj(A(cid:62)
j x) for j = 1, . . . , m and take a method satisfying Assumption 7.5.1 with
ρ > 0. Then, if γ ≤ γmax,

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ (1 − min{ρ, ωγµ, ρA})K L0,

313

where ρA

def= λmin(A(cid:62)A) minj

(cid:17)2

(cid:16) pj
(cid:107)Aj (cid:107)

, and L0 def= (cid:107)x0 −x(cid:63)(cid:107)2 +M0 +(cid:80)m

l=1 η2

l (cid:107)y0

l −y(cid:63)

l (cid:107)2.

Proof. Lemma G.5.2 with Assumption 7.5.1 yields

Lk+1 ≤ (1 − min{ρ, ωγµ}) (cid:0)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) + Mk(cid:1) + Y k − E (cid:2)(cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2(cid:3) .

By Lemma G.5.10

Y k =

m
(cid:88)

l=1

η2
l

E (cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3) =

m
(cid:88)

l=1

η2
l

E (cid:2)(cid:107)Al(βk

l − β(cid:63)

l )(cid:107)2(cid:3) .

Since we assume ψ ≡ const, we have zk − wk = xk − γvk − γyk − (xk − γvk) = −γyk
and

(cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2 = γ2(cid:107)yk − y(cid:63)(cid:107)2

=

γ2
m2 (cid:107)A(βk − β(cid:63))(cid:107)2

≥ λmin(A(cid:62)A)

= λmin(A(cid:62)A)

γ2
m2 (cid:107)βk − β(cid:63)(cid:107)2
m
p2
(cid:88)
l
(cid:107)Al(cid:107)2 η2
(cid:18) p2
l
(cid:107)Al(cid:107)2

l=1

l

≥ λmin(A(cid:62)A) min

≥ λmin(A(cid:62)A) min

l

(cid:18) p2
l
(cid:107)Al(cid:107)2

= ρA

m
(cid:88)

l=1

l (cid:107)yk
η2

l − y(cid:63)

l (cid:107)2.

l (cid:107)Al(cid:107)2(cid:107)βk

l − β(cid:63)

l (cid:107)2

(cid:19) m
(cid:88)

l=1
(cid:19) m
(cid:88)

l=1

η2
l (cid:107)Al(cid:107)2(cid:107)βk

l − β(cid:63)

l (cid:107)2

l (cid:107)Al(βk
η2

l − β(cid:63)

l )(cid:107)2

Therefore,

Y k − E (cid:2)(cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2(cid:3) ≤ (1 − ρA)

m
(cid:88)

η2
l

l=1
≤ (1 − ρA)Y k.

E (cid:2)(cid:2)(cid:107)yk

l − y(cid:63)

l (cid:107)2(cid:3)(cid:3)

Putting the pieces together, we obtain

Lk+1 ≤ (1 − min{ρ, ωγµ, ρA})Lk,

from which it follows that Lk converges to 0 linearly. Finally, note that

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ LK ≤ (1 − min{ρ, ωγµ, ρA})kL0.

(cid:4)

314

Algorithm 17 Stochastic Decoupling Method for linearly constrained problem
Require: Stepsize γ, initial vectors x0, y0 ∈ Rd, probabilities p1, . . . , pm, oracle that gives

gradient estimates, number of steps K

1: for k = 0, 1, . . . , K − 1 do
2:
3:
4:
5:
6:
7: end for

Produce an estimate vk of ∇f (xt)
zk = proxγψ(xk − γvk − γyk)
Sample j from {1, . . . , m} with probabilities {p1, . . . , pm}
xk+1 = Πj(zk)
yk+1 = yk + pj

γ (zk − xk+1)

G.5.8 Proof of Theorem 7.6.7 (linear constraints)

Here we discuss the problem of linearly constrained minimization

{f (x) : A(cid:62)x = b}.

min
x

We split matrix A = [A1, . . . , Am] and vector b = (b(cid:62)
operator Πj(·) def= Π{x:A(cid:62)
hyperplane {x : A(cid:62)

j x=bj }(·) . Since yk
j x = bj} for any x it holds

1 , . . . , b(cid:62)

m)(cid:62) and deﬁne projection
j ∈ Range (Aj), it is orthogonal to the

Πj(x + yk

j ) = Πj(x).

This allows us to write a memory-eﬃcient version of Algorithm 11 as given in Algorithm 17.
If only a subset of functions g1, . . . , gm is linear equality constraints, then similarly the
corresponding vectors yk
j are not needed in the update, although they are still useful for
the analysis.

Here we show that if f is strongly convex and the non-smooth part is constructed of
linear constraints, then we can guarantee linear rate of convergence. Moreover, the rate
will depend only on the smallest nonzero eigenvalue of A(cid:62)A, implying that even if A(cid:62)A
is degenerate, convergence will be linear.

Theorem G.5.12 (Same as Theorem 7.6.7). Under the same assumptions as in Theo-
rem 7.6.5 and assuming, in addition, that gj(x) = χ{x : A(cid:62)

j x=bj } it holds

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ (1 − min{ρ, ωγµ, ρA})KL0

with ρA = λ+
eigenvalue of A(cid:62)A.

min(A(cid:62)A) minj

(cid:17)2

(cid:16) pj
(cid:107)Aj (cid:107)

, i.e., ρA depends only on the smallest positive

Proof. The main reason we get an improved guarantee for linear constraints is that one
can write a closed form expression for the proximal operators:

proxηj gj (x) = x − Aj(A(cid:62)

j Aj)†(A(cid:62)

j x − bj).

Assume that j was sampled at iteration k, then

315

yk+1
j =

zk + ηjyk

j − proxηj gj (zk + ηjyk
j )

(cid:17)

(cid:16)

1
ηj
= Aj(A(cid:62)

j Aj)†(A(cid:62)

j (zk + ηiyk

j ) − bj).

Therefore, for any j and k there exists a vector xk

j ∈ Rd such that

j = Aj(A(cid:62)
yk
= Aj(A(cid:62)

j Aj)†(A(cid:62)
j Aj)†A(cid:62)

j xk
j (xk

j − bj)
j − x(cid:63)),

where the second step is by the fact that x(cid:63) is from the set {x : A(cid:62)
(cid:1). Then, yk
using SVD that Range (cid:0)(A(cid:62)
j Aj)†A(cid:62)
m Aj(βk
j
(cid:1). This, in turn, implies βk − β(cid:63) ∈ Range (cid:0)A(cid:62)(cid:1), so
j ∈ Range (cid:0)A(cid:62)
with βk

j x = bj}. One can show
j − y(cid:63)
j − β(cid:63)
j )

(cid:1) = Range (cid:0)A(cid:62)

j − β(cid:63)

j = 1

j

j

(cid:107)zk − wk − (x(cid:63) − w(cid:63))(cid:107)2 = γ2(cid:107)yk − y(cid:63)(cid:107)2

=

γ2
m2 (cid:107)A(βk − β(cid:63))(cid:107)2
min(A(cid:62)A)

≥ λ+

γ2
m2 (cid:107)βk − β(cid:63)(cid:107)2.

The rest of the proof goes the same way as that of Theorem 7.6.5 in Appendix G.5.7. (cid:4)

G.5.9 Proof of Theorem 7.6.8 (smooth gj)

This is the only proof where Lemma G.5.2 is used with ﬁnite smoothness constants, i.e.,
maxj=1,...,m Lj < +∞. On the other hand, we will not use the negative square term from
Lemma G.5.2, which is rather needed in the case gj(x) = φj(A(cid:62)

j x).

Theorem G.5.13 (Same as Theorem 7.6.8). Assume that f is L-smooth and µ-strongly
convex, gj is Lj-smooth for j = 1, . . . , m and Assumption 7.5.1(b) is satisﬁed. Then,
Algorithm 11 converges as

E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤

(cid:18)

(cid:26)

1 − min

ωγµ, ρ,

ν
m(1 + ν)

(cid:27)(cid:19)K

L0,

where ν def= minj=1,...,m

1
ηj Lj

.

Proof. Following the same lines as in the proof of Theorem 7.6.5, we get a contraction in
Y k. Now we obtain it from the fact that functions g1, . . . , gm are smooth, so the recursion
is

Lk+1 ≤ (1 − ωγµ)E(cid:107)xk − x(cid:63)(cid:107)2 + (1 − ρ)Mk +

(cid:18)

(cid:26)

≤

1 − min

ωγµ, ρ,

ν
m(1 + ν)

(cid:27)(cid:19)

Lk.

(cid:18)

1 −

ν
m(1 + ν)

(cid:19)

Y k

This is suﬃcient to show the claimed result.

(cid:4)

316

G.5.10 Proof of Corollary 7.6.9 (optimal stepsize)

Corollary 7.6.9 is a statement about the optimal stepsizes for the case where g1, . . . , gm
are smooth functions.
Its proof is a mere check that the choice of stepsizes gives the
claimed complexity.

Corollary G.5.14 (Same as Corollary 7.6.9). Choose as solver for f SVRG or SAGA
without mini-batching, which satisfy Assumption 7.5.1 with γmax = 1
3n, and
def= Lg and p1 = · · · = pm. Deﬁne
consider for simplicity situation where L1 = · · · = Lm
γbest
, and set the stepsize to be γ = min{γmax, γbest}. Then the complexity
ωµmLg
to get E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ ε is

5L and ρ = 1

1√

def=

(cid:32)(cid:32)

K = O

n + m +

(cid:115)

+

m

(cid:33)

Lg
µ

L
µ

log

(cid:33)

.

1
ε

Proof. According to Theorem 7.6.8, in general, for any γ ≤ γmax the complexity to get
E (cid:2)(cid:107)xK − x(cid:63)(cid:107)2(cid:3) ≤ ε is

O

(cid:18)(cid:18) 1
ρ

+ m +

1
ωγµ

+

1
ν

(cid:19)

m

log

(cid:19)

,

1
ε

where 1
ν simpliﬁes to γLg when L1 = · · · = Lm = Lg and p1 = · · · = pm = 1
In
m .
addition, for SVRG and SAGA, ω is a constant close to 1, so we can ignore it. Since m
and 1
ρ = 3n do not depend on γ, we only need to simplify the other two terms. One of
them decreases with γ and the other increases, so the best complexity is achieved when
the two quantities are equal to each other. The corresponding equation is ωγ2µmLg = 1,
whose solution is

γ = γbest =

1
(cid:112)ωµmLg

.

Thus, we see that γbest is optimal. Moreover, if γbest ≤ γmax and γ = γbest, the two
terms in the complexity both become equal to

1
ωγbestµ

= mγbestLg =

(cid:115)

mLg
ωµ

.

However, if γbest > γmax, then γ = min{γmax, γbest} = γmax is relatively small and the
1
dominating term in the complexity is
ωγµ rather than γLgm. Therefore, the complexity
is

(cid:18)

O

n + m +

1
γmaxµ

(cid:19)

(cid:18)

= O

n + m +

(cid:19)

.

L
µ

Algorithm 18 Stochastic Decoupling Method with SVRG
Require: Stepsize γ, initial vectors x0, u0, ∇f (u0), y0

1, . . . , y0

m, y0 = 1
m

(cid:80)m

j=1 y0

j , mini-

317

batch size τ , number of steps K

1: for k = 0, 1, . . . , K − 1 do
2:
3:
4:

Sample subset S from {1, . . . , n} of size τ
vk = 1
τ
zk = proxγψ(xk − γvk − γyk)

(cid:0)∇fi(xk) − ∇fi(uk) + ∇f (uk)(cid:1)

(cid:80)

i∈S

5:

6:

7:

8:

(cid:40)

uk+1 =

xk, with probability τ
n ,
uk, with probability 1 − τ
n
Sample j from {1, . . . , m} with probabilities {p1, . . . , pm} and set ηj = γ
mpj
xk+1 = proxηj gj
yk+1
j + 1
j = yk
ηj
m (yk+1
yk+1 = yk + 1

(cid:0)zk + ηjyk
j
(zk − xk+1)
j − yk
j )

(cid:1)

9:
10: end for

Combining the two complexities into one, we get the result.

(cid:4)

G.5.11 Proof of Lemma 7.5.3 (SVRG and SAGA)

Here we consider the update rule of SVRG and SAGA with mini-batch of size τ . Follow-
ing [102], we analyze SVRG and SAGA together by treating them both as memorization
methods. More precisely, SAGA stores each gradient estimate, ∇fi(uk
i ) individually, and
SVRG stores only the reference point, uk, itself and every iteration reevaluates ∇fi(uk) for
all sampled i to compute vk. To avoid any confusion, we provide the explicit formulation
of our method with the SVRG solver in Algorithm 18.

First of all, let us show that the estimate that we use, vk, is unbiased.

Lemma G.5.15. Let us sample a set of indices S of size τ from {1, . . . , n}. Then, it
holds for

vk def=

1
τ

(cid:88)

i∈S

(cid:0)∇fi(xk) − ∇fi(uk

i ) + αk(cid:1)

that it is unbiased

E (cid:2)vk(cid:3) = E (cid:2)∇f (xk)(cid:3) .

(G.12)

Proof. Clearly, since i is sampled with probability τ

n, it holds

E (cid:2)∇fi(xk) − ∇fi(uk

i )(cid:3) = E

1
τ

n
(cid:88)

i=1

1
n

(cid:34)

(cid:34)

(∇fi(xk) − ∇fi(uk

(cid:35)
i ))

= E

∇f (xk) −

(cid:35)
∇fi(uk
i )

.

1
n

n
(cid:88)

i=1

318

Therefore, E[vk] = E[∇f (xk)].

(cid:4)

We continue our analysis with the following lemma.

Lemma G.5.16. Consider SVRG and SAGA solver for f . Assume that every fi is convex
and L-smooth and deﬁne

Mk def=

3γ2
τ

n
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(uk

i ) − ∇fi(x(cid:63))(cid:107)2(cid:3) ,

(G.13)

where for SVRG uk
the point whose gradient is saved in memory for function fi. Then,

n is the reference point at moment k and for SAGA it is

1 = · · · = uk

Mk+1 ≤

(cid:16)

1 −

(cid:17)

τ
n

Mk + 6γ2LE (cid:2)Df (xk, x(cid:63))(cid:3) .

Proof. We have for SVRG that Mk+1 changes with probability τ
1 − τ

n and with probability

n it remains the same. Then,
(cid:35)

(cid:34) n

(cid:107)∇fi(uk+1

i

) − ∇fi(x(cid:63))(cid:107)2

=

E

(cid:88)

i=1

τ
n

n
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(xk) − ∇fi(x(cid:63))(cid:107)2(cid:3) +

(cid:16)

1 −

(cid:17)

τ
n

Mk.

Similarly, for SAGA we update exactly τ out of n gradient in the memory, which leads to
the following identity:

(cid:34) n

(cid:88)

E

i=1

(cid:107)∇fi(uk+1

i

) − ∇fi(x(cid:63))(cid:107)2

(cid:35)

(cid:34)

(cid:88)

(cid:107)∇fi(uk+1

i

) − ∇fi(x(cid:63))(cid:107)2

+ E

(cid:35)

i∈S
n
(cid:88)

E (cid:2)(cid:107)∇fi(xk) − ∇fi(x(cid:63))(cid:107)2(cid:3) +

i=1

= E

=

τ
n

(cid:16)

1 −

(cid:17)

τ
n

Mk.

(cid:107)∇fi(uk+1

i

) − ∇fi(x(cid:63))(cid:107)2

(cid:35)

(cid:34)

(cid:88)

i(cid:54)∈S

In both cases, we obtained the same recursion. Now let us bound the gradient diﬀerence
in the identity above:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(xk) − ∇fi(x(cid:63))(cid:107)2

(1.18)
≤

1
n

n
(cid:88)

i=1

2LDfi(xk, x(cid:63)) = 2LDf (xk, x(cid:63)).

This gives us the claimed inequality.

(cid:4)

Now let us show how the recursion looks like when Mk+1 is combined with (cid:107)wk−w(cid:63)(cid:107)2.

Lemma G.5.17. Consider the iterates of Algorithm 11 with SVRG or SAGA estimate
vk. Let f1, . . . , fn be convex and L-smooth, f be µ-strongly convex, S be a subset of

{1, . . . , n} of size τ sampled with equal probabilities, αk = 1
n
xk − γvk with

319

(cid:80)n

i=1 ∇fi(uk

i ) and wk =

vk =

1
τ

(cid:88)

i∈S

(cid:0)∇fi(xk) − ∇fi(uk

i ) + αk(cid:1)

then we have

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2 + Mk+1(cid:3) ≤ (1 − ρ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 + Mk(cid:3) ,

where w(cid:63) def= x(cid:63) − γ∇f (x(cid:63)) and ρ def= min (cid:8)γµ, τ

3n

(cid:9).

Proof. It holds

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3)
= E (cid:2)(cid:107)xk − x(cid:63) − γ(vk − ∇f (x(cid:63)))(cid:107)2(cid:3)
= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)xk − x(cid:63), E[vk | xk] − ∇f (x(cid:63))(cid:11) + γ2(cid:107)vk − ∇f (x(cid:63))(cid:107)2(cid:3)
= E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)xk − x(cid:63), ∇f (xk) − ∇f (x(cid:63))(cid:11) + γ2(cid:107)vk − ∇f (x(cid:63))(cid:107)2(cid:3) .

(G.12)

(1.20)

≤ E (cid:2)(1 − γµ)(cid:107)xk − x(cid:63)(cid:107)2 − 2γDf (xk, x(cid:63)) + γ2(cid:107)vk − ∇f (x(cid:63))(cid:107)2(cid:3) .

On the other hand, by Jensen’s and Young’s inequalities

E (cid:2)(cid:107)vk − ∇f (x(cid:63))(cid:107)2(cid:3)

(∇fi(xk) − ∇fi(uk

(cid:13)
(cid:13)
i )) + αk − ∇f (x(cid:63)))
(cid:13)
(cid:13)

2(cid:35)

(cid:13)
(cid:13)∇fi(xk) − ∇fi(x(cid:63)) + ∇fi(x(cid:63)) − ∇fi(uk

i ) + αk − ∇f (x(cid:63))(cid:13)
2
(cid:13)

(cid:35)

(cid:34)(cid:13)
1
(cid:13)
(cid:13)
τ
(cid:13)
(cid:34)

(cid:88)

i∈S

(cid:88)

= E

≤

≤

1
τ

2
τ

E

E

(cid:34)

i∈S

(cid:88)

(cid:13)∇fi(xk) − ∇fi(x(cid:63))(cid:13)
(cid:13)
2
(cid:13)

(cid:35)

i∈S

+

E

2
τ

(cid:34)

(cid:107)∇fi(x(cid:63)) − αk

i + αk − ∇f (x(cid:63))(cid:107)2

(cid:35)

(cid:88)

i∈S

(1.18)

≤ 4LE (cid:2)Df (xk, x(cid:63))(cid:3) +

2
n

n
(cid:88)

E

i=1

(cid:104)(cid:13)
(cid:13)∇fi(x(cid:63)) − αk

i + αk − ∇f (x(cid:63))(cid:13)
(cid:13)

2(cid:105)

.

Using inequality E [(cid:107)X − E [X] (cid:107)2] ≤ E [(cid:107)X(cid:107)2] that holds for any random variable X,

the second term can be simpliﬁed to

320

n
(cid:88)

E

(cid:104)(cid:13)
(cid:13)∇fi(x(cid:63)) − αk

i + αk − ∇f (x(cid:63))(cid:13)
(cid:13)

2(cid:105)

2
n

i=1
2
n

n
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(uk

i ) − ∇fi(x(cid:63))(cid:107)2(cid:3)

2τ
3n

Mk.

≤

=

Thus,

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2 + Mk+1(cid:3) ≤ (1 − γµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) +

(cid:18)(cid:16)

1 −

(cid:17)

τ
n

+

2τ
3n

(cid:19)

Mk

− 2γ (1 − 2γL − 3γL) E (cid:2)Df (xk, x(cid:63))(cid:3) .

(G.14)

The second term in the right-hand side can be dropped as 1−2γL− cL
0. In addition, ρ ≤ γµ and ρ ≤ τ

3n , so the claim follows.

nγ = 1−2γL−3γL ≤
(cid:4)

Now we are ready to prove Lemma 7.5.3.

Lemma G.5.18 (Same as Lemma 7.5.3). In SVRG and SAGA, if fi is L-smooth and
convex for all i, Assumption 7.5.1 is satisﬁed with γmax = 1

6L , ω = 1

3, ρ = 1

3n and

Mk =

3γ2
n

n
(cid:88)

i=1

E (cid:2)(cid:107)∇fi(uk

i ) − ∇fi(x(cid:63))(cid:107)2(cid:3) ,

where in SVRG uk
is
the point whose gradient is stored in memory for function fi. If f is also strongly convex,
then Assumption 7.5.1 holds with γmax = 1

i = uk is the reference point of the current loop, and in SAGA uk
i

3n and the same Mk.

5L, ω = 1, ρ = 1

Proof. Equation (G.14) gives immediately the second part of the claim.

Similarly, if γ ≤ 1

6L, from Equation G.14 we obtain by mentioning 1 − 2γL − cL

nγ =

1 − 5γL ≤ 1

6 that

(cid:34)

E

(cid:107)wk − w(cid:63)(cid:107)2 +

c
n

n
(cid:88)

i=1

(cid:107)αk+1

i − α(cid:63)

i (cid:107)2

(cid:35)

(cid:34)

≤ E

(cid:107)xk − x(cid:63)(cid:107)2 +

c
n

n
(cid:88)

i=1

(cid:107)αk

i − α(cid:63)

i (cid:107)2

−

γ
3

E (cid:2)Df (xk, x(cid:63))(cid:3) .

(cid:35)

(cid:4)

G.5.12 Proof of Lemma 7.5.4 (SGD)

321

Lemma G.5.19 (Same as Lemma 7.5.4). Assume that at an optimum x(cid:63) the variance
of stochastic gradients is ﬁnite, i.e.,

def= E (cid:2)(cid:107)∇f (x(cid:63); ξk) − ∇f (x(cid:63))(cid:107)2(cid:3) < +∞.

σ2
(cid:63)

Then, SGD that terminates after at most K iterations satisﬁes Assumption 7.5.1(a) with
γmax = 1
If f is strongly convex, it satisﬁes Assumption 7.5.1(b)
with γmax = 1

2L, ω = 1 and ρ = 0. In both cases, sequence (Mk)K

4L, ω = 1 and ρ = 0.

k=0 is given by

Mk = 2γ2(K − k)σ2
(cid:63).

Proof. Clearly, we have

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3) = E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11)(cid:3)

+ γ2E (cid:2)(cid:107)∇f (xk; ξk) − ∇f (x(cid:63))(cid:107)2(cid:3)

(G.11)

≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − 2γ (cid:10)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:11)(cid:3)

+ γ2E (cid:2)4LDf (xk, x(cid:63)) + 2σ2

(cid:63)

(cid:3)

(1.20)

≤ E (cid:2)(1 − γµ)(cid:107)xk − x(cid:63)(cid:107)2 − 2γ(1 − 2γL)Df (xk, x(cid:63))(cid:3) + 2γ2σ2
(cid:63).

If f is not strongly convex, then µ = 0 and by assuming γ ≤ γmax = 1
and

4L we get 1−2γL ≥ 1

2

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3) ≤ E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2 − γDf (xk, x(cid:63))(cid:3) + 2γ2σ2
(cid:63).

In case µ = 0, by deﬁning {Mk}K

k=0 with recursion

Mk+1 = Mk − 2γ2σ2
(cid:63),

we can verify Assumption 7.5.1(a) as long as MK = M0 − 2Kγ2σ2
reason we choose M0 = 2Kγ2σ2
(cid:63).

(cid:63) ≥ 0. This is the

On the other hand, when µ > 0 and σ(cid:63) = 0, it follows from γ ≤ γmax = 1

2L that

E (cid:2)(cid:107)wk − w(cid:63)(cid:107)2(cid:3) ≤ (1 − γµ)E (cid:2)(cid:107)xk − x(cid:63)(cid:107)2(cid:3) .

(cid:4)

G.6 Additional Experiments

Here we want to see how changing m and n aﬀects the comparison between SVRG with
exact projection and decoupled SVRG with one stochastic projection. The problem that we
consider is again (cid:96)2-regularized constrained linear regression. We took Gisette dataset from
LIBSVM, whose dimension is d = 5000, and used its ﬁrst 1000 observations to construct

322

(a) m = 100, n = 900

(b) m = 200, n = 800

(c) m = 500, n = 500

(d) m = 100, n = 900

(e) m = 200, n = 800

(f) m = 500, n = 500

Figure G.1: Comparison of SVRG with precise projection onto all constraints (labeled as
’SVRG’) to our stochastic version of SVRG (labeled as ’Double-SVRG’).

In particular, we split these observations into soft loss fi(x) = 1

i x − bi(cid:107)2
f and g.
and hard constraints gj(x) = χ{x:a(cid:62)
j x=bj } with n + m = 1000 and we considered three
choices of n: 250, 500 and 750. To make sure that the constraints can be satisﬁed, we
generated a random vector x0 from normal distribution N (0, 1√
In
d
all cases, ﬁrst part of data was used in f and the rest in g. To better see the eﬀect of
changing n, we used ﬁxed (cid:96)2 penalty of order

) and set b = Ax0.

(n+m) for all choices of n.

2(cid:107)a(cid:62)

Computing the projection of a point onto the intersection of all constraints as at least
as expensive as m individual projections and we count it as such for SVRG. In practice
it might be by orders of magnitude slower than this estimate for big matrices, but the
advantage of our method can be seen even without taking it into account. On the other
hand, to make the comparison fair in terms of computation trade-oﬀ, we use SVRG with
1
mini-batch 20 and our method with mini-batch 1. The stepsize for both methods is
(2L).

1

As we can from Figure G.1, the trade-oﬀ between projections and gradients changes
dramatically when m increases. When m = 100, which implies that the term correspond-
ing to A in the complexity is small, the diﬀerence is tremendous, partially because mini-
batching for SVRG improves only part of its complexity [86]. In the setting m = n = 500,
we see that the number of data passes taken by our method to solve the problem is a
few times bigger than than that taken by SVRG. Clearly, this happens because the term
related to A becomes dominating in the complexity and SVRG uses m = 500 times more
constraints at each iteration than our method.

020406080100120140160180Data passes10131011109107105103101||xx*||2Double-SVRGSVRG0255075100125150175200Data passes10131011109107105103101||xx*||2Double-SVRGSVRG01503004506007509001050Data passes10131011109107105103101||xx*||2Double-SVRGSVRG010002000300040005000600070008000Time10131011109107105103101||xx*||2Double-SVRGSVRG010002000300040005000600070008000Constraints passes10131011109107105103101||xx*||2Double-SVRGSVRG010002000300040005000600070008000Constraints passes10131011109107105103101||xx*||2Double-SVRGSVRG323

Appendix H

Appendix for Chapter 8

H.1 Experiments

In this section, we present numerical experiments for the PDDY, PD3O and Condat–V˜u
(CV) [52, Algorithm 3.1] algorithms. SGD was always used with a small γ, such as 0.01
ν .
where ν is the smoothness constant of f . For stochastic methods, we used a batch size of
16 for better parallelism, while the sampling type is speciﬁed in the ﬁgures. The stepsizes
were tuned with log-grid-search for all methods. We used closed-form expressions to
compute ν for all problems and tuned the stepsizes for all methods by running logarithmic
grid search with factor 1.5 over multiples of 1
ν .
We observed that the performances of these algorithms are nearly identical, when
the same stepsizes are used, so we do not provide their direct comparison in the plots.
Instead, we 1) compare diﬀerent stochastic oracles, 2) illustrate how convergence dif-
fers in functional suboptimality and distances, and 3) show how the stepsizes aﬀect the
performance.

1

PCA-Lasso In a recent work [261, Equation (12)] the following diﬃcult PCA-based
i=1 (cid:107)Lix(cid:107), where
Lasso problem was introduced: minx
W ∈ Rn×d, a ∈ Rn, λ, λ1 > 0 are given. We generate 10 matrices Li randomly with
standard normal i.i.d. entries, each with 20 rows. W and y are taken from the ’mushrooms’
dataset from the LIBSVM package [44]. We chose λ = ν
nm, where ν, the
smoothness of f , is needed to compensate for the fact that we do not normalize the
objective.

2(cid:107)Wx − a(cid:107)2 + λ(cid:107)x(cid:107)1 + λ1

10n and λ1 = 2ν

(cid:80)m

(cid:80)n

(cid:80)m

i=1 fi(x)+ λ

i x)(cid:1)(cid:1), where, λ = 2ν

i x)(cid:1)+(1−ai) log (cid:0)1−h(w(cid:62)

MNIST with Overlapping Group Lasso Now we consider the problem where
f is the (cid:96)2-regularized logistic loss and a group Lasso penalty. Given the data matrix
W ∈ Rn×d and vector of labels a ∈ {0, 1}n, f (x) = 1
2 (cid:107)x(cid:107)2 is a ﬁnite sum,
n
fi(x) = −(cid:0)ai log (cid:0)h(w(cid:62)
n , wi ∈ Rd is the i-
th row of W and h : t → 1/(1+e−t) is the sigmoid function. The non-smooth regularizer,
in turn, is given by λ1
5n, Gj ⊂ {1, . . . , p} is a given subset of
coordinates and (cid:107)x(cid:107)Gj is the (cid:96)2-norm of the corresponding block of x. To apply splitting
Gm)(cid:62), where IGj is the operator that takes x ∈ Rd and
methods, we use L = (I(cid:62)
returns only the entries from block Gj. Then, we can use H(y) = λ1
j=1 (cid:107)y(cid:107)Gj , which
is separable in y and, thus, proximable. We use the MNIST datasetw [132] of 70000 black
and white 28 × 28 images. For each pixel, we add a group of pixels Gj adjacent to it,
including the pixel itself. Since there are some border pixels, groups consist of 3, 4 or 5
coordinates, and there are 784 penalty terms in total.

j=1 (cid:107)x(cid:107)Gj , where λ1 = ν

G1, . . . , I(cid:62)

(cid:80)m

Fused Lasso Experiment

In the Fused Lasso problem, we are given a feature

324

Figure H.1: Results for the PCA-Lasso experiment. Left: convergence in the objective,
middle: convergence in norm, right: the eﬀect of the stepsizes.

Figure H.2: Results for the MNIST experiment. Left: convergence in the objective,
middle: convergence in norm, right: the eﬀect of the stepsizes.

matrix W ∈ Rn×d and an output vector a, which deﬁne the least-squares smooth objective
2 (cid:107)x(cid:107)2 and λ1(cid:107)Dx(cid:107)1, where λ = ν
2(cid:107)Wx−a(cid:107)2. This function is regularized with λ
f (x) = 1
n,
10n and D ∈ R(d−1)×d has entries Di,i = 1, Di,i+1 = −1, for i = 1, . . . , p − 1,
λ1 = ν
and Dij = 0 otherwise. We use the ’mushrooms’ dataset from the LIBSVM package.
Our numerical ﬁndings for this problem are very similar to the ones for PCA-Lasso.
In
particular, larger values of γ seem to perform signiﬁcantly better and the value of the
objective function does not oscillate, unlike in the MNIST experiment. The results are
shown in Figure H.3. The proposed Stochastic PDDY algorithm with the SAGA estimator
performs best in this setting.

Summary of results We can see from the plots that stochastic updates make the
convergence extremely faster, sometimes even without variance reduction. The stepsize
1
(cid:107)L(cid:107)2 , while the optimal value of γ might
plots suggest that it is best to keep γτ close to
sometimes be smaller than 1
ν . This is especially clearly seen from the fact that SGD works
suﬃciently fast even despite using γ inversely proportional to the number of iterations.

H.2 Proofs Related to Primal–Dual Optimality

H.2.1 Optimality conditions

Let x(cid:63) be a minimizer of Problem (8.1). Assuming a standard qualiﬁcation condition, for
instance that 0 belongs to the relative interior of dom (H) − Ldom (ψ), then for every

050100150200250300350400Data passes107105103101Objective suboptimalityPD3O-SAGAPDDY-SVRGCV-SGDPD3O050100150200250300350400Data passes104103102101100xkx*2PD3O-SAGAPDDY-SVRGCV-SGDPD3O01000020000300004000050000Iteration106105104103102101100Objective suboptimalityPD3O, =1/L, =L2PD3O, =0.2/L, =L2PD3O, =0.04/L, =L2PD3O, =0.04/L, =0.2L20102030405060Data passes108106104102Objective suboptimalityPD3O-SAGAPDDY-SVRGCV-SGDPDDY0102030405060Data passes1011109107105103101xkx*2PD3O-SAGAPDDY-SVRGCV-SGDPDDY0200040006000800010000Iteration106105104103102Objective suboptimalityPD3O, =1/L, =L2PD3O, =0.2/L, =L2PD3O, =0.04/L, =L2PD3O, =0.04/L, =0.2L2325

Figure H.3: Results for the Fused Lasso experiment. Left: convergence with respect to
the objective function, middle: convergence in norm, right:
illustration of the eﬀect of
the stepsizes.

x ∈ X ,

∂(f + ψ + H ◦ L)(x) = ∇f (x) + ∂ψ(x) + L∗∂H(Lx),

see for instance Theorem 16.47 of [15]. Then,

x(cid:63) ∈ arg min
x∈X

{f (x) + ψ(x) + H(Lx)}

⇔ 0 ∈ ∇f (x(cid:63)) + ∂ψ(x(cid:63)) + L∗∂H(Lx(cid:63))
⇔ ∃y(cid:63) ∈ ∂H(Lx(cid:63)) such that 0 ∈ ∇f (x(cid:63)) + ∂ψ(x(cid:63)) + L∗y(cid:63)
⇔ ∃y(cid:63) ∈ Y such that 0 ∈ ∇f (x(cid:63)) + ∂ψ(x(cid:63)) + L∗y(cid:63) and 0 ∈ −Lx(cid:63) + ∂H ∗(y(cid:63)),

where we used ∂H ∗ = (∂H)−1.

H.2.2 Proof of Lemma 8.3.1

Using the optimality conditions (8.7), we have

Df (x, x(cid:63)) + Dψ(x, x(cid:63)) = (f + ψ)(x) − (f + ψ)(x(cid:63)) − (cid:104)∇f (x(cid:63)) + r(cid:63), x − x(cid:63)(cid:105)

= (f + ψ)(x) − (f + ψ)(x(cid:63)) + (cid:104)L∗y(cid:63), x − x(cid:63)(cid:105)
= (f + ψ)(x) − (f + ψ)(x(cid:63)) + (cid:104)y(cid:63), Lx(cid:105) − (cid:104)y(cid:63), Lx(cid:63)(cid:105).

We also have

DH ∗(y, y(cid:63)) = H ∗(y) − H ∗(y(cid:63)) − (cid:104)h(cid:63), y − y(cid:63)(cid:105)

= H ∗(y) − H ∗(y(cid:63)) − (cid:104)Lx(cid:63), y − y(cid:63)(cid:105)
= H ∗(y) − H ∗(y(cid:63)) − (cid:104)Lx(cid:63), y(cid:105) + (cid:104)y(cid:63), Lx(cid:63)(cid:105).

Summing the two last equations, we have

Df (x, x(cid:63)) + Dψ(x, x(cid:63)) + DH ∗(y, y(cid:63))

= (f + ψ)(x) − (f + ψ)(x(cid:63)) + H ∗(y) − H ∗(y(cid:63)) − (cid:104)Lx(cid:63), y(cid:105) + (cid:104)y(cid:63), Lx(cid:105)
= L(x, y(cid:63)) − L(x(cid:63), y).

050100150200250300350400Data passes1010108106104102100Objective suboptimalityPD3O-SVRGPDDY-SAGACV-SGDPD3O050100150200250300350400Data passes107106105104103102101100xkx*2PD3O-SVRGPDDY-SAGACV-SGDPD3O0100002000030000400005000060000Iteration1010108106104102100Objective suboptimalityCV, =1/L, =L2CV, =0.2/L, =L2CV, =0.04/L, =L2CV, =0.04/L, =0.2L2H.3 Proof of Lemma 8.4.2

326

Since zk = Jγ ˜B(vk), zk ∈ vk − γ ˜B(zk) by deﬁnition of the resolvent operator. Therefore,
there exists bk ∈ ˜B(zk) such that zk = vk − γbk. Similarly,

uk+1 ∈ 2zk − vk − γ ˜C(zk) − γ ˜A(uk+1) = vk − 2γbk − γ ˜C(zk) − γ ˜A(uk+1).

Therefore, there exists ak+1 ∈ ˜A(uk+1) such that






zk = vk − γbk
uk+1 = vk − 2γbk − γ ˜C(zk) − γak+1
vk+1 = vk + uk+1 − zk.

Moreover,

vk+1 = vk − γbk − γ ˜C(zk) − γak+1.

Similarly, there exist a(cid:63) ∈ ˜A(u(cid:63)), b(cid:63) ∈ ˜B(z(cid:63)) such that






z(cid:63) = v(cid:63) − γb(cid:63)
u(cid:63) = v(cid:63) − 2γb(cid:63) − γ ˜C(z(cid:63)) − γa(cid:63)
v(cid:63) = v(cid:63) + u(cid:63) − z(cid:63),

v(cid:63) = v(cid:63) − γb(cid:63) − γ ˜C(z(cid:63)) − γa(cid:63).

and

(H.1)

(H.2)

(H.3)

(H.4)

Therefore, using (H.2) and (H.4),

(cid:107)vk+1 − v(cid:63)(cid:107)2 = (cid:107)vk − v(cid:63)(cid:107)2 − 2γ(cid:104)ak+1 + bk + ˜C(zk) −

(cid:16)

a(cid:63) + b(cid:63) + ˜C(z(cid:63))

(cid:17)

, vk − v(cid:63)(cid:105)

+ γ2(cid:107)ak+1 + bk + ˜C(zk) −

(cid:16)

a(cid:63) + b(cid:63) + ˜C(z(cid:63))

(cid:17)

(cid:107)2.

By expanding the last square at the right-hand side, and by using (H.1) and (H.3) in the
inner product, we get

(cid:107)vk+1 − v(cid:63)(cid:107)2 = (cid:107)vk − v(cid:63)(cid:107)2

− 2γ(cid:104)bk + ˜C(zk) −

(cid:16)

b(cid:63) + ˜C(z(cid:63))

− 2γ(cid:104)ak+1 − a(cid:63), uk+1 − u(cid:63)(cid:105)
− 2γ(cid:104)bk + ˜C(zk) −

(cid:16)

b(cid:63) + ˜C(z(cid:63))

(cid:17)

(cid:17)

, zk − z(cid:63)(cid:105)

, γbk − γb(cid:63)(cid:105)
(cid:16)

− 2γ(cid:104)ak+1 − a(cid:63), 2γbk + γ ˜C(zk) + γak+1 −

2γb(cid:63) + γ ˜C(z(cid:63)) + γa(cid:63)(cid:17)

(cid:105)

+ γ2(cid:107)ak+1 + bk − (a(cid:63) + b(cid:63)) (cid:107)2
+ γ2(cid:107) ˜C(zk) − ˜C(z(cid:63))(cid:107)2
+ 2γ2(cid:104)ak+1 + bk − (a(cid:63) + b(cid:63)) , ˜C(zk) − ˜C(z(cid:63))(cid:105).

Then, the last ﬁve terms at the right-hand side simplify to

327

γ2(cid:107) ˜C(zk) − ˜C(z(cid:63))(cid:107)2 − γ2(cid:107)ak+1 + bk − (a(cid:63) + b(cid:63)) (cid:107)2,

and we get the result.

H.4 Proofs Related to the PDDY and PD3O Algorithms

H.4.1 Resolvent calculus

For the sake of completeness, we reproduce a resolvent computation that can be found
in [54], showing that the PD3O algorithm is an instance of DYS.

In the notations of Section 8.5, let us state the lemma:

Lemma H.4.1. JγP−1A maps (x, y) to (x(cid:48), y(cid:48)), such that

(cid:22) y(cid:48) = proxτ H ∗

(cid:0)y + τ L(x − γL∗y)(cid:1)

x(cid:48) = x − γL∗y(cid:48).

Proof. Let (x, y) and (x(cid:48), y(cid:48)) ∈ Z, such that

where

(cid:21)

(cid:20)x(cid:48) − x
y(cid:48) − y

P

(cid:20)

∈ −γ

+ L∗y(cid:48)

−Lx(cid:48) + ∂H ∗(y(cid:48))

(cid:21)

,

P =

(cid:20)I
0 γ

0
τ I − γ2LL∗

(cid:21)

.

We shall express (x(cid:48), y(cid:48)) as a function of (x, y). First,

x(cid:48) = x − γL∗y(cid:48).

Moreover, y(cid:48) is given by
I − γ2LL∗(cid:17)

(cid:16) γ
τ

(y(cid:48)) ∈

I − γ2LL∗(cid:17)
I − γ2LL∗(cid:17)

(cid:16) γ
τ
(cid:16)γ
τ

(y) + γLx(cid:48) − γ∂H ∗(y(cid:48))

(y) + γL (x − γL∗y(cid:48)) − γ∂H ∗(y(cid:48)).

∈

Therefore, the term γ2LL∗y(cid:48) disappears from both sides and

Finally,

and

y(cid:48) ∈ y − γτ LL∗y − τ ∂H ∗(y(cid:48)) + τ Lx.

y − γτ LL∗y + τ Lx ∈ y(cid:48) + τ ∂H ∗(y(cid:48)),

y(cid:48) = proxτ H ∗(y − γτ LL∗y + τ Lx).

(H.5)

(cid:4)

H.5 Proofs Related to the Condat–V˜u Algorithm

328

H.5.1 Resolvent calculus

The results of this section rely on the following resolvent computation, which is new to
our knowledge.

In the notations of Section 8.5.3, let us state the lemma:

Lemma H.5.1. JγQ−1A maps (x, y) to (x(cid:48), y(cid:48)), such that

(cid:22) x(cid:48) = proxτ ψ

(cid:0)(I − τ γL∗L)x − τ L∗y(cid:1),

y(cid:48) = y + γLx(cid:48).

Proof. Let (x, y) and (x(cid:48), y(cid:48)) ∈ Z be such that

where

(cid:21)

(cid:20)x(cid:48) − x
y(cid:48) − y

Q

∈ −γ

(cid:20) ∂ψ(x(cid:48)) + L∗y(cid:48)
−Lx(cid:48)

(cid:21)

,

Q =

(cid:21)
(cid:20) γ
τ I − γ2L∗L 0
I

0

.

We shall express (x(cid:48), y(cid:48)) as a function of (x, y). First,

y(cid:48) = y + γLx(cid:48).

Moreover, x(cid:48) is given by

I − γ2L∗L

(cid:17)

(x(cid:48)) ∈

(cid:16)γ
τ

∈

I − γ2L∗L

I − γ2L∗L

(cid:17)

(cid:17)

(cid:16) γ
τ
(cid:16)γ
τ

(x) − γ∂ψ(x(cid:48)) − γL∗y(cid:48)

(x) − γ∂ψ(x(cid:48)) − γL∗y − γ2L∗Lx(cid:48).

Therefore, the term γ2L∗Lx(cid:48) disappears from both sides and

Finally,

and

x(cid:48) ∈ x − γτ L∗Lx − τ ∂ψ(x(cid:48)) − τ L∗y.

x − γτ L∗Lx − τ L∗y ∈ x(cid:48) + τ ∂ψ(x(cid:48)),

x(cid:48) = proxτ ψ(x − γτ L∗Lx − τ L∗y).

(H.6)

(cid:4)

H.5.2 Algorithm 3.2 of [52] as an instance of Davis–Yin Splitting

We apply DY S(Q−1B, Q−1A, Q−1C):

329













(cid:16)

xk = pk
yk = proxγH ∗(qk)
sk+1 = proxτ ψ
hk+1 = 2yk − qk + γLsk+1
pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

(I − γτ L∗L)(cid:0)2xk − pk − γ( γ

τ I − γ2L∗L)−1∇f (xk)(cid:1) − τ L∗ (cid:0)2yk − qk(cid:1) (cid:17)

Note that pk = sk = xk and qk+1 = yk + γLxk+1. So,

xk+1 = proxτ ψ
= proxτ ψ

(cid:0)xk − τ ∇f (xk) − τ L∗(2yk − qk + γLxk)(cid:1),
(cid:0)xk − τ ∇f (xk) − τ L∗(2yk − yk−1)(cid:1),

and

The sequence (yk, xk+1) follows the updates of Algorithm 3.2 in [52].

yk = proxγH ∗(yk−1 + γLxk).

H.5.3 Algorithm 3.1 of [52] as an instance of Davis–Yin Splitting

We apply DY S(Q−1A, Q−1B, Q−1C), which yields:

(cid:0)(I − γτ L∗L)pk − τ L∗qk(cid:1)












xk = proxτ ψ
yk = qk + γLxk
sk+1 = 2xk − pk − γ( γ
hk+1 = proxγH ∗(2yk − qk)
pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

τ I − γ2L∗L)−1∇f (xk)

Note that pk+1 = xk − ( 1

xk = proxτ ψ
= proxτ ψ

and

τ I − γL∗L)−1∇f (xk) and qk+1 = hk+1 − γLxk. Thus,
(cid:0)xk−1 − γτ L∗Lxk−1 − τ ∇f (xk−1) − τ L∗(hk − γLxk−1)(cid:1)
(cid:0)xk−1 − τ ∇f (xk−1) − τ L∗hk(cid:1),

hk+1 = proxγH ∗(2yk − qk)

= proxγH ∗(qk + 2γLxk)
= proxγH ∗(hk + γL(2xk − xk−1)).

The sequence (xk, hk+1) follows the updates of Algorithm 3.1 of [52].

H.5.4 Cocoercivity parameter of Q−1C

330

Since K is positive deﬁnite, (cid:107)K−1/2(cid:107)2 = (cid:107)K−1(cid:107). Let z = (x, y), z(cid:48) = (x(cid:48), y(cid:48)) ∈ Z. Then,

(cid:107)Q−1C(z) − Q−1C(z(cid:48))(cid:107)2

Q = (cid:107)K−1∇f (x) − K−1∇f (x(cid:48))(cid:107)2
K

= (cid:107)K−1/2∇f (x) − K−1/2∇f (x(cid:48))(cid:107)2
≤ (cid:107)K−1/2(cid:107)2(cid:107)∇f (x) − ∇f (x(cid:48))(cid:107)2
= (cid:107)K−1(cid:107)(cid:107)∇f (x) − ∇f (x(cid:48))(cid:107)2
≤ (cid:107)K−1(cid:107)ν(cid:104)∇f (x) − ∇f (x(cid:48)), x − x(cid:48)(cid:105)
= (cid:107)K−1(cid:107)ν(cid:104)K−1∇f (x) − K−1∇f (x(cid:48)), x − x(cid:48)(cid:105)K
= (cid:107)K−1(cid:107)ν(cid:104)Q−1C(z) − Q−1C(z(cid:48)), z − z(cid:48)(cid:105)Q.

Since (cid:107)K−1(cid:107) is the inverse of the smallest eigenvalue of K,

(cid:107)K−1(cid:107) =

1
τ − γ2(cid:107)L(cid:107)2 .

γ

Therefore, Q−1C is ξ =
Lemma 8.4.1 is equivalent to ν/2 < 1
Theorem 3.1 in [52].

γ
τ −γ2(cid:107)L(cid:107)2
ν

-cocoercive. Moreover, the condition γ < 2ξ of
τ − γ(cid:107)L(cid:107)2, which is exactly the condition (i) of

H.6 Linear Convergence Results

In this section, we provide linear convergence results for the Stochastic PD3O and the
Stochastic PDDY algorithm, in addition to Theorem 8.6.5.

About the linear convergence of DYS( ˜A, ˜B, ˜C).
In general, operator splitting meth-
ods like DYS( ˜A, ˜B, ˜C) require ˜A + ˜B + ˜C to be strongly monotone to converge linearly.
Besides, to converge linearly in general, DYS( ˜A, ˜B, ˜C) requires the stronger assumption
that ˜A or ˜B or ˜C is strongly monotone 1 and that ˜A or ˜B is cocoercive2, see [60].

Recall that the PDDY algorithm is equivalent to DYS(P−1B, P−1A, P−1C) and the
PD3O algorithm is equivalent to DYS(P−1A, P−1B, P−1C), see Section 8.5. However,
P−1A, P−1B and P−1C are not strongly monotone.
In spite of this, we shall prove
the linear convergence of the (stochastic) PDDY algorithm and the (stochastic) PD3O
algorithm.

Our assumptions. For both algorithms, we shall make the weaker assumption that
P−1A+P−1B+P−1C is strongly monotone (which turns out to be equivalent to assuming
M = A + B + C strongly monotone, i.e., f + ψ strongly convex and H smooth). Indeed,
the algorithms need to be contractive in both the primal and the dual space. For instance,
Chambolle–Pock [41, 43] algorithm, which is a particular case of the PD3O and the PDDY

1This assumption is stronger than assuming ˜A + ˜B + ˜C strongly monotone.
2For linear convergence, this assumption is proved to be necessary in general in [60].

331

algorithms, requires ψ strongly convex and H smooth to converge linearly in general.
Therefore, we shall assume M strongly monotone for both algorithms, which is weaker
than assuming P−1A, P−1B or P−1C strongly monotone.

Moreover, for the PD3O algorithm we shall add a cocoercivity assumption, as sug-
gested by the general linear convergence theory of DYS. More precisely, we shall assume
ψ smooth (i.e., P−1B cocoercive). Our ﬁrst result is therefore an extension of [282,
Theorem 3] to the stochastic setting.

For the PDDY algorithm, we shall not make any cocoercivity assumption on P−1A
and P−1B, but we shall make an assumption of the stepsize instead. Our second result
is new, even in the deterministic case gk+1 = ∇f (xk).
We denote by (cid:107) · (cid:107)γ,τ the norm induced by γ

τ I − γ2LL∗ on Y.

H.6.1 The Stochastic PD3O algorithm

Theorem H.6.1 (M strongly monotone and ψ smooth). Suppose that Assumption 8.6.1
holds. Also, suppose that H is 1/µH ∗-smooth, f is µf -strongly convex, and ψ is µψ-
strongly convex and λ-smooth, where µ def= µf + 2µψ > 0 and µH ∗ > 0. For every
κ > β/ρ and every γ, τ > 0 such that γ ≤ 1/(α + κδ) and γτ (cid:107)L(cid:107)2 < 1, deﬁne

and

Then,

V k def= (cid:107)pk − p(cid:63)(cid:107)2 + (1 + 2τ µH ∗) (cid:107)yk − y(cid:63)(cid:107)2

γ,τ + κγ2σ2
k,

r def= max

(cid:18)

1 −

(cid:18)

γµ
(1 + γλ)2 ,

1 − ρ +

(cid:19)

β
κ

,

1
1 + 2τ µH ∗

(cid:19)

.

E (cid:2)V k(cid:3) ≤ rkV 0.

(H.7)

(H.8)

(H.9)

Under smoothness and strong convexity assumptions, Theorem H.6.1 implies linear
convergence of the dual variable yk to y(cid:63), with convergence rate given by r. Since
(cid:107)xk − x(cid:63)(cid:107) ≤ (cid:107)pk − p(cid:63)(cid:107), Theorem H.6.1 also implies linear convergence of the primal
variable xk to x(cid:63), with same convergence rate. For primal–dual algorithms to converge
linearly on Problem (8.1) with any L, it seems unavoidable that the primal term f + ψ
is strongly convex and that the dual term H ∗ is strongly convex too; this means that H
must be smooth. We can notice that if H is smooth, it is tempting to use its gradient
instead of its proximity operator. We can then use the proximal gradient algorithm with
∇(f + H ◦ L)(x) = ∇f (x) + L∗∇H(Lx). However, in practice, it is often faster to use
the proximity operator instead of the gradient, see a recent analysis of this topic [48].

Remark 3 (Particular case). In the case where ψ = H = 0 and L = 0, then the
Stochastic PD3O algorithm boils down to Stochastic Gradient Descent (SGD), where
the stochastic gradient oracle satisﬁes Assumption 8.6.1. Moreover, the value of r boils
(cid:1)(cid:1). Consider the applications of SGD covered
down to r = max (cid:0)1 − γµ, (cid:0)1 − ρ + β
by Assumption 8.6.1, and mentioned in Sect. 8.6. Then, as proved in [81], the value
r = max (cid:0)1 − γµ, (cid:0)1 − ρ + β
(cid:1)(cid:1) matches the best known convergence rates for these ap-
plications, with an exception for some coordinate descent algorithms. However, if H = 0
and L = 0 but ψ (cid:54)= 0, then the Stochastic PD3O algorithm boils down to Proximal

κ

κ

(cid:16)

332
(1+γλ)2 , (cid:0)1 − ρ + β

1 − γµ

(cid:1)(cid:17)

, whereas the best known
SGD, and r boils down to r = max
rates for Proximal SGD under Assumption 8.6.1 is max (cid:0)1 − γµ, (cid:0)1 − ρ + β
(cid:1)(cid:1). Finally, if
gk+1 = ∇f (xk), the Stochastic PD3O algorithm boils down to the PD3O algorithm and
Theorem H.6.1 provides a convergence rate similar to Theorem 3 in [282]. In this case,
by taking κ = 1, we obtain

κ

κ

(cid:18)

r = max

1 − γ

µf + 2µψ
(1 + γλ)2 ,

1
1 + 2τ µH ∗

(cid:19)

,

whereas Theorem 3 in [282] provides the rate3

(cid:18)

max

1 − γ

2(µf + µψ) − γαµf
(1 + γλ)2

,

1
1 + 2τ µH ∗

(cid:19)

.

H.6.2 The Stochastic PDDY algorithm

Theorem H.6.2 (M strongly monotone). Suppose that Assumption 8.6.1 holds. Also,
suppose that H is 1/µH ∗-smooth, f is µf -strongly convex and ψ is µψ-strongly convex,
where µψ > 0 and µH ∗ > 0. For every κ > β/ρ and every γ, τ > 0 such that γ ≤
1/(α + κδ), γτ (cid:107)L(cid:107)2 < 1 and γ2 ≤ µH∗
(cid:107)L(cid:107)2µψ

, deﬁne η def= 2 (µH ∗ − γ2(cid:107)L(cid:107)2µψ) ≥ 0,

V k def= (1 + γµψ)(cid:107)pk − p(cid:63)(cid:107)2 + (1 + τ η)(cid:107)yk − y(cid:63)(cid:107)2

γ,τ + κγ2σ2
k,

(H.10)

and

Then,

r def= max

(cid:18)

1
1 + γµψ

, 1 − ρ +

(cid:19)

β
κ

,

1
1 + τ η

E (cid:2)V k(cid:3) ≤ rkV 0.

(H.11)

(H.12)

Theorem H.6.2 provides a linear convergence result for the Stochastic PDDY algo-

rithm, without assuming ψ smooth.

H.7 Proofs Related to the Stochastic PDDY Algorithm

Recall that the PDDY algorithm is equivalent to DYS(P−1B, P−1A, P−1C). We denote
by vk = (pk, qk), zk = (xk, yk), uk = (sk, hk) the iterates of DYS(P−1B, P−1A, P−1C),
where pk, xk, sk ∈ X and qk, yk, hk ∈ Y.

Using (8.13), the step

zk = JγP−1A(vk),

is equivalent to

(cid:22) xk = pk − γL∗yk
yk = proxτ H ∗

(cid:0)(I − τ γLL∗)qk + τ Lpk(cid:1).

3The reader might not recognize the rate given in Theorem 3 of [282] because of some typos in its

Equation (39).

Then, the step

is equivalent to

Finally, the step

is equivalent to

333

uk+1 = JγP−1B

(cid:0)2zk − vk − γP−1C(zk)(cid:1)

(cid:22) sk+1 = proxγψ

(cid:0)2xk − pk − γ∇f (xk)(cid:1)

hk+1 = 2yk − qk.

vk+1 = vk + uk+1 − zk

(cid:22) pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

Similarly, the ﬁxed points v(cid:63) = (p(cid:63), q(cid:63)), z(cid:63) = (x(cid:63), y(cid:63)), u(cid:63) = (s(cid:63), h(cid:63)) of DYS(P−1B, P−1A, P−1C)
satisfy






(cid:0)(I − τ γLL∗)q(cid:63) + τ Lp(cid:63)(cid:1)
(cid:0)2x(cid:63) − p(cid:63) − γ∇f (x(cid:63))(cid:1)

x(cid:63) = p(cid:63) − γL∗y(cid:63)
y(cid:63) = proxτ H ∗
s(cid:63) = proxγψ
h(cid:63) = 2y(cid:63) − q(cid:63)
p(cid:63) = p(cid:63) + s(cid:63) − x(cid:63)
q(cid:63) = q(cid:63) + h(cid:63) − y(cid:63),

and the iterates of the Stochastic PDDY algorithm satisfy












(cid:0)(I − τ γLL∗)qk + τ Lpk(cid:1)
(cid:0)2xk − pk − γgk+1(cid:1)

xk = pk − γL∗yk
yk = proxτ H ∗
sk+1 = proxγψ
hk+1 = 2yk − qk
pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

Lemma H.7.1. Suppose that (gk)k satisﬁes Assumption 8.6.1. Then, the iterates of the
Stochastic PDDY algorithm satisfy

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− 2γ(1 − γ(α + κδ))Df (xk, x(cid:63))
− 2γ(cid:104)∂H ∗(yk) − ∂H ∗(y(cid:63)), yk − y(cid:63)(cid:105)
− 2γEk

(cid:2)(cid:104)∂ψ(sk+1) − ∂ψ(s(cid:63)), sk+1 − s(cid:63)(cid:105)(cid:3) .

Proof. Applying Lemma 8.4.2 for DY S(P−1B, P−1A, P−1C) using the norm induced by

334

P, we have

(cid:107)vk+1 − v(cid:63)(cid:107)2

P = (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)P−1A(zk) − P−1A(z(cid:63)), zk − z(cid:63)(cid:105)P
− 2γ(cid:104)P−1C(zk) − P−1C(z(cid:63)), zk − z(cid:63)(cid:105)P
− 2γ(cid:104)P−1B(uk+1) − P−1B(u(cid:63)), uk+1 − u(cid:63)(cid:105)P
+ γ2(cid:107)P−1C(zk) − P−1C(z(cid:63))(cid:107)2
P
− γ2(cid:107)P−1B(uk+1) + P−1A(zk) − (cid:0)P−1B(u(cid:63)) + P−1A(z(cid:63))(cid:1) (cid:107)2

P

= (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)A(zk) − A(z(cid:63)), zk − z(cid:63)(cid:105)
− 2γ(cid:104)C(zk) − C(z(cid:63)), zk − z(cid:63)(cid:105)
− 2γ(cid:104)B(uk+1) − B(u(cid:63)), uk+1 − u(cid:63)(cid:105)
+ γ2(cid:107)P−1C(zk) − P−1C(z(cid:63))(cid:107)2
P
− γ2(cid:107)P−1B(uk+1) + P−1A(zk) − (cid:0)P−1B(u(cid:63)) + P−1A(z(cid:63))(cid:1) (cid:107)2
P.

Using

A(zk) =

(cid:20)

(cid:21)
−Lxk + ∂H ∗(yk)

L∗yk

,

B(uk+1) =

(cid:21)
(cid:20)∂ψ(sk+1)
0

,

C(zk) =

(cid:21)

(cid:20)gk+1
0

,

and

A(z(cid:63)) =

(cid:21)
(cid:20)
−Lx(cid:63) + ∂H ∗(y(cid:63))

L∗y(cid:63)

,

B(u(cid:63)) =

(cid:21)
(cid:20)∂ψ(s(cid:63))
0

,

C(z(cid:63)) =

(cid:21)
(cid:20)∇f (x(cid:63))
0

,

we have,

(cid:107)vk+1 − v(cid:63)(cid:107)2

P ≤ (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)∂H ∗(yk) − ∂H ∗(y(cid:63)), yk − y(cid:63)(cid:105)
− 2γ(cid:104)gk+1 − ∇f (x(cid:63)), xk − x(cid:63)(cid:105)
− 2γ(cid:104)∂ψ(sk+1) − ∂ψ(s(cid:63)), sk+1 − s(cid:63)(cid:105)
+ γ2(cid:107)gk+1 − ∇f (x(cid:63))(cid:107)2.

Applying the conditional expectation w.r.t. Fk and using Assumption 8.6.1,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P

− 2γ(cid:104)∂H ∗(yk) − ∂H ∗(y(cid:63)), yk − y(cid:63)(cid:105)
− 2γ(cid:104)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:105)
− 2γEk
+ γ2 (cid:0)2αDf (xk, x(cid:63)) + βσ2

(cid:2)(cid:104)∂ψ(sk+1) − ∂ψ(s(cid:63)), sk+1 − s(cid:63)(cid:105)(cid:3)

(cid:1) .

k

335

Using the convexity of f ,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P

(cid:2)(cid:104)∂ψ(sk+1) − ∂ψ(s(cid:63)), sk+1 − s(cid:63)(cid:105)(cid:3)

− 2γ(cid:104)∂H ∗(yk) − ∂H ∗(y(cid:63)), yk − y(cid:63)(cid:105)
− 2γEk
− 2γDf (xk, x(cid:63))
+ γ2 (cid:0)2αDf (xk, x(cid:63)) + βσ2

(cid:1) .

k

Using Assumption 8.6.1,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− 2γ(1 − γ(α + κδ))Df (xk, x(cid:63))
− 2γ(cid:104)∂H ∗(yk) − ∂H ∗(y(cid:63)), yk − y(cid:63)(cid:105)
− 2γEk

(cid:2)(cid:104)∂ψ(sk+1) − ∂ψ(s(cid:63)), sk+1 − s(cid:63)(cid:105)(cid:3) .

(cid:4)

H.7.1 Proof of Theorem 8.6.4

Using Lemma H.7.1 and the convexity of f, ψ, H ∗,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek
(cid:18)

(cid:2)σ2

(cid:3)

(cid:19)

≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

1 − ρ +

σ2
k

k+1
β
κ

− 2γ(cid:0)1 − γ(α + κδ)(cid:1) (cid:0)Df (xk, x(cid:63)) + DH ∗(yk, y(cid:63)) + Ek

(cid:2)Dψ(sk+1, s(cid:63))(cid:3)(cid:1) .

Since 1 − ρ + β/κ = 1, γ ≤ 1/2(α + κδ). Set

V k = (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2σ2
k.

Then

Ek

(cid:2)V k+1(cid:3) ≤ V k − γEk

(cid:2)Df (xk, x(cid:63)) + DH ∗(yk, y(cid:63)) + Dψ(sk+1, s(cid:63))(cid:3) .

Taking the expectation,

γE (cid:2)Df (xk, x(cid:63)) + DH ∗(yk, y(cid:63)) + Dψ(sk+1, s(cid:63))(cid:3) ≤ E (cid:2)V k(cid:3) − E (cid:2)V k+1(cid:3) .

Iterating and using the nonnegativity of V k,

γ

k−1
(cid:88)

j=0

E (cid:2)Df (xk, x(cid:63)) + DH ∗(yk, y(cid:63)) + Dψ(sk+1, s(cid:63))(cid:3) ≤ E (cid:2)V 0(cid:3) .

(H.13)

We conclude using the convexity of the Bregman divergence in its ﬁrst variable.

336
H.7.2 Proof of Theorem H.6.2

We ﬁrst use Lemma H.7.1 along with the strong convexity of ψ, H ∗. Note that yk = qk+1.
We have

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− 2γµH ∗Ek

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2(cid:3) − 2γµψEk

(cid:2)(cid:107)sk+1 − s(cid:63)(cid:107)2(cid:3) .

Note that sk+1 = pk+1 − γL∗yk. Therefore, sk+1 − s(cid:63) = (pk+1 − p(cid:63)) − γL∗(yk − y(cid:63)).
Using Young’s inequality −(cid:107)a + b(cid:107)2 ≤ − 1

2(cid:107)a(cid:107)2 + (cid:107)b(cid:107)2, we have

−Ek

(cid:2)(cid:107)sk+1 − s(cid:63)(cid:107)2(cid:3) ≤ −

1
2

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + γ2(cid:107)L(cid:107)2Ek

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2(cid:3) .

Hence,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:18)

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

1 − ρ +

P + κγ2
− 2γ (cid:0)µH ∗ − γ2(cid:107)L(cid:107)2µψ
− γµψEk

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) .

(cid:1) Ek

(cid:19)

σ2
k

β
κ
(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2(cid:3)

Set η = 2 (µH ∗ − γ2(cid:107)L(cid:107)2µψ) ≥ 0. Then

(1 + γµψ)Ek

≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + (1 + τ η)Ek
β
κ

1 − ρ +

σ2
k.

(cid:19)

(cid:18)

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2

γ,τ

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3)

Set

and

Then

V k = (1 + γµψ)(cid:107)pk − p(cid:63)(cid:107)2 + (1 + τ η)(cid:107)qk − q(cid:63)(cid:107)2

γ,τ + κγ2σ2
k

r = max

(cid:18)

1
1 + γµψ

, 1 − ρ +

β
κ

,

1
1 + τ η

(cid:19)

.

Ek

(cid:2)V k+1(cid:3) ≤ rV k.

H.8 Proofs Related to the Stochastic PD3O algorithm

Recall that the PD3O algorithm is equivalent to DYS(P−1A, P−1B, P−1C). We denote
by vk = (pk, qk), zk = (xk, yk), uk = (sk, hk) the variables in DYS(P−1A, P−1B, P−1C),
with pk, xk, sk ∈ X and qk, yk, hk ∈ Y.

Then, the step

zk = JγP−1B(vk),

337

(cid:22) xk = proxγψ(pk)

yk = qk.

uk+1 = JγP−1A(2zk − vk − γP−1C(zk)),

is equivalent to

Using (8.13), the step

is equivalent to

(cid:22) sk+1 = (2xk − pk − γ∇f (xk)) − γL∗hk+1

(cid:0)(I − γτ LL∗)(2yk − qk) + τ L(2xk − pk − ∇f (xk))(cid:1) .

hk+1 = proxτ H ∗

Finally, the step

is equivalent to

vk+1 = vk + uk+1 − zk,

(cid:22) pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

Similarly, the ﬁxed points v(cid:63) = (p(cid:63), q(cid:63)), z(cid:63) = (x(cid:63), y(cid:63)), u(cid:63) = (s(cid:63), h(cid:63)) of DYS(P−1A, P−1B, P−1C)
satisfy












x(cid:63) = proxγψ(p(cid:63))
y(cid:63) = q(cid:63)
s(cid:63) = (2x(cid:63) − p(cid:63) − γ∇f (x(cid:63))) − γL∗h(cid:63)
h(cid:63) = proxτ H ∗ ((I − γτ LL∗)(2y(cid:63) − q(cid:63)) + τ L(2x(cid:63) − p(cid:63) − ∇f (x(cid:63))))
p(cid:63) = p(cid:63) + s(cid:63) − x(cid:63)
q(cid:63) = q(cid:63) + h(cid:63) − y(cid:63).

and the iterates of the Stochastic PD3O algorithm satisfy












xk = proxγψ(pk)
yk = qk
sk+1 = (2xk − pk − γgk+1) − γL∗hk+1
hk+1 = proxτ H ∗
pk+1 = pk + sk+1 − xk
qk+1 = qk + hk+1 − yk.

(cid:0)(I − γτ LL∗)(2yk − qk) + τ L(2xk − pk − gk+1)(cid:1)

Lemma H.8.1. Assume that f is µf -strongly convex, for some µf ≥ 0, and that (gk)k

338

satisﬁes Assumption 8.6.1. Then, the iterates of the Stochastic PD3O algorithm satisfy

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− 2γ(1 − γ(α + κδ))Df (xk, x(cid:63)) − γµf (cid:107)xk − x(cid:63)(cid:107)2
− 2γ(cid:104)∂ψ(xk) − ∂ψ(x(cid:63)), xk − x(cid:63)(cid:105)
(H.14)
− 2γEk
− γ2Ek

(cid:2)(cid:104)∂H ∗(hk+1) − ∂H ∗(h(cid:63)), hk+1 − h(cid:63)(cid:105)(cid:3)
(cid:104)(cid:13)
(cid:13)P−1A(uk+1) + P−1B(zk)
− (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:13)
2
(cid:13)
P

(cid:105)

.

Proof. Applying Lemma 8.4.2 for DY S(P−1A, P−1B, P−1C) using the norm induced by
P we have

(cid:107)vk+1 − v(cid:63)(cid:107)2

P = (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)P−1B(zk) − P−1B(z(cid:63)), zk − z(cid:63)(cid:105)P
− 2γ(cid:104)P−1C(zk) − P−1C(z(cid:63)), zk − z(cid:63)(cid:105)P
− 2γ(cid:104)P−1A(uk+1) − P−1A(u(cid:63)), uk+1 − u(cid:63)(cid:105)P
+ γ2(cid:107)P−1C(zk) − P−1C(z(cid:63))(cid:107)2
P
− γ2(cid:107)P−1A(uk+1) + P−1B(zk) − (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:107)2

P

= (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)B(zk) − B(z(cid:63)), zk − z(cid:63)(cid:105)
− 2γ(cid:104)C(zk) − C(z(cid:63)), zk − z(cid:63)(cid:105)
− 2γ(cid:104)A(uk+1) − A(u(cid:63)), uk+1 − u(cid:63)(cid:105)
+ γ2(cid:107)P−1C(zk) − P−1C(z(cid:63))(cid:107)2
P
− γ2(cid:107)P−1A(uk+1) + P−1B(zk) − (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:107)2
P.

Using

A(uk+1) =

(cid:20)

(cid:21)
−Lsk+1 + ∂H ∗(hk+1)

L∗hk+1

, B(zk) =

(cid:21)

(cid:20)∂ψ(xk)
0

, C(zk) =

(cid:21)

(cid:20)gk+1
0

,

and

(cid:20)

A(u(cid:63)) =

L∗h(cid:63)

−Ls(cid:63) + ∂H ∗(h(cid:63))

(cid:21)

,

B(z(cid:63)) =

(cid:21)
(cid:20)∂ψ(x(cid:63))
0

,

C(z(cid:63)) =

(cid:21)

(cid:20)∇f (x(cid:63))
0

,

339

we derive

(cid:107)vk+1 − v(cid:63)(cid:107)2

P = (cid:107)vk − v(cid:63)(cid:107)2
P

− 2γ(cid:104)∂ψ(xk) − ∂ψ(x(cid:63)), xk − x(cid:63)(cid:105)
− 2γ(cid:104)gk+1 − ∇f (x(cid:63)), xk − x(cid:63)(cid:105)
− 2γ(cid:104)∂H ∗(hk+1) − ∂H ∗(h(cid:63)), hk+1 − h(cid:63)(cid:105)
+ γ2(cid:107)gk+1 − ∇f (x(cid:63))(cid:107)2
− γ2(cid:107)P−1A(uk+1) + P−1B(zk) − (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:107)2
P.

Taking conditional expectation w.r.t. Fk and using Assumption 8.6.1,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P

− 2γ(cid:104)∂ψ(xk) − ∂ψ(x(cid:63)), xk − x(cid:63)(cid:105)
− 2γ(cid:104)∇f (xk) − ∇f (x(cid:63)), xk − x(cid:63)(cid:105)
− 2γEk
+ γ2 (cid:0)2αDf (xk, x(cid:63)) + βσ2
− γ2Ek

(cid:2)(cid:104)∂H ∗(hk+1) − ∂H ∗(h(cid:63)), hk+1 − h(cid:63)(cid:105)(cid:3)
(cid:1)

k

(cid:2)(cid:107)P−1A(uk+1) + P−1B(zk) − (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:107)2

P

Using strong convexity of f ,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P
− γµf (cid:107)xk − x(cid:63)(cid:107)2
− 2γDf (xk, x(cid:63))
+ γ2 (cid:0)2αDf (xk, x(cid:63)) + βσ2
− 2γ(cid:104)∂ψ(xk) − ∂ψ(x(cid:63)), xk − x(cid:63)(cid:105)
− 2γEk
− γ2Ek

(cid:1)

k

(cid:2)(cid:104)∂H ∗(hk+1) − ∂H ∗(h(cid:63)), hk+1 − h(cid:63)(cid:105)(cid:3)
(cid:2)(cid:107)P−1A(uk+1) + P−1B(zk) − (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:107)2

P

(cid:3) .

(cid:3) .

Using Assumption 8.6.1,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− γµf (cid:107)xk − x(cid:63)(cid:107)2
− 2γ(1 − γ(α + κδ))Df (xk, x(cid:63))
− 2γ(cid:104)∂ψ(xk) − ∂ψ(x(cid:63)), xk − x(cid:63)(cid:105)
− 2γEk
− γ2Ek

(cid:2)(cid:104)∂H ∗(hk+1) − ∂H ∗(h(cid:63)), hk+1 − h(cid:63)(cid:105)(cid:3)
(cid:104)(cid:13)
(cid:13)P−1A(uk+1) + P−1B(zk)
− (cid:0)P−1A(u(cid:63)) + P−1B(z(cid:63))(cid:1) (cid:13)
2
(cid:13)
P

(cid:105)
.

340

(cid:4)

H.8.1 Proof of Theorem 8.6.3

Using Lemma H.8.1, convexity of f, ψ, H ∗, and Lemma 8.3.1,

Ek

(cid:2)(cid:107)vk+1 − v(cid:63)(cid:107)2

P

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2

(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− 2γ(1 − γ(α + κδ))Ek

(cid:2)L(xk, h(cid:63)) − L(x(cid:63), hk+1)(cid:3) .

Recall that 1 − ρ + β/κ = 1, γ ≤ 1/2(α + κδ). Set

V k = (cid:107)vk − v(cid:63)(cid:107)2

P + κγ2σ2
k.

Then,

Ek

(cid:2)V k+1(cid:3) ≤ (cid:2)V k(cid:3) − γEk

(cid:2)L(xk, h(cid:63)) − L(x(cid:63), hk+1)(cid:3) .

Taking the expectation,

γE (cid:2)L(xk, h(cid:63)) − L(x(cid:63), hk+1)(cid:3) ≤ E (cid:2)V k(cid:3) − E (cid:2)V k+1(cid:3) .

Iterating and using the nonnegativity of V k,

γ

k−1
(cid:88)

j=0

E (cid:2)L(xj, h(cid:63)) − L(x(cid:63), hj+1)(cid:3) ≤ E (cid:2)V 0(cid:3) .

We conclude using the convex-concavity of L.

H.8.2 Proof of Theorem H.6.1

We ﬁrst use Lemma H.8.1 along with the strong convexity of ψ, H ∗. Note that yk = qk
and therefore qk+1 = qk + hk+1 − qk = hk+1. We have

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + Ek
≤ (cid:107)pk − p(cid:63)(cid:107)2 + (cid:107)qk − q(cid:63)(cid:107)2

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2
γ,τ − γµ(cid:107)xk − x(cid:63)(cid:107)2

γ,τ

(cid:18)

+ κγ2

1 − ρ +

(cid:19)

β
κ

k − 2γ(1 − γ(α + κδ))Df (xk, x(cid:63))
σ2

(cid:3) + 2γµH ∗Ek

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3)

Noting that for every q ∈ Y, (cid:107)q(cid:107)2
1/(α + κδ),

γ,τ = γ

τ (cid:107)q(cid:107)2 − γ2(cid:107)L∗q(cid:107)2 ≤ γ

τ (cid:107)q(cid:107)2, and taking γ ≤

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + (1 + 2τ µH ∗) Ek

≤ (cid:107)pk − p(cid:63)(cid:107)2 + (cid:107)qk − q(cid:63)(cid:107)2

γ,τ − γµ(cid:107)xk − x(cid:63)(cid:107)2 + κγ2

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2
γ,τ
(cid:18)

(cid:3)

k+1

(cid:3) + κγ2Ek
(cid:2)σ2
(cid:19)
β
κ

1 − ρ +

σ2
k.

341

Algorithm 19 LiCoSGD (cid:0)deterministic version: gk+1 = ∇f (xk)(cid:1)
1: Input: x0 ∈ X , y0 ∈ Y, γ > 0, τ > 0
2: for k = 0, 1, 2, . . . do
wk = xk − γgk+1
3:
yk+1 = yk + τ L(wk − γL∗yk) − τ b
4:
xk+1 = wk − γL∗yk+1
5:
6: end for

Finally, since ψ is λ-smooth, (cid:107)pk − p(cid:63)(cid:107)2 ≤ (1 + 2γλ + γ2λ2)(cid:107)xk − x(cid:63)(cid:107)2.
Indeed, in
this case, applying Lemma 8.4.2 with ˜A = 0, ˜C = 0 and ˜B = ∇ψ, we obtain that if
xk = proxγψ(pk) and x(cid:63) = proxγψ(p(cid:63)), then

(cid:107)xk − x(cid:63)(cid:107)2 = (cid:107)pk − p(cid:63)(cid:107)2 − 2γ(cid:104)∇ψ(xk) − ∇ψ(x(cid:63)), xk − x(cid:63)(cid:105) − γ2(cid:107)∇ψ(xk) − ∇ψ(x(cid:63))(cid:107)2

≥ (cid:107)pk − p(cid:63)(cid:107)2 − 2γλ(cid:107)xk − x(cid:63)(cid:107)2 − γ2λ2(cid:107)xk − x(cid:63)(cid:107)2.

Hence,

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + (1 + 2τ µH ∗) Ek
γµ

≤ (cid:107)pk − p(cid:63)(cid:107)2 + (cid:107)qk − q(cid:63)(cid:107)2

γ,τ −

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2

γ,τ

(cid:3) + κγ2Ek
(cid:18)

(1 + γλ)2 (cid:107)pk − p(cid:63)(cid:107)2 + κγ2

1 − ρ +

(cid:2)σ2

(cid:3)

k+1
β
κ

(cid:19)

σ2
k.

Thus, set

and

Then,

V k = (cid:107)pk − p(cid:63)(cid:107)2 + (1 + 2τ µH ∗) (cid:107)qk − q(cid:63)(cid:107)2

γ,τ + κγ2σ2
k,

(cid:18)

r = max

1 −

γµ
(1 + γλ)2 ,

(cid:18)

1 − ρ +

(cid:19)

β
κ

,

1
1 + 2τ µH ∗

(cid:19)

.

Ek

(cid:2)V k+1(cid:3) ≤ rV k.

H.9 Convergence Results for LiCoSGD

We consider the problem

min
x∈X

f (x)

s.t. Lx = b,

(H.15)

where L : X → Y is a linear operator, X and Y are real Hilbert spaces, f is a ν-smooth
convex function, for some ν > 0, and b ∈ Range (L). This is a particular case of
Problem (8.1) with ψ = 0 and H : y (cid:55)→ (0 if y = b, +∞ else). The Stochastic PD3O
and PDDY algorithms both revert to the same algorithm, shown above, which we call
Linearly Constrained Stochastic Gradient Descent (LiCoSGD).

Theorem 8.5.1 becomes:

Theorem H.9.1 (Convergence of LiCoSGD, deterministic case). Suppose that γ ∈
(0, 2/ν) and that τ γ(cid:107)L(cid:107)2 ≤ 1. Then in LiCoSGD, (xk)k converges to some solution x(cid:63) to

342

the problem (H.15) and (yk)k converges to some dual solution y(cid:63) ∈ arg miny f ∗(−L∗y) +
(cid:104)y, b(cid:105).

Note that the case τ γ(cid:107)L(cid:107)2 = 1 is not covered by Theorem 8.5.1 but follows from

convergence of the PD3O algorithm in that case, as proved in [195].

Theorem 8.6.3 becomes:

Theorem H.9.2 (Convergence of LiCoSGD, stochastic case). Suppose that Assump-
tion 8.6.1 holds. Let κ def= β/ρ, γ, τ > 0 be such that γ ≤ 1/2(α + κδ) and γτ (cid:107)L(cid:107)2 < 1.
Set V 0 def= (cid:107)v0 − v(cid:63)(cid:107)2
0, where v0 = (w0, y0). Then,

P + γ2κσ2

E (cid:2)f (¯xk) − f (x(cid:63)) + (cid:104)L¯xk − b, y(cid:63)(cid:105)(cid:3) ≤

V 0
kγ

,

(H.16)

where ¯xk = 1
k

(cid:80)k−1

j=0 xj, x(cid:63) and y(cid:63) are some primal and dual solutions.

Note that the convex function x (cid:55)→ f (x) − f (x(cid:63)) + (cid:104)Lx − b, y(cid:63)(cid:105) is nonnegative and its
minimum is 0, attained at x(cid:63); under mild conditions, this function takes value zero only
if f (x) = f (x(cid:63)) and Lx = b, so that x is a solution.

Replacing the variable yk by the variable ak = L∗yk in LiCoSGD yields PriLiCoSGD.

In the conditions of Theorem H.9.1, (ak)k converges to a(cid:63) = −∇f (x(cid:63)).

H.9.1 Proof of Theorem 8.6.5

We ﬁrst derive the following lemma:

Lemma H.9.3. Let x ∈ Range (L∗), the range space of L∗. There exists an unique
y ∈ Range (L) such that L∗y = x. Moreover, for every y ∈ Range (L),

λ+
min(L)(cid:107)y(cid:107)2 ≤ (cid:107)L∗y(cid:107)2,

(H.17)

where λ+

min(L) is the smallest positive eigenvalue of LL∗ (or L∗L).

Proof. Using basic linear algebra, LL∗x = 0 implies L∗x ∈ Range (L∗) ∩ Ker(L) there-
fore L∗x = 0. Hence, Ker(LL∗) ⊂ Ker(L∗) and therefore Range (L) ⊂ Range (LL∗).
Since LL∗ is real symmetric, for every y ∈ Range (LL∗), (cid:104)y, LL∗y(cid:105) ≥ λ+
min(L)(cid:107)y(cid:107)2,
where λ+
min(L) is the smallest positive eigenvalue of LL∗. Therefore, for every y ∈
Range (L) , (cid:107)L∗y(cid:107)2 ≥ λ+
min(L)(cid:107)y(cid:107)2. Moreover, L∗y = 0 implies y = 0 on Range (L),
therefore there is at most one solution y in Range (L) to the equation L∗y = x. The
(cid:4)
existence of a solution follows from x ∈ Range (L∗).

Now, we prove Theorem 8.6.5. First, we deﬁne y(cid:63). In the case ψ = 0 and H = χb,
Equation (8.3) states that ∇f (x(cid:63)) ∈ Range (L∗). Using Lemma H.9.3, there exists an
unique y(cid:63) ∈ Range (L) such that ∇f (x(cid:63)) + L∗y(cid:63) = 0. Noting that y(cid:63) = h(cid:63) = q(cid:63) and

applying Lemma H.8.1 with γ ≤ (α + κδ),

343

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + Ek

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2

γ,τ

(cid:3) + κγ2Ek

≤ (cid:107)pk − p(cid:63)(cid:107)2 + (cid:107)qk − q(cid:63)(cid:107)2

γ,τ − γµf (cid:107)xk − x(cid:63)(cid:107)2 + κγ2

(cid:2)σ2

(cid:3)

k+1
(cid:18)

1 − ρ +

(cid:19)

β
κ

σ2
k

− γ2(cid:107)P−1A(uk+1) − P−1A(u(cid:63))(cid:107)2
P.

Since the component of P−1A(uk+1) − P−1A(u(cid:63)) in X is L∗hk+1 − L∗h(cid:63), we have

Ek

(cid:2)(cid:107)pk+1 − p(cid:63)(cid:107)2(cid:3) + Ek

(cid:2)(cid:107)qk+1 − q(cid:63)(cid:107)2

γ,τ

(cid:3) + κγ2Ek

(cid:2)σ2

k+1

(cid:3) ≤ (cid:107)xk − x(cid:63)(cid:107)2 + (cid:107)qk − q(cid:63)(cid:107)2
− γµf (cid:107)pk − p(cid:63)(cid:107)2
(cid:18)
β
κ

1 − ρ +

+ κγ2

σ2
k

(cid:19)

γ,τ

Inspecting the iterations of the algorithm, one can see that h0 ∈ Range (L) implies
hk+1 ∈ Range (L). Since h(cid:63) ∈ Range (L), hk+1 − h(cid:63) ∈ Range (L). Therefore, using
Lemma H.9.3, λ+
min(L)(cid:107)hk+1 − h(cid:63)(cid:107)2 ≤ (cid:107)L∗hk+1 − L∗h(cid:63)(cid:107)2. Since qk+1 = hk+1 = yk+1
and xk = pk,

− γ2(cid:107)L∗hk+1 − L∗h(cid:63)(cid:107)2.

Ek

(cid:2)(cid:107)xk+1 − x(cid:63)(cid:107)2(cid:3) + (1 + γτ λ+

min(L))Ek

≤ (1 − γµf )(cid:107)xk − x(cid:63)(cid:107)2 + (cid:107)yk − y(cid:63)(cid:107)2

γ,τ + κγ2

(cid:3) + κγ2Ek
(cid:2)(cid:107)yk+1 − y(cid:63)(cid:107)2
γ,τ
(cid:19)
β
κ

1 − ρ +

σ2
k.

(cid:18)

(cid:2)σ2

k+1

(cid:3)

Setting

and

we have

V k = (cid:107)xk − x(cid:63)(cid:107)2 + (cid:0)1 + τ γλ+

min(L)(cid:1)(cid:107)yk − y(cid:63)(cid:107)2

γ,τ + κγ2σ2
k,

(cid:18)

r = max

1 − γµ, 1 − ρ +

β
κ

,

1

1 + τ γλ+

min(L)

(cid:19)

,

Ek

(cid:2)V k+1(cid:3) ≤ rV k.

H.10 Application of PriLiCoSGD to Stochastic Decentralized Op-

timization

Consider a connected undirected graph G = (V, E), where V = {1, . . . , N } is the set
of nodes and E the set of edges. Consider a family (fi)i∈V of µ-strongly convex and
ν-smooth functions fi, for some µ ≥ 0 and ν > 0.
In this section, we consider solving
the minimization problem

min
x∈X

(cid:88)

i∈V

fi(x).

(H.18)

Consider a gossip matrix of the graph G, i.e., a N × N symmetric positive semideﬁnite
matrix (cid:99)W = ((cid:99)Wi,j)i,j∈V , such that Ker((cid:99)W) = span([1 · · · 1]T) and (cid:99)Wi,j (cid:54)= 0 if and

344

Algorithm 20 DESTROY (cid:0)deterministic version: gk+1
1: Input: x0
i ∈ X and a0
2: for k = 0, 1, 2, . . . do
3:
4:

for all i ∈ V in parallel do

i ∈ X , for every i ∈ V , such that (cid:80)

i = ∇fi(xk)(cid:1)
i∈V a0

i = 0, γ > 0, τ > 0

i + τ (cid:99)Wi,itk+1

i + τ (cid:80)

j(cid:54)=i:{i,j}∈V (cid:99)Wi,j(tk+1

j − γak
j )

i

i − γgk+1
tk+1
i = xk
ak+1
i = (1 − τ γ (cid:99)Wi,i)ak
xk+1
i = tk+1
end for

i − γak+1

.

i

5:

6:
7:
8: end for

only if i = j or {i, j} ∈ E is an edge of the graph. (cid:99)W can be the Laplacian matrix of
the graph G, for instance. Set W = (cid:99)W ⊗ I, where ⊗ is the Kronecker product and I the
identity of X ; W is a positive linear operator over X V and W(x1, . . . , xN ) = 0 if and
only if x1 = . . . = xN . Therefore, Problem (H.18) is equivalent to the lifted problem

f (˜x)

such that W1/2 ˜x = 0,

min
˜x∈X V

(H.19)

1, . . . , xk

where for every ˜x = (x1, . . . , xN ) ∈ X V , f (˜x) = (cid:80)N
i=1 fi(xi). PriLiCoSGD can be
applied to Problem (H.19).
It involves only one multiplication by W per iteration and
N ) ∈ X V . The update of each
generates the sequence (˜xk)k, where ˜xk = (xk
xk
i consists in local computations involving fi and communication steps involving xk
j ,
where j is a neighbor of i. We called the instance of PriLiCoSGD applied to this set-
ting the Decentralized Stochastic Optimization Algorithm (DESTROY), shown above.
In details, at each iteration of DESTROY, an estimate ˜gk+1 = (gk+1
N ) of
∇f (˜xk) = (cid:0)∇f1(xk
)k
i
satisﬁes Assumption 8.6.1 as an estimator of ∇fi, (˜gk+1)k satisﬁes Assumption 8.6.1 as
an estimator of ∇f . Moreover, the computation of ˜gk+1 boils down to the ‘local’ compu-
tation of gk+1
at each node i ∈ V , independently on each other. After this, decentralized
communication in the network G is performed, modeled by an application of W.

N )(cid:1) is computed. Assuming that each sequence (gk+1

1), . . . , ∇fN (xk

, . . . , gk+1

1

i

For instance, the variance reduced estimator gk

i can be the L-SVRG estimator when fi
is a ﬁnite sum, or a compressed version of ∇fi. Such compressed gradients are suitable
when communication (i.e., applications of W) is expensive, see Section 8.6.

As an application of the convergence results for LiCoSGD, we obtain the following

results for DESTROY from Theorem H.9.1.

Theorem H.10.1 (Convergence of DESTROY, deterministic case). Suppose that γ ∈
(0, 2/ν) and that τ γ(cid:107)(cid:99)W(cid:107) ≤ 1. Then in DESTROY, each (xk
i )k converges to the same
solution x(cid:63) to the problem (H.18) and each (ak

i )k converges to a(cid:63)

i = −∇fi(x(cid:63)).

Theorem H.9.2 can be applied to the stochastic case, with O(1/k) convergence of

the Lagrangian gap, where Y = X and L = L∗ = W1/2.

Similarly, Theorem 8.6.5 yields linear convergence of DESTROY in the strongly convex
case µ > 0, with L∗L replaced by W and (cid:107)L(cid:107)2 replaced by (cid:107)W(cid:107) = (cid:107)(cid:99)W(cid:107). In particular,
in the deterministic case gk+1
i = ∇fi(xk), with γ = 1/ν and τ γ = θ/(cid:107)W(cid:107) for some ﬁxed

θ ∈ (0, 1), ε-accuracy is reached after

345

O

(cid:18)(cid:18) ν
µ

+

(cid:107)W(cid:107)
λ+
min(W)

(cid:19)

log

(cid:19)

1
ε

iterations.

346

Appendix I

Published Papers

[95] Filip Hanzely, Konstantin Mishchenko, and Peter Richt´arik. SEGA: vari-
ance reduction via gradient sketching.
In Advances in Neural Information
Processing Systems, volume 31, pages 2082–2093, 2018.

[166] Konstantin Mishchenko, Filip Hanzely, and Peter Richt´arik. 99% of
worker-master communication in distributed optimization is not needed.
In
36th Conference on Uncertainty in Artiﬁcial Intelligence. AUAI, 2020.

[116] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. Tighter
theory for local SGD on identical and heterogeneous data.
In Proceedings
of the 23rd International Conference on Artiﬁcial Intelligence and Statistics,
pages 4519–4529. PMLR, 2020.

[158] Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent
In Proceedings of the 37th International Conference on
without descent.
Machine Learning, volume 119, pages 6702–6712. PMLR, 2020.

[168] Konstantin Mishchenko, Franck Iutzeler, J´erˆome Malick, and Massih-
Reza Amini. A delay-tolerant proximal-gradient algorithm for distributed learn-
ing. In Proceedings of the 35th International Conference on Machine Learning,
pages 3587–3595. PMLR, 2018.

[171] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt´arik,
and Yura Malitsky. Revisiting stochastic extragradient. In Proceedings of the
23rd International Conference on Artiﬁcial Intelligence and Statistics, pages
4573–4582. PMLR, 2020.

[167] Konstantin Mishchenko, Franck Iutzeler, and J´erˆome Malick. A dis-
tributed ﬂexible delay-tolerant proximal gradient algorithm. SIAM Journal on
Optimization, 30(1):933–959, 2020.

[169] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Random
reshuﬄing: simple analysis with vast improvements. Advances in Neural In-
formation Processing Systems, 33:17309–17320, 2020.

[249] Saeed Soori, Konstantin Mishchenko, Aryan Mokhtari, Maryam Mehri
Dehnavi, and Mert G¨urb¨uzbalabann. DAve-QN: a distributed averaged quasi-
Newton method with local superlinear convergence rate. In Proceedings of the
23rd International Conference on Artiﬁcial Intelligence and Statistics, pages
1965–1976. PMLR, 2020.

347

Appendix J

Preprints

[127] Dmitry Kovalev, Konstantin Mishchenko, and Peter Richt´arik. Stochas-
tic Newton and cubic Newton methods with simple local
linear-quadratic
rates. NeurIPS Beyond First-Order Methods in ML Workshop, arXiv preprint
arXiv:1912.01597, 2019.

[173] Konstantin Mishchenko and Peter Richt´arik. A stochastic penalty model
for convex and nonconvex optimization with big constraints. arXiv preprint
arXiv:1810.13387, 2018.

[165] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak´aˇc, and Peter
Richt´arik. Distributed learning with compressed gradient diﬀerences. arXiv
preprint arXiv:1901.09269, 2019.

[164] Konstantin Mishchenko. Sinkhorn algorithm as a special case of stochas-
tic mirror descent. NeurIPS Workshop on Optimal Transport & Machine
learning, arXiv preprint arXiv:1909.06918, 2019.

[172] Konstantin Mishchenko, Mallory Montgomery, and Federico Vaggi. A
self-supervised approach to hierarchical forecasting with applications to group-
wise synthetic controls. ICML Time Series Workshop, arXiv preprint arXiv:1906.10586,
2019.

[174] Konstantin Mishchenko and Peter Richt´arik. A stochastic decoupling
method for minimizing the sum of smooth and non-smooth functions. arXiv
preprint arXiv:1905.11535, 2019.

[170] Konstantin Mishchenko, Ahmed Khaled, and Peter Richt´arik. Proximal
and federated random reshuﬄing. arXiv preprint arXiv:2102.06704, 2021.

[175] Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, and Peter Richt´arik.
IntSGD: ﬂoatless compression of stochastic gradients. arXiv preprint arXiv:2102.08374,
2021.

[211] Xun Qian, Alibek Sailanbayev, Konstantin Mishchenko, and Peter Richt´arik.
MISO is making a comeback with better proofs and rates. arXiv preprint
arXiv:1906.01474, 2019.

348

[234] Adil Salim, Laurent Condat, Konstantin Mishchenko, and Peter Richt´arik.
Dualize, split, randomize: fast nonsmooth optimization algorithms.
arXiv
preprint arXiv:2004.02635, 2020.

[103] Samuel Horv´ath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian U.
Stich, and Peter Richt´arik. Stochastic distributed learning with gradient quan-
tization and variance reduction. arXiv preprint arXiv:1904.05115, 2019.

[115] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt´arik. First anal-
ysis of local GD on heterogeneous data. The 2nd International Workshop
on Federated Learning for Data Privacy and Conﬁdentiality, arXiv preprint
arXiv:1909.04715, 2019.

