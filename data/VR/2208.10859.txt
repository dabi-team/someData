Wavelet-Based Fast Decoding of 360° Videos

Colin Groth, Sascha Fricke, Susana Castillo, and Marcus Magnor

2
2
0
2

t
c
O
8
1

]

R
G
.
s
c
[

2
v
9
5
8
0
1
.
8
0
2
2
:
v
i
X
r
a

Figure 1: In this paper, we propose a wavelet-based video codec that is able to load and decode the data of 360° videos viewport-
dependently by exploiting the properties of the wavelet transform. While the quality is on par with state-of-the-art video codecs,
we can achieve signiﬁcantly faster playback times. Here, a visual comparison of our codec is given with HEVC, AV1 and the
uncompressed reference. The samples are out of the computer-generated City video (left), all with 8k full-frame resolution.

ABSTRACT

In this paper, we propose a wavelet-based video codec speciﬁcally
designed for VR displays that enables real-time playback of high-
resolution 360° videos. Our codec exploits the fact that only a
fraction of the full 360° video frame is visible on the display at any
time. To load and decode the video viewport-dependently in real
time, we make use of the wavelet transform for intra- as well as inter-
frame coding. Thereby, the relevant content is directly streamed
from the drive, without the need to hold the entire frames in memory.
With an average of 193 frames per second at 8192 × 8192 -pixel
full-frame resolution, the conducted evaluation demonstrates that
our codec’s decoding performance is up to 272% higher than that
of the state-of-the-art video codecs H.265 and AV1 for typical VR
displays. By means of a perceptual study, we further illustrate the
necessity of high frame rates for a better VR experience. Finally, we
demonstrate how our wavelet-based codec can also directly be used
in conjunction with foveation for further performance increase.

Index Terms: Computing methodologies—Computer graphics—
Image
compression; Computing methodologies—Computer
graphics—Graphics systems and interfaces—Virtual reality;

1 INTRODUCTION

Codecs provide efﬁcient compression allowing to store hundreds of
videos on a single drive. This efﬁciency results from a precise adap-
tation to their speciﬁc application. Video codecs like HEVC/H.265
or AV1 use the discrete cosine transform (DCT) and motion compen-
sation for high compression rates at a reasonable perceptual quality.
The high compression, however, requires complex coding proce-
dures. The speciﬁc hardware decoders of modern graphics cards
compensate for some of this decoding load.

360° videos are a sophisticated form of viewing experience which
have become known with the spread of virtual reality (VR) tech-
nology. In 360° videos the user can change their view anywhere,
since the information of the entire space is available (three degrees
of freedom (DOF)). This free exploration is not possible with tradi-
tional videos where the ﬁeld of view (FOV) is limited. Accordingly,

360° videos are best suited for an immersive user experience for
video playback in VR. For comparable quality, the resolution of
360° videos must signiﬁcantly exceed those of videos with a discrete
view, since the representation on the display device only corresponds
to a fraction of the whole video frame. For a modern VR headset
with 2000x2000-pixel resolution per eye and 90° FOV, a comparably
resolved 360° video would require 8000x8000 pixels (stereo) with
up to 120Hz temporal resolution. However, only the part of the
frame that lies within the device’s viewport at the time of decoding
is relevant for rendering. Current DCT-based codecs do not allow to
load or decode only a deﬁned part of a frame due to their complex
non-linear structure. Accordingly, with 360° videos the common
practice is to load, upload, and decode the entire 360° frame, even
though only a small part of the frame is considered for the rendering.
Furthermore, the recording quality of 360° cameras is limited. Con-
sequently, the resolution and frame rate of 360° videos nowadays
do not come close to the quality of the renderings of virtual environ-
ments. Although there are some existing ideas that already try to
improve the compression – e.g. tiling the frame into separate regions
– these approaches often go against the basic compression concept
of DCT and are only a compromise at the expense of compression
efﬁciency.

An alternative to the DCT for compression is the wavelet trans-
form, which offers two decisive advantages over the former: (1)
different parts of the image can be loaded and decoded individu-
ally, e.g. the viewport of an head-mounted display (HMD); (2) the
encoding is performed in frequency layers, which are each halved
in frequency. Decoding an area in fewer steps is equivalent to dis-
playing the image area at a lower resolution. Early attempts to use
wavelet transform for image compression were limited to traditional
presentations with a discrete FOV and have not gained wide use.

In this paper, we propose a wavelet-based codec for the compres-
sion of 360° videos. We particularly aim for high display speeds of
high-resolution videos. Our implementation of the wavelet-based
codec uses the wavelet transform for inter- and intra-frame com-
pression. To the best of our knowledge, this is the ﬁrst codec for
360° videos based on wavelet transforms. In comparison with mod-
ern codecs (HEVC/H.265 and AV1) and related work, we show
that our wavelet-based approach offers a signiﬁcant speed advan-
tage while providing a comparable video quality and reasonable
compression rate.
In addition, we introduce foveated decoding.
With foveated decoding the properties of the wavelets are used to

OursReferenceHEVCAV1 
 
 
 
 
 
gradually decrease the resolution with the distance from the focal
point. Such foveation is, so far, only known from virtual scenes
and offers further opportunities to improve both decoding speed and
image perception. Our codec will be made publicly available upon
acceptance.

The contributions of the paper are summarized as follows:

• a novel approach to encode and decode videos for fast viewport-

dependent playback based on wavelet transforms

• wavelet-based inter-frame transform without keyframes

• a technique to implicitly apply foveated rendering for wavelet-

encoded videos during run-time

• objective and perceptual evaluation and comparisons with state-

of-the-art video codecs and related work

2 RELATED WORK

Loading the entire frame for 360° video playback in VR is inefﬁcient
since only a fraction of the frame is actually rendered. However, this
procedure is the most common practice, as it reduces the need to
adjust the standard video pipeline. In the following, we ﬁrst discuss
previous work aiming at a more resource-efﬁcient presentation by
adapting existing codecs and, in the second part, we introduce former
attempts for wavelet-based codecs.

2.1 Viewport-Adaptive Display Techniques for Videos

Zare et al. [34] proposed to use a tiling scheme to increase the decod-
ing speed of streamed 360° HEVC videos. Their experimental setup
consisted of a pipeline with a dedicated server and client side. On the
server side, the same video was encoded in high and low resolution.
With the motion constrained tile sets (MCTS) extension of HEVC
the tiling was enabled for both versions of the video. The client,
on the other hand, requested the required tile sets from the server
based on the viewport. The authors tested different tiling schemes.
The scheme with the most tiles (18 tiles) showed the highest bitrate
savings (−40% based on Bjontegaard Delta Bitrate BD-BR [2]).
However, the compression losses increase proportionally with the
number of used tiles, since all tiles are saved independently.

The tiling approach was later ofﬁcially formulated by the Moving
Picture Experts Group (MPEG) into the omnidirectional media for-
mat (OMAF) standard for storage and distribution of 360° videos [5].
The idea of OMAF is comparable to the work of Zare et al. [34] and
is applied to HEVC or AVC video codecs. The viewport-dependent
streaming also uses MCTS to split the frames into tiles, each encoded
in different qualities [11].

Sreedhar et al. [28] also recognized the technical challenges
of bandwidth associated with high-resolution 360° videos. The
main focus of their work was the mapping techniques in which
the recorded spherical scenes are packed in a rectangular frame.
The most used mapping techniques are equirectangle and cubemap
projection, which were also found as the most effective in their
scenario. For the comparison, the authors presented a methodology
of the rate-distortion performance of the schemes.

In the work of Corbillon et al. [7] the 360° videos are separated
in individual tiles and offered in different resolutions. Unlike former
works, the single tiles are created in different versions with only
a selected part of every tile in a better visual quality. While the
360° video is streamed, the client communicates its viewpoint to the
server which selects the tiles so that the viewpoint is in the higher
quality region. The paper does not specify actual display speeds, but
it should be clear that the technology can save bandwidth.

2.2 Wavelet based codecs
Probably the best known use of wavelets for imagery is the
JPEG2000 image compression standard [21, 30]. At the turn of
the millennium, it initiated a new form of image compression
and was meant to replace DCT-based image compression formats.
JPEG2000 supports lossless and lossy compression. The wavelet
transform operates with the biorthogonal wavelets, either the Co-
hen–Daubechies–Feauveau (CDF) 9/7 wavelet [6] for lossy com-
pression or the LeGall-Tabatabai (LGT) 5/3 wavelet [18] for lossless
compression. The standard has four levels of decomposition as a
default since there is not signiﬁcant improvement in using higher
decomposition levels when compressing images [15]. One general
advantage of wavelet compressed imagery is the progressive decod-
ing, so that the quality of the visualisation improves progressively
when more information is received. This progressive decoding is
also supported in JPEG2000.

The JPEG2000 image standard was later extended to include
video ﬁles. The extension is known as Motion JPEG2000 and is
based on the MP4 format. This video standard uses the JPEG2000
coding for the compression of the individual frames. An inter-frame
compression does not take place. Thus, Motion JPEG2000 is more of
a container format for the joint wrapping of JPEG2000 compressed
frames.

Efforts to create video codecs based on wavelet compression are
rare and nowadays exclusively experimental. The most extensive
attempt to create a wavelet-based video format to date was under-
taken by BBC Research in 2008 [31]. The resulting versions of the
codec were named Dirac and Schr¨odinger in honour of the Nobel
Prize-winning physicists. Dirac supports lossy and lossless coding
for which it uses the same wavelets as JPEG2000 (CDF 9/7 wavelet
or LGT 5/3 wavelet). The motion compensation is performed with
the overlapped-block motion compensation (OBMC) logic for an
effective inter-frame prediction [23]. Unfortunately, this overlap
also prevents effective intra-prediction, since there are no unique
separations for overlapping blocks, as is the case with common
DCT-based codecs.

However, the codec could not gain wide popularity and further
development was discontinued more than a decade ago. The reasons
for the codecs limited success are not entirely clear, but may be
related to an inability to provide signiﬁcant improvement over estab-
lished codecs like H264. Dirac and Schr¨odinger are now abandoned
and no longer available.

3 METHOD
Two concepts that most video codecs apply for data compression
are intra- and inter-frame coding. In practice, these methods are
commonly applied with some information loss to achieve better
compression ratios. Intra-frame coding usually refers to the trans-
formation of the data of one frame to a different representation that
can be compressed more efﬁciently. Inter-frame coding utilizes
redundancies between multiple frames to reduce the data size. In the
following, we describe how we realised both concepts with wavelet
transforms.

3.1 Frame-wise Transform
The core of the frame-based compression of our codec is a 2D fast
wavelet transform (FWT). Similar to other codecs, the transform of
the frame data allows for a better compression, which in the raw
state is too large to be stored. For example, a one minute 360° video
in 8k resolution would contain around 300GB uncompressed data.
We transform an input frame s of (N × M) pixels for a discrete 2D
position (x, y) and frequency γ by the wavelet transform W with the
mother wavelet ψ.

Wψ s(γ, x, y) =

N
∑
i=0

M
∑
j=0

ψγ,x,y(i, j)s(i, j)

(1)

Figure 2: Our program ﬂow for encoding (blue) and decoding (orange) a video with our wavelet-based codec.

To compress the transformed frame Wψ s all coefﬁcients below a
certain threshold Ts are set to zero, so that:

W (cid:48)

ψ s :=

(cid:40)

Wψ s,
0,

i f |Wψ s| > Ts,
otherwise

(2)

For a properly chosen Ts, this operation has only minor implications
for the quality of the reconstructed image. This is especially true for
high frequencies and is a general characteristic of the frequency
domain. Usually, in natural images most of the information is
contained in the low frequencies, which are represented by only
a small number of coefﬁcients [32]. As usual for the FWT, with
every step of the 2D transformation, the resolution of the image
approximation is halved in both dimensions. In Wψ this is addressed
by the frequency layers over γ.

Former research has shown that the discrete wavelet transform
can achieve better image reconstruction than a DCT-based method
at high bit compression ratios [3]. For the frame-wise transformation
of the image we use the CDF 9/7 wavelet [6]. The CDF wavelet is
known to perform especially good on natural imagery and is also
used for lossy compression in the JPEG2000 standard [15].

During playback, the compressed video information is decoded
with an inverse fast wavelet transform (iFWT) to obtain the original
images. This reconstruction is not conducted for the entire image,
but only for the part of the 360° panorama that lies in the viewport
of the display device. For a viewport-dependent reconstruction, we
deﬁne the location of the viewport on a low resolution representation
of the frame in binary form. This binary mask is uploaded together
with the wavelet coefﬁcients and is used for the inverse wavelet
transform. The resolution of the binary representation is 256 × 256
pixels for a 8k stereo frame and can deﬁne arbitrary reconstruction
shapes.

In theory, the transform can be performed until only one pixel
deﬁnes the lowest frequency over the whole image. However, the
low-frequency levels of the wavelet transform contain fewer discrete
data points since the high-resolution in the frequency domain results
in a low resolution in time due to the Heisenberg theorem [12].
Also, the wavelet coefﬁcient values of these pixels can only be
compressed inefﬁciently because the low-frequency information is
signiﬁcantly more important than high frequencies in natural image
reconstruction. By default, we perform the wavelet transform for
lmax = log2( N
32 ) − 2 levels. For the number of wavelet levels, we
took inspiration from the JPEG2000 standard, but also performed
several pilot studies. While we found 6 levels to be the optimal
default solution for 8k videos, the number of wavelet levels can be
chosen individually per video.

3.2 Inter-Frame Coding

Inter-frame coding describes the compression of temporal informa-
tion. In videos, the time component t is represented implicitly by a
set of successive frames. In modern codecs the inter-frame compres-
sion is performed with keyframes and motion vectors where only
the information differences are encoded. While this technique offers
impressive compression rates, a compression with keyframes has
the disadvantage that its speed depends on the linear information
retrieval. When the video is skipped, all information since the last
keyframe has to be reloaded ﬁrst. With our codec we wanted to
get rid of this disadvantage and at the same time maintain a good
compression rate between inter-frames. To achieve this purpose
we apply a second one-dimensional wavelet transform that encodes
the temporal pixel differences. The second wavelet transform is
applied on a set of wavelet images resulting from the frame-wise
wavelet transform Wψ s. Here, we use a one-dimensional form of
Wψ with s(γ,t) for the frequency γ of the temporal changes of ev-
ery pixel over time. In other words, our wavelet based inter-frame
transform encodes the frequency changes over time. Typically, even
with movement in the frame the temporal information only changes
on single frequencies. All frequencies that do not or only slightly
change are compressed by our inter-frame transform. As result, the
speed of the decoding is unaffected by the direction in which the
viewport moves. In theory both, the frame-wise transform and the
inter-frame transform, can be combined to one 3D wavelet transform.
However, this 3D transform would not offer us the possibility to
decode different areas of a frame in different resolutions for the
same computational costs. Furthermore, the separation allows us to
apply different wavelets and thresholds per transform and respond
adaptively to individual circumstances.

Every inter-frame transform of n consecutive frames we call inter-
frame set. Thereby, n is a power of two value. The number of frames
per inter-frame set can be deﬁned per video and may be bigger the
less motion is in the video. In contrast to the frame-wise transform,
the inter-frame transform is always executed to the last level. For
the inter-frame wavelet transform we use the Haar wavelet [10].
The Haar wavelet is the only wavelet with no overlapping of the
wavelet ﬁlters and can therefore be reconstructed by loading only
one coefﬁcient per level for the high and low pass ﬁltering. Recon-
struction of one speciﬁc pixel by a Haar wavelet transform with
n levels only requires log2(n) additions of the correct wavelet co-
efﬁcients multiplied by the high pass ﬁlter position (either −1 or
1). As a result, for the inverse inter-frame transform we can iterate
over the uploaded wavelets rather than over all pixels of the target
section. This characteristic is unique to the Haar wavelet and allows
a rapid inter-frame reconstruction. The speed of the inter-frame

Start EncodingisEOFEndi < nYesNoi=0Yesi++NoRead nimages2D wavelettransform ofimage iThresholdingof transformedimageInterframe trans-form of all nwavelet imagesInterframethresholdingCompactionWrite toﬁleWriteMetadatavideostill play-ingClearimageLoad Meta-dataRebuild wavelet image (inverse inter-frame transform)Load waveletsfrom ﬁleUploadwaveletsDetermine recon-struction regionPreload next frames from drive InversewavelettransformRemappedrenderingNoYesStart DecodingEndreconstruction is important since the inverse inter-frame transform
runs on log2(n) frames every time one frame is decomposed. By
iterating over the uploaded pixels rather than a target section we
implicitly synthesise only the part of the image that is in our deﬁned
FOV.

3.3 Thresholding
In order to achieve the necessary storage savings, we have to deter-
mine which wavelet coefﬁcients are least essential for the reconstruc-
tion. We will refer to this step as thresholding. The chosen threshold
value is decisive for the intensity of the compression. Thereby, the
threshold always represents a trade-off between quality of the re-
constructed image and size of the video ﬁle. The threshold of 0
represents lossless compression while a threshold of 1 produces
the smallest ﬁle. Reconstructing an image with too little frequency
information may result in a blurry representation with less details.
We derive a threshold T from a user-deﬁned constant α and the level
l of the transform:

T (x, y) = α

(cid:19)2

(cid:18) lmax − l
lmax

+ H,

(3)

where H speciﬁes a mapping factor which depends on the mapping
technique (cf. Sec. 3.6). In the encoding, the frames are thresholded
twice: once after the frame-wise 2D FWT, and again following the
inter-frame transform. We use two separate threshold operations as
both wavelet transforms aim for a different encoding: The frame-
wise transform encodes the different frequency information in the
respective spatial resolutions. The inter-frame transform encodes
temporal frequency information of every wavelet coefﬁcient. Thresh-
olding the values only once is possible but, in our experience, can
lead to unwanted interactions and a worse compression rate. Both
thresholding operations are independent and have their own thresh-
old value. While the ﬁrst thresholding is applied on every frame
independently, the thresholding of the inter-frames considers all n
frames of the inter-frame set. In this latter thresholding operation,
the different levels are deﬁned by the relative frame number from t
rather than the pixel position inside the frame.

High frequency information was found to be less important for the
perceptual quality of an image than low frequency information [32].
We scale the threshold by the frequency level of each coefﬁcient
in a quadratic function (cf. Eq. 3). Accordingly, more coefﬁcients
may be zeroed out at high frequencies. This thresholding weighting
follows common procedures of other codecs like JPEG2000 [21, 30].
Quantization: Similar to other codecs, we represent the color
values of the pixels in the video ﬁle by one byte per color component.
In the quantization, the 32 bit ﬂoat color components of the wavelet
transform are mapped to the byte representation of the compressed
output. We use the extreme values of the wavelet coefﬁcients for
normalization in order to achieve the highest possible spatial resolu-
tion in this discretization. Therefore, one discrete color value cd is
deﬁned by

cd =

cn − cmin
cmax − cmin

∗ 255

(4)

with cn as the ﬂoating-point representation of the n-th pixel and
cmin and cmax as the minimum and maximum values of all coefﬁ-
cients, respectively. The normalization is performed with a separate
minimum and maximum for the approximation area (last layer of
the transform) and the wavelet layer and for each inter-frame. The
normalization is inverted during reconstruction. The minimum and
maximum values are stored with the metadata in the ﬁle.

3.4 File Format
The structure of our video ﬁle is illustrated in Fig. 3. We designed
the layout to allow for a fast and viewport-dependent streaming of
the data. Starting with a ﬁle header, general information on the

video is offered. This data includes the number of frames, size of the
frames and number of levels of the wavelet transform. Following the
header, metadata information on every single frame is provided. This
frame-wise metadata includes information about where the frame
starts and ends in the ﬁle or the overall number of wavelets. The
frame metadata also provides information on the individual levels of
this frame. The header as well as the entire metadata are preloaded
when the video is started and are kept in the working memory.

The position (x, y) of one particular wavelet coefﬁcient within
the frame is given by an index that is saved along with the wavelet
value. However, due to the compression, the position of individual
coefﬁcients within the ﬁle is unknown. While it is possible to
ﬁnd the data for (x, y) with binary search on the video data, this
inconsistent access to the storage drive adds an unwanted delay to
the loading process. Instead we divide the transformed frames into a
logical grid of small blocks (default size 32 × 32-pixel). This block
allocation is only relevant for the compression but does not affect
the wavelet transforms which operate on the entire images. Note,
that this is different from blocking in DCT. In the video ﬁle we
store one pointer for each block, located in the BlockEnd section
(see Fig. 3). This pointer indicates where the last wavelet coefﬁcient
inside the respective block can be found in the video ﬁle. In the ﬁle
the coefﬁcients are stored block after block, which allows a whole
series of blocks to be loaded by two of these block-end pointers.
During decoding, the block pointers of a frame are preloaded before
the frame is processed. By alternately storing the block-end and
wavelet data packages of the frames in the video ﬁle we can avoid
compute-intensive rearrangements of the ﬁle during encoding. Inside
one block of wavelet coefﬁcients or block-end information the data
is ordered level-wise starting with the lowest frequency layer.

3.5 Parallel Wavelet Processing

Since VR users move their head, the part of a 360° video that is
rendered can change continuously. These viewpoint changes compli-
cate the buffering of subsequent frames with a viewport-dependent
video stream. However, buffering is necessary to avoid load peaks
and latencies that disrupt the virtual experience and can induce cy-
bersickness [29]. Therefore, we load the viewport data of the next
frames asynchronously while the current frame is decoded. Until the
time of rendering, all frames are updated continuously in case that
the look direction changes. The number of frames that are preloaded
corresponds to the inter-frame size. Preparing more frames is not
always useful, as the view direction may change strongly over longer
periods of time. During our experiments no substantial latency was
measured that arose from the buffering.

Due to the inter-frame compression, one frame is reconstructed
by the wavelet coefﬁcients stored in multiple inter-frames of the
respective inter-frame set. We rebuild the wavelet representation
Wψ s of one frame on the GPU while we already upload the wavelet
data for the next inter-frame in parallel (see Fig. 4). This rebuild
already includes the inverse inter-frame transform, as described in
Section 3.2. The reconstruction of the original frame by the 2D
iFWT is performed once all inter-frames are processed and the
inverse inter-frame transform is completed.

3.6 Frame Mapping

360° videos are representations of a recorded 3D sphere, brought to a
rectangular frame by a projection. The position of the information in
the projected frame is deﬁned by its mapping. Among the most pop-
ular mapping techniques are equirectangular mapping and cubemaps.
Our codec is deﬁned to be independent of the mapping technique.
As the reconstructed areas are given in a low resolution presentation
of the frame (cf. Sec. 3.1), the areas needed for the frame mapping
can be set in direct relation to the ﬁnal reconstruction. We render the
ﬁnal re-projection of the FOV to the spherical presentation in an own
shader which is run after the inverse wavelet transform is completed.

Figure 3: Data arrangement of our video format. The sizes of each section are given by an example video in 8k resolution.

Figure 4: Parallel processes for the reconstruction of one frame. The
prepare thread buffers the next frames. In the wait sections the drive
is occupied by the decoding process.

This shader can react on multiple mapping types and also considers
the stereo images. For the experiments we use the equirectangular
projection. We tackle the redundancies in the pixel information near
the poles by gradually increasing the mapping factor H towards the
poles. For an equirectangular projected frame with the dimension
S we deﬁne H(y) = 1 − sin(yπ/Sy). With the adjusted threshold,
we experience equal performance at all viewing angles, including
upward views.

3.7 Foveated Decoding

The information density of visual representations of the human eye
are not equally distributed [27]. The images created on the retina of
the human visual system follow a qualitative decline starting from
the eye’s ﬁxation point. While people can perceive the full resolution
of about one sixtieth of a degree in the fovea around the focus point,
the information in the peripheral visual area is signiﬁcantly lower in
resolution [17].

So far we only discussed full resolution reconstructions of the
viewport. However, when the eye gaze direction of the observer is
available by eye tracking, we can utilise the properties of the wavelet
transform to achieve what we call foveated decoding. With foveated
decoding the resolution gradually decreases with the distance from
the fovea. Our method is comparable to classical foveated rendering,
except that in the periphery the decoding is accelerated while the
rendering load is constant. The results are bandwidth savings and
higher playback speeds.

For the foveated decoding, we utilise the level-wise structure
of the wavelet transform. As described in Section 3.1, a wavelet
representation is composed of individual levels, each corresponding
to a deﬁned frequency interval γ. The reconstruction is performed
incrementally from a low-resolution version of the frame to the full
resolution. Instead of reconstructing the same FOV at each level,
the full FOV of the viewport is reconstructed only at the lowest
frequency level. After that the FOV is reduced with every level
resulting in a compact foveal area. The sizes of the individual
resolution levels of the wavelet transform are deﬁned in regard to

Figure 5: Our foveated decoding compared with the full-resolution
reference frame. On the right, the information density of the foveated
decoding is visualised.

the properties of the human eye [19]. Like human perception, we
decrease the quality of the frames at a logarithmic rate [17]. The
area with the full video resolution which stimulates the most central
foveola is only about two percent wide [27].

The inverse wavelet transform is executed over the same num-
ber of data points as for a full resolution viewport but assumes the
coefﬁcients to be zero for the surrounding regions. With foveated
decoding, we achieve a peripheral reconstruction in a visually ap-
pealing quality with a small number of coefﬁcients (see Fig. 5).
With the foveation, up to 80% less data has to be loaded. The re-
constructed areas are in rectilinear form and follow recent ﬁndings,
which indicate advantages over a log-polar presentation [20].

4 EXPERIMENTS

In objective and subjective evaluations we compare our codec against
the common HEVC and AV1 codecs. The benchmarks for display
speeds also include a tiled HEVC implementation from related
work [34].

4.1 Dataset

For the evaluation, we aim to analyse a particularly diverse set
of videos. Both computer-generated imagery (CGI) content and
real world recordings are considered. Furthermore, we investigate
moving camera trajectories as well as ﬁxed recording positions.

The CGI reference frames are created with Unreal Engine 5 with
a high level of photorealism. The CGI scenes cover urban and nature
scenery. The City scene mainly contains geometrical structures and
straight lines and the recording takes place in a virtual New York
City. The Mountain scene depicts a natural landscape with trees and
a lake. The content of this scene contains a more heterogeneous
shape composition compared to urban imagery. In the CGI scenes
the camera trajectory moves along a predeﬁned quarter circle path.

SizeLevelIF_CountFramesFrame 1Frame 2...GeneralLvl 1Lvl 2...HeaderMeta DataLvl 1Lvl 2...Block EndFrame 1~262 kBLvl 1Lvl 2...Wavelet DataFrame 1Block EndFrame 2Wavelet DataFrame 2......20 B~59 kB~7 MB~262 kB~4 MBCPUGPUDecodePrepareLoadUploadLvl1......LoadUploadLvlnRecon-structLvln-1Recon-structLvlnInverse wavelettransformPreload / UpdateWaitWaitPreload / UpdatePreload / UpdateRecon-structLvl0WaitTimeCPULoadUploadLvl0ReferenceOursRawTable 1: Display speeds. Tiling refers to Zare et al. [34]. The foveated
decoding (FD) is run with the high resolution version of our codec
(OursHQ), all values are averages over multiple runs and given in FPS.

Table 2: Objective Quality Comparisons. The results from the
computational metrics, PSNR/SSIM (higher is better) and LPIPS
(lower is better), on all codecs compared with the reference frames.

AVG fps ↑

Scene

Metrics

HEVC

Videos

CityCGI
MountainCGI
Downhill
Horse
Climbing
Walking
Cave*
Boat*

HEVC

Tiling

AV1

OursLQ

OursHQ

OursFD

57.32
58.27
60.21
62.6
65.36
65.32
66.53
67.13

88.24
87.95
93.21
95.94
92.55
93.1
111.08
110.15

52.7
61.57
48.32
50.45
52.53
52.76
53.93
55.07

220.96
222.75
193.88
197.17
195.63
198.24
209.22
205.21

194.17
202.28
180.70
181.18
187.56
187.88
207.12
201.06

228.8
227.46
206.65
207.16
207.3
205.89
210.28
208.76

The frames are rendered as 360° stereo images with 8192 × 8192
pixel resolution.

The ﬁrst set of real-world videos is recorded with a moving cam-
era trajectory and the display of rapid motions. Here, we use the
videos of Groth and colleagues [8, 9] which have a higher resolution
than typical moving-camera 360° videos due to their custom camera
setup. The second category considers videos with a ﬁxed record-
ing position (further denoted by *). These videos were originally
recorded by M¨uhlhausen et al. [22]. The original real-world videos
are recorded with stereoscopic information in 6400 × 6400-pixel
resolution at 30 frames per second (FPS).

Reference Data Creation: Pre-recorded videos can cause two
problems for the evaluation. For one, the frame rates typically do
not match the refresh rates of VR devices. Additionally, the data is
already lossy compressed and can include serious compression arti-
facts. In order to address both problems, we ﬁrst downscale the video
data to 1024 × 1024-pixel to get rid of high-frequency compression
artifacts and then perform temporal interpolation and upscaling of
the data with state-of-the-art neural network approaches. The result-
ing frames are used as reference for our evaluation. The original
video data is downscaled with bicubic interpolation by OpenCV.
The temporal interpolation is run on the downscaled frames with
RIFE [14]. The information from both eyes is processed individ-
ually to avoid artifacts at the edge. We increase the frame rate
from the original 30 FPS to 120 FPS, which should be in line with
the frequency of most modern VR glasses. For the resolution up-
scaling we use Nvidia VFX to create the ﬁnal reference frames in
8196 × 8196-pixel resolution.

4.2 Objective Evaluation

We consider a high and low quality version of the wavelet-
compressed videos. The inter-frame transform is applied in sets
of four frames and compressed with an inter-frame threshold of
0.005. The frame-wise threshold is chosen to be 0.1 and 0.25 for
the high and low quality version, respectively. The HEVC and
AV1 encodings are performed with FFmpeg. Regarding quality, for
HEVC we use a constant rate factor (CRF) of 30 (range 0–51) and
for AV1 a CRF of 50 (range 0–65). The videos of both codecs are
encoded in the common YUV420 colour space. The OMAF inspired
tiling method is realised with HEVC encoded tiles with the fastest
tiling scheme of Zare et al. [34]. However, we extended their tiling
scheme for stereoscopic videos to a 6-by-6 grid layout (6-by-3 per
eye). Following the former work, the middle row of both eyes is
chosen with 90° height and all other rows with 45° height for a better
central view performance.

We conduct all of our experiments on a commercially available
computer with a NVIDIA RTX 3090 graphics card and an AMD
Ryzen 5950X processor. The video data is stored on an on-board
SSD. A HTC Vive Pro Eye is chosen as output device. All videos
are displayed in our self-programmed video player which uses the
Vulkan API to utilise the GPU. The decoding of HEVC and AV1

CityCGI

MountainCGI

Downhill

Horse

Climbing

Walking

Cave*

Boat*

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

PSNR ↑
SSIM ↑
LPIPS ↓

35.53
.905
.124

36.08
.927
.158

34.77
.954
.082

37.05
.968
.054

37.3
.973
.051

37.64
.97
.05

40.32
.971
.039

39.2
.984
.03

AV1

33.32
.89
.167

35.69
.925
.157

34.17
.95
.103

36.48
.966
.07

36.83
.971
.066

37.06
.969
.064

40.24
.972
.04

39.87
.986
.027

OursLQ

OursHQ

29.46
.817
.268

30.57
.851
.32

31.4
.921
.161

32.24
.939
.118

32.84
.952
.115

32.04
.928
.117

37.02
.96
.08

33.01
.954
.078

32.51
.916
.114

33.09
.933
.145

34.81
.961
.081

35.07
.97
.057

35.33
.976
.056

35.45
.967
.05

39.58
.972
.046

35.88
.978
.036

video data is performed with the Nvidia NVDECODE API. Thereby,
the HEVC and AV1 decoding beneﬁts from the hardware accelera-
tion on the GPU. All videos are created from 1200 reference frames
with 8192x8192-pixel resolution. To assure an equal comparison of
all experimental conditions, we use head and eye tracking data of
participant recordings.

The results regarding computational time are shown in Table 1.
Our proposed codec allows for an average increase in performance
of 197% compared to HEVC and AV1 and an increase of 91% over
the tiling technique. This increase is even more signiﬁcant when the
lower quality version of our codec is used. In the experiment, the
foveated decoding (OursFD) is applied on the wavelet-based video
with high quality settings. Due to the foveation, the performance
increases by 223% over HEVC allowing a better performance than
the lower quality wavelet-encoded videos. Please note that we
used the hardware accelerated on the GPU for the decoding of
HEVC and AV1. The dedicated decoding chips allow for signiﬁcant
increases in decoding speed compared to conventional decoding.
Additionally, the compute shaders for the mapping and rendering
can be executed in parallel to the decoding through the dedicated
chips. A comparable chip for decoding wavelet transforms could
also signiﬁcantly improve the performance of a wavelet-based codec
while the compute unit can be used for other tasks.

We compared the results’ quality of all codecs by the commonly
used metrics PSNR, SSIM [33], and LPIPS [35]. The given values
are averages over all frames and compared with the uncompressed
reference frames (see Table 2). In terms of image quality our method
performs equally to the other codecs, HEVC/H.265 and AV1, when
high quality settings are chosen. As can be expected, the image
quality is on a lower level when the low quality parameters are
chosen for the wavelet-based encoding.

The compression rates of the wavelet ﬁles in both quality con-
ﬁgurations can be seen in Table 3. With our wavelet-based approach,
we are able to compress the raw information to over one hundredth

Table 3: Compression ratios of the wavelet video ﬁles in relation to
the uncompressed data.

CityCGI MountainCGI Downhill Horse Climbing Walking Cave* Boat*

OursLQ
OursHQ

176:1
52:1

250:1
75:1

147:1
77:1

187:1
100:1

250:1
128:1

185:1
100:1

714:1 312:1
416:1 117:1

in size for most videos. Compared to HEVC and AV1 compres-
sion we achieve about half the compression rates, depending on the
quality of the video. The tiled HEVC videos by the technique of
Zare et al. [34] are on average three times larger than our wavelet-
compressed video ﬁles due to the signiﬁcant compression losses of
the tiling process.

4.3 Perceptual Evaluation

In a next step, we compare the codecs in terms of observer’s prefer-
ences to reveal subtle perceptual differences that were not found by
the objective metrics. In this perceptual experiment, we are partic-
ularly interested in analyzing to what extent the higher frame rates
we can achieve with our wavelet codec also contribute to a better
VR experience. As before, HEVC and AV1 serve as comparison
techniques. The wavelet videos use the high quality version that
showed to be most comparable to the other codecs in quality while
still maintaining high display speeds. Due to the better data utiliza-
tion of a wavelet-based codec, a higher resolution of the video or a
higher frame rate can be provided. Thus, there are two conditions
to be considered: the videos are in the same resolution but have
different frame rates (Speed condition), or the videos are in different
resolutions but provide the same frame rate (Quality condition). In
the Speed condition all videos are at a resolution of 8192 × 8192
pixels. While the comparison techniques provide common 30FPS
in this condition, the wavelet videos provides double the frame rate
(60FPS) based on the results of the display speed measurements. In
the Quality condition the frame rate of all videos is set to 60FPS but
the HEVC and AV1 videos, here, have a resolution of 4096 × 4096
pixels. The length of at videos is 10 seconds. For the perceptual eval-
uation we only use the CGI scenes. The upscaled real-world videos
have a corrupted stereo view because the upscaling algorithms are
not designed for stereo footage. This display error should be irrele-
vant for the image-based metrics, but makes perceptual experiments
impossible.

As the abstract feeling of comfort in a VR experience cannot
be represented by a linear scale, we utilize the paired comparisons
technique [16]. Given the same video with two different encodings
played immediately after each other, the participants were instructed
to choose the video they would prefer for a presentation in VR. The
question was intentionally kept open and participants were free to
base their decision on the visual quality, the temporal smoothness of
the video, or a lower incidence of cybersickness.

Given three codecs, there are three possible comparisons per
video: Ours vs. AV1, Ours vs. HEVC, and AV1 vs. HEVC. This
results in a total of 12 decisions per person given two scenes for
both conditions. In the experiment, the order of the paired stimuli
comparisons for each participant was counterbalanced to avoid side
effects.

A total of 23 participants took part in the experiment (10 females,
age range = 22 − 59, avg. age = 34.83, SD = 13.04) resulting in 184
votes per codec. The videos were shown in a HTC Vive Pro Eye
HMD. For every comparison, the participants were ask to maintain
a ﬁxed head position to compare the same part of the 360° video.

4.3.1 Analysis and Results
On a ﬁrst analysis, the voting of the participants (n = 23) leads us to
the displayed results in the top half of Table 4 (”Raw Data”, column
”% Preferences”). The results show the analysis for each scene as

well as an analysis per condition combining both scenes (labelled as
”AVG”).

In order to assess the results of the paired comparisons we fol-
low the methodology from Setyawan and Lagendijk [25]. We ﬁrst
study the consistency of choices within one participant as well as
the agreement in choices among all participants. The coherence of
the answers of the participants is indicated by the coefﬁcient of con-
sistency ζ ∈ [0, 1], with ζ = 1 implying perfect consistency. Low
consistency values of single participants can indicate that these indi-
viduals had difﬁculties to differentiate between the stimuli and thus,
we can expect their judgement abilities to be worse than the average.
As the number of methods m = 3, ζ < 1 can only arise with the
occurrence of one kind of circular triad such that C1 → C2 → C3
but C3 → C1. In Table 4 we present the coefﬁcient of consistency
as an average over all participants. The consistency of choices of
single participants does not necessarily mean that identical choices
were made between all participants. The diversity of preferences
for the number of participants n is described by the coefﬁcient of
agreement u. Complete agreement is achieved with u = 1, mean-
ing that all participants favoured the same method for all decisions.
High disagreement indicated by low u values, on the other hand,
suggests that participants had difﬁculties to make a joint choice.
Disagreement may suggests either that the stimuli were perceived as
not distinguishable or a general split of opinions about the stimuli.
Here, the minimum u, and accordingly complete disagreement, is
given by umin = − 1
(n−1) ≈ −0.045. By the deﬁnition of Kendall and
Babington-Smith [16], the coefﬁcient of agreement u is derived by

u =

(cid:0)n
2

2τ
(cid:1)(cid:0)m
2

(cid:1) , whereτ =

(cid:19)

m
∑
i=1

m
∑
j=1

(cid:18)ai j
2

(5)

with ai j is the number of times method i is chosen over method j,
whereby i (cid:54)= j. As before, the number of participants is denoted by
n and the number of methods by m.

We determine the statistical signiﬁcance of u by testing against
the null hypothesis that all votes were chosen randomly. For a
signiﬁcant u we can conclude the alternative hypothesis that the
agreement is above the value one would expect from random choices.
To determine the signiﬁcance under the null hypothesis we perform
a chi-squared test (χ 2). As proposed by former research [26], with
our n we can derive χ 2 in simple form.

χ 2 =

(cid:18)m
2

(cid:19)

[1 + u(n − 1)]

(6)

Since we compare three codecs, the χ 2 distribution is evaluated
(cid:1) = 3 DOF. Therefore, the statistical signiﬁcance at level p
with (cid:0)m
2
derives by χ 2
3 .

The analysis of the raw data for consistency and agreement is
shown in Table 4. While the participants show a high consistency, the
coefﬁcient of consistency ζ for the Quality condition is considerably
lower than for the Speed condition. The difference in agreement is
even more pronounced and the participants show clear disagreement
in the ratings of Quality (numbers highlighted in red). This mismatch
is most apparent in the city scene where results are not statistically
signiﬁcant anymore for the targeted threshold of p < .01.

A more in-depth analysis of the consistency of individual ratings
shows the cause for this outcome. In the Quality condition, a total
of eight participants were not able to properly distinguish between
stimuli, leading them to produce triads. Also, two participants had
difﬁculties emitting judgement in the Speed condition. All the rest
of our participants show perfect judgement capability (ζ = 1, no
triads), indicating that the inconsistencies arised from the individual
participant’s ability to judge and not from a problem with a consen-
sus between participants. Therefore, we correct the analysis for a
consistent result and remove the votes for the entire condition of all

Table 4: Perceptual results. The top half refers to the uncorrected data of all participants (n = 23). In the bottom half the results are corrected for
consistency (nSpeed = 21, nQuality = 15).

ANALYSIS

CONDITION

SCENE

Ours vs. AV1

% Preferences
Ours vs. HEVC

AV1 vs. HEVC

Raw Data

Corrected
for
Consistency

Speed

Quality

Speed

Quality

City
Mountain
AVG
City
Mountain
AVG

City
Mountain
AVG
City
Mountain
AVG

95.65%
100.00%
97.83%
56.52%
86.96%
71.74%

100.00%
100.00%
100.00%
80.00%
86.67%
83.33%

82.61%
56.52%
69.57%
56.52%
43.48%
50.00%

80.95%
57.14%
69.05%
60.00%
46.67%
53.33%

4.35%
8.70%
6.52%
21.74%
26.09%
23.91%

4.76%
4.76%
4.76%
20.00%
6.67%
13.33%

ζ

0.96
0.96
0.96
0.87
0.74
0.80

1
1
1
1
1
1

u

0.68
0.55
0.60
0.08
0.23
0.13

0.72
0.59
0.65
0.20
0.39
0.31

χ 2

48.13
39.09
84
8.13
18.22
20.55

46.24
38.62
82.48
11.4
19.4
29.60

p

<.0001
<.0001
<.0001
.0434
.0004
.0001

<.0001
<.0001
<.0001
.0097
.0002
<.0001

those participants who did not show perfect consistency. By rerun-
ning the analysis with the votings corrected for perfect consistency
(nSpeed = 21, nQuality = 15) we can obtain the values corresponding
to participants with perfect discerning abilities. These results are
show in the bottom half of Table 4 (”Corrected for Consistency”).
While the Quality condition still seems to be more controversial,
the agreement of the participants for this condition increases sub-
stantially (numbers highlighted in green). With the correction, all
differences in preference are statistically signiﬁcant for p < 0.01.

In general, the results of the perceptual study indicated that the
videos using both our codec and HEVC were favored over AV1
for all conditions. The conclusions of the comparison between our
codec and HEVC are dependent on the analyzed condition. While
the participants clearly favoured our codec in the Speed condition,
the perceived quality overall is on par with the H.265 compressed
video. A more in-depth analysis reveals the choice of scene as a
decisive factor for the participants’ preference. Straight lines and
geometric structures are key attributes of urban scenery. In these rep-
resentations with man-made content, quality differences and artifacts
seem to be more conspicuous. The Mountain scene, on the other
hand, is a natural scene and, as such, its content is less structured
and highly semantically homogeneous due to the recurring textures.
In this scene, the participants showed difﬁculties to distinguish the
quality of the different compression techniques. These observations
are consistent with the ﬁndings of previous research [4, 24]. In the
Speed condition, the scene attributes seem to became even more
salient for the observers’ choice, drifting their preferences towards
our method (preference > 80%).

5 DISCUSSION AND LIMITATIONS
In the following, we discuss further key points of consideration and
address limitations of the current implementation

Experimental Results. We compared our wavelet-based codec
against two common video codecs and previous work. For the
evaluation, we considered a low and high quality version of the
wavelet-encoded videos, because in a practical application either
quality or speed may be prioritised. The results show that the codec
can be optimised for such requirements by changing the encoding
parameters. However, even at the highest quality, we achieve signiﬁ-
cantly higher decoding speeds than the other methods. The foveated
decoding technique leverages the properties of the human visual
system, resulting in peripheral resolution differences that are unno-
ticeable to VR users compared to fully resolved viewports [19]. At
the same time, the foveated decoding allows for the highest decoding
speeds. In a perceptual experiment, we studied the importance of
the frame rate and video resolution for the perceived quality of the
VR experience. The results suggest that for 360° videos in VR, the
frame rate is of signiﬁcantly greater importance to users than the
image quality. This emphasis on frame rate for the user experience

is consistent with former research [1].

Single Operation Point. Our codec is designed for a very spe-
ciﬁc use-case. The core motivation is to have 360° videos with a
resolution and display speed that does not induce cybersickness and
is pleasant to watch. In our investigations, we explored in detail
the single operation point that best covers this scenario. For a fair
comparison we chose the parameters of our codec so that the quality
is on the same level. With this baseline, we then measured the speed
of the methods. A broader range of quality-rate scenarios can be
explored in the future to utilize wavelet-based coding for a verity of
applications.

Streaming. So far, we have primarily addressed videos that are
stored on a local drive. Online streaming is another common way to
retrieve video data. With online streaming, the amount of data that
is transmitted is much more relevant due to bandwidth limitations.
For these limitations, a wavelet-based codec beneﬁts from the direct
viewport-dependent streaming from ﬁle. This property allows to
reduce the transfer rates by up to ten times compared to the total size
of the video. In its current form, our encoding is not yet optimized
for real time execution, which we plan to address as a natural next
step.

File Size. Our wavelet format does not use any container format
but is stored in simple binary form. Neither is a color transformation
performed, for example to the YUV space. Such techniques are
applied by other codecs to reduce their ﬁle sizes to the minimum
while preserving the best possible quality. In this paper the major
focus was on display speed. In future work such techniques may be
introduced to further reduce the ﬁle sizes of wavelet-based video
coding.

Reference Data. Our objective with the reference data scaling of
the real-world videos was to generate uncompressed high-resolution,
high frame-rate video data. We used a combination of downscaling
followed by AI-based upscaling to remove compression artifacts
from the original videos. This removal is not perfect and it can
be assumed that the compression rate of a wavelet-based codec is
signiﬁcantly higher for raw footage. Such a use of a wavelet-based
codec can only be achieved when the encoding is directly performed
by the capturing device with the native color information.

Professional Filming. The videos from our experiment are con-
sidered as casual recordings. Nevertheless, 360° videos are not only
used by amateurs, but also by professional ﬁlmmakers. For profes-
sional ﬁlming, it can be necessary to display different areas of a
frame in different qualities, such as the background or the masks
of an actor, which stands out as artiﬁcial in high resolutions. With
conventional methods, this procedure requires post-processing or re-
capturing of the video. With a wavelet-based codec a pre-adjustment
is not necessary and the video can be stored in full resolution. In-
dividual quality levels may be chosen at decoding time for deﬁned
parts of the video, comparable to our foveated decoding approach

(cf. Sec. 3.7).

Eyetracking. In VR, eye tracking is nowadays mostly used for
computer-generated content, where foveation allows for signiﬁcant
increases in rendering speed. The foveated decoding of our codec
opens up the opportunity for an broader use of eye tracking in VR
where it can be used to increase the playback speed of 360° videos
through unobtrusive quality gradation in the peripheral area.

Wide FOV. When the FOV gets unusually wide, this would affect
the performance of our approach since the decoding is viewport
dependent. The headset with the widest FOV currently on the market
is the StarVR One with an overall horizontal FOV of 210° and a
vertical FOV of 130° [13]. Exploring this scenario, we found that
with wavelet coding we are still able to achieve > 100 FPS with the
high quality conﬁguration in all scenes. With foveated decoding
applied, the frame rate is signiﬁcantly higher. In comparison, the
tiling approach with this wide FOV no longer yields any performance
beneﬁt and actually performs worse than the native full frame HEVC
decoding (≈ 50 FPS).

6 CONCLUSION

In this paper we proposed wavelet-based video coding for fast
and high-resolution playback of 360° videos. We showed that
our wavelet-based compression approach allows for selective load-
ing and decoding of arbitrary video regions, which in the case of
360° videos is key for a fast decoding. While in our experiment
our codec reached display speeds at least two times higher than the
other methods tested, the quality remained at a comparable level.
The importance of high frame rates for a good VR experience is
supported by the results of our perceptual experiment. In addition,
with our codec we have introduced foveated decoding, allowing
for an unobtrusive quality decrease in the outer regions of the view.
Foveated decoding can be applied on run-time and further increases
the decoding times. In conclusion, wavelet-based video approaches
solve the problems that are raised by DCT codecs when a fast or
viewport-dependent playback of 360° videos is required. Especially
for VR environments, wavelet-based codecs show to be a valuable
extension, offering the opportunity to display 360° videos in a quality
and speed comparable to renderings of virtual worlds.

REFERENCES

[1] A. Banitalebi-Dehkordi, M. T. Pourazad, and P. Nasiopoulos. The
effect of frame rate on 3d video quality and bitrate. 3D Research,
6(1):1–13, 2015.

[2] G. Bjontegaard. Calculation of average PSNR differences between
RD-curves. ITU-T SG16/Q6 input document VCEG-M33, 2001.
[3] G. Boopathi and S. Arockiasamy. Image compression: Wavelet trans-
form using radial basis function (rbf) neural network. In India Confer-
ence, pp. 340–344. IEEE, 2012.

[4] S. Castillo, T. Judd, and D. Gutierrez. Using eye-tracking to assess
different image retargeting methods. In Proceedings of the ACM SIG-
GRAPH Symposium on Applied Perception in Graphics and Visualiza-
tion, APGV ’11, pp. 7–14, 2011. doi: 10.1145/2077451.2077453
[5] B. Choi, Y. Wang, M. Hannuksela, Y. Lim, and A. Murtaza. Information
technology–coded representation of immersive media (mpeg-i)–part 2:
Omnidirectional media format. ISO/IEC, pp. 23090–23092, 2018.
[6] A. Cohen, I. Daubechies, and J.-C. Feauveau. Biorthogonal bases of
compactly supported wavelets. Communications on Pure and Applied
Mathematics, 45(5):485–560, 1992. doi: 10.1002/cpa.3160450502
[7] X. Corbillon, G. Simon, A. Devlic, and J. Chakareski. Viewport-
adaptive navigable 360-degree video delivery. In International Confer-
ence on Communications, pp. 1–7. IEEE, 2017.

[8] C. Groth, J.-P. Tauscher, N. Heesen, S. Grogorick, S. Castillo, and
M. Magnor. Mitigation of cybersickness in immersive 360°videos.
In IEEE Virtual Reality Workshop on Immersive Sickness Prevention
(WISP), pp. 169–177. IEEE, 2021. doi: 10.1109/VRW52623.2021.0
[9] C. Groth, J.-P. Tauscher, N. Heesen, M. Hattenbach, S. Castillo, and
M. Magnor. Omnidirectional galvanic vestibular stimulation in virtual

reality. Transactions on Visualization and Computer Graphics (TVCG),
28(5):2234–2244, 2022. doi: 10.1109/TVCG.2022.315

[10] A. Haar. Zur theorie der orthogonalen funktionensysteme. Mathema-

tische Annalen, 71(1):38–53, 1911.

[11] M. M. Hannuksela, Y.-K. Wang, and A. Hourunranta. An overview of
the omaf standard for 360 video. In Data Compression Conference, pp.
418–427, 2019.

[12] W. Heisenberg. ¨Uber den anschaulichen inhalt der quantentheoretis-
chen kinematik und mechanik. Zeitschrift f¨ur Physik, 43:172–198,
1927. doi: 10.1007/BF01397280

[13] Starvr one. https://www.starvr.com/. Accessed: 2022-10-14.
[14] Z. Huang, T. Zhang, W. Heng, B. Shi, and S. Zhou. Rife: Real-time
intermediate ﬂow estimation for video frame interpolation. arXiv
preprint arXiv:2011.06294, 2021.

[15] ISO. ISO/IEC 15444-1:2019, vol. 642. International Organization for

Standardization, 2019.

[16] M. G. Kendall and B. Babington-Smith. On the method of paired
comparisons. Biometrika, 31:324–345, 1940. doi: 10.1093/biomet/31.
3-4.324

[17] H. Kolb, R. F. Nelson, P. K. Ahnelt, I. Ortu˜no-Lizar´an, and N. Cuenca.
The architecture of the human fovea. Webvision: The Organization of
the Retina and Visual System, 2020.

[18] D. Le Gall and A. Tabatabai. Sub-band coding of digital images using
symmetric short kernel ﬁlters and arithmetic coding techniques. In
International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 761–764, 1988.

[19] R. Leigh and D. Zee. The Neurology of Eye Movements. Contemporary

neurology series. Oxford University Press, 2015.

[20] D. Li, R. Du, A. Babu, C. D. Brumar, and A. Varshney. A log-rectilinear
transformation for foveated 360-degree video streaming. Transactions
on Visualization and Computer Graphics, 27(5):2638–2647, 2021.

[21] M. W. Marcellin, M. J. Gormish, A. Bilgin, and M. P. Boliek. An
overview of jpeg-2000. In Proceedings Data Compression Conference,
pp. 523–541. IEEE, 2000.

[22] M. M¨uhlhausen, M. Kappel, M. Kassubeck, P. M. Bittner, S. Castillo,
and M. Magnor. Temporal consistent motion parallax for omnidirec-
tional stereo panorama video. In ACM Symposium on Virtual Real-
ity Software and Technology (VRST), 2020. doi: 10.1145/3385956.
3418965

[23] M. Orchard and G. Sullivan. Overlapped block motion compensation:
an estimation-theoretic approach. Transactions on Image Processing,
3(5):693–699, 1994.

[24] M. Rubinstein, D. Gutierrez, O. Sorkine, and A. Shamir. A com-
parative study of image retargeting. ACM Transactions on Graphics,
Proceedings Siggraph Asia, 29(5), 2010.

[25] I. Setyawan and R. L. Lagendijk. Human perception of geometric
distortions in images. In Security, Steganography, and Watermarking
of Multimedia Contents VI, vol. 5306, pp. 256 – 267, 2004. doi: 10.
1117/12.526726

[26] S. Siegel and J. Castellan, N. J. Nonparametric statistics for the
behavioral sciences. Mcgraw-Hill Book Company, 2. ed., 1988.
[27] L. D. Silverstein. Foundations of vision. Color Research and Applica-

tion, 21:142–144, 2008.

[28] K. K. Sreedhar, A. Aminlou, M. M. Hannuksela, and M. Gabbouj.
Viewport-adaptive encoding and streaming of 360-degree video for
virtual reality applications. In International Symposium on Multimedia,
pp. 583–586, 2016.

[29] J.-P. Stauffert, F. Niebling, and M. E. Latoschik. Latency and cyber-
sickness: impact, causes, and measures. a review. Frontiers in Virtual
Reality, 1:1–10, 2020.

[30] D. Taubman and M. Marcellin. JPEG2000 image compression funda-
mentals, standards and practice, vol. 642. Springer Science & Business
Media, 2012.

[31] BBC Research. Dirac speciﬁcation (version 2.2.3). https://web.
archive.org/web/20150503015104/http://diracvideo.org/
download/specification/dirac-spec-latest.pdf, 2008.
[32] M. Unser and T. Blu. Mathematical properties of the jpeg2000 wavelet
ﬁlters. IEEE Transactions on Image Processing, 12(9):1080–1090,
2003. doi: 10.1109/TIP.2003.812329

[33] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assess-

ment: from error visibility to structural similarity. IEEE Transactions
on Image Processing, 13(4):600–612, 2004. doi: 10.1109/TIP.2003.
819861

[34] A. Zare, A. Aminlou, M. M. Hannuksela, and M. Gabbouj. Hevc-
compliant tile-based streaming of panoramic video for virtual reality
applications. In Proceedings of the International Conference on Multi-
media, pp. 601–605, 2016.

[35] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The
unreasonable effectiveness of deep features as a perceptual metric. In
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
586–595, 2018. doi: 10.1109/CVPR.2018.00068

