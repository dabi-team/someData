1

Learning-based Prediction, Rendering and

Association Optimization for MEC-enabled

Wireless Virtual Reality (VR) Network

Xiaonan Liu, Student Member, IEEE, Yansha Deng, Member, IEEE,

Abstract

Wireless-connected Virtual Reality (VR) provides immersive experience for VR users from any-

where at anytime. However, providing wireless VR users with seamless connectivity and real-time VR

video with high quality is challenging due to its requirements in high Quality of Experience (QoE) and

low VR interaction latency under limited computation capability of VR device. To address these issues,

we propose a MEC-enabled wireless VR network, where the ﬁeld of view (FoV) of each VR user can

be real-time predicted using Recurrent Neural Network (RNN), and the rendering of VR content is

moved from VR device to MEC server with rendering model migration capability. Taking into account

the geographical and FoV request correlation, we propose centralized and distributed decoupled Deep

Reinforcement Learning (DRL) strategies to maximize the long-term QoE of VR users under the VR

interaction latency constraint. Simulation results show that our proposed MEC rendering schemes and

DRL algorithms substantially improve the long-term QoE of VR users and reduce the VR interaction

latency compared to rendering at VR devices.

Index Terms

Field of view (FoV) prediction, rendering, downlink transmission, mobile edge computing (MEC),

deep reinforcement learning (DRL), and virtual reality (VR).

I. INTRODUCTION

With the development of virtual reality (VR) technology, the interactions between VR users

and their world will be revolutionized. VR can connect users across global communities within

highly immersive virtual worlds that breaks geographical boundaries. This vision has inspired

the commercial release of various hardware devices, such as Facebook Oculus Rift [1]. Indeed,

it is anticipated that 99 million VR devices are needed in 2021 [2], and that the market will

0
2
0
2

y
a
M
7
1

]
P
S
.
s
s
e
e
[

1
v
2
3
3
8
0
.
5
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

reach 108 billion dollars by then [3]. However, the poor user experience provided by traditional

computer-supported VR devices constrains the type of activities and experience of the VR user.

One of the main barriers of the wired connected VR devices is the limited mobility of VR

users. To overcome this disadvantage, wireless connected VR devices can be potential solution

in providing ubiquitous user experiences from anywhere at anytime, and also can unleash plenty

of novel VR applications [4]. Nevertheless, there are some unique challenges in wireless VR

system that does not exist in wired VR system and traditional wireless video transmission system

as identiﬁed in [4]. This includes how to provide seamless and real-time VR video with high

quality through unstable wireless channels, solve handover issues when VR users are in mobility,

and support the asymmetric and coupled trafc in the uplink and downlink transmission [4]–[7].

There are growing research interests in wireless VR networks. In [8], the echo state network

(ESN) was proposed to solve the resource block allocation problem in both uplink and downlink

wireless VR transmission to maximize the average quality of service (QoS) of VR users.

Extending from [8], the authors in [9] optimized the resource block allocation using ESN to

maximize the success transmission probability accounting for the VR user data correlation. In

[10], through caching part of VR video frames in advance at the server and computing certain

post-processing procedures on demand at the mobile VR device, the joint caching and computing

optimization problem was formulated to minimize the average required transmission rate to

reduce communication bandwidth.

The aforementioned VR research mainly considered the VR video rendering occurs at the VR

device side. For real-time interactive VR applications, the latest 2D video content needs to be

ﬁrst delivered to the VR device via wired/wireless communication, and then rendered to 3D VR

video locally. In reality, human wearing VR device only watches a portion of observable visual

world at any given time, which is so called Field of View (FoV). Rendering the full 360 degree

video in real-time can be costly both for downlink transmission and computation. One potential

solution is to only render the requested FoV each time based on the uplink tracking information

of VR users’ motion, including head and eye movements. According to [11], the data size of

the rendered FoV is 75% of that of the stitched 2D image, which means that the size of data

to be delivered via downlink transmission can be reduced by 25% compared to delivering the

stitched 2D images.

Rendering real-time VR videos with high quality demands the computing unit with high

processing ability, so that the rendering latency can be reduced, unfortunately, the computation

3

ability and battery capacity of wireless VR devices are limited. Recently, mobile edge computing

(MEC) has emerged to push mobile computing and network control to network edge, so as

to enable computation-intensive and latency-critical applications at the resource-limited mobile

devices, which promise dramatic reduction in latency and energy consumption [12].

Shifting the FoV rendering task from VR device to MEC server can not only alleviate the

computation requirement at the VR device, but also potentially decrease the VR interaction

latency especially for those VR users in the same virtual VR environment requesting the same

FoV. With MEC multicast to a group of VR users selecting the same FoV, and unicast to

single VR user selecting unique FoV, the downlink transmission cost of network can be further

decreased. In practice, each MEC has different computation capability, it would be interesting

to explore if we can obtain further gain for multiple VR user groups selecting the same FoV

but different MECs by performing rendering at only one MEC, and then migrate the rendered

FoV wired to other MECs, namely, rendered FoV migration.

As pointed out by [4], the Quality of Experience (QoE) of VR transmission is substantially

different from that of conventional video transmission, due to its unique requirements in the

VR interaction latency, and asymmetric uplink and downlink data rates. Motivated by above, in

this paper, we focus on optimizing the QoE of VR users with interactive VR applications in a

MEC-enabled wireless VR networks, and we develop a decoupled learning strategy to efﬁciently

optimize the QoE in wireless VR system, which can improve the training efﬁciency [13], [14].

The main contributions can be summarized as follows:

• We propose a MEC-enabled wireless VR networks, where the ﬁeld of view (FoV) of each

VR user can be real-time predicted, and the rendering of VR content is moved from VR

device to MEC server with rendering model migration capability.

• With the aim of optimizing the long-term QoE of VR users, we propose a decoupled

learning strategy. This strategy decouples the optimization by separately resolving two-sub

tasks, which are FoV prediction and rendering MEC association with the help of Recurrent

Neural Network (RNN) predictor and Deep Reinforcement Learning (DRL) algorithms,

respectively.

• In order to capture the complex dynamics of the FoV request from each VR device, we

propose the RNN model based on Gated Recurrent Unit (GRU) architecture at the central

controller to predict the requested FoV in the current time slot based on those in previous

time slots sent via uplink. Our results shown that our proposed FoV prediction based on

4

GRU achieves 96% in prediction accuracy.

• Accounting for the geographical and predicted FoV request correlation, we propose cen-

tralized and distributed decoupled Deep Reinforcement Learning (DRL) strategies based

on Deep Q-Network (DQN) and Actor Critic (AC) [15]–[20] to maximize the long-term

QoE of VR users, via determining optimal association between MEC and VR user group,

and optimal rendering MEC for model migration. By comparing with non-learning based

nearest MEC association, our results on centralized and distributed DQN shown substantial

gain in both QoE and VR interaction latency. Interestingly, the rendering model migration

further improve these gains.

The rest of this paper is organized as follows. The system model and problem formulation are

proposed in Section II. RNN-based FoV prediction and DRL-based MEC rendering, migration

and association scheme are presented in Section III. The simulation results and conclusions are

described in Section IV and Section V, respectively.

Fig. 1. Wireless VR system in cellular network.

II. SYSTEM MODEL AND PROBLEM FORMULATION

We consider a wireless VR system where multiple MECs are connected to a central controller

through a ﬁber link and serves KVR VR users via wireless links, as shown in Fig. 1. The central

controller is connected to the core network via ﬁber, and can fetch real-time 2D videos without

distortion from the core network.

A. System Model

Our MEC-enabled wireless VR system is consist of four main parts, including FoV selec-

tion and prediction, uplink transmission, FoV rendering, and multi-group multicast and unicast

Core NetworkCore NetworkMECMECOptical FiberUnicastRenderingin VR DeviceRenderingin MECCentralControllerMulticastCore NetworkMECOptical FiberUnicastRenderingin VR DeviceRenderingin MECCentralControllerMulticast5

Fig. 2. Brownian motion for FoV selection.

downlink transmission.

1) FoV Selection and Prediction: When VR users enjoy the VR video, because of the restricted

area of vision in human eye, they usually watch only a portion of the VR video, namely, the Field

of View (FoV) [21], [22], which speciﬁes a 150◦ × 135◦ (i.e., diagonal 200◦) FoV requirement.

As the FoV is part of the actual rendered 3D VR video, the MEC beneﬁts from obtaining the

tracking information related to the viewport of the VR user, and uses the video characteristics

such as projection and mapping formats to generate FoV. If the VR system could know the

required FoV before transmission, it could render and deliver the FoV to VR users in advance,

which can decrease the latency.

Let us ﬁrst denote the total number of FoVs of a VR virtual environment as NFoV. When VR

users enjoy the 360-degree video, they may randomly select the same or different FoVs in the

continuous time slots. The Brownian motion can be used to model the eye movement of each

VR user corresponding to different FoVs over time slots [23]. To capture the FoV selection in

the 3D VR scenario close to reality, we map the 3D VR view into a large 2D view with NFoV

FoVs in each time. When the VR user’s eyes move inside the cubic in Fig. 2, the corresponding

FoV will be selected. According to Brownian motion [24], [25], the eye movement of the VR

user at the tth time slot can be modeled by an independent Gaussian distribution with variance

2D(t) and zero mean N (0, 2D(t)). Thus, the eye movement of the kth VR user in a mapped

2D VR view at the tth time slot can be expressed as

(cid:52) Sk(t) = {N (0, 2Dk(t)), N (0, 2Dk(t))}.

(1)

At one time slot, the selected FoV of the kth VR user is observed, which can be used for FoV

prediction in the next time slot. For example, we assume that there are 8 2D FoVs in a VR

0123456736912036  6

virtual environment, as shown in Fig. 2. If the VR user selects the 2th FoV at a certain time

slot, in the next time slot, it will select one of FoVs among the 1, 2, 3, 5, 6, 7th FoVs. If the VR

user selects a boundary FoV, such as the 0th FoV at a certain time slot, it can choose one FoV

from the 0, 1, 4, 5th FoVs in the next time slot.

Based on the historical FoV selection in the previous time slots, the wireless VR system can

predict the requested FoV of the kth VR user in the next time slot.

2) Uplink Transmission: For conventional wireless VR system without FoV prediction, each

VR user needs to deliver its actual request FoV to the MEC through uplink broadcast transmis-

sion. To focus on the rendering and downlink transmission, at the tth time slot, we assume that

the received FoV (cid:100)F oV

t

is equal to the actual requested FoV F oV

t

following

t = F oV
(cid:100)F oV

t

.

(2)

For our proposed system with FoV prediction, the uplink received FoV F oV

t

after prediction will

be used to check the correctness of predicted FoV (cid:103)F oV

t

. We deﬁne the FoV prediction accuracy

over T time slots as

PFoV =

1
T

(cid:34)

T
(cid:88)

t=1

F oV

t − (cid:103)F oV
F oV
t

t

(cid:35)

× 100%.

(3)

Fig. 3. FoV rendering.

3) FoV Rendering: When VR user requests to watch VR video frames, based on the predicted

or uplink received FoV, the corresponding portion of the sphere can be rendered at MEC or VR

device.

• As shown in Fig. 3 (a) and (b), when the rendering function is executed at the MEC, a

stitched 2D image, whose color model is RGB, will be rendered into the required FoV

through equirectangular projection (ERP) mapping [22] for downlink multicast or unicast

transmission.

FoV Rendering at MECUnicast(a)(b)ERP MappingVR UserUnicastRendering at VR DeviceMulticast(c)UnicastRendering at VR DeviceMulticast(c)MulticastStitched 2D ImageFoV Rendering at MECUnicast(a)(b)ERP MappingVR UserUnicastRendering at VR DeviceMulticast(c)MulticastStitched 2D Image7

• When the rendering function is processed at the VR device, the stitched 2D picture frames

will be ﬁrst multicast or unicast to the VR users. Then, it will be rendered into the required

FoV via ERP mapping, which is shown in Fig. 3 (c).

Here, the number of pixels in the stitched 2D image is used to quantify the size of the executed

data during FoV rendering. To evaluate the best rendering strategy, we propose three FoV

rendering schemes as detailed below:

(a) MEC Rendering without Migration Scheme: In this scheme, the rendering from stitched

2D image to 3D FoV occurs at each MEC with associated VR group.

(b) MEC Rendering with Migration Scheme: With different computational capabilities at

each MEC, multiple VR user groups selecting the same FoV but different MECs only performs

FoV rendering at only one MEC, and this selected MEC can migrate the rendered FoV to other

MECs via ﬁber links to save the computational resources.

(c) VR Device Rendering Scheme: This scheme is a conventional scheme for comparison,

where the FoV rendering occurs only at the VR device, such that the stitched 2D picture frames

need to be transmitted to the VR device for rendering locally using ERP mapping. Due to the

fact that the computation ability of the VR device is much smaller than that of the MEC, we

expect that it may cost much more time for the VR device to render the required FoV.

4) Multi-group Multicast and Unicast: Based on the received FoV in the uplink or the

predicted FoV of each VR user, the VR users with the same received/predicted FoVs can be

grouped together. After FoV rendering, the MECs will multicast the required FoVs to VR users

selecting the same FoV, or unicast a single VR user selecting unique FoV, respectively. Let us

consider a set of B ={1, 2,..., B} MECs, and each MEC is equipped with N transmit antennas.

These B MECs serve the downlink transmission for B VR user groups V = {V1, V2, ..., VB} with

single antenna. The B VR user groups can be multicast group, unicast group, or inactive group

with no VR users. Assuming that there are M multicasting groups and U unicasting groups

(M + U ≤ B), these M muslticast groups and U unicast groups can be denoted as the sets

V mul = {V mul

1

, V mul
2

, ..., V mul

M } and V uni = {V uni

1

, V uni
2

, ..., V uni

of VR users in the kth multicasting group denoted as |V mul
be calculated as KVR = (cid:80)M
k=1 |V mul
one group.

k

k

U }, respectively. With the number
|, the total number of VR users can

| + U . Note that each VR user can only be assigned to just

8

B. Mathematical Model

1) FoV Rendering Model: We denote the number of pixels as R, and the size of each pixel is

8 bits. For the MEC rendering schemes, at each time slot, the size of the FoV to be transmitted

in the downlink can be calculated as

C = R × R × 3 × 8 × V = 48R2,

(4)

where the single-eye resolution is R×R, 3 presents the red, green and blue color in RGB model,

and V is the number of viewpoints with V = 2 for two eyes. According to [4], the resolution

of the FoV is at least 1080p, and C can be very large when R is high. Usually, the FoV has to

be compressed before downlink multicast or unicast. By assuming the compression ratio as CR,
the size of the compressed data for downlink transmission can be calculated as C
CR

. In addition,

through ERP mapping, 25 percent pixels of the stitched 2D image can be reduced [11], which

means that the data size of the FoV is 75 percent of that of the stitched 2D image. Thus, the
3C = 64R2, and 64R2 bits data

data size of the stitched 2D image can be calculated as M = 4

are required to be executed in the ERP mapping step.

2) Downlink Transmission Model: For VR users in the multicast groups, the multicast signal

between the bth MEC and the kth VR user in the jth multicast group at the tth time slot can

be written as

ymul
jk,b(t) = hH

j,b (t)xmul

b

jk,b(t)vmul
(cid:88)

jk,i(t)vmul
hH

m,i (t)xmul

i

(t)+

(t)+

(5)

i ∈V mul/V mul
V mul
(cid:88)

b

,m∈V mul

i

jk,l(t)vuni
hH

u,l (t)xuni

u (t) + njk(t),

l ∈V uni,u∈V uni
V uni

l

where hjk,b(t) ∈ CM ×1 ∼ CN (0, αIM ) is the uncorrelated Rayleigh fading channel vector
between the bth MEC and the kth VR user in the jth multicast group, α is the large-scale
u,l (t) ∈ CM ×1 are the
multicast and unicast vectors from the bth and lth MECs connected to the VR users in the

fading coefﬁcient of the multiscast VR users. vmul

j,b (t) ∈ CM ×1 and vuni

jth multicast group and the uth VR user in the unicast group, respectively. In (5), xmul

b

(t) and

xuni
u (t) are the multicast and unicast messages intended for the VR users in the jth multicast
group and the uth VR user in the unicast group, respectively. We assume that xmul
u (t)
are independent from each other. Meanwhile, (cid:80)
hH
jk,i(t)vmul
(t) and
(cid:80)

u (t) are the interference from the other MECs that provide

b
m,i (t)xmul

(t) and xuni

i ∈V mul/V mul
V mul

u,l (t)xuni

hH
jk,l(t)vuni

,m∈V mul

i

b

i

l ∈V uni,u∈V uni
V uni

l

FoVs for the VR users in other multicast and unicast groups, respectively. In addition, njk(t) ∼
CN (0, σ2
IM ) is the additive white Gaussian noise at the kth VR user in the jth multicast group.
jk

Based on (5), the multicast transmission rate between the kth VR user in the jth multicast

group and the bth MEC at the tth time slot can be expressed as

9

(cid:32)

Rmul

jk,b(t) = log2

1 +

where

|hH

j,b (t)|2

jk,b(t)vmul
Imul
jk,b(t) + σ2
jk

(cid:33)

,

(cid:88)

|hH

Imul
jk,b(t) =
i ∈V mul/V mul
V mul
m∈V mul
i

b

jk,i(t)vmul

(cid:88)

m,i (t)|2+
l ∈V uni
V uni
u∈V uni
l

|hH

jk,l(t)vuni

u,l (t)|2.

(6)

(7)

For the VR users in the unicast groups, the unicast signal between the bth MEC and the kth

VR user at the tth time slot can be expressed as

yuni
k,b (t) = gH

k,b (t)xuni

b (t)+

k,b(t)vuni
(cid:88)

k,i(t)vuni
gH

u,i (t)xuni

i

(8)

(t)+

i ∈V uni/V uni
V uni
(cid:88)

b

,u∈V uni

i

gH
k,l(t)vmul

m,l (t)xmul

l

(t) + nk(t),

l ∈V mul,m∈V mul
V mul

l

where gk,b(t) ∈ CM ×1 ∼ CN (0, βIM ) is the uncorrelated Rayleigh fading channel vector
between the bth MEC and the kth VR user in the unicast group, and β is the large-scale fading
coefﬁcient for the unicast VR users. Meanwhile, (cid:80)
(cid:80)

(t) are the interference from the other MECs that provide ser-

k,i(t)vuni
gH

i ∈V uni/V uni
V uni

u,i (t)xuni

(t) and

,u∈V uni

k,l(t)vmul
gH

m,l (t)xmul

i

l

b

i

V mul
l ∈V mul,m∈V mul

l

vice for VR users in the unicast and multicast groups, respectively. In (8), nk(t) ∼ CN (0, σ2

kIM )

is the additive white Gaussian noise at the kth VR user in the unicast group.

Based on (8), the unicast transmission rate between the kth VR user and the bth MEC at the

tth time slot can be written as

(cid:32)

Runi

k,b (t) = log2

1 +

where

|gH

k,b (t)|2

k,b(t)vuni
Iuni
k,b (t) + σ2
k

(cid:33)

,

(cid:88)

|gH
Iuni
k,b (t) =
i ∈V uni/V uni
V uni
u∈V uni
i

b

k,i(t)vuni

(cid:88)

u,i (t)|2+
|gH
l ∈V mul
V mul
m∈V mul
l

k,l(t)vmul

m,l (t)|2.

(9)

(10)

10

Fig. 4. VR interaction latency of the proposed MEC and VR device rendering schemes.

3) VR Interaction Latency: As deﬁned in [4], [26], the VR interaction latency T loop is the

time starting from the VR user’s movement to the time where the virtual environment responds

to its movements. It is consisted of four parts: 1) the time of the VR user to uplink its FoV

request, and other tracking information (T uplink); 2) the time of the FoV rendering at MECs or

VR devices to generate the predicted or uplink requested FoV (T render); 3) the time to migrate

the rendered FoV from one optimal MEC to the other MECs (T migration) with the VR groups

selecting the same FoV via ﬁber; and 4) the time to transmit the rendered FoV or the stitched

2D picture frames from the MEC to the VR user (T downlink) depending on rendering at the MEC

or the VR device, respectively. Thus, the VR interaction latency T loop can be calculated as

T loop = T uplink + T render + T migration + T downlink,

(11)

as shown in Fig. 4 (a).

Let us assume the execution ability of the GPU of the kth MEC or VR device as F MEC

k

and F VR

k , respectively. We use f MEC

k

and f VR

k

to represent the number of cycles required for

processing one bit of input data of the kth MEC or VR device, respectively. Here, the number of

cycles depends on the application type and the GPU architecture of the kth MEC or VR device.

For the MEC rendering scheme with migration, we assume that the bth MEC is selected to be

tt(b) MEC Rendering Scheme(c) VR Device Rendering Schemet(c2) With Prediction Downlink MigrationRenderingUplinkt(a) Durations of Different ProceduresDownlink MigrationRenderingUplinkt(a) Durations of Different Procedures(b1) Without Prediction and Without Migration(b1) Without Prediction and Without Migration(b2) Without Prediction and With Migration(b2) Without Prediction and With Migration(b3) With Prediction and Without Migration(b3) With Prediction and Without Migration(b4) With Prediction and Migration(b4) With Prediction and Migration: Received FoV at the tth time slot: Predicted FoV for the (t+1)th time slot: Received FoV at the tth time slot: Predicted FoV for the (t+1)th time slot: Actual FoV at the tth time slot: Received FoV at the tth time slot: Predicted FoV for the (t+1)th time slot: Actual FoV at the tth time slot(c1) Without Prediction (c1) Without Prediction t(b) MEC Rendering Scheme(c) VR Device Rendering Schemet(c2) With Prediction Downlink MigrationRenderingUplinkt(a) Durations of Different Procedures(b1) Without Prediction and Without Migration(b2) Without Prediction and With Migration(b3) With Prediction and Without Migration(b4) With Prediction and Migration: Received FoV at the tth time slot: Predicted FoV for the (t+1)th time slot: Actual FoV at the tth time slot(c1) Without Prediction + + +         + + + + +     11

only rendering MEC with the same FoV request, the distance between the kth MEC and the bth
MEC is ˆLk,b, and the trasnmission rate of the optical ﬁber is Rﬁber.

The VR interaction latency of the proposed MEC and VR device rendering schemes are shown

in Fig. 4 (b) and (c) and introduced in details as follows:

(a) MEC Rendering without Migration: a-1) Without prediction: At the tth time slot, the

VR user needs to deliver the actual FoV request of VR users F oV

t

through uplink broadcast

transmission, and the rendering of received FoV (cid:100)F oV

t

is executed at each MEC with associated

VR user. As shown in Fig. 4 (b1), at the tth time slot, if the kth VR user is served by the kth

MEC, the VR interaction latency can be calculated as

k = T uplink
T loop

k

= T uplink
k

+ T render
k
f MEC
k M
F MEC
k

+

+ T downlink
k

+

C
CRRdown
k,k

,

(12)

where Rdown

k,k ∈ {Rmul

k,k , Runi

k,k}, and Rmul

k,k and Runi

k,k are given in (6) and (9).

a-2) With prediction: As shown in Fig. 4 (b3), based on the uplink received FoVs at the tth

time slot (cid:100)F oV

t

and several previous time slots, we predict the FoV preference of each VR user

at the (t + 1)th time slot (cid:103)F oV
interaction latency with predicted FoV can be written as (12) with T uplink

t+1. For the kth VR user directly served by the kth MEC, the VR

= 0.

k

(b) MEC Rendering with Migration: b-1) Without prediction: For the VR user groups

requesting the same FoV (cid:100)F oV

t

through uplink transmission only select one MEC for rendering,

and the rendered FoV can be migrated to other MECs. As shown in Fig. 4 (b2), at the tth

time slot, if the kth VR user directly served by the bth MEC performs rendering itself, the VR

interaction latency of the required FoV of the kth VR user can be presented as

k = T uplink
T loop

k

= T uplink
k

+ T render
k
f MEC
b M
F MEC
b

+

+ T downlink
k

+

C
CRRdown

k,b

,

(13)

where Rdown

k,b = Rmul

k,b and Rmul
k,b

is given in (6).

If the kth VR user is served by the kth MEC, where the rendering is not performed by itself,

but by the bth MEC, the interaction latency of the kth VR user can be presented as

k = T uplink
T loop

k

+ T render
k

+ T migration
k
ˆLk,b
Rﬁber

+

+ T downlink
k

+

C
CRRdown
k,k

,

(14)

f MEC
b M
F MEC
b

= T uplink
k

+

12

where Rdown

k,k = Rmul

k,k and Rmul

k,k is given in (6).

b-2) With prediction: As shown in Fig. 4 (b4), at the tth time slot, for the kth VR user directly

served by the bth MEC, the VR interaction latency with the predicted FoV of the kth VR user
can be denoted as (13) with T uplink

= 0.

k

Otherwise, the predicted FoV will be migrated to the kth MEC from the bth MEC, and the

VR interaction latency of the predicted FoV of the kth VR user can be denoted as (14) with
T uplink
k

= 0.

(c) VR Device Rendering: c-1) Without prediction: According to Fig. 4 (c1), at the tth time

slot, for the kth VR user served by the kth MEC without FoV prediction, the VR interaction

latency of the kth VR user can be written as

k = T uplink
T loop

k

+ T downlink
k

= T uplink
k

+

M
CRRdown
k,k

+ T render
k
f VR
k M
F VR
k

+

(15)

,

where Rdown

k,k ∈ {Rmul

k,k , Runi

k,k}, and Rmul

k,k and Runi

k,k are given in (6) and (9).

c-2) With prediction: As shown in Fig. 4 (c2), at the tth time slot, the FoV preference of

the VR users for the (t + 1)th time slot will be predicted, and the VR interaction latency with

the predicted FoV of the kth VR user served by the kth MEC can be presented as (15) with
T uplink
k

= 0.

4) VR Quality of Experience: The quality of the FoV can be inﬂuenced by many factors,

such as blockiness, blur, contrast distortion, freezing, colour depth, sharpness, etc [27], [28]. To

evaluate the performance of the proposed MEC rendering schemes, we focus on the objective in

maximizing the Peak Signal-to-Noise Ratio (PSNR) [29], knowing that it is the most common

and simple objective VR video quality assessment, and the PSNR is usually deﬁned by the Mean

Squared Error (MSE) of the kth VR user between an initial FoV Ik and the distorted FoV Dk.

According to [30], to measure the QoE of the kth VR user based on the MSE, we propose a

binary function where Ik = 1 and Dk ∈ {0, 1} to represent whether the FoV can be rendered

and delivered within the threshold of VR interaction latency of the kth VR user. For real-time

interactive VR applications, the delayed FoV will bring unpleasant human experience, thus, for

the kth VR user, we revise basic QoE model by incorporating a maximum VR interaction latency

requirement T th

k . More speciﬁcally, if Tk ≤ T th

k , the rendered FoV is regarded as successfully

13

delivered to VR device, then Dk = 1, otherwise, Dk = 0. The MSE of the kth VR user can be

written as

MSEk = (Ik − Dk)2.

According to [30, Eq. (2)], the PSNR of the kth VR user is deﬁned as

PSNRk = 10 log10

1
MSEk

.

(16)

(17)

As can be seen from (17), for MSEk = 0, PSNRk → ∞. To avoid the inﬁnite value of PSNR,

we introduce a positive number (cid:52) and modify (17) as

PSNRk = 10 log10

1 + (cid:52)
MSEk + (cid:52)

,

(18)

where (cid:52) > 0 and we set (cid:52) = 1 in this paper.

C. Problem Formulation

To ensure that each requested FoV is rendered and transmitted within the VR interaction

latency, we aim to optimize the total QoE under ﬁxed VR interaction latency constraint via

determining the optimal association between MEC and VR user group, and optimal rendering

MEC for model migration.

The proposed MEC rendering schemes aim at maximizing the long-term total QoE under VR

interaction latency constraint in the continuous time slots with respect to the policy π that maps

the current state information St to the probabilities of selecting possible actions in At. Therefore,

based on the QoE of each VR user, an optimization problem (P1) is formulated as

(P1) max
π(At|St)

∞
(cid:88)

KVR(cid:88)

t=0

k=1

γtEπ[PSNRk]

Tk ≤ T th
k ,

(19)

(20)

where γ ∈ [0, 1) is the discount factor which can determine the weight of the future QoE, and

γ = 0 means that the agent just concerns the immediate reward. The state St contains the index

of the requested FoV, the location of each VR user, and the computation ability of each MEC.

The action At includes the optimal association between MEC and VR user group, and optimal

rendering MEC for model migration.

Since the dynamics of the wireless VR system is Markovian in continuous time slots, this is a

Partially Observable Markov Decision Process (POMDP) problem which is generally intractable.

14

Fig. 5. Decoupled learning strategy for MEC rendering schemes in the wireless VR network.

Here, the parital observation refers to that the MECs can only know the previous FoV requests and

the location of each VR user in the environment, while they are unable to know all the information

of the communication environment, including, but not limited to, the channel conditions, and the

FoV request in the current time slot. Furthermore, the traditional optimization methods may need

the global information to achieve the optimal solution, which not only increase the overhead of

signal transmission, but also increase the computation complexity. Approximate solutions will

be discussed in Section III.

III. DEEP REINFORCEMENT LEARNING-BASED MEC RENDERING SCHEME

Knowing the deep neural networks as one of the most impressive non-linear approximation

functions, DRL is an effective method to optimally solve POMDP problems [14]. In this section,

to solve (P1), a decoupled learning strategy is proposed for FoV prediction and MEC rendering

association, as shown in Fig. 5. Speciﬁcally, a RNN model based on GRU is used to predict

FoV preference of each user over time. Then, four DRL algorithms, including centralized DQN,

distributed DQN, centralized AC, and distributed AC, are proposed to select the FoV rendering

MEC and the associated MEC for downlink transmission.

A. FoV Prediction

Brownian Motion is used to simulate the eye movement of VR users over time, and assuming

that the uplink received FoV preference of the kth VR user at the tth time slot is (cid:100)F oV

t

k

∈

{1, 2, ..., NFoV}. In order to detect dynamics in FoV preference of each VR user, the proposed

GRUGRUGRUSoftmaxVR User Group 1VR User Group 2VR User Group 3VR User Group NRNN-based FoV PredictorUpdate RNN-based FoV Predictor using loss ∆L Centralized DQNDistributed DQNCentralized ACDistributed ACChoose One of  DRL MethodsReward RtAt C-DQNAt D-DQNAt C-ACAt D-ACError Loss ∆L MECNetwork EnvironmentSystem ParametersOtNew Observation Ot+1VR UserGRUGRUGRUSoftmaxVR User Group 1VR User Group 2VR User Group 3VR User Group NRNN-based FoV PredictorUpdate RNN-based FoV Predictor using loss ∆L Centralized DQNDistributed DQNCentralized ACDistributed ACChoose One of  DRL MethodsReward RtAt C-DQNAt D-DQNAt C-ACAt D-ACError Loss ∆L MECNetwork EnvironmentSystem ParametersOtNew Observation Ot+1VR User   + [+,,,] learning scheme aims at utilizing not only the information presents in the most recent observation

15

Ot = {O1

t , O2

t , ..., OKVR

t

}, where Ok

t = {(cid:100)F oV

t

k

}, but also the historical information in the previous

observations Ht = {Ot−T0+1, ..., Ot−2, Ot−1} given a memory window T0. To recognize FoV
preference over time, a RNN model with parameters θθθRNN, and speciﬁcally a GRU architecture,

is leveraged. θθθRNN is consisted of both the GRU internal parameters and the weights of the

softmax layer. We choose RNN due to its ability in capturing time correlation of FoV preference

over time, which can help learn the time-varying FoV preference for better prediction accuracy.

As shown in Fig. 5, the GRU layer includes multiple standard GRU units and historical obser-

vations [Ot−T0+1, ..., Ot−1, ] sequentially inputted into the RNN predictor. For the kth VR user, the

GRU layer is connected to an output layer which is consisted of a softmax non-linearity with NFoV

output values, which represents the predicted probability P{(cid:100)F oV
of the ˆf th FoV ( ˆf = 1, ..., NFoV) for the tth time slot given historical observations [Ok

t−1], θθθRNN}
t−T0+1, ..., Ok
To adapt the model parameter θθθRNN, standard Stochastic Gradient Descent (SGD) via Back-

t−T0+1, ..., Ok

t

t−1].

k

= ˆf |[Ok

Propagation Through Time (BPTT) [31] is deployed. At the (t + 1)th time slot, the parameters

θθθRNN of the RNN predictor can be updated as

θθθt+1
RNN = θθθt

RNN − λRNN∇LRNN(θθθt

RNN),

(21)

where λRNN ∈ (0, 1] is the learning rate, ∇LRNN(θθθt
RNN) to train the RNN predictor. LRNN(θθθt
LRNN(θθθt

RNN) is the gradient of the loss function
RNN) can be obtained by averaging the cross-

entropy loss as

where

Lt

RNN(θθθRNN) = −

t
(cid:88)

log

(cid:16)
P{(cid:100)F oV

t(cid:48) = (cid:103)F oV

t(cid:48) |Ot

(cid:48)
t(cid:48) −T0

t(cid:48) =t−Tb+1

Ot

(cid:48)
t(cid:48) −T0

= [Ot(cid:48) −T0+1, ..., Ot(cid:48) −1, Ot(cid:48) ],

(cid:17)
, θθθRNN}

,

(22)

(23)

and Tb is the randomly selected mini-batch size.

Through FoV prediction, MECs are able to know the FoV preference of each VR user in

advance. The VR users with the same predicted FoVs can be grouped together. After FoV

rendering, the MECs will multicast or unicast the required FoVs to VR users selecting the same

FoV, or a single VR user selecting unique FoV, respectively.

B. Deep Reinforcement Learning

The main purpose of Reinforcement Learning (RL) is to select proper MECs for MEC

rendering schemes. Through a series of action strategies, MECs are able to interact with the

16

environment, and obtain rewards due to their actions, which help to improve their action strate-

gies. After plenty of iterations, MECs can learn the optimal policy that maximizes the long-term

rewards.

We deﬁne S ∈ S, A ∈ A, and R ∈ Re as any state, action and reward from their corresponding

sets, respectively. According to the observed environmental state St at the tth time slot, MECs

choose speciﬁc actions At from the set A and receive rewards Rt, which are regarded as a metric

to measure whether the selected actions are good. Thus, the purpose of RL algorithm is to ﬁnd

an optimal policy π which can maximize the long-term reward for A = π(S). The optimization

function can be formulated as < S, A, R > and the detailed descriptions of the state, action and

reward of problem (P1) are introduced as follows.

• State: At the tth time slot, the network state can be denoted as

St = ((cid:103)F oV

t

, Lt

k,i, F MEC
i
2

with (cid:103)F oV

t = {(cid:103)F oV

t

1

, (cid:103)F oV
t

, ..., (cid:103)F oV

t

KVR

},

) ∈ S,

(24)

Lt

k,i = {lt

k,1, lt

k,2, ..., lt

k,B, },

F MEC
i

= {F MEC
1

, F MEC
2

, ..., F MEC

B

},

k

where (cid:103)F oV

t

is the index of the predicted FoV of the kth VR user at the tth time slot. lt
k,i

is the distance between the kth VR user and the ith MEC at the tth time slot. F MEC

i

is the

computation capability of the ith MEC.

• Action: The action space can be written as

At = { ˇAt

k,q, ´At

k,i} ∈ A,

(25)

with ˇAt

k,q = { ˇAk,1, ˇAk,2, ..., ˇAk,NFoV},

´At

k,i = { ´Ak,1, ´Ak,2, ..., ´Ak,KVR},

k,q ∈ {0, 1} and ´At

where ˇAt
FoV and serve the ith VR user at the tth time slot, respectively. For instance, if ˇAt
and ˇAt
choosing the same FoV. If ´At

k,i ∈ {0, 1} represent whether the kth MEC will render the qth
k,q = 1
k,j = 0 , the kth MEC will render and migrate the qth FoV to the jth (j (cid:54)= k) MEC
k,i = 1, the kth MEC will support the downlink transmission

of the ith VR user, otherwise, not.

• Reward: The immediate reward Rt is designed as
KVR(cid:88)

Rt(St, At) =

PSNRt
k.

k=1

(26)

Thus, the discounted accumulation of the long-term reward can be denoted as

V (S, π) =

∞
(cid:88)

(γ)t−1Rt(St, At),

t=1

17

(27)

where γ ∈ [0, 1) is the discount factor.

When the number of MECs and VR users are small, RL algorithm can efﬁciently obtain

the optimal policy. However, when a large number of MECs and VR users exist, the state and

action spaces will be scaled proportionally, which will inevitably result in massive computation

latency and severely affect the performance of the RL algorithm. To address this issue, deep

learning is introducted to RL, namely, deep reinforcement learning (DRL), through interaction

with the environment, DRL can directly control the behavior of each agent, and solve complex

decision-making problem. In DRL algorithm, two methods can be used to obtain optimal policy.

One is called value-based optimization, such as DQN, which indirectly optimizes the policy by

optimizing value function. While the other is policy-based optimization, such as AC, which can

directly optimize the policy. In the following sections, four DRL algorithms are introduced in

detail.

Fig. 6. The DQN diagram of the MEC rendering scheme.

1) Centralized DQN: As a value-based DRL algorithm, DQN combines a neural network with

Q-learning and approximates the state-action value function via the deep neural network (DNN).

Using DQN algorithm, a fraction of states are sampled and the neural network is applied to train

a sufﬁciently accurate state-action value function, which is able to effectively solve the problem

of high dimensionality in state space. Furthermore, the DQN algorithm uses the experience replay

to train the learning process of RL. When updating the DQN algorithm, some experiences in the

EnvironmentDQN loss functionCurrent Value NetworkReplay MemoryTarget Value NetworkReset () θ (,;) (,;) (,,,)   (,) (,;)  18

experience replay will be selected randomly to learn, so that the correlation among the training

samples can be broke and the efﬁciency of the neural network can be improved. In addition,

through averaging the selected samples, the distribution of training samples can be smoothed,

which avoids the training divergence.

As shown in Fig. 6, the action-state value function VDQN(S, A) in the DQN agent can be

parameterized by using a function VDQN(S, A; θθθDQN), where θθθDQN is the weight matrix of the

DNN with multiple layers. Consider the conventional DNN, where the neurons between two

adjacent layers are fully connected, which is so-called fully-connected layers. The input of

the DNN is the variables in state St; the hidden layers are Rectiﬁer Linear Units (ReLUs)

through utilizing the function f (x) = max(0, x); the output layer is consisted of linear units,

which are all available actions in At. The exploitation is obtained by performing propagation of

VDQN(S, A; θθθDQN) with respect to the observed state St. Moreover, the parameter θθθDQN can be

updated by using SGD as

θθθt+1
DQN = θθθt

DQN − λDQN∇LDQN(θθθt

DQN),

(28)

where λDQN ∈ (0, 1] is the learning rate, ∇LDQN(θθθt
LDQN(θθθt

DQN) is the gradient of the loss function
DQN) utilized to train the state-action value function. The loss function can be deﬁned as

LDQN(θθθt

DQN) = ( ˆVDQN − VDQN(Si, Ai; θθθt

DQN))2,

where

ˆVDQN = Ri+1 + γ max

A

VDQN(Si+1, A; ¯θθθt

DQN).

(29)

(30)

(Si, Ai, Si+1, Ri+1) are randomly selected previous samples for some i ∈ {t − Mr, ..., t} with
respect to a so-called minibatch. Mr is the replay memory. ¯θθθt
DQN is the so-called target Q-
network which is utilized to estimate the future value of the Q-function in the update rule.
Meanwhile, ¯θθθt

DQN and kept ﬁxed for some
episodes. The use of minibatch, rather than a single sample, to update the state-action value

DQN is periodically copied from the current value θθθt

function VDQN(S, A; θθθDQN) is able to improve the convergent reliability of value function.

Through deriving the loss function in (29) and calculating the expectation of the selected

previous samples in minibatch, V ∗

DQN(S, A) can be obtained. The DQN algorithm is presented

in Algorithm 1.

19

Algorithm 1 DQN to dynamic decision-making and optimization of the MEC rendering scheme
1: Initialize replay memory D to capacity ˆN , learning rate λDQN ∈ (0, 1] and discount factor

γ ∈ [0, 1).

2: Initialize state-action value function VDQN(S, A; θθθDQN), the parameters of primary Q-network

θθθDQN and target Q-network ¯θθθDQN.

3: for episode = 1,...,M do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

Input the network state S of the MEC rendering scheme.

for t = 1,...,T do

Use (cid:15)-greedy algorithm to select a random action At from action space A.

Otherwise, select At = max
A∈A

V (St, A; θθθDQN).

The selected MECs render the predicted or uplink received FoVs and multicast/unicast

them to VR users according to the selected action At.

MECs observe reward Rt and new state St+1.

Store transition (St, At, Rt, St+1) in replay memory D.

Sample random minibatch of transitions (Sj, Aj, Rj, Sj+1) from replay memory D.

if j + 1 is terminal then

ytarget
j

= Rj.

else

ytarget
j

= Rj+1 + γ max

A

VDQN(Sj+1, A; θθθDQN).

end if

Perform a gradient descent step and update parameters θθθDQN according to (28).
Update parameter ¯θθθDQN of the target network every ¯K steps.

end for

20: end for

2) Distributed DQN: In the centralized DRL algorithm, it learns a single optimization policy

centrally at the central controller, which requires the global observations, rewards, and actions of

each MEC. When the number of MECs and VR users increase, the size of the proposed model and

parameters can expand exponentially. In this case, the GPU memory in central controller not only

needs to hold the model and batch of data, but also the intermediate outputs of the feedforward

computation. With dense VR users, GPU memory can be easily overloaded in practice, especially

20

for the GPUs with lower computation capability. Meanwhile, as the number of MECs and VR

users scaling up, the centralized DRL can become inefﬁcient due to the following issues. First,

the training time is bound by the gradient computation time, and the frequency of parameter

updating grows linearly with the number of MECs and VR users. Second, as the frequency of

parameter updating grows, it could potentially slow down the optimization process and result in

problems with convergence [32].

Unlike the centralized DRL algorithm, the global objective in distributed DRL algorithm is the

combination of each agent’s local objective, and each agent needs to optimize its own objective.

In the distributed DQN method, each agent learns independently from the other agents. When one

of the agents selects an action based on the current state, the other agents can be approximated

as part of the environment [33].

In our model, the central controller stores a copy of the model parameter θθθDDQN. The ith MEC

obtains the latest model parameter θθθDDQN from the central controller with (cid:101)θθθi = θθθDDQN. Based on
t in all available actions in Ai. As a result, the
the observed state Si
t will be generated and
fed back to the ith MEC. During training, the parameter (cid:101)θθθi of the ith MEC can be updated as

environment will make a transition to the new state Si

t, it will select an action Ai

t+1 and a reward Ri

t
where λDDQN ∈ (0, 1] is the learning rate, Li((cid:101)θθθ
i) is the loss function of the ith MEC, which can

t
t
t+1
i − λDDQN∇Li((cid:101)θθθ
i = (cid:101)θθθ
(cid:101)θθθ
i),

(31)

be denoted as

where

t
Li((cid:101)θθθ

i) = ( ˆVDDQN − VDDQN(Si

j, Ai

t
i))2,
j; (cid:101)θθθ

ˆVDDQN = Ri

j+1 + γ max
Ai

VDDQN(Si

j+1, Ai;

t
¯
(cid:101)θθθ
i).

(32)

(33)

(Si

j+1, ri

j, Ai
j, Si
j+1) are randomly selected previous samples for j ∈ {t − Mr, .., t} of the ith
t
¯
(cid:101)θθθ
i is the target Q-network which is used to estimate the future value of the state-action
MEC.
value function in the update rule. Furthermore, through deriving the loss function in (32) and
DDQN(Si, Ai) can be obtained. In addition,
the updated parameter (cid:101)θθθi of the ith MEC will be transmitted to the central controller and the

computing the expectation of the selected samples, V ∗

model parameter θθθDDQN can be updated as

θθθDDQN =

1
K MEC
DDQN

KMEC
DDQN
(cid:88)

i=1

(cid:101)θθθi,

(34)

where K MEC

DDQN is the number of the MECs associated with the VR user groups.

21

Fig. 7. The Actor-Critic diagram of the MEC rendering scheme.

3) Centralized AC : In the DQN algorithm, the optimal policy of the MEC rendering scheme

is indirectly obtained through optimizing the state-action value function. However, unlike the

DQN algorithm, AC algorithm is able to directly optimize the policy of the MEC rendering

scheme.

The core idea of the AC algorithm is to combine the advantages of Q-learning (value-

based function) and the policy-gradient (policy-based function) algorithms. Consequently, the

fast convergence of the value-based function and the directness of the policy-based function

are all taken into consideration [34], [35]. As shown in Fig. 7, the AC network is consisted

of two independent networks, namely, an actor network and a critic network. Through learning

the relationship between the environment and the rewards, the critic network is able to get the

potential rewards of the current state. Then, the critic network will guide the actor network to

select proper actions and update the actor network in each epoch. Therefore, the AC algorithm

is usually developed as a two-time-scale algorithm, including critic updating step and actor

updating step, which leads to slow learning efﬁciency. A parameterized policy π(At|St; θθθAC) is

learned to select actions according to the current environment state. Then, the critic network will

obtain the reward feedback from the environment and use the state-value function VAC(St; wwwAC)

to evaluate the performed action. Meanwhile, a time-difference (TD) error is generated to reﬂect

the performance of the performed action.

In particular, after performing action At based on St with policy π, the critic network uses

TD error to evaluate the action under the current state, which can be expressed as

δt = Rt + γVAC(St+1; wwwt

AC) − VAC(St; wwwt

AC).

(35)

EnvironmentTD ErrorCritic Network(Value Function)Actor Network(Policy Function)   22

Algorithm 2 Actor-Critic to dynamic decision-making and optimization of wireless VR system

1: Initialize learning rate λcritic ∈ (0, 1], λactor ∈ (0, 1] and discount factor γ ∈ [0, 1).

2: Initialize parameters θθθAC and wwwAC for the actor and critic network, respectively.

3: Input the network state S of the MEC rendering scheme.

4: for t = 1,...,T do

5:

6:

According to π(A|St; θ), select the action A ∈ A.

The selected MECs render the required FoVs and multicast/unicast them to VR users due

to the selected action At.

7: MECs calculate the immediate reward Rt and obtain the environment state St+1.

8:

9:

10:

11:

Store transition (St, At, Rt, St+1).

Calculate TD error δt according to (35).

Update the parameters wwwAC of the critic network via (36).

Update the parameters θθθAC of the actor network via (37).

12: end for

Then, wwwt

AC can be updated as

wwwt+1

AC = wwwt

AC + λcriticδt∇wwwACVAC(St; wwwt

AC).

(36)

where λcritic ∈ (0, 1] is the learning rate of the critic network.

Meanwhile, in the actor network, the policy gradient method is usually adopted, which directly

selects actions via parameterized policy. The parameter θθθt

AC can be updated as

θθθt+1
AC = θθθt

AC + λactorδt∇θθθAC log π(At|St; θθθt

AC),

(37)

where λactor ∈ (0, 1] is the learning rate of the actor network.

Correspondingly, the parameters in the actor and critic network will be iteratively updated

to maximize the objective function. The detailed AC algorithm of MEC rendering scheme is

proposed in Algorithm 2.

4) Distributed AC: Unlike the centralized AC algorithm, the agent in the distributed AC

algorithm performs action and obtains reward based on its own observed state. For the critic

network in each agent, it shares its estimate of the value function with others through the central

controller. While for the actor network in each agent, it performs individually without the need

to infer the policies of others [36].

In our model, the ith MEC obtains the latest critic model parameter wwwDAC from the central

23

to select action Ai

current environment state Si

controller, and let its own critic parameter ¯wwwi

t obtained by the ith MEC, a parameterized policy πi(Si

t = wwwDAC. At the tth time slot, according to the
t) is learned
t. Then, the critic network in the ith MEC will receive the reward feedback by
t). Similarly, the TD error
t, and

the environment and evaluate the state-value function VDAC(Ai
t of the ith MEC can be calculated to judge the performance of the performed action Ai
δi
the parameter ¯wwwi
t of the critic network of the ith MEC can be updated as

t; ¯θθθi

t; ¯wwwi

t|Si

¯wwwi

t+1 = ¯wwwi

t + ¯λcriticδi

t∇ ¯wwwiVDAC(Si

t; ¯wwwi

t),

where ¯λcritic ∈ (0, 1] is the learning rate of the critic network, and

t = Ri
δi

t + γVDAC(Si

t+1; ¯wwwi

t) − VDAC(Si

t; ¯wwwi

t).

(38)

(39)

Furthermore, for the parameter ¯θθθi

t of the actor network of the ith MEC, it can be updated via

t+1 = ¯θθθi
¯θθθi

t + ¯λactorδi

t∇¯θθθi log π(Ai

t|Si

t; ¯θθθi
t),

(40)

where ¯λactor ∈ (0, 1] is the learning rate of the actor network. In addition, the updated parameter
¯wwwi
t in the critic network of the ith MEC will be sent to the central controller and the critic model

parameter wwwDAC can be updated as

wwwDAC =

1
K MEC
DAC

KMEC
DAC(cid:88)

¯wwwi,

(41)

where K MEC

i=1
DAC is the number of the MECs associated with the VR user groups. Correspondingly,
the parameters in the actor and critic network will be iteratively updated to maximize the objective

function.

IV. SIMULATION RESULTS

In this section, we examine the effectiveness of our proposed schemes with learning algorithms

via simulation. For the learning algorithms, we set the learning algorithms use fully-connected

neural network with two hidden layers and each layer has 128 ReLU units, we set the memory

as 20, the minibatch size as 64, the learning rate for RNN as 0.005, the number of MECs as

8, the number of VR users as 8, NFoV = 8, D(t) = 3, σ2 = −110 dBm, γ = 0.9, α = β = 3,
λDQN = 0.05, λactor = 0.005, λcritic = 0.05, T th = 30 ms, R = 1080p, CR = 200, F MEC
F MEC

max = 5 GHz,
min = 4 GHz, F VR = 2 GHz, f MEC = f VR = 1000 Cycles/bit, and Rﬁber = 10 Gb/s. Consider

a limited square area whose side length is 100 meters.

24

(a)

(b)

Fig. 8.

(a) Total reward of FoV prediction of each epoch via GRU. (b) FoV prediction accuracy of GRU for varying number

of VR users.

A. FoV Prediction

In the FoV prediction scheme, Brownian motion is deployed to simulate the eye movement of

VR users. To obtain high accuracy in predicting FoV preference of each VR user in continuous

time slots, a RNN model basd on GRU architecture is deployed. Fig. 8 (a) plots the total reward

of FoV prediction of each epoch via RNN and Fig. 8 (b) plots the FoV prediction accuracy of

RNN for varying number of VR users, respectively. It is observed that the prediction accuracy

of the RNN remains 96% despite the increasing number of VR devices. This is because RNN

utilizes a memory window with length 20 to store with the input observations, which can capture

the FoV preference of VR users in the past time slots.

B. MEC Rendering Scheme

Four DRL algorithms, including centralized DQN, distributed DQN, centralized AC, and

distributed AC, are proposed to select proper MECs to render and transmit the required FoVs

to VR users. For simplicity, we use “w/ Pred”, “w/o Pred”, “w/ Migra”, and “w/o Migra” to

represent “with prediction”, “without prediction”, “with migration”, and “without migration” in

the ﬁgures, respectively. To guarantee the fairness of each VR user, we use average QoE and

VR interaction latency in the performance results.

Fig. 9 plots the total reward of the MEC rendering with prediction and migration scheme of

each epoch via centralized/distributed DQN/AC learning algorithms. Each result is averaged over

25

Fig. 9. Total reward of the MEC rendering with prediction and migration scheme of each epoch via centralized/distributed

DQN/AC learning algorithms.

100 training trails. It is observed that the total reward and the convergence speed of these four

DRL learning algorithms follows: Centralized DQN > Distributed DQN > Centralized AC >

Distributed AC. This is due to the experience replay mechanism and randomly sampling in DQN,

which use the training samples efﬁciently and smooth the training distribution over the previous

behaviours. As the model parameters in AC algorithm are updated in two steps, including

critic step and actor step, the convergence speed of the AC algorithm is lower. Apparently,

the convergence speed of the centralized learning algorithms is faster than that of the distributed

learning algorithms. This is because the distributed learning needs more time to learn from each

agent with only local observation and reward, whereas centralized learning can learn from global

observations and rewards.

Fig. 10 plots the average VR interaction latency of various MEC and VR rendering schemes

via centralized DQN algorithm for varying uplink transmission latency. We observe that all the

MEC rendering schemes outperform that of the VR rendering schemes, with around 40 ms

gain. This is because the processing ability of the MECs is much higher than that of the VR

devices, and the data size of the FoV is smaller than that of the stitched 2D picture, which jointly

decrease the rendering and downlink transmission latency. We also observe that the average VR

interaction latency of the MEC rendering with prediction and migration scheme remains the

same with increasing the uplink transmission latency, as the MECs do not need to wait for the

uplink transmission of requested FoV from the VR devices before performing rendering.

In Fig. 10, we also compare our proposed learning-based schemes with those without learning.

26

Fig. 10. Average VR interaction latency of various MEC and VR rendering schemes via centralized DQN algorithm for varying

uplink transmission latency.

By comparing with the MEC/VR rendering scheme with nearest association scheme plotted using

dash lines, we see our proposed learning-based MEC/VR rendering schemes achieve substantial

gain in terms of VR interaction latency. This is due to that in the non-learning scheme, the VR

user needs to transmit its requested FoV through uplink transmission and is always associated

with the nearest MEC. Thus, it is possible that the MEC with low processing ability is selected

to render the required FoV, which can increase the rendering latency.

Fig. 11. Average VR interaction latency of the MEC rendering with prediction and migration scheme and the VR rendering

scheme via centralized/distributed DQN/AC learning algorithms for varying uplink transmission latency.

Fig. 11 plots the average VR interaction latency of the MEC rendering with prediction and

27

migration scheme and the VR rendering scheme via centralized/distributed DQN/AC learning

algorithms for varying uplink transmission latency. It is observed that for the MEC rendering

scheme achieves much lower latency (about 40 ms) compared to VR rendering scheme. It is also

seen that for the same rendering scheme, either the MEC or VR, the centralized DQN algorithm

can achieve the minimum average VR interaction latency. This can be explained by the fact

that the centralized learning algorithm learns a single policy common to the whole wireless VR

system based on the global observations, while in the distributed learning algorithm, each agent

only learns its own policy based on local observation.

(a)

(b)

Fig. 12. Average QoE and VR interaction latency of MEC rendering with prediction with/without migration schemes via

centralized/distributed DQN/AC learning algorithms with increasing number of VR users.

Fig. 12 plots the average QoE and VR interaction latency of MEC rendering with prediction

with/without migration schemes via centralized/distributed DQN/AC learning algorithms with

increasing number of VR users, respectively. With increasing the number of VR users, the

average QoE of VR user ﬁrst decreases then becomes nearly stable as shown in Fig. 12 (a),

whereas the average VR interaction latency ﬁrst increases, then becomes nearly stable as shown

in Fig. 12 (b). This is because with increasing number of VR users, more MECs are activated

to provide the downlink transmission for more requested different FoVs, which increase the

interference among those transmission. As the number of VR users becomes too large, all the

MECs become active to serve all VR users to render most of FoVs, the rendering latency and

interference among VR users become stable.

Interestingly, we notice that for both centralized DQN and AC algorithms, we can see the

28

performance gain of the MEC rendering with migration scheme over that without migration

scheme in Fig. 12 (a) and (b). This is because the MECs with higher computing ability will

be selected to render the same required FoV for migration, which decreases the rendering

latency. Importantly, all the learning-based MEC rendering with prediction schemes substantially

outperform the conventional non-learning based MEC rendering with nearest association scheme.

(a)

(b)

Fig. 13. Average QoE and VR interaction latency of MEC rendering with prediction with/without migration schemes via

centralized/distributed DQN/AC learning algorithms with increasing number of MECs.

Fig. 13 plots the average QoE and VR interaction latency of MEC rendering with prediction

with/without migration schemes via centralized/distributed DQN/AC learning algorithms with

increasing number of MECs, respectively. With increasing the number of MECs, the average

QoE of VR user ﬁrst increases then becomes nearly stable as shown in Fig. 13 (a), whereas

the average VR interaction latency ﬁrst decreases, then becomes nearly stable as shown in Fig.

13 (b). This is because as the number of MECs increases, the VR users will have more MEC

choices to be selected, thus, nearer MECs with higher execution ability can be utilized to render

the required FoVs, which reduces the rendering and downlink transmission latency. However, as

the number of MECs becomes too large, all MECs may be activated for rendering and downlink

transmission, which leaves little gain for improvement.

V. CONCLUSIONS

In this paper, a decoupled learning strategy was developed to optimize real-time VR video

streaming in wireless network, which considered FoV prediction and rendering MEC association.

29

Speciﬁcally, based on GRU architecture, a RNN model was used to predict FoV preference of

each VR user over time. Then, based on the correlation between the location and predicted FoV

request of VR users, centralized and distributed DRL strategies were proposed to determine the

optimal association between MEC and VR user group, and optimal rendering MEC for model

migration, so as to maximize the long-term QoE of VR users. Simulation results shown that

our proposed MEC rendering with prediction and migration scheme based on RNN and DRL

algorithms substantially improved the long-term QoE of VR users and the VR interaction latency.

REFERENCES

[1] Facebook, “Oculus rift,” Available: https://www.oculus.com/.

[2] “Virtual

reality

and

augmented

reality

device

sales

to

hit

99 million

devices

in

2021,,”

https://www.capacitymedia.com/articles/3755961/VR-and-AR-device-shipments-to-hit-99m-by-2021., Oct, 2017.

[3] “The reality of VR/AR growth,,” https://techcrunch.com/2017/01/11/the-reality-of-vrar-growth/., Jan, 2017.

[4] F. Hu, Y. Deng, W. Saad, M. Bennis, and A. H. Aghvami, “Cellular-connected wireless virtual reality: Requirements,

challenges, and solutions,” IEEE Commun. Mag., 2020.

[5] Z. Tan, Y. Li, Q. Li, Z. Zhang, Z. Li, and S. Lu, “Supporting mobile VR in LTE networks: How close are we?” in Proc.

ACM Meas. Anal. Comput. Syst., vol. 2, no. 1, pp. 1 – 31, Mar. 2018.

[6] E. Cuervo, K. Chintalapudi, and M. Kotaru, “Creating the perfect illusion : What will it take to create life-like virtual

reality headsets?” in Proc. 19th Int. Work. Mob. Comput. Syst. Appl., pp. 7 – 12, Feb. 2018.

[7] E. Bastug, M. Bennis, M. Medard, , and M. Debbah, “Toward interconnected virtual reality: Opportunities, challenges,

and enablers,” IEEE Commun. Mag., vol. 55, no. 6, pp. 110 – 117, Jun. 2017.

[8] M. Chen, W. Saad, and C. Yin, “Virtual reality over wireless networks: Quality-of-service model and learning-based

resource management,” IEEE Trans. Wireless Comm., vol. 66, no. 11, pp. 5621 – 5635, Nov. 2018.

[9] M. Chen, W. Saad, C. Yin, and M. Debbah, “Data correlation-aware resource management in wireless virtual reality (VR):

An echo state transfer learning approach,” IEEE Trans. Comm., vol. 67, no. 6, pp. 4267 – 4280, Jun. 2019.

[10] Y. Sun, Z. Chen, M. Tao, and H. Liu, “Communications, caching, and computing for mobile virtual reality: Modeling and

tradeoff,” IEEE Trans. Comm., vol. 67, no. 11, pp. 7573 – 7586, Nov. 2019.

[11] “Under the hood: Building 360 video,” https://engineering.fb.com/video-engineering/under-the-hood-building-360-video/,

Oct. 2015.

[12] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey on mobile edge computing: The communication

perspective,” IEEE Commun. Survey Tuts., vol. 19, no. 4, p. 23222358, 4th Quart. 2017.

[13] N. Jiang, Y. Deng, and A. Nallanathan, “Trafc prediction and random access control optimization: Learning and non-

learning based approaches,” arXiv:2002.07759 ., Feb. 2020.

[14] N. Jiang, Y. Deng, A. Nallanathan, and J. Yuan, “A decoupled learning strategy for massive access optimization in cellular

IoT networks,” https://arxiv.org/pdf/2005.01092.pdf., May. 2020.

[15] H. Lee, S. H. Lee, and T. Q. S. Quek, “Energy-efﬁciency oriented trafﬁc ofﬂoading in wireless networks: A brief survey

and a learning approach for heterogeneous cellular networks,” IEEE J. Sel. Areas Commun., vol. 33, no. 4, pp. 627 – 640,

Apr. 2015.

30

[16] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong, “Caching in the sky: Proactive deployment of

cache-enabled unmanned aerial vehicles for optimized quality-of-experience,” IEEE J. Sel. Areas Commun., vol. 35, no. 5,

pp. 1046 – 1061, May 2017.

[17] Z. Chen, T. Lin, and C. Wu, “Decentralized learning-based relay assignment for cooperative communications,” IEEE Trans.

Veh. Technol., vol. 65, no. 2, pp. 813 – 826, Feb. 2016.

[18] J. Song, M. Sheng, T. Q. S. Quek, C. Xu, and X. Wang, “Learning-based content caching and sharing for wireless

networks,” IEEE Trans. Comm., vol. 65, no. 10, pp. 4309 – 4324, Oct. 2017.

[19] H. Lee, S. H. Lee, and T. Q. S. Quek, “Deep learning for distributed optimization: Applications to wireless resource

management,” IEEE J. Sel. Areas Commun., vol. 37, no. 10, pp. 2251 – 2266, Oct. 2019.

[20] G. N. Shirazi, P. Y. Kong, and C. K. Tham, “Distributed reinforcement learning frameworks for cooperative retransmission

in wireless networks,” IEEE Trans. Veh. Technol., vol. 59, no. 8, pp. 4157 – 4162, Oct. 2010.

[21] Y. Bao, T. Zhang, A. Pande, H. Wu, and X. Liu, “Motion prediction based multicast for 360-degree video transmissions,”

in Proc. IEEE SECON, pp. 1 – 9, Jun. 2017.

[22] “5G; 3GPP virtual reality proﬁles for streaming applications,” ETSI TS 126 118, Apr. 2019.

[23] J. A. Roberts, G. Wallis, and M. Breakspear, “Fixational eye movements during viewing of dynamic natural scenes,” Front.

Psychol., vol. 4, no. 797, Oct. 2013.

[24] H. C. Berg, “Random walks in biology,” Princeton, NJ, USA: Princeton Univ. Press, 1993.

[25] S. Kadloor, R. S. Adve, and A. W. Eckford, “Molecular communication using brownian motion with drift,” IEEE Trans.

Nanobiosci., vol. 11, no. 2, pp. 89 – 99, Jun. 2012.

[26] “3rd generation partnership project; Technical speciﬁcation group services and system aspects; Extended reality (XR) in

5G,” 3GPP TR 26.928, Feb. 2020.

[27] S. Wedel, M. Koppetz, J. Skowronek, and A. Raake, “Viprovoq: Towards a vocabulary for video quality assessment in the

context of creative video production,” Oct. 2019, pp. 2387–2395.

[28] V. P. K. M and S. Mahapatra, “Quality of experience driven rate adaptation for adaptive HTTP streaming,” IEEE Trans.

Broadcast., vol. 64, no. 2, pp. 602–620, Jun. 2018.

[29] F. Kuipers, R. Kooij, V. D. DE, and K. Brunnstrom, “Techniques for measuring quality of experience,” Springer, Jun.

2010.

[30] L. U. Choi, M. T. Ivrlac, E. Steinbach, and J. A. Nossek, “Sequence level models for distortion-rate behaviour of compressed

video,” in Proc. IEEE ICIP, vol. 2, pp. II–486, Sept. 2005.

[31] P. J. Werbos, “Backpropagation through time: what it does and how to do it,” Proceedings of the IEEE, vol. 78, no. 10,

pp. 1550 – 1560, Oct. 1990.

[32] H. Y. Ong, K. Chavez, and A. Hong, “Distributed deep Q-learning,” arXiv:1508.04186, Oct. 2015.

[33] A. Galindo-Serrano and L. Giupponi, “Distributed Q-learning for interference control

in OFDMA-based femtocell

networks,” Proc. IEEE 71st Veh. Technol. Conf, pp. 1 – 5, May 2010.

[34]

I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska, “A survey of actor-critic reinforcement learning: Standard

and natural policy gradients,” IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),

vol. 42, no. 6, pp. 1291–1307, Nov. 2012.

[35] Y. Wei, F. R. Yu, M. Song, and Z. Han, “User scheduling and resource allocation in hetnets with hybrid energy supply:

An actor-critic reinforcement learning approach,” IEEE Trans. Wireless Comm., vol. 17, no. 1, pp. 680–692, Jan. 2018.

[36] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked

agents,” in Proc. Intl. Conf. Machine Learn., p. 5872 5881, Jul. 2018.

