Visual Computer journal manuscript No.
(will be inserted by the editor)

Scenior: An Immersive Visual Scripting system based on VR
Software Design Patterns for Experiential Training

Paul Zikas · George Papagiannakis · Nick Lydatakis · Steve Kateros ·
Stavroula Ntoa · Ilia Adami · Constantine Stephanidis

0
2
0
2

y
a
M
3

]

R
G
.
s
c
[

2
v
9
1
7
5
0
.
9
0
9
1
:
v
i
X
r
a

Received: 29/02/2020 / Accepted: 29/04/2020

Abstract Virtual reality (VR) has re-emerged as a
low-cost, highly accessible consumer product, and train-
ing on simulators is rapidly becoming standard in many
industrial sectors. However, the available systems are
either focusing on gaming context, featuring limited ca-
pabilities or they support only content creation of vir-
tual environments without any rapid prototyping and
modiﬁcation. In this project, we propose a code-free,
visual scripting platform to replicate gamiﬁed training
scenarios through rapid prototyping and VR software
design patterns. We implemented and compared two

Paul Zikas
ORamaVR
E-mail: paul@oramavr.com

George Papagiannakis
ORamaVR, Institute of Computer Science, Foundation for
Research and Technology-Hellas, University of Crete, Depart-
ment of Computer Science
E-mail: george.papagiannakis@oramavr.com

Nick Lydatakis
ORamaVR
E-mail: nick@oramavr.com

Steve Kateros
ORamaVR
E-mail: steve@oramavr.com

authoring tools: a) visual scripting and b) VR editor for
the rapid reconstruction of VR training scenarios. Our
visual scripting module is capable to generate train-
ing applications utilizing a node-based scripting system
whereas the VR editor gives user/developer the ability
to customize and populate new VR training scenarios
directly from the virtual environment. We also intro-
duce action prototypes, a new software design pattern
suitable to replicate behavioral tasks for VR experi-
ences. In addition, we present the training scenegraph
architecture as the main model to represent training
scenarios on a modular, dynamic and highly adaptive
acyclic graph based on a structured educational curricu-
lum. Finally, a user-based evaluation of the proposed
solution indicated that users - regardless of their pro-
gramming expertise - can eﬀectively use the tools to
create and modify training scenarios in VR.

Keywords Virtual Reality · Authoring Tool · VR
Training · Visual Scripting

1 Introduction

Stavroula Ntoa
Institute of Computer Science, Foundation for Research and
Technology-Hellas
E-mail: stant@ics.forth.gr

Ilia Adami
Institute of Computer Science, Foundation for Research and
Technology-Hellas
E-mail: iadami@ics.forth.gr

Constantine Stephanidis
Institute of Computer Science, Foundation for Research and
Technology-Hellas, University of Crete, Department of Com-
puter Science
E-mail: cs@ics.forth.gr

Virtual reality has advanced rapidly, oﬀering highly in-
teractive experiences, arousing interest in both the aca-
demic and the industrial community. VR is character-
ized by highly immersive and interactive digital envi-
ronments where user experiences another dimension of
possibilities. As already known from conducted trials
[2], [20], the training capabilities of VR simulations of-
fer skill transfer from the VR to real-life proposing an
eﬀective tool to ﬁt in modern curricula. From pilots to
surgeons, VR has a strong impact on training due to
embodied cognition, psychomotor capabilities (dexter-
ous use of hands) and high retention level [13].

 
 
 
 
 
 
2

Paul Zikas et al.

Authoring tools encapsulate key software function-
alities and features for content creation. The software
architecture of such system empowers programmers with
the necessary tools for content creation. However, ex-
isting platforms do not suﬃciently propose a complete
methodology to reconstruct a training scenario in vir-
tual environments. Training simulations are often im-
plemented in modern game engines using native tools
without any customization or specially designed fea-
tures for generating of immersive scenarios rapidly. In
addition, there are are no prototyped software patterns
specially formulated for VR experiences, leading to com-
plex implementations and lack of code reusability.

Previously we have proven that our VR training
platform [3] makes medical training more eﬃcient. In a
revolutionary clinical study [2] in cooperation with New
York University that established - for the ﬁrst time in
the medical bibliography - skill transfer and skill gen-
eralization from VR to the real Operating Room in a
quantiﬁable, measurable ROI.

In this project, we propose a visual scripting sys-
tem capable to generate VR training scenarios following
a modular Rapid Prototyping architecture. Our initial
goal was to deﬁne how to construct complex training
pipelines from elementary behaviors derived from pro-
totyped and reusable software building blocks. Inspired
from game programming patterns, we implemented new
software design patterns named Actions for VR expe-
riences to support a variety of commonly used interac-
tions and procedures within training scenarios oﬀering
great ﬂexibility in the development of immersive VR
metaphors. We designed our solution as a collection of
authoring tools combining a visual scripting system and
an embedded VR editor forming a bridge from product
conceptualization to product realization and develop-
ment in a reasonably fast manner without the fuss of
complex programming and ﬁxtures. Our goals and de-
sign decisions were the following:

– Educational pipeline: We are interested in repre-
senting an educational process into an eﬃcient data
structure, for simple creation, easy maintenance and
fast traversal.

– Modular Architecture: To support a wide va-
riety of interactions and diﬀerent behaviors within
the virtual environment we want our system to in-
tegrate a modular architecture of diﬀerent compo-
nents linked into a common structure.

– Code-free SDK: Our intentions were to develop a
platform where users can create VR training scenar-
ios without advanced programming knowledge. We
also want to study techniques for the visual creation
of VR experiences.

– Rapid Prototyping: We are interested in mak-
ing reusable prototyped modules to implement more
complex interactive behaviors derived from elemen-
tary blocks. Our goal is to deﬁne basic structural
elements capable to visualize simple behaviors but
when combined recreate complex scenarios.

– VR Software Design Patterns: What are the
beneﬁts of gamiﬁed software design patterns in VR
applications? We aim to support a large number of
interactive behaviors in VR applications to promote
new software patterns specially formulated to speed
up content creation in VR.

This paper is organized as follows. In Section 2 we
present the state of the art in training simulations and
similar authoring tools. In Section 3 we introduce our
solution with a brief description of our software mod-
ules. Section 4 presents the training scenegraph archi-
tecture. Section 5 describes the Action Prototypes and
our rapid prototyping solution. Section 6 presents the
visual scripting tool and Section 7 the VR editor. In
Section 8 we discuss our results from the evaluation pro-
cess. Section 9 concludes and deﬁnes the future work.

2 Related Work

In this section, we present the state of the art in VR
training, its impact on education and similar authoring
platforms.

2.1 The impact of VR in training and education

The engagement of education with novel technological
solutions provide new opportunities to increase collab-
oration and interaction through participants, making
the learning process more active, eﬀective, meaningful,
and motivating [5]. Collaborative VR applications for
learning [14], studies for the impact of VR in expo-
sure treatment [7] as well as surveys for human social
interaction [23] have shown the potential of VR as a
training tool. The cognitive aspect of VR learning is
already known from conducted trials [12]. Recent ex-
amples are featuring the learning capabilities of VR in
surgical simulations [24] with remarkable results.

Focusing on the educational factor, the use of VR for
knowledge transfer and e-learning is now extended as
the R&D grows around entire VR environments where
the learning takes place [19]. Virtual Reality rapidly in-
creases its potential and inﬂuence on e-learning appli-
cations [11] by taking advantage of two basic principles:
a) the embodiment [28] and b) the increased knowledge
retention [6] with immersive environments capable to

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

3

Fig. 1 The architectural diagram of our system. The platform consists of a training scenegraph along with the action proto-
types. In a higher hierarchy, the authoring tools (visual scripting and VR editor) are facilitating tools to generate interactive
behaviors in the virtual environment. Finally, the training scenarios are implemented from the auto-generated code.

present a realistic scenario as it is, as it would be in
real-life.

2.3 Visual Programming

2.2 Authoring tools for content creation

The main concept behind authoring tools is to develop
frameworks capable to generate content with minimal
changes to speed up content creation while improving
product maintenance.

BricklAyeR [29] proposes a collaborative platform
designed for users with limited programming skills that
allows the creation of Intelligent Environments through
a building-block interface. Another interesting project
is ARTIST [17], a platform that provides tools for real-
time interaction between human and non-human char-
acters to generate reusable, low cost and optimized MR
experiences. Its aim is to develop a code-free system
for the deployment and implementation of MR content
while using semantically data from heterogeneous re-
sources.

Another authoring tool, ExProtoVAR [26] generates
interactive experiences in AR featuring development
tools specially designed for non-programmers, without
necessarily a technical background with AR interfaces.
In the ﬁeld of interactive storytelling, StoryTec [15]
platform facilitates an authoring tool to generate and
represent storytelling-based scenarios in various domains
(serious games, e-learning and training simulations).
The platform aims to standardize the content creation
of storytelling experiences following a descriptive for-
mat.

Visual programming is getting more publicity as more
platforms and tools are emerging. We can separate them
into two categories according to their visual appearance
and basic functionalities: a) block-based and b) node-
based scripting languages

Block-based visual languages consist of modular blocks
that represent fundamental programming utilities. Open-
Blocks [27] proposes an extendable framework that en-
ables application developers to build a custom block
programming system by specifying a single XML ﬁle.
Google’s online visual scripting platform Blocky [25]
uses interlocking, graphical blocks to represent code
concepts like variables, logical expressions, loops, and
other basic programming patterns to export blocks to
many programming languages like JavaScript, Python,
PHP and Lua. Another interesting approach is the Scratch
[18] visual programming language which primarily tar-
gets ages 8 to 16 oﬀering an authoring tool to support
self-directed learning through tinkering and collabora-
tion with peers.

On the other hand, node-based visual languages,
represent structures and data ﬂow using logical nodes
to reﬂect a visual overview of data ﬂow. GRaIL [10] was
one of the ﬁrst systems that featured a visual scripting
method for the creation of computer instructions based
on cognitive visual patterns. It was used to make so-
phisticated programs that can be compiled and run at
full speed, or stepped through with a debugging inter-
preter. More recently, [16] published three case studies
on visual programming for building information model-

4

Paul Zikas et al.

ing (BIM) utilizing Dynamo, a graphical programming
framework. In addition, Unity3D game engine has re-
cently announced at their 2020 roadmap [4] an embed-
ded node-based editor and a visual scripting system
that will launch with their next update.

2.4 Editing directly from the VR environment

The development of authoring tools in virtual reality
systems led to the integration of sophisticated function-
alities. One of them is the implementation of immersive
VR editors for the reconstruction of digital worlds di-
rectly from within the virtual environment.

In SIGGRAPH 2017, Unity technologies presented
EditorVR [8], an experimental scene editor that encap-
sulates all the Unity’s features within the virtual en-
vironment giving developers the ability to create a 3D
scene while wearing the VR headset. EditorVR sup-
ports features for initially laying out a scene in VR,
making adjustments to components and building cus-
tom tools.

Except from game engines, model editors are also
emerging into immersive VR model editing. MARUI [1]
is a plugin for Autodesk Maya that lets designers per-
form modeling and animation tasks within the virtual
environment. Another noticeable project is RiftSketch
[9], a live coding VR environment, which allows the de-
velopment and design of 3D scenes within the virtual
space. RiftSketch proposes a hybrid XR system utilizing
an external RGB camera and a leap motion sensor to
record live footage from the programmer’s hands while
coding and project this image into the virtual environ-
ment.

The available VR editors feature scene management
capabilities with intuitive ways to build a scene directly
from within the virtual environment. However, there
are no available authoring tools to oﬀer a complete sys-
tem for developing a behavioral VR experience includ-
ing both the design and the programming aspect.

The state of the art shows that VR platforms do
not provide suﬃcient tools to generate training simula-
tions nor a complete methodology for representing an
educational process in VR.

3 Our Solution

The main goal of this project is to implement and com-
pare three diﬀerent authoring mechanics a) prototyped
scripting, b) visual scripting and c) VR editor for rapid
reconstruction of VR training scenarios based on our
newly deﬁned VR software design patterns. In more de-
tail, the proposed system facilitates a VR playground

to recreate training scenarios using the developed tools
and functionalities. From the developer’s perspective,
this system constitutes a Software Development Kit
(SDK) to generate VR content, which follows a well-
structured educational pipeline. After coding the train-
ing scenario, users can experience the exported simula-
tion.

For rapid operation adaptation to variations, we
implemented a schematic representation of VR experi-
ences to replicate training scenarios in a directed acyclic
graph. By prototyping commonly used interaction pat-
terns we managed to create a customizable platform
able to generate new content with minimal changes.
Inspired from game programming patterns, we imple-
mented new design patterns for VR experiences to sup-
port a variety of commonly used interactions and proce-
dures within training scenarios oﬀering great ﬂexibility
in the development of VR metaphors. We built our sys-
tem as a plugin for Unity3D engine for eﬀective setup
and distribution.

We introduce the following contributions:

– Training Scenegraph: We developed a dynamic,
modular tree data structure to represent the train-
ing scenario following a well deﬁned educational cur-
riculum. A training scenegraph tree stores data re-
garding the tasks where the trainee is asked to ac-
complish, dismantling the educational pipeline into
simpliﬁed elements and focusing on one step at the
time.

– Action Prototypes: We designed reusable pro-
totypes based on VR software design patterns to
transfer behaviors from the real to the virtual world.
Action prototypes populate the training scenegraph
with interactive tasks for the user to accomplish.
They introduce a novel methodology specially for-
mulated for the development of interactive VR con-
tent.

– Visual Scripting: We integrated a Visual Script-
ing system as an authoring tool to export training
scenarios from a node-based, coding-free user inter-
face.

– VR Editor: We embedded a run-time VR Editor
within the training scenarios to give user the ability
to customize and create new scenarios directly from
within the virtual environment.

– Pilot applications: Utilizing the proposed system
we generated two pilot training scenarios: a) a RE-
BOA (Resuscitative Endovascular Balloon Occlu-
sion of the Aorta) training scenario and b) an an-
tique clock restoration.

In the following sections, we present the software

modules and key functionalities of our system.

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

5

4 The Training Scenegraph

To achieve a goal, whether it is the restoration of a
statue, the repair of an engine’s gearbox or a surgical
procedure the trainee needs to follow a list of tasks. We
are referring to those tasks as Actions.

A simple visualization of a training scenario con-
taining Actions would be to link them in a single line
one after another. However, in complex training sce-
narios, a sequential representation would not be very
convenient due to the absence of classiﬁcation and hi-
erarchical visual representation. For this reason, we im-
plemented the training scenegraph architecture. Train-
ing scenegraph is a tree data structure representing the
tasks/Actions of a training scenario. The root of the
tree holds the structure, on the ﬁrst depth we initialize
the Lesson nodes, then the Stage nodes and ﬁnally at
leaf level the Action nodes.

Fig. 2 An example of a training scenegraph tree representing
the simple scenario of hanging a paint on the wall.

One of the main principles of this project was to
modify the training scenegraph and Actions using three
diﬀerent editors (scripting, visual scripting and the VR
editor). To achieve this, the scenegraph data is stored
to an xml ﬁle oﬀering extended functionalities, editing
abilities and easy maintenance of the scenegraph struc-
ture even for complex training scenarios.

5 Action Prototypes

In this section, we analyze how we implemented new VR
software design patterns thought rapid prototyping.

5.1 The IAction Interface

The Action object reﬂects a ﬂexible structural mod-
ule, capable to generate complex behaviors from ba-
sic elements. This also reﬂects the concept idea behind

the training scenegraph; provide developers with funda-
mental elements and tools to implement scenarios from
basic principles. Each Action is described by a script
containing its behavior in means of physical actions in
the virtual environment. Technically, each Action script
implements the IAction interface, which deﬁnes the ba-
sic rules every Action should follow, ensuring that all
Actions will have the same methods and structure. Be-
low we present the components of IAction interface.

– Initialize: This method is responsible to instanti-
ate all the necessary 3D objects for the Action to
operate.

– Perform: This method completes the current Ac-
tion and deletes unused assets before the next Ac-
tion starts.

– Undo: This method resets an Action including the
deletion of instantiated 3D assets and the necessary
procedures to set the previous Actions.

– Clear: Clears the scene from initialized objects and

references from the Action.

Designing a shared interface among the structural
elements of a system is the ﬁrst step to prototype com-
monly used components. This methodology is both ben-
eﬁcial for the user and the developer: a) users are intro-
duced with interactive patterns that are familiar with,
avoiding complex behaviors while b) developers are fol-
lowing the same implementation patterns.

5.2 From Actions to VR Design Patterns

To make our system more eﬃcient we have to limit
the capabilities of the Action entity targeting simple
but commonly used tasks in training. Modeling those
behaviors, we will generate a pool of generic behavioral
patterns suitable for VR applications.

The implementation of Action prototypes was highly
inspired by Game Programming Patterns [22] as an al-
ternative paradigm for VR experiences. The immersion
of virtual environments causes the implementation of
programming patterns to ﬁt into a more interactive way
of thinking. For this reason, the software patterns devel-
oped in this project designed to match the needs for in-
teractivity, embodied cognition and physicality on VR
experiences. For this reason, we implemented the fol-
lowing Action Prototypes:

– Insert Action: is referring to the insertion of an
object to a predeﬁned position. Technically, to im-
plement an Insert Action the developer needs to set
the initial and the ﬁnal position of an object, then
the task for the user would be to take this particu-
lar object and place it in the correct position paying
respect to its orientation.

6

Paul Zikas et al.

Fig. 3 Action Prototypes Architecture diagram.

– Remove Action: describes a step in which the user
has to remove an object using his hands. To imple-
ment a Remove Action the developer needs to deﬁne
the position where the object will be instantiated,
for user to reach and remove it.

– Use Action: refers to a step where the user needs
to interact with an object over a predeﬁned area for
a period of time.
Figure 3 illustrates an architectural diagram of Ac-
tion Prototypes to visualize their dependencies.

Action Prototypes constitute a powerful software
pattern to implement interactive tasks in VR expe-
riences. Unitizing Action Prototypes, developers can
replicate custom behaviors with a few lines of code tak-
ing advantage of their abstraction and reusability. New
Action Prototypes can be easily implemented due to
their abstraction and the use of IAction interface.

5.3 Alternative Paths

The Action prototypes propose a new design pattern for
VR experiences, a modular building block to develop
applications in combination with the training scene-
graph. However, the proposed training scenegraph ar-
chitecture generates VR experiences following a ”static”
pipeline of Actions where user needs to complete a pre-
deﬁned list of tasks. In order to transform the training
scenegraph from a static tree into a dynamic graph, we
introduced Alternative Paths.

A training scenario can lead to multiple paths ac-
cording to the user’s actions and decisions, scenegraph
adapts. In addition, certain actions or even wrong es-
timations and technical errors may deviate the origi-
nal training scenario from its normal path causing the
training scenegraph to modify itself accordingly. Except
for backtracking after wrong estimations and errors, the
Alternative Path mechanic is also used in situations
where the trainee needs to make a particular decision
over a dilemma.

Fig. 4 Top: Insert Action from the clock’s maintenance use
case. Bottom: Insert Action from the REBOA use case. The
green holograms indicate the correct position of 3D objects

From a technical perspective, Actions are able to
trigger alternative path events. Those events will be
advanced to training scenegraph informing about the
necessary follow-up actions. The scenegraph tree will
update its form accordingly by pruning or adding new
nodes to its structure to adapt to the new circum-
stances. This event will trigger a transformation of the
training scenario forcing the user to make additional or
diﬀerent steps due to this diﬀerentiation.

5.4 The Uncanny Valley of Interactivity and VR UX

After experimenting with various design patterns and
interaction techniques for VR, an interesting pattern
appeared regarding the correlation of user experience
and the interactivity of the VR application (ﬁgure 5).
An immersive experience relies signiﬁcantly on the im-
plemented interactive capabilities that form the user
experience. As a result, to make an application more
attractive in means of UX a more advanced interac-
tive system is needed. However, as we implement more
complex interaction mechanics there is a point in time-
line where the UX drops dramatically. At this point,
the application is too advanced and complex for the
user to understand and perform the tasks with ease.
We characterize this feature as heterogeneous behav-

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

7

iorism meaning that user’s actions do not follow a de-
terministic pattern resulting in the inability to complete
the implemented Actions due to their incomprehensible
complexity.

Fig. 5 The uncanny valley of interactivity. Correlation be-
tween User experience (UX) and interaction in VR.

In contrast, applications with limited interactivity
follow a linear increase in their user experience. From
applications where users are only observers (360VR videos)
to cognitive applications, the interaction curve is lin-
ear and VR experiences easy to understand. To over-
come the eﬀect mentioned before, applications need to
drastically enhance their interactivity capabilities and
oﬀer users a more intuitive VR environment to under-
stand how they are supposed to act in the virtual world.
Overpassing the valley of interactivity, applications are
evolving rapidly to follow a psychomotor methodology
integrating embodied cognition for better UX.

6.1 The visual scripting metaphor

The development of a visual scripting system as an as-
sistive tool aimed to visualize the VR training scenario
in a convenient way, if possible ﬁt everything into one
window. The simplicity of this tool was carefully mea-
sured to provide tool used also from non-programmers.
From the beginning of the project, one of the main de-
sign principles was to strategically abstract the software
building blocks into basic elements. The main idea be-
hind this abstraction was the improvement of the vi-
sual scripting and VR editor tools since fundamental
elements construct a better visual representation than
complex ones. To render the visual nodes we exploited
Unity Node Editor Base (UNEB), an open-source frame-
work, which provides basic node rendering and manage-
ment functionalities.

Moving into the visual scripting metaphor, the train-
ing scenegraph data structure forms a dynamic tree,
visualizing the scenario into a node-based editor with
nodes linked together forming logical segments. To con-
struct visual nodes, our system retrieves data from the
Action scripts through reﬂection and run-time compi-
lation. An example of a complete diagram represent-
ing a training scenario is illustrated in ﬁgure 6. Devel-
opers can utilize visual scripting to generate training
scenarios through interactive UIs. In this way, the con-
tent creation is transformed into a coding-free process,
encapsulating the system principles into equivalent vi-
sual metaphors giving users the ability to generate VR
training content without high demand in software back-
ground.

6 Visual Scripting

The training scenegraph model is capable to generate
applications from reusable fundamental elements (Ac-
tions) supporting basic insert, remove and use behav-
iors in VR. However, what is the next step? What
can be done to enhance the development process and
speed up content creation? The complexity of scene-
graph xml may cause diﬃculties visualizing the scene-
graph nodes, especially for extended training scenarios.
Another point is the programming skills required do de-
velop such experiences. Using the proposed architecture
could be challenging for inexperienced programmers

To eliminate the mentioned diﬃculties, we introduce
visual scripting as an authoring tool to manage, main-
tain and develop VR experiences. Visual scripting en-
capsulates all the functionalities from the base model
oﬀering high visualization capabilities, which are very
eﬀective especially on extended projects.

6.2 Dynamic code generation

Visual scripting generates run-time simple Action scripts
utilizing the information provided from the visual in-
put. After completing the visual construction of an Ac-
tion the next step is to generate the Action script to
save the implemented behavior in a C# code script.

To write C# code run-time, we used CodeDOM [31],
a build-in tool for .NET Framework that enables run-
time code generation and compilation. The abstraction
of Action prototypes oﬀers an elegant implementation
to generate each script using a single virtual method.
To ﬁnalize the Action script, except the Action Type
(Insert, Remove or Use) we also need the interactive
behavior. Action prototypes retrieve this information
directly from the visual scripting editor through the
linked nodes relative to the Action module.

8

Paul Zikas et al.

Fig. 6 A training scenario visualized from the Visual Scripting Editor featuring from right to left: Lessons (red), Stages
(green), Actions (blue), Action Scripts (gray) and Prefab nodes (brown). Prefabs are representing 3D objects in the virtual
environment. For example an Insert Action contains two prefabs: the interactable item and its ﬁnal position.

6.3 Expanding auto-generated scripts

Visual scripting generates a basic Action script contain-
ing the Initialize method, the minimum requirement for
an Action to run properly. However, there are cases
where developers need to implement signiﬁcantly com-
plex Action behaviors to enhance use experience with
additional information and features for a better expe-
rience.

Prototyped Actions were developed using a partic-
ular software architecture capable to provide the fun-
damental facilities but also customize Actions accord-
ing to the developer’s preferences. The Perform method
can be overridden directly from the Action script to
extend the Action’s capabilities. The same principle is
applicable to all the other virtual methods deﬁned in
the IAction interface (Undo, Initialize, etc.). For addi-
tional modiﬁcations, the best practice is to edit directly
the generated script and override the declared IAction
methods. In this way, we maintain simple scripts but
also provide custom implementations upon request to
ﬁt the training scenario.

7 VR Editor

The visual scripting system enhanced the usability and
eﬀectiveness of the scenegraph system to generate gam-
iﬁed training scenarios through a coding-free platform.
The impact on content creation was very strong due to
the additional tools and features that introduced. How-

ever, visual scripting lacks on one speciﬁc and rather
important feature: the ability to design on-the-go be-
haviors and scenarios directly within the virtual envi-
ronment. This feature will improve design capabilities
while oﬀering an intuitive way to modify applications
directly from the virtual environment.

The implementation of VR editor was designed as
an authoring tool on top of the training scenegraph ar-
chitecture, utilizing the developed features of our sys-
tem. This interactive tool reduces the time needed to
produce training scenarios due to the rapid in-game
generation of training scenarios. In addition, certain
interactive behaviors are better designed directly from
VR instead of a window due to the 3D perspective of
the medium.

7.1 The VR metaphor

The main concept behind the implementation of our
VR editor focuses on an interactive system with ﬂoppy
disks and a personal computer. Figure 7 illustrates the
design of our VR editor along with its various compo-
nents and ﬂoppy disks. The training scenegraph nodes
are represented by ﬂoppy drives on the left side of the
screen. Action scripts are initialized as ﬂoppy disks,
each one holds the script behavior that deﬁnes the 3D
objects relative to the Action. There are three types of
ﬂoppy disk separated with unique coloration; blue disks
represent Use Actions, red disk the Remove Actions and
black disks the Insert Actions.

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

9

ment new ideas and ﬁx wrong Action behaviors without
specialized programming knowledge.

8 Evaluation

To examine the overall experience of using our system,
we conducted a user-based evaluation with 18 users [21].
The main research questions were the following:

– For the VR training application: what is the overall
perceived quality of the VR training environment
and perceived educational value?

– For the Visual Scripting tool: can users successfully
complete basic programming tasks and how do they
rate the overall experience?

– For the VR Editor tool: can users successfully com-
plete basic adjustments to an existing training sce-
nario and how do they rate the overall experience?

8.1 Methodology and participants

The experiment was divided into three separate ses-
sions, one for each tool. In the ﬁrst session, the partic-
ipants were asked to restore an antique clock following
the instructions given by the VR training application
(Clock repair scenario). In the second session, the par-
ticipants were shown the capabilities and functionalities
of the Visual Scripting tool. Then, they were asked to
use the tool to generate code for a ”Use” action (’Use
the sponge to wipe dirty spot on the clock ) and a Remove
action (Remove seal from two-sided gear’ ). Finally, in
the third session, the participants were asked to com-
plete two tasks to adjust the clock restoration training
scenario directly from the VR environment using the
VR Editor tool.

A 10-point Likert Scale questionnaire was given at
the end of each session to rate the parameters identi-
ﬁed in the research questions. For the educational value
of the VR training application, participants were also
asked to indicate which steps they retained regarding
the restoration process. In addition, metrics such as the
number of help requests and time on task were recorded
for further analysis of the results. Finally, at the end of
the experiment, a semi-structured interview was con-
ducted to capture participants general impression of the
whole system.

Eighteen people participated in our experiment, 11
males and 7 females. All users were in the 25-35 age
range. They were selected based on the level of exper-
tise in using VR applications and level of expertise in
Software Development (SD), ensuring an equal number
of expert and non-expert participants in each one of the
two categories. [30], [21]

Fig. 7 Interacting with the VR editor. User holds a Use Ac-
tion preparing to generate the Action behavior.

The right panel contains the properties of the se-
lected drive. On the top side, we set the name of the
scenegraph node along with its type (Lesson, Stage or
Action). To implement a new script for an action, the
user needs to take a ﬂoppy disk and insert it in the
corresponding ﬂoppy drive.

7.2 Generate Actions and parametrization on-the-go

The functionality with the higher impact on the VR
editor is by far the ability to modify and parametrize
Actions on-the-go. This was also the main reason that
led us to implement the VR editor as an additional au-
thoring tool within the virtual environment, to support
coding-free development and give the user the ability
to modify or even generate new behaviors while play-
ing the training scenario.

Users can customize the scenegraph through VR ed-
itor by adding or deleting Scenegraph nodes to match
their needs. This functionality has a serious impact on
users that want to parametrize existing VR training
scenarios or create their own without having any pro-
gramming knowledge. We provide this ability via an
interactive UI on the VR editor with physical buttons
and knobs where users can modify and save the training
scenegraph.

The next step is the script generation from the VR
editor. To generate a new Action, users need to insert a
ﬂoppy disk into the drive representing the Action script
(Insert, Remove or Use). The system will register the
insertion of ﬂoppy disks and an empty Action script
will appear on VR editor screen ready for modiﬁcation.
In a similar way, ejecting a ﬂoppy disk from the drive
detaches the script from the Action.

With VR editor, users are no longer just observers,
they can modify the training scenarios on-the-go, imple-

10

8.2 Results

8.2.1 First session: VR Training application

The perceived quality of the VR experience was on av-
erage highly rated by all participants (8.6/10). The av-
erage rating of VR experts was somewhat lower than
that of non-experts, but the diﬀerence was not statisti-
cally signiﬁcant, as revealed by a paired t-test analysis
(t(8)=-1.83, p=0.1).

Equally high was the overall average rating score the
application received in terms of the perceived educa-
tional value (8.78/10). Small diﬀerences were exhibited
between VR experts and non-experts, however, no sta-
tistical signiﬁcance was identiﬁed (t(8)=-0.45, p=0.66).

The participants also scored rather high in the exer-
cise where they indicated which steps they could recall
from the restoration process (9.26/10). A comparison of
the achieved score between the two groups did not in-
dicate a statistically important diﬀerence (t(8)=-1.08,
p=0.31).

All

Experts Non-experts

Percieved quality of VR experience

Avg
StDev
CI1

8.67
1.029
0.512

8.22
1.202
0.924

Percieved educational value

Avg
StDev
CI

Avg
StDev
CI

8.67
1.00
0.769

8.78
8.88
0.43
Recall activity score
9.26
1.30
0.65

8.93
1.66
1.28

9.11
0.601
0.462

8.89
0.78
0.601

9.63
0.73
0.56

Table 1 VR training application - rating scores

Time on task and the number of help requests were
recorded to support further analysis of participants rat-
ings with regard to the eﬀort they invested. All partic-
ipants were able to complete the steps of the training
scenario successfully and within a reasonable time (2:50
minutes on average). Slightly higher completion time
was recorded on average for the non-experts, but with
no statistical signiﬁcance (t(8)=-1.5, p=0.17). However,
there was a statistically signiﬁcant diﬀerence in the
number of help requests between the two groups (t(8)=-
4.6, p=0.001), as expected, due to the inexperience of
the users.

1 CI: 95% Conﬁdence Interval

Paul Zikas et al.

All

Experts Non-experts

Time (min)

Avg
StDev
CI

Avg
StDev
CI

2:50
0:59
0:29

2:38
1:00
0:46

# of help requests

2.56
1.20
0.60

1.78
0.97
0.75

3:03
1:00
0:46

3.33
0.87
0.67

Table 2 VR training application - time on task and number
of help requests

8.2.2 Second session: Visual Scripting tool

All participants rated highly the perceived easiness of
completing the two given script tasks with the Visual
Scripting tool. However, paired t-testing revealed a sta-
tistically signiﬁcant diﬀerence in the scores received for
Task 1 by the SD experts and by the non-experts; t(4)=-
5.66, p = 0.005. Similarly, there was a statistically sig-
niﬁcant diﬀerence in the scores received for Task 2 by
the SD experts and the non-experts; t(8)=2.8, p=0.02.

All

T1
8.00
1.19
0.59

T2
8.28
0.95
0.48

Experts
T2
T1
8.67
8.44
1.00
1.13
0.77
0.87

Non-experts
T1
7.56
1.13
0.87

T2
7.89
0.78
0.60

Avg
StDev
CI

Table 3 Visual Scripting perceived task easiness for Task 1
(T1) and Task 2 (T2)

These observations are aligned with the diﬀerences
in the measurements of time on task and number of help
requests between the SD experts and the non-experts.
As shown in Table 4 non-experts required both more
time and assistance. Paired t-testing conﬁrmed that
the diﬀerences carried a statistical signiﬁcance both for
time on task (t(8)=-14.69, p=0.0000004) and number
of help requests (t(8)=-2.25, p=0.05).

Nevertheless, it is interesting that the additional re-
quired eﬀort by non-experts to complete the tasks, did
not aﬀect their overall experience with the tool. In fact,
all participants rated highly the overall experience of
using this tool (8.61/10), without any statistical diﬀer-
ence between the two groups; t(8)=-1.1, p=0.3.

8.2.3 Third session: VR Editor tool

The participants also rated highly the perceived easi-
ness of completing the two tasks for the VR Editor tool
evaluation. Just like in the Visual Scripting tool, the
SD experts found the tasks easier than non-experts, a

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

11

All

Experts Non-experts

Time (min)

All

Experts Non-experts

Time (min)

Avg
StDev
CI

Avg
StDev
CI

Avg
StDev
CI

9:41
1:02
0:47

12:36
4:14
2:06
# of help requests
2.89
1.41
0.70
Overall experience
8.61
0.98
0.49

7.88
0.78
1.13

2.11
1.54
1.18

17:31
1:32
1:11

3.67
0.71
0.54

8.44
1.13
0.86

Avg
StDev
CI

Avg
StDev
CI

Avg
StDev
CI

9:08
2:52
2:12

13:24
5:00
2:29
# of help requests
2.83
1.04
0.52
Overall experience
7.61
1.09
0.54

2.22
0.97
0.75

7.77
1.09
0.84

17:39
2:03
1:35

3.44
0.73
0.56

7.44
1.13
0.86

Table 4 Visual Scripting - time on task, number of help
requests (for the entire scenario), and overall experience

Table 6 VR Editor - time on task, number of help requests
(for the entire scenario), and overall experience

diﬀerence which was identiﬁed as statistically impor-
tant both for Task 1 (t(8)=2.56, p=0.02) and Task 2
(t(8)=2.34, p=0.04).

All

T1
7.50
1.26
0.67

T2
7.06
1.39
0.69

Experts
T2
T1
7.67
8.29
1.66
1.38
1.27
1.28

Non-experts
T1
6.89
0.78
0.60

T2
6.44
0.73
0.56

Avg
StDev
CI

Table 5 : VR Editor perceived task easiness for Task 1 (T1)
and Task 2 (T2)

Diﬀerences were exhibited on the time on task and
the number of help requests between the SD experts
and the non-experts as expected and in alignment with
the perceived ease of completing the tasks. Paired t-
testing conﬁrmed that the diﬀerences carried a sta-
tistical signiﬁcance both for time on task (t(8)=-7.1,
p=0.0009) and for the number of help requests: (t(8)=-
3.05, p=0.01).

Just like the observed results in the second ses-
sion with regard to the overall experience, non-experts
gave equally high rates to the overall experience as the
experts, despite the extra time and eﬀort required to
complete the tasks. The overall experience score was
7.61/10, while no statistically important diﬀerence be-
tween the groups was observed (t(8)=0.5, p=0.6).

In conclusion, all participants regardless of their ex-
pertise in VR and SD were able to successfully complete
the tasks. The non-experts did as expected require
more time and assistance, but did not seem to aﬀect
their overall experience in using the tools. The results
of the evaluation matched the general sentiment of the
participants about the overall suite expressed through
the positive comments in semi-structured interviews, as
well as through responding to a corresponding question
in the questionnaire (Figure 8).

Fig. 8 Experience scores for each one of the authoring tools
and the overall suite

9 Conclusions and Future Work

In this work, we presented a novel system capable to
generate gamiﬁed training experiences exploiting its mod-
ular architecture and the authoring tools we developed.
We introduced the scenegraph as a dynamic, acyclic
data structure to represent any training scenario follow-
ing an educational curriculum. In addition, we proposed
a category of new software design patterns, the Action
prototypes, specially formulated for interactive VR ap-
plications. Finally, we developed a visual scripting tool
along with a VR Editor to enhance the visualization
and speed up content creation.

Our system has certain limitations linked with its
components and functionalities. First of all, the evalu-
ation process highlighted weaknesses in the interaction
with the VR editor. Although it behaves well in Action
customization, the script generation process is still com-
plex due to the amount of information and steps needed
from the user. In addition, some of its interactive com-
ponents are not intuitive, resulting in the frustration
of users when asked to implement certain behaviors.
Finally, regarding the visual scripting editor, the real-
time compilation process may cause performance issues

12

Paul Zikas et al.

in complex training scenarios and delay the initializa-
tion of scenegraph.

The purpose of the evaluation was to get an overall
impression of the authoring tools. This did not allow
for in-depth analysis of each tool separately, in terms of
eﬀectiveness and eﬃciency. This is a known limitation
that will be rectiﬁed by conducting further testing for
each tool separately in future iterations.

In the future, we aim to utilize computer vision to
capture the trainer’s movements from external cameras
or directly from within the virtual environment to au-
tomatically generate interactive behaviors in VR. An-
other idea is to collect this data through video from a
real-life scenario by monitoring the trainer and after-
ward processing the data using machine learning to ex-
tract important key features and construct a template
of the training scenario.

Acknowledgements

This project has received funding from the European
Unions Horizon 2020 research and innovation programme
under grant agreement No 871793 (ACCORDION) and
No727585 (STARS-PCP) and supported by Greek na-
tional funds (projects VRADA and vipGPU).

References

1. MARUI

plugin
https://www.marui-plugin.com/marui3/

for

3

Autodesk

Maya.

2. Our clinical trial (citation not provided for the reviewing

process)

3. Our VR training platform (citation not provided for the

reviewing process)

4. Unity 2020 roadmap. https://www.slideshare.net/unity3d/

what-to-expect-in-2020-unity-roadmap-unite-
copenhagen-2019

5. Alsumait, A., Almusawi, Z.S.: Creative and innovative
e-learning using interactive storytelling.
International
Journal of Pervasive Computing and Communications
9(3), 209–226 (2013). DOI 10.1108/IJPCC-07-2013-0016
6. Andersen, S.A.W., Konge, L., Cay-Thomasen, P.,
Srensen, M.S.: Retention of Mastoidectomy Skills After
Virtual Reality Simulation Training. JAMA Otolaryngol-
ogyHead and Neck Surgery 142(7), 635–640 (2016). DOI
10.1001/jamaoto.2016.0454. URL https://doi.org/10.
1001/jamaoto.2016.0454

7. Bouchard, S., Dumoulin, S., Robillard, G., Guitard, T.,
Klinger, E., Forget, H., Loranger, C., Xavier Roucaut,
F.: Virtual reality compared with in vivo exposure in the
treatment of social anxiety disorder: A three-arm ran-
domised controlled trial. The British journal of psychi-
atry : the journal of mental science 210 (2016). DOI
10.1192/bjp.bp.116.184234

8. Ebrahimi, A., West, T., Schoen, M., Urquidi, D.: Unity:
Editorvr. In: ACM SIGGRAPH 2017 Real Time Live!,
SIGGRAPH ’17, pp. 27–27. ACM, New York, NY, USA
(2017). DOI 10.1145/3098333.3098918. URL http://
doi.acm.org/10.1145/3098333.3098918

9. Elliott, A., Peiris, B., Parnin, C.: Virtual reality in soft-
ware engineering: Aﬀordances, applications, and chal-
lenges. In: Proceedings of the 37th International Confer-
ence on Software Engineering - Volume 2, ICSE ’15, pp.
547–550. IEEE Press, Piscataway, NJ, USA (2015). URL
http://dl.acm.org/citation.cfm?id=2819009.2819098
10. Ellis T. O., J.F.H., Sibley, W.L.: The grail project: An ex-
periment in man-machine communications. RAND Cor-
poration pp. RM–5999–ARPA (1969)

11. de Faria, J.W.V., Teixeira, M.J., de Moura Sousa Jnior,
L., Otoch, J.P., Figueiredo, E.G.: Virtual and stereo-
scopic anatomy: when virtual reality meets medical edu-
cation. Journal of Neurosurgery JNS 125(5) (2016)
12. Ganier, F., Hoareau, C., Tisseau, J.: Evaluation of pro-
cedural learning transfer from a virtual environment to a
real situation: a case study on tank maintenance training.
Ergonomics 57(6). DOI 10.1080/00140139.2014.899628

13. Greenleaf, W.: How vr technology will transform health-
In: ACM SIGGRAPH 2016 VR Village, pp. 1–2

care.
(2016). DOI 10.1145/2929490.2956569

14. Greenwald, S., Kulik, A., Kunert, A., Beck, S., Froehlich,
B., Cobb, S., Parsons, S., Newbutt, N., Gouveia, C.,
Cook, C., Snyder, A., Payne, S., Holland, J., Buessing,
S., Fields, G., Corning, W., Lee, V., Xia, L., Maes, P.:
Technology and applications for collaborative learning in
virtual reality. In: CSCL (2017)

15. Gbel, S., Salvatore, L., Konrad, R.: Storytec: A digital
storytelling platform for the authoring and experiencing
In: 2008 Interna-
of interactive and non-linear stories.
tional Conference on Automated Solutions for Cross Me-
dia Content and Multi-Channel Distribution, pp. 103–
110 (2008). DOI 10.1109/AXMEDIS.2008.45

16. Kensek, K.: Visual programming for building informa-
tion modeling: Energy and shading analysis case studies.
Journal of Green Building 10(4), 28–43 (2015). DOI
10.3992/jgb.10.4.28. URL https://doi.org/10.3992/
jgb.10.4.28

17. Kotis, K.I.: Artist - a real-time low-eﬀort multi-entity
interaction system for creating reusable and optimized
MR experiences. Research Ideas and Outcomes 5, e36464
(2019). DOI 10.3897/rio.5.e36464. URL https://doi.
org/10.3897/rio.5.e36464

18. Maloney, J., Resnick, M., Rusk, N., Silverman, B., East-
mond, E.: The scratch programming language and en-
vironment. Trans. Comput. Educ. 10(4), 16:1–16:15
(2010). DOI 10.1145/1868358.1868363. URL http:
//doi.acm.org/10.1145/1868358.1868363

19. Monahan, T., McArdle, G., Bertolotto, M.: Virtual real-
ity for collaborative e-learning. Computers & Education
50(4), 1339 – 1353 (2008). DOI https://doi.org/10.1016/
j.compedu.2006.12.008

20. Murcia-Lpez, M., Steed, A.: A comparison of virtual
and physical training transfer of bimanual assembly
tasks. IEEE Transactions on Visualization and Computer
Graphics 24(4), 1574–1583 (2018). DOI 10.1109/TVCG.
2018.2793638

21. Nielsen, J.: Chapter 6 - usability testing. In: J. NIELSEN
(ed.) Usability Engineering, pp. 165 – 206. Morgan Kauf-
mann, San Diego (1993). DOI https://doi.org/10.1016/
B978-0-08-052029-2.50009-7

22. Nystrom, R.: Game Programming Patterns. Genever

Benning (2014)

23. Pan, X., Hamilton, A.: Why and how to use virtual real-
ity to study human social interaction: The challenges of
exploring a new research landscape. British Journal of
Psychology 109 (2018). DOI 10.1111/bjop.12290

Scenior: An Immersive Visual Scripting system based on VR Software Design Patterns for Experiential Training

13

24. Papagiannakis, G., Trahanias, P., Kenanidis, E., Tsiridis,
E.: Psychomotor surgical training in virtual reality. Mas-
ter Case Series & Techniques: Adult Hip pp. 827–830
(2017)

25. Pasternak, E., Fenicheland, R., Marshall, A.N.: Tips for
creating a block language with blockly. In: 2017 IEEE
Blocks and Beyond Workshop (B B), pp. 21–24 (2017).
DOI 10.1109/BLOCKS.2017.8120404

26. Pfeiﬀer-Leßmann, N., Pfeiﬀer, T.: Exprotovar: A
lightweight tool for experience-focused prototyping of
augmented reality applications using virtual reality. In:
C. Stephanidis (ed.) HCI International 2018 – Posters’
Extended Abstracts, pp. 311–318. Springer International
Publishing, Cham (2018)

27. Roque, R.: Openblocks : an extendable framework for

graphical block programming systems (2008)

28. Slater, M.: Implicit Learning Through Embodiment in
Immersive Virtual Reality, pp. 19–33. Springer Singa-
pore, Singapore (2017)

29. Stefanidi, E., Arampatzis, D., Leonidis, A., Papagian-
nakis, G.: BricklAyeR: A Platform for Building Rules for
AmI Environments in AR, pp. 417–423 (2019)

30. Tullis, T., Albert, W.: Measuring the User Experience,
Second Edition: Collecting, Analyzing, and Presenting
Usability Metrics, 2nd edn. Morgan Kaufmann Publish-
ers Inc., San Francisco, CA, USA (2013)

31. Villela, R.: Working with the CodeDOM, pp. 155–177.

Apress, Berkeley, CA (2019)

