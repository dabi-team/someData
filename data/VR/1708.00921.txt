Echo State Learning for Wireless Virtual Reality
Resource Allocation in UAV-enabled LTE-U Networks
Mingzhe Chen∗, Walid Saad†, and Changchuan Yin∗
∗Beijing Laboratory of Advanced Information Network,
Beijing University of Posts and Telecommunications, Beijing, China 100876, Emails: chenmingzhe@bupt.edu.cn, ccyin@ieee.org.
†Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA, Email: walids@vt.edu.

7
1
0
2

g
u
A
2

]
T
I
.
s
c
[

1
v
1
2
9
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—In this paper, the problem of resource management
is studied for a network of wireless virtual reality (VR) users
communicating using an unmanned aerial vehicle (UAV)-enabled
LTE-U network. In the studied model, the UAVs act as VR control
centers that collect tracking information from the VR users over the
wireless uplink and, then, send the constructed VR images to the
VR users over an LTE-U downlink. Therefore, resource allocation
in such a UAV-enabled LTE-U network must jointly consider the
uplink and downlink links over both licensed and unlicensed bands.
In such a VR setting, the UAVs can dynamically adjust the image
quality and format of each VR image to change the data size of each
VR image, then meet the delay requirement. Therefore, resource
allocation must also take into account the image quality and format.
This VR-centric resource allocation problem is formulated as a
noncooperative game that enables a joint allocation of licensed and
unlicensed spectrum bands, as well as a dynamic adaptation of VR
image quality and format. To solve this game, a learning algorithm
based on the machine learning tools of echo state networks (ESNs)
with leaky integrator neurons is proposed. Unlike conventional ESN
based learning algorithms that are suitable for discrete-time systems,
the proposed algorithm can dynamically adjust the update speed
of the ESN’s state and, hence, it can enable the UAVs to learn the
continuous dynamics of their associated VR users. Simulation results
show that the proposed algorithm achieves up to 14% and 27.1%
gains in terms of total VR QoE for all users compared to Q-learning
using LTE-U and Q-learning using LTE.

I. INTRODUCTION

Virtual reality (VR) services are viewed as one of the pri-
mary use cases of tomorrow’s 5G cellular networks [1]. Indeed,
wireless VR services will enable users to experience immersive
services, without the constraints imposed by conventional wired
VR systems. To enable the users to experience immersive VR
applications without any constraints, VR systems can be operated
using wireless networking technologies [1]. However, deploying
VR services over wireless cellular systems, such as LTE and
beyond, faces many challenges [1] that range from providing
reliable data transmission to optimizing tracking and resource
allocation.

Some recent works such as in [1]–[5] have studied a number
of problems related to wireless VR. In [1] and [2], the authors
provided a qualitative summary of the challenges of VR over
wireless. However, these works rely on simple toy examples, and
do not provide a rigorous analytical treatment of the wireless
networking challenges of VR. The authors in [3] proposed a
channel access scheme using the unlicensed band for wireless
multi-user VR system. The work in [4] investigated the use of
VR devices to control an unmanned drone with 2.4 GHz Wi-Fi
grid. However, the works in [3] and [4] only focus on how to use
WiFi technology to transmit VR images and do not consider the
resource allocation problem over downlink and uplink, licensed
and unlicensed bands for VR over cellular networks. In [5],
we proposed a wireless VR model that captures the tracking

accuracy, processing delay, and transmission delay and introduced
a machine learning algorithm for resource allocation. However,
our work in [5] is restricted to classical LTE networks in which
the VR devices and static ground base stations can access only
a single, licensed band. However, relying only on the licensed
band may not allow the cellular base stations to meet the high
data rate demands of the VR users. One potential solution is
to exploit the use of LTE over unlicensed (LTE-U) [6] bands
Moreover, in dense urban environments, the presence of high rise
buildings and obstacles, can present a major challenge for meeting
the delay needs for VR transmissions by using conventional,
terrestrial base stations. Instead, a ﬂying base station carried by an
unmanned aerial vehicle (UAV) [7] that can move to an optimal
location to establish communication links with little blockage can
be considered to provide better connectivity and assist any ground
network. Moreover, all of the existing works such as in [1]–[5]
ignore the image quality and image format [8] in the improvement
of the delay of the VR users. Indeed, as the VR image quality and
format change, the data size of each image will be varied and,
hence, wireless base stations will need to dynamically adjust the
image’ quality and format to effectively service the users.

The main contribution of this paper is to introduce a novel
framework for enabling VR applications over UAV-based LTE-U
networks. To the best of our knowledge, this is the ﬁrst work
that jointly considers licensed and unlicensed spectrum resource
allocation, image format and quality allocation for VR users
over UAV-based LTE-U networks. Hence, our key contributions
include:

• For the considered VR applications over a wireless LTE-U
network, we develop an effective resource allocation scheme
that allocates licensed and unlicensed resource blocks over
uplink and downlink and adjusts the image quality and
format for each VR user’s image. The goal of this scheme
is to maximize the quality-of-experience (QoE) of all users
while meeting the delay requirement of the VR system. We
formulate the problem as a noncooperative game in which
the players are the UAVs. Each player seeks to ﬁnd an
optimal resource allocation scheme to optimize the users’
QoE while meeting the delay requirement.

• To solve this game, we propose a learning algorithm based
on echo state networks (ESNs) with leaky integrator neurons
[9]. This ESN-based algorithm can effectively ﬁnd a Nash
equilibrium of the game. The proposed algorithm enables
the UAVs to learn the continuous dynamics of their associ-
ated VR users hence allowing adaptation to environmental
dynamics.

• Simulation results show that the proposed algorithm can,
respectively, achieve 14% and 27.1% gains in terms of total

 
 
 
 
 
 
UAV

VR image

Tracking 
Information

WAP

WiFi User

Licensed uplink 

Licensed downlink

 Unlicensed downlink 

Uplink resource block

Downlink resource block

Fig. 1. A network of immersive VR application.

Equirectangular

Cube

Pyramid

Fig. 2. The formats of a VR image.

VR QoE for all users compared to Q-learning using LTE-U
network and Q-learning using LTE network.

The rest of this paper is organized as follows. The system
model and problem formulation are presented in Section II. The
ESN-based resource allocation algorithm is proposed in Section
III. In Section IV, numerical simulation results are presented and
analyzed. Finally, conclusions are drawn in Section V.

II. SYSTEM MODEL AND PROBLEM FORMULATION

Consider the downlink and uplink transmission of an LTE-U
network composed of a set B of B unmanned aerial vehicles
(UAVs) and W ground WiFi access points (WAPs). The UAVs
are deployed to act as ﬂying LTE-U base stations to serve a
set U of U wireless VR users. Here, we consider dual-mode
UAVs that are able to access both the licensed and unlicensed
bands. The licensed band is used for both downlink and uplink
transmission while the unlicensed band is only used for the down-
link transmission (due to the more stringent downlink trafﬁc).
The downlink is used to transmit the VR images displayed on
each user’s VR device while the uplink is used to transmit the
tracking information that is used to construct a user’s VR image
as shown in Fig. 1. Here, the tracking information includes a
user’s orientation and location information. In our model, the
UAVs adopt an orthogonal frequency division multiple access
(OFDMA) technique over the licensed band and transmit over
uplink resource blocks and downlink resource blocks. Our LTE-
U model uses a time division duplexing (TDD) mode with duty
cycle method [6]. Under this method, LTE-U and WiFi users
will, separately, use the time slots on the unlicensed band. In
particular, UAVs will use a fraction ϑ of time for VR transmission
and will be muted for 1 − ϑ time which is allocated to the WiFi
transmission links.

The coverage of each UAV is considered to be a circular area
with radius r and each UAV only allocates resource blocks to the
users located in its coverage range. Each user can only connect
to a single UAV. Note that, in our model, we consider static users
that move only within conﬁned areas, as is the case for typical
VR entertainment applications, such as immersive videos.

A. VR Image Transmission

The data size of each VR image depends on the quality and
the format of each VR image. The quality of a VR image can
be captured by the resolution of a VR image (720p, 1080p, etc.).
When the resolution of each VR image increases, the number of
pixels that each UAV needs to construct the VR image will also
increase. We use a set L = {l1, l2, . . . , lK} to represent the level
of VR image quality where li ∈ L indicates that the quality of a
VR image is at level i and lK represents the highest quality of a
VR image [10] (lK > li, i = 1, 2, . . . , K −1). In VR, the standard
format of a VR image is equirectangular [8]. However, to reduce
its data size, a VR image can be transformed from the standard
equirectangular format into either cubic or pyramidal formats as
shown in Fig. 2. Using the cube or pyramid formats can reduce,
respectively, 25% and 80% of data size compared to using the
equirectangular format [8]. However, reducing the data size of a
VR image will decrease the QoE of each user. This is due to the
fact that as the data size of a VR image decreases, the number of
the pixels used to construct a VR image decreases [8] and, hence,
the users may not be able to see the entire 360◦ VR image. We
use a set F = {f1, f2, f3} to represent the formats of a VR
image. Here, f1 to f3, respectively, represent the pyramid, cube,
and standard formats. Based on the above formulation, for user i,
the data size of a VR image is deﬁned as mi (l, f ), l ∈ L, f ∈ F .
Note that mi (lK, f3) is the maximum data size of a VR image
and m (l1, f1) is the minimum data size of a VR image. The QoE
of user i can be given by Qi (l, f ) = m(l,f )
m(lK ,f3) . Here, when the
quality of a VR image increases, the QoE of a user will increase,
which corresponds to the fact that watching a high deﬁnition video
is more comfortable than watching a standard deﬁnition video.

B. WiFi data rate analysis

For the considered WiFi system, a standard carrier sense multi-
ple access with collision avoidance (CSMA/CA) protocol will be
used along with its corresponding RTS/CTS access mechanism.
The WAPs will use a CSMA/CA scheme with binary slotted
exponential backoff. We assume that one LTE time slot consists
of TW WiFi time slots. Based on the duty cycle mechanism,
the UAVs can occupy a fraction ϑ of TW time slots over the
unlicensed band while the WiFi users can occupy a fraction
(1 − ϑ) of TW time slots. Thus, the per WiFi user rate will be:

Rw =

R (N ) (1 − ϑ)
N

,

(1)

where R (N ) is the saturation capacity of N WiFi users sharing
the same unlicensed band which is given by [6]. Given the rate
requirement of each WiFi user ξ, the fraction of the unlicensed
band time slot allocated to the LTE-U users can be given by
ϑ ≤ 1 − N ξ/R(N ).

C. Delay Model

In the studied system, VR images are transmitted from the
UAVs to the users while tracking information is transmitted from
the users to the UAVs. Hence, the transmission delay over the
uplink and downlink will directly affect the users’ QoE and the
UAVs must guarantee the transmission delay for each user. In
consequence, we must consider the transmission delay in this
model. Next, we ﬁrst
introduce a probabilistic UAV channel
model and, then, we formulate the transmission delay of each

VR user. In the considered UAV channel model, probabilistic line-
of-sight (LoS) and non-line-of-sight (NLoS) links are considered
as in [11]. NLoS links experience higher attenuations than LoS
links due to the shadowing and diffraction loss [11]. The LoS
and NLoS path loss of UAV k transmitting a VR image to user
i over the licensed resource blocks can be given by (in dB) [12]:

gLoS
ij = 20 log

4πdijf
c

(cid:18)

(cid:19)

+ ηL

LoS, gNLoS

ij = 20 log

(cid:18)

4πdij f
c

(cid:19)

+ ηL

NLoS,

where 20 log (dij f 4π/c) is the free space path loss with dij being
the distance between user i and UAV j, f being the carrier
frequency, and c being the speed of light. Here, ηL
NLoS
represent, respectively, additional attenuation factors due to the
LoS/NLoS connections over the licensed resource blocks. The
LoS probability is given by [12]:

LoS and ηL

where vij = [vij,1, . . . , vij,V ]
resource
blocks that UAV j allocates to user i with vij,k ∈ {1, 0}.
γu
is the SINR between user i and UAV
ij,k =

the vector of

PU hk
ij

is

σ2+ P

PU hk
il

l∈Uk ,l6=j

j over resource block k with U k being the set of users that use
uplink resource blocks k and PU is each user’s transmit power.
Let A be the data size of the tracking information. For user i,
the total delay that consists of downlink and uplink transmission
delays that can be given by:

Dij (fi, li, sij, vij , oij ) =

m (li, fi)
cij (sij , eij)

+

A
cij (vij)

,

(7)

where the ﬁrst term is the time that UAV j needs to transmit a
VR image to user i and the second term is the time that user i
needs to transmit the tracking information to UAV j.

Pr

gLoS
ij

= (1 + X exp (−Y [φij − X]))−1,

(2)

D. Problem Formulation

(cid:0)

where X and Y are constants that depend on the environment and
φij = sin−1 (hj/dij) is the elevation angle. Then, the average
path loss from UAV j to user i will be [12]:

(cid:1)

¯gij = Pr

gLoS
ij

× gLoS

ij + Pr

gNLoS
ij

× gNLoS
ij

,

(3)

gNLoS
ij

where Pr
associated with UAV j on the licensed resource blocks is:
(cid:0)

. The downlink rate of user i

(cid:0)
= 1 − Pr

gLoS
ij

(cid:1)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

Q

cL
ij

qij

=

qij,kBlog2 (1 + γij,k),

(4)

(cid:0)

(cid:1)

k=1
X

where qij = [qij,1, . . . , qij,Q] is the vector of resource blocks
that UAV j allocates to user i with qij,k ∈ {0, 1}. Here,
qij,k = 1 indicates that resource block k is allocated to user i.
is the downlink signal-to-interference-
γij,k =

PB ¯gk
ij

σ2+ P

PB ¯gk
il

l∈Rk ,l6=j

plus-noise ratio (SINR) between UAV j and user i over licensed
resource block k. Rk represents the set of UAVs that use downlink
resource block k, B is the bandwidth of each resource block, PB
is the transmit power of UAV j which is assumed to be equal for
all UAVs, and σ2 is the variance of the Gaussian noise. For UAV
k transmitting a VR image to user i using the unlicensed band,
the LoS and NLoS path loss will be (in dB) [12]:

4πdij f
c

(cid:18)

(cid:19)

+ ηU

NLoS,

gLoS
ij = 20 log

+ ηU

LoS, gNLoS

ij = 20 log

4πdijf
c
(cid:18)
LoS and ηU

(cid:19)

where ηU
NLoS represent additional attenuation factors
due to, respectively, the LoS and NLoS connections over the
unlicensed band. The LoS probability and the average path loss
can be, respectively, calculated using a method that is similar to
(2) and (3). Therefore, the downlink rate of user i associated with
UAV j over the unlicensed band is:

cU
ij (eij) = eijϑBulog2

1 + γU

ij,k

,

(5)

where Bu is the bandwidth of the unlicensed band, eij is the
fraction of ϑ over the unlicensed band with
i eij = 1, and
γU
ij,k is the SINR between UAV j and user i over the unlicensed
P
band. The total downlink rate of user i associated with UAV j
= cL
ij (eij). The uplink rate of user i
is cij
ij
associated with UAV j can be given by:

qij, eij

+ cU

qij

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)
V

cij (vij) =

vij,kBlog2

1 + γu

ij,k

,

(6)

k=1
X

(cid:0)

(cid:1)

j∈B

this problem must

Given the deﬁned system model, our goal is to develop an
effective resource allocation scheme to allocate resource blocks,
image quality level, and image format to maximize the QoE
of all users while meeting the delay requirement of the VR
system. However,
jointly account for the
coupled problems of user association and resource allocation.
Moreover, the delay of each user depends on the decisions of
all UAVs, due to interference and user association. Therefore,
we formulate this problem as a noncooperative game [13] G =
. In this game, the players are the UAVs
B, {Aj }j∈B , {¯uj}
in set B, Aj represents the action set of each UAV j, and ¯uj
h
is the utility function of each UAV j. Here, each action aj =
qj, vj, ej, lj, f j
, consists of: (i) downlink licensed resource
allocation vector qj =
with Uj being the
(cid:2)
number of the users associated with UAV j, (ii) uplink resource
h
i
allocation vector vj =
v1j, v2j, . . . , vUj j
, (iii) downlink un-
licensed resource allocation vector ej =
,
e1j, e2j, . . . , eUj j
(iv) image quality allocation vector lj =
, and
l1j, . . . , lUj j
(cid:3)
(v) image format allocation vector f j =
. Here,
(cid:3)
lij ∈ L, fij ∈ F , respectively, represent the image quality and
format of user i. eij ∈ M where M =
is a ﬁnite
set of M -level fractions of UAV j’s total time that UAV j can
be used to transmit VR images. qj, vj and ej must satisfy:

(cid:2)
f1j, . . . , fUj j
(cid:2)
(cid:2)
ϑ, ϑ
2 , ..., ϑ

q1j, q2j, . . . , qUj j

(cid:9)

(cid:8)

M

i

(cid:3)

(cid:3)

(cid:2)

(cid:3)

qij = [1, 1, . . . , 1] ,

vij = [1, 1, . . . , 1] ,

(8)

i∈Uj
X

i∈Uj
X

eij ≤ 1, eij ∈ M,

(9)

i∈Uj
X

where (8) indicates that each UAV will allocate all of its resource
blocks to the associated users. We assume that each UAV j
chooses only one action at each time slot t. The utility function
of each UAV j can be given by:
Qi(lij, fij)1

{Dij(fij ,lij ,qij ,vij ,eij)≤γD}, (10)

uj (aj, a−j) =

i∈Uj
X

where γD represents the maximal tolerable delay for each VR user
(maximum supported by the VR system being used), aj ∈ Aj
represents an action of UAV j and a−j is the actions of all UAVs
other than UAV j. In this game, the QoE and delay of all users

are analyzed over a period T and, hence, the utility function is:

¯uj (aj, a−j) =

1
T

T

t=1
X

T

uj (aj, a−j) .

(11)

t=1
P

Let πj,aij = 1
1{aj,t=aij }= Pr (aj,t = aij ) be the probability
T
of UAV j using action aij. Here, aj,t is the action that UAV j
uses at time t and aj,t = aij indicates that UAV j adopts action
aij at time t. πj =
is the action selection
πj,a1j , . . . , πj,aAj j
strategy of UAV j with Aj being the number of actions of UAV
h
j. Based on the deﬁnition of the strategy, the utility function in
(11) can be rewritten as follows:

i

¯uj (aj , a−j) =

uj (aj , a−j)

πk,ak

,

!

k∈B
Y

a∈A  
X

(12)

where a ∈ A with A being the action set of all UAVs. (12)
actually captures the average utility value of each UAV j over a
period T .

Hence, for this model, our objective is to solve the proposed
resource allocation game. One suitable solution is the mixed-
strategy Nash equilibrium (NE), formally deﬁned as follows [13]:
A mixed strategy proﬁle π∗ = (π∗
is a
mixed-strategy Nash equilibrium if, ∀j ∈ B and πj, we have:

1, . . . , π∗

j , π∗
−j

B) =

π∗

(cid:0)

(cid:1)

¯uj

π∗

j , π∗
−j

≥ ¯uj

πj, π∗
−j

,

(13)

(cid:0)
where ¯uj (πj, π−j) =

(cid:1)

uj (aj, a−j)

(cid:0)

(cid:1)
πk,ak is the expected

a∈A
P

k∈B
Q

utility of UAV j when it selects the mixed strategy πj. For our
game, the mixed-strategy NE for the UAVs represents a solution
of the game at which each UAV j can maximize the QoE for
its associated users while meeting the delay requirement of VR
system, given the actions of its opponents.

III. ECHO STATE NETWORKS FOR SELF-ORGANIZING
RESOURCE ALLOCATION

Next, we introduce a reinforcement learning (RL) algorithm
based on the neural network tools from echo state networks
with leaky integrator neurons [9] that can be used to solve
this game and ﬁnd a mixed-strategy NE [13]. Traditional RL
algorithms such as Q-learning [14] typically use a Q-table to
record the utility values resulting from different actions and states.
However, in our model, each action consists of ﬁve components
which, if used in Q-learning, can result in an exponential increase
in the number of actions and its corresponding utility values.
Moreover, the update of Q-learning needs to traverse the Q-table
to ﬁnd the update position which will signiﬁcantly decrease the
update speed. Therefore, using a limited-size matrix to record
all of the needed utility values as done in Q-learning may not
be practical. In contrast, the proposed RL algorithm will use
echo state networks as an approximation method to record the
relationship between the actions, states and utility values and,
hence, it can record all of the needed actions, states and utility
values. Compared to the ESN algorithms in [5] and [6] that are
used for discrete-time systems, the proposed ESN with leaky
integrator neurons can dynamically adjust the update speed of
the ESN’s state and, hence, it can enable the UAVs to learn the
continuous dynamics of their associated VR users.

TABLE I
ESN-BASED LEARNING ALGORITHM FOR RESOURCE ALLOCATION

Inputs: Mixed strategies xj,t
j , W j , W out

Initialize: W in

j , and yj = 0. Set πj,t uniformly

for each time t do.
(a) Estimate the value of the utility values ˆuj,t based on (16).
if rand(.) < ε

(b) Select action randomly.

else

(c) Choose action aj = arg max
aj ∈Aj

ˆuj,t(aj ).

end if
(d) Broadcast the index of the mixed strategy to other UAVs.
(e) Receive the index of the mixed strategy as input xj,t.
(f) Perform action to calculate the actual value of utility ˆuj,t.
(g) Set the mixed strategy πj,t based on (14)
(h) Update the dynamic reservoir state based on (15).
(i) Update the output weight matrix based on (17).

end for

Output: Prediction yj,t

A. ESN Components

For each UAV, the proposed ESN algorithm consists of four
components: (1) inputs, (2) ESN model, (3) actions, and (4)
output, which are speciﬁed as follows:

• Actions: Each action of UAV j is the vector aj =
qj, vj, ej, lj, f j
that jointly considers the downlink licensed
resource allocation, uplink licensed resource allocation, downlink
(cid:2)
image quality allocation, and
unlicensed resource allocation,
image format allocation.

(cid:3)

• Input: The input of our ESN is deﬁned as a vector xj,t =
[x1, · · · , xB]T where xj represents the index of the strategy of
UAV j at time t. To guarantee that any action always has a non-
zero probability to be chosen, the ε-greedy exploration in [6]
is used. This mechanism is also used to harmonize the tradeoff
between exploitation and exploration. Therefore, the probability
of UAV j taking a certain action aj can be given by:
1 − ε + ε
ˆuj,t(aj) ,

|Aj | , arg max
aj ∈Aj

(14)

Pr (aj) =

(
where ˆuj,t (aj) =

ε
|Aj | , otherwise,

uj (aj, a−j)π−j,a−j represents the ex-

a−j ∈A−j
P

P

pected utility of a UAV j with respect to the actions of other
π (aj, a−j) represents the marginal
UAVs, π−j,a−j =
probability distribution over the action set of UAV j, and A−j =

aj ∈Aj

(cid:2)

ˆuj (aj1) , . . . , ˆuj

yj1,t, yj2,t, . . . , yj|Aj |,t

k6=j,k∈B Ak denotes the set of actions other than UAV j.
• Output: The output of the ESN model at time t is a vector
Q
of utility values yj,t =
where yji,t is
the estimated value of utility ˆuj,t (aji) due to action i of UAV j.
As time elapses, the ESN output, yj, will ﬁnally converge to the
utility ˆuj =

aj|Aj |
• ESN Model: An ESN model is a learning architecture that
(cid:0)
can ﬁnd the relationship between the input xj,t and output
yj,t, thus building the relationship between the UAVs’ actions,
strategies, and utility values. Mathematically, the ESN model
j ∈ RAj ×(Nw+B+1), the
consists of the output weight matrix W out
j ∈ RNw×B+1, and the recurrent matrix
input weight matrix W in
W j ∈ RNw×Nw with Nw being the number of the units.

(cid:1)(cid:3)

(cid:2)

(cid:3)

.

B. ESN-Based Learning Algorithm for Resource Allocation

Next, we introduce the proposed ESN-based learning algorithm
to ﬁnd a mixed strategy NE. The ESN model can store historical
ESN information such as input that can be used to ﬁnd a fast

converging process from the initial state to the mixed-strategy
NE. The historical ESN information is stored in the reservoir
state which can be given by:

µj,t = (1 − δCz) µj,t−1 + δCf

W jµj,t−1 + W in

j xj,t

,

(15)

(cid:1)

(cid:0)

where C is a time constant, z is the leaking decay rate, δ
is the step size, and f (x) = ex−e−x
ex+e−x is the tanh function. In
fact, (15) captures the difference between conventional ESN such
as in [5] and [6] and the proposed ESN with leaky integrator
neurons. When δ decreases, the ESN can be used for learning
the continuous dynamics of VR users such as data rate and data
size of a VR image. (15) shows that the reservoir state consists
of the ESN input at time t and the past dynamic reservoir state
that includes the historical reservoir states and ESN inputs. Thus,
the reservoir state can store the mixed strategy from time 0 to
time t. Using the reservoir state with the output weight matrix,
the proposed ESN algorithm can estimate the utility value ˆuj.
The estimation of ˆuj is:

yj,t = W out
j,t

µj,t; xj,t
where W out
j,t is the output weight matrix at time slot t. To enable
(cid:2)
the ESN to predict the utility value ˆuj using reservoir state µj,t
and input xj,t, we must train the output matrix W out
using a
j
linear gradient descent approach, which can be given by:

(16)

(cid:3)

,

TABLE II
SYSTEM PARAMETERS

NLoS

Parameter
V, S
LoS, ηU
ηU
DIFS
E [S]
ACK
SIFS
ξ
LoS, ηL
ηL

NLoS

Value
5, 5
1.2, 23
50 µs
1500 byte
304 µs
16 µs
4 Mbps
1, 20

Parameter
M
z
ς
X
δ
K
Y
N

Value
5
0.04
20 dB
11.9
1
3
0.13
8

ESN-based learning algorithm
Traditional ESN algorithm
Q-learning algorithm in LTE-U
Q-learning algorithm in LTE

Parameter
PB
PU
σ
RTS
Fu
C
CTS
FC

Value
15 dBm
20 dBm
-94 dBm
352 µs
20 Mbit
0.44
304 µs
2 Gbit

)
s
m

(

r
e
s
u

h
c
a
e

r
o
f

l

y
a
e
d
e
g
a
r
e
v
A

26

25

24

23

22

21

20

19

18

1

2

4
Number of UAVs
Fig. 3. Average delay of each user vs. number of UAVs.

3

5

6

ji,t + λ (ˆuji,t − yji,t) µT
j,t,

(17)

W out

ji,t+1 = W out
ji,t is row i of W out

where W out
j,t, λ is the learning rate, and ˆuji,t
is the actual utility value. Here, ˆuji,t is estimated by the utility
value resulting from the actions performed by each UAV during
each time slot t. Based on the above formulation, the distributed
ESN-based learning algorithm performed by every UAV j is
summarized in Table I.

In this algorithm, each UAV can approximate the relationship
between the actions, strategies, and utility values. During each
iteration, the ESN-based algorithm can record the the strategies
that each UAV uses and the corresponding utility values ˆuj. By
scaling the input weight matrix W in
j and input xj, the ESN-based
algorithm can satisfy the convergence conditions of [6, Theorem
2]. As time elapses, each UAV j’s output resulting from action i
will converge to a ﬁnal value ˆuji. At this convergence point, the
output results ˆuj indicates that the proposed algorithm converges
to a mixed strategy-NE. This is due to the fact when ˆuj (aj) is
maximum, Pr (aj) will be maximum (Pr (aj) = 1 − ε + ε
|Aj | ).
The strategy depends on ˆuj (aj). When the proposed learning
algorithm ﬁnds the optimal ˆuj (aj), the strategy will be optimal
and, hence, ¯uj (aj, a−j) will be maximized.

IV. SIMULATION RESULTS

In our simulations, a circular network area having a radius
r = 500 m is considered with U = 20 uniformly distributed
users and B = 5 uniformly distributed UAVs. The HTC VR
device is considered in our simulations [15] and, hence, the rate
requirement for VR transmission is set to 51.2 Mbits/s. The
requirement of total delay that consists of the downlink and uplink
transmission delay is 20 ms [16]. Other simulation parameters
are listed in Table II. We compare our approach with: a) ESN
algorithm in [5], b) Q-learning algorithm within an LTE-U, and
c) Q-learning algorithm applied to an LTE system. All statistical
results are averaged over 5000 independent runs.

ESN-based learning algorithm
Traditional ESN algorithm
Q-learning algorithm in LTE-U
Q-learning algorithm in LTE

8

7

6

5

4

3

s
r
e
s
u

l
l

a

r
o
f

E
o
Q
R
V

l

a
t
o
T

2

1

2

3

4
Number of SBSs

5

6

Fig. 4. Total VR QoE of all users vs. number of UAVs.

Fig. 3 shows how the average delay per user changes as the
number of UAVs varies. From Fig. 3, we can see that as the
number of UAVs increases, the average delay of all considered
algorithms starts by ﬁrst rapidly decreasing and then the decrease
becomes slower. This is due to the fact that, as the number of
UAVs increases, each user will have more choices of UAVs to
connect to, and, hence, the distance between the UAVs and the
users decreases. However, as the number of UAVs keeps increas-
ing, the interference from the UAVs to the users increases. Fig.
3 also shows that when the delay reaches the delay requirement,
the delay resulting from the proposed algorithm remains constant
as the number of UAVs increases. This is due to the fact that our
purpose is to maximize the QoE utility value while meeting the
delay requirement of the VR system and, hence, once the delay
requirement is met, the proposed scheme will no longer need to
further reduce it.

In Fig. 4, we show how the total VR QoE for all users changes
as the number of UAVs varies. From Fig. 4, we can see that, as
the number of UAVs increases, the total QoE utility values of
all considered algorithms will increase. This is due to the fact
that, as the number of UAVs increases, the number of the users

 
 
 
 
 
 
 
 
 
 
Proposed ESN algorithm
Traditional ESN algorithm
Q-learning algorithm in LTE-U
Q-learning algorithm in LTE
Convergent point

V
A
U
h
c
a
e

f
o
e
u
a
v

l

y
t
i
l
i
t
u

l

a
t
o
T

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

5

10

15

20

25

Number of iterations

Fig. 5. Convergence of the proposed algorithm and Q-learning. Here, we use the
ESN algorithm in [6] as a benchmark.

)

%

(

s
t
n
e
m
e
r
i
u
q
e
r

l

y
a
e
d

r
i
e
h
t

g
n
i
t
e
e
m
s
r
e
s
u

f
o

e
g
a
t
n
e
c
r
e
P

ESN-based learning algorithm
Traditional ESN algorithm
Q-learning algorithm in LTE-U
Q-learning algorithm in LTE

80

70

60

50

40

30

20

10

1

2

3

4
Number of UAVs

5

6

Fig. 6. Percentage of users meeting their delay requirements vs. number of UAVs.

located in each UAV’s coverage decreases and, hence, each UAV
can allocate more resource to its associated users. Fig. 4 also
shows that, for the case with 6 UAVs, the proposed algorithm
can achieve, respectively, up to 14% and 27.1% gains in terms of
total VR QoE for all users. These gains stem from the fact that
the proposed algorithm can use the leaky integrator neurons to
adjust the learning speed of ESNs. Hence, it enables the UAVs
to learn the utility values resulting from the dynamical users and
use the already learned utility value to estimate the new utility
value that must be learned.

Fig. 5 shows the number of iterations needed till convergence
for all considered algorithms. In this ﬁgure, we can see that, as
time elapses, the QoE utilities for both considered algorithms will
increase until convergence to their ﬁnal values. Fig. 5 also shows
that the proposed algorithm yields up to 11.3% and 17.4% gains in
terms of the number of the iterations needed to reach convergence
compared to traditional ESN algorithm and Q-learning. This
implies that the proposed algorithm can use the ESN with the
leaky integrator neurons to adjust the learning speed and, hence
it can accurately approximate the relationship between the actions,
strategies, and utility values.

Fig. 6 shows how the percentage of users meeting their delay
requirements changes as the number of UAVs varies. From Fig.
6, we can see that the percentage of VR users meeting their delay
requirements increases, as the number of UAVs increases. This
stems from the fact that as the number of UAVs increases, the
users have more UAV choices and the distances from the UAVs to
the users decrease and, hence, the data rate of each user increases.
Fig. 6 also shows that the proposed algorithm can achieve up to

40.1% gain in terms of the number of the users that meet the
delay requirement. This is due to the fact the proposed algorithm
use the ESN to record all the needed utility values to estimate
the future utility values and allocate the unlicensed band to the
users so as to increase the data rate of each user.

V. CONCLUSION

In this paper, we have developed a novel framework that uses
ﬂying UAV-enabled networks to provide service for VR users in
an LTE-U system. We have proposed a novel resource allocation
framework for optimizing the users’ QoE while meeting the delay
requirement for each user. We have formulated this problem as a
noncooperative game between the UAVs and we have developed a
novel algorithm based on the machine learning tools of echo state
networks with leaky integrator neurons. The proposed algorithm
can dynamically adjust the update speed of the ESN’s state and,
hence, it can enable the UAVs to learn the continuous dynamics
of their associated VR users. Simulation results have shown that
the proposed approach yields signiﬁcant gains in terms of total
QoE utilities for all users compared to conventional approaches.

REFERENCES

[1] E. Ba¸stu˘g, M. Bennis, M. Médard, and M. Debbah, “Towards interconnected
virtual reality: Opportunities, challenges and enablers,” IEEE Communica-
tions Magazine, vol. 55, no. 6, pp. 110–117, June 2017.

[2] AT&T, “Enabling mobile augmented and virtual reality with 5G networks,”

http://about.att.com/innovationblog/foundry_ar_vr.

[3] J. Ahn, Y. Yong Kim, and R. Y. Kim, “Delay oriented VR mode WLAN
for efﬁcient wireless multi-user virtual reality device,” in Proc. of IEEE
International Conference on Consumer Electronics, Las Vegas, NV, USA,
March 2017.

[4] C. Wu, Z. Wang, and S. Yang, “Drone streaming with Wi-Fi grid aggregation
for virtual tour,” available online: arxiv.org/abs/1605.09486, May 2016.
[5] M. Chen, W. Saad, and C. Yin, “Virtual reality over wireless networks:
Quality-of-service model and learning-based resource management,” avail-
able online: arxiv.org/abs/1703.04209, Mar. 2017.

[6] M. Chen, W. Saad, and C. Yin, “Echo state networks for self-organizing
IEEE

resource allocation in LTE-U with uplink-downlink decoupling,”
Transactions on Wireless Communications, vol. 16, no. 1, January 2017.
[7] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Unmanned aerial
vehicle with underlaid device-to-device communications: Performance and
tradeoffs,” IEEE Transactions on Wireless Communications, vol. 15, no. 6,
pp. 3949–3963, June 2016.

[8] D. Pio, “Facebook engineering,” https://www.facebook.com/Engineering/

videos/10153781047207200/.

[9] H. Jaeger, “The echo state approach to analysing and training recurrent
neural networks-with an erratum note,” German National Research Center
for Information Technology GMD Technical Report, vol. 148, no. 34, pp.
13, 2001.

[10] C. Charles, B. Mark, L. Ajay, M. Sean, M. Paul, and W. Kevin, “Virtual and
augmented reality-How do they affect the current service delivery and home
and network architectures?,” https://www.arris.com/globalassets/resources/
white-papers/exec-paper_virtual-and-augmented-reality_ﬁnal.pdf.

[11] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong,
“Caching in the sky: Proactive deployment of cache-enabled unmanned
IEEE Journal on
aerial vehicles for optimized quality-of-experience,”
Selected Areas in Communications, vol. 35, no. 5, pp. 1046–1061, May
2017.

[12] A. Al-Hourani, S. Kandeepan, and A. Jamalipour, “Modeling air-to-ground
in Proc.
path loss for low altitude platforms in urban environments,”
of IEEE Global Communications Conference (GLOBECOM), Austin, TX,
USA, December 2014.

[13] Z. Han, D. Niyato, W. Saad, T. Basar, and A. HjÃ¸rungnes, Game Theory in
Wireless and Communication Networks: Theory, Models, and Applications,
Cambridge University Press, 2012.

[14] M. Bennis and D. Niyato, “A Q-learning based approach to interference
avoidance in self-organized femtocell networks,” in Proc. of IEEE Global
Commun. Conference (GLOBECOM) Workshop on Femtocell Networks,
Miami, FL, USA, Dec. 2010.

[15] HTC, “HTC Vive,” https://www.vive.com/us/.
[16] M. Abrash,

“What VR could, should, and almost certainly will be
within two years,” https://media.steampowered.com/apps/abrashblog/Abrash
%20Dev%20Days%202014.pdf.

 
 
 
 
 
 
 
 
 
 
 
 
