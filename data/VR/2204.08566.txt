2
2
0
2

r
p
A
8
1

]

C
H
.
s
c
[

1
v
6
6
5
8
0
.
4
0
2
2
:
v
i
X
r
a

Beyond Being Real: A Sensorimotor Control Perspective on
Interactions in Virtual Reality

Parastoo Abtahi
Stanford University
Stanford, USA
parastoo@stanford.edu

James A. Landay
Stanford University
Stanford, USA
landay@stanford.edu

Sidney Q. Hough
Stanford University
Stanford, USA
shough@stanford.edu

Sean Follmer
Stanford University
Stanford, USA
sfollmer@stanford.edu

ABSTRACT
We can create Virtual Reality (VR) interactions that have no equiv-
alent in the real world by remapping spacetime or altering users’
body representation, such as stretching the user’s virtual arm for
manipulation of distant objects or scaling up the user’s avatar to
enable rapid locomotion. Prior research has leveraged such ap-
proaches, what we call beyond-real techniques, to make interac-
tions in VR more practical, efficient, ergonomic, and accessible. We
present a survey categorizing prior movement-based VR interac-
tion literature as reality-based, illusory, or beyond-real interactions.
We survey relevant conferences (CHI, IEEE VR, VRST, UIST, and
DIS) while focusing on selection, manipulation, locomotion, and
navigation in VR. For beyond-real interactions, we describe the
transformations that have been used by prior works to create novel
remappings. We discuss open research questions through the lens
of the human sensorimotor control system and highlight challenges
that need to be addressed for effective utilization of beyond-real in-
teractions in future VR applications, including plausibility, control,
long-term adaptation, and individual differences.

CCS CONCEPTS
• Human-centered computing → Virtual reality; Interaction
design theory, concepts and paradigms.

KEYWORDS
virtual reality, interaction design, framework, sensorimotor control

ACM Reference Format:
Parastoo Abtahi, Sidney Q. Hough, James A. Landay, and Sean Follmer. 2022.
Beyond Being Real: A Sensorimotor Control Perspective on Interactions in
Virtual Reality. In CHI Conference on Human Factors in Computing Systems
(CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY,
USA, 17 pages. https://doi.org/10.1145/3491102.3517706

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9157-3/22/04. . . $15.00
https://doi.org/10.1145/3491102.3517706

Figure 1: Movement-based VR interactions showcased on a
continuum from high to low degree of verity (meaning true
to life): reality-based, illusory, and beyond-real interactions,
with sensory mismatch created through body-centered (ego-
centric) or world-centered (allocentric) warping.

1 INTRODUCTION
The idea of leveraging VR beyond the replication of reality dates
back to the early days of this technology. In a 1965 article, “The
Ultimate Display,” Ivan Sutherland proposed that “there is no rea-
son why the objects displayed by a computer have to follow the
ordinary rules of physical reality” and that “such a display could
literally be the Wonderland into which Alice walked” [171]. Over
the years, other researchers have shared a similar perspective about
VR interaction design and have highlighted potential benefits of
designing VR interactions beyond reality, including improving hu-
man performance [109] and making interactions more efficient,
ergonomic, and accessible [72]. For example, the Go-Go interaction
is an arm-extension technique that stretches the user’s arm during
reach, enabling them to grasp and manipulate distant objects [122].
These interactions are designed not to overcome the limitations of
VR technology, but to overcome the limitations of our reality.

In “Beyond Being There” (1992), Hollan and Stornetta made a
parallel argument in response to telecommunication and computer
supported collaborative work research and development at the time.

 
 
 
 
 
 
CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

They argued that when comparing telecommunication to face-to-
face communication “the imitation will never be as good as the
real thing. This is true by definition if one is strict in using the old
medium as the standard of measurement. . . requiring one medium to
imitate the other inevitably pits strengths of the old medium against
weaknesses of the new” [69]. They presented a framework around
needs, media, and mechanisms, “to ask the question: what’s wrong
with (physically proximate) reality?” and explore new mechanisms
that leverage the strengths of the new medium to meet our needs.
Towards similar goals, we believe there is a need to more sys-
tematically investigate what we call beyond-real VR interactions:
movement-based interactions that are not possible in the real world.
By “real world” we do not imply that there exists an objective physi-
cal world independent of the user’s subjective mental world. Instead
we are referring to what others have called the “actual world”: what
can be experienced by the human sensory system without the use
of technology [87]. VR enables full-body interactions with digital
content where users can move and act in the virtual world. These
interactions do not have to resemble users’ movements in the real
world, as VR presents an opportunity to construct imaginative inter-
actions by remapping users’ movements and altering the resulting
sensory feedback. Due to the plasticity of the human sensorimotor
system, users have the ability to learn and perform motor tasks un-
der new remappings [117]. As HCI practitioners, we are interested
in exploring VR interactions that are usable and lead to such motor
skill acquisition given novel dynamics. Thus, we propose describing
VR interactions through the lens of the sensorimotor system, as
transformations applied to tracking and sensing inputs from the
real world. We believe such perspective highlights considerations
around action and perception that are key for understanding the
potential, as well as challenges, of beyond-real interactions.

In this paper, we present a framework based on sensorimotor
control for categorizing virtual reality interactions as reality-based,
illusory, or beyond-real, as shown in Figure 1. We further utilize
this framing for describing beyond-real interactions as a set of
transformations applied to real-world input. We apply the frame-
work to a survey of VR interactions and systematically identify and
categorize beyond-real interactions based on their underlying trans-
formations. This survey provides an overview of more than 30 years
of beyond-real interaction techniques and identifies key types of
transformations that have been explored. Finally, we use the lens of
sensorimotor control to map out open research questions central to
better understanding the effective use of beyond-real interactions.

In this work, we contribute:

• Beyond being real, a framework based on the human sensori-
motor control to describe movement-based VR interactions
as transformations applied to input from the real world.
• A literature survey to categorize existing VR interactions (at
CHI, IEEE VR, VRST, UIST, and DIS conferences) as reality-
based, illusory, or beyond-real, apply the framework to iso-
late beyond-real transformations in these selected works, and
describe transformation categories that have been explored
by prior research for creating beyond-real interactions.
• A discussion of challenges and open research questions that
require further investigation of beyond-real VR interaction
design through the lens of sensory integration.

2 BACKGROUND
In VR users can move in virtual spaces and perform full-body in-
teractions. We focus on these movement-based VR interactions
[56] and action execution in VR (p. 40) [114]. We approach inter-
action from a control and optimal behavior perspective [70], and
study interaction techniques, such as selection, manipulation, and
locomotion, that require motor performance [22].

In this section, we first present our categorization of VR inter-
actions as either reality-based, illusory, or beyond-real. We then
provide a high-level background on the human sensorimotor sys-
tem and control theory. Through this lens, we describe how VR
interactions can be thought of as transformations that directly map
or remap the user’s movements in the real world to renderings in
the virtual world. This insight is central to our work and we believe
to research that follows. Here, we use this framing to differentiate
between reality-based, illusory, and beyond-real interactions based
on whether the transformation applies a remapping and whether
that remapping is noticeable by users.

We begin by situating these three categories within the context
of prior research. Thurman and Mattoon [176] describe different
dimensions of VR, including what they call the verity, meaning true
to life, dimension. They then use verity to denote “a continuum of
simulation experiences that range from recreations of the physical
world as we know it to depictions of abstract ideas which have
no physical counterparts.” Along this continuum, VR interactions
range from interactions with high degree of verity that follow
natural laws of the real world to interactions with low degree of
verity that follow novel, original laws. Similarly, Slater and Usoh
discuss interactions on a spectrum from the mundane to the magical
[155] which map closely to the verity continuum.

Our three categories of movement-based VR interactions range
from high to low degree of verity: (1) reality-based interactions that
match the user’s real-world movements, (2) illusory interactions
that create remappings between the user’s movements and the
virtual renderings that remain unnoticed by users, and (3) beyond-
real interactions that create novel remappings between the user’s
movements and the renderings in the virtual world (see Figure 1).

2.1 Reality-Based Interactions
Highly realistic VR environments that seek to replicate our real-
world experiences are used for practical applications, such as train-
ing [68], exposure therapy for treating phobias [119, 131], and
post-traumatic stress disorders [70, 132]. These environments also
facilitate user interactions that closely resemble interactions in the
real world. Jacob et al. proposed the notion of Reality-Based Inter-
actions (RBI) to describe such interactions that employ themes of
reality and leverage users’ pre-existing knowledge of the everyday,
in VR and more broadly [72]. They highlight the benefits of RBI,
including accelerated learning, reduced mental effort, facilitated im-
provisation, and improved performance, particularly in situations
involving information overload, time pressure, or stress. They also
note that despite the advantages of RBI, designers may explicitly
give up realism to gain desired qualities by allowing users to per-
form many tasks within an application (expressive power) or across
different applications (versatility) and to do so rapidly (efficiency),
without fatigue or risk of physical injury (ergonomics), and using a

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

varied range of abilities (accessibility). In this work, we focus on
VR interactions in which designers explicitly give up realism by
creating novel remappings between user inputs and the rendered
outputs in VR to overcome the limitations of our experiences in
the real world. However, it should be noted that there are many
advantages associated with reality-based interactions, and extend-
ing interactions beyond reality is not always beneficial, nor is it
suitable for all VR applications.

2.2 Illusory Interactions
As Lanier highlights, our most important canvas in VR is the user’s
sensorimotor loop [84]. This technology offers a unique opportu-
nity for manipulating senses, as arbitrary mappings can be created
between the user’s movements and the rendering of their virtual
body. Movement-based VR illusions are remappings that result in
a subtle mismatch between the sensory feedback from the virtual
system and the sensory feedback from the real world; however,
the discrepancy is below the user’s perceptual thresholds and is
resolved such that the sensory feedback aligns with what the user
expects (i.e., the predictions of their internal model). For example,
slightly extending the length of the user’s arm (Figure 1b) or slightly
misplacing the user’s hand (Figure 1c) in VR are illusions that will
go unnoticed by users. Gonzalez-Franco and Lanier present a model
of illusions in VR that describes these processes in more detail [59].
Illusions have been explored by researchers to redirect the user’s
hand while tracing surfaces [1, 81, 212] or reaching in midair
[11, 30, 57] to provide an improved perceived haptic sensation and
overcome the current limitations of VR technology. In these visuo-
haptic illusions the mismatch between the visual and proprioceptive
feedback is resolved by visual dominance [64]. Another example
of movement-based VR illusions is redirected walking where the
rotational movement of the user’s head during turns is remapped
to a different rotational angle in VR such that their perceived walk-
ing path is altered. When studying VR illusions, researchers are
concerned with identifying users’ perceptual thresholds to ensure
that the illusion remains unnoticed [1, 163]. While these illusory
interactions are important for improving the perception of realistic
(high degree of verity) VR environments, prior research has shown
that our cognitive system can adjust to repeated exposure to con-
flicting stimuli [20]; thus, there are opportunities for exploration of
overt forms of such remapping techniques that go beyond reality.

2.3 Beyond-Real Interactions
For decades, scholars have emphasized the need for further explo-
ration of virtual experiences beyond replication of reality. In 2003,
Schneiderman highlighted that there are many opportunities for
enhancing 3D interfaces “if designers go beyond the goal of mim-
icking 3D reality” [144]. In 2005, Casati et al. argued that efforts
should be directed towards “creation of virtual perceptual objects
that have no equivalent in the hard reality” [27]. Gaggioli suggested,
in Human Computer Confluence, that “the possible uses of VR range
from the simulation of plausible possible worlds and possible selves
to the simulation of realities that break the laws of nature and even
of logic” and that VR can provide “a subjective window of presence
into unactualized but possible worlds” [54]. Bailenson, in Experi-
ence on Demand, proposed that the reality bending properties of

VR allow us to create experiences “unbound by the law of the real
world, to do impossible things in virtual settings” and that “VR is
perfect for things you couldn’t do in the real world” [12].

From an interaction design perspective, while beyond-real VR
interactions can offer benefits, such as making movement-based in-
put more efficient [109] and ergonomic [72], they create noticeable
incongruencies between the sensory feedback from the real world
and the virtual environment. This sensory mismatch has impor-
tant implications for designing usable beyond-real interactions that
people can learn, adapt to, and feel in control of. Therefore, in our
work, we carefully consider the human sensorimotor system and ap-
proach interaction from a control and optimal behavior perspective
[70]. Under this assumption, the human is a goal-directed control
system that receives feedback about the state of the world through
virtual renderings and behaves so as to change the control signal
towards a desired output. The human pursues this goal optimally
and adapts to the constraints of the virtual environment. In the next
section, we present a simplified model of the human sensorimotor
system and optimal control theory that we believe is key in the
discussion of beyond-real interactions. We use this theoretical lens
throughout the paper to describe beyond-real VR interactions as
transformations applied to real-world input. We conduct a survey
of beyond-real transformations that have been utilized by prior
research and highlight open research questions that remain in the
design and evaluation of usable beyond-real VR interactions.

2.4 Sensorimotor System and Control Theory
Human performance may be modelled at various levels of behav-
ior: skill-based, rule-based, and knowledge-based behaviors [124].
Optimal Feedback Control (OFC) theory focuses on skill-based be-
havior (e.g., catching a ball) and has been used to predict how the
human brain plans and controls movement [140] by studying the
link between high-level goals and real-time sensorimotor control
strategies [178]. This theory suggests that the Central Nervous Sys-
tem (CNS) acts as a feedback controller, continuously converting
sensory input into motor output [182] and it does so optimally,
based on a performance metric, such as obtaining minimal uncer-
tainty in the state estimate [183]. Researchers have also proposed
using a Mini-Max Feedback Control (MMFC) model, an extension
of the OFC model that minimizes energy consumption under the
assumption of worst-case uncertainty [182].

2.4.1 Overview. Figure 2 shows how the CNS interacts with the
body and the VR system during movement-based interactions. In
this diagram blocks represent key components, and arrows denote
the flow of control signals, clockwise from the top left. The opti-
mal controller outputs motor commands based on the discrepancy
between the desired and estimated states [200]. These motor com-
mands lead to movements in the real world that are then subject to
body dynamics and the effects of the environment, such as exter-
nal forces. The VR system includes sensing and tracking devices
that capture the user’s movements. Movement-based VR interac-
tions can be thought of as transformations applied to these signals
captured from the real world. In reality-based interactions the trans-
formations create a 1:1 mapping to the VR renderings; in illusory
interactions the transformations create subtle remappings that are

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

Figure 2: Flow of control signals in movement-based interactions through the central nervous system, body, and VR system.

unnoticed by users; and in beyond-real interactions the transfor-
mations create novel remappings. The human sensory apparatus
receives sensory feedback from both the real world and the virtual
system (shown in orange). The state estimator receives the sensory
feedback through the sensory apparatus as well as an efference
copy of the original motor signal [18]. While noise is present at all
stages [183], we only show discrete examples in this diagram.

2.4.2 Central nervous system. Figure 3 shows the subcomponents
of the CNS and the flow of control signals. The feedback controller
outputs motor commands based on the discrepancy between the
desired and estimated states, which is then combined with the
output of an adaptive inverse model [200]. An efferent copy of motor
signals is sent to a forward model that predicts the result of motor
commands [18]. The forward and inverse models are collectively
referred to as the internal model and capture information about the
context and the properties of the sensorimotor system.

Sensory integration. Multisensory integration is a complex
2.4.3
process that modifies the original signal based on low-level sen-
sory information, top-down influences of the internal model, and a
range of cognitive factors; therefore, it is perhaps more accurately
described as multisensory interaction [172]. This interaction is task-
dependent and may be affected by the modality of the stimulus as
well as the information content of the feedback [157]. Multisen-
sory processing is also influenced by attention [172] and human
emotional responses to stimuli [139]. Finally, the central nervous
system minimizes uncertainty by refining sensory signals based on
prior knowledge and memory [182].

Figure 3: Control signals in the central nervous system.

Redundancy in the sensorimotor system ensures robustness [46]
such that elimination of a feedback has minor effects on motor
behavior [40]; however, perturbations of the same signal may sig-
nificantly alter movement [35]. For example, while reaching without
sight results in minor errors, visual distortions have been shown
to lead to drastic compensatory movements [134, 137]. Therefore,
sensory integration predominantly addresses unexpected changes
based on prediction errors [172]. Note that OFC theory is concerned
with errors that are referred to as slips, and not mistakes that arise
from incorrect intentions (p. 414) [115].

2.4.4 Learning and adaptation. Prediction errors drive simulta-
neous perceptual and motor learning [46, 117]. While both the
forward and inverse models are adapted [200], it has been shown
that prediction learning precedes the learning of new control poli-
cies [46]. Beyond adaptation to perturbations, humans can learn
to synthesize movement under entirely novel dynamics [61]. An
example of sensorimotor learning is prism adaptation in which an
individual performs perceptual motor tasks while wearing goggles
that shift their visual field [128]. An interesting characteristic is that
the effects of adaptation after removing the goggles, known as af-
tereffects, are not global, and only result in a systematic movement
bias for the specific, practiced task [6].

We use this theoretical background to first present a descriptive
framework for beyond-real interactions and then discuss open re-
search questions and challenges that remain in the context of sen-
sory integration and adaptation.

3 THE BEYOND BEING REAL FRAMEWORK
We present beyond being real, a framework using a sensorimotor
control perspective for investigating movement-based VR interac-
tions that have no equivalent in the real world. The framework,
shown in Figure 4, provides a scaffolding for describing beyond-real
interactions in three stages, by describing the sensing and track-
ing data in the real world, the set of transformations applied to
the real-world input for creating novel remappings in VR, and the
VR renderings that provide signifiers and feedback to improve the
usability of beyond-real interactions. In this section, to highlight
this descriptive power, we return to the example of the Go-Go
interaction technique [122]. Note that while we provide a high-
level description of the Go-Go interaction here, the framework is
better suited to describing a specific implementation of the inter-
action technique that includes more details. For a more in-depth
walk-through example, please refer to Appendix B.

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Figure 4: The VR system receives input from the real world, applies beyond-real transformations, and renders the remapping.

3.1 Real-World Sensing and Tracking
The virtual system has limited information about the state of the
real world. For example, in most commercial VR systems the po-
sition of the headset is known; however, the system often does
not have direct knowledge of the user’s body pose. The real-world
information often describes the environment state (e.g., room di-
mensions, obstacles), users’ physical state (e.g., head position, hand
pose), and their cognitive state (e.g., attention, workload). In the
first stage of the framework, we identify what sensing and tracking
devices are available, what limitations they have (e.g., range, rate,
accuracy), and what real-world data from those devices is being
utilized by the virtual system. In the Go-Go interaction technique,
the inputs from the real world are the user’s physical arm length (𝑑)
and the user’s real hand position ( (cid:174)𝐻𝑟 ) in the user’s egocentric frame
of reference. In this case, the origin is the user’s chest position,
approximately determined based on the position of the VR headset.

3.2 Beyond-Real Transformations
The input passed on to the virtual system is then either disregarded,
mapped directly, or remapped. Remappings can be fixed throughout
the interaction or may dynamically change based on users’ actions
captured by the input data [85]. These mappings can be described
as transformations that take input from the real world and modify
the space, the user’s body representation, or time parameters.

In the second stage of the framework, we focus on the goal of the
beyond-real interaction (i.e., interaction task). To describe the sets
of transformations applied to the real-world data, we identify what
parameters are modified as a result of the remappings (space, body,
or time) and what the mapping type is (direct, fixed, or dynamic).
In the Go-Go interaction, the beyond-real transformation is a dy-
namic remapping that modifies the user’s body representation for
manipulation of distant objects. More specifically, the user’s virtual
hand ( (cid:174)𝐻𝑣) is extended during the last 1/3 of their reach range, for a
given coefficient k (0 < 𝑘 < 1):

(cid:26)

(cid:174)𝐻𝑣 =

(cid:174)𝐻𝑟
(cid:174)𝐻𝑟 + 𝑘 (∥ (cid:174)𝐻𝑟 ∥ − 2
3

𝑑)2

if ∥ (cid:174)𝐻𝑟 ∥ < 2
3
otherwise

𝑑

While our framework focuses on VR interactions, such transfor-
mations are not unique to VR and may be used to describe remap-
pings between movement-based user inputs and outputs of com-
puting systems more broadly. For example, by modifying the con-
trol/display ratio, the movements of the mouse can be remapped to
the movements of the cursor. However, for beyond-real VR interac-
tions specifying these transformations can be especially useful for

determining the sensory mismatches that users experience as a re-
sult of the remapping, which we discuss in more detail in section 5.

3.3 VR Renderings and Remapping Signifiers
The virtual system renders information through output devices,
such as the VR headset and headphones. While some renderings
are mapped to the input (i.e., they directly result from user ac-
tions), others are independent of the real-world input and the ap-
plied transformations. Independent renderings may communicate
to users what mappings exist, prior to the execution of actions.
These signifiers may be egocentric (e.g., visible features of the body
representation) or exocentric (e.g., features in the environment or
specialized objects). The concept of “User Representation” defined
by Seinfeld et al. [142] is closely related to egocentric signifiers.
More specifically, User Representations are virtual elements that
extend users’ physical bodies and they “may have signifiers that
communicate the actions they support.”

In the third stage of the framework, we identify the aspects of
the renderings that are independent of the real world and commu-
nicate the remapping to users (invisible, egocentric, or exocentric
signifiers). Remapping signifiers have important implications for
learnability and adaptation to novel remappings (see discussion in
section 5). In the Go-Go interaction, there are no visible signifiers
that communicate the remapping to users independent of the user’s
actions. Therefore, users can only discover the remapping after
they extend their arm more than 2/3 of their arm length.

In the following section, we present a survey of beyond-real
interactions previously presented at HCI conferences. We apply this
framework to those interactions to isolate and group the beyond-
real transformations that have been explored by prior works.

4 SURVEY OF BEYOND-REAL INTERACTIONS
We conducted a systematic review of literature, following PRISMA
guidelines [102], to (1) understand past research trends with respect
to reality-based, illusory, and beyond-real movement-based VR
interactions, (2) evaluate to what extent VR interaction research
has explored beyond-real transformations, and (3) explore whether
or not researchers have considered the human sensorimotor loop
in their exploration of beyond-real interactions.

In this survey, we focused on action execution and more specifi-
cally, on motor performance (p. 40) [114]. 3D interaction techniques
have been categorized into selection, manipulation, wayfinding,
locomotion, system control, and symbolic input [22]. We were par-
ticularly interested in selection, manipulation, and locomotion, as
they require users to act on the world. We chose to exclude symbolic

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

input and system control through which users change the mode
or state of the system, as they do not have a counterpart in our
physical reality and fall outside the scope of our work.

Navigation is conceptualized as having two components:
wayfinding refers to the cognitive component of navigation and
locomotion describes the movement from one place to another [107].
Due to its cognitive nature, wayfinding cannot be fully captured
through the lens of sensorimotor control. While we use the term
“navigation” as part of our search query to capture all locomotion
papers, we do not focus on works that only address wayfinding.

4.1 Method
4.1.1 Phase 1: Identification. We searched the ACM Digital Library
for full papers targeting the following venues: the ACM Confer-
ence on Human Factors in Computing Systems (CHI), the ACM
Symposium on Virtual Reality Software and Technology (VRST),
the ACM Symposium on User Interface Software and Technology
(UIST), and the ACM Conference on Designing Interactive Systems
(DIS). Additionally, we searched IEEE Xplore targeting the IEEE
Conference on Virtual Reality and 3D User Interfaces (IEEE VR). We
focused our search on VR interaction techniques, allowing terms
for common interaction techniques focused on action (selection,
manipulation, locomotion, navigation) to appear in either the title
or abstract. As we sought to understand trends of research attention
to reality-based, illusory, and beyond-real interactions over time,
papers placed in our identification phase date back to 1988. Note
that we did not use keywords in our search query. An example of
the way our queries were structured:

Title:((interaction* OR select* OR manipulat* OR
locomot* OR navigat*) AND (virtual OR VR)) OR
Abstract:(((interaction* OR select* OR manipulat* OR
locomot* OR navigat*) AND (virtual OR VR))

This phase found a total of 1268 full papers for further screening.

4.1.2 Phase 2: Screening. We excluded papers that were not focused
on virtual reality (326). This excluded augmented reality and other
non-immersive platforms such as tabletop displays. Furthermore,
we excluded papers that were not focused on interaction techniques
(271). We defined interaction techniques as means by which the user
engages with the virtual content through movement - as opposed
to novel infrastructure, rendering techniques, collision detection
algorithms, visualizations, descriptions of input devices or haptic
devices. Screening reduced our set to 671 papers.

Figure 5: Flow of information through the different phases
of our systematic review, following PRISMA guidelines.

4.1.3 Phase 3: Eligibility. We excluded papers that were analyses
or experimental evaluations of existing interaction techniques (283),
applications of interaction techniques to real-world problems (127),
surveys of interaction techniques (8), and revisions of the same
interaction techniques produced by the same authors (5). This step
built our final study set of 248 papers.

4.1.4 Phase 4: Dataset and coding. We coded each included paper
along four dimensions: (1) type of interaction: reality-based, illusory,
or beyond-real, and if beyond-real (2) interaction task: selection,
manipulation, locomotion, or wayfinding, (3) remapping parameter:
space, time, or body, and (4) consideration of sensorimotor loop:
yes/no. Often papers developed techniques that leveraged multiple
transformations and could be applied to multiple interaction tasks.
We applied multiple labels in these cases. Note that while we have
coded for all interaction tasks that appeared in our dataset, we
focus on interactions that require skilled motor actions and not
higher-level cognition (e.g., wayfinding) in the results.

4.2 High-Level Survey Findings
Interaction types. Of the interaction techniques we analyzed,
4.2.1
we found: 103 reality-based (42%), 48 illusory (19%), and 97 beyond-
real (39%), as shown in Figure 6. While the frequencies of beyond-
real and reality-based interaction papers remained relatively consis-
tent over time, we saw a jump in the number of illusory interactions
after 2016; 12 illusory interactions were presented before (in 20
years, 1996-2016) and 36 after (in 4 years, 2017-2021). For a full list
of illusory interaction papers, please refer to Appendix A.

Figure 6: Bar chart partitioning VR interaction papers into
reality-based, illusory, and beyond-real categories.

Interaction tasks. Of the 97 beyond-real interaction tasks, we
4.2.2
found: 51 selection (39%), 43 manipulation (33%) and 37 locomotion
(28%), shown in Figure 7. These numbers do not sum to 97 because,
as mentioned, some interaction techniques are multi-purpose. For
example, beyond-real techniques that leverage miniature recon-
structions of virtual environments allow for both selection and
manipulation of occluded objects [89].

4.3 Beyond-Real Transformations Explored
Here we focused on the subset of papers in our survey that explore
beyond-real interactions. We applied our framework (described in
section 3) to these prior works in order to isolate the beyond-real
transformations they utilize and organized these transformations
into three groups (space, body, or time) based on their remapping

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Duplication: A fixed mapping can be created between the real-world
space and multiple copies of the environment in VR. Poros is one
such interaction technique that displays proxies of remote regions
of the virtual environment [121]. The changes made to the miniature
VR space propagate to the full-size virtual environment, enabling
users to utilize the proxy to manipulate objects. Another technique,
vMirror, uses strategically placed mirrors and their reflections to
allow users to manipulate obscured objects [89].

4.3.2 Body Transformations. Beyond-real interactions may alter
users’ body representation, which has been defined in a variety
of ways. Given our focus on action, in this paper, we refer to the
categorization of bodily representation by Martel et al. [95] which
includes body image, body structural description, and body schema.
Body image relies heavily on visual input and refers to the con-
scious representation of the body’s shape and size. Body structural
description is a conscious spatial map of the body parts and their
relationships, informed primarily by somatosensory and visual sys-
tems. Body schema refers to the unconscious and highly plastic
representation of the body parts, including posture, shape, and size.
It should be noted that while some beyond-real interactions may
be described in other ways, such as space transformations, they
are perceived by users as transformations of body representation.
Due to the plasticity of body representation in the human brain,
this egocentric perspective is necessary to more accurately capture
users’ expectations and actions.

Alternate Morphologies: In VR, users can embody avatars with
novel sizes, forms, and structures. For example, Ninja Hands maps
the movement of a single hand to multiple hands to ease distant
target selection [135]. Another paper iteratively adjusts the length,
and therefore range of motion, of the avatar’s forearms and fingers
to achieve better performance on specific tasks [96]. Note that users
may perceive space scaling transformations as body scaling and a
form of alternate morphology.

Movement Remapping: Movement of the user’s body in the real
world can be altered virtually to represent another type of move-
ment. Shake-Your-Head maps lateral and vertical head movements
to walking and jumping, thus enabling in-place locomotion [175].
Walking by Cycling maps real-world pedaling motions to the walk-
ing of a virtual avatar [50]. Movement remappings can also be used
for object selection and manipulation. For example, users can move
and rotate objects by grasping an imaginary handle bar skewering
virtual objects [160]. One prominent form of movement remapping
comes in the form of gaze-based interactions, which translates eye
movement to hand input such as grabbing or shifting objects [206].

Tool Use: Increasingly, evidence is emerging that tool-use may af-
fect body image [95] and body schema [26]. This point is further
discussed by Seinfeld et al. [142] through the concept of User Rep-
resentations. Therefore, in some cases it may be appropriate to
describe tool-based interactions as a body representation transfor-
mation. For example, ray-casting is a popular tool-based selection
technique in which a light ray extends from the user’s finger and in-
tersects with various objects. Ray-casting can be enhanced to select
the nearest target [15]. For multiple object selection, researchers
have also developed techniques that map the position of users’
hands to virtual brushes and lassos [164].

Figure 7: Bar chart partitioning beyond-real interaction pa-
pers based on the interaction task they focus on.

parameter. Using a combination of inductive and deductive coding,
we then identified subcategories of transformations that we describe
in this section. It should be noted that these transformations can be
described in multiple ways. Additionally, a user may reason about
these transformations differently than how the transformations are
implemented in practice. Therefore, the choice of transformation
functions may depend on the specifics of the design, context of the
interaction, or the aim of the analysis.

Space Transformations. Space transformations create remap-

4.3.1
pings of movement in 3D space.

Translation: Prior work has explored space translation for locomo-
tion tasks in VR, specifically to augment walking. Translational
gain amplifies the shift of the virtual ground under the user’s feet
to enable users to walk more rapidly in VR [125, 195].

Scaling: The scale of rendered content can be altered in VR to enable
novel forms of interactions with virtual objects and the surrounding
environment. For example, users can scale down the environment
to obtain a high-level overview and more easily manipulate large
virtual objects [204]. Scale-based remappings have also been lever-
aged for locomotion. For example, the virtual world can be scaled
down with the center of scaling at the midpoint between the user’s
eyes, allowing them to walk rapidly through a miniature world [2].

Figure 8: Examples of beyond-real interaction techniques in
the literature survey (a: [4], b: [135], c: [204], d: [96]).

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

Table 1: Beyond-Real Interactions Categorized by Transformation Type

Transformation

Count References

Space

Translation
Scaling
Duplication

Body

Alternative Morphologies
Movement Remapping

Tool Use

Time

Time Travel
Speed Change

44

18
27
18

55

8
29

22

4

2
2

[3, 41–43, 73, 86, 91, 100, 108, 116, 125, 159, 185, 187, 191, 192, 195, 199]
[3, 9, 28, 33, 38, 41–43, 83, 86, 89, 91, 104–106, 120, 121, 159, 161, 170, 185–187, 191, 199, 204, 207]
[7, 32, 49, 89, 99, 103, 106, 112, 120, 121, 138, 167, 180, 188, 204, 204, 207, 209]

[3, 44, 60, 96, 122, 135, 189, 204]
[5, 14, 25, 39, 44, 48, 50, 52, 62, 63, 100, 145–147, 151, 158–160, 162, 173–175, 179, 205, 206, 208]
[193, 204, 210]
[8, 10, 15, 37, 53, 71, 76, 91–93, 97, 118, 123, 136, 158, 164, 168, 169, 181, 190, 203, 208]

[148, 204]
[74, 126]

4.3.3 Time Transformations. Remappings can be created that alter
the user’s perception of and interactions with time.

Time Travel: In VR it is possible to allow the user to visit the future
or retrace their temporal footsteps. One technique allows the user
to revisit old checkpoints along a path for navigation [149]. The
users’ timeline of engagements with the VR application is recorded
and becomes another dimension along which they may travel.

Speed Change: Users of virtual reality applications can develop
skills with gentler learning curves with the help of time manipula-
tion - for instance, slower motion of a tennis ball so that beginner
players can successfully return it [75]. The motion of avatars in
VR can also be accelerated or decelerated to change the user’s
perception of time [127].

4.4 Survey Results for Beyond-Real

Interactions

4.4.1 Transformation types. Of the 97 beyond-real transformation
papers, we found: 44 space transformations (45%), 55 body transfor-
mations (57%), and 4 time transformations (4%). All of the beyond-
real interactions surveyed are shown in Table 1, where they are
organized based on subcategories of transformations. Of space
transformations, we found 27 are scaling (61%), 18 are translation
(41%), and 18 are duplication (41%). Of body transformations, we
found 22 involve tool use (40%), 29 are movement remapping (53%),
and 8 are alternative morphologies (14%). Of time transformations,
2 leveraged speed change and 2 used time travel. Multiple space
and body transformations consisted of sub-transformations. For
example, techniques that scale the user’s jumps scale the environ-
ment (shrink it to make the jump appear higher) as well as translate
it (have the ground move faster while the user is in the air).

4.4.2 Consideration of sensorimotor loop. We found that 23 of the
97 beyond-real papers consider the effect of sensory conflict (24%);
only 4 of these were published before 2017. Usually discussions
of the sensorimotor loop center around simulator sickness evalu-
ated on study participants with the standard Simulator Sickness

Questionnaire (SSQ). Primarily SSQ scores are one of several met-
rics, such as frustration or movement instability, used to assess the
effectiveness of a given interaction technique. We found limited
examination of causal factors in favor of a more empirical treat-
ment. Deeper model-based analysis such as that enabled by control
theory may position researchers to design interaction techniques
that do not induce simulator sickness at the outset, as well as iterate
more efficiently upon recognition of factors responsible for sensory
mismatch. The results of this survey additionally suggest a strong
opportunity for the VR community to explore sensorimotor issues
with interaction techniques beyond simulator sickness.

5 OPEN RESEARCH QUESTIONS

Figure 9: Users receive sensory information from both the
real world and the VR system which are then integrated. Un-
derstanding sensory integration and how the user’s internal
model is updated accordingly is integral for exploring open
research questions around beyond-real VR interactions.

Our survey identified that over the years, a number of reality-
based and beyond-real interaction techniques have been devel-
oped and presented at research conferences. While a clear research
agenda has been articulated for reality-based interactions (see [72]),
as a research community, we are missing a road map for the de-
sign and development of beyond-real interactions. Our survey also

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

identified that while there are many new beyond-real interaction
techniques developed, only a small percentage of them consider
sensorimotor control issues beyond motion sickness. Our proposed
framework, beyond being real, allows for a uniform way to describe
such interactions from this sensorimotor control perspective, but
lacks any predictive ability. We believe a research agenda is needed
to help bridge this gap towards better understanding and modeling
of beyond-real interactions. Here we discuss some of the open re-
search questions for beyond-real interactions from this perspective.

5.1 Control and Usability
In the real world, once learned, we perform skilled motor actions
automatically and without conscious awareness of how they are
executed [114, 124]. Initially, beyond-real remappings are unfa-
miliar, as they have no counterpart in our physical reality, and
the forward model cannot predict the outcome of motor actions.
These unexpected prediction errors shift the user’s attention in the
virtual environment [6, 172] and lead to breakdowns that require
conscious reflection during interaction, as described by the Hei-
deggerian notion of presentness-at-hand [65, 196]. Motor learning
can be described as an experience-driven, systematic update to the
internal model that enables users to predict the outcome of motor
commands and develop new control policies [46] (see Figure 3).
For beyond-real interaction designs to be usable [113], users need
to learn to synthesize movement under the new dynamics [61].
Meaningful feedback plays a key role in these adaptations, as has
been shown for body transformations in VR [36]. However, the best
methods for supporting these adaptations are not known.

5.1.1 Drawing from Prior Experiences. The time it takes for users
to learn beyond-real remappings depends on their familiarity with
the motor task [61]. To address this lack of familiarity, designers
have leveraged users’ prior experiences by using themes from sci-
ence fiction literature, or more broadly books, movies, and other
narratives [109]. Another approach is to design interactions that
indirectly utilize skills users have already developed in the real
world. For example, eye gaze as a mechanism for selecting distant
objects leverages a skill we have developed as a result of making
eye-contact with others during conversation [72].

5.1.2 Learning Timescales. Repeated interactions are needed for
users to learn new control policies over time. For example, in a
study where the range of motion of the participants’ arms and legs
were swapped, it took around 10 minutes for participants to learn
the remapping and to utilize the range of motion of their avatar’s
body parts [201]. Motor learning is driven by different processes
at multiple timescales and it often involves quick approximations,
followed by slow adjustments that enable fine tuning [67]. Beyond-
real interactions change how we perceive the affordances of objects
and properties of the world around us. For example, body scaling
influences users’ perception of size and distance [16, 82, 184]. This
movement context is captured by the internal model, and compared
to the state updates, it changes at a much slower time-scale [200].
Moreover, “motor learning undergoes a period of consolidation,
during which time the motor memory is fragile to being disrupted”;
therefore, dynamic remappings that may interfere with one another
should not be presented in quick succession.

5.1.3 Exploration. The uncertainty associated with outcomes of
motor action directly relates to exploratory behavior [182]. When
users suspect that their knowledge of the environment could be
improved, they make exploratory choices to increase their learn-
ing rate [40]. While the specifics of these learning processes in the
brain is not fully understood [88], prior research has proposed differ-
ent strategies for encouraging exploration. For example, providing
lower quality visual feedback may increase uncertainty, promote
exploratory risk-taking, and lead to more accurate internal models
under the new dynamics [67]. This approach is at odds with the
sentiment of effective feedback in interaction design, and more
research is needed to unpack this interplay.

Due to these gaps in our understanding, more predictive models
are needed to determine the boundaries of usable beyond-real VR
interactions and how far we can push those boundaries [201]. More
specifically, new methods are needed for evaluating the usability
of a beyond-real interaction design and predicting how much time
is necessary for users to learn the remapping.

5.2 Long-Term Use and Aftereffects
It has been shown that our cognitive system can adjust to repeated
exposure to conflicting stimuli [20, 35]. This adaptation, which
is driven primarily by the forward model [18], has been studied
both in the context of illusions and interactions beyond reality. The
VR user experience begins when users choose to engage with the
virtual content and put on their headset and continues as users
exit VR [80]. Therefore, adaptation aftereffects that may carry over
into the real world need to be carefully considered. Aftereffects
are often task-dependent [6] and may also depend on the speed of
the movement [35]. These adaptations also affect other aspects of
how we perceive space [82]. However, not all adaptations lead to
aftereffects, and this may depend on the modality of the sensory
feedback. For example, in a between-subjects study where users
experienced either visual or proprioceptive distortions (created by
vibration), the effects of proprioceptive adaptations disappeared
afterwards [17]. Many research questions remain unanswered in
the context of long-term use of beyond-real interactions. Can we
train users, through extended practice, to perform effectively under
novel remappings? How will the dynamics of the interaction change
after the novelty of the beyond-real experience diminishes? Will
users maintain their ability to perform under similar remappings
the next time they return to the virtual experience?

5.3 Individual Differences
Individual differences play a significant role in how users perceive
and act in virtual environments [58]. These differences also influ-
ence sensory integration and the thresholds at which users become
aware of novel remappings. As a result, the categorization of in-
teractions as either illusory or beyond-real is user-dependent. For
example, a user that notices a reach redirection illusion [11] will
perceive this interaction as a beyond-real space transformation. In
the context of beyond-real VR interactions, various factors may
contribute to these individual differences, including users’ age [66],
physique [23], prior experiences [165, 194], familiarity with science
fiction [109], or gaming frequency [197].

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

5.3.1 Physiological Responses. Beyond-real interactions are sus-
ceptible to negative physiological responses, as users often receive
incongruent sensory feedback from the real world and the vir-
tual system. For example, some users report symptoms of motion
sickness when there is a mismatch in sensory feedback from the
vestibular and visual systems. Individuals may have different phys-
iological responses to a virtual experience. With regards to flying,
researchers have found that “a lot of people find it an endless source
of fun, but other people report tired arms and motion sickness” [24].
The user’s physiological response is an important consideration
from both safety and usability standpoints [12, 47].

5.3.2 Emotional Responses. VR has been recognized as a powerful,
immersive media that can evoke strong emotions in users [12]. In
particular, experiences beyond reality may lead to profound emo-
tional responses, both positive [54, 165], often involving the feeling
of awe [31, 77], and negative, such as the feelings of fear [21], dis-
tress [141, 166], and regret [51]. It has been shown that incongruent
sensory stimuli result in negative emotions [139]. Conversely, emo-
tional responses influence multisensory processing in ways that
can be reflected in action [85]. For example, expectations of high
reward release dopamine in the brain, such that it may no longer
operate as an optimal controller [40].

At a high-level, individual differences influence sensorimotor con-
trol and consequently, how users respond to beyond-real VR inter-
actions. However, many open research questions remain regarding
how individual differences should be accounted for in the design of
such interactions. How can we capture individual differences from
real-world input data? How might we better model user behavior
based on individual user’s interactions with the system over time?
Can we offer adaptive, personalized experiences that account for
individual differences? How might we then evaluate beyond-real
interaction designs at a large scale?

5.4 Presence and Plausibility
Presence, or place illusion, has been defined as the psychological
experience of being there: “the extent to which an individual expe-
riences the virtual setting as the one in which they are consciously
present” [156]. Designers, often guided by implicit or explicit the-
ories, seek presence in the hopes of improving other attributes
of the virtual experience, including learning or task performance
[19]. However, researchers have found inconsistent results when
studying the correlation between the sense of presence and such
attributes [154, 156], which perhaps is expected, as these are influ-
enced by many factors, including users’ abilities and prior expe-
riences. Due to individual differences, in practice, it may be chal-
lenging to evaluate the effects of beyond-real interactions on users’
subjective sense of presence. Witmer and Singer [197] have pro-
posed a collection of factors hypothesized to contribute to pres-
ence, including control, sensory, distraction, and realism factors. It
should be noted that the realism factor does not require content
that replicates reality, but relates to continuities, connectedness,
and coherence of the virtual experience.

Plausibility illusion, the illusion that what is occurring is actually
happening [152], is another psychological dimension that has been
attributed to realistic responses to virtual environments. Plausibility

illusion also does not require physical realism and is related to
causal relationships between the user’s actions and the resulting
sensations. While in this paper we do not discuss presence and
plausibility directly, human sensorimotor control naturally lends
itself to discussions around these contributing factors.

5.5 Accessibility
Beyond-real interactions must also be considered from the perspec-
tive of accessibility. Mott et al. [110] discuss the potential in VR
for increased “interaction accessibility” and equity for all people,
including people with disabilities, given that in VR people can have
abilities no person can experience in the real world, what they call
“superpowers” (e.g., flying). Sadeghian and Hassenzahl extend this
concept of superpowers into a VR interaction design methodol-
ogy [133]. While assistive technology in the physical world has
many limitations in terms of its ability to adapt, VR and specifi-
cally beyond-real interactions might support more adaptive and
ability-based interactions [198]. However, researchers have also
warned about the potential to amplify differences in ability [110]
and that the inherent body-centric perspective of VR poses sub-
stantial issues for people with physical disabilities. Gerlig and Spiel
[55] highlight the importance of considering minority bodies while
designing VR interactions and more importantly including people
with disabilities in the design of new interaction paradigms. Addi-
tionally, while some VR accessibility research, including the papers
we surveyed, focus on manipulating visual feedback (e.g., [101]),
there are opportunities for exploration of beyond-real interactions
through other sensory feedback, which would increase accessibility
for blind and visually impaired VR users, who primarily access VR
through audio [34, 143] or haptics [150, 211].

5.6 Ethical Implications
Slater et al. present a detailed discussion of the ethics of realism in
virtual reality, and many of the discussion points are highly rele-
vant to beyond-real experiences [153]. Throughout the paper, we
have also alluded to some ethical implications of beyond-real VR
interactions, such as motion sickness; however, it is necessary to
explicitly acknowledge the importance of ethical considerations
from a sensorimotor control perspective. Beyond-real remappings
may result in motor behavior changes during the interaction. For
example, when embodying an avatar with a flexible tail-like ap-
pendage, users changed the way they moved their hips [202]. When
walking around a virtual environment as a giant, users took bigger
steps in the real world [2]. “Presence in VR leads to absence in
the physical world” [12]; therefore, not accounting for behavior
changes, especially with regards to users’ movements in the real
world, could have serious consequences. Moreover, long-term use of
beyond-real VR interactions may have aftereffects that alter motor
behavior in the real world [17]. For example, in a study where users’
virtual eye position was offset from the position of their real eyes,
it was shown that, after removing the VR headset, participants’
hand-eye coordination was altered, as evidenced by their inability
to accurately point to a target [20]. Such sensorimotor adaptations
pose safety concerns that need to be carefully considered.

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

6 LIMITATIONS
In the following section, we will acknowledge some of the limi-
tations of our work and highlight opportunities for future work
related to the study of beyond-real virtual reality interactions.

6.1 Completeness
The human sensorimotor system is incredibly complex and many
details are not captured by the simplified model presented here.
For example, multisensory integration happens at multiple stages,
utilizing both bottom-up and top-down processes [172], and the
sensory signals involve considerable delays that are largely missing
in our model [200].

Moreover, our goal for conducting a survey was to apply the
framework to selected work exploring beyond-real VR interactions.
One limitation is our choice of query; a broader query may be
needed for an exhaustive categorization. For example, some VR
interaction papers, such as [90], did not appear in our search be-
cause neither “VR” nor “virtual” was mentioned in the title or the
abstract of the paper. Future work may also consider analyzing
other venues, such as the journal of Virtual Reality, and including
VR interactions that were not in the scope of our work, such as
system control and symbolic input [22].

6.2 Embodiment
Previous research has studied the sense of embodiment, which has
been defined as having three components: the sense of self-location,
the sense of agency, and the sense of body ownership [78]. Our work
lacks a coverage of this extensive body of research and how beyond-
real interactions may influence users’ sense of embodiment in VR.
For example, it has been shown that in arm-extension techniques
the sense of body ownership declines as the length of the virtual
arm increases [79]. The effects of beyond-real transformations on
body ownership have mainly been studied in isolation, and our
understanding of how different transformations might interact
with each other in more complex scenarios is limited. Consider
the beyond-real interaction technique Ninja Hands [135], where
the movement of the user’s hand is mapped to multiple virtual
hands in space. In Ninja Hands, these virtual hands are visually
disconnected from the user’s body (see Figure 8b). While prior
research has shown that virtual limbs that are visually connected
to the user’s body increase the user’s sense of body ownership (as
measured through their physiological responses) [177], it remains
unclear if many connected limbs, as in Ninja Hands, would have
a similar effect. Users’ sense of body ownership, which is subject
to individual differences [94], may have implications for learning
and adaptation of beyond-real interactions. Further research is
needed to unravel the effects of beyond-real transformations on
embodiment, including body ownership, agency, and self-location.

6.3 Focus on Action
While we focus on VR experiences that require users to act on
the world, applications that do not focus on action could also be
beneficial. For example, beyond-real experiences can be utilized
for demonstration in educational applications, as they provide a
unique opportunity for learning abstract concepts [24]. They can
facilitate transformative experiences that evoke strong emotions

and elicit new insights [54]. Beyond-real experiences can ignite
one’s imagination and foster creativity [84] and encourage positive
behavior changes that may even transfer to the real world [12].

6.4 Social Interactions
In our framework, we have taken an ego-centric approach, focusing
on a single user’s interactions; however, VR is well suited for social-
ization and collaboration. When describing how reality is reflected
in the word virtual reality, Lanier highlights that “VR functioned as
the interstices or connection between people; a role that had been
previously taken only by the physical world . . . A reality results
when a mind has faith that other minds share enough of the same
world to establish communication and empathy” (p. 240) [84].

Bailenson et al. describe techniques beyond reality that change
the nature of social interaction in collaborative virtual environ-
ments, including manipulation of self representation, sensory ca-
pabilities, and the temporal/spatial context [13]. They argue that
in VR, unlike face-to-face interaction, the user’s rendered behavior
can deviate from their actual behavior. The system can leverage this
characteristic to, for example, improve communication by altering
the user’s rendered behavior such that it mimics the nonverbal
behavior of others, referred to as the Chameleon Effect [29]. While
beyond-real social interactions have been explored [129, 130], many
research questions remain. For example, how might users with dra-
matically different scales interact [204]? How does that influence
their perception of interpersonal distance?

6.5 Other Sensory Modalities
Our work mainly addresses users’ visual and somatosensory sys-
tems. However, the current state of VR technology enables render-
ing of audio, and perhaps in the future commercial VR headsets
may be able to engage our other sensory input channels, such as
olfactory [111]. Virtual experiences beyond reality are not limited
to vision and touch, and can span other sensory modalities. For ex-
ample, in VR, we may gain the ability to smell the scent associated
with others, in ways that we could know when someone familiar
enters a room without seeing them, or whether that person has
previously been in the same room.

6.6 Mixed Reality Spectrum
Finally, while we have specifically focused on virtual reality, beyond-
real interactions may be integrated into other experiences on the
mixed reality spectrum [98]. For example, the Go-Go arm-extension
technique has been applied in augmented reality to enable inter-
actions with distant objects in the real world [45]. While similar
transformations may be used to describe such interactions, further
study of the implications and considerations is needed.

7 CONCLUSION
In this paper, we first described a simplified model of the control
signal flow during movement-based interactions and situated VR
interactions within this model. We explained how intent is con-
verted to motor commands in the central nervous system resulting
in movements in the real world. These movements are tracked by
the VR system and transformed into virtual renderings. Users re-
ceive sensory feedback from both the real world and the virtual

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

system. In most cases, the brain operates as an optimal controller
and with the use of the state estimator, responds accordingly to
perform the intended actions in VR. Using this simplified model, we
partitioned the space of VR interactions into reality-based, illusory,
and beyond-real based on the magnitude of the resulting sensory
conflict. We then presented beyond being real, a framework for de-
scribing beyond-real interactions as a set of transformations applied
to real-world input. We conducted a survey of prior HCI literature
(at CHI, IEEE VR, VRST, UIST, and DIS conferences) with a focus
on selection, manipulation, locomotion, and navigation in VR. We
applied our framework to extract and categorize the beyond-real
transformations in these works and highlighted a gap: research that
carefully considers the consequences of sensory conflict resulting
from beyond-real transformations. Lastly, we discussed challenges
and opportunities for future research towards the goal of better
understanding and evaluating interactions beyond reality.

ACKNOWLEDGMENTS
We would like to thank Jeremy Bailenson and Mar Gonzalez-Franco
for their help and guidance. We also thank Tamara Munzner, Evan
Strasnick, Cole S. Simpson, Darrel R. Deo, the Stanford HCI group,
and the anonymous CHI reviewers for their valuable feedback.

REFERENCES

[1] Parastoo Abtahi and Sean Follmer. 2018. Visuo-Haptic Illusions for Improving
the Perceived Performance of Shape Displays. In Proceedings of the 2018 CHI
Conference on Human Factors in Computing Systems. ACM, 150.

[2] Parastoo Abtahi, Mar Gonzalez-Franco, Eyal Ofek, and Anthony Steed. 2019.
I’m a Giant: Walking in Large Virtual Environments at High Speed Gains. In
Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems.
ACM, 522.

[3] Parastoo Abtahi, Mar Gonzalez-Franco, Eyal Ofek, and Anthony Steed. 2019.
I’m a Giant: Walking in Large Virtual Environments at High Speed Gains.
In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. Association for Computing Machinery, New York, NY, USA, 1–13.
https://doi.org/10.1145/3290605.3300752

[4] Parastoo Abtahi, Benoit Landry, Jackie (Junrui) Yang, Marco Pavone, Sean
Follmer, and James A. Landay. 2019. Beyond The Force: Using Quadcopters
to Appropriate Objects and the Environment for Haptics in Virtual Reality.
In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. Association for Computing Machinery, New York, NY, USA, 1–13.
https://doi.org/10.1145/3290605.3300589

[5] Sunggeun Ahn, Stephanie Santosa, Mark Parent, Daniel Wigdor, Tovi Grossman,
and Marcello Giordano. 2021. StickyPie: A Gaze-Based, Scale-Invariant Marking
Menu Optimized for AR/VR.
In Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems. Number 739. Association for Computing
Machinery, New York, NY, USA, 1–16. https://doi.org/10.1145/3411764.3445297
[6] Michael A Arbib, James B Bonaiuto, Stéphane Jacobs, and Scott H Frey. 2009.
Tool use and the distalization of the end-effector. Psychological Research PRPF
73, 4 (2009), 441–462.

[7] Ferran Argelaguet and Carlos Andujar. 2009. Visual feedback techniques for
virtual pointing on stereoscopic displays. In Proceedings of the 16th ACM Sym-
posium on Virtual Reality Software and Technology (VRST ’09). Association for
Computing Machinery, New York, NY, USA, 163–170. https://doi.org/10.1145/
1643928.1643966

[8] Ferran Argelaguet, Carlos Andujar, and Ramon Trueba. 2008. Overcoming
eye-hand visibility mismatch in 3D pointing selection. In Proceedings of the
2008 ACM symposium on Virtual reality software and technology (VRST ’08).
Association for Computing Machinery, New York, NY, USA, 43–46. https:
//doi.org/10.1145/1450579.1450588

[9] Ferran Argelaguet and Morgant Maignant. 2016. GiAnt: stereoscopic-compliant
multi-scale navigation in VEs. In Proceedings of the 22nd ACM Conference on
Virtual Reality Software and Technology (VRST ’16). Association for Computing
Machinery, New York, NY, USA, 269–277. https://doi.org/10.1145/2993369.
2993391

[10] Rahul Arora, Rubaiat Habib Kazi, Danny M. Kaufman, Wilmot Li, and Karan
Singh. 2019. MagicalHands: Mid-Air Hand Gestures for Animating in VR. In
Proceedings of the 32nd Annual ACM Symposium on User Interface Software and

Technology (UIST ’19). Association for Computing Machinery, New York, NY,
USA, 463–477. https://doi.org/10.1145/3332165.3347942

[11] Mahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D
Wilson. 2016. Haptic retargeting: Dynamic repurposing of passive haptics for
enhanced virtual reality experiences. In Proceedings of the 2016 CHI Conference
on Human Factors in Computing Systems. ACM, 1968–1979.

[12] Jeremy Bailenson. 2018. Experience on demand: What virtual reality is, how it

works, and what it can do. WW Norton & Company.

[13] Jeremy N Bailenson, Andrew C Beall, Jack Loomis, Jim Blascovich, and Matthew
Turk. 2004. Transformed social interaction: Decoupling representation from
behavior and form in collaborative virtual environments. Presence: Teleoperators
& Virtual Environments 13, 4 (2004), 428–441.

[14] Ravin Balakrishnan and Gordon Kurtenbach. 1999. Exploring bimanual camera
control and object manipulation in 3D graphics interfaces. In Proceedings of the
SIGCHI conference on Human Factors in Computing Systems (CHI ’99). Association
for Computing Machinery, New York, NY, USA, 56–62. https://doi.org/10.1145/
302979.302991

[15] Marc Baloup, Thomas Pietrzak, and Géry Casiez. 2019. RayCursor: A 3D Pointing
In Proceedings of the 2019 CHI
Facilitation Technique based on Raycasting.
Conference on Human Factors in Computing Systems. Association for Computing
Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3290605.3300331
[16] Domna Banakou, Raphaela Groten, and Mel Slater. 2013. Illusory ownership of
a virtual child body causes overestimation of object sizes and implicit attitude
changes. Proceedings of the National Academy of Sciences 110, 31 (July 2013),
12846–12851. https://doi.org/10.1073/pnas.1306779110

[17] Pierre-Michel Bernier, Romeo Chua, J Timothy Inglis, and Ian M Franks. 2007.
Sensorimotor adaptation in response to proprioceptive bias. Experimental brain
research 177, 2 (2007), 147.

[18] Nikhil Bhushan and Reza Shadmehr. 1999. Evidence for a forward dynamics
model in human adaptive motor control. In Advances in Neural Information
Processing Systems. 3–9.

[19] Frank Biocca. 1997. The cyborg’s dilemma: Progressive embodiment in vir-
tual environments. Journal of computer-mediated communication 3, 2 (1997),
JCMC324.

[20] Frank A Biocca and Jannick P Rolland. 1998. Virtual eyes can rearrange your
body: Adaptation to visual displacement in see-through, head-mounted displays.
Presence 7, 3 (1998), 262–277.

[21] Pierre Bourdin, Itxaso Barberia, Ramon Oliva, and Mel Slater. 2017. A virtual
out-of-body experience reduces fear of death. PloS one 12, 1 (2017), e0169343.
[22] Doug Bowman, Ernst Kruijff, Joseph J LaViola Jr, and Ivan P Poupyrev. 2004. 3D

User interfaces: theory and practice, CourseSmart eTextbook. Addison-Wesley.

[23] Doug A Bowman and Larry F Hodges. 1997. An Evaluation of Techniques for
Grabbing and Manipulating Remote Objects in Immersive Virtual Environments.
SI3D 97 (1997), 35–38.

[24] Meredith Bricken. 1991. Virtual Reality Learning Environments: Potentials
and Challenges. SIGGRAPH Comput. Graph. 25, 3 (July 1991), 178–184. https:
//doi.org/10.1145/126640.126657

[25] Fabio Marco Caputo, Marco Emporio, and Andrea Giachetti. 2017. The smart
pin: a novel object manipulation technique for immersive virtual environments.
In Proceedings of the 23rd ACM Symposium on Virtual Reality Software and
Technology (VRST ’17). Association for Computing Machinery, New York, NY,
USA, 1–2. https://doi.org/10.1145/3139131.3141784

[26] Lucilla Cardinali, Francesca Frassinetti, Claudio Brozzoli, Christian Urquizar,
Alice C Roy, and Alessandro Farnè. 2009. Tool-use induces morphological
updating of the body schema. Current biology 19, 12 (2009), R478–R479.
[27] Roberto Casati and Elena Pasquinelli. 2005. Is the subjective feel of “presence”
an uninteresting goal? Journal of Visual Languages & Computing 16, 5 (2005),
428–441.

[28] Jeffrey Cashion, Chadwick Wingrave, and Joseph J. LaViola Jr. 2012. Dense and
Dynamic 3D Selection for Game-Based Virtual Environments. IEEE Transactions
on Visualization and Computer Graphics 18, 4 (April 2012), 634–642. https:
//doi.org/10.1109/TVCG.2012.40

[29] Tanya L Chartrand and John A Bargh. 1999. The chameleon effect: the
perception–behavior link and social interaction. Journal of personality and
social psychology 76, 6 (1999), 893.

[30] Lung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D
Wilson. 2017. Sparse haptic proxy: Touch feedback in virtual environments
using a general passive prop. In Proceedings of the 2017 CHI Conference on Human
Factors in Computing Systems. ACM, 3718–3728.

[31] Alice Chirico, David B Yaden, Giuseppe Riva, and Andrea Gaggioli. 2016. The
potential of virtual reality for the investigation of awe. Frontiers in psychology 7
(2016), 1766.

[32] Luca Chittaro and Subramanian Venkataraman. 2006. Navigation aids for
multi-floor virtual buildings: a comparative evaluation of two approaches. In
Proceedings of the ACM symposium on Virtual reality software and technology
(VRST ’06). Association for Computing Machinery, New York, NY, USA, 227–235.
https://doi.org/10.1145/1180495.1180542

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

[33] Gabriel Cirio, Maud Marchal, Tony Regia-Corte, and Anatole Lécuyer. 2009.
The magic barrier tape: a novel metaphor for infinite navigation in virtual
worlds with a restricted walking workspace. In Proceedings of the 16th ACM
Symposium on Virtual Reality Software and Technology (VRST ’09). Association
for Computing Machinery, New York, NY, USA, 155–162. https://doi.org/10.
1145/1643928.1643965

[34] Erin Connors, Elizabeth Chrastil, Jaimie Sánchez, and Lotfi B Merabet. 2014.
Action video game play and transfer of navigation and spatial cognition skills
in adolescents who are blind. Frontiers in human neuroscience 8 (2014), 133.
[35] Holk Cruse, Jeffrey Dean, H Heuer, and RA Schmidt. 1990. Utilization of sensory
information for motor control. In Relationships between perception and action.
Springer, 43–79.

[36] Brian Day, Elham Ebrahimi, Leah S Hartman, Christopher C Pagano, Andrew C
Robb, and Sabarish V Babu. 2019. Examining the effects of altered avatars on
perception-action in virtual reality. Journal of Experimental Psychology: Applied
25, 1 (2019), 1.

[37] Gerwin de Haan, Michal Koutek, and Frits H. Post. 2002. Towards intuitive explo-
ration tools for data visualization in VR. In Proceedings of the ACM symposium
on Virtual reality software and technology (VRST ’02). Association for Computing
Machinery, New York, NY, USA, 105–112. https://doi.org/10.1145/585740.585758
[38] Henrique G. Debarba, Sami Perrin, Bruno Herbelin, and Ronan Boulic. 2015.
Embodied interaction using non-planar projections in immersive virtual reality.
In Proceedings of the 21st ACM Symposium on Virtual Reality Software and
Technology (VRST ’15). Association for Computing Machinery, New York, NY,
USA, 125–128. https://doi.org/10.1145/2821592.2821603

[39] L. Dominjon, S. Richir, A. Lecuyer, and J.-M. Burkhardt. 2006. Haptic Hybrid
Rotations: Overcoming Hardware Angular Limitations of Force-Feedback De-
vices. In IEEE Virtual Reality Conference (VR 2006). 167–174. https://doi.org/10.
1109/VR.2006.68 ISSN: 2375-5334.

[40] Kenji Doya. 2008. Modulators of decision making. Nature neuroscience 11, 4

(2008), 410–416.

[41] Niklas Elmqvist. 2005. BalloonProbe: reducing occlusion in 3D using interactive
space distortion. In Proceedings of the ACM symposium on Virtual reality software
and technology (VRST ’05). Association for Computing Machinery, New York,
NY, USA, 134–137. https://doi.org/10.1145/1101616.1101643

[42] David Englmeier, Wanja Sajko, and Andreas Butz. 2021. Spherical World in
Miniature: Exploring the Tiny Planets Metaphor for Discrete Locomotion in
Virtual Reality. In 2021 IEEE Virtual Reality and 3D User Interfaces (VR). 345–352.
https://doi.org/10.1109/VR50410.2021.00057 ISSN: 2642-5254.

[43] Sevinc Eroglu, Frederic Stefan, Alain Chevalier, Daniel Roettger, Daniel Zielasko,
Torsten W. Kuhlen, and Benjamin Weyers. 2021. Design and Evaluation of a
Free-Hand VR-based Authoring Environment for Automated Vehicle Testing.
In 2021 IEEE Virtual Reality and 3D User Interfaces (VR). 1–10. https://doi.org/
10.1109/VR50410.2021.00020 ISSN: 2642-5254.

[44] K.M. Fairchild, B.H. Lee, J. Loo, H. Ng, and L. Serra. 1993. The heaven and earth
virtual reality: Designing applications for novice users. In Proceedings of IEEE
Virtual Reality Annual International Symposium. 47–53. https://doi.org/10.1109/
VRAIS.1993.380799

[45] Tiare Feuchtner and Jörg Müller. 2017. Extending the body for interaction with
reality. In Proceedings of the 2017 CHI Conference on Human Factors in Computing
Systems. ACM, 5145–5157.

[46] J Randall Flanagan, Philipp Vetter, Roland S Johansson, and Daniel M Wolpert.
2003. Prediction precedes control in motor learning. Current Biology 13, 2 (2003),
146–150.

[47] Jesse Fox, Dylan Arena, and Jeremy N Bailenson. 2009. Virtual reality: A survival
guide for the social scientist. Journal of Media Psychology 21, 3 (2009), 95–113.
[48] S. Frees and G.D. Kessler. 2005. Precise and rapid interaction through scaled
manipulation in immersive virtual environments. In IEEE Proceedings. VR 2005.
Virtual Reality, 2005. 99–106. https://doi.org/10.1109/VR.2005.1492759 ISSN:
2375-5334.

[49] Sebastian Freitag, Benjamin Weyers, and Torsten W. Kuhlen. 2018. Interactive
Exploration Assistance for Immersive Virtual Environments Based on Object
Visibility and Viewpoint Quality. In 2018 IEEE Conference on Virtual Reality and
3D User Interfaces (VR). 355–362. https://doi.org/10.1109/VR.2018.8447553
[50] Jann Philipp Freiwald, Oscar Ariza, Omar Janeh, and Frank Steinicke. 2020.
Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated
Virtual Reality Experiences. In Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems. Association for Computing Machinery, New York,
NY, USA, 1–12. https://doi.org/10.1145/3313831.3376574

[51] Doron Friedman, Rodrigo Pizarro, Keren Or-Berkers, Solène Neyret, Xueni
Pan, and Mel Slater. 2014. A method for generating an illusion of backwards
time travel using immersive virtual reality-an exploratory study. Frontiers in
psychology 5 (2014), 943.

[52] Shinji Fukatsu, Yoshifumi Kitamura, Toshihiro Masaki, and Fumio Kishino. 1998.
Intuitive control of "bird’s eye"; overview images for navigation in an enormous
virtual environment. In Proceedings of the ACM symposium on Virtual reality
software and technology (VRST ’98). Association for Computing Machinery, New
York, NY, USA, 67–76. https://doi.org/10.1145/293701.293710

[53] Markus Funk, Florian Müller, Marco Fendrich, Megan Shene, Moritz Kolvenbach,
Niclas Dobbertin, Sebastian Günther, and Max Mühlhäuser. 2019. Assessing
the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication
for Virtual Reality using Curved Trajectories. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems. Association for Computing
Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3290605.3300377
[54] Andrea Gaggioli, Alois Ferscha, Giuseppe Riva, Stephen Dunne, and Isabell
Viaud-Delmon. 2016. Human Computer Confluence: Transforming Human Expe-
rience through Symbiotic Technologies. De Gruyter Open Berlin, Germany.
[55] Kathrin Gerling and Katta Spiel. 2021. A Critical Examination of Virtual Reality
Technology in the Context of the Minority Body. In Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems. 1–14.

[56] Marco Gillies. 2016. What is Movement Interaction in Virtual Reality for?. In
Proceedings of the 3rd International Symposium on Movement and Computing.
1–4.

[57] Eric J Gonzalez, Parastoo Abtahi, and Sean Follmer. 2020. REACH+ Extending the
Reachability of Encountered-type Haptics Devices through Dynamic Redirection
in VR. In Proceedings of the 33rd Annual ACM Symposium on User Interface
Software and Technology. 236–248.

[58] Mar Gonzalez-Franco, Parastoo Abtahi, and Anthony Steed. 2019. Individual
Differences in Embodied Distance Estimation in Virtual Reality. In 2019 IEEE
Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 941–943.
[59] Mar Gonzalez-Franco and Jaron Lanier. 2017. Model of illusions and virtual

reality. Frontiers in psychology 8 (2017), 1125.

[60] Nathan Navarro Griffin and Eelke Folmer. 2019. Out-of-body Locomotion:
Vectionless Navigation with a Continuous Avatar Representation. In 25th ACM
Symposium on Virtual Reality Software and Technology (VRST ’19). Association
for Computing Machinery, New York, NY, USA, 1–8. https://doi.org/10.1145/
3359996.3364243

[61] Adrian M Haith and John W Krakauer. 2013. Model-based and model-free
mechanisms of human motor learning. In Progress in motor control. Springer,
1–21.

[62] Sara Hanson, Richard A. Paris, Haley A. Adams, and Bobby Bodenheimer. 2019.
Improving Walking in Place Methods with Individualization and Deep Networks.
In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). 367–376.
https://doi.org/10.1109/VR.2019.8797751 ISSN: 2642-5254.

[63] Devamardeep Hayatpur, Seongkook Heo, Haijun Xia, Wolfgang Stuerzlinger,
and Daniel Wigdor. 2019. Plane, Ray, and Point: Enabling Precise Spatial
Manipulations with Shape Constraints. In Proceedings of the 32nd Annual
ACM Symposium on User Interface Software and Technology (UIST ’19). Asso-
ciation for Computing Machinery, New York, NY, USA, 1185–1195. https:
//doi.org/10.1145/3332165.3347916

[64] David Hecht and Miriam Reiner. 2009. Sensory dominance in combinations
of audio, visual and haptic stimuli. Experimental brain research 193, 2 (2009),
307–314.

[65] Martin Heidegger. 1996. Being and time: A translation of Sein und Zeit. SUNY

press.

[66] Herbert Heuer and Mathias Hegele. 2010. The effects of mechanical transparency
on adjustment to a complex visuomotor transformation at early and late working
age. Journal of Experimental Psychology: Applied 16, 4 (2010), 399.

[67] Herbert Heuer and Sandra Sülzenbrück. 2013. Towards mastery of complex
visuo-motor transformations. Frontiers in Human Neuroscience 7 (2013), 32.
[68] Randall W Hill Jr, Jonathan Gratch, Stacy Marsella, Jeff Rickel, William R
Swartout, and David R Traum. 2003. Virtual Humans in the Mission Rehearsal
Exercise System. Ki 17, 4 (2003), 5.

[69] Jim Hollan and Scott Stornetta. 1992. Beyond being there. In Proceedings of the
SIGCHI conference on Human factors in computing systems. ACM, 119–125.
[70] Kasper Hornbæk and Antti Oulasvirta. 2017. What is interaction?. In Proceedings
of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 5040–
5052.

[71] Bret Jackson, Brighten Jelke, and Gabriel Brown. 2018. Yea Big, Yea High: A
3D User Interface for Surface Selection by Progressive Refinement in Virtual
Environments. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces
(VR). 320–326. https://doi.org/10.1109/VR.2018.8447559

[72] Robert JK Jacob, Audrey Girouard, Leanne M Hirshfield, Michael S Horn, Orit
Shaer, Erin Treacy Solovey, and Jamie Zigelbaum. 2008. Reality-based inter-
action: a framework for post-WIMP interfaces. In Proceedings of the SIGCHI
conference on Human factors in computing systems. ACM, 201–210.

[73] M. P. Jacob Habgood, David Moore, David Wilson, and Sergio Alapont. 2018.
Rapid, Continuous Movement Between Nodes as an Accessible Virtual Reality
Locomotion Technique. In 2018 IEEE Conference on Virtual Reality and 3D User
Interfaces (VR). 371–378. https://doi.org/10.1109/VR.2018.8446130

[74] Shixin Jiang and Jun Rekimoto. 2020. Mediated-Timescale Learning: Manipu-
lating Timescales in Virtual Reality to Improve Real-World Tennis Forehand
Volley. In 26th ACM Symposium on Virtual Reality Software and Technology
(VRST ’20). Association for Computing Machinery, New York, NY, USA, 1–2.
https://doi.org/10.1145/3385956.3422128

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

[75] Shixin Jiang and Jun Rekimoto. 2020. Mediated-Timescale Learning: Manipulat-
ing Timescales in Virtual Reality to Improve Real-World Tennis Forehand Volley.
In 26th ACM Symposium on Virtual Reality Software and Technology (Virtual
Event, Canada) (VRST ’20). Association for Computing Machinery, New York,
NY, USA, Article 63, 2 pages. https://doi.org/10.1145/3385956.3422128
[76] Ying Jiang, Congyi Zhang, Hongbo Fu, Alberto Cannavò, Fabrizio Lamberti,
Henry Y K Lau, and Wenping Wang. 2021. HandPainter - 3D Sketching in VR
with Hand-based Physical Proxy. In Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems. Number 412. Association for Computing
Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3411764.3445302
[77] Dacher Keltner and Jonathan Haidt. 2003. Approaching awe, a moral, spiritual,

and aesthetic emotion. Cognition and emotion 17, 2 (2003), 297–314.

[78] Konstantina Kilteni, Raphaela Groten, and Mel Slater. 2012. The sense of em-
bodiment in virtual reality. Presence: Teleoperators and Virtual Environments 21,
4 (2012), 373–387.

[79] Konstantina Kilteni, Jean-Marie Normand, Maria V. Sanchez-Vives, and Mel
Slater. 2012. Extending Body Space in Immersive Virtual Reality: A Very Long
Arm Illusion. PLOS ONE 7, 7 (July 2012), e40867. https://doi.org/10.1371/journal.
pone.0040867

[80] Jarrod Knibbe, Jonas Schjerlund, Mathias Petraeus, and Kasper Hornbæk. 2018.
The Dream is Collapsing: The Experience of Exiting VR. In Proceedings of the
2018 CHI Conference on Human Factors in Computing Systems (Montreal QC,
Canada) (CHI’18). ACM, New York, NY, USA, Article 483, 13 pages. https:
//doi.org/10.1145/3173574.3174057

[81] Luv Kohli. 2010. Redirected touching: Warping space to remap passive haptics.

In 2010 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, 129–130.

[82] Elena Kokkinara, Mel Slater, and Joan López-Moliner. 2015. The effects of
visuomotor calibration to the perceived space and body, through embodiment
in immersive virtual reality. ACM Transactions on Applied Perception (TAP) 13, 1
(2015), 1–22.

[83] R. Kopper, Tao Ni, D.A. Bowman, and M. Pinho. 2006. Design and Evaluation
of Navigation Techniques for Multiscale Virtual Environments. In IEEE Virtual
Reality Conference (VR 2006). 175–182. https://doi.org/10.1109/VR.2006.47 ISSN:
2375-5334.

[84] Jaron Lanier. 2017. Dawn of the new everything: Encounters with reality and

virtual reality. Henry Holt and Company.

[85] John N Latta and David J Oberg. 1994. A conceptual virtual reality model. IEEE

Computer Graphics and Applications 14, 1 (1994), 23–29.

[86] Jong-In Lee, Paul Asente, Byungmoon Kim, Yeojin Kim, and Wolfgang Stuer-
zlinger. 2020. Evaluating Automatic Parameter Control Methods for Locomotion
in Multiscale Virtual Environments. In 26th ACM Symposium on Virtual Reality
Software and Technology (VRST ’20). Association for Computing Machinery,
New York, NY, USA, 1–10. https://doi.org/10.1145/3385956.3418961

[87] Kwan Min Lee. 2004. Presence, explicated. Communication theory 14, 1 (2004),

27–50.

[88] Sang Wan Lee, Shinsuke Shimojo, and John P O’Doherty. 2014. Neural compu-
tations underlying arbitration between model-based and model-free learning.
Neuron 81, 3 (2014), 687–699.

[89] Nianlong Li, Zhengquan Zhang, Can Liu, Zengyao Yang, Yinan Fu, Feng Tian,
Teng Han, and Mingming Fan. 2021. vMirror: Enhancing the Interaction with
Occluded or Distant Objects in VR with Virtual Mirrors.
In Proceedings of
the 2021 CHI Conference on Human Factors in Computing Systems. Number
132. Association for Computing Machinery, New York, NY, USA, 1–11. https:
//doi.org/10.1145/3411764.3445537

[90] Klemen Lilija, Henning Pohl, and Kasper Hornbæk. 2020. Who Put That There?
Temporal Navigation of Spatial Recordings by Direct Manipulation. In Pro-
ceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
1–11.

[91] Jean-Baptiste Louvet and Cédric Fleury. 2016. Combining bimanual interaction
and teleportation for 3D manipulation on multi-touch wall-sized displays. In
Proceedings of the 22nd ACM Conference on Virtual Reality Software and Tech-
nology (VRST ’16). Association for Computing Machinery, New York, NY, USA,
283–292. https://doi.org/10.1145/2993369.2993390

[92] Yiqin Lu, Chun Yu, and Yuanchun Shi. 2020. Investigating Bubble Mechanism
for Ray-Casting to Improve 3D Target Acquisition in Virtual Reality. In 2020
IEEE Conference on Virtual Reality and 3D User Interfaces (VR). 35–43. https:
//doi.org/10.1109/VR46266.2020.00021 ISSN: 2642-5254.

[93] Diako Mardanbegi, Benedikt Mayer, Ken Pfeuffer, Shahram Jalaliniya, Hans
Gellersen, and Alexander Perzl. 2019. EyeSeeThrough: Unifying Tool Selection
and Application in Virtual Environments. In 2019 IEEE Conference on Virtual
Reality and 3D User Interfaces (VR). 474–483. https://doi.org/10.1109/VR.2019.
8797988 ISSN: 2642-5254.

[94] Angela Marotta, Michele Tinazzi, Clelia Cavedini, Massimiliano Zampini, and
Mirta Fiorio. 2016. Individual differences in the rubber hand illusion are related
to sensory suggestibility. PloS one 11, 12 (2016), e0168489.

[95] Marie Martel, Lucilla Cardinali, Alice C Roy, and Alessandro Farnè. 2016. Tool-
use: An open window into body representation and its plasticity. Cognitive
neuropsychology 33, 1-2 (2016), 82–101.

[96] Jess McIntosh, Hubert Dariusz Zajac, Andreea Nicoleta Stefan, Joanna
Bergström, and Kasper Hornbæk. 2020. Iteratively Adapting Avatars using Task-
Integrated Optimisation. In Proceedings of the 33rd Annual ACM Symposium on
User Interface Software and Technology. Association for Computing Machinery,
New York, NY, USA, 709–721. https://doi.org/10.1145/3379337.3415832
[97] Vincent Meyrueis, Alexis Paljic, and Philippe Fuchs. 2009. D3: an immersive
aided design deformation method. In Proceedings of the 16th ACM Symposium on
Virtual Reality Software and Technology (VRST ’09). Association for Computing
Machinery, New York, NY, USA, 179–182. https://doi.org/10.1145/1643928.
1643968

[98] Paul Milgram, Haruo Takemura, Akira Utsumi, and Fumio Kishino. 1995. Aug-
mented reality: A class of displays on the reality-virtuality continuum. In Tele-
manipulator and telepresence technologies, Vol. 2351. International Society for
Optics and Photonics, 282–292.

[99] Seyedkoosha Mirhosseini, Parmida Ghahremani, Sushant Ojar, Joseph Marino,
and Arie Kaufrnan. 2019. Exploration of Large Omnidirectional Images in
Immersive Environments. In 2019 IEEE Conference on Virtual Reality and 3D
User Interfaces (VR). 413–422. https://doi.org/10.1109/VR.2019.8797777 ISSN:
2642-5254.

[100] Seyedkoosha Mirhosseini, Ievgeniia Gutenko, Sushant Ojal, Joseph Marino, and
Arie E. Kaufman. 2017. Automatic speed and direction control along constrained
navigation paths. In 2017 IEEE Virtual Reality (VR). 29–36. https://doi.org/10.
1109/VR.2017.7892228 ISSN: 2375-5334.

[101] Mohammadreza Mirzaei, Peter Kán, and Hannes Kaufmann. 2021. Head Up
Visualization of Spatial Sound Sources in Virtual Reality for Deaf and Hard-of-
Hearing People. In 2021 IEEE Virtual Reality and 3D User Interfaces (VR). 582–587.
https://doi.org/10.1109/VR50410.2021.00083 ISSN: 2642-5254.

[102] David Moher, Alessandro Liberati, Jennifer Tetzlaff, Douglas G Altman, and
Prisma Group. 2009. Preferred reporting items for systematic reviews and
meta-analyses: the PRISMA statement. PLoS medicine 6, 7 (2009), e1000097.

[103] Eva Monclús, José Díaz, Isabel Navazo, and Pere-Pau Vázquez. 2009. The virtual
magic lantern: an interaction metaphor for enhanced medical data inspection.
In Proceedings of the 16th ACM Symposium on Virtual Reality Software and
Technology (VRST ’09). Association for Computing Machinery, New York, NY,
USA, 119–122. https://doi.org/10.1145/1643928.1643955

[104] Roberto A. Montano-Murillo, Patricia I. Cornelio-Martinez, Sriram Subramanian,
and Diego Martinez-Plasencia. 2019. Drift-Correction Techniques for Scale-
Adaptive VR Navigation. In Proceedings of the 32nd Annual ACM Symposium on
User Interface Software and Technology (UIST ’19). Association for Computing
Machinery, New York, NY, USA, 1123–1135. https://doi.org/10.1145/3332165.
3347914

[105] Roberto A. Montano Murillo, Elia Gatti, Miguel Oliver Segovia, Marianna
Obrist, Jose P. Molina Masso, and Diego Martinez Plasencia. 2017. Navi-
Fields: Relevance fields for adaptive VR navigation. In Proceedings of the 30th
Annual ACM Symposium on User Interface Software and Technology (UIST
’17). Association for Computing Machinery, New York, NY, USA, 747–758.
https://doi.org/10.1145/3126594.3126645

[106] Roberto A. Montano-Murillo, Cuong Nguyen, Rubaiat Habib Kazi, Sriram Subra-
manian, Stephen DiVerdi, and Diego Martinez-Plasencia. 2020. Slicing-Volume:
Hybrid 3D/2D Multi-target Selection Technique for Dense Virtual Environments.
In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). 53–62.
https://doi.org/10.1109/VR46266.2020.00023 ISSN: 2642-5254.

[107] Daniel R Montello and Corina Sas. 2006. Human factors of wayfinding in

navigation. (2006).

[108] Annette Mossel and Christian Koessler. 2016. Large scale cut plane: an occlusion
management technique for immersive dense 3D reconstructions. In Proceedings
of the 22nd ACM Conference on Virtual Reality Software and Technology (VRST
’16). Association for Computing Machinery, New York, NY, USA, 201–210. https:
//doi.org/10.1145/2993369.2993384

[109] Ahmed E Mostafa, Ehud Sharlin, and Mario Costa Sousa. 2014. Poster: Superhu-
mans: A 3DUI design metaphor. In 2014 IEEE Symposium on 3D User Interfaces
(3DUI). IEEE, 143–144.

[110] Martez Mott, Edward Cutrell, Mar Gonzalez Franco, Christian Holz, Eyal Ofek,
Richard Stoakley, and Meredith Ringel Morris. 2019. Accessible by design: An
opportunity for virtual reality. In 2019 IEEE International Symposium on Mixed
and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, 451–454.

[111] Fumitaka Nakaizumi, Haruo Noma, Kenichi Hosaka, and Yasuyuki Yanagida.
2006. SpotScents: A novel method of natural scent delivery using multiple scent
projectors. In IEEE Virtual Reality Conference (VR 2006). IEEE, 207–214.
[112] Jung Who Nam, Krista McCullough, Joshua Tveite, Maria Molina Espinosa,
Charles H. Perry, Barry T. Wilson, and Daniel F. Keefe. 2019. Worlds-in-Wedges:
Combining Worlds-in-Miniature and Portals to Support Comparative Immersive
Visualization of Forestry Data. In 2019 IEEE Conference on Virtual Reality and
3D User Interfaces (VR). 747–755. https://doi.org/10.1109/VR.2019.8797871 ISSN:
2642-5254.

[113] Jakob Nielsen. 1994. Usability engineering. Elsevier.

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

[114] Don Norman. 2013. The design of everyday things: Revised and expanded edition.

Basic books.

[115] Donald A Norman and Stephen W Draper. 1986. User centered system design;

new perspectives on human-computer interaction. L. Erlbaum Associates Inc.

[116] Benjamin Nuernberger, Matthew Turk, and Tobias Höllerer. 2017. Evaluating
snapping-to-photos virtual travel interfaces for 3D reconstructed visual reality.
In Proceedings of the 23rd ACM Symposium on Virtual Reality Software and
Technology (VRST ’17). Association for Computing Machinery, New York, NY,
USA, 1–11. https://doi.org/10.1145/3139131.3139138

[117] David J Ostry and Paul L Gribble. 2016. Sensory plasticity in human motor

learning. Trends in neurosciences 39, 2 (2016), 114–123.

[118] Soonchan Park, Seokyeol Kim, and Jinah Park. 2012. Select ahead: efficient
object selection technique using the tendency of recent cursor movements. In
Proceedings of the 10th asia pacific conference on Computer human interaction
(APCHI ’12). Association for Computing Machinery, New York, NY, USA, 51–58.
https://doi.org/10.1145/2350046.2350060

[119] Thomas D Parsons and Albert A Rizzo. 2008. Affective outcomes of virtual
reality exposure therapy for anxiety and specific phobias: A meta-analysis.
Journal of behavior therapy and experimental psychiatry 39, 3 (2008), 250–261.
[120] J.S. Pierce and R. Pausch. 2004. Navigation with place representations and
visible landmarks. In IEEE Virtual Reality 2004. 173–288. https://doi.org/10.1109/
VR.2004.1310071 ISSN: 1087-8270.

[121] Henning Pohl, Klemen Lilija, Jess McIntosh, and Kasper Hornbæk. 2021. Poros:
Configurable Proxies for Distant Interactions in VR.
In Proceedings of the
2021 CHI Conference on Human Factors in Computing Systems. Number 532.
Association for Computing Machinery, New York, NY, USA, 1–12. https://doi.
org/10.1145/3411764.3445685

[122] Ivan Poupyrev, Mark Billinghurst, Suzanne Weghorst, and Tadao Ichikawa. 1996.
The go-go interaction technique: non-linear mapping for direct manipulation in
VR. In Proceedings of the 9th annual ACM symposium on User interface software
and technology (UIST ’96). Association for Computing Machinery, New York,
NY, USA, 79–80. https://doi.org/10.1145/237091.237102

[123] Arnaud Prouzeau, Maxime Cordeil, Clement Robin, Barrett Ens, Bruce H.
Thomas, and Tim Dwyer. 2019. Scaptics and Highlight-Planes: Immersive
Interaction Techniques for Finding Occluded Features in 3D Scatterplots.
In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. Association for Computing Machinery, New York, NY, USA, 1–12.
https://doi.org/10.1145/3290605.3300555

[124] Jens Rasmussen. 1983. Skills, rules, and knowledge; signals, signs, and symbols,
IEEE transactions on

and other distinctions in human performance models.
systems, man, and cybernetics 3 (1983), 257–266.

[125] Michael Rietzler, Martin Deubzer, Thomas Dreja, and Enrico Rukzio. 2020.
Telewalk: Towards Free and Endless Walking in Room-Scale Virtual Real-
ity. In Proceedings of the 2020 CHI Conference on Human Factors in Comput-
ing Systems. Association for Computing Machinery, New York, NY, USA, 1–9.
https://doi.org/10.1145/3313831.3376821

[126] Michael Rietzler, Florian Geiselhart, and Enrico Rukzio. 2017. The matrix has
you: realizing slow motion in full-body virtual reality. In Proceedings of the
23rd ACM Symposium on Virtual Reality Software and Technology (VRST ’17).
Association for Computing Machinery, New York, NY, USA, 1–10. https://doi.
org/10.1145/3139131.3139145

[127] Michael Rietzler, Florian Geiselhart, and Enrico Rukzio. 2017. The Matrix Has
You: Realizing Slow Motion in Full-Body Virtual Reality. In Proceedings of the
23rd ACM Symposium on Virtual Reality Software and Technology (Gothenburg,
Sweden) (VRST ’17). Association for Computing Machinery, New York, NY, USA,
Article 2, 10 pages. https://doi.org/10.1145/3139131.3139145

[128] Yves Rossetti, Gilles Rode, Laure Pisella, Alessandro Farné, Ling Li, Dominique
Boisson, and Marie-Thérèse Perenin. 1998. Prism adaptation to a rightward
optical deviation rehabilitates left hemispatial neglect. Nature 395, 6698 (1998),
166–169.

[129] Daniel Roth, Gary Bente, Peter Kullmann, David Mal, Chris Felix Purps, Kai
Vogeley, and Marc Erich Latoschik. 2019. Technologies for Social Augmentations
in User-Embodied Virtual Reality. In 25th ACM Symposium on Virtual Reality
Software and Technology (VRST ’19). Association for Computing Machinery,
New York, NY, USA, 1–12. https://doi.org/10.1145/3359996.3364269

[130] Daniel Roth, Constantin Klelnbeck, Tobias Feigl, Christopher Mutschler, and
Marc Erich Latoschik. 2018. Beyond Replication: Augmenting Social Behaviors
in Multi-User Virtual Realities. In 2018 IEEE Conference on Virtual Reality and
3D User Interfaces (VR). 215–222. https://doi.org/10.1109/VR.2018.8447550
[131] Barbara Olasov Rothbaum, Larry Hodges, and Rob Kooper. 1997. Virtual reality
exposure therapy. Journal of Psychotherapy Practice & Research (1997).
[132] Barbara Olasov Rothbaum, Anna Marie Ruef, Brett T Litz, Hyemee Han, and
Larry Hodges. 2004. Virtual reality exposure therapy of combat-related PTSD:
A case study using psychophysiological indicators of outcome. Advances in
the treatment of posttraumatic stress disorder: Cognitive-behavioral perspectives
(2004), 93–112.

[133] Shadan Sadeghian and Marc Hassenzahl. 2021. From Limitations to “Superpow-
ers”: A Design Approach to Better Focus on the Possibilities of Virtual Reality

to Augment Human Capabilities. In Designing Interactive Systems Conference
2021. Association for Computing Machinery, New York, NY, USA, 180–189.
https://doi.org/10.1145/3461778.3462111

[134] Jeffrey A Saunders and David C Knill. 2005. Humans use continuous visual
feedback from the hand to control both the direction and distance of pointing
movements. Experimental brain research 162, 4 (2005), 458–473.

[135] Jonas Schjerlund, Kasper Hornbæk, and Joanna Bergström. 2021. Ninja Hands:
Using Many Hands to Improve Target Selection in VR. In Proceedings of the
2021 CHI Conference on Human Factors in Computing Systems. Number 130.
Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.
org/10.1145/3411764.3445759

[136] Steven Schkolne, Michael Pruett, and Peter Schröder. 2001. Surface drawing:
creating organic 3D shapes with the hand and tangible tools. In Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’01).
Association for Computing Machinery, New York, NY, USA, 261–268. https:
//doi.org/10.1145/365024.365114

[137] Gregor Schöner. 1994. Dynamic theory of action-perception patterns: The time-

before-contact paradigm. Human Movement Science 13, 3-4 (1994), 415–439.

[138] Ephraim Schott, Alexander Kulik, and Bernd Froehlich. 2020. Virtual Projection
Planes for the Visual Comparison of Photogrammetric 3D Reconstructions
with Photo Footage. In 26th ACM Symposium on Virtual Reality Software and
Technology (VRST ’20). Association for Computing Machinery, New York, NY,
USA, 1–9. https://doi.org/10.1145/3385956.3418956

[139] Eliane Schreuder, Jan van Erp, Alexander Toet, and Victor L Kallen. 2016. Emo-
tional responses to multisensory environmental stimuli: A conceptual frame-
work and literature review. Sage Open 6, 1 (2016), 2158244016630591.

[140] Stephen H Scott. 2004. Optimal feedback control and the neural basis of voli-
tional motor control. Nature Reviews Neuroscience 5, 7 (2004), 532–545.
[141] S. Seinfeld, J. Arroyo-Palacios, G. Iruretagoyena, R. Hortensius, L. E. Zapata,
D. Borland, B. de Gelder, M. Slater, and M. V. Sanchez-Vives. 2018. Offenders
become the victim in virtual reality: impact of changing perspective in domestic
violence. Scientific Reports 8, 1 (Feb. 2018), 2692. https://doi.org/10.1038/s41598-
018-19987-7

[142] Sofia Seinfeld, Tiare Feuchtner, Antonella Maselli, and Jörg Müller. 2021. User
representations in human-computer interaction. Human–Computer Interaction
36, 5-6 (2021), 400–438.

[143] Yoshikazu Seki and Tetsuji Sato. 2010. A training system of orientation and
mobility for blind people using acoustic virtual reality. IEEE Transactions on
neural systems and rehabilitation engineering 19, 1 (2010), 95–104.

[144] Ben Shneiderman. 2003. Why not make interfaces better than 3D reality? IEEE

Computer Graphics and Applications 23, 6 (2003), 12–15.

[145] Ludwig Sidenmark, Christopher Clarke, Xuesong Zhang, Jenny Phu, and Hans
Gellersen. 2020. Outline Pursuits: Gaze-assisted Selection of Occluded Objects
in Virtual Reality. In Proceedings of the 2020 CHI Conference on Human Factors
in Computing Systems. Association for Computing Machinery, New York, NY,
USA, 1–13. https://doi.org/10.1145/3313831.3376438

[146] Ludwig Sidenmark and Hans Gellersen. 2019. Eye&Head: Synergetic Eye and
Head Movement for Gaze Pointing and Selection. In Proceedings of the 32nd
Annual ACM Symposium on User Interface Software and Technology (UIST ’19).
Association for Computing Machinery, New York, NY, USA, 1161–1174. https:
//doi.org/10.1145/3332165.3347921

[147] Ludwig Sidenmark, Dominic Potts, Bill Bapisch, and Hans Gellersen. 2021. Radi-
Eye: Hands-Free Radial Interfaces for 3D Interaction using Gaze-Activated
Head-Crossing. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems. Number 740. Association for Computing Machinery, New
York, NY, USA, 1–11. https://doi.org/10.1145/3411764.3445697

[148] Andreas Simon and Christian Stern. 2007. Active guideline: spatiotemporal
history as a motion technique and navigation aid for virtual environments. In
Proceedings of the 2007 ACM symposium on Virtual reality software and technology
(VRST ’07). Association for Computing Machinery, New York, NY, USA, 199–202.
https://doi.org/10.1145/1315184.1315222

[149] Andreas Simon and Christian Stern. 2007. Active Guideline: Spatiotemporal
History as a Motion Technique and Navigation Aid for Virtual Environments.
In Proceedings of the 2007 ACM Symposium on Virtual Reality Software and
Technology (Newport Beach, California) (VRST ’07). Association for Computing
Machinery, New York, NY, USA, 199–202. https://doi.org/10.1145/1315184.
1315222

[150] Alexa F Siu, Mike Sinclair, Robert Kovacs, Eyal Ofek, Christian Holz, and Edward
Cutrell. 2020. Virtual reality without vision: A haptic and auditory white cane
to navigate complex virtual worlds. In Proceedings of the 2020 CHI conference on
human factors in computing systems. 1–13.

[151] Dana Slambekova, Reynold Bailey, and Joe Geigel. 2012. Gaze and gesture
based object manipulation in virtual worlds. In Proceedings of the 18th ACM
symposium on Virtual reality software and technology (VRST ’12). Association
for Computing Machinery, New York, NY, USA, 203–204. https://doi.org/10.
1145/2407336.2407380

[152] Mel Slater. 2009. Place illusion and plausibility can lead to realistic behaviour in
immersive virtual environments. Philosophical Transactions of the Royal Society

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

Abtahi et al.

B: Biological Sciences 364, 1535 (2009), 3549–3557.

[153] Mel Slater, Cristina Gonzalez-Liencres, Patrick Haggard, Charlotte Vinkers, Re-
becca Gregory-Clarke, Steve Jelley, Zillah Watson, Graham Breen, Raz Schwarz,
William Steptoe, et al. 2020. The ethics of realism in virtual and augmented
reality. Frontiers in Virtual Reality 1 (2020), 1.

[154] Mel Slater, Vasilis Linakis, Martin Usoh, and Rob Kooper. 1996. Immersion,
presence and performance in virtual environments: An experiment with tri-
dimensional chess. In Proceedings of the ACM symposium on virtual reality
software and technology. ACM, 163–172.

[155] Mel Slater and Martin Usoh. 1994. Body centred interaction in immersive virtual
environments. Artificial life and virtual reality 1, 1994 (1994), 125–148.
[156] Mel Slater and Sylvia Wilbur. 1997. A framework for immersive virtual envi-
ronments (FIVE): Speculations on the role of presence in virtual environments.
Presence: Teleoperators & Virtual Environments 6, 6 (1997), 603–616.

[157] Samuel J Sober and Philip N Sabes. 2005. Flexible strategies for sensory integra-

tion during motor planning. Nature neuroscience 8, 4 (2005), 490–497.
[158] Chang Geun Song, No Jun Kwak, and Dong Hyun Jeong. 2000. Developing
an efficient technique of selection and manipulation in immersive V.E.. In
Proceedings of the ACM symposium on Virtual reality software and technology
(VRST ’00). Association for Computing Machinery, New York, NY, USA, 142–146.
https://doi.org/10.1145/502390.502417

[159] D. Song and M. Norman. 1993. Nonlinear interactive motion control tech-
niques for virtual space navigation. In Proceedings of IEEE Virtual Reality Annual
International Symposium. 111–117. https://doi.org/10.1109/VRAIS.1993.380790
[160] Peng Song, Wooi Boon Goh, William Hutama, Chi-Wing Fu, and Xiaopei Liu.
2012. A handle bar metaphor for virtual object manipulation with mid-air inter-
action. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems (CHI ’12). Association for Computing Machinery, New York, NY, USA,
1297–1306. https://doi.org/10.1145/2207676.2208585

[161] Misha Sra, Aske Mottelson, and Pattie Maes. 2018. Your Place and Mine: De-
signing a Shared VR Experience for Remotely Located Users. In Proceedings
of the 2018 Designing Interactive Systems Conference (DIS ’18). Association for
Computing Machinery, New York, NY, USA, 85–97. https://doi.org/10.1145/
3196709.3196788

[162] Misha Sra, Xuhai Xu, and Pattie Maes. 2018. BreathVR: Leveraging Breathing
as a Directly Controlled Interface for Virtual Reality Games. In Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems. Association
for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/
3173574.3173914

[163] Frank Steinicke, Gerd Bruder, Jason Jerald, Harald Frenz, and Markus Lappe.
2009. Estimation of detection thresholds for redirected walking techniques.
IEEE transactions on visualization and computer graphics 16, 1 (2009), 17–27.

[164] Rasmus Stenholt. 2012. Efficient selection of multiple objects on a large scale. In
Proceedings of the 18th ACM symposium on Virtual reality software and technology
(VRST ’12). Association for Computing Machinery, New York, NY, USA, 105–112.
https://doi.org/10.1145/2407336.2407357

[165] Ekaterina R Stepanova, Denise Quesnel, and Bernhard E Riecke. 2019. Space-A
Virtual Frontier: How to Design and Evaluate a Virtual Reality Experience of
the Overview Effect. Front. Digital Humanities 2019 (2019).

[166] William Steptoe, Anthony Steed, and Mel Slater. 2013. Human tails: ownership
and control of extended humanoid avatars. IEEE transactions on visualization
and computer graphics 19, 4 (2013), 583–590.

[167] Stanislav L. Stoev and Dieter Schmalstieg. 2002. Application and taxonomy of
through-the-lens techniques. In Proceedings of the ACM symposium on Virtual re-
ality software and technology (VRST ’02). Association for Computing Machinery,
New York, NY, USA, 57–64. https://doi.org/10.1145/585740.585751

[168] W. Stuerzlinger and G. Smith. 2002. Efficient manipulation of object groups in
virtual environments. In Proceedings IEEE Virtual Reality 2002. 251–258. https:
//doi.org/10.1109/VR.2002.996529 ISSN: 1087-8270.

[169] D. J. Sturman, D. Zeltzer, and S. Pieper. 1989. Hands-on interaction with virtual
environments. In Proceedings of the 2nd annual ACM SIGGRAPH symposium on
User interface software and technology (UIST ’89). Association for Computing
Machinery, New York, NY, USA, 19–24. https://doi.org/10.1145/73660.73663

[170] Evan A. Suma, Zachary Lipps, Samantha Finkelstein, David M. Krum, and
Impossible Spaces: Maximizing Natural Walking in Vir-
Mark Bolas. 2012.
tual Environments with Self-Overlapping Architecture.
IEEE Transactions
on Visualization and Computer Graphics 18, 4 (April 2012), 555–564. https:
//doi.org/10.1109/TVCG.2012.47

[171] Ivan E Sutherland. 1965. The ultimate display. Multimedia: From Wagner to

virtual reality (1965), 506–508.

[172] Durk Talsma. 2015. Predictive coding and multisensory integration: an atten-
tional account of the multisensory mind. Frontiers in Integrative Neuroscience 9
(2015), 19.

[173] Vildan Tanriverdi and Robert J. K. Jacob. 2000. Interacting with eye movements
in virtual environments. In Proceedings of the SIGCHI conference on Human
Factors in Computing Systems (CHI ’00). Association for Computing Machinery,
New York, NY, USA, 265–272. https://doi.org/10.1145/332040.332443

[174] Shan-Yuan Teng, Da-Yuan Huang, Chi Wang, Jun Gong, Teddy Seyed, Xing-Dong
Yang, and Bing-Yu Chen. 2019. Aarnio: Passive Kinesthetic Force Output for
Foreground Interactions on an Interactive Chair. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems. Association for Computing
Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300902
[175] Léo Terziman, Maud Marchal, Mathieu Emily, Franck Multon, Bruno Arnaldi,
and Anatole Lécuyer. 2010. Shake-your-head: revisiting walking-in-place for
desktop virtual reality. In Proceedings of the 17th ACM Symposium on Virtual Re-
ality Software and Technology (VRST ’10). Association for Computing Machinery,
New York, NY, USA, 27–34. https://doi.org/10.1145/1889863.1889867

[176] Richard A Thurman and Joseph S Mattoon. 1994. Virtual Reality: Toward Fun-
damental Improvements in Simulation-Based Training. Educational technology
34, 8 (1994), 56–64.

[177] Gaetano Tieri, Emmanuele Tidoni, Enea Francesco Pavone, and Salvatore Maria
Aglioti. 2015. Body visual discontinuity affects feeling of ownership and skin
conductance responses. Scientific reports 5, 1 (2015), 1–8.

[178] Emanuel Todorov. 2004. Optimality principles in sensorimotor control. Nature

neuroscience 7, 9 (2004), 907–915.

[179] Sam Tregillus and Eelke Folmer. 2016. VR-STEP: Walking-in-Place using Inertial
Sensing for Hands Free Navigation in Mobile VR Environments. In Proceedings
of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16).
Association for Computing Machinery, New York, NY, USA, 1250–1255. https:
//doi.org/10.1145/2858036.2858084

[180] Wen-Jie Tseng, Li-Yang Wang, and Liwei Chan. 2019. FaceWidgets: Exploring
Tangible Interaction on Face with Head-Mounted Displays. In Proceedings of
the 32nd Annual ACM Symposium on User Interface Software and Technology
(UIST ’19). Association for Computing Machinery, New York, NY, USA, 417–427.
https://doi.org/10.1145/3332165.3347946

[181] Huawei Tu, Susu Huang, Jiabin Yuan, Xiangshi Ren, and Feng Tian. 2019.
Crossing-Based Selection with Virtual Reality Head-Mounted Displays.
In
Proceedings of the 2019 CHI Conference on Human Factors in Computing Sys-
tems. Association for Computing Machinery, New York, NY, USA, 1–14. https:
//doi.org/10.1145/3290605.3300848

[182] Yuki Ueyama. 2014. Mini-max feedback control as a computational theory of
sensorimotor control in the presence of structural uncertainty. Frontiers in
computational neuroscience 8 (2014), 119.

[183] Robert J van Beers, Pierre Baraduc, and Daniel M Wolpert. 2002. Role of uncer-
tainty in sensorimotor control. Philosophical Transactions of the Royal Society of
London. Series B: Biological Sciences 357, 1424 (2002), 1137–1145.

[184] Björn Van Der Hoort, Arvid Guterstam, and H Henrik Ehrsson. 2011. Being
Barbie: the size of one’s own body determines the perceived size of the world.
PloS one 6, 5 (2011), e20195.

[185] Manuel Veit, Antonio Capobianco, and Dominique Bechmann. 2010. Dynamic
decomposition and integration of degrees of freedom for 3-D positioning. In
Proceedings of the 17th ACM Symposium on Virtual Reality Software and Tech-
nology (VRST ’10). Association for Computing Machinery, New York, NY, USA,
131–134. https://doi.org/10.1145/1889863.1889891

[186] Manuel Veit, Antonio Capobianco, and Dominique Bechmann. 2012. CrOS: a
touch screen interaction technique for cursor manipulation on 2-manifolds. In
Proceedings of the 18th ACM symposium on Virtual reality software and technology
(VRST ’12). Association for Computing Machinery, New York, NY, USA, 97–100.
https://doi.org/10.1145/2407336.2407355

[187] Julius von Willich, Martin Schmitz, Florian Müller, Daniel Schmitt, and Max
Mühlhäuser. 2020. Podoportation: Foot-Based Locomotion in Virtual Reality.
In Proceedings of the 2020 CHI Conference on Human Factors in Computing
Systems. Association for Computing Machinery, New York, NY, USA, 1–14.
https://doi.org/10.1145/3313831.3376626

[188] Chiu-Hsuan Wang, Chia-En Tsai, Seraphina Yong, and Liwei Chan. 2020. Slice
of Light: Transparent and Integrative Transition Among Realities in a Multi-
HMD-User Environment. In Proceedings of the 33rd Annual ACM Symposium on
User Interface Software and Technology. Association for Computing Machinery,
New York, NY, USA, 805–817. https://doi.org/10.1145/3379337.3415868
[189] Jia Wang and Robert W. Lindeman. 2015. Object impersonation: Towards
effective interaction in tablet- and HMD-based hybrid virtual environments. In
2015 IEEE Virtual Reality (VR). 111–118. https://doi.org/10.1109/VR.2015.7223332
ISSN: 2375-5334.

[190] Miao Wang, Zi-Ming Ye, Jin-Chuan Shi, and Yang-Liang Yang. 2021. Scene-
Context-Aware Indoor Object Selection and Movement in VR. In 2021 IEEE
Virtual Reality and 3D User Interfaces (VR). 235–244. https://doi.org/10.1109/
VR50410.2021.00045 ISSN: 2642-5254.

[191] Z. Wartell, W. Ribarsky, and L. Hodges. 1999. Third-person navigation of whole-
planet terrain in a head-tracked stereoscopic environment. In Proceedings IEEE
Virtual Reality (Cat. No. 99CB36316). 141–148. https://doi.org/10.1109/VR.1999.
756945 ISSN: 1087-8270.

[192] Tim Weissker, Alexander Kulik, and Bernd Froehlich. 2019. Multi-Ray Jumping:
Comprehensible Group Navigation for Collocated Users in Immersive Virtual
Reality. In 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR).
136–144. https://doi.org/10.1109/VR.2019.8797807 ISSN: 2642-5254.

Beyond Being Real

CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA

[193] Jeremy D. Wendt, Mary C. Whitton, and Frederick P. Brooks. 2010. GUD
WIP: Gait-Understanding-Driven Walking-In-Place. In 2010 IEEE Virtual Reality
Conference (VR). 51–58. https://doi.org/10.1109/VR.2010.5444812 ISSN: 2375-
5334.

[194] Frank White. 1998. The overview effect: Space exploration and human evolution.

AIAA.

[195] Graham Wilson, Mark McGill, Matthew Jamieson, Julie R. Williamson, and
Stephen A. Brewster. 2018. Object Manipulation in Virtual Reality Under In-
creasing Levels of Translational Gain. In Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems. Association for Computing Machinery,
New York, NY, USA, 1–13. https://doi.org/10.1145/3173574.3173673

[196] Terry Winograd, Fernando Flores, and Fernando F Flores. 1986. Understanding
computers and cognition: A new foundation for design. Intellect Books.
[197] Bob G Witmer and Michael J Singer. 1998. Measuring presence in virtual
environments: A presence questionnaire. Presence 7, 3 (1998), 225–240.
[198] Jacob O Wobbrock, Shaun K Kane, Krzysztof Z Gajos, Susumu Harada, and Jon
Froehlich. 2011. Ability-based design: Concept, principles and examples. ACM
Transactions on Accessible Computing (TACCESS) 3, 3 (2011), 1–27.

[199] Dennis Wolf, Katja Rogers, Christoph Kunder, and Enrico Rukzio. 2020. JumpVR:
Jump-Based Locomotion Augmentation for Virtual Reality. In Proceedings of
the 2020 CHI Conference on Human Factors in Computing Systems. Association
for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/
3313831.3376243

[200] Daniel M Wolpert and Zoubin Ghahramani. 2000. Computational principles of
movement neuroscience. Nature neuroscience 3, 11 (2000), 1212–1217.

[201] Andrea Stevenson Won, Jeremy Bailenson, Jimmy Lee, and Jaron Lanier. 2015.
Homuncular flexibility in virtual reality. Journal of Computer-Mediated Com-
munication 20, 3 (2015), 241–259.

[202] Andrea Stevenson Won, Jeremy N Bailenson, and Jaron Lanier. 2015. Homuncu-
lar flexibility: the human ability to inhabit nonhuman avatars. Emerging Trends
in the Social and Behavioral Sciences: An Interdisciplinary, Searchable, and Link-
able Resource (2015), 1–16.

[203] Jonathan Wonner, Jérôme Grosjean, Antonio Capobianco, and Dominique Bech-
mann. 2012. Starfish: a selection technique for dense virtual environments. In
Proceedings of the 18th ACM symposium on Virtual reality software and tech-
nology (VRST ’12). Association for Computing Machinery, New York, NY, USA,
101–104. https://doi.org/10.1145/2407336.2407356

[204] Haijun Xia, Sebastian Herscher, Ken Perlin, and Daniel Wigdor. 2018. Space-
time: Enabling Fluid Individual and Collaborative Editing in Virtual Reality. In

Proceedings of the 31st Annual ACM Symposium on User Interface Software and
Technology (UIST ’18). Association for Computing Machinery, New York, NY,
USA, 853–866. https://doi.org/10.1145/3242587.3242597

[205] Xuhai Xu, Chun Yu, Anind K. Dey, and Jennifer Mankoff. 2019. Clench Interface:
Novel Biting Input Techniques. In Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems. Association for Computing Machinery,
New York, NY, USA, 1–12. https://doi.org/10.1145/3290605.3300505

[206] Difeng Yu, Xueshi Lu, Rongkai Shi, Hai-Ning Liang, Tilman Dingler, Eduardo
Velloso, and Jorge Goncalves. 2021. Gaze-Supported 3D Object Manipulation in
Virtual Reality. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems. Number 734. Association for Computing Machinery, New
York, NY, USA, 1–13. https://doi.org/10.1145/3411764.3445343

[207] Kevin Yu, Alexander Winkler, Frieder Pankratz, Marc Lazarovici, Dirk Wilhelm,
Ulrich Eck, Daniel Roth, and Nassir Navab. 2021. Magnoramas: Magnifying
Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation. In 2021
IEEE Virtual Reality and 3D User Interfaces (VR). 392–401. https://doi.org/10.
1109/VR50410.2021.00062 ISSN: 2642-5254.

[208] R.C. Zeleznik, J.J. LaViola, D. Acevedo Feliz, and D.F. Keefe. 2002. Pop through
button devices for VE navigation and interaction. In Proceedings IEEE Virtual
Reality 2002. 127–134. https://doi.org/10.1109/VR.2002.996515 ISSN: 1087-8270.
[209] Yiran Zhang, Nicolas Ladeveze, Huyen Nguyen, Cedric Fleury, and Patrick
Bourdot. 2020. Virtual Navigation considering User Workspace: Automatic and
Manual Positioning before Teleportation. In 26th ACM Symposium on Virtual Re-
ality Software and Technology (VRST ’20). Association for Computing Machinery,
New York, NY, USA, 1–11. https://doi.org/10.1145/3385956.3418949

[210] Yaying Zhang, Bernhard E. Riecke, Thecla Schiphorst, and Carman Neustaedter.
2019. Perch to Fly: Embodied Virtual Reality Flying Locomotion with a Flexible
Perching Stance. In Proceedings of the 2019 on Designing Interactive Systems
Conference (DIS ’19). Association for Computing Machinery, New York, NY,
USA, 253–264. https://doi.org/10.1145/3322276.3322357

[211] Yuhang Zhao, Cynthia L Bennett, Hrvoje Benko, Edward Cutrell, Christian Holz,
Meredith Ringel Morris, and Mike Sinclair. 2018. Enabling people with visual
impairments to navigate virtual reality with a haptic and auditory cane simula-
tion. In Proceedings of the 2018 CHI conference on human factors in computing
systems. 1–14.

[212] Yiwei Zhao and Sean Follmer. 2018. A Functional Optimization Based Approach
for Continuous 3D Retargeted Touch of Arbitrary, Complex Boundaries in
Haptic Virtual Reality. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems. ACM, 544.

