A Stochastic PCA and SVD Algorithm
with an Exponential Convergence Rate

Ohad Shamir
Weizmann Institute of Science
ohad.shamir@weizmann.ac.il

5
1
0
2

l
u
J

1
3

]

G
L
.
s
c
[

5
v
8
4
8
2
.
9
0
4
1
:
v
i
X
r
a

Abstract

We describe and analyze a simple algorithm for principal component analysis and singular value de-
composition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponen-
tially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or
computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a
recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly con-
vex optimization, whereas here we apply it to an inherently non-convex problem, using a very different
analysis.

1 Introduction

We consider the following fundamental matrix optimization problem: Given a matrix X ∈ Rd×n, we wish
to recover its top k left singular vectors (where k (cid:28) d) by solving

max
W ∈Rd×k:W (cid:62)W =I

1
n

(cid:107)X (cid:62)W (cid:107)2
F ,

(1)

(cid:107) · (cid:107)F being the Frobenius norm and I being the identity matrix1. A prominent application in machine
learning and statistics is Principal Component Analysis (PCA), which is one of the most common tools for
unsupervised data analysis and preprocessing: Given a data matrix X whose columns consist of n instances
in Rd, we are interested in ﬁnding a k-dimensional subspace (speciﬁed by a d × k matrix W ), on which
the projection of the data has largest possible variance. Finding this subspace has numerous uses, from
dimensionality reduction and data compression to data visualization, and the problem is extremely well-
studied.

Letting x1, . . . , xn denote the columns of X, Eq. (1) can be equivalently written as

min
W ∈Rd×k:W (cid:62)W =I

−W (cid:62)

(cid:32)

1
n

n
(cid:88)

i=1

(cid:33)

xix(cid:62)
i

W,

(2)

which reveals that the solution is also the top k eigenvectors of the covariance matrix 1
i . In this
n
paper, we will mostly focus on the simplest possible form of this problem, where k = 1, in which case the
above reduces to

i=1 xix(cid:62)

(cid:80)n

min
w:(cid:107)w(cid:107)2=1

−w(cid:62)

(cid:32)

1
n

n
(cid:88)

i=1

(cid:33)

xix(cid:62)
i

w,

(3)

1The top k right singular values can also be extracted, by considering the matrix X (cid:62) in lieu of X.

1

 
 
 
 
 
 
and our goal is to ﬁnd the top eigenvector v1. However, as discussed later, the algorithm to be presented can
be readily extended to solve Eq. (2) for k > 1.

When the data size n and the dimension d are modest, this problem can be solved exactly by a full singu-
lar value decomposition of X. However, the required runtime is O (cid:0)min{nd2, n2d}(cid:1), which is prohibitive in
large-scale applications. A common alternative is to use iterative methods such as power iterations or more
sophisticated variants [6]. If the covariance matrix has bounded spectral norm and an eigengap λ between
its ﬁrst and second eigenvalues, then these algorithms can be shown to produce a unit vector which is (cid:15)-far
from v1 (or −v1) after O
iterations (where e.g. p = 1 for power iterations). However, each
iteration involves multiplying one or more vectors by the covariance matrix 1
i . Letting ds ∈ [0, d]
n
denote the average sparsity (number of non-zero entries) in each xi, this requires O(dsn) time by passing
. When λ is small, this is equivalent to
through the entire data. Thus, the total runtime is O
many passes over the data, which can be prohibitive for large datasets.

(cid:16) dsn log(1/(cid:15))
λp

(cid:16) log(1/(cid:15))
λp

i xix(cid:62)

(cid:80)

(cid:17)

(cid:17)

An alternative to these deterministic algorithms are stochastic and incremental algorithms (e.g. [11,
14, 15] and more recently, [1, 13, 2, 7, 4]). In contrast to the algorithms above, these algorithms perform
much cheaper iterations by choosing some xi (uniformly at random or otherwise), and updating the current
iterate using only xi. In general, the runtime of each iteration is only O(ds). On the ﬂip side, due to their
stochastic and incremental nature, the convergence rate (when known) is quite slow, with the number of
required iterations scaling linearly with 1/(cid:15) and additional problem parameters. This is useful for getting a
low to medium-accuracy solution, but is prohibitive when a high-accuracy solution is required.

In this paper, we propose a new stochastic algorithm for solving Eq. (3), denoted as VR-PCA 2, which

for bounded data and under suitable assumptions, has provable runtime of

(cid:18)

(cid:18)

O

ds

n +

(cid:19)

1
λ2

log

(cid:19)(cid:19)

.

(cid:18) 1
(cid:15)

This algorithm combines the advantages of the previously discussed approaches, while avoiding their main
pitfalls: On one hand, the runtime depends only logarithmically on the accuracy (cid:15), so it is suitable to get
high-accuracy solutions; while on the other hand, the runtime scales as the sum of the data size n and
a factor involving the eigengap parameter λ, rather than their product. This means that the algorithm is
still applicable when λ is relatively small. In fact, as long as λ ≥ Ω(1/
n), this runtime bound is better
than those mentioned earlier, and equals dsn up to logarithmic factors: Proportional to the time required to
perform a single scan of the data.

√

VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction (see [9]
as well as [12, 10], and [5] in a somewhat different context). However, the setting in which we apply
this technique is quite different from previous works, which crucially relied on the strong convexity of the
optimization problem, and often assume an unconstrained domain. In contrast, our algorithm attempts to
minimize the function in Eq. (3), which is nowhere convex, let alone strongly convex (in fact, it is concave
everywhere). As a result, the analysis in previous papers is inapplicable, and we require a new and different
analysis to understand the performance of the algorithm.

2 Algorithm and Analysis

The pseudo-code of our algorithm appears as Algorithm 1 below. We refer to a single execution of the inner
loop as an iteration, and each execution of the outer loop as an epoch. Thus, the algorithm consists of several

2VR stands for “variance-reduced”.

2

epochs, each of which consists of running m iterations.

Algorithm 1 VR-PCA

Parameters: Step size η, epoch length m
Input: Data matrix X = (x1, . . . , xn); Initial unit vector ˜w0
for s = 1, 2, . . . do
i=1 xi

i ˜ws−1

(cid:0)x(cid:62)

(cid:80)n

(cid:1)

˜u = 1
n
w0 = ˜ws−1
for t = 1, 2, . . . , m do

Pick it ∈ {1, . . . , n} uniformly at random
w(cid:48)
˜ws−1
wt = 1
(cid:107)w(cid:48)

t = wt−1 + η (cid:0)xit
t(cid:107) w(cid:48)

wt−1 − x(cid:62)
it

(cid:0)x(cid:62)
it

t

end for
˜ws = wm

(cid:1) + ˜u(cid:1)

end for

To understand the structure of the algorithm, it is helpful to consider ﬁrst the well-known Oja’s algorithm
for stochastic PCA optimization [14], on which our algorithm is based. In our setting, this rule is reduced to
repeatedly sampling xit uniformly at random, and performing the update

w(cid:48)

t = wt−1 + ηtxitx(cid:62)

it wt−1 , wt =

1
(cid:107)w(cid:48)
t(cid:107)

wt.

Letting A = 1

n XX (cid:62) = 1

n

(cid:80)n

i=1 xix(cid:62)

i , this can be equivalently rewritten as

w(cid:48)

t = (I + ηtA)wt−1 + ηt

(cid:16)

xitx(cid:62)

it − A

(cid:17)

wt−1 , wt =

1
(cid:107)w(cid:48)
t(cid:107)

wt.

(4)

Thus, at each iteration, the algorithm performs a power iteration (using a shifted and scaled version of the
− A(cid:1) wt−1, and projects back to the unit sphere.
(cid:0)xitx(cid:62)
matrix A), adds a stochastic zero-mean term ηt
it
Recently, [3] gave a rigorous ﬁnite-time analysis of this algorithm, showing that if ηt = O(1/t), then under
suitable conditions, we get a convergence rate of O(1/T ) after T iterations.

The reason for the relatively slow convergence rate of this algorithm is the constant variance of the
stochastic term added in each step.
Inspired by recent variance-reduced stochastic methods for convex
optimization [9], we change the algorithm in a way which encourages the variance of the stochastic term to
decay over time. Speciﬁcally, we can rewrite the update in each iteration of our VR-PCA algorithm as

w(cid:48)

t = (I + ηA)wt−1 + η

(cid:16)

xitx(cid:62)

it − A

(cid:17)

(wt−1 − ˜ws−1) , wt =

1
(cid:107)w(cid:48)
t(cid:107)

wt,

(5)

where ˜ws−1 is the vector computed at the beginning of each epoch. Comparing Eq. (5) to Eq. (4), we
see that our algorithm also performs a type of power iteration, followed by adding a stochastic zero-mean
term. However, our algorithm picks a ﬁxed step size η, which is more aggressive that a decaying step size ηt.
Moreover, the variance of the stochastic term is no longer constant, but rather controlled by (cid:107)wt−1 − ˜ws−1(cid:107).
As we get closer to the optimal solution, we expect that both ˜ws−1 and wt−1 will be closer and closer to
each other, leading to decaying variance, and a much faster convergence rate, compared to Oja’s algorithm.

Before continuing to the algorithm’s analysis, we make two important remarks:

3

1
(cid:107)w(cid:48)

t(cid:107) w(cid:48)

Remark 1. To generalize the algorithm to ﬁnd multiple singular vectors (i.e. solve Eq. (2) for k > 1), one
t , ˜W , ˜U , and replace the normalization
option is to replace the vectors wt, w(cid:48)
t, ˜w, ˜u by d×k matrices Wt, W (cid:48)
t by an orthogonalization step3. This generalization is completely analogous to how iterative
step
algorithms such as power iterations and Oja’s algorithm are generalized to the k > 1 case, and the same
intuition discussed above still holds. This is also the option used in our experiments. Another option is to re-
cover the singular vectors one-by-one via matrix deﬂation: First recover the leading vector v1, compute its
associated eigenvalue s1, and then iteratively recover the leading eigenvector and eigenvalue of the deﬂated
matrix 1
l , which is precisely vj. This is a standard method to extend power
n
iteration algorithms to recover multiple eigenvectors, and our algorithm can be applied to solve it. Algo-
rithmically, one simply needs to replace each computation of the form xx(cid:62)w with
w.
A disadvantage of this approach is that it requires a positive eigengap between all top k singular values,
otherwise our algorithm is not guaranteed to converge.

xx(cid:62) − (cid:80)j−1

i − (cid:80)j−1

l=1 slvlv(cid:62)

l=1 vlv(cid:62)
l

i=1 xix(cid:62)

(cid:80)n

(cid:17)

(cid:16)

Remark 2. Using a straightforward implementation, the runtime of each iteration is O(d), and the total
runtime of each epoch is O(dm + dsn), where ds is the average sparsity of the data points xi. However, a
more careful implementation can improve this to O(ds(m+n)). The trick is to maintain each wt as αg+β ˜u,
plus a few additional scalars, and in each iteration perform only a sparse update of g, and updates of the
scalars, all in O(ds) amortized time. See Appendix A for more details.

A formal analysis of the algorithm appears as Thm. 1 below. See Sec. 3 for further discussion of the

choice of parameters in practice.

Theorem 1. Deﬁne A as 1
eigenvalue. Suppose that

n XX (cid:62) = 1

n

(cid:80)n

i=1 xix(cid:62)

i , and let v1 be an eigenvector corresponding to its largest

• maxi (cid:107)xi(cid:107)2 ≤ r for some r > 0.

• A has eigenvalues s1 > s2 ≥ . . . ≥ sd, where s1 − s2 = λ for some λ > 0.

• (cid:104) ˜w0, v1(cid:105) ≥ 1√
2

.

Let δ, (cid:15) ∈ (0, 1) be ﬁxed. If we run the algorithm with any epoch length parameter m and step size η,

such that

η ≤

c1δ2
r2 λ , m ≥

c2 log(2/δ)
ηλ

, mη2r2 + r

(cid:112)

mη2 log(2/δ) ≤ c3,

(6)

(where c1, c2, c3 designates certain positive numerical constants), and for T =
probability at least 1 − 2 log(1/(cid:15))δ, it holds that

(cid:108) log(1/(cid:15))
log(2/δ)

(cid:109)

epochs, then with

(cid:104) ˜wT , v1(cid:105)2 ≥ 1 − (cid:15).

for any sufﬁciently large m on the order of 1

The proof of the theorem is provided in Sec. 4. It is easy to verify that for any ﬁxed δ, Eq. (6) holds
ηλ , as long as η is chosen to be sufﬁciently smaller than λ/r2.
iterations per epoch, and T = Θ(log(1/(cid:15)))

Therefore, by running the algorithm for m = Θ

(r/λ)2(cid:17)

(cid:16)

3I.e. given W (cid:48)

t , return Wt with the same column space such that W (cid:62)

parameterically close to previous iterates, and W (cid:48)
important to use an orthogonalization procedure such that Wt is close to W (cid:48)

t Wt = I. Note that the algorithm relies on Wt remaining
t is a relatively small perturbation of of an orthogonal Wt−1. Therefore, it’s

t if W (cid:48)

t is nearly orthogonal, such as Gram-Schmidt.

4

epochs, we get accuracy (cid:15) with high probability4 1 − 2 log(1/(cid:15))δ. Since each epoch requires O(ds(m + n))
time to implement, we get a total runtime of

(cid:18)

(cid:18)

O

ds

n +

(cid:16) r
λ

(cid:17)2(cid:19)

log

(cid:19)(cid:19)

,

(cid:18) 1
(cid:15)

(7)

establishing an exponential convergence rate. If λ/r ≥ Ω(1/
to log-factors, proportional to the time required just to scan the data once.

n), then the runtime is O(dsn log(1/(cid:15))) – up

√

The theorem assumes that we initialize the algorithm with ˜w0 for which (cid:104) ˜w0, v1(cid:105) ≥ 1√
2

. This is not
trivial, since if we have no prior knowledge on v1, and we choose ˜w0 uniformly at random from the unit
√
sphere, then it is well-known that |(cid:104) ˜w0, v1(cid:105)| ≤ O(1/
d) with high probability. Thus, the theorem should
be interpreted as analyzing the algorithm’s convergence after an initial “burn-in” period, which results in
some ˜w0 with a certain constant distance from v1. This period requires a separate analysis, which we leave
to future work. However, since we only need to get to a constant distance from v1, the runtime of that
period is independent of the desired accuracy (cid:15). Moreover, we note that in our experiments (see Sec. 3),
even when initialized from a random point, no “burn-in” period is discernable, and the algorithm seems to
enjoy the same exponential convergence rate starting from the very ﬁrst epoch. Finally, since the variance-
reduction technique only kicks in once we are relatively close to the optimum, it is possible to use some
different stochastic algorithm with ﬁnite-time analysis, such as Oja’s algorithm (e.g. [3]) or [7, 4] to get to
this constant accuracy, from which point our algorithm and analysis takes over (for example, the algorithm
of [4] would require O(d/λ2) iterations, starting from a randomly chosen point, according to their analysis).
In any case, note that some assumption on (cid:104) ˜w0, v1(cid:105) being bounded away from 0 must hold, otherwise the
algorithm may fail to converge in the worst-case (a similar property holds for power iterations, and follows
from the non-convex nature of the optimization problem).

3 Experiments

We now turn to present some experiments, which demonstrate the performance of the VR-PCA algorithm.
Rather than tuning its parameters, we used the following ﬁxed heuristic: The epoch length m was set to n
(number of data points, or columns in the data matrix), and η was set to η = 1
i=1 (cid:107)xi(cid:107)2
√
¯r
is the average squared norm of the data. The choice of m = n ensures that at each epoch, the runtime is about
equally divided between the stochastic updates and the computation of ˜u. The choice of η is motivated by
our theoretical analysis, which requires η on the order of 1/(maxi (cid:107)xi(cid:107)2√
n) in the regime where m should
be on the order of n. Also, note that this choice of η can be readily computed from the data, and doesn’t
require knowledge of λ.

n , where ¯r = 1

(cid:80)n

n

First, we performed experiments on several synthetic random datasets (where n = 200000, d = 10000),
with different choices of eigengap5 λ. For comparison, we also implemented Oja’s algorithm, using several
different step sizes, as well as power iterations6. All algorithms were initialized from the same random

4Strictly speaking, this statement is non-trivial only in the regime of (cid:15) where log (cid:0) 1

δ , but if δ is a reasonably small ((cid:28) 1),
then this is the practically relevant regime. Moreover, as long as the success probability is positive, we can get an algorithm which
succeeds with exponentially high probability by an ampliﬁcation argument: Simply run several independent instantiations of the
algorithm, and pick the solution w for which w(cid:62) (cid:0) 1

(cid:1) w is largest.

(cid:1) (cid:28) 1

(cid:80)n

(cid:15)

5For each choice of λ, we constructed a d × d diagonal matrix D, with diagonal (1, 1 − λ, 1 − 1.1λ, . . . , 1 − 1.4λ, q1, q2, . . .)
where qi = |gi|/d and each gi was chosen according to a standard Gaussian distribution. We then let X = U DV (cid:62), where U and
V are random d × d and n × d orthogonal matrices. This results in a data matrix X whose spectrum is the same as D.

i=1 xix(cid:62)

i

n

6We note that more sophisticated iterative algorithms, such as the Lanczos method, can attain better performance than power

5

Figure 1: Results for synthetic data. Each plot represents results for a single dataset with eigengap λ, and
compares the performance of VR-PCA to power iterations and Oja’s algorithm with different step sizes ηt.
In each plot, the x-axis represents the number of effective data passes (assuming 2 per epoch for VR-PCA),
and the y-axis equals log10

, where w is the vector obtained so far.

1 −

(cid:107)X (cid:62)w(cid:107)2
maxv:(cid:107)v(cid:107)=1 (cid:107)X (cid:62)v(cid:107)2

(cid:17)

(cid:16)

vector, chosen uniformly at random from the unit ball. Note that compared to our analysis, this makes
things harder for our algorithm, since we require it to perform well also in the ‘burn-in’ phase. The results
are displayed in ﬁgure 1, and we see that for all values of λ considered, VR-PCA converges much faster
than all versions of Oja’s algorithm, on which it is based, as well as power iterations, even though we did
not tune its parameters. Moreover, since the y-axis is in logarithmic scale, we see that the convergence rate
is indeed exponential in general, which accords with our theory. In contrast, the convergence rate of Oja’s
algorithm (no matter which step size is chosen) appears to be sub-exponential. This is not surprising, since
the algorithm does not leverage the ﬁnite nature of the training data, and the inherent variance in its updates
does not decrease exponentially fast. A similar behavior will occur with other purely stochastic algorithms
in the literature, such as [1, 13, 7, 4].

Next, we performed a similar experiment using the training data of the well-known MNIST and CCAT
datasets. The MNIST data matrix size is 784 × 70000, and was pre-processed by centering the data and
dividing each coordinate by its standard deviation times the squared root of the dimension. The CCAT data
matrix is sparse (only 0.16% of entries are non-zero), of size 23149 × 781265, and was used as-is. The
results appear in ﬁgure 2. We also present the results for a simple hybrid method, which initializes the VR-
PCA algorithm with the result of running n iterations of Oja’s algorithm. The decaying step size of Oja’s
algorithm is more suitable for the initial phase, and the resulting hybrid algorithm can perform better than
each algorithm alone.

Finally, we present a similar experiment on the MNIST and CCAT datasets, where this time we attempt
to recover k > 1 singular vectors using the generalization of VR-PCA discussed in remark 1. A simi-
lar generalization was also employed with the competitors. The results are displayed in ﬁgure 3, and are

iterations. However, they are not directly comparable to power iterations and VR-PCA, since they are inherently more complex and
can require considerably more memory.

6

0102030405060−10−8−6−4−2λ = 0.16log−error0102030405060−10−8−6−4−2λ = 0.050102030405060−10−8−6−4−2λ = 0.016# data passes0102030405060−10−8−6−4−2λ = 0.005# data passeslog−error0102030405060−10−8−6−4−2λ = 0.0016# data passes  VR−PCAPower IterationsOja, ηt=1/tOja, ηt=3/tOja,ηt=9/tOja, ηt=27/tOja, ηt=81/tOja, ηt=243/tFigure 2: Results for the MNIST and CCAT datasets, using the same algorithms as in Fig. 1, as well as the
hybrid method described in the text (represented by a thinner plain line). See Fig. 1 for a legend.

Figure 3: Results for MNIST (for k = 6 singular vectors) and CCAT (for k = 3 singular vectors). The y-
, with W ∈ Rd×k being the current iterate. This directly
axis here equals log10
generalizes the performance measure used in previous ﬁgures for the k > 1 case. See Fig. 1 for a legend.

(cid:107)X (cid:62)W (cid:107)2
F
maxV :V (cid:62)V =I (cid:107)X (cid:62)V (cid:107)2
F

1 −

(cid:17)

(cid:16)

qualitatively similar to the k = 1 case.

4 Proof of Thm. 1

To simplify the presentation of the proof, we use a few important conventions:

• Note that the algorithm remains the same if we divide each xi by

r, and multiply η by r. Since
maxi (cid:107)xi(cid:107)2 ≤ r, this corresponds to running the algorithm with step-size ηr rather than η, on a
re-scaled dataset of points with squared norm at most 1, and with an eigengap of λ/r instead of λ.
Therefore, we can simply analyze the algorithm assuming that maxi (cid:107)xi(cid:107)2 ≤ 1, and in the end plug
in λ/r instead of λ, and ηr instead of η, to get a result which holds for data with squared norm at most
r.

√

• Let A = (cid:80)d

i=1 siviv(cid:62)

i be an eigendecomposition of A, where s1 > s2 ≥ . . . ≥ sd, s1 − s2 =

7

05101520−10−9−8−7−6−5−4−3−2−1mnist# data passeslog−error05101520−10−9−8−7−6−5−4−3−2−1CCAT# data passes051015202530−10−8−6−4−20mnist# data passes051015202530−10−8−6−4−20CCAT# data passeslog−errorλ > 0, and v1, . . . , vd are orthonormal vectors. Following the discussion above, we assume that
maxi (cid:107)xi(cid:107)2 ≤ 1 and therefore {s1, . . . , sd} ⊂ [0, 1].

• Throughout the proof, we use c to designate positive numerical constants, whose value can vary at

different places (even in the same line or expression).

Part I: Establishing a Stochastic Recurrence Relation

We begin by focusing on a single epoch of the algorithm, and a single iteration t, and analyze how 1 −
(cid:104)wt, v1(cid:105)2 evolves during that iteration. The key result we need is the following lemma:

Lemma 1. Suppose that (cid:104)wt, v1(cid:105) ≥ 1

2 , and that (cid:104) ˜ws−1, v1(cid:105) ≥ 0. If η ≤ cλ, then

E (cid:2)(cid:0)1 − (cid:104)wt+1, v1(cid:105)2(cid:1)(cid:12)

(cid:12)wt

(cid:3) ≤

(cid:18)

1 −

(cid:19)

ηλ
16

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) + cη2 (cid:0)1 − (cid:104) ˜ws−1, v1(cid:105)2(cid:1)

for certain positive numerical constants c.

Proof. Since we focus on a particular epoch s, let us drop the subscript from ˜ws−1, and denote it simply at
˜w. Rewriting the update equations from the algorithm, we have that

wt+1 =

w(cid:48)
(cid:107)w(cid:48)

t+1
t+1(cid:107)

, where w(cid:48)

t+1 = (I + ηA)wt + η(xx(cid:62) − A)(wt − ˜w),

where x is the random instance chosen at iteration t.

It is easy to verify that

(cid:104)w(cid:48)

t+1, vi(cid:105) = ai + zi,

where

ai = (1 + ηsi)(cid:104)wt, vi(cid:105) , zi = ηv(cid:62)

i (xx(cid:62) − A)(wt − ˜w).

Moreover, since v1, . . . , vd form an orthonormal basis in Rd, we have

(cid:107)w(cid:48)

t+1(cid:107)2 =

d
(cid:88)

(cid:104)vi, w(cid:48)

t+1(cid:105)2 =

i=1

d
(cid:88)

i=1

(ai + zi)2.

(8)

(9)

Let E denote expectation with respect to x, conditioned on wt. Combining Eq. (8) and Eq. (9), we have

E (cid:2)(cid:104)wt+1, v1(cid:105)2(cid:3) = E

(cid:20)

(cid:104)

w(cid:48)
(cid:107)w(cid:48)

t+1
t+1(cid:107)

(cid:21)

= E

, v1(cid:105)2

(cid:21)

(cid:20) (cid:104)w(cid:48)

t+1, v1(cid:105)2
(cid:107)w(cid:48)
t+1(cid:107)2

(cid:34)

= E

(a1 + z1)2
i=1(ai + zi)2

(cid:80)d

(cid:35)

.

(10)

Note that conditioned on wt, the quantities a1 . . . ad are ﬁxed, whereas z1 . . . zd are random variables (de-
pending on the random choice of x) over which we take an expectation.

The ﬁrst step of the proof is to simplify Eq. (10), by pushing the expectations inside the numerator and
the denominator. Of course, this may change the value of the expression, so we need to account for this
change with some care. To do so, deﬁne the auxiliary non-negative random variables x, y and a function
f (x, y) as follows:

x = (a1 + z1)2 , y =

d
(cid:88)

i=2

(ai + zi)2 , f (x, y) =

x
x + y

.

8

Then we can write Eq. (10) as Ex,y[f (x, y)]. We now use a second-order Taylor expansion to relate it to
f (E[x], E[y]) =

. Speciﬁcally, we have that Ex,y[f (x, y)] can be lower bounded by

E[(a1+z1)2]

E[(cid:80)d

i=1(ai+zi)2]

(cid:34)

Ex,y

f (E[x], E[y]) + ∇f (E[x], E[y])(cid:62)

(cid:19)

(cid:18)(cid:18)x
y

= f (E[x], E[y]) − max
x,y

(cid:107)∇2f (x, y)(cid:107) max
x,y

−

(cid:19)(cid:19)

(cid:18)E[x]
E[y]
(cid:13)
(cid:18) x − E[x]
(cid:13)
(cid:13)
y − E[y]
(cid:13)

(cid:107)∇2f (x, y)(cid:107) max
x,y

(cid:19)

(cid:13)
(cid:18)x
(cid:13)
(cid:13)
y
(cid:13)

−

2(cid:35)

(cid:18)E[x]
E[y]

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

− max
x,y

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

,

(11)

where ∇2f (x, y) is the Hessian of f at (x, y).

We now upper bound the two max-terms in the expression above:

• For the ﬁrst max-term, it is easily veriﬁed that

∇2f (x, y) =

1
(x + y)3

(cid:18) −2y
x − y

(cid:19)

.

x − y
2x

Since the spectral norm is upper bounded by the Frobenius norm, which for 2 × 2 matrices is upper
bounded by 2 times the magnitude of the largest entry in the matrix (which in our case is at most
2(x + y)/(x + y)3 = 2/(x + y)2 ≤ 2/x2), we have

max
x,y

(cid:107)∇2f (x, y)(cid:107) ≤ max

x

4
x2 = max

z1

4
(a1 + z1)2 .

Now, recall that a1 ≥ 1
η(cid:107)vi(cid:107)(cid:107)xx(cid:62) − A(cid:107)(cid:107)wt − ˜w(cid:107) ≤ cη, so for η sufﬁciently small, |z1| ≤ 1
(a1+z1)2 (and hence maxx,y (cid:107)∇2f (x, y)(cid:107)) by some numerical constant c. Overall, we have

2 by the Lemma’s assumptions, and in contrast |z1| ≤ η (cid:12)

(cid:12)v(cid:62)

4

2 |a1|, and we can upper bound

i (xx(cid:62) − A)(wt − ˜w)(cid:12)

(cid:12) ≤

(cid:107)∇2f (x, y)(cid:107) ≤ c.

max
x,y

(12)

• For the second max-term in Eq. (11), recalling that x = (a1 + z1)2, y = (cid:80)d

i=2(ai + zi)2, and that the

zi’s are zero-mean, we have

max
x,y

(cid:0)(x − E[x])2 + (y − E[y])2(cid:1) = max
z1...zd

(cid:0)2a1z1 + z2

1 − E[z2

1](cid:1)2

(cid:32) d

(cid:88)

+

i=2

(cid:0)2aizi + z2

i ](cid:1)
i − E[z2

(cid:33)2

.

Using the elementary fact that (r + s)2 ≤ 2(r2 + s2) for all r, s, as well as the deﬁnition of ai, zi, this
is at most

2 max
z1

(2a1z1)2 + 2 max
z1

(z2

1 − E[z2

1])2 + 2 max
z2,...,zd

(cid:32) d

(cid:88)

i=2

(cid:33)2

2aizi

+ 2 max
z2,...,zd

(cid:32) d

(cid:88)

(cid:33)2

(z2

i − E[z2

i ])

≤ 8 max

z1

(a1z1)2 + 4 max
z1

(z2

1)2 + 4E[z2

1]2 + 8 max
z2,...,zd

(cid:32) d

(cid:88)

i=2

(cid:33)2

aizi

+ 4 max
z2,...,zd

≤ 8 max

z1

(a1z1)2 + 8 max
z1

z4
1 + 8 max
z2,...,zd

(cid:32) d

(cid:88)

i=2

(cid:33)2

aizi

+ 8 max
z2,...,zd

(cid:32) d

(cid:88)

(cid:33)2

z2
i

.

i=2

9

i=2
(cid:32) d

(cid:88)

i=2

(cid:33)

z2
i

+ 4

(cid:32) d

(cid:88)

i=2

(cid:33)2

E[z2
i ]

(13)

Recalling the deﬁnition of ai, zi, and that (cid:107)wt(cid:107),(cid:107) ˜w(cid:107),(cid:107)v1(cid:107),ηsi and (cid:107)xx(cid:62) − A(cid:107) are all bounded by
constants, we now show that each term in the expression above can be upper bounded by cη2(cid:107)wt −
˜w(cid:107)2 for some appropriate constant c:

8 max
z1

(a1z1)2 = 8 max

x

≤ 8 max

x

(cid:16)

(cid:16)

η(1 + ηs1)(cid:104)wt, v1(cid:105)v(cid:62)

1 (xx(cid:62) − A)(wt − ˜w)

(cid:17)2

η(1 + ηs1)|(cid:104)wt, v1(cid:105)|(cid:107)v1(cid:107)(cid:107)xx(cid:62) − A(cid:107)(cid:107)wt − ˜w(cid:107)

(cid:17)2

≤ c (η(cid:107)wt − ˜w(cid:107))2 = cη2(cid:107)wt − ˜w(cid:107)2.

8 max
z1

z4
1 = 8 max

x

(cid:16)

ηv(cid:62)

1 (xx(cid:62) − A)(wt − ˜w)
(cid:17)4

η(cid:107)v1(cid:107)(cid:107)xx(cid:62) − A(cid:107)(cid:107)wt − ˜w(cid:107)

(cid:16)

≤ 8

(cid:17)4

≤ c(η(cid:107)wt − ˜w(cid:107))4 = c(η(cid:107)wt − ˜w(cid:107))2(η(cid:107)wt − ˜w(cid:107))2
≤ c(η(cid:107)wt − ˜w(cid:107))2 = cη2(cid:107)wt − ˜w(cid:107)2.

8 max
z2,...,zd

(cid:32) d

(cid:88)

i=2

(cid:33)2

aizi

= 8 max

x

(cid:32) d

(cid:88)

i=2

η(1 + ηsi)(cid:104)wt, vi(cid:105)v(cid:62)

i (xx(cid:62) − A)(wt − ˜w)

(cid:33)2

(cid:32)
η

(cid:32)
η

(cid:32)
η

≤ c

= c

≤ c

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

d
(cid:88)

i=2
(cid:32) d

(cid:88)

i=2
(cid:32) d

(cid:88)

i=2

(1 + ηsi)(cid:104)wt, vi(cid:105)vi

(1 + ηsi)viv(cid:62)
i

(1 + ηsi)viv(cid:62)
i

(cid:33)2

(cid:107)wt − ˜w(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)2

(cid:107)wt − ˜w(cid:107)

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

wt

(cid:33)2

(cid:107)wt − ˜w(cid:107)

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ c (η(cid:107)wt − ˜w(cid:107))2 = cη2(cid:107)wt − ˜w(cid:107)2,

where in the last inequality we used the fact that v2 . . . vd are orthonormal vectors, and (1 + ηsi) is
bounded by a constant. Similarly,

8 max
z2,...,zd

(cid:33)2

(cid:32) d

(cid:88)

i=2

z2
i

= 8 max

x

(cid:32)

η2

d
(cid:88)

i=2

(wt − ˜w)(cid:62)(xx(cid:62) − A)viv(cid:62)

i (xx(cid:62) − A)(wt − ˜w)

(cid:33)2

η2(wt − ˜w)(cid:62)(xx(cid:62) − A)

(cid:32) d

(cid:88)

(cid:33)

(cid:33)2

viv(cid:62)
i

(xx(cid:62) − A)(wt − ˜w)

= 8 max

x

(cid:32)

(cid:32)

≤ 8 max

x

η2(cid:107)wt − ˜w(cid:107)2(cid:107)xx(cid:62) − A(cid:107)2

d
(cid:88)

i=2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=2

viv(cid:62)
i

(cid:33)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ c (cid:0)η2(cid:107)wt − ˜w(cid:107)2(cid:1)2

= c (cid:0)η2(cid:107)wt − ˜w(cid:107)2(cid:1) (cid:0)η2(cid:107)wt − ˜w(cid:107)2(cid:1) ≤ cη2(cid:107)wt − ˜w(cid:107)2.

10

Plugging these bounds back into Eq. (13), we get that

(cid:0)(x − E[x])2 + (y − E[y])2(cid:1) ≤ cη2(cid:107)wt − ˜w(cid:107)2

max
x,y

(14)

for some appropriate constant c.

Plugging Eq. (12) and Eq. (13) back into Eq. (11), we get a lower bound of
E (cid:2)(a1 + z1)2(cid:3)
(cid:104)(cid:80)d

Ex,y[f (x, y)] ≥ f (E[x], E[y]) − cη2(cid:107)wt − ˜w(cid:107)2 =

E

i=1(ai + zi)2(cid:105) − cη2(cid:107)wt − ˜w(cid:107)2,

and since each zi is zero-mean, this equals
E (cid:2)a2

(cid:3)

1 + z2
1
i + z2
i )

i=1(a2

(cid:104)(cid:80)d

E

(cid:105) − cη2(cid:107)wt − ˜w(cid:107)2

(15)

By deﬁnition of zi and the fact that v1, . . . , vd are orthonormal (hence (cid:80)
have

i viv(cid:62)
i

is the identity matrix), we

d
(cid:88)

i=1

i = η2(wt − ˜w)(cid:62)(xx(cid:62) − A)
z2

(cid:33)

viv(cid:62)
i

(xx(cid:62) − A)(wt − ˜w)

(cid:32) d

(cid:88)

i=1

= η2(wt − ˜w)(cid:62)(xx(cid:62) − A)(xx(cid:62) − A)(wt − ˜w)
= η2(cid:107)(xx(cid:62) − A)(wt − ˜w)(cid:107)2 ≤ cη2(cid:107)wt − ˜w(cid:107)2,

so we can lower bound Eq. (15) by

a2
1

(cid:80)d

i=1 a2

i + cη2(cid:107)wt − ˜w(cid:107)2

− cη2(cid:107)wt − ˜w(cid:107)2.

(16)

Focusing on the ﬁrst term in Eq. (16) for the moment, and substituting in the deﬁnition of ai, we can write
it as

(1 + ηs1)2(cid:104)wt, v1(cid:105)2

(1 + ηs1)2(cid:104)wt, v1(cid:105)2 + (cid:80)d

i=2(1 + ηsi)2(cid:104)vi, wt(cid:105)2 + cη2(cid:107)wt − ˜w(cid:107)2
(cid:104)wt, v1(cid:105)2

(cid:104)wt, v1(cid:105)2 +

(cid:16) 1+ηs2
1+ηs1

(cid:17)2 (cid:80)d

i=2(cid:104)vi, wt(cid:105)2 + cη2(cid:107)wt − ˜w(cid:107)2

≥

=

=

(cid:104)wt, v1(cid:105)2 +

(cid:17)2

(cid:16) 1+ηs2
1+ηs1

(cid:18)

1 −

1 −

(cid:17)2(cid:19)

(cid:16) 1+ηs2
1+ηs1
(cid:32)

(cid:32)

≥ (cid:104)wt, v1(cid:105)2

1 +

1 −

(cid:104)wt, v1(cid:105)2

(1 − (cid:104)wt, v1(cid:105)2) + cη2(cid:107)wt − ˜w(cid:107)2
(cid:104)wt, v1(cid:105)2

(1 − (cid:104)wt, v1(cid:105)2) + cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)2(cid:33)

(cid:18) 1 + ηs2
1 + ηs1

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:33)

,

11

where in the last step we used the elementary inequality 1
justiﬁed since (cid:104)wt, v1(cid:105) ≤ 1 and 1+ηs2
1+ηs1
(cid:18)

(cid:19)(cid:19)

(cid:18)

≤ 1). This can be further lower bounded by

1−x ≥ 1 + x for all x ≤ 1 (and this is indeed

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)

(cid:18) 1 + ηs2
1 + ηs1

η(s1 − s2)
1 + ηs1

(cid:104)wt, v1(cid:105)2

1 +

1 −

= (cid:104)wt, v1(cid:105)2

≥ (cid:104)wt, v1(cid:105)2

(cid:18)

1 +

(cid:18)

1 +

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)

ηλ
2

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)

,

where in the last inequality we used the fact that s1 − s2 = λ and that ηs1 ≤ η which is at most 1 (again
using the assumption that η is sufﬁciently small).

Plugging this lower bound on the ﬁrst term in Eq. (16), and recalling that (cid:104)wt, v1(cid:105)2 is assumed to be at

least 1/4, we get the following lower bound on Eq. (16):
(cid:18)

(cid:104)wt, v1(cid:105)2

1 +

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

ηλ
2

(cid:19)

− cη2(cid:107)wt − ˜w(cid:107)2

≥ (cid:104)wt, v1(cid:105)2

(cid:18)

1 +

ηλ
2

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)

.

To summarize the derivation so far, starting from Eq. (10) and concatenating the successive lower bounds

we have derived, we get that

E[(cid:104)wt+1, v1(cid:105)2] ≥ (cid:104)wt, v1(cid:105)2

(cid:18)

1 +

ηλ
2

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2(cid:107)wt − ˜w(cid:107)2

(cid:19)

.

(17)

We now get rid of the (cid:107)wt− ˜w(cid:107)2 term, by noting that since (x+y)2 ≤ 2(x2+y2) and (cid:107)wt(cid:107) = (cid:107)v1(cid:107) = 1,

(cid:107)wt − ˜w(cid:107)2 ≤ ((cid:107)wt − v1(cid:107) + (cid:107) ˜w − v1(cid:107))2 ≤ 2 (cid:0)(cid:107)wt − v1(cid:107)2 + (cid:107) ˜w − v1(cid:107)2(cid:1)

= 2 (2 − 2(cid:104)wt, v1(cid:105) + 2 − 2(cid:104) ˜w, v1(cid:105)) .

Since we assume that (cid:104)wt, v1(cid:105), (cid:104) ˜w, v1(cid:105) are both positive, and they are also at most 1, this is at most

2 (cid:0)2 − 2(cid:104)wt, v1(cid:105)2 + 2 − 2(cid:104) ˜w, v1(cid:105)2(cid:1) = 4 (cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) + 4 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1) .

Plugging this back into Eq. (17), we get that

E[(cid:104)wt+1, v1(cid:105)2] ≥ (cid:104)wt, v1(cid:105)2

(cid:18)

1 +

(cid:18) ηλ
2

(cid:19)

− cη2

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1)

(cid:19)

,

and since we can assume ηλ

2 − cη2 ≥ ηλ

4 by picking η sufﬁciently smaller than λ, this can be simpliﬁed to

E[(cid:104)wt+1, v1(cid:105)2] ≥ (cid:104)wt, v1(cid:105)2

(cid:18)

1 +

ηλ
4

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) − cη2 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1)

(cid:19)

.

Finally, subtracting both sides of the inequality from 1, we get

E[1 − (cid:104)wt+1, v1(cid:105)2] ≤ 1 − (cid:104)wt, v1(cid:105)2 −

ηλ
(cid:104)wt, v1(cid:105)2 (cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) + cη2(cid:104)wt, v1(cid:105)2 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1)
4
(cid:19)
(cid:104)wt, v1(cid:105)2

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) + cη2 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1) ,

(cid:18)

≤

1 −

ηλ
4

12

and since we assume (cid:104)wt, v1(cid:105) ≥ 1

(cid:18)

1 −

2 , we can upper bound this by
ηλ
16

(cid:19)

(cid:0)1 − (cid:104)wt, v1(cid:105)2(cid:1) + cη2 (cid:0)1 − (cid:104) ˜w, v1(cid:105)2(cid:1)

as required. Note that to get this bound, we assumed at several places that η is smaller than either a constant,
or a constant factor times λ (which is at most 1). Hence, the bound holds by assuming η ≤ cλ for a
sufﬁciently small constant c.

Part II: Solving the Recurrence Relation for a Single Epoch

As before, since we focus on a single epoch, we drop the subscript from ˜ws−1 and denote it simply as ˜w.

Suppose that η = αλ, where α is a sufﬁciently small constant to be chosen later. Also, let

bt = 1 − (cid:104)wt, v1(cid:105)2 and ˜b = 1 − (cid:104) ˜w, v1(cid:105)2.

Then Lemma 1 tells us that if α is sufﬁciently small, bt ≤ 3

E [bt+1|wt] ≤

(cid:16)

1 −

α
16

4 , and (cid:104) ˜w, v1(cid:105) ≥ 0, then
λ2(cid:17)

bt + cα2λ2˜b.

(18)

Lemma 2. Let B be the event that bt ≤ 3
constants c, if α ≤ c, and (cid:104) ˜w, v1(cid:105) ≥ 0, then

4 for all t = 0, 1, 2, . . . , m. Then for certain positive numerical

E[bm|B, w0] ≤

(cid:16)(cid:16)

1 −

α
16

λ2(cid:17)m

(cid:17) ˜b.

+ cα

Proof. Recall that bt is a deterministic function of the random variable wt, which depends in turn on wt−1
and the random instance chosen at round m. We assume that w0 (and hence ˜b) are ﬁxed, and consider how
bt evolves as a function of t. Using Eq. (18), we have

E[bt+1|wt, B] = E

(cid:20)
bt+1|wt, bt+1 ≤

(cid:21)

3
4

≤ E[bt+1|wt] ≤

(cid:16)

1 −

λ2(cid:17)

α
16

bt + cα2λ2˜b.

Note that the ﬁrst equality holds, since conditioned on wt, bt+1 is independent of b1, . . . , bt, so the event B
is equivalent to just requiring bt+1 ≤ 3/4.

Taking expectation over wt (conditioned on B), we get that

E[bt+1|B] ≤ E
(cid:16)

=

1 −

(cid:104)(cid:16)

1 −

α
16

α
16
λ2(cid:17)

λ2(cid:17)

(cid:12)
bt + cα2λ2˜b
(cid:12)
(cid:12)B
E [bt|B] + cα2λ2˜b.

(cid:105)

Unwinding the recursion, and using that b0 = ˜b, we therefore get that
m−1
(cid:88)

(cid:16)

E[bm|B] ≤

1 −

λ2(cid:17)m

α
16

˜b + cα2λ2˜b

(cid:16)

1 −

λ2(cid:17)i

α
16

λ2(cid:17)i

α
16

i=0
∞
(cid:88)

(cid:16)

1 −

i=0

1
(α/16)λ2

(cid:16)

(cid:16)

≤

=

=

1 −

λ2(cid:17)m

α
16

˜b + cα2λ2˜b

1 −

(cid:16)(cid:16)

1 −

λ2(cid:17)m
α
16
λ2(cid:17)m
α
16

˜b + cα2λ2˜b
(cid:17) ˜b.

+ cα

13

as required.

We now turn to prove that the event B assumed in Lemma 2 indeed holds with high probability:

Lemma 3. For certain positive numerical constants c, suppose that α ≤ c, and (cid:104) ˜w, v1(cid:105) ≥ 0. Then for any
β ∈ (0, 1) and m, if

˜b + cmα2λ2 + c

(cid:112)

mα2λ2 log(1/β) ≤

,

(19)

3
4

for a certain numerical constant c, then it holds with probability at least 1 − β that

bt ≤ ˜b + cmα2λ2 + c

(cid:112)

mα2λ2 log(1/β) ≤

3
4

for some numerical constant c and for all t = 0, 1, 2, . . . , m, as well as (cid:104)wm, v1(cid:105) ≥ 0.

Proof. To prove the lemma, we analyze the stochastic process b0(= ˜b), b1, b2, . . . , bm, and use a concentra-
tion of measure argument. First, we collect the following facts:

• ˜b = b0 ≤ 3

4 : This directly follows from the assumption stated in the lemma.

• The conditional expectation of bt+1 is close to bt, as long as bt ≤ 3

4 : Supposing that bt ≤ 3

4 for some

t, and α is sufﬁciently small, then by Eq. (18),

E [bt+1|wt] ≤

(cid:16)

1 −

λ2(cid:17)

α
16

bt + cα2λ2˜b ≤ bt + cα2λ2˜b.

• |bt+1 − bt| is bounded by cαλ: Since the norm of wt, v1 is 1, we have

|bt+1 − bt| = (cid:12)

(cid:12)(cid:104)wt+1, v1(cid:105)2 − (cid:104)wt, v1(cid:105)2(cid:12)

(cid:12) = |(cid:104)wt+1, v1(cid:105) + (cid:104)wt, v1(cid:105)| ∗ |(cid:104)wt+1, v1(cid:105) − (cid:104)wt, v1(cid:105)|

≤ 2 |(cid:104)wt+1, v1(cid:105) − (cid:104)wt, v1(cid:105)| ≤ 2(cid:107)wt+1 − wt(cid:107).

Recalling the deﬁnition of wt+1 in our algorithm, and the fact that the instances xi and hence the
matrix A are assumed to have norm at most 1, it is easy to verify that (cid:107)wt+1 − wt(cid:107) ≤ cη ≤ cαλ for
some appropriate constant c.

Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [8], it follows
that with probability at least 1−β, it holds simultaneously for all t = 1, . . . , m (and for t = 0 by assumption)
that

bt ≤ ˜b + mcα2λ2˜b + c

(cid:112)

mα2λ2 log(1/β)

for some constants c, as long as the expression above is less than 3
4 , then we get that bt ≤ 3
3
statement in the lemma.

4 . If the expression is indeed less than
4 for all t. Upper bounding ˜b and λ by 1, and slightly simplifying, we get the

It remains to prove that if bt ≤ 3

4 for all t, then (cid:104)wm, v1(cid:105) ≥ 0. Suppose on the contrary that (cid:104)wm, v1(cid:105) <
0. Since |(cid:104)wt+1, v1(cid:105) − (cid:104)wt, v1(cid:105)| ≤ (cid:107)wt+1 − wt(cid:107) ≤ cαλ as we’ve seen earlier, and (cid:104)w0, v1(cid:105) ≥ 0, it means
there must have been some wt such that (cid:104)wt, v1(cid:105) ≤ cαλ. But this means that bt = (1 − (cid:104)wt, v1(cid:105)2) ≥ 1 −
c2α2λ2 > 3
4 (as long as α is sufﬁciently small, since we assume λ is bounded), invalidating the assumption
that bt ≤ 3
4 for all t. Therefore, (cid:104)wm, v1(cid:105) ≥ 0 as required.

Combining Lemma 2 and Lemma 3, and using Markov’s inequality, we get the following corollary:

14

Lemma 4. Let conﬁdence parameters β, γ ∈ (0, 1) be ﬁxed. Suppose that (cid:104) ˜w, v1(cid:105) ≥ 0, and that m, α are
chosen such that

˜b + cmα2λ2 + c

(cid:112)

mα2λ2 log(1/β) ≤

3
4

for a certain numerical constant c. Then with probability at least 1 − (β + γ), it holds that (cid:104)wm, v1(cid:105) ≥ 0,
and

for some numerical constant c.

bm ≤

1
γ

(cid:16)(cid:16)

1 −

λ2(cid:17)m

(cid:17) ˜b.

+ cα

α
16

Part III: Analyzing the Entire Algorithm’s Run

Given the analysis in Lemma 4 for a single epoch, we are now ready to prove our theorem. Let

˜bs = 1 − (cid:104) ˜ws, v1(cid:105)2.

By assumption, at the beginning of the ﬁrst epoch, we have ˜b0 = 1 − (cid:104) ˜w0, v1(cid:105)2 ≤ 1 − 1
by Lemma 4, for any β, γ ∈ (cid:0)0, 1
2

(cid:1), if we pick any

2 = 1

2 . Therefore,

α ≤

γ2

1
2

and m ≥

48 log(1/γ)
αλ2

such that

1
2

then we get with probability at least 1 − (β + γ) that

+ cmα2λ2 + c

mα2λ2 log(1/β) ≤

(cid:112)

3
4

,

(20)

˜b1 ≤



(cid:18)



1
γ

1 −

αλ2
16

(cid:19) 48 log(1/γ)
αλ2


 ˜b0

+

γ2

1
2

Using the inequality (1 − (1/x))ax ≤ exp(−a), which holds for any x > 1 and any a, and taking x =
16/(αλ2) and a = 3 log(1/γ), we can upper bound the above by

(cid:18)

(cid:18)

exp

−3 log

1
γ

=

(cid:18)

1
γ

γ3 +

1
2

(cid:18) 1
γ
(cid:19)

γ2

˜b0 ≤ γ˜b0.

(cid:19)(cid:19)

+

(cid:19)

˜b0

γ2

1
2

Therefore, we get that ˜b1 ≤ γ˜b0. Moreover, again by Lemma 4, we have (cid:104) ˜w1, v1(cid:105) ≥ 0. Since ˜b1 is
only smaller than ˜b0, the conditions of Lemma 4 are fulﬁlled for ˜b = ˜b1, so again with probability at least
1 − (β + γ), by the same calculation, we have

˜b2 ≤ γ˜b1 ≤ γ2˜b0.

Repeatedly applying Lemma 4 and using a union bound, we get that after T epochs, with probability at least
1 − T (β + γ),

1 − (cid:104) ˜wT , v1(cid:105)2 = ˜bT ≤ γT ˜b0 < γT .

Therefore, for any desired accuracy parameter (cid:15), we simply need to use T =

1 − (cid:104) ˜wT , v1(cid:105)2 ≤ (cid:15) with probability at least 1 − T (β + γ) = 1 −

(cid:108) log(1/(cid:15))
log(1/γ)

(cid:109)

15

(β + γ).

(cid:108) log(1/(cid:15))
log(1/γ)

(cid:109)

epochs, and get

Using a conﬁdence parameter δ, we pick β = γ = δ

2 , which ensures that the accuracy bound above

holds with probability at least

1 −

(cid:25)

(cid:24) log(1/(cid:15))
log(2/δ)

δ ≥ 1 −

log(1/(cid:15))
log(2/δ)

δ ≥ 1 − 2 log

(cid:19)

(cid:18) 1
(cid:15)

δ.

Substituting this choice of β, γ into Eq. (20), and recalling that the step size η equals αλ, we get that
(cid:104) ˜wT , v1(cid:105)2 ≥ 1 − (cid:15) with probability at least 1 − 2 log(1/(cid:15))δ, provided that

η ≤ cδ2λ , m ≥

c log(2/δ)
ηλ

, mη2 +

(cid:112)

mη2 log(2/δ) ≤ c

for suitable constants c.

To get the theorem statement, recall that this analysis pertains to data whose squared norm is bounded
by 1. By the reduction discussed at the beginning of the proof, we can apply it to data with squared norm at
most r, by replacing λ with λ/r, and η with ηr, leading to the condition

η ≤

cδ2
r2 λ , m ≥

c log(2/δ)
ηλ

, mη2r2 + r

(cid:112)

mη2 log(2/δ) ≤ c.

Recalling that the different c’s above correspond to possibly different positive numerical constants, we get
the result stated in the theorem.

5 Discussion

In this paper, we presented and analyzed a stochastic algorithm for PCA and SVD with an exponential
convergence rate. Under suitable assumptions, the runtime scales as the sum of the data size n and an
eigengap factor 1
λ2 , and logarithmically in the required accuracy (cid:15). In contrast, the runtime of previous
iterative methods scale either as the product of n and an eigengap factor, or polynomially in (cid:15).

This work leaves several open questions. First, we note that in the regime of moderate data size n (in
particular, when n is dominated by (r/λ)2), the required runtime scales with 1/λ2, which is inferior to the
deterministic methods discussed in Sec. 1. Second, in the context of strongly convex optimization problems,
the variance-reduced technique we use leads to algorithms with runtime O (cid:0)d (cid:0)n + 1
(cid:1)(cid:1) , where λ
λ
is the strong convexity parameter of the problem [9]. Comparing this with our algorithm’s runtime, and
drawing a parallel between strong convexity and the eigengap in PCA problems, it is tempting to conjecture
that the 1/λ2 in our runtime analysis can be improved at least to 1/λ. However, we don’t know if this is
true, or whether the 1/λ2 factor is necessary in our setting. Third, it remains to analyze the behavior of
the algorithm starting from a randomly initialized point, before we obtain some ˜w0 sufﬁciently close to the
optimum. Experimentally, this does not seem to be an issue, but a full analysis would be more satisfactory,
and might give more guidance on how to optimally choose the step size. Finally, we believe our formal
analysis should be extendable to the k > 1 case (see remark 1), and that the dependence on the maximal
squared norm of the data can be relaxed to a dependence on the average squared norm or some weaker
moment conditions.

(cid:1) log (cid:0) 1
(cid:15)

Acknowledgments

This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel
Science Foundation grant 425/13. We thank Huy Nguyen, Mingyi Hong and Haishan Ye for spotting a bug
in the proof of lemma 1 in an earlier version of this paper.

16

References

[1] R. Arora, A. Cotter, K. Livescu, and N. Srebro. Stochastic optimization for PCA and PLS. In 2012

50th Annual Allerton Conference on Communication, Control, and Computing, 2012.

[2] R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of PCA with capped MSG. In NIPS, 2013.

[3] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental PCA. In NIPS,

2013.

[4] C. De Sa, K. Olukotun, and C. R´e. Global convergence of stochastic gradient descent for some non-

convex matrix problems. arXiv preprint arXiv:1411.1134, 2014.

[5] R. Frostig, R. Ge, S. Kakade, and A. Sidford. Competing with the empirical risk minimizer in a single

pass. CoRR, abs/1412.6606, 2014.

[6] G. Golub and C. van Loan. Matrix computations (4. ed.). Johns Hopkins University Press, 2013.

[7] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In NIPS, 2014.

[8] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

statistical association, 58(301):13–30, 1963.

[9] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In NIPS, 2013.

[10] J. Konecn´y and P. Richt´arik. Semi-stochastic gradient descent methods. CoRR, abs/1312.1666, 2013.

[11] T.P. Krasulina. The method of stochastic approximation for the determination of the least eigenvalue of
a symmetrical matrix. USSR Computational Mathematics and Mathematical Physics, 9(6):189–195,
1969.

[12] M. Mahdavi, L. Zhang, and R. Jin. Mixed optimization for smooth functions. In NIPS, 2013.

[13] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming PCA. In NIPS, 2013.

[14] E. Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical biology,

15(3):267–273, 1982.

[15] E. Oja and J. Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the
expectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69–84,
1985.

A Implementing Epochs in O(ds(m + n)) Amortized Runtime

As discussed in remark 2, the runtime of each iteration in our algorithm (as presented in our pseudo-code)
is O(d), and the total runtime of each epoch is O(dm + dsn), where ds is the average sparsity (number
of non-zero entries) in the data points xi. Here, we explain how the total epoch runtime can be improved
(at least in terms of the theoretical analysis) to O(ds(m + n)). For ease of exposition, we reproduce the
pseudo-code together with line numbers below:

17

1: Parameters: Step size η, epoch length m
2: Input: Data matrix X = (x1, . . . , xn); Initial unit vector ˜w0
3: for s = 1, 2, . . . do
˜u = 1
i=1 xi
4:
n
5: w0 = ˜ws−1
6:

for t = 1, 2, . . . , m do

i ˜ws−1

(cid:0)x(cid:62)

(cid:80)n

(cid:1)

Pick it ∈ {1, . . . , n} uniformly at random
w(cid:48)
˜ws−1
wt = 1
(cid:107)w(cid:48)

t = wt−1 + η (cid:0)xit
t(cid:107) w(cid:48)

wt−1 − x(cid:62)
it

(cid:0)x(cid:62)
it

t

(cid:1) + ˜u(cid:1)

7:

8:

9:

10:

end for
˜ws = wm

11:
12: end for

First, we can assume without loss of generality that d ≤ dsn. Otherwise, the number of non-zeros in
the n × d data matrix X is smaller than d, so the matrix must contain some all-zeros columns. But then, we
can simply drop those columns (the value of the largest singular vectors in the corresponding entries will be
zero anyway), hence reducing the effective dimension d to be at most dsn. Therefore, given a vector ˜ws−1,
we can implement line (4) in O(d + dsn) ≤ O(dsn) time, by initializing the d-dimensional vector ˜u to be 0,
(cid:1). Similarly, we can implement lines
and iteratively adding to it the sparse (on-average) vector xi
(5),(11) in O(d) ≤ O(dsn) time.

i ˜ws−1

(cid:0)x(cid:62)

It remains to show that we can implement each iteration in lines (8) and (9) in O(ds) time. To do so,
t, we only store ˜u, an auxiliary vector g, and auxiliary scalars α, β, γ, δ, ζ,

instead of explicitly storing wt, w(cid:48)
such that

• At the end of line (8), w(cid:48)

t is stored as αg + β ˜u

• At the end of line (9), wt is stored as αg + β ˜u

• It holds that γ = (cid:107)αg(cid:107)2 , δ = (cid:104)αg, ˜u(cid:105) , ζ = (cid:107)˜u(cid:107)2. This ensures that γ+2δ+ζ expresses (cid:107)αg+β ˜u(cid:107)2.

Before the beginning of the epoch (line (5)), we initialize g = ˜ws−1, α = 1, β = 0 and compute
γ = (cid:107)αg(cid:107)2 , δ = (cid:104)αg, ˜u(cid:105) , ζ = (cid:107)˜u(cid:107)2, all in time O(d) ≤ O(dsn). This ensures that w0 = αg + βu. Line
(8) can be implemented in O(ds) time as follows:

• Compute the sparse (on-average) update vector ∆g := ηxit

(cid:0)x(cid:62)
it

wt−1 − x(cid:62)
it

˜ws−1

(cid:1)

• Update g := g + ∆g/α; β := β + η; γ := γ + 2α(cid:104)g, ∆g(cid:105) + (cid:107)∆g(cid:107)2; δ := δ + (cid:104)∆g, ˜u(cid:105). This
t is represented as αg + βu, and its squared norm equals

implements line (8), and ensures that w(cid:48)
γ + 2δ + ζ.

To implement line (9), we simply divide α, β by
γ, δ accordingly. After this step, wt is represented by αg + β ˜u as required.

γ + 2δ + ζ (which equals the norm of w(cid:48)

t), and recompute

√

18

