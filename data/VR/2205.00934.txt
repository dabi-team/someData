2
2
0
2

y
a
M
2

]

V
C
.
s
c
[

1
v
4
3
9
0
0
.
5
0
2
2
:
v
i
X
r
a

Assessing unconstrained surgical cuttings in VR using CNNs

Ilias Chrysovergis
ilias.chrysovergis@oramavr.com
ORamaVR
Greece

Dimitris Angelis
dimitris.aggelis@oramavr.com
University of Crete, ORamaVR
Greece

Manos Kamarianakis
kamarianakis@uoc.gr
FORTH - ICS, University of Crete,
ORamaVR
Greece

Antonis Protopsaltis
aprotopsaltis@uowm.gr
University of Western Macedonia,
ORamaVR
Greece

Mike Kentros
mike.kentros@oramavr.com
FORTH - ICS, University of Crete,
ORamaVR
Greece

George Papagiannakis
papagian@ics.forth.gr
FORTH - ICS, University of Crete,
ORamaVR
Greece

KEYWORDS
Virtual Reality, Deep Learning, Convolutional Neural Networks

1 INTRODUCTION
In the recent years, industry and academia [8] have massively
adopted Virtual Reality (VR) applications to train students and
personnel. Despite the effort, only limited systems involve proce-
dures for assessing user progress inside the immersive environment,
that either evaluate only trivial tasks or require a huge amount of
time by the reviewers [6]. On the other hand, the need for real-time
automated evaluation of user’s actions is constantly increasing.
State-of-the-art methods for similar tasks either require the devel-
opment of complicated task specific computer vision algorithms
[10] or support very simple tasks [4].

This work proposes a deep learning based system, that is able to
assess, in real-time, user actions within a VR training scenario. The
method enables the rapid development of trained assessment func-
tions, since it utilizes data augmentation to minimize the amount
of labelled data that need to be collected. Furthermore, by using
transfer learning, these assessment functions can be reconfigured
to support similar tasks, thus reducing even more the amount of
training data. In this paper, we present the results of our method
for the task of tearing a deformable 3D model [3].

Different machine and deep learning algorithms [1, 7] were con-
sidered and compared (see Table 1). Ultimately, our proposed model
is a Convolutional Neural Network (CNN), trained on a dataset
created with a data augmentation technique [2].

2 OUR APPROACH
Data Collection: The input datasets, generated by multiple exe-
cution of the training tasks, are labelled by a score, specified by
the task designer, in our case a surgeon. These datasets contain
per-frame captured transformation data (translation & rotation)
of the active virtual tool (e.g. a scalpel), forming its trajectory and
representing the user gesture on each action.

Data Sampling and Augmentation: Since the execution of an
action is user dependent, the required task completion times vary
and the generated trajectory lengths 𝑀 are non-uniform. To amend
this, random samples are taken, creating 𝑁 -length trajectories that
can be fed to the CNN. The remaining data of the long trajectory
are sampled to create more 𝑁 -length trajectories, augmenting the

dataset. This technique significantly increases the number of the
training data and therefore enhances the training process.

Neural Network (NN) Architecture: Since low training and
inference times are preferred, a lightweight model, able to provide
high-accuracy results, was created. This was achieved by using
a 15-layer CNN, which consisted of 4 sets of two Convolutional,
two ReLU, and a Batch Normalization layer, along with 3 final
layers, a Global Average Pooling, a Flatten and a Dense layer, which
formed the classifier. Each of the 4 sets produces an output of higher
dimensionality. Thus, as the input is passed down deeper in the
network, more complex patterns in the trajectory are found.

Training: After being collected and sampled, the data are split
randomly in training, validation and testing sets. The training set
is used to train the NN, while the validation set is used to find the
best hyper-parameters and prevent over-fitting. Lastly, the testing
set is used to evaluate the model’s accuracy. The NN was trained
using the Adam optimizer with a learning rate of 10−4, the sparse
categorical cross entropy loss function, with 100 epochs and a batch
size of 16.

Inference: The trained NN model is then imported in the user’s
actions assessment module of the VR application. As before, these

Figure 1: The trained Convolutional Neural Network as-
sesses the cutting action performed on a deformable 3D
model.

 
 
 
 
 
 
No. of
Classes
2
3
6

Logistic
Regression
100%
100%
39%

KNN SVM CNN

100% 100% 100%
100% 100% 100%
95%
83%
66%

Table 1: Table comparing the accuracy of traditional ML
techniques and the proposed CNN (last column) for differ-
ent number of classes.

actions are captured and sampled into smaller length trajectories,
which are fed into the model. The user’s score is produced instantly
with no overhead on the application’s performance.

Retraining: The proposed NN can be easily adapted for similar
tasks, utilizing transfer learning. In this respect, the first layers of
the NN are frozen while the deepest ones are retrained using a
small amount of new training data. This procedure reduces training
time and the required amount of the collected data, and provides
high accurate results, since the model’s first layers represent more
generic patterns, that are common to similar actions.

Scoring System: The user’s actions are evaluated with a score
in the range of 0 − 10. However, the NN regards scores 0 − 5 (6
classes), which requires less training data. The model outputs the
probabilities 𝑝 (𝑖) of a trajectory belonging to each of the 6 classes.
Ultimately, the score 𝑆𝑡𝑟 (𝑘) for a single trajectory 𝑘 (in range 0 −
10) and the final score 𝑆𝑡𝑜𝑡 for the entire action are obtained by
evaluating

𝑆𝑡𝑟 (𝑘) = 2

5
∑︁

𝑖=0

𝑖 · 𝑝 (𝑖) and 𝑆𝑡𝑜𝑡 =

1
𝐾

𝐾
∑︁

𝑘=0

𝑆𝑡𝑟 (𝑘).

Figure 2: The confusion matrix of the convolutional neural
network for the 6 classes.

Chrysovergis, Kamarianakis, Kentros et al.

3 RESULTS & DISCUSSION
Figure 1 shows the scores calculated by the DL model for two differ-
ent cuts performed by a user on a deformable 3D model. The action
graded with 10 during training was collinear and of greater length
to the one that scored 8. In Table 1, a comparison of the accuracy
of different models is presented for various number of classes. In
the case of 2 or 3 classes, all models perform correctly, however,
when 6 classes are used, only our proposed CNN method achieves
an acceptable performance. In Figure 2, the confusion matrix of the
trained model depicts that, for most classes, the true labels coincide
with the predicted ones. The only exception is the third label which
sometimes is predicted falsely as the second one, due to the fact that
the specific classes have very similar trajectories, making it difficult
for the model to distinguish among them. As this error could also be
made by a human, the results are considered satisfying. However,
we plan to mitigate that issue by performing semi-supervised learn-
ing [9] using unlabelled data collected by users executing cutting
actions using the MAGES SDK [5]. With such training data, the
model will learn to better distinguish among different trajectories,
allowing the classifier to achieve better performance for all classes.

ACKNOWLEDGMENTS
The project was partially funded by the European Union’s Horizon
2020 research and innovation programme under grant agreements
No 871793 (ACCORDION) and No 101016509 (CHARITY).

REFERENCES
[1] Angus Dempster, François Petitjean, and Geoffrey I. Webb. 2020. ROCKET:
Exceptionally Fast and Accurate Time Series Classification Using Random Con-
volutional Kernels. Data Mining and Knowledge Discovery 34, 5 (Sept. 2020),
1454–1495. https://doi.org/10.1007/s10618-020-00701-z arXiv:1910.13051
[2] Brian Kenji Iwana and Seiichi Uchida. 2020. Time Series Data Augmenta-
tion for Neural Networks by Time Warping with a Discriminative Teacher.
arXiv:2004.08780 [cs, stat] (April 2020). arXiv:2004.08780 [cs, stat]

[3] Manos Kamarianakis and George Papagiannakis. 2021. An All-in-One Geometric
Algorithm for Cutting, Tearing, and Drilling Deformable Models. Advances in
Applied Clifford Algebras 31 (Jul 2021), 58.

[4] Vasileios Lahanas, Constantinos Loukas, Nikolaos Smailis, and Evangelos Geor-
giou. 2015. A novel augmented reality simulator for skills assessment in minimal
invasive surgery. Surg. Endosc. 29, 8 (Aug 2015), 2224–2234.

[5] George Papagiannakis, Paul Zikas, Nick Lydatakis, Steve Kateros, Mike Kentros,
Efstratios Geronikolakis, Manos Kamarianakis, Ioanna Kartsonaki, and Giannis
Evangelou. 2020. MAGES 3.0: Tying the Knot of Medical VR. In ACM SIGGRAPH
2020 Immersive Pavilion. Association for Computing Machinery, Article 6, 2 pages.
[6] Erica Southgate. 2020. Using Screen Capture Video to Understand Learning in
Virtual Reality. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces
Abstracts and Workshops (VRW). 418–421. https://doi.org/10.1109/VRW50115.
2020.00089

[7] Zhiguang Wang, Weizhong Yan, and Tim Oates. 2016. Time Series Classification
from Scratch with Deep Neural Networks: A Strong Baseline. arXiv:1611.06455
[cs, stat] (Dec. 2016). arXiv:1611.06455 [cs, stat]

[8] Biao Xie, Huimin Liu, Rawan Alghofaili, Yongqi Zhang, Yeling Jiang, Flavio Destri
Lobo, Changyang Li, Wanwan Li, Haikun Huang, Mesut Akdere, Christos Mousas,
and Lap-Fai Yu. 2021. A Review on Virtual Reality Skill Training Applications.
Frontiers in Virtual Reality 2 (2021). https://doi.org/10.3389/frvir.2021.645153
[9] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. 2021. A Survey on Deep

Semi-supervised Learning. https://doi.org/10.48550/ARXIV.2103.00550

[10] Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L Sarin, Thomas Ploetz,
Mark A Clements, and Irfan Essa. 2016. Automated video-based assessment of
surgical skills for training and evaluation in medical schools.
Int. J. Comput.
Assist. Radiol. Surg. 11, 9 (Sept. 2016), 1623–1636.

