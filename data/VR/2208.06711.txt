Virtual Reality Platform to Develop and Test
Applications on Human-Robot Social Interaction

Jair A. Bottega1, Raul Steinmetz1, Alisson H. Kolling1, Victor A. Kich1,
Junior C. de Jesus2, Ricardo B. Grando3, Daniel F. T. Gamarra1

2
2
0
2

g
u
A
3
1

]

O
R
.
s
c
[

1
v
1
1
7
6
0
.
8
0
2
2
:
v
i
X
r
a

Abstract— Robotics simulation has been an integral part of
research and development in the robotics area. The simulation
eliminates the possibility of harm to sensors, motors, and the
physical structure of a real robot by enabling robotics applica-
tion testing to be carried out quickly and affordably without
being subjected to mechanical or electronic errors. Simulation
through virtual reality (VR) offers a more immersive experience
by providing better visual cues of environments, making it
an appealing alternative for interacting with simulated robots.
This immersion is crucial, particularly when discussing sociable
robots, a subarea of the human-robot interaction (HRI) ﬁeld.
The widespread use of robots in daily life depends on HRI. In
the future, robots will be able to interact effectively with people
to perform a variety of tasks in human civilization. It is crucial
to develop simple and understandable interfaces for robots as
they begin to proliferate in the personal workspace. Due to this,
in this study, we implement a VR robotic framework with ready-
to-use tools and packages to enhance research and application
development in social HRI. Since the entire VR interface is an
open-source project, the tests can be conducted in an immersive
environment without needing a physical robot.

SUPPLEMENTARY MATERIAL

Released code and Docker
https://github.com/jajaguto/jubileo.

image is available at

I. INTRODUCTION

Since its inception, simulation in the ﬁeld of robotics
has played a crucial role in research and development.
For instance, to provide a secure and affordable human-
robot interaction experience, robotic surgical training, remote
manipulation interfaces, manufacturing training exercises, and
human-robot interaction research are frequently done on a
screen in a virtual environment. Virtual reality (VR), however,
offers a more immersive experience by better simulating
real situations, making it a desirable choice for engaging
with virtual robots. Prior research studies illustrate that
Human-Robot Interaction (HRI) can be enhanced with VR
interfaces [1], [2].

Through simulation, robotic applications can be tested
rapidly and affordably without being subjected to mechanical
or electronic mistakes, eliminating the possibility of a
real robot’s sensors, motors, or physical structure being
damaged. The importance of this aspect can be seen in [3].

1Jair A. Bottega, Raul Steinmetz, Victor A. Kich, Alisson H. Kolling and
Daniel F. T. Gamarra are with Federal University of Santa Maria - UFSM,
Santa Maria, RS, Brazil. jairaugustobottega@gmail.com

2Junior C. de Jesus is with Universidade Federal do Rio Grande - FURG,

RS, Brazil. dranaju@gmail.com

3Ricardo B. Grando is with Technological University of Uruguay - UTEC,

Rivera, Uruguay. ricardo.bedin@utec.edu.uy

Fig. 1: Social robot in VR simulated environment.

Also, the developers have no need to worry about human
safety, as discussed in [4]. Many HRI applications involve
users interacting with simulated robots. HRI is deﬁned by
Goodrich and Schultz [5] as a branch of study devoted to
comprehending, developing, and evaluating robotic systems
for usage with humans. Communication between people and
robots is necessary for human-robot social interaction, and it
can be done in a variety of ways. Humans use their voice,
gestures, and body language to convey their sentiments and
attitudes to others and to decipher the emotions, intentions,
and desires of other individuals [6]. Humanoid robots were
created in order to enable the direct social connection between
a human and a robot. These robots have human-like abilities
and resemble humans physically

Thus, this work presents an open-source VR interface that
implements the model of a social robot with an animatronic
face and a torso with two manipulators as arms. The developed
platform enables researchers to create and test applications in
the ﬁeld of HRI in a realistic manner, simulating actual robot
interaction. That strategy enhances the evaluation of social
robotics systems because it increases immersion and enables
an interaction to feel more like it would be in a real-world
setting.

The simulated robot includes a complete operational system
with ready-to-use tools that facilitates the development of
social HRI applications. Its system includes, for example,
computational vision capabilities, voice recognition, speech
synthesis, visual tracking control, joint control, and facial
expression settings. Also, the robot face was designed with
the intent to escape from the Uncanny Valley [7] concept,

 
 
 
 
 
 
which is a state where an object’s appearance is very similar
to a human’s, resulting in a negative emotional response to
it. Thus, it has a friendly appearance that aims to increase
comfort and encourage humans to engage in a conversation so
that the robot can respond socially, processing and returning
an answer by voice, gesture, or facial expressions.

The work is structured as follows: the related works are
discussed in Section II. In the Section III are explained
the systems and techniques used to build the complete
VR framework. The experiments realized are described in
Section IV, and a brief discussion about the experiments is
made in Section V. Finally, the main contributions and future
works are summarized in Section VI.

II. RELATED WORKS

The emerging technologies of Augmented Reality (AR),
Virtual Reality (VR), and Mixed Reality (XR) are making
possible the study of ﬁelds in robotics without the need for
expensive hardware. One of those areas of research is the
study of human-robot interactions (HRI). The work [8] eval-
uates existing VR-based solutions for simulating approaches
that target or embody HRI. Beyond just interaction, the
collaboration between humans and robots is signiﬁcant for
developing promising robots. Dianatfar et al. [9] reviews the
current status of virtual and augmented reality solutions in
human-robot collaboration.

In [10], Arntz et al. explores the interactions of nine
people with an AI-based representation of a robotic arm
in a virtual reality environment as they jointly assemble a
product with their robotic partner. This work demonstrates the
possibilities of VR when studying HRI, giving meaning to the
participants’ experiences without needing a real robot. The
work of Etzi et al. [11] assesses the human-robot interactions
during the execution of a collaborative task and demonstrates
that humans performance varies with the kinematic aspects of
a robot’s motion, but physiological responses don’t alter. With
such results in mind, we can assume that a VR simulation
of a robot has a semblance of interaction with a real robot.
With the increasing presence of robots in our daily lives,
their social aspects need to be worked on. Henschel et al.
works [12], studying human social cognition when interacting
with robots to understand how best to introduce robots to
complex social settings. One way of facilitating the research
on the social interactions between humans and robots is the
employment of VR settings. A Social Virtual Reality Robots
(V2R) [13] was conceived by Shahab et al. with the ability
to teach music to children with autism as well as perform
an automatic assessment of their behaviors. Shariaty et al.
transferred the model of the Arash robot to a VR setting [14].
This work found that the VR robot’s acceptance is reasonably
compatible with the actual robot since the performance of the
VR robot did not have signiﬁcant differences from the version
of the real Arash robot. These two works prove the concepts
presented in this work, that a social robot can interact with
humans through a virtual reality setting and still maintain the
social aspects of the real robot, enabling the studies of HRI
through VR.

(a) Front view.

(b) Isometric view.
Fig. 2: 3D model of the VR platform’s robot.

To be able to have meaningful interactions with a robot
in a VR setting, new tools need to be developed. One such
tool was designed by Inamura et al. SIGVerse [15], a cloud-
based VR platform to research multimodal HRI. Developed
with ROS and Unity, they show its feasibility in a robot
competition setting. Another simulator produced for virtual
reality is the work of Murnane et al. [16], which aims to
collect training data for real-world robots. Our work differs
from those previous works by focusing on the social aspect of
HRI, exploring it through spoken language and the interaction
with objects.

III. METHODOLOGY

In this section, the concept of our approach is introduced,
covering the development details of the research platform. We
present the robot model used in the framework, the developed
system that operates the robot, and the simulated virtual
environment created.

A. Robot Model

The robot model employed in this work contains an
animatronic face and a torso with arms attached to a ﬁxed
base. The face used comes from the robot Doris [17],
which participates in international robotics competitions in
the @home category. The humanoid’s animatronic face has
complete control of the movements of the jaw, neck, eyes,
eyelids, and eyebrows, making it possible to create different
facial expressions and communicate with humans through
ﬂuid movements.

The torso and manipulators contained in our platform come
from the Dimitri robot. Dimitri is an open-source full-body
biped humanoid robot [18]. As our approach does not involve
walking applications, the robot’s legs were replaced with a
ﬁxed base, where the torso was attached. Fig. 2 presents the
resulting robot’s 3d model applied to the VR platform.

Since it has a functional body and face, the resultant robot
model allows a variety of HRI applications, since it is capable
of doing facial expressions, visual tracking, speech movement,
and manipulation tasks. All our project is open-source, and the

Fig. 3: Scheme of ROS node connections structure.

complete robot’s URDF model is provided so that anyone can
use them to create a simulated environment in any supported
URDF robotics simulation platform.

B. Robot Operating System

Software for robots can be created using the Robot Oper-
ating System (ROS) [19], which offers a ﬂexible framework.
An operational system’s standard services, including hardware
abstraction, low-level device control, messaging between
processes, and package management, are provided by the
ROS set of tools and libraries. The set of ROS operations can
be represented by a graphs architecture where the processing
is performed on nodes that receive and send messages with
data related to sensors, control, state, planning, actuator, and
other topics that link those nodes. Although it is feasible to
connect ROS with real-time code, ROS is not a real-time
operating system, despite the importance of reduced latency
for robot control. To overcome this lack of a real-time system,
ROS 2 [20], which is employed in this work, is currently
being developed.

The ROS packages developed for simulated robots provide
researchers with a complete base of algorithms that enriches
the robot’s capabilities, such as computer vision, speech
recognition, and robot joint control. The Robot Operating
System is capable of communicating with the VR software
interface, sending and receiving data, allowing the system to
control the simulated robot and process sensors information
in real-time.

C. Virtual Reality Environment

The VR environment must contain a functional simulated
robot with controllable joints, working sensors, physics, and
collisions compatible with reality. Furthermore, to carry out
immersive communication with the simulated robot, we need
to build a virtual environment that contains elements that
enable social interaction, such as 3D visual models of different
objects and 3D avatars of people.

Unity is a multi-platform game and simulator creation
engine. Since it supports integration with robotics, offering a
variety of tools for developing VR aplications [21] and physics
simulation, it was chosen for developing our simulation
framework. Within the robotics tools offered by Unity, there is
communication with ROS and the import of Uniﬁed Robotics
Description Format (URDF) ﬁles [22].

The TCP/IP protocol [23], a widely used method that
establishes a connection between client and server before the

Fig. 4: VR environment.
data is delivered, is used for communication between ROS
and Unity. So, the scripts in Unity can exchange information
with the nodes from ROS, aiming to control the robot within
the simulation. A simpliﬁed overview of the ROS connection
with the VR framework is present in Fig. 3.

The virtual environment was built, containing a table with
various objects that the robot can grasp or the user can grab
in order to interact with the robot. Also, the user has a 3D
avatar attached to his head inside the simulation, with a set of
different facial expressions so that the robot can look directly
at the user and communicate. Next to the robot is a screen
that displays in real-time the image that is captured by the
camera sensor. And, in order for the researcher, for example,
to check if the communication is working or to be able to
detect errors in some developing ROS node, a debug console
screen was placed inside the virtual environment, above the
table. The VR environment can be seen in Fig. 4.

IV. EXPERIMENTS

In this section, some experiments are presented and could
demonstrate our robot’s social capabilities by having it interact
with humans, do facial communication, and execute other
activities related to the HRI ﬁeld, as explored in famous
studies [24], [25].

A. Visual Object and Face Tracking

HRI’s social applications highly depend on visual tracking
algorithms, as shown in [26], since they enable the robot to
look at people and things. Our framework provides tracking
options that can be made through neck and eye movement.
The camera sensor in the robot’s nose hole detects the target
to be followed, and joints are controlled in accordance with
the target location.

The tests in this section explore the robot’s visual tracking
effectiveness in the simulated world. The robot was initially
instructed to focus on a speciﬁc color. A computer vision
algorithm ﬁnds the item with the required color once the
speech instruction is recognized. For instance, a human can
test the algorithm by engaging with the virtual environment
and manipulating the target objects, changing their positions.
Fig. 5 shows that interaction. The VR environment also
includes a screen that displays the robot’s camera perspective
for improved user visualization.

Unity VRCameraJointsAudioTCPConnectorROSManipulationVisionFace ControllerAudioConnections DiagramFig. 5: Visual object tracking while human manipulates it.

Second, face detection and tracking is used to test the
humanoid ability to track faces visually. By giving the ”Look
at me” voice command, the robot instantly starts gazing at the
human in the simulation. For HRI, that deﬁnes an essential
ability, as discussed previously.

B. Face Recognition

The framework includes an easy-to-use, full-featured facial
recognition capability to enhance user immersion when
interacting with the robot. Thanks to this utility, researchers
can create better interaction algorithms once the robot is
able to save and use data about speciﬁc people. For example,
a routine can be developed where the robot can recognize
familiar faces and address people by name, remembering
personal details about them, and bring it to the human-robot
dialogue, which is very important in the human view [27].
Various 3D face models are available through the VR
framework platform, including multiple people and facial
expressions. These 3D avatars were used to explore the face
recognition capabilities of the robot, and that presented good
results, showing that it is capable of recognizing all the
simulated avatars correctly. A sample of different 3D avatars
provided by the VR platform is presented in Fig. 6.

C. Facial Expressions

A robot must be able to make facial expressions and identify
them in order to engage and communicate with people more
effectively [28]. This experiment shows how the robot may
convey his emotions through his expressions. Their simplicity
and quality are essential in the social HRI ﬁeld, making this
part vital to the framework. Fig. 7 shows the set of different
facial expressions performed by the robotic face.

Additionally, the framework offers a preprogrammed fea-
ture of facial expression recognition. Through this, researchers
can develop applications where, for example, the robot can
mimic a human’s mood and understand the context of the
dialogue by analyzing human emotion. The recognizer system
was tested with the platform 3D avatars and showed excellent
accuracy in expressions of joy, sadness, surprise, neutral, and
anger; meanwhile, it presented a poor performance with 3D
avatars containing fear and disgust facial expressions.

Fig. 6: Samples of the framework’s 3D avatar set.

D. Manipulators

In order to enrich the development possibilities of HRI
applications, our platform offers not only an animatronic face
but also a functional robotic body composed of a ﬁxed base,
a torso, and two manipulators as arms. That structure allows
the performance of body poses and gestures through ﬂuid
motions and more complex activities, like manipulation tasks.
This broadens the scope of applications that can be created
utilizing the framework.

The robot can therefore interact with things in the VR
world, in addition to interacting with people, as seen in Fig. 8,
which demonstrates the manipulator grabbing an object. This
competency was created to make it easier for developers
to construct and test movement patterns without using a
real robot, speeding up the creation of any application that
uses a similar manipulator with the same structure. In this
experiment, an object was picked by the robot, lifted, and
placed back on the table via a series of preprogrammed
moves.

E. Operating the Robot

Still focusing on the simulated robot body capacities, the
VR platform provides the capability for users to operate the
manipulators and neck in real-time. The operation of the
simulated robot is made through VR controllers and a VR
headset, in which each 6DOF controller operates a single
manipulator, and the headset orientation controls the robot
neck. Fig. 8 shows that the user is capable of controlling the
robot arms by doing natural human movements.

This approach allows, for example, applications of im-
itation, where the robot mimics the human gesture while
interacting and enables the recording of preprogrammed ﬂuid
motions, which can be acquired through natural movements
made by the user and applied in the simulated robot afterwards.
Furthermore, the operation of the robot can be made from a
ﬁrst-person perspective, where the user has the possibility to
control the robot joints from the robot’s visual perspective.
This technique can be implemented to teleoperate a real robot
with the same physical structure [29].

V. DISCUSSION

The experiments session demonstrated the effectiveness of
the VR approach for executing and testing HRI applications.
As a platform made for researchers, numerous facilities have
been created to aid the framework’s user in more efﬁciently
developing his study. For example, users do not need to worry
about minor code implementations because our framework
offers a variety of easy-to-use and full-featured implemented
algorithms.

As demonstrated in the IV-A subsection, our architecture
already includes visual tracking for objects or people’s faces.
With the help of these tools, researchers can develop studies
in behavioral analysis and attention-biased experiments, as
well as perform a variety of tests with a focus on color-based
visual tracking and the functionality of easy recognition,
attention, and optical monitoring of an individual or group of
people interacting socially with the robot. The neck and eye

(a) Neutral.

(b) Joy.

(c) Sadness.

(d) Surprise.
Fig. 7: Robot facial expressions.

(e) Disgust.

(f) Anger.

(g) Fear.

tracking make the immersive experience more humanized to
the VR user.

As mentioned in IV-B subsection, the framework’s built-in
face recognition technology enables the researcher to engage
in more precise interactions that increase the user’s sense of
immersion and reliability while interacting with the robot.
Also, as demonstrated in IV-C subsection, the humanoid’s
face can produce different emotions to provide rich social
interaction with humans. The ability to make and recognize
facial expressions opens up a variety of possibilities for study
into the psychological and behavioral underpinnings.

Our platform provides simple access to control the robotic
manipulation system tested in the IV-D subsection. From there,
it is possible to conduct experiments by grabbing things and
trading them with the robot [30]. Using image-based pick
and place and grasping algorithms, the researcher can quickly
enhance the framework manipulation system [31]. To do that,
the developer only needs to communicate the algorithm with
the framework’s established ROS nodes. The researcher will
thus be free to investigate new manipulation approaches and
subsequently create additional HRI applications.

Also,

the fact

that voice commands were employed
throughout all
the previous experiments as the primary
mean of communication between the user and the humanoid
robot encourages the implementation of our methodology
in additional research ﬁelds. In many contexts, the usage of
educational robots in teaching sectors has already been shown
to be effective [32], [33]. For instance, this sort of application
can be enhanced by using the platform suggested in this
study to elaborate robotics and programming lessons and
demonstrations that involve direct student-robot interaction.
It could be achieved in future works by using a multiplayer
VR feature that enables multiple users to access the same
virtual world and interact with the robot and each other.

Furthermore, it is essential to emphasize that the platform
is not limited to being used just for human-robot social
interaction situations. As demonstrated in IV-E subsections,
the framework offers an operating functionality that allows
the user to control the joints of the robot. That broadens the
possibility of exploration in studies involving manipulation
tasks. For example, applying imitation learning after collecting
demonstrations of humans controlling the robot manipulator
to accomplish determined tasks [34].

Finally, since the framework allows roboticists to test
and evaluate their algorithms and experiments without the
requirement of the actual robot, considerably speeding up
the evaluation process, our platform makes it possible to
test, for example, computer vision and reinforcement learning
algorithms without physical hardware and actual environment.
In this sense, the researcher can manage their methods and
certify their results interactively with the VR environment
through the framework.

VI. CONCLUSIONS

In this work, it was presented an open-source robotics
framework for a virtual reality setting that enables an
immersive and realistic human-robot interaction experience,
which is possible even without the actual robot’s hardware.
The platform provides an excellent tool for any research and
development in the ﬁeld of HRI since it has tremendous
potential for testing and developing new functionalities for
the robot while signiﬁcantly lowering costs, robot damage
risks, and evaluation test time.

The trials have shown the robot’s aptitude for expressing
his emotions, recognizing speech and facial expressions,
visually tracking and interacting with people and objects, and
being operated by the user. The robot performed well in the
experiments explored in this work, showing that researchers

Fig. 8: Human operating robot manipulators through VR controllers.

can use our framework’s pre-built features to develop more
complex simulated or even real-world applications.

tool

Hence, the experiments demonstrate that a VR platform
is a suitable and powerful
in the construction of
HRI applications, in which the robot can perform sociable
interactions with humans, similar to a real situation. In order
to enhance the framework and increase the simulated robot’s
capabilities, there are many functionalities to be implemented,
for example, object recognition, emotion extraction from
human spoken words, a set of preprogrammed body gestures,
and VR multiplayer feature.

ACKNOWLEDGEMENT

The authors would like to thank the VersusAI team.

REFERENCES

[1] J. T. C. Tan and T. Inamura, “Sigverse-a cloud computing architecture
simulation platform for social human-robot interaction,” in IEEE
International Conference on Robotics and Automation.
IEEE, 2012,
pp. 1310–1315.

[2] O. Liu, D. Rakita, B. Mutlu, and M. Gleicher, “Understanding
human-robot interaction in virtual reality,” in 26th IEEE international
symposium on robot and human interactive communication (RO-MAN).
IEEE, 2017, pp. 751–757.

[3] A. A. Malik and A. Brem, “Digital twins for collaborative robots:
A case study in human-robot interaction,” Robotics and Computer-
Integrated Manufacturing, vol. 68, p. 102092, 2021.

[4] A. Zacharaki, I. Kostavelis, A. Gasteratos, and I. Dokas, “Safety bounds
in human robot interaction: A survey,” Safety Science, vol. 127, p.
104667, 2020.

[5] M. Goodrich and A. Schultz, “Human-robot interaction: A survey,”
Foundations and Trends in Human-Computer Interaction, vol. 1, pp.
203–275, 01 2007.

[6] R. Plutchik, “Emotions: A general psychoevolutionary theory,” Ap-

proaches to emotion, vol. 1984, no. 197-219, pp. 2–4, 1984.

[7] M. et al, “The uncanny valley [from the ﬁeld],” IEEE Robotics &

Automation Magazine, vol. 19, no. 2, pp. 98–100, 2012.

[8] S. Kaszuba, F. Leotta, and D. Nardi, “A preliminary study on virtual
reality tools in human-robot interaction,” in Augmented Reality, Virtual
Reality, and Computer Graphics, L. T. De Paolis, P. Arpaia, and
P. Bourdot, Eds. Cham: Springer International Publishing, 2021, pp.
81–90.

[9] M. Dianatfar, J. Latokartano, and M. Lanz, “Review on existing vr/ar
solutions in human–robot collaboration,” Procedia CIRP, vol. 97, pp.
407–411, 2021, 8th CIRP Conference of Assembly Technology and
Systems.

[10] A. Arntz and S. C. Eimler, “Experiencing ai in vr: A qualitative
study on designing a human-machine collaboration scenario,” in HCI
International 2020 – Late Breaking Posters, C. Stephanidis, M. Antona,
and S. Ntoa, Eds. Cham: Springer International Publishing, 2020, pp.
299–307.

[11] Using Virtual Reality to Test Human-Robot Interaction During a
Collaborative Task, ser. International Design Engineering Technical
Conferences and Computers and Information in Engineering Confer-
ence, vol. Volume 1: 39th Computers and Information in Engineering
Conference, 08 2019, v001T02A080.

[12] A. Henschel, R. Hortensius, and E. S. Cross, “Social cognition in the
age of human–robot interaction,” Trends in Neurosciences, vol. 43,
no. 6, pp. 373–384, 2020.

[13] M. Shahab, A. Taheri, S. R. Hosseini, M. Mokhtari, A. Meghdari,
M. Alemi, H. Pouretemad, A. Shariati, and A. G. Pour, “Social virtual
reality robot (v2r): A novel concept for education and rehabilitation of
children with autism,” in 5th RSI International Conference on Robotics
and Mechatronics (ICRoM), 2017, pp. 82–87.

[14] A. Shariati, M. Shahab, A. Meghdari, A. Amoozandeh Nobaveh,
R. Rafatnejad, and B. Mozafari, “Virtual reality social robot platform:
A case study on arash social robot,” in Social Robotics, S. S. Ge, J.-J.
Cabibihan, M. A. Salichs, E. Broadbent, H. He, A. R. Wagner, and
´A. Castro-Gonz´alez, Eds. Cham: Springer International Publishing,
2018, pp. 551–560.

[15] T. Inamura and Y. Mizuchi, “Sigverse: A cloud-based vr platform for
research on multimodal human-robot interaction,” Frontiers in Robotics
and AI, vol. 8, 2021.

[16] M. Murnane, P. Higgins, M. Saraf, F. Ferraro, C. Matuszek, and
D. Engel, “A simulator for human-robot interaction in virtual reality,”
in IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts
and Workshops (VRW), 2021, pp. 470–471.

[17] B. S. Castro, C. B. da Costa, C. D. Werlang, D. R. Richards, D. S.
Brum, G. N. Niederauer, G. H. Nascimento, I. P. Maurell, J. P. C. R.
Jardim, K. H. Guimaraes, J. C. Jesus, L. S. Avila, L. A. C. Junior,
L. F. T. Franc¸a, P. L. J. Drews-Jr, R. T. Fonseca, R. B. Grando, R. S.
Guerra, and V. A. Kich, “Butia larc/cbr 2020 robocup@home team
description paper,” 2020.

[18] C. Tatsch, A. Ahmadi, F. Bottega, J. Tani, and R. S. Guerra, “Dimitri:
an open-source humanoid robot with compliant joint,” Journal of
Intelligent & Robotic Systems, pp. 1–10, 2017.

[19] S. Hart, P. Dinh, and K. Hambuchen, “The affordance template ros
package for robot task programming,” in IEEE international conference
on robotics and automation (ICRA).

IEEE, 2015, pp. 6227–6234.

[20] J. Kay and A. R. Tsouroukdissian, “Real-time control in ros and ros

2.0,” ROSCon15, 2015.

[21] V. T. Nguyen and T. Dang, “Setting up virtual reality and augmented
reality learning environment in unity,” in IEEE International Symposium
on Mixed and Augmented Reality (ISMAR-Adjunct), 2017, pp. 315–320.
[22] A. Hussein, F. Garc´ıa, and C. Olaverri-Monreal, “Ros and unity based
framework for intelligent vehicles control and simulation,” in IEEE
International Conference on Vehicular Electronics and Safety (ICVES),
2018, pp. 1–6.

[23] W. Meng, Y. Hu, J. Lin, F. Lin, and R. Teo, “Ros+unity: An efﬁcient
high-ﬁdelity 3d multi-uav navigation and control simulator in gps-
denied environments,” in IECON 2015 - 41st Annual Conference of
the IEEE Industrial Electronics Society, 2015, pp. 002 562–002 567.

[24] J. Miˇseikis, P. Caroni, P. Duchamp, A. Gasser, R. Marko, N. Miˇseikien˙e,
F. Zwilling, C. de Castelbajac, L. Eicher, M. Fr¨uh, and H. Fr¨uh,
“Lio-a personal robot assistant for human-robot interaction and care
applications,” IEEE Robotics and Automation Letters, vol. 5, no. 4, pp.
5339–5346, 2020.

[25] T. B. Sheridan, “Human–robot interaction: Status and challenges,”
Human Factors, vol. 58, no. 4, pp. 525–532, 2016, pMID: 27098262.
[26] J. Zhou, L. Feng, R. Chellali, and H. Zhu, “Detecting and tracking
objects in hri: Yolo networks for the nao “i see you” function,” in
27th IEEE International Symposium on Robot and Human Interactive
Communication (RO-MAN), 2018, pp. 479–482.

[27] A. Khalifa, A. A. Abdelrahman, D. Strazdas, J. Hintz, T. Hempel,
and A. Al-Hamadi, “Face recognition and tracking framework for
human–robot interaction,” Applied Sciences, vol. 12, no. 11, p. 5568,
2022.

[28] Z. Liu, M. Wu, W. Cao, L. Chen, J. Xu, R. Zhang, M. Zhou, and
J. Mao, “A facial expression emotion recognition based human-robot
interaction system,” IEEE/CAA Journal of Automatica Sinica, vol. 4,
no. 4, pp. 668–676, 2017.

[29] A. Franzluebbers and K. Johnson, “Remote robotic arm teleoperation
through virtual reality,” in Symposium on Spatial User Interaction,
2019, pp. 1–2.

[30] A. Mathur, C. Bansal, S. Chauhan, and O. Yadav, “A review of pick
and place operation using computer vision and ros,” Computational
and Experimental Methods in Mechanical Engineering, pp. 411–418,
2022.

[31] Q. Bai, S. Li, J. Yang, Q. Song, Z. Li, and X. Zhang, “Object detection
recognition and robot grasping based on machine learning: A survey,”
IEEE Access, vol. 8, pp. 181 855–181 879, 2020.

[32] E. Ospennikova, M. Ershov, and I. Iljin, “Educational robotics as an
inovative educational technology,” Procedia - Social and Behavioral
Sciences, vol. 214, pp. 18–26, 2015, worldwide trends in the develop-
ment of education and academic research, Soﬁa, Bulgaria, 15-18 June,
2015.

[33] S. Anwar, N. A. Bascou, M. Menekse, and A. Kardgar, “A systematic
review of studies on educational robotics,” Journal of Pre-College
Engineering Education Research (J-PEER), vol. 9, no. 2, p. 2, 2019.
[34] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and
P. Abbeel, “Deep imitation learning for complex manipulation tasks
from virtual reality teleoperation,” in IEEE International Conference
on Robotics and Automation (ICRA), 2018, pp. 5628–5635.

