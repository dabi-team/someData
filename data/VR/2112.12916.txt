1
2
0
2
c
e
D
4
2

]

V
C
.
s
c
[

1
v
6
1
9
2
1
.
2
1
1
2
:
v
i
X
r
a

Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition

Yue He1, Chen Chen2, Jing Zhang2, Juhua Liu3*, Fengxiang He4, Chaoyue Wang2, Bo Du1*
1 National Engineering Research Center for Multimedia Software, Institute of Artiﬁcial Intelligence, School of Computer
Science and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, China
2 School of Computer Science, Faculty of Engineering, The University of Sydney, Australia
3 School of Printing and Packaging, and Institute of Artiﬁcial Intelligence, Wuhan University, China
4 JD Explore Academy, China
{yuehe.cs, liujuhua, dubo}@whu.edu.cn, cche9000@uni.sydney.edu.au, {jing.zhang1, chaoyue.wang}@sydney.edu.au,
hefengxiang@jd.com

Abstract

Existing Scene Text Recognition (STR) methods typically
use a language model to optimize the joint probability of the
1D character sequence predicted by a visual recognition (VR)
model, which ignore the 2D spatial context of visual seman-
tics within and between character instances, making them not
generalize well to arbitrary shape scene text. To address this
issue, we make the ﬁrst attempt to perform textual reasoning
based on visual semantics in this paper. Technically, given the
character segmentation maps predicted by a VR model, we
construct a subgraph for each instance, where nodes represent
the pixels in it and edges are added between nodes based on
their spatial similarity. Then, these subgraphs are sequentially
connected by their root nodes and merged into a complete
graph. Based on this graph, we devise a graph convolutional
network for textual reasoning (GTR) by supervising it with a
cross-entropy loss. GTR can be easily plugged in representa-
tive STR models to improve their performance owing to bet-
ter textual reasoning. Speciﬁcally, we construct our model,
namely S-GTR, by paralleling GTR to the language model in
a segmentation-based STR baseline, which can effectively ex-
ploit the visual-linguistic complementarity via mutual learn-
ing. S-GTR sets new state-of-the-art on six challenging STR
benchmarks and generalizes well to multi-linguistic datasets.
Code is available at https://github.com/adeline-cs/GTR.

1

Introduction

Scene Text Recognition (STR) remains a fundamental and
active research topic in computer vision for its wide appli-
cations (Zhang and Tao 2020). However, this task remains
challenging in real-world deployment, since the recognition
results are highly inﬂuenced by various factors, such as com-
plex background, irregular shapes, diverse textures.

Existing methods mainly treat STR as a visual recog-
nition (VR) task and perform character-level recognition
on input images, including visual-text sequence translation-
based methods (Yang et al. 2017; Shi et al. 2018; Baek
et al. 2019; Li et al. 2019; Litman et al. 2020) and seman-
tic segmentation-based methods (Liao et al. 2019; Wan et al.

*Corresponding author. This work was done during Yue He’s

internship at JD Explore Academy.
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Diagram of different STR pipelines. (a) The VR
model. (b) The VR model with an LM. (c) The proposed
pipeline by adding GTR in parallel with LM. GTR performs
textual reasoning based on the visual semantics generated by
VR to address the irregular and blurry text. The ground truth
label is “crocs” and wrong predictions are marked in red.

2020a). Although these methods obtain reasonable perfor-
mance on identifying individual characters, they ignore vital
global textual representations, making it extremely hard to
give robust recognition outcomes in real-world scenarios.

For global textual modeling, existing efforts (Qiao et al.
2020; Yu et al. 2020; Fang et al. 2021) have been made to
leverage a language model (LM) (Jaderberg et al. 2014a)
to optimize the joint probability of the character sequence
predicted by the VR model. Though this strategy can cor-
rect mistaken predictions with linguistic context, it is hard to
generalize to arbitrary texts and ambiguous cases. As shown
in Fig. 1(b), for the irregular and blurry text, even LM could
not make correct predictions. Other than linguistic cues, spa-
tial context could also contribute to global textual modeling
of character sequences but few methods explore in this di-
rection. Hence, existing models have difﬁculty in produc-
ing satisfactory results on irregular texts in diverse fonts and
shapes as well as with blur and occlusions.

In this paper, we ﬁll this gap with a novel Graph-based
Textual Reasoning (GTR) model for introducing spatial
context into the text reasoning stage. Given the character
instances recognized by the VR model as well as the de-
rived order relations between them, we ﬁrst set up a two-
level graph to establish the local-to-global dependency. In

VRLMGTRBase ModelTextual ReasoningcrooscrosscrocsLinguisticContextSpatialContextRaw Prediction(a)(b)(c)VRVRLM 
 
 
 
 
 
the ﬁrst level, we construct a subgraph for pixels within each
character instance based on their spatial similarity. And for
the second level, 1-st level subgraphs are merged into a com-
plete graph by linking their root nodes, which represent the
geometric center of all nodes within each subgraph. Accord-
ingly, we devise a graph convolutional neural network for
context reasoning and optimizing the joint probability of the
character sequence.

Our proposed GTR is an easy-to-plug-in module and can
seamlessly function with other context modalities. Speciﬁ-
cally, we put GTR parallel to the LM to produce joint fea-
tures for text reasoning (as shown in Fig. 1(c)). To pro-
duce high-quality cross-modality representations, we design
a mutual learning protocol to enforce the consistency be-
tween predictions from LM and GTR and employ a dynamic
fusion strategy (Yue et al. 2020) to deeply combine visual
and linguistic features. Based on these designs, GTR can
largely boost the text reasoning performance comparing to
existing representative methods with LM only.

We incorporate all aforementioned designs

into a
segmentation-based STR baseline and propose S-GTR, a
uniﬁed framework of Segmentation baseline with GTR. We
evaluate S-GTR on multiple datasets with both regular and
irregular text material in different languages. Experimental
results show that our S-GTR outperforms previous methods
and obtains state-of-the-art performance on six challenging
benchmarks. In summary, the contribution of this work is
threefold:

• We propose a novel graph-based textual reasoning model
named GTR to reﬁne coarse text sequence predictions
with spatial context. It is a complementary design to the
popular reasoning fashion with LM only in existing rep-
resentative methods, and can further improve their per-
formance by acting as an easy-to-plug-in module.

• To make GTR work with LM compatibly, we further em-
ploy a mutual learning protocol and propose a dynamic
fusion strategy to produce consistent linguistic and visual
representations and high-quality joint prediction.

• We put all our designs in a uniﬁed framework (S-GTR)
of segmentation baseline with GTR. Extensive experi-
mental results indicate our S-GTR successfully sets new
state-of-the-art for regular and irregular text recognition
tasks as well as shows a superiority on both English and
Chinese text materials.

2 Related Work
Arbitrary-shaped Scene Text Recognition. Existing
STR methods for recognizing texts of arbitrary shapes
can be divided into two main categories, i.e., rectiﬁcation-
based methods and segmentation-based methods. The for-
mer methods (Gao et al. 2018; Yang et al. 2017; Cheng et al.
2018) use the spatial transformer network (Jaderberg et al.
2015) to normalize text images into the canonical shape,
i.e., horizontally aligned characters of uniform heights and
widths. These methods, however, are limited by the pre-
deﬁned text transformation set and hard to generalize to
real-world examples. The latter methods (Liao et al. 2019;
Wan et al. 2020a) follow the common processing fashion

in the text detection task (Ye et al. 2021) and formulate the
recognition task as an instance segmentation problem, where
texts are explicitly modeled into instance masks. In this way,
it can efﬁciently represent irregular texts in different fonts,
scales, orientation, and shapes, as well as with occlusions.
For this reason, we choose to build our GTR model upon a
base instance segmentation-based recognition model. In ad-
dition, since the segmentation probability maps embed use-
ful semantics and spatial context, the propose GTR model
can efﬁciently exploit them for text reasoning.

Semantic Context Reasoning. To further enhance the text
recognition performance, some methods resort to linguistic
context to improve raw outputs from the VR model. For ex-
ample, (Cheng et al. 2017) employ CNN to yield bags of N-
grams of text string for output reasoning. Some later meth-
ods (Wang et al. 2020; Wojna et al. 2017) leverage RNN
to strengthen context dependencies between characters. Re-
cently, some methods adopt semantic context reasoning to
achieve high performance. SEED (Qiao et al. 2020) pro-
poses to use word embedding from FastText (Bojanowski
et al. 2017), which relies on semantic meaning of a word
instead of its visual appearance. SRN (Yu et al. 2020) uses
transformer-based models where global semantic informa-
tion as well as long-range word dependency is modelled by
self-attention. It it computationally efﬁcient due to the par-
allel nature of transformer architecture like (Xu et al. 2021),
but their non-differentiable semantic reasoning block im-
poses a signiﬁcant limitation. Based on SRN, ABINet (Fang
et al. 2021) adopts the iterative correction for enhancing se-
mantic reasoning. Beyond semantic reasoning, we propose
a graph-based context reasoning model that supplements the
language model to exploit both visual spatial context and
linguistic context to improve the visual recognition results.

Graph-structure Data Reasoning. Considerable efforts
have been made to design graph convolutional neural net-
works (GCN) for modelling graph-structured data (Kipf and
Welling 2017; Chen et al. 2019; Wang et al. 2019). For ex-
ample, in the text detection task, (Zhang et al. 2020) adopts
a GCN to link characters that belong to the same word. GTC
(Hu et al. 2020) utilizes a GCN to guide CTC (Graves et al.
2006) for scene text recognition. Speciﬁcally, it models the
sliced visual features as the graph nodes, captures their de-
pendency, and merges features of the same instance for pre-
diction. PREN2D (Yan et al. 2021) adopts a meta-learning
framework to extract visual representations via GCN. In this
paper, we devise a two-level graph network based on GCN to
perform spatial context reasoning within and between char-
acter instances to reﬁne the visual recognition results.

3 Methodology

3.1 Overview
The full framework of S-GTR is shown in the Figure
2, which comprises a segmentation-based VR model, an
LM, and our proposed GTR. Given the input image X ∈
RH×W ×3, the segmentation-based VR ﬁrst produces a seg-
mentation map M ∈ RH×W ×C, where C is the number
of character classes. The segmentation map M is decoded

Figure 2: Overview of the proposed S-GTR model. It consists of a VR model, a LM, and the proposed GTR. GTR is stacked
on the top of the VR model and in parallel with the LM. The detailed structure of GTR as well as a pre-processing step, i.e.,
feature ordering, are also shown in the bottom part of this ﬁgure. More details can be found in Section 3.

to a preliminary text sequence prediction T ∈ RT ×C and
further processed by LM for generating linguistic context
vector L ∈ RT ×C. T is the pre-deﬁned maximum length of
output sequence.

Our proposed GTR is stacked in parallel with LM, taking
the segmentation map M as input. Firstly, we transform the
map M with a feature ordering module to build an ordered
feature tensor V ∈ RT ×H×W ×C, which comprises T at-
tention maps and represents the relations between geometry
features and text order information. Next, we build a sub-
graph for each attention map and then connect all sub-graphs
sequentially into a full graph. The graph is then deeply en-
coded with a GCN to produce the spatial context vector
S ∈ RT ×C. Finally, the coarse sequence prediction T, the
linguistic context L and the spatial context S are combined
via dynamic fusion and the reﬁned text is predicted.

3.2 GTR: Graph-based Textual Reasoning
Given the segmentation map M, we employ a fully convo-
lutional network (FCN) to obtain a series of attention maps
related to the character order and use them to attend M via
element-wise multiplication to get the ordered feature ten-
sor V ∈ RT ×H×W ×C, as shown in the bottom left part of
Figure 2. Based on V, GTR ﬁrstly builds sub-graphs for all
character instances and connects them sequentially. Then,
the graph is encoded with a GCN and pooling operation to
produce spatial context.

Graph Generation We build the two-level graph from the
ordered feature tensor V to model the local-to-global depen-
dency. We ﬁrst connect pixels belonging to the same charac-
ter in the 1-st level sub-graph. Speciﬁcally, for the i-th or-
dered feature map Vi ∈ RH×W ×C, we ﬁrst choose pixels
having the same estimation to the i-th character in the text

Figure 3: Illustration of the subgraph building process, in-
cluding node feature generation and edge matrix compute.

sequence predicted by VR. These pixels are collected as a
set Pi = {(x, y, R)j}, where R is the C-dim feature vector
in the position (x, y) of Vi and j is the pixel index. Note that
we also add a root node with average x, y and R to the set.
Then we construct the node feature vector Xi,j by embed-
ding x, y and R with three different 1 × 1 convolutions and
i with sine and cosine functions. The four parts of embed-
dings are concatenated to form node features.

Next, the adjacent matrix is built according to node sim-
ilarities. We compute the node similarity as a product be-
tween the position similarity Ep and the feature similarity
Ef , which is deﬁned as:

Ep(p, q) = 1 −

D ((xp, yp), (xq, yq))
max (H, W )

,

Ef (p, q) =

Rp · Rq
(cid:107)Rp, (cid:107) (cid:107)Rq(cid:107)

,

(1)

(2)

E(p, q) = Ep(p, q) · Ef (p, q),
(3)
where p and q are two nodes from the set Pi. The position
similarity Ep is negatively proportional to the Euclidean dis-

Prediction  Input Image   Language Model“BAKER”............Feature Ordering PoolingPoolingReadoutPooling...ccConv1Conv2GCNccConv1Conv2GCNLinear…Node CheckNode CheckRoot NodeNode feature  X2GTRDynamic Fusion...Context Vector  ... Ordered Features   ............KL(P1|P2)KL(P2|P1)Seg-based Visual Recognition ModelSegmentation Map  HxWxCHxWxCTxCHxWx3PredictionxT............Segmentation Map  Receptive FieldFCN Feature Ordering  Ordered Features   Pooling Top Rank Select NodeGraph-based Textual Reasoning Model（GTR）Conv1Conv2GCNConv1Conv2GCNUpdate Adj-MatrixNodeSimilarity ComputingSine-cosineFunction1x1 ConvOrder 𝑖Node Features 𝑋𝑖𝐺𝑖(𝑋𝑖,𝐴𝑖)1001…0⋯1⋮⋮1…⋱⋮…1Edge Matrix𝐴𝑖𝑉𝑖𝑥,𝑦,𝑅×3tance between two pixels whereas the feature similarity Ef
is the cosine similarity between pixel features. The product
of Ep(p, q) and Ef (p, q) is the overall similarity E between
node p and q. Then, we use the 1-hop rule (Wang et al. 2019)
to build the adjacent matrix Ai. Speciﬁcally, we connect
each node in Vi to other nodes that have the top-8 largest
similarity and delete the connections to the nodes outside
the 1-hop cluster.

After constructing sub-graphs Gi(Xi, Ai), we connect
them into the 2-level full graph by linking their root nodes
in sequence order. The full graph is denoted as G(X, A).

Spatial Context Reasoning Given a graph G(X, A), we
try to use the graph convolutional network to perform two-
stage spatial context reasoning by following (Zhang et al.
2020; Kipf and Welling 2017).

The ﬁrst stage is spatial reasoning. After obtaining the
feature matrix X and the adjacency matrix A, we use a graph
convolutional network to output a transformed node feature
matrix Y . This process can be described as follows:

Y l = σ([X l; KX l]W l), l = 1...L,

(4)

K = ˜D

−1

2 ˜A ˜D

−1
2 .

(5)
Here, l denotes the layer index, L = 2, X(l) ∈ RN ×di,
Y (l) ∈ RN ×do, di and do are the dimension of input and
output node features, and N is the number of nodes. [; ]
represents concatenation. W l is a layer-speciﬁc trainable
weight matrix. σ denotes a non-linear activation function. K
is an aggregation matrix of size N × N , which is computed
according to (Kipf and Welling 2017). Note that X l+1 = Y l,
i.e., the output feature matrix Y l is used as the input of the
l + 1th layer.

After spatial reasoning, we perform the contextual rea-
soning. Denoting the output graph feature matrix from the
aforementioned graph convolutional network as X l
c, we
compute a new adjacency matrix Ac based on X l
c. Then, we
calculate G according to Eq. (5) based on Ac. Next, we use
a graph convolutional network to output a transformed node
feature matrix Y l
c as follows:
c = σ((GX l
Y l

c)W l
c is a layer-speciﬁc trainable weight matrix.
Then, we perform root node check to make sure the se-
lected root node is the underlying reliable root node, i.e., the
center of the character instance. In this way, it achieves the
balance between the edges with near easy nodes and distant
hard nodes by satisfying the following criterion:

c), l = 1...L,

where W l

(6)

Giou =

Gr ∩ Gs
Gr ∪ Gs

< ε,

(7)

where Gr and Gs are two subgraphs for a same character,
given that s is a randomly selected node as the root node
while r is always the center of character. Gr ∩ Gs and Gr ∪
Gs are the intersection and the union of 1-hop neighbors of
Gr and Gs, respectively. In our experiments, ε is set to 0.75.
Next, we use a readout layer like (Lee, Lee, and Kang
2019) to aggregate node features to a ﬁxed-size representa-
tion. The output feature of this layer is calculated as follows:

i = [xi; max(x∗
x∗

j |j ∈ N (xi))],

(8)

where x∗
j is the updated feature at jth node, which is also
calculated according to Eq. (8), i.e., x∗
i is calculated in a
recursive manner. N (xi) denotes the neighboring node set
of node i. After we obtain the updated node features, we
discard 50% nodes that are most distant to the root node,
i.e., pooling the graph into a smaller new one. We iteratively
repeat the feature update and pooling process until only a
node exists in the subgraph, resulting in a node sequence.
Finally, the feature representations of the node sequence are
passed to a linear layer for classiﬁcation. We adopt the soft-
max cross-entropy loss for optimizing graph convolutional
neural network. Similar to (Wang et al. 2019), we only back-
propagate the gradient for nodes in the 1-hop neighborhood
during training.

3.3 S-GTR: A Simple Baseline for STR
We incorporate our GTR to a popular segmentation-based
VR model with LM, resulting in a simple baseline for STR,
i.e., S-GTR, as shown in Figure 2. Speciﬁcally, the VR
model is designed following (Wan et al. 2020b), and the LM
is based on SRN. We devise manifold training strategies to
make GTR better support the the STR task.

Context Consistency Since we have two different types
of reasoning features, namely linguistic context and spatial
context. To prevent S-GTR from overly relying on one of
them and avoid inconsistent reasoning cues to provide am-
biguous results, we propose a mutual learning strategy to en-
force the consistency between the two types of context fea-
tures. Speciﬁcally, we compute the Kullback Leibler (KL)
divergence between L from LM and S from GTR.

Dynamic Fusion Following (Yue et al. 2020) that uses a
dynamic fusion module to fuse information from multiple
domains, we extend it in S-GTR to combine three different
text sequences from VR, LM and GTR.Formally,

Qi = Sigmoid(W0[Ti; Li; Si]),
Zi = Qi (cid:12) (W1[Ti; Li; Si]),

(9)

where Ti, Li, Si are prediction vectors for the i-th charac-
ter. W0 and W1 are two learnable linear transformations and
(cid:12) indicates the element-wise multiplication operation. Zi is
the ﬁnal output of S-GTR for the i-th character.

Mean Teacher-based Syn-to-Real Adaptation To miti-
gate the domain shift issue when using both synthetic and
real datasets for training, we adopt the popular mean teacher
framework (Tarvainen and Valpola 2017) in the area of do-
main adaptation. Speciﬁcally, a teacher network with the
identical architecture as the segmentation-based VR model
(i.e., student network) is built and its weights are the expo-
nential moving average of those of the student network.

Loss Function The overall loss contains three parts, in-
cluding sequence prediction loss LSeq, the LM-GTR consis-
tency loss LCC, and the mean-teacher training loss LMT:

L = λSeg ∗ LSeg + λCC ∗ LCC + λMT ∗ LMT.

(10)

LSeg contains a cross-entropy loss for character classiﬁca-
tion and a smooth L1 loss for order segmentation. LCC is the

Methods

Training Data

CRNN (Shi, Bai, and Yao 2016)
FAN* (Cheng et al. 2017)
ASTER (Shi et al. 2018)
CA-FCN* (Liao et al. 2019)
TRBA (Baek et al. 2019)
Textscanner* (Wan et al. 2020a)
GTC (Hu et al. 2020)
SCATTER (Litman et al. 2020)
SEED (Qiao et al. 2020)
SRN (Yu et al. 2020)
RobustScanner (Yue et al. 2020)
Base2D (Yan et al. 2021)
PREN2D (Yan et al. 2021)
ABINet-LV† (Fang et al. 2021)
Seg-Baseline
S-GTR
GTR + CRNN[CTC]
GTR + TRBA[1DATT]
GTR + SRN[Transformer]
GTR + Base2D[2DATT]
GTR + ABINet-LV†[Transformer]
SAR(Li et al. 2019)
Textscanner* (Wan et al. 2020a)
RobustScanner (Yue et al. 2020)
ABINet (Fang et al. 2021)
S-GTR

ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ
ST + MJ + R
ST + MJ + R
ST + MJ + R
ST + MJ + R
ST + MJ + R

Regular
SVT
80.9
85.9
89.5
86.4
87.5
90.1
92.9
90.9
89.6
85.1
88.1
93.4
94.0
93.0
90.8
94.1
82.1
90.1
93.1
94.1
94.8
91.2
92.7
89.3
95.5
95.8

IIIT5k
78.2
87.4
93.4
91.9
87.9
93.9
95.5
93.2
93.8
94.8
95.3
95.4
95.6
96.3
94.2
95.8
87.6
93.2
96.0
96.1
96.8
95.0
95.7
95.4
97.2
97.5

IC13
89.4
93.3
91.8
91.5
92.3
92.9
94.3
94.1
92.8
95.5
94.8
95.9
96.4
97.0
93.6
96.8
90.1
94.0
96.1
96.6
97.7
94.0
94.9
94.1
97.7
97.8

Irregular
IC15
-
70.6
76.1
-
77.6
79.4
79.5
82.0
80.0
82.7
77.1
81.9
83.0
85.0
82.0
84.6
68.2
76.0
83.9
85.3
86.9
78.0
83.5
79.2
86.9
87.3

CUTE
-
-
79.5
79.9
74.0
83.3
92.2
84.8
83.6
87.8
90.3
89.9
91.7
89.2
87.6
92.3
78.1
82.1
90.7
92.6
93.1
89.6
91.6
92.4
94.1
94.7

SVTP
-
-
78.5
-
79.2
84.3
85.7
86.2
81.4
85.1
79.5
86.0
87.6
88. 5
84.3
87.9
68.1
80.7
87.9
88.0
89.6
86.4
84.8
82.9
90.1
90.6

Params
(×106)
8.3
-
22
-
49.6
57
-
-
-
49.3
-
59.0
-
36.7
34.0
42.1
15.2
54.2
54.3
64.1
41.6
-
57
-
-
42.1

Time
(ms)
6.8
-
73.1
-
27.6
56.8
-
-
-
26.9
-
61.6
67.4
22.0
14.0
18.8
12.8
32.9
31.6
65.7
30.9
-
56.8
-
-
18.8

Table 1: Results of our S-GTR, SOTA methods and their variants with our GTR on six regular and irregular STR datasets. “R”
denotes the real datasets. “*” means using character-level annotations during training. “†” means the batch size is set to 384 for
a fair comparison. The superscripts in the second group of rows denote the type of different methods, i.e., “CTC”: CTC-based
method, “1DATT”: 1D attention-based method, “2DATT”: 2D attention-based method, and “Transformer”: Transformer-based
method. Details can be found in Section 4.2.

KL loss for context consistency. LMT is the MSE loss on the
segmentation maps from teacher and student networks. λSeg
and λCC are both set to 1.0. λMT is set to 1.0 when using
synthetic datasets for training. After getting accurate feature
representations, it is reduced to 0 gradually.

4 Experiments

4.1 Experimental Settings
Datasets Following (Yu et al. 2020), we use two public
synthetic datasets , i.e., SynthText (ST) (Gupta, Vedaldi, and
Zisserman 2016) and MJSynth (MJ) (Jaderberg et al. 2014b,
2016) and a real datasets (R) (Baek, Matsui, and Aizawa
2021) for training. We test the trained model on six bench-
marks including three regular scene-text datasets, i.e., IC-
DAR2013 (Karatzas et al. 2013), IIIT5K (Mishra, Alahari,
and Jawahar 2012), SVT (Wang, Babenko, and Belongie
2011), and three irregular text datasets, i.e., ICDAR2015
(Karatzas et al. 2015), SVTP (Phan et al. 2013) and CUTE
(Risnumawan et al. 2014). The evaluation metric is the stan-
dard word accuracy.

Implementation Details We train the model with ADAM
optimizer on two synthetic datasets for 6 epochs and then
transferred to the real dataset for the other 2 epochs. The
total batch size is 256, equally distributed on four NVIDIA

V100 GPUs. For the pre-training stage on synthetic datasets,
the learning rate is set to 0.001 and divided by 10 at the 4-th
and 5-th epochs. Then, we utilize the mean teacher training
framework on the real dataset for the remaining 2 epochs.
The detailed training setting for this stage is deferred to the
supplementary material.

Our model recognize 63 types of characters, including “0-
9”, “a-z”, and “A-Z”. The max decoding length of the out-
put sequence T is set to 25. We follow the standard image
pre-processing that randomly resizing the width of original
images into 4 scales, i.e., 64, 128, 192 and 256, and then
padding the images to the resolution of 64 × 256. We adopt
multiple data augmentation strategies including random ro-
tation, perspective distortion, motion blur, and adding Gaus-
sian noise to the image.

4.2 Performance Analysis
Comparison with State-of-the-Art We compare the pro-
posed S-GTR with state-of-the-art methods, and the results
are summarized in Table 1, where the inference speed as
well as the number of model parameters are also reported.
As can be seen, the proposed S-GTR achieves the highest
recognition accuracy and 3× faster inference speed com-
pared with the second best method PREN2D (Yan et al.
2021). In addition, when real data is utilized for training, S-

LM

Seg-baseline
VR
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

GTR IIIT5k

SVT IC13

SVTP

IC15 CUTE

91.8
94.2
94.0
95.1

86.6
90.8
91.2
93.2

91.1
93.6
94.8
95.9

79.8
84.3
85.0
86.2

77.7
82.0
82.8
84.1

84.8
87.6
88.4
91.3

(cid:88)
(cid:88)

Figure 4: Results on some test images. Each image is
along with three texts, which are predicted by VR, the Seg-
baseline, and the proposed S-GTR model, respectively.

GTR achieves more impressive results on all the six bench-
marks, validating the effectiveness of the proposed GTR for
textual reasoning and the beneﬁt of real data.

Plugging GTR in Different Models To further verify
the effectiveness of GTR, we plug our GTR module into
four representative types of STR methods, including CTC-
based method (e.g., CRNN (Shi, Bai, and Yao 2016)), 1D
attention-based method (e.g., TRBA (Baek et al. 2019)), 2D
attention-based method (e.g., Base2D (Yan et al. 2021)), and
transformer-based methods (e.g., SRN (Yu et al. 2020) and
ABINet-LV (Fang et al. 2021)). For the 1D attention-based
method, the prediction result of VR is a 1D semantic vec-
tor. Therefore, we adopt the 2D feature map from the layer
before the prediction layer as input of GTR after feature or-
dering. The results are shown in the second group of rows
in Table 1. As can be seen, after using GTR, the perfor-
mance of all these models can be improved further. For ex-
ample, the average recognition accuracy on all the available
test sets is increased by 3.77%, 3.20%, 2.78%, 1.69%, and
1.65% for CRNN, TRBA, SRN, Base2D, and ABINet-LV,
respectively. These results demonstrate the compatibility of
our GTR to typical models.

4.3 Ablation Study
Ablation Study Results of S-GTR All the models in this
ablation study have the same training conﬁgurations as used
in S-GTR. To investigate the impact of different modules
in S-GTR, we ﬁrst train a baseline VR model, which uti-
lizes neither LM and nor GTR. As shown in Table 2, without
LM and GTR, the baseline VR model observes a signiﬁcant
performance drop. Compared with the baseline, a gain of
3.45% can be observed by using LM, since it introduces the
global linguistic textual cues for textual reasoning and cor-
rects some linguistically implausible predictions. The pro-
posed GTR module exploits the visual-spatial context infor-
mation to reﬁne the output of the VR model and increases
the average accuracy by 4.06%. When using both LM and
GTR together, the best average performance of 90.96% can
be obtained. These two modules both contribute to the im-
provement of S-GTR over the baseline, demonstrating that
the linguistic cues and spatial context from visual semantics
are complementary to each other. It is also noteworthy that
the GTR module brings more gains than LM.

Note that there is no use of mutual learning in the ex-
periments in Table 2. After comparing the results in its last
row (i.e., S-GTR without mutual learning) with the results
of S-GTR in Table 1, which is also trained on the “ST+MJ”
datasets but with mutual learning, we can ﬁnd that mutual

Table 2: Ablation study of the components in S-GTR.

# GCN

Adj

Pool

IIIT5k

SVT IC15 CUTE

2
1
3
2
2

{0,1}
Graph
{0,1}
Graph
{0,1}
Graph
Graph
[0,1]
{0,1} Average

95.8
94.3
96.0
95.9
94.8

94.1
92.9
94.0
94.2
93.2

84.6
82.5
84.8
84.9
82.3

92.3
90.8
92.6
92.4
89.8

Params Time
(×106)
(ms)
18.8
42.1
16.1
39.5
22.5
44.9
20.3
42.2
15.7
38.1

Table 3: Ablation study of GTR. “#” means the number of
GCN layers in the ﬁrst stage of GTR. “Adj” is the value type
of adjacency matrix, i.e., discrete value 0 or 1 and continu-
ous value in [0,1], respectively. “Average” denotes employ-
ing average pooling on graph feature rather than the graph
pooling described in Section 3.2.

learning contributes to a better average recognition accuracy
of 91.92%. It demonstrates that enforcing the consistency
between the context features from LM and GTR is neces-
sary to better exploit the complementary between these two
different types of textual reasoning.

For the qualitative analysis of different models, we
present some test images and their corresponding text pre-
dictions from the basic VR model (top), Seg-baseline with
LM (middle), and the proposed S-GTR (bottom) in Figure 4.
We can see that LM can correct some mistaken predictions
from the basic VR model by exploiting the global linguis-
tic context. However, it is still challenging to generalize to
arbitrary texts and some ambiguous cases. Compared to it,
S-GTR produces satisfactory results on irregular texts in dif-
ferent fonts, scales, orientations, and shapes, owing to its
better textual reasoning ability by exploiting both linguistic
cues and spatial context from visual semantics.

Inﬂuence of Different Settings in GTR Although the
proposed GTR module has shown its effectiveness in im-
proving the STR performance on multiple benchmarks, we
would also like to analyze the inﬂuence of different settings
in GTR. In this section, we evaluate the performance of GTR
variants with respect to different numbers of GCN layers in
the ﬁrst stage, different value types of adjacency matrix, and
different pooling strategies. As shown in Table 3, with the
increase of the number of GCN layers, the recognition ac-
curacy, the number of parameters, and inference time all in-
crease as well. To achieve a trade-off between recognition
accuracy and model complexity, we choose 2 layers as the
default setting. Besides, we ﬁnd that there is almost no per-
formance gain when using continuous values in the adja-
cency matrix compared to discrete values, while the infer-
ence time increases by 7.98%. Therefore, we choose the dis-
crete value as the default value type. We further compare the
graph pooling with the average pooling in the stage of con-
textual reasoning. The results show that graph pooling out-
performs average pooling signiﬁcantly since it can capture

xoollywoollyMollyJAGATIONVACATIONVACATIONCAFCCAFECAFESIIVCESIIVCESINCEcpidorcpidorEpidorStgcStgeStarFusion
Add
Concat
D-fuse

IIIT5k
94.8
95.0
95.8

SVT IC13
95.0
93.2
95.4
93.4
96.8
94.1

SVTP
84.9
85.1
87.9

IC15 CUTE
90.8
83.3
91.3
83.4
92.3
84.6

Table 4: Empirical study of the fusion strategy in S-GTR.
“Add”: element-wise addition. “Concat”: Concatenation.
“D-fuse”: Dynamic fusion.

Figure 5: Visualization of feature maps from VR, GTR and
S-GTR (from the second column to the last column).

the local-to-global dependency for reasoning. Therefore, we
choose it as the default pooling strategy.

Impact of Fusion Strategy We also investigate the im-
pact of fusion strategy in S-GTR when fusing the linguis-
tic context from LM and spatial context from GTR. In ad-
dition to the proposed dynamic fusion, we consider other
two choices, i.e., element-wise sum and concatenation. The
results are reported in Table 4. As can be seen, while the
concatenation fusion strategy performs better than element-
wise addition, it still falls behind the proposed dynamic fu-
sion strategy. We suspect that the beneﬁt may come from the
learnable fusion weights which are absent in the other two
non-parametric cases.

4.4 Further Visualization and Analysis

Visual Inspection Result For the qualitative analysis, we
visualize the feature maps from the penultimate layer in VR,
GTR and S-GTR. As shown in Figure 5, compared to the
feature maps from VR, the feature maps from GTR are more
strongly activated on the target characters owing to the tex-
tual reasoning of spatial context. Besides, the feature maps
from S-GTR cover the target characters more precisely than
GTR. These results imply that the S-GTR can learn more
discriminative features by attending to the target characters
and discard irrelevant information. In addition, we present
the visualization of the node similarity matrix in Figure 6
for better understanding the graph generation process.

Figure 6: Visualization of a node similarity matrix, which is
calculated according to Eq. (3).

Model
VR

LM
FastText
S-GTR FastText
BERT
BERT

VR
S-GTR

IIIT5k
93.9
94.8
94.3
95.8

SVT
90.5
92.9
92.0
94.6

IC13
92.6
95.1
94.0
96.7

SVTP
84.0
85.3
85.6
87.3

IC15 CUTE
87.2
82.8
90.6
84.0
90.8
83.8
92.5
85.0

Table 5: Results of S-GTR with different language models.
FastText and BERT are two pretrained language models.

CRNN ASTER

TextScanner ABINet

Acc (%)
NED

59.2
0.68

57.4
0.69

64.1
0.75

68.4
0.79

S-GTR
72.2
0.82

Table 6: Results of different methods on MLT-17. ”NED” is
short for Normalized Edit Distance.

Compatibility of GTR to Different LMs To further in-
vestigate the compatibility of GTR to LMs, we apply GTR
in a basic VR model with two different LMs, i.e., FastText
(Bojanowski et al. 2017) and BERT (Devlin et al. 2019). As
shown in Table 5, GTR contributes to consistent gains on
both FastText and BERT Language Model. In addition, we
also ﬁnd that using a better LM together with GTR can fur-
ther improve text recognition performance.

Chinese Scene Text Recognition Like English text recog-
nition, the Chinese scene text recognition task offers an al-
ternative way to assess the capability of STR models. The
Chinese STR task is more challenging as it requires model to
handle a larger vocabulary and more complex data associa-
tions. In addition to the recognition accuracy, we also report
the Normalized Edit Distance (NED) of different methods
following the ICDAR-2019 ReCTS (Zhang et al. 2019). As
shown in Table 6, S-GTR outperforms other methods sig-
niﬁcantly on the multi-language dataset MLT-2017 (Nayef
et al. 2017). It demonstrates that GTR is still very effective
for textual reasoning of Chinese text materials.

5 Conclusion
In this paper, we propose the idea of performing textual rea-
soning based on visual semantics from a basic visual recog-
nition (VR) model for the Scene Text Recognition (STR)
task. We implement it as a graph-based textual reasoning
(GTR) module, which can act as an easy-to-plug-in module
in existing representative methods. It is shown to be very
effective in improving STR performance while being com-
plementary to the common practice, i.e., language model-
based linguistic context reasoning. Experimental results on
six challenging STR benchmarks demonstrate that GTR can
be plugged in different types of state-of-the-art STR mod-
els and improve their recognition performance further. GTR
also shows good compatibility with different language mod-
els. Based on a simple segmentation-based VR model, we
construct a simple S-GTR baseline for STR, which sets
state-of-the-art on both English and Chinese regular and ir-
regular text materials. We hope this work can provide a new
perceptive to study textual reasoning in the STR task and
inspire more follow-up work in the future, such as efﬁcient
design for spatial context-based reasoning as well as the way
of effective fusion of different reasoning results.

GT: ATHIGHAHIGHATHIGHGT: crocscrooscrocsGT: ppingpringppingATHIGHcrocsppingAcknowledgements
This work was supported in part by National Natural Sci-
ence Foundation of China under Grants (No.62076186,
No.61822113), and in part by Science and Technology Ma-
jor Project of Hubei Province (Next-Generation AI Tech-
nologies) under Grant (No.2019AEA170). The numerical
calculations in this paper have been done on the supercom-
puting system in the Supercomputing Center of Wuhan Uni-
versity.

References
Baek, J.; Kim, G.; Lee, J.; Park, S.; Han, D.; Yun, S.; Oh,
S. J.; and Lee, H. 2019. What is wrong with scene text recog-
nition model comparisons? dataset and model analysis. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, 4715–4723.
Baek, J.; Matsui, Y.; and Aizawa, K. 2021. What If We
Only Use Real Datasets for Scene Text Recognition? To-
ward Scene Text Recognition With Fewer Labels. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 3113–3122.
Bojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017.
Enriching word vectors with subword information. Trans-
actions of the Association for Computational Linguistics, 5:
135–146.
Chen, Y.; Rohrbach, M.; Yan, Z.; Shuicheng, Y.; Feng, J.;
and Kalantidis, Y. 2019. Graph-based global reasoning net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 433–442.
Cheng, Z.; Bai, F.; Xu, Y.; Zheng, G.; Pu, S.; and Zhou, S.
2017. Focusing attention: Towards accurate text recognition
in natural images. In Proceedings of the IEEE International
Conference on Computer Vision, 5076–5084. Venice, Italy:
IEEE.
Cheng, Z.; Xu, Y.; Bai, F.; Niu, Y.; Pu, S.; and Zhou, S. 2018.
Aon: Towards arbitrarily-oriented text recognition. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 5571–5579. Salt Lake City, UT, USA:
IEEE.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In NAACL-HLT.
Fang, S.; Xie, H.; Wang, Y.; Mao, Z.; and Zhang, Y. 2021.
Read Like Humans: Autonomous, Bidirectional and Itera-
tive Language Modeling for Scene Text Recognition. arXiv
preprint arXiv:2103.06495.
Gao, Y.; Chen, Y.; Wang, J.; Lei, Z.; Zhang, X.; and Lu,
H. 2018. Recurrent Calibration Network for Irregular Text
Recognition. CoRR, abs/1812.07145: arXiv–1812.
Graves, A.; Fern´andez, S.; Gomez, F.; and Schmidhuber, J.
2006. Connectionist temporal classiﬁcation: labelling un-
segmented sequence data with recurrent neural networks.
In Proceedings of the International Conference on Machine
Learning, ICML ’06, 369–376. New York, NY, USA: Asso-
ciation for Computing Machinery.

Spatial

Gupta, A.; Vedaldi, A.; and Zisserman, A. 2016. Synthetic
data for text localisation in natural images. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2315–2324. Las Vegas, NV, USA: IEEE.
Hu, W.; Cai, X.; Hou, J.; Yi, S.; and Lin, Z. 2020. Gtc:
Guided training of ctc towards efﬁcient and accurate scene
In Proceedings of the AAAI Conference
text recognition.
on Artiﬁcial Intelligence, 07, 11005–11012. New York, NY,
USA: AAAI.
Jaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,
A. 2014a. Deep structured output learning for unconstrained
text recognition. arXiv preprint arXiv:1412.5903.
Jaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,
A. 2014b. Synthetic Data and Artiﬁcial Neural Networks
for Natural Scene Text Recognition. ArXiv, abs/1406.2227:
1–10.
Jaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,
A. 2016. Reading text in the wild with convolutional neural
networks. International Journal of Computer Vision, 116(1):
1–20.
and
Jaderberg, M.; Simonyan, K.; Zisserman, A.;
Kavukcuoglu, K. 2015.
transformer networks.
In Proceedings of the Annual Conference on Neural Infor-
mation Processing Systems, 2017–2025. Montreal, Quebec,
Canada: MIT Press.
Karatzas, D.; Gomez-Bigorda, L.; Nicolaou, A.; Ghosh, S.;
Bagdanov, A.; Iwamura, M.; Matas, J.; Neumann, L.; Chan-
drasekhar, V. R.; Lu, S.; et al. 2015. ICDAR 2015 competi-
tion on robust reading. In Proceedings of the International
Conference on Document Analysis and Recognition, 1156–
1160. Nancy, France: IEEE.
Karatzas, D.; Shafait, F.; Uchida, S.; Iwamura, M.; i Big-
orda, L. G.; Mestre, S. R.; Mas, J.; Mota, D. F.; Almazan,
J. A.; and De Las Heras, L. P. 2013. ICDAR 2013 robust
In Proceedings of the International
reading competition.
Conference on Document Analysis and Recognition, 1484–
1493. Washington, DC, USA: IEEE.
Kipf, T.; and Welling, M. 2017. Semi-Supervised Clas-
ArXiv,
siﬁcation with Graph Convolutional Networks.
abs/1609.02907: 1–14.
Lee, J.; Lee, I.; and Kang, J. 2019. Self-Attention Graph
Pooling. ArXiv, abs/1904.08082.
Li, H.; Wang, P.; Shen, C.; and Zhang, G. 2019. Show, at-
tend and read: A simple and strong baseline for irregular
text recognition. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 8610–8617. Honolulu, Hawaii,USA:
AAAI.
Liao, M.; Zhang, J.; Wan, Z.; Xie, F.; Liang, J.; Lyu, P.;
Yao, C.; and Bai, X. 2019. Scene text recognition from
In Proceedings of the AAAI
two-dimensional perspective.
Conference on Artiﬁcial Intelligence, 8714–8721. Honolulu,
Hawaii,USA: AAAI.
Litman, R.; Anschel, O.; Tsiper, S.; Litman, R.; Mazor,
Scatter: selective context
S.; and Manmatha, R. 2020.
In Proceedings of the
attentional scene text recognizer.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 11962–11972.

Wojna, Z.; Gorban, A. N.; Lee, D.-S.; Murphy, K.; Yu,
Q.; Li, Y.; and Ibarz, J. 2017. Attention-based extraction
In
of structured information from street view imagery.
Proceedings of the International Conference on Document
Analysis and Recognition, volume 1, 844–850. IEEE.
Xu, Y.; Zhang, Q.; Zhang, J.; and Tao, D. 2021. ViTAE:
Vision Transformer Advanced by Exploring Intrinsic Induc-
tive Bias. In Thirty-Fifth Conference on Neural Information
Processing Systems.
Yan, R.; Peng, L.; Xiao, S.; and Yao, G. 2021. Primitive Rep-
In Pro-
resentation Learning for Scene Text Recognition.
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 284–293.
Yang, X.; He, D.; Zhou, Z.; Kifer, D.; and Giles, C. L. 2017.
Learning to Read Irregular Text with Attention Mechanisms.
In Proceedings of the International Joint Conference on Ar-
tiﬁcial Intelligence, 3. Melbourne, Australia: ijcai.org.
Ye, J.; Zhang, J.; Liu, J.; Du, B.; and Tao, D. 2021.
I3CL: Intra-and Inter-Instance Collaborative Learning for
arXiv preprint
Arbitrary-shaped Scene Text Detection.
arXiv:2108.01343.
Yu, D.; Li, X.; Zhang, C.; Liu, T.; Han, J.; Liu, J.; and Ding,
E. 2020. Towards accurate scene text recognition with se-
In Proceedings of the IEEE
mantic reasoning networks.
Conference on Computer Vision and Pattern Recognition,
12113–12122. Seattle, WA, USA: IEEE.
Yue, X.; Kuang, Z.; Lin, C.; Sun, H.; and Zhang, W. 2020.
RobustScanner: Dynamically Enhancing Positional Clues
In Proceedings of the Eu-
for Robust Text Recognition.
ropean Conference on Computer Vision, 135–151. Glas-
gow,UK: Springer.
Zhang, J.; and Tao, D. 2020. Empowering things with in-
telligence: a survey of the progress, challenges, and oppor-
tunities in artiﬁcial intelligence of things. IEEE Internet of
Things Journal, 8(10): 7789–7817.
Zhang, R.; Zhou, Y.; Jiang, Q.; Song, Q.; Li, N.; Zhou, K.;
Wang, L.; Wang, D.; Liao, M.; Yang, M.; et al. 2019. Icdar
2019 robust reading challenge on reading chinese text on
In 2019 international conference on document
signboard.
analysis and recognition (ICDAR), 1577–1581. IEEE.
Zhang, S.-X.; Zhu, X.; Hou, J.-B.; Liu, C.; Yang, C.; Wang,
H.; and Yin, X.-C. 2020. Deep relational reasoning graph
network for arbitrary shape text detection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 9699–9708. Seattle, WA, USA: IEEE.

Mishra, A.; Alahari, K.; and Jawahar, C. 2012. Scene text
In British
recognition using higher order language priors.
Machine Vision Conference, 1–11. Surrey, UK: BMVA.
Nayef, N.; Yin, F.; Bizid, I.; Choi, H.; Feng, Y.; Karatzas,
D.; Luo, Z.; Pal, U.; Rigaud, C.; Chazalon, J.; et al. 2017.
Icdar2017 robust reading challenge on multi-lingual scene
text detection and script identiﬁcation-rrc-mlt. In Proceed-
ings of the International Conference on Document Analysis
and Recognition, volume 1, 1454–1459. IEEE.
Phan, T. Q.; Shivakumara, P.; Tian, S.; and Tan, C. L.
2013. Recognizing text with perspective distortion in nat-
ural scenes. In Proceedings of the IEEE International Con-
ference on Computer Vision, 569–576. Sydney,Australia:
IEEE.
Qiao, Z.; Zhou, Y.; Yang, D.; Zhou, Y.; and Wang, W. 2020.
Seed: Semantics enhanced encoder-decoder framework for
scene text recognition. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 13528–
13537.
Risnumawan, A.; Shivakumara, P.; Chan, C. S.; and Tan,
C. L. 2014. A robust arbitrary text detection system for natu-
ral scene images. Expert Systems with Applications, 41(18):
8027–8048.
Shi, B.; Bai, X.; and Yao, C. 2016. An end-to-end trainable
neural network for image-based sequence recognition and its
application to scene text recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 39(11): 2298–
2304.
Shi, B.; Yang, M.; Wang, X.; Lyu, P.; Yao, C.; and Bai, X.
2018. Aster: An attentional scene text recognizer with ﬂexi-
ble rectiﬁcation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 41(9): 2035–2048.
Tarvainen, A.; and Valpola, H. 2017. Mean Teachers are Bet-
ter Role Models: Weight-Averaged Consistency Targets Im-
prove Semi-supervised Deep Learning Results. 1196–1205.
Wan, Z.; He, M.; Chen, H.; Bai, X.; and Yao, C. 2020a.
Textscanner: Reading characters in order for robust scene
text recognition. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 12120–12127. New York, NY, USA:
AAAI.
Wan, Z.; Zhang, J.; Zhang, L.; Luo, J.; and Yao, C. 2020b.
In Pro-
On vocabulary reliance in scene text recognition.
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 11425–11434.
Wang, K.; Babenko, B.; and Belongie, S. 2011. End-
In Proceedings of the In-
to-end scene text recognition.
ternational Conference on Computer Vision, 1457–1464.
Barcelona,Spain: IEEE.
Wang, T.; Zhu, Y.; Jin, L.; Luo, C.; Chen, X.; Wu, Y.; Wang,
Q.; and Cai, M. 2020. Decoupled Attention Network for
Text Recognition. ArXiv, abs/1912.10205.
Wang, Z.; Zheng, L.; Li, Y.; and Wang, S. 2019. Linkage
based face clustering via graph convolution network. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 1117–1125. Long Beach, CA, USA:
IEEE.

