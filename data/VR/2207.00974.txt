NARRATE: A Normal Assisted Free-View Portrait Stylizer

YOUJIA WANG, ShanghaiTech University, China
TENG XU, ShanghaiTech University, China
YIWEN WU, ShanghaiTech University, China
MINZHANG LI, ShanghaiTech University, China
WENZHENG CHEN, University of Toronto, Canada
LAN XU, ShanghaiTech University, China
JINGYI YU, ShanghaiTech University, China

2
2
0
2

l
u
J

0
2

]

V
C
.
s
c
[

2
v
4
7
9
0
0
.
7
0
2
2
:
v
i
X
r
a

Fig. 1. We propose NARRATE, a novel portrait stylization pipeline that supports editing portrait lighting and perspective in a photorealistic manner. Taking a
single high resolution portrait as input, it conducts pose change, light change, facial animation, and style transfer, either separately or in combination, at a
photographic quality.

Taking a compelling portrait photo relies on deliberately designed lighting
styles and shooting angles, requiring advanced skills beyond the ability of
casual users. Building a portrait stylization tool that automates lighting
and perspective editing is beneficial to photographers and artists, enabling
numerous applications from photography/cinematography to AR/VR. This
task remains challenging as persuasive lighting adjustment, geometrically
correct perspective changes, and high photorealism maintenance need to be
achieved at the same time. Naively bridging existing relighting and view
synthesis methods produces inaccurate and unstable results.

In this work, we propose NARRATE, a novel pipeline that enables simulta-
neously editing portrait lighting and perspective in a photorealistic manner.
As a hybrid neural-physical face model, NARRATE leverages complementary
benefits of geometry-aware generative approaches [Or-El et al. 2021] and
normal-assisted physical face models. In a nutshell, NARRATE first inverts
the input portrait to a coarse geometry and employs neural rendering to
generate images resembling the input, as well as producing convincing pose
changes. However, inversion step introduces mismatch, bringing low-quality
images which fail to keep facial details. As such, we further estimate portrait
normal to enhance the coarse geometry, creating a high-fidelity physical

Authorsâ€™ addresses: Youjia Wang, ShanghaiTech University, China, wangyj2@
shanghaitech.edu.cn; Teng Xu, ShanghaiTech University, China, xuteng@shanghaitech.
edu.cn; Yiwen Wu, ShanghaiTech University, China, wuyw1@shanghaitech.edu.cn;
Minzhang Li, ShanghaiTech University, China, limzh@shanghaitech.edu.cn; Wenzheng
Chen, University of Toronto, Canada, wenzheng@cs.toronto.edu; Lan Xu, ShanghaiTech
University, China, xulan1@shanghaitech.edu.cn; Jingyi Yu, ShanghaiTech University,
China, yujingyi@shanghaitech.edu.cn.

face model to render more detailed images. In particular, we fuse the neural
and physical renderings to compensate for the imperfect inversion, resulting
in both realistic and view-consistent novel perspective images.

In relighting stage, previous works [Pandey et al. 2021; Sun et al. 2019]
focus on single view portrait relighting but ignoring consistency between
different perspectives as well, leading unstable and inconsistent lighting
effects for view changes. We extend [Pandey et al. 2021] to fix this problem
by unifying its multi-view input normal maps with the physical face model.
NARRATE conducts relighting with consistent normal maps, imposing cross-
view constraints and exhibiting stable and coherent illumination effects.

We experimentally demonstrate that NARRATE achieves more photoreal-
istic, reliable results over prior works. We further bridge NARRATE with
animation and style transfer tools, supporting pose change, light change,
facial animation, and style transfer, either separately or in combination, all
at a photographic quality. We showcase vivid free-view facial animations
as well as 3D-aware relightable stylization, which help facilitate various
AR/VR applications like virtual cinematography, 3D video conferencing, and
post-production.

CCS Concepts: â€¢ Computing methodologies â†’ Computational pho-
tography; Image-based rendering.

Additional Key Words and Phrases: novel view synthesis, relighting, facial
reenactment, style transfer, hybrid representation

 
 
 
 
 
 
2

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

INTRODUCTION

1
Even in todayâ€™s digital era, portrait photography remains inarguably
the most popular genre. For any successful portrait, be it an actor
still, a beauty shot, or a character study, lighting styles, coupled with
shooting angles, play the key role to emphasize the subjectâ€™s feature
as well as to express artistic or dramatic intent. On lighting, popular
loop lighting defines facial features with soft shadows to illustrate
harmony whereas a half lit and half in shadow face by split lighting
reflects the subjectâ€™s inner struggle. Rembrandt lighting combines a
key light and a fill light (e.g., a reflector) to cast a distinctive triangle
of light on one cheek, baking a moody atmosphere. On shooting
angle, front view (full face) shots highlight complete facial details
to convey an assertive attitude whereas side view (profile) shots
emphasize on the physical form-factor such as facial contours to
exude character. Three-quarter views, popularized by self-portraits,
reveal both with a personal touch.

Despite richness in styles, a portrait photograph, once taken, is
fixed in its final presentation. Editing its lighting and perspective to
produce an equally compelling alternative, if possible at all, requires
tremendous manual processing even for the professionals. Overall,
restylizing the photograph not only requires using a disarray of
advanced retouching tools but can be extremely time-consuming. As
such, an automated portrait stylization pipeline is highly valuable for
photographers and visual artists as well as for daily users. Yet the
challenges are multi-fold, including geometrically correct per-
spective changes, persuasive lighting adjustment, and high
photorealism maintenance.

Face pose changes have long followed the warping pipeline that
largely relies on accurate 3D facial geometry estimations. Exem-
plary techniques such as 3D morphable models (3DMM) [Blanz and
Vetter 1999] can recover large scale geometry but tend to produce
overly smooth surfaces that lack details. Further, 3DMM cannot
model geometry beyond face such as hair, clothing, glasses, and
even ears and consequently the morphing results exhibit strong
visual artifacts in respective regions. Recent implicit solutions, pop-
ularized by generative models [Goodfellow et al. 2014], can generate
unseen portraits with convincing pose variations [Karras et al. 2019;
Shen et al. 2020]. However, as a generator, they do not readily sup-
port restylizing a given portrait. Techniques such as inversion [Zhu
et al. 2016] for fitting a latent code to the known image can lead to
excessive blurs or distortions [Tov et al. 2021].

Adjusting lighting, on the other hand, requires reliable estima-
tions of a portraitâ€™s original illumination, facial reflectance, and
facial normal. Existing portrait relighting techniques have largely
focused on fixed viewpoint retouching [Pandey et al. 2021; Sun et al.
2019]. When applied to new facial poses, e.g., rigged using 3DMM,
they produce strong visual artifacts with even slight illumination
and shape inaccuracy and continuous pose changes generate strong
flickering.

In this paper, we present NARRATE (Normal Assisted Portrait
Stylizer), a novel portrait stylization pipeline that takes a single high
resolution portrait as input and conducts pose change, light change,
facial animation, and style transfer, either separately or in combina-
tion, at a photographic quality. At the core of NARRATE is a hybrid
neural-physical face model that leverages complementary benefits

of geometry-aware generative approaches [Or-El et al. 2021] and
normal-assisted physical face models. In a nutshell, NARRATE first
conducts GAN inversion [Roich et al. 2021] for finding a generated
portrait that closely resembles the input. In 3D-aware generation,
the inverted portrait latent initiates a coarse geometry in order to
produce convincing pose changes, as well as employs neural ren-
dering to synthesize complete portrait effects including hair, ear,
and clothing. However, GAN inversion brings mismatch between
the input and fitted images, resulting in low-quality, less detailed
synthesis. Thus, we further adopt high fidelity normal estimation
techniques to infer portrait albedo and normal information [Pandey
et al. 2021] to enhance the coarse geometry to a high quality phys-
ical face model, synthesizing images full of realistic facial details
with graphics rendering engines. In particular, we fuse the two ren-
derings to composite more realistic and consistent novel perspective
images.

For relighting, we extend the fixed viewpoint framework [Pandey
et al. 2021] to support consistent relighting over continuous view-
point changes. Instead of processing individual poses, We show
how to conduct neural relighting on all views at once by imposing
consistent normal and albedo maps from the same physical face
model. Our technique significantly suppresses flickering than per-
frame based baseline where a user can specify exquisite lighting
effects (e.g, Rembrandt) at any new poses to produce photographic
portraits or videos.

NARRATE also supports dynamic animations and non-photorealistic
stylization. Combined with either implicit or explicit animators [Siaro-
hin et al. 2019], NARRATE produces relightable free-viewpoint
videos of virtual human talents such as opera singing, potentially
useful for virtual cinematography and post-production. We also
bridge NARRATE with artistic style transfer tools [Praun et al. 2001;
Wang et al. 2018], creating 3D-aware styled images like oil painting
and hatching drawing, bringing famous portrait paintings to life.

We summarize our contributions as follows:

â€¢ We propose NARRATE, a novel portrait stylization pipeline
that supports editing perspective and lighting in a photoreal-
istic manner.

â€¢ Our hybrid neural-physical face model takes advantage of
geometry-aware generative methods and normal-assisted
physical models, fusing neural and graphics renderings to
create plausible and stable free-view relighting effects.

â€¢ NARRATE can further combine with animation and style
transfer tools to synthesize delicate styled facial animation,
demonstrating its potential in future AR/VR applications.

2 RELATED WORK
Our work focuses on post-capture photographic portrait adjustment
and retargeting. While previous approaches have adopted different
strategies to handle pose, lighting, style, and animations, we propose
a unified solution via hybrid neural physical face modeling.

Perspective Adjustment. Post-capture perspective adjustment to
portrait has drawn broad interest in computer graphics and vision,
and latest virtual reality. Images are essentially 2D projections of 3D
geometry and therefore previous efforts have focused on inferring
and then rigging 3D faces from the portraits. Since the problem is

inherently ill-posed, earlier approaches attempt to fit 3D template
models (3DMM) [Blanz and Vetter 1999] from landmarks via opti-
mization schemes [Blanz et al. 2004; Cao et al. 2015, 2014]. More
recent methods [Kazemi and Sullivan 2014; Richardson et al. 2016,
2017; Tewari et al. 2018, 2017; Yamaguchi et al. 2018; Zhu et al. 2017a]
adopt a data-driven strategy and produce more faithful reconstruc-
tions on both shape and appearance. By far, existing parametric
template models [Blanz and Vetter 1999; Feng et al. 2020; Li et al.
2017] cannot yet preserve fine-grained facial details. They also only
model the facial regions while ignoring other body parts such as
hair styles or even ears. Pose adjustments on high quality portrait
often lead to uncanny results where faces look plastic due to loss of
details.

The emerging implicit solutions, popularized by generative mod-
els [Goodfellow et al. 2014; Karras et al. 2021, 2019, 2020], have
shown huge potential for free-view high-quality portrait synthesis.
Although, as a generation task, they were not specifically designed
for perspective retargeting, it is possible to tweak these solutions via
neural inversion to support free-view rendering [Karras et al. 2019;
Shen et al. 2020; Shen and Zhou 2021; Tewari et al. 2020]. Yet, despite
significant advances, neural inversion still loses high frequency and
earlier generative methods produce aliased and inconsistent view
changes. Followup geometry-aware generative models [Chan et al.
2020, 2021; Gu et al. 2021; Or-El et al. 2021; Zhou et al. 2021] com-
bine 2D generation with explicit 3D geometry constraints and have
greatly improved view consistency. However, their estimated geom-
etry is still too smooth and lacks the precision for conducting other
stylization tasks such as relighting. Most recently, [Tan et al. 2022]
imposes additional lighting constraints to simultaneously achieve
relighting and perspective changes. Yet, the final quality still falls
short of the photographic level due to volumetric neural rendering
which also causes cross-view flickering .

Portrait Relighting. A separate research line on portrait stylization
is one-shot relighting. Pioneered by the LightStage [Debevec et al.
2000], approaches over the past decade have focused on capturing
and modeling the 4D reflectance field. Using a spherical lighting
rig, various version of the LightStage manage to capture one-light-
at-a-time or OLAT reflectance field [Guo et al. 2019; Meka et al.
2019; Pandey et al. 2021; Zhang et al. 2021] for a human face and
even their full body [Meka et al. 2020]. Latest advances on deep
learning have subsequently enabled efficient modeling of the OLAT
data to address various aspects of the relighting problem [Guo et al.
2019; Meka et al. 2019, 2020; Nestmeyer et al. 2020; Pandey et al.
2021; Shu et al. 2017; Sun et al. 2019; Wang et al. 2020; Zhou et al.
2019], ranging from modeling lighting using spherical harmonics
representations [Liu et al. 2021; Sengupta et al. 2018; Zhou et al.
2019] to more sophisticated environment lighting [Sun et al. 2019].
To overcome the limitation of low frequency lighting, [Nestmeyer
et al. 2020] explicitly models shadows and specularity by directional
light source whereas [Wang et al. 2020] employs richer synthetic
data to account for non-Lambertian effects. The seminal work of
TotalRelighting [Pandey et al. 2021] significantly improves prior
learning based techniques by employing specialized network de-
signs coupled with robust matting. Their technique can produce
photographic quality results under a broad range of virtual lighting.

NARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

3

Yet, TotalRelighting is still restricted to fixed viewpoint. Brute-force
application to pose adjusted portrait can lead to incorrect normal
estimation and subsequently visual artifacts.

Animation. An exciting new area of portrait stylization is to ani-
mate the face via a driving video. The solution has largely followed
facial motion transfers or reenactment and has been divided into
subject-dependent and subject-agnostic models, both can be con-
ducted either in 3D or 2D. 3D approaches [Fried et al. 2019; Geng
et al. 2018; Nagano et al. 2018; Olszewski et al. 2017] are more robust
to pose changes and potentially amenable to relighting. However,
since reliable reconstructions can only be conducted on near diffuse
regions with simple geometry, they often miss teeth, ear, hair etc,
causing the reenacted performance unconvincing. A variety of 2D
generative models [Siarohin et al. 2019; Wang et al. 2019; Zakharov
et al. 2020, 2019] can now produce highly compelling facial anima-
tions without using any 3D shape prior. The lack of 3D, however,
limits them from perspective changes, let alone relighting which
we aim to resolve. The latest Neural Talk Head [Wang et al. 2021b]
demonstrates that even slight support of 3D in the general frame-
work can lead to big improvement. By using canonical 3D keypoints,
they are able to produce free-view video synthesis from a single
face image. Their quality may be sufficient for teleconferencing but
falls short of production-level portrait photography.

Artistic Stylization. Artistic stylization has a long history in ren-
dering, ranging from early statistics model [Shih et al. 2014; Tang and
Wang 2003] to the deep learning model [Gatys et al. 2015; Johnson
et al. 2016; Liao et al. 2017; Selim et al. 2016]. Latest solutions essen-
tially model the problem as image-to-image translation [Futschik
et al. 2019; Isola et al. 2017; Zhu et al. 2017b]. As they unanimously as-
sume fixed viewpoint input [Chen et al. 2018; D Chen 2017; JamriÅ¡ka
et al. 2019], combining these solutions with free-view rendering
require high consistent across viewpoints. Recent techniques [FiÅ¡er
et al. 2017; Han et al. 2021] attempt to impose 3D constraints either
based on a depth map or an explicit 3D model, yet both lacked preci-
sion to produce faithful perspective changes while maintaining fine
details. We instead propose a hybrid neural physical model that can
be adopted to simultaneously tackle perspective change, relighting,
animation, or artistic stylization.

3 OVERVIEW
In this section, we describe NARRATE pipeline and applications. As
shown in Fig. 2, given a portrait image, NARRATE allows separate
modifications to its perspective and lighting, enabling photorealistic
and consistent editing effects at a photographic quality across dif-
ferent views. NARRATE can further integrate with animation and
style transfer tools to create free-view animated/styled faces, bene-
fiting AR/VR applications such as tele-presence as well as virtual
cinematography. We first describe photorealisitc free view synthesis
and view-consistent relighting in Sec. 4. Then, we present free-view,
relightable facial animation and style transfer applications in Sec. 5.

4 FREE-VIEW RELIGHT
Take a portrait I as input, we set out to synthesize a novel view Iv
under a new camera perspective v while maintaining photorealism.

4

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

Fig. 2. Taking a portrait as input (left), NARRATE first performs neural inversion(Sec. 4.1) to produce a coarse geometry and less-detailed neural rendered
images(middle left, top). It further creates a high-fidelity physical face model(Sec. 4.2) to render more detailed images(middle left, bottom). NARRATE warps
the two renderings(Sec. 4.3), generating realistic and consistent novel view images as well as accurate normal maps (middle right), which are readily used to
create plausible relighting effects (right).

NARRATE adopts a hybrid neural-physical face model by leveraging
a geometry-aware generative model and a normal-assisted physical
face model. As shown in Fig. 2 (left top), we first employ the powerful
geometry aware generative models [Or-El et al. 2021] and apply
GAN inversion [Roich et al. 2021] to reverse the input to the latent
code. The generator then maps it to a coarse geometry mğ‘™ and
conducts neural rendering to synthesize a high resolution novel
view image Iv
n

. We describe detailed procedures in Sec. 4.1

We observe though that both inversion and neural rendering
introduce artifacts in novel view synthesis, creating over-smoothed
and aliasing results. As such, we further propose to enhance the re-
sult with normal-assisted physical meshes. Specifically, we employ a
normal estimation network (Sec 4.3) to obtain a detailed normal map
N from the input I. With the assistance of N, we improve the gener-
ated coarse geometry mğ‘™ through Poisson normal integration [Horn
and Brooks 1986], producing a high quality physical model mâ„. We
then texture it with the original image and render a new image Iv
m
using the traditional pipeline [Woo et al. 1999]. Finally, we fuse Iv
n
and Iv
and with Poisson blending [PÃ©rez et al. 2003] and generate
m
a photorealistic image with much reduced artifacts while maintain-
ing details. We discuss the physical model generation and fusion
processes in Sec. 4.2 and Sec. 4.3, respectively.

4.1 Neural Inversion
Geometry-aware generative models can now produce images un-
der explicit geometry constraints, achieving better view consis-
tency in novel view synthesis. We build our pipeline on top of the
StyleSDF [Or-El et al. 2021] framework, currently viewed as the
state-of-the-art. StyleSDF first maps a latent code z to a coarse
Signed Distance Field (SDF), where a low resolution image is gener-
ated via volume rendering [Wang et al. 2021a] with a specific camera
view v. Different from 2D generation architectures [Karras et al.

2021, 2019, 2020], its rendering process helps regularize generation
with explicit geometrical constraints. This allows us to adjust v to
render different viewpoints towards the same shape, resulting in
consistent multi-view images. Neural rendering (generally 2D CNN
layers) is further employed to upsample the low resolution image
to a high resolution one. We denote the complete generation as
n = G(z, v; ğœƒ ) where G is the generator and ğœƒ is its parameters.
Iv
The SDF can also be used to extract a coarse mesh mğ‘™ as well as a
low quality normal map Nğ‘™ that will be used in later steps.

Conceptually, StyleSDF can be treated as a neural face model.
While StyleSDF is not designed for any subject, we perform GAN
inversion to invert the input image I back to the latent z. Once z is
ready, we can utilize the generator to synthesize novel view images
under any camera view v. We choose to use the state of the art
Iv
n
method PTI [Roich et al. 2021] technique. While PTI is designed for
StyleGAN 2D inversion, we extend it for StyleSDF 3D inversion.

Specifically, PTI contains two stages. We first fix the weights ğœƒ
of the generator G and then optimize the latent code z as well as
the camera view v such that the generated image would best match
the input. Notice that brute-force optimization over z generally
produces blurred results [Tov et al. 2021]. Alternatively, we can fix z
and v but optimize the generator weights ğœƒ to enforce the generated
image closer to the input. In either case, we refine the initial results
using normal maps in Sec. 4.2. In our implementation, we adopt the
same loss and regulation as PTI where more details can be found
in [Roich et al. 2021].

4.2 Hybrid Neural Physical Modeling
Novel views synthesized from StyleSDF and PTI, although of a much
higher quality than previous GAN-based results, still exhibit artifacts
beyond acceptable as a photograph. The causes are two-fold: first,
the image generated with PTI inversion still contains mismatch from

Novel View RGBRelightingNeuralInversionInput ImageWarpingHybridNeural Physical ModelingCoarseRGBNovel ViewNormalRelighted ImageCoarseNormalNARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

5

Fig. 3. NARRATE creates its physical face model from high quality normal
maps and depth priors. The former is inferred from a normal predictor while
the latter comes from inverted coarse geometry. We perform Poisson normal
integration for the outline region, generating a high-fidelity face mesh.

the input portrait, especially for the regions with high frequency
details. Second, precise view consistency only is only enforced on
low resolution generated images whereas the upsampling process in
neural renderings introduces additional aliasing and inconsistency,
causing vibration under view changes when presented in a high
resolution [Chan et al. 2020, 2021].

The visual artifacts are magnified on portraits due to incredible
visual acuity of human for distinguishing human faces. To mitigate
the problem, we employ an auxiliary normal-assisted physical face
model. Recall that StyleSDF generates not only 2D images but also
the 3D geometry of the face, although it is coarse by nature. We thus
aim to create a high quality face mesh, textured with the original
image, to enhance the facial details as well as reduce view inconsis-
tency. We observe the coarse mesh mğ‘™ extracted from StyleSDF is too
noisy to be directly used a proxy. We therefore propose to enhance
the geometric quality by exploiting extra normal information.

To do so, we set out to infer a high quality normal map Nâ„ from I
via a normal predictor(Sec. 4.3), with which we then construct a high
quality face mesh mâ„ from mğ‘™ with the assistance of Nâ„ by Poisson
integration. Note that Nâ„ alone is insufficient for creating a high
quality mesh mâ„: discontinuous regions (e.g., nose, mouth) exhibit
ambiguity that can lead to the reconstruction of a flattened 3D face.
We hence solve a constrained linear equations, where mğ‘™ provide
depth priors in discontinuities whereas Nâ„ sets out to preserve fine
geometric details. In our implementation, we focus on the front face
as it contains more details. Next, we re-project mâ„ to the image
with camera view v and use the original image as the texture. We
take mâ„ as our normal-assisted physical model that supports being
rendered at arbitrary view Ëœv with full details obtained from the front
view. Fig. 3 shows how mâ„ is reconstructed.

4.3 Warping and Relighting
With renderings from both neural and physical models, we are
able to create high quality, view-consistent images under arbitrary
views. For a new camera view Ëœv, we synthesize a RGB image, as
well as a normal map which is further used in subsequent relighting
module and applications. The final RGB image IËœv is fused from
the StyleSDF generation IËœv
. IËœv
n
n
contains a complete portrait with all essential regions such as hairs,
clothing, ears (and ear rings), and the even background that are

and the mesh-rendered image IËœv
m

Fig. 4. Neural Inversion only generates images missing facial details. NAR-
RATE compensates it by hybrid neural-physical model, achieving more
detailed and realistic results In addition, it provides better surface normal
maps, which helps in the subsequent relighting and other applications.

critical for preserving photorealistic. At the same time, recall IËœv
m
only presents the main face region but with fine details.

We therefore create IËœv by fusing the two by Poisson blending [PÃ©rez
et al. 2003]. In our experiments, we observe the Poisson scheme
marginally impacts photorealism but manages to maintain high con-
sistency under view changes. We further compute an companion
normal map generated by applying the same fusion technique to
two normal maps, the first from StyleSDF SDF whereas the other
from rendering under camera pose changes with the normal map
and mesh from mâ„. Fig. 4 compares our method with PTI where our
technique significantly improves the visual fidelity in both detail
preservation and noise reduction over the PTI result. Fig. 5 shows
sample results using our technique under view changes.

Next we present a technique for relighting the free view portrait
images with user specified illumination patterns. Existing single
image relighting techniques [Pandey et al. 2021; Sun et al. 2019]
produce promising results but unanimously assume fixed viewpoint.
Directly applying such methods to free-view images produces in-
consistent and even flicking results. We reduce such artifacts by
employing the normal maps as constraints to enforce high-fidelity,
consistent relighting effects across the views.

We follow a similar relighting process as [Pandey et al. 2021]
with three key modules: a normal prediction networkFğ‘ , an albedo
prediction network Fğ´ and a relighting network Fğ‘…. Since [Pandey
et al. 2021] has not yet open sourced their solution, we therefore
implemented our own version using a newly captured OLAT dataset.
Specifically, we have constructed a photometric capture stage com-
posed with 114 LED light sources synchronized with a compan-
ion 4K video camera. Using illumination multiplexing with optical
flow compensation, we managed to capture dynamic OLAT video

NormalFace MeshInput imageOutline Extractor3D-aware GeneratorNormal PredictorPoisson Normal IntegrationNeural InversionOursNormalNormalGeometryGeometry6

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

5.1 Facial Animation
NARRATE can be integrated with facial animation tools for creating
realistic animated avatars. Here, given a target video ğ· , our goal is to
drive the source image I to behave the same actions and expressions
as each frame in ğ· while keeping its own content identity.

In our system, we modify the popular image animation method
FOMM [Siarohin et al. 2019]. The original FOMM assumes that the
source image and the first frame in the target video ğ· are aligned,
i.e., they have to have an identical pose. This is a strong constraint
that greatly limits its applicable to the types of input portrait. As
NARRATE supports view changes while maintaining realism, we
can easy first produce a portrait to match the initial pose of the
video and then conduct animation transfer.

To reiterate, given a source image I and the ğ‘¡-th frame ğ·ğ‘¡ in
the driving video, FOMM detects the keypoints and estimates a
in the form of per-pixel cor-
backward dense motion field TIâ†ğ·ğ‘¡
respondence from ğ·ğ‘¡ to I, along with an occlusion field OIâ†ğ·ğ‘¡
. A
generator G is further employed to synthesize the animated image
Iğ·ğ‘¡ , where Iğ·ğ‘¡ = G(ğ¼, TIâ†ğ·ğ‘¡ , OIâ†ğ·ğ‘¡ ). Although highly effective,
the final quality using [Siarohin et al. 2019] depends on accurate
motion field estimations and thereby requires I and the first frame
ğ·1 be well aligned.

We exploit the free view generation capability of NARRATE for
supporting FOMM to drive a source image with any pose. Specif-
ically, given the input image I and target video ğ·, we first create
a reference image Iğ‘Ÿ which has the same pose as ğ·1. This allows
FOMM to obtain an accurate motion field TIğ‘Ÿ â†ğ·ğ‘¡
between the refer-
ence image Iğ‘Ÿ and the corresponding frame ğ·ğ‘¡ . We then estimate the
motion filed TIâ†Iğ‘Ÿ from Iğ‘Ÿ to the source image I. Recall that Iğ‘Ÿ and
I are the same face rendered with different poses using the known
neural-physical model. Therefore we can directly compute TIâ†Iğ‘Ÿ
from the known shape and pose changes. The complete process for
motion field estimation is as follows: TIâ†ğ·ğ‘¡ = TIğ‘Ÿ â†ğ·ğ‘¡ + TIâ†Iğ‘Ÿ .
The animated image Iğ·ğ‘¡ is generated by G in the same way where
the occlusion field is automatically computed with the motion field.
Combining NARRATE and FOMM produces exciting new appli-
cations at an unprecedented quality: NARRATE is fully automated
and only requires the user provide an input portrait and the target
environment for relighting; therefore we can connect our technique
with any pre-trained FOMM models 1 to emulate a variety of facial
animations, from Broadway performances to Peking Operas. All
results shown in the paper are automatically generated without any
fine-tuning.

Compared with state-of-the-art free view facial animation engine
NTH [Wang et al. 2021b], our approach much better preserves
photorealism with fine texture and geometry details. It is also faster
in inference speed and more memory efficient. More importantly,
we support high quality relighting that can help not only matching
facial animations and poses but also lighting effects, producing
dramatic mood that faithfully resembles the original clip. Fig. 7 and
our supplementary video show a number of examples.

Fig. 5. Once the hybrid neural-physical model is ready, we can easily render
novel view images from both neural part and physical part(middle). We then
use Poisson image blending to fuse them and get the final image(right).

sequences at 25 fps. In our training dataset, we use 600K OLAT
images covering 36 performances (18 male and 18 females) under
2810 HDR environment maps. In our experiments, we observe they
are sufficient to handle a diverse class of portraits in facial shapes,
complexions, skin textures (e.g., wrinkles).

We follow the [Ronneberger et al. 2015] footprint and use U-Net
as the backbones for Fğ‘ , Fğ´, and Fğ‘… and follow the same training
strategy as [Pandey et al. 2021]. Since the OLAT data can be used to
extract the ground truth albedo and normal maps via photometric
stereo, we use the OLAT results for training the complete network.
This allows us to first predict a high quality normal map N from the
input portrait I, where N = Fğ‘ (I) that has been used to support
free view generation, as described in Sec. 4.2. We can then apply
the albedo network to extract the reflectance map Iğ´ = Fğ´ (I, N).
Finally, we generate the relighted image Iğ‘… using the relighting
network Iğ‘… = Fğ‘ (Iğ´, N, ğ¿ğ‘’ ), where ğ¿ğ‘’ refers to a user specified
environment map.

A key different between NARRATE and TotalRelighting is that
NARRATE supports relighting at a new viewpoint IËœv. Conceptually
one can first conduct free-view rendering and then apply [Pandey
et al. 2021; Sun et al. 2019] on IËœv. This would require estimating a per-
viewpoint normal NËœv = Fğ‘ (IËœv) and a per-viewpoint albedo map
ğ´ = Fğ´ (IËœv, NËœv). Such an implementation is not only expensive but
IËœv
also difficult to maintain consistency across views in normal and
albedo estimation. In addition, robustly estimating normal maps
from side views remains challenging, especially due to discontinuity
caused by occlusions, e.g., the nose will partially occlude the cheek.
Alternatively, similar to NARRATE, one can warp the normal
field from the front view to new perspectives to void redundant
computations. However, such a warping requires using 3D geometry
(depth maps) where integrating the normal field relies on accurate
boundary estimation. In addition, when Ëœv is close to side views,
warping the normal map needs to accurately resolve occlusions. In
NARRATE, the front view normal map is computed using the fusion
method (Sec. 4.3) whereas the boundary is provided by the SDF ge-
ometry. This allows us to conduct warping-based relighting without
performing per-view normal re-estimation, i.e., IËœv
, ğ¿ğ‘’ )
â„ ).
where IËœv
ğ´ = Fğ´ (IËœv, NËœv

ğ‘… = Fğ‘ (IËœv

ğ´, NËœv
â„

5 PORTRAIT STYLIZATION
In addition to viewpoint changes and relighting, our hybrid neural
physical facial model supports a variety of stylization.

1https://github.com/AliaksandrSiarohin/first-order-model

Input ViewFusionWarppingNARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

7

Fig. 6. Photorealisitc Free View Synthesis Result. Our method produces high quality portrait images of new perspectives. This method generates images
with greater clarity than using the StyleSDFâ€™s Generator directly after PTI and maintains the maximum number of features of the portrait, including freckles,
eyes, lips, etc.

5.2 Style Transfer
Occasionally, the pose adjusted, relighted, and even animated por-
trait demand a different, artistic styles. We show it is very easy
to employ NARRATE in style transfer, e.g., for mapping a portrait
photograph to oil paintings.

State-of-the-art approaches essentially formulate style transfer
as an image to image translation problem. They, however, assume
aligned viewpoint and ignore lighting discrepancies. NARRATE, in
contrast, enables pose-aware, lighting-consistent style transfers.

We follow the same warping first, transfer second procedure as in
facial animation but in this case combine with a high quality image
transfer engine. Given oil painting style transfer as an example, we
set out train a pix2pixHD [Wang et al. 2018] network M to map
a photograph I with aligned pose to oil painting style IS = M(I)
with training data provided by [Karras et al. 2020]. To make sure
the poses are well aligned, we again use our pose change module to
render respective viewpoint that matches the one from the painting.
What is more exciting is that the transfer network, once trained,
can now support style transfers with pose changes: we simple map
the portrait under the new pose to its stylized version using the
same network. As NARRATE maintains high view consistent, the
resulting stylized sequence, although non-photorealistic, also main-
tains consistency. Fig. 10 and the video shows examples of various
painting styles that produce free-view portrait with corresponding
styles, virtually animating an oil painting figure.

We can also edit the lighting effects of IS. Notice that M does not
readily support lighting adjustment. We therefore utilize the shad-
ing maps to implicitly relight IS. Specifically, given a new lighting
condition ğ¿ğ‘’ , our goal is construct a relighted, stylized version IS
.
ğ‘™
We first apply our relighting module to produce a relighted image Iğ‘™
. Since IS directly
from I. Next, we compute the shading map ğ‘† =

Iğ‘™
I

comes from I, applying ğ¿ğ‘’ to IS can be approximated by multi-
plying IS with ğ‘† so that the final stylized result IS
ğ‘™ = ğ‘†IS. Fig. 10
also shows that our relighted oil painting portrait simultaneously
preserve styles, perspectives, and lighting.

Since NARRATE also provides a high quality normal map, it can
be used to produce additional stylization effects that have previously
relied on differential geometry operators, not easy to replicate by
learning based approaches. One of such example is hatching [Praun
et al. 2001]. Real artists hatch long the two principal directions along
the surface that can be computed from a high quality normal map.
The tone map of the hatching texture depends on shading, which
can also be adjusted by our approach. Fig. 10 shows the sample
result of a portrait stylized under hatching.

6 EXPERIMENTAL RESULTS
We evaluate our NARRATE framework on a number of portrait styl-
ization tasks, including perspective changes, relighting, animation,
style change, etc, as well as their combinations. We also discuss
implementation details and compare them with the state-of-the-art
techniques. It is worth noting that most existing approaches tackle
only one specific effect whereas NARRATE can tackle multiple ef-
fects at the same time. Specifically, we show NARRATE is better at
preserving find details, maintaining view consistency, producing
more photorealistic relighting, on both images and videos.

6.1 Novel View Synthesis
We first compare NARRATE with previous approaches on perspec-
tive changes for portraits. In prior art, the same effect corresponds to
novel view synthesis whereas we provide explicit pose controls. We
show NARRATE, using a hybrid neural physical approach, preserves
photorealism as well as coherence.

InputNeural InversionOursOursNeural InversionNovel View 1Novel View 28

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

Fig. 7. Consistent Free View Relighting Result. NARRATE supports consistent relighting for arbitrary views. For the input images (Col.1), we illuminate
them with two environment maps. We exhibit relighted images at two views under each light (Col.2&3, Col.4&5). We demonstrate plausible and consistent
relighting results among different views.

baseline results, a free-view portrait rendering along with a coarse
mesh.

Using NARRATEâ€™s physical modeling module, we infer the por-
trait normal as discussed in Sec. 4.2 and then fuse it with the coarse
mesh into a high-fidelity physical face model using Poisson in-
tegration. We texture the result with the original image and use
Pytorch3D [Ravi et al. 2020] to physically render the model. Finally,
we fuse the physically-rendered component (the facial part) and
the neural-inverted rest (hair, ear, clothing, background, etc) into
the final result. On a desktop with an Intel Core i9-10980XE CPU
(3.00 GHz) and a NVIDIA TITAN RTX GPU (24GB memory), neural
inversion converges in 20 minutes whereas the rest of the process
takes only a few seconds.

Fig. 6 compares perspective changes of three sample portrait, one
for each row. The baseline StyleSDF+PTI technique produces reason-
able results at individual new poses but the quality still falls short
of photographic quality. NARRATE, with the support of geometry-
aware generation, much better preserves fine details and produces
portrait-level images. For example, closeup view of the eye and
mouth regions demonstrate that the baseline tends to create overly-
smooth appearance with many important details washed out, which,
in contrast are well preserved by our technique.

A more severe artifact of the baseline is view consistency. In the
supplementary video, we synthesize free view portrait videos both
techniques. StyleSDF even with SDF constraints cannot fully account
for detail consistency. This is largely due to the radiance field based
multi-view SDF estimation is not yet capability of producing fine
details, resulting in temporal (cross-view) aliasing and vibrations.
In addition, such artifacts are magnified in portraits as human eyes
are particularly acute to tell differences under view changes. In
comparison, we impose the learned normal map, with is both high

Fig. 8. We extract normal maps from input images with different methods
and apply them in relighting. We compare the quality of our fused normal
and StyleSDF normal. Clearly, our normal is more accurate and produces
plausible relighting results, while StyleSDF extracts distorted normal and
shows weird relighting effects.

We use the seminal work of StyleSDF with its pre-trained model 2
as baseline. Recall that the original StyleSDF serves as a generator,
i.e., it synthesizes images from latent codes instead of directly chang-
ing style of a given portrait. We therefore need to neural inversion.
We use the latest PTI [Roich et al. 2021] inversion to find the closest
generated image that resembles the input portrait. To elaborate, we
follow the two-stage optimization strategy: we first fix the network
weights and conduct 1,000 iterations to optimize the latent code and
camera view; we then fix the latent and view point and perform
extra 1,500 iterations to fine-tune the network weights. We adopt
the MSE and Perception loss between the input and the inverted
images with the total loss as L = ğœ†ğ‘€ğ‘†ğ¸ Lğ‘€ğ‘†ğ¸ + ğœ†ğ‘ƒğ‘’ğ‘Ÿ Lğ‘ƒğ‘’ğ‘Ÿ . We use
the same setting as [Roich et al. 2021] with ğœ†ğ‘€ğ‘†ğ¸ = 0.1 and ğœ†ğ‘ƒğ‘’ğ‘Ÿ = 1.
Using the PTI inverted image as input to StyleSDF produces our

2https://github.com/royorel/StyleSDF

InputLighting 1Lighting 2Input ViewInput ViewNovel View 1Novel View 2InputNeural Inversion normalOursNeural InversionRelitOursNARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

9

Fig. 9. Free View Facial Animation. Combined with FOMM model, NARRATE can be used to create free-view animated virtual human. Given two source
images, we drive them with La La Land(Row 2) and Peking Opera(Row 3) clips. NARRATE further supports synthesizing motions in novel views, creating
consistent and natural free-view animation effects.

quality and free-view consistent and NARRATE produces stable and
aliasing-free results.

6.2 Free View Relighting
Next apply NARRATE in free view relighting. Our approach predict
plausible normal and albedo maps, achieving consistent illumination
effects for arbitrary poses.

We use the dynamic OLAT dataset to conduct end-to-end training
of the three individual modules of relighting network: the albedo
prediction network Fğ´, the normal prediction network Fğ‘ , and
the relighting network Fğ‘…. We use the Adam optimizer [Zhang
2018] and set the learning rate to 10âˆ’3, and use SSIM with MSE loss
functions. The training converges in 48 hours.

Fig. 7 shows free-view relighting results on several portraits with
different facial shapes and skin complexions. By applying a single
normal map across different perspectives at the same time main-
taining consistent 3D geometry, NARRATE achieves consistent re-
lighting, captures facial contours, and preserves texture details. Ben-
efiting from the high-quality dynamic OLAT dataset, we virtually
cast professional illuminations such as Rembrandt lighting on to
the subject. A unique trait of our result is that it produces photo-
graphic quality similar to [Pandey et al. 2021] whereas previous
single image relighting techniques tend to lose many fine details and
are limited to relatively simple lighting conditions. We can further
use NARRATE for image retargetting: given an input portrait and
a target one, we can simultaneously match the pose and lighting.
Specifically, we can first estimate the pose and illumination of the
target image and then use NARRATE to transform the input por-
trait to the target pose with corresponding lightings. Fig. 7 shows a

<15â—¦
72.069%

Algorithm
StyleSDF
Ours

<30â—¦
<5â—¦
81.638%
66.637%
67.449% 75.288% 81.281% 83.5095%
Table 1. Normal Error on OLAT Dataset. We compare our normal and
StyleSDF normal on captured OLAT data. Our normal is more accurate
than StyleSDF.

<25â—¦
78.635%

Method
StyleSDF
Ours

SSIMâ†‘
0.895
0.936

PSNRâ†‘ RMSEâ†“
0.072
23.809
0.054
26.426

Table 2. Quantitative comparison on portrait relighting. We then apply the
two normal prediction in relighting. No surprisingly, our better normal also
brings better relighting results.

few examples where we manage to produce photographic quality
retargeting results.

One of the major benefits of NARRATE is that it produces coher-
ent pose changes with consistent relighting effects. As we employ a
high quality normal map as constraints for relighting across view-
points and a hybrid neural-physical face model to ensure correct-
ness in pose-based warping, we produce very smooth and natural
transitions under continuous pose changes, e.g., for emulating a
face turning from the front to the side, as shown in Fig. 8 and the
supplementary video.

To demonstrate the importance of high quality normal maps,
we further show the results using the normal map from StyleSDF
for relighting. Although the overall geometry (e.g, per-view depth
maps) from StyleSDF tends to be smooth, computing the normal
maps from the estimated geometry leads to large errors, especially
near eyes, cheeks, chin, etc. This is mainly because StyleSDF does

SourceDriveDriveInputNovel View 1Novel View 2InputNovel View1Novel View210

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

Fig. 10. Style Transfer. NARRATE supports to integrate with image stylizers, producing free-view images or videos with the target style. We show oil painting
in top 2 rows and hatching in the last row. We change pose in Col 2,3,4 and light in Col.5. After editing, the results still preserve the coherence of input portrait
appearance.

not explicit use normal maps as constraints in either training or
inference. Table. 1 shows the quantitative comparisons between
NARRATE and StyleSDF normals for relighting. For the ground
truth, we use the OLAT data to generate 150 images under different
environment illuminations. NARRATE achieves better performance
in PSNR and MSE compared to using StyleSDF normal for relighting.

6.3 Facial Animation and Style Transfer
The capability of free-view relighting using NARRATE can be fur-
ther integrated with existing tools to support additional stylization.
Specifically, we demonstrate virtual facial animation and style trans-
fers.

Facial Animation. Combined with FOMM model, NARRATE can
be used to create free-view animated virtual human, with broad
applications in virtual and augmented reality. Given a source image
I and a drive video ğ·, NARRATE can animate I to perform the same
facial expression as ğ·. In all experiments, we use the pre-trained
FOMM model to drive the portrait without any fine-tuning.

Fig. 9 shows several sample animated faces under different view-
points. Both the La La Land and Peking Opera clips demonstrate that
we can convert a static portrait to live actions, whether singing or

chanting, at the photorealism. At any time instance, we can render
new viewpoints of the animated performance, potentially provid-
ing an immersive viewing experience to the audience. Our current
implementation cannot yet achieve near real-time performance. Yet
tailored accelerations may be possible to support real-time rendering
and thereby upgrading the 2D avatar animator to 3D.

Another unique capability of NARRATE that it lifts the restriction
of the input to FOMM: the original FOMM expects the input portrait
to have a pose that matches the first frame of the video ğ·1. Oth-
erwise the final generated video exhibits strong distortions due to
pose discrepencies. NARRATE eliminates this requirement: we can
simply render the portrait at the pose of ğ·1 and use it as input to
FOMM. Fig. 11 and the supplementary video compares the animated
faces with and without using NARRATE for pose correction. With
pose correction, facial distortions are significantly reduced and the
final results appear much more natural.

Alternative one-shot facial animation solution such the Neural
Talking Head (NHT) [Wang et al. 2021b], although effective, tend
to lose facial details. This is mainly due to they do not employ full
3D facial geometry in the synthesis process. For example, NTH
adopts a keypoint representation to decompose identity-specific

InputStyle Transferwith Perspective EditingStyle Transferwith Lighting EditingNARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

11

and motion-related information and it can achieve robust pose syn-
thesis under rotation as well as complex facial animation. However,
keypoints alone are insufficient to maintain detailed geometry or
appearance and their results cannot meet the quality of high res-
olution videos, e.g., for feature film productions. In contrast, by
inferring a high quality neural physical face model, NARRATE can
faithfully preserve details critical to preserve facial traits. NARRATE
further provides effective relighting capabilities absent in prior art,
to produce unprecedented lighting effects at a photographic quality.
Fig. 9 and our supplementary video show a number of examples.

Fig. 11. We compared our method with FOMM [Siarohin et al. 2019]. NAR-
RATE supports large pose variations between the source and driving frames,
producing animations with high-frequency details and vivid expression. On
the contrary, FOMM fails to hand such a case, producing distorted and
blurry results.

Style Transfer. NARRATE maintains coherence of appearance un-
der view and lighting changes as well as with animations. When
combined with an image stylizer, it produces free-view point images
or videos with the target style. Three are a number of high resolution
image stylization tools based on either on classical physically-based
rendering or emerging generative models. The former uses tailored
shaders for processing known geometry primitives, e.g., hatching re-
quires smooth surfaces to robustly estimate the hatching directions.
The latter such as PixeltoPixelHD [Wang et al. 2018] takes images
as inputs without known geometry and can produce a wide variety
of styles. As a hybrid neural physical model, NARRATE supports
both types of stylization.

Fig. 10 and the supplementary video show the typical examples of
synthesizing the oil painting painting and hatching. For oil painting,
we use NARRATE to produce a single image under the original or
new lighting or a video sequence with continuous pose changes
with fixed lighting or even track light with the movement. We then
feed the results into our syylizer, the pix2pixHD network, to stylize
individual frames. Combined with facial animation, style transfers
on top of NARRATEâ€™s free-view rendering can bring famous oil
painting figures to live and viewable in 3D, providing surreal expe-
riences to users.

Since most styles serve as de facto low-pass filters, they preserve
perspective and temporal coherence of NARRATE generated results.
A notable exception of hatching that produces additional high fre-
quency details rather than smoothing. Applying image to image
style transfer to emulate hatching effect cannot reach the quality as
in classic rendering based methods, e.g., hatching should align with
the principal directions on the mesh. Since NARRATE produces a
high quality normal map, it is straightforward to directly compute

Fig. 12. We show two failure cases. The first one contains odd camera angle
that is beyond the training distribution. As a result, our neural model fails to
run invert it to a reasonable view and geometry. The glasses regions in the
second example presents us from exacting correct normal for our physical
model, also resulting in a distortion when changing views.

the hatching directions to produce visually compelling results. More
importantly, since the normal map is coherent across view direc-
tion changes, conducting geometry-based hatching on free-view
portraits using NARRATE preserves view consistency, avoiding
disturbing shower-door effect common in generation based results.
Finally NARRATE can unify pose change, relighting, facial ani-
mation, and style transfer into a fully automated pipeline, achieving
these effects either separately or in combination, all at a photo-
graphic quality. Fig. 10 shows an example with all four effects: a
single portrait, we produce an oil painting style free-view video
of the subject performing opera. In an Andy Wahol style matrix
layout, we vary pose in one dimension and lighting the other. Each
synthesized image maintains fine details in facial traits and hence
identity and at the same time transitions across the matrix appear
coherent and harmonious. Additional results can be found in the
supplementary video.

6.4 Limitations
We have verified NARRATE on a variety of applications, providing
a new scheme for high quality portrait editing. However, as the first
pipeline to accomplish all the goals, NARRATE still has the follow-
ing limitations, as shown in Fig. 12. Firstly, our method relies on
the inherent capabilities of the geometry-aware generative models,
which would fail in the cases beyond the training distributions. E.g.,
when the camera angle is too lateral, StyleSDF produces poor fitting,
leading the failure of NARRATE as well. In addition, our physical
model is built on top of the coarse geometry obtained from StyleSDF.
However, the correctness of geometry is also limited in corner cases,
especially when the person is wearing adornments (e.g. glasses, hat).
This also influences accuracy of the physical model, bringing incor-
rect normal estimation and introducing artifacts when conducting
photo editing. This problem existed in both neural part and physical
part would be a valuable research topic in our future work.

FOMMSourceDriveOursFailure 1Failure 2 InputNovel View 1Novel View 212

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

Fig. 13. Comprehensive editing. We demonstrate the comprehensive editing ability of NARRATE. It can unify pose change, relighting, facial animation, and
style transfer into a fully automated pipeline, achieving these effects either separately or in combination, all at a photographic quality. In row 2&3, we show
free-view relighting. In row 4&5, we combine all the editing effects together.

OriginalImageLighting / PerspectiveEditingComprehensiveEditing7 CONCLUSION
In this paper, we present NARRATE, a novel portrait stylization
pipeline that enables high-quality perspective editing and relight-
ing. NARRATE leverages a hybrid neural-physical face model to
generate photorealistic images under novel perspectives, as well
as consistent input normal maps in relighting to create coherent
new illumination effects. It can further couple with various facial
applications, boosting them in free-view and relightable manner.

We experimentally demonstrate that NARRATE produces more
plausible and stable view editing and relighting results, outperform-
ing existing state-of-the-art methods by a large margin. To the best
of our knowledge, NARRATE is the first approach that supports per-
spective and lighting adjustment at photographic quality. We believe
it opens a new door for portrait stylization and has great potential
to facilitate various AR/VR applications like virtual cinematography,
3D video conferencing, and post-production.

REFERENCES
Volker Blanz, Kristina Scherbaum, Thomas Vetter, and Hans-Peter Seidel. 2004. Ex-
changing faces in images. In Computer Graphics Forum, Vol. 23. Wiley Online Library,
669â€“676.

Volker Blanz and Thomas Vetter. 1999. A morphable model for the synthesis of 3D faces.
In Proceedings of the 26th annual conference on Computer graphics and interactive
techniques. 187â€“194.

Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. 2015. Real-time high-fidelity
facial performance capture. ACM Transactions on Graphics (ToG) 34, 4 (2015), 1â€“9.
Chen Cao, Qiming Hou, and Kun Zhou. 2014. Displaced dynamic expression regression
for real-time facial tracking and animation. ACM Transactions on graphics (TOG) 33,
4 (2014), 1â€“10.

Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. 2020.
pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image
Synthesis. In arXiv.

Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De
Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative
Adversarial Networks. In arXiv.

Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang Hua. 2018. Stereoscopic

Neural Style Transfer. CVPR 2018 (2018).

L Yuan N Yu G Hua D Chen, J Liao. 2017. Coherent Online Video Style Transfer. ICCV

2017 (2017).

Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and
Mark Sagar. 2000. Acquiring the reflectance field of a human face. In Proceedings of
the 27th annual conference on Computer graphics and interactive techniques. 145â€“156.
Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. 2020. Learning an
Animatable Detailed 3D Face Model from In-The-Wild Images. arXiv preprint
arXiv:2012.04012 (2020).

Jakub FiÅ¡er, OndÅ™ej JamriÅ¡ka, David Simons, Eli Shechtman, Jingwan Lu, Paul Asente,
Michal LukÃ¡Ä, and Daniel S`ykora. 2017. Example-based synthesis of stylized facial
animations. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1â€“11.

Ohad Fried, Ayush Tewari, Michael ZollhÃ¶fer, Adam Finkelstein, Eli Shechtman, Dan B
Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. 2019.
Text-based editing of talking-head video. ACM Transactions on Graphics (TOG) 38, 4
(2019), 1â€“14.

David Futschik, Menglei Chai, Chen Cao, Chongyang Ma, Aleksei Stoliar, Sergey
Korolev, Sergey Tulyakov, Michal KuÄera, and Daniel S`ykora. 2019. Real-time patch-
based stylization of portraits using generative adversarial network. In Proceedings
of the 8th ACM/Eurographics Expressive Symposium on Computational Aesthetics
and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and
Rendering. 33â€“42.

Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2015. A neural algorithm of

artistic style. arXiv preprint arXiv:1508.06576 (2015).

Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and Kun Zhou. 2018. Warp-
guided gans for single-photo facial animation. ACM Transactions on Graphics (TOG)
37, 6 (2018), 1â€“12.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adver-
sarial Nets. In Advances in Neural Information Processing Systems, Z. Ghahra-
mani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.),
Vol. 27. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2014/file/

NARRATE: A Normal Assisted Free-View Portrait Stylizer

â€¢

13

5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf

Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. 2021.

StyleN-
eRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis.
arXiv:2110.08985 [cs.CV]

Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen,
Geoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. 2019. The
relightables: Volumetric performance capture of humans with realistic relighting.
ACM Transactions on Graphics (TOG) 38, 6 (2019), 1â€“19.

Fangzhou Han, Shuquan Ye, Mingming He, Menglei Chai, and Jing Liao. 2021. Exemplar-
IEEE Transactions on Visualization and Computer

based 3d portrait stylization.
Graphics (2021).

Berthold KP Horn and Michael J Brooks. 1986. The variational approach to shape from
shading. Computer Vision, Graphics, and Image Processing 33, 2 (1986), 174â€“208.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-Image

Translation with Conditional Adversarial Networks. CVPR (2017).

OndÅ™ej JamriÅ¡ka, Å Ã¡rka SochorovÃ¡, OndÅ™ej Texler, Michal LukÃ¡Ä, Jakub FiÅ¡er, Jingwan
Lu, Eli Shechtman, and Daniel SÃ½kora. 2019. Stylizing Video by Example. ACM
Transactions on Graphics 38, 4, Article 107 (2019).

Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time
style transfer and super-resolution. In European conference on computer vision.
Springer, 694â€“711.

Tero Karras, Miika Aittala, Samuli Laine, Erik HÃ¤rkÃ¶nen, Janne Hellsten, Jaakko Lehti-
nen, and Timo Aila. 2021. Alias-Free Generative Adversarial Networks. In Proc.
NeurIPS.

Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture
for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 4401â€“4410.

Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo
Aila. 2020. Analyzing and Improving the Image Quality of StyleGAN. In Proc. CVPR.
Vahid Kazemi and Josephine Sullivan. 2014. One millisecond face alignment with an
ensemble of regression trees. In Proceedings of the IEEE conference on computer vision
and pattern recognition. 1867â€“1874.

Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. 2017. Learning a
model of facial shape and expression from 4D scans. ACM Trans. Graph. 36, 6 (2017),
194â€“1.

Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing Kang. 2017. Visual attribute
transfer through deep image analogy. arXiv preprint arXiv:1705.01088 (2017).
Yang Liu, Alexandros Neophytou, Sunando Sengupta, and Eric Sommerlade. 2021.
Relighting Images in the Wild with a Self-Supervised Siamese Auto-Encoder. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.
32â€“40.

Abhimitra Meka, Christian Haene, Rohit Pandey, Michael ZollhÃ¶fer, Sean Fanello,
Graham Fyffe, Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, et al.
2019. Deep reflectance fields: high-quality facial reflectance field inference from
color gradient illumination. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1â€“12.
Abhimitra Meka, Rohit Pandey, Christian HÃ¤ne, Sergio Orts-Escolano, Peter Barnum,
Philip David-Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz,
et al. 2020. Deep relightable textures: volumetric performance capture with neural
rendering. ACM Transactions on Graphics (TOG) 39, 6 (2020), 1â€“21.

Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral
Agarwal, Jens Fursund, and Hao Li. 2018. paGAN: real-time avatars using dynamic
textures. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1â€“12.

Thomas Nestmeyer, Jean-FranÃ§ois Lalonde, Iain Matthews, and Andreas Lehrmann.
2020. Learning physics-guided face relighting under directional light. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5124â€“5133.
Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang,
Shunsuke Saito, Pushmeet Kohli, and Hao Li. 2017. Realistic dynamic facial textures
from a single image using gans. In Proceedings of the IEEE International Conference
on Computer Vision. 5429â€“5438.

Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira
Kemelmacher-Shlizerman. 2021. StyleSDF: High-Resolution 3D-Consistent Image
and Geometry Generation. arXiv preprint arXiv:2112.11427 (2021).

Rohit Kumar Pandey, Sergio Orts Escolano, Chloe LeGendre, Christian Haene, Sofien
Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Fanello. 2021. Total Relighting:
Learning to Relight Portraits for Background Replacement. (2021).

Patrick PÃ©rez, Michel Gangnet, and Andrew Blake. 2003. Poisson image editing. In

ACM SIGGRAPH 2003 Papers. 313â€“318.

Emil Praun, Hugues Hoppe, Matthew Webb, and Adam Finkelstein. 2001. Real-time
hatching. In Proceedings of the 28th annual conference on Computer graphics and
interactive techniques. 581.

Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin
Johnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with Py-
Torch3D. arXiv:2007.08501 (2020).

Elad Richardson, Matan Sela, and Ron Kimmel. 2016. 3D face reconstruction by learning
from synthetic data. In 2016 fourth international conference on 3D vision (3DV). IEEE,
460â€“469.

14

â€¢ Youjia Wang, Teng Xu, Yiwen Wu, Minzhang Li, Wenzheng Chen, Lan Xu, and Jingyi Yu

Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel. 2017. Learning detailed
face reconstruction from a single image. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 1259â€“1268.

Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2021. Pivotal
Tuning for Latent-based Editing of Real Images. arXiv preprint arXiv:2106.05744
(2021).

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional
networks for biomedical image segmentation. In International Conference on Medical
image computing and computer-assisted intervention. Springer, 234â€“241.

Ahmed Selim, Mohamed Elgharib, and Linda Doyle. 2016. Painting style transfer for
head portraits using convolutional neural networks. ACM Transactions on Graphics
(ToG) 35, 4 (2016), 1â€“18.

Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo, and David W Jacobs. 2018.
SfSNet: Learning Shape, Reflectance and Illuminance of Facesin the Wildâ€™. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 6296â€“6305.
Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the Latent

Space of GANs for Semantic Face Editing. In CVPR.

Yujun Shen and Bolei Zhou. 2021. Closed-Form Factorization of Latent Semantics in

GANs. In CVPR.

YiChang Shih, Sylvain Paris, Connelly Barnes, William T Freeman, and FrÃ©do Durand.

2014. Style transfer for headshot portraits. (2014).

Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris, and Dimitris
Samaras. 2017. Portrait lighting transfer using a mass transport approach. ACM
Transactions on Graphics (TOG) 36, 4 (2017), 1.

Aliaksandr Siarohin, StÃ©phane LathuiliÃ¨re, Sergey Tulyakov, Elisa Ricci, and Nicu
Sebe. 2019. First Order Motion Model for Image Animation. In Advances in Neural
Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-
Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.
neurips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf

Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, and Victor Lempitsky.
2020. Fast bi-layer neural synthesis of one-shot realistic head avatars. In European
Conference on Computer Vision. Springer, 524â€“540.

Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. 2019. Few-
shot adversarial learning of realistic neural talking head models. In Proceedings of
the IEEE/CVF international conference on computer vision. 9459â€“9468.

Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey,
Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, et al.
2021. Neural light transport for relighting and view synthesis. ACM Transactions on
Graphics (TOG) 40, 1 (2021), 1â€“17.

Zijun Zhang. 2018.

Improved adam optimizer for deep neural networks. In 2018
IEEE/ACM 26th International Symposium on Quality of Service (IWQoS). IEEE, 1â€“2.
Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Jacobs. 2019. Deep single-
image portrait relighting. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 7194â€“7202.

Peng Zhou, Lingxi Xie, Bingbing Ni, and Qi Tian. 2021. CIPS-3D: A 3D-Aware
Generator of GANs Based on Conditionally-Independent Pixel Synthesis. (2021).
arXiv:2110.09788

Jun-Yan Zhu, Philipp KrÃ¤henbÃ¼hl, Eli Shechtman, and Alexei A Efros. 2016. Generative
visual manipulation on the natural image manifold. In European conference on
computer vision. Springer, 597â€“613.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017b. Unpaired Image-
to-Image Translation using Cycle-Consistent Adversarial Networks. In Computer
Vision (ICCV), 2017 IEEE International Conference on.

Xiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. 2017a. Face alignment in full
pose range: A 3d total solution. IEEE transactions on pattern analysis and machine
intelligence 41, 1 (2017), 78â€“92.

Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham
Fyffe, Christoph Rhemann, Jay Busch, Paul E Debevec, and Ravi Ramamoorthi. 2019.
Single image portrait relighting. ACM Trans. Graph. 38, 4 (2019), 79â€“1.

Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio Orts-Escolano, Danhang Tang,
Rohit Pandey, Jonathan Taylor, Ping Tan, and Yinda Zhang. 2022. VoLux-GAN:
A Generative Model for 3D Face Synthesis with HDRI Relighting. arXiv preprint
arXiv:2201.04873 (2022).

Xiaoou Tang and Xiaogang Wang. 2003. Face sketch synthesis and recognition. In
Proceedings Ninth IEEE International Conference on Computer Vision. IEEE, 687â€“694.
Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel,
Patrick PÃ©rez, Michael Zollhofer, and Christian Theobalt. 2020. Stylerig: Rigging
stylegan for 3d control over portrait images. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. 6142â€“6151.

Ayush Tewari, Michael ZollhÃ¶fer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim,
Patrick PÃ©rez, and Christian Theobalt. 2018. Self-supervised multi-level face model
learning for monocular reconstruction at over 250 hz. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2549â€“2559.

Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard,
Patrick Perez, and Christian Theobalt. 2017. Mofa: Model-based deep convolutional
face autoencoder for unsupervised monocular reconstruction. In Proceedings of the
IEEE International Conference on Computer Vision Workshops. 1274â€“1283.

Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021.
arXiv preprint

Designing an Encoder for StyleGAN Image Manipulation.
arXiv:2102.02766 (2021).

Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping
Wang. 2021a. Neus: Learning neural implicit surfaces by volume rendering for
multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021).

Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catan-
zaro. 2019. Few-shot video-to-video synthesis. arXiv preprint arXiv:1910.12713
(2019).

Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan
Catanzaro. 2018. High-Resolution Image Synthesis and Semantic Manipulation with
Conditional GANs. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition.

Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. 2021b. One-shot free-view neu-
ral talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10039â€“10049.

Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and Feng Xu. 2020. Single
image portrait relighting via explicit multiple reflectance channel modeling. ACM
Transactions on Graphics (TOG) 39, 6 (2020), 1â€“13.

Mason Woo, Jackie Neider, Tom Davis, and Dave Shreiner. 1999. OpenGL programming
guide: the official guide to learning OpenGL, version 1.2. Addison-Wesley Longman
Publishing Co., Inc.

Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie Zhao, Weikai Chen, Kyle Ol-
szewski, Shigeo Morishima, and Hao Li. 2018. High-fidelity facial reflectance and
geometry inference from an unconstrained image. ACM Transactions on Graphics
(TOG) 37, 4 (2018), 1â€“14.

