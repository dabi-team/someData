2
2
0
2

g
u
A
9

]
I

N
.
s
c
[

1
v
8
9
9
4
0
.
8
0
2
2
:
v
i
X
r
a

Towards Enabling Next Generation Societal Virtual

Reality Applications for Virtual Human Teleportation1

Jacob Chakareski⋆, Mahmudur Khan⋆, †, and Murat Yuksel⋄

⋆

Department of Informatics, New Jersey Institute of Technology,

†Department of Electrical and Computer Engineering, York College,

⋄Department of Electrical and Computer Engineering, University of Central Florida

Abstract

Virtual reality (VR) is an emerging technology of great societal potential. Some of

its most exciting and promising use cases include remote scene content and unteth-

ered lifelike navigation. This article ﬁrst highlights the relevance of such future soci-

etal applications and the challenges ahead towards enabling them. It then provides a

broad and contextual high-level perspective of several emerging technologies and un-

conventional techniques and argues that only by their synergistic integration can the

fundamental performance bottlenecks of hyper-intensive computation, ultra-high data

rate, and ultra-low latency be overcome to enable untethered and lifelike VR-based re-

mote scene immersion. A novel future system concept is introduced that embodies

this holistic integration, uniﬁed with a rigorous analysis, to capture the fundamental

synergies and interplay between communications, computation, and signal scalabil-

ity that arise in this context, and advance its performance at the same time. Several

representative results highlighting these trade-offs and the beneﬁts of the envisioned

system are presented at the end.

1 Introduction and Motivation

Virtual reality (VR) holds tremendous potential to advance our society. It enables visual

immersion in virtual worlds created by means of computer graphics on a head-mounted

display worn by a user and has found applications so far in training, education, entertain-

ment, and gaming. An even broader set of use cases is anticipated ahead.

1This is an extended version (with more details) of a tutorial feature article that will appear in the IEEE

Signal Processing Magazine in September 2022.

1

 
 
 
 
 
 
Looking forward, VR is expected to make impact on quality of life, energy conserva-

tion, and the economy [1,2], and reach a $62B market by 2027 [3]. As the Internet-of-Things

(IoT) is becoming a reality, modern technologists envision transferring remote contextual

and environmental immersion experiences as part of an online VR session. In particular,

together with another emerging technology, known as 360◦ video, VR can suspend our dis-

belief of being at a remote location, akin to virtual human teleportation – a truly momentous

advance for our society [4]. The present state of the world (online classes, work from home,

telemedicine, and so on) due to the COVID-19 pandemic aptly illustrates the importance of

remote 360◦ video VR immersion and communication, enabled in a seamless, untethered,

and lifelike manner across the spectrum of our society, as illustrated in Figure 1.

Virtual Traveling

(Teleportation)

Figure 1: Seamless untethered virtual reality and lifelike remote scene 360◦ video for

virtual traveling/teleportation, to advance ﬁrst responders, environmental monitoring,

remote robot navigation and teleoperation, education and training, collaborative work,

healthcare and rehabilitation, urban planning, and large-scale infrastructure inspection.

Disaster 
relief

Environ
mental

Weather

Telecom
muting

Teleme
dicine

However, two main highly-intertwined communication system challenges stand in the

way of realizing this vision: VR requires (1) hyper-intensive computation and (2) ultra-

low latency gigabit-per-second wireless networking. Neither of these challenges can be

met by current and upcoming conventional network systems [5, 6], as the content to be

delivered is too voluminous and the VR headsets’ computing and storage capabilities are

insufﬁcient within an acceptable and wearable form factor. For instance, MPEG recom-

mends a minimum of 12K high-pixel-quality spatial resolution and 100 frames per second

temporal display rate, for a 360◦ video panorama experienced by a VR user [7]. These

requirements would map to a data rate of several Gbps, even after applying state-of-the-

art High Efﬁciency Video Coding (HEVC) compression [8]. Similarly, mobile GPUs lag

their desktop counterparts in computing power by a factor of ten and will not have the

required TFlops to provide the necessary VR decoding and rendering computation, at the

2

minimum resolutions and frame rates indicated above, in the foreseeable future, given the

current semiconductor technology trends [9].

Emerging 360◦ video practices compound these challenges, as they are highly inefﬁ-

cient and reuse traditional video/networking technologies in virtual reality contexts with-

out integrating their speciﬁcs [10]. This considerably degrades the quality of experience

and application utility. The under performing of these practices is even more dramatic in

mobile settings, due to the much lower wireless bandwidth and computing capability of

such devices. Regrettably, this is the context where such applications are expected to have

the highest societal impact, advancing disaster relief, the environmental sciences, public safety,

transportation, search and rescue, and urban planning, among others.

Therefore, these considerable challenges and shortcomings limit present VR applica-

tions to off-line operation, low-ﬁdelity graphics content, tethered high-end computing

equipment, and predominantly gaming and entertainment settings.

2 Objectives of the Feature Article

This article has multiple objectives of educating the broader Signal Processing Magazine

readership. Its ﬁrst aim is to educate about the importance of enabling next generation

virtual reality applications comprising high-ﬁdelity remote scene immersion and seamless

untethered lifelike navigation of the reconstructed remote environment. Its second aim is

to emphasize that traditional technology upgrade cycles alone would not sufﬁce to bridge

the performance gap to make such applications possible and that a holistic integration of

unconventional techniques and emerging technologies would be required instead. Its third

aim is to provide a tutorial of these methods and their synergistic interplay towards en-

abling the envisioned next generation applications. Finally, its fourth aim is to illustrate a

case study of rigorous high-level integration of these strategies and systematic end-to-end

modeling and analysis into an embodiment of a future mobile multi-user virtual reality

system for six degrees of freedom (6DOF) immersion. Besides its educational objectives,

a ﬁfth aim of the feature article is to identify a research framework and stimulate novel

research and community building, given the emerging nature of virtual reality and 360◦

video and their prospective broad societal impact.

Another beneﬁt of the ﬁfth aim is that the technical advances it can lead to can facilitate

3

fundamental research in other application areas of high-volume high-speed/low-latency

data transfer in emerging cyber-physical systems and IoT settings, where for the ﬁrst time

the spatiotemporal aspects of the data need to be closely explored and tightly integrated

with the user navigation actions, to maintain the desired quality of experience for the end

user, given the limited available system resources.

3 Existing Tutorials and Distinctions

There have been tutorials appearing before at conferences such as IEEE VR and 3D User

Interfaces, and ACM SIGGRAPH. However, they focused on unrelated aspects such as eye

tracking in 360-degree video, human perception in virtual environments, and virtual real-

ity content creation [11–13], which are traditionally associated with the ﬁelds of computer

vision and human perception. Similarly, another recent brief article from the IEEE Com-

munications Magazine [14] narrowly focuses on 5G (a technology upgrade cycle) and an

abstract wireless VR application as one prospective use case, using assumptions that do not

relate well to practice and thus do not lead to insightful observations. These include, for

instance, traditional models of user arrival processes from communications/information

theory, immersive experience measures expressed in percentages, and unrealistic compres-

sion rate characteristics of 4K 360◦ video content. The scope of this feature article is very

different and broader, as it aims to highlight and educate about the present challenges of

enabling future virtual reality applications, deployed in an untethered manner and with

high-ﬁdelity remote scene content, focusing on fundamental problems and trade-offs be-

tween signal processing, communications, and computation that arise in this context. An-

other objective of the article is to provide a tutorial like coverage of a non-conventional

framework of research that can help bridge the present performance gap to enable such

future applications and harness their expected societal beneﬁt.

4 Progress to date, synergistic advances, and broader implications

We outline related work in 360◦ video VR streaming using traditional approaches and sys-

tems, and synergistic advances in other technological domains that can help overcome the

fundamental bottlenecks of the former. Relative to traditional video streaming [15–18],

4

360◦ video streaming to VR headsets introduces the additional challenging requirements

of ultra-high data rate, hyper-intensive computation, and ultra-low latency, as introduced

earlier. Though some advances have been made in 360◦ video streaming using traditional

network systems, by investigating intelligent resource allocation and content representa-

tion [10, 19–21], the delivered immersion is still limited to low to moderate quality and

4K spatial panorama resolution, encoded at a temporal rate of 30 frames per second. This

outcome stems from the fundamental limits in data rate and latency of such systems and

their use of traditional server-client architectures. Moreover, a shared key shortcoming of

the majority of emerging studies that is important to note is the pursuit of heuristic de-

sign choices and the lack of analysis of the fundamental performance trade-offs among the

delivered immersion quality, user navigation patterns, signal representation, and system

resources.

Free-space optics (FSO) and millimeter wave (mmWave) are emerging wireless tech-

nologies that are presently investigated and developed to help overcome the bottlenecks

of traditional wireless systems. Both have the potential to enable multi-Gbps data trans-

mission rates. FSO exploits the light intensity of a light emitting diode (LED) or a laser

diode (LD) to modulate a message signal. FSO technologies using the former approach are

known as visible light communication or VLC, as they provide illumination at the same

time. After propagating through the optical wireless channel, the (infrared or visible) light

message is detected by a photo-diode (PD) [22]. Unlike the radio frequency spectrum,

plentiful unlicensed spectrum is available for light communications (300GHz–800THz),

which has put FSO on the road-map towards sixth generation (6G) networks [23]. While

being a novel technology, a few studies of design concepts and experimental testbeds have

already appeared [24, 25]. In the radio frequency spectrum, mmWave wireless communi-

cation is considered the enabling technology of next-generation wireless systems, as in the

range of 10-100 GHz, more than 20 GHz of spectrum is available for use by cellular or wire-

less LAN applications. mmWave has seen its ﬁrst commercial products operating in the

60 GHz band appear in the early 2010s. More complex transmission schemes to increase

even further the achievable data rate are currently being investigated [26].

A few disparate preliminary studies emerged so far examining the potential of FSO

and mmWave to advance mobile VR. A mmWave-based VR system was proposed in [27]

that uses WiGig modules for wireless connectivity between two laptop computers, one

5

acting as a server and the other as a client, with the VR headset attached to it. Similarly, a

mmWave reﬂector was developed in [28] to aid in connectivity maintenance with a mobile

VR headset in the event of blocking of the direct (line of sight) wireless link. In each case,

only synthetic computer graphics content was considered for transmission to and render-

ing on a VR headset. The company HTC released recently a wireless adapter operating at

60 GHz to enable untethered delivery of computer graphics content to VR headsets. How-

ever, the quality of experience is limited, due to the low quality and low resolution of the

delivered content, and the highly ineffective real-time compression applied at the trans-

mitting server. A design concept for using narrow beam FSO transceivers was proposed

in [24], highlighting the challenge of link maintenance in such settings. The study in [29]

explored the design of an FSO-enabled VR headset featuring hemispherically organized

layers of highly directional PDs to facilitate connectivity maintenance under challenging

head navigation movements of a VR user. Finally, Microsoft patented last year a design

concept for a free space optics enabled VR headset and transmitting system [30], indicating

vision and interest in the tech sector for actual products in the future.

In earlier related developments highlighting the prospective beneﬁts of raw video data

transmission, delivery of traditional uncompressed HD video over a short range 60 GHz

wireless link was studied for home settings targeting stationary consumer electronics that

do not integrate video compression, e.g., gaming consoles [31]. More recently, due to its

ability to reduce network latency and help mobile devices ofﬂoad part of their compu-

tation, edge computing and caching have started to be investigated for delivering 360◦

content in wireless cellular systems [32].

The studies and endeavors highlighted heretofore either address speciﬁc technology

aspects or lack a systematic end-to-end analysis. The envisioned next generation VR sys-

tem can help broadly advance the above efforts and the state-of-the-art, and make simul-

taneous impact on other emerging application areas of similar characteristics, as noted

earlier. Our preliminary advances highlight the substantial beneﬁts and potential of the

envisioned system [33, 34].

As complementary to the above discussion, we examine if technologies employed in

online multi-player VR gaming could be leveraged to enable further beneﬁts. Foveated

rendering exploits the very narrow ﬁeld of view (the central 1.5 − 2◦ of the entire ﬁeld of

view of the human eye) of the fovea, responsible for sharp central vision, to reduce the

6

rendering workload of generating an image to be displayed on a VR headset, by greatly

reducing its quality in the peripheral vision (outsize the zone gazed by the fovea) [35]. Es-

sentially, much fewer pixels and at lower ﬁdelity are rendered outside the fovea area. This

considerably accelerates the frame rate of display of successive image frames displayed on

a headset, which is helpful in dynamic gaming environments. Foveated rendering can be

integrated into the envisioned VR system towards the same goal. Still, two aspects need to

be considered carefully. Gaming content can be degraded considerably without noticing

artifacts up close, as the computer graphics content is not overly complex and is evolving

rapidly. On the other hand, high ﬁdelity remote immersion via 360◦ video may be suscep-

tible to visibly noticeable degradation in quality under the same setting, as actual scene

content is much more complex (to render) and thus sensitive to pixel resolution or ﬁdelity

reductions across the entire ﬁeld of view. Even more importantly, the most challenging

computing task faced by a headset in 360◦ video VR applications is decoding the massive

content at the target frame rate, not rendering the user’s viewpoint, as explained later.

Online gaming applications also facilitate server-client architectures featuring multiple

distributed servers to handle the massive client load [36]. Each server maintains and up-

dates a copy of the shared gaming environment, in response to the players’ actions control-

ling their respective avatars in the game. An updated state of the environment is then sent

back to all players every 100ms for ﬁrst person shooter games (most dynamic) or at a lower

rate for other types of games. Each player updates its own view of the environment using

its gaming device based on the received updated state. Though such traditional computer

network systems are suitable for this type of applications, the latencies, data rates, and

weak state inconsistencies they exhibit would not be conducive to enable lifelike remote

scene immersion on untethered VR headsets.

5 A Virtual Reality and 360◦ Video Primer

A high level system illustration of a VR application is included in Figure 2, left and mid-

dle. A user wearing a VR headset is linked to a powerful (typically gaming) computer,

equipped with a Graphics Processing Unit (GPU), computer graphics software, and a 3D

scene model. The link comprises a thick long integrated cable featuring as its main com-

ponent a high data rate multimedia cable such as HDMI that transports in the direction of

7

the user her present 3D viewpoint in the VR simulation rendered by the server computer

based on the direction of viewing of the user communicated over the cable in the opposite

direction. The navigation actions of the user comprise three head rotation angles denoted

as yaw, pitch, and roll, and are illustrated in Figure 2 right. They are measured with respect

to three coordinate axes centered on the user’s head, using gyroscopes built into the VR

headset, and determine the direction of viewing of the user in the synthesized 3D scene.

Computer graphics 
3D scene model

Rendered 3D viewpoint

Direction of viewing 
(yaw, pitch, and roll)

GPU-equipped 
desktop computer

Virtual reality simulation

VR navigation actions

Figure 2: VR 101: (Left/Middle) A user is linked to a powerful (gaming) desktop computer,

equipped with a GPU and computer graphics software, to experience a VR simulation on

her headset. The rendered 3D viewpoint of the user in the simulation and the direction of

viewing of the user are exchanged over a long high data rate multimedia cable. (Right)

The navigation actions of the user comprise rotation angles yaw, pitch, and roll around

three coordinate axes centered on the user’s head and determine the direction of viewing.

The virtual scene may be static or dynamically evolving. The simulation may also

include spatial audio content that is reproduced in parallel on stereo headphones worn

by the user. More recent VR application systems include the possibility for limited spatial

movement of the user in the virtual scene. The spatial coordinates of the user headset are

then tracked externally using infrared base stations mounted in the room where the system

resides. The present 3D viewpoint of the user is rendered based on her spatial coordinates

and direction of viewing in this case.

The major computing load of the application is reconstructing the 3D viewpoint of the

user dynamically in response to her navigation actions and can be quite intensive. It is

handled by the server computer and its GPU that execute demanding computer vision

algorithms on the voluminous geometric representation of the 3D virtual scene. The collo-

cated server computer and VR headset, and the cabled high data rate connection between

8

them help to minimize the interactive latency of the application and to deliver high volume

3D computer graphics at the display frame rate required to avoid motion sickness [37].

Still, having a tethered VR headset can represent a tripping hazard and reduces the

quality of experience of the user and the utility/scope of the application. Thus, most recent

VR application systems feature wireless headsets, with stand alone display and computing

capabilities or a slot in which a mobile phone is inserted to provide them. However, the ca-

pabilities of such systems in terms of delivered content quality, frame rate, and interactive

latency are not on par with their tethered counterparts, as described earlier.

A 360◦ video VR application system replaces the computer graphics 3D scene model

from Figure 2, left and middle, with actual remote scene 3D content. In particular, 360◦

video is a recent video format that is recorded by an omnidirectional camera that captures

incident light waves from every direction (see Figure 3, top left). Such cameras comprise

two or more wide-ﬁeld lenses and rely on computer vision image stitching to produce a

full 360◦ horizontal plane ﬁeld of view and 180◦ vertical plane ﬁeld of view [38]. Thereby,

the constructed scene content would appear captured on the interior surface of a sphere

centered on the camera location, akin to how we perceive the world around us.

Figure 3: 360◦ video capture and streaming, and user viewport Vc.

Concretely, 360◦ video enables a 3D 360-degree look-around of the surrounding scene

for a remote user, virtually placed at the camera location, on his VR headset, as illustrated

in Figure 3 right. After capture, the raw spherical or 360◦ video frames are ﬁrst mapped to a

wide equirectangular panorama (illustrated in Figure 3, bottom left) and then compressed

using state-of-the-art (planar) video compression such as HEVC. The former intermedi-

ate step is introduced, as compression techniques operating directly on spherical data are

much less mature and performing relative to traditional video compression operating on

2D video frames. Beyond the equirectangular mapping, which is most widely adopted,

9

cube, pyramid, and dodecahedron planar projections have also been studied [20].

The computing workload of a 360◦ video VR application system is even more intensive

and comprises decoding the compressed 360◦ content and rendering the current 3D view-

point of the user dynamically in response to his navigation actions. The latter is informally

known as viewport and corresponds to only a small portion of the 360◦ view sphere de-

noted as Vc in Figure 3 right. The task of reconstructing the viewport on the VR headset

requires remapping the decoded 360◦ panorama to the original spherical format and then

projecting pixels from Vc to their planar equivalents on the display of the headset.

For remote service, when the user and the stored 360◦ data are not collocated, the entire

monolithic 360◦ panorama is commonly streamed to the user using traditional state-of-

the-art video streaming (MPEG-Dynamic Adaptive Streaming over HTTP (DASH) [39]).

This considerably penalizes the quality of experience, due to the overwhelming volume of

360◦ data that needs to be delivered and that exceeds by orders of magnitude the avail-

able network bandwidth C and the computing capabilities of the receiving device. These

shortcomings are compounded by the reliance on ineffective network protocols such as

HTTP and TCP to deliver latency-sensitive multimedia data of this nature, which have

been adopted to lower the cost of an intelligent streaming system at the penalty of not

having good control of the data delivery process. Thus, only lower quality, frame rate,

and resolution 360◦ videos can presently be delivered online over the Internet. Yet, the

streaming also lacks the ultra-low latency interactivity needed for truly immersive experi-

ences, as traditional server-client Internet architectures are used in this case. The quality

of experience and application utility are even lower in traditional wireless settings, due to

the even lower data rates and computing/storage capabilities available therein, as noted

earlier.

It should be noted that the recent advances in spatially adaptive streaming of broad or

omnidirectional video panoramas [10, 19–21] have contributed to further activities within

MPEG and the follow-up DASH-SRD and OMAF standards that integrate them [40, 41].

However, most of the bottlenecks highlighted above of such standards-based traditional

approaches and their fundamental limitations in terms of enabled data rate, delay, and

computing capabilities still remain.

10

6 Bridging the Present Performance Gap

In the following, we provide a high-level tutorial-style description of several unconven-

tional techniques and emerging technologies and their synergistic interplay towards en-

abling next generation virtual reality applications comprising high-ﬁdelity remote scene

immersion and seamless untethered lifelike navigation of the reconstructed remote envi-

ronment. To set the discussion, we illustrate in Figure 4 a novel 6DOF VR system concept

that embodies their holistic integration to help bridge the present performance gap.

Shared computing

Edge server

Enhancement layer

Raw and compressed data

Emerging wireless

Transmitter steering/switching

Traditional wireless

Reliability and robustness

Viewport adaptive 
scalable representation

Baseline layer

User tracking (navigation 
actions, spatial position)

Dual-connectivity 
Layer integration

Figure 4: Future 6DOF virtual reality system to bridge the present performance gap. It

integrates synergistic edge computing, viewport-adaptive scalable 360◦ video representa-

Layer 
integration

tion, and dual-connectivity transmission via traditional/emerging wireless technologies.

Its key components are an edge server equipped with storage and computing capabilities that

will help bring the content and required computing closest to the user without overwhelm-

ing her device, to minimize the interactive latency; a viewport-adaptive scalable 360◦ video

representation that will facilitate dynamic spatiotemporal adaptation of the 6DOF content

in response to the user’s actions, to maximize the transmission efﬁciency; and dual connec-

tivity transmission via traditional and emerging wireless technologies that will simultaneously

provide reliability and robustness, and high-ﬁdelity immersion. The dual-connectivity

transmission will stream in parallel two synergistic content layers (baseline and enhance-

ment) over the two respective wireless technologies. The enhancement layer will comprise

raw and compressed content data to enable further performance synergies.

To enable its effective use, the transmission over the emerging wireless technology will

integrate transmitter steering or switching that will facilitate user tracking feedback cap-

turing the navigation actions of the user and her spatial position. The user will be equipped

with a dual-connectivity VR device that will integrate the two content layers.

Given the nature of the application, it will be an effective system design to employ dual-

11

connectivity transmission only for the down-link communication from the edge server

to the mobile VR users. The up-link communication in the opposite direction, i.e., from

the users to the edge server, can be effectively carried out by employing solely single-

connectivity transmission via reliable lower data rate traditional wireless technology links,

as it will only carry miscellaneous low-rate control information.

We implicitly have an indoor setting in mind for the envisioned system, due to the

nature of the target application. Moreover, some of the emerging technologies and uncon-

ventional techniques the system integrates have advanced further and would be easier to

deploy in such a setting. Still, all its key components can either directly apply to an out-

door scenario or have outdoor counterparts that have been developed in parallel. Thus,

an outdoor deployment of the envisioned system can potentially be pursued as well.

6.1 Raw data transmission of ultra-high resolution and frame rate content

Video compression has made the present Internet possible and has fueled its growth for

many years [42]. It enables a reduction of the required network transmission data rate by

a few orders of magnitude. However, it induces a decoding delay at the receiving client,

at the same time, proportional to the compressed data volume. This can be penalizing for

ultra-low latency applications such as online remote immersion via virtual reality, espe-

cially in mobile settings where client devices have limited computing capabilities and the

delivered compressed data is massive. Intelligent transmission of the required 360◦ video

content as raw data in this setting can help overcome this challenge, as explained next.

6.2 High-frequency directional wireless transmission

It may seem contradictory at ﬁrst to send raw video data as that would scale back up the

transmission delay, by a few orders of magnitude. However, the emerging wireless tech-

nologies of free-space optics and millimeter wave transmit data at much higher spectrum

frequencies (relative to traditional sub-6 GHz wireless technologies), to enable several or-

ders of magnitude higher data rates. Using such transmission can make sending raw video

data appealing, as it would cancel out the increase in transmission delay and still provide

the beneﬁt of lower computing delay at the client device.

However, these emerging technologies exhibit different transmission characteristics

12

relative to their traditional counterparts that need to be addressed to enable their efﬁcient

utilization. Concretely, their transmission beams are very narrow and directed, as reﬂected

waves from obstacles and the environment feature very poor signal quality, due to the

high career frequency that is used for transmission [43]. Therefore, the transmitter and

receiver need to be positioned in a direct line-of-sight of each other and be actively aligned

to maintain that property, in case of a mobile receiver. This challenge can be addressed in

the envisioned system by beneﬁting from the available user tracking information and the

limited spatial mobility of the users in an indoor setting.

In particular, high-frequency free-space optical transmitters can be mounted on steer-

able platforms and actively directed towards the users using servo motors, based on the

tracking information. Two servo motors can control the horizontal and vertical rotations,

or azimuth and elevation angles, of a transmitter tracking its user over the spatial area of

the system. Such motors can operate at 50 Hz control signal frequency and can cover a

360◦ rotation angle per second [44], which is much faster than a typical mobile user max-

imum speed of one meter per second indoor [45]. Similarly, high-frequency millimeter

wave transmitters adequate for mobile communication can be realized with multi-element

phased-antenna arrays (PAAs) to enable dynamic beam forming in both azimuth and ele-

vation in response to the navigation/mobility actions of the respective receiving users [26].

VR headsets can be equipped with respective receivers for such high-frequency trans-

mission. In the case of free-space optics, the upper portion of a headset can be covered

with an array of photo-diode detectors, which can receive and decode in parallel the op-

tical beam signal incident on their surface. The decoded incident signals at every photo-

detector can be combined using diversity combining techniques for more effective perfor-

mance. The dimensions and arrangement of the photo-detector array can be conﬁgured to

match the accuracy of the steering motors of the respective transmitters, the width of the

optical beam, and the head movement navigation actions of a user. Similarly, a small form

factor millimeter wave multi-element PAA receiver2 can be placed on the top portion of a

VR headset. As the respective transmitters for each technology would be typically placed

high above the users, blockage by human movement would not be a challenge and the

placement of the receivers on top of the VR headsets would be a natural choice.

We discuss in slightly more detail now an embodiment of the FSO transmitters and

2A prototype of size 5 × 20 millimeters, featuring a 16-element PAA, was recently demonstrated in [26].

13

!"#

$’-./#0%122’)#3
-*,3,’2’425)#

$’-./45.61*’)#

71))5)

$%&’

()’’*

+’,

"188&#’)

9)-*#.122’,3
6’-.

Layers

Photo-detectors

PD i at layer j

layer j

Top view

Side view

Figure 5: RGB-LD transmitter (left) and VR headset equipped with photo-detectors (right).

receivers in the envisioned system. LDs can enable higher transmission rates relative to

LEDs, and thus may be the preferred choice. As their transmission beam is very narrow

(typically < 1◦), a combination of red, green, and blue LDs, together with a diffuser, as

illustrated in Figure 5 (left), can be used to produce a slightly broader transmission beam

that will increase the transmission coverage and reliability, while limiting the reduction

in data rate. Simultaneously, this will lead to coherent white light emitted by the trans-

mitter that can prospectively serve as lighting. Early prototypes of such transmitters have

demonstrated data rates of up to 10 Gbps [46]. Beam splitters and detectors can be used to

monitor and control the different laser powers, for safety/performance considerations.

To mitigate the effect of narrow transmission beams and maintain high receive data

rate, an angle-diversity-receiver can be implemented, featuring multiple small-area PDs

installed on a platform at different inclination angles3. Such a multi-PD design can be par-

ticularly helpful in maintaining the highly directional optical link during dynamic 360◦

navigation head movements of a VR user. We recently investigated this approach towards

the design of a helmet-shaped VR headset, comprising multiple PDs placed along differ-

ent upper-hemispherical layers on the headset, as illustrated in Figure 5 (right). One PD

is placed on top, and the rest are distributed along different layers on the hemispheri-

cal headset surface. The design optimization aims to minimize the number of PDs used,

while ensuring uninterrupted connectivity with an FSO transmitter. It selects the num-

ber of layers, their placement along the headset hemisphere, and the number of PDs per

layer, in pursuit of this objective, and integrates VR head movement navigation data for

robust connectivity [29]. Power dissipation of the PDs is important in this setting and can

3The receive data rate of a PD rapidly drops with its surface area. However, a large surface area receiver in-

creases the reliability of the optical link. The envisioned receiver design integrates the best of both approaches.

14

be integrated into the design optimization, to impact the number of PDs placed on the

headset.

6.3 Low-high-frequency dual connectivity wireless streaming

High-frequency wireless transmission induces brittle pencil-beam like directed communi-

cation links. Streaming 360◦ video over them alone would lower the reliability, as such

links can be fragile and sensitive to line-of-sight misalignment between the transmitter

and the receiver. Integrating low-frequency (traditional sub-6 GHz) wireless transmission

as a synergistic supplement can help maintain the robustness of the system, as such radio

waves do not need a sender-receiver alignment and provide multi-path and reﬂected sig-

nal beneﬁts. Concretely, the traditional wireless connectivity can be employed to stream

a baseline layer or representation of the 360◦ content that alone will ensure an uninter-

rupted service and reliable application quality, if a transient high-frequency link loss oc-

curs prospectively. The high-frequency wireless connectivity can then be used to stream

an enhancement content layer or representation that will build upon the baseline layer to

enable high ﬁdelity immersion and augment the quality of experience of the user. Due to

the plentiful network transmission bandwidth enabled by the high-frequency connectiv-

ity, the enhancement content layer can be streamed at least in part as raw data, to enable

further system performance advances. An effective operation of such dual-connectivity

360◦ video streaming towards reliable high-ﬁdelity immersion can be advanced via a syn-

ergistic and efﬁcient design of the data representation that will enable the construction of

the two content layers, as explained next.

6.4 Scalable 360◦ video tiling and viewport-driven adaptation

Rather than streaming the entire 360◦ panorama, as conventionally done, one can construct

a scalable tiling-based representation of the 360◦ panorama to enable effective spatiotem-

poral adaptation of the transmitted data stream to the dynamic viewport of the user and

the available network bandwidth. This will lead to efﬁcient use of resources and higher

quality of experience for the user. Moreover, the scalable 360◦ tiling can facilitate rigor-

ous mathematical analysis of performance aspects of streaming systems, as we show later.

Tiling of traditional wide-panorama video has been introduced as an option in HEVC, to

15

facilitate parallel compression of the content in multi-core processor systems. More re-

cently, its beneﬁts have started to be recognized for 360◦ video streaming [19, 21].

A sample scalable tiling-based 360◦ video representation is illustrated in the right por-

tion of Figure 6. Concretely, each 360◦ video frame comprising a Group of Pictures or

GOP4 is spatially partitioned into a set of tile sectors or simply tiles (i, j) along its longi-

tude and latitude dimensions. The collection of tiles across the video frames of a GOP at

the same spatial location is then compressed into multiple embedded layers of progres-

sively increasing signal ﬁdelity. The ﬁrst (bottom) layer of a compressed 360◦ GOP-tile is

commonly known as a base layer, and the remaining layers are identiﬁed as enhancement

… … 

layers. The reconstruction ﬁdelity of a GOP-tile improves incrementally as more layers are

The globe below 
is not an image
being decoded progressively starting from the ﬁrst layer.

(cid:1848)(cid:3030)

Tile-level scalable 
video compression

… … 

…
…

(cid:1848)(cid:3030)

(cid:3037)
(cid:3036)

(cid:1864)

…
…
…

…
……………………………………
……………
…

…
…
…
……
…
…
…

)

(

x
e
d
n

i

r
e
y
a
L

360(cid:176)
panorama 
tiling

GOP

Longitude (

)

Scalable 360(cid:176) tiling

(cid:1861)

High-freq. link

Low-freq. link

Figure 6: GOP-level scalable signal tiling of an equirectangular 360◦ panorama.

The baseline content layer can be constructed from the scalable tiling-based 360◦ rep-

.

…

Low freq. link

resentation such that it comprises the ﬁrst Lb layers for a broader set of tiles encompassing

the user viewport, to account for (i) a prospective mismatch between viewport knowledge

at the server/sender, used to construct a complementary enhancement content layer, and

the actual user viewport at the receiving client, induced by rapid user head movements, or

(ii) a prospective transient high-frequency link loss. Accounting for these two possibilities

thereby will ensure that the viewport can be reconstructed continuously and will augment

the application reliability considerably. The enhancement content layer can then be con-

structed such that it comprises the subsequent Le layers for a narrower set of tiles focusing

closely on the viewport, to maximize its expected quality, as illustrated in the right portion

of Figure 6. The choice of number of layers and tile selection from the scalable 360◦ tiling to

4This is a block of consecutive frames that are compressed together with no reference to other frames.

16

 
 
 
 
 
 
 
 
 
construct the baseline and enhancement content layers can be made rigorous, inclusive of

the integrated selection of a subset of enhancement content layer tile to be transmitted as

raw data, as introduced earlier. We illustrate this as part of an end-to-end example analysis

of the envisioned VR system concept that we present later.

We note that cheap storage and easier implementation have made non-scalable video

compression preferred in practice. With minor adjustments, our system design can directly

apply to the case of constructing both content layers independently using non-scalable

compression. Moreover, scalable video coding has been successfully deployed in cutting-

edge technologies for diverse low-latency multi-party telepresence settings [47], providing

therein considerable beneﬁts in terms of lower server complexity and higher client quality

of experience over state-of-the-art non-scalable video compression based solutions. These

beneﬁts and related system context arise in the setting investigated here as well. Finally,

scalable video coding is consistently enhanced through research and every subsequent

generation of video codecs [48]. The next generation VR system we explore in this feature

article can provide avenues for advancing further such efforts.

6.5 6DOF Virtual Reality and 360◦ Video

A single 360◦ video (of actual or synthetic content) enables three degrees of navigation

freedom (3DOF) to a VR user, in the form of rotational head movements around three

orthogonal axes (as noted earlier, see Figure 2 right), in experiencing a remote scene im-

mersively from a single location. The streaming strategy described in Section 6.4 can be

extended naturally to the case when the application will also allow for spatial movement

of the user in the remote scene, to enable a 6DOF immersion experience over the spatial

area of the VR arena, where the content is navigated. Here, the user will have the ability to

select not only her direction of viewing but also the spatial location of the 360◦ viewpoint

in the scene to be explored, which will augment the quality of experience. The content

for each such spatial 360◦ video viewpoint available to be navigated can be represented

using the scalable 360◦ tiling approach highlighted above and the edge server can apply in

this case dynamic viewpoint adaptation and viewport-driven content adaptation jointly,

in response to the 6DOF navigation actions of the user, as illustrated in Figure 7.

Complementary signal processing explorations can be pursued in such a 6DOF system,

for further enhancements. For instance, additional virtual 360◦ video location viewpoints

17

360(cid:176) 6DOF remote scene video representation

Viewport-driven adaptation
[yaw, pitch, roll]

[x,y,z]

Edge
server

…

…

Select tiles and layers

Spatial location [x,y,z]
Head orientation 
[yaw, pitch, roll]

Figure 7: 6DOF remote scene immersion, and dynamic spatial 360◦ viewpoint and view-

port adaptation for a mobile VR user navigating the scene.

can be dynamically synthesized by the edge server for a navigating VR user using geo-

metric signal processing techniques from multi-view imaging such as depth-based view

interpolation [49]. Similarly, analysis can be carried out to establish the smallest number

of captured 360◦ video viewpoints and their spatial distribution over the scene to enable a

required quality level for such intermediate interpolated viewpoints.

6.6 Edge-based operation and mobile edge computing

Streaming the content from an edge server (bringing the content closest to the user) will

minimize the delivery latency compared to traditional server-client network architectures.

Moreover, the edge server’s computing capabilities can be leveraged to relieve the comput-

ing requirements induced upon a mobile VR headset. The former’s much more powerful

multiple GPUs can be effectively used to this end. In addition to the content decoding

computation noted earlier, another computational requirement induced upon a client de-

vice running a virtual reality application represents the dynamic rendering of the present

ﬁeld of view of the user (aka viewport). This introduces an additional delay component

into the entire end-to-end operational latency chain. Sharing the conventional headset de-

coding and rendering computation with a nearby edge server can be explored to optimize

the delivered immersion ﬁdelity, while meeting the strict system latency of the applica-

tion. Raw video transmission and scalable 360◦ tiling can synergistically integrate with

this objective, as well.

18

6.7 System Design and Integration Embodiment

We highlight here in greater detail how a system level integration of the techniques and

technologies outlined heretofore can be pursued towards realizing the envisioned VR sys-

tem concept in practice. To facilitate the discussion, we have included a high level illus-

tration of an arena embodiment of this future application system in Figure 8. There can be

multiple traditional sub-6 GHz wireless access points mounted on the walls of the arena

and a higher number of emerging high-frequency wireless transmitters mounted on the

ceiling of the arena. Both types of transmitters can be linked via ﬁber optical links to the

edge server installed on site. The server can control the dynamic transmission scheduling

of the traditional access points and next generation (xGen) transmitters. The user headsets

will be equipped with respective sub-6-GHz/xGen dual connectivity receivers. Moreover,

the edge server will be equipped with high-end storage and computing capabilities to ex-

ecute the envisioned dual-connectivity 360◦ video streaming. In the case of on-demand

remote immersion, the compressed 6DOF VR or 360◦ video content can already reside at

the edge server and be deployed there ahead of time. In the case of live remote immersion,

the content can be deployed in real time over a high-speed ﬁber optic Internet link to the

edge server, from the remote location where it would be captured and compressed using

scalable 360◦ tiling.

IR Tracking 
Base Station

xGen Tx

xGen link

Viewport specific 
enhancement

Sub-6 
GHz AP

k
n

i
l

r
e
b
F

i

Edge 
server

6DOF VR
content

Figure 8: An indoor arena integration of the envisioned virtual reality system. AP: access

point; Tx: transmitter

19

 
To track the spatial location and head orientation of the users, IR tracking base stations

can be mounted on the walls or ceiling of the arena. Present commercially available prod-

ucts of this nature provide accurate positional tracking of less than 1-mm error at temporal

resolutions of 250 samples per second or higher. The tracking information is relayed to the

edge server in real-time over high-speed links to enable its operation. The spatial area of

the arena can be conceptually split into multiple small sectors or cells, and one xGen trans-

mitter can be mounted above each such sector, as illustrated in Figure 8. At the onset of a

streaming session for remote immersion, a user can be assigned to a given sector and its

xGen transmitter. During the session, the user will dynamically navigate the 6DOF VR or

360◦ video content and may move across the arena. If this movement is limited within the

original sector, the same xGen transmitter can be dynamically steered towards the user, as

explained earlier.

On the other hand, as such high-frequency links experience strong signal degradation

over short distances, dynamically (re)assigning another xGen transmitter to the user can

be carried out, every time she will exit her present sector as the session evolves. This can

help maximize the received signal quality and thus data rate on the high frequency link

of the user, and minimize the interference to other high frequency links in the arena serv-

ing other users, at the same time. Thereby, the quality of experience of all simultaneous

users in the arena can be augmented. Finally, if more than one user happen to transiently

pass through or reside in the same sector at the same time, during the course of the ses-

sion, the resources of the high-frequency xGen transmitter of that sector can be split across

these users during that time, uniformly or preferentially, for some alternative transmitter

assignment and steering settings/methods for the system5

In particular, we highlight brieﬂy here the prospective options for xGen transmitter

steering and (re)assignment in our system, to provide further clarity and instruction to the

reader. In the case of FSO wireless technology, electronic, mechanical, or hybrid electro-

mechanical transmitter steering can be carried out. (i) The ﬁrst approach is also known as

electronic transmitter switching or assignment, as it effectively activates an FSO transmit-

5We recall that the system design to exclusively serve each user by one xGen transmitter via its narrow

transmission beam that is dynamically directed towards the user, as outlined in the Section ”High-Frequency

Directional Wireless Transmission” earlier, is the default choice that we focus on as the most adequate and

performing for the next generation VR system we investigate.

20

ter associated with a spatial cell into which a user enters, and deactivates the FSO transmit-

ter associated with the cell that the user exited. This approach comprises the same number

of FSO transmitters as spatial cells and their beam width is selected to ensure coverage of

the entire spatial area of their respective cells, while minimizing the overlap with transmit-

ter beams of adjacent cell, to minimize the inter-cell interference. Here, the FSO transmit-

ters are ﬁxed (static) in orientation and point directly downwards to the spatial area of the

cells. Lastly, when multiple users are present in the same cell, the transmission resources

of the respective FSO transmitter can be shared across them using multi-access techniques,

e.g., time-division multiple access (TDMA). (ii) Employing mechanical steering requires

having at least the same number of transmitters as users in the arena, if there are more

users than spatial sectors/cells in the arena6. In this case, there can be multiple transmit-

ters mounted above the spatial area of a single cell and their orientation is dynamically

steered towards their respective users to whom they have been assigned using servo mo-

tors, as introduced earlier. Periodic reassignment of users to transmitters over the course

of a session can be carried out here as well, as the users move across the spatial area of the

arena during the course of a media session, to enhance the system performance, as noted

a little earlier. (iii) Finally, the hybrid electro-mechanical steering aims to integrate both

prior approaches, where a user can be tracked by his or her (re)assigned transmitter using

mechanical steering until the user enters a cell where another user is already present. In

that case, both users will start to be served using a single static transmitter mounted above

the center of that cell using a multi-access technique. Once a user becomes alone again in

his or her present cell, the system will switch back to mechanical steering of the respective

transmitter assigned to the user, towards the user.

In the case of mm-wave wireless technology, electronic beam formation and steering

can be employed to ensure transmitter-receiver alignment as the users move across the

arena, as noted earlier in the Section ”High-Frequency Directional Wireless Transmission”.

Here, one option can be to have at least as many transmitters as users in the arena, if

there are more users than spatial cells in the arena. Thus, potentially there can be multiple

transmitters mounted above each cell in the arena system. Alternatively, the system can

6In the experimental evaluation, we address this choice simply by setting the number of xGen transmitters

per cell to be equal to ⌈Nu/Nc⌉, where Nu and Nc denote respectively the number of spatial cells and users in

the arena system, and ⌈·⌉ denotes the ceiling operator.

21

comprise the same number of transmitters as cells in the arena, mounted above the center

of each respective cell. In this latter case, multiple users present in the same cell can be

served by the same transmitter of that cell using multi-beam formation and steering, to

share its transmission resources. Multi-beam mm-wave transmitters or access points are

prevalently encountered in practice. Finally, in each case, dynamic reassignment of users

to transmitters can be carried out during the course of the session as the users move across

the spatial area of the arena, to enhance the system performance.

The beneﬁts and shortcomings of the transmitter steering and (re)assignment meth-

ods available for each xGen wireless technology and outlined here, in the context of our

system, have been explored recently in [50, 51].

7 Example End-to-end Analysis

We guide the reader here through several high-level analysis examples that build upon

each other to highlight the prospective beneﬁts of the envisioned VR system and motivate

new studies that can help advance it further. In particular, we ﬁrst illustrate how the 360◦

video tiling and the ability to collect user navigation data can facilitate the development

of statistical models of user navigation. In conjunction, we illustrate how effective rate-

distortion modeling of the spatiotemporal encoding characteristics of the 360◦ panorama

across its tiles can be pursued. We then highlight how these modeling advances can be

integrated to pursue further analysis that captures the fundamental trade-offs between

computing, communication, and signal representation in the context of the end-to-end

application performance enabled by the system. Finally, we illustrate how rigorous system

level optimization can be built upon these integrated modeling and analysis advances to

select key resource allocation decisions.

Navigation modeling. The head movement navigation actions of a user can be col-

lected over time at the edge server and can be used to characterize the probability that a

360◦ tile will appear in the user viewport during a time period. This will inform the under-

standing of how important each tile and its content are for the quality of experience of the

user. One way to capture this information is to integrate the number of instances a tile will

appear in the viewport during navigation of the content over the time interval of interest.
This approach can be made rigorous by identifying the spatial portion Snm,Vc

of each 360◦

j

22

tile (n, m) present in the user viewport Vc at the temporal display instance tj of the j-th

panoramic video frame comprising the 360◦ content. Care must be taken here to account

for the widely varying viewport size depending on its latitude on the 360◦ panorama, to

integrate this information properly then over time. This phenomenon is induced by the

equirectangular mapping of the native spherical data that is employed at compression, as

explained earlier. It is illustrated in Figure 9 that highlights the shape and size of a view-

port on the 360◦ panorama for two different yaw and pitch rotation angles, denoted therein

as azimuth and polar angles (ϕ, θ) in spherical coordinates.

(ϕ, θ) = (0◦, 0◦)

(ϕ, θ) = (120◦, −60◦).

Figure 9: The viewport’s latitude coordinate on the 360◦ view sphere (polar angle θ = head

rotation angle pitch) impacts its shape and size on the 360◦ panorama.

Formally, the quantity Snm,Vc

j

represents the overlap or intersection between the spatial

areas of the tile and the viewport in the 360◦ panorama at that time instance and can be
captured as Snm,Vc

j denote the sets of pixels representing

, where Snm

j ∩ Snm

and SVc

= SVc

j

j

j

the two denoted spatial areas. To account for the unequal viewport size on the equirect-

angular plane across time, in developing a statistical model of user navigation, one can

normalize the fractions of the spatial areas of every tile present in the user viewport Vc at
tj, using snm
of pixels. Thus, {snm

|Snm,Vc
n,m |Snm,Vc
j } represents the normalized distribution of the spatial area of the user

, where |S| denotes the size of a set S, in this case in number

j =

P

|

|

j

j

viewport across every tile in the 360◦ panorama, at time instance tj.

Finally, given {snm

j }, one can formulate the probability (likelihood) of the user navi-

gating tile (n, m) over a time interval spanned by the time instances [ti, tj], as P (ti,tj )
j
k=i snm
j−i+1 . In other words, P (ti,tj )
k
P
part) in the user viewport during navigation of the 360◦ video from its temporal instance

indicates how likely tile (n, m) will appear (at least in

nm =

nm

ti to its temporal instance tj, or the popularity of the 360◦ scene content captured by the

23

tile for these user and time interval. For instance, if ti and tj correspond to the ﬁrst and
last video frame of the 360◦ video, then, P (ti,tj )

nm captures the navigation probability or pop-

ularity of tile (n, m) across the entire video.

To highlight the nature of these quantities, we illustrate in Figure 10 their expected

values (averages across a large user set) for two popular 360◦ videos used in our analysis,

where we considered a popular 6 × 4 tiling7. Concretely, one can observe that in the case

of Roller Coaster, video tiles on the fringes of the 360◦ panorama are rarely navigated by

a user, i.e., they scarcely appear in the user’s viewport, during a 360◦ video navigation

session. Conversely, video tiles indexed as (n, m) = (3,2), (4,2), (3,3), and (4,3) are quite

often navigated by the user, as noted by their much higher navigation likelihoods shown

in the histogram.

Figure 10: Expected {Pnm} for 360◦ tiles for Roller Coaster (left) and Wingsuit (right).

The navigation probabilities of 360◦ video tiles for the video Wingsuit, induced by users

experiencing the content, look fairly different. This is due to the nature of this content and

the induced speciﬁc interests of the users, expressed when navigating it. In particular, one

can see here that the typical user is predominantly interested in navigating the southern

hemisphere of the 360◦ panorama, as seen from the respective tile navigation probabilities

indicated in the histogram. The more dynamic and interesting content of Wingsuit resides

spatially in the southern hemisphere of its 360◦ view sphere.

To address the prospect of imperfect or lack of knowledge of near future navigation ac-

tions and introduce further robustness into the system, e.g., in contexts such as live remote

immersion, predicting the upcoming head movement (or jointly head and body move-

7Denser tiling layouts increase the processing complexity and reduce the compression efﬁciency, but enable

more precise delineation of the user viewport and thus more accurate analysis and resource allocation.

24

ment) navigation actions or 360-degree tile navigation likelihoods for a user can be inte-

grated. This can be carried out accurately via regression or machine learning techniques,

based on the user’s navigation history and prospectively including features extracted from

the already navigated content [52, 53]. It has been shown that the navigation actions of a

user exhibit a strong low-frequency component and short-term correlation over time that

can beneﬁt such prediction methods. Moreover, it has been observed that the dynamics of

the navigation actions of a user generally correlate well with the temporal characteristics

of the content that is navigated [54].

Rate-distortion modeling. One can also accurately model the rate-distortion depen-

dency of compressed GOP tiles to facilitate analysis and optimization of the edge server’s
operation. Here, we examine two models, exponential (D = c1e−d1R) and power law
(D = c2Rd2), to capture this dependency. Concretely, we vary the encoding rate R of a

tile and record its respective Mean Squared Error (MSE) reconstruction distortion D. We

graph the obtained pairs (R, D) in Figure 11 with markers, for three representative tiles

of the popular 360◦ video Roller Coaster. The two analytical models are also graphed in

Figure 11. We can see that Tiles 3 and 16 have steeper rate-distortion dependency, due to

their relatively static content. Tile 11 requires higher encoding rate to achieve the same

reconstruction error, due to its more dynamic content. Figure 11 shows that the power law

model provides a more accurate characterization across all three tiles. This motivates its

use in our later analysis.

14

12

10

8

6

4

2

0

E
S
M

R-D Analysis - Power Law

Tile 3
Tile 11
Tile 16

0

0.5

1

1.5

2
Data rate (Mbps)

2.5

14

12

10

8

6

4

2

0

E
S
M

R-D Analysis - Exponential

Tile 3
Tile 11
Tile 16

0

0.5

1

1.5

2
Data rate (Mbps)

2.5

Figure 11: Rate-distortion dependency D(R): (Left) Power law model and (Right) Expo-

nential model. Actual data points shown as markers.

We note that in video compression the raw pixel data is stored in the three-dimensional

25

luminance-chrominance color space denoted as YUV, to enable data volume reduction.

The Y (luminance) component captures exclusively the signal strength of each pixel, i.e.,

its monochromatic intensity or brightness. The two U and V chrominance components

capture exclusively the color information associated with each pixel. Video signal distor-

tion or quality is commonly measured only on the Y component due to its nature.

Motivating resource allocation example. Building upon the two modeling advances

and scalable 360◦ tiling, we illustrate here how data rate resources can be effectively allo-

cated over the 360◦ panorama and time. When we examine and formulate the end-to-end

performance of the envisioned VR system concept later, we will integrate this motivating

example analysis therein.

When the 360◦ content is compressed using scalable tiling, the data rate and reconstruc-

tion error of a GOP tile (n, m) can be directly related to the number of embedded layers

lnm selected to represent the tile. These relationships can be made precise using the rate-

distortion modeling described earlier. Similarly, given the statical navigation proﬁle {Pnm}

the user during the GOP as

of a user for this GOP, one can formulate the expected viewport distortion experienced by
Pnm PnmDnm(lnm). Finally, given a maximum streaming data
rate of C, one can then aim to optimally select the number of layers lnm sent for every tile

(n, m) during the GOP time interval, such that the expected user viewport distortion over

that interval is minimized. This can be formally captured via the optimization:

min
{lnm} X
nm

PnmDnm(lnm), subject to:

Rnm(lnm) ≤ C.

X
nm

To highlight its beneﬁts and potential, we implemented the optimization above, varied

C, and computed the optimal solution and the respective minimum expected viewport dis-

tortion in each case (the value of the objective function at the optimal solution). Simultane-

ously, we recorded in the same context the corresponding performance of the state-of-the-

art MPEG-DASH streaming standard, implemented with spatial viewport-adaptation [10].

We examined two popular 4K 360◦ videos Wingsuit and Angel Falls to carry out this mo-

tivational assessment. We show these outcomes in Figure 12 where the y-axis indicates

the delivered viewport quality or immersion ﬁdelity via a base-10 logarithmic inverse of

the expected distortion, formally known as the peak-signal-to-noise ratio (PSNR). We note

that throughout this paper, the reported PSNR results are measured with respect to the

luminance (Y) component of the user viewport video signal, as explained earlier. On rare

26

occasions, these results are denoted as Y-PSNR to indicate this aspect.

52

50

48

46

44

42

40

)
)

B
d
(

R
N
S
P

(

y
t
i
l

a
u
q

t
r
o
p
w
e
V

i

38

8

10

12

MPEG-DASH Wingsuit

MPEG-DASH Angel Falls

Proposed Wingsuit

Proposed Angel Falls

14

16

18
Streaming data rate (Mbps)

20

22

24

26

Figure 12: 360◦ video transmission efﬁciency of the motivating resource allocation example

vs. MPEG-DASH.

We can see that the approach highlighted above enables considerable beneﬁts over

MPEG-DASH by integrating the user navigation actions and the rate-distortion trade-offs

across the 360◦ panorama, in deciding how transmission resources should be allocated.

Approximately 6-7 dB of immersion ﬁdelity improvement have been enabled across both

360◦ videos and all network bandwidth values considered in Figure 12. These advances

can in turn enable much higher operational efﬁciency for a streaming system for 360◦ video

delivery that integrates this methodology, as our subsequent analysis will highlight.

The problem formulation can be easily extended to integrate tile-level weights wnm

that will capture the amount of deformation (stretching) that the content of each tile (n, m)

undergoes on the 360◦ sphere when mapped back for viewing on the VR headset. This de-

formation is solely dependent on the tile’s latitude and emphasizes equatorial tiles as more

sensitive to content distortion because of this. By this integration, the formulation will

capture as its objective the expected tile-level weighted to spherically-uniform viewport

distortion, whose pixel-level deterministic counterpart denoted as WS-MSE has recently

been introduced as a more adequate quality metric for omnidirectional content [55].

Per-user end-to-end performance analysis. The edge server in the envisioned VR sys-

tem can beneﬁt from a navigation proﬁle {Pnm} for a GOP to preferentially treat the tiles

27

 
 
 
comprising it when streaming the content to the respective user, as motivated by previ-

ous resource allocation example. Concretely, the server can ﬁrst identify as M the set of

GOP tiles with non-zero navigation likelihoods. To construct the baseline content layer, the

server can assign streaming data rates Rnm,w to lower indexed embedded layers from the

scalable 360◦ tiling exclusively for tiles (n, m) from M , to maximize the resource utiliza-

tion. If ∆T is the temporal duration of a GOP, then the latency of streaming the baseline

content layer to a user over her traditional wireless connectivity link can be formulated as
τ w = P

, where C w indicates the transmission capacity of the link.

(n,m)∈M Rnm,w∆T
Cw

Similarly, by facilitating {Pnm}, the server can identify a subset M r of tiles in M that

are most relevant for the quality of experience of the user. The construction of M r can be

made rigorous as explained a little later. These tiles can be streamed as raw data as part

of the enhancement content layer, to maximize their impact. They can be reconstructed

ﬁrst as such at the edge server at the highest quality, from the compressed scalable 360◦

tiling that will reside there, by decoding each compressed tile from M r from all its embed-

ded layers. If the encoding data rate of the entire set of layers for tile (n, m) is Rnm,max,

then the decoding latency of reconstructing tiles from M r as raw data at the server can be
formulated as τ Z = P

, where Z is the decoding speed of the server.

(n,m)∈M r Rnm,max∆T
Z

The construction of the enhancement content layer by the server can be completed by

selecting an adequate set of higher indexed embedded layers from the scalable 360◦ tiling

for the remaining (compressed) tiles in M (the set M e = M \ M r). Let Rnm,x be the selected

streaming data rate for each such tile in the enhancement content layer. This content layer

will be streamed to the user over her xGen wireless connectivity link characterized with

transmission capacity C x. The associated transmission delay can be characterized as τ x =
|M r|Er+

(n,m)∈M e Rnm,x∆T

, where Er denotes the data size of a raw GOP tile.

P

Cx

Finally, the latency on the client device involves decoding compressed tiles and render-

ing the viewport. These tasks will be carried out twice for the baseline and enhancement

content layers streamed to the user, to enable respectively application reliability and high

quality immersion, as introduced earlier. The decoding capability (speed) z of the headset

will need to be partitioned between decoding the baseline content layer and decoding the

subset of compressed tiles from the enhancement content layer M e. Let these two decod-

ing speed allocations be denoted as zw and zx, respectively. Thus, the required time to
(n,m)∈M Rnm,w∆T
carry out each of these two decoding tasks can be formulated as τ z,w = P
zw

28

(n,m)∈M e Rnm,x∆T
zx

and τ z,x = P
will need to be partitioned between rendering the viewport solely from the baseline con-

. Similarly, the processing capability (power) r of the headset

tent layer and rendering the viewport at enhanced quality jointly from the baseline and

enhancement content layers. Let these two processing power allocations be denoted as rw

and rx, respectively. The time delay of carrying out each of these two rendering tasks can
be formulated as τ r,w = Ev
rwbh

, where Ev is the data size of the viewport

and τ r,x = Ev
rxbh

after decoding and bh is the computed data volume per CPU cycle on the headset.

Beneﬁting from the earlier modeling, the quality of immersion delivered to a VR user

by the envisioned dual-connectivity system can be accurately captured by formulating the

expected viewport distortion experienced by the user as:

D ({Rnm,w, Rnm,x}) =

PnmanmRbnm

nm,max +

X
(n,m)∈M r

X
(n,m)∈M e

Pnmanm (Rnm,x + Rnm,w)bnm ,

where anm and bnm are the rate-distortion power law model parameters for tile (n, m). Ad-

ditionally, each summation term above can be weighted by a tile-level WS-MSE coefﬁcient

wnm that captures the unequal impact of each tile (n, m) when the panoramic content is

re-projected back on the 360◦ sphere for viewing, as introduced earlier.

The analysis of latency and quality of immersion can enable pursuing end-to-end op-

timization of the system. One way of formally capturing this is via the problem formu-

lation highlighted below that aims to maximize the quality of immersion delivered to a

user given various system latency and resource constraints. To make the notation more

compact, we henceforth replace the symbols (n, m) with (i, j).

min
{Rij,w},{Rij,x},
M r,{zw,zx},{rw,rx}

D ({Rij,w, Rij,x}) ,

subject to:

τ w + τ z,w + τ r,w ≤ ∆T,

τ Z + τ x + τ z,x + τ r,x ≤ ∆T,

Rij,w ∈ [Rij,min, Rij,max], Rij,x ≤ Rij,max − Rij,w,

rw + rx ≤ r,

zw + zx ≤ z.

(1)

(2)

(3)

(4)

(5)

The inequalities (2) and (3) capture the application latency requirements associated

with the transmissions on the two wireless connectivities. The inequalities in (4) capture

the data rate limits enabled by the scalable 360◦ tiling, where Rij,min is the smallest possible

29

data rate for tile (i, j) (the data rate of its ﬁrst embedded layer). We note that the transmis-

sion latency constraints (3) and (4) are stricter than and imply the respective transmission

capacity constraints on the two wireless connectivity links. Thus, the latter two constraints

do not need to be included in the optimization (1)-(5). Finally, the inequalities in (5) capture

the limited computational resources of the VR headset of the user.

The problem (1)-(5) is mixed-integer programming, which is hard to solve optimally in

practice. Still, with a clever selection of M r, one can pursue the optimal solution at lower

complexity. In particular, for a given set of tiles to be sent as raw data, the problem above

can be transformed to geometric programming and solved exactly via iterative approaches

that converge rapidly [56]. Moreover, though the number of tiles in M is not excessive,

instead of pursuing a combinatorial approach to identify the optimal M r as part of the

overall solution to (1)-(5), one can accomplish that more effectively by sorting the tiles in

M according to an adequate criterion, related to the objective function, and constructing

M r to comprise the ﬁrst k tiles from the sorted M . By sweeping k across a range of values,

solving (1)-(5) for each such thereby constructed M r using geometric programming, and

identifying the optimal value of the objective function in each case, one can then identify

the overall solution.

Moreover, though k can technically range up to |M |, the maximum number of tiles

that can be sent as raw data will typically be smaller, as otherwise the system latency

constraints will be violated. This is a result of the large data size of a raw GOP tile and

the present transmission capacities of xGen technologies. Yet, when the latter are selected

rather too conservatively in (1)-(5), even sending one raw tile can become unfeasible, and

the choice of k = 0 lends itself as optimal. Finally, we observed that selecting the naviga-

tion likelihood weighted derivative of the rate-distortion dependency for a GOP tile as the

criterion for sorting the tiles in M in decreasing order leads to the same overall solution

produced when instead a combinatorial search is used to identify M r.

The optimization described in this section would be applied at the edge server to every

subsequent GOP of the content transmitted to a given user. In a subsequent section, we

will build upon the analysis and optimization developed here, to formulate an overall

multi-user optimization strategy for the envisioned system.

Dynamic xGEn transmitter assignment. Earlier, as part of the arena embodiment of

the envisioned system outlined in Section 6.7, we highlighted a prospective dynamic re-

30

assignment of xGen transmitters to mobile VR users during the course of a session for

further performance enhancement, to complement the multi-user system level resource

allocation optimization described next. In particular, during a session, as the mobile VR

users navigate the content across the spatial area of the arena, such reassignment can be

carried periodically such that, for instance, the smallest signal-to-noise ratio (SNR) across

all transmitter and user pairs in the arena, is maximized. As SNR is inversely propor-

tional to the distance from a user and an xGen transmitter, an equivalent approach can be

pursued aiming to minimize the longest distance between a user and a transmitter, in an

assignment. Either of these approaches will lead to a combinatorial optimization problem

that may be complex to solve, depending on the number of users and transmitters in the

system.

An alternative lower-complexity strategy would be to formalize the assignment as a

graph-bottleneck matching on a bipartite graph comprising the users and xGen transmit-

ters, as the two vertex set partitions, and where the graph edge weights would correspond

to the distances between the users and transmitters. Concretely, in this case one will seek

to ﬁnd the maximum size vertex matching on the graph, whose biggest edge weight is

smallest across all the matchings of that size. An efﬁcient algorithmic solution can then

be formulated that will integrate and beneﬁt from the Dulmage–Mendelsohn decompo-

sition of such a graph, to construct the optimal assignment π incrementally. Our recent

advances demonstrate the beneﬁts of this strategy [33]. Moreover, we have empirically

observed that for the diverse settings considered in [33] using the max min SNR as an ob-

jective metric for the transmitter assignment enables virtually the same performance, at

much lower computational cost for the optimization, over the case of using the max min

SINR (Signal-to-Interference and Noise Ratio). The performance evaluation that we carry

out subsequently integrates the impact of interference via the SINR.

Multi-user system performance analysis and optimization. The overall objective of

the system is to maximize the delivered immersion quality across all users in the arena.

Once users are assigned to xGen transmitters, as described above, this objective can be

pursued by formulating and solving a respective optimization problem of interest. Build-

ing upon the notation and formalism introduced earlier in Section ”Per-user end-to-end

performance analysis”, this multi-user problem can be formally described as:

31

(MU-OPT):

(cid:26)

{Ru
M r

ij,w},{Ru
u ,zx
u,{zw

min
ij,x},Cw
u ,rx

u},{rw

u ,Cx
u,
u},Zu(cid:27)∀u

subject to:

(2) − (5),

Zu ≤ Z,

C w

u ≤ C w,

X
u

X
u

C x

u ≤ C x,t, ∀t,

X
u∈Ut

Du

(cid:0)

X
u

{Ru

ij,w, Ru

ij,x}
(cid:1)

,

(6)

(7)

(8)

(9)

where we have introduced a subscript or superscript u to all symbols used in (1)-(5) earlier,

to indicate a speciﬁc user u here. Moreover, Z denotes the aggregate decoding capacity of

the edge server in (7), C w denotes the aggregate (down-link) transmission capacity of the

traditional wireless technology in (8), and C x,t denotes the aggregate (down-link) trans-

mission capacity of the xGen transmitter t in (9). Finally, alike in (9), Ut denotes the set of

users assigned to xGen transmitter t.

Solving the optimization problem above (MU-OPT) in its full generality can be pur-

sued via iterative techniques and the Lagrange multiplier method that would build upon

the respective solution of the per-user optimization (1)-(5) as a key inner step of such ap-

proaches. Moreover, solving for the respective Lagrange multipliers associated with the

constraints (7)-(9) as part of the overall optimization can be carried out using for instance

integrated fast sub-gradient techniques. Next, we outline the impact of parameter selec-

tion, user-transmitter assignment setting, and the indoor nature of our system on (MU-

OPT).

With judicious analysis and choice of key system resource parameters captured by the

constraints (7)-(9), the general optimization outlined above can be simpliﬁed. Concretely,

it is expected that in our system setting the edge server will be equipped with comput-

ing capabilities that exceed those of the mobile clients by orders of magnitude and that it

will be well provisioned to assist abundantly the expected number of users in the arena

system with such capabilities. Thus, one simple approach of dispensing with (7) without

preventing the integrated per-user optimization (1)-(5) from exploring the full range of per-

formance trade-offs and beneﬁts is to set Zu = Z/Nu, ∀u, where Nu is the number of users

in the arena. We have empirically veriﬁed this assessment in our extensive evaluations.

32

It is rational to follow the same approach to dispense with (7) by selecting C w

u = C w/Nu,

with virtually no impact on the optimal performance delivered to each user for the follow-

ing reasons. First, it is similarly expected that in our system setting the traditional wireless

connectivity will be well provisioned to provide plentiful down-link transmission capac-

ity to the expected number of mobile VR users to be served. Second, the envisioned arena

system concept has an indoor nature and a relatively small spatial footprint. Third, the

traditional wireless connectivity is integrated solely to augment the system/application

reliability. Optimizing the choice of C w

u will not make an impact on this objective.

Finally, dispensing with (9) and addressing the choice of C x

u , ∀u, is more subtle. On the

one hand, in an embodiment setting of the envisioned system that integrates prospective

multi-user assignment of its xGen transmitters, as discussed earlier in the last segment of

Section ”System design and integration embodiment”, it is alike expected that each xGen

transmitter t will be equipped with ample additional transmission capacity to support the

prospective serving of multiple users in parallel and provisioned adequately for the ex-

pected number of users to be assigned to a single xGen transmitter during a session. Then,

dispensing with the constraint (9) by splitting the transmission capacity of each transmit-
u = C x,t/|Ut|, can be a sensible choice in

ter t uniformly across its assigned users, i.e., C x

this context as well, which may not impair notably the truly optimal high-quality immer-

sion that can be enabled for every user in the arena via the assigned xGen transmitter. On
the other hand, the rate-distortion dependency Du (cid:16){·, Ru
ij,x}(cid:17) that captures the delivered
viewport quality may exhibit different performance trade-offs across different users u over

time, depending on the navigation characteristics of the users and the 6DOF content that

is explored. This factor may lead to some performance loss depending on the magnitude

of the difference between the uniformly assigned capacity C x

u and its rigorously selected

value as an integral part of the optimization with the constraint (9) kept in place. Still, it

should be noted that for the present values of the transmission capacity C x

u that can be as-
signed to a user and rate-distortion dependencies of existing 3DOF/6DOF 360◦ video/VR

content, the operational points of a streaming system typically lie in the (far right) satu-

ration portion of the rate-distortion performance dependency. This means that marginal

differences in the value of C x

u assigned to user u will only make a negligible difference in

the overall system streaming performance and quality of experience delivered to the user.

All the above considerations outlined here related to the choice of C x

u , ∀u, with the aim to

33

prospectively omit (9) from the optimization need to be carefully considered.

Yet, we have empirically observed in our extensive evaluations that the system setting

comprising dynamic steering of the xGen transmitters and an exclusive assignment of one

user per transmitter, i.e., the number of users and xGen transmitters in the arena is the

same, provides the best performance over other prospective settings of user-transmitter

assignment and transmitter steering, outlined earlier in the last segment of Section ”Sys-

tem design and integration embodiment”. For this characteristic setting of our system, the
u = C x,t for the unique user u

constraint (9) will be met by default, by simply setting C x

assigned to that xGen transmitter t. This will dispense with the need for (9) and will si-

multaneously enable delivering the highest possible immersion quality to user u. In turn,

the latter will also signify that the best overall performance of the system is achieved, since

the multi-user optimization (MU-OPT) formulated herein will decouple into multiple in-

dependent instances of the per-user optimization (1)-(5) applied to every user-transmitter

pair (u, t), given the additive nature of the objective function in (6) and the rationale for

dispensing with the other constraints (7)-(8) described earlier. Lastly, beyond the argument

carried out regarding the system-level constraints of (MU-OPT), as it applies to our spe-

ciﬁc context, we note that computational complexity reduction for an optimization method

and user fairness are two commonly invoked reasons to justify uniform resource sharing

across multiple entities in resource allocation problems, which that can be applicable here

too.

Let {GOPu,k}∀u denote the collection of GOP content that needs to be transmitted to

every user u in the system at the onset of the GOP temporal interval k of the 6-DOF remote

immersion session. The optimization strategy described here would be applied at the edge

server to every subsequent {GOPu,k}∀u, i.e., for k = 1, 2, . . .

Performance analysis examples. We highlight some of the key performance trade-

offs arising in the arena embodiment setting of the envisioned system by implementing

in simulation its most critical components and the optimization (1)-(5), together with the

graph matching based strategy for dynamic user to xGen transmitter assignment outlined

above. We considered that the spatial area of the arena is 6m × 4m and is split into six

sectors. We further considered that there are six users in the arena and six xGen transmit-

ters mounted on the ceiling above the center point of each cell, as illustrated in Figure 8.

The beams of the transmitters are steered towards their assigned mobile users either elec-

34

tronically, in the case of mm-wave wireless technology, or mechanically, in the case of FSO

wireless technology, as introduced earlier in the Section ”High-frequency directional wire-

less transmission”. We integrated tile-level spherical distortion coefﬁcients wnm into the

objective function in (1), when solving the optimization, and computed the PSNR of the

optimal value of the objective function, to examine in our analysis. Henceforth, we denote

this performance metric as WS-PSNR to distinguish it from the traditional PSNR, com-

puted from the optimal value of the objective function without the integration of {wnm}

into the optimization. To carry out the evaluation, we used the popular 3-DOF 8K 60-

fps 360◦ video Runner and 6DOF VR content Museum, as well as actual 3-/6-DOF user

navigation traces that we collected in our wireless VR arena lab [33, 57]. Key system per-

formance parameters were selected in the evaluation to correspond to the computational

and transmission capabilities, respectively, of present state-of-the-art GPUs, VR headsets,

and traditional/emerging wireless technologies [9]. The considered values for these pa-

rameters are captured by the ranges of values representing the axes associated with the

independent variables in the performance graphs included here.

The ﬁrst two sets of performance analysis examples consider the 360◦ video content.

The third analysis example set considers the 6DOF VR content represented as a collection

of spatial 360◦ video viewpoints that users can dynamically select to navigate as they move

across the arena (see Figure 7). Each such omnidirectional viewpoint is compressed into

scalable 360◦ tiling, at 120fps frame rate and 12K spatial resolution panorama.

Figure 13: Performance trade-offs: xGen transmission capability vs. edge server comput-

ing capability (left) and headset rendering vs. computing capabilities (right).

(i) 3-DOF 360◦ video. The performance results presented here are obtained by running

35

the multi-user system optimization as described in the respective section, for many dif-

ferent placement conﬁgurations of the (static) users across the spatial area of the arena,

and then computing the expected per-user performance. We recall that the users are not

moving spatially across the arena during the session given the 3-DOF nature of the con-

tent. In Figure 13 (left), we examine the trade-offs between the transmission capacity of the

xGen link to a user and the edge server’s computing capability, and the resulting viewport

quality experienced by the user. We can see that for lower xGen link capacities, increasing

the server’s decoding speed does not improve the delivered quality of immersion. That

is because in such cases, almost all GOP tiles comprising the enhancement content layer

are streamed as compressed data. Thus, the server’s computing capability does not have

much impact in such settings. On the other hand, for higher xGen link capacity values,

the number of GOP tiles selected to be decoded at the server and streamed as raw data in-

creases. Simultaneously, with the increasing link capacity, there is more room now for the

remaining compressed tiles of the enhancement content layer to be transmitted at higher

data rates. Both of these aspects contribute to higher quality of immersion for the VR user

under such settings.

In Figure 13 (right), we examine the trade-offs between the rendering capability and

decoding capability of the user’s VR headset, and their interrelated impact on the deliv-

ered viewport quality. One can observe that the viewport quality moderately improves

for around 0.2 dB as the rendering speed increases from 1.88 Gpixels/s to 9.4 Gpixels/s.

Further increasing the rendering speed does not impact the delivered immersion ﬁdelity.

On the other hand, one can observe that increasing the headset decoding speed impacts

the WS-PSNR more signiﬁcantly. Concretely, for any given rendering speed examined in

the ﬁgure, an improvement of around 2.5 dB is achieved as the device’s decoding speed is

increased from 100 Mbps to 500 Mbps. That is because, as the decoding speed increases,

the device can decode GOP tiles compressed and transmitted at higher data rates, without

violating the end-to-end latency constraints. Thereby, an improvement in the delivered

viewport quality is achieved.

Next, in Figure 14 (left), we examine the trade-offs between the transmission capacity of

the xGen link and the decoding speed of the headset, and their interrelated impact on the

delivered quality of immersion. One can observe that the experienced WS-PSNR improves

signiﬁcantly as either the link capacity or the decoding speed increases. Concretely, the

36

)

B
d
(

R
N
S
P
S
W

-

76

72

68

64

60

Average = 76.26 dB

 = 1.23 dB

Average = 63.68 dB

 = 1.45 dB

Envisioned
MPEG-DASH

5

10

15

GOP index

20

25

Figure 14: Performance trade-offs: xGen transmission capability vs. client computing ca-

pability (left), and performance advances over a traditional state-of-the-art method (right).

WS-PSNR increases for more than 1.5 dB when the link capacity increases from 700 Mbps

to 1.2 Gbps. Similarly, when the link capacity is 700 Mbps, the WS-PSNR increases from

74.8 dB to 77.2 dB as the headset’s decoding speed increases from 100 Mbps to 500 Mbps.

Moreover, a WS-PSNR gain of around 4 dB is achieved when the link capacity is increased

from 700 Mbps to 1.2 Gbps and the headset’s decoding speed is increased from 100 Mbps

to 500 Mbps.

Increasing the value of either of these key capabilities enables GOP tiles

compressed at higher data rates to be streamed from the server over the xGen link and

decoded in time on the client device, thus improving the WS-PSNR signiﬁcantly.

In Figure 14 (right), we examine the performance beneﬁts enabled by the envisioned

system over the traditional state-of-the-art. We implemented the current streaming stan-

dard MPEG-DASH following [10], to stream the 360◦ content, compressed using HEVC,

over the traditional wireless connectivity link of a user. One can observe that signiﬁcant

performance gains in expected viewport quality and its variation are enabled. Concretely,

MPEG-DASH can only achieve low average viewport quality of 63.68 dB, with a standard

deviation of around 1.5 dB. These outcomes are in line with performance capabilities of

emerging 360◦ practices that can only stream lower resolution and lower frame rate 360◦

videos at low to moderate quality at best, as outlined in the introduction of this article.

On the other hand, the envisioned system enables high-quality viewport with expected

WS-PSNR of 74 - 78 dB and standard deviation of 1.23 dB, for high frame rate and high

resolution 360◦ content, thus providing gains of 10-14 dB in immersion ﬁdelity. These ad-

vances are motivating and are enabled by the dual connectivity streaming, scalable 360◦

37

 
tiling, and edge computing that are synergistically integrated by the envisioned system

together with rigorous end-to-end analysis, for maximum system performance efﬁciency.

A note on interpreting WS-PSNR results and their parallel with the more familiar PSNR

metric may be appropriate here. The tile-level spherical distortion coefﬁcients wnm are less

than one and render the computed expected viewport distortion to be notably smaller. In

our empirical evaluation, this has led to a consistent difference of around 11dB between the

corresponding average PSNR and average WS-PSNR values. Moreover, a relative differ-

ence of a few dB in WS-PSNR may correspond to a difference of several dB in PSNR. Thus,

the performance trade-offs of one method and its performance advances over another are

highlighted more when interpreted through PSNR. Given the limited number of ﬁgures

one can have, we opted to include the present collection of results under the WS-PNSR

metric as most representative of the key performance aspects and advances introduced by

the envisioned VR system concept.

e
m

i
t

r
e
v
o

)

B
d

n

i

R
N
S
P
Y

-

(

y
t
i
l

a
u
Q

t
r
o
p
w
e
V

i

55

50

45

40

35

30

25

20

15
6060

Proposed: WiFi only streaming;
Baseline representation

Prop

PropNoWiFi

LiFi

)

B
d
(

R
N
S
P
Y
g
v
A

-

50

48

46

44

42

40

)

B
d
(

R
N
S
P
Y

-

15

10

5

0

i

e
m
T
n
w
o
D
%

20

15

10

5

0

6070

6080

6090

6100

6120
6110
Time steps (n* t), for 

6130
t = 4 ms

6140

6150

6160

6170

Figure 15: Sample temporal viewport quality in our system. (The user load is 6.) Y-PSNR:

User viewport luminance (Y) video signal PSNR.

(ii) 6DOF VR: Finally, we highlight the simultaneous immersion quality and applica-

tion reliability beneﬁts of the envisioned VR system, when streaming challenging 6DOF

VR content to mobile users in the arena. The performance results presented here are ob-

tained by running the multi-user system optimization as described in the respective sec-

tion. We focus on the use of VLC as a representative xGen technology in this performance

analysis example. Concretely, we examine the delivered viewport quality over time for a

38

 
 
 
 
 
 
 
 
 
 
 
user, in comparison to that for state-of-the-art reference methods. LiFi is a cellular VLC

system that dynamically assigns a moving user to the cell and its stationary transmitter

that maximize her SNR [58]. We also examined a variant of our system that uses only

(single-link) high-frequency wireless transmission (VLC) to stream the content. We refer

to this variant later as PropNoWiFi, to indicate that no traditional (sub-6 GHz) wireless

connectivity link is used in parallel in that case, in the proposed system. There are six si-

multaneous users in the arena. To contrast the earlier performance analysis examples, here

we measured the traditional PSNR of the delivered viewport for a user, to asses its quality.

We can observe from Figure 15 that the viewport quality varies over time for all three

compared methods, due to one of the following two events, both induced from time to

time by rapid head and body navigation movements. Either the VLC link is transiently

dropped or a brief mismatch takes place between the viewport knowledge used to con-

struct the enhancement content layer at the server and the actual user viewport at the

receiving client (see Section 6.4: “Scalable 360◦ Video Tiling and Viewport-Driven Adapta-

tion”). Still, the observed viewport quality variation is much lower for our system, which

considerably increases its quality of experience and reliability, relative to PropNoWiFi and

LiFi that experience an application downtime during such instances. Concretely, viewport

quality gains of 5dB and 7dB and three times smaller standard deviation of viewport qual-

ity are enabled over the latter two methods, as seen from the two smaller inset graphs

to the left. We can also observe that the user experienced 22% application downtime for

PropNoWiFi and LiFi, compared to 0% downtime for our method, as shown in the smaller

inset graph to the right. These beneﬁts merit the technical advances of the envisioned VR

system.

Table 1: Average user performance (six users in the system).

Method Data rate (T)

σT

Y-PSNR

σPSNR

Prop.

1020 Mbps

164.83 Mbps

50.37 dB

5.79 dB

LiFi

550 Mbps

210.89 Mbps

43.83 dB 14.96 dB

In Table 1, we summarize the average user performance in regard to viewport qual-

ity and delivered data rate. We can see that our system consistently outperforms LiFi

across all performance metrics considered, enabled by its synergistic technical advances.

We highlight that MPEG-DASH could only deliver inadequate viewport quality of 39 dB

39

in this setting, as expected. Finally, we note that the proposed system enables another

performance beneﬁt over LiFi, which is higher robustness to the user load, where a more

graceful degradation in performance is provided, as the number of users in the arena is

increased [33]. These results cannot be included here due to the limited space.

8 Conclusions and the road ahead

The two pedagogical aspects of the article, to educate and inspire the reader, have been

necessarily intertwined in its presentation, given the nature of its topic and its objectives.

We started by providing an overview of virtual reality and its key present challenges to-

wards further advancement, highlighting its prospectively most exciting use cases for our

society in the future, comprising high-ﬁdelity remote scene immersion and untethered life-

like navigation. Then, a broad survey of related studies using traditional approaches and

synergistic advances in other technologies, and a system-level primer on virtual reality

and 360◦ video technologies were integrated, to put these challenges in perspective, high-

light the shortcomings of present implementations, and inspire new approaches. Next,

the article provided a contextual high-level review of several emerging technologies and

unconventional techniques, highlighting that only by their synergistic integration we can

aim to overcome the bottlenecks of hyper-intensive computation, ultra-high data rate, and

ultra-low latency toward the envisioned future societal applications.

A novel 6DOF VR system concept that embodies this integration in an indoor setting

and a rigorous analysis that captures the fundamental interplay among communication,

computation, and scalable signal representation that arises herein, and that optimizes the

system’s end-to-end performance were presented subsequently. Finally, several represen-

tative performance analysis examples were provided to highlight these trade-offs and the

beneﬁts of the envisioned system. These outcomes motivate the system as a broad research

platform for further investigations spanning actual implementation and deployment, new

analyses and optimizations, and integration of further technologies and techniques. More-

over, many of the advances introduced by this feature article and such follow-up work

could beneﬁt related technologies, such as augmented reality and holograms.

40

9 Authors

Jacob Chakareski (jacobcha@njit.edu) completed his Ph.D. degree in electrical and com-

puter engineering at Rice University and Stanford University. He is an Associate Professor

in the College of Computing at the New Jersey Institute for Technology (NJIT), Newark,

New Jersey, 07103, USA, where he holds the Panasonic Chair of Sustainability and directs

the AI-Enabled Laboratory for Virtual and Augmented Reality Immersive Communica-

tions and Network Systems. Dr. Chakareski organized the ﬁrst National Science Founda-

tion (NSF) visioning workshop on future virtual and augmented reality communications

and network systems in 2018 [59]. He has held research appointments with Microsoft, HP

Labs, and Ecole Polytechnique F´ed´erale de Lausanne (EPFL), and served on the advisory

board of Frame, Inc (acquired in 2019 by Nutanix, Inc.). His research interests span next

generation virtual and augmented reality systems, UAV-IoT sensing and networking, fast

reinforcement learning, 5G wireless edge computing and caching, ubiquitous immersive

communication, and societal applications. He received the Adobe Data Science Faculty

Research Award in 2017 and 2018, the Swiss NSF Career Award Ambizione (2009), the

AFOSR Faculty Fellowship in 2016 and 2017, and Best Paper Awards at ICC 2017 and

MMSys 2021. His research has been supported by the NSF, NIH, AFOSR, Adobe, Tencent

Research, NVIDIA, and Microsoft. For further information, please visit www.jakov.org.

Mahmudur Khan received the B.Sc. degree in Electrical and Electronic Engineering

from Bangladesh University of Engineering and Technology, in 2011, the M.S. degree in

CSE from the University of Nevada Reno, in 2015, and the Ph.D. degree in CE from the

University of Central Florida, in 2018, respectively. He was a postdoctoral fellow at NJIT

and is presently an Assistant Professor at York College. His research interests include free-

space-optical communications, wireless ad hoc networks, and UAV communications.

Murat Yuksel received the BS degree in CE from Ege University, Izmir, Turkey, in 1996,

and the MS and PhD degrees in CS from RPI, in 1999 and 2002, respectively. He is an As-

sociate Professor with the ECE Department, University of Central Florida (UCF), Orlando.

Prior to UCF, he was with the CSE Department, University of Nevada, Reno as a faculty

member until 2016. He worked as a software engineer with Pepperdata, Sunnyvale, Cal-

ifornia, and a visiting researcher with AT&T Labs and the Los Alamos National Lab. His

research interests include networked, wireless, and computer systems with a recent focus

41

on big-data networking, UAV networks, optical wireless, and network management.

Acknowledgements

To Ilija Chakareski and Stojan Angjelov for their love. The work of Jacob Chakareski

and Mahmudur Khan has been supported in part by the National Science Foundation

(NSF) under awards CCF-2031881, ECCS-2032387, CNS-2040088, CNS-2032033, and CNS-

2106150; by the NIH under award R01EY030470; and by the Panasonic Chair of Sustain-

ability at NJIT. The work of Murat Yuksel has been supported in part by the NSF under

awards CNS-2115215, CNS-2120421, and CNS-1836741. The authors are grateful to the ed-

itors Dr. Matthew McKay, Dr. Robert Heath, and Dr. Laure Blanc-F´eraud, as well as to the

numerous anonymous reviewers for their constructive guidance and comments that have

considerably helped improve the quality of this feature article.

References

[1] Tim Merel, “Why virtual, augmented and mixed reality are the 4th wave of tech,”

Venture Beat, Jul. 2016. [Online]. Available: https://venturebeat.com/business/

why-virtual-augmented-and-mixed-reality-are-the-4th-wave-of-tech/

[2] J. G. Apostolopoulos, P. A. Chou, B. Culbertson, T. Kalker, M. D. Trott, and S. Wee,

“The road to immersive communication,” Proceedings of the IEEE, vol. 100, no. 4, pp.

974–990, Apr. 2012.

[3] Grand View Research, “Virtual reality market size, share & analysis report,

2020-2027,” Jun. 2020. [Online]. Available: https://www.grandviewresearch.com/

industry-analysis/virtual-reality-vr-market

[4] J. Chakareski, “UAV-IoT for next generation virtual reality,” IEEE Trans. Image Process-

ing, vol. 28, no. 12, pp. 5977–5990, Dec. 2019.

[5] B. Begole, “Why the Internet pipes will burst when virtual reality takes off,” Forbes

Magazine, Feb. 2016.

42

[6] E. Knightly, “Scaling Wi-Fi for next generation transformative applications,” Keynote

Presentation, IEEE INFOCOM, Atlanta, GA, USA, May 2017. [Online]. Available:

https://infocom2017.ieee-infocom.org/program/keynote

[7] M. Champel, T. Stockhammer, T. Fautier, E. Thomas, and R. Koenen, “Quality require-

ments for VR,” in Proc. 116th MPEG Meeting of ISO/IEC/JTC1/SC29/WG11, no. MPEG

116/m39532, Chengdu, China, Oct. 17–21, 2016.

[8] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of the high efﬁciency

video coding (HEVC) standard,” IEEE Trans. Circuits and Systems for Video Technology,

vol. 22, no. 12, pp. 1649–1668, Dec. 2012.

[9] E. Cuervo, K. Chintalapudi, and M. Kotaru, “Creating the perfect illusion: What will

it take to create life-like virtual reality headsets?” in Proc. Int’l Workshop Mobile Com-

puting Systems and Applications (HotMobile). Tempe, AZ, USA: ACM, Feb. 2018, pp.

7–12.

[10] S. Petrangeli, V. Swaminathan, M. Hosseini, and F. De Turck, “Improving virtual re-

ality streaming using HTTP/2,” in Proc. Multimedia Systems Conference. ACM, Jun.

2017, pp. 225–228.

[11] O. Le Meur and E. Jain, “Eye-tracking in 360: Methods, challenges, and opportu-

nities,” Tutorial presented at the IEEE Conf. Virtual Reality and 3D User Interfaces.

Osaka, Japan: IEEE, Mar. 2019.

[12] J. L. Gabbard, J. E. Swan, and S. R. Ellis, “Quantitative and qualitative methods for

human-subject experiments in virtual and augmented reality,” Tutorial presented at

the IEEE Conf. Virtual Reality. Orange County, CA, USA: IEEE, Mar. 2012.

[13] J. Isdale, “Introduction to virtual reality technology,” Tutorial presented at the IEEE

Conf. Virtual Reality. Los Angeles, CA, USA: IEEE, Mar. 2003.

[14] E. Bas¸tu ˘g, M. Bennis, M. Medard, and M. Debbah, “Toward interconnected virtual

reality: Opportunities, challenges, and enablers,” IEEE Communications Magazine,

vol. 55, no. 6, pp. 110–117, Jun. 2017.

[15] J. Chakareski and P. Frossard, “Distributed collaboration for enhanced sender-driven

video streaming,” IEEE Trans. Multimedia, vol. 10, no. 5, pp. 858–870, Aug. 2008.

43

[16] J. Chakareski, J. Apostolopoulos, S. Wee, W.-T. Tan, and B. Girod, “R-D hint tracks for

low-complexity R-D optimized video streaming,” in Proc. Int’l Conf. Multimedia and

Exhibition, vol. 2. Taipei, Taiwan: IEEE, Jun. 2004, pp. 1387–1390.

[17] A. B. Reis, J. Chakareski, A. Kassler, and S. Sargento, “Distortion optimized multi-

service scheduling for next generation wireless mesh networks,” in Proc. Int’l Work-

shop on Carrier-grade Wireless Mesh Networks at the Conf. on Computer Communications

(INFOCOM). San Diego, CA, USA: IEEE, Mar. 2010.

[18] P. A. Chou and Z. Miao, “Rate-distortion optimized streaming of packetized media,”

IEEE Trans. Multimedia, vol. 8, no. 2, pp. 390–404, Apr. 2006.

[19] J. Chakareski, R. Aksu, X. Corbillon, G. Simon, and V. Swaminathan, “Viewport-

driven rate-distortion optimized 360◦ video streaming,” in Proc. Int’l Conf. Commu-

nications. Kansas City, MO, USA: IEEE, May 2018.

[20] X. Corbillon, A. Devlic, G. Simon, and J. Chakareski, “Viewport-adaptive navigable

360-degree video delivery,” in Proc. Int’l Conf. Communications. Paris, France: IEEE,

May 2017, (best paper award).

[21] M. Hosseini and V. Swaminathan, “Adaptive 360 VR video streaming: Divide and

conquer!” in Proc. Int’l Symp. Multimedia. San Jose, CA, USA: IEEE, Dec. 2016.

[22] S. Dimitrov and H. Haas, Principles of LED light communications: Towards networked

Li-Fi. Cambridge University Press, 2015.

[23] E. Calvanese Strinati, S. Barbarossa, J. L. Gonzalez-Jimenez et al., “6G: The next fron-

tier: From holographic messaging to artiﬁcial intelligence using subterahertz and visi-

ble light communication,” IEEE Vehicular Technology Magazine, vol. 14, no. 3, pp. 42–50,

2019.

[24] M. S. Rahman, K. Zheng, and H. Gupta, “FSO-VR: Steerable free space optics link for

virtual reality headsets,” in Proc. ACM Workshop on Wearable Systems and Applications,

Munich, Germany, June 2018.

[25] J. Beysens, Q. Wang, A. Galisteo, D. Giustiniano, and S. Pollin, “A cell-free networking

system with visible light,” IEEE/ACM Transactions on Networking, vol. 28, no. 2, pp.

461–476, 2020.

44

[26] S. Blandino, G. Mangraviti, C. Desset, A. Bourdoux, P. Wambacq, and S. Pollin,

“Multi-user hybrid MIMO at 60 GHz using 16-antenna transmitters,” IEEE Transac-

tions on Circuits and Systems I: Regular Papers, vol. 66, no. 2, pp. 848–858, 2018.

[27] L. Liu, R. Zhong, W. Zhang, Y. Liu, J. Zhang, L. Zhang, and M. Gruteser, “Cutting the

cord: Designing a high-quality untethered VR system with low latency remote ren-

dering,” in Proc. Int’l Conf. Mobile Systems, Applications, and Services (MobiSys). Mu-

nich, Germany: ACM, Jun. 2018, pp. 68–80.

[28] O. Abari, D. Bharadia, A. Dufﬁeld, and D. Katabi, “Enabling high-quality untethered

virtual reality,” in Proc. Symp. Networked Systems Design and Implementation. Boston,

MA, USA: USENIX, Mar. 2017, pp. 531–544.

[29] M. Khan and J. Chakareski, “Visible light communication for next generation un-

tethered virtual reality systems,” in Proc. IEEE Int’l Conf. Communications Workshops,

Shanghai, China, May 2019, pp. 1–6.

[30] E. Cuervo, M. Ghobadi, K. Chintalapudi, and M. Kotaru, “Mixed reality ofﬂoad using

free space optics,” U.S. Patent 10 509 463, Dec. 17, 2019.

[31] H. Singh, X. Qin, H.-R. Shao, C. Ngo, C. Y. Kwon, and S. S. Kim, “Support of un-

compressed video streaming over 60GHz wireless networks,” in Proc. Conf. Consumer

Communications and Networking. Las Vegas, NV, USA: IEEE, Jan. 2008, pp. 243–248.

[32] J. Chakareski, “Viewport-adaptive scalable multi-user virtual reality mobile-edge

streaming,” IEEE Trans. Image Processing, vol. 29, no. 1, pp. 6330–6342, Dec. 2020.

[33] J. Chakareski and M. Khan, “Wiﬁ-VLC dual connectivity streaming system for 6DOF

multi-user virtual reality,” in Proc. Int’l Workshop on Network and Operating Systems

Support for Digital Audio and Video (NOSSDAV).

Istanbul, Turkey: ACM, Sep. 2021,

pp. 106–113, (best paper award).

[34] J. Chakareski and S. Gupta, “Multi-connectivity and edge computing for ultra-low-

latency lifelike virtual reality,” in Proc. Int’l Conf. Multimedia and Exhibition. London,

UK: IEEE, Jul. 2020.

[35] “Foveated rendering,” Wikipedia online, Feb. 2021. [Online]. Available: https://en.

wikipedia.org/wiki/Foveated rendering

45

[36] A. Bharambe, J. Pang, and S. Seshan, “Colyseus: A distributed architecture for online

multiplayer games,” in Proc. Symp. Networked Systems Design and Implementation. San

Jose, CA, USA: USENIX, May 2006, pp. 155–168.

[37] J. D. Moss and E. R. Muth, “Characteristics of headmounted displays and their effects

on simulator sickness,” Human Factors: The Journal of the Human Factors and Ergonomics

Society, vol. 53, no. 3, pp. 308–319, Jun. 2011.

[38] “Omnidirectional

(360-degree) camera,” Wikipedia online,

Jun. 2020.

[Online].

Available: https://en.wikipedia.org/wiki/Omnidirectional (360-degree) camera/

[39] I. Sodagar, “The MPEG-DASH standard for multimedia streaming over the Internet,”

IEEE Multimedia, vol. 18, no. 4, pp. 62–67, Apr. 2011.

[40] MPEG-DASH-SRD standard: ISO/IEC 23009-1:2014/Amd 2:2015, “Spatial relation-

ship description, generalized URL parameters and other extensions,” Jul. 2015.

[41] MPEG-DASH-OMAF standard: ISO/IEC FDIS 23090-2, “Omnidirectional Media For-

mat,” Apr. 2018.

[42] T. Barnett, Jr., S. Jain, U. Andra, and T. Khurana, “Cisco Visual Networking

Index (VNI) Complete Forecast Update, 2017–2022,” APJC Cisco Knowledge Network

(CKN) Presentation, Dec. 2018.

[Online]. Available:

https://www.cisco.com/

c/dam/m/en us/network-intelligence/service-provider/digital-transformation/

knowledge-network-webinars/pdfs/1213-business-services-ckn.pdf

[43] P. Popovski, Wireless Connectivity: An Intuitive and Fundamental Guide, 1st ed.

John

Wiley & Sons, Ltd, UK: Wiley, Feb. 2020.

[44] “Tower Pro MG955 Servo Motor,” . [Online]. Available: https://www.towerpro.com.

tw/product/mg995/

[45] I. Tavakkolnia, M. D. Soltani, M. A. Arfaoui, A. Ghrayeb, C. Assi, M. Safari, and

H. Haas, “MIMO system with multi-directional receiver in optical wireless commu-

nications,” in Proc. International Conference on Communications Workshops.

IEEE, 2019,

pp. 1–6.

46

[46] F. Zafar, M. Bakaul, and R. Parthiban, “Laser-diode-based visible light communica-

tion: Toward gigabit class communication,” IEEE Communications Magazine, vol. 55,

no. 2, pp. 144–151, 2017.

[47] Vidyo, Inc., “Scalable video communications for telehealth, education, and the

hybrid enterprise.” [Online]. Available: https://www.vidyo.com

[48] J. M. Boyce, Y. Ye, J. Chen, and A. K. Ramasubramonian, “Overview of SHVC: Scal-

able extensions of the high efﬁciency video coding standard,” IEEE Trans. Circuits and

Systems for Video Technology, vol. 26, no. 1, pp. 20–34, Jan. 2016.

[49] J. Chakareski, V. Velisavljevi´c, and V. Stankovi´c, “User-action-driven view and rate

scalable multiview video coding,” IEEE Trans. Image Processing, vol. 22, no. 9, pp.

3473–3484, Sep. 2013, special issue on 3D Video Representation, Compression, and

Rendering.

[50] J. Chakareski, M. Khan, T. Ropitault, and S. Blandino, “6DOF virtual reality dataset

and performance evaluation of millimeter wave vs. free-space-optical indoor com-

munications systems for lifelike mobile VR streaming,” in Proc. Asilomar Conference on

Signals, Systems, and Computers. Paciﬁc Grove, CA: IEEE, Nov. 2020.

[51] ——, “Millimeter wave and free-space-optics for future dual-connectivity 6DOF mo-

bile multi-user VR streaming,” ACM Trans. Multimedia Computing Communications and

Applications, May 2022, accepted.

[52] F. Qian, B. Han, L. Ji, and V. Gopalakrishnan, “Optimizing 360 video delivery over

cellular networks,” in Proc. Workshop All Things Cellular: Operations, Applications, and

Challenges. New York, NY, USA: ACM, Oct. 2016.

[53] X. Hou, S. Dey, J. Zhang, and M. Budagavi, “Predictive adaptive streaming to enable

mobile 360-degree and vr experiences,” IEEE Trans. Multimedia, vol. 23, pp. 716–731,

Apr. 2021.

[54] S. Blandino, T. Ropitault, R. Caromi, J. Chakareski, M. Khan, and N. Golmie, “Head

rotation model for virtual reality system level simulations,” in Proc. Int’l Symposium

on Multimedia. Naples, Italy: IEEE, Dec. 2021.

47

[55] Y. Sun, A. Lu, and L. Yu, “Weighted-to-spherically-uniform quality evaluation for

omnidirectional video,” IEEE Signal Processing Letters, vol. 24, no. 9, pp. 1408–1412,

Sep. 2017.

[56] G. Xu, “Global optimization of signomial geometric programming problems,” Elsevier

European Journal of Operational Research, vol. 233, no. 3, pp. 500–510, Mar. 2014.

[57] J. Chakareski, R. Aksu, V. Swaminathan, and M. Zink, “Full UHD 360-degree video

dataset and modeling of rate-distortion characteristics and head movement naviga-

tion,” in Proc. Multimedia Systems Conf.

Istanbul, Turkey: ACM, Sep. 2021, pp. 267–

273.

[58] Z. Zeng, M. D. Soltani, M. Safari, and H. Haas, “Angle diversity receiver in LiFi cel-

lular networks,” in Proc. IEEE International Conference on Communications, Shanghai,

China, May 2019, pp. 1–6.

[59] J. Chakareski, “The Future VR/AR Network - Towards Virtual Human/Object

Teleportation,” Signal Processing Newsletter Online, IEEE Signal Processing Society,

Mar. 2019.

[Online]. Available:

https://signalprocessingsociety.org/newsletter/

2019/03/future-vrar-network-towards-virtual-humanobject-teleportation/

48

